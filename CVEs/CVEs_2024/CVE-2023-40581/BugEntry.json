{"buggy_code": ["[\n    {\n        \"action\": \"add\",\n        \"when\": \"29cb20bd563c02671b31dd840139e93dd37150a1\",\n        \"short\": \"[priority] **A new release type has been added!**\\n    * [`nightly`](https://github.com/yt-dlp/yt-dlp/releases/tag/nightly) builds will be made after each push, containing the latest fixes (but also possibly bugs).\\n    * When using `--update`/`-U`, a release binary will only update to its current channel (either `stable` or `nightly`).\\n    * The `--update-to` option has been added allowing the user more control over program upgrades (or downgrades).\\n    * `--update-to` can change the release channel (`stable`, `nightly`) and also upgrade or downgrade to specific tags.\\n    * **Usage**: `--update-to CHANNEL`, `--update-to TAG`, `--update-to CHANNEL@TAG`\"\n    },\n    {\n        \"action\": \"add\",\n        \"when\": \"5038f6d713303e0967d002216e7a88652401c22a\",\n        \"short\": \"[priority] **YouTube throttling fixes!**\"\n    },\n    {\n        \"action\": \"remove\",\n        \"when\": \"2e023649ea4e11151545a34dc1360c114981a236\"\n    },\n    {\n        \"action\": \"add\",\n        \"when\": \"01aba2519a0884ef17d5f85608dbd2a455577147\",\n        \"short\": \"[priority] YouTube: Improved throttling and signature fixes\"\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"c86e433c35fe5da6cb29f3539eef97497f84ed38\",\n        \"short\": \"[extractor/niconico:series] Fix extraction (#6898)\",\n        \"authors\": [\"sqrtNOT\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"69a40e4a7f6caa5662527ebd2f3c4e8aa02857a2\",\n        \"short\": \"[extractor/youtube:music_search_url] Extract title (#7102)\",\n        \"authors\": [\"kangalio\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"8417f26b8a819cd7ffcd4e000ca3e45033e670fb\",\n        \"short\": \"Add option `--color` (#6904)\",\n        \"authors\": [\"Grub4K\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"b4e0d75848e9447cee2cd3646ce54d4744a7ff56\",\n        \"short\": \"Improve `--download-sections`\\n    - Support negative time-ranges\\n    - Add `*from-url` to obey time-ranges in URL\",\n        \"authors\": [\"pukkandan\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"1e75d97db21152acc764b30a688e516f04b8a142\",\n        \"short\": \"[extractor/youtube] Add `ios` to default clients used\\n        - IOS is affected neither by 403 nor by nsig so helps mitigate them preemptively\\n        - IOS also has higher bit-rate 'premium' formats though they are not labeled as such\",\n        \"authors\": [\"pukkandan\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"f2ff0f6f1914b82d4a51681a72cc0828115dcb4a\",\n        \"short\": \"[extractor/motherless] Add gallery support, fix groups (#7211)\",\n        \"authors\": [\"rexlambert22\", \"Ti4eeT4e\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"a4486bfc1dc7057efca9dd3fe70d7fa25c56f700\",\n        \"short\": \"[misc] Revert \\\"Add automatic duplicate issue detection\\\"\",\n        \"authors\": [\"pukkandan\"]\n    },\n    {\n        \"action\": \"add\",\n        \"when\": \"1ceb657bdd254ad961489e5060f2ccc7d556b729\",\n        \"short\": \"[priority] Security: [[CVE-2023-35934](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-35934)] Fix [Cookie leak](https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-v8mc-9377-rwjj)\\n    - `--add-header Cookie:` is deprecated and auto-scoped to input URL domains\\n    - Cookies are scoped when passed to external downloaders\\n    - Add `cookies` field to info.json and deprecate `http_headers.Cookie`\"\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"b03fa7834579a01cc5fba48c0e73488a16683d48\",\n        \"short\": \"[ie/twitter] Revert 92315c03774cfabb3a921884326beb4b981f786b\",\n        \"authors\": [\"pukkandan\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"fcd6a76adc49d5cd8783985c7ce35384b72e545f\",\n        \"short\": \"[test] Add tests for socks proxies (#7908)\",\n        \"authors\": [\"coletdjnz\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"4bf912282a34b58b6b35d8f7e6be535770c89c76\",\n        \"short\": \"[rh:urllib] Remove dot segments during URL normalization (#7662)\",\n        \"authors\": [\"coletdjnz\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"59e92b1f1833440bb2190f847eb735cf0f90bc85\",\n        \"short\": \"[rh:urllib] Simplify gzip decoding (#7611)\",\n        \"authors\": [\"Grub4K\"]\n    },\n    {\n        \"action\": \"add\",\n        \"when\": \"c1d71d0d9f41db5e4306c86af232f5f6220a130b\",\n        \"short\": \"[priority] **The minimum *recommended* Python version has been raised to 3.8**\\nSince Python 3.7 has reached end-of-life, support for it will be dropped soon. [Read more](https://github.com/yt-dlp/yt-dlp/issues/7803)\"\n    }\n]\n", "#!/usr/bin/env python3\n\n# Allow direct execution\nimport os\nimport sys\nimport unittest\n\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n\nimport copy\nimport json\n\nfrom test.helper import FakeYDL, assertRegexpMatches, try_rm\nfrom yt_dlp import YoutubeDL\nfrom yt_dlp.compat import compat_os_name\nfrom yt_dlp.extractor import YoutubeIE\nfrom yt_dlp.extractor.common import InfoExtractor\nfrom yt_dlp.postprocessor.common import PostProcessor\nfrom yt_dlp.utils import (\n    ExtractorError,\n    LazyList,\n    OnDemandPagedList,\n    int_or_none,\n    match_filter_func,\n)\nfrom yt_dlp.utils.traversal import traverse_obj\n\nTEST_URL = 'http://localhost/sample.mp4'\n\n\nclass YDL(FakeYDL):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.downloaded_info_dicts = []\n        self.msgs = []\n\n    def process_info(self, info_dict):\n        self.downloaded_info_dicts.append(info_dict.copy())\n\n    def to_screen(self, msg, *args, **kwargs):\n        self.msgs.append(msg)\n\n    def dl(self, *args, **kwargs):\n        assert False, 'Downloader must not be invoked for test_YoutubeDL'\n\n\ndef _make_result(formats, **kwargs):\n    res = {\n        'formats': formats,\n        'id': 'testid',\n        'title': 'testttitle',\n        'extractor': 'testex',\n        'extractor_key': 'TestEx',\n        'webpage_url': 'http://example.com/watch?v=shenanigans',\n    }\n    res.update(**kwargs)\n    return res\n\n\nclass TestFormatSelection(unittest.TestCase):\n    def test_prefer_free_formats(self):\n        # Same resolution => download webm\n        ydl = YDL()\n        ydl.params['prefer_free_formats'] = True\n        formats = [\n            {'ext': 'webm', 'height': 460, 'url': TEST_URL},\n            {'ext': 'mp4', 'height': 460, 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['ext'], 'webm')\n\n        # Different resolution => download best quality (mp4)\n        ydl = YDL()\n        ydl.params['prefer_free_formats'] = True\n        formats = [\n            {'ext': 'webm', 'height': 720, 'url': TEST_URL},\n            {'ext': 'mp4', 'height': 1080, 'url': TEST_URL},\n        ]\n        info_dict['formats'] = formats\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['ext'], 'mp4')\n\n        # No prefer_free_formats => prefer mp4 and webm\n        ydl = YDL()\n        ydl.params['prefer_free_formats'] = False\n        formats = [\n            {'ext': 'webm', 'height': 720, 'url': TEST_URL},\n            {'ext': 'mp4', 'height': 720, 'url': TEST_URL},\n            {'ext': 'flv', 'height': 720, 'url': TEST_URL},\n        ]\n        info_dict['formats'] = formats\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['ext'], 'mp4')\n\n        ydl = YDL()\n        ydl.params['prefer_free_formats'] = False\n        formats = [\n            {'ext': 'flv', 'height': 720, 'url': TEST_URL},\n            {'ext': 'webm', 'height': 720, 'url': TEST_URL},\n        ]\n        info_dict['formats'] = formats\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['ext'], 'webm')\n\n    def test_format_selection(self):\n        formats = [\n            {'format_id': '35', 'ext': 'mp4', 'preference': 0, 'url': TEST_URL},\n            {'format_id': 'example-with-dashes', 'ext': 'webm', 'preference': 1, 'url': TEST_URL},\n            {'format_id': '45', 'ext': 'webm', 'preference': 2, 'url': TEST_URL},\n            {'format_id': '47', 'ext': 'webm', 'preference': 3, 'url': TEST_URL},\n            {'format_id': '2', 'ext': 'flv', 'preference': 4, 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        def test(inp, *expected, multi=False):\n            ydl = YDL({\n                'format': inp,\n                'allow_multiple_video_streams': multi,\n                'allow_multiple_audio_streams': multi,\n            })\n            ydl.process_ie_result(info_dict.copy())\n            downloaded = map(lambda x: x['format_id'], ydl.downloaded_info_dicts)\n            self.assertEqual(list(downloaded), list(expected))\n\n        test('20/47', '47')\n        test('20/71/worst', '35')\n        test(None, '2')\n        test('webm/mp4', '47')\n        test('3gp/40/mp4', '35')\n        test('example-with-dashes', 'example-with-dashes')\n        test('all', '2', '47', '45', 'example-with-dashes', '35')\n        test('mergeall', '2+47+45+example-with-dashes+35', multi=True)\n\n    def test_format_selection_audio(self):\n        formats = [\n            {'format_id': 'audio-low', 'ext': 'webm', 'preference': 1, 'vcodec': 'none', 'url': TEST_URL},\n            {'format_id': 'audio-mid', 'ext': 'webm', 'preference': 2, 'vcodec': 'none', 'url': TEST_URL},\n            {'format_id': 'audio-high', 'ext': 'flv', 'preference': 3, 'vcodec': 'none', 'url': TEST_URL},\n            {'format_id': 'vid', 'ext': 'mp4', 'preference': 4, 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'bestaudio'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'audio-high')\n\n        ydl = YDL({'format': 'worstaudio'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'audio-low')\n\n        formats = [\n            {'format_id': 'vid-low', 'ext': 'mp4', 'preference': 1, 'url': TEST_URL},\n            {'format_id': 'vid-high', 'ext': 'mp4', 'preference': 2, 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'bestaudio/worstaudio/best'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'vid-high')\n\n    def test_format_selection_audio_exts(self):\n        formats = [\n            {'format_id': 'mp3-64', 'ext': 'mp3', 'abr': 64, 'url': 'http://_', 'vcodec': 'none'},\n            {'format_id': 'ogg-64', 'ext': 'ogg', 'abr': 64, 'url': 'http://_', 'vcodec': 'none'},\n            {'format_id': 'aac-64', 'ext': 'aac', 'abr': 64, 'url': 'http://_', 'vcodec': 'none'},\n            {'format_id': 'mp3-32', 'ext': 'mp3', 'abr': 32, 'url': 'http://_', 'vcodec': 'none'},\n            {'format_id': 'aac-32', 'ext': 'aac', 'abr': 32, 'url': 'http://_', 'vcodec': 'none'},\n        ]\n\n        info_dict = _make_result(formats)\n        ydl = YDL({'format': 'best'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(copy.deepcopy(info_dict))\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'aac-64')\n\n        ydl = YDL({'format': 'mp3'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(copy.deepcopy(info_dict))\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'mp3-64')\n\n        ydl = YDL({'prefer_free_formats': True})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(copy.deepcopy(info_dict))\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'ogg-64')\n\n    def test_format_selection_video(self):\n        formats = [\n            {'format_id': 'dash-video-low', 'ext': 'mp4', 'preference': 1, 'acodec': 'none', 'url': TEST_URL},\n            {'format_id': 'dash-video-high', 'ext': 'mp4', 'preference': 2, 'acodec': 'none', 'url': TEST_URL},\n            {'format_id': 'vid', 'ext': 'mp4', 'preference': 3, 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'bestvideo'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'dash-video-high')\n\n        ydl = YDL({'format': 'worstvideo'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'dash-video-low')\n\n        ydl = YDL({'format': 'bestvideo[format_id^=dash][format_id$=low]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'dash-video-low')\n\n        formats = [\n            {'format_id': 'vid-vcodec-dot', 'ext': 'mp4', 'preference': 1, 'vcodec': 'avc1.123456', 'acodec': 'none', 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'bestvideo[vcodec=avc1.123456]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'vid-vcodec-dot')\n\n    def test_format_selection_string_ops(self):\n        formats = [\n            {'format_id': 'abc-cba', 'ext': 'mp4', 'url': TEST_URL},\n            {'format_id': 'zxc-cxz', 'ext': 'webm', 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        # equals (=)\n        ydl = YDL({'format': '[format_id=abc-cba]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'abc-cba')\n\n        # does not equal (!=)\n        ydl = YDL({'format': '[format_id!=abc-cba]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n\n        ydl = YDL({'format': '[format_id!=abc-cba][format_id!=zxc-cxz]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n        # starts with (^=)\n        ydl = YDL({'format': '[format_id^=abc]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'abc-cba')\n\n        # does not start with (!^=)\n        ydl = YDL({'format': '[format_id!^=abc]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n\n        ydl = YDL({'format': '[format_id!^=abc][format_id!^=zxc]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n        # ends with ($=)\n        ydl = YDL({'format': '[format_id$=cba]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'abc-cba')\n\n        # does not end with (!$=)\n        ydl = YDL({'format': '[format_id!$=cba]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n\n        ydl = YDL({'format': '[format_id!$=cba][format_id!$=cxz]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n        # contains (*=)\n        ydl = YDL({'format': '[format_id*=bc-cb]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'abc-cba')\n\n        # does not contain (!*=)\n        ydl = YDL({'format': '[format_id!*=bc-cb]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n\n        ydl = YDL({'format': '[format_id!*=abc][format_id!*=zxc]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n        ydl = YDL({'format': '[format_id!*=-]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n    def test_youtube_format_selection(self):\n        # FIXME: Rewrite in accordance with the new format sorting options\n        return\n\n        order = [\n            '38', '37', '46', '22', '45', '35', '44', '18', '34', '43', '6', '5', '17', '36', '13',\n            # Apple HTTP Live Streaming\n            '96', '95', '94', '93', '92', '132', '151',\n            # 3D\n            '85', '84', '102', '83', '101', '82', '100',\n            # Dash video\n            '137', '248', '136', '247', '135', '246',\n            '245', '244', '134', '243', '133', '242', '160',\n            # Dash audio\n            '141', '172', '140', '171', '139',\n        ]\n\n        def format_info(f_id):\n            info = YoutubeIE._formats[f_id].copy()\n\n            # XXX: In real cases InfoExtractor._parse_mpd_formats() fills up 'acodec'\n            # and 'vcodec', while in tests such information is incomplete since\n            # commit a6c2c24479e5f4827ceb06f64d855329c0a6f593\n            # test_YoutubeDL.test_youtube_format_selection is broken without\n            # this fix\n            if 'acodec' in info and 'vcodec' not in info:\n                info['vcodec'] = 'none'\n            elif 'vcodec' in info and 'acodec' not in info:\n                info['acodec'] = 'none'\n\n            info['format_id'] = f_id\n            info['url'] = 'url:' + f_id\n            return info\n        formats_order = [format_info(f_id) for f_id in order]\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': 'bestvideo+bestaudio'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], '248+172')\n        self.assertEqual(downloaded['ext'], 'mp4')\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': 'bestvideo[height>=999999]+bestaudio/best'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], '38')\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': 'bestvideo/best,bestaudio'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n        self.assertEqual(downloaded_ids, ['137', '141'])\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': '(bestvideo[ext=mp4],bestvideo[ext=webm])+bestaudio'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n        self.assertEqual(downloaded_ids, ['137+141', '248+141'])\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': '(bestvideo[ext=mp4],bestvideo[ext=webm])[height<=720]+bestaudio'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n        self.assertEqual(downloaded_ids, ['136+141', '247+141'])\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': '(bestvideo[ext=none]/bestvideo[ext=webm])+bestaudio'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n        self.assertEqual(downloaded_ids, ['248+141'])\n\n        for f1, f2 in zip(formats_order, formats_order[1:]):\n            info_dict = _make_result([f1, f2], extractor='youtube')\n            ydl = YDL({'format': 'best/bestvideo'})\n            ydl.sort_formats(info_dict)\n            ydl.process_ie_result(info_dict)\n            downloaded = ydl.downloaded_info_dicts[0]\n            self.assertEqual(downloaded['format_id'], f1['format_id'])\n\n            info_dict = _make_result([f2, f1], extractor='youtube')\n            ydl = YDL({'format': 'best/bestvideo'})\n            ydl.sort_formats(info_dict)\n            ydl.process_ie_result(info_dict)\n            downloaded = ydl.downloaded_info_dicts[0]\n            self.assertEqual(downloaded['format_id'], f1['format_id'])\n\n    def test_audio_only_extractor_format_selection(self):\n        # For extractors with incomplete formats (all formats are audio-only or\n        # video-only) best and worst should fallback to corresponding best/worst\n        # video-only or audio-only formats (as per\n        # https://github.com/ytdl-org/youtube-dl/pull/5556)\n        formats = [\n            {'format_id': 'low', 'ext': 'mp3', 'preference': 1, 'vcodec': 'none', 'url': TEST_URL},\n            {'format_id': 'high', 'ext': 'mp3', 'preference': 2, 'vcodec': 'none', 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'best'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'high')\n\n        ydl = YDL({'format': 'worst'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'low')\n\n    def test_format_not_available(self):\n        formats = [\n            {'format_id': 'regular', 'ext': 'mp4', 'height': 360, 'url': TEST_URL},\n            {'format_id': 'video', 'ext': 'mp4', 'height': 720, 'acodec': 'none', 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        # This must fail since complete video-audio format does not match filter\n        # and extractor does not provide incomplete only formats (i.e. only\n        # video-only or audio-only).\n        ydl = YDL({'format': 'best[height>360]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n    def test_format_selection_issue_10083(self):\n        # See https://github.com/ytdl-org/youtube-dl/issues/10083\n        formats = [\n            {'format_id': 'regular', 'height': 360, 'url': TEST_URL},\n            {'format_id': 'video', 'height': 720, 'acodec': 'none', 'url': TEST_URL},\n            {'format_id': 'audio', 'vcodec': 'none', 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'best[height>360]/bestvideo[height>360]+bestaudio'})\n        ydl.process_ie_result(info_dict.copy())\n        self.assertEqual(ydl.downloaded_info_dicts[0]['format_id'], 'video+audio')\n\n    def test_invalid_format_specs(self):\n        def assert_syntax_error(format_spec):\n            self.assertRaises(SyntaxError, YDL, {'format': format_spec})\n\n        assert_syntax_error('bestvideo,,best')\n        assert_syntax_error('+bestaudio')\n        assert_syntax_error('bestvideo+')\n        assert_syntax_error('/')\n        assert_syntax_error('[720<height]')\n\n    def test_format_filtering(self):\n        formats = [\n            {'format_id': 'A', 'filesize': 500, 'width': 1000},\n            {'format_id': 'B', 'filesize': 1000, 'width': 500},\n            {'format_id': 'C', 'filesize': 1000, 'width': 400},\n            {'format_id': 'D', 'filesize': 2000, 'width': 600},\n            {'format_id': 'E', 'filesize': 3000},\n            {'format_id': 'F'},\n            {'format_id': 'G', 'filesize': 1000000},\n        ]\n        for f in formats:\n            f['url'] = 'http://_/'\n            f['ext'] = 'unknown'\n        info_dict = _make_result(formats, _format_sort_fields=('id', ))\n\n        ydl = YDL({'format': 'best[filesize<3000]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'D')\n\n        ydl = YDL({'format': 'best[filesize<=3000]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'E')\n\n        ydl = YDL({'format': 'best[filesize <= ? 3000]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'F')\n\n        ydl = YDL({'format': 'best [filesize = 1000] [width>450]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'B')\n\n        ydl = YDL({'format': 'best [filesize = 1000] [width!=450]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'C')\n\n        ydl = YDL({'format': '[filesize>?1]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'G')\n\n        ydl = YDL({'format': '[filesize<1M]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'E')\n\n        ydl = YDL({'format': '[filesize<1MiB]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'G')\n\n        ydl = YDL({'format': 'all[width>=400][width<=600]'})\n        ydl.process_ie_result(info_dict)\n        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n        self.assertEqual(downloaded_ids, ['D', 'C', 'B'])\n\n        ydl = YDL({'format': 'best[height<40]'})\n        try:\n            ydl.process_ie_result(info_dict)\n        except ExtractorError:\n            pass\n        self.assertEqual(ydl.downloaded_info_dicts, [])\n\n    def test_default_format_spec(self):\n        ydl = YDL({'simulate': True})\n        self.assertEqual(ydl._default_format_spec({}), 'bestvideo*+bestaudio/best')\n\n        ydl = YDL({})\n        self.assertEqual(ydl._default_format_spec({'is_live': True}), 'best/bestvideo+bestaudio')\n\n        ydl = YDL({'simulate': True})\n        self.assertEqual(ydl._default_format_spec({'is_live': True}), 'bestvideo*+bestaudio/best')\n\n        ydl = YDL({'outtmpl': '-'})\n        self.assertEqual(ydl._default_format_spec({}), 'best/bestvideo+bestaudio')\n\n        ydl = YDL({})\n        self.assertEqual(ydl._default_format_spec({}, download=False), 'bestvideo*+bestaudio/best')\n        self.assertEqual(ydl._default_format_spec({'is_live': True}), 'best/bestvideo+bestaudio')\n\n\nclass TestYoutubeDL(unittest.TestCase):\n    def test_subtitles(self):\n        def s_formats(lang, autocaption=False):\n            return [{\n                'ext': ext,\n                'url': f'http://localhost/video.{lang}.{ext}',\n                '_auto': autocaption,\n            } for ext in ['vtt', 'srt', 'ass']]\n        subtitles = {l: s_formats(l) for l in ['en', 'fr', 'es']}\n        auto_captions = {l: s_formats(l, True) for l in ['it', 'pt', 'es']}\n        info_dict = {\n            'id': 'test',\n            'title': 'Test',\n            'url': 'http://localhost/video.mp4',\n            'subtitles': subtitles,\n            'automatic_captions': auto_captions,\n            'extractor': 'TEST',\n            'webpage_url': 'http://example.com/watch?v=shenanigans',\n        }\n\n        def get_info(params={}):\n            params.setdefault('simulate', True)\n            ydl = YDL(params)\n            ydl.report_warning = lambda *args, **kargs: None\n            return ydl.process_video_result(info_dict, download=False)\n\n        result = get_info()\n        self.assertFalse(result.get('requested_subtitles'))\n        self.assertEqual(result['subtitles'], subtitles)\n        self.assertEqual(result['automatic_captions'], auto_captions)\n\n        result = get_info({'writesubtitles': True})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'en'})\n        self.assertTrue(subs['en'].get('data') is None)\n        self.assertEqual(subs['en']['ext'], 'ass')\n\n        result = get_info({'writesubtitles': True, 'subtitlesformat': 'foo/srt'})\n        subs = result['requested_subtitles']\n        self.assertEqual(subs['en']['ext'], 'srt')\n\n        result = get_info({'writesubtitles': True, 'subtitleslangs': ['es', 'fr', 'it']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'es', 'fr'})\n\n        result = get_info({'writesubtitles': True, 'subtitleslangs': ['all', '-en']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'es', 'fr'})\n\n        result = get_info({'writesubtitles': True, 'subtitleslangs': ['en', 'fr', '-en']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'fr'})\n\n        result = get_info({'writesubtitles': True, 'subtitleslangs': ['-en', 'en']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'en'})\n\n        result = get_info({'writesubtitles': True, 'subtitleslangs': ['e.+']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'es', 'en'})\n\n        result = get_info({'writesubtitles': True, 'writeautomaticsub': True, 'subtitleslangs': ['es', 'pt']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'es', 'pt'})\n        self.assertFalse(subs['es']['_auto'])\n        self.assertTrue(subs['pt']['_auto'])\n\n        result = get_info({'writeautomaticsub': True, 'subtitleslangs': ['es', 'pt']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'es', 'pt'})\n        self.assertTrue(subs['es']['_auto'])\n        self.assertTrue(subs['pt']['_auto'])\n\n    def test_add_extra_info(self):\n        test_dict = {\n            'extractor': 'Foo',\n        }\n        extra_info = {\n            'extractor': 'Bar',\n            'playlist': 'funny videos',\n        }\n        YDL.add_extra_info(test_dict, extra_info)\n        self.assertEqual(test_dict['extractor'], 'Foo')\n        self.assertEqual(test_dict['playlist'], 'funny videos')\n\n    outtmpl_info = {\n        'id': '1234',\n        'ext': 'mp4',\n        'width': None,\n        'height': 1080,\n        'filesize': 1024,\n        'title1': '$PATH',\n        'title2': '%PATH%',\n        'title3': 'foo/bar\\\\test',\n        'title4': 'foo \"bar\" test',\n        'title5': '\u00e1\u00e9\u00ed \ud835\udc00',\n        'timestamp': 1618488000,\n        'duration': 100000,\n        'playlist_index': 1,\n        'playlist_autonumber': 2,\n        '__last_playlist_index': 100,\n        'n_entries': 10,\n        'formats': [\n            {'id': 'id 1', 'height': 1080, 'width': 1920},\n            {'id': 'id 2', 'height': 720},\n            {'id': 'id 3'}\n        ]\n    }\n\n    def test_prepare_outtmpl_and_filename(self):\n        def test(tmpl, expected, *, info=None, **params):\n            params['outtmpl'] = tmpl\n            ydl = FakeYDL(params)\n            ydl._num_downloads = 1\n            self.assertEqual(ydl.validate_outtmpl(tmpl), None)\n\n            out = ydl.evaluate_outtmpl(tmpl, info or self.outtmpl_info)\n            fname = ydl.prepare_filename(info or self.outtmpl_info)\n\n            if not isinstance(expected, (list, tuple)):\n                expected = (expected, expected)\n            for (name, got), expect in zip((('outtmpl', out), ('filename', fname)), expected):\n                if callable(expect):\n                    self.assertTrue(expect(got), f'Wrong {name} from {tmpl}')\n                elif expect is not None:\n                    self.assertEqual(got, expect, f'Wrong {name} from {tmpl}')\n\n        # Side-effects\n        original_infodict = dict(self.outtmpl_info)\n        test('foo.bar', 'foo.bar')\n        original_infodict['epoch'] = self.outtmpl_info.get('epoch')\n        self.assertTrue(isinstance(original_infodict['epoch'], int))\n        test('%(epoch)d', int_or_none)\n        self.assertEqual(original_infodict, self.outtmpl_info)\n\n        # Auto-generated fields\n        test('%(id)s.%(ext)s', '1234.mp4')\n        test('%(duration_string)s', ('27:46:40', '27-46-40'))\n        test('%(resolution)s', '1080p')\n        test('%(playlist_index|)s', '001')\n        test('%(playlist_index&{}!)s', '1!')\n        test('%(playlist_autonumber)s', '02')\n        test('%(autonumber)s', '00001')\n        test('%(autonumber+2)03d', '005', autonumber_start=3)\n        test('%(autonumber)s', '001', autonumber_size=3)\n\n        # Escaping %\n        test('%', '%')\n        test('%%', '%')\n        test('%%%%', '%%')\n        test('%s', '%s')\n        test('%%%s', '%%s')\n        test('%d', '%d')\n        test('%abc%', '%abc%')\n        test('%%(width)06d.%(ext)s', '%(width)06d.mp4')\n        test('%%%(height)s', '%1080')\n        test('%(width)06d.%(ext)s', 'NA.mp4')\n        test('%(width)06d.%%(ext)s', 'NA.%(ext)s')\n        test('%%(width)06d.%(ext)s', '%(width)06d.mp4')\n\n        # ID sanitization\n        test('%(id)s', '_abcd', info={'id': '_abcd'})\n        test('%(some_id)s', '_abcd', info={'some_id': '_abcd'})\n        test('%(formats.0.id)s', '_abcd', info={'formats': [{'id': '_abcd'}]})\n        test('%(id)s', '-abcd', info={'id': '-abcd'})\n        test('%(id)s', '.abcd', info={'id': '.abcd'})\n        test('%(id)s', 'ab__cd', info={'id': 'ab__cd'})\n        test('%(id)s', ('ab:cd', 'ab\uff1acd'), info={'id': 'ab:cd'})\n        test('%(id.0)s', '-', info={'id': '--'})\n\n        # Invalid templates\n        self.assertTrue(isinstance(YoutubeDL.validate_outtmpl('%(title)'), ValueError))\n        test('%(invalid@tmpl|def)s', 'none', outtmpl_na_placeholder='none')\n        test('%(..)s', 'NA')\n        test('%(formats.{id)s', 'NA')\n\n        # Entire info_dict\n        def expect_same_infodict(out):\n            got_dict = json.loads(out)\n            for info_field, expected in self.outtmpl_info.items():\n                self.assertEqual(got_dict.get(info_field), expected, info_field)\n            return True\n\n        test('%()j', (expect_same_infodict, str))\n\n        # NA placeholder\n        NA_TEST_OUTTMPL = '%(uploader_date)s-%(width)d-%(x|def)s-%(id)s.%(ext)s'\n        test(NA_TEST_OUTTMPL, 'NA-NA-def-1234.mp4')\n        test(NA_TEST_OUTTMPL, 'none-none-def-1234.mp4', outtmpl_na_placeholder='none')\n        test(NA_TEST_OUTTMPL, '--def-1234.mp4', outtmpl_na_placeholder='')\n        test('%(non_existent.0)s', 'NA')\n\n        # String formatting\n        FMT_TEST_OUTTMPL = '%%(height)%s.%%(ext)s'\n        test(FMT_TEST_OUTTMPL % 's', '1080.mp4')\n        test(FMT_TEST_OUTTMPL % 'd', '1080.mp4')\n        test(FMT_TEST_OUTTMPL % '6d', '  1080.mp4')\n        test(FMT_TEST_OUTTMPL % '-6d', '1080  .mp4')\n        test(FMT_TEST_OUTTMPL % '06d', '001080.mp4')\n        test(FMT_TEST_OUTTMPL % ' 06d', ' 01080.mp4')\n        test(FMT_TEST_OUTTMPL % '   06d', ' 01080.mp4')\n        test(FMT_TEST_OUTTMPL % '0 6d', ' 01080.mp4')\n        test(FMT_TEST_OUTTMPL % '0   6d', ' 01080.mp4')\n        test(FMT_TEST_OUTTMPL % '   0   6d', ' 01080.mp4')\n\n        # Type casting\n        test('%(id)d', '1234')\n        test('%(height)c', '1')\n        test('%(ext)c', 'm')\n        test('%(id)d %(id)r', \"1234 '1234'\")\n        test('%(id)r %(height)r', \"'1234' 1080\")\n        test('%(title5)a %(height)a', (R\"'\\xe1\\xe9\\xed \\U0001d400' 1080\", None))\n        test('%(ext)s-%(ext|def)d', 'mp4-def')\n        test('%(width|0)04d', '0')\n        test('a%(width|b)d', 'ab', outtmpl_na_placeholder='none')\n\n        FORMATS = self.outtmpl_info['formats']\n\n        # Custom type casting\n        test('%(formats.:.id)l', 'id 1, id 2, id 3')\n        test('%(formats.:.id)#l', ('id 1\\nid 2\\nid 3', 'id 1 id 2 id 3'))\n        test('%(ext)l', 'mp4')\n        test('%(formats.:.id) 18l', '  id 1, id 2, id 3')\n        test('%(formats)j', (json.dumps(FORMATS), None))\n        test('%(formats)#j', (\n            json.dumps(FORMATS, indent=4),\n            json.dumps(FORMATS, indent=4).replace(':', '\uff1a').replace('\"', \"\uff02\").replace('\\n', ' ')\n        ))\n        test('%(title5).3B', '\u00e1')\n        test('%(title5)U', '\u00e1\u00e9\u00ed \ud835\udc00')\n        test('%(title5)#U', 'a\\u0301e\\u0301i\\u0301 \ud835\udc00')\n        test('%(title5)+U', '\u00e1\u00e9\u00ed A')\n        test('%(title5)+#U', 'a\\u0301e\\u0301i\\u0301 A')\n        test('%(height)D', '1k')\n        test('%(filesize)#D', '1Ki')\n        test('%(height)5.2D', ' 1.08k')\n        test('%(title4)#S', 'foo_bar_test')\n        test('%(title4).10S', ('foo \uff02bar\uff02 ', 'foo \uff02bar\uff02' + ('#' if compat_os_name == 'nt' else ' ')))\n        if compat_os_name == 'nt':\n            test('%(title4)q', ('\"foo \\\\\"bar\\\\\" test\"', \"\uff02foo \u29f9\uff02bar\u29f9\uff02 test\uff02\"))\n            test('%(formats.:.id)#q', ('\"id 1\" \"id 2\" \"id 3\"', '\uff02id 1\uff02 \uff02id 2\uff02 \uff02id 3\uff02'))\n            test('%(formats.0.id)#q', ('\"id 1\"', '\uff02id 1\uff02'))\n        else:\n            test('%(title4)q', ('\\'foo \"bar\" test\\'', '\\'foo \uff02bar\uff02 test\\''))\n            test('%(formats.:.id)#q', \"'id 1' 'id 2' 'id 3'\")\n            test('%(formats.0.id)#q', \"'id 1'\")\n\n        # Internal formatting\n        test('%(timestamp-1000>%H-%M-%S)s', '11-43-20')\n        test('%(title|%)s %(title|%%)s', '% %%')\n        test('%(id+1-height+3)05d', '00158')\n        test('%(width+100)05d', 'NA')\n        test('%(formats.0) 15s', ('% 15s' % FORMATS[0], None))\n        test('%(formats.0)r', (repr(FORMATS[0]), None))\n        test('%(height.0)03d', '001')\n        test('%(-height.0)04d', '-001')\n        test('%(formats.-1.id)s', FORMATS[-1]['id'])\n        test('%(formats.0.id.-1)d', FORMATS[0]['id'][-1])\n        test('%(formats.3)s', 'NA')\n        test('%(formats.:2:-1)r', repr(FORMATS[:2:-1]))\n        test('%(formats.0.id.-1+id)f', '1235.000000')\n        test('%(formats.0.id.-1+formats.1.id.-1)d', '3')\n        out = json.dumps([{'id': f['id'], 'height.:2': str(f['height'])[:2]}\n                          if 'height' in f else {'id': f['id']}\n                          for f in FORMATS])\n        test('%(formats.:.{id,height.:2})j', (out, None))\n        test('%(formats.:.{id,height}.id)l', ', '.join(f['id'] for f in FORMATS))\n        test('%(.{id,title})j', ('{\"id\": \"1234\"}', '{\uff02id\uff02\uff1a \uff021234\uff02}'))\n\n        # Alternates\n        test('%(title,id)s', '1234')\n        test('%(width-100,height+20|def)d', '1100')\n        test('%(width-100,height+width|def)s', 'def')\n        test('%(timestamp-x>%H\\\\,%M\\\\,%S,timestamp>%H\\\\,%M\\\\,%S)s', '12,00,00')\n\n        # Replacement\n        test('%(id&foo)s.bar', 'foo.bar')\n        test('%(title&foo)s.bar', 'NA.bar')\n        test('%(title&foo|baz)s.bar', 'baz.bar')\n        test('%(x,id&foo|baz)s.bar', 'foo.bar')\n        test('%(x,title&foo|baz)s.bar', 'baz.bar')\n        test('%(id&a\\nb|)s', ('a\\nb', 'a b'))\n        test('%(id&hi {:>10} {}|)s', 'hi       1234 1234')\n        test(R'%(id&{0} {}|)s', 'NA')\n        test(R'%(id&{0.1}|)s', 'NA')\n        test('%(height&{:,d})S', '1,080')\n\n        # Laziness\n        def gen():\n            yield from range(5)\n            raise self.assertTrue(False, 'LazyList should not be evaluated till here')\n        test('%(key.4)s', '4', info={'key': LazyList(gen())})\n\n        # Empty filename\n        test('%(foo|)s-%(bar|)s.%(ext)s', '-.mp4')\n        # test('%(foo|)s.%(ext)s', ('.mp4', '_.mp4'))  # fixme\n        # test('%(foo|)s', ('', '_'))  # fixme\n\n        # Environment variable expansion for prepare_filename\n        os.environ['__yt_dlp_var'] = 'expanded'\n        envvar = '%__yt_dlp_var%' if compat_os_name == 'nt' else '$__yt_dlp_var'\n        test(envvar, (envvar, 'expanded'))\n        if compat_os_name == 'nt':\n            test('%s%', ('%s%', '%s%'))\n            os.environ['s'] = 'expanded'\n            test('%s%', ('%s%', 'expanded'))  # %s% should be expanded before escaping %s\n            os.environ['(test)s'] = 'expanded'\n            test('%(test)s%', ('NA%', 'expanded'))  # Environment should take priority over template\n\n        # Path expansion and escaping\n        test('Hello %(title1)s', 'Hello $PATH')\n        test('Hello %(title2)s', 'Hello %PATH%')\n        test('%(title3)s', ('foo/bar\\\\test', 'foo\u29f8bar\u29f9test'))\n        test('folder/%(title3)s', ('folder/foo/bar\\\\test', 'folder%sfoo\u29f8bar\u29f9test' % os.path.sep))\n\n    def test_format_note(self):\n        ydl = YoutubeDL()\n        self.assertEqual(ydl._format_note({}), '')\n        assertRegexpMatches(self, ydl._format_note({\n            'vbr': 10,\n        }), r'^\\s*10k$')\n        assertRegexpMatches(self, ydl._format_note({\n            'fps': 30,\n        }), r'^30fps$')\n\n    def test_postprocessors(self):\n        filename = 'post-processor-testfile.mp4'\n        audiofile = filename + '.mp3'\n\n        class SimplePP(PostProcessor):\n            def run(self, info):\n                with open(audiofile, 'w') as f:\n                    f.write('EXAMPLE')\n                return [info['filepath']], info\n\n        def run_pp(params, PP):\n            with open(filename, 'w') as f:\n                f.write('EXAMPLE')\n            ydl = YoutubeDL(params)\n            ydl.add_post_processor(PP())\n            ydl.post_process(filename, {'filepath': filename})\n\n        run_pp({'keepvideo': True}, SimplePP)\n        self.assertTrue(os.path.exists(filename), '%s doesn\\'t exist' % filename)\n        self.assertTrue(os.path.exists(audiofile), '%s doesn\\'t exist' % audiofile)\n        os.unlink(filename)\n        os.unlink(audiofile)\n\n        run_pp({'keepvideo': False}, SimplePP)\n        self.assertFalse(os.path.exists(filename), '%s exists' % filename)\n        self.assertTrue(os.path.exists(audiofile), '%s doesn\\'t exist' % audiofile)\n        os.unlink(audiofile)\n\n        class ModifierPP(PostProcessor):\n            def run(self, info):\n                with open(info['filepath'], 'w') as f:\n                    f.write('MODIFIED')\n                return [], info\n\n        run_pp({'keepvideo': False}, ModifierPP)\n        self.assertTrue(os.path.exists(filename), '%s doesn\\'t exist' % filename)\n        os.unlink(filename)\n\n    def test_match_filter(self):\n        first = {\n            'id': '1',\n            'url': TEST_URL,\n            'title': 'one',\n            'extractor': 'TEST',\n            'duration': 30,\n            'filesize': 10 * 1024,\n            'playlist_id': '42',\n            'uploader': \"\u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u0442\u0435\u0441\u0442\",\n            'creator': \"\u0442\u0435\u0441\u0442 ' 123 ' \u0442\u0435\u0441\u0442--\",\n            'webpage_url': 'http://example.com/watch?v=shenanigans',\n        }\n        second = {\n            'id': '2',\n            'url': TEST_URL,\n            'title': 'two',\n            'extractor': 'TEST',\n            'duration': 10,\n            'description': 'foo',\n            'filesize': 5 * 1024,\n            'playlist_id': '43',\n            'uploader': \"\u0442\u0435\u0441\u0442 123\",\n            'webpage_url': 'http://example.com/watch?v=SHENANIGANS',\n        }\n        videos = [first, second]\n\n        def get_videos(filter_=None):\n            ydl = YDL({'match_filter': filter_, 'simulate': True})\n            for v in videos:\n                ydl.process_ie_result(v, download=True)\n            return [v['id'] for v in ydl.downloaded_info_dicts]\n\n        res = get_videos()\n        self.assertEqual(res, ['1', '2'])\n\n        def f(v, incomplete):\n            if v['id'] == '1':\n                return None\n            else:\n                return 'Video id is not 1'\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func('duration < 30')\n        res = get_videos(f)\n        self.assertEqual(res, ['2'])\n\n        f = match_filter_func('description = foo')\n        res = get_videos(f)\n        self.assertEqual(res, ['2'])\n\n        f = match_filter_func('description =? foo')\n        res = get_videos(f)\n        self.assertEqual(res, ['1', '2'])\n\n        f = match_filter_func('filesize > 5KiB')\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func('playlist_id = 42')\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func('uploader = \"\u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u0442\u0435\u0441\u0442\"')\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func('uploader != \"\u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u0442\u0435\u0441\u0442\"')\n        res = get_videos(f)\n        self.assertEqual(res, ['2'])\n\n        f = match_filter_func('creator = \"\u0442\u0435\u0441\u0442 \\' 123 \\' \u0442\u0435\u0441\u0442--\"')\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func(\"creator = '\u0442\u0435\u0441\u0442 \\\\' 123 \\\\' \u0442\u0435\u0441\u0442--'\")\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func(r\"creator = '\u0442\u0435\u0441\u0442 \\' 123 \\' \u0442\u0435\u0441\u0442--' & duration > 30\")\n        res = get_videos(f)\n        self.assertEqual(res, [])\n\n    def test_playlist_items_selection(self):\n        INDICES, PAGE_SIZE = list(range(1, 11)), 3\n\n        def entry(i, evaluated):\n            evaluated.append(i)\n            return {\n                'id': str(i),\n                'title': str(i),\n                'url': TEST_URL,\n            }\n\n        def pagedlist_entries(evaluated):\n            def page_func(n):\n                start = PAGE_SIZE * n\n                for i in INDICES[start: start + PAGE_SIZE]:\n                    yield entry(i, evaluated)\n            return OnDemandPagedList(page_func, PAGE_SIZE)\n\n        def page_num(i):\n            return (i + PAGE_SIZE - 1) // PAGE_SIZE\n\n        def generator_entries(evaluated):\n            for i in INDICES:\n                yield entry(i, evaluated)\n\n        def list_entries(evaluated):\n            return list(generator_entries(evaluated))\n\n        def lazylist_entries(evaluated):\n            return LazyList(generator_entries(evaluated))\n\n        def get_downloaded_info_dicts(params, entries):\n            ydl = YDL(params)\n            ydl.process_ie_result({\n                '_type': 'playlist',\n                'id': 'test',\n                'extractor': 'test:playlist',\n                'extractor_key': 'test:playlist',\n                'webpage_url': 'http://example.com',\n                'entries': entries,\n            })\n            return ydl.downloaded_info_dicts\n\n        def test_selection(params, expected_ids, evaluate_all=False):\n            expected_ids = list(expected_ids)\n            if evaluate_all:\n                generator_eval = pagedlist_eval = INDICES\n            elif not expected_ids:\n                generator_eval = pagedlist_eval = []\n            else:\n                generator_eval = INDICES[0: max(expected_ids)]\n                pagedlist_eval = INDICES[PAGE_SIZE * page_num(min(expected_ids)) - PAGE_SIZE:\n                                         PAGE_SIZE * page_num(max(expected_ids))]\n\n            for name, func, expected_eval in (\n                ('list', list_entries, INDICES),\n                ('Generator', generator_entries, generator_eval),\n                # ('LazyList', lazylist_entries, generator_eval),  # Generator and LazyList follow the exact same code path\n                ('PagedList', pagedlist_entries, pagedlist_eval),\n            ):\n                evaluated = []\n                entries = func(evaluated)\n                results = [(v['playlist_autonumber'] - 1, (int(v['id']), v['playlist_index']))\n                           for v in get_downloaded_info_dicts(params, entries)]\n                self.assertEqual(results, list(enumerate(zip(expected_ids, expected_ids))), f'Entries of {name} for {params}')\n                self.assertEqual(sorted(evaluated), expected_eval, f'Evaluation of {name} for {params}')\n\n        test_selection({}, INDICES)\n        test_selection({'playlistend': 20}, INDICES, True)\n        test_selection({'playlistend': 2}, INDICES[:2])\n        test_selection({'playliststart': 11}, [], True)\n        test_selection({'playliststart': 2}, INDICES[1:])\n        test_selection({'playlist_items': '2-4'}, INDICES[1:4])\n        test_selection({'playlist_items': '2,4'}, [2, 4])\n        test_selection({'playlist_items': '20'}, [], True)\n        test_selection({'playlist_items': '0'}, [])\n\n        # Tests for https://github.com/ytdl-org/youtube-dl/issues/10591\n        test_selection({'playlist_items': '2-4,3-4,3'}, [2, 3, 4])\n        test_selection({'playlist_items': '4,2'}, [4, 2])\n\n        # Tests for https://github.com/yt-dlp/yt-dlp/issues/720\n        # https://github.com/yt-dlp/yt-dlp/issues/302\n        test_selection({'playlistreverse': True}, INDICES[::-1])\n        test_selection({'playliststart': 2, 'playlistreverse': True}, INDICES[:0:-1])\n        test_selection({'playlist_items': '2,4', 'playlistreverse': True}, [4, 2])\n        test_selection({'playlist_items': '4,2'}, [4, 2])\n\n        # Tests for --playlist-items start:end:step\n        test_selection({'playlist_items': ':'}, INDICES, True)\n        test_selection({'playlist_items': '::1'}, INDICES, True)\n        test_selection({'playlist_items': '::-1'}, INDICES[::-1], True)\n        test_selection({'playlist_items': ':6'}, INDICES[:6])\n        test_selection({'playlist_items': ':-6'}, INDICES[:-5], True)\n        test_selection({'playlist_items': '-1:6:-2'}, INDICES[:4:-2], True)\n        test_selection({'playlist_items': '9:-6:-2'}, INDICES[8:3:-2], True)\n\n        test_selection({'playlist_items': '1:inf:2'}, INDICES[::2], True)\n        test_selection({'playlist_items': '-2:inf'}, INDICES[-2:], True)\n        test_selection({'playlist_items': ':inf:-1'}, [], True)\n        test_selection({'playlist_items': '0-2:2'}, [2])\n        test_selection({'playlist_items': '1-:2'}, INDICES[::2], True)\n        test_selection({'playlist_items': '0--2:2'}, INDICES[1:-1:2], True)\n\n        test_selection({'playlist_items': '10::3'}, [10], True)\n        test_selection({'playlist_items': '-1::3'}, [10], True)\n        test_selection({'playlist_items': '11::3'}, [], True)\n        test_selection({'playlist_items': '-15::2'}, INDICES[1::2], True)\n        test_selection({'playlist_items': '-15::15'}, [], True)\n\n    def test_do_not_override_ie_key_in_url_transparent(self):\n        ydl = YDL()\n\n        class Foo1IE(InfoExtractor):\n            _VALID_URL = r'foo1:'\n\n            def _real_extract(self, url):\n                return {\n                    '_type': 'url_transparent',\n                    'url': 'foo2:',\n                    'ie_key': 'Foo2',\n                    'title': 'foo1 title',\n                    'id': 'foo1_id',\n                }\n\n        class Foo2IE(InfoExtractor):\n            _VALID_URL = r'foo2:'\n\n            def _real_extract(self, url):\n                return {\n                    '_type': 'url',\n                    'url': 'foo3:',\n                    'ie_key': 'Foo3',\n                }\n\n        class Foo3IE(InfoExtractor):\n            _VALID_URL = r'foo3:'\n\n            def _real_extract(self, url):\n                return _make_result([{'url': TEST_URL}], title='foo3 title')\n\n        ydl.add_info_extractor(Foo1IE(ydl))\n        ydl.add_info_extractor(Foo2IE(ydl))\n        ydl.add_info_extractor(Foo3IE(ydl))\n        ydl.extract_info('foo1:')\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['url'], TEST_URL)\n        self.assertEqual(downloaded['title'], 'foo1 title')\n        self.assertEqual(downloaded['id'], 'testid')\n        self.assertEqual(downloaded['extractor'], 'testex')\n        self.assertEqual(downloaded['extractor_key'], 'TestEx')\n\n    # Test case for https://github.com/ytdl-org/youtube-dl/issues/27064\n    def test_ignoreerrors_for_playlist_with_url_transparent_iterable_entries(self):\n\n        class _YDL(YDL):\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n\n            def trouble(self, s, tb=None):\n                pass\n\n        ydl = _YDL({\n            'format': 'extra',\n            'ignoreerrors': True,\n        })\n\n        class VideoIE(InfoExtractor):\n            _VALID_URL = r'video:(?P<id>\\d+)'\n\n            def _real_extract(self, url):\n                video_id = self._match_id(url)\n                formats = [{\n                    'format_id': 'default',\n                    'url': 'url:',\n                }]\n                if video_id == '0':\n                    raise ExtractorError('foo')\n                if video_id == '2':\n                    formats.append({\n                        'format_id': 'extra',\n                        'url': TEST_URL,\n                    })\n                return {\n                    'id': video_id,\n                    'title': 'Video %s' % video_id,\n                    'formats': formats,\n                }\n\n        class PlaylistIE(InfoExtractor):\n            _VALID_URL = r'playlist:'\n\n            def _entries(self):\n                for n in range(3):\n                    video_id = str(n)\n                    yield {\n                        '_type': 'url_transparent',\n                        'ie_key': VideoIE.ie_key(),\n                        'id': video_id,\n                        'url': 'video:%s' % video_id,\n                        'title': 'Video Transparent %s' % video_id,\n                    }\n\n            def _real_extract(self, url):\n                return self.playlist_result(self._entries())\n\n        ydl.add_info_extractor(VideoIE(ydl))\n        ydl.add_info_extractor(PlaylistIE(ydl))\n        info = ydl.extract_info('playlist:')\n        entries = info['entries']\n        self.assertEqual(len(entries), 3)\n        self.assertTrue(entries[0] is None)\n        self.assertTrue(entries[1] is None)\n        self.assertEqual(len(ydl.downloaded_info_dicts), 1)\n        downloaded = ydl.downloaded_info_dicts[0]\n        entries[2].pop('requested_downloads', None)\n        self.assertEqual(entries[2], downloaded)\n        self.assertEqual(downloaded['url'], TEST_URL)\n        self.assertEqual(downloaded['title'], 'Video Transparent 2')\n        self.assertEqual(downloaded['id'], '2')\n        self.assertEqual(downloaded['extractor'], 'Video')\n        self.assertEqual(downloaded['extractor_key'], 'Video')\n\n    def test_header_cookies(self):\n        from http.cookiejar import Cookie\n\n        ydl = FakeYDL()\n        ydl.report_warning = lambda *_, **__: None\n\n        def cookie(name, value, version=None, domain='', path='', secure=False, expires=None):\n            return Cookie(\n                version or 0, name, value, None, False,\n                domain, bool(domain), bool(domain), path, bool(path),\n                secure, expires, False, None, None, rest={})\n\n        _test_url = 'https://yt.dlp/test'\n\n        def test(encoded_cookies, cookies, *, headers=False, round_trip=None, error_re=None):\n            def _test():\n                ydl.cookiejar.clear()\n                ydl._load_cookies(encoded_cookies, autoscope=headers)\n                if headers:\n                    ydl._apply_header_cookies(_test_url)\n                data = {'url': _test_url}\n                ydl._calc_headers(data)\n                self.assertCountEqual(\n                    map(vars, ydl.cookiejar), map(vars, cookies),\n                    'Extracted cookiejar.Cookie is not the same')\n                if not headers:\n                    self.assertEqual(\n                        data.get('cookies'), round_trip or encoded_cookies,\n                        'Cookie is not the same as round trip')\n                ydl.__dict__['_YoutubeDL__header_cookies'] = []\n\n            with self.subTest(msg=encoded_cookies):\n                if not error_re:\n                    _test()\n                    return\n                with self.assertRaisesRegex(Exception, error_re):\n                    _test()\n\n        test('test=value; Domain=.yt.dlp', [cookie('test', 'value', domain='.yt.dlp')])\n        test('test=value', [cookie('test', 'value')], error_re=r'Unscoped cookies are not allowed')\n        test('cookie1=value1; Domain=.yt.dlp; Path=/test; cookie2=value2; Domain=.yt.dlp; Path=/', [\n            cookie('cookie1', 'value1', domain='.yt.dlp', path='/test'),\n            cookie('cookie2', 'value2', domain='.yt.dlp', path='/')])\n        test('test=value; Domain=.yt.dlp; Path=/test; Secure; Expires=9999999999', [\n            cookie('test', 'value', domain='.yt.dlp', path='/test', secure=True, expires=9999999999)])\n        test('test=\"value; \"; path=/test; domain=.yt.dlp', [\n            cookie('test', 'value; ', domain='.yt.dlp', path='/test')],\n            round_trip='test=\"value\\\\073 \"; Domain=.yt.dlp; Path=/test')\n        test('name=; Domain=.yt.dlp', [cookie('name', '', domain='.yt.dlp')],\n             round_trip='name=\"\"; Domain=.yt.dlp')\n\n        test('test=value', [cookie('test', 'value', domain='.yt.dlp')], headers=True)\n        test('cookie1=value; Domain=.yt.dlp; cookie2=value', [], headers=True, error_re=r'Invalid syntax')\n        ydl.deprecated_feature = ydl.report_error\n        test('test=value', [], headers=True, error_re=r'Passing cookies as a header is a potential security risk')\n\n    def test_infojson_cookies(self):\n        TEST_FILE = 'test_infojson_cookies.info.json'\n        TEST_URL = 'https://example.com/example.mp4'\n        COOKIES = 'a=b; Domain=.example.com; c=d; Domain=.example.com'\n        COOKIE_HEADER = {'Cookie': 'a=b; c=d'}\n\n        ydl = FakeYDL()\n        ydl.process_info = lambda x: ydl._write_info_json('test', x, TEST_FILE)\n\n        def make_info(info_header_cookies=False, fmts_header_cookies=False, cookies_field=False):\n            fmt = {'url': TEST_URL}\n            if fmts_header_cookies:\n                fmt['http_headers'] = COOKIE_HEADER\n            if cookies_field:\n                fmt['cookies'] = COOKIES\n            return _make_result([fmt], http_headers=COOKIE_HEADER if info_header_cookies else None)\n\n        def test(initial_info, note):\n            result = {}\n            result['processed'] = ydl.process_ie_result(initial_info)\n            self.assertTrue(ydl.cookiejar.get_cookies_for_url(TEST_URL),\n                            msg=f'No cookies set in cookiejar after initial process when {note}')\n            ydl.cookiejar.clear()\n            with open(TEST_FILE) as infojson:\n                result['loaded'] = ydl.sanitize_info(json.load(infojson), True)\n            result['final'] = ydl.process_ie_result(result['loaded'].copy(), download=False)\n            self.assertTrue(ydl.cookiejar.get_cookies_for_url(TEST_URL),\n                            msg=f'No cookies set in cookiejar after final process when {note}')\n            ydl.cookiejar.clear()\n            for key in ('processed', 'loaded', 'final'):\n                info = result[key]\n                self.assertIsNone(\n                    traverse_obj(info, ((None, ('formats', 0)), 'http_headers', 'Cookie'), casesense=False, get_all=False),\n                    msg=f'Cookie header not removed in {key} result when {note}')\n                self.assertEqual(\n                    traverse_obj(info, ((None, ('formats', 0)), 'cookies'), get_all=False), COOKIES,\n                    msg=f'No cookies field found in {key} result when {note}')\n\n        test({'url': TEST_URL, 'http_headers': COOKIE_HEADER, 'id': '1', 'title': 'x'}, 'no formats field')\n        test(make_info(info_header_cookies=True), 'info_dict header cokies')\n        test(make_info(fmts_header_cookies=True), 'format header cookies')\n        test(make_info(info_header_cookies=True, fmts_header_cookies=True), 'info_dict and format header cookies')\n        test(make_info(info_header_cookies=True, fmts_header_cookies=True, cookies_field=True), 'all cookies fields')\n        test(make_info(cookies_field=True), 'cookies format field')\n        test({'url': TEST_URL, 'cookies': COOKIES, 'id': '1', 'title': 'x'}, 'info_dict cookies field only')\n\n        try_rm(TEST_FILE)\n\n    def test_add_headers_cookie(self):\n        def check_for_cookie_header(result):\n            return traverse_obj(result, ((None, ('formats', 0)), 'http_headers', 'Cookie'), casesense=False, get_all=False)\n\n        ydl = FakeYDL({'http_headers': {'Cookie': 'a=b'}})\n        ydl._apply_header_cookies(_make_result([])['webpage_url'])  # Scope to input webpage URL: .example.com\n\n        fmt = {'url': 'https://example.com/video.mp4'}\n        result = ydl.process_ie_result(_make_result([fmt]), download=False)\n        self.assertIsNone(check_for_cookie_header(result), msg='http_headers cookies in result info_dict')\n        self.assertEqual(result.get('cookies'), 'a=b; Domain=.example.com', msg='No cookies were set in cookies field')\n        self.assertIn('a=b', ydl.cookiejar.get_cookie_header(fmt['url']), msg='No cookies were set in cookiejar')\n\n        fmt = {'url': 'https://wrong.com/video.mp4'}\n        result = ydl.process_ie_result(_make_result([fmt]), download=False)\n        self.assertIsNone(check_for_cookie_header(result), msg='http_headers cookies for wrong domain')\n        self.assertFalse(result.get('cookies'), msg='Cookies set in cookies field for wrong domain')\n        self.assertFalse(ydl.cookiejar.get_cookie_header(fmt['url']), msg='Cookies set in cookiejar for wrong domain')\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "#!/usr/bin/env python3\n\n# Allow direct execution\nimport os\nimport re\nimport sys\nimport unittest\nimport warnings\n\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n\nimport contextlib\nimport io\nimport itertools\nimport json\nimport xml.etree.ElementTree\n\nfrom yt_dlp.compat import (\n    compat_etree_fromstring,\n    compat_HTMLParseError,\n    compat_os_name,\n)\nfrom yt_dlp.utils import (\n    Config,\n    DateRange,\n    ExtractorError,\n    InAdvancePagedList,\n    LazyList,\n    OnDemandPagedList,\n    age_restricted,\n    args_to_str,\n    base_url,\n    caesar,\n    clean_html,\n    clean_podcast_url,\n    cli_bool_option,\n    cli_option,\n    cli_valueless_option,\n    date_from_str,\n    datetime_from_str,\n    detect_exe_version,\n    determine_ext,\n    determine_file_encoding,\n    dfxp2srt,\n    dict_get,\n    encode_base_n,\n    encode_compat_str,\n    encodeFilename,\n    expand_path,\n    extract_attributes,\n    extract_basic_auth,\n    find_xpath_attr,\n    fix_xml_ampersands,\n    float_or_none,\n    format_bytes,\n    get_compatible_ext,\n    get_element_by_attribute,\n    get_element_by_class,\n    get_element_html_by_attribute,\n    get_element_html_by_class,\n    get_element_text_and_html_by_tag,\n    get_elements_by_attribute,\n    get_elements_by_class,\n    get_elements_html_by_attribute,\n    get_elements_html_by_class,\n    get_elements_text_and_html_by_attribute,\n    int_or_none,\n    intlist_to_bytes,\n    iri_to_uri,\n    is_html,\n    js_to_json,\n    limit_length,\n    locked_file,\n    lowercase_escape,\n    match_str,\n    merge_dicts,\n    mimetype2ext,\n    month_by_name,\n    multipart_encode,\n    ohdave_rsa_encrypt,\n    orderedSet,\n    parse_age_limit,\n    parse_bitrate,\n    parse_codecs,\n    parse_count,\n    parse_dfxp_time_expr,\n    parse_duration,\n    parse_filesize,\n    parse_iso8601,\n    parse_qs,\n    parse_resolution,\n    pkcs1pad,\n    prepend_extension,\n    read_batch_urls,\n    remove_end,\n    remove_quotes,\n    remove_start,\n    render_table,\n    replace_extension,\n    rot47,\n    sanitize_filename,\n    sanitize_path,\n    sanitize_url,\n    shell_quote,\n    smuggle_url,\n    str_or_none,\n    str_to_int,\n    strip_jsonp,\n    strip_or_none,\n    subtitles_filename,\n    timeconvert,\n    traverse_obj,\n    try_call,\n    unescapeHTML,\n    unified_strdate,\n    unified_timestamp,\n    unsmuggle_url,\n    update_url_query,\n    uppercase_escape,\n    url_basename,\n    url_or_none,\n    urlencode_postdata,\n    urljoin,\n    urshift,\n    variadic,\n    version_tuple,\n    xpath_attr,\n    xpath_element,\n    xpath_text,\n    xpath_with_ns,\n)\nfrom yt_dlp.utils.networking import (\n    HTTPHeaderDict,\n    escape_rfc3986,\n    normalize_url,\n    remove_dot_segments,\n)\n\n\nclass TestUtil(unittest.TestCase):\n    def test_timeconvert(self):\n        self.assertTrue(timeconvert('') is None)\n        self.assertTrue(timeconvert('bougrg') is None)\n\n    def test_sanitize_filename(self):\n        self.assertEqual(sanitize_filename(''), '')\n        self.assertEqual(sanitize_filename('abc'), 'abc')\n        self.assertEqual(sanitize_filename('abc_d-e'), 'abc_d-e')\n\n        self.assertEqual(sanitize_filename('123'), '123')\n\n        self.assertEqual('abc\u29f8de', sanitize_filename('abc/de'))\n        self.assertFalse('/' in sanitize_filename('abc/de///'))\n\n        self.assertEqual('abc_de', sanitize_filename('abc/<>\\\\*|de', is_id=False))\n        self.assertEqual('xxx', sanitize_filename('xxx/<>\\\\*|', is_id=False))\n        self.assertEqual('yes no', sanitize_filename('yes? no', is_id=False))\n        self.assertEqual('this - that', sanitize_filename('this: that', is_id=False))\n\n        self.assertEqual(sanitize_filename('AT&T'), 'AT&T')\n        aumlaut = '\u00e4'\n        self.assertEqual(sanitize_filename(aumlaut), aumlaut)\n        tests = '\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0446\\u0430'\n        self.assertEqual(sanitize_filename(tests), tests)\n\n        self.assertEqual(\n            sanitize_filename('New World record at 0:12:34'),\n            'New World record at 0_12_34')\n\n        self.assertEqual(sanitize_filename('--gasdgf'), '--gasdgf')\n        self.assertEqual(sanitize_filename('--gasdgf', is_id=True), '--gasdgf')\n        self.assertEqual(sanitize_filename('--gasdgf', is_id=False), '_-gasdgf')\n        self.assertEqual(sanitize_filename('.gasdgf'), '.gasdgf')\n        self.assertEqual(sanitize_filename('.gasdgf', is_id=True), '.gasdgf')\n        self.assertEqual(sanitize_filename('.gasdgf', is_id=False), 'gasdgf')\n\n        forbidden = '\"\\0\\\\/'\n        for fc in forbidden:\n            for fbc in forbidden:\n                self.assertTrue(fbc not in sanitize_filename(fc))\n\n    def test_sanitize_filename_restricted(self):\n        self.assertEqual(sanitize_filename('abc', restricted=True), 'abc')\n        self.assertEqual(sanitize_filename('abc_d-e', restricted=True), 'abc_d-e')\n\n        self.assertEqual(sanitize_filename('123', restricted=True), '123')\n\n        self.assertEqual('abc_de', sanitize_filename('abc/de', restricted=True))\n        self.assertFalse('/' in sanitize_filename('abc/de///', restricted=True))\n\n        self.assertEqual('abc_de', sanitize_filename('abc/<>\\\\*|de', restricted=True))\n        self.assertEqual('xxx', sanitize_filename('xxx/<>\\\\*|', restricted=True))\n        self.assertEqual('yes_no', sanitize_filename('yes? no', restricted=True))\n        self.assertEqual('this_-_that', sanitize_filename('this: that', restricted=True))\n\n        tests = 'a\u00e4b\\u4e2d\\u56fd\\u7684c'\n        self.assertEqual(sanitize_filename(tests, restricted=True), 'aab_c')\n        self.assertTrue(sanitize_filename('\\xf6', restricted=True) != '')  # No empty filename\n\n        forbidden = '\"\\0\\\\/&!: \\'\\t\\n()[]{}$;`^,#'\n        for fc in forbidden:\n            for fbc in forbidden:\n                self.assertTrue(fbc not in sanitize_filename(fc, restricted=True))\n\n        # Handle a common case more neatly\n        self.assertEqual(sanitize_filename('\\u5927\\u58f0\\u5e26 - Song', restricted=True), 'Song')\n        self.assertEqual(sanitize_filename('\\u603b\\u7edf: Speech', restricted=True), 'Speech')\n        # .. but make sure the file name is never empty\n        self.assertTrue(sanitize_filename('-', restricted=True) != '')\n        self.assertTrue(sanitize_filename(':', restricted=True) != '')\n\n        self.assertEqual(sanitize_filename(\n            '\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff', restricted=True),\n            'AAAAAAAECEEEEIIIIDNOOOOOOOOEUUUUUYTHssaaaaaaaeceeeeiiiionooooooooeuuuuuythy')\n\n    def test_sanitize_ids(self):\n        self.assertEqual(sanitize_filename('_n_cd26wFpw', is_id=True), '_n_cd26wFpw')\n        self.assertEqual(sanitize_filename('_BD_eEpuzXw', is_id=True), '_BD_eEpuzXw')\n        self.assertEqual(sanitize_filename('N0Y__7-UOdI', is_id=True), 'N0Y__7-UOdI')\n\n    def test_sanitize_path(self):\n        if sys.platform != 'win32':\n            return\n\n        self.assertEqual(sanitize_path('abc'), 'abc')\n        self.assertEqual(sanitize_path('abc/def'), 'abc\\\\def')\n        self.assertEqual(sanitize_path('abc\\\\def'), 'abc\\\\def')\n        self.assertEqual(sanitize_path('abc|def'), 'abc#def')\n        self.assertEqual(sanitize_path('<>:\"|?*'), '#######')\n        self.assertEqual(sanitize_path('C:/abc/def'), 'C:\\\\abc\\\\def')\n        self.assertEqual(sanitize_path('C?:/abc/def'), 'C##\\\\abc\\\\def')\n\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\UNC\\\\ComputerName\\\\abc'), '\\\\\\\\?\\\\UNC\\\\ComputerName\\\\abc')\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\UNC/ComputerName/abc'), '\\\\\\\\?\\\\UNC\\\\ComputerName\\\\abc')\n\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\C:\\\\abc'), '\\\\\\\\?\\\\C:\\\\abc')\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\C:/abc'), '\\\\\\\\?\\\\C:\\\\abc')\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\C:\\\\ab?c\\\\de:f'), '\\\\\\\\?\\\\C:\\\\ab#c\\\\de#f')\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\C:\\\\abc'), '\\\\\\\\?\\\\C:\\\\abc')\n\n        self.assertEqual(\n            sanitize_path('youtube/%(uploader)s/%(autonumber)s-%(title)s-%(upload_date)s.%(ext)s'),\n            'youtube\\\\%(uploader)s\\\\%(autonumber)s-%(title)s-%(upload_date)s.%(ext)s')\n\n        self.assertEqual(\n            sanitize_path('youtube/TheWreckingYard ./00001-Not bad, Especially for Free! (1987 Yamaha 700)-20141116.mp4.part'),\n            'youtube\\\\TheWreckingYard #\\\\00001-Not bad, Especially for Free! (1987 Yamaha 700)-20141116.mp4.part')\n        self.assertEqual(sanitize_path('abc/def...'), 'abc\\\\def..#')\n        self.assertEqual(sanitize_path('abc.../def'), 'abc..#\\\\def')\n        self.assertEqual(sanitize_path('abc.../def...'), 'abc..#\\\\def..#')\n\n        self.assertEqual(sanitize_path('../abc'), '..\\\\abc')\n        self.assertEqual(sanitize_path('../../abc'), '..\\\\..\\\\abc')\n        self.assertEqual(sanitize_path('./abc'), 'abc')\n        self.assertEqual(sanitize_path('./../abc'), '..\\\\abc')\n\n    def test_sanitize_url(self):\n        self.assertEqual(sanitize_url('//foo.bar'), 'http://foo.bar')\n        self.assertEqual(sanitize_url('httpss://foo.bar'), 'https://foo.bar')\n        self.assertEqual(sanitize_url('rmtps://foo.bar'), 'rtmps://foo.bar')\n        self.assertEqual(sanitize_url('https://foo.bar'), 'https://foo.bar')\n        self.assertEqual(sanitize_url('foo bar'), 'foo bar')\n\n    def test_expand_path(self):\n        def env(var):\n            return f'%{var}%' if sys.platform == 'win32' else f'${var}'\n\n        os.environ['yt_dlp_EXPATH_PATH'] = 'expanded'\n        self.assertEqual(expand_path(env('yt_dlp_EXPATH_PATH')), 'expanded')\n\n        old_home = os.environ.get('HOME')\n        test_str = R'C:\\Documents and Settings\\\u0442\u0435\u0441\u0442\\Application Data'\n        try:\n            os.environ['HOME'] = test_str\n            self.assertEqual(expand_path(env('HOME')), os.getenv('HOME'))\n            self.assertEqual(expand_path('~'), os.getenv('HOME'))\n            self.assertEqual(\n                expand_path('~/%s' % env('yt_dlp_EXPATH_PATH')),\n                '%s/expanded' % os.getenv('HOME'))\n        finally:\n            os.environ['HOME'] = old_home or ''\n\n    def test_prepend_extension(self):\n        self.assertEqual(prepend_extension('abc.ext', 'temp'), 'abc.temp.ext')\n        self.assertEqual(prepend_extension('abc.ext', 'temp', 'ext'), 'abc.temp.ext')\n        self.assertEqual(prepend_extension('abc.unexpected_ext', 'temp', 'ext'), 'abc.unexpected_ext.temp')\n        self.assertEqual(prepend_extension('abc', 'temp'), 'abc.temp')\n        self.assertEqual(prepend_extension('.abc', 'temp'), '.abc.temp')\n        self.assertEqual(prepend_extension('.abc.ext', 'temp'), '.abc.temp.ext')\n\n    def test_replace_extension(self):\n        self.assertEqual(replace_extension('abc.ext', 'temp'), 'abc.temp')\n        self.assertEqual(replace_extension('abc.ext', 'temp', 'ext'), 'abc.temp')\n        self.assertEqual(replace_extension('abc.unexpected_ext', 'temp', 'ext'), 'abc.unexpected_ext.temp')\n        self.assertEqual(replace_extension('abc', 'temp'), 'abc.temp')\n        self.assertEqual(replace_extension('.abc', 'temp'), '.abc.temp')\n        self.assertEqual(replace_extension('.abc.ext', 'temp'), '.abc.temp')\n\n    def test_subtitles_filename(self):\n        self.assertEqual(subtitles_filename('abc.ext', 'en', 'vtt'), 'abc.en.vtt')\n        self.assertEqual(subtitles_filename('abc.ext', 'en', 'vtt', 'ext'), 'abc.en.vtt')\n        self.assertEqual(subtitles_filename('abc.unexpected_ext', 'en', 'vtt', 'ext'), 'abc.unexpected_ext.en.vtt')\n\n    def test_remove_start(self):\n        self.assertEqual(remove_start(None, 'A - '), None)\n        self.assertEqual(remove_start('A - B', 'A - '), 'B')\n        self.assertEqual(remove_start('B - A', 'A - '), 'B - A')\n\n    def test_remove_end(self):\n        self.assertEqual(remove_end(None, ' - B'), None)\n        self.assertEqual(remove_end('A - B', ' - B'), 'A')\n        self.assertEqual(remove_end('B - A', ' - B'), 'B - A')\n\n    def test_remove_quotes(self):\n        self.assertEqual(remove_quotes(None), None)\n        self.assertEqual(remove_quotes('\"'), '\"')\n        self.assertEqual(remove_quotes(\"'\"), \"'\")\n        self.assertEqual(remove_quotes(';'), ';')\n        self.assertEqual(remove_quotes('\";'), '\";')\n        self.assertEqual(remove_quotes('\"\"'), '')\n        self.assertEqual(remove_quotes('\";\"'), ';')\n\n    def test_ordered_set(self):\n        self.assertEqual(orderedSet([1, 1, 2, 3, 4, 4, 5, 6, 7, 3, 5]), [1, 2, 3, 4, 5, 6, 7])\n        self.assertEqual(orderedSet([]), [])\n        self.assertEqual(orderedSet([1]), [1])\n        # keep the list ordered\n        self.assertEqual(orderedSet([135, 1, 1, 1]), [135, 1])\n\n    def test_unescape_html(self):\n        self.assertEqual(unescapeHTML('%20;'), '%20;')\n        self.assertEqual(unescapeHTML('&#x2F;'), '/')\n        self.assertEqual(unescapeHTML('&#47;'), '/')\n        self.assertEqual(unescapeHTML('&eacute;'), '\u00e9')\n        self.assertEqual(unescapeHTML('&#2013266066;'), '&#2013266066;')\n        self.assertEqual(unescapeHTML('&a&quot;'), '&a\"')\n        # HTML5 entities\n        self.assertEqual(unescapeHTML('&period;&apos;'), '.\\'')\n\n    def test_date_from_str(self):\n        self.assertEqual(date_from_str('yesterday'), date_from_str('now-1day'))\n        self.assertEqual(date_from_str('now+7day'), date_from_str('now+1week'))\n        self.assertEqual(date_from_str('now+14day'), date_from_str('now+2week'))\n        self.assertEqual(date_from_str('20200229+365day'), date_from_str('20200229+1year'))\n        self.assertEqual(date_from_str('20210131+28day'), date_from_str('20210131+1month'))\n\n    def test_datetime_from_str(self):\n        self.assertEqual(datetime_from_str('yesterday', precision='day'), datetime_from_str('now-1day', precision='auto'))\n        self.assertEqual(datetime_from_str('now+7day', precision='day'), datetime_from_str('now+1week', precision='auto'))\n        self.assertEqual(datetime_from_str('now+14day', precision='day'), datetime_from_str('now+2week', precision='auto'))\n        self.assertEqual(datetime_from_str('20200229+365day', precision='day'), datetime_from_str('20200229+1year', precision='auto'))\n        self.assertEqual(datetime_from_str('20210131+28day', precision='day'), datetime_from_str('20210131+1month', precision='auto'))\n        self.assertEqual(datetime_from_str('20210131+59day', precision='day'), datetime_from_str('20210131+2month', precision='auto'))\n        self.assertEqual(datetime_from_str('now+1day', precision='hour'), datetime_from_str('now+24hours', precision='auto'))\n        self.assertEqual(datetime_from_str('now+23hours', precision='hour'), datetime_from_str('now+23hours', precision='auto'))\n\n    def test_daterange(self):\n        _20century = DateRange(\"19000101\", \"20000101\")\n        self.assertFalse(\"17890714\" in _20century)\n        _ac = DateRange(\"00010101\")\n        self.assertTrue(\"19690721\" in _ac)\n        _firstmilenium = DateRange(end=\"10000101\")\n        self.assertTrue(\"07110427\" in _firstmilenium)\n\n    def test_unified_dates(self):\n        self.assertEqual(unified_strdate('December 21, 2010'), '20101221')\n        self.assertEqual(unified_strdate('8/7/2009'), '20090708')\n        self.assertEqual(unified_strdate('Dec 14, 2012'), '20121214')\n        self.assertEqual(unified_strdate('2012/10/11 01:56:38 +0000'), '20121011')\n        self.assertEqual(unified_strdate('1968 12 10'), '19681210')\n        self.assertEqual(unified_strdate('1968-12-10'), '19681210')\n        self.assertEqual(unified_strdate('31-07-2022 20:00'), '20220731')\n        self.assertEqual(unified_strdate('28/01/2014 21:00:00 +0100'), '20140128')\n        self.assertEqual(\n            unified_strdate('11/26/2014 11:30:00 AM PST', day_first=False),\n            '20141126')\n        self.assertEqual(\n            unified_strdate('2/2/2015 6:47:40 PM', day_first=False),\n            '20150202')\n        self.assertEqual(unified_strdate('Feb 14th 2016 5:45PM'), '20160214')\n        self.assertEqual(unified_strdate('25-09-2014'), '20140925')\n        self.assertEqual(unified_strdate('27.02.2016 17:30'), '20160227')\n        self.assertEqual(unified_strdate('UNKNOWN DATE FORMAT'), None)\n        self.assertEqual(unified_strdate('Feb 7, 2016 at 6:35 pm'), '20160207')\n        self.assertEqual(unified_strdate('July 15th, 2013'), '20130715')\n        self.assertEqual(unified_strdate('September 1st, 2013'), '20130901')\n        self.assertEqual(unified_strdate('Sep 2nd, 2013'), '20130902')\n        self.assertEqual(unified_strdate('November 3rd, 2019'), '20191103')\n        self.assertEqual(unified_strdate('October 23rd, 2005'), '20051023')\n\n    def test_unified_timestamps(self):\n        self.assertEqual(unified_timestamp('December 21, 2010'), 1292889600)\n        self.assertEqual(unified_timestamp('8/7/2009'), 1247011200)\n        self.assertEqual(unified_timestamp('Dec 14, 2012'), 1355443200)\n        self.assertEqual(unified_timestamp('2012/10/11 01:56:38 +0000'), 1349920598)\n        self.assertEqual(unified_timestamp('1968 12 10'), -33436800)\n        self.assertEqual(unified_timestamp('1968-12-10'), -33436800)\n        self.assertEqual(unified_timestamp('28/01/2014 21:00:00 +0100'), 1390939200)\n        self.assertEqual(\n            unified_timestamp('11/26/2014 11:30:00 AM PST', day_first=False),\n            1417001400)\n        self.assertEqual(\n            unified_timestamp('2/2/2015 6:47:40 PM', day_first=False),\n            1422902860)\n        self.assertEqual(unified_timestamp('Feb 14th 2016 5:45PM'), 1455471900)\n        self.assertEqual(unified_timestamp('25-09-2014'), 1411603200)\n        self.assertEqual(unified_timestamp('27.02.2016 17:30'), 1456594200)\n        self.assertEqual(unified_timestamp('UNKNOWN DATE FORMAT'), None)\n        self.assertEqual(unified_timestamp('May 16, 2016 11:15 PM'), 1463440500)\n        self.assertEqual(unified_timestamp('Feb 7, 2016 at 6:35 pm'), 1454870100)\n        self.assertEqual(unified_timestamp('2017-03-30T17:52:41Q'), 1490896361)\n        self.assertEqual(unified_timestamp('Sep 11, 2013 | 5:49 AM'), 1378878540)\n        self.assertEqual(unified_timestamp('December 15, 2017 at 7:49 am'), 1513324140)\n        self.assertEqual(unified_timestamp('2018-03-14T08:32:43.1493874+00:00'), 1521016363)\n\n        self.assertEqual(unified_timestamp('December 31 1969 20:00:01 EDT'), 1)\n        self.assertEqual(unified_timestamp('Wednesday 31 December 1969 18:01:26 MDT'), 86)\n        self.assertEqual(unified_timestamp('12/31/1969 20:01:18 EDT', False), 78)\n\n    def test_determine_ext(self):\n        self.assertEqual(determine_ext('http://example.com/foo/bar.mp4/?download'), 'mp4')\n        self.assertEqual(determine_ext('http://example.com/foo/bar/?download', None), None)\n        self.assertEqual(determine_ext('http://example.com/foo/bar.nonext/?download', None), None)\n        self.assertEqual(determine_ext('http://example.com/foo/bar/mp4?download', None), None)\n        self.assertEqual(determine_ext('http://example.com/foo/bar.m3u8//?download'), 'm3u8')\n        self.assertEqual(determine_ext('foobar', None), None)\n\n    def test_find_xpath_attr(self):\n        testxml = '''<root>\n            <node/>\n            <node x=\"a\"/>\n            <node x=\"a\" y=\"c\" />\n            <node x=\"b\" y=\"d\" />\n            <node x=\"\" />\n        </root>'''\n        doc = compat_etree_fromstring(testxml)\n\n        self.assertEqual(find_xpath_attr(doc, './/fourohfour', 'n'), None)\n        self.assertEqual(find_xpath_attr(doc, './/fourohfour', 'n', 'v'), None)\n        self.assertEqual(find_xpath_attr(doc, './/node', 'n'), None)\n        self.assertEqual(find_xpath_attr(doc, './/node', 'n', 'v'), None)\n        self.assertEqual(find_xpath_attr(doc, './/node', 'x'), doc[1])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'x', 'a'), doc[1])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'x', 'b'), doc[3])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'y'), doc[2])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'y', 'c'), doc[2])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'y', 'd'), doc[3])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'x', ''), doc[4])\n\n    def test_xpath_with_ns(self):\n        testxml = '''<root xmlns:media=\"http://example.com/\">\n            <media:song>\n                <media:author>The Author</media:author>\n                <url>http://server.com/download.mp3</url>\n            </media:song>\n        </root>'''\n        doc = compat_etree_fromstring(testxml)\n        find = lambda p: doc.find(xpath_with_ns(p, {'media': 'http://example.com/'}))\n        self.assertTrue(find('media:song') is not None)\n        self.assertEqual(find('media:song/media:author').text, 'The Author')\n        self.assertEqual(find('media:song/url').text, 'http://server.com/download.mp3')\n\n    def test_xpath_element(self):\n        doc = xml.etree.ElementTree.Element('root')\n        div = xml.etree.ElementTree.SubElement(doc, 'div')\n        p = xml.etree.ElementTree.SubElement(div, 'p')\n        p.text = 'Foo'\n        self.assertEqual(xpath_element(doc, 'div/p'), p)\n        self.assertEqual(xpath_element(doc, ['div/p']), p)\n        self.assertEqual(xpath_element(doc, ['div/bar', 'div/p']), p)\n        self.assertEqual(xpath_element(doc, 'div/bar', default='default'), 'default')\n        self.assertEqual(xpath_element(doc, ['div/bar'], default='default'), 'default')\n        self.assertTrue(xpath_element(doc, 'div/bar') is None)\n        self.assertTrue(xpath_element(doc, ['div/bar']) is None)\n        self.assertTrue(xpath_element(doc, ['div/bar'], 'div/baz') is None)\n        self.assertRaises(ExtractorError, xpath_element, doc, 'div/bar', fatal=True)\n        self.assertRaises(ExtractorError, xpath_element, doc, ['div/bar'], fatal=True)\n        self.assertRaises(ExtractorError, xpath_element, doc, ['div/bar', 'div/baz'], fatal=True)\n\n    def test_xpath_text(self):\n        testxml = '''<root>\n            <div>\n                <p>Foo</p>\n            </div>\n        </root>'''\n        doc = compat_etree_fromstring(testxml)\n        self.assertEqual(xpath_text(doc, 'div/p'), 'Foo')\n        self.assertEqual(xpath_text(doc, 'div/bar', default='default'), 'default')\n        self.assertTrue(xpath_text(doc, 'div/bar') is None)\n        self.assertRaises(ExtractorError, xpath_text, doc, 'div/bar', fatal=True)\n\n    def test_xpath_attr(self):\n        testxml = '''<root>\n            <div>\n                <p x=\"a\">Foo</p>\n            </div>\n        </root>'''\n        doc = compat_etree_fromstring(testxml)\n        self.assertEqual(xpath_attr(doc, 'div/p', 'x'), 'a')\n        self.assertEqual(xpath_attr(doc, 'div/bar', 'x'), None)\n        self.assertEqual(xpath_attr(doc, 'div/p', 'y'), None)\n        self.assertEqual(xpath_attr(doc, 'div/bar', 'x', default='default'), 'default')\n        self.assertEqual(xpath_attr(doc, 'div/p', 'y', default='default'), 'default')\n        self.assertRaises(ExtractorError, xpath_attr, doc, 'div/bar', 'x', fatal=True)\n        self.assertRaises(ExtractorError, xpath_attr, doc, 'div/p', 'y', fatal=True)\n\n    def test_smuggle_url(self):\n        data = {\"\u00f6\": \"\u00f6\", \"abc\": [3]}\n        url = 'https://foo.bar/baz?x=y#a'\n        smug_url = smuggle_url(url, data)\n        unsmug_url, unsmug_data = unsmuggle_url(smug_url)\n        self.assertEqual(url, unsmug_url)\n        self.assertEqual(data, unsmug_data)\n\n        res_url, res_data = unsmuggle_url(url)\n        self.assertEqual(res_url, url)\n        self.assertEqual(res_data, None)\n\n        smug_url = smuggle_url(url, {'a': 'b'})\n        smug_smug_url = smuggle_url(smug_url, {'c': 'd'})\n        res_url, res_data = unsmuggle_url(smug_smug_url)\n        self.assertEqual(res_url, url)\n        self.assertEqual(res_data, {'a': 'b', 'c': 'd'})\n\n    def test_shell_quote(self):\n        args = ['ffmpeg', '-i', encodeFilename('\u00f1\u20ac\u00df\\'.mp4')]\n        self.assertEqual(\n            shell_quote(args),\n            \"\"\"ffmpeg -i '\u00f1\u20ac\u00df'\"'\"'.mp4'\"\"\" if compat_os_name != 'nt' else '''ffmpeg -i \"\u00f1\u20ac\u00df'.mp4\"''')\n\n    def test_float_or_none(self):\n        self.assertEqual(float_or_none('42.42'), 42.42)\n        self.assertEqual(float_or_none('42'), 42.0)\n        self.assertEqual(float_or_none(''), None)\n        self.assertEqual(float_or_none(None), None)\n        self.assertEqual(float_or_none([]), None)\n        self.assertEqual(float_or_none(set()), None)\n\n    def test_int_or_none(self):\n        self.assertEqual(int_or_none('42'), 42)\n        self.assertEqual(int_or_none(''), None)\n        self.assertEqual(int_or_none(None), None)\n        self.assertEqual(int_or_none([]), None)\n        self.assertEqual(int_or_none(set()), None)\n\n    def test_str_to_int(self):\n        self.assertEqual(str_to_int('123,456'), 123456)\n        self.assertEqual(str_to_int('123.456'), 123456)\n        self.assertEqual(str_to_int(523), 523)\n        self.assertEqual(str_to_int('noninteger'), None)\n        self.assertEqual(str_to_int([]), None)\n\n    def test_url_basename(self):\n        self.assertEqual(url_basename('http://foo.de/'), '')\n        self.assertEqual(url_basename('http://foo.de/bar/baz'), 'baz')\n        self.assertEqual(url_basename('http://foo.de/bar/baz?x=y'), 'baz')\n        self.assertEqual(url_basename('http://foo.de/bar/baz#x=y'), 'baz')\n        self.assertEqual(url_basename('http://foo.de/bar/baz/'), 'baz')\n        self.assertEqual(\n            url_basename('http://media.w3.org/2010/05/sintel/trailer.mp4'),\n            'trailer.mp4')\n\n    def test_base_url(self):\n        self.assertEqual(base_url('http://foo.de/'), 'http://foo.de/')\n        self.assertEqual(base_url('http://foo.de/bar'), 'http://foo.de/')\n        self.assertEqual(base_url('http://foo.de/bar/'), 'http://foo.de/bar/')\n        self.assertEqual(base_url('http://foo.de/bar/baz'), 'http://foo.de/bar/')\n        self.assertEqual(base_url('http://foo.de/bar/baz?x=z/x/c'), 'http://foo.de/bar/')\n        self.assertEqual(base_url('http://foo.de/bar/baz&x=z&w=y/x/c'), 'http://foo.de/bar/baz&x=z&w=y/x/')\n\n    def test_urljoin(self):\n        self.assertEqual(urljoin('http://foo.de/', '/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin(b'http://foo.de/', '/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de/', b'/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin(b'http://foo.de/', b'/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('//foo.de/', '/a/b/c.txt'), '//foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de/', 'a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de', '/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de', 'a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de/', 'http://foo.de/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de/', '//foo.de/a/b/c.txt'), '//foo.de/a/b/c.txt')\n        self.assertEqual(urljoin(None, 'http://foo.de/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin(None, '//foo.de/a/b/c.txt'), '//foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('', 'http://foo.de/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin(['foobar'], 'http://foo.de/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de/', None), None)\n        self.assertEqual(urljoin('http://foo.de/', ''), None)\n        self.assertEqual(urljoin('http://foo.de/', ['foobar']), None)\n        self.assertEqual(urljoin('http://foo.de/a/b/c.txt', '.././../d.txt'), 'http://foo.de/d.txt')\n        self.assertEqual(urljoin('http://foo.de/a/b/c.txt', 'rtmp://foo.de'), 'rtmp://foo.de')\n        self.assertEqual(urljoin(None, 'rtmp://foo.de'), 'rtmp://foo.de')\n\n    def test_url_or_none(self):\n        self.assertEqual(url_or_none(None), None)\n        self.assertEqual(url_or_none(''), None)\n        self.assertEqual(url_or_none('foo'), None)\n        self.assertEqual(url_or_none('http://foo.de'), 'http://foo.de')\n        self.assertEqual(url_or_none('https://foo.de'), 'https://foo.de')\n        self.assertEqual(url_or_none('http$://foo.de'), None)\n        self.assertEqual(url_or_none('http://foo.de'), 'http://foo.de')\n        self.assertEqual(url_or_none('//foo.de'), '//foo.de')\n        self.assertEqual(url_or_none('s3://foo.de'), None)\n        self.assertEqual(url_or_none('rtmpte://foo.de'), 'rtmpte://foo.de')\n        self.assertEqual(url_or_none('mms://foo.de'), 'mms://foo.de')\n        self.assertEqual(url_or_none('rtspu://foo.de'), 'rtspu://foo.de')\n        self.assertEqual(url_or_none('ftps://foo.de'), 'ftps://foo.de')\n\n    def test_parse_age_limit(self):\n        self.assertEqual(parse_age_limit(None), None)\n        self.assertEqual(parse_age_limit(False), None)\n        self.assertEqual(parse_age_limit('invalid'), None)\n        self.assertEqual(parse_age_limit(0), 0)\n        self.assertEqual(parse_age_limit(18), 18)\n        self.assertEqual(parse_age_limit(21), 21)\n        self.assertEqual(parse_age_limit(22), None)\n        self.assertEqual(parse_age_limit('18'), 18)\n        self.assertEqual(parse_age_limit('18+'), 18)\n        self.assertEqual(parse_age_limit('PG-13'), 13)\n        self.assertEqual(parse_age_limit('TV-14'), 14)\n        self.assertEqual(parse_age_limit('TV-MA'), 17)\n        self.assertEqual(parse_age_limit('TV14'), 14)\n        self.assertEqual(parse_age_limit('TV_G'), 0)\n\n    def test_parse_duration(self):\n        self.assertEqual(parse_duration(None), None)\n        self.assertEqual(parse_duration(False), None)\n        self.assertEqual(parse_duration('invalid'), None)\n        self.assertEqual(parse_duration('1'), 1)\n        self.assertEqual(parse_duration('1337:12'), 80232)\n        self.assertEqual(parse_duration('9:12:43'), 33163)\n        self.assertEqual(parse_duration('12:00'), 720)\n        self.assertEqual(parse_duration('00:01:01'), 61)\n        self.assertEqual(parse_duration('x:y'), None)\n        self.assertEqual(parse_duration('3h11m53s'), 11513)\n        self.assertEqual(parse_duration('3h 11m 53s'), 11513)\n        self.assertEqual(parse_duration('3 hours 11 minutes 53 seconds'), 11513)\n        self.assertEqual(parse_duration('3 hours 11 mins 53 secs'), 11513)\n        self.assertEqual(parse_duration('3 hours, 11 minutes, 53 seconds'), 11513)\n        self.assertEqual(parse_duration('3 hours, 11 mins, 53 secs'), 11513)\n        self.assertEqual(parse_duration('62m45s'), 3765)\n        self.assertEqual(parse_duration('6m59s'), 419)\n        self.assertEqual(parse_duration('49s'), 49)\n        self.assertEqual(parse_duration('0h0m0s'), 0)\n        self.assertEqual(parse_duration('0m0s'), 0)\n        self.assertEqual(parse_duration('0s'), 0)\n        self.assertEqual(parse_duration('01:02:03.05'), 3723.05)\n        self.assertEqual(parse_duration('T30M38S'), 1838)\n        self.assertEqual(parse_duration('5 s'), 5)\n        self.assertEqual(parse_duration('3 min'), 180)\n        self.assertEqual(parse_duration('2.5 hours'), 9000)\n        self.assertEqual(parse_duration('02:03:04'), 7384)\n        self.assertEqual(parse_duration('01:02:03:04'), 93784)\n        self.assertEqual(parse_duration('1 hour 3 minutes'), 3780)\n        self.assertEqual(parse_duration('87 Min.'), 5220)\n        self.assertEqual(parse_duration('PT1H0.040S'), 3600.04)\n        self.assertEqual(parse_duration('PT00H03M30SZ'), 210)\n        self.assertEqual(parse_duration('P0Y0M0DT0H4M20.880S'), 260.88)\n        self.assertEqual(parse_duration('01:02:03:050'), 3723.05)\n        self.assertEqual(parse_duration('103:050'), 103.05)\n        self.assertEqual(parse_duration('1HR 3MIN'), 3780)\n        self.assertEqual(parse_duration('2hrs 3mins'), 7380)\n\n    def test_fix_xml_ampersands(self):\n        self.assertEqual(\n            fix_xml_ampersands('\"&x=y&z=a'), '\"&amp;x=y&amp;z=a')\n        self.assertEqual(\n            fix_xml_ampersands('\"&amp;x=y&wrong;&z=a'),\n            '\"&amp;x=y&amp;wrong;&amp;z=a')\n        self.assertEqual(\n            fix_xml_ampersands('&amp;&apos;&gt;&lt;&quot;'),\n            '&amp;&apos;&gt;&lt;&quot;')\n        self.assertEqual(\n            fix_xml_ampersands('&#1234;&#x1abC;'), '&#1234;&#x1abC;')\n        self.assertEqual(fix_xml_ampersands('&#&#'), '&amp;#&amp;#')\n\n    def test_paged_list(self):\n        def testPL(size, pagesize, sliceargs, expected):\n            def get_page(pagenum):\n                firstid = pagenum * pagesize\n                upto = min(size, pagenum * pagesize + pagesize)\n                yield from range(firstid, upto)\n\n            pl = OnDemandPagedList(get_page, pagesize)\n            got = pl.getslice(*sliceargs)\n            self.assertEqual(got, expected)\n\n            iapl = InAdvancePagedList(get_page, size // pagesize + 1, pagesize)\n            got = iapl.getslice(*sliceargs)\n            self.assertEqual(got, expected)\n\n        testPL(5, 2, (), [0, 1, 2, 3, 4])\n        testPL(5, 2, (1,), [1, 2, 3, 4])\n        testPL(5, 2, (2,), [2, 3, 4])\n        testPL(5, 2, (4,), [4])\n        testPL(5, 2, (0, 3), [0, 1, 2])\n        testPL(5, 2, (1, 4), [1, 2, 3])\n        testPL(5, 2, (2, 99), [2, 3, 4])\n        testPL(5, 2, (20, 99), [])\n\n    def test_read_batch_urls(self):\n        f = io.StringIO('''\\xef\\xbb\\xbf foo\n            bar\\r\n            baz\n            # More after this line\\r\n            ; or after this\n            bam''')\n        self.assertEqual(read_batch_urls(f), ['foo', 'bar', 'baz', 'bam'])\n\n    def test_urlencode_postdata(self):\n        data = urlencode_postdata({'username': 'foo@bar.com', 'password': '1234'})\n        self.assertTrue(isinstance(data, bytes))\n\n    def test_update_url_query(self):\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'quality': ['HD'], 'format': ['mp4']})),\n            parse_qs('http://example.com/path?quality=HD&format=mp4'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'system': ['LINUX', 'WINDOWS']})),\n            parse_qs('http://example.com/path?system=LINUX&system=WINDOWS'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'fields': 'id,formats,subtitles'})),\n            parse_qs('http://example.com/path?fields=id,formats,subtitles'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'fields': ('id,formats,subtitles', 'thumbnails')})),\n            parse_qs('http://example.com/path?fields=id,formats,subtitles&fields=thumbnails'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path?manifest=f4m', {'manifest': []})),\n            parse_qs('http://example.com/path'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path?system=LINUX&system=WINDOWS', {'system': 'LINUX'})),\n            parse_qs('http://example.com/path?system=LINUX'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'fields': b'id,formats,subtitles'})),\n            parse_qs('http://example.com/path?fields=id,formats,subtitles'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'width': 1080, 'height': 720})),\n            parse_qs('http://example.com/path?width=1080&height=720'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'bitrate': 5020.43})),\n            parse_qs('http://example.com/path?bitrate=5020.43'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'test': '\u7b2c\u4e8c\u884c\u0442\u0435\u0441\u0442'})),\n            parse_qs('http://example.com/path?test=%E7%AC%AC%E4%BA%8C%E8%A1%8C%D1%82%D0%B5%D1%81%D1%82'))\n\n    def test_multipart_encode(self):\n        self.assertEqual(\n            multipart_encode({b'field': b'value'}, boundary='AAAAAA')[0],\n            b'--AAAAAA\\r\\nContent-Disposition: form-data; name=\"field\"\\r\\n\\r\\nvalue\\r\\n--AAAAAA--\\r\\n')\n        self.assertEqual(\n            multipart_encode({'\u6b04\u4f4d'.encode(): '\u503c'.encode()}, boundary='AAAAAA')[0],\n            b'--AAAAAA\\r\\nContent-Disposition: form-data; name=\"\\xe6\\xac\\x84\\xe4\\xbd\\x8d\"\\r\\n\\r\\n\\xe5\\x80\\xbc\\r\\n--AAAAAA--\\r\\n')\n        self.assertRaises(\n            ValueError, multipart_encode, {b'field': b'value'}, boundary='value')\n\n    def test_dict_get(self):\n        FALSE_VALUES = {\n            'none': None,\n            'false': False,\n            'zero': 0,\n            'empty_string': '',\n            'empty_list': [],\n        }\n        d = FALSE_VALUES.copy()\n        d['a'] = 42\n        self.assertEqual(dict_get(d, 'a'), 42)\n        self.assertEqual(dict_get(d, 'b'), None)\n        self.assertEqual(dict_get(d, 'b', 42), 42)\n        self.assertEqual(dict_get(d, ('a', )), 42)\n        self.assertEqual(dict_get(d, ('b', 'a', )), 42)\n        self.assertEqual(dict_get(d, ('b', 'c', 'a', 'd', )), 42)\n        self.assertEqual(dict_get(d, ('b', 'c', )), None)\n        self.assertEqual(dict_get(d, ('b', 'c', ), 42), 42)\n        for key, false_value in FALSE_VALUES.items():\n            self.assertEqual(dict_get(d, ('b', 'c', key, )), None)\n            self.assertEqual(dict_get(d, ('b', 'c', key, ), skip_false_values=False), false_value)\n\n    def test_merge_dicts(self):\n        self.assertEqual(merge_dicts({'a': 1}, {'b': 2}), {'a': 1, 'b': 2})\n        self.assertEqual(merge_dicts({'a': 1}, {'a': 2}), {'a': 1})\n        self.assertEqual(merge_dicts({'a': 1}, {'a': None}), {'a': 1})\n        self.assertEqual(merge_dicts({'a': 1}, {'a': ''}), {'a': 1})\n        self.assertEqual(merge_dicts({'a': 1}, {}), {'a': 1})\n        self.assertEqual(merge_dicts({'a': None}, {'a': 1}), {'a': 1})\n        self.assertEqual(merge_dicts({'a': ''}, {'a': 1}), {'a': ''})\n        self.assertEqual(merge_dicts({'a': ''}, {'a': 'abc'}), {'a': 'abc'})\n        self.assertEqual(merge_dicts({'a': None}, {'a': ''}, {'a': 'abc'}), {'a': 'abc'})\n\n    def test_encode_compat_str(self):\n        self.assertEqual(encode_compat_str(b'\\xd1\\x82\\xd0\\xb5\\xd1\\x81\\xd1\\x82', 'utf-8'), '\u0442\u0435\u0441\u0442')\n        self.assertEqual(encode_compat_str('\u0442\u0435\u0441\u0442', 'utf-8'), '\u0442\u0435\u0441\u0442')\n\n    def test_parse_iso8601(self):\n        self.assertEqual(parse_iso8601('2014-03-23T23:04:26+0100'), 1395612266)\n        self.assertEqual(parse_iso8601('2014-03-23T22:04:26+0000'), 1395612266)\n        self.assertEqual(parse_iso8601('2014-03-23T22:04:26Z'), 1395612266)\n        self.assertEqual(parse_iso8601('2014-03-23T22:04:26.1234Z'), 1395612266)\n        self.assertEqual(parse_iso8601('2015-09-29T08:27:31.727'), 1443515251)\n        self.assertEqual(parse_iso8601('2015-09-29T08-27-31.727'), None)\n\n    def test_strip_jsonp(self):\n        stripped = strip_jsonp('cb ([ {\"id\":\"532cb\",\\n\\n\\n\"x\":\\n3}\\n]\\n);')\n        d = json.loads(stripped)\n        self.assertEqual(d, [{\"id\": \"532cb\", \"x\": 3}])\n\n        stripped = strip_jsonp('parseMetadata({\"STATUS\":\"OK\"})\\n\\n\\n//epc')\n        d = json.loads(stripped)\n        self.assertEqual(d, {'STATUS': 'OK'})\n\n        stripped = strip_jsonp('ps.embedHandler({\"status\": \"success\"});')\n        d = json.loads(stripped)\n        self.assertEqual(d, {'status': 'success'})\n\n        stripped = strip_jsonp('window.cb && window.cb({\"status\": \"success\"});')\n        d = json.loads(stripped)\n        self.assertEqual(d, {'status': 'success'})\n\n        stripped = strip_jsonp('window.cb && cb({\"status\": \"success\"});')\n        d = json.loads(stripped)\n        self.assertEqual(d, {'status': 'success'})\n\n        stripped = strip_jsonp('({\"status\": \"success\"});')\n        d = json.loads(stripped)\n        self.assertEqual(d, {'status': 'success'})\n\n    def test_strip_or_none(self):\n        self.assertEqual(strip_or_none(' abc'), 'abc')\n        self.assertEqual(strip_or_none('abc '), 'abc')\n        self.assertEqual(strip_or_none(' abc '), 'abc')\n        self.assertEqual(strip_or_none('\\tabc\\t'), 'abc')\n        self.assertEqual(strip_or_none('\\n\\tabc\\n\\t'), 'abc')\n        self.assertEqual(strip_or_none('abc'), 'abc')\n        self.assertEqual(strip_or_none(''), '')\n        self.assertEqual(strip_or_none(None), None)\n        self.assertEqual(strip_or_none(42), None)\n        self.assertEqual(strip_or_none([]), None)\n\n    def test_uppercase_escape(self):\n        self.assertEqual(uppercase_escape('a\u00e4'), 'a\u00e4')\n        self.assertEqual(uppercase_escape('\\\\U0001d550'), '\ud835\udd50')\n\n    def test_lowercase_escape(self):\n        self.assertEqual(lowercase_escape('a\u00e4'), 'a\u00e4')\n        self.assertEqual(lowercase_escape('\\\\u0026'), '&')\n\n    def test_limit_length(self):\n        self.assertEqual(limit_length(None, 12), None)\n        self.assertEqual(limit_length('foo', 12), 'foo')\n        self.assertTrue(\n            limit_length('foo bar baz asd', 12).startswith('foo bar'))\n        self.assertTrue('...' in limit_length('foo bar baz asd', 12))\n\n    def test_mimetype2ext(self):\n        self.assertEqual(mimetype2ext(None), None)\n        self.assertEqual(mimetype2ext('video/x-flv'), 'flv')\n        self.assertEqual(mimetype2ext('application/x-mpegURL'), 'm3u8')\n        self.assertEqual(mimetype2ext('text/vtt'), 'vtt')\n        self.assertEqual(mimetype2ext('text/vtt;charset=utf-8'), 'vtt')\n        self.assertEqual(mimetype2ext('text/html; charset=utf-8'), 'html')\n        self.assertEqual(mimetype2ext('audio/x-wav'), 'wav')\n        self.assertEqual(mimetype2ext('audio/x-wav;codec=pcm'), 'wav')\n\n    def test_month_by_name(self):\n        self.assertEqual(month_by_name(None), None)\n        self.assertEqual(month_by_name('December', 'en'), 12)\n        self.assertEqual(month_by_name('d\u00e9cembre', 'fr'), 12)\n        self.assertEqual(month_by_name('December'), 12)\n        self.assertEqual(month_by_name('d\u00e9cembre'), None)\n        self.assertEqual(month_by_name('Unknown', 'unknown'), None)\n\n    def test_parse_codecs(self):\n        self.assertEqual(parse_codecs(''), {})\n        self.assertEqual(parse_codecs('avc1.77.30, mp4a.40.2'), {\n            'vcodec': 'avc1.77.30',\n            'acodec': 'mp4a.40.2',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('mp4a.40.2'), {\n            'vcodec': 'none',\n            'acodec': 'mp4a.40.2',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('mp4a.40.5,avc1.42001e'), {\n            'vcodec': 'avc1.42001e',\n            'acodec': 'mp4a.40.5',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('avc3.640028'), {\n            'vcodec': 'avc3.640028',\n            'acodec': 'none',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs(', h264,,newcodec,aac'), {\n            'vcodec': 'h264',\n            'acodec': 'aac',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('av01.0.05M.08'), {\n            'vcodec': 'av01.0.05M.08',\n            'acodec': 'none',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('vp9.2'), {\n            'vcodec': 'vp9.2',\n            'acodec': 'none',\n            'dynamic_range': 'HDR10',\n        })\n        self.assertEqual(parse_codecs('av01.0.12M.10.0.110.09.16.09.0'), {\n            'vcodec': 'av01.0.12M.10.0.110.09.16.09.0',\n            'acodec': 'none',\n            'dynamic_range': 'HDR10',\n        })\n        self.assertEqual(parse_codecs('dvhe'), {\n            'vcodec': 'dvhe',\n            'acodec': 'none',\n            'dynamic_range': 'DV',\n        })\n        self.assertEqual(parse_codecs('theora, vorbis'), {\n            'vcodec': 'theora',\n            'acodec': 'vorbis',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('unknownvcodec, unknownacodec'), {\n            'vcodec': 'unknownvcodec',\n            'acodec': 'unknownacodec',\n        })\n        self.assertEqual(parse_codecs('unknown'), {})\n\n    def test_escape_rfc3986(self):\n        reserved = \"!*'();:@&=+$,/?#[]\"\n        unreserved = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_.~'\n        self.assertEqual(escape_rfc3986(reserved), reserved)\n        self.assertEqual(escape_rfc3986(unreserved), unreserved)\n        self.assertEqual(escape_rfc3986('\u0442\u0435\u0441\u0442'), '%D1%82%D0%B5%D1%81%D1%82')\n        self.assertEqual(escape_rfc3986('%D1%82%D0%B5%D1%81%D1%82'), '%D1%82%D0%B5%D1%81%D1%82')\n        self.assertEqual(escape_rfc3986('foo bar'), 'foo%20bar')\n        self.assertEqual(escape_rfc3986('foo%20bar'), 'foo%20bar')\n\n    def test_normalize_url(self):\n        self.assertEqual(\n            normalize_url('http://wowza.imust.org/srv/vod/telemb/new/UPLOAD/UPLOAD/20224_IncendieHavre\u0301_FD.mp4'),\n            'http://wowza.imust.org/srv/vod/telemb/new/UPLOAD/UPLOAD/20224_IncendieHavre%CC%81_FD.mp4'\n        )\n        self.assertEqual(\n            normalize_url('http://www.ardmediathek.de/tv/Sturm-der-Liebe/Folge-2036-Zu-Mann-und-Frau-erkl\u00e4rt/Das-Erste/Video?documentId=22673108&bcastId=5290'),\n            'http://www.ardmediathek.de/tv/Sturm-der-Liebe/Folge-2036-Zu-Mann-und-Frau-erkl%C3%A4rt/Das-Erste/Video?documentId=22673108&bcastId=5290'\n        )\n        self.assertEqual(\n            normalize_url('http://\u0442\u0435\u0441\u0442.\u0440\u0444/\u0444\u0440\u0430\u0433\u043c\u0435\u043d\u0442'),\n            'http://xn--e1aybc.xn--p1ai/%D1%84%D1%80%D0%B0%D0%B3%D0%BC%D0%B5%D0%BD%D1%82'\n        )\n        self.assertEqual(\n            normalize_url('http://\u0442\u0435\u0441\u0442.\u0440\u0444/\u0430\u0431\u0432?\u0430\u0431\u0432=\u0430\u0431\u0432#\u0430\u0431\u0432'),\n            'http://xn--e1aybc.xn--p1ai/%D0%B0%D0%B1%D0%B2?%D0%B0%D0%B1%D0%B2=%D0%B0%D0%B1%D0%B2#%D0%B0%D0%B1%D0%B2'\n        )\n        self.assertEqual(normalize_url('http://vimeo.com/56015672#at=0'), 'http://vimeo.com/56015672#at=0')\n\n        self.assertEqual(normalize_url('http://www.example.com/../a/b/../c/./d.html'), 'http://www.example.com/a/c/d.html')\n\n    def test_remove_dot_segments(self):\n        self.assertEqual(remove_dot_segments('/a/b/c/./../../g'), '/a/g')\n        self.assertEqual(remove_dot_segments('mid/content=5/../6'), 'mid/6')\n        self.assertEqual(remove_dot_segments('/ad/../cd'), '/cd')\n        self.assertEqual(remove_dot_segments('/ad/../cd/'), '/cd/')\n        self.assertEqual(remove_dot_segments('/..'), '/')\n        self.assertEqual(remove_dot_segments('/./'), '/')\n        self.assertEqual(remove_dot_segments('/./a'), '/a')\n        self.assertEqual(remove_dot_segments('/abc/./.././d/././e/.././f/./../../ghi'), '/ghi')\n        self.assertEqual(remove_dot_segments('/'), '/')\n        self.assertEqual(remove_dot_segments('/t'), '/t')\n        self.assertEqual(remove_dot_segments('t'), 't')\n        self.assertEqual(remove_dot_segments(''), '')\n        self.assertEqual(remove_dot_segments('/../a/b/c'), '/a/b/c')\n        self.assertEqual(remove_dot_segments('../a'), 'a')\n        self.assertEqual(remove_dot_segments('./a'), 'a')\n        self.assertEqual(remove_dot_segments('.'), '')\n        self.assertEqual(remove_dot_segments('////'), '////')\n\n    def test_js_to_json_vars_strings(self):\n        self.assertDictEqual(\n            json.loads(js_to_json(\n                '''{\n                    'null': a,\n                    'nullStr': b,\n                    'true': c,\n                    'trueStr': d,\n                    'false': e,\n                    'falseStr': f,\n                    'unresolvedVar': g,\n                }''',\n                {\n                    'a': 'null',\n                    'b': '\"null\"',\n                    'c': 'true',\n                    'd': '\"true\"',\n                    'e': 'false',\n                    'f': '\"false\"',\n                    'g': 'var',\n                }\n            )),\n            {\n                'null': None,\n                'nullStr': 'null',\n                'true': True,\n                'trueStr': 'true',\n                'false': False,\n                'falseStr': 'false',\n                'unresolvedVar': 'var'\n            }\n        )\n\n        self.assertDictEqual(\n            json.loads(js_to_json(\n                '''{\n                    'int': a,\n                    'intStr': b,\n                    'float': c,\n                    'floatStr': d,\n                }''',\n                {\n                    'a': '123',\n                    'b': '\"123\"',\n                    'c': '1.23',\n                    'd': '\"1.23\"',\n                }\n            )),\n            {\n                'int': 123,\n                'intStr': '123',\n                'float': 1.23,\n                'floatStr': '1.23',\n            }\n        )\n\n        self.assertDictEqual(\n            json.loads(js_to_json(\n                '''{\n                    'object': a,\n                    'objectStr': b,\n                    'array': c,\n                    'arrayStr': d,\n                }''',\n                {\n                    'a': '{}',\n                    'b': '\"{}\"',\n                    'c': '[]',\n                    'd': '\"[]\"',\n                }\n            )),\n            {\n                'object': {},\n                'objectStr': '{}',\n                'array': [],\n                'arrayStr': '[]',\n            }\n        )\n\n    def test_js_to_json_realworld(self):\n        inp = '''{\n            'clip':{'provider':'pseudo'}\n        }'''\n        self.assertEqual(js_to_json(inp), '''{\n            \"clip\":{\"provider\":\"pseudo\"}\n        }''')\n        json.loads(js_to_json(inp))\n\n        inp = '''{\n            'playlist':[{'controls':{'all':null}}]\n        }'''\n        self.assertEqual(js_to_json(inp), '''{\n            \"playlist\":[{\"controls\":{\"all\":null}}]\n        }''')\n\n        inp = '''\"The CW\\\\'s \\\\'Crazy Ex-Girlfriend\\\\'\"'''\n        self.assertEqual(js_to_json(inp), '''\"The CW's 'Crazy Ex-Girlfriend'\"''')\n\n        inp = '\"SAND Number: SAND 2013-7800P\\\\nPresenter: Tom Russo\\\\nHabanero Software Training - Xyce Software\\\\nXyce, Sandia\\\\u0027s\"'\n        json_code = js_to_json(inp)\n        self.assertEqual(json.loads(json_code), json.loads(inp))\n\n        inp = '''{\n            0:{src:'skipped', type: 'application/dash+xml'},\n            1:{src:'skipped', type: 'application/vnd.apple.mpegURL'},\n        }'''\n        self.assertEqual(js_to_json(inp), '''{\n            \"0\":{\"src\":\"skipped\", \"type\": \"application/dash+xml\"},\n            \"1\":{\"src\":\"skipped\", \"type\": \"application/vnd.apple.mpegURL\"}\n        }''')\n\n        inp = '''{\"foo\":101}'''\n        self.assertEqual(js_to_json(inp), '''{\"foo\":101}''')\n\n        inp = '''{\"duration\": \"00:01:07\"}'''\n        self.assertEqual(js_to_json(inp), '''{\"duration\": \"00:01:07\"}''')\n\n        inp = '''{segments: [{\"offset\":-3.885780586188048e-16,\"duration\":39.75000000000001}]}'''\n        self.assertEqual(js_to_json(inp), '''{\"segments\": [{\"offset\":-3.885780586188048e-16,\"duration\":39.75000000000001}]}''')\n\n    def test_js_to_json_edgecases(self):\n        on = js_to_json(\"{abc_def:'1\\\\'\\\\\\\\2\\\\\\\\\\\\'3\\\"4'}\")\n        self.assertEqual(json.loads(on), {\"abc_def\": \"1'\\\\2\\\\'3\\\"4\"})\n\n        on = js_to_json('{\"abc\": true}')\n        self.assertEqual(json.loads(on), {'abc': True})\n\n        # Ignore JavaScript code as well\n        on = js_to_json('''{\n            \"x\": 1,\n            y: \"a\",\n            z: some.code\n        }''')\n        d = json.loads(on)\n        self.assertEqual(d['x'], 1)\n        self.assertEqual(d['y'], 'a')\n\n        # Just drop ! prefix for now though this results in a wrong value\n        on = js_to_json('''{\n            a: !0,\n            b: !1,\n            c: !!0,\n            d: !!42.42,\n            e: !!![],\n            f: !\"abc\",\n            g: !\"\",\n            !42: 42\n        }''')\n        self.assertEqual(json.loads(on), {\n            'a': 0,\n            'b': 1,\n            'c': 0,\n            'd': 42.42,\n            'e': [],\n            'f': \"abc\",\n            'g': \"\",\n            '42': 42\n        })\n\n        on = js_to_json('[\"abc\", \"def\",]')\n        self.assertEqual(json.loads(on), ['abc', 'def'])\n\n        on = js_to_json('[/*comment\\n*/\"abc\"/*comment\\n*/,/*comment\\n*/\"def\",/*comment\\n*/]')\n        self.assertEqual(json.loads(on), ['abc', 'def'])\n\n        on = js_to_json('[//comment\\n\"abc\" //comment\\n,//comment\\n\"def\",//comment\\n]')\n        self.assertEqual(json.loads(on), ['abc', 'def'])\n\n        on = js_to_json('{\"abc\": \"def\",}')\n        self.assertEqual(json.loads(on), {'abc': 'def'})\n\n        on = js_to_json('{/*comment\\n*/\"abc\"/*comment\\n*/:/*comment\\n*/\"def\"/*comment\\n*/,/*comment\\n*/}')\n        self.assertEqual(json.loads(on), {'abc': 'def'})\n\n        on = js_to_json('{ 0: /* \" \\n */ \",]\" , }')\n        self.assertEqual(json.loads(on), {'0': ',]'})\n\n        on = js_to_json('{ /*comment\\n*/0/*comment\\n*/: /* \" \\n */ \",]\" , }')\n        self.assertEqual(json.loads(on), {'0': ',]'})\n\n        on = js_to_json('{ 0: // comment\\n1 }')\n        self.assertEqual(json.loads(on), {'0': 1})\n\n        on = js_to_json(r'[\"<p>x<\\/p>\"]')\n        self.assertEqual(json.loads(on), ['<p>x</p>'])\n\n        on = js_to_json(r'[\"\\xaa\"]')\n        self.assertEqual(json.loads(on), ['\\u00aa'])\n\n        on = js_to_json(\"['a\\\\\\nb']\")\n        self.assertEqual(json.loads(on), ['ab'])\n\n        on = js_to_json(\"/*comment\\n*/[/*comment\\n*/'a\\\\\\nb'/*comment\\n*/]/*comment\\n*/\")\n        self.assertEqual(json.loads(on), ['ab'])\n\n        on = js_to_json('{0xff:0xff}')\n        self.assertEqual(json.loads(on), {'255': 255})\n\n        on = js_to_json('{/*comment\\n*/0xff/*comment\\n*/:/*comment\\n*/0xff/*comment\\n*/}')\n        self.assertEqual(json.loads(on), {'255': 255})\n\n        on = js_to_json('{077:077}')\n        self.assertEqual(json.loads(on), {'63': 63})\n\n        on = js_to_json('{/*comment\\n*/077/*comment\\n*/:/*comment\\n*/077/*comment\\n*/}')\n        self.assertEqual(json.loads(on), {'63': 63})\n\n        on = js_to_json('{42:42}')\n        self.assertEqual(json.loads(on), {'42': 42})\n\n        on = js_to_json('{/*comment\\n*/42/*comment\\n*/:/*comment\\n*/42/*comment\\n*/}')\n        self.assertEqual(json.loads(on), {'42': 42})\n\n        on = js_to_json('{42:4.2e1}')\n        self.assertEqual(json.loads(on), {'42': 42.0})\n\n        on = js_to_json('{ \"0x40\": \"0x40\" }')\n        self.assertEqual(json.loads(on), {'0x40': '0x40'})\n\n        on = js_to_json('{ \"040\": \"040\" }')\n        self.assertEqual(json.loads(on), {'040': '040'})\n\n        on = js_to_json('[1,//{},\\n2]')\n        self.assertEqual(json.loads(on), [1, 2])\n\n        on = js_to_json(R'\"\\^\\$\\#\"')\n        self.assertEqual(json.loads(on), R'^$#', msg='Unnecessary escapes should be stripped')\n\n        on = js_to_json('\\'\"\\\\\"\"\\'')\n        self.assertEqual(json.loads(on), '\"\"\"', msg='Unnecessary quote escape should be escaped')\n\n    def test_js_to_json_malformed(self):\n        self.assertEqual(js_to_json('42a1'), '42\"a1\"')\n        self.assertEqual(js_to_json('42a-1'), '42\"a\"-1')\n\n    def test_js_to_json_template_literal(self):\n        self.assertEqual(js_to_json('`Hello ${name}`', {'name': '\"world\"'}), '\"Hello world\"')\n        self.assertEqual(js_to_json('`${name}${name}`', {'name': '\"X\"'}), '\"XX\"')\n        self.assertEqual(js_to_json('`${name}${name}`', {'name': '5'}), '\"55\"')\n        self.assertEqual(js_to_json('`${name}\"${name}\"`', {'name': '5'}), '\"5\\\\\"5\\\\\"\"')\n        self.assertEqual(js_to_json('`${name}`', {}), '\"name\"')\n\n    def test_js_to_json_map_array_constructors(self):\n        self.assertEqual(json.loads(js_to_json('new Map([[\"a\", 5]])')), {'a': 5})\n        self.assertEqual(json.loads(js_to_json('Array(5, 10)')), [5, 10])\n        self.assertEqual(json.loads(js_to_json('new Array(15,5)')), [15, 5])\n        self.assertEqual(json.loads(js_to_json('new Map([Array(5, 10),new Array(15,5)])')), {'5': 10, '15': 5})\n\n    def test_extract_attributes(self):\n        self.assertEqual(extract_attributes('<e x=\"y\">'), {'x': 'y'})\n        self.assertEqual(extract_attributes(\"<e x='y'>\"), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e x=y>'), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e x=\"a \\'b\\' c\">'), {'x': \"a 'b' c\"})\n        self.assertEqual(extract_attributes('<e x=\\'a \"b\" c\\'>'), {'x': 'a \"b\" c'})\n        self.assertEqual(extract_attributes('<e x=\"&#121;\">'), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e x=\"&#x79;\">'), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e x=\"&amp;\">'), {'x': '&'})  # XML\n        self.assertEqual(extract_attributes('<e x=\"&quot;\">'), {'x': '\"'})\n        self.assertEqual(extract_attributes('<e x=\"&pound;\">'), {'x': '\u00a3'})  # HTML 3.2\n        self.assertEqual(extract_attributes('<e x=\"&lambda;\">'), {'x': '\u03bb'})  # HTML 4.0\n        self.assertEqual(extract_attributes('<e x=\"&foo\">'), {'x': '&foo'})\n        self.assertEqual(extract_attributes('<e x=\"\\'\">'), {'x': \"'\"})\n        self.assertEqual(extract_attributes('<e x=\\'\"\\'>'), {'x': '\"'})\n        self.assertEqual(extract_attributes('<e x >'), {'x': None})\n        self.assertEqual(extract_attributes('<e x=y a>'), {'x': 'y', 'a': None})\n        self.assertEqual(extract_attributes('<e x= y>'), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e x=1 y=2 x=3>'), {'y': '2', 'x': '3'})\n        self.assertEqual(extract_attributes('<e \\nx=\\ny\\n>'), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e \\nx=\\n\"y\"\\n>'), {'x': 'y'})\n        self.assertEqual(extract_attributes(\"<e \\nx=\\n'y'\\n>\"), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e \\nx=\"\\ny\\n\">'), {'x': '\\ny\\n'})\n        self.assertEqual(extract_attributes('<e CAPS=x>'), {'caps': 'x'})  # Names lowercased\n        self.assertEqual(extract_attributes('<e x=1 X=2>'), {'x': '2'})\n        self.assertEqual(extract_attributes('<e X=1 x=2>'), {'x': '2'})\n        self.assertEqual(extract_attributes('<e _:funny-name1=1>'), {'_:funny-name1': '1'})\n        self.assertEqual(extract_attributes('<e x=\"F\u00e1ilte \u4e16\u754c \\U0001f600\">'), {'x': 'F\u00e1ilte \u4e16\u754c \\U0001f600'})\n        self.assertEqual(extract_attributes('<e x=\"d\u00e9compose&#769;\">'), {'x': 'd\u00e9compose\\u0301'})\n        # \"Narrow\" Python builds don't support unicode code points outside BMP.\n        try:\n            chr(0x10000)\n            supports_outside_bmp = True\n        except ValueError:\n            supports_outside_bmp = False\n        if supports_outside_bmp:\n            self.assertEqual(extract_attributes('<e x=\"Smile &#128512;!\">'), {'x': 'Smile \\U0001f600!'})\n        # Malformed HTML should not break attributes extraction on older Python\n        self.assertEqual(extract_attributes('<mal\"formed/>'), {})\n\n    def test_clean_html(self):\n        self.assertEqual(clean_html('a:\\nb'), 'a: b')\n        self.assertEqual(clean_html('a:\\n   \"b\"'), 'a: \"b\"')\n        self.assertEqual(clean_html('a<br>\\xa0b'), 'a\\nb')\n\n    def test_intlist_to_bytes(self):\n        self.assertEqual(\n            intlist_to_bytes([0, 1, 127, 128, 255]),\n            b'\\x00\\x01\\x7f\\x80\\xff')\n\n    def test_args_to_str(self):\n        self.assertEqual(\n            args_to_str(['foo', 'ba/r', '-baz', '2 be', '']),\n            'foo ba/r -baz \\'2 be\\' \\'\\'' if compat_os_name != 'nt' else 'foo ba/r -baz \"2 be\" \"\"'\n        )\n\n    def test_parse_filesize(self):\n        self.assertEqual(parse_filesize(None), None)\n        self.assertEqual(parse_filesize(''), None)\n        self.assertEqual(parse_filesize('91 B'), 91)\n        self.assertEqual(parse_filesize('foobar'), None)\n        self.assertEqual(parse_filesize('2 MiB'), 2097152)\n        self.assertEqual(parse_filesize('5 GB'), 5000000000)\n        self.assertEqual(parse_filesize('1.2Tb'), 1200000000000)\n        self.assertEqual(parse_filesize('1.2tb'), 1200000000000)\n        self.assertEqual(parse_filesize('1,24 KB'), 1240)\n        self.assertEqual(parse_filesize('1,24 kb'), 1240)\n        self.assertEqual(parse_filesize('8.5 megabytes'), 8500000)\n\n    def test_parse_count(self):\n        self.assertEqual(parse_count(None), None)\n        self.assertEqual(parse_count(''), None)\n        self.assertEqual(parse_count('0'), 0)\n        self.assertEqual(parse_count('1000'), 1000)\n        self.assertEqual(parse_count('1.000'), 1000)\n        self.assertEqual(parse_count('1.1k'), 1100)\n        self.assertEqual(parse_count('1.1 k'), 1100)\n        self.assertEqual(parse_count('1,1 k'), 1100)\n        self.assertEqual(parse_count('1.1kk'), 1100000)\n        self.assertEqual(parse_count('1.1kk '), 1100000)\n        self.assertEqual(parse_count('1,1kk'), 1100000)\n        self.assertEqual(parse_count('100 views'), 100)\n        self.assertEqual(parse_count('1,100 views'), 1100)\n        self.assertEqual(parse_count('1.1kk views'), 1100000)\n        self.assertEqual(parse_count('10M views'), 10000000)\n        self.assertEqual(parse_count('has 10M views'), 10000000)\n\n    def test_parse_resolution(self):\n        self.assertEqual(parse_resolution(None), {})\n        self.assertEqual(parse_resolution(''), {})\n        self.assertEqual(parse_resolution(' 1920x1080'), {'width': 1920, 'height': 1080})\n        self.assertEqual(parse_resolution('1920\u00d71080 '), {'width': 1920, 'height': 1080})\n        self.assertEqual(parse_resolution('1920 x 1080'), {'width': 1920, 'height': 1080})\n        self.assertEqual(parse_resolution('720p'), {'height': 720})\n        self.assertEqual(parse_resolution('4k'), {'height': 2160})\n        self.assertEqual(parse_resolution('8K'), {'height': 4320})\n        self.assertEqual(parse_resolution('pre_1920x1080_post'), {'width': 1920, 'height': 1080})\n        self.assertEqual(parse_resolution('ep1x2'), {})\n        self.assertEqual(parse_resolution('1920, 1080'), {'width': 1920, 'height': 1080})\n\n    def test_parse_bitrate(self):\n        self.assertEqual(parse_bitrate(None), None)\n        self.assertEqual(parse_bitrate(''), None)\n        self.assertEqual(parse_bitrate('300kbps'), 300)\n        self.assertEqual(parse_bitrate('1500kbps'), 1500)\n        self.assertEqual(parse_bitrate('300 kbps'), 300)\n\n    def test_version_tuple(self):\n        self.assertEqual(version_tuple('1'), (1,))\n        self.assertEqual(version_tuple('10.23.344'), (10, 23, 344))\n        self.assertEqual(version_tuple('10.1-6'), (10, 1, 6))  # avconv style\n\n    def test_detect_exe_version(self):\n        self.assertEqual(detect_exe_version('''ffmpeg version 1.2.1\nbuilt on May 27 2013 08:37:26 with gcc 4.7 (Debian 4.7.3-4)\nconfiguration: --prefix=/usr --extra-'''), '1.2.1')\n        self.assertEqual(detect_exe_version('''ffmpeg version N-63176-g1fb4685\nbuilt on May 15 2014 22:09:06 with gcc 4.8.2 (GCC)'''), 'N-63176-g1fb4685')\n        self.assertEqual(detect_exe_version('''X server found. dri2 connection failed!\nTrying to open render node...\nSuccess at /dev/dri/renderD128.\nffmpeg version 2.4.4 Copyright (c) 2000-2014 the FFmpeg ...'''), '2.4.4')\n\n    def test_age_restricted(self):\n        self.assertFalse(age_restricted(None, 10))  # unrestricted content\n        self.assertFalse(age_restricted(1, None))  # unrestricted policy\n        self.assertFalse(age_restricted(8, 10))\n        self.assertTrue(age_restricted(18, 14))\n        self.assertFalse(age_restricted(18, 18))\n\n    def test_is_html(self):\n        self.assertFalse(is_html(b'\\x49\\x44\\x43<html'))\n        self.assertTrue(is_html(b'<!DOCTYPE foo>\\xaaa'))\n        self.assertTrue(is_html(  # UTF-8 with BOM\n            b'\\xef\\xbb\\xbf<!DOCTYPE foo>\\xaaa'))\n        self.assertTrue(is_html(  # UTF-16-LE\n            b'\\xff\\xfe<\\x00h\\x00t\\x00m\\x00l\\x00>\\x00\\xe4\\x00'\n        ))\n        self.assertTrue(is_html(  # UTF-16-BE\n            b'\\xfe\\xff\\x00<\\x00h\\x00t\\x00m\\x00l\\x00>\\x00\\xe4'\n        ))\n        self.assertTrue(is_html(  # UTF-32-BE\n            b'\\x00\\x00\\xFE\\xFF\\x00\\x00\\x00<\\x00\\x00\\x00h\\x00\\x00\\x00t\\x00\\x00\\x00m\\x00\\x00\\x00l\\x00\\x00\\x00>\\x00\\x00\\x00\\xe4'))\n        self.assertTrue(is_html(  # UTF-32-LE\n            b'\\xFF\\xFE\\x00\\x00<\\x00\\x00\\x00h\\x00\\x00\\x00t\\x00\\x00\\x00m\\x00\\x00\\x00l\\x00\\x00\\x00>\\x00\\x00\\x00\\xe4\\x00\\x00\\x00'))\n\n    def test_render_table(self):\n        self.assertEqual(\n            render_table(\n                ['a', 'empty', 'bcd'],\n                [[123, '', 4], [9999, '', 51]]),\n            'a    empty bcd\\n'\n            '123        4\\n'\n            '9999       51')\n\n        self.assertEqual(\n            render_table(\n                ['a', 'empty', 'bcd'],\n                [[123, '', 4], [9999, '', 51]],\n                hide_empty=True),\n            'a    bcd\\n'\n            '123  4\\n'\n            '9999 51')\n\n        self.assertEqual(\n            render_table(\n                ['\\ta', 'bcd'],\n                [['1\\t23', 4], ['\\t9999', 51]]),\n            '   a bcd\\n'\n            '1 23 4\\n'\n            '9999 51')\n\n        self.assertEqual(\n            render_table(\n                ['a', 'bcd'],\n                [[123, 4], [9999, 51]],\n                delim='-'),\n            'a    bcd\\n'\n            '--------\\n'\n            '123  4\\n'\n            '9999 51')\n\n        self.assertEqual(\n            render_table(\n                ['a', 'bcd'],\n                [[123, 4], [9999, 51]],\n                delim='-', extra_gap=2),\n            'a      bcd\\n'\n            '----------\\n'\n            '123    4\\n'\n            '9999   51')\n\n    def test_match_str(self):\n        # Unary\n        self.assertFalse(match_str('xy', {'x': 1200}))\n        self.assertTrue(match_str('!xy', {'x': 1200}))\n        self.assertTrue(match_str('x', {'x': 1200}))\n        self.assertFalse(match_str('!x', {'x': 1200}))\n        self.assertTrue(match_str('x', {'x': 0}))\n        self.assertTrue(match_str('is_live', {'is_live': True}))\n        self.assertFalse(match_str('is_live', {'is_live': False}))\n        self.assertFalse(match_str('is_live', {'is_live': None}))\n        self.assertFalse(match_str('is_live', {}))\n        self.assertFalse(match_str('!is_live', {'is_live': True}))\n        self.assertTrue(match_str('!is_live', {'is_live': False}))\n        self.assertTrue(match_str('!is_live', {'is_live': None}))\n        self.assertTrue(match_str('!is_live', {}))\n        self.assertTrue(match_str('title', {'title': 'abc'}))\n        self.assertTrue(match_str('title', {'title': ''}))\n        self.assertFalse(match_str('!title', {'title': 'abc'}))\n        self.assertFalse(match_str('!title', {'title': ''}))\n\n        # Numeric\n        self.assertFalse(match_str('x>0', {'x': 0}))\n        self.assertFalse(match_str('x>0', {}))\n        self.assertTrue(match_str('x>?0', {}))\n        self.assertTrue(match_str('x>1K', {'x': 1200}))\n        self.assertFalse(match_str('x>2K', {'x': 1200}))\n        self.assertTrue(match_str('x>=1200 & x < 1300', {'x': 1200}))\n        self.assertFalse(match_str('x>=1100 & x < 1200', {'x': 1200}))\n        self.assertTrue(match_str('x > 1:0:0', {'x': 3700}))\n\n        # String\n        self.assertFalse(match_str('y=a212', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y=foobar42', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y!=foobar42', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y!=foobar2', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y^=foo', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y!^=foo', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y^=bar', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y!^=bar', {'y': 'foobar42'}))\n        self.assertRaises(ValueError, match_str, 'x^=42', {'x': 42})\n        self.assertTrue(match_str('y*=bar', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y!*=bar', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y*=baz', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y!*=baz', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y$=42', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y$=43', {'y': 'foobar42'}))\n\n        # And\n        self.assertFalse(match_str(\n            'like_count > 100 & dislike_count <? 50 & description',\n            {'like_count': 90, 'description': 'foo'}))\n        self.assertTrue(match_str(\n            'like_count > 100 & dislike_count <? 50 & description',\n            {'like_count': 190, 'description': 'foo'}))\n        self.assertFalse(match_str(\n            'like_count > 100 & dislike_count <? 50 & description',\n            {'like_count': 190, 'dislike_count': 60, 'description': 'foo'}))\n        self.assertFalse(match_str(\n            'like_count > 100 & dislike_count <? 50 & description',\n            {'like_count': 190, 'dislike_count': 10}))\n\n        # Regex\n        self.assertTrue(match_str(r'x~=\\bbar', {'x': 'foo bar'}))\n        self.assertFalse(match_str(r'x~=\\bbar.+', {'x': 'foo bar'}))\n        self.assertFalse(match_str(r'x~=^FOO', {'x': 'foo bar'}))\n        self.assertTrue(match_str(r'x~=(?i)^FOO', {'x': 'foo bar'}))\n\n        # Quotes\n        self.assertTrue(match_str(r'x^=\"foo\"', {'x': 'foo \"bar\"'}))\n        self.assertFalse(match_str(r'x^=\"foo  \"', {'x': 'foo \"bar\"'}))\n        self.assertFalse(match_str(r'x$=\"bar\"', {'x': 'foo \"bar\"'}))\n        self.assertTrue(match_str(r'x$=\" \\\"bar\\\"\"', {'x': 'foo \"bar\"'}))\n\n        # Escaping &\n        self.assertFalse(match_str(r'x=foo & bar', {'x': 'foo & bar'}))\n        self.assertTrue(match_str(r'x=foo \\& bar', {'x': 'foo & bar'}))\n        self.assertTrue(match_str(r'x=foo \\& bar & x^=foo', {'x': 'foo & bar'}))\n        self.assertTrue(match_str(r'x=\"foo \\& bar\" & x^=foo', {'x': 'foo & bar'}))\n\n        # Example from docs\n        self.assertTrue(match_str(\n            r\"!is_live & like_count>?100 & description~='(?i)\\bcats \\& dogs\\b'\",\n            {'description': 'Raining Cats & Dogs'}))\n\n        # Incomplete\n        self.assertFalse(match_str('id!=foo', {'id': 'foo'}, True))\n        self.assertTrue(match_str('x', {'id': 'foo'}, True))\n        self.assertTrue(match_str('!x', {'id': 'foo'}, True))\n        self.assertFalse(match_str('x', {'id': 'foo'}, False))\n\n    def test_parse_dfxp_time_expr(self):\n        self.assertEqual(parse_dfxp_time_expr(None), None)\n        self.assertEqual(parse_dfxp_time_expr(''), None)\n        self.assertEqual(parse_dfxp_time_expr('0.1'), 0.1)\n        self.assertEqual(parse_dfxp_time_expr('0.1s'), 0.1)\n        self.assertEqual(parse_dfxp_time_expr('00:00:01'), 1.0)\n        self.assertEqual(parse_dfxp_time_expr('00:00:01.100'), 1.1)\n        self.assertEqual(parse_dfxp_time_expr('00:00:01:100'), 1.1)\n\n    def test_dfxp2srt(self):\n        dfxp_data = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <tt xmlns=\"http://www.w3.org/ns/ttml\" xml:lang=\"en\" xmlns:tts=\"http://www.w3.org/ns/ttml#parameter\">\n            <body>\n                <div xml:lang=\"en\">\n                    <p begin=\"0\" end=\"1\">The following line contains Chinese characters and special symbols</p>\n                    <p begin=\"1\" end=\"2\">\u7b2c\u4e8c\u884c<br/>\u266a\u266a</p>\n                    <p begin=\"2\" dur=\"1\"><span>Third<br/>Line</span></p>\n                    <p begin=\"3\" end=\"-1\">Lines with invalid timestamps are ignored</p>\n                    <p begin=\"-1\" end=\"-1\">Ignore, two</p>\n                    <p begin=\"3\" dur=\"-1\">Ignored, three</p>\n                </div>\n            </body>\n            </tt>'''.encode()\n        srt_data = '''1\n00:00:00,000 --> 00:00:01,000\nThe following line contains Chinese characters and special symbols\n\n2\n00:00:01,000 --> 00:00:02,000\n\u7b2c\u4e8c\u884c\n\u266a\u266a\n\n3\n00:00:02,000 --> 00:00:03,000\nThird\nLine\n\n'''\n        self.assertEqual(dfxp2srt(dfxp_data), srt_data)\n\n        dfxp_data_no_default_namespace = b'''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <tt xml:lang=\"en\" xmlns:tts=\"http://www.w3.org/ns/ttml#parameter\">\n            <body>\n                <div xml:lang=\"en\">\n                    <p begin=\"0\" end=\"1\">The first line</p>\n                </div>\n            </body>\n            </tt>'''\n        srt_data = '''1\n00:00:00,000 --> 00:00:01,000\nThe first line\n\n'''\n        self.assertEqual(dfxp2srt(dfxp_data_no_default_namespace), srt_data)\n\n        dfxp_data_with_style = b'''<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<tt xmlns=\"http://www.w3.org/2006/10/ttaf1\" xmlns:ttp=\"http://www.w3.org/2006/10/ttaf1#parameter\" ttp:timeBase=\"media\" xmlns:tts=\"http://www.w3.org/2006/10/ttaf1#style\" xml:lang=\"en\" xmlns:ttm=\"http://www.w3.org/2006/10/ttaf1#metadata\">\n  <head>\n    <styling>\n      <style id=\"s2\" style=\"s0\" tts:color=\"cyan\" tts:fontWeight=\"bold\" />\n      <style id=\"s1\" style=\"s0\" tts:color=\"yellow\" tts:fontStyle=\"italic\" />\n      <style id=\"s3\" style=\"s0\" tts:color=\"lime\" tts:textDecoration=\"underline\" />\n      <style id=\"s0\" tts:backgroundColor=\"black\" tts:fontStyle=\"normal\" tts:fontSize=\"16\" tts:fontFamily=\"sansSerif\" tts:color=\"white\" />\n    </styling>\n  </head>\n  <body tts:textAlign=\"center\" style=\"s0\">\n    <div>\n      <p begin=\"00:00:02.08\" id=\"p0\" end=\"00:00:05.84\">default style<span tts:color=\"red\">custom style</span></p>\n      <p style=\"s2\" begin=\"00:00:02.08\" id=\"p0\" end=\"00:00:05.84\"><span tts:color=\"lime\">part 1<br /></span><span tts:color=\"cyan\">part 2</span></p>\n      <p style=\"s3\" begin=\"00:00:05.84\" id=\"p1\" end=\"00:00:09.56\">line 3<br />part 3</p>\n      <p style=\"s1\" tts:textDecoration=\"underline\" begin=\"00:00:09.56\" id=\"p2\" end=\"00:00:12.36\"><span style=\"s2\" tts:color=\"lime\">inner<br /> </span>style</p>\n    </div>\n  </body>\n</tt>'''\n        srt_data = '''1\n00:00:02,080 --> 00:00:05,840\n<font color=\"white\" face=\"sansSerif\" size=\"16\">default style<font color=\"red\">custom style</font></font>\n\n2\n00:00:02,080 --> 00:00:05,840\n<b><font color=\"cyan\" face=\"sansSerif\" size=\"16\"><font color=\"lime\">part 1\n</font>part 2</font></b>\n\n3\n00:00:05,840 --> 00:00:09,560\n<u><font color=\"lime\">line 3\npart 3</font></u>\n\n4\n00:00:09,560 --> 00:00:12,360\n<i><u><font color=\"yellow\"><font color=\"lime\">inner\n </font>style</font></u></i>\n\n'''\n        self.assertEqual(dfxp2srt(dfxp_data_with_style), srt_data)\n\n        dfxp_data_non_utf8 = '''<?xml version=\"1.0\" encoding=\"UTF-16\"?>\n            <tt xmlns=\"http://www.w3.org/ns/ttml\" xml:lang=\"en\" xmlns:tts=\"http://www.w3.org/ns/ttml#parameter\">\n            <body>\n                <div xml:lang=\"en\">\n                    <p begin=\"0\" end=\"1\">Line 1</p>\n                    <p begin=\"1\" end=\"2\">\u7b2c\u4e8c\u884c</p>\n                </div>\n            </body>\n            </tt>'''.encode('utf-16')\n        srt_data = '''1\n00:00:00,000 --> 00:00:01,000\nLine 1\n\n2\n00:00:01,000 --> 00:00:02,000\n\u7b2c\u4e8c\u884c\n\n'''\n        self.assertEqual(dfxp2srt(dfxp_data_non_utf8), srt_data)\n\n    def test_cli_option(self):\n        self.assertEqual(cli_option({'proxy': '127.0.0.1:3128'}, '--proxy', 'proxy'), ['--proxy', '127.0.0.1:3128'])\n        self.assertEqual(cli_option({'proxy': None}, '--proxy', 'proxy'), [])\n        self.assertEqual(cli_option({}, '--proxy', 'proxy'), [])\n        self.assertEqual(cli_option({'retries': 10}, '--retries', 'retries'), ['--retries', '10'])\n\n    def test_cli_valueless_option(self):\n        self.assertEqual(cli_valueless_option(\n            {'downloader': 'external'}, '--external-downloader', 'downloader', 'external'), ['--external-downloader'])\n        self.assertEqual(cli_valueless_option(\n            {'downloader': 'internal'}, '--external-downloader', 'downloader', 'external'), [])\n        self.assertEqual(cli_valueless_option(\n            {'nocheckcertificate': True}, '--no-check-certificate', 'nocheckcertificate'), ['--no-check-certificate'])\n        self.assertEqual(cli_valueless_option(\n            {'nocheckcertificate': False}, '--no-check-certificate', 'nocheckcertificate'), [])\n        self.assertEqual(cli_valueless_option(\n            {'checkcertificate': True}, '--no-check-certificate', 'checkcertificate', False), [])\n        self.assertEqual(cli_valueless_option(\n            {'checkcertificate': False}, '--no-check-certificate', 'checkcertificate', False), ['--no-check-certificate'])\n\n    def test_cli_bool_option(self):\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': True}, '--no-check-certificate', 'nocheckcertificate'),\n            ['--no-check-certificate', 'true'])\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': True}, '--no-check-certificate', 'nocheckcertificate', separator='='),\n            ['--no-check-certificate=true'])\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': True}, '--check-certificate', 'nocheckcertificate', 'false', 'true'),\n            ['--check-certificate', 'false'])\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': True}, '--check-certificate', 'nocheckcertificate', 'false', 'true', '='),\n            ['--check-certificate=false'])\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': False}, '--check-certificate', 'nocheckcertificate', 'false', 'true'),\n            ['--check-certificate', 'true'])\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': False}, '--check-certificate', 'nocheckcertificate', 'false', 'true', '='),\n            ['--check-certificate=true'])\n        self.assertEqual(\n            cli_bool_option(\n                {}, '--check-certificate', 'nocheckcertificate', 'false', 'true', '='),\n            [])\n\n    def test_ohdave_rsa_encrypt(self):\n        N = 0xab86b6371b5318aaa1d3c9e612a9f1264f372323c8c0f19875b5fc3b3fd3afcc1e5bec527aa94bfa85bffc157e4245aebda05389a5357b75115ac94f074aefcd\n        e = 65537\n\n        self.assertEqual(\n            ohdave_rsa_encrypt(b'aa111222', e, N),\n            '726664bd9a23fd0c70f9f1b84aab5e3905ce1e45a584e9cbcf9bcc7510338fc1986d6c599ff990d923aa43c51c0d9013cd572e13bc58f4ae48f2ed8c0b0ba881')\n\n    def test_pkcs1pad(self):\n        data = [1, 2, 3]\n        padded_data = pkcs1pad(data, 32)\n        self.assertEqual(padded_data[:2], [0, 2])\n        self.assertEqual(padded_data[28:], [0, 1, 2, 3])\n\n        self.assertRaises(ValueError, pkcs1pad, data, 8)\n\n    def test_encode_base_n(self):\n        self.assertEqual(encode_base_n(0, 30), '0')\n        self.assertEqual(encode_base_n(80, 30), '2k')\n\n        custom_table = '9876543210ZYXWVUTSRQPONMLKJIHGFEDCBA'\n        self.assertEqual(encode_base_n(0, 30, custom_table), '9')\n        self.assertEqual(encode_base_n(80, 30, custom_table), '7P')\n\n        self.assertRaises(ValueError, encode_base_n, 0, 70)\n        self.assertRaises(ValueError, encode_base_n, 0, 60, custom_table)\n\n    def test_caesar(self):\n        self.assertEqual(caesar('ace', 'abcdef', 2), 'cea')\n        self.assertEqual(caesar('cea', 'abcdef', -2), 'ace')\n        self.assertEqual(caesar('ace', 'abcdef', -2), 'eac')\n        self.assertEqual(caesar('eac', 'abcdef', 2), 'ace')\n        self.assertEqual(caesar('ace', 'abcdef', 0), 'ace')\n        self.assertEqual(caesar('xyz', 'abcdef', 2), 'xyz')\n        self.assertEqual(caesar('abc', 'acegik', 2), 'ebg')\n        self.assertEqual(caesar('ebg', 'acegik', -2), 'abc')\n\n    def test_rot47(self):\n        self.assertEqual(rot47('yt-dlp'), r'JE\\5=A')\n        self.assertEqual(rot47('YT-DLP'), r'*%\\s{!')\n\n    def test_urshift(self):\n        self.assertEqual(urshift(3, 1), 1)\n        self.assertEqual(urshift(-3, 1), 2147483646)\n\n    GET_ELEMENT_BY_CLASS_TEST_STRING = '''\n        <span class=\"foo bar\">nice</span>\n    '''\n\n    def test_get_element_by_class(self):\n        html = self.GET_ELEMENT_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_element_by_class('foo', html), 'nice')\n        self.assertEqual(get_element_by_class('no-such-class', html), None)\n\n    def test_get_element_html_by_class(self):\n        html = self.GET_ELEMENT_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_element_html_by_class('foo', html), html.strip())\n        self.assertEqual(get_element_by_class('no-such-class', html), None)\n\n    GET_ELEMENT_BY_ATTRIBUTE_TEST_STRING = '''\n        <div itemprop=\"author\" itemscope>foo</div>\n    '''\n\n    def test_get_element_by_attribute(self):\n        html = self.GET_ELEMENT_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_element_by_attribute('class', 'foo bar', html), 'nice')\n        self.assertEqual(get_element_by_attribute('class', 'foo', html), None)\n        self.assertEqual(get_element_by_attribute('class', 'no-such-foo', html), None)\n\n        html = self.GET_ELEMENT_BY_ATTRIBUTE_TEST_STRING\n\n        self.assertEqual(get_element_by_attribute('itemprop', 'author', html), 'foo')\n\n    def test_get_element_html_by_attribute(self):\n        html = self.GET_ELEMENT_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_element_html_by_attribute('class', 'foo bar', html), html.strip())\n        self.assertEqual(get_element_html_by_attribute('class', 'foo', html), None)\n        self.assertEqual(get_element_html_by_attribute('class', 'no-such-foo', html), None)\n\n        html = self.GET_ELEMENT_BY_ATTRIBUTE_TEST_STRING\n\n        self.assertEqual(get_element_html_by_attribute('itemprop', 'author', html), html.strip())\n\n    GET_ELEMENTS_BY_CLASS_TEST_STRING = '''\n        <span class=\"foo bar\">nice</span><span class=\"foo bar\">also nice</span>\n    '''\n    GET_ELEMENTS_BY_CLASS_RES = ['<span class=\"foo bar\">nice</span>', '<span class=\"foo bar\">also nice</span>']\n\n    def test_get_elements_by_class(self):\n        html = self.GET_ELEMENTS_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_elements_by_class('foo', html), ['nice', 'also nice'])\n        self.assertEqual(get_elements_by_class('no-such-class', html), [])\n\n    def test_get_elements_html_by_class(self):\n        html = self.GET_ELEMENTS_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_elements_html_by_class('foo', html), self.GET_ELEMENTS_BY_CLASS_RES)\n        self.assertEqual(get_elements_html_by_class('no-such-class', html), [])\n\n    def test_get_elements_by_attribute(self):\n        html = self.GET_ELEMENTS_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_elements_by_attribute('class', 'foo bar', html), ['nice', 'also nice'])\n        self.assertEqual(get_elements_by_attribute('class', 'foo', html), [])\n        self.assertEqual(get_elements_by_attribute('class', 'no-such-foo', html), [])\n\n    def test_get_elements_html_by_attribute(self):\n        html = self.GET_ELEMENTS_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_elements_html_by_attribute('class', 'foo bar', html), self.GET_ELEMENTS_BY_CLASS_RES)\n        self.assertEqual(get_elements_html_by_attribute('class', 'foo', html), [])\n        self.assertEqual(get_elements_html_by_attribute('class', 'no-such-foo', html), [])\n\n    def test_get_elements_text_and_html_by_attribute(self):\n        html = self.GET_ELEMENTS_BY_CLASS_TEST_STRING\n\n        self.assertEqual(\n            list(get_elements_text_and_html_by_attribute('class', 'foo bar', html)),\n            list(zip(['nice', 'also nice'], self.GET_ELEMENTS_BY_CLASS_RES)))\n        self.assertEqual(list(get_elements_text_and_html_by_attribute('class', 'foo', html)), [])\n        self.assertEqual(list(get_elements_text_and_html_by_attribute('class', 'no-such-foo', html)), [])\n\n        self.assertEqual(list(get_elements_text_and_html_by_attribute(\n            'class', 'foo', '<a class=\"foo\">nice</a><span class=\"foo\">nice</span>', tag='a')), [('nice', '<a class=\"foo\">nice</a>')])\n\n    GET_ELEMENT_BY_TAG_TEST_STRING = '''\n    random text lorem ipsum</p>\n    <div>\n        this should be returned\n        <span>this should also be returned</span>\n        <div>\n            this should also be returned\n        </div>\n        closing tag above should not trick, so this should also be returned\n    </div>\n    but this text should not be returned\n    '''\n    GET_ELEMENT_BY_TAG_RES_OUTERDIV_HTML = GET_ELEMENT_BY_TAG_TEST_STRING.strip()[32:276]\n    GET_ELEMENT_BY_TAG_RES_OUTERDIV_TEXT = GET_ELEMENT_BY_TAG_RES_OUTERDIV_HTML[5:-6]\n    GET_ELEMENT_BY_TAG_RES_INNERSPAN_HTML = GET_ELEMENT_BY_TAG_TEST_STRING.strip()[78:119]\n    GET_ELEMENT_BY_TAG_RES_INNERSPAN_TEXT = GET_ELEMENT_BY_TAG_RES_INNERSPAN_HTML[6:-7]\n\n    def test_get_element_text_and_html_by_tag(self):\n        html = self.GET_ELEMENT_BY_TAG_TEST_STRING\n\n        self.assertEqual(\n            get_element_text_and_html_by_tag('div', html),\n            (self.GET_ELEMENT_BY_TAG_RES_OUTERDIV_TEXT, self.GET_ELEMENT_BY_TAG_RES_OUTERDIV_HTML))\n        self.assertEqual(\n            get_element_text_and_html_by_tag('span', html),\n            (self.GET_ELEMENT_BY_TAG_RES_INNERSPAN_TEXT, self.GET_ELEMENT_BY_TAG_RES_INNERSPAN_HTML))\n        self.assertRaises(compat_HTMLParseError, get_element_text_and_html_by_tag, 'article', html)\n\n    def test_iri_to_uri(self):\n        self.assertEqual(\n            iri_to_uri('https://www.google.com/search?q=foo&ie=utf-8&oe=utf-8&client=firefox-b'),\n            'https://www.google.com/search?q=foo&ie=utf-8&oe=utf-8&client=firefox-b')  # Same\n        self.assertEqual(\n            iri_to_uri('https://www.google.com/search?q=K\u00e4seso\u00dfenr\u00fchrl\u00f6ffel'),  # German for cheese sauce stirring spoon\n            'https://www.google.com/search?q=K%C3%A4seso%C3%9Fenr%C3%BChrl%C3%B6ffel')\n        self.assertEqual(\n            iri_to_uri('https://www.google.com/search?q=lt<+gt>+eq%3D+amp%26+percent%25+hash%23+colon%3A+tilde~#trash=?&garbage=#'),\n            'https://www.google.com/search?q=lt%3C+gt%3E+eq%3D+amp%26+percent%25+hash%23+colon%3A+tilde~#trash=?&garbage=#')\n        self.assertEqual(\n            iri_to_uri('http://\u043f\u0440\u0430\u0432\u043e\u0437\u0430\u0449\u0438\u0442\u043038.\u0440\u0444/category/news/'),\n            'http://xn--38-6kcaak9aj5chl4a3g.xn--p1ai/category/news/')\n        self.assertEqual(\n            iri_to_uri('http://www.\u043f\u0440\u0430\u0432\u043e\u0437\u0430\u0449\u0438\u0442\u043038.\u0440\u0444/category/news/'),\n            'http://www.xn--38-6kcaak9aj5chl4a3g.xn--p1ai/category/news/')\n        self.assertEqual(\n            iri_to_uri('https://i\u2764.ws/emojidomain/\ud83d\udc4d\ud83d\udc4f\ud83e\udd1d\ud83d\udcaa'),\n            'https://xn--i-7iq.ws/emojidomain/%F0%9F%91%8D%F0%9F%91%8F%F0%9F%A4%9D%F0%9F%92%AA')\n        self.assertEqual(\n            iri_to_uri('http://\u65e5\u672c\u8a9e.jp/'),\n            'http://xn--wgv71a119e.jp/')\n        self.assertEqual(\n            iri_to_uri('http://\u5bfc\u822a.\u4e2d\u56fd/'),\n            'http://xn--fet810g.xn--fiqs8s/')\n\n    def test_clean_podcast_url(self):\n        self.assertEqual(clean_podcast_url('https://www.podtrac.com/pts/redirect.mp3/chtbl.com/track/5899E/traffic.megaphone.fm/HSW7835899191.mp3'), 'https://traffic.megaphone.fm/HSW7835899191.mp3')\n        self.assertEqual(clean_podcast_url('https://play.podtrac.com/npr-344098539/edge1.pod.npr.org/anon.npr-podcasts/podcast/npr/waitwait/2020/10/20201003_waitwait_wwdtmpodcast201003-015621a5-f035-4eca-a9a1-7c118d90bc3c.mp3'), 'https://edge1.pod.npr.org/anon.npr-podcasts/podcast/npr/waitwait/2020/10/20201003_waitwait_wwdtmpodcast201003-015621a5-f035-4eca-a9a1-7c118d90bc3c.mp3')\n        self.assertEqual(clean_podcast_url('https://pdst.fm/e/2.gum.fm/chtbl.com/track/chrt.fm/track/34D33/pscrb.fm/rss/p/traffic.megaphone.fm/ITLLC7765286967.mp3?updated=1687282661'), 'https://traffic.megaphone.fm/ITLLC7765286967.mp3?updated=1687282661')\n        self.assertEqual(clean_podcast_url('https://pdst.fm/e/https://mgln.ai/e/441/www.buzzsprout.com/1121972/13019085-ep-252-the-deep-life-stack.mp3'), 'https://www.buzzsprout.com/1121972/13019085-ep-252-the-deep-life-stack.mp3')\n\n    def test_LazyList(self):\n        it = list(range(10))\n\n        self.assertEqual(list(LazyList(it)), it)\n        self.assertEqual(LazyList(it).exhaust(), it)\n        self.assertEqual(LazyList(it)[5], it[5])\n\n        self.assertEqual(LazyList(it)[5:], it[5:])\n        self.assertEqual(LazyList(it)[:5], it[:5])\n        self.assertEqual(LazyList(it)[::2], it[::2])\n        self.assertEqual(LazyList(it)[1::2], it[1::2])\n        self.assertEqual(LazyList(it)[5::-1], it[5::-1])\n        self.assertEqual(LazyList(it)[6:2:-2], it[6:2:-2])\n        self.assertEqual(LazyList(it)[::-1], it[::-1])\n\n        self.assertTrue(LazyList(it))\n        self.assertFalse(LazyList(range(0)))\n        self.assertEqual(len(LazyList(it)), len(it))\n        self.assertEqual(repr(LazyList(it)), repr(it))\n        self.assertEqual(str(LazyList(it)), str(it))\n\n        self.assertEqual(list(LazyList(it, reverse=True)), it[::-1])\n        self.assertEqual(list(reversed(LazyList(it))[::-1]), it)\n        self.assertEqual(list(reversed(LazyList(it))[1:3:7]), it[::-1][1:3:7])\n\n    def test_LazyList_laziness(self):\n\n        def test(ll, idx, val, cache):\n            self.assertEqual(ll[idx], val)\n            self.assertEqual(ll._cache, list(cache))\n\n        ll = LazyList(range(10))\n        test(ll, 0, 0, range(1))\n        test(ll, 5, 5, range(6))\n        test(ll, -3, 7, range(10))\n\n        ll = LazyList(range(10), reverse=True)\n        test(ll, -1, 0, range(1))\n        test(ll, 3, 6, range(10))\n\n        ll = LazyList(itertools.count())\n        test(ll, 10, 10, range(11))\n        ll = reversed(ll)\n        test(ll, -15, 14, range(15))\n\n    def test_format_bytes(self):\n        self.assertEqual(format_bytes(0), '0.00B')\n        self.assertEqual(format_bytes(1000), '1000.00B')\n        self.assertEqual(format_bytes(1024), '1.00KiB')\n        self.assertEqual(format_bytes(1024**2), '1.00MiB')\n        self.assertEqual(format_bytes(1024**3), '1.00GiB')\n        self.assertEqual(format_bytes(1024**4), '1.00TiB')\n        self.assertEqual(format_bytes(1024**5), '1.00PiB')\n        self.assertEqual(format_bytes(1024**6), '1.00EiB')\n        self.assertEqual(format_bytes(1024**7), '1.00ZiB')\n        self.assertEqual(format_bytes(1024**8), '1.00YiB')\n        self.assertEqual(format_bytes(1024**9), '1024.00YiB')\n\n    def test_hide_login_info(self):\n        self.assertEqual(Config.hide_login_info(['-u', 'foo', '-p', 'bar']),\n                         ['-u', 'PRIVATE', '-p', 'PRIVATE'])\n        self.assertEqual(Config.hide_login_info(['-u']), ['-u'])\n        self.assertEqual(Config.hide_login_info(['-u', 'foo', '-u', 'bar']),\n                         ['-u', 'PRIVATE', '-u', 'PRIVATE'])\n        self.assertEqual(Config.hide_login_info(['--username=foo']),\n                         ['--username=PRIVATE'])\n\n    def test_locked_file(self):\n        TEXT = 'test_locked_file\\n'\n        FILE = 'test_locked_file.ytdl'\n        MODES = 'war'  # Order is important\n\n        try:\n            for lock_mode in MODES:\n                with locked_file(FILE, lock_mode, False) as f:\n                    if lock_mode == 'r':\n                        self.assertEqual(f.read(), TEXT * 2, 'Wrong file content')\n                    else:\n                        f.write(TEXT)\n                    for test_mode in MODES:\n                        testing_write = test_mode != 'r'\n                        try:\n                            with locked_file(FILE, test_mode, False):\n                                pass\n                        except (BlockingIOError, PermissionError):\n                            if not testing_write:  # FIXME\n                                print(f'Known issue: Exclusive lock ({lock_mode}) blocks read access ({test_mode})')\n                                continue\n                            self.assertTrue(testing_write, f'{test_mode} is blocked by {lock_mode}')\n                        else:\n                            self.assertFalse(testing_write, f'{test_mode} is not blocked by {lock_mode}')\n        finally:\n            with contextlib.suppress(OSError):\n                os.remove(FILE)\n\n    def test_determine_file_encoding(self):\n        self.assertEqual(determine_file_encoding(b''), (None, 0))\n        self.assertEqual(determine_file_encoding(b'--verbose -x --audio-format mkv\\n'), (None, 0))\n\n        self.assertEqual(determine_file_encoding(b'\\xef\\xbb\\xbf'), ('utf-8', 3))\n        self.assertEqual(determine_file_encoding(b'\\x00\\x00\\xfe\\xff'), ('utf-32-be', 4))\n        self.assertEqual(determine_file_encoding(b'\\xff\\xfe'), ('utf-16-le', 2))\n\n        self.assertEqual(determine_file_encoding(b'\\xff\\xfe# coding: utf-8\\n--verbose'), ('utf-16-le', 2))\n\n        self.assertEqual(determine_file_encoding(b'# coding: utf-8\\n--verbose'), ('utf-8', 0))\n        self.assertEqual(determine_file_encoding(b'# coding: someencodinghere-12345\\n--verbose'), ('someencodinghere-12345', 0))\n\n        self.assertEqual(determine_file_encoding(b'#coding:utf-8\\n--verbose'), ('utf-8', 0))\n        self.assertEqual(determine_file_encoding(b'#  coding:   utf-8   \\r\\n--verbose'), ('utf-8', 0))\n\n        self.assertEqual(determine_file_encoding('# coding: utf-32-be'.encode('utf-32-be')), ('utf-32-be', 0))\n        self.assertEqual(determine_file_encoding('# coding: utf-16-le'.encode('utf-16-le')), ('utf-16-le', 0))\n\n    def test_get_compatible_ext(self):\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None, None], vexts=['mp4'], aexts=['m4a', 'm4a']), 'mkv')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['flv'], aexts=['flv']), 'flv')\n\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['mp4'], aexts=['m4a']), 'mp4')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['mp4'], aexts=['webm']), 'mkv')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['webm'], aexts=['m4a']), 'mkv')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['webm'], aexts=['webm']), 'webm')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['webm'], aexts=['weba']), 'webm')\n\n        self.assertEqual(get_compatible_ext(\n            vcodecs=['h264'], acodecs=['mp4a'], vexts=['mov'], aexts=['m4a']), 'mp4')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=['av01.0.12M.08'], acodecs=['opus'], vexts=['mp4'], aexts=['webm']), 'webm')\n\n        self.assertEqual(get_compatible_ext(\n            vcodecs=['vp9'], acodecs=['opus'], vexts=['webm'], aexts=['webm'], preferences=['flv', 'mp4']), 'mp4')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=['av1'], acodecs=['mp4a'], vexts=['webm'], aexts=['m4a'], preferences=('webm', 'mkv')), 'mkv')\n\n    def test_try_call(self):\n        def total(*x, **kwargs):\n            return sum(x) + sum(kwargs.values())\n\n        self.assertEqual(try_call(None), None,\n                         msg='not a fn should give None')\n        self.assertEqual(try_call(lambda: 1), 1,\n                         msg='int fn with no expected_type should give int')\n        self.assertEqual(try_call(lambda: 1, expected_type=int), 1,\n                         msg='int fn with expected_type int should give int')\n        self.assertEqual(try_call(lambda: 1, expected_type=dict), None,\n                         msg='int fn with wrong expected_type should give None')\n        self.assertEqual(try_call(total, args=(0, 1, 0, ), expected_type=int), 1,\n                         msg='fn should accept arglist')\n        self.assertEqual(try_call(total, kwargs={'a': 0, 'b': 1, 'c': 0}, expected_type=int), 1,\n                         msg='fn should accept kwargs')\n        self.assertEqual(try_call(lambda: 1, expected_type=dict), None,\n                         msg='int fn with no expected_type should give None')\n        self.assertEqual(try_call(lambda x: {}, total, args=(42, ), expected_type=int), 42,\n                         msg='expect first int result with expected_type int')\n\n    def test_variadic(self):\n        self.assertEqual(variadic(None), (None, ))\n        self.assertEqual(variadic('spam'), ('spam', ))\n        self.assertEqual(variadic('spam', allowed_types=dict), 'spam')\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            self.assertEqual(variadic('spam', allowed_types=[dict]), 'spam')\n\n    def test_traverse_obj(self):\n        _TEST_DATA = {\n            100: 100,\n            1.2: 1.2,\n            'str': 'str',\n            'None': None,\n            '...': ...,\n            'urls': [\n                {'index': 0, 'url': 'https://www.example.com/0'},\n                {'index': 1, 'url': 'https://www.example.com/1'},\n            ],\n            'data': (\n                {'index': 2},\n                {'index': 3},\n            ),\n            'dict': {},\n        }\n\n        # Test base functionality\n        self.assertEqual(traverse_obj(_TEST_DATA, ('str',)), 'str',\n                         msg='allow tuple path')\n        self.assertEqual(traverse_obj(_TEST_DATA, ['str']), 'str',\n                         msg='allow list path')\n        self.assertEqual(traverse_obj(_TEST_DATA, (value for value in (\"str\",))), 'str',\n                         msg='allow iterable path')\n        self.assertEqual(traverse_obj(_TEST_DATA, 'str'), 'str',\n                         msg='single items should be treated as a path')\n        self.assertEqual(traverse_obj(_TEST_DATA, None), _TEST_DATA)\n        self.assertEqual(traverse_obj(_TEST_DATA, 100), 100)\n        self.assertEqual(traverse_obj(_TEST_DATA, 1.2), 1.2)\n\n        # Test Ellipsis behavior\n        self.assertCountEqual(traverse_obj(_TEST_DATA, ...),\n                              (item for item in _TEST_DATA.values() if item not in (None, {})),\n                              msg='`...` should give all non discarded values')\n        self.assertCountEqual(traverse_obj(_TEST_DATA, ('urls', 0, ...)), _TEST_DATA['urls'][0].values(),\n                              msg='`...` selection for dicts should select all values')\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., ..., 'url')),\n                         ['https://www.example.com/0', 'https://www.example.com/1'],\n                         msg='nested `...` queries should work')\n        self.assertCountEqual(traverse_obj(_TEST_DATA, (..., ..., 'index')), range(4),\n                              msg='`...` query result should be flattened')\n        self.assertEqual(traverse_obj(iter(range(4)), ...), list(range(4)),\n                         msg='`...` should accept iterables')\n\n        # Test function as key\n        self.assertEqual(traverse_obj(_TEST_DATA, lambda x, y: x == 'urls' and isinstance(y, list)),\n                         [_TEST_DATA['urls']],\n                         msg='function as query key should perform a filter based on (key, value)')\n        self.assertCountEqual(traverse_obj(_TEST_DATA, lambda _, x: isinstance(x[0], str)), {'str'},\n                              msg='exceptions in the query function should be catched')\n        self.assertEqual(traverse_obj(iter(range(4)), lambda _, x: x % 2 == 0), [0, 2],\n                         msg='function key should accept iterables')\n        if __debug__:\n            with self.assertRaises(Exception, msg='Wrong function signature should raise in debug'):\n                traverse_obj(_TEST_DATA, lambda a: ...)\n            with self.assertRaises(Exception, msg='Wrong function signature should raise in debug'):\n                traverse_obj(_TEST_DATA, lambda a, b, c: ...)\n\n        # Test set as key (transformation/type, like `expected_type`)\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., {str.upper}, )), ['STR'],\n                         msg='Function in set should be a transformation')\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., {str})), ['str'],\n                         msg='Type in set should be a type filter')\n        self.assertEqual(traverse_obj(_TEST_DATA, {dict}), _TEST_DATA,\n                         msg='A single set should be wrapped into a path')\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., {str.upper})), ['STR'],\n                         msg='Transformation function should not raise')\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., {str_or_none})),\n                         [item for item in map(str_or_none, _TEST_DATA.values()) if item is not None],\n                         msg='Function in set should be a transformation')\n        if __debug__:\n            with self.assertRaises(Exception, msg='Sets with length != 1 should raise in debug'):\n                traverse_obj(_TEST_DATA, set())\n            with self.assertRaises(Exception, msg='Sets with length != 1 should raise in debug'):\n                traverse_obj(_TEST_DATA, {str.upper, str})\n\n        # Test `slice` as a key\n        _SLICE_DATA = [0, 1, 2, 3, 4]\n        self.assertEqual(traverse_obj(_TEST_DATA, ('dict', slice(1))), None,\n                         msg='slice on a dictionary should not throw')\n        self.assertEqual(traverse_obj(_SLICE_DATA, slice(1)), _SLICE_DATA[:1],\n                         msg='slice key should apply slice to sequence')\n        self.assertEqual(traverse_obj(_SLICE_DATA, slice(1, 2)), _SLICE_DATA[1:2],\n                         msg='slice key should apply slice to sequence')\n        self.assertEqual(traverse_obj(_SLICE_DATA, slice(1, 4, 2)), _SLICE_DATA[1:4:2],\n                         msg='slice key should apply slice to sequence')\n\n        # Test alternative paths\n        self.assertEqual(traverse_obj(_TEST_DATA, 'fail', 'str'), 'str',\n                         msg='multiple `paths` should be treated as alternative paths')\n        self.assertEqual(traverse_obj(_TEST_DATA, 'str', 100), 'str',\n                         msg='alternatives should exit early')\n        self.assertEqual(traverse_obj(_TEST_DATA, 'fail', 'fail'), None,\n                         msg='alternatives should return `default` if exhausted')\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., 'fail'), 100), 100,\n                         msg='alternatives should track their own branching return')\n        self.assertEqual(traverse_obj(_TEST_DATA, ('dict', ...), ('data', ...)), list(_TEST_DATA['data']),\n                         msg='alternatives on empty objects should search further')\n\n        # Test branch and path nesting\n        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', (3, 0), 'url')), ['https://www.example.com/0'],\n                         msg='tuple as key should be treated as branches')\n        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', [3, 0], 'url')), ['https://www.example.com/0'],\n                         msg='list as key should be treated as branches')\n        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', ((1, 'fail'), (0, 'url')))), ['https://www.example.com/0'],\n                         msg='double nesting in path should be treated as paths')\n        self.assertEqual(traverse_obj(['0', [1, 2]], [(0, 1), 0]), [1],\n                         msg='do not fail early on branching')\n        self.assertCountEqual(traverse_obj(_TEST_DATA, ('urls', ((1, ('fail', 'url')), (0, 'url')))),\n                              ['https://www.example.com/0', 'https://www.example.com/1'],\n                              msg='tripple nesting in path should be treated as branches')\n        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', ('fail', (..., 'url')))),\n                         ['https://www.example.com/0', 'https://www.example.com/1'],\n                         msg='ellipsis as branch path start gets flattened')\n\n        # Test dictionary as key\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 100, 1: 1.2}), {0: 100, 1: 1.2},\n                         msg='dict key should result in a dict with the same keys')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', 0, 'url')}),\n                         {0: 'https://www.example.com/0'},\n                         msg='dict key should allow paths')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', (3, 0), 'url')}),\n                         {0: ['https://www.example.com/0']},\n                         msg='tuple in dict path should be treated as branches')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', ((1, 'fail'), (0, 'url')))}),\n                         {0: ['https://www.example.com/0']},\n                         msg='double nesting in dict path should be treated as paths')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', ((1, ('fail', 'url')), (0, 'url')))}),\n                         {0: ['https://www.example.com/1', 'https://www.example.com/0']},\n                         msg='tripple nesting in dict path should be treated as branches')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'fail'}), {},\n                         msg='remove `None` values when top level dict key fails')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'fail'}, default=...), {0: ...},\n                         msg='use `default` if key fails and `default`')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'dict'}), {},\n                         msg='remove empty values when dict key')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'dict'}, default=...), {0: ...},\n                         msg='use `default` when dict key and `default`')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: {0: 'fail'}}), {},\n                         msg='remove empty values when nested dict key fails')\n        self.assertEqual(traverse_obj(None, {0: 'fail'}), {},\n                         msg='default to dict if pruned')\n        self.assertEqual(traverse_obj(None, {0: 'fail'}, default=...), {0: ...},\n                         msg='default to dict if pruned and default is given')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: {0: 'fail'}}, default=...), {0: {0: ...}},\n                         msg='use nested `default` when nested dict key fails and `default`')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('dict', ...)}), {},\n                         msg='remove key if branch in dict key not successful')\n\n        # Testing default parameter behavior\n        _DEFAULT_DATA = {'None': None, 'int': 0, 'list': []}\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'fail'), None,\n                         msg='default value should be `None`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'fail', 'fail', default=...), ...,\n                         msg='chained fails should result in default')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'None', 'int'), 0,\n                         msg='should not short cirquit on `None`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'fail', default=1), 1,\n                         msg='invalid dict key should result in `default`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'None', default=1), 1,\n                         msg='`None` is a deliberate sentinel and should become `default`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, ('list', 10)), None,\n                         msg='`IndexError` should result in `default`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, (..., 'fail'), default=1), 1,\n                         msg='if branched but not successful return `default` if defined, not `[]`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, (..., 'fail'), default=None), None,\n                         msg='if branched but not successful return `default` even if `default` is `None`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, (..., 'fail')), [],\n                         msg='if branched but not successful return `[]`, not `default`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, ('list', ...)), [],\n                         msg='if branched but object is empty return `[]`, not `default`')\n        self.assertEqual(traverse_obj(None, ...), [],\n                         msg='if branched but object is `None` return `[]`, not `default`')\n        self.assertEqual(traverse_obj({0: None}, (0, ...)), [],\n                         msg='if branched but state is `None` return `[]`, not `default`')\n\n        branching_paths = [\n            ('fail', ...),\n            (..., 'fail'),\n            100 * ('fail',) + (...,),\n            (...,) + 100 * ('fail',),\n        ]\n        for branching_path in branching_paths:\n            self.assertEqual(traverse_obj({}, branching_path), [],\n                             msg='if branched but state is `None`, return `[]` (not `default`)')\n            self.assertEqual(traverse_obj({}, 'fail', branching_path), [],\n                             msg='if branching in last alternative and previous did not match, return `[]` (not `default`)')\n            self.assertEqual(traverse_obj({0: 'x'}, 0, branching_path), 'x',\n                             msg='if branching in last alternative and previous did match, return single value')\n            self.assertEqual(traverse_obj({0: 'x'}, branching_path, 0), 'x',\n                             msg='if branching in first alternative and non-branching path does match, return single value')\n            self.assertEqual(traverse_obj({}, branching_path, 'fail'), None,\n                             msg='if branching in first alternative and non-branching path does not match, return `default`')\n\n        # Testing expected_type behavior\n        _EXPECTED_TYPE_DATA = {'str': 'str', 'int': 0}\n        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=str),\n                         'str', msg='accept matching `expected_type` type')\n        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=int),\n                         None, msg='reject non matching `expected_type` type')\n        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'int', expected_type=lambda x: str(x)),\n                         '0', msg='transform type using type function')\n        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=lambda _: 1 / 0),\n                         None, msg='wrap expected_type fuction in try_call')\n        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, ..., expected_type=str),\n                         ['str'], msg='eliminate items that expected_type fails on')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 100, 1: 1.2}, expected_type=int),\n                         {0: 100}, msg='type as expected_type should filter dict values')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 100, 1: 1.2, 2: 'None'}, expected_type=str_or_none),\n                         {0: '100', 1: '1.2'}, msg='function as expected_type should transform dict values')\n        self.assertEqual(traverse_obj(_TEST_DATA, ({0: 1.2}, 0, {int_or_none}), expected_type=int),\n                         1, msg='expected_type should not filter non final dict values')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: {0: 100, 1: 'str'}}, expected_type=int),\n                         {0: {0: 100}}, msg='expected_type should transform deep dict values')\n        self.assertEqual(traverse_obj(_TEST_DATA, [({0: '...'}, {0: '...'})], expected_type=type(...)),\n                         [{0: ...}, {0: ...}], msg='expected_type should transform branched dict values')\n        self.assertEqual(traverse_obj({1: {3: 4}}, [(1, 2), 3], expected_type=int),\n                         [4], msg='expected_type regression for type matching in tuple branching')\n        self.assertEqual(traverse_obj(_TEST_DATA, ['data', ...], expected_type=int),\n                         [], msg='expected_type regression for type matching in dict result')\n\n        # Test get_all behavior\n        _GET_ALL_DATA = {'key': [0, 1, 2]}\n        self.assertEqual(traverse_obj(_GET_ALL_DATA, ('key', ...), get_all=False), 0,\n                         msg='if not `get_all`, return only first matching value')\n        self.assertEqual(traverse_obj(_GET_ALL_DATA, ..., get_all=False), [0, 1, 2],\n                         msg='do not overflatten if not `get_all`')\n\n        # Test casesense behavior\n        _CASESENSE_DATA = {\n            'KeY': 'value0',\n            0: {\n                'KeY': 'value1',\n                0: {'KeY': 'value2'},\n            },\n        }\n        self.assertEqual(traverse_obj(_CASESENSE_DATA, 'key'), None,\n                         msg='dict keys should be case sensitive unless `casesense`')\n        self.assertEqual(traverse_obj(_CASESENSE_DATA, 'keY',\n                                      casesense=False), 'value0',\n                         msg='allow non matching key case if `casesense`')\n        self.assertEqual(traverse_obj(_CASESENSE_DATA, (0, ('keY',)),\n                                      casesense=False), ['value1'],\n                         msg='allow non matching key case in branch if `casesense`')\n        self.assertEqual(traverse_obj(_CASESENSE_DATA, (0, ((0, 'keY'),)),\n                                      casesense=False), ['value2'],\n                         msg='allow non matching key case in branch path if `casesense`')\n\n        # Test traverse_string behavior\n        _TRAVERSE_STRING_DATA = {'str': 'str', 1.2: 1.2}\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', 0)), None,\n                         msg='do not traverse into string if not `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', 0),\n                                      traverse_string=True), 's',\n                         msg='traverse into string if `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, (1.2, 1),\n                                      traverse_string=True), '.',\n                         msg='traverse into converted data if `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', ...),\n                                      traverse_string=True), 'str',\n                         msg='`...` should result in string (same value) if `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', slice(0, None, 2)),\n                                      traverse_string=True), 'sr',\n                         msg='`slice` should result in string if `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', lambda i, v: i or v == \"s\"),\n                                      traverse_string=True), 'str',\n                         msg='function should result in string if `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', (0, 2)),\n                                      traverse_string=True), ['s', 'r'],\n                         msg='branching should result in list if `traverse_string`')\n        self.assertEqual(traverse_obj({}, (0, ...), traverse_string=True), [],\n                         msg='branching should result in list if `traverse_string`')\n        self.assertEqual(traverse_obj({}, (0, lambda x, y: True), traverse_string=True), [],\n                         msg='branching should result in list if `traverse_string`')\n        self.assertEqual(traverse_obj({}, (0, slice(1)), traverse_string=True), [],\n                         msg='branching should result in list if `traverse_string`')\n\n        # Test is_user_input behavior\n        _IS_USER_INPUT_DATA = {'range8': list(range(8))}\n        self.assertEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', '3'),\n                                      is_user_input=True), 3,\n                         msg='allow for string indexing if `is_user_input`')\n        self.assertCountEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', '3:'),\n                                           is_user_input=True), tuple(range(8))[3:],\n                              msg='allow for string slice if `is_user_input`')\n        self.assertCountEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', ':4:2'),\n                                           is_user_input=True), tuple(range(8))[:4:2],\n                              msg='allow step in string slice if `is_user_input`')\n        self.assertCountEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', ':'),\n                                           is_user_input=True), range(8),\n                              msg='`:` should be treated as `...` if `is_user_input`')\n        with self.assertRaises(TypeError, msg='too many params should result in error'):\n            traverse_obj(_IS_USER_INPUT_DATA, ('range8', ':::'), is_user_input=True)\n\n        # Test re.Match as input obj\n        mobj = re.fullmatch(r'0(12)(?P<group>3)(4)?', '0123')\n        self.assertEqual(traverse_obj(mobj, ...), [x for x in mobj.groups() if x is not None],\n                         msg='`...` on a `re.Match` should give its `groups()`')\n        self.assertEqual(traverse_obj(mobj, lambda k, _: k in (0, 2)), ['0123', '3'],\n                         msg='function on a `re.Match` should give groupno, value starting at 0')\n        self.assertEqual(traverse_obj(mobj, 'group'), '3',\n                         msg='str key on a `re.Match` should give group with that name')\n        self.assertEqual(traverse_obj(mobj, 2), '3',\n                         msg='int key on a `re.Match` should give group with that name')\n        self.assertEqual(traverse_obj(mobj, 'gRoUp', casesense=False), '3',\n                         msg='str key on a `re.Match` should respect casesense')\n        self.assertEqual(traverse_obj(mobj, 'fail'), None,\n                         msg='failing str key on a `re.Match` should return `default`')\n        self.assertEqual(traverse_obj(mobj, 'gRoUpS', casesense=False), None,\n                         msg='failing str key on a `re.Match` should return `default`')\n        self.assertEqual(traverse_obj(mobj, 8), None,\n                         msg='failing int key on a `re.Match` should return `default`')\n        self.assertEqual(traverse_obj(mobj, lambda k, _: k in (0, 'group')), ['0123', '3'],\n                         msg='function on a `re.Match` should give group name as well')\n\n    def test_http_header_dict(self):\n        headers = HTTPHeaderDict()\n        headers['ytdl-test'] = b'0'\n        self.assertEqual(list(headers.items()), [('Ytdl-Test', '0')])\n        headers['ytdl-test'] = 1\n        self.assertEqual(list(headers.items()), [('Ytdl-Test', '1')])\n        headers['Ytdl-test'] = '2'\n        self.assertEqual(list(headers.items()), [('Ytdl-Test', '2')])\n        self.assertTrue('ytDl-Test' in headers)\n        self.assertEqual(str(headers), str(dict(headers)))\n        self.assertEqual(repr(headers), str(dict(headers)))\n\n        headers.update({'X-dlp': 'data'})\n        self.assertEqual(set(headers.items()), {('Ytdl-Test', '2'), ('X-Dlp', 'data')})\n        self.assertEqual(dict(headers), {'Ytdl-Test': '2', 'X-Dlp': 'data'})\n        self.assertEqual(len(headers), 2)\n        self.assertEqual(headers.copy(), headers)\n        headers2 = HTTPHeaderDict({'X-dlp': 'data3'}, **headers, **{'X-dlp': 'data2'})\n        self.assertEqual(set(headers2.items()), {('Ytdl-Test', '2'), ('X-Dlp', 'data2')})\n        self.assertEqual(len(headers2), 2)\n        headers2.clear()\n        self.assertEqual(len(headers2), 0)\n\n        # ensure we prefer latter headers\n        headers3 = HTTPHeaderDict({'Ytdl-TeSt': 1}, {'Ytdl-test': 2})\n        self.assertEqual(set(headers3.items()), {('Ytdl-Test', '2')})\n        del headers3['ytdl-tesT']\n        self.assertEqual(dict(headers3), {})\n\n        headers4 = HTTPHeaderDict({'ytdl-test': 'data;'})\n        self.assertEqual(set(headers4.items()), {('Ytdl-Test', 'data;')})\n\n    def test_extract_basic_auth(self):\n        assert extract_basic_auth('http://:foo.bar') == ('http://:foo.bar', None)\n        assert extract_basic_auth('http://foo.bar') == ('http://foo.bar', None)\n        assert extract_basic_auth('http://@foo.bar') == ('http://foo.bar', 'Basic Og==')\n        assert extract_basic_auth('http://:pass@foo.bar') == ('http://foo.bar', 'Basic OnBhc3M=')\n        assert extract_basic_auth('http://user:@foo.bar') == ('http://foo.bar', 'Basic dXNlcjo=')\n        assert extract_basic_auth('http://user:pass@foo.bar') == ('http://foo.bar', 'Basic dXNlcjpwYXNz')\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "import os\nimport sys\nimport xml.etree.ElementTree as etree\n\nfrom .compat_utils import passthrough_module\n\npassthrough_module(__name__, '._deprecated')\ndel passthrough_module\n\n\n# HTMLParseError has been deprecated in Python 3.3 and removed in\n# Python 3.5. Introducing dummy exception for Python >3.5 for compatible\n# and uniform cross-version exception handling\nclass compat_HTMLParseError(ValueError):\n    pass\n\n\nclass _TreeBuilder(etree.TreeBuilder):\n    def doctype(self, name, pubid, system):\n        pass\n\n\ndef compat_etree_fromstring(text):\n    return etree.XML(text, parser=etree.XMLParser(target=_TreeBuilder()))\n\n\ncompat_os_name = os._name if os.name == 'java' else os.name\n\n\nif compat_os_name == 'nt':\n    def compat_shlex_quote(s):\n        import re\n        return s if re.match(r'^[-_\\w./]+$', s) else '\"%s\"' % s.replace('\"', '\\\\\"')\nelse:\n    from shlex import quote as compat_shlex_quote  # noqa: F401\n\n\ndef compat_ord(c):\n    return c if isinstance(c, int) else ord(c)\n\n\nif compat_os_name == 'nt' and sys.version_info < (3, 8):\n    # os.path.realpath on Windows does not follow symbolic links\n    # prior to Python 3.8 (see https://bugs.python.org/issue9949)\n    def compat_realpath(path):\n        while os.path.islink(path):\n            path = os.path.abspath(os.readlink(path))\n        return os.path.realpath(path)\nelse:\n    compat_realpath = os.path.realpath\n\n\n# Python 3.8+ does not honor %HOME% on windows, but this breaks compatibility with youtube-dl\n# See https://github.com/yt-dlp/yt-dlp/issues/792\n# https://docs.python.org/3/library/os.path.html#os.path.expanduser\nif compat_os_name in ('nt', 'ce'):\n    def compat_expanduser(path):\n        HOME = os.environ.get('HOME')\n        if not HOME:\n            return os.path.expanduser(path)\n        elif not path.startswith('~'):\n            return path\n        i = path.replace('\\\\', '/', 1).find('/')  # ~user\n        if i < 0:\n            i = len(path)\n        userhome = os.path.join(os.path.dirname(HOME), path[1:i]) if i > 1 else HOME\n        return userhome + path[i:]\nelse:\n    compat_expanduser = os.path.expanduser\n\n\ndef urllib_req_to_req(urllib_request):\n    \"\"\"Convert urllib Request to a networking Request\"\"\"\n    from ..networking import Request\n    from ..utils.networking import HTTPHeaderDict\n    return Request(\n        urllib_request.get_full_url(), data=urllib_request.data, method=urllib_request.get_method(),\n        headers=HTTPHeaderDict(urllib_request.headers, urllib_request.unredirected_hdrs),\n        extensions={'timeout': urllib_request.timeout} if hasattr(urllib_request, 'timeout') else None)\n", "import subprocess\n\nfrom .common import PostProcessor\nfrom ..compat import compat_shlex_quote\nfrom ..utils import PostProcessingError, encodeArgument, variadic\n\n\nclass ExecPP(PostProcessor):\n\n    def __init__(self, downloader, exec_cmd):\n        PostProcessor.__init__(self, downloader)\n        self.exec_cmd = variadic(exec_cmd)\n\n    def parse_cmd(self, cmd, info):\n        tmpl, tmpl_dict = self._downloader.prepare_outtmpl(cmd, info)\n        if tmpl_dict:  # if there are no replacements, tmpl_dict = {}\n            return self._downloader.escape_outtmpl(tmpl) % tmpl_dict\n\n        filepath = info.get('filepath', info.get('_filename'))\n        # If video, and no replacements are found, replace {} for backard compatibility\n        if filepath:\n            if '{}' not in cmd:\n                cmd += ' {}'\n            cmd = cmd.replace('{}', compat_shlex_quote(filepath))\n        return cmd\n\n    def run(self, info):\n        for tmpl in self.exec_cmd:\n            cmd = self.parse_cmd(tmpl, info)\n            self.to_screen('Executing command: %s' % cmd)\n            retCode = subprocess.call(encodeArgument(cmd), shell=True)\n            if retCode != 0:\n                raise PostProcessingError('Command returned error code %d' % retCode)\n        return [], info\n\n\n# Deprecated\nclass ExecAfterDownloadPP(ExecPP):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.deprecation_warning(\n            'yt_dlp.postprocessor.ExecAfterDownloadPP is deprecated '\n            'and may be removed in a future version. Use yt_dlp.postprocessor.ExecPP instead')\n", "import asyncio\nimport atexit\nimport base64\nimport binascii\nimport calendar\nimport codecs\nimport collections\nimport collections.abc\nimport contextlib\nimport datetime\nimport email.header\nimport email.utils\nimport errno\nimport hashlib\nimport hmac\nimport html.entities\nimport html.parser\nimport inspect\nimport io\nimport itertools\nimport json\nimport locale\nimport math\nimport mimetypes\nimport netrc\nimport operator\nimport os\nimport platform\nimport random\nimport re\nimport shlex\nimport socket\nimport ssl\nimport struct\nimport subprocess\nimport sys\nimport tempfile\nimport time\nimport traceback\nimport types\nimport unicodedata\nimport urllib.error\nimport urllib.parse\nimport urllib.request\nimport xml.etree.ElementTree\n\nfrom . import traversal\n\nfrom ..compat import functools  # isort: split\nfrom ..compat import (\n    compat_etree_fromstring,\n    compat_expanduser,\n    compat_HTMLParseError,\n    compat_os_name,\n    compat_shlex_quote,\n)\nfrom ..dependencies import websockets, xattr\n\n__name__ = __name__.rsplit('.', 1)[0]  # Pretend to be the parent module\n\n# This is not clearly defined otherwise\ncompiled_regex_type = type(re.compile(''))\n\n\nclass NO_DEFAULT:\n    pass\n\n\ndef IDENTITY(x):\n    return x\n\n\nENGLISH_MONTH_NAMES = [\n    'January', 'February', 'March', 'April', 'May', 'June',\n    'July', 'August', 'September', 'October', 'November', 'December']\n\nMONTH_NAMES = {\n    'en': ENGLISH_MONTH_NAMES,\n    'fr': [\n        'janvier', 'f\u00e9vrier', 'mars', 'avril', 'mai', 'juin',\n        'juillet', 'ao\u00fbt', 'septembre', 'octobre', 'novembre', 'd\u00e9cembre'],\n    # these follow the genitive grammatical case (dope\u0142niacz)\n    # some websites might be using nominative, which will require another month list\n    # https://en.wikibooks.org/wiki/Polish/Noun_cases\n    'pl': ['stycznia', 'lutego', 'marca', 'kwietnia', 'maja', 'czerwca',\n           'lipca', 'sierpnia', 'wrze\u015bnia', 'pa\u017adziernika', 'listopada', 'grudnia'],\n}\n\n# From https://github.com/python/cpython/blob/3.11/Lib/email/_parseaddr.py#L36-L42\nTIMEZONE_NAMES = {\n    'UT': 0, 'UTC': 0, 'GMT': 0, 'Z': 0,\n    'AST': -4, 'ADT': -3,  # Atlantic (used in Canada)\n    'EST': -5, 'EDT': -4,  # Eastern\n    'CST': -6, 'CDT': -5,  # Central\n    'MST': -7, 'MDT': -6,  # Mountain\n    'PST': -8, 'PDT': -7   # Pacific\n}\n\n# needed for sanitizing filenames in restricted mode\nACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',\n                        itertools.chain('AAAAAA', ['AE'], 'CEEEEIIIIDNOOOOOOO', ['OE'], 'UUUUUY', ['TH', 'ss'],\n                                        'aaaaaa', ['ae'], 'ceeeeiiiionooooooo', ['oe'], 'uuuuuy', ['th'], 'y')))\n\nDATE_FORMATS = (\n    '%d %B %Y',\n    '%d %b %Y',\n    '%B %d %Y',\n    '%B %dst %Y',\n    '%B %dnd %Y',\n    '%B %drd %Y',\n    '%B %dth %Y',\n    '%b %d %Y',\n    '%b %dst %Y',\n    '%b %dnd %Y',\n    '%b %drd %Y',\n    '%b %dth %Y',\n    '%b %dst %Y %I:%M',\n    '%b %dnd %Y %I:%M',\n    '%b %drd %Y %I:%M',\n    '%b %dth %Y %I:%M',\n    '%Y %m %d',\n    '%Y-%m-%d',\n    '%Y.%m.%d.',\n    '%Y/%m/%d',\n    '%Y/%m/%d %H:%M',\n    '%Y/%m/%d %H:%M:%S',\n    '%Y%m%d%H%M',\n    '%Y%m%d%H%M%S',\n    '%Y%m%d',\n    '%Y-%m-%d %H:%M',\n    '%Y-%m-%d %H:%M:%S',\n    '%Y-%m-%d %H:%M:%S.%f',\n    '%Y-%m-%d %H:%M:%S:%f',\n    '%d.%m.%Y %H:%M',\n    '%d.%m.%Y %H.%M',\n    '%Y-%m-%dT%H:%M:%SZ',\n    '%Y-%m-%dT%H:%M:%S.%fZ',\n    '%Y-%m-%dT%H:%M:%S.%f0Z',\n    '%Y-%m-%dT%H:%M:%S',\n    '%Y-%m-%dT%H:%M:%S.%f',\n    '%Y-%m-%dT%H:%M',\n    '%b %d %Y at %H:%M',\n    '%b %d %Y at %H:%M:%S',\n    '%B %d %Y at %H:%M',\n    '%B %d %Y at %H:%M:%S',\n    '%H:%M %d-%b-%Y',\n)\n\nDATE_FORMATS_DAY_FIRST = list(DATE_FORMATS)\nDATE_FORMATS_DAY_FIRST.extend([\n    '%d-%m-%Y',\n    '%d.%m.%Y',\n    '%d.%m.%y',\n    '%d/%m/%Y',\n    '%d/%m/%y',\n    '%d/%m/%Y %H:%M:%S',\n    '%d-%m-%Y %H:%M',\n    '%H:%M %d/%m/%Y',\n])\n\nDATE_FORMATS_MONTH_FIRST = list(DATE_FORMATS)\nDATE_FORMATS_MONTH_FIRST.extend([\n    '%m-%d-%Y',\n    '%m.%d.%Y',\n    '%m/%d/%Y',\n    '%m/%d/%y',\n    '%m/%d/%Y %H:%M:%S',\n])\n\nPACKED_CODES_RE = r\"}\\('(.+)',(\\d+),(\\d+),'([^']+)'\\.split\\('\\|'\\)\"\nJSON_LD_RE = r'(?is)<script[^>]+type=([\"\\']?)application/ld\\+json\\1[^>]*>\\s*(?P<json_ld>{.+?}|\\[.+?\\])\\s*</script>'\n\nNUMBER_RE = r'\\d+(?:\\.\\d+)?'\n\n\n@functools.cache\ndef preferredencoding():\n    \"\"\"Get preferred encoding.\n\n    Returns the best encoding scheme for the system, based on\n    locale.getpreferredencoding() and some further tweaks.\n    \"\"\"\n    try:\n        pref = locale.getpreferredencoding()\n        'TEST'.encode(pref)\n    except Exception:\n        pref = 'UTF-8'\n\n    return pref\n\n\ndef write_json_file(obj, fn):\n    \"\"\" Encode obj as JSON and write it to fn, atomically if possible \"\"\"\n\n    tf = tempfile.NamedTemporaryFile(\n        prefix=f'{os.path.basename(fn)}.', dir=os.path.dirname(fn),\n        suffix='.tmp', delete=False, mode='w', encoding='utf-8')\n\n    try:\n        with tf:\n            json.dump(obj, tf, ensure_ascii=False)\n        if sys.platform == 'win32':\n            # Need to remove existing file on Windows, else os.rename raises\n            # WindowsError or FileExistsError.\n            with contextlib.suppress(OSError):\n                os.unlink(fn)\n        with contextlib.suppress(OSError):\n            mask = os.umask(0)\n            os.umask(mask)\n            os.chmod(tf.name, 0o666 & ~mask)\n        os.rename(tf.name, fn)\n    except Exception:\n        with contextlib.suppress(OSError):\n            os.remove(tf.name)\n        raise\n\n\ndef find_xpath_attr(node, xpath, key, val=None):\n    \"\"\" Find the xpath xpath[@key=val] \"\"\"\n    assert re.match(r'^[a-zA-Z_-]+$', key)\n    expr = xpath + ('[@%s]' % key if val is None else f\"[@{key}='{val}']\")\n    return node.find(expr)\n\n# On python2.6 the xml.etree.ElementTree.Element methods don't support\n# the namespace parameter\n\n\ndef xpath_with_ns(path, ns_map):\n    components = [c.split(':') for c in path.split('/')]\n    replaced = []\n    for c in components:\n        if len(c) == 1:\n            replaced.append(c[0])\n        else:\n            ns, tag = c\n            replaced.append('{%s}%s' % (ns_map[ns], tag))\n    return '/'.join(replaced)\n\n\ndef xpath_element(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    def _find_xpath(xpath):\n        return node.find(xpath)\n\n    if isinstance(xpath, str):\n        n = _find_xpath(xpath)\n    else:\n        for xp in xpath:\n            n = _find_xpath(xp)\n            if n is not None:\n                break\n\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element %s' % name)\n        else:\n            return None\n    return n\n\n\ndef xpath_text(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    n = xpath_element(node, xpath, name, fatal=fatal, default=default)\n    if n is None or n == default:\n        return n\n    if n.text is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element\\'s text %s' % name)\n        else:\n            return None\n    return n.text\n\n\ndef xpath_attr(node, xpath, key, name=None, fatal=False, default=NO_DEFAULT):\n    n = find_xpath_attr(node, xpath, key)\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = f'{xpath}[@{key}]' if name is None else name\n            raise ExtractorError('Could not find XML attribute %s' % name)\n        else:\n            return None\n    return n.attrib[key]\n\n\ndef get_element_by_id(id, html, **kwargs):\n    \"\"\"Return the content of the tag with the specified ID in the passed HTML document\"\"\"\n    return get_element_by_attribute('id', id, html, **kwargs)\n\n\ndef get_element_html_by_id(id, html, **kwargs):\n    \"\"\"Return the html of the tag with the specified ID in the passed HTML document\"\"\"\n    return get_element_html_by_attribute('id', id, html, **kwargs)\n\n\ndef get_element_by_class(class_name, html):\n    \"\"\"Return the content of the first tag with the specified class in the passed HTML document\"\"\"\n    retval = get_elements_by_class(class_name, html)\n    return retval[0] if retval else None\n\n\ndef get_element_html_by_class(class_name, html):\n    \"\"\"Return the html of the first tag with the specified class in the passed HTML document\"\"\"\n    retval = get_elements_html_by_class(class_name, html)\n    return retval[0] if retval else None\n\n\ndef get_element_by_attribute(attribute, value, html, **kwargs):\n    retval = get_elements_by_attribute(attribute, value, html, **kwargs)\n    return retval[0] if retval else None\n\n\ndef get_element_html_by_attribute(attribute, value, html, **kargs):\n    retval = get_elements_html_by_attribute(attribute, value, html, **kargs)\n    return retval[0] if retval else None\n\n\ndef get_elements_by_class(class_name, html, **kargs):\n    \"\"\"Return the content of all tags with the specified class in the passed HTML document as a list\"\"\"\n    return get_elements_by_attribute(\n        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),\n        html, escape_value=False)\n\n\ndef get_elements_html_by_class(class_name, html):\n    \"\"\"Return the html of all tags with the specified class in the passed HTML document as a list\"\"\"\n    return get_elements_html_by_attribute(\n        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),\n        html, escape_value=False)\n\n\ndef get_elements_by_attribute(*args, **kwargs):\n    \"\"\"Return the content of the tag with the specified attribute in the passed HTML document\"\"\"\n    return [content for content, _ in get_elements_text_and_html_by_attribute(*args, **kwargs)]\n\n\ndef get_elements_html_by_attribute(*args, **kwargs):\n    \"\"\"Return the html of the tag with the specified attribute in the passed HTML document\"\"\"\n    return [whole for _, whole in get_elements_text_and_html_by_attribute(*args, **kwargs)]\n\n\ndef get_elements_text_and_html_by_attribute(attribute, value, html, *, tag=r'[\\w:.-]+', escape_value=True):\n    \"\"\"\n    Return the text (content) and the html (whole) of the tag with the specified\n    attribute in the passed HTML document\n    \"\"\"\n    if not value:\n        return\n\n    quote = '' if re.match(r'''[\\s\"'`=<>]''', value) else '?'\n\n    value = re.escape(value) if escape_value else value\n\n    partial_element_re = rf'''(?x)\n        <(?P<tag>{tag})\n         (?:\\s(?:[^>\"']|\"[^\"]*\"|'[^']*')*)?\n         \\s{re.escape(attribute)}\\s*=\\s*(?P<_q>['\"]{quote})(?-x:{value})(?P=_q)\n        '''\n\n    for m in re.finditer(partial_element_re, html):\n        content, whole = get_element_text_and_html_by_tag(m.group('tag'), html[m.start():])\n\n        yield (\n            unescapeHTML(re.sub(r'^(?P<q>[\"\\'])(?P<content>.*)(?P=q)$', r'\\g<content>', content, flags=re.DOTALL)),\n            whole\n        )\n\n\nclass HTMLBreakOnClosingTagParser(html.parser.HTMLParser):\n    \"\"\"\n    HTML parser which raises HTMLBreakOnClosingTagException upon reaching the\n    closing tag for the first opening tag it has encountered, and can be used\n    as a context manager\n    \"\"\"\n\n    class HTMLBreakOnClosingTagException(Exception):\n        pass\n\n    def __init__(self):\n        self.tagstack = collections.deque()\n        html.parser.HTMLParser.__init__(self)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *_):\n        self.close()\n\n    def close(self):\n        # handle_endtag does not return upon raising HTMLBreakOnClosingTagException,\n        # so data remains buffered; we no longer have any interest in it, thus\n        # override this method to discard it\n        pass\n\n    def handle_starttag(self, tag, _):\n        self.tagstack.append(tag)\n\n    def handle_endtag(self, tag):\n        if not self.tagstack:\n            raise compat_HTMLParseError('no tags in the stack')\n        while self.tagstack:\n            inner_tag = self.tagstack.pop()\n            if inner_tag == tag:\n                break\n        else:\n            raise compat_HTMLParseError(f'matching opening tag for closing {tag} tag not found')\n        if not self.tagstack:\n            raise self.HTMLBreakOnClosingTagException()\n\n\n# XXX: This should be far less strict\ndef get_element_text_and_html_by_tag(tag, html):\n    \"\"\"\n    For the first element with the specified tag in the passed HTML document\n    return its' content (text) and the whole element (html)\n    \"\"\"\n    def find_or_raise(haystack, needle, exc):\n        try:\n            return haystack.index(needle)\n        except ValueError:\n            raise exc\n    closing_tag = f'</{tag}>'\n    whole_start = find_or_raise(\n        html, f'<{tag}', compat_HTMLParseError(f'opening {tag} tag not found'))\n    content_start = find_or_raise(\n        html[whole_start:], '>', compat_HTMLParseError(f'malformed opening {tag} tag'))\n    content_start += whole_start + 1\n    with HTMLBreakOnClosingTagParser() as parser:\n        parser.feed(html[whole_start:content_start])\n        if not parser.tagstack or parser.tagstack[0] != tag:\n            raise compat_HTMLParseError(f'parser did not match opening {tag} tag')\n        offset = content_start\n        while offset < len(html):\n            next_closing_tag_start = find_or_raise(\n                html[offset:], closing_tag,\n                compat_HTMLParseError(f'closing {tag} tag not found'))\n            next_closing_tag_end = next_closing_tag_start + len(closing_tag)\n            try:\n                parser.feed(html[offset:offset + next_closing_tag_end])\n                offset += next_closing_tag_end\n            except HTMLBreakOnClosingTagParser.HTMLBreakOnClosingTagException:\n                return html[content_start:offset + next_closing_tag_start], \\\n                    html[whole_start:offset + next_closing_tag_end]\n        raise compat_HTMLParseError('unexpected end of html')\n\n\nclass HTMLAttributeParser(html.parser.HTMLParser):\n    \"\"\"Trivial HTML parser to gather the attributes for a single element\"\"\"\n\n    def __init__(self):\n        self.attrs = {}\n        html.parser.HTMLParser.__init__(self)\n\n    def handle_starttag(self, tag, attrs):\n        self.attrs = dict(attrs)\n        raise compat_HTMLParseError('done')\n\n\nclass HTMLListAttrsParser(html.parser.HTMLParser):\n    \"\"\"HTML parser to gather the attributes for the elements of a list\"\"\"\n\n    def __init__(self):\n        html.parser.HTMLParser.__init__(self)\n        self.items = []\n        self._level = 0\n\n    def handle_starttag(self, tag, attrs):\n        if tag == 'li' and self._level == 0:\n            self.items.append(dict(attrs))\n        self._level += 1\n\n    def handle_endtag(self, tag):\n        self._level -= 1\n\n\ndef extract_attributes(html_element):\n    \"\"\"Given a string for an HTML element such as\n    <el\n         a=\"foo\" B=\"bar\" c=\"&98;az\" d=boz\n         empty= noval entity=\"&amp;\"\n         sq='\"' dq=\"'\"\n    >\n    Decode and return a dictionary of attributes.\n    {\n        'a': 'foo', 'b': 'bar', c: 'baz', d: 'boz',\n        'empty': '', 'noval': None, 'entity': '&',\n        'sq': '\"', 'dq': '\\''\n    }.\n    \"\"\"\n    parser = HTMLAttributeParser()\n    with contextlib.suppress(compat_HTMLParseError):\n        parser.feed(html_element)\n        parser.close()\n    return parser.attrs\n\n\ndef parse_list(webpage):\n    \"\"\"Given a string for an series of HTML <li> elements,\n    return a dictionary of their attributes\"\"\"\n    parser = HTMLListAttrsParser()\n    parser.feed(webpage)\n    parser.close()\n    return parser.items\n\n\ndef clean_html(html):\n    \"\"\"Clean an HTML snippet into a readable string\"\"\"\n\n    if html is None:  # Convenience for sanitizing descriptions etc.\n        return html\n\n    html = re.sub(r'\\s+', ' ', html)\n    html = re.sub(r'(?u)\\s?<\\s?br\\s?/?\\s?>\\s?', '\\n', html)\n    html = re.sub(r'(?u)<\\s?/\\s?p\\s?>\\s?<\\s?p[^>]*>', '\\n', html)\n    # Strip html tags\n    html = re.sub('<.*?>', '', html)\n    # Replace html entities\n    html = unescapeHTML(html)\n    return html.strip()\n\n\nclass LenientJSONDecoder(json.JSONDecoder):\n    # TODO: Write tests\n    def __init__(self, *args, transform_source=None, ignore_extra=False, close_objects=0, **kwargs):\n        self.transform_source, self.ignore_extra = transform_source, ignore_extra\n        self._close_attempts = 2 * close_objects\n        super().__init__(*args, **kwargs)\n\n    @staticmethod\n    def _close_object(err):\n        doc = err.doc[:err.pos]\n        # We need to add comma first to get the correct error message\n        if err.msg.startswith('Expecting \\',\\''):\n            return doc + ','\n        elif not doc.endswith(','):\n            return\n\n        if err.msg.startswith('Expecting property name'):\n            return doc[:-1] + '}'\n        elif err.msg.startswith('Expecting value'):\n            return doc[:-1] + ']'\n\n    def decode(self, s):\n        if self.transform_source:\n            s = self.transform_source(s)\n        for attempt in range(self._close_attempts + 1):\n            try:\n                if self.ignore_extra:\n                    return self.raw_decode(s.lstrip())[0]\n                return super().decode(s)\n            except json.JSONDecodeError as e:\n                if e.pos is None:\n                    raise\n                elif attempt < self._close_attempts:\n                    s = self._close_object(e)\n                    if s is not None:\n                        continue\n                raise type(e)(f'{e.msg} in {s[e.pos-10:e.pos+10]!r}', s, e.pos)\n        assert False, 'Too many attempts to decode JSON'\n\n\ndef sanitize_open(filename, open_mode):\n    \"\"\"Try to open the given filename, and slightly tweak it if this fails.\n\n    Attempts to open the given filename. If this fails, it tries to change\n    the filename slightly, step by step, until it's either able to open it\n    or it fails and raises a final exception, like the standard open()\n    function.\n\n    It returns the tuple (stream, definitive_file_name).\n    \"\"\"\n    if filename == '-':\n        if sys.platform == 'win32':\n            import msvcrt\n\n            # stdout may be any IO stream, e.g. when using contextlib.redirect_stdout\n            with contextlib.suppress(io.UnsupportedOperation):\n                msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n        return (sys.stdout.buffer if hasattr(sys.stdout, 'buffer') else sys.stdout, filename)\n\n    for attempt in range(2):\n        try:\n            try:\n                if sys.platform == 'win32':\n                    # FIXME: An exclusive lock also locks the file from being read.\n                    # Since windows locks are mandatory, don't lock the file on windows (for now).\n                    # Ref: https://github.com/yt-dlp/yt-dlp/issues/3124\n                    raise LockingUnsupportedError()\n                stream = locked_file(filename, open_mode, block=False).__enter__()\n            except OSError:\n                stream = open(filename, open_mode)\n            return stream, filename\n        except OSError as err:\n            if attempt or err.errno in (errno.EACCES,):\n                raise\n            old_filename, filename = filename, sanitize_path(filename)\n            if old_filename == filename:\n                raise\n\n\ndef timeconvert(timestr):\n    \"\"\"Convert RFC 2822 defined time string into system timestamp\"\"\"\n    timestamp = None\n    timetuple = email.utils.parsedate_tz(timestr)\n    if timetuple is not None:\n        timestamp = email.utils.mktime_tz(timetuple)\n    return timestamp\n\n\ndef sanitize_filename(s, restricted=False, is_id=NO_DEFAULT):\n    \"\"\"Sanitizes a string so it could be used as part of a filename.\n    @param restricted   Use a stricter subset of allowed characters\n    @param is_id        Whether this is an ID that should be kept unchanged if possible.\n                        If unset, yt-dlp's new sanitization rules are in effect\n    \"\"\"\n    if s == '':\n        return ''\n\n    def replace_insane(char):\n        if restricted and char in ACCENT_CHARS:\n            return ACCENT_CHARS[char]\n        elif not restricted and char == '\\n':\n            return '\\0 '\n        elif is_id is NO_DEFAULT and not restricted and char in '\"*:<>?|/\\\\':\n            # Replace with their full-width unicode counterparts\n            return {'/': '\\u29F8', '\\\\': '\\u29f9'}.get(char, chr(ord(char) + 0xfee0))\n        elif char == '?' or ord(char) < 32 or ord(char) == 127:\n            return ''\n        elif char == '\"':\n            return '' if restricted else '\\''\n        elif char == ':':\n            return '\\0_\\0-' if restricted else '\\0 \\0-'\n        elif char in '\\\\/|*<>':\n            return '\\0_'\n        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace() or ord(char) > 127):\n            return '\\0_'\n        return char\n\n    # Replace look-alike Unicode glyphs\n    if restricted and (is_id is NO_DEFAULT or not is_id):\n        s = unicodedata.normalize('NFKC', s)\n    s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s)  # Handle timestamps\n    result = ''.join(map(replace_insane, s))\n    if is_id is NO_DEFAULT:\n        result = re.sub(r'(\\0.)(?:(?=\\1)..)+', r'\\1', result)  # Remove repeated substitute chars\n        STRIP_RE = r'(?:\\0.|[ _-])*'\n        result = re.sub(f'^\\0.{STRIP_RE}|{STRIP_RE}\\0.$', '', result)  # Remove substitute chars from start/end\n    result = result.replace('\\0', '') or '_'\n\n    if not is_id:\n        while '__' in result:\n            result = result.replace('__', '_')\n        result = result.strip('_')\n        # Common case of \"Foreign band name - English song title\"\n        if restricted and result.startswith('-_'):\n            result = result[2:]\n        if result.startswith('-'):\n            result = '_' + result[len('-'):]\n        result = result.lstrip('.')\n        if not result:\n            result = '_'\n    return result\n\n\ndef sanitize_path(s, force=False):\n    \"\"\"Sanitizes and normalizes path on Windows\"\"\"\n    # XXX: this handles drive relative paths (c:sth) incorrectly\n    if sys.platform == 'win32':\n        force = False\n        drive_or_unc, _ = os.path.splitdrive(s)\n    elif force:\n        drive_or_unc = ''\n    else:\n        return s\n\n    norm_path = os.path.normpath(remove_start(s, drive_or_unc)).split(os.path.sep)\n    if drive_or_unc:\n        norm_path.pop(0)\n    sanitized_path = [\n        path_part if path_part in ['.', '..'] else re.sub(r'(?:[/<>:\"\\|\\\\?\\*]|[\\s.]$)', '#', path_part)\n        for path_part in norm_path]\n    if drive_or_unc:\n        sanitized_path.insert(0, drive_or_unc + os.path.sep)\n    elif force and s and s[0] == os.path.sep:\n        sanitized_path.insert(0, os.path.sep)\n    # TODO: Fix behavioral differences <3.12\n    # The workaround using `normpath` only superficially passes tests\n    # Ref: https://github.com/python/cpython/pull/100351\n    return os.path.normpath(os.path.join(*sanitized_path))\n\n\ndef sanitize_url(url, *, scheme='http'):\n    # Prepend protocol-less URLs with `http:` scheme in order to mitigate\n    # the number of unwanted failures due to missing protocol\n    if url is None:\n        return\n    elif url.startswith('//'):\n        return f'{scheme}:{url}'\n    # Fix some common typos seen so far\n    COMMON_TYPOS = (\n        # https://github.com/ytdl-org/youtube-dl/issues/15649\n        (r'^httpss://', r'https://'),\n        # https://bx1.be/lives/direct-tv/\n        (r'^rmtp([es]?)://', r'rtmp\\1://'),\n    )\n    for mistake, fixup in COMMON_TYPOS:\n        if re.match(mistake, url):\n            return re.sub(mistake, fixup, url)\n    return url\n\n\ndef extract_basic_auth(url):\n    parts = urllib.parse.urlsplit(url)\n    if parts.username is None:\n        return url, None\n    url = urllib.parse.urlunsplit(parts._replace(netloc=(\n        parts.hostname if parts.port is None\n        else '%s:%d' % (parts.hostname, parts.port))))\n    auth_payload = base64.b64encode(\n        ('%s:%s' % (parts.username, parts.password or '')).encode())\n    return url, f'Basic {auth_payload.decode()}'\n\n\ndef expand_path(s):\n    \"\"\"Expand shell variables and ~\"\"\"\n    return os.path.expandvars(compat_expanduser(s))\n\n\ndef orderedSet(iterable, *, lazy=False):\n    \"\"\"Remove all duplicates from the input iterable\"\"\"\n    def _iter():\n        seen = []  # Do not use set since the items can be unhashable\n        for x in iterable:\n            if x not in seen:\n                seen.append(x)\n                yield x\n\n    return _iter() if lazy else list(_iter())\n\n\ndef _htmlentity_transform(entity_with_semicolon):\n    \"\"\"Transforms an HTML entity to a character.\"\"\"\n    entity = entity_with_semicolon[:-1]\n\n    # Known non-numeric HTML entity\n    if entity in html.entities.name2codepoint:\n        return chr(html.entities.name2codepoint[entity])\n\n    # TODO: HTML5 allows entities without a semicolon.\n    # E.g. '&Eacuteric' should be decoded as '\u00c9ric'.\n    if entity_with_semicolon in html.entities.html5:\n        return html.entities.html5[entity_with_semicolon]\n\n    mobj = re.match(r'#(x[0-9a-fA-F]+|[0-9]+)', entity)\n    if mobj is not None:\n        numstr = mobj.group(1)\n        if numstr.startswith('x'):\n            base = 16\n            numstr = '0%s' % numstr\n        else:\n            base = 10\n        # See https://github.com/ytdl-org/youtube-dl/issues/7518\n        with contextlib.suppress(ValueError):\n            return chr(int(numstr, base))\n\n    # Unknown entity in name, return its literal representation\n    return '&%s;' % entity\n\n\ndef unescapeHTML(s):\n    if s is None:\n        return None\n    assert isinstance(s, str)\n\n    return re.sub(\n        r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)\n\n\ndef escapeHTML(text):\n    return (\n        text\n        .replace('&', '&amp;')\n        .replace('<', '&lt;')\n        .replace('>', '&gt;')\n        .replace('\"', '&quot;')\n        .replace(\"'\", '&#39;')\n    )\n\n\nclass netrc_from_content(netrc.netrc):\n    def __init__(self, content):\n        self.hosts, self.macros = {}, {}\n        with io.StringIO(content) as stream:\n            self._parse('-', stream, False)\n\n\nclass Popen(subprocess.Popen):\n    if sys.platform == 'win32':\n        _startupinfo = subprocess.STARTUPINFO()\n        _startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n    else:\n        _startupinfo = None\n\n    @staticmethod\n    def _fix_pyinstaller_ld_path(env):\n        \"\"\"Restore LD_LIBRARY_PATH when using PyInstaller\n            Ref: https://github.com/pyinstaller/pyinstaller/blob/develop/doc/runtime-information.rst#ld_library_path--libpath-considerations\n                 https://github.com/yt-dlp/yt-dlp/issues/4573\n        \"\"\"\n        if not hasattr(sys, '_MEIPASS'):\n            return\n\n        def _fix(key):\n            orig = env.get(f'{key}_ORIG')\n            if orig is None:\n                env.pop(key, None)\n            else:\n                env[key] = orig\n\n        _fix('LD_LIBRARY_PATH')  # Linux\n        _fix('DYLD_LIBRARY_PATH')  # macOS\n\n    def __init__(self, *args, env=None, text=False, **kwargs):\n        if env is None:\n            env = os.environ.copy()\n        self._fix_pyinstaller_ld_path(env)\n\n        self.__text_mode = kwargs.get('encoding') or kwargs.get('errors') or text or kwargs.get('universal_newlines')\n        if text is True:\n            kwargs['universal_newlines'] = True  # For 3.6 compatibility\n            kwargs.setdefault('encoding', 'utf-8')\n            kwargs.setdefault('errors', 'replace')\n        super().__init__(*args, env=env, **kwargs, startupinfo=self._startupinfo)\n\n    def communicate_or_kill(self, *args, **kwargs):\n        try:\n            return self.communicate(*args, **kwargs)\n        except BaseException:  # Including KeyboardInterrupt\n            self.kill(timeout=None)\n            raise\n\n    def kill(self, *, timeout=0):\n        super().kill()\n        if timeout != 0:\n            self.wait(timeout=timeout)\n\n    @classmethod\n    def run(cls, *args, timeout=None, **kwargs):\n        with cls(*args, **kwargs) as proc:\n            default = '' if proc.__text_mode else b''\n            stdout, stderr = proc.communicate_or_kill(timeout=timeout)\n            return stdout or default, stderr or default, proc.returncode\n\n\ndef encodeArgument(s):\n    # Legacy code that uses byte strings\n    # Uncomment the following line after fixing all post processors\n    # assert isinstance(s, str), 'Internal error: %r should be of type %r, is %r' % (s, str, type(s))\n    return s if isinstance(s, str) else s.decode('ascii')\n\n\n_timetuple = collections.namedtuple('Time', ('hours', 'minutes', 'seconds', 'milliseconds'))\n\n\ndef timetuple_from_msec(msec):\n    secs, msec = divmod(msec, 1000)\n    mins, secs = divmod(secs, 60)\n    hrs, mins = divmod(mins, 60)\n    return _timetuple(hrs, mins, secs, msec)\n\n\ndef formatSeconds(secs, delim=':', msec=False):\n    time = timetuple_from_msec(secs * 1000)\n    if time.hours:\n        ret = '%d%s%02d%s%02d' % (time.hours, delim, time.minutes, delim, time.seconds)\n    elif time.minutes:\n        ret = '%d%s%02d' % (time.minutes, delim, time.seconds)\n    else:\n        ret = '%d' % time.seconds\n    return '%s.%03d' % (ret, time.milliseconds) if msec else ret\n\n\ndef bug_reports_message(before=';'):\n    from ..update import REPOSITORY\n\n    msg = (f'please report this issue on  https://github.com/{REPOSITORY}/issues?q= , '\n           'filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U')\n\n    before = before.rstrip()\n    if not before or before.endswith(('.', '!', '?')):\n        msg = msg[0].title() + msg[1:]\n\n    return (before + ' ' if before else '') + msg\n\n\nclass YoutubeDLError(Exception):\n    \"\"\"Base exception for YoutubeDL errors.\"\"\"\n    msg = None\n\n    def __init__(self, msg=None):\n        if msg is not None:\n            self.msg = msg\n        elif self.msg is None:\n            self.msg = type(self).__name__\n        super().__init__(self.msg)\n\n\nclass ExtractorError(YoutubeDLError):\n    \"\"\"Error during info extraction.\"\"\"\n\n    def __init__(self, msg, tb=None, expected=False, cause=None, video_id=None, ie=None):\n        \"\"\" tb, if given, is the original traceback (so that it can be printed out).\n        If expected is set, this is a normal error message and most likely not a bug in yt-dlp.\n        \"\"\"\n        from ..networking.exceptions import network_exceptions\n        if sys.exc_info()[0] in network_exceptions:\n            expected = True\n\n        self.orig_msg = str(msg)\n        self.traceback = tb\n        self.expected = expected\n        self.cause = cause\n        self.video_id = video_id\n        self.ie = ie\n        self.exc_info = sys.exc_info()  # preserve original exception\n        if isinstance(self.exc_info[1], ExtractorError):\n            self.exc_info = self.exc_info[1].exc_info\n        super().__init__(self.__msg)\n\n    @property\n    def __msg(self):\n        return ''.join((\n            format_field(self.ie, None, '[%s] '),\n            format_field(self.video_id, None, '%s: '),\n            self.orig_msg,\n            format_field(self.cause, None, ' (caused by %r)'),\n            '' if self.expected else bug_reports_message()))\n\n    def format_traceback(self):\n        return join_nonempty(\n            self.traceback and ''.join(traceback.format_tb(self.traceback)),\n            self.cause and ''.join(traceback.format_exception(None, self.cause, self.cause.__traceback__)[1:]),\n            delim='\\n') or None\n\n    def __setattr__(self, name, value):\n        super().__setattr__(name, value)\n        if getattr(self, 'msg', None) and name not in ('msg', 'args'):\n            self.msg = self.__msg or type(self).__name__\n            self.args = (self.msg, )  # Cannot be property\n\n\nclass UnsupportedError(ExtractorError):\n    def __init__(self, url):\n        super().__init__(\n            'Unsupported URL: %s' % url, expected=True)\n        self.url = url\n\n\nclass RegexNotFoundError(ExtractorError):\n    \"\"\"Error when a regex didn't match\"\"\"\n    pass\n\n\nclass GeoRestrictedError(ExtractorError):\n    \"\"\"Geographic restriction Error exception.\n\n    This exception may be thrown when a video is not available from your\n    geographic location due to geographic restrictions imposed by a website.\n    \"\"\"\n\n    def __init__(self, msg, countries=None, **kwargs):\n        kwargs['expected'] = True\n        super().__init__(msg, **kwargs)\n        self.countries = countries\n\n\nclass UserNotLive(ExtractorError):\n    \"\"\"Error when a channel/user is not live\"\"\"\n\n    def __init__(self, msg=None, **kwargs):\n        kwargs['expected'] = True\n        super().__init__(msg or 'The channel is not currently live', **kwargs)\n\n\nclass DownloadError(YoutubeDLError):\n    \"\"\"Download Error exception.\n\n    This exception may be thrown by FileDownloader objects if they are not\n    configured to continue on errors. They will contain the appropriate\n    error message.\n    \"\"\"\n\n    def __init__(self, msg, exc_info=None):\n        \"\"\" exc_info, if given, is the original exception that caused the trouble (as returned by sys.exc_info()). \"\"\"\n        super().__init__(msg)\n        self.exc_info = exc_info\n\n\nclass EntryNotInPlaylist(YoutubeDLError):\n    \"\"\"Entry not in playlist exception.\n\n    This exception will be thrown by YoutubeDL when a requested entry\n    is not found in the playlist info_dict\n    \"\"\"\n    msg = 'Entry not found in info'\n\n\nclass SameFileError(YoutubeDLError):\n    \"\"\"Same File exception.\n\n    This exception will be thrown by FileDownloader objects if they detect\n    multiple files would have to be downloaded to the same file on disk.\n    \"\"\"\n    msg = 'Fixed output name but more than one file to download'\n\n    def __init__(self, filename=None):\n        if filename is not None:\n            self.msg += f': {filename}'\n        super().__init__(self.msg)\n\n\nclass PostProcessingError(YoutubeDLError):\n    \"\"\"Post Processing exception.\n\n    This exception may be raised by PostProcessor's .run() method to\n    indicate an error in the postprocessing task.\n    \"\"\"\n\n\nclass DownloadCancelled(YoutubeDLError):\n    \"\"\" Exception raised when the download queue should be interrupted \"\"\"\n    msg = 'The download was cancelled'\n\n\nclass ExistingVideoReached(DownloadCancelled):\n    \"\"\" --break-on-existing triggered \"\"\"\n    msg = 'Encountered a video that is already in the archive, stopping due to --break-on-existing'\n\n\nclass RejectedVideoReached(DownloadCancelled):\n    \"\"\" --break-match-filter triggered \"\"\"\n    msg = 'Encountered a video that did not match filter, stopping due to --break-match-filter'\n\n\nclass MaxDownloadsReached(DownloadCancelled):\n    \"\"\" --max-downloads limit has been reached. \"\"\"\n    msg = 'Maximum number of downloads reached, stopping due to --max-downloads'\n\n\nclass ReExtractInfo(YoutubeDLError):\n    \"\"\" Video info needs to be re-extracted. \"\"\"\n\n    def __init__(self, msg, expected=False):\n        super().__init__(msg)\n        self.expected = expected\n\n\nclass ThrottledDownload(ReExtractInfo):\n    \"\"\" Download speed below --throttled-rate. \"\"\"\n    msg = 'The download speed is below throttle limit'\n\n    def __init__(self):\n        super().__init__(self.msg, expected=False)\n\n\nclass UnavailableVideoError(YoutubeDLError):\n    \"\"\"Unavailable Format exception.\n\n    This exception will be thrown when a video is requested\n    in a format that is not available for that video.\n    \"\"\"\n    msg = 'Unable to download video'\n\n    def __init__(self, err=None):\n        if err is not None:\n            self.msg += f': {err}'\n        super().__init__(self.msg)\n\n\nclass ContentTooShortError(YoutubeDLError):\n    \"\"\"Content Too Short exception.\n\n    This exception may be raised by FileDownloader objects when a file they\n    download is too small for what the server announced first, indicating\n    the connection was probably interrupted.\n    \"\"\"\n\n    def __init__(self, downloaded, expected):\n        super().__init__(f'Downloaded {downloaded} bytes, expected {expected} bytes')\n        # Both in bytes\n        self.downloaded = downloaded\n        self.expected = expected\n\n\nclass XAttrMetadataError(YoutubeDLError):\n    def __init__(self, code=None, msg='Unknown error'):\n        super().__init__(msg)\n        self.code = code\n        self.msg = msg\n\n        # Parsing code and msg\n        if (self.code in (errno.ENOSPC, errno.EDQUOT)\n                or 'No space left' in self.msg or 'Disk quota exceeded' in self.msg):\n            self.reason = 'NO_SPACE'\n        elif self.code == errno.E2BIG or 'Argument list too long' in self.msg:\n            self.reason = 'VALUE_TOO_LONG'\n        else:\n            self.reason = 'NOT_SUPPORTED'\n\n\nclass XAttrUnavailableError(YoutubeDLError):\n    pass\n\n\ndef is_path_like(f):\n    return isinstance(f, (str, bytes, os.PathLike))\n\n\ndef extract_timezone(date_str):\n    m = re.search(\n        r'''(?x)\n            ^.{8,}?                                              # >=8 char non-TZ prefix, if present\n            (?P<tz>Z|                                            # just the UTC Z, or\n                (?:(?<=.\\b\\d{4}|\\b\\d{2}:\\d\\d)|                   # preceded by 4 digits or hh:mm or\n                   (?<!.\\b[a-zA-Z]{3}|[a-zA-Z]{4}|..\\b\\d\\d))     # not preceded by 3 alpha word or >= 4 alpha or 2 digits\n                   [ ]?                                          # optional space\n                (?P<sign>\\+|-)                                   # +/-\n                (?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})       # hh[:]mm\n            $)\n        ''', date_str)\n    if not m:\n        m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n        timezone = TIMEZONE_NAMES.get(m and m.group('tz').strip())\n        if timezone is not None:\n            date_str = date_str[:-len(m.group('tz'))]\n        timezone = datetime.timedelta(hours=timezone or 0)\n    else:\n        date_str = date_str[:-len(m.group('tz'))]\n        if not m.group('sign'):\n            timezone = datetime.timedelta()\n        else:\n            sign = 1 if m.group('sign') == '+' else -1\n            timezone = datetime.timedelta(\n                hours=sign * int(m.group('hours')),\n                minutes=sign * int(m.group('minutes')))\n    return timezone, date_str\n\n\ndef parse_iso8601(date_str, delimiter='T', timezone=None):\n    \"\"\" Return a UNIX timestamp from the given date \"\"\"\n\n    if date_str is None:\n        return None\n\n    date_str = re.sub(r'\\.[0-9]+', '', date_str)\n\n    if timezone is None:\n        timezone, date_str = extract_timezone(date_str)\n\n    with contextlib.suppress(ValueError):\n        date_format = f'%Y-%m-%d{delimiter}%H:%M:%S'\n        dt = datetime.datetime.strptime(date_str, date_format) - timezone\n        return calendar.timegm(dt.timetuple())\n\n\ndef date_formats(day_first=True):\n    return DATE_FORMATS_DAY_FIRST if day_first else DATE_FORMATS_MONTH_FIRST\n\n\ndef unified_strdate(date_str, day_first=True):\n    \"\"\"Return a string with the date in the format YYYYMMDD\"\"\"\n\n    if date_str is None:\n        return None\n    upload_date = None\n    # Replace commas\n    date_str = date_str.replace(',', ' ')\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n    _, date_str = extract_timezone(date_str)\n\n    for expression in date_formats(day_first):\n        with contextlib.suppress(ValueError):\n            upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n    if upload_date is None:\n        timetuple = email.utils.parsedate_tz(date_str)\n        if timetuple:\n            with contextlib.suppress(ValueError):\n                upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    if upload_date is not None:\n        return str(upload_date)\n\n\ndef unified_timestamp(date_str, day_first=True):\n    if not isinstance(date_str, str):\n        return None\n\n    date_str = re.sub(r'\\s+', ' ', re.sub(\n        r'(?i)[,|]|(mon|tues?|wed(nes)?|thu(rs)?|fri|sat(ur)?)(day)?', '', date_str))\n\n    pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0\n    timezone, date_str = extract_timezone(date_str)\n\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n\n    # Remove unrecognized timezones from ISO 8601 alike timestamps\n    m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n    if m:\n        date_str = date_str[:-len(m.group('tz'))]\n\n    # Python only supports microseconds, so remove nanoseconds\n    m = re.search(r'^([0-9]{4,}-[0-9]{1,2}-[0-9]{1,2}T[0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\\.[0-9]{6})[0-9]+$', date_str)\n    if m:\n        date_str = m.group(1)\n\n    for expression in date_formats(day_first):\n        with contextlib.suppress(ValueError):\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n            return calendar.timegm(dt.timetuple())\n\n    timetuple = email.utils.parsedate_tz(date_str)\n    if timetuple:\n        return calendar.timegm(timetuple) + pm_delta * 3600 - timezone.total_seconds()\n\n\ndef determine_ext(url, default_ext='unknown_video'):\n    if url is None or '.' not in url:\n        return default_ext\n    guess = url.partition('?')[0].rpartition('.')[2]\n    if re.match(r'^[A-Za-z0-9]+$', guess):\n        return guess\n    # Try extract ext from URLs like http://example.com/foo/bar.mp4/?download\n    elif guess.rstrip('/') in KNOWN_EXTENSIONS:\n        return guess.rstrip('/')\n    else:\n        return default_ext\n\n\ndef subtitles_filename(filename, sub_lang, sub_format, expected_real_ext=None):\n    return replace_extension(filename, sub_lang + '.' + sub_format, expected_real_ext)\n\n\ndef datetime_from_str(date_str, precision='auto', format='%Y%m%d'):\n    R\"\"\"\n    Return a datetime object from a string.\n    Supported format:\n        (now|today|yesterday|DATE)([+-]\\d+(microsecond|second|minute|hour|day|week|month|year)s?)?\n\n    @param format       strftime format of DATE\n    @param precision    Round the datetime object: auto|microsecond|second|minute|hour|day\n                        auto: round to the unit provided in date_str (if applicable).\n    \"\"\"\n    auto_precision = False\n    if precision == 'auto':\n        auto_precision = True\n        precision = 'microsecond'\n    today = datetime_round(datetime.datetime.now(datetime.timezone.utc), precision)\n    if date_str in ('now', 'today'):\n        return today\n    if date_str == 'yesterday':\n        return today - datetime.timedelta(days=1)\n    match = re.match(\n        r'(?P<start>.+)(?P<sign>[+-])(?P<time>\\d+)(?P<unit>microsecond|second|minute|hour|day|week|month|year)s?',\n        date_str)\n    if match is not None:\n        start_time = datetime_from_str(match.group('start'), precision, format)\n        time = int(match.group('time')) * (-1 if match.group('sign') == '-' else 1)\n        unit = match.group('unit')\n        if unit == 'month' or unit == 'year':\n            new_date = datetime_add_months(start_time, time * 12 if unit == 'year' else time)\n            unit = 'day'\n        else:\n            if unit == 'week':\n                unit = 'day'\n                time *= 7\n            delta = datetime.timedelta(**{unit + 's': time})\n            new_date = start_time + delta\n        if auto_precision:\n            return datetime_round(new_date, unit)\n        return new_date\n\n    return datetime_round(datetime.datetime.strptime(date_str, format), precision)\n\n\ndef date_from_str(date_str, format='%Y%m%d', strict=False):\n    R\"\"\"\n    Return a date object from a string using datetime_from_str\n\n    @param strict  Restrict allowed patterns to \"YYYYMMDD\" and\n                   (now|today|yesterday)(-\\d+(day|week|month|year)s?)?\n    \"\"\"\n    if strict and not re.fullmatch(r'\\d{8}|(now|today|yesterday)(-\\d+(day|week|month|year)s?)?', date_str):\n        raise ValueError(f'Invalid date format \"{date_str}\"')\n    return datetime_from_str(date_str, precision='microsecond', format=format).date()\n\n\ndef datetime_add_months(dt, months):\n    \"\"\"Increment/Decrement a datetime object by months.\"\"\"\n    month = dt.month + months - 1\n    year = dt.year + month // 12\n    month = month % 12 + 1\n    day = min(dt.day, calendar.monthrange(year, month)[1])\n    return dt.replace(year, month, day)\n\n\ndef datetime_round(dt, precision='day'):\n    \"\"\"\n    Round a datetime object's time to a specific precision\n    \"\"\"\n    if precision == 'microsecond':\n        return dt\n\n    unit_seconds = {\n        'day': 86400,\n        'hour': 3600,\n        'minute': 60,\n        'second': 1,\n    }\n    roundto = lambda x, n: ((x + n / 2) // n) * n\n    timestamp = roundto(calendar.timegm(dt.timetuple()), unit_seconds[precision])\n    return datetime.datetime.fromtimestamp(timestamp, datetime.timezone.utc)\n\n\ndef hyphenate_date(date_str):\n    \"\"\"\n    Convert a date in 'YYYYMMDD' format to 'YYYY-MM-DD' format\"\"\"\n    match = re.match(r'^(\\d\\d\\d\\d)(\\d\\d)(\\d\\d)$', date_str)\n    if match is not None:\n        return '-'.join(match.groups())\n    else:\n        return date_str\n\n\nclass DateRange:\n    \"\"\"Represents a time interval between two dates\"\"\"\n\n    def __init__(self, start=None, end=None):\n        \"\"\"start and end must be strings in the format accepted by date\"\"\"\n        if start is not None:\n            self.start = date_from_str(start, strict=True)\n        else:\n            self.start = datetime.datetime.min.date()\n        if end is not None:\n            self.end = date_from_str(end, strict=True)\n        else:\n            self.end = datetime.datetime.max.date()\n        if self.start > self.end:\n            raise ValueError('Date range: \"%s\" , the start date must be before the end date' % self)\n\n    @classmethod\n    def day(cls, day):\n        \"\"\"Returns a range that only contains the given day\"\"\"\n        return cls(day, day)\n\n    def __contains__(self, date):\n        \"\"\"Check if the date is in the range\"\"\"\n        if not isinstance(date, datetime.date):\n            date = date_from_str(date)\n        return self.start <= date <= self.end\n\n    def __repr__(self):\n        return f'{__name__}.{type(self).__name__}({self.start.isoformat()!r}, {self.end.isoformat()!r})'\n\n    def __eq__(self, other):\n        return (isinstance(other, DateRange)\n                and self.start == other.start and self.end == other.end)\n\n\n@functools.cache\ndef system_identifier():\n    python_implementation = platform.python_implementation()\n    if python_implementation == 'PyPy' and hasattr(sys, 'pypy_version_info'):\n        python_implementation += ' version %d.%d.%d' % sys.pypy_version_info[:3]\n    libc_ver = []\n    with contextlib.suppress(OSError):  # We may not have access to the executable\n        libc_ver = platform.libc_ver()\n\n    return 'Python %s (%s %s %s) - %s (%s%s)' % (\n        platform.python_version(),\n        python_implementation,\n        platform.machine(),\n        platform.architecture()[0],\n        platform.platform(),\n        ssl.OPENSSL_VERSION,\n        format_field(join_nonempty(*libc_ver, delim=' '), None, ', %s'),\n    )\n\n\n@functools.cache\ndef get_windows_version():\n    ''' Get Windows version. returns () if it's not running on Windows '''\n    if compat_os_name == 'nt':\n        return version_tuple(platform.win32_ver()[1])\n    else:\n        return ()\n\n\ndef write_string(s, out=None, encoding=None):\n    assert isinstance(s, str)\n    out = out or sys.stderr\n    # `sys.stderr` might be `None` (Ref: https://github.com/pyinstaller/pyinstaller/pull/7217)\n    if not out:\n        return\n\n    if compat_os_name == 'nt' and supports_terminal_sequences(out):\n        s = re.sub(r'([\\r\\n]+)', r' \\1', s)\n\n    enc, buffer = None, out\n    if 'b' in getattr(out, 'mode', ''):\n        enc = encoding or preferredencoding()\n    elif hasattr(out, 'buffer'):\n        buffer = out.buffer\n        enc = encoding or getattr(out, 'encoding', None) or preferredencoding()\n\n    buffer.write(s.encode(enc, 'ignore') if enc else s)\n    out.flush()\n\n\n# TODO: Use global logger\ndef deprecation_warning(msg, *, printer=None, stacklevel=0, **kwargs):\n    from .. import _IN_CLI\n    if _IN_CLI:\n        if msg in deprecation_warning._cache:\n            return\n        deprecation_warning._cache.add(msg)\n        if printer:\n            return printer(f'{msg}{bug_reports_message()}', **kwargs)\n        return write_string(f'ERROR: {msg}{bug_reports_message()}\\n', **kwargs)\n    else:\n        import warnings\n        warnings.warn(DeprecationWarning(msg), stacklevel=stacklevel + 3)\n\n\ndeprecation_warning._cache = set()\n\n\ndef bytes_to_intlist(bs):\n    if not bs:\n        return []\n    if isinstance(bs[0], int):  # Python 3\n        return list(bs)\n    else:\n        return [ord(c) for c in bs]\n\n\ndef intlist_to_bytes(xs):\n    if not xs:\n        return b''\n    return struct.pack('%dB' % len(xs), *xs)\n\n\nclass LockingUnsupportedError(OSError):\n    msg = 'File locking is not supported'\n\n    def __init__(self):\n        super().__init__(self.msg)\n\n\n# Cross-platform file locking\nif sys.platform == 'win32':\n    import ctypes\n    import ctypes.wintypes\n    import msvcrt\n\n    class OVERLAPPED(ctypes.Structure):\n        _fields_ = [\n            ('Internal', ctypes.wintypes.LPVOID),\n            ('InternalHigh', ctypes.wintypes.LPVOID),\n            ('Offset', ctypes.wintypes.DWORD),\n            ('OffsetHigh', ctypes.wintypes.DWORD),\n            ('hEvent', ctypes.wintypes.HANDLE),\n        ]\n\n    kernel32 = ctypes.WinDLL('kernel32')\n    LockFileEx = kernel32.LockFileEx\n    LockFileEx.argtypes = [\n        ctypes.wintypes.HANDLE,     # hFile\n        ctypes.wintypes.DWORD,      # dwFlags\n        ctypes.wintypes.DWORD,      # dwReserved\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh\n        ctypes.POINTER(OVERLAPPED)  # Overlapped\n    ]\n    LockFileEx.restype = ctypes.wintypes.BOOL\n    UnlockFileEx = kernel32.UnlockFileEx\n    UnlockFileEx.argtypes = [\n        ctypes.wintypes.HANDLE,     # hFile\n        ctypes.wintypes.DWORD,      # dwReserved\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh\n        ctypes.POINTER(OVERLAPPED)  # Overlapped\n    ]\n    UnlockFileEx.restype = ctypes.wintypes.BOOL\n    whole_low = 0xffffffff\n    whole_high = 0x7fffffff\n\n    def _lock_file(f, exclusive, block):\n        overlapped = OVERLAPPED()\n        overlapped.Offset = 0\n        overlapped.OffsetHigh = 0\n        overlapped.hEvent = 0\n        f._lock_file_overlapped_p = ctypes.pointer(overlapped)\n\n        if not LockFileEx(msvcrt.get_osfhandle(f.fileno()),\n                          (0x2 if exclusive else 0x0) | (0x0 if block else 0x1),\n                          0, whole_low, whole_high, f._lock_file_overlapped_p):\n            # NB: No argument form of \"ctypes.FormatError\" does not work on PyPy\n            raise BlockingIOError(f'Locking file failed: {ctypes.FormatError(ctypes.GetLastError())!r}')\n\n    def _unlock_file(f):\n        assert f._lock_file_overlapped_p\n        handle = msvcrt.get_osfhandle(f.fileno())\n        if not UnlockFileEx(handle, 0, whole_low, whole_high, f._lock_file_overlapped_p):\n            raise OSError('Unlocking file failed: %r' % ctypes.FormatError())\n\nelse:\n    try:\n        import fcntl\n\n        def _lock_file(f, exclusive, block):\n            flags = fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH\n            if not block:\n                flags |= fcntl.LOCK_NB\n            try:\n                fcntl.flock(f, flags)\n            except BlockingIOError:\n                raise\n            except OSError:  # AOSP does not have flock()\n                fcntl.lockf(f, flags)\n\n        def _unlock_file(f):\n            with contextlib.suppress(OSError):\n                return fcntl.flock(f, fcntl.LOCK_UN)\n            with contextlib.suppress(OSError):\n                return fcntl.lockf(f, fcntl.LOCK_UN)  # AOSP does not have flock()\n            return fcntl.flock(f, fcntl.LOCK_UN | fcntl.LOCK_NB)  # virtiofs needs LOCK_NB on unlocking\n\n    except ImportError:\n\n        def _lock_file(f, exclusive, block):\n            raise LockingUnsupportedError()\n\n        def _unlock_file(f):\n            raise LockingUnsupportedError()\n\n\nclass locked_file:\n    locked = False\n\n    def __init__(self, filename, mode, block=True, encoding=None):\n        if mode not in {'r', 'rb', 'a', 'ab', 'w', 'wb'}:\n            raise NotImplementedError(mode)\n        self.mode, self.block = mode, block\n\n        writable = any(f in mode for f in 'wax+')\n        readable = any(f in mode for f in 'r+')\n        flags = functools.reduce(operator.ior, (\n            getattr(os, 'O_CLOEXEC', 0),  # UNIX only\n            getattr(os, 'O_BINARY', 0),  # Windows only\n            getattr(os, 'O_NOINHERIT', 0),  # Windows only\n            os.O_CREAT if writable else 0,  # O_TRUNC only after locking\n            os.O_APPEND if 'a' in mode else 0,\n            os.O_EXCL if 'x' in mode else 0,\n            os.O_RDONLY if not writable else os.O_RDWR if readable else os.O_WRONLY,\n        ))\n\n        self.f = os.fdopen(os.open(filename, flags, 0o666), mode, encoding=encoding)\n\n    def __enter__(self):\n        exclusive = 'r' not in self.mode\n        try:\n            _lock_file(self.f, exclusive, self.block)\n            self.locked = True\n        except OSError:\n            self.f.close()\n            raise\n        if 'w' in self.mode:\n            try:\n                self.f.truncate()\n            except OSError as e:\n                if e.errno not in (\n                    errno.ESPIPE,  # Illegal seek - expected for FIFO\n                    errno.EINVAL,  # Invalid argument - expected for /dev/null\n                ):\n                    raise\n        return self\n\n    def unlock(self):\n        if not self.locked:\n            return\n        try:\n            _unlock_file(self.f)\n        finally:\n            self.locked = False\n\n    def __exit__(self, *_):\n        try:\n            self.unlock()\n        finally:\n            self.f.close()\n\n    open = __enter__\n    close = __exit__\n\n    def __getattr__(self, attr):\n        return getattr(self.f, attr)\n\n    def __iter__(self):\n        return iter(self.f)\n\n\n@functools.cache\ndef get_filesystem_encoding():\n    encoding = sys.getfilesystemencoding()\n    return encoding if encoding is not None else 'utf-8'\n\n\ndef shell_quote(args):\n    quoted_args = []\n    encoding = get_filesystem_encoding()\n    for a in args:\n        if isinstance(a, bytes):\n            # We may get a filename encoded with 'encodeFilename'\n            a = a.decode(encoding)\n        quoted_args.append(compat_shlex_quote(a))\n    return ' '.join(quoted_args)\n\n\ndef smuggle_url(url, data):\n    \"\"\" Pass additional data in a URL for internal use. \"\"\"\n\n    url, idata = unsmuggle_url(url, {})\n    data.update(idata)\n    sdata = urllib.parse.urlencode(\n        {'__youtubedl_smuggle': json.dumps(data)})\n    return url + '#' + sdata\n\n\ndef unsmuggle_url(smug_url, default=None):\n    if '#__youtubedl_smuggle' not in smug_url:\n        return smug_url, default\n    url, _, sdata = smug_url.rpartition('#')\n    jsond = urllib.parse.parse_qs(sdata)['__youtubedl_smuggle'][0]\n    data = json.loads(jsond)\n    return url, data\n\n\ndef format_decimal_suffix(num, fmt='%d%s', *, factor=1000):\n    \"\"\" Formats numbers with decimal sufixes like K, M, etc \"\"\"\n    num, factor = float_or_none(num), float(factor)\n    if num is None or num < 0:\n        return None\n    POSSIBLE_SUFFIXES = 'kMGTPEZY'\n    exponent = 0 if num == 0 else min(int(math.log(num, factor)), len(POSSIBLE_SUFFIXES))\n    suffix = ['', *POSSIBLE_SUFFIXES][exponent]\n    if factor == 1024:\n        suffix = {'k': 'Ki', '': ''}.get(suffix, f'{suffix}i')\n    converted = num / (factor ** exponent)\n    return fmt % (converted, suffix)\n\n\ndef format_bytes(bytes):\n    return format_decimal_suffix(bytes, '%.2f%sB', factor=1024) or 'N/A'\n\n\ndef lookup_unit_table(unit_table, s, strict=False):\n    num_re = NUMBER_RE if strict else NUMBER_RE.replace(R'\\.', '[,.]')\n    units_re = '|'.join(re.escape(u) for u in unit_table)\n    m = (re.fullmatch if strict else re.match)(\n        rf'(?P<num>{num_re})\\s*(?P<unit>{units_re})\\b', s)\n    if not m:\n        return None\n\n    num = float(m.group('num').replace(',', '.'))\n    mult = unit_table[m.group('unit')]\n    return round(num * mult)\n\n\ndef parse_bytes(s):\n    \"\"\"Parse a string indicating a byte quantity into an integer\"\"\"\n    return lookup_unit_table(\n        {u: 1024**i for i, u in enumerate(['', *'KMGTPEZY'])},\n        s.upper(), strict=True)\n\n\ndef parse_filesize(s):\n    if s is None:\n        return None\n\n    # The lower-case forms are of course incorrect and unofficial,\n    # but we support those too\n    _UNIT_TABLE = {\n        'B': 1,\n        'b': 1,\n        'bytes': 1,\n        'KiB': 1024,\n        'KB': 1000,\n        'kB': 1024,\n        'Kb': 1000,\n        'kb': 1000,\n        'kilobytes': 1000,\n        'kibibytes': 1024,\n        'MiB': 1024 ** 2,\n        'MB': 1000 ** 2,\n        'mB': 1024 ** 2,\n        'Mb': 1000 ** 2,\n        'mb': 1000 ** 2,\n        'megabytes': 1000 ** 2,\n        'mebibytes': 1024 ** 2,\n        'GiB': 1024 ** 3,\n        'GB': 1000 ** 3,\n        'gB': 1024 ** 3,\n        'Gb': 1000 ** 3,\n        'gb': 1000 ** 3,\n        'gigabytes': 1000 ** 3,\n        'gibibytes': 1024 ** 3,\n        'TiB': 1024 ** 4,\n        'TB': 1000 ** 4,\n        'tB': 1024 ** 4,\n        'Tb': 1000 ** 4,\n        'tb': 1000 ** 4,\n        'terabytes': 1000 ** 4,\n        'tebibytes': 1024 ** 4,\n        'PiB': 1024 ** 5,\n        'PB': 1000 ** 5,\n        'pB': 1024 ** 5,\n        'Pb': 1000 ** 5,\n        'pb': 1000 ** 5,\n        'petabytes': 1000 ** 5,\n        'pebibytes': 1024 ** 5,\n        'EiB': 1024 ** 6,\n        'EB': 1000 ** 6,\n        'eB': 1024 ** 6,\n        'Eb': 1000 ** 6,\n        'eb': 1000 ** 6,\n        'exabytes': 1000 ** 6,\n        'exbibytes': 1024 ** 6,\n        'ZiB': 1024 ** 7,\n        'ZB': 1000 ** 7,\n        'zB': 1024 ** 7,\n        'Zb': 1000 ** 7,\n        'zb': 1000 ** 7,\n        'zettabytes': 1000 ** 7,\n        'zebibytes': 1024 ** 7,\n        'YiB': 1024 ** 8,\n        'YB': 1000 ** 8,\n        'yB': 1024 ** 8,\n        'Yb': 1000 ** 8,\n        'yb': 1000 ** 8,\n        'yottabytes': 1000 ** 8,\n        'yobibytes': 1024 ** 8,\n    }\n\n    return lookup_unit_table(_UNIT_TABLE, s)\n\n\ndef parse_count(s):\n    if s is None:\n        return None\n\n    s = re.sub(r'^[^\\d]+\\s', '', s).strip()\n\n    if re.match(r'^[\\d,.]+$', s):\n        return str_to_int(s)\n\n    _UNIT_TABLE = {\n        'k': 1000,\n        'K': 1000,\n        'm': 1000 ** 2,\n        'M': 1000 ** 2,\n        'kk': 1000 ** 2,\n        'KK': 1000 ** 2,\n        'b': 1000 ** 3,\n        'B': 1000 ** 3,\n    }\n\n    ret = lookup_unit_table(_UNIT_TABLE, s)\n    if ret is not None:\n        return ret\n\n    mobj = re.match(r'([\\d,.]+)(?:$|\\s)', s)\n    if mobj:\n        return str_to_int(mobj.group(1))\n\n\ndef parse_resolution(s, *, lenient=False):\n    if s is None:\n        return {}\n\n    if lenient:\n        mobj = re.search(r'(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)', s)\n    else:\n        mobj = re.search(r'(?<![a-zA-Z0-9])(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)(?![a-zA-Z0-9])', s)\n    if mobj:\n        return {\n            'width': int(mobj.group('w')),\n            'height': int(mobj.group('h')),\n        }\n\n    mobj = re.search(r'(?<![a-zA-Z0-9])(\\d+)[pPiI](?![a-zA-Z0-9])', s)\n    if mobj:\n        return {'height': int(mobj.group(1))}\n\n    mobj = re.search(r'\\b([48])[kK]\\b', s)\n    if mobj:\n        return {'height': int(mobj.group(1)) * 540}\n\n    return {}\n\n\ndef parse_bitrate(s):\n    if not isinstance(s, str):\n        return\n    mobj = re.search(r'\\b(\\d+)\\s*kbps', s)\n    if mobj:\n        return int(mobj.group(1))\n\n\ndef month_by_name(name, lang='en'):\n    \"\"\" Return the number of a month by (locale-independently) English name \"\"\"\n\n    month_names = MONTH_NAMES.get(lang, MONTH_NAMES['en'])\n\n    try:\n        return month_names.index(name) + 1\n    except ValueError:\n        return None\n\n\ndef month_by_abbreviation(abbrev):\n    \"\"\" Return the number of a month by (locale-independently) English\n        abbreviations \"\"\"\n\n    try:\n        return [s[:3] for s in ENGLISH_MONTH_NAMES].index(abbrev) + 1\n    except ValueError:\n        return None\n\n\ndef fix_xml_ampersands(xml_str):\n    \"\"\"Replace all the '&' by '&amp;' in XML\"\"\"\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',\n        '&amp;',\n        xml_str)\n\n\ndef setproctitle(title):\n    assert isinstance(title, str)\n\n    # Workaround for https://github.com/yt-dlp/yt-dlp/issues/4541\n    try:\n        import ctypes\n    except ImportError:\n        return\n\n    try:\n        libc = ctypes.cdll.LoadLibrary('libc.so.6')\n    except OSError:\n        return\n    except TypeError:\n        # LoadLibrary in Windows Python 2.7.13 only expects\n        # a bytestring, but since unicode_literals turns\n        # every string into a unicode string, it fails.\n        return\n    title_bytes = title.encode()\n    buf = ctypes.create_string_buffer(len(title_bytes))\n    buf.value = title_bytes\n    try:\n        libc.prctl(15, buf, 0, 0, 0)\n    except AttributeError:\n        return  # Strange libc, just skip this\n\n\ndef remove_start(s, start):\n    return s[len(start):] if s is not None and s.startswith(start) else s\n\n\ndef remove_end(s, end):\n    return s[:-len(end)] if s is not None and s.endswith(end) else s\n\n\ndef remove_quotes(s):\n    if s is None or len(s) < 2:\n        return s\n    for quote in ('\"', \"'\", ):\n        if s[0] == quote and s[-1] == quote:\n            return s[1:-1]\n    return s\n\n\ndef get_domain(url):\n    \"\"\"\n    This implementation is inconsistent, but is kept for compatibility.\n    Use this only for \"webpage_url_domain\"\n    \"\"\"\n    return remove_start(urllib.parse.urlparse(url).netloc, 'www.') or None\n\n\ndef url_basename(url):\n    path = urllib.parse.urlparse(url).path\n    return path.strip('/').split('/')[-1]\n\n\ndef base_url(url):\n    return re.match(r'https?://[^?#]+/', url).group()\n\n\ndef urljoin(base, path):\n    if isinstance(path, bytes):\n        path = path.decode()\n    if not isinstance(path, str) or not path:\n        return None\n    if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):\n        return path\n    if isinstance(base, bytes):\n        base = base.decode()\n    if not isinstance(base, str) or not re.match(\n            r'^(?:https?:)?//', base):\n        return None\n    return urllib.parse.urljoin(base, path)\n\n\ndef int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):\n    if get_attr and v is not None:\n        v = getattr(v, get_attr, None)\n    try:\n        return int(v) * invscale // scale\n    except (ValueError, TypeError, OverflowError):\n        return default\n\n\ndef str_or_none(v, default=None):\n    return default if v is None else str(v)\n\n\ndef str_to_int(int_str):\n    \"\"\" A more relaxed version of int_or_none \"\"\"\n    if isinstance(int_str, int):\n        return int_str\n    elif isinstance(int_str, str):\n        int_str = re.sub(r'[,\\.\\+]', '', int_str)\n        return int_or_none(int_str)\n\n\ndef float_or_none(v, scale=1, invscale=1, default=None):\n    if v is None:\n        return default\n    try:\n        return float(v) * invscale / scale\n    except (ValueError, TypeError):\n        return default\n\n\ndef bool_or_none(v, default=None):\n    return v if isinstance(v, bool) else default\n\n\ndef strip_or_none(v, default=None):\n    return v.strip() if isinstance(v, str) else default\n\n\ndef url_or_none(url):\n    if not url or not isinstance(url, str):\n        return None\n    url = url.strip()\n    return url if re.match(r'^(?:(?:https?|rt(?:m(?:pt?[es]?|fp)|sp[su]?)|mms|ftps?):)?//', url) else None\n\n\ndef strftime_or_none(timestamp, date_format='%Y%m%d', default=None):\n    datetime_object = None\n    try:\n        if isinstance(timestamp, (int, float)):  # unix timestamp\n            # Using naive datetime here can break timestamp() in Windows\n            # Ref: https://github.com/yt-dlp/yt-dlp/issues/5185, https://github.com/python/cpython/issues/94414\n            # Also, datetime.datetime.fromtimestamp breaks for negative timestamps\n            # Ref: https://github.com/yt-dlp/yt-dlp/issues/6706#issuecomment-1496842642\n            datetime_object = (datetime.datetime.fromtimestamp(0, datetime.timezone.utc)\n                               + datetime.timedelta(seconds=timestamp))\n        elif isinstance(timestamp, str):  # assume YYYYMMDD\n            datetime_object = datetime.datetime.strptime(timestamp, '%Y%m%d')\n        date_format = re.sub(  # Support %s on windows\n            r'(?<!%)(%%)*%s', rf'\\g<1>{int(datetime_object.timestamp())}', date_format)\n        return datetime_object.strftime(date_format)\n    except (ValueError, TypeError, AttributeError):\n        return default\n\n\ndef parse_duration(s):\n    if not isinstance(s, str):\n        return None\n    s = s.strip()\n    if not s:\n        return None\n\n    days, hours, mins, secs, ms = [None] * 5\n    m = re.match(r'''(?x)\n            (?P<before_secs>\n                (?:(?:(?P<days>[0-9]+):)?(?P<hours>[0-9]+):)?(?P<mins>[0-9]+):)?\n            (?P<secs>(?(before_secs)[0-9]{1,2}|[0-9]+))\n            (?P<ms>[.:][0-9]+)?Z?$\n        ''', s)\n    if m:\n        days, hours, mins, secs, ms = m.group('days', 'hours', 'mins', 'secs', 'ms')\n    else:\n        m = re.match(\n            r'''(?ix)(?:P?\n                (?:\n                    [0-9]+\\s*y(?:ears?)?,?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*m(?:onths?)?,?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*w(?:eeks?)?,?\\s*\n                )?\n                (?:\n                    (?P<days>[0-9]+)\\s*d(?:ays?)?,?\\s*\n                )?\n                T)?\n                (?:\n                    (?P<hours>[0-9]+)\\s*h(?:(?:ou)?rs?)?,?\\s*\n                )?\n                (?:\n                    (?P<mins>[0-9]+)\\s*m(?:in(?:ute)?s?)?,?\\s*\n                )?\n                (?:\n                    (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*s(?:ec(?:ond)?s?)?\\s*\n                )?Z?$''', s)\n        if m:\n            days, hours, mins, secs, ms = m.groups()\n        else:\n            m = re.match(r'(?i)(?:(?P<hours>[0-9.]+)\\s*(?:hours?)|(?P<mins>[0-9.]+)\\s*(?:mins?\\.?|minutes?)\\s*)Z?$', s)\n            if m:\n                hours, mins = m.groups()\n            else:\n                return None\n\n    if ms:\n        ms = ms.replace(':', '.')\n    return sum(float(part or 0) * mult for part, mult in (\n        (days, 86400), (hours, 3600), (mins, 60), (secs, 1), (ms, 1)))\n\n\ndef prepend_extension(filename, ext, expected_real_ext=None):\n    name, real_ext = os.path.splitext(filename)\n    return (\n        f'{name}.{ext}{real_ext}'\n        if not expected_real_ext or real_ext[1:] == expected_real_ext\n        else f'{filename}.{ext}')\n\n\ndef replace_extension(filename, ext, expected_real_ext=None):\n    name, real_ext = os.path.splitext(filename)\n    return '{}.{}'.format(\n        name if not expected_real_ext or real_ext[1:] == expected_real_ext else filename,\n        ext)\n\n\ndef check_executable(exe, args=[]):\n    \"\"\" Checks if the given binary is installed somewhere in PATH, and returns its name.\n    args can be a list of arguments for a short output (like -version) \"\"\"\n    try:\n        Popen.run([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    except OSError:\n        return False\n    return exe\n\n\ndef _get_exe_version_output(exe, args):\n    try:\n        # STDIN should be redirected too. On UNIX-like systems, ffmpeg triggers\n        # SIGTTOU if yt-dlp is run in the background.\n        # See https://github.com/ytdl-org/youtube-dl/issues/955#issuecomment-209789656\n        stdout, _, ret = Popen.run([encodeArgument(exe)] + args, text=True,\n                                   stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        if ret:\n            return None\n    except OSError:\n        return False\n    return stdout\n\n\ndef detect_exe_version(output, version_re=None, unrecognized='present'):\n    assert isinstance(output, str)\n    if version_re is None:\n        version_re = r'version\\s+([-0-9._a-zA-Z]+)'\n    m = re.search(version_re, output)\n    if m:\n        return m.group(1)\n    else:\n        return unrecognized\n\n\ndef get_exe_version(exe, args=['--version'],\n                    version_re=None, unrecognized=('present', 'broken')):\n    \"\"\" Returns the version of the specified executable,\n    or False if the executable is not present \"\"\"\n    unrecognized = variadic(unrecognized)\n    assert len(unrecognized) in (1, 2)\n    out = _get_exe_version_output(exe, args)\n    if out is None:\n        return unrecognized[-1]\n    return out and detect_exe_version(out, version_re, unrecognized[0])\n\n\ndef frange(start=0, stop=None, step=1):\n    \"\"\"Float range\"\"\"\n    if stop is None:\n        start, stop = 0, start\n    sign = [-1, 1][step > 0] if step else 0\n    while sign * start < sign * stop:\n        yield start\n        start += step\n\n\nclass LazyList(collections.abc.Sequence):\n    \"\"\"Lazy immutable list from an iterable\n    Note that slices of a LazyList are lists and not LazyList\"\"\"\n\n    class IndexError(IndexError):\n        pass\n\n    def __init__(self, iterable, *, reverse=False, _cache=None):\n        self._iterable = iter(iterable)\n        self._cache = [] if _cache is None else _cache\n        self._reversed = reverse\n\n    def __iter__(self):\n        if self._reversed:\n            # We need to consume the entire iterable to iterate in reverse\n            yield from self.exhaust()\n            return\n        yield from self._cache\n        for item in self._iterable:\n            self._cache.append(item)\n            yield item\n\n    def _exhaust(self):\n        self._cache.extend(self._iterable)\n        self._iterable = []  # Discard the emptied iterable to make it pickle-able\n        return self._cache\n\n    def exhaust(self):\n        \"\"\"Evaluate the entire iterable\"\"\"\n        return self._exhaust()[::-1 if self._reversed else 1]\n\n    @staticmethod\n    def _reverse_index(x):\n        return None if x is None else ~x\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            if self._reversed:\n                idx = slice(self._reverse_index(idx.start), self._reverse_index(idx.stop), -(idx.step or 1))\n            start, stop, step = idx.start, idx.stop, idx.step or 1\n        elif isinstance(idx, int):\n            if self._reversed:\n                idx = self._reverse_index(idx)\n            start, stop, step = idx, idx, 0\n        else:\n            raise TypeError('indices must be integers or slices')\n        if ((start or 0) < 0 or (stop or 0) < 0\n                or (start is None and step < 0)\n                or (stop is None and step > 0)):\n            # We need to consume the entire iterable to be able to slice from the end\n            # Obviously, never use this with infinite iterables\n            self._exhaust()\n            try:\n                return self._cache[idx]\n            except IndexError as e:\n                raise self.IndexError(e) from e\n        n = max(start or 0, stop or 0) - len(self._cache) + 1\n        if n > 0:\n            self._cache.extend(itertools.islice(self._iterable, n))\n        try:\n            return self._cache[idx]\n        except IndexError as e:\n            raise self.IndexError(e) from e\n\n    def __bool__(self):\n        try:\n            self[-1] if self._reversed else self[0]\n        except self.IndexError:\n            return False\n        return True\n\n    def __len__(self):\n        self._exhaust()\n        return len(self._cache)\n\n    def __reversed__(self):\n        return type(self)(self._iterable, reverse=not self._reversed, _cache=self._cache)\n\n    def __copy__(self):\n        return type(self)(self._iterable, reverse=self._reversed, _cache=self._cache)\n\n    def __repr__(self):\n        # repr and str should mimic a list. So we exhaust the iterable\n        return repr(self.exhaust())\n\n    def __str__(self):\n        return repr(self.exhaust())\n\n\nclass PagedList:\n\n    class IndexError(IndexError):\n        pass\n\n    def __len__(self):\n        # This is only useful for tests\n        return len(self.getslice())\n\n    def __init__(self, pagefunc, pagesize, use_cache=True):\n        self._pagefunc = pagefunc\n        self._pagesize = pagesize\n        self._pagecount = float('inf')\n        self._use_cache = use_cache\n        self._cache = {}\n\n    def getpage(self, pagenum):\n        page_results = self._cache.get(pagenum)\n        if page_results is None:\n            page_results = [] if pagenum > self._pagecount else list(self._pagefunc(pagenum))\n        if self._use_cache:\n            self._cache[pagenum] = page_results\n        return page_results\n\n    def getslice(self, start=0, end=None):\n        return list(self._getslice(start, end))\n\n    def _getslice(self, start, end):\n        raise NotImplementedError('This method must be implemented by subclasses')\n\n    def __getitem__(self, idx):\n        assert self._use_cache, 'Indexing PagedList requires cache'\n        if not isinstance(idx, int) or idx < 0:\n            raise TypeError('indices must be non-negative integers')\n        entries = self.getslice(idx, idx + 1)\n        if not entries:\n            raise self.IndexError()\n        return entries[0]\n\n\nclass OnDemandPagedList(PagedList):\n    \"\"\"Download pages until a page with less than maximum results\"\"\"\n\n    def _getslice(self, start, end):\n        for pagenum in itertools.count(start // self._pagesize):\n            firstid = pagenum * self._pagesize\n            nextfirstid = pagenum * self._pagesize + self._pagesize\n            if start >= nextfirstid:\n                continue\n\n            startv = (\n                start % self._pagesize\n                if firstid <= start < nextfirstid\n                else 0)\n            endv = (\n                ((end - 1) % self._pagesize) + 1\n                if (end is not None and firstid <= end <= nextfirstid)\n                else None)\n\n            try:\n                page_results = self.getpage(pagenum)\n            except Exception:\n                self._pagecount = pagenum - 1\n                raise\n            if startv != 0 or endv is not None:\n                page_results = page_results[startv:endv]\n            yield from page_results\n\n            # A little optimization - if current page is not \"full\", ie. does\n            # not contain page_size videos then we can assume that this page\n            # is the last one - there are no more ids on further pages -\n            # i.e. no need to query again.\n            if len(page_results) + startv < self._pagesize:\n                break\n\n            # If we got the whole page, but the next page is not interesting,\n            # break out early as well\n            if end == nextfirstid:\n                break\n\n\nclass InAdvancePagedList(PagedList):\n    \"\"\"PagedList with total number of pages known in advance\"\"\"\n\n    def __init__(self, pagefunc, pagecount, pagesize):\n        PagedList.__init__(self, pagefunc, pagesize, True)\n        self._pagecount = pagecount\n\n    def _getslice(self, start, end):\n        start_page = start // self._pagesize\n        end_page = self._pagecount if end is None else min(self._pagecount, end // self._pagesize + 1)\n        skip_elems = start - start_page * self._pagesize\n        only_more = None if end is None else end - start\n        for pagenum in range(start_page, end_page):\n            page_results = self.getpage(pagenum)\n            if skip_elems:\n                page_results = page_results[skip_elems:]\n                skip_elems = None\n            if only_more is not None:\n                if len(page_results) < only_more:\n                    only_more -= len(page_results)\n                else:\n                    yield from page_results[:only_more]\n                    break\n            yield from page_results\n\n\nclass PlaylistEntries:\n    MissingEntry = object()\n    is_exhausted = False\n\n    def __init__(self, ydl, info_dict):\n        self.ydl = ydl\n\n        # _entries must be assigned now since infodict can change during iteration\n        entries = info_dict.get('entries')\n        if entries is None:\n            raise EntryNotInPlaylist('There are no entries')\n        elif isinstance(entries, list):\n            self.is_exhausted = True\n\n        requested_entries = info_dict.get('requested_entries')\n        self.is_incomplete = requested_entries is not None\n        if self.is_incomplete:\n            assert self.is_exhausted\n            self._entries = [self.MissingEntry] * max(requested_entries or [0])\n            for i, entry in zip(requested_entries, entries):\n                self._entries[i - 1] = entry\n        elif isinstance(entries, (list, PagedList, LazyList)):\n            self._entries = entries\n        else:\n            self._entries = LazyList(entries)\n\n    PLAYLIST_ITEMS_RE = re.compile(r'''(?x)\n        (?P<start>[+-]?\\d+)?\n        (?P<range>[:-]\n            (?P<end>[+-]?\\d+|inf(?:inite)?)?\n            (?::(?P<step>[+-]?\\d+))?\n        )?''')\n\n    @classmethod\n    def parse_playlist_items(cls, string):\n        for segment in string.split(','):\n            if not segment:\n                raise ValueError('There is two or more consecutive commas')\n            mobj = cls.PLAYLIST_ITEMS_RE.fullmatch(segment)\n            if not mobj:\n                raise ValueError(f'{segment!r} is not a valid specification')\n            start, end, step, has_range = mobj.group('start', 'end', 'step', 'range')\n            if int_or_none(step) == 0:\n                raise ValueError(f'Step in {segment!r} cannot be zero')\n            yield slice(int_or_none(start), float_or_none(end), int_or_none(step)) if has_range else int(start)\n\n    def get_requested_items(self):\n        playlist_items = self.ydl.params.get('playlist_items')\n        playlist_start = self.ydl.params.get('playliststart', 1)\n        playlist_end = self.ydl.params.get('playlistend')\n        # For backwards compatibility, interpret -1 as whole list\n        if playlist_end in (-1, None):\n            playlist_end = ''\n        if not playlist_items:\n            playlist_items = f'{playlist_start}:{playlist_end}'\n        elif playlist_start != 1 or playlist_end:\n            self.ydl.report_warning('Ignoring playliststart and playlistend because playlistitems was given', only_once=True)\n\n        for index in self.parse_playlist_items(playlist_items):\n            for i, entry in self[index]:\n                yield i, entry\n                if not entry:\n                    continue\n                try:\n                    # The item may have just been added to archive. Don't break due to it\n                    if not self.ydl.params.get('lazy_playlist'):\n                        # TODO: Add auto-generated fields\n                        self.ydl._match_entry(entry, incomplete=True, silent=True)\n                except (ExistingVideoReached, RejectedVideoReached):\n                    return\n\n    def get_full_count(self):\n        if self.is_exhausted and not self.is_incomplete:\n            return len(self)\n        elif isinstance(self._entries, InAdvancePagedList):\n            if self._entries._pagesize == 1:\n                return self._entries._pagecount\n\n    @functools.cached_property\n    def _getter(self):\n        if isinstance(self._entries, list):\n            def get_entry(i):\n                try:\n                    entry = self._entries[i]\n                except IndexError:\n                    entry = self.MissingEntry\n                    if not self.is_incomplete:\n                        raise self.IndexError()\n                if entry is self.MissingEntry:\n                    raise EntryNotInPlaylist(f'Entry {i + 1} cannot be found')\n                return entry\n        else:\n            def get_entry(i):\n                try:\n                    return type(self.ydl)._handle_extraction_exceptions(lambda _, i: self._entries[i])(self.ydl, i)\n                except (LazyList.IndexError, PagedList.IndexError):\n                    raise self.IndexError()\n        return get_entry\n\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            idx = slice(idx, idx)\n\n        # NB: PlaylistEntries[1:10] => (0, 1, ... 9)\n        step = 1 if idx.step is None else idx.step\n        if idx.start is None:\n            start = 0 if step > 0 else len(self) - 1\n        else:\n            start = idx.start - 1 if idx.start >= 0 else len(self) + idx.start\n\n        # NB: Do not call len(self) when idx == [:]\n        if idx.stop is None:\n            stop = 0 if step < 0 else float('inf')\n        else:\n            stop = idx.stop - 1 if idx.stop >= 0 else len(self) + idx.stop\n        stop += [-1, 1][step > 0]\n\n        for i in frange(start, stop, step):\n            if i < 0:\n                continue\n            try:\n                entry = self._getter(i)\n            except self.IndexError:\n                self.is_exhausted = True\n                if step > 0:\n                    break\n                continue\n            yield i + 1, entry\n\n    def __len__(self):\n        return len(tuple(self[:]))\n\n    class IndexError(IndexError):\n        pass\n\n\ndef uppercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\U[0-9a-fA-F]{8}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)\n\n\ndef lowercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\u[0-9a-fA-F]{4}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)\n\n\ndef parse_qs(url, **kwargs):\n    return urllib.parse.parse_qs(urllib.parse.urlparse(url).query, **kwargs)\n\n\ndef read_batch_urls(batch_fd):\n    def fixup(url):\n        if not isinstance(url, str):\n            url = url.decode('utf-8', 'replace')\n        BOM_UTF8 = ('\\xef\\xbb\\xbf', '\\ufeff')\n        for bom in BOM_UTF8:\n            if url.startswith(bom):\n                url = url[len(bom):]\n        url = url.lstrip()\n        if not url or url.startswith(('#', ';', ']')):\n            return False\n        # \"#\" cannot be stripped out since it is part of the URI\n        # However, it can be safely stripped out if following a whitespace\n        return re.split(r'\\s#', url, 1)[0].rstrip()\n\n    with contextlib.closing(batch_fd) as fd:\n        return [url for url in map(fixup, fd) if url]\n\n\ndef urlencode_postdata(*args, **kargs):\n    return urllib.parse.urlencode(*args, **kargs).encode('ascii')\n\n\ndef update_url(url, *, query_update=None, **kwargs):\n    \"\"\"Replace URL components specified by kwargs\n       @param url           str or parse url tuple\n       @param query_update  update query\n       @returns             str\n    \"\"\"\n    if isinstance(url, str):\n        if not kwargs and not query_update:\n            return url\n        else:\n            url = urllib.parse.urlparse(url)\n    if query_update:\n        assert 'query' not in kwargs, 'query_update and query cannot be specified at the same time'\n        kwargs['query'] = urllib.parse.urlencode({\n            **urllib.parse.parse_qs(url.query),\n            **query_update\n        }, True)\n    return urllib.parse.urlunparse(url._replace(**kwargs))\n\n\ndef update_url_query(url, query):\n    return update_url(url, query_update=query)\n\n\ndef _multipart_encode_impl(data, boundary):\n    content_type = 'multipart/form-data; boundary=%s' % boundary\n\n    out = b''\n    for k, v in data.items():\n        out += b'--' + boundary.encode('ascii') + b'\\r\\n'\n        if isinstance(k, str):\n            k = k.encode()\n        if isinstance(v, str):\n            v = v.encode()\n        # RFC 2047 requires non-ASCII field names to be encoded, while RFC 7578\n        # suggests sending UTF-8 directly. Firefox sends UTF-8, too\n        content = b'Content-Disposition: form-data; name=\"' + k + b'\"\\r\\n\\r\\n' + v + b'\\r\\n'\n        if boundary.encode('ascii') in content:\n            raise ValueError('Boundary overlaps with data')\n        out += content\n\n    out += b'--' + boundary.encode('ascii') + b'--\\r\\n'\n\n    return out, content_type\n\n\ndef multipart_encode(data, boundary=None):\n    '''\n    Encode a dict to RFC 7578-compliant form-data\n\n    data:\n        A dict where keys and values can be either Unicode or bytes-like\n        objects.\n    boundary:\n        If specified a Unicode object, it's used as the boundary. Otherwise\n        a random boundary is generated.\n\n    Reference: https://tools.ietf.org/html/rfc7578\n    '''\n    has_specified_boundary = boundary is not None\n\n    while True:\n        if boundary is None:\n            boundary = '---------------' + str(random.randrange(0x0fffffff, 0xffffffff))\n\n        try:\n            out, content_type = _multipart_encode_impl(data, boundary)\n            break\n        except ValueError:\n            if has_specified_boundary:\n                raise\n            boundary = None\n\n    return out, content_type\n\n\ndef is_iterable_like(x, allowed_types=collections.abc.Iterable, blocked_types=NO_DEFAULT):\n    if blocked_types is NO_DEFAULT:\n        blocked_types = (str, bytes, collections.abc.Mapping)\n    return isinstance(x, allowed_types) and not isinstance(x, blocked_types)\n\n\ndef variadic(x, allowed_types=NO_DEFAULT):\n    if not isinstance(allowed_types, (tuple, type)):\n        deprecation_warning('allowed_types should be a tuple or a type')\n        allowed_types = tuple(allowed_types)\n    return x if is_iterable_like(x, blocked_types=allowed_types) else (x, )\n\n\ndef try_call(*funcs, expected_type=None, args=[], kwargs={}):\n    for f in funcs:\n        try:\n            val = f(*args, **kwargs)\n        except (AttributeError, KeyError, TypeError, IndexError, ValueError, ZeroDivisionError):\n            pass\n        else:\n            if expected_type is None or isinstance(val, expected_type):\n                return val\n\n\ndef try_get(src, getter, expected_type=None):\n    return try_call(*variadic(getter), args=(src,), expected_type=expected_type)\n\n\ndef filter_dict(dct, cndn=lambda _, v: v is not None):\n    return {k: v for k, v in dct.items() if cndn(k, v)}\n\n\ndef merge_dicts(*dicts):\n    merged = {}\n    for a_dict in dicts:\n        for k, v in a_dict.items():\n            if (v is not None and k not in merged\n                    or isinstance(v, str) and merged[k] == ''):\n                merged[k] = v\n    return merged\n\n\ndef encode_compat_str(string, encoding=preferredencoding(), errors='strict'):\n    return string if isinstance(string, str) else str(string, encoding, errors)\n\n\nUS_RATINGS = {\n    'G': 0,\n    'PG': 10,\n    'PG-13': 13,\n    'R': 16,\n    'NC': 18,\n}\n\n\nTV_PARENTAL_GUIDELINES = {\n    'TV-Y': 0,\n    'TV-Y7': 7,\n    'TV-G': 0,\n    'TV-PG': 0,\n    'TV-14': 14,\n    'TV-MA': 17,\n}\n\n\ndef parse_age_limit(s):\n    # isinstance(False, int) is True. So type() must be used instead\n    if type(s) is int:  # noqa: E721\n        return s if 0 <= s <= 21 else None\n    elif not isinstance(s, str):\n        return None\n    m = re.match(r'^(?P<age>\\d{1,2})\\+?$', s)\n    if m:\n        return int(m.group('age'))\n    s = s.upper()\n    if s in US_RATINGS:\n        return US_RATINGS[s]\n    m = re.match(r'^TV[_-]?(%s)$' % '|'.join(k[3:] for k in TV_PARENTAL_GUIDELINES), s)\n    if m:\n        return TV_PARENTAL_GUIDELINES['TV-' + m.group(1)]\n    return None\n\n\ndef strip_jsonp(code):\n    return re.sub(\n        r'''(?sx)^\n            (?:window\\.)?(?P<func_name>[a-zA-Z0-9_.$]*)\n            (?:\\s*&&\\s*(?P=func_name))?\n            \\s*\\(\\s*(?P<callback_data>.*)\\);?\n            \\s*?(?://[^\\n]*)*$''',\n        r'\\g<callback_data>', code)\n\n\ndef js_to_json(code, vars={}, *, strict=False):\n    # vars is a dict of var, val pairs to substitute\n    STRING_QUOTES = '\\'\"`'\n    STRING_RE = '|'.join(rf'{q}(?:\\\\.|[^\\\\{q}])*{q}' for q in STRING_QUOTES)\n    COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*\\n'\n    SKIP_RE = fr'\\s*(?:{COMMENT_RE})?\\s*'\n    INTEGER_TABLE = (\n        (fr'(?s)^(0[xX][0-9a-fA-F]+){SKIP_RE}:?$', 16),\n        (fr'(?s)^(0+[0-7]+){SKIP_RE}:?$', 8),\n    )\n\n    def process_escape(match):\n        JSON_PASSTHROUGH_ESCAPES = R'\"\\bfnrtu'\n        escape = match.group(1) or match.group(2)\n\n        return (Rf'\\{escape}' if escape in JSON_PASSTHROUGH_ESCAPES\n                else R'\\u00' if escape == 'x'\n                else '' if escape == '\\n'\n                else escape)\n\n    def template_substitute(match):\n        evaluated = js_to_json(match.group(1), vars, strict=strict)\n        if evaluated[0] == '\"':\n            return json.loads(evaluated)\n        return evaluated\n\n    def fix_kv(m):\n        v = m.group(0)\n        if v in ('true', 'false', 'null'):\n            return v\n        elif v in ('undefined', 'void 0'):\n            return 'null'\n        elif v.startswith('/*') or v.startswith('//') or v.startswith('!') or v == ',':\n            return ''\n\n        if v[0] in STRING_QUOTES:\n            v = re.sub(r'(?s)\\${([^}]+)}', template_substitute, v[1:-1]) if v[0] == '`' else v[1:-1]\n            escaped = re.sub(r'(?s)(\")|\\\\(.)', process_escape, v)\n            return f'\"{escaped}\"'\n\n        for regex, base in INTEGER_TABLE:\n            im = re.match(regex, v)\n            if im:\n                i = int(im.group(1), base)\n                return f'\"{i}\":' if v.endswith(':') else str(i)\n\n        if v in vars:\n            try:\n                if not strict:\n                    json.loads(vars[v])\n            except json.JSONDecodeError:\n                return json.dumps(vars[v])\n            else:\n                return vars[v]\n\n        if not strict:\n            return f'\"{v}\"'\n\n        raise ValueError(f'Unknown value: {v}')\n\n    def create_map(mobj):\n        return json.dumps(dict(json.loads(js_to_json(mobj.group(1) or '[]', vars=vars))))\n\n    code = re.sub(r'(?:new\\s+)?Array\\((.*?)\\)', r'[\\g<1>]', code)\n    code = re.sub(r'new Map\\((\\[.*?\\])?\\)', create_map, code)\n    if not strict:\n        code = re.sub(r'new Date\\((\".+\")\\)', r'\\g<1>', code)\n        code = re.sub(r'new \\w+\\((.*?)\\)', lambda m: json.dumps(m.group(0)), code)\n        code = re.sub(r'parseInt\\([^\\d]+(\\d+)[^\\d]+\\)', r'\\1', code)\n        code = re.sub(r'\\(function\\([^)]*\\)\\s*\\{[^}]*\\}\\s*\\)\\s*\\(\\s*([\"\\'][^)]*[\"\\'])\\s*\\)', r'\\1', code)\n\n    return re.sub(rf'''(?sx)\n        {STRING_RE}|\n        {COMMENT_RE}|,(?={SKIP_RE}[\\]}}])|\n        void\\s0|(?:(?<![0-9])[eE]|[a-df-zA-DF-Z_$])[.a-zA-Z_$0-9]*|\n        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{SKIP_RE}:)?|\n        [0-9]+(?={SKIP_RE}:)|\n        !+\n        ''', fix_kv, code)\n\n\ndef qualities(quality_ids):\n    \"\"\" Get a numeric quality value out of a list of possible values \"\"\"\n    def q(qid):\n        try:\n            return quality_ids.index(qid)\n        except ValueError:\n            return -1\n    return q\n\n\nPOSTPROCESS_WHEN = ('pre_process', 'after_filter', 'video', 'before_dl', 'post_process', 'after_move', 'after_video', 'playlist')\n\n\nDEFAULT_OUTTMPL = {\n    'default': '%(title)s [%(id)s].%(ext)s',\n    'chapter': '%(title)s - %(section_number)03d %(section_title)s [%(id)s].%(ext)s',\n}\nOUTTMPL_TYPES = {\n    'chapter': None,\n    'subtitle': None,\n    'thumbnail': None,\n    'description': 'description',\n    'annotation': 'annotations.xml',\n    'infojson': 'info.json',\n    'link': None,\n    'pl_video': None,\n    'pl_thumbnail': None,\n    'pl_description': 'description',\n    'pl_infojson': 'info.json',\n}\n\n# As of [1] format syntax is:\n#  %[mapping_key][conversion_flags][minimum_width][.precision][length_modifier]type\n# 1. https://docs.python.org/2/library/stdtypes.html#string-formatting\nSTR_FORMAT_RE_TMPL = r'''(?x)\n    (?<!%)(?P<prefix>(?:%%)*)\n    %\n    (?P<has_key>\\((?P<key>{0})\\))?\n    (?P<format>\n        (?P<conversion>[#0\\-+ ]+)?\n        (?P<min_width>\\d+)?\n        (?P<precision>\\.\\d+)?\n        (?P<len_mod>[hlL])?  # unused in python\n        {1}  # conversion type\n    )\n'''\n\n\nSTR_FORMAT_TYPES = 'diouxXeEfFgGcrsa'\n\n\ndef limit_length(s, length):\n    \"\"\" Add ellipses to overly long strings \"\"\"\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s\n\n\ndef version_tuple(v):\n    return tuple(int(e) for e in re.split(r'[-.]', v))\n\n\ndef is_outdated_version(version, limit, assume_new=True):\n    if not version:\n        return not assume_new\n    try:\n        return version_tuple(version) < version_tuple(limit)\n    except ValueError:\n        return not assume_new\n\n\ndef ytdl_is_updateable():\n    \"\"\" Returns if yt-dlp can be updated with -U \"\"\"\n\n    from ..update import is_non_updateable\n\n    return not is_non_updateable()\n\n\ndef args_to_str(args):\n    # Get a short string representation for a subprocess command\n    return ' '.join(compat_shlex_quote(a) for a in args)\n\n\ndef error_to_str(err):\n    return f'{type(err).__name__}: {err}'\n\n\ndef mimetype2ext(mt, default=NO_DEFAULT):\n    if not isinstance(mt, str):\n        if default is not NO_DEFAULT:\n            return default\n        return None\n\n    MAP = {\n        # video\n        '3gpp': '3gp',\n        'mp2t': 'ts',\n        'mp4': 'mp4',\n        'mpeg': 'mpeg',\n        'mpegurl': 'm3u8',\n        'quicktime': 'mov',\n        'webm': 'webm',\n        'vp9': 'vp9',\n        'video/ogg': 'ogv',\n        'x-flv': 'flv',\n        'x-m4v': 'm4v',\n        'x-matroska': 'mkv',\n        'x-mng': 'mng',\n        'x-mp4-fragmented': 'mp4',\n        'x-ms-asf': 'asf',\n        'x-ms-wmv': 'wmv',\n        'x-msvideo': 'avi',\n\n        # application (streaming playlists)\n        'dash+xml': 'mpd',\n        'f4m+xml': 'f4m',\n        'hds+xml': 'f4m',\n        'vnd.apple.mpegurl': 'm3u8',\n        'vnd.ms-sstr+xml': 'ism',\n        'x-mpegurl': 'm3u8',\n\n        # audio\n        'audio/mp4': 'm4a',\n        # Per RFC 3003, audio/mpeg can be .mp1, .mp2 or .mp3.\n        # Using .mp3 as it's the most popular one\n        'audio/mpeg': 'mp3',\n        'audio/webm': 'webm',\n        'audio/x-matroska': 'mka',\n        'audio/x-mpegurl': 'm3u',\n        'midi': 'mid',\n        'ogg': 'ogg',\n        'wav': 'wav',\n        'wave': 'wav',\n        'x-aac': 'aac',\n        'x-flac': 'flac',\n        'x-m4a': 'm4a',\n        'x-realaudio': 'ra',\n        'x-wav': 'wav',\n\n        # image\n        'avif': 'avif',\n        'bmp': 'bmp',\n        'gif': 'gif',\n        'jpeg': 'jpg',\n        'png': 'png',\n        'svg+xml': 'svg',\n        'tiff': 'tif',\n        'vnd.wap.wbmp': 'wbmp',\n        'webp': 'webp',\n        'x-icon': 'ico',\n        'x-jng': 'jng',\n        'x-ms-bmp': 'bmp',\n\n        # caption\n        'filmstrip+json': 'fs',\n        'smptett+xml': 'tt',\n        'ttaf+xml': 'dfxp',\n        'ttml+xml': 'ttml',\n        'x-ms-sami': 'sami',\n\n        # misc\n        'gzip': 'gz',\n        'json': 'json',\n        'xml': 'xml',\n        'zip': 'zip',\n    }\n\n    mimetype = mt.partition(';')[0].strip().lower()\n    _, _, subtype = mimetype.rpartition('/')\n\n    ext = traversal.traverse_obj(MAP, mimetype, subtype, subtype.rsplit('+')[-1])\n    if ext:\n        return ext\n    elif default is not NO_DEFAULT:\n        return default\n    return subtype.replace('+', '.')\n\n\ndef ext2mimetype(ext_or_url):\n    if not ext_or_url:\n        return None\n    if '.' not in ext_or_url:\n        ext_or_url = f'file.{ext_or_url}'\n    return mimetypes.guess_type(ext_or_url)[0]\n\n\ndef parse_codecs(codecs_str):\n    # http://tools.ietf.org/html/rfc6381\n    if not codecs_str:\n        return {}\n    split_codecs = list(filter(None, map(\n        str.strip, codecs_str.strip().strip(',').split(','))))\n    vcodec, acodec, scodec, hdr = None, None, None, None\n    for full_codec in split_codecs:\n        parts = re.sub(r'0+(?=\\d)', '', full_codec).split('.')\n        if parts[0] in ('avc1', 'avc2', 'avc3', 'avc4', 'vp9', 'vp8', 'hev1', 'hev2',\n                        'h263', 'h264', 'mp4v', 'hvc1', 'av1', 'theora', 'dvh1', 'dvhe'):\n            if vcodec:\n                continue\n            vcodec = full_codec\n            if parts[0] in ('dvh1', 'dvhe'):\n                hdr = 'DV'\n            elif parts[0] == 'av1' and traversal.traverse_obj(parts, 3) == '10':\n                hdr = 'HDR10'\n            elif parts[:2] == ['vp9', '2']:\n                hdr = 'HDR10'\n        elif parts[0] in ('flac', 'mp4a', 'opus', 'vorbis', 'mp3', 'aac', 'ac-4',\n                          'ac-3', 'ec-3', 'eac3', 'dtsc', 'dtse', 'dtsh', 'dtsl'):\n            acodec = acodec or full_codec\n        elif parts[0] in ('stpp', 'wvtt'):\n            scodec = scodec or full_codec\n        else:\n            write_string(f'WARNING: Unknown codec {full_codec}\\n')\n    if vcodec or acodec or scodec:\n        return {\n            'vcodec': vcodec or 'none',\n            'acodec': acodec or 'none',\n            'dynamic_range': hdr,\n            **({'scodec': scodec} if scodec is not None else {}),\n        }\n    elif len(split_codecs) == 2:\n        return {\n            'vcodec': split_codecs[0],\n            'acodec': split_codecs[1],\n        }\n    return {}\n\n\ndef get_compatible_ext(*, vcodecs, acodecs, vexts, aexts, preferences=None):\n    assert len(vcodecs) == len(vexts) and len(acodecs) == len(aexts)\n\n    allow_mkv = not preferences or 'mkv' in preferences\n\n    if allow_mkv and max(len(acodecs), len(vcodecs)) > 1:\n        return 'mkv'  # TODO: any other format allows this?\n\n    # TODO: All codecs supported by parse_codecs isn't handled here\n    COMPATIBLE_CODECS = {\n        'mp4': {\n            'av1', 'hevc', 'avc1', 'mp4a', 'ac-4',  # fourcc (m3u8, mpd)\n            'h264', 'aacl', 'ec-3',  # Set in ISM\n        },\n        'webm': {\n            'av1', 'vp9', 'vp8', 'opus', 'vrbs',\n            'vp9x', 'vp8x',  # in the webm spec\n        },\n    }\n\n    sanitize_codec = functools.partial(\n        try_get, getter=lambda x: x[0].split('.')[0].replace('0', '').lower())\n    vcodec, acodec = sanitize_codec(vcodecs), sanitize_codec(acodecs)\n\n    for ext in preferences or COMPATIBLE_CODECS.keys():\n        codec_set = COMPATIBLE_CODECS.get(ext, set())\n        if ext == 'mkv' or codec_set.issuperset((vcodec, acodec)):\n            return ext\n\n    COMPATIBLE_EXTS = (\n        {'mp3', 'mp4', 'm4a', 'm4p', 'm4b', 'm4r', 'm4v', 'ismv', 'isma', 'mov'},\n        {'webm', 'weba'},\n    )\n    for ext in preferences or vexts:\n        current_exts = {ext, *vexts, *aexts}\n        if ext == 'mkv' or current_exts == {ext} or any(\n                ext_sets.issuperset(current_exts) for ext_sets in COMPATIBLE_EXTS):\n            return ext\n    return 'mkv' if allow_mkv else preferences[-1]\n\n\ndef urlhandle_detect_ext(url_handle, default=NO_DEFAULT):\n    getheader = url_handle.headers.get\n\n    cd = getheader('Content-Disposition')\n    if cd:\n        m = re.match(r'attachment;\\s*filename=\"(?P<filename>[^\"]+)\"', cd)\n        if m:\n            e = determine_ext(m.group('filename'), default_ext=None)\n            if e:\n                return e\n\n    meta_ext = getheader('x-amz-meta-name')\n    if meta_ext:\n        e = meta_ext.rpartition('.')[2]\n        if e:\n            return e\n\n    return mimetype2ext(getheader('Content-Type'), default=default)\n\n\ndef encode_data_uri(data, mime_type):\n    return 'data:%s;base64,%s' % (mime_type, base64.b64encode(data).decode('ascii'))\n\n\ndef age_restricted(content_limit, age_limit):\n    \"\"\" Returns True iff the content should be blocked \"\"\"\n\n    if age_limit is None:  # No limit set\n        return False\n    if content_limit is None:\n        return False  # Content available for everyone\n    return age_limit < content_limit\n\n\n# List of known byte-order-marks (BOM)\nBOMS = [\n    (b'\\xef\\xbb\\xbf', 'utf-8'),\n    (b'\\x00\\x00\\xfe\\xff', 'utf-32-be'),\n    (b'\\xff\\xfe\\x00\\x00', 'utf-32-le'),\n    (b'\\xff\\xfe', 'utf-16-le'),\n    (b'\\xfe\\xff', 'utf-16-be'),\n]\n\n\ndef is_html(first_bytes):\n    \"\"\" Detect whether a file contains HTML by examining its first bytes. \"\"\"\n\n    encoding = 'utf-8'\n    for bom, enc in BOMS:\n        while first_bytes.startswith(bom):\n            encoding, first_bytes = enc, first_bytes[len(bom):]\n\n    return re.match(r'^\\s*<', first_bytes.decode(encoding, 'replace'))\n\n\ndef determine_protocol(info_dict):\n    protocol = info_dict.get('protocol')\n    if protocol is not None:\n        return protocol\n\n    url = sanitize_url(info_dict['url'])\n    if url.startswith('rtmp'):\n        return 'rtmp'\n    elif url.startswith('mms'):\n        return 'mms'\n    elif url.startswith('rtsp'):\n        return 'rtsp'\n\n    ext = determine_ext(url)\n    if ext == 'm3u8':\n        return 'm3u8' if info_dict.get('is_live') else 'm3u8_native'\n    elif ext == 'f4m':\n        return 'f4m'\n\n    return urllib.parse.urlparse(url).scheme\n\n\ndef render_table(header_row, data, delim=False, extra_gap=0, hide_empty=False):\n    \"\"\" Render a list of rows, each as a list of values.\n    Text after a \\t will be right aligned \"\"\"\n    def width(string):\n        return len(remove_terminal_sequences(string).replace('\\t', ''))\n\n    def get_max_lens(table):\n        return [max(width(str(v)) for v in col) for col in zip(*table)]\n\n    def filter_using_list(row, filterArray):\n        return [col for take, col in itertools.zip_longest(filterArray, row, fillvalue=True) if take]\n\n    max_lens = get_max_lens(data) if hide_empty else []\n    header_row = filter_using_list(header_row, max_lens)\n    data = [filter_using_list(row, max_lens) for row in data]\n\n    table = [header_row] + data\n    max_lens = get_max_lens(table)\n    extra_gap += 1\n    if delim:\n        table = [header_row, [delim * (ml + extra_gap) for ml in max_lens]] + data\n        table[1][-1] = table[1][-1][:-extra_gap * len(delim)]  # Remove extra_gap from end of delimiter\n    for row in table:\n        for pos, text in enumerate(map(str, row)):\n            if '\\t' in text:\n                row[pos] = text.replace('\\t', ' ' * (max_lens[pos] - width(text))) + ' ' * extra_gap\n            else:\n                row[pos] = text + ' ' * (max_lens[pos] - width(text) + extra_gap)\n    ret = '\\n'.join(''.join(row).rstrip() for row in table)\n    return ret\n\n\ndef _match_one(filter_part, dct, incomplete):\n    # TODO: Generalize code with YoutubeDL._build_format_filter\n    STRING_OPERATORS = {\n        '*=': operator.contains,\n        '^=': lambda attr, value: attr.startswith(value),\n        '$=': lambda attr, value: attr.endswith(value),\n        '~=': lambda attr, value: re.search(value, attr),\n    }\n    COMPARISON_OPERATORS = {\n        **STRING_OPERATORS,\n        '<=': operator.le,  # \"<=\" must be defined above \"<\"\n        '<': operator.lt,\n        '>=': operator.ge,\n        '>': operator.gt,\n        '=': operator.eq,\n    }\n\n    if isinstance(incomplete, bool):\n        is_incomplete = lambda _: incomplete\n    else:\n        is_incomplete = lambda k: k in incomplete\n\n    operator_rex = re.compile(r'''(?x)\n        (?P<key>[a-z_]+)\n        \\s*(?P<negation>!\\s*)?(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n        (?:\n            (?P<quote>[\"\\'])(?P<quotedstrval>.+?)(?P=quote)|\n            (?P<strval>.+?)\n        )\n        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))\n    m = operator_rex.fullmatch(filter_part.strip())\n    if m:\n        m = m.groupdict()\n        unnegated_op = COMPARISON_OPERATORS[m['op']]\n        if m['negation']:\n            op = lambda attr, value: not unnegated_op(attr, value)\n        else:\n            op = unnegated_op\n        comparison_value = m['quotedstrval'] or m['strval'] or m['intval']\n        if m['quote']:\n            comparison_value = comparison_value.replace(r'\\%s' % m['quote'], m['quote'])\n        actual_value = dct.get(m['key'])\n        numeric_comparison = None\n        if isinstance(actual_value, (int, float)):\n            # If the original field is a string and matching comparisonvalue is\n            # a number we should respect the origin of the original field\n            # and process comparison value as a string (see\n            # https://github.com/ytdl-org/youtube-dl/issues/11082)\n            try:\n                numeric_comparison = int(comparison_value)\n            except ValueError:\n                numeric_comparison = parse_filesize(comparison_value)\n                if numeric_comparison is None:\n                    numeric_comparison = parse_filesize(f'{comparison_value}B')\n                if numeric_comparison is None:\n                    numeric_comparison = parse_duration(comparison_value)\n        if numeric_comparison is not None and m['op'] in STRING_OPERATORS:\n            raise ValueError('Operator %s only supports string values!' % m['op'])\n        if actual_value is None:\n            return is_incomplete(m['key']) or m['none_inclusive']\n        return op(actual_value, comparison_value if numeric_comparison is None else numeric_comparison)\n\n    UNARY_OPERATORS = {\n        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),\n        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),\n    }\n    operator_rex = re.compile(r'''(?x)\n        (?P<op>%s)\\s*(?P<key>[a-z_]+)\n        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))\n    m = operator_rex.fullmatch(filter_part.strip())\n    if m:\n        op = UNARY_OPERATORS[m.group('op')]\n        actual_value = dct.get(m.group('key'))\n        if is_incomplete(m.group('key')) and actual_value is None:\n            return True\n        return op(actual_value)\n\n    raise ValueError('Invalid filter part %r' % filter_part)\n\n\ndef match_str(filter_str, dct, incomplete=False):\n    \"\"\" Filter a dictionary with a simple string syntax.\n    @returns           Whether the filter passes\n    @param incomplete  Set of keys that is expected to be missing from dct.\n                       Can be True/False to indicate all/none of the keys may be missing.\n                       All conditions on incomplete keys pass if the key is missing\n    \"\"\"\n    return all(\n        _match_one(filter_part.replace(r'\\&', '&'), dct, incomplete)\n        for filter_part in re.split(r'(?<!\\\\)&', filter_str))\n\n\ndef match_filter_func(filters, breaking_filters=None):\n    if not filters and not breaking_filters:\n        return None\n    breaking_filters = match_filter_func(breaking_filters) or (lambda _, __: None)\n    filters = set(variadic(filters or []))\n\n    interactive = '-' in filters\n    if interactive:\n        filters.remove('-')\n\n    def _match_func(info_dict, incomplete=False):\n        ret = breaking_filters(info_dict, incomplete)\n        if ret is not None:\n            raise RejectedVideoReached(ret)\n\n        if not filters or any(match_str(f, info_dict, incomplete) for f in filters):\n            return NO_DEFAULT if interactive and not incomplete else None\n        else:\n            video_title = info_dict.get('title') or info_dict.get('id') or 'entry'\n            filter_str = ') | ('.join(map(str.strip, filters))\n            return f'{video_title} does not pass filter ({filter_str}), skipping ..'\n    return _match_func\n\n\nclass download_range_func:\n    def __init__(self, chapters, ranges, from_info=False):\n        self.chapters, self.ranges, self.from_info = chapters, ranges, from_info\n\n    def __call__(self, info_dict, ydl):\n\n        warning = ('There are no chapters matching the regex' if info_dict.get('chapters')\n                   else 'Cannot match chapters since chapter information is unavailable')\n        for regex in self.chapters or []:\n            for i, chapter in enumerate(info_dict.get('chapters') or []):\n                if re.search(regex, chapter['title']):\n                    warning = None\n                    yield {**chapter, 'index': i}\n        if self.chapters and warning:\n            ydl.to_screen(f'[info] {info_dict[\"id\"]}: {warning}')\n\n        for start, end in self.ranges or []:\n            yield {\n                'start_time': self._handle_negative_timestamp(start, info_dict),\n                'end_time': self._handle_negative_timestamp(end, info_dict),\n            }\n\n        if self.from_info and (info_dict.get('start_time') or info_dict.get('end_time')):\n            yield {\n                'start_time': info_dict.get('start_time') or 0,\n                'end_time': info_dict.get('end_time') or float('inf'),\n            }\n        elif not self.ranges and not self.chapters:\n            yield {}\n\n    @staticmethod\n    def _handle_negative_timestamp(time, info):\n        return max(info['duration'] + time, 0) if info.get('duration') and time < 0 else time\n\n    def __eq__(self, other):\n        return (isinstance(other, download_range_func)\n                and self.chapters == other.chapters and self.ranges == other.ranges)\n\n    def __repr__(self):\n        return f'{__name__}.{type(self).__name__}({self.chapters}, {self.ranges})'\n\n\ndef parse_dfxp_time_expr(time_expr):\n    if not time_expr:\n        return\n\n    mobj = re.match(rf'^(?P<time_offset>{NUMBER_RE})s?$', time_expr)\n    if mobj:\n        return float(mobj.group('time_offset'))\n\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)\n    if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))\n\n\ndef srt_subtitles_timecode(seconds):\n    return '%02d:%02d:%02d,%03d' % timetuple_from_msec(seconds * 1000)\n\n\ndef ass_subtitles_timecode(seconds):\n    time = timetuple_from_msec(seconds * 1000)\n    return '%01d:%02d:%02d.%02d' % (*time[:-1], time.milliseconds / 10)\n\n\ndef dfxp2srt(dfxp_data):\n    '''\n    @param dfxp_data A bytes-like object containing DFXP data\n    @returns A unicode object containing converted SRT data\n    '''\n    LEGACY_NAMESPACES = (\n        (b'http://www.w3.org/ns/ttml', [\n            b'http://www.w3.org/2004/11/ttaf1',\n            b'http://www.w3.org/2006/04/ttaf1',\n            b'http://www.w3.org/2006/10/ttaf1',\n        ]),\n        (b'http://www.w3.org/ns/ttml#styling', [\n            b'http://www.w3.org/ns/ttml#style',\n        ]),\n    )\n\n    SUPPORTED_STYLING = [\n        'color',\n        'fontFamily',\n        'fontSize',\n        'fontStyle',\n        'fontWeight',\n        'textDecoration'\n    ]\n\n    _x = functools.partial(xpath_with_ns, ns_map={\n        'xml': 'http://www.w3.org/XML/1998/namespace',\n        'ttml': 'http://www.w3.org/ns/ttml',\n        'tts': 'http://www.w3.org/ns/ttml#styling',\n    })\n\n    styles = {}\n    default_style = {}\n\n    class TTMLPElementParser:\n        _out = ''\n        _unclosed_elements = []\n        _applied_styles = []\n\n        def start(self, tag, attrib):\n            if tag in (_x('ttml:br'), 'br'):\n                self._out += '\\n'\n            else:\n                unclosed_elements = []\n                style = {}\n                element_style_id = attrib.get('style')\n                if default_style:\n                    style.update(default_style)\n                if element_style_id:\n                    style.update(styles.get(element_style_id, {}))\n                for prop in SUPPORTED_STYLING:\n                    prop_val = attrib.get(_x('tts:' + prop))\n                    if prop_val:\n                        style[prop] = prop_val\n                if style:\n                    font = ''\n                    for k, v in sorted(style.items()):\n                        if self._applied_styles and self._applied_styles[-1].get(k) == v:\n                            continue\n                        if k == 'color':\n                            font += ' color=\"%s\"' % v\n                        elif k == 'fontSize':\n                            font += ' size=\"%s\"' % v\n                        elif k == 'fontFamily':\n                            font += ' face=\"%s\"' % v\n                        elif k == 'fontWeight' and v == 'bold':\n                            self._out += '<b>'\n                            unclosed_elements.append('b')\n                        elif k == 'fontStyle' and v == 'italic':\n                            self._out += '<i>'\n                            unclosed_elements.append('i')\n                        elif k == 'textDecoration' and v == 'underline':\n                            self._out += '<u>'\n                            unclosed_elements.append('u')\n                    if font:\n                        self._out += '<font' + font + '>'\n                        unclosed_elements.append('font')\n                    applied_style = {}\n                    if self._applied_styles:\n                        applied_style.update(self._applied_styles[-1])\n                    applied_style.update(style)\n                    self._applied_styles.append(applied_style)\n                self._unclosed_elements.append(unclosed_elements)\n\n        def end(self, tag):\n            if tag not in (_x('ttml:br'), 'br'):\n                unclosed_elements = self._unclosed_elements.pop()\n                for element in reversed(unclosed_elements):\n                    self._out += '</%s>' % element\n                if unclosed_elements and self._applied_styles:\n                    self._applied_styles.pop()\n\n        def data(self, data):\n            self._out += data\n\n        def close(self):\n            return self._out.strip()\n\n    # Fix UTF-8 encoded file wrongly marked as UTF-16. See https://github.com/yt-dlp/yt-dlp/issues/6543#issuecomment-1477169870\n    # This will not trigger false positives since only UTF-8 text is being replaced\n    dfxp_data = dfxp_data.replace(b'encoding=\\'UTF-16\\'', b'encoding=\\'UTF-8\\'')\n\n    def parse_node(node):\n        target = TTMLPElementParser()\n        parser = xml.etree.ElementTree.XMLParser(target=target)\n        parser.feed(xml.etree.ElementTree.tostring(node))\n        return parser.close()\n\n    for k, v in LEGACY_NAMESPACES:\n        for ns in v:\n            dfxp_data = dfxp_data.replace(ns, k)\n\n    dfxp = compat_etree_fromstring(dfxp_data)\n    out = []\n    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')\n\n    if not paras:\n        raise ValueError('Invalid dfxp/TTML subtitle')\n\n    repeat = False\n    while True:\n        for style in dfxp.findall(_x('.//ttml:style')):\n            style_id = style.get('id') or style.get(_x('xml:id'))\n            if not style_id:\n                continue\n            parent_style_id = style.get('style')\n            if parent_style_id:\n                if parent_style_id not in styles:\n                    repeat = True\n                    continue\n                styles[style_id] = styles[parent_style_id].copy()\n            for prop in SUPPORTED_STYLING:\n                prop_val = style.get(_x('tts:' + prop))\n                if prop_val:\n                    styles.setdefault(style_id, {})[prop] = prop_val\n        if repeat:\n            repeat = False\n        else:\n            break\n\n    for p in ('body', 'div'):\n        ele = xpath_element(dfxp, [_x('.//ttml:' + p), './/' + p])\n        if ele is None:\n            continue\n        style = styles.get(ele.get('style'))\n        if not style:\n            continue\n        default_style.update(style)\n\n    for para, index in zip(paras, itertools.count(1)):\n        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))\n        end_time = parse_dfxp_time_expr(para.attrib.get('end'))\n        dur = parse_dfxp_time_expr(para.attrib.get('dur'))\n        if begin_time is None:\n            continue\n        if not end_time:\n            if not dur:\n                continue\n            end_time = begin_time + dur\n        out.append('%d\\n%s --> %s\\n%s\\n\\n' % (\n            index,\n            srt_subtitles_timecode(begin_time),\n            srt_subtitles_timecode(end_time),\n            parse_node(para)))\n\n    return ''.join(out)\n\n\ndef cli_option(params, command_option, param, separator=None):\n    param = params.get(param)\n    return ([] if param is None\n            else [command_option, str(param)] if separator is None\n            else [f'{command_option}{separator}{param}'])\n\n\ndef cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n    param = params.get(param)\n    assert param in (True, False, None)\n    return cli_option({True: true_value, False: false_value}, command_option, param, separator)\n\n\ndef cli_valueless_option(params, command_option, param, expected_value=True):\n    return [command_option] if params.get(param) == expected_value else []\n\n\ndef cli_configuration_args(argdict, keys, default=[], use_compat=True):\n    if isinstance(argdict, (list, tuple)):  # for backward compatibility\n        if use_compat:\n            return argdict\n        else:\n            argdict = None\n    if argdict is None:\n        return default\n    assert isinstance(argdict, dict)\n\n    assert isinstance(keys, (list, tuple))\n    for key_list in keys:\n        arg_list = list(filter(\n            lambda x: x is not None,\n            [argdict.get(key.lower()) for key in variadic(key_list)]))\n        if arg_list:\n            return [arg for args in arg_list for arg in args]\n    return default\n\n\ndef _configuration_args(main_key, argdict, exe, keys=None, default=[], use_compat=True):\n    main_key, exe = main_key.lower(), exe.lower()\n    root_key = exe if main_key == exe else f'{main_key}+{exe}'\n    keys = [f'{root_key}{k}' for k in (keys or [''])]\n    if root_key in keys:\n        if main_key != exe:\n            keys.append((main_key, exe))\n        keys.append('default')\n    else:\n        use_compat = False\n    return cli_configuration_args(argdict, keys, default, use_compat)\n\n\nclass ISO639Utils:\n    # See http://www.loc.gov/standards/iso639-2/ISO-639-2_utf-8.txt\n    _lang_map = {\n        'aa': 'aar',\n        'ab': 'abk',\n        'ae': 'ave',\n        'af': 'afr',\n        'ak': 'aka',\n        'am': 'amh',\n        'an': 'arg',\n        'ar': 'ara',\n        'as': 'asm',\n        'av': 'ava',\n        'ay': 'aym',\n        'az': 'aze',\n        'ba': 'bak',\n        'be': 'bel',\n        'bg': 'bul',\n        'bh': 'bih',\n        'bi': 'bis',\n        'bm': 'bam',\n        'bn': 'ben',\n        'bo': 'bod',\n        'br': 'bre',\n        'bs': 'bos',\n        'ca': 'cat',\n        'ce': 'che',\n        'ch': 'cha',\n        'co': 'cos',\n        'cr': 'cre',\n        'cs': 'ces',\n        'cu': 'chu',\n        'cv': 'chv',\n        'cy': 'cym',\n        'da': 'dan',\n        'de': 'deu',\n        'dv': 'div',\n        'dz': 'dzo',\n        'ee': 'ewe',\n        'el': 'ell',\n        'en': 'eng',\n        'eo': 'epo',\n        'es': 'spa',\n        'et': 'est',\n        'eu': 'eus',\n        'fa': 'fas',\n        'ff': 'ful',\n        'fi': 'fin',\n        'fj': 'fij',\n        'fo': 'fao',\n        'fr': 'fra',\n        'fy': 'fry',\n        'ga': 'gle',\n        'gd': 'gla',\n        'gl': 'glg',\n        'gn': 'grn',\n        'gu': 'guj',\n        'gv': 'glv',\n        'ha': 'hau',\n        'he': 'heb',\n        'iw': 'heb',  # Replaced by he in 1989 revision\n        'hi': 'hin',\n        'ho': 'hmo',\n        'hr': 'hrv',\n        'ht': 'hat',\n        'hu': 'hun',\n        'hy': 'hye',\n        'hz': 'her',\n        'ia': 'ina',\n        'id': 'ind',\n        'in': 'ind',  # Replaced by id in 1989 revision\n        'ie': 'ile',\n        'ig': 'ibo',\n        'ii': 'iii',\n        'ik': 'ipk',\n        'io': 'ido',\n        'is': 'isl',\n        'it': 'ita',\n        'iu': 'iku',\n        'ja': 'jpn',\n        'jv': 'jav',\n        'ka': 'kat',\n        'kg': 'kon',\n        'ki': 'kik',\n        'kj': 'kua',\n        'kk': 'kaz',\n        'kl': 'kal',\n        'km': 'khm',\n        'kn': 'kan',\n        'ko': 'kor',\n        'kr': 'kau',\n        'ks': 'kas',\n        'ku': 'kur',\n        'kv': 'kom',\n        'kw': 'cor',\n        'ky': 'kir',\n        'la': 'lat',\n        'lb': 'ltz',\n        'lg': 'lug',\n        'li': 'lim',\n        'ln': 'lin',\n        'lo': 'lao',\n        'lt': 'lit',\n        'lu': 'lub',\n        'lv': 'lav',\n        'mg': 'mlg',\n        'mh': 'mah',\n        'mi': 'mri',\n        'mk': 'mkd',\n        'ml': 'mal',\n        'mn': 'mon',\n        'mr': 'mar',\n        'ms': 'msa',\n        'mt': 'mlt',\n        'my': 'mya',\n        'na': 'nau',\n        'nb': 'nob',\n        'nd': 'nde',\n        'ne': 'nep',\n        'ng': 'ndo',\n        'nl': 'nld',\n        'nn': 'nno',\n        'no': 'nor',\n        'nr': 'nbl',\n        'nv': 'nav',\n        'ny': 'nya',\n        'oc': 'oci',\n        'oj': 'oji',\n        'om': 'orm',\n        'or': 'ori',\n        'os': 'oss',\n        'pa': 'pan',\n        'pe': 'per',\n        'pi': 'pli',\n        'pl': 'pol',\n        'ps': 'pus',\n        'pt': 'por',\n        'qu': 'que',\n        'rm': 'roh',\n        'rn': 'run',\n        'ro': 'ron',\n        'ru': 'rus',\n        'rw': 'kin',\n        'sa': 'san',\n        'sc': 'srd',\n        'sd': 'snd',\n        'se': 'sme',\n        'sg': 'sag',\n        'si': 'sin',\n        'sk': 'slk',\n        'sl': 'slv',\n        'sm': 'smo',\n        'sn': 'sna',\n        'so': 'som',\n        'sq': 'sqi',\n        'sr': 'srp',\n        'ss': 'ssw',\n        'st': 'sot',\n        'su': 'sun',\n        'sv': 'swe',\n        'sw': 'swa',\n        'ta': 'tam',\n        'te': 'tel',\n        'tg': 'tgk',\n        'th': 'tha',\n        'ti': 'tir',\n        'tk': 'tuk',\n        'tl': 'tgl',\n        'tn': 'tsn',\n        'to': 'ton',\n        'tr': 'tur',\n        'ts': 'tso',\n        'tt': 'tat',\n        'tw': 'twi',\n        'ty': 'tah',\n        'ug': 'uig',\n        'uk': 'ukr',\n        'ur': 'urd',\n        'uz': 'uzb',\n        've': 'ven',\n        'vi': 'vie',\n        'vo': 'vol',\n        'wa': 'wln',\n        'wo': 'wol',\n        'xh': 'xho',\n        'yi': 'yid',\n        'ji': 'yid',  # Replaced by yi in 1989 revision\n        'yo': 'yor',\n        'za': 'zha',\n        'zh': 'zho',\n        'zu': 'zul',\n    }\n\n    @classmethod\n    def short2long(cls, code):\n        \"\"\"Convert language code from ISO 639-1 to ISO 639-2/T\"\"\"\n        return cls._lang_map.get(code[:2])\n\n    @classmethod\n    def long2short(cls, code):\n        \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"\n        for short_name, long_name in cls._lang_map.items():\n            if long_name == code:\n                return short_name\n\n\nclass ISO3166Utils:\n    # From http://data.okfn.org/data/core/country-list\n    _country_map = {\n        'AF': 'Afghanistan',\n        'AX': '\u00c5land Islands',\n        'AL': 'Albania',\n        'DZ': 'Algeria',\n        'AS': 'American Samoa',\n        'AD': 'Andorra',\n        'AO': 'Angola',\n        'AI': 'Anguilla',\n        'AQ': 'Antarctica',\n        'AG': 'Antigua and Barbuda',\n        'AR': 'Argentina',\n        'AM': 'Armenia',\n        'AW': 'Aruba',\n        'AU': 'Australia',\n        'AT': 'Austria',\n        'AZ': 'Azerbaijan',\n        'BS': 'Bahamas',\n        'BH': 'Bahrain',\n        'BD': 'Bangladesh',\n        'BB': 'Barbados',\n        'BY': 'Belarus',\n        'BE': 'Belgium',\n        'BZ': 'Belize',\n        'BJ': 'Benin',\n        'BM': 'Bermuda',\n        'BT': 'Bhutan',\n        'BO': 'Bolivia, Plurinational State of',\n        'BQ': 'Bonaire, Sint Eustatius and Saba',\n        'BA': 'Bosnia and Herzegovina',\n        'BW': 'Botswana',\n        'BV': 'Bouvet Island',\n        'BR': 'Brazil',\n        'IO': 'British Indian Ocean Territory',\n        'BN': 'Brunei Darussalam',\n        'BG': 'Bulgaria',\n        'BF': 'Burkina Faso',\n        'BI': 'Burundi',\n        'KH': 'Cambodia',\n        'CM': 'Cameroon',\n        'CA': 'Canada',\n        'CV': 'Cape Verde',\n        'KY': 'Cayman Islands',\n        'CF': 'Central African Republic',\n        'TD': 'Chad',\n        'CL': 'Chile',\n        'CN': 'China',\n        'CX': 'Christmas Island',\n        'CC': 'Cocos (Keeling) Islands',\n        'CO': 'Colombia',\n        'KM': 'Comoros',\n        'CG': 'Congo',\n        'CD': 'Congo, the Democratic Republic of the',\n        'CK': 'Cook Islands',\n        'CR': 'Costa Rica',\n        'CI': 'C\u00f4te d\\'Ivoire',\n        'HR': 'Croatia',\n        'CU': 'Cuba',\n        'CW': 'Cura\u00e7ao',\n        'CY': 'Cyprus',\n        'CZ': 'Czech Republic',\n        'DK': 'Denmark',\n        'DJ': 'Djibouti',\n        'DM': 'Dominica',\n        'DO': 'Dominican Republic',\n        'EC': 'Ecuador',\n        'EG': 'Egypt',\n        'SV': 'El Salvador',\n        'GQ': 'Equatorial Guinea',\n        'ER': 'Eritrea',\n        'EE': 'Estonia',\n        'ET': 'Ethiopia',\n        'FK': 'Falkland Islands (Malvinas)',\n        'FO': 'Faroe Islands',\n        'FJ': 'Fiji',\n        'FI': 'Finland',\n        'FR': 'France',\n        'GF': 'French Guiana',\n        'PF': 'French Polynesia',\n        'TF': 'French Southern Territories',\n        'GA': 'Gabon',\n        'GM': 'Gambia',\n        'GE': 'Georgia',\n        'DE': 'Germany',\n        'GH': 'Ghana',\n        'GI': 'Gibraltar',\n        'GR': 'Greece',\n        'GL': 'Greenland',\n        'GD': 'Grenada',\n        'GP': 'Guadeloupe',\n        'GU': 'Guam',\n        'GT': 'Guatemala',\n        'GG': 'Guernsey',\n        'GN': 'Guinea',\n        'GW': 'Guinea-Bissau',\n        'GY': 'Guyana',\n        'HT': 'Haiti',\n        'HM': 'Heard Island and McDonald Islands',\n        'VA': 'Holy See (Vatican City State)',\n        'HN': 'Honduras',\n        'HK': 'Hong Kong',\n        'HU': 'Hungary',\n        'IS': 'Iceland',\n        'IN': 'India',\n        'ID': 'Indonesia',\n        'IR': 'Iran, Islamic Republic of',\n        'IQ': 'Iraq',\n        'IE': 'Ireland',\n        'IM': 'Isle of Man',\n        'IL': 'Israel',\n        'IT': 'Italy',\n        'JM': 'Jamaica',\n        'JP': 'Japan',\n        'JE': 'Jersey',\n        'JO': 'Jordan',\n        'KZ': 'Kazakhstan',\n        'KE': 'Kenya',\n        'KI': 'Kiribati',\n        'KP': 'Korea, Democratic People\\'s Republic of',\n        'KR': 'Korea, Republic of',\n        'KW': 'Kuwait',\n        'KG': 'Kyrgyzstan',\n        'LA': 'Lao People\\'s Democratic Republic',\n        'LV': 'Latvia',\n        'LB': 'Lebanon',\n        'LS': 'Lesotho',\n        'LR': 'Liberia',\n        'LY': 'Libya',\n        'LI': 'Liechtenstein',\n        'LT': 'Lithuania',\n        'LU': 'Luxembourg',\n        'MO': 'Macao',\n        'MK': 'Macedonia, the Former Yugoslav Republic of',\n        'MG': 'Madagascar',\n        'MW': 'Malawi',\n        'MY': 'Malaysia',\n        'MV': 'Maldives',\n        'ML': 'Mali',\n        'MT': 'Malta',\n        'MH': 'Marshall Islands',\n        'MQ': 'Martinique',\n        'MR': 'Mauritania',\n        'MU': 'Mauritius',\n        'YT': 'Mayotte',\n        'MX': 'Mexico',\n        'FM': 'Micronesia, Federated States of',\n        'MD': 'Moldova, Republic of',\n        'MC': 'Monaco',\n        'MN': 'Mongolia',\n        'ME': 'Montenegro',\n        'MS': 'Montserrat',\n        'MA': 'Morocco',\n        'MZ': 'Mozambique',\n        'MM': 'Myanmar',\n        'NA': 'Namibia',\n        'NR': 'Nauru',\n        'NP': 'Nepal',\n        'NL': 'Netherlands',\n        'NC': 'New Caledonia',\n        'NZ': 'New Zealand',\n        'NI': 'Nicaragua',\n        'NE': 'Niger',\n        'NG': 'Nigeria',\n        'NU': 'Niue',\n        'NF': 'Norfolk Island',\n        'MP': 'Northern Mariana Islands',\n        'NO': 'Norway',\n        'OM': 'Oman',\n        'PK': 'Pakistan',\n        'PW': 'Palau',\n        'PS': 'Palestine, State of',\n        'PA': 'Panama',\n        'PG': 'Papua New Guinea',\n        'PY': 'Paraguay',\n        'PE': 'Peru',\n        'PH': 'Philippines',\n        'PN': 'Pitcairn',\n        'PL': 'Poland',\n        'PT': 'Portugal',\n        'PR': 'Puerto Rico',\n        'QA': 'Qatar',\n        'RE': 'R\u00e9union',\n        'RO': 'Romania',\n        'RU': 'Russian Federation',\n        'RW': 'Rwanda',\n        'BL': 'Saint Barth\u00e9lemy',\n        'SH': 'Saint Helena, Ascension and Tristan da Cunha',\n        'KN': 'Saint Kitts and Nevis',\n        'LC': 'Saint Lucia',\n        'MF': 'Saint Martin (French part)',\n        'PM': 'Saint Pierre and Miquelon',\n        'VC': 'Saint Vincent and the Grenadines',\n        'WS': 'Samoa',\n        'SM': 'San Marino',\n        'ST': 'Sao Tome and Principe',\n        'SA': 'Saudi Arabia',\n        'SN': 'Senegal',\n        'RS': 'Serbia',\n        'SC': 'Seychelles',\n        'SL': 'Sierra Leone',\n        'SG': 'Singapore',\n        'SX': 'Sint Maarten (Dutch part)',\n        'SK': 'Slovakia',\n        'SI': 'Slovenia',\n        'SB': 'Solomon Islands',\n        'SO': 'Somalia',\n        'ZA': 'South Africa',\n        'GS': 'South Georgia and the South Sandwich Islands',\n        'SS': 'South Sudan',\n        'ES': 'Spain',\n        'LK': 'Sri Lanka',\n        'SD': 'Sudan',\n        'SR': 'Suriname',\n        'SJ': 'Svalbard and Jan Mayen',\n        'SZ': 'Swaziland',\n        'SE': 'Sweden',\n        'CH': 'Switzerland',\n        'SY': 'Syrian Arab Republic',\n        'TW': 'Taiwan, Province of China',\n        'TJ': 'Tajikistan',\n        'TZ': 'Tanzania, United Republic of',\n        'TH': 'Thailand',\n        'TL': 'Timor-Leste',\n        'TG': 'Togo',\n        'TK': 'Tokelau',\n        'TO': 'Tonga',\n        'TT': 'Trinidad and Tobago',\n        'TN': 'Tunisia',\n        'TR': 'Turkey',\n        'TM': 'Turkmenistan',\n        'TC': 'Turks and Caicos Islands',\n        'TV': 'Tuvalu',\n        'UG': 'Uganda',\n        'UA': 'Ukraine',\n        'AE': 'United Arab Emirates',\n        'GB': 'United Kingdom',\n        'US': 'United States',\n        'UM': 'United States Minor Outlying Islands',\n        'UY': 'Uruguay',\n        'UZ': 'Uzbekistan',\n        'VU': 'Vanuatu',\n        'VE': 'Venezuela, Bolivarian Republic of',\n        'VN': 'Viet Nam',\n        'VG': 'Virgin Islands, British',\n        'VI': 'Virgin Islands, U.S.',\n        'WF': 'Wallis and Futuna',\n        'EH': 'Western Sahara',\n        'YE': 'Yemen',\n        'ZM': 'Zambia',\n        'ZW': 'Zimbabwe',\n        # Not ISO 3166 codes, but used for IP blocks\n        'AP': 'Asia/Pacific Region',\n        'EU': 'Europe',\n    }\n\n    @classmethod\n    def short2full(cls, code):\n        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"\n        return cls._country_map.get(code.upper())\n\n\nclass GeoUtils:\n    # Major IPv4 address blocks per country\n    _country_ip_map = {\n        'AD': '46.172.224.0/19',\n        'AE': '94.200.0.0/13',\n        'AF': '149.54.0.0/17',\n        'AG': '209.59.64.0/18',\n        'AI': '204.14.248.0/21',\n        'AL': '46.99.0.0/16',\n        'AM': '46.70.0.0/15',\n        'AO': '105.168.0.0/13',\n        'AP': '182.50.184.0/21',\n        'AQ': '23.154.160.0/24',\n        'AR': '181.0.0.0/12',\n        'AS': '202.70.112.0/20',\n        'AT': '77.116.0.0/14',\n        'AU': '1.128.0.0/11',\n        'AW': '181.41.0.0/18',\n        'AX': '185.217.4.0/22',\n        'AZ': '5.197.0.0/16',\n        'BA': '31.176.128.0/17',\n        'BB': '65.48.128.0/17',\n        'BD': '114.130.0.0/16',\n        'BE': '57.0.0.0/8',\n        'BF': '102.178.0.0/15',\n        'BG': '95.42.0.0/15',\n        'BH': '37.131.0.0/17',\n        'BI': '154.117.192.0/18',\n        'BJ': '137.255.0.0/16',\n        'BL': '185.212.72.0/23',\n        'BM': '196.12.64.0/18',\n        'BN': '156.31.0.0/16',\n        'BO': '161.56.0.0/16',\n        'BQ': '161.0.80.0/20',\n        'BR': '191.128.0.0/12',\n        'BS': '24.51.64.0/18',\n        'BT': '119.2.96.0/19',\n        'BW': '168.167.0.0/16',\n        'BY': '178.120.0.0/13',\n        'BZ': '179.42.192.0/18',\n        'CA': '99.224.0.0/11',\n        'CD': '41.243.0.0/16',\n        'CF': '197.242.176.0/21',\n        'CG': '160.113.0.0/16',\n        'CH': '85.0.0.0/13',\n        'CI': '102.136.0.0/14',\n        'CK': '202.65.32.0/19',\n        'CL': '152.172.0.0/14',\n        'CM': '102.244.0.0/14',\n        'CN': '36.128.0.0/10',\n        'CO': '181.240.0.0/12',\n        'CR': '201.192.0.0/12',\n        'CU': '152.206.0.0/15',\n        'CV': '165.90.96.0/19',\n        'CW': '190.88.128.0/17',\n        'CY': '31.153.0.0/16',\n        'CZ': '88.100.0.0/14',\n        'DE': '53.0.0.0/8',\n        'DJ': '197.241.0.0/17',\n        'DK': '87.48.0.0/12',\n        'DM': '192.243.48.0/20',\n        'DO': '152.166.0.0/15',\n        'DZ': '41.96.0.0/12',\n        'EC': '186.68.0.0/15',\n        'EE': '90.190.0.0/15',\n        'EG': '156.160.0.0/11',\n        'ER': '196.200.96.0/20',\n        'ES': '88.0.0.0/11',\n        'ET': '196.188.0.0/14',\n        'EU': '2.16.0.0/13',\n        'FI': '91.152.0.0/13',\n        'FJ': '144.120.0.0/16',\n        'FK': '80.73.208.0/21',\n        'FM': '119.252.112.0/20',\n        'FO': '88.85.32.0/19',\n        'FR': '90.0.0.0/9',\n        'GA': '41.158.0.0/15',\n        'GB': '25.0.0.0/8',\n        'GD': '74.122.88.0/21',\n        'GE': '31.146.0.0/16',\n        'GF': '161.22.64.0/18',\n        'GG': '62.68.160.0/19',\n        'GH': '154.160.0.0/12',\n        'GI': '95.164.0.0/16',\n        'GL': '88.83.0.0/19',\n        'GM': '160.182.0.0/15',\n        'GN': '197.149.192.0/18',\n        'GP': '104.250.0.0/19',\n        'GQ': '105.235.224.0/20',\n        'GR': '94.64.0.0/13',\n        'GT': '168.234.0.0/16',\n        'GU': '168.123.0.0/16',\n        'GW': '197.214.80.0/20',\n        'GY': '181.41.64.0/18',\n        'HK': '113.252.0.0/14',\n        'HN': '181.210.0.0/16',\n        'HR': '93.136.0.0/13',\n        'HT': '148.102.128.0/17',\n        'HU': '84.0.0.0/14',\n        'ID': '39.192.0.0/10',\n        'IE': '87.32.0.0/12',\n        'IL': '79.176.0.0/13',\n        'IM': '5.62.80.0/20',\n        'IN': '117.192.0.0/10',\n        'IO': '203.83.48.0/21',\n        'IQ': '37.236.0.0/14',\n        'IR': '2.176.0.0/12',\n        'IS': '82.221.0.0/16',\n        'IT': '79.0.0.0/10',\n        'JE': '87.244.64.0/18',\n        'JM': '72.27.0.0/17',\n        'JO': '176.29.0.0/16',\n        'JP': '133.0.0.0/8',\n        'KE': '105.48.0.0/12',\n        'KG': '158.181.128.0/17',\n        'KH': '36.37.128.0/17',\n        'KI': '103.25.140.0/22',\n        'KM': '197.255.224.0/20',\n        'KN': '198.167.192.0/19',\n        'KP': '175.45.176.0/22',\n        'KR': '175.192.0.0/10',\n        'KW': '37.36.0.0/14',\n        'KY': '64.96.0.0/15',\n        'KZ': '2.72.0.0/13',\n        'LA': '115.84.64.0/18',\n        'LB': '178.135.0.0/16',\n        'LC': '24.92.144.0/20',\n        'LI': '82.117.0.0/19',\n        'LK': '112.134.0.0/15',\n        'LR': '102.183.0.0/16',\n        'LS': '129.232.0.0/17',\n        'LT': '78.56.0.0/13',\n        'LU': '188.42.0.0/16',\n        'LV': '46.109.0.0/16',\n        'LY': '41.252.0.0/14',\n        'MA': '105.128.0.0/11',\n        'MC': '88.209.64.0/18',\n        'MD': '37.246.0.0/16',\n        'ME': '178.175.0.0/17',\n        'MF': '74.112.232.0/21',\n        'MG': '154.126.0.0/17',\n        'MH': '117.103.88.0/21',\n        'MK': '77.28.0.0/15',\n        'ML': '154.118.128.0/18',\n        'MM': '37.111.0.0/17',\n        'MN': '49.0.128.0/17',\n        'MO': '60.246.0.0/16',\n        'MP': '202.88.64.0/20',\n        'MQ': '109.203.224.0/19',\n        'MR': '41.188.64.0/18',\n        'MS': '208.90.112.0/22',\n        'MT': '46.11.0.0/16',\n        'MU': '105.16.0.0/12',\n        'MV': '27.114.128.0/18',\n        'MW': '102.70.0.0/15',\n        'MX': '187.192.0.0/11',\n        'MY': '175.136.0.0/13',\n        'MZ': '197.218.0.0/15',\n        'NA': '41.182.0.0/16',\n        'NC': '101.101.0.0/18',\n        'NE': '197.214.0.0/18',\n        'NF': '203.17.240.0/22',\n        'NG': '105.112.0.0/12',\n        'NI': '186.76.0.0/15',\n        'NL': '145.96.0.0/11',\n        'NO': '84.208.0.0/13',\n        'NP': '36.252.0.0/15',\n        'NR': '203.98.224.0/19',\n        'NU': '49.156.48.0/22',\n        'NZ': '49.224.0.0/14',\n        'OM': '5.36.0.0/15',\n        'PA': '186.72.0.0/15',\n        'PE': '186.160.0.0/14',\n        'PF': '123.50.64.0/18',\n        'PG': '124.240.192.0/19',\n        'PH': '49.144.0.0/13',\n        'PK': '39.32.0.0/11',\n        'PL': '83.0.0.0/11',\n        'PM': '70.36.0.0/20',\n        'PR': '66.50.0.0/16',\n        'PS': '188.161.0.0/16',\n        'PT': '85.240.0.0/13',\n        'PW': '202.124.224.0/20',\n        'PY': '181.120.0.0/14',\n        'QA': '37.210.0.0/15',\n        'RE': '102.35.0.0/16',\n        'RO': '79.112.0.0/13',\n        'RS': '93.86.0.0/15',\n        'RU': '5.136.0.0/13',\n        'RW': '41.186.0.0/16',\n        'SA': '188.48.0.0/13',\n        'SB': '202.1.160.0/19',\n        'SC': '154.192.0.0/11',\n        'SD': '102.120.0.0/13',\n        'SE': '78.64.0.0/12',\n        'SG': '8.128.0.0/10',\n        'SI': '188.196.0.0/14',\n        'SK': '78.98.0.0/15',\n        'SL': '102.143.0.0/17',\n        'SM': '89.186.32.0/19',\n        'SN': '41.82.0.0/15',\n        'SO': '154.115.192.0/18',\n        'SR': '186.179.128.0/17',\n        'SS': '105.235.208.0/21',\n        'ST': '197.159.160.0/19',\n        'SV': '168.243.0.0/16',\n        'SX': '190.102.0.0/20',\n        'SY': '5.0.0.0/16',\n        'SZ': '41.84.224.0/19',\n        'TC': '65.255.48.0/20',\n        'TD': '154.68.128.0/19',\n        'TG': '196.168.0.0/14',\n        'TH': '171.96.0.0/13',\n        'TJ': '85.9.128.0/18',\n        'TK': '27.96.24.0/21',\n        'TL': '180.189.160.0/20',\n        'TM': '95.85.96.0/19',\n        'TN': '197.0.0.0/11',\n        'TO': '175.176.144.0/21',\n        'TR': '78.160.0.0/11',\n        'TT': '186.44.0.0/15',\n        'TV': '202.2.96.0/19',\n        'TW': '120.96.0.0/11',\n        'TZ': '156.156.0.0/14',\n        'UA': '37.52.0.0/14',\n        'UG': '102.80.0.0/13',\n        'US': '6.0.0.0/8',\n        'UY': '167.56.0.0/13',\n        'UZ': '84.54.64.0/18',\n        'VA': '212.77.0.0/19',\n        'VC': '207.191.240.0/21',\n        'VE': '186.88.0.0/13',\n        'VG': '66.81.192.0/20',\n        'VI': '146.226.0.0/16',\n        'VN': '14.160.0.0/11',\n        'VU': '202.80.32.0/20',\n        'WF': '117.20.32.0/21',\n        'WS': '202.4.32.0/19',\n        'YE': '134.35.0.0/16',\n        'YT': '41.242.116.0/22',\n        'ZA': '41.0.0.0/11',\n        'ZM': '102.144.0.0/13',\n        'ZW': '102.177.192.0/18',\n    }\n\n    @classmethod\n    def random_ipv4(cls, code_or_block):\n        if len(code_or_block) == 2:\n            block = cls._country_ip_map.get(code_or_block.upper())\n            if not block:\n                return None\n        else:\n            block = code_or_block\n        addr, preflen = block.split('/')\n        addr_min = struct.unpack('!L', socket.inet_aton(addr))[0]\n        addr_max = addr_min | (0xffffffff >> int(preflen))\n        return str(socket.inet_ntoa(\n            struct.pack('!L', random.randint(addr_min, addr_max))))\n\n\n# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is\n# released into Public Domain\n# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387\n\ndef long_to_bytes(n, blocksize=0):\n    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n    Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front of the\n    byte string with binary zeros so that the length is a multiple of\n    blocksize.\n    \"\"\"\n    # after much testing, this algorithm was deemed to be the fastest\n    s = b''\n    n = int(n)\n    while n > 0:\n        s = struct.pack('>I', n & 0xffffffff) + s\n        n = n >> 32\n    # strip off leading zeros\n    for i in range(len(s)):\n        if s[i] != b'\\000'[0]:\n            break\n    else:\n        # only happens when n == 0\n        s = b'\\000'\n        i = 0\n    s = s[i:]\n    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n    # de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n    return s\n\n\ndef bytes_to_long(s):\n    \"\"\"bytes_to_long(string) : long\n    Convert a byte string to a long integer.\n\n    This is (essentially) the inverse of long_to_bytes().\n    \"\"\"\n    acc = 0\n    length = len(s)\n    if length % 4:\n        extra = (4 - length % 4)\n        s = b'\\000' * extra + s\n        length = length + extra\n    for i in range(0, length, 4):\n        acc = (acc << 32) + struct.unpack('>I', s[i:i + 4])[0]\n    return acc\n\n\ndef ohdave_rsa_encrypt(data, exponent, modulus):\n    '''\n    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n\n    Input:\n        data: data to encrypt, bytes-like object\n        exponent, modulus: parameter e and N of RSA algorithm, both integer\n    Output: hex string of encrypted data\n\n    Limitation: supports one block encryption only\n    '''\n\n    payload = int(binascii.hexlify(data[::-1]), 16)\n    encrypted = pow(payload, exponent, modulus)\n    return '%x' % encrypted\n\n\ndef pkcs1pad(data, length):\n    \"\"\"\n    Padding input data with PKCS#1 scheme\n\n    @param {int[]} data        input data\n    @param {int}   length      target length\n    @returns {int[]}           padded data\n    \"\"\"\n    if len(data) > length - 11:\n        raise ValueError('Input data too long for PKCS#1 padding')\n\n    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n    return [0, 2] + pseudo_random + [0] + data\n\n\ndef _base_n_table(n, table):\n    if not table and not n:\n        raise ValueError('Either table or n must be specified')\n    table = (table or '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')[:n]\n\n    if n and n != len(table):\n        raise ValueError(f'base {n} exceeds table length {len(table)}')\n    return table\n\n\ndef encode_base_n(num, n=None, table=None):\n    \"\"\"Convert given int to a base-n string\"\"\"\n    table = _base_n_table(n, table)\n    if not num:\n        return table[0]\n\n    result, base = '', len(table)\n    while num:\n        result = table[num % base] + result\n        num = num // base\n    return result\n\n\ndef decode_base_n(string, n=None, table=None):\n    \"\"\"Convert given base-n string to int\"\"\"\n    table = {char: index for index, char in enumerate(_base_n_table(n, table))}\n    result, base = 0, len(table)\n    for char in string:\n        result = result * base + table[char]\n    return result\n\n\ndef decode_packed_codes(code):\n    mobj = re.search(PACKED_CODES_RE, code)\n    obfuscated_code, base, count, symbols = mobj.groups()\n    base = int(base)\n    count = int(count)\n    symbols = symbols.split('|')\n    symbol_table = {}\n\n    while count:\n        count -= 1\n        base_n_count = encode_base_n(count, base)\n        symbol_table[base_n_count] = symbols[count] or base_n_count\n\n    return re.sub(\n        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n        obfuscated_code)\n\n\ndef caesar(s, alphabet, shift):\n    if shift == 0:\n        return s\n    l = len(alphabet)\n    return ''.join(\n        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n        for c in s)\n\n\ndef rot47(s):\n    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)\n\n\ndef parse_m3u8_attributes(attrib):\n    info = {}\n    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n        if val.startswith('\"'):\n            val = val[1:-1]\n        info[key] = val\n    return info\n\n\ndef urshift(val, n):\n    return val >> n if val >= 0 else (val + 0x100000000) >> n\n\n\ndef write_xattr(path, key, value):\n    # Windows: Write xattrs to NTFS Alternate Data Streams:\n    # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29\n    if compat_os_name == 'nt':\n        assert ':' not in key\n        assert os.path.exists(path)\n\n        try:\n            with open(f'{path}:{key}', 'wb') as f:\n                f.write(value)\n        except OSError as e:\n            raise XAttrMetadataError(e.errno, e.strerror)\n        return\n\n    # UNIX Method 1. Use xattrs/pyxattrs modules\n\n    setxattr = None\n    if getattr(xattr, '_yt_dlp__identifier', None) == 'pyxattr':\n        # Unicode arguments are not supported in pyxattr until version 0.5.0\n        # See https://github.com/ytdl-org/youtube-dl/issues/5498\n        if version_tuple(xattr.__version__) >= (0, 5, 0):\n            setxattr = xattr.set\n    elif xattr:\n        setxattr = xattr.setxattr\n\n    if setxattr:\n        try:\n            setxattr(path, key, value)\n        except OSError as e:\n            raise XAttrMetadataError(e.errno, e.strerror)\n        return\n\n    # UNIX Method 2. Use setfattr/xattr executables\n    exe = ('setfattr' if check_executable('setfattr', ['--version'])\n           else 'xattr' if check_executable('xattr', ['-h']) else None)\n    if not exe:\n        raise XAttrUnavailableError(\n            'Couldn\\'t find a tool to set the xattrs. Install either the python \"xattr\" or \"pyxattr\" modules or the '\n            + ('\"xattr\" binary' if sys.platform != 'linux' else 'GNU \"attr\" package (which contains the \"setfattr\" tool)'))\n\n    value = value.decode()\n    try:\n        _, stderr, returncode = Popen.run(\n            [exe, '-w', key, value, path] if exe == 'xattr' else [exe, '-n', key, '-v', value, path],\n            text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n    except OSError as e:\n        raise XAttrMetadataError(e.errno, e.strerror)\n    if returncode:\n        raise XAttrMetadataError(returncode, stderr)\n\n\ndef random_birthday(year_field, month_field, day_field):\n    start_date = datetime.date(1950, 1, 1)\n    end_date = datetime.date(1995, 12, 31)\n    offset = random.randint(0, (end_date - start_date).days)\n    random_date = start_date + datetime.timedelta(offset)\n    return {\n        year_field: str(random_date.year),\n        month_field: str(random_date.month),\n        day_field: str(random_date.day),\n    }\n\n\ndef find_available_port(interface=''):\n    try:\n        with socket.socket() as sock:\n            sock.bind((interface, 0))\n            return sock.getsockname()[1]\n    except OSError:\n        return None\n\n\n# Templates for internet shortcut files, which are plain text files.\nDOT_URL_LINK_TEMPLATE = '''\\\n[InternetShortcut]\nURL=%(url)s\n'''\n\nDOT_WEBLOC_LINK_TEMPLATE = '''\\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n\\t<key>URL</key>\n\\t<string>%(url)s</string>\n</dict>\n</plist>\n'''\n\nDOT_DESKTOP_LINK_TEMPLATE = '''\\\n[Desktop Entry]\nEncoding=UTF-8\nName=%(filename)s\nType=Link\nURL=%(url)s\nIcon=text-html\n'''\n\nLINK_TEMPLATES = {\n    'url': DOT_URL_LINK_TEMPLATE,\n    'desktop': DOT_DESKTOP_LINK_TEMPLATE,\n    'webloc': DOT_WEBLOC_LINK_TEMPLATE,\n}\n\n\ndef iri_to_uri(iri):\n    \"\"\"\n    Converts an IRI (Internationalized Resource Identifier, allowing Unicode characters) to a URI (Uniform Resource Identifier, ASCII-only).\n\n    The function doesn't add an additional layer of escaping; e.g., it doesn't escape `%3C` as `%253C`. Instead, it percent-escapes characters with an underlying UTF-8 encoding *besides* those already escaped, leaving the URI intact.\n    \"\"\"\n\n    iri_parts = urllib.parse.urlparse(iri)\n\n    if '[' in iri_parts.netloc:\n        raise ValueError('IPv6 URIs are not, yet, supported.')\n        # Querying `.netloc`, when there's only one bracket, also raises a ValueError.\n\n    # The `safe` argument values, that the following code uses, contain the characters that should not be percent-encoded. Everything else but letters, digits and '_.-' will be percent-encoded with an underlying UTF-8 encoding. Everything already percent-encoded will be left as is.\n\n    net_location = ''\n    if iri_parts.username:\n        net_location += urllib.parse.quote(iri_parts.username, safe=r\"!$%&'()*+,~\")\n        if iri_parts.password is not None:\n            net_location += ':' + urllib.parse.quote(iri_parts.password, safe=r\"!$%&'()*+,~\")\n        net_location += '@'\n\n    net_location += iri_parts.hostname.encode('idna').decode()  # Punycode for Unicode hostnames.\n    # The 'idna' encoding produces ASCII text.\n    if iri_parts.port is not None and iri_parts.port != 80:\n        net_location += ':' + str(iri_parts.port)\n\n    return urllib.parse.urlunparse(\n        (iri_parts.scheme,\n            net_location,\n\n            urllib.parse.quote_plus(iri_parts.path, safe=r\"!$%&'()*+,/:;=@|~\"),\n\n            # Unsure about the `safe` argument, since this is a legacy way of handling parameters.\n            urllib.parse.quote_plus(iri_parts.params, safe=r\"!$%&'()*+,/:;=@|~\"),\n\n            # Not totally sure about the `safe` argument, since the source does not explicitly mention the query URI component.\n            urllib.parse.quote_plus(iri_parts.query, safe=r\"!$%&'()*+,/:;=?@{|}~\"),\n\n            urllib.parse.quote_plus(iri_parts.fragment, safe=r\"!#$%&'()*+,/:;=?@{|}~\")))\n\n    # Source for `safe` arguments: https://url.spec.whatwg.org/#percent-encoded-bytes.\n\n\ndef to_high_limit_path(path):\n    if sys.platform in ['win32', 'cygwin']:\n        # Work around MAX_PATH limitation on Windows. The maximum allowed length for the individual path segments may still be quite limited.\n        return '\\\\\\\\?\\\\' + os.path.abspath(path)\n\n    return path\n\n\ndef format_field(obj, field=None, template='%s', ignore=NO_DEFAULT, default='', func=IDENTITY):\n    val = traversal.traverse_obj(obj, *variadic(field))\n    if not val if ignore is NO_DEFAULT else val in variadic(ignore):\n        return default\n    return template % func(val)\n\n\ndef clean_podcast_url(url):\n    url = re.sub(r'''(?x)\n        (?:\n            (?:\n                chtbl\\.com/track|\n                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/\n                play\\.podtrac\\.com|\n                chrt\\.fm/track|\n                mgln\\.ai/e\n            )(?:/[^/.]+)?|\n            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure\n            flex\\.acast\\.com|\n            pd(?:\n                cn\\.co| # https://podcorn.com/analytics-prefix/\n                st\\.fm # https://podsights.com/docs/\n            )/e|\n            [0-9]\\.gum\\.fm|\n            pscrb\\.fm/rss/p\n        )/''', '', url)\n    return re.sub(r'^\\w+://(\\w+://)', r'\\1', url)\n\n\n_HEX_TABLE = '0123456789abcdef'\n\n\ndef random_uuidv4():\n    return re.sub(r'[xy]', lambda x: _HEX_TABLE[random.randint(0, 15)], 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx')\n\n\ndef make_dir(path, to_screen=None):\n    try:\n        dn = os.path.dirname(path)\n        if dn:\n            os.makedirs(dn, exist_ok=True)\n        return True\n    except OSError as err:\n        if callable(to_screen) is not None:\n            to_screen(f'unable to create directory {err}')\n        return False\n\n\ndef get_executable_path():\n    from ..update import _get_variant_and_executable_path\n\n    return os.path.dirname(os.path.abspath(_get_variant_and_executable_path()[1]))\n\n\ndef get_user_config_dirs(package_name):\n    # .config (e.g. ~/.config/package_name)\n    xdg_config_home = os.getenv('XDG_CONFIG_HOME') or compat_expanduser('~/.config')\n    yield os.path.join(xdg_config_home, package_name)\n\n    # appdata (%APPDATA%/package_name)\n    appdata_dir = os.getenv('appdata')\n    if appdata_dir:\n        yield os.path.join(appdata_dir, package_name)\n\n    # home (~/.package_name)\n    yield os.path.join(compat_expanduser('~'), f'.{package_name}')\n\n\ndef get_system_config_dirs(package_name):\n    # /etc/package_name\n    yield os.path.join('/etc', package_name)\n\n\ndef time_seconds(**kwargs):\n    \"\"\"\n    Returns TZ-aware time in seconds since the epoch (1970-01-01T00:00:00Z)\n    \"\"\"\n    return time.time() + datetime.timedelta(**kwargs).total_seconds()\n\n\n# create a JSON Web Signature (jws) with HS256 algorithm\n# the resulting format is in JWS Compact Serialization\n# implemented following JWT https://www.rfc-editor.org/rfc/rfc7519.html\n# implemented following JWS https://www.rfc-editor.org/rfc/rfc7515.html\ndef jwt_encode_hs256(payload_data, key, headers={}):\n    header_data = {\n        'alg': 'HS256',\n        'typ': 'JWT',\n    }\n    if headers:\n        header_data.update(headers)\n    header_b64 = base64.b64encode(json.dumps(header_data).encode())\n    payload_b64 = base64.b64encode(json.dumps(payload_data).encode())\n    h = hmac.new(key.encode(), header_b64 + b'.' + payload_b64, hashlib.sha256)\n    signature_b64 = base64.b64encode(h.digest())\n    token = header_b64 + b'.' + payload_b64 + b'.' + signature_b64\n    return token\n\n\n# can be extended in future to verify the signature and parse header and return the algorithm used if it's not HS256\ndef jwt_decode_hs256(jwt):\n    header_b64, payload_b64, signature_b64 = jwt.split('.')\n    # add trailing ='s that may have been stripped, superfluous ='s are ignored\n    payload_data = json.loads(base64.urlsafe_b64decode(f'{payload_b64}==='))\n    return payload_data\n\n\nWINDOWS_VT_MODE = False if compat_os_name == 'nt' else None\n\n\n@functools.cache\ndef supports_terminal_sequences(stream):\n    if compat_os_name == 'nt':\n        if not WINDOWS_VT_MODE:\n            return False\n    elif not os.getenv('TERM'):\n        return False\n    try:\n        return stream.isatty()\n    except BaseException:\n        return False\n\n\ndef windows_enable_vt_mode():\n    \"\"\"Ref: https://bugs.python.org/issue30075 \"\"\"\n    if get_windows_version() < (10, 0, 10586):\n        return\n\n    import ctypes\n    import ctypes.wintypes\n    import msvcrt\n\n    ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x0004\n\n    dll = ctypes.WinDLL('kernel32', use_last_error=False)\n    handle = os.open('CONOUT$', os.O_RDWR)\n    try:\n        h_out = ctypes.wintypes.HANDLE(msvcrt.get_osfhandle(handle))\n        dw_original_mode = ctypes.wintypes.DWORD()\n        success = dll.GetConsoleMode(h_out, ctypes.byref(dw_original_mode))\n        if not success:\n            raise Exception('GetConsoleMode failed')\n\n        success = dll.SetConsoleMode(h_out, ctypes.wintypes.DWORD(\n            dw_original_mode.value | ENABLE_VIRTUAL_TERMINAL_PROCESSING))\n        if not success:\n            raise Exception('SetConsoleMode failed')\n    finally:\n        os.close(handle)\n\n    global WINDOWS_VT_MODE\n    WINDOWS_VT_MODE = True\n    supports_terminal_sequences.cache_clear()\n\n\n_terminal_sequences_re = re.compile('\\033\\\\[[^m]+m')\n\n\ndef remove_terminal_sequences(string):\n    return _terminal_sequences_re.sub('', string)\n\n\ndef number_of_digits(number):\n    return len('%d' % number)\n\n\ndef join_nonempty(*values, delim='-', from_dict=None):\n    if from_dict is not None:\n        values = (traversal.traverse_obj(from_dict, variadic(v)) for v in values)\n    return delim.join(map(str, filter(None, values)))\n\n\ndef scale_thumbnails_to_max_format_width(formats, thumbnails, url_width_re):\n    \"\"\"\n    Find the largest format dimensions in terms of video width and, for each thumbnail:\n    * Modify the URL: Match the width with the provided regex and replace with the former width\n    * Update dimensions\n\n    This function is useful with video services that scale the provided thumbnails on demand\n    \"\"\"\n    _keys = ('width', 'height')\n    max_dimensions = max(\n        (tuple(format.get(k) or 0 for k in _keys) for format in formats),\n        default=(0, 0))\n    if not max_dimensions[0]:\n        return thumbnails\n    return [\n        merge_dicts(\n            {'url': re.sub(url_width_re, str(max_dimensions[0]), thumbnail['url'])},\n            dict(zip(_keys, max_dimensions)), thumbnail)\n        for thumbnail in thumbnails\n    ]\n\n\ndef parse_http_range(range):\n    \"\"\" Parse value of \"Range\" or \"Content-Range\" HTTP header into tuple. \"\"\"\n    if not range:\n        return None, None, None\n    crg = re.search(r'bytes[ =](\\d+)-(\\d+)?(?:/(\\d+))?', range)\n    if not crg:\n        return None, None, None\n    return int(crg.group(1)), int_or_none(crg.group(2)), int_or_none(crg.group(3))\n\n\ndef read_stdin(what):\n    eof = 'Ctrl+Z' if compat_os_name == 'nt' else 'Ctrl+D'\n    write_string(f'Reading {what} from STDIN - EOF ({eof}) to end:\\n')\n    return sys.stdin\n\n\ndef determine_file_encoding(data):\n    \"\"\"\n    Detect the text encoding used\n    @returns (encoding, bytes to skip)\n    \"\"\"\n\n    # BOM marks are given priority over declarations\n    for bom, enc in BOMS:\n        if data.startswith(bom):\n            return enc, len(bom)\n\n    # Strip off all null bytes to match even when UTF-16 or UTF-32 is used.\n    # We ignore the endianness to get a good enough match\n    data = data.replace(b'\\0', b'')\n    mobj = re.match(rb'(?m)^#\\s*coding\\s*:\\s*(\\S+)\\s*$', data)\n    return mobj.group(1).decode() if mobj else None, 0\n\n\nclass Config:\n    own_args = None\n    parsed_args = None\n    filename = None\n    __initialized = False\n\n    def __init__(self, parser, label=None):\n        self.parser, self.label = parser, label\n        self._loaded_paths, self.configs = set(), []\n\n    def init(self, args=None, filename=None):\n        assert not self.__initialized\n        self.own_args, self.filename = args, filename\n        return self.load_configs()\n\n    def load_configs(self):\n        directory = ''\n        if self.filename:\n            location = os.path.realpath(self.filename)\n            directory = os.path.dirname(location)\n            if location in self._loaded_paths:\n                return False\n            self._loaded_paths.add(location)\n\n        self.__initialized = True\n        opts, _ = self.parser.parse_known_args(self.own_args)\n        self.parsed_args = self.own_args\n        for location in opts.config_locations or []:\n            if location == '-':\n                if location in self._loaded_paths:\n                    continue\n                self._loaded_paths.add(location)\n                self.append_config(shlex.split(read_stdin('options'), comments=True), label='stdin')\n                continue\n            location = os.path.join(directory, expand_path(location))\n            if os.path.isdir(location):\n                location = os.path.join(location, 'yt-dlp.conf')\n            if not os.path.exists(location):\n                self.parser.error(f'config location {location} does not exist')\n            self.append_config(self.read_file(location), location)\n        return True\n\n    def __str__(self):\n        label = join_nonempty(\n            self.label, 'config', f'\"{self.filename}\"' if self.filename else '',\n            delim=' ')\n        return join_nonempty(\n            self.own_args is not None and f'{label[0].upper()}{label[1:]}: {self.hide_login_info(self.own_args)}',\n            *(f'\\n{c}'.replace('\\n', '\\n| ')[1:] for c in self.configs),\n            delim='\\n')\n\n    @staticmethod\n    def read_file(filename, default=[]):\n        try:\n            optionf = open(filename, 'rb')\n        except OSError:\n            return default  # silently skip if file is not present\n        try:\n            enc, skip = determine_file_encoding(optionf.read(512))\n            optionf.seek(skip, io.SEEK_SET)\n        except OSError:\n            enc = None  # silently skip read errors\n        try:\n            # FIXME: https://github.com/ytdl-org/youtube-dl/commit/dfe5fa49aed02cf36ba9f743b11b0903554b5e56\n            contents = optionf.read().decode(enc or preferredencoding())\n            res = shlex.split(contents, comments=True)\n        except Exception as err:\n            raise ValueError(f'Unable to parse \"{filename}\": {err}')\n        finally:\n            optionf.close()\n        return res\n\n    @staticmethod\n    def hide_login_info(opts):\n        PRIVATE_OPTS = {'-p', '--password', '-u', '--username', '--video-password', '--ap-password', '--ap-username'}\n        eqre = re.compile('^(?P<key>' + ('|'.join(re.escape(po) for po in PRIVATE_OPTS)) + ')=.+$')\n\n        def _scrub_eq(o):\n            m = eqre.match(o)\n            if m:\n                return m.group('key') + '=PRIVATE'\n            else:\n                return o\n\n        opts = list(map(_scrub_eq, opts))\n        for idx, opt in enumerate(opts):\n            if opt in PRIVATE_OPTS and idx + 1 < len(opts):\n                opts[idx + 1] = 'PRIVATE'\n        return opts\n\n    def append_config(self, *args, label=None):\n        config = type(self)(self.parser, label)\n        config._loaded_paths = self._loaded_paths\n        if config.init(*args):\n            self.configs.append(config)\n\n    @property\n    def all_args(self):\n        for config in reversed(self.configs):\n            yield from config.all_args\n        yield from self.parsed_args or []\n\n    def parse_known_args(self, **kwargs):\n        return self.parser.parse_known_args(self.all_args, **kwargs)\n\n    def parse_args(self):\n        return self.parser.parse_args(self.all_args)\n\n\nclass WebSocketsWrapper:\n    \"\"\"Wraps websockets module to use in non-async scopes\"\"\"\n    pool = None\n\n    def __init__(self, url, headers=None, connect=True):\n        self.loop = asyncio.new_event_loop()\n        # XXX: \"loop\" is deprecated\n        self.conn = websockets.connect(\n            url, extra_headers=headers, ping_interval=None,\n            close_timeout=float('inf'), loop=self.loop, ping_timeout=float('inf'))\n        if connect:\n            self.__enter__()\n        atexit.register(self.__exit__, None, None, None)\n\n    def __enter__(self):\n        if not self.pool:\n            self.pool = self.run_with_loop(self.conn.__aenter__(), self.loop)\n        return self\n\n    def send(self, *args):\n        self.run_with_loop(self.pool.send(*args), self.loop)\n\n    def recv(self, *args):\n        return self.run_with_loop(self.pool.recv(*args), self.loop)\n\n    def __exit__(self, type, value, traceback):\n        try:\n            return self.run_with_loop(self.conn.__aexit__(type, value, traceback), self.loop)\n        finally:\n            self.loop.close()\n            self._cancel_all_tasks(self.loop)\n\n    # taken from https://github.com/python/cpython/blob/3.9/Lib/asyncio/runners.py with modifications\n    # for contributors: If there's any new library using asyncio needs to be run in non-async, move these function out of this class\n    @staticmethod\n    def run_with_loop(main, loop):\n        if not asyncio.iscoroutine(main):\n            raise ValueError(f'a coroutine was expected, got {main!r}')\n\n        try:\n            return loop.run_until_complete(main)\n        finally:\n            loop.run_until_complete(loop.shutdown_asyncgens())\n            if hasattr(loop, 'shutdown_default_executor'):\n                loop.run_until_complete(loop.shutdown_default_executor())\n\n    @staticmethod\n    def _cancel_all_tasks(loop):\n        to_cancel = asyncio.all_tasks(loop)\n\n        if not to_cancel:\n            return\n\n        for task in to_cancel:\n            task.cancel()\n\n        # XXX: \"loop\" is removed in python 3.10+\n        loop.run_until_complete(\n            asyncio.gather(*to_cancel, loop=loop, return_exceptions=True))\n\n        for task in to_cancel:\n            if task.cancelled():\n                continue\n            if task.exception() is not None:\n                loop.call_exception_handler({\n                    'message': 'unhandled exception during asyncio.run() shutdown',\n                    'exception': task.exception(),\n                    'task': task,\n                })\n\n\ndef merge_headers(*dicts):\n    \"\"\"Merge dicts of http headers case insensitively, prioritizing the latter ones\"\"\"\n    return {k.title(): v for k, v in itertools.chain.from_iterable(map(dict.items, dicts))}\n\n\ndef cached_method(f):\n    \"\"\"Cache a method\"\"\"\n    signature = inspect.signature(f)\n\n    @functools.wraps(f)\n    def wrapper(self, *args, **kwargs):\n        bound_args = signature.bind(self, *args, **kwargs)\n        bound_args.apply_defaults()\n        key = tuple(bound_args.arguments.values())[1:]\n\n        cache = vars(self).setdefault('_cached_method__cache', {}).setdefault(f.__name__, {})\n        if key not in cache:\n            cache[key] = f(self, *args, **kwargs)\n        return cache[key]\n    return wrapper\n\n\nclass classproperty:\n    \"\"\"property access for class methods with optional caching\"\"\"\n    def __new__(cls, func=None, *args, **kwargs):\n        if not func:\n            return functools.partial(cls, *args, **kwargs)\n        return super().__new__(cls)\n\n    def __init__(self, func, *, cache=False):\n        functools.update_wrapper(self, func)\n        self.func = func\n        self._cache = {} if cache else None\n\n    def __get__(self, _, cls):\n        if self._cache is None:\n            return self.func(cls)\n        elif cls not in self._cache:\n            self._cache[cls] = self.func(cls)\n        return self._cache[cls]\n\n\nclass function_with_repr:\n    def __init__(self, func, repr_=None):\n        functools.update_wrapper(self, func)\n        self.func, self.__repr = func, repr_\n\n    def __call__(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n    def __repr__(self):\n        if self.__repr:\n            return self.__repr\n        return f'{self.func.__module__}.{self.func.__qualname__}'\n\n\nclass Namespace(types.SimpleNamespace):\n    \"\"\"Immutable namespace\"\"\"\n\n    def __iter__(self):\n        return iter(self.__dict__.values())\n\n    @property\n    def items_(self):\n        return self.__dict__.items()\n\n\nMEDIA_EXTENSIONS = Namespace(\n    common_video=('avi', 'flv', 'mkv', 'mov', 'mp4', 'webm'),\n    video=('3g2', '3gp', 'f4v', 'mk3d', 'divx', 'mpg', 'ogv', 'm4v', 'wmv'),\n    common_audio=('aiff', 'alac', 'flac', 'm4a', 'mka', 'mp3', 'ogg', 'opus', 'wav'),\n    audio=('aac', 'ape', 'asf', 'f4a', 'f4b', 'm4b', 'm4p', 'm4r', 'oga', 'ogx', 'spx', 'vorbis', 'wma', 'weba'),\n    thumbnails=('jpg', 'png', 'webp'),\n    storyboards=('mhtml', ),\n    subtitles=('srt', 'vtt', 'ass', 'lrc'),\n    manifests=('f4f', 'f4m', 'm3u8', 'smil', 'mpd'),\n)\nMEDIA_EXTENSIONS.video += MEDIA_EXTENSIONS.common_video\nMEDIA_EXTENSIONS.audio += MEDIA_EXTENSIONS.common_audio\n\nKNOWN_EXTENSIONS = (*MEDIA_EXTENSIONS.video, *MEDIA_EXTENSIONS.audio, *MEDIA_EXTENSIONS.manifests)\n\n\nclass RetryManager:\n    \"\"\"Usage:\n        for retry in RetryManager(...):\n            try:\n                ...\n            except SomeException as err:\n                retry.error = err\n                continue\n    \"\"\"\n    attempt, _error = 0, None\n\n    def __init__(self, _retries, _error_callback, **kwargs):\n        self.retries = _retries or 0\n        self.error_callback = functools.partial(_error_callback, **kwargs)\n\n    def _should_retry(self):\n        return self._error is not NO_DEFAULT and self.attempt <= self.retries\n\n    @property\n    def error(self):\n        if self._error is NO_DEFAULT:\n            return None\n        return self._error\n\n    @error.setter\n    def error(self, value):\n        self._error = value\n\n    def __iter__(self):\n        while self._should_retry():\n            self.error = NO_DEFAULT\n            self.attempt += 1\n            yield self\n            if self.error:\n                self.error_callback(self.error, self.attempt, self.retries)\n\n    @staticmethod\n    def report_retry(e, count, retries, *, sleep_func, info, warn, error=None, suffix=None):\n        \"\"\"Utility function for reporting retries\"\"\"\n        if count > retries:\n            if error:\n                return error(f'{e}. Giving up after {count - 1} retries') if count > 1 else error(str(e))\n            raise e\n\n        if not count:\n            return warn(e)\n        elif isinstance(e, ExtractorError):\n            e = remove_end(str_or_none(e.cause) or e.orig_msg, '.')\n        warn(f'{e}. Retrying{format_field(suffix, None, \" %s\")} ({count}/{retries})...')\n\n        delay = float_or_none(sleep_func(n=count - 1)) if callable(sleep_func) else sleep_func\n        if delay:\n            info(f'Sleeping {delay:.2f} seconds ...')\n            time.sleep(delay)\n\n\ndef make_archive_id(ie, video_id):\n    ie_key = ie if isinstance(ie, str) else ie.ie_key()\n    return f'{ie_key.lower()} {video_id}'\n\n\ndef truncate_string(s, left, right=0):\n    assert left > 3 and right >= 0\n    if s is None or len(s) <= left + right:\n        return s\n    return f'{s[:left-3]}...{s[-right:] if right else \"\"}'\n\n\ndef orderedSet_from_options(options, alias_dict, *, use_regex=False, start=None):\n    assert 'all' in alias_dict, '\"all\" alias is required'\n    requested = list(start or [])\n    for val in options:\n        discard = val.startswith('-')\n        if discard:\n            val = val[1:]\n\n        if val in alias_dict:\n            val = alias_dict[val] if not discard else [\n                i[1:] if i.startswith('-') else f'-{i}' for i in alias_dict[val]]\n            # NB: Do not allow regex in aliases for performance\n            requested = orderedSet_from_options(val, alias_dict, start=requested)\n            continue\n\n        current = (filter(re.compile(val, re.I).fullmatch, alias_dict['all']) if use_regex\n                   else [val] if val in alias_dict['all'] else None)\n        if current is None:\n            raise ValueError(val)\n\n        if discard:\n            for item in current:\n                while item in requested:\n                    requested.remove(item)\n        else:\n            requested.extend(current)\n\n    return orderedSet(requested)\n\n\n# TODO: Rewrite\nclass FormatSorter:\n    regex = r' *((?P<reverse>\\+)?(?P<field>[a-zA-Z0-9_]+)((?P<separator>[~:])(?P<limit>.*?))?)? *$'\n\n    default = ('hidden', 'aud_or_vid', 'hasvid', 'ie_pref', 'lang', 'quality',\n               'res', 'fps', 'hdr:12', 'vcodec:vp9.2', 'channels', 'acodec',\n               'size', 'br', 'asr', 'proto', 'ext', 'hasaud', 'source', 'id')  # These must not be aliases\n    ytdl_default = ('hasaud', 'lang', 'quality', 'tbr', 'filesize', 'vbr',\n                    'height', 'width', 'proto', 'vext', 'abr', 'aext',\n                    'fps', 'fs_approx', 'source', 'id')\n\n    settings = {\n        'vcodec': {'type': 'ordered', 'regex': True,\n                   'order': ['av0?1', 'vp0?9.2', 'vp0?9', '[hx]265|he?vc?', '[hx]264|avc', 'vp0?8', 'mp4v|h263', 'theora', '', None, 'none']},\n        'acodec': {'type': 'ordered', 'regex': True,\n                   'order': ['[af]lac', 'wav|aiff', 'opus', 'vorbis|ogg', 'aac', 'mp?4a?', 'mp3', 'ac-?4', 'e-?a?c-?3', 'ac-?3', 'dts', '', None, 'none']},\n        'hdr': {'type': 'ordered', 'regex': True, 'field': 'dynamic_range',\n                'order': ['dv', '(hdr)?12', r'(hdr)?10\\+', '(hdr)?10', 'hlg', '', 'sdr', None]},\n        'proto': {'type': 'ordered', 'regex': True, 'field': 'protocol',\n                  'order': ['(ht|f)tps', '(ht|f)tp$', 'm3u8.*', '.*dash', 'websocket_frag', 'rtmpe?', '', 'mms|rtsp', 'ws|websocket', 'f4']},\n        'vext': {'type': 'ordered', 'field': 'video_ext',\n                 'order': ('mp4', 'mov', 'webm', 'flv', '', 'none'),\n                 'order_free': ('webm', 'mp4', 'mov', 'flv', '', 'none')},\n        'aext': {'type': 'ordered', 'regex': True, 'field': 'audio_ext',\n                 'order': ('m4a', 'aac', 'mp3', 'ogg', 'opus', 'web[am]', '', 'none'),\n                 'order_free': ('ogg', 'opus', 'web[am]', 'mp3', 'm4a', 'aac', '', 'none')},\n        'hidden': {'visible': False, 'forced': True, 'type': 'extractor', 'max': -1000},\n        'aud_or_vid': {'visible': False, 'forced': True, 'type': 'multiple',\n                       'field': ('vcodec', 'acodec'),\n                       'function': lambda it: int(any(v != 'none' for v in it))},\n        'ie_pref': {'priority': True, 'type': 'extractor'},\n        'hasvid': {'priority': True, 'field': 'vcodec', 'type': 'boolean', 'not_in_list': ('none',)},\n        'hasaud': {'field': 'acodec', 'type': 'boolean', 'not_in_list': ('none',)},\n        'lang': {'convert': 'float', 'field': 'language_preference', 'default': -1},\n        'quality': {'convert': 'float', 'default': -1},\n        'filesize': {'convert': 'bytes'},\n        'fs_approx': {'convert': 'bytes', 'field': 'filesize_approx'},\n        'id': {'convert': 'string', 'field': 'format_id'},\n        'height': {'convert': 'float_none'},\n        'width': {'convert': 'float_none'},\n        'fps': {'convert': 'float_none'},\n        'channels': {'convert': 'float_none', 'field': 'audio_channels'},\n        'tbr': {'convert': 'float_none'},\n        'vbr': {'convert': 'float_none'},\n        'abr': {'convert': 'float_none'},\n        'asr': {'convert': 'float_none'},\n        'source': {'convert': 'float', 'field': 'source_preference', 'default': -1},\n\n        'codec': {'type': 'combined', 'field': ('vcodec', 'acodec')},\n        'br': {'type': 'multiple', 'field': ('tbr', 'vbr', 'abr'), 'convert': 'float_none',\n               'function': lambda it: next(filter(None, it), None)},\n        'size': {'type': 'multiple', 'field': ('filesize', 'fs_approx'), 'convert': 'bytes',\n                 'function': lambda it: next(filter(None, it), None)},\n        'ext': {'type': 'combined', 'field': ('vext', 'aext')},\n        'res': {'type': 'multiple', 'field': ('height', 'width'),\n                'function': lambda it: (lambda l: min(l) if l else 0)(tuple(filter(None, it)))},\n\n        # Actual field names\n        'format_id': {'type': 'alias', 'field': 'id'},\n        'preference': {'type': 'alias', 'field': 'ie_pref'},\n        'language_preference': {'type': 'alias', 'field': 'lang'},\n        'source_preference': {'type': 'alias', 'field': 'source'},\n        'protocol': {'type': 'alias', 'field': 'proto'},\n        'filesize_approx': {'type': 'alias', 'field': 'fs_approx'},\n        'audio_channels': {'type': 'alias', 'field': 'channels'},\n\n        # Deprecated\n        'dimension': {'type': 'alias', 'field': 'res', 'deprecated': True},\n        'resolution': {'type': 'alias', 'field': 'res', 'deprecated': True},\n        'extension': {'type': 'alias', 'field': 'ext', 'deprecated': True},\n        'bitrate': {'type': 'alias', 'field': 'br', 'deprecated': True},\n        'total_bitrate': {'type': 'alias', 'field': 'tbr', 'deprecated': True},\n        'video_bitrate': {'type': 'alias', 'field': 'vbr', 'deprecated': True},\n        'audio_bitrate': {'type': 'alias', 'field': 'abr', 'deprecated': True},\n        'framerate': {'type': 'alias', 'field': 'fps', 'deprecated': True},\n        'filesize_estimate': {'type': 'alias', 'field': 'size', 'deprecated': True},\n        'samplerate': {'type': 'alias', 'field': 'asr', 'deprecated': True},\n        'video_ext': {'type': 'alias', 'field': 'vext', 'deprecated': True},\n        'audio_ext': {'type': 'alias', 'field': 'aext', 'deprecated': True},\n        'video_codec': {'type': 'alias', 'field': 'vcodec', 'deprecated': True},\n        'audio_codec': {'type': 'alias', 'field': 'acodec', 'deprecated': True},\n        'video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},\n        'has_video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},\n        'audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},\n        'has_audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},\n        'extractor': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},\n        'extractor_preference': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},\n    }\n\n    def __init__(self, ydl, field_preference):\n        self.ydl = ydl\n        self._order = []\n        self.evaluate_params(self.ydl.params, field_preference)\n        if ydl.params.get('verbose'):\n            self.print_verbose_info(self.ydl.write_debug)\n\n    def _get_field_setting(self, field, key):\n        if field not in self.settings:\n            if key in ('forced', 'priority'):\n                return False\n            self.ydl.deprecated_feature(f'Using arbitrary fields ({field}) for format sorting is '\n                                        'deprecated and may be removed in a future version')\n            self.settings[field] = {}\n        propObj = self.settings[field]\n        if key not in propObj:\n            type = propObj.get('type')\n            if key == 'field':\n                default = 'preference' if type == 'extractor' else (field,) if type in ('combined', 'multiple') else field\n            elif key == 'convert':\n                default = 'order' if type == 'ordered' else 'float_string' if field else 'ignore'\n            else:\n                default = {'type': 'field', 'visible': True, 'order': [], 'not_in_list': (None,)}.get(key, None)\n            propObj[key] = default\n        return propObj[key]\n\n    def _resolve_field_value(self, field, value, convertNone=False):\n        if value is None:\n            if not convertNone:\n                return None\n        else:\n            value = value.lower()\n        conversion = self._get_field_setting(field, 'convert')\n        if conversion == 'ignore':\n            return None\n        if conversion == 'string':\n            return value\n        elif conversion == 'float_none':\n            return float_or_none(value)\n        elif conversion == 'bytes':\n            return parse_bytes(value)\n        elif conversion == 'order':\n            order_list = (self._use_free_order and self._get_field_setting(field, 'order_free')) or self._get_field_setting(field, 'order')\n            use_regex = self._get_field_setting(field, 'regex')\n            list_length = len(order_list)\n            empty_pos = order_list.index('') if '' in order_list else list_length + 1\n            if use_regex and value is not None:\n                for i, regex in enumerate(order_list):\n                    if regex and re.match(regex, value):\n                        return list_length - i\n                return list_length - empty_pos  # not in list\n            else:  # not regex or  value = None\n                return list_length - (order_list.index(value) if value in order_list else empty_pos)\n        else:\n            if value.isnumeric():\n                return float(value)\n            else:\n                self.settings[field]['convert'] = 'string'\n                return value\n\n    def evaluate_params(self, params, sort_extractor):\n        self._use_free_order = params.get('prefer_free_formats', False)\n        self._sort_user = params.get('format_sort', [])\n        self._sort_extractor = sort_extractor\n\n        def add_item(field, reverse, closest, limit_text):\n            field = field.lower()\n            if field in self._order:\n                return\n            self._order.append(field)\n            limit = self._resolve_field_value(field, limit_text)\n            data = {\n                'reverse': reverse,\n                'closest': False if limit is None else closest,\n                'limit_text': limit_text,\n                'limit': limit}\n            if field in self.settings:\n                self.settings[field].update(data)\n            else:\n                self.settings[field] = data\n\n        sort_list = (\n            tuple(field for field in self.default if self._get_field_setting(field, 'forced'))\n            + (tuple() if params.get('format_sort_force', False)\n                else tuple(field for field in self.default if self._get_field_setting(field, 'priority')))\n            + tuple(self._sort_user) + tuple(sort_extractor) + self.default)\n\n        for item in sort_list:\n            match = re.match(self.regex, item)\n            if match is None:\n                raise ExtractorError('Invalid format sort string \"%s\" given by extractor' % item)\n            field = match.group('field')\n            if field is None:\n                continue\n            if self._get_field_setting(field, 'type') == 'alias':\n                alias, field = field, self._get_field_setting(field, 'field')\n                if self._get_field_setting(alias, 'deprecated'):\n                    self.ydl.deprecated_feature(f'Format sorting alias {alias} is deprecated and may '\n                                                f'be removed in a future version. Please use {field} instead')\n            reverse = match.group('reverse') is not None\n            closest = match.group('separator') == '~'\n            limit_text = match.group('limit')\n\n            has_limit = limit_text is not None\n            has_multiple_fields = self._get_field_setting(field, 'type') == 'combined'\n            has_multiple_limits = has_limit and has_multiple_fields and not self._get_field_setting(field, 'same_limit')\n\n            fields = self._get_field_setting(field, 'field') if has_multiple_fields else (field,)\n            limits = limit_text.split(':') if has_multiple_limits else (limit_text,) if has_limit else tuple()\n            limit_count = len(limits)\n            for (i, f) in enumerate(fields):\n                add_item(f, reverse, closest,\n                         limits[i] if i < limit_count\n                         else limits[0] if has_limit and not has_multiple_limits\n                         else None)\n\n    def print_verbose_info(self, write_debug):\n        if self._sort_user:\n            write_debug('Sort order given by user: %s' % ', '.join(self._sort_user))\n        if self._sort_extractor:\n            write_debug('Sort order given by extractor: %s' % ', '.join(self._sort_extractor))\n        write_debug('Formats sorted by: %s' % ', '.join(['%s%s%s' % (\n            '+' if self._get_field_setting(field, 'reverse') else '', field,\n            '%s%s(%s)' % ('~' if self._get_field_setting(field, 'closest') else ':',\n                          self._get_field_setting(field, 'limit_text'),\n                          self._get_field_setting(field, 'limit'))\n            if self._get_field_setting(field, 'limit_text') is not None else '')\n            for field in self._order if self._get_field_setting(field, 'visible')]))\n\n    def _calculate_field_preference_from_value(self, format, field, type, value):\n        reverse = self._get_field_setting(field, 'reverse')\n        closest = self._get_field_setting(field, 'closest')\n        limit = self._get_field_setting(field, 'limit')\n\n        if type == 'extractor':\n            maximum = self._get_field_setting(field, 'max')\n            if value is None or (maximum is not None and value >= maximum):\n                value = -1\n        elif type == 'boolean':\n            in_list = self._get_field_setting(field, 'in_list')\n            not_in_list = self._get_field_setting(field, 'not_in_list')\n            value = 0 if ((in_list is None or value in in_list) and (not_in_list is None or value not in not_in_list)) else -1\n        elif type == 'ordered':\n            value = self._resolve_field_value(field, value, True)\n\n        # try to convert to number\n        val_num = float_or_none(value, default=self._get_field_setting(field, 'default'))\n        is_num = self._get_field_setting(field, 'convert') != 'string' and val_num is not None\n        if is_num:\n            value = val_num\n\n        return ((-10, 0) if value is None\n                else (1, value, 0) if not is_num  # if a field has mixed strings and numbers, strings are sorted higher\n                else (0, -abs(value - limit), value - limit if reverse else limit - value) if closest\n                else (0, value, 0) if not reverse and (limit is None or value <= limit)\n                else (0, -value, 0) if limit is None or (reverse and value == limit) or value > limit\n                else (-1, value, 0))\n\n    def _calculate_field_preference(self, format, field):\n        type = self._get_field_setting(field, 'type')  # extractor, boolean, ordered, field, multiple\n        get_value = lambda f: format.get(self._get_field_setting(f, 'field'))\n        if type == 'multiple':\n            type = 'field'  # Only 'field' is allowed in multiple for now\n            actual_fields = self._get_field_setting(field, 'field')\n\n            value = self._get_field_setting(field, 'function')(get_value(f) for f in actual_fields)\n        else:\n            value = get_value(field)\n        return self._calculate_field_preference_from_value(format, field, type, value)\n\n    def calculate_preference(self, format):\n        # Determine missing protocol\n        if not format.get('protocol'):\n            format['protocol'] = determine_protocol(format)\n\n        # Determine missing ext\n        if not format.get('ext') and 'url' in format:\n            format['ext'] = determine_ext(format['url'])\n        if format.get('vcodec') == 'none':\n            format['audio_ext'] = format['ext'] if format.get('acodec') != 'none' else 'none'\n            format['video_ext'] = 'none'\n        else:\n            format['video_ext'] = format['ext']\n            format['audio_ext'] = 'none'\n        # if format.get('preference') is None and format.get('ext') in ('f4f', 'f4m'):  # Not supported?\n        #    format['preference'] = -1000\n\n        if format.get('preference') is None and format.get('ext') == 'flv' and re.match('[hx]265|he?vc?', format.get('vcodec') or ''):\n            # HEVC-over-FLV is out-of-spec by FLV's original spec\n            # ref. https://trac.ffmpeg.org/ticket/6389\n            # ref. https://github.com/yt-dlp/yt-dlp/pull/5821\n            format['preference'] = -100\n\n        # Determine missing bitrates\n        if format.get('vcodec') == 'none':\n            format['vbr'] = 0\n        if format.get('acodec') == 'none':\n            format['abr'] = 0\n        if not format.get('vbr') and format.get('vcodec') != 'none':\n            format['vbr'] = try_call(lambda: format['tbr'] - format['abr']) or None\n        if not format.get('abr') and format.get('acodec') != 'none':\n            format['abr'] = try_call(lambda: format['tbr'] - format['vbr']) or None\n        if not format.get('tbr'):\n            format['tbr'] = try_call(lambda: format['vbr'] + format['abr']) or None\n\n        return tuple(self._calculate_field_preference(format, field) for field in self._order)\n\n\n# XXX: Temporary\nclass _YDLLogger:\n    def __init__(self, ydl=None):\n        self._ydl = ydl\n\n    def debug(self, message):\n        if self._ydl:\n            self._ydl.write_debug(message)\n\n    def info(self, message):\n        if self._ydl:\n            self._ydl.to_screen(message)\n\n    def warning(self, message, *, once=False):\n        if self._ydl:\n            self._ydl.report_warning(message, once)\n\n    def error(self, message, *, is_error=True):\n        if self._ydl:\n            self._ydl.report_error(message, is_error=is_error)\n\n    def stdout(self, message):\n        if self._ydl:\n            self._ydl.to_stdout(message)\n\n    def stderr(self, message):\n        if self._ydl:\n            self._ydl.to_stderr(message)\n"], "fixing_code": ["[\n    {\n        \"action\": \"add\",\n        \"when\": \"29cb20bd563c02671b31dd840139e93dd37150a1\",\n        \"short\": \"[priority] **A new release type has been added!**\\n    * [`nightly`](https://github.com/yt-dlp/yt-dlp/releases/tag/nightly) builds will be made after each push, containing the latest fixes (but also possibly bugs).\\n    * When using `--update`/`-U`, a release binary will only update to its current channel (either `stable` or `nightly`).\\n    * The `--update-to` option has been added allowing the user more control over program upgrades (or downgrades).\\n    * `--update-to` can change the release channel (`stable`, `nightly`) and also upgrade or downgrade to specific tags.\\n    * **Usage**: `--update-to CHANNEL`, `--update-to TAG`, `--update-to CHANNEL@TAG`\"\n    },\n    {\n        \"action\": \"add\",\n        \"when\": \"5038f6d713303e0967d002216e7a88652401c22a\",\n        \"short\": \"[priority] **YouTube throttling fixes!**\"\n    },\n    {\n        \"action\": \"remove\",\n        \"when\": \"2e023649ea4e11151545a34dc1360c114981a236\"\n    },\n    {\n        \"action\": \"add\",\n        \"when\": \"01aba2519a0884ef17d5f85608dbd2a455577147\",\n        \"short\": \"[priority] YouTube: Improved throttling and signature fixes\"\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"c86e433c35fe5da6cb29f3539eef97497f84ed38\",\n        \"short\": \"[extractor/niconico:series] Fix extraction (#6898)\",\n        \"authors\": [\"sqrtNOT\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"69a40e4a7f6caa5662527ebd2f3c4e8aa02857a2\",\n        \"short\": \"[extractor/youtube:music_search_url] Extract title (#7102)\",\n        \"authors\": [\"kangalio\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"8417f26b8a819cd7ffcd4e000ca3e45033e670fb\",\n        \"short\": \"Add option `--color` (#6904)\",\n        \"authors\": [\"Grub4K\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"b4e0d75848e9447cee2cd3646ce54d4744a7ff56\",\n        \"short\": \"Improve `--download-sections`\\n    - Support negative time-ranges\\n    - Add `*from-url` to obey time-ranges in URL\",\n        \"authors\": [\"pukkandan\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"1e75d97db21152acc764b30a688e516f04b8a142\",\n        \"short\": \"[extractor/youtube] Add `ios` to default clients used\\n        - IOS is affected neither by 403 nor by nsig so helps mitigate them preemptively\\n        - IOS also has higher bit-rate 'premium' formats though they are not labeled as such\",\n        \"authors\": [\"pukkandan\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"f2ff0f6f1914b82d4a51681a72cc0828115dcb4a\",\n        \"short\": \"[extractor/motherless] Add gallery support, fix groups (#7211)\",\n        \"authors\": [\"rexlambert22\", \"Ti4eeT4e\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"a4486bfc1dc7057efca9dd3fe70d7fa25c56f700\",\n        \"short\": \"[misc] Revert \\\"Add automatic duplicate issue detection\\\"\",\n        \"authors\": [\"pukkandan\"]\n    },\n    {\n        \"action\": \"add\",\n        \"when\": \"1ceb657bdd254ad961489e5060f2ccc7d556b729\",\n        \"short\": \"[priority] Security: [[CVE-2023-35934](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-35934)] Fix [Cookie leak](https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-v8mc-9377-rwjj)\\n    - `--add-header Cookie:` is deprecated and auto-scoped to input URL domains\\n    - Cookies are scoped when passed to external downloaders\\n    - Add `cookies` field to info.json and deprecate `http_headers.Cookie`\"\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"b03fa7834579a01cc5fba48c0e73488a16683d48\",\n        \"short\": \"[ie/twitter] Revert 92315c03774cfabb3a921884326beb4b981f786b\",\n        \"authors\": [\"pukkandan\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"fcd6a76adc49d5cd8783985c7ce35384b72e545f\",\n        \"short\": \"[test] Add tests for socks proxies (#7908)\",\n        \"authors\": [\"coletdjnz\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"4bf912282a34b58b6b35d8f7e6be535770c89c76\",\n        \"short\": \"[rh:urllib] Remove dot segments during URL normalization (#7662)\",\n        \"authors\": [\"coletdjnz\"]\n    },\n    {\n        \"action\": \"change\",\n        \"when\": \"59e92b1f1833440bb2190f847eb735cf0f90bc85\",\n        \"short\": \"[rh:urllib] Simplify gzip decoding (#7611)\",\n        \"authors\": [\"Grub4K\"]\n    },\n    {\n        \"action\": \"add\",\n        \"when\": \"c1d71d0d9f41db5e4306c86af232f5f6220a130b\",\n        \"short\": \"[priority] **The minimum *recommended* Python version has been raised to 3.8**\\nSince Python 3.7 has reached end-of-life, support for it will be dropped soon. [Read more](https://github.com/yt-dlp/yt-dlp/issues/7803)\"\n    },\n    {\n        \"action\": \"add\",\n        \"when\": \"61bdf15fc7400601c3da1aa7a43917310a5bf391\",\n        \"short\": \"[priority] Security: [[CVE-2023-40581](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-40581)] [Prevent RCE when using `--exec` with `%q` on Windows](https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-42h4-v29r-42qg)\\n    - The shell escape function is now using `\\\"\\\"` instead of `\\\\\\\"`.\\n    - `utils.Popen` has been patched to properly quote commands.\"\n    }\n]\n", "#!/usr/bin/env python3\n\n# Allow direct execution\nimport os\nimport sys\nimport unittest\n\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n\nimport copy\nimport json\n\nfrom test.helper import FakeYDL, assertRegexpMatches, try_rm\nfrom yt_dlp import YoutubeDL\nfrom yt_dlp.compat import compat_os_name\nfrom yt_dlp.extractor import YoutubeIE\nfrom yt_dlp.extractor.common import InfoExtractor\nfrom yt_dlp.postprocessor.common import PostProcessor\nfrom yt_dlp.utils import (\n    ExtractorError,\n    LazyList,\n    OnDemandPagedList,\n    int_or_none,\n    match_filter_func,\n)\nfrom yt_dlp.utils.traversal import traverse_obj\n\nTEST_URL = 'http://localhost/sample.mp4'\n\n\nclass YDL(FakeYDL):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.downloaded_info_dicts = []\n        self.msgs = []\n\n    def process_info(self, info_dict):\n        self.downloaded_info_dicts.append(info_dict.copy())\n\n    def to_screen(self, msg, *args, **kwargs):\n        self.msgs.append(msg)\n\n    def dl(self, *args, **kwargs):\n        assert False, 'Downloader must not be invoked for test_YoutubeDL'\n\n\ndef _make_result(formats, **kwargs):\n    res = {\n        'formats': formats,\n        'id': 'testid',\n        'title': 'testttitle',\n        'extractor': 'testex',\n        'extractor_key': 'TestEx',\n        'webpage_url': 'http://example.com/watch?v=shenanigans',\n    }\n    res.update(**kwargs)\n    return res\n\n\nclass TestFormatSelection(unittest.TestCase):\n    def test_prefer_free_formats(self):\n        # Same resolution => download webm\n        ydl = YDL()\n        ydl.params['prefer_free_formats'] = True\n        formats = [\n            {'ext': 'webm', 'height': 460, 'url': TEST_URL},\n            {'ext': 'mp4', 'height': 460, 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['ext'], 'webm')\n\n        # Different resolution => download best quality (mp4)\n        ydl = YDL()\n        ydl.params['prefer_free_formats'] = True\n        formats = [\n            {'ext': 'webm', 'height': 720, 'url': TEST_URL},\n            {'ext': 'mp4', 'height': 1080, 'url': TEST_URL},\n        ]\n        info_dict['formats'] = formats\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['ext'], 'mp4')\n\n        # No prefer_free_formats => prefer mp4 and webm\n        ydl = YDL()\n        ydl.params['prefer_free_formats'] = False\n        formats = [\n            {'ext': 'webm', 'height': 720, 'url': TEST_URL},\n            {'ext': 'mp4', 'height': 720, 'url': TEST_URL},\n            {'ext': 'flv', 'height': 720, 'url': TEST_URL},\n        ]\n        info_dict['formats'] = formats\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['ext'], 'mp4')\n\n        ydl = YDL()\n        ydl.params['prefer_free_formats'] = False\n        formats = [\n            {'ext': 'flv', 'height': 720, 'url': TEST_URL},\n            {'ext': 'webm', 'height': 720, 'url': TEST_URL},\n        ]\n        info_dict['formats'] = formats\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['ext'], 'webm')\n\n    def test_format_selection(self):\n        formats = [\n            {'format_id': '35', 'ext': 'mp4', 'preference': 0, 'url': TEST_URL},\n            {'format_id': 'example-with-dashes', 'ext': 'webm', 'preference': 1, 'url': TEST_URL},\n            {'format_id': '45', 'ext': 'webm', 'preference': 2, 'url': TEST_URL},\n            {'format_id': '47', 'ext': 'webm', 'preference': 3, 'url': TEST_URL},\n            {'format_id': '2', 'ext': 'flv', 'preference': 4, 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        def test(inp, *expected, multi=False):\n            ydl = YDL({\n                'format': inp,\n                'allow_multiple_video_streams': multi,\n                'allow_multiple_audio_streams': multi,\n            })\n            ydl.process_ie_result(info_dict.copy())\n            downloaded = map(lambda x: x['format_id'], ydl.downloaded_info_dicts)\n            self.assertEqual(list(downloaded), list(expected))\n\n        test('20/47', '47')\n        test('20/71/worst', '35')\n        test(None, '2')\n        test('webm/mp4', '47')\n        test('3gp/40/mp4', '35')\n        test('example-with-dashes', 'example-with-dashes')\n        test('all', '2', '47', '45', 'example-with-dashes', '35')\n        test('mergeall', '2+47+45+example-with-dashes+35', multi=True)\n\n    def test_format_selection_audio(self):\n        formats = [\n            {'format_id': 'audio-low', 'ext': 'webm', 'preference': 1, 'vcodec': 'none', 'url': TEST_URL},\n            {'format_id': 'audio-mid', 'ext': 'webm', 'preference': 2, 'vcodec': 'none', 'url': TEST_URL},\n            {'format_id': 'audio-high', 'ext': 'flv', 'preference': 3, 'vcodec': 'none', 'url': TEST_URL},\n            {'format_id': 'vid', 'ext': 'mp4', 'preference': 4, 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'bestaudio'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'audio-high')\n\n        ydl = YDL({'format': 'worstaudio'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'audio-low')\n\n        formats = [\n            {'format_id': 'vid-low', 'ext': 'mp4', 'preference': 1, 'url': TEST_URL},\n            {'format_id': 'vid-high', 'ext': 'mp4', 'preference': 2, 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'bestaudio/worstaudio/best'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'vid-high')\n\n    def test_format_selection_audio_exts(self):\n        formats = [\n            {'format_id': 'mp3-64', 'ext': 'mp3', 'abr': 64, 'url': 'http://_', 'vcodec': 'none'},\n            {'format_id': 'ogg-64', 'ext': 'ogg', 'abr': 64, 'url': 'http://_', 'vcodec': 'none'},\n            {'format_id': 'aac-64', 'ext': 'aac', 'abr': 64, 'url': 'http://_', 'vcodec': 'none'},\n            {'format_id': 'mp3-32', 'ext': 'mp3', 'abr': 32, 'url': 'http://_', 'vcodec': 'none'},\n            {'format_id': 'aac-32', 'ext': 'aac', 'abr': 32, 'url': 'http://_', 'vcodec': 'none'},\n        ]\n\n        info_dict = _make_result(formats)\n        ydl = YDL({'format': 'best'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(copy.deepcopy(info_dict))\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'aac-64')\n\n        ydl = YDL({'format': 'mp3'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(copy.deepcopy(info_dict))\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'mp3-64')\n\n        ydl = YDL({'prefer_free_formats': True})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(copy.deepcopy(info_dict))\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'ogg-64')\n\n    def test_format_selection_video(self):\n        formats = [\n            {'format_id': 'dash-video-low', 'ext': 'mp4', 'preference': 1, 'acodec': 'none', 'url': TEST_URL},\n            {'format_id': 'dash-video-high', 'ext': 'mp4', 'preference': 2, 'acodec': 'none', 'url': TEST_URL},\n            {'format_id': 'vid', 'ext': 'mp4', 'preference': 3, 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'bestvideo'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'dash-video-high')\n\n        ydl = YDL({'format': 'worstvideo'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'dash-video-low')\n\n        ydl = YDL({'format': 'bestvideo[format_id^=dash][format_id$=low]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'dash-video-low')\n\n        formats = [\n            {'format_id': 'vid-vcodec-dot', 'ext': 'mp4', 'preference': 1, 'vcodec': 'avc1.123456', 'acodec': 'none', 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'bestvideo[vcodec=avc1.123456]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'vid-vcodec-dot')\n\n    def test_format_selection_string_ops(self):\n        formats = [\n            {'format_id': 'abc-cba', 'ext': 'mp4', 'url': TEST_URL},\n            {'format_id': 'zxc-cxz', 'ext': 'webm', 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        # equals (=)\n        ydl = YDL({'format': '[format_id=abc-cba]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'abc-cba')\n\n        # does not equal (!=)\n        ydl = YDL({'format': '[format_id!=abc-cba]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n\n        ydl = YDL({'format': '[format_id!=abc-cba][format_id!=zxc-cxz]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n        # starts with (^=)\n        ydl = YDL({'format': '[format_id^=abc]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'abc-cba')\n\n        # does not start with (!^=)\n        ydl = YDL({'format': '[format_id!^=abc]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n\n        ydl = YDL({'format': '[format_id!^=abc][format_id!^=zxc]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n        # ends with ($=)\n        ydl = YDL({'format': '[format_id$=cba]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'abc-cba')\n\n        # does not end with (!$=)\n        ydl = YDL({'format': '[format_id!$=cba]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n\n        ydl = YDL({'format': '[format_id!$=cba][format_id!$=cxz]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n        # contains (*=)\n        ydl = YDL({'format': '[format_id*=bc-cb]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'abc-cba')\n\n        # does not contain (!*=)\n        ydl = YDL({'format': '[format_id!*=bc-cb]'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'zxc-cxz')\n\n        ydl = YDL({'format': '[format_id!*=abc][format_id!*=zxc]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n        ydl = YDL({'format': '[format_id!*=-]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n    def test_youtube_format_selection(self):\n        # FIXME: Rewrite in accordance with the new format sorting options\n        return\n\n        order = [\n            '38', '37', '46', '22', '45', '35', '44', '18', '34', '43', '6', '5', '17', '36', '13',\n            # Apple HTTP Live Streaming\n            '96', '95', '94', '93', '92', '132', '151',\n            # 3D\n            '85', '84', '102', '83', '101', '82', '100',\n            # Dash video\n            '137', '248', '136', '247', '135', '246',\n            '245', '244', '134', '243', '133', '242', '160',\n            # Dash audio\n            '141', '172', '140', '171', '139',\n        ]\n\n        def format_info(f_id):\n            info = YoutubeIE._formats[f_id].copy()\n\n            # XXX: In real cases InfoExtractor._parse_mpd_formats() fills up 'acodec'\n            # and 'vcodec', while in tests such information is incomplete since\n            # commit a6c2c24479e5f4827ceb06f64d855329c0a6f593\n            # test_YoutubeDL.test_youtube_format_selection is broken without\n            # this fix\n            if 'acodec' in info and 'vcodec' not in info:\n                info['vcodec'] = 'none'\n            elif 'vcodec' in info and 'acodec' not in info:\n                info['acodec'] = 'none'\n\n            info['format_id'] = f_id\n            info['url'] = 'url:' + f_id\n            return info\n        formats_order = [format_info(f_id) for f_id in order]\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': 'bestvideo+bestaudio'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], '248+172')\n        self.assertEqual(downloaded['ext'], 'mp4')\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': 'bestvideo[height>=999999]+bestaudio/best'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], '38')\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': 'bestvideo/best,bestaudio'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n        self.assertEqual(downloaded_ids, ['137', '141'])\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': '(bestvideo[ext=mp4],bestvideo[ext=webm])+bestaudio'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n        self.assertEqual(downloaded_ids, ['137+141', '248+141'])\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': '(bestvideo[ext=mp4],bestvideo[ext=webm])[height<=720]+bestaudio'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n        self.assertEqual(downloaded_ids, ['136+141', '247+141'])\n\n        info_dict = _make_result(list(formats_order), extractor='youtube')\n        ydl = YDL({'format': '(bestvideo[ext=none]/bestvideo[ext=webm])+bestaudio'})\n        ydl.sort_formats(info_dict)\n        ydl.process_ie_result(info_dict)\n        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n        self.assertEqual(downloaded_ids, ['248+141'])\n\n        for f1, f2 in zip(formats_order, formats_order[1:]):\n            info_dict = _make_result([f1, f2], extractor='youtube')\n            ydl = YDL({'format': 'best/bestvideo'})\n            ydl.sort_formats(info_dict)\n            ydl.process_ie_result(info_dict)\n            downloaded = ydl.downloaded_info_dicts[0]\n            self.assertEqual(downloaded['format_id'], f1['format_id'])\n\n            info_dict = _make_result([f2, f1], extractor='youtube')\n            ydl = YDL({'format': 'best/bestvideo'})\n            ydl.sort_formats(info_dict)\n            ydl.process_ie_result(info_dict)\n            downloaded = ydl.downloaded_info_dicts[0]\n            self.assertEqual(downloaded['format_id'], f1['format_id'])\n\n    def test_audio_only_extractor_format_selection(self):\n        # For extractors with incomplete formats (all formats are audio-only or\n        # video-only) best and worst should fallback to corresponding best/worst\n        # video-only or audio-only formats (as per\n        # https://github.com/ytdl-org/youtube-dl/pull/5556)\n        formats = [\n            {'format_id': 'low', 'ext': 'mp3', 'preference': 1, 'vcodec': 'none', 'url': TEST_URL},\n            {'format_id': 'high', 'ext': 'mp3', 'preference': 2, 'vcodec': 'none', 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'best'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'high')\n\n        ydl = YDL({'format': 'worst'})\n        ydl.process_ie_result(info_dict.copy())\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'low')\n\n    def test_format_not_available(self):\n        formats = [\n            {'format_id': 'regular', 'ext': 'mp4', 'height': 360, 'url': TEST_URL},\n            {'format_id': 'video', 'ext': 'mp4', 'height': 720, 'acodec': 'none', 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        # This must fail since complete video-audio format does not match filter\n        # and extractor does not provide incomplete only formats (i.e. only\n        # video-only or audio-only).\n        ydl = YDL({'format': 'best[height>360]'})\n        self.assertRaises(ExtractorError, ydl.process_ie_result, info_dict.copy())\n\n    def test_format_selection_issue_10083(self):\n        # See https://github.com/ytdl-org/youtube-dl/issues/10083\n        formats = [\n            {'format_id': 'regular', 'height': 360, 'url': TEST_URL},\n            {'format_id': 'video', 'height': 720, 'acodec': 'none', 'url': TEST_URL},\n            {'format_id': 'audio', 'vcodec': 'none', 'url': TEST_URL},\n        ]\n        info_dict = _make_result(formats)\n\n        ydl = YDL({'format': 'best[height>360]/bestvideo[height>360]+bestaudio'})\n        ydl.process_ie_result(info_dict.copy())\n        self.assertEqual(ydl.downloaded_info_dicts[0]['format_id'], 'video+audio')\n\n    def test_invalid_format_specs(self):\n        def assert_syntax_error(format_spec):\n            self.assertRaises(SyntaxError, YDL, {'format': format_spec})\n\n        assert_syntax_error('bestvideo,,best')\n        assert_syntax_error('+bestaudio')\n        assert_syntax_error('bestvideo+')\n        assert_syntax_error('/')\n        assert_syntax_error('[720<height]')\n\n    def test_format_filtering(self):\n        formats = [\n            {'format_id': 'A', 'filesize': 500, 'width': 1000},\n            {'format_id': 'B', 'filesize': 1000, 'width': 500},\n            {'format_id': 'C', 'filesize': 1000, 'width': 400},\n            {'format_id': 'D', 'filesize': 2000, 'width': 600},\n            {'format_id': 'E', 'filesize': 3000},\n            {'format_id': 'F'},\n            {'format_id': 'G', 'filesize': 1000000},\n        ]\n        for f in formats:\n            f['url'] = 'http://_/'\n            f['ext'] = 'unknown'\n        info_dict = _make_result(formats, _format_sort_fields=('id', ))\n\n        ydl = YDL({'format': 'best[filesize<3000]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'D')\n\n        ydl = YDL({'format': 'best[filesize<=3000]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'E')\n\n        ydl = YDL({'format': 'best[filesize <= ? 3000]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'F')\n\n        ydl = YDL({'format': 'best [filesize = 1000] [width>450]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'B')\n\n        ydl = YDL({'format': 'best [filesize = 1000] [width!=450]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'C')\n\n        ydl = YDL({'format': '[filesize>?1]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'G')\n\n        ydl = YDL({'format': '[filesize<1M]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'E')\n\n        ydl = YDL({'format': '[filesize<1MiB]'})\n        ydl.process_ie_result(info_dict)\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['format_id'], 'G')\n\n        ydl = YDL({'format': 'all[width>=400][width<=600]'})\n        ydl.process_ie_result(info_dict)\n        downloaded_ids = [info['format_id'] for info in ydl.downloaded_info_dicts]\n        self.assertEqual(downloaded_ids, ['D', 'C', 'B'])\n\n        ydl = YDL({'format': 'best[height<40]'})\n        try:\n            ydl.process_ie_result(info_dict)\n        except ExtractorError:\n            pass\n        self.assertEqual(ydl.downloaded_info_dicts, [])\n\n    def test_default_format_spec(self):\n        ydl = YDL({'simulate': True})\n        self.assertEqual(ydl._default_format_spec({}), 'bestvideo*+bestaudio/best')\n\n        ydl = YDL({})\n        self.assertEqual(ydl._default_format_spec({'is_live': True}), 'best/bestvideo+bestaudio')\n\n        ydl = YDL({'simulate': True})\n        self.assertEqual(ydl._default_format_spec({'is_live': True}), 'bestvideo*+bestaudio/best')\n\n        ydl = YDL({'outtmpl': '-'})\n        self.assertEqual(ydl._default_format_spec({}), 'best/bestvideo+bestaudio')\n\n        ydl = YDL({})\n        self.assertEqual(ydl._default_format_spec({}, download=False), 'bestvideo*+bestaudio/best')\n        self.assertEqual(ydl._default_format_spec({'is_live': True}), 'best/bestvideo+bestaudio')\n\n\nclass TestYoutubeDL(unittest.TestCase):\n    def test_subtitles(self):\n        def s_formats(lang, autocaption=False):\n            return [{\n                'ext': ext,\n                'url': f'http://localhost/video.{lang}.{ext}',\n                '_auto': autocaption,\n            } for ext in ['vtt', 'srt', 'ass']]\n        subtitles = {l: s_formats(l) for l in ['en', 'fr', 'es']}\n        auto_captions = {l: s_formats(l, True) for l in ['it', 'pt', 'es']}\n        info_dict = {\n            'id': 'test',\n            'title': 'Test',\n            'url': 'http://localhost/video.mp4',\n            'subtitles': subtitles,\n            'automatic_captions': auto_captions,\n            'extractor': 'TEST',\n            'webpage_url': 'http://example.com/watch?v=shenanigans',\n        }\n\n        def get_info(params={}):\n            params.setdefault('simulate', True)\n            ydl = YDL(params)\n            ydl.report_warning = lambda *args, **kargs: None\n            return ydl.process_video_result(info_dict, download=False)\n\n        result = get_info()\n        self.assertFalse(result.get('requested_subtitles'))\n        self.assertEqual(result['subtitles'], subtitles)\n        self.assertEqual(result['automatic_captions'], auto_captions)\n\n        result = get_info({'writesubtitles': True})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'en'})\n        self.assertTrue(subs['en'].get('data') is None)\n        self.assertEqual(subs['en']['ext'], 'ass')\n\n        result = get_info({'writesubtitles': True, 'subtitlesformat': 'foo/srt'})\n        subs = result['requested_subtitles']\n        self.assertEqual(subs['en']['ext'], 'srt')\n\n        result = get_info({'writesubtitles': True, 'subtitleslangs': ['es', 'fr', 'it']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'es', 'fr'})\n\n        result = get_info({'writesubtitles': True, 'subtitleslangs': ['all', '-en']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'es', 'fr'})\n\n        result = get_info({'writesubtitles': True, 'subtitleslangs': ['en', 'fr', '-en']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'fr'})\n\n        result = get_info({'writesubtitles': True, 'subtitleslangs': ['-en', 'en']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'en'})\n\n        result = get_info({'writesubtitles': True, 'subtitleslangs': ['e.+']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'es', 'en'})\n\n        result = get_info({'writesubtitles': True, 'writeautomaticsub': True, 'subtitleslangs': ['es', 'pt']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'es', 'pt'})\n        self.assertFalse(subs['es']['_auto'])\n        self.assertTrue(subs['pt']['_auto'])\n\n        result = get_info({'writeautomaticsub': True, 'subtitleslangs': ['es', 'pt']})\n        subs = result['requested_subtitles']\n        self.assertTrue(subs)\n        self.assertEqual(set(subs.keys()), {'es', 'pt'})\n        self.assertTrue(subs['es']['_auto'])\n        self.assertTrue(subs['pt']['_auto'])\n\n    def test_add_extra_info(self):\n        test_dict = {\n            'extractor': 'Foo',\n        }\n        extra_info = {\n            'extractor': 'Bar',\n            'playlist': 'funny videos',\n        }\n        YDL.add_extra_info(test_dict, extra_info)\n        self.assertEqual(test_dict['extractor'], 'Foo')\n        self.assertEqual(test_dict['playlist'], 'funny videos')\n\n    outtmpl_info = {\n        'id': '1234',\n        'ext': 'mp4',\n        'width': None,\n        'height': 1080,\n        'filesize': 1024,\n        'title1': '$PATH',\n        'title2': '%PATH%',\n        'title3': 'foo/bar\\\\test',\n        'title4': 'foo \"bar\" test',\n        'title5': '\u00e1\u00e9\u00ed \ud835\udc00',\n        'timestamp': 1618488000,\n        'duration': 100000,\n        'playlist_index': 1,\n        'playlist_autonumber': 2,\n        '__last_playlist_index': 100,\n        'n_entries': 10,\n        'formats': [\n            {'id': 'id 1', 'height': 1080, 'width': 1920},\n            {'id': 'id 2', 'height': 720},\n            {'id': 'id 3'}\n        ]\n    }\n\n    def test_prepare_outtmpl_and_filename(self):\n        def test(tmpl, expected, *, info=None, **params):\n            params['outtmpl'] = tmpl\n            ydl = FakeYDL(params)\n            ydl._num_downloads = 1\n            self.assertEqual(ydl.validate_outtmpl(tmpl), None)\n\n            out = ydl.evaluate_outtmpl(tmpl, info or self.outtmpl_info)\n            fname = ydl.prepare_filename(info or self.outtmpl_info)\n\n            if not isinstance(expected, (list, tuple)):\n                expected = (expected, expected)\n            for (name, got), expect in zip((('outtmpl', out), ('filename', fname)), expected):\n                if callable(expect):\n                    self.assertTrue(expect(got), f'Wrong {name} from {tmpl}')\n                elif expect is not None:\n                    self.assertEqual(got, expect, f'Wrong {name} from {tmpl}')\n\n        # Side-effects\n        original_infodict = dict(self.outtmpl_info)\n        test('foo.bar', 'foo.bar')\n        original_infodict['epoch'] = self.outtmpl_info.get('epoch')\n        self.assertTrue(isinstance(original_infodict['epoch'], int))\n        test('%(epoch)d', int_or_none)\n        self.assertEqual(original_infodict, self.outtmpl_info)\n\n        # Auto-generated fields\n        test('%(id)s.%(ext)s', '1234.mp4')\n        test('%(duration_string)s', ('27:46:40', '27-46-40'))\n        test('%(resolution)s', '1080p')\n        test('%(playlist_index|)s', '001')\n        test('%(playlist_index&{}!)s', '1!')\n        test('%(playlist_autonumber)s', '02')\n        test('%(autonumber)s', '00001')\n        test('%(autonumber+2)03d', '005', autonumber_start=3)\n        test('%(autonumber)s', '001', autonumber_size=3)\n\n        # Escaping %\n        test('%', '%')\n        test('%%', '%')\n        test('%%%%', '%%')\n        test('%s', '%s')\n        test('%%%s', '%%s')\n        test('%d', '%d')\n        test('%abc%', '%abc%')\n        test('%%(width)06d.%(ext)s', '%(width)06d.mp4')\n        test('%%%(height)s', '%1080')\n        test('%(width)06d.%(ext)s', 'NA.mp4')\n        test('%(width)06d.%%(ext)s', 'NA.%(ext)s')\n        test('%%(width)06d.%(ext)s', '%(width)06d.mp4')\n\n        # ID sanitization\n        test('%(id)s', '_abcd', info={'id': '_abcd'})\n        test('%(some_id)s', '_abcd', info={'some_id': '_abcd'})\n        test('%(formats.0.id)s', '_abcd', info={'formats': [{'id': '_abcd'}]})\n        test('%(id)s', '-abcd', info={'id': '-abcd'})\n        test('%(id)s', '.abcd', info={'id': '.abcd'})\n        test('%(id)s', 'ab__cd', info={'id': 'ab__cd'})\n        test('%(id)s', ('ab:cd', 'ab\uff1acd'), info={'id': 'ab:cd'})\n        test('%(id.0)s', '-', info={'id': '--'})\n\n        # Invalid templates\n        self.assertTrue(isinstance(YoutubeDL.validate_outtmpl('%(title)'), ValueError))\n        test('%(invalid@tmpl|def)s', 'none', outtmpl_na_placeholder='none')\n        test('%(..)s', 'NA')\n        test('%(formats.{id)s', 'NA')\n\n        # Entire info_dict\n        def expect_same_infodict(out):\n            got_dict = json.loads(out)\n            for info_field, expected in self.outtmpl_info.items():\n                self.assertEqual(got_dict.get(info_field), expected, info_field)\n            return True\n\n        test('%()j', (expect_same_infodict, str))\n\n        # NA placeholder\n        NA_TEST_OUTTMPL = '%(uploader_date)s-%(width)d-%(x|def)s-%(id)s.%(ext)s'\n        test(NA_TEST_OUTTMPL, 'NA-NA-def-1234.mp4')\n        test(NA_TEST_OUTTMPL, 'none-none-def-1234.mp4', outtmpl_na_placeholder='none')\n        test(NA_TEST_OUTTMPL, '--def-1234.mp4', outtmpl_na_placeholder='')\n        test('%(non_existent.0)s', 'NA')\n\n        # String formatting\n        FMT_TEST_OUTTMPL = '%%(height)%s.%%(ext)s'\n        test(FMT_TEST_OUTTMPL % 's', '1080.mp4')\n        test(FMT_TEST_OUTTMPL % 'd', '1080.mp4')\n        test(FMT_TEST_OUTTMPL % '6d', '  1080.mp4')\n        test(FMT_TEST_OUTTMPL % '-6d', '1080  .mp4')\n        test(FMT_TEST_OUTTMPL % '06d', '001080.mp4')\n        test(FMT_TEST_OUTTMPL % ' 06d', ' 01080.mp4')\n        test(FMT_TEST_OUTTMPL % '   06d', ' 01080.mp4')\n        test(FMT_TEST_OUTTMPL % '0 6d', ' 01080.mp4')\n        test(FMT_TEST_OUTTMPL % '0   6d', ' 01080.mp4')\n        test(FMT_TEST_OUTTMPL % '   0   6d', ' 01080.mp4')\n\n        # Type casting\n        test('%(id)d', '1234')\n        test('%(height)c', '1')\n        test('%(ext)c', 'm')\n        test('%(id)d %(id)r', \"1234 '1234'\")\n        test('%(id)r %(height)r', \"'1234' 1080\")\n        test('%(title5)a %(height)a', (R\"'\\xe1\\xe9\\xed \\U0001d400' 1080\", None))\n        test('%(ext)s-%(ext|def)d', 'mp4-def')\n        test('%(width|0)04d', '0')\n        test('a%(width|b)d', 'ab', outtmpl_na_placeholder='none')\n\n        FORMATS = self.outtmpl_info['formats']\n\n        # Custom type casting\n        test('%(formats.:.id)l', 'id 1, id 2, id 3')\n        test('%(formats.:.id)#l', ('id 1\\nid 2\\nid 3', 'id 1 id 2 id 3'))\n        test('%(ext)l', 'mp4')\n        test('%(formats.:.id) 18l', '  id 1, id 2, id 3')\n        test('%(formats)j', (json.dumps(FORMATS), None))\n        test('%(formats)#j', (\n            json.dumps(FORMATS, indent=4),\n            json.dumps(FORMATS, indent=4).replace(':', '\uff1a').replace('\"', \"\uff02\").replace('\\n', ' ')\n        ))\n        test('%(title5).3B', '\u00e1')\n        test('%(title5)U', '\u00e1\u00e9\u00ed \ud835\udc00')\n        test('%(title5)#U', 'a\\u0301e\\u0301i\\u0301 \ud835\udc00')\n        test('%(title5)+U', '\u00e1\u00e9\u00ed A')\n        test('%(title5)+#U', 'a\\u0301e\\u0301i\\u0301 A')\n        test('%(height)D', '1k')\n        test('%(filesize)#D', '1Ki')\n        test('%(height)5.2D', ' 1.08k')\n        test('%(title4)#S', 'foo_bar_test')\n        test('%(title4).10S', ('foo \uff02bar\uff02 ', 'foo \uff02bar\uff02' + ('#' if compat_os_name == 'nt' else ' ')))\n        if compat_os_name == 'nt':\n            test('%(title4)q', ('\"foo \"\"bar\"\" test\"', None))\n            test('%(formats.:.id)#q', ('\"id 1\" \"id 2\" \"id 3\"', None))\n            test('%(formats.0.id)#q', ('\"id 1\"', None))\n        else:\n            test('%(title4)q', ('\\'foo \"bar\" test\\'', '\\'foo \uff02bar\uff02 test\\''))\n            test('%(formats.:.id)#q', \"'id 1' 'id 2' 'id 3'\")\n            test('%(formats.0.id)#q', \"'id 1'\")\n\n        # Internal formatting\n        test('%(timestamp-1000>%H-%M-%S)s', '11-43-20')\n        test('%(title|%)s %(title|%%)s', '% %%')\n        test('%(id+1-height+3)05d', '00158')\n        test('%(width+100)05d', 'NA')\n        test('%(formats.0) 15s', ('% 15s' % FORMATS[0], None))\n        test('%(formats.0)r', (repr(FORMATS[0]), None))\n        test('%(height.0)03d', '001')\n        test('%(-height.0)04d', '-001')\n        test('%(formats.-1.id)s', FORMATS[-1]['id'])\n        test('%(formats.0.id.-1)d', FORMATS[0]['id'][-1])\n        test('%(formats.3)s', 'NA')\n        test('%(formats.:2:-1)r', repr(FORMATS[:2:-1]))\n        test('%(formats.0.id.-1+id)f', '1235.000000')\n        test('%(formats.0.id.-1+formats.1.id.-1)d', '3')\n        out = json.dumps([{'id': f['id'], 'height.:2': str(f['height'])[:2]}\n                          if 'height' in f else {'id': f['id']}\n                          for f in FORMATS])\n        test('%(formats.:.{id,height.:2})j', (out, None))\n        test('%(formats.:.{id,height}.id)l', ', '.join(f['id'] for f in FORMATS))\n        test('%(.{id,title})j', ('{\"id\": \"1234\"}', '{\uff02id\uff02\uff1a \uff021234\uff02}'))\n\n        # Alternates\n        test('%(title,id)s', '1234')\n        test('%(width-100,height+20|def)d', '1100')\n        test('%(width-100,height+width|def)s', 'def')\n        test('%(timestamp-x>%H\\\\,%M\\\\,%S,timestamp>%H\\\\,%M\\\\,%S)s', '12,00,00')\n\n        # Replacement\n        test('%(id&foo)s.bar', 'foo.bar')\n        test('%(title&foo)s.bar', 'NA.bar')\n        test('%(title&foo|baz)s.bar', 'baz.bar')\n        test('%(x,id&foo|baz)s.bar', 'foo.bar')\n        test('%(x,title&foo|baz)s.bar', 'baz.bar')\n        test('%(id&a\\nb|)s', ('a\\nb', 'a b'))\n        test('%(id&hi {:>10} {}|)s', 'hi       1234 1234')\n        test(R'%(id&{0} {}|)s', 'NA')\n        test(R'%(id&{0.1}|)s', 'NA')\n        test('%(height&{:,d})S', '1,080')\n\n        # Laziness\n        def gen():\n            yield from range(5)\n            raise self.assertTrue(False, 'LazyList should not be evaluated till here')\n        test('%(key.4)s', '4', info={'key': LazyList(gen())})\n\n        # Empty filename\n        test('%(foo|)s-%(bar|)s.%(ext)s', '-.mp4')\n        # test('%(foo|)s.%(ext)s', ('.mp4', '_.mp4'))  # fixme\n        # test('%(foo|)s', ('', '_'))  # fixme\n\n        # Environment variable expansion for prepare_filename\n        os.environ['__yt_dlp_var'] = 'expanded'\n        envvar = '%__yt_dlp_var%' if compat_os_name == 'nt' else '$__yt_dlp_var'\n        test(envvar, (envvar, 'expanded'))\n        if compat_os_name == 'nt':\n            test('%s%', ('%s%', '%s%'))\n            os.environ['s'] = 'expanded'\n            test('%s%', ('%s%', 'expanded'))  # %s% should be expanded before escaping %s\n            os.environ['(test)s'] = 'expanded'\n            test('%(test)s%', ('NA%', 'expanded'))  # Environment should take priority over template\n\n        # Path expansion and escaping\n        test('Hello %(title1)s', 'Hello $PATH')\n        test('Hello %(title2)s', 'Hello %PATH%')\n        test('%(title3)s', ('foo/bar\\\\test', 'foo\u29f8bar\u29f9test'))\n        test('folder/%(title3)s', ('folder/foo/bar\\\\test', 'folder%sfoo\u29f8bar\u29f9test' % os.path.sep))\n\n    def test_format_note(self):\n        ydl = YoutubeDL()\n        self.assertEqual(ydl._format_note({}), '')\n        assertRegexpMatches(self, ydl._format_note({\n            'vbr': 10,\n        }), r'^\\s*10k$')\n        assertRegexpMatches(self, ydl._format_note({\n            'fps': 30,\n        }), r'^30fps$')\n\n    def test_postprocessors(self):\n        filename = 'post-processor-testfile.mp4'\n        audiofile = filename + '.mp3'\n\n        class SimplePP(PostProcessor):\n            def run(self, info):\n                with open(audiofile, 'w') as f:\n                    f.write('EXAMPLE')\n                return [info['filepath']], info\n\n        def run_pp(params, PP):\n            with open(filename, 'w') as f:\n                f.write('EXAMPLE')\n            ydl = YoutubeDL(params)\n            ydl.add_post_processor(PP())\n            ydl.post_process(filename, {'filepath': filename})\n\n        run_pp({'keepvideo': True}, SimplePP)\n        self.assertTrue(os.path.exists(filename), '%s doesn\\'t exist' % filename)\n        self.assertTrue(os.path.exists(audiofile), '%s doesn\\'t exist' % audiofile)\n        os.unlink(filename)\n        os.unlink(audiofile)\n\n        run_pp({'keepvideo': False}, SimplePP)\n        self.assertFalse(os.path.exists(filename), '%s exists' % filename)\n        self.assertTrue(os.path.exists(audiofile), '%s doesn\\'t exist' % audiofile)\n        os.unlink(audiofile)\n\n        class ModifierPP(PostProcessor):\n            def run(self, info):\n                with open(info['filepath'], 'w') as f:\n                    f.write('MODIFIED')\n                return [], info\n\n        run_pp({'keepvideo': False}, ModifierPP)\n        self.assertTrue(os.path.exists(filename), '%s doesn\\'t exist' % filename)\n        os.unlink(filename)\n\n    def test_match_filter(self):\n        first = {\n            'id': '1',\n            'url': TEST_URL,\n            'title': 'one',\n            'extractor': 'TEST',\n            'duration': 30,\n            'filesize': 10 * 1024,\n            'playlist_id': '42',\n            'uploader': \"\u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u0442\u0435\u0441\u0442\",\n            'creator': \"\u0442\u0435\u0441\u0442 ' 123 ' \u0442\u0435\u0441\u0442--\",\n            'webpage_url': 'http://example.com/watch?v=shenanigans',\n        }\n        second = {\n            'id': '2',\n            'url': TEST_URL,\n            'title': 'two',\n            'extractor': 'TEST',\n            'duration': 10,\n            'description': 'foo',\n            'filesize': 5 * 1024,\n            'playlist_id': '43',\n            'uploader': \"\u0442\u0435\u0441\u0442 123\",\n            'webpage_url': 'http://example.com/watch?v=SHENANIGANS',\n        }\n        videos = [first, second]\n\n        def get_videos(filter_=None):\n            ydl = YDL({'match_filter': filter_, 'simulate': True})\n            for v in videos:\n                ydl.process_ie_result(v, download=True)\n            return [v['id'] for v in ydl.downloaded_info_dicts]\n\n        res = get_videos()\n        self.assertEqual(res, ['1', '2'])\n\n        def f(v, incomplete):\n            if v['id'] == '1':\n                return None\n            else:\n                return 'Video id is not 1'\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func('duration < 30')\n        res = get_videos(f)\n        self.assertEqual(res, ['2'])\n\n        f = match_filter_func('description = foo')\n        res = get_videos(f)\n        self.assertEqual(res, ['2'])\n\n        f = match_filter_func('description =? foo')\n        res = get_videos(f)\n        self.assertEqual(res, ['1', '2'])\n\n        f = match_filter_func('filesize > 5KiB')\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func('playlist_id = 42')\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func('uploader = \"\u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u0442\u0435\u0441\u0442\"')\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func('uploader != \"\u8b8a\u614b\u598d\u5b57\u5e55\u7248 \u592a\u598d \u0442\u0435\u0441\u0442\"')\n        res = get_videos(f)\n        self.assertEqual(res, ['2'])\n\n        f = match_filter_func('creator = \"\u0442\u0435\u0441\u0442 \\' 123 \\' \u0442\u0435\u0441\u0442--\"')\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func(\"creator = '\u0442\u0435\u0441\u0442 \\\\' 123 \\\\' \u0442\u0435\u0441\u0442--'\")\n        res = get_videos(f)\n        self.assertEqual(res, ['1'])\n\n        f = match_filter_func(r\"creator = '\u0442\u0435\u0441\u0442 \\' 123 \\' \u0442\u0435\u0441\u0442--' & duration > 30\")\n        res = get_videos(f)\n        self.assertEqual(res, [])\n\n    def test_playlist_items_selection(self):\n        INDICES, PAGE_SIZE = list(range(1, 11)), 3\n\n        def entry(i, evaluated):\n            evaluated.append(i)\n            return {\n                'id': str(i),\n                'title': str(i),\n                'url': TEST_URL,\n            }\n\n        def pagedlist_entries(evaluated):\n            def page_func(n):\n                start = PAGE_SIZE * n\n                for i in INDICES[start: start + PAGE_SIZE]:\n                    yield entry(i, evaluated)\n            return OnDemandPagedList(page_func, PAGE_SIZE)\n\n        def page_num(i):\n            return (i + PAGE_SIZE - 1) // PAGE_SIZE\n\n        def generator_entries(evaluated):\n            for i in INDICES:\n                yield entry(i, evaluated)\n\n        def list_entries(evaluated):\n            return list(generator_entries(evaluated))\n\n        def lazylist_entries(evaluated):\n            return LazyList(generator_entries(evaluated))\n\n        def get_downloaded_info_dicts(params, entries):\n            ydl = YDL(params)\n            ydl.process_ie_result({\n                '_type': 'playlist',\n                'id': 'test',\n                'extractor': 'test:playlist',\n                'extractor_key': 'test:playlist',\n                'webpage_url': 'http://example.com',\n                'entries': entries,\n            })\n            return ydl.downloaded_info_dicts\n\n        def test_selection(params, expected_ids, evaluate_all=False):\n            expected_ids = list(expected_ids)\n            if evaluate_all:\n                generator_eval = pagedlist_eval = INDICES\n            elif not expected_ids:\n                generator_eval = pagedlist_eval = []\n            else:\n                generator_eval = INDICES[0: max(expected_ids)]\n                pagedlist_eval = INDICES[PAGE_SIZE * page_num(min(expected_ids)) - PAGE_SIZE:\n                                         PAGE_SIZE * page_num(max(expected_ids))]\n\n            for name, func, expected_eval in (\n                ('list', list_entries, INDICES),\n                ('Generator', generator_entries, generator_eval),\n                # ('LazyList', lazylist_entries, generator_eval),  # Generator and LazyList follow the exact same code path\n                ('PagedList', pagedlist_entries, pagedlist_eval),\n            ):\n                evaluated = []\n                entries = func(evaluated)\n                results = [(v['playlist_autonumber'] - 1, (int(v['id']), v['playlist_index']))\n                           for v in get_downloaded_info_dicts(params, entries)]\n                self.assertEqual(results, list(enumerate(zip(expected_ids, expected_ids))), f'Entries of {name} for {params}')\n                self.assertEqual(sorted(evaluated), expected_eval, f'Evaluation of {name} for {params}')\n\n        test_selection({}, INDICES)\n        test_selection({'playlistend': 20}, INDICES, True)\n        test_selection({'playlistend': 2}, INDICES[:2])\n        test_selection({'playliststart': 11}, [], True)\n        test_selection({'playliststart': 2}, INDICES[1:])\n        test_selection({'playlist_items': '2-4'}, INDICES[1:4])\n        test_selection({'playlist_items': '2,4'}, [2, 4])\n        test_selection({'playlist_items': '20'}, [], True)\n        test_selection({'playlist_items': '0'}, [])\n\n        # Tests for https://github.com/ytdl-org/youtube-dl/issues/10591\n        test_selection({'playlist_items': '2-4,3-4,3'}, [2, 3, 4])\n        test_selection({'playlist_items': '4,2'}, [4, 2])\n\n        # Tests for https://github.com/yt-dlp/yt-dlp/issues/720\n        # https://github.com/yt-dlp/yt-dlp/issues/302\n        test_selection({'playlistreverse': True}, INDICES[::-1])\n        test_selection({'playliststart': 2, 'playlistreverse': True}, INDICES[:0:-1])\n        test_selection({'playlist_items': '2,4', 'playlistreverse': True}, [4, 2])\n        test_selection({'playlist_items': '4,2'}, [4, 2])\n\n        # Tests for --playlist-items start:end:step\n        test_selection({'playlist_items': ':'}, INDICES, True)\n        test_selection({'playlist_items': '::1'}, INDICES, True)\n        test_selection({'playlist_items': '::-1'}, INDICES[::-1], True)\n        test_selection({'playlist_items': ':6'}, INDICES[:6])\n        test_selection({'playlist_items': ':-6'}, INDICES[:-5], True)\n        test_selection({'playlist_items': '-1:6:-2'}, INDICES[:4:-2], True)\n        test_selection({'playlist_items': '9:-6:-2'}, INDICES[8:3:-2], True)\n\n        test_selection({'playlist_items': '1:inf:2'}, INDICES[::2], True)\n        test_selection({'playlist_items': '-2:inf'}, INDICES[-2:], True)\n        test_selection({'playlist_items': ':inf:-1'}, [], True)\n        test_selection({'playlist_items': '0-2:2'}, [2])\n        test_selection({'playlist_items': '1-:2'}, INDICES[::2], True)\n        test_selection({'playlist_items': '0--2:2'}, INDICES[1:-1:2], True)\n\n        test_selection({'playlist_items': '10::3'}, [10], True)\n        test_selection({'playlist_items': '-1::3'}, [10], True)\n        test_selection({'playlist_items': '11::3'}, [], True)\n        test_selection({'playlist_items': '-15::2'}, INDICES[1::2], True)\n        test_selection({'playlist_items': '-15::15'}, [], True)\n\n    def test_do_not_override_ie_key_in_url_transparent(self):\n        ydl = YDL()\n\n        class Foo1IE(InfoExtractor):\n            _VALID_URL = r'foo1:'\n\n            def _real_extract(self, url):\n                return {\n                    '_type': 'url_transparent',\n                    'url': 'foo2:',\n                    'ie_key': 'Foo2',\n                    'title': 'foo1 title',\n                    'id': 'foo1_id',\n                }\n\n        class Foo2IE(InfoExtractor):\n            _VALID_URL = r'foo2:'\n\n            def _real_extract(self, url):\n                return {\n                    '_type': 'url',\n                    'url': 'foo3:',\n                    'ie_key': 'Foo3',\n                }\n\n        class Foo3IE(InfoExtractor):\n            _VALID_URL = r'foo3:'\n\n            def _real_extract(self, url):\n                return _make_result([{'url': TEST_URL}], title='foo3 title')\n\n        ydl.add_info_extractor(Foo1IE(ydl))\n        ydl.add_info_extractor(Foo2IE(ydl))\n        ydl.add_info_extractor(Foo3IE(ydl))\n        ydl.extract_info('foo1:')\n        downloaded = ydl.downloaded_info_dicts[0]\n        self.assertEqual(downloaded['url'], TEST_URL)\n        self.assertEqual(downloaded['title'], 'foo1 title')\n        self.assertEqual(downloaded['id'], 'testid')\n        self.assertEqual(downloaded['extractor'], 'testex')\n        self.assertEqual(downloaded['extractor_key'], 'TestEx')\n\n    # Test case for https://github.com/ytdl-org/youtube-dl/issues/27064\n    def test_ignoreerrors_for_playlist_with_url_transparent_iterable_entries(self):\n\n        class _YDL(YDL):\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n\n            def trouble(self, s, tb=None):\n                pass\n\n        ydl = _YDL({\n            'format': 'extra',\n            'ignoreerrors': True,\n        })\n\n        class VideoIE(InfoExtractor):\n            _VALID_URL = r'video:(?P<id>\\d+)'\n\n            def _real_extract(self, url):\n                video_id = self._match_id(url)\n                formats = [{\n                    'format_id': 'default',\n                    'url': 'url:',\n                }]\n                if video_id == '0':\n                    raise ExtractorError('foo')\n                if video_id == '2':\n                    formats.append({\n                        'format_id': 'extra',\n                        'url': TEST_URL,\n                    })\n                return {\n                    'id': video_id,\n                    'title': 'Video %s' % video_id,\n                    'formats': formats,\n                }\n\n        class PlaylistIE(InfoExtractor):\n            _VALID_URL = r'playlist:'\n\n            def _entries(self):\n                for n in range(3):\n                    video_id = str(n)\n                    yield {\n                        '_type': 'url_transparent',\n                        'ie_key': VideoIE.ie_key(),\n                        'id': video_id,\n                        'url': 'video:%s' % video_id,\n                        'title': 'Video Transparent %s' % video_id,\n                    }\n\n            def _real_extract(self, url):\n                return self.playlist_result(self._entries())\n\n        ydl.add_info_extractor(VideoIE(ydl))\n        ydl.add_info_extractor(PlaylistIE(ydl))\n        info = ydl.extract_info('playlist:')\n        entries = info['entries']\n        self.assertEqual(len(entries), 3)\n        self.assertTrue(entries[0] is None)\n        self.assertTrue(entries[1] is None)\n        self.assertEqual(len(ydl.downloaded_info_dicts), 1)\n        downloaded = ydl.downloaded_info_dicts[0]\n        entries[2].pop('requested_downloads', None)\n        self.assertEqual(entries[2], downloaded)\n        self.assertEqual(downloaded['url'], TEST_URL)\n        self.assertEqual(downloaded['title'], 'Video Transparent 2')\n        self.assertEqual(downloaded['id'], '2')\n        self.assertEqual(downloaded['extractor'], 'Video')\n        self.assertEqual(downloaded['extractor_key'], 'Video')\n\n    def test_header_cookies(self):\n        from http.cookiejar import Cookie\n\n        ydl = FakeYDL()\n        ydl.report_warning = lambda *_, **__: None\n\n        def cookie(name, value, version=None, domain='', path='', secure=False, expires=None):\n            return Cookie(\n                version or 0, name, value, None, False,\n                domain, bool(domain), bool(domain), path, bool(path),\n                secure, expires, False, None, None, rest={})\n\n        _test_url = 'https://yt.dlp/test'\n\n        def test(encoded_cookies, cookies, *, headers=False, round_trip=None, error_re=None):\n            def _test():\n                ydl.cookiejar.clear()\n                ydl._load_cookies(encoded_cookies, autoscope=headers)\n                if headers:\n                    ydl._apply_header_cookies(_test_url)\n                data = {'url': _test_url}\n                ydl._calc_headers(data)\n                self.assertCountEqual(\n                    map(vars, ydl.cookiejar), map(vars, cookies),\n                    'Extracted cookiejar.Cookie is not the same')\n                if not headers:\n                    self.assertEqual(\n                        data.get('cookies'), round_trip or encoded_cookies,\n                        'Cookie is not the same as round trip')\n                ydl.__dict__['_YoutubeDL__header_cookies'] = []\n\n            with self.subTest(msg=encoded_cookies):\n                if not error_re:\n                    _test()\n                    return\n                with self.assertRaisesRegex(Exception, error_re):\n                    _test()\n\n        test('test=value; Domain=.yt.dlp', [cookie('test', 'value', domain='.yt.dlp')])\n        test('test=value', [cookie('test', 'value')], error_re=r'Unscoped cookies are not allowed')\n        test('cookie1=value1; Domain=.yt.dlp; Path=/test; cookie2=value2; Domain=.yt.dlp; Path=/', [\n            cookie('cookie1', 'value1', domain='.yt.dlp', path='/test'),\n            cookie('cookie2', 'value2', domain='.yt.dlp', path='/')])\n        test('test=value; Domain=.yt.dlp; Path=/test; Secure; Expires=9999999999', [\n            cookie('test', 'value', domain='.yt.dlp', path='/test', secure=True, expires=9999999999)])\n        test('test=\"value; \"; path=/test; domain=.yt.dlp', [\n            cookie('test', 'value; ', domain='.yt.dlp', path='/test')],\n            round_trip='test=\"value\\\\073 \"; Domain=.yt.dlp; Path=/test')\n        test('name=; Domain=.yt.dlp', [cookie('name', '', domain='.yt.dlp')],\n             round_trip='name=\"\"; Domain=.yt.dlp')\n\n        test('test=value', [cookie('test', 'value', domain='.yt.dlp')], headers=True)\n        test('cookie1=value; Domain=.yt.dlp; cookie2=value', [], headers=True, error_re=r'Invalid syntax')\n        ydl.deprecated_feature = ydl.report_error\n        test('test=value', [], headers=True, error_re=r'Passing cookies as a header is a potential security risk')\n\n    def test_infojson_cookies(self):\n        TEST_FILE = 'test_infojson_cookies.info.json'\n        TEST_URL = 'https://example.com/example.mp4'\n        COOKIES = 'a=b; Domain=.example.com; c=d; Domain=.example.com'\n        COOKIE_HEADER = {'Cookie': 'a=b; c=d'}\n\n        ydl = FakeYDL()\n        ydl.process_info = lambda x: ydl._write_info_json('test', x, TEST_FILE)\n\n        def make_info(info_header_cookies=False, fmts_header_cookies=False, cookies_field=False):\n            fmt = {'url': TEST_URL}\n            if fmts_header_cookies:\n                fmt['http_headers'] = COOKIE_HEADER\n            if cookies_field:\n                fmt['cookies'] = COOKIES\n            return _make_result([fmt], http_headers=COOKIE_HEADER if info_header_cookies else None)\n\n        def test(initial_info, note):\n            result = {}\n            result['processed'] = ydl.process_ie_result(initial_info)\n            self.assertTrue(ydl.cookiejar.get_cookies_for_url(TEST_URL),\n                            msg=f'No cookies set in cookiejar after initial process when {note}')\n            ydl.cookiejar.clear()\n            with open(TEST_FILE) as infojson:\n                result['loaded'] = ydl.sanitize_info(json.load(infojson), True)\n            result['final'] = ydl.process_ie_result(result['loaded'].copy(), download=False)\n            self.assertTrue(ydl.cookiejar.get_cookies_for_url(TEST_URL),\n                            msg=f'No cookies set in cookiejar after final process when {note}')\n            ydl.cookiejar.clear()\n            for key in ('processed', 'loaded', 'final'):\n                info = result[key]\n                self.assertIsNone(\n                    traverse_obj(info, ((None, ('formats', 0)), 'http_headers', 'Cookie'), casesense=False, get_all=False),\n                    msg=f'Cookie header not removed in {key} result when {note}')\n                self.assertEqual(\n                    traverse_obj(info, ((None, ('formats', 0)), 'cookies'), get_all=False), COOKIES,\n                    msg=f'No cookies field found in {key} result when {note}')\n\n        test({'url': TEST_URL, 'http_headers': COOKIE_HEADER, 'id': '1', 'title': 'x'}, 'no formats field')\n        test(make_info(info_header_cookies=True), 'info_dict header cokies')\n        test(make_info(fmts_header_cookies=True), 'format header cookies')\n        test(make_info(info_header_cookies=True, fmts_header_cookies=True), 'info_dict and format header cookies')\n        test(make_info(info_header_cookies=True, fmts_header_cookies=True, cookies_field=True), 'all cookies fields')\n        test(make_info(cookies_field=True), 'cookies format field')\n        test({'url': TEST_URL, 'cookies': COOKIES, 'id': '1', 'title': 'x'}, 'info_dict cookies field only')\n\n        try_rm(TEST_FILE)\n\n    def test_add_headers_cookie(self):\n        def check_for_cookie_header(result):\n            return traverse_obj(result, ((None, ('formats', 0)), 'http_headers', 'Cookie'), casesense=False, get_all=False)\n\n        ydl = FakeYDL({'http_headers': {'Cookie': 'a=b'}})\n        ydl._apply_header_cookies(_make_result([])['webpage_url'])  # Scope to input webpage URL: .example.com\n\n        fmt = {'url': 'https://example.com/video.mp4'}\n        result = ydl.process_ie_result(_make_result([fmt]), download=False)\n        self.assertIsNone(check_for_cookie_header(result), msg='http_headers cookies in result info_dict')\n        self.assertEqual(result.get('cookies'), 'a=b; Domain=.example.com', msg='No cookies were set in cookies field')\n        self.assertIn('a=b', ydl.cookiejar.get_cookie_header(fmt['url']), msg='No cookies were set in cookiejar')\n\n        fmt = {'url': 'https://wrong.com/video.mp4'}\n        result = ydl.process_ie_result(_make_result([fmt]), download=False)\n        self.assertIsNone(check_for_cookie_header(result), msg='http_headers cookies for wrong domain')\n        self.assertFalse(result.get('cookies'), msg='Cookies set in cookies field for wrong domain')\n        self.assertFalse(ydl.cookiejar.get_cookie_header(fmt['url']), msg='Cookies set in cookiejar for wrong domain')\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "#!/usr/bin/env python3\n\n# Allow direct execution\nimport os\nimport re\nimport sys\nimport unittest\nimport warnings\n\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n\nimport contextlib\nimport io\nimport itertools\nimport json\nimport subprocess\nimport xml.etree.ElementTree\n\nfrom yt_dlp.compat import (\n    compat_etree_fromstring,\n    compat_HTMLParseError,\n    compat_os_name,\n)\nfrom yt_dlp.utils import (\n    Config,\n    DateRange,\n    ExtractorError,\n    InAdvancePagedList,\n    LazyList,\n    OnDemandPagedList,\n    Popen,\n    age_restricted,\n    args_to_str,\n    base_url,\n    caesar,\n    clean_html,\n    clean_podcast_url,\n    cli_bool_option,\n    cli_option,\n    cli_valueless_option,\n    date_from_str,\n    datetime_from_str,\n    detect_exe_version,\n    determine_ext,\n    determine_file_encoding,\n    dfxp2srt,\n    dict_get,\n    encode_base_n,\n    encode_compat_str,\n    encodeFilename,\n    expand_path,\n    extract_attributes,\n    extract_basic_auth,\n    find_xpath_attr,\n    fix_xml_ampersands,\n    float_or_none,\n    format_bytes,\n    get_compatible_ext,\n    get_element_by_attribute,\n    get_element_by_class,\n    get_element_html_by_attribute,\n    get_element_html_by_class,\n    get_element_text_and_html_by_tag,\n    get_elements_by_attribute,\n    get_elements_by_class,\n    get_elements_html_by_attribute,\n    get_elements_html_by_class,\n    get_elements_text_and_html_by_attribute,\n    int_or_none,\n    intlist_to_bytes,\n    iri_to_uri,\n    is_html,\n    js_to_json,\n    limit_length,\n    locked_file,\n    lowercase_escape,\n    match_str,\n    merge_dicts,\n    mimetype2ext,\n    month_by_name,\n    multipart_encode,\n    ohdave_rsa_encrypt,\n    orderedSet,\n    parse_age_limit,\n    parse_bitrate,\n    parse_codecs,\n    parse_count,\n    parse_dfxp_time_expr,\n    parse_duration,\n    parse_filesize,\n    parse_iso8601,\n    parse_qs,\n    parse_resolution,\n    pkcs1pad,\n    prepend_extension,\n    read_batch_urls,\n    remove_end,\n    remove_quotes,\n    remove_start,\n    render_table,\n    replace_extension,\n    rot47,\n    sanitize_filename,\n    sanitize_path,\n    sanitize_url,\n    shell_quote,\n    smuggle_url,\n    str_or_none,\n    str_to_int,\n    strip_jsonp,\n    strip_or_none,\n    subtitles_filename,\n    timeconvert,\n    traverse_obj,\n    try_call,\n    unescapeHTML,\n    unified_strdate,\n    unified_timestamp,\n    unsmuggle_url,\n    update_url_query,\n    uppercase_escape,\n    url_basename,\n    url_or_none,\n    urlencode_postdata,\n    urljoin,\n    urshift,\n    variadic,\n    version_tuple,\n    xpath_attr,\n    xpath_element,\n    xpath_text,\n    xpath_with_ns,\n)\nfrom yt_dlp.utils.networking import (\n    HTTPHeaderDict,\n    escape_rfc3986,\n    normalize_url,\n    remove_dot_segments,\n)\n\n\nclass TestUtil(unittest.TestCase):\n    def test_timeconvert(self):\n        self.assertTrue(timeconvert('') is None)\n        self.assertTrue(timeconvert('bougrg') is None)\n\n    def test_sanitize_filename(self):\n        self.assertEqual(sanitize_filename(''), '')\n        self.assertEqual(sanitize_filename('abc'), 'abc')\n        self.assertEqual(sanitize_filename('abc_d-e'), 'abc_d-e')\n\n        self.assertEqual(sanitize_filename('123'), '123')\n\n        self.assertEqual('abc\u29f8de', sanitize_filename('abc/de'))\n        self.assertFalse('/' in sanitize_filename('abc/de///'))\n\n        self.assertEqual('abc_de', sanitize_filename('abc/<>\\\\*|de', is_id=False))\n        self.assertEqual('xxx', sanitize_filename('xxx/<>\\\\*|', is_id=False))\n        self.assertEqual('yes no', sanitize_filename('yes? no', is_id=False))\n        self.assertEqual('this - that', sanitize_filename('this: that', is_id=False))\n\n        self.assertEqual(sanitize_filename('AT&T'), 'AT&T')\n        aumlaut = '\u00e4'\n        self.assertEqual(sanitize_filename(aumlaut), aumlaut)\n        tests = '\\u043a\\u0438\\u0440\\u0438\\u043b\\u043b\\u0438\\u0446\\u0430'\n        self.assertEqual(sanitize_filename(tests), tests)\n\n        self.assertEqual(\n            sanitize_filename('New World record at 0:12:34'),\n            'New World record at 0_12_34')\n\n        self.assertEqual(sanitize_filename('--gasdgf'), '--gasdgf')\n        self.assertEqual(sanitize_filename('--gasdgf', is_id=True), '--gasdgf')\n        self.assertEqual(sanitize_filename('--gasdgf', is_id=False), '_-gasdgf')\n        self.assertEqual(sanitize_filename('.gasdgf'), '.gasdgf')\n        self.assertEqual(sanitize_filename('.gasdgf', is_id=True), '.gasdgf')\n        self.assertEqual(sanitize_filename('.gasdgf', is_id=False), 'gasdgf')\n\n        forbidden = '\"\\0\\\\/'\n        for fc in forbidden:\n            for fbc in forbidden:\n                self.assertTrue(fbc not in sanitize_filename(fc))\n\n    def test_sanitize_filename_restricted(self):\n        self.assertEqual(sanitize_filename('abc', restricted=True), 'abc')\n        self.assertEqual(sanitize_filename('abc_d-e', restricted=True), 'abc_d-e')\n\n        self.assertEqual(sanitize_filename('123', restricted=True), '123')\n\n        self.assertEqual('abc_de', sanitize_filename('abc/de', restricted=True))\n        self.assertFalse('/' in sanitize_filename('abc/de///', restricted=True))\n\n        self.assertEqual('abc_de', sanitize_filename('abc/<>\\\\*|de', restricted=True))\n        self.assertEqual('xxx', sanitize_filename('xxx/<>\\\\*|', restricted=True))\n        self.assertEqual('yes_no', sanitize_filename('yes? no', restricted=True))\n        self.assertEqual('this_-_that', sanitize_filename('this: that', restricted=True))\n\n        tests = 'a\u00e4b\\u4e2d\\u56fd\\u7684c'\n        self.assertEqual(sanitize_filename(tests, restricted=True), 'aab_c')\n        self.assertTrue(sanitize_filename('\\xf6', restricted=True) != '')  # No empty filename\n\n        forbidden = '\"\\0\\\\/&!: \\'\\t\\n()[]{}$;`^,#'\n        for fc in forbidden:\n            for fbc in forbidden:\n                self.assertTrue(fbc not in sanitize_filename(fc, restricted=True))\n\n        # Handle a common case more neatly\n        self.assertEqual(sanitize_filename('\\u5927\\u58f0\\u5e26 - Song', restricted=True), 'Song')\n        self.assertEqual(sanitize_filename('\\u603b\\u7edf: Speech', restricted=True), 'Speech')\n        # .. but make sure the file name is never empty\n        self.assertTrue(sanitize_filename('-', restricted=True) != '')\n        self.assertTrue(sanitize_filename(':', restricted=True) != '')\n\n        self.assertEqual(sanitize_filename(\n            '\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff', restricted=True),\n            'AAAAAAAECEEEEIIIIDNOOOOOOOOEUUUUUYTHssaaaaaaaeceeeeiiiionooooooooeuuuuuythy')\n\n    def test_sanitize_ids(self):\n        self.assertEqual(sanitize_filename('_n_cd26wFpw', is_id=True), '_n_cd26wFpw')\n        self.assertEqual(sanitize_filename('_BD_eEpuzXw', is_id=True), '_BD_eEpuzXw')\n        self.assertEqual(sanitize_filename('N0Y__7-UOdI', is_id=True), 'N0Y__7-UOdI')\n\n    def test_sanitize_path(self):\n        if sys.platform != 'win32':\n            return\n\n        self.assertEqual(sanitize_path('abc'), 'abc')\n        self.assertEqual(sanitize_path('abc/def'), 'abc\\\\def')\n        self.assertEqual(sanitize_path('abc\\\\def'), 'abc\\\\def')\n        self.assertEqual(sanitize_path('abc|def'), 'abc#def')\n        self.assertEqual(sanitize_path('<>:\"|?*'), '#######')\n        self.assertEqual(sanitize_path('C:/abc/def'), 'C:\\\\abc\\\\def')\n        self.assertEqual(sanitize_path('C?:/abc/def'), 'C##\\\\abc\\\\def')\n\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\UNC\\\\ComputerName\\\\abc'), '\\\\\\\\?\\\\UNC\\\\ComputerName\\\\abc')\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\UNC/ComputerName/abc'), '\\\\\\\\?\\\\UNC\\\\ComputerName\\\\abc')\n\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\C:\\\\abc'), '\\\\\\\\?\\\\C:\\\\abc')\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\C:/abc'), '\\\\\\\\?\\\\C:\\\\abc')\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\C:\\\\ab?c\\\\de:f'), '\\\\\\\\?\\\\C:\\\\ab#c\\\\de#f')\n        self.assertEqual(sanitize_path('\\\\\\\\?\\\\C:\\\\abc'), '\\\\\\\\?\\\\C:\\\\abc')\n\n        self.assertEqual(\n            sanitize_path('youtube/%(uploader)s/%(autonumber)s-%(title)s-%(upload_date)s.%(ext)s'),\n            'youtube\\\\%(uploader)s\\\\%(autonumber)s-%(title)s-%(upload_date)s.%(ext)s')\n\n        self.assertEqual(\n            sanitize_path('youtube/TheWreckingYard ./00001-Not bad, Especially for Free! (1987 Yamaha 700)-20141116.mp4.part'),\n            'youtube\\\\TheWreckingYard #\\\\00001-Not bad, Especially for Free! (1987 Yamaha 700)-20141116.mp4.part')\n        self.assertEqual(sanitize_path('abc/def...'), 'abc\\\\def..#')\n        self.assertEqual(sanitize_path('abc.../def'), 'abc..#\\\\def')\n        self.assertEqual(sanitize_path('abc.../def...'), 'abc..#\\\\def..#')\n\n        self.assertEqual(sanitize_path('../abc'), '..\\\\abc')\n        self.assertEqual(sanitize_path('../../abc'), '..\\\\..\\\\abc')\n        self.assertEqual(sanitize_path('./abc'), 'abc')\n        self.assertEqual(sanitize_path('./../abc'), '..\\\\abc')\n\n    def test_sanitize_url(self):\n        self.assertEqual(sanitize_url('//foo.bar'), 'http://foo.bar')\n        self.assertEqual(sanitize_url('httpss://foo.bar'), 'https://foo.bar')\n        self.assertEqual(sanitize_url('rmtps://foo.bar'), 'rtmps://foo.bar')\n        self.assertEqual(sanitize_url('https://foo.bar'), 'https://foo.bar')\n        self.assertEqual(sanitize_url('foo bar'), 'foo bar')\n\n    def test_expand_path(self):\n        def env(var):\n            return f'%{var}%' if sys.platform == 'win32' else f'${var}'\n\n        os.environ['yt_dlp_EXPATH_PATH'] = 'expanded'\n        self.assertEqual(expand_path(env('yt_dlp_EXPATH_PATH')), 'expanded')\n\n        old_home = os.environ.get('HOME')\n        test_str = R'C:\\Documents and Settings\\\u0442\u0435\u0441\u0442\\Application Data'\n        try:\n            os.environ['HOME'] = test_str\n            self.assertEqual(expand_path(env('HOME')), os.getenv('HOME'))\n            self.assertEqual(expand_path('~'), os.getenv('HOME'))\n            self.assertEqual(\n                expand_path('~/%s' % env('yt_dlp_EXPATH_PATH')),\n                '%s/expanded' % os.getenv('HOME'))\n        finally:\n            os.environ['HOME'] = old_home or ''\n\n    def test_prepend_extension(self):\n        self.assertEqual(prepend_extension('abc.ext', 'temp'), 'abc.temp.ext')\n        self.assertEqual(prepend_extension('abc.ext', 'temp', 'ext'), 'abc.temp.ext')\n        self.assertEqual(prepend_extension('abc.unexpected_ext', 'temp', 'ext'), 'abc.unexpected_ext.temp')\n        self.assertEqual(prepend_extension('abc', 'temp'), 'abc.temp')\n        self.assertEqual(prepend_extension('.abc', 'temp'), '.abc.temp')\n        self.assertEqual(prepend_extension('.abc.ext', 'temp'), '.abc.temp.ext')\n\n    def test_replace_extension(self):\n        self.assertEqual(replace_extension('abc.ext', 'temp'), 'abc.temp')\n        self.assertEqual(replace_extension('abc.ext', 'temp', 'ext'), 'abc.temp')\n        self.assertEqual(replace_extension('abc.unexpected_ext', 'temp', 'ext'), 'abc.unexpected_ext.temp')\n        self.assertEqual(replace_extension('abc', 'temp'), 'abc.temp')\n        self.assertEqual(replace_extension('.abc', 'temp'), '.abc.temp')\n        self.assertEqual(replace_extension('.abc.ext', 'temp'), '.abc.temp')\n\n    def test_subtitles_filename(self):\n        self.assertEqual(subtitles_filename('abc.ext', 'en', 'vtt'), 'abc.en.vtt')\n        self.assertEqual(subtitles_filename('abc.ext', 'en', 'vtt', 'ext'), 'abc.en.vtt')\n        self.assertEqual(subtitles_filename('abc.unexpected_ext', 'en', 'vtt', 'ext'), 'abc.unexpected_ext.en.vtt')\n\n    def test_remove_start(self):\n        self.assertEqual(remove_start(None, 'A - '), None)\n        self.assertEqual(remove_start('A - B', 'A - '), 'B')\n        self.assertEqual(remove_start('B - A', 'A - '), 'B - A')\n\n    def test_remove_end(self):\n        self.assertEqual(remove_end(None, ' - B'), None)\n        self.assertEqual(remove_end('A - B', ' - B'), 'A')\n        self.assertEqual(remove_end('B - A', ' - B'), 'B - A')\n\n    def test_remove_quotes(self):\n        self.assertEqual(remove_quotes(None), None)\n        self.assertEqual(remove_quotes('\"'), '\"')\n        self.assertEqual(remove_quotes(\"'\"), \"'\")\n        self.assertEqual(remove_quotes(';'), ';')\n        self.assertEqual(remove_quotes('\";'), '\";')\n        self.assertEqual(remove_quotes('\"\"'), '')\n        self.assertEqual(remove_quotes('\";\"'), ';')\n\n    def test_ordered_set(self):\n        self.assertEqual(orderedSet([1, 1, 2, 3, 4, 4, 5, 6, 7, 3, 5]), [1, 2, 3, 4, 5, 6, 7])\n        self.assertEqual(orderedSet([]), [])\n        self.assertEqual(orderedSet([1]), [1])\n        # keep the list ordered\n        self.assertEqual(orderedSet([135, 1, 1, 1]), [135, 1])\n\n    def test_unescape_html(self):\n        self.assertEqual(unescapeHTML('%20;'), '%20;')\n        self.assertEqual(unescapeHTML('&#x2F;'), '/')\n        self.assertEqual(unescapeHTML('&#47;'), '/')\n        self.assertEqual(unescapeHTML('&eacute;'), '\u00e9')\n        self.assertEqual(unescapeHTML('&#2013266066;'), '&#2013266066;')\n        self.assertEqual(unescapeHTML('&a&quot;'), '&a\"')\n        # HTML5 entities\n        self.assertEqual(unescapeHTML('&period;&apos;'), '.\\'')\n\n    def test_date_from_str(self):\n        self.assertEqual(date_from_str('yesterday'), date_from_str('now-1day'))\n        self.assertEqual(date_from_str('now+7day'), date_from_str('now+1week'))\n        self.assertEqual(date_from_str('now+14day'), date_from_str('now+2week'))\n        self.assertEqual(date_from_str('20200229+365day'), date_from_str('20200229+1year'))\n        self.assertEqual(date_from_str('20210131+28day'), date_from_str('20210131+1month'))\n\n    def test_datetime_from_str(self):\n        self.assertEqual(datetime_from_str('yesterday', precision='day'), datetime_from_str('now-1day', precision='auto'))\n        self.assertEqual(datetime_from_str('now+7day', precision='day'), datetime_from_str('now+1week', precision='auto'))\n        self.assertEqual(datetime_from_str('now+14day', precision='day'), datetime_from_str('now+2week', precision='auto'))\n        self.assertEqual(datetime_from_str('20200229+365day', precision='day'), datetime_from_str('20200229+1year', precision='auto'))\n        self.assertEqual(datetime_from_str('20210131+28day', precision='day'), datetime_from_str('20210131+1month', precision='auto'))\n        self.assertEqual(datetime_from_str('20210131+59day', precision='day'), datetime_from_str('20210131+2month', precision='auto'))\n        self.assertEqual(datetime_from_str('now+1day', precision='hour'), datetime_from_str('now+24hours', precision='auto'))\n        self.assertEqual(datetime_from_str('now+23hours', precision='hour'), datetime_from_str('now+23hours', precision='auto'))\n\n    def test_daterange(self):\n        _20century = DateRange(\"19000101\", \"20000101\")\n        self.assertFalse(\"17890714\" in _20century)\n        _ac = DateRange(\"00010101\")\n        self.assertTrue(\"19690721\" in _ac)\n        _firstmilenium = DateRange(end=\"10000101\")\n        self.assertTrue(\"07110427\" in _firstmilenium)\n\n    def test_unified_dates(self):\n        self.assertEqual(unified_strdate('December 21, 2010'), '20101221')\n        self.assertEqual(unified_strdate('8/7/2009'), '20090708')\n        self.assertEqual(unified_strdate('Dec 14, 2012'), '20121214')\n        self.assertEqual(unified_strdate('2012/10/11 01:56:38 +0000'), '20121011')\n        self.assertEqual(unified_strdate('1968 12 10'), '19681210')\n        self.assertEqual(unified_strdate('1968-12-10'), '19681210')\n        self.assertEqual(unified_strdate('31-07-2022 20:00'), '20220731')\n        self.assertEqual(unified_strdate('28/01/2014 21:00:00 +0100'), '20140128')\n        self.assertEqual(\n            unified_strdate('11/26/2014 11:30:00 AM PST', day_first=False),\n            '20141126')\n        self.assertEqual(\n            unified_strdate('2/2/2015 6:47:40 PM', day_first=False),\n            '20150202')\n        self.assertEqual(unified_strdate('Feb 14th 2016 5:45PM'), '20160214')\n        self.assertEqual(unified_strdate('25-09-2014'), '20140925')\n        self.assertEqual(unified_strdate('27.02.2016 17:30'), '20160227')\n        self.assertEqual(unified_strdate('UNKNOWN DATE FORMAT'), None)\n        self.assertEqual(unified_strdate('Feb 7, 2016 at 6:35 pm'), '20160207')\n        self.assertEqual(unified_strdate('July 15th, 2013'), '20130715')\n        self.assertEqual(unified_strdate('September 1st, 2013'), '20130901')\n        self.assertEqual(unified_strdate('Sep 2nd, 2013'), '20130902')\n        self.assertEqual(unified_strdate('November 3rd, 2019'), '20191103')\n        self.assertEqual(unified_strdate('October 23rd, 2005'), '20051023')\n\n    def test_unified_timestamps(self):\n        self.assertEqual(unified_timestamp('December 21, 2010'), 1292889600)\n        self.assertEqual(unified_timestamp('8/7/2009'), 1247011200)\n        self.assertEqual(unified_timestamp('Dec 14, 2012'), 1355443200)\n        self.assertEqual(unified_timestamp('2012/10/11 01:56:38 +0000'), 1349920598)\n        self.assertEqual(unified_timestamp('1968 12 10'), -33436800)\n        self.assertEqual(unified_timestamp('1968-12-10'), -33436800)\n        self.assertEqual(unified_timestamp('28/01/2014 21:00:00 +0100'), 1390939200)\n        self.assertEqual(\n            unified_timestamp('11/26/2014 11:30:00 AM PST', day_first=False),\n            1417001400)\n        self.assertEqual(\n            unified_timestamp('2/2/2015 6:47:40 PM', day_first=False),\n            1422902860)\n        self.assertEqual(unified_timestamp('Feb 14th 2016 5:45PM'), 1455471900)\n        self.assertEqual(unified_timestamp('25-09-2014'), 1411603200)\n        self.assertEqual(unified_timestamp('27.02.2016 17:30'), 1456594200)\n        self.assertEqual(unified_timestamp('UNKNOWN DATE FORMAT'), None)\n        self.assertEqual(unified_timestamp('May 16, 2016 11:15 PM'), 1463440500)\n        self.assertEqual(unified_timestamp('Feb 7, 2016 at 6:35 pm'), 1454870100)\n        self.assertEqual(unified_timestamp('2017-03-30T17:52:41Q'), 1490896361)\n        self.assertEqual(unified_timestamp('Sep 11, 2013 | 5:49 AM'), 1378878540)\n        self.assertEqual(unified_timestamp('December 15, 2017 at 7:49 am'), 1513324140)\n        self.assertEqual(unified_timestamp('2018-03-14T08:32:43.1493874+00:00'), 1521016363)\n\n        self.assertEqual(unified_timestamp('December 31 1969 20:00:01 EDT'), 1)\n        self.assertEqual(unified_timestamp('Wednesday 31 December 1969 18:01:26 MDT'), 86)\n        self.assertEqual(unified_timestamp('12/31/1969 20:01:18 EDT', False), 78)\n\n    def test_determine_ext(self):\n        self.assertEqual(determine_ext('http://example.com/foo/bar.mp4/?download'), 'mp4')\n        self.assertEqual(determine_ext('http://example.com/foo/bar/?download', None), None)\n        self.assertEqual(determine_ext('http://example.com/foo/bar.nonext/?download', None), None)\n        self.assertEqual(determine_ext('http://example.com/foo/bar/mp4?download', None), None)\n        self.assertEqual(determine_ext('http://example.com/foo/bar.m3u8//?download'), 'm3u8')\n        self.assertEqual(determine_ext('foobar', None), None)\n\n    def test_find_xpath_attr(self):\n        testxml = '''<root>\n            <node/>\n            <node x=\"a\"/>\n            <node x=\"a\" y=\"c\" />\n            <node x=\"b\" y=\"d\" />\n            <node x=\"\" />\n        </root>'''\n        doc = compat_etree_fromstring(testxml)\n\n        self.assertEqual(find_xpath_attr(doc, './/fourohfour', 'n'), None)\n        self.assertEqual(find_xpath_attr(doc, './/fourohfour', 'n', 'v'), None)\n        self.assertEqual(find_xpath_attr(doc, './/node', 'n'), None)\n        self.assertEqual(find_xpath_attr(doc, './/node', 'n', 'v'), None)\n        self.assertEqual(find_xpath_attr(doc, './/node', 'x'), doc[1])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'x', 'a'), doc[1])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'x', 'b'), doc[3])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'y'), doc[2])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'y', 'c'), doc[2])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'y', 'd'), doc[3])\n        self.assertEqual(find_xpath_attr(doc, './/node', 'x', ''), doc[4])\n\n    def test_xpath_with_ns(self):\n        testxml = '''<root xmlns:media=\"http://example.com/\">\n            <media:song>\n                <media:author>The Author</media:author>\n                <url>http://server.com/download.mp3</url>\n            </media:song>\n        </root>'''\n        doc = compat_etree_fromstring(testxml)\n        find = lambda p: doc.find(xpath_with_ns(p, {'media': 'http://example.com/'}))\n        self.assertTrue(find('media:song') is not None)\n        self.assertEqual(find('media:song/media:author').text, 'The Author')\n        self.assertEqual(find('media:song/url').text, 'http://server.com/download.mp3')\n\n    def test_xpath_element(self):\n        doc = xml.etree.ElementTree.Element('root')\n        div = xml.etree.ElementTree.SubElement(doc, 'div')\n        p = xml.etree.ElementTree.SubElement(div, 'p')\n        p.text = 'Foo'\n        self.assertEqual(xpath_element(doc, 'div/p'), p)\n        self.assertEqual(xpath_element(doc, ['div/p']), p)\n        self.assertEqual(xpath_element(doc, ['div/bar', 'div/p']), p)\n        self.assertEqual(xpath_element(doc, 'div/bar', default='default'), 'default')\n        self.assertEqual(xpath_element(doc, ['div/bar'], default='default'), 'default')\n        self.assertTrue(xpath_element(doc, 'div/bar') is None)\n        self.assertTrue(xpath_element(doc, ['div/bar']) is None)\n        self.assertTrue(xpath_element(doc, ['div/bar'], 'div/baz') is None)\n        self.assertRaises(ExtractorError, xpath_element, doc, 'div/bar', fatal=True)\n        self.assertRaises(ExtractorError, xpath_element, doc, ['div/bar'], fatal=True)\n        self.assertRaises(ExtractorError, xpath_element, doc, ['div/bar', 'div/baz'], fatal=True)\n\n    def test_xpath_text(self):\n        testxml = '''<root>\n            <div>\n                <p>Foo</p>\n            </div>\n        </root>'''\n        doc = compat_etree_fromstring(testxml)\n        self.assertEqual(xpath_text(doc, 'div/p'), 'Foo')\n        self.assertEqual(xpath_text(doc, 'div/bar', default='default'), 'default')\n        self.assertTrue(xpath_text(doc, 'div/bar') is None)\n        self.assertRaises(ExtractorError, xpath_text, doc, 'div/bar', fatal=True)\n\n    def test_xpath_attr(self):\n        testxml = '''<root>\n            <div>\n                <p x=\"a\">Foo</p>\n            </div>\n        </root>'''\n        doc = compat_etree_fromstring(testxml)\n        self.assertEqual(xpath_attr(doc, 'div/p', 'x'), 'a')\n        self.assertEqual(xpath_attr(doc, 'div/bar', 'x'), None)\n        self.assertEqual(xpath_attr(doc, 'div/p', 'y'), None)\n        self.assertEqual(xpath_attr(doc, 'div/bar', 'x', default='default'), 'default')\n        self.assertEqual(xpath_attr(doc, 'div/p', 'y', default='default'), 'default')\n        self.assertRaises(ExtractorError, xpath_attr, doc, 'div/bar', 'x', fatal=True)\n        self.assertRaises(ExtractorError, xpath_attr, doc, 'div/p', 'y', fatal=True)\n\n    def test_smuggle_url(self):\n        data = {\"\u00f6\": \"\u00f6\", \"abc\": [3]}\n        url = 'https://foo.bar/baz?x=y#a'\n        smug_url = smuggle_url(url, data)\n        unsmug_url, unsmug_data = unsmuggle_url(smug_url)\n        self.assertEqual(url, unsmug_url)\n        self.assertEqual(data, unsmug_data)\n\n        res_url, res_data = unsmuggle_url(url)\n        self.assertEqual(res_url, url)\n        self.assertEqual(res_data, None)\n\n        smug_url = smuggle_url(url, {'a': 'b'})\n        smug_smug_url = smuggle_url(smug_url, {'c': 'd'})\n        res_url, res_data = unsmuggle_url(smug_smug_url)\n        self.assertEqual(res_url, url)\n        self.assertEqual(res_data, {'a': 'b', 'c': 'd'})\n\n    def test_shell_quote(self):\n        args = ['ffmpeg', '-i', encodeFilename('\u00f1\u20ac\u00df\\'.mp4')]\n        self.assertEqual(\n            shell_quote(args),\n            \"\"\"ffmpeg -i '\u00f1\u20ac\u00df'\"'\"'.mp4'\"\"\" if compat_os_name != 'nt' else '''ffmpeg -i \"\u00f1\u20ac\u00df'.mp4\"''')\n\n    def test_float_or_none(self):\n        self.assertEqual(float_or_none('42.42'), 42.42)\n        self.assertEqual(float_or_none('42'), 42.0)\n        self.assertEqual(float_or_none(''), None)\n        self.assertEqual(float_or_none(None), None)\n        self.assertEqual(float_or_none([]), None)\n        self.assertEqual(float_or_none(set()), None)\n\n    def test_int_or_none(self):\n        self.assertEqual(int_or_none('42'), 42)\n        self.assertEqual(int_or_none(''), None)\n        self.assertEqual(int_or_none(None), None)\n        self.assertEqual(int_or_none([]), None)\n        self.assertEqual(int_or_none(set()), None)\n\n    def test_str_to_int(self):\n        self.assertEqual(str_to_int('123,456'), 123456)\n        self.assertEqual(str_to_int('123.456'), 123456)\n        self.assertEqual(str_to_int(523), 523)\n        self.assertEqual(str_to_int('noninteger'), None)\n        self.assertEqual(str_to_int([]), None)\n\n    def test_url_basename(self):\n        self.assertEqual(url_basename('http://foo.de/'), '')\n        self.assertEqual(url_basename('http://foo.de/bar/baz'), 'baz')\n        self.assertEqual(url_basename('http://foo.de/bar/baz?x=y'), 'baz')\n        self.assertEqual(url_basename('http://foo.de/bar/baz#x=y'), 'baz')\n        self.assertEqual(url_basename('http://foo.de/bar/baz/'), 'baz')\n        self.assertEqual(\n            url_basename('http://media.w3.org/2010/05/sintel/trailer.mp4'),\n            'trailer.mp4')\n\n    def test_base_url(self):\n        self.assertEqual(base_url('http://foo.de/'), 'http://foo.de/')\n        self.assertEqual(base_url('http://foo.de/bar'), 'http://foo.de/')\n        self.assertEqual(base_url('http://foo.de/bar/'), 'http://foo.de/bar/')\n        self.assertEqual(base_url('http://foo.de/bar/baz'), 'http://foo.de/bar/')\n        self.assertEqual(base_url('http://foo.de/bar/baz?x=z/x/c'), 'http://foo.de/bar/')\n        self.assertEqual(base_url('http://foo.de/bar/baz&x=z&w=y/x/c'), 'http://foo.de/bar/baz&x=z&w=y/x/')\n\n    def test_urljoin(self):\n        self.assertEqual(urljoin('http://foo.de/', '/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin(b'http://foo.de/', '/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de/', b'/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin(b'http://foo.de/', b'/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('//foo.de/', '/a/b/c.txt'), '//foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de/', 'a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de', '/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de', 'a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de/', 'http://foo.de/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de/', '//foo.de/a/b/c.txt'), '//foo.de/a/b/c.txt')\n        self.assertEqual(urljoin(None, 'http://foo.de/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin(None, '//foo.de/a/b/c.txt'), '//foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('', 'http://foo.de/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin(['foobar'], 'http://foo.de/a/b/c.txt'), 'http://foo.de/a/b/c.txt')\n        self.assertEqual(urljoin('http://foo.de/', None), None)\n        self.assertEqual(urljoin('http://foo.de/', ''), None)\n        self.assertEqual(urljoin('http://foo.de/', ['foobar']), None)\n        self.assertEqual(urljoin('http://foo.de/a/b/c.txt', '.././../d.txt'), 'http://foo.de/d.txt')\n        self.assertEqual(urljoin('http://foo.de/a/b/c.txt', 'rtmp://foo.de'), 'rtmp://foo.de')\n        self.assertEqual(urljoin(None, 'rtmp://foo.de'), 'rtmp://foo.de')\n\n    def test_url_or_none(self):\n        self.assertEqual(url_or_none(None), None)\n        self.assertEqual(url_or_none(''), None)\n        self.assertEqual(url_or_none('foo'), None)\n        self.assertEqual(url_or_none('http://foo.de'), 'http://foo.de')\n        self.assertEqual(url_or_none('https://foo.de'), 'https://foo.de')\n        self.assertEqual(url_or_none('http$://foo.de'), None)\n        self.assertEqual(url_or_none('http://foo.de'), 'http://foo.de')\n        self.assertEqual(url_or_none('//foo.de'), '//foo.de')\n        self.assertEqual(url_or_none('s3://foo.de'), None)\n        self.assertEqual(url_or_none('rtmpte://foo.de'), 'rtmpte://foo.de')\n        self.assertEqual(url_or_none('mms://foo.de'), 'mms://foo.de')\n        self.assertEqual(url_or_none('rtspu://foo.de'), 'rtspu://foo.de')\n        self.assertEqual(url_or_none('ftps://foo.de'), 'ftps://foo.de')\n\n    def test_parse_age_limit(self):\n        self.assertEqual(parse_age_limit(None), None)\n        self.assertEqual(parse_age_limit(False), None)\n        self.assertEqual(parse_age_limit('invalid'), None)\n        self.assertEqual(parse_age_limit(0), 0)\n        self.assertEqual(parse_age_limit(18), 18)\n        self.assertEqual(parse_age_limit(21), 21)\n        self.assertEqual(parse_age_limit(22), None)\n        self.assertEqual(parse_age_limit('18'), 18)\n        self.assertEqual(parse_age_limit('18+'), 18)\n        self.assertEqual(parse_age_limit('PG-13'), 13)\n        self.assertEqual(parse_age_limit('TV-14'), 14)\n        self.assertEqual(parse_age_limit('TV-MA'), 17)\n        self.assertEqual(parse_age_limit('TV14'), 14)\n        self.assertEqual(parse_age_limit('TV_G'), 0)\n\n    def test_parse_duration(self):\n        self.assertEqual(parse_duration(None), None)\n        self.assertEqual(parse_duration(False), None)\n        self.assertEqual(parse_duration('invalid'), None)\n        self.assertEqual(parse_duration('1'), 1)\n        self.assertEqual(parse_duration('1337:12'), 80232)\n        self.assertEqual(parse_duration('9:12:43'), 33163)\n        self.assertEqual(parse_duration('12:00'), 720)\n        self.assertEqual(parse_duration('00:01:01'), 61)\n        self.assertEqual(parse_duration('x:y'), None)\n        self.assertEqual(parse_duration('3h11m53s'), 11513)\n        self.assertEqual(parse_duration('3h 11m 53s'), 11513)\n        self.assertEqual(parse_duration('3 hours 11 minutes 53 seconds'), 11513)\n        self.assertEqual(parse_duration('3 hours 11 mins 53 secs'), 11513)\n        self.assertEqual(parse_duration('3 hours, 11 minutes, 53 seconds'), 11513)\n        self.assertEqual(parse_duration('3 hours, 11 mins, 53 secs'), 11513)\n        self.assertEqual(parse_duration('62m45s'), 3765)\n        self.assertEqual(parse_duration('6m59s'), 419)\n        self.assertEqual(parse_duration('49s'), 49)\n        self.assertEqual(parse_duration('0h0m0s'), 0)\n        self.assertEqual(parse_duration('0m0s'), 0)\n        self.assertEqual(parse_duration('0s'), 0)\n        self.assertEqual(parse_duration('01:02:03.05'), 3723.05)\n        self.assertEqual(parse_duration('T30M38S'), 1838)\n        self.assertEqual(parse_duration('5 s'), 5)\n        self.assertEqual(parse_duration('3 min'), 180)\n        self.assertEqual(parse_duration('2.5 hours'), 9000)\n        self.assertEqual(parse_duration('02:03:04'), 7384)\n        self.assertEqual(parse_duration('01:02:03:04'), 93784)\n        self.assertEqual(parse_duration('1 hour 3 minutes'), 3780)\n        self.assertEqual(parse_duration('87 Min.'), 5220)\n        self.assertEqual(parse_duration('PT1H0.040S'), 3600.04)\n        self.assertEqual(parse_duration('PT00H03M30SZ'), 210)\n        self.assertEqual(parse_duration('P0Y0M0DT0H4M20.880S'), 260.88)\n        self.assertEqual(parse_duration('01:02:03:050'), 3723.05)\n        self.assertEqual(parse_duration('103:050'), 103.05)\n        self.assertEqual(parse_duration('1HR 3MIN'), 3780)\n        self.assertEqual(parse_duration('2hrs 3mins'), 7380)\n\n    def test_fix_xml_ampersands(self):\n        self.assertEqual(\n            fix_xml_ampersands('\"&x=y&z=a'), '\"&amp;x=y&amp;z=a')\n        self.assertEqual(\n            fix_xml_ampersands('\"&amp;x=y&wrong;&z=a'),\n            '\"&amp;x=y&amp;wrong;&amp;z=a')\n        self.assertEqual(\n            fix_xml_ampersands('&amp;&apos;&gt;&lt;&quot;'),\n            '&amp;&apos;&gt;&lt;&quot;')\n        self.assertEqual(\n            fix_xml_ampersands('&#1234;&#x1abC;'), '&#1234;&#x1abC;')\n        self.assertEqual(fix_xml_ampersands('&#&#'), '&amp;#&amp;#')\n\n    def test_paged_list(self):\n        def testPL(size, pagesize, sliceargs, expected):\n            def get_page(pagenum):\n                firstid = pagenum * pagesize\n                upto = min(size, pagenum * pagesize + pagesize)\n                yield from range(firstid, upto)\n\n            pl = OnDemandPagedList(get_page, pagesize)\n            got = pl.getslice(*sliceargs)\n            self.assertEqual(got, expected)\n\n            iapl = InAdvancePagedList(get_page, size // pagesize + 1, pagesize)\n            got = iapl.getslice(*sliceargs)\n            self.assertEqual(got, expected)\n\n        testPL(5, 2, (), [0, 1, 2, 3, 4])\n        testPL(5, 2, (1,), [1, 2, 3, 4])\n        testPL(5, 2, (2,), [2, 3, 4])\n        testPL(5, 2, (4,), [4])\n        testPL(5, 2, (0, 3), [0, 1, 2])\n        testPL(5, 2, (1, 4), [1, 2, 3])\n        testPL(5, 2, (2, 99), [2, 3, 4])\n        testPL(5, 2, (20, 99), [])\n\n    def test_read_batch_urls(self):\n        f = io.StringIO('''\\xef\\xbb\\xbf foo\n            bar\\r\n            baz\n            # More after this line\\r\n            ; or after this\n            bam''')\n        self.assertEqual(read_batch_urls(f), ['foo', 'bar', 'baz', 'bam'])\n\n    def test_urlencode_postdata(self):\n        data = urlencode_postdata({'username': 'foo@bar.com', 'password': '1234'})\n        self.assertTrue(isinstance(data, bytes))\n\n    def test_update_url_query(self):\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'quality': ['HD'], 'format': ['mp4']})),\n            parse_qs('http://example.com/path?quality=HD&format=mp4'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'system': ['LINUX', 'WINDOWS']})),\n            parse_qs('http://example.com/path?system=LINUX&system=WINDOWS'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'fields': 'id,formats,subtitles'})),\n            parse_qs('http://example.com/path?fields=id,formats,subtitles'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'fields': ('id,formats,subtitles', 'thumbnails')})),\n            parse_qs('http://example.com/path?fields=id,formats,subtitles&fields=thumbnails'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path?manifest=f4m', {'manifest': []})),\n            parse_qs('http://example.com/path'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path?system=LINUX&system=WINDOWS', {'system': 'LINUX'})),\n            parse_qs('http://example.com/path?system=LINUX'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'fields': b'id,formats,subtitles'})),\n            parse_qs('http://example.com/path?fields=id,formats,subtitles'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'width': 1080, 'height': 720})),\n            parse_qs('http://example.com/path?width=1080&height=720'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'bitrate': 5020.43})),\n            parse_qs('http://example.com/path?bitrate=5020.43'))\n        self.assertEqual(parse_qs(update_url_query(\n            'http://example.com/path', {'test': '\u7b2c\u4e8c\u884c\u0442\u0435\u0441\u0442'})),\n            parse_qs('http://example.com/path?test=%E7%AC%AC%E4%BA%8C%E8%A1%8C%D1%82%D0%B5%D1%81%D1%82'))\n\n    def test_multipart_encode(self):\n        self.assertEqual(\n            multipart_encode({b'field': b'value'}, boundary='AAAAAA')[0],\n            b'--AAAAAA\\r\\nContent-Disposition: form-data; name=\"field\"\\r\\n\\r\\nvalue\\r\\n--AAAAAA--\\r\\n')\n        self.assertEqual(\n            multipart_encode({'\u6b04\u4f4d'.encode(): '\u503c'.encode()}, boundary='AAAAAA')[0],\n            b'--AAAAAA\\r\\nContent-Disposition: form-data; name=\"\\xe6\\xac\\x84\\xe4\\xbd\\x8d\"\\r\\n\\r\\n\\xe5\\x80\\xbc\\r\\n--AAAAAA--\\r\\n')\n        self.assertRaises(\n            ValueError, multipart_encode, {b'field': b'value'}, boundary='value')\n\n    def test_dict_get(self):\n        FALSE_VALUES = {\n            'none': None,\n            'false': False,\n            'zero': 0,\n            'empty_string': '',\n            'empty_list': [],\n        }\n        d = FALSE_VALUES.copy()\n        d['a'] = 42\n        self.assertEqual(dict_get(d, 'a'), 42)\n        self.assertEqual(dict_get(d, 'b'), None)\n        self.assertEqual(dict_get(d, 'b', 42), 42)\n        self.assertEqual(dict_get(d, ('a', )), 42)\n        self.assertEqual(dict_get(d, ('b', 'a', )), 42)\n        self.assertEqual(dict_get(d, ('b', 'c', 'a', 'd', )), 42)\n        self.assertEqual(dict_get(d, ('b', 'c', )), None)\n        self.assertEqual(dict_get(d, ('b', 'c', ), 42), 42)\n        for key, false_value in FALSE_VALUES.items():\n            self.assertEqual(dict_get(d, ('b', 'c', key, )), None)\n            self.assertEqual(dict_get(d, ('b', 'c', key, ), skip_false_values=False), false_value)\n\n    def test_merge_dicts(self):\n        self.assertEqual(merge_dicts({'a': 1}, {'b': 2}), {'a': 1, 'b': 2})\n        self.assertEqual(merge_dicts({'a': 1}, {'a': 2}), {'a': 1})\n        self.assertEqual(merge_dicts({'a': 1}, {'a': None}), {'a': 1})\n        self.assertEqual(merge_dicts({'a': 1}, {'a': ''}), {'a': 1})\n        self.assertEqual(merge_dicts({'a': 1}, {}), {'a': 1})\n        self.assertEqual(merge_dicts({'a': None}, {'a': 1}), {'a': 1})\n        self.assertEqual(merge_dicts({'a': ''}, {'a': 1}), {'a': ''})\n        self.assertEqual(merge_dicts({'a': ''}, {'a': 'abc'}), {'a': 'abc'})\n        self.assertEqual(merge_dicts({'a': None}, {'a': ''}, {'a': 'abc'}), {'a': 'abc'})\n\n    def test_encode_compat_str(self):\n        self.assertEqual(encode_compat_str(b'\\xd1\\x82\\xd0\\xb5\\xd1\\x81\\xd1\\x82', 'utf-8'), '\u0442\u0435\u0441\u0442')\n        self.assertEqual(encode_compat_str('\u0442\u0435\u0441\u0442', 'utf-8'), '\u0442\u0435\u0441\u0442')\n\n    def test_parse_iso8601(self):\n        self.assertEqual(parse_iso8601('2014-03-23T23:04:26+0100'), 1395612266)\n        self.assertEqual(parse_iso8601('2014-03-23T22:04:26+0000'), 1395612266)\n        self.assertEqual(parse_iso8601('2014-03-23T22:04:26Z'), 1395612266)\n        self.assertEqual(parse_iso8601('2014-03-23T22:04:26.1234Z'), 1395612266)\n        self.assertEqual(parse_iso8601('2015-09-29T08:27:31.727'), 1443515251)\n        self.assertEqual(parse_iso8601('2015-09-29T08-27-31.727'), None)\n\n    def test_strip_jsonp(self):\n        stripped = strip_jsonp('cb ([ {\"id\":\"532cb\",\\n\\n\\n\"x\":\\n3}\\n]\\n);')\n        d = json.loads(stripped)\n        self.assertEqual(d, [{\"id\": \"532cb\", \"x\": 3}])\n\n        stripped = strip_jsonp('parseMetadata({\"STATUS\":\"OK\"})\\n\\n\\n//epc')\n        d = json.loads(stripped)\n        self.assertEqual(d, {'STATUS': 'OK'})\n\n        stripped = strip_jsonp('ps.embedHandler({\"status\": \"success\"});')\n        d = json.loads(stripped)\n        self.assertEqual(d, {'status': 'success'})\n\n        stripped = strip_jsonp('window.cb && window.cb({\"status\": \"success\"});')\n        d = json.loads(stripped)\n        self.assertEqual(d, {'status': 'success'})\n\n        stripped = strip_jsonp('window.cb && cb({\"status\": \"success\"});')\n        d = json.loads(stripped)\n        self.assertEqual(d, {'status': 'success'})\n\n        stripped = strip_jsonp('({\"status\": \"success\"});')\n        d = json.loads(stripped)\n        self.assertEqual(d, {'status': 'success'})\n\n    def test_strip_or_none(self):\n        self.assertEqual(strip_or_none(' abc'), 'abc')\n        self.assertEqual(strip_or_none('abc '), 'abc')\n        self.assertEqual(strip_or_none(' abc '), 'abc')\n        self.assertEqual(strip_or_none('\\tabc\\t'), 'abc')\n        self.assertEqual(strip_or_none('\\n\\tabc\\n\\t'), 'abc')\n        self.assertEqual(strip_or_none('abc'), 'abc')\n        self.assertEqual(strip_or_none(''), '')\n        self.assertEqual(strip_or_none(None), None)\n        self.assertEqual(strip_or_none(42), None)\n        self.assertEqual(strip_or_none([]), None)\n\n    def test_uppercase_escape(self):\n        self.assertEqual(uppercase_escape('a\u00e4'), 'a\u00e4')\n        self.assertEqual(uppercase_escape('\\\\U0001d550'), '\ud835\udd50')\n\n    def test_lowercase_escape(self):\n        self.assertEqual(lowercase_escape('a\u00e4'), 'a\u00e4')\n        self.assertEqual(lowercase_escape('\\\\u0026'), '&')\n\n    def test_limit_length(self):\n        self.assertEqual(limit_length(None, 12), None)\n        self.assertEqual(limit_length('foo', 12), 'foo')\n        self.assertTrue(\n            limit_length('foo bar baz asd', 12).startswith('foo bar'))\n        self.assertTrue('...' in limit_length('foo bar baz asd', 12))\n\n    def test_mimetype2ext(self):\n        self.assertEqual(mimetype2ext(None), None)\n        self.assertEqual(mimetype2ext('video/x-flv'), 'flv')\n        self.assertEqual(mimetype2ext('application/x-mpegURL'), 'm3u8')\n        self.assertEqual(mimetype2ext('text/vtt'), 'vtt')\n        self.assertEqual(mimetype2ext('text/vtt;charset=utf-8'), 'vtt')\n        self.assertEqual(mimetype2ext('text/html; charset=utf-8'), 'html')\n        self.assertEqual(mimetype2ext('audio/x-wav'), 'wav')\n        self.assertEqual(mimetype2ext('audio/x-wav;codec=pcm'), 'wav')\n\n    def test_month_by_name(self):\n        self.assertEqual(month_by_name(None), None)\n        self.assertEqual(month_by_name('December', 'en'), 12)\n        self.assertEqual(month_by_name('d\u00e9cembre', 'fr'), 12)\n        self.assertEqual(month_by_name('December'), 12)\n        self.assertEqual(month_by_name('d\u00e9cembre'), None)\n        self.assertEqual(month_by_name('Unknown', 'unknown'), None)\n\n    def test_parse_codecs(self):\n        self.assertEqual(parse_codecs(''), {})\n        self.assertEqual(parse_codecs('avc1.77.30, mp4a.40.2'), {\n            'vcodec': 'avc1.77.30',\n            'acodec': 'mp4a.40.2',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('mp4a.40.2'), {\n            'vcodec': 'none',\n            'acodec': 'mp4a.40.2',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('mp4a.40.5,avc1.42001e'), {\n            'vcodec': 'avc1.42001e',\n            'acodec': 'mp4a.40.5',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('avc3.640028'), {\n            'vcodec': 'avc3.640028',\n            'acodec': 'none',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs(', h264,,newcodec,aac'), {\n            'vcodec': 'h264',\n            'acodec': 'aac',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('av01.0.05M.08'), {\n            'vcodec': 'av01.0.05M.08',\n            'acodec': 'none',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('vp9.2'), {\n            'vcodec': 'vp9.2',\n            'acodec': 'none',\n            'dynamic_range': 'HDR10',\n        })\n        self.assertEqual(parse_codecs('av01.0.12M.10.0.110.09.16.09.0'), {\n            'vcodec': 'av01.0.12M.10.0.110.09.16.09.0',\n            'acodec': 'none',\n            'dynamic_range': 'HDR10',\n        })\n        self.assertEqual(parse_codecs('dvhe'), {\n            'vcodec': 'dvhe',\n            'acodec': 'none',\n            'dynamic_range': 'DV',\n        })\n        self.assertEqual(parse_codecs('theora, vorbis'), {\n            'vcodec': 'theora',\n            'acodec': 'vorbis',\n            'dynamic_range': None,\n        })\n        self.assertEqual(parse_codecs('unknownvcodec, unknownacodec'), {\n            'vcodec': 'unknownvcodec',\n            'acodec': 'unknownacodec',\n        })\n        self.assertEqual(parse_codecs('unknown'), {})\n\n    def test_escape_rfc3986(self):\n        reserved = \"!*'();:@&=+$,/?#[]\"\n        unreserved = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_.~'\n        self.assertEqual(escape_rfc3986(reserved), reserved)\n        self.assertEqual(escape_rfc3986(unreserved), unreserved)\n        self.assertEqual(escape_rfc3986('\u0442\u0435\u0441\u0442'), '%D1%82%D0%B5%D1%81%D1%82')\n        self.assertEqual(escape_rfc3986('%D1%82%D0%B5%D1%81%D1%82'), '%D1%82%D0%B5%D1%81%D1%82')\n        self.assertEqual(escape_rfc3986('foo bar'), 'foo%20bar')\n        self.assertEqual(escape_rfc3986('foo%20bar'), 'foo%20bar')\n\n    def test_normalize_url(self):\n        self.assertEqual(\n            normalize_url('http://wowza.imust.org/srv/vod/telemb/new/UPLOAD/UPLOAD/20224_IncendieHavre\u0301_FD.mp4'),\n            'http://wowza.imust.org/srv/vod/telemb/new/UPLOAD/UPLOAD/20224_IncendieHavre%CC%81_FD.mp4'\n        )\n        self.assertEqual(\n            normalize_url('http://www.ardmediathek.de/tv/Sturm-der-Liebe/Folge-2036-Zu-Mann-und-Frau-erkl\u00e4rt/Das-Erste/Video?documentId=22673108&bcastId=5290'),\n            'http://www.ardmediathek.de/tv/Sturm-der-Liebe/Folge-2036-Zu-Mann-und-Frau-erkl%C3%A4rt/Das-Erste/Video?documentId=22673108&bcastId=5290'\n        )\n        self.assertEqual(\n            normalize_url('http://\u0442\u0435\u0441\u0442.\u0440\u0444/\u0444\u0440\u0430\u0433\u043c\u0435\u043d\u0442'),\n            'http://xn--e1aybc.xn--p1ai/%D1%84%D1%80%D0%B0%D0%B3%D0%BC%D0%B5%D0%BD%D1%82'\n        )\n        self.assertEqual(\n            normalize_url('http://\u0442\u0435\u0441\u0442.\u0440\u0444/\u0430\u0431\u0432?\u0430\u0431\u0432=\u0430\u0431\u0432#\u0430\u0431\u0432'),\n            'http://xn--e1aybc.xn--p1ai/%D0%B0%D0%B1%D0%B2?%D0%B0%D0%B1%D0%B2=%D0%B0%D0%B1%D0%B2#%D0%B0%D0%B1%D0%B2'\n        )\n        self.assertEqual(normalize_url('http://vimeo.com/56015672#at=0'), 'http://vimeo.com/56015672#at=0')\n\n        self.assertEqual(normalize_url('http://www.example.com/../a/b/../c/./d.html'), 'http://www.example.com/a/c/d.html')\n\n    def test_remove_dot_segments(self):\n        self.assertEqual(remove_dot_segments('/a/b/c/./../../g'), '/a/g')\n        self.assertEqual(remove_dot_segments('mid/content=5/../6'), 'mid/6')\n        self.assertEqual(remove_dot_segments('/ad/../cd'), '/cd')\n        self.assertEqual(remove_dot_segments('/ad/../cd/'), '/cd/')\n        self.assertEqual(remove_dot_segments('/..'), '/')\n        self.assertEqual(remove_dot_segments('/./'), '/')\n        self.assertEqual(remove_dot_segments('/./a'), '/a')\n        self.assertEqual(remove_dot_segments('/abc/./.././d/././e/.././f/./../../ghi'), '/ghi')\n        self.assertEqual(remove_dot_segments('/'), '/')\n        self.assertEqual(remove_dot_segments('/t'), '/t')\n        self.assertEqual(remove_dot_segments('t'), 't')\n        self.assertEqual(remove_dot_segments(''), '')\n        self.assertEqual(remove_dot_segments('/../a/b/c'), '/a/b/c')\n        self.assertEqual(remove_dot_segments('../a'), 'a')\n        self.assertEqual(remove_dot_segments('./a'), 'a')\n        self.assertEqual(remove_dot_segments('.'), '')\n        self.assertEqual(remove_dot_segments('////'), '////')\n\n    def test_js_to_json_vars_strings(self):\n        self.assertDictEqual(\n            json.loads(js_to_json(\n                '''{\n                    'null': a,\n                    'nullStr': b,\n                    'true': c,\n                    'trueStr': d,\n                    'false': e,\n                    'falseStr': f,\n                    'unresolvedVar': g,\n                }''',\n                {\n                    'a': 'null',\n                    'b': '\"null\"',\n                    'c': 'true',\n                    'd': '\"true\"',\n                    'e': 'false',\n                    'f': '\"false\"',\n                    'g': 'var',\n                }\n            )),\n            {\n                'null': None,\n                'nullStr': 'null',\n                'true': True,\n                'trueStr': 'true',\n                'false': False,\n                'falseStr': 'false',\n                'unresolvedVar': 'var'\n            }\n        )\n\n        self.assertDictEqual(\n            json.loads(js_to_json(\n                '''{\n                    'int': a,\n                    'intStr': b,\n                    'float': c,\n                    'floatStr': d,\n                }''',\n                {\n                    'a': '123',\n                    'b': '\"123\"',\n                    'c': '1.23',\n                    'd': '\"1.23\"',\n                }\n            )),\n            {\n                'int': 123,\n                'intStr': '123',\n                'float': 1.23,\n                'floatStr': '1.23',\n            }\n        )\n\n        self.assertDictEqual(\n            json.loads(js_to_json(\n                '''{\n                    'object': a,\n                    'objectStr': b,\n                    'array': c,\n                    'arrayStr': d,\n                }''',\n                {\n                    'a': '{}',\n                    'b': '\"{}\"',\n                    'c': '[]',\n                    'd': '\"[]\"',\n                }\n            )),\n            {\n                'object': {},\n                'objectStr': '{}',\n                'array': [],\n                'arrayStr': '[]',\n            }\n        )\n\n    def test_js_to_json_realworld(self):\n        inp = '''{\n            'clip':{'provider':'pseudo'}\n        }'''\n        self.assertEqual(js_to_json(inp), '''{\n            \"clip\":{\"provider\":\"pseudo\"}\n        }''')\n        json.loads(js_to_json(inp))\n\n        inp = '''{\n            'playlist':[{'controls':{'all':null}}]\n        }'''\n        self.assertEqual(js_to_json(inp), '''{\n            \"playlist\":[{\"controls\":{\"all\":null}}]\n        }''')\n\n        inp = '''\"The CW\\\\'s \\\\'Crazy Ex-Girlfriend\\\\'\"'''\n        self.assertEqual(js_to_json(inp), '''\"The CW's 'Crazy Ex-Girlfriend'\"''')\n\n        inp = '\"SAND Number: SAND 2013-7800P\\\\nPresenter: Tom Russo\\\\nHabanero Software Training - Xyce Software\\\\nXyce, Sandia\\\\u0027s\"'\n        json_code = js_to_json(inp)\n        self.assertEqual(json.loads(json_code), json.loads(inp))\n\n        inp = '''{\n            0:{src:'skipped', type: 'application/dash+xml'},\n            1:{src:'skipped', type: 'application/vnd.apple.mpegURL'},\n        }'''\n        self.assertEqual(js_to_json(inp), '''{\n            \"0\":{\"src\":\"skipped\", \"type\": \"application/dash+xml\"},\n            \"1\":{\"src\":\"skipped\", \"type\": \"application/vnd.apple.mpegURL\"}\n        }''')\n\n        inp = '''{\"foo\":101}'''\n        self.assertEqual(js_to_json(inp), '''{\"foo\":101}''')\n\n        inp = '''{\"duration\": \"00:01:07\"}'''\n        self.assertEqual(js_to_json(inp), '''{\"duration\": \"00:01:07\"}''')\n\n        inp = '''{segments: [{\"offset\":-3.885780586188048e-16,\"duration\":39.75000000000001}]}'''\n        self.assertEqual(js_to_json(inp), '''{\"segments\": [{\"offset\":-3.885780586188048e-16,\"duration\":39.75000000000001}]}''')\n\n    def test_js_to_json_edgecases(self):\n        on = js_to_json(\"{abc_def:'1\\\\'\\\\\\\\2\\\\\\\\\\\\'3\\\"4'}\")\n        self.assertEqual(json.loads(on), {\"abc_def\": \"1'\\\\2\\\\'3\\\"4\"})\n\n        on = js_to_json('{\"abc\": true}')\n        self.assertEqual(json.loads(on), {'abc': True})\n\n        # Ignore JavaScript code as well\n        on = js_to_json('''{\n            \"x\": 1,\n            y: \"a\",\n            z: some.code\n        }''')\n        d = json.loads(on)\n        self.assertEqual(d['x'], 1)\n        self.assertEqual(d['y'], 'a')\n\n        # Just drop ! prefix for now though this results in a wrong value\n        on = js_to_json('''{\n            a: !0,\n            b: !1,\n            c: !!0,\n            d: !!42.42,\n            e: !!![],\n            f: !\"abc\",\n            g: !\"\",\n            !42: 42\n        }''')\n        self.assertEqual(json.loads(on), {\n            'a': 0,\n            'b': 1,\n            'c': 0,\n            'd': 42.42,\n            'e': [],\n            'f': \"abc\",\n            'g': \"\",\n            '42': 42\n        })\n\n        on = js_to_json('[\"abc\", \"def\",]')\n        self.assertEqual(json.loads(on), ['abc', 'def'])\n\n        on = js_to_json('[/*comment\\n*/\"abc\"/*comment\\n*/,/*comment\\n*/\"def\",/*comment\\n*/]')\n        self.assertEqual(json.loads(on), ['abc', 'def'])\n\n        on = js_to_json('[//comment\\n\"abc\" //comment\\n,//comment\\n\"def\",//comment\\n]')\n        self.assertEqual(json.loads(on), ['abc', 'def'])\n\n        on = js_to_json('{\"abc\": \"def\",}')\n        self.assertEqual(json.loads(on), {'abc': 'def'})\n\n        on = js_to_json('{/*comment\\n*/\"abc\"/*comment\\n*/:/*comment\\n*/\"def\"/*comment\\n*/,/*comment\\n*/}')\n        self.assertEqual(json.loads(on), {'abc': 'def'})\n\n        on = js_to_json('{ 0: /* \" \\n */ \",]\" , }')\n        self.assertEqual(json.loads(on), {'0': ',]'})\n\n        on = js_to_json('{ /*comment\\n*/0/*comment\\n*/: /* \" \\n */ \",]\" , }')\n        self.assertEqual(json.loads(on), {'0': ',]'})\n\n        on = js_to_json('{ 0: // comment\\n1 }')\n        self.assertEqual(json.loads(on), {'0': 1})\n\n        on = js_to_json(r'[\"<p>x<\\/p>\"]')\n        self.assertEqual(json.loads(on), ['<p>x</p>'])\n\n        on = js_to_json(r'[\"\\xaa\"]')\n        self.assertEqual(json.loads(on), ['\\u00aa'])\n\n        on = js_to_json(\"['a\\\\\\nb']\")\n        self.assertEqual(json.loads(on), ['ab'])\n\n        on = js_to_json(\"/*comment\\n*/[/*comment\\n*/'a\\\\\\nb'/*comment\\n*/]/*comment\\n*/\")\n        self.assertEqual(json.loads(on), ['ab'])\n\n        on = js_to_json('{0xff:0xff}')\n        self.assertEqual(json.loads(on), {'255': 255})\n\n        on = js_to_json('{/*comment\\n*/0xff/*comment\\n*/:/*comment\\n*/0xff/*comment\\n*/}')\n        self.assertEqual(json.loads(on), {'255': 255})\n\n        on = js_to_json('{077:077}')\n        self.assertEqual(json.loads(on), {'63': 63})\n\n        on = js_to_json('{/*comment\\n*/077/*comment\\n*/:/*comment\\n*/077/*comment\\n*/}')\n        self.assertEqual(json.loads(on), {'63': 63})\n\n        on = js_to_json('{42:42}')\n        self.assertEqual(json.loads(on), {'42': 42})\n\n        on = js_to_json('{/*comment\\n*/42/*comment\\n*/:/*comment\\n*/42/*comment\\n*/}')\n        self.assertEqual(json.loads(on), {'42': 42})\n\n        on = js_to_json('{42:4.2e1}')\n        self.assertEqual(json.loads(on), {'42': 42.0})\n\n        on = js_to_json('{ \"0x40\": \"0x40\" }')\n        self.assertEqual(json.loads(on), {'0x40': '0x40'})\n\n        on = js_to_json('{ \"040\": \"040\" }')\n        self.assertEqual(json.loads(on), {'040': '040'})\n\n        on = js_to_json('[1,//{},\\n2]')\n        self.assertEqual(json.loads(on), [1, 2])\n\n        on = js_to_json(R'\"\\^\\$\\#\"')\n        self.assertEqual(json.loads(on), R'^$#', msg='Unnecessary escapes should be stripped')\n\n        on = js_to_json('\\'\"\\\\\"\"\\'')\n        self.assertEqual(json.loads(on), '\"\"\"', msg='Unnecessary quote escape should be escaped')\n\n    def test_js_to_json_malformed(self):\n        self.assertEqual(js_to_json('42a1'), '42\"a1\"')\n        self.assertEqual(js_to_json('42a-1'), '42\"a\"-1')\n\n    def test_js_to_json_template_literal(self):\n        self.assertEqual(js_to_json('`Hello ${name}`', {'name': '\"world\"'}), '\"Hello world\"')\n        self.assertEqual(js_to_json('`${name}${name}`', {'name': '\"X\"'}), '\"XX\"')\n        self.assertEqual(js_to_json('`${name}${name}`', {'name': '5'}), '\"55\"')\n        self.assertEqual(js_to_json('`${name}\"${name}\"`', {'name': '5'}), '\"5\\\\\"5\\\\\"\"')\n        self.assertEqual(js_to_json('`${name}`', {}), '\"name\"')\n\n    def test_js_to_json_map_array_constructors(self):\n        self.assertEqual(json.loads(js_to_json('new Map([[\"a\", 5]])')), {'a': 5})\n        self.assertEqual(json.loads(js_to_json('Array(5, 10)')), [5, 10])\n        self.assertEqual(json.loads(js_to_json('new Array(15,5)')), [15, 5])\n        self.assertEqual(json.loads(js_to_json('new Map([Array(5, 10),new Array(15,5)])')), {'5': 10, '15': 5})\n\n    def test_extract_attributes(self):\n        self.assertEqual(extract_attributes('<e x=\"y\">'), {'x': 'y'})\n        self.assertEqual(extract_attributes(\"<e x='y'>\"), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e x=y>'), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e x=\"a \\'b\\' c\">'), {'x': \"a 'b' c\"})\n        self.assertEqual(extract_attributes('<e x=\\'a \"b\" c\\'>'), {'x': 'a \"b\" c'})\n        self.assertEqual(extract_attributes('<e x=\"&#121;\">'), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e x=\"&#x79;\">'), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e x=\"&amp;\">'), {'x': '&'})  # XML\n        self.assertEqual(extract_attributes('<e x=\"&quot;\">'), {'x': '\"'})\n        self.assertEqual(extract_attributes('<e x=\"&pound;\">'), {'x': '\u00a3'})  # HTML 3.2\n        self.assertEqual(extract_attributes('<e x=\"&lambda;\">'), {'x': '\u03bb'})  # HTML 4.0\n        self.assertEqual(extract_attributes('<e x=\"&foo\">'), {'x': '&foo'})\n        self.assertEqual(extract_attributes('<e x=\"\\'\">'), {'x': \"'\"})\n        self.assertEqual(extract_attributes('<e x=\\'\"\\'>'), {'x': '\"'})\n        self.assertEqual(extract_attributes('<e x >'), {'x': None})\n        self.assertEqual(extract_attributes('<e x=y a>'), {'x': 'y', 'a': None})\n        self.assertEqual(extract_attributes('<e x= y>'), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e x=1 y=2 x=3>'), {'y': '2', 'x': '3'})\n        self.assertEqual(extract_attributes('<e \\nx=\\ny\\n>'), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e \\nx=\\n\"y\"\\n>'), {'x': 'y'})\n        self.assertEqual(extract_attributes(\"<e \\nx=\\n'y'\\n>\"), {'x': 'y'})\n        self.assertEqual(extract_attributes('<e \\nx=\"\\ny\\n\">'), {'x': '\\ny\\n'})\n        self.assertEqual(extract_attributes('<e CAPS=x>'), {'caps': 'x'})  # Names lowercased\n        self.assertEqual(extract_attributes('<e x=1 X=2>'), {'x': '2'})\n        self.assertEqual(extract_attributes('<e X=1 x=2>'), {'x': '2'})\n        self.assertEqual(extract_attributes('<e _:funny-name1=1>'), {'_:funny-name1': '1'})\n        self.assertEqual(extract_attributes('<e x=\"F\u00e1ilte \u4e16\u754c \\U0001f600\">'), {'x': 'F\u00e1ilte \u4e16\u754c \\U0001f600'})\n        self.assertEqual(extract_attributes('<e x=\"d\u00e9compose&#769;\">'), {'x': 'd\u00e9compose\\u0301'})\n        # \"Narrow\" Python builds don't support unicode code points outside BMP.\n        try:\n            chr(0x10000)\n            supports_outside_bmp = True\n        except ValueError:\n            supports_outside_bmp = False\n        if supports_outside_bmp:\n            self.assertEqual(extract_attributes('<e x=\"Smile &#128512;!\">'), {'x': 'Smile \\U0001f600!'})\n        # Malformed HTML should not break attributes extraction on older Python\n        self.assertEqual(extract_attributes('<mal\"formed/>'), {})\n\n    def test_clean_html(self):\n        self.assertEqual(clean_html('a:\\nb'), 'a: b')\n        self.assertEqual(clean_html('a:\\n   \"b\"'), 'a: \"b\"')\n        self.assertEqual(clean_html('a<br>\\xa0b'), 'a\\nb')\n\n    def test_intlist_to_bytes(self):\n        self.assertEqual(\n            intlist_to_bytes([0, 1, 127, 128, 255]),\n            b'\\x00\\x01\\x7f\\x80\\xff')\n\n    def test_args_to_str(self):\n        self.assertEqual(\n            args_to_str(['foo', 'ba/r', '-baz', '2 be', '']),\n            'foo ba/r -baz \\'2 be\\' \\'\\'' if compat_os_name != 'nt' else 'foo ba/r -baz \"2 be\" \"\"'\n        )\n\n    def test_parse_filesize(self):\n        self.assertEqual(parse_filesize(None), None)\n        self.assertEqual(parse_filesize(''), None)\n        self.assertEqual(parse_filesize('91 B'), 91)\n        self.assertEqual(parse_filesize('foobar'), None)\n        self.assertEqual(parse_filesize('2 MiB'), 2097152)\n        self.assertEqual(parse_filesize('5 GB'), 5000000000)\n        self.assertEqual(parse_filesize('1.2Tb'), 1200000000000)\n        self.assertEqual(parse_filesize('1.2tb'), 1200000000000)\n        self.assertEqual(parse_filesize('1,24 KB'), 1240)\n        self.assertEqual(parse_filesize('1,24 kb'), 1240)\n        self.assertEqual(parse_filesize('8.5 megabytes'), 8500000)\n\n    def test_parse_count(self):\n        self.assertEqual(parse_count(None), None)\n        self.assertEqual(parse_count(''), None)\n        self.assertEqual(parse_count('0'), 0)\n        self.assertEqual(parse_count('1000'), 1000)\n        self.assertEqual(parse_count('1.000'), 1000)\n        self.assertEqual(parse_count('1.1k'), 1100)\n        self.assertEqual(parse_count('1.1 k'), 1100)\n        self.assertEqual(parse_count('1,1 k'), 1100)\n        self.assertEqual(parse_count('1.1kk'), 1100000)\n        self.assertEqual(parse_count('1.1kk '), 1100000)\n        self.assertEqual(parse_count('1,1kk'), 1100000)\n        self.assertEqual(parse_count('100 views'), 100)\n        self.assertEqual(parse_count('1,100 views'), 1100)\n        self.assertEqual(parse_count('1.1kk views'), 1100000)\n        self.assertEqual(parse_count('10M views'), 10000000)\n        self.assertEqual(parse_count('has 10M views'), 10000000)\n\n    def test_parse_resolution(self):\n        self.assertEqual(parse_resolution(None), {})\n        self.assertEqual(parse_resolution(''), {})\n        self.assertEqual(parse_resolution(' 1920x1080'), {'width': 1920, 'height': 1080})\n        self.assertEqual(parse_resolution('1920\u00d71080 '), {'width': 1920, 'height': 1080})\n        self.assertEqual(parse_resolution('1920 x 1080'), {'width': 1920, 'height': 1080})\n        self.assertEqual(parse_resolution('720p'), {'height': 720})\n        self.assertEqual(parse_resolution('4k'), {'height': 2160})\n        self.assertEqual(parse_resolution('8K'), {'height': 4320})\n        self.assertEqual(parse_resolution('pre_1920x1080_post'), {'width': 1920, 'height': 1080})\n        self.assertEqual(parse_resolution('ep1x2'), {})\n        self.assertEqual(parse_resolution('1920, 1080'), {'width': 1920, 'height': 1080})\n\n    def test_parse_bitrate(self):\n        self.assertEqual(parse_bitrate(None), None)\n        self.assertEqual(parse_bitrate(''), None)\n        self.assertEqual(parse_bitrate('300kbps'), 300)\n        self.assertEqual(parse_bitrate('1500kbps'), 1500)\n        self.assertEqual(parse_bitrate('300 kbps'), 300)\n\n    def test_version_tuple(self):\n        self.assertEqual(version_tuple('1'), (1,))\n        self.assertEqual(version_tuple('10.23.344'), (10, 23, 344))\n        self.assertEqual(version_tuple('10.1-6'), (10, 1, 6))  # avconv style\n\n    def test_detect_exe_version(self):\n        self.assertEqual(detect_exe_version('''ffmpeg version 1.2.1\nbuilt on May 27 2013 08:37:26 with gcc 4.7 (Debian 4.7.3-4)\nconfiguration: --prefix=/usr --extra-'''), '1.2.1')\n        self.assertEqual(detect_exe_version('''ffmpeg version N-63176-g1fb4685\nbuilt on May 15 2014 22:09:06 with gcc 4.8.2 (GCC)'''), 'N-63176-g1fb4685')\n        self.assertEqual(detect_exe_version('''X server found. dri2 connection failed!\nTrying to open render node...\nSuccess at /dev/dri/renderD128.\nffmpeg version 2.4.4 Copyright (c) 2000-2014 the FFmpeg ...'''), '2.4.4')\n\n    def test_age_restricted(self):\n        self.assertFalse(age_restricted(None, 10))  # unrestricted content\n        self.assertFalse(age_restricted(1, None))  # unrestricted policy\n        self.assertFalse(age_restricted(8, 10))\n        self.assertTrue(age_restricted(18, 14))\n        self.assertFalse(age_restricted(18, 18))\n\n    def test_is_html(self):\n        self.assertFalse(is_html(b'\\x49\\x44\\x43<html'))\n        self.assertTrue(is_html(b'<!DOCTYPE foo>\\xaaa'))\n        self.assertTrue(is_html(  # UTF-8 with BOM\n            b'\\xef\\xbb\\xbf<!DOCTYPE foo>\\xaaa'))\n        self.assertTrue(is_html(  # UTF-16-LE\n            b'\\xff\\xfe<\\x00h\\x00t\\x00m\\x00l\\x00>\\x00\\xe4\\x00'\n        ))\n        self.assertTrue(is_html(  # UTF-16-BE\n            b'\\xfe\\xff\\x00<\\x00h\\x00t\\x00m\\x00l\\x00>\\x00\\xe4'\n        ))\n        self.assertTrue(is_html(  # UTF-32-BE\n            b'\\x00\\x00\\xFE\\xFF\\x00\\x00\\x00<\\x00\\x00\\x00h\\x00\\x00\\x00t\\x00\\x00\\x00m\\x00\\x00\\x00l\\x00\\x00\\x00>\\x00\\x00\\x00\\xe4'))\n        self.assertTrue(is_html(  # UTF-32-LE\n            b'\\xFF\\xFE\\x00\\x00<\\x00\\x00\\x00h\\x00\\x00\\x00t\\x00\\x00\\x00m\\x00\\x00\\x00l\\x00\\x00\\x00>\\x00\\x00\\x00\\xe4\\x00\\x00\\x00'))\n\n    def test_render_table(self):\n        self.assertEqual(\n            render_table(\n                ['a', 'empty', 'bcd'],\n                [[123, '', 4], [9999, '', 51]]),\n            'a    empty bcd\\n'\n            '123        4\\n'\n            '9999       51')\n\n        self.assertEqual(\n            render_table(\n                ['a', 'empty', 'bcd'],\n                [[123, '', 4], [9999, '', 51]],\n                hide_empty=True),\n            'a    bcd\\n'\n            '123  4\\n'\n            '9999 51')\n\n        self.assertEqual(\n            render_table(\n                ['\\ta', 'bcd'],\n                [['1\\t23', 4], ['\\t9999', 51]]),\n            '   a bcd\\n'\n            '1 23 4\\n'\n            '9999 51')\n\n        self.assertEqual(\n            render_table(\n                ['a', 'bcd'],\n                [[123, 4], [9999, 51]],\n                delim='-'),\n            'a    bcd\\n'\n            '--------\\n'\n            '123  4\\n'\n            '9999 51')\n\n        self.assertEqual(\n            render_table(\n                ['a', 'bcd'],\n                [[123, 4], [9999, 51]],\n                delim='-', extra_gap=2),\n            'a      bcd\\n'\n            '----------\\n'\n            '123    4\\n'\n            '9999   51')\n\n    def test_match_str(self):\n        # Unary\n        self.assertFalse(match_str('xy', {'x': 1200}))\n        self.assertTrue(match_str('!xy', {'x': 1200}))\n        self.assertTrue(match_str('x', {'x': 1200}))\n        self.assertFalse(match_str('!x', {'x': 1200}))\n        self.assertTrue(match_str('x', {'x': 0}))\n        self.assertTrue(match_str('is_live', {'is_live': True}))\n        self.assertFalse(match_str('is_live', {'is_live': False}))\n        self.assertFalse(match_str('is_live', {'is_live': None}))\n        self.assertFalse(match_str('is_live', {}))\n        self.assertFalse(match_str('!is_live', {'is_live': True}))\n        self.assertTrue(match_str('!is_live', {'is_live': False}))\n        self.assertTrue(match_str('!is_live', {'is_live': None}))\n        self.assertTrue(match_str('!is_live', {}))\n        self.assertTrue(match_str('title', {'title': 'abc'}))\n        self.assertTrue(match_str('title', {'title': ''}))\n        self.assertFalse(match_str('!title', {'title': 'abc'}))\n        self.assertFalse(match_str('!title', {'title': ''}))\n\n        # Numeric\n        self.assertFalse(match_str('x>0', {'x': 0}))\n        self.assertFalse(match_str('x>0', {}))\n        self.assertTrue(match_str('x>?0', {}))\n        self.assertTrue(match_str('x>1K', {'x': 1200}))\n        self.assertFalse(match_str('x>2K', {'x': 1200}))\n        self.assertTrue(match_str('x>=1200 & x < 1300', {'x': 1200}))\n        self.assertFalse(match_str('x>=1100 & x < 1200', {'x': 1200}))\n        self.assertTrue(match_str('x > 1:0:0', {'x': 3700}))\n\n        # String\n        self.assertFalse(match_str('y=a212', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y=foobar42', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y!=foobar42', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y!=foobar2', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y^=foo', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y!^=foo', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y^=bar', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y!^=bar', {'y': 'foobar42'}))\n        self.assertRaises(ValueError, match_str, 'x^=42', {'x': 42})\n        self.assertTrue(match_str('y*=bar', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y!*=bar', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y*=baz', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y!*=baz', {'y': 'foobar42'}))\n        self.assertTrue(match_str('y$=42', {'y': 'foobar42'}))\n        self.assertFalse(match_str('y$=43', {'y': 'foobar42'}))\n\n        # And\n        self.assertFalse(match_str(\n            'like_count > 100 & dislike_count <? 50 & description',\n            {'like_count': 90, 'description': 'foo'}))\n        self.assertTrue(match_str(\n            'like_count > 100 & dislike_count <? 50 & description',\n            {'like_count': 190, 'description': 'foo'}))\n        self.assertFalse(match_str(\n            'like_count > 100 & dislike_count <? 50 & description',\n            {'like_count': 190, 'dislike_count': 60, 'description': 'foo'}))\n        self.assertFalse(match_str(\n            'like_count > 100 & dislike_count <? 50 & description',\n            {'like_count': 190, 'dislike_count': 10}))\n\n        # Regex\n        self.assertTrue(match_str(r'x~=\\bbar', {'x': 'foo bar'}))\n        self.assertFalse(match_str(r'x~=\\bbar.+', {'x': 'foo bar'}))\n        self.assertFalse(match_str(r'x~=^FOO', {'x': 'foo bar'}))\n        self.assertTrue(match_str(r'x~=(?i)^FOO', {'x': 'foo bar'}))\n\n        # Quotes\n        self.assertTrue(match_str(r'x^=\"foo\"', {'x': 'foo \"bar\"'}))\n        self.assertFalse(match_str(r'x^=\"foo  \"', {'x': 'foo \"bar\"'}))\n        self.assertFalse(match_str(r'x$=\"bar\"', {'x': 'foo \"bar\"'}))\n        self.assertTrue(match_str(r'x$=\" \\\"bar\\\"\"', {'x': 'foo \"bar\"'}))\n\n        # Escaping &\n        self.assertFalse(match_str(r'x=foo & bar', {'x': 'foo & bar'}))\n        self.assertTrue(match_str(r'x=foo \\& bar', {'x': 'foo & bar'}))\n        self.assertTrue(match_str(r'x=foo \\& bar & x^=foo', {'x': 'foo & bar'}))\n        self.assertTrue(match_str(r'x=\"foo \\& bar\" & x^=foo', {'x': 'foo & bar'}))\n\n        # Example from docs\n        self.assertTrue(match_str(\n            r\"!is_live & like_count>?100 & description~='(?i)\\bcats \\& dogs\\b'\",\n            {'description': 'Raining Cats & Dogs'}))\n\n        # Incomplete\n        self.assertFalse(match_str('id!=foo', {'id': 'foo'}, True))\n        self.assertTrue(match_str('x', {'id': 'foo'}, True))\n        self.assertTrue(match_str('!x', {'id': 'foo'}, True))\n        self.assertFalse(match_str('x', {'id': 'foo'}, False))\n\n    def test_parse_dfxp_time_expr(self):\n        self.assertEqual(parse_dfxp_time_expr(None), None)\n        self.assertEqual(parse_dfxp_time_expr(''), None)\n        self.assertEqual(parse_dfxp_time_expr('0.1'), 0.1)\n        self.assertEqual(parse_dfxp_time_expr('0.1s'), 0.1)\n        self.assertEqual(parse_dfxp_time_expr('00:00:01'), 1.0)\n        self.assertEqual(parse_dfxp_time_expr('00:00:01.100'), 1.1)\n        self.assertEqual(parse_dfxp_time_expr('00:00:01:100'), 1.1)\n\n    def test_dfxp2srt(self):\n        dfxp_data = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <tt xmlns=\"http://www.w3.org/ns/ttml\" xml:lang=\"en\" xmlns:tts=\"http://www.w3.org/ns/ttml#parameter\">\n            <body>\n                <div xml:lang=\"en\">\n                    <p begin=\"0\" end=\"1\">The following line contains Chinese characters and special symbols</p>\n                    <p begin=\"1\" end=\"2\">\u7b2c\u4e8c\u884c<br/>\u266a\u266a</p>\n                    <p begin=\"2\" dur=\"1\"><span>Third<br/>Line</span></p>\n                    <p begin=\"3\" end=\"-1\">Lines with invalid timestamps are ignored</p>\n                    <p begin=\"-1\" end=\"-1\">Ignore, two</p>\n                    <p begin=\"3\" dur=\"-1\">Ignored, three</p>\n                </div>\n            </body>\n            </tt>'''.encode()\n        srt_data = '''1\n00:00:00,000 --> 00:00:01,000\nThe following line contains Chinese characters and special symbols\n\n2\n00:00:01,000 --> 00:00:02,000\n\u7b2c\u4e8c\u884c\n\u266a\u266a\n\n3\n00:00:02,000 --> 00:00:03,000\nThird\nLine\n\n'''\n        self.assertEqual(dfxp2srt(dfxp_data), srt_data)\n\n        dfxp_data_no_default_namespace = b'''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n            <tt xml:lang=\"en\" xmlns:tts=\"http://www.w3.org/ns/ttml#parameter\">\n            <body>\n                <div xml:lang=\"en\">\n                    <p begin=\"0\" end=\"1\">The first line</p>\n                </div>\n            </body>\n            </tt>'''\n        srt_data = '''1\n00:00:00,000 --> 00:00:01,000\nThe first line\n\n'''\n        self.assertEqual(dfxp2srt(dfxp_data_no_default_namespace), srt_data)\n\n        dfxp_data_with_style = b'''<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<tt xmlns=\"http://www.w3.org/2006/10/ttaf1\" xmlns:ttp=\"http://www.w3.org/2006/10/ttaf1#parameter\" ttp:timeBase=\"media\" xmlns:tts=\"http://www.w3.org/2006/10/ttaf1#style\" xml:lang=\"en\" xmlns:ttm=\"http://www.w3.org/2006/10/ttaf1#metadata\">\n  <head>\n    <styling>\n      <style id=\"s2\" style=\"s0\" tts:color=\"cyan\" tts:fontWeight=\"bold\" />\n      <style id=\"s1\" style=\"s0\" tts:color=\"yellow\" tts:fontStyle=\"italic\" />\n      <style id=\"s3\" style=\"s0\" tts:color=\"lime\" tts:textDecoration=\"underline\" />\n      <style id=\"s0\" tts:backgroundColor=\"black\" tts:fontStyle=\"normal\" tts:fontSize=\"16\" tts:fontFamily=\"sansSerif\" tts:color=\"white\" />\n    </styling>\n  </head>\n  <body tts:textAlign=\"center\" style=\"s0\">\n    <div>\n      <p begin=\"00:00:02.08\" id=\"p0\" end=\"00:00:05.84\">default style<span tts:color=\"red\">custom style</span></p>\n      <p style=\"s2\" begin=\"00:00:02.08\" id=\"p0\" end=\"00:00:05.84\"><span tts:color=\"lime\">part 1<br /></span><span tts:color=\"cyan\">part 2</span></p>\n      <p style=\"s3\" begin=\"00:00:05.84\" id=\"p1\" end=\"00:00:09.56\">line 3<br />part 3</p>\n      <p style=\"s1\" tts:textDecoration=\"underline\" begin=\"00:00:09.56\" id=\"p2\" end=\"00:00:12.36\"><span style=\"s2\" tts:color=\"lime\">inner<br /> </span>style</p>\n    </div>\n  </body>\n</tt>'''\n        srt_data = '''1\n00:00:02,080 --> 00:00:05,840\n<font color=\"white\" face=\"sansSerif\" size=\"16\">default style<font color=\"red\">custom style</font></font>\n\n2\n00:00:02,080 --> 00:00:05,840\n<b><font color=\"cyan\" face=\"sansSerif\" size=\"16\"><font color=\"lime\">part 1\n</font>part 2</font></b>\n\n3\n00:00:05,840 --> 00:00:09,560\n<u><font color=\"lime\">line 3\npart 3</font></u>\n\n4\n00:00:09,560 --> 00:00:12,360\n<i><u><font color=\"yellow\"><font color=\"lime\">inner\n </font>style</font></u></i>\n\n'''\n        self.assertEqual(dfxp2srt(dfxp_data_with_style), srt_data)\n\n        dfxp_data_non_utf8 = '''<?xml version=\"1.0\" encoding=\"UTF-16\"?>\n            <tt xmlns=\"http://www.w3.org/ns/ttml\" xml:lang=\"en\" xmlns:tts=\"http://www.w3.org/ns/ttml#parameter\">\n            <body>\n                <div xml:lang=\"en\">\n                    <p begin=\"0\" end=\"1\">Line 1</p>\n                    <p begin=\"1\" end=\"2\">\u7b2c\u4e8c\u884c</p>\n                </div>\n            </body>\n            </tt>'''.encode('utf-16')\n        srt_data = '''1\n00:00:00,000 --> 00:00:01,000\nLine 1\n\n2\n00:00:01,000 --> 00:00:02,000\n\u7b2c\u4e8c\u884c\n\n'''\n        self.assertEqual(dfxp2srt(dfxp_data_non_utf8), srt_data)\n\n    def test_cli_option(self):\n        self.assertEqual(cli_option({'proxy': '127.0.0.1:3128'}, '--proxy', 'proxy'), ['--proxy', '127.0.0.1:3128'])\n        self.assertEqual(cli_option({'proxy': None}, '--proxy', 'proxy'), [])\n        self.assertEqual(cli_option({}, '--proxy', 'proxy'), [])\n        self.assertEqual(cli_option({'retries': 10}, '--retries', 'retries'), ['--retries', '10'])\n\n    def test_cli_valueless_option(self):\n        self.assertEqual(cli_valueless_option(\n            {'downloader': 'external'}, '--external-downloader', 'downloader', 'external'), ['--external-downloader'])\n        self.assertEqual(cli_valueless_option(\n            {'downloader': 'internal'}, '--external-downloader', 'downloader', 'external'), [])\n        self.assertEqual(cli_valueless_option(\n            {'nocheckcertificate': True}, '--no-check-certificate', 'nocheckcertificate'), ['--no-check-certificate'])\n        self.assertEqual(cli_valueless_option(\n            {'nocheckcertificate': False}, '--no-check-certificate', 'nocheckcertificate'), [])\n        self.assertEqual(cli_valueless_option(\n            {'checkcertificate': True}, '--no-check-certificate', 'checkcertificate', False), [])\n        self.assertEqual(cli_valueless_option(\n            {'checkcertificate': False}, '--no-check-certificate', 'checkcertificate', False), ['--no-check-certificate'])\n\n    def test_cli_bool_option(self):\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': True}, '--no-check-certificate', 'nocheckcertificate'),\n            ['--no-check-certificate', 'true'])\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': True}, '--no-check-certificate', 'nocheckcertificate', separator='='),\n            ['--no-check-certificate=true'])\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': True}, '--check-certificate', 'nocheckcertificate', 'false', 'true'),\n            ['--check-certificate', 'false'])\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': True}, '--check-certificate', 'nocheckcertificate', 'false', 'true', '='),\n            ['--check-certificate=false'])\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': False}, '--check-certificate', 'nocheckcertificate', 'false', 'true'),\n            ['--check-certificate', 'true'])\n        self.assertEqual(\n            cli_bool_option(\n                {'nocheckcertificate': False}, '--check-certificate', 'nocheckcertificate', 'false', 'true', '='),\n            ['--check-certificate=true'])\n        self.assertEqual(\n            cli_bool_option(\n                {}, '--check-certificate', 'nocheckcertificate', 'false', 'true', '='),\n            [])\n\n    def test_ohdave_rsa_encrypt(self):\n        N = 0xab86b6371b5318aaa1d3c9e612a9f1264f372323c8c0f19875b5fc3b3fd3afcc1e5bec527aa94bfa85bffc157e4245aebda05389a5357b75115ac94f074aefcd\n        e = 65537\n\n        self.assertEqual(\n            ohdave_rsa_encrypt(b'aa111222', e, N),\n            '726664bd9a23fd0c70f9f1b84aab5e3905ce1e45a584e9cbcf9bcc7510338fc1986d6c599ff990d923aa43c51c0d9013cd572e13bc58f4ae48f2ed8c0b0ba881')\n\n    def test_pkcs1pad(self):\n        data = [1, 2, 3]\n        padded_data = pkcs1pad(data, 32)\n        self.assertEqual(padded_data[:2], [0, 2])\n        self.assertEqual(padded_data[28:], [0, 1, 2, 3])\n\n        self.assertRaises(ValueError, pkcs1pad, data, 8)\n\n    def test_encode_base_n(self):\n        self.assertEqual(encode_base_n(0, 30), '0')\n        self.assertEqual(encode_base_n(80, 30), '2k')\n\n        custom_table = '9876543210ZYXWVUTSRQPONMLKJIHGFEDCBA'\n        self.assertEqual(encode_base_n(0, 30, custom_table), '9')\n        self.assertEqual(encode_base_n(80, 30, custom_table), '7P')\n\n        self.assertRaises(ValueError, encode_base_n, 0, 70)\n        self.assertRaises(ValueError, encode_base_n, 0, 60, custom_table)\n\n    def test_caesar(self):\n        self.assertEqual(caesar('ace', 'abcdef', 2), 'cea')\n        self.assertEqual(caesar('cea', 'abcdef', -2), 'ace')\n        self.assertEqual(caesar('ace', 'abcdef', -2), 'eac')\n        self.assertEqual(caesar('eac', 'abcdef', 2), 'ace')\n        self.assertEqual(caesar('ace', 'abcdef', 0), 'ace')\n        self.assertEqual(caesar('xyz', 'abcdef', 2), 'xyz')\n        self.assertEqual(caesar('abc', 'acegik', 2), 'ebg')\n        self.assertEqual(caesar('ebg', 'acegik', -2), 'abc')\n\n    def test_rot47(self):\n        self.assertEqual(rot47('yt-dlp'), r'JE\\5=A')\n        self.assertEqual(rot47('YT-DLP'), r'*%\\s{!')\n\n    def test_urshift(self):\n        self.assertEqual(urshift(3, 1), 1)\n        self.assertEqual(urshift(-3, 1), 2147483646)\n\n    GET_ELEMENT_BY_CLASS_TEST_STRING = '''\n        <span class=\"foo bar\">nice</span>\n    '''\n\n    def test_get_element_by_class(self):\n        html = self.GET_ELEMENT_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_element_by_class('foo', html), 'nice')\n        self.assertEqual(get_element_by_class('no-such-class', html), None)\n\n    def test_get_element_html_by_class(self):\n        html = self.GET_ELEMENT_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_element_html_by_class('foo', html), html.strip())\n        self.assertEqual(get_element_by_class('no-such-class', html), None)\n\n    GET_ELEMENT_BY_ATTRIBUTE_TEST_STRING = '''\n        <div itemprop=\"author\" itemscope>foo</div>\n    '''\n\n    def test_get_element_by_attribute(self):\n        html = self.GET_ELEMENT_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_element_by_attribute('class', 'foo bar', html), 'nice')\n        self.assertEqual(get_element_by_attribute('class', 'foo', html), None)\n        self.assertEqual(get_element_by_attribute('class', 'no-such-foo', html), None)\n\n        html = self.GET_ELEMENT_BY_ATTRIBUTE_TEST_STRING\n\n        self.assertEqual(get_element_by_attribute('itemprop', 'author', html), 'foo')\n\n    def test_get_element_html_by_attribute(self):\n        html = self.GET_ELEMENT_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_element_html_by_attribute('class', 'foo bar', html), html.strip())\n        self.assertEqual(get_element_html_by_attribute('class', 'foo', html), None)\n        self.assertEqual(get_element_html_by_attribute('class', 'no-such-foo', html), None)\n\n        html = self.GET_ELEMENT_BY_ATTRIBUTE_TEST_STRING\n\n        self.assertEqual(get_element_html_by_attribute('itemprop', 'author', html), html.strip())\n\n    GET_ELEMENTS_BY_CLASS_TEST_STRING = '''\n        <span class=\"foo bar\">nice</span><span class=\"foo bar\">also nice</span>\n    '''\n    GET_ELEMENTS_BY_CLASS_RES = ['<span class=\"foo bar\">nice</span>', '<span class=\"foo bar\">also nice</span>']\n\n    def test_get_elements_by_class(self):\n        html = self.GET_ELEMENTS_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_elements_by_class('foo', html), ['nice', 'also nice'])\n        self.assertEqual(get_elements_by_class('no-such-class', html), [])\n\n    def test_get_elements_html_by_class(self):\n        html = self.GET_ELEMENTS_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_elements_html_by_class('foo', html), self.GET_ELEMENTS_BY_CLASS_RES)\n        self.assertEqual(get_elements_html_by_class('no-such-class', html), [])\n\n    def test_get_elements_by_attribute(self):\n        html = self.GET_ELEMENTS_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_elements_by_attribute('class', 'foo bar', html), ['nice', 'also nice'])\n        self.assertEqual(get_elements_by_attribute('class', 'foo', html), [])\n        self.assertEqual(get_elements_by_attribute('class', 'no-such-foo', html), [])\n\n    def test_get_elements_html_by_attribute(self):\n        html = self.GET_ELEMENTS_BY_CLASS_TEST_STRING\n\n        self.assertEqual(get_elements_html_by_attribute('class', 'foo bar', html), self.GET_ELEMENTS_BY_CLASS_RES)\n        self.assertEqual(get_elements_html_by_attribute('class', 'foo', html), [])\n        self.assertEqual(get_elements_html_by_attribute('class', 'no-such-foo', html), [])\n\n    def test_get_elements_text_and_html_by_attribute(self):\n        html = self.GET_ELEMENTS_BY_CLASS_TEST_STRING\n\n        self.assertEqual(\n            list(get_elements_text_and_html_by_attribute('class', 'foo bar', html)),\n            list(zip(['nice', 'also nice'], self.GET_ELEMENTS_BY_CLASS_RES)))\n        self.assertEqual(list(get_elements_text_and_html_by_attribute('class', 'foo', html)), [])\n        self.assertEqual(list(get_elements_text_and_html_by_attribute('class', 'no-such-foo', html)), [])\n\n        self.assertEqual(list(get_elements_text_and_html_by_attribute(\n            'class', 'foo', '<a class=\"foo\">nice</a><span class=\"foo\">nice</span>', tag='a')), [('nice', '<a class=\"foo\">nice</a>')])\n\n    GET_ELEMENT_BY_TAG_TEST_STRING = '''\n    random text lorem ipsum</p>\n    <div>\n        this should be returned\n        <span>this should also be returned</span>\n        <div>\n            this should also be returned\n        </div>\n        closing tag above should not trick, so this should also be returned\n    </div>\n    but this text should not be returned\n    '''\n    GET_ELEMENT_BY_TAG_RES_OUTERDIV_HTML = GET_ELEMENT_BY_TAG_TEST_STRING.strip()[32:276]\n    GET_ELEMENT_BY_TAG_RES_OUTERDIV_TEXT = GET_ELEMENT_BY_TAG_RES_OUTERDIV_HTML[5:-6]\n    GET_ELEMENT_BY_TAG_RES_INNERSPAN_HTML = GET_ELEMENT_BY_TAG_TEST_STRING.strip()[78:119]\n    GET_ELEMENT_BY_TAG_RES_INNERSPAN_TEXT = GET_ELEMENT_BY_TAG_RES_INNERSPAN_HTML[6:-7]\n\n    def test_get_element_text_and_html_by_tag(self):\n        html = self.GET_ELEMENT_BY_TAG_TEST_STRING\n\n        self.assertEqual(\n            get_element_text_and_html_by_tag('div', html),\n            (self.GET_ELEMENT_BY_TAG_RES_OUTERDIV_TEXT, self.GET_ELEMENT_BY_TAG_RES_OUTERDIV_HTML))\n        self.assertEqual(\n            get_element_text_and_html_by_tag('span', html),\n            (self.GET_ELEMENT_BY_TAG_RES_INNERSPAN_TEXT, self.GET_ELEMENT_BY_TAG_RES_INNERSPAN_HTML))\n        self.assertRaises(compat_HTMLParseError, get_element_text_and_html_by_tag, 'article', html)\n\n    def test_iri_to_uri(self):\n        self.assertEqual(\n            iri_to_uri('https://www.google.com/search?q=foo&ie=utf-8&oe=utf-8&client=firefox-b'),\n            'https://www.google.com/search?q=foo&ie=utf-8&oe=utf-8&client=firefox-b')  # Same\n        self.assertEqual(\n            iri_to_uri('https://www.google.com/search?q=K\u00e4seso\u00dfenr\u00fchrl\u00f6ffel'),  # German for cheese sauce stirring spoon\n            'https://www.google.com/search?q=K%C3%A4seso%C3%9Fenr%C3%BChrl%C3%B6ffel')\n        self.assertEqual(\n            iri_to_uri('https://www.google.com/search?q=lt<+gt>+eq%3D+amp%26+percent%25+hash%23+colon%3A+tilde~#trash=?&garbage=#'),\n            'https://www.google.com/search?q=lt%3C+gt%3E+eq%3D+amp%26+percent%25+hash%23+colon%3A+tilde~#trash=?&garbage=#')\n        self.assertEqual(\n            iri_to_uri('http://\u043f\u0440\u0430\u0432\u043e\u0437\u0430\u0449\u0438\u0442\u043038.\u0440\u0444/category/news/'),\n            'http://xn--38-6kcaak9aj5chl4a3g.xn--p1ai/category/news/')\n        self.assertEqual(\n            iri_to_uri('http://www.\u043f\u0440\u0430\u0432\u043e\u0437\u0430\u0449\u0438\u0442\u043038.\u0440\u0444/category/news/'),\n            'http://www.xn--38-6kcaak9aj5chl4a3g.xn--p1ai/category/news/')\n        self.assertEqual(\n            iri_to_uri('https://i\u2764.ws/emojidomain/\ud83d\udc4d\ud83d\udc4f\ud83e\udd1d\ud83d\udcaa'),\n            'https://xn--i-7iq.ws/emojidomain/%F0%9F%91%8D%F0%9F%91%8F%F0%9F%A4%9D%F0%9F%92%AA')\n        self.assertEqual(\n            iri_to_uri('http://\u65e5\u672c\u8a9e.jp/'),\n            'http://xn--wgv71a119e.jp/')\n        self.assertEqual(\n            iri_to_uri('http://\u5bfc\u822a.\u4e2d\u56fd/'),\n            'http://xn--fet810g.xn--fiqs8s/')\n\n    def test_clean_podcast_url(self):\n        self.assertEqual(clean_podcast_url('https://www.podtrac.com/pts/redirect.mp3/chtbl.com/track/5899E/traffic.megaphone.fm/HSW7835899191.mp3'), 'https://traffic.megaphone.fm/HSW7835899191.mp3')\n        self.assertEqual(clean_podcast_url('https://play.podtrac.com/npr-344098539/edge1.pod.npr.org/anon.npr-podcasts/podcast/npr/waitwait/2020/10/20201003_waitwait_wwdtmpodcast201003-015621a5-f035-4eca-a9a1-7c118d90bc3c.mp3'), 'https://edge1.pod.npr.org/anon.npr-podcasts/podcast/npr/waitwait/2020/10/20201003_waitwait_wwdtmpodcast201003-015621a5-f035-4eca-a9a1-7c118d90bc3c.mp3')\n        self.assertEqual(clean_podcast_url('https://pdst.fm/e/2.gum.fm/chtbl.com/track/chrt.fm/track/34D33/pscrb.fm/rss/p/traffic.megaphone.fm/ITLLC7765286967.mp3?updated=1687282661'), 'https://traffic.megaphone.fm/ITLLC7765286967.mp3?updated=1687282661')\n        self.assertEqual(clean_podcast_url('https://pdst.fm/e/https://mgln.ai/e/441/www.buzzsprout.com/1121972/13019085-ep-252-the-deep-life-stack.mp3'), 'https://www.buzzsprout.com/1121972/13019085-ep-252-the-deep-life-stack.mp3')\n\n    def test_LazyList(self):\n        it = list(range(10))\n\n        self.assertEqual(list(LazyList(it)), it)\n        self.assertEqual(LazyList(it).exhaust(), it)\n        self.assertEqual(LazyList(it)[5], it[5])\n\n        self.assertEqual(LazyList(it)[5:], it[5:])\n        self.assertEqual(LazyList(it)[:5], it[:5])\n        self.assertEqual(LazyList(it)[::2], it[::2])\n        self.assertEqual(LazyList(it)[1::2], it[1::2])\n        self.assertEqual(LazyList(it)[5::-1], it[5::-1])\n        self.assertEqual(LazyList(it)[6:2:-2], it[6:2:-2])\n        self.assertEqual(LazyList(it)[::-1], it[::-1])\n\n        self.assertTrue(LazyList(it))\n        self.assertFalse(LazyList(range(0)))\n        self.assertEqual(len(LazyList(it)), len(it))\n        self.assertEqual(repr(LazyList(it)), repr(it))\n        self.assertEqual(str(LazyList(it)), str(it))\n\n        self.assertEqual(list(LazyList(it, reverse=True)), it[::-1])\n        self.assertEqual(list(reversed(LazyList(it))[::-1]), it)\n        self.assertEqual(list(reversed(LazyList(it))[1:3:7]), it[::-1][1:3:7])\n\n    def test_LazyList_laziness(self):\n\n        def test(ll, idx, val, cache):\n            self.assertEqual(ll[idx], val)\n            self.assertEqual(ll._cache, list(cache))\n\n        ll = LazyList(range(10))\n        test(ll, 0, 0, range(1))\n        test(ll, 5, 5, range(6))\n        test(ll, -3, 7, range(10))\n\n        ll = LazyList(range(10), reverse=True)\n        test(ll, -1, 0, range(1))\n        test(ll, 3, 6, range(10))\n\n        ll = LazyList(itertools.count())\n        test(ll, 10, 10, range(11))\n        ll = reversed(ll)\n        test(ll, -15, 14, range(15))\n\n    def test_format_bytes(self):\n        self.assertEqual(format_bytes(0), '0.00B')\n        self.assertEqual(format_bytes(1000), '1000.00B')\n        self.assertEqual(format_bytes(1024), '1.00KiB')\n        self.assertEqual(format_bytes(1024**2), '1.00MiB')\n        self.assertEqual(format_bytes(1024**3), '1.00GiB')\n        self.assertEqual(format_bytes(1024**4), '1.00TiB')\n        self.assertEqual(format_bytes(1024**5), '1.00PiB')\n        self.assertEqual(format_bytes(1024**6), '1.00EiB')\n        self.assertEqual(format_bytes(1024**7), '1.00ZiB')\n        self.assertEqual(format_bytes(1024**8), '1.00YiB')\n        self.assertEqual(format_bytes(1024**9), '1024.00YiB')\n\n    def test_hide_login_info(self):\n        self.assertEqual(Config.hide_login_info(['-u', 'foo', '-p', 'bar']),\n                         ['-u', 'PRIVATE', '-p', 'PRIVATE'])\n        self.assertEqual(Config.hide_login_info(['-u']), ['-u'])\n        self.assertEqual(Config.hide_login_info(['-u', 'foo', '-u', 'bar']),\n                         ['-u', 'PRIVATE', '-u', 'PRIVATE'])\n        self.assertEqual(Config.hide_login_info(['--username=foo']),\n                         ['--username=PRIVATE'])\n\n    def test_locked_file(self):\n        TEXT = 'test_locked_file\\n'\n        FILE = 'test_locked_file.ytdl'\n        MODES = 'war'  # Order is important\n\n        try:\n            for lock_mode in MODES:\n                with locked_file(FILE, lock_mode, False) as f:\n                    if lock_mode == 'r':\n                        self.assertEqual(f.read(), TEXT * 2, 'Wrong file content')\n                    else:\n                        f.write(TEXT)\n                    for test_mode in MODES:\n                        testing_write = test_mode != 'r'\n                        try:\n                            with locked_file(FILE, test_mode, False):\n                                pass\n                        except (BlockingIOError, PermissionError):\n                            if not testing_write:  # FIXME\n                                print(f'Known issue: Exclusive lock ({lock_mode}) blocks read access ({test_mode})')\n                                continue\n                            self.assertTrue(testing_write, f'{test_mode} is blocked by {lock_mode}')\n                        else:\n                            self.assertFalse(testing_write, f'{test_mode} is not blocked by {lock_mode}')\n        finally:\n            with contextlib.suppress(OSError):\n                os.remove(FILE)\n\n    def test_determine_file_encoding(self):\n        self.assertEqual(determine_file_encoding(b''), (None, 0))\n        self.assertEqual(determine_file_encoding(b'--verbose -x --audio-format mkv\\n'), (None, 0))\n\n        self.assertEqual(determine_file_encoding(b'\\xef\\xbb\\xbf'), ('utf-8', 3))\n        self.assertEqual(determine_file_encoding(b'\\x00\\x00\\xfe\\xff'), ('utf-32-be', 4))\n        self.assertEqual(determine_file_encoding(b'\\xff\\xfe'), ('utf-16-le', 2))\n\n        self.assertEqual(determine_file_encoding(b'\\xff\\xfe# coding: utf-8\\n--verbose'), ('utf-16-le', 2))\n\n        self.assertEqual(determine_file_encoding(b'# coding: utf-8\\n--verbose'), ('utf-8', 0))\n        self.assertEqual(determine_file_encoding(b'# coding: someencodinghere-12345\\n--verbose'), ('someencodinghere-12345', 0))\n\n        self.assertEqual(determine_file_encoding(b'#coding:utf-8\\n--verbose'), ('utf-8', 0))\n        self.assertEqual(determine_file_encoding(b'#  coding:   utf-8   \\r\\n--verbose'), ('utf-8', 0))\n\n        self.assertEqual(determine_file_encoding('# coding: utf-32-be'.encode('utf-32-be')), ('utf-32-be', 0))\n        self.assertEqual(determine_file_encoding('# coding: utf-16-le'.encode('utf-16-le')), ('utf-16-le', 0))\n\n    def test_get_compatible_ext(self):\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None, None], vexts=['mp4'], aexts=['m4a', 'm4a']), 'mkv')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['flv'], aexts=['flv']), 'flv')\n\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['mp4'], aexts=['m4a']), 'mp4')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['mp4'], aexts=['webm']), 'mkv')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['webm'], aexts=['m4a']), 'mkv')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['webm'], aexts=['webm']), 'webm')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=[None], acodecs=[None], vexts=['webm'], aexts=['weba']), 'webm')\n\n        self.assertEqual(get_compatible_ext(\n            vcodecs=['h264'], acodecs=['mp4a'], vexts=['mov'], aexts=['m4a']), 'mp4')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=['av01.0.12M.08'], acodecs=['opus'], vexts=['mp4'], aexts=['webm']), 'webm')\n\n        self.assertEqual(get_compatible_ext(\n            vcodecs=['vp9'], acodecs=['opus'], vexts=['webm'], aexts=['webm'], preferences=['flv', 'mp4']), 'mp4')\n        self.assertEqual(get_compatible_ext(\n            vcodecs=['av1'], acodecs=['mp4a'], vexts=['webm'], aexts=['m4a'], preferences=('webm', 'mkv')), 'mkv')\n\n    def test_try_call(self):\n        def total(*x, **kwargs):\n            return sum(x) + sum(kwargs.values())\n\n        self.assertEqual(try_call(None), None,\n                         msg='not a fn should give None')\n        self.assertEqual(try_call(lambda: 1), 1,\n                         msg='int fn with no expected_type should give int')\n        self.assertEqual(try_call(lambda: 1, expected_type=int), 1,\n                         msg='int fn with expected_type int should give int')\n        self.assertEqual(try_call(lambda: 1, expected_type=dict), None,\n                         msg='int fn with wrong expected_type should give None')\n        self.assertEqual(try_call(total, args=(0, 1, 0, ), expected_type=int), 1,\n                         msg='fn should accept arglist')\n        self.assertEqual(try_call(total, kwargs={'a': 0, 'b': 1, 'c': 0}, expected_type=int), 1,\n                         msg='fn should accept kwargs')\n        self.assertEqual(try_call(lambda: 1, expected_type=dict), None,\n                         msg='int fn with no expected_type should give None')\n        self.assertEqual(try_call(lambda x: {}, total, args=(42, ), expected_type=int), 42,\n                         msg='expect first int result with expected_type int')\n\n    def test_variadic(self):\n        self.assertEqual(variadic(None), (None, ))\n        self.assertEqual(variadic('spam'), ('spam', ))\n        self.assertEqual(variadic('spam', allowed_types=dict), 'spam')\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            self.assertEqual(variadic('spam', allowed_types=[dict]), 'spam')\n\n    def test_traverse_obj(self):\n        _TEST_DATA = {\n            100: 100,\n            1.2: 1.2,\n            'str': 'str',\n            'None': None,\n            '...': ...,\n            'urls': [\n                {'index': 0, 'url': 'https://www.example.com/0'},\n                {'index': 1, 'url': 'https://www.example.com/1'},\n            ],\n            'data': (\n                {'index': 2},\n                {'index': 3},\n            ),\n            'dict': {},\n        }\n\n        # Test base functionality\n        self.assertEqual(traverse_obj(_TEST_DATA, ('str',)), 'str',\n                         msg='allow tuple path')\n        self.assertEqual(traverse_obj(_TEST_DATA, ['str']), 'str',\n                         msg='allow list path')\n        self.assertEqual(traverse_obj(_TEST_DATA, (value for value in (\"str\",))), 'str',\n                         msg='allow iterable path')\n        self.assertEqual(traverse_obj(_TEST_DATA, 'str'), 'str',\n                         msg='single items should be treated as a path')\n        self.assertEqual(traverse_obj(_TEST_DATA, None), _TEST_DATA)\n        self.assertEqual(traverse_obj(_TEST_DATA, 100), 100)\n        self.assertEqual(traverse_obj(_TEST_DATA, 1.2), 1.2)\n\n        # Test Ellipsis behavior\n        self.assertCountEqual(traverse_obj(_TEST_DATA, ...),\n                              (item for item in _TEST_DATA.values() if item not in (None, {})),\n                              msg='`...` should give all non discarded values')\n        self.assertCountEqual(traverse_obj(_TEST_DATA, ('urls', 0, ...)), _TEST_DATA['urls'][0].values(),\n                              msg='`...` selection for dicts should select all values')\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., ..., 'url')),\n                         ['https://www.example.com/0', 'https://www.example.com/1'],\n                         msg='nested `...` queries should work')\n        self.assertCountEqual(traverse_obj(_TEST_DATA, (..., ..., 'index')), range(4),\n                              msg='`...` query result should be flattened')\n        self.assertEqual(traverse_obj(iter(range(4)), ...), list(range(4)),\n                         msg='`...` should accept iterables')\n\n        # Test function as key\n        self.assertEqual(traverse_obj(_TEST_DATA, lambda x, y: x == 'urls' and isinstance(y, list)),\n                         [_TEST_DATA['urls']],\n                         msg='function as query key should perform a filter based on (key, value)')\n        self.assertCountEqual(traverse_obj(_TEST_DATA, lambda _, x: isinstance(x[0], str)), {'str'},\n                              msg='exceptions in the query function should be catched')\n        self.assertEqual(traverse_obj(iter(range(4)), lambda _, x: x % 2 == 0), [0, 2],\n                         msg='function key should accept iterables')\n        if __debug__:\n            with self.assertRaises(Exception, msg='Wrong function signature should raise in debug'):\n                traverse_obj(_TEST_DATA, lambda a: ...)\n            with self.assertRaises(Exception, msg='Wrong function signature should raise in debug'):\n                traverse_obj(_TEST_DATA, lambda a, b, c: ...)\n\n        # Test set as key (transformation/type, like `expected_type`)\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., {str.upper}, )), ['STR'],\n                         msg='Function in set should be a transformation')\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., {str})), ['str'],\n                         msg='Type in set should be a type filter')\n        self.assertEqual(traverse_obj(_TEST_DATA, {dict}), _TEST_DATA,\n                         msg='A single set should be wrapped into a path')\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., {str.upper})), ['STR'],\n                         msg='Transformation function should not raise')\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., {str_or_none})),\n                         [item for item in map(str_or_none, _TEST_DATA.values()) if item is not None],\n                         msg='Function in set should be a transformation')\n        if __debug__:\n            with self.assertRaises(Exception, msg='Sets with length != 1 should raise in debug'):\n                traverse_obj(_TEST_DATA, set())\n            with self.assertRaises(Exception, msg='Sets with length != 1 should raise in debug'):\n                traverse_obj(_TEST_DATA, {str.upper, str})\n\n        # Test `slice` as a key\n        _SLICE_DATA = [0, 1, 2, 3, 4]\n        self.assertEqual(traverse_obj(_TEST_DATA, ('dict', slice(1))), None,\n                         msg='slice on a dictionary should not throw')\n        self.assertEqual(traverse_obj(_SLICE_DATA, slice(1)), _SLICE_DATA[:1],\n                         msg='slice key should apply slice to sequence')\n        self.assertEqual(traverse_obj(_SLICE_DATA, slice(1, 2)), _SLICE_DATA[1:2],\n                         msg='slice key should apply slice to sequence')\n        self.assertEqual(traverse_obj(_SLICE_DATA, slice(1, 4, 2)), _SLICE_DATA[1:4:2],\n                         msg='slice key should apply slice to sequence')\n\n        # Test alternative paths\n        self.assertEqual(traverse_obj(_TEST_DATA, 'fail', 'str'), 'str',\n                         msg='multiple `paths` should be treated as alternative paths')\n        self.assertEqual(traverse_obj(_TEST_DATA, 'str', 100), 'str',\n                         msg='alternatives should exit early')\n        self.assertEqual(traverse_obj(_TEST_DATA, 'fail', 'fail'), None,\n                         msg='alternatives should return `default` if exhausted')\n        self.assertEqual(traverse_obj(_TEST_DATA, (..., 'fail'), 100), 100,\n                         msg='alternatives should track their own branching return')\n        self.assertEqual(traverse_obj(_TEST_DATA, ('dict', ...), ('data', ...)), list(_TEST_DATA['data']),\n                         msg='alternatives on empty objects should search further')\n\n        # Test branch and path nesting\n        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', (3, 0), 'url')), ['https://www.example.com/0'],\n                         msg='tuple as key should be treated as branches')\n        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', [3, 0], 'url')), ['https://www.example.com/0'],\n                         msg='list as key should be treated as branches')\n        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', ((1, 'fail'), (0, 'url')))), ['https://www.example.com/0'],\n                         msg='double nesting in path should be treated as paths')\n        self.assertEqual(traverse_obj(['0', [1, 2]], [(0, 1), 0]), [1],\n                         msg='do not fail early on branching')\n        self.assertCountEqual(traverse_obj(_TEST_DATA, ('urls', ((1, ('fail', 'url')), (0, 'url')))),\n                              ['https://www.example.com/0', 'https://www.example.com/1'],\n                              msg='tripple nesting in path should be treated as branches')\n        self.assertEqual(traverse_obj(_TEST_DATA, ('urls', ('fail', (..., 'url')))),\n                         ['https://www.example.com/0', 'https://www.example.com/1'],\n                         msg='ellipsis as branch path start gets flattened')\n\n        # Test dictionary as key\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 100, 1: 1.2}), {0: 100, 1: 1.2},\n                         msg='dict key should result in a dict with the same keys')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', 0, 'url')}),\n                         {0: 'https://www.example.com/0'},\n                         msg='dict key should allow paths')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', (3, 0), 'url')}),\n                         {0: ['https://www.example.com/0']},\n                         msg='tuple in dict path should be treated as branches')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', ((1, 'fail'), (0, 'url')))}),\n                         {0: ['https://www.example.com/0']},\n                         msg='double nesting in dict path should be treated as paths')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('urls', ((1, ('fail', 'url')), (0, 'url')))}),\n                         {0: ['https://www.example.com/1', 'https://www.example.com/0']},\n                         msg='tripple nesting in dict path should be treated as branches')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'fail'}), {},\n                         msg='remove `None` values when top level dict key fails')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'fail'}, default=...), {0: ...},\n                         msg='use `default` if key fails and `default`')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'dict'}), {},\n                         msg='remove empty values when dict key')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 'dict'}, default=...), {0: ...},\n                         msg='use `default` when dict key and `default`')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: {0: 'fail'}}), {},\n                         msg='remove empty values when nested dict key fails')\n        self.assertEqual(traverse_obj(None, {0: 'fail'}), {},\n                         msg='default to dict if pruned')\n        self.assertEqual(traverse_obj(None, {0: 'fail'}, default=...), {0: ...},\n                         msg='default to dict if pruned and default is given')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: {0: 'fail'}}, default=...), {0: {0: ...}},\n                         msg='use nested `default` when nested dict key fails and `default`')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: ('dict', ...)}), {},\n                         msg='remove key if branch in dict key not successful')\n\n        # Testing default parameter behavior\n        _DEFAULT_DATA = {'None': None, 'int': 0, 'list': []}\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'fail'), None,\n                         msg='default value should be `None`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'fail', 'fail', default=...), ...,\n                         msg='chained fails should result in default')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'None', 'int'), 0,\n                         msg='should not short cirquit on `None`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'fail', default=1), 1,\n                         msg='invalid dict key should result in `default`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, 'None', default=1), 1,\n                         msg='`None` is a deliberate sentinel and should become `default`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, ('list', 10)), None,\n                         msg='`IndexError` should result in `default`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, (..., 'fail'), default=1), 1,\n                         msg='if branched but not successful return `default` if defined, not `[]`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, (..., 'fail'), default=None), None,\n                         msg='if branched but not successful return `default` even if `default` is `None`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, (..., 'fail')), [],\n                         msg='if branched but not successful return `[]`, not `default`')\n        self.assertEqual(traverse_obj(_DEFAULT_DATA, ('list', ...)), [],\n                         msg='if branched but object is empty return `[]`, not `default`')\n        self.assertEqual(traverse_obj(None, ...), [],\n                         msg='if branched but object is `None` return `[]`, not `default`')\n        self.assertEqual(traverse_obj({0: None}, (0, ...)), [],\n                         msg='if branched but state is `None` return `[]`, not `default`')\n\n        branching_paths = [\n            ('fail', ...),\n            (..., 'fail'),\n            100 * ('fail',) + (...,),\n            (...,) + 100 * ('fail',),\n        ]\n        for branching_path in branching_paths:\n            self.assertEqual(traverse_obj({}, branching_path), [],\n                             msg='if branched but state is `None`, return `[]` (not `default`)')\n            self.assertEqual(traverse_obj({}, 'fail', branching_path), [],\n                             msg='if branching in last alternative and previous did not match, return `[]` (not `default`)')\n            self.assertEqual(traverse_obj({0: 'x'}, 0, branching_path), 'x',\n                             msg='if branching in last alternative and previous did match, return single value')\n            self.assertEqual(traverse_obj({0: 'x'}, branching_path, 0), 'x',\n                             msg='if branching in first alternative and non-branching path does match, return single value')\n            self.assertEqual(traverse_obj({}, branching_path, 'fail'), None,\n                             msg='if branching in first alternative and non-branching path does not match, return `default`')\n\n        # Testing expected_type behavior\n        _EXPECTED_TYPE_DATA = {'str': 'str', 'int': 0}\n        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=str),\n                         'str', msg='accept matching `expected_type` type')\n        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=int),\n                         None, msg='reject non matching `expected_type` type')\n        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'int', expected_type=lambda x: str(x)),\n                         '0', msg='transform type using type function')\n        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, 'str', expected_type=lambda _: 1 / 0),\n                         None, msg='wrap expected_type fuction in try_call')\n        self.assertEqual(traverse_obj(_EXPECTED_TYPE_DATA, ..., expected_type=str),\n                         ['str'], msg='eliminate items that expected_type fails on')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 100, 1: 1.2}, expected_type=int),\n                         {0: 100}, msg='type as expected_type should filter dict values')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: 100, 1: 1.2, 2: 'None'}, expected_type=str_or_none),\n                         {0: '100', 1: '1.2'}, msg='function as expected_type should transform dict values')\n        self.assertEqual(traverse_obj(_TEST_DATA, ({0: 1.2}, 0, {int_or_none}), expected_type=int),\n                         1, msg='expected_type should not filter non final dict values')\n        self.assertEqual(traverse_obj(_TEST_DATA, {0: {0: 100, 1: 'str'}}, expected_type=int),\n                         {0: {0: 100}}, msg='expected_type should transform deep dict values')\n        self.assertEqual(traverse_obj(_TEST_DATA, [({0: '...'}, {0: '...'})], expected_type=type(...)),\n                         [{0: ...}, {0: ...}], msg='expected_type should transform branched dict values')\n        self.assertEqual(traverse_obj({1: {3: 4}}, [(1, 2), 3], expected_type=int),\n                         [4], msg='expected_type regression for type matching in tuple branching')\n        self.assertEqual(traverse_obj(_TEST_DATA, ['data', ...], expected_type=int),\n                         [], msg='expected_type regression for type matching in dict result')\n\n        # Test get_all behavior\n        _GET_ALL_DATA = {'key': [0, 1, 2]}\n        self.assertEqual(traverse_obj(_GET_ALL_DATA, ('key', ...), get_all=False), 0,\n                         msg='if not `get_all`, return only first matching value')\n        self.assertEqual(traverse_obj(_GET_ALL_DATA, ..., get_all=False), [0, 1, 2],\n                         msg='do not overflatten if not `get_all`')\n\n        # Test casesense behavior\n        _CASESENSE_DATA = {\n            'KeY': 'value0',\n            0: {\n                'KeY': 'value1',\n                0: {'KeY': 'value2'},\n            },\n        }\n        self.assertEqual(traverse_obj(_CASESENSE_DATA, 'key'), None,\n                         msg='dict keys should be case sensitive unless `casesense`')\n        self.assertEqual(traverse_obj(_CASESENSE_DATA, 'keY',\n                                      casesense=False), 'value0',\n                         msg='allow non matching key case if `casesense`')\n        self.assertEqual(traverse_obj(_CASESENSE_DATA, (0, ('keY',)),\n                                      casesense=False), ['value1'],\n                         msg='allow non matching key case in branch if `casesense`')\n        self.assertEqual(traverse_obj(_CASESENSE_DATA, (0, ((0, 'keY'),)),\n                                      casesense=False), ['value2'],\n                         msg='allow non matching key case in branch path if `casesense`')\n\n        # Test traverse_string behavior\n        _TRAVERSE_STRING_DATA = {'str': 'str', 1.2: 1.2}\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', 0)), None,\n                         msg='do not traverse into string if not `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', 0),\n                                      traverse_string=True), 's',\n                         msg='traverse into string if `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, (1.2, 1),\n                                      traverse_string=True), '.',\n                         msg='traverse into converted data if `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', ...),\n                                      traverse_string=True), 'str',\n                         msg='`...` should result in string (same value) if `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', slice(0, None, 2)),\n                                      traverse_string=True), 'sr',\n                         msg='`slice` should result in string if `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', lambda i, v: i or v == \"s\"),\n                                      traverse_string=True), 'str',\n                         msg='function should result in string if `traverse_string`')\n        self.assertEqual(traverse_obj(_TRAVERSE_STRING_DATA, ('str', (0, 2)),\n                                      traverse_string=True), ['s', 'r'],\n                         msg='branching should result in list if `traverse_string`')\n        self.assertEqual(traverse_obj({}, (0, ...), traverse_string=True), [],\n                         msg='branching should result in list if `traverse_string`')\n        self.assertEqual(traverse_obj({}, (0, lambda x, y: True), traverse_string=True), [],\n                         msg='branching should result in list if `traverse_string`')\n        self.assertEqual(traverse_obj({}, (0, slice(1)), traverse_string=True), [],\n                         msg='branching should result in list if `traverse_string`')\n\n        # Test is_user_input behavior\n        _IS_USER_INPUT_DATA = {'range8': list(range(8))}\n        self.assertEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', '3'),\n                                      is_user_input=True), 3,\n                         msg='allow for string indexing if `is_user_input`')\n        self.assertCountEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', '3:'),\n                                           is_user_input=True), tuple(range(8))[3:],\n                              msg='allow for string slice if `is_user_input`')\n        self.assertCountEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', ':4:2'),\n                                           is_user_input=True), tuple(range(8))[:4:2],\n                              msg='allow step in string slice if `is_user_input`')\n        self.assertCountEqual(traverse_obj(_IS_USER_INPUT_DATA, ('range8', ':'),\n                                           is_user_input=True), range(8),\n                              msg='`:` should be treated as `...` if `is_user_input`')\n        with self.assertRaises(TypeError, msg='too many params should result in error'):\n            traverse_obj(_IS_USER_INPUT_DATA, ('range8', ':::'), is_user_input=True)\n\n        # Test re.Match as input obj\n        mobj = re.fullmatch(r'0(12)(?P<group>3)(4)?', '0123')\n        self.assertEqual(traverse_obj(mobj, ...), [x for x in mobj.groups() if x is not None],\n                         msg='`...` on a `re.Match` should give its `groups()`')\n        self.assertEqual(traverse_obj(mobj, lambda k, _: k in (0, 2)), ['0123', '3'],\n                         msg='function on a `re.Match` should give groupno, value starting at 0')\n        self.assertEqual(traverse_obj(mobj, 'group'), '3',\n                         msg='str key on a `re.Match` should give group with that name')\n        self.assertEqual(traverse_obj(mobj, 2), '3',\n                         msg='int key on a `re.Match` should give group with that name')\n        self.assertEqual(traverse_obj(mobj, 'gRoUp', casesense=False), '3',\n                         msg='str key on a `re.Match` should respect casesense')\n        self.assertEqual(traverse_obj(mobj, 'fail'), None,\n                         msg='failing str key on a `re.Match` should return `default`')\n        self.assertEqual(traverse_obj(mobj, 'gRoUpS', casesense=False), None,\n                         msg='failing str key on a `re.Match` should return `default`')\n        self.assertEqual(traverse_obj(mobj, 8), None,\n                         msg='failing int key on a `re.Match` should return `default`')\n        self.assertEqual(traverse_obj(mobj, lambda k, _: k in (0, 'group')), ['0123', '3'],\n                         msg='function on a `re.Match` should give group name as well')\n\n    def test_http_header_dict(self):\n        headers = HTTPHeaderDict()\n        headers['ytdl-test'] = b'0'\n        self.assertEqual(list(headers.items()), [('Ytdl-Test', '0')])\n        headers['ytdl-test'] = 1\n        self.assertEqual(list(headers.items()), [('Ytdl-Test', '1')])\n        headers['Ytdl-test'] = '2'\n        self.assertEqual(list(headers.items()), [('Ytdl-Test', '2')])\n        self.assertTrue('ytDl-Test' in headers)\n        self.assertEqual(str(headers), str(dict(headers)))\n        self.assertEqual(repr(headers), str(dict(headers)))\n\n        headers.update({'X-dlp': 'data'})\n        self.assertEqual(set(headers.items()), {('Ytdl-Test', '2'), ('X-Dlp', 'data')})\n        self.assertEqual(dict(headers), {'Ytdl-Test': '2', 'X-Dlp': 'data'})\n        self.assertEqual(len(headers), 2)\n        self.assertEqual(headers.copy(), headers)\n        headers2 = HTTPHeaderDict({'X-dlp': 'data3'}, **headers, **{'X-dlp': 'data2'})\n        self.assertEqual(set(headers2.items()), {('Ytdl-Test', '2'), ('X-Dlp', 'data2')})\n        self.assertEqual(len(headers2), 2)\n        headers2.clear()\n        self.assertEqual(len(headers2), 0)\n\n        # ensure we prefer latter headers\n        headers3 = HTTPHeaderDict({'Ytdl-TeSt': 1}, {'Ytdl-test': 2})\n        self.assertEqual(set(headers3.items()), {('Ytdl-Test', '2')})\n        del headers3['ytdl-tesT']\n        self.assertEqual(dict(headers3), {})\n\n        headers4 = HTTPHeaderDict({'ytdl-test': 'data;'})\n        self.assertEqual(set(headers4.items()), {('Ytdl-Test', 'data;')})\n\n    def test_extract_basic_auth(self):\n        assert extract_basic_auth('http://:foo.bar') == ('http://:foo.bar', None)\n        assert extract_basic_auth('http://foo.bar') == ('http://foo.bar', None)\n        assert extract_basic_auth('http://@foo.bar') == ('http://foo.bar', 'Basic Og==')\n        assert extract_basic_auth('http://:pass@foo.bar') == ('http://foo.bar', 'Basic OnBhc3M=')\n        assert extract_basic_auth('http://user:@foo.bar') == ('http://foo.bar', 'Basic dXNlcjo=')\n        assert extract_basic_auth('http://user:pass@foo.bar') == ('http://foo.bar', 'Basic dXNlcjpwYXNz')\n\n    @unittest.skipUnless(compat_os_name == 'nt', 'Only relevant on Windows')\n    def test_Popen_windows_escaping(self):\n        def run_shell(args):\n            stdout, stderr, error = Popen.run(\n                args, text=True, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            assert not stderr\n            assert not error\n            return stdout\n\n        # Test escaping\n        assert run_shell(['echo', 'test\"&']) == '\"test\"\"&\"\\n'\n        # Test if delayed expansion is disabled\n        assert run_shell(['echo', '^!']) == '\"^!\"\\n'\n        assert run_shell('echo \"^!\"') == '\"^!\"\\n'\n\nif __name__ == '__main__':\n    unittest.main()\n", "import os\nimport sys\nimport xml.etree.ElementTree as etree\n\nfrom .compat_utils import passthrough_module\n\npassthrough_module(__name__, '._deprecated')\ndel passthrough_module\n\n\n# HTMLParseError has been deprecated in Python 3.3 and removed in\n# Python 3.5. Introducing dummy exception for Python >3.5 for compatible\n# and uniform cross-version exception handling\nclass compat_HTMLParseError(ValueError):\n    pass\n\n\nclass _TreeBuilder(etree.TreeBuilder):\n    def doctype(self, name, pubid, system):\n        pass\n\n\ndef compat_etree_fromstring(text):\n    return etree.XML(text, parser=etree.XMLParser(target=_TreeBuilder()))\n\n\ncompat_os_name = os._name if os.name == 'java' else os.name\n\n\nif compat_os_name == 'nt':\n    def compat_shlex_quote(s):\n        import re\n        return s if re.match(r'^[-_\\w./]+$', s) else s.replace('\"', '\"\"').join('\"\"')\nelse:\n    from shlex import quote as compat_shlex_quote  # noqa: F401\n\n\ndef compat_ord(c):\n    return c if isinstance(c, int) else ord(c)\n\n\nif compat_os_name == 'nt' and sys.version_info < (3, 8):\n    # os.path.realpath on Windows does not follow symbolic links\n    # prior to Python 3.8 (see https://bugs.python.org/issue9949)\n    def compat_realpath(path):\n        while os.path.islink(path):\n            path = os.path.abspath(os.readlink(path))\n        return os.path.realpath(path)\nelse:\n    compat_realpath = os.path.realpath\n\n\n# Python 3.8+ does not honor %HOME% on windows, but this breaks compatibility with youtube-dl\n# See https://github.com/yt-dlp/yt-dlp/issues/792\n# https://docs.python.org/3/library/os.path.html#os.path.expanduser\nif compat_os_name in ('nt', 'ce'):\n    def compat_expanduser(path):\n        HOME = os.environ.get('HOME')\n        if not HOME:\n            return os.path.expanduser(path)\n        elif not path.startswith('~'):\n            return path\n        i = path.replace('\\\\', '/', 1).find('/')  # ~user\n        if i < 0:\n            i = len(path)\n        userhome = os.path.join(os.path.dirname(HOME), path[1:i]) if i > 1 else HOME\n        return userhome + path[i:]\nelse:\n    compat_expanduser = os.path.expanduser\n\n\ndef urllib_req_to_req(urllib_request):\n    \"\"\"Convert urllib Request to a networking Request\"\"\"\n    from ..networking import Request\n    from ..utils.networking import HTTPHeaderDict\n    return Request(\n        urllib_request.get_full_url(), data=urllib_request.data, method=urllib_request.get_method(),\n        headers=HTTPHeaderDict(urllib_request.headers, urllib_request.unredirected_hdrs),\n        extensions={'timeout': urllib_request.timeout} if hasattr(urllib_request, 'timeout') else None)\n", "from .common import PostProcessor\nfrom ..compat import compat_shlex_quote\nfrom ..utils import Popen, PostProcessingError, variadic\n\n\nclass ExecPP(PostProcessor):\n\n    def __init__(self, downloader, exec_cmd):\n        PostProcessor.__init__(self, downloader)\n        self.exec_cmd = variadic(exec_cmd)\n\n    def parse_cmd(self, cmd, info):\n        tmpl, tmpl_dict = self._downloader.prepare_outtmpl(cmd, info)\n        if tmpl_dict:  # if there are no replacements, tmpl_dict = {}\n            return self._downloader.escape_outtmpl(tmpl) % tmpl_dict\n\n        filepath = info.get('filepath', info.get('_filename'))\n        # If video, and no replacements are found, replace {} for backard compatibility\n        if filepath:\n            if '{}' not in cmd:\n                cmd += ' {}'\n            cmd = cmd.replace('{}', compat_shlex_quote(filepath))\n        return cmd\n\n    def run(self, info):\n        for tmpl in self.exec_cmd:\n            cmd = self.parse_cmd(tmpl, info)\n            self.to_screen(f'Executing command: {cmd}')\n            _, _, return_code = Popen.run(cmd, shell=True)\n            if return_code != 0:\n                raise PostProcessingError(f'Command returned error code {return_code}')\n        return [], info\n\n\n# Deprecated\nclass ExecAfterDownloadPP(ExecPP):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.deprecation_warning(\n            'yt_dlp.postprocessor.ExecAfterDownloadPP is deprecated '\n            'and may be removed in a future version. Use yt_dlp.postprocessor.ExecPP instead')\n", "import asyncio\nimport atexit\nimport base64\nimport binascii\nimport calendar\nimport codecs\nimport collections\nimport collections.abc\nimport contextlib\nimport datetime\nimport email.header\nimport email.utils\nimport errno\nimport hashlib\nimport hmac\nimport html.entities\nimport html.parser\nimport inspect\nimport io\nimport itertools\nimport json\nimport locale\nimport math\nimport mimetypes\nimport netrc\nimport operator\nimport os\nimport platform\nimport random\nimport re\nimport shlex\nimport socket\nimport ssl\nimport struct\nimport subprocess\nimport sys\nimport tempfile\nimport time\nimport traceback\nimport types\nimport unicodedata\nimport urllib.error\nimport urllib.parse\nimport urllib.request\nimport xml.etree.ElementTree\n\nfrom . import traversal\n\nfrom ..compat import functools  # isort: split\nfrom ..compat import (\n    compat_etree_fromstring,\n    compat_expanduser,\n    compat_HTMLParseError,\n    compat_os_name,\n    compat_shlex_quote,\n)\nfrom ..dependencies import websockets, xattr\n\n__name__ = __name__.rsplit('.', 1)[0]  # Pretend to be the parent module\n\n# This is not clearly defined otherwise\ncompiled_regex_type = type(re.compile(''))\n\n\nclass NO_DEFAULT:\n    pass\n\n\ndef IDENTITY(x):\n    return x\n\n\nENGLISH_MONTH_NAMES = [\n    'January', 'February', 'March', 'April', 'May', 'June',\n    'July', 'August', 'September', 'October', 'November', 'December']\n\nMONTH_NAMES = {\n    'en': ENGLISH_MONTH_NAMES,\n    'fr': [\n        'janvier', 'f\u00e9vrier', 'mars', 'avril', 'mai', 'juin',\n        'juillet', 'ao\u00fbt', 'septembre', 'octobre', 'novembre', 'd\u00e9cembre'],\n    # these follow the genitive grammatical case (dope\u0142niacz)\n    # some websites might be using nominative, which will require another month list\n    # https://en.wikibooks.org/wiki/Polish/Noun_cases\n    'pl': ['stycznia', 'lutego', 'marca', 'kwietnia', 'maja', 'czerwca',\n           'lipca', 'sierpnia', 'wrze\u015bnia', 'pa\u017adziernika', 'listopada', 'grudnia'],\n}\n\n# From https://github.com/python/cpython/blob/3.11/Lib/email/_parseaddr.py#L36-L42\nTIMEZONE_NAMES = {\n    'UT': 0, 'UTC': 0, 'GMT': 0, 'Z': 0,\n    'AST': -4, 'ADT': -3,  # Atlantic (used in Canada)\n    'EST': -5, 'EDT': -4,  # Eastern\n    'CST': -6, 'CDT': -5,  # Central\n    'MST': -7, 'MDT': -6,  # Mountain\n    'PST': -8, 'PDT': -7   # Pacific\n}\n\n# needed for sanitizing filenames in restricted mode\nACCENT_CHARS = dict(zip('\u00c2\u00c3\u00c4\u00c0\u00c1\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u0150\u00d8\u0152\u00d9\u00da\u00db\u00dc\u0170\u00dd\u00de\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u0151\u00f8\u0153\u00f9\u00fa\u00fb\u00fc\u0171\u00fd\u00fe\u00ff',\n                        itertools.chain('AAAAAA', ['AE'], 'CEEEEIIIIDNOOOOOOO', ['OE'], 'UUUUUY', ['TH', 'ss'],\n                                        'aaaaaa', ['ae'], 'ceeeeiiiionooooooo', ['oe'], 'uuuuuy', ['th'], 'y')))\n\nDATE_FORMATS = (\n    '%d %B %Y',\n    '%d %b %Y',\n    '%B %d %Y',\n    '%B %dst %Y',\n    '%B %dnd %Y',\n    '%B %drd %Y',\n    '%B %dth %Y',\n    '%b %d %Y',\n    '%b %dst %Y',\n    '%b %dnd %Y',\n    '%b %drd %Y',\n    '%b %dth %Y',\n    '%b %dst %Y %I:%M',\n    '%b %dnd %Y %I:%M',\n    '%b %drd %Y %I:%M',\n    '%b %dth %Y %I:%M',\n    '%Y %m %d',\n    '%Y-%m-%d',\n    '%Y.%m.%d.',\n    '%Y/%m/%d',\n    '%Y/%m/%d %H:%M',\n    '%Y/%m/%d %H:%M:%S',\n    '%Y%m%d%H%M',\n    '%Y%m%d%H%M%S',\n    '%Y%m%d',\n    '%Y-%m-%d %H:%M',\n    '%Y-%m-%d %H:%M:%S',\n    '%Y-%m-%d %H:%M:%S.%f',\n    '%Y-%m-%d %H:%M:%S:%f',\n    '%d.%m.%Y %H:%M',\n    '%d.%m.%Y %H.%M',\n    '%Y-%m-%dT%H:%M:%SZ',\n    '%Y-%m-%dT%H:%M:%S.%fZ',\n    '%Y-%m-%dT%H:%M:%S.%f0Z',\n    '%Y-%m-%dT%H:%M:%S',\n    '%Y-%m-%dT%H:%M:%S.%f',\n    '%Y-%m-%dT%H:%M',\n    '%b %d %Y at %H:%M',\n    '%b %d %Y at %H:%M:%S',\n    '%B %d %Y at %H:%M',\n    '%B %d %Y at %H:%M:%S',\n    '%H:%M %d-%b-%Y',\n)\n\nDATE_FORMATS_DAY_FIRST = list(DATE_FORMATS)\nDATE_FORMATS_DAY_FIRST.extend([\n    '%d-%m-%Y',\n    '%d.%m.%Y',\n    '%d.%m.%y',\n    '%d/%m/%Y',\n    '%d/%m/%y',\n    '%d/%m/%Y %H:%M:%S',\n    '%d-%m-%Y %H:%M',\n    '%H:%M %d/%m/%Y',\n])\n\nDATE_FORMATS_MONTH_FIRST = list(DATE_FORMATS)\nDATE_FORMATS_MONTH_FIRST.extend([\n    '%m-%d-%Y',\n    '%m.%d.%Y',\n    '%m/%d/%Y',\n    '%m/%d/%y',\n    '%m/%d/%Y %H:%M:%S',\n])\n\nPACKED_CODES_RE = r\"}\\('(.+)',(\\d+),(\\d+),'([^']+)'\\.split\\('\\|'\\)\"\nJSON_LD_RE = r'(?is)<script[^>]+type=([\"\\']?)application/ld\\+json\\1[^>]*>\\s*(?P<json_ld>{.+?}|\\[.+?\\])\\s*</script>'\n\nNUMBER_RE = r'\\d+(?:\\.\\d+)?'\n\n\n@functools.cache\ndef preferredencoding():\n    \"\"\"Get preferred encoding.\n\n    Returns the best encoding scheme for the system, based on\n    locale.getpreferredencoding() and some further tweaks.\n    \"\"\"\n    try:\n        pref = locale.getpreferredencoding()\n        'TEST'.encode(pref)\n    except Exception:\n        pref = 'UTF-8'\n\n    return pref\n\n\ndef write_json_file(obj, fn):\n    \"\"\" Encode obj as JSON and write it to fn, atomically if possible \"\"\"\n\n    tf = tempfile.NamedTemporaryFile(\n        prefix=f'{os.path.basename(fn)}.', dir=os.path.dirname(fn),\n        suffix='.tmp', delete=False, mode='w', encoding='utf-8')\n\n    try:\n        with tf:\n            json.dump(obj, tf, ensure_ascii=False)\n        if sys.platform == 'win32':\n            # Need to remove existing file on Windows, else os.rename raises\n            # WindowsError or FileExistsError.\n            with contextlib.suppress(OSError):\n                os.unlink(fn)\n        with contextlib.suppress(OSError):\n            mask = os.umask(0)\n            os.umask(mask)\n            os.chmod(tf.name, 0o666 & ~mask)\n        os.rename(tf.name, fn)\n    except Exception:\n        with contextlib.suppress(OSError):\n            os.remove(tf.name)\n        raise\n\n\ndef find_xpath_attr(node, xpath, key, val=None):\n    \"\"\" Find the xpath xpath[@key=val] \"\"\"\n    assert re.match(r'^[a-zA-Z_-]+$', key)\n    expr = xpath + ('[@%s]' % key if val is None else f\"[@{key}='{val}']\")\n    return node.find(expr)\n\n# On python2.6 the xml.etree.ElementTree.Element methods don't support\n# the namespace parameter\n\n\ndef xpath_with_ns(path, ns_map):\n    components = [c.split(':') for c in path.split('/')]\n    replaced = []\n    for c in components:\n        if len(c) == 1:\n            replaced.append(c[0])\n        else:\n            ns, tag = c\n            replaced.append('{%s}%s' % (ns_map[ns], tag))\n    return '/'.join(replaced)\n\n\ndef xpath_element(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    def _find_xpath(xpath):\n        return node.find(xpath)\n\n    if isinstance(xpath, str):\n        n = _find_xpath(xpath)\n    else:\n        for xp in xpath:\n            n = _find_xpath(xp)\n            if n is not None:\n                break\n\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element %s' % name)\n        else:\n            return None\n    return n\n\n\ndef xpath_text(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    n = xpath_element(node, xpath, name, fatal=fatal, default=default)\n    if n is None or n == default:\n        return n\n    if n.text is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element\\'s text %s' % name)\n        else:\n            return None\n    return n.text\n\n\ndef xpath_attr(node, xpath, key, name=None, fatal=False, default=NO_DEFAULT):\n    n = find_xpath_attr(node, xpath, key)\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = f'{xpath}[@{key}]' if name is None else name\n            raise ExtractorError('Could not find XML attribute %s' % name)\n        else:\n            return None\n    return n.attrib[key]\n\n\ndef get_element_by_id(id, html, **kwargs):\n    \"\"\"Return the content of the tag with the specified ID in the passed HTML document\"\"\"\n    return get_element_by_attribute('id', id, html, **kwargs)\n\n\ndef get_element_html_by_id(id, html, **kwargs):\n    \"\"\"Return the html of the tag with the specified ID in the passed HTML document\"\"\"\n    return get_element_html_by_attribute('id', id, html, **kwargs)\n\n\ndef get_element_by_class(class_name, html):\n    \"\"\"Return the content of the first tag with the specified class in the passed HTML document\"\"\"\n    retval = get_elements_by_class(class_name, html)\n    return retval[0] if retval else None\n\n\ndef get_element_html_by_class(class_name, html):\n    \"\"\"Return the html of the first tag with the specified class in the passed HTML document\"\"\"\n    retval = get_elements_html_by_class(class_name, html)\n    return retval[0] if retval else None\n\n\ndef get_element_by_attribute(attribute, value, html, **kwargs):\n    retval = get_elements_by_attribute(attribute, value, html, **kwargs)\n    return retval[0] if retval else None\n\n\ndef get_element_html_by_attribute(attribute, value, html, **kargs):\n    retval = get_elements_html_by_attribute(attribute, value, html, **kargs)\n    return retval[0] if retval else None\n\n\ndef get_elements_by_class(class_name, html, **kargs):\n    \"\"\"Return the content of all tags with the specified class in the passed HTML document as a list\"\"\"\n    return get_elements_by_attribute(\n        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),\n        html, escape_value=False)\n\n\ndef get_elements_html_by_class(class_name, html):\n    \"\"\"Return the html of all tags with the specified class in the passed HTML document as a list\"\"\"\n    return get_elements_html_by_attribute(\n        'class', r'[^\\'\"]*(?<=[\\'\"\\s])%s(?=[\\'\"\\s])[^\\'\"]*' % re.escape(class_name),\n        html, escape_value=False)\n\n\ndef get_elements_by_attribute(*args, **kwargs):\n    \"\"\"Return the content of the tag with the specified attribute in the passed HTML document\"\"\"\n    return [content for content, _ in get_elements_text_and_html_by_attribute(*args, **kwargs)]\n\n\ndef get_elements_html_by_attribute(*args, **kwargs):\n    \"\"\"Return the html of the tag with the specified attribute in the passed HTML document\"\"\"\n    return [whole for _, whole in get_elements_text_and_html_by_attribute(*args, **kwargs)]\n\n\ndef get_elements_text_and_html_by_attribute(attribute, value, html, *, tag=r'[\\w:.-]+', escape_value=True):\n    \"\"\"\n    Return the text (content) and the html (whole) of the tag with the specified\n    attribute in the passed HTML document\n    \"\"\"\n    if not value:\n        return\n\n    quote = '' if re.match(r'''[\\s\"'`=<>]''', value) else '?'\n\n    value = re.escape(value) if escape_value else value\n\n    partial_element_re = rf'''(?x)\n        <(?P<tag>{tag})\n         (?:\\s(?:[^>\"']|\"[^\"]*\"|'[^']*')*)?\n         \\s{re.escape(attribute)}\\s*=\\s*(?P<_q>['\"]{quote})(?-x:{value})(?P=_q)\n        '''\n\n    for m in re.finditer(partial_element_re, html):\n        content, whole = get_element_text_and_html_by_tag(m.group('tag'), html[m.start():])\n\n        yield (\n            unescapeHTML(re.sub(r'^(?P<q>[\"\\'])(?P<content>.*)(?P=q)$', r'\\g<content>', content, flags=re.DOTALL)),\n            whole\n        )\n\n\nclass HTMLBreakOnClosingTagParser(html.parser.HTMLParser):\n    \"\"\"\n    HTML parser which raises HTMLBreakOnClosingTagException upon reaching the\n    closing tag for the first opening tag it has encountered, and can be used\n    as a context manager\n    \"\"\"\n\n    class HTMLBreakOnClosingTagException(Exception):\n        pass\n\n    def __init__(self):\n        self.tagstack = collections.deque()\n        html.parser.HTMLParser.__init__(self)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *_):\n        self.close()\n\n    def close(self):\n        # handle_endtag does not return upon raising HTMLBreakOnClosingTagException,\n        # so data remains buffered; we no longer have any interest in it, thus\n        # override this method to discard it\n        pass\n\n    def handle_starttag(self, tag, _):\n        self.tagstack.append(tag)\n\n    def handle_endtag(self, tag):\n        if not self.tagstack:\n            raise compat_HTMLParseError('no tags in the stack')\n        while self.tagstack:\n            inner_tag = self.tagstack.pop()\n            if inner_tag == tag:\n                break\n        else:\n            raise compat_HTMLParseError(f'matching opening tag for closing {tag} tag not found')\n        if not self.tagstack:\n            raise self.HTMLBreakOnClosingTagException()\n\n\n# XXX: This should be far less strict\ndef get_element_text_and_html_by_tag(tag, html):\n    \"\"\"\n    For the first element with the specified tag in the passed HTML document\n    return its' content (text) and the whole element (html)\n    \"\"\"\n    def find_or_raise(haystack, needle, exc):\n        try:\n            return haystack.index(needle)\n        except ValueError:\n            raise exc\n    closing_tag = f'</{tag}>'\n    whole_start = find_or_raise(\n        html, f'<{tag}', compat_HTMLParseError(f'opening {tag} tag not found'))\n    content_start = find_or_raise(\n        html[whole_start:], '>', compat_HTMLParseError(f'malformed opening {tag} tag'))\n    content_start += whole_start + 1\n    with HTMLBreakOnClosingTagParser() as parser:\n        parser.feed(html[whole_start:content_start])\n        if not parser.tagstack or parser.tagstack[0] != tag:\n            raise compat_HTMLParseError(f'parser did not match opening {tag} tag')\n        offset = content_start\n        while offset < len(html):\n            next_closing_tag_start = find_or_raise(\n                html[offset:], closing_tag,\n                compat_HTMLParseError(f'closing {tag} tag not found'))\n            next_closing_tag_end = next_closing_tag_start + len(closing_tag)\n            try:\n                parser.feed(html[offset:offset + next_closing_tag_end])\n                offset += next_closing_tag_end\n            except HTMLBreakOnClosingTagParser.HTMLBreakOnClosingTagException:\n                return html[content_start:offset + next_closing_tag_start], \\\n                    html[whole_start:offset + next_closing_tag_end]\n        raise compat_HTMLParseError('unexpected end of html')\n\n\nclass HTMLAttributeParser(html.parser.HTMLParser):\n    \"\"\"Trivial HTML parser to gather the attributes for a single element\"\"\"\n\n    def __init__(self):\n        self.attrs = {}\n        html.parser.HTMLParser.__init__(self)\n\n    def handle_starttag(self, tag, attrs):\n        self.attrs = dict(attrs)\n        raise compat_HTMLParseError('done')\n\n\nclass HTMLListAttrsParser(html.parser.HTMLParser):\n    \"\"\"HTML parser to gather the attributes for the elements of a list\"\"\"\n\n    def __init__(self):\n        html.parser.HTMLParser.__init__(self)\n        self.items = []\n        self._level = 0\n\n    def handle_starttag(self, tag, attrs):\n        if tag == 'li' and self._level == 0:\n            self.items.append(dict(attrs))\n        self._level += 1\n\n    def handle_endtag(self, tag):\n        self._level -= 1\n\n\ndef extract_attributes(html_element):\n    \"\"\"Given a string for an HTML element such as\n    <el\n         a=\"foo\" B=\"bar\" c=\"&98;az\" d=boz\n         empty= noval entity=\"&amp;\"\n         sq='\"' dq=\"'\"\n    >\n    Decode and return a dictionary of attributes.\n    {\n        'a': 'foo', 'b': 'bar', c: 'baz', d: 'boz',\n        'empty': '', 'noval': None, 'entity': '&',\n        'sq': '\"', 'dq': '\\''\n    }.\n    \"\"\"\n    parser = HTMLAttributeParser()\n    with contextlib.suppress(compat_HTMLParseError):\n        parser.feed(html_element)\n        parser.close()\n    return parser.attrs\n\n\ndef parse_list(webpage):\n    \"\"\"Given a string for an series of HTML <li> elements,\n    return a dictionary of their attributes\"\"\"\n    parser = HTMLListAttrsParser()\n    parser.feed(webpage)\n    parser.close()\n    return parser.items\n\n\ndef clean_html(html):\n    \"\"\"Clean an HTML snippet into a readable string\"\"\"\n\n    if html is None:  # Convenience for sanitizing descriptions etc.\n        return html\n\n    html = re.sub(r'\\s+', ' ', html)\n    html = re.sub(r'(?u)\\s?<\\s?br\\s?/?\\s?>\\s?', '\\n', html)\n    html = re.sub(r'(?u)<\\s?/\\s?p\\s?>\\s?<\\s?p[^>]*>', '\\n', html)\n    # Strip html tags\n    html = re.sub('<.*?>', '', html)\n    # Replace html entities\n    html = unescapeHTML(html)\n    return html.strip()\n\n\nclass LenientJSONDecoder(json.JSONDecoder):\n    # TODO: Write tests\n    def __init__(self, *args, transform_source=None, ignore_extra=False, close_objects=0, **kwargs):\n        self.transform_source, self.ignore_extra = transform_source, ignore_extra\n        self._close_attempts = 2 * close_objects\n        super().__init__(*args, **kwargs)\n\n    @staticmethod\n    def _close_object(err):\n        doc = err.doc[:err.pos]\n        # We need to add comma first to get the correct error message\n        if err.msg.startswith('Expecting \\',\\''):\n            return doc + ','\n        elif not doc.endswith(','):\n            return\n\n        if err.msg.startswith('Expecting property name'):\n            return doc[:-1] + '}'\n        elif err.msg.startswith('Expecting value'):\n            return doc[:-1] + ']'\n\n    def decode(self, s):\n        if self.transform_source:\n            s = self.transform_source(s)\n        for attempt in range(self._close_attempts + 1):\n            try:\n                if self.ignore_extra:\n                    return self.raw_decode(s.lstrip())[0]\n                return super().decode(s)\n            except json.JSONDecodeError as e:\n                if e.pos is None:\n                    raise\n                elif attempt < self._close_attempts:\n                    s = self._close_object(e)\n                    if s is not None:\n                        continue\n                raise type(e)(f'{e.msg} in {s[e.pos-10:e.pos+10]!r}', s, e.pos)\n        assert False, 'Too many attempts to decode JSON'\n\n\ndef sanitize_open(filename, open_mode):\n    \"\"\"Try to open the given filename, and slightly tweak it if this fails.\n\n    Attempts to open the given filename. If this fails, it tries to change\n    the filename slightly, step by step, until it's either able to open it\n    or it fails and raises a final exception, like the standard open()\n    function.\n\n    It returns the tuple (stream, definitive_file_name).\n    \"\"\"\n    if filename == '-':\n        if sys.platform == 'win32':\n            import msvcrt\n\n            # stdout may be any IO stream, e.g. when using contextlib.redirect_stdout\n            with contextlib.suppress(io.UnsupportedOperation):\n                msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n        return (sys.stdout.buffer if hasattr(sys.stdout, 'buffer') else sys.stdout, filename)\n\n    for attempt in range(2):\n        try:\n            try:\n                if sys.platform == 'win32':\n                    # FIXME: An exclusive lock also locks the file from being read.\n                    # Since windows locks are mandatory, don't lock the file on windows (for now).\n                    # Ref: https://github.com/yt-dlp/yt-dlp/issues/3124\n                    raise LockingUnsupportedError()\n                stream = locked_file(filename, open_mode, block=False).__enter__()\n            except OSError:\n                stream = open(filename, open_mode)\n            return stream, filename\n        except OSError as err:\n            if attempt or err.errno in (errno.EACCES,):\n                raise\n            old_filename, filename = filename, sanitize_path(filename)\n            if old_filename == filename:\n                raise\n\n\ndef timeconvert(timestr):\n    \"\"\"Convert RFC 2822 defined time string into system timestamp\"\"\"\n    timestamp = None\n    timetuple = email.utils.parsedate_tz(timestr)\n    if timetuple is not None:\n        timestamp = email.utils.mktime_tz(timetuple)\n    return timestamp\n\n\ndef sanitize_filename(s, restricted=False, is_id=NO_DEFAULT):\n    \"\"\"Sanitizes a string so it could be used as part of a filename.\n    @param restricted   Use a stricter subset of allowed characters\n    @param is_id        Whether this is an ID that should be kept unchanged if possible.\n                        If unset, yt-dlp's new sanitization rules are in effect\n    \"\"\"\n    if s == '':\n        return ''\n\n    def replace_insane(char):\n        if restricted and char in ACCENT_CHARS:\n            return ACCENT_CHARS[char]\n        elif not restricted and char == '\\n':\n            return '\\0 '\n        elif is_id is NO_DEFAULT and not restricted and char in '\"*:<>?|/\\\\':\n            # Replace with their full-width unicode counterparts\n            return {'/': '\\u29F8', '\\\\': '\\u29f9'}.get(char, chr(ord(char) + 0xfee0))\n        elif char == '?' or ord(char) < 32 or ord(char) == 127:\n            return ''\n        elif char == '\"':\n            return '' if restricted else '\\''\n        elif char == ':':\n            return '\\0_\\0-' if restricted else '\\0 \\0-'\n        elif char in '\\\\/|*<>':\n            return '\\0_'\n        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace() or ord(char) > 127):\n            return '\\0_'\n        return char\n\n    # Replace look-alike Unicode glyphs\n    if restricted and (is_id is NO_DEFAULT or not is_id):\n        s = unicodedata.normalize('NFKC', s)\n    s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s)  # Handle timestamps\n    result = ''.join(map(replace_insane, s))\n    if is_id is NO_DEFAULT:\n        result = re.sub(r'(\\0.)(?:(?=\\1)..)+', r'\\1', result)  # Remove repeated substitute chars\n        STRIP_RE = r'(?:\\0.|[ _-])*'\n        result = re.sub(f'^\\0.{STRIP_RE}|{STRIP_RE}\\0.$', '', result)  # Remove substitute chars from start/end\n    result = result.replace('\\0', '') or '_'\n\n    if not is_id:\n        while '__' in result:\n            result = result.replace('__', '_')\n        result = result.strip('_')\n        # Common case of \"Foreign band name - English song title\"\n        if restricted and result.startswith('-_'):\n            result = result[2:]\n        if result.startswith('-'):\n            result = '_' + result[len('-'):]\n        result = result.lstrip('.')\n        if not result:\n            result = '_'\n    return result\n\n\ndef sanitize_path(s, force=False):\n    \"\"\"Sanitizes and normalizes path on Windows\"\"\"\n    # XXX: this handles drive relative paths (c:sth) incorrectly\n    if sys.platform == 'win32':\n        force = False\n        drive_or_unc, _ = os.path.splitdrive(s)\n    elif force:\n        drive_or_unc = ''\n    else:\n        return s\n\n    norm_path = os.path.normpath(remove_start(s, drive_or_unc)).split(os.path.sep)\n    if drive_or_unc:\n        norm_path.pop(0)\n    sanitized_path = [\n        path_part if path_part in ['.', '..'] else re.sub(r'(?:[/<>:\"\\|\\\\?\\*]|[\\s.]$)', '#', path_part)\n        for path_part in norm_path]\n    if drive_or_unc:\n        sanitized_path.insert(0, drive_or_unc + os.path.sep)\n    elif force and s and s[0] == os.path.sep:\n        sanitized_path.insert(0, os.path.sep)\n    # TODO: Fix behavioral differences <3.12\n    # The workaround using `normpath` only superficially passes tests\n    # Ref: https://github.com/python/cpython/pull/100351\n    return os.path.normpath(os.path.join(*sanitized_path))\n\n\ndef sanitize_url(url, *, scheme='http'):\n    # Prepend protocol-less URLs with `http:` scheme in order to mitigate\n    # the number of unwanted failures due to missing protocol\n    if url is None:\n        return\n    elif url.startswith('//'):\n        return f'{scheme}:{url}'\n    # Fix some common typos seen so far\n    COMMON_TYPOS = (\n        # https://github.com/ytdl-org/youtube-dl/issues/15649\n        (r'^httpss://', r'https://'),\n        # https://bx1.be/lives/direct-tv/\n        (r'^rmtp([es]?)://', r'rtmp\\1://'),\n    )\n    for mistake, fixup in COMMON_TYPOS:\n        if re.match(mistake, url):\n            return re.sub(mistake, fixup, url)\n    return url\n\n\ndef extract_basic_auth(url):\n    parts = urllib.parse.urlsplit(url)\n    if parts.username is None:\n        return url, None\n    url = urllib.parse.urlunsplit(parts._replace(netloc=(\n        parts.hostname if parts.port is None\n        else '%s:%d' % (parts.hostname, parts.port))))\n    auth_payload = base64.b64encode(\n        ('%s:%s' % (parts.username, parts.password or '')).encode())\n    return url, f'Basic {auth_payload.decode()}'\n\n\ndef expand_path(s):\n    \"\"\"Expand shell variables and ~\"\"\"\n    return os.path.expandvars(compat_expanduser(s))\n\n\ndef orderedSet(iterable, *, lazy=False):\n    \"\"\"Remove all duplicates from the input iterable\"\"\"\n    def _iter():\n        seen = []  # Do not use set since the items can be unhashable\n        for x in iterable:\n            if x not in seen:\n                seen.append(x)\n                yield x\n\n    return _iter() if lazy else list(_iter())\n\n\ndef _htmlentity_transform(entity_with_semicolon):\n    \"\"\"Transforms an HTML entity to a character.\"\"\"\n    entity = entity_with_semicolon[:-1]\n\n    # Known non-numeric HTML entity\n    if entity in html.entities.name2codepoint:\n        return chr(html.entities.name2codepoint[entity])\n\n    # TODO: HTML5 allows entities without a semicolon.\n    # E.g. '&Eacuteric' should be decoded as '\u00c9ric'.\n    if entity_with_semicolon in html.entities.html5:\n        return html.entities.html5[entity_with_semicolon]\n\n    mobj = re.match(r'#(x[0-9a-fA-F]+|[0-9]+)', entity)\n    if mobj is not None:\n        numstr = mobj.group(1)\n        if numstr.startswith('x'):\n            base = 16\n            numstr = '0%s' % numstr\n        else:\n            base = 10\n        # See https://github.com/ytdl-org/youtube-dl/issues/7518\n        with contextlib.suppress(ValueError):\n            return chr(int(numstr, base))\n\n    # Unknown entity in name, return its literal representation\n    return '&%s;' % entity\n\n\ndef unescapeHTML(s):\n    if s is None:\n        return None\n    assert isinstance(s, str)\n\n    return re.sub(\n        r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)\n\n\ndef escapeHTML(text):\n    return (\n        text\n        .replace('&', '&amp;')\n        .replace('<', '&lt;')\n        .replace('>', '&gt;')\n        .replace('\"', '&quot;')\n        .replace(\"'\", '&#39;')\n    )\n\n\nclass netrc_from_content(netrc.netrc):\n    def __init__(self, content):\n        self.hosts, self.macros = {}, {}\n        with io.StringIO(content) as stream:\n            self._parse('-', stream, False)\n\n\nclass Popen(subprocess.Popen):\n    if sys.platform == 'win32':\n        _startupinfo = subprocess.STARTUPINFO()\n        _startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n    else:\n        _startupinfo = None\n\n    @staticmethod\n    def _fix_pyinstaller_ld_path(env):\n        \"\"\"Restore LD_LIBRARY_PATH when using PyInstaller\n            Ref: https://github.com/pyinstaller/pyinstaller/blob/develop/doc/runtime-information.rst#ld_library_path--libpath-considerations\n                 https://github.com/yt-dlp/yt-dlp/issues/4573\n        \"\"\"\n        if not hasattr(sys, '_MEIPASS'):\n            return\n\n        def _fix(key):\n            orig = env.get(f'{key}_ORIG')\n            if orig is None:\n                env.pop(key, None)\n            else:\n                env[key] = orig\n\n        _fix('LD_LIBRARY_PATH')  # Linux\n        _fix('DYLD_LIBRARY_PATH')  # macOS\n\n    def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs):\n        if env is None:\n            env = os.environ.copy()\n        self._fix_pyinstaller_ld_path(env)\n\n        self.__text_mode = kwargs.get('encoding') or kwargs.get('errors') or text or kwargs.get('universal_newlines')\n        if text is True:\n            kwargs['universal_newlines'] = True  # For 3.6 compatibility\n            kwargs.setdefault('encoding', 'utf-8')\n            kwargs.setdefault('errors', 'replace')\n\n        if shell and compat_os_name == 'nt' and kwargs.get('executable') is None:\n            if not isinstance(args, str):\n                args = ' '.join(compat_shlex_quote(a) for a in args)\n            shell = False\n            args = f'{self.__comspec()} /Q /S /D /V:OFF /C \"{args}\"'\n\n        super().__init__(args, *remaining, env=env, shell=shell, **kwargs, startupinfo=self._startupinfo)\n\n    def __comspec(self):\n        comspec = os.environ.get('ComSpec') or os.path.join(\n            os.environ.get('SystemRoot', ''), 'System32', 'cmd.exe')\n        if os.path.isabs(comspec):\n            return comspec\n        raise FileNotFoundError('shell not found: neither %ComSpec% nor %SystemRoot% is set')\n\n    def communicate_or_kill(self, *args, **kwargs):\n        try:\n            return self.communicate(*args, **kwargs)\n        except BaseException:  # Including KeyboardInterrupt\n            self.kill(timeout=None)\n            raise\n\n    def kill(self, *, timeout=0):\n        super().kill()\n        if timeout != 0:\n            self.wait(timeout=timeout)\n\n    @classmethod\n    def run(cls, *args, timeout=None, **kwargs):\n        with cls(*args, **kwargs) as proc:\n            default = '' if proc.__text_mode else b''\n            stdout, stderr = proc.communicate_or_kill(timeout=timeout)\n            return stdout or default, stderr or default, proc.returncode\n\n\ndef encodeArgument(s):\n    # Legacy code that uses byte strings\n    # Uncomment the following line after fixing all post processors\n    # assert isinstance(s, str), 'Internal error: %r should be of type %r, is %r' % (s, str, type(s))\n    return s if isinstance(s, str) else s.decode('ascii')\n\n\n_timetuple = collections.namedtuple('Time', ('hours', 'minutes', 'seconds', 'milliseconds'))\n\n\ndef timetuple_from_msec(msec):\n    secs, msec = divmod(msec, 1000)\n    mins, secs = divmod(secs, 60)\n    hrs, mins = divmod(mins, 60)\n    return _timetuple(hrs, mins, secs, msec)\n\n\ndef formatSeconds(secs, delim=':', msec=False):\n    time = timetuple_from_msec(secs * 1000)\n    if time.hours:\n        ret = '%d%s%02d%s%02d' % (time.hours, delim, time.minutes, delim, time.seconds)\n    elif time.minutes:\n        ret = '%d%s%02d' % (time.minutes, delim, time.seconds)\n    else:\n        ret = '%d' % time.seconds\n    return '%s.%03d' % (ret, time.milliseconds) if msec else ret\n\n\ndef bug_reports_message(before=';'):\n    from ..update import REPOSITORY\n\n    msg = (f'please report this issue on  https://github.com/{REPOSITORY}/issues?q= , '\n           'filling out the appropriate issue template. Confirm you are on the latest version using  yt-dlp -U')\n\n    before = before.rstrip()\n    if not before or before.endswith(('.', '!', '?')):\n        msg = msg[0].title() + msg[1:]\n\n    return (before + ' ' if before else '') + msg\n\n\nclass YoutubeDLError(Exception):\n    \"\"\"Base exception for YoutubeDL errors.\"\"\"\n    msg = None\n\n    def __init__(self, msg=None):\n        if msg is not None:\n            self.msg = msg\n        elif self.msg is None:\n            self.msg = type(self).__name__\n        super().__init__(self.msg)\n\n\nclass ExtractorError(YoutubeDLError):\n    \"\"\"Error during info extraction.\"\"\"\n\n    def __init__(self, msg, tb=None, expected=False, cause=None, video_id=None, ie=None):\n        \"\"\" tb, if given, is the original traceback (so that it can be printed out).\n        If expected is set, this is a normal error message and most likely not a bug in yt-dlp.\n        \"\"\"\n        from ..networking.exceptions import network_exceptions\n        if sys.exc_info()[0] in network_exceptions:\n            expected = True\n\n        self.orig_msg = str(msg)\n        self.traceback = tb\n        self.expected = expected\n        self.cause = cause\n        self.video_id = video_id\n        self.ie = ie\n        self.exc_info = sys.exc_info()  # preserve original exception\n        if isinstance(self.exc_info[1], ExtractorError):\n            self.exc_info = self.exc_info[1].exc_info\n        super().__init__(self.__msg)\n\n    @property\n    def __msg(self):\n        return ''.join((\n            format_field(self.ie, None, '[%s] '),\n            format_field(self.video_id, None, '%s: '),\n            self.orig_msg,\n            format_field(self.cause, None, ' (caused by %r)'),\n            '' if self.expected else bug_reports_message()))\n\n    def format_traceback(self):\n        return join_nonempty(\n            self.traceback and ''.join(traceback.format_tb(self.traceback)),\n            self.cause and ''.join(traceback.format_exception(None, self.cause, self.cause.__traceback__)[1:]),\n            delim='\\n') or None\n\n    def __setattr__(self, name, value):\n        super().__setattr__(name, value)\n        if getattr(self, 'msg', None) and name not in ('msg', 'args'):\n            self.msg = self.__msg or type(self).__name__\n            self.args = (self.msg, )  # Cannot be property\n\n\nclass UnsupportedError(ExtractorError):\n    def __init__(self, url):\n        super().__init__(\n            'Unsupported URL: %s' % url, expected=True)\n        self.url = url\n\n\nclass RegexNotFoundError(ExtractorError):\n    \"\"\"Error when a regex didn't match\"\"\"\n    pass\n\n\nclass GeoRestrictedError(ExtractorError):\n    \"\"\"Geographic restriction Error exception.\n\n    This exception may be thrown when a video is not available from your\n    geographic location due to geographic restrictions imposed by a website.\n    \"\"\"\n\n    def __init__(self, msg, countries=None, **kwargs):\n        kwargs['expected'] = True\n        super().__init__(msg, **kwargs)\n        self.countries = countries\n\n\nclass UserNotLive(ExtractorError):\n    \"\"\"Error when a channel/user is not live\"\"\"\n\n    def __init__(self, msg=None, **kwargs):\n        kwargs['expected'] = True\n        super().__init__(msg or 'The channel is not currently live', **kwargs)\n\n\nclass DownloadError(YoutubeDLError):\n    \"\"\"Download Error exception.\n\n    This exception may be thrown by FileDownloader objects if they are not\n    configured to continue on errors. They will contain the appropriate\n    error message.\n    \"\"\"\n\n    def __init__(self, msg, exc_info=None):\n        \"\"\" exc_info, if given, is the original exception that caused the trouble (as returned by sys.exc_info()). \"\"\"\n        super().__init__(msg)\n        self.exc_info = exc_info\n\n\nclass EntryNotInPlaylist(YoutubeDLError):\n    \"\"\"Entry not in playlist exception.\n\n    This exception will be thrown by YoutubeDL when a requested entry\n    is not found in the playlist info_dict\n    \"\"\"\n    msg = 'Entry not found in info'\n\n\nclass SameFileError(YoutubeDLError):\n    \"\"\"Same File exception.\n\n    This exception will be thrown by FileDownloader objects if they detect\n    multiple files would have to be downloaded to the same file on disk.\n    \"\"\"\n    msg = 'Fixed output name but more than one file to download'\n\n    def __init__(self, filename=None):\n        if filename is not None:\n            self.msg += f': {filename}'\n        super().__init__(self.msg)\n\n\nclass PostProcessingError(YoutubeDLError):\n    \"\"\"Post Processing exception.\n\n    This exception may be raised by PostProcessor's .run() method to\n    indicate an error in the postprocessing task.\n    \"\"\"\n\n\nclass DownloadCancelled(YoutubeDLError):\n    \"\"\" Exception raised when the download queue should be interrupted \"\"\"\n    msg = 'The download was cancelled'\n\n\nclass ExistingVideoReached(DownloadCancelled):\n    \"\"\" --break-on-existing triggered \"\"\"\n    msg = 'Encountered a video that is already in the archive, stopping due to --break-on-existing'\n\n\nclass RejectedVideoReached(DownloadCancelled):\n    \"\"\" --break-match-filter triggered \"\"\"\n    msg = 'Encountered a video that did not match filter, stopping due to --break-match-filter'\n\n\nclass MaxDownloadsReached(DownloadCancelled):\n    \"\"\" --max-downloads limit has been reached. \"\"\"\n    msg = 'Maximum number of downloads reached, stopping due to --max-downloads'\n\n\nclass ReExtractInfo(YoutubeDLError):\n    \"\"\" Video info needs to be re-extracted. \"\"\"\n\n    def __init__(self, msg, expected=False):\n        super().__init__(msg)\n        self.expected = expected\n\n\nclass ThrottledDownload(ReExtractInfo):\n    \"\"\" Download speed below --throttled-rate. \"\"\"\n    msg = 'The download speed is below throttle limit'\n\n    def __init__(self):\n        super().__init__(self.msg, expected=False)\n\n\nclass UnavailableVideoError(YoutubeDLError):\n    \"\"\"Unavailable Format exception.\n\n    This exception will be thrown when a video is requested\n    in a format that is not available for that video.\n    \"\"\"\n    msg = 'Unable to download video'\n\n    def __init__(self, err=None):\n        if err is not None:\n            self.msg += f': {err}'\n        super().__init__(self.msg)\n\n\nclass ContentTooShortError(YoutubeDLError):\n    \"\"\"Content Too Short exception.\n\n    This exception may be raised by FileDownloader objects when a file they\n    download is too small for what the server announced first, indicating\n    the connection was probably interrupted.\n    \"\"\"\n\n    def __init__(self, downloaded, expected):\n        super().__init__(f'Downloaded {downloaded} bytes, expected {expected} bytes')\n        # Both in bytes\n        self.downloaded = downloaded\n        self.expected = expected\n\n\nclass XAttrMetadataError(YoutubeDLError):\n    def __init__(self, code=None, msg='Unknown error'):\n        super().__init__(msg)\n        self.code = code\n        self.msg = msg\n\n        # Parsing code and msg\n        if (self.code in (errno.ENOSPC, errno.EDQUOT)\n                or 'No space left' in self.msg or 'Disk quota exceeded' in self.msg):\n            self.reason = 'NO_SPACE'\n        elif self.code == errno.E2BIG or 'Argument list too long' in self.msg:\n            self.reason = 'VALUE_TOO_LONG'\n        else:\n            self.reason = 'NOT_SUPPORTED'\n\n\nclass XAttrUnavailableError(YoutubeDLError):\n    pass\n\n\ndef is_path_like(f):\n    return isinstance(f, (str, bytes, os.PathLike))\n\n\ndef extract_timezone(date_str):\n    m = re.search(\n        r'''(?x)\n            ^.{8,}?                                              # >=8 char non-TZ prefix, if present\n            (?P<tz>Z|                                            # just the UTC Z, or\n                (?:(?<=.\\b\\d{4}|\\b\\d{2}:\\d\\d)|                   # preceded by 4 digits or hh:mm or\n                   (?<!.\\b[a-zA-Z]{3}|[a-zA-Z]{4}|..\\b\\d\\d))     # not preceded by 3 alpha word or >= 4 alpha or 2 digits\n                   [ ]?                                          # optional space\n                (?P<sign>\\+|-)                                   # +/-\n                (?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})       # hh[:]mm\n            $)\n        ''', date_str)\n    if not m:\n        m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n        timezone = TIMEZONE_NAMES.get(m and m.group('tz').strip())\n        if timezone is not None:\n            date_str = date_str[:-len(m.group('tz'))]\n        timezone = datetime.timedelta(hours=timezone or 0)\n    else:\n        date_str = date_str[:-len(m.group('tz'))]\n        if not m.group('sign'):\n            timezone = datetime.timedelta()\n        else:\n            sign = 1 if m.group('sign') == '+' else -1\n            timezone = datetime.timedelta(\n                hours=sign * int(m.group('hours')),\n                minutes=sign * int(m.group('minutes')))\n    return timezone, date_str\n\n\ndef parse_iso8601(date_str, delimiter='T', timezone=None):\n    \"\"\" Return a UNIX timestamp from the given date \"\"\"\n\n    if date_str is None:\n        return None\n\n    date_str = re.sub(r'\\.[0-9]+', '', date_str)\n\n    if timezone is None:\n        timezone, date_str = extract_timezone(date_str)\n\n    with contextlib.suppress(ValueError):\n        date_format = f'%Y-%m-%d{delimiter}%H:%M:%S'\n        dt = datetime.datetime.strptime(date_str, date_format) - timezone\n        return calendar.timegm(dt.timetuple())\n\n\ndef date_formats(day_first=True):\n    return DATE_FORMATS_DAY_FIRST if day_first else DATE_FORMATS_MONTH_FIRST\n\n\ndef unified_strdate(date_str, day_first=True):\n    \"\"\"Return a string with the date in the format YYYYMMDD\"\"\"\n\n    if date_str is None:\n        return None\n    upload_date = None\n    # Replace commas\n    date_str = date_str.replace(',', ' ')\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n    _, date_str = extract_timezone(date_str)\n\n    for expression in date_formats(day_first):\n        with contextlib.suppress(ValueError):\n            upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n    if upload_date is None:\n        timetuple = email.utils.parsedate_tz(date_str)\n        if timetuple:\n            with contextlib.suppress(ValueError):\n                upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    if upload_date is not None:\n        return str(upload_date)\n\n\ndef unified_timestamp(date_str, day_first=True):\n    if not isinstance(date_str, str):\n        return None\n\n    date_str = re.sub(r'\\s+', ' ', re.sub(\n        r'(?i)[,|]|(mon|tues?|wed(nes)?|thu(rs)?|fri|sat(ur)?)(day)?', '', date_str))\n\n    pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0\n    timezone, date_str = extract_timezone(date_str)\n\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n\n    # Remove unrecognized timezones from ISO 8601 alike timestamps\n    m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n    if m:\n        date_str = date_str[:-len(m.group('tz'))]\n\n    # Python only supports microseconds, so remove nanoseconds\n    m = re.search(r'^([0-9]{4,}-[0-9]{1,2}-[0-9]{1,2}T[0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\\.[0-9]{6})[0-9]+$', date_str)\n    if m:\n        date_str = m.group(1)\n\n    for expression in date_formats(day_first):\n        with contextlib.suppress(ValueError):\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n            return calendar.timegm(dt.timetuple())\n\n    timetuple = email.utils.parsedate_tz(date_str)\n    if timetuple:\n        return calendar.timegm(timetuple) + pm_delta * 3600 - timezone.total_seconds()\n\n\ndef determine_ext(url, default_ext='unknown_video'):\n    if url is None or '.' not in url:\n        return default_ext\n    guess = url.partition('?')[0].rpartition('.')[2]\n    if re.match(r'^[A-Za-z0-9]+$', guess):\n        return guess\n    # Try extract ext from URLs like http://example.com/foo/bar.mp4/?download\n    elif guess.rstrip('/') in KNOWN_EXTENSIONS:\n        return guess.rstrip('/')\n    else:\n        return default_ext\n\n\ndef subtitles_filename(filename, sub_lang, sub_format, expected_real_ext=None):\n    return replace_extension(filename, sub_lang + '.' + sub_format, expected_real_ext)\n\n\ndef datetime_from_str(date_str, precision='auto', format='%Y%m%d'):\n    R\"\"\"\n    Return a datetime object from a string.\n    Supported format:\n        (now|today|yesterday|DATE)([+-]\\d+(microsecond|second|minute|hour|day|week|month|year)s?)?\n\n    @param format       strftime format of DATE\n    @param precision    Round the datetime object: auto|microsecond|second|minute|hour|day\n                        auto: round to the unit provided in date_str (if applicable).\n    \"\"\"\n    auto_precision = False\n    if precision == 'auto':\n        auto_precision = True\n        precision = 'microsecond'\n    today = datetime_round(datetime.datetime.now(datetime.timezone.utc), precision)\n    if date_str in ('now', 'today'):\n        return today\n    if date_str == 'yesterday':\n        return today - datetime.timedelta(days=1)\n    match = re.match(\n        r'(?P<start>.+)(?P<sign>[+-])(?P<time>\\d+)(?P<unit>microsecond|second|minute|hour|day|week|month|year)s?',\n        date_str)\n    if match is not None:\n        start_time = datetime_from_str(match.group('start'), precision, format)\n        time = int(match.group('time')) * (-1 if match.group('sign') == '-' else 1)\n        unit = match.group('unit')\n        if unit == 'month' or unit == 'year':\n            new_date = datetime_add_months(start_time, time * 12 if unit == 'year' else time)\n            unit = 'day'\n        else:\n            if unit == 'week':\n                unit = 'day'\n                time *= 7\n            delta = datetime.timedelta(**{unit + 's': time})\n            new_date = start_time + delta\n        if auto_precision:\n            return datetime_round(new_date, unit)\n        return new_date\n\n    return datetime_round(datetime.datetime.strptime(date_str, format), precision)\n\n\ndef date_from_str(date_str, format='%Y%m%d', strict=False):\n    R\"\"\"\n    Return a date object from a string using datetime_from_str\n\n    @param strict  Restrict allowed patterns to \"YYYYMMDD\" and\n                   (now|today|yesterday)(-\\d+(day|week|month|year)s?)?\n    \"\"\"\n    if strict and not re.fullmatch(r'\\d{8}|(now|today|yesterday)(-\\d+(day|week|month|year)s?)?', date_str):\n        raise ValueError(f'Invalid date format \"{date_str}\"')\n    return datetime_from_str(date_str, precision='microsecond', format=format).date()\n\n\ndef datetime_add_months(dt, months):\n    \"\"\"Increment/Decrement a datetime object by months.\"\"\"\n    month = dt.month + months - 1\n    year = dt.year + month // 12\n    month = month % 12 + 1\n    day = min(dt.day, calendar.monthrange(year, month)[1])\n    return dt.replace(year, month, day)\n\n\ndef datetime_round(dt, precision='day'):\n    \"\"\"\n    Round a datetime object's time to a specific precision\n    \"\"\"\n    if precision == 'microsecond':\n        return dt\n\n    unit_seconds = {\n        'day': 86400,\n        'hour': 3600,\n        'minute': 60,\n        'second': 1,\n    }\n    roundto = lambda x, n: ((x + n / 2) // n) * n\n    timestamp = roundto(calendar.timegm(dt.timetuple()), unit_seconds[precision])\n    return datetime.datetime.fromtimestamp(timestamp, datetime.timezone.utc)\n\n\ndef hyphenate_date(date_str):\n    \"\"\"\n    Convert a date in 'YYYYMMDD' format to 'YYYY-MM-DD' format\"\"\"\n    match = re.match(r'^(\\d\\d\\d\\d)(\\d\\d)(\\d\\d)$', date_str)\n    if match is not None:\n        return '-'.join(match.groups())\n    else:\n        return date_str\n\n\nclass DateRange:\n    \"\"\"Represents a time interval between two dates\"\"\"\n\n    def __init__(self, start=None, end=None):\n        \"\"\"start and end must be strings in the format accepted by date\"\"\"\n        if start is not None:\n            self.start = date_from_str(start, strict=True)\n        else:\n            self.start = datetime.datetime.min.date()\n        if end is not None:\n            self.end = date_from_str(end, strict=True)\n        else:\n            self.end = datetime.datetime.max.date()\n        if self.start > self.end:\n            raise ValueError('Date range: \"%s\" , the start date must be before the end date' % self)\n\n    @classmethod\n    def day(cls, day):\n        \"\"\"Returns a range that only contains the given day\"\"\"\n        return cls(day, day)\n\n    def __contains__(self, date):\n        \"\"\"Check if the date is in the range\"\"\"\n        if not isinstance(date, datetime.date):\n            date = date_from_str(date)\n        return self.start <= date <= self.end\n\n    def __repr__(self):\n        return f'{__name__}.{type(self).__name__}({self.start.isoformat()!r}, {self.end.isoformat()!r})'\n\n    def __eq__(self, other):\n        return (isinstance(other, DateRange)\n                and self.start == other.start and self.end == other.end)\n\n\n@functools.cache\ndef system_identifier():\n    python_implementation = platform.python_implementation()\n    if python_implementation == 'PyPy' and hasattr(sys, 'pypy_version_info'):\n        python_implementation += ' version %d.%d.%d' % sys.pypy_version_info[:3]\n    libc_ver = []\n    with contextlib.suppress(OSError):  # We may not have access to the executable\n        libc_ver = platform.libc_ver()\n\n    return 'Python %s (%s %s %s) - %s (%s%s)' % (\n        platform.python_version(),\n        python_implementation,\n        platform.machine(),\n        platform.architecture()[0],\n        platform.platform(),\n        ssl.OPENSSL_VERSION,\n        format_field(join_nonempty(*libc_ver, delim=' '), None, ', %s'),\n    )\n\n\n@functools.cache\ndef get_windows_version():\n    ''' Get Windows version. returns () if it's not running on Windows '''\n    if compat_os_name == 'nt':\n        return version_tuple(platform.win32_ver()[1])\n    else:\n        return ()\n\n\ndef write_string(s, out=None, encoding=None):\n    assert isinstance(s, str)\n    out = out or sys.stderr\n    # `sys.stderr` might be `None` (Ref: https://github.com/pyinstaller/pyinstaller/pull/7217)\n    if not out:\n        return\n\n    if compat_os_name == 'nt' and supports_terminal_sequences(out):\n        s = re.sub(r'([\\r\\n]+)', r' \\1', s)\n\n    enc, buffer = None, out\n    if 'b' in getattr(out, 'mode', ''):\n        enc = encoding or preferredencoding()\n    elif hasattr(out, 'buffer'):\n        buffer = out.buffer\n        enc = encoding or getattr(out, 'encoding', None) or preferredencoding()\n\n    buffer.write(s.encode(enc, 'ignore') if enc else s)\n    out.flush()\n\n\n# TODO: Use global logger\ndef deprecation_warning(msg, *, printer=None, stacklevel=0, **kwargs):\n    from .. import _IN_CLI\n    if _IN_CLI:\n        if msg in deprecation_warning._cache:\n            return\n        deprecation_warning._cache.add(msg)\n        if printer:\n            return printer(f'{msg}{bug_reports_message()}', **kwargs)\n        return write_string(f'ERROR: {msg}{bug_reports_message()}\\n', **kwargs)\n    else:\n        import warnings\n        warnings.warn(DeprecationWarning(msg), stacklevel=stacklevel + 3)\n\n\ndeprecation_warning._cache = set()\n\n\ndef bytes_to_intlist(bs):\n    if not bs:\n        return []\n    if isinstance(bs[0], int):  # Python 3\n        return list(bs)\n    else:\n        return [ord(c) for c in bs]\n\n\ndef intlist_to_bytes(xs):\n    if not xs:\n        return b''\n    return struct.pack('%dB' % len(xs), *xs)\n\n\nclass LockingUnsupportedError(OSError):\n    msg = 'File locking is not supported'\n\n    def __init__(self):\n        super().__init__(self.msg)\n\n\n# Cross-platform file locking\nif sys.platform == 'win32':\n    import ctypes\n    import ctypes.wintypes\n    import msvcrt\n\n    class OVERLAPPED(ctypes.Structure):\n        _fields_ = [\n            ('Internal', ctypes.wintypes.LPVOID),\n            ('InternalHigh', ctypes.wintypes.LPVOID),\n            ('Offset', ctypes.wintypes.DWORD),\n            ('OffsetHigh', ctypes.wintypes.DWORD),\n            ('hEvent', ctypes.wintypes.HANDLE),\n        ]\n\n    kernel32 = ctypes.WinDLL('kernel32')\n    LockFileEx = kernel32.LockFileEx\n    LockFileEx.argtypes = [\n        ctypes.wintypes.HANDLE,     # hFile\n        ctypes.wintypes.DWORD,      # dwFlags\n        ctypes.wintypes.DWORD,      # dwReserved\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh\n        ctypes.POINTER(OVERLAPPED)  # Overlapped\n    ]\n    LockFileEx.restype = ctypes.wintypes.BOOL\n    UnlockFileEx = kernel32.UnlockFileEx\n    UnlockFileEx.argtypes = [\n        ctypes.wintypes.HANDLE,     # hFile\n        ctypes.wintypes.DWORD,      # dwReserved\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockLow\n        ctypes.wintypes.DWORD,      # nNumberOfBytesToLockHigh\n        ctypes.POINTER(OVERLAPPED)  # Overlapped\n    ]\n    UnlockFileEx.restype = ctypes.wintypes.BOOL\n    whole_low = 0xffffffff\n    whole_high = 0x7fffffff\n\n    def _lock_file(f, exclusive, block):\n        overlapped = OVERLAPPED()\n        overlapped.Offset = 0\n        overlapped.OffsetHigh = 0\n        overlapped.hEvent = 0\n        f._lock_file_overlapped_p = ctypes.pointer(overlapped)\n\n        if not LockFileEx(msvcrt.get_osfhandle(f.fileno()),\n                          (0x2 if exclusive else 0x0) | (0x0 if block else 0x1),\n                          0, whole_low, whole_high, f._lock_file_overlapped_p):\n            # NB: No argument form of \"ctypes.FormatError\" does not work on PyPy\n            raise BlockingIOError(f'Locking file failed: {ctypes.FormatError(ctypes.GetLastError())!r}')\n\n    def _unlock_file(f):\n        assert f._lock_file_overlapped_p\n        handle = msvcrt.get_osfhandle(f.fileno())\n        if not UnlockFileEx(handle, 0, whole_low, whole_high, f._lock_file_overlapped_p):\n            raise OSError('Unlocking file failed: %r' % ctypes.FormatError())\n\nelse:\n    try:\n        import fcntl\n\n        def _lock_file(f, exclusive, block):\n            flags = fcntl.LOCK_EX if exclusive else fcntl.LOCK_SH\n            if not block:\n                flags |= fcntl.LOCK_NB\n            try:\n                fcntl.flock(f, flags)\n            except BlockingIOError:\n                raise\n            except OSError:  # AOSP does not have flock()\n                fcntl.lockf(f, flags)\n\n        def _unlock_file(f):\n            with contextlib.suppress(OSError):\n                return fcntl.flock(f, fcntl.LOCK_UN)\n            with contextlib.suppress(OSError):\n                return fcntl.lockf(f, fcntl.LOCK_UN)  # AOSP does not have flock()\n            return fcntl.flock(f, fcntl.LOCK_UN | fcntl.LOCK_NB)  # virtiofs needs LOCK_NB on unlocking\n\n    except ImportError:\n\n        def _lock_file(f, exclusive, block):\n            raise LockingUnsupportedError()\n\n        def _unlock_file(f):\n            raise LockingUnsupportedError()\n\n\nclass locked_file:\n    locked = False\n\n    def __init__(self, filename, mode, block=True, encoding=None):\n        if mode not in {'r', 'rb', 'a', 'ab', 'w', 'wb'}:\n            raise NotImplementedError(mode)\n        self.mode, self.block = mode, block\n\n        writable = any(f in mode for f in 'wax+')\n        readable = any(f in mode for f in 'r+')\n        flags = functools.reduce(operator.ior, (\n            getattr(os, 'O_CLOEXEC', 0),  # UNIX only\n            getattr(os, 'O_BINARY', 0),  # Windows only\n            getattr(os, 'O_NOINHERIT', 0),  # Windows only\n            os.O_CREAT if writable else 0,  # O_TRUNC only after locking\n            os.O_APPEND if 'a' in mode else 0,\n            os.O_EXCL if 'x' in mode else 0,\n            os.O_RDONLY if not writable else os.O_RDWR if readable else os.O_WRONLY,\n        ))\n\n        self.f = os.fdopen(os.open(filename, flags, 0o666), mode, encoding=encoding)\n\n    def __enter__(self):\n        exclusive = 'r' not in self.mode\n        try:\n            _lock_file(self.f, exclusive, self.block)\n            self.locked = True\n        except OSError:\n            self.f.close()\n            raise\n        if 'w' in self.mode:\n            try:\n                self.f.truncate()\n            except OSError as e:\n                if e.errno not in (\n                    errno.ESPIPE,  # Illegal seek - expected for FIFO\n                    errno.EINVAL,  # Invalid argument - expected for /dev/null\n                ):\n                    raise\n        return self\n\n    def unlock(self):\n        if not self.locked:\n            return\n        try:\n            _unlock_file(self.f)\n        finally:\n            self.locked = False\n\n    def __exit__(self, *_):\n        try:\n            self.unlock()\n        finally:\n            self.f.close()\n\n    open = __enter__\n    close = __exit__\n\n    def __getattr__(self, attr):\n        return getattr(self.f, attr)\n\n    def __iter__(self):\n        return iter(self.f)\n\n\n@functools.cache\ndef get_filesystem_encoding():\n    encoding = sys.getfilesystemencoding()\n    return encoding if encoding is not None else 'utf-8'\n\n\ndef shell_quote(args):\n    quoted_args = []\n    encoding = get_filesystem_encoding()\n    for a in args:\n        if isinstance(a, bytes):\n            # We may get a filename encoded with 'encodeFilename'\n            a = a.decode(encoding)\n        quoted_args.append(compat_shlex_quote(a))\n    return ' '.join(quoted_args)\n\n\ndef smuggle_url(url, data):\n    \"\"\" Pass additional data in a URL for internal use. \"\"\"\n\n    url, idata = unsmuggle_url(url, {})\n    data.update(idata)\n    sdata = urllib.parse.urlencode(\n        {'__youtubedl_smuggle': json.dumps(data)})\n    return url + '#' + sdata\n\n\ndef unsmuggle_url(smug_url, default=None):\n    if '#__youtubedl_smuggle' not in smug_url:\n        return smug_url, default\n    url, _, sdata = smug_url.rpartition('#')\n    jsond = urllib.parse.parse_qs(sdata)['__youtubedl_smuggle'][0]\n    data = json.loads(jsond)\n    return url, data\n\n\ndef format_decimal_suffix(num, fmt='%d%s', *, factor=1000):\n    \"\"\" Formats numbers with decimal sufixes like K, M, etc \"\"\"\n    num, factor = float_or_none(num), float(factor)\n    if num is None or num < 0:\n        return None\n    POSSIBLE_SUFFIXES = 'kMGTPEZY'\n    exponent = 0 if num == 0 else min(int(math.log(num, factor)), len(POSSIBLE_SUFFIXES))\n    suffix = ['', *POSSIBLE_SUFFIXES][exponent]\n    if factor == 1024:\n        suffix = {'k': 'Ki', '': ''}.get(suffix, f'{suffix}i')\n    converted = num / (factor ** exponent)\n    return fmt % (converted, suffix)\n\n\ndef format_bytes(bytes):\n    return format_decimal_suffix(bytes, '%.2f%sB', factor=1024) or 'N/A'\n\n\ndef lookup_unit_table(unit_table, s, strict=False):\n    num_re = NUMBER_RE if strict else NUMBER_RE.replace(R'\\.', '[,.]')\n    units_re = '|'.join(re.escape(u) for u in unit_table)\n    m = (re.fullmatch if strict else re.match)(\n        rf'(?P<num>{num_re})\\s*(?P<unit>{units_re})\\b', s)\n    if not m:\n        return None\n\n    num = float(m.group('num').replace(',', '.'))\n    mult = unit_table[m.group('unit')]\n    return round(num * mult)\n\n\ndef parse_bytes(s):\n    \"\"\"Parse a string indicating a byte quantity into an integer\"\"\"\n    return lookup_unit_table(\n        {u: 1024**i for i, u in enumerate(['', *'KMGTPEZY'])},\n        s.upper(), strict=True)\n\n\ndef parse_filesize(s):\n    if s is None:\n        return None\n\n    # The lower-case forms are of course incorrect and unofficial,\n    # but we support those too\n    _UNIT_TABLE = {\n        'B': 1,\n        'b': 1,\n        'bytes': 1,\n        'KiB': 1024,\n        'KB': 1000,\n        'kB': 1024,\n        'Kb': 1000,\n        'kb': 1000,\n        'kilobytes': 1000,\n        'kibibytes': 1024,\n        'MiB': 1024 ** 2,\n        'MB': 1000 ** 2,\n        'mB': 1024 ** 2,\n        'Mb': 1000 ** 2,\n        'mb': 1000 ** 2,\n        'megabytes': 1000 ** 2,\n        'mebibytes': 1024 ** 2,\n        'GiB': 1024 ** 3,\n        'GB': 1000 ** 3,\n        'gB': 1024 ** 3,\n        'Gb': 1000 ** 3,\n        'gb': 1000 ** 3,\n        'gigabytes': 1000 ** 3,\n        'gibibytes': 1024 ** 3,\n        'TiB': 1024 ** 4,\n        'TB': 1000 ** 4,\n        'tB': 1024 ** 4,\n        'Tb': 1000 ** 4,\n        'tb': 1000 ** 4,\n        'terabytes': 1000 ** 4,\n        'tebibytes': 1024 ** 4,\n        'PiB': 1024 ** 5,\n        'PB': 1000 ** 5,\n        'pB': 1024 ** 5,\n        'Pb': 1000 ** 5,\n        'pb': 1000 ** 5,\n        'petabytes': 1000 ** 5,\n        'pebibytes': 1024 ** 5,\n        'EiB': 1024 ** 6,\n        'EB': 1000 ** 6,\n        'eB': 1024 ** 6,\n        'Eb': 1000 ** 6,\n        'eb': 1000 ** 6,\n        'exabytes': 1000 ** 6,\n        'exbibytes': 1024 ** 6,\n        'ZiB': 1024 ** 7,\n        'ZB': 1000 ** 7,\n        'zB': 1024 ** 7,\n        'Zb': 1000 ** 7,\n        'zb': 1000 ** 7,\n        'zettabytes': 1000 ** 7,\n        'zebibytes': 1024 ** 7,\n        'YiB': 1024 ** 8,\n        'YB': 1000 ** 8,\n        'yB': 1024 ** 8,\n        'Yb': 1000 ** 8,\n        'yb': 1000 ** 8,\n        'yottabytes': 1000 ** 8,\n        'yobibytes': 1024 ** 8,\n    }\n\n    return lookup_unit_table(_UNIT_TABLE, s)\n\n\ndef parse_count(s):\n    if s is None:\n        return None\n\n    s = re.sub(r'^[^\\d]+\\s', '', s).strip()\n\n    if re.match(r'^[\\d,.]+$', s):\n        return str_to_int(s)\n\n    _UNIT_TABLE = {\n        'k': 1000,\n        'K': 1000,\n        'm': 1000 ** 2,\n        'M': 1000 ** 2,\n        'kk': 1000 ** 2,\n        'KK': 1000 ** 2,\n        'b': 1000 ** 3,\n        'B': 1000 ** 3,\n    }\n\n    ret = lookup_unit_table(_UNIT_TABLE, s)\n    if ret is not None:\n        return ret\n\n    mobj = re.match(r'([\\d,.]+)(?:$|\\s)', s)\n    if mobj:\n        return str_to_int(mobj.group(1))\n\n\ndef parse_resolution(s, *, lenient=False):\n    if s is None:\n        return {}\n\n    if lenient:\n        mobj = re.search(r'(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)', s)\n    else:\n        mobj = re.search(r'(?<![a-zA-Z0-9])(?P<w>\\d+)\\s*[xX\u00d7,]\\s*(?P<h>\\d+)(?![a-zA-Z0-9])', s)\n    if mobj:\n        return {\n            'width': int(mobj.group('w')),\n            'height': int(mobj.group('h')),\n        }\n\n    mobj = re.search(r'(?<![a-zA-Z0-9])(\\d+)[pPiI](?![a-zA-Z0-9])', s)\n    if mobj:\n        return {'height': int(mobj.group(1))}\n\n    mobj = re.search(r'\\b([48])[kK]\\b', s)\n    if mobj:\n        return {'height': int(mobj.group(1)) * 540}\n\n    return {}\n\n\ndef parse_bitrate(s):\n    if not isinstance(s, str):\n        return\n    mobj = re.search(r'\\b(\\d+)\\s*kbps', s)\n    if mobj:\n        return int(mobj.group(1))\n\n\ndef month_by_name(name, lang='en'):\n    \"\"\" Return the number of a month by (locale-independently) English name \"\"\"\n\n    month_names = MONTH_NAMES.get(lang, MONTH_NAMES['en'])\n\n    try:\n        return month_names.index(name) + 1\n    except ValueError:\n        return None\n\n\ndef month_by_abbreviation(abbrev):\n    \"\"\" Return the number of a month by (locale-independently) English\n        abbreviations \"\"\"\n\n    try:\n        return [s[:3] for s in ENGLISH_MONTH_NAMES].index(abbrev) + 1\n    except ValueError:\n        return None\n\n\ndef fix_xml_ampersands(xml_str):\n    \"\"\"Replace all the '&' by '&amp;' in XML\"\"\"\n    return re.sub(\n        r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',\n        '&amp;',\n        xml_str)\n\n\ndef setproctitle(title):\n    assert isinstance(title, str)\n\n    # Workaround for https://github.com/yt-dlp/yt-dlp/issues/4541\n    try:\n        import ctypes\n    except ImportError:\n        return\n\n    try:\n        libc = ctypes.cdll.LoadLibrary('libc.so.6')\n    except OSError:\n        return\n    except TypeError:\n        # LoadLibrary in Windows Python 2.7.13 only expects\n        # a bytestring, but since unicode_literals turns\n        # every string into a unicode string, it fails.\n        return\n    title_bytes = title.encode()\n    buf = ctypes.create_string_buffer(len(title_bytes))\n    buf.value = title_bytes\n    try:\n        libc.prctl(15, buf, 0, 0, 0)\n    except AttributeError:\n        return  # Strange libc, just skip this\n\n\ndef remove_start(s, start):\n    return s[len(start):] if s is not None and s.startswith(start) else s\n\n\ndef remove_end(s, end):\n    return s[:-len(end)] if s is not None and s.endswith(end) else s\n\n\ndef remove_quotes(s):\n    if s is None or len(s) < 2:\n        return s\n    for quote in ('\"', \"'\", ):\n        if s[0] == quote and s[-1] == quote:\n            return s[1:-1]\n    return s\n\n\ndef get_domain(url):\n    \"\"\"\n    This implementation is inconsistent, but is kept for compatibility.\n    Use this only for \"webpage_url_domain\"\n    \"\"\"\n    return remove_start(urllib.parse.urlparse(url).netloc, 'www.') or None\n\n\ndef url_basename(url):\n    path = urllib.parse.urlparse(url).path\n    return path.strip('/').split('/')[-1]\n\n\ndef base_url(url):\n    return re.match(r'https?://[^?#]+/', url).group()\n\n\ndef urljoin(base, path):\n    if isinstance(path, bytes):\n        path = path.decode()\n    if not isinstance(path, str) or not path:\n        return None\n    if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):\n        return path\n    if isinstance(base, bytes):\n        base = base.decode()\n    if not isinstance(base, str) or not re.match(\n            r'^(?:https?:)?//', base):\n        return None\n    return urllib.parse.urljoin(base, path)\n\n\ndef int_or_none(v, scale=1, default=None, get_attr=None, invscale=1):\n    if get_attr and v is not None:\n        v = getattr(v, get_attr, None)\n    try:\n        return int(v) * invscale // scale\n    except (ValueError, TypeError, OverflowError):\n        return default\n\n\ndef str_or_none(v, default=None):\n    return default if v is None else str(v)\n\n\ndef str_to_int(int_str):\n    \"\"\" A more relaxed version of int_or_none \"\"\"\n    if isinstance(int_str, int):\n        return int_str\n    elif isinstance(int_str, str):\n        int_str = re.sub(r'[,\\.\\+]', '', int_str)\n        return int_or_none(int_str)\n\n\ndef float_or_none(v, scale=1, invscale=1, default=None):\n    if v is None:\n        return default\n    try:\n        return float(v) * invscale / scale\n    except (ValueError, TypeError):\n        return default\n\n\ndef bool_or_none(v, default=None):\n    return v if isinstance(v, bool) else default\n\n\ndef strip_or_none(v, default=None):\n    return v.strip() if isinstance(v, str) else default\n\n\ndef url_or_none(url):\n    if not url or not isinstance(url, str):\n        return None\n    url = url.strip()\n    return url if re.match(r'^(?:(?:https?|rt(?:m(?:pt?[es]?|fp)|sp[su]?)|mms|ftps?):)?//', url) else None\n\n\ndef strftime_or_none(timestamp, date_format='%Y%m%d', default=None):\n    datetime_object = None\n    try:\n        if isinstance(timestamp, (int, float)):  # unix timestamp\n            # Using naive datetime here can break timestamp() in Windows\n            # Ref: https://github.com/yt-dlp/yt-dlp/issues/5185, https://github.com/python/cpython/issues/94414\n            # Also, datetime.datetime.fromtimestamp breaks for negative timestamps\n            # Ref: https://github.com/yt-dlp/yt-dlp/issues/6706#issuecomment-1496842642\n            datetime_object = (datetime.datetime.fromtimestamp(0, datetime.timezone.utc)\n                               + datetime.timedelta(seconds=timestamp))\n        elif isinstance(timestamp, str):  # assume YYYYMMDD\n            datetime_object = datetime.datetime.strptime(timestamp, '%Y%m%d')\n        date_format = re.sub(  # Support %s on windows\n            r'(?<!%)(%%)*%s', rf'\\g<1>{int(datetime_object.timestamp())}', date_format)\n        return datetime_object.strftime(date_format)\n    except (ValueError, TypeError, AttributeError):\n        return default\n\n\ndef parse_duration(s):\n    if not isinstance(s, str):\n        return None\n    s = s.strip()\n    if not s:\n        return None\n\n    days, hours, mins, secs, ms = [None] * 5\n    m = re.match(r'''(?x)\n            (?P<before_secs>\n                (?:(?:(?P<days>[0-9]+):)?(?P<hours>[0-9]+):)?(?P<mins>[0-9]+):)?\n            (?P<secs>(?(before_secs)[0-9]{1,2}|[0-9]+))\n            (?P<ms>[.:][0-9]+)?Z?$\n        ''', s)\n    if m:\n        days, hours, mins, secs, ms = m.group('days', 'hours', 'mins', 'secs', 'ms')\n    else:\n        m = re.match(\n            r'''(?ix)(?:P?\n                (?:\n                    [0-9]+\\s*y(?:ears?)?,?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*m(?:onths?)?,?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*w(?:eeks?)?,?\\s*\n                )?\n                (?:\n                    (?P<days>[0-9]+)\\s*d(?:ays?)?,?\\s*\n                )?\n                T)?\n                (?:\n                    (?P<hours>[0-9]+)\\s*h(?:(?:ou)?rs?)?,?\\s*\n                )?\n                (?:\n                    (?P<mins>[0-9]+)\\s*m(?:in(?:ute)?s?)?,?\\s*\n                )?\n                (?:\n                    (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*s(?:ec(?:ond)?s?)?\\s*\n                )?Z?$''', s)\n        if m:\n            days, hours, mins, secs, ms = m.groups()\n        else:\n            m = re.match(r'(?i)(?:(?P<hours>[0-9.]+)\\s*(?:hours?)|(?P<mins>[0-9.]+)\\s*(?:mins?\\.?|minutes?)\\s*)Z?$', s)\n            if m:\n                hours, mins = m.groups()\n            else:\n                return None\n\n    if ms:\n        ms = ms.replace(':', '.')\n    return sum(float(part or 0) * mult for part, mult in (\n        (days, 86400), (hours, 3600), (mins, 60), (secs, 1), (ms, 1)))\n\n\ndef prepend_extension(filename, ext, expected_real_ext=None):\n    name, real_ext = os.path.splitext(filename)\n    return (\n        f'{name}.{ext}{real_ext}'\n        if not expected_real_ext or real_ext[1:] == expected_real_ext\n        else f'{filename}.{ext}')\n\n\ndef replace_extension(filename, ext, expected_real_ext=None):\n    name, real_ext = os.path.splitext(filename)\n    return '{}.{}'.format(\n        name if not expected_real_ext or real_ext[1:] == expected_real_ext else filename,\n        ext)\n\n\ndef check_executable(exe, args=[]):\n    \"\"\" Checks if the given binary is installed somewhere in PATH, and returns its name.\n    args can be a list of arguments for a short output (like -version) \"\"\"\n    try:\n        Popen.run([exe] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    except OSError:\n        return False\n    return exe\n\n\ndef _get_exe_version_output(exe, args):\n    try:\n        # STDIN should be redirected too. On UNIX-like systems, ffmpeg triggers\n        # SIGTTOU if yt-dlp is run in the background.\n        # See https://github.com/ytdl-org/youtube-dl/issues/955#issuecomment-209789656\n        stdout, _, ret = Popen.run([encodeArgument(exe)] + args, text=True,\n                                   stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n        if ret:\n            return None\n    except OSError:\n        return False\n    return stdout\n\n\ndef detect_exe_version(output, version_re=None, unrecognized='present'):\n    assert isinstance(output, str)\n    if version_re is None:\n        version_re = r'version\\s+([-0-9._a-zA-Z]+)'\n    m = re.search(version_re, output)\n    if m:\n        return m.group(1)\n    else:\n        return unrecognized\n\n\ndef get_exe_version(exe, args=['--version'],\n                    version_re=None, unrecognized=('present', 'broken')):\n    \"\"\" Returns the version of the specified executable,\n    or False if the executable is not present \"\"\"\n    unrecognized = variadic(unrecognized)\n    assert len(unrecognized) in (1, 2)\n    out = _get_exe_version_output(exe, args)\n    if out is None:\n        return unrecognized[-1]\n    return out and detect_exe_version(out, version_re, unrecognized[0])\n\n\ndef frange(start=0, stop=None, step=1):\n    \"\"\"Float range\"\"\"\n    if stop is None:\n        start, stop = 0, start\n    sign = [-1, 1][step > 0] if step else 0\n    while sign * start < sign * stop:\n        yield start\n        start += step\n\n\nclass LazyList(collections.abc.Sequence):\n    \"\"\"Lazy immutable list from an iterable\n    Note that slices of a LazyList are lists and not LazyList\"\"\"\n\n    class IndexError(IndexError):\n        pass\n\n    def __init__(self, iterable, *, reverse=False, _cache=None):\n        self._iterable = iter(iterable)\n        self._cache = [] if _cache is None else _cache\n        self._reversed = reverse\n\n    def __iter__(self):\n        if self._reversed:\n            # We need to consume the entire iterable to iterate in reverse\n            yield from self.exhaust()\n            return\n        yield from self._cache\n        for item in self._iterable:\n            self._cache.append(item)\n            yield item\n\n    def _exhaust(self):\n        self._cache.extend(self._iterable)\n        self._iterable = []  # Discard the emptied iterable to make it pickle-able\n        return self._cache\n\n    def exhaust(self):\n        \"\"\"Evaluate the entire iterable\"\"\"\n        return self._exhaust()[::-1 if self._reversed else 1]\n\n    @staticmethod\n    def _reverse_index(x):\n        return None if x is None else ~x\n\n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            if self._reversed:\n                idx = slice(self._reverse_index(idx.start), self._reverse_index(idx.stop), -(idx.step or 1))\n            start, stop, step = idx.start, idx.stop, idx.step or 1\n        elif isinstance(idx, int):\n            if self._reversed:\n                idx = self._reverse_index(idx)\n            start, stop, step = idx, idx, 0\n        else:\n            raise TypeError('indices must be integers or slices')\n        if ((start or 0) < 0 or (stop or 0) < 0\n                or (start is None and step < 0)\n                or (stop is None and step > 0)):\n            # We need to consume the entire iterable to be able to slice from the end\n            # Obviously, never use this with infinite iterables\n            self._exhaust()\n            try:\n                return self._cache[idx]\n            except IndexError as e:\n                raise self.IndexError(e) from e\n        n = max(start or 0, stop or 0) - len(self._cache) + 1\n        if n > 0:\n            self._cache.extend(itertools.islice(self._iterable, n))\n        try:\n            return self._cache[idx]\n        except IndexError as e:\n            raise self.IndexError(e) from e\n\n    def __bool__(self):\n        try:\n            self[-1] if self._reversed else self[0]\n        except self.IndexError:\n            return False\n        return True\n\n    def __len__(self):\n        self._exhaust()\n        return len(self._cache)\n\n    def __reversed__(self):\n        return type(self)(self._iterable, reverse=not self._reversed, _cache=self._cache)\n\n    def __copy__(self):\n        return type(self)(self._iterable, reverse=self._reversed, _cache=self._cache)\n\n    def __repr__(self):\n        # repr and str should mimic a list. So we exhaust the iterable\n        return repr(self.exhaust())\n\n    def __str__(self):\n        return repr(self.exhaust())\n\n\nclass PagedList:\n\n    class IndexError(IndexError):\n        pass\n\n    def __len__(self):\n        # This is only useful for tests\n        return len(self.getslice())\n\n    def __init__(self, pagefunc, pagesize, use_cache=True):\n        self._pagefunc = pagefunc\n        self._pagesize = pagesize\n        self._pagecount = float('inf')\n        self._use_cache = use_cache\n        self._cache = {}\n\n    def getpage(self, pagenum):\n        page_results = self._cache.get(pagenum)\n        if page_results is None:\n            page_results = [] if pagenum > self._pagecount else list(self._pagefunc(pagenum))\n        if self._use_cache:\n            self._cache[pagenum] = page_results\n        return page_results\n\n    def getslice(self, start=0, end=None):\n        return list(self._getslice(start, end))\n\n    def _getslice(self, start, end):\n        raise NotImplementedError('This method must be implemented by subclasses')\n\n    def __getitem__(self, idx):\n        assert self._use_cache, 'Indexing PagedList requires cache'\n        if not isinstance(idx, int) or idx < 0:\n            raise TypeError('indices must be non-negative integers')\n        entries = self.getslice(idx, idx + 1)\n        if not entries:\n            raise self.IndexError()\n        return entries[0]\n\n\nclass OnDemandPagedList(PagedList):\n    \"\"\"Download pages until a page with less than maximum results\"\"\"\n\n    def _getslice(self, start, end):\n        for pagenum in itertools.count(start // self._pagesize):\n            firstid = pagenum * self._pagesize\n            nextfirstid = pagenum * self._pagesize + self._pagesize\n            if start >= nextfirstid:\n                continue\n\n            startv = (\n                start % self._pagesize\n                if firstid <= start < nextfirstid\n                else 0)\n            endv = (\n                ((end - 1) % self._pagesize) + 1\n                if (end is not None and firstid <= end <= nextfirstid)\n                else None)\n\n            try:\n                page_results = self.getpage(pagenum)\n            except Exception:\n                self._pagecount = pagenum - 1\n                raise\n            if startv != 0 or endv is not None:\n                page_results = page_results[startv:endv]\n            yield from page_results\n\n            # A little optimization - if current page is not \"full\", ie. does\n            # not contain page_size videos then we can assume that this page\n            # is the last one - there are no more ids on further pages -\n            # i.e. no need to query again.\n            if len(page_results) + startv < self._pagesize:\n                break\n\n            # If we got the whole page, but the next page is not interesting,\n            # break out early as well\n            if end == nextfirstid:\n                break\n\n\nclass InAdvancePagedList(PagedList):\n    \"\"\"PagedList with total number of pages known in advance\"\"\"\n\n    def __init__(self, pagefunc, pagecount, pagesize):\n        PagedList.__init__(self, pagefunc, pagesize, True)\n        self._pagecount = pagecount\n\n    def _getslice(self, start, end):\n        start_page = start // self._pagesize\n        end_page = self._pagecount if end is None else min(self._pagecount, end // self._pagesize + 1)\n        skip_elems = start - start_page * self._pagesize\n        only_more = None if end is None else end - start\n        for pagenum in range(start_page, end_page):\n            page_results = self.getpage(pagenum)\n            if skip_elems:\n                page_results = page_results[skip_elems:]\n                skip_elems = None\n            if only_more is not None:\n                if len(page_results) < only_more:\n                    only_more -= len(page_results)\n                else:\n                    yield from page_results[:only_more]\n                    break\n            yield from page_results\n\n\nclass PlaylistEntries:\n    MissingEntry = object()\n    is_exhausted = False\n\n    def __init__(self, ydl, info_dict):\n        self.ydl = ydl\n\n        # _entries must be assigned now since infodict can change during iteration\n        entries = info_dict.get('entries')\n        if entries is None:\n            raise EntryNotInPlaylist('There are no entries')\n        elif isinstance(entries, list):\n            self.is_exhausted = True\n\n        requested_entries = info_dict.get('requested_entries')\n        self.is_incomplete = requested_entries is not None\n        if self.is_incomplete:\n            assert self.is_exhausted\n            self._entries = [self.MissingEntry] * max(requested_entries or [0])\n            for i, entry in zip(requested_entries, entries):\n                self._entries[i - 1] = entry\n        elif isinstance(entries, (list, PagedList, LazyList)):\n            self._entries = entries\n        else:\n            self._entries = LazyList(entries)\n\n    PLAYLIST_ITEMS_RE = re.compile(r'''(?x)\n        (?P<start>[+-]?\\d+)?\n        (?P<range>[:-]\n            (?P<end>[+-]?\\d+|inf(?:inite)?)?\n            (?::(?P<step>[+-]?\\d+))?\n        )?''')\n\n    @classmethod\n    def parse_playlist_items(cls, string):\n        for segment in string.split(','):\n            if not segment:\n                raise ValueError('There is two or more consecutive commas')\n            mobj = cls.PLAYLIST_ITEMS_RE.fullmatch(segment)\n            if not mobj:\n                raise ValueError(f'{segment!r} is not a valid specification')\n            start, end, step, has_range = mobj.group('start', 'end', 'step', 'range')\n            if int_or_none(step) == 0:\n                raise ValueError(f'Step in {segment!r} cannot be zero')\n            yield slice(int_or_none(start), float_or_none(end), int_or_none(step)) if has_range else int(start)\n\n    def get_requested_items(self):\n        playlist_items = self.ydl.params.get('playlist_items')\n        playlist_start = self.ydl.params.get('playliststart', 1)\n        playlist_end = self.ydl.params.get('playlistend')\n        # For backwards compatibility, interpret -1 as whole list\n        if playlist_end in (-1, None):\n            playlist_end = ''\n        if not playlist_items:\n            playlist_items = f'{playlist_start}:{playlist_end}'\n        elif playlist_start != 1 or playlist_end:\n            self.ydl.report_warning('Ignoring playliststart and playlistend because playlistitems was given', only_once=True)\n\n        for index in self.parse_playlist_items(playlist_items):\n            for i, entry in self[index]:\n                yield i, entry\n                if not entry:\n                    continue\n                try:\n                    # The item may have just been added to archive. Don't break due to it\n                    if not self.ydl.params.get('lazy_playlist'):\n                        # TODO: Add auto-generated fields\n                        self.ydl._match_entry(entry, incomplete=True, silent=True)\n                except (ExistingVideoReached, RejectedVideoReached):\n                    return\n\n    def get_full_count(self):\n        if self.is_exhausted and not self.is_incomplete:\n            return len(self)\n        elif isinstance(self._entries, InAdvancePagedList):\n            if self._entries._pagesize == 1:\n                return self._entries._pagecount\n\n    @functools.cached_property\n    def _getter(self):\n        if isinstance(self._entries, list):\n            def get_entry(i):\n                try:\n                    entry = self._entries[i]\n                except IndexError:\n                    entry = self.MissingEntry\n                    if not self.is_incomplete:\n                        raise self.IndexError()\n                if entry is self.MissingEntry:\n                    raise EntryNotInPlaylist(f'Entry {i + 1} cannot be found')\n                return entry\n        else:\n            def get_entry(i):\n                try:\n                    return type(self.ydl)._handle_extraction_exceptions(lambda _, i: self._entries[i])(self.ydl, i)\n                except (LazyList.IndexError, PagedList.IndexError):\n                    raise self.IndexError()\n        return get_entry\n\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            idx = slice(idx, idx)\n\n        # NB: PlaylistEntries[1:10] => (0, 1, ... 9)\n        step = 1 if idx.step is None else idx.step\n        if idx.start is None:\n            start = 0 if step > 0 else len(self) - 1\n        else:\n            start = idx.start - 1 if idx.start >= 0 else len(self) + idx.start\n\n        # NB: Do not call len(self) when idx == [:]\n        if idx.stop is None:\n            stop = 0 if step < 0 else float('inf')\n        else:\n            stop = idx.stop - 1 if idx.stop >= 0 else len(self) + idx.stop\n        stop += [-1, 1][step > 0]\n\n        for i in frange(start, stop, step):\n            if i < 0:\n                continue\n            try:\n                entry = self._getter(i)\n            except self.IndexError:\n                self.is_exhausted = True\n                if step > 0:\n                    break\n                continue\n            yield i + 1, entry\n\n    def __len__(self):\n        return len(tuple(self[:]))\n\n    class IndexError(IndexError):\n        pass\n\n\ndef uppercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\U[0-9a-fA-F]{8}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)\n\n\ndef lowercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\u[0-9a-fA-F]{4}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)\n\n\ndef parse_qs(url, **kwargs):\n    return urllib.parse.parse_qs(urllib.parse.urlparse(url).query, **kwargs)\n\n\ndef read_batch_urls(batch_fd):\n    def fixup(url):\n        if not isinstance(url, str):\n            url = url.decode('utf-8', 'replace')\n        BOM_UTF8 = ('\\xef\\xbb\\xbf', '\\ufeff')\n        for bom in BOM_UTF8:\n            if url.startswith(bom):\n                url = url[len(bom):]\n        url = url.lstrip()\n        if not url or url.startswith(('#', ';', ']')):\n            return False\n        # \"#\" cannot be stripped out since it is part of the URI\n        # However, it can be safely stripped out if following a whitespace\n        return re.split(r'\\s#', url, 1)[0].rstrip()\n\n    with contextlib.closing(batch_fd) as fd:\n        return [url for url in map(fixup, fd) if url]\n\n\ndef urlencode_postdata(*args, **kargs):\n    return urllib.parse.urlencode(*args, **kargs).encode('ascii')\n\n\ndef update_url(url, *, query_update=None, **kwargs):\n    \"\"\"Replace URL components specified by kwargs\n       @param url           str or parse url tuple\n       @param query_update  update query\n       @returns             str\n    \"\"\"\n    if isinstance(url, str):\n        if not kwargs and not query_update:\n            return url\n        else:\n            url = urllib.parse.urlparse(url)\n    if query_update:\n        assert 'query' not in kwargs, 'query_update and query cannot be specified at the same time'\n        kwargs['query'] = urllib.parse.urlencode({\n            **urllib.parse.parse_qs(url.query),\n            **query_update\n        }, True)\n    return urllib.parse.urlunparse(url._replace(**kwargs))\n\n\ndef update_url_query(url, query):\n    return update_url(url, query_update=query)\n\n\ndef _multipart_encode_impl(data, boundary):\n    content_type = 'multipart/form-data; boundary=%s' % boundary\n\n    out = b''\n    for k, v in data.items():\n        out += b'--' + boundary.encode('ascii') + b'\\r\\n'\n        if isinstance(k, str):\n            k = k.encode()\n        if isinstance(v, str):\n            v = v.encode()\n        # RFC 2047 requires non-ASCII field names to be encoded, while RFC 7578\n        # suggests sending UTF-8 directly. Firefox sends UTF-8, too\n        content = b'Content-Disposition: form-data; name=\"' + k + b'\"\\r\\n\\r\\n' + v + b'\\r\\n'\n        if boundary.encode('ascii') in content:\n            raise ValueError('Boundary overlaps with data')\n        out += content\n\n    out += b'--' + boundary.encode('ascii') + b'--\\r\\n'\n\n    return out, content_type\n\n\ndef multipart_encode(data, boundary=None):\n    '''\n    Encode a dict to RFC 7578-compliant form-data\n\n    data:\n        A dict where keys and values can be either Unicode or bytes-like\n        objects.\n    boundary:\n        If specified a Unicode object, it's used as the boundary. Otherwise\n        a random boundary is generated.\n\n    Reference: https://tools.ietf.org/html/rfc7578\n    '''\n    has_specified_boundary = boundary is not None\n\n    while True:\n        if boundary is None:\n            boundary = '---------------' + str(random.randrange(0x0fffffff, 0xffffffff))\n\n        try:\n            out, content_type = _multipart_encode_impl(data, boundary)\n            break\n        except ValueError:\n            if has_specified_boundary:\n                raise\n            boundary = None\n\n    return out, content_type\n\n\ndef is_iterable_like(x, allowed_types=collections.abc.Iterable, blocked_types=NO_DEFAULT):\n    if blocked_types is NO_DEFAULT:\n        blocked_types = (str, bytes, collections.abc.Mapping)\n    return isinstance(x, allowed_types) and not isinstance(x, blocked_types)\n\n\ndef variadic(x, allowed_types=NO_DEFAULT):\n    if not isinstance(allowed_types, (tuple, type)):\n        deprecation_warning('allowed_types should be a tuple or a type')\n        allowed_types = tuple(allowed_types)\n    return x if is_iterable_like(x, blocked_types=allowed_types) else (x, )\n\n\ndef try_call(*funcs, expected_type=None, args=[], kwargs={}):\n    for f in funcs:\n        try:\n            val = f(*args, **kwargs)\n        except (AttributeError, KeyError, TypeError, IndexError, ValueError, ZeroDivisionError):\n            pass\n        else:\n            if expected_type is None or isinstance(val, expected_type):\n                return val\n\n\ndef try_get(src, getter, expected_type=None):\n    return try_call(*variadic(getter), args=(src,), expected_type=expected_type)\n\n\ndef filter_dict(dct, cndn=lambda _, v: v is not None):\n    return {k: v for k, v in dct.items() if cndn(k, v)}\n\n\ndef merge_dicts(*dicts):\n    merged = {}\n    for a_dict in dicts:\n        for k, v in a_dict.items():\n            if (v is not None and k not in merged\n                    or isinstance(v, str) and merged[k] == ''):\n                merged[k] = v\n    return merged\n\n\ndef encode_compat_str(string, encoding=preferredencoding(), errors='strict'):\n    return string if isinstance(string, str) else str(string, encoding, errors)\n\n\nUS_RATINGS = {\n    'G': 0,\n    'PG': 10,\n    'PG-13': 13,\n    'R': 16,\n    'NC': 18,\n}\n\n\nTV_PARENTAL_GUIDELINES = {\n    'TV-Y': 0,\n    'TV-Y7': 7,\n    'TV-G': 0,\n    'TV-PG': 0,\n    'TV-14': 14,\n    'TV-MA': 17,\n}\n\n\ndef parse_age_limit(s):\n    # isinstance(False, int) is True. So type() must be used instead\n    if type(s) is int:  # noqa: E721\n        return s if 0 <= s <= 21 else None\n    elif not isinstance(s, str):\n        return None\n    m = re.match(r'^(?P<age>\\d{1,2})\\+?$', s)\n    if m:\n        return int(m.group('age'))\n    s = s.upper()\n    if s in US_RATINGS:\n        return US_RATINGS[s]\n    m = re.match(r'^TV[_-]?(%s)$' % '|'.join(k[3:] for k in TV_PARENTAL_GUIDELINES), s)\n    if m:\n        return TV_PARENTAL_GUIDELINES['TV-' + m.group(1)]\n    return None\n\n\ndef strip_jsonp(code):\n    return re.sub(\n        r'''(?sx)^\n            (?:window\\.)?(?P<func_name>[a-zA-Z0-9_.$]*)\n            (?:\\s*&&\\s*(?P=func_name))?\n            \\s*\\(\\s*(?P<callback_data>.*)\\);?\n            \\s*?(?://[^\\n]*)*$''',\n        r'\\g<callback_data>', code)\n\n\ndef js_to_json(code, vars={}, *, strict=False):\n    # vars is a dict of var, val pairs to substitute\n    STRING_QUOTES = '\\'\"`'\n    STRING_RE = '|'.join(rf'{q}(?:\\\\.|[^\\\\{q}])*{q}' for q in STRING_QUOTES)\n    COMMENT_RE = r'/\\*(?:(?!\\*/).)*?\\*/|//[^\\n]*\\n'\n    SKIP_RE = fr'\\s*(?:{COMMENT_RE})?\\s*'\n    INTEGER_TABLE = (\n        (fr'(?s)^(0[xX][0-9a-fA-F]+){SKIP_RE}:?$', 16),\n        (fr'(?s)^(0+[0-7]+){SKIP_RE}:?$', 8),\n    )\n\n    def process_escape(match):\n        JSON_PASSTHROUGH_ESCAPES = R'\"\\bfnrtu'\n        escape = match.group(1) or match.group(2)\n\n        return (Rf'\\{escape}' if escape in JSON_PASSTHROUGH_ESCAPES\n                else R'\\u00' if escape == 'x'\n                else '' if escape == '\\n'\n                else escape)\n\n    def template_substitute(match):\n        evaluated = js_to_json(match.group(1), vars, strict=strict)\n        if evaluated[0] == '\"':\n            return json.loads(evaluated)\n        return evaluated\n\n    def fix_kv(m):\n        v = m.group(0)\n        if v in ('true', 'false', 'null'):\n            return v\n        elif v in ('undefined', 'void 0'):\n            return 'null'\n        elif v.startswith('/*') or v.startswith('//') or v.startswith('!') or v == ',':\n            return ''\n\n        if v[0] in STRING_QUOTES:\n            v = re.sub(r'(?s)\\${([^}]+)}', template_substitute, v[1:-1]) if v[0] == '`' else v[1:-1]\n            escaped = re.sub(r'(?s)(\")|\\\\(.)', process_escape, v)\n            return f'\"{escaped}\"'\n\n        for regex, base in INTEGER_TABLE:\n            im = re.match(regex, v)\n            if im:\n                i = int(im.group(1), base)\n                return f'\"{i}\":' if v.endswith(':') else str(i)\n\n        if v in vars:\n            try:\n                if not strict:\n                    json.loads(vars[v])\n            except json.JSONDecodeError:\n                return json.dumps(vars[v])\n            else:\n                return vars[v]\n\n        if not strict:\n            return f'\"{v}\"'\n\n        raise ValueError(f'Unknown value: {v}')\n\n    def create_map(mobj):\n        return json.dumps(dict(json.loads(js_to_json(mobj.group(1) or '[]', vars=vars))))\n\n    code = re.sub(r'(?:new\\s+)?Array\\((.*?)\\)', r'[\\g<1>]', code)\n    code = re.sub(r'new Map\\((\\[.*?\\])?\\)', create_map, code)\n    if not strict:\n        code = re.sub(r'new Date\\((\".+\")\\)', r'\\g<1>', code)\n        code = re.sub(r'new \\w+\\((.*?)\\)', lambda m: json.dumps(m.group(0)), code)\n        code = re.sub(r'parseInt\\([^\\d]+(\\d+)[^\\d]+\\)', r'\\1', code)\n        code = re.sub(r'\\(function\\([^)]*\\)\\s*\\{[^}]*\\}\\s*\\)\\s*\\(\\s*([\"\\'][^)]*[\"\\'])\\s*\\)', r'\\1', code)\n\n    return re.sub(rf'''(?sx)\n        {STRING_RE}|\n        {COMMENT_RE}|,(?={SKIP_RE}[\\]}}])|\n        void\\s0|(?:(?<![0-9])[eE]|[a-df-zA-DF-Z_$])[.a-zA-Z_$0-9]*|\n        \\b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{SKIP_RE}:)?|\n        [0-9]+(?={SKIP_RE}:)|\n        !+\n        ''', fix_kv, code)\n\n\ndef qualities(quality_ids):\n    \"\"\" Get a numeric quality value out of a list of possible values \"\"\"\n    def q(qid):\n        try:\n            return quality_ids.index(qid)\n        except ValueError:\n            return -1\n    return q\n\n\nPOSTPROCESS_WHEN = ('pre_process', 'after_filter', 'video', 'before_dl', 'post_process', 'after_move', 'after_video', 'playlist')\n\n\nDEFAULT_OUTTMPL = {\n    'default': '%(title)s [%(id)s].%(ext)s',\n    'chapter': '%(title)s - %(section_number)03d %(section_title)s [%(id)s].%(ext)s',\n}\nOUTTMPL_TYPES = {\n    'chapter': None,\n    'subtitle': None,\n    'thumbnail': None,\n    'description': 'description',\n    'annotation': 'annotations.xml',\n    'infojson': 'info.json',\n    'link': None,\n    'pl_video': None,\n    'pl_thumbnail': None,\n    'pl_description': 'description',\n    'pl_infojson': 'info.json',\n}\n\n# As of [1] format syntax is:\n#  %[mapping_key][conversion_flags][minimum_width][.precision][length_modifier]type\n# 1. https://docs.python.org/2/library/stdtypes.html#string-formatting\nSTR_FORMAT_RE_TMPL = r'''(?x)\n    (?<!%)(?P<prefix>(?:%%)*)\n    %\n    (?P<has_key>\\((?P<key>{0})\\))?\n    (?P<format>\n        (?P<conversion>[#0\\-+ ]+)?\n        (?P<min_width>\\d+)?\n        (?P<precision>\\.\\d+)?\n        (?P<len_mod>[hlL])?  # unused in python\n        {1}  # conversion type\n    )\n'''\n\n\nSTR_FORMAT_TYPES = 'diouxXeEfFgGcrsa'\n\n\ndef limit_length(s, length):\n    \"\"\" Add ellipses to overly long strings \"\"\"\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s\n\n\ndef version_tuple(v):\n    return tuple(int(e) for e in re.split(r'[-.]', v))\n\n\ndef is_outdated_version(version, limit, assume_new=True):\n    if not version:\n        return not assume_new\n    try:\n        return version_tuple(version) < version_tuple(limit)\n    except ValueError:\n        return not assume_new\n\n\ndef ytdl_is_updateable():\n    \"\"\" Returns if yt-dlp can be updated with -U \"\"\"\n\n    from ..update import is_non_updateable\n\n    return not is_non_updateable()\n\n\ndef args_to_str(args):\n    # Get a short string representation for a subprocess command\n    return ' '.join(compat_shlex_quote(a) for a in args)\n\n\ndef error_to_str(err):\n    return f'{type(err).__name__}: {err}'\n\n\ndef mimetype2ext(mt, default=NO_DEFAULT):\n    if not isinstance(mt, str):\n        if default is not NO_DEFAULT:\n            return default\n        return None\n\n    MAP = {\n        # video\n        '3gpp': '3gp',\n        'mp2t': 'ts',\n        'mp4': 'mp4',\n        'mpeg': 'mpeg',\n        'mpegurl': 'm3u8',\n        'quicktime': 'mov',\n        'webm': 'webm',\n        'vp9': 'vp9',\n        'video/ogg': 'ogv',\n        'x-flv': 'flv',\n        'x-m4v': 'm4v',\n        'x-matroska': 'mkv',\n        'x-mng': 'mng',\n        'x-mp4-fragmented': 'mp4',\n        'x-ms-asf': 'asf',\n        'x-ms-wmv': 'wmv',\n        'x-msvideo': 'avi',\n\n        # application (streaming playlists)\n        'dash+xml': 'mpd',\n        'f4m+xml': 'f4m',\n        'hds+xml': 'f4m',\n        'vnd.apple.mpegurl': 'm3u8',\n        'vnd.ms-sstr+xml': 'ism',\n        'x-mpegurl': 'm3u8',\n\n        # audio\n        'audio/mp4': 'm4a',\n        # Per RFC 3003, audio/mpeg can be .mp1, .mp2 or .mp3.\n        # Using .mp3 as it's the most popular one\n        'audio/mpeg': 'mp3',\n        'audio/webm': 'webm',\n        'audio/x-matroska': 'mka',\n        'audio/x-mpegurl': 'm3u',\n        'midi': 'mid',\n        'ogg': 'ogg',\n        'wav': 'wav',\n        'wave': 'wav',\n        'x-aac': 'aac',\n        'x-flac': 'flac',\n        'x-m4a': 'm4a',\n        'x-realaudio': 'ra',\n        'x-wav': 'wav',\n\n        # image\n        'avif': 'avif',\n        'bmp': 'bmp',\n        'gif': 'gif',\n        'jpeg': 'jpg',\n        'png': 'png',\n        'svg+xml': 'svg',\n        'tiff': 'tif',\n        'vnd.wap.wbmp': 'wbmp',\n        'webp': 'webp',\n        'x-icon': 'ico',\n        'x-jng': 'jng',\n        'x-ms-bmp': 'bmp',\n\n        # caption\n        'filmstrip+json': 'fs',\n        'smptett+xml': 'tt',\n        'ttaf+xml': 'dfxp',\n        'ttml+xml': 'ttml',\n        'x-ms-sami': 'sami',\n\n        # misc\n        'gzip': 'gz',\n        'json': 'json',\n        'xml': 'xml',\n        'zip': 'zip',\n    }\n\n    mimetype = mt.partition(';')[0].strip().lower()\n    _, _, subtype = mimetype.rpartition('/')\n\n    ext = traversal.traverse_obj(MAP, mimetype, subtype, subtype.rsplit('+')[-1])\n    if ext:\n        return ext\n    elif default is not NO_DEFAULT:\n        return default\n    return subtype.replace('+', '.')\n\n\ndef ext2mimetype(ext_or_url):\n    if not ext_or_url:\n        return None\n    if '.' not in ext_or_url:\n        ext_or_url = f'file.{ext_or_url}'\n    return mimetypes.guess_type(ext_or_url)[0]\n\n\ndef parse_codecs(codecs_str):\n    # http://tools.ietf.org/html/rfc6381\n    if not codecs_str:\n        return {}\n    split_codecs = list(filter(None, map(\n        str.strip, codecs_str.strip().strip(',').split(','))))\n    vcodec, acodec, scodec, hdr = None, None, None, None\n    for full_codec in split_codecs:\n        parts = re.sub(r'0+(?=\\d)', '', full_codec).split('.')\n        if parts[0] in ('avc1', 'avc2', 'avc3', 'avc4', 'vp9', 'vp8', 'hev1', 'hev2',\n                        'h263', 'h264', 'mp4v', 'hvc1', 'av1', 'theora', 'dvh1', 'dvhe'):\n            if vcodec:\n                continue\n            vcodec = full_codec\n            if parts[0] in ('dvh1', 'dvhe'):\n                hdr = 'DV'\n            elif parts[0] == 'av1' and traversal.traverse_obj(parts, 3) == '10':\n                hdr = 'HDR10'\n            elif parts[:2] == ['vp9', '2']:\n                hdr = 'HDR10'\n        elif parts[0] in ('flac', 'mp4a', 'opus', 'vorbis', 'mp3', 'aac', 'ac-4',\n                          'ac-3', 'ec-3', 'eac3', 'dtsc', 'dtse', 'dtsh', 'dtsl'):\n            acodec = acodec or full_codec\n        elif parts[0] in ('stpp', 'wvtt'):\n            scodec = scodec or full_codec\n        else:\n            write_string(f'WARNING: Unknown codec {full_codec}\\n')\n    if vcodec or acodec or scodec:\n        return {\n            'vcodec': vcodec or 'none',\n            'acodec': acodec or 'none',\n            'dynamic_range': hdr,\n            **({'scodec': scodec} if scodec is not None else {}),\n        }\n    elif len(split_codecs) == 2:\n        return {\n            'vcodec': split_codecs[0],\n            'acodec': split_codecs[1],\n        }\n    return {}\n\n\ndef get_compatible_ext(*, vcodecs, acodecs, vexts, aexts, preferences=None):\n    assert len(vcodecs) == len(vexts) and len(acodecs) == len(aexts)\n\n    allow_mkv = not preferences or 'mkv' in preferences\n\n    if allow_mkv and max(len(acodecs), len(vcodecs)) > 1:\n        return 'mkv'  # TODO: any other format allows this?\n\n    # TODO: All codecs supported by parse_codecs isn't handled here\n    COMPATIBLE_CODECS = {\n        'mp4': {\n            'av1', 'hevc', 'avc1', 'mp4a', 'ac-4',  # fourcc (m3u8, mpd)\n            'h264', 'aacl', 'ec-3',  # Set in ISM\n        },\n        'webm': {\n            'av1', 'vp9', 'vp8', 'opus', 'vrbs',\n            'vp9x', 'vp8x',  # in the webm spec\n        },\n    }\n\n    sanitize_codec = functools.partial(\n        try_get, getter=lambda x: x[0].split('.')[0].replace('0', '').lower())\n    vcodec, acodec = sanitize_codec(vcodecs), sanitize_codec(acodecs)\n\n    for ext in preferences or COMPATIBLE_CODECS.keys():\n        codec_set = COMPATIBLE_CODECS.get(ext, set())\n        if ext == 'mkv' or codec_set.issuperset((vcodec, acodec)):\n            return ext\n\n    COMPATIBLE_EXTS = (\n        {'mp3', 'mp4', 'm4a', 'm4p', 'm4b', 'm4r', 'm4v', 'ismv', 'isma', 'mov'},\n        {'webm', 'weba'},\n    )\n    for ext in preferences or vexts:\n        current_exts = {ext, *vexts, *aexts}\n        if ext == 'mkv' or current_exts == {ext} or any(\n                ext_sets.issuperset(current_exts) for ext_sets in COMPATIBLE_EXTS):\n            return ext\n    return 'mkv' if allow_mkv else preferences[-1]\n\n\ndef urlhandle_detect_ext(url_handle, default=NO_DEFAULT):\n    getheader = url_handle.headers.get\n\n    cd = getheader('Content-Disposition')\n    if cd:\n        m = re.match(r'attachment;\\s*filename=\"(?P<filename>[^\"]+)\"', cd)\n        if m:\n            e = determine_ext(m.group('filename'), default_ext=None)\n            if e:\n                return e\n\n    meta_ext = getheader('x-amz-meta-name')\n    if meta_ext:\n        e = meta_ext.rpartition('.')[2]\n        if e:\n            return e\n\n    return mimetype2ext(getheader('Content-Type'), default=default)\n\n\ndef encode_data_uri(data, mime_type):\n    return 'data:%s;base64,%s' % (mime_type, base64.b64encode(data).decode('ascii'))\n\n\ndef age_restricted(content_limit, age_limit):\n    \"\"\" Returns True iff the content should be blocked \"\"\"\n\n    if age_limit is None:  # No limit set\n        return False\n    if content_limit is None:\n        return False  # Content available for everyone\n    return age_limit < content_limit\n\n\n# List of known byte-order-marks (BOM)\nBOMS = [\n    (b'\\xef\\xbb\\xbf', 'utf-8'),\n    (b'\\x00\\x00\\xfe\\xff', 'utf-32-be'),\n    (b'\\xff\\xfe\\x00\\x00', 'utf-32-le'),\n    (b'\\xff\\xfe', 'utf-16-le'),\n    (b'\\xfe\\xff', 'utf-16-be'),\n]\n\n\ndef is_html(first_bytes):\n    \"\"\" Detect whether a file contains HTML by examining its first bytes. \"\"\"\n\n    encoding = 'utf-8'\n    for bom, enc in BOMS:\n        while first_bytes.startswith(bom):\n            encoding, first_bytes = enc, first_bytes[len(bom):]\n\n    return re.match(r'^\\s*<', first_bytes.decode(encoding, 'replace'))\n\n\ndef determine_protocol(info_dict):\n    protocol = info_dict.get('protocol')\n    if protocol is not None:\n        return protocol\n\n    url = sanitize_url(info_dict['url'])\n    if url.startswith('rtmp'):\n        return 'rtmp'\n    elif url.startswith('mms'):\n        return 'mms'\n    elif url.startswith('rtsp'):\n        return 'rtsp'\n\n    ext = determine_ext(url)\n    if ext == 'm3u8':\n        return 'm3u8' if info_dict.get('is_live') else 'm3u8_native'\n    elif ext == 'f4m':\n        return 'f4m'\n\n    return urllib.parse.urlparse(url).scheme\n\n\ndef render_table(header_row, data, delim=False, extra_gap=0, hide_empty=False):\n    \"\"\" Render a list of rows, each as a list of values.\n    Text after a \\t will be right aligned \"\"\"\n    def width(string):\n        return len(remove_terminal_sequences(string).replace('\\t', ''))\n\n    def get_max_lens(table):\n        return [max(width(str(v)) for v in col) for col in zip(*table)]\n\n    def filter_using_list(row, filterArray):\n        return [col for take, col in itertools.zip_longest(filterArray, row, fillvalue=True) if take]\n\n    max_lens = get_max_lens(data) if hide_empty else []\n    header_row = filter_using_list(header_row, max_lens)\n    data = [filter_using_list(row, max_lens) for row in data]\n\n    table = [header_row] + data\n    max_lens = get_max_lens(table)\n    extra_gap += 1\n    if delim:\n        table = [header_row, [delim * (ml + extra_gap) for ml in max_lens]] + data\n        table[1][-1] = table[1][-1][:-extra_gap * len(delim)]  # Remove extra_gap from end of delimiter\n    for row in table:\n        for pos, text in enumerate(map(str, row)):\n            if '\\t' in text:\n                row[pos] = text.replace('\\t', ' ' * (max_lens[pos] - width(text))) + ' ' * extra_gap\n            else:\n                row[pos] = text + ' ' * (max_lens[pos] - width(text) + extra_gap)\n    ret = '\\n'.join(''.join(row).rstrip() for row in table)\n    return ret\n\n\ndef _match_one(filter_part, dct, incomplete):\n    # TODO: Generalize code with YoutubeDL._build_format_filter\n    STRING_OPERATORS = {\n        '*=': operator.contains,\n        '^=': lambda attr, value: attr.startswith(value),\n        '$=': lambda attr, value: attr.endswith(value),\n        '~=': lambda attr, value: re.search(value, attr),\n    }\n    COMPARISON_OPERATORS = {\n        **STRING_OPERATORS,\n        '<=': operator.le,  # \"<=\" must be defined above \"<\"\n        '<': operator.lt,\n        '>=': operator.ge,\n        '>': operator.gt,\n        '=': operator.eq,\n    }\n\n    if isinstance(incomplete, bool):\n        is_incomplete = lambda _: incomplete\n    else:\n        is_incomplete = lambda k: k in incomplete\n\n    operator_rex = re.compile(r'''(?x)\n        (?P<key>[a-z_]+)\n        \\s*(?P<negation>!\\s*)?(?P<op>%s)(?P<none_inclusive>\\s*\\?)?\\s*\n        (?:\n            (?P<quote>[\"\\'])(?P<quotedstrval>.+?)(?P=quote)|\n            (?P<strval>.+?)\n        )\n        ''' % '|'.join(map(re.escape, COMPARISON_OPERATORS.keys())))\n    m = operator_rex.fullmatch(filter_part.strip())\n    if m:\n        m = m.groupdict()\n        unnegated_op = COMPARISON_OPERATORS[m['op']]\n        if m['negation']:\n            op = lambda attr, value: not unnegated_op(attr, value)\n        else:\n            op = unnegated_op\n        comparison_value = m['quotedstrval'] or m['strval'] or m['intval']\n        if m['quote']:\n            comparison_value = comparison_value.replace(r'\\%s' % m['quote'], m['quote'])\n        actual_value = dct.get(m['key'])\n        numeric_comparison = None\n        if isinstance(actual_value, (int, float)):\n            # If the original field is a string and matching comparisonvalue is\n            # a number we should respect the origin of the original field\n            # and process comparison value as a string (see\n            # https://github.com/ytdl-org/youtube-dl/issues/11082)\n            try:\n                numeric_comparison = int(comparison_value)\n            except ValueError:\n                numeric_comparison = parse_filesize(comparison_value)\n                if numeric_comparison is None:\n                    numeric_comparison = parse_filesize(f'{comparison_value}B')\n                if numeric_comparison is None:\n                    numeric_comparison = parse_duration(comparison_value)\n        if numeric_comparison is not None and m['op'] in STRING_OPERATORS:\n            raise ValueError('Operator %s only supports string values!' % m['op'])\n        if actual_value is None:\n            return is_incomplete(m['key']) or m['none_inclusive']\n        return op(actual_value, comparison_value if numeric_comparison is None else numeric_comparison)\n\n    UNARY_OPERATORS = {\n        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),\n        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),\n    }\n    operator_rex = re.compile(r'''(?x)\n        (?P<op>%s)\\s*(?P<key>[a-z_]+)\n        ''' % '|'.join(map(re.escape, UNARY_OPERATORS.keys())))\n    m = operator_rex.fullmatch(filter_part.strip())\n    if m:\n        op = UNARY_OPERATORS[m.group('op')]\n        actual_value = dct.get(m.group('key'))\n        if is_incomplete(m.group('key')) and actual_value is None:\n            return True\n        return op(actual_value)\n\n    raise ValueError('Invalid filter part %r' % filter_part)\n\n\ndef match_str(filter_str, dct, incomplete=False):\n    \"\"\" Filter a dictionary with a simple string syntax.\n    @returns           Whether the filter passes\n    @param incomplete  Set of keys that is expected to be missing from dct.\n                       Can be True/False to indicate all/none of the keys may be missing.\n                       All conditions on incomplete keys pass if the key is missing\n    \"\"\"\n    return all(\n        _match_one(filter_part.replace(r'\\&', '&'), dct, incomplete)\n        for filter_part in re.split(r'(?<!\\\\)&', filter_str))\n\n\ndef match_filter_func(filters, breaking_filters=None):\n    if not filters and not breaking_filters:\n        return None\n    breaking_filters = match_filter_func(breaking_filters) or (lambda _, __: None)\n    filters = set(variadic(filters or []))\n\n    interactive = '-' in filters\n    if interactive:\n        filters.remove('-')\n\n    def _match_func(info_dict, incomplete=False):\n        ret = breaking_filters(info_dict, incomplete)\n        if ret is not None:\n            raise RejectedVideoReached(ret)\n\n        if not filters or any(match_str(f, info_dict, incomplete) for f in filters):\n            return NO_DEFAULT if interactive and not incomplete else None\n        else:\n            video_title = info_dict.get('title') or info_dict.get('id') or 'entry'\n            filter_str = ') | ('.join(map(str.strip, filters))\n            return f'{video_title} does not pass filter ({filter_str}), skipping ..'\n    return _match_func\n\n\nclass download_range_func:\n    def __init__(self, chapters, ranges, from_info=False):\n        self.chapters, self.ranges, self.from_info = chapters, ranges, from_info\n\n    def __call__(self, info_dict, ydl):\n\n        warning = ('There are no chapters matching the regex' if info_dict.get('chapters')\n                   else 'Cannot match chapters since chapter information is unavailable')\n        for regex in self.chapters or []:\n            for i, chapter in enumerate(info_dict.get('chapters') or []):\n                if re.search(regex, chapter['title']):\n                    warning = None\n                    yield {**chapter, 'index': i}\n        if self.chapters and warning:\n            ydl.to_screen(f'[info] {info_dict[\"id\"]}: {warning}')\n\n        for start, end in self.ranges or []:\n            yield {\n                'start_time': self._handle_negative_timestamp(start, info_dict),\n                'end_time': self._handle_negative_timestamp(end, info_dict),\n            }\n\n        if self.from_info and (info_dict.get('start_time') or info_dict.get('end_time')):\n            yield {\n                'start_time': info_dict.get('start_time') or 0,\n                'end_time': info_dict.get('end_time') or float('inf'),\n            }\n        elif not self.ranges and not self.chapters:\n            yield {}\n\n    @staticmethod\n    def _handle_negative_timestamp(time, info):\n        return max(info['duration'] + time, 0) if info.get('duration') and time < 0 else time\n\n    def __eq__(self, other):\n        return (isinstance(other, download_range_func)\n                and self.chapters == other.chapters and self.ranges == other.ranges)\n\n    def __repr__(self):\n        return f'{__name__}.{type(self).__name__}({self.chapters}, {self.ranges})'\n\n\ndef parse_dfxp_time_expr(time_expr):\n    if not time_expr:\n        return\n\n    mobj = re.match(rf'^(?P<time_offset>{NUMBER_RE})s?$', time_expr)\n    if mobj:\n        return float(mobj.group('time_offset'))\n\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)\n    if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))\n\n\ndef srt_subtitles_timecode(seconds):\n    return '%02d:%02d:%02d,%03d' % timetuple_from_msec(seconds * 1000)\n\n\ndef ass_subtitles_timecode(seconds):\n    time = timetuple_from_msec(seconds * 1000)\n    return '%01d:%02d:%02d.%02d' % (*time[:-1], time.milliseconds / 10)\n\n\ndef dfxp2srt(dfxp_data):\n    '''\n    @param dfxp_data A bytes-like object containing DFXP data\n    @returns A unicode object containing converted SRT data\n    '''\n    LEGACY_NAMESPACES = (\n        (b'http://www.w3.org/ns/ttml', [\n            b'http://www.w3.org/2004/11/ttaf1',\n            b'http://www.w3.org/2006/04/ttaf1',\n            b'http://www.w3.org/2006/10/ttaf1',\n        ]),\n        (b'http://www.w3.org/ns/ttml#styling', [\n            b'http://www.w3.org/ns/ttml#style',\n        ]),\n    )\n\n    SUPPORTED_STYLING = [\n        'color',\n        'fontFamily',\n        'fontSize',\n        'fontStyle',\n        'fontWeight',\n        'textDecoration'\n    ]\n\n    _x = functools.partial(xpath_with_ns, ns_map={\n        'xml': 'http://www.w3.org/XML/1998/namespace',\n        'ttml': 'http://www.w3.org/ns/ttml',\n        'tts': 'http://www.w3.org/ns/ttml#styling',\n    })\n\n    styles = {}\n    default_style = {}\n\n    class TTMLPElementParser:\n        _out = ''\n        _unclosed_elements = []\n        _applied_styles = []\n\n        def start(self, tag, attrib):\n            if tag in (_x('ttml:br'), 'br'):\n                self._out += '\\n'\n            else:\n                unclosed_elements = []\n                style = {}\n                element_style_id = attrib.get('style')\n                if default_style:\n                    style.update(default_style)\n                if element_style_id:\n                    style.update(styles.get(element_style_id, {}))\n                for prop in SUPPORTED_STYLING:\n                    prop_val = attrib.get(_x('tts:' + prop))\n                    if prop_val:\n                        style[prop] = prop_val\n                if style:\n                    font = ''\n                    for k, v in sorted(style.items()):\n                        if self._applied_styles and self._applied_styles[-1].get(k) == v:\n                            continue\n                        if k == 'color':\n                            font += ' color=\"%s\"' % v\n                        elif k == 'fontSize':\n                            font += ' size=\"%s\"' % v\n                        elif k == 'fontFamily':\n                            font += ' face=\"%s\"' % v\n                        elif k == 'fontWeight' and v == 'bold':\n                            self._out += '<b>'\n                            unclosed_elements.append('b')\n                        elif k == 'fontStyle' and v == 'italic':\n                            self._out += '<i>'\n                            unclosed_elements.append('i')\n                        elif k == 'textDecoration' and v == 'underline':\n                            self._out += '<u>'\n                            unclosed_elements.append('u')\n                    if font:\n                        self._out += '<font' + font + '>'\n                        unclosed_elements.append('font')\n                    applied_style = {}\n                    if self._applied_styles:\n                        applied_style.update(self._applied_styles[-1])\n                    applied_style.update(style)\n                    self._applied_styles.append(applied_style)\n                self._unclosed_elements.append(unclosed_elements)\n\n        def end(self, tag):\n            if tag not in (_x('ttml:br'), 'br'):\n                unclosed_elements = self._unclosed_elements.pop()\n                for element in reversed(unclosed_elements):\n                    self._out += '</%s>' % element\n                if unclosed_elements and self._applied_styles:\n                    self._applied_styles.pop()\n\n        def data(self, data):\n            self._out += data\n\n        def close(self):\n            return self._out.strip()\n\n    # Fix UTF-8 encoded file wrongly marked as UTF-16. See https://github.com/yt-dlp/yt-dlp/issues/6543#issuecomment-1477169870\n    # This will not trigger false positives since only UTF-8 text is being replaced\n    dfxp_data = dfxp_data.replace(b'encoding=\\'UTF-16\\'', b'encoding=\\'UTF-8\\'')\n\n    def parse_node(node):\n        target = TTMLPElementParser()\n        parser = xml.etree.ElementTree.XMLParser(target=target)\n        parser.feed(xml.etree.ElementTree.tostring(node))\n        return parser.close()\n\n    for k, v in LEGACY_NAMESPACES:\n        for ns in v:\n            dfxp_data = dfxp_data.replace(ns, k)\n\n    dfxp = compat_etree_fromstring(dfxp_data)\n    out = []\n    paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')\n\n    if not paras:\n        raise ValueError('Invalid dfxp/TTML subtitle')\n\n    repeat = False\n    while True:\n        for style in dfxp.findall(_x('.//ttml:style')):\n            style_id = style.get('id') or style.get(_x('xml:id'))\n            if not style_id:\n                continue\n            parent_style_id = style.get('style')\n            if parent_style_id:\n                if parent_style_id not in styles:\n                    repeat = True\n                    continue\n                styles[style_id] = styles[parent_style_id].copy()\n            for prop in SUPPORTED_STYLING:\n                prop_val = style.get(_x('tts:' + prop))\n                if prop_val:\n                    styles.setdefault(style_id, {})[prop] = prop_val\n        if repeat:\n            repeat = False\n        else:\n            break\n\n    for p in ('body', 'div'):\n        ele = xpath_element(dfxp, [_x('.//ttml:' + p), './/' + p])\n        if ele is None:\n            continue\n        style = styles.get(ele.get('style'))\n        if not style:\n            continue\n        default_style.update(style)\n\n    for para, index in zip(paras, itertools.count(1)):\n        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))\n        end_time = parse_dfxp_time_expr(para.attrib.get('end'))\n        dur = parse_dfxp_time_expr(para.attrib.get('dur'))\n        if begin_time is None:\n            continue\n        if not end_time:\n            if not dur:\n                continue\n            end_time = begin_time + dur\n        out.append('%d\\n%s --> %s\\n%s\\n\\n' % (\n            index,\n            srt_subtitles_timecode(begin_time),\n            srt_subtitles_timecode(end_time),\n            parse_node(para)))\n\n    return ''.join(out)\n\n\ndef cli_option(params, command_option, param, separator=None):\n    param = params.get(param)\n    return ([] if param is None\n            else [command_option, str(param)] if separator is None\n            else [f'{command_option}{separator}{param}'])\n\n\ndef cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n    param = params.get(param)\n    assert param in (True, False, None)\n    return cli_option({True: true_value, False: false_value}, command_option, param, separator)\n\n\ndef cli_valueless_option(params, command_option, param, expected_value=True):\n    return [command_option] if params.get(param) == expected_value else []\n\n\ndef cli_configuration_args(argdict, keys, default=[], use_compat=True):\n    if isinstance(argdict, (list, tuple)):  # for backward compatibility\n        if use_compat:\n            return argdict\n        else:\n            argdict = None\n    if argdict is None:\n        return default\n    assert isinstance(argdict, dict)\n\n    assert isinstance(keys, (list, tuple))\n    for key_list in keys:\n        arg_list = list(filter(\n            lambda x: x is not None,\n            [argdict.get(key.lower()) for key in variadic(key_list)]))\n        if arg_list:\n            return [arg for args in arg_list for arg in args]\n    return default\n\n\ndef _configuration_args(main_key, argdict, exe, keys=None, default=[], use_compat=True):\n    main_key, exe = main_key.lower(), exe.lower()\n    root_key = exe if main_key == exe else f'{main_key}+{exe}'\n    keys = [f'{root_key}{k}' for k in (keys or [''])]\n    if root_key in keys:\n        if main_key != exe:\n            keys.append((main_key, exe))\n        keys.append('default')\n    else:\n        use_compat = False\n    return cli_configuration_args(argdict, keys, default, use_compat)\n\n\nclass ISO639Utils:\n    # See http://www.loc.gov/standards/iso639-2/ISO-639-2_utf-8.txt\n    _lang_map = {\n        'aa': 'aar',\n        'ab': 'abk',\n        'ae': 'ave',\n        'af': 'afr',\n        'ak': 'aka',\n        'am': 'amh',\n        'an': 'arg',\n        'ar': 'ara',\n        'as': 'asm',\n        'av': 'ava',\n        'ay': 'aym',\n        'az': 'aze',\n        'ba': 'bak',\n        'be': 'bel',\n        'bg': 'bul',\n        'bh': 'bih',\n        'bi': 'bis',\n        'bm': 'bam',\n        'bn': 'ben',\n        'bo': 'bod',\n        'br': 'bre',\n        'bs': 'bos',\n        'ca': 'cat',\n        'ce': 'che',\n        'ch': 'cha',\n        'co': 'cos',\n        'cr': 'cre',\n        'cs': 'ces',\n        'cu': 'chu',\n        'cv': 'chv',\n        'cy': 'cym',\n        'da': 'dan',\n        'de': 'deu',\n        'dv': 'div',\n        'dz': 'dzo',\n        'ee': 'ewe',\n        'el': 'ell',\n        'en': 'eng',\n        'eo': 'epo',\n        'es': 'spa',\n        'et': 'est',\n        'eu': 'eus',\n        'fa': 'fas',\n        'ff': 'ful',\n        'fi': 'fin',\n        'fj': 'fij',\n        'fo': 'fao',\n        'fr': 'fra',\n        'fy': 'fry',\n        'ga': 'gle',\n        'gd': 'gla',\n        'gl': 'glg',\n        'gn': 'grn',\n        'gu': 'guj',\n        'gv': 'glv',\n        'ha': 'hau',\n        'he': 'heb',\n        'iw': 'heb',  # Replaced by he in 1989 revision\n        'hi': 'hin',\n        'ho': 'hmo',\n        'hr': 'hrv',\n        'ht': 'hat',\n        'hu': 'hun',\n        'hy': 'hye',\n        'hz': 'her',\n        'ia': 'ina',\n        'id': 'ind',\n        'in': 'ind',  # Replaced by id in 1989 revision\n        'ie': 'ile',\n        'ig': 'ibo',\n        'ii': 'iii',\n        'ik': 'ipk',\n        'io': 'ido',\n        'is': 'isl',\n        'it': 'ita',\n        'iu': 'iku',\n        'ja': 'jpn',\n        'jv': 'jav',\n        'ka': 'kat',\n        'kg': 'kon',\n        'ki': 'kik',\n        'kj': 'kua',\n        'kk': 'kaz',\n        'kl': 'kal',\n        'km': 'khm',\n        'kn': 'kan',\n        'ko': 'kor',\n        'kr': 'kau',\n        'ks': 'kas',\n        'ku': 'kur',\n        'kv': 'kom',\n        'kw': 'cor',\n        'ky': 'kir',\n        'la': 'lat',\n        'lb': 'ltz',\n        'lg': 'lug',\n        'li': 'lim',\n        'ln': 'lin',\n        'lo': 'lao',\n        'lt': 'lit',\n        'lu': 'lub',\n        'lv': 'lav',\n        'mg': 'mlg',\n        'mh': 'mah',\n        'mi': 'mri',\n        'mk': 'mkd',\n        'ml': 'mal',\n        'mn': 'mon',\n        'mr': 'mar',\n        'ms': 'msa',\n        'mt': 'mlt',\n        'my': 'mya',\n        'na': 'nau',\n        'nb': 'nob',\n        'nd': 'nde',\n        'ne': 'nep',\n        'ng': 'ndo',\n        'nl': 'nld',\n        'nn': 'nno',\n        'no': 'nor',\n        'nr': 'nbl',\n        'nv': 'nav',\n        'ny': 'nya',\n        'oc': 'oci',\n        'oj': 'oji',\n        'om': 'orm',\n        'or': 'ori',\n        'os': 'oss',\n        'pa': 'pan',\n        'pe': 'per',\n        'pi': 'pli',\n        'pl': 'pol',\n        'ps': 'pus',\n        'pt': 'por',\n        'qu': 'que',\n        'rm': 'roh',\n        'rn': 'run',\n        'ro': 'ron',\n        'ru': 'rus',\n        'rw': 'kin',\n        'sa': 'san',\n        'sc': 'srd',\n        'sd': 'snd',\n        'se': 'sme',\n        'sg': 'sag',\n        'si': 'sin',\n        'sk': 'slk',\n        'sl': 'slv',\n        'sm': 'smo',\n        'sn': 'sna',\n        'so': 'som',\n        'sq': 'sqi',\n        'sr': 'srp',\n        'ss': 'ssw',\n        'st': 'sot',\n        'su': 'sun',\n        'sv': 'swe',\n        'sw': 'swa',\n        'ta': 'tam',\n        'te': 'tel',\n        'tg': 'tgk',\n        'th': 'tha',\n        'ti': 'tir',\n        'tk': 'tuk',\n        'tl': 'tgl',\n        'tn': 'tsn',\n        'to': 'ton',\n        'tr': 'tur',\n        'ts': 'tso',\n        'tt': 'tat',\n        'tw': 'twi',\n        'ty': 'tah',\n        'ug': 'uig',\n        'uk': 'ukr',\n        'ur': 'urd',\n        'uz': 'uzb',\n        've': 'ven',\n        'vi': 'vie',\n        'vo': 'vol',\n        'wa': 'wln',\n        'wo': 'wol',\n        'xh': 'xho',\n        'yi': 'yid',\n        'ji': 'yid',  # Replaced by yi in 1989 revision\n        'yo': 'yor',\n        'za': 'zha',\n        'zh': 'zho',\n        'zu': 'zul',\n    }\n\n    @classmethod\n    def short2long(cls, code):\n        \"\"\"Convert language code from ISO 639-1 to ISO 639-2/T\"\"\"\n        return cls._lang_map.get(code[:2])\n\n    @classmethod\n    def long2short(cls, code):\n        \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"\n        for short_name, long_name in cls._lang_map.items():\n            if long_name == code:\n                return short_name\n\n\nclass ISO3166Utils:\n    # From http://data.okfn.org/data/core/country-list\n    _country_map = {\n        'AF': 'Afghanistan',\n        'AX': '\u00c5land Islands',\n        'AL': 'Albania',\n        'DZ': 'Algeria',\n        'AS': 'American Samoa',\n        'AD': 'Andorra',\n        'AO': 'Angola',\n        'AI': 'Anguilla',\n        'AQ': 'Antarctica',\n        'AG': 'Antigua and Barbuda',\n        'AR': 'Argentina',\n        'AM': 'Armenia',\n        'AW': 'Aruba',\n        'AU': 'Australia',\n        'AT': 'Austria',\n        'AZ': 'Azerbaijan',\n        'BS': 'Bahamas',\n        'BH': 'Bahrain',\n        'BD': 'Bangladesh',\n        'BB': 'Barbados',\n        'BY': 'Belarus',\n        'BE': 'Belgium',\n        'BZ': 'Belize',\n        'BJ': 'Benin',\n        'BM': 'Bermuda',\n        'BT': 'Bhutan',\n        'BO': 'Bolivia, Plurinational State of',\n        'BQ': 'Bonaire, Sint Eustatius and Saba',\n        'BA': 'Bosnia and Herzegovina',\n        'BW': 'Botswana',\n        'BV': 'Bouvet Island',\n        'BR': 'Brazil',\n        'IO': 'British Indian Ocean Territory',\n        'BN': 'Brunei Darussalam',\n        'BG': 'Bulgaria',\n        'BF': 'Burkina Faso',\n        'BI': 'Burundi',\n        'KH': 'Cambodia',\n        'CM': 'Cameroon',\n        'CA': 'Canada',\n        'CV': 'Cape Verde',\n        'KY': 'Cayman Islands',\n        'CF': 'Central African Republic',\n        'TD': 'Chad',\n        'CL': 'Chile',\n        'CN': 'China',\n        'CX': 'Christmas Island',\n        'CC': 'Cocos (Keeling) Islands',\n        'CO': 'Colombia',\n        'KM': 'Comoros',\n        'CG': 'Congo',\n        'CD': 'Congo, the Democratic Republic of the',\n        'CK': 'Cook Islands',\n        'CR': 'Costa Rica',\n        'CI': 'C\u00f4te d\\'Ivoire',\n        'HR': 'Croatia',\n        'CU': 'Cuba',\n        'CW': 'Cura\u00e7ao',\n        'CY': 'Cyprus',\n        'CZ': 'Czech Republic',\n        'DK': 'Denmark',\n        'DJ': 'Djibouti',\n        'DM': 'Dominica',\n        'DO': 'Dominican Republic',\n        'EC': 'Ecuador',\n        'EG': 'Egypt',\n        'SV': 'El Salvador',\n        'GQ': 'Equatorial Guinea',\n        'ER': 'Eritrea',\n        'EE': 'Estonia',\n        'ET': 'Ethiopia',\n        'FK': 'Falkland Islands (Malvinas)',\n        'FO': 'Faroe Islands',\n        'FJ': 'Fiji',\n        'FI': 'Finland',\n        'FR': 'France',\n        'GF': 'French Guiana',\n        'PF': 'French Polynesia',\n        'TF': 'French Southern Territories',\n        'GA': 'Gabon',\n        'GM': 'Gambia',\n        'GE': 'Georgia',\n        'DE': 'Germany',\n        'GH': 'Ghana',\n        'GI': 'Gibraltar',\n        'GR': 'Greece',\n        'GL': 'Greenland',\n        'GD': 'Grenada',\n        'GP': 'Guadeloupe',\n        'GU': 'Guam',\n        'GT': 'Guatemala',\n        'GG': 'Guernsey',\n        'GN': 'Guinea',\n        'GW': 'Guinea-Bissau',\n        'GY': 'Guyana',\n        'HT': 'Haiti',\n        'HM': 'Heard Island and McDonald Islands',\n        'VA': 'Holy See (Vatican City State)',\n        'HN': 'Honduras',\n        'HK': 'Hong Kong',\n        'HU': 'Hungary',\n        'IS': 'Iceland',\n        'IN': 'India',\n        'ID': 'Indonesia',\n        'IR': 'Iran, Islamic Republic of',\n        'IQ': 'Iraq',\n        'IE': 'Ireland',\n        'IM': 'Isle of Man',\n        'IL': 'Israel',\n        'IT': 'Italy',\n        'JM': 'Jamaica',\n        'JP': 'Japan',\n        'JE': 'Jersey',\n        'JO': 'Jordan',\n        'KZ': 'Kazakhstan',\n        'KE': 'Kenya',\n        'KI': 'Kiribati',\n        'KP': 'Korea, Democratic People\\'s Republic of',\n        'KR': 'Korea, Republic of',\n        'KW': 'Kuwait',\n        'KG': 'Kyrgyzstan',\n        'LA': 'Lao People\\'s Democratic Republic',\n        'LV': 'Latvia',\n        'LB': 'Lebanon',\n        'LS': 'Lesotho',\n        'LR': 'Liberia',\n        'LY': 'Libya',\n        'LI': 'Liechtenstein',\n        'LT': 'Lithuania',\n        'LU': 'Luxembourg',\n        'MO': 'Macao',\n        'MK': 'Macedonia, the Former Yugoslav Republic of',\n        'MG': 'Madagascar',\n        'MW': 'Malawi',\n        'MY': 'Malaysia',\n        'MV': 'Maldives',\n        'ML': 'Mali',\n        'MT': 'Malta',\n        'MH': 'Marshall Islands',\n        'MQ': 'Martinique',\n        'MR': 'Mauritania',\n        'MU': 'Mauritius',\n        'YT': 'Mayotte',\n        'MX': 'Mexico',\n        'FM': 'Micronesia, Federated States of',\n        'MD': 'Moldova, Republic of',\n        'MC': 'Monaco',\n        'MN': 'Mongolia',\n        'ME': 'Montenegro',\n        'MS': 'Montserrat',\n        'MA': 'Morocco',\n        'MZ': 'Mozambique',\n        'MM': 'Myanmar',\n        'NA': 'Namibia',\n        'NR': 'Nauru',\n        'NP': 'Nepal',\n        'NL': 'Netherlands',\n        'NC': 'New Caledonia',\n        'NZ': 'New Zealand',\n        'NI': 'Nicaragua',\n        'NE': 'Niger',\n        'NG': 'Nigeria',\n        'NU': 'Niue',\n        'NF': 'Norfolk Island',\n        'MP': 'Northern Mariana Islands',\n        'NO': 'Norway',\n        'OM': 'Oman',\n        'PK': 'Pakistan',\n        'PW': 'Palau',\n        'PS': 'Palestine, State of',\n        'PA': 'Panama',\n        'PG': 'Papua New Guinea',\n        'PY': 'Paraguay',\n        'PE': 'Peru',\n        'PH': 'Philippines',\n        'PN': 'Pitcairn',\n        'PL': 'Poland',\n        'PT': 'Portugal',\n        'PR': 'Puerto Rico',\n        'QA': 'Qatar',\n        'RE': 'R\u00e9union',\n        'RO': 'Romania',\n        'RU': 'Russian Federation',\n        'RW': 'Rwanda',\n        'BL': 'Saint Barth\u00e9lemy',\n        'SH': 'Saint Helena, Ascension and Tristan da Cunha',\n        'KN': 'Saint Kitts and Nevis',\n        'LC': 'Saint Lucia',\n        'MF': 'Saint Martin (French part)',\n        'PM': 'Saint Pierre and Miquelon',\n        'VC': 'Saint Vincent and the Grenadines',\n        'WS': 'Samoa',\n        'SM': 'San Marino',\n        'ST': 'Sao Tome and Principe',\n        'SA': 'Saudi Arabia',\n        'SN': 'Senegal',\n        'RS': 'Serbia',\n        'SC': 'Seychelles',\n        'SL': 'Sierra Leone',\n        'SG': 'Singapore',\n        'SX': 'Sint Maarten (Dutch part)',\n        'SK': 'Slovakia',\n        'SI': 'Slovenia',\n        'SB': 'Solomon Islands',\n        'SO': 'Somalia',\n        'ZA': 'South Africa',\n        'GS': 'South Georgia and the South Sandwich Islands',\n        'SS': 'South Sudan',\n        'ES': 'Spain',\n        'LK': 'Sri Lanka',\n        'SD': 'Sudan',\n        'SR': 'Suriname',\n        'SJ': 'Svalbard and Jan Mayen',\n        'SZ': 'Swaziland',\n        'SE': 'Sweden',\n        'CH': 'Switzerland',\n        'SY': 'Syrian Arab Republic',\n        'TW': 'Taiwan, Province of China',\n        'TJ': 'Tajikistan',\n        'TZ': 'Tanzania, United Republic of',\n        'TH': 'Thailand',\n        'TL': 'Timor-Leste',\n        'TG': 'Togo',\n        'TK': 'Tokelau',\n        'TO': 'Tonga',\n        'TT': 'Trinidad and Tobago',\n        'TN': 'Tunisia',\n        'TR': 'Turkey',\n        'TM': 'Turkmenistan',\n        'TC': 'Turks and Caicos Islands',\n        'TV': 'Tuvalu',\n        'UG': 'Uganda',\n        'UA': 'Ukraine',\n        'AE': 'United Arab Emirates',\n        'GB': 'United Kingdom',\n        'US': 'United States',\n        'UM': 'United States Minor Outlying Islands',\n        'UY': 'Uruguay',\n        'UZ': 'Uzbekistan',\n        'VU': 'Vanuatu',\n        'VE': 'Venezuela, Bolivarian Republic of',\n        'VN': 'Viet Nam',\n        'VG': 'Virgin Islands, British',\n        'VI': 'Virgin Islands, U.S.',\n        'WF': 'Wallis and Futuna',\n        'EH': 'Western Sahara',\n        'YE': 'Yemen',\n        'ZM': 'Zambia',\n        'ZW': 'Zimbabwe',\n        # Not ISO 3166 codes, but used for IP blocks\n        'AP': 'Asia/Pacific Region',\n        'EU': 'Europe',\n    }\n\n    @classmethod\n    def short2full(cls, code):\n        \"\"\"Convert an ISO 3166-2 country code to the corresponding full name\"\"\"\n        return cls._country_map.get(code.upper())\n\n\nclass GeoUtils:\n    # Major IPv4 address blocks per country\n    _country_ip_map = {\n        'AD': '46.172.224.0/19',\n        'AE': '94.200.0.0/13',\n        'AF': '149.54.0.0/17',\n        'AG': '209.59.64.0/18',\n        'AI': '204.14.248.0/21',\n        'AL': '46.99.0.0/16',\n        'AM': '46.70.0.0/15',\n        'AO': '105.168.0.0/13',\n        'AP': '182.50.184.0/21',\n        'AQ': '23.154.160.0/24',\n        'AR': '181.0.0.0/12',\n        'AS': '202.70.112.0/20',\n        'AT': '77.116.0.0/14',\n        'AU': '1.128.0.0/11',\n        'AW': '181.41.0.0/18',\n        'AX': '185.217.4.0/22',\n        'AZ': '5.197.0.0/16',\n        'BA': '31.176.128.0/17',\n        'BB': '65.48.128.0/17',\n        'BD': '114.130.0.0/16',\n        'BE': '57.0.0.0/8',\n        'BF': '102.178.0.0/15',\n        'BG': '95.42.0.0/15',\n        'BH': '37.131.0.0/17',\n        'BI': '154.117.192.0/18',\n        'BJ': '137.255.0.0/16',\n        'BL': '185.212.72.0/23',\n        'BM': '196.12.64.0/18',\n        'BN': '156.31.0.0/16',\n        'BO': '161.56.0.0/16',\n        'BQ': '161.0.80.0/20',\n        'BR': '191.128.0.0/12',\n        'BS': '24.51.64.0/18',\n        'BT': '119.2.96.0/19',\n        'BW': '168.167.0.0/16',\n        'BY': '178.120.0.0/13',\n        'BZ': '179.42.192.0/18',\n        'CA': '99.224.0.0/11',\n        'CD': '41.243.0.0/16',\n        'CF': '197.242.176.0/21',\n        'CG': '160.113.0.0/16',\n        'CH': '85.0.0.0/13',\n        'CI': '102.136.0.0/14',\n        'CK': '202.65.32.0/19',\n        'CL': '152.172.0.0/14',\n        'CM': '102.244.0.0/14',\n        'CN': '36.128.0.0/10',\n        'CO': '181.240.0.0/12',\n        'CR': '201.192.0.0/12',\n        'CU': '152.206.0.0/15',\n        'CV': '165.90.96.0/19',\n        'CW': '190.88.128.0/17',\n        'CY': '31.153.0.0/16',\n        'CZ': '88.100.0.0/14',\n        'DE': '53.0.0.0/8',\n        'DJ': '197.241.0.0/17',\n        'DK': '87.48.0.0/12',\n        'DM': '192.243.48.0/20',\n        'DO': '152.166.0.0/15',\n        'DZ': '41.96.0.0/12',\n        'EC': '186.68.0.0/15',\n        'EE': '90.190.0.0/15',\n        'EG': '156.160.0.0/11',\n        'ER': '196.200.96.0/20',\n        'ES': '88.0.0.0/11',\n        'ET': '196.188.0.0/14',\n        'EU': '2.16.0.0/13',\n        'FI': '91.152.0.0/13',\n        'FJ': '144.120.0.0/16',\n        'FK': '80.73.208.0/21',\n        'FM': '119.252.112.0/20',\n        'FO': '88.85.32.0/19',\n        'FR': '90.0.0.0/9',\n        'GA': '41.158.0.0/15',\n        'GB': '25.0.0.0/8',\n        'GD': '74.122.88.0/21',\n        'GE': '31.146.0.0/16',\n        'GF': '161.22.64.0/18',\n        'GG': '62.68.160.0/19',\n        'GH': '154.160.0.0/12',\n        'GI': '95.164.0.0/16',\n        'GL': '88.83.0.0/19',\n        'GM': '160.182.0.0/15',\n        'GN': '197.149.192.0/18',\n        'GP': '104.250.0.0/19',\n        'GQ': '105.235.224.0/20',\n        'GR': '94.64.0.0/13',\n        'GT': '168.234.0.0/16',\n        'GU': '168.123.0.0/16',\n        'GW': '197.214.80.0/20',\n        'GY': '181.41.64.0/18',\n        'HK': '113.252.0.0/14',\n        'HN': '181.210.0.0/16',\n        'HR': '93.136.0.0/13',\n        'HT': '148.102.128.0/17',\n        'HU': '84.0.0.0/14',\n        'ID': '39.192.0.0/10',\n        'IE': '87.32.0.0/12',\n        'IL': '79.176.0.0/13',\n        'IM': '5.62.80.0/20',\n        'IN': '117.192.0.0/10',\n        'IO': '203.83.48.0/21',\n        'IQ': '37.236.0.0/14',\n        'IR': '2.176.0.0/12',\n        'IS': '82.221.0.0/16',\n        'IT': '79.0.0.0/10',\n        'JE': '87.244.64.0/18',\n        'JM': '72.27.0.0/17',\n        'JO': '176.29.0.0/16',\n        'JP': '133.0.0.0/8',\n        'KE': '105.48.0.0/12',\n        'KG': '158.181.128.0/17',\n        'KH': '36.37.128.0/17',\n        'KI': '103.25.140.0/22',\n        'KM': '197.255.224.0/20',\n        'KN': '198.167.192.0/19',\n        'KP': '175.45.176.0/22',\n        'KR': '175.192.0.0/10',\n        'KW': '37.36.0.0/14',\n        'KY': '64.96.0.0/15',\n        'KZ': '2.72.0.0/13',\n        'LA': '115.84.64.0/18',\n        'LB': '178.135.0.0/16',\n        'LC': '24.92.144.0/20',\n        'LI': '82.117.0.0/19',\n        'LK': '112.134.0.0/15',\n        'LR': '102.183.0.0/16',\n        'LS': '129.232.0.0/17',\n        'LT': '78.56.0.0/13',\n        'LU': '188.42.0.0/16',\n        'LV': '46.109.0.0/16',\n        'LY': '41.252.0.0/14',\n        'MA': '105.128.0.0/11',\n        'MC': '88.209.64.0/18',\n        'MD': '37.246.0.0/16',\n        'ME': '178.175.0.0/17',\n        'MF': '74.112.232.0/21',\n        'MG': '154.126.0.0/17',\n        'MH': '117.103.88.0/21',\n        'MK': '77.28.0.0/15',\n        'ML': '154.118.128.0/18',\n        'MM': '37.111.0.0/17',\n        'MN': '49.0.128.0/17',\n        'MO': '60.246.0.0/16',\n        'MP': '202.88.64.0/20',\n        'MQ': '109.203.224.0/19',\n        'MR': '41.188.64.0/18',\n        'MS': '208.90.112.0/22',\n        'MT': '46.11.0.0/16',\n        'MU': '105.16.0.0/12',\n        'MV': '27.114.128.0/18',\n        'MW': '102.70.0.0/15',\n        'MX': '187.192.0.0/11',\n        'MY': '175.136.0.0/13',\n        'MZ': '197.218.0.0/15',\n        'NA': '41.182.0.0/16',\n        'NC': '101.101.0.0/18',\n        'NE': '197.214.0.0/18',\n        'NF': '203.17.240.0/22',\n        'NG': '105.112.0.0/12',\n        'NI': '186.76.0.0/15',\n        'NL': '145.96.0.0/11',\n        'NO': '84.208.0.0/13',\n        'NP': '36.252.0.0/15',\n        'NR': '203.98.224.0/19',\n        'NU': '49.156.48.0/22',\n        'NZ': '49.224.0.0/14',\n        'OM': '5.36.0.0/15',\n        'PA': '186.72.0.0/15',\n        'PE': '186.160.0.0/14',\n        'PF': '123.50.64.0/18',\n        'PG': '124.240.192.0/19',\n        'PH': '49.144.0.0/13',\n        'PK': '39.32.0.0/11',\n        'PL': '83.0.0.0/11',\n        'PM': '70.36.0.0/20',\n        'PR': '66.50.0.0/16',\n        'PS': '188.161.0.0/16',\n        'PT': '85.240.0.0/13',\n        'PW': '202.124.224.0/20',\n        'PY': '181.120.0.0/14',\n        'QA': '37.210.0.0/15',\n        'RE': '102.35.0.0/16',\n        'RO': '79.112.0.0/13',\n        'RS': '93.86.0.0/15',\n        'RU': '5.136.0.0/13',\n        'RW': '41.186.0.0/16',\n        'SA': '188.48.0.0/13',\n        'SB': '202.1.160.0/19',\n        'SC': '154.192.0.0/11',\n        'SD': '102.120.0.0/13',\n        'SE': '78.64.0.0/12',\n        'SG': '8.128.0.0/10',\n        'SI': '188.196.0.0/14',\n        'SK': '78.98.0.0/15',\n        'SL': '102.143.0.0/17',\n        'SM': '89.186.32.0/19',\n        'SN': '41.82.0.0/15',\n        'SO': '154.115.192.0/18',\n        'SR': '186.179.128.0/17',\n        'SS': '105.235.208.0/21',\n        'ST': '197.159.160.0/19',\n        'SV': '168.243.0.0/16',\n        'SX': '190.102.0.0/20',\n        'SY': '5.0.0.0/16',\n        'SZ': '41.84.224.0/19',\n        'TC': '65.255.48.0/20',\n        'TD': '154.68.128.0/19',\n        'TG': '196.168.0.0/14',\n        'TH': '171.96.0.0/13',\n        'TJ': '85.9.128.0/18',\n        'TK': '27.96.24.0/21',\n        'TL': '180.189.160.0/20',\n        'TM': '95.85.96.0/19',\n        'TN': '197.0.0.0/11',\n        'TO': '175.176.144.0/21',\n        'TR': '78.160.0.0/11',\n        'TT': '186.44.0.0/15',\n        'TV': '202.2.96.0/19',\n        'TW': '120.96.0.0/11',\n        'TZ': '156.156.0.0/14',\n        'UA': '37.52.0.0/14',\n        'UG': '102.80.0.0/13',\n        'US': '6.0.0.0/8',\n        'UY': '167.56.0.0/13',\n        'UZ': '84.54.64.0/18',\n        'VA': '212.77.0.0/19',\n        'VC': '207.191.240.0/21',\n        'VE': '186.88.0.0/13',\n        'VG': '66.81.192.0/20',\n        'VI': '146.226.0.0/16',\n        'VN': '14.160.0.0/11',\n        'VU': '202.80.32.0/20',\n        'WF': '117.20.32.0/21',\n        'WS': '202.4.32.0/19',\n        'YE': '134.35.0.0/16',\n        'YT': '41.242.116.0/22',\n        'ZA': '41.0.0.0/11',\n        'ZM': '102.144.0.0/13',\n        'ZW': '102.177.192.0/18',\n    }\n\n    @classmethod\n    def random_ipv4(cls, code_or_block):\n        if len(code_or_block) == 2:\n            block = cls._country_ip_map.get(code_or_block.upper())\n            if not block:\n                return None\n        else:\n            block = code_or_block\n        addr, preflen = block.split('/')\n        addr_min = struct.unpack('!L', socket.inet_aton(addr))[0]\n        addr_max = addr_min | (0xffffffff >> int(preflen))\n        return str(socket.inet_ntoa(\n            struct.pack('!L', random.randint(addr_min, addr_max))))\n\n\n# Both long_to_bytes and bytes_to_long are adapted from PyCrypto, which is\n# released into Public Domain\n# https://github.com/dlitz/pycrypto/blob/master/lib/Crypto/Util/number.py#L387\n\ndef long_to_bytes(n, blocksize=0):\n    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n    Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front of the\n    byte string with binary zeros so that the length is a multiple of\n    blocksize.\n    \"\"\"\n    # after much testing, this algorithm was deemed to be the fastest\n    s = b''\n    n = int(n)\n    while n > 0:\n        s = struct.pack('>I', n & 0xffffffff) + s\n        n = n >> 32\n    # strip off leading zeros\n    for i in range(len(s)):\n        if s[i] != b'\\000'[0]:\n            break\n    else:\n        # only happens when n == 0\n        s = b'\\000'\n        i = 0\n    s = s[i:]\n    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n    # de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n    return s\n\n\ndef bytes_to_long(s):\n    \"\"\"bytes_to_long(string) : long\n    Convert a byte string to a long integer.\n\n    This is (essentially) the inverse of long_to_bytes().\n    \"\"\"\n    acc = 0\n    length = len(s)\n    if length % 4:\n        extra = (4 - length % 4)\n        s = b'\\000' * extra + s\n        length = length + extra\n    for i in range(0, length, 4):\n        acc = (acc << 32) + struct.unpack('>I', s[i:i + 4])[0]\n    return acc\n\n\ndef ohdave_rsa_encrypt(data, exponent, modulus):\n    '''\n    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n\n    Input:\n        data: data to encrypt, bytes-like object\n        exponent, modulus: parameter e and N of RSA algorithm, both integer\n    Output: hex string of encrypted data\n\n    Limitation: supports one block encryption only\n    '''\n\n    payload = int(binascii.hexlify(data[::-1]), 16)\n    encrypted = pow(payload, exponent, modulus)\n    return '%x' % encrypted\n\n\ndef pkcs1pad(data, length):\n    \"\"\"\n    Padding input data with PKCS#1 scheme\n\n    @param {int[]} data        input data\n    @param {int}   length      target length\n    @returns {int[]}           padded data\n    \"\"\"\n    if len(data) > length - 11:\n        raise ValueError('Input data too long for PKCS#1 padding')\n\n    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n    return [0, 2] + pseudo_random + [0] + data\n\n\ndef _base_n_table(n, table):\n    if not table and not n:\n        raise ValueError('Either table or n must be specified')\n    table = (table or '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')[:n]\n\n    if n and n != len(table):\n        raise ValueError(f'base {n} exceeds table length {len(table)}')\n    return table\n\n\ndef encode_base_n(num, n=None, table=None):\n    \"\"\"Convert given int to a base-n string\"\"\"\n    table = _base_n_table(n, table)\n    if not num:\n        return table[0]\n\n    result, base = '', len(table)\n    while num:\n        result = table[num % base] + result\n        num = num // base\n    return result\n\n\ndef decode_base_n(string, n=None, table=None):\n    \"\"\"Convert given base-n string to int\"\"\"\n    table = {char: index for index, char in enumerate(_base_n_table(n, table))}\n    result, base = 0, len(table)\n    for char in string:\n        result = result * base + table[char]\n    return result\n\n\ndef decode_packed_codes(code):\n    mobj = re.search(PACKED_CODES_RE, code)\n    obfuscated_code, base, count, symbols = mobj.groups()\n    base = int(base)\n    count = int(count)\n    symbols = symbols.split('|')\n    symbol_table = {}\n\n    while count:\n        count -= 1\n        base_n_count = encode_base_n(count, base)\n        symbol_table[base_n_count] = symbols[count] or base_n_count\n\n    return re.sub(\n        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n        obfuscated_code)\n\n\ndef caesar(s, alphabet, shift):\n    if shift == 0:\n        return s\n    l = len(alphabet)\n    return ''.join(\n        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n        for c in s)\n\n\ndef rot47(s):\n    return caesar(s, r'''!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~''', 47)\n\n\ndef parse_m3u8_attributes(attrib):\n    info = {}\n    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n        if val.startswith('\"'):\n            val = val[1:-1]\n        info[key] = val\n    return info\n\n\ndef urshift(val, n):\n    return val >> n if val >= 0 else (val + 0x100000000) >> n\n\n\ndef write_xattr(path, key, value):\n    # Windows: Write xattrs to NTFS Alternate Data Streams:\n    # http://en.wikipedia.org/wiki/NTFS#Alternate_data_streams_.28ADS.29\n    if compat_os_name == 'nt':\n        assert ':' not in key\n        assert os.path.exists(path)\n\n        try:\n            with open(f'{path}:{key}', 'wb') as f:\n                f.write(value)\n        except OSError as e:\n            raise XAttrMetadataError(e.errno, e.strerror)\n        return\n\n    # UNIX Method 1. Use xattrs/pyxattrs modules\n\n    setxattr = None\n    if getattr(xattr, '_yt_dlp__identifier', None) == 'pyxattr':\n        # Unicode arguments are not supported in pyxattr until version 0.5.0\n        # See https://github.com/ytdl-org/youtube-dl/issues/5498\n        if version_tuple(xattr.__version__) >= (0, 5, 0):\n            setxattr = xattr.set\n    elif xattr:\n        setxattr = xattr.setxattr\n\n    if setxattr:\n        try:\n            setxattr(path, key, value)\n        except OSError as e:\n            raise XAttrMetadataError(e.errno, e.strerror)\n        return\n\n    # UNIX Method 2. Use setfattr/xattr executables\n    exe = ('setfattr' if check_executable('setfattr', ['--version'])\n           else 'xattr' if check_executable('xattr', ['-h']) else None)\n    if not exe:\n        raise XAttrUnavailableError(\n            'Couldn\\'t find a tool to set the xattrs. Install either the python \"xattr\" or \"pyxattr\" modules or the '\n            + ('\"xattr\" binary' if sys.platform != 'linux' else 'GNU \"attr\" package (which contains the \"setfattr\" tool)'))\n\n    value = value.decode()\n    try:\n        _, stderr, returncode = Popen.run(\n            [exe, '-w', key, value, path] if exe == 'xattr' else [exe, '-n', key, '-v', value, path],\n            text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n    except OSError as e:\n        raise XAttrMetadataError(e.errno, e.strerror)\n    if returncode:\n        raise XAttrMetadataError(returncode, stderr)\n\n\ndef random_birthday(year_field, month_field, day_field):\n    start_date = datetime.date(1950, 1, 1)\n    end_date = datetime.date(1995, 12, 31)\n    offset = random.randint(0, (end_date - start_date).days)\n    random_date = start_date + datetime.timedelta(offset)\n    return {\n        year_field: str(random_date.year),\n        month_field: str(random_date.month),\n        day_field: str(random_date.day),\n    }\n\n\ndef find_available_port(interface=''):\n    try:\n        with socket.socket() as sock:\n            sock.bind((interface, 0))\n            return sock.getsockname()[1]\n    except OSError:\n        return None\n\n\n# Templates for internet shortcut files, which are plain text files.\nDOT_URL_LINK_TEMPLATE = '''\\\n[InternetShortcut]\nURL=%(url)s\n'''\n\nDOT_WEBLOC_LINK_TEMPLATE = '''\\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n\\t<key>URL</key>\n\\t<string>%(url)s</string>\n</dict>\n</plist>\n'''\n\nDOT_DESKTOP_LINK_TEMPLATE = '''\\\n[Desktop Entry]\nEncoding=UTF-8\nName=%(filename)s\nType=Link\nURL=%(url)s\nIcon=text-html\n'''\n\nLINK_TEMPLATES = {\n    'url': DOT_URL_LINK_TEMPLATE,\n    'desktop': DOT_DESKTOP_LINK_TEMPLATE,\n    'webloc': DOT_WEBLOC_LINK_TEMPLATE,\n}\n\n\ndef iri_to_uri(iri):\n    \"\"\"\n    Converts an IRI (Internationalized Resource Identifier, allowing Unicode characters) to a URI (Uniform Resource Identifier, ASCII-only).\n\n    The function doesn't add an additional layer of escaping; e.g., it doesn't escape `%3C` as `%253C`. Instead, it percent-escapes characters with an underlying UTF-8 encoding *besides* those already escaped, leaving the URI intact.\n    \"\"\"\n\n    iri_parts = urllib.parse.urlparse(iri)\n\n    if '[' in iri_parts.netloc:\n        raise ValueError('IPv6 URIs are not, yet, supported.')\n        # Querying `.netloc`, when there's only one bracket, also raises a ValueError.\n\n    # The `safe` argument values, that the following code uses, contain the characters that should not be percent-encoded. Everything else but letters, digits and '_.-' will be percent-encoded with an underlying UTF-8 encoding. Everything already percent-encoded will be left as is.\n\n    net_location = ''\n    if iri_parts.username:\n        net_location += urllib.parse.quote(iri_parts.username, safe=r\"!$%&'()*+,~\")\n        if iri_parts.password is not None:\n            net_location += ':' + urllib.parse.quote(iri_parts.password, safe=r\"!$%&'()*+,~\")\n        net_location += '@'\n\n    net_location += iri_parts.hostname.encode('idna').decode()  # Punycode for Unicode hostnames.\n    # The 'idna' encoding produces ASCII text.\n    if iri_parts.port is not None and iri_parts.port != 80:\n        net_location += ':' + str(iri_parts.port)\n\n    return urllib.parse.urlunparse(\n        (iri_parts.scheme,\n            net_location,\n\n            urllib.parse.quote_plus(iri_parts.path, safe=r\"!$%&'()*+,/:;=@|~\"),\n\n            # Unsure about the `safe` argument, since this is a legacy way of handling parameters.\n            urllib.parse.quote_plus(iri_parts.params, safe=r\"!$%&'()*+,/:;=@|~\"),\n\n            # Not totally sure about the `safe` argument, since the source does not explicitly mention the query URI component.\n            urllib.parse.quote_plus(iri_parts.query, safe=r\"!$%&'()*+,/:;=?@{|}~\"),\n\n            urllib.parse.quote_plus(iri_parts.fragment, safe=r\"!#$%&'()*+,/:;=?@{|}~\")))\n\n    # Source for `safe` arguments: https://url.spec.whatwg.org/#percent-encoded-bytes.\n\n\ndef to_high_limit_path(path):\n    if sys.platform in ['win32', 'cygwin']:\n        # Work around MAX_PATH limitation on Windows. The maximum allowed length for the individual path segments may still be quite limited.\n        return '\\\\\\\\?\\\\' + os.path.abspath(path)\n\n    return path\n\n\ndef format_field(obj, field=None, template='%s', ignore=NO_DEFAULT, default='', func=IDENTITY):\n    val = traversal.traverse_obj(obj, *variadic(field))\n    if not val if ignore is NO_DEFAULT else val in variadic(ignore):\n        return default\n    return template % func(val)\n\n\ndef clean_podcast_url(url):\n    url = re.sub(r'''(?x)\n        (?:\n            (?:\n                chtbl\\.com/track|\n                media\\.blubrry\\.com| # https://create.blubrry.com/resources/podcast-media-download-statistics/getting-started/\n                play\\.podtrac\\.com|\n                chrt\\.fm/track|\n                mgln\\.ai/e\n            )(?:/[^/.]+)?|\n            (?:dts|www)\\.podtrac\\.com/(?:pts/)?redirect\\.[0-9a-z]{3,4}| # http://analytics.podtrac.com/how-to-measure\n            flex\\.acast\\.com|\n            pd(?:\n                cn\\.co| # https://podcorn.com/analytics-prefix/\n                st\\.fm # https://podsights.com/docs/\n            )/e|\n            [0-9]\\.gum\\.fm|\n            pscrb\\.fm/rss/p\n        )/''', '', url)\n    return re.sub(r'^\\w+://(\\w+://)', r'\\1', url)\n\n\n_HEX_TABLE = '0123456789abcdef'\n\n\ndef random_uuidv4():\n    return re.sub(r'[xy]', lambda x: _HEX_TABLE[random.randint(0, 15)], 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx')\n\n\ndef make_dir(path, to_screen=None):\n    try:\n        dn = os.path.dirname(path)\n        if dn:\n            os.makedirs(dn, exist_ok=True)\n        return True\n    except OSError as err:\n        if callable(to_screen) is not None:\n            to_screen(f'unable to create directory {err}')\n        return False\n\n\ndef get_executable_path():\n    from ..update import _get_variant_and_executable_path\n\n    return os.path.dirname(os.path.abspath(_get_variant_and_executable_path()[1]))\n\n\ndef get_user_config_dirs(package_name):\n    # .config (e.g. ~/.config/package_name)\n    xdg_config_home = os.getenv('XDG_CONFIG_HOME') or compat_expanduser('~/.config')\n    yield os.path.join(xdg_config_home, package_name)\n\n    # appdata (%APPDATA%/package_name)\n    appdata_dir = os.getenv('appdata')\n    if appdata_dir:\n        yield os.path.join(appdata_dir, package_name)\n\n    # home (~/.package_name)\n    yield os.path.join(compat_expanduser('~'), f'.{package_name}')\n\n\ndef get_system_config_dirs(package_name):\n    # /etc/package_name\n    yield os.path.join('/etc', package_name)\n\n\ndef time_seconds(**kwargs):\n    \"\"\"\n    Returns TZ-aware time in seconds since the epoch (1970-01-01T00:00:00Z)\n    \"\"\"\n    return time.time() + datetime.timedelta(**kwargs).total_seconds()\n\n\n# create a JSON Web Signature (jws) with HS256 algorithm\n# the resulting format is in JWS Compact Serialization\n# implemented following JWT https://www.rfc-editor.org/rfc/rfc7519.html\n# implemented following JWS https://www.rfc-editor.org/rfc/rfc7515.html\ndef jwt_encode_hs256(payload_data, key, headers={}):\n    header_data = {\n        'alg': 'HS256',\n        'typ': 'JWT',\n    }\n    if headers:\n        header_data.update(headers)\n    header_b64 = base64.b64encode(json.dumps(header_data).encode())\n    payload_b64 = base64.b64encode(json.dumps(payload_data).encode())\n    h = hmac.new(key.encode(), header_b64 + b'.' + payload_b64, hashlib.sha256)\n    signature_b64 = base64.b64encode(h.digest())\n    token = header_b64 + b'.' + payload_b64 + b'.' + signature_b64\n    return token\n\n\n# can be extended in future to verify the signature and parse header and return the algorithm used if it's not HS256\ndef jwt_decode_hs256(jwt):\n    header_b64, payload_b64, signature_b64 = jwt.split('.')\n    # add trailing ='s that may have been stripped, superfluous ='s are ignored\n    payload_data = json.loads(base64.urlsafe_b64decode(f'{payload_b64}==='))\n    return payload_data\n\n\nWINDOWS_VT_MODE = False if compat_os_name == 'nt' else None\n\n\n@functools.cache\ndef supports_terminal_sequences(stream):\n    if compat_os_name == 'nt':\n        if not WINDOWS_VT_MODE:\n            return False\n    elif not os.getenv('TERM'):\n        return False\n    try:\n        return stream.isatty()\n    except BaseException:\n        return False\n\n\ndef windows_enable_vt_mode():\n    \"\"\"Ref: https://bugs.python.org/issue30075 \"\"\"\n    if get_windows_version() < (10, 0, 10586):\n        return\n\n    import ctypes\n    import ctypes.wintypes\n    import msvcrt\n\n    ENABLE_VIRTUAL_TERMINAL_PROCESSING = 0x0004\n\n    dll = ctypes.WinDLL('kernel32', use_last_error=False)\n    handle = os.open('CONOUT$', os.O_RDWR)\n    try:\n        h_out = ctypes.wintypes.HANDLE(msvcrt.get_osfhandle(handle))\n        dw_original_mode = ctypes.wintypes.DWORD()\n        success = dll.GetConsoleMode(h_out, ctypes.byref(dw_original_mode))\n        if not success:\n            raise Exception('GetConsoleMode failed')\n\n        success = dll.SetConsoleMode(h_out, ctypes.wintypes.DWORD(\n            dw_original_mode.value | ENABLE_VIRTUAL_TERMINAL_PROCESSING))\n        if not success:\n            raise Exception('SetConsoleMode failed')\n    finally:\n        os.close(handle)\n\n    global WINDOWS_VT_MODE\n    WINDOWS_VT_MODE = True\n    supports_terminal_sequences.cache_clear()\n\n\n_terminal_sequences_re = re.compile('\\033\\\\[[^m]+m')\n\n\ndef remove_terminal_sequences(string):\n    return _terminal_sequences_re.sub('', string)\n\n\ndef number_of_digits(number):\n    return len('%d' % number)\n\n\ndef join_nonempty(*values, delim='-', from_dict=None):\n    if from_dict is not None:\n        values = (traversal.traverse_obj(from_dict, variadic(v)) for v in values)\n    return delim.join(map(str, filter(None, values)))\n\n\ndef scale_thumbnails_to_max_format_width(formats, thumbnails, url_width_re):\n    \"\"\"\n    Find the largest format dimensions in terms of video width and, for each thumbnail:\n    * Modify the URL: Match the width with the provided regex and replace with the former width\n    * Update dimensions\n\n    This function is useful with video services that scale the provided thumbnails on demand\n    \"\"\"\n    _keys = ('width', 'height')\n    max_dimensions = max(\n        (tuple(format.get(k) or 0 for k in _keys) for format in formats),\n        default=(0, 0))\n    if not max_dimensions[0]:\n        return thumbnails\n    return [\n        merge_dicts(\n            {'url': re.sub(url_width_re, str(max_dimensions[0]), thumbnail['url'])},\n            dict(zip(_keys, max_dimensions)), thumbnail)\n        for thumbnail in thumbnails\n    ]\n\n\ndef parse_http_range(range):\n    \"\"\" Parse value of \"Range\" or \"Content-Range\" HTTP header into tuple. \"\"\"\n    if not range:\n        return None, None, None\n    crg = re.search(r'bytes[ =](\\d+)-(\\d+)?(?:/(\\d+))?', range)\n    if not crg:\n        return None, None, None\n    return int(crg.group(1)), int_or_none(crg.group(2)), int_or_none(crg.group(3))\n\n\ndef read_stdin(what):\n    eof = 'Ctrl+Z' if compat_os_name == 'nt' else 'Ctrl+D'\n    write_string(f'Reading {what} from STDIN - EOF ({eof}) to end:\\n')\n    return sys.stdin\n\n\ndef determine_file_encoding(data):\n    \"\"\"\n    Detect the text encoding used\n    @returns (encoding, bytes to skip)\n    \"\"\"\n\n    # BOM marks are given priority over declarations\n    for bom, enc in BOMS:\n        if data.startswith(bom):\n            return enc, len(bom)\n\n    # Strip off all null bytes to match even when UTF-16 or UTF-32 is used.\n    # We ignore the endianness to get a good enough match\n    data = data.replace(b'\\0', b'')\n    mobj = re.match(rb'(?m)^#\\s*coding\\s*:\\s*(\\S+)\\s*$', data)\n    return mobj.group(1).decode() if mobj else None, 0\n\n\nclass Config:\n    own_args = None\n    parsed_args = None\n    filename = None\n    __initialized = False\n\n    def __init__(self, parser, label=None):\n        self.parser, self.label = parser, label\n        self._loaded_paths, self.configs = set(), []\n\n    def init(self, args=None, filename=None):\n        assert not self.__initialized\n        self.own_args, self.filename = args, filename\n        return self.load_configs()\n\n    def load_configs(self):\n        directory = ''\n        if self.filename:\n            location = os.path.realpath(self.filename)\n            directory = os.path.dirname(location)\n            if location in self._loaded_paths:\n                return False\n            self._loaded_paths.add(location)\n\n        self.__initialized = True\n        opts, _ = self.parser.parse_known_args(self.own_args)\n        self.parsed_args = self.own_args\n        for location in opts.config_locations or []:\n            if location == '-':\n                if location in self._loaded_paths:\n                    continue\n                self._loaded_paths.add(location)\n                self.append_config(shlex.split(read_stdin('options'), comments=True), label='stdin')\n                continue\n            location = os.path.join(directory, expand_path(location))\n            if os.path.isdir(location):\n                location = os.path.join(location, 'yt-dlp.conf')\n            if not os.path.exists(location):\n                self.parser.error(f'config location {location} does not exist')\n            self.append_config(self.read_file(location), location)\n        return True\n\n    def __str__(self):\n        label = join_nonempty(\n            self.label, 'config', f'\"{self.filename}\"' if self.filename else '',\n            delim=' ')\n        return join_nonempty(\n            self.own_args is not None and f'{label[0].upper()}{label[1:]}: {self.hide_login_info(self.own_args)}',\n            *(f'\\n{c}'.replace('\\n', '\\n| ')[1:] for c in self.configs),\n            delim='\\n')\n\n    @staticmethod\n    def read_file(filename, default=[]):\n        try:\n            optionf = open(filename, 'rb')\n        except OSError:\n            return default  # silently skip if file is not present\n        try:\n            enc, skip = determine_file_encoding(optionf.read(512))\n            optionf.seek(skip, io.SEEK_SET)\n        except OSError:\n            enc = None  # silently skip read errors\n        try:\n            # FIXME: https://github.com/ytdl-org/youtube-dl/commit/dfe5fa49aed02cf36ba9f743b11b0903554b5e56\n            contents = optionf.read().decode(enc or preferredencoding())\n            res = shlex.split(contents, comments=True)\n        except Exception as err:\n            raise ValueError(f'Unable to parse \"{filename}\": {err}')\n        finally:\n            optionf.close()\n        return res\n\n    @staticmethod\n    def hide_login_info(opts):\n        PRIVATE_OPTS = {'-p', '--password', '-u', '--username', '--video-password', '--ap-password', '--ap-username'}\n        eqre = re.compile('^(?P<key>' + ('|'.join(re.escape(po) for po in PRIVATE_OPTS)) + ')=.+$')\n\n        def _scrub_eq(o):\n            m = eqre.match(o)\n            if m:\n                return m.group('key') + '=PRIVATE'\n            else:\n                return o\n\n        opts = list(map(_scrub_eq, opts))\n        for idx, opt in enumerate(opts):\n            if opt in PRIVATE_OPTS and idx + 1 < len(opts):\n                opts[idx + 1] = 'PRIVATE'\n        return opts\n\n    def append_config(self, *args, label=None):\n        config = type(self)(self.parser, label)\n        config._loaded_paths = self._loaded_paths\n        if config.init(*args):\n            self.configs.append(config)\n\n    @property\n    def all_args(self):\n        for config in reversed(self.configs):\n            yield from config.all_args\n        yield from self.parsed_args or []\n\n    def parse_known_args(self, **kwargs):\n        return self.parser.parse_known_args(self.all_args, **kwargs)\n\n    def parse_args(self):\n        return self.parser.parse_args(self.all_args)\n\n\nclass WebSocketsWrapper:\n    \"\"\"Wraps websockets module to use in non-async scopes\"\"\"\n    pool = None\n\n    def __init__(self, url, headers=None, connect=True):\n        self.loop = asyncio.new_event_loop()\n        # XXX: \"loop\" is deprecated\n        self.conn = websockets.connect(\n            url, extra_headers=headers, ping_interval=None,\n            close_timeout=float('inf'), loop=self.loop, ping_timeout=float('inf'))\n        if connect:\n            self.__enter__()\n        atexit.register(self.__exit__, None, None, None)\n\n    def __enter__(self):\n        if not self.pool:\n            self.pool = self.run_with_loop(self.conn.__aenter__(), self.loop)\n        return self\n\n    def send(self, *args):\n        self.run_with_loop(self.pool.send(*args), self.loop)\n\n    def recv(self, *args):\n        return self.run_with_loop(self.pool.recv(*args), self.loop)\n\n    def __exit__(self, type, value, traceback):\n        try:\n            return self.run_with_loop(self.conn.__aexit__(type, value, traceback), self.loop)\n        finally:\n            self.loop.close()\n            self._cancel_all_tasks(self.loop)\n\n    # taken from https://github.com/python/cpython/blob/3.9/Lib/asyncio/runners.py with modifications\n    # for contributors: If there's any new library using asyncio needs to be run in non-async, move these function out of this class\n    @staticmethod\n    def run_with_loop(main, loop):\n        if not asyncio.iscoroutine(main):\n            raise ValueError(f'a coroutine was expected, got {main!r}')\n\n        try:\n            return loop.run_until_complete(main)\n        finally:\n            loop.run_until_complete(loop.shutdown_asyncgens())\n            if hasattr(loop, 'shutdown_default_executor'):\n                loop.run_until_complete(loop.shutdown_default_executor())\n\n    @staticmethod\n    def _cancel_all_tasks(loop):\n        to_cancel = asyncio.all_tasks(loop)\n\n        if not to_cancel:\n            return\n\n        for task in to_cancel:\n            task.cancel()\n\n        # XXX: \"loop\" is removed in python 3.10+\n        loop.run_until_complete(\n            asyncio.gather(*to_cancel, loop=loop, return_exceptions=True))\n\n        for task in to_cancel:\n            if task.cancelled():\n                continue\n            if task.exception() is not None:\n                loop.call_exception_handler({\n                    'message': 'unhandled exception during asyncio.run() shutdown',\n                    'exception': task.exception(),\n                    'task': task,\n                })\n\n\ndef merge_headers(*dicts):\n    \"\"\"Merge dicts of http headers case insensitively, prioritizing the latter ones\"\"\"\n    return {k.title(): v for k, v in itertools.chain.from_iterable(map(dict.items, dicts))}\n\n\ndef cached_method(f):\n    \"\"\"Cache a method\"\"\"\n    signature = inspect.signature(f)\n\n    @functools.wraps(f)\n    def wrapper(self, *args, **kwargs):\n        bound_args = signature.bind(self, *args, **kwargs)\n        bound_args.apply_defaults()\n        key = tuple(bound_args.arguments.values())[1:]\n\n        cache = vars(self).setdefault('_cached_method__cache', {}).setdefault(f.__name__, {})\n        if key not in cache:\n            cache[key] = f(self, *args, **kwargs)\n        return cache[key]\n    return wrapper\n\n\nclass classproperty:\n    \"\"\"property access for class methods with optional caching\"\"\"\n    def __new__(cls, func=None, *args, **kwargs):\n        if not func:\n            return functools.partial(cls, *args, **kwargs)\n        return super().__new__(cls)\n\n    def __init__(self, func, *, cache=False):\n        functools.update_wrapper(self, func)\n        self.func = func\n        self._cache = {} if cache else None\n\n    def __get__(self, _, cls):\n        if self._cache is None:\n            return self.func(cls)\n        elif cls not in self._cache:\n            self._cache[cls] = self.func(cls)\n        return self._cache[cls]\n\n\nclass function_with_repr:\n    def __init__(self, func, repr_=None):\n        functools.update_wrapper(self, func)\n        self.func, self.__repr = func, repr_\n\n    def __call__(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n    def __repr__(self):\n        if self.__repr:\n            return self.__repr\n        return f'{self.func.__module__}.{self.func.__qualname__}'\n\n\nclass Namespace(types.SimpleNamespace):\n    \"\"\"Immutable namespace\"\"\"\n\n    def __iter__(self):\n        return iter(self.__dict__.values())\n\n    @property\n    def items_(self):\n        return self.__dict__.items()\n\n\nMEDIA_EXTENSIONS = Namespace(\n    common_video=('avi', 'flv', 'mkv', 'mov', 'mp4', 'webm'),\n    video=('3g2', '3gp', 'f4v', 'mk3d', 'divx', 'mpg', 'ogv', 'm4v', 'wmv'),\n    common_audio=('aiff', 'alac', 'flac', 'm4a', 'mka', 'mp3', 'ogg', 'opus', 'wav'),\n    audio=('aac', 'ape', 'asf', 'f4a', 'f4b', 'm4b', 'm4p', 'm4r', 'oga', 'ogx', 'spx', 'vorbis', 'wma', 'weba'),\n    thumbnails=('jpg', 'png', 'webp'),\n    storyboards=('mhtml', ),\n    subtitles=('srt', 'vtt', 'ass', 'lrc'),\n    manifests=('f4f', 'f4m', 'm3u8', 'smil', 'mpd'),\n)\nMEDIA_EXTENSIONS.video += MEDIA_EXTENSIONS.common_video\nMEDIA_EXTENSIONS.audio += MEDIA_EXTENSIONS.common_audio\n\nKNOWN_EXTENSIONS = (*MEDIA_EXTENSIONS.video, *MEDIA_EXTENSIONS.audio, *MEDIA_EXTENSIONS.manifests)\n\n\nclass RetryManager:\n    \"\"\"Usage:\n        for retry in RetryManager(...):\n            try:\n                ...\n            except SomeException as err:\n                retry.error = err\n                continue\n    \"\"\"\n    attempt, _error = 0, None\n\n    def __init__(self, _retries, _error_callback, **kwargs):\n        self.retries = _retries or 0\n        self.error_callback = functools.partial(_error_callback, **kwargs)\n\n    def _should_retry(self):\n        return self._error is not NO_DEFAULT and self.attempt <= self.retries\n\n    @property\n    def error(self):\n        if self._error is NO_DEFAULT:\n            return None\n        return self._error\n\n    @error.setter\n    def error(self, value):\n        self._error = value\n\n    def __iter__(self):\n        while self._should_retry():\n            self.error = NO_DEFAULT\n            self.attempt += 1\n            yield self\n            if self.error:\n                self.error_callback(self.error, self.attempt, self.retries)\n\n    @staticmethod\n    def report_retry(e, count, retries, *, sleep_func, info, warn, error=None, suffix=None):\n        \"\"\"Utility function for reporting retries\"\"\"\n        if count > retries:\n            if error:\n                return error(f'{e}. Giving up after {count - 1} retries') if count > 1 else error(str(e))\n            raise e\n\n        if not count:\n            return warn(e)\n        elif isinstance(e, ExtractorError):\n            e = remove_end(str_or_none(e.cause) or e.orig_msg, '.')\n        warn(f'{e}. Retrying{format_field(suffix, None, \" %s\")} ({count}/{retries})...')\n\n        delay = float_or_none(sleep_func(n=count - 1)) if callable(sleep_func) else sleep_func\n        if delay:\n            info(f'Sleeping {delay:.2f} seconds ...')\n            time.sleep(delay)\n\n\ndef make_archive_id(ie, video_id):\n    ie_key = ie if isinstance(ie, str) else ie.ie_key()\n    return f'{ie_key.lower()} {video_id}'\n\n\ndef truncate_string(s, left, right=0):\n    assert left > 3 and right >= 0\n    if s is None or len(s) <= left + right:\n        return s\n    return f'{s[:left-3]}...{s[-right:] if right else \"\"}'\n\n\ndef orderedSet_from_options(options, alias_dict, *, use_regex=False, start=None):\n    assert 'all' in alias_dict, '\"all\" alias is required'\n    requested = list(start or [])\n    for val in options:\n        discard = val.startswith('-')\n        if discard:\n            val = val[1:]\n\n        if val in alias_dict:\n            val = alias_dict[val] if not discard else [\n                i[1:] if i.startswith('-') else f'-{i}' for i in alias_dict[val]]\n            # NB: Do not allow regex in aliases for performance\n            requested = orderedSet_from_options(val, alias_dict, start=requested)\n            continue\n\n        current = (filter(re.compile(val, re.I).fullmatch, alias_dict['all']) if use_regex\n                   else [val] if val in alias_dict['all'] else None)\n        if current is None:\n            raise ValueError(val)\n\n        if discard:\n            for item in current:\n                while item in requested:\n                    requested.remove(item)\n        else:\n            requested.extend(current)\n\n    return orderedSet(requested)\n\n\n# TODO: Rewrite\nclass FormatSorter:\n    regex = r' *((?P<reverse>\\+)?(?P<field>[a-zA-Z0-9_]+)((?P<separator>[~:])(?P<limit>.*?))?)? *$'\n\n    default = ('hidden', 'aud_or_vid', 'hasvid', 'ie_pref', 'lang', 'quality',\n               'res', 'fps', 'hdr:12', 'vcodec:vp9.2', 'channels', 'acodec',\n               'size', 'br', 'asr', 'proto', 'ext', 'hasaud', 'source', 'id')  # These must not be aliases\n    ytdl_default = ('hasaud', 'lang', 'quality', 'tbr', 'filesize', 'vbr',\n                    'height', 'width', 'proto', 'vext', 'abr', 'aext',\n                    'fps', 'fs_approx', 'source', 'id')\n\n    settings = {\n        'vcodec': {'type': 'ordered', 'regex': True,\n                   'order': ['av0?1', 'vp0?9.2', 'vp0?9', '[hx]265|he?vc?', '[hx]264|avc', 'vp0?8', 'mp4v|h263', 'theora', '', None, 'none']},\n        'acodec': {'type': 'ordered', 'regex': True,\n                   'order': ['[af]lac', 'wav|aiff', 'opus', 'vorbis|ogg', 'aac', 'mp?4a?', 'mp3', 'ac-?4', 'e-?a?c-?3', 'ac-?3', 'dts', '', None, 'none']},\n        'hdr': {'type': 'ordered', 'regex': True, 'field': 'dynamic_range',\n                'order': ['dv', '(hdr)?12', r'(hdr)?10\\+', '(hdr)?10', 'hlg', '', 'sdr', None]},\n        'proto': {'type': 'ordered', 'regex': True, 'field': 'protocol',\n                  'order': ['(ht|f)tps', '(ht|f)tp$', 'm3u8.*', '.*dash', 'websocket_frag', 'rtmpe?', '', 'mms|rtsp', 'ws|websocket', 'f4']},\n        'vext': {'type': 'ordered', 'field': 'video_ext',\n                 'order': ('mp4', 'mov', 'webm', 'flv', '', 'none'),\n                 'order_free': ('webm', 'mp4', 'mov', 'flv', '', 'none')},\n        'aext': {'type': 'ordered', 'regex': True, 'field': 'audio_ext',\n                 'order': ('m4a', 'aac', 'mp3', 'ogg', 'opus', 'web[am]', '', 'none'),\n                 'order_free': ('ogg', 'opus', 'web[am]', 'mp3', 'm4a', 'aac', '', 'none')},\n        'hidden': {'visible': False, 'forced': True, 'type': 'extractor', 'max': -1000},\n        'aud_or_vid': {'visible': False, 'forced': True, 'type': 'multiple',\n                       'field': ('vcodec', 'acodec'),\n                       'function': lambda it: int(any(v != 'none' for v in it))},\n        'ie_pref': {'priority': True, 'type': 'extractor'},\n        'hasvid': {'priority': True, 'field': 'vcodec', 'type': 'boolean', 'not_in_list': ('none',)},\n        'hasaud': {'field': 'acodec', 'type': 'boolean', 'not_in_list': ('none',)},\n        'lang': {'convert': 'float', 'field': 'language_preference', 'default': -1},\n        'quality': {'convert': 'float', 'default': -1},\n        'filesize': {'convert': 'bytes'},\n        'fs_approx': {'convert': 'bytes', 'field': 'filesize_approx'},\n        'id': {'convert': 'string', 'field': 'format_id'},\n        'height': {'convert': 'float_none'},\n        'width': {'convert': 'float_none'},\n        'fps': {'convert': 'float_none'},\n        'channels': {'convert': 'float_none', 'field': 'audio_channels'},\n        'tbr': {'convert': 'float_none'},\n        'vbr': {'convert': 'float_none'},\n        'abr': {'convert': 'float_none'},\n        'asr': {'convert': 'float_none'},\n        'source': {'convert': 'float', 'field': 'source_preference', 'default': -1},\n\n        'codec': {'type': 'combined', 'field': ('vcodec', 'acodec')},\n        'br': {'type': 'multiple', 'field': ('tbr', 'vbr', 'abr'), 'convert': 'float_none',\n               'function': lambda it: next(filter(None, it), None)},\n        'size': {'type': 'multiple', 'field': ('filesize', 'fs_approx'), 'convert': 'bytes',\n                 'function': lambda it: next(filter(None, it), None)},\n        'ext': {'type': 'combined', 'field': ('vext', 'aext')},\n        'res': {'type': 'multiple', 'field': ('height', 'width'),\n                'function': lambda it: (lambda l: min(l) if l else 0)(tuple(filter(None, it)))},\n\n        # Actual field names\n        'format_id': {'type': 'alias', 'field': 'id'},\n        'preference': {'type': 'alias', 'field': 'ie_pref'},\n        'language_preference': {'type': 'alias', 'field': 'lang'},\n        'source_preference': {'type': 'alias', 'field': 'source'},\n        'protocol': {'type': 'alias', 'field': 'proto'},\n        'filesize_approx': {'type': 'alias', 'field': 'fs_approx'},\n        'audio_channels': {'type': 'alias', 'field': 'channels'},\n\n        # Deprecated\n        'dimension': {'type': 'alias', 'field': 'res', 'deprecated': True},\n        'resolution': {'type': 'alias', 'field': 'res', 'deprecated': True},\n        'extension': {'type': 'alias', 'field': 'ext', 'deprecated': True},\n        'bitrate': {'type': 'alias', 'field': 'br', 'deprecated': True},\n        'total_bitrate': {'type': 'alias', 'field': 'tbr', 'deprecated': True},\n        'video_bitrate': {'type': 'alias', 'field': 'vbr', 'deprecated': True},\n        'audio_bitrate': {'type': 'alias', 'field': 'abr', 'deprecated': True},\n        'framerate': {'type': 'alias', 'field': 'fps', 'deprecated': True},\n        'filesize_estimate': {'type': 'alias', 'field': 'size', 'deprecated': True},\n        'samplerate': {'type': 'alias', 'field': 'asr', 'deprecated': True},\n        'video_ext': {'type': 'alias', 'field': 'vext', 'deprecated': True},\n        'audio_ext': {'type': 'alias', 'field': 'aext', 'deprecated': True},\n        'video_codec': {'type': 'alias', 'field': 'vcodec', 'deprecated': True},\n        'audio_codec': {'type': 'alias', 'field': 'acodec', 'deprecated': True},\n        'video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},\n        'has_video': {'type': 'alias', 'field': 'hasvid', 'deprecated': True},\n        'audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},\n        'has_audio': {'type': 'alias', 'field': 'hasaud', 'deprecated': True},\n        'extractor': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},\n        'extractor_preference': {'type': 'alias', 'field': 'ie_pref', 'deprecated': True},\n    }\n\n    def __init__(self, ydl, field_preference):\n        self.ydl = ydl\n        self._order = []\n        self.evaluate_params(self.ydl.params, field_preference)\n        if ydl.params.get('verbose'):\n            self.print_verbose_info(self.ydl.write_debug)\n\n    def _get_field_setting(self, field, key):\n        if field not in self.settings:\n            if key in ('forced', 'priority'):\n                return False\n            self.ydl.deprecated_feature(f'Using arbitrary fields ({field}) for format sorting is '\n                                        'deprecated and may be removed in a future version')\n            self.settings[field] = {}\n        propObj = self.settings[field]\n        if key not in propObj:\n            type = propObj.get('type')\n            if key == 'field':\n                default = 'preference' if type == 'extractor' else (field,) if type in ('combined', 'multiple') else field\n            elif key == 'convert':\n                default = 'order' if type == 'ordered' else 'float_string' if field else 'ignore'\n            else:\n                default = {'type': 'field', 'visible': True, 'order': [], 'not_in_list': (None,)}.get(key, None)\n            propObj[key] = default\n        return propObj[key]\n\n    def _resolve_field_value(self, field, value, convertNone=False):\n        if value is None:\n            if not convertNone:\n                return None\n        else:\n            value = value.lower()\n        conversion = self._get_field_setting(field, 'convert')\n        if conversion == 'ignore':\n            return None\n        if conversion == 'string':\n            return value\n        elif conversion == 'float_none':\n            return float_or_none(value)\n        elif conversion == 'bytes':\n            return parse_bytes(value)\n        elif conversion == 'order':\n            order_list = (self._use_free_order and self._get_field_setting(field, 'order_free')) or self._get_field_setting(field, 'order')\n            use_regex = self._get_field_setting(field, 'regex')\n            list_length = len(order_list)\n            empty_pos = order_list.index('') if '' in order_list else list_length + 1\n            if use_regex and value is not None:\n                for i, regex in enumerate(order_list):\n                    if regex and re.match(regex, value):\n                        return list_length - i\n                return list_length - empty_pos  # not in list\n            else:  # not regex or  value = None\n                return list_length - (order_list.index(value) if value in order_list else empty_pos)\n        else:\n            if value.isnumeric():\n                return float(value)\n            else:\n                self.settings[field]['convert'] = 'string'\n                return value\n\n    def evaluate_params(self, params, sort_extractor):\n        self._use_free_order = params.get('prefer_free_formats', False)\n        self._sort_user = params.get('format_sort', [])\n        self._sort_extractor = sort_extractor\n\n        def add_item(field, reverse, closest, limit_text):\n            field = field.lower()\n            if field in self._order:\n                return\n            self._order.append(field)\n            limit = self._resolve_field_value(field, limit_text)\n            data = {\n                'reverse': reverse,\n                'closest': False if limit is None else closest,\n                'limit_text': limit_text,\n                'limit': limit}\n            if field in self.settings:\n                self.settings[field].update(data)\n            else:\n                self.settings[field] = data\n\n        sort_list = (\n            tuple(field for field in self.default if self._get_field_setting(field, 'forced'))\n            + (tuple() if params.get('format_sort_force', False)\n                else tuple(field for field in self.default if self._get_field_setting(field, 'priority')))\n            + tuple(self._sort_user) + tuple(sort_extractor) + self.default)\n\n        for item in sort_list:\n            match = re.match(self.regex, item)\n            if match is None:\n                raise ExtractorError('Invalid format sort string \"%s\" given by extractor' % item)\n            field = match.group('field')\n            if field is None:\n                continue\n            if self._get_field_setting(field, 'type') == 'alias':\n                alias, field = field, self._get_field_setting(field, 'field')\n                if self._get_field_setting(alias, 'deprecated'):\n                    self.ydl.deprecated_feature(f'Format sorting alias {alias} is deprecated and may '\n                                                f'be removed in a future version. Please use {field} instead')\n            reverse = match.group('reverse') is not None\n            closest = match.group('separator') == '~'\n            limit_text = match.group('limit')\n\n            has_limit = limit_text is not None\n            has_multiple_fields = self._get_field_setting(field, 'type') == 'combined'\n            has_multiple_limits = has_limit and has_multiple_fields and not self._get_field_setting(field, 'same_limit')\n\n            fields = self._get_field_setting(field, 'field') if has_multiple_fields else (field,)\n            limits = limit_text.split(':') if has_multiple_limits else (limit_text,) if has_limit else tuple()\n            limit_count = len(limits)\n            for (i, f) in enumerate(fields):\n                add_item(f, reverse, closest,\n                         limits[i] if i < limit_count\n                         else limits[0] if has_limit and not has_multiple_limits\n                         else None)\n\n    def print_verbose_info(self, write_debug):\n        if self._sort_user:\n            write_debug('Sort order given by user: %s' % ', '.join(self._sort_user))\n        if self._sort_extractor:\n            write_debug('Sort order given by extractor: %s' % ', '.join(self._sort_extractor))\n        write_debug('Formats sorted by: %s' % ', '.join(['%s%s%s' % (\n            '+' if self._get_field_setting(field, 'reverse') else '', field,\n            '%s%s(%s)' % ('~' if self._get_field_setting(field, 'closest') else ':',\n                          self._get_field_setting(field, 'limit_text'),\n                          self._get_field_setting(field, 'limit'))\n            if self._get_field_setting(field, 'limit_text') is not None else '')\n            for field in self._order if self._get_field_setting(field, 'visible')]))\n\n    def _calculate_field_preference_from_value(self, format, field, type, value):\n        reverse = self._get_field_setting(field, 'reverse')\n        closest = self._get_field_setting(field, 'closest')\n        limit = self._get_field_setting(field, 'limit')\n\n        if type == 'extractor':\n            maximum = self._get_field_setting(field, 'max')\n            if value is None or (maximum is not None and value >= maximum):\n                value = -1\n        elif type == 'boolean':\n            in_list = self._get_field_setting(field, 'in_list')\n            not_in_list = self._get_field_setting(field, 'not_in_list')\n            value = 0 if ((in_list is None or value in in_list) and (not_in_list is None or value not in not_in_list)) else -1\n        elif type == 'ordered':\n            value = self._resolve_field_value(field, value, True)\n\n        # try to convert to number\n        val_num = float_or_none(value, default=self._get_field_setting(field, 'default'))\n        is_num = self._get_field_setting(field, 'convert') != 'string' and val_num is not None\n        if is_num:\n            value = val_num\n\n        return ((-10, 0) if value is None\n                else (1, value, 0) if not is_num  # if a field has mixed strings and numbers, strings are sorted higher\n                else (0, -abs(value - limit), value - limit if reverse else limit - value) if closest\n                else (0, value, 0) if not reverse and (limit is None or value <= limit)\n                else (0, -value, 0) if limit is None or (reverse and value == limit) or value > limit\n                else (-1, value, 0))\n\n    def _calculate_field_preference(self, format, field):\n        type = self._get_field_setting(field, 'type')  # extractor, boolean, ordered, field, multiple\n        get_value = lambda f: format.get(self._get_field_setting(f, 'field'))\n        if type == 'multiple':\n            type = 'field'  # Only 'field' is allowed in multiple for now\n            actual_fields = self._get_field_setting(field, 'field')\n\n            value = self._get_field_setting(field, 'function')(get_value(f) for f in actual_fields)\n        else:\n            value = get_value(field)\n        return self._calculate_field_preference_from_value(format, field, type, value)\n\n    def calculate_preference(self, format):\n        # Determine missing protocol\n        if not format.get('protocol'):\n            format['protocol'] = determine_protocol(format)\n\n        # Determine missing ext\n        if not format.get('ext') and 'url' in format:\n            format['ext'] = determine_ext(format['url'])\n        if format.get('vcodec') == 'none':\n            format['audio_ext'] = format['ext'] if format.get('acodec') != 'none' else 'none'\n            format['video_ext'] = 'none'\n        else:\n            format['video_ext'] = format['ext']\n            format['audio_ext'] = 'none'\n        # if format.get('preference') is None and format.get('ext') in ('f4f', 'f4m'):  # Not supported?\n        #    format['preference'] = -1000\n\n        if format.get('preference') is None and format.get('ext') == 'flv' and re.match('[hx]265|he?vc?', format.get('vcodec') or ''):\n            # HEVC-over-FLV is out-of-spec by FLV's original spec\n            # ref. https://trac.ffmpeg.org/ticket/6389\n            # ref. https://github.com/yt-dlp/yt-dlp/pull/5821\n            format['preference'] = -100\n\n        # Determine missing bitrates\n        if format.get('vcodec') == 'none':\n            format['vbr'] = 0\n        if format.get('acodec') == 'none':\n            format['abr'] = 0\n        if not format.get('vbr') and format.get('vcodec') != 'none':\n            format['vbr'] = try_call(lambda: format['tbr'] - format['abr']) or None\n        if not format.get('abr') and format.get('acodec') != 'none':\n            format['abr'] = try_call(lambda: format['tbr'] - format['vbr']) or None\n        if not format.get('tbr'):\n            format['tbr'] = try_call(lambda: format['vbr'] + format['abr']) or None\n\n        return tuple(self._calculate_field_preference(format, field) for field in self._order)\n\n\n# XXX: Temporary\nclass _YDLLogger:\n    def __init__(self, ydl=None):\n        self._ydl = ydl\n\n    def debug(self, message):\n        if self._ydl:\n            self._ydl.write_debug(message)\n\n    def info(self, message):\n        if self._ydl:\n            self._ydl.to_screen(message)\n\n    def warning(self, message, *, once=False):\n        if self._ydl:\n            self._ydl.report_warning(message, once)\n\n    def error(self, message, *, is_error=True):\n        if self._ydl:\n            self._ydl.report_error(message, is_error=is_error)\n\n    def stdout(self, message):\n        if self._ydl:\n            self._ydl.to_stdout(message)\n\n    def stderr(self, message):\n        if self._ydl:\n            self._ydl.to_stderr(message)\n"], "filenames": ["devscripts/changelog_override.json", "test/test_YoutubeDL.py", "test/test_utils.py", "yt_dlp/compat/__init__.py", "yt_dlp/postprocessor/exec.py", "yt_dlp/utils/_utils.py"], "buggy_code_start_loc": [95, 787, 16, 33, 1, 828], "buggy_code_end_loc": [95, 790, 2390, 34, 34, 839], "fixing_code_start_loc": [96, 787, 17, 33, 0, 828], "fixing_code_end_loc": [101, 790, 2407, 34, 32, 853], "type": "CWE-78", "message": "yt-dlp is a youtube-dl fork with additional features and fixes. yt-dlp allows the user to provide shell command lines to be executed at various stages in its download steps through the `--exec` flag. This flag allows output template expansion in its argument, so that metadata values may be used in the shell commands. The metadata fields can be combined with the `%q` conversion, which is intended to quote/escape these values so they can be safely passed to the shell. However, the escaping used for `cmd` (the shell used by Python's `subprocess` on Windows) does not properly escape special characters, which can allow for remote code execution if `--exec` is used directly with maliciously crafted remote data. This vulnerability only impacts `yt-dlp` on Windows, and the vulnerability is present regardless of whether `yt-dlp` is run from `cmd` or from `PowerShell`. Support for output template expansion in `--exec`, along with this vulnerable behavior, was added to `yt-dlp` in version 2021.04.11. yt-dlp version 2023.09.24 fixes this issue by properly escaping each special character. `\\n` will be replaced by `\\r` as no way of escaping it has been found. It is recommended to upgrade yt-dlp to version 2023.09.24 as soon as possible. Also, always be careful when using --exec, because while this specific vulnerability has been patched, using unvalidated input in shell commands is inherently dangerous. For Windows users who are not able to upgrade: 1. Avoid using any output template expansion in --exec other than {} (filepath). 2. If expansion in --exec is needed, verify the fields you are using do not contain \", | or &. 3. Instead of using --exec, write the info json and load the fields from it instead.\n", "other": {"cve": {"id": "CVE-2023-40581", "sourceIdentifier": "security-advisories@github.com", "published": "2023-09-25T19:15:09.960", "lastModified": "2023-09-27T14:48:35.773", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "yt-dlp is a youtube-dl fork with additional features and fixes. yt-dlp allows the user to provide shell command lines to be executed at various stages in its download steps through the `--exec` flag. This flag allows output template expansion in its argument, so that metadata values may be used in the shell commands. The metadata fields can be combined with the `%q` conversion, which is intended to quote/escape these values so they can be safely passed to the shell. However, the escaping used for `cmd` (the shell used by Python's `subprocess` on Windows) does not properly escape special characters, which can allow for remote code execution if `--exec` is used directly with maliciously crafted remote data. This vulnerability only impacts `yt-dlp` on Windows, and the vulnerability is present regardless of whether `yt-dlp` is run from `cmd` or from `PowerShell`. Support for output template expansion in `--exec`, along with this vulnerable behavior, was added to `yt-dlp` in version 2021.04.11. yt-dlp version 2023.09.24 fixes this issue by properly escaping each special character. `\\n` will be replaced by `\\r` as no way of escaping it has been found. It is recommended to upgrade yt-dlp to version 2023.09.24 as soon as possible. Also, always be careful when using --exec, because while this specific vulnerability has been patched, using unvalidated input in shell commands is inherently dangerous. For Windows users who are not able to upgrade: 1. Avoid using any output template expansion in --exec other than {} (filepath). 2. If expansion in --exec is needed, verify the fields you are using do not contain \", | or &. 3. Instead of using --exec, write the info json and load the fields from it instead.\n"}, {"lang": "es", "value": "yt-dlp es un fork de youtube-dl con funciones y correcciones adicionales. yt-dlp permite al usuario proporcionar l\u00edneas de comando de shell para ejecutar en varias etapas de sus pasos de descarga a trav\u00e9s del indicador `--exec`. Este indicador permite la expansi\u00f3n de la plantilla de salida en su argumento, de modo que los valores de metadatos se puedan usar en los comandos del shell. Los campos de metadatos se pueden combinar con la conversi\u00f3n `%q`, cuyo objetivo es citar/escapar estos valores para que puedan pasarse de forma segura al shell. Sin embargo, el escape usado para `cmd` (el shell usado por el `subproceso` de Python en Windows) no escapa adecuadamente a los caracteres especiales, lo que puede permitir la ejecuci\u00f3n remota de c\u00f3digo si `--exec` se usa directamente con datos remotos creados con fines maliciosos. Esta vulnerabilidad solo afecta a \"yt-dlp\" en Windows y est\u00e1 presente independientemente de si \"yt-dlp\" se ejecuta desde \"cmd\" o desde \"PowerShell\". La compatibilidad con la expansi\u00f3n de la plantilla de salida en `--exec`, junto con este comportamiento vulnerable, se agreg\u00f3 a `yt-dlp` en la versi\u00f3n 2021.04.11. La versi\u00f3n 2023.09.24 de yt-dlp soluciona este problema escapando correctamente cada car\u00e1cter especial. `\\n` ser\u00e1 reemplazado por `\\r` ya que no se ha encontrado ninguna forma de escapar. Se recomienda actualizar yt-dlp a la versi\u00f3n 2023.09.24 lo antes posible. Adem\u00e1s, siempre tenga cuidado al usar --exec, porque si bien esta vulnerabilidad espec\u00edfica ha sido parcheada, usar entradas no validadas en los comandos del shell es inherentemente peligroso. Para usuarios de Windows que no pueden actualizar: 1. Evite utilizar cualquier expansi\u00f3n de plantilla de salida en --exec que no sea {} (ruta de archivo). 2. Si se necesita expansi\u00f3n en --exec, verifique que los campos que est\u00e1 usando no contengan \", | o &amp;. 3. En lugar de usar --exec, escriba la informaci\u00f3n json y cargue los campos desde all\u00ed."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:C/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.3, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.6, "impactScore": 6.0}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-78"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:yt-dlp_project:yt-dlp:*:*:*:*:*:*:*:*", "versionStartIncluding": "2021.04.11", "versionEndExcluding": "2023.09.24", "matchCriteriaId": "AAE6AF84-62B8-469A-9F5D-FC638DCFE350"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:o:microsoft:windows:-:*:*:*:*:*:*:*", "matchCriteriaId": "A2572D17-1DE6-457B-99CC-64AFD54487EA"}]}]}], "references": [{"url": "https://github.com/yt-dlp/yt-dlp-nightly-builds/releases/tag/2023.09.24.003044", "source": "security-advisories@github.com", "tags": ["Product", "Release Notes"]}, {"url": "https://github.com/yt-dlp/yt-dlp/commit/de015e930747165dbb8fcd360f8775fd973b7d6e", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/yt-dlp/yt-dlp/releases/tag/2021.04.11", "source": "security-advisories@github.com", "tags": ["Product", "Release Notes"]}, {"url": "https://github.com/yt-dlp/yt-dlp/releases/tag/2023.09.24", "source": "security-advisories@github.com", "tags": ["Product", "Release Notes"]}, {"url": "https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-42h4-v29r-42qg", "source": "security-advisories@github.com", "tags": ["Exploit", "Mitigation", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/yt-dlp/yt-dlp/commit/de015e930747165dbb8fcd360f8775fd973b7d6e"}}