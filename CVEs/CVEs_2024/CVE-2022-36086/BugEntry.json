{"buggy_code": ["use core::alloc::Layout;\nuse core::mem;\nuse core::mem::{align_of, size_of};\nuse core::ptr::null_mut;\nuse core::ptr::NonNull;\n\nuse crate::align_up_size;\n\nuse super::align_up;\n\n/// A sorted list of holes. It uses the the holes itself to store its nodes.\npub struct HoleList {\n    pub(crate) first: Hole, // dummy\n    pub(crate) bottom: *mut u8,\n    pub(crate) top: *mut u8,\n}\n\npub(crate) struct Cursor {\n    prev: NonNull<Hole>,\n    hole: NonNull<Hole>,\n    top: *mut u8,\n}\n\n/// A block containing free memory. It points to the next hole and thus forms a linked list.\npub(crate) struct Hole {\n    pub size: usize,\n    pub next: Option<NonNull<Hole>>,\n}\n\n/// Basic information about a hole.\n#[derive(Debug, Clone, Copy)]\nstruct HoleInfo {\n    addr: *mut u8,\n    size: usize,\n}\n\nimpl Cursor {\n    fn next(mut self) -> Option<Self> {\n        unsafe {\n            self.hole.as_mut().next.map(|nhole| Cursor {\n                prev: self.hole,\n                hole: nhole,\n                top: self.top,\n            })\n        }\n    }\n\n    fn current(&self) -> &Hole {\n        unsafe { self.hole.as_ref() }\n    }\n\n    fn previous(&self) -> &Hole {\n        unsafe { self.prev.as_ref() }\n    }\n\n    // On success, it returns the new allocation, and the linked list has been updated\n    // to accomodate any new holes and allocation. On error, it returns the cursor\n    // unmodified, and has made no changes to the linked list of holes.\n    fn split_current(self, required_layout: Layout) -> Result<(*mut u8, usize), Self> {\n        let front_padding;\n        let alloc_ptr;\n        let alloc_size;\n        let back_padding;\n\n        // Here we create a scope, JUST to make sure that any created references do not\n        // live to the point where we start doing pointer surgery below.\n        {\n            let hole_size = self.current().size;\n            let hole_addr_u8 = self.hole.as_ptr().cast::<u8>();\n            let required_size = required_layout.size();\n            let required_align = required_layout.align();\n\n            // Quick check: If the new item is larger than the current hole, it's never gunna\n            // work. Go ahead and bail early to save ourselves some math.\n            if hole_size < required_size {\n                return Err(self);\n            }\n\n            // Attempt to fracture the current hole into the following parts:\n            // ([front_padding], allocation, [back_padding])\n            //\n            // The paddings are optional, and only placed if required.\n            //\n            // First, figure out if front padding is necessary. This would be necessary if the new\n            // allocation has a larger alignment requirement than the current hole, and we didn't get\n            // lucky that the current position was well-aligned enough for the new item.\n            let aligned_addr = if hole_addr_u8 == align_up(hole_addr_u8, required_align) {\n                // hole has already the required alignment, no front padding is needed.\n                front_padding = None;\n                hole_addr_u8\n            } else {\n                // Unfortunately, we did not get lucky. Instead: Push the \"starting location\" FORWARD the size\n                // of a hole node, to guarantee there is at least enough room for the hole header, and\n                // potentially additional space.\n                let new_start = hole_addr_u8.wrapping_add(HoleList::min_size());\n\n                let aligned_addr = align_up(new_start, required_align);\n                front_padding = Some(HoleInfo {\n                    // Our new front padding will exist at the same location as the previous hole,\n                    // it will just have a smaller size after we have chopped off the \"tail\" for\n                    // the allocation.\n                    addr: hole_addr_u8,\n                    size: (aligned_addr as usize) - (hole_addr_u8 as usize),\n                });\n                aligned_addr\n            };\n\n            // Okay, now that we found space, we need to see if the decisions we just made\n            // ACTUALLY fit in the previous hole space\n            let allocation_end = aligned_addr.wrapping_add(required_size);\n            let hole_end = hole_addr_u8.wrapping_add(hole_size);\n\n            if allocation_end > hole_end {\n                // hole is too small\n                return Err(self);\n            }\n\n            // Yes! We have successfully placed our allocation as well.\n            alloc_ptr = aligned_addr;\n            alloc_size = required_size;\n\n            // Okay, time to move onto the back padding. Here, we are opportunistic -\n            // if it fits, we sits. Otherwise we just skip adding the back padding, and\n            // sort of assume that the allocation is actually a bit larger than it\n            // actually needs to be.\n            //\n            // NOTE: Because we always use `HoleList::align_layout`, the size of\n            // the new allocation is always \"rounded up\" to cover any partial gaps that\n            // would have occurred. For this reason, we DON'T need to \"round up\"\n            // to account for an unaligned hole spot.\n            let hole_layout = Layout::new::<Hole>();\n            let back_padding_start = align_up(allocation_end, hole_layout.align());\n            let back_padding_end = back_padding_start.wrapping_add(hole_layout.size());\n\n            // Will the proposed new back padding actually fit in the old hole slot?\n            back_padding = if back_padding_end <= hole_end {\n                // Yes, it does! Place a back padding node\n                Some(HoleInfo {\n                    addr: back_padding_start,\n                    size: (hole_end as usize) - (back_padding_start as usize),\n                })\n            } else {\n                // No, it does not. We are now pretending the allocation now\n                // holds the extra 0..size_of::<Hole>() bytes that are not\n                // big enough to hold what SHOULD be back_padding\n                None\n            };\n        }\n\n        ////////////////////////////////////////////////////////////////////////////\n        // This is where we actually perform surgery on the linked list.\n        ////////////////////////////////////////////////////////////////////////////\n        let Cursor {\n            mut prev, mut hole, ..\n        } = self;\n        // Remove the current location from the previous node\n        unsafe {\n            prev.as_mut().next = None;\n        }\n        // Take the next node out of our current node\n        let maybe_next_addr: Option<NonNull<Hole>> = unsafe { hole.as_mut().next.take() };\n\n        // As of now, the old `Hole` is no more. We are about to replace it with one or more of\n        // the front padding, the allocation, and the back padding.\n\n        match (front_padding, back_padding) {\n            (None, None) => {\n                // No padding at all, how lucky! We still need to connect the PREVIOUS node\n                // to the NEXT node, if there was one\n                unsafe {\n                    prev.as_mut().next = maybe_next_addr;\n                }\n            }\n            (None, Some(singlepad)) | (Some(singlepad), None) => unsafe {\n                // We have front padding OR back padding, but not both.\n                //\n                // Replace the old node with the new single node. We need to stitch the new node\n                // into the linked list. Start by writing the padding into the proper location\n                let singlepad_ptr = singlepad.addr.cast::<Hole>();\n                singlepad_ptr.write(Hole {\n                    size: singlepad.size,\n                    // If the old hole had a next pointer, the single padding now takes\n                    // \"ownership\" of that link\n                    next: maybe_next_addr,\n                });\n\n                // Then connect the OLD previous to the NEW single padding\n                prev.as_mut().next = Some(NonNull::new_unchecked(singlepad_ptr));\n            },\n            (Some(frontpad), Some(backpad)) => unsafe {\n                // We have front padding AND back padding.\n                //\n                // We need to stich them together as two nodes where there used to\n                // only be one. Start with the back padding.\n                let backpad_ptr = backpad.addr.cast::<Hole>();\n                backpad_ptr.write(Hole {\n                    size: backpad.size,\n                    // If the old hole had a next pointer, the BACK padding now takes\n                    // \"ownership\" of that link\n                    next: maybe_next_addr,\n                });\n\n                // Now we emplace the front padding, and link it to both the back padding,\n                // and the old previous\n                let frontpad_ptr = frontpad.addr.cast::<Hole>();\n                frontpad_ptr.write(Hole {\n                    size: frontpad.size,\n                    // We now connect the FRONT padding to the BACK padding\n                    next: Some(NonNull::new_unchecked(backpad_ptr)),\n                });\n\n                // Then connect the OLD previous to the NEW FRONT padding\n                prev.as_mut().next = Some(NonNull::new_unchecked(frontpad_ptr));\n            },\n        }\n\n        // Well that went swimmingly! Hand off the allocation, with surgery performed successfully!\n        Ok((alloc_ptr, alloc_size))\n    }\n}\n\n// See if we can extend this hole towards the end of the allocation region\n// If so: increase the size of the node. If no: keep the node as-is\nfn check_merge_top(mut node: NonNull<Hole>, top: *mut u8) {\n    let node_u8 = node.as_ptr().cast::<u8>();\n    let node_sz = unsafe { node.as_ref().size };\n\n    // If this is the last node, we need to see if we need to merge to the end\n    let end = node_u8.wrapping_add(node_sz);\n    let hole_layout = Layout::new::<Hole>();\n    if end < top {\n        let next_hole_end = align_up(end, hole_layout.align()).wrapping_add(hole_layout.size());\n\n        if next_hole_end > top {\n            let offset = (top as usize) - (end as usize);\n            unsafe {\n                node.as_mut().size += offset;\n            }\n        }\n    }\n}\n\n// See if we can scoot this hole back to the bottom of the allocation region\n// If so: create and return the new hole. If not: return the existing hole\nfn check_merge_bottom(node: NonNull<Hole>, bottom: *mut u8) -> NonNull<Hole> {\n    debug_assert_eq!(bottom as usize % align_of::<Hole>(), 0);\n\n    if bottom.wrapping_add(core::mem::size_of::<Hole>()) > node.as_ptr().cast::<u8>() {\n        let offset = (node.as_ptr() as usize) - (bottom as usize);\n        let size = unsafe { node.as_ref() }.size + offset;\n        unsafe { make_hole(bottom, size) }\n    } else {\n        node\n    }\n}\n\nimpl HoleList {\n    /// Creates an empty `HoleList`.\n    #[cfg(not(feature = \"const_mut_refs\"))]\n    pub fn empty() -> HoleList {\n        HoleList {\n            first: Hole {\n                size: 0,\n                next: None,\n            },\n            bottom: null_mut(),\n            top: null_mut(),\n        }\n    }\n\n    /// Creates an empty `HoleList`.\n    #[cfg(feature = \"const_mut_refs\")]\n    pub const fn empty() -> HoleList {\n        HoleList {\n            first: Hole {\n                size: 0,\n                next: None,\n            },\n            bottom: null_mut(),\n            top: null_mut(),\n        }\n    }\n\n    pub(crate) fn cursor(&mut self) -> Option<Cursor> {\n        if let Some(hole) = self.first.next {\n            Some(Cursor {\n                hole,\n                prev: NonNull::new(&mut self.first)?,\n                top: self.top,\n            })\n        } else {\n            None\n        }\n    }\n\n    #[cfg(test)]\n    #[allow(dead_code)]\n    pub(crate) fn debug(&mut self) {\n        if let Some(cursor) = self.cursor() {\n            let mut cursor = cursor;\n            loop {\n                println!(\n                    \"prev: {:?}[{}], hole: {:?}[{}]\",\n                    cursor.previous() as *const Hole,\n                    cursor.previous().size,\n                    cursor.current() as *const Hole,\n                    cursor.current().size,\n                );\n                if let Some(c) = cursor.next() {\n                    cursor = c;\n                } else {\n                    println!(\"Done!\");\n                    return;\n                }\n            }\n        } else {\n            println!(\"No holes\");\n        }\n    }\n\n    /// Creates a `HoleList` that contains the given hole.\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe because it creates a hole at the given `hole_addr`.\n    /// This can cause undefined behavior if this address is invalid or if memory from the\n    /// `[hole_addr, hole_addr+size)` range is used somewhere else.\n    ///\n    /// The pointer to `hole_addr` is automatically aligned.\n    pub unsafe fn new(hole_addr: *mut u8, hole_size: usize) -> HoleList {\n        assert_eq!(size_of::<Hole>(), Self::min_size());\n\n        let aligned_hole_addr = align_up(hole_addr, align_of::<Hole>());\n        let ptr = aligned_hole_addr as *mut Hole;\n        ptr.write(Hole {\n            size: hole_size - ((aligned_hole_addr as usize) - (hole_addr as usize)),\n            next: None,\n        });\n\n        HoleList {\n            first: Hole {\n                size: 0,\n                next: Some(NonNull::new_unchecked(ptr)),\n            },\n            bottom: aligned_hole_addr,\n            top: hole_addr.wrapping_add(hole_size),\n        }\n    }\n\n    /// Aligns the given layout for use with `HoleList`.\n    ///\n    /// Returns a layout with size increased to fit at least `HoleList::min_size` and proper\n    /// alignment of a `Hole`.\n    ///\n    /// The [`allocate_first_fit`][HoleList::allocate_first_fit] and\n    /// [`deallocate`][HoleList::deallocate] methods perform the required alignment\n    /// themselves, so calling this function manually is not necessary.\n    pub fn align_layout(layout: Layout) -> Layout {\n        let mut size = layout.size();\n        if size < Self::min_size() {\n            size = Self::min_size();\n        }\n        let size = align_up_size(size, mem::align_of::<Hole>());\n        Layout::from_size_align(size, layout.align()).unwrap()\n    }\n\n    /// Searches the list for a big enough hole.\n    ///\n    /// A hole is big enough if it can hold an allocation of `layout.size()` bytes with\n    /// the given `layout.align()`. If such a hole is found in the list, a block of the\n    /// required size is allocated from it. Then the start address of that\n    /// block and the aligned layout are returned. The automatic layout alignment is required\n    /// because the `HoleList` has some additional layout requirements for each memory block.\n    ///\n    /// This function uses the \u201cfirst fit\u201d strategy, so it uses the first hole that is big\n    /// enough. Thus the runtime is in O(n) but it should be reasonably fast for small allocations.\n    //\n    // NOTE: We could probably replace this with an `Option` instead of a `Result` in a later\n    // release to remove this clippy warning\n    #[allow(clippy::result_unit_err)]\n    pub fn allocate_first_fit(&mut self, layout: Layout) -> Result<(NonNull<u8>, Layout), ()> {\n        let aligned_layout = Self::align_layout(layout);\n        let mut cursor = self.cursor().ok_or(())?;\n\n        loop {\n            match cursor.split_current(aligned_layout) {\n                Ok((ptr, _len)) => {\n                    return Ok((NonNull::new(ptr).ok_or(())?, aligned_layout));\n                }\n                Err(curs) => {\n                    cursor = curs.next().ok_or(())?;\n                }\n            }\n        }\n    }\n\n    /// Frees the allocation given by `ptr` and `layout`.\n    ///\n    /// This function walks the list and inserts the given block at the correct place. If the freed\n    /// block is adjacent to another free block, the blocks are merged again.\n    /// This operation is in `O(n)` since the list needs to be sorted by address.\n    ///\n    /// [`allocate_first_fit`]: HoleList::allocate_first_fit\n    ///\n    /// # Safety\n    ///\n    /// `ptr` must be a pointer returned by a call to the [`allocate_first_fit`] function with\n    /// identical layout. Undefined behavior may occur for invalid arguments.\n    /// The function performs exactly the same layout adjustments as [`allocate_first_fit`] and\n    /// returns the aligned layout.\n    pub unsafe fn deallocate(&mut self, ptr: NonNull<u8>, layout: Layout) -> Layout {\n        let aligned_layout = Self::align_layout(layout);\n        deallocate(self, ptr.as_ptr(), aligned_layout.size());\n        aligned_layout\n    }\n\n    /// Returns the minimal allocation size. Smaller allocations or deallocations are not allowed.\n    pub fn min_size() -> usize {\n        size_of::<usize>() * 2\n    }\n\n    /// Returns information about the first hole for test purposes.\n    #[cfg(test)]\n    pub fn first_hole(&self) -> Option<(*const u8, usize)> {\n        self.first.next.as_ref().map(|hole| {\n            (hole.as_ptr() as *mut u8 as *const u8, unsafe {\n                hole.as_ref().size\n            })\n        })\n    }\n}\n\nunsafe fn make_hole(addr: *mut u8, size: usize) -> NonNull<Hole> {\n    let hole_addr = addr.cast::<Hole>();\n    debug_assert_eq!(\n        addr as usize % align_of::<Hole>(),\n        0,\n        \"Hole address not aligned!\",\n    );\n    hole_addr.write(Hole { size, next: None });\n    NonNull::new_unchecked(hole_addr)\n}\n\nimpl Cursor {\n    fn try_insert_back(self, node: NonNull<Hole>, bottom: *mut u8) -> Result<Self, Self> {\n        // Covers the case where the new hole exists BEFORE the current pointer,\n        // which only happens when previous is the stub pointer\n        if node < self.hole {\n            let node_u8 = node.as_ptr().cast::<u8>();\n            let node_size = unsafe { node.as_ref().size };\n            let hole_u8 = self.hole.as_ptr().cast::<u8>();\n\n            assert!(\n                node_u8.wrapping_add(node_size) <= hole_u8,\n                \"Freed node aliases existing hole! Bad free?\",\n            );\n            debug_assert_eq!(self.previous().size, 0);\n\n            let Cursor {\n                mut prev,\n                hole,\n                top,\n            } = self;\n            unsafe {\n                let mut node = check_merge_bottom(node, bottom);\n                prev.as_mut().next = Some(node);\n                node.as_mut().next = Some(hole);\n            }\n            Ok(Cursor {\n                prev,\n                hole: node,\n                top,\n            })\n        } else {\n            Err(self)\n        }\n    }\n\n    fn try_insert_after(&mut self, mut node: NonNull<Hole>) -> Result<(), ()> {\n        let node_u8 = node.as_ptr().cast::<u8>();\n        let node_size = unsafe { node.as_ref().size };\n\n        // If we have a next, does the node overlap next?\n        if let Some(next) = self.current().next.as_ref() {\n            if node < *next {\n                let node_u8 = node_u8 as *const u8;\n                assert!(\n                    node_u8.wrapping_add(node_size) <= next.as_ptr().cast::<u8>(),\n                    \"Freed node aliases existing hole! Bad free?\",\n                );\n            } else {\n                // The new hole isn't between current and next.\n                return Err(());\n            }\n        }\n\n        // At this point, we either have no \"next\" pointer, or the hole is\n        // between current and \"next\". The following assert can only trigger\n        // if we've gotten our list out of order.\n        debug_assert!(self.hole < node, \"Hole list out of order?\");\n\n        let hole_u8 = self.hole.as_ptr().cast::<u8>();\n        let hole_size = self.current().size;\n\n        // Does hole overlap node?\n        assert!(\n            hole_u8.wrapping_add(hole_size) <= node_u8,\n            \"Freed node aliases existing hole! Bad free?\",\n        );\n\n        // All good! Let's insert that after.\n        unsafe {\n            let maybe_next = self.hole.as_mut().next.replace(node);\n            node.as_mut().next = maybe_next;\n        }\n\n        Ok(())\n    }\n\n    // Merge the current node with up to n following nodes\n    fn try_merge_next_n(self, max: usize) {\n        let Cursor {\n            prev: _,\n            mut hole,\n            top,\n            ..\n        } = self;\n\n        for _ in 0..max {\n            // Is there a next node?\n            let mut next = if let Some(next) = unsafe { hole.as_mut() }.next.as_ref() {\n                *next\n            } else {\n                // Since there is no NEXT node, we need to check whether the current\n                // hole SHOULD extend to the end, but doesn't. This would happen when\n                // there isn't enough remaining space to place a hole after the current\n                // node's placement.\n                check_merge_top(hole, top);\n                return;\n            };\n\n            // Can we directly merge these? e.g. are they touching?\n            //\n            // NOTE: Because we always use `HoleList::align_layout`, the size of\n            // the new hole is always \"rounded up\" to cover any partial gaps that\n            // would have occurred. For this reason, we DON'T need to \"round up\"\n            // to account for an unaligned hole spot.\n            let hole_u8 = hole.as_ptr().cast::<u8>();\n            let hole_sz = unsafe { hole.as_ref().size };\n            let next_u8 = next.as_ptr().cast::<u8>();\n            let end = hole_u8.wrapping_add(hole_sz);\n\n            let touching = end == next_u8;\n\n            if touching {\n                let next_sz;\n                let next_next;\n                unsafe {\n                    let next_mut = next.as_mut();\n                    next_sz = next_mut.size;\n                    next_next = next_mut.next.take();\n                }\n                unsafe {\n                    let hole_mut = hole.as_mut();\n                    hole_mut.next = next_next;\n                    hole_mut.size += next_sz;\n                }\n                // Okay, we just merged the next item. DON'T move the cursor, as we can\n                // just try to merge the next_next, which is now our next.\n            } else {\n                // Welp, not touching, can't merge. Move to the next node.\n                hole = next;\n            }\n        }\n    }\n}\n\n/// Frees the allocation given by `(addr, size)`. It starts at the given hole and walks the list to\n/// find the correct place (the list is sorted by address).\nfn deallocate(list: &mut HoleList, addr: *mut u8, size: usize) {\n    // Start off by just making this allocation a hole where it stands.\n    // We'll attempt to merge it with other nodes once we figure out where\n    // it should live\n    let hole = unsafe { make_hole(addr, size) };\n\n    // Now, try to get a cursor to the list - this only works if we have at least\n    // one non-\"dummy\" hole in the list\n    let cursor = if let Some(cursor) = list.cursor() {\n        cursor\n    } else {\n        // Oh hey, there are no \"real\" holes at all. That means this just\n        // becomes the only \"real\" hole! Check if this is touching the end\n        // or the beginning of the allocation range\n        let hole = check_merge_bottom(hole, list.bottom);\n        check_merge_top(hole, list.top);\n        list.first.next = Some(hole);\n        return;\n    };\n\n    // First, check if we can just insert this node at the top of the list. If the\n    // insertion succeeded, then our cursor now points to the NEW node, behind the\n    // previous location the cursor was pointing to.\n    //\n    // Otherwise, our cursor will point at the current non-\"dummy\" head of the list\n    let (cursor, n) = match cursor.try_insert_back(hole, list.bottom) {\n        Ok(cursor) => {\n            // Yup! It lives at the front of the list. Hooray! Attempt to merge\n            // it with just ONE next node, since it is at the front of the list\n            (cursor, 1)\n        }\n        Err(mut cursor) => {\n            // Nope. It lives somewhere else. Advance the list until we find its home\n            while let Err(()) = cursor.try_insert_after(hole) {\n                cursor = cursor\n                    .next()\n                    .expect(\"Reached end of holes without finding deallocation hole!\");\n            }\n            // Great! We found a home for it, our cursor is now JUST BEFORE the new\n            // node we inserted, so we need to try to merge up to twice: One to combine\n            // the current node to the new node, then once more to combine the new node\n            // with the node after that.\n            (cursor, 2)\n        }\n    };\n\n    // We now need to merge up to two times to combine the current node with the next\n    // two nodes.\n    cursor.try_merge_next_n(n);\n}\n\n#[cfg(test)]\npub mod test {\n    use crate::Heap;\n    use core::alloc::Layout;\n    use std::mem::MaybeUninit;\n    use std::prelude::v1::*;\n\n    #[repr(align(128))]\n    struct Chonk<const N: usize> {\n        data: [MaybeUninit<u8>; N],\n    }\n\n    impl<const N: usize> Chonk<N> {\n        pub fn new() -> Self {\n            Self {\n                data: [MaybeUninit::uninit(); N],\n            }\n        }\n    }\n\n    fn new_heap() -> Heap {\n        const HEAP_SIZE: usize = 1000;\n        let heap_space = Box::leak(Box::new(Chonk::<HEAP_SIZE>::new()));\n        let data = &mut heap_space.data;\n        let assumed_location = data.as_mut_ptr().cast();\n\n        let heap = Heap::from_slice(data);\n        assert!(heap.bottom() == assumed_location);\n        assert!(heap.size() == HEAP_SIZE);\n        heap\n    }\n\n    #[test]\n    fn cursor() {\n        let mut heap = new_heap();\n        let curs = heap.holes.cursor().unwrap();\n        // This is the \"dummy\" node\n        assert_eq!(curs.previous().size, 0);\n        // This is the \"full\" heap\n        assert_eq!(curs.current().size, 1000);\n        // There is no other hole\n        assert!(curs.next().is_none());\n    }\n\n    #[test]\n    fn aff() {\n        let mut heap = new_heap();\n        let reqd = Layout::from_size_align(256, 1).unwrap();\n        let _ = heap.allocate_first_fit(reqd).unwrap();\n    }\n}\n", "#![cfg_attr(feature = \"const_mut_refs\", feature(const_mut_refs))]\n#![cfg_attr(\n    feature = \"alloc_ref\",\n    feature(allocator_api, alloc_layout_extra, nonnull_slice_from_raw_parts)\n)]\n#![no_std]\n\n#[cfg(test)]\n#[macro_use]\nextern crate std;\n\n#[cfg(feature = \"use_spin\")]\nextern crate spinning_top;\n\n#[cfg(feature = \"use_spin\")]\nuse core::alloc::GlobalAlloc;\nuse core::alloc::Layout;\n#[cfg(feature = \"alloc_ref\")]\nuse core::alloc::{AllocError, Allocator};\nuse core::mem::MaybeUninit;\n#[cfg(feature = \"use_spin\")]\nuse core::ops::Deref;\nuse core::ptr::NonNull;\n#[cfg(test)]\nuse hole::Hole;\nuse hole::HoleList;\n#[cfg(feature = \"use_spin\")]\nuse spinning_top::Spinlock;\n\npub mod hole;\n#[cfg(test)]\nmod test;\n\n/// A fixed size heap backed by a linked list of free memory blocks.\npub struct Heap {\n    used: usize,\n    holes: HoleList,\n}\n\nunsafe impl Send for Heap {}\n\nimpl Heap {\n    /// Creates an empty heap. All allocate calls will return `None`.\n    #[cfg(not(feature = \"const_mut_refs\"))]\n    pub fn empty() -> Heap {\n        Heap {\n            used: 0,\n            holes: HoleList::empty(),\n        }\n    }\n\n    #[cfg(feature = \"const_mut_refs\")]\n    pub const fn empty() -> Heap {\n        Heap {\n            used: 0,\n            holes: HoleList::empty(),\n        }\n    }\n\n    /// Initializes an empty heap\n    ///\n    /// # Safety\n    ///\n    /// This function must be called at most once and must only be used on an\n    /// empty heap.\n    ///\n    /// The bottom address must be valid and the memory in the\n    /// `[heap_bottom, heap_bottom + heap_size)` range must not be used for anything else.\n    /// This function is unsafe because it can cause undefined behavior if the given address\n    /// is invalid.\n    ///\n    /// The provided memory range must be valid for the `'static` lifetime.\n    pub unsafe fn init(&mut self, heap_bottom: *mut u8, heap_size: usize) {\n        self.used = 0;\n        self.holes = HoleList::new(heap_bottom, heap_size);\n    }\n\n    /// Initialize an empty heap with provided memory.\n    ///\n    /// The caller is responsible for procuring a region of raw memory that may be utilized by the\n    /// allocator. This might be done via any method such as (unsafely) taking a region from the\n    /// program's memory, from a mutable static, or by allocating and leaking such memory from\n    /// another allocator.\n    ///\n    /// The latter method may be especially useful if the underlying allocator does not perform\n    /// deallocation (e.g. a simple bump allocator). Then the overlaid linked-list-allocator can\n    /// provide memory reclamation.\n    ///\n    /// # Panics\n    ///\n    /// This method panics if the heap is already initialized.\n    pub fn init_from_slice(&mut self, mem: &'static mut [MaybeUninit<u8>]) {\n        assert!(\n            self.bottom().is_null(),\n            \"The heap has already been initialized.\"\n        );\n        let size = mem.len();\n        let address = mem.as_mut_ptr().cast();\n        // SAFETY: All initialization requires the bottom address to be valid, which implies it\n        // must not be 0. Initially the address is 0. The assertion above ensures that no\n        // initialization had been called before.\n        // The given address and size is valid according to the safety invariants of the mutable\n        // reference handed to us by the caller.\n        unsafe { self.init(address, size) }\n    }\n\n    /// Creates a new heap with the given `bottom` and `size`.\n    ///\n    /// # Safety\n    ///\n    /// The bottom address must be valid and the memory in the\n    /// `[heap_bottom, heap_bottom + heap_size)` range must not be used for anything else.\n    /// This function is unsafe because it can cause undefined behavior if the given address\n    /// is invalid.\n    ///\n    /// The provided memory range must be valid for the `'static` lifetime.\n    pub unsafe fn new(heap_bottom: *mut u8, heap_size: usize) -> Heap {\n        if heap_size < HoleList::min_size() {\n            Self::empty()\n        } else {\n            Heap {\n                used: 0,\n                holes: HoleList::new(heap_bottom, heap_size),\n            }\n        }\n    }\n\n    /// Creates a new heap from a slice of raw memory.\n    ///\n    /// This has the same effect as [`init_from_slice`] on an empty heap, but it is combined into a\n    /// single operation that can not panic.\n    pub fn from_slice(mem: &'static mut [MaybeUninit<u8>]) -> Heap {\n        let size = mem.len();\n        let address = mem.as_mut_ptr().cast();\n        // SAFETY: The given address and size is valid according to the safety invariants of the\n        // mutable reference handed to us by the caller.\n        unsafe { Self::new(address, size) }\n    }\n\n    /// Allocates a chunk of the given size with the given alignment. Returns a pointer to the\n    /// beginning of that chunk if it was successful. Else it returns `None`.\n    /// This function scans the list of free memory blocks and uses the first block that is big\n    /// enough. The runtime is in O(n) where n is the number of free blocks, but it should be\n    /// reasonably fast for small allocations.\n    //\n    // NOTE: We could probably replace this with an `Option` instead of a `Result` in a later\n    // release to remove this clippy warning\n    #[allow(clippy::result_unit_err)]\n    pub fn allocate_first_fit(&mut self, layout: Layout) -> Result<NonNull<u8>, ()> {\n        match self.holes.allocate_first_fit(layout) {\n            Ok((ptr, aligned_layout)) => {\n                self.used += aligned_layout.size();\n                Ok(ptr)\n            }\n            Err(err) => Err(err),\n        }\n    }\n\n    /// Frees the given allocation. `ptr` must be a pointer returned\n    /// by a call to the `allocate_first_fit` function with identical size and alignment.\n    ///\n    /// This function walks the list of free memory blocks and inserts the freed block at the\n    /// correct place. If the freed block is adjacent to another free block, the blocks are merged\n    /// again. This operation is in `O(n)` since the list needs to be sorted by address.\n    ///\n    /// # Safety\n    ///\n    /// `ptr` must be a pointer returned by a call to the [`allocate_first_fit`] function with\n    /// identical layout. Undefined behavior may occur for invalid arguments.\n    pub unsafe fn deallocate(&mut self, ptr: NonNull<u8>, layout: Layout) {\n        self.used -= self.holes.deallocate(ptr, layout).size();\n    }\n\n    /// Returns the bottom address of the heap.\n    pub fn bottom(&self) -> *mut u8 {\n        self.holes.bottom\n    }\n\n    /// Returns the size of the heap.\n    pub fn size(&self) -> usize {\n        (self.top() as usize) - (self.bottom() as usize)\n    }\n\n    /// Return the top address of the heap\n    pub fn top(&self) -> *mut u8 {\n        self.holes.top\n    }\n\n    /// Returns the size of the used part of the heap\n    pub fn used(&self) -> usize {\n        self.used\n    }\n\n    /// Returns the size of the free part of the heap\n    pub fn free(&self) -> usize {\n        self.size() - self.used\n    }\n\n    /// Extends the size of the heap by creating a new hole at the end\n    ///\n    /// # Safety\n    ///\n    /// The amount of data given in `by` MUST exist directly after the original\n    /// range of data provided when constructing the [Heap]. The additional data\n    /// must have the same lifetime of the original range of data.\n    pub unsafe fn extend(&mut self, by: usize) {\n        let top = self.top();\n        let layout = Layout::from_size_align(by, 1).unwrap();\n        self.holes\n            .deallocate(NonNull::new_unchecked(top as *mut u8), layout);\n        self.holes.top = self.holes.top.add(by);\n    }\n}\n\n#[cfg(all(feature = \"alloc_ref\", feature = \"use_spin\"))]\nunsafe impl Allocator for LockedHeap {\n    fn allocate(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError> {\n        if layout.size() == 0 {\n            return Ok(NonNull::slice_from_raw_parts(layout.dangling(), 0));\n        }\n        match self.0.lock().allocate_first_fit(layout) {\n            Ok(ptr) => Ok(NonNull::slice_from_raw_parts(ptr, layout.size())),\n            Err(()) => Err(AllocError),\n        }\n    }\n\n    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout) {\n        if layout.size() != 0 {\n            self.0.lock().deallocate(ptr, layout);\n        }\n    }\n}\n\n#[cfg(feature = \"use_spin\")]\npub struct LockedHeap(Spinlock<Heap>);\n\n#[cfg(feature = \"use_spin\")]\nimpl LockedHeap {\n    /// Creates an empty heap. All allocate calls will return `None`.\n    #[cfg(feature = \"use_spin_nightly\")]\n    pub const fn empty() -> LockedHeap {\n        LockedHeap(Spinlock::new(Heap::empty()))\n    }\n\n    /// Creates an empty heap. All allocate calls will return `None`.\n    #[cfg(not(feature = \"use_spin_nightly\"))]\n    pub fn empty() -> LockedHeap {\n        LockedHeap(Spinlock::new(Heap::empty()))\n    }\n\n    /// Creates a new heap with the given `bottom` and `size`.\n    ///\n    /// # Safety\n    ///\n    /// The bottom address must be valid and the memory in the\n    /// `[heap_bottom, heap_bottom + heap_size)` range must not be used for anything else.\n    /// This function is unsafe because it can cause undefined behavior if the given address\n    /// is invalid.\n    ///\n    /// The provided memory range must be valid for the `'static` lifetime.\n    pub unsafe fn new(heap_bottom: *mut u8, heap_size: usize) -> LockedHeap {\n        LockedHeap(Spinlock::new(Heap {\n            used: 0,\n            holes: HoleList::new(heap_bottom, heap_size),\n        }))\n    }\n}\n\n#[cfg(feature = \"use_spin\")]\nimpl Deref for LockedHeap {\n    type Target = Spinlock<Heap>;\n\n    fn deref(&self) -> &Spinlock<Heap> {\n        &self.0\n    }\n}\n\n#[cfg(feature = \"use_spin\")]\nunsafe impl GlobalAlloc for LockedHeap {\n    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {\n        self.0\n            .lock()\n            .allocate_first_fit(layout)\n            .ok()\n            .map_or(core::ptr::null_mut(), |allocation| allocation.as_ptr())\n    }\n\n    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {\n        self.0\n            .lock()\n            .deallocate(NonNull::new_unchecked(ptr), layout)\n    }\n}\n\n/// Align downwards. Returns the greatest x with alignment `align`\n/// so that x <= addr. The alignment must be a power of 2.\npub fn align_down_size(size: usize, align: usize) -> usize {\n    if align.is_power_of_two() {\n        size & !(align - 1)\n    } else if align == 0 {\n        size\n    } else {\n        panic!(\"`align` must be a power of 2\");\n    }\n}\n\npub fn align_up_size(size: usize, align: usize) -> usize {\n    align_down_size(size + align - 1, align)\n}\n\n/// Align upwards. Returns the smallest x with alignment `align`\n/// so that x >= addr. The alignment must be a power of 2.\npub fn align_up(addr: *mut u8, align: usize) -> *mut u8 {\n    let offset = addr.align_offset(align);\n    addr.wrapping_add(offset)\n}\n", "use super::*;\nuse core::alloc::Layout;\nuse std::mem::{align_of, size_of, MaybeUninit};\nuse std::prelude::v1::*;\n\n#[repr(align(128))]\nstruct Chonk<const N: usize> {\n    data: [MaybeUninit<u8>; N],\n}\n\nimpl<const N: usize> Chonk<N> {\n    pub fn new() -> Self {\n        Self {\n            data: [MaybeUninit::uninit(); N],\n        }\n    }\n}\n\nfn new_heap() -> Heap {\n    const HEAP_SIZE: usize = 1000;\n    let heap_space = Box::leak(Box::new(Chonk::<HEAP_SIZE>::new()));\n    let data = &mut heap_space.data;\n    let assumed_location = data.as_mut_ptr().cast();\n\n    let heap = Heap::from_slice(data);\n    assert!(heap.bottom() == assumed_location);\n    assert!(heap.size() == HEAP_SIZE);\n    heap\n}\n\nfn new_max_heap() -> Heap {\n    const HEAP_SIZE: usize = 1024;\n    const HEAP_SIZE_MAX: usize = 2048;\n    let heap_space = Box::leak(Box::new(Chonk::<HEAP_SIZE_MAX>::new()));\n    let data = &mut heap_space.data;\n    let start_ptr = data.as_mut_ptr().cast();\n\n    // Unsafe so that we have provenance over the whole allocation.\n    let heap = unsafe { Heap::new(start_ptr, HEAP_SIZE) };\n    assert!(heap.bottom() == start_ptr);\n    assert!(heap.size() == HEAP_SIZE);\n    heap\n}\n\n#[test]\nfn empty() {\n    let mut heap = Heap::empty();\n    let layout = Layout::from_size_align(1, 1).unwrap();\n    assert!(heap.allocate_first_fit(layout.clone()).is_err());\n}\n\n#[test]\nfn oom() {\n    let mut heap = new_heap();\n    let layout = Layout::from_size_align(heap.size() + 1, align_of::<usize>());\n    let addr = heap.allocate_first_fit(layout.unwrap());\n    assert!(addr.is_err());\n}\n\n#[test]\nfn allocate_double_usize() {\n    let mut heap = new_heap();\n    let size = size_of::<usize>() * 2;\n    let layout = Layout::from_size_align(size, align_of::<usize>());\n    let addr = heap.allocate_first_fit(layout.unwrap());\n    assert!(addr.is_ok());\n    let addr = addr.unwrap().as_ptr();\n    assert!(addr == heap.bottom());\n    let (hole_addr, hole_size) = heap.holes.first_hole().expect(\"ERROR: no hole left\");\n    assert!(hole_addr == heap.bottom().wrapping_add(size));\n    assert!(hole_size == heap.size() - size);\n\n    unsafe {\n        assert_eq!(\n            (*((addr.wrapping_add(size)) as *const Hole)).size,\n            heap.size() - size\n        );\n    }\n}\n\n#[test]\nfn allocate_and_free_double_usize() {\n    let mut heap = new_heap();\n\n    let layout = Layout::from_size_align(size_of::<usize>() * 2, align_of::<usize>()).unwrap();\n    let x = heap.allocate_first_fit(layout.clone()).unwrap();\n    unsafe {\n        *(x.as_ptr() as *mut (usize, usize)) = (0xdeafdeadbeafbabe, 0xdeafdeadbeafbabe);\n\n        heap.deallocate(x, layout.clone());\n        let real_first = heap.holes.first.next.as_ref().unwrap().as_ref();\n\n        assert_eq!(real_first.size, heap.size());\n        assert!(real_first.next.is_none());\n    }\n}\n\n#[test]\nfn deallocate_right_before() {\n    let mut heap = new_heap();\n    let layout = Layout::from_size_align(size_of::<usize>() * 5, 1).unwrap();\n\n    let x = heap.allocate_first_fit(layout.clone()).unwrap();\n    let y = heap.allocate_first_fit(layout.clone()).unwrap();\n    let z = heap.allocate_first_fit(layout.clone()).unwrap();\n\n    unsafe {\n        heap.deallocate(y, layout.clone());\n        assert_eq!((*(y.as_ptr() as *const Hole)).size, layout.size());\n        heap.deallocate(x, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, layout.size() * 2);\n        heap.deallocate(z, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, heap.size());\n    }\n}\n\n#[test]\nfn deallocate_right_behind() {\n    let mut heap = new_heap();\n    let size = size_of::<usize>() * 5;\n    let layout = Layout::from_size_align(size, 1).unwrap();\n\n    let x = heap.allocate_first_fit(layout.clone()).unwrap();\n    let y = heap.allocate_first_fit(layout.clone()).unwrap();\n    let z = heap.allocate_first_fit(layout.clone()).unwrap();\n\n    unsafe {\n        heap.deallocate(x, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, size);\n        heap.deallocate(y, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, size * 2);\n        heap.deallocate(z, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, heap.size());\n    }\n}\n\n#[test]\nfn deallocate_middle() {\n    let mut heap = new_heap();\n    let size = size_of::<usize>() * 5;\n    let layout = Layout::from_size_align(size, 1).unwrap();\n\n    let x = heap.allocate_first_fit(layout.clone()).unwrap();\n    let y = heap.allocate_first_fit(layout.clone()).unwrap();\n    let z = heap.allocate_first_fit(layout.clone()).unwrap();\n    let a = heap.allocate_first_fit(layout.clone()).unwrap();\n\n    unsafe {\n        heap.deallocate(x, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, size);\n        heap.deallocate(z, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, size);\n        assert_eq!((*(z.as_ptr() as *const Hole)).size, size);\n        heap.deallocate(y, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, size * 3);\n        heap.deallocate(a, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, heap.size());\n    }\n}\n\n#[test]\nfn reallocate_double_usize() {\n    let mut heap = new_heap();\n\n    let layout = Layout::from_size_align(size_of::<usize>() * 2, align_of::<usize>()).unwrap();\n\n    let x = heap.allocate_first_fit(layout.clone()).unwrap();\n    unsafe {\n        heap.deallocate(x, layout.clone());\n    }\n\n    let y = heap.allocate_first_fit(layout.clone()).unwrap();\n    unsafe {\n        heap.deallocate(y, layout.clone());\n    }\n\n    assert_eq!(x, y);\n}\n\n#[test]\nfn allocate_many_size_aligns() {\n    use core::ops::{Range, RangeInclusive};\n\n    #[cfg(not(miri))]\n    const SIZE: RangeInclusive<usize> = 1..=512;\n\n    #[cfg(miri)]\n    const SIZE: RangeInclusive<usize> = 256..=(256 + core::mem::size_of::<crate::hole::Hole>());\n\n    #[cfg(not(miri))]\n    const ALIGN: Range<usize> = 0..10;\n\n    #[cfg(miri)]\n    const ALIGN: Range<usize> = 1..4;\n\n    const STRATS: Range<usize> = 0..4;\n\n    let mut heap = new_heap();\n    assert_eq!(heap.size(), 1000);\n\n    heap.holes.debug();\n\n    let max_alloc = Layout::from_size_align(1000, 1).unwrap();\n    let full = heap.allocate_first_fit(max_alloc).unwrap();\n    unsafe {\n        heap.deallocate(full, max_alloc);\n    }\n\n    heap.holes.debug();\n\n    struct Alloc {\n        alloc: NonNull<u8>,\n        layout: Layout,\n    }\n\n    // NOTE: Printing to the console SIGNIFICANTLY slows down miri.\n\n    for strat in STRATS {\n        for align in ALIGN {\n            for size in SIZE {\n                #[cfg(not(miri))]\n                {\n                    println!(\"=========================================================\");\n                    println!(\"Align: {}\", 1 << align);\n                    println!(\"Size:  {}\", size);\n                    println!(\"Free Pattern: {}/0..4\", strat);\n                    println!();\n                }\n                let mut allocs = vec![];\n\n                let layout = Layout::from_size_align(size, 1 << align).unwrap();\n                while let Ok(alloc) = heap.allocate_first_fit(layout) {\n                    #[cfg(not(miri))]\n                    heap.holes.debug();\n                    allocs.push(Alloc { alloc, layout });\n                }\n\n                #[cfg(not(miri))]\n                println!(\"Allocs: {} - {} bytes\", allocs.len(), allocs.len() * size);\n\n                match strat {\n                    0 => {\n                        // Forward\n                        allocs.drain(..).for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                    }\n                    1 => {\n                        // Backwards\n                        allocs.drain(..).rev().for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                    }\n                    2 => {\n                        // Interleaved forwards\n                        let mut a = Vec::new();\n                        let mut b = Vec::new();\n                        for (i, alloc) in allocs.drain(..).enumerate() {\n                            if (i % 2) == 0 {\n                                a.push(alloc);\n                            } else {\n                                b.push(alloc);\n                            }\n                        }\n                        a.drain(..).for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                        b.drain(..).for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                    }\n                    3 => {\n                        // Interleaved backwards\n                        let mut a = Vec::new();\n                        let mut b = Vec::new();\n                        for (i, alloc) in allocs.drain(..).rev().enumerate() {\n                            if (i % 2) == 0 {\n                                a.push(alloc);\n                            } else {\n                                b.push(alloc);\n                            }\n                        }\n                        a.drain(..).for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                        b.drain(..).for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                    }\n                    _ => panic!(),\n                }\n\n                #[cfg(not(miri))]\n                println!(\"MAX CHECK\");\n\n                let full = heap.allocate_first_fit(max_alloc).unwrap();\n                unsafe {\n                    heap.deallocate(full, max_alloc);\n                }\n\n                #[cfg(not(miri))]\n                println!();\n            }\n        }\n    }\n}\n\n#[test]\nfn allocate_multiple_sizes() {\n    let mut heap = new_heap();\n    let base_size = size_of::<usize>();\n    let base_align = align_of::<usize>();\n\n    let layout_1 = Layout::from_size_align(base_size * 2, base_align).unwrap();\n    let layout_2 = Layout::from_size_align(base_size * 7, base_align).unwrap();\n    let layout_3 = Layout::from_size_align(base_size * 3, base_align * 4).unwrap();\n    let layout_4 = Layout::from_size_align(base_size * 4, base_align).unwrap();\n\n    let x = heap.allocate_first_fit(layout_1.clone()).unwrap();\n    let y = heap.allocate_first_fit(layout_2.clone()).unwrap();\n    assert_eq!(y.as_ptr() as usize, x.as_ptr() as usize + base_size * 2);\n    let z = heap.allocate_first_fit(layout_3.clone()).unwrap();\n    assert_eq!(z.as_ptr() as usize % (base_size * 4), 0);\n\n    unsafe {\n        heap.deallocate(x, layout_1.clone());\n    }\n\n    let a = heap.allocate_first_fit(layout_4.clone()).unwrap();\n    let b = heap.allocate_first_fit(layout_1.clone()).unwrap();\n    assert_eq!(b, x);\n\n    unsafe {\n        heap.deallocate(y, layout_2);\n        heap.deallocate(z, layout_3);\n        heap.deallocate(a, layout_4);\n        heap.deallocate(b, layout_1);\n    }\n}\n\n// This test makes sure that the heap works correctly when the input slice has\n// a variety of non-Hole aligned starting addresses\n#[test]\nfn allocate_multiple_unaligned() {\n    for offset in 0..=Layout::new::<Hole>().size() {\n        let mut heap = new_heap_skip(offset);\n        let base_size = size_of::<usize>();\n        let base_align = align_of::<usize>();\n\n        let layout_1 = Layout::from_size_align(base_size * 2, base_align).unwrap();\n        let layout_2 = Layout::from_size_align(base_size * 7, base_align).unwrap();\n        let layout_3 = Layout::from_size_align(base_size * 3, base_align * 4).unwrap();\n        let layout_4 = Layout::from_size_align(base_size * 4, base_align).unwrap();\n\n        let x = heap.allocate_first_fit(layout_1.clone()).unwrap();\n        let y = heap.allocate_first_fit(layout_2.clone()).unwrap();\n        assert_eq!(y.as_ptr() as usize, x.as_ptr() as usize + base_size * 2);\n        let z = heap.allocate_first_fit(layout_3.clone()).unwrap();\n        assert_eq!(z.as_ptr() as usize % (base_size * 4), 0);\n\n        unsafe {\n            heap.deallocate(x, layout_1.clone());\n        }\n\n        let a = heap.allocate_first_fit(layout_4.clone()).unwrap();\n        let b = heap.allocate_first_fit(layout_1.clone()).unwrap();\n        assert_eq!(b, x);\n\n        unsafe {\n            heap.deallocate(y, layout_2);\n            heap.deallocate(z, layout_3);\n            heap.deallocate(a, layout_4);\n            heap.deallocate(b, layout_1);\n        }\n    }\n}\n\nfn new_heap_skip(ct: usize) -> Heap {\n    const HEAP_SIZE: usize = 1000;\n    let heap_space = Box::leak(Box::new(Chonk::<HEAP_SIZE>::new()));\n    let data = &mut heap_space.data[ct..];\n    let heap = Heap::from_slice(data);\n    heap\n}\n\n#[test]\nfn allocate_usize() {\n    let mut heap = new_heap();\n\n    let layout = Layout::from_size_align(size_of::<usize>(), 1).unwrap();\n\n    assert!(heap.allocate_first_fit(layout.clone()).is_ok());\n}\n\n#[test]\nfn allocate_usize_in_bigger_block() {\n    let mut heap = new_heap();\n\n    let layout_1 = Layout::from_size_align(size_of::<usize>() * 2, 1).unwrap();\n    let layout_2 = Layout::from_size_align(size_of::<usize>(), 1).unwrap();\n\n    let x = heap.allocate_first_fit(layout_1.clone()).unwrap();\n    let y = heap.allocate_first_fit(layout_1.clone()).unwrap();\n    unsafe {\n        heap.deallocate(x, layout_1.clone());\n    }\n\n    let z = heap.allocate_first_fit(layout_2.clone());\n    assert!(z.is_ok());\n    let z = z.unwrap();\n    assert_eq!(x, z);\n\n    unsafe {\n        heap.deallocate(y, layout_1.clone());\n        heap.deallocate(z, layout_2);\n    }\n}\n\n#[test]\n// see https://github.com/phil-opp/blog_os/issues/160\nfn align_from_small_to_big() {\n    let mut heap = new_heap();\n\n    let layout_1 = Layout::from_size_align(28, 4).unwrap();\n    let layout_2 = Layout::from_size_align(8, 8).unwrap();\n\n    // allocate 28 bytes so that the heap end is only 4 byte aligned\n    assert!(heap.allocate_first_fit(layout_1.clone()).is_ok());\n    // try to allocate a 8 byte aligned block\n    assert!(heap.allocate_first_fit(layout_2.clone()).is_ok());\n}\n\n#[test]\nfn extend_empty_heap() {\n    let mut heap = new_max_heap();\n\n    unsafe {\n        heap.extend(1024);\n    }\n\n    // Try to allocate full heap after extend\n    let layout = Layout::from_size_align(2048, 1).unwrap();\n    assert!(heap.allocate_first_fit(layout.clone()).is_ok());\n}\n\n#[test]\nfn extend_full_heap() {\n    let mut heap = new_max_heap();\n\n    let layout = Layout::from_size_align(1024, 1).unwrap();\n\n    // Allocate full heap, extend and allocate again to the max\n    assert!(heap.allocate_first_fit(layout.clone()).is_ok());\n    unsafe {\n        heap.extend(1024);\n    }\n    assert!(heap.allocate_first_fit(layout.clone()).is_ok());\n}\n\n#[test]\nfn extend_fragmented_heap() {\n    let mut heap = new_max_heap();\n\n    let layout_1 = Layout::from_size_align(512, 1).unwrap();\n    let layout_2 = Layout::from_size_align(1024, 1).unwrap();\n\n    let alloc1 = heap.allocate_first_fit(layout_1.clone());\n    let alloc2 = heap.allocate_first_fit(layout_1.clone());\n\n    assert!(alloc1.is_ok());\n    assert!(alloc2.is_ok());\n\n    unsafe {\n        // Create a hole at the beginning of the heap\n        heap.deallocate(alloc1.unwrap(), layout_1.clone());\n    }\n\n    unsafe {\n        heap.extend(1024);\n    }\n\n    // We got additional 1024 bytes hole at the end of the heap\n    // Try to allocate there\n    assert!(heap.allocate_first_fit(layout_2.clone()).is_ok());\n}\n"], "fixing_code": ["use core::alloc::Layout;\nuse core::mem;\nuse core::mem::{align_of, size_of};\nuse core::ptr::null_mut;\nuse core::ptr::NonNull;\n\nuse crate::{align_down_size, align_up_size};\n\nuse super::align_up;\n\n/// A sorted list of holes. It uses the the holes itself to store its nodes.\npub struct HoleList {\n    pub(crate) first: Hole, // dummy\n    pub(crate) bottom: *mut u8,\n    pub(crate) top: *mut u8,\n    pub(crate) pending_extend: u8,\n}\n\npub(crate) struct Cursor {\n    prev: NonNull<Hole>,\n    hole: NonNull<Hole>,\n    top: *mut u8,\n}\n\n/// A block containing free memory. It points to the next hole and thus forms a linked list.\npub(crate) struct Hole {\n    pub size: usize,\n    pub next: Option<NonNull<Hole>>,\n}\n\n/// Basic information about a hole.\n#[derive(Debug, Clone, Copy)]\nstruct HoleInfo {\n    addr: *mut u8,\n    size: usize,\n}\n\nimpl Cursor {\n    fn next(mut self) -> Option<Self> {\n        unsafe {\n            self.hole.as_mut().next.map(|nhole| Cursor {\n                prev: self.hole,\n                hole: nhole,\n                top: self.top,\n            })\n        }\n    }\n\n    fn current(&self) -> &Hole {\n        unsafe { self.hole.as_ref() }\n    }\n\n    fn previous(&self) -> &Hole {\n        unsafe { self.prev.as_ref() }\n    }\n\n    // On success, it returns the new allocation, and the linked list has been updated\n    // to accomodate any new holes and allocation. On error, it returns the cursor\n    // unmodified, and has made no changes to the linked list of holes.\n    fn split_current(self, required_layout: Layout) -> Result<(*mut u8, usize), Self> {\n        let front_padding;\n        let alloc_ptr;\n        let alloc_size;\n        let back_padding;\n\n        // Here we create a scope, JUST to make sure that any created references do not\n        // live to the point where we start doing pointer surgery below.\n        {\n            let hole_size = self.current().size;\n            let hole_addr_u8 = self.hole.as_ptr().cast::<u8>();\n            let required_size = required_layout.size();\n            let required_align = required_layout.align();\n\n            // Quick check: If the new item is larger than the current hole, it's never gunna\n            // work. Go ahead and bail early to save ourselves some math.\n            if hole_size < required_size {\n                return Err(self);\n            }\n\n            // Attempt to fracture the current hole into the following parts:\n            // ([front_padding], allocation, [back_padding])\n            //\n            // The paddings are optional, and only placed if required.\n            //\n            // First, figure out if front padding is necessary. This would be necessary if the new\n            // allocation has a larger alignment requirement than the current hole, and we didn't get\n            // lucky that the current position was well-aligned enough for the new item.\n            let aligned_addr = if hole_addr_u8 == align_up(hole_addr_u8, required_align) {\n                // hole has already the required alignment, no front padding is needed.\n                front_padding = None;\n                hole_addr_u8\n            } else {\n                // Unfortunately, we did not get lucky. Instead: Push the \"starting location\" FORWARD the size\n                // of a hole node, to guarantee there is at least enough room for the hole header, and\n                // potentially additional space.\n                let new_start = hole_addr_u8.wrapping_add(HoleList::min_size());\n\n                let aligned_addr = align_up(new_start, required_align);\n                front_padding = Some(HoleInfo {\n                    // Our new front padding will exist at the same location as the previous hole,\n                    // it will just have a smaller size after we have chopped off the \"tail\" for\n                    // the allocation.\n                    addr: hole_addr_u8,\n                    size: (aligned_addr as usize) - (hole_addr_u8 as usize),\n                });\n                aligned_addr\n            };\n\n            // Okay, now that we found space, we need to see if the decisions we just made\n            // ACTUALLY fit in the previous hole space\n            let allocation_end = aligned_addr.wrapping_add(required_size);\n            let hole_end = hole_addr_u8.wrapping_add(hole_size);\n\n            if allocation_end > hole_end {\n                // hole is too small\n                return Err(self);\n            }\n\n            // Yes! We have successfully placed our allocation as well.\n            alloc_ptr = aligned_addr;\n            alloc_size = required_size;\n\n            // Okay, time to move onto the back padding. Here, we are opportunistic -\n            // if it fits, we sits. Otherwise we just skip adding the back padding, and\n            // sort of assume that the allocation is actually a bit larger than it\n            // actually needs to be.\n            //\n            // NOTE: Because we always use `HoleList::align_layout`, the size of\n            // the new allocation is always \"rounded up\" to cover any partial gaps that\n            // would have occurred. For this reason, we DON'T need to \"round up\"\n            // to account for an unaligned hole spot.\n            let hole_layout = Layout::new::<Hole>();\n            let back_padding_start = align_up(allocation_end, hole_layout.align());\n            let back_padding_end = back_padding_start.wrapping_add(hole_layout.size());\n\n            // Will the proposed new back padding actually fit in the old hole slot?\n            back_padding = if back_padding_end <= hole_end {\n                // Yes, it does! Place a back padding node\n                Some(HoleInfo {\n                    addr: back_padding_start,\n                    size: (hole_end as usize) - (back_padding_start as usize),\n                })\n            } else {\n                // No, it does not. We are now pretending the allocation now\n                // holds the extra 0..size_of::<Hole>() bytes that are not\n                // big enough to hold what SHOULD be back_padding\n                None\n            };\n        }\n\n        ////////////////////////////////////////////////////////////////////////////\n        // This is where we actually perform surgery on the linked list.\n        ////////////////////////////////////////////////////////////////////////////\n        let Cursor {\n            mut prev, mut hole, ..\n        } = self;\n        // Remove the current location from the previous node\n        unsafe {\n            prev.as_mut().next = None;\n        }\n        // Take the next node out of our current node\n        let maybe_next_addr: Option<NonNull<Hole>> = unsafe { hole.as_mut().next.take() };\n\n        // As of now, the old `Hole` is no more. We are about to replace it with one or more of\n        // the front padding, the allocation, and the back padding.\n\n        match (front_padding, back_padding) {\n            (None, None) => {\n                // No padding at all, how lucky! We still need to connect the PREVIOUS node\n                // to the NEXT node, if there was one\n                unsafe {\n                    prev.as_mut().next = maybe_next_addr;\n                }\n            }\n            (None, Some(singlepad)) | (Some(singlepad), None) => unsafe {\n                // We have front padding OR back padding, but not both.\n                //\n                // Replace the old node with the new single node. We need to stitch the new node\n                // into the linked list. Start by writing the padding into the proper location\n                let singlepad_ptr = singlepad.addr.cast::<Hole>();\n                singlepad_ptr.write(Hole {\n                    size: singlepad.size,\n                    // If the old hole had a next pointer, the single padding now takes\n                    // \"ownership\" of that link\n                    next: maybe_next_addr,\n                });\n\n                // Then connect the OLD previous to the NEW single padding\n                prev.as_mut().next = Some(NonNull::new_unchecked(singlepad_ptr));\n            },\n            (Some(frontpad), Some(backpad)) => unsafe {\n                // We have front padding AND back padding.\n                //\n                // We need to stich them together as two nodes where there used to\n                // only be one. Start with the back padding.\n                let backpad_ptr = backpad.addr.cast::<Hole>();\n                backpad_ptr.write(Hole {\n                    size: backpad.size,\n                    // If the old hole had a next pointer, the BACK padding now takes\n                    // \"ownership\" of that link\n                    next: maybe_next_addr,\n                });\n\n                // Now we emplace the front padding, and link it to both the back padding,\n                // and the old previous\n                let frontpad_ptr = frontpad.addr.cast::<Hole>();\n                frontpad_ptr.write(Hole {\n                    size: frontpad.size,\n                    // We now connect the FRONT padding to the BACK padding\n                    next: Some(NonNull::new_unchecked(backpad_ptr)),\n                });\n\n                // Then connect the OLD previous to the NEW FRONT padding\n                prev.as_mut().next = Some(NonNull::new_unchecked(frontpad_ptr));\n            },\n        }\n\n        // Well that went swimmingly! Hand off the allocation, with surgery performed successfully!\n        Ok((alloc_ptr, alloc_size))\n    }\n}\n\n// See if we can extend this hole towards the end of the allocation region\n// If so: increase the size of the node. If no: keep the node as-is\nfn check_merge_top(mut node: NonNull<Hole>, top: *mut u8) {\n    let node_u8 = node.as_ptr().cast::<u8>();\n    let node_sz = unsafe { node.as_ref().size };\n\n    // If this is the last node, we need to see if we need to merge to the end\n    let end = node_u8.wrapping_add(node_sz);\n    let hole_layout = Layout::new::<Hole>();\n    if end < top {\n        let next_hole_end = align_up(end, hole_layout.align()).wrapping_add(hole_layout.size());\n\n        if next_hole_end > top {\n            let offset = (top as usize) - (end as usize);\n            unsafe {\n                node.as_mut().size += offset;\n            }\n        }\n    }\n}\n\n// See if we can scoot this hole back to the bottom of the allocation region\n// If so: create and return the new hole. If not: return the existing hole\nfn check_merge_bottom(node: NonNull<Hole>, bottom: *mut u8) -> NonNull<Hole> {\n    debug_assert_eq!(bottom as usize % align_of::<Hole>(), 0);\n\n    if bottom.wrapping_add(core::mem::size_of::<Hole>()) > node.as_ptr().cast::<u8>() {\n        let offset = (node.as_ptr() as usize) - (bottom as usize);\n        let size = unsafe { node.as_ref() }.size + offset;\n        unsafe { make_hole(bottom, size) }\n    } else {\n        node\n    }\n}\n\nimpl HoleList {\n    /// Creates an empty `HoleList`.\n    #[cfg(not(feature = \"const_mut_refs\"))]\n    pub fn empty() -> HoleList {\n        HoleList {\n            first: Hole {\n                size: 0,\n                next: None,\n            },\n            bottom: null_mut(),\n            top: null_mut(),\n        }\n    }\n\n    /// Creates an empty `HoleList`.\n    #[cfg(feature = \"const_mut_refs\")]\n    pub const fn empty() -> HoleList {\n        HoleList {\n            first: Hole {\n                size: 0,\n                next: None,\n            },\n            bottom: null_mut(),\n            top: null_mut(),\n            pending_extend: 0,\n        }\n    }\n\n    pub(crate) fn cursor(&mut self) -> Option<Cursor> {\n        if let Some(hole) = self.first.next {\n            Some(Cursor {\n                hole,\n                prev: NonNull::new(&mut self.first)?,\n                top: self.top,\n            })\n        } else {\n            None\n        }\n    }\n\n    #[cfg(test)]\n    #[allow(dead_code)]\n    pub(crate) fn debug(&mut self) {\n        if let Some(cursor) = self.cursor() {\n            let mut cursor = cursor;\n            loop {\n                println!(\n                    \"prev: {:?}[{}], hole: {:?}[{}]\",\n                    cursor.previous() as *const Hole,\n                    cursor.previous().size,\n                    cursor.current() as *const Hole,\n                    cursor.current().size,\n                );\n                if let Some(c) = cursor.next() {\n                    cursor = c;\n                } else {\n                    println!(\"Done!\");\n                    return;\n                }\n            }\n        } else {\n            println!(\"No holes\");\n        }\n    }\n\n    /// Creates a `HoleList` that contains the given hole.\n    ///\n    /// The `hole_addr` pointer is automatically aligned, so the `bottom`\n    /// field might be larger than the given `hole_addr`.\n    ///\n    /// The given `hole_size` must be large enough to store the required\n    /// metadata, otherwise this function will panic. Depending on the\n    /// alignment of the `hole_addr` pointer, the minimum size is between\n    /// `2 * size_of::<usize>` and `3 * size_of::<usize>`.\n    ///\n    /// The usable size for allocations will be truncated to the nearest\n    /// alignment of `align_of::<usize>`. Any extra bytes left at the end\n    /// will be reclaimed once sufficient additional space is given to\n    /// [`extend`][crate::Heap::extend].\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe because it creates a hole at the given `hole_addr`.\n    /// This can cause undefined behavior if this address is invalid or if memory from the\n    /// `[hole_addr, hole_addr+size)` range is used somewhere else.\n    pub unsafe fn new(hole_addr: *mut u8, hole_size: usize) -> HoleList {\n        assert_eq!(size_of::<Hole>(), Self::min_size());\n        assert!(hole_size >= size_of::<Hole>());\n\n        let aligned_hole_addr = align_up(hole_addr, align_of::<Hole>());\n        let requested_hole_size = hole_size - ((aligned_hole_addr as usize) - (hole_addr as usize));\n        let aligned_hole_size = align_down_size(requested_hole_size, align_of::<Hole>());\n        assert!(aligned_hole_size >= size_of::<Hole>());\n\n        let ptr = aligned_hole_addr as *mut Hole;\n        ptr.write(Hole {\n            size: aligned_hole_size,\n            next: None,\n        });\n\n        assert_eq!(\n            hole_addr.wrapping_add(hole_size),\n            aligned_hole_addr.wrapping_add(requested_hole_size)\n        );\n\n        HoleList {\n            first: Hole {\n                size: 0,\n                next: Some(NonNull::new_unchecked(ptr)),\n            },\n            bottom: aligned_hole_addr,\n            top: aligned_hole_addr.wrapping_add(aligned_hole_size),\n            pending_extend: (requested_hole_size - aligned_hole_size) as u8,\n        }\n    }\n\n    /// Aligns the given layout for use with `HoleList`.\n    ///\n    /// Returns a layout with size increased to fit at least `HoleList::min_size` and proper\n    /// alignment of a `Hole`.\n    ///\n    /// The [`allocate_first_fit`][HoleList::allocate_first_fit] and\n    /// [`deallocate`][HoleList::deallocate] methods perform the required alignment\n    /// themselves, so calling this function manually is not necessary.\n    pub fn align_layout(layout: Layout) -> Layout {\n        let mut size = layout.size();\n        if size < Self::min_size() {\n            size = Self::min_size();\n        }\n        let size = align_up_size(size, mem::align_of::<Hole>());\n        Layout::from_size_align(size, layout.align()).unwrap()\n    }\n\n    /// Searches the list for a big enough hole.\n    ///\n    /// A hole is big enough if it can hold an allocation of `layout.size()` bytes with\n    /// the given `layout.align()`. If such a hole is found in the list, a block of the\n    /// required size is allocated from it. Then the start address of that\n    /// block and the aligned layout are returned. The automatic layout alignment is required\n    /// because the `HoleList` has some additional layout requirements for each memory block.\n    ///\n    /// This function uses the \u201cfirst fit\u201d strategy, so it uses the first hole that is big\n    /// enough. Thus the runtime is in O(n) but it should be reasonably fast for small allocations.\n    //\n    // NOTE: We could probably replace this with an `Option` instead of a `Result` in a later\n    // release to remove this clippy warning\n    #[allow(clippy::result_unit_err)]\n    pub fn allocate_first_fit(&mut self, layout: Layout) -> Result<(NonNull<u8>, Layout), ()> {\n        let aligned_layout = Self::align_layout(layout);\n        let mut cursor = self.cursor().ok_or(())?;\n\n        loop {\n            match cursor.split_current(aligned_layout) {\n                Ok((ptr, _len)) => {\n                    return Ok((NonNull::new(ptr).ok_or(())?, aligned_layout));\n                }\n                Err(curs) => {\n                    cursor = curs.next().ok_or(())?;\n                }\n            }\n        }\n    }\n\n    /// Frees the allocation given by `ptr` and `layout`.\n    ///\n    /// This function walks the list and inserts the given block at the correct place. If the freed\n    /// block is adjacent to another free block, the blocks are merged again.\n    /// This operation is in `O(n)` since the list needs to be sorted by address.\n    ///\n    /// [`allocate_first_fit`]: HoleList::allocate_first_fit\n    ///\n    /// # Safety\n    ///\n    /// `ptr` must be a pointer returned by a call to the [`allocate_first_fit`] function with\n    /// identical layout. Undefined behavior may occur for invalid arguments.\n    /// The function performs exactly the same layout adjustments as [`allocate_first_fit`] and\n    /// returns the aligned layout.\n    pub unsafe fn deallocate(&mut self, ptr: NonNull<u8>, layout: Layout) -> Layout {\n        let aligned_layout = Self::align_layout(layout);\n        deallocate(self, ptr.as_ptr(), aligned_layout.size());\n        aligned_layout\n    }\n\n    /// Returns the minimal allocation size. Smaller allocations or deallocations are not allowed.\n    pub fn min_size() -> usize {\n        size_of::<usize>() * 2\n    }\n\n    /// Returns information about the first hole for test purposes.\n    #[cfg(test)]\n    pub fn first_hole(&self) -> Option<(*const u8, usize)> {\n        self.first.next.as_ref().map(|hole| {\n            (hole.as_ptr() as *mut u8 as *const u8, unsafe {\n                hole.as_ref().size\n            })\n        })\n    }\n\n    pub(crate) unsafe fn extend(&mut self, by: usize) {\n        assert!(!self.top.is_null(), \"tried to extend an empty heap\");\n\n        let top = self.top;\n\n        let dead_space = top.align_offset(align_of::<Hole>());\n        debug_assert_eq!(\n            0, dead_space,\n            \"dead space detected during extend: {} bytes. This means top was unaligned\",\n            dead_space\n        );\n\n        debug_assert!(\n            (self.pending_extend as usize) < Self::min_size(),\n            \"pending extend was larger than expected\"\n        );\n\n        // join this extend request with any pending (but not yet acted on) extension\n        let extend_by = self.pending_extend as usize + by;\n\n        let minimum_extend = Self::min_size();\n        if extend_by < minimum_extend {\n            self.pending_extend = extend_by as u8;\n            return;\n        }\n\n        // only extend up to another valid boundary\n        let new_hole_size = align_down_size(extend_by, align_of::<Hole>());\n        let layout = Layout::from_size_align(new_hole_size, 1).unwrap();\n\n        // instantiate the hole by forcing a deallocation on the new memory\n        self.deallocate(NonNull::new_unchecked(top as *mut u8), layout);\n        self.top = top.add(new_hole_size);\n\n        // save extra bytes given to extend that weren't aligned to the hole size\n        self.pending_extend = (extend_by - new_hole_size) as u8;\n    }\n}\n\nunsafe fn make_hole(addr: *mut u8, size: usize) -> NonNull<Hole> {\n    let hole_addr = addr.cast::<Hole>();\n    debug_assert_eq!(\n        addr as usize % align_of::<Hole>(),\n        0,\n        \"Hole address not aligned!\",\n    );\n    hole_addr.write(Hole { size, next: None });\n    NonNull::new_unchecked(hole_addr)\n}\n\nimpl Cursor {\n    fn try_insert_back(self, node: NonNull<Hole>, bottom: *mut u8) -> Result<Self, Self> {\n        // Covers the case where the new hole exists BEFORE the current pointer,\n        // which only happens when previous is the stub pointer\n        if node < self.hole {\n            let node_u8 = node.as_ptr().cast::<u8>();\n            let node_size = unsafe { node.as_ref().size };\n            let hole_u8 = self.hole.as_ptr().cast::<u8>();\n\n            assert!(\n                node_u8.wrapping_add(node_size) <= hole_u8,\n                \"Freed node aliases existing hole! Bad free?\",\n            );\n            debug_assert_eq!(self.previous().size, 0);\n\n            let Cursor {\n                mut prev,\n                hole,\n                top,\n            } = self;\n            unsafe {\n                let mut node = check_merge_bottom(node, bottom);\n                prev.as_mut().next = Some(node);\n                node.as_mut().next = Some(hole);\n            }\n            Ok(Cursor {\n                prev,\n                hole: node,\n                top,\n            })\n        } else {\n            Err(self)\n        }\n    }\n\n    fn try_insert_after(&mut self, mut node: NonNull<Hole>) -> Result<(), ()> {\n        let node_u8 = node.as_ptr().cast::<u8>();\n        let node_size = unsafe { node.as_ref().size };\n\n        // If we have a next, does the node overlap next?\n        if let Some(next) = self.current().next.as_ref() {\n            if node < *next {\n                let node_u8 = node_u8 as *const u8;\n                assert!(\n                    node_u8.wrapping_add(node_size) <= next.as_ptr().cast::<u8>(),\n                    \"Freed node aliases existing hole! Bad free?\",\n                );\n            } else {\n                // The new hole isn't between current and next.\n                return Err(());\n            }\n        }\n\n        // At this point, we either have no \"next\" pointer, or the hole is\n        // between current and \"next\". The following assert can only trigger\n        // if we've gotten our list out of order.\n        debug_assert!(self.hole < node, \"Hole list out of order?\");\n\n        let hole_u8 = self.hole.as_ptr().cast::<u8>();\n        let hole_size = self.current().size;\n\n        // Does hole overlap node?\n        assert!(\n            hole_u8.wrapping_add(hole_size) <= node_u8,\n            \"Freed node ({:?}) aliases existing hole ({:?}[{}])! Bad free?\",\n            node_u8,\n            hole_u8,\n            hole_size,\n        );\n\n        // All good! Let's insert that after.\n        unsafe {\n            let maybe_next = self.hole.as_mut().next.replace(node);\n            node.as_mut().next = maybe_next;\n        }\n\n        Ok(())\n    }\n\n    // Merge the current node with up to n following nodes\n    fn try_merge_next_n(self, max: usize) {\n        let Cursor {\n            prev: _,\n            mut hole,\n            top,\n            ..\n        } = self;\n\n        for _ in 0..max {\n            // Is there a next node?\n            let mut next = if let Some(next) = unsafe { hole.as_mut() }.next.as_ref() {\n                *next\n            } else {\n                // Since there is no NEXT node, we need to check whether the current\n                // hole SHOULD extend to the end, but doesn't. This would happen when\n                // there isn't enough remaining space to place a hole after the current\n                // node's placement.\n                check_merge_top(hole, top);\n                return;\n            };\n\n            // Can we directly merge these? e.g. are they touching?\n            //\n            // NOTE: Because we always use `HoleList::align_layout`, the size of\n            // the new hole is always \"rounded up\" to cover any partial gaps that\n            // would have occurred. For this reason, we DON'T need to \"round up\"\n            // to account for an unaligned hole spot.\n            let hole_u8 = hole.as_ptr().cast::<u8>();\n            let hole_sz = unsafe { hole.as_ref().size };\n            let next_u8 = next.as_ptr().cast::<u8>();\n            let end = hole_u8.wrapping_add(hole_sz);\n\n            let touching = end == next_u8;\n\n            if touching {\n                let next_sz;\n                let next_next;\n                unsafe {\n                    let next_mut = next.as_mut();\n                    next_sz = next_mut.size;\n                    next_next = next_mut.next.take();\n                }\n                unsafe {\n                    let hole_mut = hole.as_mut();\n                    hole_mut.next = next_next;\n                    hole_mut.size += next_sz;\n                }\n                // Okay, we just merged the next item. DON'T move the cursor, as we can\n                // just try to merge the next_next, which is now our next.\n            } else {\n                // Welp, not touching, can't merge. Move to the next node.\n                hole = next;\n            }\n        }\n    }\n}\n\n/// Frees the allocation given by `(addr, size)`. It starts at the given hole and walks the list to\n/// find the correct place (the list is sorted by address).\nfn deallocate(list: &mut HoleList, addr: *mut u8, size: usize) {\n    // Start off by just making this allocation a hole where it stands.\n    // We'll attempt to merge it with other nodes once we figure out where\n    // it should live\n    let hole = unsafe { make_hole(addr, size) };\n\n    // Now, try to get a cursor to the list - this only works if we have at least\n    // one non-\"dummy\" hole in the list\n    let cursor = if let Some(cursor) = list.cursor() {\n        cursor\n    } else {\n        // Oh hey, there are no \"real\" holes at all. That means this just\n        // becomes the only \"real\" hole! Check if this is touching the end\n        // or the beginning of the allocation range\n        let hole = check_merge_bottom(hole, list.bottom);\n        check_merge_top(hole, list.top);\n        list.first.next = Some(hole);\n        return;\n    };\n\n    // First, check if we can just insert this node at the top of the list. If the\n    // insertion succeeded, then our cursor now points to the NEW node, behind the\n    // previous location the cursor was pointing to.\n    //\n    // Otherwise, our cursor will point at the current non-\"dummy\" head of the list\n    let (cursor, n) = match cursor.try_insert_back(hole, list.bottom) {\n        Ok(cursor) => {\n            // Yup! It lives at the front of the list. Hooray! Attempt to merge\n            // it with just ONE next node, since it is at the front of the list\n            (cursor, 1)\n        }\n        Err(mut cursor) => {\n            // Nope. It lives somewhere else. Advance the list until we find its home\n            while let Err(()) = cursor.try_insert_after(hole) {\n                cursor = cursor\n                    .next()\n                    .expect(\"Reached end of holes without finding deallocation hole!\");\n            }\n            // Great! We found a home for it, our cursor is now JUST BEFORE the new\n            // node we inserted, so we need to try to merge up to twice: One to combine\n            // the current node to the new node, then once more to combine the new node\n            // with the node after that.\n            (cursor, 2)\n        }\n    };\n\n    // We now need to merge up to two times to combine the current node with the next\n    // two nodes.\n    cursor.try_merge_next_n(n);\n}\n\n#[cfg(test)]\npub mod test {\n    use super::HoleList;\n    use crate::{align_down_size, Heap};\n    use core::mem::size_of;\n    use std::{alloc::Layout, convert::TryInto, mem::MaybeUninit, prelude::v1::*, ptr::NonNull};\n\n    #[repr(align(128))]\n    struct Chonk<const N: usize> {\n        data: [MaybeUninit<u8>; N],\n    }\n\n    impl<const N: usize> Chonk<N> {\n        pub fn new() -> Self {\n            Self {\n                data: [MaybeUninit::uninit(); N],\n            }\n        }\n    }\n\n    fn new_heap() -> Heap {\n        const HEAP_SIZE: usize = 1000;\n        let heap_space = Box::leak(Box::new(Chonk::<HEAP_SIZE>::new()));\n        let data = &mut heap_space.data;\n        let assumed_location = data.as_mut_ptr().cast();\n\n        let heap = Heap::from_slice(data);\n        assert_eq!(heap.bottom(), assumed_location);\n        assert_eq!(heap.size(), align_down_size(HEAP_SIZE, size_of::<usize>()));\n        heap\n    }\n\n    #[test]\n    fn cursor() {\n        let mut heap = new_heap();\n        let curs = heap.holes.cursor().unwrap();\n        // This is the \"dummy\" node\n        assert_eq!(curs.previous().size, 0);\n        // This is the \"full\" heap\n        assert_eq!(\n            curs.current().size,\n            align_down_size(1000, size_of::<usize>())\n        );\n        // There is no other hole\n        assert!(curs.next().is_none());\n    }\n\n    #[test]\n    fn aff() {\n        let mut heap = new_heap();\n        let reqd = Layout::from_size_align(256, 1).unwrap();\n        let _ = heap.allocate_first_fit(reqd).unwrap();\n    }\n\n    /// Tests `HoleList::new` with the minimal allowed `hole_size`.\n    #[test]\n    fn hole_list_new_min_size() {\n        // define an array of `u64` instead of `u8` for alignment\n        static mut HEAP: [u64; 2] = [0; 2];\n        let heap =\n            unsafe { HoleList::new(HEAP.as_mut_ptr().cast(), 2 * core::mem::size_of::<usize>()) };\n        assert_eq!(heap.bottom.cast(), unsafe { HEAP.as_mut_ptr() });\n        assert_eq!(heap.top.cast(), unsafe { HEAP.as_mut_ptr().add(2) });\n        assert_eq!(heap.first.size, 0); // dummy\n        assert_eq!(\n            heap.first.next,\n            Some(NonNull::new(heap.bottom.cast())).unwrap()\n        );\n        assert_eq!(\n            unsafe { &*(heap.first.next.unwrap().as_ptr()) }.size,\n            2 * core::mem::size_of::<usize>()\n        );\n        assert_eq!(unsafe { &*(heap.first.next.unwrap().as_ptr()) }.next, None);\n    }\n\n    /// Tests that `HoleList::new` aligns the `hole_addr` correctly and adjusts the size\n    /// accordingly.\n    #[test]\n    fn hole_list_new_align() {\n        // define an array of `u64` instead of `u8` for alignment\n        static mut HEAP: [u64; 3] = [0; 3];\n\n        let heap_start: *mut u8 = unsafe { HEAP.as_mut_ptr().add(1) }.cast();\n        // initialize the HoleList with a hole_addr one byte before `heap_start`\n        // -> the function should align it up to `heap_start`\n        let heap =\n            unsafe { HoleList::new(heap_start.sub(1), 2 * core::mem::size_of::<usize>() + 1) };\n        assert_eq!(heap.bottom, heap_start);\n        assert_eq!(heap.top.cast(), unsafe {\n            // one byte less than the `hole_size` given to `new` because of alignment\n            heap_start.add(2 * core::mem::size_of::<usize>())\n        });\n\n        assert_eq!(heap.first.size, 0); // dummy\n        assert_eq!(\n            heap.first.next,\n            Some(NonNull::new(heap.bottom.cast())).unwrap()\n        );\n        assert_eq!(\n            unsafe { &*(heap.first.next.unwrap().as_ptr()) }.size,\n            unsafe { heap.top.offset_from(heap.bottom) }\n                .try_into()\n                .unwrap()\n        );\n        assert_eq!(unsafe { &*(heap.first.next.unwrap().as_ptr()) }.next, None);\n    }\n\n    #[test]\n    #[should_panic]\n    fn hole_list_new_too_small() {\n        // define an array of `u64` instead of `u8` for alignment\n        static mut HEAP: [u64; 3] = [0; 3];\n\n        let heap_start: *mut u8 = unsafe { HEAP.as_mut_ptr().add(1) }.cast();\n        // initialize the HoleList with a hole_addr one byte before `heap_start`\n        // -> the function should align it up to `heap_start`, but then the\n        // available size is too small to store a hole -> it should panic\n        unsafe { HoleList::new(heap_start.sub(1), 2 * core::mem::size_of::<usize>()) };\n    }\n\n    #[test]\n    #[should_panic]\n    fn extend_empty() {\n        unsafe { HoleList::empty().extend(16) };\n    }\n}\n", "#![cfg_attr(feature = \"const_mut_refs\", feature(const_mut_refs))]\n#![cfg_attr(\n    feature = \"alloc_ref\",\n    feature(allocator_api, alloc_layout_extra, nonnull_slice_from_raw_parts)\n)]\n#![no_std]\n\n#[cfg(test)]\n#[macro_use]\nextern crate std;\n\n#[cfg(feature = \"use_spin\")]\nextern crate spinning_top;\n\n#[cfg(feature = \"use_spin\")]\nuse core::alloc::GlobalAlloc;\nuse core::alloc::Layout;\n#[cfg(feature = \"alloc_ref\")]\nuse core::alloc::{AllocError, Allocator};\nuse core::mem::MaybeUninit;\n#[cfg(feature = \"use_spin\")]\nuse core::ops::Deref;\nuse core::ptr::NonNull;\n#[cfg(test)]\nuse hole::Hole;\nuse hole::HoleList;\n#[cfg(feature = \"use_spin\")]\nuse spinning_top::Spinlock;\n\npub mod hole;\n#[cfg(test)]\nmod test;\n\n/// A fixed size heap backed by a linked list of free memory blocks.\npub struct Heap {\n    used: usize,\n    holes: HoleList,\n}\n\nunsafe impl Send for Heap {}\n\nimpl Heap {\n    /// Creates an empty heap. All allocate calls will return `None`.\n    #[cfg(not(feature = \"const_mut_refs\"))]\n    pub fn empty() -> Heap {\n        Heap {\n            used: 0,\n            holes: HoleList::empty(),\n        }\n    }\n\n    #[cfg(feature = \"const_mut_refs\")]\n    pub const fn empty() -> Heap {\n        Heap {\n            used: 0,\n            holes: HoleList::empty(),\n        }\n    }\n\n    /// Initializes an empty heap\n    ///\n    /// The `heap_bottom` pointer is automatically aligned, so the [`bottom()`][Self::bottom]\n    /// method might return a pointer that is larger than `heap_bottom` after construction.\n    ///\n    /// The given `heap_size` must be large enough to store the required\n    /// metadata, otherwise this function will panic. Depending on the\n    /// alignment of the `hole_addr` pointer, the minimum size is between\n    /// `2 * size_of::<usize>` and `3 * size_of::<usize>`.\n    ///\n    /// The usable size for allocations will be truncated to the nearest\n    /// alignment of `align_of::<usize>`. Any extra bytes left at the end\n    /// will be reclaimed once sufficient additional space is given to\n    /// [`extend`][Heap::extend].\n    ///\n    /// # Safety\n    ///\n    /// This function must be called at most once and must only be used on an\n    /// empty heap.\n    ///\n    /// The bottom address must be valid and the memory in the\n    /// `[heap_bottom, heap_bottom + heap_size)` range must not be used for anything else.\n    /// This function is unsafe because it can cause undefined behavior if the given address\n    /// is invalid.\n    ///\n    /// The provided memory range must be valid for the `'static` lifetime.\n    pub unsafe fn init(&mut self, heap_bottom: *mut u8, heap_size: usize) {\n        self.used = 0;\n        self.holes = HoleList::new(heap_bottom, heap_size);\n    }\n\n    /// Initialize an empty heap with provided memory.\n    ///\n    /// The caller is responsible for procuring a region of raw memory that may be utilized by the\n    /// allocator. This might be done via any method such as (unsafely) taking a region from the\n    /// program's memory, from a mutable static, or by allocating and leaking such memory from\n    /// another allocator.\n    ///\n    /// The latter approach may be especially useful if the underlying allocator does not perform\n    /// deallocation (e.g. a simple bump allocator). Then the overlaid linked-list-allocator can\n    /// provide memory reclamation.\n    ///\n    /// The usable size for allocations will be truncated to the nearest\n    /// alignment of `align_of::<usize>`. Any extra bytes left at the end\n    /// will be reclaimed once sufficient additional space is given to\n    /// [`extend`][Heap::extend].\n    ///\n    /// # Panics\n    ///\n    /// This method panics if the heap is already initialized.\n    ///\n    /// It also panics when the length of the given `mem` slice is not large enough to\n    /// store the required metadata. Depending on the alignment of the slice, the minimum\n    /// size is between `2 * size_of::<usize>` and `3 * size_of::<usize>`.\n    pub fn init_from_slice(&mut self, mem: &'static mut [MaybeUninit<u8>]) {\n        assert!(\n            self.bottom().is_null(),\n            \"The heap has already been initialized.\"\n        );\n        let size = mem.len();\n        let address = mem.as_mut_ptr().cast();\n        // SAFETY: All initialization requires the bottom address to be valid, which implies it\n        // must not be 0. Initially the address is 0. The assertion above ensures that no\n        // initialization had been called before.\n        // The given address and size is valid according to the safety invariants of the mutable\n        // reference handed to us by the caller.\n        unsafe { self.init(address, size) }\n    }\n\n    /// Creates a new heap with the given `bottom` and `size`.\n    ///\n    /// The `heap_bottom` pointer is automatically aligned, so the [`bottom()`][Self::bottom]\n    /// method might return a pointer that is larger than `heap_bottom` after construction.\n    ///\n    /// The given `heap_size` must be large enough to store the required\n    /// metadata, otherwise this function will panic. Depending on the\n    /// alignment of the `hole_addr` pointer, the minimum size is between\n    /// `2 * size_of::<usize>` and `3 * size_of::<usize>`.\n    ///\n    /// The usable size for allocations will be truncated to the nearest\n    /// alignment of `align_of::<usize>`. Any extra bytes left at the end\n    /// will be reclaimed once sufficient additional space is given to\n    /// [`extend`][Heap::extend].\n    ///\n    /// # Safety\n    ///\n    /// The bottom address must be valid and the memory in the\n    /// `[heap_bottom, heap_bottom + heap_size)` range must not be used for anything else.\n    /// This function is unsafe because it can cause undefined behavior if the given address\n    /// is invalid.\n    ///\n    /// The provided memory range must be valid for the `'static` lifetime.\n    pub unsafe fn new(heap_bottom: *mut u8, heap_size: usize) -> Heap {\n        Heap {\n            used: 0,\n            holes: HoleList::new(heap_bottom, heap_size),\n        }\n    }\n\n    /// Creates a new heap from a slice of raw memory.\n    ///\n    /// This is a convenience function that has the same effect as calling\n    /// [`init_from_slice`] on an empty heap. All the requirements of `init_from_slice`\n    /// apply to this function as well.\n    pub fn from_slice(mem: &'static mut [MaybeUninit<u8>]) -> Heap {\n        let size = mem.len();\n        let address = mem.as_mut_ptr().cast();\n        // SAFETY: The given address and size is valid according to the safety invariants of the\n        // mutable reference handed to us by the caller.\n        unsafe { Self::new(address, size) }\n    }\n\n    /// Allocates a chunk of the given size with the given alignment. Returns a pointer to the\n    /// beginning of that chunk if it was successful. Else it returns `None`.\n    /// This function scans the list of free memory blocks and uses the first block that is big\n    /// enough. The runtime is in O(n) where n is the number of free blocks, but it should be\n    /// reasonably fast for small allocations.\n    //\n    // NOTE: We could probably replace this with an `Option` instead of a `Result` in a later\n    // release to remove this clippy warning\n    #[allow(clippy::result_unit_err)]\n    pub fn allocate_first_fit(&mut self, layout: Layout) -> Result<NonNull<u8>, ()> {\n        match self.holes.allocate_first_fit(layout) {\n            Ok((ptr, aligned_layout)) => {\n                self.used += aligned_layout.size();\n                Ok(ptr)\n            }\n            Err(err) => Err(err),\n        }\n    }\n\n    /// Frees the given allocation. `ptr` must be a pointer returned\n    /// by a call to the `allocate_first_fit` function with identical size and alignment.\n    ///\n    /// This function walks the list of free memory blocks and inserts the freed block at the\n    /// correct place. If the freed block is adjacent to another free block, the blocks are merged\n    /// again. This operation is in `O(n)` since the list needs to be sorted by address.\n    ///\n    /// # Safety\n    ///\n    /// `ptr` must be a pointer returned by a call to the [`allocate_first_fit`] function with\n    /// identical layout. Undefined behavior may occur for invalid arguments.\n    pub unsafe fn deallocate(&mut self, ptr: NonNull<u8>, layout: Layout) {\n        self.used -= self.holes.deallocate(ptr, layout).size();\n    }\n\n    /// Returns the bottom address of the heap.\n    ///\n    /// The bottom pointer is automatically aligned, so the returned pointer\n    /// might be larger than the bottom pointer used for initialization.\n    pub fn bottom(&self) -> *mut u8 {\n        self.holes.bottom\n    }\n\n    /// Returns the size of the heap.\n    ///\n    /// This is the size the heap is using for allocations, not necessarily the\n    /// total amount of bytes given to the heap. To determine the exact memory\n    /// boundaries, use [`bottom`][Self::bottom] and [`top`][Self::top].\n    pub fn size(&self) -> usize {\n        unsafe { self.holes.top.offset_from(self.holes.bottom) as usize }\n    }\n\n    /// Return the top address of the heap.\n    ///\n    /// Note: The heap may choose to not use bytes at the end for allocations\n    /// until there is enough room for metadata, but it still retains ownership\n    /// over memory from [`bottom`][Self::bottom] to the address returned.\n    pub fn top(&self) -> *mut u8 {\n        unsafe { self.holes.top.add(self.holes.pending_extend as usize) }\n    }\n\n    /// Returns the size of the used part of the heap\n    pub fn used(&self) -> usize {\n        self.used\n    }\n\n    /// Returns the size of the free part of the heap\n    pub fn free(&self) -> usize {\n        self.size() - self.used\n    }\n\n    /// Extends the size of the heap by creating a new hole at the end.\n    ///\n    /// Small extensions are not guaranteed to grow the usable size of\n    /// the heap. In order to grow the Heap most effectively, extend by\n    /// at least `2 * size_of::<usize>`, keeping the amount a multiple of\n    /// `size_of::<usize>`.\n    ///\n    /// Calling this method on an uninitialized Heap will panic.\n    ///\n    /// # Safety\n    ///\n    /// The amount of data given in `by` MUST exist directly after the original\n    /// range of data provided when constructing the [Heap]. The additional data\n    /// must have the same lifetime of the original range of data.\n    ///\n    /// Even if this operation doesn't increase the [usable size][`Self::size`]\n    /// by exactly `by` bytes, those bytes are still owned by the Heap for\n    /// later use.\n    pub unsafe fn extend(&mut self, by: usize) {\n        self.holes.extend(by);\n    }\n}\n\n#[cfg(all(feature = \"alloc_ref\", feature = \"use_spin\"))]\nunsafe impl Allocator for LockedHeap {\n    fn allocate(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError> {\n        if layout.size() == 0 {\n            return Ok(NonNull::slice_from_raw_parts(layout.dangling(), 0));\n        }\n        match self.0.lock().allocate_first_fit(layout) {\n            Ok(ptr) => Ok(NonNull::slice_from_raw_parts(ptr, layout.size())),\n            Err(()) => Err(AllocError),\n        }\n    }\n\n    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout) {\n        if layout.size() != 0 {\n            self.0.lock().deallocate(ptr, layout);\n        }\n    }\n}\n\n#[cfg(feature = \"use_spin\")]\npub struct LockedHeap(Spinlock<Heap>);\n\n#[cfg(feature = \"use_spin\")]\nimpl LockedHeap {\n    /// Creates an empty heap. All allocate calls will return `None`.\n    #[cfg(feature = \"use_spin_nightly\")]\n    pub const fn empty() -> LockedHeap {\n        LockedHeap(Spinlock::new(Heap::empty()))\n    }\n\n    /// Creates an empty heap. All allocate calls will return `None`.\n    #[cfg(not(feature = \"use_spin_nightly\"))]\n    pub fn empty() -> LockedHeap {\n        LockedHeap(Spinlock::new(Heap::empty()))\n    }\n\n    /// Creates a new heap with the given `bottom` and `size`.\n    ///\n    /// The `heap_bottom` pointer is automatically aligned, so the [`bottom()`][Heap::bottom]\n    /// method might return a pointer that is larger than `heap_bottom` after construction.\n    ///\n    /// The given `heap_size` must be large enough to store the required\n    /// metadata, otherwise this function will panic. Depending on the\n    /// alignment of the `hole_addr` pointer, the minimum size is between\n    /// `2 * size_of::<usize>` and `3 * size_of::<usize>`.\n    ///\n    /// # Safety\n    ///\n    /// The bottom address must be valid and the memory in the\n    /// `[heap_bottom, heap_bottom + heap_size)` range must not be used for anything else.\n    /// This function is unsafe because it can cause undefined behavior if the given address\n    /// is invalid.\n    ///\n    /// The provided memory range must be valid for the `'static` lifetime.\n    pub unsafe fn new(heap_bottom: *mut u8, heap_size: usize) -> LockedHeap {\n        LockedHeap(Spinlock::new(Heap {\n            used: 0,\n            holes: HoleList::new(heap_bottom, heap_size),\n        }))\n    }\n}\n\n#[cfg(feature = \"use_spin\")]\nimpl Deref for LockedHeap {\n    type Target = Spinlock<Heap>;\n\n    fn deref(&self) -> &Spinlock<Heap> {\n        &self.0\n    }\n}\n\n#[cfg(feature = \"use_spin\")]\nunsafe impl GlobalAlloc for LockedHeap {\n    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {\n        self.0\n            .lock()\n            .allocate_first_fit(layout)\n            .ok()\n            .map_or(core::ptr::null_mut(), |allocation| allocation.as_ptr())\n    }\n\n    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {\n        self.0\n            .lock()\n            .deallocate(NonNull::new_unchecked(ptr), layout)\n    }\n}\n\n/// Align downwards. Returns the greatest x with alignment `align`\n/// so that x <= addr. The alignment must be a power of 2.\npub fn align_down_size(size: usize, align: usize) -> usize {\n    if align.is_power_of_two() {\n        size & !(align - 1)\n    } else if align == 0 {\n        size\n    } else {\n        panic!(\"`align` must be a power of 2\");\n    }\n}\n\npub fn align_up_size(size: usize, align: usize) -> usize {\n    align_down_size(size + align - 1, align)\n}\n\n/// Align upwards. Returns the smallest x with alignment `align`\n/// so that x >= addr. The alignment must be a power of 2.\npub fn align_up(addr: *mut u8, align: usize) -> *mut u8 {\n    let offset = addr.align_offset(align);\n    addr.wrapping_add(offset)\n}\n", "use super::*;\nuse core::alloc::Layout;\nuse std::mem::{align_of, size_of, MaybeUninit};\nuse std::prelude::v1::*;\n\n#[repr(align(128))]\nstruct Chonk<const N: usize> {\n    data: [MaybeUninit<u8>; N],\n}\n\nimpl<const N: usize> Chonk<N> {\n    pub fn new() -> Self {\n        Self {\n            data: [MaybeUninit::uninit(); N],\n        }\n    }\n}\n\nfn new_heap() -> Heap {\n    const HEAP_SIZE: usize = 1000;\n    let heap_space = Box::leak(Box::new(Chonk::<HEAP_SIZE>::new()));\n    let data = &mut heap_space.data;\n    let assumed_location = data.as_mut_ptr().cast();\n\n    let heap = Heap::from_slice(data);\n    assert_eq!(heap.bottom(), assumed_location);\n    assert_eq!(heap.size(), align_down_size(HEAP_SIZE, size_of::<usize>()));\n    heap\n}\n\nfn new_max_heap() -> Heap {\n    const HEAP_SIZE: usize = 1024;\n    const HEAP_SIZE_MAX: usize = 2048;\n    let heap_space = Box::leak(Box::new(Chonk::<HEAP_SIZE_MAX>::new()));\n    let data = &mut heap_space.data;\n    let start_ptr = data.as_mut_ptr().cast();\n\n    // Unsafe so that we have provenance over the whole allocation.\n    let heap = unsafe { Heap::new(start_ptr, HEAP_SIZE) };\n    assert_eq!(heap.bottom(), start_ptr);\n    assert_eq!(heap.size(), HEAP_SIZE);\n    heap\n}\n\n#[test]\nfn empty() {\n    let mut heap = Heap::empty();\n    let layout = Layout::from_size_align(1, 1).unwrap();\n    assert!(heap.allocate_first_fit(layout.clone()).is_err());\n}\n\n#[test]\nfn oom() {\n    let mut heap = new_heap();\n    let layout = Layout::from_size_align(heap.size() + 1, align_of::<usize>());\n    let addr = heap.allocate_first_fit(layout.unwrap());\n    assert!(addr.is_err());\n}\n\n#[test]\nfn allocate_double_usize() {\n    let mut heap = new_heap();\n    let size = size_of::<usize>() * 2;\n    let layout = Layout::from_size_align(size, align_of::<usize>());\n    let addr = heap.allocate_first_fit(layout.unwrap());\n    assert!(addr.is_ok());\n    let addr = addr.unwrap().as_ptr();\n    assert!(addr == heap.bottom());\n    let (hole_addr, hole_size) = heap.holes.first_hole().expect(\"ERROR: no hole left\");\n    assert!(hole_addr == heap.bottom().wrapping_add(size));\n    assert!(hole_size == heap.size() - size);\n\n    unsafe {\n        assert_eq!(\n            (*((addr.wrapping_add(size)) as *const Hole)).size,\n            heap.size() - size\n        );\n    }\n}\n\n#[test]\nfn allocate_and_free_double_usize() {\n    let mut heap = new_heap();\n\n    let layout = Layout::from_size_align(size_of::<usize>() * 2, align_of::<usize>()).unwrap();\n    let x = heap.allocate_first_fit(layout.clone()).unwrap();\n    unsafe {\n        *(x.as_ptr() as *mut (usize, usize)) = (0xdeafdeadbeafbabe, 0xdeafdeadbeafbabe);\n\n        heap.deallocate(x, layout.clone());\n        let real_first = heap.holes.first.next.as_ref().unwrap().as_ref();\n\n        assert_eq!(real_first.size, heap.size());\n        assert!(real_first.next.is_none());\n    }\n}\n\n#[test]\nfn deallocate_right_before() {\n    let mut heap = new_heap();\n    let layout = Layout::from_size_align(size_of::<usize>() * 5, 1).unwrap();\n\n    let x = heap.allocate_first_fit(layout.clone()).unwrap();\n    let y = heap.allocate_first_fit(layout.clone()).unwrap();\n    let z = heap.allocate_first_fit(layout.clone()).unwrap();\n\n    unsafe {\n        heap.deallocate(y, layout.clone());\n        assert_eq!((*(y.as_ptr() as *const Hole)).size, layout.size());\n        heap.deallocate(x, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, layout.size() * 2);\n        heap.deallocate(z, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, heap.size());\n    }\n}\n\n#[test]\nfn deallocate_right_behind() {\n    let mut heap = new_heap();\n    let size = size_of::<usize>() * 5;\n    let layout = Layout::from_size_align(size, 1).unwrap();\n\n    let x = heap.allocate_first_fit(layout.clone()).unwrap();\n    let y = heap.allocate_first_fit(layout.clone()).unwrap();\n    let z = heap.allocate_first_fit(layout.clone()).unwrap();\n\n    unsafe {\n        heap.deallocate(x, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, size);\n        heap.deallocate(y, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, size * 2);\n        heap.deallocate(z, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, heap.size());\n    }\n}\n\n#[test]\nfn deallocate_middle() {\n    let mut heap = new_heap();\n    let size = size_of::<usize>() * 5;\n    let layout = Layout::from_size_align(size, 1).unwrap();\n\n    let x = heap.allocate_first_fit(layout.clone()).unwrap();\n    let y = heap.allocate_first_fit(layout.clone()).unwrap();\n    let z = heap.allocate_first_fit(layout.clone()).unwrap();\n    let a = heap.allocate_first_fit(layout.clone()).unwrap();\n\n    unsafe {\n        heap.deallocate(x, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, size);\n        heap.deallocate(z, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, size);\n        assert_eq!((*(z.as_ptr() as *const Hole)).size, size);\n        heap.deallocate(y, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, size * 3);\n        heap.deallocate(a, layout.clone());\n        assert_eq!((*(x.as_ptr() as *const Hole)).size, heap.size());\n    }\n}\n\n#[test]\nfn reallocate_double_usize() {\n    let mut heap = new_heap();\n\n    let layout = Layout::from_size_align(size_of::<usize>() * 2, align_of::<usize>()).unwrap();\n\n    let x = heap.allocate_first_fit(layout.clone()).unwrap();\n    unsafe {\n        heap.deallocate(x, layout.clone());\n    }\n\n    let y = heap.allocate_first_fit(layout.clone()).unwrap();\n    unsafe {\n        heap.deallocate(y, layout.clone());\n    }\n\n    assert_eq!(x, y);\n}\n\n#[test]\nfn allocate_many_size_aligns() {\n    use core::ops::{Range, RangeInclusive};\n\n    #[cfg(not(miri))]\n    const SIZE: RangeInclusive<usize> = 1..=512;\n\n    #[cfg(miri)]\n    const SIZE: RangeInclusive<usize> = 256..=(256 + core::mem::size_of::<crate::hole::Hole>());\n\n    #[cfg(not(miri))]\n    const ALIGN: Range<usize> = 0..10;\n\n    #[cfg(miri)]\n    const ALIGN: Range<usize> = 1..4;\n\n    const STRATS: Range<usize> = 0..4;\n\n    let mut heap = new_heap();\n    let aligned_heap_size = align_down_size(1000, size_of::<usize>());\n    assert_eq!(heap.size(), aligned_heap_size);\n\n    heap.holes.debug();\n\n    let max_alloc = Layout::from_size_align(aligned_heap_size, 1).unwrap();\n    let full = heap.allocate_first_fit(max_alloc).unwrap();\n    unsafe {\n        heap.deallocate(full, max_alloc);\n    }\n\n    heap.holes.debug();\n\n    struct Alloc {\n        alloc: NonNull<u8>,\n        layout: Layout,\n    }\n\n    // NOTE: Printing to the console SIGNIFICANTLY slows down miri.\n\n    for strat in STRATS {\n        for align in ALIGN {\n            for size in SIZE {\n                #[cfg(not(miri))]\n                {\n                    println!(\"=========================================================\");\n                    println!(\"Align: {}\", 1 << align);\n                    println!(\"Size:  {}\", size);\n                    println!(\"Free Pattern: {}/0..4\", strat);\n                    println!();\n                }\n                let mut allocs = vec![];\n\n                let layout = Layout::from_size_align(size, 1 << align).unwrap();\n                while let Ok(alloc) = heap.allocate_first_fit(layout) {\n                    #[cfg(not(miri))]\n                    heap.holes.debug();\n                    allocs.push(Alloc { alloc, layout });\n                }\n\n                #[cfg(not(miri))]\n                println!(\"Allocs: {} - {} bytes\", allocs.len(), allocs.len() * size);\n\n                match strat {\n                    0 => {\n                        // Forward\n                        allocs.drain(..).for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                    }\n                    1 => {\n                        // Backwards\n                        allocs.drain(..).rev().for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                    }\n                    2 => {\n                        // Interleaved forwards\n                        let mut a = Vec::new();\n                        let mut b = Vec::new();\n                        for (i, alloc) in allocs.drain(..).enumerate() {\n                            if (i % 2) == 0 {\n                                a.push(alloc);\n                            } else {\n                                b.push(alloc);\n                            }\n                        }\n                        a.drain(..).for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                        b.drain(..).for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                    }\n                    3 => {\n                        // Interleaved backwards\n                        let mut a = Vec::new();\n                        let mut b = Vec::new();\n                        for (i, alloc) in allocs.drain(..).rev().enumerate() {\n                            if (i % 2) == 0 {\n                                a.push(alloc);\n                            } else {\n                                b.push(alloc);\n                            }\n                        }\n                        a.drain(..).for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                        b.drain(..).for_each(|a| unsafe {\n                            heap.deallocate(a.alloc, a.layout);\n                            #[cfg(not(miri))]\n                            heap.holes.debug();\n                        });\n                    }\n                    _ => panic!(),\n                }\n\n                #[cfg(not(miri))]\n                println!(\"MAX CHECK\");\n\n                let full = heap.allocate_first_fit(max_alloc).unwrap();\n                unsafe {\n                    heap.deallocate(full, max_alloc);\n                }\n\n                #[cfg(not(miri))]\n                println!();\n            }\n        }\n    }\n}\n\n#[test]\nfn allocate_multiple_sizes() {\n    let mut heap = new_heap();\n    let base_size = size_of::<usize>();\n    let base_align = align_of::<usize>();\n\n    let layout_1 = Layout::from_size_align(base_size * 2, base_align).unwrap();\n    let layout_2 = Layout::from_size_align(base_size * 7, base_align).unwrap();\n    let layout_3 = Layout::from_size_align(base_size * 3, base_align * 4).unwrap();\n    let layout_4 = Layout::from_size_align(base_size * 4, base_align).unwrap();\n\n    let x = heap.allocate_first_fit(layout_1.clone()).unwrap();\n    let y = heap.allocate_first_fit(layout_2.clone()).unwrap();\n    assert_eq!(y.as_ptr() as usize, x.as_ptr() as usize + base_size * 2);\n    let z = heap.allocate_first_fit(layout_3.clone()).unwrap();\n    assert_eq!(z.as_ptr() as usize % (base_size * 4), 0);\n\n    unsafe {\n        heap.deallocate(x, layout_1.clone());\n    }\n\n    let a = heap.allocate_first_fit(layout_4.clone()).unwrap();\n    let b = heap.allocate_first_fit(layout_1.clone()).unwrap();\n    assert_eq!(b, x);\n\n    unsafe {\n        heap.deallocate(y, layout_2);\n        heap.deallocate(z, layout_3);\n        heap.deallocate(a, layout_4);\n        heap.deallocate(b, layout_1);\n    }\n}\n\n// This test makes sure that the heap works correctly when the input slice has\n// a variety of non-Hole aligned starting addresses\n#[test]\nfn allocate_multiple_unaligned() {\n    for offset in 0..=Layout::new::<Hole>().size() {\n        let mut heap = new_heap_skip(offset);\n        let base_size = size_of::<usize>();\n        let base_align = align_of::<usize>();\n\n        let layout_1 = Layout::from_size_align(base_size * 2, base_align).unwrap();\n        let layout_2 = Layout::from_size_align(base_size * 7, base_align).unwrap();\n        let layout_3 = Layout::from_size_align(base_size * 3, base_align * 4).unwrap();\n        let layout_4 = Layout::from_size_align(base_size * 4, base_align).unwrap();\n\n        let x = heap.allocate_first_fit(layout_1.clone()).unwrap();\n        let y = heap.allocate_first_fit(layout_2.clone()).unwrap();\n        assert_eq!(y.as_ptr() as usize, x.as_ptr() as usize + base_size * 2);\n        let z = heap.allocate_first_fit(layout_3.clone()).unwrap();\n        assert_eq!(z.as_ptr() as usize % (base_size * 4), 0);\n\n        unsafe {\n            heap.deallocate(x, layout_1.clone());\n        }\n\n        let a = heap.allocate_first_fit(layout_4.clone()).unwrap();\n        let b = heap.allocate_first_fit(layout_1.clone()).unwrap();\n        assert_eq!(b, x);\n\n        unsafe {\n            heap.deallocate(y, layout_2);\n            heap.deallocate(z, layout_3);\n            heap.deallocate(a, layout_4);\n            heap.deallocate(b, layout_1);\n        }\n    }\n}\n\nfn new_heap_skip(ct: usize) -> Heap {\n    const HEAP_SIZE: usize = 1000;\n    let heap_space = Box::leak(Box::new(Chonk::<HEAP_SIZE>::new()));\n    let data = &mut heap_space.data[ct..];\n    let heap = Heap::from_slice(data);\n    heap\n}\n\n#[test]\nfn allocate_usize() {\n    let mut heap = new_heap();\n\n    let layout = Layout::from_size_align(size_of::<usize>(), 1).unwrap();\n\n    assert!(heap.allocate_first_fit(layout.clone()).is_ok());\n}\n\n#[test]\nfn allocate_usize_in_bigger_block() {\n    let mut heap = new_heap();\n\n    let layout_1 = Layout::from_size_align(size_of::<usize>() * 2, 1).unwrap();\n    let layout_2 = Layout::from_size_align(size_of::<usize>(), 1).unwrap();\n\n    let x = heap.allocate_first_fit(layout_1.clone()).unwrap();\n    let y = heap.allocate_first_fit(layout_1.clone()).unwrap();\n    unsafe {\n        heap.deallocate(x, layout_1.clone());\n    }\n\n    let z = heap.allocate_first_fit(layout_2.clone());\n    assert!(z.is_ok());\n    let z = z.unwrap();\n    assert_eq!(x, z);\n\n    unsafe {\n        heap.deallocate(y, layout_1.clone());\n        heap.deallocate(z, layout_2);\n    }\n}\n\n#[test]\n// see https://github.com/phil-opp/blog_os/issues/160\nfn align_from_small_to_big() {\n    let mut heap = new_heap();\n\n    let layout_1 = Layout::from_size_align(28, 4).unwrap();\n    let layout_2 = Layout::from_size_align(8, 8).unwrap();\n\n    // allocate 28 bytes so that the heap end is only 4 byte aligned\n    assert!(heap.allocate_first_fit(layout_1.clone()).is_ok());\n    // try to allocate a 8 byte aligned block\n    assert!(heap.allocate_first_fit(layout_2.clone()).is_ok());\n}\n\n#[test]\nfn extend_empty_heap() {\n    let mut heap = new_max_heap();\n\n    unsafe {\n        heap.extend(1024);\n    }\n\n    // Try to allocate full heap after extend\n    let layout = Layout::from_size_align(2048, 1).unwrap();\n    assert!(heap.allocate_first_fit(layout.clone()).is_ok());\n}\n\n#[test]\nfn extend_full_heap() {\n    let mut heap = new_max_heap();\n\n    let layout = Layout::from_size_align(1024, 1).unwrap();\n\n    // Allocate full heap, extend and allocate again to the max\n    assert!(heap.allocate_first_fit(layout.clone()).is_ok());\n    unsafe {\n        heap.extend(1024);\n    }\n    assert!(heap.allocate_first_fit(layout.clone()).is_ok());\n}\n\n#[test]\nfn extend_fragmented_heap() {\n    let mut heap = new_max_heap();\n\n    let layout_1 = Layout::from_size_align(512, 1).unwrap();\n    let layout_2 = Layout::from_size_align(1024, 1).unwrap();\n\n    let alloc1 = heap.allocate_first_fit(layout_1.clone());\n    let alloc2 = heap.allocate_first_fit(layout_1.clone());\n\n    assert!(alloc1.is_ok());\n    assert!(alloc2.is_ok());\n\n    unsafe {\n        // Create a hole at the beginning of the heap\n        heap.deallocate(alloc1.unwrap(), layout_1.clone());\n    }\n\n    unsafe {\n        heap.extend(1024);\n    }\n\n    // We got additional 1024 bytes hole at the end of the heap\n    // Try to allocate there\n    assert!(heap.allocate_first_fit(layout_2.clone()).is_ok());\n}\n\n/// Ensures that `Heap::extend` fails for very small sizes.\n///\n/// The size needs to be big enough to hold a hole, otherwise\n/// the hole write would result in an out of bounds write.\n#[test]\nfn small_heap_extension() {\n    // define an array of `u64` instead of `u8` for alignment\n    static mut HEAP: [u64; 5] = [0; 5];\n    unsafe {\n        let mut heap = Heap::new(HEAP.as_mut_ptr().cast(), 32);\n        heap.extend(1);\n        assert_eq!(1, heap.holes.pending_extend);\n    }\n}\n\n/// Ensures that `Heap::extend` fails for sizes that are not a multiple of the hole size.\n#[test]\nfn oddly_sized_heap_extension() {\n    // define an array of `u64` instead of `u8` for alignment\n    static mut HEAP: [u64; 5] = [0; 5];\n    unsafe {\n        let mut heap = Heap::new(HEAP.as_mut_ptr().cast(), 16);\n        heap.extend(17);\n        assert_eq!(1, heap.holes.pending_extend);\n        assert_eq!(16 + 16, heap.size());\n    }\n}\n\n/// Ensures that heap extension fails when trying to extend an oddly-sized heap.\n///\n/// To extend the heap, we need to place a hole at the old top of the heap. This\n/// only works if the top pointer is sufficiently aligned.\n#[test]\nfn extend_odd_size() {\n    // define an array of `u64` instead of `u8` for alignment\n    static mut HEAP: [u64; 5] = [0; 5];\n    unsafe {\n        let mut heap = Heap::new(HEAP.as_mut_ptr().cast(), 17);\n        assert_eq!(1, heap.holes.pending_extend);\n        heap.extend(16);\n        assert_eq!(1, heap.holes.pending_extend);\n        heap.extend(15);\n        assert_eq!(0, heap.holes.pending_extend);\n        assert_eq!(17 + 16 + 15, heap.size());\n    }\n}\n"], "filenames": ["src/hole.rs", "src/lib.rs", "src/test.rs"], "buggy_code_start_loc": [7, 61, 26], "buggy_code_end_loc": [682, 251, 497], "fixing_code_start_loc": [7, 62, 26], "fixing_code_end_loc": [822, 310, 546], "type": "CWE-119", "message": "linked_list_allocator is an allocator usable for no_std systems. Prior to version 0.10.2, the heap initialization methods were missing a minimum size check for the given heap size argument. This could lead to out-of-bound writes when a heap was initialized with a size smaller than `3 * size_of::<usize>` because of metadata write operations. This vulnerability impacts all the initialization functions on the `Heap` and `LockedHeap` types, including `Heap::new`, `Heap::init`, `Heap::init_from_slice`, and `LockedHeap::new`. It also affects multiple uses of the `Heap::extend` method. Version 0.10.2 contains a patch for the issue. As a workaround, ensure that the heap is only initialized with a size larger than `3 * size_of::<usize>` and that the `Heap::extend` method is only called with sizes larger than `2 * size_of::<usize>()`. Also, ensure that the total heap size is (and stays) a multiple of `2 * size_of::<usize>()`.", "other": {"cve": {"id": "CVE-2022-36086", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-07T23:15:14.097", "lastModified": "2022-09-12T18:20:52.413", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "linked_list_allocator is an allocator usable for no_std systems. Prior to version 0.10.2, the heap initialization methods were missing a minimum size check for the given heap size argument. This could lead to out-of-bound writes when a heap was initialized with a size smaller than `3 * size_of::<usize>` because of metadata write operations. This vulnerability impacts all the initialization functions on the `Heap` and `LockedHeap` types, including `Heap::new`, `Heap::init`, `Heap::init_from_slice`, and `LockedHeap::new`. It also affects multiple uses of the `Heap::extend` method. Version 0.10.2 contains a patch for the issue. As a workaround, ensure that the heap is only initialized with a size larger than `3 * size_of::<usize>` and that the `Heap::extend` method is only called with sizes larger than `2 * size_of::<usize>()`. Also, ensure that the total heap size is (and stays) a multiple of `2 * size_of::<usize>()`."}, {"lang": "es", "value": "linked_list_allocator es un asignador usable en sistemas no_std. En versiones anteriores a 0.10.2, los m\u00e9todos de inicializaci\u00f3n de la pila carec\u00edan de una comprobaci\u00f3n del tama\u00f1o m\u00ednimo para el argumento del tama\u00f1o de la pila. Esto pod\u00eda conllevar a escrituras fuera de l\u00edmites cuando una pila es inicializada con un tama\u00f1o inferior a \"3 * size_of::(usize)\" debido a las operaciones de escritura de metadatos. Esta vulnerabilidad afecta a todas las funciones de inicializaci\u00f3n de los tipos \"Heap\" y \"LockedHeap\", incluyendo \"Heap::new\", \"Heap::init\", \"Heap::init_from_slice\", y \"LockedHeap::new\". Tambi\u00e9n afecta a m\u00faltiples usos del m\u00e9todo \"Heap::extend\". La versi\u00f3n 0.10.2 contiene un parche para este problema. Como mitigaci\u00f3n, aseg\u00farese de que la pila s\u00f3lo es inicializada con un tama\u00f1o superior a \"3 * size_of::(usize)\" y que el m\u00e9todo \"Heap::extend\" s\u00f3lo es llamado con tama\u00f1os superiores a \"2 * size_of::(usize)()\". Adem\u00e1s, aseg\u00farese de que el tama\u00f1o total de la pila es (y es mantenido) un m\u00faltiplo de \"2 * size_of::(usize)()\""}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.4, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.5, "impactScore": 5.9}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-119"}, {"lang": "en", "value": "CWE-787"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:rust-osdev:linked-list-allocator:*:*:*:*:*:rust:*:*", "versionEndExcluding": "0.10.2", "matchCriteriaId": "D1EB783F-18F3-471C-8006-9949D06163C3"}]}]}], "references": [{"url": "https://github.com/rust-osdev/linked-list-allocator/commit/013b0758643943e8df5b17bbb495460ff47e8bbf", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/rust-osdev/linked-list-allocator/security/advisories/GHSA-xg8p-34w2-j49j", "source": "security-advisories@github.com", "tags": ["Exploit", "Mitigation", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/rust-osdev/linked-list-allocator/commit/013b0758643943e8df5b17bbb495460ff47e8bbf"}}