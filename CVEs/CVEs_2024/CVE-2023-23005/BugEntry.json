{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n#include <linux/slab.h>\n#include <linux/lockdep.h>\n#include <linux/sysfs.h>\n#include <linux/kobject.h>\n#include <linux/memory.h>\n#include <linux/memory-tiers.h>\n\n#include \"internal.h\"\n\nstruct memory_tier {\n\t/* hierarchy of memory tiers */\n\tstruct list_head list;\n\t/* list of all memory types part of this tier */\n\tstruct list_head memory_types;\n\t/*\n\t * start value of abstract distance. memory tier maps\n\t * an abstract distance  range,\n\t * adistance_start .. adistance_start + MEMTIER_CHUNK_SIZE\n\t */\n\tint adistance_start;\n\tstruct device dev;\n\t/* All the nodes that are part of all the lower memory tiers. */\n\tnodemask_t lower_tier_mask;\n};\n\nstruct demotion_nodes {\n\tnodemask_t preferred;\n};\n\nstruct node_memory_type_map {\n\tstruct memory_dev_type *memtype;\n\tint map_count;\n};\n\nstatic DEFINE_MUTEX(memory_tier_lock);\nstatic LIST_HEAD(memory_tiers);\nstatic struct node_memory_type_map node_memory_types[MAX_NUMNODES];\nstatic struct memory_dev_type *default_dram_type;\n\nstatic struct bus_type memory_tier_subsys = {\n\t.name = \"memory_tiering\",\n\t.dev_name = \"memory_tier\",\n};\n\n#ifdef CONFIG_MIGRATION\nstatic int top_tier_adistance;\n/*\n * node_demotion[] examples:\n *\n * Example 1:\n *\n * Node 0 & 1 are CPU + DRAM nodes, node 2 & 3 are PMEM nodes.\n *\n * node distances:\n * node   0    1    2    3\n *    0  10   20   30   40\n *    1  20   10   40   30\n *    2  30   40   10   40\n *    3  40   30   40   10\n *\n * memory_tiers0 = 0-1\n * memory_tiers1 = 2-3\n *\n * node_demotion[0].preferred = 2\n * node_demotion[1].preferred = 3\n * node_demotion[2].preferred = <empty>\n * node_demotion[3].preferred = <empty>\n *\n * Example 2:\n *\n * Node 0 & 1 are CPU + DRAM nodes, node 2 is memory-only DRAM node.\n *\n * node distances:\n * node   0    1    2\n *    0  10   20   30\n *    1  20   10   30\n *    2  30   30   10\n *\n * memory_tiers0 = 0-2\n *\n * node_demotion[0].preferred = <empty>\n * node_demotion[1].preferred = <empty>\n * node_demotion[2].preferred = <empty>\n *\n * Example 3:\n *\n * Node 0 is CPU + DRAM nodes, Node 1 is HBM node, node 2 is PMEM node.\n *\n * node distances:\n * node   0    1    2\n *    0  10   20   30\n *    1  20   10   40\n *    2  30   40   10\n *\n * memory_tiers0 = 1\n * memory_tiers1 = 0\n * memory_tiers2 = 2\n *\n * node_demotion[0].preferred = 2\n * node_demotion[1].preferred = 0\n * node_demotion[2].preferred = <empty>\n *\n */\nstatic struct demotion_nodes *node_demotion __read_mostly;\n#endif /* CONFIG_MIGRATION */\n\nstatic inline struct memory_tier *to_memory_tier(struct device *device)\n{\n\treturn container_of(device, struct memory_tier, dev);\n}\n\nstatic __always_inline nodemask_t get_memtier_nodemask(struct memory_tier *memtier)\n{\n\tnodemask_t nodes = NODE_MASK_NONE;\n\tstruct memory_dev_type *memtype;\n\n\tlist_for_each_entry(memtype, &memtier->memory_types, tier_sibiling)\n\t\tnodes_or(nodes, nodes, memtype->nodes);\n\n\treturn nodes;\n}\n\nstatic void memory_tier_device_release(struct device *dev)\n{\n\tstruct memory_tier *tier = to_memory_tier(dev);\n\t/*\n\t * synchronize_rcu in clear_node_memory_tier makes sure\n\t * we don't have rcu access to this memory tier.\n\t */\n\tkfree(tier);\n}\n\nstatic ssize_t nodelist_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tint ret;\n\tnodemask_t nmask;\n\n\tmutex_lock(&memory_tier_lock);\n\tnmask = get_memtier_nodemask(to_memory_tier(dev));\n\tret = sysfs_emit(buf, \"%*pbl\\n\", nodemask_pr_args(&nmask));\n\tmutex_unlock(&memory_tier_lock);\n\treturn ret;\n}\nstatic DEVICE_ATTR_RO(nodelist);\n\nstatic struct attribute *memtier_dev_attrs[] = {\n\t&dev_attr_nodelist.attr,\n\tNULL\n};\n\nstatic const struct attribute_group memtier_dev_group = {\n\t.attrs = memtier_dev_attrs,\n};\n\nstatic const struct attribute_group *memtier_dev_groups[] = {\n\t&memtier_dev_group,\n\tNULL\n};\n\nstatic struct memory_tier *find_create_memory_tier(struct memory_dev_type *memtype)\n{\n\tint ret;\n\tbool found_slot = false;\n\tstruct memory_tier *memtier, *new_memtier;\n\tint adistance = memtype->adistance;\n\tunsigned int memtier_adistance_chunk_size = MEMTIER_CHUNK_SIZE;\n\n\tlockdep_assert_held_once(&memory_tier_lock);\n\n\tadistance = round_down(adistance, memtier_adistance_chunk_size);\n\t/*\n\t * If the memtype is already part of a memory tier,\n\t * just return that.\n\t */\n\tif (!list_empty(&memtype->tier_sibiling)) {\n\t\tlist_for_each_entry(memtier, &memory_tiers, list) {\n\t\t\tif (adistance == memtier->adistance_start)\n\t\t\t\treturn memtier;\n\t\t}\n\t\tWARN_ON(1);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tlist_for_each_entry(memtier, &memory_tiers, list) {\n\t\tif (adistance == memtier->adistance_start) {\n\t\t\tgoto link_memtype;\n\t\t} else if (adistance < memtier->adistance_start) {\n\t\t\tfound_slot = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tnew_memtier = kzalloc(sizeof(struct memory_tier), GFP_KERNEL);\n\tif (!new_memtier)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tnew_memtier->adistance_start = adistance;\n\tINIT_LIST_HEAD(&new_memtier->list);\n\tINIT_LIST_HEAD(&new_memtier->memory_types);\n\tif (found_slot)\n\t\tlist_add_tail(&new_memtier->list, &memtier->list);\n\telse\n\t\tlist_add_tail(&new_memtier->list, &memory_tiers);\n\n\tnew_memtier->dev.id = adistance >> MEMTIER_CHUNK_BITS;\n\tnew_memtier->dev.bus = &memory_tier_subsys;\n\tnew_memtier->dev.release = memory_tier_device_release;\n\tnew_memtier->dev.groups = memtier_dev_groups;\n\n\tret = device_register(&new_memtier->dev);\n\tif (ret) {\n\t\tlist_del(&memtier->list);\n\t\tput_device(&memtier->dev);\n\t\treturn ERR_PTR(ret);\n\t}\n\tmemtier = new_memtier;\n\nlink_memtype:\n\tlist_add(&memtype->tier_sibiling, &memtier->memory_types);\n\treturn memtier;\n}\n\nstatic struct memory_tier *__node_get_memory_tier(int node)\n{\n\tpg_data_t *pgdat;\n\n\tpgdat = NODE_DATA(node);\n\tif (!pgdat)\n\t\treturn NULL;\n\t/*\n\t * Since we hold memory_tier_lock, we can avoid\n\t * RCU read locks when accessing the details. No\n\t * parallel updates are possible here.\n\t */\n\treturn rcu_dereference_check(pgdat->memtier,\n\t\t\t\t     lockdep_is_held(&memory_tier_lock));\n}\n\n#ifdef CONFIG_MIGRATION\nbool node_is_toptier(int node)\n{\n\tbool toptier;\n\tpg_data_t *pgdat;\n\tstruct memory_tier *memtier;\n\n\tpgdat = NODE_DATA(node);\n\tif (!pgdat)\n\t\treturn false;\n\n\trcu_read_lock();\n\tmemtier = rcu_dereference(pgdat->memtier);\n\tif (!memtier) {\n\t\ttoptier = true;\n\t\tgoto out;\n\t}\n\tif (memtier->adistance_start <= top_tier_adistance)\n\t\ttoptier = true;\n\telse\n\t\ttoptier = false;\nout:\n\trcu_read_unlock();\n\treturn toptier;\n}\n\nvoid node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets)\n{\n\tstruct memory_tier *memtier;\n\n\t/*\n\t * pg_data_t.memtier updates includes a synchronize_rcu()\n\t * which ensures that we either find NULL or a valid memtier\n\t * in NODE_DATA. protect the access via rcu_read_lock();\n\t */\n\trcu_read_lock();\n\tmemtier = rcu_dereference(pgdat->memtier);\n\tif (memtier)\n\t\t*targets = memtier->lower_tier_mask;\n\telse\n\t\t*targets = NODE_MASK_NONE;\n\trcu_read_unlock();\n}\n\n/**\n * next_demotion_node() - Get the next node in the demotion path\n * @node: The starting node to lookup the next node\n *\n * Return: node id for next memory node in the demotion path hierarchy\n * from @node; NUMA_NO_NODE if @node is terminal.  This does not keep\n * @node online or guarantee that it *continues* to be the next demotion\n * target.\n */\nint next_demotion_node(int node)\n{\n\tstruct demotion_nodes *nd;\n\tint target;\n\n\tif (!node_demotion)\n\t\treturn NUMA_NO_NODE;\n\n\tnd = &node_demotion[node];\n\n\t/*\n\t * node_demotion[] is updated without excluding this\n\t * function from running.\n\t *\n\t * Make sure to use RCU over entire code blocks if\n\t * node_demotion[] reads need to be consistent.\n\t */\n\trcu_read_lock();\n\t/*\n\t * If there are multiple target nodes, just select one\n\t * target node randomly.\n\t *\n\t * In addition, we can also use round-robin to select\n\t * target node, but we should introduce another variable\n\t * for node_demotion[] to record last selected target node,\n\t * that may cause cache ping-pong due to the changing of\n\t * last target node. Or introducing per-cpu data to avoid\n\t * caching issue, which seems more complicated. So selecting\n\t * target node randomly seems better until now.\n\t */\n\ttarget = node_random(&nd->preferred);\n\trcu_read_unlock();\n\n\treturn target;\n}\n\nstatic void disable_all_demotion_targets(void)\n{\n\tstruct memory_tier *memtier;\n\tint node;\n\n\tfor_each_node_state(node, N_MEMORY) {\n\t\tnode_demotion[node].preferred = NODE_MASK_NONE;\n\t\t/*\n\t\t * We are holding memory_tier_lock, it is safe\n\t\t * to access pgda->memtier.\n\t\t */\n\t\tmemtier = __node_get_memory_tier(node);\n\t\tif (memtier)\n\t\t\tmemtier->lower_tier_mask = NODE_MASK_NONE;\n\t}\n\t/*\n\t * Ensure that the \"disable\" is visible across the system.\n\t * Readers will see either a combination of before+disable\n\t * state or disable+after.  They will never see before and\n\t * after state together.\n\t */\n\tsynchronize_rcu();\n}\n\n/*\n * Find an automatic demotion target for all memory\n * nodes. Failing here is OK.  It might just indicate\n * being at the end of a chain.\n */\nstatic void establish_demotion_targets(void)\n{\n\tstruct memory_tier *memtier;\n\tstruct demotion_nodes *nd;\n\tint target = NUMA_NO_NODE, node;\n\tint distance, best_distance;\n\tnodemask_t tier_nodes, lower_tier;\n\n\tlockdep_assert_held_once(&memory_tier_lock);\n\n\tif (!node_demotion || !IS_ENABLED(CONFIG_MIGRATION))\n\t\treturn;\n\n\tdisable_all_demotion_targets();\n\n\tfor_each_node_state(node, N_MEMORY) {\n\t\tbest_distance = -1;\n\t\tnd = &node_demotion[node];\n\n\t\tmemtier = __node_get_memory_tier(node);\n\t\tif (!memtier || list_is_last(&memtier->list, &memory_tiers))\n\t\t\tcontinue;\n\t\t/*\n\t\t * Get the lower memtier to find the  demotion node list.\n\t\t */\n\t\tmemtier = list_next_entry(memtier, list);\n\t\ttier_nodes = get_memtier_nodemask(memtier);\n\t\t/*\n\t\t * find_next_best_node, use 'used' nodemask as a skip list.\n\t\t * Add all memory nodes except the selected memory tier\n\t\t * nodelist to skip list so that we find the best node from the\n\t\t * memtier nodelist.\n\t\t */\n\t\tnodes_andnot(tier_nodes, node_states[N_MEMORY], tier_nodes);\n\n\t\t/*\n\t\t * Find all the nodes in the memory tier node list of same best distance.\n\t\t * add them to the preferred mask. We randomly select between nodes\n\t\t * in the preferred mask when allocating pages during demotion.\n\t\t */\n\t\tdo {\n\t\t\ttarget = find_next_best_node(node, &tier_nodes);\n\t\t\tif (target == NUMA_NO_NODE)\n\t\t\t\tbreak;\n\n\t\t\tdistance = node_distance(node, target);\n\t\t\tif (distance == best_distance || best_distance == -1) {\n\t\t\t\tbest_distance = distance;\n\t\t\t\tnode_set(target, nd->preferred);\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (1);\n\t}\n\t/*\n\t * Promotion is allowed from a memory tier to higher\n\t * memory tier only if the memory tier doesn't include\n\t * compute. We want to skip promotion from a memory tier,\n\t * if any node that is part of the memory tier have CPUs.\n\t * Once we detect such a memory tier, we consider that tier\n\t * as top tiper from which promotion is not allowed.\n\t */\n\tlist_for_each_entry_reverse(memtier, &memory_tiers, list) {\n\t\ttier_nodes = get_memtier_nodemask(memtier);\n\t\tnodes_and(tier_nodes, node_states[N_CPU], tier_nodes);\n\t\tif (!nodes_empty(tier_nodes)) {\n\t\t\t/*\n\t\t\t * abstract distance below the max value of this memtier\n\t\t\t * is considered toptier.\n\t\t\t */\n\t\t\ttop_tier_adistance = memtier->adistance_start +\n\t\t\t\t\t\tMEMTIER_CHUNK_SIZE - 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\t/*\n\t * Now build the lower_tier mask for each node collecting node mask from\n\t * all memory tier below it. This allows us to fallback demotion page\n\t * allocation to a set of nodes that is closer the above selected\n\t * perferred node.\n\t */\n\tlower_tier = node_states[N_MEMORY];\n\tlist_for_each_entry(memtier, &memory_tiers, list) {\n\t\t/*\n\t\t * Keep removing current tier from lower_tier nodes,\n\t\t * This will remove all nodes in current and above\n\t\t * memory tier from the lower_tier mask.\n\t\t */\n\t\ttier_nodes = get_memtier_nodemask(memtier);\n\t\tnodes_andnot(lower_tier, lower_tier, tier_nodes);\n\t\tmemtier->lower_tier_mask = lower_tier;\n\t}\n}\n\n#else\nstatic inline void disable_all_demotion_targets(void) {}\nstatic inline void establish_demotion_targets(void) {}\n#endif /* CONFIG_MIGRATION */\n\nstatic inline void __init_node_memory_type(int node, struct memory_dev_type *memtype)\n{\n\tif (!node_memory_types[node].memtype)\n\t\tnode_memory_types[node].memtype = memtype;\n\t/*\n\t * for each device getting added in the same NUMA node\n\t * with this specific memtype, bump the map count. We\n\t * Only take memtype device reference once, so that\n\t * changing a node memtype can be done by droping the\n\t * only reference count taken here.\n\t */\n\n\tif (node_memory_types[node].memtype == memtype) {\n\t\tif (!node_memory_types[node].map_count++)\n\t\t\tkref_get(&memtype->kref);\n\t}\n}\n\nstatic struct memory_tier *set_node_memory_tier(int node)\n{\n\tstruct memory_tier *memtier;\n\tstruct memory_dev_type *memtype;\n\tpg_data_t *pgdat = NODE_DATA(node);\n\n\n\tlockdep_assert_held_once(&memory_tier_lock);\n\n\tif (!node_state(node, N_MEMORY))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t__init_node_memory_type(node, default_dram_type);\n\n\tmemtype = node_memory_types[node].memtype;\n\tnode_set(node, memtype->nodes);\n\tmemtier = find_create_memory_tier(memtype);\n\tif (!IS_ERR(memtier))\n\t\trcu_assign_pointer(pgdat->memtier, memtier);\n\treturn memtier;\n}\n\nstatic void destroy_memory_tier(struct memory_tier *memtier)\n{\n\tlist_del(&memtier->list);\n\tdevice_unregister(&memtier->dev);\n}\n\nstatic bool clear_node_memory_tier(int node)\n{\n\tbool cleared = false;\n\tpg_data_t *pgdat;\n\tstruct memory_tier *memtier;\n\n\tpgdat = NODE_DATA(node);\n\tif (!pgdat)\n\t\treturn false;\n\n\t/*\n\t * Make sure that anybody looking at NODE_DATA who finds\n\t * a valid memtier finds memory_dev_types with nodes still\n\t * linked to the memtier. We achieve this by waiting for\n\t * rcu read section to finish using synchronize_rcu.\n\t * This also enables us to free the destroyed memory tier\n\t * with kfree instead of kfree_rcu\n\t */\n\tmemtier = __node_get_memory_tier(node);\n\tif (memtier) {\n\t\tstruct memory_dev_type *memtype;\n\n\t\trcu_assign_pointer(pgdat->memtier, NULL);\n\t\tsynchronize_rcu();\n\t\tmemtype = node_memory_types[node].memtype;\n\t\tnode_clear(node, memtype->nodes);\n\t\tif (nodes_empty(memtype->nodes)) {\n\t\t\tlist_del_init(&memtype->tier_sibiling);\n\t\t\tif (list_empty(&memtier->memory_types))\n\t\t\t\tdestroy_memory_tier(memtier);\n\t\t}\n\t\tcleared = true;\n\t}\n\treturn cleared;\n}\n\nstatic void release_memtype(struct kref *kref)\n{\n\tstruct memory_dev_type *memtype;\n\n\tmemtype = container_of(kref, struct memory_dev_type, kref);\n\tkfree(memtype);\n}\n\nstruct memory_dev_type *alloc_memory_type(int adistance)\n{\n\tstruct memory_dev_type *memtype;\n\n\tmemtype = kmalloc(sizeof(*memtype), GFP_KERNEL);\n\tif (!memtype)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmemtype->adistance = adistance;\n\tINIT_LIST_HEAD(&memtype->tier_sibiling);\n\tmemtype->nodes  = NODE_MASK_NONE;\n\tkref_init(&memtype->kref);\n\treturn memtype;\n}\nEXPORT_SYMBOL_GPL(alloc_memory_type);\n\nvoid destroy_memory_type(struct memory_dev_type *memtype)\n{\n\tkref_put(&memtype->kref, release_memtype);\n}\nEXPORT_SYMBOL_GPL(destroy_memory_type);\n\nvoid init_node_memory_type(int node, struct memory_dev_type *memtype)\n{\n\n\tmutex_lock(&memory_tier_lock);\n\t__init_node_memory_type(node, memtype);\n\tmutex_unlock(&memory_tier_lock);\n}\nEXPORT_SYMBOL_GPL(init_node_memory_type);\n\nvoid clear_node_memory_type(int node, struct memory_dev_type *memtype)\n{\n\tmutex_lock(&memory_tier_lock);\n\tif (node_memory_types[node].memtype == memtype)\n\t\tnode_memory_types[node].map_count--;\n\t/*\n\t * If we umapped all the attached devices to this node,\n\t * clear the node memory type.\n\t */\n\tif (!node_memory_types[node].map_count) {\n\t\tnode_memory_types[node].memtype = NULL;\n\t\tkref_put(&memtype->kref, release_memtype);\n\t}\n\tmutex_unlock(&memory_tier_lock);\n}\nEXPORT_SYMBOL_GPL(clear_node_memory_type);\n\nstatic int __meminit memtier_hotplug_callback(struct notifier_block *self,\n\t\t\t\t\t      unsigned long action, void *_arg)\n{\n\tstruct memory_tier *memtier;\n\tstruct memory_notify *arg = _arg;\n\n\t/*\n\t * Only update the node migration order when a node is\n\t * changing status, like online->offline.\n\t */\n\tif (arg->status_change_nid < 0)\n\t\treturn notifier_from_errno(0);\n\n\tswitch (action) {\n\tcase MEM_OFFLINE:\n\t\tmutex_lock(&memory_tier_lock);\n\t\tif (clear_node_memory_tier(arg->status_change_nid))\n\t\t\testablish_demotion_targets();\n\t\tmutex_unlock(&memory_tier_lock);\n\t\tbreak;\n\tcase MEM_ONLINE:\n\t\tmutex_lock(&memory_tier_lock);\n\t\tmemtier = set_node_memory_tier(arg->status_change_nid);\n\t\tif (!IS_ERR(memtier))\n\t\t\testablish_demotion_targets();\n\t\tmutex_unlock(&memory_tier_lock);\n\t\tbreak;\n\t}\n\n\treturn notifier_from_errno(0);\n}\n\nstatic int __init memory_tier_init(void)\n{\n\tint ret, node;\n\tstruct memory_tier *memtier;\n\n\tret = subsys_virtual_register(&memory_tier_subsys, NULL);\n\tif (ret)\n\t\tpanic(\"%s() failed to register memory tier subsystem\\n\", __func__);\n\n#ifdef CONFIG_MIGRATION\n\tnode_demotion = kcalloc(nr_node_ids, sizeof(struct demotion_nodes),\n\t\t\t\tGFP_KERNEL);\n\tWARN_ON(!node_demotion);\n#endif\n\tmutex_lock(&memory_tier_lock);\n\t/*\n\t * For now we can have 4 faster memory tiers with smaller adistance\n\t * than default DRAM tier.\n\t */\n\tdefault_dram_type = alloc_memory_type(MEMTIER_ADISTANCE_DRAM);\n\tif (!default_dram_type)\n\t\tpanic(\"%s() failed to allocate default DRAM tier\\n\", __func__);\n\n\t/*\n\t * Look at all the existing N_MEMORY nodes and add them to\n\t * default memory tier or to a tier if we already have memory\n\t * types assigned.\n\t */\n\tfor_each_node_state(node, N_MEMORY) {\n\t\tmemtier = set_node_memory_tier(node);\n\t\tif (IS_ERR(memtier))\n\t\t\t/*\n\t\t\t * Continue with memtiers we are able to setup\n\t\t\t */\n\t\t\tbreak;\n\t}\n\testablish_demotion_targets();\n\tmutex_unlock(&memory_tier_lock);\n\n\thotplug_memory_notifier(memtier_hotplug_callback, MEMTIER_HOTPLUG_PRI);\n\treturn 0;\n}\nsubsys_initcall(memory_tier_init);\n\nbool numa_demotion_enabled = false;\n\n#ifdef CONFIG_MIGRATION\n#ifdef CONFIG_SYSFS\nstatic ssize_t numa_demotion_enabled_show(struct kobject *kobj,\n\t\t\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%s\\n\",\n\t\t\t  numa_demotion_enabled ? \"true\" : \"false\");\n}\n\nstatic ssize_t numa_demotion_enabled_store(struct kobject *kobj,\n\t\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t\t   const char *buf, size_t count)\n{\n\tssize_t ret;\n\n\tret = kstrtobool(buf, &numa_demotion_enabled);\n\tif (ret)\n\t\treturn ret;\n\n\treturn count;\n}\n\nstatic struct kobj_attribute numa_demotion_enabled_attr =\n\t__ATTR(demotion_enabled, 0644, numa_demotion_enabled_show,\n\t       numa_demotion_enabled_store);\n\nstatic struct attribute *numa_attrs[] = {\n\t&numa_demotion_enabled_attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group numa_attr_group = {\n\t.attrs = numa_attrs,\n};\n\nstatic int __init numa_init_sysfs(void)\n{\n\tint err;\n\tstruct kobject *numa_kobj;\n\n\tnuma_kobj = kobject_create_and_add(\"numa\", mm_kobj);\n\tif (!numa_kobj) {\n\t\tpr_err(\"failed to create numa kobject\\n\");\n\t\treturn -ENOMEM;\n\t}\n\terr = sysfs_create_group(numa_kobj, &numa_attr_group);\n\tif (err) {\n\t\tpr_err(\"failed to register numa group\\n\");\n\t\tgoto delete_obj;\n\t}\n\treturn 0;\n\ndelete_obj:\n\tkobject_put(numa_kobj);\n\treturn err;\n}\nsubsys_initcall(numa_init_sysfs);\n#endif /* CONFIG_SYSFS */\n#endif\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n#include <linux/slab.h>\n#include <linux/lockdep.h>\n#include <linux/sysfs.h>\n#include <linux/kobject.h>\n#include <linux/memory.h>\n#include <linux/memory-tiers.h>\n\n#include \"internal.h\"\n\nstruct memory_tier {\n\t/* hierarchy of memory tiers */\n\tstruct list_head list;\n\t/* list of all memory types part of this tier */\n\tstruct list_head memory_types;\n\t/*\n\t * start value of abstract distance. memory tier maps\n\t * an abstract distance  range,\n\t * adistance_start .. adistance_start + MEMTIER_CHUNK_SIZE\n\t */\n\tint adistance_start;\n\tstruct device dev;\n\t/* All the nodes that are part of all the lower memory tiers. */\n\tnodemask_t lower_tier_mask;\n};\n\nstruct demotion_nodes {\n\tnodemask_t preferred;\n};\n\nstruct node_memory_type_map {\n\tstruct memory_dev_type *memtype;\n\tint map_count;\n};\n\nstatic DEFINE_MUTEX(memory_tier_lock);\nstatic LIST_HEAD(memory_tiers);\nstatic struct node_memory_type_map node_memory_types[MAX_NUMNODES];\nstatic struct memory_dev_type *default_dram_type;\n\nstatic struct bus_type memory_tier_subsys = {\n\t.name = \"memory_tiering\",\n\t.dev_name = \"memory_tier\",\n};\n\n#ifdef CONFIG_MIGRATION\nstatic int top_tier_adistance;\n/*\n * node_demotion[] examples:\n *\n * Example 1:\n *\n * Node 0 & 1 are CPU + DRAM nodes, node 2 & 3 are PMEM nodes.\n *\n * node distances:\n * node   0    1    2    3\n *    0  10   20   30   40\n *    1  20   10   40   30\n *    2  30   40   10   40\n *    3  40   30   40   10\n *\n * memory_tiers0 = 0-1\n * memory_tiers1 = 2-3\n *\n * node_demotion[0].preferred = 2\n * node_demotion[1].preferred = 3\n * node_demotion[2].preferred = <empty>\n * node_demotion[3].preferred = <empty>\n *\n * Example 2:\n *\n * Node 0 & 1 are CPU + DRAM nodes, node 2 is memory-only DRAM node.\n *\n * node distances:\n * node   0    1    2\n *    0  10   20   30\n *    1  20   10   30\n *    2  30   30   10\n *\n * memory_tiers0 = 0-2\n *\n * node_demotion[0].preferred = <empty>\n * node_demotion[1].preferred = <empty>\n * node_demotion[2].preferred = <empty>\n *\n * Example 3:\n *\n * Node 0 is CPU + DRAM nodes, Node 1 is HBM node, node 2 is PMEM node.\n *\n * node distances:\n * node   0    1    2\n *    0  10   20   30\n *    1  20   10   40\n *    2  30   40   10\n *\n * memory_tiers0 = 1\n * memory_tiers1 = 0\n * memory_tiers2 = 2\n *\n * node_demotion[0].preferred = 2\n * node_demotion[1].preferred = 0\n * node_demotion[2].preferred = <empty>\n *\n */\nstatic struct demotion_nodes *node_demotion __read_mostly;\n#endif /* CONFIG_MIGRATION */\n\nstatic inline struct memory_tier *to_memory_tier(struct device *device)\n{\n\treturn container_of(device, struct memory_tier, dev);\n}\n\nstatic __always_inline nodemask_t get_memtier_nodemask(struct memory_tier *memtier)\n{\n\tnodemask_t nodes = NODE_MASK_NONE;\n\tstruct memory_dev_type *memtype;\n\n\tlist_for_each_entry(memtype, &memtier->memory_types, tier_sibiling)\n\t\tnodes_or(nodes, nodes, memtype->nodes);\n\n\treturn nodes;\n}\n\nstatic void memory_tier_device_release(struct device *dev)\n{\n\tstruct memory_tier *tier = to_memory_tier(dev);\n\t/*\n\t * synchronize_rcu in clear_node_memory_tier makes sure\n\t * we don't have rcu access to this memory tier.\n\t */\n\tkfree(tier);\n}\n\nstatic ssize_t nodelist_show(struct device *dev,\n\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tint ret;\n\tnodemask_t nmask;\n\n\tmutex_lock(&memory_tier_lock);\n\tnmask = get_memtier_nodemask(to_memory_tier(dev));\n\tret = sysfs_emit(buf, \"%*pbl\\n\", nodemask_pr_args(&nmask));\n\tmutex_unlock(&memory_tier_lock);\n\treturn ret;\n}\nstatic DEVICE_ATTR_RO(nodelist);\n\nstatic struct attribute *memtier_dev_attrs[] = {\n\t&dev_attr_nodelist.attr,\n\tNULL\n};\n\nstatic const struct attribute_group memtier_dev_group = {\n\t.attrs = memtier_dev_attrs,\n};\n\nstatic const struct attribute_group *memtier_dev_groups[] = {\n\t&memtier_dev_group,\n\tNULL\n};\n\nstatic struct memory_tier *find_create_memory_tier(struct memory_dev_type *memtype)\n{\n\tint ret;\n\tbool found_slot = false;\n\tstruct memory_tier *memtier, *new_memtier;\n\tint adistance = memtype->adistance;\n\tunsigned int memtier_adistance_chunk_size = MEMTIER_CHUNK_SIZE;\n\n\tlockdep_assert_held_once(&memory_tier_lock);\n\n\tadistance = round_down(adistance, memtier_adistance_chunk_size);\n\t/*\n\t * If the memtype is already part of a memory tier,\n\t * just return that.\n\t */\n\tif (!list_empty(&memtype->tier_sibiling)) {\n\t\tlist_for_each_entry(memtier, &memory_tiers, list) {\n\t\t\tif (adistance == memtier->adistance_start)\n\t\t\t\treturn memtier;\n\t\t}\n\t\tWARN_ON(1);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tlist_for_each_entry(memtier, &memory_tiers, list) {\n\t\tif (adistance == memtier->adistance_start) {\n\t\t\tgoto link_memtype;\n\t\t} else if (adistance < memtier->adistance_start) {\n\t\t\tfound_slot = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tnew_memtier = kzalloc(sizeof(struct memory_tier), GFP_KERNEL);\n\tif (!new_memtier)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tnew_memtier->adistance_start = adistance;\n\tINIT_LIST_HEAD(&new_memtier->list);\n\tINIT_LIST_HEAD(&new_memtier->memory_types);\n\tif (found_slot)\n\t\tlist_add_tail(&new_memtier->list, &memtier->list);\n\telse\n\t\tlist_add_tail(&new_memtier->list, &memory_tiers);\n\n\tnew_memtier->dev.id = adistance >> MEMTIER_CHUNK_BITS;\n\tnew_memtier->dev.bus = &memory_tier_subsys;\n\tnew_memtier->dev.release = memory_tier_device_release;\n\tnew_memtier->dev.groups = memtier_dev_groups;\n\n\tret = device_register(&new_memtier->dev);\n\tif (ret) {\n\t\tlist_del(&memtier->list);\n\t\tput_device(&memtier->dev);\n\t\treturn ERR_PTR(ret);\n\t}\n\tmemtier = new_memtier;\n\nlink_memtype:\n\tlist_add(&memtype->tier_sibiling, &memtier->memory_types);\n\treturn memtier;\n}\n\nstatic struct memory_tier *__node_get_memory_tier(int node)\n{\n\tpg_data_t *pgdat;\n\n\tpgdat = NODE_DATA(node);\n\tif (!pgdat)\n\t\treturn NULL;\n\t/*\n\t * Since we hold memory_tier_lock, we can avoid\n\t * RCU read locks when accessing the details. No\n\t * parallel updates are possible here.\n\t */\n\treturn rcu_dereference_check(pgdat->memtier,\n\t\t\t\t     lockdep_is_held(&memory_tier_lock));\n}\n\n#ifdef CONFIG_MIGRATION\nbool node_is_toptier(int node)\n{\n\tbool toptier;\n\tpg_data_t *pgdat;\n\tstruct memory_tier *memtier;\n\n\tpgdat = NODE_DATA(node);\n\tif (!pgdat)\n\t\treturn false;\n\n\trcu_read_lock();\n\tmemtier = rcu_dereference(pgdat->memtier);\n\tif (!memtier) {\n\t\ttoptier = true;\n\t\tgoto out;\n\t}\n\tif (memtier->adistance_start <= top_tier_adistance)\n\t\ttoptier = true;\n\telse\n\t\ttoptier = false;\nout:\n\trcu_read_unlock();\n\treturn toptier;\n}\n\nvoid node_get_allowed_targets(pg_data_t *pgdat, nodemask_t *targets)\n{\n\tstruct memory_tier *memtier;\n\n\t/*\n\t * pg_data_t.memtier updates includes a synchronize_rcu()\n\t * which ensures that we either find NULL or a valid memtier\n\t * in NODE_DATA. protect the access via rcu_read_lock();\n\t */\n\trcu_read_lock();\n\tmemtier = rcu_dereference(pgdat->memtier);\n\tif (memtier)\n\t\t*targets = memtier->lower_tier_mask;\n\telse\n\t\t*targets = NODE_MASK_NONE;\n\trcu_read_unlock();\n}\n\n/**\n * next_demotion_node() - Get the next node in the demotion path\n * @node: The starting node to lookup the next node\n *\n * Return: node id for next memory node in the demotion path hierarchy\n * from @node; NUMA_NO_NODE if @node is terminal.  This does not keep\n * @node online or guarantee that it *continues* to be the next demotion\n * target.\n */\nint next_demotion_node(int node)\n{\n\tstruct demotion_nodes *nd;\n\tint target;\n\n\tif (!node_demotion)\n\t\treturn NUMA_NO_NODE;\n\n\tnd = &node_demotion[node];\n\n\t/*\n\t * node_demotion[] is updated without excluding this\n\t * function from running.\n\t *\n\t * Make sure to use RCU over entire code blocks if\n\t * node_demotion[] reads need to be consistent.\n\t */\n\trcu_read_lock();\n\t/*\n\t * If there are multiple target nodes, just select one\n\t * target node randomly.\n\t *\n\t * In addition, we can also use round-robin to select\n\t * target node, but we should introduce another variable\n\t * for node_demotion[] to record last selected target node,\n\t * that may cause cache ping-pong due to the changing of\n\t * last target node. Or introducing per-cpu data to avoid\n\t * caching issue, which seems more complicated. So selecting\n\t * target node randomly seems better until now.\n\t */\n\ttarget = node_random(&nd->preferred);\n\trcu_read_unlock();\n\n\treturn target;\n}\n\nstatic void disable_all_demotion_targets(void)\n{\n\tstruct memory_tier *memtier;\n\tint node;\n\n\tfor_each_node_state(node, N_MEMORY) {\n\t\tnode_demotion[node].preferred = NODE_MASK_NONE;\n\t\t/*\n\t\t * We are holding memory_tier_lock, it is safe\n\t\t * to access pgda->memtier.\n\t\t */\n\t\tmemtier = __node_get_memory_tier(node);\n\t\tif (memtier)\n\t\t\tmemtier->lower_tier_mask = NODE_MASK_NONE;\n\t}\n\t/*\n\t * Ensure that the \"disable\" is visible across the system.\n\t * Readers will see either a combination of before+disable\n\t * state or disable+after.  They will never see before and\n\t * after state together.\n\t */\n\tsynchronize_rcu();\n}\n\n/*\n * Find an automatic demotion target for all memory\n * nodes. Failing here is OK.  It might just indicate\n * being at the end of a chain.\n */\nstatic void establish_demotion_targets(void)\n{\n\tstruct memory_tier *memtier;\n\tstruct demotion_nodes *nd;\n\tint target = NUMA_NO_NODE, node;\n\tint distance, best_distance;\n\tnodemask_t tier_nodes, lower_tier;\n\n\tlockdep_assert_held_once(&memory_tier_lock);\n\n\tif (!node_demotion || !IS_ENABLED(CONFIG_MIGRATION))\n\t\treturn;\n\n\tdisable_all_demotion_targets();\n\n\tfor_each_node_state(node, N_MEMORY) {\n\t\tbest_distance = -1;\n\t\tnd = &node_demotion[node];\n\n\t\tmemtier = __node_get_memory_tier(node);\n\t\tif (!memtier || list_is_last(&memtier->list, &memory_tiers))\n\t\t\tcontinue;\n\t\t/*\n\t\t * Get the lower memtier to find the  demotion node list.\n\t\t */\n\t\tmemtier = list_next_entry(memtier, list);\n\t\ttier_nodes = get_memtier_nodemask(memtier);\n\t\t/*\n\t\t * find_next_best_node, use 'used' nodemask as a skip list.\n\t\t * Add all memory nodes except the selected memory tier\n\t\t * nodelist to skip list so that we find the best node from the\n\t\t * memtier nodelist.\n\t\t */\n\t\tnodes_andnot(tier_nodes, node_states[N_MEMORY], tier_nodes);\n\n\t\t/*\n\t\t * Find all the nodes in the memory tier node list of same best distance.\n\t\t * add them to the preferred mask. We randomly select between nodes\n\t\t * in the preferred mask when allocating pages during demotion.\n\t\t */\n\t\tdo {\n\t\t\ttarget = find_next_best_node(node, &tier_nodes);\n\t\t\tif (target == NUMA_NO_NODE)\n\t\t\t\tbreak;\n\n\t\t\tdistance = node_distance(node, target);\n\t\t\tif (distance == best_distance || best_distance == -1) {\n\t\t\t\tbest_distance = distance;\n\t\t\t\tnode_set(target, nd->preferred);\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (1);\n\t}\n\t/*\n\t * Promotion is allowed from a memory tier to higher\n\t * memory tier only if the memory tier doesn't include\n\t * compute. We want to skip promotion from a memory tier,\n\t * if any node that is part of the memory tier have CPUs.\n\t * Once we detect such a memory tier, we consider that tier\n\t * as top tiper from which promotion is not allowed.\n\t */\n\tlist_for_each_entry_reverse(memtier, &memory_tiers, list) {\n\t\ttier_nodes = get_memtier_nodemask(memtier);\n\t\tnodes_and(tier_nodes, node_states[N_CPU], tier_nodes);\n\t\tif (!nodes_empty(tier_nodes)) {\n\t\t\t/*\n\t\t\t * abstract distance below the max value of this memtier\n\t\t\t * is considered toptier.\n\t\t\t */\n\t\t\ttop_tier_adistance = memtier->adistance_start +\n\t\t\t\t\t\tMEMTIER_CHUNK_SIZE - 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\t/*\n\t * Now build the lower_tier mask for each node collecting node mask from\n\t * all memory tier below it. This allows us to fallback demotion page\n\t * allocation to a set of nodes that is closer the above selected\n\t * perferred node.\n\t */\n\tlower_tier = node_states[N_MEMORY];\n\tlist_for_each_entry(memtier, &memory_tiers, list) {\n\t\t/*\n\t\t * Keep removing current tier from lower_tier nodes,\n\t\t * This will remove all nodes in current and above\n\t\t * memory tier from the lower_tier mask.\n\t\t */\n\t\ttier_nodes = get_memtier_nodemask(memtier);\n\t\tnodes_andnot(lower_tier, lower_tier, tier_nodes);\n\t\tmemtier->lower_tier_mask = lower_tier;\n\t}\n}\n\n#else\nstatic inline void disable_all_demotion_targets(void) {}\nstatic inline void establish_demotion_targets(void) {}\n#endif /* CONFIG_MIGRATION */\n\nstatic inline void __init_node_memory_type(int node, struct memory_dev_type *memtype)\n{\n\tif (!node_memory_types[node].memtype)\n\t\tnode_memory_types[node].memtype = memtype;\n\t/*\n\t * for each device getting added in the same NUMA node\n\t * with this specific memtype, bump the map count. We\n\t * Only take memtype device reference once, so that\n\t * changing a node memtype can be done by droping the\n\t * only reference count taken here.\n\t */\n\n\tif (node_memory_types[node].memtype == memtype) {\n\t\tif (!node_memory_types[node].map_count++)\n\t\t\tkref_get(&memtype->kref);\n\t}\n}\n\nstatic struct memory_tier *set_node_memory_tier(int node)\n{\n\tstruct memory_tier *memtier;\n\tstruct memory_dev_type *memtype;\n\tpg_data_t *pgdat = NODE_DATA(node);\n\n\n\tlockdep_assert_held_once(&memory_tier_lock);\n\n\tif (!node_state(node, N_MEMORY))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t__init_node_memory_type(node, default_dram_type);\n\n\tmemtype = node_memory_types[node].memtype;\n\tnode_set(node, memtype->nodes);\n\tmemtier = find_create_memory_tier(memtype);\n\tif (!IS_ERR(memtier))\n\t\trcu_assign_pointer(pgdat->memtier, memtier);\n\treturn memtier;\n}\n\nstatic void destroy_memory_tier(struct memory_tier *memtier)\n{\n\tlist_del(&memtier->list);\n\tdevice_unregister(&memtier->dev);\n}\n\nstatic bool clear_node_memory_tier(int node)\n{\n\tbool cleared = false;\n\tpg_data_t *pgdat;\n\tstruct memory_tier *memtier;\n\n\tpgdat = NODE_DATA(node);\n\tif (!pgdat)\n\t\treturn false;\n\n\t/*\n\t * Make sure that anybody looking at NODE_DATA who finds\n\t * a valid memtier finds memory_dev_types with nodes still\n\t * linked to the memtier. We achieve this by waiting for\n\t * rcu read section to finish using synchronize_rcu.\n\t * This also enables us to free the destroyed memory tier\n\t * with kfree instead of kfree_rcu\n\t */\n\tmemtier = __node_get_memory_tier(node);\n\tif (memtier) {\n\t\tstruct memory_dev_type *memtype;\n\n\t\trcu_assign_pointer(pgdat->memtier, NULL);\n\t\tsynchronize_rcu();\n\t\tmemtype = node_memory_types[node].memtype;\n\t\tnode_clear(node, memtype->nodes);\n\t\tif (nodes_empty(memtype->nodes)) {\n\t\t\tlist_del_init(&memtype->tier_sibiling);\n\t\t\tif (list_empty(&memtier->memory_types))\n\t\t\t\tdestroy_memory_tier(memtier);\n\t\t}\n\t\tcleared = true;\n\t}\n\treturn cleared;\n}\n\nstatic void release_memtype(struct kref *kref)\n{\n\tstruct memory_dev_type *memtype;\n\n\tmemtype = container_of(kref, struct memory_dev_type, kref);\n\tkfree(memtype);\n}\n\nstruct memory_dev_type *alloc_memory_type(int adistance)\n{\n\tstruct memory_dev_type *memtype;\n\n\tmemtype = kmalloc(sizeof(*memtype), GFP_KERNEL);\n\tif (!memtype)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmemtype->adistance = adistance;\n\tINIT_LIST_HEAD(&memtype->tier_sibiling);\n\tmemtype->nodes  = NODE_MASK_NONE;\n\tkref_init(&memtype->kref);\n\treturn memtype;\n}\nEXPORT_SYMBOL_GPL(alloc_memory_type);\n\nvoid destroy_memory_type(struct memory_dev_type *memtype)\n{\n\tkref_put(&memtype->kref, release_memtype);\n}\nEXPORT_SYMBOL_GPL(destroy_memory_type);\n\nvoid init_node_memory_type(int node, struct memory_dev_type *memtype)\n{\n\n\tmutex_lock(&memory_tier_lock);\n\t__init_node_memory_type(node, memtype);\n\tmutex_unlock(&memory_tier_lock);\n}\nEXPORT_SYMBOL_GPL(init_node_memory_type);\n\nvoid clear_node_memory_type(int node, struct memory_dev_type *memtype)\n{\n\tmutex_lock(&memory_tier_lock);\n\tif (node_memory_types[node].memtype == memtype)\n\t\tnode_memory_types[node].map_count--;\n\t/*\n\t * If we umapped all the attached devices to this node,\n\t * clear the node memory type.\n\t */\n\tif (!node_memory_types[node].map_count) {\n\t\tnode_memory_types[node].memtype = NULL;\n\t\tkref_put(&memtype->kref, release_memtype);\n\t}\n\tmutex_unlock(&memory_tier_lock);\n}\nEXPORT_SYMBOL_GPL(clear_node_memory_type);\n\nstatic int __meminit memtier_hotplug_callback(struct notifier_block *self,\n\t\t\t\t\t      unsigned long action, void *_arg)\n{\n\tstruct memory_tier *memtier;\n\tstruct memory_notify *arg = _arg;\n\n\t/*\n\t * Only update the node migration order when a node is\n\t * changing status, like online->offline.\n\t */\n\tif (arg->status_change_nid < 0)\n\t\treturn notifier_from_errno(0);\n\n\tswitch (action) {\n\tcase MEM_OFFLINE:\n\t\tmutex_lock(&memory_tier_lock);\n\t\tif (clear_node_memory_tier(arg->status_change_nid))\n\t\t\testablish_demotion_targets();\n\t\tmutex_unlock(&memory_tier_lock);\n\t\tbreak;\n\tcase MEM_ONLINE:\n\t\tmutex_lock(&memory_tier_lock);\n\t\tmemtier = set_node_memory_tier(arg->status_change_nid);\n\t\tif (!IS_ERR(memtier))\n\t\t\testablish_demotion_targets();\n\t\tmutex_unlock(&memory_tier_lock);\n\t\tbreak;\n\t}\n\n\treturn notifier_from_errno(0);\n}\n\nstatic int __init memory_tier_init(void)\n{\n\tint ret, node;\n\tstruct memory_tier *memtier;\n\n\tret = subsys_virtual_register(&memory_tier_subsys, NULL);\n\tif (ret)\n\t\tpanic(\"%s() failed to register memory tier subsystem\\n\", __func__);\n\n#ifdef CONFIG_MIGRATION\n\tnode_demotion = kcalloc(nr_node_ids, sizeof(struct demotion_nodes),\n\t\t\t\tGFP_KERNEL);\n\tWARN_ON(!node_demotion);\n#endif\n\tmutex_lock(&memory_tier_lock);\n\t/*\n\t * For now we can have 4 faster memory tiers with smaller adistance\n\t * than default DRAM tier.\n\t */\n\tdefault_dram_type = alloc_memory_type(MEMTIER_ADISTANCE_DRAM);\n\tif (IS_ERR(default_dram_type))\n\t\tpanic(\"%s() failed to allocate default DRAM tier\\n\", __func__);\n\n\t/*\n\t * Look at all the existing N_MEMORY nodes and add them to\n\t * default memory tier or to a tier if we already have memory\n\t * types assigned.\n\t */\n\tfor_each_node_state(node, N_MEMORY) {\n\t\tmemtier = set_node_memory_tier(node);\n\t\tif (IS_ERR(memtier))\n\t\t\t/*\n\t\t\t * Continue with memtiers we are able to setup\n\t\t\t */\n\t\t\tbreak;\n\t}\n\testablish_demotion_targets();\n\tmutex_unlock(&memory_tier_lock);\n\n\thotplug_memory_notifier(memtier_hotplug_callback, MEMTIER_HOTPLUG_PRI);\n\treturn 0;\n}\nsubsys_initcall(memory_tier_init);\n\nbool numa_demotion_enabled = false;\n\n#ifdef CONFIG_MIGRATION\n#ifdef CONFIG_SYSFS\nstatic ssize_t numa_demotion_enabled_show(struct kobject *kobj,\n\t\t\t\t\t  struct kobj_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%s\\n\",\n\t\t\t  numa_demotion_enabled ? \"true\" : \"false\");\n}\n\nstatic ssize_t numa_demotion_enabled_store(struct kobject *kobj,\n\t\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t\t   const char *buf, size_t count)\n{\n\tssize_t ret;\n\n\tret = kstrtobool(buf, &numa_demotion_enabled);\n\tif (ret)\n\t\treturn ret;\n\n\treturn count;\n}\n\nstatic struct kobj_attribute numa_demotion_enabled_attr =\n\t__ATTR(demotion_enabled, 0644, numa_demotion_enabled_show,\n\t       numa_demotion_enabled_store);\n\nstatic struct attribute *numa_attrs[] = {\n\t&numa_demotion_enabled_attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group numa_attr_group = {\n\t.attrs = numa_attrs,\n};\n\nstatic int __init numa_init_sysfs(void)\n{\n\tint err;\n\tstruct kobject *numa_kobj;\n\n\tnuma_kobj = kobject_create_and_add(\"numa\", mm_kobj);\n\tif (!numa_kobj) {\n\t\tpr_err(\"failed to create numa kobject\\n\");\n\t\treturn -ENOMEM;\n\t}\n\terr = sysfs_create_group(numa_kobj, &numa_attr_group);\n\tif (err) {\n\t\tpr_err(\"failed to register numa group\\n\");\n\t\tgoto delete_obj;\n\t}\n\treturn 0;\n\ndelete_obj:\n\tkobject_put(numa_kobj);\n\treturn err;\n}\nsubsys_initcall(numa_init_sysfs);\n#endif /* CONFIG_SYSFS */\n#endif\n"], "filenames": ["mm/memory-tiers.c"], "buggy_code_start_loc": [648], "buggy_code_end_loc": [649], "fixing_code_start_loc": [648], "fixing_code_end_loc": [649], "type": "CWE-476", "message": "** DISPUTED ** In the Linux kernel before 6.2, mm/memory-tiers.c misinterprets the alloc_memory_type return value (expects it to be NULL in the error case, whereas it is actually an error pointer). NOTE: this is disputed by third parties because there are no realistic cases in which a user can cause the alloc_memory_type error case to be reached.", "other": {"cve": {"id": "CVE-2023-23005", "sourceIdentifier": "cve@mitre.org", "published": "2023-03-01T20:15:15.100", "lastModified": "2023-03-13T15:23:56.267", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "** DISPUTED ** In the Linux kernel before 6.2, mm/memory-tiers.c misinterprets the alloc_memory_type return value (expects it to be NULL in the error case, whereas it is actually an error pointer). NOTE: this is disputed by third parties because there are no realistic cases in which a user can cause the alloc_memory_type error case to be reached."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "6.2", "matchCriteriaId": "108695B6-7133-4B6C-80AF-0F66880FE858"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_server:15:sp5:*:*:*:*:*:*", "matchCriteriaId": "F6461786-1240-4D6A-B767-9EE3BD4A6DAA"}]}]}], "references": [{"url": "https://bugzilla.suse.com/show_bug.cgi?id=1208844#c2", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v6.x/ChangeLog-6.2", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Release Notes"]}, {"url": "https://github.com/torvalds/linux/commit/4a625ceee8a0ab0273534cb6b432ce6b331db5ee", "source": "cve@mitre.org", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/4a625ceee8a0ab0273534cb6b432ce6b331db5ee"}}