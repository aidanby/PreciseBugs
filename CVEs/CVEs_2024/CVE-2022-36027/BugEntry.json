{"buggy_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/compiler/mlir/lite/quantization/quantization_utils.h\"\n\n#include <algorithm>\n#include <cstdint>\n#include <iterator>\n#include <limits>\n#include <memory>\n#include <numeric>\n#include <string>\n\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/FakeQuantSupport.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/QuantOps.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/QuantTypes.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/QuantizeUtils.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/UniformSupport.h\"  // from @llvm-project\n#include \"mlir/IR/Attributes.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinAttributes.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinTypes.h\"  // from @llvm-project\n#include \"mlir/IR/Diagnostics.h\"  // from @llvm-project\n#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n#include \"mlir/Support/LogicalResult.h\"  // from @llvm-project\n#include \"tensorflow/compiler/mlir/lite/quantization/quantization_traits.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n#include \"tensorflow/lite/tools/optimize/quantization_utils.h\"\n\nnamespace mlir {\n\n// This includes the interface class definition. It couldn't be in a namespace\n// because the table gen doesn't emit the namespace when it is used.\n#include \"tensorflow/compiler/mlir/lite/quantization/quantization_interface.cc.inc\"\n\nnamespace quant {\n\nnamespace {\nconstexpr double kSmallestHalfRange = kNearZeroTolerance / 2;\nusing QType = quant::QuantizedType;\n\n// This method expands the range to be larger than or equal to 1.0e-6, if it is\n// very small (< 1.0e-6). This is to prevent very large quantized value by this\n// range.\nvoid ExpandVerySmallRange(ArrayRef<double> mins, ArrayRef<double> maxs,\n                          SmallVectorImpl<double>* effective_mins,\n                          SmallVectorImpl<double>* effective_maxs) {\n  for (auto arg : llvm::zip(mins, maxs)) {\n    double min = std::get<0>(arg);\n    double max = std::get<1>(arg);\n    // The range is wide, then use the same min/max.\n    if ((max - min) > kNearZeroTolerance) {\n      effective_mins->push_back(min);\n      effective_maxs->push_back(max);\n      continue;\n    }\n\n    // The range is small. Expands the range to stride 0.0 and also at least\n    // 1.0e-6.\n    effective_mins->push_back(std::min(min, -kSmallestHalfRange));\n    effective_maxs->push_back(std::max(max, kSmallestHalfRange));\n  }\n}\n\n// Set the min / max, scale and zero_points from the fake quant num_bits\n// attribute from QAT.\nQuantizedType ResetMinMaxFromNumBits(QuantizedType type, int num_bits,\n                                     bool narrow_range, bool is_signed) {\n  if (num_bits >= 8) {\n    return type;\n  }\n  int64_t qmin = QType::getDefaultMinimumForInteger(is_signed, num_bits);\n  int64_t qmax = QType::getDefaultMaximumForInteger(is_signed, num_bits);\n  if (narrow_range) {\n    qmin += 1;\n  }\n  const int64_t storage_type_min = type.getStorageTypeMin();\n  const int64_t storage_type_max = type.getStorageTypeMax();\n  const double rate =\n      static_cast<double>(storage_type_max - storage_type_min) / (qmax - qmin);\n  const auto& recalculate_scale = [&](double scale) -> double {\n    return scale * rate;\n  };\n  const auto& recalculate_zero_point = [&](int64_t zero_point) -> int64_t {\n    return qmax - std::round((storage_type_max - zero_point) / rate);\n  };\n  if (auto q_type = type.dyn_cast<UniformQuantizedType>()) {\n    const double scale = recalculate_scale(q_type.getScale());\n    const double zero_point = recalculate_zero_point(q_type.getZeroPoint());\n    return UniformQuantizedType::get(q_type.getFlags(), q_type.getStorageType(),\n                                     q_type.getExpressedType(), scale,\n                                     zero_point, qmin, qmax);\n  } else if (auto q_type = type.dyn_cast<UniformQuantizedPerAxisType>()) {\n    const int size = q_type.getScales().size();\n    SmallVector<double, 4> scales(size);\n    SmallVector<int64_t, 4> zero_points(size);\n    for (int i = 0; i < size; ++i) {\n      scales[i] = recalculate_scale(q_type.getScales()[i]);\n      zero_points[i] = recalculate_zero_point(q_type.getZeroPoints()[i]);\n    }\n    return UniformQuantizedPerAxisType::get(\n        q_type.getFlags(), q_type.getStorageType(), q_type.getExpressedType(),\n        scales, zero_points, q_type.getQuantizedDimension(), qmin, qmax);\n  } else {\n    llvm_unreachable(\"Unsupported QuantizedType in ResetMinMaxFromNumBits\");\n  }\n  return type;\n}\n\n// Repeats the content of `data` multiple times to resize to `target_size`.\n// Note that this only broadcast across one dimension.\ntemplate <typename T>\nbool BroadcastVector(int target_size, SmallVectorImpl<T>& data) {\n  int size = data.size();\n  if (size != target_size) {\n    if (target_size % size != 0) return true;\n    data.reserve(target_size);\n    for (int i = 1, e = target_size / size; i != e; ++i) {\n      data.insert(data.end(), data.begin(), data.begin() + size);\n    }\n  }\n  return false;\n}\n\n// Changes the axis of the input per-channel quantized type to match the\n// dimension of the target type. Returns nullptr if it fails.\nquant::UniformQuantizedPerAxisType ResetAxisAndBroadcast(\n    ArrayRef<int64_t> shape, quant::UniformQuantizedPerAxisType qtype,\n    Type target, int quant_dim) {\n  auto shaped = target.dyn_cast<RankedTensorType>();\n  if (!shaped) return {};\n  ArrayRef<int64_t> new_shape = shaped.getShape();\n\n  SmallVector<double, 4> scales(qtype.getScales().begin(),\n                                qtype.getScales().end());\n  SmallVector<int64_t, 4> zero_points(qtype.getZeroPoints().begin(),\n                                      qtype.getZeroPoints().end());\n\n  if (new_shape.size() == shape.size()) {  // same rank\n    // Broadcast the scales and zero points to match the target size, which is\n    // usually the axis-th dimension of the target type. Currently, it covers\n    // two cases:\n    // - for Transpose, the data layout is changed so the `dim[axis]` still\n    // equals to the `scales_size`. The broadcast skips;\n    // - for Reshape, the data layout isn't changed but the innermost dimension\n    // is expand to cover the last two original dimensions. Thus we just need to\n    // be repeated the `scales` dim[2] times to covers the new dim length.\n    //\n    // TODO(b/141709944): after the fix, the `scales` can be for dim[2], thus we\n    // have to repeat each elements in the `scales` locally dim[3] times.\n    if (BroadcastVector<double>(shaped.getDimSize(quant_dim), scales) ||\n        BroadcastVector<int64_t>(shaped.getDimSize(quant_dim), zero_points)) {\n      return {};\n    }\n  } else if ((new_shape.size() == shape.size() + 1) && new_shape.back() == 1) {\n    // This is a trivial shift left, then we shift the quant_dim as well.\n    if (std::equal(shape.begin(), shape.end(), new_shape.begin()) &&\n        quant_dim == -1) {\n      quant_dim = shape.size() + quant_dim;\n    } else {\n      return {};\n    }\n  } else {\n    return {};\n  }\n\n  return quant::UniformQuantizedPerAxisType::get(\n      qtype.getFlags(), qtype.getStorageType(), qtype.getExpressedType(),\n      scales, zero_points, quant_dim, qtype.getStorageTypeMin(),\n      qtype.getStorageTypeMax());\n}\n\n}  // namespace\n\nbool IsOpNotQuantizable(Operation* op) {\n  // If it is terminator or not quantizable or any ops form the mlir quant\n  // ops dialect, we shouldn't rewrite.\n  bool attr_enforced_quantizable =\n      op->hasAttrOfType<StringAttr>(kQuantTraitAttrName) &&\n      op->getAttrOfType<StringAttr>(kQuantTraitAttrName).getValue().str() ==\n          QuantTraitValues[QuantizationTrait::FullyQuantizable];\n\n  // Constant ops do not have QuantizableResult attribute but they can deal with\n  // quantized tensors.\n  if (llvm::isa<func::ConstantOp, arith::ConstantOp, quant::StatisticsOp>(op))\n    return false;\n\n  bool prop_enforced_quantizable =\n      op->hasTrait<OpTrait::quant::QuantizableResult>();\n\n  return op->hasTrait<OpTrait::IsTerminator>() ||\n         llvm::isa<quant::QuantizeCastOp, quant::DequantizeCastOp>(op) ||\n         (!attr_enforced_quantizable && !prop_enforced_quantizable);\n}\n\n// Returns the quantized type for the\n// input_type/min/max/storag_type_width/narrow_range.\n// This is entry point to the Quant dialect and used for both quantizing\n// activations and weights.\nType GetQuantizedType(Builder builder, Type input_type, ArrayRef<double> min,\n                      ArrayRef<double> max, int quant_dim,\n                      int storage_type_width, bool narrow_range, bool is_signed,\n                      bool legacy_float_scale, bool use_fake_quant_num_bits) {\n  auto converter =\n      quant::ExpressedToQuantizedConverter::forInputType(input_type);\n\n  // Expand the range to prevent extremely small scales and large quantized\n  // integers which can cause overflow. This leads to scale\n  // 7.843137254901961e-9 with 8 bits.\n  SmallVector<double, 4> effective_mins, effective_maxs;\n  ExpandVerySmallRange(min, max, &effective_mins, &effective_maxs);\n\n  quant::QuantizedType quantizedEleType;\n  if (min.size() == 1 && max.size() == 1 && quant_dim == -1) {\n    quantizedEleType = quant::fakeQuantAttrsToType(\n        builder.getUnknownLoc(), storage_type_width, effective_mins[0],\n        effective_maxs[0], narrow_range, converter.expressedType, is_signed);\n    if (legacy_float_scale) {\n      quantizedEleType =\n          DownCastScale(quantizedEleType, effective_mins[0], effective_maxs[0],\n                        builder.getUnknownLoc());\n    }\n  } else if (min.size() == max.size()) {\n    auto shape = input_type.dyn_cast<ShapedType>();\n    if (!shape || shape.getRank() <= quant_dim ||\n        static_cast<int64_t>(min.size()) != shape.getDimSize(quant_dim)) {\n      return {};\n    }\n    // The quantization dim is set to the last dimension.\n    quantizedEleType = quant::fakeQuantAttrsToType(\n        builder.getUnknownLoc(), storage_type_width, quant_dim, effective_mins,\n        effective_maxs, narrow_range, converter.expressedType, is_signed);\n    if (legacy_float_scale) {\n      quantizedEleType = DownCastScale(quantizedEleType, effective_mins,\n                                       effective_maxs, builder.getUnknownLoc());\n    }\n  }\n  if (!quantizedEleType) return {};\n  // Use fake quant configured bit-widths (only supported for\n  // 1 < num_bits < 8 bits) instead of using 8bit defaults.\n  if (use_fake_quant_num_bits && (storage_type_width > 1) &&\n      (storage_type_width < 8) &&\n      (quantizedEleType.getStorageTypeMax() >\n       QType::getDefaultMinimumForInteger(is_signed, storage_type_width))) {\n    auto resetEleType = ResetMinMaxFromNumBits(\n        quantizedEleType, storage_type_width, narrow_range, is_signed);\n    return converter.convert(resetEleType);\n  }\n  return converter.convert(quantizedEleType);\n}\n\n// TODO(fengliuai): promote this utility method to mlir QuantOps.\nTypeAttr RescaleQuantizedType(Type input, Attribute factor) {\n  auto factor_values = factor.dyn_cast_or_null<DenseFPElementsAttr>();\n  if (!factor_values) return {};\n  auto ele_type = quant::QuantizedType::getQuantizedElementType(input);\n  if (!ele_type) return {};\n  if (auto qtype = ele_type.dyn_cast<quant::UniformQuantizedPerAxisType>()) {\n    ArrayRef<double> scales = qtype.getScales();\n    // Broadcasting hasn't been implemented yet.\n    if (static_cast<int64_t>(scales.size()) != factor_values.getNumElements())\n      return {};\n    SmallVector<double, 4> new_scales;\n    new_scales.reserve(scales.size());\n    auto scales_iter = scales.begin();\n    for (const auto& f : factor_values) {\n      new_scales.push_back(*(scales_iter++) *\n                           std::fabs(FloatAttr::getValueAsDouble(f)));\n    }\n    // We are assuming symmetric quantization.\n    auto new_ele_type = quant::UniformQuantizedPerAxisType::get(\n        qtype.getFlags(), qtype.getStorageType(), qtype.getExpressedType(),\n        new_scales, qtype.getZeroPoints(), qtype.getQuantizedDimension(),\n        qtype.getStorageTypeMin(), qtype.getStorageTypeMax());\n    if (auto new_type = new_ele_type.castFromExpressedType(\n            quant::QuantizedType::castToExpressedType(input))) {\n      return TypeAttr::get(new_type);\n    }\n  }\n  // Currently, we only support per-axis quantized type.\n  return {};\n}\n\nTypeAttr GetQuantizedTypeAttr(Builder builder, Type input_type, Attribute min,\n                              Attribute max, int quant_dim,\n                              IntegerAttr num_bits, BoolAttr narrow_range,\n                              bool is_signed, bool legacy_float_scale,\n                              bool use_fake_quant_num_bits) {\n  SmallVector<double, 4> min_value, max_value;\n  auto mins = min.dyn_cast<DenseFPElementsAttr>();\n  auto maxs = max.dyn_cast<DenseFPElementsAttr>();\n  if (mins && maxs) {\n    min_value.reserve(mins.getNumElements());\n    max_value.reserve(maxs.getNumElements());\n    for (auto it = mins.begin(), e = mins.end(); it != e; ++it) {\n      min_value.push_back(FloatAttr::getValueAsDouble(*it));\n    }\n    for (auto it = maxs.begin(), e = maxs.end(); it != e; ++it) {\n      max_value.push_back(FloatAttr::getValueAsDouble(*it));\n    }\n  } else {\n    auto fmin = min.dyn_cast<FloatAttr>();\n    auto fmax = max.dyn_cast<FloatAttr>();\n    if (fmin && fmax) {\n      min_value.push_back(fmin.getValueAsDouble());\n      max_value.push_back(fmax.getValueAsDouble());\n    } else {\n      return {};\n    }\n  }\n  Type final_type =\n      GetQuantizedType(builder, input_type, min_value, max_value, quant_dim,\n                       num_bits.getInt(), narrow_range.getValue(), is_signed,\n                       legacy_float_scale, use_fake_quant_num_bits);\n  if (!final_type) return {};\n  return TypeAttr::get(final_type);\n}\n\nTypeAttr CastQuantizedTypeAttrFromExpressedType(Builder builder,\n                                                TypeAttr source, Type target,\n                                                int axis) {\n  auto source_type = source.getValue().dyn_cast_or_null<ShapedType>();\n  if (!source_type) return {};\n  auto src_ele_type = source_type.getElementType();\n  auto qtype = src_ele_type.dyn_cast<quant::QuantizedType>();\n\n  // Reset the quantization dimensions if it is per-axis.\n  if (auto per_axis =\n          qtype.dyn_cast_or_null<quant::UniformQuantizedPerAxisType>()) {\n    qtype =\n        ResetAxisAndBroadcast(source_type.getShape(), per_axis, target, axis);\n  }\n  if (!qtype) return {};\n  Type final_type = qtype.castFromExpressedType(target);\n  if (!final_type) return {};\n  return TypeAttr::get(final_type);\n}\n\nvoid ExtractMinMaxFromAttr(DenseFPElementsAttr values, int dim_size,\n                           int slice_size, bool symmetric,\n                           SmallVectorImpl<double>& mins,\n                           SmallVectorImpl<double>& maxs) {\n  // If all the element values are same we don't need to scan the content.\n  if (values.isSplat()) {\n    double single_value =\n        FloatAttr::getValueAsDouble(values.getSplatValue<llvm::APFloat>());\n\n    // When the single value isn't 0.0, we expand it to a range to include\n    // this single value and 0.0. This will give us a scale and zero point\n    // works for both this value and 0.0.\n    if (single_value < 0.0) {\n      mins[0] = single_value;\n      maxs[0] = symmetric ? -single_value : 0.0;\n    } else if (single_value > 0.0) {\n      mins[0] = symmetric ? -single_value : 0.0;\n      maxs[0] = single_value;\n    } else {\n      mins[0] = maxs[0] = single_value;\n    }\n    for (int i = 1; i < dim_size; ++i) {\n      mins[i] = mins[0];\n      maxs[i] = maxs[0];\n    }\n  } else {\n    int64_t flatten_index = 0;\n    for (auto it = values.begin(), e = values.end(); it != e;\n         ++it, ++flatten_index) {\n      double ele_value = FloatAttr::getValueAsDouble(*it);\n      int slice_index = flatten_index / slice_size;\n      int channel_index = slice_index % dim_size;\n      mins[channel_index] = std::min(mins[channel_index], ele_value);\n      maxs[channel_index] = std::max(maxs[channel_index], ele_value);\n    }\n    // Expand range to include 0.\n    for (int i = 0; i < dim_size; ++i) {\n      maxs[i] = std::max(maxs[i], 0.0);\n      mins[i] = std::min(mins[i], 0.0);\n    }\n    if (symmetric) {\n      for (int i = 0; i < dim_size; ++i) {\n        maxs[i] = std::max(std::abs(mins[i]), std::abs(maxs[i]));\n        mins[i] = -maxs[i];\n      }\n    }\n  }\n}\n\nType GetUniformQuantizedTypeForWeight(ElementsAttr attr, bool symmetric,\n                                      unsigned num_bits, bool is_signed,\n                                      bool narrow_range,\n                                      bool legacy_float_scale,\n                                      bool use_fake_quant_num_bits) {\n  Builder builder(attr.getContext());\n  // `symmetric` can only be used when it is `signed` and `narrow_range`.\n  if (symmetric && (!is_signed || !narrow_range)) return {};\n\n  SmallVector<double, 4> mins(1, std::numeric_limits<double>::max());\n  SmallVector<double, 4> maxs(1, std::numeric_limits<double>::min());\n  auto fp = attr.dyn_cast<DenseFPElementsAttr>();\n  if (!fp) return {};\n\n  // Computes the effective min/max values of the attribute values.\n  ExtractMinMaxFromAttr(fp, /*dim_size=*/1, /*slice_size=*/1, symmetric, mins,\n                        maxs);\n\n  auto type =\n      GetQuantizedType(builder, attr.getType(), mins[0], maxs[0],\n                       /*quant_dim=*/-1, num_bits, narrow_range, is_signed,\n                       legacy_float_scale, use_fake_quant_num_bits);\n  if (auto ele_type = type.dyn_cast_or_null<TensorType>())\n    return ele_type.getElementType();\n\n  return {};\n}\n\nType GetUniformQuantizedPerAxisTypeForWeight(ElementsAttr attr, int quant_dim,\n                                             bool symmetric, unsigned num_bits,\n                                             bool is_signed, bool narrow_range,\n                                             bool legacy_float_scale,\n                                             bool use_fake_quant_num_bits) {\n  Builder builder(attr.getContext());\n  auto shape = attr.getType().cast<ShapedType>().getShape();\n  if (static_cast<int>(shape.size()) <= quant_dim) return {};\n  // `symmetric` can only be used when it is `signed` and `narrow_range`.\n  if (symmetric && (!is_signed || !narrow_range)) return {};\n\n  int dim_size = shape[quant_dim];\n  int slice_size = std::accumulate(std::next(shape.begin(), quant_dim + 1),\n                                   shape.end(), 1, std::multiplies<int64_t>());\n  SmallVector<double, 4> mins(dim_size, std::numeric_limits<double>::max());\n  SmallVector<double, 4> maxs(dim_size, std::numeric_limits<double>::min());\n  auto fp = attr.dyn_cast<DenseFPElementsAttr>();\n  if (!fp) return {};\n\n  // Computes the effective min/max values of the attribute values.\n  ExtractMinMaxFromAttr(fp, dim_size, slice_size, symmetric, mins, maxs);\n\n  auto type = GetQuantizedType(builder, attr.getType(), mins, maxs, quant_dim,\n                               num_bits, narrow_range, is_signed,\n                               legacy_float_scale, use_fake_quant_num_bits);\n  if (auto ele_type = type.dyn_cast_or_null<TensorType>())\n    return ele_type.getElementType();\n\n  return {};\n}\n\nquant::QuantizedType GetUniformQuantizedTypeForBias(\n    const std::vector<quant::QuantizedType>& op_types,\n    bool legacy_float_scale) {\n  if (op_types.empty()) return {};\n\n  size_t axis_size = 1;\n  int32_t quant_dim = -1;\n  Type expressed_type;\n  // Requires all the op types are valid UniformQuantizedTypes or\n  // UniformQuantizedPerAxisTypes and also have same expressed type. For all\n  // the UniformQuantizedPerAxisTypes, the quantization dimension index and\n  // dimension sizes are same.\n  for (auto op_type : op_types) {\n    if (!op_type) return {};\n    if (expressed_type && expressed_type != op_type.getExpressedType()) {\n      return {};\n    }\n    expressed_type = op_type.getExpressedType();\n\n    if (auto type = op_type.dyn_cast<quant::UniformQuantizedPerAxisType>()) {\n      if ((axis_size != 1 && axis_size != type.getScales().size())) return {};\n      if (quant_dim != -1 && quant_dim != type.getQuantizedDimension())\n        return {};\n      axis_size = type.getScales().size();\n      quant_dim = type.getQuantizedDimension();\n    } else if (!op_type.isa<quant::UniformQuantizedType>()) {\n      return {};\n    }\n  }\n\n  // The scale from the UniformQuantizedTypes is broadcasted if there are\n  // UniformQuantizedPerAxisTypes.\n  llvm::SmallVector<double, 4> scales(axis_size, 1.0);\n  for (auto op_type : op_types) {\n    if (auto type = op_type.dyn_cast<quant::UniformQuantizedPerAxisType>()) {\n      for (const auto& index_scale : llvm::enumerate(type.getScales())) {\n        scales[index_scale.index()] *= index_scale.value();\n      }\n    } else if (auto type = op_type.dyn_cast<quant::UniformQuantizedType>()) {\n      for (int index = 0, e = axis_size; index != e; ++index) {\n        scales[index] *= type.getScale();\n      }\n    }\n  }\n  if (legacy_float_scale) {\n    for (int i = 0; i < scales.size(); ++i) {\n      scales[i] = static_cast<float>(scales[i]);\n    }\n  }\n\n  // Builds the result quantized type, which has signed 32 bits storage type.\n  Builder builder(expressed_type.getContext());\n  IntegerType storage_type = builder.getIntegerType(32);\n  int64_t storage_type_min =\n      quant::QuantizedType::getDefaultMinimumForInteger(/*isSigned=*/true, 32);\n  int64_t storage_type_max =\n      quant::QuantizedType::getDefaultMaximumForInteger(/*isSigned=*/true, 32);\n  if (axis_size == 1) {\n    return quant::UniformQuantizedType::getChecked(\n        builder.getUnknownLoc(),\n        /*flags=*/true, storage_type, expressed_type, scales[0],\n        /*zeroPoint=*/0, storage_type_min, storage_type_max);\n  } else {\n    llvm::SmallVector<int64_t, 4> zero_points(axis_size, 0);\n    // Assume the bias is a 1-D tensor, and set the quantization dim to the last\n    // dimension, which is 0. If the bias rank is larger than 1, this returned\n    // quantized type couldn't be used to quantize the bias.\n    return quant::UniformQuantizedPerAxisType::getChecked(\n        builder.getUnknownLoc(),\n        /*flags=*/true, storage_type, expressed_type, scales, zero_points,\n        /*quantizedDimension=*/0, storage_type_min, storage_type_max);\n  }\n}\n\nElementsAttr QuantizeLegacy(Attribute real_value, Type tensor_type) {\n  if (!real_value.isa<DenseFPElementsAttr>() ||\n      !quant::QuantizedType::getQuantizedElementType(tensor_type)) {\n    return {};\n  }\n  auto real_values_attr = real_value.cast<DenseFPElementsAttr>();\n  auto q_type = quant::QuantizedType::getQuantizedElementType(tensor_type);\n  std::vector<float> real_values;\n  llvm::SmallVector<APInt, 8> quantized_attr;\n  real_values.reserve(real_values_attr.getNumElements());\n  quantized_attr.reserve(real_values_attr.getNumElements());\n  std::transform(real_values_attr.begin(), real_values_attr.end(),\n                 std::back_inserter(real_values), [&](APFloat value) -> float {\n                   return value.convertToFloat();\n                 });\n  ShapedType new_dense_type =\n      q_type.castExpressedToStorageType(real_values_attr.getType())\n          .dyn_cast_or_null<ShapedType>();\n  int width = q_type.getStorageType().dyn_cast<mlir::IntegerType>().getWidth();\n\n  if (width == 8 && q_type.getStorageTypeMax() == 127 &&\n      q_type.getStorageTypeMin() == -127) {\n    std::vector<int8_t> quantized_values(real_values_attr.getNumElements());\n    if (auto uniform_type = q_type.dyn_cast<UniformQuantizedType>()) {\n      float min, max, scale;\n      tflite::tensor_utils::SymmetricQuantizeFloats(\n          real_values.data(), real_values.size(), quantized_values.data(), &min,\n          &max, &scale);\n      // The scale has been adjusted, so the adjusted scale should be respected.\n      if (std::abs(scale - uniform_type.getScale()) > 1e-3) {\n        return Quantize(real_value, tensor_type);\n      }\n    } else if (auto uniform_type =\n                   q_type.dyn_cast<UniformQuantizedPerAxisType>()) {\n      std::vector<float> scales_inv;\n      std::vector<int32_t> dimension;\n      dimension.insert(dimension.end(), new_dense_type.getShape().begin(),\n                       new_dense_type.getShape().end());\n      std::transform(uniform_type.getScales().begin(),\n                     uniform_type.getScales().end(),\n                     std::back_inserter(scales_inv),\n                     [](float scale) { return 1.0 / scale; });\n\n      tflite::optimize::utils::SymmetricPerChannelQuantizeValues(\n          real_values.data(), scales_inv, dimension,\n          uniform_type.getQuantizedDimension(), &quantized_values);\n    } else {\n      return {};\n    }\n    std::transform(quantized_values.begin(), quantized_values.end(),\n                   std::back_inserter(quantized_attr),\n                   [&](int8_t value) -> APInt {\n                     return APInt(8, value, /*isSigned=*/true);\n                   });\n    return DenseElementsAttr::get(new_dense_type, quantized_attr);\n  } else if (width == 8) {\n    // This can be a state tensor, or an actual constant tensor with\n    // asymmetric range. For a state tensor, assigining correct quantization\n    // parameters is sufficient, and for constants with asymmetric range it's\n    // not correctly quantized by legacy quantizer so call the new Quantize.\n    return Quantize(real_value, tensor_type);\n  } else if (width == 16) {\n    if (auto uniform_type = q_type.dyn_cast<UniformQuantizedType>()) {\n      auto quantized_values =\n          tflite::optimize::utils::SymmetricQuantizeFloatsToInt16(\n              real_values.data(), real_values.size(), uniform_type.getScale());\n      std::transform(quantized_values.begin(), quantized_values.end(),\n                     std::back_inserter(quantized_attr),\n                     [&](int16_t value) -> APInt {\n                       return APInt(16, value, /*isSigned=*/true);\n                     });\n      return DenseElementsAttr::get(new_dense_type, quantized_attr);\n    }\n  } else if (width == 32) {\n    std::vector<float> scales;\n    if (auto uniform_type = q_type.dyn_cast<UniformQuantizedType>()) {\n      scales.push_back(uniform_type.getScale());\n    } else if (auto uniform_type =\n                   q_type.dyn_cast<UniformQuantizedPerAxisType>()) {\n      scales.insert(scales.end(), uniform_type.getScales().begin(),\n                    uniform_type.getScales().end());\n    } else {\n      return {};\n    }\n    auto quantized_bias =\n        tflite::optimize::utils::SymmetricBiasQuantize<std::int32_t>(\n            real_values.data(), real_values.size(), scales);\n    std::transform(quantized_bias.begin(), quantized_bias.end(),\n                   std::back_inserter(quantized_attr),\n                   [&](int32_t value) -> APInt {\n                     return APInt(32, value, /*isSigned=*/true);\n                   });\n    return DenseElementsAttr::get(new_dense_type, quantized_attr);\n  }\n  return {};\n}\n\nElementsAttr Quantize(Attribute real_value, Type tensor_type) {\n  if (auto q_type =\n          quant::QuantizedType::getQuantizedElementType(tensor_type)) {\n    Type converted_type;\n    return quant::quantizeAttr(real_value, q_type, converted_type)\n        .dyn_cast<ElementsAttr>();\n  }\n  return {};\n}\n\nQuantizedType DownCastScale(QuantizedType type, double min, double max,\n                            Location loc) {\n  SmallVector<double, 1> mins = {min};\n  SmallVector<double, 1> maxs = {max};\n  return DownCastScale(type, mins, maxs, loc);\n}\n\nQuantizedType DownCastScale(QuantizedType type,\n                            const SmallVectorImpl<double>& mins,\n                            const SmallVectorImpl<double>& maxs, Location loc) {\n  // The given type can be null. For example, there can be an invalid scale and\n  // so on.\n  if (!type) return type;\n  SmallVector<double, 4> scales(mins.size());\n  SmallVector<int64_t, 4> zero_points(mins.size());\n  if (auto q_type = type.dyn_cast<UniformQuantizedType>()) {\n    zero_points.push_back(q_type.getZeroPoint());\n  } else if (auto q_type = type.dyn_cast<UniformQuantizedPerAxisType>()) {\n    zero_points = {q_type.getZeroPoints().begin(),\n                   q_type.getZeroPoints().end()};\n  }\n  for (int i = 0; i < mins.size(); ++i) {\n    scales[i] = (static_cast<float>(maxs[i]) - static_cast<float>(mins[i])) /\n                (type.getStorageTypeMax() - type.getStorageTypeMin());\n    if (type.getStorageTypeMax() != -type.getStorageTypeMin()) {\n      // Only applies for asymmetric quantized range with original scale.\n      float zero_point_from_min =\n          type.getStorageTypeMin() - mins[i] / scales[i];\n      if (zero_point_from_min < type.getStorageTypeMin()) {\n        zero_points[i] = static_cast<int64_t>(type.getStorageTypeMin());\n      } else if (zero_point_from_min > type.getStorageTypeMax()) {\n        zero_points[i] = static_cast<int64_t>(type.getStorageTypeMax());\n      } else {\n        zero_points[i] = static_cast<int64_t>(std::round(zero_point_from_min));\n      }\n    }\n  }\n  if (auto q_type = type.dyn_cast<UniformQuantizedType>()) {\n    return UniformQuantizedType::get(q_type.getFlags(), q_type.getStorageType(),\n                                     q_type.getExpressedType(), scales[0],\n                                     zero_points[0], q_type.getStorageTypeMin(),\n                                     q_type.getStorageTypeMax());\n  } else if (auto q_type = type.dyn_cast<UniformQuantizedPerAxisType>()) {\n    return UniformQuantizedPerAxisType::get(\n        q_type.getFlags(), q_type.getStorageType(), q_type.getExpressedType(),\n        scales, zero_points, q_type.getQuantizedDimension(),\n        q_type.getStorageTypeMin(), q_type.getStorageTypeMax());\n  }\n  return type;\n}\n\n// A heuristic to determine whether the scales needs to be from operands or\n// from results for the ops with the `SameOperandsAndResultsScale` property.\n// The current implementation is based on the number of operands.\nstatic bool PreferResultScale(Operation* op) {\n  int float_operands = 0;\n  for (auto operand : op->getOperands()) {\n    if (auto operand_type = operand.getType().dyn_cast<ShapedType>()) {\n      if (operand_type.getElementType().isa<FloatType>()) {\n        if (++float_operands > 1) return true;\n      }\n    }\n  }\n  return false;\n}\n\nstd::unique_ptr<OpQuantScaleSpec> GetDefaultQuantScaleSpec(Operation* op) {\n  auto spec = std::make_unique<OpQuantScaleSpec>();\n  if (llvm::isa<SameScalesOpInterface>(op)) {\n    spec->has_same_scale_requirement = true;\n    spec->required_same_scale_func = [op](bool sign, int bit_width) {\n      return llvm::cast<SameScalesOpInterface>(op)\n          .RequiredSameOperandsAndResultsScale(sign, bit_width);\n    };\n    spec->required_same_quantized_axes_func = [op]() {\n      return llvm::cast<SameScalesOpInterface>(op).RequiredSameQuantizedAxes();\n    };\n  }\n  if (llvm::isa<FixedOutputRangeInterface>(op)) {\n    spec->has_fixed_output_range = true;\n    spec->fixed_output_range_func = [op](bool sign, int bit_width) {\n      return llvm::cast<FixedOutputRangeInterface>(op).GetFixedOutputRange(\n          sign, bit_width);\n    };\n  }\n  return spec;\n}\n\n// The stats op of some of the ops can be redundant. The current implementation\n// only considers the ops with restricted output params.\nstatic bool IsStatsRedundant(\n    Operation* op, OpQuantSpecGetter op_quant_spec_getter,\n    OpQuantScaleSpecGetter op_quant_scale_spec_getter) {\n  // If it has FixedOutputRangeInterface, no need to manually create spec.\n  return llvm::isa<FixedOutputRangeInterface>(op) ||\n         op_quant_scale_spec_getter(op)->has_fixed_output_range;\n}\n\nstatic bool IsSameScaleOp(Operation* op,\n                          OpQuantScaleSpecGetter op_quant_scale_spec_getter) {\n  // If it has SameScalesOpInterface, no need to manually create spec.\n  return llvm::dyn_cast<SameScalesOpInterface>(op) ||\n         op_quant_scale_spec_getter(op)->has_same_scale_requirement;\n}\n\nbool RemoveRedundantStatsOps(\n    mlir::func::FuncOp func, OpQuantSpecGetter op_quant_spec_getter,\n    OpQuantScaleSpecGetter op_quant_scale_spec_getter) {\n  llvm::SmallVector<quant::StatisticsOp, 16> all_stats_ops;\n  llvm::DenseSet<Operation*> redundant_stats_ops;\n\n  // Step 0: remove the quant::StatisticsOp which are used by the quant.qcast\n  // op in case it overrides the information from training FakeQuant ops.\n  func.walk([&](quant::QuantizeCastOp q) {\n    auto input_op = q.getArg().getDefiningOp();\n    if (auto stats = llvm::dyn_cast_or_null<quant::StatisticsOp>(input_op)) {\n      q.setOperand(stats.getArg());\n      if (stats.use_empty()) stats.erase();\n    }\n  });\n\n  // Step 1: forward pass: propagate any value scales which are not produces\n  // by `SameOperandsAndResultsScale`. Additionally, remove the value scales\n  // which are produced by the ops with the `FixedOutputRangeInterface`.\n  // Note that we don't propagate across the multiple-operands\n  // `SameOperandsAndResultsScale` ops like `concatenation`.\n  func.walk(\n      [&](quant::StatisticsOp stats_op) { all_stats_ops.push_back(stats_op); });\n\n  while (!all_stats_ops.empty()) {\n    quant::StatisticsOp stats_op = all_stats_ops.back();\n    all_stats_ops.pop_back();\n\n    if (auto def = stats_op.getArg().getDefiningOp()) {\n      if (IsStatsRedundant(def, op_quant_spec_getter,\n                           op_quant_scale_spec_getter)) {\n        redundant_stats_ops.insert(stats_op);\n      }\n    }\n\n    for (auto user : stats_op.getResult().getUsers()) {\n      // We don't propagate this parameter down if it has multiple operands.\n      // We want to use the result parameter scales instead.\n      if (!IsSameScaleOp(user, op_quant_scale_spec_getter) ||\n          PreferResultScale(user)) {\n        continue;\n      }\n      for (Value res : user->getResults()) {\n        if (!res.hasOneUse()) {\n          continue;\n        }\n        if (auto next_stats =\n                llvm::dyn_cast<quant::StatisticsOp>(*res.getUsers().begin())) {\n          // quantization parameters can be propagated to next_stats\n          redundant_stats_ops.insert(next_stats);\n          // add next_stats to the work list so propagation can continue.\n          all_stats_ops.push_back(next_stats);\n        }\n      }\n    }\n  }\n\n  // Step 2: backward pass: For the ops skiped in the forward pass, propagate\n  // its results scale backwards as far as possible.\n  func.walk([&](quant::StatisticsOp stats_op) {\n    if (redundant_stats_ops.find(stats_op) == redundant_stats_ops.end()) {\n      all_stats_ops.push_back(stats_op);\n    }\n  });\n\n  while (!all_stats_ops.empty()) {\n    quant::StatisticsOp stats_op = all_stats_ops.back();\n    all_stats_ops.pop_back();\n\n    if (auto def = stats_op.getArg().getDefiningOp()) {\n      if (!IsSameScaleOp(def, op_quant_scale_spec_getter)) {\n        continue;\n      }\n      for (auto input : def->getOperands()) {\n        if (auto next_stats = llvm::dyn_cast_or_null<quant::StatisticsOp>(\n                input.getDefiningOp())) {\n          redundant_stats_ops.insert(next_stats);\n          all_stats_ops.push_back(next_stats);\n        }\n      }\n    }\n  }\n\n  // Step3: Remove all the redundant stats ops\n  for (auto it : redundant_stats_ops) {\n    if (!llvm::isa<quant::StatisticsOp>(it)) return true;\n    auto stats_op = llvm::cast<quant::StatisticsOp>(it);\n    stats_op.getResult().replaceAllUsesWith(stats_op.getArg());\n    stats_op.erase();\n  }\n\n  // Returns false if the steps finish without errors.\n  return false;\n}\n\nLogicalResult VerifySameScales(Operation* op) {\n  auto same_scale_op = llvm::cast<SameScalesOpInterface>(op);\n\n  llvm::SmallVector<QuantizedType, 4> collected_quant_params;\n  for (auto input : op->getOperands()) {\n    auto quant_params = QuantizedType::getQuantizedElementType(input.getType());\n    // Skip non-quantizable operands.\n    if (quant_params) {\n      collected_quant_params.push_back(quant_params);\n    }\n  }\n\n  for (auto output : op->getResults()) {\n    auto quant_params =\n        QuantizedType::getQuantizedElementType(output.getType());\n    // Skip non-quantizable results.\n    if (quant_params) {\n      collected_quant_params.push_back(quant_params);\n    }\n  }\n\n  if (collected_quant_params.size() <= 1) return success();\n  const auto& expected_params = collected_quant_params[0];\n  for (int i = 1; i < collected_quant_params.size(); i++) {\n    const auto& compared_params = collected_quant_params[i];\n    // For some ops (such as Transpose or Squeeze), the quantized axis might not\n    // be the same, this function only verifies the scale and zero point in\n    // that case. The quantized axis should be verified in their own verifier\n    // method.\n    if (!same_scale_op.RequiredSameQuantizedAxes()) {\n      auto expected_per_axis_qtype =\n          expected_params.dyn_cast<UniformQuantizedPerAxisType>();\n      auto compared_per_axis_qtype =\n          compared_params.dyn_cast<UniformQuantizedPerAxisType>();\n      if (expected_per_axis_qtype && compared_per_axis_qtype &&\n          llvm::equal(expected_per_axis_qtype.getScales(),\n                      compared_per_axis_qtype.getScales()) &&\n          llvm::equal(expected_per_axis_qtype.getZeroPoints(),\n                      compared_per_axis_qtype.getZeroPoints()) &&\n          expected_params.getStorageType() ==\n              compared_params.getStorageType() &&\n          expected_params.getExpressedType() ==\n              compared_params.getExpressedType()) {\n        continue;\n      }\n    }\n    // Same quantization parameters are always ok.\n    if (expected_params == compared_params) continue;\n    // If the quantization parameters are not the same, as long as it has the\n    // same storage type and the op interface doesn't require same scale\n    // constraint for this storage type, it is still ok.\n    if ((expected_params.isSigned() == compared_params.isSigned() &&\n         expected_params.getStorageTypeIntegralWidth() ==\n             compared_params.getStorageTypeIntegralWidth()) &&\n        !same_scale_op.RequiredSameOperandsAndResultsScale(\n            expected_params.isSigned(),\n            expected_params.getStorageTypeIntegralWidth()))\n      continue;\n\n    std::string err_msg =\n        \"quantization parameters violate the same scale constraint: \";\n    llvm::raw_string_ostream os(err_msg);\n    expected_params.print(os);\n    os << \" vs. \";\n    compared_params.print(os);\n    os.flush();\n    return op->emitOpError(err_msg);\n  }\n  return success();\n}\n\nquant::UniformQuantizedType GetFixedOutputRange(bool is_signed, int bit_width,\n                                                Type tensor_type, double scale,\n                                                int64_t zero_point,\n                                                int64_t storage_min,\n                                                int64_t storage_max) {\n  auto result_type = tensor_type.cast<ShapedType>();\n  if (!result_type.getElementType().isa<FloatType>()) return {};\n  Builder builder(result_type.getContext());\n\n  // Only support 8-bits\n  if (bit_width != 8) return {};\n  IntegerType storage_type = builder.getIntegerType(bit_width);\n  if (!is_signed) {\n    zero_point += 128;\n    storage_min += 128;\n    storage_max += 128;\n  }\n  return quant::UniformQuantizedType::getChecked(\n      builder.getUnknownLoc(), is_signed, storage_type,\n      result_type.getElementType(), scale, zero_point, storage_min,\n      storage_max);\n}\n\nType ConvertSignedQuantizedToUnsigned(Type signed_tensor_type, Location loc) {\n  auto qtype = QType::getQuantizedElementType(signed_tensor_type);\n  if (!qtype || !qtype.isSigned()) return {};\n\n  int num_bits = qtype.getStorageTypeIntegralWidth();\n  // This is a negative value, and will be applied on zero points and fixed\n  // point ranges.\n  int64_t offset =\n      QType::getDefaultMinimumForInteger(/*isSigned=*/true, num_bits) -\n      QType::getDefaultMinimumForInteger(/*isSigned=*/false, num_bits);\n\n  auto flags = !quant::QuantizationFlags::Signed;\n  QType new_qtype;\n  if (auto uqtype = qtype.dyn_cast<quant::UniformQuantizedType>()) {\n    new_qtype = quant::UniformQuantizedType::getChecked(\n        loc, flags, qtype.getStorageType(), qtype.getExpressedType(),\n        uqtype.getScale(), uqtype.getZeroPoint() - offset,\n        uqtype.getStorageTypeMin() - offset,\n        uqtype.getStorageTypeMax() - offset);\n  } else if (auto aqtype =\n                 qtype.dyn_cast<quant::UniformQuantizedPerAxisType>()) {\n    auto zero_points = aqtype.getZeroPoints();\n    llvm::SmallVector<int64_t, 4> new_zero_points(zero_points.begin(),\n                                                  zero_points.end());\n    for (int i = 0, e = new_zero_points.size(); i != e; ++i) {\n      new_zero_points[i] -= offset;\n    }\n    new_qtype = quant::UniformQuantizedPerAxisType::getChecked(\n        loc, flags, qtype.getStorageType(), qtype.getExpressedType(),\n        aqtype.getScales(), new_zero_points, aqtype.getQuantizedDimension(),\n        aqtype.getStorageTypeMin() - offset,\n        aqtype.getStorageTypeMax() - offset);\n  }\n  return new_qtype.castFromExpressedType(\n      QType::castToExpressedType(signed_tensor_type));\n}\n\nLogicalResult RemoveDebugAttrPattern::matchAndRewrite(\n    Operation* op, PatternRewriter& rewriter) const {\n  // removeAttr will return nullptr if the attribute did not exist. Thus we can\n  // return success(result) to indicate if this op has changed.\n  return success(/*isSuccess=*/\n                 op->removeAttr(kDebugModeOpQuantAttrName) ||\n                 op->removeAttr(kDebugModeOpFloatAttrName));\n}\n\n}  // namespace quant\n}  // namespace mlir\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// This transformation pass prepares for legalization to the TFLite dialect by\n// converting operations in TensorFlow dialect into operations that can be\n// legalized to TensorFlow Lite dialect with simple replacements.  The newly\n// created operations are in the TensorFlow dialect if the operation can be\n// represented using a TensorFlow op.  Otherwise, TensorFlow Lite dialect op is\n// used.  For example, Conv2D in TFLite which uses OHWI data format for filters\n// is not supported in TensorFlow because TensorFlow requires filters in the\n// HWIO data format.\n//\n// Motivation to prepare for the TFLite legalization before the actual\n// legalization is to exploit constant folding opportunities in any newly\n// created ops by leveraging constant folding support for the TensorFlow ops.\n// This way TFLite can be used as a serialization format only and does not\n// require access to the TFLite runtime for optimizations as required by the\n// TFLite team.\n\n#include <climits>\n#include <cstdint>\n#include <utility>\n\n#include \"absl/memory/memory.h\"\n#include \"absl/numeric/bits.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/StringSwitch.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/Debug.h\"\n#include \"mlir/Dialect/Affine/Analysis/LoopAnalysis.h\"  // from @llvm-project\n#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"  // from @llvm-project\n#include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/FakeQuantSupport.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/QuantOps.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/UniformSupport.h\"  // from @llvm-project\n#include \"mlir/IR/Attributes.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinOps.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinTypes.h\"  // from @llvm-project\n#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n#include \"mlir/IR/Operation.h\"  // from @llvm-project\n#include \"mlir/Pass/Pass.h\"  // from @llvm-project\n#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n#include \"mlir/Support/LogicalResult.h\"  // from @llvm-project\n#include \"mlir/Transforms/DialectConversion.h\"  // from @llvm-project\n#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"  // from @llvm-project\n#include \"tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/IR/hlo_ops.h\"\n#include \"tensorflow/compiler/mlir/lite/ir/tfl_ops.h\"\n#include \"tensorflow/compiler/mlir/lite/quantization/quantization_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/transforms/dilated_conv.h\"\n#include \"tensorflow/compiler/mlir/lite/transforms/passes.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/attribute_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/constant_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/fake_quant_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/validators.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/transforms/einsum.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/transforms/passes.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/transforms/unroll_batch_matmul.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/utils/verification_utils.h\"\n#include \"tensorflow/compiler/mlir/xla/transforms/passes.h\"\n\n#define DEBUG_TYPE \"tf-tfl-legalization\"\n\nnamespace mlir {\nnamespace TFL {\nnamespace {\n// Returns a TF_CastOp to I32. This function is used for CastOps that are\n// intermediate nodes in a TableGen pattern result. In such a case, the\n// destination type is not inferred and must be given explicitly.\n//\n// Preconditions: The given value must have a ShapedType.\nstatic Value CreateTFCastOpI32(OpBuilder *builder, Location loc, Value x,\n                               BoolAttr truncate) {\n  auto x_type = x.getType().dyn_cast_or_null<ShapedType>();\n  if (!x_type) llvm_unreachable(\"unsupported type\");\n  Type type = x_type.clone(builder->getI32Type());\n  return builder->create<TF::CastOp>(loc, type, x, truncate);\n}\n}  // namespace\n\n//===----------------------------------------------------------------------===//\n// The actual PrepareTF Pass.\n//\n// TODO(hinsu): Add and use TensorFlow dialect ops for the ops created in this\n// pass.\nnamespace {\n#define GEN_PASS_CLASSES\n#include \"tensorflow/compiler/mlir/lite/transforms/passes.h.inc\"\n\n// Prepare TF operations in functions for subsequent legalization.\nclass PrepareTFPass : public PrepareTFPassBase<PrepareTFPass> {\n public:\n  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(PrepareTFPass)\n\n  PrepareTFPass() = default;\n  PrepareTFPass(const PrepareTFPass &) {}\n  explicit PrepareTFPass(bool unfold_batch_matmul,\n                         bool allow_bf16_and_f16_type_legalization,\n                         bool use_fake_quant_num_bits = false) {\n    this->unfold_batch_matmul_ = unfold_batch_matmul;\n    this->allow_bf16_and_f16_type_legalization_ =\n        allow_bf16_and_f16_type_legalization;\n    this->use_fake_quant_num_bits_ = use_fake_quant_num_bits;\n  }\n\n  void runOnOperation() override;\n};\n\n// Transient state for preserving data from match to rewrite\nstruct ConvertTFConvOpMatchState {\n  IntegerAttr dilation_height_factor;\n  IntegerAttr dilation_width_factor;\n  StringAttr padding;\n  IntegerAttr stride_height;\n  IntegerAttr stride_width;\n};\n\n// Templated class for declaring a converter from some TensorFlow convolution\n// op into its counterpart in TensorFlow Lite.\n//\n// The `ConcreteType` deriving from this template must provide the following\n// method for constructing TensorFlow Lite op:\n//\n//   TFL::[op] createTFLOp(ConvertTFConvOpMatchState *state,\n//                         PatternRewriter &rewriter, Location loc,\n//                         Type result_type, Value input,\n//                         Value filter, Value bias) const;\n//\n// And also the following method for getting the dimension for bias tensor:\n//\n//  int64_t getBiasDim(ArrayRef<int64_t> filterShape) const;\ntemplate <typename ConcreteType, typename TFConvOpType>\nclass ConvertTFConvOp : public RewritePattern {\n public:\n  ConvertTFConvOp(MLIRContext *context,\n                  bool allow_bf16_and_f16_type_legalization)\n      : RewritePattern(TFConvOpType::getOperationName(), 1, context),\n        intAttrOne(Builder(context).getI32IntegerAttr(1)),\n        allow_bf16_and_f16_type_legalization_(\n            allow_bf16_and_f16_type_legalization) {}\n\n  LogicalResult matchAndRewrite(Operation *op,\n                                PatternRewriter &rewriter) const override {\n    // Assumes TensorFlow convolution op is already verified to be\n    // in valid form.\n\n    // Match a TFConvOpType under the following conditions:\n    // * The 'T' attribute must exist and be of value DT_FLOAT.\n    // * The 'data_format' attribute must exist and be of value \"NHWC\".\n    // * The 'strides' attribute must exist and is of the form [1, X, Y, 1].\n    // * The 'dilations' attribute is optional, but it must be of the form\n    //   [1, X, Y, 1] if exists.\n\n    TFConvOpType tf_op = cast<TFConvOpType>(op);\n    if (!TFTypeIsFloat32Tensor(tf_op.input()) &&\n        !(allow_bf16_and_f16_type_legalization_ &&\n          TFTypeIsBFloat16OrHalfTensor(tf_op.input())))\n      return failure();\n\n    if (!TFDataFormatIsNHWC(op)) return failure();\n\n    IntegerAttr height, width;\n    if (!TFIntListIs1XY1(op, \"strides\", &height, &width)) return failure();\n\n    ConvertTFConvOpMatchState state;\n    state.stride_height = height;\n    state.stride_width = width;\n\n    if (TFIntListIs1XY1(op, \"dilations\", &height, &width)) {\n      state.dilation_height_factor = height;\n      state.dilation_width_factor = width;\n    } else {\n      // If the 'dilations' attribute is missing, we use the default value (1)\n      // for both dilation height and width factor.\n      state.dilation_height_factor = intAttrOne;\n      state.dilation_width_factor = intAttrOne;\n    }\n\n    TFPaddingIsSameOrValid(op, &state.padding);\n\n    // Additionally, we require the filter operand to be of 4-D tensor type so\n    // that we can extract info from the shape (e.g., for constructing bias\n    // tensor, for setting depth_multiplier attribute, etc.).\n    auto filter = tf_op.filter();\n    auto filter_type = filter.getType().template dyn_cast<RankedTensorType>();\n    if (!filter_type || filter_type.getRank() != 4 ||\n        !filter_type.hasStaticShape())\n      return failure();\n\n    Value input = tf_op.input();\n    RankedTensorType input_type =\n        input.getType().template dyn_cast<RankedTensorType>();\n    // Only rank size four input will be only available by the tf.Conv2D\n    // operator verification.\n    if (!input_type || input_type.isDynamicDim(3)) {\n      return failure();\n    }\n    // Check if the given op is based on grouped convolution.\n    // Dim size zero will be verified by the tf.Conv2D operator verification.\n    if (input_type.getDimSize(3) % filter_type.getDimSize(2) != 0) {\n      return failure();\n    }\n\n    // TensorFlow convolution op only has two inputs, while the TFLite one has\n    // three, with the bias vector marked as optional. However, TOCO has a\n    // dedicated pass, EnsureBiasVectors, to create default bias vectors for all\n    // those missing. So we model TFLite convolution op as requiring three\n    // inputs to achieve the legalization task of EnsureBiasVector. this\n    // requires the filter tensor to have static shape.\n\n    // TODO(antiagainst): also handle the case of tf.Add(tf.[op], <bias>)\n\n    // Get a splat zero tensor with the expected dimension for the bias tensor\n    auto elem_type = filter_type.getElementType();\n    auto bias_dim = static_cast<const ConcreteType *>(this)->getBiasDim(\n        filter_type.getShape());\n    auto bias_type = RankedTensorType::get({bias_dim}, elem_type);\n    auto bias_attr = rewriter.getZeroAttr(bias_type);\n    auto bias =\n        rewriter.create<TF::ConstOp>(op->getLoc(), bias_type, bias_attr);\n\n    if (op->getAttrOfType<StringAttr>(\"padding\").getValue() == \"EXPLICIT\") {\n      // Add Const op for padding value.\n      ArrayRef<Attribute> padding_attr_array =\n          op->getAttrOfType<ArrayAttr>(\"explicit_paddings\").getValue();\n\n      auto get_int = [](Attribute attr) {\n        return attr.template cast<IntegerAttr>().getInt();\n      };\n\n      SmallVector<int32_t> padding_values(padding_attr_array.size());\n      for (int i = 0; i < padding_attr_array.size(); i++) {\n        padding_values[i] =\n            static_cast<int32_t>(get_int(padding_attr_array[i]));\n      }\n\n      RankedTensorType padding_attr_type = RankedTensorType::get(\n          {filter_type.getRank(), 2}, rewriter.getIntegerType(32));\n      auto padding_attr =\n          mlir::DenseIntElementsAttr::get(padding_attr_type, padding_values);\n\n      auto padding_const =\n          rewriter.create<TF::ConstOp>(op->getLoc(), padding_attr);\n\n      // Add Pad op.\n      auto pad_output_type = UnrankedTensorType::get(elem_type);\n      input = rewriter.create<TF::PadOp>(op->getLoc(), pad_output_type, input,\n                                         padding_const);\n\n      // Set Conv padding to `VALID` since padding has been handled by Pad op.\n      state.padding = rewriter.getStringAttr(\"VALID\");\n    }\n    auto conv_op = static_cast<const ConcreteType *>(this)->createTFLOp(\n        &state, rewriter, op->getLoc(), tf_op.getType(), input, filter, bias);\n\n    rewriter.replaceOp(op, conv_op.getResult());\n    return success();\n  }\n\n  const IntegerAttr intAttrOne;\n\n private:\n  bool allow_bf16_and_f16_type_legalization_;\n};\n\nclass ConvertTFConv2D : public ConvertTFConvOp<ConvertTFConv2D, TF::Conv2DOp> {\n public:\n  using BaseType = ConvertTFConvOp<ConvertTFConv2D, TF::Conv2DOp>;\n\n  ConvertTFConv2D(MLIRContext *context, bool allow_bf16_type_legalization)\n      : BaseType(context, allow_bf16_type_legalization) {}\n\n  int64_t getBiasDim(ArrayRef<int64_t> filterShape) const {\n    return filterShape.back();\n  }\n\n  TFL::Conv2DOp createTFLOp(ConvertTFConvOpMatchState *state,\n                            PatternRewriter &rewriter, Location loc,\n                            Type result_type, Value input, Value filter,\n                            Value bias) const {\n    filter = legalizeFilter(rewriter, loc, filter);\n    return rewriter.create<TFL::Conv2DOp>(\n        loc, result_type, input, filter, bias,\n        /*dilation_h_factor=*/state->dilation_height_factor,\n        /*dilation_w_factor=*/state->dilation_width_factor,\n        /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n        /*padding=*/state->padding,\n        /*stride_h=*/state->stride_height,\n        /*stride_w=*/state->stride_width);\n  }\n\n private:\n  // Legalize the given filter by converting it from TensorFlow filter data\n  // format HWIO to TFLite Conv2D op filter data format OHWI and return Value\n  // for the converted filter.  Requires that filter is verified by the match\n  // method that it is a 4-D RankedTensorType.\n  Value legalizeFilter(PatternRewriter &rewriter, Location loc,\n                       Value filter) const {\n    // Create a constant op for HWIO to OHWI transpose permutation.\n    SmallVector<int, 4> perm = {3, 0, 1, 2};\n    auto perm_type = RankedTensorType::get({static_cast<int>(perm.size())},\n                                           rewriter.getIntegerType(32));\n    auto perm_attr =\n        DenseElementsAttr::get(perm_type, llvm::makeArrayRef<int>(perm));\n    auto perm_op = rewriter.create<TF::ConstOp>(loc, perm_type, perm_attr);\n\n    // Create tensor type for the transpose result.\n    auto filter_type = filter.getType().cast<RankedTensorType>();\n    auto result_shape =\n        llvm::to_vector<4>(llvm::map_range(perm, [filter_type](int64_t dim) {\n          return filter_type.getDimSize(dim);\n        }));\n    auto elem_type = filter_type.getElementType();\n    auto result_type = RankedTensorType::get(result_shape, elem_type);\n\n    return rewriter.create<TF::TransposeOp>(loc, result_type, filter, perm_op);\n  }\n};\n\nclass ConvertTFDepthwiseConv2dNative\n    : public ConvertTFConvOp<ConvertTFDepthwiseConv2dNative,\n                             TF::DepthwiseConv2dNativeOp> {\n public:\n  using BaseType = ConvertTFConvOp<ConvertTFDepthwiseConv2dNative,\n                                   TF::DepthwiseConv2dNativeOp>;\n\n  ConvertTFDepthwiseConv2dNative(MLIRContext *context,\n                                 bool allow_bf16_type_legalization)\n      : BaseType(context, allow_bf16_type_legalization) {}\n\n  int64_t getBiasDim(ArrayRef<int64_t> filterShape) const {\n    return filterShape[2] * filterShape[3];\n  }\n\n  TFL::DepthwiseConv2DOp createTFLOp(ConvertTFConvOpMatchState *state,\n                                     PatternRewriter &rewriter, Location loc,\n                                     Type result_type, Value input,\n                                     Value filter, Value bias) const {\n    // Compared to tfl.conv_2d, tfl.depthwise_conv_2d has an additional\n    // 'depth_multiplier' attribute. However, tf.DepthwiseConv2dNative does not\n    // have a corresponding 'depth_multiplier' attribute; the multiplier is the\n    // fourth dimension in the 4-D filter tensor. We query the multiplier from\n    // tf.DepthwiseConv2dNative and set it as the attribute value accordingly.\n    auto multiplier = filter.getType().cast<RankedTensorType>().getDimSize(3);\n\n    filter = legalizeFilter(rewriter, loc, filter);\n    return rewriter.create<TFL::DepthwiseConv2DOp>(\n        loc, result_type, input, filter, bias,\n        /*dilation_h_factor=*/state->dilation_height_factor,\n        /*dilation_w_factor=*/state->dilation_width_factor,\n        /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n        /*padding=*/state->padding,\n        /*stride_h=*/state->stride_height,\n        /*stride_w=*/state->stride_width,\n        /*depth_multiplier=*/rewriter.getI32IntegerAttr(multiplier));\n  }\n\n private:\n  /// Legalize the given filter by converting it from TensorFlow filter data\n  /// format to TFLite DepthwiseConv2D op filter data format and return Value\n  /// for the converted filter.  TensorFlow filter data format is\n  /// [filter_height, filter_width, in_channels, channel_multiplier] and TFLite\n  /// filter data format is [1, filter_height, filter_width, out_channels].\n  /// Requires that filter is verified by the match method that it is a 4-D\n  /// RankedTensorType.\n  Value legalizeFilter(PatternRewriter &rewriter, Location loc,\n                       Value filter) const {\n    auto filter_type = filter.getType().cast<RankedTensorType>();\n    auto filterShape = filter_type.getShape();\n    SmallVector<int64_t, 4> result_shape = {1, filterShape[0], filterShape[1],\n                                            filterShape[2] * filterShape[3]};\n    auto elem_type = filter_type.getElementType();\n    auto result_type = RankedTensorType::get(result_shape, elem_type);\n    // TensorFlow Lite `Reshape` op only support int32 shape tensor currently.\n    auto shape_type = RankedTensorType::get({4}, rewriter.getIntegerType(32));\n    SmallVector<Attribute, 4> result_shape_data(4);\n    for (int i = 0; i < 4; ++i) {\n      result_shape_data[i] =\n          rewriter.getI32IntegerAttr(static_cast<int32_t>(result_shape[i]));\n    }\n    auto shape_attr = DenseElementsAttr::get(shape_type, result_shape_data);\n    auto shape = rewriter.create<TF::ConstOp>(loc, shape_type, shape_attr);\n\n    return rewriter.create<TF::ReshapeOp>(loc, result_type, filter, shape);\n  }\n};\n\n// StridedSlice can have complicated attributes like begin_axis_mask,\n// end_axis_mask, ellipsis_axis_mask, new_axis_mask, shrink_axis_mask. These\n// masks will complicate the strided_slice computation logic, we can simplify\n// the logic by inserting a reshape op to pad the inputs so strided_slice can\n// be easier to handle.\n//\n// So the graph may looks like below:\n//   original_input -> strided_slice -> output\n//      (transforms)\n//   original_input -> reshape -> strided_slice -> output\n//\n// And the new shape is computed based on the masks.\n//\n// An example for new_axis_mask. say the new_axis_mask is 9 which represents\n// [1 0 0 1], and that means we're inserting two new axes at 0 & 3 dim, so\n// if original shape is [2, 3], now we reshape that into [1, 2, 3, 1].\nstruct ConvertTFStridedSlice : public RewritePattern {\n  explicit ConvertTFStridedSlice(MLIRContext *context)\n      : RewritePattern(TF::StridedSliceOp::getOperationName(), 2, context) {}\n\n  LogicalResult RewriteNewAxisMask(Operation *op,\n                                   PatternRewriter &rewriter) const {\n    TF::StridedSliceOp strided_slice_op = llvm::cast<TF::StridedSliceOp>(op);\n    uint64_t new_axis_mask = strided_slice_op.new_axis_mask();\n\n    if (strided_slice_op.ellipsis_mask() != 0) {\n      // Ellipsis mask should have been lowered-away prior to invoking this\n      // function.\n      op->emitError() << \"encountered a logical error\";\n      return failure();\n    }\n\n    // Insert a new reshape op.\n    Value original_input = strided_slice_op.input();\n    RankedTensorType original_input_type =\n        original_input.getType().dyn_cast<RankedTensorType>();\n    if (!original_input_type) {\n      return failure();\n    }\n\n    const ArrayRef<int64_t> &original_input_shape =\n        original_input_type.getShape();\n    SmallVector<int64_t, 4> revised_shape;\n    int index = 0;\n    const int original_input_rank = original_input_shape.size();\n    while (index < original_input_rank || new_axis_mask) {\n      if (new_axis_mask & 1) {\n        revised_shape.emplace_back(1);\n      } else {\n        revised_shape.emplace_back(original_input_shape[index++]);\n      }\n      new_axis_mask >>= 1;\n    }\n\n    if (failed(TF::VerifyShapeOfReshapeOp(revised_shape))) return failure();\n\n    const int dim_size = revised_shape.size();\n    Location loc = strided_slice_op.getLoc();\n    auto shape_type =\n        RankedTensorType::get({dim_size}, rewriter.getIntegerType(32));\n    SmallVector<Attribute, 4> result_shape_data(dim_size);\n    for (int i = 0; i < dim_size; ++i) {\n      result_shape_data[i] =\n          rewriter.getI32IntegerAttr(static_cast<int32_t>(revised_shape[i]));\n    }\n\n    auto shape_attr = DenseElementsAttr::get(shape_type, result_shape_data);\n    auto shape =\n        rewriter.create<arith::ConstantOp>(loc, shape_type, shape_attr);\n    auto revised_output_type = RankedTensorType::get(\n        revised_shape, original_input_type.getElementType());\n    TF::ReshapeOp reshape = rewriter.create<TF::ReshapeOp>(\n        loc, revised_output_type, original_input, shape);\n\n    // Replace the original strided_slice.\n    uint64_t revised_begin_mask = strided_slice_op.begin_mask();\n    uint64_t revised_end_mask = strided_slice_op.end_mask();\n    // Since we expand the dims, we need to apply them to the begin_mask &\n    // end_mask.\n    revised_begin_mask |= strided_slice_op.new_axis_mask();\n    revised_end_mask |= strided_slice_op.new_axis_mask();\n\n    // Enforce operator precedence.\n    uint64_t revised_shrink_axis_mask =\n        strided_slice_op.shrink_axis_mask() & ~strided_slice_op.new_axis_mask();\n\n    auto attribute_type = rewriter.getIntegerType(64);\n    rewriter.replaceOpWithNewOp<TF::StridedSliceOp>(\n        op, strided_slice_op.getType(), reshape, strided_slice_op.begin(),\n        strided_slice_op.end(), strided_slice_op.strides(),\n        rewriter.getIntegerAttr(attribute_type, revised_begin_mask),\n        rewriter.getIntegerAttr(attribute_type, revised_end_mask),\n        rewriter.getIntegerAttr(attribute_type,\n                                strided_slice_op.ellipsis_mask()),\n        rewriter.getI64IntegerAttr(0),\n        rewriter.getIntegerAttr(attribute_type, revised_shrink_axis_mask));\n    return success();\n  }\n\n  LogicalResult RewriteEllipsisMask(Operation *op,\n                                    PatternRewriter &rewriter) const {\n    TF::StridedSliceOp strided_slice_op = llvm::cast<TF::StridedSliceOp>(op);\n\n    uint64_t ellipsis_mask = strided_slice_op.ellipsis_mask();\n    uint64_t shrink_axis_mask = strided_slice_op.shrink_axis_mask();\n    uint64_t new_axis_mask = strided_slice_op.new_axis_mask();\n\n    // Enforce operator precedence.\n    shrink_axis_mask &= ~ellipsis_mask;\n    new_axis_mask &= ~ellipsis_mask;\n\n    DenseIntElementsAttr begin_dense_elem_attr;\n    Value begin = strided_slice_op.begin();\n    auto begin_ranked_attr_type = begin.getType().dyn_cast<RankedTensorType>();\n    if (!begin_ranked_attr_type ||\n        !matchPattern(begin, m_Constant(&begin_dense_elem_attr))) {\n      return failure();\n    }\n\n    DenseIntElementsAttr end_dense_elem_attr;\n    Value end = strided_slice_op.end();\n    auto end_ranked_attr_type = end.getType().dyn_cast<RankedTensorType>();\n    if (!end_ranked_attr_type ||\n        !matchPattern(end, m_Constant(&end_dense_elem_attr))) {\n      return failure();\n    }\n\n    DenseIntElementsAttr stride_dense_elem_attr;\n    Value stride = strided_slice_op.strides();\n    auto stride_ranked_attr_type =\n        stride.getType().dyn_cast<RankedTensorType>();\n    if (!stride_ranked_attr_type ||\n        !matchPattern(stride, m_Constant(&stride_dense_elem_attr))) {\n      return failure();\n    }\n\n    Value input = strided_slice_op.input();\n    RankedTensorType input_type = input.getType().dyn_cast<RankedTensorType>();\n    if (!input_type) {\n      return failure();\n    }\n    const ArrayRef<int64_t> input_shape = input_type.getShape();\n\n    const int input_size = input_shape.size();\n\n    RankedTensorType begin_type = begin.getType().cast<RankedTensorType>();\n    const ArrayRef<int64_t> begin_shape = begin_type.getShape();\n    const int begin_dim = begin_shape.size();\n\n    if (begin_dim != 1) return failure();\n\n    // The ellipsis fill might exceed the current output shape because we are\n    // also taking account of any to-be-inserted new axes.\n    const int ellipsis_filled_dim_size =\n        input_size - begin_shape[0] + 1 + absl::popcount(new_axis_mask);\n\n    int64_t begin_mask = strided_slice_op.begin_mask();\n    int64_t end_mask = strided_slice_op.end_mask();\n    int64_t revised_begin_mask = 0;\n    int64_t revised_end_mask = 0;\n    int64_t revised_shrink_axis_mask = 0;\n    int64_t revised_new_axis_mask = 0;\n\n    SmallVector<int32_t, 4> padded_begin;\n    SmallVector<int32_t, 4> padded_end;\n    SmallVector<int32_t, 4> padded_stride;\n\n    // Before the ellipsis.\n    int index = 0;\n    int new_index = 0;\n    while (((ellipsis_mask >> index) & 1) == 0) {\n      padded_begin.push_back(begin_dense_elem_attr.getValues<int32_t>()[index]);\n      padded_end.push_back(end_dense_elem_attr.getValues<int32_t>()[index]);\n      padded_stride.push_back(\n          stride_dense_elem_attr.getValues<int32_t>()[index]);\n      if ((begin_mask >> index) & 1) revised_begin_mask |= (1 << new_index);\n      if ((end_mask >> index) & 1) revised_end_mask |= (1 << new_index);\n      if ((shrink_axis_mask >> index) & 1)\n        revised_shrink_axis_mask |= (1 << new_index);\n\n      if ((new_axis_mask >> index) & 1)\n        revised_new_axis_mask |= (1 << new_index);\n\n      ++index;\n      ++new_index;\n    }\n\n    // Ellipsis.\n    for (; new_index < index + ellipsis_filled_dim_size; ++new_index) {\n      revised_begin_mask |= (1 << new_index);\n      revised_end_mask |= (1 << new_index);\n\n      // Mimic the begin/end/strides mask behavior.\n      padded_begin.push_back(0);\n      padded_end.push_back(0);\n      padded_stride.push_back(1);\n    }\n\n    // Account for ellipsis mask.\n    ++index;\n\n    // After the ellipsis.\n    for (; index < begin_shape[0];) {\n      padded_begin.push_back(begin_dense_elem_attr.getValues<int32_t>()[index]);\n      padded_end.push_back(end_dense_elem_attr.getValues<int32_t>()[index]);\n      padded_stride.push_back(\n          stride_dense_elem_attr.getValues<int32_t>()[index]);\n\n      if ((begin_mask >> index) & 1) revised_begin_mask |= (1 << new_index);\n      if ((end_mask >> index) & 1) revised_end_mask |= (1 << new_index);\n      if ((shrink_axis_mask >> index) & 1)\n        revised_shrink_axis_mask |= (1 << new_index);\n      if ((new_axis_mask >> index) & 1)\n        revised_new_axis_mask |= (1 << new_index);\n\n      ++index;\n      ++new_index;\n    }\n\n    auto attribute_type = rewriter.getIntegerType(64);\n\n    int full_dim_count = padded_begin.size();\n    auto type =\n        RankedTensorType::get({full_dim_count}, rewriter.getIntegerType(32));\n\n    auto begin_attr = DenseElementsAttr::get<int32_t>(type, padded_begin);\n    auto begin_op =\n        rewriter.create<arith::ConstantOp>(op->getLoc(), type, begin_attr);\n    auto end_attr = DenseElementsAttr::get<int32_t>(type, padded_end);\n    auto end_op =\n        rewriter.create<arith::ConstantOp>(op->getLoc(), type, end_attr);\n    auto stride_attr = DenseElementsAttr::get<int32_t>(type, padded_stride);\n    auto stride_op =\n        rewriter.create<arith::ConstantOp>(op->getLoc(), type, stride_attr);\n\n    rewriter.replaceOpWithNewOp<TF::StridedSliceOp>(\n        op, strided_slice_op.getType(), input, begin_op.getResult(),\n        end_op.getResult(), stride_op.getResult(),\n        rewriter.getIntegerAttr(attribute_type, revised_begin_mask),\n        rewriter.getIntegerAttr(attribute_type, revised_end_mask),\n        /*ellipsis_mask=*/rewriter.getI64IntegerAttr(0),\n        rewriter.getIntegerAttr(attribute_type, revised_new_axis_mask),\n        rewriter.getIntegerAttr(attribute_type, revised_shrink_axis_mask));\n\n    return success();\n  }\n\n  void PadStridedSliceAttributeArray(DenseIntElementsAttr dense_elem_attr,\n                                     SmallVectorImpl<int32_t> &val,\n                                     SmallVectorImpl<int32_t> &padded_val,\n                                     ArrayRef<int32_t> padding_val,\n                                     int *mask) const {\n    for (const auto &idx : dense_elem_attr.getValues<APInt>()) {\n      val.push_back(idx.getSExtValue());\n      padded_val.push_back(idx.getSExtValue());\n    }\n    int attr_dim_count = val.size();\n    int full_dim_count = padding_val.size();\n    for (int i = attr_dim_count; i < full_dim_count; ++i) {\n      padded_val.push_back(padding_val[i]);\n      if (mask) *mask |= 1 << i;\n    }\n  }\n\n  LogicalResult matchAndRewrite(Operation *op,\n                                PatternRewriter &rewriter) const override {\n    TF::StridedSliceOp strided_slice_op = llvm::cast<TF::StridedSliceOp>(op);\n\n    // Handle ellipsis mask.\n    if (strided_slice_op.ellipsis_mask() != 0) {\n      return RewriteEllipsisMask(strided_slice_op, rewriter);\n    }\n\n    // Handle new axis mask.\n    if (strided_slice_op.new_axis_mask() != 0) {\n      return RewriteNewAxisMask(strided_slice_op, rewriter);\n    }\n\n    auto ranked_input_type =\n        strided_slice_op.input().getType().dyn_cast<RankedTensorType>();\n    if (!ranked_input_type) {\n      return failure();\n    }\n\n    auto begin_attr = strided_slice_op.begin();\n    auto end_attr = strided_slice_op.end();\n    auto strides_attr = strided_slice_op.strides();\n\n    auto begin_attr_type = begin_attr.getType().dyn_cast<RankedTensorType>();\n    auto end_attr_type = end_attr.getType().dyn_cast<RankedTensorType>();\n    auto strides_attr_type =\n        strides_attr.getType().dyn_cast<RankedTensorType>();\n\n    DenseIntElementsAttr begin_elem_attr;\n    DenseIntElementsAttr end_elem_attr;\n    DenseIntElementsAttr strides_elem_attr;\n\n    if (!begin_attr_type ||\n        !matchPattern(begin_attr, m_Constant(&begin_elem_attr))) {\n      return failure();\n    }\n    if (!end_attr_type || !matchPattern(end_attr, m_Constant(&end_elem_attr))) {\n      return failure();\n    }\n    if (!strides_attr_type ||\n        !matchPattern(strides_attr, m_Constant(&strides_elem_attr))) {\n      return failure();\n    }\n\n    SmallVector<int32_t, 4> begin, end, strides;\n    SmallVector<int32_t, 4> padded_begin, padded_end, padded_strides;\n\n    int num_input_dims = ranked_input_type.getRank();\n    SmallVector<int32_t, 4> padding_begin(num_input_dims, 0);\n    auto input_shape = ranked_input_type.getShape();\n    SmallVector<int32_t, 4> padding_end(input_shape.begin(), input_shape.end());\n    SmallVector<int32_t, 4> padding_strides(num_input_dims, 1);\n\n    int begin_mask = strided_slice_op.begin_mask();\n    int end_mask = strided_slice_op.end_mask();\n\n    PadStridedSliceAttributeArray(begin_elem_attr, begin, padded_begin,\n                                  padding_begin, &begin_mask);\n    PadStridedSliceAttributeArray(end_elem_attr, end, padded_end, padding_end,\n                                  &end_mask);\n    PadStridedSliceAttributeArray(strides_elem_attr, strides, padded_strides,\n                                  padding_strides, nullptr);\n\n    if (begin == padded_begin && end == padded_end &&\n        strides == padded_strides &&\n        begin_mask == strided_slice_op.begin_mask() &&\n        end_mask == strided_slice_op.end_mask()) {\n      return failure();\n    }\n\n    auto begin_end_type =\n        RankedTensorType::get({num_input_dims}, rewriter.getIntegerType(32));\n    auto new_begin_attr = rewriter.create<arith::ConstantOp>(\n        op->getLoc(), begin_end_type,\n        DenseElementsAttr::get<int32_t>(begin_end_type, padded_begin));\n    auto new_end_attr = rewriter.create<arith::ConstantOp>(\n        op->getLoc(), begin_end_type,\n        DenseElementsAttr::get<int32_t>(begin_end_type, padded_end));\n    auto strides_type =\n        RankedTensorType::get({static_cast<long>(padded_strides.size())},\n                              rewriter.getIntegerType(32));\n    auto new_strides_attr = rewriter.create<arith::ConstantOp>(\n        op->getLoc(), strides_type,\n        DenseElementsAttr::get<int32_t>(strides_type, padded_strides));\n\n    auto attribute_type = rewriter.getIntegerType(64);\n    rewriter.replaceOpWithNewOp<TF::StridedSliceOp>(\n        op, strided_slice_op.output().getType(), strided_slice_op.input(),\n        new_begin_attr, new_end_attr, new_strides_attr,\n        rewriter.getIntegerAttr(attribute_type, begin_mask),\n        rewriter.getIntegerAttr(attribute_type, end_mask),\n        rewriter.getIntegerAttr(attribute_type,\n                                strided_slice_op.ellipsis_mask()),\n        rewriter.getIntegerAttr(attribute_type,\n                                strided_slice_op.new_axis_mask()),\n        rewriter.getIntegerAttr(attribute_type,\n                                strided_slice_op.shrink_axis_mask()));\n\n    return success();\n  }\n};\n\nstruct ConvertTFBroadcastTo : public RewritePattern {\n  explicit ConvertTFBroadcastTo(MLIRContext *context)\n      : RewritePattern(TF::BroadcastToOp::getOperationName(), 1, context) {}\n\n  LogicalResult matchAndRewrite(Operation *op,\n                                PatternRewriter &rewriter) const override {\n    auto tf_broadcast_to_op = cast<TF::BroadcastToOp>(op);\n    auto input_type = tf_broadcast_to_op.input().getType().cast<ShapedType>();\n    auto output_type = tf_broadcast_to_op.output().getType().cast<ShapedType>();\n    auto shape_type = tf_broadcast_to_op.shape().getType().cast<ShapedType>();\n    Type element_type = input_type.getElementType();\n\n    // Allow lowering when low dimension inputs are given and its type is F32 or\n    // I32.\n    if (!((output_type.hasRank() && output_type.getRank() <= 4) ||\n          (shape_type.hasStaticShape() && shape_type.getRank() == 1 &&\n           shape_type.getDimSize(0) <= 4)))\n      return failure();\n\n    if (!(element_type.isa<BFloat16Type, Float32Type>() ||\n          element_type.isInteger(32) || element_type.isInteger(16)))\n      return failure();\n\n    auto status_or_const_op =\n        CreateConstOpWithSingleValue(&rewriter, op->getLoc(), input_type, 1);\n    if (!status_or_const_op.ok()) {\n      return failure();\n    }\n\n    auto tf_fill_op = rewriter.create<TF::FillOp>(\n        op->getLoc(), output_type, tf_broadcast_to_op.shape(),\n        status_or_const_op.ValueOrDie());\n\n    auto mul_op = rewriter.create<TF::MulOp>(\n        op->getLoc(), output_type, tf_broadcast_to_op.input(), tf_fill_op);\n    rewriter.replaceOp(op, mul_op.getResult());\n    return success();\n  }\n};\n\n// The below pattern is equivalent to the DRR rule below\n// The checks are dependent on generated values, so we can't add\n// the checks on intermediate values, ideally we should find equivalent\n// checks that guarantees the resultant ops are valid.\n// The extra conditions are the broadcasting conditions.\n//\n// The pattern lower FusedBatchNormV3 to arithmetic ops.\n// Specifically, performs the following calculation:\n//\n//   (x - mean) * scale / sqrt(variance + epsilon) + offset\n//\n// Let multiplier = scale / sqrt(variance + epsilon),\n// to compute\n//   (x - mean) * scale / sqrt(variance + epsilon) + offset,\n// is then to compute\n//   (x * multiplier) + (offset - mean * multiplier).\n//\n// def : Pattern<\n//     (TF_FusedBatchNormV3Op:$root\n//         $x, $scale, $offset, $mean, $variance,\n//         F32Attr:$epsilon, $exponential_avg_factor,\n//         $data_format, FalseBoolAttr:$is_training),\n//     [(TF_AddOp\n//         (TF_MulOp\n//             $x,\n//             (TF_MulOp:$multiplier\n//                 $scale,\n//                 (TF_RsqrtOp\n//                     (TF_AddOp $variance,\n//                               (TF_ConstOp $epsilon))))),\n//         (TF_SubOp $offset, (TF_MulOp $mean, $multiplier))),\n//    // We already guaranteed that the last five results have no use so it does\n//    // not matter what value we provide here for replacement.\n//      /*batch_mean=*/(replaceWithValue $x),\n//      /*batch_variance=*/(replaceWithValue $x),\n//      /*reserve_space_1=*/(replaceWithValue $x),\n//      /*reserve_space_2=*/(replaceWithValue $x),\n//      /*reserve_space_3=*/(replaceWithValue $x)],\n//     [(HasNoUseOf:$root__1), (HasNoUseOf:$root__2),\n//      (HasNoUseOf:$root__3), (HasNoUseOf:$root__4),\n//      (HasNoUseOf:$root__5), (AreBroadcastableTypes $multiplier, $x)]>;\n//\n// When is_training is set to true, the given variance and mean are not used.\n// In above calculation, they are replaced by new values. These new mean and\n// variance are calculated as following:\n// new_mean = mean(x, axis=[0, 1, 2])\n// new_variance = mean(squared_difference(x, new_mean), axis=[0, 1, 2])\n//\n// The DDR rule for the is_training equals true case is as following:\n// def : Pattern<\n//     (TF_FusedBatchNormV3Op:$root\n//         $x, $scale, $offset, $mean, $variance,\n//         F32Attr:$epsilon, $exponential_avg_factor,\n//         $data_format, FalseBoolAttr:$is_training),\n//     [(TF_AddOp\n//         (TF_MulOp\n//             $x,\n//             (TF_MulOp:$multiplier\n//                 $scale,\n//                 (TF_RsqrtOp\n//                     (TF_AddOp\n//                         (TF_MeanOp\n//                             (TF_SquaredDifferenceOp $x, $new_mean),\n//                             (TF_ConstOp [0,1,2])),\n//                         (TF_ConstOp $epsilon))))),\n//         (TF_SubOp\n//             $offset,\n//             (TF_MulOp\n//                 (TF_MeanOp $x, (TF_ConstOp [0,1,2])),\n//                 $multiplier))),\n//    // We already guaranteed that the last five results have no use so it does\n//    // not matter what value we provide here for replacement.\n//      /*batch_mean=*/(replaceWithValue $x),\n//      /*batch_variance=*/(replaceWithValue $x),\n//      /*reserve_space_1=*/(replaceWithValue $x),\n//      /*reserve_space_2=*/(replaceWithValue $x),\n//      /*reserve_space_3=*/(replaceWithValue $x)],\n//     [(HasNoUseOf:$root__1), (HasNoUseOf:$root__2),\n//      (HasNoUseOf:$root__3), (HasNoUseOf:$root__4),\n//      (HasNoUseOf:$root__5), (AreBroadcastableTypes $multiplier, $x)]>;\n\nstruct FusedBatchNormV3Pat : public ::mlir::RewritePattern {\n  explicit FusedBatchNormV3Pat(::mlir::MLIRContext *context)\n      : ::mlir::RewritePattern(\n            \"tf.FusedBatchNormV3\", 1, context,\n            {\"tf.Add\", \"tf.Const\", \"tf.Mul\", \"tf.Rsqrt\", \"tf.Sub\"}) {}\n\n  ::mlir::LogicalResult matchAndRewrite(\n      ::mlir::Operation *fused_batch_norm,\n      ::mlir::PatternRewriter &rewriter) const override {\n    // Variables for capturing values and attributes used for creating ops\n    Operation::operand_range mean(fused_batch_norm->getOperands());\n    ::mlir::FloatAttr exponential_avg_factor;\n    ::mlir::TF::FusedBatchNormV3Op root;\n    Operation::operand_range offset(fused_batch_norm->getOperands());\n    Operation::operand_range x(fused_batch_norm->getOperands());\n    Operation::operand_range scale(fused_batch_norm->getOperands());\n    Operation::operand_range variance(fused_batch_norm->getOperands());\n    ::mlir::FloatAttr epsilon;\n    ::mlir::BoolAttr is_training;\n\n    // Match\n    auto fused_batch_norm_op =\n        dyn_cast_or_null<::mlir::TF::FusedBatchNormV3Op>(fused_batch_norm);\n    root = fused_batch_norm_op;\n    x = fused_batch_norm_op.getODSOperands(0);\n    scale = fused_batch_norm_op.getODSOperands(1);\n    offset = fused_batch_norm_op.getODSOperands(2);\n    mean = fused_batch_norm_op.getODSOperands(3);\n    variance = fused_batch_norm_op.getODSOperands(4);\n\n    ::mlir::Value mean_value = (*mean.begin());\n    ::mlir::Value variance_value = (*variance.begin());\n\n    if (!TFTypeIsFloat32Tensor(fused_batch_norm_op.x())) return failure();\n\n    {\n      epsilon =\n          fused_batch_norm_op->getAttrOfType<::mlir::FloatAttr>(\"epsilon\");\n      if (!epsilon)\n        epsilon = rewriter.getFloatAttr(rewriter.getF32Type(), 0.0001f);\n\n      if (!(((epsilon.isa<::mlir::FloatAttr>())) &&\n            ((epsilon.cast<::mlir::FloatAttr>().getType().isF32())))) {\n        return rewriter.notifyMatchFailure(\n            fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n              diag << \"op 'tf.FusedBatchNormV3' attribute 'epsilon' failed to \"\n                      \"satisfy constraint: 32-bit float attribute\";\n            });\n      }\n    }\n    {\n      exponential_avg_factor =\n          fused_batch_norm_op->getAttrOfType<::mlir::FloatAttr>(\n              \"exponential_avg_factor\");\n      if (!exponential_avg_factor)\n        exponential_avg_factor =\n            rewriter.getFloatAttr(rewriter.getF32Type(), 1.0f);\n    }\n    if (!TFDataFormatIsNHWC(fused_batch_norm_op) &&\n        !TFDataFormatIsNDHWC(fused_batch_norm_op))\n      return failure();\n\n    if (!(((*root.getODSResults(1).begin()).use_empty()))) {\n      return rewriter.notifyMatchFailure(\n          fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n            diag << \"entities '' failed to satisfy constraint: has no use\";\n          });\n    }\n\n    if (!(((*root.getODSResults(2).begin()).use_empty()))) {\n      return rewriter.notifyMatchFailure(\n          fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n            diag << \"entities '' failed to satisfy constraint: has no use\";\n          });\n    }\n\n    if (!(((*root.getODSResults(3).begin()).use_empty()))) {\n      return rewriter.notifyMatchFailure(\n          fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n            diag << \"entities '' failed to satisfy constraint: has no use\";\n          });\n    }\n\n    if (!(((*root.getODSResults(4).begin()).use_empty()))) {\n      return rewriter.notifyMatchFailure(\n          fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n            diag << \"entities '' failed to satisfy constraint: has no use\";\n          });\n    }\n\n    if (!(((*root.getODSResults(5).begin()).use_empty()))) {\n      return rewriter.notifyMatchFailure(\n          fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n            diag << \"entities '' failed to satisfy constraint: has no use\";\n          });\n    }\n\n    is_training =\n        fused_batch_norm_op->getAttrOfType<::mlir::BoolAttr>(\"is_training\");\n    auto odsLoc = rewriter.getFusedLoc({fused_batch_norm->getLoc()});\n\n    // We need to make sure input and output shapes are compatible.\n    int64_t last_dim = -1;\n    {\n      auto is_last_dim_compatible = [](const Value &v, int64_t &last_dim) {\n        auto v_type = v.getType().dyn_cast_or_null<RankedTensorType>();\n        if (!v_type) return true;\n        int64_t v_last_dim = v_type.getDimSize(v_type.getRank() - 1);\n        if (v_last_dim == -1) return true;\n        if (last_dim != -1 && v_last_dim != last_dim) return false;\n        last_dim = v_last_dim;\n        return true;\n      };\n\n      if (!is_last_dim_compatible(*x.begin(), last_dim) ||\n          !is_last_dim_compatible(*scale.begin(), last_dim) ||\n          !is_last_dim_compatible(*offset.begin(), last_dim)) {\n        return rewriter.notifyMatchFailure(\n            fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n              diag << \"Shapes of scale and offset should be 1D and \"\n                      \"compatible with x\";\n            });\n      }\n\n      if (!is_training.getValue()) {\n        if (!is_last_dim_compatible(mean_value, last_dim) ||\n            !is_last_dim_compatible(variance_value, last_dim)) {\n          return rewriter.notifyMatchFailure(\n              fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n                diag << \"Shapes of mean and variance should be 1D and \"\n                        \"compatible with x\";\n              });\n        }\n      }\n\n      // Check if output shape and input shape are compatible.\n      auto x_type = (*x.begin()).getType();\n      auto y_type = (*root.getODSResults(0).begin()).getType();\n      if (!OpTrait::util::getBroadcastedType(x_type, y_type)) {\n        return rewriter.notifyMatchFailure(\n            fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n              diag << \"Shapes of x and the first output should be compatible\";\n            });\n      }\n    }\n\n    // For training, mean and variance is calculated from input values.\n    if (is_training.getValue()) {\n      auto input_type = fused_batch_norm_op.x()\n                            .getType()\n                            .dyn_cast_or_null<RankedTensorType>();\n      if (!input_type || input_type.getRank() != 4) {\n        return rewriter.notifyMatchFailure(\n            fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n              diag << \"op 'tf.FusedBatchNormV3' that has 'is_training' equals \"\n                      \"True is only supported with input of rank 4\";\n            });\n      }\n\n      ::mlir::TF::ConstOp reduce_dim_op;\n      {\n        auto reduce_dim_type =\n            ::mlir::RankedTensorType::get({3}, rewriter.getIntegerType(32));\n        ::mlir::SmallVector<int32_t, 3> reduce_dim_values = {0, 1, 2};\n        reduce_dim_op = rewriter.create<TF::ConstOp>(\n            odsLoc, ::mlir::DenseIntElementsAttr::get(reduce_dim_type,\n                                                      reduce_dim_values));\n      }\n\n      auto new_mean_type =\n          ::mlir::RankedTensorType::get({last_dim}, rewriter.getF32Type());\n      ::mlir::TF::MeanOp mean_op_1;\n      {\n        ::mlir::Value x_value = (*x.begin());\n        mean_op_1 = rewriter.create<TF::MeanOp>(\n            odsLoc, new_mean_type, x_value, reduce_dim_op,\n            /*keep_dims=*/rewriter.getBoolAttr(false));\n      }\n\n      ::mlir::TF::SquaredDifferenceOp square_diff_op;\n      {\n        ::mlir::Value tblgen_value_0 = (*x.begin());\n        ::mlir::Value tblgen_value_1 = (*mean_op_1.getODSResults(0).begin());\n        // If x has shape of [b, h, w, c], the result of mean_op_1 will have\n        // shape of [c]. Therefore, their shapes are always compatible.\n        square_diff_op = rewriter.create<::mlir::TF::SquaredDifferenceOp>(\n            odsLoc, tblgen_value_0, tblgen_value_1);\n      }\n\n      ::mlir::TF::MeanOp mean_op_2;\n      {\n        ::mlir::Value input_value = (*square_diff_op.getODSResults(0).begin());\n        mean_op_2 = rewriter.create<TF::MeanOp>(\n            odsLoc, new_mean_type, input_value, reduce_dim_op,\n            /*keep_dims=*/rewriter.getBoolAttr(false));\n      }\n\n      mean_value = (*mean_op_1.getODSResults(0).begin());\n      variance_value = (*mean_op_2.getODSResults(0).begin());\n    }  // End is_training equals true if.\n\n    ::llvm::SmallVector<::mlir::Value, 4> replace_values;\n    ::mlir::TF::ConstOp epsilon_const_op;\n    {\n      epsilon_const_op =\n          rewriter.create<::mlir::TF::ConstOp>(odsLoc,\n                                               /*value=*/epsilon);\n    }\n    ::mlir::TF::AddOp add_op_1;\n    {\n      ::mlir::Value epsilon_value =\n          (*epsilon_const_op.getODSResults(0).begin());\n      // Multiplying with a constant, no need to check broadcastibility.\n      add_op_1 = rewriter.create<::mlir::TF::AddOp>(odsLoc,\n                                                    /*x=*/variance_value,\n                                                    /*y=*/epsilon_value);\n    }\n    ::mlir::TF::RsqrtOp rsqrt_op;\n    {\n      ::mlir::SmallVector<::mlir::Value, 4> tblgen_values;\n      ::mlir::SmallVector<::mlir::NamedAttribute, 4> tblgen_attrs;\n      tblgen_values.push_back((*add_op_1.getODSResults(0).begin()));\n      rsqrt_op = rewriter.create<::mlir::TF::RsqrtOp>(odsLoc, tblgen_values,\n                                                      tblgen_attrs);\n    }\n    ::mlir::TF::MulOp multiplier;\n    {\n      ::mlir::Value tblgen_value_0 = (*scale.begin());\n      ::mlir::Value tblgen_value_1 = (*rsqrt_op.getODSResults(0).begin());\n      multiplier = rewriter.create<::mlir::TF::MulOp>(odsLoc,\n                                                      /*x=*/tblgen_value_0,\n                                                      /*y=*/tblgen_value_1);\n    }\n    ::mlir::TF::MulOp mul_op_1;\n    {\n      ::mlir::Value tblgen_value_0 = (*x.begin());\n      ::mlir::Value tblgen_value_1 = (*multiplier.getODSResults(0).begin());\n      mul_op_1 = rewriter.create<::mlir::TF::MulOp>(odsLoc,\n                                                    /*x=*/tblgen_value_0,\n                                                    /*y=*/tblgen_value_1);\n    }\n    ::mlir::TF::MulOp mul_op_2;\n    {\n      ::mlir::Value multiplier_value = (*multiplier.getODSResults(0).begin());\n      mul_op_2 = rewriter.create<::mlir::TF::MulOp>(odsLoc,\n                                                    /*x=*/mean_value,\n                                                    /*y=*/multiplier_value);\n    }\n    ::mlir::TF::SubOp sub_op;\n    {\n      ::mlir::Value tblgen_value_0 = (*offset.begin());\n      ::mlir::Value tblgen_value_1 = (*mul_op_2.getODSResults(0).begin());\n      sub_op = rewriter.create<::mlir::TF::SubOp>(odsLoc,\n                                                  /*x=*/tblgen_value_0,\n                                                  /*y=*/tblgen_value_1);\n    }\n    ::mlir::TF::AddOp add_op_2;\n    {\n      ::mlir::SmallVector<::mlir::Value, 4> tblgen_values;\n      ::mlir::SmallVector<::mlir::NamedAttribute, 4> tblgen_attrs;\n      tblgen_values.push_back((*mul_op_1.getODSResults(0).begin()));\n      tblgen_values.push_back((*sub_op.getODSResults(0).begin()));\n      ::mlir::SmallVector<::mlir::Type, 4> tblgen_types;\n      for (auto v : fused_batch_norm_op.getODSResults(0)) {\n        tblgen_types.push_back(v.getType());\n      }\n      add_op_2 = rewriter.create<::mlir::TF::AddOp>(\n          odsLoc, tblgen_types, tblgen_values, tblgen_attrs);\n    }\n    for (auto v :\n         ::llvm::SmallVector<::mlir::Value, 4>{add_op_2.getODSResults(0)}) {\n      replace_values.push_back(v);\n    }\n    for (auto v : ::llvm::SmallVector<::mlir::Value, 4>{x}) {\n      replace_values.push_back(v);\n    }\n    for (auto v : ::llvm::SmallVector<::mlir::Value, 4>{x}) {\n      replace_values.push_back(v);\n    }\n    for (auto v : ::llvm::SmallVector<::mlir::Value, 4>{x}) {\n      replace_values.push_back(v);\n    }\n    for (auto v : ::llvm::SmallVector<::mlir::Value, 4>{x}) {\n      replace_values.push_back(v);\n    }\n    for (auto v : ::llvm::SmallVector<::mlir::Value, 4>{x}) {\n      replace_values.push_back(v);\n    }\n    rewriter.replaceOp(fused_batch_norm, replace_values);\n    return success();\n  };\n};\n\n#include \"tensorflow/compiler/mlir/lite/transforms/generated_prepare_tf.inc\"\n\n// Returns success if all the operations in the `op`'s regions including `op`\n// itself are legal in a TFLite pipeline.\nLogicalResult ValidateOp(Operation *op) {\n  bool has_illegal_ops = false;\n  op->walk([&](Operation *op) {\n    if (isa<TF::VariableV2Op>(op)) {\n      has_illegal_ops = true;\n      op->emitOpError() << \"is illegal in a TFLite pipeline\";\n    }\n  });\n\n  return failure(has_illegal_ops);\n}\n\n// Converts a set of TF2XLA ops into pure TF ops for future legalizations as\n// TF2XLA ops aren't supported by later stages.\nLogicalResult ConvertTf2XlaOps(func::FuncOp func, MLIRContext *context) {\n  ConversionTarget target(*context);\n  target.addLegalDialect<arith::ArithmeticDialect>();\n  target.addLegalDialect<func::FuncDialect>();\n  target.addLegalDialect<TF::TensorFlowDialect>();\n  target.addLegalOp<ModuleOp>();\n  target.addLegalOp<func::FuncOp>();\n  target.addIllegalOp<TF::XlaConvV2Op>();\n  target.addIllegalOp<TF::XlaGatherOp>();\n\n  RewritePatternSet patterns(context);\n  mhlo::PopulateLegalizeTfWithTf2XlaPatterns(\"XLA_CPU_JIT\", patterns, context);\n  mhlo::PopulateLegalizeTfPatterns(context, &patterns);\n  TF::PopulateLegalizeHloToTfPatterns(&patterns, context);\n  mhlo::GatherOp::getCanonicalizationPatterns(patterns, context);\n\n  return applyPartialConversion(func, target, std::move(patterns));\n}\n\n// Convert rfft to rfft2d.\n// The transformation pattern looks like below:\n//\n//    input     fft_len\n//     \\      /\n//     rfft\n//\n//     ||\n//     \\/\n//\n//   input       fft_len\n//    \\            /\n//   expand_dim    concat with [1] at the front\n//      \\         /\n//     rfft_2d\n//       |\n//     squeeze\nstruct ConvertRfftToRfft2d : public RewritePattern {\n  explicit ConvertRfftToRfft2d(MLIRContext *context)\n      : RewritePattern(TF::RFFTOp::getOperationName(), 1, context) {}\n\n  LogicalResult matchAndRewrite(Operation *op,\n                                PatternRewriter &rewriter) const override {\n    auto rfft_op = dyn_cast<TF::RFFTOp>(op);\n\n    auto input = rfft_op.input();\n    auto input_type = input.getType().dyn_cast_or_null<RankedTensorType>();\n    if (!input_type) return failure();\n    auto fft_len = rfft_op.fft_length();\n    auto fft_len_type = fft_len.getType().dyn_cast_or_null<ShapedType>();\n    if (!fft_len_type) return failure();\n\n    auto output_type =\n        rfft_op.getResult().getType().dyn_cast_or_null<RankedTensorType>();\n    if (!output_type) return failure();\n\n    // Expanded inputs.\n    // Insert at -2 location.\n    auto one_ele_type =\n        mlir::RankedTensorType::get({1}, rewriter.getIntegerType(32));\n    auto minus_two = CreateConstOpWithSingleValue(&rewriter, rfft_op.getLoc(),\n                                                  one_ele_type, -2);\n\n    SmallVector<int64_t, 4> expanded_input_shape;\n    SmallVector<int64_t, 4> expanded_output_shape;\n    int expanded_rank = input_type.getRank() + 1;\n    int r = 0;\n    for (int i = 0; i < expanded_rank; ++i) {\n      if (i == expanded_rank - 2) {\n        expanded_input_shape.push_back(1);\n        expanded_output_shape.push_back(1);\n      } else {\n        expanded_input_shape.push_back(input_type.getDimSize(r));\n        expanded_output_shape.push_back(output_type.getDimSize(r));\n        r++;\n      }\n    }\n\n    auto expaned_input_type = mlir::RankedTensorType::get(\n        expanded_input_shape, input_type.getElementType());\n    TF::ExpandDimsOp expanded_input = rewriter.create<TF::ExpandDimsOp>(\n        rfft_op.getLoc(), expaned_input_type, input, minus_two->getResult());\n\n    // Expanded fft_len.\n    auto one_attr = mlir::DenseIntElementsAttr::get(one_ele_type, {1});\n\n    auto one = rewriter.create<TF::ConstOp>(rfft_op.getLoc(), one_attr);\n\n    auto zero = CreateConstOpWithSingleValue(&rewriter, rfft_op.getLoc(),\n                                             one_ele_type, 0);\n\n    auto expanded_fft_len_type =\n        mlir::RankedTensorType::get({2}, fft_len_type.getElementType());\n\n    TF::ConcatV2Op expanded_fft_len = rewriter.create<TF::ConcatV2Op>(\n        rfft_op.getLoc(), expanded_fft_len_type,\n        SmallVector<Value, 2>({one.getResult(), fft_len}), zero->getResult());\n\n    // Insert the rfft_2d.\n    auto rfft2d_out_type = mlir::RankedTensorType::get(\n        expanded_output_shape, output_type.getElementType());\n    TF::RFFT2DOp rfft2d = rewriter.create<TF::RFFT2DOp>(\n        rfft_op.getLoc(), rfft2d_out_type, expanded_input.getResult(),\n        expanded_fft_len.getResult());\n\n    // Insert the squeeze op.\n    auto squeeze_dim = rewriter.getI64ArrayAttr({-2});\n    TF::SqueezeOp squeeze = rewriter.create<TF::SqueezeOp>(\n        rfft_op.getLoc(), output_type, rfft2d.getResult(), squeeze_dim);\n\n    rewriter.replaceOp(op, squeeze.getResult());\n\n    return success();\n  }\n};\n\n// Replaces the Identity op with its input in either of the following scenarios\n// : 1) The Identity op's input and output have same types/shapes. 2) The result\n// of Identity op is only used by TF ops.\nstruct RemoveIdentity : public OpRewritePattern<TF::IdentityOp> {\n  using OpRewritePattern<TF::IdentityOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TF::IdentityOp identity,\n                                PatternRewriter &rewriter) const override {\n    // Replace the op with the input if input and result have the same type.\n    if (identity.input().getType() == identity.getType()) {\n      rewriter.replaceOp(identity, identity.input());\n      return success();\n    }\n    // Replace the op with the input if output is only used by TF ops.\n    // Currently this is more on the conservative side since we need to ensure\n    // every consumer op to be a TF op before applying this pattern. We can\n    // consider to revisit this in the future if this turns out to be too\n    // restrictive.\n    for (Operation *user : identity->getUsers()) {\n      if (user->getDialect()->getNamespace() != \"tf\") {\n        return failure();\n      }\n    }\n\n    rewriter.replaceOp(identity, identity.input());\n    return success();\n  }\n};\n\nvoid PrepareTFPass::runOnOperation() {\n  MLIRContext *ctx = &getContext();\n  RewritePatternSet patterns(ctx);\n  RewritePatternSet phase_2_patterns(ctx);\n  auto func = getOperation();\n\n  // Check illegal ops in a TFLite pipeline (e.g. trainning only ops) , since\n  // PrepareTFPass is the very first TFLite pass in the pipeline.\n  // TODO(jingpu): It might be better to split this check into its own pass\n  // to make things more modular.\n  if (failed(ValidateOp(func))) {\n    func.emitError() << \"tfl-prepare-tf pass failed.\";\n    signalPassFailure();\n    return;\n  }\n\n  if (failed(ConvertTf2XlaOps(func, ctx))) {\n    signalPassFailure();\n    return;\n  }\n\n  // This pattern will try to identify and optimize for dilated convolution.\n  // e.g. Patterns like \"SpaceToBatchND -> Conv2D -> BatchToSpaceND\" will be\n  // replaced with a single Conv op with dilation parameter.\n  patterns.add<ConvertTFDilatedConvOp<TF::Conv2DOp>, FusedBatchNormV3Pat,\n               ConvertTFDilatedConvOp<TF::DepthwiseConv2dNativeOp>>(ctx);\n\n  patterns.add<RemoveIdentity>(ctx);\n  TFL::populateWithGenerated(patterns);\n  // TODO(karimnosseir): Split to separate pass probably after\n  // deciding on long term plan for this optimization.\n  // This will allow optimizing any TF_Mul->TF_Conv in the graph\n  // and any expanded from FusedBatchNorm. We need to do this\n  // before converting TF_Conv to TFL_Conv\n  (void)applyPatternsAndFoldGreedily(func, std::move(patterns));\n\n  // Remove the wrapper of the tf.FakeQuant* ops and also insert the\n  // tfl.quantize and tfl.dequantize to preserve the quantization parameters.\n  // This is done after the first round of optimization to make sure all the\n  // min/max operands of the tf.FakeQuant* are constants to be matched. The\n  // following round of optimization will folding the unwrapped\n  // tf.FakeQuant* ops with the weight constants.\n  if (failed(ConvertFakeQuantOps(func, ctx, use_fake_quant_num_bits_))) {\n    signalPassFailure();\n    return;\n  }\n\n  // Load the generated pattern again, so new quantization pass-through\n  // will be applied.\n  TFL::populateWithGenerated(phase_2_patterns);\n  if (unfold_batch_matmul_) {\n    TF::PopulateUnrollTfBatchMatMul(ctx, phase_2_patterns);\n  }\n  phase_2_patterns\n      .add<TF::ConvertTFEinsumOp, ConvertTFBroadcastTo, ConvertTFStridedSlice,\n           ConvertRfftToRfft2d, RemoveIdentity>(ctx);\n  phase_2_patterns.add<ConvertTFConv2D, ConvertTFDepthwiseConv2dNative>(\n      ctx, allow_bf16_and_f16_type_legalization_);\n\n  (void)applyPatternsAndFoldGreedily(func, std::move(phase_2_patterns));\n}\n\n}  // namespace\n\n// Creates an instance of the TensorFlow Lite dialect PrepareTF pass.\nstd::unique_ptr<OperationPass<func::FuncOp>> CreatePrepareTFPass(\n    bool unfold_batch_matmul, bool allow_bf16_and_f16_type_legalization,\n    bool use_fake_quant_num_bits) {\n  return std::make_unique<PrepareTFPass>(unfold_batch_matmul,\n                                         allow_bf16_and_f16_type_legalization,\n                                         use_fake_quant_num_bits);\n}\n\n// Creates an instance of the TensorFlow Lite dialect PrepareTF pass.\nstd::unique_ptr<OperationPass<func::FuncOp>> CreatePrepareTFPass() {\n  return std::make_unique<PrepareTFPass>();\n}\n\n}  // namespace TFL\n}  // namespace mlir\n", "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for lite.py functionality related to TensorFlow 2.0.\"\"\"\n\nimport ctypes\nimport functools\nimport itertools\nimport os\nimport sys\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow as tf\n\n# Force loaded shared object symbols to be globally visible. This is needed so\n# that the interpreter_wrapper, in one .so file, can see the test_registerer,\n# in a different .so file. Note that this may already be set by default.\n# pylint: disable=g-import-not-at-top\nif hasattr(sys, 'setdlopenflags') and hasattr(sys, 'getdlopenflags'):\n  sys.setdlopenflags(sys.getdlopenflags() | ctypes.RTLD_GLOBAL)\n\nfrom tensorflow.lite.python import conversion_metadata_schema_py_generated as metadata_fb\nfrom tensorflow.lite.python import convert\nfrom tensorflow.lite.python import lite\nfrom tensorflow.lite.python import lite_v2_test_util\nfrom tensorflow.lite.python import schema_py_generated as schema_fb\nfrom tensorflow.lite.python import test_util as tflite_test_util\nfrom tensorflow.lite.python import util\nfrom tensorflow.lite.python.convert import mlir_quantize\nfrom tensorflow.lite.python.interpreter import Interpreter\nfrom tensorflow.lite.python.interpreter import InterpreterWithCustomOps\nfrom tensorflow.lite.python.interpreter import OpResolverType\nfrom tensorflow.lite.python.testdata import _pywrap_test_registerer as test_registerer\nfrom tensorflow.lite.python.testdata import double_op\nfrom tensorflow.lite.python.util import get_conversion_metadata\nfrom tensorflow.lite.toco import types_pb2 as _types_pb2\nfrom tensorflow.lite.tools.flatbuffer_utils import convert_bytearray_to_object as _convert_bytearray_to_object\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.framework import versions\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow.python.ops import map_ops\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.platform import resource_loader\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.saved_model import save_options\nfrom tensorflow.python.saved_model import saved_model\nfrom tensorflow.python.saved_model.loader_impl import parse_saved_model\nfrom tensorflow.python.saved_model.save import save\nfrom tensorflow.python.trackable import autotrackable\n\n# Only run jax related tests when we can import jax.\nDISABLE_JAX_TEST = False\ntry:\n  import jax\n  from jax import numpy as jnp\nexcept ImportError:\n  DISABLE_JAX_TEST = True\n# pylint: enable=g-import-not-at-top\n\n\nclass FromConcreteFunctionTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testTypeInvalid(self):\n    root = self._getSimpleVariableModel()\n    with self.assertRaises(ValueError) as error:\n      _ = lite.TFLiteConverterV2.from_concrete_functions([root.f], root)\n    self.assertIn('call get_concrete_function', str(error.exception))\n\n  @test_util.run_v2_only\n  def testFloat(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check output value from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @parameterized.named_parameters(('_INT8InputOutput', dtypes.int8),\n                                  ('_UINT8InputOutput', dtypes.uint8),\n                                  ('_INT16InputOutput', dtypes.int16))\n  @test_util.run_v2_only\n  def testInvalidFloat(self, inference_input_output_type):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    with self.assertRaises(ValueError) as error:\n      converter.inference_input_type = inference_input_output_type\n      converter.inference_output_type = inference_input_output_type\n      converter.convert()\n    self.assertEqual(\n        'The inference_input_type and inference_output_type '\n        'must be tf.float32.', str(error.exception))\n\n  @test_util.run_v2_only\n  def testScalarInput(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testModelWithoutInputs(self):\n\n    def _get_random_number_gen():\n      root = autotrackable.AutoTrackable()\n\n      @tf.function(input_signature=[])\n      def func():\n        return tf.random.uniform(shape=[1], dtype=tf.float32)\n\n      root.f = func\n      to_save = root.f.get_concrete_function()\n      return (root, to_save)\n\n    # Model with no input\n    root, concrete_func = _get_random_number_gen()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n  @test_util.run_v2_only\n  def testMultiFunctionModel(self):\n    \"\"\"Convert a single model in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.add.get_concrete_function(input_data)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.add(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testConvertMultipleFunctions(self):\n    \"\"\"Convert multiple functions in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [add_func, sub_func], root)\n    tflite_model = converter.convert()\n\n    # Check signatures are valid from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 2)\n    self.assertEqual(list(signature_defs.keys()), ['add', 'sub'])\n    self.assertEqual(len(signature_defs.values()), 2)\n    self.assertEqual(list(signature_defs['add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['add']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['add']['outputs']), ['output_0'])\n    self.assertEqual(list(signature_defs['sub'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['sub']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['sub']['outputs']), ['output_0'])\n\n    # Verify the Signature runner executions.\n    add_signature_runner = interpreter.get_signature_runner('add')\n    add_output = add_signature_runner(x=input_data)\n    self.assertEqual(add_output['output_0'], 3)\n    input_details = add_signature_runner.get_input_details()\n    self.assertEqual(1, len(input_details))\n    self.assertEqual('add_x:0', input_details['x']['name'])\n    self.assertEqual(np.float32, input_details['x']['dtype'])\n    self.assertTrue(([1] == input_details['x']['shape']).all())\n    self.assertEqual((0.0, 0), input_details['x']['quantization'])\n\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    sub_output = sub_signature_runner(x=input_data)\n    self.assertEqual(sub_output['output_0'], -2)\n    output_details = sub_signature_runner.get_output_details()\n    self.assertEqual(1, len(output_details))\n    self.assertEqual('StatefulPartitionedCall:0',\n                     output_details['output_0']['name'])\n    self.assertEqual(np.float32, output_details['output_0']['dtype'])\n    self.assertTrue(([1] == output_details['output_0']['shape']).all())\n    self.assertEqual((0.0, 0), output_details['output_0']['quantization'])\n\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.TF_CONCRETE_FUNCTIONS)\n    self.assertAllEqual([], metadata.options.modelOptimizationModes)\n\n  def _getIntegerQuantizeModel(self, num_filters=16):\n    np.random.seed(0)\n\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[1, 5, 5, 3], dtype=tf.float32)])\n    def func(inp):\n      conv = tf.nn.conv2d(\n          inp,\n          tf.ones([3, 3, 3, num_filters]), strides=[1, 1, 1, 1], padding='SAME')\n      output = tf.nn.relu(conv, name='output')\n      return output\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32)]\n\n    root.f = func\n    to_save = root.f.get_concrete_function()\n    return (root, to_save, calibration_gen)\n\n  @parameterized.named_parameters(\n      ('EnableMlirQuantizer', True),  # enable mlir quantizer\n      ('DisableMlirQuantizer', False))  # disable mlir quantizer\n  def testPostTrainingCalibrateAndQuantize(self, mlir_quantizer):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_converter.experimental_new_quantizer = mlir_quantizer\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(\n        metadata.environment.tensorflowVersion.decode('utf-8'),\n        versions.__version__)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.TF_CONCRETE_FUNCTIONS)\n    self.assertEqual(metadata.options.allowCustomOps, False)\n    self.assertEqual(metadata.options.enableSelectTfOps, False)\n    self.assertEqual(metadata.options.forceSelectTfOps, False)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER],\n                        metadata.options.modelOptimizationModes)\n\n    # The default input and output types should be float.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @parameterized.named_parameters(('_INT8InputOutput', dtypes.int8),\n                                  ('_UINT8InputOutput', dtypes.uint8),\n                                  ('_INT16InputOutput', dtypes.int16))\n  @test_util.run_v2_only\n  def testInvalidPostTrainingDynamicRangeQuantization(\n      self, inference_input_output_type):\n    root, func, _ = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    with self.assertRaises(ValueError) as error:\n      quantized_converter.inference_input_type = inference_input_output_type\n      quantized_converter.inference_output_type = inference_input_output_type\n      quantized_converter.convert()\n    self.assertEqual(\n        'The inference_input_type and inference_output_type '\n        'must be tf.float32.', str(error.exception))\n\n  def _createV2QATSavedModelWithFloatOpsAtEnd(self):\n    \"\"\"Create a simple QAT SavedModel that includes float ops at the end.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'qat_float_ops_at_end')\n    input_tensor = tf.keras.layers.Input((32, 32, 128))\n    x = tf.quantization.fake_quant_with_min_max_args(input_tensor, -3.0, 3.0)\n    x = tf.keras.layers.Conv2D(1, (3, 3))(x)\n    x = tf.quantization.fake_quant_with_min_max_args(x, -3.0, 3.0)\n    # Exclude the quantization of the following Dense layer by not putting\n    # fake quant layer after the dense layer.\n    output_tensor = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    model = tf.keras.Model(input_tensor, output_tensor)\n    model.save(saved_model_dir)\n    return saved_model_dir\n\n  def testQuantizationRemovesQDQsForFloatIOInQAT(self):\n    saved_model_dir = self._createV2QATSavedModelWithFloatOpsAtEnd()\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_model = converter.convert()\n\n    # Because assertions on the model later, we opt out applying default TFLite\n    # delegates (i.e. the XNNPACK delegate).\n    interpreter = Interpreter(\n        model_content=quantized_model,\n        experimental_op_resolver_type=OpResolverType\n        .BUILTIN_WITHOUT_DEFAULT_DELEGATES)\n    interpreter.allocate_tensors()\n    # The model should have LOGISTIC op, instead of DEQUANTIZE op.\n    op_details = interpreter._get_ops_details()\n    self.assertEqual(op_details[len(op_details) - 1]['op_name'], 'LOGISTIC')\n\n  @parameterized.named_parameters(\n      ('EnableMlirQuantizer', True),  # enable mlir quantizer\n      ('DisableMlirQuantizer', False))  # disable mlir quantizer\n  def testQuantizationRemovesQDQsForFloatIO(self, mlir_quantizer):\n    func, calibration_gen = self._getSqrtModel()\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [func.get_concrete_function()])\n    converter.representative_dataset = calibration_gen\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.experimental_new_quantizer = mlir_quantizer\n    quantized_model = converter.convert()\n\n    # Because assertions on the model later, we opt out applying default TFLite\n    # delegates (i.e. the XNNPACK delegate).\n    interpreter = Interpreter(\n        model_content=quantized_model,\n        experimental_op_resolver_type=OpResolverType\n        .BUILTIN_WITHOUT_DEFAULT_DELEGATES)\n    interpreter.allocate_tensors()\n    # The model should have only one sqrt op.\n    op_details = interpreter._get_ops_details()\n    self.assertLen(op_details, 1)\n    self.assertEqual(op_details[0]['op_name'], 'SQRT')\n\n  @parameterized.named_parameters(\n      ('_Default', False, False, dtypes.float32),\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize', False, True, dtypes.float32),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly', True, False, dtypes.float32),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize', True, True, dtypes.float32),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16))\n  def testIntegerQuantization(self, is_int_only, is_int16_quantize,\n                              inference_input_output_type):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER]\n    if is_int16_quantize:\n      expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_INT16]\n    self.assertAllEqual(expected_opt_options,\n                        metadata.options.modelOptimizationModes)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details[0]['dtype'])\n\n    # Ensure that the quantized tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(tflite_model))\n\n  @parameterized.named_parameters(\n      ('_INT16Quantize_INT8InputOutput', True, dtypes.int8))\n  def testInvalidIntegerQuantization(self, is_int16_quantize,\n                                     inference_input_output_type):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    if is_int16_quantize:\n      quantized_converter.target_spec.supported_ops = [\n          lite.OpsSet.\n          EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n          lite.OpsSet.TFLITE_BUILTINS\n      ]\n    with self.assertRaises(ValueError) as error:\n      quantized_converter.inference_input_type = dtypes.int8\n      quantized_converter.inference_output_type = dtypes.int8\n      quantized_converter.convert()\n    self.assertEqual(\n        'The inference_input_type and inference_output_type '\n        \"must be in ['tf.float32', 'tf.int16'].\", str(error.exception))\n\n  def testCalibrateAndQuantizeBuiltinInt16(self):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    # TODO(b/156309549): We should add INT16 to the builtin types.\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS_INT8]\n    converter.representative_dataset = calibration_gen\n    converter._experimental_calibrate_only = True\n    calibrated_tflite = converter.convert()\n    quantized_tflite_model = mlir_quantize(\n        calibrated_tflite, inference_type=_types_pb2.QUANTIZED_INT16)\n\n    self.assertIsNotNone(quantized_tflite_model)\n\n    # The default input and output types should be float.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @test_util.run_v2_only\n  def testSignatureDefs(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2([add_func], trackable_obj=root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = add_func(input_data)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default', {'x': input_data})\n    self.assertLen(list(results.keys()), 1)\n    self.assertStartsWith(list(results.keys())[0], 'output')\n    self.assertAllClose(\n        expected_value.numpy(),\n        results[signature_defs['serving_default']['outputs'][0]])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['serving_default'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['serving_default'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['serving_default']['inputs'], ['x'])\n    self.assertLen(list(signature_defs['serving_default']['outputs']), 1)\n    self.assertStartsWith(\n        list(signature_defs['serving_default']['outputs'])[0], 'output')\n\n  @test_util.run_v2_only\n  def testNoSignatureDefsWhenTrackingObjIsNone(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               None)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    # Verify that there is no SignatureDef structure found.\n    self.assertEqual(len(signature_defs), 0)\n\n  @test_util.run_v2_only\n  def testNoSignatureDefsWhenInvalidTrackingObjIsGiven(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], trackable_obj=autotrackable.AutoTrackable())\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    # Verify that there is no SignatureDef structure found.\n    self.assertEqual(len(signature_defs), 0)\n\n  @test_util.run_v2_only\n  def testTrackbleObject(self):\n    \"\"\"Test converting with trackable objects.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [add_func], trackable_obj=root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = add_func(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  def _getTrainingTimeQuantizedModel(self):\n\n    class QLinear(tf.keras.layers.Layer):\n\n      def __init__(self, units=3, **kwargs):\n        super(QLinear, self).__init__(**kwargs)\n        self.units = units\n\n      def build(self, input_shape):\n        self.w = self.add_weight(\n            'weight',\n            shape=(input_shape[-1], self.units),\n            initializer='random_normal',\n            trainable=True)\n        self.min_var = self.add_weight(\n            'min',\n            initializer=tf.keras.initializers.Constant(-6.0),\n            trainable=False)\n        self.max_var = self.add_weight(\n            'max',\n            initializer=tf.keras.initializers.Constant(6.0),\n            trainable=False)\n\n      def call(self, inputs):\n        x = tf.quantization.fake_quant_with_min_max_vars(\n            inputs, self.min_var, self.max_var)\n\n        w_fq = tf.quantization.fake_quant_with_min_max_vars(\n            self.w, self.min_var, self.max_var)\n        x = tf.matmul(x, w_fq)\n\n        x = tf.quantization.fake_quant_with_min_max_vars(\n            x, self.min_var, self.max_var)\n\n        return x\n\n    return tf.keras.Sequential(QLinear(3, input_shape=(2,)))\n\n  @parameterized.named_parameters(\n      ('_DefaultFLOAT32InputOutput', dtypes.float32),\n      ('_INT8InputOutput', dtypes.int8), ('_UINT8InputOutput', dtypes.uint8))\n  @test_util.run_v2_only\n  def testTrainingTimeQuantization(self, inference_input_output_type):\n    model = self._getTrainingTimeQuantizedModel()\n\n    float_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n\n    quantized_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual(\n        [metadata_fb.ModelOptimizationMode.QUANTIZATION_AWARE_TRAINING],\n        metadata.options.modelOptimizationModes)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details[0]['dtype'])\n\n    # Ensure that the quantized tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @test_util.run_v2_only\n  def testNewQuantizer(self):\n    \"\"\"Test the model quantized by the new converter.\"\"\"\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS_INT8\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n\n    # default quantizer\n    quantized_converter.experimental_new_quantizer = False\n    old_tflite = quantized_converter.convert()\n\n    # new quantizer\n    quantized_converter.experimental_new_quantizer = True\n    new_tflite = quantized_converter.convert()\n\n    for _ in range(5):\n      input_data = tf.constant(\n          np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32))\n      old_value = self._evaluateTFLiteModel(old_tflite, [input_data])\n      new_value = self._evaluateTFLiteModel(new_tflite, [input_data])\n      self.assertAllClose(old_value, new_value, atol=1e-01)\n\n  @test_util.run_v2_only\n  def testEmbeddings(self):\n    \"\"\"Test model with embeddings.\"\"\"\n    input_data = tf.constant(\n        np.array(np.random.random_sample((20)), dtype=np.int32))\n\n    class EmbeddingModel(tf.keras.Model):\n\n      def __init__(self):\n        super(EmbeddingModel, self).__init__()\n        self.shared_weights = self.add_weight(\n            'weights',\n            shape=(2000, 300),\n            dtype=tf.float32,\n            initializer=tf.random_normal_initializer(\n                mean=0.0, stddev=300**(-0.5)))\n\n      @tf.function(input_signature=[tf.TensorSpec(shape=(20), dtype=tf.int32)])\n      def func(self, x):\n        return tf.gather(self.shared_weights, x)\n\n    # Building the model.\n    root = EmbeddingModel()\n    concrete_func = root.func.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.func(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertAllClose(expected_value.numpy(), actual_value[0], atol=1e-05)\n\n  @test_util.run_v2_only\n  def testGraphDebugInfo(self):\n    \"\"\"Test a concrete function has debug info captured.\"\"\"\n    root = autotrackable.AutoTrackable()\n    root.v1 = tf.Variable(3.)\n    root.f = tf.function(lambda x: root.v1 * x)\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.convert()\n    self._assertValidDebugInfo(converter._debug_info)\n\n  def _getIntegerQuantizationModelWithFlexOp(self):\n    np.random.seed(0)\n\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[3, 3, 3, 3, 3], dtype=tf.float32)\n    ])\n    def func(inp):\n      tanh = tf.math.tanh(inp)\n      # Flex delegate will merge the consecutive conv3d and erf ops into one\n      # Delegate node.\n      conv3d = tf.nn.conv3d(\n          tanh,\n          tf.ones([3, 3, 3, 3, 3]),\n          strides=[1, 1, 1, 1, 1],\n          padding='SAME')\n      erf = tf.math.erf(conv3d)\n      output = tf.math.tanh(erf)\n      return output\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(3, 3, 3, 3, 3)).astype(np.float32)\n        ]\n\n    root.f = func\n    return (root, root.f.get_concrete_function(), calibration_gen)\n\n  @parameterized.named_parameters(\n      ('_Default', False, False, dtypes.float32),\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize', False, True, dtypes.float32),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly', True, False, dtypes.float32),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize', True, True, dtypes.float32),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16))\n  @test_util.run_v2_only\n  def testIntegerQuantizationWithFlexOp(self, is_int_only, is_int16_quantize,\n                                        inference_input_output_type):\n    root, func, calibration_gen = self._getIntegerQuantizationModelWithFlexOp()\n\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.SELECT_TF_OPS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8, lite.OpsSet.SELECT_TF_OPS\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS,\n            lite.OpsSet.SELECT_TF_OPS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS, lite.OpsSet.SELECT_TF_OPS\n        ]\n\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.options.enableSelectTfOps, True)\n    expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER]\n    if is_int16_quantize:\n      expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_INT16]\n    self.assertAllEqual(expected_opt_options,\n                        metadata.options.modelOptimizationModes)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details[0]['dtype'])\n\n  def _getIntegerQuantizationModelWithUnsupportedOps(self):\n    np.random.seed(0)\n\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[3], dtype=tf.float32),\n        tf.TensorSpec(shape=[3], dtype=tf.float32)\n    ])\n    def func(a, b):\n      # ceil kernel does not support int8 nor int16 types neither.\n      left = tf.math.ceil(a)\n      right = tf.nn.tanh(b)\n      add = tf.math.add(left, right)\n      # ceil kernel does not support int8 nor int16 types neither.\n      output = tf.math.ceil(add)\n      return (output, right)\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(3)).astype(np.float32),\n            np.random.uniform(-1, 1, size=(3)).astype(np.float32)\n        ]\n\n    root.f = func\n    return (root, root.f.get_concrete_function(), calibration_gen)\n\n  @parameterized.named_parameters(\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutputMlirQuant', True, False, dtypes.int8, True),\n      ('_IntOnly_UINT8InputOutputMlirQuant', True, False, dtypes.uint8, True))\n  @test_util.run_v2_only\n  def testIntegerQuantizationWithUnsupportedOps(self,\n                                                is_int_only,\n                                                is_int16_quantize,\n                                                inference_input_output_type,\n                                                enable_mlir_quantizer=False):\n    root, func, calib_gen = self._getIntegerQuantizationModelWithUnsupportedOps(\n    )\n\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calib_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8, lite.OpsSet.TFLITE_BUILTINS\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    expected_dtype = inference_input_output_type.as_numpy_dtype\n    # Allow float32 for fallback on non-quantizable op.\n    expected_ceil_dtype = (\n        expected_dtype if enable_mlir_quantizer else dtypes.float32)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 2)\n    self.assertEqual(input_details[0]['dtype'], expected_dtype)\n    self.assertEqual(input_details[1]['dtype'], expected_ceil_dtype)\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 2)\n    self.assertEqual(output_details[0]['dtype'], expected_dtype)\n    self.assertEqual(output_details[1]['dtype'], expected_ceil_dtype)\n\n  def _getIntegerQuantizationModelWithControlFlow(self):\n    def true_fn(x):\n      return x\n\n    def false_fn(x):\n      return x\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, 2], dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.bool)\n    ])\n    def model(x, b):\n      x = x + x\n      x = tf.cond(b, true_fn=lambda: true_fn(x), false_fn=lambda: false_fn(x))\n      return x + x\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(\n                1,\n                2,\n            )).astype(np.float32),\n            tf.constant(True),\n        ]\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(\n                1,\n                2,\n            )).astype(np.float32),\n            tf.constant(False),\n        ]\n\n    return (model, model.get_concrete_function(), calibration_gen)\n\n  @parameterized.named_parameters(\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16),\n      # TODO(b/198231624): Support control flow ops in MLIR quantizer\n      # ('_IntOnly_INT8InputOutputMlirQuant', True, False, dtypes.int8, True),\n      # ('_IntOnly_UINT8InputOutputMlirQuant', True, False, dtypes.uint8, True),\n  )\n  @test_util.run_v2_only\n  def testIntegerQuantizationWithControlFlow(self,\n                                             is_int_only,\n                                             is_int16_quantize,\n                                             inference_input_output_type,\n                                             enable_mlir_quantizer=False):\n    root, func, calib_gen = self._getIntegerQuantizationModelWithControlFlow()\n\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calib_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8, lite.OpsSet.TFLITE_BUILTINS\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    expected_dtype = inference_input_output_type.as_numpy_dtype\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 2)\n    self.assertEqual(input_details[0]['dtype'], expected_dtype)\n    self.assertEqual(input_details[1]['dtype'], dtypes.bool)\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(output_details[0]['dtype'], expected_dtype)\n\n  @parameterized.named_parameters(\n      ('_BlocklistedNoneWithLowering', None, None, True),\n      ('_BlocklistedNoneWithoutLowering', None, None, False),\n      ('_BlocklistedOpsWithLowering', {'CONV_2D'}, None, True),\n      ('_BlocklistedOpsWithoutLowering', {'CONV_2D'}, None, False),\n      ('_BlocklistedNodesWithLowering', None, {'PartitionedCall:0'}, True),\n      ('_BlocklistedNodesWithoutLowering', None, {'Identity'}, False))\n  @test_util.run_v2_only\n  def testNewQuantizerBlocklistingArgs(self, denylisted_ops, denylisted_nodes,\n                                       lower_to_saved_model):\n    \"\"\"Test the model quantized by the new converter and denylisted options.\"\"\"\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS_INT8\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.experimental_new_quantizer = True\n    quantized_converter._experimental_calibrate_only = True\n    quantized_converter.experimental_lower_to_saved_model = lower_to_saved_model\n    calibrated = quantized_converter.convert()\n    quantized_tflite_model = mlir_quantize(\n        calibrated,\n        denylisted_ops=denylisted_ops,\n        denylisted_nodes=denylisted_nodes)\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    details = interpreter.get_tensor_details()\n    num_quantized_tensors = sum(\n        [1 for detail in details\n         if len(detail['quantization_parameters']['scales'])])\n    if denylisted_nodes or denylisted_ops:\n      self.assertEqual(num_quantized_tensors, 0)\n      return\n    self.assertEqual(num_quantized_tensors, 4)  # quant, filter, bias, dequant\n\n  @parameterized.named_parameters(\n      ('_SingleLayer', False),\n      ('_WholeModel', True),\n  )\n  @test_util.run_v2_only\n  def testNewQuantizerNumericVerificationDebugMode(self, whole_model_verify):\n    \"\"\"Test the model quantized by the new converter with numeric verify ops.\"\"\"\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS_INT8\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n\n    # Create a TFLite model with new quantizer.\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.experimental_new_quantizer = True\n    production_tflite = quantized_converter.convert()\n    # Create a TFLite model with new quantizer and numeric verify ops.\n    quantized_converter._experimental_calibrate_only = True\n    calibrated = quantized_converter.convert()\n    debug_mode_tflite = mlir_quantize(\n        calibrated,\n        enable_numeric_verify=True,\n        enable_whole_model_verify=whole_model_verify)\n\n    # Check if adding debug mode should output a different flatbuffer.\n    self.assertNotEqual(production_tflite, debug_mode_tflite)\n\n    # Check if newly added ops are numeric verify ops.\n    input_data = tf.constant(\n        np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32))\n\n    def examine_tflite_model(tflite_content, input_data):\n      interpreter = Interpreter(\n          model_content=tflite_content,\n          experimental_op_resolver_type=OpResolverType\n          .BUILTIN_WITHOUT_DEFAULT_DELEGATES)\n      interpreter.allocate_tensors()\n      input_details = interpreter.get_input_details()\n      interpreter.set_tensor(input_details[0]['index'], input_data.numpy())\n      interpreter.invoke()\n      tensor_details = interpreter.get_tensor_details()\n      return {\n          details['name']: interpreter.get_tensor(details['index'])\n          for details in interpreter.get_tensor_details()\n      }, tensor_details\n\n    tflite_result, _ = examine_tflite_model(production_tflite, input_data)\n    debug_mode_tflite_result, debug_tensor_details = examine_tflite_model(\n        debug_mode_tflite, input_data)\n\n    # MLIR-based quantizer should output flatbuffer model with `tfl.quantize`.\n    num_production_quantize_ops = len([\n        None for output_tensor_name in tflite_result\n        if 'tfl.quantize' in output_tensor_name\n    ])\n    self.assertEqual(num_production_quantize_ops, 1)\n    # MLIR-based quantizer should output flatbuffer model with `tfl.quantize`.\n    num_debug_quantize_ops = len([\n        None for output_tensor_name in debug_mode_tflite_result\n        if 'tfl.quantize' in output_tensor_name\n    ])\n    # Two numbers should be equal.\n    self.assertEqual(num_production_quantize_ops, num_debug_quantize_ops)\n    # DebugMode TFLite flatbuffer should have NumericVerifyOps more than zero.\n    # The name has the prefix \"NumericVerify/{name}:{id}\n    # where {name} is the tensor name of the original quantized op's activation,\n    # and {id} is its tensor id.\n    num_debug_ops = 0\n    for output_tensor_name in debug_mode_tflite_result:\n      if 'NumericVerify' in output_tensor_name:\n        pos_end_prefix = len('NumericVerify/')\n        pos_colon = output_tensor_name.rfind(':')\n        self.assertEqual('NumericVerify/', output_tensor_name[:pos_end_prefix])\n        tensor_id = int(output_tensor_name[pos_colon + 1:])\n        original_tensor_name = output_tensor_name[pos_end_prefix:pos_colon]\n        self.assertEqual(original_tensor_name,\n                         debug_tensor_details[tensor_id]['name'])\n        num_debug_ops += 1\n    self.assertEqual(num_debug_ops, 1)\n    # The number of debug ops should be equal to that of quantized ops.\n    self.assertEqual(num_debug_ops, num_debug_quantize_ops)\n\n  @parameterized.named_parameters(\n      ('_PerChannelQuant', False, False),\n      ('_PerChannelMlirQuant', False, True),\n      ('_PerTensorQuant', True, False),\n      ('_PerTensorMlirQuant', True, True),\n      ('_PerChannelDynamicRange', False, False, False),\n      ('_PerTensorDynamicRange', True, False, False))\n  @test_util.run_v2_only\n  def testDisablePerChannelQuantization(self, disable_per_channel=False,\n                                        enable_mlir_quantizer=False,\n                                        representative_dataset=True):\n    k_conv_name = 'Conv2D'\n    # Dynamic range quant requires total num elements of filters > 1024.\n    k_num_filters = 38\n    root, func, calib_gen = self._getIntegerQuantizeModel(k_num_filters)\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calib_gen\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS\n    ]\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n    if disable_per_channel:\n      quantized_converter._experimental_disable_per_channel = (\n          disable_per_channel)\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    detail = next((d for d in interpreter.get_tensor_details()\n                   if d['name'].startswith(k_conv_name)))\n    quant_params = detail['quantization_parameters']\n    expected_num_params = 1 if disable_per_channel else k_num_filters\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n  @parameterized.named_parameters(('MlirQuantize', True),\n                                  ('TocoQuantize', False))\n  @test_util.run_v2_only\n  def testQuantizeBiasOverflow(self, enable_mlir_quantizer):\n    \"\"\"Tests if the quantizer handles bias overflow by adjusting scales.\"\"\"\n    input_data = np.array([[-1e-3, 1e-3]], dtype=np.float32)\n\n    def calibration_gen():\n      yield {'x': input_data}\n\n    root = self._getMatMulModelWithSmallWeights()\n    input_data = tf.constant([-1e-3, 1e-3], shape=(1, 2))\n    concrete_func = root.matmul.get_concrete_function(input_data)\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.experimental_new_quantizer = enable_mlir_quantizer\n    quantized_model = converter.convert()\n\n    interpreter = Interpreter(model_content=quantized_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    output_details = interpreter.get_output_details()\n    output = interpreter.get_tensor(output_details[0]['index'])\n    # the inputs and weights are far smaller than the biases, so the final\n    # result should be equal to the biases.\n    self.assertAllClose(root.bias, output.flatten())\n\n  @test_util.run_v2_only\n  def testOpVersion(self):\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[5, 5], dtype=tf.float32)])\n    def custom_resize(image):\n      # Add \"batch\" and \"channels\" dimensions\n      image = image[tf.newaxis, ..., tf.newaxis]\n      # ResizeBilinear version 3.\n      resize1 = tf.compat.v1.image.resize_bilinear(\n          image, [2, 2], half_pixel_centers=True)\n      # ResizeBilinear version 1.\n      resize2 = tf.compat.v1.image.resize_bilinear(image, [2, 2])\n      return resize1 + resize2\n\n    concrete_func = custom_resize.get_concrete_function()\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               custom_resize)\n    tflite_model = converter.convert()\n    model_object = schema_fb.Model.GetRootAsModel(tflite_model, 0)\n    model = schema_fb.ModelT.InitFromObj(model_object)\n\n    for operator in model.operatorCodes:\n      if operator.builtinCode == schema_fb.BuiltinOperator.RESIZE_BILINEAR:\n        # half_pixel_centers is supported by ResizeBilinear version 3.\n        self.assertEqual(operator.version, 3)\n        break\n\n  @test_util.run_v2_only\n  def testForceSelectTFOps(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.options.forceSelectTfOps, True)\n\n    # Check output value from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  def testExcludeConversionMetadata(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.exclude_conversion_metadata = True\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNone(metadata)\n\n  def testConversionMetadataForDynamicRange(self):\n    func, _ = self._getSqrtModel()\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [func.get_concrete_function()])\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.PTQ_DYNAMIC_RANGE],\n                        metadata.options.modelOptimizationModes)\n\n  def testConversionMetadataForFloat16(self):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.target_spec.supported_types = [dtypes.float16]\n    quantized_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.PTQ_FLOAT16],\n                        metadata.options.modelOptimizationModes)\n\n\nclass FromSavedModelTest(lite_v2_test_util.ModelTest):\n\n  def _createV1SavedModel(self, shape):\n    \"\"\"Create a simple SavedModel.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_savedmodel')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor_1 = tf.compat.v1.placeholder(\n            shape=shape, dtype=tf.float32, name='inputB')\n        in_tensor_2 = tf.compat.v1.placeholder(\n            shape=shape, dtype=tf.float32, name='inputA')\n        variable_node = tf.Variable(1.0, name='variable_node')\n        out_tensor = in_tensor_1 + in_tensor_2 * variable_node\n        inputs = {'x': in_tensor_1, 'y': in_tensor_2}\n        outputs = {'z': out_tensor}\n        sess.run(tf.compat.v1.variables_initializer([variable_node]))\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n    return saved_model_dir\n\n  def _createV2QATSavedModel(self, shape):\n    \"\"\"Create a simple QAT SavedModel in TF 2.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    input_name = 'input'\n    output_name = 'scores'\n\n    input_tensor = tf.keras.layers.Input((32, 32, 128), name=input_name)\n    x = tf.quantization.fake_quant_with_min_max_args(input_tensor, -3.0, 3.0)\n    x = tf.keras.layers.Conv2D(1, (3, 3))(x)\n    x = tf.quantization.fake_quant_with_min_max_args(x, -3.0, 3.0)\n    scores = tf.keras.layers.Reshape((-1,), name=output_name)(x)\n    model = tf.keras.Model(input_tensor, scores)\n    model.save(saved_model_dir)\n    return saved_model_dir, input_name, output_name\n\n  @test_util.run_v2_only\n  def testV1SimpleModel(self):\n    \"\"\"Test a SavedModel.\"\"\"\n    with tf.Graph().as_default():\n      saved_model_dir = self._createV1SavedModel(shape=[1, 16, 16, 3])\n\n      # Convert model and ensure model is not None.\n      converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n      tflite_model = converter.convert()\n      self.assertTrue(tflite_model)\n\n      interpreter = Interpreter(model_content=tflite_model)\n      interpreter.allocate_tensors()\n\n      input_details = interpreter.get_input_details()\n      self.assertLen(input_details, 2)\n      self.assertStartsWith(input_details[0]['name'], 'inputA')\n      self.assertEqual(np.float32, input_details[0]['dtype'])\n      self.assertAllEqual([1, 16, 16, 3], input_details[0]['shape'])\n      self.assertEqual((0., 0.), input_details[0]['quantization'])\n\n      self.assertStartsWith(\n          input_details[1]['name'],\n          'inputB',\n      )\n      self.assertEqual(np.float32, input_details[1]['dtype'])\n      self.assertTrue([1, 16, 16, 3], input_details[1]['shape'])\n      self.assertEqual((0., 0.), input_details[1]['quantization'])\n\n      output_details = interpreter.get_output_details()\n      self.assertLen(output_details, 1)\n      self.assertStartsWith(output_details[0]['name'], 'add')\n      self.assertEqual(np.float32, output_details[0]['dtype'])\n      self.assertTrue([1, 16, 16, 3], output_details[0]['shape'])\n      self.assertEqual((0., 0.), output_details[0]['quantization'])\n\n  @parameterized.named_parameters(\n      ('Default', False),\n      ('UnfoldLargeConstant', True),\n  )\n  @test_util.run_v2_only\n  def testUnfoldLargeConstant(self, unfold_large_constant):\n    \"\"\"Test unfolding large splat constant in a TF Lite model.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_savedmodel')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor = tf.compat.v1.placeholder(\n            shape=[1000, 1000], dtype=tf.float32, name='input')\n        constant = tf.constant(value=1, dtype=tf.float32, shape=[1000, 1000])\n        out_tensor = in_tensor + constant\n        inputs = {'x': in_tensor}\n        outputs = {'y': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter._experimental_unfold_large_splat_constant = unfold_large_constant\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    model = util._convert_model_from_bytearray_to_object(tflite_model)\n    if unfold_large_constant:\n      self.assertEqual(model.operatorCodes[0].builtinCode,\n                       schema_fb.BuiltinOperator.FILL)\n      self.assertEqual(model.operatorCodes[1].builtinCode,\n                       schema_fb.BuiltinOperator.ADD)\n    else:\n      self.assertEqual(model.operatorCodes[0].builtinCode,\n                       schema_fb.BuiltinOperator.ADD)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual('input:0', input_details[0]['name'])\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertAllEqual([1000, 1000], input_details[0]['shape'])\n    self.assertEqual((0., 0.), input_details[0]['quantization'])\n\n    output_details = interpreter.get_output_details()\n    self.assertEqual('add:0', output_details[0]['name'])\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n    self.assertAllEqual([1000, 1000], output_details[0]['shape'])\n    self.assertEqual((0., 0.), output_details[0]['quantization'])\n\n    interpreter.set_tensor(input_details[0]['index'],\n                           np.ones(shape=[1000, 1000], dtype=np.float32))\n    interpreter.invoke()\n    self.assertAllEqual(\n        np.full(shape=[1000, 1000], fill_value=2.0, dtype=np.float32),\n        interpreter.get_tensor(output_details[0]['index']))\n\n  @test_util.run_v2_only\n  def testPreserveAssert(self):\n    \"\"\"Test preserving AssertOp in a TF Lite model.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_savedmodel')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor = tf.compat.v1.placeholder(\n            shape=[10, 10], dtype=tf.float32, name='input')\n        constant = tf.constant(value=1, dtype=tf.float32, shape=[10, 10])\n        assert_op = tf.Assert(tf.less_equal(in_tensor, constant), [in_tensor])\n        with tf.control_dependencies([assert_op]):\n          out_tensor = in_tensor + constant\n        inputs = {'x': in_tensor}\n        outputs = {'y': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    converter._experimental_preserve_assert_op = True\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    model = util._convert_model_from_bytearray_to_object(tflite_model)\n    has_assert = False\n    for op_code in model.operatorCodes:\n      if op_code.customCode == b'FlexAssert':\n        has_assert = True\n        break\n    self.assertTrue(has_assert)\n\n  @test_util.run_v2_only\n  def testTF1HubFormattedModel(self):\n    \"\"\"Test a TF1 hub formatted model.\"\"\"\n    saved_model_dir = self._createV1SavedModel(shape=[1, 16, 16, 3])\n\n    # TF1 hub model is based on V1 saved model and they omit the saved model\n    # schema version setting.\n    saved_model_proto = parse_saved_model(saved_model_dir)\n    saved_model_proto.saved_model_schema_version = 0\n\n    saved_model_pb_file_path = os.path.join(saved_model_dir, 'saved_model.pb')\n    with file_io.FileIO(saved_model_pb_file_path, 'wb') as writer:\n      writer.write(saved_model_proto.SerializeToString())\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  def _createV1ModelWithHashTableInitializer(self):\n    # Create a v1 saved model with hash table initializers.\n    tf.compat.v1.disable_eager_execution()\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'savedmodel_with_hashtable')\n\n    table_initializer = tf.lookup.KeyValueTensorInitializer(\n        keys=['a', 'b', 'c', 'd'],\n        values=[1, 2, 3, 4],\n        key_dtype=tf.string,\n        value_dtype=tf.int64)\n    table = tf.lookup.StaticHashTable(\n        table_initializer, default_value=tf.constant(-1, dtype=tf.int64))\n\n    x = tf.compat.v1.placeholder(tf.string, shape=(), name='input')\n    y = table.lookup(x)\n\n    tensor_info_x = tf.compat.v1.saved_model.utils.build_tensor_info(x)\n    tensor_info_y = tf.compat.v1.saved_model.utils.build_tensor_info(y)\n\n    signature_def_map, init_op, assets_collection = {\n        'serving_default':\n            (tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n                inputs={'x': tensor_info_x},\n                outputs={'y': tensor_info_y},\n                method_name='some_function'))\n    }, tf.compat.v1.tables_initializer(), None\n\n    sess = tf.compat.v1.Session()\n    sess.run(tf.compat.v1.initializers.global_variables())\n\n    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(\n        saved_model_dir)\n    builder.add_meta_graph_and_variables(\n        sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\n        signature_def_map,\n        main_op=init_op,\n        assets_collection=assets_collection,\n        strip_default_attrs=True)\n    builder.save()\n\n    # Restore TF v2 behavior.\n    tf.compat.v1.reset_default_graph()\n    tf.compat.v1.enable_eager_execution()\n    return saved_model_dir\n\n  @test_util.run_v2_only\n  def testModelWithHashTableInitializer(self):\n    \"\"\"Test a model with saved_model's session initializer for hash tables.\"\"\"\n    saved_model_dir = self._createV1ModelWithHashTableInitializer()\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_data = np.array(['a', 'b', 'c', 'z'], dtype=np.string_)\n    interpreter.resize_tensor_input(\n        input_details[0]['index'], [4], strict=False)\n    interpreter.allocate_tensors()\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Invoke multiple times to ensure the initializer graph runs only once.\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 2, 3, -1], list(actual_value))\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 2, 3, -1], list(actual_value))\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 2, 3, -1], list(actual_value))\n\n  def _createV1ModelWithMutableHashTable(self):\n    # Create a v1 saved model with mutable hash table.\n    tf.compat.v1.disable_eager_execution()\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'savedmodel_with_mutable_hashtable')\n\n    table = tf.raw_ops.MutableHashTableV2(\n        key_dtype=tf.string, value_dtype=tf.int64)\n    x = tf.compat.v1.placeholder(tf.string, shape=(), name='input')\n    keys = tf.constant(['a', 'b'], tf.string)\n    values = tf.constant([1, 5], tf.int64)\n    default_value = tf.constant(-1, tf.int64)\n    insert_call = tf.raw_ops.LookupTableInsertV2(\n        table_handle=table, keys=keys, values=values)\n    with tf.control_dependencies([insert_call]):\n      y = tf.raw_ops.LookupTableFindV2(\n          table_handle=table, keys=x, default_value=default_value)\n\n    tensor_info_x = tf.compat.v1.saved_model.utils.build_tensor_info(x)\n    tensor_info_y = tf.compat.v1.saved_model.utils.build_tensor_info(y)\n\n    signature_def_map, init_op, assets_collection = {\n        'serving_default':\n            (tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n                inputs={'x': tensor_info_x},\n                outputs={'y': tensor_info_y},\n                method_name='some_function'))\n    }, tf.compat.v1.tables_initializer(), None\n\n    sess = tf.compat.v1.Session()\n\n    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(\n        saved_model_dir)\n    builder.add_meta_graph_and_variables(\n        sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\n        signature_def_map,\n        main_op=init_op,\n        assets_collection=assets_collection,\n        strip_default_attrs=True)\n    builder.save()\n\n    # Restore TF v2 behavior.\n    tf.compat.v1.reset_default_graph()\n    tf.compat.v1.enable_eager_execution()\n    return saved_model_dir\n\n  @test_util.run_v2_only\n  def testModelWithMutableHashTable(self):\n    \"\"\"Test a model with saved_model's session initializer for hash tables.\"\"\"\n    saved_model_dir = self._createV1ModelWithMutableHashTable()\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_data = np.array(['a', 'b', 'c'], dtype=np.string_)\n    interpreter.resize_tensor_input(\n        input_details[0]['index'], [3], strict=False)\n    interpreter.allocate_tensors()\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 5, -1], list(actual_value))\n\n  @test_util.run_v2_only\n  def testReduceSumWithInt16Quant(self):\n    \"\"\"Test a model with quantized int16 reduce sum op.\"\"\"\n    inp = tf.keras.Input([3, 3], 3, name='x')\n    m = tf.keras.Model(inp, tf.reduce_sum(inp, axis=-1))\n\n    converter = tf.lite.TFLiteConverter.from_keras_model(m)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet\n        .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n    ]\n    converter.inference_input_type = tf.int16\n    converter.inference_output_type = tf.int16\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    inputs = {\n        i.name: np.random.normal(size=i.shape).astype(np.float32)\n        for i in m.inputs\n    }\n    converter.representative_dataset = lambda: [inputs]\n    content = converter.convert()\n\n    interpreter = tf.lite.Interpreter(model_content=content)\n    runner = interpreter.get_signature_runner('serving_default')\n    y = runner(x=np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]).astype(np.int16))\n    self.assertEqual([3, 6, 9], list(list(y.values())[0]))\n\n  @test_util.run_v2_only\n  def testConstModel(self):\n    \"\"\"Test a basic model with functions to make sure functions are inlined.\"\"\"\n    input_data = tf.constant(1., shape=[1])\n    root = autotrackable.AutoTrackable()\n    root.f = tf.function(lambda x: 2. * x)\n    to_save = root.f.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testVariableModel(self):\n    \"\"\"Test a basic model with Variables with saving/loading the SavedModel.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    to_save = root.f.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.TF_SAVED_MODEL)\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @parameterized.named_parameters(('EnableResourceVariables', True),\n                                  ('DisableResourceVariables', False))\n  @test_util.run_v2_only\n  def testNativeVariablesModel(self, enable_resource_variables):\n    \"\"\"Test a basic model with Variables with saving/loading the SavedModel.\"\"\"\n    root = self._getSimpleModelWithVariables()\n    input_data = tf.constant(1., shape=[1, 10])\n    to_save = root.assign_add.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    converter.experimental_enable_resource_variables = enable_resource_variables\n\n    if not enable_resource_variables:\n      with self.assertRaises(convert.ConverterError) as error:\n        tflite_model = converter.convert()\n      self.assertIn(\n          'Variable constant folding is failed. Please consider using enabling '\n          '`experimental_enable_resource_variables` flag in the TFLite '\n          'converter object.',\n          str(error.exception))\n      return\n\n    # Enable resource variables.\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.assign_add(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    for tf_result, tflite_result in zip(expected_value, actual_value[0]):\n      self.assertAllClose(tf_result, tflite_result, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testSignatures(self):\n    \"\"\"Test values for `signature_keys` argument.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    to_save = root.f.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model with invalid `signature_keys`.\n    with self.assertRaises(ValueError) as error:\n      _ = lite.TFLiteConverterV2.from_saved_model(\n          save_dir, signature_keys=['INVALID'])\n    self.assertIn(\"Invalid signature key 'INVALID'\", str(error.exception))\n\n    # Convert model with empty `signature_keys`.\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=[])\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testSignatureDefsWithFullIntegerQuantization(self):\n    # SETUP\n    # 1. Define input shapes\n    tf_input_shape = (32, 32, 128)\n    tflite_input_shape = (1,) + tf_input_shape\n    # 2. Define model\n    tf_saved_model_dir, input_name, output_name = (\n        self._createV2QATSavedModel(tf_input_shape))\n\n    # MODEL 1: TFLite (float) model\n    # 1. Create TFLite model\n    converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_dir)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n    # 2. Initialize the Intepreter\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()[0]\n    interpreter.resize_tensor_input(input_details['index'], tflite_input_shape)\n    interpreter.allocate_tensors()\n    signature_list = interpreter._get_full_signature_list()['serving_default']\n    # 3. (Skip) Verify that signature def input/output tensors are in the model.\n    # 4. Evaluate the model\n    input_data = np.random.random(tflite_input_shape).astype(np.float32)\n    result = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default', {input_name: input_data})[output_name]\n\n    # MODEL 2: TFLite (full integer quantized) model\n    # 1. Create TFLite model\n    converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_dir)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.inference_input_type = tf.int8\n    converter.inference_output_type = tf.int8\n    tflite_model_quant = converter.convert()\n    # 2. Initialize the Intepreter\n    interpreter = Interpreter(model_content=tflite_model_quant)\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()[0]\n    interpreter.resize_tensor_input(input_details['index'], tflite_input_shape)\n    interpreter.allocate_tensors()\n    # 3. Verify that signature def input/output tensors are in the model.\n    all_indices = {item['index'] for item in interpreter.get_tensor_details()}\n    signature_list = interpreter._get_full_signature_list()['serving_default']\n    input_tensor_indices = set(signature_list['inputs'].values())\n    assert input_tensor_indices.issubset(all_indices)\n    output_tensor_indices = set(signature_list['outputs'].values())\n    assert output_tensor_indices.issubset(all_indices)\n\n    # 4. Evaluate the model\n    input_data = np.random.random(tflite_input_shape)\n    input_scale, input_zero_point = input_details['quantization']\n    if (input_scale, input_zero_point) != (0.0, 0):\n      input_data = input_data / input_scale + input_zero_point\n      input_data = input_data.astype(input_details['dtype'])\n    result_quant = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model_quant, 'serving_default',\n        {input_name: input_data})[output_name]\n    output_scale, output_zero_point = output_details['quantization']\n    if (output_scale, output_zero_point) != (0.0, 0):\n      result_quant = result_quant.astype(np.float32)\n      result_quant = (result_quant - output_zero_point) * output_scale\n\n    # COMPARE: Validate that results from both models are approx. the same.\n    root_mean_squared = np.sqrt(np.mean((result-result_quant)**2))\n    assert root_mean_squared < 1.0\n\n  @test_util.run_v2_only\n  def testSignatureDefs(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data_0 = tf.constant(1., shape=[1])\n    input_data_1 = tf.constant(3., shape=[1])\n    mul_add_func = root.mul_add.get_concrete_function(input_data_1,\n                                                      input_data_0)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'mul_add': mul_add_func})\n\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=['mul_add'])\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.mul_add(input_data_1, input_data_0)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'mul_add', {\n            'y': input_data_0,\n            'x': input_data_1\n        })\n    self.assertEqual(list(results.keys()), ['output_0'])\n    self.assertEqual(expected_value.numpy(), results['output_0'])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['mul_add'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['mul_add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['mul_add']['inputs'], ['x', 'y'])\n    self.assertEqual(list(signature_defs['mul_add']['outputs']), ['output_0'])\n\n  @test_util.run_v2_only\n  def testSignatureDefsWithDefaultValue(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\n\n    This test uses None as signature_key to test default behavior.\n    \"\"\"\n    root = self._getMultiFunctionModel()\n    input_data_0 = tf.constant(1., shape=[1])\n    input_data_1 = tf.constant(3., shape=[1])\n    mul_add_func = root.mul_add.get_concrete_function(input_data_1,\n                                                      input_data_0)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'mul_add': mul_add_func})\n\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=['mul_add'])\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.mul_add(input_data_1, input_data_0)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, None, {\n            'y': input_data_0,\n            'x': input_data_1\n        })\n    self.assertEqual(list(results.keys()), ['output_0'])\n    self.assertEqual(expected_value.numpy(), results['output_0'])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['mul_add'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['mul_add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['mul_add']['inputs'], ['x', 'y'])\n    self.assertEqual(list(signature_defs['mul_add']['outputs']), ['output_0'])\n\n  @test_util.run_v2_only\n  def testSignatureDefsQuantizedModel(self):\n    \"\"\"Test converting SignatureDef on quantized model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data_0 = tf.constant(1., shape=[1])\n    input_data_1 = tf.constant(3., shape=[1])\n    mul_add_func = root.mul_add.get_concrete_function(input_data_1,\n                                                      input_data_0)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'mul_add': mul_add_func})\n\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=['mul_add'])\n\n    def representative_dataset_gen():\n      for _ in range(2):\n        yield {\n            'x':\n                np.random.uniform(low=0, high=1,\n                                  size=(1, 1)).astype(np.float32),\n            'y':\n                np.random.uniform(low=0, high=1, size=(1, 1)).astype(np.float32)\n        }\n\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = representative_dataset_gen\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n    tflite_model = converter.convert()\n\n    # Check signatures are valid from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['mul_add'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['mul_add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['mul_add']['inputs'], ['x', 'y'])\n    self.assertEqual(list(signature_defs['mul_add']['outputs']), ['output_0'])\n\n  @test_util.run_v2_only\n  def testMultipleFunctionModel(self):\n    \"\"\"Convert multiple functions in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'add': add_func, 'sub': sub_func})\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 2)\n    self.assertEqual(list(signature_defs.keys()), ['add', 'sub'])\n    self.assertEqual(len(signature_defs.values()), 2)\n    self.assertEqual(list(signature_defs['add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['add']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['add']['outputs']), ['output_0'])\n    self.assertEqual(list(signature_defs['sub'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['sub']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['sub']['outputs']), ['output_0'])\n\n    # Verify the Signature runner executions.\n    add_signature_runner = interpreter.get_signature_runner('add')\n    add_output = add_signature_runner(x=input_data)\n    self.assertEqual(add_output['output_0'], 3)\n\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    sub_output = sub_signature_runner(x=input_data)\n    self.assertEqual(sub_output['output_0'], -2)\n\n  @parameterized.named_parameters(\n      ('_Default', False, False, dtypes.float32, False),\n      ('_DefaultMlirQuant', False, False, dtypes.float32, True),\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutputMlirQuant', True, False, dtypes.int8, True),\n      ('_IntOnly_UINT8InputOutputMlirQuant', True, False, dtypes.uint8, True))\n  @test_util.run_v2_only\n  def testMultipleFunctionQuantizedModel(self,\n                                         is_int_only,\n                                         is_int16_quantize,\n                                         inference_input_output_type,\n                                         enable_mlir_quantizer=False):\n    \"\"\"Convert multiple functions in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'add': add_func, 'sub': sub_func})\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n\n    def representative_dataset_gen():\n      for _ in range(2):\n        yield ('add', {\n            'x': np.random.uniform(low=0, high=1, size=(1,)).astype(np.float32),\n        })\n      for _ in range(2):\n        yield ('sub', {\n            'x': np.random.uniform(low=0, high=1, size=(1,)).astype(np.float32),\n        })\n\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = representative_dataset_gen\n    if is_int_only:\n      if is_int16_quantize:\n        converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n        ]\n      else:\n        converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS_INT8]\n    else:\n      if is_int16_quantize:\n        converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n        ]\n      else:\n        converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS]\n    converter.inference_input_type = inference_input_output_type\n    converter.inference_output_type = inference_input_output_type\n    converter.experimental_new_quantizer = enable_mlir_quantizer\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 2)\n    self.assertEqual(list(signature_defs.keys()), ['add', 'sub'])\n    self.assertEqual(len(signature_defs.values()), 2)\n    self.assertEqual(list(signature_defs['add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['add']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['add']['outputs']), ['output_0'])\n    self.assertEqual(list(signature_defs['sub'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['sub']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['sub']['outputs']), ['output_0'])\n\n    # Verify the Signature runner executions.\n    input_data = tf.constant(\n        np.random.uniform(-1, 1, size=(1,)).astype(\n            inference_input_output_type.as_numpy_dtype))\n    add_signature_runner = interpreter.get_signature_runner('add')\n    add_output = add_signature_runner(x=input_data)\n    self.assertIsNotNone(add_output['output_0'])\n    input_details = add_signature_runner.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertStartsWith(input_details['x']['name'], 'add_x:0')\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details['x']['dtype'])\n    self.assertTrue(([1] == input_details['x']['shape']).all())\n    if inference_input_output_type == dtypes.float32:\n      self.assertEqual((0.0, 0), input_details['x']['quantization'])\n\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    sub_output = sub_signature_runner(x=input_data)\n    self.assertIsNotNone(sub_output['output_0'])\n    output_details = sub_signature_runner.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertStartsWith(output_details['output_0']['name'],\n                          'StatefulPartitionedCall:0')\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details['output_0']['dtype'])\n    self.assertTrue(([1] == output_details['output_0']['shape']).all())\n    if inference_input_output_type == dtypes.float32:\n      self.assertEqual((0.0, 0), output_details['output_0']['quantization'])\n\n  @test_util.run_v2_only\n  def testMultipleFunctionModelWithSharedWeight(self):\n    \"\"\"Convert multiple functions with the shared weight.\"\"\"\n    root = self._getMultiFunctionModelWithSharedWeight()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n    mul_func = root.mul.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'add': add_func, 'sub': sub_func, 'mul': mul_func})\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Make sure that the weight tensors are shared.\n    self.assertLess(len(tflite_model), 1100000)\n\n    # TODO(b/184696047): Write down the test codes for multiple signature\n    #                    runners once the Python API is ready to use.\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    self.assertLen(signature_defs, 3)\n    add_signature_runner = interpreter.get_signature_runner('add')\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    mul_signature_runner = interpreter.get_signature_runner('mul')\n    self.assertIsNotNone(add_signature_runner)\n    self.assertIsNotNone(sub_signature_runner)\n    self.assertIsNotNone(mul_signature_runner)\n\n  @test_util.run_v2_only\n  def testNoConcreteFunctionModel(self):\n    root = self._getMultiFunctionModel()\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir)\n\n    with self.assertRaises(ValueError) as error:\n      _ = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    self.assertIn('Only support at least one signature key.',\n                  str(error.exception))\n\n  @test_util.run_v2_only\n  def testKerasSequentialModel(self):\n    \"\"\"Test a simple sequential tf.Keras model.\"\"\"\n    input_data = tf.constant(1., shape=[1, 1])\n\n    x = np.array([[1.], [2.]])\n    y = np.array([[2.], [4.]])\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1),\n    ])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(x, y, epochs=1)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(model, save_dir)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testGraphDebugInfo(self):\n    \"\"\"Test a SavedModel has debug info captured.\"\"\"\n    input_data = tf.constant(1., shape=[1])\n    root = autotrackable.AutoTrackable()\n    root.f = tf.function(lambda x: 2. * x)\n    to_save = root.f.get_concrete_function(input_data)\n    options = save_options.SaveOptions(save_debug_info=True)\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save, options)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    converter.convert()\n    self._assertValidDebugInfo(converter._debug_info)\n\n  @test_util.run_v2_only\n  def testNonStatefulConvLSTM2D(self):\n    \"\"\"Test saved model with non stateful ConvLSTM2D keras layer.\"\"\"\n    # Create keras model\n    model = tf.keras.Sequential([\n        tf.keras.layers.ConvLSTM2D(\n            32, (3, 3),\n            padding='same',\n            return_sequences=True,\n            stateful=False,\n            batch_input_shape=(1, 1, 10, 10, 1))\n    ])\n    model.compile()\n\n    # Export the keras model to saved model.\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'conv_lstm_2d')\n    model.save(saved_model_dir, save_format='tf', include_optimizer=False)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  @test_util.run_v2_only\n  def testKerasConvLSTM2DWithMoreThanOneDilationRate(self):\n    input_tensor = tf.keras.layers.Input(\n        batch_size=8,\n        shape=[9, 10, 11, 12],\n        name='input_tensor',\n        dtype=tf.float32)\n\n    output = tf.keras.layers.ConvLSTM2D(\n        filters=3,\n        kernel_size=3,\n        strides=1,\n        padding='VALID',\n        dilation_rate=2,\n        use_bias=False,\n        bias_initializer='ones',\n        data_format='channels_last')(\n            input_tensor)\n\n    model = tf.keras.Model(inputs=[input_tensor], outputs=output)\n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy'])\n\n    # Export the keras model to saved model.\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'conv_lstm_2d_with_dilation_rate')\n    model.save(saved_model_dir, save_format='tf', include_optimizer=False)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  @test_util.run_v2_only\n  def testKerasFullyConnectedOutputShape3D(self):\n    \"\"\"Create a simple FullyConnected Model with an output of three dimensions.\"\"\"\n    input_tensor = tf.keras.layers.Input(\n        batch_size=1, shape=[3, 3], name='input_tensor', dtype=tf.float32)\n\n    x = tf.quantization.fake_quant_with_min_max_args(input_tensor, -3.0, 3.0)\n    x = tf.keras.layers.Dense(3)(x)\n    x = tf.quantization.fake_quant_with_min_max_args(x, -3.0, 3.0)\n    model = tf.keras.Model(input_tensor, x)\n\n    model.compile(\n        optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n    # Export the keras model to saved model.\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'fully_connected_output_3d')\n    model.save(saved_model_dir, save_format='tf', include_optimizer=False)\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    interpreter = Interpreter(model_content=tflite_model)\n    output_details = interpreter.get_output_details()\n    input_details = interpreter.get_input_details()\n    interpreter.allocate_tensors()\n\n    input_data = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]], np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    expected_value = model.predict(input_data)\n\n    self.assertLen(output_details[0]['shape_signature'], 3)\n    self.assertAllClose(expected_value, actual_value, atol=1e-1)\n    self.assertEqual(\n        list(output_details[0]['shape_signature']),\n        list(model.layers[-1].output_shape))\n\n  def _createModelWithInputShape(self, shape):\n    \"\"\"Create a simple SavedModel with a certain shape.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'input_shape_model')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        unknown_shape = tf.TensorShape(shape)\n        in_tensor = tf.compat.v1.placeholder(\n            shape=unknown_shape, dtype=tf.float32, name='input')\n        out_tensor = in_tensor + in_tensor\n        inputs = {'input': in_tensor}\n        outputs = {'output': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n    return saved_model_dir\n\n  @test_util.run_v2_only\n  def testUnknownInputShapeModel(self):\n    \"\"\"Test a SavedModel with an unknown input shape.\"\"\"\n    saved_model_dir = self._createModelWithInputShape(None)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Validate that tensors with unknown shape have unknown rank.\n    tflite_model_obj = _convert_bytearray_to_object(tflite_model)\n    for tensor in tflite_model_obj.subgraphs[0].tensors:\n      self.assertEqual(False, tensor.hasRank)\n      self.assertEqual([], tensor.shape.tolist())\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_data = np.array([1., 2., 3.], dtype=np.float32)\n    interpreter.resize_tensor_input(\n        input_details[0]['index'], [3], strict=False)\n    interpreter.allocate_tensors()\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([2., 4., 6.], list(actual_value))\n\n  @test_util.run_v2_only\n  def testScalarInputShapeModel(self):\n    \"\"\"Test a SavedModel with a scalar input.\"\"\"\n    saved_model_dir = self._createModelWithInputShape([])\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Validate that scalar tensors have a rank = 0.\n    tflite_model_obj = _convert_bytearray_to_object(tflite_model)\n    for tensor in tflite_model_obj.subgraphs[0].tensors:\n      self.assertEqual(True, tensor.hasRank)\n      self.assertEqual([], tensor.shape.tolist())\n\n  @test_util.run_v2_only\n  def testMatrixInputShapeModel(self):\n    \"\"\"Test a SavedModel with a matrix input.\"\"\"\n    saved_model_dir = self._createModelWithInputShape([2, 3])\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Validate that matrix tensors have a rank = 2.\n    tflite_model_obj = _convert_bytearray_to_object(tflite_model)\n    for tensor in tflite_model_obj.subgraphs[0].tensors:\n      self.assertEqual(True, tensor.hasRank)\n      self.assertEqual([2, 3], tensor.shape.tolist())\n\n  @parameterized.named_parameters(\n      ('_PerChannelQuant', False, False),\n      ('_PerChannelMlirQuant', False, True),\n      ('_PerTensorQuant', True, False),\n      ('_PerTensorMlirQuant', True, True),\n      ('_PerChannelDynamicRange', False, False, True),\n      ('_PerTensorDynamicRange', True, False, True))\n  @test_util.run_v2_only\n  def testDisablePerChannelQuantization(self,\n                                        disable_per_channel=False,\n                                        enable_mlir_quantizer=False,\n                                        representative_dataset=True):\n    # Dynamic range quant requires total num elements of filters > 1024.\n    k_num_filters = 38\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(k_num_filters, (3, 3), activation='relu')\n    ])\n    model.build(input_shape=(1, 5, 5, 3))\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'conv_saved_model')\n    save(model, saved_model_dir)\n    k_conv_name = 'sequential/conv2d/Conv2D'\n    quantized_converter = tf.lite.TFLiteConverter.from_saved_model(\n        saved_model_dir)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    if representative_dataset:\n      def calib_gen():\n        for _ in range(5):\n          yield [np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32)]\n      quantized_converter.representative_dataset = calib_gen\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS\n    ]\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n    if disable_per_channel:\n      quantized_converter._experimental_disable_per_channel = (\n          disable_per_channel)\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    detail = next((d for d in interpreter.get_tensor_details()\n                   if d['name'].startswith(k_conv_name)))\n    quant_params = detail['quantization_parameters']\n    expected_num_params = k_num_filters\n    if disable_per_channel:\n      expected_num_params = 1\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n  @parameterized.named_parameters(\n      ('_INT8Quant_INT32Bias', False, False, dtypes.int32, True),\n      ('_INT16Quant_INT64Bias', True, False, dtypes.int64, True),\n      ('_INT8Quant_INT32Bias_Set', False, True, dtypes.int32, True),\n      ('_INT8Quant_INT64Bias_Set', False, True, dtypes.int64, False),\n      ('_INT16Quant_INT32Bias_Set', True, True, dtypes.int32, True),\n      ('_INT16Quant_INT64Bias_Set', True, True, dtypes.int64, True),\n      ('_INT16Quant_FLOAT32Bias_Set', True, True, dtypes.float32, False),\n  )\n  @test_util.run_v2_only\n  def testBiasQuantization(self, is_int16_quantize, explicitly_set_bias,\n                           bias_type, is_valid_bias_type):\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(\n            1024, input_shape=[1024], activation=None, bias_initializer='ones')\n    ])\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'dense_saved_model')\n    save(model, saved_model_dir)\n    k_dense_bias_name = 'sequential/dense/BiasAdd/ReadVariableOp'\n    quantized_converter = tf.lite.TFLiteConverter.from_saved_model(\n        saved_model_dir)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n\n    if explicitly_set_bias:\n      quantized_converter._experimental_full_integer_quantization_bias_type = bias_type\n\n    if is_int16_quantize:\n      quantized_converter.target_spec.supported_ops = [\n          lite.OpsSet\n          .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n      ]\n    else:\n      quantized_converter.target_spec.supported_ops = [\n          lite.OpsSet.TFLITE_BUILTINS_INT8\n      ]\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [np.random.randn(1, 1024).astype(np.float32)]\n\n    quantized_converter.representative_dataset = calibration_gen\n\n    if not is_valid_bias_type:\n      with self.assertRaisesRegex(ValueError, 'Expected bias type to be'):\n        quantized_converter.convert()\n      return\n\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    dense_bias = next((d for d in interpreter.get_tensor_details()\n                       if d['name'].startswith(k_dense_bias_name)))\n    self.assertEqual(bias_type, dense_bias['dtype'])\n\n  @parameterized.named_parameters(\n      ('_Int8PerChannelMlirDynamicRangeQuant', True, False, False),\n      ('_Int8PerChannelTocoDynamicRangeQuant', False, False, False),\n      ('_Int8PerTensorMlirDynamicRangeQuant', True, True, False),\n      ('_Int8PerTensorTocoDynamicRangeQuant', False, True, False),\n      ('_Float16DynamicRangeQuant', True, False, True))\n  @test_util.run_v2_only\n  def testMlirDynamicRangeQuantization(self, enable_new_dynamic_range_quantizer,\n                                       disable_per_channel,\n                                       enable_float16_quant):\n    num_filters = 1024\n    conv_name = 'sequential/conv2d/Conv2D'\n    model = tf.keras.models.Sequential(\n        [tf.keras.layers.Conv2D(num_filters, (3, 3), activation='relu')])\n    model.build(input_shape=(1, 32, 32, 3))\n    saved_model_dir = self.create_tempdir()\n    save(model, saved_model_dir.full_path)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(\n        saved_model_dir.full_path)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.experimental_new_dynamic_range_quantizer = (\n        enable_new_dynamic_range_quantizer)\n    converter._experimental_disable_per_channel = disable_per_channel\n    if enable_float16_quant:\n      converter.target_spec.supported_types = [tf.float16]\n    quantized_tflite_model = converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    quantized_weight = None\n    quantized_weight_with_one_postfix = None\n    quantized_weight_without_one_postfix = None\n    for d in interpreter.get_tensor_details():\n      if d['name'] == conv_name + '1':\n        quantized_weight = d\n        quantized_weight_with_one_postfix = d\n        break\n    for d in interpreter.get_tensor_details():\n      if d['name'].startswith(conv_name):\n        if quantized_weight is None:\n          quantized_weight = d\n        quantized_weight_without_one_postfix = d\n        break\n\n    self.assertIsNotNone(quantized_weight)\n    quant_params = quantized_weight['quantization_parameters']\n\n    if enable_float16_quant:\n      expected_num_params = 0\n    else:\n      expected_num_params = 1 if disable_per_channel else num_filters\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n    if enable_float16_quant:\n      self.assertTrue(\n          (quantized_weight_with_one_postfix is not None and\n           np.float16 == quantized_weight_with_one_postfix['dtype']) or\n          (quantized_weight_without_one_postfix is not None and\n           np.float16 == quantized_weight_without_one_postfix['dtype']))\n    else:\n      self.assertEqual(np.int8, quantized_weight['dtype'])\n\n\nclass FromKerasModelTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testSequentialModel(self):\n    \"\"\"Test a simple sequential tf.Keras model.\"\"\"\n    input_data = tf.constant(1., shape=[1, 1])\n\n    # Create a simple Keras model.\n    x = np.array([[1.], [2.]])\n    y = np.array([[2.], [4.]])\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(units=1, input_shape=[1])\n    ])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(x, y, epochs=1)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.KERAS_MODEL)\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testSequentialMultiInputOutputModel(self):\n    \"\"\"Test a tf.Keras model with multiple inputs and outputs.\"\"\"\n    left_input_data = tf.constant(1., shape=[1, 3])\n    right_input_data = tf.constant(1., shape=[1, 3])\n\n    # Create a simple Keras model.\n    input_a_np = np.random.random((10, 3))\n    input_b_np = np.random.random((10, 3))\n    output_c_np = np.random.random((10, 3))\n    output_d_np = np.random.random((10, 2))\n\n    input_a = tf.keras.layers.Input(shape=(3,), name='input_a')\n    input_b = tf.keras.layers.Input(shape=(3,), name='input_b')\n\n    dense = tf.keras.layers.Dense(8, name='dense_1')\n    interm_a = dense(input_a)\n    interm_b = dense(input_b)\n    merged = tf.keras.layers.concatenate([interm_a, interm_b], name='merge')\n\n    output_c = tf.keras.layers.Dense(\n        3, activation='softmax', name='dense_2')(\n            merged)\n    output_d = tf.keras.layers.Dense(\n        2, activation='softmax', name='dense_3')(\n            merged)\n\n    model = tf.keras.models.Model(\n        inputs=[input_a, input_b], outputs=[output_c, output_d])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit([input_a_np, input_b_np], [output_c_np, output_d_np], epochs=1)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    input_data = [left_input_data, right_input_data]\n    expected_value = model.predict(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, input_data)\n    for tf_result, tflite_result in zip(expected_value, actual_value):\n      self.assertAllClose(tf_result, tflite_result, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testGraphDebugInfo(self):\n    \"\"\"Test a tf.Keras model has debug info captured.\"\"\"\n    # Create a simple Keras model.\n    x = [-1, 0, 1, 2, 3, 4]\n    y = [-3, -1, 1, 3, 5, 7]\n    model = tf.keras.models.Sequential(\n        [tf.keras.layers.Dense(units=1, input_shape=[1])])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(x, y, epochs=1)\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter.convert()\n    self._assertValidDebugInfo(converter._debug_info)\n\n  @test_util.run_v2_only\n  def testKerasFallbackPath(self):\n    \"\"\"Test keras model which failed when exporting to the saved model.\"\"\"\n    input_data = tf.constant(\n        np.array(np.random.random_sample((20)), dtype=np.float32))\n\n    class Model(tf.keras.Model):\n\n      def __init__(self):\n        super(Model, self).__init__()\n        # A None name will cause a failure in exporting to a saved model.\n        self.shared_weights = self.add_weight(\n            name=None,\n            shape=(20, 1),\n            dtype=tf.float32,\n            initializer=tf.random_normal_initializer(\n                mean=0.0, stddev=300**(-0.5)))\n\n      def call(self, x):\n        return tf.add(self.shared_weights, x)\n\n    # Building the model.\n    model = Model()\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(input_data, input_data, epochs=1)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  @test_util.run_v2_only\n  def testSignatureDefs(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    keras_model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(\n            32,\n            kernel_size=3,\n            padding='same',\n            activation='relu',\n            input_shape=(32, 32, 3),\n            name='tensor'),\n        tf.keras.layers.Dense(10, name='output_tensor')\n    ])\n\n    converter = lite.TFLiteConverterV2.from_keras_model(keras_model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    input_data = tf.constant(\n        np.random.uniform(-1, 1, size=(1, 32, 32, 3)).astype(np.float32))\n    expected_value = keras_model(input_data)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default', {'tensor_input': input_data})\n    self.assertEqual(list(results.keys()), ['output_tensor'])\n    self.assertAllClose(expected_value.numpy(), results['output_tensor'])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['serving_default'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['serving_default'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['serving_default']['inputs'],\n                          ['tensor_input'])\n    self.assertEqual(\n        list(signature_defs['serving_default']['outputs']), ['output_tensor'])\n\n  @parameterized.named_parameters(\n      ('_PerChannelMlirDynamicRangeQuant', True, False, False),\n      ('_PerChannelTocoDynamicRangeQuant', False, False, False),\n      ('_PerTensorMlirDynamicRangeQuant', True, True, False),\n      ('_PerTensorTocoDynamicRangeQuant', False, True, False),\n      ('_Float16DynamicRangeQuant', True, False, True))\n  @test_util.run_v2_only\n  def testMlirDynamicRangeQuantization(self, enable_new_dynamic_range_quantizer,\n                                       disable_per_channel,\n                                       enable_float16_quant):\n    num_filters = 1024\n    conv_name = 'sequential/conv2d/Conv2D'\n    model = tf.keras.models.Sequential(\n        [tf.keras.Input(shape=(32, 32, 3)),\n         tf.keras.layers.Conv2D(num_filters, (3, 3), activation='relu')])\n    model.build()\n\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.experimental_new_dynamic_range_quantizer = (\n        enable_new_dynamic_range_quantizer)\n    converter._experimental_disable_per_channel = disable_per_channel\n    if enable_float16_quant:\n      converter.target_spec.supported_types = [tf.float16]\n    quantized_tflite_model = converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    quantized_weight = None\n    quantized_weight_with_one_postfix = None\n    quantized_weight_without_one_postfix = None\n    for d in interpreter.get_tensor_details():\n      if d['name'] == conv_name + '1':\n        quantized_weight = d\n        quantized_weight_with_one_postfix = d\n        break\n    for d in interpreter.get_tensor_details():\n      if d['name'].startswith(conv_name):\n        if quantized_weight is None:\n          quantized_weight = d\n        quantized_weight_without_one_postfix = d\n        break\n\n    self.assertIsNotNone(quantized_weight)\n    quant_params = quantized_weight['quantization_parameters']\n\n    if enable_float16_quant:\n      expected_num_params = 0\n    else:\n      expected_num_params = 1 if disable_per_channel else num_filters\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n    if enable_float16_quant:\n      self.assertTrue(\n          (quantized_weight_with_one_postfix is not None and\n           np.float16 == quantized_weight_with_one_postfix['dtype']) or\n          (quantized_weight_without_one_postfix is not None and\n           np.float16 == quantized_weight_without_one_postfix['dtype']))\n    else:\n      self.assertEqual(np.int8, quantized_weight['dtype'])\n\n  @parameterized.named_parameters([\n      ('{}BitWeightOnly={}LowBit={}'.format(num_bits, weight_only, low_bit),\n       num_bits, weight_only, low_bit) for num_bits, weight_only, low_bit\n      in itertools.product((2, 4, 6), (True, False), (True, False))])\n  @test_util.run_v2_only\n  def testQATLowBitKerasModel(self, num_bits, weight_only, low_bit):\n    bit_max = (1 << (num_bits - 1)) - 1\n    bit_min = -bit_max\n    tf_input_shape = (5, 5, 3)\n    tflite_input_shape = (1,) + tf_input_shape\n    model, input_name, output_name = (self._createV2QATLowBitKerasModel(\n        tf_input_shape, weight_only, num_bits, bit_min, bit_max))\n    input_data = np.linspace(\n        0, 6, np.prod(tflite_input_shape)).reshape(tflite_input_shape)\n    tf_result = model(input_data)\n\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    if low_bit:\n      converter._experimental_low_bit_qat = True\n    tflite_model = converter.convert()\n    result = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default',\n        {input_name: input_data.astype(np.float32)})[output_name]\n    self.assertAllClose(\n        [np.linalg.norm(result - tf_result.numpy().astype(np.float32))], [0.0])\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    interpreter.allocate_tensors()\n    num_8bit_activations = 0\n    num_8bit_weights = 0\n    kernel_name = ('model/conv_wrapper/Conv2D;model/conv_wrapper/'\n                   'FakeQuantWithMinMaxVarsPerChannel')\n\n    for detail in interpreter.get_tensor_details():\n      if (detail['dtype'] == np.int8 and detail['name'] and\n          detail['name'] == kernel_name):\n        num_8bit_weights += 1\n        weights = interpreter.get_tensor(detail['index'])\n        if low_bit:\n          self.assertFalse((bit_min > weights).any() or\n                           (weights > bit_max).any())\n        else:\n          self.assertTrue((bit_min > weights).any() or\n                          (weights > bit_max).any())\n        self.assertIn('scales', detail['quantization_parameters'])\n        if low_bit and detail['quantization_parameters']['scales']:\n          self.assertAllClose(\n              detail['quantization_parameters']['scales'], [1.0])\n      elif detail['dtype'] == np.int8 and detail['name']:\n        self.assertFalse(weight_only)\n        self.assertIn('scales', detail['quantization_parameters'])\n        if detail['quantization_parameters']['scales']:\n          self.assertAllClose(\n              detail['quantization_parameters']['scales'], [6/255])\n        num_8bit_activations += 1\n\n    self.assertEqual(num_8bit_weights, 0 if weight_only and not low_bit else 1)\n    # 3 activations with full integer: conv_input, conv_output, reshape_output\n    self.assertEqual(num_8bit_activations, 0 if weight_only else 3)\n\n\nclass FromJaxModelTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testInvalidInputsModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def simple_model(input1, input2):\n      return jnp.sin(input1) + jnp.cos(input2)\n\n    input_tensor = jnp.zeros([10, 10])\n    # Invalid case: not specify serving_func\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        None, [{\n            'input1': input_tensor\n        }])\n    with self.assertRaisesRegex(ValueError, 'No serving func is specified.'):\n      converter.convert()\n\n    # Invalid case: not specify input\n    converter = lite.TFLiteConverterV2.experimental_from_jax([simple_model],\n                                                             None)\n    with self.assertRaisesRegex(ValueError, 'Input tensors are not specified.'):\n      converter.convert()\n\n    converter = lite.TFLiteConverterV2.experimental_from_jax([simple_model], [])\n    with self.assertRaisesRegex(ValueError, 'Input tensors are not specified.'):\n      converter.convert()\n\n    # Invalid case: not wrap input_tensor in a list.\n    converter = lite.TFLiteConverterV2.experimental_from_jax([simple_model],\n                                                             input_tensor)\n    with self.assertRaisesRegex(\n        ValueError,\n        'The truth value of an array with more than one element is ambiguous.'):\n      converter.convert()\n\n    # Invalid case: only partial inputs are provided.\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [simple_model], [[('input1', input_tensor)]])\n    with self.assertRaisesRegex(\n        ValueError, 'Failed to convert the given Jax function to hlo.'):\n      converter.convert()\n\n    # Invalid case: serving functions length does not match input mapping.\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [simple_model, simple_model], [[\n            ('input1', input_tensor),\n            ('input2', input_tensor),\n        ]])\n    with self.assertRaisesRegex(\n        ValueError,\n        'Input tensor mapping len 1 does not match serving func len 2.'):\n      converter.convert()\n\n    # Invalid case: multiple serving function is provided.\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [simple_model, simple_model], [[\n            ('input1', input_tensor),\n            ('input2', input_tensor),\n        ], [\n            ('input1', input_tensor),\n            ('input2', input_tensor),\n        ]])\n    with self.assertRaisesRegex(\n        ValueError, 'Currently only support single serving function.'):\n      converter.convert()\n\n  @test_util.run_v2_only\n  def testSingleInputModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def single_input(input_tensor):\n      return jnp.sin(input_tensor)\n\n    # Convert model.\n    input_tensor = jnp.zeros([10, 10])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [single_input], [[('input_tensor', input_tensor)]])\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.modelType, metadata_fb.ModelType.JAX)\n\n    # Check values from converted_model\n    input_data = np.random.random_sample((10, 10))\n    tf_input_data = tf.constant(input_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [tf_input_data])[0]\n    expected_value = single_input(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testMultipleInputsModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def multiple_inputs(input1, input2):\n      return input1 + input2\n\n    # Convert model.\n    input1 = jnp.zeros([10, 10])\n    input2 = jnp.zeros([10, 1])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [multiple_inputs], [[('input1', input1), ('input2', input2)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input1_data = np.random.random_sample((10, 10))\n    tf_input1_data = tf.constant(input1_data, dtype=np.float32)\n    input2_data = np.random.random_sample((10, 1))\n    tf_input2_data = tf.constant(input2_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [tf_input1_data, tf_input2_data])[0]\n    expected_value = multiple_inputs(input1_data, input2_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testInputSignaturesModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def multiple_inputs(input1, input2):\n      return input1 + input2\n\n    # Convert model.\n    input1 = jnp.zeros([10, 10])\n    input2 = jnp.zeros([10, 1])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [multiple_inputs], [[('input1', input1), ('input2', input2)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input1_data = np.random.random_sample((10, 10))\n    tf_input1_data = tf.constant(input1_data, dtype=np.float32)\n    input2_data = np.random.random_sample((10, 1))\n    tf_input2_data = tf.constant(input2_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [tf_input1_data, tf_input2_data])[0]\n    expected_value = multiple_inputs(input1_data, input2_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testModelWithParams(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def model(inputs, weights):\n      return jnp.matmul(weights, inputs)\n\n    weights = np.random.random_sample((10, 10))\n    serving_func = functools.partial(model, weights=weights)\n\n    # Convert model\n    input_tensor = jnp.zeros([10, 10])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [serving_func], [[('inputs', input_tensor)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input_data = np.random.random_sample((10, 10))\n    tf_input_data = tf.constant(input_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [tf_input_data])[0]\n    expected_value = serving_func(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testWhileLoop(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def condition(x):\n      return jnp.sum(x, keepdims=False) < 100\n\n    def body(x):\n      return jnp.add(x, 2.0)\n\n    def model(x):\n      result = jax.lax.while_loop(condition, body, x)\n      return result[0]\n\n    # Convert model.\n    input_tensor = jnp.zeros([3, 3])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [model], [[('x', input_tensor)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input_data = np.random.random_sample((3, 3))\n    tf_input_data = tf.constant(input_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [tf_input_data])[0]\n    expected_value = model(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n\nclass ControlFlowTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testCond(self):\n    input_data = {\n        'x': tf.constant([1., 2.], shape=[1, 2]),\n        'b': tf.constant(True)\n    }\n\n    weights = tf.Variable([[0.1, 0.2], [0.3, 0.4]], dtype=tf.float32)\n\n    def true_fn(x):\n      return tf.matmul(x, weights)\n\n    def false_fn(x):\n      return tf.add(x, weights)\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, 2], dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.bool)\n    ])\n    def model(x, b):\n      return tf.cond(\n          b, true_fn=lambda: true_fn(x), false_fn=lambda: false_fn(x))\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(**input_data)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data['x'], input_data['b']])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testCondWithFullIntegerQuantization(self):\n    weights = tf.Variable([[0.1, 0.2], [0.3, 0.4]], dtype=tf.float32)\n\n    def true_fn(x):\n      return tf.matmul(x, weights)\n\n    def false_fn(x):\n      return tf.add(x, weights)\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, 2], dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.bool)\n    ])\n    def model(x, b):\n      return tf.cond(\n          b, true_fn=lambda: true_fn(x), false_fn=lambda: false_fn(x))\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(1, 2)).astype(np.float32),\n            tf.constant(True)\n        ]\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(1, 2)).astype(np.float32),\n            tf.constant(False)\n        ]\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n  @test_util.run_v2_only\n  def testConverterErrorOnControlFlowV1Ops(self):\n    filename = resource_loader.get_path_to_datafile(\n        'testdata/control_flow_v1_saved_model')\n    converter = lite.TFLiteConverterV2.from_saved_model(filename)\n    with self.assertRaises(convert.ConverterError) as error:\n      converter.convert()\n    self.assertIn(\n        'Failed to functionalize Control Flow V1 ops. Consider using Control '\n        'Flow V2 ops instead. See https://www.tensorflow.org/api_docs/python/'\n        'tf/compat/v1/enable_control_flow_v2.', str(error.exception))\n\n  @test_util.run_v2_only\n  def testStaticRnn(self):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((3, 10)), dtype=np.float32))\n\n    cell = tf.keras.layers.LSTMCell(10)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[3, 10], dtype=tf.float32)])\n    def model(x):\n      seq = tf.split(x, 3, 0)\n      return rnn.static_rnn(cell, seq, dtype=tf.float32, sequence_length=[1])\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)[0]\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    for expected, actual in zip(expected_value, actual_value):\n      self.assertAllClose(expected, actual)\n\n  @test_util.run_v2_only\n  def testWhileLoop(self):\n    input_data = tf.constant([1., 2., 3., 4.], shape=[2, 2])\n\n    weights = tf.Variable([[0.1, 0.2], [0.3, 0.4]], dtype=tf.float32)\n\n    def condition(x):\n      return tf.reduce_sum(x) < 100\n\n    def body(x):\n      return tf.add(x, weights)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[2, 2], dtype=tf.float32)])\n    def model(x):\n      return tf.while_loop(condition, body, [x])\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)[0]\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testDynamicRnn(self):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((3, 10, 10)), dtype=np.float32))\n\n    cell = tf.keras.layers.LSTMCell(10)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[3, 10, 10], dtype=tf.float32)])\n    def model(x):\n      rnn_layer = tf.keras.layers.RNN([cell], return_sequences=True)\n      return rnn_layer(x)\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)\n    lite_outputs = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertLen(lite_outputs, 1)\n    actual_value = lite_outputs[0]\n    for expected, actual in zip(expected_value, actual_value):\n      self.assertAllClose(expected, actual)\n\n  @parameterized.named_parameters(\n      ('LSTMBatchSizeOne', tf.keras.layers.LSTM, True),\n      ('LSTM', tf.keras.layers.LSTM, False),\n      ('SimpleRNNBatchSizeOne', tf.keras.layers.SimpleRNN, True),\n      ('SimpleRNN', tf.keras.layers.SimpleRNN, False),\n      ('GRUBatchSizeOne', tf.keras.layers.GRU, True),\n      ('GRU', tf.keras.layers.GRU, False))\n  @test_util.run_v2_only\n  def testKerasRNN(self, rnn_layer, default_to_single_batch):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 10, 10)), dtype=np.float32))\n    rnn_obj = rnn_layer(units=10, input_shape=(10, 10))\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Input(shape=(10, 10), name='input'),\n        rnn_obj,\n    ])\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter._experimental_default_to_single_batch_in_tensor_list_ops = default_to_single_batch\n    if not default_to_single_batch:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @parameterized.named_parameters(('LSTM', tf.keras.layers.LSTM),\n                                  ('SimpleRNN', tf.keras.layers.SimpleRNN),\n                                  ('GRU', tf.keras.layers.GRU))\n  @test_util.run_v2_only\n  def testKerasRNNMultiBatches(self, rnn_layer):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((4, 10, 10)), dtype=np.float32))\n    # Specify a fixed batch size(4) for the test model.\n    x = tf.keras.layers.Input(batch_shape=(4, 10, 10))\n    y = rnn_layer(units=10, input_shape=(10, 10))(x)\n    model = tf.keras.Model(inputs=[x], outputs=[y])\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @parameterized.named_parameters(('ForceToUseBatchSizeOne', True),\n                                  ('DontForceToUseBatchSizeOne', False))\n  @test_util.run_v2_only\n  def testKerasBidirectionalRNNReturnSequence(self, default_to_single_batch):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 10, 10)), dtype=np.float32))\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input(shape=(10, 10), name='input'))\n    model.add(\n        tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(units=10, return_sequences=True),\n            input_shape=(10, 10)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(5))\n    model.add(tf.keras.layers.Activation('softmax'))\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter._experimental_default_to_single_batch_in_tensor_list_ops = default_to_single_batch\n    if not default_to_single_batch:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @parameterized.named_parameters(('ForceToUseBatchSizeOne', True),\n                                  ('DontForceToUseBatchSizeOne', False))\n  @test_util.run_v2_only\n  def testKerasBidirectionalRNN(self, default_to_single_batch):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 10, 10)), dtype=np.float32))\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input(shape=(10, 10), name='input'))\n    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=10)))\n    model.add(tf.keras.layers.Dense(5))\n    model.add(tf.keras.layers.Activation('softmax'))\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter._experimental_default_to_single_batch_in_tensor_list_ops = default_to_single_batch\n    if not default_to_single_batch:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n\nclass GrapplerTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testConstantFolding(self):\n    # Constant folding handles the tf.broadcast_to operation which was not\n    # supported by the TFLite at the time this test was added.\n    input_data = tf.constant([1., 2., 3., 4., 5., 6., 7., 8., 9.], shape=[3, 3])\n\n    @tf.function\n    def func(x):\n      y_const = tf.constant([1., 2., 3.])\n      y_broadcast = tf.broadcast_to(y_const, [3, 3])\n      return tf.matmul(x, y_broadcast)\n\n    root = autotrackable.AutoTrackable()\n    root.f = func\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n    # Enable hybrid quantization, same result\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n\nclass UnknownShapes(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testMatMul(self):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((10, 4)), dtype=np.float32))\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[None, 4], dtype=tf.float32)])\n    def model(in_tensor):\n      shape = tf.shape(in_tensor)\n      fill = tf.transpose(tf.fill(shape, 1.))\n      return tf.matmul(fill, in_tensor)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data], input_shapes=[([-1, 4], [10, 4])])[0]\n    self.assertAllClose(expected_value, actual_value, atol=1e-06)\n\n  def _getIntegerQuantizeModelWithUnknownShapes(self):\n    np.random.seed(0)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[None, 33], dtype=tf.float32)])\n    def model(input_tensor):\n      \"\"\"Define a model with tf.MatMul and unknown shapes.\"\"\"\n      # We need the tensor to have more than 1024 elements for quantize_weights\n      # to kick in. Thus, the [33, 33] shape.\n      const_tensor = tf.constant(\n          np.random.uniform(low=-10., high=10., size=[33, 33]),\n          shape=[33, 33],\n          dtype=tf.float32,\n          name='inputB')\n\n      shape = tf.shape(input_tensor)\n      fill = tf.transpose(tf.fill(shape, 1.))\n      mult = tf.matmul(fill, input_tensor)\n      return tf.matmul(mult, const_tensor)\n\n    root = autotrackable.AutoTrackable()\n    root.f = model\n    concrete_func = root.f.get_concrete_function()\n\n    def calibration_gen():\n      for batch in range(5, 20, 5):\n        for _ in range(5):\n          yield [np.random.uniform(-1, 1, size=(batch, 33)).astype(np.float32)]\n\n    return root, concrete_func, calibration_gen\n\n  @test_util.run_v2_only\n  def testMatMulQuantize(self):\n    root, concrete_func, _ = self._getIntegerQuantizeModelWithUnknownShapes()\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    float_tflite_model = float_converter.convert()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_tflite_model = quantized_converter.convert()\n\n    # The default input and output types should be float.\n    quantized_interpreter = Interpreter(model_content=quantized_tflite_model)\n    quantized_interpreter.allocate_tensors()\n    input_details = quantized_interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertAllEqual([-1, 33], input_details[0]['shape_signature'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @test_util.run_v2_only\n  def testMatMulCalibrateAndQuantize(self):\n    root, concrete_func, calibration_gen = (\n        self._getIntegerQuantizeModelWithUnknownShapes())\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    float_tflite_model = float_converter.convert()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_tflite_model = quantized_converter.convert()\n\n    # The default input and output types should be float.\n    quantized_interpreter = Interpreter(model_content=quantized_tflite_model)\n    quantized_interpreter.allocate_tensors()\n    input_details = quantized_interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertAllEqual([-1, 33], input_details[0]['shape_signature'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  def testBatchMatMul(self):\n    input_data_1 = tf.constant(\n        np.array(np.random.random_sample((1, 256, 256)), dtype=np.float32))\n    input_data_2 = tf.constant(\n        np.array(np.random.random_sample((1, 256, 256)), dtype=np.float32))\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, 256, 256], dtype=tf.float32),\n        tf.TensorSpec(shape=[None, 256, 256], dtype=tf.float32)\n    ])\n    def model(in_tensor_1, in_tensor_2):\n      return tf.matmul(in_tensor_1, in_tensor_2)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data_1, input_data_2)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data_1, input_data_2],\n        input_shapes=[([-1, 256, 256], [1, 256, 256])])[0]\n    self.assertAllClose(expected_value, actual_value, atol=4)\n\n  def testBatchMatMulInputInt8Int8OutputInt32(self):\n    input_data_1 = tf.constant(\n        np.array(\n            np.random.random_integers(-128, high=127, size=(1, 20, 30)),\n            dtype=np.int8))\n    input_data_2 = tf.constant(\n        np.array(\n            np.random.random_integers(-128, high=127, size=(1, 30, 10)),\n            dtype=np.int8))\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, 20, 30], dtype=tf.int8),\n        tf.TensorSpec(shape=[None, 30, 10], dtype=tf.int8)\n    ])\n    def model(in_tensor_1, in_tensor_2):\n      return tf.matmul(in_tensor_1, in_tensor_2, output_type=tf.int32)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data_1, input_data_2)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data_1, input_data_2],\n        input_shapes=[([-1, 20, 30], [1, 20, 30]), ([-1, 30, 10], [1, 30,\n                                                                   10])])[0]\n    self.assertAllEqual(expected_value, actual_value)\n\n  def testBatchMatMulHybrid(self):\n    # Test model that does batch matmul of:\n    # lhs input (1, 256, 128), rhs const (1, 128, 256).\n    # For dynamic range quantization situation, this will result in hybrid batch\n    # matmul, where lhs type is float32 and rhs type is int8.\n\n    # Intentionally set lhs, rhs sizes to satisfy following conditions:\n    # 1. rhs const num_elements >= 1024, since dynamic range quantization\n    # requires const tensor num_elements to be larger than\n    # min_elements_for_weights (which defaults to 1024).\n    # (https://github.com/tensorflow/tensorflow/blob/25e649ac3688655547da998eba2715cf70b3e5c9/tensorflow/compiler/mlir/lite/transforms/prepare_quantize_dynamic_range.cc#L262)\n    # 2. batch_size (256) > accum_dim_size (128) and\n    # num_units (256) > accum_dim_size (128), to test if the sizes are set\n    # correctly according to dimensions. See HybridAsymmetricBatchMatMulOpTest\n    # tests in\n    # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/batch_matmul_test.cc.\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 256, 128)), dtype=np.float32))\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, 256, 128], dtype=tf.float32)\n    ])\n    def model(in_tensor):\n      rhs = tf.constant(\n          np.array(np.random.random_sample((1, 128, 256)), dtype=np.float32))\n      return tf.matmul(in_tensor, rhs)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data],\n        input_shapes=[([-1, 256, 128], [1, 256, 128])])[0]\n    self.assertAllClose(expected_value, actual_value, atol=4)\n\n  def testSizeInvalid(self):\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, None, 16, 3], dtype=tf.float32)\n    ])\n    def model(in_tensor):\n      return in_tensor + in_tensor\n\n    concrete_func = model.get_concrete_function()\n\n    # Test invalid shape. None after 1st dimension. Run with TOCO in order to\n    # invoke shape checking code.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.experimental_new_converter = False\n    with self.assertRaises(ValueError) as error:\n      converter.convert()\n    self.assertEqual(\n        'None is only supported in the 1st dimension. Tensor '\n        '\\'in_tensor\\' has invalid shape \\'[1, None, 16, 3]\\'.',\n        str(error.exception))\n\n\nclass ResourceAndVariantTypes(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testVariants(self):\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=[1], dtype=tf.float32)])\n    def model(v):\n      m = map_ops.empty_tensor_map()\n      k = tf.constant(1.0)\n      p = tf.add(k, v)\n      with ops.control_dependencies([m]):\n        m2 = map_ops.tensor_map_insert(m, p, v)\n        with ops.control_dependencies([m2]):\n          return map_ops.tensor_map_size(m2)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(1, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(1, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(1, actual_value)\n\n  @test_util.run_v2_only\n  def testVariantsWithCond(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'variants_with_cond')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          m = map_ops.empty_tensor_map()\n\n          def body(i, m):\n            m = map_ops.tensor_map_insert(m, i, i)\n            return i + 1, m\n\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.int32, name='input')\n          _, result_m = tf.cond(in_tensor < 10, lambda: body(in_tensor, m),\n                                lambda: body(in_tensor + 1, m))\n          out_tensor = in_tensor + map_ops.tensor_map_size(result_m)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([0], dtype=np.int32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    expected_value = np.array([1], dtype=np.int32)\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(expected_value, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(expected_value, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testVariantsWithWhile(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'variants_with_while')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          m = map_ops.empty_tensor_map()\n\n          def cond(i, m):\n            del m\n            return i < 10\n\n          def body(i, m):\n            m = map_ops.tensor_map_insert(m, i, i)\n            return i + 1, m\n\n          _, result_m = tf.while_loop(cond, body, [0, m])\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.int32, name='input')\n          out_tensor = in_tensor + map_ops.tensor_map_size(result_m)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([0], dtype=np.int32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n  @test_util.run_v2_only\n  def testResources(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_resources')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          stack = tf.raw_ops.StackV2(max_size=10, elem_type=tf.float32)\n          w = tf.raw_ops.StackPushV2(handle=stack, elem=in_tensor)\n          with ops.control_dependencies([w]):\n            a = in_tensor + in_tensor\n            with ops.control_dependencies([a]):\n              out_tensor = a + tf.raw_ops.StackPopV2(\n                  handle=stack, elem_type=tf.float32)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(3.0, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(3.0, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(3.0, actual_value)\n\n  @test_util.run_v2_only\n  def testResourcesWithCond(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'resources_with_cond')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          def body(i, arr):\n            n = tf.raw_ops.StackPushV2(\n                handle=arr, elem=tf.cast(i, dtype=tf.float32))\n            return n, arr\n\n          arr = tf.raw_ops.StackV2(max_size=10, elem_type=tf.float32)\n          n, result_arr = tf.cond(in_tensor < 10, lambda: body(0, arr),\n                                  lambda: body(1, arr))\n\n          with ops.control_dependencies([result_arr, n]):\n            out_tensor = tf.raw_ops.StackPopV2(\n                handle=result_arr, elem_type=tf.float32)\n\n          inputs = {'x': in_tensor}\n          outputs = {'a': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(0.0, actual_value)\n\n  @test_util.run_v2_only\n  def testResourcesWithWhile(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(),\n                                     'resources_with_while')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          def cond(i, arr, m):\n            del arr\n            del m\n            return i < 10\n\n          def body(i, arr, m):\n            del m\n            n = tf.raw_ops.StackPushV2(\n                handle=arr, elem=tf.cast(i, dtype=tf.float32))\n            return i + 1, arr, n\n\n          arr = tf.raw_ops.StackV2(max_size=10, elem_type=tf.float32)\n          _, result_arr, n = tf.while_loop(cond, body, [0, arr, 0.0])\n\n          with ops.control_dependencies([result_arr, n]):\n            out_tensor = tf.raw_ops.StackPopV2(\n                handle=result_arr, elem_type=tf.float32)\n\n          inputs = {'x': in_tensor}\n          outputs = {'a': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(9.0, actual_value)\n\n  @parameterized.named_parameters(('EnableLoweringTensorListOps', True),\n                                  ('DisableLoweringTensorListOps', False))\n  @test_util.run_v2_only\n  def testTensorListWithStaticSize(self, lower_tensor_list_ops):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(),\n                                     'simple_mutable_variable')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          ta = tf.TensorArray(\n              tf.float32, size=3, dynamic_size=False, clear_after_read=False)\n          ta = ta.write(0, 10.0)\n          ta = ta.write(1, 20.0)\n          ta = ta.write(2, 30.0)\n\n          out_tensor = ta.read(0) + ta.read(2)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    if not lower_tensor_list_ops:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    converter._experimental_lower_tensor_list_ops = lower_tensor_list_ops\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(40.0, actual_value)\n\n  @parameterized.named_parameters(('EnableLoweringTensorListOps', True),\n                                  ('DisableLoweringTensorListOps', False))\n  @test_util.run_v2_only\n  def testTensorListWithDynamicSize(self, lower_tensor_list_ops):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(),\n                                     'simple_mutable_variable')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          ta = tf.TensorArray(\n              tf.float32, size=0, dynamic_size=True, clear_after_read=False)\n          ta = ta.write(0, 10.0)\n          ta = ta.write(1, 20.0)\n          ta = ta.write(2, 30.0)\n\n          out_tensor = ta.read(0) + ta.read(2)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    if lower_tensor_list_ops:\n      with self.assertRaises(convert.ConverterError) as error:\n        converter.convert()\n      self.assertIn(\n          'Lowering tensor list ops is failed. Please consider using Select '\n          'TF ops and disabling `_experimental_lower_tensor_list_ops` flag in '\n          'the TFLite converter object.', str(error.exception))\n\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(40.0, actual_value)\n\n\nclass CalibrateAndQuantizeWithCustomOpTest(lite_v2_test_util.ModelTest):\n\n  def _createGraphWithCustomOp(self):\n    # Create a graph that has one double op.\n    np.random.seed(0)\n\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'double_model')\n    with ops.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor = tf.compat.v1.placeholder(\n            shape=[1, 4], dtype=dtypes.float32, name='input')\n        out_tensor = double_op.double(in_tensor)\n        inputs = {'x': in_tensor}\n        outputs = {'z': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n\n    def calibration_gen():\n      for _ in range(100):\n        yield [np.random.uniform(-1, 1, size=(1, 4)).astype(np.float32)]\n\n    return (saved_model_dir, calibration_gen)\n\n  def testCustomOpRegistererByName(self):\n    \"\"\"Test a calibration with custom op registered by name.\"\"\"\n    saved_model_dir, calibration_gen = self._createGraphWithCustomOp()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.allow_custom_ops = True\n    converter.target_spec._experimental_custom_op_registerers = [\n        'TF_TestRegisterer'\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n    self.assertGreater(test_registerer.get_num_test_registerer_calls(), 0)\n    self.assertIn('Double', tflite_test_util.get_ops_list(tflite_model))\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.options.allowCustomOps, True)\n\n    # Check the model works with custom ops.\n    interpreter = InterpreterWithCustomOps(\n        model_content=tflite_model, custom_op_registerers=['TF_TestRegisterer'])\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    test_input = np.array([[0.0, 0.1, 0.2, 0.3]], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], test_input)\n    interpreter.invoke()\n\n    output_details = interpreter.get_output_details()\n    expected_output = np.array([[0.0, 0.2, 0.4, 0.6]], dtype=np.float32)\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(expected_output[0], output_data[0], err=1e-2)\n\n  def testCustomOpRegistererByFunc(self):\n    \"\"\"Test a calibration with custom op registered by function.\"\"\"\n    saved_model_dir, calibration_gen = self._createGraphWithCustomOp()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.allow_custom_ops = True\n    converter.target_spec._experimental_custom_op_registerers = [\n        test_registerer.TF_TestRegisterer\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n    self.assertGreater(test_registerer.get_num_test_registerer_calls(), 0)\n    self.assertIn('Double', tflite_test_util.get_ops_list(tflite_model))\n\n    # Check the model works with custom ops.\n    interpreter = InterpreterWithCustomOps(\n        model_content=tflite_model,\n        custom_op_registerers=[test_registerer.TF_TestRegisterer])\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    test_input = np.array([[0.0, 0.1, 0.2, 0.3]], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], test_input)\n    interpreter.invoke()\n\n    output_details = interpreter.get_output_details()\n    expected_output = np.array([[0.0, 0.2, 0.4, 0.6]], dtype=np.float32)\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(expected_output[0], output_data[0], err=1e-2)\n\n  def testCustomOpRegistererFailure(self):\n    \"\"\"Test a calibration with wrong custom op registerer.\"\"\"\n    saved_model_dir, calibration_gen = self._createGraphWithCustomOp()\n\n    bogus_name = 'CompletelyBogusRegistererName'\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.allow_custom_ops = True\n    converter.target_spec._experimental_custom_op_registerers = [bogus_name]\n\n    with self.assertRaisesRegex(\n        ValueError, 'Looking up symbol \\'' + bogus_name + '\\' failed'):\n      converter.convert()\n\n\nclass IntermediatesTest(lite_v2_test_util.ModelTest):\n\n  def _run(self, experimental_preserve_all_tensors):\n\n    @tf.function\n    def f(x):\n      y = tf.add(x, x, name='y')\n      z = tf.add(y, y, name='z')\n      w = tf.add(z, z, name='w')\n      return w\n\n    # NOTE this is exactly representable as a float as are the intermeidates of\n    # f. So direct comparison is ok below.\n\n    input_data = np.array(2.0, np.float32)\n    concrete_func = f.get_concrete_function(input_data)\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               f)\n    tflite_model = converter.convert()\n    interpreter = Interpreter(\n        model_content=tflite_model,\n        experimental_preserve_all_tensors=experimental_preserve_all_tensors)\n    interpreter.allocate_tensors()\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'],\n                           input_data)\n    interpreter.invoke()\n    out = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\n    tensors = {}\n    for t in interpreter.get_tensor_details():\n      # With Tensorflow Lite default delegate applied to the model graph, the\n      # access to original tensors of a delegated op could cause a ValueError\n      # (i.e. 'Tensor data is null. Run allocate_tensors() first') to be thrown\n      # out because the tensor memory isn't allocated at all.\n      val = None\n      try:\n        val = interpreter.get_tensor(t['index'])\n      except ValueError:\n        pass\n      tensors.update({t['name']: val})\n    return (tensors, out)\n\n  def testPreserve(self):\n    tensors, result = self._run(experimental_preserve_all_tensors=True)\n    # All intermediates should be true and result be true.\n    self.assertAllClose(tensors['x'], 2.0)\n    self.assertAllClose(tensors['y'], 4.0)\n    self.assertAllClose(tensors['z'], 8.0)\n    self.assertAllClose(result, 16.0)\n\n  def testNoPreserve(self):\n    tensors, result = self._run(experimental_preserve_all_tensors=False)\n    # One of them should be wrong if preserve is not true, but result should be\n    # ok. Input should still be ok for repeated invocation.\n    self.assertAllClose(tensors['x'], 2.0)\n    self.assertTrue(tensors['y'] != 4.0 or tensors['z'] != 8.0)\n    self.assertAllClose(result, 16.0)\n\n\nclass DatasetOpsTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testReduceDataset(self):\n\n    @tf.function\n    def model():\n      dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4])\n      output = dataset.reduce(np.int32(0), lambda x, y: x + y)\n      return output\n\n    concrete_func = model.get_concrete_function()\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n\nclass SparsityTest(lite_v2_test_util.ModelTest):\n\n  def _getSparsificableModel(self, matrix_b_values):\n    np.random.seed(0)\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[16, 4], dtype=tf.float32)])\n    def func(inp):\n      matrix_b = tf.constant(matrix_b_values, dtype=tf.float32)\n      matrix_b = tf.reshape(matrix_b, [4, 8])\n      matmul = tf.matmul(inp, matrix_b, transpose_a=False, transpose_b=False)\n      output = tf.nn.relu(matmul, name='output')\n      return output\n\n    root.f = func\n    to_save = root.f.get_concrete_function()\n    return (root, to_save)\n\n  def testRandomSparsity(self):\n    matrix_b_values = [\n        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 1\n    ]\n    root, func = self._getSparsificableModel(matrix_b_values)\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_converter.optimizations = [lite.Optimize.EXPERIMENTAL_SPARSITY]\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(float_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.RANDOM_SPARSITY],\n                        metadata.options.modelOptimizationModes)\n\n  def testBlockSparsity(self):\n    matrix_b_values = [\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 0\n    ]\n    root, func = self._getSparsificableModel(matrix_b_values)\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_converter.optimizations = [lite.Optimize.EXPERIMENTAL_SPARSITY]\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(float_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.BLOCK_SPARSITY],\n                        metadata.options.modelOptimizationModes)\n\n  def testQuantizedBlockSparsity(self):\n    weight_values = np.array([\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [3, 0, 7, 0, 0, 0, -6, -2, 0, 0, 0, 0, 0, -2, 0, 6],\n    ])\n\n    custom_init = tf.constant_initializer(weight_values.transpose())\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(\n            8, kernel_initializer=custom_init, input_shape=[16])\n    ])\n\n    def calibration_gen():\n      for _ in range(10):\n        yield [np.random.uniform(-1, 1, size=(1, 16)).astype(np.float32) * 16]\n\n    quantized_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    quantized_converter.optimizations = [\n        lite.Optimize.EXPERIMENTAL_SPARSITY, lite.Optimize.DEFAULT\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(\n        metadata.environment.tensorflowVersion.decode('utf-8'),\n        versions.__version__)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertAllEqual([\n        metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER,\n        metadata_fb.ModelOptimizationMode.BLOCK_SPARSITY,\n    ], metadata.options.modelOptimizationModes)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.allocate_tensors()\n    input_data = np.array(\n        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]],\n        dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(\n        np.array([0, 87, 0, 0, 0, 0, 0, 34], dtype=np.float32),\n        actual_value.flatten(),\n        err=1)\n\n  def testQuantizedButNotEnoughBlockSparsity(self):\n    # Sparsity level is 25%, which is not enough to apply the sparse conversion.\n    weight_values = np.array(\n        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [4, 4, -3, 4, 4, 1, -2, -2, 1, 3, 4, 1, 1, 1, -4, -5],\n         [1, 1, 5, -1, 3, -1, 1, -3, 4, -3, 2, -3, 3, -1, 3, -4],\n         [0, -3, -2, 5, 4, 2, 1, 4, -4, 4, 1, -2, 3, -2, -2, -1]])\n\n    custom_init = tf.constant_initializer(weight_values.transpose())\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(\n            4, kernel_initializer=custom_init, input_shape=[16])\n    ])\n\n    def calibration_gen():\n      for _ in range(10):\n        yield [np.random.uniform(-1, 1, size=(1, 16)).astype(np.float32) * 16]\n\n    quantized_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    quantized_converter.optimizations = [\n        lite.Optimize.EXPERIMENTAL_SPARSITY, lite.Optimize.DEFAULT\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(\n        metadata.environment.tensorflowVersion.decode('utf-8'),\n        versions.__version__)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertAllEqual([\n        metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER,\n    ], metadata.options.modelOptimizationModes)\n    self.assertNotIn(metadata_fb.ModelOptimizationMode.RANDOM_SPARSITY,\n                     metadata.options.modelOptimizationModes)\n    self.assertNotIn(metadata_fb.ModelOptimizationMode.BLOCK_SPARSITY,\n                     metadata.options.modelOptimizationModes)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.allocate_tensors()\n    input_data = np.array(\n        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]],\n        dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(\n        np.array([0, -3, 4, 35], dtype=np.float32),\n        actual_value.flatten(),\n        err=1)\n\nif __name__ == '__main__':\n  test.main()\n"], "fixing_code": ["/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/compiler/mlir/lite/quantization/quantization_utils.h\"\n\n#include <algorithm>\n#include <cstdint>\n#include <iterator>\n#include <limits>\n#include <memory>\n#include <numeric>\n#include <string>\n\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/SmallVector.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/FakeQuantSupport.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/QuantOps.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/QuantTypes.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/QuantizeUtils.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/UniformSupport.h\"  // from @llvm-project\n#include \"mlir/IR/Attributes.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinAttributes.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinTypes.h\"  // from @llvm-project\n#include \"mlir/IR/Diagnostics.h\"  // from @llvm-project\n#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n#include \"mlir/Support/LogicalResult.h\"  // from @llvm-project\n#include \"tensorflow/compiler/mlir/lite/quantization/quantization_traits.h\"\n#include \"tensorflow/lite/kernels/internal/tensor_utils.h\"\n#include \"tensorflow/lite/tools/optimize/quantization_utils.h\"\n\nnamespace mlir {\n\n// This includes the interface class definition. It couldn't be in a namespace\n// because the table gen doesn't emit the namespace when it is used.\n#include \"tensorflow/compiler/mlir/lite/quantization/quantization_interface.cc.inc\"\n\nnamespace quant {\n\nnamespace {\nconstexpr double kSmallestHalfRange = kNearZeroTolerance / 2;\nusing QType = quant::QuantizedType;\n\n// This method expands the range to be larger than or equal to 1.0e-6, if it is\n// very small (< 1.0e-6). This is to prevent very large quantized value by this\n// range.\nvoid ExpandVerySmallRange(ArrayRef<double> mins, ArrayRef<double> maxs,\n                          SmallVectorImpl<double>* effective_mins,\n                          SmallVectorImpl<double>* effective_maxs) {\n  for (auto arg : llvm::zip(mins, maxs)) {\n    double min = std::get<0>(arg);\n    double max = std::get<1>(arg);\n    // The range is wide, then use the same min/max.\n    if ((max - min) > kNearZeroTolerance) {\n      effective_mins->push_back(min);\n      effective_maxs->push_back(max);\n      continue;\n    }\n\n    // The range is small. Expands the range to stride 0.0 and also at least\n    // 1.0e-6.\n    effective_mins->push_back(std::min(min, -kSmallestHalfRange));\n    effective_maxs->push_back(std::max(max, kSmallestHalfRange));\n  }\n}\n\n// Set the min / max, scale and zero_points from the fake quant num_bits\n// attribute from QAT.\nQuantizedType ResetMinMaxFromNumBits(QuantizedType type, int num_bits,\n                                     bool narrow_range, bool is_signed) {\n  if (num_bits >= 8) {\n    return type;\n  }\n  int64_t qmin = QType::getDefaultMinimumForInteger(is_signed, num_bits);\n  int64_t qmax = QType::getDefaultMaximumForInteger(is_signed, num_bits);\n  if (narrow_range) {\n    qmin += 1;\n  }\n  const int64_t storage_type_min = type.getStorageTypeMin();\n  const int64_t storage_type_max = type.getStorageTypeMax();\n  const double rate =\n      static_cast<double>(storage_type_max - storage_type_min) / (qmax - qmin);\n  const auto& recalculate_scale = [&](double scale) -> double {\n    return scale * rate;\n  };\n  const auto& recalculate_zero_point = [&](int64_t zero_point) -> int64_t {\n    return qmax - std::round((storage_type_max - zero_point) / rate);\n  };\n  if (auto q_type = type.dyn_cast<UniformQuantizedType>()) {\n    const double scale = recalculate_scale(q_type.getScale());\n    const double zero_point = recalculate_zero_point(q_type.getZeroPoint());\n    return UniformQuantizedType::get(q_type.getFlags(), q_type.getStorageType(),\n                                     q_type.getExpressedType(), scale,\n                                     zero_point, qmin, qmax);\n  } else if (auto q_type = type.dyn_cast<UniformQuantizedPerAxisType>()) {\n    const int size = q_type.getScales().size();\n    SmallVector<double, 4> scales(size);\n    SmallVector<int64_t, 4> zero_points(size);\n    for (int i = 0; i < size; ++i) {\n      scales[i] = recalculate_scale(q_type.getScales()[i]);\n      zero_points[i] = recalculate_zero_point(q_type.getZeroPoints()[i]);\n    }\n    return UniformQuantizedPerAxisType::get(\n        q_type.getFlags(), q_type.getStorageType(), q_type.getExpressedType(),\n        scales, zero_points, q_type.getQuantizedDimension(), qmin, qmax);\n  } else {\n    llvm_unreachable(\"Unsupported QuantizedType in ResetMinMaxFromNumBits\");\n  }\n  return type;\n}\n\n// Repeats the content of `data` multiple times to resize to `target_size`.\n// Note that this only broadcast across one dimension.\ntemplate <typename T>\nbool BroadcastVector(int target_size, SmallVectorImpl<T>& data) {\n  int size = data.size();\n  if (size != target_size) {\n    if (target_size % size != 0) return true;\n    data.reserve(target_size);\n    for (int i = 1, e = target_size / size; i != e; ++i) {\n      data.insert(data.end(), data.begin(), data.begin() + size);\n    }\n  }\n  return false;\n}\n\n// Changes the axis of the input per-channel quantized type to match the\n// dimension of the target type. Returns nullptr if it fails.\nquant::UniformQuantizedPerAxisType ResetAxisAndBroadcast(\n    ArrayRef<int64_t> shape, quant::UniformQuantizedPerAxisType qtype,\n    Type target, int quant_dim) {\n  auto shaped = target.dyn_cast<RankedTensorType>();\n  if (!shaped) return {};\n  ArrayRef<int64_t> new_shape = shaped.getShape();\n\n  SmallVector<double, 4> scales(qtype.getScales().begin(),\n                                qtype.getScales().end());\n  SmallVector<int64_t, 4> zero_points(qtype.getZeroPoints().begin(),\n                                      qtype.getZeroPoints().end());\n\n  if (new_shape.size() == shape.size()) {  // same rank\n    // Broadcast the scales and zero points to match the target size, which is\n    // usually the axis-th dimension of the target type. Currently, it covers\n    // two cases:\n    // - for Transpose, the data layout is changed so the `dim[axis]` still\n    // equals to the `scales_size`. The broadcast skips;\n    // - for Reshape, the data layout isn't changed but the innermost dimension\n    // is expand to cover the last two original dimensions. Thus we just need to\n    // be repeated the `scales` dim[2] times to covers the new dim length.\n    //\n    // TODO(b/141709944): after the fix, the `scales` can be for dim[2], thus we\n    // have to repeat each elements in the `scales` locally dim[3] times.\n    if (BroadcastVector<double>(shaped.getDimSize(quant_dim), scales) ||\n        BroadcastVector<int64_t>(shaped.getDimSize(quant_dim), zero_points)) {\n      return {};\n    }\n  } else if ((new_shape.size() == shape.size() + 1) && new_shape.front() == 1) {\n    // Handle the [A, B, C] -> [1, A, B, C] reshape case.\n    if (!(std::equal(shape.begin(), shape.end(), new_shape.begin() + 1) &&\n          quant_dim == new_shape.size() - 1)) {\n      return {};\n    }\n  } else {\n    return {};\n  }\n\n  return quant::UniformQuantizedPerAxisType::get(\n      qtype.getFlags(), qtype.getStorageType(), qtype.getExpressedType(),\n      scales, zero_points, quant_dim, qtype.getStorageTypeMin(),\n      qtype.getStorageTypeMax());\n}\n\n}  // namespace\n\nbool IsOpNotQuantizable(Operation* op) {\n  // If it is terminator or not quantizable or any ops form the mlir quant\n  // ops dialect, we shouldn't rewrite.\n  bool attr_enforced_quantizable =\n      op->hasAttrOfType<StringAttr>(kQuantTraitAttrName) &&\n      op->getAttrOfType<StringAttr>(kQuantTraitAttrName).getValue().str() ==\n          QuantTraitValues[QuantizationTrait::FullyQuantizable];\n\n  // Constant ops do not have QuantizableResult attribute but they can deal with\n  // quantized tensors.\n  if (llvm::isa<func::ConstantOp, arith::ConstantOp, quant::StatisticsOp>(op))\n    return false;\n\n  bool prop_enforced_quantizable =\n      op->hasTrait<OpTrait::quant::QuantizableResult>();\n\n  return op->hasTrait<OpTrait::IsTerminator>() ||\n         llvm::isa<quant::QuantizeCastOp, quant::DequantizeCastOp>(op) ||\n         (!attr_enforced_quantizable && !prop_enforced_quantizable);\n}\n\n// Returns the quantized type for the\n// input_type/min/max/storag_type_width/narrow_range.\n// This is entry point to the Quant dialect and used for both quantizing\n// activations and weights.\nType GetQuantizedType(Builder builder, Type input_type, ArrayRef<double> min,\n                      ArrayRef<double> max, int quant_dim,\n                      int storage_type_width, bool narrow_range, bool is_signed,\n                      bool legacy_float_scale, bool use_fake_quant_num_bits) {\n  auto converter =\n      quant::ExpressedToQuantizedConverter::forInputType(input_type);\n\n  // Expand the range to prevent extremely small scales and large quantized\n  // integers which can cause overflow. This leads to scale\n  // 7.843137254901961e-9 with 8 bits.\n  SmallVector<double, 4> effective_mins, effective_maxs;\n  ExpandVerySmallRange(min, max, &effective_mins, &effective_maxs);\n\n  quant::QuantizedType quantizedEleType;\n  if (min.size() == 1 && max.size() == 1 && quant_dim == -1) {\n    quantizedEleType = quant::fakeQuantAttrsToType(\n        builder.getUnknownLoc(), storage_type_width, effective_mins[0],\n        effective_maxs[0], narrow_range, converter.expressedType, is_signed);\n    if (legacy_float_scale) {\n      quantizedEleType =\n          DownCastScale(quantizedEleType, effective_mins[0], effective_maxs[0],\n                        builder.getUnknownLoc());\n    }\n  } else if (min.size() == max.size()) {\n    auto shape = input_type.dyn_cast<ShapedType>();\n    if (!shape || shape.getRank() <= quant_dim ||\n        static_cast<int64_t>(min.size()) != shape.getDimSize(quant_dim)) {\n      return {};\n    }\n    // The quantization dim is set to the last dimension.\n    quantizedEleType = quant::fakeQuantAttrsToType(\n        builder.getUnknownLoc(), storage_type_width, quant_dim, effective_mins,\n        effective_maxs, narrow_range, converter.expressedType, is_signed);\n    if (legacy_float_scale) {\n      quantizedEleType = DownCastScale(quantizedEleType, effective_mins,\n                                       effective_maxs, builder.getUnknownLoc());\n    }\n  }\n  if (!quantizedEleType) return {};\n  // Use fake quant configured bit-widths (only supported for\n  // 1 < num_bits < 8 bits) instead of using 8bit defaults.\n  if (use_fake_quant_num_bits && (storage_type_width > 1) &&\n      (storage_type_width < 8) &&\n      (quantizedEleType.getStorageTypeMax() >\n       QType::getDefaultMinimumForInteger(is_signed, storage_type_width))) {\n    auto resetEleType = ResetMinMaxFromNumBits(\n        quantizedEleType, storage_type_width, narrow_range, is_signed);\n    return converter.convert(resetEleType);\n  }\n  return converter.convert(quantizedEleType);\n}\n\n// TODO(fengliuai): promote this utility method to mlir QuantOps.\nTypeAttr RescaleQuantizedType(Type input, Attribute factor) {\n  auto factor_values = factor.dyn_cast_or_null<DenseFPElementsAttr>();\n  if (!factor_values) return {};\n  auto ele_type = quant::QuantizedType::getQuantizedElementType(input);\n  if (!ele_type) return {};\n  if (auto qtype = ele_type.dyn_cast<quant::UniformQuantizedPerAxisType>()) {\n    ArrayRef<double> scales = qtype.getScales();\n    // Broadcasting hasn't been implemented yet.\n    if (static_cast<int64_t>(scales.size()) != factor_values.getNumElements())\n      return {};\n    SmallVector<double, 4> new_scales;\n    new_scales.reserve(scales.size());\n    auto scales_iter = scales.begin();\n    for (const auto& f : factor_values) {\n      new_scales.push_back(*(scales_iter++) *\n                           std::fabs(FloatAttr::getValueAsDouble(f)));\n    }\n    // We are assuming symmetric quantization.\n    auto new_ele_type = quant::UniformQuantizedPerAxisType::get(\n        qtype.getFlags(), qtype.getStorageType(), qtype.getExpressedType(),\n        new_scales, qtype.getZeroPoints(), qtype.getQuantizedDimension(),\n        qtype.getStorageTypeMin(), qtype.getStorageTypeMax());\n    if (auto new_type = new_ele_type.castFromExpressedType(\n            quant::QuantizedType::castToExpressedType(input))) {\n      return TypeAttr::get(new_type);\n    }\n  }\n  // Currently, we only support per-axis quantized type.\n  return {};\n}\n\nTypeAttr GetQuantizedTypeAttr(Builder builder, Type input_type, Attribute min,\n                              Attribute max, int quant_dim,\n                              IntegerAttr num_bits, BoolAttr narrow_range,\n                              bool is_signed, bool legacy_float_scale,\n                              bool use_fake_quant_num_bits) {\n  SmallVector<double, 4> min_value, max_value;\n  auto mins = min.dyn_cast<DenseFPElementsAttr>();\n  auto maxs = max.dyn_cast<DenseFPElementsAttr>();\n  if (mins && maxs) {\n    min_value.reserve(mins.getNumElements());\n    max_value.reserve(maxs.getNumElements());\n    for (auto it = mins.begin(), e = mins.end(); it != e; ++it) {\n      min_value.push_back(FloatAttr::getValueAsDouble(*it));\n    }\n    for (auto it = maxs.begin(), e = maxs.end(); it != e; ++it) {\n      max_value.push_back(FloatAttr::getValueAsDouble(*it));\n    }\n  } else {\n    auto fmin = min.dyn_cast<FloatAttr>();\n    auto fmax = max.dyn_cast<FloatAttr>();\n    if (fmin && fmax) {\n      min_value.push_back(fmin.getValueAsDouble());\n      max_value.push_back(fmax.getValueAsDouble());\n    } else {\n      return {};\n    }\n  }\n  Type final_type =\n      GetQuantizedType(builder, input_type, min_value, max_value, quant_dim,\n                       num_bits.getInt(), narrow_range.getValue(), is_signed,\n                       legacy_float_scale, use_fake_quant_num_bits);\n  if (!final_type) return {};\n  return TypeAttr::get(final_type);\n}\n\nTypeAttr CastQuantizedTypeAttrFromExpressedType(Builder builder,\n                                                TypeAttr source, Type target,\n                                                int axis) {\n  auto source_type = source.getValue().dyn_cast_or_null<ShapedType>();\n  if (!source_type) return {};\n  auto src_ele_type = source_type.getElementType();\n  auto qtype = src_ele_type.dyn_cast<quant::QuantizedType>();\n\n  // Reset the quantization dimensions if it is per-axis.\n  if (auto per_axis =\n          qtype.dyn_cast_or_null<quant::UniformQuantizedPerAxisType>()) {\n    // For the pass-through ops, we don't know which the dimension will be the\n    // new quantization dimension. Only if the new quantization dimension can\n    // be inferred, it is safe to reset the per-axis quantized type.\n    if (axis == -1) return {};\n    qtype =\n        ResetAxisAndBroadcast(source_type.getShape(), per_axis, target, axis);\n  }\n  if (!qtype) return {};\n  Type final_type = qtype.castFromExpressedType(target);\n  if (!final_type) return {};\n  return TypeAttr::get(final_type);\n}\n\nvoid ExtractMinMaxFromAttr(DenseFPElementsAttr values, int dim_size,\n                           int slice_size, bool symmetric,\n                           SmallVectorImpl<double>& mins,\n                           SmallVectorImpl<double>& maxs) {\n  // If all the element values are same we don't need to scan the content.\n  if (values.isSplat()) {\n    double single_value =\n        FloatAttr::getValueAsDouble(values.getSplatValue<llvm::APFloat>());\n\n    // When the single value isn't 0.0, we expand it to a range to include\n    // this single value and 0.0. This will give us a scale and zero point\n    // works for both this value and 0.0.\n    if (single_value < 0.0) {\n      mins[0] = single_value;\n      maxs[0] = symmetric ? -single_value : 0.0;\n    } else if (single_value > 0.0) {\n      mins[0] = symmetric ? -single_value : 0.0;\n      maxs[0] = single_value;\n    } else {\n      mins[0] = maxs[0] = single_value;\n    }\n    for (int i = 1; i < dim_size; ++i) {\n      mins[i] = mins[0];\n      maxs[i] = maxs[0];\n    }\n  } else {\n    int64_t flatten_index = 0;\n    for (auto it = values.begin(), e = values.end(); it != e;\n         ++it, ++flatten_index) {\n      double ele_value = FloatAttr::getValueAsDouble(*it);\n      int slice_index = flatten_index / slice_size;\n      int channel_index = slice_index % dim_size;\n      mins[channel_index] = std::min(mins[channel_index], ele_value);\n      maxs[channel_index] = std::max(maxs[channel_index], ele_value);\n    }\n    // Expand range to include 0.\n    for (int i = 0; i < dim_size; ++i) {\n      maxs[i] = std::max(maxs[i], 0.0);\n      mins[i] = std::min(mins[i], 0.0);\n    }\n    if (symmetric) {\n      for (int i = 0; i < dim_size; ++i) {\n        maxs[i] = std::max(std::abs(mins[i]), std::abs(maxs[i]));\n        mins[i] = -maxs[i];\n      }\n    }\n  }\n}\n\nType GetUniformQuantizedTypeForWeight(ElementsAttr attr, bool symmetric,\n                                      unsigned num_bits, bool is_signed,\n                                      bool narrow_range,\n                                      bool legacy_float_scale,\n                                      bool use_fake_quant_num_bits) {\n  Builder builder(attr.getContext());\n  // `symmetric` can only be used when it is `signed` and `narrow_range`.\n  if (symmetric && (!is_signed || !narrow_range)) return {};\n\n  SmallVector<double, 4> mins(1, std::numeric_limits<double>::max());\n  SmallVector<double, 4> maxs(1, std::numeric_limits<double>::min());\n  auto fp = attr.dyn_cast<DenseFPElementsAttr>();\n  if (!fp) return {};\n\n  // Computes the effective min/max values of the attribute values.\n  ExtractMinMaxFromAttr(fp, /*dim_size=*/1, /*slice_size=*/1, symmetric, mins,\n                        maxs);\n\n  auto type =\n      GetQuantizedType(builder, attr.getType(), mins[0], maxs[0],\n                       /*quant_dim=*/-1, num_bits, narrow_range, is_signed,\n                       legacy_float_scale, use_fake_quant_num_bits);\n  if (auto ele_type = type.dyn_cast_or_null<TensorType>())\n    return ele_type.getElementType();\n\n  return {};\n}\n\nType GetUniformQuantizedPerAxisTypeForWeight(ElementsAttr attr, int quant_dim,\n                                             bool symmetric, unsigned num_bits,\n                                             bool is_signed, bool narrow_range,\n                                             bool legacy_float_scale,\n                                             bool use_fake_quant_num_bits) {\n  Builder builder(attr.getContext());\n  auto shape = attr.getType().cast<ShapedType>().getShape();\n  if (static_cast<int>(shape.size()) <= quant_dim) return {};\n  // `symmetric` can only be used when it is `signed` and `narrow_range`.\n  if (symmetric && (!is_signed || !narrow_range)) return {};\n\n  int dim_size = shape[quant_dim];\n  int slice_size = std::accumulate(std::next(shape.begin(), quant_dim + 1),\n                                   shape.end(), 1, std::multiplies<int64_t>());\n  SmallVector<double, 4> mins(dim_size, std::numeric_limits<double>::max());\n  SmallVector<double, 4> maxs(dim_size, std::numeric_limits<double>::min());\n  auto fp = attr.dyn_cast<DenseFPElementsAttr>();\n  if (!fp) return {};\n\n  // Computes the effective min/max values of the attribute values.\n  ExtractMinMaxFromAttr(fp, dim_size, slice_size, symmetric, mins, maxs);\n\n  auto type = GetQuantizedType(builder, attr.getType(), mins, maxs, quant_dim,\n                               num_bits, narrow_range, is_signed,\n                               legacy_float_scale, use_fake_quant_num_bits);\n  if (auto ele_type = type.dyn_cast_or_null<TensorType>())\n    return ele_type.getElementType();\n\n  return {};\n}\n\nquant::QuantizedType GetUniformQuantizedTypeForBias(\n    const std::vector<quant::QuantizedType>& op_types,\n    bool legacy_float_scale) {\n  if (op_types.empty()) return {};\n\n  size_t axis_size = 1;\n  int32_t quant_dim = -1;\n  Type expressed_type;\n  // Requires all the op types are valid UniformQuantizedTypes or\n  // UniformQuantizedPerAxisTypes and also have same expressed type. For all\n  // the UniformQuantizedPerAxisTypes, the quantization dimension index and\n  // dimension sizes are same.\n  for (auto op_type : op_types) {\n    if (!op_type) return {};\n    if (expressed_type && expressed_type != op_type.getExpressedType()) {\n      return {};\n    }\n    expressed_type = op_type.getExpressedType();\n\n    if (auto type = op_type.dyn_cast<quant::UniformQuantizedPerAxisType>()) {\n      if ((axis_size != 1 && axis_size != type.getScales().size())) return {};\n      if (quant_dim != -1 && quant_dim != type.getQuantizedDimension())\n        return {};\n      axis_size = type.getScales().size();\n      quant_dim = type.getQuantizedDimension();\n    } else if (!op_type.isa<quant::UniformQuantizedType>()) {\n      return {};\n    }\n  }\n\n  // The scale from the UniformQuantizedTypes is broadcasted if there are\n  // UniformQuantizedPerAxisTypes.\n  llvm::SmallVector<double, 4> scales(axis_size, 1.0);\n  for (auto op_type : op_types) {\n    if (auto type = op_type.dyn_cast<quant::UniformQuantizedPerAxisType>()) {\n      for (const auto& index_scale : llvm::enumerate(type.getScales())) {\n        scales[index_scale.index()] *= index_scale.value();\n      }\n    } else if (auto type = op_type.dyn_cast<quant::UniformQuantizedType>()) {\n      for (int index = 0, e = axis_size; index != e; ++index) {\n        scales[index] *= type.getScale();\n      }\n    }\n  }\n  if (legacy_float_scale) {\n    for (int i = 0; i < scales.size(); ++i) {\n      scales[i] = static_cast<float>(scales[i]);\n    }\n  }\n\n  // Builds the result quantized type, which has signed 32 bits storage type.\n  Builder builder(expressed_type.getContext());\n  IntegerType storage_type = builder.getIntegerType(32);\n  int64_t storage_type_min =\n      quant::QuantizedType::getDefaultMinimumForInteger(/*isSigned=*/true, 32);\n  int64_t storage_type_max =\n      quant::QuantizedType::getDefaultMaximumForInteger(/*isSigned=*/true, 32);\n  if (axis_size == 1) {\n    return quant::UniformQuantizedType::getChecked(\n        builder.getUnknownLoc(),\n        /*flags=*/true, storage_type, expressed_type, scales[0],\n        /*zeroPoint=*/0, storage_type_min, storage_type_max);\n  } else {\n    llvm::SmallVector<int64_t, 4> zero_points(axis_size, 0);\n    // Assume the bias is a 1-D tensor, and set the quantization dim to the last\n    // dimension, which is 0. If the bias rank is larger than 1, this returned\n    // quantized type couldn't be used to quantize the bias.\n    return quant::UniformQuantizedPerAxisType::getChecked(\n        builder.getUnknownLoc(),\n        /*flags=*/true, storage_type, expressed_type, scales, zero_points,\n        /*quantizedDimension=*/0, storage_type_min, storage_type_max);\n  }\n}\n\nElementsAttr QuantizeLegacy(Attribute real_value, Type tensor_type) {\n  if (!real_value.isa<DenseFPElementsAttr>() ||\n      !quant::QuantizedType::getQuantizedElementType(tensor_type)) {\n    return {};\n  }\n  auto real_values_attr = real_value.cast<DenseFPElementsAttr>();\n  auto q_type = quant::QuantizedType::getQuantizedElementType(tensor_type);\n  std::vector<float> real_values;\n  llvm::SmallVector<APInt, 8> quantized_attr;\n  real_values.reserve(real_values_attr.getNumElements());\n  quantized_attr.reserve(real_values_attr.getNumElements());\n  std::transform(real_values_attr.begin(), real_values_attr.end(),\n                 std::back_inserter(real_values), [&](APFloat value) -> float {\n                   return value.convertToFloat();\n                 });\n  ShapedType new_dense_type =\n      q_type.castExpressedToStorageType(real_values_attr.getType())\n          .dyn_cast_or_null<ShapedType>();\n  int width = q_type.getStorageType().dyn_cast<mlir::IntegerType>().getWidth();\n\n  if (width == 8 && q_type.getStorageTypeMax() == 127 &&\n      q_type.getStorageTypeMin() == -127) {\n    std::vector<int8_t> quantized_values(real_values_attr.getNumElements());\n    if (auto uniform_type = q_type.dyn_cast<UniformQuantizedType>()) {\n      float min, max, scale;\n      tflite::tensor_utils::SymmetricQuantizeFloats(\n          real_values.data(), real_values.size(), quantized_values.data(), &min,\n          &max, &scale);\n      // The scale has been adjusted, so the adjusted scale should be respected.\n      if (std::abs(scale - uniform_type.getScale()) > 1e-3) {\n        return Quantize(real_value, tensor_type);\n      }\n    } else if (auto uniform_type =\n                   q_type.dyn_cast<UniformQuantizedPerAxisType>()) {\n      std::vector<float> scales_inv;\n      std::vector<int32_t> dimension;\n      dimension.insert(dimension.end(), new_dense_type.getShape().begin(),\n                       new_dense_type.getShape().end());\n      std::transform(uniform_type.getScales().begin(),\n                     uniform_type.getScales().end(),\n                     std::back_inserter(scales_inv),\n                     [](float scale) { return 1.0 / scale; });\n\n      tflite::optimize::utils::SymmetricPerChannelQuantizeValues(\n          real_values.data(), scales_inv, dimension,\n          uniform_type.getQuantizedDimension(), &quantized_values);\n    } else {\n      return {};\n    }\n    std::transform(quantized_values.begin(), quantized_values.end(),\n                   std::back_inserter(quantized_attr),\n                   [&](int8_t value) -> APInt {\n                     return APInt(8, value, /*isSigned=*/true);\n                   });\n    return DenseElementsAttr::get(new_dense_type, quantized_attr);\n  } else if (width == 8) {\n    // This can be a state tensor, or an actual constant tensor with\n    // asymmetric range. For a state tensor, assigining correct quantization\n    // parameters is sufficient, and for constants with asymmetric range it's\n    // not correctly quantized by legacy quantizer so call the new Quantize.\n    return Quantize(real_value, tensor_type);\n  } else if (width == 16) {\n    if (auto uniform_type = q_type.dyn_cast<UniformQuantizedType>()) {\n      auto quantized_values =\n          tflite::optimize::utils::SymmetricQuantizeFloatsToInt16(\n              real_values.data(), real_values.size(), uniform_type.getScale());\n      std::transform(quantized_values.begin(), quantized_values.end(),\n                     std::back_inserter(quantized_attr),\n                     [&](int16_t value) -> APInt {\n                       return APInt(16, value, /*isSigned=*/true);\n                     });\n      return DenseElementsAttr::get(new_dense_type, quantized_attr);\n    }\n  } else if (width == 32) {\n    std::vector<float> scales;\n    if (auto uniform_type = q_type.dyn_cast<UniformQuantizedType>()) {\n      scales.push_back(uniform_type.getScale());\n    } else if (auto uniform_type =\n                   q_type.dyn_cast<UniformQuantizedPerAxisType>()) {\n      scales.insert(scales.end(), uniform_type.getScales().begin(),\n                    uniform_type.getScales().end());\n    } else {\n      return {};\n    }\n    auto quantized_bias =\n        tflite::optimize::utils::SymmetricBiasQuantize<std::int32_t>(\n            real_values.data(), real_values.size(), scales);\n    std::transform(quantized_bias.begin(), quantized_bias.end(),\n                   std::back_inserter(quantized_attr),\n                   [&](int32_t value) -> APInt {\n                     return APInt(32, value, /*isSigned=*/true);\n                   });\n    return DenseElementsAttr::get(new_dense_type, quantized_attr);\n  }\n  return {};\n}\n\nElementsAttr Quantize(Attribute real_value, Type tensor_type) {\n  if (auto q_type =\n          quant::QuantizedType::getQuantizedElementType(tensor_type)) {\n    Type converted_type;\n    return quant::quantizeAttr(real_value, q_type, converted_type)\n        .dyn_cast<ElementsAttr>();\n  }\n  return {};\n}\n\nQuantizedType DownCastScale(QuantizedType type, double min, double max,\n                            Location loc) {\n  SmallVector<double, 1> mins = {min};\n  SmallVector<double, 1> maxs = {max};\n  return DownCastScale(type, mins, maxs, loc);\n}\n\nQuantizedType DownCastScale(QuantizedType type,\n                            const SmallVectorImpl<double>& mins,\n                            const SmallVectorImpl<double>& maxs, Location loc) {\n  // The given type can be null. For example, there can be an invalid scale and\n  // so on.\n  if (!type) return type;\n  SmallVector<double, 4> scales(mins.size());\n  SmallVector<int64_t, 4> zero_points(mins.size());\n  if (auto q_type = type.dyn_cast<UniformQuantizedType>()) {\n    zero_points.push_back(q_type.getZeroPoint());\n  } else if (auto q_type = type.dyn_cast<UniformQuantizedPerAxisType>()) {\n    zero_points = {q_type.getZeroPoints().begin(),\n                   q_type.getZeroPoints().end()};\n  }\n  for (int i = 0; i < mins.size(); ++i) {\n    scales[i] = (static_cast<float>(maxs[i]) - static_cast<float>(mins[i])) /\n                (type.getStorageTypeMax() - type.getStorageTypeMin());\n    if (type.getStorageTypeMax() != -type.getStorageTypeMin()) {\n      // Only applies for asymmetric quantized range with original scale.\n      float zero_point_from_min =\n          type.getStorageTypeMin() - mins[i] / scales[i];\n      if (zero_point_from_min < type.getStorageTypeMin()) {\n        zero_points[i] = static_cast<int64_t>(type.getStorageTypeMin());\n      } else if (zero_point_from_min > type.getStorageTypeMax()) {\n        zero_points[i] = static_cast<int64_t>(type.getStorageTypeMax());\n      } else {\n        zero_points[i] = static_cast<int64_t>(std::round(zero_point_from_min));\n      }\n    }\n  }\n  if (auto q_type = type.dyn_cast<UniformQuantizedType>()) {\n    return UniformQuantizedType::get(q_type.getFlags(), q_type.getStorageType(),\n                                     q_type.getExpressedType(), scales[0],\n                                     zero_points[0], q_type.getStorageTypeMin(),\n                                     q_type.getStorageTypeMax());\n  } else if (auto q_type = type.dyn_cast<UniformQuantizedPerAxisType>()) {\n    return UniformQuantizedPerAxisType::get(\n        q_type.getFlags(), q_type.getStorageType(), q_type.getExpressedType(),\n        scales, zero_points, q_type.getQuantizedDimension(),\n        q_type.getStorageTypeMin(), q_type.getStorageTypeMax());\n  }\n  return type;\n}\n\n// A heuristic to determine whether the scales needs to be from operands or\n// from results for the ops with the `SameOperandsAndResultsScale` property.\n// The current implementation is based on the number of operands.\nstatic bool PreferResultScale(Operation* op) {\n  int float_operands = 0;\n  for (auto operand : op->getOperands()) {\n    if (auto operand_type = operand.getType().dyn_cast<ShapedType>()) {\n      if (operand_type.getElementType().isa<FloatType>()) {\n        if (++float_operands > 1) return true;\n      }\n    }\n  }\n  return false;\n}\n\nstd::unique_ptr<OpQuantScaleSpec> GetDefaultQuantScaleSpec(Operation* op) {\n  auto spec = std::make_unique<OpQuantScaleSpec>();\n  if (llvm::isa<SameScalesOpInterface>(op)) {\n    spec->has_same_scale_requirement = true;\n    spec->required_same_scale_func = [op](bool sign, int bit_width) {\n      return llvm::cast<SameScalesOpInterface>(op)\n          .RequiredSameOperandsAndResultsScale(sign, bit_width);\n    };\n    spec->required_same_quantized_axes_func = [op]() {\n      return llvm::cast<SameScalesOpInterface>(op).RequiredSameQuantizedAxes();\n    };\n  }\n  if (llvm::isa<FixedOutputRangeInterface>(op)) {\n    spec->has_fixed_output_range = true;\n    spec->fixed_output_range_func = [op](bool sign, int bit_width) {\n      return llvm::cast<FixedOutputRangeInterface>(op).GetFixedOutputRange(\n          sign, bit_width);\n    };\n  }\n  return spec;\n}\n\n// The stats op of some of the ops can be redundant. The current implementation\n// only considers the ops with restricted output params.\nstatic bool IsStatsRedundant(\n    Operation* op, OpQuantSpecGetter op_quant_spec_getter,\n    OpQuantScaleSpecGetter op_quant_scale_spec_getter) {\n  // If it has FixedOutputRangeInterface, no need to manually create spec.\n  return llvm::isa<FixedOutputRangeInterface>(op) ||\n         op_quant_scale_spec_getter(op)->has_fixed_output_range;\n}\n\nstatic bool IsSameScaleOp(Operation* op,\n                          OpQuantScaleSpecGetter op_quant_scale_spec_getter) {\n  // If it has SameScalesOpInterface, no need to manually create spec.\n  return llvm::dyn_cast<SameScalesOpInterface>(op) ||\n         op_quant_scale_spec_getter(op)->has_same_scale_requirement;\n}\n\nbool RemoveRedundantStatsOps(\n    mlir::func::FuncOp func, OpQuantSpecGetter op_quant_spec_getter,\n    OpQuantScaleSpecGetter op_quant_scale_spec_getter) {\n  llvm::SmallVector<quant::StatisticsOp, 16> all_stats_ops;\n  llvm::DenseSet<Operation*> redundant_stats_ops;\n\n  // Step 0: remove the quant::StatisticsOp which are used by the quant.qcast\n  // op in case it overrides the information from training FakeQuant ops.\n  func.walk([&](quant::QuantizeCastOp q) {\n    auto input_op = q.getArg().getDefiningOp();\n    if (auto stats = llvm::dyn_cast_or_null<quant::StatisticsOp>(input_op)) {\n      q.setOperand(stats.getArg());\n      if (stats.use_empty()) stats.erase();\n    }\n  });\n\n  // Step 1: forward pass: propagate any value scales which are not produces\n  // by `SameOperandsAndResultsScale`. Additionally, remove the value scales\n  // which are produced by the ops with the `FixedOutputRangeInterface`.\n  // Note that we don't propagate across the multiple-operands\n  // `SameOperandsAndResultsScale` ops like `concatenation`.\n  func.walk(\n      [&](quant::StatisticsOp stats_op) { all_stats_ops.push_back(stats_op); });\n\n  while (!all_stats_ops.empty()) {\n    quant::StatisticsOp stats_op = all_stats_ops.back();\n    all_stats_ops.pop_back();\n\n    if (auto def = stats_op.getArg().getDefiningOp()) {\n      if (IsStatsRedundant(def, op_quant_spec_getter,\n                           op_quant_scale_spec_getter)) {\n        redundant_stats_ops.insert(stats_op);\n      }\n    }\n\n    for (auto user : stats_op.getResult().getUsers()) {\n      // We don't propagate this parameter down if it has multiple operands.\n      // We want to use the result parameter scales instead.\n      if (!IsSameScaleOp(user, op_quant_scale_spec_getter) ||\n          PreferResultScale(user)) {\n        continue;\n      }\n      for (Value res : user->getResults()) {\n        if (!res.hasOneUse()) {\n          continue;\n        }\n        if (auto next_stats =\n                llvm::dyn_cast<quant::StatisticsOp>(*res.getUsers().begin())) {\n          // quantization parameters can be propagated to next_stats\n          redundant_stats_ops.insert(next_stats);\n          // add next_stats to the work list so propagation can continue.\n          all_stats_ops.push_back(next_stats);\n        }\n      }\n    }\n  }\n\n  // Step 2: backward pass: For the ops skiped in the forward pass, propagate\n  // its results scale backwards as far as possible.\n  func.walk([&](quant::StatisticsOp stats_op) {\n    if (redundant_stats_ops.find(stats_op) == redundant_stats_ops.end()) {\n      all_stats_ops.push_back(stats_op);\n    }\n  });\n\n  while (!all_stats_ops.empty()) {\n    quant::StatisticsOp stats_op = all_stats_ops.back();\n    all_stats_ops.pop_back();\n\n    if (auto def = stats_op.getArg().getDefiningOp()) {\n      if (!IsSameScaleOp(def, op_quant_scale_spec_getter)) {\n        continue;\n      }\n      for (auto input : def->getOperands()) {\n        if (auto next_stats = llvm::dyn_cast_or_null<quant::StatisticsOp>(\n                input.getDefiningOp())) {\n          redundant_stats_ops.insert(next_stats);\n          all_stats_ops.push_back(next_stats);\n        }\n      }\n    }\n  }\n\n  // Step3: Remove all the redundant stats ops\n  for (auto it : redundant_stats_ops) {\n    if (!llvm::isa<quant::StatisticsOp>(it)) return true;\n    auto stats_op = llvm::cast<quant::StatisticsOp>(it);\n    stats_op.getResult().replaceAllUsesWith(stats_op.getArg());\n    stats_op.erase();\n  }\n\n  // Returns false if the steps finish without errors.\n  return false;\n}\n\nLogicalResult VerifySameScales(Operation* op) {\n  auto same_scale_op = llvm::cast<SameScalesOpInterface>(op);\n\n  llvm::SmallVector<QuantizedType, 4> collected_quant_params;\n  for (auto input : op->getOperands()) {\n    auto quant_params = QuantizedType::getQuantizedElementType(input.getType());\n    // Skip non-quantizable operands.\n    if (quant_params) {\n      collected_quant_params.push_back(quant_params);\n    }\n  }\n\n  for (auto output : op->getResults()) {\n    auto quant_params =\n        QuantizedType::getQuantizedElementType(output.getType());\n    // Skip non-quantizable results.\n    if (quant_params) {\n      collected_quant_params.push_back(quant_params);\n    }\n  }\n\n  if (collected_quant_params.size() <= 1) return success();\n  const auto& expected_params = collected_quant_params[0];\n  for (int i = 1; i < collected_quant_params.size(); i++) {\n    const auto& compared_params = collected_quant_params[i];\n    // For some ops (such as Transpose or Squeeze), the quantized axis might not\n    // be the same, this function only verifies the scale and zero point in\n    // that case. The quantized axis should be verified in their own verifier\n    // method.\n    if (!same_scale_op.RequiredSameQuantizedAxes()) {\n      auto expected_per_axis_qtype =\n          expected_params.dyn_cast<UniformQuantizedPerAxisType>();\n      auto compared_per_axis_qtype =\n          compared_params.dyn_cast<UniformQuantizedPerAxisType>();\n      if (expected_per_axis_qtype && compared_per_axis_qtype &&\n          llvm::equal(expected_per_axis_qtype.getScales(),\n                      compared_per_axis_qtype.getScales()) &&\n          llvm::equal(expected_per_axis_qtype.getZeroPoints(),\n                      compared_per_axis_qtype.getZeroPoints()) &&\n          expected_params.getStorageType() ==\n              compared_params.getStorageType() &&\n          expected_params.getExpressedType() ==\n              compared_params.getExpressedType()) {\n        continue;\n      }\n    }\n    // Same quantization parameters are always ok.\n    if (expected_params == compared_params) continue;\n    // If the quantization parameters are not the same, as long as it has the\n    // same storage type and the op interface doesn't require same scale\n    // constraint for this storage type, it is still ok.\n    if ((expected_params.isSigned() == compared_params.isSigned() &&\n         expected_params.getStorageTypeIntegralWidth() ==\n             compared_params.getStorageTypeIntegralWidth()) &&\n        !same_scale_op.RequiredSameOperandsAndResultsScale(\n            expected_params.isSigned(),\n            expected_params.getStorageTypeIntegralWidth()))\n      continue;\n\n    std::string err_msg =\n        \"quantization parameters violate the same scale constraint: \";\n    llvm::raw_string_ostream os(err_msg);\n    expected_params.print(os);\n    os << \" vs. \";\n    compared_params.print(os);\n    os.flush();\n    return op->emitOpError(err_msg);\n  }\n  return success();\n}\n\nquant::UniformQuantizedType GetFixedOutputRange(bool is_signed, int bit_width,\n                                                Type tensor_type, double scale,\n                                                int64_t zero_point,\n                                                int64_t storage_min,\n                                                int64_t storage_max) {\n  auto result_type = tensor_type.cast<ShapedType>();\n  if (!result_type.getElementType().isa<FloatType>()) return {};\n  Builder builder(result_type.getContext());\n\n  // Only support 8-bits\n  if (bit_width != 8) return {};\n  IntegerType storage_type = builder.getIntegerType(bit_width);\n  if (!is_signed) {\n    zero_point += 128;\n    storage_min += 128;\n    storage_max += 128;\n  }\n  return quant::UniformQuantizedType::getChecked(\n      builder.getUnknownLoc(), is_signed, storage_type,\n      result_type.getElementType(), scale, zero_point, storage_min,\n      storage_max);\n}\n\nType ConvertSignedQuantizedToUnsigned(Type signed_tensor_type, Location loc) {\n  auto qtype = QType::getQuantizedElementType(signed_tensor_type);\n  if (!qtype || !qtype.isSigned()) return {};\n\n  int num_bits = qtype.getStorageTypeIntegralWidth();\n  // This is a negative value, and will be applied on zero points and fixed\n  // point ranges.\n  int64_t offset =\n      QType::getDefaultMinimumForInteger(/*isSigned=*/true, num_bits) -\n      QType::getDefaultMinimumForInteger(/*isSigned=*/false, num_bits);\n\n  auto flags = !quant::QuantizationFlags::Signed;\n  QType new_qtype;\n  if (auto uqtype = qtype.dyn_cast<quant::UniformQuantizedType>()) {\n    new_qtype = quant::UniformQuantizedType::getChecked(\n        loc, flags, qtype.getStorageType(), qtype.getExpressedType(),\n        uqtype.getScale(), uqtype.getZeroPoint() - offset,\n        uqtype.getStorageTypeMin() - offset,\n        uqtype.getStorageTypeMax() - offset);\n  } else if (auto aqtype =\n                 qtype.dyn_cast<quant::UniformQuantizedPerAxisType>()) {\n    auto zero_points = aqtype.getZeroPoints();\n    llvm::SmallVector<int64_t, 4> new_zero_points(zero_points.begin(),\n                                                  zero_points.end());\n    for (int i = 0, e = new_zero_points.size(); i != e; ++i) {\n      new_zero_points[i] -= offset;\n    }\n    new_qtype = quant::UniformQuantizedPerAxisType::getChecked(\n        loc, flags, qtype.getStorageType(), qtype.getExpressedType(),\n        aqtype.getScales(), new_zero_points, aqtype.getQuantizedDimension(),\n        aqtype.getStorageTypeMin() - offset,\n        aqtype.getStorageTypeMax() - offset);\n  }\n  return new_qtype.castFromExpressedType(\n      QType::castToExpressedType(signed_tensor_type));\n}\n\nLogicalResult RemoveDebugAttrPattern::matchAndRewrite(\n    Operation* op, PatternRewriter& rewriter) const {\n  // removeAttr will return nullptr if the attribute did not exist. Thus we can\n  // return success(result) to indicate if this op has changed.\n  return success(/*isSuccess=*/\n                 op->removeAttr(kDebugModeOpQuantAttrName) ||\n                 op->removeAttr(kDebugModeOpFloatAttrName));\n}\n\n}  // namespace quant\n}  // namespace mlir\n", "/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n// This transformation pass prepares for legalization to the TFLite dialect by\n// converting operations in TensorFlow dialect into operations that can be\n// legalized to TensorFlow Lite dialect with simple replacements.  The newly\n// created operations are in the TensorFlow dialect if the operation can be\n// represented using a TensorFlow op.  Otherwise, TensorFlow Lite dialect op is\n// used.  For example, Conv2D in TFLite which uses OHWI data format for filters\n// is not supported in TensorFlow because TensorFlow requires filters in the\n// HWIO data format.\n//\n// Motivation to prepare for the TFLite legalization before the actual\n// legalization is to exploit constant folding opportunities in any newly\n// created ops by leveraging constant folding support for the TensorFlow ops.\n// This way TFLite can be used as a serialization format only and does not\n// require access to the TFLite runtime for optimizations as required by the\n// TFLite team.\n\n#include <climits>\n#include <cstdint>\n#include <utility>\n\n#include \"absl/memory/memory.h\"\n#include \"absl/numeric/bits.h\"\n#include \"llvm/ADT/ArrayRef.h\"\n#include \"llvm/ADT/STLExtras.h\"\n#include \"llvm/ADT/StringSwitch.h\"\n#include \"llvm/Support/Casting.h\"\n#include \"llvm/Support/Debug.h\"\n#include \"mlir/Dialect/Affine/Analysis/LoopAnalysis.h\"  // from @llvm-project\n#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"  // from @llvm-project\n#include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/FakeQuantSupport.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/QuantOps.h\"  // from @llvm-project\n#include \"mlir/Dialect/Quant/UniformSupport.h\"  // from @llvm-project\n#include \"mlir/IR/Attributes.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinOps.h\"  // from @llvm-project\n#include \"mlir/IR/BuiltinTypes.h\"  // from @llvm-project\n#include \"mlir/IR/MLIRContext.h\"  // from @llvm-project\n#include \"mlir/IR/Operation.h\"  // from @llvm-project\n#include \"mlir/Pass/Pass.h\"  // from @llvm-project\n#include \"mlir/Support/LLVM.h\"  // from @llvm-project\n#include \"mlir/Support/LogicalResult.h\"  // from @llvm-project\n#include \"mlir/Transforms/DialectConversion.h\"  // from @llvm-project\n#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"  // from @llvm-project\n#include \"tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/IR/hlo_ops.h\"\n#include \"tensorflow/compiler/mlir/lite/ir/tfl_ops.h\"\n#include \"tensorflow/compiler/mlir/lite/quantization/quantization_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/transforms/dilated_conv.h\"\n#include \"tensorflow/compiler/mlir/lite/transforms/passes.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/attribute_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/constant_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/fake_quant_utils.h\"\n#include \"tensorflow/compiler/mlir/lite/utils/validators.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/transforms/einsum.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/transforms/passes.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/transforms/unroll_batch_matmul.h\"\n#include \"tensorflow/compiler/mlir/tensorflow/utils/verification_utils.h\"\n#include \"tensorflow/compiler/mlir/xla/transforms/passes.h\"\n\n#define DEBUG_TYPE \"tf-tfl-legalization\"\n\nnamespace mlir {\nnamespace TFL {\nnamespace {\n// Returns a TF_CastOp to I32. This function is used for CastOps that are\n// intermediate nodes in a TableGen pattern result. In such a case, the\n// destination type is not inferred and must be given explicitly.\n//\n// Preconditions: The given value must have a ShapedType.\nstatic Value CreateTFCastOpI32(OpBuilder *builder, Location loc, Value x,\n                               BoolAttr truncate) {\n  auto x_type = x.getType().dyn_cast_or_null<ShapedType>();\n  if (!x_type) llvm_unreachable(\"unsupported type\");\n  Type type = x_type.clone(builder->getI32Type());\n  return builder->create<TF::CastOp>(loc, type, x, truncate);\n}\n}  // namespace\n\n//===----------------------------------------------------------------------===//\n// The actual PrepareTF Pass.\n//\n// TODO(hinsu): Add and use TensorFlow dialect ops for the ops created in this\n// pass.\nnamespace {\n#define GEN_PASS_CLASSES\n#include \"tensorflow/compiler/mlir/lite/transforms/passes.h.inc\"\n\n// Prepare TF operations in functions for subsequent legalization.\nclass PrepareTFPass : public PrepareTFPassBase<PrepareTFPass> {\n public:\n  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(PrepareTFPass)\n\n  PrepareTFPass() = default;\n  PrepareTFPass(const PrepareTFPass &) {}\n  explicit PrepareTFPass(bool unfold_batch_matmul,\n                         bool allow_bf16_and_f16_type_legalization,\n                         bool use_fake_quant_num_bits = false) {\n    this->unfold_batch_matmul_ = unfold_batch_matmul;\n    this->allow_bf16_and_f16_type_legalization_ =\n        allow_bf16_and_f16_type_legalization;\n    this->use_fake_quant_num_bits_ = use_fake_quant_num_bits;\n  }\n\n  void runOnOperation() override;\n};\n\n// Transient state for preserving data from match to rewrite\nstruct ConvertTFConvOpMatchState {\n  IntegerAttr dilation_height_factor;\n  IntegerAttr dilation_width_factor;\n  StringAttr padding;\n  IntegerAttr stride_height;\n  IntegerAttr stride_width;\n};\n\n// Templated class for declaring a converter from some TensorFlow convolution\n// op into its counterpart in TensorFlow Lite.\n//\n// The `ConcreteType` deriving from this template must provide the following\n// method for constructing TensorFlow Lite op:\n//\n//   TFL::[op] createTFLOp(ConvertTFConvOpMatchState *state,\n//                         PatternRewriter &rewriter, Location loc,\n//                         Type result_type, Value input,\n//                         Value filter, Value bias) const;\n//\n// And also the following method for getting the dimension for bias tensor:\n//\n//  int64_t getBiasDim(ArrayRef<int64_t> filterShape) const;\ntemplate <typename ConcreteType, typename TFConvOpType>\nclass ConvertTFConvOp : public RewritePattern {\n public:\n  ConvertTFConvOp(MLIRContext *context,\n                  bool allow_bf16_and_f16_type_legalization)\n      : RewritePattern(TFConvOpType::getOperationName(), 1, context),\n        intAttrOne(Builder(context).getI32IntegerAttr(1)),\n        allow_bf16_and_f16_type_legalization_(\n            allow_bf16_and_f16_type_legalization) {}\n\n  LogicalResult matchAndRewrite(Operation *op,\n                                PatternRewriter &rewriter) const override {\n    // Assumes TensorFlow convolution op is already verified to be\n    // in valid form.\n\n    // Match a TFConvOpType under the following conditions:\n    // * The 'T' attribute must exist and be of value DT_FLOAT.\n    // * The 'data_format' attribute must exist and be of value \"NHWC\".\n    // * The 'strides' attribute must exist and is of the form [1, X, Y, 1].\n    // * The 'dilations' attribute is optional, but it must be of the form\n    //   [1, X, Y, 1] if exists.\n\n    TFConvOpType tf_op = cast<TFConvOpType>(op);\n    if (!TFTypeIsFloat32Tensor(tf_op.input()) &&\n        !(allow_bf16_and_f16_type_legalization_ &&\n          TFTypeIsBFloat16OrHalfTensor(tf_op.input())))\n      return failure();\n\n    if (!TFDataFormatIsNHWC(op)) return failure();\n\n    IntegerAttr height, width;\n    if (!TFIntListIs1XY1(op, \"strides\", &height, &width)) return failure();\n\n    ConvertTFConvOpMatchState state;\n    state.stride_height = height;\n    state.stride_width = width;\n\n    if (TFIntListIs1XY1(op, \"dilations\", &height, &width)) {\n      state.dilation_height_factor = height;\n      state.dilation_width_factor = width;\n    } else {\n      // If the 'dilations' attribute is missing, we use the default value (1)\n      // for both dilation height and width factor.\n      state.dilation_height_factor = intAttrOne;\n      state.dilation_width_factor = intAttrOne;\n    }\n\n    TFPaddingIsSameOrValid(op, &state.padding);\n\n    // Additionally, we require the filter operand to be of 4-D tensor type so\n    // that we can extract info from the shape (e.g., for constructing bias\n    // tensor, for setting depth_multiplier attribute, etc.).\n    auto filter = tf_op.filter();\n    auto filter_type = filter.getType().template dyn_cast<RankedTensorType>();\n    if (!filter_type || filter_type.getRank() != 4 ||\n        !filter_type.hasStaticShape())\n      return failure();\n\n    Value input = tf_op.input();\n    RankedTensorType input_type =\n        input.getType().template dyn_cast<RankedTensorType>();\n    // Only rank size four input will be only available by the tf.Conv2D\n    // operator verification.\n    if (!input_type || input_type.isDynamicDim(3)) {\n      return failure();\n    }\n    // Check if the given op is based on grouped convolution.\n    // Dim size zero will be verified by the tf.Conv2D operator verification.\n    if (input_type.getDimSize(3) % filter_type.getDimSize(2) != 0) {\n      return failure();\n    }\n\n    // TensorFlow convolution op only has two inputs, while the TFLite one has\n    // three, with the bias vector marked as optional. However, TOCO has a\n    // dedicated pass, EnsureBiasVectors, to create default bias vectors for all\n    // those missing. So we model TFLite convolution op as requiring three\n    // inputs to achieve the legalization task of EnsureBiasVector. this\n    // requires the filter tensor to have static shape.\n\n    // TODO(antiagainst): also handle the case of tf.Add(tf.[op], <bias>)\n\n    // Get a splat zero tensor with the expected dimension for the bias tensor\n    auto elem_type = filter_type.getElementType();\n    auto bias_dim = static_cast<const ConcreteType *>(this)->getBiasDim(\n        filter_type.getShape());\n    auto bias_type = RankedTensorType::get({bias_dim}, elem_type);\n    auto bias_attr = rewriter.getZeroAttr(bias_type);\n    auto bias =\n        rewriter.create<TF::ConstOp>(op->getLoc(), bias_type, bias_attr);\n\n    if (op->getAttrOfType<StringAttr>(\"padding\").getValue() == \"EXPLICIT\") {\n      // Add Const op for padding value.\n      ArrayRef<Attribute> padding_attr_array =\n          op->getAttrOfType<ArrayAttr>(\"explicit_paddings\").getValue();\n\n      auto get_int = [](Attribute attr) {\n        return attr.template cast<IntegerAttr>().getInt();\n      };\n\n      SmallVector<int32_t> padding_values(padding_attr_array.size());\n      for (int i = 0; i < padding_attr_array.size(); i++) {\n        padding_values[i] =\n            static_cast<int32_t>(get_int(padding_attr_array[i]));\n      }\n\n      RankedTensorType padding_attr_type = RankedTensorType::get(\n          {filter_type.getRank(), 2}, rewriter.getIntegerType(32));\n      auto padding_attr =\n          mlir::DenseIntElementsAttr::get(padding_attr_type, padding_values);\n\n      auto padding_const =\n          rewriter.create<TF::ConstOp>(op->getLoc(), padding_attr);\n\n      // Add Pad op.\n      auto pad_output_type = UnrankedTensorType::get(elem_type);\n      input = rewriter.create<TF::PadOp>(op->getLoc(), pad_output_type, input,\n                                         padding_const);\n\n      // Set Conv padding to `VALID` since padding has been handled by Pad op.\n      state.padding = rewriter.getStringAttr(\"VALID\");\n    }\n    auto conv_op = static_cast<const ConcreteType *>(this)->createTFLOp(\n        &state, rewriter, op->getLoc(), tf_op.getType(), input, filter, bias);\n\n    rewriter.replaceOp(op, conv_op.getResult());\n    return success();\n  }\n\n  const IntegerAttr intAttrOne;\n\n private:\n  bool allow_bf16_and_f16_type_legalization_;\n};\n\nclass ConvertTFConv2D : public ConvertTFConvOp<ConvertTFConv2D, TF::Conv2DOp> {\n public:\n  using BaseType = ConvertTFConvOp<ConvertTFConv2D, TF::Conv2DOp>;\n\n  ConvertTFConv2D(MLIRContext *context, bool allow_bf16_type_legalization)\n      : BaseType(context, allow_bf16_type_legalization) {}\n\n  int64_t getBiasDim(ArrayRef<int64_t> filterShape) const {\n    return filterShape.back();\n  }\n\n  TFL::Conv2DOp createTFLOp(ConvertTFConvOpMatchState *state,\n                            PatternRewriter &rewriter, Location loc,\n                            Type result_type, Value input, Value filter,\n                            Value bias) const {\n    filter = legalizeFilter(rewriter, loc, filter);\n    return rewriter.create<TFL::Conv2DOp>(\n        loc, result_type, input, filter, bias,\n        /*dilation_h_factor=*/state->dilation_height_factor,\n        /*dilation_w_factor=*/state->dilation_width_factor,\n        /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n        /*padding=*/state->padding,\n        /*stride_h=*/state->stride_height,\n        /*stride_w=*/state->stride_width);\n  }\n\n private:\n  // Legalize the given filter by converting it from TensorFlow filter data\n  // format HWIO to TFLite Conv2D op filter data format OHWI and return Value\n  // for the converted filter.  Requires that filter is verified by the match\n  // method that it is a 4-D RankedTensorType.\n  Value legalizeFilter(PatternRewriter &rewriter, Location loc,\n                       Value filter) const {\n    // Create a constant op for HWIO to OHWI transpose permutation.\n    SmallVector<int, 4> perm = {3, 0, 1, 2};\n    auto perm_type = RankedTensorType::get({static_cast<int>(perm.size())},\n                                           rewriter.getIntegerType(32));\n    auto perm_attr =\n        DenseElementsAttr::get(perm_type, llvm::makeArrayRef<int>(perm));\n    auto perm_op = rewriter.create<TF::ConstOp>(loc, perm_type, perm_attr);\n\n    // Create tensor type for the transpose result.\n    auto filter_type = filter.getType().cast<RankedTensorType>();\n    auto result_shape =\n        llvm::to_vector<4>(llvm::map_range(perm, [filter_type](int64_t dim) {\n          return filter_type.getDimSize(dim);\n        }));\n    auto elem_type = filter_type.getElementType();\n    auto result_type = RankedTensorType::get(result_shape, elem_type);\n\n    return rewriter.create<TF::TransposeOp>(loc, result_type, filter, perm_op);\n  }\n};\n\nclass ConvertTFDepthwiseConv2dNative\n    : public ConvertTFConvOp<ConvertTFDepthwiseConv2dNative,\n                             TF::DepthwiseConv2dNativeOp> {\n public:\n  using BaseType = ConvertTFConvOp<ConvertTFDepthwiseConv2dNative,\n                                   TF::DepthwiseConv2dNativeOp>;\n\n  ConvertTFDepthwiseConv2dNative(MLIRContext *context,\n                                 bool allow_bf16_type_legalization)\n      : BaseType(context, allow_bf16_type_legalization) {}\n\n  int64_t getBiasDim(ArrayRef<int64_t> filterShape) const {\n    return filterShape[2] * filterShape[3];\n  }\n\n  TFL::DepthwiseConv2DOp createTFLOp(ConvertTFConvOpMatchState *state,\n                                     PatternRewriter &rewriter, Location loc,\n                                     Type result_type, Value input,\n                                     Value filter, Value bias) const {\n    // Compared to tfl.conv_2d, tfl.depthwise_conv_2d has an additional\n    // 'depth_multiplier' attribute. However, tf.DepthwiseConv2dNative does not\n    // have a corresponding 'depth_multiplier' attribute; the multiplier is the\n    // fourth dimension in the 4-D filter tensor. We query the multiplier from\n    // tf.DepthwiseConv2dNative and set it as the attribute value accordingly.\n    auto multiplier = filter.getType().cast<RankedTensorType>().getDimSize(3);\n\n    filter = legalizeFilter(rewriter, loc, filter);\n    return rewriter.create<TFL::DepthwiseConv2DOp>(\n        loc, result_type, input, filter, bias,\n        /*dilation_h_factor=*/state->dilation_height_factor,\n        /*dilation_w_factor=*/state->dilation_width_factor,\n        /*fused_activation_function=*/rewriter.getStringAttr(\"NONE\"),\n        /*padding=*/state->padding,\n        /*stride_h=*/state->stride_height,\n        /*stride_w=*/state->stride_width,\n        /*depth_multiplier=*/rewriter.getI32IntegerAttr(multiplier));\n  }\n\n private:\n  /// Legalize the given filter by converting it from TensorFlow filter data\n  /// format to TFLite DepthwiseConv2D op filter data format and return Value\n  /// for the converted filter.  TensorFlow filter data format is\n  /// [filter_height, filter_width, in_channels, channel_multiplier] and TFLite\n  /// filter data format is [1, filter_height, filter_width, out_channels].\n  /// Requires that filter is verified by the match method that it is a 4-D\n  /// RankedTensorType.\n  Value legalizeFilter(PatternRewriter &rewriter, Location loc,\n                       Value filter) const {\n    auto filter_type = filter.getType().cast<RankedTensorType>();\n    auto filterShape = filter_type.getShape();\n    SmallVector<int64_t, 4> result_shape = {1, filterShape[0], filterShape[1],\n                                            filterShape[2] * filterShape[3]};\n    auto elem_type = filter_type.getElementType();\n    auto result_type = RankedTensorType::get(result_shape, elem_type);\n    // TensorFlow Lite `Reshape` op only support int32 shape tensor currently.\n    auto shape_type = RankedTensorType::get({4}, rewriter.getIntegerType(32));\n    SmallVector<Attribute, 4> result_shape_data(4);\n    for (int i = 0; i < 4; ++i) {\n      result_shape_data[i] =\n          rewriter.getI32IntegerAttr(static_cast<int32_t>(result_shape[i]));\n    }\n    auto shape_attr = DenseElementsAttr::get(shape_type, result_shape_data);\n    auto shape = rewriter.create<TF::ConstOp>(loc, shape_type, shape_attr);\n\n    return rewriter.create<TF::ReshapeOp>(loc, result_type, filter, shape);\n  }\n};\n\n// StridedSlice can have complicated attributes like begin_axis_mask,\n// end_axis_mask, ellipsis_axis_mask, new_axis_mask, shrink_axis_mask. These\n// masks will complicate the strided_slice computation logic, we can simplify\n// the logic by inserting a reshape op to pad the inputs so strided_slice can\n// be easier to handle.\n//\n// So the graph may looks like below:\n//   original_input -> strided_slice -> output\n//      (transforms)\n//   original_input -> reshape -> strided_slice -> output\n//\n// And the new shape is computed based on the masks.\n//\n// An example for new_axis_mask. say the new_axis_mask is 9 which represents\n// [1 0 0 1], and that means we're inserting two new axes at 0 & 3 dim, so\n// if original shape is [2, 3], now we reshape that into [1, 2, 3, 1].\nstruct ConvertTFStridedSlice : public RewritePattern {\n  explicit ConvertTFStridedSlice(MLIRContext *context)\n      : RewritePattern(TF::StridedSliceOp::getOperationName(), 2, context) {}\n\n  LogicalResult RewriteNewAxisMask(Operation *op,\n                                   PatternRewriter &rewriter) const {\n    TF::StridedSliceOp strided_slice_op = llvm::cast<TF::StridedSliceOp>(op);\n    uint64_t new_axis_mask = strided_slice_op.new_axis_mask();\n\n    if (strided_slice_op.ellipsis_mask() != 0) {\n      // Ellipsis mask should have been lowered-away prior to invoking this\n      // function.\n      op->emitError() << \"encountered a logical error\";\n      return failure();\n    }\n\n    // Insert a new reshape op.\n    Value original_input = strided_slice_op.input();\n    RankedTensorType original_input_type =\n        original_input.getType().dyn_cast<RankedTensorType>();\n    if (!original_input_type) {\n      return failure();\n    }\n\n    const ArrayRef<int64_t> &original_input_shape =\n        original_input_type.getShape();\n    SmallVector<int64_t, 4> revised_shape;\n    int index = 0;\n    const int original_input_rank = original_input_shape.size();\n    while (index < original_input_rank || new_axis_mask) {\n      if (new_axis_mask & 1) {\n        revised_shape.emplace_back(1);\n      } else {\n        revised_shape.emplace_back(original_input_shape[index++]);\n      }\n      new_axis_mask >>= 1;\n    }\n\n    if (failed(TF::VerifyShapeOfReshapeOp(revised_shape))) return failure();\n\n    const int dim_size = revised_shape.size();\n    Location loc = strided_slice_op.getLoc();\n    auto shape_type =\n        RankedTensorType::get({dim_size}, rewriter.getIntegerType(32));\n    SmallVector<Attribute, 4> result_shape_data(dim_size);\n    for (int i = 0; i < dim_size; ++i) {\n      result_shape_data[i] =\n          rewriter.getI32IntegerAttr(static_cast<int32_t>(revised_shape[i]));\n    }\n\n    auto shape_attr = DenseElementsAttr::get(shape_type, result_shape_data);\n    auto shape =\n        rewriter.create<arith::ConstantOp>(loc, shape_type, shape_attr);\n    auto revised_output_type = RankedTensorType::get(\n        revised_shape, original_input_type.getElementType());\n    TF::ReshapeOp reshape = rewriter.create<TF::ReshapeOp>(\n        loc, revised_output_type, original_input, shape);\n\n    // Replace the original strided_slice.\n    uint64_t revised_begin_mask = strided_slice_op.begin_mask();\n    uint64_t revised_end_mask = strided_slice_op.end_mask();\n    // Since we expand the dims, we need to apply them to the begin_mask &\n    // end_mask.\n    revised_begin_mask |= strided_slice_op.new_axis_mask();\n    revised_end_mask |= strided_slice_op.new_axis_mask();\n\n    // Enforce operator precedence.\n    uint64_t revised_shrink_axis_mask =\n        strided_slice_op.shrink_axis_mask() & ~strided_slice_op.new_axis_mask();\n\n    auto attribute_type = rewriter.getIntegerType(64);\n    rewriter.replaceOpWithNewOp<TF::StridedSliceOp>(\n        op, strided_slice_op.getType(), reshape, strided_slice_op.begin(),\n        strided_slice_op.end(), strided_slice_op.strides(),\n        rewriter.getIntegerAttr(attribute_type, revised_begin_mask),\n        rewriter.getIntegerAttr(attribute_type, revised_end_mask),\n        rewriter.getIntegerAttr(attribute_type,\n                                strided_slice_op.ellipsis_mask()),\n        rewriter.getI64IntegerAttr(0),\n        rewriter.getIntegerAttr(attribute_type, revised_shrink_axis_mask));\n    return success();\n  }\n\n  LogicalResult RewriteEllipsisMask(Operation *op,\n                                    PatternRewriter &rewriter) const {\n    TF::StridedSliceOp strided_slice_op = llvm::cast<TF::StridedSliceOp>(op);\n\n    uint64_t ellipsis_mask = strided_slice_op.ellipsis_mask();\n    uint64_t shrink_axis_mask = strided_slice_op.shrink_axis_mask();\n    uint64_t new_axis_mask = strided_slice_op.new_axis_mask();\n\n    // Enforce operator precedence.\n    shrink_axis_mask &= ~ellipsis_mask;\n    new_axis_mask &= ~ellipsis_mask;\n\n    DenseIntElementsAttr begin_dense_elem_attr;\n    Value begin = strided_slice_op.begin();\n    auto begin_ranked_attr_type = begin.getType().dyn_cast<RankedTensorType>();\n    if (!begin_ranked_attr_type ||\n        !matchPattern(begin, m_Constant(&begin_dense_elem_attr))) {\n      return failure();\n    }\n\n    DenseIntElementsAttr end_dense_elem_attr;\n    Value end = strided_slice_op.end();\n    auto end_ranked_attr_type = end.getType().dyn_cast<RankedTensorType>();\n    if (!end_ranked_attr_type ||\n        !matchPattern(end, m_Constant(&end_dense_elem_attr))) {\n      return failure();\n    }\n\n    DenseIntElementsAttr stride_dense_elem_attr;\n    Value stride = strided_slice_op.strides();\n    auto stride_ranked_attr_type =\n        stride.getType().dyn_cast<RankedTensorType>();\n    if (!stride_ranked_attr_type ||\n        !matchPattern(stride, m_Constant(&stride_dense_elem_attr))) {\n      return failure();\n    }\n\n    Value input = strided_slice_op.input();\n    RankedTensorType input_type = input.getType().dyn_cast<RankedTensorType>();\n    if (!input_type) {\n      return failure();\n    }\n    const ArrayRef<int64_t> input_shape = input_type.getShape();\n\n    const int input_size = input_shape.size();\n\n    RankedTensorType begin_type = begin.getType().cast<RankedTensorType>();\n    const ArrayRef<int64_t> begin_shape = begin_type.getShape();\n    const int begin_dim = begin_shape.size();\n\n    if (begin_dim != 1) return failure();\n\n    // The ellipsis fill might exceed the current output shape because we are\n    // also taking account of any to-be-inserted new axes.\n    const int ellipsis_filled_dim_size =\n        input_size - begin_shape[0] + 1 + absl::popcount(new_axis_mask);\n\n    int64_t begin_mask = strided_slice_op.begin_mask();\n    int64_t end_mask = strided_slice_op.end_mask();\n    int64_t revised_begin_mask = 0;\n    int64_t revised_end_mask = 0;\n    int64_t revised_shrink_axis_mask = 0;\n    int64_t revised_new_axis_mask = 0;\n\n    SmallVector<int32_t, 4> padded_begin;\n    SmallVector<int32_t, 4> padded_end;\n    SmallVector<int32_t, 4> padded_stride;\n\n    // Before the ellipsis.\n    int index = 0;\n    int new_index = 0;\n    while (((ellipsis_mask >> index) & 1) == 0) {\n      padded_begin.push_back(begin_dense_elem_attr.getValues<int32_t>()[index]);\n      padded_end.push_back(end_dense_elem_attr.getValues<int32_t>()[index]);\n      padded_stride.push_back(\n          stride_dense_elem_attr.getValues<int32_t>()[index]);\n      if ((begin_mask >> index) & 1) revised_begin_mask |= (1 << new_index);\n      if ((end_mask >> index) & 1) revised_end_mask |= (1 << new_index);\n      if ((shrink_axis_mask >> index) & 1)\n        revised_shrink_axis_mask |= (1 << new_index);\n\n      if ((new_axis_mask >> index) & 1)\n        revised_new_axis_mask |= (1 << new_index);\n\n      ++index;\n      ++new_index;\n    }\n\n    // Ellipsis.\n    for (; new_index < index + ellipsis_filled_dim_size; ++new_index) {\n      revised_begin_mask |= (1 << new_index);\n      revised_end_mask |= (1 << new_index);\n\n      // Mimic the begin/end/strides mask behavior.\n      padded_begin.push_back(0);\n      padded_end.push_back(0);\n      padded_stride.push_back(1);\n    }\n\n    // Account for ellipsis mask.\n    ++index;\n\n    // After the ellipsis.\n    for (; index < begin_shape[0];) {\n      padded_begin.push_back(begin_dense_elem_attr.getValues<int32_t>()[index]);\n      padded_end.push_back(end_dense_elem_attr.getValues<int32_t>()[index]);\n      padded_stride.push_back(\n          stride_dense_elem_attr.getValues<int32_t>()[index]);\n\n      if ((begin_mask >> index) & 1) revised_begin_mask |= (1 << new_index);\n      if ((end_mask >> index) & 1) revised_end_mask |= (1 << new_index);\n      if ((shrink_axis_mask >> index) & 1)\n        revised_shrink_axis_mask |= (1 << new_index);\n      if ((new_axis_mask >> index) & 1)\n        revised_new_axis_mask |= (1 << new_index);\n\n      ++index;\n      ++new_index;\n    }\n\n    auto attribute_type = rewriter.getIntegerType(64);\n\n    int full_dim_count = padded_begin.size();\n    auto type =\n        RankedTensorType::get({full_dim_count}, rewriter.getIntegerType(32));\n\n    auto begin_attr = DenseElementsAttr::get<int32_t>(type, padded_begin);\n    auto begin_op =\n        rewriter.create<arith::ConstantOp>(op->getLoc(), type, begin_attr);\n    auto end_attr = DenseElementsAttr::get<int32_t>(type, padded_end);\n    auto end_op =\n        rewriter.create<arith::ConstantOp>(op->getLoc(), type, end_attr);\n    auto stride_attr = DenseElementsAttr::get<int32_t>(type, padded_stride);\n    auto stride_op =\n        rewriter.create<arith::ConstantOp>(op->getLoc(), type, stride_attr);\n\n    rewriter.replaceOpWithNewOp<TF::StridedSliceOp>(\n        op, strided_slice_op.getType(), input, begin_op.getResult(),\n        end_op.getResult(), stride_op.getResult(),\n        rewriter.getIntegerAttr(attribute_type, revised_begin_mask),\n        rewriter.getIntegerAttr(attribute_type, revised_end_mask),\n        /*ellipsis_mask=*/rewriter.getI64IntegerAttr(0),\n        rewriter.getIntegerAttr(attribute_type, revised_new_axis_mask),\n        rewriter.getIntegerAttr(attribute_type, revised_shrink_axis_mask));\n\n    return success();\n  }\n\n  void PadStridedSliceAttributeArray(DenseIntElementsAttr dense_elem_attr,\n                                     SmallVectorImpl<int32_t> &val,\n                                     SmallVectorImpl<int32_t> &padded_val,\n                                     ArrayRef<int32_t> padding_val,\n                                     int *mask) const {\n    for (const auto &idx : dense_elem_attr.getValues<APInt>()) {\n      val.push_back(idx.getSExtValue());\n      padded_val.push_back(idx.getSExtValue());\n    }\n    int attr_dim_count = val.size();\n    int full_dim_count = padding_val.size();\n    for (int i = attr_dim_count; i < full_dim_count; ++i) {\n      padded_val.push_back(padding_val[i]);\n      if (mask) *mask |= 1 << i;\n    }\n  }\n\n  LogicalResult matchAndRewrite(Operation *op,\n                                PatternRewriter &rewriter) const override {\n    TF::StridedSliceOp strided_slice_op = llvm::cast<TF::StridedSliceOp>(op);\n\n    // Handle ellipsis mask.\n    if (strided_slice_op.ellipsis_mask() != 0) {\n      return RewriteEllipsisMask(strided_slice_op, rewriter);\n    }\n\n    // Handle new axis mask.\n    if (strided_slice_op.new_axis_mask() != 0) {\n      return RewriteNewAxisMask(strided_slice_op, rewriter);\n    }\n\n    auto ranked_input_type =\n        strided_slice_op.input().getType().dyn_cast<RankedTensorType>();\n    if (!ranked_input_type) {\n      return failure();\n    }\n\n    auto begin_attr = strided_slice_op.begin();\n    auto end_attr = strided_slice_op.end();\n    auto strides_attr = strided_slice_op.strides();\n\n    auto begin_attr_type = begin_attr.getType().dyn_cast<RankedTensorType>();\n    auto end_attr_type = end_attr.getType().dyn_cast<RankedTensorType>();\n    auto strides_attr_type =\n        strides_attr.getType().dyn_cast<RankedTensorType>();\n\n    DenseIntElementsAttr begin_elem_attr;\n    DenseIntElementsAttr end_elem_attr;\n    DenseIntElementsAttr strides_elem_attr;\n\n    if (!begin_attr_type ||\n        !matchPattern(begin_attr, m_Constant(&begin_elem_attr))) {\n      return failure();\n    }\n    if (!end_attr_type || !matchPattern(end_attr, m_Constant(&end_elem_attr))) {\n      return failure();\n    }\n    if (!strides_attr_type ||\n        !matchPattern(strides_attr, m_Constant(&strides_elem_attr))) {\n      return failure();\n    }\n\n    SmallVector<int32_t, 4> begin, end, strides;\n    SmallVector<int32_t, 4> padded_begin, padded_end, padded_strides;\n\n    int num_input_dims = ranked_input_type.getRank();\n    SmallVector<int32_t, 4> padding_begin(num_input_dims, 0);\n    auto input_shape = ranked_input_type.getShape();\n    SmallVector<int32_t, 4> padding_end(input_shape.begin(), input_shape.end());\n    SmallVector<int32_t, 4> padding_strides(num_input_dims, 1);\n\n    int begin_mask = strided_slice_op.begin_mask();\n    int end_mask = strided_slice_op.end_mask();\n\n    PadStridedSliceAttributeArray(begin_elem_attr, begin, padded_begin,\n                                  padding_begin, &begin_mask);\n    PadStridedSliceAttributeArray(end_elem_attr, end, padded_end, padding_end,\n                                  &end_mask);\n    PadStridedSliceAttributeArray(strides_elem_attr, strides, padded_strides,\n                                  padding_strides, nullptr);\n\n    if (begin == padded_begin && end == padded_end &&\n        strides == padded_strides &&\n        begin_mask == strided_slice_op.begin_mask() &&\n        end_mask == strided_slice_op.end_mask()) {\n      return failure();\n    }\n\n    auto begin_end_type =\n        RankedTensorType::get({num_input_dims}, rewriter.getIntegerType(32));\n    auto new_begin_attr = rewriter.create<arith::ConstantOp>(\n        op->getLoc(), begin_end_type,\n        DenseElementsAttr::get<int32_t>(begin_end_type, padded_begin));\n    auto new_end_attr = rewriter.create<arith::ConstantOp>(\n        op->getLoc(), begin_end_type,\n        DenseElementsAttr::get<int32_t>(begin_end_type, padded_end));\n    auto strides_type =\n        RankedTensorType::get({static_cast<long>(padded_strides.size())},\n                              rewriter.getIntegerType(32));\n    auto new_strides_attr = rewriter.create<arith::ConstantOp>(\n        op->getLoc(), strides_type,\n        DenseElementsAttr::get<int32_t>(strides_type, padded_strides));\n\n    auto attribute_type = rewriter.getIntegerType(64);\n    rewriter.replaceOpWithNewOp<TF::StridedSliceOp>(\n        op, strided_slice_op.output().getType(), strided_slice_op.input(),\n        new_begin_attr, new_end_attr, new_strides_attr,\n        rewriter.getIntegerAttr(attribute_type, begin_mask),\n        rewriter.getIntegerAttr(attribute_type, end_mask),\n        rewriter.getIntegerAttr(attribute_type,\n                                strided_slice_op.ellipsis_mask()),\n        rewriter.getIntegerAttr(attribute_type,\n                                strided_slice_op.new_axis_mask()),\n        rewriter.getIntegerAttr(attribute_type,\n                                strided_slice_op.shrink_axis_mask()));\n\n    return success();\n  }\n};\n\nstruct ConvertTFBroadcastTo : public RewritePattern {\n  explicit ConvertTFBroadcastTo(MLIRContext *context)\n      : RewritePattern(TF::BroadcastToOp::getOperationName(), 1, context) {}\n\n  LogicalResult matchAndRewrite(Operation *op,\n                                PatternRewriter &rewriter) const override {\n    auto tf_broadcast_to_op = cast<TF::BroadcastToOp>(op);\n    auto input_type = tf_broadcast_to_op.input().getType().cast<ShapedType>();\n    auto output_type = tf_broadcast_to_op.output().getType().cast<ShapedType>();\n    auto shape_type = tf_broadcast_to_op.shape().getType().cast<ShapedType>();\n    Type element_type = input_type.getElementType();\n\n    // Allow lowering when low dimension inputs are given and its type is F32 or\n    // I32.\n    if (!((output_type.hasRank() && output_type.getRank() <= 4) ||\n          (shape_type.hasStaticShape() && shape_type.getRank() == 1 &&\n           shape_type.getDimSize(0) <= 4)))\n      return failure();\n\n    if (!(element_type.isa<BFloat16Type, Float32Type>() ||\n          element_type.isInteger(32) || element_type.isInteger(16)))\n      return failure();\n\n    auto status_or_const_op =\n        CreateConstOpWithSingleValue(&rewriter, op->getLoc(), input_type, 1);\n    if (!status_or_const_op.ok()) {\n      return failure();\n    }\n\n    auto tf_fill_op = rewriter.create<TF::FillOp>(\n        op->getLoc(), output_type, tf_broadcast_to_op.shape(),\n        status_or_const_op.ValueOrDie());\n\n    auto mul_op = rewriter.create<TF::MulOp>(\n        op->getLoc(), output_type, tf_broadcast_to_op.input(), tf_fill_op);\n    rewriter.replaceOp(op, mul_op.getResult());\n    return success();\n  }\n};\n\n// The below pattern is equivalent to the DRR rule below\n// The checks are dependent on generated values, so we can't add\n// the checks on intermediate values, ideally we should find equivalent\n// checks that guarantees the resultant ops are valid.\n// The extra conditions are the broadcasting conditions.\n//\n// The pattern lower FusedBatchNormV3 to arithmetic ops.\n// Specifically, performs the following calculation:\n//\n//   (x - mean) * scale / sqrt(variance + epsilon) + offset\n//\n// Let multiplier = scale / sqrt(variance + epsilon),\n// to compute\n//   (x - mean) * scale / sqrt(variance + epsilon) + offset,\n// is then to compute\n//   (x * multiplier) + (offset - mean * multiplier).\n//\n// def : Pattern<\n//     (TF_FusedBatchNormV3Op:$root\n//         $x, $scale, $offset, $mean, $variance,\n//         F32Attr:$epsilon, $exponential_avg_factor,\n//         $data_format, FalseBoolAttr:$is_training),\n//     [(TF_AddOp\n//         (TF_MulOp\n//             $x,\n//             (TF_MulOp:$multiplier\n//                 $scale,\n//                 (TF_RsqrtOp\n//                     (TF_AddOp $variance,\n//                               (TF_ConstOp $epsilon))))),\n//         (TF_SubOp $offset, (TF_MulOp $mean, $multiplier))),\n//    // We already guaranteed that the last five results have no use so it does\n//    // not matter what value we provide here for replacement.\n//      /*batch_mean=*/(replaceWithValue $x),\n//      /*batch_variance=*/(replaceWithValue $x),\n//      /*reserve_space_1=*/(replaceWithValue $x),\n//      /*reserve_space_2=*/(replaceWithValue $x),\n//      /*reserve_space_3=*/(replaceWithValue $x)],\n//     [(HasNoUseOf:$root__1), (HasNoUseOf:$root__2),\n//      (HasNoUseOf:$root__3), (HasNoUseOf:$root__4),\n//      (HasNoUseOf:$root__5), (AreBroadcastableTypes $multiplier, $x)]>;\n//\n// When is_training is set to true, the given variance and mean are not used.\n// In above calculation, they are replaced by new values. These new mean and\n// variance are calculated as following:\n// new_mean = mean(x, axis=[0, 1, 2])\n// new_variance = mean(squared_difference(x, new_mean), axis=[0, 1, 2])\n//\n// The DDR rule for the is_training equals true case is as following:\n// def : Pattern<\n//     (TF_FusedBatchNormV3Op:$root\n//         $x, $scale, $offset, $mean, $variance,\n//         F32Attr:$epsilon, $exponential_avg_factor,\n//         $data_format, FalseBoolAttr:$is_training),\n//     [(TF_AddOp\n//         (TF_MulOp\n//             $x,\n//             (TF_MulOp:$multiplier\n//                 $scale,\n//                 (TF_RsqrtOp\n//                     (TF_AddOp\n//                         (TF_MeanOp\n//                             (TF_SquaredDifferenceOp $x, $new_mean),\n//                             (TF_ConstOp [0,1,2])),\n//                         (TF_ConstOp $epsilon))))),\n//         (TF_SubOp\n//             $offset,\n//             (TF_MulOp\n//                 (TF_MeanOp $x, (TF_ConstOp [0,1,2])),\n//                 $multiplier))),\n//    // We already guaranteed that the last five results have no use so it does\n//    // not matter what value we provide here for replacement.\n//      /*batch_mean=*/(replaceWithValue $x),\n//      /*batch_variance=*/(replaceWithValue $x),\n//      /*reserve_space_1=*/(replaceWithValue $x),\n//      /*reserve_space_2=*/(replaceWithValue $x),\n//      /*reserve_space_3=*/(replaceWithValue $x)],\n//     [(HasNoUseOf:$root__1), (HasNoUseOf:$root__2),\n//      (HasNoUseOf:$root__3), (HasNoUseOf:$root__4),\n//      (HasNoUseOf:$root__5), (AreBroadcastableTypes $multiplier, $x)]>;\n\nstruct FusedBatchNormV3Pat : public ::mlir::RewritePattern {\n  explicit FusedBatchNormV3Pat(::mlir::MLIRContext *context)\n      : ::mlir::RewritePattern(\n            \"tf.FusedBatchNormV3\", 1, context,\n            {\"tf.Add\", \"tf.Const\", \"tf.Mul\", \"tf.Rsqrt\", \"tf.Sub\"}) {}\n\n  ::mlir::LogicalResult matchAndRewrite(\n      ::mlir::Operation *fused_batch_norm,\n      ::mlir::PatternRewriter &rewriter) const override {\n    // Variables for capturing values and attributes used for creating ops\n    Operation::operand_range mean(fused_batch_norm->getOperands());\n    ::mlir::FloatAttr exponential_avg_factor;\n    ::mlir::TF::FusedBatchNormV3Op root;\n    Operation::operand_range offset(fused_batch_norm->getOperands());\n    Operation::operand_range x(fused_batch_norm->getOperands());\n    Operation::operand_range scale(fused_batch_norm->getOperands());\n    Operation::operand_range variance(fused_batch_norm->getOperands());\n    ::mlir::FloatAttr epsilon;\n    ::mlir::BoolAttr is_training;\n\n    // Match\n    auto fused_batch_norm_op =\n        dyn_cast_or_null<::mlir::TF::FusedBatchNormV3Op>(fused_batch_norm);\n    root = fused_batch_norm_op;\n    x = fused_batch_norm_op.getODSOperands(0);\n    scale = fused_batch_norm_op.getODSOperands(1);\n    offset = fused_batch_norm_op.getODSOperands(2);\n    mean = fused_batch_norm_op.getODSOperands(3);\n    variance = fused_batch_norm_op.getODSOperands(4);\n\n    ::mlir::Value mean_value = (*mean.begin());\n    ::mlir::Value variance_value = (*variance.begin());\n\n    if (!TFTypeIsFloat32Tensor(fused_batch_norm_op.x())) return failure();\n\n    {\n      epsilon =\n          fused_batch_norm_op->getAttrOfType<::mlir::FloatAttr>(\"epsilon\");\n      if (!epsilon)\n        epsilon = rewriter.getFloatAttr(rewriter.getF32Type(), 0.0001f);\n\n      if (!(((epsilon.isa<::mlir::FloatAttr>())) &&\n            ((epsilon.cast<::mlir::FloatAttr>().getType().isF32())))) {\n        return rewriter.notifyMatchFailure(\n            fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n              diag << \"op 'tf.FusedBatchNormV3' attribute 'epsilon' failed to \"\n                      \"satisfy constraint: 32-bit float attribute\";\n            });\n      }\n    }\n    {\n      exponential_avg_factor =\n          fused_batch_norm_op->getAttrOfType<::mlir::FloatAttr>(\n              \"exponential_avg_factor\");\n      if (!exponential_avg_factor)\n        exponential_avg_factor =\n            rewriter.getFloatAttr(rewriter.getF32Type(), 1.0f);\n    }\n    if (!TFDataFormatIsNHWC(fused_batch_norm_op) &&\n        !TFDataFormatIsNDHWC(fused_batch_norm_op))\n      return failure();\n\n    if (!(((*root.getODSResults(1).begin()).use_empty()))) {\n      return rewriter.notifyMatchFailure(\n          fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n            diag << \"entities '' failed to satisfy constraint: has no use\";\n          });\n    }\n\n    if (!(((*root.getODSResults(2).begin()).use_empty()))) {\n      return rewriter.notifyMatchFailure(\n          fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n            diag << \"entities '' failed to satisfy constraint: has no use\";\n          });\n    }\n\n    if (!(((*root.getODSResults(3).begin()).use_empty()))) {\n      return rewriter.notifyMatchFailure(\n          fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n            diag << \"entities '' failed to satisfy constraint: has no use\";\n          });\n    }\n\n    if (!(((*root.getODSResults(4).begin()).use_empty()))) {\n      return rewriter.notifyMatchFailure(\n          fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n            diag << \"entities '' failed to satisfy constraint: has no use\";\n          });\n    }\n\n    if (!(((*root.getODSResults(5).begin()).use_empty()))) {\n      return rewriter.notifyMatchFailure(\n          fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n            diag << \"entities '' failed to satisfy constraint: has no use\";\n          });\n    }\n\n    is_training =\n        fused_batch_norm_op->getAttrOfType<::mlir::BoolAttr>(\"is_training\");\n    auto odsLoc = rewriter.getFusedLoc({fused_batch_norm->getLoc()});\n\n    // We need to make sure input and output shapes are compatible.\n    int64_t last_dim = -1;\n    {\n      auto is_last_dim_compatible = [](const Value &v, int64_t &last_dim) {\n        auto v_type = v.getType().dyn_cast_or_null<RankedTensorType>();\n        if (!v_type) return true;\n        int64_t v_last_dim = v_type.getDimSize(v_type.getRank() - 1);\n        if (v_last_dim == -1) return true;\n        if (last_dim != -1 && v_last_dim != last_dim) return false;\n        last_dim = v_last_dim;\n        return true;\n      };\n\n      if (!is_last_dim_compatible(*x.begin(), last_dim) ||\n          !is_last_dim_compatible(*scale.begin(), last_dim) ||\n          !is_last_dim_compatible(*offset.begin(), last_dim)) {\n        return rewriter.notifyMatchFailure(\n            fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n              diag << \"Shapes of scale and offset should be 1D and \"\n                      \"compatible with x\";\n            });\n      }\n\n      if (!is_training.getValue()) {\n        if (!is_last_dim_compatible(mean_value, last_dim) ||\n            !is_last_dim_compatible(variance_value, last_dim)) {\n          return rewriter.notifyMatchFailure(\n              fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n                diag << \"Shapes of mean and variance should be 1D and \"\n                        \"compatible with x\";\n              });\n        }\n      }\n\n      // Check if output shape and input shape are compatible.\n      auto x_type = (*x.begin()).getType();\n      auto y_type = (*root.getODSResults(0).begin()).getType();\n      if (!OpTrait::util::getBroadcastedType(x_type, y_type)) {\n        return rewriter.notifyMatchFailure(\n            fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n              diag << \"Shapes of x and the first output should be compatible\";\n            });\n      }\n    }\n\n    // For training, mean and variance is calculated from input values.\n    if (is_training.getValue()) {\n      auto input_type = fused_batch_norm_op.x()\n                            .getType()\n                            .dyn_cast_or_null<RankedTensorType>();\n      if (!input_type || input_type.getRank() != 4) {\n        return rewriter.notifyMatchFailure(\n            fused_batch_norm_op, [&](::mlir::Diagnostic &diag) {\n              diag << \"op 'tf.FusedBatchNormV3' that has 'is_training' equals \"\n                      \"True is only supported with input of rank 4\";\n            });\n      }\n\n      ::mlir::TF::ConstOp reduce_dim_op;\n      {\n        auto reduce_dim_type =\n            ::mlir::RankedTensorType::get({3}, rewriter.getIntegerType(32));\n        ::mlir::SmallVector<int32_t, 3> reduce_dim_values = {0, 1, 2};\n        reduce_dim_op = rewriter.create<TF::ConstOp>(\n            odsLoc, ::mlir::DenseIntElementsAttr::get(reduce_dim_type,\n                                                      reduce_dim_values));\n      }\n\n      auto new_mean_type =\n          ::mlir::RankedTensorType::get({last_dim}, rewriter.getF32Type());\n      ::mlir::TF::MeanOp mean_op_1;\n      {\n        ::mlir::Value x_value = (*x.begin());\n        mean_op_1 = rewriter.create<TF::MeanOp>(\n            odsLoc, new_mean_type, x_value, reduce_dim_op,\n            /*keep_dims=*/rewriter.getBoolAttr(false));\n      }\n\n      ::mlir::TF::SquaredDifferenceOp square_diff_op;\n      {\n        ::mlir::Value tblgen_value_0 = (*x.begin());\n        ::mlir::Value tblgen_value_1 = (*mean_op_1.getODSResults(0).begin());\n        // If x has shape of [b, h, w, c], the result of mean_op_1 will have\n        // shape of [c]. Therefore, their shapes are always compatible.\n        square_diff_op = rewriter.create<::mlir::TF::SquaredDifferenceOp>(\n            odsLoc, tblgen_value_0, tblgen_value_1);\n      }\n\n      ::mlir::TF::MeanOp mean_op_2;\n      {\n        ::mlir::Value input_value = (*square_diff_op.getODSResults(0).begin());\n        mean_op_2 = rewriter.create<TF::MeanOp>(\n            odsLoc, new_mean_type, input_value, reduce_dim_op,\n            /*keep_dims=*/rewriter.getBoolAttr(false));\n      }\n\n      mean_value = (*mean_op_1.getODSResults(0).begin());\n      variance_value = (*mean_op_2.getODSResults(0).begin());\n    }  // End is_training equals true if.\n\n    ::llvm::SmallVector<::mlir::Value, 4> replace_values;\n    ::mlir::TF::ConstOp epsilon_const_op;\n    {\n      epsilon_const_op =\n          rewriter.create<::mlir::TF::ConstOp>(odsLoc,\n                                               /*value=*/epsilon);\n    }\n    ::mlir::TF::AddOp add_op_1;\n    {\n      ::mlir::Value epsilon_value =\n          (*epsilon_const_op.getODSResults(0).begin());\n      // Multiplying with a constant, no need to check broadcastibility.\n      add_op_1 = rewriter.create<::mlir::TF::AddOp>(odsLoc,\n                                                    /*x=*/variance_value,\n                                                    /*y=*/epsilon_value);\n    }\n    ::mlir::TF::RsqrtOp rsqrt_op;\n    {\n      ::mlir::SmallVector<::mlir::Value, 4> tblgen_values;\n      ::mlir::SmallVector<::mlir::NamedAttribute, 4> tblgen_attrs;\n      tblgen_values.push_back((*add_op_1.getODSResults(0).begin()));\n      rsqrt_op = rewriter.create<::mlir::TF::RsqrtOp>(odsLoc, tblgen_values,\n                                                      tblgen_attrs);\n    }\n    ::mlir::TF::MulOp multiplier;\n    {\n      ::mlir::Value tblgen_value_0 = (*scale.begin());\n      ::mlir::Value tblgen_value_1 = (*rsqrt_op.getODSResults(0).begin());\n      multiplier = rewriter.create<::mlir::TF::MulOp>(odsLoc,\n                                                      /*x=*/tblgen_value_0,\n                                                      /*y=*/tblgen_value_1);\n    }\n    ::mlir::TF::MulOp mul_op_1;\n    {\n      ::mlir::Value tblgen_value_0 = (*x.begin());\n      ::mlir::Value tblgen_value_1 = (*multiplier.getODSResults(0).begin());\n      mul_op_1 = rewriter.create<::mlir::TF::MulOp>(odsLoc,\n                                                    /*x=*/tblgen_value_0,\n                                                    /*y=*/tblgen_value_1);\n    }\n    ::mlir::TF::MulOp mul_op_2;\n    {\n      ::mlir::Value multiplier_value = (*multiplier.getODSResults(0).begin());\n      mul_op_2 = rewriter.create<::mlir::TF::MulOp>(odsLoc,\n                                                    /*x=*/mean_value,\n                                                    /*y=*/multiplier_value);\n    }\n    ::mlir::TF::SubOp sub_op;\n    {\n      ::mlir::Value tblgen_value_0 = (*offset.begin());\n      ::mlir::Value tblgen_value_1 = (*mul_op_2.getODSResults(0).begin());\n      sub_op = rewriter.create<::mlir::TF::SubOp>(odsLoc,\n                                                  /*x=*/tblgen_value_0,\n                                                  /*y=*/tblgen_value_1);\n    }\n    ::mlir::TF::AddOp add_op_2;\n    {\n      ::mlir::SmallVector<::mlir::Value, 4> tblgen_values;\n      ::mlir::SmallVector<::mlir::NamedAttribute, 4> tblgen_attrs;\n      tblgen_values.push_back((*mul_op_1.getODSResults(0).begin()));\n      tblgen_values.push_back((*sub_op.getODSResults(0).begin()));\n      ::mlir::SmallVector<::mlir::Type, 4> tblgen_types;\n      for (auto v : fused_batch_norm_op.getODSResults(0)) {\n        tblgen_types.push_back(v.getType());\n      }\n      add_op_2 = rewriter.create<::mlir::TF::AddOp>(\n          odsLoc, tblgen_types, tblgen_values, tblgen_attrs);\n    }\n    for (auto v :\n         ::llvm::SmallVector<::mlir::Value, 4>{add_op_2.getODSResults(0)}) {\n      replace_values.push_back(v);\n    }\n    for (auto v : ::llvm::SmallVector<::mlir::Value, 4>{x}) {\n      replace_values.push_back(v);\n    }\n    for (auto v : ::llvm::SmallVector<::mlir::Value, 4>{x}) {\n      replace_values.push_back(v);\n    }\n    for (auto v : ::llvm::SmallVector<::mlir::Value, 4>{x}) {\n      replace_values.push_back(v);\n    }\n    for (auto v : ::llvm::SmallVector<::mlir::Value, 4>{x}) {\n      replace_values.push_back(v);\n    }\n    for (auto v : ::llvm::SmallVector<::mlir::Value, 4>{x}) {\n      replace_values.push_back(v);\n    }\n    rewriter.replaceOp(fused_batch_norm, replace_values);\n    return success();\n  };\n};\n\n#include \"tensorflow/compiler/mlir/lite/transforms/generated_prepare_tf.inc\"\n\n// Returns success if all the operations in the `op`'s regions including `op`\n// itself are legal in a TFLite pipeline.\nLogicalResult ValidateOp(Operation *op) {\n  bool has_illegal_ops = false;\n  op->walk([&](Operation *op) {\n    if (isa<TF::VariableV2Op>(op)) {\n      has_illegal_ops = true;\n      op->emitOpError() << \"is illegal in a TFLite pipeline\";\n    }\n  });\n\n  return failure(has_illegal_ops);\n}\n\n// Converts a set of TF2XLA ops into pure TF ops for future legalizations as\n// TF2XLA ops aren't supported by later stages.\nLogicalResult ConvertTf2XlaOps(func::FuncOp func, MLIRContext *context) {\n  ConversionTarget target(*context);\n  target.addLegalDialect<arith::ArithmeticDialect>();\n  target.addLegalDialect<func::FuncDialect>();\n  target.addLegalDialect<TF::TensorFlowDialect>();\n  target.addLegalOp<ModuleOp>();\n  target.addLegalOp<func::FuncOp>();\n  target.addIllegalOp<TF::XlaConvV2Op>();\n  target.addIllegalOp<TF::XlaGatherOp>();\n\n  RewritePatternSet patterns(context);\n  mhlo::PopulateLegalizeTfWithTf2XlaPatterns(\"XLA_CPU_JIT\", patterns, context);\n  mhlo::PopulateLegalizeTfPatterns(context, &patterns);\n  TF::PopulateLegalizeHloToTfPatterns(&patterns, context);\n  mhlo::GatherOp::getCanonicalizationPatterns(patterns, context);\n\n  return applyPartialConversion(func, target, std::move(patterns));\n}\n\n// Convert rfft to rfft2d.\n// The transformation pattern looks like below:\n//\n//    input     fft_len\n//     \\      /\n//     rfft\n//\n//     ||\n//     \\/\n//\n//   input       fft_len\n//    \\            /\n//   expand_dim    concat with [1] at the front\n//      \\         /\n//     rfft_2d\n//       |\n//     squeeze\nstruct ConvertRfftToRfft2d : public RewritePattern {\n  explicit ConvertRfftToRfft2d(MLIRContext *context)\n      : RewritePattern(TF::RFFTOp::getOperationName(), 1, context) {}\n\n  LogicalResult matchAndRewrite(Operation *op,\n                                PatternRewriter &rewriter) const override {\n    auto rfft_op = dyn_cast<TF::RFFTOp>(op);\n\n    auto input = rfft_op.input();\n    auto input_type = input.getType().dyn_cast_or_null<RankedTensorType>();\n    if (!input_type) return failure();\n    auto fft_len = rfft_op.fft_length();\n    auto fft_len_type = fft_len.getType().dyn_cast_or_null<ShapedType>();\n    if (!fft_len_type) return failure();\n\n    auto output_type =\n        rfft_op.getResult().getType().dyn_cast_or_null<RankedTensorType>();\n    if (!output_type) return failure();\n\n    // Expanded inputs.\n    // Insert at -2 location.\n    auto one_ele_type =\n        mlir::RankedTensorType::get({1}, rewriter.getIntegerType(32));\n    auto minus_two = CreateConstOpWithSingleValue(&rewriter, rfft_op.getLoc(),\n                                                  one_ele_type, -2);\n\n    SmallVector<int64_t, 4> expanded_input_shape;\n    SmallVector<int64_t, 4> expanded_output_shape;\n    int expanded_rank = input_type.getRank() + 1;\n    int r = 0;\n    for (int i = 0; i < expanded_rank; ++i) {\n      if (i == expanded_rank - 2) {\n        expanded_input_shape.push_back(1);\n        expanded_output_shape.push_back(1);\n      } else {\n        expanded_input_shape.push_back(input_type.getDimSize(r));\n        expanded_output_shape.push_back(output_type.getDimSize(r));\n        r++;\n      }\n    }\n\n    auto expaned_input_type = mlir::RankedTensorType::get(\n        expanded_input_shape, input_type.getElementType());\n    TF::ExpandDimsOp expanded_input = rewriter.create<TF::ExpandDimsOp>(\n        rfft_op.getLoc(), expaned_input_type, input, minus_two->getResult());\n\n    // Expanded fft_len.\n    auto one_attr = mlir::DenseIntElementsAttr::get(one_ele_type, {1});\n\n    auto one = rewriter.create<TF::ConstOp>(rfft_op.getLoc(), one_attr);\n\n    auto zero = CreateConstOpWithSingleValue(&rewriter, rfft_op.getLoc(),\n                                             one_ele_type, 0);\n\n    auto expanded_fft_len_type =\n        mlir::RankedTensorType::get({2}, fft_len_type.getElementType());\n\n    TF::ConcatV2Op expanded_fft_len = rewriter.create<TF::ConcatV2Op>(\n        rfft_op.getLoc(), expanded_fft_len_type,\n        SmallVector<Value, 2>({one.getResult(), fft_len}), zero->getResult());\n\n    // Insert the rfft_2d.\n    auto rfft2d_out_type = mlir::RankedTensorType::get(\n        expanded_output_shape, output_type.getElementType());\n    TF::RFFT2DOp rfft2d = rewriter.create<TF::RFFT2DOp>(\n        rfft_op.getLoc(), rfft2d_out_type, expanded_input.getResult(),\n        expanded_fft_len.getResult());\n\n    // Insert the squeeze op.\n    auto squeeze_dim = rewriter.getI64ArrayAttr({-2});\n    TF::SqueezeOp squeeze = rewriter.create<TF::SqueezeOp>(\n        rfft_op.getLoc(), output_type, rfft2d.getResult(), squeeze_dim);\n\n    rewriter.replaceOp(op, squeeze.getResult());\n\n    return success();\n  }\n};\n\n// Replaces the Identity op with its input in either of the following scenarios\n// : 1) The Identity op's input and output have same types/shapes. 2) The result\n// of Identity op is only used by TF ops.\nstruct RemoveIdentity : public OpRewritePattern<TF::IdentityOp> {\n  using OpRewritePattern<TF::IdentityOp>::OpRewritePattern;\n\n  LogicalResult matchAndRewrite(TF::IdentityOp identity,\n                                PatternRewriter &rewriter) const override {\n    // Replace the op with the input if input and result have the same type.\n    if (identity.input().getType() == identity.getType()) {\n      rewriter.replaceOp(identity, identity.input());\n      return success();\n    }\n    // Replace the op with the input if output is only used by TF ops.\n    // Currently this is more on the conservative side since we need to ensure\n    // every consumer op to be a TF op before applying this pattern. We can\n    // consider to revisit this in the future if this turns out to be too\n    // restrictive.\n    for (Operation *user : identity->getUsers()) {\n      if (user->getDialect()->getNamespace() != \"tf\") {\n        return failure();\n      }\n    }\n\n    rewriter.replaceOp(identity, identity.input());\n    return success();\n  }\n};\n\nvoid PrepareTFPass::runOnOperation() {\n  MLIRContext *ctx = &getContext();\n  RewritePatternSet patterns(ctx);\n  RewritePatternSet phase_2_patterns(ctx);\n  auto func = getOperation();\n\n  // Check illegal ops in a TFLite pipeline (e.g. trainning only ops) , since\n  // PrepareTFPass is the very first TFLite pass in the pipeline.\n  // TODO(jingpu): It might be better to split this check into its own pass\n  // to make things more modular.\n  if (failed(ValidateOp(func))) {\n    func.emitError() << \"tfl-prepare-tf pass failed.\";\n    signalPassFailure();\n    return;\n  }\n\n  if (failed(ConvertTf2XlaOps(func, ctx))) {\n    signalPassFailure();\n    return;\n  }\n\n  // This pattern will try to identify and optimize for dilated convolution.\n  // e.g. Patterns like \"SpaceToBatchND -> Conv2D -> BatchToSpaceND\" will be\n  // replaced with a single Conv op with dilation parameter.\n  patterns.add<ConvertTFDilatedConvOp<TF::Conv2DOp>, FusedBatchNormV3Pat,\n               ConvertTFDilatedConvOp<TF::DepthwiseConv2dNativeOp>>(ctx);\n\n  patterns.add<RemoveIdentity>(ctx);\n  TFL::populateWithGenerated(patterns);\n  // Remove redundant reshape ops.\n  TF::ReshapeOp::getCanonicalizationPatterns(patterns, ctx);\n  // TODO(karimnosseir): Split to separate pass probably after\n  // deciding on long term plan for this optimization.\n  // This will allow optimizing any TF_Mul->TF_Conv in the graph\n  // and any expanded from FusedBatchNorm. We need to do this\n  // before converting TF_Conv to TFL_Conv\n  (void)applyPatternsAndFoldGreedily(func, std::move(patterns));\n\n  // Remove the wrapper of the tf.FakeQuant* ops and also insert the\n  // tfl.quantize and tfl.dequantize to preserve the quantization parameters.\n  // This is done after the first round of optimization to make sure all the\n  // min/max operands of the tf.FakeQuant* are constants to be matched. The\n  // following round of optimization will folding the unwrapped\n  // tf.FakeQuant* ops with the weight constants.\n  if (failed(ConvertFakeQuantOps(func, ctx, use_fake_quant_num_bits_))) {\n    signalPassFailure();\n    return;\n  }\n\n  // Load the generated pattern again, so new quantization pass-through\n  // will be applied.\n  TFL::populateWithGenerated(phase_2_patterns);\n  if (unfold_batch_matmul_) {\n    TF::PopulateUnrollTfBatchMatMul(ctx, phase_2_patterns);\n  }\n  phase_2_patterns\n      .add<TF::ConvertTFEinsumOp, ConvertTFBroadcastTo, ConvertTFStridedSlice,\n           ConvertRfftToRfft2d, RemoveIdentity>(ctx);\n  phase_2_patterns.add<ConvertTFConv2D, ConvertTFDepthwiseConv2dNative>(\n      ctx, allow_bf16_and_f16_type_legalization_);\n  // Remove redundant reshape ops.\n  TF::ReshapeOp::getCanonicalizationPatterns(phase_2_patterns, ctx);\n\n  (void)applyPatternsAndFoldGreedily(func, std::move(phase_2_patterns));\n}\n\n}  // namespace\n\n// Creates an instance of the TensorFlow Lite dialect PrepareTF pass.\nstd::unique_ptr<OperationPass<func::FuncOp>> CreatePrepareTFPass(\n    bool unfold_batch_matmul, bool allow_bf16_and_f16_type_legalization,\n    bool use_fake_quant_num_bits) {\n  return std::make_unique<PrepareTFPass>(unfold_batch_matmul,\n                                         allow_bf16_and_f16_type_legalization,\n                                         use_fake_quant_num_bits);\n}\n\n// Creates an instance of the TensorFlow Lite dialect PrepareTF pass.\nstd::unique_ptr<OperationPass<func::FuncOp>> CreatePrepareTFPass() {\n  return std::make_unique<PrepareTFPass>();\n}\n\n}  // namespace TFL\n}  // namespace mlir\n", "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for lite.py functionality related to TensorFlow 2.0.\"\"\"\n\nimport ctypes\nimport functools\nimport itertools\nimport os\nimport sys\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom six.moves import range\nfrom six.moves import zip\nimport tensorflow as tf\n\n# Force loaded shared object symbols to be globally visible. This is needed so\n# that the interpreter_wrapper, in one .so file, can see the test_registerer,\n# in a different .so file. Note that this may already be set by default.\n# pylint: disable=g-import-not-at-top\nif hasattr(sys, 'setdlopenflags') and hasattr(sys, 'getdlopenflags'):\n  sys.setdlopenflags(sys.getdlopenflags() | ctypes.RTLD_GLOBAL)\n\nfrom tensorflow.lite.python import conversion_metadata_schema_py_generated as metadata_fb\nfrom tensorflow.lite.python import convert\nfrom tensorflow.lite.python import lite\nfrom tensorflow.lite.python import lite_v2_test_util\nfrom tensorflow.lite.python import schema_py_generated as schema_fb\nfrom tensorflow.lite.python import test_util as tflite_test_util\nfrom tensorflow.lite.python import util\nfrom tensorflow.lite.python.convert import mlir_quantize\nfrom tensorflow.lite.python.interpreter import Interpreter\nfrom tensorflow.lite.python.interpreter import InterpreterWithCustomOps\nfrom tensorflow.lite.python.interpreter import OpResolverType\nfrom tensorflow.lite.python.testdata import _pywrap_test_registerer as test_registerer\nfrom tensorflow.lite.python.testdata import double_op\nfrom tensorflow.lite.python.util import get_conversion_metadata\nfrom tensorflow.lite.toco import types_pb2 as _types_pb2\nfrom tensorflow.lite.tools.flatbuffer_utils import convert_bytearray_to_object as _convert_bytearray_to_object\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.framework import versions\nfrom tensorflow.python.lib.io import file_io\nfrom tensorflow.python.ops import map_ops\nfrom tensorflow.python.ops import rnn\nfrom tensorflow.python.platform import resource_loader\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.saved_model import save_options\nfrom tensorflow.python.saved_model import saved_model\nfrom tensorflow.python.saved_model.loader_impl import parse_saved_model\nfrom tensorflow.python.saved_model.save import save\nfrom tensorflow.python.trackable import autotrackable\n\n# Only run jax related tests when we can import jax.\nDISABLE_JAX_TEST = False\ntry:\n  import jax\n  from jax import numpy as jnp\nexcept ImportError:\n  DISABLE_JAX_TEST = True\n# pylint: enable=g-import-not-at-top\n\n\nclass FromConcreteFunctionTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testTypeInvalid(self):\n    root = self._getSimpleVariableModel()\n    with self.assertRaises(ValueError) as error:\n      _ = lite.TFLiteConverterV2.from_concrete_functions([root.f], root)\n    self.assertIn('call get_concrete_function', str(error.exception))\n\n  @test_util.run_v2_only\n  def testFloat(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check output value from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @parameterized.named_parameters(('_INT8InputOutput', dtypes.int8),\n                                  ('_UINT8InputOutput', dtypes.uint8),\n                                  ('_INT16InputOutput', dtypes.int16))\n  @test_util.run_v2_only\n  def testInvalidFloat(self, inference_input_output_type):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    with self.assertRaises(ValueError) as error:\n      converter.inference_input_type = inference_input_output_type\n      converter.inference_output_type = inference_input_output_type\n      converter.convert()\n    self.assertEqual(\n        'The inference_input_type and inference_output_type '\n        'must be tf.float32.', str(error.exception))\n\n  @test_util.run_v2_only\n  def testScalarInput(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testModelWithoutInputs(self):\n\n    def _get_random_number_gen():\n      root = autotrackable.AutoTrackable()\n\n      @tf.function(input_signature=[])\n      def func():\n        return tf.random.uniform(shape=[1], dtype=tf.float32)\n\n      root.f = func\n      to_save = root.f.get_concrete_function()\n      return (root, to_save)\n\n    # Model with no input\n    root, concrete_func = _get_random_number_gen()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n  @test_util.run_v2_only\n  def testMultiFunctionModel(self):\n    \"\"\"Convert a single model in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.add.get_concrete_function(input_data)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.add(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testConvertMultipleFunctions(self):\n    \"\"\"Convert multiple functions in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [add_func, sub_func], root)\n    tflite_model = converter.convert()\n\n    # Check signatures are valid from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 2)\n    self.assertEqual(list(signature_defs.keys()), ['add', 'sub'])\n    self.assertEqual(len(signature_defs.values()), 2)\n    self.assertEqual(list(signature_defs['add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['add']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['add']['outputs']), ['output_0'])\n    self.assertEqual(list(signature_defs['sub'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['sub']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['sub']['outputs']), ['output_0'])\n\n    # Verify the Signature runner executions.\n    add_signature_runner = interpreter.get_signature_runner('add')\n    add_output = add_signature_runner(x=input_data)\n    self.assertEqual(add_output['output_0'], 3)\n    input_details = add_signature_runner.get_input_details()\n    self.assertEqual(1, len(input_details))\n    self.assertEqual('add_x:0', input_details['x']['name'])\n    self.assertEqual(np.float32, input_details['x']['dtype'])\n    self.assertTrue(([1] == input_details['x']['shape']).all())\n    self.assertEqual((0.0, 0), input_details['x']['quantization'])\n\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    sub_output = sub_signature_runner(x=input_data)\n    self.assertEqual(sub_output['output_0'], -2)\n    output_details = sub_signature_runner.get_output_details()\n    self.assertEqual(1, len(output_details))\n    self.assertEqual('StatefulPartitionedCall:0',\n                     output_details['output_0']['name'])\n    self.assertEqual(np.float32, output_details['output_0']['dtype'])\n    self.assertTrue(([1] == output_details['output_0']['shape']).all())\n    self.assertEqual((0.0, 0), output_details['output_0']['quantization'])\n\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.TF_CONCRETE_FUNCTIONS)\n    self.assertAllEqual([], metadata.options.modelOptimizationModes)\n\n  def _getIntegerQuantizeModel(self, num_filters=16):\n    np.random.seed(0)\n\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[1, 5, 5, 3], dtype=tf.float32)])\n    def func(inp):\n      conv = tf.nn.conv2d(\n          inp,\n          tf.ones([3, 3, 3, num_filters]), strides=[1, 1, 1, 1], padding='SAME')\n      output = tf.nn.relu(conv, name='output')\n      return output\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32)]\n\n    root.f = func\n    to_save = root.f.get_concrete_function()\n    return (root, to_save, calibration_gen)\n\n  @parameterized.named_parameters(\n      ('EnableMlirQuantizer', True),  # enable mlir quantizer\n      ('DisableMlirQuantizer', False))  # disable mlir quantizer\n  def testPostTrainingCalibrateAndQuantize(self, mlir_quantizer):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_converter.experimental_new_quantizer = mlir_quantizer\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(\n        metadata.environment.tensorflowVersion.decode('utf-8'),\n        versions.__version__)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.TF_CONCRETE_FUNCTIONS)\n    self.assertEqual(metadata.options.allowCustomOps, False)\n    self.assertEqual(metadata.options.enableSelectTfOps, False)\n    self.assertEqual(metadata.options.forceSelectTfOps, False)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER],\n                        metadata.options.modelOptimizationModes)\n\n    # The default input and output types should be float.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @parameterized.named_parameters(('_INT8InputOutput', dtypes.int8),\n                                  ('_UINT8InputOutput', dtypes.uint8),\n                                  ('_INT16InputOutput', dtypes.int16))\n  @test_util.run_v2_only\n  def testInvalidPostTrainingDynamicRangeQuantization(\n      self, inference_input_output_type):\n    root, func, _ = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    with self.assertRaises(ValueError) as error:\n      quantized_converter.inference_input_type = inference_input_output_type\n      quantized_converter.inference_output_type = inference_input_output_type\n      quantized_converter.convert()\n    self.assertEqual(\n        'The inference_input_type and inference_output_type '\n        'must be tf.float32.', str(error.exception))\n\n  def _createV2QATSavedModelWithFloatOpsAtEnd(self):\n    \"\"\"Create a simple QAT SavedModel that includes float ops at the end.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'qat_float_ops_at_end')\n    input_tensor = tf.keras.layers.Input((32, 32, 128))\n    x = tf.quantization.fake_quant_with_min_max_args(input_tensor, -3.0, 3.0)\n    x = tf.keras.layers.Conv2D(1, (3, 3))(x)\n    x = tf.quantization.fake_quant_with_min_max_args(x, -3.0, 3.0)\n    # Exclude the quantization of the following Dense layer by not putting\n    # fake quant layer after the dense layer.\n    output_tensor = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    model = tf.keras.Model(input_tensor, output_tensor)\n    model.save(saved_model_dir)\n    return saved_model_dir\n\n  def testQuantizationRemovesQDQsForFloatIOInQAT(self):\n    saved_model_dir = self._createV2QATSavedModelWithFloatOpsAtEnd()\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_model = converter.convert()\n\n    # Because assertions on the model later, we opt out applying default TFLite\n    # delegates (i.e. the XNNPACK delegate).\n    interpreter = Interpreter(\n        model_content=quantized_model,\n        experimental_op_resolver_type=OpResolverType\n        .BUILTIN_WITHOUT_DEFAULT_DELEGATES)\n    interpreter.allocate_tensors()\n    # The model should have LOGISTIC op, instead of DEQUANTIZE op.\n    op_details = interpreter._get_ops_details()\n    self.assertEqual(op_details[len(op_details) - 1]['op_name'], 'LOGISTIC')\n\n  @parameterized.named_parameters(\n      ('EnableMlirQuantizer', True),  # enable mlir quantizer\n      ('DisableMlirQuantizer', False))  # disable mlir quantizer\n  def testQuantizationRemovesQDQsForFloatIO(self, mlir_quantizer):\n    func, calibration_gen = self._getSqrtModel()\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [func.get_concrete_function()])\n    converter.representative_dataset = calibration_gen\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.experimental_new_quantizer = mlir_quantizer\n    quantized_model = converter.convert()\n\n    # Because assertions on the model later, we opt out applying default TFLite\n    # delegates (i.e. the XNNPACK delegate).\n    interpreter = Interpreter(\n        model_content=quantized_model,\n        experimental_op_resolver_type=OpResolverType\n        .BUILTIN_WITHOUT_DEFAULT_DELEGATES)\n    interpreter.allocate_tensors()\n    # The model should have only one sqrt op.\n    op_details = interpreter._get_ops_details()\n    self.assertLen(op_details, 1)\n    self.assertEqual(op_details[0]['op_name'], 'SQRT')\n\n  @parameterized.named_parameters(\n      ('_Default', False, False, dtypes.float32),\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize', False, True, dtypes.float32),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly', True, False, dtypes.float32),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize', True, True, dtypes.float32),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16))\n  def testIntegerQuantization(self, is_int_only, is_int16_quantize,\n                              inference_input_output_type):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER]\n    if is_int16_quantize:\n      expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_INT16]\n    self.assertAllEqual(expected_opt_options,\n                        metadata.options.modelOptimizationModes)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details[0]['dtype'])\n\n    # Ensure that the quantized tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(tflite_model))\n\n  @parameterized.named_parameters(\n      ('_INT16Quantize_INT8InputOutput', True, dtypes.int8))\n  def testInvalidIntegerQuantization(self, is_int16_quantize,\n                                     inference_input_output_type):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert quantized model.\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    if is_int16_quantize:\n      quantized_converter.target_spec.supported_ops = [\n          lite.OpsSet.\n          EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n          lite.OpsSet.TFLITE_BUILTINS\n      ]\n    with self.assertRaises(ValueError) as error:\n      quantized_converter.inference_input_type = dtypes.int8\n      quantized_converter.inference_output_type = dtypes.int8\n      quantized_converter.convert()\n    self.assertEqual(\n        'The inference_input_type and inference_output_type '\n        \"must be in ['tf.float32', 'tf.int16'].\", str(error.exception))\n\n  def testCalibrateAndQuantizeBuiltinInt16(self):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    # Convert float model.\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    # TODO(b/156309549): We should add INT16 to the builtin types.\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS_INT8]\n    converter.representative_dataset = calibration_gen\n    converter._experimental_calibrate_only = True\n    calibrated_tflite = converter.convert()\n    quantized_tflite_model = mlir_quantize(\n        calibrated_tflite, inference_type=_types_pb2.QUANTIZED_INT16)\n\n    self.assertIsNotNone(quantized_tflite_model)\n\n    # The default input and output types should be float.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @test_util.run_v2_only\n  def testSignatureDefs(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2([add_func], trackable_obj=root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = add_func(input_data)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default', {'x': input_data})\n    self.assertLen(list(results.keys()), 1)\n    self.assertStartsWith(list(results.keys())[0], 'output')\n    self.assertAllClose(\n        expected_value.numpy(),\n        results[signature_defs['serving_default']['outputs'][0]])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['serving_default'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['serving_default'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['serving_default']['inputs'], ['x'])\n    self.assertLen(list(signature_defs['serving_default']['outputs']), 1)\n    self.assertStartsWith(\n        list(signature_defs['serving_default']['outputs'])[0], 'output')\n\n  @test_util.run_v2_only\n  def testNoSignatureDefsWhenTrackingObjIsNone(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               None)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    # Verify that there is no SignatureDef structure found.\n    self.assertEqual(len(signature_defs), 0)\n\n  @test_util.run_v2_only\n  def testNoSignatureDefsWhenInvalidTrackingObjIsGiven(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], trackable_obj=autotrackable.AutoTrackable())\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    # Verify that there is no SignatureDef structure found.\n    self.assertEqual(len(signature_defs), 0)\n\n  @test_util.run_v2_only\n  def testTrackbleObject(self):\n    \"\"\"Test converting with trackable objects.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [add_func], trackable_obj=root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = add_func(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  def _getTrainingTimeQuantizedModel(self):\n\n    class QLinear(tf.keras.layers.Layer):\n\n      def __init__(self, units=3, **kwargs):\n        super(QLinear, self).__init__(**kwargs)\n        self.units = units\n\n      def build(self, input_shape):\n        self.w = self.add_weight(\n            'weight',\n            shape=(input_shape[-1], self.units),\n            initializer='random_normal',\n            trainable=True)\n        self.min_var = self.add_weight(\n            'min',\n            initializer=tf.keras.initializers.Constant(-6.0),\n            trainable=False)\n        self.max_var = self.add_weight(\n            'max',\n            initializer=tf.keras.initializers.Constant(6.0),\n            trainable=False)\n\n      def call(self, inputs):\n        x = tf.quantization.fake_quant_with_min_max_vars(\n            inputs, self.min_var, self.max_var)\n\n        w_fq = tf.quantization.fake_quant_with_min_max_vars(\n            self.w, self.min_var, self.max_var)\n        x = tf.matmul(x, w_fq)\n\n        x = tf.quantization.fake_quant_with_min_max_vars(\n            x, self.min_var, self.max_var)\n\n        return x\n\n    return tf.keras.Sequential(QLinear(3, input_shape=(2,)))\n\n  @parameterized.named_parameters(\n      ('_DefaultFLOAT32InputOutput', dtypes.float32),\n      ('_INT8InputOutput', dtypes.int8), ('_UINT8InputOutput', dtypes.uint8))\n  @test_util.run_v2_only\n  def testTrainingTimeQuantization(self, inference_input_output_type):\n    model = self._getTrainingTimeQuantizedModel()\n\n    float_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n\n    quantized_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual(\n        [metadata_fb.ModelOptimizationMode.QUANTIZATION_AWARE_TRAINING],\n        metadata.options.modelOptimizationModes)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details[0]['dtype'])\n\n    # Ensure that the quantized tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @test_util.run_v2_only\n  def testNewQuantizer(self):\n    \"\"\"Test the model quantized by the new converter.\"\"\"\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS_INT8\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n\n    # default quantizer\n    quantized_converter.experimental_new_quantizer = False\n    old_tflite = quantized_converter.convert()\n\n    # new quantizer\n    quantized_converter.experimental_new_quantizer = True\n    new_tflite = quantized_converter.convert()\n\n    for _ in range(5):\n      input_data = tf.constant(\n          np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32))\n      old_value = self._evaluateTFLiteModel(old_tflite, [input_data])\n      new_value = self._evaluateTFLiteModel(new_tflite, [input_data])\n      self.assertAllClose(old_value, new_value, atol=1e-01)\n\n  @test_util.run_v2_only\n  def testEmbeddings(self):\n    \"\"\"Test model with embeddings.\"\"\"\n    input_data = tf.constant(\n        np.array(np.random.random_sample((20)), dtype=np.int32))\n\n    class EmbeddingModel(tf.keras.Model):\n\n      def __init__(self):\n        super(EmbeddingModel, self).__init__()\n        self.shared_weights = self.add_weight(\n            'weights',\n            shape=(2000, 300),\n            dtype=tf.float32,\n            initializer=tf.random_normal_initializer(\n                mean=0.0, stddev=300**(-0.5)))\n\n      @tf.function(input_signature=[tf.TensorSpec(shape=(20), dtype=tf.int32)])\n      def func(self, x):\n        return tf.gather(self.shared_weights, x)\n\n    # Building the model.\n    root = EmbeddingModel()\n    concrete_func = root.func.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.func(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertAllClose(expected_value.numpy(), actual_value[0], atol=1e-05)\n\n  @test_util.run_v2_only\n  def testGraphDebugInfo(self):\n    \"\"\"Test a concrete function has debug info captured.\"\"\"\n    root = autotrackable.AutoTrackable()\n    root.v1 = tf.Variable(3.)\n    root.f = tf.function(lambda x: root.v1 * x)\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.convert()\n    self._assertValidDebugInfo(converter._debug_info)\n\n  def _getIntegerQuantizationModelWithFlexOp(self):\n    np.random.seed(0)\n\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[3, 3, 3, 3, 3], dtype=tf.float32)\n    ])\n    def func(inp):\n      tanh = tf.math.tanh(inp)\n      # Flex delegate will merge the consecutive conv3d and erf ops into one\n      # Delegate node.\n      conv3d = tf.nn.conv3d(\n          tanh,\n          tf.ones([3, 3, 3, 3, 3]),\n          strides=[1, 1, 1, 1, 1],\n          padding='SAME')\n      erf = tf.math.erf(conv3d)\n      output = tf.math.tanh(erf)\n      return output\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(3, 3, 3, 3, 3)).astype(np.float32)\n        ]\n\n    root.f = func\n    return (root, root.f.get_concrete_function(), calibration_gen)\n\n  @parameterized.named_parameters(\n      ('_Default', False, False, dtypes.float32),\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize', False, True, dtypes.float32),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly', True, False, dtypes.float32),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize', True, True, dtypes.float32),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16))\n  @test_util.run_v2_only\n  def testIntegerQuantizationWithFlexOp(self, is_int_only, is_int16_quantize,\n                                        inference_input_output_type):\n    root, func, calibration_gen = self._getIntegerQuantizationModelWithFlexOp()\n\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.SELECT_TF_OPS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8, lite.OpsSet.SELECT_TF_OPS\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS,\n            lite.OpsSet.SELECT_TF_OPS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS, lite.OpsSet.SELECT_TF_OPS\n        ]\n\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.options.enableSelectTfOps, True)\n    expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER]\n    if is_int16_quantize:\n      expected_opt_options = [metadata_fb.ModelOptimizationMode.PTQ_INT16]\n    self.assertAllEqual(expected_opt_options,\n                        metadata.options.modelOptimizationModes)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details[0]['dtype'])\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details[0]['dtype'])\n\n  def _getIntegerQuantizationModelWithUnsupportedOps(self):\n    np.random.seed(0)\n\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[3], dtype=tf.float32),\n        tf.TensorSpec(shape=[3], dtype=tf.float32)\n    ])\n    def func(a, b):\n      # ceil kernel does not support int8 nor int16 types neither.\n      left = tf.math.ceil(a)\n      right = tf.nn.tanh(b)\n      add = tf.math.add(left, right)\n      # ceil kernel does not support int8 nor int16 types neither.\n      output = tf.math.ceil(add)\n      return (output, right)\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(3)).astype(np.float32),\n            np.random.uniform(-1, 1, size=(3)).astype(np.float32)\n        ]\n\n    root.f = func\n    return (root, root.f.get_concrete_function(), calibration_gen)\n\n  @parameterized.named_parameters(\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutputMlirQuant', True, False, dtypes.int8, True),\n      ('_IntOnly_UINT8InputOutputMlirQuant', True, False, dtypes.uint8, True))\n  @test_util.run_v2_only\n  def testIntegerQuantizationWithUnsupportedOps(self,\n                                                is_int_only,\n                                                is_int16_quantize,\n                                                inference_input_output_type,\n                                                enable_mlir_quantizer=False):\n    root, func, calib_gen = self._getIntegerQuantizationModelWithUnsupportedOps(\n    )\n\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calib_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8, lite.OpsSet.TFLITE_BUILTINS\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.\n            EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    expected_dtype = inference_input_output_type.as_numpy_dtype\n    # Allow float32 for fallback on non-quantizable op.\n    expected_ceil_dtype = (\n        expected_dtype if enable_mlir_quantizer else dtypes.float32)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 2)\n    self.assertEqual(input_details[0]['dtype'], expected_dtype)\n    self.assertEqual(input_details[1]['dtype'], expected_ceil_dtype)\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 2)\n    self.assertEqual(output_details[0]['dtype'], expected_dtype)\n    self.assertEqual(output_details[1]['dtype'], expected_ceil_dtype)\n\n  def _getIntegerQuantizationModelWithControlFlow(self):\n    def true_fn(x):\n      return x\n\n    def false_fn(x):\n      return x\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, 2], dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.bool)\n    ])\n    def model(x, b):\n      x = x + x\n      x = tf.cond(b, true_fn=lambda: true_fn(x), false_fn=lambda: false_fn(x))\n      return x + x\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(\n                1,\n                2,\n            )).astype(np.float32),\n            tf.constant(True),\n        ]\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(\n                1,\n                2,\n            )).astype(np.float32),\n            tf.constant(False),\n        ]\n\n    return (model, model.get_concrete_function(), calibration_gen)\n\n  @parameterized.named_parameters(\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16),\n      # TODO(b/198231624): Support control flow ops in MLIR quantizer\n      # ('_IntOnly_INT8InputOutputMlirQuant', True, False, dtypes.int8, True),\n      # ('_IntOnly_UINT8InputOutputMlirQuant', True, False, dtypes.uint8, True),\n  )\n  @test_util.run_v2_only\n  def testIntegerQuantizationWithControlFlow(self,\n                                             is_int_only,\n                                             is_int16_quantize,\n                                             inference_input_output_type,\n                                             enable_mlir_quantizer=False):\n    root, func, calib_gen = self._getIntegerQuantizationModelWithControlFlow()\n\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calib_gen\n    if is_int_only:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS_INT8, lite.OpsSet.TFLITE_BUILTINS\n        ]\n    else:\n      if is_int16_quantize:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n      else:\n        quantized_converter.target_spec.supported_ops = [\n            lite.OpsSet.TFLITE_BUILTINS\n        ]\n\n    quantized_converter.inference_input_type = inference_input_output_type\n    quantized_converter.inference_output_type = inference_input_output_type\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    expected_dtype = inference_input_output_type.as_numpy_dtype\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 2)\n    self.assertEqual(input_details[0]['dtype'], expected_dtype)\n    self.assertEqual(input_details[1]['dtype'], dtypes.bool)\n    output_details = interpreter.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertEqual(output_details[0]['dtype'], expected_dtype)\n\n  @parameterized.named_parameters(\n      ('_BlocklistedNoneWithLowering', None, None, True),\n      ('_BlocklistedNoneWithoutLowering', None, None, False),\n      ('_BlocklistedOpsWithLowering', {'CONV_2D'}, None, True),\n      ('_BlocklistedOpsWithoutLowering', {'CONV_2D'}, None, False),\n      ('_BlocklistedNodesWithLowering', None, {'PartitionedCall:0'}, True),\n      ('_BlocklistedNodesWithoutLowering', None, {'Identity'}, False))\n  @test_util.run_v2_only\n  def testNewQuantizerBlocklistingArgs(self, denylisted_ops, denylisted_nodes,\n                                       lower_to_saved_model):\n    \"\"\"Test the model quantized by the new converter and denylisted options.\"\"\"\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS_INT8\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.experimental_new_quantizer = True\n    quantized_converter._experimental_calibrate_only = True\n    quantized_converter.experimental_lower_to_saved_model = lower_to_saved_model\n    calibrated = quantized_converter.convert()\n    quantized_tflite_model = mlir_quantize(\n        calibrated,\n        denylisted_ops=denylisted_ops,\n        denylisted_nodes=denylisted_nodes)\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    details = interpreter.get_tensor_details()\n    num_quantized_tensors = sum(\n        [1 for detail in details\n         if len(detail['quantization_parameters']['scales'])])\n    if denylisted_nodes or denylisted_ops:\n      self.assertEqual(num_quantized_tensors, 0)\n      return\n    self.assertEqual(num_quantized_tensors, 4)  # quant, filter, bias, dequant\n\n  @parameterized.named_parameters(\n      ('_SingleLayer', False),\n      ('_WholeModel', True),\n  )\n  @test_util.run_v2_only\n  def testNewQuantizerNumericVerificationDebugMode(self, whole_model_verify):\n    \"\"\"Test the model quantized by the new converter with numeric verify ops.\"\"\"\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                         root)\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS_INT8\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n\n    # Create a TFLite model with new quantizer.\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.experimental_new_quantizer = True\n    production_tflite = quantized_converter.convert()\n    # Create a TFLite model with new quantizer and numeric verify ops.\n    quantized_converter._experimental_calibrate_only = True\n    calibrated = quantized_converter.convert()\n    debug_mode_tflite = mlir_quantize(\n        calibrated,\n        enable_numeric_verify=True,\n        enable_whole_model_verify=whole_model_verify)\n\n    # Check if adding debug mode should output a different flatbuffer.\n    self.assertNotEqual(production_tflite, debug_mode_tflite)\n\n    # Check if newly added ops are numeric verify ops.\n    input_data = tf.constant(\n        np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32))\n\n    def examine_tflite_model(tflite_content, input_data):\n      interpreter = Interpreter(\n          model_content=tflite_content,\n          experimental_op_resolver_type=OpResolverType\n          .BUILTIN_WITHOUT_DEFAULT_DELEGATES)\n      interpreter.allocate_tensors()\n      input_details = interpreter.get_input_details()\n      interpreter.set_tensor(input_details[0]['index'], input_data.numpy())\n      interpreter.invoke()\n      tensor_details = interpreter.get_tensor_details()\n      return {\n          details['name']: interpreter.get_tensor(details['index'])\n          for details in interpreter.get_tensor_details()\n      }, tensor_details\n\n    tflite_result, _ = examine_tflite_model(production_tflite, input_data)\n    debug_mode_tflite_result, debug_tensor_details = examine_tflite_model(\n        debug_mode_tflite, input_data)\n\n    # MLIR-based quantizer should output flatbuffer model with `tfl.quantize`.\n    num_production_quantize_ops = len([\n        None for output_tensor_name in tflite_result\n        if 'tfl.quantize' in output_tensor_name\n    ])\n    self.assertEqual(num_production_quantize_ops, 1)\n    # MLIR-based quantizer should output flatbuffer model with `tfl.quantize`.\n    num_debug_quantize_ops = len([\n        None for output_tensor_name in debug_mode_tflite_result\n        if 'tfl.quantize' in output_tensor_name\n    ])\n    # Two numbers should be equal.\n    self.assertEqual(num_production_quantize_ops, num_debug_quantize_ops)\n    # DebugMode TFLite flatbuffer should have NumericVerifyOps more than zero.\n    # The name has the prefix \"NumericVerify/{name}:{id}\n    # where {name} is the tensor name of the original quantized op's activation,\n    # and {id} is its tensor id.\n    num_debug_ops = 0\n    for output_tensor_name in debug_mode_tflite_result:\n      if 'NumericVerify' in output_tensor_name:\n        pos_end_prefix = len('NumericVerify/')\n        pos_colon = output_tensor_name.rfind(':')\n        self.assertEqual('NumericVerify/', output_tensor_name[:pos_end_prefix])\n        tensor_id = int(output_tensor_name[pos_colon + 1:])\n        original_tensor_name = output_tensor_name[pos_end_prefix:pos_colon]\n        self.assertEqual(original_tensor_name,\n                         debug_tensor_details[tensor_id]['name'])\n        num_debug_ops += 1\n    self.assertEqual(num_debug_ops, 1)\n    # The number of debug ops should be equal to that of quantized ops.\n    self.assertEqual(num_debug_ops, num_debug_quantize_ops)\n\n  @parameterized.named_parameters(\n      ('_PerChannelQuant', False, False),\n      ('_PerChannelMlirQuant', False, True),\n      ('_PerTensorQuant', True, False),\n      ('_PerTensorMlirQuant', True, True),\n      ('_PerChannelDynamicRange', False, False, False),\n      ('_PerTensorDynamicRange', True, False, False))\n  @test_util.run_v2_only\n  def testDisablePerChannelQuantization(self, disable_per_channel=False,\n                                        enable_mlir_quantizer=False,\n                                        representative_dataset=True):\n    k_conv_name = 'Conv2D'\n    # Dynamic range quant requires total num elements of filters > 1024.\n    k_num_filters = 38\n    root, func, calib_gen = self._getIntegerQuantizeModel(k_num_filters)\n    quantized_converter = tf.lite.TFLiteConverter.from_concrete_functions(\n        [func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calib_gen\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS\n    ]\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n    if disable_per_channel:\n      quantized_converter._experimental_disable_per_channel = (\n          disable_per_channel)\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    detail = next((d for d in interpreter.get_tensor_details()\n                   if d['name'].startswith(k_conv_name)))\n    quant_params = detail['quantization_parameters']\n    expected_num_params = 1 if disable_per_channel else k_num_filters\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n  @parameterized.named_parameters(('MlirQuantize', True),\n                                  ('TocoQuantize', False))\n  @test_util.run_v2_only\n  def testQuantizeBiasOverflow(self, enable_mlir_quantizer):\n    \"\"\"Tests if the quantizer handles bias overflow by adjusting scales.\"\"\"\n    input_data = np.array([[-1e-3, 1e-3]], dtype=np.float32)\n\n    def calibration_gen():\n      yield {'x': input_data}\n\n    root = self._getMatMulModelWithSmallWeights()\n    input_data = tf.constant([-1e-3, 1e-3], shape=(1, 2))\n    concrete_func = root.matmul.get_concrete_function(input_data)\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.experimental_new_quantizer = enable_mlir_quantizer\n    quantized_model = converter.convert()\n\n    interpreter = Interpreter(model_content=quantized_model)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    output_details = interpreter.get_output_details()\n    output = interpreter.get_tensor(output_details[0]['index'])\n    # the inputs and weights are far smaller than the biases, so the final\n    # result should be equal to the biases.\n    self.assertAllClose(root.bias, output.flatten())\n\n  @test_util.run_v2_only\n  def testOpVersion(self):\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[5, 5], dtype=tf.float32)])\n    def custom_resize(image):\n      # Add \"batch\" and \"channels\" dimensions\n      image = image[tf.newaxis, ..., tf.newaxis]\n      # ResizeBilinear version 3.\n      resize1 = tf.compat.v1.image.resize_bilinear(\n          image, [2, 2], half_pixel_centers=True)\n      # ResizeBilinear version 1.\n      resize2 = tf.compat.v1.image.resize_bilinear(image, [2, 2])\n      return resize1 + resize2\n\n    concrete_func = custom_resize.get_concrete_function()\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               custom_resize)\n    tflite_model = converter.convert()\n    model_object = schema_fb.Model.GetRootAsModel(tflite_model, 0)\n    model = schema_fb.ModelT.InitFromObj(model_object)\n\n    for operator in model.operatorCodes:\n      if operator.builtinCode == schema_fb.BuiltinOperator.RESIZE_BILINEAR:\n        # half_pixel_centers is supported by ResizeBilinear version 3.\n        self.assertEqual(operator.version, 3)\n        break\n\n  @test_util.run_v2_only\n  def testForceSelectTFOps(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.options.forceSelectTfOps, True)\n\n    # Check output value from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  def testExcludeConversionMetadata(self):\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    converter.exclude_conversion_metadata = True\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNone(metadata)\n\n  def testConversionMetadataForDynamicRange(self):\n    func, _ = self._getSqrtModel()\n    converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [func.get_concrete_function()])\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.PTQ_DYNAMIC_RANGE],\n                        metadata.options.modelOptimizationModes)\n\n  def testConversionMetadataForFloat16(self):\n    root, func, calibration_gen = self._getIntegerQuantizeModel()\n    converter = lite.TFLiteConverterV2.from_concrete_functions([func], root)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.target_spec.supported_types = [dtypes.float16]\n    quantized_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.PTQ_FLOAT16],\n                        metadata.options.modelOptimizationModes)\n\n\nclass FromSavedModelTest(lite_v2_test_util.ModelTest):\n\n  def _createV1SavedModel(self, shape):\n    \"\"\"Create a simple SavedModel.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_savedmodel')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor_1 = tf.compat.v1.placeholder(\n            shape=shape, dtype=tf.float32, name='inputB')\n        in_tensor_2 = tf.compat.v1.placeholder(\n            shape=shape, dtype=tf.float32, name='inputA')\n        variable_node = tf.Variable(1.0, name='variable_node')\n        out_tensor = in_tensor_1 + in_tensor_2 * variable_node\n        inputs = {'x': in_tensor_1, 'y': in_tensor_2}\n        outputs = {'z': out_tensor}\n        sess.run(tf.compat.v1.variables_initializer([variable_node]))\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n    return saved_model_dir\n\n  def _createV2QATSavedModel(self, shape):\n    \"\"\"Create a simple QAT SavedModel in TF 2.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    input_name = 'input'\n    output_name = 'scores'\n\n    input_tensor = tf.keras.layers.Input((32, 32, 128), name=input_name)\n    x = tf.quantization.fake_quant_with_min_max_args(input_tensor, -3.0, 3.0)\n    x = tf.keras.layers.Conv2D(1, (3, 3))(x)\n    x = tf.quantization.fake_quant_with_min_max_args(x, -3.0, 3.0)\n    scores = tf.keras.layers.Reshape((-1,), name=output_name)(x)\n    model = tf.keras.Model(input_tensor, scores)\n    model.save(saved_model_dir)\n    return saved_model_dir, input_name, output_name\n\n  @test_util.run_v2_only\n  def testV1SimpleModel(self):\n    \"\"\"Test a SavedModel.\"\"\"\n    with tf.Graph().as_default():\n      saved_model_dir = self._createV1SavedModel(shape=[1, 16, 16, 3])\n\n      # Convert model and ensure model is not None.\n      converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n      tflite_model = converter.convert()\n      self.assertTrue(tflite_model)\n\n      interpreter = Interpreter(model_content=tflite_model)\n      interpreter.allocate_tensors()\n\n      input_details = interpreter.get_input_details()\n      self.assertLen(input_details, 2)\n      self.assertStartsWith(input_details[0]['name'], 'inputA')\n      self.assertEqual(np.float32, input_details[0]['dtype'])\n      self.assertAllEqual([1, 16, 16, 3], input_details[0]['shape'])\n      self.assertEqual((0., 0.), input_details[0]['quantization'])\n\n      self.assertStartsWith(\n          input_details[1]['name'],\n          'inputB',\n      )\n      self.assertEqual(np.float32, input_details[1]['dtype'])\n      self.assertTrue([1, 16, 16, 3], input_details[1]['shape'])\n      self.assertEqual((0., 0.), input_details[1]['quantization'])\n\n      output_details = interpreter.get_output_details()\n      self.assertLen(output_details, 1)\n      self.assertStartsWith(output_details[0]['name'], 'add')\n      self.assertEqual(np.float32, output_details[0]['dtype'])\n      self.assertTrue([1, 16, 16, 3], output_details[0]['shape'])\n      self.assertEqual((0., 0.), output_details[0]['quantization'])\n\n  @parameterized.named_parameters(\n      ('Default', False),\n      ('UnfoldLargeConstant', True),\n  )\n  @test_util.run_v2_only\n  def testUnfoldLargeConstant(self, unfold_large_constant):\n    \"\"\"Test unfolding large splat constant in a TF Lite model.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_savedmodel')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor = tf.compat.v1.placeholder(\n            shape=[1000, 1000], dtype=tf.float32, name='input')\n        constant = tf.constant(value=1, dtype=tf.float32, shape=[1000, 1000])\n        out_tensor = in_tensor + constant\n        inputs = {'x': in_tensor}\n        outputs = {'y': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter._experimental_unfold_large_splat_constant = unfold_large_constant\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    model = util._convert_model_from_bytearray_to_object(tflite_model)\n    if unfold_large_constant:\n      self.assertEqual(model.operatorCodes[0].builtinCode,\n                       schema_fb.BuiltinOperator.FILL)\n      self.assertEqual(model.operatorCodes[1].builtinCode,\n                       schema_fb.BuiltinOperator.ADD)\n    else:\n      self.assertEqual(model.operatorCodes[0].builtinCode,\n                       schema_fb.BuiltinOperator.ADD)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual('input:0', input_details[0]['name'])\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertAllEqual([1000, 1000], input_details[0]['shape'])\n    self.assertEqual((0., 0.), input_details[0]['quantization'])\n\n    output_details = interpreter.get_output_details()\n    self.assertEqual('add:0', output_details[0]['name'])\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n    self.assertAllEqual([1000, 1000], output_details[0]['shape'])\n    self.assertEqual((0., 0.), output_details[0]['quantization'])\n\n    interpreter.set_tensor(input_details[0]['index'],\n                           np.ones(shape=[1000, 1000], dtype=np.float32))\n    interpreter.invoke()\n    self.assertAllEqual(\n        np.full(shape=[1000, 1000], fill_value=2.0, dtype=np.float32),\n        interpreter.get_tensor(output_details[0]['index']))\n\n  @test_util.run_v2_only\n  def testPreserveAssert(self):\n    \"\"\"Test preserving AssertOp in a TF Lite model.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_savedmodel')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor = tf.compat.v1.placeholder(\n            shape=[10, 10], dtype=tf.float32, name='input')\n        constant = tf.constant(value=1, dtype=tf.float32, shape=[10, 10])\n        assert_op = tf.Assert(tf.less_equal(in_tensor, constant), [in_tensor])\n        with tf.control_dependencies([assert_op]):\n          out_tensor = in_tensor + constant\n        inputs = {'x': in_tensor}\n        outputs = {'y': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    converter._experimental_preserve_assert_op = True\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    model = util._convert_model_from_bytearray_to_object(tflite_model)\n    has_assert = False\n    for op_code in model.operatorCodes:\n      if op_code.customCode == b'FlexAssert':\n        has_assert = True\n        break\n    self.assertTrue(has_assert)\n\n  @test_util.run_v2_only\n  def testTF1HubFormattedModel(self):\n    \"\"\"Test a TF1 hub formatted model.\"\"\"\n    saved_model_dir = self._createV1SavedModel(shape=[1, 16, 16, 3])\n\n    # TF1 hub model is based on V1 saved model and they omit the saved model\n    # schema version setting.\n    saved_model_proto = parse_saved_model(saved_model_dir)\n    saved_model_proto.saved_model_schema_version = 0\n\n    saved_model_pb_file_path = os.path.join(saved_model_dir, 'saved_model.pb')\n    with file_io.FileIO(saved_model_pb_file_path, 'wb') as writer:\n      writer.write(saved_model_proto.SerializeToString())\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  def _createV1ModelWithHashTableInitializer(self):\n    # Create a v1 saved model with hash table initializers.\n    tf.compat.v1.disable_eager_execution()\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'savedmodel_with_hashtable')\n\n    table_initializer = tf.lookup.KeyValueTensorInitializer(\n        keys=['a', 'b', 'c', 'd'],\n        values=[1, 2, 3, 4],\n        key_dtype=tf.string,\n        value_dtype=tf.int64)\n    table = tf.lookup.StaticHashTable(\n        table_initializer, default_value=tf.constant(-1, dtype=tf.int64))\n\n    x = tf.compat.v1.placeholder(tf.string, shape=(), name='input')\n    y = table.lookup(x)\n\n    tensor_info_x = tf.compat.v1.saved_model.utils.build_tensor_info(x)\n    tensor_info_y = tf.compat.v1.saved_model.utils.build_tensor_info(y)\n\n    signature_def_map, init_op, assets_collection = {\n        'serving_default':\n            (tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n                inputs={'x': tensor_info_x},\n                outputs={'y': tensor_info_y},\n                method_name='some_function'))\n    }, tf.compat.v1.tables_initializer(), None\n\n    sess = tf.compat.v1.Session()\n    sess.run(tf.compat.v1.initializers.global_variables())\n\n    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(\n        saved_model_dir)\n    builder.add_meta_graph_and_variables(\n        sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\n        signature_def_map,\n        main_op=init_op,\n        assets_collection=assets_collection,\n        strip_default_attrs=True)\n    builder.save()\n\n    # Restore TF v2 behavior.\n    tf.compat.v1.reset_default_graph()\n    tf.compat.v1.enable_eager_execution()\n    return saved_model_dir\n\n  @test_util.run_v2_only\n  def testModelWithHashTableInitializer(self):\n    \"\"\"Test a model with saved_model's session initializer for hash tables.\"\"\"\n    saved_model_dir = self._createV1ModelWithHashTableInitializer()\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_data = np.array(['a', 'b', 'c', 'z'], dtype=np.string_)\n    interpreter.resize_tensor_input(\n        input_details[0]['index'], [4], strict=False)\n    interpreter.allocate_tensors()\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Invoke multiple times to ensure the initializer graph runs only once.\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 2, 3, -1], list(actual_value))\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 2, 3, -1], list(actual_value))\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 2, 3, -1], list(actual_value))\n\n  def _createV1ModelWithMutableHashTable(self):\n    # Create a v1 saved model with mutable hash table.\n    tf.compat.v1.disable_eager_execution()\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'savedmodel_with_mutable_hashtable')\n\n    table = tf.raw_ops.MutableHashTableV2(\n        key_dtype=tf.string, value_dtype=tf.int64)\n    x = tf.compat.v1.placeholder(tf.string, shape=(), name='input')\n    keys = tf.constant(['a', 'b'], tf.string)\n    values = tf.constant([1, 5], tf.int64)\n    default_value = tf.constant(-1, tf.int64)\n    insert_call = tf.raw_ops.LookupTableInsertV2(\n        table_handle=table, keys=keys, values=values)\n    with tf.control_dependencies([insert_call]):\n      y = tf.raw_ops.LookupTableFindV2(\n          table_handle=table, keys=x, default_value=default_value)\n\n    tensor_info_x = tf.compat.v1.saved_model.utils.build_tensor_info(x)\n    tensor_info_y = tf.compat.v1.saved_model.utils.build_tensor_info(y)\n\n    signature_def_map, init_op, assets_collection = {\n        'serving_default':\n            (tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n                inputs={'x': tensor_info_x},\n                outputs={'y': tensor_info_y},\n                method_name='some_function'))\n    }, tf.compat.v1.tables_initializer(), None\n\n    sess = tf.compat.v1.Session()\n\n    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(\n        saved_model_dir)\n    builder.add_meta_graph_and_variables(\n        sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\n        signature_def_map,\n        main_op=init_op,\n        assets_collection=assets_collection,\n        strip_default_attrs=True)\n    builder.save()\n\n    # Restore TF v2 behavior.\n    tf.compat.v1.reset_default_graph()\n    tf.compat.v1.enable_eager_execution()\n    return saved_model_dir\n\n  @test_util.run_v2_only\n  def testModelWithMutableHashTable(self):\n    \"\"\"Test a model with saved_model's session initializer for hash tables.\"\"\"\n    saved_model_dir = self._createV1ModelWithMutableHashTable()\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_data = np.array(['a', 'b', 'c'], dtype=np.string_)\n    interpreter.resize_tensor_input(\n        input_details[0]['index'], [3], strict=False)\n    interpreter.allocate_tensors()\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([1, 5, -1], list(actual_value))\n\n  @test_util.run_v2_only\n  def testReduceSumWithInt16Quant(self):\n    \"\"\"Test a model with quantized int16 reduce sum op.\"\"\"\n    inp = tf.keras.Input([3, 3], 3, name='x')\n    m = tf.keras.Model(inp, tf.reduce_sum(inp, axis=-1))\n\n    converter = tf.lite.TFLiteConverter.from_keras_model(m)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet\n        .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n    ]\n    converter.inference_input_type = tf.int16\n    converter.inference_output_type = tf.int16\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    inputs = {\n        i.name: np.random.normal(size=i.shape).astype(np.float32)\n        for i in m.inputs\n    }\n    converter.representative_dataset = lambda: [inputs]\n    content = converter.convert()\n\n    interpreter = tf.lite.Interpreter(model_content=content)\n    runner = interpreter.get_signature_runner('serving_default')\n    y = runner(x=np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]).astype(np.int16))\n    self.assertEqual([3, 6, 9], list(list(y.values())[0]))\n\n  @test_util.run_v2_only\n  def testConstModel(self):\n    \"\"\"Test a basic model with functions to make sure functions are inlined.\"\"\"\n    input_data = tf.constant(1., shape=[1])\n    root = autotrackable.AutoTrackable()\n    root.f = tf.function(lambda x: 2. * x)\n    to_save = root.f.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testVariableModel(self):\n    \"\"\"Test a basic model with Variables with saving/loading the SavedModel.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    to_save = root.f.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.TF_SAVED_MODEL)\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @parameterized.named_parameters(('EnableResourceVariables', True),\n                                  ('DisableResourceVariables', False))\n  @test_util.run_v2_only\n  def testNativeVariablesModel(self, enable_resource_variables):\n    \"\"\"Test a basic model with Variables with saving/loading the SavedModel.\"\"\"\n    root = self._getSimpleModelWithVariables()\n    input_data = tf.constant(1., shape=[1, 10])\n    to_save = root.assign_add.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    converter.experimental_enable_resource_variables = enable_resource_variables\n\n    if not enable_resource_variables:\n      with self.assertRaises(convert.ConverterError) as error:\n        tflite_model = converter.convert()\n      self.assertIn(\n          'Variable constant folding is failed. Please consider using enabling '\n          '`experimental_enable_resource_variables` flag in the TFLite '\n          'converter object.',\n          str(error.exception))\n      return\n\n    # Enable resource variables.\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.assign_add(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    for tf_result, tflite_result in zip(expected_value, actual_value[0]):\n      self.assertAllClose(tf_result, tflite_result, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testSignatures(self):\n    \"\"\"Test values for `signature_keys` argument.\"\"\"\n    root = self._getSimpleVariableModel()\n    input_data = tf.constant(1., shape=[1])\n    to_save = root.f.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save)\n\n    # Convert model with invalid `signature_keys`.\n    with self.assertRaises(ValueError) as error:\n      _ = lite.TFLiteConverterV2.from_saved_model(\n          save_dir, signature_keys=['INVALID'])\n    self.assertIn(\"Invalid signature key 'INVALID'\", str(error.exception))\n\n    # Convert model with empty `signature_keys`.\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=[])\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value.numpy(), actual_value)\n\n  @test_util.run_v2_only\n  def testSignatureDefsWithFullIntegerQuantization(self):\n    # SETUP\n    # 1. Define input shapes\n    tf_input_shape = (32, 32, 128)\n    tflite_input_shape = (1,) + tf_input_shape\n    # 2. Define model\n    tf_saved_model_dir, input_name, output_name = (\n        self._createV2QATSavedModel(tf_input_shape))\n\n    # MODEL 1: TFLite (float) model\n    # 1. Create TFLite model\n    converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_dir)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n    # 2. Initialize the Intepreter\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()[0]\n    interpreter.resize_tensor_input(input_details['index'], tflite_input_shape)\n    interpreter.allocate_tensors()\n    signature_list = interpreter._get_full_signature_list()['serving_default']\n    # 3. (Skip) Verify that signature def input/output tensors are in the model.\n    # 4. Evaluate the model\n    input_data = np.random.random(tflite_input_shape).astype(np.float32)\n    result = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default', {input_name: input_data})[output_name]\n\n    # MODEL 2: TFLite (full integer quantized) model\n    # 1. Create TFLite model\n    converter = tf.lite.TFLiteConverter.from_saved_model(tf_saved_model_dir)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.inference_input_type = tf.int8\n    converter.inference_output_type = tf.int8\n    tflite_model_quant = converter.convert()\n    # 2. Initialize the Intepreter\n    interpreter = Interpreter(model_content=tflite_model_quant)\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()[0]\n    interpreter.resize_tensor_input(input_details['index'], tflite_input_shape)\n    interpreter.allocate_tensors()\n    # 3. Verify that signature def input/output tensors are in the model.\n    all_indices = {item['index'] for item in interpreter.get_tensor_details()}\n    signature_list = interpreter._get_full_signature_list()['serving_default']\n    input_tensor_indices = set(signature_list['inputs'].values())\n    assert input_tensor_indices.issubset(all_indices)\n    output_tensor_indices = set(signature_list['outputs'].values())\n    assert output_tensor_indices.issubset(all_indices)\n\n    # 4. Evaluate the model\n    input_data = np.random.random(tflite_input_shape)\n    input_scale, input_zero_point = input_details['quantization']\n    if (input_scale, input_zero_point) != (0.0, 0):\n      input_data = input_data / input_scale + input_zero_point\n      input_data = input_data.astype(input_details['dtype'])\n    result_quant = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model_quant, 'serving_default',\n        {input_name: input_data})[output_name]\n    output_scale, output_zero_point = output_details['quantization']\n    if (output_scale, output_zero_point) != (0.0, 0):\n      result_quant = result_quant.astype(np.float32)\n      result_quant = (result_quant - output_zero_point) * output_scale\n\n    # COMPARE: Validate that results from both models are approx. the same.\n    root_mean_squared = np.sqrt(np.mean((result-result_quant)**2))\n    assert root_mean_squared < 1.0\n\n  @test_util.run_v2_only\n  def testSignatureDefs(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data_0 = tf.constant(1., shape=[1])\n    input_data_1 = tf.constant(3., shape=[1])\n    mul_add_func = root.mul_add.get_concrete_function(input_data_1,\n                                                      input_data_0)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'mul_add': mul_add_func})\n\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=['mul_add'])\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.mul_add(input_data_1, input_data_0)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'mul_add', {\n            'y': input_data_0,\n            'x': input_data_1\n        })\n    self.assertEqual(list(results.keys()), ['output_0'])\n    self.assertEqual(expected_value.numpy(), results['output_0'])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['mul_add'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['mul_add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['mul_add']['inputs'], ['x', 'y'])\n    self.assertEqual(list(signature_defs['mul_add']['outputs']), ['output_0'])\n\n  @test_util.run_v2_only\n  def testSignatureDefsWithDefaultValue(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\n\n    This test uses None as signature_key to test default behavior.\n    \"\"\"\n    root = self._getMultiFunctionModel()\n    input_data_0 = tf.constant(1., shape=[1])\n    input_data_1 = tf.constant(3., shape=[1])\n    mul_add_func = root.mul_add.get_concrete_function(input_data_1,\n                                                      input_data_0)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'mul_add': mul_add_func})\n\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=['mul_add'])\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.mul_add(input_data_1, input_data_0)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, None, {\n            'y': input_data_0,\n            'x': input_data_1\n        })\n    self.assertEqual(list(results.keys()), ['output_0'])\n    self.assertEqual(expected_value.numpy(), results['output_0'])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['mul_add'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['mul_add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['mul_add']['inputs'], ['x', 'y'])\n    self.assertEqual(list(signature_defs['mul_add']['outputs']), ['output_0'])\n\n  @test_util.run_v2_only\n  def testSignatureDefsQuantizedModel(self):\n    \"\"\"Test converting SignatureDef on quantized model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data_0 = tf.constant(1., shape=[1])\n    input_data_1 = tf.constant(3., shape=[1])\n    mul_add_func = root.mul_add.get_concrete_function(input_data_1,\n                                                      input_data_0)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'mul_add': mul_add_func})\n\n    converter = lite.TFLiteConverterV2.from_saved_model(\n        save_dir, signature_keys=['mul_add'])\n\n    def representative_dataset_gen():\n      for _ in range(2):\n        yield {\n            'x':\n                np.random.uniform(low=0, high=1,\n                                  size=(1, 1)).astype(np.float32),\n            'y':\n                np.random.uniform(low=0, high=1, size=(1, 1)).astype(np.float32)\n        }\n\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = representative_dataset_gen\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n    tflite_model = converter.convert()\n\n    # Check signatures are valid from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['mul_add'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['mul_add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['mul_add']['inputs'], ['x', 'y'])\n    self.assertEqual(list(signature_defs['mul_add']['outputs']), ['output_0'])\n\n  @test_util.run_v2_only\n  def testMultipleFunctionModel(self):\n    \"\"\"Convert multiple functions in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'add': add_func, 'sub': sub_func})\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 2)\n    self.assertEqual(list(signature_defs.keys()), ['add', 'sub'])\n    self.assertEqual(len(signature_defs.values()), 2)\n    self.assertEqual(list(signature_defs['add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['add']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['add']['outputs']), ['output_0'])\n    self.assertEqual(list(signature_defs['sub'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['sub']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['sub']['outputs']), ['output_0'])\n\n    # Verify the Signature runner executions.\n    add_signature_runner = interpreter.get_signature_runner('add')\n    add_output = add_signature_runner(x=input_data)\n    self.assertEqual(add_output['output_0'], 3)\n\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    sub_output = sub_signature_runner(x=input_data)\n    self.assertEqual(sub_output['output_0'], -2)\n\n  @parameterized.named_parameters(\n      ('_Default', False, False, dtypes.float32, False),\n      ('_DefaultMlirQuant', False, False, dtypes.float32, True),\n      ('_INT8InputOutput', False, False, dtypes.int8),\n      ('_UINT8InputOutput', False, False, dtypes.uint8),\n      ('_INT16Quantize_INT16InputOutput', False, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutput', True, False, dtypes.int8),\n      ('_IntOnly_UINT8InputOutput', True, False, dtypes.uint8),\n      ('_IntOnly_INT16Quantize_INT16InputOutput', True, True, dtypes.int16),\n      ('_IntOnly_INT8InputOutputMlirQuant', True, False, dtypes.int8, True),\n      ('_IntOnly_UINT8InputOutputMlirQuant', True, False, dtypes.uint8, True))\n  @test_util.run_v2_only\n  def testMultipleFunctionQuantizedModel(self,\n                                         is_int_only,\n                                         is_int16_quantize,\n                                         inference_input_output_type,\n                                         enable_mlir_quantizer=False):\n    \"\"\"Convert multiple functions in a multi-functional model.\"\"\"\n    root = self._getMultiFunctionModel()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'add': add_func, 'sub': sub_func})\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n\n    def representative_dataset_gen():\n      for _ in range(2):\n        yield ('add', {\n            'x': np.random.uniform(low=0, high=1, size=(1,)).astype(np.float32),\n        })\n      for _ in range(2):\n        yield ('sub', {\n            'x': np.random.uniform(low=0, high=1, size=(1,)).astype(np.float32),\n        })\n\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = representative_dataset_gen\n    if is_int_only:\n      if is_int16_quantize:\n        converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n        ]\n      else:\n        converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS_INT8]\n    else:\n      if is_int16_quantize:\n        converter.target_spec.supported_ops = [\n            lite.OpsSet\n            .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n        ]\n      else:\n        converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS]\n    converter.inference_input_type = inference_input_output_type\n    converter.inference_output_type = inference_input_output_type\n    converter.experimental_new_quantizer = enable_mlir_quantizer\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 2)\n    self.assertEqual(list(signature_defs.keys()), ['add', 'sub'])\n    self.assertEqual(len(signature_defs.values()), 2)\n    self.assertEqual(list(signature_defs['add'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['add']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['add']['outputs']), ['output_0'])\n    self.assertEqual(list(signature_defs['sub'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['sub']['inputs'], ['x'])\n    self.assertEqual(list(signature_defs['sub']['outputs']), ['output_0'])\n\n    # Verify the Signature runner executions.\n    input_data = tf.constant(\n        np.random.uniform(-1, 1, size=(1,)).astype(\n            inference_input_output_type.as_numpy_dtype))\n    add_signature_runner = interpreter.get_signature_runner('add')\n    add_output = add_signature_runner(x=input_data)\n    self.assertIsNotNone(add_output['output_0'])\n    input_details = add_signature_runner.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertStartsWith(input_details['x']['name'], 'add_x:0')\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     input_details['x']['dtype'])\n    self.assertTrue(([1] == input_details['x']['shape']).all())\n    if inference_input_output_type == dtypes.float32:\n      self.assertEqual((0.0, 0), input_details['x']['quantization'])\n\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    sub_output = sub_signature_runner(x=input_data)\n    self.assertIsNotNone(sub_output['output_0'])\n    output_details = sub_signature_runner.get_output_details()\n    self.assertLen(output_details, 1)\n    self.assertStartsWith(output_details['output_0']['name'],\n                          'StatefulPartitionedCall:0')\n    self.assertEqual(inference_input_output_type.as_numpy_dtype,\n                     output_details['output_0']['dtype'])\n    self.assertTrue(([1] == output_details['output_0']['shape']).all())\n    if inference_input_output_type == dtypes.float32:\n      self.assertEqual((0.0, 0), output_details['output_0']['quantization'])\n\n  @test_util.run_v2_only\n  def testMultipleFunctionModelWithSharedWeight(self):\n    \"\"\"Convert multiple functions with the shared weight.\"\"\"\n    root = self._getMultiFunctionModelWithSharedWeight()\n    input_data = tf.constant(1., shape=[1])\n    add_func = root.add.get_concrete_function(input_data)\n    sub_func = root.sub.get_concrete_function(input_data)\n    mul_func = root.mul.get_concrete_function(input_data)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, {'add': add_func, 'sub': sub_func, 'mul': mul_func})\n\n    # Try converting multiple functions.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Make sure that the weight tensors are shared.\n    self.assertLess(len(tflite_model), 1100000)\n\n    # TODO(b/184696047): Write down the test codes for multiple signature\n    #                    runners once the Python API is ready to use.\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    self.assertLen(signature_defs, 3)\n    add_signature_runner = interpreter.get_signature_runner('add')\n    sub_signature_runner = interpreter.get_signature_runner('sub')\n    mul_signature_runner = interpreter.get_signature_runner('mul')\n    self.assertIsNotNone(add_signature_runner)\n    self.assertIsNotNone(sub_signature_runner)\n    self.assertIsNotNone(mul_signature_runner)\n\n  @test_util.run_v2_only\n  def testNoConcreteFunctionModel(self):\n    root = self._getMultiFunctionModel()\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir)\n\n    with self.assertRaises(ValueError) as error:\n      _ = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    self.assertIn('Only support at least one signature key.',\n                  str(error.exception))\n\n  @test_util.run_v2_only\n  def testKerasSequentialModel(self):\n    \"\"\"Test a simple sequential tf.Keras model.\"\"\"\n    input_data = tf.constant(1., shape=[1, 1])\n\n    x = np.array([[1.], [2.]])\n    y = np.array([[2.], [4.]])\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1),\n    ])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(x, y, epochs=1)\n\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(model, save_dir)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testGraphDebugInfo(self):\n    \"\"\"Test a SavedModel has debug info captured.\"\"\"\n    input_data = tf.constant(1., shape=[1])\n    root = autotrackable.AutoTrackable()\n    root.f = tf.function(lambda x: 2. * x)\n    to_save = root.f.get_concrete_function(input_data)\n    options = save_options.SaveOptions(save_debug_info=True)\n    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n    save(root, save_dir, to_save, options)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_saved_model(save_dir)\n    converter.convert()\n    self._assertValidDebugInfo(converter._debug_info)\n\n  @test_util.run_v2_only\n  def testNonStatefulConvLSTM2D(self):\n    \"\"\"Test saved model with non stateful ConvLSTM2D keras layer.\"\"\"\n    # Create keras model\n    model = tf.keras.Sequential([\n        tf.keras.layers.ConvLSTM2D(\n            32, (3, 3),\n            padding='same',\n            return_sequences=True,\n            stateful=False,\n            batch_input_shape=(1, 1, 10, 10, 1))\n    ])\n    model.compile()\n\n    # Export the keras model to saved model.\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'conv_lstm_2d')\n    model.save(saved_model_dir, save_format='tf', include_optimizer=False)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  @test_util.run_v2_only\n  def testKerasConvLSTM2DWithMoreThanOneDilationRate(self):\n    input_tensor = tf.keras.layers.Input(\n        batch_size=8,\n        shape=[9, 10, 11, 12],\n        name='input_tensor',\n        dtype=tf.float32)\n\n    output = tf.keras.layers.ConvLSTM2D(\n        filters=3,\n        kernel_size=3,\n        strides=1,\n        padding='VALID',\n        dilation_rate=2,\n        use_bias=False,\n        bias_initializer='ones',\n        data_format='channels_last')(\n            input_tensor)\n\n    model = tf.keras.Model(inputs=[input_tensor], outputs=output)\n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy'])\n\n    # Export the keras model to saved model.\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'conv_lstm_2d_with_dilation_rate')\n    model.save(saved_model_dir, save_format='tf', include_optimizer=False)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  @test_util.run_v2_only\n  def testKerasFullyConnectedOutputShape3D(self):\n    \"\"\"Create a simple FullyConnected Model with an output of three dimensions.\"\"\"\n    input_tensor = tf.keras.layers.Input(\n        batch_size=1, shape=[3, 3], name='input_tensor', dtype=tf.float32)\n\n    x = tf.quantization.fake_quant_with_min_max_args(input_tensor, -3.0, 3.0)\n    x = tf.keras.layers.Dense(3)(x)\n    x = tf.quantization.fake_quant_with_min_max_args(x, -3.0, 3.0)\n    model = tf.keras.Model(input_tensor, x)\n\n    model.compile(\n        optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n    # Export the keras model to saved model.\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'fully_connected_output_3d')\n    model.save(saved_model_dir, save_format='tf', include_optimizer=False)\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    interpreter = Interpreter(model_content=tflite_model)\n    output_details = interpreter.get_output_details()\n    input_details = interpreter.get_input_details()\n    interpreter.allocate_tensors()\n\n    input_data = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]], np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    expected_value = model.predict(input_data)\n\n    self.assertLen(output_details[0]['shape_signature'], 3)\n    self.assertAllClose(expected_value, actual_value, atol=1e-1)\n    self.assertEqual(\n        list(output_details[0]['shape_signature']),\n        list(model.layers[-1].output_shape))\n\n  @test_util.run_v2_only\n  def testKerasConv2DTransposedWithMismatchQuantizedAxes(self):\n\n    class QuantConv2DTransposed(tf.keras.layers.Layer):\n\n      def build(self, input_shape):\n        self.kernel = self.add_weight('kernel', [3, 3, input_shape[-1], 24])\n\n      def call(self, inputs):\n        filters = tf.quantization.fake_quant_with_min_max_vars_per_channel(\n            self.kernel,\n            -3.0 * tf.ones([24]),\n            3.0 * tf.ones([24]),\n            narrow_range=True)\n        filters = tf.transpose(filters, (0, 1, 3, 2))\n        return tf.nn.conv2d_transpose(inputs, filters, [*inputs.shape[:-1], 24],\n                                      1)\n\n    inp = tf.keras.Input(shape=(6, 8, 48), batch_size=1)\n    x = tf.quantization.fake_quant_with_min_max_vars(\n        inp, -3.0, 3.0, narrow_range=True)\n    x = QuantConv2DTransposed()(x)\n    x = tf.quantization.fake_quant_with_min_max_vars(\n        x, -3.0, 3.0, narrow_range=True)\n\n    model = tf.keras.Model(inp, x)\n\n    saved_model_dir = os.path.join(self.get_temp_dir(),\n                                   'keras_conv2d_transpose')\n    model.save(saved_model_dir)\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n    with self.assertRaises(convert.ConverterError) as error:\n      _ = converter.convert()\n    self.assertIn('mismatched quantized axes of input and output',\n                  str(error.exception))\n\n  def _createModelWithInputShape(self, shape):\n    \"\"\"Create a simple SavedModel with a certain shape.\"\"\"\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'input_shape_model')\n    with tf.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        unknown_shape = tf.TensorShape(shape)\n        in_tensor = tf.compat.v1.placeholder(\n            shape=unknown_shape, dtype=tf.float32, name='input')\n        out_tensor = in_tensor + in_tensor\n        inputs = {'input': in_tensor}\n        outputs = {'output': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n    return saved_model_dir\n\n  @test_util.run_v2_only\n  def testUnknownInputShapeModel(self):\n    \"\"\"Test a SavedModel with an unknown input shape.\"\"\"\n    saved_model_dir = self._createModelWithInputShape(None)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Validate that tensors with unknown shape have unknown rank.\n    tflite_model_obj = _convert_bytearray_to_object(tflite_model)\n    for tensor in tflite_model_obj.subgraphs[0].tensors:\n      self.assertEqual(False, tensor.hasRank)\n      self.assertEqual([], tensor.shape.tolist())\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_data = np.array([1., 2., 3.], dtype=np.float32)\n    interpreter.resize_tensor_input(\n        input_details[0]['index'], [3], strict=False)\n    interpreter.allocate_tensors()\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual([2., 4., 6.], list(actual_value))\n\n  @test_util.run_v2_only\n  def testScalarInputShapeModel(self):\n    \"\"\"Test a SavedModel with a scalar input.\"\"\"\n    saved_model_dir = self._createModelWithInputShape([])\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Validate that scalar tensors have a rank = 0.\n    tflite_model_obj = _convert_bytearray_to_object(tflite_model)\n    for tensor in tflite_model_obj.subgraphs[0].tensors:\n      self.assertEqual(True, tensor.hasRank)\n      self.assertEqual([], tensor.shape.tolist())\n\n  @test_util.run_v2_only\n  def testMatrixInputShapeModel(self):\n    \"\"\"Test a SavedModel with a matrix input.\"\"\"\n    saved_model_dir = self._createModelWithInputShape([2, 3])\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n    # Validate that matrix tensors have a rank = 2.\n    tflite_model_obj = _convert_bytearray_to_object(tflite_model)\n    for tensor in tflite_model_obj.subgraphs[0].tensors:\n      self.assertEqual(True, tensor.hasRank)\n      self.assertEqual([2, 3], tensor.shape.tolist())\n\n  @parameterized.named_parameters(\n      ('_PerChannelQuant', False, False),\n      ('_PerChannelMlirQuant', False, True),\n      ('_PerTensorQuant', True, False),\n      ('_PerTensorMlirQuant', True, True),\n      ('_PerChannelDynamicRange', False, False, True),\n      ('_PerTensorDynamicRange', True, False, True))\n  @test_util.run_v2_only\n  def testDisablePerChannelQuantization(self,\n                                        disable_per_channel=False,\n                                        enable_mlir_quantizer=False,\n                                        representative_dataset=True):\n    # Dynamic range quant requires total num elements of filters > 1024.\n    k_num_filters = 38\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(k_num_filters, (3, 3), activation='relu')\n    ])\n    model.build(input_shape=(1, 5, 5, 3))\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'conv_saved_model')\n    save(model, saved_model_dir)\n    k_conv_name = 'sequential/conv2d/Conv2D'\n    quantized_converter = tf.lite.TFLiteConverter.from_saved_model(\n        saved_model_dir)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    if representative_dataset:\n      def calib_gen():\n        for _ in range(5):\n          yield [np.random.uniform(-1, 1, size=(1, 5, 5, 3)).astype(np.float32)]\n      quantized_converter.representative_dataset = calib_gen\n    quantized_converter.target_spec.supported_ops = [\n        lite.OpsSet.TFLITE_BUILTINS\n    ]\n    quantized_converter.experimental_new_quantizer = enable_mlir_quantizer\n    if disable_per_channel:\n      quantized_converter._experimental_disable_per_channel = (\n          disable_per_channel)\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    detail = next((d for d in interpreter.get_tensor_details()\n                   if d['name'].startswith(k_conv_name)))\n    quant_params = detail['quantization_parameters']\n    expected_num_params = k_num_filters\n    if disable_per_channel:\n      expected_num_params = 1\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n  @parameterized.named_parameters(\n      ('_INT8Quant_INT32Bias', False, False, dtypes.int32, True),\n      ('_INT16Quant_INT64Bias', True, False, dtypes.int64, True),\n      ('_INT8Quant_INT32Bias_Set', False, True, dtypes.int32, True),\n      ('_INT8Quant_INT64Bias_Set', False, True, dtypes.int64, False),\n      ('_INT16Quant_INT32Bias_Set', True, True, dtypes.int32, True),\n      ('_INT16Quant_INT64Bias_Set', True, True, dtypes.int64, True),\n      ('_INT16Quant_FLOAT32Bias_Set', True, True, dtypes.float32, False),\n  )\n  @test_util.run_v2_only\n  def testBiasQuantization(self, is_int16_quantize, explicitly_set_bias,\n                           bias_type, is_valid_bias_type):\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(\n            1024, input_shape=[1024], activation=None, bias_initializer='ones')\n    ])\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'dense_saved_model')\n    save(model, saved_model_dir)\n    k_dense_bias_name = 'sequential/dense/BiasAdd/ReadVariableOp'\n    quantized_converter = tf.lite.TFLiteConverter.from_saved_model(\n        saved_model_dir)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n\n    if explicitly_set_bias:\n      quantized_converter._experimental_full_integer_quantization_bias_type = bias_type\n\n    if is_int16_quantize:\n      quantized_converter.target_spec.supported_ops = [\n          lite.OpsSet\n          .EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\n      ]\n    else:\n      quantized_converter.target_spec.supported_ops = [\n          lite.OpsSet.TFLITE_BUILTINS_INT8\n      ]\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [np.random.randn(1, 1024).astype(np.float32)]\n\n    quantized_converter.representative_dataset = calibration_gen\n\n    if not is_valid_bias_type:\n      with self.assertRaisesRegex(ValueError, 'Expected bias type to be'):\n        quantized_converter.convert()\n      return\n\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    dense_bias = next((d for d in interpreter.get_tensor_details()\n                       if d['name'].startswith(k_dense_bias_name)))\n    self.assertEqual(bias_type, dense_bias['dtype'])\n\n  @parameterized.named_parameters(\n      ('_Int8PerChannelMlirDynamicRangeQuant', True, False, False),\n      ('_Int8PerChannelTocoDynamicRangeQuant', False, False, False),\n      ('_Int8PerTensorMlirDynamicRangeQuant', True, True, False),\n      ('_Int8PerTensorTocoDynamicRangeQuant', False, True, False),\n      ('_Float16DynamicRangeQuant', True, False, True))\n  @test_util.run_v2_only\n  def testMlirDynamicRangeQuantization(self, enable_new_dynamic_range_quantizer,\n                                       disable_per_channel,\n                                       enable_float16_quant):\n    num_filters = 1024\n    conv_name = 'sequential/conv2d/Conv2D'\n    model = tf.keras.models.Sequential(\n        [tf.keras.layers.Conv2D(num_filters, (3, 3), activation='relu')])\n    model.build(input_shape=(1, 32, 32, 3))\n    saved_model_dir = self.create_tempdir()\n    save(model, saved_model_dir.full_path)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(\n        saved_model_dir.full_path)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.experimental_new_dynamic_range_quantizer = (\n        enable_new_dynamic_range_quantizer)\n    converter._experimental_disable_per_channel = disable_per_channel\n    if enable_float16_quant:\n      converter.target_spec.supported_types = [tf.float16]\n    quantized_tflite_model = converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    quantized_weight = None\n    quantized_weight_with_one_postfix = None\n    quantized_weight_without_one_postfix = None\n    for d in interpreter.get_tensor_details():\n      if d['name'] == conv_name + '1':\n        quantized_weight = d\n        quantized_weight_with_one_postfix = d\n        break\n    for d in interpreter.get_tensor_details():\n      if d['name'].startswith(conv_name):\n        if quantized_weight is None:\n          quantized_weight = d\n        quantized_weight_without_one_postfix = d\n        break\n\n    self.assertIsNotNone(quantized_weight)\n    quant_params = quantized_weight['quantization_parameters']\n\n    if enable_float16_quant:\n      expected_num_params = 0\n    else:\n      expected_num_params = 1 if disable_per_channel else num_filters\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n    if enable_float16_quant:\n      self.assertTrue(\n          (quantized_weight_with_one_postfix is not None and\n           np.float16 == quantized_weight_with_one_postfix['dtype']) or\n          (quantized_weight_without_one_postfix is not None and\n           np.float16 == quantized_weight_without_one_postfix['dtype']))\n    else:\n      self.assertEqual(np.int8, quantized_weight['dtype'])\n\n\nclass FromKerasModelTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testSequentialModel(self):\n    \"\"\"Test a simple sequential tf.Keras model.\"\"\"\n    input_data = tf.constant(1., shape=[1, 1])\n\n    # Create a simple Keras model.\n    x = np.array([[1.], [2.]])\n    y = np.array([[2.], [4.]])\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(units=1, input_shape=[1])\n    ])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(x, y, epochs=1)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.modelType,\n                     metadata_fb.ModelType.KERAS_MODEL)\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertEqual(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testSequentialMultiInputOutputModel(self):\n    \"\"\"Test a tf.Keras model with multiple inputs and outputs.\"\"\"\n    left_input_data = tf.constant(1., shape=[1, 3])\n    right_input_data = tf.constant(1., shape=[1, 3])\n\n    # Create a simple Keras model.\n    input_a_np = np.random.random((10, 3))\n    input_b_np = np.random.random((10, 3))\n    output_c_np = np.random.random((10, 3))\n    output_d_np = np.random.random((10, 2))\n\n    input_a = tf.keras.layers.Input(shape=(3,), name='input_a')\n    input_b = tf.keras.layers.Input(shape=(3,), name='input_b')\n\n    dense = tf.keras.layers.Dense(8, name='dense_1')\n    interm_a = dense(input_a)\n    interm_b = dense(input_b)\n    merged = tf.keras.layers.concatenate([interm_a, interm_b], name='merge')\n\n    output_c = tf.keras.layers.Dense(\n        3, activation='softmax', name='dense_2')(\n            merged)\n    output_d = tf.keras.layers.Dense(\n        2, activation='softmax', name='dense_3')(\n            merged)\n\n    model = tf.keras.models.Model(\n        inputs=[input_a, input_b], outputs=[output_c, output_d])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit([input_a_np, input_b_np], [output_c_np, output_d_np], epochs=1)\n\n    # Convert model and ensure model is not None.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    input_data = [left_input_data, right_input_data]\n    expected_value = model.predict(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, input_data)\n    for tf_result, tflite_result in zip(expected_value, actual_value):\n      self.assertAllClose(tf_result, tflite_result, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testGraphDebugInfo(self):\n    \"\"\"Test a tf.Keras model has debug info captured.\"\"\"\n    # Create a simple Keras model.\n    x = [-1, 0, 1, 2, 3, 4]\n    y = [-3, -1, 1, 3, 5, 7]\n    model = tf.keras.models.Sequential(\n        [tf.keras.layers.Dense(units=1, input_shape=[1])])\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(x, y, epochs=1)\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter.convert()\n    self._assertValidDebugInfo(converter._debug_info)\n\n  @test_util.run_v2_only\n  def testKerasFallbackPath(self):\n    \"\"\"Test keras model which failed when exporting to the saved model.\"\"\"\n    input_data = tf.constant(\n        np.array(np.random.random_sample((20)), dtype=np.float32))\n\n    class Model(tf.keras.Model):\n\n      def __init__(self):\n        super(Model, self).__init__()\n        # A None name will cause a failure in exporting to a saved model.\n        self.shared_weights = self.add_weight(\n            name=None,\n            shape=(20, 1),\n            dtype=tf.float32,\n            initializer=tf.random_normal_initializer(\n                mean=0.0, stddev=300**(-0.5)))\n\n      def call(self, x):\n        return tf.add(self.shared_weights, x)\n\n    # Building the model.\n    model = Model()\n    model.compile(optimizer='sgd', loss='mean_squared_error')\n    model.fit(input_data, input_data, epochs=1)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n\n  @test_util.run_v2_only\n  def testSignatureDefs(self):\n    \"\"\"Test converting SignatureDef is correct and uses SignatureDef API.\"\"\"\n    keras_model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(\n            32,\n            kernel_size=3,\n            padding='same',\n            activation='relu',\n            input_shape=(32, 32, 3),\n            name='tensor'),\n        tf.keras.layers.Dense(10, name='output_tensor')\n    ])\n\n    converter = lite.TFLiteConverterV2.from_keras_model(keras_model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    input_data = tf.constant(\n        np.random.uniform(-1, 1, size=(1, 32, 32, 3)).astype(np.float32))\n    expected_value = keras_model(input_data)\n    interpreter = Interpreter(model_content=tflite_model)\n    signature_defs = interpreter.get_signature_list()\n    results = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default', {'tensor_input': input_data})\n    self.assertEqual(list(results.keys()), ['output_tensor'])\n    self.assertAllClose(expected_value.numpy(), results['output_tensor'])\n\n    # Verify the SignatureDef structure returned is as expected.\n    self.assertEqual(len(signature_defs), 1)\n    self.assertEqual(list(signature_defs.keys()), ['serving_default'])\n    self.assertEqual(len(signature_defs.values()), 1)\n    self.assertEqual(\n        list(signature_defs['serving_default'].keys()), ['inputs', 'outputs'])\n    self.assertCountEqual(signature_defs['serving_default']['inputs'],\n                          ['tensor_input'])\n    self.assertEqual(\n        list(signature_defs['serving_default']['outputs']), ['output_tensor'])\n\n  @parameterized.named_parameters(\n      ('_PerChannelMlirDynamicRangeQuant', True, False, False),\n      ('_PerChannelTocoDynamicRangeQuant', False, False, False),\n      ('_PerTensorMlirDynamicRangeQuant', True, True, False),\n      ('_PerTensorTocoDynamicRangeQuant', False, True, False),\n      ('_Float16DynamicRangeQuant', True, False, True))\n  @test_util.run_v2_only\n  def testMlirDynamicRangeQuantization(self, enable_new_dynamic_range_quantizer,\n                                       disable_per_channel,\n                                       enable_float16_quant):\n    num_filters = 1024\n    conv_name = 'sequential/conv2d/Conv2D'\n    model = tf.keras.models.Sequential(\n        [tf.keras.Input(shape=(32, 32, 3)),\n         tf.keras.layers.Conv2D(num_filters, (3, 3), activation='relu')])\n    model.build()\n\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.experimental_new_dynamic_range_quantizer = (\n        enable_new_dynamic_range_quantizer)\n    converter._experimental_disable_per_channel = disable_per_channel\n    if enable_float16_quant:\n      converter.target_spec.supported_types = [tf.float16]\n    quantized_tflite_model = converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    interpreter.allocate_tensors()\n    quantized_weight = None\n    quantized_weight_with_one_postfix = None\n    quantized_weight_without_one_postfix = None\n    for d in interpreter.get_tensor_details():\n      if d['name'] == conv_name + '1':\n        quantized_weight = d\n        quantized_weight_with_one_postfix = d\n        break\n    for d in interpreter.get_tensor_details():\n      if d['name'].startswith(conv_name):\n        if quantized_weight is None:\n          quantized_weight = d\n        quantized_weight_without_one_postfix = d\n        break\n\n    self.assertIsNotNone(quantized_weight)\n    quant_params = quantized_weight['quantization_parameters']\n\n    if enable_float16_quant:\n      expected_num_params = 0\n    else:\n      expected_num_params = 1 if disable_per_channel else num_filters\n    self.assertLen(quant_params['scales'], expected_num_params)\n    self.assertLen(quant_params['zero_points'], expected_num_params)\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertEqual(np.float32, output_details[0]['dtype'])\n    if enable_float16_quant:\n      self.assertTrue(\n          (quantized_weight_with_one_postfix is not None and\n           np.float16 == quantized_weight_with_one_postfix['dtype']) or\n          (quantized_weight_without_one_postfix is not None and\n           np.float16 == quantized_weight_without_one_postfix['dtype']))\n    else:\n      self.assertEqual(np.int8, quantized_weight['dtype'])\n\n  @parameterized.named_parameters([\n      ('{}BitWeightOnly={}LowBit={}'.format(num_bits, weight_only, low_bit),\n       num_bits, weight_only, low_bit) for num_bits, weight_only, low_bit\n      in itertools.product((2, 4, 6), (True, False), (True, False))])\n  @test_util.run_v2_only\n  def testQATLowBitKerasModel(self, num_bits, weight_only, low_bit):\n    bit_max = (1 << (num_bits - 1)) - 1\n    bit_min = -bit_max\n    tf_input_shape = (5, 5, 3)\n    tflite_input_shape = (1,) + tf_input_shape\n    model, input_name, output_name = (self._createV2QATLowBitKerasModel(\n        tf_input_shape, weight_only, num_bits, bit_min, bit_max))\n    input_data = np.linspace(\n        0, 6, np.prod(tflite_input_shape)).reshape(tflite_input_shape)\n    tf_result = model(input_data)\n\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    if low_bit:\n      converter._experimental_low_bit_qat = True\n    tflite_model = converter.convert()\n    result = self._evaluateTFLiteModelUsingSignatureDef(\n        tflite_model, 'serving_default',\n        {input_name: input_data.astype(np.float32)})[output_name]\n    self.assertAllClose(\n        [np.linalg.norm(result - tf_result.numpy().astype(np.float32))], [0.0])\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n    interpreter.allocate_tensors()\n    num_8bit_activations = 0\n    num_8bit_weights = 0\n    kernel_name = ('model/conv_wrapper/Conv2D;model/conv_wrapper/'\n                   'FakeQuantWithMinMaxVarsPerChannel')\n\n    for detail in interpreter.get_tensor_details():\n      if (detail['dtype'] == np.int8 and detail['name'] and\n          detail['name'] == kernel_name):\n        num_8bit_weights += 1\n        weights = interpreter.get_tensor(detail['index'])\n        if low_bit:\n          self.assertFalse((bit_min > weights).any() or\n                           (weights > bit_max).any())\n        else:\n          self.assertTrue((bit_min > weights).any() or\n                          (weights > bit_max).any())\n        self.assertIn('scales', detail['quantization_parameters'])\n        if low_bit and detail['quantization_parameters']['scales']:\n          self.assertAllClose(\n              detail['quantization_parameters']['scales'], [1.0])\n      elif detail['dtype'] == np.int8 and detail['name']:\n        self.assertFalse(weight_only)\n        self.assertIn('scales', detail['quantization_parameters'])\n        if detail['quantization_parameters']['scales']:\n          self.assertAllClose(\n              detail['quantization_parameters']['scales'], [6/255])\n        num_8bit_activations += 1\n\n    self.assertEqual(num_8bit_weights, 0 if weight_only and not low_bit else 1)\n    # 3 activations with full integer: conv_input, conv_output, reshape_output\n    self.assertEqual(num_8bit_activations, 0 if weight_only else 3)\n\n\nclass FromJaxModelTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testInvalidInputsModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def simple_model(input1, input2):\n      return jnp.sin(input1) + jnp.cos(input2)\n\n    input_tensor = jnp.zeros([10, 10])\n    # Invalid case: not specify serving_func\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        None, [{\n            'input1': input_tensor\n        }])\n    with self.assertRaisesRegex(ValueError, 'No serving func is specified.'):\n      converter.convert()\n\n    # Invalid case: not specify input\n    converter = lite.TFLiteConverterV2.experimental_from_jax([simple_model],\n                                                             None)\n    with self.assertRaisesRegex(ValueError, 'Input tensors are not specified.'):\n      converter.convert()\n\n    converter = lite.TFLiteConverterV2.experimental_from_jax([simple_model], [])\n    with self.assertRaisesRegex(ValueError, 'Input tensors are not specified.'):\n      converter.convert()\n\n    # Invalid case: not wrap input_tensor in a list.\n    converter = lite.TFLiteConverterV2.experimental_from_jax([simple_model],\n                                                             input_tensor)\n    with self.assertRaisesRegex(\n        ValueError,\n        'The truth value of an array with more than one element is ambiguous.'):\n      converter.convert()\n\n    # Invalid case: only partial inputs are provided.\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [simple_model], [[('input1', input_tensor)]])\n    with self.assertRaisesRegex(\n        ValueError, 'Failed to convert the given Jax function to hlo.'):\n      converter.convert()\n\n    # Invalid case: serving functions length does not match input mapping.\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [simple_model, simple_model], [[\n            ('input1', input_tensor),\n            ('input2', input_tensor),\n        ]])\n    with self.assertRaisesRegex(\n        ValueError,\n        'Input tensor mapping len 1 does not match serving func len 2.'):\n      converter.convert()\n\n    # Invalid case: multiple serving function is provided.\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [simple_model, simple_model], [[\n            ('input1', input_tensor),\n            ('input2', input_tensor),\n        ], [\n            ('input1', input_tensor),\n            ('input2', input_tensor),\n        ]])\n    with self.assertRaisesRegex(\n        ValueError, 'Currently only support single serving function.'):\n      converter.convert()\n\n  @test_util.run_v2_only\n  def testSingleInputModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def single_input(input_tensor):\n      return jnp.sin(input_tensor)\n\n    # Convert model.\n    input_tensor = jnp.zeros([10, 10])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [single_input], [[('input_tensor', input_tensor)]])\n    tflite_model = converter.convert()\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.environment.modelType, metadata_fb.ModelType.JAX)\n\n    # Check values from converted_model\n    input_data = np.random.random_sample((10, 10))\n    tf_input_data = tf.constant(input_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [tf_input_data])[0]\n    expected_value = single_input(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testMultipleInputsModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def multiple_inputs(input1, input2):\n      return input1 + input2\n\n    # Convert model.\n    input1 = jnp.zeros([10, 10])\n    input2 = jnp.zeros([10, 1])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [multiple_inputs], [[('input1', input1), ('input2', input2)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input1_data = np.random.random_sample((10, 10))\n    tf_input1_data = tf.constant(input1_data, dtype=np.float32)\n    input2_data = np.random.random_sample((10, 1))\n    tf_input2_data = tf.constant(input2_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [tf_input1_data, tf_input2_data])[0]\n    expected_value = multiple_inputs(input1_data, input2_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testInputSignaturesModel(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def multiple_inputs(input1, input2):\n      return input1 + input2\n\n    # Convert model.\n    input1 = jnp.zeros([10, 10])\n    input2 = jnp.zeros([10, 1])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [multiple_inputs], [[('input1', input1), ('input2', input2)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input1_data = np.random.random_sample((10, 10))\n    tf_input1_data = tf.constant(input1_data, dtype=np.float32)\n    input2_data = np.random.random_sample((10, 1))\n    tf_input2_data = tf.constant(input2_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [tf_input1_data, tf_input2_data])[0]\n    expected_value = multiple_inputs(input1_data, input2_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testModelWithParams(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def model(inputs, weights):\n      return jnp.matmul(weights, inputs)\n\n    weights = np.random.random_sample((10, 10))\n    serving_func = functools.partial(model, weights=weights)\n\n    # Convert model\n    input_tensor = jnp.zeros([10, 10])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [serving_func], [[('inputs', input_tensor)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input_data = np.random.random_sample((10, 10))\n    tf_input_data = tf.constant(input_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [tf_input_data])[0]\n    expected_value = serving_func(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @test_util.run_v2_only\n  def testWhileLoop(self):\n    if DISABLE_JAX_TEST:\n      return\n\n    def condition(x):\n      return jnp.sum(x, keepdims=False) < 100\n\n    def body(x):\n      return jnp.add(x, 2.0)\n\n    def model(x):\n      result = jax.lax.while_loop(condition, body, x)\n      return result[0]\n\n    # Convert model.\n    input_tensor = jnp.zeros([3, 3])\n    converter = lite.TFLiteConverterV2.experimental_from_jax(\n        [model], [[('x', input_tensor)]])\n    tflite_model = converter.convert()\n\n    # Check values from converted_model\n    input_data = np.random.random_sample((3, 3))\n    tf_input_data = tf.constant(input_data, dtype=np.float32)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [tf_input_data])[0]\n    expected_value = model(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n\nclass ControlFlowTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testCond(self):\n    input_data = {\n        'x': tf.constant([1., 2.], shape=[1, 2]),\n        'b': tf.constant(True)\n    }\n\n    weights = tf.Variable([[0.1, 0.2], [0.3, 0.4]], dtype=tf.float32)\n\n    def true_fn(x):\n      return tf.matmul(x, weights)\n\n    def false_fn(x):\n      return tf.add(x, weights)\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, 2], dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.bool)\n    ])\n    def model(x, b):\n      return tf.cond(\n          b, true_fn=lambda: true_fn(x), false_fn=lambda: false_fn(x))\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(**input_data)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data['x'], input_data['b']])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testCondWithFullIntegerQuantization(self):\n    weights = tf.Variable([[0.1, 0.2], [0.3, 0.4]], dtype=tf.float32)\n\n    def true_fn(x):\n      return tf.matmul(x, weights)\n\n    def false_fn(x):\n      return tf.add(x, weights)\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, 2], dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.bool)\n    ])\n    def model(x, b):\n      return tf.cond(\n          b, true_fn=lambda: true_fn(x), false_fn=lambda: false_fn(x))\n\n    def calibration_gen():\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(1, 2)).astype(np.float32),\n            tf.constant(True)\n        ]\n      for _ in range(5):\n        yield [\n            np.random.uniform(-1, 1, size=(1, 2)).astype(np.float32),\n            tf.constant(False)\n        ]\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n  @test_util.run_v2_only\n  def testConverterErrorOnControlFlowV1Ops(self):\n    filename = resource_loader.get_path_to_datafile(\n        'testdata/control_flow_v1_saved_model')\n    converter = lite.TFLiteConverterV2.from_saved_model(filename)\n    with self.assertRaises(convert.ConverterError) as error:\n      converter.convert()\n    self.assertIn(\n        'Failed to functionalize Control Flow V1 ops. Consider using Control '\n        'Flow V2 ops instead. See https://www.tensorflow.org/api_docs/python/'\n        'tf/compat/v1/enable_control_flow_v2.', str(error.exception))\n\n  @test_util.run_v2_only\n  def testStaticRnn(self):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((3, 10)), dtype=np.float32))\n\n    cell = tf.keras.layers.LSTMCell(10)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[3, 10], dtype=tf.float32)])\n    def model(x):\n      seq = tf.split(x, 3, 0)\n      return rnn.static_rnn(cell, seq, dtype=tf.float32, sequence_length=[1])\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)[0]\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])\n    for expected, actual in zip(expected_value, actual_value):\n      self.assertAllClose(expected, actual)\n\n  @test_util.run_v2_only\n  def testWhileLoop(self):\n    input_data = tf.constant([1., 2., 3., 4.], shape=[2, 2])\n\n    weights = tf.Variable([[0.1, 0.2], [0.3, 0.4]], dtype=tf.float32)\n\n    def condition(x):\n      return tf.reduce_sum(x) < 100\n\n    def body(x):\n      return tf.add(x, weights)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[2, 2], dtype=tf.float32)])\n    def model(x):\n      return tf.while_loop(condition, body, [x])\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)[0]\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testDynamicRnn(self):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((3, 10, 10)), dtype=np.float32))\n\n    cell = tf.keras.layers.LSTMCell(10)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[3, 10, 10], dtype=tf.float32)])\n    def model(x):\n      rnn_layer = tf.keras.layers.RNN([cell], return_sequences=True)\n      return rnn_layer(x)\n\n    concrete_func = model.get_concrete_function()\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)\n    lite_outputs = self._evaluateTFLiteModel(tflite_model, [input_data])\n    self.assertLen(lite_outputs, 1)\n    actual_value = lite_outputs[0]\n    for expected, actual in zip(expected_value, actual_value):\n      self.assertAllClose(expected, actual)\n\n  @parameterized.named_parameters(\n      ('LSTMBatchSizeOne', tf.keras.layers.LSTM, True),\n      ('LSTM', tf.keras.layers.LSTM, False),\n      ('SimpleRNNBatchSizeOne', tf.keras.layers.SimpleRNN, True),\n      ('SimpleRNN', tf.keras.layers.SimpleRNN, False),\n      ('GRUBatchSizeOne', tf.keras.layers.GRU, True),\n      ('GRU', tf.keras.layers.GRU, False))\n  @test_util.run_v2_only\n  def testKerasRNN(self, rnn_layer, default_to_single_batch):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 10, 10)), dtype=np.float32))\n    rnn_obj = rnn_layer(units=10, input_shape=(10, 10))\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Input(shape=(10, 10), name='input'),\n        rnn_obj,\n    ])\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter._experimental_default_to_single_batch_in_tensor_list_ops = default_to_single_batch\n    if not default_to_single_batch:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @parameterized.named_parameters(('LSTM', tf.keras.layers.LSTM),\n                                  ('SimpleRNN', tf.keras.layers.SimpleRNN),\n                                  ('GRU', tf.keras.layers.GRU))\n  @test_util.run_v2_only\n  def testKerasRNNMultiBatches(self, rnn_layer):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((4, 10, 10)), dtype=np.float32))\n    # Specify a fixed batch size(4) for the test model.\n    x = tf.keras.layers.Input(batch_shape=(4, 10, 10))\n    y = rnn_layer(units=10, input_shape=(10, 10))(x)\n    model = tf.keras.Model(inputs=[x], outputs=[y])\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @parameterized.named_parameters(('ForceToUseBatchSizeOne', True),\n                                  ('DontForceToUseBatchSizeOne', False))\n  @test_util.run_v2_only\n  def testKerasBidirectionalRNNReturnSequence(self, default_to_single_batch):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 10, 10)), dtype=np.float32))\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input(shape=(10, 10), name='input'))\n    model.add(\n        tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(units=10, return_sequences=True),\n            input_shape=(10, 10)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(5))\n    model.add(tf.keras.layers.Activation('softmax'))\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter._experimental_default_to_single_batch_in_tensor_list_ops = default_to_single_batch\n    if not default_to_single_batch:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n  @parameterized.named_parameters(('ForceToUseBatchSizeOne', True),\n                                  ('DontForceToUseBatchSizeOne', False))\n  @test_util.run_v2_only\n  def testKerasBidirectionalRNN(self, default_to_single_batch):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 10, 10)), dtype=np.float32))\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input(shape=(10, 10), name='input'))\n    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=10)))\n    model.add(tf.keras.layers.Dense(5))\n    model.add(tf.keras.layers.Activation('softmax'))\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_keras_model(model)\n    converter._experimental_default_to_single_batch_in_tensor_list_ops = default_to_single_batch\n    if not default_to_single_batch:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n\n    # Check values from converted model.\n    expected_value = model.predict(input_data)\n    self.assertAllClose(expected_value, actual_value, atol=1e-05)\n\n\nclass GrapplerTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testConstantFolding(self):\n    # Constant folding handles the tf.broadcast_to operation which was not\n    # supported by the TFLite at the time this test was added.\n    input_data = tf.constant([1., 2., 3., 4., 5., 6., 7., 8., 9.], shape=[3, 3])\n\n    @tf.function\n    def func(x):\n      y_const = tf.constant([1., 2., 3.])\n      y_broadcast = tf.broadcast_to(y_const, [3, 3])\n      return tf.matmul(x, y_broadcast)\n\n    root = autotrackable.AutoTrackable()\n    root.f = func\n    concrete_func = root.f.get_concrete_function(input_data)\n\n    # Convert model.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               root)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = root.f(input_data)\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n    # Enable hybrid quantization, same result\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n    actual_value = self._evaluateTFLiteModel(tflite_model, [input_data])[0]\n    self.assertAllClose(expected_value, actual_value)\n\n\nclass UnknownShapes(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testMatMul(self):\n    input_data = tf.constant(\n        np.array(np.random.random_sample((10, 4)), dtype=np.float32))\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[None, 4], dtype=tf.float32)])\n    def model(in_tensor):\n      shape = tf.shape(in_tensor)\n      fill = tf.transpose(tf.fill(shape, 1.))\n      return tf.matmul(fill, in_tensor)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data], input_shapes=[([-1, 4], [10, 4])])[0]\n    self.assertAllClose(expected_value, actual_value, atol=1e-06)\n\n  def _getIntegerQuantizeModelWithUnknownShapes(self):\n    np.random.seed(0)\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[None, 33], dtype=tf.float32)])\n    def model(input_tensor):\n      \"\"\"Define a model with tf.MatMul and unknown shapes.\"\"\"\n      # We need the tensor to have more than 1024 elements for quantize_weights\n      # to kick in. Thus, the [33, 33] shape.\n      const_tensor = tf.constant(\n          np.random.uniform(low=-10., high=10., size=[33, 33]),\n          shape=[33, 33],\n          dtype=tf.float32,\n          name='inputB')\n\n      shape = tf.shape(input_tensor)\n      fill = tf.transpose(tf.fill(shape, 1.))\n      mult = tf.matmul(fill, input_tensor)\n      return tf.matmul(mult, const_tensor)\n\n    root = autotrackable.AutoTrackable()\n    root.f = model\n    concrete_func = root.f.get_concrete_function()\n\n    def calibration_gen():\n      for batch in range(5, 20, 5):\n        for _ in range(5):\n          yield [np.random.uniform(-1, 1, size=(batch, 33)).astype(np.float32)]\n\n    return root, concrete_func, calibration_gen\n\n  @test_util.run_v2_only\n  def testMatMulQuantize(self):\n    root, concrete_func, _ = self._getIntegerQuantizeModelWithUnknownShapes()\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    float_tflite_model = float_converter.convert()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_tflite_model = quantized_converter.convert()\n\n    # The default input and output types should be float.\n    quantized_interpreter = Interpreter(model_content=quantized_tflite_model)\n    quantized_interpreter.allocate_tensors()\n    input_details = quantized_interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertAllEqual([-1, 33], input_details[0]['shape_signature'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  @test_util.run_v2_only\n  def testMatMulCalibrateAndQuantize(self):\n    root, concrete_func, calibration_gen = (\n        self._getIntegerQuantizeModelWithUnknownShapes())\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    float_tflite_model = float_converter.convert()\n\n    quantized_converter = lite.TFLiteConverterV2.from_concrete_functions(\n        [concrete_func], root)\n    quantized_converter.optimizations = [lite.Optimize.DEFAULT]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_tflite_model = quantized_converter.convert()\n\n    # The default input and output types should be float.\n    quantized_interpreter = Interpreter(model_content=quantized_tflite_model)\n    quantized_interpreter.allocate_tensors()\n    input_details = quantized_interpreter.get_input_details()\n    self.assertLen(input_details, 1)\n    self.assertEqual(np.float32, input_details[0]['dtype'])\n    self.assertAllEqual([-1, 33], input_details[0]['shape_signature'])\n\n    # Ensure that the quantized weights tflite model is smaller.\n    self.assertLess(len(quantized_tflite_model), len(float_tflite_model))\n\n  def testBatchMatMul(self):\n    input_data_1 = tf.constant(\n        np.array(np.random.random_sample((1, 256, 256)), dtype=np.float32))\n    input_data_2 = tf.constant(\n        np.array(np.random.random_sample((1, 256, 256)), dtype=np.float32))\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, 256, 256], dtype=tf.float32),\n        tf.TensorSpec(shape=[None, 256, 256], dtype=tf.float32)\n    ])\n    def model(in_tensor_1, in_tensor_2):\n      return tf.matmul(in_tensor_1, in_tensor_2)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data_1, input_data_2)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data_1, input_data_2],\n        input_shapes=[([-1, 256, 256], [1, 256, 256])])[0]\n    self.assertAllClose(expected_value, actual_value, atol=4)\n\n  def testBatchMatMulInputInt8Int8OutputInt32(self):\n    input_data_1 = tf.constant(\n        np.array(\n            np.random.random_integers(-128, high=127, size=(1, 20, 30)),\n            dtype=np.int8))\n    input_data_2 = tf.constant(\n        np.array(\n            np.random.random_integers(-128, high=127, size=(1, 30, 10)),\n            dtype=np.int8))\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, 20, 30], dtype=tf.int8),\n        tf.TensorSpec(shape=[None, 30, 10], dtype=tf.int8)\n    ])\n    def model(in_tensor_1, in_tensor_2):\n      return tf.matmul(in_tensor_1, in_tensor_2, output_type=tf.int32)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data_1, input_data_2)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data_1, input_data_2],\n        input_shapes=[([-1, 20, 30], [1, 20, 30]), ([-1, 30, 10], [1, 30,\n                                                                   10])])[0]\n    self.assertAllEqual(expected_value, actual_value)\n\n  def testBatchMatMulHybrid(self):\n    # Test model that does batch matmul of:\n    # lhs input (1, 256, 128), rhs const (1, 128, 256).\n    # For dynamic range quantization situation, this will result in hybrid batch\n    # matmul, where lhs type is float32 and rhs type is int8.\n\n    # Intentionally set lhs, rhs sizes to satisfy following conditions:\n    # 1. rhs const num_elements >= 1024, since dynamic range quantization\n    # requires const tensor num_elements to be larger than\n    # min_elements_for_weights (which defaults to 1024).\n    # (https://github.com/tensorflow/tensorflow/blob/25e649ac3688655547da998eba2715cf70b3e5c9/tensorflow/compiler/mlir/lite/transforms/prepare_quantize_dynamic_range.cc#L262)\n    # 2. batch_size (256) > accum_dim_size (128) and\n    # num_units (256) > accum_dim_size (128), to test if the sizes are set\n    # correctly according to dimensions. See HybridAsymmetricBatchMatMulOpTest\n    # tests in\n    # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/batch_matmul_test.cc.\n    input_data = tf.constant(\n        np.array(np.random.random_sample((1, 256, 128)), dtype=np.float32))\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[None, 256, 128], dtype=tf.float32)\n    ])\n    def model(in_tensor):\n      rhs = tf.constant(\n          np.array(np.random.random_sample((1, 128, 256)), dtype=np.float32))\n      return tf.matmul(in_tensor, rhs)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n\n    # Check values from converted model.\n    expected_value = concrete_func(input_data)\n    actual_value = self._evaluateTFLiteModel(\n        tflite_model, [input_data],\n        input_shapes=[([-1, 256, 128], [1, 256, 128])])[0]\n    self.assertAllClose(expected_value, actual_value, atol=4)\n\n  def testSizeInvalid(self):\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=[1, None, 16, 3], dtype=tf.float32)\n    ])\n    def model(in_tensor):\n      return in_tensor + in_tensor\n\n    concrete_func = model.get_concrete_function()\n\n    # Test invalid shape. None after 1st dimension. Run with TOCO in order to\n    # invoke shape checking code.\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.experimental_new_converter = False\n    with self.assertRaises(ValueError) as error:\n      converter.convert()\n    self.assertEqual(\n        'None is only supported in the 1st dimension. Tensor '\n        '\\'in_tensor\\' has invalid shape \\'[1, None, 16, 3]\\'.',\n        str(error.exception))\n\n\nclass ResourceAndVariantTypes(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testVariants(self):\n\n    @tf.function(input_signature=[tf.TensorSpec(shape=[1], dtype=tf.float32)])\n    def model(v):\n      m = map_ops.empty_tensor_map()\n      k = tf.constant(1.0)\n      p = tf.add(k, v)\n      with ops.control_dependencies([m]):\n        m2 = map_ops.tensor_map_insert(m, p, v)\n        with ops.control_dependencies([m2]):\n          return map_ops.tensor_map_size(m2)\n\n    concrete_func = model.get_concrete_function()\n\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(1, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(1, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(1, actual_value)\n\n  @test_util.run_v2_only\n  def testVariantsWithCond(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'variants_with_cond')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          m = map_ops.empty_tensor_map()\n\n          def body(i, m):\n            m = map_ops.tensor_map_insert(m, i, i)\n            return i + 1, m\n\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.int32, name='input')\n          _, result_m = tf.cond(in_tensor < 10, lambda: body(in_tensor, m),\n                                lambda: body(in_tensor + 1, m))\n          out_tensor = in_tensor + map_ops.tensor_map_size(result_m)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([0], dtype=np.int32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    expected_value = np.array([1], dtype=np.int32)\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(expected_value, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(expected_value, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(expected_value, actual_value)\n\n  @test_util.run_v2_only\n  def testVariantsWithWhile(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'variants_with_while')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          m = map_ops.empty_tensor_map()\n\n          def cond(i, m):\n            del m\n            return i < 10\n\n          def body(i, m):\n            m = map_ops.tensor_map_insert(m, i, i)\n            return i + 1, m\n\n          _, result_m = tf.while_loop(cond, body, [0, m])\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.int32, name='input')\n          out_tensor = in_tensor + map_ops.tensor_map_size(result_m)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([0], dtype=np.int32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n  @test_util.run_v2_only\n  def testResources(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'simple_resources')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          stack = tf.raw_ops.StackV2(max_size=10, elem_type=tf.float32)\n          w = tf.raw_ops.StackPushV2(handle=stack, elem=in_tensor)\n          with ops.control_dependencies([w]):\n            a = in_tensor + in_tensor\n            with ops.control_dependencies([a]):\n              out_tensor = a + tf.raw_ops.StackPopV2(\n                  handle=stack, elem_type=tf.float32)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(3.0, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(3.0, actual_value)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(3.0, actual_value)\n\n  @test_util.run_v2_only\n  def testResourcesWithCond(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(), 'resources_with_cond')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          def body(i, arr):\n            n = tf.raw_ops.StackPushV2(\n                handle=arr, elem=tf.cast(i, dtype=tf.float32))\n            return n, arr\n\n          arr = tf.raw_ops.StackV2(max_size=10, elem_type=tf.float32)\n          n, result_arr = tf.cond(in_tensor < 10, lambda: body(0, arr),\n                                  lambda: body(1, arr))\n\n          with ops.control_dependencies([result_arr, n]):\n            out_tensor = tf.raw_ops.StackPopV2(\n                handle=result_arr, elem_type=tf.float32)\n\n          inputs = {'x': in_tensor}\n          outputs = {'a': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(0.0, actual_value)\n\n  @test_util.run_v2_only\n  def testResourcesWithWhile(self):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(),\n                                     'resources_with_while')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          def cond(i, arr, m):\n            del arr\n            del m\n            return i < 10\n\n          def body(i, arr, m):\n            del m\n            n = tf.raw_ops.StackPushV2(\n                handle=arr, elem=tf.cast(i, dtype=tf.float32))\n            return i + 1, arr, n\n\n          arr = tf.raw_ops.StackV2(max_size=10, elem_type=tf.float32)\n          _, result_arr, n = tf.while_loop(cond, body, [0, arr, 0.0])\n\n          with ops.control_dependencies([result_arr, n]):\n            out_tensor = tf.raw_ops.StackPopV2(\n                handle=result_arr, elem_type=tf.float32)\n\n          inputs = {'x': in_tensor}\n          outputs = {'a': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(9.0, actual_value)\n\n  @parameterized.named_parameters(('EnableLoweringTensorListOps', True),\n                                  ('DisableLoweringTensorListOps', False))\n  @test_util.run_v2_only\n  def testTensorListWithStaticSize(self, lower_tensor_list_ops):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(),\n                                     'simple_mutable_variable')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          ta = tf.TensorArray(\n              tf.float32, size=3, dynamic_size=False, clear_after_read=False)\n          ta = ta.write(0, 10.0)\n          ta = ta.write(1, 20.0)\n          ta = ta.write(2, 30.0)\n\n          out_tensor = ta.read(0) + ta.read(2)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    if not lower_tensor_list_ops:\n      converter.target_spec.supported_ops = [\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n      ]\n    converter._experimental_lower_tensor_list_ops = lower_tensor_list_ops\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(40.0, actual_value)\n\n  @parameterized.named_parameters(('EnableLoweringTensorListOps', True),\n                                  ('DisableLoweringTensorListOps', False))\n  @test_util.run_v2_only\n  def testTensorListWithDynamicSize(self, lower_tensor_list_ops):\n\n    def create_v1_saved_model():\n      saved_model_dir = os.path.join(self.get_temp_dir(),\n                                     'simple_mutable_variable')\n      with tf.Graph().as_default():\n        with tf.compat.v1.Session() as sess:\n          in_tensor = tf.compat.v1.placeholder(\n              shape=[1], dtype=tf.float32, name='input')\n\n          ta = tf.TensorArray(\n              tf.float32, size=0, dynamic_size=True, clear_after_read=False)\n          ta = ta.write(0, 10.0)\n          ta = ta.write(1, 20.0)\n          ta = ta.write(2, 30.0)\n\n          out_tensor = ta.read(0) + ta.read(2)\n\n          inputs = {'x': in_tensor}\n          outputs = {'z': out_tensor}\n          saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n      return saved_model_dir\n\n    saved_model_dir = create_v1_saved_model()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    if lower_tensor_list_ops:\n      with self.assertRaises(convert.ConverterError) as error:\n        converter.convert()\n      self.assertIn(\n          'Lowering tensor list ops is failed. Please consider using Select '\n          'TF ops and disabling `_experimental_lower_tensor_list_ops` flag in '\n          'the TFLite converter object.', str(error.exception))\n\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    input_data = np.array([1.0], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(40.0, actual_value)\n\n\nclass CalibrateAndQuantizeWithCustomOpTest(lite_v2_test_util.ModelTest):\n\n  def _createGraphWithCustomOp(self):\n    # Create a graph that has one double op.\n    np.random.seed(0)\n\n    saved_model_dir = os.path.join(self.get_temp_dir(), 'double_model')\n    with ops.Graph().as_default():\n      with tf.compat.v1.Session() as sess:\n        in_tensor = tf.compat.v1.placeholder(\n            shape=[1, 4], dtype=dtypes.float32, name='input')\n        out_tensor = double_op.double(in_tensor)\n        inputs = {'x': in_tensor}\n        outputs = {'z': out_tensor}\n        saved_model.simple_save(sess, saved_model_dir, inputs, outputs)\n\n    def calibration_gen():\n      for _ in range(100):\n        yield [np.random.uniform(-1, 1, size=(1, 4)).astype(np.float32)]\n\n    return (saved_model_dir, calibration_gen)\n\n  def testCustomOpRegistererByName(self):\n    \"\"\"Test a calibration with custom op registered by name.\"\"\"\n    saved_model_dir, calibration_gen = self._createGraphWithCustomOp()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.allow_custom_ops = True\n    converter.target_spec._experimental_custom_op_registerers = [\n        'TF_TestRegisterer'\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n    self.assertGreater(test_registerer.get_num_test_registerer_calls(), 0)\n    self.assertIn('Double', tflite_test_util.get_ops_list(tflite_model))\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(metadata.options.allowCustomOps, True)\n\n    # Check the model works with custom ops.\n    interpreter = InterpreterWithCustomOps(\n        model_content=tflite_model, custom_op_registerers=['TF_TestRegisterer'])\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    test_input = np.array([[0.0, 0.1, 0.2, 0.3]], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], test_input)\n    interpreter.invoke()\n\n    output_details = interpreter.get_output_details()\n    expected_output = np.array([[0.0, 0.2, 0.4, 0.6]], dtype=np.float32)\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(expected_output[0], output_data[0], err=1e-2)\n\n  def testCustomOpRegistererByFunc(self):\n    \"\"\"Test a calibration with custom op registered by function.\"\"\"\n    saved_model_dir, calibration_gen = self._createGraphWithCustomOp()\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.allow_custom_ops = True\n    converter.target_spec._experimental_custom_op_registerers = [\n        test_registerer.TF_TestRegisterer\n    ]\n    tflite_model = converter.convert()\n    self.assertTrue(tflite_model)\n    self.assertGreater(test_registerer.get_num_test_registerer_calls(), 0)\n    self.assertIn('Double', tflite_test_util.get_ops_list(tflite_model))\n\n    # Check the model works with custom ops.\n    interpreter = InterpreterWithCustomOps(\n        model_content=tflite_model,\n        custom_op_registerers=[test_registerer.TF_TestRegisterer])\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    test_input = np.array([[0.0, 0.1, 0.2, 0.3]], dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], test_input)\n    interpreter.invoke()\n\n    output_details = interpreter.get_output_details()\n    expected_output = np.array([[0.0, 0.2, 0.4, 0.6]], dtype=np.float32)\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(expected_output[0], output_data[0], err=1e-2)\n\n  def testCustomOpRegistererFailure(self):\n    \"\"\"Test a calibration with wrong custom op registerer.\"\"\"\n    saved_model_dir, calibration_gen = self._createGraphWithCustomOp()\n\n    bogus_name = 'CompletelyBogusRegistererName'\n\n    converter = lite.TFLiteConverterV2.from_saved_model(saved_model_dir)\n    converter.optimizations = [lite.Optimize.DEFAULT]\n    converter.representative_dataset = calibration_gen\n    converter.allow_custom_ops = True\n    converter.target_spec._experimental_custom_op_registerers = [bogus_name]\n\n    with self.assertRaisesRegex(\n        ValueError, 'Looking up symbol \\'' + bogus_name + '\\' failed'):\n      converter.convert()\n\n\nclass IntermediatesTest(lite_v2_test_util.ModelTest):\n\n  def _run(self, experimental_preserve_all_tensors):\n\n    @tf.function\n    def f(x):\n      y = tf.add(x, x, name='y')\n      z = tf.add(y, y, name='z')\n      w = tf.add(z, z, name='w')\n      return w\n\n    # NOTE this is exactly representable as a float as are the intermeidates of\n    # f. So direct comparison is ok below.\n\n    input_data = np.array(2.0, np.float32)\n    concrete_func = f.get_concrete_function(input_data)\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               f)\n    tflite_model = converter.convert()\n    interpreter = Interpreter(\n        model_content=tflite_model,\n        experimental_preserve_all_tensors=experimental_preserve_all_tensors)\n    interpreter.allocate_tensors()\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'],\n                           input_data)\n    interpreter.invoke()\n    out = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\n    tensors = {}\n    for t in interpreter.get_tensor_details():\n      # With Tensorflow Lite default delegate applied to the model graph, the\n      # access to original tensors of a delegated op could cause a ValueError\n      # (i.e. 'Tensor data is null. Run allocate_tensors() first') to be thrown\n      # out because the tensor memory isn't allocated at all.\n      val = None\n      try:\n        val = interpreter.get_tensor(t['index'])\n      except ValueError:\n        pass\n      tensors.update({t['name']: val})\n    return (tensors, out)\n\n  def testPreserve(self):\n    tensors, result = self._run(experimental_preserve_all_tensors=True)\n    # All intermediates should be true and result be true.\n    self.assertAllClose(tensors['x'], 2.0)\n    self.assertAllClose(tensors['y'], 4.0)\n    self.assertAllClose(tensors['z'], 8.0)\n    self.assertAllClose(result, 16.0)\n\n  def testNoPreserve(self):\n    tensors, result = self._run(experimental_preserve_all_tensors=False)\n    # One of them should be wrong if preserve is not true, but result should be\n    # ok. Input should still be ok for repeated invocation.\n    self.assertAllClose(tensors['x'], 2.0)\n    self.assertTrue(tensors['y'] != 4.0 or tensors['z'] != 8.0)\n    self.assertAllClose(result, 16.0)\n\n\nclass DatasetOpsTest(lite_v2_test_util.ModelTest):\n\n  @test_util.run_v2_only\n  def testReduceDataset(self):\n\n    @tf.function\n    def model():\n      dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4])\n      output = dataset.reduce(np.int32(0), lambda x, y: x + y)\n      return output\n\n    concrete_func = model.get_concrete_function()\n    converter = lite.TFLiteConverterV2.from_concrete_functions([concrete_func],\n                                                               model)\n    converter.target_spec.supported_ops = [\n        tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\n    ]\n    tflite_model = converter.convert()\n    self.assertIsNotNone(tflite_model)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=tflite_model)\n    output_details = interpreter.get_output_details()\n\n    interpreter.allocate_tensors()\n\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertEqual(10, actual_value)\n\n\nclass SparsityTest(lite_v2_test_util.ModelTest):\n\n  def _getSparsificableModel(self, matrix_b_values):\n    np.random.seed(0)\n    root = autotrackable.AutoTrackable()\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=[16, 4], dtype=tf.float32)])\n    def func(inp):\n      matrix_b = tf.constant(matrix_b_values, dtype=tf.float32)\n      matrix_b = tf.reshape(matrix_b, [4, 8])\n      matmul = tf.matmul(inp, matrix_b, transpose_a=False, transpose_b=False)\n      output = tf.nn.relu(matmul, name='output')\n      return output\n\n    root.f = func\n    to_save = root.f.get_concrete_function()\n    return (root, to_save)\n\n  def testRandomSparsity(self):\n    matrix_b_values = [\n        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 1\n    ]\n    root, func = self._getSparsificableModel(matrix_b_values)\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_converter.optimizations = [lite.Optimize.EXPERIMENTAL_SPARSITY]\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(float_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.RANDOM_SPARSITY],\n                        metadata.options.modelOptimizationModes)\n\n  def testBlockSparsity(self):\n    matrix_b_values = [\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 0\n    ]\n    root, func = self._getSparsificableModel(matrix_b_values)\n    float_converter = lite.TFLiteConverterV2.from_concrete_functions([func],\n                                                                     root)\n    float_converter.optimizations = [lite.Optimize.EXPERIMENTAL_SPARSITY]\n    float_tflite_model = float_converter.convert()\n    self.assertIsNotNone(float_tflite_model)\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(float_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertAllEqual([metadata_fb.ModelOptimizationMode.BLOCK_SPARSITY],\n                        metadata.options.modelOptimizationModes)\n\n  def testQuantizedBlockSparsity(self):\n    weight_values = np.array([\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 0, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [3, 0, 7, 0, 0, 0, -6, -2, 0, 0, 0, 0, 0, -2, 0, 6],\n    ])\n\n    custom_init = tf.constant_initializer(weight_values.transpose())\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(\n            8, kernel_initializer=custom_init, input_shape=[16])\n    ])\n\n    def calibration_gen():\n      for _ in range(10):\n        yield [np.random.uniform(-1, 1, size=(1, 16)).astype(np.float32) * 16]\n\n    quantized_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    quantized_converter.optimizations = [\n        lite.Optimize.EXPERIMENTAL_SPARSITY, lite.Optimize.DEFAULT\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(\n        metadata.environment.tensorflowVersion.decode('utf-8'),\n        versions.__version__)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertAllEqual([\n        metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER,\n        metadata_fb.ModelOptimizationMode.BLOCK_SPARSITY,\n    ], metadata.options.modelOptimizationModes)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.allocate_tensors()\n    input_data = np.array(\n        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]],\n        dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(\n        np.array([0, 87, 0, 0, 0, 0, 0, 34], dtype=np.float32),\n        actual_value.flatten(),\n        err=1)\n\n  def testQuantizedButNotEnoughBlockSparsity(self):\n    # Sparsity level is 25%, which is not enough to apply the sparse conversion.\n    weight_values = np.array(\n        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [4, 4, -3, 4, 4, 1, -2, -2, 1, 3, 4, 1, 1, 1, -4, -5],\n         [1, 1, 5, -1, 3, -1, 1, -3, 4, -3, 2, -3, 3, -1, 3, -4],\n         [0, -3, -2, 5, 4, 2, 1, 4, -4, 4, 1, -2, 3, -2, -2, -1]])\n\n    custom_init = tf.constant_initializer(weight_values.transpose())\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(\n            4, kernel_initializer=custom_init, input_shape=[16])\n    ])\n\n    def calibration_gen():\n      for _ in range(10):\n        yield [np.random.uniform(-1, 1, size=(1, 16)).astype(np.float32) * 16]\n\n    quantized_converter = lite.TFLiteConverterV2.from_keras_model(model)\n    quantized_converter.optimizations = [\n        lite.Optimize.EXPERIMENTAL_SPARSITY, lite.Optimize.DEFAULT\n    ]\n    quantized_converter.representative_dataset = calibration_gen\n    quantized_tflite_model = quantized_converter.convert()\n    self.assertIsNotNone(quantized_tflite_model)\n\n    # Check the conversion metadata.\n    metadata = get_conversion_metadata(quantized_tflite_model)\n    self.assertIsNotNone(metadata)\n    self.assertEqual(\n        metadata.environment.tensorflowVersion.decode('utf-8'),\n        versions.__version__)\n    self.assertEqual(metadata.environment.apiVersion, 2)\n    self.assertAllEqual([\n        metadata_fb.ModelOptimizationMode.PTQ_FULL_INTEGER,\n    ], metadata.options.modelOptimizationModes)\n    self.assertNotIn(metadata_fb.ModelOptimizationMode.RANDOM_SPARSITY,\n                     metadata.options.modelOptimizationModes)\n    self.assertNotIn(metadata_fb.ModelOptimizationMode.BLOCK_SPARSITY,\n                     metadata.options.modelOptimizationModes)\n\n    # Check values from converted model.\n    interpreter = Interpreter(model_content=quantized_tflite_model)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.allocate_tensors()\n    input_data = np.array(\n        [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]],\n        dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    actual_value = interpreter.get_tensor(output_details[0]['index'])\n    self.assertArrayNear(\n        np.array([0, -3, 4, 35], dtype=np.float32),\n        actual_value.flatten(),\n        err=1)\n\nif __name__ == '__main__':\n  test.main()\n"], "filenames": ["tensorflow/compiler/mlir/lite/quantization/quantization_utils.cc", "tensorflow/compiler/mlir/lite/transforms/prepare_tf.cc", "tensorflow/lite/python/lite_v2_test.py"], "buggy_code_start_loc": [171, 1372, 2313], "buggy_code_end_loc": [345, 1401, 2313], "fixing_code_start_loc": [171, 1373, 2314], "fixing_code_end_loc": [348, 1406, 2352], "type": "NVD-CWE-noinfo", "message": "TensorFlow is an open source platform for machine learning. When converting transposed convolutions using per-channel weight quantization the converter segfaults and crashes the Python process. We have patched the issue in GitHub commit aa0b852a4588cea4d36b74feb05d93055540b450. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-36027", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T23:15:11.430", "lastModified": "2022-09-20T14:38:28.217", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. When converting transposed convolutions using per-channel weight quantization the converter segfaults and crashes the Python process. We have patched the issue in GitHub commit aa0b852a4588cea4d36b74feb05d93055540b450. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. Cuando son convertidas las convoluciones transpuestas usando la cuantificaci\u00f3n del peso por canal, el convertidor falla y es bloqueado el proceso de Python. Hemos parcheado el problema en el commit aa0b852a4588cea4d36b74feb05d93055540b450 de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C6622D95-1C86-45C5-AB55-E6EEEA0996DF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/aa0b852a4588cea4d36b74feb05d93055540b450", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/issues/53767", "source": "security-advisories@github.com", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-79h2-q768-fpxr", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/aa0b852a4588cea4d36b74feb05d93055540b450"}}