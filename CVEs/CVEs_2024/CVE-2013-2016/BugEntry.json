{"buggy_code": ["/*\n * Virtio Support\n *\n * Copyright IBM, Corp. 2007\n *\n * Authors:\n *  Anthony Liguori   <aliguori@us.ibm.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n#include <inttypes.h>\n\n#include \"trace.h\"\n#include \"qemu/error-report.h\"\n#include \"hw/virtio/virtio.h\"\n#include \"qemu/atomic.h\"\n#include \"hw/virtio/virtio-bus.h\"\n\n/* The alignment to use between consumer and producer parts of vring.\n * x86 pagesize again. */\n#define VIRTIO_PCI_VRING_ALIGN         4096\n\ntypedef struct VRingDesc\n{\n    uint64_t addr;\n    uint32_t len;\n    uint16_t flags;\n    uint16_t next;\n} VRingDesc;\n\ntypedef struct VRingAvail\n{\n    uint16_t flags;\n    uint16_t idx;\n    uint16_t ring[0];\n} VRingAvail;\n\ntypedef struct VRingUsedElem\n{\n    uint32_t id;\n    uint32_t len;\n} VRingUsedElem;\n\ntypedef struct VRingUsed\n{\n    uint16_t flags;\n    uint16_t idx;\n    VRingUsedElem ring[0];\n} VRingUsed;\n\ntypedef struct VRing\n{\n    unsigned int num;\n    hwaddr desc;\n    hwaddr avail;\n    hwaddr used;\n} VRing;\n\nstruct VirtQueue\n{\n    VRing vring;\n    hwaddr pa;\n    uint16_t last_avail_idx;\n    /* Last used index value we have signalled on */\n    uint16_t signalled_used;\n\n    /* Last used index value we have signalled on */\n    bool signalled_used_valid;\n\n    /* Notification enabled? */\n    bool notification;\n\n    uint16_t queue_index;\n\n    int inuse;\n\n    uint16_t vector;\n    void (*handle_output)(VirtIODevice *vdev, VirtQueue *vq);\n    VirtIODevice *vdev;\n    EventNotifier guest_notifier;\n    EventNotifier host_notifier;\n};\n\n/* virt queue functions */\nstatic void virtqueue_init(VirtQueue *vq)\n{\n    hwaddr pa = vq->pa;\n\n    vq->vring.desc = pa;\n    vq->vring.avail = pa + vq->vring.num * sizeof(VRingDesc);\n    vq->vring.used = vring_align(vq->vring.avail +\n                                 offsetof(VRingAvail, ring[vq->vring.num]),\n                                 VIRTIO_PCI_VRING_ALIGN);\n}\n\nstatic inline uint64_t vring_desc_addr(hwaddr desc_pa, int i)\n{\n    hwaddr pa;\n    pa = desc_pa + sizeof(VRingDesc) * i + offsetof(VRingDesc, addr);\n    return ldq_phys(pa);\n}\n\nstatic inline uint32_t vring_desc_len(hwaddr desc_pa, int i)\n{\n    hwaddr pa;\n    pa = desc_pa + sizeof(VRingDesc) * i + offsetof(VRingDesc, len);\n    return ldl_phys(pa);\n}\n\nstatic inline uint16_t vring_desc_flags(hwaddr desc_pa, int i)\n{\n    hwaddr pa;\n    pa = desc_pa + sizeof(VRingDesc) * i + offsetof(VRingDesc, flags);\n    return lduw_phys(pa);\n}\n\nstatic inline uint16_t vring_desc_next(hwaddr desc_pa, int i)\n{\n    hwaddr pa;\n    pa = desc_pa + sizeof(VRingDesc) * i + offsetof(VRingDesc, next);\n    return lduw_phys(pa);\n}\n\nstatic inline uint16_t vring_avail_flags(VirtQueue *vq)\n{\n    hwaddr pa;\n    pa = vq->vring.avail + offsetof(VRingAvail, flags);\n    return lduw_phys(pa);\n}\n\nstatic inline uint16_t vring_avail_idx(VirtQueue *vq)\n{\n    hwaddr pa;\n    pa = vq->vring.avail + offsetof(VRingAvail, idx);\n    return lduw_phys(pa);\n}\n\nstatic inline uint16_t vring_avail_ring(VirtQueue *vq, int i)\n{\n    hwaddr pa;\n    pa = vq->vring.avail + offsetof(VRingAvail, ring[i]);\n    return lduw_phys(pa);\n}\n\nstatic inline uint16_t vring_used_event(VirtQueue *vq)\n{\n    return vring_avail_ring(vq, vq->vring.num);\n}\n\nstatic inline void vring_used_ring_id(VirtQueue *vq, int i, uint32_t val)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, ring[i].id);\n    stl_phys(pa, val);\n}\n\nstatic inline void vring_used_ring_len(VirtQueue *vq, int i, uint32_t val)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, ring[i].len);\n    stl_phys(pa, val);\n}\n\nstatic uint16_t vring_used_idx(VirtQueue *vq)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, idx);\n    return lduw_phys(pa);\n}\n\nstatic inline void vring_used_idx_set(VirtQueue *vq, uint16_t val)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, idx);\n    stw_phys(pa, val);\n}\n\nstatic inline void vring_used_flags_set_bit(VirtQueue *vq, int mask)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, flags);\n    stw_phys(pa, lduw_phys(pa) | mask);\n}\n\nstatic inline void vring_used_flags_unset_bit(VirtQueue *vq, int mask)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, flags);\n    stw_phys(pa, lduw_phys(pa) & ~mask);\n}\n\nstatic inline void vring_avail_event(VirtQueue *vq, uint16_t val)\n{\n    hwaddr pa;\n    if (!vq->notification) {\n        return;\n    }\n    pa = vq->vring.used + offsetof(VRingUsed, ring[vq->vring.num]);\n    stw_phys(pa, val);\n}\n\nvoid virtio_queue_set_notification(VirtQueue *vq, int enable)\n{\n    vq->notification = enable;\n    if (vq->vdev->guest_features & (1 << VIRTIO_RING_F_EVENT_IDX)) {\n        vring_avail_event(vq, vring_avail_idx(vq));\n    } else if (enable) {\n        vring_used_flags_unset_bit(vq, VRING_USED_F_NO_NOTIFY);\n    } else {\n        vring_used_flags_set_bit(vq, VRING_USED_F_NO_NOTIFY);\n    }\n    if (enable) {\n        /* Expose avail event/used flags before caller checks the avail idx. */\n        smp_mb();\n    }\n}\n\nint virtio_queue_ready(VirtQueue *vq)\n{\n    return vq->vring.avail != 0;\n}\n\nint virtio_queue_empty(VirtQueue *vq)\n{\n    return vring_avail_idx(vq) == vq->last_avail_idx;\n}\n\nvoid virtqueue_fill(VirtQueue *vq, const VirtQueueElement *elem,\n                    unsigned int len, unsigned int idx)\n{\n    unsigned int offset;\n    int i;\n\n    trace_virtqueue_fill(vq, elem, len, idx);\n\n    offset = 0;\n    for (i = 0; i < elem->in_num; i++) {\n        size_t size = MIN(len - offset, elem->in_sg[i].iov_len);\n\n        cpu_physical_memory_unmap(elem->in_sg[i].iov_base,\n                                  elem->in_sg[i].iov_len,\n                                  1, size);\n\n        offset += size;\n    }\n\n    for (i = 0; i < elem->out_num; i++)\n        cpu_physical_memory_unmap(elem->out_sg[i].iov_base,\n                                  elem->out_sg[i].iov_len,\n                                  0, elem->out_sg[i].iov_len);\n\n    idx = (idx + vring_used_idx(vq)) % vq->vring.num;\n\n    /* Get a pointer to the next entry in the used ring. */\n    vring_used_ring_id(vq, idx, elem->index);\n    vring_used_ring_len(vq, idx, len);\n}\n\nvoid virtqueue_flush(VirtQueue *vq, unsigned int count)\n{\n    uint16_t old, new;\n    /* Make sure buffer is written before we update index. */\n    smp_wmb();\n    trace_virtqueue_flush(vq, count);\n    old = vring_used_idx(vq);\n    new = old + count;\n    vring_used_idx_set(vq, new);\n    vq->inuse -= count;\n    if (unlikely((int16_t)(new - vq->signalled_used) < (uint16_t)(new - old)))\n        vq->signalled_used_valid = false;\n}\n\nvoid virtqueue_push(VirtQueue *vq, const VirtQueueElement *elem,\n                    unsigned int len)\n{\n    virtqueue_fill(vq, elem, len, 0);\n    virtqueue_flush(vq, 1);\n}\n\nstatic int virtqueue_num_heads(VirtQueue *vq, unsigned int idx)\n{\n    uint16_t num_heads = vring_avail_idx(vq) - idx;\n\n    /* Check it isn't doing very strange things with descriptor numbers. */\n    if (num_heads > vq->vring.num) {\n        error_report(\"Guest moved used index from %u to %u\",\n                     idx, vring_avail_idx(vq));\n        exit(1);\n    }\n    /* On success, callers read a descriptor at vq->last_avail_idx.\n     * Make sure descriptor read does not bypass avail index read. */\n    if (num_heads) {\n        smp_rmb();\n    }\n\n    return num_heads;\n}\n\nstatic unsigned int virtqueue_get_head(VirtQueue *vq, unsigned int idx)\n{\n    unsigned int head;\n\n    /* Grab the next descriptor number they're advertising, and increment\n     * the index we've seen. */\n    head = vring_avail_ring(vq, idx % vq->vring.num);\n\n    /* If their number is silly, that's a fatal mistake. */\n    if (head >= vq->vring.num) {\n        error_report(\"Guest says index %u is available\", head);\n        exit(1);\n    }\n\n    return head;\n}\n\nstatic unsigned virtqueue_next_desc(hwaddr desc_pa,\n                                    unsigned int i, unsigned int max)\n{\n    unsigned int next;\n\n    /* If this descriptor says it doesn't chain, we're done. */\n    if (!(vring_desc_flags(desc_pa, i) & VRING_DESC_F_NEXT))\n        return max;\n\n    /* Check they're not leading us off end of descriptors. */\n    next = vring_desc_next(desc_pa, i);\n    /* Make sure compiler knows to grab that: we don't want it changing! */\n    smp_wmb();\n\n    if (next >= max) {\n        error_report(\"Desc next is %u\", next);\n        exit(1);\n    }\n\n    return next;\n}\n\nvoid virtqueue_get_avail_bytes(VirtQueue *vq, unsigned int *in_bytes,\n                               unsigned int *out_bytes,\n                               unsigned max_in_bytes, unsigned max_out_bytes)\n{\n    unsigned int idx;\n    unsigned int total_bufs, in_total, out_total;\n\n    idx = vq->last_avail_idx;\n\n    total_bufs = in_total = out_total = 0;\n    while (virtqueue_num_heads(vq, idx)) {\n        unsigned int max, num_bufs, indirect = 0;\n        hwaddr desc_pa;\n        int i;\n\n        max = vq->vring.num;\n        num_bufs = total_bufs;\n        i = virtqueue_get_head(vq, idx++);\n        desc_pa = vq->vring.desc;\n\n        if (vring_desc_flags(desc_pa, i) & VRING_DESC_F_INDIRECT) {\n            if (vring_desc_len(desc_pa, i) % sizeof(VRingDesc)) {\n                error_report(\"Invalid size for indirect buffer table\");\n                exit(1);\n            }\n\n            /* If we've got too many, that implies a descriptor loop. */\n            if (num_bufs >= max) {\n                error_report(\"Looped descriptor\");\n                exit(1);\n            }\n\n            /* loop over the indirect descriptor table */\n            indirect = 1;\n            max = vring_desc_len(desc_pa, i) / sizeof(VRingDesc);\n            num_bufs = i = 0;\n            desc_pa = vring_desc_addr(desc_pa, i);\n        }\n\n        do {\n            /* If we've got too many, that implies a descriptor loop. */\n            if (++num_bufs > max) {\n                error_report(\"Looped descriptor\");\n                exit(1);\n            }\n\n            if (vring_desc_flags(desc_pa, i) & VRING_DESC_F_WRITE) {\n                in_total += vring_desc_len(desc_pa, i);\n            } else {\n                out_total += vring_desc_len(desc_pa, i);\n            }\n            if (in_total >= max_in_bytes && out_total >= max_out_bytes) {\n                goto done;\n            }\n        } while ((i = virtqueue_next_desc(desc_pa, i, max)) != max);\n\n        if (!indirect)\n            total_bufs = num_bufs;\n        else\n            total_bufs++;\n    }\ndone:\n    if (in_bytes) {\n        *in_bytes = in_total;\n    }\n    if (out_bytes) {\n        *out_bytes = out_total;\n    }\n}\n\nint virtqueue_avail_bytes(VirtQueue *vq, unsigned int in_bytes,\n                          unsigned int out_bytes)\n{\n    unsigned int in_total, out_total;\n\n    virtqueue_get_avail_bytes(vq, &in_total, &out_total, in_bytes, out_bytes);\n    return in_bytes <= in_total && out_bytes <= out_total;\n}\n\nvoid virtqueue_map_sg(struct iovec *sg, hwaddr *addr,\n    size_t num_sg, int is_write)\n{\n    unsigned int i;\n    hwaddr len;\n\n    for (i = 0; i < num_sg; i++) {\n        len = sg[i].iov_len;\n        sg[i].iov_base = cpu_physical_memory_map(addr[i], &len, is_write);\n        if (sg[i].iov_base == NULL || len != sg[i].iov_len) {\n            error_report(\"virtio: trying to map MMIO memory\");\n            exit(1);\n        }\n    }\n}\n\nint virtqueue_pop(VirtQueue *vq, VirtQueueElement *elem)\n{\n    unsigned int i, head, max;\n    hwaddr desc_pa = vq->vring.desc;\n\n    if (!virtqueue_num_heads(vq, vq->last_avail_idx))\n        return 0;\n\n    /* When we start there are none of either input nor output. */\n    elem->out_num = elem->in_num = 0;\n\n    max = vq->vring.num;\n\n    i = head = virtqueue_get_head(vq, vq->last_avail_idx++);\n    if (vq->vdev->guest_features & (1 << VIRTIO_RING_F_EVENT_IDX)) {\n        vring_avail_event(vq, vring_avail_idx(vq));\n    }\n\n    if (vring_desc_flags(desc_pa, i) & VRING_DESC_F_INDIRECT) {\n        if (vring_desc_len(desc_pa, i) % sizeof(VRingDesc)) {\n            error_report(\"Invalid size for indirect buffer table\");\n            exit(1);\n        }\n\n        /* loop over the indirect descriptor table */\n        max = vring_desc_len(desc_pa, i) / sizeof(VRingDesc);\n        desc_pa = vring_desc_addr(desc_pa, i);\n        i = 0;\n    }\n\n    /* Collect all the descriptors */\n    do {\n        struct iovec *sg;\n\n        if (vring_desc_flags(desc_pa, i) & VRING_DESC_F_WRITE) {\n            if (elem->in_num >= ARRAY_SIZE(elem->in_sg)) {\n                error_report(\"Too many write descriptors in indirect table\");\n                exit(1);\n            }\n            elem->in_addr[elem->in_num] = vring_desc_addr(desc_pa, i);\n            sg = &elem->in_sg[elem->in_num++];\n        } else {\n            if (elem->out_num >= ARRAY_SIZE(elem->out_sg)) {\n                error_report(\"Too many read descriptors in indirect table\");\n                exit(1);\n            }\n            elem->out_addr[elem->out_num] = vring_desc_addr(desc_pa, i);\n            sg = &elem->out_sg[elem->out_num++];\n        }\n\n        sg->iov_len = vring_desc_len(desc_pa, i);\n\n        /* If we've got too many, that implies a descriptor loop. */\n        if ((elem->in_num + elem->out_num) > max) {\n            error_report(\"Looped descriptor\");\n            exit(1);\n        }\n    } while ((i = virtqueue_next_desc(desc_pa, i, max)) != max);\n\n    /* Now map what we have collected */\n    virtqueue_map_sg(elem->in_sg, elem->in_addr, elem->in_num, 1);\n    virtqueue_map_sg(elem->out_sg, elem->out_addr, elem->out_num, 0);\n\n    elem->index = head;\n\n    vq->inuse++;\n\n    trace_virtqueue_pop(vq, elem, elem->in_num, elem->out_num);\n    return elem->in_num + elem->out_num;\n}\n\n/* virtio device */\nstatic void virtio_notify_vector(VirtIODevice *vdev, uint16_t vector)\n{\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n\n    if (k->notify) {\n        k->notify(qbus->parent, vector);\n    }\n}\n\nvoid virtio_update_irq(VirtIODevice *vdev)\n{\n    virtio_notify_vector(vdev, VIRTIO_NO_VECTOR);\n}\n\nvoid virtio_set_status(VirtIODevice *vdev, uint8_t val)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    trace_virtio_set_status(vdev, val);\n\n    if (k->set_status) {\n        k->set_status(vdev, val);\n    }\n    vdev->status = val;\n}\n\nvoid virtio_reset(void *opaque)\n{\n    VirtIODevice *vdev = opaque;\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    int i;\n\n    virtio_set_status(vdev, 0);\n\n    if (k->reset) {\n        k->reset(vdev);\n    }\n\n    vdev->guest_features = 0;\n    vdev->queue_sel = 0;\n    vdev->status = 0;\n    vdev->isr = 0;\n    vdev->config_vector = VIRTIO_NO_VECTOR;\n    virtio_notify_vector(vdev, vdev->config_vector);\n\n    for(i = 0; i < VIRTIO_PCI_QUEUE_MAX; i++) {\n        vdev->vq[i].vring.desc = 0;\n        vdev->vq[i].vring.avail = 0;\n        vdev->vq[i].vring.used = 0;\n        vdev->vq[i].last_avail_idx = 0;\n        vdev->vq[i].pa = 0;\n        vdev->vq[i].vector = VIRTIO_NO_VECTOR;\n        vdev->vq[i].signalled_used = 0;\n        vdev->vq[i].signalled_used_valid = false;\n        vdev->vq[i].notification = true;\n    }\n}\n\nuint32_t virtio_config_readb(VirtIODevice *vdev, uint32_t addr)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint8_t val;\n\n    k->get_config(vdev, vdev->config);\n\n    if (addr > (vdev->config_len - sizeof(val)))\n        return (uint32_t)-1;\n\n    val = ldub_p(vdev->config + addr);\n    return val;\n}\n\nuint32_t virtio_config_readw(VirtIODevice *vdev, uint32_t addr)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint16_t val;\n\n    k->get_config(vdev, vdev->config);\n\n    if (addr > (vdev->config_len - sizeof(val)))\n        return (uint32_t)-1;\n\n    val = lduw_p(vdev->config + addr);\n    return val;\n}\n\nuint32_t virtio_config_readl(VirtIODevice *vdev, uint32_t addr)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint32_t val;\n\n    k->get_config(vdev, vdev->config);\n\n    if (addr > (vdev->config_len - sizeof(val)))\n        return (uint32_t)-1;\n\n    val = ldl_p(vdev->config + addr);\n    return val;\n}\n\nvoid virtio_config_writeb(VirtIODevice *vdev, uint32_t addr, uint32_t data)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint8_t val = data;\n\n    if (addr > (vdev->config_len - sizeof(val)))\n        return;\n\n    stb_p(vdev->config + addr, val);\n\n    if (k->set_config) {\n        k->set_config(vdev, vdev->config);\n    }\n}\n\nvoid virtio_config_writew(VirtIODevice *vdev, uint32_t addr, uint32_t data)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint16_t val = data;\n\n    if (addr > (vdev->config_len - sizeof(val)))\n        return;\n\n    stw_p(vdev->config + addr, val);\n\n    if (k->set_config) {\n        k->set_config(vdev, vdev->config);\n    }\n}\n\nvoid virtio_config_writel(VirtIODevice *vdev, uint32_t addr, uint32_t data)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint32_t val = data;\n\n    if (addr > (vdev->config_len - sizeof(val)))\n        return;\n\n    stl_p(vdev->config + addr, val);\n\n    if (k->set_config) {\n        k->set_config(vdev, vdev->config);\n    }\n}\n\nvoid virtio_queue_set_addr(VirtIODevice *vdev, int n, hwaddr addr)\n{\n    vdev->vq[n].pa = addr;\n    virtqueue_init(&vdev->vq[n]);\n}\n\nhwaddr virtio_queue_get_addr(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].pa;\n}\n\nint virtio_queue_get_num(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.num;\n}\n\nint virtio_queue_get_id(VirtQueue *vq)\n{\n    VirtIODevice *vdev = vq->vdev;\n    assert(vq >= &vdev->vq[0] && vq < &vdev->vq[VIRTIO_PCI_QUEUE_MAX]);\n    return vq - &vdev->vq[0];\n}\n\nvoid virtio_queue_notify_vq(VirtQueue *vq)\n{\n    if (vq->vring.desc) {\n        VirtIODevice *vdev = vq->vdev;\n        trace_virtio_queue_notify(vdev, vq - vdev->vq, vq);\n        vq->handle_output(vdev, vq);\n    }\n}\n\nvoid virtio_queue_notify(VirtIODevice *vdev, int n)\n{\n    virtio_queue_notify_vq(&vdev->vq[n]);\n}\n\nuint16_t virtio_queue_vector(VirtIODevice *vdev, int n)\n{\n    return n < VIRTIO_PCI_QUEUE_MAX ? vdev->vq[n].vector :\n        VIRTIO_NO_VECTOR;\n}\n\nvoid virtio_queue_set_vector(VirtIODevice *vdev, int n, uint16_t vector)\n{\n    if (n < VIRTIO_PCI_QUEUE_MAX)\n        vdev->vq[n].vector = vector;\n}\n\nVirtQueue *virtio_add_queue(VirtIODevice *vdev, int queue_size,\n                            void (*handle_output)(VirtIODevice *, VirtQueue *))\n{\n    int i;\n\n    for (i = 0; i < VIRTIO_PCI_QUEUE_MAX; i++) {\n        if (vdev->vq[i].vring.num == 0)\n            break;\n    }\n\n    if (i == VIRTIO_PCI_QUEUE_MAX || queue_size > VIRTQUEUE_MAX_SIZE)\n        abort();\n\n    vdev->vq[i].vring.num = queue_size;\n    vdev->vq[i].handle_output = handle_output;\n\n    return &vdev->vq[i];\n}\n\nvoid virtio_del_queue(VirtIODevice *vdev, int n)\n{\n    if (n < 0 || n >= VIRTIO_PCI_QUEUE_MAX) {\n        abort();\n    }\n\n    vdev->vq[n].vring.num = 0;\n}\n\nvoid virtio_irq(VirtQueue *vq)\n{\n    trace_virtio_irq(vq);\n    vq->vdev->isr |= 0x01;\n    virtio_notify_vector(vq->vdev, vq->vector);\n}\n\n/* Assuming a given event_idx value from the other size, if\n * we have just incremented index from old to new_idx,\n * should we trigger an event? */\nstatic inline int vring_need_event(uint16_t event, uint16_t new, uint16_t old)\n{\n\t/* Note: Xen has similar logic for notification hold-off\n\t * in include/xen/interface/io/ring.h with req_event and req_prod\n\t * corresponding to event_idx + 1 and new respectively.\n\t * Note also that req_event and req_prod in Xen start at 1,\n\t * event indexes in virtio start at 0. */\n\treturn (uint16_t)(new - event - 1) < (uint16_t)(new - old);\n}\n\nstatic bool vring_notify(VirtIODevice *vdev, VirtQueue *vq)\n{\n    uint16_t old, new;\n    bool v;\n    /* We need to expose used array entries before checking used event. */\n    smp_mb();\n    /* Always notify when queue is empty (when feature acknowledge) */\n    if (((vdev->guest_features & (1 << VIRTIO_F_NOTIFY_ON_EMPTY)) &&\n         !vq->inuse && vring_avail_idx(vq) == vq->last_avail_idx)) {\n        return true;\n    }\n\n    if (!(vdev->guest_features & (1 << VIRTIO_RING_F_EVENT_IDX))) {\n        return !(vring_avail_flags(vq) & VRING_AVAIL_F_NO_INTERRUPT);\n    }\n\n    v = vq->signalled_used_valid;\n    vq->signalled_used_valid = true;\n    old = vq->signalled_used;\n    new = vq->signalled_used = vring_used_idx(vq);\n    return !v || vring_need_event(vring_used_event(vq), new, old);\n}\n\nvoid virtio_notify(VirtIODevice *vdev, VirtQueue *vq)\n{\n    if (!vring_notify(vdev, vq)) {\n        return;\n    }\n\n    trace_virtio_notify(vdev, vq);\n    vdev->isr |= 0x01;\n    virtio_notify_vector(vdev, vq->vector);\n}\n\nvoid virtio_notify_config(VirtIODevice *vdev)\n{\n    if (!(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK))\n        return;\n\n    vdev->isr |= 0x03;\n    virtio_notify_vector(vdev, vdev->config_vector);\n}\n\nvoid virtio_save(VirtIODevice *vdev, QEMUFile *f)\n{\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n    int i;\n\n    if (k->save_config) {\n        k->save_config(qbus->parent, f);\n    }\n\n    qemu_put_8s(f, &vdev->status);\n    qemu_put_8s(f, &vdev->isr);\n    qemu_put_be16s(f, &vdev->queue_sel);\n    qemu_put_be32s(f, &vdev->guest_features);\n    qemu_put_be32(f, vdev->config_len);\n    qemu_put_buffer(f, vdev->config, vdev->config_len);\n\n    for (i = 0; i < VIRTIO_PCI_QUEUE_MAX; i++) {\n        if (vdev->vq[i].vring.num == 0)\n            break;\n    }\n\n    qemu_put_be32(f, i);\n\n    for (i = 0; i < VIRTIO_PCI_QUEUE_MAX; i++) {\n        if (vdev->vq[i].vring.num == 0)\n            break;\n\n        qemu_put_be32(f, vdev->vq[i].vring.num);\n        qemu_put_be64(f, vdev->vq[i].pa);\n        qemu_put_be16s(f, &vdev->vq[i].last_avail_idx);\n        if (k->save_queue) {\n            k->save_queue(qbus->parent, i, f);\n        }\n    }\n}\n\nint virtio_set_features(VirtIODevice *vdev, uint32_t val)\n{\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *vbusk = VIRTIO_BUS_GET_CLASS(qbus);\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint32_t supported_features = vbusk->get_features(qbus->parent);\n    bool bad = (val & ~supported_features) != 0;\n\n    val &= supported_features;\n    if (k->set_features) {\n        k->set_features(vdev, val);\n    }\n    vdev->guest_features = val;\n    return bad ? -1 : 0;\n}\n\nint virtio_load(VirtIODevice *vdev, QEMUFile *f)\n{\n    int num, i, ret;\n    uint32_t features;\n    uint32_t supported_features;\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n\n    if (k->load_config) {\n        ret = k->load_config(qbus->parent, f);\n        if (ret)\n            return ret;\n    }\n\n    qemu_get_8s(f, &vdev->status);\n    qemu_get_8s(f, &vdev->isr);\n    qemu_get_be16s(f, &vdev->queue_sel);\n    qemu_get_be32s(f, &features);\n\n    if (virtio_set_features(vdev, features) < 0) {\n        supported_features = k->get_features(qbus->parent);\n        error_report(\"Features 0x%x unsupported. Allowed features: 0x%x\",\n                     features, supported_features);\n        return -1;\n    }\n    vdev->config_len = qemu_get_be32(f);\n    qemu_get_buffer(f, vdev->config, vdev->config_len);\n\n    num = qemu_get_be32(f);\n\n    for (i = 0; i < num; i++) {\n        vdev->vq[i].vring.num = qemu_get_be32(f);\n        vdev->vq[i].pa = qemu_get_be64(f);\n        qemu_get_be16s(f, &vdev->vq[i].last_avail_idx);\n        vdev->vq[i].signalled_used_valid = false;\n        vdev->vq[i].notification = true;\n\n        if (vdev->vq[i].pa) {\n            uint16_t nheads;\n            virtqueue_init(&vdev->vq[i]);\n            nheads = vring_avail_idx(&vdev->vq[i]) - vdev->vq[i].last_avail_idx;\n            /* Check it isn't doing very strange things with descriptor numbers. */\n            if (nheads > vdev->vq[i].vring.num) {\n                error_report(\"VQ %d size 0x%x Guest index 0x%x \"\n                             \"inconsistent with Host index 0x%x: delta 0x%x\",\n                             i, vdev->vq[i].vring.num,\n                             vring_avail_idx(&vdev->vq[i]),\n                             vdev->vq[i].last_avail_idx, nheads);\n                return -1;\n            }\n        } else if (vdev->vq[i].last_avail_idx) {\n            error_report(\"VQ %d address 0x0 \"\n                         \"inconsistent with Host index 0x%x\",\n                         i, vdev->vq[i].last_avail_idx);\n                return -1;\n\t}\n        if (k->load_queue) {\n            ret = k->load_queue(qbus->parent, i, f);\n            if (ret)\n                return ret;\n        }\n    }\n\n    virtio_notify_vector(vdev, VIRTIO_NO_VECTOR);\n    return 0;\n}\n\nvoid virtio_cleanup(VirtIODevice *vdev)\n{\n    qemu_del_vm_change_state_handler(vdev->vmstate);\n    g_free(vdev->config);\n    g_free(vdev->vq);\n}\n\nstatic void virtio_vmstate_change(void *opaque, int running, RunState state)\n{\n    VirtIODevice *vdev = opaque;\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n    bool backend_run = running && (vdev->status & VIRTIO_CONFIG_S_DRIVER_OK);\n    vdev->vm_running = running;\n\n    if (backend_run) {\n        virtio_set_status(vdev, vdev->status);\n    }\n\n    if (k->vmstate_change) {\n        k->vmstate_change(qbus->parent, backend_run);\n    }\n\n    if (!backend_run) {\n        virtio_set_status(vdev, vdev->status);\n    }\n}\n\nvoid virtio_init(VirtIODevice *vdev, const char *name,\n                 uint16_t device_id, size_t config_size)\n{\n    int i;\n    vdev->device_id = device_id;\n    vdev->status = 0;\n    vdev->isr = 0;\n    vdev->queue_sel = 0;\n    vdev->config_vector = VIRTIO_NO_VECTOR;\n    vdev->vq = g_malloc0(sizeof(VirtQueue) * VIRTIO_PCI_QUEUE_MAX);\n    vdev->vm_running = runstate_is_running();\n    for (i = 0; i < VIRTIO_PCI_QUEUE_MAX; i++) {\n        vdev->vq[i].vector = VIRTIO_NO_VECTOR;\n        vdev->vq[i].vdev = vdev;\n        vdev->vq[i].queue_index = i;\n    }\n\n    vdev->name = name;\n    vdev->config_len = config_size;\n    if (vdev->config_len) {\n        vdev->config = g_malloc0(config_size);\n    } else {\n        vdev->config = NULL;\n    }\n    vdev->vmstate = qemu_add_vm_change_state_handler(virtio_vmstate_change,\n                                                     vdev);\n}\n\nhwaddr virtio_queue_get_desc_addr(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.desc;\n}\n\nhwaddr virtio_queue_get_avail_addr(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.avail;\n}\n\nhwaddr virtio_queue_get_used_addr(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.used;\n}\n\nhwaddr virtio_queue_get_ring_addr(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.desc;\n}\n\nhwaddr virtio_queue_get_desc_size(VirtIODevice *vdev, int n)\n{\n    return sizeof(VRingDesc) * vdev->vq[n].vring.num;\n}\n\nhwaddr virtio_queue_get_avail_size(VirtIODevice *vdev, int n)\n{\n    return offsetof(VRingAvail, ring) +\n        sizeof(uint64_t) * vdev->vq[n].vring.num;\n}\n\nhwaddr virtio_queue_get_used_size(VirtIODevice *vdev, int n)\n{\n    return offsetof(VRingUsed, ring) +\n        sizeof(VRingUsedElem) * vdev->vq[n].vring.num;\n}\n\nhwaddr virtio_queue_get_ring_size(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.used - vdev->vq[n].vring.desc +\n\t    virtio_queue_get_used_size(vdev, n);\n}\n\nuint16_t virtio_queue_get_last_avail_idx(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].last_avail_idx;\n}\n\nvoid virtio_queue_set_last_avail_idx(VirtIODevice *vdev, int n, uint16_t idx)\n{\n    vdev->vq[n].last_avail_idx = idx;\n}\n\nVirtQueue *virtio_get_queue(VirtIODevice *vdev, int n)\n{\n    return vdev->vq + n;\n}\n\nuint16_t virtio_get_queue_index(VirtQueue *vq)\n{\n    return vq->queue_index;\n}\n\nstatic void virtio_queue_guest_notifier_read(EventNotifier *n)\n{\n    VirtQueue *vq = container_of(n, VirtQueue, guest_notifier);\n    if (event_notifier_test_and_clear(n)) {\n        virtio_irq(vq);\n    }\n}\n\nvoid virtio_queue_set_guest_notifier_fd_handler(VirtQueue *vq, bool assign,\n                                                bool with_irqfd)\n{\n    if (assign && !with_irqfd) {\n        event_notifier_set_handler(&vq->guest_notifier,\n                                   virtio_queue_guest_notifier_read);\n    } else {\n        event_notifier_set_handler(&vq->guest_notifier, NULL);\n    }\n    if (!assign) {\n        /* Test and clear notifier before closing it,\n         * in case poll callback didn't have time to run. */\n        virtio_queue_guest_notifier_read(&vq->guest_notifier);\n    }\n}\n\nEventNotifier *virtio_queue_get_guest_notifier(VirtQueue *vq)\n{\n    return &vq->guest_notifier;\n}\n\nstatic void virtio_queue_host_notifier_read(EventNotifier *n)\n{\n    VirtQueue *vq = container_of(n, VirtQueue, host_notifier);\n    if (event_notifier_test_and_clear(n)) {\n        virtio_queue_notify_vq(vq);\n    }\n}\n\nvoid virtio_queue_set_host_notifier_fd_handler(VirtQueue *vq, bool assign,\n                                               bool set_handler)\n{\n    if (assign && set_handler) {\n        event_notifier_set_handler(&vq->host_notifier,\n                                   virtio_queue_host_notifier_read);\n    } else {\n        event_notifier_set_handler(&vq->host_notifier, NULL);\n    }\n    if (!assign) {\n        /* Test and clear notifier before after disabling event,\n         * in case poll callback didn't have time to run. */\n        virtio_queue_host_notifier_read(&vq->host_notifier);\n    }\n}\n\nEventNotifier *virtio_queue_get_host_notifier(VirtQueue *vq)\n{\n    return &vq->host_notifier;\n}\n\nvoid virtio_device_set_child_bus_name(VirtIODevice *vdev, char *bus_name)\n{\n    if (vdev->bus_name) {\n        g_free(vdev->bus_name);\n        vdev->bus_name = NULL;\n    }\n\n    if (bus_name) {\n        vdev->bus_name = g_strdup(bus_name);\n    }\n}\n\nstatic int virtio_device_init(DeviceState *qdev)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(qdev);\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(qdev);\n    assert(k->init != NULL);\n    if (k->init(vdev) < 0) {\n        return -1;\n    }\n    virtio_bus_plug_device(vdev);\n    return 0;\n}\n\nstatic int virtio_device_exit(DeviceState *qdev)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(qdev);\n\n    if (vdev->bus_name) {\n        g_free(vdev->bus_name);\n        vdev->bus_name = NULL;\n    }\n    return 0;\n}\n\nstatic void virtio_device_class_init(ObjectClass *klass, void *data)\n{\n    /* Set the default value here. */\n    DeviceClass *dc = DEVICE_CLASS(klass);\n    dc->init = virtio_device_init;\n    dc->exit = virtio_device_exit;\n    dc->bus_type = TYPE_VIRTIO_BUS;\n}\n\nstatic const TypeInfo virtio_device_info = {\n    .name = TYPE_VIRTIO_DEVICE,\n    .parent = TYPE_DEVICE,\n    .instance_size = sizeof(VirtIODevice),\n    .class_init = virtio_device_class_init,\n    .abstract = true,\n    .class_size = sizeof(VirtioDeviceClass),\n};\n\nstatic void virtio_register_types(void)\n{\n    type_register_static(&virtio_device_info);\n}\n\ntype_init(virtio_register_types)\n"], "fixing_code": ["/*\n * Virtio Support\n *\n * Copyright IBM, Corp. 2007\n *\n * Authors:\n *  Anthony Liguori   <aliguori@us.ibm.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n#include <inttypes.h>\n\n#include \"trace.h\"\n#include \"qemu/error-report.h\"\n#include \"hw/virtio/virtio.h\"\n#include \"qemu/atomic.h\"\n#include \"hw/virtio/virtio-bus.h\"\n\n/* The alignment to use between consumer and producer parts of vring.\n * x86 pagesize again. */\n#define VIRTIO_PCI_VRING_ALIGN         4096\n\ntypedef struct VRingDesc\n{\n    uint64_t addr;\n    uint32_t len;\n    uint16_t flags;\n    uint16_t next;\n} VRingDesc;\n\ntypedef struct VRingAvail\n{\n    uint16_t flags;\n    uint16_t idx;\n    uint16_t ring[0];\n} VRingAvail;\n\ntypedef struct VRingUsedElem\n{\n    uint32_t id;\n    uint32_t len;\n} VRingUsedElem;\n\ntypedef struct VRingUsed\n{\n    uint16_t flags;\n    uint16_t idx;\n    VRingUsedElem ring[0];\n} VRingUsed;\n\ntypedef struct VRing\n{\n    unsigned int num;\n    hwaddr desc;\n    hwaddr avail;\n    hwaddr used;\n} VRing;\n\nstruct VirtQueue\n{\n    VRing vring;\n    hwaddr pa;\n    uint16_t last_avail_idx;\n    /* Last used index value we have signalled on */\n    uint16_t signalled_used;\n\n    /* Last used index value we have signalled on */\n    bool signalled_used_valid;\n\n    /* Notification enabled? */\n    bool notification;\n\n    uint16_t queue_index;\n\n    int inuse;\n\n    uint16_t vector;\n    void (*handle_output)(VirtIODevice *vdev, VirtQueue *vq);\n    VirtIODevice *vdev;\n    EventNotifier guest_notifier;\n    EventNotifier host_notifier;\n};\n\n/* virt queue functions */\nstatic void virtqueue_init(VirtQueue *vq)\n{\n    hwaddr pa = vq->pa;\n\n    vq->vring.desc = pa;\n    vq->vring.avail = pa + vq->vring.num * sizeof(VRingDesc);\n    vq->vring.used = vring_align(vq->vring.avail +\n                                 offsetof(VRingAvail, ring[vq->vring.num]),\n                                 VIRTIO_PCI_VRING_ALIGN);\n}\n\nstatic inline uint64_t vring_desc_addr(hwaddr desc_pa, int i)\n{\n    hwaddr pa;\n    pa = desc_pa + sizeof(VRingDesc) * i + offsetof(VRingDesc, addr);\n    return ldq_phys(pa);\n}\n\nstatic inline uint32_t vring_desc_len(hwaddr desc_pa, int i)\n{\n    hwaddr pa;\n    pa = desc_pa + sizeof(VRingDesc) * i + offsetof(VRingDesc, len);\n    return ldl_phys(pa);\n}\n\nstatic inline uint16_t vring_desc_flags(hwaddr desc_pa, int i)\n{\n    hwaddr pa;\n    pa = desc_pa + sizeof(VRingDesc) * i + offsetof(VRingDesc, flags);\n    return lduw_phys(pa);\n}\n\nstatic inline uint16_t vring_desc_next(hwaddr desc_pa, int i)\n{\n    hwaddr pa;\n    pa = desc_pa + sizeof(VRingDesc) * i + offsetof(VRingDesc, next);\n    return lduw_phys(pa);\n}\n\nstatic inline uint16_t vring_avail_flags(VirtQueue *vq)\n{\n    hwaddr pa;\n    pa = vq->vring.avail + offsetof(VRingAvail, flags);\n    return lduw_phys(pa);\n}\n\nstatic inline uint16_t vring_avail_idx(VirtQueue *vq)\n{\n    hwaddr pa;\n    pa = vq->vring.avail + offsetof(VRingAvail, idx);\n    return lduw_phys(pa);\n}\n\nstatic inline uint16_t vring_avail_ring(VirtQueue *vq, int i)\n{\n    hwaddr pa;\n    pa = vq->vring.avail + offsetof(VRingAvail, ring[i]);\n    return lduw_phys(pa);\n}\n\nstatic inline uint16_t vring_used_event(VirtQueue *vq)\n{\n    return vring_avail_ring(vq, vq->vring.num);\n}\n\nstatic inline void vring_used_ring_id(VirtQueue *vq, int i, uint32_t val)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, ring[i].id);\n    stl_phys(pa, val);\n}\n\nstatic inline void vring_used_ring_len(VirtQueue *vq, int i, uint32_t val)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, ring[i].len);\n    stl_phys(pa, val);\n}\n\nstatic uint16_t vring_used_idx(VirtQueue *vq)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, idx);\n    return lduw_phys(pa);\n}\n\nstatic inline void vring_used_idx_set(VirtQueue *vq, uint16_t val)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, idx);\n    stw_phys(pa, val);\n}\n\nstatic inline void vring_used_flags_set_bit(VirtQueue *vq, int mask)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, flags);\n    stw_phys(pa, lduw_phys(pa) | mask);\n}\n\nstatic inline void vring_used_flags_unset_bit(VirtQueue *vq, int mask)\n{\n    hwaddr pa;\n    pa = vq->vring.used + offsetof(VRingUsed, flags);\n    stw_phys(pa, lduw_phys(pa) & ~mask);\n}\n\nstatic inline void vring_avail_event(VirtQueue *vq, uint16_t val)\n{\n    hwaddr pa;\n    if (!vq->notification) {\n        return;\n    }\n    pa = vq->vring.used + offsetof(VRingUsed, ring[vq->vring.num]);\n    stw_phys(pa, val);\n}\n\nvoid virtio_queue_set_notification(VirtQueue *vq, int enable)\n{\n    vq->notification = enable;\n    if (vq->vdev->guest_features & (1 << VIRTIO_RING_F_EVENT_IDX)) {\n        vring_avail_event(vq, vring_avail_idx(vq));\n    } else if (enable) {\n        vring_used_flags_unset_bit(vq, VRING_USED_F_NO_NOTIFY);\n    } else {\n        vring_used_flags_set_bit(vq, VRING_USED_F_NO_NOTIFY);\n    }\n    if (enable) {\n        /* Expose avail event/used flags before caller checks the avail idx. */\n        smp_mb();\n    }\n}\n\nint virtio_queue_ready(VirtQueue *vq)\n{\n    return vq->vring.avail != 0;\n}\n\nint virtio_queue_empty(VirtQueue *vq)\n{\n    return vring_avail_idx(vq) == vq->last_avail_idx;\n}\n\nvoid virtqueue_fill(VirtQueue *vq, const VirtQueueElement *elem,\n                    unsigned int len, unsigned int idx)\n{\n    unsigned int offset;\n    int i;\n\n    trace_virtqueue_fill(vq, elem, len, idx);\n\n    offset = 0;\n    for (i = 0; i < elem->in_num; i++) {\n        size_t size = MIN(len - offset, elem->in_sg[i].iov_len);\n\n        cpu_physical_memory_unmap(elem->in_sg[i].iov_base,\n                                  elem->in_sg[i].iov_len,\n                                  1, size);\n\n        offset += size;\n    }\n\n    for (i = 0; i < elem->out_num; i++)\n        cpu_physical_memory_unmap(elem->out_sg[i].iov_base,\n                                  elem->out_sg[i].iov_len,\n                                  0, elem->out_sg[i].iov_len);\n\n    idx = (idx + vring_used_idx(vq)) % vq->vring.num;\n\n    /* Get a pointer to the next entry in the used ring. */\n    vring_used_ring_id(vq, idx, elem->index);\n    vring_used_ring_len(vq, idx, len);\n}\n\nvoid virtqueue_flush(VirtQueue *vq, unsigned int count)\n{\n    uint16_t old, new;\n    /* Make sure buffer is written before we update index. */\n    smp_wmb();\n    trace_virtqueue_flush(vq, count);\n    old = vring_used_idx(vq);\n    new = old + count;\n    vring_used_idx_set(vq, new);\n    vq->inuse -= count;\n    if (unlikely((int16_t)(new - vq->signalled_used) < (uint16_t)(new - old)))\n        vq->signalled_used_valid = false;\n}\n\nvoid virtqueue_push(VirtQueue *vq, const VirtQueueElement *elem,\n                    unsigned int len)\n{\n    virtqueue_fill(vq, elem, len, 0);\n    virtqueue_flush(vq, 1);\n}\n\nstatic int virtqueue_num_heads(VirtQueue *vq, unsigned int idx)\n{\n    uint16_t num_heads = vring_avail_idx(vq) - idx;\n\n    /* Check it isn't doing very strange things with descriptor numbers. */\n    if (num_heads > vq->vring.num) {\n        error_report(\"Guest moved used index from %u to %u\",\n                     idx, vring_avail_idx(vq));\n        exit(1);\n    }\n    /* On success, callers read a descriptor at vq->last_avail_idx.\n     * Make sure descriptor read does not bypass avail index read. */\n    if (num_heads) {\n        smp_rmb();\n    }\n\n    return num_heads;\n}\n\nstatic unsigned int virtqueue_get_head(VirtQueue *vq, unsigned int idx)\n{\n    unsigned int head;\n\n    /* Grab the next descriptor number they're advertising, and increment\n     * the index we've seen. */\n    head = vring_avail_ring(vq, idx % vq->vring.num);\n\n    /* If their number is silly, that's a fatal mistake. */\n    if (head >= vq->vring.num) {\n        error_report(\"Guest says index %u is available\", head);\n        exit(1);\n    }\n\n    return head;\n}\n\nstatic unsigned virtqueue_next_desc(hwaddr desc_pa,\n                                    unsigned int i, unsigned int max)\n{\n    unsigned int next;\n\n    /* If this descriptor says it doesn't chain, we're done. */\n    if (!(vring_desc_flags(desc_pa, i) & VRING_DESC_F_NEXT))\n        return max;\n\n    /* Check they're not leading us off end of descriptors. */\n    next = vring_desc_next(desc_pa, i);\n    /* Make sure compiler knows to grab that: we don't want it changing! */\n    smp_wmb();\n\n    if (next >= max) {\n        error_report(\"Desc next is %u\", next);\n        exit(1);\n    }\n\n    return next;\n}\n\nvoid virtqueue_get_avail_bytes(VirtQueue *vq, unsigned int *in_bytes,\n                               unsigned int *out_bytes,\n                               unsigned max_in_bytes, unsigned max_out_bytes)\n{\n    unsigned int idx;\n    unsigned int total_bufs, in_total, out_total;\n\n    idx = vq->last_avail_idx;\n\n    total_bufs = in_total = out_total = 0;\n    while (virtqueue_num_heads(vq, idx)) {\n        unsigned int max, num_bufs, indirect = 0;\n        hwaddr desc_pa;\n        int i;\n\n        max = vq->vring.num;\n        num_bufs = total_bufs;\n        i = virtqueue_get_head(vq, idx++);\n        desc_pa = vq->vring.desc;\n\n        if (vring_desc_flags(desc_pa, i) & VRING_DESC_F_INDIRECT) {\n            if (vring_desc_len(desc_pa, i) % sizeof(VRingDesc)) {\n                error_report(\"Invalid size for indirect buffer table\");\n                exit(1);\n            }\n\n            /* If we've got too many, that implies a descriptor loop. */\n            if (num_bufs >= max) {\n                error_report(\"Looped descriptor\");\n                exit(1);\n            }\n\n            /* loop over the indirect descriptor table */\n            indirect = 1;\n            max = vring_desc_len(desc_pa, i) / sizeof(VRingDesc);\n            num_bufs = i = 0;\n            desc_pa = vring_desc_addr(desc_pa, i);\n        }\n\n        do {\n            /* If we've got too many, that implies a descriptor loop. */\n            if (++num_bufs > max) {\n                error_report(\"Looped descriptor\");\n                exit(1);\n            }\n\n            if (vring_desc_flags(desc_pa, i) & VRING_DESC_F_WRITE) {\n                in_total += vring_desc_len(desc_pa, i);\n            } else {\n                out_total += vring_desc_len(desc_pa, i);\n            }\n            if (in_total >= max_in_bytes && out_total >= max_out_bytes) {\n                goto done;\n            }\n        } while ((i = virtqueue_next_desc(desc_pa, i, max)) != max);\n\n        if (!indirect)\n            total_bufs = num_bufs;\n        else\n            total_bufs++;\n    }\ndone:\n    if (in_bytes) {\n        *in_bytes = in_total;\n    }\n    if (out_bytes) {\n        *out_bytes = out_total;\n    }\n}\n\nint virtqueue_avail_bytes(VirtQueue *vq, unsigned int in_bytes,\n                          unsigned int out_bytes)\n{\n    unsigned int in_total, out_total;\n\n    virtqueue_get_avail_bytes(vq, &in_total, &out_total, in_bytes, out_bytes);\n    return in_bytes <= in_total && out_bytes <= out_total;\n}\n\nvoid virtqueue_map_sg(struct iovec *sg, hwaddr *addr,\n    size_t num_sg, int is_write)\n{\n    unsigned int i;\n    hwaddr len;\n\n    for (i = 0; i < num_sg; i++) {\n        len = sg[i].iov_len;\n        sg[i].iov_base = cpu_physical_memory_map(addr[i], &len, is_write);\n        if (sg[i].iov_base == NULL || len != sg[i].iov_len) {\n            error_report(\"virtio: trying to map MMIO memory\");\n            exit(1);\n        }\n    }\n}\n\nint virtqueue_pop(VirtQueue *vq, VirtQueueElement *elem)\n{\n    unsigned int i, head, max;\n    hwaddr desc_pa = vq->vring.desc;\n\n    if (!virtqueue_num_heads(vq, vq->last_avail_idx))\n        return 0;\n\n    /* When we start there are none of either input nor output. */\n    elem->out_num = elem->in_num = 0;\n\n    max = vq->vring.num;\n\n    i = head = virtqueue_get_head(vq, vq->last_avail_idx++);\n    if (vq->vdev->guest_features & (1 << VIRTIO_RING_F_EVENT_IDX)) {\n        vring_avail_event(vq, vring_avail_idx(vq));\n    }\n\n    if (vring_desc_flags(desc_pa, i) & VRING_DESC_F_INDIRECT) {\n        if (vring_desc_len(desc_pa, i) % sizeof(VRingDesc)) {\n            error_report(\"Invalid size for indirect buffer table\");\n            exit(1);\n        }\n\n        /* loop over the indirect descriptor table */\n        max = vring_desc_len(desc_pa, i) / sizeof(VRingDesc);\n        desc_pa = vring_desc_addr(desc_pa, i);\n        i = 0;\n    }\n\n    /* Collect all the descriptors */\n    do {\n        struct iovec *sg;\n\n        if (vring_desc_flags(desc_pa, i) & VRING_DESC_F_WRITE) {\n            if (elem->in_num >= ARRAY_SIZE(elem->in_sg)) {\n                error_report(\"Too many write descriptors in indirect table\");\n                exit(1);\n            }\n            elem->in_addr[elem->in_num] = vring_desc_addr(desc_pa, i);\n            sg = &elem->in_sg[elem->in_num++];\n        } else {\n            if (elem->out_num >= ARRAY_SIZE(elem->out_sg)) {\n                error_report(\"Too many read descriptors in indirect table\");\n                exit(1);\n            }\n            elem->out_addr[elem->out_num] = vring_desc_addr(desc_pa, i);\n            sg = &elem->out_sg[elem->out_num++];\n        }\n\n        sg->iov_len = vring_desc_len(desc_pa, i);\n\n        /* If we've got too many, that implies a descriptor loop. */\n        if ((elem->in_num + elem->out_num) > max) {\n            error_report(\"Looped descriptor\");\n            exit(1);\n        }\n    } while ((i = virtqueue_next_desc(desc_pa, i, max)) != max);\n\n    /* Now map what we have collected */\n    virtqueue_map_sg(elem->in_sg, elem->in_addr, elem->in_num, 1);\n    virtqueue_map_sg(elem->out_sg, elem->out_addr, elem->out_num, 0);\n\n    elem->index = head;\n\n    vq->inuse++;\n\n    trace_virtqueue_pop(vq, elem, elem->in_num, elem->out_num);\n    return elem->in_num + elem->out_num;\n}\n\n/* virtio device */\nstatic void virtio_notify_vector(VirtIODevice *vdev, uint16_t vector)\n{\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n\n    if (k->notify) {\n        k->notify(qbus->parent, vector);\n    }\n}\n\nvoid virtio_update_irq(VirtIODevice *vdev)\n{\n    virtio_notify_vector(vdev, VIRTIO_NO_VECTOR);\n}\n\nvoid virtio_set_status(VirtIODevice *vdev, uint8_t val)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    trace_virtio_set_status(vdev, val);\n\n    if (k->set_status) {\n        k->set_status(vdev, val);\n    }\n    vdev->status = val;\n}\n\nvoid virtio_reset(void *opaque)\n{\n    VirtIODevice *vdev = opaque;\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    int i;\n\n    virtio_set_status(vdev, 0);\n\n    if (k->reset) {\n        k->reset(vdev);\n    }\n\n    vdev->guest_features = 0;\n    vdev->queue_sel = 0;\n    vdev->status = 0;\n    vdev->isr = 0;\n    vdev->config_vector = VIRTIO_NO_VECTOR;\n    virtio_notify_vector(vdev, vdev->config_vector);\n\n    for(i = 0; i < VIRTIO_PCI_QUEUE_MAX; i++) {\n        vdev->vq[i].vring.desc = 0;\n        vdev->vq[i].vring.avail = 0;\n        vdev->vq[i].vring.used = 0;\n        vdev->vq[i].last_avail_idx = 0;\n        vdev->vq[i].pa = 0;\n        vdev->vq[i].vector = VIRTIO_NO_VECTOR;\n        vdev->vq[i].signalled_used = 0;\n        vdev->vq[i].signalled_used_valid = false;\n        vdev->vq[i].notification = true;\n    }\n}\n\nuint32_t virtio_config_readb(VirtIODevice *vdev, uint32_t addr)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint8_t val;\n\n    if (addr + sizeof(val) > vdev->config_len) {\n        return (uint32_t)-1;\n    }\n\n    k->get_config(vdev, vdev->config);\n\n    val = ldub_p(vdev->config + addr);\n    return val;\n}\n\nuint32_t virtio_config_readw(VirtIODevice *vdev, uint32_t addr)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint16_t val;\n\n    if (addr + sizeof(val) > vdev->config_len) {\n        return (uint32_t)-1;\n    }\n\n    k->get_config(vdev, vdev->config);\n\n    val = lduw_p(vdev->config + addr);\n    return val;\n}\n\nuint32_t virtio_config_readl(VirtIODevice *vdev, uint32_t addr)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint32_t val;\n\n    if (addr + sizeof(val) > vdev->config_len) {\n        return (uint32_t)-1;\n    }\n\n    k->get_config(vdev, vdev->config);\n\n    val = ldl_p(vdev->config + addr);\n    return val;\n}\n\nvoid virtio_config_writeb(VirtIODevice *vdev, uint32_t addr, uint32_t data)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint8_t val = data;\n\n    if (addr + sizeof(val) > vdev->config_len) {\n        return;\n    }\n\n    stb_p(vdev->config + addr, val);\n\n    if (k->set_config) {\n        k->set_config(vdev, vdev->config);\n    }\n}\n\nvoid virtio_config_writew(VirtIODevice *vdev, uint32_t addr, uint32_t data)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint16_t val = data;\n\n    if (addr + sizeof(val) > vdev->config_len) {\n        return;\n    }\n\n    stw_p(vdev->config + addr, val);\n\n    if (k->set_config) {\n        k->set_config(vdev, vdev->config);\n    }\n}\n\nvoid virtio_config_writel(VirtIODevice *vdev, uint32_t addr, uint32_t data)\n{\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint32_t val = data;\n\n    if (addr + sizeof(val) > vdev->config_len) {\n        return;\n    }\n\n    stl_p(vdev->config + addr, val);\n\n    if (k->set_config) {\n        k->set_config(vdev, vdev->config);\n    }\n}\n\nvoid virtio_queue_set_addr(VirtIODevice *vdev, int n, hwaddr addr)\n{\n    vdev->vq[n].pa = addr;\n    virtqueue_init(&vdev->vq[n]);\n}\n\nhwaddr virtio_queue_get_addr(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].pa;\n}\n\nint virtio_queue_get_num(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.num;\n}\n\nint virtio_queue_get_id(VirtQueue *vq)\n{\n    VirtIODevice *vdev = vq->vdev;\n    assert(vq >= &vdev->vq[0] && vq < &vdev->vq[VIRTIO_PCI_QUEUE_MAX]);\n    return vq - &vdev->vq[0];\n}\n\nvoid virtio_queue_notify_vq(VirtQueue *vq)\n{\n    if (vq->vring.desc) {\n        VirtIODevice *vdev = vq->vdev;\n        trace_virtio_queue_notify(vdev, vq - vdev->vq, vq);\n        vq->handle_output(vdev, vq);\n    }\n}\n\nvoid virtio_queue_notify(VirtIODevice *vdev, int n)\n{\n    virtio_queue_notify_vq(&vdev->vq[n]);\n}\n\nuint16_t virtio_queue_vector(VirtIODevice *vdev, int n)\n{\n    return n < VIRTIO_PCI_QUEUE_MAX ? vdev->vq[n].vector :\n        VIRTIO_NO_VECTOR;\n}\n\nvoid virtio_queue_set_vector(VirtIODevice *vdev, int n, uint16_t vector)\n{\n    if (n < VIRTIO_PCI_QUEUE_MAX)\n        vdev->vq[n].vector = vector;\n}\n\nVirtQueue *virtio_add_queue(VirtIODevice *vdev, int queue_size,\n                            void (*handle_output)(VirtIODevice *, VirtQueue *))\n{\n    int i;\n\n    for (i = 0; i < VIRTIO_PCI_QUEUE_MAX; i++) {\n        if (vdev->vq[i].vring.num == 0)\n            break;\n    }\n\n    if (i == VIRTIO_PCI_QUEUE_MAX || queue_size > VIRTQUEUE_MAX_SIZE)\n        abort();\n\n    vdev->vq[i].vring.num = queue_size;\n    vdev->vq[i].handle_output = handle_output;\n\n    return &vdev->vq[i];\n}\n\nvoid virtio_del_queue(VirtIODevice *vdev, int n)\n{\n    if (n < 0 || n >= VIRTIO_PCI_QUEUE_MAX) {\n        abort();\n    }\n\n    vdev->vq[n].vring.num = 0;\n}\n\nvoid virtio_irq(VirtQueue *vq)\n{\n    trace_virtio_irq(vq);\n    vq->vdev->isr |= 0x01;\n    virtio_notify_vector(vq->vdev, vq->vector);\n}\n\n/* Assuming a given event_idx value from the other size, if\n * we have just incremented index from old to new_idx,\n * should we trigger an event? */\nstatic inline int vring_need_event(uint16_t event, uint16_t new, uint16_t old)\n{\n\t/* Note: Xen has similar logic for notification hold-off\n\t * in include/xen/interface/io/ring.h with req_event and req_prod\n\t * corresponding to event_idx + 1 and new respectively.\n\t * Note also that req_event and req_prod in Xen start at 1,\n\t * event indexes in virtio start at 0. */\n\treturn (uint16_t)(new - event - 1) < (uint16_t)(new - old);\n}\n\nstatic bool vring_notify(VirtIODevice *vdev, VirtQueue *vq)\n{\n    uint16_t old, new;\n    bool v;\n    /* We need to expose used array entries before checking used event. */\n    smp_mb();\n    /* Always notify when queue is empty (when feature acknowledge) */\n    if (((vdev->guest_features & (1 << VIRTIO_F_NOTIFY_ON_EMPTY)) &&\n         !vq->inuse && vring_avail_idx(vq) == vq->last_avail_idx)) {\n        return true;\n    }\n\n    if (!(vdev->guest_features & (1 << VIRTIO_RING_F_EVENT_IDX))) {\n        return !(vring_avail_flags(vq) & VRING_AVAIL_F_NO_INTERRUPT);\n    }\n\n    v = vq->signalled_used_valid;\n    vq->signalled_used_valid = true;\n    old = vq->signalled_used;\n    new = vq->signalled_used = vring_used_idx(vq);\n    return !v || vring_need_event(vring_used_event(vq), new, old);\n}\n\nvoid virtio_notify(VirtIODevice *vdev, VirtQueue *vq)\n{\n    if (!vring_notify(vdev, vq)) {\n        return;\n    }\n\n    trace_virtio_notify(vdev, vq);\n    vdev->isr |= 0x01;\n    virtio_notify_vector(vdev, vq->vector);\n}\n\nvoid virtio_notify_config(VirtIODevice *vdev)\n{\n    if (!(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK))\n        return;\n\n    vdev->isr |= 0x03;\n    virtio_notify_vector(vdev, vdev->config_vector);\n}\n\nvoid virtio_save(VirtIODevice *vdev, QEMUFile *f)\n{\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n    int i;\n\n    if (k->save_config) {\n        k->save_config(qbus->parent, f);\n    }\n\n    qemu_put_8s(f, &vdev->status);\n    qemu_put_8s(f, &vdev->isr);\n    qemu_put_be16s(f, &vdev->queue_sel);\n    qemu_put_be32s(f, &vdev->guest_features);\n    qemu_put_be32(f, vdev->config_len);\n    qemu_put_buffer(f, vdev->config, vdev->config_len);\n\n    for (i = 0; i < VIRTIO_PCI_QUEUE_MAX; i++) {\n        if (vdev->vq[i].vring.num == 0)\n            break;\n    }\n\n    qemu_put_be32(f, i);\n\n    for (i = 0; i < VIRTIO_PCI_QUEUE_MAX; i++) {\n        if (vdev->vq[i].vring.num == 0)\n            break;\n\n        qemu_put_be32(f, vdev->vq[i].vring.num);\n        qemu_put_be64(f, vdev->vq[i].pa);\n        qemu_put_be16s(f, &vdev->vq[i].last_avail_idx);\n        if (k->save_queue) {\n            k->save_queue(qbus->parent, i, f);\n        }\n    }\n}\n\nint virtio_set_features(VirtIODevice *vdev, uint32_t val)\n{\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *vbusk = VIRTIO_BUS_GET_CLASS(qbus);\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(vdev);\n    uint32_t supported_features = vbusk->get_features(qbus->parent);\n    bool bad = (val & ~supported_features) != 0;\n\n    val &= supported_features;\n    if (k->set_features) {\n        k->set_features(vdev, val);\n    }\n    vdev->guest_features = val;\n    return bad ? -1 : 0;\n}\n\nint virtio_load(VirtIODevice *vdev, QEMUFile *f)\n{\n    int num, i, ret;\n    uint32_t features;\n    uint32_t supported_features;\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n\n    if (k->load_config) {\n        ret = k->load_config(qbus->parent, f);\n        if (ret)\n            return ret;\n    }\n\n    qemu_get_8s(f, &vdev->status);\n    qemu_get_8s(f, &vdev->isr);\n    qemu_get_be16s(f, &vdev->queue_sel);\n    qemu_get_be32s(f, &features);\n\n    if (virtio_set_features(vdev, features) < 0) {\n        supported_features = k->get_features(qbus->parent);\n        error_report(\"Features 0x%x unsupported. Allowed features: 0x%x\",\n                     features, supported_features);\n        return -1;\n    }\n    vdev->config_len = qemu_get_be32(f);\n    qemu_get_buffer(f, vdev->config, vdev->config_len);\n\n    num = qemu_get_be32(f);\n\n    for (i = 0; i < num; i++) {\n        vdev->vq[i].vring.num = qemu_get_be32(f);\n        vdev->vq[i].pa = qemu_get_be64(f);\n        qemu_get_be16s(f, &vdev->vq[i].last_avail_idx);\n        vdev->vq[i].signalled_used_valid = false;\n        vdev->vq[i].notification = true;\n\n        if (vdev->vq[i].pa) {\n            uint16_t nheads;\n            virtqueue_init(&vdev->vq[i]);\n            nheads = vring_avail_idx(&vdev->vq[i]) - vdev->vq[i].last_avail_idx;\n            /* Check it isn't doing very strange things with descriptor numbers. */\n            if (nheads > vdev->vq[i].vring.num) {\n                error_report(\"VQ %d size 0x%x Guest index 0x%x \"\n                             \"inconsistent with Host index 0x%x: delta 0x%x\",\n                             i, vdev->vq[i].vring.num,\n                             vring_avail_idx(&vdev->vq[i]),\n                             vdev->vq[i].last_avail_idx, nheads);\n                return -1;\n            }\n        } else if (vdev->vq[i].last_avail_idx) {\n            error_report(\"VQ %d address 0x0 \"\n                         \"inconsistent with Host index 0x%x\",\n                         i, vdev->vq[i].last_avail_idx);\n                return -1;\n\t}\n        if (k->load_queue) {\n            ret = k->load_queue(qbus->parent, i, f);\n            if (ret)\n                return ret;\n        }\n    }\n\n    virtio_notify_vector(vdev, VIRTIO_NO_VECTOR);\n    return 0;\n}\n\nvoid virtio_cleanup(VirtIODevice *vdev)\n{\n    qemu_del_vm_change_state_handler(vdev->vmstate);\n    g_free(vdev->config);\n    g_free(vdev->vq);\n}\n\nstatic void virtio_vmstate_change(void *opaque, int running, RunState state)\n{\n    VirtIODevice *vdev = opaque;\n    BusState *qbus = qdev_get_parent_bus(DEVICE(vdev));\n    VirtioBusClass *k = VIRTIO_BUS_GET_CLASS(qbus);\n    bool backend_run = running && (vdev->status & VIRTIO_CONFIG_S_DRIVER_OK);\n    vdev->vm_running = running;\n\n    if (backend_run) {\n        virtio_set_status(vdev, vdev->status);\n    }\n\n    if (k->vmstate_change) {\n        k->vmstate_change(qbus->parent, backend_run);\n    }\n\n    if (!backend_run) {\n        virtio_set_status(vdev, vdev->status);\n    }\n}\n\nvoid virtio_init(VirtIODevice *vdev, const char *name,\n                 uint16_t device_id, size_t config_size)\n{\n    int i;\n    vdev->device_id = device_id;\n    vdev->status = 0;\n    vdev->isr = 0;\n    vdev->queue_sel = 0;\n    vdev->config_vector = VIRTIO_NO_VECTOR;\n    vdev->vq = g_malloc0(sizeof(VirtQueue) * VIRTIO_PCI_QUEUE_MAX);\n    vdev->vm_running = runstate_is_running();\n    for (i = 0; i < VIRTIO_PCI_QUEUE_MAX; i++) {\n        vdev->vq[i].vector = VIRTIO_NO_VECTOR;\n        vdev->vq[i].vdev = vdev;\n        vdev->vq[i].queue_index = i;\n    }\n\n    vdev->name = name;\n    vdev->config_len = config_size;\n    if (vdev->config_len) {\n        vdev->config = g_malloc0(config_size);\n    } else {\n        vdev->config = NULL;\n    }\n    vdev->vmstate = qemu_add_vm_change_state_handler(virtio_vmstate_change,\n                                                     vdev);\n}\n\nhwaddr virtio_queue_get_desc_addr(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.desc;\n}\n\nhwaddr virtio_queue_get_avail_addr(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.avail;\n}\n\nhwaddr virtio_queue_get_used_addr(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.used;\n}\n\nhwaddr virtio_queue_get_ring_addr(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.desc;\n}\n\nhwaddr virtio_queue_get_desc_size(VirtIODevice *vdev, int n)\n{\n    return sizeof(VRingDesc) * vdev->vq[n].vring.num;\n}\n\nhwaddr virtio_queue_get_avail_size(VirtIODevice *vdev, int n)\n{\n    return offsetof(VRingAvail, ring) +\n        sizeof(uint64_t) * vdev->vq[n].vring.num;\n}\n\nhwaddr virtio_queue_get_used_size(VirtIODevice *vdev, int n)\n{\n    return offsetof(VRingUsed, ring) +\n        sizeof(VRingUsedElem) * vdev->vq[n].vring.num;\n}\n\nhwaddr virtio_queue_get_ring_size(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].vring.used - vdev->vq[n].vring.desc +\n\t    virtio_queue_get_used_size(vdev, n);\n}\n\nuint16_t virtio_queue_get_last_avail_idx(VirtIODevice *vdev, int n)\n{\n    return vdev->vq[n].last_avail_idx;\n}\n\nvoid virtio_queue_set_last_avail_idx(VirtIODevice *vdev, int n, uint16_t idx)\n{\n    vdev->vq[n].last_avail_idx = idx;\n}\n\nVirtQueue *virtio_get_queue(VirtIODevice *vdev, int n)\n{\n    return vdev->vq + n;\n}\n\nuint16_t virtio_get_queue_index(VirtQueue *vq)\n{\n    return vq->queue_index;\n}\n\nstatic void virtio_queue_guest_notifier_read(EventNotifier *n)\n{\n    VirtQueue *vq = container_of(n, VirtQueue, guest_notifier);\n    if (event_notifier_test_and_clear(n)) {\n        virtio_irq(vq);\n    }\n}\n\nvoid virtio_queue_set_guest_notifier_fd_handler(VirtQueue *vq, bool assign,\n                                                bool with_irqfd)\n{\n    if (assign && !with_irqfd) {\n        event_notifier_set_handler(&vq->guest_notifier,\n                                   virtio_queue_guest_notifier_read);\n    } else {\n        event_notifier_set_handler(&vq->guest_notifier, NULL);\n    }\n    if (!assign) {\n        /* Test and clear notifier before closing it,\n         * in case poll callback didn't have time to run. */\n        virtio_queue_guest_notifier_read(&vq->guest_notifier);\n    }\n}\n\nEventNotifier *virtio_queue_get_guest_notifier(VirtQueue *vq)\n{\n    return &vq->guest_notifier;\n}\n\nstatic void virtio_queue_host_notifier_read(EventNotifier *n)\n{\n    VirtQueue *vq = container_of(n, VirtQueue, host_notifier);\n    if (event_notifier_test_and_clear(n)) {\n        virtio_queue_notify_vq(vq);\n    }\n}\n\nvoid virtio_queue_set_host_notifier_fd_handler(VirtQueue *vq, bool assign,\n                                               bool set_handler)\n{\n    if (assign && set_handler) {\n        event_notifier_set_handler(&vq->host_notifier,\n                                   virtio_queue_host_notifier_read);\n    } else {\n        event_notifier_set_handler(&vq->host_notifier, NULL);\n    }\n    if (!assign) {\n        /* Test and clear notifier before after disabling event,\n         * in case poll callback didn't have time to run. */\n        virtio_queue_host_notifier_read(&vq->host_notifier);\n    }\n}\n\nEventNotifier *virtio_queue_get_host_notifier(VirtQueue *vq)\n{\n    return &vq->host_notifier;\n}\n\nvoid virtio_device_set_child_bus_name(VirtIODevice *vdev, char *bus_name)\n{\n    if (vdev->bus_name) {\n        g_free(vdev->bus_name);\n        vdev->bus_name = NULL;\n    }\n\n    if (bus_name) {\n        vdev->bus_name = g_strdup(bus_name);\n    }\n}\n\nstatic int virtio_device_init(DeviceState *qdev)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(qdev);\n    VirtioDeviceClass *k = VIRTIO_DEVICE_GET_CLASS(qdev);\n    assert(k->init != NULL);\n    if (k->init(vdev) < 0) {\n        return -1;\n    }\n    virtio_bus_plug_device(vdev);\n    return 0;\n}\n\nstatic int virtio_device_exit(DeviceState *qdev)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(qdev);\n\n    if (vdev->bus_name) {\n        g_free(vdev->bus_name);\n        vdev->bus_name = NULL;\n    }\n    return 0;\n}\n\nstatic void virtio_device_class_init(ObjectClass *klass, void *data)\n{\n    /* Set the default value here. */\n    DeviceClass *dc = DEVICE_CLASS(klass);\n    dc->init = virtio_device_init;\n    dc->exit = virtio_device_exit;\n    dc->bus_type = TYPE_VIRTIO_BUS;\n}\n\nstatic const TypeInfo virtio_device_info = {\n    .name = TYPE_VIRTIO_DEVICE,\n    .parent = TYPE_DEVICE,\n    .instance_size = sizeof(VirtIODevice),\n    .class_init = virtio_device_class_init,\n    .abstract = true,\n    .class_size = sizeof(VirtioDeviceClass),\n};\n\nstatic void virtio_register_types(void)\n{\n    type_register_static(&virtio_device_info);\n}\n\ntype_init(virtio_register_types)\n"], "filenames": ["hw/virtio/virtio.c"], "buggy_code_start_loc": [570], "buggy_code_end_loc": [644], "fixing_code_start_loc": [571], "fixing_code_end_loc": [651], "type": "CWE-269", "message": "A flaw was found in the way qemu v1.3.0 and later (virtio-rng) validates addresses when guest accesses the config space of a virtio device. If the virtio device has zero/small sized config space, such as virtio-rng, a privileged guest user could use this flaw to access the matching host's qemu address space and thus increase their privileges on the host.", "other": {"cve": {"id": "CVE-2013-2016", "sourceIdentifier": "secalert@redhat.com", "published": "2019-12-30T22:15:11.387", "lastModified": "2020-08-18T15:05:58.080", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A flaw was found in the way qemu v1.3.0 and later (virtio-rng) validates addresses when guest accesses the config space of a virtio device. If the virtio device has zero/small sized config space, such as virtio-rng, a privileged guest user could use this flaw to access the matching host's qemu address space and thus increase their privileges on the host."}, {"lang": "es", "value": "Se encontr\u00f3 un fallo en la manera en que qemu versi\u00f3n v1.3.0 y posteriores (virtio-rng) comprueba las direcciones cuando el invitado accede al espacio de configuraci\u00f3n de un dispositivo virtio. Si el dispositivo virtio posee un espacio de configuraci\u00f3n de tama\u00f1o cero o peque\u00f1o, ta y como virtio-rng, un usuario invitado privilegiado podr\u00eda usar este fallo para acceder al espacio de direcciones qemu del host correspondiente y as\u00ed aumentar sus privilegios en el host."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-269"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:qemu:qemu:*:*:*:*:*:*:*:*", "versionStartIncluding": "1.3.0", "versionEndIncluding": "1.4.2", "matchCriteriaId": "6265D005-1DD4-4CE2-89E1-01EC33046D1F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:qemu:qemu:1.5.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "35B1E3F1-4647-47FF-9546-0742F10B607B"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:novell:open_desktop_server:11.0:sp3:*:*:*:linux_kernel:*:*", "matchCriteriaId": "51188661-157E-476E-A6EC-23F1EFE44B52"}, {"vulnerable": true, "criteria": "cpe:2.3:a:novell:open_enterprise_server:11.0:sp3:*:*:*:linux_kernel:*:*", "matchCriteriaId": "8D3C2E01-F02A-4D69-AB67-53C0A59F123B"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2014-05/msg00002.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2013/04/29/5", "source": "secalert@redhat.com", "tags": ["Exploit", "Mailing List", "Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2013/04/29/6", "source": "secalert@redhat.com", "tags": ["Exploit", "Mailing List", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/59541", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/security/cve/cve-2013-2016", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=CVE-2013-2016", "source": "secalert@redhat.com", "tags": ["Exploit", "Issue Tracking"]}, {"url": "https://exchange.xforce.ibmcloud.com/vulnerabilities/83850", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/qemu/qemu/commit/5f5a1318653c08e435cfa52f60b6a712815b659d", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security-tracker.debian.org/tracker/CVE-2013-2016", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/qemu/qemu/commit/5f5a1318653c08e435cfa52f60b6a712815b659d"}}