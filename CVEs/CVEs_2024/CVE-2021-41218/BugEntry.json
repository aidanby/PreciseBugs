{"buggy_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n\nnamespace tensorflow {\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\nREGISTER_OP(\"AllToAll\")\n    .Input(\"input: T\")\n    .Input(\"group_assignment: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: {numbertype, bool}\")\n    .Attr(\"concat_dimension: int\")\n    .Attr(\"split_dimension: int\")\n    .Attr(\"split_count: int\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      if (!c->RankKnown(input)) {\n        c->set_output(0, c->UnknownShape());\n        return Status::OK();\n      }\n\n      int64_t rank = c->Rank(input);\n      int concat_dimension;\n      int split_dimension;\n      int split_count;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"split_count\", &split_count));\n\n      TF_RETURN_IF_ERROR(c->GetAttr(\"concat_dimension\", &concat_dimension));\n\n      if (concat_dimension < 0 || concat_dimension >= rank) {\n        return errors::InvalidArgument(\"concat_dimension \", concat_dimension,\n                                       \" is out of range of input rank \", rank);\n      }\n\n      TF_RETURN_IF_ERROR(c->GetAttr(\"split_dimension\", &split_dimension));\n      if (split_dimension < 0 || split_dimension >= rank) {\n        return errors::InvalidArgument(\"split_dimension \", split_dimension,\n                                       \" is out of range of input rank \", rank);\n      }\n\n      std::vector<DimensionHandle> dims;\n      dims.resize(rank);\n\n      for (int32_t i = 0; i < rank; ++i) {\n        dims[i] = c->Dim(input, i);\n        if (i == concat_dimension) {\n          dims[i] = c->MakeDim(c->Value(dims[i]) * split_count);\n        }\n        if (i == split_dimension) {\n          dims[i] = c->MakeDim(c->Value(dims[i]) / split_count);\n        }\n      }\n\n      c->set_output(0, c->MakeShape(dims));\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CrossReplicaSum\")\n    .Input(\"input: T\")\n    .Input(\"group_assignment: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, float64, int32, uint32}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"CollectivePermute\")\n    .Input(\"input: T\")\n    .Input(\"source_target_pairs: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n}  // namespace tensorflow\n", "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n\"\"\"Tests for tpu_function helpers.\"\"\"\n\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.layers import convolutional\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import special_math_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.tpu import tpu\nfrom tensorflow.python.tpu import tpu_feed\nfrom tensorflow.python.tpu import training_loop\n\n\nclass TPUContextTest(test.TestCase):\n\n  def testIsInContext(self):\n    \"\"\"Test that control_flow_util can check that we're in a TPU context.\"\"\"\n    with ops.Graph().as_default():\n      z1 = array_ops.identity(1)\n      pivot = control_flow_ops.no_op()\n      context = tpu.TPUReplicateContext(b\"context\", 1, pivot=pivot)\n      context.Enter()\n      z2 = array_ops.identity(1)\n      context.Exit()\n      self.assertFalse(control_flow_util.IsInXLAContext(z1.op))\n      self.assertTrue(control_flow_util.IsInXLAContext(z2.op))\n\n  def testHandlesNameCollision(self):\n    \"\"\"Test AddValue handles name collisions for ops from different graphs.\"\"\"\n    with ops.Graph().as_default():\n      z = array_ops.zeros([2, 3], name=\"a\")\n      assert z.name == \"a:0\", \"Expected: a:0, Found: %s\" % z.name\n\n      @def_function.function\n      def f():\n        pivot = control_flow_ops.no_op()\n        context = tpu.TPUReplicateContext(b\"context\", 1, pivot=pivot)\n        context.Enter()\n        array_ops.identity(z)  # Capture z.\n        z1 = array_ops.zeros([3, 2], name=\"a\")\n        assert z1.name == \"a:0\", \"Expected: a:0, Found: %s\" % z1.name\n        z2 = array_ops.zeros([3, 2], name=\"a\")\n        # Prior to fixing b/166794533 this would fail with a shape mismatch\n        # because context.AddValue would have cached `z` by its name which\n        # collides with z1's name.\n        result = z1 + z2\n        context.Exit()\n        return result\n\n      f.get_concrete_function()\n\n\nclass TPULayerRewriteTest(test.TestCase):\n\n  def testUsingInfeedQueueWithRegularizer(self):\n    \"\"\"Test that Layer regularizers can reference data created in loops.\"\"\"\n\n    with ops.Graph().as_default():\n\n      def make_regularizer(scale):\n        def regularizer(inputs):\n          return scale * math_ops.reduce_sum(math_ops.square(inputs))\n        return regularizer\n\n      def training_step(inputs, scale):\n        outputs = convolutional.conv2d(\n            inputs,\n            filters=16,\n            kernel_size=(3, 3),\n            data_format=\"channels_first\",\n            kernel_regularizer=make_regularizer(scale))\n        loss = math_ops.reduce_mean(math_ops.square(outputs))\n        return loss.op\n\n      inputs = array_ops.zeros(shape=(128, 32, 32, 16))\n      scale = array_ops.ones(shape=())\n      infeed = tpu_feed.InfeedQueue(\n          tuple_types=[dtypes.float32, dtypes.float32],\n          tuple_shapes=[inputs.shape, scale.shape])\n\n      def loop():\n        return training_loop.repeat(5, training_step, infeed_queue=infeed)\n\n      # This should not throw an error.\n      tpu.rewrite(loop)\n\n\nclass TPUGraphPruneTest(test.TestCase):\n\n  def test_prune_unconnected_ops(self):\n    with ops.Graph().as_default():\n      a = array_ops.placeholder(dtype=dtypes.float32, name=\"a\")\n      b = array_ops.placeholder(dtype=dtypes.float32, name=\"b\")\n      constant_op.constant(1.0, name=\"constant\")\n      x = variable_scope.get_variable(\n          name=\"x\",\n          dtype=dtypes.float32,\n          shape=[],\n          use_resource=True,\n          initializer=init_ops.constant_initializer(2.0))\n      y = variable_scope.get_variable(\n          name=\"y\",\n          dtype=dtypes.float32,\n          shape=[],\n          use_resource=True,\n          initializer=init_ops.constant_initializer(3.0))\n      math_ops.add(a, b)\n      math_ops.add(x, y)\n      graph_def = ops.get_default_graph().as_graph_def()\n\n      for node in graph_def.node:\n        # Attach a TPU_REPLICATE_ATTR to each node.\n        node.attr[tpu._TPU_REPLICATE_ATTR].s = b\"0\"\n        # Rewire placeholder \"a\" and variable \"y\" leaving them unconnected.\n        for (input_index, node_input) in enumerate(node.input):\n          if node_input == \"b\":\n            node.input[input_index] = \"constant\"\n          if node_input == \"y\":\n            node.input[input_index] = \"x\"\n\n    with ops.Graph().as_default() as graph:\n      # Reimport the graph and prune unconnected ops.\n      importer.import_graph_def(graph_def)\n      tpu.prune_unconnected_ops_from_xla(ops.get_default_graph())\n\n      # Verify that ops \"a\" and \"x\" still have TPU_REPLICATE_ATTR.\n      a = graph.get_operation_by_name(\"import/a\").get_attr(\n          tpu._TPU_REPLICATE_ATTR)\n      self.assertEqual(b\"0\", a)\n      x = graph.get_operation_by_name(\"import/x\").get_attr(\n          tpu._TPU_REPLICATE_ATTR)\n      self.assertEqual(b\"0\", x)\n      # Verify that ops \"b\" and \"y\" have TPU_REPLICATE_ATTR removed.\n      with self.assertRaisesRegex(\n          ValueError,\n          \"Operation \\'import/b\\' has no attr named \\'_tpu_replicate\\'\"):\n        graph.get_operation_by_name(\"import/b\").get_attr(\n            tpu._TPU_REPLICATE_ATTR)\n      with self.assertRaisesRegex(\n          ValueError,\n          \"Operation \\'import/y\\' has no attr named \\'_tpu_replicate\\'\"):\n        graph.get_operation_by_name(\"import/y\").get_attr(\n            tpu._TPU_REPLICATE_ATTR)\n\ndef do_einsum():\n  a = array_ops.placeholder(dtype=dtypes.float32, name=\"a\", shape=[2, 3, 4])\n  b = array_ops.placeholder(dtype=dtypes.float32, name=\"b\", shape=[2, 4, 5])\n  return special_math_ops.einsum(\"abc,acd->abd\", a, b)\n\n\ndef find_einsum(g):\n  graph_def = g.as_graph_def()\n  for node in graph_def.node:\n    if node.op == \"Einsum\":\n      return True\n  return False\n\n\ndef find_xla_einsum(g):\n  graph_def = g.as_graph_def()\n  for node in graph_def.node:\n    if node.op == \"XlaEinsum\":\n      return True\n  return False\n\n\nclass TPUXlaEinsumTest(test.TestCase):\n\n  def test_tpu_rewrite_uses_xla_einsum(self):\n    with ops.Graph().as_default() as g:\n      tpu.rewrite(do_einsum)\n      self.assertTrue(find_einsum(g) or find_xla_einsum(g))\n\n  def test_default_does_not_use_xla_einsum(self):\n    with ops.Graph().as_default() as g:\n      do_einsum()\n      self.assertFalse(find_xla_einsum(g))\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n\nnamespace tensorflow {\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\nREGISTER_OP(\"AllToAll\")\n    .Input(\"input: T\")\n    .Input(\"group_assignment: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: {numbertype, bool}\")\n    .Attr(\"concat_dimension: int\")\n    .Attr(\"split_dimension: int\")\n    .Attr(\"split_count: int\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle input = c->input(0);\n      ShapeHandle group_assignment = c->input(1);\n      if (!c->RankKnown(input)) {\n        c->set_output(0, c->UnknownShape());\n        return Status::OK();\n      }\n\n      int64_t rank = c->Rank(input);\n      int concat_dimension;\n      int split_dimension;\n      int split_count;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"split_count\", &split_count));\n      if (split_count < 1) {\n        return errors::InvalidArgument(\"split_count \", split_count,\n                                       \" must at least be one.\");\n      }\n      if (c->RankKnown(group_assignment) && c->Rank(group_assignment) != 2) {\n        return errors::InvalidArgument(\"group_assignment must have rank 2.\");\n      }\n      DimensionHandle num_replicas_per_group = c->Dim(group_assignment, 1);\n      if (c->ValueKnown(num_replicas_per_group) &&\n          (c->Value(num_replicas_per_group) != split_count)) {\n        return errors::InvalidArgument(\n            \"split_count \", split_count,\n            \" must equal the size of the second dimension of group_assignment \",\n            c->Value(num_replicas_per_group));\n      }\n\n      TF_RETURN_IF_ERROR(c->GetAttr(\"concat_dimension\", &concat_dimension));\n\n      if (concat_dimension < 0 || concat_dimension >= rank) {\n        return errors::InvalidArgument(\"concat_dimension \", concat_dimension,\n                                       \" is out of range of input rank \", rank);\n      }\n\n      TF_RETURN_IF_ERROR(c->GetAttr(\"split_dimension\", &split_dimension));\n      if (split_dimension < 0 || split_dimension >= rank) {\n        return errors::InvalidArgument(\"split_dimension \", split_dimension,\n                                       \" is out of range of input rank \", rank);\n      }\n\n      std::vector<DimensionHandle> dims;\n      dims.resize(rank);\n\n      for (int32_t i = 0; i < rank; ++i) {\n        dims[i] = c->Dim(input, i);\n        if (i == concat_dimension) {\n          dims[i] = c->MakeDim(c->Value(dims[i]) * split_count);\n        }\n        if (i == split_dimension) {\n          if (c->ValueKnown(dims[i]) &&\n              (c->Value(dims[i]) % split_count != 0)) {\n            return errors::InvalidArgument(\n                \"input dimension \", c->Value(dims[i]),\n                \" not divisible by split_count \", split_count);\n          }\n          dims[i] = c->MakeDim(c->Value(dims[i]) / split_count);\n        }\n      }\n\n      c->set_output(0, c->MakeShape(dims));\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CrossReplicaSum\")\n    .Input(\"input: T\")\n    .Input(\"group_assignment: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: {half, bfloat16, float, float64, int32, uint32}\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n\nREGISTER_OP(\"CollectivePermute\")\n    .Input(\"input: T\")\n    .Input(\"source_target_pairs: int32\")\n    .Output(\"output: T\")\n    .Attr(\"T: numbertype\")\n    .SetShapeFn(shape_inference::UnchangedShape);\n}  // namespace tensorflow\n", "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\n\n\"\"\"Tests for tpu_function helpers.\"\"\"\n\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import importer\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.layers import convolutional\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import control_flow_util\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import special_math_ops\nfrom tensorflow.python.ops import variable_scope\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.tpu import tpu\nfrom tensorflow.python.tpu import tpu_feed\nfrom tensorflow.python.tpu import training_loop\nfrom tensorflow.python.tpu.ops import tpu_ops\n\n\nclass TPUContextTest(test.TestCase):\n\n  def testIsInContext(self):\n    \"\"\"Test that control_flow_util can check that we're in a TPU context.\"\"\"\n    with ops.Graph().as_default():\n      z1 = array_ops.identity(1)\n      pivot = control_flow_ops.no_op()\n      context = tpu.TPUReplicateContext(b\"context\", 1, pivot=pivot)\n      context.Enter()\n      z2 = array_ops.identity(1)\n      context.Exit()\n      self.assertFalse(control_flow_util.IsInXLAContext(z1.op))\n      self.assertTrue(control_flow_util.IsInXLAContext(z2.op))\n\n  def testHandlesNameCollision(self):\n    \"\"\"Test AddValue handles name collisions for ops from different graphs.\"\"\"\n    with ops.Graph().as_default():\n      z = array_ops.zeros([2, 3], name=\"a\")\n      assert z.name == \"a:0\", \"Expected: a:0, Found: %s\" % z.name\n\n      @def_function.function\n      def f():\n        pivot = control_flow_ops.no_op()\n        context = tpu.TPUReplicateContext(b\"context\", 1, pivot=pivot)\n        context.Enter()\n        array_ops.identity(z)  # Capture z.\n        z1 = array_ops.zeros([3, 2], name=\"a\")\n        assert z1.name == \"a:0\", \"Expected: a:0, Found: %s\" % z1.name\n        z2 = array_ops.zeros([3, 2], name=\"a\")\n        # Prior to fixing b/166794533 this would fail with a shape mismatch\n        # because context.AddValue would have cached `z` by its name which\n        # collides with z1's name.\n        result = z1 + z2\n        context.Exit()\n        return result\n\n      f.get_concrete_function()\n\n\nclass TPULayerRewriteTest(test.TestCase):\n\n  def testUsingInfeedQueueWithRegularizer(self):\n    \"\"\"Test that Layer regularizers can reference data created in loops.\"\"\"\n\n    with ops.Graph().as_default():\n\n      def make_regularizer(scale):\n        def regularizer(inputs):\n          return scale * math_ops.reduce_sum(math_ops.square(inputs))\n        return regularizer\n\n      def training_step(inputs, scale):\n        outputs = convolutional.conv2d(\n            inputs,\n            filters=16,\n            kernel_size=(3, 3),\n            data_format=\"channels_first\",\n            kernel_regularizer=make_regularizer(scale))\n        loss = math_ops.reduce_mean(math_ops.square(outputs))\n        return loss.op\n\n      inputs = array_ops.zeros(shape=(128, 32, 32, 16))\n      scale = array_ops.ones(shape=())\n      infeed = tpu_feed.InfeedQueue(\n          tuple_types=[dtypes.float32, dtypes.float32],\n          tuple_shapes=[inputs.shape, scale.shape])\n\n      def loop():\n        return training_loop.repeat(5, training_step, infeed_queue=infeed)\n\n      # This should not throw an error.\n      tpu.rewrite(loop)\n\n\nclass TPUGraphPruneTest(test.TestCase):\n\n  def test_prune_unconnected_ops(self):\n    with ops.Graph().as_default():\n      a = array_ops.placeholder(dtype=dtypes.float32, name=\"a\")\n      b = array_ops.placeholder(dtype=dtypes.float32, name=\"b\")\n      constant_op.constant(1.0, name=\"constant\")\n      x = variable_scope.get_variable(\n          name=\"x\",\n          dtype=dtypes.float32,\n          shape=[],\n          use_resource=True,\n          initializer=init_ops.constant_initializer(2.0))\n      y = variable_scope.get_variable(\n          name=\"y\",\n          dtype=dtypes.float32,\n          shape=[],\n          use_resource=True,\n          initializer=init_ops.constant_initializer(3.0))\n      math_ops.add(a, b)\n      math_ops.add(x, y)\n      graph_def = ops.get_default_graph().as_graph_def()\n\n      for node in graph_def.node:\n        # Attach a TPU_REPLICATE_ATTR to each node.\n        node.attr[tpu._TPU_REPLICATE_ATTR].s = b\"0\"\n        # Rewire placeholder \"a\" and variable \"y\" leaving them unconnected.\n        for (input_index, node_input) in enumerate(node.input):\n          if node_input == \"b\":\n            node.input[input_index] = \"constant\"\n          if node_input == \"y\":\n            node.input[input_index] = \"x\"\n\n    with ops.Graph().as_default() as graph:\n      # Reimport the graph and prune unconnected ops.\n      importer.import_graph_def(graph_def)\n      tpu.prune_unconnected_ops_from_xla(ops.get_default_graph())\n\n      # Verify that ops \"a\" and \"x\" still have TPU_REPLICATE_ATTR.\n      a = graph.get_operation_by_name(\"import/a\").get_attr(\n          tpu._TPU_REPLICATE_ATTR)\n      self.assertEqual(b\"0\", a)\n      x = graph.get_operation_by_name(\"import/x\").get_attr(\n          tpu._TPU_REPLICATE_ATTR)\n      self.assertEqual(b\"0\", x)\n      # Verify that ops \"b\" and \"y\" have TPU_REPLICATE_ATTR removed.\n      with self.assertRaisesRegex(\n          ValueError,\n          \"Operation \\'import/b\\' has no attr named \\'_tpu_replicate\\'\"):\n        graph.get_operation_by_name(\"import/b\").get_attr(\n            tpu._TPU_REPLICATE_ATTR)\n      with self.assertRaisesRegex(\n          ValueError,\n          \"Operation \\'import/y\\' has no attr named \\'_tpu_replicate\\'\"):\n        graph.get_operation_by_name(\"import/y\").get_attr(\n            tpu._TPU_REPLICATE_ATTR)\n\n\nclass TPUOpsTest(test.TestCase):\n\n  def test_all_to_all_zero_split_count(self):\n    with self.assertRaisesRegex(\n        ValueError, \"split_count 0 must at least be one\"):\n      tpu_ops.all_to_all(\n          x=[0.0, 0.1652, 0.6543],\n          group_assignment=[1, -1],\n          concat_dimension=0,\n          split_dimension=0,\n          split_count=0)\n\n  def test_all_to_all_group_assignment_wrong_shape(self):\n    with self.assertRaisesRegex(\n        ValueError, \"group_assignment must have rank 2\"):\n      tpu_ops.all_to_all(\n          x=[0.0, 0.1652, 0.6543],\n          group_assignment=[1, -1],\n          concat_dimension=0,\n          split_dimension=0,\n          split_count=2)\n\n  def test_all_to_all_split_count_not_equal_to_group_assignment_shape(self):\n    with self.assertRaisesRegex(\n        ValueError, \"split_count 1 must equal the size of the second dimension \"\n        \"of group_assignment 2\"):\n      tpu_ops.all_to_all(\n          x=[0.0, 0.1652, 0.6543],\n          group_assignment=[[0, 1], [2, 3]],\n          concat_dimension=0,\n          split_dimension=0,\n          split_count=1)\n\n  def test_all_to_all_split_count_not_divide_input_shape(self):\n    with self.assertRaisesRegex(\n        ValueError, \"input dimension 3 not divisible by split_count 2\"):\n      tpu_ops.all_to_all(\n          x=[[0.0], [0.1652], [0.6543]],\n          group_assignment=[[0, 1], [2, 3]],\n          concat_dimension=1,\n          split_dimension=0,\n          split_count=2)\n\n\ndef do_einsum():\n  a = array_ops.placeholder(dtype=dtypes.float32, name=\"a\", shape=[2, 3, 4])\n  b = array_ops.placeholder(dtype=dtypes.float32, name=\"b\", shape=[2, 4, 5])\n  return special_math_ops.einsum(\"abc,acd->abd\", a, b)\n\n\ndef find_einsum(g):\n  graph_def = g.as_graph_def()\n  for node in graph_def.node:\n    if node.op == \"Einsum\":\n      return True\n  return False\n\n\ndef find_xla_einsum(g):\n  graph_def = g.as_graph_def()\n  for node in graph_def.node:\n    if node.op == \"XlaEinsum\":\n      return True\n  return False\n\n\nclass TPUXlaEinsumTest(test.TestCase):\n\n  def test_tpu_rewrite_uses_xla_einsum(self):\n    with ops.Graph().as_default() as g:\n      tpu.rewrite(do_einsum)\n      self.assertTrue(find_einsum(g) or find_xla_einsum(g))\n\n  def test_default_does_not_use_xla_einsum(self):\n    with ops.Graph().as_default() as g:\n      do_einsum()\n      self.assertFalse(find_xla_einsum(g))\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "filenames": ["tensorflow/core/ops/tpu_cross_replica_ops.cc", "tensorflow/python/tpu/tpu_test.py"], "buggy_code_start_loc": [34, 34], "buggy_code_end_loc": [67, 167], "fixing_code_start_loc": [35, 35], "fixing_code_end_loc": [90, 214], "type": "CWE-369", "message": "TensorFlow is an open source platform for machine learning. In affected versions the shape inference code for `AllToAll` can be made to execute a division by 0. This occurs whenever the `split_count` argument is 0. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-41218", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-05T22:15:08.667", "lastModified": "2021-11-09T18:43:36.637", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. In affected versions the shape inference code for `AllToAll` can be made to execute a division by 0. This occurs whenever the `split_count` argument is 0. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En las versiones afectadas, el c\u00f3digo de inferencia de formas para \"AllToAll\" puede hacer que se ejecute una divisi\u00f3n por 0. Esto ocurre siempre que el argumento \"split_count\" sea 0. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.7.0. Tambi\u00e9n ser\u00e1 incluida este commit en TensorFlow versi\u00f3n 2.6.1, TensorFlow versi\u00f3n 2.5.2, y TensorFlow versi\u00f3n 2.4.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-369"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.4", "matchCriteriaId": "0E596567-6F67-4880-8EC4-CB262BF02E0D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.5.0", "versionEndExcluding": "2.5.2", "matchCriteriaId": "035CDF63-1548-4FB4-B8A9-B8D328FAF910"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndExcluding": "2.6.1", "matchCriteriaId": "5D68D8D1-DB27-4395-9D3D-2BED901B852C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "A58EDA5C-66D6-46F1-962E-60AFB7C784A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "89522760-C2DF-400D-9624-626D8F160CBA"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/a8ad3e5e79c75f36edb81e0ba3f3c0c5442aeddc", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-9crf-c6qr-r273", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/a8ad3e5e79c75f36edb81e0ba3f3c0c5442aeddc"}}