{"buggy_code": ["/*\n * Copyright (C) 2001 Jens Axboe <axboe@suse.de>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n *\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n *\n */\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/module.h>\n#include <linux/blkdev.h>\n#include <linux/capability.h>\n#include <linux/completion.h>\n#include <linux/cdrom.h>\n#include <linux/slab.h>\n#include <linux/times.h>\n#include <asm/uaccess.h>\n\n#include <scsi/scsi.h>\n#include <scsi/scsi_ioctl.h>\n#include <scsi/scsi_cmnd.h>\n\nstruct blk_cmd_filter {\n\tunsigned long read_ok[BLK_SCSI_CMD_PER_LONG];\n\tunsigned long write_ok[BLK_SCSI_CMD_PER_LONG];\n};\n\nstatic struct blk_cmd_filter blk_default_cmd_filter;\n\n/* Command group 3 is reserved and should never be used.  */\nconst unsigned char scsi_command_size_tbl[8] =\n{\n\t6, 10, 10, 12,\n\t16, 12, 10, 10\n};\nEXPORT_SYMBOL(scsi_command_size_tbl);\n\n#include <scsi/sg.h>\n\nstatic int sg_get_version(int __user *p)\n{\n\tstatic const int sg_version_num = 30527;\n\treturn put_user(sg_version_num, p);\n}\n\nstatic int scsi_get_idlun(struct request_queue *q, int __user *p)\n{\n\treturn put_user(0, p);\n}\n\nstatic int scsi_get_bus(struct request_queue *q, int __user *p)\n{\n\treturn put_user(0, p);\n}\n\nstatic int sg_get_timeout(struct request_queue *q)\n{\n\treturn jiffies_to_clock_t(q->sg_timeout);\n}\n\nstatic int sg_set_timeout(struct request_queue *q, int __user *p)\n{\n\tint timeout, err = get_user(timeout, p);\n\n\tif (!err)\n\t\tq->sg_timeout = clock_t_to_jiffies(timeout);\n\n\treturn err;\n}\n\nstatic int sg_get_reserved_size(struct request_queue *q, int __user *p)\n{\n\tunsigned val = min(q->sg_reserved_size, queue_max_sectors(q) << 9);\n\n\treturn put_user(val, p);\n}\n\nstatic int sg_set_reserved_size(struct request_queue *q, int __user *p)\n{\n\tint size, err = get_user(size, p);\n\n\tif (err)\n\t\treturn err;\n\n\tif (size < 0)\n\t\treturn -EINVAL;\n\tif (size > (queue_max_sectors(q) << 9))\n\t\tsize = queue_max_sectors(q) << 9;\n\n\tq->sg_reserved_size = size;\n\treturn 0;\n}\n\n/*\n * will always return that we are ATAPI even for a real SCSI drive, I'm not\n * so sure this is worth doing anything about (why would you care??)\n */\nstatic int sg_emulated_host(struct request_queue *q, int __user *p)\n{\n\treturn put_user(1, p);\n}\n\nstatic void blk_set_cmd_filter_defaults(struct blk_cmd_filter *filter)\n{\n\t/* Basic read-only commands */\n\t__set_bit(TEST_UNIT_READY, filter->read_ok);\n\t__set_bit(REQUEST_SENSE, filter->read_ok);\n\t__set_bit(READ_6, filter->read_ok);\n\t__set_bit(READ_10, filter->read_ok);\n\t__set_bit(READ_12, filter->read_ok);\n\t__set_bit(READ_16, filter->read_ok);\n\t__set_bit(READ_BUFFER, filter->read_ok);\n\t__set_bit(READ_DEFECT_DATA, filter->read_ok);\n\t__set_bit(READ_CAPACITY, filter->read_ok);\n\t__set_bit(READ_LONG, filter->read_ok);\n\t__set_bit(INQUIRY, filter->read_ok);\n\t__set_bit(MODE_SENSE, filter->read_ok);\n\t__set_bit(MODE_SENSE_10, filter->read_ok);\n\t__set_bit(LOG_SENSE, filter->read_ok);\n\t__set_bit(START_STOP, filter->read_ok);\n\t__set_bit(GPCMD_VERIFY_10, filter->read_ok);\n\t__set_bit(VERIFY_16, filter->read_ok);\n\t__set_bit(REPORT_LUNS, filter->read_ok);\n\t__set_bit(SERVICE_ACTION_IN, filter->read_ok);\n\t__set_bit(RECEIVE_DIAGNOSTIC, filter->read_ok);\n\t__set_bit(MAINTENANCE_IN, filter->read_ok);\n\t__set_bit(GPCMD_READ_BUFFER_CAPACITY, filter->read_ok);\n\n\t/* Audio CD commands */\n\t__set_bit(GPCMD_PLAY_CD, filter->read_ok);\n\t__set_bit(GPCMD_PLAY_AUDIO_10, filter->read_ok);\n\t__set_bit(GPCMD_PLAY_AUDIO_MSF, filter->read_ok);\n\t__set_bit(GPCMD_PLAY_AUDIO_TI, filter->read_ok);\n\t__set_bit(GPCMD_PAUSE_RESUME, filter->read_ok);\n\n\t/* CD/DVD data reading */\n\t__set_bit(GPCMD_READ_CD, filter->read_ok);\n\t__set_bit(GPCMD_READ_CD_MSF, filter->read_ok);\n\t__set_bit(GPCMD_READ_DISC_INFO, filter->read_ok);\n\t__set_bit(GPCMD_READ_CDVD_CAPACITY, filter->read_ok);\n\t__set_bit(GPCMD_READ_DVD_STRUCTURE, filter->read_ok);\n\t__set_bit(GPCMD_READ_HEADER, filter->read_ok);\n\t__set_bit(GPCMD_READ_TRACK_RZONE_INFO, filter->read_ok);\n\t__set_bit(GPCMD_READ_SUBCHANNEL, filter->read_ok);\n\t__set_bit(GPCMD_READ_TOC_PMA_ATIP, filter->read_ok);\n\t__set_bit(GPCMD_REPORT_KEY, filter->read_ok);\n\t__set_bit(GPCMD_SCAN, filter->read_ok);\n\t__set_bit(GPCMD_GET_CONFIGURATION, filter->read_ok);\n\t__set_bit(GPCMD_READ_FORMAT_CAPACITIES, filter->read_ok);\n\t__set_bit(GPCMD_GET_EVENT_STATUS_NOTIFICATION, filter->read_ok);\n\t__set_bit(GPCMD_GET_PERFORMANCE, filter->read_ok);\n\t__set_bit(GPCMD_SEEK, filter->read_ok);\n\t__set_bit(GPCMD_STOP_PLAY_SCAN, filter->read_ok);\n\n\t/* Basic writing commands */\n\t__set_bit(WRITE_6, filter->write_ok);\n\t__set_bit(WRITE_10, filter->write_ok);\n\t__set_bit(WRITE_VERIFY, filter->write_ok);\n\t__set_bit(WRITE_12, filter->write_ok);\n\t__set_bit(WRITE_VERIFY_12, filter->write_ok);\n\t__set_bit(WRITE_16, filter->write_ok);\n\t__set_bit(WRITE_LONG, filter->write_ok);\n\t__set_bit(WRITE_LONG_2, filter->write_ok);\n\t__set_bit(ERASE, filter->write_ok);\n\t__set_bit(GPCMD_MODE_SELECT_10, filter->write_ok);\n\t__set_bit(MODE_SELECT, filter->write_ok);\n\t__set_bit(LOG_SELECT, filter->write_ok);\n\t__set_bit(GPCMD_BLANK, filter->write_ok);\n\t__set_bit(GPCMD_CLOSE_TRACK, filter->write_ok);\n\t__set_bit(GPCMD_FLUSH_CACHE, filter->write_ok);\n\t__set_bit(GPCMD_FORMAT_UNIT, filter->write_ok);\n\t__set_bit(GPCMD_REPAIR_RZONE_TRACK, filter->write_ok);\n\t__set_bit(GPCMD_RESERVE_RZONE_TRACK, filter->write_ok);\n\t__set_bit(GPCMD_SEND_DVD_STRUCTURE, filter->write_ok);\n\t__set_bit(GPCMD_SEND_EVENT, filter->write_ok);\n\t__set_bit(GPCMD_SEND_KEY, filter->write_ok);\n\t__set_bit(GPCMD_SEND_OPC, filter->write_ok);\n\t__set_bit(GPCMD_SEND_CUE_SHEET, filter->write_ok);\n\t__set_bit(GPCMD_SET_SPEED, filter->write_ok);\n\t__set_bit(GPCMD_PREVENT_ALLOW_MEDIUM_REMOVAL, filter->write_ok);\n\t__set_bit(GPCMD_LOAD_UNLOAD, filter->write_ok);\n\t__set_bit(GPCMD_SET_STREAMING, filter->write_ok);\n\t__set_bit(GPCMD_SET_READ_AHEAD, filter->write_ok);\n}\n\nint blk_verify_command(unsigned char *cmd, fmode_t has_write_perm)\n{\n\tstruct blk_cmd_filter *filter = &blk_default_cmd_filter;\n\n\t/* root can do any command. */\n\tif (capable(CAP_SYS_RAWIO))\n\t\treturn 0;\n\n\t/* if there's no filter set, assume we're filtering everything out */\n\tif (!filter)\n\t\treturn -EPERM;\n\n\t/* Anybody who can open the device can do a read-safe command */\n\tif (test_bit(cmd[0], filter->read_ok))\n\t\treturn 0;\n\n\t/* Write-safe commands require a writable open */\n\tif (test_bit(cmd[0], filter->write_ok) && has_write_perm)\n\t\treturn 0;\n\n\treturn -EPERM;\n}\nEXPORT_SYMBOL(blk_verify_command);\n\nstatic int blk_fill_sghdr_rq(struct request_queue *q, struct request *rq,\n\t\t\t     struct sg_io_hdr *hdr, fmode_t mode)\n{\n\tif (copy_from_user(rq->cmd, hdr->cmdp, hdr->cmd_len))\n\t\treturn -EFAULT;\n\tif (blk_verify_command(rq->cmd, mode & FMODE_WRITE))\n\t\treturn -EPERM;\n\n\t/*\n\t * fill in request structure\n\t */\n\trq->cmd_len = hdr->cmd_len;\n\trq->cmd_type = REQ_TYPE_BLOCK_PC;\n\n\trq->timeout = msecs_to_jiffies(hdr->timeout);\n\tif (!rq->timeout)\n\t\trq->timeout = q->sg_timeout;\n\tif (!rq->timeout)\n\t\trq->timeout = BLK_DEFAULT_SG_TIMEOUT;\n\tif (rq->timeout < BLK_MIN_SG_TIMEOUT)\n\t\trq->timeout = BLK_MIN_SG_TIMEOUT;\n\n\treturn 0;\n}\n\nstatic int blk_complete_sghdr_rq(struct request *rq, struct sg_io_hdr *hdr,\n\t\t\t\t struct bio *bio)\n{\n\tint r, ret = 0;\n\n\t/*\n\t * fill in all the output members\n\t */\n\thdr->status = rq->errors & 0xff;\n\thdr->masked_status = status_byte(rq->errors);\n\thdr->msg_status = msg_byte(rq->errors);\n\thdr->host_status = host_byte(rq->errors);\n\thdr->driver_status = driver_byte(rq->errors);\n\thdr->info = 0;\n\tif (hdr->masked_status || hdr->host_status || hdr->driver_status)\n\t\thdr->info |= SG_INFO_CHECK;\n\thdr->resid = rq->resid_len;\n\thdr->sb_len_wr = 0;\n\n\tif (rq->sense_len && hdr->sbp) {\n\t\tint len = min((unsigned int) hdr->mx_sb_len, rq->sense_len);\n\n\t\tif (!copy_to_user(hdr->sbp, rq->sense, len))\n\t\t\thdr->sb_len_wr = len;\n\t\telse\n\t\t\tret = -EFAULT;\n\t}\n\n\tr = blk_rq_unmap_user(bio);\n\tif (!ret)\n\t\tret = r;\n\tblk_put_request(rq);\n\n\treturn ret;\n}\n\nstatic int sg_io(struct request_queue *q, struct gendisk *bd_disk,\n\t\tstruct sg_io_hdr *hdr, fmode_t mode)\n{\n\tunsigned long start_time;\n\tint writing = 0, ret = 0;\n\tstruct request *rq;\n\tchar sense[SCSI_SENSE_BUFFERSIZE];\n\tstruct bio *bio;\n\n\tif (hdr->interface_id != 'S')\n\t\treturn -EINVAL;\n\tif (hdr->cmd_len > BLK_MAX_CDB)\n\t\treturn -EINVAL;\n\n\tif (hdr->dxfer_len > (queue_max_hw_sectors(q) << 9))\n\t\treturn -EIO;\n\n\tif (hdr->dxfer_len)\n\t\tswitch (hdr->dxfer_direction) {\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\tcase SG_DXFER_TO_DEV:\n\t\t\twriting = 1;\n\t\t\tbreak;\n\t\tcase SG_DXFER_TO_FROM_DEV:\n\t\tcase SG_DXFER_FROM_DEV:\n\t\t\tbreak;\n\t\t}\n\n\trq = blk_get_request(q, writing ? WRITE : READ, GFP_KERNEL);\n\tif (!rq)\n\t\treturn -ENOMEM;\n\n\tif (blk_fill_sghdr_rq(q, rq, hdr, mode)) {\n\t\tblk_put_request(rq);\n\t\treturn -EFAULT;\n\t}\n\n\tif (hdr->iovec_count) {\n\t\tconst int size = sizeof(struct sg_iovec) * hdr->iovec_count;\n\t\tsize_t iov_data_len;\n\t\tstruct sg_iovec *sg_iov;\n\t\tstruct iovec *iov;\n\t\tint i;\n\n\t\tsg_iov = kmalloc(size, GFP_KERNEL);\n\t\tif (!sg_iov) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (copy_from_user(sg_iov, hdr->dxferp, size)) {\n\t\t\tkfree(sg_iov);\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * Sum up the vecs, making sure they don't overflow\n\t\t */\n\t\tiov = (struct iovec *) sg_iov;\n\t\tiov_data_len = 0;\n\t\tfor (i = 0; i < hdr->iovec_count; i++) {\n\t\t\tif (iov_data_len + iov[i].iov_len < iov_data_len) {\n\t\t\t\tkfree(sg_iov);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tiov_data_len += iov[i].iov_len;\n\t\t}\n\n\t\t/* SG_IO howto says that the shorter of the two wins */\n\t\tif (hdr->dxfer_len < iov_data_len) {\n\t\t\thdr->iovec_count = iov_shorten(iov,\n\t\t\t\t\t\t       hdr->iovec_count,\n\t\t\t\t\t\t       hdr->dxfer_len);\n\t\t\tiov_data_len = hdr->dxfer_len;\n\t\t}\n\n\t\tret = blk_rq_map_user_iov(q, rq, NULL, sg_iov, hdr->iovec_count,\n\t\t\t\t\t  iov_data_len, GFP_KERNEL);\n\t\tkfree(sg_iov);\n\t} else if (hdr->dxfer_len)\n\t\tret = blk_rq_map_user(q, rq, NULL, hdr->dxferp, hdr->dxfer_len,\n\t\t\t\t      GFP_KERNEL);\n\n\tif (ret)\n\t\tgoto out;\n\n\tbio = rq->bio;\n\tmemset(sense, 0, sizeof(sense));\n\trq->sense = sense;\n\trq->sense_len = 0;\n\trq->retries = 0;\n\n\tstart_time = jiffies;\n\n\t/* ignore return value. All information is passed back to caller\n\t * (if he doesn't check that is his problem).\n\t * N.B. a non-zero SCSI status is _not_ necessarily an error.\n\t */\n\tblk_execute_rq(q, bd_disk, rq, 0);\n\n\thdr->duration = jiffies_to_msecs(jiffies - start_time);\n\n\treturn blk_complete_sghdr_rq(rq, hdr, bio);\nout:\n\tblk_put_request(rq);\n\treturn ret;\n}\n\n/**\n * sg_scsi_ioctl  --  handle deprecated SCSI_IOCTL_SEND_COMMAND ioctl\n * @file:\tfile this ioctl operates on (optional)\n * @q:\t\trequest queue to send scsi commands down\n * @disk:\tgendisk to operate on (option)\n * @sic:\tuserspace structure describing the command to perform\n *\n * Send down the scsi command described by @sic to the device below\n * the request queue @q.  If @file is non-NULL it's used to perform\n * fine-grained permission checks that allow users to send down\n * non-destructive SCSI commands.  If the caller has a struct gendisk\n * available it should be passed in as @disk to allow the low level\n * driver to use the information contained in it.  A non-NULL @disk\n * is only allowed if the caller knows that the low level driver doesn't\n * need it (e.g. in the scsi subsystem).\n *\n * Notes:\n *   -  This interface is deprecated - users should use the SG_IO\n *      interface instead, as this is a more flexible approach to\n *      performing SCSI commands on a device.\n *   -  The SCSI command length is determined by examining the 1st byte\n *      of the given command. There is no way to override this.\n *   -  Data transfers are limited to PAGE_SIZE\n *   -  The length (x + y) must be at least OMAX_SB_LEN bytes long to\n *      accommodate the sense buffer when an error occurs.\n *      The sense buffer is truncated to OMAX_SB_LEN (16) bytes so that\n *      old code will not be surprised.\n *   -  If a Unix error occurs (e.g. ENOMEM) then the user will receive\n *      a negative return and the Unix error code in 'errno'.\n *      If the SCSI command succeeds then 0 is returned.\n *      Positive numbers returned are the compacted SCSI error codes (4\n *      bytes in one int) where the lowest byte is the SCSI status.\n */\n#define OMAX_SB_LEN 16          /* For backward compatibility */\nint sg_scsi_ioctl(struct request_queue *q, struct gendisk *disk, fmode_t mode,\n\t\tstruct scsi_ioctl_command __user *sic)\n{\n\tstruct request *rq;\n\tint err;\n\tunsigned int in_len, out_len, bytes, opcode, cmdlen;\n\tchar *buffer = NULL, sense[SCSI_SENSE_BUFFERSIZE];\n\n\tif (!sic)\n\t\treturn -EINVAL;\n\n\t/*\n\t * get in an out lengths, verify they don't exceed a page worth of data\n\t */\n\tif (get_user(in_len, &sic->inlen))\n\t\treturn -EFAULT;\n\tif (get_user(out_len, &sic->outlen))\n\t\treturn -EFAULT;\n\tif (in_len > PAGE_SIZE || out_len > PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (get_user(opcode, sic->data))\n\t\treturn -EFAULT;\n\n\tbytes = max(in_len, out_len);\n\tif (bytes) {\n\t\tbuffer = kzalloc(bytes, q->bounce_gfp | GFP_USER| __GFP_NOWARN);\n\t\tif (!buffer)\n\t\t\treturn -ENOMEM;\n\n\t}\n\n\trq = blk_get_request(q, in_len ? WRITE : READ, __GFP_WAIT);\n\n\tcmdlen = COMMAND_SIZE(opcode);\n\n\t/*\n\t * get command and data to send to device, if any\n\t */\n\terr = -EFAULT;\n\trq->cmd_len = cmdlen;\n\tif (copy_from_user(rq->cmd, sic->data, cmdlen))\n\t\tgoto error;\n\n\tif (in_len && copy_from_user(buffer, sic->data + cmdlen, in_len))\n\t\tgoto error;\n\n\terr = blk_verify_command(rq->cmd, mode & FMODE_WRITE);\n\tif (err)\n\t\tgoto error;\n\n\t/* default.  possible overriden later */\n\trq->retries = 5;\n\n\tswitch (opcode) {\n\tcase SEND_DIAGNOSTIC:\n\tcase FORMAT_UNIT:\n\t\trq->timeout = FORMAT_UNIT_TIMEOUT;\n\t\trq->retries = 1;\n\t\tbreak;\n\tcase START_STOP:\n\t\trq->timeout = START_STOP_TIMEOUT;\n\t\tbreak;\n\tcase MOVE_MEDIUM:\n\t\trq->timeout = MOVE_MEDIUM_TIMEOUT;\n\t\tbreak;\n\tcase READ_ELEMENT_STATUS:\n\t\trq->timeout = READ_ELEMENT_STATUS_TIMEOUT;\n\t\tbreak;\n\tcase READ_DEFECT_DATA:\n\t\trq->timeout = READ_DEFECT_DATA_TIMEOUT;\n\t\trq->retries = 1;\n\t\tbreak;\n\tdefault:\n\t\trq->timeout = BLK_DEFAULT_SG_TIMEOUT;\n\t\tbreak;\n\t}\n\n\tif (bytes && blk_rq_map_kern(q, rq, buffer, bytes, __GFP_WAIT)) {\n\t\terr = DRIVER_ERROR << 24;\n\t\tgoto out;\n\t}\n\n\tmemset(sense, 0, sizeof(sense));\n\trq->sense = sense;\n\trq->sense_len = 0;\n\trq->cmd_type = REQ_TYPE_BLOCK_PC;\n\n\tblk_execute_rq(q, disk, rq, 0);\n\nout:\n\terr = rq->errors & 0xff;\t/* only 8 bit SCSI status */\n\tif (err) {\n\t\tif (rq->sense_len && rq->sense) {\n\t\t\tbytes = (OMAX_SB_LEN > rq->sense_len) ?\n\t\t\t\trq->sense_len : OMAX_SB_LEN;\n\t\t\tif (copy_to_user(sic->data, rq->sense, bytes))\n\t\t\t\terr = -EFAULT;\n\t\t}\n\t} else {\n\t\tif (copy_to_user(sic->data, buffer, out_len))\n\t\t\terr = -EFAULT;\n\t}\n\t\nerror:\n\tkfree(buffer);\n\tblk_put_request(rq);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(sg_scsi_ioctl);\n\n/* Send basic block requests */\nstatic int __blk_send_generic(struct request_queue *q, struct gendisk *bd_disk,\n\t\t\t      int cmd, int data)\n{\n\tstruct request *rq;\n\tint err;\n\n\trq = blk_get_request(q, WRITE, __GFP_WAIT);\n\trq->cmd_type = REQ_TYPE_BLOCK_PC;\n\trq->timeout = BLK_DEFAULT_SG_TIMEOUT;\n\trq->cmd[0] = cmd;\n\trq->cmd[4] = data;\n\trq->cmd_len = 6;\n\terr = blk_execute_rq(q, bd_disk, rq, 0);\n\tblk_put_request(rq);\n\n\treturn err;\n}\n\nstatic inline int blk_send_start_stop(struct request_queue *q,\n\t\t\t\t      struct gendisk *bd_disk, int data)\n{\n\treturn __blk_send_generic(q, bd_disk, GPCMD_START_STOP_UNIT, data);\n}\n\nint scsi_cmd_ioctl(struct request_queue *q, struct gendisk *bd_disk, fmode_t mode,\n\t\t   unsigned int cmd, void __user *arg)\n{\n\tint err;\n\n\tif (!q)\n\t\treturn -ENXIO;\n\n\tswitch (cmd) {\n\t\t/*\n\t\t * new sgv3 interface\n\t\t */\n\t\tcase SG_GET_VERSION_NUM:\n\t\t\terr = sg_get_version(arg);\n\t\t\tbreak;\n\t\tcase SCSI_IOCTL_GET_IDLUN:\n\t\t\terr = scsi_get_idlun(q, arg);\n\t\t\tbreak;\n\t\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\t\t\terr = scsi_get_bus(q, arg);\n\t\t\tbreak;\n\t\tcase SG_SET_TIMEOUT:\n\t\t\terr = sg_set_timeout(q, arg);\n\t\t\tbreak;\n\t\tcase SG_GET_TIMEOUT:\n\t\t\terr = sg_get_timeout(q);\n\t\t\tbreak;\n\t\tcase SG_GET_RESERVED_SIZE:\n\t\t\terr = sg_get_reserved_size(q, arg);\n\t\t\tbreak;\n\t\tcase SG_SET_RESERVED_SIZE:\n\t\t\terr = sg_set_reserved_size(q, arg);\n\t\t\tbreak;\n\t\tcase SG_EMULATED_HOST:\n\t\t\terr = sg_emulated_host(q, arg);\n\t\t\tbreak;\n\t\tcase SG_IO: {\n\t\t\tstruct sg_io_hdr hdr;\n\n\t\t\terr = -EFAULT;\n\t\t\tif (copy_from_user(&hdr, arg, sizeof(hdr)))\n\t\t\t\tbreak;\n\t\t\terr = sg_io(q, bd_disk, &hdr, mode);\n\t\t\tif (err == -EFAULT)\n\t\t\t\tbreak;\n\n\t\t\tif (copy_to_user(arg, &hdr, sizeof(hdr)))\n\t\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcase CDROM_SEND_PACKET: {\n\t\t\tstruct cdrom_generic_command cgc;\n\t\t\tstruct sg_io_hdr hdr;\n\n\t\t\terr = -EFAULT;\n\t\t\tif (copy_from_user(&cgc, arg, sizeof(cgc)))\n\t\t\t\tbreak;\n\t\t\tcgc.timeout = clock_t_to_jiffies(cgc.timeout);\n\t\t\tmemset(&hdr, 0, sizeof(hdr));\n\t\t\thdr.interface_id = 'S';\n\t\t\thdr.cmd_len = sizeof(cgc.cmd);\n\t\t\thdr.dxfer_len = cgc.buflen;\n\t\t\terr = 0;\n\t\t\tswitch (cgc.data_direction) {\n\t\t\t\tcase CGC_DATA_UNKNOWN:\n\t\t\t\t\thdr.dxfer_direction = SG_DXFER_UNKNOWN;\n\t\t\t\t\tbreak;\n\t\t\t\tcase CGC_DATA_WRITE:\n\t\t\t\t\thdr.dxfer_direction = SG_DXFER_TO_DEV;\n\t\t\t\t\tbreak;\n\t\t\t\tcase CGC_DATA_READ:\n\t\t\t\t\thdr.dxfer_direction = SG_DXFER_FROM_DEV;\n\t\t\t\t\tbreak;\n\t\t\t\tcase CGC_DATA_NONE:\n\t\t\t\t\thdr.dxfer_direction = SG_DXFER_NONE;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\terr = -EINVAL;\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tbreak;\n\n\t\t\thdr.dxferp = cgc.buffer;\n\t\t\thdr.sbp = cgc.sense;\n\t\t\tif (hdr.sbp)\n\t\t\t\thdr.mx_sb_len = sizeof(struct request_sense);\n\t\t\thdr.timeout = jiffies_to_msecs(cgc.timeout);\n\t\t\thdr.cmdp = ((struct cdrom_generic_command __user*) arg)->cmd;\n\t\t\thdr.cmd_len = sizeof(cgc.cmd);\n\n\t\t\terr = sg_io(q, bd_disk, &hdr, mode);\n\t\t\tif (err == -EFAULT)\n\t\t\t\tbreak;\n\n\t\t\tif (hdr.status)\n\t\t\t\terr = -EIO;\n\n\t\t\tcgc.stat = err;\n\t\t\tcgc.buflen = hdr.resid;\n\t\t\tif (copy_to_user(arg, &cgc, sizeof(cgc)))\n\t\t\t\terr = -EFAULT;\n\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * old junk scsi send command ioctl\n\t\t */\n\t\tcase SCSI_IOCTL_SEND_COMMAND:\n\t\t\tprintk(KERN_WARNING \"program %s is using a deprecated SCSI ioctl, please convert it to SG_IO\\n\", current->comm);\n\t\t\terr = -EINVAL;\n\t\t\tif (!arg)\n\t\t\t\tbreak;\n\n\t\t\terr = sg_scsi_ioctl(q, bd_disk, mode, arg);\n\t\t\tbreak;\n\t\tcase CDROMCLOSETRAY:\n\t\t\terr = blk_send_start_stop(q, bd_disk, 0x03);\n\t\t\tbreak;\n\t\tcase CDROMEJECT:\n\t\t\terr = blk_send_start_stop(q, bd_disk, 0x02);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -ENOTTY;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(scsi_cmd_ioctl);\n\nint scsi_cmd_blk_ioctl(struct block_device *bd, fmode_t mode,\n\t\t       unsigned int cmd, void __user *arg)\n{\n\treturn scsi_cmd_ioctl(bd->bd_disk->queue, bd->bd_disk, mode, cmd, arg);\n}\nEXPORT_SYMBOL(scsi_cmd_blk_ioctl);\n\nstatic int __init blk_scsi_ioctl_init(void)\n{\n\tblk_set_cmd_filter_defaults(&blk_default_cmd_filter);\n\treturn 0;\n}\nfs_initcall(blk_scsi_ioctl_init);\n", "/*\n *      sd.c Copyright (C) 1992 Drew Eckhardt\n *           Copyright (C) 1993, 1994, 1995, 1999 Eric Youngdale\n *\n *      Linux scsi disk driver\n *              Initial versions: Drew Eckhardt\n *              Subsequent revisions: Eric Youngdale\n *\tModification history:\n *       - Drew Eckhardt <drew@colorado.edu> original\n *       - Eric Youngdale <eric@andante.org> add scatter-gather, multiple \n *         outstanding request, and other enhancements.\n *         Support loadable low-level scsi drivers.\n *       - Jirka Hanika <geo@ff.cuni.cz> support more scsi disks using \n *         eight major numbers.\n *       - Richard Gooch <rgooch@atnf.csiro.au> support devfs.\n *\t - Torben Mathiasen <tmm@image.dk> Resource allocation fixes in \n *\t   sd_init and cleanups.\n *\t - Alex Davis <letmein@erols.com> Fix problem where partition info\n *\t   not being read in sd_open. Fix problem where removable media \n *\t   could be ejected after sd_open.\n *\t - Douglas Gilbert <dgilbert@interlog.com> cleanup for lk 2.5.x\n *\t - Badari Pulavarty <pbadari@us.ibm.com>, Matthew Wilcox \n *\t   <willy@debian.org>, Kurt Garloff <garloff@suse.de>: \n *\t   Support 32k/1M disks.\n *\n *\tLogging policy (needs CONFIG_SCSI_LOGGING defined):\n *\t - setting up transfer: SCSI_LOG_HLQUEUE levels 1 and 2\n *\t - end of transfer (bh + scsi_lib): SCSI_LOG_HLCOMPLETE level 1\n *\t - entering sd_ioctl: SCSI_LOG_IOCTL level 1\n *\t - entering other commands: SCSI_LOG_HLQUEUE level 3\n *\tNote: when the logging level is set by the user, it must be greater\n *\tthan the level indicated above to trigger output.\t\n */\n\n#include <linux/module.h>\n#include <linux/fs.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/bio.h>\n#include <linux/genhd.h>\n#include <linux/hdreg.h>\n#include <linux/errno.h>\n#include <linux/idr.h>\n#include <linux/interrupt.h>\n#include <linux/init.h>\n#include <linux/blkdev.h>\n#include <linux/blkpg.h>\n#include <linux/delay.h>\n#include <linux/mutex.h>\n#include <linux/string_helpers.h>\n#include <linux/async.h>\n#include <linux/slab.h>\n#include <linux/pm_runtime.h>\n#include <asm/uaccess.h>\n#include <asm/unaligned.h>\n\n#include <scsi/scsi.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_dbg.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_driver.h>\n#include <scsi/scsi_eh.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_ioctl.h>\n#include <scsi/scsicam.h>\n\n#include \"sd.h\"\n#include \"scsi_logging.h\"\n\nMODULE_AUTHOR(\"Eric Youngdale\");\nMODULE_DESCRIPTION(\"SCSI disk (sd) driver\");\nMODULE_LICENSE(\"GPL\");\n\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK0_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK1_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK2_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK3_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK4_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK5_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK6_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK7_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK8_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK9_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK10_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK11_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK12_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK13_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK14_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK15_MAJOR);\nMODULE_ALIAS_SCSI_DEVICE(TYPE_DISK);\nMODULE_ALIAS_SCSI_DEVICE(TYPE_MOD);\nMODULE_ALIAS_SCSI_DEVICE(TYPE_RBC);\n\n#if !defined(CONFIG_DEBUG_BLOCK_EXT_DEVT)\n#define SD_MINORS\t16\n#else\n#define SD_MINORS\t0\n#endif\n\nstatic void sd_config_discard(struct scsi_disk *, unsigned int);\nstatic int  sd_revalidate_disk(struct gendisk *);\nstatic void sd_unlock_native_capacity(struct gendisk *disk);\nstatic int  sd_probe(struct device *);\nstatic int  sd_remove(struct device *);\nstatic void sd_shutdown(struct device *);\nstatic int sd_suspend(struct device *, pm_message_t state);\nstatic int sd_resume(struct device *);\nstatic void sd_rescan(struct device *);\nstatic int sd_done(struct scsi_cmnd *);\nstatic void sd_read_capacity(struct scsi_disk *sdkp, unsigned char *buffer);\nstatic void scsi_disk_release(struct device *cdev);\nstatic void sd_print_sense_hdr(struct scsi_disk *, struct scsi_sense_hdr *);\nstatic void sd_print_result(struct scsi_disk *, int);\n\nstatic DEFINE_SPINLOCK(sd_index_lock);\nstatic DEFINE_IDA(sd_index_ida);\n\n/* This semaphore is used to mediate the 0->1 reference get in the\n * face of object destruction (i.e. we can't allow a get on an\n * object after last put) */\nstatic DEFINE_MUTEX(sd_ref_mutex);\n\nstatic struct kmem_cache *sd_cdb_cache;\nstatic mempool_t *sd_cdb_pool;\n\nstatic const char *sd_cache_types[] = {\n\t\"write through\", \"none\", \"write back\",\n\t\"write back, no read (daft)\"\n};\n\nstatic ssize_t\nsd_store_cache_type(struct device *dev, struct device_attribute *attr,\n\t\t    const char *buf, size_t count)\n{\n\tint i, ct = -1, rcd, wce, sp;\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\tchar buffer[64];\n\tchar *buffer_data;\n\tstruct scsi_mode_data data;\n\tstruct scsi_sense_hdr sshdr;\n\tint len;\n\n\tif (sdp->type != TYPE_DISK)\n\t\t/* no cache control on RBC devices; theoretically they\n\t\t * can do it, but there's probably so many exceptions\n\t\t * it's not worth the risk */\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < ARRAY_SIZE(sd_cache_types); i++) {\n\t\tlen = strlen(sd_cache_types[i]);\n\t\tif (strncmp(sd_cache_types[i], buf, len) == 0 &&\n\t\t    buf[len] == '\\n') {\n\t\t\tct = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (ct < 0)\n\t\treturn -EINVAL;\n\trcd = ct & 0x01 ? 1 : 0;\n\twce = ct & 0x02 ? 1 : 0;\n\tif (scsi_mode_sense(sdp, 0x08, 8, buffer, sizeof(buffer), SD_TIMEOUT,\n\t\t\t    SD_MAX_RETRIES, &data, NULL))\n\t\treturn -EINVAL;\n\tlen = min_t(size_t, sizeof(buffer), data.length - data.header_length -\n\t\t  data.block_descriptor_length);\n\tbuffer_data = buffer + data.header_length +\n\t\tdata.block_descriptor_length;\n\tbuffer_data[2] &= ~0x05;\n\tbuffer_data[2] |= wce << 2 | rcd;\n\tsp = buffer_data[0] & 0x80 ? 1 : 0;\n\n\tif (scsi_mode_select(sdp, 1, sp, 8, buffer_data, len, SD_TIMEOUT,\n\t\t\t     SD_MAX_RETRIES, &data, &sshdr)) {\n\t\tif (scsi_sense_valid(&sshdr))\n\t\t\tsd_print_sense_hdr(sdkp, &sshdr);\n\t\treturn -EINVAL;\n\t}\n\trevalidate_disk(sdkp->disk);\n\treturn count;\n}\n\nstatic ssize_t\nsd_store_manage_start_stop(struct device *dev, struct device_attribute *attr,\n\t\t\t   const char *buf, size_t count)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tsdp->manage_start_stop = simple_strtoul(buf, NULL, 10);\n\n\treturn count;\n}\n\nstatic ssize_t\nsd_store_allow_restart(struct device *dev, struct device_attribute *attr,\n\t\t       const char *buf, size_t count)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tif (sdp->type != TYPE_DISK)\n\t\treturn -EINVAL;\n\n\tsdp->allow_restart = simple_strtoul(buf, NULL, 10);\n\n\treturn count;\n}\n\nstatic ssize_t\nsd_show_cache_type(struct device *dev, struct device_attribute *attr,\n\t\t   char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tint ct = sdkp->RCD + 2*sdkp->WCE;\n\n\treturn snprintf(buf, 40, \"%s\\n\", sd_cache_types[ct]);\n}\n\nstatic ssize_t\nsd_show_fua(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 20, \"%u\\n\", sdkp->DPOFUA);\n}\n\nstatic ssize_t\nsd_show_manage_start_stop(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\n\treturn snprintf(buf, 20, \"%u\\n\", sdp->manage_start_stop);\n}\n\nstatic ssize_t\nsd_show_allow_restart(struct device *dev, struct device_attribute *attr,\n\t\t      char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 40, \"%d\\n\", sdkp->device->allow_restart);\n}\n\nstatic ssize_t\nsd_show_protection_type(struct device *dev, struct device_attribute *attr,\n\t\t\tchar *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 20, \"%u\\n\", sdkp->protection_type);\n}\n\nstatic ssize_t\nsd_show_protection_mode(struct device *dev, struct device_attribute *attr,\n\t\t\tchar *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\tunsigned int dif, dix;\n\n\tdif = scsi_host_dif_capable(sdp->host, sdkp->protection_type);\n\tdix = scsi_host_dix_capable(sdp->host, sdkp->protection_type);\n\n\tif (!dix && scsi_host_dix_capable(sdp->host, SD_DIF_TYPE0_PROTECTION)) {\n\t\tdif = 0;\n\t\tdix = 1;\n\t}\n\n\tif (!dif && !dix)\n\t\treturn snprintf(buf, 20, \"none\\n\");\n\n\treturn snprintf(buf, 20, \"%s%u\\n\", dix ? \"dix\" : \"dif\", dif);\n}\n\nstatic ssize_t\nsd_show_app_tag_own(struct device *dev, struct device_attribute *attr,\n\t\t    char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 20, \"%u\\n\", sdkp->ATO);\n}\n\nstatic ssize_t\nsd_show_thin_provisioning(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 20, \"%u\\n\", sdkp->lbpme);\n}\n\nstatic const char *lbp_mode[] = {\n\t[SD_LBP_FULL]\t\t= \"full\",\n\t[SD_LBP_UNMAP]\t\t= \"unmap\",\n\t[SD_LBP_WS16]\t\t= \"writesame_16\",\n\t[SD_LBP_WS10]\t\t= \"writesame_10\",\n\t[SD_LBP_ZERO]\t\t= \"writesame_zero\",\n\t[SD_LBP_DISABLE]\t= \"disabled\",\n};\n\nstatic ssize_t\nsd_show_provisioning_mode(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 20, \"%s\\n\", lbp_mode[sdkp->provisioning_mode]);\n}\n\nstatic ssize_t\nsd_store_provisioning_mode(struct device *dev, struct device_attribute *attr,\n\t\t\t   const char *buf, size_t count)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tif (sdp->type != TYPE_DISK)\n\t\treturn -EINVAL;\n\n\tif (!strncmp(buf, lbp_mode[SD_LBP_UNMAP], 20))\n\t\tsd_config_discard(sdkp, SD_LBP_UNMAP);\n\telse if (!strncmp(buf, lbp_mode[SD_LBP_WS16], 20))\n\t\tsd_config_discard(sdkp, SD_LBP_WS16);\n\telse if (!strncmp(buf, lbp_mode[SD_LBP_WS10], 20))\n\t\tsd_config_discard(sdkp, SD_LBP_WS10);\n\telse if (!strncmp(buf, lbp_mode[SD_LBP_ZERO], 20))\n\t\tsd_config_discard(sdkp, SD_LBP_ZERO);\n\telse if (!strncmp(buf, lbp_mode[SD_LBP_DISABLE], 20))\n\t\tsd_config_discard(sdkp, SD_LBP_DISABLE);\n\telse\n\t\treturn -EINVAL;\n\n\treturn count;\n}\n\nstatic struct device_attribute sd_disk_attrs[] = {\n\t__ATTR(cache_type, S_IRUGO|S_IWUSR, sd_show_cache_type,\n\t       sd_store_cache_type),\n\t__ATTR(FUA, S_IRUGO, sd_show_fua, NULL),\n\t__ATTR(allow_restart, S_IRUGO|S_IWUSR, sd_show_allow_restart,\n\t       sd_store_allow_restart),\n\t__ATTR(manage_start_stop, S_IRUGO|S_IWUSR, sd_show_manage_start_stop,\n\t       sd_store_manage_start_stop),\n\t__ATTR(protection_type, S_IRUGO, sd_show_protection_type, NULL),\n\t__ATTR(protection_mode, S_IRUGO, sd_show_protection_mode, NULL),\n\t__ATTR(app_tag_own, S_IRUGO, sd_show_app_tag_own, NULL),\n\t__ATTR(thin_provisioning, S_IRUGO, sd_show_thin_provisioning, NULL),\n\t__ATTR(provisioning_mode, S_IRUGO|S_IWUSR, sd_show_provisioning_mode,\n\t       sd_store_provisioning_mode),\n\t__ATTR_NULL,\n};\n\nstatic struct class sd_disk_class = {\n\t.name\t\t= \"scsi_disk\",\n\t.owner\t\t= THIS_MODULE,\n\t.dev_release\t= scsi_disk_release,\n\t.dev_attrs\t= sd_disk_attrs,\n};\n\nstatic struct scsi_driver sd_template = {\n\t.owner\t\t\t= THIS_MODULE,\n\t.gendrv = {\n\t\t.name\t\t= \"sd\",\n\t\t.probe\t\t= sd_probe,\n\t\t.remove\t\t= sd_remove,\n\t\t.suspend\t= sd_suspend,\n\t\t.resume\t\t= sd_resume,\n\t\t.shutdown\t= sd_shutdown,\n\t},\n\t.rescan\t\t\t= sd_rescan,\n\t.done\t\t\t= sd_done,\n};\n\n/*\n * Device no to disk mapping:\n * \n *       major         disc2     disc  p1\n *   |............|.............|....|....| <- dev_t\n *    31        20 19          8 7  4 3  0\n * \n * Inside a major, we have 16k disks, however mapped non-\n * contiguously. The first 16 disks are for major0, the next\n * ones with major1, ... Disk 256 is for major0 again, disk 272 \n * for major1, ... \n * As we stay compatible with our numbering scheme, we can reuse \n * the well-know SCSI majors 8, 65--71, 136--143.\n */\nstatic int sd_major(int major_idx)\n{\n\tswitch (major_idx) {\n\tcase 0:\n\t\treturn SCSI_DISK0_MAJOR;\n\tcase 1 ... 7:\n\t\treturn SCSI_DISK1_MAJOR + major_idx - 1;\n\tcase 8 ... 15:\n\t\treturn SCSI_DISK8_MAJOR + major_idx - 8;\n\tdefault:\n\t\tBUG();\n\t\treturn 0;\t/* shut up gcc */\n\t}\n}\n\nstatic struct scsi_disk *__scsi_disk_get(struct gendisk *disk)\n{\n\tstruct scsi_disk *sdkp = NULL;\n\n\tif (disk->private_data) {\n\t\tsdkp = scsi_disk(disk);\n\t\tif (scsi_device_get(sdkp->device) == 0)\n\t\t\tget_device(&sdkp->dev);\n\t\telse\n\t\t\tsdkp = NULL;\n\t}\n\treturn sdkp;\n}\n\nstatic struct scsi_disk *scsi_disk_get(struct gendisk *disk)\n{\n\tstruct scsi_disk *sdkp;\n\n\tmutex_lock(&sd_ref_mutex);\n\tsdkp = __scsi_disk_get(disk);\n\tmutex_unlock(&sd_ref_mutex);\n\treturn sdkp;\n}\n\nstatic struct scsi_disk *scsi_disk_get_from_dev(struct device *dev)\n{\n\tstruct scsi_disk *sdkp;\n\n\tmutex_lock(&sd_ref_mutex);\n\tsdkp = dev_get_drvdata(dev);\n\tif (sdkp)\n\t\tsdkp = __scsi_disk_get(sdkp->disk);\n\tmutex_unlock(&sd_ref_mutex);\n\treturn sdkp;\n}\n\nstatic void scsi_disk_put(struct scsi_disk *sdkp)\n{\n\tstruct scsi_device *sdev = sdkp->device;\n\n\tmutex_lock(&sd_ref_mutex);\n\tput_device(&sdkp->dev);\n\tscsi_device_put(sdev);\n\tmutex_unlock(&sd_ref_mutex);\n}\n\nstatic void sd_prot_op(struct scsi_cmnd *scmd, unsigned int dif)\n{\n\tunsigned int prot_op = SCSI_PROT_NORMAL;\n\tunsigned int dix = scsi_prot_sg_count(scmd);\n\n\tif (scmd->sc_data_direction == DMA_FROM_DEVICE) {\n\t\tif (dif && dix)\n\t\t\tprot_op = SCSI_PROT_READ_PASS;\n\t\telse if (dif && !dix)\n\t\t\tprot_op = SCSI_PROT_READ_STRIP;\n\t\telse if (!dif && dix)\n\t\t\tprot_op = SCSI_PROT_READ_INSERT;\n\t} else {\n\t\tif (dif && dix)\n\t\t\tprot_op = SCSI_PROT_WRITE_PASS;\n\t\telse if (dif && !dix)\n\t\t\tprot_op = SCSI_PROT_WRITE_INSERT;\n\t\telse if (!dif && dix)\n\t\t\tprot_op = SCSI_PROT_WRITE_STRIP;\n\t}\n\n\tscsi_set_prot_op(scmd, prot_op);\n\tscsi_set_prot_type(scmd, dif);\n}\n\nstatic void sd_config_discard(struct scsi_disk *sdkp, unsigned int mode)\n{\n\tstruct request_queue *q = sdkp->disk->queue;\n\tunsigned int logical_block_size = sdkp->device->sector_size;\n\tunsigned int max_blocks = 0;\n\n\tq->limits.discard_zeroes_data = sdkp->lbprz;\n\tq->limits.discard_alignment = sdkp->unmap_alignment *\n\t\tlogical_block_size;\n\tq->limits.discard_granularity =\n\t\tmax(sdkp->physical_block_size,\n\t\t    sdkp->unmap_granularity * logical_block_size);\n\n\tswitch (mode) {\n\n\tcase SD_LBP_DISABLE:\n\t\tq->limits.max_discard_sectors = 0;\n\t\tqueue_flag_clear_unlocked(QUEUE_FLAG_DISCARD, q);\n\t\treturn;\n\n\tcase SD_LBP_UNMAP:\n\t\tmax_blocks = min_not_zero(sdkp->max_unmap_blocks, 0xffffffff);\n\t\tbreak;\n\n\tcase SD_LBP_WS16:\n\t\tmax_blocks = min_not_zero(sdkp->max_ws_blocks, 0xffffffff);\n\t\tbreak;\n\n\tcase SD_LBP_WS10:\n\t\tmax_blocks = min_not_zero(sdkp->max_ws_blocks, (u32)0xffff);\n\t\tbreak;\n\n\tcase SD_LBP_ZERO:\n\t\tmax_blocks = min_not_zero(sdkp->max_ws_blocks, (u32)0xffff);\n\t\tq->limits.discard_zeroes_data = 1;\n\t\tbreak;\n\t}\n\n\tq->limits.max_discard_sectors = max_blocks * (logical_block_size >> 9);\n\tqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);\n\n\tsdkp->provisioning_mode = mode;\n}\n\n/**\n * scsi_setup_discard_cmnd - unmap blocks on thinly provisioned device\n * @sdp: scsi device to operate one\n * @rq: Request to prepare\n *\n * Will issue either UNMAP or WRITE SAME(16) depending on preference\n * indicated by target device.\n **/\nstatic int scsi_setup_discard_cmnd(struct scsi_device *sdp, struct request *rq)\n{\n\tstruct scsi_disk *sdkp = scsi_disk(rq->rq_disk);\n\tstruct bio *bio = rq->bio;\n\tsector_t sector = bio->bi_sector;\n\tunsigned int nr_sectors = bio_sectors(bio);\n\tunsigned int len;\n\tint ret;\n\tchar *buf;\n\tstruct page *page;\n\n\tif (sdkp->device->sector_size == 4096) {\n\t\tsector >>= 3;\n\t\tnr_sectors >>= 3;\n\t}\n\n\trq->timeout = SD_TIMEOUT;\n\n\tmemset(rq->cmd, 0, rq->cmd_len);\n\n\tpage = alloc_page(GFP_ATOMIC | __GFP_ZERO);\n\tif (!page)\n\t\treturn BLKPREP_DEFER;\n\n\tswitch (sdkp->provisioning_mode) {\n\tcase SD_LBP_UNMAP:\n\t\tbuf = page_address(page);\n\n\t\trq->cmd_len = 10;\n\t\trq->cmd[0] = UNMAP;\n\t\trq->cmd[8] = 24;\n\n\t\tput_unaligned_be16(6 + 16, &buf[0]);\n\t\tput_unaligned_be16(16, &buf[2]);\n\t\tput_unaligned_be64(sector, &buf[8]);\n\t\tput_unaligned_be32(nr_sectors, &buf[16]);\n\n\t\tlen = 24;\n\t\tbreak;\n\n\tcase SD_LBP_WS16:\n\t\trq->cmd_len = 16;\n\t\trq->cmd[0] = WRITE_SAME_16;\n\t\trq->cmd[1] = 0x8; /* UNMAP */\n\t\tput_unaligned_be64(sector, &rq->cmd[2]);\n\t\tput_unaligned_be32(nr_sectors, &rq->cmd[10]);\n\n\t\tlen = sdkp->device->sector_size;\n\t\tbreak;\n\n\tcase SD_LBP_WS10:\n\tcase SD_LBP_ZERO:\n\t\trq->cmd_len = 10;\n\t\trq->cmd[0] = WRITE_SAME;\n\t\tif (sdkp->provisioning_mode == SD_LBP_WS10)\n\t\t\trq->cmd[1] = 0x8; /* UNMAP */\n\t\tput_unaligned_be32(sector, &rq->cmd[2]);\n\t\tput_unaligned_be16(nr_sectors, &rq->cmd[7]);\n\n\t\tlen = sdkp->device->sector_size;\n\t\tbreak;\n\n\tdefault:\n\t\tret = BLKPREP_KILL;\n\t\tgoto out;\n\t}\n\n\tblk_add_request_payload(rq, page, len);\n\tret = scsi_setup_blk_pc_cmnd(sdp, rq);\n\trq->buffer = page_address(page);\n\nout:\n\tif (ret != BLKPREP_OK) {\n\t\t__free_page(page);\n\t\trq->buffer = NULL;\n\t}\n\treturn ret;\n}\n\nstatic int scsi_setup_flush_cmnd(struct scsi_device *sdp, struct request *rq)\n{\n\trq->timeout = SD_FLUSH_TIMEOUT;\n\trq->retries = SD_MAX_RETRIES;\n\trq->cmd[0] = SYNCHRONIZE_CACHE;\n\trq->cmd_len = 10;\n\n\treturn scsi_setup_blk_pc_cmnd(sdp, rq);\n}\n\nstatic void sd_unprep_fn(struct request_queue *q, struct request *rq)\n{\n\tif (rq->cmd_flags & REQ_DISCARD) {\n\t\tfree_page((unsigned long)rq->buffer);\n\t\trq->buffer = NULL;\n\t}\n}\n\n/**\n *\tsd_init_command - build a scsi (read or write) command from\n *\tinformation in the request structure.\n *\t@SCpnt: pointer to mid-level's per scsi command structure that\n *\tcontains request and into which the scsi command is written\n *\n *\tReturns 1 if successful and 0 if error (or cannot be done now).\n **/\nstatic int sd_prep_fn(struct request_queue *q, struct request *rq)\n{\n\tstruct scsi_cmnd *SCpnt;\n\tstruct scsi_device *sdp = q->queuedata;\n\tstruct gendisk *disk = rq->rq_disk;\n\tstruct scsi_disk *sdkp;\n\tsector_t block = blk_rq_pos(rq);\n\tsector_t threshold;\n\tunsigned int this_count = blk_rq_sectors(rq);\n\tint ret, host_dif;\n\tunsigned char protect;\n\n\t/*\n\t * Discard request come in as REQ_TYPE_FS but we turn them into\n\t * block PC requests to make life easier.\n\t */\n\tif (rq->cmd_flags & REQ_DISCARD) {\n\t\tret = scsi_setup_discard_cmnd(sdp, rq);\n\t\tgoto out;\n\t} else if (rq->cmd_flags & REQ_FLUSH) {\n\t\tret = scsi_setup_flush_cmnd(sdp, rq);\n\t\tgoto out;\n\t} else if (rq->cmd_type == REQ_TYPE_BLOCK_PC) {\n\t\tret = scsi_setup_blk_pc_cmnd(sdp, rq);\n\t\tgoto out;\n\t} else if (rq->cmd_type != REQ_TYPE_FS) {\n\t\tret = BLKPREP_KILL;\n\t\tgoto out;\n\t}\n\tret = scsi_setup_fs_cmnd(sdp, rq);\n\tif (ret != BLKPREP_OK)\n\t\tgoto out;\n\tSCpnt = rq->special;\n\tsdkp = scsi_disk(disk);\n\n\t/* from here on until we're complete, any goto out\n\t * is used for a killable error condition */\n\tret = BLKPREP_KILL;\n\n\tSCSI_LOG_HLQUEUE(1, scmd_printk(KERN_INFO, SCpnt,\n\t\t\t\t\t\"sd_init_command: block=%llu, \"\n\t\t\t\t\t\"count=%d\\n\",\n\t\t\t\t\t(unsigned long long)block,\n\t\t\t\t\tthis_count));\n\n\tif (!sdp || !scsi_device_online(sdp) ||\n\t    block + blk_rq_sectors(rq) > get_capacity(disk)) {\n\t\tSCSI_LOG_HLQUEUE(2, scmd_printk(KERN_INFO, SCpnt,\n\t\t\t\t\t\t\"Finishing %u sectors\\n\",\n\t\t\t\t\t\tblk_rq_sectors(rq)));\n\t\tSCSI_LOG_HLQUEUE(2, scmd_printk(KERN_INFO, SCpnt,\n\t\t\t\t\t\t\"Retry with 0x%p\\n\", SCpnt));\n\t\tgoto out;\n\t}\n\n\tif (sdp->changed) {\n\t\t/*\n\t\t * quietly refuse to do anything to a changed disc until \n\t\t * the changed bit has been reset\n\t\t */\n\t\t/* printk(\"SCSI disk has been changed or is not present. Prohibiting further I/O.\\n\"); */\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Some SD card readers can't handle multi-sector accesses which touch\n\t * the last one or two hardware sectors.  Split accesses as needed.\n\t */\n\tthreshold = get_capacity(disk) - SD_LAST_BUGGY_SECTORS *\n\t\t(sdp->sector_size / 512);\n\n\tif (unlikely(sdp->last_sector_bug && block + this_count > threshold)) {\n\t\tif (block < threshold) {\n\t\t\t/* Access up to the threshold but not beyond */\n\t\t\tthis_count = threshold - block;\n\t\t} else {\n\t\t\t/* Access only a single hardware sector */\n\t\t\tthis_count = sdp->sector_size / 512;\n\t\t}\n\t}\n\n\tSCSI_LOG_HLQUEUE(2, scmd_printk(KERN_INFO, SCpnt, \"block=%llu\\n\",\n\t\t\t\t\t(unsigned long long)block));\n\n\t/*\n\t * If we have a 1K hardware sectorsize, prevent access to single\n\t * 512 byte sectors.  In theory we could handle this - in fact\n\t * the scsi cdrom driver must be able to handle this because\n\t * we typically use 1K blocksizes, and cdroms typically have\n\t * 2K hardware sectorsizes.  Of course, things are simpler\n\t * with the cdrom, since it is read-only.  For performance\n\t * reasons, the filesystems should be able to handle this\n\t * and not force the scsi disk driver to use bounce buffers\n\t * for this.\n\t */\n\tif (sdp->sector_size == 1024) {\n\t\tif ((block & 1) || (blk_rq_sectors(rq) & 1)) {\n\t\t\tscmd_printk(KERN_ERR, SCpnt,\n\t\t\t\t    \"Bad block number requested\\n\");\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tblock = block >> 1;\n\t\t\tthis_count = this_count >> 1;\n\t\t}\n\t}\n\tif (sdp->sector_size == 2048) {\n\t\tif ((block & 3) || (blk_rq_sectors(rq) & 3)) {\n\t\t\tscmd_printk(KERN_ERR, SCpnt,\n\t\t\t\t    \"Bad block number requested\\n\");\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tblock = block >> 2;\n\t\t\tthis_count = this_count >> 2;\n\t\t}\n\t}\n\tif (sdp->sector_size == 4096) {\n\t\tif ((block & 7) || (blk_rq_sectors(rq) & 7)) {\n\t\t\tscmd_printk(KERN_ERR, SCpnt,\n\t\t\t\t    \"Bad block number requested\\n\");\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tblock = block >> 3;\n\t\t\tthis_count = this_count >> 3;\n\t\t}\n\t}\n\tif (rq_data_dir(rq) == WRITE) {\n\t\tif (!sdp->writeable) {\n\t\t\tgoto out;\n\t\t}\n\t\tSCpnt->cmnd[0] = WRITE_6;\n\t\tSCpnt->sc_data_direction = DMA_TO_DEVICE;\n\n\t\tif (blk_integrity_rq(rq) &&\n\t\t    sd_dif_prepare(rq, block, sdp->sector_size) == -EIO)\n\t\t\tgoto out;\n\n\t} else if (rq_data_dir(rq) == READ) {\n\t\tSCpnt->cmnd[0] = READ_6;\n\t\tSCpnt->sc_data_direction = DMA_FROM_DEVICE;\n\t} else {\n\t\tscmd_printk(KERN_ERR, SCpnt, \"Unknown command %x\\n\", rq->cmd_flags);\n\t\tgoto out;\n\t}\n\n\tSCSI_LOG_HLQUEUE(2, scmd_printk(KERN_INFO, SCpnt,\n\t\t\t\t\t\"%s %d/%u 512 byte blocks.\\n\",\n\t\t\t\t\t(rq_data_dir(rq) == WRITE) ?\n\t\t\t\t\t\"writing\" : \"reading\", this_count,\n\t\t\t\t\tblk_rq_sectors(rq)));\n\n\t/* Set RDPROTECT/WRPROTECT if disk is formatted with DIF */\n\thost_dif = scsi_host_dif_capable(sdp->host, sdkp->protection_type);\n\tif (host_dif)\n\t\tprotect = 1 << 5;\n\telse\n\t\tprotect = 0;\n\n\tif (host_dif == SD_DIF_TYPE2_PROTECTION) {\n\t\tSCpnt->cmnd = mempool_alloc(sd_cdb_pool, GFP_ATOMIC);\n\n\t\tif (unlikely(SCpnt->cmnd == NULL)) {\n\t\t\tret = BLKPREP_DEFER;\n\t\t\tgoto out;\n\t\t}\n\n\t\tSCpnt->cmd_len = SD_EXT_CDB_SIZE;\n\t\tmemset(SCpnt->cmnd, 0, SCpnt->cmd_len);\n\t\tSCpnt->cmnd[0] = VARIABLE_LENGTH_CMD;\n\t\tSCpnt->cmnd[7] = 0x18;\n\t\tSCpnt->cmnd[9] = (rq_data_dir(rq) == READ) ? READ_32 : WRITE_32;\n\t\tSCpnt->cmnd[10] = protect | ((rq->cmd_flags & REQ_FUA) ? 0x8 : 0);\n\n\t\t/* LBA */\n\t\tSCpnt->cmnd[12] = sizeof(block) > 4 ? (unsigned char) (block >> 56) & 0xff : 0;\n\t\tSCpnt->cmnd[13] = sizeof(block) > 4 ? (unsigned char) (block >> 48) & 0xff : 0;\n\t\tSCpnt->cmnd[14] = sizeof(block) > 4 ? (unsigned char) (block >> 40) & 0xff : 0;\n\t\tSCpnt->cmnd[15] = sizeof(block) > 4 ? (unsigned char) (block >> 32) & 0xff : 0;\n\t\tSCpnt->cmnd[16] = (unsigned char) (block >> 24) & 0xff;\n\t\tSCpnt->cmnd[17] = (unsigned char) (block >> 16) & 0xff;\n\t\tSCpnt->cmnd[18] = (unsigned char) (block >> 8) & 0xff;\n\t\tSCpnt->cmnd[19] = (unsigned char) block & 0xff;\n\n\t\t/* Expected Indirect LBA */\n\t\tSCpnt->cmnd[20] = (unsigned char) (block >> 24) & 0xff;\n\t\tSCpnt->cmnd[21] = (unsigned char) (block >> 16) & 0xff;\n\t\tSCpnt->cmnd[22] = (unsigned char) (block >> 8) & 0xff;\n\t\tSCpnt->cmnd[23] = (unsigned char) block & 0xff;\n\n\t\t/* Transfer length */\n\t\tSCpnt->cmnd[28] = (unsigned char) (this_count >> 24) & 0xff;\n\t\tSCpnt->cmnd[29] = (unsigned char) (this_count >> 16) & 0xff;\n\t\tSCpnt->cmnd[30] = (unsigned char) (this_count >> 8) & 0xff;\n\t\tSCpnt->cmnd[31] = (unsigned char) this_count & 0xff;\n\t} else if (block > 0xffffffff) {\n\t\tSCpnt->cmnd[0] += READ_16 - READ_6;\n\t\tSCpnt->cmnd[1] = protect | ((rq->cmd_flags & REQ_FUA) ? 0x8 : 0);\n\t\tSCpnt->cmnd[2] = sizeof(block) > 4 ? (unsigned char) (block >> 56) & 0xff : 0;\n\t\tSCpnt->cmnd[3] = sizeof(block) > 4 ? (unsigned char) (block >> 48) & 0xff : 0;\n\t\tSCpnt->cmnd[4] = sizeof(block) > 4 ? (unsigned char) (block >> 40) & 0xff : 0;\n\t\tSCpnt->cmnd[5] = sizeof(block) > 4 ? (unsigned char) (block >> 32) & 0xff : 0;\n\t\tSCpnt->cmnd[6] = (unsigned char) (block >> 24) & 0xff;\n\t\tSCpnt->cmnd[7] = (unsigned char) (block >> 16) & 0xff;\n\t\tSCpnt->cmnd[8] = (unsigned char) (block >> 8) & 0xff;\n\t\tSCpnt->cmnd[9] = (unsigned char) block & 0xff;\n\t\tSCpnt->cmnd[10] = (unsigned char) (this_count >> 24) & 0xff;\n\t\tSCpnt->cmnd[11] = (unsigned char) (this_count >> 16) & 0xff;\n\t\tSCpnt->cmnd[12] = (unsigned char) (this_count >> 8) & 0xff;\n\t\tSCpnt->cmnd[13] = (unsigned char) this_count & 0xff;\n\t\tSCpnt->cmnd[14] = SCpnt->cmnd[15] = 0;\n\t} else if ((this_count > 0xff) || (block > 0x1fffff) ||\n\t\t   scsi_device_protection(SCpnt->device) ||\n\t\t   SCpnt->device->use_10_for_rw) {\n\t\tif (this_count > 0xffff)\n\t\t\tthis_count = 0xffff;\n\n\t\tSCpnt->cmnd[0] += READ_10 - READ_6;\n\t\tSCpnt->cmnd[1] = protect | ((rq->cmd_flags & REQ_FUA) ? 0x8 : 0);\n\t\tSCpnt->cmnd[2] = (unsigned char) (block >> 24) & 0xff;\n\t\tSCpnt->cmnd[3] = (unsigned char) (block >> 16) & 0xff;\n\t\tSCpnt->cmnd[4] = (unsigned char) (block >> 8) & 0xff;\n\t\tSCpnt->cmnd[5] = (unsigned char) block & 0xff;\n\t\tSCpnt->cmnd[6] = SCpnt->cmnd[9] = 0;\n\t\tSCpnt->cmnd[7] = (unsigned char) (this_count >> 8) & 0xff;\n\t\tSCpnt->cmnd[8] = (unsigned char) this_count & 0xff;\n\t} else {\n\t\tif (unlikely(rq->cmd_flags & REQ_FUA)) {\n\t\t\t/*\n\t\t\t * This happens only if this drive failed\n\t\t\t * 10byte rw command with ILLEGAL_REQUEST\n\t\t\t * during operation and thus turned off\n\t\t\t * use_10_for_rw.\n\t\t\t */\n\t\t\tscmd_printk(KERN_ERR, SCpnt,\n\t\t\t\t    \"FUA write on READ/WRITE(6) drive\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tSCpnt->cmnd[1] |= (unsigned char) ((block >> 16) & 0x1f);\n\t\tSCpnt->cmnd[2] = (unsigned char) ((block >> 8) & 0xff);\n\t\tSCpnt->cmnd[3] = (unsigned char) block & 0xff;\n\t\tSCpnt->cmnd[4] = (unsigned char) this_count;\n\t\tSCpnt->cmnd[5] = 0;\n\t}\n\tSCpnt->sdb.length = this_count * sdp->sector_size;\n\n\t/* If DIF or DIX is enabled, tell HBA how to handle request */\n\tif (host_dif || scsi_prot_sg_count(SCpnt))\n\t\tsd_prot_op(SCpnt, host_dif);\n\n\t/*\n\t * We shouldn't disconnect in the middle of a sector, so with a dumb\n\t * host adapter, it's safe to assume that we can at least transfer\n\t * this many bytes between each connect / disconnect.\n\t */\n\tSCpnt->transfersize = sdp->sector_size;\n\tSCpnt->underflow = this_count << 9;\n\tSCpnt->allowed = SD_MAX_RETRIES;\n\n\t/*\n\t * This indicates that the command is ready from our end to be\n\t * queued.\n\t */\n\tret = BLKPREP_OK;\n out:\n\treturn scsi_prep_return(q, rq, ret);\n}\n\n/**\n *\tsd_open - open a scsi disk device\n *\t@inode: only i_rdev member may be used\n *\t@filp: only f_mode and f_flags may be used\n *\n *\tReturns 0 if successful. Returns a negated errno value in case \n *\tof error.\n *\n *\tNote: This can be called from a user context (e.g. fsck(1) )\n *\tor from within the kernel (e.g. as a result of a mount(1) ).\n *\tIn the latter case @inode and @filp carry an abridged amount\n *\tof information as noted above.\n *\n *\tLocking: called with bdev->bd_mutex held.\n **/\nstatic int sd_open(struct block_device *bdev, fmode_t mode)\n{\n\tstruct scsi_disk *sdkp = scsi_disk_get(bdev->bd_disk);\n\tstruct scsi_device *sdev;\n\tint retval;\n\n\tif (!sdkp)\n\t\treturn -ENXIO;\n\n\tSCSI_LOG_HLQUEUE(3, sd_printk(KERN_INFO, sdkp, \"sd_open\\n\"));\n\n\tsdev = sdkp->device;\n\n\tretval = scsi_autopm_get_device(sdev);\n\tif (retval)\n\t\tgoto error_autopm;\n\n\t/*\n\t * If the device is in error recovery, wait until it is done.\n\t * If the device is offline, then disallow any access to it.\n\t */\n\tretval = -ENXIO;\n\tif (!scsi_block_when_processing_errors(sdev))\n\t\tgoto error_out;\n\n\tif (sdev->removable || sdkp->write_prot)\n\t\tcheck_disk_change(bdev);\n\n\t/*\n\t * If the drive is empty, just let the open fail.\n\t */\n\tretval = -ENOMEDIUM;\n\tif (sdev->removable && !sdkp->media_present && !(mode & FMODE_NDELAY))\n\t\tgoto error_out;\n\n\t/*\n\t * If the device has the write protect tab set, have the open fail\n\t * if the user expects to be able to write to the thing.\n\t */\n\tretval = -EROFS;\n\tif (sdkp->write_prot && (mode & FMODE_WRITE))\n\t\tgoto error_out;\n\n\t/*\n\t * It is possible that the disk changing stuff resulted in\n\t * the device being taken offline.  If this is the case,\n\t * report this to the user, and don't pretend that the\n\t * open actually succeeded.\n\t */\n\tretval = -ENXIO;\n\tif (!scsi_device_online(sdev))\n\t\tgoto error_out;\n\n\tif ((atomic_inc_return(&sdkp->openers) == 1) && sdev->removable) {\n\t\tif (scsi_block_when_processing_errors(sdev))\n\t\t\tscsi_set_medium_removal(sdev, SCSI_REMOVAL_PREVENT);\n\t}\n\n\treturn 0;\n\nerror_out:\n\tscsi_autopm_put_device(sdev);\nerror_autopm:\n\tscsi_disk_put(sdkp);\n\treturn retval;\t\n}\n\n/**\n *\tsd_release - invoked when the (last) close(2) is called on this\n *\tscsi disk.\n *\t@inode: only i_rdev member may be used\n *\t@filp: only f_mode and f_flags may be used\n *\n *\tReturns 0. \n *\n *\tNote: may block (uninterruptible) if error recovery is underway\n *\ton this disk.\n *\n *\tLocking: called with bdev->bd_mutex held.\n **/\nstatic int sd_release(struct gendisk *disk, fmode_t mode)\n{\n\tstruct scsi_disk *sdkp = scsi_disk(disk);\n\tstruct scsi_device *sdev = sdkp->device;\n\n\tSCSI_LOG_HLQUEUE(3, sd_printk(KERN_INFO, sdkp, \"sd_release\\n\"));\n\n\tif (atomic_dec_return(&sdkp->openers) == 0 && sdev->removable) {\n\t\tif (scsi_block_when_processing_errors(sdev))\n\t\t\tscsi_set_medium_removal(sdev, SCSI_REMOVAL_ALLOW);\n\t}\n\n\t/*\n\t * XXX and what if there are packets in flight and this close()\n\t * XXX is followed by a \"rmmod sd_mod\"?\n\t */\n\n\tscsi_autopm_put_device(sdev);\n\tscsi_disk_put(sdkp);\n\treturn 0;\n}\n\nstatic int sd_getgeo(struct block_device *bdev, struct hd_geometry *geo)\n{\n\tstruct scsi_disk *sdkp = scsi_disk(bdev->bd_disk);\n\tstruct scsi_device *sdp = sdkp->device;\n\tstruct Scsi_Host *host = sdp->host;\n\tint diskinfo[4];\n\n\t/* default to most commonly used values */\n        diskinfo[0] = 0x40;\t/* 1 << 6 */\n       \tdiskinfo[1] = 0x20;\t/* 1 << 5 */\n       \tdiskinfo[2] = sdkp->capacity >> 11;\n\t\n\t/* override with calculated, extended default, or driver values */\n\tif (host->hostt->bios_param)\n\t\thost->hostt->bios_param(sdp, bdev, sdkp->capacity, diskinfo);\n\telse\n\t\tscsicam_bios_param(bdev, sdkp->capacity, diskinfo);\n\n\tgeo->heads = diskinfo[0];\n\tgeo->sectors = diskinfo[1];\n\tgeo->cylinders = diskinfo[2];\n\treturn 0;\n}\n\n/**\n *\tsd_ioctl - process an ioctl\n *\t@inode: only i_rdev/i_bdev members may be used\n *\t@filp: only f_mode and f_flags may be used\n *\t@cmd: ioctl command number\n *\t@arg: this is third argument given to ioctl(2) system call.\n *\tOften contains a pointer.\n *\n *\tReturns 0 if successful (some ioctls return positive numbers on\n *\tsuccess as well). Returns a negated errno value in case of error.\n *\n *\tNote: most ioctls are forward onto the block subsystem or further\n *\tdown in the scsi subsystem.\n **/\nstatic int sd_ioctl(struct block_device *bdev, fmode_t mode,\n\t\t    unsigned int cmd, unsigned long arg)\n{\n\tstruct gendisk *disk = bdev->bd_disk;\n\tstruct scsi_disk *sdkp = scsi_disk(disk);\n\tstruct scsi_device *sdp = sdkp->device;\n\tvoid __user *p = (void __user *)arg;\n\tint error;\n    \n\tSCSI_LOG_IOCTL(1, sd_printk(KERN_INFO, sdkp, \"sd_ioctl: disk=%s, \"\n\t\t\t\t    \"cmd=0x%x\\n\", disk->disk_name, cmd));\n\n\t/*\n\t * If we are in the middle of error recovery, don't let anyone\n\t * else try and use this device.  Also, if error recovery fails, it\n\t * may try and take the device offline, in which case all further\n\t * access to the device is prohibited.\n\t */\n\terror = scsi_nonblockable_ioctl(sdp, cmd, p,\n\t\t\t\t\t(mode & FMODE_NDELAY) != 0);\n\tif (!scsi_block_when_processing_errors(sdp) || !error)\n\t\tgoto out;\n\n\t/*\n\t * Send SCSI addressing ioctls directly to mid level, send other\n\t * ioctls to block level and then onto mid level if they can't be\n\t * resolved.\n\t */\n\tswitch (cmd) {\n\t\tcase SCSI_IOCTL_GET_IDLUN:\n\t\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\t\t\terror = scsi_ioctl(sdp, cmd, p);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terror = scsi_cmd_blk_ioctl(bdev, mode, cmd, p);\n\t\t\tif (error != -ENOTTY)\n\t\t\t\tbreak;\n\t\t\terror = scsi_ioctl(sdp, cmd, p);\n\t\t\tbreak;\n\t}\nout:\n\treturn error;\n}\n\nstatic void set_media_not_present(struct scsi_disk *sdkp)\n{\n\tif (sdkp->media_present)\n\t\tsdkp->device->changed = 1;\n\n\tif (sdkp->device->removable) {\n\t\tsdkp->media_present = 0;\n\t\tsdkp->capacity = 0;\n\t}\n}\n\nstatic int media_not_present(struct scsi_disk *sdkp,\n\t\t\t     struct scsi_sense_hdr *sshdr)\n{\n\tif (!scsi_sense_valid(sshdr))\n\t\treturn 0;\n\n\t/* not invoked for commands that could return deferred errors */\n\tswitch (sshdr->sense_key) {\n\tcase UNIT_ATTENTION:\n\tcase NOT_READY:\n\t\t/* medium not present */\n\t\tif (sshdr->asc == 0x3A) {\n\t\t\tset_media_not_present(sdkp);\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/**\n *\tsd_check_events - check media events\n *\t@disk: kernel device descriptor\n *\t@clearing: disk events currently being cleared\n *\n *\tReturns mask of DISK_EVENT_*.\n *\n *\tNote: this function is invoked from the block subsystem.\n **/\nstatic unsigned int sd_check_events(struct gendisk *disk, unsigned int clearing)\n{\n\tstruct scsi_disk *sdkp = scsi_disk(disk);\n\tstruct scsi_device *sdp = sdkp->device;\n\tstruct scsi_sense_hdr *sshdr = NULL;\n\tint retval;\n\n\tSCSI_LOG_HLQUEUE(3, sd_printk(KERN_INFO, sdkp, \"sd_check_events\\n\"));\n\n\t/*\n\t * If the device is offline, don't send any commands - just pretend as\n\t * if the command failed.  If the device ever comes back online, we\n\t * can deal with it then.  It is only because of unrecoverable errors\n\t * that we would ever take a device offline in the first place.\n\t */\n\tif (!scsi_device_online(sdp)) {\n\t\tset_media_not_present(sdkp);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Using TEST_UNIT_READY enables differentiation between drive with\n\t * no cartridge loaded - NOT READY, drive with changed cartridge -\n\t * UNIT ATTENTION, or with same cartridge - GOOD STATUS.\n\t *\n\t * Drives that auto spin down. eg iomega jaz 1G, will be started\n\t * by sd_spinup_disk() from sd_revalidate_disk(), which happens whenever\n\t * sd_revalidate() is called.\n\t */\n\tretval = -ENODEV;\n\n\tif (scsi_block_when_processing_errors(sdp)) {\n\t\tsshdr  = kzalloc(sizeof(*sshdr), GFP_KERNEL);\n\t\tretval = scsi_test_unit_ready(sdp, SD_TIMEOUT, SD_MAX_RETRIES,\n\t\t\t\t\t      sshdr);\n\t}\n\n\t/* failed to execute TUR, assume media not present */\n\tif (host_byte(retval)) {\n\t\tset_media_not_present(sdkp);\n\t\tgoto out;\n\t}\n\n\tif (media_not_present(sdkp, sshdr))\n\t\tgoto out;\n\n\t/*\n\t * For removable scsi disk we have to recognise the presence\n\t * of a disk in the drive.\n\t */\n\tif (!sdkp->media_present)\n\t\tsdp->changed = 1;\n\tsdkp->media_present = 1;\nout:\n\t/*\n\t * sdp->changed is set under the following conditions:\n\t *\n\t *\tMedium present state has changed in either direction.\n\t *\tDevice has indicated UNIT_ATTENTION.\n\t */\n\tkfree(sshdr);\n\tretval = sdp->changed ? DISK_EVENT_MEDIA_CHANGE : 0;\n\tsdp->changed = 0;\n\treturn retval;\n}\n\nstatic int sd_sync_cache(struct scsi_disk *sdkp)\n{\n\tint retries, res;\n\tstruct scsi_device *sdp = sdkp->device;\n\tstruct scsi_sense_hdr sshdr;\n\n\tif (!scsi_device_online(sdp))\n\t\treturn -ENODEV;\n\n\n\tfor (retries = 3; retries > 0; --retries) {\n\t\tunsigned char cmd[10] = { 0 };\n\n\t\tcmd[0] = SYNCHRONIZE_CACHE;\n\t\t/*\n\t\t * Leave the rest of the command zero to indicate\n\t\t * flush everything.\n\t\t */\n\t\tres = scsi_execute_req(sdp, cmd, DMA_NONE, NULL, 0, &sshdr,\n\t\t\t\t       SD_FLUSH_TIMEOUT, SD_MAX_RETRIES, NULL);\n\t\tif (res == 0)\n\t\t\tbreak;\n\t}\n\n\tif (res) {\n\t\tsd_print_result(sdkp, res);\n\t\tif (driver_byte(res) & DRIVER_SENSE)\n\t\t\tsd_print_sense_hdr(sdkp, &sshdr);\n\t}\n\n\tif (res)\n\t\treturn -EIO;\n\treturn 0;\n}\n\nstatic void sd_rescan(struct device *dev)\n{\n\tstruct scsi_disk *sdkp = scsi_disk_get_from_dev(dev);\n\n\tif (sdkp) {\n\t\trevalidate_disk(sdkp->disk);\n\t\tscsi_disk_put(sdkp);\n\t}\n}\n\n\n#ifdef CONFIG_COMPAT\n/* \n * This gets directly called from VFS. When the ioctl \n * is not recognized we go back to the other translation paths. \n */\nstatic int sd_compat_ioctl(struct block_device *bdev, fmode_t mode,\n\t\t\t   unsigned int cmd, unsigned long arg)\n{\n\tstruct scsi_device *sdev = scsi_disk(bdev->bd_disk)->device;\n\n\t/*\n\t * If we are in the middle of error recovery, don't let anyone\n\t * else try and use this device.  Also, if error recovery fails, it\n\t * may try and take the device offline, in which case all further\n\t * access to the device is prohibited.\n\t */\n\tif (!scsi_block_when_processing_errors(sdev))\n\t\treturn -ENODEV;\n\t       \n\tif (sdev->host->hostt->compat_ioctl) {\n\t\tint ret;\n\n\t\tret = sdev->host->hostt->compat_ioctl(sdev, cmd, (void __user *)arg);\n\n\t\treturn ret;\n\t}\n\n\t/* \n\t * Let the static ioctl translation table take care of it.\n\t */\n\treturn -ENOIOCTLCMD; \n}\n#endif\n\nstatic const struct block_device_operations sd_fops = {\n\t.owner\t\t\t= THIS_MODULE,\n\t.open\t\t\t= sd_open,\n\t.release\t\t= sd_release,\n\t.ioctl\t\t\t= sd_ioctl,\n\t.getgeo\t\t\t= sd_getgeo,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t\t= sd_compat_ioctl,\n#endif\n\t.check_events\t\t= sd_check_events,\n\t.revalidate_disk\t= sd_revalidate_disk,\n\t.unlock_native_capacity\t= sd_unlock_native_capacity,\n};\n\nstatic unsigned int sd_completed_bytes(struct scsi_cmnd *scmd)\n{\n\tu64 start_lba = blk_rq_pos(scmd->request);\n\tu64 end_lba = blk_rq_pos(scmd->request) + (scsi_bufflen(scmd) / 512);\n\tu64 bad_lba;\n\tint info_valid;\n\t/*\n\t * resid is optional but mostly filled in.  When it's unused,\n\t * its value is zero, so we assume the whole buffer transferred\n\t */\n\tunsigned int transferred = scsi_bufflen(scmd) - scsi_get_resid(scmd);\n\tunsigned int good_bytes;\n\n\tif (scmd->request->cmd_type != REQ_TYPE_FS)\n\t\treturn 0;\n\n\tinfo_valid = scsi_get_sense_info_fld(scmd->sense_buffer,\n\t\t\t\t\t     SCSI_SENSE_BUFFERSIZE,\n\t\t\t\t\t     &bad_lba);\n\tif (!info_valid)\n\t\treturn 0;\n\n\tif (scsi_bufflen(scmd) <= scmd->device->sector_size)\n\t\treturn 0;\n\n\tif (scmd->device->sector_size < 512) {\n\t\t/* only legitimate sector_size here is 256 */\n\t\tstart_lba <<= 1;\n\t\tend_lba <<= 1;\n\t} else {\n\t\t/* be careful ... don't want any overflows */\n\t\tu64 factor = scmd->device->sector_size / 512;\n\t\tdo_div(start_lba, factor);\n\t\tdo_div(end_lba, factor);\n\t}\n\n\t/* The bad lba was reported incorrectly, we have no idea where\n\t * the error is.\n\t */\n\tif (bad_lba < start_lba  || bad_lba >= end_lba)\n\t\treturn 0;\n\n\t/* This computation should always be done in terms of\n\t * the resolution of the device's medium.\n\t */\n\tgood_bytes = (bad_lba - start_lba) * scmd->device->sector_size;\n\treturn min(good_bytes, transferred);\n}\n\n/**\n *\tsd_done - bottom half handler: called when the lower level\n *\tdriver has completed (successfully or otherwise) a scsi command.\n *\t@SCpnt: mid-level's per command structure.\n *\n *\tNote: potentially run from within an ISR. Must not block.\n **/\nstatic int sd_done(struct scsi_cmnd *SCpnt)\n{\n\tint result = SCpnt->result;\n\tunsigned int good_bytes = result ? 0 : scsi_bufflen(SCpnt);\n\tstruct scsi_sense_hdr sshdr;\n\tstruct scsi_disk *sdkp = scsi_disk(SCpnt->request->rq_disk);\n\tint sense_valid = 0;\n\tint sense_deferred = 0;\n\tunsigned char op = SCpnt->cmnd[0];\n\n\tif ((SCpnt->request->cmd_flags & REQ_DISCARD) && !result)\n\t\tscsi_set_resid(SCpnt, 0);\n\n\tif (result) {\n\t\tsense_valid = scsi_command_normalize_sense(SCpnt, &sshdr);\n\t\tif (sense_valid)\n\t\t\tsense_deferred = scsi_sense_is_deferred(&sshdr);\n\t}\n#ifdef CONFIG_SCSI_LOGGING\n\tSCSI_LOG_HLCOMPLETE(1, scsi_print_result(SCpnt));\n\tif (sense_valid) {\n\t\tSCSI_LOG_HLCOMPLETE(1, scmd_printk(KERN_INFO, SCpnt,\n\t\t\t\t\t\t   \"sd_done: sb[respc,sk,asc,\"\n\t\t\t\t\t\t   \"ascq]=%x,%x,%x,%x\\n\",\n\t\t\t\t\t\t   sshdr.response_code,\n\t\t\t\t\t\t   sshdr.sense_key, sshdr.asc,\n\t\t\t\t\t\t   sshdr.ascq));\n\t}\n#endif\n\tif (driver_byte(result) != DRIVER_SENSE &&\n\t    (!sense_valid || sense_deferred))\n\t\tgoto out;\n\n\tswitch (sshdr.sense_key) {\n\tcase HARDWARE_ERROR:\n\tcase MEDIUM_ERROR:\n\t\tgood_bytes = sd_completed_bytes(SCpnt);\n\t\tbreak;\n\tcase RECOVERED_ERROR:\n\t\tgood_bytes = scsi_bufflen(SCpnt);\n\t\tbreak;\n\tcase NO_SENSE:\n\t\t/* This indicates a false check condition, so ignore it.  An\n\t\t * unknown amount of data was transferred so treat it as an\n\t\t * error.\n\t\t */\n\t\tscsi_print_sense(\"sd\", SCpnt);\n\t\tSCpnt->result = 0;\n\t\tmemset(SCpnt->sense_buffer, 0, SCSI_SENSE_BUFFERSIZE);\n\t\tbreak;\n\tcase ABORTED_COMMAND:\n\t\tif (sshdr.asc == 0x10)  /* DIF: Target detected corruption */\n\t\t\tgood_bytes = sd_completed_bytes(SCpnt);\n\t\tbreak;\n\tcase ILLEGAL_REQUEST:\n\t\tif (sshdr.asc == 0x10)  /* DIX: Host detected corruption */\n\t\t\tgood_bytes = sd_completed_bytes(SCpnt);\n\t\t/* INVALID COMMAND OPCODE or INVALID FIELD IN CDB */\n\t\tif ((sshdr.asc == 0x20 || sshdr.asc == 0x24) &&\n\t\t    (op == UNMAP || op == WRITE_SAME_16 || op == WRITE_SAME))\n\t\t\tsd_config_discard(sdkp, SD_LBP_DISABLE);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n out:\n\tif (rq_data_dir(SCpnt->request) == READ && scsi_prot_sg_count(SCpnt))\n\t\tsd_dif_complete(SCpnt, good_bytes);\n\n\tif (scsi_host_dif_capable(sdkp->device->host, sdkp->protection_type)\n\t    == SD_DIF_TYPE2_PROTECTION && SCpnt->cmnd != SCpnt->request->cmd) {\n\n\t\t/* We have to print a failed command here as the\n\t\t * extended CDB gets freed before scsi_io_completion()\n\t\t * is called.\n\t\t */\n\t\tif (result)\n\t\t\tscsi_print_command(SCpnt);\n\n\t\tmempool_free(SCpnt->cmnd, sd_cdb_pool);\n\t\tSCpnt->cmnd = NULL;\n\t\tSCpnt->cmd_len = 0;\n\t}\n\n\treturn good_bytes;\n}\n\n/*\n * spinup disk - called only in sd_revalidate_disk()\n */\nstatic void\nsd_spinup_disk(struct scsi_disk *sdkp)\n{\n\tunsigned char cmd[10];\n\tunsigned long spintime_expire = 0;\n\tint retries, spintime;\n\tunsigned int the_result;\n\tstruct scsi_sense_hdr sshdr;\n\tint sense_valid = 0;\n\n\tspintime = 0;\n\n\t/* Spin up drives, as required.  Only do this at boot time */\n\t/* Spinup needs to be done for module loads too. */\n\tdo {\n\t\tretries = 0;\n\n\t\tdo {\n\t\t\tcmd[0] = TEST_UNIT_READY;\n\t\t\tmemset((void *) &cmd[1], 0, 9);\n\n\t\t\tthe_result = scsi_execute_req(sdkp->device, cmd,\n\t\t\t\t\t\t      DMA_NONE, NULL, 0,\n\t\t\t\t\t\t      &sshdr, SD_TIMEOUT,\n\t\t\t\t\t\t      SD_MAX_RETRIES, NULL);\n\n\t\t\t/*\n\t\t\t * If the drive has indicated to us that it\n\t\t\t * doesn't have any media in it, don't bother\n\t\t\t * with any more polling.\n\t\t\t */\n\t\t\tif (media_not_present(sdkp, &sshdr))\n\t\t\t\treturn;\n\n\t\t\tif (the_result)\n\t\t\t\tsense_valid = scsi_sense_valid(&sshdr);\n\t\t\tretries++;\n\t\t} while (retries < 3 && \n\t\t\t (!scsi_status_is_good(the_result) ||\n\t\t\t  ((driver_byte(the_result) & DRIVER_SENSE) &&\n\t\t\t  sense_valid && sshdr.sense_key == UNIT_ATTENTION)));\n\n\t\tif ((driver_byte(the_result) & DRIVER_SENSE) == 0) {\n\t\t\t/* no sense, TUR either succeeded or failed\n\t\t\t * with a status error */\n\t\t\tif(!spintime && !scsi_status_is_good(the_result)) {\n\t\t\t\tsd_printk(KERN_NOTICE, sdkp, \"Unit Not Ready\\n\");\n\t\t\t\tsd_print_result(sdkp, the_result);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\t\t\t\t\n\t\t/*\n\t\t * The device does not want the automatic start to be issued.\n\t\t */\n\t\tif (sdkp->device->no_start_on_add)\n\t\t\tbreak;\n\n\t\tif (sense_valid && sshdr.sense_key == NOT_READY) {\n\t\t\tif (sshdr.asc == 4 && sshdr.ascq == 3)\n\t\t\t\tbreak;\t/* manual intervention required */\n\t\t\tif (sshdr.asc == 4 && sshdr.ascq == 0xb)\n\t\t\t\tbreak;\t/* standby */\n\t\t\tif (sshdr.asc == 4 && sshdr.ascq == 0xc)\n\t\t\t\tbreak;\t/* unavailable */\n\t\t\t/*\n\t\t\t * Issue command to spin up drive when not ready\n\t\t\t */\n\t\t\tif (!spintime) {\n\t\t\t\tsd_printk(KERN_NOTICE, sdkp, \"Spinning up disk...\");\n\t\t\t\tcmd[0] = START_STOP;\n\t\t\t\tcmd[1] = 1;\t/* Return immediately */\n\t\t\t\tmemset((void *) &cmd[2], 0, 8);\n\t\t\t\tcmd[4] = 1;\t/* Start spin cycle */\n\t\t\t\tif (sdkp->device->start_stop_pwr_cond)\n\t\t\t\t\tcmd[4] |= 1 << 4;\n\t\t\t\tscsi_execute_req(sdkp->device, cmd, DMA_NONE,\n\t\t\t\t\t\t NULL, 0, &sshdr,\n\t\t\t\t\t\t SD_TIMEOUT, SD_MAX_RETRIES,\n\t\t\t\t\t\t NULL);\n\t\t\t\tspintime_expire = jiffies + 100 * HZ;\n\t\t\t\tspintime = 1;\n\t\t\t}\n\t\t\t/* Wait 1 second for next try */\n\t\t\tmsleep(1000);\n\t\t\tprintk(\".\");\n\n\t\t/*\n\t\t * Wait for USB flash devices with slow firmware.\n\t\t * Yes, this sense key/ASC combination shouldn't\n\t\t * occur here.  It's characteristic of these devices.\n\t\t */\n\t\t} else if (sense_valid &&\n\t\t\t\tsshdr.sense_key == UNIT_ATTENTION &&\n\t\t\t\tsshdr.asc == 0x28) {\n\t\t\tif (!spintime) {\n\t\t\t\tspintime_expire = jiffies + 5 * HZ;\n\t\t\t\tspintime = 1;\n\t\t\t}\n\t\t\t/* Wait 1 second for next try */\n\t\t\tmsleep(1000);\n\t\t} else {\n\t\t\t/* we don't understand the sense code, so it's\n\t\t\t * probably pointless to loop */\n\t\t\tif(!spintime) {\n\t\t\t\tsd_printk(KERN_NOTICE, sdkp, \"Unit Not Ready\\n\");\n\t\t\t\tsd_print_sense_hdr(sdkp, &sshdr);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\t\t\t\n\t} while (spintime && time_before_eq(jiffies, spintime_expire));\n\n\tif (spintime) {\n\t\tif (scsi_status_is_good(the_result))\n\t\t\tprintk(\"ready\\n\");\n\t\telse\n\t\t\tprintk(\"not responding...\\n\");\n\t}\n}\n\n\n/*\n * Determine whether disk supports Data Integrity Field.\n */\nstatic void sd_read_protection_type(struct scsi_disk *sdkp, unsigned char *buffer)\n{\n\tstruct scsi_device *sdp = sdkp->device;\n\tu8 type;\n\n\tif (scsi_device_protection(sdp) == 0 || (buffer[12] & 1) == 0)\n\t\treturn;\n\n\ttype = ((buffer[12] >> 1) & 7) + 1; /* P_TYPE 0 = Type 1 */\n\n\tif (type == sdkp->protection_type || !sdkp->first_scan)\n\t\treturn;\n\n\tsdkp->protection_type = type;\n\n\tif (type > SD_DIF_TYPE3_PROTECTION) {\n\t\tsd_printk(KERN_ERR, sdkp, \"formatted with unsupported \"\t\\\n\t\t\t  \"protection type %u. Disabling disk!\\n\", type);\n\t\tsdkp->capacity = 0;\n\t\treturn;\n\t}\n\n\tif (scsi_host_dif_capable(sdp->host, type))\n\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t  \"Enabling DIF Type %u protection\\n\", type);\n\telse\n\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t  \"Disabling DIF Type %u protection\\n\", type);\n}\n\nstatic void read_capacity_error(struct scsi_disk *sdkp, struct scsi_device *sdp,\n\t\t\tstruct scsi_sense_hdr *sshdr, int sense_valid,\n\t\t\tint the_result)\n{\n\tsd_print_result(sdkp, the_result);\n\tif (driver_byte(the_result) & DRIVER_SENSE)\n\t\tsd_print_sense_hdr(sdkp, sshdr);\n\telse\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Sense not available.\\n\");\n\n\t/*\n\t * Set dirty bit for removable devices if not ready -\n\t * sometimes drives will not report this properly.\n\t */\n\tif (sdp->removable &&\n\t    sense_valid && sshdr->sense_key == NOT_READY)\n\t\tset_media_not_present(sdkp);\n\n\t/*\n\t * We used to set media_present to 0 here to indicate no media\n\t * in the drive, but some drives fail read capacity even with\n\t * media present, so we can't do that.\n\t */\n\tsdkp->capacity = 0; /* unknown mapped to zero - as usual */\n}\n\n#define RC16_LEN 32\n#if RC16_LEN > SD_BUF_SIZE\n#error RC16_LEN must not be more than SD_BUF_SIZE\n#endif\n\n#define READ_CAPACITY_RETRIES_ON_RESET\t10\n\nstatic int read_capacity_16(struct scsi_disk *sdkp, struct scsi_device *sdp,\n\t\t\t\t\t\tunsigned char *buffer)\n{\n\tunsigned char cmd[16];\n\tstruct scsi_sense_hdr sshdr;\n\tint sense_valid = 0;\n\tint the_result;\n\tint retries = 3, reset_retries = READ_CAPACITY_RETRIES_ON_RESET;\n\tunsigned int alignment;\n\tunsigned long long lba;\n\tunsigned sector_size;\n\n\tif (sdp->no_read_capacity_16)\n\t\treturn -EINVAL;\n\n\tdo {\n\t\tmemset(cmd, 0, 16);\n\t\tcmd[0] = SERVICE_ACTION_IN;\n\t\tcmd[1] = SAI_READ_CAPACITY_16;\n\t\tcmd[13] = RC16_LEN;\n\t\tmemset(buffer, 0, RC16_LEN);\n\n\t\tthe_result = scsi_execute_req(sdp, cmd, DMA_FROM_DEVICE,\n\t\t\t\t\tbuffer, RC16_LEN, &sshdr,\n\t\t\t\t\tSD_TIMEOUT, SD_MAX_RETRIES, NULL);\n\n\t\tif (media_not_present(sdkp, &sshdr))\n\t\t\treturn -ENODEV;\n\n\t\tif (the_result) {\n\t\t\tsense_valid = scsi_sense_valid(&sshdr);\n\t\t\tif (sense_valid &&\n\t\t\t    sshdr.sense_key == ILLEGAL_REQUEST &&\n\t\t\t    (sshdr.asc == 0x20 || sshdr.asc == 0x24) &&\n\t\t\t    sshdr.ascq == 0x00)\n\t\t\t\t/* Invalid Command Operation Code or\n\t\t\t\t * Invalid Field in CDB, just retry\n\t\t\t\t * silently with RC10 */\n\t\t\t\treturn -EINVAL;\n\t\t\tif (sense_valid &&\n\t\t\t    sshdr.sense_key == UNIT_ATTENTION &&\n\t\t\t    sshdr.asc == 0x29 && sshdr.ascq == 0x00)\n\t\t\t\t/* Device reset might occur several times,\n\t\t\t\t * give it one more chance */\n\t\t\t\tif (--reset_retries > 0)\n\t\t\t\t\tcontinue;\n\t\t}\n\t\tretries--;\n\n\t} while (the_result && retries);\n\n\tif (the_result) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"READ CAPACITY(16) failed\\n\");\n\t\tread_capacity_error(sdkp, sdp, &sshdr, sense_valid, the_result);\n\t\treturn -EINVAL;\n\t}\n\n\tsector_size = get_unaligned_be32(&buffer[8]);\n\tlba = get_unaligned_be64(&buffer[0]);\n\n\tsd_read_protection_type(sdkp, buffer);\n\n\tif ((sizeof(sdkp->capacity) == 4) && (lba >= 0xffffffffULL)) {\n\t\tsd_printk(KERN_ERR, sdkp, \"Too big for this kernel. Use a \"\n\t\t\t\"kernel compiled with support for large block \"\n\t\t\t\"devices.\\n\");\n\t\tsdkp->capacity = 0;\n\t\treturn -EOVERFLOW;\n\t}\n\n\t/* Logical blocks per physical block exponent */\n\tsdkp->physical_block_size = (1 << (buffer[13] & 0xf)) * sector_size;\n\n\t/* Lowest aligned logical block */\n\talignment = ((buffer[14] & 0x3f) << 8 | buffer[15]) * sector_size;\n\tblk_queue_alignment_offset(sdp->request_queue, alignment);\n\tif (alignment && sdkp->first_scan)\n\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t  \"physical block alignment offset: %u\\n\", alignment);\n\n\tif (buffer[14] & 0x80) { /* LBPME */\n\t\tsdkp->lbpme = 1;\n\n\t\tif (buffer[14] & 0x40) /* LBPRZ */\n\t\t\tsdkp->lbprz = 1;\n\n\t\tsd_config_discard(sdkp, SD_LBP_WS16);\n\t}\n\n\tsdkp->capacity = lba + 1;\n\treturn sector_size;\n}\n\nstatic int read_capacity_10(struct scsi_disk *sdkp, struct scsi_device *sdp,\n\t\t\t\t\t\tunsigned char *buffer)\n{\n\tunsigned char cmd[16];\n\tstruct scsi_sense_hdr sshdr;\n\tint sense_valid = 0;\n\tint the_result;\n\tint retries = 3, reset_retries = READ_CAPACITY_RETRIES_ON_RESET;\n\tsector_t lba;\n\tunsigned sector_size;\n\n\tdo {\n\t\tcmd[0] = READ_CAPACITY;\n\t\tmemset(&cmd[1], 0, 9);\n\t\tmemset(buffer, 0, 8);\n\n\t\tthe_result = scsi_execute_req(sdp, cmd, DMA_FROM_DEVICE,\n\t\t\t\t\tbuffer, 8, &sshdr,\n\t\t\t\t\tSD_TIMEOUT, SD_MAX_RETRIES, NULL);\n\n\t\tif (media_not_present(sdkp, &sshdr))\n\t\t\treturn -ENODEV;\n\n\t\tif (the_result) {\n\t\t\tsense_valid = scsi_sense_valid(&sshdr);\n\t\t\tif (sense_valid &&\n\t\t\t    sshdr.sense_key == UNIT_ATTENTION &&\n\t\t\t    sshdr.asc == 0x29 && sshdr.ascq == 0x00)\n\t\t\t\t/* Device reset might occur several times,\n\t\t\t\t * give it one more chance */\n\t\t\t\tif (--reset_retries > 0)\n\t\t\t\t\tcontinue;\n\t\t}\n\t\tretries--;\n\n\t} while (the_result && retries);\n\n\tif (the_result) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"READ CAPACITY failed\\n\");\n\t\tread_capacity_error(sdkp, sdp, &sshdr, sense_valid, the_result);\n\t\treturn -EINVAL;\n\t}\n\n\tsector_size = get_unaligned_be32(&buffer[4]);\n\tlba = get_unaligned_be32(&buffer[0]);\n\n\tif (sdp->no_read_capacity_16 && (lba == 0xffffffff)) {\n\t\t/* Some buggy (usb cardreader) devices return an lba of\n\t\t   0xffffffff when the want to report a size of 0 (with\n\t\t   which they really mean no media is present) */\n\t\tsdkp->capacity = 0;\n\t\tsdkp->physical_block_size = sector_size;\n\t\treturn sector_size;\n\t}\n\n\tif ((sizeof(sdkp->capacity) == 4) && (lba == 0xffffffff)) {\n\t\tsd_printk(KERN_ERR, sdkp, \"Too big for this kernel. Use a \"\n\t\t\t\"kernel compiled with support for large block \"\n\t\t\t\"devices.\\n\");\n\t\tsdkp->capacity = 0;\n\t\treturn -EOVERFLOW;\n\t}\n\n\tsdkp->capacity = lba + 1;\n\tsdkp->physical_block_size = sector_size;\n\treturn sector_size;\n}\n\nstatic int sd_try_rc16_first(struct scsi_device *sdp)\n{\n\tif (sdp->host->max_cmd_len < 16)\n\t\treturn 0;\n\tif (sdp->scsi_level > SCSI_SPC_2)\n\t\treturn 1;\n\tif (scsi_device_protection(sdp))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * read disk capacity\n */\nstatic void\nsd_read_capacity(struct scsi_disk *sdkp, unsigned char *buffer)\n{\n\tint sector_size;\n\tstruct scsi_device *sdp = sdkp->device;\n\tsector_t old_capacity = sdkp->capacity;\n\n\tif (sd_try_rc16_first(sdp)) {\n\t\tsector_size = read_capacity_16(sdkp, sdp, buffer);\n\t\tif (sector_size == -EOVERFLOW)\n\t\t\tgoto got_data;\n\t\tif (sector_size == -ENODEV)\n\t\t\treturn;\n\t\tif (sector_size < 0)\n\t\t\tsector_size = read_capacity_10(sdkp, sdp, buffer);\n\t\tif (sector_size < 0)\n\t\t\treturn;\n\t} else {\n\t\tsector_size = read_capacity_10(sdkp, sdp, buffer);\n\t\tif (sector_size == -EOVERFLOW)\n\t\t\tgoto got_data;\n\t\tif (sector_size < 0)\n\t\t\treturn;\n\t\tif ((sizeof(sdkp->capacity) > 4) &&\n\t\t    (sdkp->capacity > 0xffffffffULL)) {\n\t\t\tint old_sector_size = sector_size;\n\t\t\tsd_printk(KERN_NOTICE, sdkp, \"Very big device. \"\n\t\t\t\t\t\"Trying to use READ CAPACITY(16).\\n\");\n\t\t\tsector_size = read_capacity_16(sdkp, sdp, buffer);\n\t\t\tif (sector_size < 0) {\n\t\t\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t\t\t\"Using 0xffffffff as device size\\n\");\n\t\t\t\tsdkp->capacity = 1 + (sector_t) 0xffffffff;\n\t\t\t\tsector_size = old_sector_size;\n\t\t\t\tgoto got_data;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Some devices are known to return the total number of blocks,\n\t * not the highest block number.  Some devices have versions\n\t * which do this and others which do not.  Some devices we might\n\t * suspect of doing this but we don't know for certain.\n\t *\n\t * If we know the reported capacity is wrong, decrement it.  If\n\t * we can only guess, then assume the number of blocks is even\n\t * (usually true but not always) and err on the side of lowering\n\t * the capacity.\n\t */\n\tif (sdp->fix_capacity ||\n\t    (sdp->guess_capacity && (sdkp->capacity & 0x01))) {\n\t\tsd_printk(KERN_INFO, sdkp, \"Adjusting the sector count \"\n\t\t\t\t\"from its reported value: %llu\\n\",\n\t\t\t\t(unsigned long long) sdkp->capacity);\n\t\t--sdkp->capacity;\n\t}\n\ngot_data:\n\tif (sector_size == 0) {\n\t\tsector_size = 512;\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Sector size 0 reported, \"\n\t\t\t  \"assuming 512.\\n\");\n\t}\n\n\tif (sector_size != 512 &&\n\t    sector_size != 1024 &&\n\t    sector_size != 2048 &&\n\t    sector_size != 4096 &&\n\t    sector_size != 256) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Unsupported sector size %d.\\n\",\n\t\t\t  sector_size);\n\t\t/*\n\t\t * The user might want to re-format the drive with\n\t\t * a supported sectorsize.  Once this happens, it\n\t\t * would be relatively trivial to set the thing up.\n\t\t * For this reason, we leave the thing in the table.\n\t\t */\n\t\tsdkp->capacity = 0;\n\t\t/*\n\t\t * set a bogus sector size so the normal read/write\n\t\t * logic in the block layer will eventually refuse any\n\t\t * request on this device without tripping over power\n\t\t * of two sector size assumptions\n\t\t */\n\t\tsector_size = 512;\n\t}\n\tblk_queue_logical_block_size(sdp->request_queue, sector_size);\n\n\t{\n\t\tchar cap_str_2[10], cap_str_10[10];\n\t\tu64 sz = (u64)sdkp->capacity << ilog2(sector_size);\n\n\t\tstring_get_size(sz, STRING_UNITS_2, cap_str_2,\n\t\t\t\tsizeof(cap_str_2));\n\t\tstring_get_size(sz, STRING_UNITS_10, cap_str_10,\n\t\t\t\tsizeof(cap_str_10));\n\n\t\tif (sdkp->first_scan || old_capacity != sdkp->capacity) {\n\t\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t\t  \"%llu %d-byte logical blocks: (%s/%s)\\n\",\n\t\t\t\t  (unsigned long long)sdkp->capacity,\n\t\t\t\t  sector_size, cap_str_10, cap_str_2);\n\n\t\t\tif (sdkp->physical_block_size != sector_size)\n\t\t\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t\t\t  \"%u-byte physical blocks\\n\",\n\t\t\t\t\t  sdkp->physical_block_size);\n\t\t}\n\t}\n\n\t/* Rescale capacity to 512-byte units */\n\tif (sector_size == 4096)\n\t\tsdkp->capacity <<= 3;\n\telse if (sector_size == 2048)\n\t\tsdkp->capacity <<= 2;\n\telse if (sector_size == 1024)\n\t\tsdkp->capacity <<= 1;\n\telse if (sector_size == 256)\n\t\tsdkp->capacity >>= 1;\n\n\tblk_queue_physical_block_size(sdp->request_queue,\n\t\t\t\t      sdkp->physical_block_size);\n\tsdkp->device->sector_size = sector_size;\n}\n\n/* called with buffer of length 512 */\nstatic inline int\nsd_do_mode_sense(struct scsi_device *sdp, int dbd, int modepage,\n\t\t unsigned char *buffer, int len, struct scsi_mode_data *data,\n\t\t struct scsi_sense_hdr *sshdr)\n{\n\treturn scsi_mode_sense(sdp, dbd, modepage, buffer, len,\n\t\t\t       SD_TIMEOUT, SD_MAX_RETRIES, data,\n\t\t\t       sshdr);\n}\n\n/*\n * read write protect setting, if possible - called only in sd_revalidate_disk()\n * called with buffer of length SD_BUF_SIZE\n */\nstatic void\nsd_read_write_protect_flag(struct scsi_disk *sdkp, unsigned char *buffer)\n{\n\tint res;\n\tstruct scsi_device *sdp = sdkp->device;\n\tstruct scsi_mode_data data;\n\tint old_wp = sdkp->write_prot;\n\n\tset_disk_ro(sdkp->disk, 0);\n\tif (sdp->skip_ms_page_3f) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Assuming Write Enabled\\n\");\n\t\treturn;\n\t}\n\n\tif (sdp->use_192_bytes_for_3f) {\n\t\tres = sd_do_mode_sense(sdp, 0, 0x3F, buffer, 192, &data, NULL);\n\t} else {\n\t\t/*\n\t\t * First attempt: ask for all pages (0x3F), but only 4 bytes.\n\t\t * We have to start carefully: some devices hang if we ask\n\t\t * for more than is available.\n\t\t */\n\t\tres = sd_do_mode_sense(sdp, 0, 0x3F, buffer, 4, &data, NULL);\n\n\t\t/*\n\t\t * Second attempt: ask for page 0 When only page 0 is\n\t\t * implemented, a request for page 3F may return Sense Key\n\t\t * 5: Illegal Request, Sense Code 24: Invalid field in\n\t\t * CDB.\n\t\t */\n\t\tif (!scsi_status_is_good(res))\n\t\t\tres = sd_do_mode_sense(sdp, 0, 0, buffer, 4, &data, NULL);\n\n\t\t/*\n\t\t * Third attempt: ask 255 bytes, as we did earlier.\n\t\t */\n\t\tif (!scsi_status_is_good(res))\n\t\t\tres = sd_do_mode_sense(sdp, 0, 0x3F, buffer, 255,\n\t\t\t\t\t       &data, NULL);\n\t}\n\n\tif (!scsi_status_is_good(res)) {\n\t\tsd_printk(KERN_WARNING, sdkp,\n\t\t\t  \"Test WP failed, assume Write Enabled\\n\");\n\t} else {\n\t\tsdkp->write_prot = ((data.device_specific & 0x80) != 0);\n\t\tset_disk_ro(sdkp->disk, sdkp->write_prot);\n\t\tif (sdkp->first_scan || old_wp != sdkp->write_prot) {\n\t\t\tsd_printk(KERN_NOTICE, sdkp, \"Write Protect is %s\\n\",\n\t\t\t\t  sdkp->write_prot ? \"on\" : \"off\");\n\t\t\tsd_printk(KERN_DEBUG, sdkp,\n\t\t\t\t  \"Mode Sense: %02x %02x %02x %02x\\n\",\n\t\t\t\t  buffer[0], buffer[1], buffer[2], buffer[3]);\n\t\t}\n\t}\n}\n\n/*\n * sd_read_cache_type - called only from sd_revalidate_disk()\n * called with buffer of length SD_BUF_SIZE\n */\nstatic void\nsd_read_cache_type(struct scsi_disk *sdkp, unsigned char *buffer)\n{\n\tint len = 0, res;\n\tstruct scsi_device *sdp = sdkp->device;\n\n\tint dbd;\n\tint modepage;\n\tint first_len;\n\tstruct scsi_mode_data data;\n\tstruct scsi_sense_hdr sshdr;\n\tint old_wce = sdkp->WCE;\n\tint old_rcd = sdkp->RCD;\n\tint old_dpofua = sdkp->DPOFUA;\n\n\tfirst_len = 4;\n\tif (sdp->skip_ms_page_8) {\n\t\tif (sdp->type == TYPE_RBC)\n\t\t\tgoto defaults;\n\t\telse {\n\t\t\tif (sdp->skip_ms_page_3f)\n\t\t\t\tgoto defaults;\n\t\t\tmodepage = 0x3F;\n\t\t\tif (sdp->use_192_bytes_for_3f)\n\t\t\t\tfirst_len = 192;\n\t\t\tdbd = 0;\n\t\t}\n\t} else if (sdp->type == TYPE_RBC) {\n\t\tmodepage = 6;\n\t\tdbd = 8;\n\t} else {\n\t\tmodepage = 8;\n\t\tdbd = 0;\n\t}\n\n\t/* cautiously ask */\n\tres = sd_do_mode_sense(sdp, dbd, modepage, buffer, first_len,\n\t\t\t&data, &sshdr);\n\n\tif (!scsi_status_is_good(res))\n\t\tgoto bad_sense;\n\n\tif (!data.header_length) {\n\t\tmodepage = 6;\n\t\tfirst_len = 0;\n\t\tsd_printk(KERN_ERR, sdkp, \"Missing header in MODE_SENSE response\\n\");\n\t}\n\n\t/* that went OK, now ask for the proper length */\n\tlen = data.length;\n\n\t/*\n\t * We're only interested in the first three bytes, actually.\n\t * But the data cache page is defined for the first 20.\n\t */\n\tif (len < 3)\n\t\tgoto bad_sense;\n\telse if (len > SD_BUF_SIZE) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Truncating mode parameter \"\n\t\t\t  \"data from %d to %d bytes\\n\", len, SD_BUF_SIZE);\n\t\tlen = SD_BUF_SIZE;\n\t}\n\tif (modepage == 0x3F && sdp->use_192_bytes_for_3f)\n\t\tlen = 192;\n\n\t/* Get the data */\n\tif (len > first_len)\n\t\tres = sd_do_mode_sense(sdp, dbd, modepage, buffer, len,\n\t\t\t\t&data, &sshdr);\n\n\tif (scsi_status_is_good(res)) {\n\t\tint offset = data.header_length + data.block_descriptor_length;\n\n\t\twhile (offset < len) {\n\t\t\tu8 page_code = buffer[offset] & 0x3F;\n\t\t\tu8 spf       = buffer[offset] & 0x40;\n\n\t\t\tif (page_code == 8 || page_code == 6) {\n\t\t\t\t/* We're interested only in the first 3 bytes.\n\t\t\t\t */\n\t\t\t\tif (len - offset <= 2) {\n\t\t\t\t\tsd_printk(KERN_ERR, sdkp, \"Incomplete \"\n\t\t\t\t\t\t  \"mode parameter data\\n\");\n\t\t\t\t\tgoto defaults;\n\t\t\t\t} else {\n\t\t\t\t\tmodepage = page_code;\n\t\t\t\t\tgoto Page_found;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* Go to the next page */\n\t\t\t\tif (spf && len - offset > 3)\n\t\t\t\t\toffset += 4 + (buffer[offset+2] << 8) +\n\t\t\t\t\t\tbuffer[offset+3];\n\t\t\t\telse if (!spf && len - offset > 1)\n\t\t\t\t\toffset += 2 + buffer[offset+1];\n\t\t\t\telse {\n\t\t\t\t\tsd_printk(KERN_ERR, sdkp, \"Incomplete \"\n\t\t\t\t\t\t  \"mode parameter data\\n\");\n\t\t\t\t\tgoto defaults;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (modepage == 0x3F) {\n\t\t\tsd_printk(KERN_ERR, sdkp, \"No Caching mode page \"\n\t\t\t\t  \"present\\n\");\n\t\t\tgoto defaults;\n\t\t} else if ((buffer[offset] & 0x3f) != modepage) {\n\t\t\tsd_printk(KERN_ERR, sdkp, \"Got wrong page\\n\");\n\t\t\tgoto defaults;\n\t\t}\n\tPage_found:\n\t\tif (modepage == 8) {\n\t\t\tsdkp->WCE = ((buffer[offset + 2] & 0x04) != 0);\n\t\t\tsdkp->RCD = ((buffer[offset + 2] & 0x01) != 0);\n\t\t} else {\n\t\t\tsdkp->WCE = ((buffer[offset + 2] & 0x01) == 0);\n\t\t\tsdkp->RCD = 0;\n\t\t}\n\n\t\tsdkp->DPOFUA = (data.device_specific & 0x10) != 0;\n\t\tif (sdkp->DPOFUA && !sdkp->device->use_10_for_rw) {\n\t\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t\t  \"Uses READ/WRITE(6), disabling FUA\\n\");\n\t\t\tsdkp->DPOFUA = 0;\n\t\t}\n\n\t\tif (sdkp->first_scan || old_wce != sdkp->WCE ||\n\t\t    old_rcd != sdkp->RCD || old_dpofua != sdkp->DPOFUA)\n\t\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t\t  \"Write cache: %s, read cache: %s, %s\\n\",\n\t\t\t\t  sdkp->WCE ? \"enabled\" : \"disabled\",\n\t\t\t\t  sdkp->RCD ? \"disabled\" : \"enabled\",\n\t\t\t\t  sdkp->DPOFUA ? \"supports DPO and FUA\"\n\t\t\t\t  : \"doesn't support DPO or FUA\");\n\n\t\treturn;\n\t}\n\nbad_sense:\n\tif (scsi_sense_valid(&sshdr) &&\n\t    sshdr.sense_key == ILLEGAL_REQUEST &&\n\t    sshdr.asc == 0x24 && sshdr.ascq == 0x0)\n\t\t/* Invalid field in CDB */\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Cache data unavailable\\n\");\n\telse\n\t\tsd_printk(KERN_ERR, sdkp, \"Asking for cache data failed\\n\");\n\ndefaults:\n\tsd_printk(KERN_ERR, sdkp, \"Assuming drive cache: write through\\n\");\n\tsdkp->WCE = 0;\n\tsdkp->RCD = 0;\n\tsdkp->DPOFUA = 0;\n}\n\n/*\n * The ATO bit indicates whether the DIF application tag is available\n * for use by the operating system.\n */\nstatic void sd_read_app_tag_own(struct scsi_disk *sdkp, unsigned char *buffer)\n{\n\tint res, offset;\n\tstruct scsi_device *sdp = sdkp->device;\n\tstruct scsi_mode_data data;\n\tstruct scsi_sense_hdr sshdr;\n\n\tif (sdp->type != TYPE_DISK)\n\t\treturn;\n\n\tif (sdkp->protection_type == 0)\n\t\treturn;\n\n\tres = scsi_mode_sense(sdp, 1, 0x0a, buffer, 36, SD_TIMEOUT,\n\t\t\t      SD_MAX_RETRIES, &data, &sshdr);\n\n\tif (!scsi_status_is_good(res) || !data.header_length ||\n\t    data.length < 6) {\n\t\tsd_printk(KERN_WARNING, sdkp,\n\t\t\t  \"getting Control mode page failed, assume no ATO\\n\");\n\n\t\tif (scsi_sense_valid(&sshdr))\n\t\t\tsd_print_sense_hdr(sdkp, &sshdr);\n\n\t\treturn;\n\t}\n\n\toffset = data.header_length + data.block_descriptor_length;\n\n\tif ((buffer[offset] & 0x3f) != 0x0a) {\n\t\tsd_printk(KERN_ERR, sdkp, \"ATO Got wrong page\\n\");\n\t\treturn;\n\t}\n\n\tif ((buffer[offset + 5] & 0x80) == 0)\n\t\treturn;\n\n\tsdkp->ATO = 1;\n\n\treturn;\n}\n\n/**\n * sd_read_block_limits - Query disk device for preferred I/O sizes.\n * @disk: disk to query\n */\nstatic void sd_read_block_limits(struct scsi_disk *sdkp)\n{\n\tunsigned int sector_sz = sdkp->device->sector_size;\n\tconst int vpd_len = 64;\n\tunsigned char *buffer = kmalloc(vpd_len, GFP_KERNEL);\n\n\tif (!buffer ||\n\t    /* Block Limits VPD */\n\t    scsi_get_vpd_page(sdkp->device, 0xb0, buffer, vpd_len))\n\t\tgoto out;\n\n\tblk_queue_io_min(sdkp->disk->queue,\n\t\t\t get_unaligned_be16(&buffer[6]) * sector_sz);\n\tblk_queue_io_opt(sdkp->disk->queue,\n\t\t\t get_unaligned_be32(&buffer[12]) * sector_sz);\n\n\tif (buffer[3] == 0x3c) {\n\t\tunsigned int lba_count, desc_count;\n\n\t\tsdkp->max_ws_blocks =\n\t\t\t(u32) min_not_zero(get_unaligned_be64(&buffer[36]),\n\t\t\t\t\t   (u64)0xffffffff);\n\n\t\tif (!sdkp->lbpme)\n\t\t\tgoto out;\n\n\t\tlba_count = get_unaligned_be32(&buffer[20]);\n\t\tdesc_count = get_unaligned_be32(&buffer[24]);\n\n\t\tif (lba_count && desc_count)\n\t\t\tsdkp->max_unmap_blocks = lba_count;\n\n\t\tsdkp->unmap_granularity = get_unaligned_be32(&buffer[28]);\n\n\t\tif (buffer[32] & 0x80)\n\t\t\tsdkp->unmap_alignment =\n\t\t\t\tget_unaligned_be32(&buffer[32]) & ~(1 << 31);\n\n\t\tif (!sdkp->lbpvpd) { /* LBP VPD page not provided */\n\n\t\t\tif (sdkp->max_unmap_blocks)\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_UNMAP);\n\t\t\telse\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_WS16);\n\n\t\t} else {\t/* LBP VPD page tells us what to use */\n\n\t\t\tif (sdkp->lbpu && sdkp->max_unmap_blocks)\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_UNMAP);\n\t\t\telse if (sdkp->lbpws)\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_WS16);\n\t\t\telse if (sdkp->lbpws10)\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_WS10);\n\t\t\telse\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_DISABLE);\n\t\t}\n\t}\n\n out:\n\tkfree(buffer);\n}\n\n/**\n * sd_read_block_characteristics - Query block dev. characteristics\n * @disk: disk to query\n */\nstatic void sd_read_block_characteristics(struct scsi_disk *sdkp)\n{\n\tunsigned char *buffer;\n\tu16 rot;\n\tconst int vpd_len = 64;\n\n\tbuffer = kmalloc(vpd_len, GFP_KERNEL);\n\n\tif (!buffer ||\n\t    /* Block Device Characteristics VPD */\n\t    scsi_get_vpd_page(sdkp->device, 0xb1, buffer, vpd_len))\n\t\tgoto out;\n\n\trot = get_unaligned_be16(&buffer[4]);\n\n\tif (rot == 1)\n\t\tqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, sdkp->disk->queue);\n\n out:\n\tkfree(buffer);\n}\n\n/**\n * sd_read_block_provisioning - Query provisioning VPD page\n * @disk: disk to query\n */\nstatic void sd_read_block_provisioning(struct scsi_disk *sdkp)\n{\n\tunsigned char *buffer;\n\tconst int vpd_len = 8;\n\n\tif (sdkp->lbpme == 0)\n\t\treturn;\n\n\tbuffer = kmalloc(vpd_len, GFP_KERNEL);\n\n\tif (!buffer || scsi_get_vpd_page(sdkp->device, 0xb2, buffer, vpd_len))\n\t\tgoto out;\n\n\tsdkp->lbpvpd\t= 1;\n\tsdkp->lbpu\t= (buffer[5] >> 7) & 1;\t/* UNMAP */\n\tsdkp->lbpws\t= (buffer[5] >> 6) & 1;\t/* WRITE SAME(16) with UNMAP */\n\tsdkp->lbpws10\t= (buffer[5] >> 5) & 1;\t/* WRITE SAME(10) with UNMAP */\n\n out:\n\tkfree(buffer);\n}\n\nstatic int sd_try_extended_inquiry(struct scsi_device *sdp)\n{\n\t/*\n\t * Although VPD inquiries can go to SCSI-2 type devices,\n\t * some USB ones crash on receiving them, and the pages\n\t * we currently ask for are for SPC-3 and beyond\n\t */\n\tif (sdp->scsi_level > SCSI_SPC_2)\n\t\treturn 1;\n\treturn 0;\n}\n\n/**\n *\tsd_revalidate_disk - called the first time a new disk is seen,\n *\tperforms disk spin up, read_capacity, etc.\n *\t@disk: struct gendisk we care about\n **/\nstatic int sd_revalidate_disk(struct gendisk *disk)\n{\n\tstruct scsi_disk *sdkp = scsi_disk(disk);\n\tstruct scsi_device *sdp = sdkp->device;\n\tunsigned char *buffer;\n\tunsigned flush = 0;\n\n\tSCSI_LOG_HLQUEUE(3, sd_printk(KERN_INFO, sdkp,\n\t\t\t\t      \"sd_revalidate_disk\\n\"));\n\n\t/*\n\t * If the device is offline, don't try and read capacity or any\n\t * of the other niceties.\n\t */\n\tif (!scsi_device_online(sdp))\n\t\tgoto out;\n\n\tbuffer = kmalloc(SD_BUF_SIZE, GFP_KERNEL);\n\tif (!buffer) {\n\t\tsd_printk(KERN_WARNING, sdkp, \"sd_revalidate_disk: Memory \"\n\t\t\t  \"allocation failure.\\n\");\n\t\tgoto out;\n\t}\n\n\tsd_spinup_disk(sdkp);\n\n\t/*\n\t * Without media there is no reason to ask; moreover, some devices\n\t * react badly if we do.\n\t */\n\tif (sdkp->media_present) {\n\t\tsd_read_capacity(sdkp, buffer);\n\n\t\tif (sd_try_extended_inquiry(sdp)) {\n\t\t\tsd_read_block_provisioning(sdkp);\n\t\t\tsd_read_block_limits(sdkp);\n\t\t\tsd_read_block_characteristics(sdkp);\n\t\t}\n\n\t\tsd_read_write_protect_flag(sdkp, buffer);\n\t\tsd_read_cache_type(sdkp, buffer);\n\t\tsd_read_app_tag_own(sdkp, buffer);\n\t}\n\n\tsdkp->first_scan = 0;\n\n\t/*\n\t * We now have all cache related info, determine how we deal\n\t * with flush requests.\n\t */\n\tif (sdkp->WCE) {\n\t\tflush |= REQ_FLUSH;\n\t\tif (sdkp->DPOFUA)\n\t\t\tflush |= REQ_FUA;\n\t}\n\n\tblk_queue_flush(sdkp->disk->queue, flush);\n\n\tset_capacity(disk, sdkp->capacity);\n\tkfree(buffer);\n\n out:\n\treturn 0;\n}\n\n/**\n *\tsd_unlock_native_capacity - unlock native capacity\n *\t@disk: struct gendisk to set capacity for\n *\n *\tBlock layer calls this function if it detects that partitions\n *\ton @disk reach beyond the end of the device.  If the SCSI host\n *\timplements ->unlock_native_capacity() method, it's invoked to\n *\tgive it a chance to adjust the device capacity.\n *\n *\tCONTEXT:\n *\tDefined by block layer.  Might sleep.\n */\nstatic void sd_unlock_native_capacity(struct gendisk *disk)\n{\n\tstruct scsi_device *sdev = scsi_disk(disk)->device;\n\n\tif (sdev->host->hostt->unlock_native_capacity)\n\t\tsdev->host->hostt->unlock_native_capacity(sdev);\n}\n\n/**\n *\tsd_format_disk_name - format disk name\n *\t@prefix: name prefix - ie. \"sd\" for SCSI disks\n *\t@index: index of the disk to format name for\n *\t@buf: output buffer\n *\t@buflen: length of the output buffer\n *\n *\tSCSI disk names starts at sda.  The 26th device is sdz and the\n *\t27th is sdaa.  The last one for two lettered suffix is sdzz\n *\twhich is followed by sdaaa.\n *\n *\tThis is basically 26 base counting with one extra 'nil' entry\n *\tat the beginning from the second digit on and can be\n *\tdetermined using similar method as 26 base conversion with the\n *\tindex shifted -1 after each digit is computed.\n *\n *\tCONTEXT:\n *\tDon't care.\n *\n *\tRETURNS:\n *\t0 on success, -errno on failure.\n */\nstatic int sd_format_disk_name(char *prefix, int index, char *buf, int buflen)\n{\n\tconst int base = 'z' - 'a' + 1;\n\tchar *begin = buf + strlen(prefix);\n\tchar *end = buf + buflen;\n\tchar *p;\n\tint unit;\n\n\tp = end - 1;\n\t*p = '\\0';\n\tunit = base;\n\tdo {\n\t\tif (p == begin)\n\t\t\treturn -EINVAL;\n\t\t*--p = 'a' + (index % unit);\n\t\tindex = (index / unit) - 1;\n\t} while (index >= 0);\n\n\tmemmove(begin, p, end - p);\n\tmemcpy(buf, prefix, strlen(prefix));\n\n\treturn 0;\n}\n\n/*\n * The asynchronous part of sd_probe\n */\nstatic void sd_probe_async(void *data, async_cookie_t cookie)\n{\n\tstruct scsi_disk *sdkp = data;\n\tstruct scsi_device *sdp;\n\tstruct gendisk *gd;\n\tu32 index;\n\tstruct device *dev;\n\n\tsdp = sdkp->device;\n\tgd = sdkp->disk;\n\tindex = sdkp->index;\n\tdev = &sdp->sdev_gendev;\n\n\tgd->major = sd_major((index & 0xf0) >> 4);\n\tgd->first_minor = ((index & 0xf) << 4) | (index & 0xfff00);\n\tgd->minors = SD_MINORS;\n\n\tgd->fops = &sd_fops;\n\tgd->private_data = &sdkp->driver;\n\tgd->queue = sdkp->device->request_queue;\n\n\t/* defaults, until the device tells us otherwise */\n\tsdp->sector_size = 512;\n\tsdkp->capacity = 0;\n\tsdkp->media_present = 1;\n\tsdkp->write_prot = 0;\n\tsdkp->WCE = 0;\n\tsdkp->RCD = 0;\n\tsdkp->ATO = 0;\n\tsdkp->first_scan = 1;\n\n\tsd_revalidate_disk(gd);\n\n\tblk_queue_prep_rq(sdp->request_queue, sd_prep_fn);\n\tblk_queue_unprep_rq(sdp->request_queue, sd_unprep_fn);\n\n\tgd->driverfs_dev = &sdp->sdev_gendev;\n\tgd->flags = GENHD_FL_EXT_DEVT;\n\tif (sdp->removable) {\n\t\tgd->flags |= GENHD_FL_REMOVABLE;\n\t\tgd->events |= DISK_EVENT_MEDIA_CHANGE;\n\t}\n\n\tadd_disk(gd);\n\tsd_dif_config_host(sdkp);\n\n\tsd_revalidate_disk(gd);\n\n\tsd_printk(KERN_NOTICE, sdkp, \"Attached SCSI %sdisk\\n\",\n\t\t  sdp->removable ? \"removable \" : \"\");\n\tscsi_autopm_put_device(sdp);\n\tput_device(&sdkp->dev);\n}\n\n/**\n *\tsd_probe - called during driver initialization and whenever a\n *\tnew scsi device is attached to the system. It is called once\n *\tfor each scsi device (not just disks) present.\n *\t@dev: pointer to device object\n *\n *\tReturns 0 if successful (or not interested in this scsi device \n *\t(e.g. scanner)); 1 when there is an error.\n *\n *\tNote: this function is invoked from the scsi mid-level.\n *\tThis function sets up the mapping between a given \n *\t<host,channel,id,lun> (found in sdp) and new device name \n *\t(e.g. /dev/sda). More precisely it is the block device major \n *\tand minor number that is chosen here.\n *\n *\tAssume sd_attach is not re-entrant (for time being)\n *\tAlso think about sd_attach() and sd_remove() running coincidentally.\n **/\nstatic int sd_probe(struct device *dev)\n{\n\tstruct scsi_device *sdp = to_scsi_device(dev);\n\tstruct scsi_disk *sdkp;\n\tstruct gendisk *gd;\n\tint index;\n\tint error;\n\n\terror = -ENODEV;\n\tif (sdp->type != TYPE_DISK && sdp->type != TYPE_MOD && sdp->type != TYPE_RBC)\n\t\tgoto out;\n\n\tSCSI_LOG_HLQUEUE(3, sdev_printk(KERN_INFO, sdp,\n\t\t\t\t\t\"sd_attach\\n\"));\n\n\terror = -ENOMEM;\n\tsdkp = kzalloc(sizeof(*sdkp), GFP_KERNEL);\n\tif (!sdkp)\n\t\tgoto out;\n\n\tgd = alloc_disk(SD_MINORS);\n\tif (!gd)\n\t\tgoto out_free;\n\n\tdo {\n\t\tif (!ida_pre_get(&sd_index_ida, GFP_KERNEL))\n\t\t\tgoto out_put;\n\n\t\tspin_lock(&sd_index_lock);\n\t\terror = ida_get_new(&sd_index_ida, &index);\n\t\tspin_unlock(&sd_index_lock);\n\t} while (error == -EAGAIN);\n\n\tif (error) {\n\t\tsdev_printk(KERN_WARNING, sdp, \"sd_probe: memory exhausted.\\n\");\n\t\tgoto out_put;\n\t}\n\n\terror = sd_format_disk_name(\"sd\", index, gd->disk_name, DISK_NAME_LEN);\n\tif (error) {\n\t\tsdev_printk(KERN_WARNING, sdp, \"SCSI disk (sd) name length exceeded.\\n\");\n\t\tgoto out_free_index;\n\t}\n\n\tsdkp->device = sdp;\n\tsdkp->driver = &sd_template;\n\tsdkp->disk = gd;\n\tsdkp->index = index;\n\tatomic_set(&sdkp->openers, 0);\n\n\tif (!sdp->request_queue->rq_timeout) {\n\t\tif (sdp->type != TYPE_MOD)\n\t\t\tblk_queue_rq_timeout(sdp->request_queue, SD_TIMEOUT);\n\t\telse\n\t\t\tblk_queue_rq_timeout(sdp->request_queue,\n\t\t\t\t\t     SD_MOD_TIMEOUT);\n\t}\n\n\tdevice_initialize(&sdkp->dev);\n\tsdkp->dev.parent = dev;\n\tsdkp->dev.class = &sd_disk_class;\n\tdev_set_name(&sdkp->dev, dev_name(dev));\n\n\tif (device_add(&sdkp->dev))\n\t\tgoto out_free_index;\n\n\tget_device(dev);\n\tdev_set_drvdata(dev, sdkp);\n\n\tget_device(&sdkp->dev);\t/* prevent release before async_schedule */\n\tasync_schedule(sd_probe_async, sdkp);\n\n\treturn 0;\n\n out_free_index:\n\tspin_lock(&sd_index_lock);\n\tida_remove(&sd_index_ida, index);\n\tspin_unlock(&sd_index_lock);\n out_put:\n\tput_disk(gd);\n out_free:\n\tkfree(sdkp);\n out:\n\treturn error;\n}\n\n/**\n *\tsd_remove - called whenever a scsi disk (previously recognized by\n *\tsd_probe) is detached from the system. It is called (potentially\n *\tmultiple times) during sd module unload.\n *\t@sdp: pointer to mid level scsi device object\n *\n *\tNote: this function is invoked from the scsi mid-level.\n *\tThis function potentially frees up a device name (e.g. /dev/sdc)\n *\tthat could be re-used by a subsequent sd_probe().\n *\tThis function is not called when the built-in sd driver is \"exit-ed\".\n **/\nstatic int sd_remove(struct device *dev)\n{\n\tstruct scsi_disk *sdkp;\n\n\tsdkp = dev_get_drvdata(dev);\n\tscsi_autopm_get_device(sdkp->device);\n\n\tasync_synchronize_full();\n\tblk_queue_prep_rq(sdkp->device->request_queue, scsi_prep_fn);\n\tblk_queue_unprep_rq(sdkp->device->request_queue, NULL);\n\tdevice_del(&sdkp->dev);\n\tdel_gendisk(sdkp->disk);\n\tsd_shutdown(dev);\n\n\tmutex_lock(&sd_ref_mutex);\n\tdev_set_drvdata(dev, NULL);\n\tput_device(&sdkp->dev);\n\tmutex_unlock(&sd_ref_mutex);\n\n\treturn 0;\n}\n\n/**\n *\tscsi_disk_release - Called to free the scsi_disk structure\n *\t@dev: pointer to embedded class device\n *\n *\tsd_ref_mutex must be held entering this routine.  Because it is\n *\tcalled on last put, you should always use the scsi_disk_get()\n *\tscsi_disk_put() helpers which manipulate the semaphore directly\n *\tand never do a direct put_device.\n **/\nstatic void scsi_disk_release(struct device *dev)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct gendisk *disk = sdkp->disk;\n\t\n\tspin_lock(&sd_index_lock);\n\tida_remove(&sd_index_ida, sdkp->index);\n\tspin_unlock(&sd_index_lock);\n\n\tdisk->private_data = NULL;\n\tput_disk(disk);\n\tput_device(&sdkp->device->sdev_gendev);\n\n\tkfree(sdkp);\n}\n\nstatic int sd_start_stop_device(struct scsi_disk *sdkp, int start)\n{\n\tunsigned char cmd[6] = { START_STOP };\t/* START_VALID */\n\tstruct scsi_sense_hdr sshdr;\n\tstruct scsi_device *sdp = sdkp->device;\n\tint res;\n\n\tif (start)\n\t\tcmd[4] |= 1;\t/* START */\n\n\tif (sdp->start_stop_pwr_cond)\n\t\tcmd[4] |= start ? 1 << 4 : 3 << 4;\t/* Active or Standby */\n\n\tif (!scsi_device_online(sdp))\n\t\treturn -ENODEV;\n\n\tres = scsi_execute_req(sdp, cmd, DMA_NONE, NULL, 0, &sshdr,\n\t\t\t       SD_TIMEOUT, SD_MAX_RETRIES, NULL);\n\tif (res) {\n\t\tsd_printk(KERN_WARNING, sdkp, \"START_STOP FAILED\\n\");\n\t\tsd_print_result(sdkp, res);\n\t\tif (driver_byte(res) & DRIVER_SENSE)\n\t\t\tsd_print_sense_hdr(sdkp, &sshdr);\n\t}\n\n\treturn res;\n}\n\n/*\n * Send a SYNCHRONIZE CACHE instruction down to the device through\n * the normal SCSI command structure.  Wait for the command to\n * complete.\n */\nstatic void sd_shutdown(struct device *dev)\n{\n\tstruct scsi_disk *sdkp = scsi_disk_get_from_dev(dev);\n\n\tif (!sdkp)\n\t\treturn;         /* this can happen */\n\n\tif (pm_runtime_suspended(dev))\n\t\tgoto exit;\n\n\tif (sdkp->WCE) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Synchronizing SCSI cache\\n\");\n\t\tsd_sync_cache(sdkp);\n\t}\n\n\tif (system_state != SYSTEM_RESTART && sdkp->device->manage_start_stop) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Stopping disk\\n\");\n\t\tsd_start_stop_device(sdkp, 0);\n\t}\n\nexit:\n\tscsi_disk_put(sdkp);\n}\n\nstatic int sd_suspend(struct device *dev, pm_message_t mesg)\n{\n\tstruct scsi_disk *sdkp = scsi_disk_get_from_dev(dev);\n\tint ret = 0;\n\n\tif (!sdkp)\n\t\treturn 0;\t/* this can happen */\n\n\tif (sdkp->WCE) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Synchronizing SCSI cache\\n\");\n\t\tret = sd_sync_cache(sdkp);\n\t\tif (ret)\n\t\t\tgoto done;\n\t}\n\n\tif ((mesg.event & PM_EVENT_SLEEP) && sdkp->device->manage_start_stop) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Stopping disk\\n\");\n\t\tret = sd_start_stop_device(sdkp, 0);\n\t}\n\ndone:\n\tscsi_disk_put(sdkp);\n\treturn ret;\n}\n\nstatic int sd_resume(struct device *dev)\n{\n\tstruct scsi_disk *sdkp = scsi_disk_get_from_dev(dev);\n\tint ret = 0;\n\n\tif (!sdkp->device->manage_start_stop)\n\t\tgoto done;\n\n\tsd_printk(KERN_NOTICE, sdkp, \"Starting disk\\n\");\n\tret = sd_start_stop_device(sdkp, 1);\n\ndone:\n\tscsi_disk_put(sdkp);\n\treturn ret;\n}\n\n/**\n *\tinit_sd - entry point for this driver (both when built in or when\n *\ta module).\n *\n *\tNote: this function registers this driver with the scsi mid-level.\n **/\nstatic int __init init_sd(void)\n{\n\tint majors = 0, i, err;\n\n\tSCSI_LOG_HLQUEUE(3, printk(\"init_sd: sd driver entry point\\n\"));\n\n\tfor (i = 0; i < SD_MAJORS; i++)\n\t\tif (register_blkdev(sd_major(i), \"sd\") == 0)\n\t\t\tmajors++;\n\n\tif (!majors)\n\t\treturn -ENODEV;\n\n\terr = class_register(&sd_disk_class);\n\tif (err)\n\t\tgoto err_out;\n\n\terr = scsi_register_driver(&sd_template.gendrv);\n\tif (err)\n\t\tgoto err_out_class;\n\n\tsd_cdb_cache = kmem_cache_create(\"sd_ext_cdb\", SD_EXT_CDB_SIZE,\n\t\t\t\t\t 0, 0, NULL);\n\tif (!sd_cdb_cache) {\n\t\tprintk(KERN_ERR \"sd: can't init extended cdb cache\\n\");\n\t\tgoto err_out_class;\n\t}\n\n\tsd_cdb_pool = mempool_create_slab_pool(SD_MEMPOOL_SIZE, sd_cdb_cache);\n\tif (!sd_cdb_pool) {\n\t\tprintk(KERN_ERR \"sd: can't init extended cdb pool\\n\");\n\t\tgoto err_out_cache;\n\t}\n\n\treturn 0;\n\nerr_out_cache:\n\tkmem_cache_destroy(sd_cdb_cache);\n\nerr_out_class:\n\tclass_unregister(&sd_disk_class);\nerr_out:\n\tfor (i = 0; i < SD_MAJORS; i++)\n\t\tunregister_blkdev(sd_major(i), \"sd\");\n\treturn err;\n}\n\n/**\n *\texit_sd - exit point for this driver (when it is a module).\n *\n *\tNote: this function unregisters this driver from the scsi mid-level.\n **/\nstatic void __exit exit_sd(void)\n{\n\tint i;\n\n\tSCSI_LOG_HLQUEUE(3, printk(\"exit_sd: exiting sd driver\\n\"));\n\n\tmempool_destroy(sd_cdb_pool);\n\tkmem_cache_destroy(sd_cdb_cache);\n\n\tscsi_unregister_driver(&sd_template.gendrv);\n\tclass_unregister(&sd_disk_class);\n\n\tfor (i = 0; i < SD_MAJORS; i++)\n\t\tunregister_blkdev(sd_major(i), \"sd\");\n}\n\nmodule_init(init_sd);\nmodule_exit(exit_sd);\n\nstatic void sd_print_sense_hdr(struct scsi_disk *sdkp,\n\t\t\t       struct scsi_sense_hdr *sshdr)\n{\n\tsd_printk(KERN_INFO, sdkp, \" \");\n\tscsi_show_sense_hdr(sshdr);\n\tsd_printk(KERN_INFO, sdkp, \" \");\n\tscsi_show_extd_sense(sshdr->asc, sshdr->ascq);\n}\n\nstatic void sd_print_result(struct scsi_disk *sdkp, int result)\n{\n\tsd_printk(KERN_INFO, sdkp, \" \");\n\tscsi_show_result(result);\n}\n\n", "#ifndef _LINUX_BLKDEV_H\n#define _LINUX_BLKDEV_H\n\n#ifdef CONFIG_BLOCK\n\n#include <linux/sched.h>\n#include <linux/major.h>\n#include <linux/genhd.h>\n#include <linux/list.h>\n#include <linux/timer.h>\n#include <linux/workqueue.h>\n#include <linux/pagemap.h>\n#include <linux/backing-dev.h>\n#include <linux/wait.h>\n#include <linux/mempool.h>\n#include <linux/bio.h>\n#include <linux/stringify.h>\n#include <linux/gfp.h>\n#include <linux/bsg.h>\n#include <linux/smp.h>\n\n#include <asm/scatterlist.h>\n\nstruct module;\nstruct scsi_ioctl_command;\n\nstruct request_queue;\nstruct elevator_queue;\nstruct request_pm_state;\nstruct blk_trace;\nstruct request;\nstruct sg_io_hdr;\nstruct bsg_job;\n\n#define BLKDEV_MIN_RQ\t4\n#define BLKDEV_MAX_RQ\t128\t/* Default maximum */\n\nstruct request;\ntypedef void (rq_end_io_fn)(struct request *, int);\n\nstruct request_list {\n\t/*\n\t * count[], starved[], and wait[] are indexed by\n\t * BLK_RW_SYNC/BLK_RW_ASYNC\n\t */\n\tint count[2];\n\tint starved[2];\n\tint elvpriv;\n\tmempool_t *rq_pool;\n\twait_queue_head_t wait[2];\n};\n\n/*\n * request command types\n */\nenum rq_cmd_type_bits {\n\tREQ_TYPE_FS\t\t= 1,\t/* fs request */\n\tREQ_TYPE_BLOCK_PC,\t\t/* scsi command */\n\tREQ_TYPE_SENSE,\t\t\t/* sense request */\n\tREQ_TYPE_PM_SUSPEND,\t\t/* suspend request */\n\tREQ_TYPE_PM_RESUME,\t\t/* resume request */\n\tREQ_TYPE_PM_SHUTDOWN,\t\t/* shutdown request */\n\tREQ_TYPE_SPECIAL,\t\t/* driver defined type */\n\t/*\n\t * for ATA/ATAPI devices. this really doesn't belong here, ide should\n\t * use REQ_TYPE_SPECIAL and use rq->cmd[0] with the range of driver\n\t * private REQ_LB opcodes to differentiate what type of request this is\n\t */\n\tREQ_TYPE_ATA_TASKFILE,\n\tREQ_TYPE_ATA_PC,\n};\n\n#define BLK_MAX_CDB\t16\n\n/*\n * try to put the fields that are referenced together in the same cacheline.\n * if you modify this structure, be sure to check block/blk-core.c:blk_rq_init()\n * as well!\n */\nstruct request {\n\tstruct list_head queuelist;\n\tstruct call_single_data csd;\n\n\tstruct request_queue *q;\n\n\tunsigned int cmd_flags;\n\tenum rq_cmd_type_bits cmd_type;\n\tunsigned long atomic_flags;\n\n\tint cpu;\n\n\t/* the following two fields are internal, NEVER access directly */\n\tunsigned int __data_len;\t/* total data len */\n\tsector_t __sector;\t\t/* sector cursor */\n\n\tstruct bio *bio;\n\tstruct bio *biotail;\n\n\tstruct hlist_node hash;\t/* merge hash */\n\t/*\n\t * The rb_node is only used inside the io scheduler, requests\n\t * are pruned when moved to the dispatch queue. So let the\n\t * completion_data share space with the rb_node.\n\t */\n\tunion {\n\t\tstruct rb_node rb_node;\t/* sort/lookup */\n\t\tvoid *completion_data;\n\t};\n\n\t/*\n\t * Three pointers are available for the IO schedulers, if they need\n\t * more they have to dynamically allocate it.  Flush requests are\n\t * never put on the IO scheduler. So let the flush fields share\n\t * space with the three elevator_private pointers.\n\t */\n\tunion {\n\t\tvoid *elevator_private[3];\n\t\tstruct {\n\t\t\tunsigned int\t\tseq;\n\t\t\tstruct list_head\tlist;\n\t\t\trq_end_io_fn\t\t*saved_end_io;\n\t\t} flush;\n\t};\n\n\tstruct gendisk *rq_disk;\n\tstruct hd_struct *part;\n\tunsigned long start_time;\n#ifdef CONFIG_BLK_CGROUP\n\tunsigned long long start_time_ns;\n\tunsigned long long io_start_time_ns;    /* when passed to hardware */\n#endif\n\t/* Number of scatter-gather DMA addr+len pairs after\n\t * physical address coalescing is performed.\n\t */\n\tunsigned short nr_phys_segments;\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\tunsigned short nr_integrity_segments;\n#endif\n\n\tunsigned short ioprio;\n\n\tint ref_count;\n\n\tvoid *special;\t\t/* opaque pointer available for LLD use */\n\tchar *buffer;\t\t/* kaddr of the current segment if available */\n\n\tint tag;\n\tint errors;\n\n\t/*\n\t * when request is used as a packet command carrier\n\t */\n\tunsigned char __cmd[BLK_MAX_CDB];\n\tunsigned char *cmd;\n\tunsigned short cmd_len;\n\n\tunsigned int extra_len;\t/* length of alignment and padding */\n\tunsigned int sense_len;\n\tunsigned int resid_len;\t/* residual count */\n\tvoid *sense;\n\n\tunsigned long deadline;\n\tstruct list_head timeout_list;\n\tunsigned int timeout;\n\tint retries;\n\n\t/*\n\t * completion callback.\n\t */\n\trq_end_io_fn *end_io;\n\tvoid *end_io_data;\n\n\t/* for bidi */\n\tstruct request *next_rq;\n};\n\nstatic inline unsigned short req_get_ioprio(struct request *req)\n{\n\treturn req->ioprio;\n}\n\n/*\n * State information carried for REQ_TYPE_PM_SUSPEND and REQ_TYPE_PM_RESUME\n * requests. Some step values could eventually be made generic.\n */\nstruct request_pm_state\n{\n\t/* PM state machine step value, currently driver specific */\n\tint\tpm_step;\n\t/* requested PM state value (S1, S2, S3, S4, ...) */\n\tu32\tpm_state;\n\tvoid*\tdata;\t\t/* for driver use */\n};\n\n#include <linux/elevator.h>\n\ntypedef void (request_fn_proc) (struct request_queue *q);\ntypedef void (make_request_fn) (struct request_queue *q, struct bio *bio);\ntypedef int (prep_rq_fn) (struct request_queue *, struct request *);\ntypedef void (unprep_rq_fn) (struct request_queue *, struct request *);\n\nstruct bio_vec;\nstruct bvec_merge_data {\n\tstruct block_device *bi_bdev;\n\tsector_t bi_sector;\n\tunsigned bi_size;\n\tunsigned long bi_rw;\n};\ntypedef int (merge_bvec_fn) (struct request_queue *, struct bvec_merge_data *,\n\t\t\t     struct bio_vec *);\ntypedef void (softirq_done_fn)(struct request *);\ntypedef int (dma_drain_needed_fn)(struct request *);\ntypedef int (lld_busy_fn) (struct request_queue *q);\ntypedef int (bsg_job_fn) (struct bsg_job *);\n\nenum blk_eh_timer_return {\n\tBLK_EH_NOT_HANDLED,\n\tBLK_EH_HANDLED,\n\tBLK_EH_RESET_TIMER,\n};\n\ntypedef enum blk_eh_timer_return (rq_timed_out_fn)(struct request *);\n\nenum blk_queue_state {\n\tQueue_down,\n\tQueue_up,\n};\n\nstruct blk_queue_tag {\n\tstruct request **tag_index;\t/* map of busy tags */\n\tunsigned long *tag_map;\t\t/* bit map of free/busy tags */\n\tint busy;\t\t\t/* current depth */\n\tint max_depth;\t\t\t/* what we will send to device */\n\tint real_max_depth;\t\t/* what the array can hold */\n\tatomic_t refcnt;\t\t/* map can be shared */\n};\n\n#define BLK_SCSI_MAX_CMDS\t(256)\n#define BLK_SCSI_CMD_PER_LONG\t(BLK_SCSI_MAX_CMDS / (sizeof(long) * 8))\n\nstruct queue_limits {\n\tunsigned long\t\tbounce_pfn;\n\tunsigned long\t\tseg_boundary_mask;\n\n\tunsigned int\t\tmax_hw_sectors;\n\tunsigned int\t\tmax_sectors;\n\tunsigned int\t\tmax_segment_size;\n\tunsigned int\t\tphysical_block_size;\n\tunsigned int\t\talignment_offset;\n\tunsigned int\t\tio_min;\n\tunsigned int\t\tio_opt;\n\tunsigned int\t\tmax_discard_sectors;\n\tunsigned int\t\tdiscard_granularity;\n\tunsigned int\t\tdiscard_alignment;\n\n\tunsigned short\t\tlogical_block_size;\n\tunsigned short\t\tmax_segments;\n\tunsigned short\t\tmax_integrity_segments;\n\n\tunsigned char\t\tmisaligned;\n\tunsigned char\t\tdiscard_misaligned;\n\tunsigned char\t\tcluster;\n\tunsigned char\t\tdiscard_zeroes_data;\n};\n\nstruct request_queue {\n\t/*\n\t * Together with queue_head for cacheline sharing\n\t */\n\tstruct list_head\tqueue_head;\n\tstruct request\t\t*last_merge;\n\tstruct elevator_queue\t*elevator;\n\n\t/*\n\t * the queue request freelist, one for reads and one for writes\n\t */\n\tstruct request_list\trq;\n\n\trequest_fn_proc\t\t*request_fn;\n\tmake_request_fn\t\t*make_request_fn;\n\tprep_rq_fn\t\t*prep_rq_fn;\n\tunprep_rq_fn\t\t*unprep_rq_fn;\n\tmerge_bvec_fn\t\t*merge_bvec_fn;\n\tsoftirq_done_fn\t\t*softirq_done_fn;\n\trq_timed_out_fn\t\t*rq_timed_out_fn;\n\tdma_drain_needed_fn\t*dma_drain_needed;\n\tlld_busy_fn\t\t*lld_busy_fn;\n\n\t/*\n\t * Dispatch queue sorting\n\t */\n\tsector_t\t\tend_sector;\n\tstruct request\t\t*boundary_rq;\n\n\t/*\n\t * Delayed queue handling\n\t */\n\tstruct delayed_work\tdelay_work;\n\n\tstruct backing_dev_info\tbacking_dev_info;\n\n\t/*\n\t * The queue owner gets to use this for whatever they like.\n\t * ll_rw_blk doesn't touch it.\n\t */\n\tvoid\t\t\t*queuedata;\n\n\t/*\n\t * various queue flags, see QUEUE_* below\n\t */\n\tunsigned long\t\tqueue_flags;\n\n\t/*\n\t * queue needs bounce pages for pages above this limit\n\t */\n\tgfp_t\t\t\tbounce_gfp;\n\n\t/*\n\t * protects queue structures from reentrancy. ->__queue_lock should\n\t * _never_ be used directly, it is queue private. always use\n\t * ->queue_lock.\n\t */\n\tspinlock_t\t\t__queue_lock;\n\tspinlock_t\t\t*queue_lock;\n\n\t/*\n\t * queue kobject\n\t */\n\tstruct kobject kobj;\n\n\t/*\n\t * queue settings\n\t */\n\tunsigned long\t\tnr_requests;\t/* Max # of requests */\n\tunsigned int\t\tnr_congestion_on;\n\tunsigned int\t\tnr_congestion_off;\n\tunsigned int\t\tnr_batching;\n\n\tunsigned int\t\tdma_drain_size;\n\tvoid\t\t\t*dma_drain_buffer;\n\tunsigned int\t\tdma_pad_mask;\n\tunsigned int\t\tdma_alignment;\n\n\tstruct blk_queue_tag\t*queue_tags;\n\tstruct list_head\ttag_busy_list;\n\n\tunsigned int\t\tnr_sorted;\n\tunsigned int\t\tin_flight[2];\n\n\tunsigned int\t\trq_timeout;\n\tstruct timer_list\ttimeout;\n\tstruct list_head\ttimeout_list;\n\n\tstruct queue_limits\tlimits;\n\n\t/*\n\t * sg stuff\n\t */\n\tunsigned int\t\tsg_timeout;\n\tunsigned int\t\tsg_reserved_size;\n\tint\t\t\tnode;\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tstruct blk_trace\t*blk_trace;\n#endif\n\t/*\n\t * for flush operations\n\t */\n\tunsigned int\t\tflush_flags;\n\tunsigned int\t\tflush_not_queueable:1;\n\tunsigned int\t\tflush_queue_delayed:1;\n\tunsigned int\t\tflush_pending_idx:1;\n\tunsigned int\t\tflush_running_idx:1;\n\tunsigned long\t\tflush_pending_since;\n\tstruct list_head\tflush_queue[2];\n\tstruct list_head\tflush_data_in_flight;\n\tstruct request\t\tflush_rq;\n\n\tstruct mutex\t\tsysfs_lock;\n\n#if defined(CONFIG_BLK_DEV_BSG)\n\tbsg_job_fn\t\t*bsg_job_fn;\n\tint\t\t\tbsg_job_size;\n\tstruct bsg_class_device bsg_dev;\n#endif\n\n#ifdef CONFIG_BLK_DEV_THROTTLING\n\t/* Throttle data */\n\tstruct throtl_data *td;\n#endif\n};\n\n#define QUEUE_FLAG_QUEUED\t1\t/* uses generic tag queueing */\n#define QUEUE_FLAG_STOPPED\t2\t/* queue is stopped */\n#define\tQUEUE_FLAG_SYNCFULL\t3\t/* read queue has been filled */\n#define QUEUE_FLAG_ASYNCFULL\t4\t/* write queue has been filled */\n#define QUEUE_FLAG_DEAD\t\t5\t/* queue being torn down */\n#define QUEUE_FLAG_ELVSWITCH\t6\t/* don't use elevator, just do FIFO */\n#define QUEUE_FLAG_BIDI\t\t7\t/* queue supports bidi requests */\n#define QUEUE_FLAG_NOMERGES     8\t/* disable merge attempts */\n#define QUEUE_FLAG_SAME_COMP\t9\t/* complete on same CPU-group */\n#define QUEUE_FLAG_FAIL_IO     10\t/* fake timeout */\n#define QUEUE_FLAG_STACKABLE   11\t/* supports request stacking */\n#define QUEUE_FLAG_NONROT      12\t/* non-rotational device (SSD) */\n#define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */\n#define QUEUE_FLAG_IO_STAT     13\t/* do IO stats */\n#define QUEUE_FLAG_DISCARD     14\t/* supports DISCARD */\n#define QUEUE_FLAG_NOXMERGES   15\t/* No extended merges */\n#define QUEUE_FLAG_ADD_RANDOM  16\t/* Contributes to random pool */\n#define QUEUE_FLAG_SECDISCARD  17\t/* supports SECDISCARD */\n#define QUEUE_FLAG_SAME_FORCE  18\t/* force complete on same CPU */\n\n#define QUEUE_FLAG_DEFAULT\t((1 << QUEUE_FLAG_IO_STAT) |\t\t\\\n\t\t\t\t (1 << QUEUE_FLAG_STACKABLE)\t|\t\\\n\t\t\t\t (1 << QUEUE_FLAG_SAME_COMP)\t|\t\\\n\t\t\t\t (1 << QUEUE_FLAG_ADD_RANDOM))\n\nstatic inline int queue_is_locked(struct request_queue *q)\n{\n#ifdef CONFIG_SMP\n\tspinlock_t *lock = q->queue_lock;\n\treturn lock && spin_is_locked(lock);\n#else\n\treturn 1;\n#endif\n}\n\nstatic inline void queue_flag_set_unlocked(unsigned int flag,\n\t\t\t\t\t   struct request_queue *q)\n{\n\t__set_bit(flag, &q->queue_flags);\n}\n\nstatic inline int queue_flag_test_and_clear(unsigned int flag,\n\t\t\t\t\t    struct request_queue *q)\n{\n\tWARN_ON_ONCE(!queue_is_locked(q));\n\n\tif (test_bit(flag, &q->queue_flags)) {\n\t\t__clear_bit(flag, &q->queue_flags);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline int queue_flag_test_and_set(unsigned int flag,\n\t\t\t\t\t  struct request_queue *q)\n{\n\tWARN_ON_ONCE(!queue_is_locked(q));\n\n\tif (!test_bit(flag, &q->queue_flags)) {\n\t\t__set_bit(flag, &q->queue_flags);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic inline void queue_flag_set(unsigned int flag, struct request_queue *q)\n{\n\tWARN_ON_ONCE(!queue_is_locked(q));\n\t__set_bit(flag, &q->queue_flags);\n}\n\nstatic inline void queue_flag_clear_unlocked(unsigned int flag,\n\t\t\t\t\t     struct request_queue *q)\n{\n\t__clear_bit(flag, &q->queue_flags);\n}\n\nstatic inline int queue_in_flight(struct request_queue *q)\n{\n\treturn q->in_flight[0] + q->in_flight[1];\n}\n\nstatic inline void queue_flag_clear(unsigned int flag, struct request_queue *q)\n{\n\tWARN_ON_ONCE(!queue_is_locked(q));\n\t__clear_bit(flag, &q->queue_flags);\n}\n\n#define blk_queue_tagged(q)\ttest_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)\n#define blk_queue_stopped(q)\ttest_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)\n#define blk_queue_nomerges(q)\ttest_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)\n#define blk_queue_noxmerges(q)\t\\\n\ttest_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)\n#define blk_queue_nonrot(q)\ttest_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)\n#define blk_queue_io_stat(q)\ttest_bit(QUEUE_FLAG_IO_STAT, &(q)->queue_flags)\n#define blk_queue_add_random(q)\ttest_bit(QUEUE_FLAG_ADD_RANDOM, &(q)->queue_flags)\n#define blk_queue_stackable(q)\t\\\n\ttest_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)\n#define blk_queue_discard(q)\ttest_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)\n#define blk_queue_secdiscard(q)\t(blk_queue_discard(q) && \\\n\ttest_bit(QUEUE_FLAG_SECDISCARD, &(q)->queue_flags))\n\n#define blk_noretry_request(rq) \\\n\t((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \\\n\t\t\t     REQ_FAILFAST_DRIVER))\n\n#define blk_account_rq(rq) \\\n\t(((rq)->cmd_flags & REQ_STARTED) && \\\n\t ((rq)->cmd_type == REQ_TYPE_FS || \\\n\t  ((rq)->cmd_flags & REQ_DISCARD)))\n\n#define blk_pm_request(rq)\t\\\n\t((rq)->cmd_type == REQ_TYPE_PM_SUSPEND || \\\n\t (rq)->cmd_type == REQ_TYPE_PM_RESUME)\n\n#define blk_rq_cpu_valid(rq)\t((rq)->cpu != -1)\n#define blk_bidi_rq(rq)\t\t((rq)->next_rq != NULL)\n/* rq->queuelist of dequeued request must be list_empty() */\n#define blk_queued_rq(rq)\t(!list_empty(&(rq)->queuelist))\n\n#define list_entry_rq(ptr)\tlist_entry((ptr), struct request, queuelist)\n\n#define rq_data_dir(rq)\t\t((rq)->cmd_flags & 1)\n\nstatic inline unsigned int blk_queue_cluster(struct request_queue *q)\n{\n\treturn q->limits.cluster;\n}\n\n/*\n * We regard a request as sync, if either a read or a sync write\n */\nstatic inline bool rw_is_sync(unsigned int rw_flags)\n{\n\treturn !(rw_flags & REQ_WRITE) || (rw_flags & REQ_SYNC);\n}\n\nstatic inline bool rq_is_sync(struct request *rq)\n{\n\treturn rw_is_sync(rq->cmd_flags);\n}\n\nstatic inline int blk_queue_full(struct request_queue *q, int sync)\n{\n\tif (sync)\n\t\treturn test_bit(QUEUE_FLAG_SYNCFULL, &q->queue_flags);\n\treturn test_bit(QUEUE_FLAG_ASYNCFULL, &q->queue_flags);\n}\n\nstatic inline void blk_set_queue_full(struct request_queue *q, int sync)\n{\n\tif (sync)\n\t\tqueue_flag_set(QUEUE_FLAG_SYNCFULL, q);\n\telse\n\t\tqueue_flag_set(QUEUE_FLAG_ASYNCFULL, q);\n}\n\nstatic inline void blk_clear_queue_full(struct request_queue *q, int sync)\n{\n\tif (sync)\n\t\tqueue_flag_clear(QUEUE_FLAG_SYNCFULL, q);\n\telse\n\t\tqueue_flag_clear(QUEUE_FLAG_ASYNCFULL, q);\n}\n\n\n/*\n * mergeable request must not have _NOMERGE or _BARRIER bit set, nor may\n * it already be started by driver.\n */\n#define RQ_NOMERGE_FLAGS\t\\\n\t(REQ_NOMERGE | REQ_STARTED | REQ_SOFTBARRIER | REQ_FLUSH | REQ_FUA)\n#define rq_mergeable(rq)\t\\\n\t(!((rq)->cmd_flags & RQ_NOMERGE_FLAGS) && \\\n\t (((rq)->cmd_flags & REQ_DISCARD) || \\\n\t  (rq)->cmd_type == REQ_TYPE_FS))\n\n/*\n * q->prep_rq_fn return values\n */\n#define BLKPREP_OK\t\t0\t/* serve it */\n#define BLKPREP_KILL\t\t1\t/* fatal error, kill */\n#define BLKPREP_DEFER\t\t2\t/* leave on queue */\n\nextern unsigned long blk_max_low_pfn, blk_max_pfn;\n\n/*\n * standard bounce addresses:\n *\n * BLK_BOUNCE_HIGH\t: bounce all highmem pages\n * BLK_BOUNCE_ANY\t: don't bounce anything\n * BLK_BOUNCE_ISA\t: bounce pages above ISA DMA boundary\n */\n\n#if BITS_PER_LONG == 32\n#define BLK_BOUNCE_HIGH\t\t((u64)blk_max_low_pfn << PAGE_SHIFT)\n#else\n#define BLK_BOUNCE_HIGH\t\t-1ULL\n#endif\n#define BLK_BOUNCE_ANY\t\t(-1ULL)\n#define BLK_BOUNCE_ISA\t\t(DMA_BIT_MASK(24))\n\n/*\n * default timeout for SG_IO if none specified\n */\n#define BLK_DEFAULT_SG_TIMEOUT\t(60 * HZ)\n#define BLK_MIN_SG_TIMEOUT\t(7 * HZ)\n\n#ifdef CONFIG_BOUNCE\nextern int init_emergency_isa_pool(void);\nextern void blk_queue_bounce(struct request_queue *q, struct bio **bio);\n#else\nstatic inline int init_emergency_isa_pool(void)\n{\n\treturn 0;\n}\nstatic inline void blk_queue_bounce(struct request_queue *q, struct bio **bio)\n{\n}\n#endif /* CONFIG_MMU */\n\nstruct rq_map_data {\n\tstruct page **pages;\n\tint page_order;\n\tint nr_entries;\n\tunsigned long offset;\n\tint null_mapped;\n\tint from_user;\n};\n\nstruct req_iterator {\n\tint i;\n\tstruct bio *bio;\n};\n\n/* This should not be used directly - use rq_for_each_segment */\n#define for_each_bio(_bio)\t\t\\\n\tfor (; _bio; _bio = _bio->bi_next)\n#define __rq_for_each_bio(_bio, rq)\t\\\n\tif ((rq->bio))\t\t\t\\\n\t\tfor (_bio = (rq)->bio; _bio; _bio = _bio->bi_next)\n\n#define rq_for_each_segment(bvl, _rq, _iter)\t\t\t\\\n\t__rq_for_each_bio(_iter.bio, _rq)\t\t\t\\\n\t\tbio_for_each_segment(bvl, _iter.bio, _iter.i)\n\n#define rq_iter_last(rq, _iter)\t\t\t\t\t\\\n\t\t(_iter.bio->bi_next == NULL && _iter.i == _iter.bio->bi_vcnt-1)\n\n#ifndef ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE\n# error\t\"You should define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE for your platform\"\n#endif\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE\nextern void rq_flush_dcache_pages(struct request *rq);\n#else\nstatic inline void rq_flush_dcache_pages(struct request *rq)\n{\n}\n#endif\n\nextern int blk_register_queue(struct gendisk *disk);\nextern void blk_unregister_queue(struct gendisk *disk);\nextern void generic_make_request(struct bio *bio);\nextern void blk_rq_init(struct request_queue *q, struct request *rq);\nextern void blk_put_request(struct request *);\nextern void __blk_put_request(struct request_queue *, struct request *);\nextern struct request *blk_get_request(struct request_queue *, int, gfp_t);\nextern struct request *blk_make_request(struct request_queue *, struct bio *,\n\t\t\t\t\tgfp_t);\nextern void blk_insert_request(struct request_queue *, struct request *, int, void *);\nextern void blk_requeue_request(struct request_queue *, struct request *);\nextern void blk_add_request_payload(struct request *rq, struct page *page,\n\t\tunsigned int len);\nextern int blk_rq_check_limits(struct request_queue *q, struct request *rq);\nextern int blk_lld_busy(struct request_queue *q);\nextern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,\n\t\t\t     struct bio_set *bs, gfp_t gfp_mask,\n\t\t\t     int (*bio_ctr)(struct bio *, struct bio *, void *),\n\t\t\t     void *data);\nextern void blk_rq_unprep_clone(struct request *rq);\nextern int blk_insert_cloned_request(struct request_queue *q,\n\t\t\t\t     struct request *rq);\nextern void blk_delay_queue(struct request_queue *, unsigned long);\nextern void blk_recount_segments(struct request_queue *, struct bio *);\nextern int scsi_cmd_blk_ioctl(struct block_device *, fmode_t,\n\t\t\t      unsigned int, void __user *);\nextern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,\n\t\t\t  unsigned int, void __user *);\nextern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,\n\t\t\t struct scsi_ioctl_command __user *);\n\nextern void blk_queue_bio(struct request_queue *q, struct bio *bio);\n\n/*\n * A queue has just exitted congestion.  Note this in the global counter of\n * congested queues, and wake up anyone who was waiting for requests to be\n * put back.\n */\nstatic inline void blk_clear_queue_congested(struct request_queue *q, int sync)\n{\n\tclear_bdi_congested(&q->backing_dev_info, sync);\n}\n\n/*\n * A queue has just entered congestion.  Flag that in the queue's VM-visible\n * state flags and increment the global gounter of congested queues.\n */\nstatic inline void blk_set_queue_congested(struct request_queue *q, int sync)\n{\n\tset_bdi_congested(&q->backing_dev_info, sync);\n}\n\nextern void blk_start_queue(struct request_queue *q);\nextern void blk_stop_queue(struct request_queue *q);\nextern void blk_sync_queue(struct request_queue *q);\nextern void __blk_stop_queue(struct request_queue *q);\nextern void __blk_run_queue(struct request_queue *q);\nextern void blk_run_queue(struct request_queue *);\nextern void blk_run_queue_async(struct request_queue *q);\nextern int blk_rq_map_user(struct request_queue *, struct request *,\n\t\t\t   struct rq_map_data *, void __user *, unsigned long,\n\t\t\t   gfp_t);\nextern int blk_rq_unmap_user(struct bio *);\nextern int blk_rq_map_kern(struct request_queue *, struct request *, void *, unsigned int, gfp_t);\nextern int blk_rq_map_user_iov(struct request_queue *, struct request *,\n\t\t\t       struct rq_map_data *, struct sg_iovec *, int,\n\t\t\t       unsigned int, gfp_t);\nextern int blk_execute_rq(struct request_queue *, struct gendisk *,\n\t\t\t  struct request *, int);\nextern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,\n\t\t\t\t  struct request *, int, rq_end_io_fn *);\n\nstatic inline struct request_queue *bdev_get_queue(struct block_device *bdev)\n{\n\treturn bdev->bd_disk->queue;\n}\n\n/*\n * blk_rq_pos()\t\t\t: the current sector\n * blk_rq_bytes()\t\t: bytes left in the entire request\n * blk_rq_cur_bytes()\t\t: bytes left in the current segment\n * blk_rq_err_bytes()\t\t: bytes left till the next error boundary\n * blk_rq_sectors()\t\t: sectors left in the entire request\n * blk_rq_cur_sectors()\t\t: sectors left in the current segment\n */\nstatic inline sector_t blk_rq_pos(const struct request *rq)\n{\n\treturn rq->__sector;\n}\n\nstatic inline unsigned int blk_rq_bytes(const struct request *rq)\n{\n\treturn rq->__data_len;\n}\n\nstatic inline int blk_rq_cur_bytes(const struct request *rq)\n{\n\treturn rq->bio ? bio_cur_bytes(rq->bio) : 0;\n}\n\nextern unsigned int blk_rq_err_bytes(const struct request *rq);\n\nstatic inline unsigned int blk_rq_sectors(const struct request *rq)\n{\n\treturn blk_rq_bytes(rq) >> 9;\n}\n\nstatic inline unsigned int blk_rq_cur_sectors(const struct request *rq)\n{\n\treturn blk_rq_cur_bytes(rq) >> 9;\n}\n\n/*\n * Request issue related functions.\n */\nextern struct request *blk_peek_request(struct request_queue *q);\nextern void blk_start_request(struct request *rq);\nextern struct request *blk_fetch_request(struct request_queue *q);\n\n/*\n * Request completion related functions.\n *\n * blk_update_request() completes given number of bytes and updates\n * the request without completing it.\n *\n * blk_end_request() and friends.  __blk_end_request() must be called\n * with the request queue spinlock acquired.\n *\n * Several drivers define their own end_request and call\n * blk_end_request() for parts of the original function.\n * This prevents code duplication in drivers.\n */\nextern bool blk_update_request(struct request *rq, int error,\n\t\t\t       unsigned int nr_bytes);\nextern bool blk_end_request(struct request *rq, int error,\n\t\t\t    unsigned int nr_bytes);\nextern void blk_end_request_all(struct request *rq, int error);\nextern bool blk_end_request_cur(struct request *rq, int error);\nextern bool blk_end_request_err(struct request *rq, int error);\nextern bool __blk_end_request(struct request *rq, int error,\n\t\t\t      unsigned int nr_bytes);\nextern void __blk_end_request_all(struct request *rq, int error);\nextern bool __blk_end_request_cur(struct request *rq, int error);\nextern bool __blk_end_request_err(struct request *rq, int error);\n\nextern void blk_complete_request(struct request *);\nextern void __blk_complete_request(struct request *);\nextern void blk_abort_request(struct request *);\nextern void blk_abort_queue(struct request_queue *);\nextern void blk_unprep_request(struct request *);\n\n/*\n * Access functions for manipulating queue properties\n */\nextern struct request_queue *blk_init_queue_node(request_fn_proc *rfn,\n\t\t\t\t\tspinlock_t *lock, int node_id);\nextern struct request_queue *blk_init_queue(request_fn_proc *, spinlock_t *);\nextern struct request_queue *blk_init_allocated_queue(struct request_queue *,\n\t\t\t\t\t\t      request_fn_proc *, spinlock_t *);\nextern void blk_cleanup_queue(struct request_queue *);\nextern void blk_queue_make_request(struct request_queue *, make_request_fn *);\nextern void blk_queue_bounce_limit(struct request_queue *, u64);\nextern void blk_limits_max_hw_sectors(struct queue_limits *, unsigned int);\nextern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);\nextern void blk_queue_max_segments(struct request_queue *, unsigned short);\nextern void blk_queue_max_segment_size(struct request_queue *, unsigned int);\nextern void blk_queue_max_discard_sectors(struct request_queue *q,\n\t\tunsigned int max_discard_sectors);\nextern void blk_queue_logical_block_size(struct request_queue *, unsigned short);\nextern void blk_queue_physical_block_size(struct request_queue *, unsigned int);\nextern void blk_queue_alignment_offset(struct request_queue *q,\n\t\t\t\t       unsigned int alignment);\nextern void blk_limits_io_min(struct queue_limits *limits, unsigned int min);\nextern void blk_queue_io_min(struct request_queue *q, unsigned int min);\nextern void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt);\nextern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);\nextern void blk_set_default_limits(struct queue_limits *lim);\nextern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,\n\t\t\t    sector_t offset);\nextern int bdev_stack_limits(struct queue_limits *t, struct block_device *bdev,\n\t\t\t    sector_t offset);\nextern void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,\n\t\t\t      sector_t offset);\nextern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);\nextern void blk_queue_dma_pad(struct request_queue *, unsigned int);\nextern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);\nextern int blk_queue_dma_drain(struct request_queue *q,\n\t\t\t       dma_drain_needed_fn *dma_drain_needed,\n\t\t\t       void *buf, unsigned int size);\nextern void blk_queue_lld_busy(struct request_queue *q, lld_busy_fn *fn);\nextern void blk_queue_segment_boundary(struct request_queue *, unsigned long);\nextern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);\nextern void blk_queue_unprep_rq(struct request_queue *, unprep_rq_fn *ufn);\nextern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);\nextern void blk_queue_dma_alignment(struct request_queue *, int);\nextern void blk_queue_update_dma_alignment(struct request_queue *, int);\nextern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);\nextern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);\nextern void blk_queue_rq_timeout(struct request_queue *, unsigned int);\nextern void blk_queue_flush(struct request_queue *q, unsigned int flush);\nextern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);\nextern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);\n\nextern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);\nextern void blk_dump_rq_flags(struct request *, char *);\nextern long nr_blockdev_pages(void);\n\nint blk_get_queue(struct request_queue *);\nstruct request_queue *blk_alloc_queue(gfp_t);\nstruct request_queue *blk_alloc_queue_node(gfp_t, int);\nextern void blk_put_queue(struct request_queue *);\n\n/*\n * blk_plug permits building a queue of related requests by holding the I/O\n * fragments for a short period. This allows merging of sequential requests\n * into single larger request. As the requests are moved from a per-task list to\n * the device's request_queue in a batch, this results in improved scalability\n * as the lock contention for request_queue lock is reduced.\n *\n * It is ok not to disable preemption when adding the request to the plug list\n * or when attempting a merge, because blk_schedule_flush_list() will only flush\n * the plug list when the task sleeps by itself. For details, please see\n * schedule() where blk_schedule_flush_plug() is called.\n */\nstruct blk_plug {\n\tunsigned long magic; /* detect uninitialized use-cases */\n\tstruct list_head list; /* requests */\n\tstruct list_head cb_list; /* md requires an unplug callback */\n\tunsigned int should_sort; /* list to be sorted before flushing? */\n};\n#define BLK_MAX_REQUEST_COUNT 16\n\nstruct blk_plug_cb {\n\tstruct list_head list;\n\tvoid (*callback)(struct blk_plug_cb *);\n};\n\nextern void blk_start_plug(struct blk_plug *);\nextern void blk_finish_plug(struct blk_plug *);\nextern void blk_flush_plug_list(struct blk_plug *, bool);\n\nstatic inline void blk_flush_plug(struct task_struct *tsk)\n{\n\tstruct blk_plug *plug = tsk->plug;\n\n\tif (plug)\n\t\tblk_flush_plug_list(plug, false);\n}\n\nstatic inline void blk_schedule_flush_plug(struct task_struct *tsk)\n{\n\tstruct blk_plug *plug = tsk->plug;\n\n\tif (plug)\n\t\tblk_flush_plug_list(plug, true);\n}\n\nstatic inline bool blk_needs_flush_plug(struct task_struct *tsk)\n{\n\tstruct blk_plug *plug = tsk->plug;\n\n\treturn plug && (!list_empty(&plug->list) || !list_empty(&plug->cb_list));\n}\n\n/*\n * tag stuff\n */\n#define blk_rq_tagged(rq)\t\t((rq)->cmd_flags & REQ_QUEUED)\nextern int blk_queue_start_tag(struct request_queue *, struct request *);\nextern struct request *blk_queue_find_tag(struct request_queue *, int);\nextern void blk_queue_end_tag(struct request_queue *, struct request *);\nextern int blk_queue_init_tags(struct request_queue *, int, struct blk_queue_tag *);\nextern void blk_queue_free_tags(struct request_queue *);\nextern int blk_queue_resize_tags(struct request_queue *, int);\nextern void blk_queue_invalidate_tags(struct request_queue *);\nextern struct blk_queue_tag *blk_init_tags(int);\nextern void blk_free_tags(struct blk_queue_tag *);\n\nstatic inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,\n\t\t\t\t\t\tint tag)\n{\n\tif (unlikely(bqt == NULL || tag >= bqt->real_max_depth))\n\t\treturn NULL;\n\treturn bqt->tag_index[tag];\n}\n\n#define BLKDEV_DISCARD_SECURE  0x01    /* secure discard */\n\nextern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);\nextern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp_mask, unsigned long flags);\nextern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,\n\t\t\tsector_t nr_sects, gfp_t gfp_mask);\nstatic inline int sb_issue_discard(struct super_block *sb, sector_t block,\n\t\tsector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)\n{\n\treturn blkdev_issue_discard(sb->s_bdev, block << (sb->s_blocksize_bits - 9),\n\t\t\t\t    nr_blocks << (sb->s_blocksize_bits - 9),\n\t\t\t\t    gfp_mask, flags);\n}\nstatic inline int sb_issue_zeroout(struct super_block *sb, sector_t block,\n\t\tsector_t nr_blocks, gfp_t gfp_mask)\n{\n\treturn blkdev_issue_zeroout(sb->s_bdev,\n\t\t\t\t    block << (sb->s_blocksize_bits - 9),\n\t\t\t\t    nr_blocks << (sb->s_blocksize_bits - 9),\n\t\t\t\t    gfp_mask);\n}\n\nextern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);\n\nenum blk_default_limits {\n\tBLK_MAX_SEGMENTS\t= 128,\n\tBLK_SAFE_MAX_SECTORS\t= 255,\n\tBLK_DEF_MAX_SECTORS\t= 1024,\n\tBLK_MAX_SEGMENT_SIZE\t= 65536,\n\tBLK_SEG_BOUNDARY_MASK\t= 0xFFFFFFFFUL,\n};\n\n#define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)\n\nstatic inline unsigned long queue_bounce_pfn(struct request_queue *q)\n{\n\treturn q->limits.bounce_pfn;\n}\n\nstatic inline unsigned long queue_segment_boundary(struct request_queue *q)\n{\n\treturn q->limits.seg_boundary_mask;\n}\n\nstatic inline unsigned int queue_max_sectors(struct request_queue *q)\n{\n\treturn q->limits.max_sectors;\n}\n\nstatic inline unsigned int queue_max_hw_sectors(struct request_queue *q)\n{\n\treturn q->limits.max_hw_sectors;\n}\n\nstatic inline unsigned short queue_max_segments(struct request_queue *q)\n{\n\treturn q->limits.max_segments;\n}\n\nstatic inline unsigned int queue_max_segment_size(struct request_queue *q)\n{\n\treturn q->limits.max_segment_size;\n}\n\nstatic inline unsigned short queue_logical_block_size(struct request_queue *q)\n{\n\tint retval = 512;\n\n\tif (q && q->limits.logical_block_size)\n\t\tretval = q->limits.logical_block_size;\n\n\treturn retval;\n}\n\nstatic inline unsigned short bdev_logical_block_size(struct block_device *bdev)\n{\n\treturn queue_logical_block_size(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_physical_block_size(struct request_queue *q)\n{\n\treturn q->limits.physical_block_size;\n}\n\nstatic inline unsigned int bdev_physical_block_size(struct block_device *bdev)\n{\n\treturn queue_physical_block_size(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_io_min(struct request_queue *q)\n{\n\treturn q->limits.io_min;\n}\n\nstatic inline int bdev_io_min(struct block_device *bdev)\n{\n\treturn queue_io_min(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_io_opt(struct request_queue *q)\n{\n\treturn q->limits.io_opt;\n}\n\nstatic inline int bdev_io_opt(struct block_device *bdev)\n{\n\treturn queue_io_opt(bdev_get_queue(bdev));\n}\n\nstatic inline int queue_alignment_offset(struct request_queue *q)\n{\n\tif (q->limits.misaligned)\n\t\treturn -1;\n\n\treturn q->limits.alignment_offset;\n}\n\nstatic inline int queue_limit_alignment_offset(struct queue_limits *lim, sector_t sector)\n{\n\tunsigned int granularity = max(lim->physical_block_size, lim->io_min);\n\tunsigned int alignment = (sector << 9) & (granularity - 1);\n\n\treturn (granularity + lim->alignment_offset - alignment)\n\t\t& (granularity - 1);\n}\n\nstatic inline int bdev_alignment_offset(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q->limits.misaligned)\n\t\treturn -1;\n\n\tif (bdev != bdev->bd_contains)\n\t\treturn bdev->bd_part->alignment_offset;\n\n\treturn q->limits.alignment_offset;\n}\n\nstatic inline int queue_discard_alignment(struct request_queue *q)\n{\n\tif (q->limits.discard_misaligned)\n\t\treturn -1;\n\n\treturn q->limits.discard_alignment;\n}\n\nstatic inline int queue_limit_discard_alignment(struct queue_limits *lim, sector_t sector)\n{\n\tunsigned int alignment = (sector << 9) & (lim->discard_granularity - 1);\n\n\tif (!lim->max_discard_sectors)\n\t\treturn 0;\n\n\treturn (lim->discard_granularity + lim->discard_alignment - alignment)\n\t\t& (lim->discard_granularity - 1);\n}\n\nstatic inline unsigned int queue_discard_zeroes_data(struct request_queue *q)\n{\n\tif (q->limits.max_discard_sectors && q->limits.discard_zeroes_data == 1)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic inline unsigned int bdev_discard_zeroes_data(struct block_device *bdev)\n{\n\treturn queue_discard_zeroes_data(bdev_get_queue(bdev));\n}\n\nstatic inline int queue_dma_alignment(struct request_queue *q)\n{\n\treturn q ? q->dma_alignment : 511;\n}\n\nstatic inline int blk_rq_aligned(struct request_queue *q, unsigned long addr,\n\t\t\t\t unsigned int len)\n{\n\tunsigned int alignment = queue_dma_alignment(q) | q->dma_pad_mask;\n\treturn !(addr & alignment) && !(len & alignment);\n}\n\n/* assumes size > 256 */\nstatic inline unsigned int blksize_bits(unsigned int size)\n{\n\tunsigned int bits = 8;\n\tdo {\n\t\tbits++;\n\t\tsize >>= 1;\n\t} while (size > 256);\n\treturn bits;\n}\n\nstatic inline unsigned int block_size(struct block_device *bdev)\n{\n\treturn bdev->bd_block_size;\n}\n\nstatic inline bool queue_flush_queueable(struct request_queue *q)\n{\n\treturn !q->flush_not_queueable;\n}\n\ntypedef struct {struct page *v;} Sector;\n\nunsigned char *read_dev_sector(struct block_device *, sector_t, Sector *);\n\nstatic inline void put_dev_sector(Sector p)\n{\n\tpage_cache_release(p.v);\n}\n\nstruct work_struct;\nint kblockd_schedule_work(struct request_queue *q, struct work_struct *work);\n\n#ifdef CONFIG_BLK_CGROUP\n/*\n * This should not be using sched_clock(). A real patch is in progress\n * to fix this up, until that is in place we need to disable preemption\n * around sched_clock() in this function and set_io_start_time_ns().\n */\nstatic inline void set_start_time_ns(struct request *req)\n{\n\tpreempt_disable();\n\treq->start_time_ns = sched_clock();\n\tpreempt_enable();\n}\n\nstatic inline void set_io_start_time_ns(struct request *req)\n{\n\tpreempt_disable();\n\treq->io_start_time_ns = sched_clock();\n\tpreempt_enable();\n}\n\nstatic inline uint64_t rq_start_time_ns(struct request *req)\n{\n        return req->start_time_ns;\n}\n\nstatic inline uint64_t rq_io_start_time_ns(struct request *req)\n{\n        return req->io_start_time_ns;\n}\n#else\nstatic inline void set_start_time_ns(struct request *req) {}\nstatic inline void set_io_start_time_ns(struct request *req) {}\nstatic inline uint64_t rq_start_time_ns(struct request *req)\n{\n\treturn 0;\n}\nstatic inline uint64_t rq_io_start_time_ns(struct request *req)\n{\n\treturn 0;\n}\n#endif\n\n#define MODULE_ALIAS_BLOCKDEV(major,minor) \\\n\tMODULE_ALIAS(\"block-major-\" __stringify(major) \"-\" __stringify(minor))\n#define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \\\n\tMODULE_ALIAS(\"block-major-\" __stringify(major) \"-*\")\n\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\n#define INTEGRITY_FLAG_READ\t2\t/* verify data integrity on read */\n#define INTEGRITY_FLAG_WRITE\t4\t/* generate data integrity on write */\n\nstruct blk_integrity_exchg {\n\tvoid\t\t\t*prot_buf;\n\tvoid\t\t\t*data_buf;\n\tsector_t\t\tsector;\n\tunsigned int\t\tdata_size;\n\tunsigned short\t\tsector_size;\n\tconst char\t\t*disk_name;\n};\n\ntypedef void (integrity_gen_fn) (struct blk_integrity_exchg *);\ntypedef int (integrity_vrfy_fn) (struct blk_integrity_exchg *);\ntypedef void (integrity_set_tag_fn) (void *, void *, unsigned int);\ntypedef void (integrity_get_tag_fn) (void *, void *, unsigned int);\n\nstruct blk_integrity {\n\tintegrity_gen_fn\t*generate_fn;\n\tintegrity_vrfy_fn\t*verify_fn;\n\tintegrity_set_tag_fn\t*set_tag_fn;\n\tintegrity_get_tag_fn\t*get_tag_fn;\n\n\tunsigned short\t\tflags;\n\tunsigned short\t\ttuple_size;\n\tunsigned short\t\tsector_size;\n\tunsigned short\t\ttag_size;\n\n\tconst char\t\t*name;\n\n\tstruct kobject\t\tkobj;\n};\n\nextern bool blk_integrity_is_initialized(struct gendisk *);\nextern int blk_integrity_register(struct gendisk *, struct blk_integrity *);\nextern void blk_integrity_unregister(struct gendisk *);\nextern int blk_integrity_compare(struct gendisk *, struct gendisk *);\nextern int blk_rq_map_integrity_sg(struct request_queue *, struct bio *,\n\t\t\t\t   struct scatterlist *);\nextern int blk_rq_count_integrity_sg(struct request_queue *, struct bio *);\nextern int blk_integrity_merge_rq(struct request_queue *, struct request *,\n\t\t\t\t  struct request *);\nextern int blk_integrity_merge_bio(struct request_queue *, struct request *,\n\t\t\t\t   struct bio *);\n\nstatic inline\nstruct blk_integrity *bdev_get_integrity(struct block_device *bdev)\n{\n\treturn bdev->bd_disk->integrity;\n}\n\nstatic inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)\n{\n\treturn disk->integrity;\n}\n\nstatic inline int blk_integrity_rq(struct request *rq)\n{\n\tif (rq->bio == NULL)\n\t\treturn 0;\n\n\treturn bio_integrity(rq->bio);\n}\n\nstatic inline void blk_queue_max_integrity_segments(struct request_queue *q,\n\t\t\t\t\t\t    unsigned int segs)\n{\n\tq->limits.max_integrity_segments = segs;\n}\n\nstatic inline unsigned short\nqueue_max_integrity_segments(struct request_queue *q)\n{\n\treturn q->limits.max_integrity_segments;\n}\n\n#else /* CONFIG_BLK_DEV_INTEGRITY */\n\n#define blk_integrity_rq(rq)\t\t\t(0)\n#define blk_rq_count_integrity_sg(a, b)\t\t(0)\n#define blk_rq_map_integrity_sg(a, b, c)\t(0)\n#define bdev_get_integrity(a)\t\t\t(0)\n#define blk_get_integrity(a)\t\t\t(0)\n#define blk_integrity_compare(a, b)\t\t(0)\n#define blk_integrity_register(a, b)\t\t(0)\n#define blk_integrity_unregister(a)\t\tdo { } while (0)\n#define blk_queue_max_integrity_segments(a, b)\tdo { } while (0)\n#define queue_max_integrity_segments(a)\t\t(0)\n#define blk_integrity_merge_rq(a, b, c)\t\t(0)\n#define blk_integrity_merge_bio(a, b, c)\t(0)\n#define blk_integrity_is_initialized(a)\t\t(0)\n\n#endif /* CONFIG_BLK_DEV_INTEGRITY */\n\nstruct block_device_operations {\n\tint (*open) (struct block_device *, fmode_t);\n\tint (*release) (struct gendisk *, fmode_t);\n\tint (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);\n\tint (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);\n\tint (*direct_access) (struct block_device *, sector_t,\n\t\t\t\t\t\tvoid **, unsigned long *);\n\tunsigned int (*check_events) (struct gendisk *disk,\n\t\t\t\t      unsigned int clearing);\n\t/* ->media_changed() is DEPRECATED, use ->check_events() instead */\n\tint (*media_changed) (struct gendisk *);\n\tvoid (*unlock_native_capacity) (struct gendisk *);\n\tint (*revalidate_disk) (struct gendisk *);\n\tint (*getgeo)(struct block_device *, struct hd_geometry *);\n\t/* this callback is with swap_lock and sometimes page table lock held */\n\tvoid (*swap_slot_free_notify) (struct block_device *, unsigned long);\n\tstruct module *owner;\n};\n\nextern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,\n\t\t\t\t unsigned long);\n#else /* CONFIG_BLOCK */\n/*\n * stubs for when the block layer is configured out\n */\n#define buffer_heads_over_limit 0\n\nstatic inline long nr_blockdev_pages(void)\n{\n\treturn 0;\n}\n\nstruct blk_plug {\n};\n\nstatic inline void blk_start_plug(struct blk_plug *plug)\n{\n}\n\nstatic inline void blk_finish_plug(struct blk_plug *plug)\n{\n}\n\nstatic inline void blk_flush_plug(struct task_struct *task)\n{\n}\n\nstatic inline void blk_schedule_flush_plug(struct task_struct *task)\n{\n}\n\n\nstatic inline bool blk_needs_flush_plug(struct task_struct *tsk)\n{\n\treturn false;\n}\n\n#endif /* CONFIG_BLOCK */\n\n#endif\n"], "fixing_code": ["/*\n * Copyright (C) 2001 Jens Axboe <axboe@suse.de>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n *\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public Licens\n * along with this program; if not, write to the Free Software\n * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-\n *\n */\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/module.h>\n#include <linux/blkdev.h>\n#include <linux/capability.h>\n#include <linux/completion.h>\n#include <linux/cdrom.h>\n#include <linux/ratelimit.h>\n#include <linux/slab.h>\n#include <linux/times.h>\n#include <asm/uaccess.h>\n\n#include <scsi/scsi.h>\n#include <scsi/scsi_ioctl.h>\n#include <scsi/scsi_cmnd.h>\n\nstruct blk_cmd_filter {\n\tunsigned long read_ok[BLK_SCSI_CMD_PER_LONG];\n\tunsigned long write_ok[BLK_SCSI_CMD_PER_LONG];\n};\n\nstatic struct blk_cmd_filter blk_default_cmd_filter;\n\n/* Command group 3 is reserved and should never be used.  */\nconst unsigned char scsi_command_size_tbl[8] =\n{\n\t6, 10, 10, 12,\n\t16, 12, 10, 10\n};\nEXPORT_SYMBOL(scsi_command_size_tbl);\n\n#include <scsi/sg.h>\n\nstatic int sg_get_version(int __user *p)\n{\n\tstatic const int sg_version_num = 30527;\n\treturn put_user(sg_version_num, p);\n}\n\nstatic int scsi_get_idlun(struct request_queue *q, int __user *p)\n{\n\treturn put_user(0, p);\n}\n\nstatic int scsi_get_bus(struct request_queue *q, int __user *p)\n{\n\treturn put_user(0, p);\n}\n\nstatic int sg_get_timeout(struct request_queue *q)\n{\n\treturn jiffies_to_clock_t(q->sg_timeout);\n}\n\nstatic int sg_set_timeout(struct request_queue *q, int __user *p)\n{\n\tint timeout, err = get_user(timeout, p);\n\n\tif (!err)\n\t\tq->sg_timeout = clock_t_to_jiffies(timeout);\n\n\treturn err;\n}\n\nstatic int sg_get_reserved_size(struct request_queue *q, int __user *p)\n{\n\tunsigned val = min(q->sg_reserved_size, queue_max_sectors(q) << 9);\n\n\treturn put_user(val, p);\n}\n\nstatic int sg_set_reserved_size(struct request_queue *q, int __user *p)\n{\n\tint size, err = get_user(size, p);\n\n\tif (err)\n\t\treturn err;\n\n\tif (size < 0)\n\t\treturn -EINVAL;\n\tif (size > (queue_max_sectors(q) << 9))\n\t\tsize = queue_max_sectors(q) << 9;\n\n\tq->sg_reserved_size = size;\n\treturn 0;\n}\n\n/*\n * will always return that we are ATAPI even for a real SCSI drive, I'm not\n * so sure this is worth doing anything about (why would you care??)\n */\nstatic int sg_emulated_host(struct request_queue *q, int __user *p)\n{\n\treturn put_user(1, p);\n}\n\nstatic void blk_set_cmd_filter_defaults(struct blk_cmd_filter *filter)\n{\n\t/* Basic read-only commands */\n\t__set_bit(TEST_UNIT_READY, filter->read_ok);\n\t__set_bit(REQUEST_SENSE, filter->read_ok);\n\t__set_bit(READ_6, filter->read_ok);\n\t__set_bit(READ_10, filter->read_ok);\n\t__set_bit(READ_12, filter->read_ok);\n\t__set_bit(READ_16, filter->read_ok);\n\t__set_bit(READ_BUFFER, filter->read_ok);\n\t__set_bit(READ_DEFECT_DATA, filter->read_ok);\n\t__set_bit(READ_CAPACITY, filter->read_ok);\n\t__set_bit(READ_LONG, filter->read_ok);\n\t__set_bit(INQUIRY, filter->read_ok);\n\t__set_bit(MODE_SENSE, filter->read_ok);\n\t__set_bit(MODE_SENSE_10, filter->read_ok);\n\t__set_bit(LOG_SENSE, filter->read_ok);\n\t__set_bit(START_STOP, filter->read_ok);\n\t__set_bit(GPCMD_VERIFY_10, filter->read_ok);\n\t__set_bit(VERIFY_16, filter->read_ok);\n\t__set_bit(REPORT_LUNS, filter->read_ok);\n\t__set_bit(SERVICE_ACTION_IN, filter->read_ok);\n\t__set_bit(RECEIVE_DIAGNOSTIC, filter->read_ok);\n\t__set_bit(MAINTENANCE_IN, filter->read_ok);\n\t__set_bit(GPCMD_READ_BUFFER_CAPACITY, filter->read_ok);\n\n\t/* Audio CD commands */\n\t__set_bit(GPCMD_PLAY_CD, filter->read_ok);\n\t__set_bit(GPCMD_PLAY_AUDIO_10, filter->read_ok);\n\t__set_bit(GPCMD_PLAY_AUDIO_MSF, filter->read_ok);\n\t__set_bit(GPCMD_PLAY_AUDIO_TI, filter->read_ok);\n\t__set_bit(GPCMD_PAUSE_RESUME, filter->read_ok);\n\n\t/* CD/DVD data reading */\n\t__set_bit(GPCMD_READ_CD, filter->read_ok);\n\t__set_bit(GPCMD_READ_CD_MSF, filter->read_ok);\n\t__set_bit(GPCMD_READ_DISC_INFO, filter->read_ok);\n\t__set_bit(GPCMD_READ_CDVD_CAPACITY, filter->read_ok);\n\t__set_bit(GPCMD_READ_DVD_STRUCTURE, filter->read_ok);\n\t__set_bit(GPCMD_READ_HEADER, filter->read_ok);\n\t__set_bit(GPCMD_READ_TRACK_RZONE_INFO, filter->read_ok);\n\t__set_bit(GPCMD_READ_SUBCHANNEL, filter->read_ok);\n\t__set_bit(GPCMD_READ_TOC_PMA_ATIP, filter->read_ok);\n\t__set_bit(GPCMD_REPORT_KEY, filter->read_ok);\n\t__set_bit(GPCMD_SCAN, filter->read_ok);\n\t__set_bit(GPCMD_GET_CONFIGURATION, filter->read_ok);\n\t__set_bit(GPCMD_READ_FORMAT_CAPACITIES, filter->read_ok);\n\t__set_bit(GPCMD_GET_EVENT_STATUS_NOTIFICATION, filter->read_ok);\n\t__set_bit(GPCMD_GET_PERFORMANCE, filter->read_ok);\n\t__set_bit(GPCMD_SEEK, filter->read_ok);\n\t__set_bit(GPCMD_STOP_PLAY_SCAN, filter->read_ok);\n\n\t/* Basic writing commands */\n\t__set_bit(WRITE_6, filter->write_ok);\n\t__set_bit(WRITE_10, filter->write_ok);\n\t__set_bit(WRITE_VERIFY, filter->write_ok);\n\t__set_bit(WRITE_12, filter->write_ok);\n\t__set_bit(WRITE_VERIFY_12, filter->write_ok);\n\t__set_bit(WRITE_16, filter->write_ok);\n\t__set_bit(WRITE_LONG, filter->write_ok);\n\t__set_bit(WRITE_LONG_2, filter->write_ok);\n\t__set_bit(ERASE, filter->write_ok);\n\t__set_bit(GPCMD_MODE_SELECT_10, filter->write_ok);\n\t__set_bit(MODE_SELECT, filter->write_ok);\n\t__set_bit(LOG_SELECT, filter->write_ok);\n\t__set_bit(GPCMD_BLANK, filter->write_ok);\n\t__set_bit(GPCMD_CLOSE_TRACK, filter->write_ok);\n\t__set_bit(GPCMD_FLUSH_CACHE, filter->write_ok);\n\t__set_bit(GPCMD_FORMAT_UNIT, filter->write_ok);\n\t__set_bit(GPCMD_REPAIR_RZONE_TRACK, filter->write_ok);\n\t__set_bit(GPCMD_RESERVE_RZONE_TRACK, filter->write_ok);\n\t__set_bit(GPCMD_SEND_DVD_STRUCTURE, filter->write_ok);\n\t__set_bit(GPCMD_SEND_EVENT, filter->write_ok);\n\t__set_bit(GPCMD_SEND_KEY, filter->write_ok);\n\t__set_bit(GPCMD_SEND_OPC, filter->write_ok);\n\t__set_bit(GPCMD_SEND_CUE_SHEET, filter->write_ok);\n\t__set_bit(GPCMD_SET_SPEED, filter->write_ok);\n\t__set_bit(GPCMD_PREVENT_ALLOW_MEDIUM_REMOVAL, filter->write_ok);\n\t__set_bit(GPCMD_LOAD_UNLOAD, filter->write_ok);\n\t__set_bit(GPCMD_SET_STREAMING, filter->write_ok);\n\t__set_bit(GPCMD_SET_READ_AHEAD, filter->write_ok);\n}\n\nint blk_verify_command(unsigned char *cmd, fmode_t has_write_perm)\n{\n\tstruct blk_cmd_filter *filter = &blk_default_cmd_filter;\n\n\t/* root can do any command. */\n\tif (capable(CAP_SYS_RAWIO))\n\t\treturn 0;\n\n\t/* if there's no filter set, assume we're filtering everything out */\n\tif (!filter)\n\t\treturn -EPERM;\n\n\t/* Anybody who can open the device can do a read-safe command */\n\tif (test_bit(cmd[0], filter->read_ok))\n\t\treturn 0;\n\n\t/* Write-safe commands require a writable open */\n\tif (test_bit(cmd[0], filter->write_ok) && has_write_perm)\n\t\treturn 0;\n\n\treturn -EPERM;\n}\nEXPORT_SYMBOL(blk_verify_command);\n\nstatic int blk_fill_sghdr_rq(struct request_queue *q, struct request *rq,\n\t\t\t     struct sg_io_hdr *hdr, fmode_t mode)\n{\n\tif (copy_from_user(rq->cmd, hdr->cmdp, hdr->cmd_len))\n\t\treturn -EFAULT;\n\tif (blk_verify_command(rq->cmd, mode & FMODE_WRITE))\n\t\treturn -EPERM;\n\n\t/*\n\t * fill in request structure\n\t */\n\trq->cmd_len = hdr->cmd_len;\n\trq->cmd_type = REQ_TYPE_BLOCK_PC;\n\n\trq->timeout = msecs_to_jiffies(hdr->timeout);\n\tif (!rq->timeout)\n\t\trq->timeout = q->sg_timeout;\n\tif (!rq->timeout)\n\t\trq->timeout = BLK_DEFAULT_SG_TIMEOUT;\n\tif (rq->timeout < BLK_MIN_SG_TIMEOUT)\n\t\trq->timeout = BLK_MIN_SG_TIMEOUT;\n\n\treturn 0;\n}\n\nstatic int blk_complete_sghdr_rq(struct request *rq, struct sg_io_hdr *hdr,\n\t\t\t\t struct bio *bio)\n{\n\tint r, ret = 0;\n\n\t/*\n\t * fill in all the output members\n\t */\n\thdr->status = rq->errors & 0xff;\n\thdr->masked_status = status_byte(rq->errors);\n\thdr->msg_status = msg_byte(rq->errors);\n\thdr->host_status = host_byte(rq->errors);\n\thdr->driver_status = driver_byte(rq->errors);\n\thdr->info = 0;\n\tif (hdr->masked_status || hdr->host_status || hdr->driver_status)\n\t\thdr->info |= SG_INFO_CHECK;\n\thdr->resid = rq->resid_len;\n\thdr->sb_len_wr = 0;\n\n\tif (rq->sense_len && hdr->sbp) {\n\t\tint len = min((unsigned int) hdr->mx_sb_len, rq->sense_len);\n\n\t\tif (!copy_to_user(hdr->sbp, rq->sense, len))\n\t\t\thdr->sb_len_wr = len;\n\t\telse\n\t\t\tret = -EFAULT;\n\t}\n\n\tr = blk_rq_unmap_user(bio);\n\tif (!ret)\n\t\tret = r;\n\tblk_put_request(rq);\n\n\treturn ret;\n}\n\nstatic int sg_io(struct request_queue *q, struct gendisk *bd_disk,\n\t\tstruct sg_io_hdr *hdr, fmode_t mode)\n{\n\tunsigned long start_time;\n\tint writing = 0, ret = 0;\n\tstruct request *rq;\n\tchar sense[SCSI_SENSE_BUFFERSIZE];\n\tstruct bio *bio;\n\n\tif (hdr->interface_id != 'S')\n\t\treturn -EINVAL;\n\tif (hdr->cmd_len > BLK_MAX_CDB)\n\t\treturn -EINVAL;\n\n\tif (hdr->dxfer_len > (queue_max_hw_sectors(q) << 9))\n\t\treturn -EIO;\n\n\tif (hdr->dxfer_len)\n\t\tswitch (hdr->dxfer_direction) {\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\tcase SG_DXFER_TO_DEV:\n\t\t\twriting = 1;\n\t\t\tbreak;\n\t\tcase SG_DXFER_TO_FROM_DEV:\n\t\tcase SG_DXFER_FROM_DEV:\n\t\t\tbreak;\n\t\t}\n\n\trq = blk_get_request(q, writing ? WRITE : READ, GFP_KERNEL);\n\tif (!rq)\n\t\treturn -ENOMEM;\n\n\tif (blk_fill_sghdr_rq(q, rq, hdr, mode)) {\n\t\tblk_put_request(rq);\n\t\treturn -EFAULT;\n\t}\n\n\tif (hdr->iovec_count) {\n\t\tconst int size = sizeof(struct sg_iovec) * hdr->iovec_count;\n\t\tsize_t iov_data_len;\n\t\tstruct sg_iovec *sg_iov;\n\t\tstruct iovec *iov;\n\t\tint i;\n\n\t\tsg_iov = kmalloc(size, GFP_KERNEL);\n\t\tif (!sg_iov) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (copy_from_user(sg_iov, hdr->dxferp, size)) {\n\t\t\tkfree(sg_iov);\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\t/*\n\t\t * Sum up the vecs, making sure they don't overflow\n\t\t */\n\t\tiov = (struct iovec *) sg_iov;\n\t\tiov_data_len = 0;\n\t\tfor (i = 0; i < hdr->iovec_count; i++) {\n\t\t\tif (iov_data_len + iov[i].iov_len < iov_data_len) {\n\t\t\t\tkfree(sg_iov);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tiov_data_len += iov[i].iov_len;\n\t\t}\n\n\t\t/* SG_IO howto says that the shorter of the two wins */\n\t\tif (hdr->dxfer_len < iov_data_len) {\n\t\t\thdr->iovec_count = iov_shorten(iov,\n\t\t\t\t\t\t       hdr->iovec_count,\n\t\t\t\t\t\t       hdr->dxfer_len);\n\t\t\tiov_data_len = hdr->dxfer_len;\n\t\t}\n\n\t\tret = blk_rq_map_user_iov(q, rq, NULL, sg_iov, hdr->iovec_count,\n\t\t\t\t\t  iov_data_len, GFP_KERNEL);\n\t\tkfree(sg_iov);\n\t} else if (hdr->dxfer_len)\n\t\tret = blk_rq_map_user(q, rq, NULL, hdr->dxferp, hdr->dxfer_len,\n\t\t\t\t      GFP_KERNEL);\n\n\tif (ret)\n\t\tgoto out;\n\n\tbio = rq->bio;\n\tmemset(sense, 0, sizeof(sense));\n\trq->sense = sense;\n\trq->sense_len = 0;\n\trq->retries = 0;\n\n\tstart_time = jiffies;\n\n\t/* ignore return value. All information is passed back to caller\n\t * (if he doesn't check that is his problem).\n\t * N.B. a non-zero SCSI status is _not_ necessarily an error.\n\t */\n\tblk_execute_rq(q, bd_disk, rq, 0);\n\n\thdr->duration = jiffies_to_msecs(jiffies - start_time);\n\n\treturn blk_complete_sghdr_rq(rq, hdr, bio);\nout:\n\tblk_put_request(rq);\n\treturn ret;\n}\n\n/**\n * sg_scsi_ioctl  --  handle deprecated SCSI_IOCTL_SEND_COMMAND ioctl\n * @file:\tfile this ioctl operates on (optional)\n * @q:\t\trequest queue to send scsi commands down\n * @disk:\tgendisk to operate on (option)\n * @sic:\tuserspace structure describing the command to perform\n *\n * Send down the scsi command described by @sic to the device below\n * the request queue @q.  If @file is non-NULL it's used to perform\n * fine-grained permission checks that allow users to send down\n * non-destructive SCSI commands.  If the caller has a struct gendisk\n * available it should be passed in as @disk to allow the low level\n * driver to use the information contained in it.  A non-NULL @disk\n * is only allowed if the caller knows that the low level driver doesn't\n * need it (e.g. in the scsi subsystem).\n *\n * Notes:\n *   -  This interface is deprecated - users should use the SG_IO\n *      interface instead, as this is a more flexible approach to\n *      performing SCSI commands on a device.\n *   -  The SCSI command length is determined by examining the 1st byte\n *      of the given command. There is no way to override this.\n *   -  Data transfers are limited to PAGE_SIZE\n *   -  The length (x + y) must be at least OMAX_SB_LEN bytes long to\n *      accommodate the sense buffer when an error occurs.\n *      The sense buffer is truncated to OMAX_SB_LEN (16) bytes so that\n *      old code will not be surprised.\n *   -  If a Unix error occurs (e.g. ENOMEM) then the user will receive\n *      a negative return and the Unix error code in 'errno'.\n *      If the SCSI command succeeds then 0 is returned.\n *      Positive numbers returned are the compacted SCSI error codes (4\n *      bytes in one int) where the lowest byte is the SCSI status.\n */\n#define OMAX_SB_LEN 16          /* For backward compatibility */\nint sg_scsi_ioctl(struct request_queue *q, struct gendisk *disk, fmode_t mode,\n\t\tstruct scsi_ioctl_command __user *sic)\n{\n\tstruct request *rq;\n\tint err;\n\tunsigned int in_len, out_len, bytes, opcode, cmdlen;\n\tchar *buffer = NULL, sense[SCSI_SENSE_BUFFERSIZE];\n\n\tif (!sic)\n\t\treturn -EINVAL;\n\n\t/*\n\t * get in an out lengths, verify they don't exceed a page worth of data\n\t */\n\tif (get_user(in_len, &sic->inlen))\n\t\treturn -EFAULT;\n\tif (get_user(out_len, &sic->outlen))\n\t\treturn -EFAULT;\n\tif (in_len > PAGE_SIZE || out_len > PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (get_user(opcode, sic->data))\n\t\treturn -EFAULT;\n\n\tbytes = max(in_len, out_len);\n\tif (bytes) {\n\t\tbuffer = kzalloc(bytes, q->bounce_gfp | GFP_USER| __GFP_NOWARN);\n\t\tif (!buffer)\n\t\t\treturn -ENOMEM;\n\n\t}\n\n\trq = blk_get_request(q, in_len ? WRITE : READ, __GFP_WAIT);\n\n\tcmdlen = COMMAND_SIZE(opcode);\n\n\t/*\n\t * get command and data to send to device, if any\n\t */\n\terr = -EFAULT;\n\trq->cmd_len = cmdlen;\n\tif (copy_from_user(rq->cmd, sic->data, cmdlen))\n\t\tgoto error;\n\n\tif (in_len && copy_from_user(buffer, sic->data + cmdlen, in_len))\n\t\tgoto error;\n\n\terr = blk_verify_command(rq->cmd, mode & FMODE_WRITE);\n\tif (err)\n\t\tgoto error;\n\n\t/* default.  possible overriden later */\n\trq->retries = 5;\n\n\tswitch (opcode) {\n\tcase SEND_DIAGNOSTIC:\n\tcase FORMAT_UNIT:\n\t\trq->timeout = FORMAT_UNIT_TIMEOUT;\n\t\trq->retries = 1;\n\t\tbreak;\n\tcase START_STOP:\n\t\trq->timeout = START_STOP_TIMEOUT;\n\t\tbreak;\n\tcase MOVE_MEDIUM:\n\t\trq->timeout = MOVE_MEDIUM_TIMEOUT;\n\t\tbreak;\n\tcase READ_ELEMENT_STATUS:\n\t\trq->timeout = READ_ELEMENT_STATUS_TIMEOUT;\n\t\tbreak;\n\tcase READ_DEFECT_DATA:\n\t\trq->timeout = READ_DEFECT_DATA_TIMEOUT;\n\t\trq->retries = 1;\n\t\tbreak;\n\tdefault:\n\t\trq->timeout = BLK_DEFAULT_SG_TIMEOUT;\n\t\tbreak;\n\t}\n\n\tif (bytes && blk_rq_map_kern(q, rq, buffer, bytes, __GFP_WAIT)) {\n\t\terr = DRIVER_ERROR << 24;\n\t\tgoto out;\n\t}\n\n\tmemset(sense, 0, sizeof(sense));\n\trq->sense = sense;\n\trq->sense_len = 0;\n\trq->cmd_type = REQ_TYPE_BLOCK_PC;\n\n\tblk_execute_rq(q, disk, rq, 0);\n\nout:\n\terr = rq->errors & 0xff;\t/* only 8 bit SCSI status */\n\tif (err) {\n\t\tif (rq->sense_len && rq->sense) {\n\t\t\tbytes = (OMAX_SB_LEN > rq->sense_len) ?\n\t\t\t\trq->sense_len : OMAX_SB_LEN;\n\t\t\tif (copy_to_user(sic->data, rq->sense, bytes))\n\t\t\t\terr = -EFAULT;\n\t\t}\n\t} else {\n\t\tif (copy_to_user(sic->data, buffer, out_len))\n\t\t\terr = -EFAULT;\n\t}\n\t\nerror:\n\tkfree(buffer);\n\tblk_put_request(rq);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(sg_scsi_ioctl);\n\n/* Send basic block requests */\nstatic int __blk_send_generic(struct request_queue *q, struct gendisk *bd_disk,\n\t\t\t      int cmd, int data)\n{\n\tstruct request *rq;\n\tint err;\n\n\trq = blk_get_request(q, WRITE, __GFP_WAIT);\n\trq->cmd_type = REQ_TYPE_BLOCK_PC;\n\trq->timeout = BLK_DEFAULT_SG_TIMEOUT;\n\trq->cmd[0] = cmd;\n\trq->cmd[4] = data;\n\trq->cmd_len = 6;\n\terr = blk_execute_rq(q, bd_disk, rq, 0);\n\tblk_put_request(rq);\n\n\treturn err;\n}\n\nstatic inline int blk_send_start_stop(struct request_queue *q,\n\t\t\t\t      struct gendisk *bd_disk, int data)\n{\n\treturn __blk_send_generic(q, bd_disk, GPCMD_START_STOP_UNIT, data);\n}\n\nint scsi_cmd_ioctl(struct request_queue *q, struct gendisk *bd_disk, fmode_t mode,\n\t\t   unsigned int cmd, void __user *arg)\n{\n\tint err;\n\n\tif (!q)\n\t\treturn -ENXIO;\n\n\tswitch (cmd) {\n\t\t/*\n\t\t * new sgv3 interface\n\t\t */\n\t\tcase SG_GET_VERSION_NUM:\n\t\t\terr = sg_get_version(arg);\n\t\t\tbreak;\n\t\tcase SCSI_IOCTL_GET_IDLUN:\n\t\t\terr = scsi_get_idlun(q, arg);\n\t\t\tbreak;\n\t\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\t\t\terr = scsi_get_bus(q, arg);\n\t\t\tbreak;\n\t\tcase SG_SET_TIMEOUT:\n\t\t\terr = sg_set_timeout(q, arg);\n\t\t\tbreak;\n\t\tcase SG_GET_TIMEOUT:\n\t\t\terr = sg_get_timeout(q);\n\t\t\tbreak;\n\t\tcase SG_GET_RESERVED_SIZE:\n\t\t\terr = sg_get_reserved_size(q, arg);\n\t\t\tbreak;\n\t\tcase SG_SET_RESERVED_SIZE:\n\t\t\terr = sg_set_reserved_size(q, arg);\n\t\t\tbreak;\n\t\tcase SG_EMULATED_HOST:\n\t\t\terr = sg_emulated_host(q, arg);\n\t\t\tbreak;\n\t\tcase SG_IO: {\n\t\t\tstruct sg_io_hdr hdr;\n\n\t\t\terr = -EFAULT;\n\t\t\tif (copy_from_user(&hdr, arg, sizeof(hdr)))\n\t\t\t\tbreak;\n\t\t\terr = sg_io(q, bd_disk, &hdr, mode);\n\t\t\tif (err == -EFAULT)\n\t\t\t\tbreak;\n\n\t\t\tif (copy_to_user(arg, &hdr, sizeof(hdr)))\n\t\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcase CDROM_SEND_PACKET: {\n\t\t\tstruct cdrom_generic_command cgc;\n\t\t\tstruct sg_io_hdr hdr;\n\n\t\t\terr = -EFAULT;\n\t\t\tif (copy_from_user(&cgc, arg, sizeof(cgc)))\n\t\t\t\tbreak;\n\t\t\tcgc.timeout = clock_t_to_jiffies(cgc.timeout);\n\t\t\tmemset(&hdr, 0, sizeof(hdr));\n\t\t\thdr.interface_id = 'S';\n\t\t\thdr.cmd_len = sizeof(cgc.cmd);\n\t\t\thdr.dxfer_len = cgc.buflen;\n\t\t\terr = 0;\n\t\t\tswitch (cgc.data_direction) {\n\t\t\t\tcase CGC_DATA_UNKNOWN:\n\t\t\t\t\thdr.dxfer_direction = SG_DXFER_UNKNOWN;\n\t\t\t\t\tbreak;\n\t\t\t\tcase CGC_DATA_WRITE:\n\t\t\t\t\thdr.dxfer_direction = SG_DXFER_TO_DEV;\n\t\t\t\t\tbreak;\n\t\t\t\tcase CGC_DATA_READ:\n\t\t\t\t\thdr.dxfer_direction = SG_DXFER_FROM_DEV;\n\t\t\t\t\tbreak;\n\t\t\t\tcase CGC_DATA_NONE:\n\t\t\t\t\thdr.dxfer_direction = SG_DXFER_NONE;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\terr = -EINVAL;\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tbreak;\n\n\t\t\thdr.dxferp = cgc.buffer;\n\t\t\thdr.sbp = cgc.sense;\n\t\t\tif (hdr.sbp)\n\t\t\t\thdr.mx_sb_len = sizeof(struct request_sense);\n\t\t\thdr.timeout = jiffies_to_msecs(cgc.timeout);\n\t\t\thdr.cmdp = ((struct cdrom_generic_command __user*) arg)->cmd;\n\t\t\thdr.cmd_len = sizeof(cgc.cmd);\n\n\t\t\terr = sg_io(q, bd_disk, &hdr, mode);\n\t\t\tif (err == -EFAULT)\n\t\t\t\tbreak;\n\n\t\t\tif (hdr.status)\n\t\t\t\terr = -EIO;\n\n\t\t\tcgc.stat = err;\n\t\t\tcgc.buflen = hdr.resid;\n\t\t\tif (copy_to_user(arg, &cgc, sizeof(cgc)))\n\t\t\t\terr = -EFAULT;\n\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * old junk scsi send command ioctl\n\t\t */\n\t\tcase SCSI_IOCTL_SEND_COMMAND:\n\t\t\tprintk(KERN_WARNING \"program %s is using a deprecated SCSI ioctl, please convert it to SG_IO\\n\", current->comm);\n\t\t\terr = -EINVAL;\n\t\t\tif (!arg)\n\t\t\t\tbreak;\n\n\t\t\terr = sg_scsi_ioctl(q, bd_disk, mode, arg);\n\t\t\tbreak;\n\t\tcase CDROMCLOSETRAY:\n\t\t\terr = blk_send_start_stop(q, bd_disk, 0x03);\n\t\t\tbreak;\n\t\tcase CDROMEJECT:\n\t\t\terr = blk_send_start_stop(q, bd_disk, 0x02);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -ENOTTY;\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL(scsi_cmd_ioctl);\n\nint scsi_verify_blk_ioctl(struct block_device *bd, unsigned int cmd)\n{\n\tif (bd && bd == bd->bd_contains)\n\t\treturn 0;\n\n\t/* Actually none of these is particularly useful on a partition,\n\t * but they are safe.\n\t */\n\tswitch (cmd) {\n\tcase SCSI_IOCTL_GET_IDLUN:\n\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\tcase SCSI_IOCTL_GET_PCI:\n\tcase SCSI_IOCTL_PROBE_HOST:\n\tcase SG_GET_VERSION_NUM:\n\tcase SG_SET_TIMEOUT:\n\tcase SG_GET_TIMEOUT:\n\tcase SG_GET_RESERVED_SIZE:\n\tcase SG_SET_RESERVED_SIZE:\n\tcase SG_EMULATED_HOST:\n\t\treturn 0;\n\tcase CDROM_GET_CAPABILITY:\n\t\t/* Keep this until we remove the printk below.  udev sends it\n\t\t * and we do not want to spam dmesg about it.   CD-ROMs do\n\t\t * not have partitions, so we get here only for disks.\n\t\t */\n\t\treturn -ENOIOCTLCMD;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t/* In particular, rule out all resets and host-specific ioctls.  */\n\tprintk_ratelimited(KERN_WARNING\n\t\t\t   \"%s: sending ioctl %x to a partition!\\n\", current->comm, cmd);\n\n\treturn capable(CAP_SYS_RAWIO) ? 0 : -ENOIOCTLCMD;\n}\nEXPORT_SYMBOL(scsi_verify_blk_ioctl);\n\nint scsi_cmd_blk_ioctl(struct block_device *bd, fmode_t mode,\n\t\t       unsigned int cmd, void __user *arg)\n{\n\tint ret;\n\n\tret = scsi_verify_blk_ioctl(bd, cmd);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn scsi_cmd_ioctl(bd->bd_disk->queue, bd->bd_disk, mode, cmd, arg);\n}\nEXPORT_SYMBOL(scsi_cmd_blk_ioctl);\n\nstatic int __init blk_scsi_ioctl_init(void)\n{\n\tblk_set_cmd_filter_defaults(&blk_default_cmd_filter);\n\treturn 0;\n}\nfs_initcall(blk_scsi_ioctl_init);\n", "/*\n *      sd.c Copyright (C) 1992 Drew Eckhardt\n *           Copyright (C) 1993, 1994, 1995, 1999 Eric Youngdale\n *\n *      Linux scsi disk driver\n *              Initial versions: Drew Eckhardt\n *              Subsequent revisions: Eric Youngdale\n *\tModification history:\n *       - Drew Eckhardt <drew@colorado.edu> original\n *       - Eric Youngdale <eric@andante.org> add scatter-gather, multiple \n *         outstanding request, and other enhancements.\n *         Support loadable low-level scsi drivers.\n *       - Jirka Hanika <geo@ff.cuni.cz> support more scsi disks using \n *         eight major numbers.\n *       - Richard Gooch <rgooch@atnf.csiro.au> support devfs.\n *\t - Torben Mathiasen <tmm@image.dk> Resource allocation fixes in \n *\t   sd_init and cleanups.\n *\t - Alex Davis <letmein@erols.com> Fix problem where partition info\n *\t   not being read in sd_open. Fix problem where removable media \n *\t   could be ejected after sd_open.\n *\t - Douglas Gilbert <dgilbert@interlog.com> cleanup for lk 2.5.x\n *\t - Badari Pulavarty <pbadari@us.ibm.com>, Matthew Wilcox \n *\t   <willy@debian.org>, Kurt Garloff <garloff@suse.de>: \n *\t   Support 32k/1M disks.\n *\n *\tLogging policy (needs CONFIG_SCSI_LOGGING defined):\n *\t - setting up transfer: SCSI_LOG_HLQUEUE levels 1 and 2\n *\t - end of transfer (bh + scsi_lib): SCSI_LOG_HLCOMPLETE level 1\n *\t - entering sd_ioctl: SCSI_LOG_IOCTL level 1\n *\t - entering other commands: SCSI_LOG_HLQUEUE level 3\n *\tNote: when the logging level is set by the user, it must be greater\n *\tthan the level indicated above to trigger output.\t\n */\n\n#include <linux/module.h>\n#include <linux/fs.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/bio.h>\n#include <linux/genhd.h>\n#include <linux/hdreg.h>\n#include <linux/errno.h>\n#include <linux/idr.h>\n#include <linux/interrupt.h>\n#include <linux/init.h>\n#include <linux/blkdev.h>\n#include <linux/blkpg.h>\n#include <linux/delay.h>\n#include <linux/mutex.h>\n#include <linux/string_helpers.h>\n#include <linux/async.h>\n#include <linux/slab.h>\n#include <linux/pm_runtime.h>\n#include <asm/uaccess.h>\n#include <asm/unaligned.h>\n\n#include <scsi/scsi.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_dbg.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_driver.h>\n#include <scsi/scsi_eh.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_ioctl.h>\n#include <scsi/scsicam.h>\n\n#include \"sd.h\"\n#include \"scsi_logging.h\"\n\nMODULE_AUTHOR(\"Eric Youngdale\");\nMODULE_DESCRIPTION(\"SCSI disk (sd) driver\");\nMODULE_LICENSE(\"GPL\");\n\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK0_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK1_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK2_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK3_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK4_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK5_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK6_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK7_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK8_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK9_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK10_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK11_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK12_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK13_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK14_MAJOR);\nMODULE_ALIAS_BLOCKDEV_MAJOR(SCSI_DISK15_MAJOR);\nMODULE_ALIAS_SCSI_DEVICE(TYPE_DISK);\nMODULE_ALIAS_SCSI_DEVICE(TYPE_MOD);\nMODULE_ALIAS_SCSI_DEVICE(TYPE_RBC);\n\n#if !defined(CONFIG_DEBUG_BLOCK_EXT_DEVT)\n#define SD_MINORS\t16\n#else\n#define SD_MINORS\t0\n#endif\n\nstatic void sd_config_discard(struct scsi_disk *, unsigned int);\nstatic int  sd_revalidate_disk(struct gendisk *);\nstatic void sd_unlock_native_capacity(struct gendisk *disk);\nstatic int  sd_probe(struct device *);\nstatic int  sd_remove(struct device *);\nstatic void sd_shutdown(struct device *);\nstatic int sd_suspend(struct device *, pm_message_t state);\nstatic int sd_resume(struct device *);\nstatic void sd_rescan(struct device *);\nstatic int sd_done(struct scsi_cmnd *);\nstatic void sd_read_capacity(struct scsi_disk *sdkp, unsigned char *buffer);\nstatic void scsi_disk_release(struct device *cdev);\nstatic void sd_print_sense_hdr(struct scsi_disk *, struct scsi_sense_hdr *);\nstatic void sd_print_result(struct scsi_disk *, int);\n\nstatic DEFINE_SPINLOCK(sd_index_lock);\nstatic DEFINE_IDA(sd_index_ida);\n\n/* This semaphore is used to mediate the 0->1 reference get in the\n * face of object destruction (i.e. we can't allow a get on an\n * object after last put) */\nstatic DEFINE_MUTEX(sd_ref_mutex);\n\nstatic struct kmem_cache *sd_cdb_cache;\nstatic mempool_t *sd_cdb_pool;\n\nstatic const char *sd_cache_types[] = {\n\t\"write through\", \"none\", \"write back\",\n\t\"write back, no read (daft)\"\n};\n\nstatic ssize_t\nsd_store_cache_type(struct device *dev, struct device_attribute *attr,\n\t\t    const char *buf, size_t count)\n{\n\tint i, ct = -1, rcd, wce, sp;\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\tchar buffer[64];\n\tchar *buffer_data;\n\tstruct scsi_mode_data data;\n\tstruct scsi_sense_hdr sshdr;\n\tint len;\n\n\tif (sdp->type != TYPE_DISK)\n\t\t/* no cache control on RBC devices; theoretically they\n\t\t * can do it, but there's probably so many exceptions\n\t\t * it's not worth the risk */\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < ARRAY_SIZE(sd_cache_types); i++) {\n\t\tlen = strlen(sd_cache_types[i]);\n\t\tif (strncmp(sd_cache_types[i], buf, len) == 0 &&\n\t\t    buf[len] == '\\n') {\n\t\t\tct = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (ct < 0)\n\t\treturn -EINVAL;\n\trcd = ct & 0x01 ? 1 : 0;\n\twce = ct & 0x02 ? 1 : 0;\n\tif (scsi_mode_sense(sdp, 0x08, 8, buffer, sizeof(buffer), SD_TIMEOUT,\n\t\t\t    SD_MAX_RETRIES, &data, NULL))\n\t\treturn -EINVAL;\n\tlen = min_t(size_t, sizeof(buffer), data.length - data.header_length -\n\t\t  data.block_descriptor_length);\n\tbuffer_data = buffer + data.header_length +\n\t\tdata.block_descriptor_length;\n\tbuffer_data[2] &= ~0x05;\n\tbuffer_data[2] |= wce << 2 | rcd;\n\tsp = buffer_data[0] & 0x80 ? 1 : 0;\n\n\tif (scsi_mode_select(sdp, 1, sp, 8, buffer_data, len, SD_TIMEOUT,\n\t\t\t     SD_MAX_RETRIES, &data, &sshdr)) {\n\t\tif (scsi_sense_valid(&sshdr))\n\t\t\tsd_print_sense_hdr(sdkp, &sshdr);\n\t\treturn -EINVAL;\n\t}\n\trevalidate_disk(sdkp->disk);\n\treturn count;\n}\n\nstatic ssize_t\nsd_store_manage_start_stop(struct device *dev, struct device_attribute *attr,\n\t\t\t   const char *buf, size_t count)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tsdp->manage_start_stop = simple_strtoul(buf, NULL, 10);\n\n\treturn count;\n}\n\nstatic ssize_t\nsd_store_allow_restart(struct device *dev, struct device_attribute *attr,\n\t\t       const char *buf, size_t count)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tif (sdp->type != TYPE_DISK)\n\t\treturn -EINVAL;\n\n\tsdp->allow_restart = simple_strtoul(buf, NULL, 10);\n\n\treturn count;\n}\n\nstatic ssize_t\nsd_show_cache_type(struct device *dev, struct device_attribute *attr,\n\t\t   char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tint ct = sdkp->RCD + 2*sdkp->WCE;\n\n\treturn snprintf(buf, 40, \"%s\\n\", sd_cache_types[ct]);\n}\n\nstatic ssize_t\nsd_show_fua(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 20, \"%u\\n\", sdkp->DPOFUA);\n}\n\nstatic ssize_t\nsd_show_manage_start_stop(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\n\treturn snprintf(buf, 20, \"%u\\n\", sdp->manage_start_stop);\n}\n\nstatic ssize_t\nsd_show_allow_restart(struct device *dev, struct device_attribute *attr,\n\t\t      char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 40, \"%d\\n\", sdkp->device->allow_restart);\n}\n\nstatic ssize_t\nsd_show_protection_type(struct device *dev, struct device_attribute *attr,\n\t\t\tchar *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 20, \"%u\\n\", sdkp->protection_type);\n}\n\nstatic ssize_t\nsd_show_protection_mode(struct device *dev, struct device_attribute *attr,\n\t\t\tchar *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\tunsigned int dif, dix;\n\n\tdif = scsi_host_dif_capable(sdp->host, sdkp->protection_type);\n\tdix = scsi_host_dix_capable(sdp->host, sdkp->protection_type);\n\n\tif (!dix && scsi_host_dix_capable(sdp->host, SD_DIF_TYPE0_PROTECTION)) {\n\t\tdif = 0;\n\t\tdix = 1;\n\t}\n\n\tif (!dif && !dix)\n\t\treturn snprintf(buf, 20, \"none\\n\");\n\n\treturn snprintf(buf, 20, \"%s%u\\n\", dix ? \"dix\" : \"dif\", dif);\n}\n\nstatic ssize_t\nsd_show_app_tag_own(struct device *dev, struct device_attribute *attr,\n\t\t    char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 20, \"%u\\n\", sdkp->ATO);\n}\n\nstatic ssize_t\nsd_show_thin_provisioning(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 20, \"%u\\n\", sdkp->lbpme);\n}\n\nstatic const char *lbp_mode[] = {\n\t[SD_LBP_FULL]\t\t= \"full\",\n\t[SD_LBP_UNMAP]\t\t= \"unmap\",\n\t[SD_LBP_WS16]\t\t= \"writesame_16\",\n\t[SD_LBP_WS10]\t\t= \"writesame_10\",\n\t[SD_LBP_ZERO]\t\t= \"writesame_zero\",\n\t[SD_LBP_DISABLE]\t= \"disabled\",\n};\n\nstatic ssize_t\nsd_show_provisioning_mode(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *buf)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\n\treturn snprintf(buf, 20, \"%s\\n\", lbp_mode[sdkp->provisioning_mode]);\n}\n\nstatic ssize_t\nsd_store_provisioning_mode(struct device *dev, struct device_attribute *attr,\n\t\t\t   const char *buf, size_t count)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct scsi_device *sdp = sdkp->device;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tif (sdp->type != TYPE_DISK)\n\t\treturn -EINVAL;\n\n\tif (!strncmp(buf, lbp_mode[SD_LBP_UNMAP], 20))\n\t\tsd_config_discard(sdkp, SD_LBP_UNMAP);\n\telse if (!strncmp(buf, lbp_mode[SD_LBP_WS16], 20))\n\t\tsd_config_discard(sdkp, SD_LBP_WS16);\n\telse if (!strncmp(buf, lbp_mode[SD_LBP_WS10], 20))\n\t\tsd_config_discard(sdkp, SD_LBP_WS10);\n\telse if (!strncmp(buf, lbp_mode[SD_LBP_ZERO], 20))\n\t\tsd_config_discard(sdkp, SD_LBP_ZERO);\n\telse if (!strncmp(buf, lbp_mode[SD_LBP_DISABLE], 20))\n\t\tsd_config_discard(sdkp, SD_LBP_DISABLE);\n\telse\n\t\treturn -EINVAL;\n\n\treturn count;\n}\n\nstatic struct device_attribute sd_disk_attrs[] = {\n\t__ATTR(cache_type, S_IRUGO|S_IWUSR, sd_show_cache_type,\n\t       sd_store_cache_type),\n\t__ATTR(FUA, S_IRUGO, sd_show_fua, NULL),\n\t__ATTR(allow_restart, S_IRUGO|S_IWUSR, sd_show_allow_restart,\n\t       sd_store_allow_restart),\n\t__ATTR(manage_start_stop, S_IRUGO|S_IWUSR, sd_show_manage_start_stop,\n\t       sd_store_manage_start_stop),\n\t__ATTR(protection_type, S_IRUGO, sd_show_protection_type, NULL),\n\t__ATTR(protection_mode, S_IRUGO, sd_show_protection_mode, NULL),\n\t__ATTR(app_tag_own, S_IRUGO, sd_show_app_tag_own, NULL),\n\t__ATTR(thin_provisioning, S_IRUGO, sd_show_thin_provisioning, NULL),\n\t__ATTR(provisioning_mode, S_IRUGO|S_IWUSR, sd_show_provisioning_mode,\n\t       sd_store_provisioning_mode),\n\t__ATTR_NULL,\n};\n\nstatic struct class sd_disk_class = {\n\t.name\t\t= \"scsi_disk\",\n\t.owner\t\t= THIS_MODULE,\n\t.dev_release\t= scsi_disk_release,\n\t.dev_attrs\t= sd_disk_attrs,\n};\n\nstatic struct scsi_driver sd_template = {\n\t.owner\t\t\t= THIS_MODULE,\n\t.gendrv = {\n\t\t.name\t\t= \"sd\",\n\t\t.probe\t\t= sd_probe,\n\t\t.remove\t\t= sd_remove,\n\t\t.suspend\t= sd_suspend,\n\t\t.resume\t\t= sd_resume,\n\t\t.shutdown\t= sd_shutdown,\n\t},\n\t.rescan\t\t\t= sd_rescan,\n\t.done\t\t\t= sd_done,\n};\n\n/*\n * Device no to disk mapping:\n * \n *       major         disc2     disc  p1\n *   |............|.............|....|....| <- dev_t\n *    31        20 19          8 7  4 3  0\n * \n * Inside a major, we have 16k disks, however mapped non-\n * contiguously. The first 16 disks are for major0, the next\n * ones with major1, ... Disk 256 is for major0 again, disk 272 \n * for major1, ... \n * As we stay compatible with our numbering scheme, we can reuse \n * the well-know SCSI majors 8, 65--71, 136--143.\n */\nstatic int sd_major(int major_idx)\n{\n\tswitch (major_idx) {\n\tcase 0:\n\t\treturn SCSI_DISK0_MAJOR;\n\tcase 1 ... 7:\n\t\treturn SCSI_DISK1_MAJOR + major_idx - 1;\n\tcase 8 ... 15:\n\t\treturn SCSI_DISK8_MAJOR + major_idx - 8;\n\tdefault:\n\t\tBUG();\n\t\treturn 0;\t/* shut up gcc */\n\t}\n}\n\nstatic struct scsi_disk *__scsi_disk_get(struct gendisk *disk)\n{\n\tstruct scsi_disk *sdkp = NULL;\n\n\tif (disk->private_data) {\n\t\tsdkp = scsi_disk(disk);\n\t\tif (scsi_device_get(sdkp->device) == 0)\n\t\t\tget_device(&sdkp->dev);\n\t\telse\n\t\t\tsdkp = NULL;\n\t}\n\treturn sdkp;\n}\n\nstatic struct scsi_disk *scsi_disk_get(struct gendisk *disk)\n{\n\tstruct scsi_disk *sdkp;\n\n\tmutex_lock(&sd_ref_mutex);\n\tsdkp = __scsi_disk_get(disk);\n\tmutex_unlock(&sd_ref_mutex);\n\treturn sdkp;\n}\n\nstatic struct scsi_disk *scsi_disk_get_from_dev(struct device *dev)\n{\n\tstruct scsi_disk *sdkp;\n\n\tmutex_lock(&sd_ref_mutex);\n\tsdkp = dev_get_drvdata(dev);\n\tif (sdkp)\n\t\tsdkp = __scsi_disk_get(sdkp->disk);\n\tmutex_unlock(&sd_ref_mutex);\n\treturn sdkp;\n}\n\nstatic void scsi_disk_put(struct scsi_disk *sdkp)\n{\n\tstruct scsi_device *sdev = sdkp->device;\n\n\tmutex_lock(&sd_ref_mutex);\n\tput_device(&sdkp->dev);\n\tscsi_device_put(sdev);\n\tmutex_unlock(&sd_ref_mutex);\n}\n\nstatic void sd_prot_op(struct scsi_cmnd *scmd, unsigned int dif)\n{\n\tunsigned int prot_op = SCSI_PROT_NORMAL;\n\tunsigned int dix = scsi_prot_sg_count(scmd);\n\n\tif (scmd->sc_data_direction == DMA_FROM_DEVICE) {\n\t\tif (dif && dix)\n\t\t\tprot_op = SCSI_PROT_READ_PASS;\n\t\telse if (dif && !dix)\n\t\t\tprot_op = SCSI_PROT_READ_STRIP;\n\t\telse if (!dif && dix)\n\t\t\tprot_op = SCSI_PROT_READ_INSERT;\n\t} else {\n\t\tif (dif && dix)\n\t\t\tprot_op = SCSI_PROT_WRITE_PASS;\n\t\telse if (dif && !dix)\n\t\t\tprot_op = SCSI_PROT_WRITE_INSERT;\n\t\telse if (!dif && dix)\n\t\t\tprot_op = SCSI_PROT_WRITE_STRIP;\n\t}\n\n\tscsi_set_prot_op(scmd, prot_op);\n\tscsi_set_prot_type(scmd, dif);\n}\n\nstatic void sd_config_discard(struct scsi_disk *sdkp, unsigned int mode)\n{\n\tstruct request_queue *q = sdkp->disk->queue;\n\tunsigned int logical_block_size = sdkp->device->sector_size;\n\tunsigned int max_blocks = 0;\n\n\tq->limits.discard_zeroes_data = sdkp->lbprz;\n\tq->limits.discard_alignment = sdkp->unmap_alignment *\n\t\tlogical_block_size;\n\tq->limits.discard_granularity =\n\t\tmax(sdkp->physical_block_size,\n\t\t    sdkp->unmap_granularity * logical_block_size);\n\n\tswitch (mode) {\n\n\tcase SD_LBP_DISABLE:\n\t\tq->limits.max_discard_sectors = 0;\n\t\tqueue_flag_clear_unlocked(QUEUE_FLAG_DISCARD, q);\n\t\treturn;\n\n\tcase SD_LBP_UNMAP:\n\t\tmax_blocks = min_not_zero(sdkp->max_unmap_blocks, 0xffffffff);\n\t\tbreak;\n\n\tcase SD_LBP_WS16:\n\t\tmax_blocks = min_not_zero(sdkp->max_ws_blocks, 0xffffffff);\n\t\tbreak;\n\n\tcase SD_LBP_WS10:\n\t\tmax_blocks = min_not_zero(sdkp->max_ws_blocks, (u32)0xffff);\n\t\tbreak;\n\n\tcase SD_LBP_ZERO:\n\t\tmax_blocks = min_not_zero(sdkp->max_ws_blocks, (u32)0xffff);\n\t\tq->limits.discard_zeroes_data = 1;\n\t\tbreak;\n\t}\n\n\tq->limits.max_discard_sectors = max_blocks * (logical_block_size >> 9);\n\tqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);\n\n\tsdkp->provisioning_mode = mode;\n}\n\n/**\n * scsi_setup_discard_cmnd - unmap blocks on thinly provisioned device\n * @sdp: scsi device to operate one\n * @rq: Request to prepare\n *\n * Will issue either UNMAP or WRITE SAME(16) depending on preference\n * indicated by target device.\n **/\nstatic int scsi_setup_discard_cmnd(struct scsi_device *sdp, struct request *rq)\n{\n\tstruct scsi_disk *sdkp = scsi_disk(rq->rq_disk);\n\tstruct bio *bio = rq->bio;\n\tsector_t sector = bio->bi_sector;\n\tunsigned int nr_sectors = bio_sectors(bio);\n\tunsigned int len;\n\tint ret;\n\tchar *buf;\n\tstruct page *page;\n\n\tif (sdkp->device->sector_size == 4096) {\n\t\tsector >>= 3;\n\t\tnr_sectors >>= 3;\n\t}\n\n\trq->timeout = SD_TIMEOUT;\n\n\tmemset(rq->cmd, 0, rq->cmd_len);\n\n\tpage = alloc_page(GFP_ATOMIC | __GFP_ZERO);\n\tif (!page)\n\t\treturn BLKPREP_DEFER;\n\n\tswitch (sdkp->provisioning_mode) {\n\tcase SD_LBP_UNMAP:\n\t\tbuf = page_address(page);\n\n\t\trq->cmd_len = 10;\n\t\trq->cmd[0] = UNMAP;\n\t\trq->cmd[8] = 24;\n\n\t\tput_unaligned_be16(6 + 16, &buf[0]);\n\t\tput_unaligned_be16(16, &buf[2]);\n\t\tput_unaligned_be64(sector, &buf[8]);\n\t\tput_unaligned_be32(nr_sectors, &buf[16]);\n\n\t\tlen = 24;\n\t\tbreak;\n\n\tcase SD_LBP_WS16:\n\t\trq->cmd_len = 16;\n\t\trq->cmd[0] = WRITE_SAME_16;\n\t\trq->cmd[1] = 0x8; /* UNMAP */\n\t\tput_unaligned_be64(sector, &rq->cmd[2]);\n\t\tput_unaligned_be32(nr_sectors, &rq->cmd[10]);\n\n\t\tlen = sdkp->device->sector_size;\n\t\tbreak;\n\n\tcase SD_LBP_WS10:\n\tcase SD_LBP_ZERO:\n\t\trq->cmd_len = 10;\n\t\trq->cmd[0] = WRITE_SAME;\n\t\tif (sdkp->provisioning_mode == SD_LBP_WS10)\n\t\t\trq->cmd[1] = 0x8; /* UNMAP */\n\t\tput_unaligned_be32(sector, &rq->cmd[2]);\n\t\tput_unaligned_be16(nr_sectors, &rq->cmd[7]);\n\n\t\tlen = sdkp->device->sector_size;\n\t\tbreak;\n\n\tdefault:\n\t\tret = BLKPREP_KILL;\n\t\tgoto out;\n\t}\n\n\tblk_add_request_payload(rq, page, len);\n\tret = scsi_setup_blk_pc_cmnd(sdp, rq);\n\trq->buffer = page_address(page);\n\nout:\n\tif (ret != BLKPREP_OK) {\n\t\t__free_page(page);\n\t\trq->buffer = NULL;\n\t}\n\treturn ret;\n}\n\nstatic int scsi_setup_flush_cmnd(struct scsi_device *sdp, struct request *rq)\n{\n\trq->timeout = SD_FLUSH_TIMEOUT;\n\trq->retries = SD_MAX_RETRIES;\n\trq->cmd[0] = SYNCHRONIZE_CACHE;\n\trq->cmd_len = 10;\n\n\treturn scsi_setup_blk_pc_cmnd(sdp, rq);\n}\n\nstatic void sd_unprep_fn(struct request_queue *q, struct request *rq)\n{\n\tif (rq->cmd_flags & REQ_DISCARD) {\n\t\tfree_page((unsigned long)rq->buffer);\n\t\trq->buffer = NULL;\n\t}\n}\n\n/**\n *\tsd_init_command - build a scsi (read or write) command from\n *\tinformation in the request structure.\n *\t@SCpnt: pointer to mid-level's per scsi command structure that\n *\tcontains request and into which the scsi command is written\n *\n *\tReturns 1 if successful and 0 if error (or cannot be done now).\n **/\nstatic int sd_prep_fn(struct request_queue *q, struct request *rq)\n{\n\tstruct scsi_cmnd *SCpnt;\n\tstruct scsi_device *sdp = q->queuedata;\n\tstruct gendisk *disk = rq->rq_disk;\n\tstruct scsi_disk *sdkp;\n\tsector_t block = blk_rq_pos(rq);\n\tsector_t threshold;\n\tunsigned int this_count = blk_rq_sectors(rq);\n\tint ret, host_dif;\n\tunsigned char protect;\n\n\t/*\n\t * Discard request come in as REQ_TYPE_FS but we turn them into\n\t * block PC requests to make life easier.\n\t */\n\tif (rq->cmd_flags & REQ_DISCARD) {\n\t\tret = scsi_setup_discard_cmnd(sdp, rq);\n\t\tgoto out;\n\t} else if (rq->cmd_flags & REQ_FLUSH) {\n\t\tret = scsi_setup_flush_cmnd(sdp, rq);\n\t\tgoto out;\n\t} else if (rq->cmd_type == REQ_TYPE_BLOCK_PC) {\n\t\tret = scsi_setup_blk_pc_cmnd(sdp, rq);\n\t\tgoto out;\n\t} else if (rq->cmd_type != REQ_TYPE_FS) {\n\t\tret = BLKPREP_KILL;\n\t\tgoto out;\n\t}\n\tret = scsi_setup_fs_cmnd(sdp, rq);\n\tif (ret != BLKPREP_OK)\n\t\tgoto out;\n\tSCpnt = rq->special;\n\tsdkp = scsi_disk(disk);\n\n\t/* from here on until we're complete, any goto out\n\t * is used for a killable error condition */\n\tret = BLKPREP_KILL;\n\n\tSCSI_LOG_HLQUEUE(1, scmd_printk(KERN_INFO, SCpnt,\n\t\t\t\t\t\"sd_init_command: block=%llu, \"\n\t\t\t\t\t\"count=%d\\n\",\n\t\t\t\t\t(unsigned long long)block,\n\t\t\t\t\tthis_count));\n\n\tif (!sdp || !scsi_device_online(sdp) ||\n\t    block + blk_rq_sectors(rq) > get_capacity(disk)) {\n\t\tSCSI_LOG_HLQUEUE(2, scmd_printk(KERN_INFO, SCpnt,\n\t\t\t\t\t\t\"Finishing %u sectors\\n\",\n\t\t\t\t\t\tblk_rq_sectors(rq)));\n\t\tSCSI_LOG_HLQUEUE(2, scmd_printk(KERN_INFO, SCpnt,\n\t\t\t\t\t\t\"Retry with 0x%p\\n\", SCpnt));\n\t\tgoto out;\n\t}\n\n\tif (sdp->changed) {\n\t\t/*\n\t\t * quietly refuse to do anything to a changed disc until \n\t\t * the changed bit has been reset\n\t\t */\n\t\t/* printk(\"SCSI disk has been changed or is not present. Prohibiting further I/O.\\n\"); */\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Some SD card readers can't handle multi-sector accesses which touch\n\t * the last one or two hardware sectors.  Split accesses as needed.\n\t */\n\tthreshold = get_capacity(disk) - SD_LAST_BUGGY_SECTORS *\n\t\t(sdp->sector_size / 512);\n\n\tif (unlikely(sdp->last_sector_bug && block + this_count > threshold)) {\n\t\tif (block < threshold) {\n\t\t\t/* Access up to the threshold but not beyond */\n\t\t\tthis_count = threshold - block;\n\t\t} else {\n\t\t\t/* Access only a single hardware sector */\n\t\t\tthis_count = sdp->sector_size / 512;\n\t\t}\n\t}\n\n\tSCSI_LOG_HLQUEUE(2, scmd_printk(KERN_INFO, SCpnt, \"block=%llu\\n\",\n\t\t\t\t\t(unsigned long long)block));\n\n\t/*\n\t * If we have a 1K hardware sectorsize, prevent access to single\n\t * 512 byte sectors.  In theory we could handle this - in fact\n\t * the scsi cdrom driver must be able to handle this because\n\t * we typically use 1K blocksizes, and cdroms typically have\n\t * 2K hardware sectorsizes.  Of course, things are simpler\n\t * with the cdrom, since it is read-only.  For performance\n\t * reasons, the filesystems should be able to handle this\n\t * and not force the scsi disk driver to use bounce buffers\n\t * for this.\n\t */\n\tif (sdp->sector_size == 1024) {\n\t\tif ((block & 1) || (blk_rq_sectors(rq) & 1)) {\n\t\t\tscmd_printk(KERN_ERR, SCpnt,\n\t\t\t\t    \"Bad block number requested\\n\");\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tblock = block >> 1;\n\t\t\tthis_count = this_count >> 1;\n\t\t}\n\t}\n\tif (sdp->sector_size == 2048) {\n\t\tif ((block & 3) || (blk_rq_sectors(rq) & 3)) {\n\t\t\tscmd_printk(KERN_ERR, SCpnt,\n\t\t\t\t    \"Bad block number requested\\n\");\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tblock = block >> 2;\n\t\t\tthis_count = this_count >> 2;\n\t\t}\n\t}\n\tif (sdp->sector_size == 4096) {\n\t\tif ((block & 7) || (blk_rq_sectors(rq) & 7)) {\n\t\t\tscmd_printk(KERN_ERR, SCpnt,\n\t\t\t\t    \"Bad block number requested\\n\");\n\t\t\tgoto out;\n\t\t} else {\n\t\t\tblock = block >> 3;\n\t\t\tthis_count = this_count >> 3;\n\t\t}\n\t}\n\tif (rq_data_dir(rq) == WRITE) {\n\t\tif (!sdp->writeable) {\n\t\t\tgoto out;\n\t\t}\n\t\tSCpnt->cmnd[0] = WRITE_6;\n\t\tSCpnt->sc_data_direction = DMA_TO_DEVICE;\n\n\t\tif (blk_integrity_rq(rq) &&\n\t\t    sd_dif_prepare(rq, block, sdp->sector_size) == -EIO)\n\t\t\tgoto out;\n\n\t} else if (rq_data_dir(rq) == READ) {\n\t\tSCpnt->cmnd[0] = READ_6;\n\t\tSCpnt->sc_data_direction = DMA_FROM_DEVICE;\n\t} else {\n\t\tscmd_printk(KERN_ERR, SCpnt, \"Unknown command %x\\n\", rq->cmd_flags);\n\t\tgoto out;\n\t}\n\n\tSCSI_LOG_HLQUEUE(2, scmd_printk(KERN_INFO, SCpnt,\n\t\t\t\t\t\"%s %d/%u 512 byte blocks.\\n\",\n\t\t\t\t\t(rq_data_dir(rq) == WRITE) ?\n\t\t\t\t\t\"writing\" : \"reading\", this_count,\n\t\t\t\t\tblk_rq_sectors(rq)));\n\n\t/* Set RDPROTECT/WRPROTECT if disk is formatted with DIF */\n\thost_dif = scsi_host_dif_capable(sdp->host, sdkp->protection_type);\n\tif (host_dif)\n\t\tprotect = 1 << 5;\n\telse\n\t\tprotect = 0;\n\n\tif (host_dif == SD_DIF_TYPE2_PROTECTION) {\n\t\tSCpnt->cmnd = mempool_alloc(sd_cdb_pool, GFP_ATOMIC);\n\n\t\tif (unlikely(SCpnt->cmnd == NULL)) {\n\t\t\tret = BLKPREP_DEFER;\n\t\t\tgoto out;\n\t\t}\n\n\t\tSCpnt->cmd_len = SD_EXT_CDB_SIZE;\n\t\tmemset(SCpnt->cmnd, 0, SCpnt->cmd_len);\n\t\tSCpnt->cmnd[0] = VARIABLE_LENGTH_CMD;\n\t\tSCpnt->cmnd[7] = 0x18;\n\t\tSCpnt->cmnd[9] = (rq_data_dir(rq) == READ) ? READ_32 : WRITE_32;\n\t\tSCpnt->cmnd[10] = protect | ((rq->cmd_flags & REQ_FUA) ? 0x8 : 0);\n\n\t\t/* LBA */\n\t\tSCpnt->cmnd[12] = sizeof(block) > 4 ? (unsigned char) (block >> 56) & 0xff : 0;\n\t\tSCpnt->cmnd[13] = sizeof(block) > 4 ? (unsigned char) (block >> 48) & 0xff : 0;\n\t\tSCpnt->cmnd[14] = sizeof(block) > 4 ? (unsigned char) (block >> 40) & 0xff : 0;\n\t\tSCpnt->cmnd[15] = sizeof(block) > 4 ? (unsigned char) (block >> 32) & 0xff : 0;\n\t\tSCpnt->cmnd[16] = (unsigned char) (block >> 24) & 0xff;\n\t\tSCpnt->cmnd[17] = (unsigned char) (block >> 16) & 0xff;\n\t\tSCpnt->cmnd[18] = (unsigned char) (block >> 8) & 0xff;\n\t\tSCpnt->cmnd[19] = (unsigned char) block & 0xff;\n\n\t\t/* Expected Indirect LBA */\n\t\tSCpnt->cmnd[20] = (unsigned char) (block >> 24) & 0xff;\n\t\tSCpnt->cmnd[21] = (unsigned char) (block >> 16) & 0xff;\n\t\tSCpnt->cmnd[22] = (unsigned char) (block >> 8) & 0xff;\n\t\tSCpnt->cmnd[23] = (unsigned char) block & 0xff;\n\n\t\t/* Transfer length */\n\t\tSCpnt->cmnd[28] = (unsigned char) (this_count >> 24) & 0xff;\n\t\tSCpnt->cmnd[29] = (unsigned char) (this_count >> 16) & 0xff;\n\t\tSCpnt->cmnd[30] = (unsigned char) (this_count >> 8) & 0xff;\n\t\tSCpnt->cmnd[31] = (unsigned char) this_count & 0xff;\n\t} else if (block > 0xffffffff) {\n\t\tSCpnt->cmnd[0] += READ_16 - READ_6;\n\t\tSCpnt->cmnd[1] = protect | ((rq->cmd_flags & REQ_FUA) ? 0x8 : 0);\n\t\tSCpnt->cmnd[2] = sizeof(block) > 4 ? (unsigned char) (block >> 56) & 0xff : 0;\n\t\tSCpnt->cmnd[3] = sizeof(block) > 4 ? (unsigned char) (block >> 48) & 0xff : 0;\n\t\tSCpnt->cmnd[4] = sizeof(block) > 4 ? (unsigned char) (block >> 40) & 0xff : 0;\n\t\tSCpnt->cmnd[5] = sizeof(block) > 4 ? (unsigned char) (block >> 32) & 0xff : 0;\n\t\tSCpnt->cmnd[6] = (unsigned char) (block >> 24) & 0xff;\n\t\tSCpnt->cmnd[7] = (unsigned char) (block >> 16) & 0xff;\n\t\tSCpnt->cmnd[8] = (unsigned char) (block >> 8) & 0xff;\n\t\tSCpnt->cmnd[9] = (unsigned char) block & 0xff;\n\t\tSCpnt->cmnd[10] = (unsigned char) (this_count >> 24) & 0xff;\n\t\tSCpnt->cmnd[11] = (unsigned char) (this_count >> 16) & 0xff;\n\t\tSCpnt->cmnd[12] = (unsigned char) (this_count >> 8) & 0xff;\n\t\tSCpnt->cmnd[13] = (unsigned char) this_count & 0xff;\n\t\tSCpnt->cmnd[14] = SCpnt->cmnd[15] = 0;\n\t} else if ((this_count > 0xff) || (block > 0x1fffff) ||\n\t\t   scsi_device_protection(SCpnt->device) ||\n\t\t   SCpnt->device->use_10_for_rw) {\n\t\tif (this_count > 0xffff)\n\t\t\tthis_count = 0xffff;\n\n\t\tSCpnt->cmnd[0] += READ_10 - READ_6;\n\t\tSCpnt->cmnd[1] = protect | ((rq->cmd_flags & REQ_FUA) ? 0x8 : 0);\n\t\tSCpnt->cmnd[2] = (unsigned char) (block >> 24) & 0xff;\n\t\tSCpnt->cmnd[3] = (unsigned char) (block >> 16) & 0xff;\n\t\tSCpnt->cmnd[4] = (unsigned char) (block >> 8) & 0xff;\n\t\tSCpnt->cmnd[5] = (unsigned char) block & 0xff;\n\t\tSCpnt->cmnd[6] = SCpnt->cmnd[9] = 0;\n\t\tSCpnt->cmnd[7] = (unsigned char) (this_count >> 8) & 0xff;\n\t\tSCpnt->cmnd[8] = (unsigned char) this_count & 0xff;\n\t} else {\n\t\tif (unlikely(rq->cmd_flags & REQ_FUA)) {\n\t\t\t/*\n\t\t\t * This happens only if this drive failed\n\t\t\t * 10byte rw command with ILLEGAL_REQUEST\n\t\t\t * during operation and thus turned off\n\t\t\t * use_10_for_rw.\n\t\t\t */\n\t\t\tscmd_printk(KERN_ERR, SCpnt,\n\t\t\t\t    \"FUA write on READ/WRITE(6) drive\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\tSCpnt->cmnd[1] |= (unsigned char) ((block >> 16) & 0x1f);\n\t\tSCpnt->cmnd[2] = (unsigned char) ((block >> 8) & 0xff);\n\t\tSCpnt->cmnd[3] = (unsigned char) block & 0xff;\n\t\tSCpnt->cmnd[4] = (unsigned char) this_count;\n\t\tSCpnt->cmnd[5] = 0;\n\t}\n\tSCpnt->sdb.length = this_count * sdp->sector_size;\n\n\t/* If DIF or DIX is enabled, tell HBA how to handle request */\n\tif (host_dif || scsi_prot_sg_count(SCpnt))\n\t\tsd_prot_op(SCpnt, host_dif);\n\n\t/*\n\t * We shouldn't disconnect in the middle of a sector, so with a dumb\n\t * host adapter, it's safe to assume that we can at least transfer\n\t * this many bytes between each connect / disconnect.\n\t */\n\tSCpnt->transfersize = sdp->sector_size;\n\tSCpnt->underflow = this_count << 9;\n\tSCpnt->allowed = SD_MAX_RETRIES;\n\n\t/*\n\t * This indicates that the command is ready from our end to be\n\t * queued.\n\t */\n\tret = BLKPREP_OK;\n out:\n\treturn scsi_prep_return(q, rq, ret);\n}\n\n/**\n *\tsd_open - open a scsi disk device\n *\t@inode: only i_rdev member may be used\n *\t@filp: only f_mode and f_flags may be used\n *\n *\tReturns 0 if successful. Returns a negated errno value in case \n *\tof error.\n *\n *\tNote: This can be called from a user context (e.g. fsck(1) )\n *\tor from within the kernel (e.g. as a result of a mount(1) ).\n *\tIn the latter case @inode and @filp carry an abridged amount\n *\tof information as noted above.\n *\n *\tLocking: called with bdev->bd_mutex held.\n **/\nstatic int sd_open(struct block_device *bdev, fmode_t mode)\n{\n\tstruct scsi_disk *sdkp = scsi_disk_get(bdev->bd_disk);\n\tstruct scsi_device *sdev;\n\tint retval;\n\n\tif (!sdkp)\n\t\treturn -ENXIO;\n\n\tSCSI_LOG_HLQUEUE(3, sd_printk(KERN_INFO, sdkp, \"sd_open\\n\"));\n\n\tsdev = sdkp->device;\n\n\tretval = scsi_autopm_get_device(sdev);\n\tif (retval)\n\t\tgoto error_autopm;\n\n\t/*\n\t * If the device is in error recovery, wait until it is done.\n\t * If the device is offline, then disallow any access to it.\n\t */\n\tretval = -ENXIO;\n\tif (!scsi_block_when_processing_errors(sdev))\n\t\tgoto error_out;\n\n\tif (sdev->removable || sdkp->write_prot)\n\t\tcheck_disk_change(bdev);\n\n\t/*\n\t * If the drive is empty, just let the open fail.\n\t */\n\tretval = -ENOMEDIUM;\n\tif (sdev->removable && !sdkp->media_present && !(mode & FMODE_NDELAY))\n\t\tgoto error_out;\n\n\t/*\n\t * If the device has the write protect tab set, have the open fail\n\t * if the user expects to be able to write to the thing.\n\t */\n\tretval = -EROFS;\n\tif (sdkp->write_prot && (mode & FMODE_WRITE))\n\t\tgoto error_out;\n\n\t/*\n\t * It is possible that the disk changing stuff resulted in\n\t * the device being taken offline.  If this is the case,\n\t * report this to the user, and don't pretend that the\n\t * open actually succeeded.\n\t */\n\tretval = -ENXIO;\n\tif (!scsi_device_online(sdev))\n\t\tgoto error_out;\n\n\tif ((atomic_inc_return(&sdkp->openers) == 1) && sdev->removable) {\n\t\tif (scsi_block_when_processing_errors(sdev))\n\t\t\tscsi_set_medium_removal(sdev, SCSI_REMOVAL_PREVENT);\n\t}\n\n\treturn 0;\n\nerror_out:\n\tscsi_autopm_put_device(sdev);\nerror_autopm:\n\tscsi_disk_put(sdkp);\n\treturn retval;\t\n}\n\n/**\n *\tsd_release - invoked when the (last) close(2) is called on this\n *\tscsi disk.\n *\t@inode: only i_rdev member may be used\n *\t@filp: only f_mode and f_flags may be used\n *\n *\tReturns 0. \n *\n *\tNote: may block (uninterruptible) if error recovery is underway\n *\ton this disk.\n *\n *\tLocking: called with bdev->bd_mutex held.\n **/\nstatic int sd_release(struct gendisk *disk, fmode_t mode)\n{\n\tstruct scsi_disk *sdkp = scsi_disk(disk);\n\tstruct scsi_device *sdev = sdkp->device;\n\n\tSCSI_LOG_HLQUEUE(3, sd_printk(KERN_INFO, sdkp, \"sd_release\\n\"));\n\n\tif (atomic_dec_return(&sdkp->openers) == 0 && sdev->removable) {\n\t\tif (scsi_block_when_processing_errors(sdev))\n\t\t\tscsi_set_medium_removal(sdev, SCSI_REMOVAL_ALLOW);\n\t}\n\n\t/*\n\t * XXX and what if there are packets in flight and this close()\n\t * XXX is followed by a \"rmmod sd_mod\"?\n\t */\n\n\tscsi_autopm_put_device(sdev);\n\tscsi_disk_put(sdkp);\n\treturn 0;\n}\n\nstatic int sd_getgeo(struct block_device *bdev, struct hd_geometry *geo)\n{\n\tstruct scsi_disk *sdkp = scsi_disk(bdev->bd_disk);\n\tstruct scsi_device *sdp = sdkp->device;\n\tstruct Scsi_Host *host = sdp->host;\n\tint diskinfo[4];\n\n\t/* default to most commonly used values */\n        diskinfo[0] = 0x40;\t/* 1 << 6 */\n       \tdiskinfo[1] = 0x20;\t/* 1 << 5 */\n       \tdiskinfo[2] = sdkp->capacity >> 11;\n\t\n\t/* override with calculated, extended default, or driver values */\n\tif (host->hostt->bios_param)\n\t\thost->hostt->bios_param(sdp, bdev, sdkp->capacity, diskinfo);\n\telse\n\t\tscsicam_bios_param(bdev, sdkp->capacity, diskinfo);\n\n\tgeo->heads = diskinfo[0];\n\tgeo->sectors = diskinfo[1];\n\tgeo->cylinders = diskinfo[2];\n\treturn 0;\n}\n\n/**\n *\tsd_ioctl - process an ioctl\n *\t@inode: only i_rdev/i_bdev members may be used\n *\t@filp: only f_mode and f_flags may be used\n *\t@cmd: ioctl command number\n *\t@arg: this is third argument given to ioctl(2) system call.\n *\tOften contains a pointer.\n *\n *\tReturns 0 if successful (some ioctls return positive numbers on\n *\tsuccess as well). Returns a negated errno value in case of error.\n *\n *\tNote: most ioctls are forward onto the block subsystem or further\n *\tdown in the scsi subsystem.\n **/\nstatic int sd_ioctl(struct block_device *bdev, fmode_t mode,\n\t\t    unsigned int cmd, unsigned long arg)\n{\n\tstruct gendisk *disk = bdev->bd_disk;\n\tstruct scsi_disk *sdkp = scsi_disk(disk);\n\tstruct scsi_device *sdp = sdkp->device;\n\tvoid __user *p = (void __user *)arg;\n\tint error;\n    \n\tSCSI_LOG_IOCTL(1, sd_printk(KERN_INFO, sdkp, \"sd_ioctl: disk=%s, \"\n\t\t\t\t    \"cmd=0x%x\\n\", disk->disk_name, cmd));\n\n\terror = scsi_verify_blk_ioctl(bdev, cmd);\n\tif (error < 0)\n\t\treturn error;\n\n\t/*\n\t * If we are in the middle of error recovery, don't let anyone\n\t * else try and use this device.  Also, if error recovery fails, it\n\t * may try and take the device offline, in which case all further\n\t * access to the device is prohibited.\n\t */\n\terror = scsi_nonblockable_ioctl(sdp, cmd, p,\n\t\t\t\t\t(mode & FMODE_NDELAY) != 0);\n\tif (!scsi_block_when_processing_errors(sdp) || !error)\n\t\tgoto out;\n\n\t/*\n\t * Send SCSI addressing ioctls directly to mid level, send other\n\t * ioctls to block level and then onto mid level if they can't be\n\t * resolved.\n\t */\n\tswitch (cmd) {\n\t\tcase SCSI_IOCTL_GET_IDLUN:\n\t\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\t\t\terror = scsi_ioctl(sdp, cmd, p);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terror = scsi_cmd_blk_ioctl(bdev, mode, cmd, p);\n\t\t\tif (error != -ENOTTY)\n\t\t\t\tbreak;\n\t\t\terror = scsi_ioctl(sdp, cmd, p);\n\t\t\tbreak;\n\t}\nout:\n\treturn error;\n}\n\nstatic void set_media_not_present(struct scsi_disk *sdkp)\n{\n\tif (sdkp->media_present)\n\t\tsdkp->device->changed = 1;\n\n\tif (sdkp->device->removable) {\n\t\tsdkp->media_present = 0;\n\t\tsdkp->capacity = 0;\n\t}\n}\n\nstatic int media_not_present(struct scsi_disk *sdkp,\n\t\t\t     struct scsi_sense_hdr *sshdr)\n{\n\tif (!scsi_sense_valid(sshdr))\n\t\treturn 0;\n\n\t/* not invoked for commands that could return deferred errors */\n\tswitch (sshdr->sense_key) {\n\tcase UNIT_ATTENTION:\n\tcase NOT_READY:\n\t\t/* medium not present */\n\t\tif (sshdr->asc == 0x3A) {\n\t\t\tset_media_not_present(sdkp);\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/**\n *\tsd_check_events - check media events\n *\t@disk: kernel device descriptor\n *\t@clearing: disk events currently being cleared\n *\n *\tReturns mask of DISK_EVENT_*.\n *\n *\tNote: this function is invoked from the block subsystem.\n **/\nstatic unsigned int sd_check_events(struct gendisk *disk, unsigned int clearing)\n{\n\tstruct scsi_disk *sdkp = scsi_disk(disk);\n\tstruct scsi_device *sdp = sdkp->device;\n\tstruct scsi_sense_hdr *sshdr = NULL;\n\tint retval;\n\n\tSCSI_LOG_HLQUEUE(3, sd_printk(KERN_INFO, sdkp, \"sd_check_events\\n\"));\n\n\t/*\n\t * If the device is offline, don't send any commands - just pretend as\n\t * if the command failed.  If the device ever comes back online, we\n\t * can deal with it then.  It is only because of unrecoverable errors\n\t * that we would ever take a device offline in the first place.\n\t */\n\tif (!scsi_device_online(sdp)) {\n\t\tset_media_not_present(sdkp);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Using TEST_UNIT_READY enables differentiation between drive with\n\t * no cartridge loaded - NOT READY, drive with changed cartridge -\n\t * UNIT ATTENTION, or with same cartridge - GOOD STATUS.\n\t *\n\t * Drives that auto spin down. eg iomega jaz 1G, will be started\n\t * by sd_spinup_disk() from sd_revalidate_disk(), which happens whenever\n\t * sd_revalidate() is called.\n\t */\n\tretval = -ENODEV;\n\n\tif (scsi_block_when_processing_errors(sdp)) {\n\t\tsshdr  = kzalloc(sizeof(*sshdr), GFP_KERNEL);\n\t\tretval = scsi_test_unit_ready(sdp, SD_TIMEOUT, SD_MAX_RETRIES,\n\t\t\t\t\t      sshdr);\n\t}\n\n\t/* failed to execute TUR, assume media not present */\n\tif (host_byte(retval)) {\n\t\tset_media_not_present(sdkp);\n\t\tgoto out;\n\t}\n\n\tif (media_not_present(sdkp, sshdr))\n\t\tgoto out;\n\n\t/*\n\t * For removable scsi disk we have to recognise the presence\n\t * of a disk in the drive.\n\t */\n\tif (!sdkp->media_present)\n\t\tsdp->changed = 1;\n\tsdkp->media_present = 1;\nout:\n\t/*\n\t * sdp->changed is set under the following conditions:\n\t *\n\t *\tMedium present state has changed in either direction.\n\t *\tDevice has indicated UNIT_ATTENTION.\n\t */\n\tkfree(sshdr);\n\tretval = sdp->changed ? DISK_EVENT_MEDIA_CHANGE : 0;\n\tsdp->changed = 0;\n\treturn retval;\n}\n\nstatic int sd_sync_cache(struct scsi_disk *sdkp)\n{\n\tint retries, res;\n\tstruct scsi_device *sdp = sdkp->device;\n\tstruct scsi_sense_hdr sshdr;\n\n\tif (!scsi_device_online(sdp))\n\t\treturn -ENODEV;\n\n\n\tfor (retries = 3; retries > 0; --retries) {\n\t\tunsigned char cmd[10] = { 0 };\n\n\t\tcmd[0] = SYNCHRONIZE_CACHE;\n\t\t/*\n\t\t * Leave the rest of the command zero to indicate\n\t\t * flush everything.\n\t\t */\n\t\tres = scsi_execute_req(sdp, cmd, DMA_NONE, NULL, 0, &sshdr,\n\t\t\t\t       SD_FLUSH_TIMEOUT, SD_MAX_RETRIES, NULL);\n\t\tif (res == 0)\n\t\t\tbreak;\n\t}\n\n\tif (res) {\n\t\tsd_print_result(sdkp, res);\n\t\tif (driver_byte(res) & DRIVER_SENSE)\n\t\t\tsd_print_sense_hdr(sdkp, &sshdr);\n\t}\n\n\tif (res)\n\t\treturn -EIO;\n\treturn 0;\n}\n\nstatic void sd_rescan(struct device *dev)\n{\n\tstruct scsi_disk *sdkp = scsi_disk_get_from_dev(dev);\n\n\tif (sdkp) {\n\t\trevalidate_disk(sdkp->disk);\n\t\tscsi_disk_put(sdkp);\n\t}\n}\n\n\n#ifdef CONFIG_COMPAT\n/* \n * This gets directly called from VFS. When the ioctl \n * is not recognized we go back to the other translation paths. \n */\nstatic int sd_compat_ioctl(struct block_device *bdev, fmode_t mode,\n\t\t\t   unsigned int cmd, unsigned long arg)\n{\n\tstruct scsi_device *sdev = scsi_disk(bdev->bd_disk)->device;\n\tint ret;\n\n\tret = scsi_verify_blk_ioctl(bdev, cmd);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/*\n\t * If we are in the middle of error recovery, don't let anyone\n\t * else try and use this device.  Also, if error recovery fails, it\n\t * may try and take the device offline, in which case all further\n\t * access to the device is prohibited.\n\t */\n\tif (!scsi_block_when_processing_errors(sdev))\n\t\treturn -ENODEV;\n\t       \n\tif (sdev->host->hostt->compat_ioctl) {\n\t\tret = sdev->host->hostt->compat_ioctl(sdev, cmd, (void __user *)arg);\n\n\t\treturn ret;\n\t}\n\n\t/* \n\t * Let the static ioctl translation table take care of it.\n\t */\n\treturn -ENOIOCTLCMD; \n}\n#endif\n\nstatic const struct block_device_operations sd_fops = {\n\t.owner\t\t\t= THIS_MODULE,\n\t.open\t\t\t= sd_open,\n\t.release\t\t= sd_release,\n\t.ioctl\t\t\t= sd_ioctl,\n\t.getgeo\t\t\t= sd_getgeo,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t\t= sd_compat_ioctl,\n#endif\n\t.check_events\t\t= sd_check_events,\n\t.revalidate_disk\t= sd_revalidate_disk,\n\t.unlock_native_capacity\t= sd_unlock_native_capacity,\n};\n\nstatic unsigned int sd_completed_bytes(struct scsi_cmnd *scmd)\n{\n\tu64 start_lba = blk_rq_pos(scmd->request);\n\tu64 end_lba = blk_rq_pos(scmd->request) + (scsi_bufflen(scmd) / 512);\n\tu64 bad_lba;\n\tint info_valid;\n\t/*\n\t * resid is optional but mostly filled in.  When it's unused,\n\t * its value is zero, so we assume the whole buffer transferred\n\t */\n\tunsigned int transferred = scsi_bufflen(scmd) - scsi_get_resid(scmd);\n\tunsigned int good_bytes;\n\n\tif (scmd->request->cmd_type != REQ_TYPE_FS)\n\t\treturn 0;\n\n\tinfo_valid = scsi_get_sense_info_fld(scmd->sense_buffer,\n\t\t\t\t\t     SCSI_SENSE_BUFFERSIZE,\n\t\t\t\t\t     &bad_lba);\n\tif (!info_valid)\n\t\treturn 0;\n\n\tif (scsi_bufflen(scmd) <= scmd->device->sector_size)\n\t\treturn 0;\n\n\tif (scmd->device->sector_size < 512) {\n\t\t/* only legitimate sector_size here is 256 */\n\t\tstart_lba <<= 1;\n\t\tend_lba <<= 1;\n\t} else {\n\t\t/* be careful ... don't want any overflows */\n\t\tu64 factor = scmd->device->sector_size / 512;\n\t\tdo_div(start_lba, factor);\n\t\tdo_div(end_lba, factor);\n\t}\n\n\t/* The bad lba was reported incorrectly, we have no idea where\n\t * the error is.\n\t */\n\tif (bad_lba < start_lba  || bad_lba >= end_lba)\n\t\treturn 0;\n\n\t/* This computation should always be done in terms of\n\t * the resolution of the device's medium.\n\t */\n\tgood_bytes = (bad_lba - start_lba) * scmd->device->sector_size;\n\treturn min(good_bytes, transferred);\n}\n\n/**\n *\tsd_done - bottom half handler: called when the lower level\n *\tdriver has completed (successfully or otherwise) a scsi command.\n *\t@SCpnt: mid-level's per command structure.\n *\n *\tNote: potentially run from within an ISR. Must not block.\n **/\nstatic int sd_done(struct scsi_cmnd *SCpnt)\n{\n\tint result = SCpnt->result;\n\tunsigned int good_bytes = result ? 0 : scsi_bufflen(SCpnt);\n\tstruct scsi_sense_hdr sshdr;\n\tstruct scsi_disk *sdkp = scsi_disk(SCpnt->request->rq_disk);\n\tint sense_valid = 0;\n\tint sense_deferred = 0;\n\tunsigned char op = SCpnt->cmnd[0];\n\n\tif ((SCpnt->request->cmd_flags & REQ_DISCARD) && !result)\n\t\tscsi_set_resid(SCpnt, 0);\n\n\tif (result) {\n\t\tsense_valid = scsi_command_normalize_sense(SCpnt, &sshdr);\n\t\tif (sense_valid)\n\t\t\tsense_deferred = scsi_sense_is_deferred(&sshdr);\n\t}\n#ifdef CONFIG_SCSI_LOGGING\n\tSCSI_LOG_HLCOMPLETE(1, scsi_print_result(SCpnt));\n\tif (sense_valid) {\n\t\tSCSI_LOG_HLCOMPLETE(1, scmd_printk(KERN_INFO, SCpnt,\n\t\t\t\t\t\t   \"sd_done: sb[respc,sk,asc,\"\n\t\t\t\t\t\t   \"ascq]=%x,%x,%x,%x\\n\",\n\t\t\t\t\t\t   sshdr.response_code,\n\t\t\t\t\t\t   sshdr.sense_key, sshdr.asc,\n\t\t\t\t\t\t   sshdr.ascq));\n\t}\n#endif\n\tif (driver_byte(result) != DRIVER_SENSE &&\n\t    (!sense_valid || sense_deferred))\n\t\tgoto out;\n\n\tswitch (sshdr.sense_key) {\n\tcase HARDWARE_ERROR:\n\tcase MEDIUM_ERROR:\n\t\tgood_bytes = sd_completed_bytes(SCpnt);\n\t\tbreak;\n\tcase RECOVERED_ERROR:\n\t\tgood_bytes = scsi_bufflen(SCpnt);\n\t\tbreak;\n\tcase NO_SENSE:\n\t\t/* This indicates a false check condition, so ignore it.  An\n\t\t * unknown amount of data was transferred so treat it as an\n\t\t * error.\n\t\t */\n\t\tscsi_print_sense(\"sd\", SCpnt);\n\t\tSCpnt->result = 0;\n\t\tmemset(SCpnt->sense_buffer, 0, SCSI_SENSE_BUFFERSIZE);\n\t\tbreak;\n\tcase ABORTED_COMMAND:\n\t\tif (sshdr.asc == 0x10)  /* DIF: Target detected corruption */\n\t\t\tgood_bytes = sd_completed_bytes(SCpnt);\n\t\tbreak;\n\tcase ILLEGAL_REQUEST:\n\t\tif (sshdr.asc == 0x10)  /* DIX: Host detected corruption */\n\t\t\tgood_bytes = sd_completed_bytes(SCpnt);\n\t\t/* INVALID COMMAND OPCODE or INVALID FIELD IN CDB */\n\t\tif ((sshdr.asc == 0x20 || sshdr.asc == 0x24) &&\n\t\t    (op == UNMAP || op == WRITE_SAME_16 || op == WRITE_SAME))\n\t\t\tsd_config_discard(sdkp, SD_LBP_DISABLE);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n out:\n\tif (rq_data_dir(SCpnt->request) == READ && scsi_prot_sg_count(SCpnt))\n\t\tsd_dif_complete(SCpnt, good_bytes);\n\n\tif (scsi_host_dif_capable(sdkp->device->host, sdkp->protection_type)\n\t    == SD_DIF_TYPE2_PROTECTION && SCpnt->cmnd != SCpnt->request->cmd) {\n\n\t\t/* We have to print a failed command here as the\n\t\t * extended CDB gets freed before scsi_io_completion()\n\t\t * is called.\n\t\t */\n\t\tif (result)\n\t\t\tscsi_print_command(SCpnt);\n\n\t\tmempool_free(SCpnt->cmnd, sd_cdb_pool);\n\t\tSCpnt->cmnd = NULL;\n\t\tSCpnt->cmd_len = 0;\n\t}\n\n\treturn good_bytes;\n}\n\n/*\n * spinup disk - called only in sd_revalidate_disk()\n */\nstatic void\nsd_spinup_disk(struct scsi_disk *sdkp)\n{\n\tunsigned char cmd[10];\n\tunsigned long spintime_expire = 0;\n\tint retries, spintime;\n\tunsigned int the_result;\n\tstruct scsi_sense_hdr sshdr;\n\tint sense_valid = 0;\n\n\tspintime = 0;\n\n\t/* Spin up drives, as required.  Only do this at boot time */\n\t/* Spinup needs to be done for module loads too. */\n\tdo {\n\t\tretries = 0;\n\n\t\tdo {\n\t\t\tcmd[0] = TEST_UNIT_READY;\n\t\t\tmemset((void *) &cmd[1], 0, 9);\n\n\t\t\tthe_result = scsi_execute_req(sdkp->device, cmd,\n\t\t\t\t\t\t      DMA_NONE, NULL, 0,\n\t\t\t\t\t\t      &sshdr, SD_TIMEOUT,\n\t\t\t\t\t\t      SD_MAX_RETRIES, NULL);\n\n\t\t\t/*\n\t\t\t * If the drive has indicated to us that it\n\t\t\t * doesn't have any media in it, don't bother\n\t\t\t * with any more polling.\n\t\t\t */\n\t\t\tif (media_not_present(sdkp, &sshdr))\n\t\t\t\treturn;\n\n\t\t\tif (the_result)\n\t\t\t\tsense_valid = scsi_sense_valid(&sshdr);\n\t\t\tretries++;\n\t\t} while (retries < 3 && \n\t\t\t (!scsi_status_is_good(the_result) ||\n\t\t\t  ((driver_byte(the_result) & DRIVER_SENSE) &&\n\t\t\t  sense_valid && sshdr.sense_key == UNIT_ATTENTION)));\n\n\t\tif ((driver_byte(the_result) & DRIVER_SENSE) == 0) {\n\t\t\t/* no sense, TUR either succeeded or failed\n\t\t\t * with a status error */\n\t\t\tif(!spintime && !scsi_status_is_good(the_result)) {\n\t\t\t\tsd_printk(KERN_NOTICE, sdkp, \"Unit Not Ready\\n\");\n\t\t\t\tsd_print_result(sdkp, the_result);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\t\t\t\t\n\t\t/*\n\t\t * The device does not want the automatic start to be issued.\n\t\t */\n\t\tif (sdkp->device->no_start_on_add)\n\t\t\tbreak;\n\n\t\tif (sense_valid && sshdr.sense_key == NOT_READY) {\n\t\t\tif (sshdr.asc == 4 && sshdr.ascq == 3)\n\t\t\t\tbreak;\t/* manual intervention required */\n\t\t\tif (sshdr.asc == 4 && sshdr.ascq == 0xb)\n\t\t\t\tbreak;\t/* standby */\n\t\t\tif (sshdr.asc == 4 && sshdr.ascq == 0xc)\n\t\t\t\tbreak;\t/* unavailable */\n\t\t\t/*\n\t\t\t * Issue command to spin up drive when not ready\n\t\t\t */\n\t\t\tif (!spintime) {\n\t\t\t\tsd_printk(KERN_NOTICE, sdkp, \"Spinning up disk...\");\n\t\t\t\tcmd[0] = START_STOP;\n\t\t\t\tcmd[1] = 1;\t/* Return immediately */\n\t\t\t\tmemset((void *) &cmd[2], 0, 8);\n\t\t\t\tcmd[4] = 1;\t/* Start spin cycle */\n\t\t\t\tif (sdkp->device->start_stop_pwr_cond)\n\t\t\t\t\tcmd[4] |= 1 << 4;\n\t\t\t\tscsi_execute_req(sdkp->device, cmd, DMA_NONE,\n\t\t\t\t\t\t NULL, 0, &sshdr,\n\t\t\t\t\t\t SD_TIMEOUT, SD_MAX_RETRIES,\n\t\t\t\t\t\t NULL);\n\t\t\t\tspintime_expire = jiffies + 100 * HZ;\n\t\t\t\tspintime = 1;\n\t\t\t}\n\t\t\t/* Wait 1 second for next try */\n\t\t\tmsleep(1000);\n\t\t\tprintk(\".\");\n\n\t\t/*\n\t\t * Wait for USB flash devices with slow firmware.\n\t\t * Yes, this sense key/ASC combination shouldn't\n\t\t * occur here.  It's characteristic of these devices.\n\t\t */\n\t\t} else if (sense_valid &&\n\t\t\t\tsshdr.sense_key == UNIT_ATTENTION &&\n\t\t\t\tsshdr.asc == 0x28) {\n\t\t\tif (!spintime) {\n\t\t\t\tspintime_expire = jiffies + 5 * HZ;\n\t\t\t\tspintime = 1;\n\t\t\t}\n\t\t\t/* Wait 1 second for next try */\n\t\t\tmsleep(1000);\n\t\t} else {\n\t\t\t/* we don't understand the sense code, so it's\n\t\t\t * probably pointless to loop */\n\t\t\tif(!spintime) {\n\t\t\t\tsd_printk(KERN_NOTICE, sdkp, \"Unit Not Ready\\n\");\n\t\t\t\tsd_print_sense_hdr(sdkp, &sshdr);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\t\t\t\n\t} while (spintime && time_before_eq(jiffies, spintime_expire));\n\n\tif (spintime) {\n\t\tif (scsi_status_is_good(the_result))\n\t\t\tprintk(\"ready\\n\");\n\t\telse\n\t\t\tprintk(\"not responding...\\n\");\n\t}\n}\n\n\n/*\n * Determine whether disk supports Data Integrity Field.\n */\nstatic void sd_read_protection_type(struct scsi_disk *sdkp, unsigned char *buffer)\n{\n\tstruct scsi_device *sdp = sdkp->device;\n\tu8 type;\n\n\tif (scsi_device_protection(sdp) == 0 || (buffer[12] & 1) == 0)\n\t\treturn;\n\n\ttype = ((buffer[12] >> 1) & 7) + 1; /* P_TYPE 0 = Type 1 */\n\n\tif (type == sdkp->protection_type || !sdkp->first_scan)\n\t\treturn;\n\n\tsdkp->protection_type = type;\n\n\tif (type > SD_DIF_TYPE3_PROTECTION) {\n\t\tsd_printk(KERN_ERR, sdkp, \"formatted with unsupported \"\t\\\n\t\t\t  \"protection type %u. Disabling disk!\\n\", type);\n\t\tsdkp->capacity = 0;\n\t\treturn;\n\t}\n\n\tif (scsi_host_dif_capable(sdp->host, type))\n\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t  \"Enabling DIF Type %u protection\\n\", type);\n\telse\n\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t  \"Disabling DIF Type %u protection\\n\", type);\n}\n\nstatic void read_capacity_error(struct scsi_disk *sdkp, struct scsi_device *sdp,\n\t\t\tstruct scsi_sense_hdr *sshdr, int sense_valid,\n\t\t\tint the_result)\n{\n\tsd_print_result(sdkp, the_result);\n\tif (driver_byte(the_result) & DRIVER_SENSE)\n\t\tsd_print_sense_hdr(sdkp, sshdr);\n\telse\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Sense not available.\\n\");\n\n\t/*\n\t * Set dirty bit for removable devices if not ready -\n\t * sometimes drives will not report this properly.\n\t */\n\tif (sdp->removable &&\n\t    sense_valid && sshdr->sense_key == NOT_READY)\n\t\tset_media_not_present(sdkp);\n\n\t/*\n\t * We used to set media_present to 0 here to indicate no media\n\t * in the drive, but some drives fail read capacity even with\n\t * media present, so we can't do that.\n\t */\n\tsdkp->capacity = 0; /* unknown mapped to zero - as usual */\n}\n\n#define RC16_LEN 32\n#if RC16_LEN > SD_BUF_SIZE\n#error RC16_LEN must not be more than SD_BUF_SIZE\n#endif\n\n#define READ_CAPACITY_RETRIES_ON_RESET\t10\n\nstatic int read_capacity_16(struct scsi_disk *sdkp, struct scsi_device *sdp,\n\t\t\t\t\t\tunsigned char *buffer)\n{\n\tunsigned char cmd[16];\n\tstruct scsi_sense_hdr sshdr;\n\tint sense_valid = 0;\n\tint the_result;\n\tint retries = 3, reset_retries = READ_CAPACITY_RETRIES_ON_RESET;\n\tunsigned int alignment;\n\tunsigned long long lba;\n\tunsigned sector_size;\n\n\tif (sdp->no_read_capacity_16)\n\t\treturn -EINVAL;\n\n\tdo {\n\t\tmemset(cmd, 0, 16);\n\t\tcmd[0] = SERVICE_ACTION_IN;\n\t\tcmd[1] = SAI_READ_CAPACITY_16;\n\t\tcmd[13] = RC16_LEN;\n\t\tmemset(buffer, 0, RC16_LEN);\n\n\t\tthe_result = scsi_execute_req(sdp, cmd, DMA_FROM_DEVICE,\n\t\t\t\t\tbuffer, RC16_LEN, &sshdr,\n\t\t\t\t\tSD_TIMEOUT, SD_MAX_RETRIES, NULL);\n\n\t\tif (media_not_present(sdkp, &sshdr))\n\t\t\treturn -ENODEV;\n\n\t\tif (the_result) {\n\t\t\tsense_valid = scsi_sense_valid(&sshdr);\n\t\t\tif (sense_valid &&\n\t\t\t    sshdr.sense_key == ILLEGAL_REQUEST &&\n\t\t\t    (sshdr.asc == 0x20 || sshdr.asc == 0x24) &&\n\t\t\t    sshdr.ascq == 0x00)\n\t\t\t\t/* Invalid Command Operation Code or\n\t\t\t\t * Invalid Field in CDB, just retry\n\t\t\t\t * silently with RC10 */\n\t\t\t\treturn -EINVAL;\n\t\t\tif (sense_valid &&\n\t\t\t    sshdr.sense_key == UNIT_ATTENTION &&\n\t\t\t    sshdr.asc == 0x29 && sshdr.ascq == 0x00)\n\t\t\t\t/* Device reset might occur several times,\n\t\t\t\t * give it one more chance */\n\t\t\t\tif (--reset_retries > 0)\n\t\t\t\t\tcontinue;\n\t\t}\n\t\tretries--;\n\n\t} while (the_result && retries);\n\n\tif (the_result) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"READ CAPACITY(16) failed\\n\");\n\t\tread_capacity_error(sdkp, sdp, &sshdr, sense_valid, the_result);\n\t\treturn -EINVAL;\n\t}\n\n\tsector_size = get_unaligned_be32(&buffer[8]);\n\tlba = get_unaligned_be64(&buffer[0]);\n\n\tsd_read_protection_type(sdkp, buffer);\n\n\tif ((sizeof(sdkp->capacity) == 4) && (lba >= 0xffffffffULL)) {\n\t\tsd_printk(KERN_ERR, sdkp, \"Too big for this kernel. Use a \"\n\t\t\t\"kernel compiled with support for large block \"\n\t\t\t\"devices.\\n\");\n\t\tsdkp->capacity = 0;\n\t\treturn -EOVERFLOW;\n\t}\n\n\t/* Logical blocks per physical block exponent */\n\tsdkp->physical_block_size = (1 << (buffer[13] & 0xf)) * sector_size;\n\n\t/* Lowest aligned logical block */\n\talignment = ((buffer[14] & 0x3f) << 8 | buffer[15]) * sector_size;\n\tblk_queue_alignment_offset(sdp->request_queue, alignment);\n\tif (alignment && sdkp->first_scan)\n\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t  \"physical block alignment offset: %u\\n\", alignment);\n\n\tif (buffer[14] & 0x80) { /* LBPME */\n\t\tsdkp->lbpme = 1;\n\n\t\tif (buffer[14] & 0x40) /* LBPRZ */\n\t\t\tsdkp->lbprz = 1;\n\n\t\tsd_config_discard(sdkp, SD_LBP_WS16);\n\t}\n\n\tsdkp->capacity = lba + 1;\n\treturn sector_size;\n}\n\nstatic int read_capacity_10(struct scsi_disk *sdkp, struct scsi_device *sdp,\n\t\t\t\t\t\tunsigned char *buffer)\n{\n\tunsigned char cmd[16];\n\tstruct scsi_sense_hdr sshdr;\n\tint sense_valid = 0;\n\tint the_result;\n\tint retries = 3, reset_retries = READ_CAPACITY_RETRIES_ON_RESET;\n\tsector_t lba;\n\tunsigned sector_size;\n\n\tdo {\n\t\tcmd[0] = READ_CAPACITY;\n\t\tmemset(&cmd[1], 0, 9);\n\t\tmemset(buffer, 0, 8);\n\n\t\tthe_result = scsi_execute_req(sdp, cmd, DMA_FROM_DEVICE,\n\t\t\t\t\tbuffer, 8, &sshdr,\n\t\t\t\t\tSD_TIMEOUT, SD_MAX_RETRIES, NULL);\n\n\t\tif (media_not_present(sdkp, &sshdr))\n\t\t\treturn -ENODEV;\n\n\t\tif (the_result) {\n\t\t\tsense_valid = scsi_sense_valid(&sshdr);\n\t\t\tif (sense_valid &&\n\t\t\t    sshdr.sense_key == UNIT_ATTENTION &&\n\t\t\t    sshdr.asc == 0x29 && sshdr.ascq == 0x00)\n\t\t\t\t/* Device reset might occur several times,\n\t\t\t\t * give it one more chance */\n\t\t\t\tif (--reset_retries > 0)\n\t\t\t\t\tcontinue;\n\t\t}\n\t\tretries--;\n\n\t} while (the_result && retries);\n\n\tif (the_result) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"READ CAPACITY failed\\n\");\n\t\tread_capacity_error(sdkp, sdp, &sshdr, sense_valid, the_result);\n\t\treturn -EINVAL;\n\t}\n\n\tsector_size = get_unaligned_be32(&buffer[4]);\n\tlba = get_unaligned_be32(&buffer[0]);\n\n\tif (sdp->no_read_capacity_16 && (lba == 0xffffffff)) {\n\t\t/* Some buggy (usb cardreader) devices return an lba of\n\t\t   0xffffffff when the want to report a size of 0 (with\n\t\t   which they really mean no media is present) */\n\t\tsdkp->capacity = 0;\n\t\tsdkp->physical_block_size = sector_size;\n\t\treturn sector_size;\n\t}\n\n\tif ((sizeof(sdkp->capacity) == 4) && (lba == 0xffffffff)) {\n\t\tsd_printk(KERN_ERR, sdkp, \"Too big for this kernel. Use a \"\n\t\t\t\"kernel compiled with support for large block \"\n\t\t\t\"devices.\\n\");\n\t\tsdkp->capacity = 0;\n\t\treturn -EOVERFLOW;\n\t}\n\n\tsdkp->capacity = lba + 1;\n\tsdkp->physical_block_size = sector_size;\n\treturn sector_size;\n}\n\nstatic int sd_try_rc16_first(struct scsi_device *sdp)\n{\n\tif (sdp->host->max_cmd_len < 16)\n\t\treturn 0;\n\tif (sdp->scsi_level > SCSI_SPC_2)\n\t\treturn 1;\n\tif (scsi_device_protection(sdp))\n\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * read disk capacity\n */\nstatic void\nsd_read_capacity(struct scsi_disk *sdkp, unsigned char *buffer)\n{\n\tint sector_size;\n\tstruct scsi_device *sdp = sdkp->device;\n\tsector_t old_capacity = sdkp->capacity;\n\n\tif (sd_try_rc16_first(sdp)) {\n\t\tsector_size = read_capacity_16(sdkp, sdp, buffer);\n\t\tif (sector_size == -EOVERFLOW)\n\t\t\tgoto got_data;\n\t\tif (sector_size == -ENODEV)\n\t\t\treturn;\n\t\tif (sector_size < 0)\n\t\t\tsector_size = read_capacity_10(sdkp, sdp, buffer);\n\t\tif (sector_size < 0)\n\t\t\treturn;\n\t} else {\n\t\tsector_size = read_capacity_10(sdkp, sdp, buffer);\n\t\tif (sector_size == -EOVERFLOW)\n\t\t\tgoto got_data;\n\t\tif (sector_size < 0)\n\t\t\treturn;\n\t\tif ((sizeof(sdkp->capacity) > 4) &&\n\t\t    (sdkp->capacity > 0xffffffffULL)) {\n\t\t\tint old_sector_size = sector_size;\n\t\t\tsd_printk(KERN_NOTICE, sdkp, \"Very big device. \"\n\t\t\t\t\t\"Trying to use READ CAPACITY(16).\\n\");\n\t\t\tsector_size = read_capacity_16(sdkp, sdp, buffer);\n\t\t\tif (sector_size < 0) {\n\t\t\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t\t\t\"Using 0xffffffff as device size\\n\");\n\t\t\t\tsdkp->capacity = 1 + (sector_t) 0xffffffff;\n\t\t\t\tsector_size = old_sector_size;\n\t\t\t\tgoto got_data;\n\t\t\t}\n\t\t}\n\t}\n\n\t/* Some devices are known to return the total number of blocks,\n\t * not the highest block number.  Some devices have versions\n\t * which do this and others which do not.  Some devices we might\n\t * suspect of doing this but we don't know for certain.\n\t *\n\t * If we know the reported capacity is wrong, decrement it.  If\n\t * we can only guess, then assume the number of blocks is even\n\t * (usually true but not always) and err on the side of lowering\n\t * the capacity.\n\t */\n\tif (sdp->fix_capacity ||\n\t    (sdp->guess_capacity && (sdkp->capacity & 0x01))) {\n\t\tsd_printk(KERN_INFO, sdkp, \"Adjusting the sector count \"\n\t\t\t\t\"from its reported value: %llu\\n\",\n\t\t\t\t(unsigned long long) sdkp->capacity);\n\t\t--sdkp->capacity;\n\t}\n\ngot_data:\n\tif (sector_size == 0) {\n\t\tsector_size = 512;\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Sector size 0 reported, \"\n\t\t\t  \"assuming 512.\\n\");\n\t}\n\n\tif (sector_size != 512 &&\n\t    sector_size != 1024 &&\n\t    sector_size != 2048 &&\n\t    sector_size != 4096 &&\n\t    sector_size != 256) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Unsupported sector size %d.\\n\",\n\t\t\t  sector_size);\n\t\t/*\n\t\t * The user might want to re-format the drive with\n\t\t * a supported sectorsize.  Once this happens, it\n\t\t * would be relatively trivial to set the thing up.\n\t\t * For this reason, we leave the thing in the table.\n\t\t */\n\t\tsdkp->capacity = 0;\n\t\t/*\n\t\t * set a bogus sector size so the normal read/write\n\t\t * logic in the block layer will eventually refuse any\n\t\t * request on this device without tripping over power\n\t\t * of two sector size assumptions\n\t\t */\n\t\tsector_size = 512;\n\t}\n\tblk_queue_logical_block_size(sdp->request_queue, sector_size);\n\n\t{\n\t\tchar cap_str_2[10], cap_str_10[10];\n\t\tu64 sz = (u64)sdkp->capacity << ilog2(sector_size);\n\n\t\tstring_get_size(sz, STRING_UNITS_2, cap_str_2,\n\t\t\t\tsizeof(cap_str_2));\n\t\tstring_get_size(sz, STRING_UNITS_10, cap_str_10,\n\t\t\t\tsizeof(cap_str_10));\n\n\t\tif (sdkp->first_scan || old_capacity != sdkp->capacity) {\n\t\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t\t  \"%llu %d-byte logical blocks: (%s/%s)\\n\",\n\t\t\t\t  (unsigned long long)sdkp->capacity,\n\t\t\t\t  sector_size, cap_str_10, cap_str_2);\n\n\t\t\tif (sdkp->physical_block_size != sector_size)\n\t\t\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t\t\t  \"%u-byte physical blocks\\n\",\n\t\t\t\t\t  sdkp->physical_block_size);\n\t\t}\n\t}\n\n\t/* Rescale capacity to 512-byte units */\n\tif (sector_size == 4096)\n\t\tsdkp->capacity <<= 3;\n\telse if (sector_size == 2048)\n\t\tsdkp->capacity <<= 2;\n\telse if (sector_size == 1024)\n\t\tsdkp->capacity <<= 1;\n\telse if (sector_size == 256)\n\t\tsdkp->capacity >>= 1;\n\n\tblk_queue_physical_block_size(sdp->request_queue,\n\t\t\t\t      sdkp->physical_block_size);\n\tsdkp->device->sector_size = sector_size;\n}\n\n/* called with buffer of length 512 */\nstatic inline int\nsd_do_mode_sense(struct scsi_device *sdp, int dbd, int modepage,\n\t\t unsigned char *buffer, int len, struct scsi_mode_data *data,\n\t\t struct scsi_sense_hdr *sshdr)\n{\n\treturn scsi_mode_sense(sdp, dbd, modepage, buffer, len,\n\t\t\t       SD_TIMEOUT, SD_MAX_RETRIES, data,\n\t\t\t       sshdr);\n}\n\n/*\n * read write protect setting, if possible - called only in sd_revalidate_disk()\n * called with buffer of length SD_BUF_SIZE\n */\nstatic void\nsd_read_write_protect_flag(struct scsi_disk *sdkp, unsigned char *buffer)\n{\n\tint res;\n\tstruct scsi_device *sdp = sdkp->device;\n\tstruct scsi_mode_data data;\n\tint old_wp = sdkp->write_prot;\n\n\tset_disk_ro(sdkp->disk, 0);\n\tif (sdp->skip_ms_page_3f) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Assuming Write Enabled\\n\");\n\t\treturn;\n\t}\n\n\tif (sdp->use_192_bytes_for_3f) {\n\t\tres = sd_do_mode_sense(sdp, 0, 0x3F, buffer, 192, &data, NULL);\n\t} else {\n\t\t/*\n\t\t * First attempt: ask for all pages (0x3F), but only 4 bytes.\n\t\t * We have to start carefully: some devices hang if we ask\n\t\t * for more than is available.\n\t\t */\n\t\tres = sd_do_mode_sense(sdp, 0, 0x3F, buffer, 4, &data, NULL);\n\n\t\t/*\n\t\t * Second attempt: ask for page 0 When only page 0 is\n\t\t * implemented, a request for page 3F may return Sense Key\n\t\t * 5: Illegal Request, Sense Code 24: Invalid field in\n\t\t * CDB.\n\t\t */\n\t\tif (!scsi_status_is_good(res))\n\t\t\tres = sd_do_mode_sense(sdp, 0, 0, buffer, 4, &data, NULL);\n\n\t\t/*\n\t\t * Third attempt: ask 255 bytes, as we did earlier.\n\t\t */\n\t\tif (!scsi_status_is_good(res))\n\t\t\tres = sd_do_mode_sense(sdp, 0, 0x3F, buffer, 255,\n\t\t\t\t\t       &data, NULL);\n\t}\n\n\tif (!scsi_status_is_good(res)) {\n\t\tsd_printk(KERN_WARNING, sdkp,\n\t\t\t  \"Test WP failed, assume Write Enabled\\n\");\n\t} else {\n\t\tsdkp->write_prot = ((data.device_specific & 0x80) != 0);\n\t\tset_disk_ro(sdkp->disk, sdkp->write_prot);\n\t\tif (sdkp->first_scan || old_wp != sdkp->write_prot) {\n\t\t\tsd_printk(KERN_NOTICE, sdkp, \"Write Protect is %s\\n\",\n\t\t\t\t  sdkp->write_prot ? \"on\" : \"off\");\n\t\t\tsd_printk(KERN_DEBUG, sdkp,\n\t\t\t\t  \"Mode Sense: %02x %02x %02x %02x\\n\",\n\t\t\t\t  buffer[0], buffer[1], buffer[2], buffer[3]);\n\t\t}\n\t}\n}\n\n/*\n * sd_read_cache_type - called only from sd_revalidate_disk()\n * called with buffer of length SD_BUF_SIZE\n */\nstatic void\nsd_read_cache_type(struct scsi_disk *sdkp, unsigned char *buffer)\n{\n\tint len = 0, res;\n\tstruct scsi_device *sdp = sdkp->device;\n\n\tint dbd;\n\tint modepage;\n\tint first_len;\n\tstruct scsi_mode_data data;\n\tstruct scsi_sense_hdr sshdr;\n\tint old_wce = sdkp->WCE;\n\tint old_rcd = sdkp->RCD;\n\tint old_dpofua = sdkp->DPOFUA;\n\n\tfirst_len = 4;\n\tif (sdp->skip_ms_page_8) {\n\t\tif (sdp->type == TYPE_RBC)\n\t\t\tgoto defaults;\n\t\telse {\n\t\t\tif (sdp->skip_ms_page_3f)\n\t\t\t\tgoto defaults;\n\t\t\tmodepage = 0x3F;\n\t\t\tif (sdp->use_192_bytes_for_3f)\n\t\t\t\tfirst_len = 192;\n\t\t\tdbd = 0;\n\t\t}\n\t} else if (sdp->type == TYPE_RBC) {\n\t\tmodepage = 6;\n\t\tdbd = 8;\n\t} else {\n\t\tmodepage = 8;\n\t\tdbd = 0;\n\t}\n\n\t/* cautiously ask */\n\tres = sd_do_mode_sense(sdp, dbd, modepage, buffer, first_len,\n\t\t\t&data, &sshdr);\n\n\tif (!scsi_status_is_good(res))\n\t\tgoto bad_sense;\n\n\tif (!data.header_length) {\n\t\tmodepage = 6;\n\t\tfirst_len = 0;\n\t\tsd_printk(KERN_ERR, sdkp, \"Missing header in MODE_SENSE response\\n\");\n\t}\n\n\t/* that went OK, now ask for the proper length */\n\tlen = data.length;\n\n\t/*\n\t * We're only interested in the first three bytes, actually.\n\t * But the data cache page is defined for the first 20.\n\t */\n\tif (len < 3)\n\t\tgoto bad_sense;\n\telse if (len > SD_BUF_SIZE) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Truncating mode parameter \"\n\t\t\t  \"data from %d to %d bytes\\n\", len, SD_BUF_SIZE);\n\t\tlen = SD_BUF_SIZE;\n\t}\n\tif (modepage == 0x3F && sdp->use_192_bytes_for_3f)\n\t\tlen = 192;\n\n\t/* Get the data */\n\tif (len > first_len)\n\t\tres = sd_do_mode_sense(sdp, dbd, modepage, buffer, len,\n\t\t\t\t&data, &sshdr);\n\n\tif (scsi_status_is_good(res)) {\n\t\tint offset = data.header_length + data.block_descriptor_length;\n\n\t\twhile (offset < len) {\n\t\t\tu8 page_code = buffer[offset] & 0x3F;\n\t\t\tu8 spf       = buffer[offset] & 0x40;\n\n\t\t\tif (page_code == 8 || page_code == 6) {\n\t\t\t\t/* We're interested only in the first 3 bytes.\n\t\t\t\t */\n\t\t\t\tif (len - offset <= 2) {\n\t\t\t\t\tsd_printk(KERN_ERR, sdkp, \"Incomplete \"\n\t\t\t\t\t\t  \"mode parameter data\\n\");\n\t\t\t\t\tgoto defaults;\n\t\t\t\t} else {\n\t\t\t\t\tmodepage = page_code;\n\t\t\t\t\tgoto Page_found;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t/* Go to the next page */\n\t\t\t\tif (spf && len - offset > 3)\n\t\t\t\t\toffset += 4 + (buffer[offset+2] << 8) +\n\t\t\t\t\t\tbuffer[offset+3];\n\t\t\t\telse if (!spf && len - offset > 1)\n\t\t\t\t\toffset += 2 + buffer[offset+1];\n\t\t\t\telse {\n\t\t\t\t\tsd_printk(KERN_ERR, sdkp, \"Incomplete \"\n\t\t\t\t\t\t  \"mode parameter data\\n\");\n\t\t\t\t\tgoto defaults;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (modepage == 0x3F) {\n\t\t\tsd_printk(KERN_ERR, sdkp, \"No Caching mode page \"\n\t\t\t\t  \"present\\n\");\n\t\t\tgoto defaults;\n\t\t} else if ((buffer[offset] & 0x3f) != modepage) {\n\t\t\tsd_printk(KERN_ERR, sdkp, \"Got wrong page\\n\");\n\t\t\tgoto defaults;\n\t\t}\n\tPage_found:\n\t\tif (modepage == 8) {\n\t\t\tsdkp->WCE = ((buffer[offset + 2] & 0x04) != 0);\n\t\t\tsdkp->RCD = ((buffer[offset + 2] & 0x01) != 0);\n\t\t} else {\n\t\t\tsdkp->WCE = ((buffer[offset + 2] & 0x01) == 0);\n\t\t\tsdkp->RCD = 0;\n\t\t}\n\n\t\tsdkp->DPOFUA = (data.device_specific & 0x10) != 0;\n\t\tif (sdkp->DPOFUA && !sdkp->device->use_10_for_rw) {\n\t\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t\t  \"Uses READ/WRITE(6), disabling FUA\\n\");\n\t\t\tsdkp->DPOFUA = 0;\n\t\t}\n\n\t\tif (sdkp->first_scan || old_wce != sdkp->WCE ||\n\t\t    old_rcd != sdkp->RCD || old_dpofua != sdkp->DPOFUA)\n\t\t\tsd_printk(KERN_NOTICE, sdkp,\n\t\t\t\t  \"Write cache: %s, read cache: %s, %s\\n\",\n\t\t\t\t  sdkp->WCE ? \"enabled\" : \"disabled\",\n\t\t\t\t  sdkp->RCD ? \"disabled\" : \"enabled\",\n\t\t\t\t  sdkp->DPOFUA ? \"supports DPO and FUA\"\n\t\t\t\t  : \"doesn't support DPO or FUA\");\n\n\t\treturn;\n\t}\n\nbad_sense:\n\tif (scsi_sense_valid(&sshdr) &&\n\t    sshdr.sense_key == ILLEGAL_REQUEST &&\n\t    sshdr.asc == 0x24 && sshdr.ascq == 0x0)\n\t\t/* Invalid field in CDB */\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Cache data unavailable\\n\");\n\telse\n\t\tsd_printk(KERN_ERR, sdkp, \"Asking for cache data failed\\n\");\n\ndefaults:\n\tsd_printk(KERN_ERR, sdkp, \"Assuming drive cache: write through\\n\");\n\tsdkp->WCE = 0;\n\tsdkp->RCD = 0;\n\tsdkp->DPOFUA = 0;\n}\n\n/*\n * The ATO bit indicates whether the DIF application tag is available\n * for use by the operating system.\n */\nstatic void sd_read_app_tag_own(struct scsi_disk *sdkp, unsigned char *buffer)\n{\n\tint res, offset;\n\tstruct scsi_device *sdp = sdkp->device;\n\tstruct scsi_mode_data data;\n\tstruct scsi_sense_hdr sshdr;\n\n\tif (sdp->type != TYPE_DISK)\n\t\treturn;\n\n\tif (sdkp->protection_type == 0)\n\t\treturn;\n\n\tres = scsi_mode_sense(sdp, 1, 0x0a, buffer, 36, SD_TIMEOUT,\n\t\t\t      SD_MAX_RETRIES, &data, &sshdr);\n\n\tif (!scsi_status_is_good(res) || !data.header_length ||\n\t    data.length < 6) {\n\t\tsd_printk(KERN_WARNING, sdkp,\n\t\t\t  \"getting Control mode page failed, assume no ATO\\n\");\n\n\t\tif (scsi_sense_valid(&sshdr))\n\t\t\tsd_print_sense_hdr(sdkp, &sshdr);\n\n\t\treturn;\n\t}\n\n\toffset = data.header_length + data.block_descriptor_length;\n\n\tif ((buffer[offset] & 0x3f) != 0x0a) {\n\t\tsd_printk(KERN_ERR, sdkp, \"ATO Got wrong page\\n\");\n\t\treturn;\n\t}\n\n\tif ((buffer[offset + 5] & 0x80) == 0)\n\t\treturn;\n\n\tsdkp->ATO = 1;\n\n\treturn;\n}\n\n/**\n * sd_read_block_limits - Query disk device for preferred I/O sizes.\n * @disk: disk to query\n */\nstatic void sd_read_block_limits(struct scsi_disk *sdkp)\n{\n\tunsigned int sector_sz = sdkp->device->sector_size;\n\tconst int vpd_len = 64;\n\tunsigned char *buffer = kmalloc(vpd_len, GFP_KERNEL);\n\n\tif (!buffer ||\n\t    /* Block Limits VPD */\n\t    scsi_get_vpd_page(sdkp->device, 0xb0, buffer, vpd_len))\n\t\tgoto out;\n\n\tblk_queue_io_min(sdkp->disk->queue,\n\t\t\t get_unaligned_be16(&buffer[6]) * sector_sz);\n\tblk_queue_io_opt(sdkp->disk->queue,\n\t\t\t get_unaligned_be32(&buffer[12]) * sector_sz);\n\n\tif (buffer[3] == 0x3c) {\n\t\tunsigned int lba_count, desc_count;\n\n\t\tsdkp->max_ws_blocks =\n\t\t\t(u32) min_not_zero(get_unaligned_be64(&buffer[36]),\n\t\t\t\t\t   (u64)0xffffffff);\n\n\t\tif (!sdkp->lbpme)\n\t\t\tgoto out;\n\n\t\tlba_count = get_unaligned_be32(&buffer[20]);\n\t\tdesc_count = get_unaligned_be32(&buffer[24]);\n\n\t\tif (lba_count && desc_count)\n\t\t\tsdkp->max_unmap_blocks = lba_count;\n\n\t\tsdkp->unmap_granularity = get_unaligned_be32(&buffer[28]);\n\n\t\tif (buffer[32] & 0x80)\n\t\t\tsdkp->unmap_alignment =\n\t\t\t\tget_unaligned_be32(&buffer[32]) & ~(1 << 31);\n\n\t\tif (!sdkp->lbpvpd) { /* LBP VPD page not provided */\n\n\t\t\tif (sdkp->max_unmap_blocks)\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_UNMAP);\n\t\t\telse\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_WS16);\n\n\t\t} else {\t/* LBP VPD page tells us what to use */\n\n\t\t\tif (sdkp->lbpu && sdkp->max_unmap_blocks)\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_UNMAP);\n\t\t\telse if (sdkp->lbpws)\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_WS16);\n\t\t\telse if (sdkp->lbpws10)\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_WS10);\n\t\t\telse\n\t\t\t\tsd_config_discard(sdkp, SD_LBP_DISABLE);\n\t\t}\n\t}\n\n out:\n\tkfree(buffer);\n}\n\n/**\n * sd_read_block_characteristics - Query block dev. characteristics\n * @disk: disk to query\n */\nstatic void sd_read_block_characteristics(struct scsi_disk *sdkp)\n{\n\tunsigned char *buffer;\n\tu16 rot;\n\tconst int vpd_len = 64;\n\n\tbuffer = kmalloc(vpd_len, GFP_KERNEL);\n\n\tif (!buffer ||\n\t    /* Block Device Characteristics VPD */\n\t    scsi_get_vpd_page(sdkp->device, 0xb1, buffer, vpd_len))\n\t\tgoto out;\n\n\trot = get_unaligned_be16(&buffer[4]);\n\n\tif (rot == 1)\n\t\tqueue_flag_set_unlocked(QUEUE_FLAG_NONROT, sdkp->disk->queue);\n\n out:\n\tkfree(buffer);\n}\n\n/**\n * sd_read_block_provisioning - Query provisioning VPD page\n * @disk: disk to query\n */\nstatic void sd_read_block_provisioning(struct scsi_disk *sdkp)\n{\n\tunsigned char *buffer;\n\tconst int vpd_len = 8;\n\n\tif (sdkp->lbpme == 0)\n\t\treturn;\n\n\tbuffer = kmalloc(vpd_len, GFP_KERNEL);\n\n\tif (!buffer || scsi_get_vpd_page(sdkp->device, 0xb2, buffer, vpd_len))\n\t\tgoto out;\n\n\tsdkp->lbpvpd\t= 1;\n\tsdkp->lbpu\t= (buffer[5] >> 7) & 1;\t/* UNMAP */\n\tsdkp->lbpws\t= (buffer[5] >> 6) & 1;\t/* WRITE SAME(16) with UNMAP */\n\tsdkp->lbpws10\t= (buffer[5] >> 5) & 1;\t/* WRITE SAME(10) with UNMAP */\n\n out:\n\tkfree(buffer);\n}\n\nstatic int sd_try_extended_inquiry(struct scsi_device *sdp)\n{\n\t/*\n\t * Although VPD inquiries can go to SCSI-2 type devices,\n\t * some USB ones crash on receiving them, and the pages\n\t * we currently ask for are for SPC-3 and beyond\n\t */\n\tif (sdp->scsi_level > SCSI_SPC_2)\n\t\treturn 1;\n\treturn 0;\n}\n\n/**\n *\tsd_revalidate_disk - called the first time a new disk is seen,\n *\tperforms disk spin up, read_capacity, etc.\n *\t@disk: struct gendisk we care about\n **/\nstatic int sd_revalidate_disk(struct gendisk *disk)\n{\n\tstruct scsi_disk *sdkp = scsi_disk(disk);\n\tstruct scsi_device *sdp = sdkp->device;\n\tunsigned char *buffer;\n\tunsigned flush = 0;\n\n\tSCSI_LOG_HLQUEUE(3, sd_printk(KERN_INFO, sdkp,\n\t\t\t\t      \"sd_revalidate_disk\\n\"));\n\n\t/*\n\t * If the device is offline, don't try and read capacity or any\n\t * of the other niceties.\n\t */\n\tif (!scsi_device_online(sdp))\n\t\tgoto out;\n\n\tbuffer = kmalloc(SD_BUF_SIZE, GFP_KERNEL);\n\tif (!buffer) {\n\t\tsd_printk(KERN_WARNING, sdkp, \"sd_revalidate_disk: Memory \"\n\t\t\t  \"allocation failure.\\n\");\n\t\tgoto out;\n\t}\n\n\tsd_spinup_disk(sdkp);\n\n\t/*\n\t * Without media there is no reason to ask; moreover, some devices\n\t * react badly if we do.\n\t */\n\tif (sdkp->media_present) {\n\t\tsd_read_capacity(sdkp, buffer);\n\n\t\tif (sd_try_extended_inquiry(sdp)) {\n\t\t\tsd_read_block_provisioning(sdkp);\n\t\t\tsd_read_block_limits(sdkp);\n\t\t\tsd_read_block_characteristics(sdkp);\n\t\t}\n\n\t\tsd_read_write_protect_flag(sdkp, buffer);\n\t\tsd_read_cache_type(sdkp, buffer);\n\t\tsd_read_app_tag_own(sdkp, buffer);\n\t}\n\n\tsdkp->first_scan = 0;\n\n\t/*\n\t * We now have all cache related info, determine how we deal\n\t * with flush requests.\n\t */\n\tif (sdkp->WCE) {\n\t\tflush |= REQ_FLUSH;\n\t\tif (sdkp->DPOFUA)\n\t\t\tflush |= REQ_FUA;\n\t}\n\n\tblk_queue_flush(sdkp->disk->queue, flush);\n\n\tset_capacity(disk, sdkp->capacity);\n\tkfree(buffer);\n\n out:\n\treturn 0;\n}\n\n/**\n *\tsd_unlock_native_capacity - unlock native capacity\n *\t@disk: struct gendisk to set capacity for\n *\n *\tBlock layer calls this function if it detects that partitions\n *\ton @disk reach beyond the end of the device.  If the SCSI host\n *\timplements ->unlock_native_capacity() method, it's invoked to\n *\tgive it a chance to adjust the device capacity.\n *\n *\tCONTEXT:\n *\tDefined by block layer.  Might sleep.\n */\nstatic void sd_unlock_native_capacity(struct gendisk *disk)\n{\n\tstruct scsi_device *sdev = scsi_disk(disk)->device;\n\n\tif (sdev->host->hostt->unlock_native_capacity)\n\t\tsdev->host->hostt->unlock_native_capacity(sdev);\n}\n\n/**\n *\tsd_format_disk_name - format disk name\n *\t@prefix: name prefix - ie. \"sd\" for SCSI disks\n *\t@index: index of the disk to format name for\n *\t@buf: output buffer\n *\t@buflen: length of the output buffer\n *\n *\tSCSI disk names starts at sda.  The 26th device is sdz and the\n *\t27th is sdaa.  The last one for two lettered suffix is sdzz\n *\twhich is followed by sdaaa.\n *\n *\tThis is basically 26 base counting with one extra 'nil' entry\n *\tat the beginning from the second digit on and can be\n *\tdetermined using similar method as 26 base conversion with the\n *\tindex shifted -1 after each digit is computed.\n *\n *\tCONTEXT:\n *\tDon't care.\n *\n *\tRETURNS:\n *\t0 on success, -errno on failure.\n */\nstatic int sd_format_disk_name(char *prefix, int index, char *buf, int buflen)\n{\n\tconst int base = 'z' - 'a' + 1;\n\tchar *begin = buf + strlen(prefix);\n\tchar *end = buf + buflen;\n\tchar *p;\n\tint unit;\n\n\tp = end - 1;\n\t*p = '\\0';\n\tunit = base;\n\tdo {\n\t\tif (p == begin)\n\t\t\treturn -EINVAL;\n\t\t*--p = 'a' + (index % unit);\n\t\tindex = (index / unit) - 1;\n\t} while (index >= 0);\n\n\tmemmove(begin, p, end - p);\n\tmemcpy(buf, prefix, strlen(prefix));\n\n\treturn 0;\n}\n\n/*\n * The asynchronous part of sd_probe\n */\nstatic void sd_probe_async(void *data, async_cookie_t cookie)\n{\n\tstruct scsi_disk *sdkp = data;\n\tstruct scsi_device *sdp;\n\tstruct gendisk *gd;\n\tu32 index;\n\tstruct device *dev;\n\n\tsdp = sdkp->device;\n\tgd = sdkp->disk;\n\tindex = sdkp->index;\n\tdev = &sdp->sdev_gendev;\n\n\tgd->major = sd_major((index & 0xf0) >> 4);\n\tgd->first_minor = ((index & 0xf) << 4) | (index & 0xfff00);\n\tgd->minors = SD_MINORS;\n\n\tgd->fops = &sd_fops;\n\tgd->private_data = &sdkp->driver;\n\tgd->queue = sdkp->device->request_queue;\n\n\t/* defaults, until the device tells us otherwise */\n\tsdp->sector_size = 512;\n\tsdkp->capacity = 0;\n\tsdkp->media_present = 1;\n\tsdkp->write_prot = 0;\n\tsdkp->WCE = 0;\n\tsdkp->RCD = 0;\n\tsdkp->ATO = 0;\n\tsdkp->first_scan = 1;\n\n\tsd_revalidate_disk(gd);\n\n\tblk_queue_prep_rq(sdp->request_queue, sd_prep_fn);\n\tblk_queue_unprep_rq(sdp->request_queue, sd_unprep_fn);\n\n\tgd->driverfs_dev = &sdp->sdev_gendev;\n\tgd->flags = GENHD_FL_EXT_DEVT;\n\tif (sdp->removable) {\n\t\tgd->flags |= GENHD_FL_REMOVABLE;\n\t\tgd->events |= DISK_EVENT_MEDIA_CHANGE;\n\t}\n\n\tadd_disk(gd);\n\tsd_dif_config_host(sdkp);\n\n\tsd_revalidate_disk(gd);\n\n\tsd_printk(KERN_NOTICE, sdkp, \"Attached SCSI %sdisk\\n\",\n\t\t  sdp->removable ? \"removable \" : \"\");\n\tscsi_autopm_put_device(sdp);\n\tput_device(&sdkp->dev);\n}\n\n/**\n *\tsd_probe - called during driver initialization and whenever a\n *\tnew scsi device is attached to the system. It is called once\n *\tfor each scsi device (not just disks) present.\n *\t@dev: pointer to device object\n *\n *\tReturns 0 if successful (or not interested in this scsi device \n *\t(e.g. scanner)); 1 when there is an error.\n *\n *\tNote: this function is invoked from the scsi mid-level.\n *\tThis function sets up the mapping between a given \n *\t<host,channel,id,lun> (found in sdp) and new device name \n *\t(e.g. /dev/sda). More precisely it is the block device major \n *\tand minor number that is chosen here.\n *\n *\tAssume sd_attach is not re-entrant (for time being)\n *\tAlso think about sd_attach() and sd_remove() running coincidentally.\n **/\nstatic int sd_probe(struct device *dev)\n{\n\tstruct scsi_device *sdp = to_scsi_device(dev);\n\tstruct scsi_disk *sdkp;\n\tstruct gendisk *gd;\n\tint index;\n\tint error;\n\n\terror = -ENODEV;\n\tif (sdp->type != TYPE_DISK && sdp->type != TYPE_MOD && sdp->type != TYPE_RBC)\n\t\tgoto out;\n\n\tSCSI_LOG_HLQUEUE(3, sdev_printk(KERN_INFO, sdp,\n\t\t\t\t\t\"sd_attach\\n\"));\n\n\terror = -ENOMEM;\n\tsdkp = kzalloc(sizeof(*sdkp), GFP_KERNEL);\n\tif (!sdkp)\n\t\tgoto out;\n\n\tgd = alloc_disk(SD_MINORS);\n\tif (!gd)\n\t\tgoto out_free;\n\n\tdo {\n\t\tif (!ida_pre_get(&sd_index_ida, GFP_KERNEL))\n\t\t\tgoto out_put;\n\n\t\tspin_lock(&sd_index_lock);\n\t\terror = ida_get_new(&sd_index_ida, &index);\n\t\tspin_unlock(&sd_index_lock);\n\t} while (error == -EAGAIN);\n\n\tif (error) {\n\t\tsdev_printk(KERN_WARNING, sdp, \"sd_probe: memory exhausted.\\n\");\n\t\tgoto out_put;\n\t}\n\n\terror = sd_format_disk_name(\"sd\", index, gd->disk_name, DISK_NAME_LEN);\n\tif (error) {\n\t\tsdev_printk(KERN_WARNING, sdp, \"SCSI disk (sd) name length exceeded.\\n\");\n\t\tgoto out_free_index;\n\t}\n\n\tsdkp->device = sdp;\n\tsdkp->driver = &sd_template;\n\tsdkp->disk = gd;\n\tsdkp->index = index;\n\tatomic_set(&sdkp->openers, 0);\n\n\tif (!sdp->request_queue->rq_timeout) {\n\t\tif (sdp->type != TYPE_MOD)\n\t\t\tblk_queue_rq_timeout(sdp->request_queue, SD_TIMEOUT);\n\t\telse\n\t\t\tblk_queue_rq_timeout(sdp->request_queue,\n\t\t\t\t\t     SD_MOD_TIMEOUT);\n\t}\n\n\tdevice_initialize(&sdkp->dev);\n\tsdkp->dev.parent = dev;\n\tsdkp->dev.class = &sd_disk_class;\n\tdev_set_name(&sdkp->dev, dev_name(dev));\n\n\tif (device_add(&sdkp->dev))\n\t\tgoto out_free_index;\n\n\tget_device(dev);\n\tdev_set_drvdata(dev, sdkp);\n\n\tget_device(&sdkp->dev);\t/* prevent release before async_schedule */\n\tasync_schedule(sd_probe_async, sdkp);\n\n\treturn 0;\n\n out_free_index:\n\tspin_lock(&sd_index_lock);\n\tida_remove(&sd_index_ida, index);\n\tspin_unlock(&sd_index_lock);\n out_put:\n\tput_disk(gd);\n out_free:\n\tkfree(sdkp);\n out:\n\treturn error;\n}\n\n/**\n *\tsd_remove - called whenever a scsi disk (previously recognized by\n *\tsd_probe) is detached from the system. It is called (potentially\n *\tmultiple times) during sd module unload.\n *\t@sdp: pointer to mid level scsi device object\n *\n *\tNote: this function is invoked from the scsi mid-level.\n *\tThis function potentially frees up a device name (e.g. /dev/sdc)\n *\tthat could be re-used by a subsequent sd_probe().\n *\tThis function is not called when the built-in sd driver is \"exit-ed\".\n **/\nstatic int sd_remove(struct device *dev)\n{\n\tstruct scsi_disk *sdkp;\n\n\tsdkp = dev_get_drvdata(dev);\n\tscsi_autopm_get_device(sdkp->device);\n\n\tasync_synchronize_full();\n\tblk_queue_prep_rq(sdkp->device->request_queue, scsi_prep_fn);\n\tblk_queue_unprep_rq(sdkp->device->request_queue, NULL);\n\tdevice_del(&sdkp->dev);\n\tdel_gendisk(sdkp->disk);\n\tsd_shutdown(dev);\n\n\tmutex_lock(&sd_ref_mutex);\n\tdev_set_drvdata(dev, NULL);\n\tput_device(&sdkp->dev);\n\tmutex_unlock(&sd_ref_mutex);\n\n\treturn 0;\n}\n\n/**\n *\tscsi_disk_release - Called to free the scsi_disk structure\n *\t@dev: pointer to embedded class device\n *\n *\tsd_ref_mutex must be held entering this routine.  Because it is\n *\tcalled on last put, you should always use the scsi_disk_get()\n *\tscsi_disk_put() helpers which manipulate the semaphore directly\n *\tand never do a direct put_device.\n **/\nstatic void scsi_disk_release(struct device *dev)\n{\n\tstruct scsi_disk *sdkp = to_scsi_disk(dev);\n\tstruct gendisk *disk = sdkp->disk;\n\t\n\tspin_lock(&sd_index_lock);\n\tida_remove(&sd_index_ida, sdkp->index);\n\tspin_unlock(&sd_index_lock);\n\n\tdisk->private_data = NULL;\n\tput_disk(disk);\n\tput_device(&sdkp->device->sdev_gendev);\n\n\tkfree(sdkp);\n}\n\nstatic int sd_start_stop_device(struct scsi_disk *sdkp, int start)\n{\n\tunsigned char cmd[6] = { START_STOP };\t/* START_VALID */\n\tstruct scsi_sense_hdr sshdr;\n\tstruct scsi_device *sdp = sdkp->device;\n\tint res;\n\n\tif (start)\n\t\tcmd[4] |= 1;\t/* START */\n\n\tif (sdp->start_stop_pwr_cond)\n\t\tcmd[4] |= start ? 1 << 4 : 3 << 4;\t/* Active or Standby */\n\n\tif (!scsi_device_online(sdp))\n\t\treturn -ENODEV;\n\n\tres = scsi_execute_req(sdp, cmd, DMA_NONE, NULL, 0, &sshdr,\n\t\t\t       SD_TIMEOUT, SD_MAX_RETRIES, NULL);\n\tif (res) {\n\t\tsd_printk(KERN_WARNING, sdkp, \"START_STOP FAILED\\n\");\n\t\tsd_print_result(sdkp, res);\n\t\tif (driver_byte(res) & DRIVER_SENSE)\n\t\t\tsd_print_sense_hdr(sdkp, &sshdr);\n\t}\n\n\treturn res;\n}\n\n/*\n * Send a SYNCHRONIZE CACHE instruction down to the device through\n * the normal SCSI command structure.  Wait for the command to\n * complete.\n */\nstatic void sd_shutdown(struct device *dev)\n{\n\tstruct scsi_disk *sdkp = scsi_disk_get_from_dev(dev);\n\n\tif (!sdkp)\n\t\treturn;         /* this can happen */\n\n\tif (pm_runtime_suspended(dev))\n\t\tgoto exit;\n\n\tif (sdkp->WCE) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Synchronizing SCSI cache\\n\");\n\t\tsd_sync_cache(sdkp);\n\t}\n\n\tif (system_state != SYSTEM_RESTART && sdkp->device->manage_start_stop) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Stopping disk\\n\");\n\t\tsd_start_stop_device(sdkp, 0);\n\t}\n\nexit:\n\tscsi_disk_put(sdkp);\n}\n\nstatic int sd_suspend(struct device *dev, pm_message_t mesg)\n{\n\tstruct scsi_disk *sdkp = scsi_disk_get_from_dev(dev);\n\tint ret = 0;\n\n\tif (!sdkp)\n\t\treturn 0;\t/* this can happen */\n\n\tif (sdkp->WCE) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Synchronizing SCSI cache\\n\");\n\t\tret = sd_sync_cache(sdkp);\n\t\tif (ret)\n\t\t\tgoto done;\n\t}\n\n\tif ((mesg.event & PM_EVENT_SLEEP) && sdkp->device->manage_start_stop) {\n\t\tsd_printk(KERN_NOTICE, sdkp, \"Stopping disk\\n\");\n\t\tret = sd_start_stop_device(sdkp, 0);\n\t}\n\ndone:\n\tscsi_disk_put(sdkp);\n\treturn ret;\n}\n\nstatic int sd_resume(struct device *dev)\n{\n\tstruct scsi_disk *sdkp = scsi_disk_get_from_dev(dev);\n\tint ret = 0;\n\n\tif (!sdkp->device->manage_start_stop)\n\t\tgoto done;\n\n\tsd_printk(KERN_NOTICE, sdkp, \"Starting disk\\n\");\n\tret = sd_start_stop_device(sdkp, 1);\n\ndone:\n\tscsi_disk_put(sdkp);\n\treturn ret;\n}\n\n/**\n *\tinit_sd - entry point for this driver (both when built in or when\n *\ta module).\n *\n *\tNote: this function registers this driver with the scsi mid-level.\n **/\nstatic int __init init_sd(void)\n{\n\tint majors = 0, i, err;\n\n\tSCSI_LOG_HLQUEUE(3, printk(\"init_sd: sd driver entry point\\n\"));\n\n\tfor (i = 0; i < SD_MAJORS; i++)\n\t\tif (register_blkdev(sd_major(i), \"sd\") == 0)\n\t\t\tmajors++;\n\n\tif (!majors)\n\t\treturn -ENODEV;\n\n\terr = class_register(&sd_disk_class);\n\tif (err)\n\t\tgoto err_out;\n\n\terr = scsi_register_driver(&sd_template.gendrv);\n\tif (err)\n\t\tgoto err_out_class;\n\n\tsd_cdb_cache = kmem_cache_create(\"sd_ext_cdb\", SD_EXT_CDB_SIZE,\n\t\t\t\t\t 0, 0, NULL);\n\tif (!sd_cdb_cache) {\n\t\tprintk(KERN_ERR \"sd: can't init extended cdb cache\\n\");\n\t\tgoto err_out_class;\n\t}\n\n\tsd_cdb_pool = mempool_create_slab_pool(SD_MEMPOOL_SIZE, sd_cdb_cache);\n\tif (!sd_cdb_pool) {\n\t\tprintk(KERN_ERR \"sd: can't init extended cdb pool\\n\");\n\t\tgoto err_out_cache;\n\t}\n\n\treturn 0;\n\nerr_out_cache:\n\tkmem_cache_destroy(sd_cdb_cache);\n\nerr_out_class:\n\tclass_unregister(&sd_disk_class);\nerr_out:\n\tfor (i = 0; i < SD_MAJORS; i++)\n\t\tunregister_blkdev(sd_major(i), \"sd\");\n\treturn err;\n}\n\n/**\n *\texit_sd - exit point for this driver (when it is a module).\n *\n *\tNote: this function unregisters this driver from the scsi mid-level.\n **/\nstatic void __exit exit_sd(void)\n{\n\tint i;\n\n\tSCSI_LOG_HLQUEUE(3, printk(\"exit_sd: exiting sd driver\\n\"));\n\n\tmempool_destroy(sd_cdb_pool);\n\tkmem_cache_destroy(sd_cdb_cache);\n\n\tscsi_unregister_driver(&sd_template.gendrv);\n\tclass_unregister(&sd_disk_class);\n\n\tfor (i = 0; i < SD_MAJORS; i++)\n\t\tunregister_blkdev(sd_major(i), \"sd\");\n}\n\nmodule_init(init_sd);\nmodule_exit(exit_sd);\n\nstatic void sd_print_sense_hdr(struct scsi_disk *sdkp,\n\t\t\t       struct scsi_sense_hdr *sshdr)\n{\n\tsd_printk(KERN_INFO, sdkp, \" \");\n\tscsi_show_sense_hdr(sshdr);\n\tsd_printk(KERN_INFO, sdkp, \" \");\n\tscsi_show_extd_sense(sshdr->asc, sshdr->ascq);\n}\n\nstatic void sd_print_result(struct scsi_disk *sdkp, int result)\n{\n\tsd_printk(KERN_INFO, sdkp, \" \");\n\tscsi_show_result(result);\n}\n\n", "#ifndef _LINUX_BLKDEV_H\n#define _LINUX_BLKDEV_H\n\n#ifdef CONFIG_BLOCK\n\n#include <linux/sched.h>\n#include <linux/major.h>\n#include <linux/genhd.h>\n#include <linux/list.h>\n#include <linux/timer.h>\n#include <linux/workqueue.h>\n#include <linux/pagemap.h>\n#include <linux/backing-dev.h>\n#include <linux/wait.h>\n#include <linux/mempool.h>\n#include <linux/bio.h>\n#include <linux/stringify.h>\n#include <linux/gfp.h>\n#include <linux/bsg.h>\n#include <linux/smp.h>\n\n#include <asm/scatterlist.h>\n\nstruct module;\nstruct scsi_ioctl_command;\n\nstruct request_queue;\nstruct elevator_queue;\nstruct request_pm_state;\nstruct blk_trace;\nstruct request;\nstruct sg_io_hdr;\nstruct bsg_job;\n\n#define BLKDEV_MIN_RQ\t4\n#define BLKDEV_MAX_RQ\t128\t/* Default maximum */\n\nstruct request;\ntypedef void (rq_end_io_fn)(struct request *, int);\n\nstruct request_list {\n\t/*\n\t * count[], starved[], and wait[] are indexed by\n\t * BLK_RW_SYNC/BLK_RW_ASYNC\n\t */\n\tint count[2];\n\tint starved[2];\n\tint elvpriv;\n\tmempool_t *rq_pool;\n\twait_queue_head_t wait[2];\n};\n\n/*\n * request command types\n */\nenum rq_cmd_type_bits {\n\tREQ_TYPE_FS\t\t= 1,\t/* fs request */\n\tREQ_TYPE_BLOCK_PC,\t\t/* scsi command */\n\tREQ_TYPE_SENSE,\t\t\t/* sense request */\n\tREQ_TYPE_PM_SUSPEND,\t\t/* suspend request */\n\tREQ_TYPE_PM_RESUME,\t\t/* resume request */\n\tREQ_TYPE_PM_SHUTDOWN,\t\t/* shutdown request */\n\tREQ_TYPE_SPECIAL,\t\t/* driver defined type */\n\t/*\n\t * for ATA/ATAPI devices. this really doesn't belong here, ide should\n\t * use REQ_TYPE_SPECIAL and use rq->cmd[0] with the range of driver\n\t * private REQ_LB opcodes to differentiate what type of request this is\n\t */\n\tREQ_TYPE_ATA_TASKFILE,\n\tREQ_TYPE_ATA_PC,\n};\n\n#define BLK_MAX_CDB\t16\n\n/*\n * try to put the fields that are referenced together in the same cacheline.\n * if you modify this structure, be sure to check block/blk-core.c:blk_rq_init()\n * as well!\n */\nstruct request {\n\tstruct list_head queuelist;\n\tstruct call_single_data csd;\n\n\tstruct request_queue *q;\n\n\tunsigned int cmd_flags;\n\tenum rq_cmd_type_bits cmd_type;\n\tunsigned long atomic_flags;\n\n\tint cpu;\n\n\t/* the following two fields are internal, NEVER access directly */\n\tunsigned int __data_len;\t/* total data len */\n\tsector_t __sector;\t\t/* sector cursor */\n\n\tstruct bio *bio;\n\tstruct bio *biotail;\n\n\tstruct hlist_node hash;\t/* merge hash */\n\t/*\n\t * The rb_node is only used inside the io scheduler, requests\n\t * are pruned when moved to the dispatch queue. So let the\n\t * completion_data share space with the rb_node.\n\t */\n\tunion {\n\t\tstruct rb_node rb_node;\t/* sort/lookup */\n\t\tvoid *completion_data;\n\t};\n\n\t/*\n\t * Three pointers are available for the IO schedulers, if they need\n\t * more they have to dynamically allocate it.  Flush requests are\n\t * never put on the IO scheduler. So let the flush fields share\n\t * space with the three elevator_private pointers.\n\t */\n\tunion {\n\t\tvoid *elevator_private[3];\n\t\tstruct {\n\t\t\tunsigned int\t\tseq;\n\t\t\tstruct list_head\tlist;\n\t\t\trq_end_io_fn\t\t*saved_end_io;\n\t\t} flush;\n\t};\n\n\tstruct gendisk *rq_disk;\n\tstruct hd_struct *part;\n\tunsigned long start_time;\n#ifdef CONFIG_BLK_CGROUP\n\tunsigned long long start_time_ns;\n\tunsigned long long io_start_time_ns;    /* when passed to hardware */\n#endif\n\t/* Number of scatter-gather DMA addr+len pairs after\n\t * physical address coalescing is performed.\n\t */\n\tunsigned short nr_phys_segments;\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\tunsigned short nr_integrity_segments;\n#endif\n\n\tunsigned short ioprio;\n\n\tint ref_count;\n\n\tvoid *special;\t\t/* opaque pointer available for LLD use */\n\tchar *buffer;\t\t/* kaddr of the current segment if available */\n\n\tint tag;\n\tint errors;\n\n\t/*\n\t * when request is used as a packet command carrier\n\t */\n\tunsigned char __cmd[BLK_MAX_CDB];\n\tunsigned char *cmd;\n\tunsigned short cmd_len;\n\n\tunsigned int extra_len;\t/* length of alignment and padding */\n\tunsigned int sense_len;\n\tunsigned int resid_len;\t/* residual count */\n\tvoid *sense;\n\n\tunsigned long deadline;\n\tstruct list_head timeout_list;\n\tunsigned int timeout;\n\tint retries;\n\n\t/*\n\t * completion callback.\n\t */\n\trq_end_io_fn *end_io;\n\tvoid *end_io_data;\n\n\t/* for bidi */\n\tstruct request *next_rq;\n};\n\nstatic inline unsigned short req_get_ioprio(struct request *req)\n{\n\treturn req->ioprio;\n}\n\n/*\n * State information carried for REQ_TYPE_PM_SUSPEND and REQ_TYPE_PM_RESUME\n * requests. Some step values could eventually be made generic.\n */\nstruct request_pm_state\n{\n\t/* PM state machine step value, currently driver specific */\n\tint\tpm_step;\n\t/* requested PM state value (S1, S2, S3, S4, ...) */\n\tu32\tpm_state;\n\tvoid*\tdata;\t\t/* for driver use */\n};\n\n#include <linux/elevator.h>\n\ntypedef void (request_fn_proc) (struct request_queue *q);\ntypedef void (make_request_fn) (struct request_queue *q, struct bio *bio);\ntypedef int (prep_rq_fn) (struct request_queue *, struct request *);\ntypedef void (unprep_rq_fn) (struct request_queue *, struct request *);\n\nstruct bio_vec;\nstruct bvec_merge_data {\n\tstruct block_device *bi_bdev;\n\tsector_t bi_sector;\n\tunsigned bi_size;\n\tunsigned long bi_rw;\n};\ntypedef int (merge_bvec_fn) (struct request_queue *, struct bvec_merge_data *,\n\t\t\t     struct bio_vec *);\ntypedef void (softirq_done_fn)(struct request *);\ntypedef int (dma_drain_needed_fn)(struct request *);\ntypedef int (lld_busy_fn) (struct request_queue *q);\ntypedef int (bsg_job_fn) (struct bsg_job *);\n\nenum blk_eh_timer_return {\n\tBLK_EH_NOT_HANDLED,\n\tBLK_EH_HANDLED,\n\tBLK_EH_RESET_TIMER,\n};\n\ntypedef enum blk_eh_timer_return (rq_timed_out_fn)(struct request *);\n\nenum blk_queue_state {\n\tQueue_down,\n\tQueue_up,\n};\n\nstruct blk_queue_tag {\n\tstruct request **tag_index;\t/* map of busy tags */\n\tunsigned long *tag_map;\t\t/* bit map of free/busy tags */\n\tint busy;\t\t\t/* current depth */\n\tint max_depth;\t\t\t/* what we will send to device */\n\tint real_max_depth;\t\t/* what the array can hold */\n\tatomic_t refcnt;\t\t/* map can be shared */\n};\n\n#define BLK_SCSI_MAX_CMDS\t(256)\n#define BLK_SCSI_CMD_PER_LONG\t(BLK_SCSI_MAX_CMDS / (sizeof(long) * 8))\n\nstruct queue_limits {\n\tunsigned long\t\tbounce_pfn;\n\tunsigned long\t\tseg_boundary_mask;\n\n\tunsigned int\t\tmax_hw_sectors;\n\tunsigned int\t\tmax_sectors;\n\tunsigned int\t\tmax_segment_size;\n\tunsigned int\t\tphysical_block_size;\n\tunsigned int\t\talignment_offset;\n\tunsigned int\t\tio_min;\n\tunsigned int\t\tio_opt;\n\tunsigned int\t\tmax_discard_sectors;\n\tunsigned int\t\tdiscard_granularity;\n\tunsigned int\t\tdiscard_alignment;\n\n\tunsigned short\t\tlogical_block_size;\n\tunsigned short\t\tmax_segments;\n\tunsigned short\t\tmax_integrity_segments;\n\n\tunsigned char\t\tmisaligned;\n\tunsigned char\t\tdiscard_misaligned;\n\tunsigned char\t\tcluster;\n\tunsigned char\t\tdiscard_zeroes_data;\n};\n\nstruct request_queue {\n\t/*\n\t * Together with queue_head for cacheline sharing\n\t */\n\tstruct list_head\tqueue_head;\n\tstruct request\t\t*last_merge;\n\tstruct elevator_queue\t*elevator;\n\n\t/*\n\t * the queue request freelist, one for reads and one for writes\n\t */\n\tstruct request_list\trq;\n\n\trequest_fn_proc\t\t*request_fn;\n\tmake_request_fn\t\t*make_request_fn;\n\tprep_rq_fn\t\t*prep_rq_fn;\n\tunprep_rq_fn\t\t*unprep_rq_fn;\n\tmerge_bvec_fn\t\t*merge_bvec_fn;\n\tsoftirq_done_fn\t\t*softirq_done_fn;\n\trq_timed_out_fn\t\t*rq_timed_out_fn;\n\tdma_drain_needed_fn\t*dma_drain_needed;\n\tlld_busy_fn\t\t*lld_busy_fn;\n\n\t/*\n\t * Dispatch queue sorting\n\t */\n\tsector_t\t\tend_sector;\n\tstruct request\t\t*boundary_rq;\n\n\t/*\n\t * Delayed queue handling\n\t */\n\tstruct delayed_work\tdelay_work;\n\n\tstruct backing_dev_info\tbacking_dev_info;\n\n\t/*\n\t * The queue owner gets to use this for whatever they like.\n\t * ll_rw_blk doesn't touch it.\n\t */\n\tvoid\t\t\t*queuedata;\n\n\t/*\n\t * various queue flags, see QUEUE_* below\n\t */\n\tunsigned long\t\tqueue_flags;\n\n\t/*\n\t * queue needs bounce pages for pages above this limit\n\t */\n\tgfp_t\t\t\tbounce_gfp;\n\n\t/*\n\t * protects queue structures from reentrancy. ->__queue_lock should\n\t * _never_ be used directly, it is queue private. always use\n\t * ->queue_lock.\n\t */\n\tspinlock_t\t\t__queue_lock;\n\tspinlock_t\t\t*queue_lock;\n\n\t/*\n\t * queue kobject\n\t */\n\tstruct kobject kobj;\n\n\t/*\n\t * queue settings\n\t */\n\tunsigned long\t\tnr_requests;\t/* Max # of requests */\n\tunsigned int\t\tnr_congestion_on;\n\tunsigned int\t\tnr_congestion_off;\n\tunsigned int\t\tnr_batching;\n\n\tunsigned int\t\tdma_drain_size;\n\tvoid\t\t\t*dma_drain_buffer;\n\tunsigned int\t\tdma_pad_mask;\n\tunsigned int\t\tdma_alignment;\n\n\tstruct blk_queue_tag\t*queue_tags;\n\tstruct list_head\ttag_busy_list;\n\n\tunsigned int\t\tnr_sorted;\n\tunsigned int\t\tin_flight[2];\n\n\tunsigned int\t\trq_timeout;\n\tstruct timer_list\ttimeout;\n\tstruct list_head\ttimeout_list;\n\n\tstruct queue_limits\tlimits;\n\n\t/*\n\t * sg stuff\n\t */\n\tunsigned int\t\tsg_timeout;\n\tunsigned int\t\tsg_reserved_size;\n\tint\t\t\tnode;\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tstruct blk_trace\t*blk_trace;\n#endif\n\t/*\n\t * for flush operations\n\t */\n\tunsigned int\t\tflush_flags;\n\tunsigned int\t\tflush_not_queueable:1;\n\tunsigned int\t\tflush_queue_delayed:1;\n\tunsigned int\t\tflush_pending_idx:1;\n\tunsigned int\t\tflush_running_idx:1;\n\tunsigned long\t\tflush_pending_since;\n\tstruct list_head\tflush_queue[2];\n\tstruct list_head\tflush_data_in_flight;\n\tstruct request\t\tflush_rq;\n\n\tstruct mutex\t\tsysfs_lock;\n\n#if defined(CONFIG_BLK_DEV_BSG)\n\tbsg_job_fn\t\t*bsg_job_fn;\n\tint\t\t\tbsg_job_size;\n\tstruct bsg_class_device bsg_dev;\n#endif\n\n#ifdef CONFIG_BLK_DEV_THROTTLING\n\t/* Throttle data */\n\tstruct throtl_data *td;\n#endif\n};\n\n#define QUEUE_FLAG_QUEUED\t1\t/* uses generic tag queueing */\n#define QUEUE_FLAG_STOPPED\t2\t/* queue is stopped */\n#define\tQUEUE_FLAG_SYNCFULL\t3\t/* read queue has been filled */\n#define QUEUE_FLAG_ASYNCFULL\t4\t/* write queue has been filled */\n#define QUEUE_FLAG_DEAD\t\t5\t/* queue being torn down */\n#define QUEUE_FLAG_ELVSWITCH\t6\t/* don't use elevator, just do FIFO */\n#define QUEUE_FLAG_BIDI\t\t7\t/* queue supports bidi requests */\n#define QUEUE_FLAG_NOMERGES     8\t/* disable merge attempts */\n#define QUEUE_FLAG_SAME_COMP\t9\t/* complete on same CPU-group */\n#define QUEUE_FLAG_FAIL_IO     10\t/* fake timeout */\n#define QUEUE_FLAG_STACKABLE   11\t/* supports request stacking */\n#define QUEUE_FLAG_NONROT      12\t/* non-rotational device (SSD) */\n#define QUEUE_FLAG_VIRT        QUEUE_FLAG_NONROT /* paravirt device */\n#define QUEUE_FLAG_IO_STAT     13\t/* do IO stats */\n#define QUEUE_FLAG_DISCARD     14\t/* supports DISCARD */\n#define QUEUE_FLAG_NOXMERGES   15\t/* No extended merges */\n#define QUEUE_FLAG_ADD_RANDOM  16\t/* Contributes to random pool */\n#define QUEUE_FLAG_SECDISCARD  17\t/* supports SECDISCARD */\n#define QUEUE_FLAG_SAME_FORCE  18\t/* force complete on same CPU */\n\n#define QUEUE_FLAG_DEFAULT\t((1 << QUEUE_FLAG_IO_STAT) |\t\t\\\n\t\t\t\t (1 << QUEUE_FLAG_STACKABLE)\t|\t\\\n\t\t\t\t (1 << QUEUE_FLAG_SAME_COMP)\t|\t\\\n\t\t\t\t (1 << QUEUE_FLAG_ADD_RANDOM))\n\nstatic inline int queue_is_locked(struct request_queue *q)\n{\n#ifdef CONFIG_SMP\n\tspinlock_t *lock = q->queue_lock;\n\treturn lock && spin_is_locked(lock);\n#else\n\treturn 1;\n#endif\n}\n\nstatic inline void queue_flag_set_unlocked(unsigned int flag,\n\t\t\t\t\t   struct request_queue *q)\n{\n\t__set_bit(flag, &q->queue_flags);\n}\n\nstatic inline int queue_flag_test_and_clear(unsigned int flag,\n\t\t\t\t\t    struct request_queue *q)\n{\n\tWARN_ON_ONCE(!queue_is_locked(q));\n\n\tif (test_bit(flag, &q->queue_flags)) {\n\t\t__clear_bit(flag, &q->queue_flags);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline int queue_flag_test_and_set(unsigned int flag,\n\t\t\t\t\t  struct request_queue *q)\n{\n\tWARN_ON_ONCE(!queue_is_locked(q));\n\n\tif (!test_bit(flag, &q->queue_flags)) {\n\t\t__set_bit(flag, &q->queue_flags);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic inline void queue_flag_set(unsigned int flag, struct request_queue *q)\n{\n\tWARN_ON_ONCE(!queue_is_locked(q));\n\t__set_bit(flag, &q->queue_flags);\n}\n\nstatic inline void queue_flag_clear_unlocked(unsigned int flag,\n\t\t\t\t\t     struct request_queue *q)\n{\n\t__clear_bit(flag, &q->queue_flags);\n}\n\nstatic inline int queue_in_flight(struct request_queue *q)\n{\n\treturn q->in_flight[0] + q->in_flight[1];\n}\n\nstatic inline void queue_flag_clear(unsigned int flag, struct request_queue *q)\n{\n\tWARN_ON_ONCE(!queue_is_locked(q));\n\t__clear_bit(flag, &q->queue_flags);\n}\n\n#define blk_queue_tagged(q)\ttest_bit(QUEUE_FLAG_QUEUED, &(q)->queue_flags)\n#define blk_queue_stopped(q)\ttest_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)\n#define blk_queue_nomerges(q)\ttest_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)\n#define blk_queue_noxmerges(q)\t\\\n\ttest_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)\n#define blk_queue_nonrot(q)\ttest_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)\n#define blk_queue_io_stat(q)\ttest_bit(QUEUE_FLAG_IO_STAT, &(q)->queue_flags)\n#define blk_queue_add_random(q)\ttest_bit(QUEUE_FLAG_ADD_RANDOM, &(q)->queue_flags)\n#define blk_queue_stackable(q)\t\\\n\ttest_bit(QUEUE_FLAG_STACKABLE, &(q)->queue_flags)\n#define blk_queue_discard(q)\ttest_bit(QUEUE_FLAG_DISCARD, &(q)->queue_flags)\n#define blk_queue_secdiscard(q)\t(blk_queue_discard(q) && \\\n\ttest_bit(QUEUE_FLAG_SECDISCARD, &(q)->queue_flags))\n\n#define blk_noretry_request(rq) \\\n\t((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \\\n\t\t\t     REQ_FAILFAST_DRIVER))\n\n#define blk_account_rq(rq) \\\n\t(((rq)->cmd_flags & REQ_STARTED) && \\\n\t ((rq)->cmd_type == REQ_TYPE_FS || \\\n\t  ((rq)->cmd_flags & REQ_DISCARD)))\n\n#define blk_pm_request(rq)\t\\\n\t((rq)->cmd_type == REQ_TYPE_PM_SUSPEND || \\\n\t (rq)->cmd_type == REQ_TYPE_PM_RESUME)\n\n#define blk_rq_cpu_valid(rq)\t((rq)->cpu != -1)\n#define blk_bidi_rq(rq)\t\t((rq)->next_rq != NULL)\n/* rq->queuelist of dequeued request must be list_empty() */\n#define blk_queued_rq(rq)\t(!list_empty(&(rq)->queuelist))\n\n#define list_entry_rq(ptr)\tlist_entry((ptr), struct request, queuelist)\n\n#define rq_data_dir(rq)\t\t((rq)->cmd_flags & 1)\n\nstatic inline unsigned int blk_queue_cluster(struct request_queue *q)\n{\n\treturn q->limits.cluster;\n}\n\n/*\n * We regard a request as sync, if either a read or a sync write\n */\nstatic inline bool rw_is_sync(unsigned int rw_flags)\n{\n\treturn !(rw_flags & REQ_WRITE) || (rw_flags & REQ_SYNC);\n}\n\nstatic inline bool rq_is_sync(struct request *rq)\n{\n\treturn rw_is_sync(rq->cmd_flags);\n}\n\nstatic inline int blk_queue_full(struct request_queue *q, int sync)\n{\n\tif (sync)\n\t\treturn test_bit(QUEUE_FLAG_SYNCFULL, &q->queue_flags);\n\treturn test_bit(QUEUE_FLAG_ASYNCFULL, &q->queue_flags);\n}\n\nstatic inline void blk_set_queue_full(struct request_queue *q, int sync)\n{\n\tif (sync)\n\t\tqueue_flag_set(QUEUE_FLAG_SYNCFULL, q);\n\telse\n\t\tqueue_flag_set(QUEUE_FLAG_ASYNCFULL, q);\n}\n\nstatic inline void blk_clear_queue_full(struct request_queue *q, int sync)\n{\n\tif (sync)\n\t\tqueue_flag_clear(QUEUE_FLAG_SYNCFULL, q);\n\telse\n\t\tqueue_flag_clear(QUEUE_FLAG_ASYNCFULL, q);\n}\n\n\n/*\n * mergeable request must not have _NOMERGE or _BARRIER bit set, nor may\n * it already be started by driver.\n */\n#define RQ_NOMERGE_FLAGS\t\\\n\t(REQ_NOMERGE | REQ_STARTED | REQ_SOFTBARRIER | REQ_FLUSH | REQ_FUA)\n#define rq_mergeable(rq)\t\\\n\t(!((rq)->cmd_flags & RQ_NOMERGE_FLAGS) && \\\n\t (((rq)->cmd_flags & REQ_DISCARD) || \\\n\t  (rq)->cmd_type == REQ_TYPE_FS))\n\n/*\n * q->prep_rq_fn return values\n */\n#define BLKPREP_OK\t\t0\t/* serve it */\n#define BLKPREP_KILL\t\t1\t/* fatal error, kill */\n#define BLKPREP_DEFER\t\t2\t/* leave on queue */\n\nextern unsigned long blk_max_low_pfn, blk_max_pfn;\n\n/*\n * standard bounce addresses:\n *\n * BLK_BOUNCE_HIGH\t: bounce all highmem pages\n * BLK_BOUNCE_ANY\t: don't bounce anything\n * BLK_BOUNCE_ISA\t: bounce pages above ISA DMA boundary\n */\n\n#if BITS_PER_LONG == 32\n#define BLK_BOUNCE_HIGH\t\t((u64)blk_max_low_pfn << PAGE_SHIFT)\n#else\n#define BLK_BOUNCE_HIGH\t\t-1ULL\n#endif\n#define BLK_BOUNCE_ANY\t\t(-1ULL)\n#define BLK_BOUNCE_ISA\t\t(DMA_BIT_MASK(24))\n\n/*\n * default timeout for SG_IO if none specified\n */\n#define BLK_DEFAULT_SG_TIMEOUT\t(60 * HZ)\n#define BLK_MIN_SG_TIMEOUT\t(7 * HZ)\n\n#ifdef CONFIG_BOUNCE\nextern int init_emergency_isa_pool(void);\nextern void blk_queue_bounce(struct request_queue *q, struct bio **bio);\n#else\nstatic inline int init_emergency_isa_pool(void)\n{\n\treturn 0;\n}\nstatic inline void blk_queue_bounce(struct request_queue *q, struct bio **bio)\n{\n}\n#endif /* CONFIG_MMU */\n\nstruct rq_map_data {\n\tstruct page **pages;\n\tint page_order;\n\tint nr_entries;\n\tunsigned long offset;\n\tint null_mapped;\n\tint from_user;\n};\n\nstruct req_iterator {\n\tint i;\n\tstruct bio *bio;\n};\n\n/* This should not be used directly - use rq_for_each_segment */\n#define for_each_bio(_bio)\t\t\\\n\tfor (; _bio; _bio = _bio->bi_next)\n#define __rq_for_each_bio(_bio, rq)\t\\\n\tif ((rq->bio))\t\t\t\\\n\t\tfor (_bio = (rq)->bio; _bio; _bio = _bio->bi_next)\n\n#define rq_for_each_segment(bvl, _rq, _iter)\t\t\t\\\n\t__rq_for_each_bio(_iter.bio, _rq)\t\t\t\\\n\t\tbio_for_each_segment(bvl, _iter.bio, _iter.i)\n\n#define rq_iter_last(rq, _iter)\t\t\t\t\t\\\n\t\t(_iter.bio->bi_next == NULL && _iter.i == _iter.bio->bi_vcnt-1)\n\n#ifndef ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE\n# error\t\"You should define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE for your platform\"\n#endif\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE\nextern void rq_flush_dcache_pages(struct request *rq);\n#else\nstatic inline void rq_flush_dcache_pages(struct request *rq)\n{\n}\n#endif\n\nextern int blk_register_queue(struct gendisk *disk);\nextern void blk_unregister_queue(struct gendisk *disk);\nextern void generic_make_request(struct bio *bio);\nextern void blk_rq_init(struct request_queue *q, struct request *rq);\nextern void blk_put_request(struct request *);\nextern void __blk_put_request(struct request_queue *, struct request *);\nextern struct request *blk_get_request(struct request_queue *, int, gfp_t);\nextern struct request *blk_make_request(struct request_queue *, struct bio *,\n\t\t\t\t\tgfp_t);\nextern void blk_insert_request(struct request_queue *, struct request *, int, void *);\nextern void blk_requeue_request(struct request_queue *, struct request *);\nextern void blk_add_request_payload(struct request *rq, struct page *page,\n\t\tunsigned int len);\nextern int blk_rq_check_limits(struct request_queue *q, struct request *rq);\nextern int blk_lld_busy(struct request_queue *q);\nextern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,\n\t\t\t     struct bio_set *bs, gfp_t gfp_mask,\n\t\t\t     int (*bio_ctr)(struct bio *, struct bio *, void *),\n\t\t\t     void *data);\nextern void blk_rq_unprep_clone(struct request *rq);\nextern int blk_insert_cloned_request(struct request_queue *q,\n\t\t\t\t     struct request *rq);\nextern void blk_delay_queue(struct request_queue *, unsigned long);\nextern void blk_recount_segments(struct request_queue *, struct bio *);\nextern int scsi_verify_blk_ioctl(struct block_device *, unsigned int);\nextern int scsi_cmd_blk_ioctl(struct block_device *, fmode_t,\n\t\t\t      unsigned int, void __user *);\nextern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,\n\t\t\t  unsigned int, void __user *);\nextern int sg_scsi_ioctl(struct request_queue *, struct gendisk *, fmode_t,\n\t\t\t struct scsi_ioctl_command __user *);\n\nextern void blk_queue_bio(struct request_queue *q, struct bio *bio);\n\n/*\n * A queue has just exitted congestion.  Note this in the global counter of\n * congested queues, and wake up anyone who was waiting for requests to be\n * put back.\n */\nstatic inline void blk_clear_queue_congested(struct request_queue *q, int sync)\n{\n\tclear_bdi_congested(&q->backing_dev_info, sync);\n}\n\n/*\n * A queue has just entered congestion.  Flag that in the queue's VM-visible\n * state flags and increment the global gounter of congested queues.\n */\nstatic inline void blk_set_queue_congested(struct request_queue *q, int sync)\n{\n\tset_bdi_congested(&q->backing_dev_info, sync);\n}\n\nextern void blk_start_queue(struct request_queue *q);\nextern void blk_stop_queue(struct request_queue *q);\nextern void blk_sync_queue(struct request_queue *q);\nextern void __blk_stop_queue(struct request_queue *q);\nextern void __blk_run_queue(struct request_queue *q);\nextern void blk_run_queue(struct request_queue *);\nextern void blk_run_queue_async(struct request_queue *q);\nextern int blk_rq_map_user(struct request_queue *, struct request *,\n\t\t\t   struct rq_map_data *, void __user *, unsigned long,\n\t\t\t   gfp_t);\nextern int blk_rq_unmap_user(struct bio *);\nextern int blk_rq_map_kern(struct request_queue *, struct request *, void *, unsigned int, gfp_t);\nextern int blk_rq_map_user_iov(struct request_queue *, struct request *,\n\t\t\t       struct rq_map_data *, struct sg_iovec *, int,\n\t\t\t       unsigned int, gfp_t);\nextern int blk_execute_rq(struct request_queue *, struct gendisk *,\n\t\t\t  struct request *, int);\nextern void blk_execute_rq_nowait(struct request_queue *, struct gendisk *,\n\t\t\t\t  struct request *, int, rq_end_io_fn *);\n\nstatic inline struct request_queue *bdev_get_queue(struct block_device *bdev)\n{\n\treturn bdev->bd_disk->queue;\n}\n\n/*\n * blk_rq_pos()\t\t\t: the current sector\n * blk_rq_bytes()\t\t: bytes left in the entire request\n * blk_rq_cur_bytes()\t\t: bytes left in the current segment\n * blk_rq_err_bytes()\t\t: bytes left till the next error boundary\n * blk_rq_sectors()\t\t: sectors left in the entire request\n * blk_rq_cur_sectors()\t\t: sectors left in the current segment\n */\nstatic inline sector_t blk_rq_pos(const struct request *rq)\n{\n\treturn rq->__sector;\n}\n\nstatic inline unsigned int blk_rq_bytes(const struct request *rq)\n{\n\treturn rq->__data_len;\n}\n\nstatic inline int blk_rq_cur_bytes(const struct request *rq)\n{\n\treturn rq->bio ? bio_cur_bytes(rq->bio) : 0;\n}\n\nextern unsigned int blk_rq_err_bytes(const struct request *rq);\n\nstatic inline unsigned int blk_rq_sectors(const struct request *rq)\n{\n\treturn blk_rq_bytes(rq) >> 9;\n}\n\nstatic inline unsigned int blk_rq_cur_sectors(const struct request *rq)\n{\n\treturn blk_rq_cur_bytes(rq) >> 9;\n}\n\n/*\n * Request issue related functions.\n */\nextern struct request *blk_peek_request(struct request_queue *q);\nextern void blk_start_request(struct request *rq);\nextern struct request *blk_fetch_request(struct request_queue *q);\n\n/*\n * Request completion related functions.\n *\n * blk_update_request() completes given number of bytes and updates\n * the request without completing it.\n *\n * blk_end_request() and friends.  __blk_end_request() must be called\n * with the request queue spinlock acquired.\n *\n * Several drivers define their own end_request and call\n * blk_end_request() for parts of the original function.\n * This prevents code duplication in drivers.\n */\nextern bool blk_update_request(struct request *rq, int error,\n\t\t\t       unsigned int nr_bytes);\nextern bool blk_end_request(struct request *rq, int error,\n\t\t\t    unsigned int nr_bytes);\nextern void blk_end_request_all(struct request *rq, int error);\nextern bool blk_end_request_cur(struct request *rq, int error);\nextern bool blk_end_request_err(struct request *rq, int error);\nextern bool __blk_end_request(struct request *rq, int error,\n\t\t\t      unsigned int nr_bytes);\nextern void __blk_end_request_all(struct request *rq, int error);\nextern bool __blk_end_request_cur(struct request *rq, int error);\nextern bool __blk_end_request_err(struct request *rq, int error);\n\nextern void blk_complete_request(struct request *);\nextern void __blk_complete_request(struct request *);\nextern void blk_abort_request(struct request *);\nextern void blk_abort_queue(struct request_queue *);\nextern void blk_unprep_request(struct request *);\n\n/*\n * Access functions for manipulating queue properties\n */\nextern struct request_queue *blk_init_queue_node(request_fn_proc *rfn,\n\t\t\t\t\tspinlock_t *lock, int node_id);\nextern struct request_queue *blk_init_queue(request_fn_proc *, spinlock_t *);\nextern struct request_queue *blk_init_allocated_queue(struct request_queue *,\n\t\t\t\t\t\t      request_fn_proc *, spinlock_t *);\nextern void blk_cleanup_queue(struct request_queue *);\nextern void blk_queue_make_request(struct request_queue *, make_request_fn *);\nextern void blk_queue_bounce_limit(struct request_queue *, u64);\nextern void blk_limits_max_hw_sectors(struct queue_limits *, unsigned int);\nextern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);\nextern void blk_queue_max_segments(struct request_queue *, unsigned short);\nextern void blk_queue_max_segment_size(struct request_queue *, unsigned int);\nextern void blk_queue_max_discard_sectors(struct request_queue *q,\n\t\tunsigned int max_discard_sectors);\nextern void blk_queue_logical_block_size(struct request_queue *, unsigned short);\nextern void blk_queue_physical_block_size(struct request_queue *, unsigned int);\nextern void blk_queue_alignment_offset(struct request_queue *q,\n\t\t\t\t       unsigned int alignment);\nextern void blk_limits_io_min(struct queue_limits *limits, unsigned int min);\nextern void blk_queue_io_min(struct request_queue *q, unsigned int min);\nextern void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt);\nextern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);\nextern void blk_set_default_limits(struct queue_limits *lim);\nextern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,\n\t\t\t    sector_t offset);\nextern int bdev_stack_limits(struct queue_limits *t, struct block_device *bdev,\n\t\t\t    sector_t offset);\nextern void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,\n\t\t\t      sector_t offset);\nextern void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b);\nextern void blk_queue_dma_pad(struct request_queue *, unsigned int);\nextern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);\nextern int blk_queue_dma_drain(struct request_queue *q,\n\t\t\t       dma_drain_needed_fn *dma_drain_needed,\n\t\t\t       void *buf, unsigned int size);\nextern void blk_queue_lld_busy(struct request_queue *q, lld_busy_fn *fn);\nextern void blk_queue_segment_boundary(struct request_queue *, unsigned long);\nextern void blk_queue_prep_rq(struct request_queue *, prep_rq_fn *pfn);\nextern void blk_queue_unprep_rq(struct request_queue *, unprep_rq_fn *ufn);\nextern void blk_queue_merge_bvec(struct request_queue *, merge_bvec_fn *);\nextern void blk_queue_dma_alignment(struct request_queue *, int);\nextern void blk_queue_update_dma_alignment(struct request_queue *, int);\nextern void blk_queue_softirq_done(struct request_queue *, softirq_done_fn *);\nextern void blk_queue_rq_timed_out(struct request_queue *, rq_timed_out_fn *);\nextern void blk_queue_rq_timeout(struct request_queue *, unsigned int);\nextern void blk_queue_flush(struct request_queue *q, unsigned int flush);\nextern void blk_queue_flush_queueable(struct request_queue *q, bool queueable);\nextern struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev);\n\nextern int blk_rq_map_sg(struct request_queue *, struct request *, struct scatterlist *);\nextern void blk_dump_rq_flags(struct request *, char *);\nextern long nr_blockdev_pages(void);\n\nint blk_get_queue(struct request_queue *);\nstruct request_queue *blk_alloc_queue(gfp_t);\nstruct request_queue *blk_alloc_queue_node(gfp_t, int);\nextern void blk_put_queue(struct request_queue *);\n\n/*\n * blk_plug permits building a queue of related requests by holding the I/O\n * fragments for a short period. This allows merging of sequential requests\n * into single larger request. As the requests are moved from a per-task list to\n * the device's request_queue in a batch, this results in improved scalability\n * as the lock contention for request_queue lock is reduced.\n *\n * It is ok not to disable preemption when adding the request to the plug list\n * or when attempting a merge, because blk_schedule_flush_list() will only flush\n * the plug list when the task sleeps by itself. For details, please see\n * schedule() where blk_schedule_flush_plug() is called.\n */\nstruct blk_plug {\n\tunsigned long magic; /* detect uninitialized use-cases */\n\tstruct list_head list; /* requests */\n\tstruct list_head cb_list; /* md requires an unplug callback */\n\tunsigned int should_sort; /* list to be sorted before flushing? */\n};\n#define BLK_MAX_REQUEST_COUNT 16\n\nstruct blk_plug_cb {\n\tstruct list_head list;\n\tvoid (*callback)(struct blk_plug_cb *);\n};\n\nextern void blk_start_plug(struct blk_plug *);\nextern void blk_finish_plug(struct blk_plug *);\nextern void blk_flush_plug_list(struct blk_plug *, bool);\n\nstatic inline void blk_flush_plug(struct task_struct *tsk)\n{\n\tstruct blk_plug *plug = tsk->plug;\n\n\tif (plug)\n\t\tblk_flush_plug_list(plug, false);\n}\n\nstatic inline void blk_schedule_flush_plug(struct task_struct *tsk)\n{\n\tstruct blk_plug *plug = tsk->plug;\n\n\tif (plug)\n\t\tblk_flush_plug_list(plug, true);\n}\n\nstatic inline bool blk_needs_flush_plug(struct task_struct *tsk)\n{\n\tstruct blk_plug *plug = tsk->plug;\n\n\treturn plug && (!list_empty(&plug->list) || !list_empty(&plug->cb_list));\n}\n\n/*\n * tag stuff\n */\n#define blk_rq_tagged(rq)\t\t((rq)->cmd_flags & REQ_QUEUED)\nextern int blk_queue_start_tag(struct request_queue *, struct request *);\nextern struct request *blk_queue_find_tag(struct request_queue *, int);\nextern void blk_queue_end_tag(struct request_queue *, struct request *);\nextern int blk_queue_init_tags(struct request_queue *, int, struct blk_queue_tag *);\nextern void blk_queue_free_tags(struct request_queue *);\nextern int blk_queue_resize_tags(struct request_queue *, int);\nextern void blk_queue_invalidate_tags(struct request_queue *);\nextern struct blk_queue_tag *blk_init_tags(int);\nextern void blk_free_tags(struct blk_queue_tag *);\n\nstatic inline struct request *blk_map_queue_find_tag(struct blk_queue_tag *bqt,\n\t\t\t\t\t\tint tag)\n{\n\tif (unlikely(bqt == NULL || tag >= bqt->real_max_depth))\n\t\treturn NULL;\n\treturn bqt->tag_index[tag];\n}\n\n#define BLKDEV_DISCARD_SECURE  0x01    /* secure discard */\n\nextern int blkdev_issue_flush(struct block_device *, gfp_t, sector_t *);\nextern int blkdev_issue_discard(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp_mask, unsigned long flags);\nextern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,\n\t\t\tsector_t nr_sects, gfp_t gfp_mask);\nstatic inline int sb_issue_discard(struct super_block *sb, sector_t block,\n\t\tsector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)\n{\n\treturn blkdev_issue_discard(sb->s_bdev, block << (sb->s_blocksize_bits - 9),\n\t\t\t\t    nr_blocks << (sb->s_blocksize_bits - 9),\n\t\t\t\t    gfp_mask, flags);\n}\nstatic inline int sb_issue_zeroout(struct super_block *sb, sector_t block,\n\t\tsector_t nr_blocks, gfp_t gfp_mask)\n{\n\treturn blkdev_issue_zeroout(sb->s_bdev,\n\t\t\t\t    block << (sb->s_blocksize_bits - 9),\n\t\t\t\t    nr_blocks << (sb->s_blocksize_bits - 9),\n\t\t\t\t    gfp_mask);\n}\n\nextern int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm);\n\nenum blk_default_limits {\n\tBLK_MAX_SEGMENTS\t= 128,\n\tBLK_SAFE_MAX_SECTORS\t= 255,\n\tBLK_DEF_MAX_SECTORS\t= 1024,\n\tBLK_MAX_SEGMENT_SIZE\t= 65536,\n\tBLK_SEG_BOUNDARY_MASK\t= 0xFFFFFFFFUL,\n};\n\n#define blkdev_entry_to_request(entry) list_entry((entry), struct request, queuelist)\n\nstatic inline unsigned long queue_bounce_pfn(struct request_queue *q)\n{\n\treturn q->limits.bounce_pfn;\n}\n\nstatic inline unsigned long queue_segment_boundary(struct request_queue *q)\n{\n\treturn q->limits.seg_boundary_mask;\n}\n\nstatic inline unsigned int queue_max_sectors(struct request_queue *q)\n{\n\treturn q->limits.max_sectors;\n}\n\nstatic inline unsigned int queue_max_hw_sectors(struct request_queue *q)\n{\n\treturn q->limits.max_hw_sectors;\n}\n\nstatic inline unsigned short queue_max_segments(struct request_queue *q)\n{\n\treturn q->limits.max_segments;\n}\n\nstatic inline unsigned int queue_max_segment_size(struct request_queue *q)\n{\n\treturn q->limits.max_segment_size;\n}\n\nstatic inline unsigned short queue_logical_block_size(struct request_queue *q)\n{\n\tint retval = 512;\n\n\tif (q && q->limits.logical_block_size)\n\t\tretval = q->limits.logical_block_size;\n\n\treturn retval;\n}\n\nstatic inline unsigned short bdev_logical_block_size(struct block_device *bdev)\n{\n\treturn queue_logical_block_size(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_physical_block_size(struct request_queue *q)\n{\n\treturn q->limits.physical_block_size;\n}\n\nstatic inline unsigned int bdev_physical_block_size(struct block_device *bdev)\n{\n\treturn queue_physical_block_size(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_io_min(struct request_queue *q)\n{\n\treturn q->limits.io_min;\n}\n\nstatic inline int bdev_io_min(struct block_device *bdev)\n{\n\treturn queue_io_min(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_io_opt(struct request_queue *q)\n{\n\treturn q->limits.io_opt;\n}\n\nstatic inline int bdev_io_opt(struct block_device *bdev)\n{\n\treturn queue_io_opt(bdev_get_queue(bdev));\n}\n\nstatic inline int queue_alignment_offset(struct request_queue *q)\n{\n\tif (q->limits.misaligned)\n\t\treturn -1;\n\n\treturn q->limits.alignment_offset;\n}\n\nstatic inline int queue_limit_alignment_offset(struct queue_limits *lim, sector_t sector)\n{\n\tunsigned int granularity = max(lim->physical_block_size, lim->io_min);\n\tunsigned int alignment = (sector << 9) & (granularity - 1);\n\n\treturn (granularity + lim->alignment_offset - alignment)\n\t\t& (granularity - 1);\n}\n\nstatic inline int bdev_alignment_offset(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q->limits.misaligned)\n\t\treturn -1;\n\n\tif (bdev != bdev->bd_contains)\n\t\treturn bdev->bd_part->alignment_offset;\n\n\treturn q->limits.alignment_offset;\n}\n\nstatic inline int queue_discard_alignment(struct request_queue *q)\n{\n\tif (q->limits.discard_misaligned)\n\t\treturn -1;\n\n\treturn q->limits.discard_alignment;\n}\n\nstatic inline int queue_limit_discard_alignment(struct queue_limits *lim, sector_t sector)\n{\n\tunsigned int alignment = (sector << 9) & (lim->discard_granularity - 1);\n\n\tif (!lim->max_discard_sectors)\n\t\treturn 0;\n\n\treturn (lim->discard_granularity + lim->discard_alignment - alignment)\n\t\t& (lim->discard_granularity - 1);\n}\n\nstatic inline unsigned int queue_discard_zeroes_data(struct request_queue *q)\n{\n\tif (q->limits.max_discard_sectors && q->limits.discard_zeroes_data == 1)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic inline unsigned int bdev_discard_zeroes_data(struct block_device *bdev)\n{\n\treturn queue_discard_zeroes_data(bdev_get_queue(bdev));\n}\n\nstatic inline int queue_dma_alignment(struct request_queue *q)\n{\n\treturn q ? q->dma_alignment : 511;\n}\n\nstatic inline int blk_rq_aligned(struct request_queue *q, unsigned long addr,\n\t\t\t\t unsigned int len)\n{\n\tunsigned int alignment = queue_dma_alignment(q) | q->dma_pad_mask;\n\treturn !(addr & alignment) && !(len & alignment);\n}\n\n/* assumes size > 256 */\nstatic inline unsigned int blksize_bits(unsigned int size)\n{\n\tunsigned int bits = 8;\n\tdo {\n\t\tbits++;\n\t\tsize >>= 1;\n\t} while (size > 256);\n\treturn bits;\n}\n\nstatic inline unsigned int block_size(struct block_device *bdev)\n{\n\treturn bdev->bd_block_size;\n}\n\nstatic inline bool queue_flush_queueable(struct request_queue *q)\n{\n\treturn !q->flush_not_queueable;\n}\n\ntypedef struct {struct page *v;} Sector;\n\nunsigned char *read_dev_sector(struct block_device *, sector_t, Sector *);\n\nstatic inline void put_dev_sector(Sector p)\n{\n\tpage_cache_release(p.v);\n}\n\nstruct work_struct;\nint kblockd_schedule_work(struct request_queue *q, struct work_struct *work);\n\n#ifdef CONFIG_BLK_CGROUP\n/*\n * This should not be using sched_clock(). A real patch is in progress\n * to fix this up, until that is in place we need to disable preemption\n * around sched_clock() in this function and set_io_start_time_ns().\n */\nstatic inline void set_start_time_ns(struct request *req)\n{\n\tpreempt_disable();\n\treq->start_time_ns = sched_clock();\n\tpreempt_enable();\n}\n\nstatic inline void set_io_start_time_ns(struct request *req)\n{\n\tpreempt_disable();\n\treq->io_start_time_ns = sched_clock();\n\tpreempt_enable();\n}\n\nstatic inline uint64_t rq_start_time_ns(struct request *req)\n{\n        return req->start_time_ns;\n}\n\nstatic inline uint64_t rq_io_start_time_ns(struct request *req)\n{\n        return req->io_start_time_ns;\n}\n#else\nstatic inline void set_start_time_ns(struct request *req) {}\nstatic inline void set_io_start_time_ns(struct request *req) {}\nstatic inline uint64_t rq_start_time_ns(struct request *req)\n{\n\treturn 0;\n}\nstatic inline uint64_t rq_io_start_time_ns(struct request *req)\n{\n\treturn 0;\n}\n#endif\n\n#define MODULE_ALIAS_BLOCKDEV(major,minor) \\\n\tMODULE_ALIAS(\"block-major-\" __stringify(major) \"-\" __stringify(minor))\n#define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \\\n\tMODULE_ALIAS(\"block-major-\" __stringify(major) \"-*\")\n\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\n#define INTEGRITY_FLAG_READ\t2\t/* verify data integrity on read */\n#define INTEGRITY_FLAG_WRITE\t4\t/* generate data integrity on write */\n\nstruct blk_integrity_exchg {\n\tvoid\t\t\t*prot_buf;\n\tvoid\t\t\t*data_buf;\n\tsector_t\t\tsector;\n\tunsigned int\t\tdata_size;\n\tunsigned short\t\tsector_size;\n\tconst char\t\t*disk_name;\n};\n\ntypedef void (integrity_gen_fn) (struct blk_integrity_exchg *);\ntypedef int (integrity_vrfy_fn) (struct blk_integrity_exchg *);\ntypedef void (integrity_set_tag_fn) (void *, void *, unsigned int);\ntypedef void (integrity_get_tag_fn) (void *, void *, unsigned int);\n\nstruct blk_integrity {\n\tintegrity_gen_fn\t*generate_fn;\n\tintegrity_vrfy_fn\t*verify_fn;\n\tintegrity_set_tag_fn\t*set_tag_fn;\n\tintegrity_get_tag_fn\t*get_tag_fn;\n\n\tunsigned short\t\tflags;\n\tunsigned short\t\ttuple_size;\n\tunsigned short\t\tsector_size;\n\tunsigned short\t\ttag_size;\n\n\tconst char\t\t*name;\n\n\tstruct kobject\t\tkobj;\n};\n\nextern bool blk_integrity_is_initialized(struct gendisk *);\nextern int blk_integrity_register(struct gendisk *, struct blk_integrity *);\nextern void blk_integrity_unregister(struct gendisk *);\nextern int blk_integrity_compare(struct gendisk *, struct gendisk *);\nextern int blk_rq_map_integrity_sg(struct request_queue *, struct bio *,\n\t\t\t\t   struct scatterlist *);\nextern int blk_rq_count_integrity_sg(struct request_queue *, struct bio *);\nextern int blk_integrity_merge_rq(struct request_queue *, struct request *,\n\t\t\t\t  struct request *);\nextern int blk_integrity_merge_bio(struct request_queue *, struct request *,\n\t\t\t\t   struct bio *);\n\nstatic inline\nstruct blk_integrity *bdev_get_integrity(struct block_device *bdev)\n{\n\treturn bdev->bd_disk->integrity;\n}\n\nstatic inline struct blk_integrity *blk_get_integrity(struct gendisk *disk)\n{\n\treturn disk->integrity;\n}\n\nstatic inline int blk_integrity_rq(struct request *rq)\n{\n\tif (rq->bio == NULL)\n\t\treturn 0;\n\n\treturn bio_integrity(rq->bio);\n}\n\nstatic inline void blk_queue_max_integrity_segments(struct request_queue *q,\n\t\t\t\t\t\t    unsigned int segs)\n{\n\tq->limits.max_integrity_segments = segs;\n}\n\nstatic inline unsigned short\nqueue_max_integrity_segments(struct request_queue *q)\n{\n\treturn q->limits.max_integrity_segments;\n}\n\n#else /* CONFIG_BLK_DEV_INTEGRITY */\n\n#define blk_integrity_rq(rq)\t\t\t(0)\n#define blk_rq_count_integrity_sg(a, b)\t\t(0)\n#define blk_rq_map_integrity_sg(a, b, c)\t(0)\n#define bdev_get_integrity(a)\t\t\t(0)\n#define blk_get_integrity(a)\t\t\t(0)\n#define blk_integrity_compare(a, b)\t\t(0)\n#define blk_integrity_register(a, b)\t\t(0)\n#define blk_integrity_unregister(a)\t\tdo { } while (0)\n#define blk_queue_max_integrity_segments(a, b)\tdo { } while (0)\n#define queue_max_integrity_segments(a)\t\t(0)\n#define blk_integrity_merge_rq(a, b, c)\t\t(0)\n#define blk_integrity_merge_bio(a, b, c)\t(0)\n#define blk_integrity_is_initialized(a)\t\t(0)\n\n#endif /* CONFIG_BLK_DEV_INTEGRITY */\n\nstruct block_device_operations {\n\tint (*open) (struct block_device *, fmode_t);\n\tint (*release) (struct gendisk *, fmode_t);\n\tint (*ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);\n\tint (*compat_ioctl) (struct block_device *, fmode_t, unsigned, unsigned long);\n\tint (*direct_access) (struct block_device *, sector_t,\n\t\t\t\t\t\tvoid **, unsigned long *);\n\tunsigned int (*check_events) (struct gendisk *disk,\n\t\t\t\t      unsigned int clearing);\n\t/* ->media_changed() is DEPRECATED, use ->check_events() instead */\n\tint (*media_changed) (struct gendisk *);\n\tvoid (*unlock_native_capacity) (struct gendisk *);\n\tint (*revalidate_disk) (struct gendisk *);\n\tint (*getgeo)(struct block_device *, struct hd_geometry *);\n\t/* this callback is with swap_lock and sometimes page table lock held */\n\tvoid (*swap_slot_free_notify) (struct block_device *, unsigned long);\n\tstruct module *owner;\n};\n\nextern int __blkdev_driver_ioctl(struct block_device *, fmode_t, unsigned int,\n\t\t\t\t unsigned long);\n#else /* CONFIG_BLOCK */\n/*\n * stubs for when the block layer is configured out\n */\n#define buffer_heads_over_limit 0\n\nstatic inline long nr_blockdev_pages(void)\n{\n\treturn 0;\n}\n\nstruct blk_plug {\n};\n\nstatic inline void blk_start_plug(struct blk_plug *plug)\n{\n}\n\nstatic inline void blk_finish_plug(struct blk_plug *plug)\n{\n}\n\nstatic inline void blk_flush_plug(struct task_struct *task)\n{\n}\n\nstatic inline void blk_schedule_flush_plug(struct task_struct *task)\n{\n}\n\n\nstatic inline bool blk_needs_flush_plug(struct task_struct *tsk)\n{\n\treturn false;\n}\n\n#endif /* CONFIG_BLOCK */\n\n#endif\n"], "filenames": ["block/scsi_ioctl.c", "drivers/scsi/sd.c", "include/linux/blkdev.h"], "buggy_code_start_loc": [26, 1077, 677], "buggy_code_end_loc": [695, 1283, 677], "fixing_code_start_loc": [27, 1078, 678], "fixing_code_end_loc": [741, 1289, 679], "type": "CWE-264", "message": "The Linux kernel before 3.2.2 does not properly restrict SG_IO ioctl calls, which allows local users to bypass intended restrictions on disk read and write operations by sending a SCSI command to (1) a partition block device or (2) an LVM volume.", "other": {"cve": {"id": "CVE-2011-4127", "sourceIdentifier": "secalert@redhat.com", "published": "2012-07-03T16:40:31.350", "lastModified": "2023-02-13T01:21:31.223", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Linux kernel before 3.2.2 does not properly restrict SG_IO ioctl calls, which allows local users to bypass intended restrictions on disk read and write operations by sending a SCSI command to (1) a partition block device or (2) an LVM volume."}, {"lang": "es", "value": "El kernel de Linux anterior a v3.2.2 no restringe adecuadamente llamadas SG_IO ioctl, permitiendo a usuarios locales eludir restricciones de lectura y escritura en disco  mediante el env\u00edo de un comando SCSI a (1) un dispositivo de bloques de particiones o (2) un volumen LVM."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-264"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:suse:linux_enterprise_server:10:sp4:*:*:*:*:*:*", "matchCriteriaId": "A53FF936-C785-4CEF-BAD0-3C3EB90EE466"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.2.1", "matchCriteriaId": "E239663C-BBE3-4762-9FEB-9034F1666235"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "D30AEC07-3CBD-4F4F-9646-BEAA1D98750B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C2AA8E68-691B-499C-AEDD-3C0BFFE70044"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc3:*:*:*:*:*:*", "matchCriteriaId": "9440475B-5960-4066-A204-F30AAFC87846"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc4:*:*:*:*:*:*", "matchCriteriaId": "53BCFBFB-6AF0-4525-8623-7633CC5E17DB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc5:*:*:*:*:*:*", "matchCriteriaId": "6ED4E86A-74F0-436A-BEB4-3F4EE93A5421"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc6:*:*:*:*:*:*", "matchCriteriaId": "BF0365B0-8E16-4F30-BD92-5DD538CC8135"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0:rc7:*:*:*:*:*:*", "matchCriteriaId": "079505E8-2942-4C33-93D1-35ADA4C39E72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.1:*:*:*:*:*:*:*", "matchCriteriaId": "38989541-2360-4E0A-AE5A-3D6144AA6114"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.2:*:*:*:*:*:*:*", "matchCriteriaId": "4E51646B-7A0E-40F3-B8C9-239C1DA81DD1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.3:*:*:*:*:*:*:*", "matchCriteriaId": "42A8A507-F8E2-491C-A144-B2448A1DB26E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.4:*:*:*:*:*:*:*", "matchCriteriaId": "901FC6F3-2C2A-4112-AE27-AB102BBE8DEE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.5:*:*:*:*:*:*:*", "matchCriteriaId": "203AD334-DB9F-41B0-A4D1-A6C158EF8C40"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.6:*:*:*:*:*:*:*", "matchCriteriaId": "B3611753-E440-410F-8250-600C996A4B8E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.7:*:*:*:*:*:*:*", "matchCriteriaId": "9739BB47-EEAF-42F1-A557-2AE2EA9526A3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.8:*:*:*:*:*:*:*", "matchCriteriaId": "5A95E3BB-0AFC-4C2E-B9BE-C975E902A266"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.9:*:*:*:*:*:*:*", "matchCriteriaId": "482A6C9A-9B8E-4D1C-917A-F16370745E7C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.10:*:*:*:*:*:*:*", "matchCriteriaId": "C6D87357-63E0-41D0-9F02-1BCBF9A77E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.11:*:*:*:*:*:*:*", "matchCriteriaId": "3765A2D6-2D78-4FB1-989E-D5106BFA3F5E"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.12:*:*:*:*:*:*:*", "matchCriteriaId": "F54257DB-7023-43C4-AC4D-9590B815CD92"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.13:*:*:*:*:*:*:*", "matchCriteriaId": "61FF5FCD-A4A1-4803-AC53-320A4C838AF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.14:*:*:*:*:*:*:*", "matchCriteriaId": "9F096553-064F-46A2-877B-F32F163A0F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.15:*:*:*:*:*:*:*", "matchCriteriaId": "C0D762D1-E3AD-40EA-8D39-83EEB51B5E85"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.16:*:*:*:*:*:*:*", "matchCriteriaId": "A6187D19-7148-4B87-AD7E-244FF9EE0FA6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.17:*:*:*:*:*:*:*", "matchCriteriaId": "99AC64C2-E391-485C-9CD7-BA09C8FA5E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.18:*:*:*:*:*:*:*", "matchCriteriaId": "8CDA5E95-7805-441B-BEF7-4448EA45E964"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.19:*:*:*:*:*:*:*", "matchCriteriaId": "51561053-6C28-4F38-BC9B-3F7A7508EB72"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.20:*:*:*:*:*:*:*", "matchCriteriaId": "118F4A5B-C498-4FC3-BE28-50D18EBE4F22"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.21:*:*:*:*:*:*:*", "matchCriteriaId": "BD38EBE6-FE1A-4B55-9FB5-07952253B7A5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.22:*:*:*:*:*:*:*", "matchCriteriaId": "3A491E47-82AD-4055-9444-2EC0D6715326"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.23:*:*:*:*:*:*:*", "matchCriteriaId": "13C5FD16-23B6-467F-9438-5B554922F974"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.24:*:*:*:*:*:*:*", "matchCriteriaId": "9C67235F-5B51-4BF7-89EC-4810F720246F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.25:*:*:*:*:*:*:*", "matchCriteriaId": "08405DEF-05F4-45F0-AC95-DBF914A36D93"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.26:*:*:*:*:*:*:*", "matchCriteriaId": "1A7B9C4B-4A41-4175-9F07-191C1EE98C1F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.27:*:*:*:*:*:*:*", "matchCriteriaId": "B306E0A8-4D4A-4895-8128-A500D30A7E0C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.28:*:*:*:*:*:*:*", "matchCriteriaId": "295C839A-F34E-4853-A926-55EABC639412"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.29:*:*:*:*:*:*:*", "matchCriteriaId": "2AFD5F49-7EF9-4CFE-95BD-8FD19B500B0A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.30:*:*:*:*:*:*:*", "matchCriteriaId": "00B3DDDD-B2F6-4753-BA38-65A24017857D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.31:*:*:*:*:*:*:*", "matchCriteriaId": "33FCD39E-F4BF-432D-9CF9-F195CF5844F3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.32:*:*:*:*:*:*:*", "matchCriteriaId": "C7308690-CB0D-4758-B80F-D2ADCD2A9D66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.33:*:*:*:*:*:*:*", "matchCriteriaId": "313A470B-8A2B-478A-82B5-B27D2718331C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.0.34:*:*:*:*:*:*:*", "matchCriteriaId": "83FF021E-07E3-41CC-AAE8-D99D7FF24B9D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:*:*:*:*:*:*:*", "matchCriteriaId": "3DFFE5A6-6A67-4992-84A3-C0F05FACDEAD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc1:*:*:*:*:*:*", "matchCriteriaId": "13BBD2A3-AE10-48B9-8776-4FB1CAC37D44"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc2:*:*:*:*:*:*", "matchCriteriaId": "B25680CC-8918-4F27-8D7E-A6579215450B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc3:*:*:*:*:*:*", "matchCriteriaId": "92C48B4C-410C-4BA8-A28A-B2E928320FCC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1:rc4:*:*:*:*:*:*", "matchCriteriaId": "CB447523-855B-461E-8197-95169BE86EB0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "B155BBDF-6DF6-4FF5-9C41-D8A5266DCC67"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "28476DEC-9630-4B40-9D4D-9BC151DC4CA4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.3:*:*:*:*:*:*:*", "matchCriteriaId": "5646880A-2355-4BDD-89E7-825863A0311F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.4:*:*:*:*:*:*:*", "matchCriteriaId": "7FF99148-267A-46F8-9927-A9082269BAF6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.5:*:*:*:*:*:*:*", "matchCriteriaId": "A783C083-5D9C-48F9-B5A6-A97A9604FB19"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.6:*:*:*:*:*:*:*", "matchCriteriaId": "2B817A24-03AC-46CD-BEFA-505457FD2A5D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.7:*:*:*:*:*:*:*", "matchCriteriaId": "51CF1BCE-090E-4B70-BA16-ACB74411293B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.8:*:*:*:*:*:*:*", "matchCriteriaId": "187AAD67-10D7-4B57-B4C6-00443E246AF3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.9:*:*:*:*:*:*:*", "matchCriteriaId": "F341CE88-C5BC-4CDD-9CB5-B6BAD7152E63"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.1.10:*:*:*:*:*:*:*", "matchCriteriaId": "37ACE2A6-C229-4236-8E9F-235F008F3AA0"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.2:*:*:*:*:*:*:*", "matchCriteriaId": "D3220B70-917F-4F9F-8A3B-2BF581281E8D"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=0bfc96cb77224736dfa35c3c555d37b3646ef35e", "source": "secalert@redhat.com"}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=ec8013beddd717d1740cfefb1a9b900deef85462", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2012-04/msg00021.html", "source": "secalert@redhat.com"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2015-04/msg00020.html", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.2.2", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2011/12/22/5", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=752375", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/0bfc96cb77224736dfa35c3c555d37b3646ef35e", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}, {"url": "https://github.com/torvalds/linux/commit/ec8013beddd717d1740cfefb1a9b900deef85462", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/0bfc96cb77224736dfa35c3c555d37b3646ef35e"}}