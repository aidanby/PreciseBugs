{"buggy_code": ["Latest changes\n==============\n\nDevelopment version\n-------------------\n\n- Make sure that joblib works even when multiprocessing is not available,\n  for instance with Pyodide\n  https://github.com/joblib/joblib/pull/1256\n\n- Avoid unnecessary warnings when workers and main process delete\n  the temporary memmap folder contents concurrently.\n  https://github.com/joblib/joblib/pull/1263\n\n- Vendor loky 3.1.0 with several fixes to more robustly forcibly terminate\n  worker processes in case of a crash.\n  https://github.com/joblib/joblib/pull/1269\n\n- Fix memory alignment bug for pickles containing numpy arrays.\n  This is especially important when loading the pickle with\n  ``mmap_mode != None`` as the resulting ``numpy.memmap`` object\n  would not be able to correct the misalignment without performing\n  a memory copy.\n  This bug would cause invalid computation and segmentation faults\n  with native code that would directly access the underlying data\n  buffer of a numpy array, for instance C/C++/Cython code compiled\n  with older GCC versions or some old OpenBLAS written in platform\n  specific assembly.\n  https://github.com/joblib/joblib/pull/1254\n\nRelease 1.1.0\n--------------\n\n- Fix byte order inconsistency issue during deserialization using joblib.load\n  in cross-endian environment: the numpy arrays are now always loaded to\n  use the system byte order, independently of the byte order of the system\n  that serialized the pickle.\n  https://github.com/joblib/joblib/pull/1181\n\n- Fix joblib.Memory bug with the ``ignore`` parameter when the cached function\n  is a decorated function.\n  https://github.com/joblib/joblib/pull/1165\n\n- Fix `joblib.Memory` to properly handle caching for functions defined\n  interactively in a IPython session or in Jupyter notebook cell.\n  https://github.com/joblib/joblib/pull/1214\n\n- Update vendored loky (from version 2.9 to 3.0) and cloudpickle (from\n  version 1.6 to 2.0)\n  https://github.com/joblib/joblib/pull/1218\n\nRelease 1.0.1\n-------------\n\n- Add check_call_in_cache method to check cache without calling function.\n  https://github.com/joblib/joblib/pull/820\n \n- dask: avoid redundant scattering of large arguments to make a more\n  efficient use of the network resources and avoid crashing dask with\n  \"OSError: [Errno 55] No buffer space available\"\n  or \"ConnectionResetError: [Errno 104] connection reset by peer\".\n  https://github.com/joblib/joblib/pull/1133\n\nRelease 1.0.0\n-------------\n\n- Make `joblib.hash` and `joblib.Memory` caching system compatible with `numpy\n  >= 1.20.0`. Also make it explicit in the documentation that users should now\n  expect to have their `joblib.Memory` cache invalidated when either `joblib`\n  or a third party library involved in the cached values definition is\n  upgraded.  In particular, users updating `joblib` to a release that includes\n  this fix will see their previous cache invalidated if they contained\n  reference to `numpy` objects. \n  https://github.com/joblib/joblib/pull/1136\n\n- Remove deprecated `check_pickle` argument in `delayed`.\n  https://github.com/joblib/joblib/pull/903\n\nRelease 0.17.0\n--------------\n\n- Fix a spurious invalidation of `Memory.cache`'d functions called with\n  `Parallel` under Jupyter or IPython.\n  https://github.com/joblib/joblib/pull/1093\n\n- Bump vendored loky to 2.9.0 and cloudpickle to 1.6.0. In particular\n  this fixes a problem to add compat for Python 3.9.\n\nRelease 0.16.0\n--------------\n\n- Fix a problem in the constructors of of Parallel backends classes that\n  inherit from the `AutoBatchingMixin` that prevented the dask backend to\n  properly batch short tasks.\n  https://github.com/joblib/joblib/pull/1062\n\n- Fix a problem in the way the joblib dask backend batches calls that would\n  badly interact with the dask callable pickling cache and lead to wrong\n  results or errors.\n  https://github.com/joblib/joblib/pull/1055\n\n- Prevent a dask.distributed bug from surfacing in joblib's dask backend\n  during nested Parallel calls (due to joblib's auto-scattering feature)\n  https://github.com/joblib/joblib/pull/1061\n\n- Workaround for a race condition after Parallel calls with the dask backend\n  that would cause low level warnings from asyncio coroutines:\n  https://github.com/joblib/joblib/pull/1078\n\nRelease 0.15.1\n--------------\n\n- Make joblib work on Python 3 installation that do not ship with the lzma\n  package in their standard library.\n\nRelease 0.15.0\n--------------\n\n- Drop support for Python 2 and Python 3.5. All objects in\n  ``joblib.my_exceptions`` and ``joblib.format_stack`` are now deprecated and\n  will be removed in joblib 0.16. Note that no deprecation warning will be\n  raised for these objects Python < 3.7.\n  https://github.com/joblib/joblib/pull/1018\n\n- Fix many bugs related to the temporary files and folder generated when\n  automatically memory mapping large numpy arrays for efficient inter-process\n  communication. In particular, this would cause `PermissionError` exceptions\n  to be raised under Windows and large leaked files in `/dev/shm` under Linux\n  in case of crash.\n  https://github.com/joblib/joblib/pull/966\n\n- Make the dask backend collect results as soon as they complete\n  leading to a performance improvement:\n  https://github.com/joblib/joblib/pull/1025\n\n- Fix the number of jobs reported by ``effective_n_jobs`` when ``n_jobs=None``\n  called in a parallel backend context.\n  https://github.com/joblib/joblib/pull/985\n\n- Upgraded vendored cloupickle to 1.4.1 and loky to 2.8.0. This allows for\n  Parallel calls of dynamically defined functions with type annotations\n  in particular.\n\n\nRelease 0.14.1\n--------------\n\n- Configure the loky workers' environment to mitigate oversubsription with\n  nested multi-threaded code in the following case:\n\n  - allow for a suitable number of threads for numba (``NUMBA_NUM_THREADS``);\n\n  - enable Interprocess Communication for scheduler coordination when the\n    nested code uses Threading Building Blocks (TBB) (``ENABLE_IPC=1``)\n\n  https://github.com/joblib/joblib/pull/951\n\n- Fix a regression where the loky backend was not reusing previously\n  spawned workers.\n  https://github.com/joblib/joblib/pull/968\n\n- Revert https://github.com/joblib/joblib/pull/847 to avoid using\n  `pkg_resources` that introduced a performance regression under Windows:\n  https://github.com/joblib/joblib/issues/965\n\nRelease 0.14.0\n--------------\n\n- Improved the load balancing between workers to avoid stranglers caused by an\n  excessively large batch size when the task duration is varying significantly\n  (because of the combined use of ``joblib.Parallel`` and ``joblib.Memory``\n  with a partially warmed cache for instance).\n  https://github.com/joblib/joblib/pull/899\n\n- Add official support for Python 3.8: fixed protocol number in `Hasher`\n  and updated tests.\n\n- Fix a deadlock when using the dask backend (when scattering large numpy\n  arrays).\n  https://github.com/joblib/joblib/pull/914\n\n- Warn users that they should never use `joblib.load` with files from\n  untrusted sources. Fix security related API change introduced in numpy\n  1.6.3 that would prevent using joblib with recent numpy versions.\n  https://github.com/joblib/joblib/pull/879\n\n- Upgrade to cloudpickle 1.1.1 that add supports for the upcoming\n  Python 3.8 release among other things.\n  https://github.com/joblib/joblib/pull/878\n\n- Fix semaphore availability checker to avoid spawning resource trackers\n  on module import.\n  https://github.com/joblib/joblib/pull/893\n\n- Fix the oversubscription protection to only protect against nested\n  `Parallel` calls. This allows `joblib` to be run in background threads.\n  https://github.com/joblib/joblib/pull/934\n\n- Fix `ValueError` (negative dimensions) when pickling large numpy arrays on\n  Windows.\n  https://github.com/joblib/joblib/pull/920\n\n- Upgrade to loky 2.6.0 that add supports for the setting environment variables\n  in child before loading any module.\n  https://github.com/joblib/joblib/pull/940\n\n- Fix the oversubscription protection for native libraries using threadpools\n  (OpenBLAS, MKL, Blis and OpenMP runtimes).\n  The maximal number of threads is can now be set in children using the\n  ``inner_max_num_threads`` in ``parallel_backend``. It defaults to\n  ``cpu_count() // n_jobs``.\n  https://github.com/joblib/joblib/pull/940\n\n\nRelease 0.13.2\n--------------\n\nPierre Glaser\n\n   Upgrade to cloudpickle 0.8.0\n\n   Add a non-regression test related to joblib issues #836 and #833, reporting\n   that cloudpickle versions between 0.5.4 and 0.7 introduced a bug where\n   global variables changes in a parent process between two calls to\n   joblib.Parallel would not be propagated into the workers\n\n\nRelease 0.13.1\n--------------\n\nPierre Glaser\n\n   Memory now accepts pathlib.Path objects as ``location`` parameter.\n   Also, a warning is raised if the returned backend is None while\n   ``location`` is not None.\n\nOlivier Grisel\n\n   Make ``Parallel`` raise an informative ``RuntimeError`` when the\n   active parallel backend has zero worker.\n\n   Make the ``DaskDistributedBackend`` wait for workers before trying to\n   schedule work. This is useful in particular when the workers are\n   provisionned dynamically but provisionning is not immediate (for\n   instance using Kubernetes, Yarn or an HPC job queue).\n\n\nRelease 0.13.0\n--------------\n\nThomas Moreau\n\n   Include loky 2.4.2 with default serialization with ``cloudpickle``.\n   This can be tweaked with the environment variable ``LOKY_PICKLER``.\n\nThomas Moreau\n\n   Fix nested backend in SequentialBackend to avoid changing the default\n   backend to Sequential. (#792)\n\nThomas Moreau, Olivier Grisel\n\n    Fix nested_backend behavior to avoid setting the default number of\n    workers to -1 when the backend is not dask. (#784)\n\nRelease 0.12.5\n--------------\n\nThomas Moreau, Olivier Grisel\n\n    Include loky 2.3.1 with better error reporting when a worker is\n    abruptly terminated. Also fixes spurious debug output.\n\n\nPierre Glaser\n\n    Include cloudpickle 0.5.6. Fix a bug with the handling of global\n    variables by locally defined functions.\n\n\nRelease 0.12.4\n--------------\n\nThomas Moreau, Pierre Glaser, Olivier Grisel\n\n    Include loky 2.3.0 with many bugfixes, notably w.r.t. when setting\n    non-default multiprocessing contexts. Also include improvement on\n    memory management of long running worker processes and fixed issues\n    when using the loky backend under PyPy.\n\n\nMaxime Weyl\n\n    Raises a more explicit exception when a corrupted MemorizedResult is loaded.\n\nMaxime Weyl\n\n    Loading a corrupted cached file with mmap mode enabled would\n    recompute the results and return them without memory mapping.\n\n\nRelease 0.12.3\n--------------\n\nThomas Moreau\n\n    Fix joblib import setting the global start_method for multiprocessing.\n\nAlexandre Abadie\n\n    Fix MemorizedResult not picklable (#747).\n\nLo\u00efc Est\u00e8ve\n\n    Fix Memory, MemorizedFunc and MemorizedResult round-trip pickling +\n    unpickling (#746).\n\nJames Collins\n\n    Fixed a regression in Memory when positional arguments are called as\n    kwargs several times with different values (#751).\n\nThomas Moreau and Olivier Grisel\n\n    Integration of loky 2.2.2 that fixes issues with the selection of the\n    default start method and improve the reporting when calling functions\n    with arguments that raise an exception when unpickling.\n\n\nMaxime Weyl\n\n    Prevent MemorizedFunc.call_and_shelve from loading cached results to\n    RAM when not necessary. Results in big performance improvements\n\n\nRelease 0.12.2\n--------------\n\nOlivier Grisel\n\n   Integrate loky 2.2.0 to fix regression with unpicklable arguments and\n   functions reported by users (#723, #643).\n\n   Loky 2.2.0 also provides a protection against memory leaks long running\n   applications when psutil is installed (reported as #721).\n\n   Joblib now includes the code for the dask backend which has been updated\n   to properly handle nested parallelism and data scattering at the same\n   time (#722).\n\nAlexandre Abadie and Olivier Grisel\n\n   Restored some private API attribute and arguments\n   (`MemorizedResult.argument_hash` and `BatchedCalls.__init__`'s\n   `pickle_cache`) for backward compat. (#716, #732).\n\n\nJoris Van den Bossche\n\n   Fix a deprecation warning message (for `Memory`'s `cachedir`) (#720).\n\n\nRelease 0.12.1\n--------------\n\nThomas Moreau\n\n    Make sure that any exception triggered when serializing jobs in the queue\n    will be wrapped as a PicklingError as in past versions of joblib.\n\nNoam Hershtig\n\n    Fix kwonlydefaults key error in filter_args (#715)\n\n\nRelease 0.12\n------------\n\nThomas Moreau\n\n    Implement the ``'loky'`` backend with @ogrisel. This backend relies on\n    a robust implementation of ``concurrent.futures.ProcessPoolExecutor``\n    with spawned processes that can be reused across the ``Parallel``\n    calls. This fixes the bad integration with third paty libraries relying on\n    thread pools, described in https://pythonhosted.org/joblib/parallel.html#bad-interaction-of-multiprocessing-and-third-party-libraries\n\n    Limit the number of threads used in worker processes by C-libraries that\n    relies on threadpools. This functionality works for MKL, OpenBLAS, OpenMP\n    and Accelerated.\n\nElizabeth Sander\n\n    Prevent numpy arrays with the same shape and data from hashing to\n    the same memmap, to prevent jobs with preallocated arrays from\n    writing over each other.\n\nOlivier Grisel\n\n    Reduce overhead of automatic memmap by removing the need to hash the\n    array.\n\n    Make ``Memory.cache`` robust to ``PermissionError (errno 13)`` under\n    Windows when run in combination with ``Parallel``.\n\n    The automatic array memory mapping feature of ``Parallel`` does no longer\n    use ``/dev/shm`` if it is too small (less than 2 GB). In particular in\n    docker containers ``/dev/shm`` is only 64 MB by default which would cause\n    frequent failures when running joblib in Docker containers.\n\n    Make it possible to hint for thread-based parallelism with\n    ``prefer='threads'`` or enforce shared-memory semantics with\n    ``require='sharedmem'``.\n\n    Rely on the built-in exception nesting system of Python 3 to preserve\n    traceback information when an exception is raised on a remote worker\n    process. This avoid verbose and redundant exception reports under\n    Python 3.\n\n    Preserve exception type information when doing nested Parallel calls\n    instead of mapping the exception to the generic ``JoblibException`` type.\n\n\nAlexandre Abadie\n\n    Introduce the concept of 'store' and refactor the ``Memory`` internal\n    storage implementation to make it accept extra store backends for caching\n    results. ``backend`` and ``backend_options`` are the new options added to\n    ``Memory`` to specify and configure a store backend.\n\n    Add the ``register_store_backend`` function to extend the store backend\n    used by default with Memory. This default store backend is named 'local'\n    and corresponds to the local filesystem.\n\n    The store backend API is experimental and thus is subject to change in the\n    future without deprecation.\n\n    The ``cachedir`` parameter of ``Memory`` is now marked as deprecated, use\n    ``location`` instead.\n\n    Add support for LZ4 compression if ``lz4`` package is installed.\n\n    Add ``register_compressor`` function for extending available compressors.\n\n    Allow passing a string to ``compress`` parameter in ``dump`` function. This\n    string should correspond to the compressor used (e.g. zlib, gzip, lz4,\n    etc). The default compression level is used in this case.\n\nMatthew Rocklin\n\n    Allow ``parallel_backend`` to be used globally instead of only as a context\n    manager.\n    Support lazy registration of external parallel backends\n\nRelease 0.11\n------------\n\nAlexandre Abadie\n\n    Remove support for python 2.6\n\nAlexandre Abadie\n\n    Remove deprecated `format_signature`, `format_call` and `load_output`\n    functions from Memory API.\n\nLo\u00efc Est\u00e8ve\n\n    Add initial implementation of LRU cache cleaning. You can specify\n    the size limit of a ``Memory`` object via the ``bytes_limit``\n    parameter and then need to clean explicitly the cache via the\n    ``Memory.reduce_size`` method.\n\nOlivier Grisel\n\n    Make the multiprocessing backend work even when the name of the main\n    thread is not the Python default. Thanks to Roman Yurchak for the\n    suggestion.\n\nKaran Desai\n\n    pytest is used to run the tests instead of nosetests.\n    ``python setup.py test`` or ``python setup.py nosetests`` do not work\n    anymore, run ``pytest joblib`` instead.\n\nLo\u00efc Est\u00e8ve\n\n    An instance of ``joblib.ParallelBackendBase`` can be passed into\n    the ``parallel`` argument in ``joblib.Parallel``.\n\n\nLo\u00efc Est\u00e8ve\n\n    Fix handling of memmap objects with offsets greater than\n    mmap.ALLOCATIONGRANULARITY in ``joblib.Parallel``. See\n    https://github.com/joblib/joblib/issues/451 for more details.\n\nLo\u00efc Est\u00e8ve\n\n    Fix performance regression in ``joblib.Parallel`` with\n    n_jobs=1. See https://github.com/joblib/joblib/issues/483 for more\n    details.\n\nLo\u00efc Est\u00e8ve\n\n    Fix race condition when a function cached with\n    ``joblib.Memory.cache`` was used inside a ``joblib.Parallel``. See\n    https://github.com/joblib/joblib/issues/490 for more details.\n\nRelease 0.10.3\n--------------\n\nLo\u00efc Est\u00e8ve\n\n    Fix tests when multiprocessing is disabled via the\n    JOBLIB_MULTIPROCESSING environment variable.\n\nharishmk\n\n    Remove warnings in nested Parallel objects when the inner Parallel\n    has n_jobs=1. See https://github.com/joblib/joblib/pull/406 for\n    more details.\n\nRelease 0.10.2\n--------------\n\nLo\u00efc Est\u00e8ve\n\n    FIX a bug in stack formatting when the error happens in a compiled\n    extension. See https://github.com/joblib/joblib/pull/382 for more\n    details.\n\nVincent Latrouite\n\n    FIX a bug in the constructor of BinaryZlibFile that would throw an\n    exception when passing unicode filename (Python 2 only).\n    See https://github.com/joblib/joblib/pull/384 for more details.\n\nOlivier Grisel\n\n    Expose :class:`joblib.parallel.ParallelBackendBase` and\n    :class:`joblib.parallel.AutoBatchingMixin` in the public API to\n    make them officially re-usable by backend implementers.\n\n\nRelease 0.10.0\n--------------\n\nAlexandre Abadie\n\n    ENH: joblib.dump/load now accept file-like objects besides filenames.\n    https://github.com/joblib/joblib/pull/351 for more details.\n\nNiels Zeilemaker and Olivier Grisel\n\n    Refactored joblib.Parallel to enable the registration of custom\n    computational backends.\n    https://github.com/joblib/joblib/pull/306\n    Note the API to register custom backends is considered experimental\n    and subject to change without deprecation.\n\nAlexandre Abadie\n\n    Joblib pickle format change: joblib.dump always create a single pickle file\n    and joblib.dump/joblib.save never do any memory copy when writing/reading\n    pickle files. Reading pickle files generated with joblib versions prior\n    to 0.10 will be supported for a limited amount of time, we advise to\n    regenerate them from scratch when convenient.\n    joblib.dump and joblib.load also support pickle files compressed using\n    various strategies: zlib, gzip, bz2, lzma and xz. Note that lzma and xz are\n    only available with python >= 3.3.\n    https://github.com/joblib/joblib/pull/260 for more details.\n\nAntony Lee\n\n    ENH: joblib.dump/load now accept pathlib.Path objects as filenames.\n    https://github.com/joblib/joblib/pull/316 for more details.\n\nOlivier Grisel\n\n    Workaround for \"WindowsError: [Error 5] Access is denied\" when trying to\n    terminate a multiprocessing pool under Windows:\n    https://github.com/joblib/joblib/issues/354\n\n\nRelease 0.9.4\n-------------\n\nOlivier Grisel\n\n    FIX a race condition that could cause a joblib.Parallel to hang\n    when collecting the result of a job that triggers an exception.\n    https://github.com/joblib/joblib/pull/296\n\nOlivier Grisel\n\n    FIX a bug that caused joblib.Parallel to wrongly reuse previously\n    memmapped arrays instead of creating new temporary files.\n    https://github.com/joblib/joblib/pull/294 for more details.\n\nLo\u00efc Est\u00e8ve\n\n    FIX for raising non inheritable exceptions in a Parallel call. See\n    https://github.com/joblib/joblib/issues/269 for more details.\n\nAlexandre Abadie\n\n    FIX joblib.hash error with mixed types sets and dicts containing mixed\n    types keys when using Python 3.\n    see https://github.com/joblib/joblib/issues/254\n\nLo\u00efc Est\u00e8ve\n\n    FIX joblib.dump/load for big numpy arrays with dtype=object. See\n    https://github.com/joblib/joblib/issues/220 for more details.\n\nLo\u00efc Est\u00e8ve\n\n    FIX joblib.Parallel hanging when used with an exhausted\n    iterator. See https://github.com/joblib/joblib/issues/292 for more\n    details.\n\nRelease 0.9.3\n-------------\n\nOlivier Grisel\n\n    Revert back to the ``fork`` start method (instead of\n    ``forkserver``) as the latter was found to cause crashes in\n    interactive Python sessions.\n\nRelease 0.9.2\n-------------\n\nLo\u00efc Est\u00e8ve\n\n    Joblib hashing now uses the default pickle protocol (2 for Python\n    2 and 3 for Python 3). This makes it very unlikely to get the same\n    hash for a given object under Python 2 and Python 3.\n\n    In particular, for Python 3 users, this means that the output of\n    joblib.hash changes when switching from joblib 0.8.4 to 0.9.2 . We\n    strive to ensure that the output of joblib.hash does not change\n    needlessly in future versions of joblib but this is not officially\n    guaranteed.\n\nLo\u00efc Est\u00e8ve\n\n    Joblib pickles generated with Python 2 can not be loaded with\n    Python 3 and the same applies for joblib pickles generated with\n    Python 3 and loaded with Python 2.\n\n    During the beta period 0.9.0b2 to 0.9.0b4, we experimented with\n    a joblib serialization that aimed to make pickles serialized with\n    Python 3 loadable under Python 2. Unfortunately this serialization\n    strategy proved to be too fragile as far as the long-term\n    maintenance was concerned (For example see\n    https://github.com/joblib/joblib/pull/243). That means that joblib\n    pickles generated with joblib 0.9.0bN can not be loaded under\n    joblib 0.9.2. Joblib beta testers, who are the only ones likely to\n    be affected by this, are advised to delete their joblib cache when\n    they upgrade from 0.9.0bN to 0.9.2.\n\nArthur Mensch\n\n    Fixed a bug with ``joblib.hash`` that used to return unstable values for\n    strings and numpy.dtype instances depending on interning states.\n\nOlivier Grisel\n\n    Make joblib use the 'forkserver' start method by default under Python 3.4+\n    to avoid causing crash with 3rd party libraries (such as Apple vecLib /\n    Accelerate or the GCC OpenMP runtime) that use an internal thread pool that\n    is not not reinitialized when a ``fork`` system call happens.\n\nOlivier Grisel\n\n    New context manager based API (``with`` block) to re-use\n    the same pool of workers across consecutive parallel calls.\n\nVlad Niculae and Olivier Grisel\n\n    Automated batching of fast tasks into longer running jobs to\n    hide multiprocessing dispatching overhead when possible.\n\nOlivier Grisel\n\n    FIX make it possible to call ``joblib.load(filename, mmap_mode='r')``\n    on pickled objects that include a mix of arrays of both\n    memory memmapable dtypes and object dtype.\n\n\nRelease 0.8.4\n-------------\n\n2014-11-20\nOlivier Grisel\n\n    OPTIM use the C-optimized pickler under Python 3\n\n    This makes it possible to efficiently process parallel jobs that deal with\n    numerous Python objects such as large dictionaries.\n\n\nRelease 0.8.3\n-------------\n\n2014-08-19\nOlivier Grisel\n\n    FIX disable memmapping for object arrays\n\n2014-08-07\nLars Buitinck\n\n    MAINT NumPy 1.10-safe version comparisons\n\n\n2014-07-11\nOlivier Grisel\n\n    FIX #146: Heisen test failure caused by thread-unsafe Python lists\n\n    This fix uses a queue.Queue datastructure in the failing test. This\n    datastructure is thread-safe thanks to an internal Lock. This Lock instance\n    not picklable hence cause the picklability check of delayed to check fail.\n\n    When using the threading backend, picklability is no longer required, hence\n    this PRs give the user the ability to disable it on a case by case basis.\n\n\nRelease 0.8.2\n-------------\n\n2014-06-30\nOlivier Grisel\n\n    BUG: use mmap_mode='r' by default in Parallel and MemmappingPool\n\n    The former default of mmap_mode='c' (copy-on-write) caused\n    problematic use of the paging file under Windows.\n\n2014-06-27\nOlivier Grisel\n\n    BUG: fix usage of the /dev/shm folder under Linux\n\n\nRelease 0.8.1\n-------------\n\n2014-05-29\nGael Varoquaux\n\n    BUG: fix crash with high verbosity\n\n\nRelease 0.8.0\n-------------\n\n2014-05-14\nOlivier Grisel\n\n   Fix a bug in exception reporting under Python 3\n\n2014-05-10\nOlivier Grisel\n\n   Fixed a potential segfault when passing non-contiguous memmap\n   instances.\n\n2014-04-22\nGael Varoquaux\n\n    ENH: Make memory robust to modification of source files while the\n    interpreter is running. Should lead to less spurious cache flushes\n    and recomputations.\n\n\n2014-02-24\nPhilippe Gervais\n\n   New ``Memory.call_and_shelve`` API to handle memoized results by\n   reference instead of by value.\n\n\nRelease 0.8.0a3\n---------------\n\n2014-01-10\nOlivier Grisel & Gael Varoquaux\n\n   FIX #105: Race condition in task iterable consumption when\n   pre_dispatch != 'all' that could cause crash with error messages \"Pools\n   seems closed\" and \"ValueError: generator already executing\".\n\n2014-01-12\nOlivier Grisel\n\n   FIX #72: joblib cannot persist \"output_dir\" keyword argument.\n\n\nRelease 0.8.0a2\n---------------\n\n2013-12-23\nOlivier Grisel\n\n    ENH: set default value of Parallel's max_nbytes to 100MB\n\n    Motivation: avoid introducing disk latency on medium sized\n    parallel workload where memory usage is not an issue.\n\n    FIX: properly handle the JOBLIB_MULTIPROCESSING env variable\n\n    FIX: timeout test failures under windows\n\n\nRelease 0.8.0a\n--------------\n\n2013-12-19\nOlivier Grisel\n\n    FIX: support the new Python 3.4 multiprocessing API\n\n\n2013-12-05\nOlivier Grisel\n\n    ENH: make Memory respect mmap_mode at first call too\n\n    ENH: add a threading based backend to Parallel\n\n    This is low overhead alternative backend to the default multiprocessing\n    backend that is suitable when calling compiled extensions that release\n    the GIL.\n\n\nAuthor: Dan Stahlke <dan@stahlke.org>\nDate:   2013-11-08\n\n    FIX: use safe_repr to print arg vals in trace\n\n    This fixes a problem in which extremely long (and slow) stack traces would\n    be produced when function parameters are large numpy arrays.\n\n\n2013-09-10\nOlivier Grisel\n\n    ENH: limit memory copy with Parallel by leveraging numpy.memmap when\n    possible\n\n\nRelease 0.7.1\n---------------\n\n2013-07-25\nGael Varoquaux\n\n    MISC: capture meaningless argument (n_jobs=0) in Parallel\n\n2013-07-09\nLars Buitinck\n\n    ENH Handles tuples, sets and Python 3's dict_keys type the same as\n    lists. in pre_dispatch\n\n2013-05-23\nMartin Luessi\n\n    ENH: fix function caching for IPython\n\nRelease 0.7.0\n---------------\n\n**This release drops support for Python 2.5 in favor of support for\nPython 3.0**\n\n2013-02-13\nGael Varoquaux\n\n    BUG: fix nasty hash collisions\n\n2012-11-19\nGael Varoquaux\n\n    ENH: Parallel: Turn of pre-dispatch for already expanded lists\n\n\nGael Varoquaux\n2012-11-19\n\n    ENH: detect recursive sub-process spawning, as when people do not\n    protect the __main__ in scripts under Windows, and raise a useful\n    error.\n\n\nGael Varoquaux\n2012-11-16\n\n    ENH: Full python 3 support\n\nRelease 0.6.5\n---------------\n\n2012-09-15\nYannick Schwartz\n\n    BUG: make sure that sets and dictionaries give reproducible hashes\n\n\n2012-07-18\nMarek Rudnicki\n\n    BUG: make sure that object-dtype numpy array hash correctly\n\n2012-07-12\nGaelVaroquaux\n\n    BUG: Bad default n_jobs for Parallel\n\nRelease 0.6.4\n---------------\n\n2012-05-07\nVlad Niculae\n\n    ENH: controlled randomness in tests and doctest fix\n\n2012-02-21\nGaelVaroquaux\n\n    ENH: add verbosity in memory\n\n2012-02-21\nGaelVaroquaux\n\n    BUG: non-reproducible hashing: order of kwargs\n\n    The ordering of a dictionary is random. As a result the function hashing\n    was not reproducible. Pretty hard to test\n\nRelease 0.6.3\n---------------\n\n2012-02-14\nGaelVaroquaux\n\n    BUG: fix joblib Memory pickling\n\n2012-02-11\nGaelVaroquaux\n\n    BUG: fix hasher with Python 3\n\n2012-02-09\nGaelVaroquaux\n\n    API: filter_args:  `*args, **kwargs -> args, kwargs`\n\nRelease 0.6.2\n---------------\n\n2012-02-06\nGael Varoquaux\n\n    BUG: make sure Memory pickles even if cachedir=None\n\nRelease 0.6.1\n---------------\n\nBugfix release because of a merge error in release 0.6.0\n\nRelease 0.6.0\n---------------\n\n**Beta 3**\n\n2012-01-11\nGael Varoquaux\n\n    BUG: ensure compatibility with old numpy\n\n    DOC: update installation instructions\n\n    BUG: file semantic to work under Windows\n\n2012-01-10\nYaroslav Halchenko\n\n    BUG: a fix toward 2.5 compatibility\n\n**Beta 2**\n\n2012-01-07\nGael Varoquaux\n\n    ENH: hash: bugware to be able to hash objects defined interactively\n    in IPython\n\n2012-01-07\nGael Varoquaux\n\n    ENH: Parallel: warn and not fail for nested loops\n\n    ENH: Parallel: n_jobs=-2 now uses all CPUs but one\n\n2012-01-01\nJuan Manuel Caicedo Carvajal and Gael Varoquaux\n\n    ENH: add verbosity levels in Parallel\n\nRelease 0.5.7\n---------------\n\n2011-12-28\nGael varoquaux\n\n    API: zipped -> compress\n\n2011-12-26\nGael varoquaux\n\n    ENH: Add a zipped option to Memory\n\n    API: Memory no longer accepts save_npy\n\n2011-12-22\nKenneth C. Arnold and Gael varoquaux\n\n    BUG: fix numpy_pickle for array subclasses\n\n2011-12-21\nGael varoquaux\n\n    ENH: add zip-based pickling\n\n2011-12-19\nFabian Pedregosa\n\n    Py3k: compatibility fixes.\n    This makes run fine the tests test_disk and test_parallel\n\nRelease 0.5.6\n---------------\n\n2011-12-11\nLars Buitinck\n\n    ENH: Replace os.path.exists before makedirs with exception check\n    New disk.mkdirp will fail with other errnos than EEXIST.\n\n2011-12-10\nBala Subrahmanyam Varanasi\n\n    MISC: pep8 compliant\n\n\nRelease 0.5.5\n---------------\n\n2011-19-10\nFabian Pedregosa\n\n    ENH: Make joblib installable under Python 3.X\n\nRelease 0.5.4\n---------------\n\n2011-09-29\nJon Olav Vik\n\n    BUG: Make mangling path to filename work on Windows\n\n2011-09-25\nOlivier Grisel\n\n    FIX: doctest heisenfailure on execution time\n\n2011-08-24\nRalf Gommers\n\n    STY: PEP8 cleanup.\n\n\nRelease 0.5.3\n---------------\n\n2011-06-25\nGael varoquaux\n\n   API: All the useful symbols in the __init__\n\n\nRelease 0.5.2\n---------------\n\n2011-06-25\nGael varoquaux\n\n    ENH: Add cpu_count\n\n2011-06-06\nGael varoquaux\n\n    ENH: Make sure memory hash in a reproducible way\n\n\nRelease 0.5.1\n---------------\n\n2011-04-12\nGael varoquaux\n\n    TEST: Better testing of parallel and pre_dispatch\n\nYaroslav Halchenko\n2011-04-12\n\n    DOC: quick pass over docs -- trailing spaces/spelling\n\nYaroslav Halchenko\n2011-04-11\n\n    ENH: JOBLIB_MULTIPROCESSING env var to disable multiprocessing from the\n    environment\n\nAlexandre Gramfort\n2011-04-08\n\n    ENH : adding log message to know how long it takes to load from disk the\n    cache\n\n\nRelease 0.5.0\n---------------\n\n2011-04-01\nGael varoquaux\n\n    BUG: pickling MemoizeFunc does not store timestamp\n\n2011-03-31\nNicolas Pinto\n\n    TEST: expose hashing bug with cached method\n\n2011-03-26...2011-03-27\nPietro Berkes\n\n    BUG: fix error management in rm_subdirs\n    BUG: fix for race condition during tests in mem.clear()\n\nGael varoquaux\n2011-03-22...2011-03-26\n\n    TEST: Improve test coverage and robustness\n\nGael varoquaux\n2011-03-19\n\n    BUG: hashing functions with only \\*var \\**kwargs\n\nGael varoquaux\n2011-02-01... 2011-03-22\n\n    BUG: Many fixes to capture interprocess race condition when mem.cache\n    is used by several processes on the same cache.\n\nFabian Pedregosa\n2011-02-28\n\n    First work on Py3K compatibility\n\nGael varoquaux\n2011-02-27\n\n    ENH: pre_dispatch in parallel: lazy generation of jobs in parallel\n    for to avoid drowning memory.\n\nGaelVaroquaux\n2011-02-24\n\n    ENH: Add the option of overloading the arguments of the mother\n    'Memory' object in the cache method that is doing the decoration.\n\nGael varoquaux\n2010-11-21\n\n    ENH: Add a verbosity level for more verbosity\n\nRelease 0.4.6\n----------------\n\nGael varoquaux\n2010-11-15\n\n    ENH: Deal with interruption in parallel\n\nGael varoquaux\n2010-11-13\n\n    BUG: Exceptions raised by Parallel when n_job=1 are no longer captured.\n\nGael varoquaux\n2010-11-13\n\n    BUG: Capture wrong arguments properly (better error message)\n\n\nRelease 0.4.5\n----------------\n\nPietro Berkes\n2010-09-04\n\n    BUG: Fix Windows peculiarities with path separators and file names\n    BUG: Fix more windows locking bugs\n\nGael varoquaux\n2010-09-03\n\n    ENH: Make sure that exceptions raised in Parallel also inherit from\n    the original exception class\n    ENH: Add a shadow set of exceptions\n\nFabian Pedregosa\n2010-09-01\n\n    ENH: Clean up the code for parallel. Thanks to Fabian Pedregosa for\n    the patch.\n\n\nRelease 0.4.4\n----------------\n\nGael varoquaux\n2010-08-23\n\n    BUG: Fix Parallel on computers with only one CPU, for n_jobs=-1.\n\nGael varoquaux\n2010-08-02\n\n    BUG: Fix setup.py for extra setuptools args.\n\nGael varoquaux\n2010-07-29\n\n    MISC: Silence tests (and hopefully Yaroslav :P)\n\nRelease 0.4.3\n----------------\n\nGael Varoquaux\n2010-07-22\n\n    BUG: Fix hashing for function with a side effect modifying their input\n    argument. Thanks to Pietro Berkes for reporting the bug and proving the\n    patch.\n\nRelease 0.4.2\n----------------\n\nGael Varoquaux\n2010-07-16\n\n    BUG: Make sure that joblib still works with Python2.5. => release 0.4.2\n\nRelease 0.4.1\n----------------\n", "\"\"\"\nHelpers for embarrassingly parallel code.\n\"\"\"\n# Author: Gael Varoquaux < gael dot varoquaux at normalesup dot org >\n# Copyright: 2010, Gael Varoquaux\n# License: BSD 3 clause\n\nfrom __future__ import division\n\nimport os\nimport sys\nfrom math import sqrt\nimport functools\nimport time\nimport threading\nimport itertools\nfrom uuid import uuid4\nfrom numbers import Integral\nimport warnings\nimport queue\n\nfrom ._multiprocessing_helpers import mp\n\nfrom .logger import Logger, short_format_time\nfrom .disk import memstr_to_bytes\nfrom ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n                                 ThreadingBackend, SequentialBackend,\n                                 LokyBackend)\nfrom .externals.cloudpickle import dumps, loads\n\n# Make sure that those two classes are part of the public joblib.parallel API\n# so that 3rd party backend implementers can import them from here.\nfrom ._parallel_backends import AutoBatchingMixin  # noqa\nfrom ._parallel_backends import ParallelBackendBase  # noqa\n\n\nBACKENDS = {\n    'threading': ThreadingBackend,\n    'sequential': SequentialBackend,\n}\n# name of the backend used by default by Parallel outside of any context\n# managed by ``parallel_backend``.\n\n# threading is the only backend that is always everywhere\nDEFAULT_BACKEND = 'threading'\n\nDEFAULT_N_JOBS = 1\n\nMAYBE_AVAILABLE_BACKENDS = {'multiprocessing', 'loky'}\n\n# if multiprocessing is available, so is loky, we set it as the default\n# backend\nif mp is not None:\n    BACKENDS['multiprocessing'] = MultiprocessingBackend\n    from .externals import loky\n    BACKENDS['loky'] = LokyBackend\n    DEFAULT_BACKEND = 'loky'\n\n\nDEFAULT_THREAD_BACKEND = 'threading'\n\n# Thread local value that can be overridden by the ``parallel_backend`` context\n# manager\n_backend = threading.local()\n\nVALID_BACKEND_HINTS = ('processes', 'threads', None)\nVALID_BACKEND_CONSTRAINTS = ('sharedmem', None)\n\n\ndef _register_dask():\n    \"\"\" Register Dask Backend if called with parallel_backend(\"dask\") \"\"\"\n    try:\n        from ._dask import DaskDistributedBackend\n        register_parallel_backend('dask', DaskDistributedBackend)\n    except ImportError as e:\n        msg = (\"To use the dask.distributed backend you must install both \"\n               \"the `dask` and distributed modules.\\n\\n\"\n               \"See https://dask.pydata.org/en/latest/install.html for more \"\n               \"information.\")\n        raise ImportError(msg) from e\n\n\nEXTERNAL_BACKENDS = {\n    'dask': _register_dask,\n}\n\n\ndef get_active_backend(prefer=None, require=None, verbose=0):\n    \"\"\"Return the active default backend\"\"\"\n    if prefer not in VALID_BACKEND_HINTS:\n        raise ValueError(\"prefer=%r is not a valid backend hint, \"\n                         \"expected one of %r\" % (prefer, VALID_BACKEND_HINTS))\n    if require not in VALID_BACKEND_CONSTRAINTS:\n        raise ValueError(\"require=%r is not a valid backend constraint, \"\n                         \"expected one of %r\"\n                         % (require, VALID_BACKEND_CONSTRAINTS))\n\n    if prefer == 'processes' and require == 'sharedmem':\n        raise ValueError(\"prefer == 'processes' and require == 'sharedmem'\"\n                         \" are inconsistent settings\")\n    backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)\n    if backend_and_jobs is not None:\n        # Try to use the backend set by the user with the context manager.\n        backend, n_jobs = backend_and_jobs\n        nesting_level = backend.nesting_level\n        supports_sharedmem = getattr(backend, 'supports_sharedmem', False)\n        if require == 'sharedmem' and not supports_sharedmem:\n            # This backend does not match the shared memory constraint:\n            # fallback to the default thead-based backend.\n            sharedmem_backend = BACKENDS[DEFAULT_THREAD_BACKEND](\n                nesting_level=nesting_level)\n            if verbose >= 10:\n                print(\"Using %s as joblib.Parallel backend instead of %s \"\n                      \"as the latter does not provide shared memory semantics.\"\n                      % (sharedmem_backend.__class__.__name__,\n                         backend.__class__.__name__))\n            return sharedmem_backend, DEFAULT_N_JOBS\n        else:\n            return backend_and_jobs\n\n    # We are outside of the scope of any parallel_backend context manager,\n    # create the default backend instance now.\n    backend = BACKENDS[DEFAULT_BACKEND](nesting_level=0)\n    supports_sharedmem = getattr(backend, 'supports_sharedmem', False)\n    uses_threads = getattr(backend, 'uses_threads', False)\n    if ((require == 'sharedmem' and not supports_sharedmem) or\n            (prefer == 'threads' and not uses_threads)):\n        # Make sure the selected default backend match the soft hints and\n        # hard constraints:\n        backend = BACKENDS[DEFAULT_THREAD_BACKEND](nesting_level=0)\n    return backend, DEFAULT_N_JOBS\n\n\nclass parallel_backend(object):\n    \"\"\"Change the default backend used by Parallel inside a with block.\n\n    If ``backend`` is a string it must match a previously registered\n    implementation using the :func:`~register_parallel_backend` function.\n\n    By default the following backends are available:\n\n    - 'loky': single-host, process-based parallelism (used by default),\n    - 'threading': single-host, thread-based parallelism,\n    - 'multiprocessing': legacy single-host, process-based parallelism.\n\n    'loky' is recommended to run functions that manipulate Python objects.\n    'threading' is a low-overhead alternative that is most efficient for\n    functions that release the Global Interpreter Lock: e.g. I/O-bound code or\n    CPU-bound code in a few calls to native code that explicitly releases the\n    GIL. Note that on some rare systems (such as pyiodine),\n    multiprocessing and loky may not be available, in which case joblib\n    defaults to threading.\n\n    In addition, if the `dask` and `distributed` Python packages are installed,\n    it is possible to use the 'dask' backend for better scheduling of nested\n    parallel calls without over-subscription and potentially distribute\n    parallel calls over a networked cluster of several hosts.\n\n    It is also possible to use the distributed 'ray' backend for distributing\n    the workload to a cluster of nodes. To use the 'ray' joblib backend add\n    the following lines::\n\n     >>> from ray.util.joblib import register_ray  # doctest: +SKIP\n     >>> register_ray()  # doctest: +SKIP\n     >>> with parallel_backend(\"ray\"):  # doctest: +SKIP\n     ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n     [-1, -2, -3, -4, -5]\n\n    Alternatively the backend can be passed directly as an instance.\n\n    By default all available workers will be used (``n_jobs=-1``) unless the\n    caller passes an explicit value for the ``n_jobs`` parameter.\n\n    This is an alternative to passing a ``backend='backend_name'`` argument to\n    the :class:`~Parallel` class constructor. It is particularly useful when\n    calling into library code that uses joblib internally but does not expose\n    the backend argument in its own API.\n\n    >>> from operator import neg\n    >>> with parallel_backend('threading'):\n    ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n    ...\n    [-1, -2, -3, -4, -5]\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    Joblib also tries to limit the oversubscription by limiting the number of\n    threads usable in some third-party library threadpools like OpenBLAS, MKL\n    or OpenMP. The default limit in each worker is set to\n    ``max(cpu_count() // effective_n_jobs, 1)`` but this limit can be\n    overwritten with the ``inner_max_num_threads`` argument which will be used\n    to set this limit in the child processes.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    def __init__(self, backend, n_jobs=-1, inner_max_num_threads=None,\n                 **backend_params):\n        if isinstance(backend, str):\n            if backend not in BACKENDS:\n                if backend in EXTERNAL_BACKENDS:\n                    register = EXTERNAL_BACKENDS[backend]\n                    register()\n                elif backend in MAYBE_AVAILABLE_BACKENDS:\n                    warnings.warn(\n                        f\"joblib backend '{backend}' is not available on \"\n                        f\"your system, falling back to {DEFAULT_BACKEND}.\",\n                        UserWarning,\n                        stacklevel=2)\n                    BACKENDS[backend] = BACKENDS[DEFAULT_BACKEND]\n                else:\n                    raise ValueError(\"Invalid backend: %s, expected one of %r\"\n                                     % (backend, sorted(BACKENDS.keys())))\n\n            backend = BACKENDS[backend](**backend_params)\n\n        if inner_max_num_threads is not None:\n            msg = (\"{} does not accept setting the inner_max_num_threads \"\n                   \"argument.\".format(backend.__class__.__name__))\n            assert backend.supports_inner_max_num_threads, msg\n            backend.inner_max_num_threads = inner_max_num_threads\n\n        # If the nesting_level of the backend is not set previously, use the\n        # nesting level from the previous active_backend to set it\n        current_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)\n        if backend.nesting_level is None:\n            if current_backend_and_jobs is None:\n                nesting_level = 0\n            else:\n                nesting_level = current_backend_and_jobs[0].nesting_level\n\n            backend.nesting_level = nesting_level\n\n        # Save the backends info and set the active backend\n        self.old_backend_and_jobs = current_backend_and_jobs\n        self.new_backend_and_jobs = (backend, n_jobs)\n\n        _backend.backend_and_jobs = (backend, n_jobs)\n\n    def __enter__(self):\n        return self.new_backend_and_jobs\n\n    def __exit__(self, type, value, traceback):\n        self.unregister()\n\n    def unregister(self):\n        if self.old_backend_and_jobs is None:\n            if getattr(_backend, 'backend_and_jobs', None) is not None:\n                del _backend.backend_and_jobs\n        else:\n            _backend.backend_and_jobs = self.old_backend_and_jobs\n\n\n# Under Linux or OS X the default start method of multiprocessing\n# can cause third party libraries to crash. Under Python 3.4+ it is possible\n# to set an environment variable to switch the default start method from\n# 'fork' to 'forkserver' or 'spawn' to avoid this issue albeit at the cost\n# of causing semantic changes and some additional pool instantiation overhead.\nDEFAULT_MP_CONTEXT = None\nif hasattr(mp, 'get_context'):\n    method = os.environ.get('JOBLIB_START_METHOD', '').strip() or None\n    if method is not None:\n        DEFAULT_MP_CONTEXT = mp.get_context(method=method)\n\n\nclass BatchedCalls(object):\n    \"\"\"Wrap a sequence of (func, args, kwargs) tuples as a single callable\"\"\"\n\n    def __init__(self, iterator_slice, backend_and_jobs, reducer_callback=None,\n                 pickle_cache=None):\n        self.items = list(iterator_slice)\n        self._size = len(self.items)\n        self._reducer_callback = reducer_callback\n        if isinstance(backend_and_jobs, tuple):\n            self._backend, self._n_jobs = backend_and_jobs\n        else:\n            # this is for backward compatibility purposes. Before 0.12.6,\n            # nested backends were returned without n_jobs indications.\n            self._backend, self._n_jobs = backend_and_jobs, None\n        self._pickle_cache = pickle_cache if pickle_cache is not None else {}\n\n    def __call__(self):\n        # Set the default nested backend to self._backend but do not set the\n        # change the default number of processes to -1\n        with parallel_backend(self._backend, n_jobs=self._n_jobs):\n            return [func(*args, **kwargs)\n                    for func, args, kwargs in self.items]\n\n    def __reduce__(self):\n        if self._reducer_callback is not None:\n            self._reducer_callback()\n        # no need pickle the callback.\n        return (\n            BatchedCalls,\n            (self.items, (self._backend, self._n_jobs), None,\n             self._pickle_cache)\n        )\n\n    def __len__(self):\n        return self._size\n\n\n###############################################################################\n# CPU count that works also when multiprocessing has been disabled via\n# the JOBLIB_MULTIPROCESSING environment variable\ndef cpu_count(only_physical_cores=False):\n    \"\"\"Return the number of CPUs.\n\n    This delegates to loky.cpu_count that takes into account additional\n    constraints such as Linux CFS scheduler quotas (typically set by container\n    runtimes such as docker) and CPU affinity (for instance using the taskset\n    command on Linux).\n\n    If only_physical_cores is True, do not take hyperthreading / SMT logical\n    cores into account.\n    \"\"\"\n    if mp is None:\n        return 1\n\n    return loky.cpu_count(only_physical_cores=only_physical_cores)\n\n\n###############################################################################\n# For verbosity\n\ndef _verbosity_filter(index, verbose):\n    \"\"\" Returns False for indices increasingly apart, the distance\n        depending on the value of verbose.\n\n        We use a lag increasing as the square of index\n    \"\"\"\n    if not verbose:\n        return True\n    elif verbose > 10:\n        return False\n    if index == 0:\n        return False\n    verbose = .5 * (11 - verbose) ** 2\n    scale = sqrt(index / verbose)\n    next_scale = sqrt((index + 1) / verbose)\n    return (int(next_scale) == int(scale))\n\n\n###############################################################################\ndef delayed(function):\n    \"\"\"Decorator used to capture the arguments of a function.\"\"\"\n\n    def delayed_function(*args, **kwargs):\n        return function, args, kwargs\n    try:\n        delayed_function = functools.wraps(function)(delayed_function)\n    except AttributeError:\n        \" functools.wraps fails on some callable objects \"\n    return delayed_function\n\n\n###############################################################################\nclass BatchCompletionCallBack(object):\n    \"\"\"Callback used by joblib.Parallel's multiprocessing backend.\n\n    This callable is executed by the parent process whenever a worker process\n    has returned the results of a batch of tasks.\n\n    It is used for progress reporting, to update estimate of the batch\n    processing duration and to schedule the next batch of tasks to be\n    processed.\n\n    \"\"\"\n    def __init__(self, dispatch_timestamp, batch_size, parallel):\n        self.dispatch_timestamp = dispatch_timestamp\n        self.batch_size = batch_size\n        self.parallel = parallel\n\n    def __call__(self, out):\n        self.parallel.n_completed_tasks += self.batch_size\n        this_batch_duration = time.time() - self.dispatch_timestamp\n\n        self.parallel._backend.batch_completed(self.batch_size,\n                                               this_batch_duration)\n        self.parallel.print_progress()\n        with self.parallel._lock:\n            if self.parallel._original_iterator is not None:\n                self.parallel.dispatch_next()\n\n\n###############################################################################\ndef register_parallel_backend(name, factory, make_default=False):\n    \"\"\"Register a new Parallel backend factory.\n\n    The new backend can then be selected by passing its name as the backend\n    argument to the :class:`~Parallel` class. Moreover, the default backend can\n    be overwritten globally by setting make_default=True.\n\n    The factory can be any callable that takes no argument and return an\n    instance of ``ParallelBackendBase``.\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    BACKENDS[name] = factory\n    if make_default:\n        global DEFAULT_BACKEND\n        DEFAULT_BACKEND = name\n\n\ndef effective_n_jobs(n_jobs=-1):\n    \"\"\"Determine the number of jobs that can actually run in parallel\n\n    n_jobs is the number of workers requested by the callers. Passing n_jobs=-1\n    means requesting all available workers for instance matching the number of\n    CPU cores on the worker host(s).\n\n    This method should return a guesstimate of the number of workers that can\n    actually perform work concurrently with the currently enabled default\n    backend. The primary use case is to make it possible for the caller to know\n    in how many chunks to slice the work.\n\n    In general working on larger data chunks is more efficient (less scheduling\n    overhead and better use of CPU cache prefetching heuristics) as long as all\n    the workers have enough work to do.\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    backend, backend_n_jobs = get_active_backend()\n    if n_jobs is None:\n        n_jobs = backend_n_jobs\n    return backend.effective_n_jobs(n_jobs=n_jobs)\n\n\n###############################################################################\nclass Parallel(Logger):\n    ''' Helper class for readable parallel mapping.\n\n        Read more in the :ref:`User Guide <parallel>`.\n\n        Parameters\n        -----------\n        n_jobs: int, default: None\n            The maximum number of concurrently running jobs, such as the number\n            of Python worker processes when backend=\"multiprocessing\"\n            or the size of the thread-pool when backend=\"threading\".\n            If -1 all CPUs are used. If 1 is given, no parallel computing code\n            is used at all, which is useful for debugging. For n_jobs below -1,\n            (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all\n            CPUs but one are used.\n            None is a marker for 'unset' that will be interpreted as n_jobs=1\n            (sequential execution) unless the call is performed under a\n            :func:`~parallel_backend` context manager that sets another value\n            for n_jobs.\n        backend: str, ParallelBackendBase instance or None, default: 'loky'\n            Specify the parallelization backend implementation.\n            Supported backends are:\n\n            - \"loky\" used by default, can induce some\n              communication and memory overhead when exchanging input and\n              output data with the worker Python processes. On some rare\n              systems (such as Pyiodide), the loky backend may not be\n              available.\n            - \"multiprocessing\" previous process-based backend based on\n              `multiprocessing.Pool`. Less robust than `loky`.\n            - \"threading\" is a very low-overhead backend but it suffers\n              from the Python Global Interpreter Lock if the called function\n              relies a lot on Python objects. \"threading\" is mostly useful\n              when the execution bottleneck is a compiled extension that\n              explicitly releases the GIL (for instance a Cython loop wrapped\n              in a \"with nogil\" block or an expensive call to a library such\n              as NumPy).\n            - finally, you can register backends by calling\n              :func:`~register_parallel_backend`. This will allow you to\n              implement a backend of your liking.\n\n            It is not recommended to hard-code the backend name in a call to\n            :class:`~Parallel` in a library. Instead it is recommended to set\n            soft hints (prefer) or hard constraints (require) so as to make it\n            possible for library users to change the backend from the outside\n            using the :func:`~parallel_backend` context manager.\n        prefer: str in {'processes', 'threads'} or None, default: None\n            Soft hint to choose the default backend if no specific backend\n            was selected with the :func:`~parallel_backend` context manager.\n            The default process-based backend is 'loky' and the default\n            thread-based backend is 'threading'. Ignored if the ``backend``\n            parameter is specified.\n        require: 'sharedmem' or None, default None\n            Hard constraint to select the backend. If set to 'sharedmem',\n            the selected backend will be single-host and thread-based even\n            if the user asked for a non-thread based backend with\n            parallel_backend.\n        verbose: int, optional\n            The verbosity level: if non zero, progress messages are\n            printed. Above 50, the output is sent to stdout.\n            The frequency of the messages increases with the verbosity level.\n            If it more than 10, all iterations are reported.\n        timeout: float, optional\n            Timeout limit for each task to complete.  If any task takes longer\n            a TimeOutError will be raised. Only applied when n_jobs != 1\n        pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}\n            The number of batches (of tasks) to be pre-dispatched.\n            Default is '2*n_jobs'. When batch_size=\"auto\" this is reasonable\n            default and the workers should never starve.\n        batch_size: int or 'auto', default: 'auto'\n            The number of atomic tasks to dispatch at once to each\n            worker. When individual evaluations are very fast, dispatching\n            calls to workers can be slower than sequential computation because\n            of the overhead. Batching fast computations together can mitigate\n            this.\n            The ``'auto'`` strategy keeps track of the time it takes for a batch\n            to complete, and dynamically adjusts the batch size to keep the time\n            on the order of half a second, using a heuristic. The initial batch\n            size is 1.\n            ``batch_size=\"auto\"`` with ``backend=\"threading\"`` will dispatch\n            batches of a single task at a time as the threading backend has\n            very little overhead and using larger batch size has not proved to\n            bring any gain in that case.\n        temp_folder: str, optional\n            Folder to be used by the pool for memmapping large arrays\n            for sharing memory with worker processes. If None, this will try in\n            order:\n\n            - a folder pointed by the JOBLIB_TEMP_FOLDER environment\n              variable,\n            - /dev/shm if the folder exists and is writable: this is a\n              RAM disk filesystem available by default on modern Linux\n              distributions,\n            - the default system temporary folder that can be\n              overridden with TMP, TMPDIR or TEMP environment\n              variables, typically /tmp under Unix operating systems.\n\n            Only active when backend=\"loky\" or \"multiprocessing\".\n        max_nbytes int, str, or None, optional, 1M by default\n            Threshold on the size of arrays passed to the workers that\n            triggers automated memory mapping in temp_folder. Can be an int\n            in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.\n            Use None to disable memmapping of large arrays.\n            Only active when backend=\"loky\" or \"multiprocessing\".\n        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, default: 'r'\n            Memmapping mode for numpy arrays passed to workers. None will\n            disable memmapping, other modes defined in the numpy.memmap doc:\n            https://numpy.org/doc/stable/reference/generated/numpy.memmap.html\n            Also, see 'max_nbytes' parameter documentation for more details.\n\n        Notes\n        -----\n\n        This object uses workers to compute in parallel the application of a\n        function to many different arguments. The main functionality it brings\n        in addition to using the raw multiprocessing or concurrent.futures API\n        are (see examples for details):\n\n        * More readable code, in particular since it avoids\n          constructing list of arguments.\n\n        * Easier debugging:\n            - informative tracebacks even when the error happens on\n              the client side\n            - using 'n_jobs=1' enables to turn off parallel computing\n              for debugging without changing the codepath\n            - early capture of pickling errors\n\n        * An optional progress meter.\n\n        * Interruption of multiprocesses jobs with 'Ctrl-C'\n\n        * Flexible pickling control for the communication to and from\n          the worker processes.\n\n        * Ability to use shared memory efficiently with worker\n          processes for large numpy-based datastructures.\n\n        Examples\n        --------\n\n        A simple example:\n\n        >>> from math import sqrt\n        >>> from joblib import Parallel, delayed\n        >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n\n        Reshaping the output when the function has several return\n        values:\n\n        >>> from math import modf\n        >>> from joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))\n        >>> res, i = zip(*r)\n        >>> res\n        (0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)\n        >>> i\n        (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)\n\n        The progress meter: the higher the value of `verbose`, the more\n        messages:\n\n        >>> from time import sleep\n        >>> from joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=2, verbose=10)(delayed(sleep)(.2) for _ in range(10)) #doctest: +SKIP\n        [Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s\n        [Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.8s\n        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    1.4s finished\n\n        Traceback example, note how the line of the error is indicated\n        as well as the values of the parameter passed to the function that\n        triggered the exception, even though the traceback happens in the\n        child process:\n\n        >>> from heapq import nlargest\n        >>> from joblib import Parallel, delayed\n        >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP\n        #...\n        ---------------------------------------------------------------------------\n        Sub-process traceback:\n        ---------------------------------------------------------------------------\n        TypeError                                          Mon Nov 12 11:37:46 2012\n        PID: 12934                                    Python 2.7.3: /usr/bin/python\n        ...........................................................................\n        /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)\n            419         if n >= size:\n            420             return sorted(iterable, key=key, reverse=True)[:n]\n            421\n            422     # When key is none, use simpler decoration\n            423     if key is None:\n        --> 424         it = izip(iterable, count(0,-1))                    # decorate\n            425         result = _nlargest(n, it)\n            426         return map(itemgetter(0), result)                   # undecorate\n            427\n            428     # General case, slowest method\n         TypeError: izip argument #1 must support iteration\n        ___________________________________________________________________________\n\n\n        Using pre_dispatch in a producer/consumer situation, where the\n        data is generated on the fly. Note how the producer is first\n        called 3 times before the parallel loop is initiated, and then\n        called to generate new data on the fly:\n\n        >>> from math import sqrt\n        >>> from joblib import Parallel, delayed\n        >>> def producer():\n        ...     for i in range(6):\n        ...         print('Produced %s' % i)\n        ...         yield i\n        >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(\n        ...                delayed(sqrt)(i) for i in producer()) #doctest: +SKIP\n        Produced 0\n        Produced 1\n        Produced 2\n        [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s\n        Produced 3\n        [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s\n        Produced 4\n        [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s\n        Produced 5\n        [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s\n        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s\n        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished\n\n    '''\n    def __init__(self, n_jobs=None, backend=None, verbose=0, timeout=None,\n                 pre_dispatch='2 * n_jobs', batch_size='auto',\n                 temp_folder=None, max_nbytes='1M', mmap_mode='r',\n                 prefer=None, require=None):\n        active_backend, context_n_jobs = get_active_backend(\n            prefer=prefer, require=require, verbose=verbose)\n        nesting_level = active_backend.nesting_level\n        if backend is None and n_jobs is None:\n            # If we are under a parallel_backend context manager, look up\n            # the default number of jobs and use that instead:\n            n_jobs = context_n_jobs\n        if n_jobs is None:\n            # No specific context override and no specific value request:\n            # default to 1.\n            n_jobs = 1\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        self.timeout = timeout\n        self.pre_dispatch = pre_dispatch\n        self._ready_batches = queue.Queue()\n        self._id = uuid4().hex\n        self._reducer_callback = None\n\n        if isinstance(max_nbytes, str):\n            max_nbytes = memstr_to_bytes(max_nbytes)\n\n        self._backend_args = dict(\n            max_nbytes=max_nbytes,\n            mmap_mode=mmap_mode,\n            temp_folder=temp_folder,\n            prefer=prefer,\n            require=require,\n            verbose=max(0, self.verbose - 50),\n        )\n        if DEFAULT_MP_CONTEXT is not None:\n            self._backend_args['context'] = DEFAULT_MP_CONTEXT\n        elif hasattr(mp, \"get_context\"):\n            self._backend_args['context'] = mp.get_context()\n\n        if backend is None:\n            backend = active_backend\n\n        elif isinstance(backend, ParallelBackendBase):\n            # Use provided backend as is, with the current nesting_level if it\n            # is not set yet.\n            if backend.nesting_level is None:\n                backend.nesting_level = nesting_level\n\n        elif hasattr(backend, 'Pool') and hasattr(backend, 'Lock'):\n            # Make it possible to pass a custom multiprocessing context as\n            # backend to change the start method to forkserver or spawn or\n            # preload modules on the forkserver helper process.\n            self._backend_args['context'] = backend\n            backend = MultiprocessingBackend(nesting_level=nesting_level)\n\n        elif backend not in BACKENDS and backend in MAYBE_AVAILABLE_BACKENDS:\n            warnings.warn(\n                f\"joblib backend '{backend}' is not available on \"\n                f\"your system, falling back to {DEFAULT_BACKEND}.\",\n                UserWarning,\n                stacklevel=2)\n            BACKENDS[backend] = BACKENDS[DEFAULT_BACKEND]\n            backend = BACKENDS[DEFAULT_BACKEND](nesting_level=nesting_level)\n\n        else:\n            try:\n                backend_factory = BACKENDS[backend]\n            except KeyError as e:\n                raise ValueError(\"Invalid backend: %s, expected one of %r\"\n                                 % (backend, sorted(BACKENDS.keys()))) from e\n            backend = backend_factory(nesting_level=nesting_level)\n\n        if (require == 'sharedmem' and\n                not getattr(backend, 'supports_sharedmem', False)):\n            raise ValueError(\"Backend %s does not support shared memory\"\n                             % backend)\n\n        if (batch_size == 'auto' or isinstance(batch_size, Integral) and\n                batch_size > 0):\n            self.batch_size = batch_size\n        else:\n            raise ValueError(\n                \"batch_size must be 'auto' or a positive integer, got: %r\"\n                % batch_size)\n\n        self._backend = backend\n        self._output = None\n        self._jobs = list()\n        self._managed_backend = False\n\n        # This lock is used coordinate the main thread of this process with\n        # the async callback thread of our the pool.\n        self._lock = threading.RLock()\n\n    def __enter__(self):\n        self._managed_backend = True\n        self._initialize_backend()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._terminate_backend()\n        self._managed_backend = False\n\n    def _initialize_backend(self):\n        \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n        try:\n            n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n                                             **self._backend_args)\n            if self.timeout is not None and not self._backend.supports_timeout:\n                warnings.warn(\n                    'The backend class {!r} does not support timeout. '\n                    \"You have set 'timeout={}' in Parallel but \"\n                    \"the 'timeout' parameter will not be used.\".format(\n                        self._backend.__class__.__name__,\n                        self.timeout))\n\n        except FallbackToBackend as e:\n            # Recursively initialize the backend in case of requested fallback.\n            self._backend = e.backend\n            n_jobs = self._initialize_backend()\n\n        return n_jobs\n\n    def _effective_n_jobs(self):\n        if self._backend:\n            return self._backend.effective_n_jobs(self.n_jobs)\n        return 1\n\n    def _terminate_backend(self):\n        if self._backend is not None:\n            self._backend.terminate()\n\n    def _dispatch(self, batch):\n        \"\"\"Queue the batch for computing, with or without multiprocessing\n\n        WARNING: this method is not thread-safe: it should be only called\n        indirectly via dispatch_one_batch.\n\n        \"\"\"\n        # If job.get() catches an exception, it closes the queue:\n        if self._aborting:\n            return\n\n        self.n_dispatched_tasks += len(batch)\n        self.n_dispatched_batches += 1\n\n        dispatch_timestamp = time.time()\n        cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\n        with self._lock:\n            job_idx = len(self._jobs)\n            job = self._backend.apply_async(batch, callback=cb)\n            # A job can complete so quickly than its callback is\n            # called before we get here, causing self._jobs to\n            # grow. To ensure correct results ordering, .insert is\n            # used (rather than .append) in the following line\n            self._jobs.insert(job_idx, job)\n\n    def dispatch_next(self):\n        \"\"\"Dispatch more data for parallel processing\n\n        This method is meant to be called concurrently by the multiprocessing\n        callback. We rely on the thread-safety of dispatch_one_batch to protect\n        against concurrent consumption of the unprotected iterator.\n\n        \"\"\"\n        if not self.dispatch_one_batch(self._original_iterator):\n            self._iterating = False\n            self._original_iterator = None\n\n    def dispatch_one_batch(self, iterator):\n        \"\"\"Prefetch the tasks for the next batch and dispatch them.\n\n        The effective size of the batch is computed here.\n        If there are no more jobs to dispatch, return False, else return True.\n\n        The iterator consumption and dispatching is protected by the same\n        lock so calling this function should be thread safe.\n\n        \"\"\"\n        if self.batch_size == 'auto':\n            batch_size = self._backend.compute_batch_size()\n        else:\n            # Fixed batch size strategy\n            batch_size = self.batch_size\n\n        with self._lock:\n            # to ensure an even distribution of the workolad between workers,\n            # we look ahead in the original iterators more than batch_size\n            # tasks - However, we keep consuming only one batch at each\n            # dispatch_one_batch call. The extra tasks are stored in a local\n            # queue, _ready_batches, that is looked-up prior to re-consuming\n            # tasks from the origal iterator.\n            try:\n                tasks = self._ready_batches.get(block=False)\n            except queue.Empty:\n                # slice the iterator n_jobs * batchsize items at a time. If the\n                # slice returns less than that, then the current batchsize puts\n                # too much weight on a subset of workers, while other may end\n                # up starving. So in this case, re-scale the batch size\n                # accordingly to distribute evenly the last items between all\n                # workers.\n                n_jobs = self._cached_effective_n_jobs\n                big_batch_size = batch_size * n_jobs\n\n                islice = list(itertools.islice(iterator, big_batch_size))\n                if len(islice) == 0:\n                    return False\n                elif (iterator is self._original_iterator\n                      and len(islice) < big_batch_size):\n                    # We reached the end of the original iterator (unless\n                    # iterator is the ``pre_dispatch``-long initial slice of\n                    # the original iterator) -- decrease the batch size to\n                    # account for potential variance in the batches running\n                    # time.\n                    final_batch_size = max(1, len(islice) // (10 * n_jobs))\n                else:\n                    final_batch_size = max(1, len(islice) // n_jobs)\n\n                # enqueue n_jobs batches in a local queue\n                for i in range(0, len(islice), final_batch_size):\n                    tasks = BatchedCalls(islice[i:i + final_batch_size],\n                                         self._backend.get_nested_backend(),\n                                         self._reducer_callback,\n                                         self._pickle_cache)\n                    self._ready_batches.put(tasks)\n\n                # finally, get one task.\n                tasks = self._ready_batches.get(block=False)\n            if len(tasks) == 0:\n                # No more tasks available in the iterator: tell caller to stop.\n                return False\n            else:\n                self._dispatch(tasks)\n                return True\n\n    def _print(self, msg, msg_args):\n        \"\"\"Display the message on stout or stderr depending on verbosity\"\"\"\n        # XXX: Not using the logger framework: need to\n        # learn to use logger better.\n        if not self.verbose:\n            return\n        if self.verbose < 50:\n            writer = sys.stderr.write\n        else:\n            writer = sys.stdout.write\n        msg = msg % msg_args\n        writer('[%s]: %s\\n' % (self, msg))\n\n    def print_progress(self):\n        \"\"\"Display the process of the parallel execution only a fraction\n           of time, controlled by self.verbose.\n        \"\"\"\n        if not self.verbose:\n            return\n        elapsed_time = time.time() - self._start_time\n\n        # Original job iterator becomes None once it has been fully\n        # consumed : at this point we know the total number of jobs and we are\n        # able to display an estimation of the remaining time based on already\n        # completed jobs. Otherwise, we simply display the number of completed\n        # tasks.\n        if self._original_iterator is not None:\n            if _verbosity_filter(self.n_dispatched_batches, self.verbose):\n                return\n            self._print('Done %3i tasks      | elapsed: %s',\n                        (self.n_completed_tasks,\n                         short_format_time(elapsed_time), ))\n        else:\n            index = self.n_completed_tasks\n            # We are finished dispatching\n            total_tasks = self.n_dispatched_tasks\n            # We always display the first loop\n            if not index == 0:\n                # Display depending on the number of remaining items\n                # A message as soon as we finish dispatching, cursor is 0\n                cursor = (total_tasks - index + 1 -\n                          self._pre_dispatch_amount)\n                frequency = (total_tasks // self.verbose) + 1\n                is_last_item = (index + 1 == total_tasks)\n                if (is_last_item or cursor % frequency):\n                    return\n            remaining_time = (elapsed_time / index) * \\\n                             (self.n_dispatched_tasks - index * 1.0)\n            # only display status if remaining time is greater or equal to 0\n            self._print('Done %3i out of %3i | elapsed: %s remaining: %s',\n                        (index,\n                         total_tasks,\n                         short_format_time(elapsed_time),\n                         short_format_time(remaining_time),\n                         ))\n\n    def retrieve(self):\n        self._output = list()\n        while self._iterating or len(self._jobs) > 0:\n            if len(self._jobs) == 0:\n                # Wait for an async callback to dispatch new jobs\n                time.sleep(0.01)\n                continue\n            # We need to be careful: the job list can be filling up as\n            # we empty it and Python list are not thread-safe by default hence\n            # the use of the lock\n            with self._lock:\n                job = self._jobs.pop(0)\n\n            try:\n                if getattr(self._backend, 'supports_timeout', False):\n                    self._output.extend(job.get(timeout=self.timeout))\n                else:\n                    self._output.extend(job.get())\n\n            except BaseException as exception:\n                # Note: we catch any BaseException instead of just Exception\n                # instances to also include KeyboardInterrupt.\n\n                # Stop dispatching any new job in the async callback thread\n                self._aborting = True\n\n                # If the backend allows it, cancel or kill remaining running\n                # tasks without waiting for the results as we will raise\n                # the exception we got back to the caller instead of returning\n                # any result.\n                backend = self._backend\n                if (backend is not None and\n                        hasattr(backend, 'abort_everything')):\n                    # If the backend is managed externally we need to make sure\n                    # to leave it in a working state to allow for future jobs\n                    # scheduling.\n                    ensure_ready = self._managed_backend\n                    backend.abort_everything(ensure_ready=ensure_ready)\n                raise\n\n    def __call__(self, iterable):\n        if self._jobs:\n            raise ValueError('This Parallel instance is already running')\n        # A flag used to abort the dispatching of jobs in case an\n        # exception is found\n        self._aborting = False\n\n        if not self._managed_backend:\n            n_jobs = self._initialize_backend()\n        else:\n            n_jobs = self._effective_n_jobs()\n\n        if isinstance(self._backend, LokyBackend):\n            # For the loky backend, we add a callback executed when reducing\n            # BatchCalls, that makes the loky executor use a temporary folder\n            # specific to this Parallel object when pickling temporary memmaps.\n            # This callback is necessary to ensure that several Parallel\n            # objects using the same resuable executor don't use the same\n            # temporary resources.\n\n            def _batched_calls_reducer_callback():\n                # Relevant implementation detail: the following lines, called\n                # when reducing BatchedCalls, are called in a thread-safe\n                # situation, meaning that the context of the temporary folder\n                # manager will not be changed in between the callback execution\n                # and the end of the BatchedCalls pickling. The reason is that\n                # pickling (the only place where set_current_context is used)\n                # is done from a single thread (the queue_feeder_thread).\n                self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n                    self._id\n                )\n            self._reducer_callback = _batched_calls_reducer_callback\n\n        # self._effective_n_jobs should be called in the Parallel.__call__\n        # thread only -- store its value in an attribute for further queries.\n        self._cached_effective_n_jobs = n_jobs\n\n        backend_name = self._backend.__class__.__name__\n        if n_jobs == 0:\n            raise RuntimeError(\"%s has no active worker.\" % backend_name)\n\n        self._print(\"Using backend %s with %d concurrent workers.\",\n                    (backend_name, n_jobs))\n        if hasattr(self._backend, 'start_call'):\n            self._backend.start_call()\n        iterator = iter(iterable)\n        pre_dispatch = self.pre_dispatch\n\n        if pre_dispatch == 'all' or n_jobs == 1:\n            # prevent further dispatch via multiprocessing callback thread\n            self._original_iterator = None\n            self._pre_dispatch_amount = 0\n        else:\n            self._original_iterator = iterator\n            if hasattr(pre_dispatch, 'endswith'):\n                pre_dispatch = eval(pre_dispatch)\n            self._pre_dispatch_amount = pre_dispatch = int(pre_dispatch)\n\n            # The main thread will consume the first pre_dispatch items and\n            # the remaining items will later be lazily dispatched by async\n            # callbacks upon task completions.\n\n            # TODO: this iterator should be batch_size * n_jobs\n            iterator = itertools.islice(iterator, self._pre_dispatch_amount)\n\n        self._start_time = time.time()\n        self.n_dispatched_batches = 0\n        self.n_dispatched_tasks = 0\n        self.n_completed_tasks = 0\n        # Use a caching dict for callables that are pickled with cloudpickle to\n        # improve performances. This cache is used only in the case of\n        # functions that are defined in the __main__ module, functions that are\n        # defined locally (inside another function) and lambda expressions.\n        self._pickle_cache = dict()\n        try:\n            # Only set self._iterating to True if at least a batch\n            # was dispatched. In particular this covers the edge\n            # case of Parallel used with an exhausted iterator. If\n            # self._original_iterator is None, then this means either\n            # that pre_dispatch == \"all\", n_jobs == 1 or that the first batch\n            # was very quick and its callback already dispatched all the\n            # remaining jobs.\n            self._iterating = False\n            if self.dispatch_one_batch(iterator):\n                self._iterating = self._original_iterator is not None\n\n            while self.dispatch_one_batch(iterator):\n                pass\n\n            if pre_dispatch == \"all\" or n_jobs == 1:\n                # The iterable was consumed all at once by the above for loop.\n                # No need to wait for async callbacks to trigger to\n                # consumption.\n                self._iterating = False\n\n            with self._backend.retrieval_context():\n                self.retrieve()\n            # Make sure that we get a last message telling us we are done\n            elapsed_time = time.time() - self._start_time\n            self._print('Done %3i out of %3i | elapsed: %s finished',\n                        (len(self._output), len(self._output),\n                         short_format_time(elapsed_time)))\n        finally:\n            if hasattr(self._backend, 'stop_call'):\n                self._backend.stop_call()\n            if not self._managed_backend:\n                self._terminate_backend()\n            self._jobs = list()\n            self._pickle_cache = None\n        output = self._output\n        self._output = None\n        return output\n\n    def __repr__(self):\n        return '%s(n_jobs=%s)' % (self.__class__.__name__, self.n_jobs)\n"], "fixing_code": ["Latest changes\n==============\n\nDevelopment version\n-------------------\n\n- Make sure that joblib works even when multiprocessing is not available,\n  for instance with Pyodide\n  https://github.com/joblib/joblib/pull/1256\n\n- Avoid unnecessary warnings when workers and main process delete\n  the temporary memmap folder contents concurrently.\n  https://github.com/joblib/joblib/pull/1263\n\n- Vendor loky 3.1.0 with several fixes to more robustly forcibly terminate\n  worker processes in case of a crash.\n  https://github.com/joblib/joblib/pull/1269\n\n- Fix memory alignment bug for pickles containing numpy arrays.\n  This is especially important when loading the pickle with\n  ``mmap_mode != None`` as the resulting ``numpy.memmap`` object\n  would not be able to correct the misalignment without performing\n  a memory copy.\n  This bug would cause invalid computation and segmentation faults\n  with native code that would directly access the underlying data\n  buffer of a numpy array, for instance C/C++/Cython code compiled\n  with older GCC versions or some old OpenBLAS written in platform\n  specific assembly.\n  https://github.com/joblib/joblib/pull/1254\n\n- Fix a security issue where ``eval(pre_dispatch)`` could potentially run\n  arbitrary code. Now only basic numerics are supported.\n  https://github.com/joblib/joblib/pull/1321\n\nRelease 1.1.0\n--------------\n\n- Fix byte order inconsistency issue during deserialization using joblib.load\n  in cross-endian environment: the numpy arrays are now always loaded to\n  use the system byte order, independently of the byte order of the system\n  that serialized the pickle.\n  https://github.com/joblib/joblib/pull/1181\n\n- Fix joblib.Memory bug with the ``ignore`` parameter when the cached function\n  is a decorated function.\n  https://github.com/joblib/joblib/pull/1165\n\n- Fix `joblib.Memory` to properly handle caching for functions defined\n  interactively in a IPython session or in Jupyter notebook cell.\n  https://github.com/joblib/joblib/pull/1214\n\n- Update vendored loky (from version 2.9 to 3.0) and cloudpickle (from\n  version 1.6 to 2.0)\n  https://github.com/joblib/joblib/pull/1218\n\nRelease 1.0.1\n-------------\n\n- Add check_call_in_cache method to check cache without calling function.\n  https://github.com/joblib/joblib/pull/820\n \n- dask: avoid redundant scattering of large arguments to make a more\n  efficient use of the network resources and avoid crashing dask with\n  \"OSError: [Errno 55] No buffer space available\"\n  or \"ConnectionResetError: [Errno 104] connection reset by peer\".\n  https://github.com/joblib/joblib/pull/1133\n\nRelease 1.0.0\n-------------\n\n- Make `joblib.hash` and `joblib.Memory` caching system compatible with `numpy\n  >= 1.20.0`. Also make it explicit in the documentation that users should now\n  expect to have their `joblib.Memory` cache invalidated when either `joblib`\n  or a third party library involved in the cached values definition is\n  upgraded.  In particular, users updating `joblib` to a release that includes\n  this fix will see their previous cache invalidated if they contained\n  reference to `numpy` objects. \n  https://github.com/joblib/joblib/pull/1136\n\n- Remove deprecated `check_pickle` argument in `delayed`.\n  https://github.com/joblib/joblib/pull/903\n\nRelease 0.17.0\n--------------\n\n- Fix a spurious invalidation of `Memory.cache`'d functions called with\n  `Parallel` under Jupyter or IPython.\n  https://github.com/joblib/joblib/pull/1093\n\n- Bump vendored loky to 2.9.0 and cloudpickle to 1.6.0. In particular\n  this fixes a problem to add compat for Python 3.9.\n\nRelease 0.16.0\n--------------\n\n- Fix a problem in the constructors of of Parallel backends classes that\n  inherit from the `AutoBatchingMixin` that prevented the dask backend to\n  properly batch short tasks.\n  https://github.com/joblib/joblib/pull/1062\n\n- Fix a problem in the way the joblib dask backend batches calls that would\n  badly interact with the dask callable pickling cache and lead to wrong\n  results or errors.\n  https://github.com/joblib/joblib/pull/1055\n\n- Prevent a dask.distributed bug from surfacing in joblib's dask backend\n  during nested Parallel calls (due to joblib's auto-scattering feature)\n  https://github.com/joblib/joblib/pull/1061\n\n- Workaround for a race condition after Parallel calls with the dask backend\n  that would cause low level warnings from asyncio coroutines:\n  https://github.com/joblib/joblib/pull/1078\n\nRelease 0.15.1\n--------------\n\n- Make joblib work on Python 3 installation that do not ship with the lzma\n  package in their standard library.\n\nRelease 0.15.0\n--------------\n\n- Drop support for Python 2 and Python 3.5. All objects in\n  ``joblib.my_exceptions`` and ``joblib.format_stack`` are now deprecated and\n  will be removed in joblib 0.16. Note that no deprecation warning will be\n  raised for these objects Python < 3.7.\n  https://github.com/joblib/joblib/pull/1018\n\n- Fix many bugs related to the temporary files and folder generated when\n  automatically memory mapping large numpy arrays for efficient inter-process\n  communication. In particular, this would cause `PermissionError` exceptions\n  to be raised under Windows and large leaked files in `/dev/shm` under Linux\n  in case of crash.\n  https://github.com/joblib/joblib/pull/966\n\n- Make the dask backend collect results as soon as they complete\n  leading to a performance improvement:\n  https://github.com/joblib/joblib/pull/1025\n\n- Fix the number of jobs reported by ``effective_n_jobs`` when ``n_jobs=None``\n  called in a parallel backend context.\n  https://github.com/joblib/joblib/pull/985\n\n- Upgraded vendored cloupickle to 1.4.1 and loky to 2.8.0. This allows for\n  Parallel calls of dynamically defined functions with type annotations\n  in particular.\n\n\nRelease 0.14.1\n--------------\n\n- Configure the loky workers' environment to mitigate oversubsription with\n  nested multi-threaded code in the following case:\n\n  - allow for a suitable number of threads for numba (``NUMBA_NUM_THREADS``);\n\n  - enable Interprocess Communication for scheduler coordination when the\n    nested code uses Threading Building Blocks (TBB) (``ENABLE_IPC=1``)\n\n  https://github.com/joblib/joblib/pull/951\n\n- Fix a regression where the loky backend was not reusing previously\n  spawned workers.\n  https://github.com/joblib/joblib/pull/968\n\n- Revert https://github.com/joblib/joblib/pull/847 to avoid using\n  `pkg_resources` that introduced a performance regression under Windows:\n  https://github.com/joblib/joblib/issues/965\n\nRelease 0.14.0\n--------------\n\n- Improved the load balancing between workers to avoid stranglers caused by an\n  excessively large batch size when the task duration is varying significantly\n  (because of the combined use of ``joblib.Parallel`` and ``joblib.Memory``\n  with a partially warmed cache for instance).\n  https://github.com/joblib/joblib/pull/899\n\n- Add official support for Python 3.8: fixed protocol number in `Hasher`\n  and updated tests.\n\n- Fix a deadlock when using the dask backend (when scattering large numpy\n  arrays).\n  https://github.com/joblib/joblib/pull/914\n\n- Warn users that they should never use `joblib.load` with files from\n  untrusted sources. Fix security related API change introduced in numpy\n  1.6.3 that would prevent using joblib with recent numpy versions.\n  https://github.com/joblib/joblib/pull/879\n\n- Upgrade to cloudpickle 1.1.1 that add supports for the upcoming\n  Python 3.8 release among other things.\n  https://github.com/joblib/joblib/pull/878\n\n- Fix semaphore availability checker to avoid spawning resource trackers\n  on module import.\n  https://github.com/joblib/joblib/pull/893\n\n- Fix the oversubscription protection to only protect against nested\n  `Parallel` calls. This allows `joblib` to be run in background threads.\n  https://github.com/joblib/joblib/pull/934\n\n- Fix `ValueError` (negative dimensions) when pickling large numpy arrays on\n  Windows.\n  https://github.com/joblib/joblib/pull/920\n\n- Upgrade to loky 2.6.0 that add supports for the setting environment variables\n  in child before loading any module.\n  https://github.com/joblib/joblib/pull/940\n\n- Fix the oversubscription protection for native libraries using threadpools\n  (OpenBLAS, MKL, Blis and OpenMP runtimes).\n  The maximal number of threads is can now be set in children using the\n  ``inner_max_num_threads`` in ``parallel_backend``. It defaults to\n  ``cpu_count() // n_jobs``.\n  https://github.com/joblib/joblib/pull/940\n\n\nRelease 0.13.2\n--------------\n\nPierre Glaser\n\n   Upgrade to cloudpickle 0.8.0\n\n   Add a non-regression test related to joblib issues #836 and #833, reporting\n   that cloudpickle versions between 0.5.4 and 0.7 introduced a bug where\n   global variables changes in a parent process between two calls to\n   joblib.Parallel would not be propagated into the workers\n\n\nRelease 0.13.1\n--------------\n\nPierre Glaser\n\n   Memory now accepts pathlib.Path objects as ``location`` parameter.\n   Also, a warning is raised if the returned backend is None while\n   ``location`` is not None.\n\nOlivier Grisel\n\n   Make ``Parallel`` raise an informative ``RuntimeError`` when the\n   active parallel backend has zero worker.\n\n   Make the ``DaskDistributedBackend`` wait for workers before trying to\n   schedule work. This is useful in particular when the workers are\n   provisionned dynamically but provisionning is not immediate (for\n   instance using Kubernetes, Yarn or an HPC job queue).\n\n\nRelease 0.13.0\n--------------\n\nThomas Moreau\n\n   Include loky 2.4.2 with default serialization with ``cloudpickle``.\n   This can be tweaked with the environment variable ``LOKY_PICKLER``.\n\nThomas Moreau\n\n   Fix nested backend in SequentialBackend to avoid changing the default\n   backend to Sequential. (#792)\n\nThomas Moreau, Olivier Grisel\n\n    Fix nested_backend behavior to avoid setting the default number of\n    workers to -1 when the backend is not dask. (#784)\n\nRelease 0.12.5\n--------------\n\nThomas Moreau, Olivier Grisel\n\n    Include loky 2.3.1 with better error reporting when a worker is\n    abruptly terminated. Also fixes spurious debug output.\n\n\nPierre Glaser\n\n    Include cloudpickle 0.5.6. Fix a bug with the handling of global\n    variables by locally defined functions.\n\n\nRelease 0.12.4\n--------------\n\nThomas Moreau, Pierre Glaser, Olivier Grisel\n\n    Include loky 2.3.0 with many bugfixes, notably w.r.t. when setting\n    non-default multiprocessing contexts. Also include improvement on\n    memory management of long running worker processes and fixed issues\n    when using the loky backend under PyPy.\n\n\nMaxime Weyl\n\n    Raises a more explicit exception when a corrupted MemorizedResult is loaded.\n\nMaxime Weyl\n\n    Loading a corrupted cached file with mmap mode enabled would\n    recompute the results and return them without memory mapping.\n\n\nRelease 0.12.3\n--------------\n\nThomas Moreau\n\n    Fix joblib import setting the global start_method for multiprocessing.\n\nAlexandre Abadie\n\n    Fix MemorizedResult not picklable (#747).\n\nLo\u00efc Est\u00e8ve\n\n    Fix Memory, MemorizedFunc and MemorizedResult round-trip pickling +\n    unpickling (#746).\n\nJames Collins\n\n    Fixed a regression in Memory when positional arguments are called as\n    kwargs several times with different values (#751).\n\nThomas Moreau and Olivier Grisel\n\n    Integration of loky 2.2.2 that fixes issues with the selection of the\n    default start method and improve the reporting when calling functions\n    with arguments that raise an exception when unpickling.\n\n\nMaxime Weyl\n\n    Prevent MemorizedFunc.call_and_shelve from loading cached results to\n    RAM when not necessary. Results in big performance improvements\n\n\nRelease 0.12.2\n--------------\n\nOlivier Grisel\n\n   Integrate loky 2.2.0 to fix regression with unpicklable arguments and\n   functions reported by users (#723, #643).\n\n   Loky 2.2.0 also provides a protection against memory leaks long running\n   applications when psutil is installed (reported as #721).\n\n   Joblib now includes the code for the dask backend which has been updated\n   to properly handle nested parallelism and data scattering at the same\n   time (#722).\n\nAlexandre Abadie and Olivier Grisel\n\n   Restored some private API attribute and arguments\n   (`MemorizedResult.argument_hash` and `BatchedCalls.__init__`'s\n   `pickle_cache`) for backward compat. (#716, #732).\n\n\nJoris Van den Bossche\n\n   Fix a deprecation warning message (for `Memory`'s `cachedir`) (#720).\n\n\nRelease 0.12.1\n--------------\n\nThomas Moreau\n\n    Make sure that any exception triggered when serializing jobs in the queue\n    will be wrapped as a PicklingError as in past versions of joblib.\n\nNoam Hershtig\n\n    Fix kwonlydefaults key error in filter_args (#715)\n\n\nRelease 0.12\n------------\n\nThomas Moreau\n\n    Implement the ``'loky'`` backend with @ogrisel. This backend relies on\n    a robust implementation of ``concurrent.futures.ProcessPoolExecutor``\n    with spawned processes that can be reused across the ``Parallel``\n    calls. This fixes the bad integration with third paty libraries relying on\n    thread pools, described in https://pythonhosted.org/joblib/parallel.html#bad-interaction-of-multiprocessing-and-third-party-libraries\n\n    Limit the number of threads used in worker processes by C-libraries that\n    relies on threadpools. This functionality works for MKL, OpenBLAS, OpenMP\n    and Accelerated.\n\nElizabeth Sander\n\n    Prevent numpy arrays with the same shape and data from hashing to\n    the same memmap, to prevent jobs with preallocated arrays from\n    writing over each other.\n\nOlivier Grisel\n\n    Reduce overhead of automatic memmap by removing the need to hash the\n    array.\n\n    Make ``Memory.cache`` robust to ``PermissionError (errno 13)`` under\n    Windows when run in combination with ``Parallel``.\n\n    The automatic array memory mapping feature of ``Parallel`` does no longer\n    use ``/dev/shm`` if it is too small (less than 2 GB). In particular in\n    docker containers ``/dev/shm`` is only 64 MB by default which would cause\n    frequent failures when running joblib in Docker containers.\n\n    Make it possible to hint for thread-based parallelism with\n    ``prefer='threads'`` or enforce shared-memory semantics with\n    ``require='sharedmem'``.\n\n    Rely on the built-in exception nesting system of Python 3 to preserve\n    traceback information when an exception is raised on a remote worker\n    process. This avoid verbose and redundant exception reports under\n    Python 3.\n\n    Preserve exception type information when doing nested Parallel calls\n    instead of mapping the exception to the generic ``JoblibException`` type.\n\n\nAlexandre Abadie\n\n    Introduce the concept of 'store' and refactor the ``Memory`` internal\n    storage implementation to make it accept extra store backends for caching\n    results. ``backend`` and ``backend_options`` are the new options added to\n    ``Memory`` to specify and configure a store backend.\n\n    Add the ``register_store_backend`` function to extend the store backend\n    used by default with Memory. This default store backend is named 'local'\n    and corresponds to the local filesystem.\n\n    The store backend API is experimental and thus is subject to change in the\n    future without deprecation.\n\n    The ``cachedir`` parameter of ``Memory`` is now marked as deprecated, use\n    ``location`` instead.\n\n    Add support for LZ4 compression if ``lz4`` package is installed.\n\n    Add ``register_compressor`` function for extending available compressors.\n\n    Allow passing a string to ``compress`` parameter in ``dump`` function. This\n    string should correspond to the compressor used (e.g. zlib, gzip, lz4,\n    etc). The default compression level is used in this case.\n\nMatthew Rocklin\n\n    Allow ``parallel_backend`` to be used globally instead of only as a context\n    manager.\n    Support lazy registration of external parallel backends\n\nRelease 0.11\n------------\n\nAlexandre Abadie\n\n    Remove support for python 2.6\n\nAlexandre Abadie\n\n    Remove deprecated `format_signature`, `format_call` and `load_output`\n    functions from Memory API.\n\nLo\u00efc Est\u00e8ve\n\n    Add initial implementation of LRU cache cleaning. You can specify\n    the size limit of a ``Memory`` object via the ``bytes_limit``\n    parameter and then need to clean explicitly the cache via the\n    ``Memory.reduce_size`` method.\n\nOlivier Grisel\n\n    Make the multiprocessing backend work even when the name of the main\n    thread is not the Python default. Thanks to Roman Yurchak for the\n    suggestion.\n\nKaran Desai\n\n    pytest is used to run the tests instead of nosetests.\n    ``python setup.py test`` or ``python setup.py nosetests`` do not work\n    anymore, run ``pytest joblib`` instead.\n\nLo\u00efc Est\u00e8ve\n\n    An instance of ``joblib.ParallelBackendBase`` can be passed into\n    the ``parallel`` argument in ``joblib.Parallel``.\n\n\nLo\u00efc Est\u00e8ve\n\n    Fix handling of memmap objects with offsets greater than\n    mmap.ALLOCATIONGRANULARITY in ``joblib.Parallel``. See\n    https://github.com/joblib/joblib/issues/451 for more details.\n\nLo\u00efc Est\u00e8ve\n\n    Fix performance regression in ``joblib.Parallel`` with\n    n_jobs=1. See https://github.com/joblib/joblib/issues/483 for more\n    details.\n\nLo\u00efc Est\u00e8ve\n\n    Fix race condition when a function cached with\n    ``joblib.Memory.cache`` was used inside a ``joblib.Parallel``. See\n    https://github.com/joblib/joblib/issues/490 for more details.\n\nRelease 0.10.3\n--------------\n\nLo\u00efc Est\u00e8ve\n\n    Fix tests when multiprocessing is disabled via the\n    JOBLIB_MULTIPROCESSING environment variable.\n\nharishmk\n\n    Remove warnings in nested Parallel objects when the inner Parallel\n    has n_jobs=1. See https://github.com/joblib/joblib/pull/406 for\n    more details.\n\nRelease 0.10.2\n--------------\n\nLo\u00efc Est\u00e8ve\n\n    FIX a bug in stack formatting when the error happens in a compiled\n    extension. See https://github.com/joblib/joblib/pull/382 for more\n    details.\n\nVincent Latrouite\n\n    FIX a bug in the constructor of BinaryZlibFile that would throw an\n    exception when passing unicode filename (Python 2 only).\n    See https://github.com/joblib/joblib/pull/384 for more details.\n\nOlivier Grisel\n\n    Expose :class:`joblib.parallel.ParallelBackendBase` and\n    :class:`joblib.parallel.AutoBatchingMixin` in the public API to\n    make them officially re-usable by backend implementers.\n\n\nRelease 0.10.0\n--------------\n\nAlexandre Abadie\n\n    ENH: joblib.dump/load now accept file-like objects besides filenames.\n    https://github.com/joblib/joblib/pull/351 for more details.\n\nNiels Zeilemaker and Olivier Grisel\n\n    Refactored joblib.Parallel to enable the registration of custom\n    computational backends.\n    https://github.com/joblib/joblib/pull/306\n    Note the API to register custom backends is considered experimental\n    and subject to change without deprecation.\n\nAlexandre Abadie\n\n    Joblib pickle format change: joblib.dump always create a single pickle file\n    and joblib.dump/joblib.save never do any memory copy when writing/reading\n    pickle files. Reading pickle files generated with joblib versions prior\n    to 0.10 will be supported for a limited amount of time, we advise to\n    regenerate them from scratch when convenient.\n    joblib.dump and joblib.load also support pickle files compressed using\n    various strategies: zlib, gzip, bz2, lzma and xz. Note that lzma and xz are\n    only available with python >= 3.3.\n    https://github.com/joblib/joblib/pull/260 for more details.\n\nAntony Lee\n\n    ENH: joblib.dump/load now accept pathlib.Path objects as filenames.\n    https://github.com/joblib/joblib/pull/316 for more details.\n\nOlivier Grisel\n\n    Workaround for \"WindowsError: [Error 5] Access is denied\" when trying to\n    terminate a multiprocessing pool under Windows:\n    https://github.com/joblib/joblib/issues/354\n\n\nRelease 0.9.4\n-------------\n\nOlivier Grisel\n\n    FIX a race condition that could cause a joblib.Parallel to hang\n    when collecting the result of a job that triggers an exception.\n    https://github.com/joblib/joblib/pull/296\n\nOlivier Grisel\n\n    FIX a bug that caused joblib.Parallel to wrongly reuse previously\n    memmapped arrays instead of creating new temporary files.\n    https://github.com/joblib/joblib/pull/294 for more details.\n\nLo\u00efc Est\u00e8ve\n\n    FIX for raising non inheritable exceptions in a Parallel call. See\n    https://github.com/joblib/joblib/issues/269 for more details.\n\nAlexandre Abadie\n\n    FIX joblib.hash error with mixed types sets and dicts containing mixed\n    types keys when using Python 3.\n    see https://github.com/joblib/joblib/issues/254\n\nLo\u00efc Est\u00e8ve\n\n    FIX joblib.dump/load for big numpy arrays with dtype=object. See\n    https://github.com/joblib/joblib/issues/220 for more details.\n\nLo\u00efc Est\u00e8ve\n\n    FIX joblib.Parallel hanging when used with an exhausted\n    iterator. See https://github.com/joblib/joblib/issues/292 for more\n    details.\n\nRelease 0.9.3\n-------------\n\nOlivier Grisel\n\n    Revert back to the ``fork`` start method (instead of\n    ``forkserver``) as the latter was found to cause crashes in\n    interactive Python sessions.\n\nRelease 0.9.2\n-------------\n\nLo\u00efc Est\u00e8ve\n\n    Joblib hashing now uses the default pickle protocol (2 for Python\n    2 and 3 for Python 3). This makes it very unlikely to get the same\n    hash for a given object under Python 2 and Python 3.\n\n    In particular, for Python 3 users, this means that the output of\n    joblib.hash changes when switching from joblib 0.8.4 to 0.9.2 . We\n    strive to ensure that the output of joblib.hash does not change\n    needlessly in future versions of joblib but this is not officially\n    guaranteed.\n\nLo\u00efc Est\u00e8ve\n\n    Joblib pickles generated with Python 2 can not be loaded with\n    Python 3 and the same applies for joblib pickles generated with\n    Python 3 and loaded with Python 2.\n\n    During the beta period 0.9.0b2 to 0.9.0b4, we experimented with\n    a joblib serialization that aimed to make pickles serialized with\n    Python 3 loadable under Python 2. Unfortunately this serialization\n    strategy proved to be too fragile as far as the long-term\n    maintenance was concerned (For example see\n    https://github.com/joblib/joblib/pull/243). That means that joblib\n    pickles generated with joblib 0.9.0bN can not be loaded under\n    joblib 0.9.2. Joblib beta testers, who are the only ones likely to\n    be affected by this, are advised to delete their joblib cache when\n    they upgrade from 0.9.0bN to 0.9.2.\n\nArthur Mensch\n\n    Fixed a bug with ``joblib.hash`` that used to return unstable values for\n    strings and numpy.dtype instances depending on interning states.\n\nOlivier Grisel\n\n    Make joblib use the 'forkserver' start method by default under Python 3.4+\n    to avoid causing crash with 3rd party libraries (such as Apple vecLib /\n    Accelerate or the GCC OpenMP runtime) that use an internal thread pool that\n    is not not reinitialized when a ``fork`` system call happens.\n\nOlivier Grisel\n\n    New context manager based API (``with`` block) to re-use\n    the same pool of workers across consecutive parallel calls.\n\nVlad Niculae and Olivier Grisel\n\n    Automated batching of fast tasks into longer running jobs to\n    hide multiprocessing dispatching overhead when possible.\n\nOlivier Grisel\n\n    FIX make it possible to call ``joblib.load(filename, mmap_mode='r')``\n    on pickled objects that include a mix of arrays of both\n    memory memmapable dtypes and object dtype.\n\n\nRelease 0.8.4\n-------------\n\n2014-11-20\nOlivier Grisel\n\n    OPTIM use the C-optimized pickler under Python 3\n\n    This makes it possible to efficiently process parallel jobs that deal with\n    numerous Python objects such as large dictionaries.\n\n\nRelease 0.8.3\n-------------\n\n2014-08-19\nOlivier Grisel\n\n    FIX disable memmapping for object arrays\n\n2014-08-07\nLars Buitinck\n\n    MAINT NumPy 1.10-safe version comparisons\n\n\n2014-07-11\nOlivier Grisel\n\n    FIX #146: Heisen test failure caused by thread-unsafe Python lists\n\n    This fix uses a queue.Queue datastructure in the failing test. This\n    datastructure is thread-safe thanks to an internal Lock. This Lock instance\n    not picklable hence cause the picklability check of delayed to check fail.\n\n    When using the threading backend, picklability is no longer required, hence\n    this PRs give the user the ability to disable it on a case by case basis.\n\n\nRelease 0.8.2\n-------------\n\n2014-06-30\nOlivier Grisel\n\n    BUG: use mmap_mode='r' by default in Parallel and MemmappingPool\n\n    The former default of mmap_mode='c' (copy-on-write) caused\n    problematic use of the paging file under Windows.\n\n2014-06-27\nOlivier Grisel\n\n    BUG: fix usage of the /dev/shm folder under Linux\n\n\nRelease 0.8.1\n-------------\n\n2014-05-29\nGael Varoquaux\n\n    BUG: fix crash with high verbosity\n\n\nRelease 0.8.0\n-------------\n\n2014-05-14\nOlivier Grisel\n\n   Fix a bug in exception reporting under Python 3\n\n2014-05-10\nOlivier Grisel\n\n   Fixed a potential segfault when passing non-contiguous memmap\n   instances.\n\n2014-04-22\nGael Varoquaux\n\n    ENH: Make memory robust to modification of source files while the\n    interpreter is running. Should lead to less spurious cache flushes\n    and recomputations.\n\n\n2014-02-24\nPhilippe Gervais\n\n   New ``Memory.call_and_shelve`` API to handle memoized results by\n   reference instead of by value.\n\n\nRelease 0.8.0a3\n---------------\n\n2014-01-10\nOlivier Grisel & Gael Varoquaux\n\n   FIX #105: Race condition in task iterable consumption when\n   pre_dispatch != 'all' that could cause crash with error messages \"Pools\n   seems closed\" and \"ValueError: generator already executing\".\n\n2014-01-12\nOlivier Grisel\n\n   FIX #72: joblib cannot persist \"output_dir\" keyword argument.\n\n\nRelease 0.8.0a2\n---------------\n\n2013-12-23\nOlivier Grisel\n\n    ENH: set default value of Parallel's max_nbytes to 100MB\n\n    Motivation: avoid introducing disk latency on medium sized\n    parallel workload where memory usage is not an issue.\n\n    FIX: properly handle the JOBLIB_MULTIPROCESSING env variable\n\n    FIX: timeout test failures under windows\n\n\nRelease 0.8.0a\n--------------\n\n2013-12-19\nOlivier Grisel\n\n    FIX: support the new Python 3.4 multiprocessing API\n\n\n2013-12-05\nOlivier Grisel\n\n    ENH: make Memory respect mmap_mode at first call too\n\n    ENH: add a threading based backend to Parallel\n\n    This is low overhead alternative backend to the default multiprocessing\n    backend that is suitable when calling compiled extensions that release\n    the GIL.\n\n\nAuthor: Dan Stahlke <dan@stahlke.org>\nDate:   2013-11-08\n\n    FIX: use safe_repr to print arg vals in trace\n\n    This fixes a problem in which extremely long (and slow) stack traces would\n    be produced when function parameters are large numpy arrays.\n\n\n2013-09-10\nOlivier Grisel\n\n    ENH: limit memory copy with Parallel by leveraging numpy.memmap when\n    possible\n\n\nRelease 0.7.1\n---------------\n\n2013-07-25\nGael Varoquaux\n\n    MISC: capture meaningless argument (n_jobs=0) in Parallel\n\n2013-07-09\nLars Buitinck\n\n    ENH Handles tuples, sets and Python 3's dict_keys type the same as\n    lists. in pre_dispatch\n\n2013-05-23\nMartin Luessi\n\n    ENH: fix function caching for IPython\n\nRelease 0.7.0\n---------------\n\n**This release drops support for Python 2.5 in favor of support for\nPython 3.0**\n\n2013-02-13\nGael Varoquaux\n\n    BUG: fix nasty hash collisions\n\n2012-11-19\nGael Varoquaux\n\n    ENH: Parallel: Turn of pre-dispatch for already expanded lists\n\n\nGael Varoquaux\n2012-11-19\n\n    ENH: detect recursive sub-process spawning, as when people do not\n    protect the __main__ in scripts under Windows, and raise a useful\n    error.\n\n\nGael Varoquaux\n2012-11-16\n\n    ENH: Full python 3 support\n\nRelease 0.6.5\n---------------\n\n2012-09-15\nYannick Schwartz\n\n    BUG: make sure that sets and dictionaries give reproducible hashes\n\n\n2012-07-18\nMarek Rudnicki\n\n    BUG: make sure that object-dtype numpy array hash correctly\n\n2012-07-12\nGaelVaroquaux\n\n    BUG: Bad default n_jobs for Parallel\n\nRelease 0.6.4\n---------------\n\n2012-05-07\nVlad Niculae\n\n    ENH: controlled randomness in tests and doctest fix\n\n2012-02-21\nGaelVaroquaux\n\n    ENH: add verbosity in memory\n\n2012-02-21\nGaelVaroquaux\n\n    BUG: non-reproducible hashing: order of kwargs\n\n    The ordering of a dictionary is random. As a result the function hashing\n    was not reproducible. Pretty hard to test\n\nRelease 0.6.3\n---------------\n\n2012-02-14\nGaelVaroquaux\n\n    BUG: fix joblib Memory pickling\n\n2012-02-11\nGaelVaroquaux\n\n    BUG: fix hasher with Python 3\n\n2012-02-09\nGaelVaroquaux\n\n    API: filter_args:  `*args, **kwargs -> args, kwargs`\n\nRelease 0.6.2\n---------------\n\n2012-02-06\nGael Varoquaux\n\n    BUG: make sure Memory pickles even if cachedir=None\n\nRelease 0.6.1\n---------------\n\nBugfix release because of a merge error in release 0.6.0\n\nRelease 0.6.0\n---------------\n\n**Beta 3**\n\n2012-01-11\nGael Varoquaux\n\n    BUG: ensure compatibility with old numpy\n\n    DOC: update installation instructions\n\n    BUG: file semantic to work under Windows\n\n2012-01-10\nYaroslav Halchenko\n\n    BUG: a fix toward 2.5 compatibility\n\n**Beta 2**\n\n2012-01-07\nGael Varoquaux\n\n    ENH: hash: bugware to be able to hash objects defined interactively\n    in IPython\n\n2012-01-07\nGael Varoquaux\n\n    ENH: Parallel: warn and not fail for nested loops\n\n    ENH: Parallel: n_jobs=-2 now uses all CPUs but one\n\n2012-01-01\nJuan Manuel Caicedo Carvajal and Gael Varoquaux\n\n    ENH: add verbosity levels in Parallel\n\nRelease 0.5.7\n---------------\n\n2011-12-28\nGael varoquaux\n\n    API: zipped -> compress\n\n2011-12-26\nGael varoquaux\n\n    ENH: Add a zipped option to Memory\n\n    API: Memory no longer accepts save_npy\n\n2011-12-22\nKenneth C. Arnold and Gael varoquaux\n\n    BUG: fix numpy_pickle for array subclasses\n\n2011-12-21\nGael varoquaux\n\n    ENH: add zip-based pickling\n\n2011-12-19\nFabian Pedregosa\n\n    Py3k: compatibility fixes.\n    This makes run fine the tests test_disk and test_parallel\n\nRelease 0.5.6\n---------------\n\n2011-12-11\nLars Buitinck\n\n    ENH: Replace os.path.exists before makedirs with exception check\n    New disk.mkdirp will fail with other errnos than EEXIST.\n\n2011-12-10\nBala Subrahmanyam Varanasi\n\n    MISC: pep8 compliant\n\n\nRelease 0.5.5\n---------------\n\n2011-19-10\nFabian Pedregosa\n\n    ENH: Make joblib installable under Python 3.X\n\nRelease 0.5.4\n---------------\n\n2011-09-29\nJon Olav Vik\n\n    BUG: Make mangling path to filename work on Windows\n\n2011-09-25\nOlivier Grisel\n\n    FIX: doctest heisenfailure on execution time\n\n2011-08-24\nRalf Gommers\n\n    STY: PEP8 cleanup.\n\n\nRelease 0.5.3\n---------------\n\n2011-06-25\nGael varoquaux\n\n   API: All the useful symbols in the __init__\n\n\nRelease 0.5.2\n---------------\n\n2011-06-25\nGael varoquaux\n\n    ENH: Add cpu_count\n\n2011-06-06\nGael varoquaux\n\n    ENH: Make sure memory hash in a reproducible way\n\n\nRelease 0.5.1\n---------------\n\n2011-04-12\nGael varoquaux\n\n    TEST: Better testing of parallel and pre_dispatch\n\nYaroslav Halchenko\n2011-04-12\n\n    DOC: quick pass over docs -- trailing spaces/spelling\n\nYaroslav Halchenko\n2011-04-11\n\n    ENH: JOBLIB_MULTIPROCESSING env var to disable multiprocessing from the\n    environment\n\nAlexandre Gramfort\n2011-04-08\n\n    ENH : adding log message to know how long it takes to load from disk the\n    cache\n\n\nRelease 0.5.0\n---------------\n\n2011-04-01\nGael varoquaux\n\n    BUG: pickling MemoizeFunc does not store timestamp\n\n2011-03-31\nNicolas Pinto\n\n    TEST: expose hashing bug with cached method\n\n2011-03-26...2011-03-27\nPietro Berkes\n\n    BUG: fix error management in rm_subdirs\n    BUG: fix for race condition during tests in mem.clear()\n\nGael varoquaux\n2011-03-22...2011-03-26\n\n    TEST: Improve test coverage and robustness\n\nGael varoquaux\n2011-03-19\n\n    BUG: hashing functions with only \\*var \\**kwargs\n\nGael varoquaux\n2011-02-01... 2011-03-22\n\n    BUG: Many fixes to capture interprocess race condition when mem.cache\n    is used by several processes on the same cache.\n\nFabian Pedregosa\n2011-02-28\n\n    First work on Py3K compatibility\n\nGael varoquaux\n2011-02-27\n\n    ENH: pre_dispatch in parallel: lazy generation of jobs in parallel\n    for to avoid drowning memory.\n\nGaelVaroquaux\n2011-02-24\n\n    ENH: Add the option of overloading the arguments of the mother\n    'Memory' object in the cache method that is doing the decoration.\n\nGael varoquaux\n2010-11-21\n\n    ENH: Add a verbosity level for more verbosity\n\nRelease 0.4.6\n----------------\n\nGael varoquaux\n2010-11-15\n\n    ENH: Deal with interruption in parallel\n\nGael varoquaux\n2010-11-13\n\n    BUG: Exceptions raised by Parallel when n_job=1 are no longer captured.\n\nGael varoquaux\n2010-11-13\n\n    BUG: Capture wrong arguments properly (better error message)\n\n\nRelease 0.4.5\n----------------\n\nPietro Berkes\n2010-09-04\n\n    BUG: Fix Windows peculiarities with path separators and file names\n    BUG: Fix more windows locking bugs\n\nGael varoquaux\n2010-09-03\n\n    ENH: Make sure that exceptions raised in Parallel also inherit from\n    the original exception class\n    ENH: Add a shadow set of exceptions\n\nFabian Pedregosa\n2010-09-01\n\n    ENH: Clean up the code for parallel. Thanks to Fabian Pedregosa for\n    the patch.\n\n\nRelease 0.4.4\n----------------\n\nGael varoquaux\n2010-08-23\n\n    BUG: Fix Parallel on computers with only one CPU, for n_jobs=-1.\n\nGael varoquaux\n2010-08-02\n\n    BUG: Fix setup.py for extra setuptools args.\n\nGael varoquaux\n2010-07-29\n\n    MISC: Silence tests (and hopefully Yaroslav :P)\n\nRelease 0.4.3\n----------------\n\nGael Varoquaux\n2010-07-22\n\n    BUG: Fix hashing for function with a side effect modifying their input\n    argument. Thanks to Pietro Berkes for reporting the bug and proving the\n    patch.\n\nRelease 0.4.2\n----------------\n\nGael Varoquaux\n2010-07-16\n\n    BUG: Make sure that joblib still works with Python2.5. => release 0.4.2\n\nRelease 0.4.1\n----------------\n", "\"\"\"\nHelpers for embarrassingly parallel code.\n\"\"\"\n# Author: Gael Varoquaux < gael dot varoquaux at normalesup dot org >\n# Copyright: 2010, Gael Varoquaux\n# License: BSD 3 clause\n\nfrom __future__ import division\n\nimport os\nimport sys\nfrom math import sqrt\nimport functools\nimport time\nimport threading\nimport itertools\nfrom uuid import uuid4\nfrom numbers import Integral\nimport warnings\nimport queue\n\nfrom ._multiprocessing_helpers import mp\n\nfrom .logger import Logger, short_format_time\nfrom .disk import memstr_to_bytes\nfrom ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n                                 ThreadingBackend, SequentialBackend,\n                                 LokyBackend)\nfrom .externals.cloudpickle import dumps, loads\n\n# Make sure that those two classes are part of the public joblib.parallel API\n# so that 3rd party backend implementers can import them from here.\nfrom ._parallel_backends import AutoBatchingMixin  # noqa\nfrom ._parallel_backends import ParallelBackendBase  # noqa\n\n\nBACKENDS = {\n    'threading': ThreadingBackend,\n    'sequential': SequentialBackend,\n}\n# name of the backend used by default by Parallel outside of any context\n# managed by ``parallel_backend``.\n\n# threading is the only backend that is always everywhere\nDEFAULT_BACKEND = 'threading'\n\nDEFAULT_N_JOBS = 1\n\nMAYBE_AVAILABLE_BACKENDS = {'multiprocessing', 'loky'}\n\n# if multiprocessing is available, so is loky, we set it as the default\n# backend\nif mp is not None:\n    BACKENDS['multiprocessing'] = MultiprocessingBackend\n    from .externals import loky\n    BACKENDS['loky'] = LokyBackend\n    DEFAULT_BACKEND = 'loky'\n\n\nDEFAULT_THREAD_BACKEND = 'threading'\n\n# Thread local value that can be overridden by the ``parallel_backend`` context\n# manager\n_backend = threading.local()\n\nVALID_BACKEND_HINTS = ('processes', 'threads', None)\nVALID_BACKEND_CONSTRAINTS = ('sharedmem', None)\n\n\ndef _register_dask():\n    \"\"\" Register Dask Backend if called with parallel_backend(\"dask\") \"\"\"\n    try:\n        from ._dask import DaskDistributedBackend\n        register_parallel_backend('dask', DaskDistributedBackend)\n    except ImportError as e:\n        msg = (\"To use the dask.distributed backend you must install both \"\n               \"the `dask` and distributed modules.\\n\\n\"\n               \"See https://dask.pydata.org/en/latest/install.html for more \"\n               \"information.\")\n        raise ImportError(msg) from e\n\n\nEXTERNAL_BACKENDS = {\n    'dask': _register_dask,\n}\n\n\ndef get_active_backend(prefer=None, require=None, verbose=0):\n    \"\"\"Return the active default backend\"\"\"\n    if prefer not in VALID_BACKEND_HINTS:\n        raise ValueError(\"prefer=%r is not a valid backend hint, \"\n                         \"expected one of %r\" % (prefer, VALID_BACKEND_HINTS))\n    if require not in VALID_BACKEND_CONSTRAINTS:\n        raise ValueError(\"require=%r is not a valid backend constraint, \"\n                         \"expected one of %r\"\n                         % (require, VALID_BACKEND_CONSTRAINTS))\n\n    if prefer == 'processes' and require == 'sharedmem':\n        raise ValueError(\"prefer == 'processes' and require == 'sharedmem'\"\n                         \" are inconsistent settings\")\n    backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)\n    if backend_and_jobs is not None:\n        # Try to use the backend set by the user with the context manager.\n        backend, n_jobs = backend_and_jobs\n        nesting_level = backend.nesting_level\n        supports_sharedmem = getattr(backend, 'supports_sharedmem', False)\n        if require == 'sharedmem' and not supports_sharedmem:\n            # This backend does not match the shared memory constraint:\n            # fallback to the default thead-based backend.\n            sharedmem_backend = BACKENDS[DEFAULT_THREAD_BACKEND](\n                nesting_level=nesting_level)\n            if verbose >= 10:\n                print(\"Using %s as joblib.Parallel backend instead of %s \"\n                      \"as the latter does not provide shared memory semantics.\"\n                      % (sharedmem_backend.__class__.__name__,\n                         backend.__class__.__name__))\n            return sharedmem_backend, DEFAULT_N_JOBS\n        else:\n            return backend_and_jobs\n\n    # We are outside of the scope of any parallel_backend context manager,\n    # create the default backend instance now.\n    backend = BACKENDS[DEFAULT_BACKEND](nesting_level=0)\n    supports_sharedmem = getattr(backend, 'supports_sharedmem', False)\n    uses_threads = getattr(backend, 'uses_threads', False)\n    if ((require == 'sharedmem' and not supports_sharedmem) or\n            (prefer == 'threads' and not uses_threads)):\n        # Make sure the selected default backend match the soft hints and\n        # hard constraints:\n        backend = BACKENDS[DEFAULT_THREAD_BACKEND](nesting_level=0)\n    return backend, DEFAULT_N_JOBS\n\n\nclass parallel_backend(object):\n    \"\"\"Change the default backend used by Parallel inside a with block.\n\n    If ``backend`` is a string it must match a previously registered\n    implementation using the :func:`~register_parallel_backend` function.\n\n    By default the following backends are available:\n\n    - 'loky': single-host, process-based parallelism (used by default),\n    - 'threading': single-host, thread-based parallelism,\n    - 'multiprocessing': legacy single-host, process-based parallelism.\n\n    'loky' is recommended to run functions that manipulate Python objects.\n    'threading' is a low-overhead alternative that is most efficient for\n    functions that release the Global Interpreter Lock: e.g. I/O-bound code or\n    CPU-bound code in a few calls to native code that explicitly releases the\n    GIL. Note that on some rare systems (such as pyiodine),\n    multiprocessing and loky may not be available, in which case joblib\n    defaults to threading.\n\n    In addition, if the `dask` and `distributed` Python packages are installed,\n    it is possible to use the 'dask' backend for better scheduling of nested\n    parallel calls without over-subscription and potentially distribute\n    parallel calls over a networked cluster of several hosts.\n\n    It is also possible to use the distributed 'ray' backend for distributing\n    the workload to a cluster of nodes. To use the 'ray' joblib backend add\n    the following lines::\n\n     >>> from ray.util.joblib import register_ray  # doctest: +SKIP\n     >>> register_ray()  # doctest: +SKIP\n     >>> with parallel_backend(\"ray\"):  # doctest: +SKIP\n     ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n     [-1, -2, -3, -4, -5]\n\n    Alternatively the backend can be passed directly as an instance.\n\n    By default all available workers will be used (``n_jobs=-1``) unless the\n    caller passes an explicit value for the ``n_jobs`` parameter.\n\n    This is an alternative to passing a ``backend='backend_name'`` argument to\n    the :class:`~Parallel` class constructor. It is particularly useful when\n    calling into library code that uses joblib internally but does not expose\n    the backend argument in its own API.\n\n    >>> from operator import neg\n    >>> with parallel_backend('threading'):\n    ...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n    ...\n    [-1, -2, -3, -4, -5]\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    Joblib also tries to limit the oversubscription by limiting the number of\n    threads usable in some third-party library threadpools like OpenBLAS, MKL\n    or OpenMP. The default limit in each worker is set to\n    ``max(cpu_count() // effective_n_jobs, 1)`` but this limit can be\n    overwritten with the ``inner_max_num_threads`` argument which will be used\n    to set this limit in the child processes.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    def __init__(self, backend, n_jobs=-1, inner_max_num_threads=None,\n                 **backend_params):\n        if isinstance(backend, str):\n            if backend not in BACKENDS:\n                if backend in EXTERNAL_BACKENDS:\n                    register = EXTERNAL_BACKENDS[backend]\n                    register()\n                elif backend in MAYBE_AVAILABLE_BACKENDS:\n                    warnings.warn(\n                        f\"joblib backend '{backend}' is not available on \"\n                        f\"your system, falling back to {DEFAULT_BACKEND}.\",\n                        UserWarning,\n                        stacklevel=2)\n                    BACKENDS[backend] = BACKENDS[DEFAULT_BACKEND]\n                else:\n                    raise ValueError(\"Invalid backend: %s, expected one of %r\"\n                                     % (backend, sorted(BACKENDS.keys())))\n\n            backend = BACKENDS[backend](**backend_params)\n\n        if inner_max_num_threads is not None:\n            msg = (\"{} does not accept setting the inner_max_num_threads \"\n                   \"argument.\".format(backend.__class__.__name__))\n            assert backend.supports_inner_max_num_threads, msg\n            backend.inner_max_num_threads = inner_max_num_threads\n\n        # If the nesting_level of the backend is not set previously, use the\n        # nesting level from the previous active_backend to set it\n        current_backend_and_jobs = getattr(_backend, 'backend_and_jobs', None)\n        if backend.nesting_level is None:\n            if current_backend_and_jobs is None:\n                nesting_level = 0\n            else:\n                nesting_level = current_backend_and_jobs[0].nesting_level\n\n            backend.nesting_level = nesting_level\n\n        # Save the backends info and set the active backend\n        self.old_backend_and_jobs = current_backend_and_jobs\n        self.new_backend_and_jobs = (backend, n_jobs)\n\n        _backend.backend_and_jobs = (backend, n_jobs)\n\n    def __enter__(self):\n        return self.new_backend_and_jobs\n\n    def __exit__(self, type, value, traceback):\n        self.unregister()\n\n    def unregister(self):\n        if self.old_backend_and_jobs is None:\n            if getattr(_backend, 'backend_and_jobs', None) is not None:\n                del _backend.backend_and_jobs\n        else:\n            _backend.backend_and_jobs = self.old_backend_and_jobs\n\n\n# Under Linux or OS X the default start method of multiprocessing\n# can cause third party libraries to crash. Under Python 3.4+ it is possible\n# to set an environment variable to switch the default start method from\n# 'fork' to 'forkserver' or 'spawn' to avoid this issue albeit at the cost\n# of causing semantic changes and some additional pool instantiation overhead.\nDEFAULT_MP_CONTEXT = None\nif hasattr(mp, 'get_context'):\n    method = os.environ.get('JOBLIB_START_METHOD', '').strip() or None\n    if method is not None:\n        DEFAULT_MP_CONTEXT = mp.get_context(method=method)\n\n\nclass BatchedCalls(object):\n    \"\"\"Wrap a sequence of (func, args, kwargs) tuples as a single callable\"\"\"\n\n    def __init__(self, iterator_slice, backend_and_jobs, reducer_callback=None,\n                 pickle_cache=None):\n        self.items = list(iterator_slice)\n        self._size = len(self.items)\n        self._reducer_callback = reducer_callback\n        if isinstance(backend_and_jobs, tuple):\n            self._backend, self._n_jobs = backend_and_jobs\n        else:\n            # this is for backward compatibility purposes. Before 0.12.6,\n            # nested backends were returned without n_jobs indications.\n            self._backend, self._n_jobs = backend_and_jobs, None\n        self._pickle_cache = pickle_cache if pickle_cache is not None else {}\n\n    def __call__(self):\n        # Set the default nested backend to self._backend but do not set the\n        # change the default number of processes to -1\n        with parallel_backend(self._backend, n_jobs=self._n_jobs):\n            return [func(*args, **kwargs)\n                    for func, args, kwargs in self.items]\n\n    def __reduce__(self):\n        if self._reducer_callback is not None:\n            self._reducer_callback()\n        # no need pickle the callback.\n        return (\n            BatchedCalls,\n            (self.items, (self._backend, self._n_jobs), None,\n             self._pickle_cache)\n        )\n\n    def __len__(self):\n        return self._size\n\n\n###############################################################################\n# CPU count that works also when multiprocessing has been disabled via\n# the JOBLIB_MULTIPROCESSING environment variable\ndef cpu_count(only_physical_cores=False):\n    \"\"\"Return the number of CPUs.\n\n    This delegates to loky.cpu_count that takes into account additional\n    constraints such as Linux CFS scheduler quotas (typically set by container\n    runtimes such as docker) and CPU affinity (for instance using the taskset\n    command on Linux).\n\n    If only_physical_cores is True, do not take hyperthreading / SMT logical\n    cores into account.\n    \"\"\"\n    if mp is None:\n        return 1\n\n    return loky.cpu_count(only_physical_cores=only_physical_cores)\n\n\n###############################################################################\n# For verbosity\n\ndef _verbosity_filter(index, verbose):\n    \"\"\" Returns False for indices increasingly apart, the distance\n        depending on the value of verbose.\n\n        We use a lag increasing as the square of index\n    \"\"\"\n    if not verbose:\n        return True\n    elif verbose > 10:\n        return False\n    if index == 0:\n        return False\n    verbose = .5 * (11 - verbose) ** 2\n    scale = sqrt(index / verbose)\n    next_scale = sqrt((index + 1) / verbose)\n    return (int(next_scale) == int(scale))\n\n\n###############################################################################\ndef delayed(function):\n    \"\"\"Decorator used to capture the arguments of a function.\"\"\"\n\n    def delayed_function(*args, **kwargs):\n        return function, args, kwargs\n    try:\n        delayed_function = functools.wraps(function)(delayed_function)\n    except AttributeError:\n        \" functools.wraps fails on some callable objects \"\n    return delayed_function\n\n\n###############################################################################\nclass BatchCompletionCallBack(object):\n    \"\"\"Callback used by joblib.Parallel's multiprocessing backend.\n\n    This callable is executed by the parent process whenever a worker process\n    has returned the results of a batch of tasks.\n\n    It is used for progress reporting, to update estimate of the batch\n    processing duration and to schedule the next batch of tasks to be\n    processed.\n\n    \"\"\"\n    def __init__(self, dispatch_timestamp, batch_size, parallel):\n        self.dispatch_timestamp = dispatch_timestamp\n        self.batch_size = batch_size\n        self.parallel = parallel\n\n    def __call__(self, out):\n        self.parallel.n_completed_tasks += self.batch_size\n        this_batch_duration = time.time() - self.dispatch_timestamp\n\n        self.parallel._backend.batch_completed(self.batch_size,\n                                               this_batch_duration)\n        self.parallel.print_progress()\n        with self.parallel._lock:\n            if self.parallel._original_iterator is not None:\n                self.parallel.dispatch_next()\n\n\n###############################################################################\ndef register_parallel_backend(name, factory, make_default=False):\n    \"\"\"Register a new Parallel backend factory.\n\n    The new backend can then be selected by passing its name as the backend\n    argument to the :class:`~Parallel` class. Moreover, the default backend can\n    be overwritten globally by setting make_default=True.\n\n    The factory can be any callable that takes no argument and return an\n    instance of ``ParallelBackendBase``.\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    BACKENDS[name] = factory\n    if make_default:\n        global DEFAULT_BACKEND\n        DEFAULT_BACKEND = name\n\n\ndef effective_n_jobs(n_jobs=-1):\n    \"\"\"Determine the number of jobs that can actually run in parallel\n\n    n_jobs is the number of workers requested by the callers. Passing n_jobs=-1\n    means requesting all available workers for instance matching the number of\n    CPU cores on the worker host(s).\n\n    This method should return a guesstimate of the number of workers that can\n    actually perform work concurrently with the currently enabled default\n    backend. The primary use case is to make it possible for the caller to know\n    in how many chunks to slice the work.\n\n    In general working on larger data chunks is more efficient (less scheduling\n    overhead and better use of CPU cache prefetching heuristics) as long as all\n    the workers have enough work to do.\n\n    Warning: this function is experimental and subject to change in a future\n    version of joblib.\n\n    .. versionadded:: 0.10\n\n    \"\"\"\n    backend, backend_n_jobs = get_active_backend()\n    if n_jobs is None:\n        n_jobs = backend_n_jobs\n    return backend.effective_n_jobs(n_jobs=n_jobs)\n\n\n###############################################################################\nclass Parallel(Logger):\n    ''' Helper class for readable parallel mapping.\n\n        Read more in the :ref:`User Guide <parallel>`.\n\n        Parameters\n        -----------\n        n_jobs: int, default: None\n            The maximum number of concurrently running jobs, such as the number\n            of Python worker processes when backend=\"multiprocessing\"\n            or the size of the thread-pool when backend=\"threading\".\n            If -1 all CPUs are used. If 1 is given, no parallel computing code\n            is used at all, which is useful for debugging. For n_jobs below -1,\n            (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all\n            CPUs but one are used.\n            None is a marker for 'unset' that will be interpreted as n_jobs=1\n            (sequential execution) unless the call is performed under a\n            :func:`~parallel_backend` context manager that sets another value\n            for n_jobs.\n        backend: str, ParallelBackendBase instance or None, default: 'loky'\n            Specify the parallelization backend implementation.\n            Supported backends are:\n\n            - \"loky\" used by default, can induce some\n              communication and memory overhead when exchanging input and\n              output data with the worker Python processes. On some rare\n              systems (such as Pyiodide), the loky backend may not be\n              available.\n            - \"multiprocessing\" previous process-based backend based on\n              `multiprocessing.Pool`. Less robust than `loky`.\n            - \"threading\" is a very low-overhead backend but it suffers\n              from the Python Global Interpreter Lock if the called function\n              relies a lot on Python objects. \"threading\" is mostly useful\n              when the execution bottleneck is a compiled extension that\n              explicitly releases the GIL (for instance a Cython loop wrapped\n              in a \"with nogil\" block or an expensive call to a library such\n              as NumPy).\n            - finally, you can register backends by calling\n              :func:`~register_parallel_backend`. This will allow you to\n              implement a backend of your liking.\n\n            It is not recommended to hard-code the backend name in a call to\n            :class:`~Parallel` in a library. Instead it is recommended to set\n            soft hints (prefer) or hard constraints (require) so as to make it\n            possible for library users to change the backend from the outside\n            using the :func:`~parallel_backend` context manager.\n        prefer: str in {'processes', 'threads'} or None, default: None\n            Soft hint to choose the default backend if no specific backend\n            was selected with the :func:`~parallel_backend` context manager.\n            The default process-based backend is 'loky' and the default\n            thread-based backend is 'threading'. Ignored if the ``backend``\n            parameter is specified.\n        require: 'sharedmem' or None, default None\n            Hard constraint to select the backend. If set to 'sharedmem',\n            the selected backend will be single-host and thread-based even\n            if the user asked for a non-thread based backend with\n            parallel_backend.\n        verbose: int, optional\n            The verbosity level: if non zero, progress messages are\n            printed. Above 50, the output is sent to stdout.\n            The frequency of the messages increases with the verbosity level.\n            If it more than 10, all iterations are reported.\n        timeout: float, optional\n            Timeout limit for each task to complete.  If any task takes longer\n            a TimeOutError will be raised. Only applied when n_jobs != 1\n        pre_dispatch: {'all', integer, or expression, as in '3*n_jobs'}\n            The number of batches (of tasks) to be pre-dispatched.\n            Default is '2*n_jobs'. When batch_size=\"auto\" this is reasonable\n            default and the workers should never starve. Note that only basic\n            arithmetics are allowed here and no modules can be used in this\n            expression.\n        batch_size: int or 'auto', default: 'auto'\n            The number of atomic tasks to dispatch at once to each\n            worker. When individual evaluations are very fast, dispatching\n            calls to workers can be slower than sequential computation because\n            of the overhead. Batching fast computations together can mitigate\n            this.\n            The ``'auto'`` strategy keeps track of the time it takes for a batch\n            to complete, and dynamically adjusts the batch size to keep the time\n            on the order of half a second, using a heuristic. The initial batch\n            size is 1.\n            ``batch_size=\"auto\"`` with ``backend=\"threading\"`` will dispatch\n            batches of a single task at a time as the threading backend has\n            very little overhead and using larger batch size has not proved to\n            bring any gain in that case.\n        temp_folder: str, optional\n            Folder to be used by the pool for memmapping large arrays\n            for sharing memory with worker processes. If None, this will try in\n            order:\n\n            - a folder pointed by the JOBLIB_TEMP_FOLDER environment\n              variable,\n            - /dev/shm if the folder exists and is writable: this is a\n              RAM disk filesystem available by default on modern Linux\n              distributions,\n            - the default system temporary folder that can be\n              overridden with TMP, TMPDIR or TEMP environment\n              variables, typically /tmp under Unix operating systems.\n\n            Only active when backend=\"loky\" or \"multiprocessing\".\n        max_nbytes int, str, or None, optional, 1M by default\n            Threshold on the size of arrays passed to the workers that\n            triggers automated memory mapping in temp_folder. Can be an int\n            in Bytes, or a human-readable string, e.g., '1M' for 1 megabyte.\n            Use None to disable memmapping of large arrays.\n            Only active when backend=\"loky\" or \"multiprocessing\".\n        mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, default: 'r'\n            Memmapping mode for numpy arrays passed to workers. None will\n            disable memmapping, other modes defined in the numpy.memmap doc:\n            https://numpy.org/doc/stable/reference/generated/numpy.memmap.html\n            Also, see 'max_nbytes' parameter documentation for more details.\n\n        Notes\n        -----\n\n        This object uses workers to compute in parallel the application of a\n        function to many different arguments. The main functionality it brings\n        in addition to using the raw multiprocessing or concurrent.futures API\n        are (see examples for details):\n\n        * More readable code, in particular since it avoids\n          constructing list of arguments.\n\n        * Easier debugging:\n            - informative tracebacks even when the error happens on\n              the client side\n            - using 'n_jobs=1' enables to turn off parallel computing\n              for debugging without changing the codepath\n            - early capture of pickling errors\n\n        * An optional progress meter.\n\n        * Interruption of multiprocesses jobs with 'Ctrl-C'\n\n        * Flexible pickling control for the communication to and from\n          the worker processes.\n\n        * Ability to use shared memory efficiently with worker\n          processes for large numpy-based datastructures.\n\n        Examples\n        --------\n\n        A simple example:\n\n        >>> from math import sqrt\n        >>> from joblib import Parallel, delayed\n        >>> Parallel(n_jobs=1)(delayed(sqrt)(i**2) for i in range(10))\n        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n\n        Reshaping the output when the function has several return\n        values:\n\n        >>> from math import modf\n        >>> from joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=1)(delayed(modf)(i/2.) for i in range(10))\n        >>> res, i = zip(*r)\n        >>> res\n        (0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5)\n        >>> i\n        (0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0)\n\n        The progress meter: the higher the value of `verbose`, the more\n        messages:\n\n        >>> from time import sleep\n        >>> from joblib import Parallel, delayed\n        >>> r = Parallel(n_jobs=2, verbose=10)(delayed(sleep)(.2) for _ in range(10)) #doctest: +SKIP\n        [Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.6s\n        [Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.8s\n        [Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    1.4s finished\n\n        Traceback example, note how the line of the error is indicated\n        as well as the values of the parameter passed to the function that\n        triggered the exception, even though the traceback happens in the\n        child process:\n\n        >>> from heapq import nlargest\n        >>> from joblib import Parallel, delayed\n        >>> Parallel(n_jobs=2)(delayed(nlargest)(2, n) for n in (range(4), 'abcde', 3)) #doctest: +SKIP\n        #...\n        ---------------------------------------------------------------------------\n        Sub-process traceback:\n        ---------------------------------------------------------------------------\n        TypeError                                          Mon Nov 12 11:37:46 2012\n        PID: 12934                                    Python 2.7.3: /usr/bin/python\n        ...........................................................................\n        /usr/lib/python2.7/heapq.pyc in nlargest(n=2, iterable=3, key=None)\n            419         if n >= size:\n            420             return sorted(iterable, key=key, reverse=True)[:n]\n            421\n            422     # When key is none, use simpler decoration\n            423     if key is None:\n        --> 424         it = izip(iterable, count(0,-1))                    # decorate\n            425         result = _nlargest(n, it)\n            426         return map(itemgetter(0), result)                   # undecorate\n            427\n            428     # General case, slowest method\n         TypeError: izip argument #1 must support iteration\n        ___________________________________________________________________________\n\n\n        Using pre_dispatch in a producer/consumer situation, where the\n        data is generated on the fly. Note how the producer is first\n        called 3 times before the parallel loop is initiated, and then\n        called to generate new data on the fly:\n\n        >>> from math import sqrt\n        >>> from joblib import Parallel, delayed\n        >>> def producer():\n        ...     for i in range(6):\n        ...         print('Produced %s' % i)\n        ...         yield i\n        >>> out = Parallel(n_jobs=2, verbose=100, pre_dispatch='1.5*n_jobs')(\n        ...                delayed(sqrt)(i) for i in producer()) #doctest: +SKIP\n        Produced 0\n        Produced 1\n        Produced 2\n        [Parallel(n_jobs=2)]: Done 1 jobs     | elapsed:  0.0s\n        Produced 3\n        [Parallel(n_jobs=2)]: Done 2 jobs     | elapsed:  0.0s\n        Produced 4\n        [Parallel(n_jobs=2)]: Done 3 jobs     | elapsed:  0.0s\n        Produced 5\n        [Parallel(n_jobs=2)]: Done 4 jobs     | elapsed:  0.0s\n        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s remaining: 0.0s\n        [Parallel(n_jobs=2)]: Done 6 out of 6 | elapsed:  0.0s finished\n\n    '''\n    def __init__(self, n_jobs=None, backend=None, verbose=0, timeout=None,\n                 pre_dispatch='2 * n_jobs', batch_size='auto',\n                 temp_folder=None, max_nbytes='1M', mmap_mode='r',\n                 prefer=None, require=None):\n        active_backend, context_n_jobs = get_active_backend(\n            prefer=prefer, require=require, verbose=verbose)\n        nesting_level = active_backend.nesting_level\n        if backend is None and n_jobs is None:\n            # If we are under a parallel_backend context manager, look up\n            # the default number of jobs and use that instead:\n            n_jobs = context_n_jobs\n        if n_jobs is None:\n            # No specific context override and no specific value request:\n            # default to 1.\n            n_jobs = 1\n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        self.timeout = timeout\n        self.pre_dispatch = pre_dispatch\n        self._ready_batches = queue.Queue()\n        self._id = uuid4().hex\n        self._reducer_callback = None\n\n        if isinstance(max_nbytes, str):\n            max_nbytes = memstr_to_bytes(max_nbytes)\n\n        self._backend_args = dict(\n            max_nbytes=max_nbytes,\n            mmap_mode=mmap_mode,\n            temp_folder=temp_folder,\n            prefer=prefer,\n            require=require,\n            verbose=max(0, self.verbose - 50),\n        )\n        if DEFAULT_MP_CONTEXT is not None:\n            self._backend_args['context'] = DEFAULT_MP_CONTEXT\n        elif hasattr(mp, \"get_context\"):\n            self._backend_args['context'] = mp.get_context()\n\n        if backend is None:\n            backend = active_backend\n\n        elif isinstance(backend, ParallelBackendBase):\n            # Use provided backend as is, with the current nesting_level if it\n            # is not set yet.\n            if backend.nesting_level is None:\n                backend.nesting_level = nesting_level\n\n        elif hasattr(backend, 'Pool') and hasattr(backend, 'Lock'):\n            # Make it possible to pass a custom multiprocessing context as\n            # backend to change the start method to forkserver or spawn or\n            # preload modules on the forkserver helper process.\n            self._backend_args['context'] = backend\n            backend = MultiprocessingBackend(nesting_level=nesting_level)\n\n        elif backend not in BACKENDS and backend in MAYBE_AVAILABLE_BACKENDS:\n            warnings.warn(\n                f\"joblib backend '{backend}' is not available on \"\n                f\"your system, falling back to {DEFAULT_BACKEND}.\",\n                UserWarning,\n                stacklevel=2)\n            BACKENDS[backend] = BACKENDS[DEFAULT_BACKEND]\n            backend = BACKENDS[DEFAULT_BACKEND](nesting_level=nesting_level)\n\n        else:\n            try:\n                backend_factory = BACKENDS[backend]\n            except KeyError as e:\n                raise ValueError(\"Invalid backend: %s, expected one of %r\"\n                                 % (backend, sorted(BACKENDS.keys()))) from e\n            backend = backend_factory(nesting_level=nesting_level)\n\n        if (require == 'sharedmem' and\n                not getattr(backend, 'supports_sharedmem', False)):\n            raise ValueError(\"Backend %s does not support shared memory\"\n                             % backend)\n\n        if (batch_size == 'auto' or isinstance(batch_size, Integral) and\n                batch_size > 0):\n            self.batch_size = batch_size\n        else:\n            raise ValueError(\n                \"batch_size must be 'auto' or a positive integer, got: %r\"\n                % batch_size)\n\n        self._backend = backend\n        self._output = None\n        self._jobs = list()\n        self._managed_backend = False\n\n        # This lock is used coordinate the main thread of this process with\n        # the async callback thread of our the pool.\n        self._lock = threading.RLock()\n\n    def __enter__(self):\n        self._managed_backend = True\n        self._initialize_backend()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._terminate_backend()\n        self._managed_backend = False\n\n    def _initialize_backend(self):\n        \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n        try:\n            n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n                                             **self._backend_args)\n            if self.timeout is not None and not self._backend.supports_timeout:\n                warnings.warn(\n                    'The backend class {!r} does not support timeout. '\n                    \"You have set 'timeout={}' in Parallel but \"\n                    \"the 'timeout' parameter will not be used.\".format(\n                        self._backend.__class__.__name__,\n                        self.timeout))\n\n        except FallbackToBackend as e:\n            # Recursively initialize the backend in case of requested fallback.\n            self._backend = e.backend\n            n_jobs = self._initialize_backend()\n\n        return n_jobs\n\n    def _effective_n_jobs(self):\n        if self._backend:\n            return self._backend.effective_n_jobs(self.n_jobs)\n        return 1\n\n    def _terminate_backend(self):\n        if self._backend is not None:\n            self._backend.terminate()\n\n    def _dispatch(self, batch):\n        \"\"\"Queue the batch for computing, with or without multiprocessing\n\n        WARNING: this method is not thread-safe: it should be only called\n        indirectly via dispatch_one_batch.\n\n        \"\"\"\n        # If job.get() catches an exception, it closes the queue:\n        if self._aborting:\n            return\n\n        self.n_dispatched_tasks += len(batch)\n        self.n_dispatched_batches += 1\n\n        dispatch_timestamp = time.time()\n        cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\n        with self._lock:\n            job_idx = len(self._jobs)\n            job = self._backend.apply_async(batch, callback=cb)\n            # A job can complete so quickly than its callback is\n            # called before we get here, causing self._jobs to\n            # grow. To ensure correct results ordering, .insert is\n            # used (rather than .append) in the following line\n            self._jobs.insert(job_idx, job)\n\n    def dispatch_next(self):\n        \"\"\"Dispatch more data for parallel processing\n\n        This method is meant to be called concurrently by the multiprocessing\n        callback. We rely on the thread-safety of dispatch_one_batch to protect\n        against concurrent consumption of the unprotected iterator.\n\n        \"\"\"\n        if not self.dispatch_one_batch(self._original_iterator):\n            self._iterating = False\n            self._original_iterator = None\n\n    def dispatch_one_batch(self, iterator):\n        \"\"\"Prefetch the tasks for the next batch and dispatch them.\n\n        The effective size of the batch is computed here.\n        If there are no more jobs to dispatch, return False, else return True.\n\n        The iterator consumption and dispatching is protected by the same\n        lock so calling this function should be thread safe.\n\n        \"\"\"\n        if self.batch_size == 'auto':\n            batch_size = self._backend.compute_batch_size()\n        else:\n            # Fixed batch size strategy\n            batch_size = self.batch_size\n\n        with self._lock:\n            # to ensure an even distribution of the workolad between workers,\n            # we look ahead in the original iterators more than batch_size\n            # tasks - However, we keep consuming only one batch at each\n            # dispatch_one_batch call. The extra tasks are stored in a local\n            # queue, _ready_batches, that is looked-up prior to re-consuming\n            # tasks from the origal iterator.\n            try:\n                tasks = self._ready_batches.get(block=False)\n            except queue.Empty:\n                # slice the iterator n_jobs * batchsize items at a time. If the\n                # slice returns less than that, then the current batchsize puts\n                # too much weight on a subset of workers, while other may end\n                # up starving. So in this case, re-scale the batch size\n                # accordingly to distribute evenly the last items between all\n                # workers.\n                n_jobs = self._cached_effective_n_jobs\n                big_batch_size = batch_size * n_jobs\n\n                islice = list(itertools.islice(iterator, big_batch_size))\n                if len(islice) == 0:\n                    return False\n                elif (iterator is self._original_iterator\n                      and len(islice) < big_batch_size):\n                    # We reached the end of the original iterator (unless\n                    # iterator is the ``pre_dispatch``-long initial slice of\n                    # the original iterator) -- decrease the batch size to\n                    # account for potential variance in the batches running\n                    # time.\n                    final_batch_size = max(1, len(islice) // (10 * n_jobs))\n                else:\n                    final_batch_size = max(1, len(islice) // n_jobs)\n\n                # enqueue n_jobs batches in a local queue\n                for i in range(0, len(islice), final_batch_size):\n                    tasks = BatchedCalls(islice[i:i + final_batch_size],\n                                         self._backend.get_nested_backend(),\n                                         self._reducer_callback,\n                                         self._pickle_cache)\n                    self._ready_batches.put(tasks)\n\n                # finally, get one task.\n                tasks = self._ready_batches.get(block=False)\n            if len(tasks) == 0:\n                # No more tasks available in the iterator: tell caller to stop.\n                return False\n            else:\n                self._dispatch(tasks)\n                return True\n\n    def _print(self, msg, msg_args):\n        \"\"\"Display the message on stout or stderr depending on verbosity\"\"\"\n        # XXX: Not using the logger framework: need to\n        # learn to use logger better.\n        if not self.verbose:\n            return\n        if self.verbose < 50:\n            writer = sys.stderr.write\n        else:\n            writer = sys.stdout.write\n        msg = msg % msg_args\n        writer('[%s]: %s\\n' % (self, msg))\n\n    def print_progress(self):\n        \"\"\"Display the process of the parallel execution only a fraction\n           of time, controlled by self.verbose.\n        \"\"\"\n        if not self.verbose:\n            return\n        elapsed_time = time.time() - self._start_time\n\n        # Original job iterator becomes None once it has been fully\n        # consumed : at this point we know the total number of jobs and we are\n        # able to display an estimation of the remaining time based on already\n        # completed jobs. Otherwise, we simply display the number of completed\n        # tasks.\n        if self._original_iterator is not None:\n            if _verbosity_filter(self.n_dispatched_batches, self.verbose):\n                return\n            self._print('Done %3i tasks      | elapsed: %s',\n                        (self.n_completed_tasks,\n                         short_format_time(elapsed_time), ))\n        else:\n            index = self.n_completed_tasks\n            # We are finished dispatching\n            total_tasks = self.n_dispatched_tasks\n            # We always display the first loop\n            if not index == 0:\n                # Display depending on the number of remaining items\n                # A message as soon as we finish dispatching, cursor is 0\n                cursor = (total_tasks - index + 1 -\n                          self._pre_dispatch_amount)\n                frequency = (total_tasks // self.verbose) + 1\n                is_last_item = (index + 1 == total_tasks)\n                if (is_last_item or cursor % frequency):\n                    return\n            remaining_time = (elapsed_time / index) * \\\n                             (self.n_dispatched_tasks - index * 1.0)\n            # only display status if remaining time is greater or equal to 0\n            self._print('Done %3i out of %3i | elapsed: %s remaining: %s',\n                        (index,\n                         total_tasks,\n                         short_format_time(elapsed_time),\n                         short_format_time(remaining_time),\n                         ))\n\n    def retrieve(self):\n        self._output = list()\n        while self._iterating or len(self._jobs) > 0:\n            if len(self._jobs) == 0:\n                # Wait for an async callback to dispatch new jobs\n                time.sleep(0.01)\n                continue\n            # We need to be careful: the job list can be filling up as\n            # we empty it and Python list are not thread-safe by default hence\n            # the use of the lock\n            with self._lock:\n                job = self._jobs.pop(0)\n\n            try:\n                if getattr(self._backend, 'supports_timeout', False):\n                    self._output.extend(job.get(timeout=self.timeout))\n                else:\n                    self._output.extend(job.get())\n\n            except BaseException as exception:\n                # Note: we catch any BaseException instead of just Exception\n                # instances to also include KeyboardInterrupt.\n\n                # Stop dispatching any new job in the async callback thread\n                self._aborting = True\n\n                # If the backend allows it, cancel or kill remaining running\n                # tasks without waiting for the results as we will raise\n                # the exception we got back to the caller instead of returning\n                # any result.\n                backend = self._backend\n                if (backend is not None and\n                        hasattr(backend, 'abort_everything')):\n                    # If the backend is managed externally we need to make sure\n                    # to leave it in a working state to allow for future jobs\n                    # scheduling.\n                    ensure_ready = self._managed_backend\n                    backend.abort_everything(ensure_ready=ensure_ready)\n                raise\n\n    def __call__(self, iterable):\n        if self._jobs:\n            raise ValueError('This Parallel instance is already running')\n        # A flag used to abort the dispatching of jobs in case an\n        # exception is found\n        self._aborting = False\n\n        if not self._managed_backend:\n            n_jobs = self._initialize_backend()\n        else:\n            n_jobs = self._effective_n_jobs()\n\n        if isinstance(self._backend, LokyBackend):\n            # For the loky backend, we add a callback executed when reducing\n            # BatchCalls, that makes the loky executor use a temporary folder\n            # specific to this Parallel object when pickling temporary memmaps.\n            # This callback is necessary to ensure that several Parallel\n            # objects using the same resuable executor don't use the same\n            # temporary resources.\n\n            def _batched_calls_reducer_callback():\n                # Relevant implementation detail: the following lines, called\n                # when reducing BatchedCalls, are called in a thread-safe\n                # situation, meaning that the context of the temporary folder\n                # manager will not be changed in between the callback execution\n                # and the end of the BatchedCalls pickling. The reason is that\n                # pickling (the only place where set_current_context is used)\n                # is done from a single thread (the queue_feeder_thread).\n                self._backend._workers._temp_folder_manager.set_current_context(  # noqa\n                    self._id\n                )\n            self._reducer_callback = _batched_calls_reducer_callback\n\n        # self._effective_n_jobs should be called in the Parallel.__call__\n        # thread only -- store its value in an attribute for further queries.\n        self._cached_effective_n_jobs = n_jobs\n\n        backend_name = self._backend.__class__.__name__\n        if n_jobs == 0:\n            raise RuntimeError(\"%s has no active worker.\" % backend_name)\n\n        self._print(\"Using backend %s with %d concurrent workers.\",\n                    (backend_name, n_jobs))\n        if hasattr(self._backend, 'start_call'):\n            self._backend.start_call()\n        iterator = iter(iterable)\n        pre_dispatch = self.pre_dispatch\n\n        if pre_dispatch == 'all' or n_jobs == 1:\n            # prevent further dispatch via multiprocessing callback thread\n            self._original_iterator = None\n            self._pre_dispatch_amount = 0\n        else:\n            self._original_iterator = iterator\n            if hasattr(pre_dispatch, 'endswith'):\n                pre_dispatch = eval(\n                    pre_dispatch,\n                    {\"n_jobs\": n_jobs, \"__builtins__\": {}},  # globals\n                    {}  # locals\n                )\n            self._pre_dispatch_amount = pre_dispatch = int(pre_dispatch)\n\n            # The main thread will consume the first pre_dispatch items and\n            # the remaining items will later be lazily dispatched by async\n            # callbacks upon task completions.\n\n            # TODO: this iterator should be batch_size * n_jobs\n            iterator = itertools.islice(iterator, self._pre_dispatch_amount)\n\n        self._start_time = time.time()\n        self.n_dispatched_batches = 0\n        self.n_dispatched_tasks = 0\n        self.n_completed_tasks = 0\n        # Use a caching dict for callables that are pickled with cloudpickle to\n        # improve performances. This cache is used only in the case of\n        # functions that are defined in the __main__ module, functions that are\n        # defined locally (inside another function) and lambda expressions.\n        self._pickle_cache = dict()\n        try:\n            # Only set self._iterating to True if at least a batch\n            # was dispatched. In particular this covers the edge\n            # case of Parallel used with an exhausted iterator. If\n            # self._original_iterator is None, then this means either\n            # that pre_dispatch == \"all\", n_jobs == 1 or that the first batch\n            # was very quick and its callback already dispatched all the\n            # remaining jobs.\n            self._iterating = False\n            if self.dispatch_one_batch(iterator):\n                self._iterating = self._original_iterator is not None\n\n            while self.dispatch_one_batch(iterator):\n                pass\n\n            if pre_dispatch == \"all\" or n_jobs == 1:\n                # The iterable was consumed all at once by the above for loop.\n                # No need to wait for async callbacks to trigger to\n                # consumption.\n                self._iterating = False\n\n            with self._backend.retrieval_context():\n                self.retrieve()\n            # Make sure that we get a last message telling us we are done\n            elapsed_time = time.time() - self._start_time\n            self._print('Done %3i out of %3i | elapsed: %s finished',\n                        (len(self._output), len(self._output),\n                         short_format_time(elapsed_time)))\n        finally:\n            if hasattr(self._backend, 'stop_call'):\n                self._backend.stop_call()\n            if not self._managed_backend:\n                self._terminate_backend()\n            self._jobs = list()\n            self._pickle_cache = None\n        output = self._output\n        self._output = None\n        return output\n\n    def __repr__(self):\n        return '%s(n_jobs=%s)' % (self.__class__.__name__, self.n_jobs)\n"], "filenames": ["CHANGES.rst", "joblib/parallel.py"], "buggy_code_start_loc": [30, 507], "buggy_code_end_loc": [30, 1053], "fixing_code_start_loc": [31, 507], "fixing_code_end_loc": [35, 1059], "type": "NVD-CWE-noinfo", "message": "The package joblib from 0 and before 1.2.0 are vulnerable to Arbitrary Code Execution via the pre_dispatch flag in Parallel() class due to the eval() statement.", "other": {"cve": {"id": "CVE-2022-21797", "sourceIdentifier": "report@snyk.io", "published": "2022-09-26T05:15:10.427", "lastModified": "2023-04-20T14:41:03.180", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The package joblib from 0 and before 1.2.0 are vulnerable to Arbitrary Code Execution via the pre_dispatch flag in Parallel() class due to the eval() statement."}, {"lang": "es", "value": "El paquete joblib versiones a partir de 0 anteriores a 1.2.0, son vulnerables a una Ejecuci\u00f3n de C\u00f3digo Arbitraria por medio del flag pre_dispatch en la clase Parallel() debido a la sentencia eval().\n"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}, {"source": "report@snyk.io", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:L", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 7.3, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.4}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:joblib_project:joblib:*:*:*:*:*:python:*:*", "versionEndExcluding": "1.1.1", "matchCriteriaId": "50D353B2-3D1D-47D8-9590-1BAB36CFC87C"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:36:*:*:*:*:*:*:*", "matchCriteriaId": "5C675112-476C-4D7C-BCB9-A2FB2D0BC9FD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:37:*:*:*:*:*:*:*", "matchCriteriaId": "E30D0E6F-4AE8-4284-8716-991DFA48CC5D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}], "references": [{"url": "https://github.com/joblib/joblib/commit/b90f10efeb670a2cc877fb88ebb3f2019189e059", "source": "report@snyk.io", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/joblib/joblib/issues/1128", "source": "report@snyk.io", "tags": ["Exploit", "Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/joblib/joblib/pull/1321", "source": "report@snyk.io", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/11/msg00020.html", "source": "report@snyk.io", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2023/03/msg00027.html", "source": "report@snyk.io", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/BVOMMW37OXZWU2EV5ONAAS462IQEHZOF/", "source": "report@snyk.io", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/MJ5XTJS6OKJRRVXWFN5J67K3BYPEOBDF/", "source": "report@snyk.io", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://security.snyk.io/vuln/SNYK-PYTHON-JOBLIB-3027033", "source": "report@snyk.io", "tags": ["Exploit", "Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/joblib/joblib/commit/b90f10efeb670a2cc877fb88ebb3f2019189e059"}}