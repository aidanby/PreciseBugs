{"buggy_code": ["// SPDX-License-Identifier: ISC\n/*\n * Copyright (C) 2016 Felix Fietkau <nbd@nbd.name>\n */\n\n#include <linux/dma-mapping.h>\n#include \"mt76.h\"\n#include \"dma.h\"\n\nstatic int\nmt76_dma_alloc_queue(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t     int idx, int n_desc, int bufsize,\n\t\t     u32 ring_base)\n{\n\tint size;\n\tint i;\n\n\tspin_lock_init(&q->lock);\n\n\tq->regs = dev->mmio.regs + ring_base + idx * MT_RING_SIZE;\n\tq->ndesc = n_desc;\n\tq->buf_size = bufsize;\n\tq->hw_idx = idx;\n\n\tsize = q->ndesc * sizeof(struct mt76_desc);\n\tq->desc = dmam_alloc_coherent(dev->dev, size, &q->desc_dma, GFP_KERNEL);\n\tif (!q->desc)\n\t\treturn -ENOMEM;\n\n\tsize = q->ndesc * sizeof(*q->entry);\n\tq->entry = devm_kzalloc(dev->dev, size, GFP_KERNEL);\n\tif (!q->entry)\n\t\treturn -ENOMEM;\n\n\t/* clear descriptors */\n\tfor (i = 0; i < q->ndesc; i++)\n\t\tq->desc[i].ctrl = cpu_to_le32(MT_DMA_CTL_DMA_DONE);\n\n\twritel(q->desc_dma, &q->regs->desc_base);\n\twritel(0, &q->regs->cpu_idx);\n\twritel(0, &q->regs->dma_idx);\n\twritel(q->ndesc, &q->regs->ring_size);\n\n\treturn 0;\n}\n\nstatic int\nmt76_dma_add_buf(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t struct mt76_queue_buf *buf, int nbufs, u32 info,\n\t\t struct sk_buff *skb, void *txwi)\n{\n\tstruct mt76_desc *desc;\n\tu32 ctrl;\n\tint i, idx = -1;\n\n\tif (txwi) {\n\t\tq->entry[q->head].txwi = DMA_DUMMY_DATA;\n\t\tq->entry[q->head].skip_buf0 = true;\n\t}\n\n\tfor (i = 0; i < nbufs; i += 2, buf += 2) {\n\t\tu32 buf0 = buf[0].addr, buf1 = 0;\n\n\t\tctrl = FIELD_PREP(MT_DMA_CTL_SD_LEN0, buf[0].len);\n\t\tif (i < nbufs - 1) {\n\t\t\tbuf1 = buf[1].addr;\n\t\t\tctrl |= FIELD_PREP(MT_DMA_CTL_SD_LEN1, buf[1].len);\n\t\t}\n\n\t\tif (i == nbufs - 1)\n\t\t\tctrl |= MT_DMA_CTL_LAST_SEC0;\n\t\telse if (i == nbufs - 2)\n\t\t\tctrl |= MT_DMA_CTL_LAST_SEC1;\n\n\t\tidx = q->head;\n\t\tq->head = (q->head + 1) % q->ndesc;\n\n\t\tdesc = &q->desc[idx];\n\n\t\tWRITE_ONCE(desc->buf0, cpu_to_le32(buf0));\n\t\tWRITE_ONCE(desc->buf1, cpu_to_le32(buf1));\n\t\tWRITE_ONCE(desc->info, cpu_to_le32(info));\n\t\tWRITE_ONCE(desc->ctrl, cpu_to_le32(ctrl));\n\n\t\tq->queued++;\n\t}\n\n\tq->entry[idx].txwi = txwi;\n\tq->entry[idx].skb = skb;\n\n\treturn idx;\n}\n\nstatic void\nmt76_dma_tx_cleanup_idx(struct mt76_dev *dev, struct mt76_queue *q, int idx,\n\t\t\tstruct mt76_queue_entry *prev_e)\n{\n\tstruct mt76_queue_entry *e = &q->entry[idx];\n\t__le32 __ctrl = READ_ONCE(q->desc[idx].ctrl);\n\tu32 ctrl = le32_to_cpu(__ctrl);\n\n\tif (!e->skip_buf0) {\n\t\t__le32 addr = READ_ONCE(q->desc[idx].buf0);\n\t\tu32 len = FIELD_GET(MT_DMA_CTL_SD_LEN0, ctrl);\n\n\t\tdma_unmap_single(dev->dev, le32_to_cpu(addr), len,\n\t\t\t\t DMA_TO_DEVICE);\n\t}\n\n\tif (!(ctrl & MT_DMA_CTL_LAST_SEC0)) {\n\t\t__le32 addr = READ_ONCE(q->desc[idx].buf1);\n\t\tu32 len = FIELD_GET(MT_DMA_CTL_SD_LEN1, ctrl);\n\n\t\tdma_unmap_single(dev->dev, le32_to_cpu(addr), len,\n\t\t\t\t DMA_TO_DEVICE);\n\t}\n\n\tif (e->txwi == DMA_DUMMY_DATA)\n\t\te->txwi = NULL;\n\n\tif (e->skb == DMA_DUMMY_DATA)\n\t\te->skb = NULL;\n\n\t*prev_e = *e;\n\tmemset(e, 0, sizeof(*e));\n}\n\nstatic void\nmt76_dma_sync_idx(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\twritel(q->desc_dma, &q->regs->desc_base);\n\twritel(q->ndesc, &q->regs->ring_size);\n\tq->head = readl(&q->regs->dma_idx);\n\tq->tail = q->head;\n\twritel(q->head, &q->regs->cpu_idx);\n}\n\nstatic void\nmt76_dma_tx_cleanup(struct mt76_dev *dev, enum mt76_txq_id qid, bool flush)\n{\n\tstruct mt76_sw_queue *sq = &dev->q_tx[qid];\n\tstruct mt76_queue *q = sq->q;\n\tstruct mt76_queue_entry entry;\n\tunsigned int n_swq_queued[4] = {};\n\tunsigned int n_queued = 0;\n\tbool wake = false;\n\tint i, last;\n\n\tif (!q)\n\t\treturn;\n\n\tif (flush)\n\t\tlast = -1;\n\telse\n\t\tlast = readl(&q->regs->dma_idx);\n\n\twhile ((q->queued > n_queued) && q->tail != last) {\n\t\tmt76_dma_tx_cleanup_idx(dev, q, q->tail, &entry);\n\t\tif (entry.schedule)\n\t\t\tn_swq_queued[entry.qid]++;\n\n\t\tq->tail = (q->tail + 1) % q->ndesc;\n\t\tn_queued++;\n\n\t\tif (entry.skb)\n\t\t\tdev->drv->tx_complete_skb(dev, qid, &entry);\n\n\t\tif (entry.txwi) {\n\t\t\tif (!(dev->drv->drv_flags & MT_DRV_TXWI_NO_FREE))\n\t\t\t\tmt76_put_txwi(dev, entry.txwi);\n\t\t\twake = !flush;\n\t\t}\n\n\t\tif (!flush && q->tail == last)\n\t\t\tlast = readl(&q->regs->dma_idx);\n\t}\n\n\tspin_lock_bh(&q->lock);\n\n\tq->queued -= n_queued;\n\tfor (i = 0; i < ARRAY_SIZE(n_swq_queued); i++) {\n\t\tif (!n_swq_queued[i])\n\t\t\tcontinue;\n\n\t\tdev->q_tx[i].swq_queued -= n_swq_queued[i];\n\t}\n\n\tif (flush)\n\t\tmt76_dma_sync_idx(dev, q);\n\n\twake = wake && q->stopped &&\n\t       qid < IEEE80211_NUM_ACS && q->queued < q->ndesc - 8;\n\tif (wake)\n\t\tq->stopped = false;\n\n\tif (!q->queued)\n\t\twake_up(&dev->tx_wait);\n\n\tspin_unlock_bh(&q->lock);\n\n\tif (wake)\n\t\tieee80211_wake_queue(dev->hw, qid);\n}\n\nstatic void *\nmt76_dma_get_buf(struct mt76_dev *dev, struct mt76_queue *q, int idx,\n\t\t int *len, u32 *info, bool *more)\n{\n\tstruct mt76_queue_entry *e = &q->entry[idx];\n\tstruct mt76_desc *desc = &q->desc[idx];\n\tdma_addr_t buf_addr;\n\tvoid *buf = e->buf;\n\tint buf_len = SKB_WITH_OVERHEAD(q->buf_size);\n\n\tbuf_addr = le32_to_cpu(READ_ONCE(desc->buf0));\n\tif (len) {\n\t\tu32 ctl = le32_to_cpu(READ_ONCE(desc->ctrl));\n\t\t*len = FIELD_GET(MT_DMA_CTL_SD_LEN0, ctl);\n\t\t*more = !(ctl & MT_DMA_CTL_LAST_SEC0);\n\t}\n\n\tif (info)\n\t\t*info = le32_to_cpu(desc->info);\n\n\tdma_unmap_single(dev->dev, buf_addr, buf_len, DMA_FROM_DEVICE);\n\te->buf = NULL;\n\n\treturn buf;\n}\n\nstatic void *\nmt76_dma_dequeue(struct mt76_dev *dev, struct mt76_queue *q, bool flush,\n\t\t int *len, u32 *info, bool *more)\n{\n\tint idx = q->tail;\n\n\t*more = false;\n\tif (!q->queued)\n\t\treturn NULL;\n\n\tif (!flush && !(q->desc[idx].ctrl & cpu_to_le32(MT_DMA_CTL_DMA_DONE)))\n\t\treturn NULL;\n\n\tq->tail = (q->tail + 1) % q->ndesc;\n\tq->queued--;\n\n\treturn mt76_dma_get_buf(dev, q, idx, len, info, more);\n}\n\nstatic void\nmt76_dma_kick_queue(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\twritel(q->head, &q->regs->cpu_idx);\n}\n\nstatic int\nmt76_dma_tx_queue_skb_raw(struct mt76_dev *dev, enum mt76_txq_id qid,\n\t\t\t  struct sk_buff *skb, u32 tx_info)\n{\n\tstruct mt76_queue *q = dev->q_tx[qid].q;\n\tstruct mt76_queue_buf buf;\n\tdma_addr_t addr;\n\n\taddr = dma_map_single(dev->dev, skb->data, skb->len,\n\t\t\t      DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev->dev, addr)))\n\t\treturn -ENOMEM;\n\n\tbuf.addr = addr;\n\tbuf.len = skb->len;\n\n\tspin_lock_bh(&q->lock);\n\tmt76_dma_add_buf(dev, q, &buf, 1, tx_info, skb, NULL);\n\tmt76_dma_kick_queue(dev, q);\n\tspin_unlock_bh(&q->lock);\n\n\treturn 0;\n}\n\nstatic int\nmt76_dma_tx_queue_skb(struct mt76_dev *dev, enum mt76_txq_id qid,\n\t\t      struct sk_buff *skb, struct mt76_wcid *wcid,\n\t\t      struct ieee80211_sta *sta)\n{\n\tstruct mt76_queue *q = dev->q_tx[qid].q;\n\tstruct mt76_tx_info tx_info = {\n\t\t.skb = skb,\n\t};\n\tint len, n = 0, ret = -ENOMEM;\n\tstruct mt76_queue_entry e;\n\tstruct mt76_txwi_cache *t;\n\tstruct sk_buff *iter;\n\tdma_addr_t addr;\n\tu8 *txwi;\n\n\tt = mt76_get_txwi(dev);\n\tif (!t) {\n\t\tieee80211_free_txskb(dev->hw, skb);\n\t\treturn -ENOMEM;\n\t}\n\ttxwi = mt76_get_txwi_ptr(dev, t);\n\n\tskb->prev = skb->next = NULL;\n\tif (dev->drv->drv_flags & MT_DRV_TX_ALIGNED4_SKBS)\n\t\tmt76_insert_hdr_pad(skb);\n\n\tlen = skb_headlen(skb);\n\taddr = dma_map_single(dev->dev, skb->data, len, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev->dev, addr)))\n\t\tgoto free;\n\n\ttx_info.buf[n].addr = t->dma_addr;\n\ttx_info.buf[n++].len = dev->drv->txwi_size;\n\ttx_info.buf[n].addr = addr;\n\ttx_info.buf[n++].len = len;\n\n\tskb_walk_frags(skb, iter) {\n\t\tif (n == ARRAY_SIZE(tx_info.buf))\n\t\t\tgoto unmap;\n\n\t\taddr = dma_map_single(dev->dev, iter->data, iter->len,\n\t\t\t\t      DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(dev->dev, addr)))\n\t\t\tgoto unmap;\n\n\t\ttx_info.buf[n].addr = addr;\n\t\ttx_info.buf[n++].len = iter->len;\n\t}\n\ttx_info.nbuf = n;\n\n\tdma_sync_single_for_cpu(dev->dev, t->dma_addr, dev->drv->txwi_size,\n\t\t\t\tDMA_TO_DEVICE);\n\tret = dev->drv->tx_prepare_skb(dev, txwi, qid, wcid, sta, &tx_info);\n\tdma_sync_single_for_device(dev->dev, t->dma_addr, dev->drv->txwi_size,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret < 0)\n\t\tgoto unmap;\n\n\tif (q->queued + (tx_info.nbuf + 1) / 2 >= q->ndesc - 1) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap;\n\t}\n\n\treturn mt76_dma_add_buf(dev, q, tx_info.buf, tx_info.nbuf,\n\t\t\t\ttx_info.info, tx_info.skb, t);\n\nunmap:\n\tfor (n--; n > 0; n--)\n\t\tdma_unmap_single(dev->dev, tx_info.buf[n].addr,\n\t\t\t\t tx_info.buf[n].len, DMA_TO_DEVICE);\n\nfree:\n\te.skb = tx_info.skb;\n\te.txwi = t;\n\tdev->drv->tx_complete_skb(dev, qid, &e);\n\tmt76_put_txwi(dev, t);\n\treturn ret;\n}\n\nstatic int\nmt76_dma_rx_fill(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\tdma_addr_t addr;\n\tvoid *buf;\n\tint frames = 0;\n\tint len = SKB_WITH_OVERHEAD(q->buf_size);\n\tint offset = q->buf_offset;\n\n\tspin_lock_bh(&q->lock);\n\n\twhile (q->queued < q->ndesc - 1) {\n\t\tstruct mt76_queue_buf qbuf;\n\n\t\tbuf = page_frag_alloc(&q->rx_page, q->buf_size, GFP_ATOMIC);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\taddr = dma_map_single(dev->dev, buf, len, DMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(dev->dev, addr))) {\n\t\t\tskb_free_frag(buf);\n\t\t\tbreak;\n\t\t}\n\n\t\tqbuf.addr = addr + offset;\n\t\tqbuf.len = len - offset;\n\t\tmt76_dma_add_buf(dev, q, &qbuf, 1, 0, buf, NULL);\n\t\tframes++;\n\t}\n\n\tif (frames)\n\t\tmt76_dma_kick_queue(dev, q);\n\n\tspin_unlock_bh(&q->lock);\n\n\treturn frames;\n}\n\nstatic void\nmt76_dma_rx_cleanup(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\tstruct page *page;\n\tvoid *buf;\n\tbool more;\n\n\tspin_lock_bh(&q->lock);\n\tdo {\n\t\tbuf = mt76_dma_dequeue(dev, q, true, NULL, NULL, &more);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tskb_free_frag(buf);\n\t} while (1);\n\tspin_unlock_bh(&q->lock);\n\n\tif (!q->rx_page.va)\n\t\treturn;\n\n\tpage = virt_to_page(q->rx_page.va);\n\t__page_frag_cache_drain(page, q->rx_page.pagecnt_bias);\n\tmemset(&q->rx_page, 0, sizeof(q->rx_page));\n}\n\nstatic void\nmt76_dma_rx_reset(struct mt76_dev *dev, enum mt76_rxq_id qid)\n{\n\tstruct mt76_queue *q = &dev->q_rx[qid];\n\tint i;\n\n\tfor (i = 0; i < q->ndesc; i++)\n\t\tq->desc[i].ctrl &= ~cpu_to_le32(MT_DMA_CTL_DMA_DONE);\n\n\tmt76_dma_rx_cleanup(dev, q);\n\tmt76_dma_sync_idx(dev, q);\n\tmt76_dma_rx_fill(dev, q);\n\n\tif (!q->rx_head)\n\t\treturn;\n\n\tdev_kfree_skb(q->rx_head);\n\tq->rx_head = NULL;\n}\n\nstatic void\nmt76_add_fragment(struct mt76_dev *dev, struct mt76_queue *q, void *data,\n\t\t  int len, bool more)\n{\n\tstruct page *page = virt_to_head_page(data);\n\tint offset = data - page_address(page);\n\tstruct sk_buff *skb = q->rx_head;\n\n\toffset += q->buf_offset;\n\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page, offset, len,\n\t\t\tq->buf_size);\n\n\tif (more)\n\t\treturn;\n\n\tq->rx_head = NULL;\n\tdev->drv->rx_skb(dev, q - dev->q_rx, skb);\n}\n\nstatic int\nmt76_dma_rx_process(struct mt76_dev *dev, struct mt76_queue *q, int budget)\n{\n\tint len, data_len, done = 0;\n\tstruct sk_buff *skb;\n\tunsigned char *data;\n\tbool more;\n\n\twhile (done < budget) {\n\t\tu32 info;\n\n\t\tdata = mt76_dma_dequeue(dev, q, false, &len, &info, &more);\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\tif (q->rx_head)\n\t\t\tdata_len = q->buf_size;\n\t\telse\n\t\t\tdata_len = SKB_WITH_OVERHEAD(q->buf_size);\n\n\t\tif (data_len < len + q->buf_offset) {\n\t\t\tdev_kfree_skb(q->rx_head);\n\t\t\tq->rx_head = NULL;\n\n\t\t\tskb_free_frag(data);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (q->rx_head) {\n\t\t\tmt76_add_fragment(dev, q, data, len, more);\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb = build_skb(data, q->buf_size);\n\t\tif (!skb) {\n\t\t\tskb_free_frag(data);\n\t\t\tcontinue;\n\t\t}\n\t\tskb_reserve(skb, q->buf_offset);\n\n\t\tif (q == &dev->q_rx[MT_RXQ_MCU]) {\n\t\t\tu32 *rxfce = (u32 *)skb->cb;\n\t\t\t*rxfce = info;\n\t\t}\n\n\t\t__skb_put(skb, len);\n\t\tdone++;\n\n\t\tif (more) {\n\t\t\tq->rx_head = skb;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev->drv->rx_skb(dev, q - dev->q_rx, skb);\n\t}\n\n\tmt76_dma_rx_fill(dev, q);\n\treturn done;\n}\n\nstatic int\nmt76_dma_rx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct mt76_dev *dev;\n\tint qid, done = 0, cur;\n\n\tdev = container_of(napi->dev, struct mt76_dev, napi_dev);\n\tqid = napi - dev->napi;\n\n\trcu_read_lock();\n\n\tdo {\n\t\tcur = mt76_dma_rx_process(dev, &dev->q_rx[qid], budget - done);\n\t\tmt76_rx_poll_complete(dev, qid, napi);\n\t\tdone += cur;\n\t} while (cur && done < budget);\n\n\trcu_read_unlock();\n\n\tif (done < budget && napi_complete(napi))\n\t\tdev->drv->rx_poll_complete(dev, qid);\n\n\treturn done;\n}\n\nstatic int\nmt76_dma_init(struct mt76_dev *dev)\n{\n\tint i;\n\n\tinit_dummy_netdev(&dev->napi_dev);\n\n\tfor (i = 0; i < ARRAY_SIZE(dev->q_rx); i++) {\n\t\tnetif_napi_add(&dev->napi_dev, &dev->napi[i], mt76_dma_rx_poll,\n\t\t\t       64);\n\t\tmt76_dma_rx_fill(dev, &dev->q_rx[i]);\n\t\tskb_queue_head_init(&dev->rx_skb[i]);\n\t\tnapi_enable(&dev->napi[i]);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct mt76_queue_ops mt76_dma_ops = {\n\t.init = mt76_dma_init,\n\t.alloc = mt76_dma_alloc_queue,\n\t.tx_queue_skb_raw = mt76_dma_tx_queue_skb_raw,\n\t.tx_queue_skb = mt76_dma_tx_queue_skb,\n\t.tx_cleanup = mt76_dma_tx_cleanup,\n\t.rx_reset = mt76_dma_rx_reset,\n\t.kick = mt76_dma_kick_queue,\n};\n\nvoid mt76_dma_attach(struct mt76_dev *dev)\n{\n\tdev->queue_ops = &mt76_dma_ops;\n}\nEXPORT_SYMBOL_GPL(mt76_dma_attach);\n\nvoid mt76_dma_cleanup(struct mt76_dev *dev)\n{\n\tint i;\n\n\tnetif_napi_del(&dev->tx_napi);\n\tfor (i = 0; i < ARRAY_SIZE(dev->q_tx); i++)\n\t\tmt76_dma_tx_cleanup(dev, i, true);\n\n\tfor (i = 0; i < ARRAY_SIZE(dev->q_rx); i++) {\n\t\tnetif_napi_del(&dev->napi[i]);\n\t\tmt76_dma_rx_cleanup(dev, &dev->q_rx[i]);\n\t}\n}\nEXPORT_SYMBOL_GPL(mt76_dma_cleanup);\n"], "fixing_code": ["// SPDX-License-Identifier: ISC\n/*\n * Copyright (C) 2016 Felix Fietkau <nbd@nbd.name>\n */\n\n#include <linux/dma-mapping.h>\n#include \"mt76.h\"\n#include \"dma.h\"\n\nstatic int\nmt76_dma_alloc_queue(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t     int idx, int n_desc, int bufsize,\n\t\t     u32 ring_base)\n{\n\tint size;\n\tint i;\n\n\tspin_lock_init(&q->lock);\n\n\tq->regs = dev->mmio.regs + ring_base + idx * MT_RING_SIZE;\n\tq->ndesc = n_desc;\n\tq->buf_size = bufsize;\n\tq->hw_idx = idx;\n\n\tsize = q->ndesc * sizeof(struct mt76_desc);\n\tq->desc = dmam_alloc_coherent(dev->dev, size, &q->desc_dma, GFP_KERNEL);\n\tif (!q->desc)\n\t\treturn -ENOMEM;\n\n\tsize = q->ndesc * sizeof(*q->entry);\n\tq->entry = devm_kzalloc(dev->dev, size, GFP_KERNEL);\n\tif (!q->entry)\n\t\treturn -ENOMEM;\n\n\t/* clear descriptors */\n\tfor (i = 0; i < q->ndesc; i++)\n\t\tq->desc[i].ctrl = cpu_to_le32(MT_DMA_CTL_DMA_DONE);\n\n\twritel(q->desc_dma, &q->regs->desc_base);\n\twritel(0, &q->regs->cpu_idx);\n\twritel(0, &q->regs->dma_idx);\n\twritel(q->ndesc, &q->regs->ring_size);\n\n\treturn 0;\n}\n\nstatic int\nmt76_dma_add_buf(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t struct mt76_queue_buf *buf, int nbufs, u32 info,\n\t\t struct sk_buff *skb, void *txwi)\n{\n\tstruct mt76_desc *desc;\n\tu32 ctrl;\n\tint i, idx = -1;\n\n\tif (txwi) {\n\t\tq->entry[q->head].txwi = DMA_DUMMY_DATA;\n\t\tq->entry[q->head].skip_buf0 = true;\n\t}\n\n\tfor (i = 0; i < nbufs; i += 2, buf += 2) {\n\t\tu32 buf0 = buf[0].addr, buf1 = 0;\n\n\t\tctrl = FIELD_PREP(MT_DMA_CTL_SD_LEN0, buf[0].len);\n\t\tif (i < nbufs - 1) {\n\t\t\tbuf1 = buf[1].addr;\n\t\t\tctrl |= FIELD_PREP(MT_DMA_CTL_SD_LEN1, buf[1].len);\n\t\t}\n\n\t\tif (i == nbufs - 1)\n\t\t\tctrl |= MT_DMA_CTL_LAST_SEC0;\n\t\telse if (i == nbufs - 2)\n\t\t\tctrl |= MT_DMA_CTL_LAST_SEC1;\n\n\t\tidx = q->head;\n\t\tq->head = (q->head + 1) % q->ndesc;\n\n\t\tdesc = &q->desc[idx];\n\n\t\tWRITE_ONCE(desc->buf0, cpu_to_le32(buf0));\n\t\tWRITE_ONCE(desc->buf1, cpu_to_le32(buf1));\n\t\tWRITE_ONCE(desc->info, cpu_to_le32(info));\n\t\tWRITE_ONCE(desc->ctrl, cpu_to_le32(ctrl));\n\n\t\tq->queued++;\n\t}\n\n\tq->entry[idx].txwi = txwi;\n\tq->entry[idx].skb = skb;\n\n\treturn idx;\n}\n\nstatic void\nmt76_dma_tx_cleanup_idx(struct mt76_dev *dev, struct mt76_queue *q, int idx,\n\t\t\tstruct mt76_queue_entry *prev_e)\n{\n\tstruct mt76_queue_entry *e = &q->entry[idx];\n\t__le32 __ctrl = READ_ONCE(q->desc[idx].ctrl);\n\tu32 ctrl = le32_to_cpu(__ctrl);\n\n\tif (!e->skip_buf0) {\n\t\t__le32 addr = READ_ONCE(q->desc[idx].buf0);\n\t\tu32 len = FIELD_GET(MT_DMA_CTL_SD_LEN0, ctrl);\n\n\t\tdma_unmap_single(dev->dev, le32_to_cpu(addr), len,\n\t\t\t\t DMA_TO_DEVICE);\n\t}\n\n\tif (!(ctrl & MT_DMA_CTL_LAST_SEC0)) {\n\t\t__le32 addr = READ_ONCE(q->desc[idx].buf1);\n\t\tu32 len = FIELD_GET(MT_DMA_CTL_SD_LEN1, ctrl);\n\n\t\tdma_unmap_single(dev->dev, le32_to_cpu(addr), len,\n\t\t\t\t DMA_TO_DEVICE);\n\t}\n\n\tif (e->txwi == DMA_DUMMY_DATA)\n\t\te->txwi = NULL;\n\n\tif (e->skb == DMA_DUMMY_DATA)\n\t\te->skb = NULL;\n\n\t*prev_e = *e;\n\tmemset(e, 0, sizeof(*e));\n}\n\nstatic void\nmt76_dma_sync_idx(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\twritel(q->desc_dma, &q->regs->desc_base);\n\twritel(q->ndesc, &q->regs->ring_size);\n\tq->head = readl(&q->regs->dma_idx);\n\tq->tail = q->head;\n\twritel(q->head, &q->regs->cpu_idx);\n}\n\nstatic void\nmt76_dma_tx_cleanup(struct mt76_dev *dev, enum mt76_txq_id qid, bool flush)\n{\n\tstruct mt76_sw_queue *sq = &dev->q_tx[qid];\n\tstruct mt76_queue *q = sq->q;\n\tstruct mt76_queue_entry entry;\n\tunsigned int n_swq_queued[4] = {};\n\tunsigned int n_queued = 0;\n\tbool wake = false;\n\tint i, last;\n\n\tif (!q)\n\t\treturn;\n\n\tif (flush)\n\t\tlast = -1;\n\telse\n\t\tlast = readl(&q->regs->dma_idx);\n\n\twhile ((q->queued > n_queued) && q->tail != last) {\n\t\tmt76_dma_tx_cleanup_idx(dev, q, q->tail, &entry);\n\t\tif (entry.schedule)\n\t\t\tn_swq_queued[entry.qid]++;\n\n\t\tq->tail = (q->tail + 1) % q->ndesc;\n\t\tn_queued++;\n\n\t\tif (entry.skb)\n\t\t\tdev->drv->tx_complete_skb(dev, qid, &entry);\n\n\t\tif (entry.txwi) {\n\t\t\tif (!(dev->drv->drv_flags & MT_DRV_TXWI_NO_FREE))\n\t\t\t\tmt76_put_txwi(dev, entry.txwi);\n\t\t\twake = !flush;\n\t\t}\n\n\t\tif (!flush && q->tail == last)\n\t\t\tlast = readl(&q->regs->dma_idx);\n\t}\n\n\tspin_lock_bh(&q->lock);\n\n\tq->queued -= n_queued;\n\tfor (i = 0; i < ARRAY_SIZE(n_swq_queued); i++) {\n\t\tif (!n_swq_queued[i])\n\t\t\tcontinue;\n\n\t\tdev->q_tx[i].swq_queued -= n_swq_queued[i];\n\t}\n\n\tif (flush)\n\t\tmt76_dma_sync_idx(dev, q);\n\n\twake = wake && q->stopped &&\n\t       qid < IEEE80211_NUM_ACS && q->queued < q->ndesc - 8;\n\tif (wake)\n\t\tq->stopped = false;\n\n\tif (!q->queued)\n\t\twake_up(&dev->tx_wait);\n\n\tspin_unlock_bh(&q->lock);\n\n\tif (wake)\n\t\tieee80211_wake_queue(dev->hw, qid);\n}\n\nstatic void *\nmt76_dma_get_buf(struct mt76_dev *dev, struct mt76_queue *q, int idx,\n\t\t int *len, u32 *info, bool *more)\n{\n\tstruct mt76_queue_entry *e = &q->entry[idx];\n\tstruct mt76_desc *desc = &q->desc[idx];\n\tdma_addr_t buf_addr;\n\tvoid *buf = e->buf;\n\tint buf_len = SKB_WITH_OVERHEAD(q->buf_size);\n\n\tbuf_addr = le32_to_cpu(READ_ONCE(desc->buf0));\n\tif (len) {\n\t\tu32 ctl = le32_to_cpu(READ_ONCE(desc->ctrl));\n\t\t*len = FIELD_GET(MT_DMA_CTL_SD_LEN0, ctl);\n\t\t*more = !(ctl & MT_DMA_CTL_LAST_SEC0);\n\t}\n\n\tif (info)\n\t\t*info = le32_to_cpu(desc->info);\n\n\tdma_unmap_single(dev->dev, buf_addr, buf_len, DMA_FROM_DEVICE);\n\te->buf = NULL;\n\n\treturn buf;\n}\n\nstatic void *\nmt76_dma_dequeue(struct mt76_dev *dev, struct mt76_queue *q, bool flush,\n\t\t int *len, u32 *info, bool *more)\n{\n\tint idx = q->tail;\n\n\t*more = false;\n\tif (!q->queued)\n\t\treturn NULL;\n\n\tif (!flush && !(q->desc[idx].ctrl & cpu_to_le32(MT_DMA_CTL_DMA_DONE)))\n\t\treturn NULL;\n\n\tq->tail = (q->tail + 1) % q->ndesc;\n\tq->queued--;\n\n\treturn mt76_dma_get_buf(dev, q, idx, len, info, more);\n}\n\nstatic void\nmt76_dma_kick_queue(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\twritel(q->head, &q->regs->cpu_idx);\n}\n\nstatic int\nmt76_dma_tx_queue_skb_raw(struct mt76_dev *dev, enum mt76_txq_id qid,\n\t\t\t  struct sk_buff *skb, u32 tx_info)\n{\n\tstruct mt76_queue *q = dev->q_tx[qid].q;\n\tstruct mt76_queue_buf buf;\n\tdma_addr_t addr;\n\n\taddr = dma_map_single(dev->dev, skb->data, skb->len,\n\t\t\t      DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev->dev, addr)))\n\t\treturn -ENOMEM;\n\n\tbuf.addr = addr;\n\tbuf.len = skb->len;\n\n\tspin_lock_bh(&q->lock);\n\tmt76_dma_add_buf(dev, q, &buf, 1, tx_info, skb, NULL);\n\tmt76_dma_kick_queue(dev, q);\n\tspin_unlock_bh(&q->lock);\n\n\treturn 0;\n}\n\nstatic int\nmt76_dma_tx_queue_skb(struct mt76_dev *dev, enum mt76_txq_id qid,\n\t\t      struct sk_buff *skb, struct mt76_wcid *wcid,\n\t\t      struct ieee80211_sta *sta)\n{\n\tstruct mt76_queue *q = dev->q_tx[qid].q;\n\tstruct mt76_tx_info tx_info = {\n\t\t.skb = skb,\n\t};\n\tint len, n = 0, ret = -ENOMEM;\n\tstruct mt76_queue_entry e;\n\tstruct mt76_txwi_cache *t;\n\tstruct sk_buff *iter;\n\tdma_addr_t addr;\n\tu8 *txwi;\n\n\tt = mt76_get_txwi(dev);\n\tif (!t) {\n\t\tieee80211_free_txskb(dev->hw, skb);\n\t\treturn -ENOMEM;\n\t}\n\ttxwi = mt76_get_txwi_ptr(dev, t);\n\n\tskb->prev = skb->next = NULL;\n\tif (dev->drv->drv_flags & MT_DRV_TX_ALIGNED4_SKBS)\n\t\tmt76_insert_hdr_pad(skb);\n\n\tlen = skb_headlen(skb);\n\taddr = dma_map_single(dev->dev, skb->data, len, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev->dev, addr)))\n\t\tgoto free;\n\n\ttx_info.buf[n].addr = t->dma_addr;\n\ttx_info.buf[n++].len = dev->drv->txwi_size;\n\ttx_info.buf[n].addr = addr;\n\ttx_info.buf[n++].len = len;\n\n\tskb_walk_frags(skb, iter) {\n\t\tif (n == ARRAY_SIZE(tx_info.buf))\n\t\t\tgoto unmap;\n\n\t\taddr = dma_map_single(dev->dev, iter->data, iter->len,\n\t\t\t\t      DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(dev->dev, addr)))\n\t\t\tgoto unmap;\n\n\t\ttx_info.buf[n].addr = addr;\n\t\ttx_info.buf[n++].len = iter->len;\n\t}\n\ttx_info.nbuf = n;\n\n\tdma_sync_single_for_cpu(dev->dev, t->dma_addr, dev->drv->txwi_size,\n\t\t\t\tDMA_TO_DEVICE);\n\tret = dev->drv->tx_prepare_skb(dev, txwi, qid, wcid, sta, &tx_info);\n\tdma_sync_single_for_device(dev->dev, t->dma_addr, dev->drv->txwi_size,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret < 0)\n\t\tgoto unmap;\n\n\tif (q->queued + (tx_info.nbuf + 1) / 2 >= q->ndesc - 1) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap;\n\t}\n\n\treturn mt76_dma_add_buf(dev, q, tx_info.buf, tx_info.nbuf,\n\t\t\t\ttx_info.info, tx_info.skb, t);\n\nunmap:\n\tfor (n--; n > 0; n--)\n\t\tdma_unmap_single(dev->dev, tx_info.buf[n].addr,\n\t\t\t\t tx_info.buf[n].len, DMA_TO_DEVICE);\n\nfree:\n\te.skb = tx_info.skb;\n\te.txwi = t;\n\tdev->drv->tx_complete_skb(dev, qid, &e);\n\tmt76_put_txwi(dev, t);\n\treturn ret;\n}\n\nstatic int\nmt76_dma_rx_fill(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\tdma_addr_t addr;\n\tvoid *buf;\n\tint frames = 0;\n\tint len = SKB_WITH_OVERHEAD(q->buf_size);\n\tint offset = q->buf_offset;\n\n\tspin_lock_bh(&q->lock);\n\n\twhile (q->queued < q->ndesc - 1) {\n\t\tstruct mt76_queue_buf qbuf;\n\n\t\tbuf = page_frag_alloc(&q->rx_page, q->buf_size, GFP_ATOMIC);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\taddr = dma_map_single(dev->dev, buf, len, DMA_FROM_DEVICE);\n\t\tif (unlikely(dma_mapping_error(dev->dev, addr))) {\n\t\t\tskb_free_frag(buf);\n\t\t\tbreak;\n\t\t}\n\n\t\tqbuf.addr = addr + offset;\n\t\tqbuf.len = len - offset;\n\t\tmt76_dma_add_buf(dev, q, &qbuf, 1, 0, buf, NULL);\n\t\tframes++;\n\t}\n\n\tif (frames)\n\t\tmt76_dma_kick_queue(dev, q);\n\n\tspin_unlock_bh(&q->lock);\n\n\treturn frames;\n}\n\nstatic void\nmt76_dma_rx_cleanup(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\tstruct page *page;\n\tvoid *buf;\n\tbool more;\n\n\tspin_lock_bh(&q->lock);\n\tdo {\n\t\tbuf = mt76_dma_dequeue(dev, q, true, NULL, NULL, &more);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tskb_free_frag(buf);\n\t} while (1);\n\tspin_unlock_bh(&q->lock);\n\n\tif (!q->rx_page.va)\n\t\treturn;\n\n\tpage = virt_to_page(q->rx_page.va);\n\t__page_frag_cache_drain(page, q->rx_page.pagecnt_bias);\n\tmemset(&q->rx_page, 0, sizeof(q->rx_page));\n}\n\nstatic void\nmt76_dma_rx_reset(struct mt76_dev *dev, enum mt76_rxq_id qid)\n{\n\tstruct mt76_queue *q = &dev->q_rx[qid];\n\tint i;\n\n\tfor (i = 0; i < q->ndesc; i++)\n\t\tq->desc[i].ctrl &= ~cpu_to_le32(MT_DMA_CTL_DMA_DONE);\n\n\tmt76_dma_rx_cleanup(dev, q);\n\tmt76_dma_sync_idx(dev, q);\n\tmt76_dma_rx_fill(dev, q);\n\n\tif (!q->rx_head)\n\t\treturn;\n\n\tdev_kfree_skb(q->rx_head);\n\tq->rx_head = NULL;\n}\n\nstatic void\nmt76_add_fragment(struct mt76_dev *dev, struct mt76_queue *q, void *data,\n\t\t  int len, bool more)\n{\n\tstruct page *page = virt_to_head_page(data);\n\tint offset = data - page_address(page);\n\tstruct sk_buff *skb = q->rx_head;\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tif (shinfo->nr_frags < ARRAY_SIZE(shinfo->frags)) {\n\t\toffset += q->buf_offset;\n\t\tskb_add_rx_frag(skb, shinfo->nr_frags, page, offset, len,\n\t\t\t\tq->buf_size);\n\t}\n\n\tif (more)\n\t\treturn;\n\n\tq->rx_head = NULL;\n\tdev->drv->rx_skb(dev, q - dev->q_rx, skb);\n}\n\nstatic int\nmt76_dma_rx_process(struct mt76_dev *dev, struct mt76_queue *q, int budget)\n{\n\tint len, data_len, done = 0;\n\tstruct sk_buff *skb;\n\tunsigned char *data;\n\tbool more;\n\n\twhile (done < budget) {\n\t\tu32 info;\n\n\t\tdata = mt76_dma_dequeue(dev, q, false, &len, &info, &more);\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\tif (q->rx_head)\n\t\t\tdata_len = q->buf_size;\n\t\telse\n\t\t\tdata_len = SKB_WITH_OVERHEAD(q->buf_size);\n\n\t\tif (data_len < len + q->buf_offset) {\n\t\t\tdev_kfree_skb(q->rx_head);\n\t\t\tq->rx_head = NULL;\n\n\t\t\tskb_free_frag(data);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (q->rx_head) {\n\t\t\tmt76_add_fragment(dev, q, data, len, more);\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb = build_skb(data, q->buf_size);\n\t\tif (!skb) {\n\t\t\tskb_free_frag(data);\n\t\t\tcontinue;\n\t\t}\n\t\tskb_reserve(skb, q->buf_offset);\n\n\t\tif (q == &dev->q_rx[MT_RXQ_MCU]) {\n\t\t\tu32 *rxfce = (u32 *)skb->cb;\n\t\t\t*rxfce = info;\n\t\t}\n\n\t\t__skb_put(skb, len);\n\t\tdone++;\n\n\t\tif (more) {\n\t\t\tq->rx_head = skb;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev->drv->rx_skb(dev, q - dev->q_rx, skb);\n\t}\n\n\tmt76_dma_rx_fill(dev, q);\n\treturn done;\n}\n\nstatic int\nmt76_dma_rx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct mt76_dev *dev;\n\tint qid, done = 0, cur;\n\n\tdev = container_of(napi->dev, struct mt76_dev, napi_dev);\n\tqid = napi - dev->napi;\n\n\trcu_read_lock();\n\n\tdo {\n\t\tcur = mt76_dma_rx_process(dev, &dev->q_rx[qid], budget - done);\n\t\tmt76_rx_poll_complete(dev, qid, napi);\n\t\tdone += cur;\n\t} while (cur && done < budget);\n\n\trcu_read_unlock();\n\n\tif (done < budget && napi_complete(napi))\n\t\tdev->drv->rx_poll_complete(dev, qid);\n\n\treturn done;\n}\n\nstatic int\nmt76_dma_init(struct mt76_dev *dev)\n{\n\tint i;\n\n\tinit_dummy_netdev(&dev->napi_dev);\n\n\tfor (i = 0; i < ARRAY_SIZE(dev->q_rx); i++) {\n\t\tnetif_napi_add(&dev->napi_dev, &dev->napi[i], mt76_dma_rx_poll,\n\t\t\t       64);\n\t\tmt76_dma_rx_fill(dev, &dev->q_rx[i]);\n\t\tskb_queue_head_init(&dev->rx_skb[i]);\n\t\tnapi_enable(&dev->napi[i]);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct mt76_queue_ops mt76_dma_ops = {\n\t.init = mt76_dma_init,\n\t.alloc = mt76_dma_alloc_queue,\n\t.tx_queue_skb_raw = mt76_dma_tx_queue_skb_raw,\n\t.tx_queue_skb = mt76_dma_tx_queue_skb,\n\t.tx_cleanup = mt76_dma_tx_cleanup,\n\t.rx_reset = mt76_dma_rx_reset,\n\t.kick = mt76_dma_kick_queue,\n};\n\nvoid mt76_dma_attach(struct mt76_dev *dev)\n{\n\tdev->queue_ops = &mt76_dma_ops;\n}\nEXPORT_SYMBOL_GPL(mt76_dma_attach);\n\nvoid mt76_dma_cleanup(struct mt76_dev *dev)\n{\n\tint i;\n\n\tnetif_napi_del(&dev->tx_napi);\n\tfor (i = 0; i < ARRAY_SIZE(dev->q_tx); i++)\n\t\tmt76_dma_tx_cleanup(dev, i, true);\n\n\tfor (i = 0; i < ARRAY_SIZE(dev->q_rx); i++) {\n\t\tnetif_napi_del(&dev->napi[i]);\n\t\tmt76_dma_rx_cleanup(dev, &dev->q_rx[i]);\n\t}\n}\nEXPORT_SYMBOL_GPL(mt76_dma_cleanup);\n"], "filenames": ["drivers/net/wireless/mediatek/mt76/dma.c"], "buggy_code_start_loc": [450], "buggy_code_end_loc": [454], "fixing_code_start_loc": [450], "fixing_code_end_loc": [457], "type": "CWE-120", "message": "An array overflow was discovered in mt76_add_fragment in drivers/net/wireless/mediatek/mt76/dma.c in the Linux kernel before 5.5.10, aka CID-b102f0c522cf. An oversized packet with too many rx fragments can corrupt memory of adjacent pages.", "other": {"cve": {"id": "CVE-2020-12465", "sourceIdentifier": "cve@mitre.org", "published": "2020-04-29T19:15:12.907", "lastModified": "2020-06-08T13:15:13.403", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "An array overflow was discovered in mt76_add_fragment in drivers/net/wireless/mediatek/mt76/dma.c in the Linux kernel before 5.5.10, aka CID-b102f0c522cf. An oversized packet with too many rx fragments can corrupt memory of adjacent pages."}, {"lang": "es", "value": "Se descubri\u00f3 un desbordamiento de matriz en la funci\u00f3n mt76_add_fragment en el archivo drivers/net/wireless/mediatek/mt76/dma.c en el kernel de Linux versiones anteriores a  la versi\u00f3n 5.5.10, tambi\u00e9n se conoce como CID-b102f0c522cf. Un paquete de gran tama\u00f1o con muchos fragmentos rx puede corromper la memoria de p\u00e1ginas adyacentes."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 6.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 0.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-120"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.5.10", "matchCriteriaId": "F8FFF863-3E0D-410C-A030-32383C1EFFB2"}]}]}], "references": [{"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.5.10", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=b102f0c522cf668c8382c56a4f771b37d011cda2", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/b102f0c522cf668c8382c56a4f771b37d011cda2", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20200608-0001/", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/b102f0c522cf668c8382c56a4f771b37d011cda2"}}