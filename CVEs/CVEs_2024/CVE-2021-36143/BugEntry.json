{"buggy_code": ["/*-\n * Copyright (c) 2013  Chris Torek <torek @ torek net>\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n * SUCH DAMAGE.\n */\n\n#include <sys/uio.h>\n#include <stdio.h>\n#include <stddef.h>\n#include <pthread.h>\n#include <string.h>\n#include <stdlib.h>\n\n#include \"dm.h\"\n#include \"pci_core.h\"\n#include \"virtio.h\"\n#include \"timer.h\"\n#include <atomic.h>\n\n/*\n * Functions for dealing with generalized \"virtual devices\" as\n * defined by <https://www.google.com/#output=search&q=virtio+spec>\n */\n\n/*\n * In case we decide to relax the \"virtio struct comes at the\n * front of virtio-based device struct\" constraint, let's use\n * this to convert.\n */\n#define DEV_STRUCT(vs) ((void *)(vs))\n\nstatic uint8_t virtio_poll_enabled;\nstatic size_t virtio_poll_interval;\n\nstatic void\nvirtio_start_timer(struct acrn_timer *timer, time_t sec, time_t nsec)\n{\n\tstruct itimerspec ts;\n\n\t/* setting the interval time */\n\tts.it_interval.tv_sec = 0;\n\tts.it_interval.tv_nsec = 0;\n\t/* set the delay time it will be started when timer_setting */\n\tts.it_value.tv_sec = sec;\n\tts.it_value.tv_nsec = nsec;\n\tif (acrn_timer_settime(timer, &ts) != 0) {\n\t\tpr_err(\"acrn timer set time failed\\n\");\n\t\treturn;\n\t}\n}\n\nstatic void\nvirtio_poll_timer(void *arg, uint64_t nexp)\n{\n\tstruct virtio_base *base;\n\tstruct virtio_ops *vops;\n\tstruct virtio_vq_info *vq;\n\tconst char *name;\n\tint i;\n\n\tbase = arg;\n\tvops = base->vops;\n\tname = vops->name;\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tbase->polling_in_progress = 1;\n\n\tfor (i = 0; i < base->vops->nvq; i++) {\n\t\tvq = &base->queues[i];\n\t\tif(!vq_ring_ready(vq))\n\t\t\tcontinue;\n\t\tvq->used->flags |= VRING_USED_F_NO_NOTIFY;\n\t\t/* TODO: call notify when necessary */\n\t\tif (vq->notify)\n\t\t\t(*vq->notify)(DEV_STRUCT(base), vq);\n\t\telse if (vops->qnotify)\n\t\t\t(*vops->qnotify)(DEV_STRUCT(base), vq);\n\t\telse\n\t\t\tpr_err(\"%s: qnotify queue %d: missing vq/vops notify\\r\\n\",\n\t\t\t\tname, i);\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n\n\tvirtio_start_timer(&base->polling_timer, 0, virtio_poll_interval);\n}\n\n/**\n * @brief Link a virtio_base to its constants, the virtio device,\n * and the PCI emulation.\n *\n * @param base Pointer to struct virtio_base.\n * @param vops Pointer to struct virtio_ops.\n * @param pci_virtio_dev Pointer to instance of certain virtio device.\n * @param dev Pointer to struct pci_vdev which emulates a PCI device.\n * @param queues Pointer to struct virtio_vq_info, normally an array.\n *\n * @return None\n */\nvoid\nvirtio_linkup(struct virtio_base *base, struct virtio_ops *vops,\n\t      void *pci_virtio_dev, struct pci_vdev *dev,\n\t      struct virtio_vq_info *queues,\n\t      int backend_type)\n{\n\tint i;\n\n\t/* base and pci_virtio_dev addresses must match */\n\tif ((void *)base != pci_virtio_dev) {\n\t\tpr_err(\"virtio_base and pci_virtio_dev addresses don't match!\\n\");\n\t\treturn;\n\t}\n\tbase->vops = vops;\n\tbase->dev = dev;\n\tdev->arg = base;\n\tbase->backend_type = backend_type;\n\n\tbase->queues = queues;\n\tfor (i = 0; i < vops->nvq; i++) {\n\t\tqueues[i].base = base;\n\t\tqueues[i].num = i;\n\t}\n}\n\n/**\n * @brief Reset device (device-wide).\n *\n * This erases all queues, i.e., all the queues become invalid.\n * But we don't wipe out the internal pointers, by just clearing\n * the VQ_ALLOC flag.\n *\n * It resets negotiated features to \"none\".\n * If MSI-X is enabled, this also resets all the vectors to NO_VECTOR.\n *\n * @param base Pointer to struct virtio_base.\n *\n * @return None\n */\nvoid\nvirtio_reset_dev(struct virtio_base *base)\n{\n\tstruct virtio_vq_info *vq;\n\tint i, nvq;\n\n/* if (base->mtx) */\n/* assert(pthread_mutex_isowned_np(base->mtx)); */\n\n\tacrn_timer_deinit(&base->polling_timer);\n\tbase->polling_in_progress = 0;\n\n\tnvq = base->vops->nvq;\n\tfor (vq = base->queues, i = 0; i < nvq; vq++, i++) {\n\t\tvq->flags = 0;\n\t\tvq->last_avail = 0;\n\t\tvq->save_used = 0;\n\t\tvq->pfn = 0;\n\t\tvq->msix_idx = VIRTIO_MSI_NO_VECTOR;\n\t\tvq->gpa_desc[0] = 0;\n\t\tvq->gpa_desc[1] = 0;\n\t\tvq->gpa_avail[0] = 0;\n\t\tvq->gpa_avail[1] = 0;\n\t\tvq->gpa_used[0] = 0;\n\t\tvq->gpa_used[1] = 0;\n\t\tvq->enabled = 0;\n\t}\n\tbase->negotiated_caps = 0;\n\tbase->curq = 0;\n\t/* base->status = 0; -- redundant */\n\tif (base->isr)\n\t\tpci_lintr_deassert(base->dev);\n\tbase->isr = 0;\n\tbase->msix_cfg_idx = VIRTIO_MSI_NO_VECTOR;\n\tbase->device_feature_select = 0;\n\tbase->driver_feature_select = 0;\n\tbase->config_generation = 0;\n}\n\n/**\n * @brief Set I/O BAR (usually 0) to map PCI config registers.\n *\n * @param base Pointer to struct virtio_base.\n * @param barnum Which BAR[0..5] to use.\n *\n * @return None\n */\nvoid\nvirtio_set_io_bar(struct virtio_base *base, int barnum)\n{\n\tsize_t size;\n\n\t/*\n\t * ??? should we use VIRTIO_PCI_CONFIG_OFF(0) if MSI-X\n\t * is disabled? Existing code did not...\n\t */\n\tsize = VIRTIO_PCI_CONFIG_OFF(1) + base->vops->cfgsize;\n\tpci_emul_alloc_bar(base->dev, barnum, PCIBAR_IO, size);\n\tbase->legacy_pio_bar_idx = barnum;\n}\n\n/**\n * @brief Initialize MSI-X vector capabilities if we're to use MSI-X,\n * or MSI capabilities if not.\n *\n * We assume we want one MSI-X vector per queue, here, plus one\n * for the config vec.\n *\n *\n * @param base Pointer to struct virtio_base.\n * @param barnum Which BAR[0..5] to use.\n * @param use_msix If using MSI-X.\n *\n * @return 0 on success and non-zero on fail.\n */\nint\nvirtio_intr_init(struct virtio_base *base, int barnum, int use_msix)\n{\n\tint nvec;\n\n\tif (use_msix) {\n\t\tbase->flags |= VIRTIO_USE_MSIX;\n\t\tVIRTIO_BASE_LOCK(base);\n\t\tvirtio_reset_dev(base); /* set all vectors to NO_VECTOR */\n\t\tVIRTIO_BASE_UNLOCK(base);\n\t\tnvec = base->vops->nvq + 1;\n\t\tif (pci_emul_add_msixcap(base->dev, nvec, barnum))\n\t\t\treturn -1;\n\t} else\n\t\tbase->flags &= ~VIRTIO_USE_MSIX;\n\n\t/* Only 1 MSI vector for acrn-dm */\n\tpci_emul_add_msicap(base->dev, 1);\n\n\t/* Legacy interrupts are mandatory for virtio devices */\n\tpci_lintr_request(base->dev);\n\n\treturn 0;\n}\n\n/**\n * @brief Initialize MSI-X vector capabilities if we're to use MSI-X,\n * or MSI capabilities if not.\n *\n * Wrapper function for virtio_intr_init() for cases we directly use\n * BAR 1 for MSI-X capabilities.\n *\n * @param base Pointer to struct virtio_base.\n * @param use_msix If using MSI-X.\n *\n * @return 0 on success and non-zero on fail.\n */\nint\nvirtio_interrupt_init(struct virtio_base *base, int use_msix)\n{\n\treturn virtio_intr_init(base, 1, use_msix);\n}\n\n/*\n * Initialize the currently-selected virtio queue (base->curq).\n * The guest just gave us a page frame number, from which we can\n * calculate the addresses of the queue.\n * This interface is only valid for virtio legacy.\n */\nstatic void\nvirtio_vq_init(struct virtio_base *base, uint32_t pfn)\n{\n\tstruct virtio_vq_info *vq;\n\tuint64_t phys;\n\tsize_t size;\n\tchar *vb;\n\n\tvq = &base->queues[base->curq];\n\tvq->pfn = pfn;\n\tphys = (uint64_t)pfn << VRING_PAGE_BITS;\n\tsize = vring_size(vq->qsize, VIRTIO_PCI_VRING_ALIGN);\n\tvb = paddr_guest2host(base->dev->vmctx, phys, size);\n\tif (!vb)\n\t\tgoto error;\n\n\t/* First page(s) are descriptors... */\n\tvq->desc = (struct vring_desc *)vb;\n\tvb += vq->qsize * sizeof(struct vring_desc);\n\n\t/* ... immediately followed by \"avail\" ring (entirely uint16_t's) */\n\tvq->avail = (struct vring_avail *)vb;\n\tvb += (2 + vq->qsize + 1) * sizeof(uint16_t);\n\n\t/* Then it's rounded up to the next page... */\n\tvb = (char *)roundup2((uintptr_t)vb, VIRTIO_PCI_VRING_ALIGN);\n\n\t/* ... and the last page(s) are the used ring. */\n\tvq->used = (struct vring_used *)vb;\n\n\t/* Start at 0 when we use it. */\n\tvq->last_avail = 0;\n\tvq->save_used = 0;\n\n\t/* Mark queue as allocated after initialization is complete. */\n\tmb();\n\tvq->flags = VQ_ALLOC;\n\n\treturn;\n\nerror:\n\tvq->flags = 0;\n\tpr_err(\"%s: vq enable failed\\n\", __func__);\n}\n\n/*\n * Initialize the currently-selected virtio queue (base->curq).\n * The guest just gave us the gpa of desc array, avail ring and\n * used ring, from which we can initialize the virtqueue.\n * This interface is only valid for virtio modern.\n */\nstatic void\nvirtio_vq_enable(struct virtio_base *base)\n{\n\tstruct virtio_vq_info *vq;\n\tuint16_t qsz;\n\tuint64_t phys;\n\tsize_t size;\n\tchar *vb;\n\n\tvq = &base->queues[base->curq];\n\tqsz = vq->qsize;\n\n\t/* descriptors */\n\tphys = (((uint64_t)vq->gpa_desc[1]) << 32) | vq->gpa_desc[0];\n\tsize = qsz * sizeof(struct vring_desc);\n\tvb = paddr_guest2host(base->dev->vmctx, phys, size);\n\tif (!vb)\n\t\tgoto error;\n\tvq->desc = (struct vring_desc *)vb;\n\n\t/* available ring */\n\tphys = (((uint64_t)vq->gpa_avail[1]) << 32) | vq->gpa_avail[0];\n\tsize = (2 + qsz + 1) * sizeof(uint16_t);\n\tvb = paddr_guest2host(base->dev->vmctx, phys, size);\n\tif (!vb)\n\t\tgoto error;\n\n\tvq->avail = (struct vring_avail *)vb;\n\n\t/* used ring */\n\tphys = (((uint64_t)vq->gpa_used[1]) << 32) | vq->gpa_used[0];\n\tsize = sizeof(uint16_t) * 3 + sizeof(struct vring_used_elem) * qsz;\n\tvb = paddr_guest2host(base->dev->vmctx, phys, size);\n\tif (!vb)\n\t\tgoto error;\n\tvq->used = (struct vring_used *)vb;\n\n\t/* Start at 0 when we use it. */\n\tvq->last_avail = 0;\n\tvq->save_used = 0;\n\n\t/* Mark queue as enabled. */\n\tvq->enabled = true;\n\n\t/* Mark queue as allocated after initialization is complete. */\n\tmb();\n\tvq->flags = VQ_ALLOC;\n\treturn;\n error:\n\tvq->flags = 0;\n\tpr_err(\"%s: vq enable failed\\n\", __func__);\n}\n\n/*\n * Helper inline for vq_getchain(): record the i'th \"real\"\n * descriptor.\n * Return 0 on success and -1 when i is out of range  or mapping\n *        fails.\n */\nstatic inline int\n_vq_record(int i, volatile struct vring_desc *vd, struct vmctx *ctx,\n\t   struct iovec *iov, int n_iov, uint16_t *flags) {\n\n\tvoid *host_addr;\n\n\tif (i >= n_iov)\n\t\treturn -1;\n\thost_addr = paddr_guest2host(ctx, vd->addr, vd->len);\n\tif (!host_addr)\n\t\treturn -1;\n\tiov[i].iov_base = host_addr;\n\tiov[i].iov_len = vd->len;\n\tif (flags != NULL)\n\t\tflags[i] = vd->flags;\n\treturn 0;\n}\n#define\tVQ_MAX_DESCRIPTORS\t512\t/* see below */\n\n/*\n * Examine the chain of descriptors starting at the \"next one\" to\n * make sure that they describe a sensible request.  If so, return\n * the number of \"real\" descriptors that would be needed/used in\n * acting on this request.  This may be smaller than the number of\n * available descriptors, e.g., if there are two available but\n * they are two separate requests, this just returns 1.  Or, it\n * may be larger: if there are indirect descriptors involved,\n * there may only be one descriptor available but it may be an\n * indirect pointing to eight more.  We return 8 in this case,\n * i.e., we do not count the indirect descriptors, only the \"real\"\n * ones.\n *\n * Basically, this vets the flags and vd_next field of each\n * descriptor and tells you how many are involved.  Since some may\n * be indirect, this also needs the vmctx (in the pci_vdev\n * at base->dev) so that it can find indirect descriptors.\n *\n * As we process each descriptor, we copy and adjust it (guest to\n * host address wise, also using the vmtctx) into the given iov[]\n * array (of the given size).  If the array overflows, we stop\n * placing values into the array but keep processing descriptors,\n * up to VQ_MAX_DESCRIPTORS, before giving up and returning -1.\n * So you, the caller, must not assume that iov[] is as big as the\n * return value (you can process the same thing twice to allocate\n * a larger iov array if needed, or supply a zero length to find\n * out how much space is needed).\n *\n * If you want to verify the WRITE flag on each descriptor, pass a\n * non-NULL \"flags\" pointer to an array of \"uint16_t\" of the same size\n * as n_iov and we'll copy each flags field after unwinding any\n * indirects.\n *\n * If some descriptor(s) are invalid, this prints a diagnostic message\n * and returns -1.  If no descriptors are ready now it simply returns 0.\n *\n * You are assumed to have done a vq_ring_ready() if needed (note\n * that vq_has_descs() does one).\n */\nint\nvq_getchain(struct virtio_vq_info *vq, uint16_t *pidx,\n\t    struct iovec *iov, int n_iov, uint16_t *flags)\n{\n\tint i;\n\tu_int ndesc, n_indir;\n\tu_int idx, next;\n\n\tvolatile struct vring_desc *vdir, *vindir, *vp;\n\tstruct vmctx *ctx;\n\tstruct virtio_base *base;\n\tconst char *name;\n\n\tbase = vq->base;\n\tname = base->vops->name;\n\n\t/*\n\t * Note: it's the responsibility of the guest not to\n\t * update vq->avail->idx until all of the descriptors\n\t * the guest has written are valid (including all their\n\t * next fields and vd_flags).\n\t *\n\t * Compute (last_avail - idx) in integers mod 2**16.  This is\n\t * the number of descriptors the device has made available\n\t * since the last time we updated vq->last_avail.\n\t *\n\t * We just need to do the subtraction as an unsigned int,\n\t * then trim off excess bits.\n\t */\n\tidx = vq->last_avail;\n\tndesc = (uint16_t)((u_int)vq->avail->idx - idx);\n\tif (ndesc == 0)\n\t\treturn 0;\n\tif (ndesc > vq->qsize) {\n\t\t/* XXX need better way to diagnose issues */\n\t\tpr_err(\"%s: ndesc (%u) out of range, driver confused?\\r\\n\",\n\t\t    name, (u_int)ndesc);\n\t\treturn -1;\n\t}\n\n\t/*\n\t * Now count/parse \"involved\" descriptors starting from\n\t * the head of the chain.\n\t *\n\t * To prevent loops, we could be more complicated and\n\t * check whether we're re-visiting a previously visited\n\t * index, but we just abort if the count gets excessive.\n\t */\n\tctx = base->dev->vmctx;\n\t*pidx = next = vq->avail->ring[idx & (vq->qsize - 1)];\n\tvq->last_avail++;\n\tfor (i = 0; i < VQ_MAX_DESCRIPTORS; next = vdir->next) {\n\t\tif (next >= vq->qsize) {\n\t\t\tpr_err(\"%s: descriptor index %u out of range, \"\n\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t    name, next);\n\t\t\treturn -1;\n\t\t}\n\t\tvdir = &vq->desc[next];\n\t\tif ((vdir->flags & VRING_DESC_F_INDIRECT) == 0) {\n\t\t\tif (_vq_record(i, vdir, ctx, iov, n_iov, flags)) {\n\t\t\t\tpr_err(\"%s: mapping to host failed\\r\\n\", name);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\ti++;\n\t\t} else if ((base->device_caps &\n\t\t    (1 << VIRTIO_RING_F_INDIRECT_DESC)) == 0) {\n\t\t\tpr_err(\"%s: descriptor has forbidden INDIRECT flag, \"\n\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t    name);\n\t\t\treturn -1;\n\t\t} else {\n\t\t\tn_indir = vdir->len / 16;\n\t\t\tif ((vdir->len & 0xf) || n_indir == 0) {\n\t\t\t\tpr_err(\"%s: invalid indir len 0x%x, \"\n\t\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t\t    name, (u_int)vdir->len);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tvindir = paddr_guest2host(ctx,\n\t\t\t    vdir->addr, vdir->len);\n\n\t\t\tif (!vindir) {\n\t\t\t\tpr_err(\"%s cannot get host memory\\r\\n\", name);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Indirects start at the 0th, then follow\n\t\t\t * their own embedded \"next\"s until those run\n\t\t\t * out.  Each one's indirect flag must be off\n\t\t\t * (we don't really have to check, could just\n\t\t\t * ignore errors...).\n\t\t\t */\n\t\t\tnext = 0;\n\t\t\tfor (;;) {\n\t\t\t\tvp = &vindir[next];\n\t\t\t\tif (vp->flags & VRING_DESC_F_INDIRECT) {\n\t\t\t\t\tpr_err(\"%s: indirect desc has INDIR flag,\"\n\t\t\t\t\t    \" driver confused?\\r\\n\",\n\t\t\t\t\t    name);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tif (_vq_record(i, vp, ctx, iov, n_iov, flags)) {\n\t\t\t\t\tpr_err(\"%s: mapping to host failed\\r\\n\", name);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tif (++i > VQ_MAX_DESCRIPTORS)\n\t\t\t\t\tgoto loopy;\n\t\t\t\tif ((vp->flags & VRING_DESC_F_NEXT) == 0)\n\t\t\t\t\tbreak;\n\t\t\t\tnext = vp->next;\n\t\t\t\tif (next >= n_indir) {\n\t\t\t\t\tpr_err(\"%s: invalid next %u > %u, \"\n\t\t\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t\t\t    name, (u_int)next, n_indir);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ((vdir->flags & VRING_DESC_F_NEXT) == 0)\n\t\t\treturn i;\n\t}\nloopy:\n\tpr_err(\"%s: descriptor loop? count > %d - driver confused?\\r\\n\",\n\t    name, i);\n\treturn -1;\n}\n\n/*\n * Return the currently-first request chain back to the available queue.\n *\n * (This chain is the one you handled when you called vq_getchain()\n * and used its positive return value.)\n */\nvoid\nvq_retchain(struct virtio_vq_info *vq)\n{\n\tvq->last_avail--;\n}\n\n/*\n * Return specified request chain to the guest, setting its I/O length\n * to the provided value.\n *\n * (This chain is the one you handled when you called vq_getchain()\n * and used its positive return value.)\n */\nvoid\nvq_relchain(struct virtio_vq_info *vq, uint16_t idx, uint32_t iolen)\n{\n\tuint16_t uidx, mask;\n\tvolatile struct vring_used *vuh;\n\tvolatile struct vring_used_elem *vue;\n\n\t/*\n\t * Notes:\n\t *  - mask is N-1 where N is a power of 2 so computes x % N\n\t *  - vuh points to the \"used\" data shared with guest\n\t *  - vue points to the \"used\" ring entry we want to update\n\t *  - head is the same value we compute in vq_iovecs().\n\t *\n\t * (I apologize for the two fields named idx; the\n\t * virtio spec calls the one that vue points to, \"id\"...)\n\t */\n\tmask = vq->qsize - 1;\n\tvuh = vq->used;\n\n\tuidx = vuh->idx;\n\tvue = &vuh->ring[uidx++ & mask];\n\tvue->id = idx;\n\tvue->len = iolen;\n\tvuh->idx = uidx;\n}\n\n/*\n * Driver has finished processing \"available\" chains and calling\n * vq_relchain on each one.  If driver used all the available\n * chains, used_all should be set.\n *\n * If the \"used\" index moved we may need to inform the guest, i.e.,\n * deliver an interrupt.  Even if the used index did NOT move we\n * may need to deliver an interrupt, if the avail ring is empty and\n * we are supposed to interrupt on empty.\n *\n * Note that used_all_avail is provided by the caller because it's\n * a snapshot of the ring state when he decided to finish interrupt\n * processing -- it's possible that descriptors became available after\n * that point.  (It's also typically a constant 1/True as well.)\n */\nvoid\nvq_endchains(struct virtio_vq_info *vq, int used_all_avail)\n{\n\tstruct virtio_base *base;\n\tuint16_t event_idx, new_idx, old_idx;\n\tint intr;\n\n\t/*\n\t * Interrupt generation: if we're using EVENT_IDX,\n\t * interrupt if we've crossed the event threshold.\n\t * Otherwise interrupt is generated if we added \"used\" entries,\n\t * but suppressed by VRING_AVAIL_F_NO_INTERRUPT.\n\t *\n\t * In any case, though, if NOTIFY_ON_EMPTY is set and the\n\t * entire avail was processed, we need to interrupt always.\n\t */\n\n\tatomic_thread_fence();\n\n\tbase = vq->base;\n\told_idx = vq->save_used;\n\tvq->save_used = new_idx = vq->used->idx;\n\tif (used_all_avail &&\n\t    (base->negotiated_caps & (1 << VIRTIO_F_NOTIFY_ON_EMPTY)))\n\t\tintr = 1;\n\telse if (base->negotiated_caps & (1 << VIRTIO_RING_F_EVENT_IDX)) {\n\t\tevent_idx = VQ_USED_EVENT_IDX(vq);\n\t\t/*\n\t\t * This calculation is per docs and the kernel\n\t\t * (see src/sys/dev/virtio/virtio_ring.h).\n\t\t */\n\t\tintr = (uint16_t)(new_idx - event_idx - 1) <\n\t\t\t(uint16_t)(new_idx - old_idx);\n\t} else {\n\t\tintr = new_idx != old_idx &&\n\t\t    !(vq->avail->flags & VRING_AVAIL_F_NO_INTERRUPT);\n\t}\n\tif (intr)\n\t\tvq_interrupt(base, vq);\n}\n\n/**\n * @brief Helper function for clearing used ring flags.\n *\n * Driver should always use this helper function to clear used ring flags.\n * For virtio poll mode, in order to avoid trap, we should never really\n * clear used ring flags.\n *\n * @param base Pointer to struct virtio_base.\n * @param vq Pointer to struct virtio_vq_info.\n *\n * @return None\n */\nvoid vq_clear_used_ring_flags(struct virtio_base *base, struct virtio_vq_info *vq)\n{\n\tint backend_type = base->backend_type;\n\tint polling_in_progress = base->polling_in_progress;\n\n\t/* we should never unmask notification in polling mode */\n\tif (virtio_poll_enabled && backend_type == BACKEND_VBSU && polling_in_progress == 1)\n\t\treturn;\n\n\tvq->used->flags &= ~VRING_USED_F_NO_NOTIFY;\n}\n\nstruct config_reg {\n\tuint16_t\toffset;\t/* register offset */\n\tuint8_t\t\tsize;\t/* size (bytes) */\n\tuint8_t\t\tro;\t/* true => reg is read only */\n\tconst char\t*name;\t/* name of reg */\n};\n\n/* Note: these are in sorted order to make for a fast search */\nstatic struct config_reg legacy_config_regs[] = {\n\t{ VIRTIO_PCI_HOST_FEATURES,\t4, 1, \"HOSTCAP\" },\n\t{ VIRTIO_PCI_GUEST_FEATURES,\t4, 0, \"GUESTCAP\" },\n\t{ VIRTIO_PCI_QUEUE_PFN,\t\t4, 0, \"PFN\" },\n\t{ VIRTIO_PCI_QUEUE_NUM,\t\t2, 1, \"QNUM\" },\n\t{ VIRTIO_PCI_QUEUE_SEL,\t\t2, 0, \"QSEL\" },\n\t{ VIRTIO_PCI_QUEUE_NOTIFY,\t2, 0, \"QNOTIFY\" },\n\t{ VIRTIO_PCI_STATUS,\t\t1, 0, \"STATUS\" },\n\t{ VIRTIO_PCI_ISR,\t\t1, 0, \"ISR\" },\n\t{ VIRTIO_MSI_CONFIG_VECTOR,\t2, 0, \"CFGVEC\" },\n\t{ VIRTIO_MSI_QUEUE_VECTOR,\t2, 0, \"QVEC\" },\n};\n\n/* Note: these are in sorted order to make for a fast search */\nstatic struct config_reg modern_config_regs[] = {\n\t{ VIRTIO_PCI_COMMON_DFSELECT,\t\t4, 0, \"DFSELECT\" },\n\t{ VIRTIO_PCI_COMMON_DF,\t\t\t4, 1, \"DF\" },\n\t{ VIRTIO_PCI_COMMON_GFSELECT,\t\t4, 0, \"GFSELECT\" },\n\t{ VIRTIO_PCI_COMMON_GF,\t\t\t4, 0, \"GF\" },\n\t{ VIRTIO_PCI_COMMON_MSIX,\t\t2, 0, \"MSIX\" },\n\t{ VIRTIO_PCI_COMMON_NUMQ,\t\t2, 1, \"NUMQ\" },\n\t{ VIRTIO_PCI_COMMON_STATUS,\t\t1, 0, \"STATUS\" },\n\t{ VIRTIO_PCI_COMMON_CFGGENERATION,\t1, 1, \"CFGGENERATION\" },\n\t{ VIRTIO_PCI_COMMON_Q_SELECT,\t\t2, 0, \"Q_SELECT\" },\n\t{ VIRTIO_PCI_COMMON_Q_SIZE,\t\t2, 0, \"Q_SIZE\" },\n\t{ VIRTIO_PCI_COMMON_Q_MSIX,\t\t2, 0, \"Q_MSIX\" },\n\t{ VIRTIO_PCI_COMMON_Q_ENABLE,\t\t2, 0, \"Q_ENABLE\" },\n\t{ VIRTIO_PCI_COMMON_Q_NOFF,\t\t2, 1, \"Q_NOFF\" },\n\t{ VIRTIO_PCI_COMMON_Q_DESCLO,\t\t4, 0, \"Q_DESCLO\" },\n\t{ VIRTIO_PCI_COMMON_Q_DESCHI,\t\t4, 0, \"Q_DESCHI\" },\n\t{ VIRTIO_PCI_COMMON_Q_AVAILLO,\t\t4, 0, \"Q_AVAILLO\" },\n\t{ VIRTIO_PCI_COMMON_Q_AVAILHI,\t\t4, 0, \"Q_AVAILHI\" },\n\t{ VIRTIO_PCI_COMMON_Q_USEDLO,\t\t4, 0, \"Q_USEDLO\" },\n\t{ VIRTIO_PCI_COMMON_Q_USEDHI,\t\t4, 0, \"Q_USEDHI\" },\n};\n\nstatic inline const struct config_reg *\nvirtio_find_cr(const struct config_reg *p_cr_array, u_int array_size,\n\t       int offset) {\n\tu_int hi, lo, mid;\n\tconst struct config_reg *cr;\n\n\tlo = 0;\n\thi = array_size - 1;\n\twhile (hi >= lo) {\n\t\tmid = (hi + lo) >> 1;\n\t\tcr = p_cr_array + mid;\n\t\tif (cr->offset == offset)\n\t\t\treturn cr;\n\t\tif (cr->offset < offset)\n\t\t\tlo = mid + 1;\n\t\telse\n\t\t\thi = mid - 1;\n\t}\n\treturn NULL;\n}\n\nstatic inline const struct config_reg *\nvirtio_find_legacy_cr(int offset) {\n\treturn virtio_find_cr(legacy_config_regs,\n\t\tsizeof(legacy_config_regs) / sizeof(*legacy_config_regs),\n\t\toffset);\n}\n\nstatic inline const struct config_reg *\nvirtio_find_modern_cr(int offset) {\n\treturn virtio_find_cr(modern_config_regs,\n\t\tsizeof(modern_config_regs) / sizeof(*modern_config_regs),\n\t\toffset);\n}\n\n/*\n * Handle pci config space reads.\n * If it's to the MSI-X info, do that.\n * If it's part of the virtio standard stuff, do that.\n * Otherwise dispatch to the actual driver.\n */\nstatic uint64_t\nvirtio_pci_legacy_read(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t       int baridx, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst struct config_reg *cr;\n\tuint64_t virtio_config_size, max;\n\tconst char *name;\n\tuint32_t newoff;\n\tuint32_t value;\n\tint error = -1;\n\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tvops = base->vops;\n\tname = vops->name;\n\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\n\tif (size != 1 && size != 2 && size != 4)\n\t\tgoto bad;\n\n\tif (pci_msix_enabled(dev))\n\t\tvirtio_config_size = VIRTIO_PCI_CONFIG_OFF(1);\n\telse\n\t\tvirtio_config_size = VIRTIO_PCI_CONFIG_OFF(0);\n\n\tif (offset >= virtio_config_size) {\n\t\t/*\n\t\t * Subtract off the standard size (including MSI-X\n\t\t * registers if enabled) and dispatch to underlying driver.\n\t\t * If that fails, fall into general code.\n\t\t */\n\t\tnewoff = offset - virtio_config_size;\n\t\tmax = vops->cfgsize ? vops->cfgsize : 0x100000000;\n\t\tif (newoff + size > max)\n\t\t\tgoto bad;\n\n\t\tif (vops->cfgread) {\n\t\t\terror = (*vops->cfgread)(DEV_STRUCT(base), newoff,\n\t\t\t\t\t\t size, &value);\n\t\t}\n\t\tif (!error)\n\t\t\tgoto done;\n\t}\n\nbad:\n\tcr = virtio_find_legacy_cr(offset);\n\tif (cr == NULL || cr->size != size) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, so size must be bad */\n\t\t\tpr_err(\"%s: read from %s: bad size %d\\r\\n\",\n\t\t\t    name, cr->name, size);\n\t\t} else {\n\t\t\tpr_err(\"%s: read from bad offset/size %jd/%d\\r\\n\",\n\t\t\t    name, (uintmax_t)offset, size);\n\t\t}\n\t\tgoto done;\n\t}\n\n\tswitch (offset) {\n\tcase VIRTIO_PCI_HOST_FEATURES:\n\t\tvalue = base->device_caps;\n\t\tbreak;\n\tcase VIRTIO_PCI_GUEST_FEATURES:\n\t\tvalue = base->negotiated_caps;\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_PFN:\n\t\tif (base->curq < vops->nvq)\n\t\t\tvalue = base->queues[base->curq].pfn;\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_NUM:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].qsize : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_SEL:\n\t\tvalue = base->curq;\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_NOTIFY:\n\t\tvalue = 0;\t/* XXX */\n\t\tbreak;\n\tcase VIRTIO_PCI_STATUS:\n\t\tvalue = base->status;\n\t\tbreak;\n\tcase VIRTIO_PCI_ISR:\n\t\tvalue = base->isr;\n\t\tbase->isr = 0;\t\t/* a read clears this flag */\n\t\tif (value)\n\t\t\tpci_lintr_deassert(dev);\n\t\tbreak;\n\tcase VIRTIO_MSI_CONFIG_VECTOR:\n\t\tvalue = base->msix_cfg_idx;\n\t\tbreak;\n\tcase VIRTIO_MSI_QUEUE_VECTOR:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t    base->queues[base->curq].msix_idx :\n\t\t    VIRTIO_MSI_NO_VECTOR;\n\t\tbreak;\n\t}\ndone:\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n\treturn value;\n}\n\n/*\n * Handle pci config space writes.\n * If it's to the MSI-X info, do that.\n * If it's part of the virtio standard stuff, do that.\n * Otherwise dispatch to the actual driver.\n */\nstatic void\nvirtio_pci_legacy_write(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t\tint baridx, uint64_t offset, int size, uint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_vq_info *vq;\n\tstruct virtio_ops *vops;\n\tconst struct config_reg *cr;\n\tuint64_t virtio_config_size, max;\n\tconst char *name;\n\tuint32_t newoff;\n\tint error = -1;\n\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tvops = base->vops;\n\tname = vops->name;\n\n\tif (size != 1 && size != 2 && size != 4)\n\t\tgoto bad;\n\n\tif (pci_msix_enabled(dev))\n\t\tvirtio_config_size = VIRTIO_PCI_CONFIG_OFF(1);\n\telse\n\t\tvirtio_config_size = VIRTIO_PCI_CONFIG_OFF(0);\n\n\tif (offset >= virtio_config_size) {\n\t\t/*\n\t\t * Subtract off the standard size (including MSI-X\n\t\t * registers if enabled) and dispatch to underlying driver.\n\t\t */\n\t\tnewoff = offset - virtio_config_size;\n\t\tmax = vops->cfgsize ? vops->cfgsize : 0x100000000;\n\t\tif (newoff + size > max)\n\t\t\tgoto bad;\n\t\tif (vops->cfgwrite) {\n\t\t\terror = (*vops->cfgwrite)(DEV_STRUCT(base), newoff,\n\t\t\t\t\t\t  size, value);\n\t\t}\n\t\tif (!error)\n\t\t\tgoto done;\n\t}\n\nbad:\n\tcr = virtio_find_legacy_cr(offset);\n\tif (cr == NULL || cr->size != size || cr->ro) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, wrong size and/or reg is R/O */\n\t\t\tif (cr->size != size)\n\t\t\t\tpr_err(\"%s: write to %s: bad size %d\\r\\n\",\n\t\t\t\t    name, cr->name, size);\n\t\t\tif (cr->ro)\n\t\t\t\tpr_err(\"%s: write to read-only reg %s\\r\\n\",\n\t\t\t\t    name, cr->name);\n\t\t} else {\n\t\t\tpr_err(\"%s: write to bad offset/size %jd/%d\\r\\n\",\n\t\t\t    name, (uintmax_t)offset, size);\n\t\t}\n\t\tgoto done;\n\t}\n\n\tswitch (offset) {\n\tcase VIRTIO_PCI_GUEST_FEATURES:\n\t\tbase->negotiated_caps = value & base->device_caps;\n\t\tif (vops->apply_features)\n\t\t\t(*vops->apply_features)(DEV_STRUCT(base),\n\t\t\t    base->negotiated_caps);\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_PFN:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvirtio_vq_init(base, value);\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_SEL:\n\t\t/*\n\t\t * Note that the guest is allowed to select an\n\t\t * invalid queue; we just need to return a QNUM\n\t\t * of 0 while the bad queue is selected.\n\t\t */\n\t\tbase->curq = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_NOTIFY:\n\t\tif (value >= vops->nvq) {\n\t\t\tpr_err(\"%s: queue %d notify out of range\\r\\n\",\n\t\t\t\tname, (int)value);\n\t\t\tgoto done;\n\t\t}\n\t\tvq = &base->queues[value];\n\t\tif (vq->notify)\n\t\t\t(*vq->notify)(DEV_STRUCT(base), vq);\n\t\telse if (vops->qnotify)\n\t\t\t(*vops->qnotify)(DEV_STRUCT(base), vq);\n\t\telse\n\t\t\tpr_err(\"%s: qnotify queue %d: missing vq/vops notify\\r\\n\",\n\t\t\t\tname, (int)value);\n\t\tbreak;\n\tcase VIRTIO_PCI_STATUS:\n\t\tbase->status = value;\n\t\tif (vops->set_status)\n\t\t\t(*vops->set_status)(DEV_STRUCT(base), value);\n\t\tif ((value == 0) && (vops->reset))\n\t\t\t(*vops->reset)(DEV_STRUCT(base));\n\t\tif ((value & VIRTIO_CONFIG_S_DRIVER_OK) &&\n\t\t     base->backend_type == BACKEND_VBSU &&\n\t\t     virtio_poll_enabled) {\n\t\t\tbase->polling_timer.clockid = CLOCK_MONOTONIC;\n\t\t\tacrn_timer_init(&base->polling_timer, virtio_poll_timer, base);\n\t\t\t/* wait 5s to start virtio poll mode\n\t\t\t * skip vsbl and make sure device initialization completed\n\t\t\t * FIXME: Need optimization in the future\n\t\t\t */\n\t\t\tvirtio_start_timer(&base->polling_timer, 5, 0);\n\t\t}\n\t\tbreak;\n\tcase VIRTIO_MSI_CONFIG_VECTOR:\n\t\tbase->msix_cfg_idx = value;\n\t\tbreak;\n\tcase VIRTIO_MSI_QUEUE_VECTOR:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->msix_idx = value;\n\t\tbreak;\n\t}\n\tgoto done;\n\nbad_qindex:\n\tpr_err(\"%s: write config reg %s: curq %d >= max %d\\r\\n\",\n\t    name, cr->name, base->curq, vops->nvq);\ndone:\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n}\n\nstatic int\nvirtio_find_capability(struct virtio_base *base, uint8_t cfg_type)\n{\n\tstruct pci_vdev *dev = base->dev;\n\tuint8_t type;\n\tint rc, coff = 0;\n\n\trc = pci_emul_find_capability(dev, PCIY_VENDOR, &coff);\n\twhile (!rc) {\n\t\ttype = pci_get_cfgdata8(dev,\n\t\t\tcoff + offsetof(struct virtio_pci_cap, cfg_type));\n\t\tif (type == cfg_type)\n\t\t\treturn coff;\n\t\trc = pci_emul_find_capability(dev, PCIY_VENDOR, &coff);\n\t}\n\n\treturn -1;\n}\n\n/*\n * Set virtio modern MMIO BAR (usually 4) to map the 4 capabilities.\n */\nstatic int\nvirtio_set_modern_mmio_bar(struct virtio_base *base, int barnum)\n{\n\tstruct virtio_ops *vops;\n\tint rc;\n\tstruct virtio_pci_cap cap = {\n\t\t.cap_vndr = PCIY_VENDOR,\n\t\t.cap_next = 0,\n\t\t.cap_len = sizeof(cap),\n\t\t.bar = barnum,\n\t};\n\tstruct virtio_pci_notify_cap notify = {\n\t\t.cap.cap_vndr = PCIY_VENDOR,\n\t\t.cap.cap_next = 0,\n\t\t.cap.cap_len = sizeof(notify),\n\t\t.cap.cfg_type = VIRTIO_PCI_CAP_NOTIFY_CFG,\n\t\t.cap.bar = barnum,\n\t\t.cap.offset = VIRTIO_CAP_NOTIFY_OFFSET,\n\t\t.cap.length = VIRTIO_CAP_NOTIFY_SIZE,\n\t\t.notify_off_multiplier = VIRTIO_MODERN_NOTIFY_OFF_MULT,\n\t};\n\tstruct virtio_pci_cfg_cap cfg = {\n\t\t.cap.cap_vndr = PCIY_VENDOR,\n\t\t.cap.cap_next = 0,\n\t\t.cap.cap_len = sizeof(cfg),\n\t\t.cap.cfg_type = VIRTIO_PCI_CAP_PCI_CFG,\n\t};\n\n\tvops = base->vops;\n\n\tif (vops->cfgsize > VIRTIO_CAP_DEVICE_SIZE) {\n\t\tpr_err(\"%s: cfgsize %lu > max %d\\r\\n\",\n\t\t\tvops->name, vops->cfgsize, VIRTIO_CAP_DEVICE_SIZE);\n\t\treturn -1;\n\t}\n\n\t/* common configuration capability */\n\tcap.cfg_type = VIRTIO_PCI_CAP_COMMON_CFG;\n\tcap.offset = VIRTIO_CAP_COMMON_OFFSET;\n\tcap.length = VIRTIO_CAP_COMMON_SIZE;\n\trc = pci_emul_add_capability(base->dev, (u_char *)&cap, sizeof(cap));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add common configuration capability failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* isr status capability */\n\tcap.cfg_type = VIRTIO_PCI_CAP_ISR_CFG;\n\tcap.offset = VIRTIO_CAP_ISR_OFFSET;\n\tcap.length = VIRTIO_CAP_ISR_SIZE;\n\trc = pci_emul_add_capability(base->dev, (u_char *)&cap, sizeof(cap));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add isr status capability failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* device specific configuration capability */\n\tcap.cfg_type = VIRTIO_PCI_CAP_DEVICE_CFG;\n\tcap.offset = VIRTIO_CAP_DEVICE_OFFSET;\n\tcap.length = VIRTIO_CAP_DEVICE_SIZE;\n\trc = pci_emul_add_capability(base->dev, (u_char *)&cap, sizeof(cap));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add device specific configuration capability failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* notification capability */\n\trc = pci_emul_add_capability(base->dev, (u_char *)&notify,\n\t\tsizeof(notify));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add notification capability failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* pci alternative configuration access capability */\n\trc = pci_emul_add_capability(base->dev, (u_char *)&cfg, sizeof(cfg));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add alternative configuration access capability failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* allocate and register modern memory bar */\n\trc = pci_emul_alloc_bar(base->dev, barnum, PCIBAR_MEM64,\n\t\t\t\tVIRTIO_MODERN_MEM_BAR_SIZE);\n\tif (rc != 0) {\n\t\tpr_err(\"allocate and register modern memory bar failed\\n\");\n\t\treturn -1;\n\t}\n\n\tbase->cfg_coff = virtio_find_capability(base, VIRTIO_PCI_CAP_PCI_CFG);\n\tif (base->cfg_coff < 0) {\n\t\tpr_err(\"%s: VIRTIO_PCI_CAP_PCI_CFG not found\\r\\n\",\n\t\t\tvops->name);\n\t\treturn -1;\n\t}\n\n\tbase->modern_mmio_bar_idx = barnum;\n\treturn 0;\n}\n\n/*\n * Set virtio modern PIO BAR (usually 2) to map notify capability.\n */\nstatic int\nvirtio_set_modern_pio_bar(struct virtio_base *base, int barnum)\n{\n\tint rc;\n\tstruct virtio_pci_notify_cap notify_pio = {\n\t\t.cap.cap_vndr = PCIY_VENDOR,\n\t\t.cap.cap_next = 0,\n\t\t.cap.cap_len = sizeof(notify_pio),\n\t\t.cap.cfg_type = VIRTIO_PCI_CAP_NOTIFY_CFG,\n\t\t.cap.bar = barnum,\n\t\t.cap.offset = 0,\n\t\t.cap.length = 4,\n\t\t.notify_off_multiplier = 0,\n\t};\n\n\t/* notification capability */\n\trc = pci_emul_add_capability(base->dev, (u_char *)&notify_pio,\n\t\tsizeof(notify_pio));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add notification capability for virtio modern PIO BAR failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* allocate and register modern pio bar */\n\trc = pci_emul_alloc_bar(base->dev, barnum, PCIBAR_IO, 4);\n\tif (rc != 0) {\n\t\tpr_err(\"allocate and register modern pio bar failed\\n\");\n\t\treturn -1;\n\t}\n\n\tbase->modern_pio_bar_idx = barnum;\n\treturn 0;\n}\n\n/**\n * @brief Set modern BAR (usually 4) to map PCI config registers.\n *\n * Set modern MMIO BAR (usually 4) to map virtio 1.0 capabilities and optional\n * set modern PIO BAR (usually 2) to map notify capability. This interface is\n * only valid for modern virtio.\n *\n * @param base Pointer to struct virtio_base.\n * @param use_notify_pio Whether use pio for notify capability.\n *\n * @return 0 on success and non-zero on fail.\n */\nint\nvirtio_set_modern_bar(struct virtio_base *base, bool use_notify_pio)\n{\n\tstruct virtio_ops *vops;\n\tint rc = 0;\n\n\tvops = base->vops;\n\n\tif (!vops || (base->device_caps & (1UL << VIRTIO_F_VERSION_1)) == 0)\n\t\treturn -1;\n\n\tif (use_notify_pio)\n\t\trc = virtio_set_modern_pio_bar(base,\n\t\t\tVIRTIO_MODERN_PIO_BAR_IDX);\n\tif (!rc)\n\t\trc = virtio_set_modern_mmio_bar(base,\n\t\t\tVIRTIO_MODERN_MMIO_BAR_IDX);\n\n\treturn rc;\n}\n\nstatic struct cap_region {\n\tuint64_t\tcap_offset;\t/* offset of capability region */\n\tint\t\tcap_size;\t/* size of capability region */\n\tint\t\tcap_id;\t\t/* capability id */\n} cap_regions[] = {\n\t{VIRTIO_CAP_COMMON_OFFSET, VIRTIO_CAP_COMMON_SIZE,\n\t\tVIRTIO_PCI_CAP_COMMON_CFG},\n\t{VIRTIO_CAP_ISR_OFFSET, VIRTIO_CAP_ISR_SIZE,\n\t\tVIRTIO_PCI_CAP_ISR_CFG},\n\t{VIRTIO_CAP_DEVICE_OFFSET, VIRTIO_CAP_DEVICE_SIZE,\n\t\tVIRTIO_PCI_CAP_DEVICE_CFG},\n\t{VIRTIO_CAP_NOTIFY_OFFSET, VIRTIO_CAP_NOTIFY_SIZE,\n\t\tVIRTIO_PCI_CAP_NOTIFY_CFG},\n};\n\nstatic inline int\nvirtio_get_cap_id(uint64_t offset, int size)\n{\n\tint i, rc = -1;\n\n\tfor (i = 0; i < ARRAY_SIZE(cap_regions); i++) {\n\t\tif (offset >= cap_regions[i].cap_offset &&\n\t\t\toffset + size <= cap_regions[i].cap_offset +\n\t\t\tcap_regions[i].cap_size)\n\t\t\treturn cap_regions[i].cap_id;\n\t}\n\n\treturn rc;\n}\n\nstatic uint32_t\nvirtio_common_cfg_read(struct pci_vdev *dev, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst struct config_reg *cr;\n\tconst char *name;\n\tuint32_t value;\n\n\tvops = base->vops;\n\tname = vops->name;\n\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\n\tcr = virtio_find_modern_cr(offset);\n\tif (cr == NULL || cr->size != size) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, so size must be bad */\n\t\t\tpr_err(\"%s: read from %s: bad size %d\\r\\n\",\n\t\t\t\tname, cr->name, size);\n\t\t} else {\n\t\t\tpr_err(\"%s: read from bad offset/size %jd/%d\\r\\n\",\n\t\t\t\tname, (uintmax_t)offset, size);\n\t\t}\n\n\t\treturn value;\n\t}\n\n\tswitch (offset) {\n\tcase VIRTIO_PCI_COMMON_DFSELECT:\n\t\tvalue = base->device_feature_select;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_DF:\n\t\tif (base->device_feature_select == 0)\n\t\t\tvalue = base->device_caps & 0xffffffff;\n\t\telse if (base->device_feature_select == 1)\n\t\t\tvalue = (base->device_caps >> 32) & 0xffffffff;\n\t\telse /* present 0, see 4.1.4.3.1 */\n\t\t\tvalue = 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_GFSELECT:\n\t\tvalue = base->driver_feature_select;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_GF:\n\t\t/* see 4.1.4.3.1. Present any valid feature bits the driver\n\t\t * has written in driver_feature. Valid feature bits are those\n\t\t * which are subset of the corresponding device_feature bits\n\t\t */\n\t\tif (base->driver_feature_select == 0)\n\t\t\tvalue = base->negotiated_caps & 0xffffffff;\n\t\telse if (base->driver_feature_select == 1)\n\t\t\tvalue = (base->negotiated_caps >> 32) & 0xffffffff;\n\t\telse\n\t\t\tvalue = 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_MSIX:\n\t\tvalue = base->msix_cfg_idx;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_NUMQ:\n\t\tvalue = vops->nvq;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_STATUS:\n\t\tvalue = base->status;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_CFGGENERATION:\n\t\tvalue = base->config_generation;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_SELECT:\n\t\tvalue = base->curq;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_SIZE:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].qsize : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_MSIX:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].msix_idx :\n\t\t\tVIRTIO_MSI_NO_VECTOR;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_ENABLE:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].enabled : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_NOFF:\n\t\tvalue = base->curq;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_DESCLO:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_desc[0] : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_DESCHI:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_desc[1] : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_AVAILLO:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_avail[0] : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_AVAILHI:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_avail[1] : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_USEDLO:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_used[0] : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_USEDHI:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_used[1] : 0;\n\t\tbreak;\n\t}\n\n\treturn value;\n}\n\nstatic void\nvirtio_common_cfg_write(struct pci_vdev *dev, uint64_t offset, int size,\n\t\t\tuint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_vq_info *vq;\n\tstruct virtio_ops *vops;\n\tconst struct config_reg *cr;\n\tconst char *name;\n\n\tvops = base->vops;\n\tname = vops->name;\n\n\tcr = virtio_find_modern_cr(offset);\n\tif (cr == NULL || cr->size != size || cr->ro) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, wrong size and/or reg is R/O */\n\t\t\tif (cr->size != size)\n\t\t\t\tpr_err(\"%s: write to %s: bad size %d\\r\\n\",\n\t\t\t\t\tname, cr->name, size);\n\t\t\tif (cr->ro)\n\t\t\t\tpr_err(\"%s: write to read-only reg %s\\r\\n\",\n\t\t\t\t\tname, cr->name);\n\t\t} else {\n\t\t\tpr_err(\"%s: write to bad offset/size %jd/%d\\r\\n\",\n\t\t\t\tname, (uintmax_t)offset, size);\n\t\t}\n\n\t\treturn;\n\t}\n\n\tswitch (offset) {\n\tcase VIRTIO_PCI_COMMON_DFSELECT:\n\t\tbase->device_feature_select = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_GFSELECT:\n\t\tbase->driver_feature_select = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_GF:\n\t\tif (base->status & VIRTIO_CONFIG_S_DRIVER_OK)\n\t\t\tbreak;\n\t\tif (base->driver_feature_select < 2) {\n\t\t\tvalue &= 0xffffffff;\n\t\t\tbase->negotiated_caps =\n\t\t\t\t(value << (base->driver_feature_select * 32))\n\t\t\t\t& base->device_caps;\n\t\t\tif (vops->apply_features)\n\t\t\t\t(*vops->apply_features)(DEV_STRUCT(base),\n\t\t\t\t\tbase->negotiated_caps);\n\t\t}\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_MSIX:\n\t\tbase->msix_cfg_idx = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_STATUS:\n\t\tbase->status = value & 0xff;\n\t\tif (vops->set_status)\n\t\t\t(*vops->set_status)(DEV_STRUCT(base), value);\n\t\tif ((base->status == 0) && (vops->reset))\n\t\t\t(*vops->reset)(DEV_STRUCT(base));\n\t\t/* TODO: virtio poll mode for modern devices */\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_SELECT:\n\t\t/*\n\t\t * Note that the guest is allowed to select an\n\t\t * invalid queue; we just need to return a QNUM\n\t\t * of 0 while the bad queue is selected.\n\t\t */\n\t\tbase->curq = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_SIZE:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->qsize = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_MSIX:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->msix_idx = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_ENABLE:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvirtio_vq_enable(base);\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_DESCLO:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_desc[0] = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_DESCHI:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_desc[1] = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_AVAILLO:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_avail[0] = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_AVAILHI:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_avail[1] = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_USEDLO:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_used[0] = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_USEDHI:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_used[1] = value;\n\t\tbreak;\n\t}\n\n\treturn;\n\nbad_qindex:\n\tpr_err(\"%s: write config reg %s: curq %d >= max %d\\r\\n\",\n\t\tname, cr->name, base->curq, vops->nvq);\n}\n\n/* ignore driver writes to ISR region, and only support ISR region read */\nstatic uint32_t\nvirtio_isr_cfg_read(struct pci_vdev *dev, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\tuint32_t value = 0;\n\n\tvalue = base->isr;\n\tbase->isr = 0;\t\t/* a read clears this flag */\n\tif (value)\n\t\tpci_lintr_deassert(dev);\n\n\treturn value;\n}\n\nstatic uint32_t\nvirtio_device_cfg_read(struct pci_vdev *dev, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tuint32_t value;\n\tuint64_t max;\n\tint error = -1;\n\n\tvops = base->vops;\n\tname = vops->name;\n\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\tmax = vops->cfgsize ? vops->cfgsize : 0x100000000;\n\n\tif (offset + size > max) {\n\t\tpr_err(\"%s: reading from 0x%lx size %d exceeds limit\\r\\n\",\n\t\t\tname, offset, size);\n\t\treturn value;\n\t}\n\n\tif (vops->cfgread) {\n\t\terror = (*vops->cfgread)(DEV_STRUCT(base), offset, size, &value);\n\t}\n\tif (error) {\n\t\tpr_err(\"%s: reading from 0x%lx size %d failed %d\\r\\n\",\n\t\t\tname, offset, size, error);\n\t\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\t}\n\n\treturn value;\n}\n\nstatic void\nvirtio_device_cfg_write(struct pci_vdev *dev, uint64_t offset, int size,\n\t\t\tuint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tuint64_t max;\n\tint error = -1;\n\n\tvops = base->vops;\n\tname = vops->name;\n\tmax = vops->cfgsize ? vops->cfgsize : 0x100000000;\n\n\tif (offset + size > max) {\n\t\tpr_err(\"%s: writing to 0x%lx size %d exceeds limit\\r\\n\",\n\t\t\tname, offset, size);\n\t\treturn;\n\t}\n\n\tif (vops->cfgwrite) {\n\t\terror = (*vops->cfgwrite)(DEV_STRUCT(base), offset, size, value);\n\t}\n\tif (error)\n\t\tpr_err(\"%s: writing ot 0x%lx size %d failed %d\\r\\n\",\n\t\t\tname, offset, size, error);\n}\n\n/*\n * ignore driver reads from notify region, and only support notify region\n * write\n */\nstatic void\nvirtio_notify_cfg_write(struct pci_vdev *dev, uint64_t offset, int size,\n\t\t\tuint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_vq_info *vq;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tuint64_t idx;\n\n\tidx = offset / VIRTIO_MODERN_NOTIFY_OFF_MULT;\n\tvops = base->vops;\n\tname = vops->name;\n\n\tif (idx >= vops->nvq) {\n\t\tpr_err(\"%s: queue %lu notify out of range\\r\\n\", name, idx);\n\t\treturn;\n\t}\n\n\tvq = &base->queues[idx];\n\tif (vq->notify)\n\t\t(*vq->notify)(DEV_STRUCT(base), vq);\n\telse if (vops->qnotify)\n\t\t(*vops->qnotify)(DEV_STRUCT(base), vq);\n\telse\n\t\tpr_err(\"%s: qnotify queue %lu: missing vq/vops notify\\r\\n\",\n\t\t\tname, idx);\n}\n\nstatic uint32_t\nvirtio_pci_modern_mmio_read(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t\t    int baridx, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tuint32_t value;\n\tint capid;\n\n\n\tvops = base->vops;\n\tname = vops->name;\n\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\n\tif (size != 1 && size != 2 && size != 4) {\n\t\tpr_err(\"%s: read from [%d:0x%lx] bad size %d\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t\treturn value;\n\t}\n\n\tcapid = virtio_get_cap_id(offset, size);\n\tif (capid < 0) {\n\t\tpr_err(\"%s: read from [%d:0x%lx] bad range %d\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t\treturn value;\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tswitch (capid) {\n\tcase VIRTIO_PCI_CAP_COMMON_CFG:\n\t\toffset -= VIRTIO_CAP_COMMON_OFFSET;\n\t\tvalue = virtio_common_cfg_read(dev, offset, size);\n\t\tbreak;\n\tcase VIRTIO_PCI_CAP_ISR_CFG:\n\t\toffset -= VIRTIO_CAP_ISR_OFFSET;\n\t\tvalue = virtio_isr_cfg_read(dev, offset, size);\n\t\tbreak;\n\tcase VIRTIO_PCI_CAP_DEVICE_CFG:\n\t\toffset -= VIRTIO_CAP_DEVICE_OFFSET;\n\t\tvalue = virtio_device_cfg_read(dev, offset, size);\n\t\tbreak;\n\tdefault: /* guest driver should not read from notify region */\n\t\tpr_err(\"%s: read from [%d:0x%lx] size %d not supported\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n\treturn value;\n}\n\nstatic void\nvirtio_pci_modern_mmio_write(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t\t     int baridx, uint64_t offset, int size,\n\t\t\t     uint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tint capid;\n\n\n\tvops = base->vops;\n\tname = vops->name;\n\n\tif (size != 1 && size != 2 && size != 4) {\n\t\tpr_err(\"%s: write to [%d:0x%lx] bad size %d\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t\treturn;\n\t}\n\n\tcapid = virtio_get_cap_id(offset, size);\n\tif (capid < 0) {\n\t\tpr_err(\"%s: write to [%d:0x%lx] bad range %d\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t\treturn;\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tswitch (capid) {\n\tcase VIRTIO_PCI_CAP_COMMON_CFG:\n\t\toffset -= VIRTIO_CAP_COMMON_OFFSET;\n\t\tvirtio_common_cfg_write(dev, offset, size, value);\n\t\tbreak;\n\tcase VIRTIO_PCI_CAP_DEVICE_CFG:\n\t\toffset -= VIRTIO_CAP_DEVICE_OFFSET;\n\t\tvirtio_device_cfg_write(dev, offset, size, value);\n\t\tbreak;\n\tcase VIRTIO_PCI_CAP_NOTIFY_CFG:\n\t\toffset -= VIRTIO_CAP_NOTIFY_OFFSET;\n\t\tvirtio_notify_cfg_write(dev, offset, size, value);\n\t\tbreak;\n\tdefault: /* guest driver should not write to ISR region */\n\t\tpr_err(\"%s: write to [%d:0x%lx] size %d not supported\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n}\n\nstatic uint32_t\nvirtio_pci_modern_pio_read(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t\t   int baridx, uint64_t offset, int size)\n{\n\t/* guest driver should not read notify pio */\n\treturn 0;\n}\n\nstatic void\nvirtio_pci_modern_pio_write(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t\t    int baridx, uint64_t offset, int size,\n\t\t\t    uint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_vq_info *vq;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tuint64_t idx;\n\n\n\tvops = base->vops;\n\tname = vops->name;\n\tidx = value;\n\n\tif (size != 1 && size != 2 && size != 4) {\n\t\tpr_err(\"%s: write to [%d:0x%lx] bad size %d\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t\treturn;\n\t}\n\n\tif (idx >= vops->nvq) {\n\t\tpr_err(\"%s: queue %lu notify out of range\\r\\n\", name, idx);\n\t\treturn;\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tvq = &base->queues[idx];\n\tif (vq->notify)\n\t\t(*vq->notify)(DEV_STRUCT(base), vq);\n\telse if (vops->qnotify)\n\t\t(*vops->qnotify)(DEV_STRUCT(base), vq);\n\telse\n\t\tpr_err(\"%s: qnotify queue %lu: missing vq/vops notify\\r\\n\",\n\t\t\tname, idx);\n\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n}\n\n/**\n * @brief Handle PCI configuration space reads.\n *\n * Handle virtio standard register reads, and dispatch other reads to\n * actual virtio device driver.\n *\n * @param ctx Pointer to struct vmctx representing VM context.\n * @param vcpu VCPU ID.\n * @param dev Pointer to struct pci_vdev which emulates a PCI device.\n * @param baridx Which BAR[0..5] to use.\n * @param offset Register offset in bytes within a BAR region.\n * @param size Access range in bytes.\n *\n * @return register value.\n */\nuint64_t\nvirtio_pci_read(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\tint baridx, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\n\tif (base->flags & VIRTIO_USE_MSIX) {\n\t\tif (baridx == pci_msix_table_bar(dev) ||\n\t\t    baridx == pci_msix_pba_bar(dev)) {\n\t\t\treturn pci_emul_msix_tread(dev, offset, size);\n\t\t}\n\t}\n\n\tif (baridx == base->legacy_pio_bar_idx)\n\t\treturn virtio_pci_legacy_read(ctx, vcpu, dev, baridx,\n\t\t\toffset, size);\n\n\tif (baridx == base->modern_mmio_bar_idx)\n\t\treturn virtio_pci_modern_mmio_read(ctx, vcpu, dev, baridx,\n\t\t\toffset, size);\n\n\tif (baridx == base->modern_pio_bar_idx)\n\t\treturn virtio_pci_modern_pio_read(ctx, vcpu, dev, baridx,\n\t\t\toffset, size);\n\n\tpr_err(\"%s: read unexpected baridx %d\\r\\n\",\n\t\tbase->vops->name, baridx);\n\treturn size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n}\n\n/**\n * @brief Handle PCI configuration space writes.\n *\n * Handle virtio standard register writes, and dispatch other writes to\n * actual virtio device driver.\n *\n * @param ctx Pointer to struct vmctx representing VM context.\n * @param vcpu VCPU ID.\n * @param dev Pointer to struct pci_vdev which emulates a PCI device.\n * @param baridx Which BAR[0..5] to use.\n * @param offset Register offset in bytes within a BAR region.\n * @param size Access range in bytes.\n * @param value Data value to be written into register.\n *\n * @return None\n */\nvoid\nvirtio_pci_write(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t int baridx, uint64_t offset, int size, uint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\n\tif (base->flags & VIRTIO_USE_MSIX) {\n\t\tif (baridx == pci_msix_table_bar(dev) ||\n\t\t    baridx == pci_msix_pba_bar(dev)) {\n\t\t\tpci_emul_msix_twrite(dev, offset, size, value);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (baridx == base->legacy_pio_bar_idx) {\n\t\tvirtio_pci_legacy_write(ctx, vcpu, dev, baridx,\n\t\t\toffset, size, value);\n\t\treturn;\n\t}\n\n\tif (baridx == base->modern_mmio_bar_idx) {\n\t\tvirtio_pci_modern_mmio_write(ctx, vcpu, dev, baridx,\n\t\t\toffset, size, value);\n\t\treturn;\n\t}\n\n\tif (baridx == base->modern_pio_bar_idx) {\n\t\tvirtio_pci_modern_pio_write(ctx, vcpu, dev, baridx,\n\t\t\toffset, size, value);\n\t\treturn;\n\t}\n\n\tpr_err(\"%s: write unexpected baridx %d\\r\\n\",\n\t\tbase->vops->name, baridx);\n}\n\n/**\n * @brief Get the virtio poll parameters\n *\n * @param optarg Pointer to parameters string.\n *\n * @return fail -1 success 0\n */\nint\nacrn_parse_virtio_poll_interval(const char *optarg)\n{\n\tchar *ptr;\n\n\tvirtio_poll_interval = strtoul(optarg, &ptr, 0);\n\n\t/* poll interval is limited from 1us to 10ms */\n\tif (virtio_poll_interval < 1 || virtio_poll_interval > 10000000)\n\t\treturn -1;\n\n\tvirtio_poll_enabled = 1;\n\n\treturn 0;\n}\n"], "fixing_code": ["/*-\n * Copyright (c) 2013  Chris Torek <torek @ torek net>\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n *\n * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND\n * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n * SUCH DAMAGE.\n */\n\n#include <sys/uio.h>\n#include <stdio.h>\n#include <stddef.h>\n#include <pthread.h>\n#include <string.h>\n#include <stdlib.h>\n\n#include \"dm.h\"\n#include \"pci_core.h\"\n#include \"virtio.h\"\n#include \"timer.h\"\n#include <atomic.h>\n\n/*\n * Functions for dealing with generalized \"virtual devices\" as\n * defined by <https://www.google.com/#output=search&q=virtio+spec>\n */\n\n/*\n * In case we decide to relax the \"virtio struct comes at the\n * front of virtio-based device struct\" constraint, let's use\n * this to convert.\n */\n#define DEV_STRUCT(vs) ((void *)(vs))\n\nstatic uint8_t virtio_poll_enabled;\nstatic size_t virtio_poll_interval;\n\nstatic void\nvirtio_start_timer(struct acrn_timer *timer, time_t sec, time_t nsec)\n{\n\tstruct itimerspec ts;\n\n\t/* setting the interval time */\n\tts.it_interval.tv_sec = 0;\n\tts.it_interval.tv_nsec = 0;\n\t/* set the delay time it will be started when timer_setting */\n\tts.it_value.tv_sec = sec;\n\tts.it_value.tv_nsec = nsec;\n\tif (acrn_timer_settime(timer, &ts) != 0) {\n\t\tpr_err(\"acrn timer set time failed\\n\");\n\t\treturn;\n\t}\n}\n\nstatic void\nvirtio_poll_timer(void *arg, uint64_t nexp)\n{\n\tstruct virtio_base *base;\n\tstruct virtio_ops *vops;\n\tstruct virtio_vq_info *vq;\n\tconst char *name;\n\tint i;\n\n\tbase = arg;\n\tvops = base->vops;\n\tname = vops->name;\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tbase->polling_in_progress = 1;\n\n\tfor (i = 0; i < base->vops->nvq; i++) {\n\t\tvq = &base->queues[i];\n\t\tif(!vq_ring_ready(vq))\n\t\t\tcontinue;\n\t\tvq->used->flags |= VRING_USED_F_NO_NOTIFY;\n\t\t/* TODO: call notify when necessary */\n\t\tif (vq->notify)\n\t\t\t(*vq->notify)(DEV_STRUCT(base), vq);\n\t\telse if (vops->qnotify)\n\t\t\t(*vops->qnotify)(DEV_STRUCT(base), vq);\n\t\telse\n\t\t\tpr_err(\"%s: qnotify queue %d: missing vq/vops notify\\r\\n\",\n\t\t\t\tname, i);\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n\n\tvirtio_start_timer(&base->polling_timer, 0, virtio_poll_interval);\n}\n\n/**\n * @brief Link a virtio_base to its constants, the virtio device,\n * and the PCI emulation.\n *\n * @param base Pointer to struct virtio_base.\n * @param vops Pointer to struct virtio_ops.\n * @param pci_virtio_dev Pointer to instance of certain virtio device.\n * @param dev Pointer to struct pci_vdev which emulates a PCI device.\n * @param queues Pointer to struct virtio_vq_info, normally an array.\n *\n * @return None\n */\nvoid\nvirtio_linkup(struct virtio_base *base, struct virtio_ops *vops,\n\t      void *pci_virtio_dev, struct pci_vdev *dev,\n\t      struct virtio_vq_info *queues,\n\t      int backend_type)\n{\n\tint i;\n\n\t/* base and pci_virtio_dev addresses must match */\n\tif ((void *)base != pci_virtio_dev) {\n\t\tpr_err(\"virtio_base and pci_virtio_dev addresses don't match!\\n\");\n\t\treturn;\n\t}\n\tbase->vops = vops;\n\tbase->dev = dev;\n\tdev->arg = base;\n\tbase->backend_type = backend_type;\n\n\tbase->queues = queues;\n\tfor (i = 0; i < vops->nvq; i++) {\n\t\tqueues[i].base = base;\n\t\tqueues[i].num = i;\n\t}\n}\n\n/**\n * @brief Reset device (device-wide).\n *\n * This erases all queues, i.e., all the queues become invalid.\n * But we don't wipe out the internal pointers, by just clearing\n * the VQ_ALLOC flag.\n *\n * It resets negotiated features to \"none\".\n * If MSI-X is enabled, this also resets all the vectors to NO_VECTOR.\n *\n * @param base Pointer to struct virtio_base.\n *\n * @return None\n */\nvoid\nvirtio_reset_dev(struct virtio_base *base)\n{\n\tstruct virtio_vq_info *vq;\n\tint i, nvq;\n\n/* if (base->mtx) */\n/* assert(pthread_mutex_isowned_np(base->mtx)); */\n\n\tacrn_timer_deinit(&base->polling_timer);\n\tbase->polling_in_progress = 0;\n\n\tnvq = base->vops->nvq;\n\tfor (vq = base->queues, i = 0; i < nvq; vq++, i++) {\n\t\tvq->flags = 0;\n\t\tvq->last_avail = 0;\n\t\tvq->save_used = 0;\n\t\tvq->pfn = 0;\n\t\tvq->msix_idx = VIRTIO_MSI_NO_VECTOR;\n\t\tvq->gpa_desc[0] = 0;\n\t\tvq->gpa_desc[1] = 0;\n\t\tvq->gpa_avail[0] = 0;\n\t\tvq->gpa_avail[1] = 0;\n\t\tvq->gpa_used[0] = 0;\n\t\tvq->gpa_used[1] = 0;\n\t\tvq->enabled = 0;\n\t}\n\tbase->negotiated_caps = 0;\n\tbase->curq = 0;\n\t/* base->status = 0; -- redundant */\n\tif (base->isr)\n\t\tpci_lintr_deassert(base->dev);\n\tbase->isr = 0;\n\tbase->msix_cfg_idx = VIRTIO_MSI_NO_VECTOR;\n\tbase->device_feature_select = 0;\n\tbase->driver_feature_select = 0;\n\tbase->config_generation = 0;\n}\n\n/**\n * @brief Set I/O BAR (usually 0) to map PCI config registers.\n *\n * @param base Pointer to struct virtio_base.\n * @param barnum Which BAR[0..5] to use.\n *\n * @return None\n */\nvoid\nvirtio_set_io_bar(struct virtio_base *base, int barnum)\n{\n\tsize_t size;\n\n\t/*\n\t * ??? should we use VIRTIO_PCI_CONFIG_OFF(0) if MSI-X\n\t * is disabled? Existing code did not...\n\t */\n\tsize = VIRTIO_PCI_CONFIG_OFF(1) + base->vops->cfgsize;\n\tpci_emul_alloc_bar(base->dev, barnum, PCIBAR_IO, size);\n\tbase->legacy_pio_bar_idx = barnum;\n}\n\n/**\n * @brief Initialize MSI-X vector capabilities if we're to use MSI-X,\n * or MSI capabilities if not.\n *\n * We assume we want one MSI-X vector per queue, here, plus one\n * for the config vec.\n *\n *\n * @param base Pointer to struct virtio_base.\n * @param barnum Which BAR[0..5] to use.\n * @param use_msix If using MSI-X.\n *\n * @return 0 on success and non-zero on fail.\n */\nint\nvirtio_intr_init(struct virtio_base *base, int barnum, int use_msix)\n{\n\tint nvec;\n\n\tif (use_msix) {\n\t\tbase->flags |= VIRTIO_USE_MSIX;\n\t\tVIRTIO_BASE_LOCK(base);\n\t\tvirtio_reset_dev(base); /* set all vectors to NO_VECTOR */\n\t\tVIRTIO_BASE_UNLOCK(base);\n\t\tnvec = base->vops->nvq + 1;\n\t\tif (pci_emul_add_msixcap(base->dev, nvec, barnum))\n\t\t\treturn -1;\n\t} else\n\t\tbase->flags &= ~VIRTIO_USE_MSIX;\n\n\t/* Only 1 MSI vector for acrn-dm */\n\tpci_emul_add_msicap(base->dev, 1);\n\n\t/* Legacy interrupts are mandatory for virtio devices */\n\tpci_lintr_request(base->dev);\n\n\treturn 0;\n}\n\n/**\n * @brief Initialize MSI-X vector capabilities if we're to use MSI-X,\n * or MSI capabilities if not.\n *\n * Wrapper function for virtio_intr_init() for cases we directly use\n * BAR 1 for MSI-X capabilities.\n *\n * @param base Pointer to struct virtio_base.\n * @param use_msix If using MSI-X.\n *\n * @return 0 on success and non-zero on fail.\n */\nint\nvirtio_interrupt_init(struct virtio_base *base, int use_msix)\n{\n\treturn virtio_intr_init(base, 1, use_msix);\n}\n\n/*\n * Initialize the currently-selected virtio queue (base->curq).\n * The guest just gave us a page frame number, from which we can\n * calculate the addresses of the queue.\n * This interface is only valid for virtio legacy.\n */\nstatic void\nvirtio_vq_init(struct virtio_base *base, uint32_t pfn)\n{\n\tstruct virtio_vq_info *vq;\n\tuint64_t phys;\n\tsize_t size;\n\tchar *vb;\n\n\tvq = &base->queues[base->curq];\n\tvq->pfn = pfn;\n\tphys = (uint64_t)pfn << VRING_PAGE_BITS;\n\tsize = vring_size(vq->qsize, VIRTIO_PCI_VRING_ALIGN);\n\tvb = paddr_guest2host(base->dev->vmctx, phys, size);\n\tif (!vb)\n\t\tgoto error;\n\n\t/* First page(s) are descriptors... */\n\tvq->desc = (struct vring_desc *)vb;\n\tvb += vq->qsize * sizeof(struct vring_desc);\n\n\t/* ... immediately followed by \"avail\" ring (entirely uint16_t's) */\n\tvq->avail = (struct vring_avail *)vb;\n\tvb += (2 + vq->qsize + 1) * sizeof(uint16_t);\n\n\t/* Then it's rounded up to the next page... */\n\tvb = (char *)roundup2((uintptr_t)vb, VIRTIO_PCI_VRING_ALIGN);\n\n\t/* ... and the last page(s) are the used ring. */\n\tvq->used = (struct vring_used *)vb;\n\n\t/* Start at 0 when we use it. */\n\tvq->last_avail = 0;\n\tvq->save_used = 0;\n\n\t/* Mark queue as allocated after initialization is complete. */\n\tmb();\n\tvq->flags = VQ_ALLOC;\n\n\treturn;\n\nerror:\n\tvq->flags = 0;\n\tpr_err(\"%s: vq enable failed\\n\", __func__);\n}\n\n/*\n * Initialize the currently-selected virtio queue (base->curq).\n * The guest just gave us the gpa of desc array, avail ring and\n * used ring, from which we can initialize the virtqueue.\n * This interface is only valid for virtio modern.\n */\nstatic void\nvirtio_vq_enable(struct virtio_base *base)\n{\n\tstruct virtio_vq_info *vq;\n\tuint16_t qsz;\n\tuint64_t phys;\n\tsize_t size;\n\tchar *vb;\n\n\tvq = &base->queues[base->curq];\n\tqsz = vq->qsize;\n\n\t/* descriptors */\n\tphys = (((uint64_t)vq->gpa_desc[1]) << 32) | vq->gpa_desc[0];\n\tsize = qsz * sizeof(struct vring_desc);\n\tvb = paddr_guest2host(base->dev->vmctx, phys, size);\n\tif (!vb)\n\t\tgoto error;\n\tvq->desc = (struct vring_desc *)vb;\n\n\t/* available ring */\n\tphys = (((uint64_t)vq->gpa_avail[1]) << 32) | vq->gpa_avail[0];\n\tsize = (2 + qsz + 1) * sizeof(uint16_t);\n\tvb = paddr_guest2host(base->dev->vmctx, phys, size);\n\tif (!vb)\n\t\tgoto error;\n\n\tvq->avail = (struct vring_avail *)vb;\n\n\t/* used ring */\n\tphys = (((uint64_t)vq->gpa_used[1]) << 32) | vq->gpa_used[0];\n\tsize = sizeof(uint16_t) * 3 + sizeof(struct vring_used_elem) * qsz;\n\tvb = paddr_guest2host(base->dev->vmctx, phys, size);\n\tif (!vb)\n\t\tgoto error;\n\tvq->used = (struct vring_used *)vb;\n\n\t/* Start at 0 when we use it. */\n\tvq->last_avail = 0;\n\tvq->save_used = 0;\n\n\t/* Mark queue as enabled. */\n\tvq->enabled = true;\n\n\t/* Mark queue as allocated after initialization is complete. */\n\tmb();\n\tvq->flags = VQ_ALLOC;\n\treturn;\n error:\n\tvq->flags = 0;\n\tpr_err(\"%s: vq enable failed\\n\", __func__);\n}\n\n/*\n * Helper inline for vq_getchain(): record the i'th \"real\"\n * descriptor.\n * Return 0 on success and -1 when i is out of range  or mapping\n *        fails.\n */\nstatic inline int\n_vq_record(int i, volatile struct vring_desc *vd, struct vmctx *ctx,\n\t   struct iovec *iov, int n_iov, uint16_t *flags) {\n\n\tvoid *host_addr;\n\n\tif (i >= n_iov)\n\t\treturn -1;\n\thost_addr = paddr_guest2host(ctx, vd->addr, vd->len);\n\tif (!host_addr)\n\t\treturn -1;\n\tiov[i].iov_base = host_addr;\n\tiov[i].iov_len = vd->len;\n\tif (flags != NULL)\n\t\tflags[i] = vd->flags;\n\treturn 0;\n}\n#define\tVQ_MAX_DESCRIPTORS\t512\t/* see below */\n\n/*\n * Examine the chain of descriptors starting at the \"next one\" to\n * make sure that they describe a sensible request.  If so, return\n * the number of \"real\" descriptors that would be needed/used in\n * acting on this request.  This may be smaller than the number of\n * available descriptors, e.g., if there are two available but\n * they are two separate requests, this just returns 1.  Or, it\n * may be larger: if there are indirect descriptors involved,\n * there may only be one descriptor available but it may be an\n * indirect pointing to eight more.  We return 8 in this case,\n * i.e., we do not count the indirect descriptors, only the \"real\"\n * ones.\n *\n * Basically, this vets the flags and vd_next field of each\n * descriptor and tells you how many are involved.  Since some may\n * be indirect, this also needs the vmctx (in the pci_vdev\n * at base->dev) so that it can find indirect descriptors.\n *\n * As we process each descriptor, we copy and adjust it (guest to\n * host address wise, also using the vmtctx) into the given iov[]\n * array (of the given size).  If the array overflows, we stop\n * placing values into the array but keep processing descriptors,\n * up to VQ_MAX_DESCRIPTORS, before giving up and returning -1.\n * So you, the caller, must not assume that iov[] is as big as the\n * return value (you can process the same thing twice to allocate\n * a larger iov array if needed, or supply a zero length to find\n * out how much space is needed).\n *\n * If you want to verify the WRITE flag on each descriptor, pass a\n * non-NULL \"flags\" pointer to an array of \"uint16_t\" of the same size\n * as n_iov and we'll copy each flags field after unwinding any\n * indirects.\n *\n * If some descriptor(s) are invalid, this prints a diagnostic message\n * and returns -1.  If no descriptors are ready now it simply returns 0.\n *\n * You are assumed to have done a vq_ring_ready() if needed (note\n * that vq_has_descs() does one).\n */\nint\nvq_getchain(struct virtio_vq_info *vq, uint16_t *pidx,\n\t    struct iovec *iov, int n_iov, uint16_t *flags)\n{\n\tint i;\n\tu_int ndesc, n_indir;\n\tu_int idx, next;\n\n\tvolatile struct vring_desc *vdir, *vindir, *vp;\n\tstruct vmctx *ctx;\n\tstruct virtio_base *base;\n\tconst char *name;\n\n\tbase = vq->base;\n\tname = base->vops->name;\n\n\t/*\n\t * Note: it's the responsibility of the guest not to\n\t * update vq->avail->idx until all of the descriptors\n\t * the guest has written are valid (including all their\n\t * next fields and vd_flags).\n\t *\n\t * Compute (last_avail - idx) in integers mod 2**16.  This is\n\t * the number of descriptors the device has made available\n\t * since the last time we updated vq->last_avail.\n\t *\n\t * We just need to do the subtraction as an unsigned int,\n\t * then trim off excess bits.\n\t */\n\tidx = vq->last_avail;\n\tndesc = (uint16_t)((u_int)vq->avail->idx - idx);\n\tif (ndesc == 0)\n\t\treturn 0;\n\tif (ndesc > vq->qsize) {\n\t\t/* XXX need better way to diagnose issues */\n\t\tpr_err(\"%s: ndesc (%u) out of range, driver confused?\\r\\n\",\n\t\t    name, (u_int)ndesc);\n\t\treturn -1;\n\t}\n\n\t/*\n\t * Now count/parse \"involved\" descriptors starting from\n\t * the head of the chain.\n\t *\n\t * To prevent loops, we could be more complicated and\n\t * check whether we're re-visiting a previously visited\n\t * index, but we just abort if the count gets excessive.\n\t */\n\tctx = base->dev->vmctx;\n\t*pidx = next = vq->avail->ring[idx & (vq->qsize - 1)];\n\tvq->last_avail++;\n\tfor (i = 0; i < VQ_MAX_DESCRIPTORS; next = vdir->next) {\n\t\tif (next >= vq->qsize) {\n\t\t\tpr_err(\"%s: descriptor index %u out of range, \"\n\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t    name, next);\n\t\t\treturn -1;\n\t\t}\n\t\tvdir = &vq->desc[next];\n\t\tif ((vdir->flags & VRING_DESC_F_INDIRECT) == 0) {\n\t\t\tif (_vq_record(i, vdir, ctx, iov, n_iov, flags)) {\n\t\t\t\tpr_err(\"%s: mapping to host failed\\r\\n\", name);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\ti++;\n\t\t} else if ((base->device_caps &\n\t\t    (1 << VIRTIO_RING_F_INDIRECT_DESC)) == 0) {\n\t\t\tpr_err(\"%s: descriptor has forbidden INDIRECT flag, \"\n\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t    name);\n\t\t\treturn -1;\n\t\t} else {\n\t\t\tn_indir = vdir->len / 16;\n\t\t\tif ((vdir->len & 0xf) || n_indir == 0) {\n\t\t\t\tpr_err(\"%s: invalid indir len 0x%x, \"\n\t\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t\t    name, (u_int)vdir->len);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tvindir = paddr_guest2host(ctx,\n\t\t\t    vdir->addr, vdir->len);\n\n\t\t\tif (!vindir) {\n\t\t\t\tpr_err(\"%s cannot get host memory\\r\\n\", name);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\t/*\n\t\t\t * Indirects start at the 0th, then follow\n\t\t\t * their own embedded \"next\"s until those run\n\t\t\t * out.  Each one's indirect flag must be off\n\t\t\t * (we don't really have to check, could just\n\t\t\t * ignore errors...).\n\t\t\t */\n\t\t\tnext = 0;\n\t\t\tfor (;;) {\n\t\t\t\tvp = &vindir[next];\n\t\t\t\tif (vp->flags & VRING_DESC_F_INDIRECT) {\n\t\t\t\t\tpr_err(\"%s: indirect desc has INDIR flag,\"\n\t\t\t\t\t    \" driver confused?\\r\\n\",\n\t\t\t\t\t    name);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tif (_vq_record(i, vp, ctx, iov, n_iov, flags)) {\n\t\t\t\t\tpr_err(\"%s: mapping to host failed\\r\\n\", name);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t\tif (++i > VQ_MAX_DESCRIPTORS)\n\t\t\t\t\tgoto loopy;\n\t\t\t\tif ((vp->flags & VRING_DESC_F_NEXT) == 0)\n\t\t\t\t\tbreak;\n\t\t\t\tnext = vp->next;\n\t\t\t\tif (next >= n_indir) {\n\t\t\t\t\tpr_err(\"%s: invalid next %u > %u, \"\n\t\t\t\t\t    \"driver confused?\\r\\n\",\n\t\t\t\t\t    name, (u_int)next, n_indir);\n\t\t\t\t\treturn -1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ((vdir->flags & VRING_DESC_F_NEXT) == 0)\n\t\t\treturn i;\n\t}\nloopy:\n\tpr_err(\"%s: descriptor loop? count > %d - driver confused?\\r\\n\",\n\t    name, i);\n\treturn -1;\n}\n\n/*\n * Return the currently-first request chain back to the available queue.\n *\n * (This chain is the one you handled when you called vq_getchain()\n * and used its positive return value.)\n */\nvoid\nvq_retchain(struct virtio_vq_info *vq)\n{\n\tvq->last_avail--;\n}\n\n/*\n * Return specified request chain to the guest, setting its I/O length\n * to the provided value.\n *\n * (This chain is the one you handled when you called vq_getchain()\n * and used its positive return value.)\n */\nvoid\nvq_relchain(struct virtio_vq_info *vq, uint16_t idx, uint32_t iolen)\n{\n\tuint16_t uidx, mask;\n\tvolatile struct vring_used *vuh;\n\tvolatile struct vring_used_elem *vue;\n\n\t/*\n\t * Notes:\n\t *  - mask is N-1 where N is a power of 2 so computes x % N\n\t *  - vuh points to the \"used\" data shared with guest\n\t *  - vue points to the \"used\" ring entry we want to update\n\t *  - head is the same value we compute in vq_iovecs().\n\t *\n\t * (I apologize for the two fields named idx; the\n\t * virtio spec calls the one that vue points to, \"id\"...)\n\t */\n\tmask = vq->qsize - 1;\n\tvuh = vq->used;\n\n\tuidx = vuh->idx;\n\tvue = &vuh->ring[uidx++ & mask];\n\tvue->id = idx;\n\tvue->len = iolen;\n\tvuh->idx = uidx;\n}\n\n/*\n * Driver has finished processing \"available\" chains and calling\n * vq_relchain on each one.  If driver used all the available\n * chains, used_all should be set.\n *\n * If the \"used\" index moved we may need to inform the guest, i.e.,\n * deliver an interrupt.  Even if the used index did NOT move we\n * may need to deliver an interrupt, if the avail ring is empty and\n * we are supposed to interrupt on empty.\n *\n * Note that used_all_avail is provided by the caller because it's\n * a snapshot of the ring state when he decided to finish interrupt\n * processing -- it's possible that descriptors became available after\n * that point.  (It's also typically a constant 1/True as well.)\n */\nvoid\nvq_endchains(struct virtio_vq_info *vq, int used_all_avail)\n{\n\tstruct virtio_base *base;\n\tuint16_t event_idx, new_idx, old_idx;\n\tint intr;\n\n\tif (!vq || !vq->used)\n\t\treturn;\n\n\t/*\n\t * Interrupt generation: if we're using EVENT_IDX,\n\t * interrupt if we've crossed the event threshold.\n\t * Otherwise interrupt is generated if we added \"used\" entries,\n\t * but suppressed by VRING_AVAIL_F_NO_INTERRUPT.\n\t *\n\t * In any case, though, if NOTIFY_ON_EMPTY is set and the\n\t * entire avail was processed, we need to interrupt always.\n\t */\n\n\tatomic_thread_fence();\n\n\tbase = vq->base;\n\told_idx = vq->save_used;\n\tvq->save_used = new_idx = vq->used->idx;\n\tif (used_all_avail &&\n\t    (base->negotiated_caps & (1 << VIRTIO_F_NOTIFY_ON_EMPTY)))\n\t\tintr = 1;\n\telse if (base->negotiated_caps & (1 << VIRTIO_RING_F_EVENT_IDX)) {\n\t\tevent_idx = VQ_USED_EVENT_IDX(vq);\n\t\t/*\n\t\t * This calculation is per docs and the kernel\n\t\t * (see src/sys/dev/virtio/virtio_ring.h).\n\t\t */\n\t\tintr = (uint16_t)(new_idx - event_idx - 1) <\n\t\t\t(uint16_t)(new_idx - old_idx);\n\t} else {\n\t\tintr = new_idx != old_idx &&\n\t\t    !(vq->avail->flags & VRING_AVAIL_F_NO_INTERRUPT);\n\t}\n\tif (intr)\n\t\tvq_interrupt(base, vq);\n}\n\n/**\n * @brief Helper function for clearing used ring flags.\n *\n * Driver should always use this helper function to clear used ring flags.\n * For virtio poll mode, in order to avoid trap, we should never really\n * clear used ring flags.\n *\n * @param base Pointer to struct virtio_base.\n * @param vq Pointer to struct virtio_vq_info.\n *\n * @return None\n */\nvoid vq_clear_used_ring_flags(struct virtio_base *base, struct virtio_vq_info *vq)\n{\n\tint backend_type = base->backend_type;\n\tint polling_in_progress = base->polling_in_progress;\n\n\t/* we should never unmask notification in polling mode */\n\tif (virtio_poll_enabled && backend_type == BACKEND_VBSU && polling_in_progress == 1)\n\t\treturn;\n\n\tvq->used->flags &= ~VRING_USED_F_NO_NOTIFY;\n}\n\nstruct config_reg {\n\tuint16_t\toffset;\t/* register offset */\n\tuint8_t\t\tsize;\t/* size (bytes) */\n\tuint8_t\t\tro;\t/* true => reg is read only */\n\tconst char\t*name;\t/* name of reg */\n};\n\n/* Note: these are in sorted order to make for a fast search */\nstatic struct config_reg legacy_config_regs[] = {\n\t{ VIRTIO_PCI_HOST_FEATURES,\t4, 1, \"HOSTCAP\" },\n\t{ VIRTIO_PCI_GUEST_FEATURES,\t4, 0, \"GUESTCAP\" },\n\t{ VIRTIO_PCI_QUEUE_PFN,\t\t4, 0, \"PFN\" },\n\t{ VIRTIO_PCI_QUEUE_NUM,\t\t2, 1, \"QNUM\" },\n\t{ VIRTIO_PCI_QUEUE_SEL,\t\t2, 0, \"QSEL\" },\n\t{ VIRTIO_PCI_QUEUE_NOTIFY,\t2, 0, \"QNOTIFY\" },\n\t{ VIRTIO_PCI_STATUS,\t\t1, 0, \"STATUS\" },\n\t{ VIRTIO_PCI_ISR,\t\t1, 0, \"ISR\" },\n\t{ VIRTIO_MSI_CONFIG_VECTOR,\t2, 0, \"CFGVEC\" },\n\t{ VIRTIO_MSI_QUEUE_VECTOR,\t2, 0, \"QVEC\" },\n};\n\n/* Note: these are in sorted order to make for a fast search */\nstatic struct config_reg modern_config_regs[] = {\n\t{ VIRTIO_PCI_COMMON_DFSELECT,\t\t4, 0, \"DFSELECT\" },\n\t{ VIRTIO_PCI_COMMON_DF,\t\t\t4, 1, \"DF\" },\n\t{ VIRTIO_PCI_COMMON_GFSELECT,\t\t4, 0, \"GFSELECT\" },\n\t{ VIRTIO_PCI_COMMON_GF,\t\t\t4, 0, \"GF\" },\n\t{ VIRTIO_PCI_COMMON_MSIX,\t\t2, 0, \"MSIX\" },\n\t{ VIRTIO_PCI_COMMON_NUMQ,\t\t2, 1, \"NUMQ\" },\n\t{ VIRTIO_PCI_COMMON_STATUS,\t\t1, 0, \"STATUS\" },\n\t{ VIRTIO_PCI_COMMON_CFGGENERATION,\t1, 1, \"CFGGENERATION\" },\n\t{ VIRTIO_PCI_COMMON_Q_SELECT,\t\t2, 0, \"Q_SELECT\" },\n\t{ VIRTIO_PCI_COMMON_Q_SIZE,\t\t2, 0, \"Q_SIZE\" },\n\t{ VIRTIO_PCI_COMMON_Q_MSIX,\t\t2, 0, \"Q_MSIX\" },\n\t{ VIRTIO_PCI_COMMON_Q_ENABLE,\t\t2, 0, \"Q_ENABLE\" },\n\t{ VIRTIO_PCI_COMMON_Q_NOFF,\t\t2, 1, \"Q_NOFF\" },\n\t{ VIRTIO_PCI_COMMON_Q_DESCLO,\t\t4, 0, \"Q_DESCLO\" },\n\t{ VIRTIO_PCI_COMMON_Q_DESCHI,\t\t4, 0, \"Q_DESCHI\" },\n\t{ VIRTIO_PCI_COMMON_Q_AVAILLO,\t\t4, 0, \"Q_AVAILLO\" },\n\t{ VIRTIO_PCI_COMMON_Q_AVAILHI,\t\t4, 0, \"Q_AVAILHI\" },\n\t{ VIRTIO_PCI_COMMON_Q_USEDLO,\t\t4, 0, \"Q_USEDLO\" },\n\t{ VIRTIO_PCI_COMMON_Q_USEDHI,\t\t4, 0, \"Q_USEDHI\" },\n};\n\nstatic inline const struct config_reg *\nvirtio_find_cr(const struct config_reg *p_cr_array, u_int array_size,\n\t       int offset) {\n\tu_int hi, lo, mid;\n\tconst struct config_reg *cr;\n\n\tlo = 0;\n\thi = array_size - 1;\n\twhile (hi >= lo) {\n\t\tmid = (hi + lo) >> 1;\n\t\tcr = p_cr_array + mid;\n\t\tif (cr->offset == offset)\n\t\t\treturn cr;\n\t\tif (cr->offset < offset)\n\t\t\tlo = mid + 1;\n\t\telse\n\t\t\thi = mid - 1;\n\t}\n\treturn NULL;\n}\n\nstatic inline const struct config_reg *\nvirtio_find_legacy_cr(int offset) {\n\treturn virtio_find_cr(legacy_config_regs,\n\t\tsizeof(legacy_config_regs) / sizeof(*legacy_config_regs),\n\t\toffset);\n}\n\nstatic inline const struct config_reg *\nvirtio_find_modern_cr(int offset) {\n\treturn virtio_find_cr(modern_config_regs,\n\t\tsizeof(modern_config_regs) / sizeof(*modern_config_regs),\n\t\toffset);\n}\n\n/*\n * Handle pci config space reads.\n * If it's to the MSI-X info, do that.\n * If it's part of the virtio standard stuff, do that.\n * Otherwise dispatch to the actual driver.\n */\nstatic uint64_t\nvirtio_pci_legacy_read(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t       int baridx, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst struct config_reg *cr;\n\tuint64_t virtio_config_size, max;\n\tconst char *name;\n\tuint32_t newoff;\n\tuint32_t value;\n\tint error = -1;\n\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tvops = base->vops;\n\tname = vops->name;\n\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\n\tif (size != 1 && size != 2 && size != 4)\n\t\tgoto bad;\n\n\tif (pci_msix_enabled(dev))\n\t\tvirtio_config_size = VIRTIO_PCI_CONFIG_OFF(1);\n\telse\n\t\tvirtio_config_size = VIRTIO_PCI_CONFIG_OFF(0);\n\n\tif (offset >= virtio_config_size) {\n\t\t/*\n\t\t * Subtract off the standard size (including MSI-X\n\t\t * registers if enabled) and dispatch to underlying driver.\n\t\t * If that fails, fall into general code.\n\t\t */\n\t\tnewoff = offset - virtio_config_size;\n\t\tmax = vops->cfgsize ? vops->cfgsize : 0x100000000;\n\t\tif (newoff + size > max)\n\t\t\tgoto bad;\n\n\t\tif (vops->cfgread) {\n\t\t\terror = (*vops->cfgread)(DEV_STRUCT(base), newoff,\n\t\t\t\t\t\t size, &value);\n\t\t}\n\t\tif (!error)\n\t\t\tgoto done;\n\t}\n\nbad:\n\tcr = virtio_find_legacy_cr(offset);\n\tif (cr == NULL || cr->size != size) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, so size must be bad */\n\t\t\tpr_err(\"%s: read from %s: bad size %d\\r\\n\",\n\t\t\t    name, cr->name, size);\n\t\t} else {\n\t\t\tpr_err(\"%s: read from bad offset/size %jd/%d\\r\\n\",\n\t\t\t    name, (uintmax_t)offset, size);\n\t\t}\n\t\tgoto done;\n\t}\n\n\tswitch (offset) {\n\tcase VIRTIO_PCI_HOST_FEATURES:\n\t\tvalue = base->device_caps;\n\t\tbreak;\n\tcase VIRTIO_PCI_GUEST_FEATURES:\n\t\tvalue = base->negotiated_caps;\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_PFN:\n\t\tif (base->curq < vops->nvq)\n\t\t\tvalue = base->queues[base->curq].pfn;\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_NUM:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].qsize : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_SEL:\n\t\tvalue = base->curq;\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_NOTIFY:\n\t\tvalue = 0;\t/* XXX */\n\t\tbreak;\n\tcase VIRTIO_PCI_STATUS:\n\t\tvalue = base->status;\n\t\tbreak;\n\tcase VIRTIO_PCI_ISR:\n\t\tvalue = base->isr;\n\t\tbase->isr = 0;\t\t/* a read clears this flag */\n\t\tif (value)\n\t\t\tpci_lintr_deassert(dev);\n\t\tbreak;\n\tcase VIRTIO_MSI_CONFIG_VECTOR:\n\t\tvalue = base->msix_cfg_idx;\n\t\tbreak;\n\tcase VIRTIO_MSI_QUEUE_VECTOR:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t    base->queues[base->curq].msix_idx :\n\t\t    VIRTIO_MSI_NO_VECTOR;\n\t\tbreak;\n\t}\ndone:\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n\treturn value;\n}\n\n/*\n * Handle pci config space writes.\n * If it's to the MSI-X info, do that.\n * If it's part of the virtio standard stuff, do that.\n * Otherwise dispatch to the actual driver.\n */\nstatic void\nvirtio_pci_legacy_write(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t\tint baridx, uint64_t offset, int size, uint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_vq_info *vq;\n\tstruct virtio_ops *vops;\n\tconst struct config_reg *cr;\n\tuint64_t virtio_config_size, max;\n\tconst char *name;\n\tuint32_t newoff;\n\tint error = -1;\n\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tvops = base->vops;\n\tname = vops->name;\n\n\tif (size != 1 && size != 2 && size != 4)\n\t\tgoto bad;\n\n\tif (pci_msix_enabled(dev))\n\t\tvirtio_config_size = VIRTIO_PCI_CONFIG_OFF(1);\n\telse\n\t\tvirtio_config_size = VIRTIO_PCI_CONFIG_OFF(0);\n\n\tif (offset >= virtio_config_size) {\n\t\t/*\n\t\t * Subtract off the standard size (including MSI-X\n\t\t * registers if enabled) and dispatch to underlying driver.\n\t\t */\n\t\tnewoff = offset - virtio_config_size;\n\t\tmax = vops->cfgsize ? vops->cfgsize : 0x100000000;\n\t\tif (newoff + size > max)\n\t\t\tgoto bad;\n\t\tif (vops->cfgwrite) {\n\t\t\terror = (*vops->cfgwrite)(DEV_STRUCT(base), newoff,\n\t\t\t\t\t\t  size, value);\n\t\t}\n\t\tif (!error)\n\t\t\tgoto done;\n\t}\n\nbad:\n\tcr = virtio_find_legacy_cr(offset);\n\tif (cr == NULL || cr->size != size || cr->ro) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, wrong size and/or reg is R/O */\n\t\t\tif (cr->size != size)\n\t\t\t\tpr_err(\"%s: write to %s: bad size %d\\r\\n\",\n\t\t\t\t    name, cr->name, size);\n\t\t\tif (cr->ro)\n\t\t\t\tpr_err(\"%s: write to read-only reg %s\\r\\n\",\n\t\t\t\t    name, cr->name);\n\t\t} else {\n\t\t\tpr_err(\"%s: write to bad offset/size %jd/%d\\r\\n\",\n\t\t\t    name, (uintmax_t)offset, size);\n\t\t}\n\t\tgoto done;\n\t}\n\n\tswitch (offset) {\n\tcase VIRTIO_PCI_GUEST_FEATURES:\n\t\tbase->negotiated_caps = value & base->device_caps;\n\t\tif (vops->apply_features)\n\t\t\t(*vops->apply_features)(DEV_STRUCT(base),\n\t\t\t    base->negotiated_caps);\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_PFN:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvirtio_vq_init(base, value);\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_SEL:\n\t\t/*\n\t\t * Note that the guest is allowed to select an\n\t\t * invalid queue; we just need to return a QNUM\n\t\t * of 0 while the bad queue is selected.\n\t\t */\n\t\tbase->curq = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_QUEUE_NOTIFY:\n\t\tif (value >= vops->nvq) {\n\t\t\tpr_err(\"%s: queue %d notify out of range\\r\\n\",\n\t\t\t\tname, (int)value);\n\t\t\tgoto done;\n\t\t}\n\t\tvq = &base->queues[value];\n\t\tif (vq->notify)\n\t\t\t(*vq->notify)(DEV_STRUCT(base), vq);\n\t\telse if (vops->qnotify)\n\t\t\t(*vops->qnotify)(DEV_STRUCT(base), vq);\n\t\telse\n\t\t\tpr_err(\"%s: qnotify queue %d: missing vq/vops notify\\r\\n\",\n\t\t\t\tname, (int)value);\n\t\tbreak;\n\tcase VIRTIO_PCI_STATUS:\n\t\tbase->status = value;\n\t\tif (vops->set_status)\n\t\t\t(*vops->set_status)(DEV_STRUCT(base), value);\n\t\tif ((value == 0) && (vops->reset))\n\t\t\t(*vops->reset)(DEV_STRUCT(base));\n\t\tif ((value & VIRTIO_CONFIG_S_DRIVER_OK) &&\n\t\t     base->backend_type == BACKEND_VBSU &&\n\t\t     virtio_poll_enabled) {\n\t\t\tbase->polling_timer.clockid = CLOCK_MONOTONIC;\n\t\t\tacrn_timer_init(&base->polling_timer, virtio_poll_timer, base);\n\t\t\t/* wait 5s to start virtio poll mode\n\t\t\t * skip vsbl and make sure device initialization completed\n\t\t\t * FIXME: Need optimization in the future\n\t\t\t */\n\t\t\tvirtio_start_timer(&base->polling_timer, 5, 0);\n\t\t}\n\t\tbreak;\n\tcase VIRTIO_MSI_CONFIG_VECTOR:\n\t\tbase->msix_cfg_idx = value;\n\t\tbreak;\n\tcase VIRTIO_MSI_QUEUE_VECTOR:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->msix_idx = value;\n\t\tbreak;\n\t}\n\tgoto done;\n\nbad_qindex:\n\tpr_err(\"%s: write config reg %s: curq %d >= max %d\\r\\n\",\n\t    name, cr->name, base->curq, vops->nvq);\ndone:\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n}\n\nstatic int\nvirtio_find_capability(struct virtio_base *base, uint8_t cfg_type)\n{\n\tstruct pci_vdev *dev = base->dev;\n\tuint8_t type;\n\tint rc, coff = 0;\n\n\trc = pci_emul_find_capability(dev, PCIY_VENDOR, &coff);\n\twhile (!rc) {\n\t\ttype = pci_get_cfgdata8(dev,\n\t\t\tcoff + offsetof(struct virtio_pci_cap, cfg_type));\n\t\tif (type == cfg_type)\n\t\t\treturn coff;\n\t\trc = pci_emul_find_capability(dev, PCIY_VENDOR, &coff);\n\t}\n\n\treturn -1;\n}\n\n/*\n * Set virtio modern MMIO BAR (usually 4) to map the 4 capabilities.\n */\nstatic int\nvirtio_set_modern_mmio_bar(struct virtio_base *base, int barnum)\n{\n\tstruct virtio_ops *vops;\n\tint rc;\n\tstruct virtio_pci_cap cap = {\n\t\t.cap_vndr = PCIY_VENDOR,\n\t\t.cap_next = 0,\n\t\t.cap_len = sizeof(cap),\n\t\t.bar = barnum,\n\t};\n\tstruct virtio_pci_notify_cap notify = {\n\t\t.cap.cap_vndr = PCIY_VENDOR,\n\t\t.cap.cap_next = 0,\n\t\t.cap.cap_len = sizeof(notify),\n\t\t.cap.cfg_type = VIRTIO_PCI_CAP_NOTIFY_CFG,\n\t\t.cap.bar = barnum,\n\t\t.cap.offset = VIRTIO_CAP_NOTIFY_OFFSET,\n\t\t.cap.length = VIRTIO_CAP_NOTIFY_SIZE,\n\t\t.notify_off_multiplier = VIRTIO_MODERN_NOTIFY_OFF_MULT,\n\t};\n\tstruct virtio_pci_cfg_cap cfg = {\n\t\t.cap.cap_vndr = PCIY_VENDOR,\n\t\t.cap.cap_next = 0,\n\t\t.cap.cap_len = sizeof(cfg),\n\t\t.cap.cfg_type = VIRTIO_PCI_CAP_PCI_CFG,\n\t};\n\n\tvops = base->vops;\n\n\tif (vops->cfgsize > VIRTIO_CAP_DEVICE_SIZE) {\n\t\tpr_err(\"%s: cfgsize %lu > max %d\\r\\n\",\n\t\t\tvops->name, vops->cfgsize, VIRTIO_CAP_DEVICE_SIZE);\n\t\treturn -1;\n\t}\n\n\t/* common configuration capability */\n\tcap.cfg_type = VIRTIO_PCI_CAP_COMMON_CFG;\n\tcap.offset = VIRTIO_CAP_COMMON_OFFSET;\n\tcap.length = VIRTIO_CAP_COMMON_SIZE;\n\trc = pci_emul_add_capability(base->dev, (u_char *)&cap, sizeof(cap));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add common configuration capability failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* isr status capability */\n\tcap.cfg_type = VIRTIO_PCI_CAP_ISR_CFG;\n\tcap.offset = VIRTIO_CAP_ISR_OFFSET;\n\tcap.length = VIRTIO_CAP_ISR_SIZE;\n\trc = pci_emul_add_capability(base->dev, (u_char *)&cap, sizeof(cap));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add isr status capability failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* device specific configuration capability */\n\tcap.cfg_type = VIRTIO_PCI_CAP_DEVICE_CFG;\n\tcap.offset = VIRTIO_CAP_DEVICE_OFFSET;\n\tcap.length = VIRTIO_CAP_DEVICE_SIZE;\n\trc = pci_emul_add_capability(base->dev, (u_char *)&cap, sizeof(cap));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add device specific configuration capability failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* notification capability */\n\trc = pci_emul_add_capability(base->dev, (u_char *)&notify,\n\t\tsizeof(notify));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add notification capability failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* pci alternative configuration access capability */\n\trc = pci_emul_add_capability(base->dev, (u_char *)&cfg, sizeof(cfg));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add alternative configuration access capability failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* allocate and register modern memory bar */\n\trc = pci_emul_alloc_bar(base->dev, barnum, PCIBAR_MEM64,\n\t\t\t\tVIRTIO_MODERN_MEM_BAR_SIZE);\n\tif (rc != 0) {\n\t\tpr_err(\"allocate and register modern memory bar failed\\n\");\n\t\treturn -1;\n\t}\n\n\tbase->cfg_coff = virtio_find_capability(base, VIRTIO_PCI_CAP_PCI_CFG);\n\tif (base->cfg_coff < 0) {\n\t\tpr_err(\"%s: VIRTIO_PCI_CAP_PCI_CFG not found\\r\\n\",\n\t\t\tvops->name);\n\t\treturn -1;\n\t}\n\n\tbase->modern_mmio_bar_idx = barnum;\n\treturn 0;\n}\n\n/*\n * Set virtio modern PIO BAR (usually 2) to map notify capability.\n */\nstatic int\nvirtio_set_modern_pio_bar(struct virtio_base *base, int barnum)\n{\n\tint rc;\n\tstruct virtio_pci_notify_cap notify_pio = {\n\t\t.cap.cap_vndr = PCIY_VENDOR,\n\t\t.cap.cap_next = 0,\n\t\t.cap.cap_len = sizeof(notify_pio),\n\t\t.cap.cfg_type = VIRTIO_PCI_CAP_NOTIFY_CFG,\n\t\t.cap.bar = barnum,\n\t\t.cap.offset = 0,\n\t\t.cap.length = 4,\n\t\t.notify_off_multiplier = 0,\n\t};\n\n\t/* notification capability */\n\trc = pci_emul_add_capability(base->dev, (u_char *)&notify_pio,\n\t\tsizeof(notify_pio));\n\tif (rc != 0) {\n\t\tpr_err(\"pci emulation add notification capability for virtio modern PIO BAR failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* allocate and register modern pio bar */\n\trc = pci_emul_alloc_bar(base->dev, barnum, PCIBAR_IO, 4);\n\tif (rc != 0) {\n\t\tpr_err(\"allocate and register modern pio bar failed\\n\");\n\t\treturn -1;\n\t}\n\n\tbase->modern_pio_bar_idx = barnum;\n\treturn 0;\n}\n\n/**\n * @brief Set modern BAR (usually 4) to map PCI config registers.\n *\n * Set modern MMIO BAR (usually 4) to map virtio 1.0 capabilities and optional\n * set modern PIO BAR (usually 2) to map notify capability. This interface is\n * only valid for modern virtio.\n *\n * @param base Pointer to struct virtio_base.\n * @param use_notify_pio Whether use pio for notify capability.\n *\n * @return 0 on success and non-zero on fail.\n */\nint\nvirtio_set_modern_bar(struct virtio_base *base, bool use_notify_pio)\n{\n\tstruct virtio_ops *vops;\n\tint rc = 0;\n\n\tvops = base->vops;\n\n\tif (!vops || (base->device_caps & (1UL << VIRTIO_F_VERSION_1)) == 0)\n\t\treturn -1;\n\n\tif (use_notify_pio)\n\t\trc = virtio_set_modern_pio_bar(base,\n\t\t\tVIRTIO_MODERN_PIO_BAR_IDX);\n\tif (!rc)\n\t\trc = virtio_set_modern_mmio_bar(base,\n\t\t\tVIRTIO_MODERN_MMIO_BAR_IDX);\n\n\treturn rc;\n}\n\nstatic struct cap_region {\n\tuint64_t\tcap_offset;\t/* offset of capability region */\n\tint\t\tcap_size;\t/* size of capability region */\n\tint\t\tcap_id;\t\t/* capability id */\n} cap_regions[] = {\n\t{VIRTIO_CAP_COMMON_OFFSET, VIRTIO_CAP_COMMON_SIZE,\n\t\tVIRTIO_PCI_CAP_COMMON_CFG},\n\t{VIRTIO_CAP_ISR_OFFSET, VIRTIO_CAP_ISR_SIZE,\n\t\tVIRTIO_PCI_CAP_ISR_CFG},\n\t{VIRTIO_CAP_DEVICE_OFFSET, VIRTIO_CAP_DEVICE_SIZE,\n\t\tVIRTIO_PCI_CAP_DEVICE_CFG},\n\t{VIRTIO_CAP_NOTIFY_OFFSET, VIRTIO_CAP_NOTIFY_SIZE,\n\t\tVIRTIO_PCI_CAP_NOTIFY_CFG},\n};\n\nstatic inline int\nvirtio_get_cap_id(uint64_t offset, int size)\n{\n\tint i, rc = -1;\n\n\tfor (i = 0; i < ARRAY_SIZE(cap_regions); i++) {\n\t\tif (offset >= cap_regions[i].cap_offset &&\n\t\t\toffset + size <= cap_regions[i].cap_offset +\n\t\t\tcap_regions[i].cap_size)\n\t\t\treturn cap_regions[i].cap_id;\n\t}\n\n\treturn rc;\n}\n\nstatic uint32_t\nvirtio_common_cfg_read(struct pci_vdev *dev, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst struct config_reg *cr;\n\tconst char *name;\n\tuint32_t value;\n\n\tvops = base->vops;\n\tname = vops->name;\n\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\n\tcr = virtio_find_modern_cr(offset);\n\tif (cr == NULL || cr->size != size) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, so size must be bad */\n\t\t\tpr_err(\"%s: read from %s: bad size %d\\r\\n\",\n\t\t\t\tname, cr->name, size);\n\t\t} else {\n\t\t\tpr_err(\"%s: read from bad offset/size %jd/%d\\r\\n\",\n\t\t\t\tname, (uintmax_t)offset, size);\n\t\t}\n\n\t\treturn value;\n\t}\n\n\tswitch (offset) {\n\tcase VIRTIO_PCI_COMMON_DFSELECT:\n\t\tvalue = base->device_feature_select;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_DF:\n\t\tif (base->device_feature_select == 0)\n\t\t\tvalue = base->device_caps & 0xffffffff;\n\t\telse if (base->device_feature_select == 1)\n\t\t\tvalue = (base->device_caps >> 32) & 0xffffffff;\n\t\telse /* present 0, see 4.1.4.3.1 */\n\t\t\tvalue = 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_GFSELECT:\n\t\tvalue = base->driver_feature_select;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_GF:\n\t\t/* see 4.1.4.3.1. Present any valid feature bits the driver\n\t\t * has written in driver_feature. Valid feature bits are those\n\t\t * which are subset of the corresponding device_feature bits\n\t\t */\n\t\tif (base->driver_feature_select == 0)\n\t\t\tvalue = base->negotiated_caps & 0xffffffff;\n\t\telse if (base->driver_feature_select == 1)\n\t\t\tvalue = (base->negotiated_caps >> 32) & 0xffffffff;\n\t\telse\n\t\t\tvalue = 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_MSIX:\n\t\tvalue = base->msix_cfg_idx;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_NUMQ:\n\t\tvalue = vops->nvq;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_STATUS:\n\t\tvalue = base->status;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_CFGGENERATION:\n\t\tvalue = base->config_generation;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_SELECT:\n\t\tvalue = base->curq;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_SIZE:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].qsize : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_MSIX:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].msix_idx :\n\t\t\tVIRTIO_MSI_NO_VECTOR;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_ENABLE:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].enabled : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_NOFF:\n\t\tvalue = base->curq;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_DESCLO:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_desc[0] : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_DESCHI:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_desc[1] : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_AVAILLO:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_avail[0] : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_AVAILHI:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_avail[1] : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_USEDLO:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_used[0] : 0;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_USEDHI:\n\t\tvalue = base->curq < vops->nvq ?\n\t\t\tbase->queues[base->curq].gpa_used[1] : 0;\n\t\tbreak;\n\t}\n\n\treturn value;\n}\n\nstatic void\nvirtio_common_cfg_write(struct pci_vdev *dev, uint64_t offset, int size,\n\t\t\tuint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_vq_info *vq;\n\tstruct virtio_ops *vops;\n\tconst struct config_reg *cr;\n\tconst char *name;\n\n\tvops = base->vops;\n\tname = vops->name;\n\n\tcr = virtio_find_modern_cr(offset);\n\tif (cr == NULL || cr->size != size || cr->ro) {\n\t\tif (cr != NULL) {\n\t\t\t/* offset must be OK, wrong size and/or reg is R/O */\n\t\t\tif (cr->size != size)\n\t\t\t\tpr_err(\"%s: write to %s: bad size %d\\r\\n\",\n\t\t\t\t\tname, cr->name, size);\n\t\t\tif (cr->ro)\n\t\t\t\tpr_err(\"%s: write to read-only reg %s\\r\\n\",\n\t\t\t\t\tname, cr->name);\n\t\t} else {\n\t\t\tpr_err(\"%s: write to bad offset/size %jd/%d\\r\\n\",\n\t\t\t\tname, (uintmax_t)offset, size);\n\t\t}\n\n\t\treturn;\n\t}\n\n\tswitch (offset) {\n\tcase VIRTIO_PCI_COMMON_DFSELECT:\n\t\tbase->device_feature_select = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_GFSELECT:\n\t\tbase->driver_feature_select = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_GF:\n\t\tif (base->status & VIRTIO_CONFIG_S_DRIVER_OK)\n\t\t\tbreak;\n\t\tif (base->driver_feature_select < 2) {\n\t\t\tvalue &= 0xffffffff;\n\t\t\tbase->negotiated_caps =\n\t\t\t\t(value << (base->driver_feature_select * 32))\n\t\t\t\t& base->device_caps;\n\t\t\tif (vops->apply_features)\n\t\t\t\t(*vops->apply_features)(DEV_STRUCT(base),\n\t\t\t\t\tbase->negotiated_caps);\n\t\t}\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_MSIX:\n\t\tbase->msix_cfg_idx = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_STATUS:\n\t\tbase->status = value & 0xff;\n\t\tif (vops->set_status)\n\t\t\t(*vops->set_status)(DEV_STRUCT(base), value);\n\t\tif ((base->status == 0) && (vops->reset))\n\t\t\t(*vops->reset)(DEV_STRUCT(base));\n\t\t/* TODO: virtio poll mode for modern devices */\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_SELECT:\n\t\t/*\n\t\t * Note that the guest is allowed to select an\n\t\t * invalid queue; we just need to return a QNUM\n\t\t * of 0 while the bad queue is selected.\n\t\t */\n\t\tbase->curq = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_SIZE:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->qsize = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_MSIX:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->msix_idx = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_ENABLE:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvirtio_vq_enable(base);\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_DESCLO:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_desc[0] = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_DESCHI:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_desc[1] = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_AVAILLO:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_avail[0] = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_AVAILHI:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_avail[1] = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_USEDLO:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_used[0] = value;\n\t\tbreak;\n\tcase VIRTIO_PCI_COMMON_Q_USEDHI:\n\t\tif (base->curq >= vops->nvq)\n\t\t\tgoto bad_qindex;\n\t\tvq = &base->queues[base->curq];\n\t\tvq->gpa_used[1] = value;\n\t\tbreak;\n\t}\n\n\treturn;\n\nbad_qindex:\n\tpr_err(\"%s: write config reg %s: curq %d >= max %d\\r\\n\",\n\t\tname, cr->name, base->curq, vops->nvq);\n}\n\n/* ignore driver writes to ISR region, and only support ISR region read */\nstatic uint32_t\nvirtio_isr_cfg_read(struct pci_vdev *dev, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\tuint32_t value = 0;\n\n\tvalue = base->isr;\n\tbase->isr = 0;\t\t/* a read clears this flag */\n\tif (value)\n\t\tpci_lintr_deassert(dev);\n\n\treturn value;\n}\n\nstatic uint32_t\nvirtio_device_cfg_read(struct pci_vdev *dev, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tuint32_t value;\n\tuint64_t max;\n\tint error = -1;\n\n\tvops = base->vops;\n\tname = vops->name;\n\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\tmax = vops->cfgsize ? vops->cfgsize : 0x100000000;\n\n\tif (offset + size > max) {\n\t\tpr_err(\"%s: reading from 0x%lx size %d exceeds limit\\r\\n\",\n\t\t\tname, offset, size);\n\t\treturn value;\n\t}\n\n\tif (vops->cfgread) {\n\t\terror = (*vops->cfgread)(DEV_STRUCT(base), offset, size, &value);\n\t}\n\tif (error) {\n\t\tpr_err(\"%s: reading from 0x%lx size %d failed %d\\r\\n\",\n\t\t\tname, offset, size, error);\n\t\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\t}\n\n\treturn value;\n}\n\nstatic void\nvirtio_device_cfg_write(struct pci_vdev *dev, uint64_t offset, int size,\n\t\t\tuint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tuint64_t max;\n\tint error = -1;\n\n\tvops = base->vops;\n\tname = vops->name;\n\tmax = vops->cfgsize ? vops->cfgsize : 0x100000000;\n\n\tif (offset + size > max) {\n\t\tpr_err(\"%s: writing to 0x%lx size %d exceeds limit\\r\\n\",\n\t\t\tname, offset, size);\n\t\treturn;\n\t}\n\n\tif (vops->cfgwrite) {\n\t\terror = (*vops->cfgwrite)(DEV_STRUCT(base), offset, size, value);\n\t}\n\tif (error)\n\t\tpr_err(\"%s: writing ot 0x%lx size %d failed %d\\r\\n\",\n\t\t\tname, offset, size, error);\n}\n\n/*\n * ignore driver reads from notify region, and only support notify region\n * write\n */\nstatic void\nvirtio_notify_cfg_write(struct pci_vdev *dev, uint64_t offset, int size,\n\t\t\tuint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_vq_info *vq;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tuint64_t idx;\n\n\tidx = offset / VIRTIO_MODERN_NOTIFY_OFF_MULT;\n\tvops = base->vops;\n\tname = vops->name;\n\n\tif (idx >= vops->nvq) {\n\t\tpr_err(\"%s: queue %lu notify out of range\\r\\n\", name, idx);\n\t\treturn;\n\t}\n\n\tvq = &base->queues[idx];\n\tif (vq->notify)\n\t\t(*vq->notify)(DEV_STRUCT(base), vq);\n\telse if (vops->qnotify)\n\t\t(*vops->qnotify)(DEV_STRUCT(base), vq);\n\telse\n\t\tpr_err(\"%s: qnotify queue %lu: missing vq/vops notify\\r\\n\",\n\t\t\tname, idx);\n}\n\nstatic uint32_t\nvirtio_pci_modern_mmio_read(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t\t    int baridx, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tuint32_t value;\n\tint capid;\n\n\n\tvops = base->vops;\n\tname = vops->name;\n\tvalue = size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n\n\tif (size != 1 && size != 2 && size != 4) {\n\t\tpr_err(\"%s: read from [%d:0x%lx] bad size %d\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t\treturn value;\n\t}\n\n\tcapid = virtio_get_cap_id(offset, size);\n\tif (capid < 0) {\n\t\tpr_err(\"%s: read from [%d:0x%lx] bad range %d\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t\treturn value;\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tswitch (capid) {\n\tcase VIRTIO_PCI_CAP_COMMON_CFG:\n\t\toffset -= VIRTIO_CAP_COMMON_OFFSET;\n\t\tvalue = virtio_common_cfg_read(dev, offset, size);\n\t\tbreak;\n\tcase VIRTIO_PCI_CAP_ISR_CFG:\n\t\toffset -= VIRTIO_CAP_ISR_OFFSET;\n\t\tvalue = virtio_isr_cfg_read(dev, offset, size);\n\t\tbreak;\n\tcase VIRTIO_PCI_CAP_DEVICE_CFG:\n\t\toffset -= VIRTIO_CAP_DEVICE_OFFSET;\n\t\tvalue = virtio_device_cfg_read(dev, offset, size);\n\t\tbreak;\n\tdefault: /* guest driver should not read from notify region */\n\t\tpr_err(\"%s: read from [%d:0x%lx] size %d not supported\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n\treturn value;\n}\n\nstatic void\nvirtio_pci_modern_mmio_write(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t\t     int baridx, uint64_t offset, int size,\n\t\t\t     uint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tint capid;\n\n\n\tvops = base->vops;\n\tname = vops->name;\n\n\tif (size != 1 && size != 2 && size != 4) {\n\t\tpr_err(\"%s: write to [%d:0x%lx] bad size %d\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t\treturn;\n\t}\n\n\tcapid = virtio_get_cap_id(offset, size);\n\tif (capid < 0) {\n\t\tpr_err(\"%s: write to [%d:0x%lx] bad range %d\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t\treturn;\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tswitch (capid) {\n\tcase VIRTIO_PCI_CAP_COMMON_CFG:\n\t\toffset -= VIRTIO_CAP_COMMON_OFFSET;\n\t\tvirtio_common_cfg_write(dev, offset, size, value);\n\t\tbreak;\n\tcase VIRTIO_PCI_CAP_DEVICE_CFG:\n\t\toffset -= VIRTIO_CAP_DEVICE_OFFSET;\n\t\tvirtio_device_cfg_write(dev, offset, size, value);\n\t\tbreak;\n\tcase VIRTIO_PCI_CAP_NOTIFY_CFG:\n\t\toffset -= VIRTIO_CAP_NOTIFY_OFFSET;\n\t\tvirtio_notify_cfg_write(dev, offset, size, value);\n\t\tbreak;\n\tdefault: /* guest driver should not write to ISR region */\n\t\tpr_err(\"%s: write to [%d:0x%lx] size %d not supported\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n}\n\nstatic uint32_t\nvirtio_pci_modern_pio_read(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t\t   int baridx, uint64_t offset, int size)\n{\n\t/* guest driver should not read notify pio */\n\treturn 0;\n}\n\nstatic void\nvirtio_pci_modern_pio_write(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t\t    int baridx, uint64_t offset, int size,\n\t\t\t    uint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\tstruct virtio_vq_info *vq;\n\tstruct virtio_ops *vops;\n\tconst char *name;\n\tuint64_t idx;\n\n\n\tvops = base->vops;\n\tname = vops->name;\n\tidx = value;\n\n\tif (size != 1 && size != 2 && size != 4) {\n\t\tpr_err(\"%s: write to [%d:0x%lx] bad size %d\\r\\n\",\n\t\t\tname, baridx, offset, size);\n\t\treturn;\n\t}\n\n\tif (idx >= vops->nvq) {\n\t\tpr_err(\"%s: queue %lu notify out of range\\r\\n\", name, idx);\n\t\treturn;\n\t}\n\n\tif (base->mtx)\n\t\tpthread_mutex_lock(base->mtx);\n\n\tvq = &base->queues[idx];\n\tif (vq->notify)\n\t\t(*vq->notify)(DEV_STRUCT(base), vq);\n\telse if (vops->qnotify)\n\t\t(*vops->qnotify)(DEV_STRUCT(base), vq);\n\telse\n\t\tpr_err(\"%s: qnotify queue %lu: missing vq/vops notify\\r\\n\",\n\t\t\tname, idx);\n\n\tif (base->mtx)\n\t\tpthread_mutex_unlock(base->mtx);\n}\n\n/**\n * @brief Handle PCI configuration space reads.\n *\n * Handle virtio standard register reads, and dispatch other reads to\n * actual virtio device driver.\n *\n * @param ctx Pointer to struct vmctx representing VM context.\n * @param vcpu VCPU ID.\n * @param dev Pointer to struct pci_vdev which emulates a PCI device.\n * @param baridx Which BAR[0..5] to use.\n * @param offset Register offset in bytes within a BAR region.\n * @param size Access range in bytes.\n *\n * @return register value.\n */\nuint64_t\nvirtio_pci_read(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\tint baridx, uint64_t offset, int size)\n{\n\tstruct virtio_base *base = dev->arg;\n\n\tif (base->flags & VIRTIO_USE_MSIX) {\n\t\tif (baridx == pci_msix_table_bar(dev) ||\n\t\t    baridx == pci_msix_pba_bar(dev)) {\n\t\t\treturn pci_emul_msix_tread(dev, offset, size);\n\t\t}\n\t}\n\n\tif (baridx == base->legacy_pio_bar_idx)\n\t\treturn virtio_pci_legacy_read(ctx, vcpu, dev, baridx,\n\t\t\toffset, size);\n\n\tif (baridx == base->modern_mmio_bar_idx)\n\t\treturn virtio_pci_modern_mmio_read(ctx, vcpu, dev, baridx,\n\t\t\toffset, size);\n\n\tif (baridx == base->modern_pio_bar_idx)\n\t\treturn virtio_pci_modern_pio_read(ctx, vcpu, dev, baridx,\n\t\t\toffset, size);\n\n\tpr_err(\"%s: read unexpected baridx %d\\r\\n\",\n\t\tbase->vops->name, baridx);\n\treturn size == 1 ? 0xff : size == 2 ? 0xffff : 0xffffffff;\n}\n\n/**\n * @brief Handle PCI configuration space writes.\n *\n * Handle virtio standard register writes, and dispatch other writes to\n * actual virtio device driver.\n *\n * @param ctx Pointer to struct vmctx representing VM context.\n * @param vcpu VCPU ID.\n * @param dev Pointer to struct pci_vdev which emulates a PCI device.\n * @param baridx Which BAR[0..5] to use.\n * @param offset Register offset in bytes within a BAR region.\n * @param size Access range in bytes.\n * @param value Data value to be written into register.\n *\n * @return None\n */\nvoid\nvirtio_pci_write(struct vmctx *ctx, int vcpu, struct pci_vdev *dev,\n\t\t int baridx, uint64_t offset, int size, uint64_t value)\n{\n\tstruct virtio_base *base = dev->arg;\n\n\tif (base->flags & VIRTIO_USE_MSIX) {\n\t\tif (baridx == pci_msix_table_bar(dev) ||\n\t\t    baridx == pci_msix_pba_bar(dev)) {\n\t\t\tpci_emul_msix_twrite(dev, offset, size, value);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (baridx == base->legacy_pio_bar_idx) {\n\t\tvirtio_pci_legacy_write(ctx, vcpu, dev, baridx,\n\t\t\toffset, size, value);\n\t\treturn;\n\t}\n\n\tif (baridx == base->modern_mmio_bar_idx) {\n\t\tvirtio_pci_modern_mmio_write(ctx, vcpu, dev, baridx,\n\t\t\toffset, size, value);\n\t\treturn;\n\t}\n\n\tif (baridx == base->modern_pio_bar_idx) {\n\t\tvirtio_pci_modern_pio_write(ctx, vcpu, dev, baridx,\n\t\t\toffset, size, value);\n\t\treturn;\n\t}\n\n\tpr_err(\"%s: write unexpected baridx %d\\r\\n\",\n\t\tbase->vops->name, baridx);\n}\n\n/**\n * @brief Get the virtio poll parameters\n *\n * @param optarg Pointer to parameters string.\n *\n * @return fail -1 success 0\n */\nint\nacrn_parse_virtio_poll_interval(const char *optarg)\n{\n\tchar *ptr;\n\n\tvirtio_poll_interval = strtoul(optarg, &ptr, 0);\n\n\t/* poll interval is limited from 1us to 10ms */\n\tif (virtio_poll_interval < 1 || virtio_poll_interval > 10000000)\n\t\treturn -1;\n\n\tvirtio_poll_enabled = 1;\n\n\treturn 0;\n}\n"], "filenames": ["devicemodel/hw/pci/virtio/virtio.c"], "buggy_code_start_loc": [649], "buggy_code_end_loc": [649], "fixing_code_start_loc": [650], "fixing_code_end_loc": [653], "type": "CWE-476", "message": "ACRN before 2.5 has a hw/pci/virtio/virtio.c vq_endchains NULL Pointer Dereference.", "other": {"cve": {"id": "CVE-2021-36143", "sourceIdentifier": "cve@mitre.org", "published": "2021-07-02T22:15:08.833", "lastModified": "2021-07-08T18:07:26.143", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "ACRN before 2.5 has a hw/pci/virtio/virtio.c vq_endchains NULL Pointer Dereference."}, {"lang": "es", "value": "ACRN versiones anteriores a 2.5, presenta una Desreferencia del Puntero NULL en la funci\u00f3n vq_endchains en el archivo hw/pci/virtio/virtio.c"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:acrn:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.5", "matchCriteriaId": "CF2731E5-0DF2-4018-8266-7028AC919320"}]}]}], "references": [{"url": "https://github.com/projectacrn/acrn-hypervisor/commit/154fe59531c12b82e26d1b24b5531f5066d224f5", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/projectacrn/acrn-hypervisor/commit/154fe59531c12b82e26d1b24b5531f5066d224f5"}}