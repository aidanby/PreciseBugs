{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#define EIGEN_USE_THREADS\n\n#include <algorithm>\n#include <cmath>\n#include <random>\n#include <vector>\n\n#include \"tensorflow/core/kernels/fractional_pool_common.h\"\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/lib/random/random.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/mutex.h\"\n#include \"tensorflow/core/util/guarded_philox_random.h\"\n\nnamespace tensorflow {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename T>\nclass FractionalMaxPoolOp : public OpKernel {\n public:\n  explicit FractionalMaxPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\"pooling_ratio field must \"\n                                        \"specify 4 dimensions\"));\n\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional max pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    constexpr int tensor_in_and_out_dims = 4;\n\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n\n      OP_REQUIRES(\n          context, input_size[i] >= pooling_ratio_[i],\n          errors::InvalidArgument(\"Pooling ratio is higher than input \"\n                                  \"dimension size for dimension \",\n                                  i, \". Input dim size: \", input_size[i],\n                                  \" pooling ratio: \", pooling_ratio_[i]));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      // This must match the same logic in the shape function in\n      // core/ops/nn_ops.cc.\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n\n    // Generate pooling sequence.\n    std::vector<int64_t> height_cum_seq;\n    std::vector<int64_t> width_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    height_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                             &generator, pseudo_random_);\n    width_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                            &generator, pseudo_random_);\n\n    // Prepare output.\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_height_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            1, TensorShape({static_cast<int64_t>(height_cum_seq.size())}),\n            &output_height_seq_tensor));\n    Tensor* output_width_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            2, TensorShape({static_cast<int64_t>(width_cum_seq.size())}),\n            &output_width_seq_tensor));\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n\n    // Initializes the output tensor with MIN<T>.\n    output_tensor->flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n\n    auto output_height_seq_flat = output_height_seq_tensor->flat<int64_t>();\n    auto output_width_seq_flat = output_width_seq_tensor->flat<int64_t>();\n\n    // Set output tensors.\n    for (int i = 0; i < height_cum_seq.size(); ++i) {\n      output_height_seq_flat(i) = height_cum_seq[i];\n    }\n\n    for (int i = 0; i < width_cum_seq.size(); ++i) {\n      output_width_seq_flat(i) = width_cum_seq[i];\n    }\n\n    // For both input and output,\n    // 0: batch\n    // 1: height / row\n    // 2: width / col\n    // 3: depth / channel\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // height sequence.\n      for (int64_t hs = 0; hs < height_cum_seq.size() - 1; ++hs) {\n        // height start and end.\n        const int64_t height_start = height_cum_seq[hs];\n        int64_t height_end =\n            overlapping_ ? height_cum_seq[hs + 1] : height_cum_seq[hs + 1] - 1;\n        height_end = std::min(height_end, height_max);\n\n        // width sequence.\n        for (int64_t ws = 0; ws < width_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // width start and end.\n          const int64_t width_start = width_cum_seq[ws];\n          int64_t width_end =\n              overlapping_ ? width_cum_seq[ws + 1] : width_cum_seq[ws + 1] - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) =\n                  out_mat.col(out_offset).cwiseMax(in_mat.col(in_offset));\n            }\n          }\n        }\n      }\n    }\n  }\n\n private:\n  bool deterministic_;\n  int64_t seed_;\n  int64_t seed2_;\n  std::vector<float> pooling_ratio_;\n  bool pseudo_random_;\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALMAXPOOL(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                                    \\\n      Name(\"FractionalMaxPool\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      FractionalMaxPoolOp<type>)\n\nREGISTER_FRACTIONALMAXPOOL(int32);\nREGISTER_FRACTIONALMAXPOOL(int64_t);\nREGISTER_FRACTIONALMAXPOOL(float);\nREGISTER_FRACTIONALMAXPOOL(double);\n\n#undef REGISTER_FRACTIONALMAXPOOL\n\nstatic const int kInvalidMaxPoolingIndex = -1;\n\ntemplate <class T>\nclass FractionalMaxPoolGradOp : public OpKernel {\n public:\n  explicit FractionalMaxPoolGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // There are two steps when calculating gradient for FractionalMaxPool.\n    // 1) Walk through the process of calculating fractional pooling given\n    //    pooling region; however, in the process, keep track of where the max\n    //    element comes from. (arg_max)\n    // 2) Populate the value of out_backprop to where arg_max indicates. If\n    //    we support overlapping, it is likely to have multiple out_backprop[i]\n    //    propagates back to the same arg_max value.\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<int64, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenIndexMatrixMap;\n\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_backprop = context->input(2);\n    const Tensor& height_seq_tensor = context->input(3);\n    const Tensor& width_seq_tensor = context->input(4);\n\n    // Just to make it similar to FractionalMaxPoolOp.\n    constexpr int tensor_in_and_out_dims = 4;\n    OP_REQUIRES(\n        context, tensor_in.dims() == tensor_in_and_out_dims,\n        errors::InvalidArgument(\"orig_input should be a tensor of rank 4, got \",\n                                tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"orig_input must not be empty, got \",\n                                        tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_out.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\n                    \"orig_output should be a tensor of rank 4, got \",\n                    tensor_out.DebugString()));\n    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                errors::InvalidArgument(\"orig_output must not be empty, got \",\n                                        tensor_out.DebugString()));\n    std::vector<int64_t> input_size(tensor_in_and_out_dims);\n    std::vector<int64_t> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n    }\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] = tensor_out.dim_size(i);\n    }\n\n    // ---------\n    // Step 1\n    // ---------\n    Tensor tensor_out_dup;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_temp(\n                                {1}, DataTypeToEnum<T>::v(), tensor_out.shape(),\n                                &tensor_out_dup));\n    Tensor tensor_out_arg_max;\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<int64_t>::v(),\n                                                   tensor_out.shape(),\n                                                   &tensor_out_arg_max));\n    // Find arg_max for each tensor_out\n    ConstEigenMatrixMap tensor_in_mat(\n        tensor_in.flat<T>().data(), input_size[3],\n        input_size[2] * input_size[1] * input_size[0]);\n    EigenMatrixMap tensor_out_dup_mat(\n        tensor_out_dup.flat<T>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n    EigenIndexMatrixMap tensor_out_arg_max_mat(\n        tensor_out_arg_max.flat<int64_t>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n\n    tensor_out_arg_max.flat<int64_t>().setConstant(kInvalidMaxPoolingIndex);\n    // Initializes the duplicate output tensor with MIN<T>.\n    tensor_out_dup.flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n\n    auto height_seq_tensor_flat = height_seq_tensor.flat<int64_t>();\n    auto width_seq_tensor_flat = width_seq_tensor.flat<int64_t>();\n\n    // Now walk through the process of fractional max pooling again.\n    // For both input and output,\n    // 0: batch\n    // 1: height / row\n    // 2: width / col\n    // 3: depth / channel\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // height sequence.\n      for (int64_t hs = 0; hs < height_seq_tensor.dim_size(0) - 1; ++hs) {\n        // height start and end.\n        const int64_t height_start = height_seq_tensor_flat(hs);\n        int64_t height_end = overlapping_ ? height_seq_tensor_flat(hs + 1)\n                                          : height_seq_tensor_flat(hs + 1) - 1;\n        height_end = std::min(height_end, height_max);\n\n        // width sequence.\n        for (int64_t ws = 0; ws < width_seq_tensor.dim_size(0) - 1; ++ws) {\n          const int64_t out_index =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // width start and end.\n          const int64_t width_start = width_seq_tensor_flat(ws);\n          int64_t width_end = overlapping_ ? width_seq_tensor_flat(ws + 1)\n                                           : width_seq_tensor_flat(ws + 1) - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_index =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              // Walk through each channel (depth).\n              for (int64_t d = 0; d < input_size[3]; ++d) {\n                const T& input_ref = tensor_in_mat.coeffRef(d, in_index);\n                T& output_ref = tensor_out_dup_mat.coeffRef(d, out_index);\n                int64_t& out_arg_max_ref =\n                    tensor_out_arg_max_mat.coeffRef(d, out_index);\n                if (output_ref < input_ref ||\n                    out_arg_max_ref == kInvalidMaxPoolingIndex) {\n                  output_ref = input_ref;\n                  int input_offset = in_index * input_size[3] + d;\n                  out_arg_max_ref = input_offset;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // Check tensor_out_dup is the same as tensor_out.\n    ConstEigenMatrixMap tensor_out_mat(\n        tensor_out.flat<T>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n    const int64_t num_reshaped_cols =\n        output_size[2] * output_size[1] * output_size[0];\n    for (int64_t i = 0; i < num_reshaped_cols; ++i) {\n      for (int64_t j = 0; j < output_size[3]; ++j) {\n        DCHECK_EQ(tensor_out_dup_mat(j, i), tensor_out_mat(j, i));\n      }\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, tensor_in.shape(), &output));\n    output->flat<T>().setZero();\n\n    auto out_backprop_flat = out_backprop.flat<T>();\n    auto input_backprop_flat = output->flat<T>();\n    auto out_arg_max_flat = tensor_out_arg_max.flat<int64_t>();\n    int num_total_outputs = out_backprop_flat.size();\n    int num_total_inputs = input_backprop_flat.size();\n\n    for (int index = 0; index < num_total_outputs; ++index) {\n      int input_backprop_index = out_arg_max_flat(index);\n      // According to maxpooling_op.cc, the performance impact below is small.\n      CHECK(input_backprop_index >= 0 &&\n            input_backprop_index < num_total_inputs)\n          << \"Invalid input backprop index: \" << input_backprop_index << \", \"\n          << num_total_inputs;\n      input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n    }\n  }\n\n private:\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALMAXPOOLGRAD(type)              \\\n  REGISTER_KERNEL_BUILDER(Name(\"FractionalMaxPoolGrad\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          FractionalMaxPoolGradOp<type>)\n\nREGISTER_FRACTIONALMAXPOOLGRAD(int32);\nREGISTER_FRACTIONALMAXPOOLGRAD(int64_t);\nREGISTER_FRACTIONALMAXPOOLGRAD(float);\nREGISTER_FRACTIONALMAXPOOLGRAD(double);\n\n#undef REGISTER_FRACTIONALMAXPOOLGRAD\n}  // namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for fractional max pool operation.\"\"\"\n\nimport math\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\nclass FractionalMaxPoolTest(test.TestCase):\n\n  # Random number generate with seed.\n  _PRNG = np.random.RandomState(341261)\n  _SEED = 123456\n\n  def _MaxPoolAlongRows(self, input_matrix, row_seq, overlapping):\n    \"\"\"Perform max pool along row of a 2-D matrix based on row_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      row_seq: Cumulative pooling sequence along row.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = len(row_seq)-1\n        * num_cols = input_matrix.num_cols.\n    \"\"\"\n    output_image = np.zeros(input_matrix.shape[1])\n    row_max = row_seq[-1]\n    for i in range(row_seq.shape[0] - 1):\n      row_start = row_seq[i]\n      row_end = row_seq[i + 1] + 1 if overlapping else row_seq[i + 1]\n      row_end = min(row_end, row_max)\n      output_image = np.vstack((output_image, np.amax(\n          input_matrix[row_start:row_end, :], axis=0)))  # axis 0 is along row\n    # remove the sentinel row\n    return output_image[1:, :]\n\n  def _MaxPoolAlongCols(self, input_matrix, col_seq, overlapping):\n    \"\"\"Perform max pool along column of a 2-D matrix based on col_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = input_matrix.num_rows\n        * num_cols = len(col_seq)-1.\n    \"\"\"\n    input_matrix = input_matrix.transpose()\n    output_matrix = self._MaxPoolAlongRows(input_matrix, col_seq, overlapping)\n    return output_matrix.transpose()\n\n  def _GetExpectedFractionalMaxPoolResult(self, input_tensor, row_seq, col_seq,\n                                          overlapping):\n    \"\"\"Get expected fractional max pool result.\n\n    row_seq and col_seq together defines the fractional pooling region.\n\n    Args:\n      input_tensor: Original input tensor, assuming it is a 4-D tensor, with\n        dimension as [batch, height/row, width/column, channels/depth].\n      row_seq: Cumulative pooling sequence along row.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Use overlapping when doing pooling.\n\n    Returns:\n      A 4-D tensor that is the result of max pooling on input_tensor based on\n        pooling region defined by row_seq and col_seq, conditioned on whether or\n        not overlapping is used.\n    \"\"\"\n    input_shape = input_tensor.shape\n    output_shape = (input_shape[0], len(row_seq) - 1, len(col_seq) - 1,\n                    input_shape[3])\n    output_tensor = np.zeros(shape=output_shape, dtype=input_tensor.dtype)\n    for batch in range(input_shape[0]):\n      for channel in range(input_shape[3]):\n        two_dim_slice = input_tensor[batch, :, :, channel]\n        tmp = self._MaxPoolAlongRows(two_dim_slice, row_seq, overlapping)\n        output_tensor[batch, :, :, channel] = self._MaxPoolAlongCols(\n            tmp, col_seq, overlapping)\n\n    return output_tensor\n\n  def _ValidateFractionalMaxPoolResult(self, input_tensor, pooling_ratio,\n                                       pseudo_random, overlapping):\n    \"\"\"Validate FractionalMaxPool's result against expected.\n\n    Expected result is computed given input_tensor, and pooling region defined\n    by row_seq and col_seq.\n\n    Args:\n      input_tensor: A tensor or numpy ndarray.\n      pooling_ratio: A list or tuple of length 4, first and last element be 1.\n      pseudo_random: Use pseudo random method to generate pooling sequence.\n      overlapping: Use overlapping when pooling.\n\n    Returns:\n      None\n    \"\"\"\n    with self.cached_session() as sess:\n      p, r, c = nn_ops.fractional_max_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      actual, row_seq, col_seq = self.evaluate([p, r, c])\n      expected = self._GetExpectedFractionalMaxPoolResult(input_tensor, row_seq,\n                                                          col_seq, overlapping)\n      self.assertShapeEqual(expected, p)\n      self.assertAllClose(expected, actual)\n\n  def _testVisually(self):\n    \"\"\"Manual test by printing out intermediate result of a small random tensor.\n\n    Since _GetExpectedFractionalMaxPoolResult is 'automated', it feel safer to\n    have a test case that you can see what's happening.\n    This test will generate a small, random, int 2D matrix, and feed it to\n    FractionalMaxPool and _GetExpectedFractionalMaxPoolResult.\n    \"\"\"\n    num_rows = 6\n    num_cols = 6\n    tensor_shape = (1, num_rows, num_cols, 1)\n    pseudo_random = False\n    for overlapping in True, False:\n      print(\"-\" * 70)\n      print(\"Testing FractionalMaxPool with overlapping = {}\".format(\n          overlapping))\n      rand_mat = self._PRNG.randint(10, size=tensor_shape)\n      pooling_ratio = [1, math.sqrt(2), math.sqrt(2), 1]\n      with self.cached_session() as sess:\n        p, r, c = nn_ops.fractional_max_pool_v2(\n            rand_mat,\n            pooling_ratio,\n            pseudo_random,\n            overlapping,\n            seed=self._SEED)\n        tensor_output, row_seq, col_seq = self.evaluate([p, r, c])\n        expected_result = self._GetExpectedFractionalMaxPoolResult(rand_mat,\n                                                                   row_seq,\n                                                                   col_seq,\n                                                                   overlapping)\n        print(\"row sequence:\")\n        print(row_seq)\n        print(\"column sequence:\")\n        print(col_seq)\n\n        print(\"Input:\")\n        # Print input with pooling region marked.\n        for i in range(num_rows):\n          row_to_print = []\n          for j in range(num_cols):\n            if j in col_seq:\n              row_to_print.append(\"|\")\n            row_to_print.append(str(rand_mat[0, i, j, 0]))\n          row_to_print.append(\"|\")\n          if i in row_seq:\n            print(\"-\" * 2 * len(row_to_print))\n          print(\" \".join(row_to_print))\n        print(\"-\" * 2 * len(row_to_print))\n\n        print(\"Output from FractionalMaxPool:\")\n        print(tensor_output[0, :, :, 0])\n        print(\"Expected result:\")\n        print(expected_result[0, :, :, 0])\n\n  def testAllInputOptions(self):\n    \"\"\"Try all possible input options for fractional_max_pool.\n    \"\"\"\n    num_batches = 5\n    num_channels = 3\n    num_rows = 20\n    num_cols = 30\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalMaxPoolResult(\n            rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n            overlapping)\n\n  def testIntegerTensorInput(self):\n    \"\"\"Test it works fine when input tensor is integer type.\n    \"\"\"\n    num_batches = 5\n    num_channels = 3\n    num_rows = 20\n    num_cols = 30\n    pseudo_random = True\n    overlapping = True\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    rand_mat = self._PRNG.randint(1000, size=tensor_shape)\n    self._ValidateFractionalMaxPoolResult(rand_mat,\n                                          [1, math.sqrt(3), math.sqrt(2), 1],\n                                          pseudo_random, overlapping)\n\n  def testDifferentTensorShapes(self):\n    \"\"\"Test different shapes of input tensor.\n\n    Mainly test different combinations of num_rows and num_cols.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    for num_batches in [1, 3]:\n      for num_channels in [1, 3]:\n        for num_rows in [10, 20, 50]:\n          for num_cols in [10, 20, 50]:\n            tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n            # random tensor with value in [-500.0, 500.0)\n            rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n            self._ValidateFractionalMaxPoolResult(\n                rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n                overlapping)\n\n  def testLargePoolingRatio(self):\n    \"\"\"Test when pooling ratio is not within [1, 2).\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    for row_ratio in [math.sqrt(11), math.sqrt(37)]:\n      for col_ratio in [math.sqrt(11), math.sqrt(27)]:\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalMaxPoolResult(rand_mat,\n                                              [1, row_ratio, col_ratio, 1],\n                                              pseudo_random, overlapping)\n\n  def testDivisiblePoolingRatio(self):\n    \"\"\"Test when num of rows/cols can evenly divide pooling ratio.\n\n    This is a case regular max pooling can handle. Should be handled by\n    fractional pooling as well.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    # random tensor with value in [-500.0, 500.0)\n    rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n    self._ValidateFractionalMaxPoolResult(rand_mat, [1, 2, 2, 1], pseudo_random,\n                                          overlapping)\n\n  @test_util.run_deprecated_v1\n  def testDifferentInputTensorShape(self):\n    \"\"\"Runs the operation in one session with different input tensor shapes.\"\"\"\n    with self.cached_session() as sess:\n      input_holder = array_ops.placeholder(dtypes.float32,\n                                           [None, None, None, 3])\n      pooling_ratio = [1, 1.5, 1.5, 1]\n      pseudo_random = False\n      overlapping = False\n      p, r, c = nn_ops.fractional_max_pool_v2(\n          input_holder,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      # First run.\n      input_a = np.zeros([3, 32, 32, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_a})\n      expected = self._GetExpectedFractionalMaxPoolResult(\n          input_a, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n      # Second run.\n      input_b = np.zeros([4, 45, 45, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_b})\n      expected = self._GetExpectedFractionalMaxPoolResult(\n          input_b, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n\n  def testDeterminismExceptionThrowing(self):\n    tensor_shape = (5, 20, 20, 3)\n    rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n    with test_util.deterministic_ops():\n      with self.assertRaisesRegex(\n          ValueError, \"requires a non-zero seed to be passed in when \"\n          \"determinism is enabled\"):\n        nn_ops.fractional_max_pool_v2(rand_mat, [1, 1.5, 1.5, 1])\n      nn_ops.fractional_max_pool_v2(rand_mat, [1, 1.5, 1.5, 1], seed=1)\n\n      with self.assertRaisesRegex(ValueError,\n                                  'requires \"seed\" and \"seed2\" to be non-zero'):\n        nn_ops.fractional_max_pool(rand_mat, [1, 1.5, 1.5, 1])\n      nn_ops.fractional_max_pool(\n          rand_mat, [1, 1.5, 1.5, 1], seed=1, seed2=1, deterministic=True)\n\n  def testPoolingRatio(self):\n    with self.cached_session() as _:\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"Pooling ratio is higher than input dimension size for dimension 1.*\"\n      ):\n        result = nn_ops.gen_nn_ops.fractional_max_pool(\n            value=constant_op.constant(\n                value=[[[[1, 4, 2, 3]]]], dtype=dtypes.int64),\n            pooling_ratio=[1.0, 1.44, 1.73, 1.0],\n            pseudo_random=False,\n            overlapping=False,\n            deterministic=False,\n            seed=0,\n            seed2=0,\n            name=None)\n        self.evaluate(result)\n\n\nclass FractionalMaxPoolGradTest(test.TestCase):\n  \"\"\"Tests for FractionalMaxPoolGrad.\n\n  Two types of tests for FractionalMaxPoolGrad.\n  1) Test fractional_max_pool_grad() directly.\n    This type of test relies on gen_nn_ops.max_pool_grad() returns the correct\n  result. For example:\n    * input_tensor_shape = (1, 10, 10, 1)\n    * window_size = (1, 2, 2, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not really import, since 10/2 is divisible\n  max pooling should generate the same result as fractional max pooling with:\n    * row_sequence = [0, 2, 4, 6, 8, 10]\n    * col_sequence = [0, 2, 4, 6, 8, 10]\n    * overlapping = False\n  This also means their gradients in such case will be the same.\n\n    Similarly, when\n    * input_tensor_shape = (1, 7, 7, 1)\n    * window_size = (1, 3, 3, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not important\n  max pooling should generate the same result as fractional max pooling with:\n    * row_sequence = [0, 2, 4, 7]\n    * col_sequence = [0, 2, 4, 7]\n    * overlapping = True\n  2) Test through compute_gradient_error()\n  \"\"\"\n\n  _PRNG = np.random.RandomState(341261)\n  _SEED = 123456\n\n  def _GenerateUniqueRandomInputTensor(self, shape):\n    \"\"\"Generate 'unique' random input tensor.\n\n    'Unique' means there's no collision values in the tensor, all elements are\n    different. This is done by generating sequence of integers with step of 1\n    and then randomly shuffle these integers.\n\n    Args:\n      shape: Shape of the tensor desired.\n\n    Returns:\n      A numpy ndarray with size = shape and dtype = numpy.float32.\n    \"\"\"\n    num_elements = 1\n    for size in shape:\n      num_elements *= size\n    x = np.arange(num_elements, dtype=np.float32)\n    self._PRNG.shuffle(x)\n    return x.reshape(shape)\n\n  def testDirectNotUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = row_window_size * 5\n          num_cols = col_window_size * 7\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateUniqueRandomInputTensor(input_shape))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size, col_window_size, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.max_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              output_backprop = self._PRNG.randint(100, size=output_data.shape)\n              input_backprop_tensor = gen_nn_ops.max_pool_grad(\n                  input_tensor, output_tensor, output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows + 1, row_window_size))\n              col_seq = list(range(0, num_cols + 1, col_window_size))\n              fmp_input_backprop_tensor = gen_nn_ops.fractional_max_pool_grad(\n                  input_tensor,\n                  output_tensor,\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=False)\n              fmp_input_backprop = self.evaluate(fmp_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fmp_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fmp_input_backprop)\n\n  def testDirectUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = (row_window_size - 1) * 5 + 1\n          num_cols = (col_window_size - 1) * 7 + 1\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateUniqueRandomInputTensor(input_shape))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size - 1, col_window_size - 1, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.max_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              output_backprop = self._PRNG.randint(100, size=output_data.shape)\n              input_backprop_tensor = gen_nn_ops.max_pool_grad(\n                  input_tensor, output_tensor, output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows, row_window_size - 1))\n              col_seq = list(range(0, num_cols, col_window_size - 1))\n              row_seq[-1] += 1\n              col_seq[-1] += 1\n              fmp_input_backprop_tensor = gen_nn_ops.fractional_max_pool_grad(\n                  input_tensor,\n                  output_tensor,\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=True)\n              fmp_input_backprop = self.evaluate(fmp_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fmp_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fmp_input_backprop)\n\n  @test_util.run_deprecated_v1\n  def testAllInputOptionsThroughGradientError(self):\n    input_shape = (1, 7, 13, 1)\n    input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n    # Add some randomness to make input_data not so 'integer'\n    input_data += self._PRNG.random_sample(input_shape)\n    pooling_ratio = [1, math.sqrt(2), math.sqrt(3), 1]\n\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        with self.cached_session() as _:\n          input_tensor = constant_op.constant(input_data, shape=input_shape)\n          output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n              input_tensor,\n              pooling_ratio,\n              pseudo_random=pseudo_random,\n              overlapping=overlapping,\n              seed=self._SEED)\n          output_data = self.evaluate(output_tensor)\n          output_shape = output_data.shape\n          # error_margin and delta setting is similar to max_pool_grad.\n          error_margin = 1e-3\n          gradient_error = gradient_checker.compute_gradient_error(\n              input_tensor,\n              input_shape,\n              output_tensor,\n              output_shape,\n              x_init_value=input_data.reshape(input_shape),\n              delta=1e-2)\n          self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testDifferentTensorShapesThroughGradientError(self):\n    pseudo_random = True\n    overlapping = True\n    pooling_ratio = [1, math.sqrt(3), math.sqrt(2), 1]\n    for num_batches in [1, 2]:\n      for num_rows in [5, 13]:\n        for num_cols in [5, 11]:\n          for num_channels in [1, 3]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n            # Add some randomness to make input_data not so 'integer'\n            input_data += self._PRNG.random_sample(input_shape)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(input_data, shape=input_shape)\n              output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n                  input_tensor,\n                  pooling_ratio,\n                  pseudo_random=pseudo_random,\n                  overlapping=overlapping,\n                  seed=self._SEED)\n              output_data = self.evaluate(output_tensor)\n              output_shape = output_data.shape\n              # error_margin and delta setting is similar to max_pool_grad.\n              error_margin = 1e-3\n              gradient_error = gradient_checker.compute_gradient_error(\n                  input_tensor,\n                  input_shape,\n                  output_tensor,\n                  output_shape,\n                  x_init_value=input_data.reshape(input_shape),\n                  delta=1e-2)\n              self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testLargePoolingRatioThroughGradientError(self):\n    input_shape = (1, 17, 23, 1)\n    input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n    # Add some randomness to make input_data not so 'integer'\n    input_data += self._PRNG.random_sample(input_shape)\n    pooling_ratio = (1, math.sqrt(13), math.sqrt(7), 1)\n    output_shape = [int(a / b) for a, b in zip(input_shape, pooling_ratio)]\n    overlapping = True\n    pseudo_random = False\n\n    with self.cached_session() as _:\n      input_tensor = constant_op.constant(input_data, shape=input_shape)\n      output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random=pseudo_random,\n          overlapping=overlapping,\n          seed=self._SEED)\n      # error_margin and delta setting is similar to max_pool_grad.\n      error_margin = 1e-3\n      gradient_error = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_shape,\n          output_tensor,\n          output_shape,\n          x_init_value=input_data.reshape(input_shape),\n          delta=1e-2)\n      self.assertLess(gradient_error, error_margin)\n\n  def testWhenRepeatedMaxValueInPoolingRegion(self):\n    \"\"\"Test when there's repeating value in pooling region.\n\n    There's no formal definition for what the gradient should be when there're\n    multiple max value within a pooling cell. Such as\n        | 1 5 |\n        | 5 3 |\n    The expected result depends heavily on implementation, if someone swap the\n    order of a nested for loop when walking through the tensor, result would be\n    very different.\n\n    The goal of this test is to alert when someone else change the\n    implementation. Current implementation scans row-by-row.\n    \"\"\"\n    input_data = [5.0, 4.0, 6.0, 7.0,\n                  3.0, 5.0, 9.0, 6.0,\n                  8.0, 8.0, 9.0, 5.0,\n                  7.0, 4.0, 0.0, 0.0]  # pyformat: disable\n    input_size = [1, 4, 4, 1]\n    output_backprop = [12.0, 15.0,\n                       17.0, -5.0,\n                       6.0, 21.0]  # pyformat: disable\n    row_seq = [0, 1, 3, 4]\n    col_seq = [0, 2, 4]\n    output_data_not_overlapping = [5.0, 7.0,\n                                   8.0, 9.0,\n                                   7.0, 0.0]  # pyformat: disable\n    output_data_overlapping = [9.0, 9.0,\n                               9.0, 9.0,\n                               7.0, 0.0]  # pyformat: disable\n    output_size = [1, 3, 2, 1]\n    expected_input_backprop_not_overlapping = np.reshape(\n        [12.0, 0.0, 0.0, 15.0,\n         0.0, 0.0, -5.0, 0.0,\n         17.0, 0.0, 0.0, 0.0,\n         6.0, 0.0, 21.0, 0.0],\n        input_size)  # pyformat: disable\n    expected_input_backprop_overlapping = np.reshape(\n        [0.0, 0.0, 0.0, 0.0,\n         0.0, 0.0, 39.0, 0.0,\n         0.0, 0.0, 0.0, 0.0,\n         6.0, 0.0, 21.0, 0.0],\n        input_size)  # pyformat: disable\n    with self.cached_session() as _:\n      # Test when overlapping is False\n      input_tensor = constant_op.constant(input_data, shape=input_size)\n      output_tensor = constant_op.constant(\n          output_data_not_overlapping, shape=output_size)\n      grad = constant_op.constant(output_backprop, shape=output_size)\n      r = gen_nn_ops.fractional_max_pool_grad(\n          input_tensor,\n          output_tensor,\n          grad,\n          row_seq,\n          col_seq,\n          overlapping=False)\n      input_backprop_not_overlapping = self.evaluate(r)\n      self.assertShapeEqual(\n          np.reshape(expected_input_backprop_not_overlapping, input_size), r)\n      self.assertAllClose(expected_input_backprop_not_overlapping,\n                          input_backprop_not_overlapping)\n      # Test when overlapping is True\n      output_tensor = constant_op.constant(\n          output_data_overlapping, shape=output_size)\n      r = gen_nn_ops.fractional_max_pool_grad(\n          input_tensor, output_tensor, grad, row_seq, col_seq, overlapping=True)\n      input_backprop_overlapping = self.evaluate(r)\n      self.assertShapeEqual(\n          np.reshape(expected_input_backprop_overlapping, input_size), r)\n      self.assertAllClose(expected_input_backprop_overlapping,\n                          input_backprop_overlapping)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#define EIGEN_USE_THREADS\n\n#include <algorithm>\n#include <cmath>\n#include <random>\n#include <vector>\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/numeric_op.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/op_requires.h\"\n#include \"tensorflow/core/kernels/fractional_pool_common.h\"\n#include \"tensorflow/core/lib/random/random.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/mutex.h\"\n#include \"tensorflow/core/util/guarded_philox_random.h\"\n\nnamespace tensorflow {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <typename T>\nclass FractionalMaxPoolOp : public OpKernel {\n public:\n  explicit FractionalMaxPoolOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"pooling_ratio\", &pooling_ratio_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"pseudo_random\", &pseudo_random_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n\n    OP_REQUIRES(context, pooling_ratio_.size() == 4,\n                errors::InvalidArgument(\"pooling_ratio field must \"\n                                        \"specify 4 dimensions\"));\n\n    OP_REQUIRES(\n        context, pooling_ratio_[0] == 1 || pooling_ratio_[3] == 1,\n        errors::Unimplemented(\"Fractional max pooling is not yet \"\n                              \"supported on the batch nor channel dimension.\"));\n\n    OP_REQUIRES_OK(context, context->GetAttr(\"deterministic\", &deterministic_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed\", &seed_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seed2\", &seed2_));\n    if (deterministic_) {\n      // If both seeds are not set when deterministic_ is true, force set seeds.\n      if ((seed_ == 0) && (seed2_ == 0)) {\n        seed_ = random::New64();\n        seed2_ = random::New64();\n      }\n    } else {\n      OP_REQUIRES(\n          context, (seed_ == 0) && (seed2_ == 0),\n          errors::InvalidArgument(\n              \"Both seed and seed2 should be 0 if deterministic is false.\"));\n    }\n  }\n\n  void Compute(OpKernelContext* context) override {\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n\n    constexpr int tensor_in_and_out_dims = 4;\n\n    const Tensor& tensor_in = context->input(0);\n    OP_REQUIRES(context, tensor_in.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n\n      OP_REQUIRES(\n          context, input_size[i] >= pooling_ratio_[i],\n          errors::InvalidArgument(\"Pooling ratio is higher than input \"\n                                  \"dimension size for dimension \",\n                                  i, \". Input dim size: \", input_size[i],\n                                  \" pooling ratio: \", pooling_ratio_[i]));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      // This must match the same logic in the shape function in\n      // core/ops/nn_ops.cc.\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));\n      DCHECK_GT(output_size[i], 0);\n    }\n\n    // Generate pooling sequence.\n    std::vector<int64_t> height_cum_seq;\n    std::vector<int64_t> width_cum_seq;\n    GuardedPhiloxRandom generator;\n    generator.Init(seed_, seed2_);\n    height_cum_seq = GeneratePoolingSequence(input_size[1], output_size[1],\n                                             &generator, pseudo_random_);\n    width_cum_seq = GeneratePoolingSequence(input_size[2], output_size[2],\n                                            &generator, pseudo_random_);\n\n    // Prepare output.\n    Tensor* output_tensor = nullptr;\n    OP_REQUIRES_OK(context, context->allocate_output(\n                                0,\n                                TensorShape({output_size[0], output_size[1],\n                                             output_size[2], output_size[3]}),\n                                &output_tensor));\n    Tensor* output_height_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            1, TensorShape({static_cast<int64_t>(height_cum_seq.size())}),\n            &output_height_seq_tensor));\n    Tensor* output_width_seq_tensor = nullptr;\n    OP_REQUIRES_OK(\n        context,\n        context->allocate_output(\n            2, TensorShape({static_cast<int64_t>(width_cum_seq.size())}),\n            &output_width_seq_tensor));\n\n    ConstEigenMatrixMap in_mat(tensor_in.flat<T>().data(), input_size[3],\n                               input_size[2] * input_size[1] * input_size[0]);\n\n    EigenMatrixMap out_mat(output_tensor->flat<T>().data(), output_size[3],\n                           output_size[2] * output_size[1] * output_size[0]);\n\n    // Initializes the output tensor with MIN<T>.\n    output_tensor->flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n\n    auto output_height_seq_flat = output_height_seq_tensor->flat<int64_t>();\n    auto output_width_seq_flat = output_width_seq_tensor->flat<int64_t>();\n\n    // Set output tensors.\n    for (int i = 0; i < height_cum_seq.size(); ++i) {\n      output_height_seq_flat(i) = height_cum_seq[i];\n    }\n\n    for (int i = 0; i < width_cum_seq.size(); ++i) {\n      output_width_seq_flat(i) = width_cum_seq[i];\n    }\n\n    // For both input and output,\n    // 0: batch\n    // 1: height / row\n    // 2: width / col\n    // 3: depth / channel\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // height sequence.\n      for (int64_t hs = 0; hs < height_cum_seq.size() - 1; ++hs) {\n        // height start and end.\n        const int64_t height_start = height_cum_seq[hs];\n        int64_t height_end =\n            overlapping_ ? height_cum_seq[hs + 1] : height_cum_seq[hs + 1] - 1;\n        height_end = std::min(height_end, height_max);\n\n        // width sequence.\n        for (int64_t ws = 0; ws < width_cum_seq.size() - 1; ++ws) {\n          const int64_t out_offset =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // width start and end.\n          const int64_t width_start = width_cum_seq[ws];\n          int64_t width_end =\n              overlapping_ ? width_cum_seq[ws + 1] : width_cum_seq[ws + 1] - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_offset =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              out_mat.col(out_offset) =\n                  out_mat.col(out_offset).cwiseMax(in_mat.col(in_offset));\n            }\n          }\n        }\n      }\n    }\n  }\n\n private:\n  bool deterministic_;\n  int64_t seed_;\n  int64_t seed2_;\n  std::vector<float> pooling_ratio_;\n  bool pseudo_random_;\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALMAXPOOL(type)                                      \\\n  REGISTER_KERNEL_BUILDER(                                                    \\\n      Name(\"FractionalMaxPool\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      FractionalMaxPoolOp<type>)\n\nREGISTER_FRACTIONALMAXPOOL(int32);\nREGISTER_FRACTIONALMAXPOOL(int64_t);\nREGISTER_FRACTIONALMAXPOOL(float);\nREGISTER_FRACTIONALMAXPOOL(double);\n\n#undef REGISTER_FRACTIONALMAXPOOL\n\nstatic const int kInvalidMaxPoolingIndex = -1;\n\ntemplate <class T>\nclass FractionalMaxPoolGradOp : public OpKernel {\n public:\n  explicit FractionalMaxPoolGradOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"overlapping\", &overlapping_));\n  }\n\n  void Compute(OpKernelContext* context) override {\n    // There are two steps when calculating gradient for FractionalMaxPool.\n    // 1) Walk through the process of calculating fractional pooling given\n    //    pooling region; however, in the process, keep track of where the max\n    //    element comes from. (arg_max)\n    // 2) Populate the value of out_backprop to where arg_max indicates. If\n    //    we support overlapping, it is likely to have multiple out_backprop[i]\n    //    propagates back to the same arg_max value.\n    typedef Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        ConstEigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenMatrixMap;\n    typedef Eigen::Map<Eigen::Matrix<int64, Eigen::Dynamic, Eigen::Dynamic>>\n        EigenIndexMatrixMap;\n\n    const Tensor& tensor_in = context->input(0);\n    const Tensor& tensor_out = context->input(1);\n    const Tensor& out_backprop = context->input(2);\n    const Tensor& height_seq_tensor = context->input(3);\n    const Tensor& width_seq_tensor = context->input(4);\n\n    // Just to make it similar to FractionalMaxPoolOp.\n    constexpr int tensor_in_and_out_dims = 4;\n    OP_REQUIRES(\n        context, tensor_in.dims() == tensor_in_and_out_dims,\n        errors::InvalidArgument(\"orig_input should be a tensor of rank 4, got \",\n                                tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n                errors::InvalidArgument(\"orig_input must not be empty, got \",\n                                        tensor_in.DebugString()));\n    OP_REQUIRES(context, tensor_out.dims() == tensor_in_and_out_dims,\n                errors::InvalidArgument(\n                    \"orig_output should be a tensor of rank 4, got \",\n                    tensor_out.DebugString()));\n    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n                errors::InvalidArgument(\"orig_output must not be empty, got \",\n                                        tensor_out.DebugString()));\n    std::vector<int64_t> input_size(tensor_in_and_out_dims);\n    std::vector<int64_t> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n    }\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] = tensor_out.dim_size(i);\n    }\n\n    // ---------\n    // Step 1\n    // ---------\n    Tensor tensor_out_dup;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_temp(\n                                {1}, DataTypeToEnum<T>::v(), tensor_out.shape(),\n                                &tensor_out_dup));\n    Tensor tensor_out_arg_max;\n    OP_REQUIRES_OK(context, context->allocate_temp(DataTypeToEnum<int64_t>::v(),\n                                                   tensor_out.shape(),\n                                                   &tensor_out_arg_max));\n    // Find arg_max for each tensor_out\n    ConstEigenMatrixMap tensor_in_mat(\n        tensor_in.flat<T>().data(), input_size[3],\n        input_size[2] * input_size[1] * input_size[0]);\n    EigenMatrixMap tensor_out_dup_mat(\n        tensor_out_dup.flat<T>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n    EigenIndexMatrixMap tensor_out_arg_max_mat(\n        tensor_out_arg_max.flat<int64_t>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n\n    tensor_out_arg_max.flat<int64_t>().setConstant(kInvalidMaxPoolingIndex);\n    // Initializes the duplicate output tensor with MIN<T>.\n    tensor_out_dup.flat<T>().setConstant(Eigen::NumTraits<T>::lowest());\n\n    auto height_seq_tensor_flat = height_seq_tensor.flat<int64_t>();\n    auto width_seq_tensor_flat = width_seq_tensor.flat<int64_t>();\n\n    // Now walk through the process of fractional max pooling again.\n    // For both input and output,\n    // 0: batch\n    // 1: height / row\n    // 2: width / col\n    // 3: depth / channel\n    const int64_t height_max = input_size[1] - 1;\n    const int64_t width_max = input_size[2] - 1;\n    for (int64_t b = 0; b < input_size[0]; ++b) {\n      // height sequence.\n      for (int64_t hs = 0; hs < height_seq_tensor.dim_size(0) - 1; ++hs) {\n        // height start and end.\n        const int64_t height_start = height_seq_tensor_flat(hs);\n        int64_t height_end = overlapping_ ? height_seq_tensor_flat(hs + 1)\n                                          : height_seq_tensor_flat(hs + 1) - 1;\n        height_end = std::min(height_end, height_max);\n\n        // width sequence.\n        for (int64_t ws = 0; ws < width_seq_tensor.dim_size(0) - 1; ++ws) {\n          const int64_t out_index =\n              (b * output_size[1] + hs) * output_size[2] + ws;\n          // width start and end.\n          const int64_t width_start = width_seq_tensor_flat(ws);\n          int64_t width_end = overlapping_ ? width_seq_tensor_flat(ws + 1)\n                                           : width_seq_tensor_flat(ws + 1) - 1;\n          width_end = std::min(width_end, width_max);\n          for (int64_t h = height_start; h <= height_end; ++h) {\n            for (int64_t w = width_start; w <= width_end; ++w) {\n              const int64_t in_index =\n                  (b * input_size[1] + h) * input_size[2] + w;\n              // Walk through each channel (depth).\n              for (int64_t d = 0; d < input_size[3]; ++d) {\n                const T& input_ref = tensor_in_mat.coeffRef(d, in_index);\n                T& output_ref = tensor_out_dup_mat.coeffRef(d, out_index);\n                int64_t& out_arg_max_ref =\n                    tensor_out_arg_max_mat.coeffRef(d, out_index);\n                if (output_ref < input_ref ||\n                    out_arg_max_ref == kInvalidMaxPoolingIndex) {\n                  output_ref = input_ref;\n                  int input_offset = in_index * input_size[3] + d;\n                  out_arg_max_ref = input_offset;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n\n    // Check tensor_out_dup is the same as tensor_out.\n    ConstEigenMatrixMap tensor_out_mat(\n        tensor_out.flat<T>().data(), output_size[3],\n        output_size[2] * output_size[1] * output_size[0]);\n    const int64_t num_reshaped_cols =\n        output_size[2] * output_size[1] * output_size[0];\n    for (int64_t i = 0; i < num_reshaped_cols; ++i) {\n      for (int64_t j = 0; j < output_size[3]; ++j) {\n        OP_REQUIRES(context, tensor_out_dup_mat(j, i) == tensor_out_mat(j, i),\n                    errors::InvalidArgument(\n                        \"tensor_out_dup is not the same as tensor_out\"));\n      }\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 0, tensor_in.shape(), &output));\n    output->flat<T>().setZero();\n\n    auto out_backprop_flat = out_backprop.flat<T>();\n    auto input_backprop_flat = output->flat<T>();\n    auto out_arg_max_flat = tensor_out_arg_max.flat<int64_t>();\n    int num_total_outputs = out_backprop_flat.size();\n    int num_total_inputs = input_backprop_flat.size();\n\n    for (int index = 0; index < num_total_outputs; ++index) {\n      int input_backprop_index = out_arg_max_flat(index);\n      OP_REQUIRES(\n          context,\n          input_backprop_index >= 0 && input_backprop_index < num_total_inputs,\n          errors::InvalidArgument(\n              \"Invalid input backprop index: \", input_backprop_index, \", \",\n              num_total_inputs));\n      input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n    }\n  }\n\n private:\n  bool overlapping_;\n};\n\n#define REGISTER_FRACTIONALMAXPOOLGRAD(type)              \\\n  REGISTER_KERNEL_BUILDER(Name(\"FractionalMaxPoolGrad\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          FractionalMaxPoolGradOp<type>)\n\nREGISTER_FRACTIONALMAXPOOLGRAD(int32);\nREGISTER_FRACTIONALMAXPOOLGRAD(int64_t);\nREGISTER_FRACTIONALMAXPOOLGRAD(float);\nREGISTER_FRACTIONALMAXPOOLGRAD(double);\n\n#undef REGISTER_FRACTIONALMAXPOOLGRAD\n}  // namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for fractional max pool operation.\"\"\"\n\nimport math\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\nclass FractionalMaxPoolTest(test.TestCase):\n\n  # Random number generate with seed.\n  _PRNG = np.random.RandomState(341261)\n  _SEED = 123456\n\n  def _MaxPoolAlongRows(self, input_matrix, row_seq, overlapping):\n    \"\"\"Perform max pool along row of a 2-D matrix based on row_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      row_seq: Cumulative pooling sequence along row.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = len(row_seq)-1\n        * num_cols = input_matrix.num_cols.\n    \"\"\"\n    output_image = np.zeros(input_matrix.shape[1])\n    row_max = row_seq[-1]\n    for i in range(row_seq.shape[0] - 1):\n      row_start = row_seq[i]\n      row_end = row_seq[i + 1] + 1 if overlapping else row_seq[i + 1]\n      row_end = min(row_end, row_max)\n      output_image = np.vstack((output_image, np.amax(\n          input_matrix[row_start:row_end, :], axis=0)))  # axis 0 is along row\n    # remove the sentinel row\n    return output_image[1:, :]\n\n  def _MaxPoolAlongCols(self, input_matrix, col_seq, overlapping):\n    \"\"\"Perform max pool along column of a 2-D matrix based on col_seq.\n\n    Args:\n      input_matrix: A 2-D matrix.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Whether or not use overlapping when pooling.\n\n    Returns:\n      A 2-D matrix, with\n        * num_rows = input_matrix.num_rows\n        * num_cols = len(col_seq)-1.\n    \"\"\"\n    input_matrix = input_matrix.transpose()\n    output_matrix = self._MaxPoolAlongRows(input_matrix, col_seq, overlapping)\n    return output_matrix.transpose()\n\n  def _GetExpectedFractionalMaxPoolResult(self, input_tensor, row_seq, col_seq,\n                                          overlapping):\n    \"\"\"Get expected fractional max pool result.\n\n    row_seq and col_seq together defines the fractional pooling region.\n\n    Args:\n      input_tensor: Original input tensor, assuming it is a 4-D tensor, with\n        dimension as [batch, height/row, width/column, channels/depth].\n      row_seq: Cumulative pooling sequence along row.\n      col_seq: Cumulative pooling sequence along column.\n      overlapping: Use overlapping when doing pooling.\n\n    Returns:\n      A 4-D tensor that is the result of max pooling on input_tensor based on\n        pooling region defined by row_seq and col_seq, conditioned on whether or\n        not overlapping is used.\n    \"\"\"\n    input_shape = input_tensor.shape\n    output_shape = (input_shape[0], len(row_seq) - 1, len(col_seq) - 1,\n                    input_shape[3])\n    output_tensor = np.zeros(shape=output_shape, dtype=input_tensor.dtype)\n    for batch in range(input_shape[0]):\n      for channel in range(input_shape[3]):\n        two_dim_slice = input_tensor[batch, :, :, channel]\n        tmp = self._MaxPoolAlongRows(two_dim_slice, row_seq, overlapping)\n        output_tensor[batch, :, :, channel] = self._MaxPoolAlongCols(\n            tmp, col_seq, overlapping)\n\n    return output_tensor\n\n  def _ValidateFractionalMaxPoolResult(self, input_tensor, pooling_ratio,\n                                       pseudo_random, overlapping):\n    \"\"\"Validate FractionalMaxPool's result against expected.\n\n    Expected result is computed given input_tensor, and pooling region defined\n    by row_seq and col_seq.\n\n    Args:\n      input_tensor: A tensor or numpy ndarray.\n      pooling_ratio: A list or tuple of length 4, first and last element be 1.\n      pseudo_random: Use pseudo random method to generate pooling sequence.\n      overlapping: Use overlapping when pooling.\n\n    Returns:\n      None\n    \"\"\"\n    with self.cached_session():\n      p, r, c = nn_ops.fractional_max_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      actual, row_seq, col_seq = self.evaluate([p, r, c])\n      expected = self._GetExpectedFractionalMaxPoolResult(input_tensor, row_seq,\n                                                          col_seq, overlapping)\n      self.assertShapeEqual(expected, p)\n      self.assertAllClose(expected, actual)\n\n  def _testVisually(self):\n    \"\"\"Manual test by printing out intermediate result of a small random tensor.\n\n    Since _GetExpectedFractionalMaxPoolResult is 'automated', it feel safer to\n    have a test case that you can see what's happening.\n    This test will generate a small, random, int 2D matrix, and feed it to\n    FractionalMaxPool and _GetExpectedFractionalMaxPoolResult.\n    \"\"\"\n    num_rows = 6\n    num_cols = 6\n    tensor_shape = (1, num_rows, num_cols, 1)\n    pseudo_random = False\n    for overlapping in True, False:\n      print(\"-\" * 70)\n      print(\"Testing FractionalMaxPool with overlapping = {}\".format(\n          overlapping))\n      rand_mat = self._PRNG.randint(10, size=tensor_shape)\n      pooling_ratio = [1, math.sqrt(2), math.sqrt(2), 1]\n      with self.cached_session():\n        p, r, c = nn_ops.fractional_max_pool_v2(\n            rand_mat,\n            pooling_ratio,\n            pseudo_random,\n            overlapping,\n            seed=self._SEED)\n        tensor_output, row_seq, col_seq = self.evaluate([p, r, c])\n        expected_result = self._GetExpectedFractionalMaxPoolResult(rand_mat,\n                                                                   row_seq,\n                                                                   col_seq,\n                                                                   overlapping)\n        print(\"row sequence:\")\n        print(row_seq)\n        print(\"column sequence:\")\n        print(col_seq)\n\n        print(\"Input:\")\n        # Print input with pooling region marked.\n        for i in range(num_rows):\n          row_to_print = []\n          for j in range(num_cols):\n            if j in col_seq:\n              row_to_print.append(\"|\")\n            row_to_print.append(str(rand_mat[0, i, j, 0]))\n          row_to_print.append(\"|\")\n          if i in row_seq:\n            print(\"-\" * 2 * len(row_to_print))\n          print(\" \".join(row_to_print))\n        print(\"-\" * 2 * len(row_to_print))\n\n        print(\"Output from FractionalMaxPool:\")\n        print(tensor_output[0, :, :, 0])\n        print(\"Expected result:\")\n        print(expected_result[0, :, :, 0])\n\n  def testAllInputOptions(self):\n    \"\"\"Try all possible input options for fractional_max_pool.\n    \"\"\"\n    num_batches = 5\n    num_channels = 3\n    num_rows = 20\n    num_cols = 30\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalMaxPoolResult(\n            rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n            overlapping)\n\n  def testIntegerTensorInput(self):\n    \"\"\"Test it works fine when input tensor is integer type.\n    \"\"\"\n    num_batches = 5\n    num_channels = 3\n    num_rows = 20\n    num_cols = 30\n    pseudo_random = True\n    overlapping = True\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    rand_mat = self._PRNG.randint(1000, size=tensor_shape)\n    self._ValidateFractionalMaxPoolResult(rand_mat,\n                                          [1, math.sqrt(3), math.sqrt(2), 1],\n                                          pseudo_random, overlapping)\n\n  def testDifferentTensorShapes(self):\n    \"\"\"Test different shapes of input tensor.\n\n    Mainly test different combinations of num_rows and num_cols.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    for num_batches in [1, 3]:\n      for num_channels in [1, 3]:\n        for num_rows in [10, 20, 50]:\n          for num_cols in [10, 20, 50]:\n            tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n            # random tensor with value in [-500.0, 500.0)\n            rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n            self._ValidateFractionalMaxPoolResult(\n                rand_mat, [1, math.sqrt(3), math.sqrt(2), 1], pseudo_random,\n                overlapping)\n\n  def testLargePoolingRatio(self):\n    \"\"\"Test when pooling ratio is not within [1, 2).\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    for row_ratio in [math.sqrt(11), math.sqrt(37)]:\n      for col_ratio in [math.sqrt(11), math.sqrt(27)]:\n        # random tensor with value in [-500.0, 500.0)\n        rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n        self._ValidateFractionalMaxPoolResult(rand_mat,\n                                              [1, row_ratio, col_ratio, 1],\n                                              pseudo_random, overlapping)\n\n  def testDivisiblePoolingRatio(self):\n    \"\"\"Test when num of rows/cols can evenly divide pooling ratio.\n\n    This is a case regular max pooling can handle. Should be handled by\n    fractional pooling as well.\n    \"\"\"\n    pseudo_random = True\n    overlapping = True\n    num_batches = 3\n    num_channels = 3\n    num_rows = 30\n    num_cols = 50\n    tensor_shape = (num_batches, num_rows, num_cols, num_channels)\n    # random tensor with value in [-500.0, 500.0)\n    rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n    self._ValidateFractionalMaxPoolResult(rand_mat, [1, 2, 2, 1], pseudo_random,\n                                          overlapping)\n\n  @test_util.run_deprecated_v1\n  def testDifferentInputTensorShape(self):\n    \"\"\"Runs the operation in one session with different input tensor shapes.\"\"\"\n    with self.cached_session() as sess:\n      input_holder = array_ops.placeholder(dtypes.float32,\n                                           [None, None, None, 3])\n      pooling_ratio = [1, 1.5, 1.5, 1]\n      pseudo_random = False\n      overlapping = False\n      p, r, c = nn_ops.fractional_max_pool_v2(\n          input_holder,\n          pooling_ratio,\n          pseudo_random,\n          overlapping,\n          seed=self._SEED)\n      # First run.\n      input_a = np.zeros([3, 32, 32, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_a})\n      expected = self._GetExpectedFractionalMaxPoolResult(\n          input_a, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n      # Second run.\n      input_b = np.zeros([4, 45, 45, 3])\n      actual, row_seq, col_seq = sess.run([p, r, c], {input_holder: input_b})\n      expected = self._GetExpectedFractionalMaxPoolResult(\n          input_b, row_seq, col_seq, overlapping)\n      self.assertSequenceEqual(expected.shape, actual.shape)\n\n  def testDeterminismExceptionThrowing(self):\n    tensor_shape = (5, 20, 20, 3)\n    rand_mat = self._PRNG.random_sample(tensor_shape) * 1000 - 500\n    with test_util.deterministic_ops():\n      with self.assertRaisesRegex(\n          ValueError, \"requires a non-zero seed to be passed in when \"\n          \"determinism is enabled\"):\n        nn_ops.fractional_max_pool_v2(rand_mat, [1, 1.5, 1.5, 1])\n      nn_ops.fractional_max_pool_v2(rand_mat, [1, 1.5, 1.5, 1], seed=1)\n\n      with self.assertRaisesRegex(ValueError,\n                                  'requires \"seed\" and \"seed2\" to be non-zero'):\n        nn_ops.fractional_max_pool(rand_mat, [1, 1.5, 1.5, 1])\n      nn_ops.fractional_max_pool(\n          rand_mat, [1, 1.5, 1.5, 1], seed=1, seed2=1, deterministic=True)\n\n  def testPoolingRatio(self):\n    with self.cached_session() as _:\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          r\"Pooling ratio is higher than input dimension size for dimension 1.*\"\n      ):\n        result = nn_ops.gen_nn_ops.fractional_max_pool(\n            value=constant_op.constant(\n                value=[[[[1, 4, 2, 3]]]], dtype=dtypes.int64),\n            pooling_ratio=[1.0, 1.44, 1.73, 1.0],\n            pseudo_random=False,\n            overlapping=False,\n            deterministic=False,\n            seed=0,\n            seed2=0,\n            name=None)\n        self.evaluate(result)\n\n\nclass FractionalMaxPoolGradTest(test.TestCase):\n  \"\"\"Tests for FractionalMaxPoolGrad.\n\n  Two types of tests for FractionalMaxPoolGrad.\n  1) Test fractional_max_pool_grad() directly.\n    This type of test relies on gen_nn_ops.max_pool_grad() returns the correct\n  result. For example:\n    * input_tensor_shape = (1, 10, 10, 1)\n    * window_size = (1, 2, 2, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not really import, since 10/2 is divisible\n  max pooling should generate the same result as fractional max pooling with:\n    * row_sequence = [0, 2, 4, 6, 8, 10]\n    * col_sequence = [0, 2, 4, 6, 8, 10]\n    * overlapping = False\n  This also means their gradients in such case will be the same.\n\n    Similarly, when\n    * input_tensor_shape = (1, 7, 7, 1)\n    * window_size = (1, 3, 3, 1)\n    * stride_size = (1, 2, 2, 1)\n    * padding: not important\n  max pooling should generate the same result as fractional max pooling with:\n    * row_sequence = [0, 2, 4, 7]\n    * col_sequence = [0, 2, 4, 7]\n    * overlapping = True\n  2) Test through compute_gradient_error()\n  \"\"\"\n\n  _PRNG = np.random.RandomState(341261)\n  _SEED = 123456\n\n  def _GenerateUniqueRandomInputTensor(self, shape):\n    \"\"\"Generate 'unique' random input tensor.\n\n    'Unique' means there's no collision values in the tensor, all elements are\n    different. This is done by generating sequence of integers with step of 1\n    and then randomly shuffle these integers.\n\n    Args:\n      shape: Shape of the tensor desired.\n\n    Returns:\n      A numpy ndarray with size = shape and dtype = numpy.float32.\n    \"\"\"\n    num_elements = 1\n    for size in shape:\n      num_elements *= size\n    x = np.arange(num_elements, dtype=np.float32)\n    self._PRNG.shuffle(x)\n    return x.reshape(shape)\n\n  def testDirectNotUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = row_window_size * 5\n          num_cols = col_window_size * 7\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateUniqueRandomInputTensor(input_shape))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size, col_window_size, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.max_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              output_backprop = self._PRNG.randint(100, size=output_data.shape)\n              input_backprop_tensor = gen_nn_ops.max_pool_grad(\n                  input_tensor, output_tensor, output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows + 1, row_window_size))\n              col_seq = list(range(0, num_cols + 1, col_window_size))\n              fmp_input_backprop_tensor = gen_nn_ops.fractional_max_pool_grad(\n                  input_tensor,\n                  output_tensor,\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=False)\n              fmp_input_backprop = self.evaluate(fmp_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fmp_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fmp_input_backprop)\n\n  def testDirectUseOverlapping(self):\n    for num_batches in [1, 3]:\n      for row_window_size in [2, 5]:\n        for col_window_size in [2, 4]:\n          num_rows = (row_window_size - 1) * 5 + 1\n          num_cols = (col_window_size - 1) * 7 + 1\n          for num_channels in [1, 2]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(\n                  self._GenerateUniqueRandomInputTensor(input_shape))\n              window_size = [1, row_window_size, col_window_size, 1]\n              stride_size = [1, row_window_size - 1, col_window_size - 1, 1]\n              padding = \"VALID\"\n              output_tensor = nn_ops.max_pool(input_tensor, window_size,\n                                              stride_size, padding)\n              output_data = self.evaluate(output_tensor)\n              output_backprop = self._PRNG.randint(100, size=output_data.shape)\n              input_backprop_tensor = gen_nn_ops.max_pool_grad(\n                  input_tensor, output_tensor, output_backprop, window_size,\n                  stride_size, padding)\n              input_backprop = self.evaluate(input_backprop_tensor)\n              row_seq = list(range(0, num_rows, row_window_size - 1))\n              col_seq = list(range(0, num_cols, col_window_size - 1))\n              row_seq[-1] += 1\n              col_seq[-1] += 1\n              fmp_input_backprop_tensor = gen_nn_ops.fractional_max_pool_grad(\n                  input_tensor,\n                  output_tensor,\n                  output_backprop,\n                  row_seq,\n                  col_seq,\n                  overlapping=True)\n              fmp_input_backprop = self.evaluate(fmp_input_backprop_tensor)\n              self.assertShapeEqual(input_backprop, fmp_input_backprop_tensor)\n              self.assertAllClose(input_backprop, fmp_input_backprop)\n\n  @test_util.run_deprecated_v1\n  def testAllInputOptionsThroughGradientError(self):\n    input_shape = (1, 7, 13, 1)\n    input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n    # Add some randomness to make input_data not so 'integer'\n    input_data += self._PRNG.random_sample(input_shape)\n    pooling_ratio = [1, math.sqrt(2), math.sqrt(3), 1]\n\n    for pseudo_random in True, False:\n      for overlapping in True, False:\n        with self.cached_session() as _:\n          input_tensor = constant_op.constant(input_data, shape=input_shape)\n          output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n              input_tensor,\n              pooling_ratio,\n              pseudo_random=pseudo_random,\n              overlapping=overlapping,\n              seed=self._SEED)\n          output_data = self.evaluate(output_tensor)\n          output_shape = output_data.shape\n          # error_margin and delta setting is similar to max_pool_grad.\n          error_margin = 1e-3\n          gradient_error = gradient_checker.compute_gradient_error(\n              input_tensor,\n              input_shape,\n              output_tensor,\n              output_shape,\n              x_init_value=input_data.reshape(input_shape),\n              delta=1e-2)\n          self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testDifferentTensorShapesThroughGradientError(self):\n    pseudo_random = True\n    overlapping = True\n    pooling_ratio = [1, math.sqrt(3), math.sqrt(2), 1]\n    for num_batches in [1, 2]:\n      for num_rows in [5, 13]:\n        for num_cols in [5, 11]:\n          for num_channels in [1, 3]:\n            input_shape = (num_batches, num_rows, num_cols, num_channels)\n            input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n            # Add some randomness to make input_data not so 'integer'\n            input_data += self._PRNG.random_sample(input_shape)\n            with self.cached_session() as _:\n              input_tensor = constant_op.constant(input_data, shape=input_shape)\n              output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n                  input_tensor,\n                  pooling_ratio,\n                  pseudo_random=pseudo_random,\n                  overlapping=overlapping,\n                  seed=self._SEED)\n              output_data = self.evaluate(output_tensor)\n              output_shape = output_data.shape\n              # error_margin and delta setting is similar to max_pool_grad.\n              error_margin = 1e-3\n              gradient_error = gradient_checker.compute_gradient_error(\n                  input_tensor,\n                  input_shape,\n                  output_tensor,\n                  output_shape,\n                  x_init_value=input_data.reshape(input_shape),\n                  delta=1e-2)\n              self.assertLess(gradient_error, error_margin)\n\n  @test_util.run_deprecated_v1\n  def testLargePoolingRatioThroughGradientError(self):\n    input_shape = (1, 17, 23, 1)\n    input_data = self._GenerateUniqueRandomInputTensor(input_shape)\n    # Add some randomness to make input_data not so 'integer'\n    input_data += self._PRNG.random_sample(input_shape)\n    pooling_ratio = (1, math.sqrt(13), math.sqrt(7), 1)\n    output_shape = [int(a / b) for a, b in zip(input_shape, pooling_ratio)]\n    overlapping = True\n    pseudo_random = False\n\n    with self.cached_session() as _:\n      input_tensor = constant_op.constant(input_data, shape=input_shape)\n      output_tensor, unused_a, unused_b = nn_ops.fractional_max_pool_v2(\n          input_tensor,\n          pooling_ratio,\n          pseudo_random=pseudo_random,\n          overlapping=overlapping,\n          seed=self._SEED)\n      # error_margin and delta setting is similar to max_pool_grad.\n      error_margin = 1e-3\n      gradient_error = gradient_checker.compute_gradient_error(\n          input_tensor,\n          input_shape,\n          output_tensor,\n          output_shape,\n          x_init_value=input_data.reshape(input_shape),\n          delta=1e-2)\n      self.assertLess(gradient_error, error_margin)\n\n  def testWhenRepeatedMaxValueInPoolingRegion(self):\n    \"\"\"Test when there's repeating value in pooling region.\n\n    There's no formal definition for what the gradient should be when there're\n    multiple max value within a pooling cell. Such as\n        | 1 5 |\n        | 5 3 |\n    The expected result depends heavily on implementation, if someone swap the\n    order of a nested for loop when walking through the tensor, result would be\n    very different.\n\n    The goal of this test is to alert when someone else change the\n    implementation. Current implementation scans row-by-row.\n    \"\"\"\n    input_data = [5.0, 4.0, 6.0, 7.0,\n                  3.0, 5.0, 9.0, 6.0,\n                  8.0, 8.0, 9.0, 5.0,\n                  7.0, 4.0, 0.0, 0.0]  # pyformat: disable\n    input_size = [1, 4, 4, 1]\n    output_backprop = [12.0, 15.0,\n                       17.0, -5.0,\n                       6.0, 21.0]  # pyformat: disable\n    row_seq = [0, 1, 3, 4]\n    col_seq = [0, 2, 4]\n    output_data_not_overlapping = [5.0, 7.0,\n                                   8.0, 9.0,\n                                   7.0, 0.0]  # pyformat: disable\n    output_data_overlapping = [9.0, 9.0,\n                               9.0, 9.0,\n                               7.0, 0.0]  # pyformat: disable\n    output_size = [1, 3, 2, 1]\n    expected_input_backprop_not_overlapping = np.reshape(\n        [12.0, 0.0, 0.0, 15.0,\n         0.0, 0.0, -5.0, 0.0,\n         17.0, 0.0, 0.0, 0.0,\n         6.0, 0.0, 21.0, 0.0],\n        input_size)  # pyformat: disable\n    expected_input_backprop_overlapping = np.reshape(\n        [0.0, 0.0, 0.0, 0.0,\n         0.0, 0.0, 39.0, 0.0,\n         0.0, 0.0, 0.0, 0.0,\n         6.0, 0.0, 21.0, 0.0],\n        input_size)  # pyformat: disable\n    with self.cached_session() as _:\n      # Test when overlapping is False\n      input_tensor = constant_op.constant(input_data, shape=input_size)\n      output_tensor = constant_op.constant(\n          output_data_not_overlapping, shape=output_size)\n      grad = constant_op.constant(output_backprop, shape=output_size)\n      r = gen_nn_ops.fractional_max_pool_grad(\n          input_tensor,\n          output_tensor,\n          grad,\n          row_seq,\n          col_seq,\n          overlapping=False)\n      input_backprop_not_overlapping = self.evaluate(r)\n      self.assertShapeEqual(\n          np.reshape(expected_input_backprop_not_overlapping, input_size), r)\n      self.assertAllClose(expected_input_backprop_not_overlapping,\n                          input_backprop_not_overlapping)\n      # Test when overlapping is True\n      output_tensor = constant_op.constant(\n          output_data_overlapping, shape=output_size)\n      r = gen_nn_ops.fractional_max_pool_grad(\n          input_tensor, output_tensor, grad, row_seq, col_seq, overlapping=True)\n      input_backprop_overlapping = self.evaluate(r)\n      self.assertShapeEqual(\n          np.reshape(expected_input_backprop_overlapping, input_size), r)\n      self.assertAllClose(expected_input_backprop_overlapping,\n                          input_backprop_overlapping)\n\n  def testInvalidSeqRaiseErrorForFractionalMaxPoolGrad(self):\n    with self.assertRaises(errors.InvalidArgumentError):\n      with self.cached_session() as _:\n        overlapping = True\n        orig_input = constant_op.constant(\n            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n        orig_output = constant_op.constant(\n            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n        out_backprop = constant_op.constant(\n            .453409232, shape=[1, 7, 13, 1], dtype=dtypes.float32)\n        row_pooling_sequence = constant_op.constant(\n            0, shape=[5], dtype=dtypes.int64)\n        col_pooling_sequence = constant_op.constant(\n            0, shape=[5], dtype=dtypes.int64)\n        t = gen_nn_ops.FractionalMaxPoolGrad(\n            orig_input=orig_input,\n            orig_output=orig_output,\n            out_backprop=out_backprop,\n            row_pooling_sequence=row_pooling_sequence,\n            col_pooling_sequence=col_pooling_sequence,\n            overlapping=overlapping)\n        self.evaluate(t)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/fractional_max_pool_op.cc", "tensorflow/python/kernel_tests/nn_ops/fractional_max_pool_op_test.py"], "buggy_code_start_loc": [22, 127], "buggy_code_end_loc": [377, 632], "fixing_code_start_loc": [21, 127], "fixing_code_end_loc": [381, 656], "type": "CWE-617", "message": "TensorFlow is an open source platform for machine learning. `FractionalMaxPoolGrad` validates its inputs with `CHECK` failures instead of with returning errors. If it gets incorrectly sized inputs, the `CHECK` failure can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 8741e57d163a079db05a7107a7609af70931def4. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-35981", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T22:15:11.183", "lastModified": "2022-09-20T14:53:05.007", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. `FractionalMaxPoolGrad` validates its inputs with `CHECK` failures instead of with returning errors. If it gets incorrectly sized inputs, the `CHECK` failure can be used to trigger a denial of service attack. We have patched the issue in GitHub commit 8741e57d163a079db05a7107a7609af70931def4. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. \"FractionalMaxPoolGrad\" comprueba sus entradas con fallos \"CHECK\" en lugar de devolver errores. Si recibe entradas de tama\u00f1o incorrecto, el fallo de \"CHECK\" puede ser usado para desencadenar un ataque de denegaci\u00f3n de servicio. Hemos parcheado el problema en el commit 8741e57d163a079db05a7107a7609af70931def4 de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-617"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C6622D95-1C86-45C5-AB55-E6EEEA0996DF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/8741e57d163a079db05a7107a7609af70931def4", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-vxv8-r8q2-63xw", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/8741e57d163a079db05a7107a7609af70931def4"}}