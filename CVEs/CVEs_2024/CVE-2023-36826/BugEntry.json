{"buggy_code": ["import logging\nfrom datetime import datetime, timedelta\nfrom typing import List, Mapping, Optional, Sequence, Set, Tuple\n\nimport pytz\nfrom django.db import transaction\nfrom django.http import Http404, HttpResponse, StreamingHttpResponse\nfrom rest_framework.request import Request\nfrom rest_framework.response import Response\nfrom symbolic import SymbolicError, normalize_debug_id\n\nfrom sentry import options, ratelimits\nfrom sentry.api.base import region_silo_endpoint\nfrom sentry.api.bases.project import ProjectEndpoint, ProjectReleasePermission\nfrom sentry.api.endpoints.debug_files import has_download_permission\nfrom sentry.api.serializers import serialize\nfrom sentry.auth.system import is_system_auth\nfrom sentry.lang.native.sources import get_internal_artifact_lookup_source_url\nfrom sentry.models import (\n    ArtifactBundle,\n    DebugIdArtifactBundle,\n    Distribution,\n    File,\n    Project,\n    ProjectArtifactBundle,\n    Release,\n    ReleaseArtifactBundle,\n    ReleaseFile,\n)\nfrom sentry.utils import metrics\n\nlogger = logging.getLogger(\"sentry.api\")\n\n# The marker for \"release\" bundles\nRELEASE_BUNDLE_TYPE = \"release.bundle\"\n# The number of bundles (\"artifact\" or \"release\") that we query\nMAX_BUNDLES_QUERY = 5\n# The number of files returned by the `get_releasefiles` query\nMAX_RELEASEFILES_QUERY = 10\n# Number of days that determine whether an artifact bundle is ready for being renewed.\nAVAILABLE_FOR_RENEWAL_DAYS = 30\n\n\n@region_silo_endpoint\nclass ProjectArtifactLookupEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    def download_file(self, file_id, project: Project):\n        rate_limited = ratelimits.is_limited(\n            project=project,\n            key=f\"rl:ArtifactLookupEndpoint:download:{file_id}:{project.id}\",\n            limit=10,\n        )\n        if rate_limited:\n            logger.info(\n                \"notification.rate_limited\",\n                extra={\"project_id\": project.id, \"file_id\": file_id},\n            )\n            return HttpResponse({\"Too many download requests\"}, status=429)\n\n        file = File.objects.filter(id=file_id).first()\n\n        if file is None:\n            raise Http404\n\n        try:\n            fp = file.getfile()\n            response = StreamingHttpResponse(\n                iter(lambda: fp.read(4096), b\"\"), content_type=\"application/octet-stream\"\n            )\n            response[\"Content-Length\"] = file.size\n            response[\"Content-Disposition\"] = f'attachment; filename=\"{file.name}\"'\n            return response\n        except OSError:\n            raise Http404\n\n    def get(self, request: Request, project: Project) -> Response:\n        \"\"\"\n        List a Project's Individual Artifacts or Bundles\n        ````````````````````````````````````````\n\n        Retrieve a list of individual artifacts or artifact bundles for a given project.\n\n        :pparam string organization_slug: the slug of the organization to query.\n        :pparam string project_slug: the slug of the project to query.\n        :qparam string debug_id: if set, will query and return the artifact\n                                 bundle that matches the given `debug_id`.\n        :qparam string url: if set, will query and return all the individual\n                            artifacts, or artifact bundles that contain files\n                            that match the `url`. This is using a substring-match.\n        :qparam string release: used in conjunction with `url`.\n        :qparam string dist: used in conjunction with `url`.\n\n        :auth: required\n        \"\"\"\n        if request.GET.get(\"download\") is not None:\n            if has_download_permission(request, project):\n                return self.download_file(request.GET.get(\"download\"), project)\n            else:\n                return Response(status=403)\n\n        debug_id = request.GET.get(\"debug_id\")\n        try:\n            debug_id = normalize_debug_id(debug_id)\n        except SymbolicError:\n            pass\n        url = request.GET.get(\"url\")\n        release_name = request.GET.get(\"release\")\n        dist_name = request.GET.get(\"dist\")\n\n        used_artifact_bundles = dict()\n        bundle_file_ids = set()\n\n        def update_bundles(inner_bundles: Set[Tuple[int, datetime, int]]):\n            for (bundle_id, date_added, file_id) in inner_bundles:\n                used_artifact_bundles[bundle_id] = date_added\n                bundle_file_ids.add(file_id)\n\n        if debug_id:\n            bundles = get_artifact_bundles_containing_debug_id(debug_id, project)\n            update_bundles(bundles)\n\n        individual_files = set()\n        if url and release_name and not bundle_file_ids:\n            # Get both the newest X release artifact bundles,\n            # and also query the legacy artifact bundles. One of those should have the\n            # file we are looking for. We want to return more here, even bundles that\n            # do *not* contain the file, rather than opening up each bundle. We want to\n            # avoid opening up bundles at all cost.\n            bundles = get_release_artifacts(project, release_name, dist_name)\n            update_bundles(bundles)\n\n            release, dist = try_resolve_release_dist(project, release_name, dist_name)\n            if release:\n                bundle_file_ids |= get_legacy_release_bundles(release, dist)\n                individual_files = get_legacy_releasefile_by_file_url(release, dist, url)\n\n        if options.get(\"sourcemaps.artifact-bundles.enable-renewal\") == 1.0:\n            with metrics.timer(\"artifact_lookup.get.renew_artifact_bundles\"):\n                # Before constructing the response, we want to update the artifact bundles renewal date.\n                renew_artifact_bundles(used_artifact_bundles)\n\n        # Then: Construct our response\n        url_constructor = UrlConstructor(request, project)\n\n        found_artifacts = []\n        for file_id in bundle_file_ids:\n            found_artifacts.append(\n                {\n                    \"id\": str(file_id),\n                    \"type\": \"bundle\",\n                    \"url\": url_constructor.url_for_file_id(file_id),\n                }\n            )\n\n        for release_file in individual_files:\n            found_artifacts.append(\n                {\n                    \"id\": str(release_file.file.id),\n                    \"type\": \"file\",\n                    \"url\": url_constructor.url_for_file_id(release_file.file.id),\n                    # The `name` is the url/abs_path of the file,\n                    # as in: `\"~/path/to/file.min.js\"`.\n                    \"abs_path\": release_file.name,\n                    # These headers should ideally include the `Sourcemap` reference\n                    \"headers\": release_file.file.headers,\n                }\n            )\n\n        # NOTE: We do not paginate this response, as we have very tight limits on all the individual queries.\n        return Response(serialize(found_artifacts, request.user))\n\n\ndef renew_artifact_bundles(used_artifact_bundles: Mapping[int, datetime]):\n    # We take a snapshot in time that MUST be consistent across all updates.\n    now = datetime.now(tz=pytz.UTC)\n    # We compute the threshold used to determine whether we want to renew the specific bundle.\n    threshold_date = now - timedelta(days=AVAILABLE_FOR_RENEWAL_DAYS)\n\n    for (artifact_bundle_id, date_added) in used_artifact_bundles.items():\n        metrics.incr(\"artifact_lookup.get.renew_artifact_bundles.should_be_renewed\")\n        # We perform the condition check also before running the query, in order to reduce the amount of queries to the\n        # database.\n        if date_added <= threshold_date:\n            metrics.incr(\"artifact_lookup.get.renew_artifact_bundles.renewed\")\n            # We want to use a transaction, in order to keep the `date_added` consistent across multiple tables.\n            with transaction.atomic():\n                # We check again for the date_added condition in order to achieve consistency, this is done because\n                # the `can_be_renewed` call is using a time which differs from the one of the actual update in the db.\n                updated_rows_count = ArtifactBundle.objects.filter(\n                    id=artifact_bundle_id, date_added__lte=threshold_date\n                ).update(date_added=now)\n                # We want to make cascading queries only if there were actual changes in the db.\n                if updated_rows_count > 0:\n                    ProjectArtifactBundle.objects.filter(\n                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date\n                    ).update(date_added=now)\n                    ReleaseArtifactBundle.objects.filter(\n                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date\n                    ).update(date_added=now)\n                    DebugIdArtifactBundle.objects.filter(\n                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date\n                    ).update(date_added=now)\n\n\ndef get_artifact_bundles_containing_debug_id(\n    debug_id: str, project: Project\n) -> Set[Tuple[int, datetime, int]]:\n    # We want to have the newest `File` for each `debug_id`.\n    return set(\n        ArtifactBundle.objects.filter(\n            organization_id=project.organization.id,\n            debugidartifactbundle__debug_id=debug_id,\n        )\n        .values_list(\"id\", \"date_added\", \"file_id\")\n        .order_by(\"-date_uploaded\")[:1]\n    )\n\n\ndef get_release_artifacts(\n    project: Project,\n    release_name: str,\n    dist_name: Optional[str],\n) -> Set[Tuple[int, datetime, int]]:\n    return set(\n        ArtifactBundle.objects.filter(\n            organization_id=project.organization.id,\n            projectartifactbundle__project_id=project.id,\n            releaseartifactbundle__release_name=release_name,\n            # In case no dist is provided, we will fall back to \"\" which is the NULL equivalent for our tables.\n            # See `_create_artifact_bundle` in `src/sentry/tasks/assemble.py` for the reference.\n            releaseartifactbundle__dist_name=dist_name or \"\",\n        )\n        .values_list(\"id\", \"date_added\", \"file_id\")\n        .order_by(\"-date_uploaded\")[:MAX_BUNDLES_QUERY]\n    )\n\n\ndef try_resolve_release_dist(\n    project: Project, release_name: str, dist_name: Optional[str]\n) -> Tuple[Optional[Release], Optional[Distribution]]:\n    release = None\n    dist = None\n    try:\n        release = Release.objects.get(\n            organization_id=project.organization_id,\n            projects=project,\n            version=release_name,\n        )\n\n        # We cannot query for dist without a release anyway\n        if dist_name:\n            dist = Distribution.objects.get(release=release, name=dist_name)\n    except (Release.DoesNotExist, Distribution.DoesNotExist):\n        pass\n    except Exception as exc:\n        logger.error(\"Failed to read\", exc_info=exc)\n\n    return release, dist\n\n\ndef get_legacy_release_bundles(release: Release, dist: Optional[Distribution]):\n    return set(\n        ReleaseFile.objects.select_related(\"file\")\n        .filter(\n            release_id=release.id,\n            dist_id=dist.id if dist else None,\n            # a `ReleaseFile` with `0` artifacts represents a release archive,\n            # see the comment above the definition of `artifact_count`.\n            artifact_count=0,\n            # similarly the special `type` is also used for release archives.\n            file__type=RELEASE_BUNDLE_TYPE,\n        )\n        .values_list(\"file_id\", flat=True)\n        # TODO: this `order_by` might be incredibly slow\n        # we want to have a hard limit on the returned bundles here. and we would\n        # want to pick the most recently uploaded ones. that should mostly be\n        # relevant for customers that upload multiple bundles, or are uploading\n        # newer files for existing releases. In that case the symbolication is\n        # already degraded, so meh...\n        # .order_by(\"-file__timestamp\")\n        [:MAX_BUNDLES_QUERY]\n    )\n\n\ndef get_legacy_releasefile_by_file_url(\n    release: Release, dist: Optional[Distribution], url: List[str]\n) -> Sequence[ReleaseFile]:\n    # Exclude files which are also present in archive:\n    return (\n        ReleaseFile.public_objects.filter(\n            release_id=release.id,\n            dist_id=dist.id if dist else None,\n        )\n        .exclude(artifact_count=0)\n        .select_related(\"file\")\n    ).filter(name__icontains=url)[:MAX_RELEASEFILES_QUERY]\n\n\nclass UrlConstructor:\n    def __init__(self, request: Request, project: Project):\n        if is_system_auth(request.auth):\n            self.base_url = get_internal_artifact_lookup_source_url(project)\n        else:\n            self.base_url = request.build_absolute_uri(request.path)\n\n    def url_for_file_id(self, file_id: int) -> str:\n        # NOTE: Returning a self-route that requires authentication (via Bearer token)\n        # is not really forward compatible with a pre-signed URL that does not\n        # require any authentication or headers whatsoever.\n        # This also requires a workaround in Symbolicator, as its generic http\n        # downloader blocks \"internal\" IPs, whereas the internal Sentry downloader\n        # is explicitly exempt.\n        return f\"{self.base_url}?download={file_id}\"\n", "import logging\nimport posixpath\nimport re\n\nimport jsonschema\nfrom django.db import router\nfrom django.db.models import Q\nfrom django.http import Http404, HttpResponse, StreamingHttpResponse\nfrom rest_framework.request import Request\nfrom rest_framework.response import Response\nfrom symbolic import SymbolicError, normalize_debug_id\n\nfrom sentry import ratelimits, roles\nfrom sentry.api.base import region_silo_endpoint\nfrom sentry.api.bases.project import ProjectEndpoint, ProjectReleasePermission\nfrom sentry.api.exceptions import ResourceDoesNotExist\nfrom sentry.api.paginator import OffsetPaginator\nfrom sentry.api.serializers import serialize\nfrom sentry.auth.superuser import is_active_superuser\nfrom sentry.auth.system import is_system_auth\nfrom sentry.constants import DEBUG_FILES_ROLE_DEFAULT, KNOWN_DIF_FORMATS\nfrom sentry.models import (\n    File,\n    FileBlobOwner,\n    OrganizationMember,\n    ProjectDebugFile,\n    Release,\n    ReleaseFile,\n    create_files_from_dif_zip,\n)\nfrom sentry.models.release import get_artifact_counts\nfrom sentry.tasks.assemble import (\n    AssembleTask,\n    ChunkFileState,\n    get_assemble_status,\n    set_assemble_status,\n)\nfrom sentry.utils import json\nfrom sentry.utils.db import atomic_transaction\n\nlogger = logging.getLogger(\"sentry.api\")\nERR_FILE_EXISTS = \"A file matching this debug identifier already exists\"\nDIF_MIMETYPES = {v: k for k, v in KNOWN_DIF_FORMATS.items()}\n_release_suffix = re.compile(r\"^(.*)\\s+\\(([^)]+)\\)\\s*$\")\n\n\ndef upload_from_request(request, project):\n    if \"file\" not in request.data:\n        return Response({\"detail\": \"Missing uploaded file\"}, status=400)\n    fileobj = request.data[\"file\"]\n    files = create_files_from_dif_zip(fileobj, project=project)\n    return Response(serialize(files, request.user), status=201)\n\n\ndef has_download_permission(request, project):\n    if is_system_auth(request.auth) or is_active_superuser(request):\n        return True\n\n    if not request.user.is_authenticated:\n        return False\n\n    organization = project.organization\n    required_role = organization.get_option(\"sentry:debug_files_role\") or DEBUG_FILES_ROLE_DEFAULT\n\n    if request.user.is_sentry_app:\n        if roles.get(required_role).priority > roles.get(\"member\").priority:\n            return request.access.has_scope(\"project:write\")\n        else:\n            return request.access.has_scope(\"project:read\")\n\n    try:\n        current_role = (\n            OrganizationMember.objects.filter(organization=organization, user_id=request.user.id)\n            .values_list(\"role\", flat=True)\n            .get()\n        )\n    except OrganizationMember.DoesNotExist:\n        return False\n\n    return roles.get(current_role).priority >= roles.get(required_role).priority\n\n\n@region_silo_endpoint\nclass DebugFilesEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    def download(self, debug_file_id, project):\n        rate_limited = ratelimits.is_limited(\n            project=project,\n            key=f\"rl:DSymFilesEndpoint:download:{debug_file_id}:{project.id}\",\n            limit=10,\n        )\n        if rate_limited:\n            logger.info(\n                \"notification.rate_limited\",\n                extra={\"project_id\": project.id, \"project_debug_file_id\": debug_file_id},\n            )\n            return HttpResponse({\"Too many download requests\"}, status=403)\n\n        debug_file = ProjectDebugFile.objects.filter(id=debug_file_id).first()\n\n        if debug_file is None:\n            raise Http404\n\n        try:\n            fp = debug_file.file.getfile()\n            response = StreamingHttpResponse(\n                iter(lambda: fp.read(4096), b\"\"), content_type=\"application/octet-stream\"\n            )\n            response[\"Content-Length\"] = debug_file.file.size\n            response[\"Content-Disposition\"] = 'attachment; filename=\"{}{}\"'.format(\n                posixpath.basename(debug_file.debug_id),\n                debug_file.file_extension,\n            )\n            return response\n        except OSError:\n            raise Http404\n\n    def get(self, request: Request, project) -> Response:\n        \"\"\"\n        List a Project's Debug Information Files\n        ````````````````````````````````````````\n\n        Retrieve a list of debug information files for a given project.\n\n        :pparam string organization_slug: the slug of the organization the\n                                          file belongs to.\n        :pparam string project_slug: the slug of the project to list the\n                                     DIFs of.\n        :qparam string query: If set, this parameter is used to locate DIFs with.\n        :qparam string id: If set, the specified DIF will be sent in the response.\n        :qparam string file_formats: If set, only DIFs with these formats will be returned.\n        :auth: required\n        \"\"\"\n        download_requested = request.GET.get(\"id\") is not None\n        if download_requested and (has_download_permission(request, project)):\n            return self.download(request.GET.get(\"id\"), project)\n        elif download_requested:\n            return Response(status=403)\n\n        code_id = request.GET.get(\"code_id\")\n        debug_id = request.GET.get(\"debug_id\")\n        query = request.GET.get(\"query\")\n        file_formats = request.GET.getlist(\"file_formats\")\n\n        # If this query contains a debug identifier, normalize it to allow for\n        # more lenient queries (e.g. supporting Breakpad ids). Use the index to\n        # speed up such queries.\n        if query and len(query) <= 45 and not debug_id:\n            try:\n                debug_id = normalize_debug_id(query.strip())\n            except SymbolicError:\n                pass\n\n        if debug_id:\n            # If a debug ID is specified, do not consider the stored code\n            # identifier and strictly filter by debug identifier. Often there\n            # are mismatches in the code identifier in PEs.\n            q = Q(debug_id__exact=debug_id)\n        elif code_id:\n            q = Q(code_id__exact=code_id)\n        elif query:\n            q = (\n                Q(object_name__icontains=query)\n                | Q(debug_id__icontains=query)\n                | Q(code_id__icontains=query)\n                | Q(cpu_name__icontains=query)\n                | Q(file__headers__icontains=query)\n            )\n\n            known_file_format = DIF_MIMETYPES.get(query)\n            if known_file_format:\n                q |= Q(file__headers__icontains=known_file_format)\n        else:\n            q = Q()\n\n        file_format_q = Q()\n        for file_format in file_formats:\n            known_file_format = DIF_MIMETYPES.get(file_format)\n            if known_file_format:\n                file_format_q |= Q(file__headers__icontains=known_file_format)\n\n        q &= file_format_q\n\n        queryset = ProjectDebugFile.objects.filter(q, project_id=project.id).select_related(\"file\")\n\n        return self.paginate(\n            request=request,\n            queryset=queryset,\n            order_by=\"-id\",\n            paginator_cls=OffsetPaginator,\n            default_per_page=20,\n            on_results=lambda x: serialize(x, request.user),\n        )\n\n    def delete(self, request: Request, project) -> Response:\n        \"\"\"\n        Delete a specific Project's Debug Information File\n        ```````````````````````````````````````````````````\n\n        Delete a debug information file for a given project.\n\n        :pparam string organization_slug: the slug of the organization the\n                                          file belongs to.\n        :pparam string project_slug: the slug of the project to delete the\n                                     DIF.\n        :qparam string id: The id of the DIF to delete.\n        :auth: required\n        \"\"\"\n\n        if request.GET.get(\"id\") and (request.access.has_scope(\"project:write\")):\n            with atomic_transaction(using=router.db_for_write(File)):\n                debug_file = (\n                    ProjectDebugFile.objects.filter(id=request.GET.get(\"id\"), project_id=project.id)\n                    .select_related(\"file\")\n                    .first()\n                )\n                if debug_file is not None:\n                    debug_file.delete()\n                    return Response(status=204)\n\n        return Response(status=404)\n\n    def post(self, request: Request, project) -> Response:\n        \"\"\"\n        Upload a New File\n        `````````````````\n\n        Upload a new debug information file for the given release.\n\n        Unlike other API requests, files must be uploaded using the\n        traditional multipart/form-data content-type.\n\n        The file uploaded is a zip archive of a Apple .dSYM folder which\n        contains the individual debug images.  Uploading through this endpoint\n        will create different files for the contained images.\n\n        :pparam string organization_slug: the slug of the organization the\n                                          release belongs to.\n        :pparam string project_slug: the slug of the project to change the\n                                     release of.\n        :param file file: the multipart encoded file.\n        :auth: required\n        \"\"\"\n        return upload_from_request(request, project=project)\n\n\n@region_silo_endpoint\nclass UnknownDebugFilesEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    def get(self, request: Request, project) -> Response:\n        checksums = request.GET.getlist(\"checksums\")\n        missing = ProjectDebugFile.objects.find_missing(checksums, project=project)\n        return Response({\"missing\": missing})\n\n\n@region_silo_endpoint\nclass AssociateDSymFilesEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    # Legacy endpoint, kept for backwards compatibility\n    def post(self, request: Request, project) -> Response:\n        return Response({\"associatedDsymFiles\": []})\n\n\ndef find_missing_chunks(organization, chunks):\n    \"\"\"Returns a list of chunks which are missing for an org.\"\"\"\n    owned = set(\n        FileBlobOwner.objects.filter(\n            blob__checksum__in=chunks, organization_id=organization.id\n        ).values_list(\"blob__checksum\", flat=True)\n    )\n    return list(set(chunks) - owned)\n\n\n@region_silo_endpoint\nclass DifAssembleEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    def post(self, request: Request, project) -> Response:\n        \"\"\"\n        Assemble one or multiple chunks (FileBlob) into debug files\n        ````````````````````````````````````````````````````````````\n\n        :auth: required\n        \"\"\"\n        schema = {\n            \"type\": \"object\",\n            \"patternProperties\": {\n                \"^[0-9a-f]{40}$\": {\n                    \"type\": \"object\",\n                    \"required\": [\"name\", \"chunks\"],\n                    \"properties\": {\n                        \"name\": {\"type\": \"string\"},\n                        \"debug_id\": {\"type\": \"string\"},\n                        \"chunks\": {\n                            \"type\": \"array\",\n                            \"items\": {\"type\": \"string\", \"pattern\": \"^[0-9a-f]{40}$\"},\n                        },\n                    },\n                    \"additionalProperties\": True,\n                }\n            },\n            \"additionalProperties\": False,\n        }\n\n        try:\n            files = json.loads(request.body)\n            jsonschema.validate(files, schema)\n        except jsonschema.ValidationError as e:\n            return Response({\"error\": str(e).splitlines()[0]}, status=400)\n        except Exception:\n            return Response({\"error\": \"Invalid json body\"}, status=400)\n\n        file_response = {}\n\n        for checksum, file_to_assemble in files.items():\n            name = file_to_assemble.get(\"name\", None)\n            debug_id = file_to_assemble.get(\"debug_id\", None)\n            chunks = file_to_assemble.get(\"chunks\", [])\n\n            # First, check the cached assemble status. During assembling, a\n            # ProjectDebugFile will be created and we need to prevent a race\n            # condition.\n            state, detail = get_assemble_status(AssembleTask.DIF, project.id, checksum)\n            if state == ChunkFileState.OK:\n                file_response[checksum] = {\n                    \"state\": state,\n                    \"detail\": None,\n                    \"missingChunks\": [],\n                    \"dif\": detail,\n                }\n                continue\n            elif state is not None:\n                file_response[checksum] = {\"state\": state, \"detail\": detail, \"missingChunks\": []}\n                continue\n\n            # Next, check if this project already owns the ProjectDebugFile.\n            # This can under rare circumstances yield more than one file\n            # which is why we use first() here instead of get().\n            dif = (\n                ProjectDebugFile.objects.filter(project_id=project.id, checksum=checksum)\n                .select_related(\"file\")\n                .order_by(\"-id\")\n                .first()\n            )\n\n            if dif is not None:\n                file_response[checksum] = {\n                    \"state\": ChunkFileState.OK,\n                    \"detail\": None,\n                    \"missingChunks\": [],\n                    \"dif\": serialize(dif),\n                }\n                continue\n\n            # There is neither a known file nor a cached state, so we will\n            # have to create a new file.  Assure that there are checksums.\n            # If not, we assume this is a poll and report NOT_FOUND\n            if not chunks:\n                file_response[checksum] = {\"state\": ChunkFileState.NOT_FOUND, \"missingChunks\": []}\n                continue\n\n            # Check if all requested chunks have been uploaded.\n            missing_chunks = find_missing_chunks(project.organization, chunks)\n            if missing_chunks:\n                file_response[checksum] = {\n                    \"state\": ChunkFileState.NOT_FOUND,\n                    \"missingChunks\": missing_chunks,\n                }\n                continue\n\n            # We don't have a state yet, this means we can now start\n            # an assemble job in the background.\n            set_assemble_status(AssembleTask.DIF, project.id, checksum, ChunkFileState.CREATED)\n\n            from sentry.tasks.assemble import assemble_dif\n\n            assemble_dif.apply_async(\n                kwargs={\n                    \"project_id\": project.id,\n                    \"name\": name,\n                    \"debug_id\": debug_id,\n                    \"checksum\": checksum,\n                    \"chunks\": chunks,\n                }\n            )\n\n            file_response[checksum] = {\"state\": ChunkFileState.CREATED, \"missingChunks\": []}\n\n        return Response(file_response, status=200)\n\n\n@region_silo_endpoint\nclass SourceMapsEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    def get(self, request: Request, project) -> Response:\n        \"\"\"\n        List a Project's Source Map Archives\n        ````````````````````````````````````\n\n        Retrieve a list of source map archives (releases, later bundles) for a given project.\n\n        :pparam string organization_slug: the slug of the organization the\n                                          source map archive belongs to.\n        :pparam string project_slug: the slug of the project to list the\n                                     source map archives of.\n        :qparam string query: If set, this parameter is used to locate source map archives with.\n        :auth: required\n        \"\"\"\n        query = request.GET.get(\"query\")\n\n        try:\n            queryset = Release.objects.filter(\n                projects=project, organization_id=project.organization_id\n            ).values(\"id\", \"version\", \"date_added\")\n        except Release.DoesNotExist:\n            raise ResourceDoesNotExist\n\n        if query:\n            query_q = Q(version__icontains=query)\n\n            suffix_match = _release_suffix.match(query)\n            if suffix_match is not None:\n                query_q |= Q(version__icontains=\"%s+%s\" % suffix_match.groups())\n\n            queryset = queryset.filter(query_q)\n\n        def expose_release(release, count):\n            return {\n                \"type\": \"release\",\n                \"id\": release[\"id\"],\n                \"name\": release[\"version\"],\n                \"date\": release[\"date_added\"],\n                \"fileCount\": count,\n            }\n\n        def serialize_results(results):\n            file_count_map = get_artifact_counts([r[\"id\"] for r in results])\n            # In case we didn't find a file count for a specific release, we will return -1, signaling to the\n            # frontend that this release doesn't have one or more ReleaseFile.\n            return serialize(\n                [expose_release(r, file_count_map.get(r[\"id\"], -1)) for r in results], request.user\n            )\n\n        sort_by = request.GET.get(\"sortBy\", \"-date_added\")\n        if sort_by not in {\"-date_added\", \"date_added\"}:\n            return Response(\n                {\"error\": \"You can either sort via 'date_added' or '-date_added'\"}, status=400\n            )\n\n        return self.paginate(\n            request=request,\n            queryset=queryset,\n            order_by=sort_by,\n            paginator_cls=OffsetPaginator,\n            default_per_page=10,\n            on_results=serialize_results,\n        )\n\n    def delete(self, request: Request, project) -> Response:\n        \"\"\"\n        Delete an Archive\n        ```````````````````````````````````````````````````\n\n        Delete all artifacts inside given archive.\n\n        :pparam string organization_slug: the slug of the organization the\n                                            archive belongs to.\n        :pparam string project_slug: the slug of the project to delete the\n                                        archive of.\n        :qparam string name: The name of the archive to delete.\n        :auth: required\n        \"\"\"\n\n        archive_name = request.GET.get(\"name\")\n\n        if archive_name:\n            with atomic_transaction(using=router.db_for_write(ReleaseFile)):\n                release = Release.objects.get(\n                    organization_id=project.organization_id, projects=project, version=archive_name\n                )\n                if release is not None:\n                    release_files = ReleaseFile.objects.filter(release_id=release.id)\n                    release_files.delete()\n                    return Response(status=204)\n\n        return Response(status=404)\n", "import uuid\nimport zipfile\nfrom datetime import datetime, timedelta, timezone\nfrom io import BytesIO\nfrom uuid import uuid4\n\nimport pytz\nfrom django.urls import reverse\nfrom freezegun import freeze_time\n\nfrom sentry.models import (\n    ArtifactBundle,\n    DebugIdArtifactBundle,\n    File,\n    ProjectArtifactBundle,\n    ReleaseArtifactBundle,\n    ReleaseFile,\n    SourceFileType,\n)\nfrom sentry.models.releasefile import read_artifact_index, update_artifact_index\nfrom sentry.testutils import APITestCase\nfrom sentry.utils import json\n\n\ndef make_file(artifact_name, content, type=\"artifact.bundle\", headers=None):\n    file = File.objects.create(name=artifact_name, type=type, headers=(headers or {}))\n    file.putfile(BytesIO(content))\n    return file\n\n\ndef make_compressed_zip_file(artifact_name, files):\n    def remove_and_return(dictionary, key):\n        dictionary.pop(key)\n        return dictionary\n\n    compressed = BytesIO()\n    with zipfile.ZipFile(compressed, mode=\"w\") as zip_file:\n        for file_path, info in files.items():\n            zip_file.writestr(file_path, bytes(info[\"content\"]))\n\n        zip_file.writestr(\n            \"manifest.json\",\n            json.dumps(\n                {\n                    # We remove the \"content\" key in the original dict, thus no subsequent calls should be made.\n                    \"files\": {\n                        file_path: remove_and_return(info, \"content\")\n                        for file_path, info in files.items()\n                    }\n                }\n            ),\n        )\n    compressed.seek(0)\n\n    file = File.objects.create(name=artifact_name, type=\"artifact.bundle\")\n    file.putfile(compressed)\n\n    return file\n\n\nclass ArtifactLookupTest(APITestCase):\n    def assert_download_matches_file(self, url: str, file: File):\n        response = self.client.get(url)\n        with file.getfile() as file:\n            for chunk in response:\n                assert file.read(len(chunk)) == chunk\n\n    def create_archive(self, fields, files, dist=None):\n        manifest = dict(\n            fields, files={filename: {\"url\": f\"fake://{filename}\"} for filename in files}\n        )\n        buffer = BytesIO()\n        with zipfile.ZipFile(buffer, mode=\"w\") as zf:\n            zf.writestr(\"manifest.json\", json.dumps(manifest))\n            for filename, content in files.items():\n                zf.writestr(filename, content)\n\n        buffer.seek(0)\n        name = f\"release-artifacts-{uuid.uuid4().hex}.zip\"\n        file_ = File.objects.create(name=name, type=\"release.bundle\")\n        file_.putfile(buffer)\n        file_.update(timestamp=datetime(2021, 6, 11, 9, 13, 1, 317902, tzinfo=timezone.utc))\n\n        return (update_artifact_index(self.release, dist, file_), file_)\n\n    def test_query_by_debug_ids(self):\n        debug_id_a = \"aaaaaaaa-0000-0000-0000-000000000000\"\n        debug_id_b = \"bbbbbbbb-0000-0000-0000-000000000000\"\n        file_ab = make_compressed_zip_file(\n            \"bundle_ab.zip\",\n            {\n                \"path/in/zip/a\": {\n                    \"url\": \"~/path/to/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"foo\",\n                    \"headers\": {\n                        \"debug-id\": debug_id_a,\n                    },\n                },\n                \"path/in/zip/b\": {\n                    \"url\": \"~/path/to/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"bar\",\n                    \"headers\": {\n                        \"debug-id\": debug_id_b,\n                    },\n                },\n            },\n        )\n\n        bundle_id_ab = uuid4()\n        artifact_bundle_ab = ArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            bundle_id=bundle_id_ab,\n            file=file_ab,\n            artifact_count=2,\n        )\n\n        DebugIdArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            debug_id=debug_id_a,\n            artifact_bundle=artifact_bundle_ab,\n            source_file_type=SourceFileType.SOURCE_MAP.value,\n        )\n        DebugIdArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            debug_id=debug_id_b,\n            artifact_bundle=artifact_bundle_ab,\n            source_file_type=SourceFileType.SOURCE_MAP.value,\n        )\n\n        debug_id_c = \"cccccccc-0000-0000-0000-000000000000\"\n        file_c = make_compressed_zip_file(\n            \"bundle_c.zip\",\n            {\n                \"path/in/zip/c\": {\n                    \"url\": \"~/path/to/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"baz\",\n                    \"headers\": {\n                        \"debug-id\": debug_id_c,\n                    },\n                },\n            },\n        )\n\n        bundle_id_c = uuid4()\n        artifact_bundle_c = ArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            bundle_id=bundle_id_c,\n            file=file_c,\n            artifact_count=1,\n        )\n\n        DebugIdArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            debug_id=debug_id_c,\n            artifact_bundle=artifact_bundle_c,\n            source_file_type=SourceFileType.SOURCE_MAP.value,\n        )\n\n        self.login_as(user=self.user)\n\n        url = reverse(\n            \"sentry-api-0-project-artifact-lookup\",\n            kwargs={\n                \"organization_slug\": self.project.organization.slug,\n                \"project_slug\": self.project.slug,\n            },\n        )\n\n        # query by one debug-id\n        response = self.client.get(f\"{url}?debug_id={debug_id_a}\").json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], file_ab)\n\n        # query by another debug-id pointing to the same bundle\n        response = self.client.get(f\"{url}?debug_id={debug_id_b}\").json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], file_ab)\n\n        # query by another debug-id pointing to different bundles\n        response = self.client.get(f\"{url}?debug_id={debug_id_c}\").json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], file_c)\n\n    def test_query_by_url(self):\n        debug_id_a = \"aaaaaaaa-0000-0000-0000-000000000000\"\n        file_a = make_compressed_zip_file(\n            \"bundle_a.zip\",\n            {\n                \"path/in/zip\": {\n                    \"url\": \"~/path/to/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"foo\",\n                    \"headers\": {\n                        \"debug-id\": debug_id_a,\n                    },\n                },\n            },\n        )\n        file_b = make_compressed_zip_file(\n            \"bundle_b.zip\",\n            {\n                \"path/in/zip_a\": {\n                    \"url\": \"~/path/to/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"foo\",\n                },\n                \"path/in/zip_b\": {\n                    \"url\": \"~/path/to/other/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"bar\",\n                },\n            },\n        )\n\n        artifact_bundle_a = ArtifactBundle.objects.create(\n            organization_id=self.organization.id, bundle_id=uuid4(), file=file_a, artifact_count=1\n        )\n\n        DebugIdArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            debug_id=debug_id_a,\n            artifact_bundle=artifact_bundle_a,\n            source_file_type=SourceFileType.SOURCE_MAP.value,\n        )\n\n        artifact_bundle_b = ArtifactBundle.objects.create(\n            organization_id=self.organization.id, bundle_id=uuid4(), file=file_b, artifact_count=2\n        )\n\n        dist = self.release.add_dist(\"whatever\")\n\n        ReleaseArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            release_name=self.release.version,\n            dist_name=dist.name,\n            artifact_bundle=artifact_bundle_a,\n        )\n        ProjectArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            project_id=self.project.id,\n            artifact_bundle=artifact_bundle_a,\n        )\n        ReleaseArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            release_name=self.release.version,\n            dist_name=dist.name,\n            artifact_bundle=artifact_bundle_b,\n        )\n        ProjectArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            project_id=self.project.id,\n            artifact_bundle=artifact_bundle_b,\n        )\n\n        self.login_as(user=self.user)\n\n        url = reverse(\n            \"sentry-api-0-project-artifact-lookup\",\n            kwargs={\n                \"organization_slug\": self.project.organization.slug,\n                \"project_slug\": self.project.slug,\n            },\n        )\n\n        # query by url that is in both files, so we get both files\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&dist={dist.name}&url=path/to/app\"\n        ).json()\n\n        assert len(response) == 2\n        assert response[0][\"type\"] == \"bundle\"\n        assert response[1][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], file_a)\n        self.assert_download_matches_file(response[1][\"url\"], file_b)\n\n        # query by both debug-id and url with overlapping bundles\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&dist={dist.name}&debug_id={debug_id_a}&url=path/to/app\"\n        ).json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], file_a)\n\n    def test_query_by_url_from_releasefiles(self):\n        file_headers = {\"Sourcemap\": \"application.js.map\"}\n        file = make_file(\"application.js\", b\"wat\", \"release.file\", file_headers)\n        ReleaseFile.objects.create(\n            organization_id=self.project.organization_id,\n            release_id=self.release.id,\n            file=file,\n            name=\"http://example.com/application.js\",\n        )\n\n        self.login_as(user=self.user)\n\n        url = reverse(\n            \"sentry-api-0-project-artifact-lookup\",\n            kwargs={\n                \"organization_slug\": self.project.organization.slug,\n                \"project_slug\": self.project.slug,\n            },\n        )\n\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&url=application.js\"\n        ).json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"file\"\n        assert response[0][\"abs_path\"] == \"http://example.com/application.js\"\n        assert response[0][\"headers\"] == file_headers\n        self.assert_download_matches_file(response[0][\"url\"], file)\n\n    def test_query_by_url_from_legacy_bundle(self):\n        self.login_as(user=self.user)\n\n        url = reverse(\n            \"sentry-api-0-project-artifact-lookup\",\n            kwargs={\n                \"organization_slug\": self.project.organization.slug,\n                \"project_slug\": self.project.slug,\n            },\n        )\n\n        assert read_artifact_index(self.release, None) is None\n\n        archive1, archive1_file = self.create_archive(\n            fields={},\n            files={\n                \"foo\": \"foo\",\n                \"bar\": \"bar\",\n            },\n        )\n\n        assert read_artifact_index(self.release, None) == {\n            \"files\": {\n                \"fake://foo\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"foo\",\n                    \"sha1\": \"0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33\",\n                    \"size\": 3,\n                },\n                \"fake://bar\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"bar\",\n                    \"sha1\": \"62cdb7020ff920e5aa642c3d4066950dd1f01f4d\",\n                    \"size\": 3,\n                },\n            },\n        }\n\n        # Should download 1 archives as both files are within a single archive\n        response = self.client.get(f\"{url}?release={self.release.version}&url=foo&url=bar\").json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n\n        # Override `bar` file inside the index. It will now have different `sha1`` and different `archive_ident` as it comes from other archive.\n        archive2, archive2_file = self.create_archive(\n            fields={},\n            files={\n                \"bar\": \"BAR\",\n            },\n        )\n\n        assert read_artifact_index(self.release, None) == {\n            \"files\": {\n                \"fake://foo\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"foo\",\n                    \"sha1\": \"0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33\",\n                    \"size\": 3,\n                },\n                \"fake://bar\": {\n                    \"archive_ident\": archive2.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"bar\",\n                    \"sha1\": \"a5d5c1bba91fdb6c669e1ae0413820885bbfc455\",\n                    \"size\": 3,\n                },\n            },\n        }\n\n        response = self.client.get(f\"{url}?release={self.release.version}&url=foo\").json()\n\n        assert len(response) == 2\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n        assert response[1][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[1][\"url\"], archive2_file)\n\n        response = self.client.get(f\"{url}?release={self.release.version}&url=bar\").json()\n\n        assert len(response) == 2\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n        assert response[1][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[1][\"url\"], archive2_file)\n\n    def test_query_by_url_and_dist_from_legacy_bundle(self):\n        self.login_as(user=self.user)\n\n        url = reverse(\n            \"sentry-api-0-project-artifact-lookup\",\n            kwargs={\n                \"organization_slug\": self.project.organization.slug,\n                \"project_slug\": self.project.slug,\n            },\n        )\n\n        dist = self.release.add_dist(\"foo\")\n\n        archive1, archive1_file = self.create_archive(\n            fields={},\n            files={\n                \"foo\": \"foo\",\n                \"bar\": \"bar\",\n            },\n            dist=dist,\n        )\n\n        # No index for dist-less requests.\n        assert read_artifact_index(self.release, None) is None\n\n        assert read_artifact_index(self.release, dist) == {\n            \"files\": {\n                \"fake://foo\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"foo\",\n                    \"sha1\": \"0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33\",\n                    \"size\": 3,\n                },\n                \"fake://bar\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"bar\",\n                    \"sha1\": \"62cdb7020ff920e5aa642c3d4066950dd1f01f4d\",\n                    \"size\": 3,\n                },\n            },\n        }\n\n        # Should download 1 archives as both files are within a single archive\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&url=foo&url=bar&dist=foo\"\n        ).json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n\n        # Override `bar` file inside the index. It will now have different `sha1`` and different `archive_ident` as it comes from other archive.\n        archive2, archive2_file = self.create_archive(\n            fields={},\n            files={\n                \"bar\": \"BAR\",\n            },\n            dist=dist,\n        )\n\n        assert read_artifact_index(self.release, dist) == {\n            \"files\": {\n                \"fake://foo\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"foo\",\n                    \"sha1\": \"0beec7b5ea3f0fdbc95d0dd47f3c5bc275da8a33\",\n                    \"size\": 3,\n                },\n                \"fake://bar\": {\n                    \"archive_ident\": archive2.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"bar\",\n                    \"sha1\": \"a5d5c1bba91fdb6c669e1ae0413820885bbfc455\",\n                    \"size\": 3,\n                },\n            },\n        }\n\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&dist={dist.name}&url=foo\"\n        ).json()\n\n        assert len(response) == 2\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n        assert response[1][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[1][\"url\"], archive2_file)\n\n        # Should download 2 archives as they have different `archive_ident`\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&dist={dist.name}&url=bar\"\n        ).json()\n\n        assert len(response) == 2\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n        assert response[1][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[1][\"url\"], archive2_file)\n\n    @freeze_time(\"2023-05-23 10:00:00\")\n    def test_renewal_with_debug_id(self):\n        with self.options({\"sourcemaps.artifact-bundles.enable-renewal\": 1.0}):\n            for days_before, expected_date_added, debug_id in (\n                (\n                    2,\n                    datetime.now(tz=pytz.UTC) - timedelta(days=2),\n                    \"2432d9ad-fe87-4f77-938d-50cc9b2b2e2a\",\n                ),\n                (35, datetime.now(tz=pytz.UTC), \"ef88bc3e-d334-4809-9723-5c5dbc8bd4e9\"),\n            ):\n                file = make_compressed_zip_file(\n                    \"bundle_c.zip\",\n                    {\n                        \"path/in/zip/c\": {\n                            \"url\": \"~/path/to/app.js\",\n                            \"type\": \"source_map\",\n                            \"content\": b\"baz\",\n                            \"headers\": {\n                                \"debug-id\": debug_id,\n                            },\n                        },\n                    },\n                )\n                bundle_id = uuid4()\n                date_added = datetime.now(tz=pytz.UTC) - timedelta(days=days_before)\n\n                artifact_bundle = ArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    bundle_id=bundle_id,\n                    file=file,\n                    artifact_count=1,\n                    date_added=date_added,\n                )\n                ProjectArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    project_id=self.project.id,\n                    artifact_bundle=artifact_bundle,\n                    date_added=date_added,\n                )\n                DebugIdArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    debug_id=debug_id,\n                    artifact_bundle=artifact_bundle,\n                    source_file_type=SourceFileType.SOURCE_MAP.value,\n                    date_added=date_added,\n                )\n\n                self.login_as(user=self.user)\n\n                url = reverse(\n                    \"sentry-api-0-project-artifact-lookup\",\n                    kwargs={\n                        \"organization_slug\": self.project.organization.slug,\n                        \"project_slug\": self.project.slug,\n                    },\n                )\n\n                with self.tasks():\n                    self.client.get(f\"{url}?debug_id={debug_id}\")\n\n                assert (\n                    ArtifactBundle.objects.get(id=artifact_bundle.id).date_added\n                    == expected_date_added\n                )\n                assert (\n                    ProjectArtifactBundle.objects.get(\n                        artifact_bundle_id=artifact_bundle.id\n                    ).date_added\n                    == expected_date_added\n                )\n                assert (\n                    DebugIdArtifactBundle.objects.get(\n                        artifact_bundle_id=artifact_bundle.id\n                    ).date_added\n                    == expected_date_added\n                )\n\n    @freeze_time(\"2023-05-23 10:00:00\")\n    def test_renewal_with_url(self):\n        with self.options({\"sourcemaps.artifact-bundles.enable-renewal\": 1.0}):\n            file = make_compressed_zip_file(\n                \"bundle_c.zip\",\n                {\n                    \"path/in/zip/c\": {\n                        \"url\": \"~/path/to/app.js\",\n                        \"type\": \"source_map\",\n                        \"content\": b\"baz\",\n                    },\n                },\n            )\n\n            for days_before, expected_date_added, release in (\n                (\n                    2,\n                    datetime.now(tz=pytz.UTC) - timedelta(days=2),\n                    self.create_release(version=\"1.0\"),\n                ),\n                (35, datetime.now(tz=pytz.UTC), self.create_release(version=\"2.0\")),\n            ):\n                dist = release.add_dist(\"android\")\n                bundle_id = uuid4()\n                date_added = datetime.now(tz=pytz.UTC) - timedelta(days=days_before)\n\n                artifact_bundle = ArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    bundle_id=bundle_id,\n                    file=file,\n                    artifact_count=1,\n                    date_added=date_added,\n                )\n                ProjectArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    project_id=self.project.id,\n                    artifact_bundle=artifact_bundle,\n                    date_added=date_added,\n                )\n                ReleaseArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    release_name=release.version,\n                    dist_name=dist.name,\n                    artifact_bundle=artifact_bundle,\n                    date_added=date_added,\n                )\n\n                self.login_as(user=self.user)\n\n                url = reverse(\n                    \"sentry-api-0-project-artifact-lookup\",\n                    kwargs={\n                        \"organization_slug\": self.project.organization.slug,\n                        \"project_slug\": self.project.slug,\n                    },\n                )\n\n                with self.tasks():\n                    self.client.get(\n                        f\"{url}?release={release.version}&dist={dist.name}&url=path/to/app\"\n                    )\n\n                assert (\n                    ArtifactBundle.objects.get(id=artifact_bundle.id).date_added\n                    == expected_date_added\n                )\n                assert (\n                    ProjectArtifactBundle.objects.get(\n                        artifact_bundle_id=artifact_bundle.id\n                    ).date_added\n                    == expected_date_added\n                )\n                assert (\n                    ReleaseArtifactBundle.objects.get(\n                        artifact_bundle_id=artifact_bundle.id\n                    ).date_added\n                    == expected_date_added\n                )\n"], "fixing_code": ["import logging\nfrom datetime import datetime, timedelta\nfrom typing import List, Mapping, Optional, Sequence, Set, Tuple\n\nimport pytz\nfrom django.db import transaction\nfrom django.http import Http404, HttpResponse, StreamingHttpResponse\nfrom rest_framework.request import Request\nfrom rest_framework.response import Response\nfrom symbolic import SymbolicError, normalize_debug_id\n\nfrom sentry import options, ratelimits\nfrom sentry.api.base import region_silo_endpoint\nfrom sentry.api.bases.project import ProjectEndpoint, ProjectReleasePermission\nfrom sentry.api.endpoints.debug_files import has_download_permission\nfrom sentry.api.serializers import serialize\nfrom sentry.auth.system import is_system_auth\nfrom sentry.lang.native.sources import get_internal_artifact_lookup_source_url\nfrom sentry.models import (\n    ArtifactBundle,\n    DebugIdArtifactBundle,\n    Distribution,\n    Project,\n    ProjectArtifactBundle,\n    Release,\n    ReleaseArtifactBundle,\n    ReleaseFile,\n)\nfrom sentry.utils import metrics\n\nlogger = logging.getLogger(\"sentry.api\")\n\n# The marker for \"release\" bundles\nRELEASE_BUNDLE_TYPE = \"release.bundle\"\n# The number of bundles (\"artifact\" or \"release\") that we query\nMAX_BUNDLES_QUERY = 5\n# The number of files returned by the `get_releasefiles` query\nMAX_RELEASEFILES_QUERY = 10\n# Number of days that determine whether an artifact bundle is ready for being renewed.\nAVAILABLE_FOR_RENEWAL_DAYS = 30\n\n\n@region_silo_endpoint\nclass ProjectArtifactLookupEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    def download_file(self, download_id, project: Project):\n        ty, ty_id = download_id.split(\"/\")\n\n        rate_limited = ratelimits.is_limited(\n            project=project,\n            key=f\"rl:ArtifactLookupEndpoint:download:{download_id}:{project.id}\",\n            limit=10,\n        )\n        if rate_limited:\n            logger.info(\n                \"notification.rate_limited\",\n                extra={\"project_id\": project.id, \"file_id\": download_id},\n            )\n            return HttpResponse({\"Too many download requests\"}, status=429)\n\n        file = None\n        if ty == \"artifact_bundle\":\n            file = (\n                ArtifactBundle.objects.filter(\n                    id=ty_id,\n                    projectartifactbundle__project_id=project.id,\n                )\n                .select_related(\"file\")\n                .first()\n            )\n        elif ty == \"release_file\":\n            # NOTE: `ReleaseFile` does have a `project_id`, but that seems to\n            # be always empty, so using the `organization_id` instead.\n            file = (\n                ReleaseFile.objects.filter(id=ty_id, organization_id=project.organization.id)\n                .select_related(\"file\")\n                .first()\n            )\n\n        if file is None:\n            raise Http404\n        file = file.file\n\n        try:\n            fp = file.getfile()\n            response = StreamingHttpResponse(\n                iter(lambda: fp.read(4096), b\"\"), content_type=\"application/octet-stream\"\n            )\n            response[\"Content-Length\"] = file.size\n            response[\"Content-Disposition\"] = f'attachment; filename=\"{file.name}\"'\n            return response\n        except OSError:\n            raise Http404\n\n    def get(self, request: Request, project: Project) -> Response:\n        \"\"\"\n        List a Project's Individual Artifacts or Bundles\n        ````````````````````````````````````````\n\n        Retrieve a list of individual artifacts or artifact bundles for a given project.\n\n        :pparam string organization_slug: the slug of the organization to query.\n        :pparam string project_slug: the slug of the project to query.\n        :qparam string debug_id: if set, will query and return the artifact\n                                 bundle that matches the given `debug_id`.\n        :qparam string url: if set, will query and return all the individual\n                            artifacts, or artifact bundles that contain files\n                            that match the `url`. This is using a substring-match.\n        :qparam string release: used in conjunction with `url`.\n        :qparam string dist: used in conjunction with `url`.\n\n        :auth: required\n        \"\"\"\n        if (download_id := request.GET.get(\"download\")) is not None:\n            if has_download_permission(request, project):\n                return self.download_file(download_id, project)\n            else:\n                return Response(status=403)\n\n        debug_id = request.GET.get(\"debug_id\")\n        try:\n            debug_id = normalize_debug_id(debug_id)\n        except SymbolicError:\n            pass\n        url = request.GET.get(\"url\")\n        release_name = request.GET.get(\"release\")\n        dist_name = request.GET.get(\"dist\")\n\n        used_artifact_bundles = dict()\n        bundle_file_ids = set()\n\n        def update_bundles(inner_bundles: Set[Tuple[int, datetime, int]]):\n            for (bundle_id, date_added, file_id) in inner_bundles:\n                used_artifact_bundles[bundle_id] = date_added\n                bundle_file_ids.add((\"artifact_bundle\", bundle_id, file_id))\n\n        if debug_id:\n            bundles = get_artifact_bundles_containing_debug_id(debug_id, project)\n            update_bundles(bundles)\n\n        individual_files = set()\n        if url and release_name and not bundle_file_ids:\n            # Get both the newest X release artifact bundles,\n            # and also query the legacy artifact bundles. One of those should have the\n            # file we are looking for. We want to return more here, even bundles that\n            # do *not* contain the file, rather than opening up each bundle. We want to\n            # avoid opening up bundles at all cost.\n            bundles = get_release_artifacts(project, release_name, dist_name)\n            update_bundles(bundles)\n\n            release, dist = try_resolve_release_dist(project, release_name, dist_name)\n            if release:\n                for (releasefile_id, file_id) in get_legacy_release_bundles(release, dist):\n                    bundle_file_ids.add((\"release_file\", releasefile_id, file_id))\n                individual_files = get_legacy_releasefile_by_file_url(release, dist, url)\n\n        if options.get(\"sourcemaps.artifact-bundles.enable-renewal\") == 1.0:\n            with metrics.timer(\"artifact_lookup.get.renew_artifact_bundles\"):\n                # Before constructing the response, we want to update the artifact bundles renewal date.\n                renew_artifact_bundles(used_artifact_bundles)\n\n        # Then: Construct our response\n        url_constructor = UrlConstructor(request, project)\n\n        found_artifacts = []\n        # NOTE: the reason we use the `file_id` as the `id` we return is because\n        # downstream symbolicator relies on that for its internal caching.\n        # We do not want to hard-refresh those caches quite yet, and the `id`\n        # should also be as unique as possible, which the `file_id` is.\n        for (ty, ty_id, file_id) in bundle_file_ids:\n            found_artifacts.append(\n                {\n                    \"id\": str(file_id),\n                    \"type\": \"bundle\",\n                    \"url\": url_constructor.url_for_file_id(ty, ty_id),\n                }\n            )\n\n        for release_file in individual_files:\n            found_artifacts.append(\n                {\n                    \"id\": str(release_file.file.id),\n                    \"type\": \"file\",\n                    \"url\": url_constructor.url_for_file_id(\"release_file\", release_file.id),\n                    # The `name` is the url/abs_path of the file,\n                    # as in: `\"~/path/to/file.min.js\"`.\n                    \"abs_path\": release_file.name,\n                    # These headers should ideally include the `Sourcemap` reference\n                    \"headers\": release_file.file.headers,\n                }\n            )\n\n        # make sure we have a stable sort order for tests\n        found_artifacts.sort(key=lambda x: int(x[\"id\"]))\n\n        # NOTE: We do not paginate this response, as we have very tight limits on all the individual queries.\n        return Response(serialize(found_artifacts, request.user))\n\n\ndef renew_artifact_bundles(used_artifact_bundles: Mapping[int, datetime]):\n    # We take a snapshot in time that MUST be consistent across all updates.\n    now = datetime.now(tz=pytz.UTC)\n    # We compute the threshold used to determine whether we want to renew the specific bundle.\n    threshold_date = now - timedelta(days=AVAILABLE_FOR_RENEWAL_DAYS)\n\n    for (artifact_bundle_id, date_added) in used_artifact_bundles.items():\n        metrics.incr(\"artifact_lookup.get.renew_artifact_bundles.should_be_renewed\")\n        # We perform the condition check also before running the query, in order to reduce the amount of queries to the\n        # database.\n        if date_added <= threshold_date:\n            metrics.incr(\"artifact_lookup.get.renew_artifact_bundles.renewed\")\n            # We want to use a transaction, in order to keep the `date_added` consistent across multiple tables.\n            with transaction.atomic():\n                # We check again for the date_added condition in order to achieve consistency, this is done because\n                # the `can_be_renewed` call is using a time which differs from the one of the actual update in the db.\n                updated_rows_count = ArtifactBundle.objects.filter(\n                    id=artifact_bundle_id, date_added__lte=threshold_date\n                ).update(date_added=now)\n                # We want to make cascading queries only if there were actual changes in the db.\n                if updated_rows_count > 0:\n                    ProjectArtifactBundle.objects.filter(\n                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date\n                    ).update(date_added=now)\n                    ReleaseArtifactBundle.objects.filter(\n                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date\n                    ).update(date_added=now)\n                    DebugIdArtifactBundle.objects.filter(\n                        artifact_bundle_id=artifact_bundle_id, date_added__lte=threshold_date\n                    ).update(date_added=now)\n\n\ndef get_artifact_bundles_containing_debug_id(\n    debug_id: str, project: Project\n) -> Set[Tuple[int, datetime, int]]:\n    # We want to have the newest `File` for each `debug_id`.\n    return set(\n        ArtifactBundle.objects.filter(\n            organization_id=project.organization.id,\n            debugidartifactbundle__debug_id=debug_id,\n        )\n        .values_list(\"id\", \"date_added\", \"file_id\")\n        .order_by(\"-date_uploaded\")[:1]\n    )\n\n\ndef get_release_artifacts(\n    project: Project,\n    release_name: str,\n    dist_name: Optional[str],\n) -> Set[Tuple[int, datetime, int]]:\n    return set(\n        ArtifactBundle.objects.filter(\n            organization_id=project.organization.id,\n            projectartifactbundle__project_id=project.id,\n            releaseartifactbundle__release_name=release_name,\n            # In case no dist is provided, we will fall back to \"\" which is the NULL equivalent for our tables.\n            # See `_create_artifact_bundle` in `src/sentry/tasks/assemble.py` for the reference.\n            releaseartifactbundle__dist_name=dist_name or \"\",\n        )\n        .values_list(\"id\", \"date_added\", \"file_id\")\n        .order_by(\"-date_uploaded\")[:MAX_BUNDLES_QUERY]\n    )\n\n\ndef try_resolve_release_dist(\n    project: Project, release_name: str, dist_name: Optional[str]\n) -> Tuple[Optional[Release], Optional[Distribution]]:\n    release = None\n    dist = None\n    try:\n        release = Release.objects.get(\n            organization_id=project.organization_id,\n            projects=project,\n            version=release_name,\n        )\n\n        # We cannot query for dist without a release anyway\n        if dist_name:\n            dist = Distribution.objects.get(release=release, name=dist_name)\n    except (Release.DoesNotExist, Distribution.DoesNotExist):\n        pass\n    except Exception as exc:\n        logger.error(\"Failed to read\", exc_info=exc)\n\n    return release, dist\n\n\ndef get_legacy_release_bundles(\n    release: Release, dist: Optional[Distribution]\n) -> Set[Tuple[int, int]]:\n    return set(\n        ReleaseFile.objects.filter(\n            release_id=release.id,\n            dist_id=dist.id if dist else None,\n            # a `ReleaseFile` with `0` artifacts represents a release archive,\n            # see the comment above the definition of `artifact_count`.\n            artifact_count=0,\n            # similarly the special `type` is also used for release archives.\n            file__type=RELEASE_BUNDLE_TYPE,\n        )\n        .select_related(\"file\")\n        .values_list(\"id\", \"file_id\")\n        # TODO: this `order_by` might be incredibly slow\n        # we want to have a hard limit on the returned bundles here. and we would\n        # want to pick the most recently uploaded ones. that should mostly be\n        # relevant for customers that upload multiple bundles, or are uploading\n        # newer files for existing releases. In that case the symbolication is\n        # already degraded, so meh...\n        # .order_by(\"-file__timestamp\")\n        [:MAX_BUNDLES_QUERY]\n    )\n\n\ndef get_legacy_releasefile_by_file_url(\n    release: Release, dist: Optional[Distribution], url: List[str]\n) -> Sequence[ReleaseFile]:\n    # Exclude files which are also present in archive:\n    return (\n        ReleaseFile.public_objects.filter(\n            release_id=release.id,\n            dist_id=dist.id if dist else None,\n        )\n        .exclude(artifact_count=0)\n        .select_related(\"file\")\n    ).filter(name__icontains=url)[:MAX_RELEASEFILES_QUERY]\n\n\nclass UrlConstructor:\n    def __init__(self, request: Request, project: Project):\n        if is_system_auth(request.auth):\n            self.base_url = get_internal_artifact_lookup_source_url(project)\n        else:\n            self.base_url = request.build_absolute_uri(request.path)\n\n    def url_for_file_id(self, ty: str, file_id: int) -> str:\n        # NOTE: Returning a self-route that requires authentication (via Bearer token)\n        # is not really forward compatible with a pre-signed URL that does not\n        # require any authentication or headers whatsoever.\n        # This also requires a workaround in Symbolicator, as its generic http\n        # downloader blocks \"internal\" IPs, whereas the internal Sentry downloader\n        # is explicitly exempt.\n        return f\"{self.base_url}?download={ty}/{file_id}\"\n", "import logging\nimport posixpath\nimport re\n\nimport jsonschema\nfrom django.db import router\nfrom django.db.models import Q\nfrom django.http import Http404, HttpResponse, StreamingHttpResponse\nfrom rest_framework.request import Request\nfrom rest_framework.response import Response\nfrom symbolic import SymbolicError, normalize_debug_id\n\nfrom sentry import ratelimits, roles\nfrom sentry.api.base import region_silo_endpoint\nfrom sentry.api.bases.project import ProjectEndpoint, ProjectReleasePermission\nfrom sentry.api.exceptions import ResourceDoesNotExist\nfrom sentry.api.paginator import OffsetPaginator\nfrom sentry.api.serializers import serialize\nfrom sentry.auth.superuser import is_active_superuser\nfrom sentry.auth.system import is_system_auth\nfrom sentry.constants import DEBUG_FILES_ROLE_DEFAULT, KNOWN_DIF_FORMATS\nfrom sentry.models import (\n    File,\n    FileBlobOwner,\n    OrganizationMember,\n    ProjectDebugFile,\n    Release,\n    ReleaseFile,\n    create_files_from_dif_zip,\n)\nfrom sentry.models.release import get_artifact_counts\nfrom sentry.tasks.assemble import (\n    AssembleTask,\n    ChunkFileState,\n    get_assemble_status,\n    set_assemble_status,\n)\nfrom sentry.utils import json\nfrom sentry.utils.db import atomic_transaction\n\nlogger = logging.getLogger(\"sentry.api\")\nERR_FILE_EXISTS = \"A file matching this debug identifier already exists\"\nDIF_MIMETYPES = {v: k for k, v in KNOWN_DIF_FORMATS.items()}\n_release_suffix = re.compile(r\"^(.*)\\s+\\(([^)]+)\\)\\s*$\")\n\n\ndef upload_from_request(request, project):\n    if \"file\" not in request.data:\n        return Response({\"detail\": \"Missing uploaded file\"}, status=400)\n    fileobj = request.data[\"file\"]\n    files = create_files_from_dif_zip(fileobj, project=project)\n    return Response(serialize(files, request.user), status=201)\n\n\ndef has_download_permission(request, project):\n    if is_system_auth(request.auth) or is_active_superuser(request):\n        return True\n\n    if not request.user.is_authenticated:\n        return False\n\n    organization = project.organization\n    required_role = organization.get_option(\"sentry:debug_files_role\") or DEBUG_FILES_ROLE_DEFAULT\n\n    if request.user.is_sentry_app:\n        if roles.get(required_role).priority > roles.get(\"member\").priority:\n            return request.access.has_scope(\"project:write\")\n        else:\n            return request.access.has_scope(\"project:read\")\n\n    try:\n        current_role = (\n            OrganizationMember.objects.filter(organization=organization, user_id=request.user.id)\n            .values_list(\"role\", flat=True)\n            .get()\n        )\n    except OrganizationMember.DoesNotExist:\n        return False\n\n    return roles.get(current_role).priority >= roles.get(required_role).priority\n\n\n@region_silo_endpoint\nclass DebugFilesEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    def download(self, debug_file_id, project):\n        rate_limited = ratelimits.is_limited(\n            project=project,\n            key=f\"rl:DSymFilesEndpoint:download:{debug_file_id}:{project.id}\",\n            limit=10,\n        )\n        if rate_limited:\n            logger.info(\n                \"notification.rate_limited\",\n                extra={\"project_id\": project.id, \"project_debug_file_id\": debug_file_id},\n            )\n            return HttpResponse({\"Too many download requests\"}, status=429)\n\n        debug_file = ProjectDebugFile.objects.filter(\n            id=debug_file_id, project_id=project.id\n        ).first()\n\n        if debug_file is None:\n            raise Http404\n\n        try:\n            fp = debug_file.file.getfile()\n            response = StreamingHttpResponse(\n                iter(lambda: fp.read(4096), b\"\"), content_type=\"application/octet-stream\"\n            )\n            response[\"Content-Length\"] = debug_file.file.size\n            response[\"Content-Disposition\"] = 'attachment; filename=\"{}{}\"'.format(\n                posixpath.basename(debug_file.debug_id),\n                debug_file.file_extension,\n            )\n            return response\n        except OSError:\n            raise Http404\n\n    def get(self, request: Request, project) -> Response:\n        \"\"\"\n        List a Project's Debug Information Files\n        ````````````````````````````````````````\n\n        Retrieve a list of debug information files for a given project.\n\n        :pparam string organization_slug: the slug of the organization the\n                                          file belongs to.\n        :pparam string project_slug: the slug of the project to list the\n                                     DIFs of.\n        :qparam string query: If set, this parameter is used to locate DIFs with.\n        :qparam string id: If set, the specified DIF will be sent in the response.\n        :qparam string file_formats: If set, only DIFs with these formats will be returned.\n        :auth: required\n        \"\"\"\n        download_requested = request.GET.get(\"id\") is not None\n        if download_requested and (has_download_permission(request, project)):\n            return self.download(request.GET.get(\"id\"), project)\n        elif download_requested:\n            return Response(status=403)\n\n        code_id = request.GET.get(\"code_id\")\n        debug_id = request.GET.get(\"debug_id\")\n        query = request.GET.get(\"query\")\n        file_formats = request.GET.getlist(\"file_formats\")\n\n        # If this query contains a debug identifier, normalize it to allow for\n        # more lenient queries (e.g. supporting Breakpad ids). Use the index to\n        # speed up such queries.\n        if query and len(query) <= 45 and not debug_id:\n            try:\n                debug_id = normalize_debug_id(query.strip())\n            except SymbolicError:\n                pass\n\n        if debug_id:\n            # If a debug ID is specified, do not consider the stored code\n            # identifier and strictly filter by debug identifier. Often there\n            # are mismatches in the code identifier in PEs.\n            q = Q(debug_id__exact=debug_id)\n        elif code_id:\n            q = Q(code_id__exact=code_id)\n        elif query:\n            q = (\n                Q(object_name__icontains=query)\n                | Q(debug_id__icontains=query)\n                | Q(code_id__icontains=query)\n                | Q(cpu_name__icontains=query)\n                | Q(file__headers__icontains=query)\n            )\n\n            known_file_format = DIF_MIMETYPES.get(query)\n            if known_file_format:\n                q |= Q(file__headers__icontains=known_file_format)\n        else:\n            q = Q()\n\n        file_format_q = Q()\n        for file_format in file_formats:\n            known_file_format = DIF_MIMETYPES.get(file_format)\n            if known_file_format:\n                file_format_q |= Q(file__headers__icontains=known_file_format)\n\n        q &= file_format_q\n\n        queryset = ProjectDebugFile.objects.filter(q, project_id=project.id).select_related(\"file\")\n\n        return self.paginate(\n            request=request,\n            queryset=queryset,\n            order_by=\"-id\",\n            paginator_cls=OffsetPaginator,\n            default_per_page=20,\n            on_results=lambda x: serialize(x, request.user),\n        )\n\n    def delete(self, request: Request, project) -> Response:\n        \"\"\"\n        Delete a specific Project's Debug Information File\n        ```````````````````````````````````````````````````\n\n        Delete a debug information file for a given project.\n\n        :pparam string organization_slug: the slug of the organization the\n                                          file belongs to.\n        :pparam string project_slug: the slug of the project to delete the\n                                     DIF.\n        :qparam string id: The id of the DIF to delete.\n        :auth: required\n        \"\"\"\n\n        if request.GET.get(\"id\") and (request.access.has_scope(\"project:write\")):\n            with atomic_transaction(using=router.db_for_write(File)):\n                debug_file = (\n                    ProjectDebugFile.objects.filter(id=request.GET.get(\"id\"), project_id=project.id)\n                    .select_related(\"file\")\n                    .first()\n                )\n                if debug_file is not None:\n                    debug_file.delete()\n                    return Response(status=204)\n\n        return Response(status=404)\n\n    def post(self, request: Request, project) -> Response:\n        \"\"\"\n        Upload a New File\n        `````````````````\n\n        Upload a new debug information file for the given release.\n\n        Unlike other API requests, files must be uploaded using the\n        traditional multipart/form-data content-type.\n\n        The file uploaded is a zip archive of a Apple .dSYM folder which\n        contains the individual debug images.  Uploading through this endpoint\n        will create different files for the contained images.\n\n        :pparam string organization_slug: the slug of the organization the\n                                          release belongs to.\n        :pparam string project_slug: the slug of the project to change the\n                                     release of.\n        :param file file: the multipart encoded file.\n        :auth: required\n        \"\"\"\n        return upload_from_request(request, project=project)\n\n\n@region_silo_endpoint\nclass UnknownDebugFilesEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    def get(self, request: Request, project) -> Response:\n        checksums = request.GET.getlist(\"checksums\")\n        missing = ProjectDebugFile.objects.find_missing(checksums, project=project)\n        return Response({\"missing\": missing})\n\n\n@region_silo_endpoint\nclass AssociateDSymFilesEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    # Legacy endpoint, kept for backwards compatibility\n    def post(self, request: Request, project) -> Response:\n        return Response({\"associatedDsymFiles\": []})\n\n\ndef find_missing_chunks(organization, chunks):\n    \"\"\"Returns a list of chunks which are missing for an org.\"\"\"\n    owned = set(\n        FileBlobOwner.objects.filter(\n            blob__checksum__in=chunks, organization_id=organization.id\n        ).values_list(\"blob__checksum\", flat=True)\n    )\n    return list(set(chunks) - owned)\n\n\n@region_silo_endpoint\nclass DifAssembleEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    def post(self, request: Request, project) -> Response:\n        \"\"\"\n        Assemble one or multiple chunks (FileBlob) into debug files\n        ````````````````````````````````````````````````````````````\n\n        :auth: required\n        \"\"\"\n        schema = {\n            \"type\": \"object\",\n            \"patternProperties\": {\n                \"^[0-9a-f]{40}$\": {\n                    \"type\": \"object\",\n                    \"required\": [\"name\", \"chunks\"],\n                    \"properties\": {\n                        \"name\": {\"type\": \"string\"},\n                        \"debug_id\": {\"type\": \"string\"},\n                        \"chunks\": {\n                            \"type\": \"array\",\n                            \"items\": {\"type\": \"string\", \"pattern\": \"^[0-9a-f]{40}$\"},\n                        },\n                    },\n                    \"additionalProperties\": True,\n                }\n            },\n            \"additionalProperties\": False,\n        }\n\n        try:\n            files = json.loads(request.body)\n            jsonschema.validate(files, schema)\n        except jsonschema.ValidationError as e:\n            return Response({\"error\": str(e).splitlines()[0]}, status=400)\n        except Exception:\n            return Response({\"error\": \"Invalid json body\"}, status=400)\n\n        file_response = {}\n\n        for checksum, file_to_assemble in files.items():\n            name = file_to_assemble.get(\"name\", None)\n            debug_id = file_to_assemble.get(\"debug_id\", None)\n            chunks = file_to_assemble.get(\"chunks\", [])\n\n            # First, check the cached assemble status. During assembling, a\n            # ProjectDebugFile will be created and we need to prevent a race\n            # condition.\n            state, detail = get_assemble_status(AssembleTask.DIF, project.id, checksum)\n            if state == ChunkFileState.OK:\n                file_response[checksum] = {\n                    \"state\": state,\n                    \"detail\": None,\n                    \"missingChunks\": [],\n                    \"dif\": detail,\n                }\n                continue\n            elif state is not None:\n                file_response[checksum] = {\"state\": state, \"detail\": detail, \"missingChunks\": []}\n                continue\n\n            # Next, check if this project already owns the ProjectDebugFile.\n            # This can under rare circumstances yield more than one file\n            # which is why we use first() here instead of get().\n            dif = (\n                ProjectDebugFile.objects.filter(project_id=project.id, checksum=checksum)\n                .select_related(\"file\")\n                .order_by(\"-id\")\n                .first()\n            )\n\n            if dif is not None:\n                file_response[checksum] = {\n                    \"state\": ChunkFileState.OK,\n                    \"detail\": None,\n                    \"missingChunks\": [],\n                    \"dif\": serialize(dif),\n                }\n                continue\n\n            # There is neither a known file nor a cached state, so we will\n            # have to create a new file.  Assure that there are checksums.\n            # If not, we assume this is a poll and report NOT_FOUND\n            if not chunks:\n                file_response[checksum] = {\"state\": ChunkFileState.NOT_FOUND, \"missingChunks\": []}\n                continue\n\n            # Check if all requested chunks have been uploaded.\n            missing_chunks = find_missing_chunks(project.organization, chunks)\n            if missing_chunks:\n                file_response[checksum] = {\n                    \"state\": ChunkFileState.NOT_FOUND,\n                    \"missingChunks\": missing_chunks,\n                }\n                continue\n\n            # We don't have a state yet, this means we can now start\n            # an assemble job in the background.\n            set_assemble_status(AssembleTask.DIF, project.id, checksum, ChunkFileState.CREATED)\n\n            from sentry.tasks.assemble import assemble_dif\n\n            assemble_dif.apply_async(\n                kwargs={\n                    \"project_id\": project.id,\n                    \"name\": name,\n                    \"debug_id\": debug_id,\n                    \"checksum\": checksum,\n                    \"chunks\": chunks,\n                }\n            )\n\n            file_response[checksum] = {\"state\": ChunkFileState.CREATED, \"missingChunks\": []}\n\n        return Response(file_response, status=200)\n\n\n@region_silo_endpoint\nclass SourceMapsEndpoint(ProjectEndpoint):\n    permission_classes = (ProjectReleasePermission,)\n\n    def get(self, request: Request, project) -> Response:\n        \"\"\"\n        List a Project's Source Map Archives\n        ````````````````````````````````````\n\n        Retrieve a list of source map archives (releases, later bundles) for a given project.\n\n        :pparam string organization_slug: the slug of the organization the\n                                          source map archive belongs to.\n        :pparam string project_slug: the slug of the project to list the\n                                     source map archives of.\n        :qparam string query: If set, this parameter is used to locate source map archives with.\n        :auth: required\n        \"\"\"\n        query = request.GET.get(\"query\")\n\n        try:\n            queryset = Release.objects.filter(\n                projects=project, organization_id=project.organization_id\n            ).values(\"id\", \"version\", \"date_added\")\n        except Release.DoesNotExist:\n            raise ResourceDoesNotExist\n\n        if query:\n            query_q = Q(version__icontains=query)\n\n            suffix_match = _release_suffix.match(query)\n            if suffix_match is not None:\n                query_q |= Q(version__icontains=\"%s+%s\" % suffix_match.groups())\n\n            queryset = queryset.filter(query_q)\n\n        def expose_release(release, count):\n            return {\n                \"type\": \"release\",\n                \"id\": release[\"id\"],\n                \"name\": release[\"version\"],\n                \"date\": release[\"date_added\"],\n                \"fileCount\": count,\n            }\n\n        def serialize_results(results):\n            file_count_map = get_artifact_counts([r[\"id\"] for r in results])\n            # In case we didn't find a file count for a specific release, we will return -1, signaling to the\n            # frontend that this release doesn't have one or more ReleaseFile.\n            return serialize(\n                [expose_release(r, file_count_map.get(r[\"id\"], -1)) for r in results], request.user\n            )\n\n        sort_by = request.GET.get(\"sortBy\", \"-date_added\")\n        if sort_by not in {\"-date_added\", \"date_added\"}:\n            return Response(\n                {\"error\": \"You can either sort via 'date_added' or '-date_added'\"}, status=400\n            )\n\n        return self.paginate(\n            request=request,\n            queryset=queryset,\n            order_by=sort_by,\n            paginator_cls=OffsetPaginator,\n            default_per_page=10,\n            on_results=serialize_results,\n        )\n\n    def delete(self, request: Request, project) -> Response:\n        \"\"\"\n        Delete an Archive\n        ```````````````````````````````````````````````````\n\n        Delete all artifacts inside given archive.\n\n        :pparam string organization_slug: the slug of the organization the\n                                            archive belongs to.\n        :pparam string project_slug: the slug of the project to delete the\n                                        archive of.\n        :qparam string name: The name of the archive to delete.\n        :auth: required\n        \"\"\"\n\n        archive_name = request.GET.get(\"name\")\n\n        if archive_name:\n            with atomic_transaction(using=router.db_for_write(ReleaseFile)):\n                release = Release.objects.get(\n                    organization_id=project.organization_id, projects=project, version=archive_name\n                )\n                if release is not None:\n                    release_files = ReleaseFile.objects.filter(release_id=release.id)\n                    release_files.delete()\n                    return Response(status=204)\n\n        return Response(status=404)\n", "import uuid\nimport zipfile\nfrom datetime import datetime, timedelta, timezone\nfrom io import BytesIO\nfrom uuid import uuid4\n\nimport pytz\nfrom django.urls import reverse\nfrom freezegun import freeze_time\n\nfrom sentry.models import (\n    ArtifactBundle,\n    DebugIdArtifactBundle,\n    File,\n    ProjectArtifactBundle,\n    ReleaseArtifactBundle,\n    ReleaseFile,\n    SourceFileType,\n)\nfrom sentry.models.releasefile import read_artifact_index, update_artifact_index\nfrom sentry.testutils import APITestCase\nfrom sentry.utils import json\n\n\ndef make_file(artifact_name, content, type=\"artifact.bundle\", headers=None):\n    file = File.objects.create(name=artifact_name, type=type, headers=(headers or {}))\n    file.putfile(BytesIO(content))\n    return file\n\n\ndef make_compressed_zip_file(artifact_name, files):\n    def remove_and_return(dictionary, key):\n        dictionary.pop(key)\n        return dictionary\n\n    compressed = BytesIO()\n    with zipfile.ZipFile(compressed, mode=\"w\") as zip_file:\n        for file_path, info in files.items():\n            zip_file.writestr(file_path, bytes(info[\"content\"]))\n\n        zip_file.writestr(\n            \"manifest.json\",\n            json.dumps(\n                {\n                    # We remove the \"content\" key in the original dict, thus no subsequent calls should be made.\n                    \"files\": {\n                        file_path: remove_and_return(info, \"content\")\n                        for file_path, info in files.items()\n                    }\n                }\n            ),\n        )\n    compressed.seek(0)\n\n    file = File.objects.create(name=artifact_name, type=\"artifact.bundle\")\n    file.putfile(compressed)\n\n    return file\n\n\nclass ArtifactLookupTest(APITestCase):\n    def assert_download_matches_file(self, url: str, file: File):\n        response = self.client.get(url)\n        with file.getfile() as file:\n            for chunk in response:\n                assert file.read(len(chunk)) == chunk\n\n    def create_archive(self, fields, files, dist=None):\n        manifest = dict(\n            fields, files={filename: {\"url\": f\"fake://{filename}\"} for filename in files}\n        )\n        buffer = BytesIO()\n        with zipfile.ZipFile(buffer, mode=\"w\") as zf:\n            zf.writestr(\"manifest.json\", json.dumps(manifest))\n            for filename, content in files.items():\n                zf.writestr(filename, content)\n\n        buffer.seek(0)\n        name = f\"release-artifacts-{uuid.uuid4().hex}.zip\"\n        file_ = File.objects.create(name=name, type=\"release.bundle\")\n        file_.putfile(buffer)\n        file_.update(timestamp=datetime(2021, 6, 11, 9, 13, 1, 317902, tzinfo=timezone.utc))\n\n        return (update_artifact_index(self.release, dist, file_), file_)\n\n    def test_query_by_debug_ids(self):\n        debug_id_a = \"aaaaaaaa-0000-0000-0000-000000000000\"\n        debug_id_b = \"bbbbbbbb-0000-0000-0000-000000000000\"\n        file_ab = make_compressed_zip_file(\n            \"bundle_ab.zip\",\n            {\n                \"path/in/zip/a\": {\n                    \"url\": \"~/path/to/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"foo_id\",\n                    \"headers\": {\n                        \"debug-id\": debug_id_a,\n                    },\n                },\n                \"path/in/zip/b\": {\n                    \"url\": \"~/path/to/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"bar_id\",\n                    \"headers\": {\n                        \"debug-id\": debug_id_b,\n                    },\n                },\n            },\n        )\n\n        bundle_id_ab = uuid4()\n        artifact_bundle_ab = ArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            bundle_id=bundle_id_ab,\n            file=file_ab,\n            artifact_count=2,\n        )\n        ProjectArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            project_id=self.project.id,\n            artifact_bundle=artifact_bundle_ab,\n        )\n        DebugIdArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            debug_id=debug_id_a,\n            artifact_bundle=artifact_bundle_ab,\n            source_file_type=SourceFileType.SOURCE_MAP.value,\n        )\n        DebugIdArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            debug_id=debug_id_b,\n            artifact_bundle=artifact_bundle_ab,\n            source_file_type=SourceFileType.SOURCE_MAP.value,\n        )\n\n        debug_id_c = \"cccccccc-0000-0000-0000-000000000000\"\n        file_c = make_compressed_zip_file(\n            \"bundle_c.zip\",\n            {\n                \"path/in/zip/c\": {\n                    \"url\": \"~/path/to/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"baz_id\",\n                    \"headers\": {\n                        \"debug-id\": debug_id_c,\n                    },\n                },\n            },\n        )\n\n        bundle_id_c = uuid4()\n        artifact_bundle_c = ArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            bundle_id=bundle_id_c,\n            file=file_c,\n            artifact_count=1,\n        )\n        ProjectArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            project_id=self.project.id,\n            artifact_bundle=artifact_bundle_c,\n        )\n        DebugIdArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            debug_id=debug_id_c,\n            artifact_bundle=artifact_bundle_c,\n            source_file_type=SourceFileType.SOURCE_MAP.value,\n        )\n\n        self.login_as(user=self.user)\n\n        url = reverse(\n            \"sentry-api-0-project-artifact-lookup\",\n            kwargs={\n                \"organization_slug\": self.project.organization.slug,\n                \"project_slug\": self.project.slug,\n            },\n        )\n\n        # query by one debug-id\n        response = self.client.get(f\"{url}?debug_id={debug_id_a}\").json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], file_ab)\n\n        # query by another debug-id pointing to the same bundle\n        response = self.client.get(f\"{url}?debug_id={debug_id_b}\").json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], file_ab)\n\n        # query by another debug-id pointing to different bundles\n        response = self.client.get(f\"{url}?debug_id={debug_id_c}\").json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], file_c)\n\n    def test_query_by_url(self):\n        debug_id_a = \"aaaaaaaa-0000-0000-0000-000000000000\"\n        file_a = make_compressed_zip_file(\n            \"bundle_a.zip\",\n            {\n                \"path/in/zip\": {\n                    \"url\": \"~/path/to/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"foo_url\",\n                    \"headers\": {\n                        \"debug-id\": debug_id_a,\n                    },\n                },\n            },\n        )\n        file_b = make_compressed_zip_file(\n            \"bundle_b.zip\",\n            {\n                \"path/in/zip_a\": {\n                    \"url\": \"~/path/to/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"foo_url\",\n                },\n                \"path/in/zip_b\": {\n                    \"url\": \"~/path/to/other/app.js\",\n                    \"type\": \"source_map\",\n                    \"content\": b\"bar_url\",\n                },\n            },\n        )\n\n        artifact_bundle_a = ArtifactBundle.objects.create(\n            organization_id=self.organization.id, bundle_id=uuid4(), file=file_a, artifact_count=1\n        )\n        DebugIdArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            debug_id=debug_id_a,\n            artifact_bundle=artifact_bundle_a,\n            source_file_type=SourceFileType.SOURCE_MAP.value,\n        )\n\n        artifact_bundle_b = ArtifactBundle.objects.create(\n            organization_id=self.organization.id, bundle_id=uuid4(), file=file_b, artifact_count=2\n        )\n\n        dist = self.release.add_dist(\"whatever\")\n\n        ReleaseArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            release_name=self.release.version,\n            dist_name=dist.name,\n            artifact_bundle=artifact_bundle_a,\n        )\n        ProjectArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            project_id=self.project.id,\n            artifact_bundle=artifact_bundle_a,\n        )\n        ReleaseArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            release_name=self.release.version,\n            dist_name=dist.name,\n            artifact_bundle=artifact_bundle_b,\n        )\n        ProjectArtifactBundle.objects.create(\n            organization_id=self.organization.id,\n            project_id=self.project.id,\n            artifact_bundle=artifact_bundle_b,\n        )\n\n        self.login_as(user=self.user)\n\n        url = reverse(\n            \"sentry-api-0-project-artifact-lookup\",\n            kwargs={\n                \"organization_slug\": self.project.organization.slug,\n                \"project_slug\": self.project.slug,\n            },\n        )\n\n        # query by url that is in both files, so we get both files\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&dist={dist.name}&url=path/to/app\"\n        ).json()\n\n        assert len(response) == 2\n        assert response[0][\"type\"] == \"bundle\"\n        assert response[1][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], file_a)\n        self.assert_download_matches_file(response[1][\"url\"], file_b)\n\n        # query by both debug-id and url with overlapping bundles\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&dist={dist.name}&debug_id={debug_id_a}&url=path/to/app\"\n        ).json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], file_a)\n\n    def test_query_by_url_from_releasefiles(self):\n        file_headers = {\"Sourcemap\": \"application.js.map\"}\n        file = make_file(\"application.js\", b\"wat\", \"release.file\", file_headers)\n        ReleaseFile.objects.create(\n            organization_id=self.project.organization_id,\n            release_id=self.release.id,\n            file=file,\n            name=\"http://example.com/application.js\",\n        )\n\n        self.login_as(user=self.user)\n\n        url = reverse(\n            \"sentry-api-0-project-artifact-lookup\",\n            kwargs={\n                \"organization_slug\": self.project.organization.slug,\n                \"project_slug\": self.project.slug,\n            },\n        )\n\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&url=application.js\"\n        ).json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"file\"\n        assert response[0][\"abs_path\"] == \"http://example.com/application.js\"\n        assert response[0][\"headers\"] == file_headers\n        self.assert_download_matches_file(response[0][\"url\"], file)\n\n    def test_query_by_url_from_legacy_bundle(self):\n        self.login_as(user=self.user)\n\n        url = reverse(\n            \"sentry-api-0-project-artifact-lookup\",\n            kwargs={\n                \"organization_slug\": self.project.organization.slug,\n                \"project_slug\": self.project.slug,\n            },\n        )\n\n        assert read_artifact_index(self.release, None) is None\n\n        archive1, archive1_file = self.create_archive(\n            fields={},\n            files={\n                \"foo\": \"foo1\",\n                \"bar\": \"bar1\",\n            },\n        )\n\n        assert read_artifact_index(self.release, None) == {\n            \"files\": {\n                \"fake://foo\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"foo\",\n                    \"sha1\": \"18a16d4530763ef43321d306c9f6c59ffed33072\",\n                    \"size\": 4,\n                },\n                \"fake://bar\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"bar\",\n                    \"sha1\": \"763675d6a1d8d0a3a28deca62bb68abd8baf86f3\",\n                    \"size\": 4,\n                },\n            },\n        }\n\n        # Should download 1 archives as both files are within a single archive\n        response = self.client.get(f\"{url}?release={self.release.version}&url=foo&url=bar\").json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n\n        # Override `bar` file inside the index. It will now have different `sha1`` and different `archive_ident` as it comes from other archive.\n        archive2, archive2_file = self.create_archive(\n            fields={},\n            files={\n                \"bar\": \"BAR1\",\n            },\n        )\n\n        assert read_artifact_index(self.release, None) == {\n            \"files\": {\n                \"fake://foo\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"foo\",\n                    \"sha1\": \"18a16d4530763ef43321d306c9f6c59ffed33072\",\n                    \"size\": 4,\n                },\n                \"fake://bar\": {\n                    \"archive_ident\": archive2.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"bar\",\n                    \"sha1\": \"7f9353c7b307875542883ba558a1692706fcad33\",\n                    \"size\": 4,\n                },\n            },\n        }\n\n        response = self.client.get(f\"{url}?release={self.release.version}&url=foo\").json()\n\n        assert len(response) == 2\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n        assert response[1][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[1][\"url\"], archive2_file)\n\n        response = self.client.get(f\"{url}?release={self.release.version}&url=bar\").json()\n\n        assert len(response) == 2\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n        assert response[1][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[1][\"url\"], archive2_file)\n\n    def test_query_by_url_and_dist_from_legacy_bundle(self):\n        self.login_as(user=self.user)\n\n        url = reverse(\n            \"sentry-api-0-project-artifact-lookup\",\n            kwargs={\n                \"organization_slug\": self.project.organization.slug,\n                \"project_slug\": self.project.slug,\n            },\n        )\n\n        dist = self.release.add_dist(\"foo\")\n\n        archive1, archive1_file = self.create_archive(\n            fields={},\n            files={\n                \"foo\": \"foo2\",\n                \"bar\": \"bar2\",\n            },\n            dist=dist,\n        )\n\n        # No index for dist-less requests.\n        assert read_artifact_index(self.release, None) is None\n\n        assert read_artifact_index(self.release, dist) == {\n            \"files\": {\n                \"fake://foo\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"foo\",\n                    \"sha1\": \"aaadd94977b8fbf3f6fb09fc3bbbc9edbdfa8427\",\n                    \"size\": 4,\n                },\n                \"fake://bar\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"bar\",\n                    \"sha1\": \"033c4846b506a4a48e32cdf54515c91d3499adb3\",\n                    \"size\": 4,\n                },\n            },\n        }\n\n        # Should download 1 archives as both files are within a single archive\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&url=foo&url=bar&dist=foo\"\n        ).json()\n\n        assert len(response) == 1\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n\n        # Override `bar` file inside the index. It will now have different `sha1`` and different `archive_ident` as it comes from other archive.\n        archive2, archive2_file = self.create_archive(\n            fields={},\n            files={\n                \"bar\": \"BAR2\",\n            },\n            dist=dist,\n        )\n\n        assert read_artifact_index(self.release, dist) == {\n            \"files\": {\n                \"fake://foo\": {\n                    \"archive_ident\": archive1.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"foo\",\n                    \"sha1\": \"aaadd94977b8fbf3f6fb09fc3bbbc9edbdfa8427\",\n                    \"size\": 4,\n                },\n                \"fake://bar\": {\n                    \"archive_ident\": archive2.ident,\n                    \"date_created\": \"2021-06-11T09:13:01.317902Z\",\n                    \"filename\": \"bar\",\n                    \"sha1\": \"528c5563f06a1e98954d17d365a219b68dd93baf\",\n                    \"size\": 4,\n                },\n            },\n        }\n\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&dist={dist.name}&url=foo\"\n        ).json()\n\n        assert len(response) == 2\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n        assert response[1][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[1][\"url\"], archive2_file)\n\n        # Should download 2 archives as they have different `archive_ident`\n        response = self.client.get(\n            f\"{url}?release={self.release.version}&dist={dist.name}&url=bar\"\n        ).json()\n\n        assert len(response) == 2\n        assert response[0][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[0][\"url\"], archive1_file)\n        assert response[1][\"type\"] == \"bundle\"\n        self.assert_download_matches_file(response[1][\"url\"], archive2_file)\n\n    @freeze_time(\"2023-05-23 10:00:00\")\n    def test_renewal_with_debug_id(self):\n        with self.options({\"sourcemaps.artifact-bundles.enable-renewal\": 1.0}):\n            for days_before, expected_date_added, debug_id in (\n                (\n                    2,\n                    datetime.now(tz=pytz.UTC) - timedelta(days=2),\n                    \"2432d9ad-fe87-4f77-938d-50cc9b2b2e2a\",\n                ),\n                (35, datetime.now(tz=pytz.UTC), \"ef88bc3e-d334-4809-9723-5c5dbc8bd4e9\"),\n            ):\n                file = make_compressed_zip_file(\n                    \"bundle_c.zip\",\n                    {\n                        \"path/in/zip/c\": {\n                            \"url\": \"~/path/to/app.js\",\n                            \"type\": \"source_map\",\n                            \"content\": b\"baz_renew\",\n                            \"headers\": {\n                                \"debug-id\": debug_id,\n                            },\n                        },\n                    },\n                )\n                bundle_id = uuid4()\n                date_added = datetime.now(tz=pytz.UTC) - timedelta(days=days_before)\n\n                artifact_bundle = ArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    bundle_id=bundle_id,\n                    file=file,\n                    artifact_count=1,\n                    date_added=date_added,\n                )\n                ProjectArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    project_id=self.project.id,\n                    artifact_bundle=artifact_bundle,\n                    date_added=date_added,\n                )\n                DebugIdArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    debug_id=debug_id,\n                    artifact_bundle=artifact_bundle,\n                    source_file_type=SourceFileType.SOURCE_MAP.value,\n                    date_added=date_added,\n                )\n\n                self.login_as(user=self.user)\n\n                url = reverse(\n                    \"sentry-api-0-project-artifact-lookup\",\n                    kwargs={\n                        \"organization_slug\": self.project.organization.slug,\n                        \"project_slug\": self.project.slug,\n                    },\n                )\n\n                with self.tasks():\n                    self.client.get(f\"{url}?debug_id={debug_id}\")\n\n                assert (\n                    ArtifactBundle.objects.get(id=artifact_bundle.id).date_added\n                    == expected_date_added\n                )\n                assert (\n                    ProjectArtifactBundle.objects.get(\n                        artifact_bundle_id=artifact_bundle.id\n                    ).date_added\n                    == expected_date_added\n                )\n                assert (\n                    DebugIdArtifactBundle.objects.get(\n                        artifact_bundle_id=artifact_bundle.id\n                    ).date_added\n                    == expected_date_added\n                )\n\n    @freeze_time(\"2023-05-23 10:00:00\")\n    def test_renewal_with_url(self):\n        with self.options({\"sourcemaps.artifact-bundles.enable-renewal\": 1.0}):\n            file = make_compressed_zip_file(\n                \"bundle_c.zip\",\n                {\n                    \"path/in/zip/c\": {\n                        \"url\": \"~/path/to/app.js\",\n                        \"type\": \"source_map\",\n                        \"content\": b\"baz_renew\",\n                    },\n                },\n            )\n\n            for days_before, expected_date_added, release in (\n                (\n                    2,\n                    datetime.now(tz=pytz.UTC) - timedelta(days=2),\n                    self.create_release(version=\"1.0\"),\n                ),\n                (35, datetime.now(tz=pytz.UTC), self.create_release(version=\"2.0\")),\n            ):\n                dist = release.add_dist(\"android\")\n                bundle_id = uuid4()\n                date_added = datetime.now(tz=pytz.UTC) - timedelta(days=days_before)\n\n                artifact_bundle = ArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    bundle_id=bundle_id,\n                    file=file,\n                    artifact_count=1,\n                    date_added=date_added,\n                )\n                ProjectArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    project_id=self.project.id,\n                    artifact_bundle=artifact_bundle,\n                    date_added=date_added,\n                )\n                ReleaseArtifactBundle.objects.create(\n                    organization_id=self.organization.id,\n                    release_name=release.version,\n                    dist_name=dist.name,\n                    artifact_bundle=artifact_bundle,\n                    date_added=date_added,\n                )\n\n                self.login_as(user=self.user)\n\n                url = reverse(\n                    \"sentry-api-0-project-artifact-lookup\",\n                    kwargs={\n                        \"organization_slug\": self.project.organization.slug,\n                        \"project_slug\": self.project.slug,\n                    },\n                )\n\n                with self.tasks():\n                    self.client.get(\n                        f\"{url}?release={release.version}&dist={dist.name}&url=path/to/app\"\n                    )\n\n                assert (\n                    ArtifactBundle.objects.get(id=artifact_bundle.id).date_added\n                    == expected_date_added\n                )\n                assert (\n                    ProjectArtifactBundle.objects.get(\n                        artifact_bundle_id=artifact_bundle.id\n                    ).date_added\n                    == expected_date_added\n                )\n                assert (\n                    ReleaseArtifactBundle.objects.get(\n                        artifact_bundle_id=artifact_bundle.id\n                    ).date_added\n                    == expected_date_added\n                )\n"], "filenames": ["src/sentry/api/endpoints/artifact_lookup.py", "src/sentry/api/endpoints/debug_files.py", "tests/sentry/api/endpoints/test_project_artifact_lookup.py"], "buggy_code_start_loc": [23, 98, 95], "buggy_code_end_loc": [315, 101, 604], "fixing_code_start_loc": [22, 98, 95], "fixing_code_end_loc": [344, 103, 611], "type": "CWE-863", "message": "Sentry is an error tracking and performance monitoring platform. Starting in version 8.21.0 and prior to version 23.5.2, an authenticated user can download a debug or artifact bundle from arbitrary organizations and projects with a known bundle ID. The user does not need to be a member of the organization or have permissions on the project. A patch was issued in version 23.5.2 to ensure authorization checks are properly scoped on requests to retrieve debug or artifact bundles. Authenticated users who do not have the necessary permissions on the particular project are no longer able to download them. Sentry SaaS users do not need to take any action. Self-Hosted Sentry users should upgrade to version 23.5.2 or higher.", "other": {"cve": {"id": "CVE-2023-36826", "sourceIdentifier": "security-advisories@github.com", "published": "2023-07-25T19:15:11.640", "lastModified": "2023-08-02T15:57:30.653", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Sentry is an error tracking and performance monitoring platform. Starting in version 8.21.0 and prior to version 23.5.2, an authenticated user can download a debug or artifact bundle from arbitrary organizations and projects with a known bundle ID. The user does not need to be a member of the organization or have permissions on the project. A patch was issued in version 23.5.2 to ensure authorization checks are properly scoped on requests to retrieve debug or artifact bundles. Authenticated users who do not have the necessary permissions on the particular project are no longer able to download them. Sentry SaaS users do not need to take any action. Self-Hosted Sentry users should upgrade to version 23.5.2 or higher."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 7.7, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.1, "impactScore": 4.0}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-863"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-285"}, {"lang": "en", "value": "CWE-863"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:sentry:sentry:*:*:*:*:*:*:*:*", "versionStartIncluding": "8.21.0", "versionEndExcluding": "23.5.2", "matchCriteriaId": "3BFE13DE-CDC1-443A-BA64-4F8B52C3C02E"}]}]}], "references": [{"url": "https://github.com/getsentry/sentry/commit/e932b15435bf36239431eaa3790a6bcfa47046a9", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/getsentry/sentry/pull/49680", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/getsentry/sentry/security/advisories/GHSA-m4hc-m2v6-hfw8", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/getsentry/sentry/commit/e932b15435bf36239431eaa3790a6bcfa47046a9"}}