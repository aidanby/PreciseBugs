{"buggy_code": ["# Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n* Add pypy-3.8 to test matrix\n* Add support for unverified checksum upload mode\n* Add dedicated exception for unverified email\n\n### Fixed\n* Fix downloading files with unverified checksum\n\n## [1.14.0] - 2021-12-23\n\n### Fixed\n* Relax constraint on arrow to allow for versions >= 1.0.2\n\n## [1.13.0] - 2021-10-24\n\n### Added\n* Add support for Python 3.10\n\n### Changed\n* Update a list with all capabilities\n\n### Fixed\n* Fix pypy selector in CI\n\n## [1.12.0] - 2021-08-06\n\n### Changed\n* The `importlib-metadata` requirement is less strictly bound now (just >=3.3.0 for python > 3.5).\n* `B2Api` `update_file_legal_hold` and `update_file_retention_setting` now return the set values \n\n### Added\n* `BucketIdNotFound` thrown based on B2 cloud response\n* `_clone` method to `FileVersion` and `DownloadVersion`\n* `delete`, `update_legal_hold`, `update_retention` and `download` methods added to `FileVersion`\n\n### Fixed\n* FileSimulator returns special file info headers properly\n\n### Removed\n* One unused import.\n\n## [1.11.0] - 2021-06-24\n\n### Changed\n* apiver `v2` interface released. `from b2sdk.v2 import ...` is now the recommended import, \n  but `from b2sdk.v1 import ...` works as before\n\n## [1.10.0] - 2021-06-23\n\n### Added\n* `get_fresh_state` method added to `FileVersion` and `Bucket`\n\n### Changed\n* `download_file_*` methods refactored to allow for inspecting DownloadVersion before downloading the whole file\n* `B2Api.get_file_info` returns a `FileVersion` object in v2\n* `B2RawApi` renamed to `B2RawHTTPApi`\n* `B2HTTP` tests are now common\n* `B2HttpApiConfig` class introduced to provide parameters like `user_agent_append` to `B2Api` without using internal classes in v2\n* `Bucket.update` returns a `Bucket` object in v2\n* `Bucket.ls` argument `show_versions` renamed to `latest_only` in v2\n* `B2Api` application key methods refactored to operate with dataclasses instead of dicts in v2\n* `B2Api.list_keys` is a generator lazily fetching all keys in v2\n* `account_id` and `bucket_id` added to FileVersion\n\n### Fixed\n* Fix EncryptionSetting.from_response_headers\n* Fix FileVersion.size and FileVersion.mod_time_millis type ambiguity\n* Old buckets (from past tests) are cleaned up before running integration tests in a single thread\n\n### Removed\n* Remove deprecated `SyncReport` methods \n\n## [1.9.0] - 2021-06-07\n\n### Added\n* `ScanPoliciesManager` is able to filter b2 files by upload timestamp\n\n### Changed\n* `Synchronizer.make_file_sync_actions` and `Synchronizer.make_folder_sync_actions` were made private in v2 interface\n* Refactored `sync.file.*File` and `sync.file.*FileVersion` to `sync.path.*SyncPath` in v2\n* Refactored `FileVersionInfo` to `FileVersion` in v2\n* `ScanPoliciesManager` exclusion interface changed in v2\n* `B2Api` unittests for v0, v1 and v2 are now common\n* `B2Api.cancel_large_file` returns a `FileIdAndName` object instead of a `FileVersion` object in v2\n* `FileVersion` has a mandatory `api` parameter in v2\n* `B2Folder` holds a handle to B2Api \n* `Bucket` unit tests for v1 and v2 are now common\n\n### Fixed\n* Fix call to incorrect internal api in `B2Api.get_download_url_for_file_name`\n\n## [1.8.0] - 2021-05-21\n\n### Added\n* Add `get_bucket_name_or_none_from_bucket_id` to `AccountInfo` and `Cache`\n* Add possibility to change realm during integration tests\n* Add support for \"file locks\": file retention, legal hold and default bucket retention\n\n### Fixed\n* Cleanup sync errors related to directories\n* Use proper error handling in `ScanPoliciesManager`\n* Application key restriction message reverted to previous form\n* Added missing apiver wrappers for FileVersionInfo\n* Fix crash when Content-Range header is missing\n* Pin dependency versions appropriately\n\n### Changed\n* `b2sdk.v1.sync` refactored to reflect `b2sdk.sync` structure\n* Make `B2Api.get_bucket_by_id` return populated bucket objects in v2\n* Add proper support of `recommended_part_size` and `absolute_minimum_part_size` in `AccountInfo`\n* Refactored `minimum_part_size` to `recommended_part_size` (the value used stays the same)\n* Encryption settings, types and providers are now part of the public API\n\n### Removed\n* Remove `Bucket.copy_file` and `Bucket.start_large_file` \n* Remove `FileVersionInfo.format_ls_entry` and `FileVersionInfo.format_folder_ls_entry`\n\n## [1.7.0] - 2021-04-22\n\n### Added\n* Add `__slots__` and `__eq__` to `FileVersionInfo` for memory usage optimization and ease of testing\n* Add support for SSE-C server-side encryption mode\n* Add support for `XDG_CONFIG_HOME` for determining the location of `SqliteAccountInfo` db file\n\n### Changed\n* `BasicSyncEncryptionSettingsProvider` supports different settings sets for reading and writing\n* Refactored AccountInfo tests to a single file using pytest\n\n### Fixed\n* Fix clearing cache during `authorize_account`\n* Fix `ChainedStream` (needed in `Bucket.create_file` etc.)\n* Make tqdm-based progress reporters less jumpy and easier to read\n* Fix emerger examples in docs\n\n## [1.6.0] - 2021-04-08\n\n### Added\n* Fetch S3-compatible API URL from `authorize_account`\n\n### Fixed\n* Exclude packages inside the test package when installing\n* Fix for server response change regarding SSE\n\n## [1.5.0] - 2021-03-25\n\n### Added\n* Add `dependabot.yml`\n* Add support for SSE-B2 server-side encryption mode\n\n### Changed\n* Add upper version limit for the requirements\n\n### Fixed\n* Pin `setuptools-scm<6.0` as `>=6.0` doesn't support Python 3.5\n\n## [1.4.0] - 2021-03-03\n\n### Changed\n* Add an ability to provide `bucket_id` filter parameter for `list_buckets`\n* Add `is_same_key` method to `AccountInfo`\n* Add upper version limit for arrow dependency, because of a breaking change\n\n### Fixed\n* Fix docs autogen\n\n## [1.3.0] - 2021-01-13\n\n### Added\n* Add custom exception for `403 transaction_cap_exceeded`\n* Add `get_file_info_by_id` and `get_file_info_by_name` to `Bucket`\n* `FileNotPresent` and `NonExistentBucket` now subclass new exceptions `FileOrBucketNotFound` and `ResourceNotFound`\n\n### Changed\n* Fix missing import in the synchronization example\n* Use `setuptools-scm` for versioning\n* Clean up CI steps\n\n## [1.2.0] - 2020-11-03\n\n### Added\n* Add support for Python 3.9\n* Support for bucket to bucket sync\n* Add a possibility to append a string to the User-Agent in `B2Http`\n\n### Changed\n* Change default fetch count for `ls` to 10000\n\n### Removed\n* Drop Python 2 and Python 3.4 support :tada:\n* Remove `--prefix` from `ls` (it didn't really work, use `folderName` argument)\n\n### Fixed\n* Allow to set an empty bucket info during the update\n* Fix docs generation in CI\n\n## [1.1.4] - 2020-07-15\n\n### Added\n* Allow specifying custom realm in B2Session.authorize_account\n\n## [1.1.2] - 2020-07-06\n\n### Fixed\n* Fix upload part for file range on Python 2.7\n\n## [1.1.0] - 2020-06-24\n\n### Added\n* Add `list_file_versions` method to buckets.\n* Add server-side copy support for large files\n* Add ability to synthesize objects from local and remote sources\n* Add AuthInfoCache, InMemoryCache and AbstractCache to public interface\n* Add ability to filter in ScanPoliciesManager based on modification time\n* Add ScanPoliciesManager and SyncReport to public interface\n* Add md5 checksum to FileVersionInfo\n* Add more keys to dicts returned by as_dict() methods\n\n### Changed\n* Make sync treat hidden files as deleted\n* Ignore urllib3 \"connection pool is full\" warning\n\n### Removed\n* Remove arrow warnings caused by https://github.com/crsmithdev/arrow/issues/612\n\n### Fixed\n* Fix handling of modification time of files\n\n## [1.0.2] - 2019-10-15\n\n### Changed\n* Remove upper version limit for arrow dependency\n\n## [1.0.0] - 2019-10-03\n\n### Fixed\n* Minor bug fix.\n\n## [1.0.0-rc1] - 2019-07-09\n\n### Deprecated\n* Deprecate some transitional method names to v0 in preparation for v1.0.0.\n\n## [0.1.10] - 2019-07-09\n\n### Removed\n* Remove a parameter (which did nothing, really) from `b2sdk.v1.Bucket.copy_file` signature\n\n## [0.1.8] - 2019-06-28\n\n### Added\n* Add support for b2_copy_file\n* Add support for `prefix` parameter on ls-like calls\n\n## [0.1.6] - 2019-04-24\n\n### Changed\n* Rename account ID for authentication to application key ID.\nAccount ID is still backwards compatible, only the terminology\nhas changed.\n\n### Fixed\n* Fix transferer crashing on empty file download attempt\n\n## [0.1.4] - 2019-04-04\n\n### Added\nInitial official release of SDK as a separate package (until now it was a part of B2 CLI)\n\n[Unreleased]: https://github.com/Backblaze/b2-sdk-python/compare/v1.14.0...HEAD\n[1.14.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.13.0...v1.14.0\n[1.13.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.12.0...v1.13.0\n[1.12.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.11.0...v1.12.0\n[1.11.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.10.0...v1.11.0\n[1.10.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.9.0...v1.10.0\n[1.9.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.8.0...v1.9.0\n[1.8.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.7.0...v1.8.0\n[1.7.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.6.0...v1.7.0\n[1.6.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.5.0...v1.6.0\n[1.5.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.4.0...v1.5.0\n[1.4.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.3.0...v1.4.0\n[1.3.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.2.0...v1.3.0\n[1.2.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.1.4...v1.2.0\n[1.1.4]: https://github.com/Backblaze/b2-sdk-python/compare/v1.1.2...v1.1.4\n[1.1.2]: https://github.com/Backblaze/b2-sdk-python/compare/v1.1.0...v1.1.2\n[1.1.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.0.2...v1.1.0\n[1.0.2]: https://github.com/Backblaze/b2-sdk-python/compare/v1.0.0...v1.0.2\n[1.0.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.0.0-rc1...v1.0.0\n[1.0.0-rc1]: https://github.com/Backblaze/b2-sdk-python/compare/v0.1.10...v1.0.0-rc1\n[0.1.10]: https://github.com/Backblaze/b2-sdk-python/compare/v0.1.8...v0.1.10\n[0.1.8]: https://github.com/Backblaze/b2-sdk-python/compare/v0.1.6...v0.1.8\n[0.1.6]: https://github.com/Backblaze/b2-sdk-python/compare/v0.1.4...v0.1.6\n[0.1.4]: https://github.com/Backblaze/b2-sdk-python/compare/4fd290c...v0.1.4\n", "######################################################################\n#\n# File: b2sdk/account_info/sqlite_account_info.py\n#\n# Copyright 2019 Backblaze Inc. All Rights Reserved.\n#\n# License https://www.backblaze.com/using_b2_code.html\n#\n######################################################################\n\nimport json\nimport logging\nimport os\nimport stat\nimport threading\nfrom typing import Optional, List\n\nfrom .exception import (CorruptAccountInfo, MissingAccountData)\nfrom .upload_url_pool import UrlPoolAccountInfo\n\nimport sqlite3\n\nlogger = logging.getLogger(__name__)\n\nB2_ACCOUNT_INFO_ENV_VAR = 'B2_ACCOUNT_INFO'\nB2_ACCOUNT_INFO_DEFAULT_FILE = '~/.b2_account_info'\nXDG_CONFIG_HOME_ENV_VAR = 'XDG_CONFIG_HOME'\n\nDEFAULT_ABSOLUTE_MINIMUM_PART_SIZE = 5000000  # this value is used ONLY in migrating db, and in v1 wrapper, it is not\n# meant to be a default for other applications\n\n\nclass SqliteAccountInfo(UrlPoolAccountInfo):\n    \"\"\"\n    Store account information in an `sqlite3 <https://www.sqlite.org>`_ database which is\n    used to manage concurrent access to the data.\n\n    The ``update_done`` table tracks the schema updates that have been\n    completed.\n    \"\"\"\n\n    def __init__(self, file_name=None, last_upgrade_to_run=None):\n        \"\"\"\n        Initialize SqliteAccountInfo.\n\n        The exact algorithm used to determine the location of the database file is not API in any sense.\n        If the location of the database file is required (for cleanup, etc), do not assume a specific resolution:\n        instead, use ``self.filename`` to get the actual resolved location.\n\n        SqliteAccountInfo currently checks locations in the following order:\n\n        * ``file_name``, if truthy\n        * ``{B2_ACCOUNT_INFO_ENV_VAR}`` env var's value, if set\n        * ``{B2_ACCOUNT_INFO_DEFAULT_FILE}``, if it exists\n        * ``{XDG_CONFIG_HOME_ENV_VAR}/b2/account_info``, if ``{XDG_CONFIG_HOME_ENV_VAR}`` env var is set\n        * ``{B2_ACCOUNT_INFO_DEFAULT_FILE}``, as default\n\n        If the directory ``{XDG_CONFIG_HOME_ENV_VAR}/b2`` does not exist (and is needed), it is created.\n\n        :param str file_name: The sqlite file to use; overrides the default.\n        :param int last_upgrade_to_run: For testing only, override the auto-update on the db.\n        \"\"\"\n        self.thread_local = threading.local()\n\n        if file_name:\n            user_account_info_path = file_name\n        elif B2_ACCOUNT_INFO_ENV_VAR in os.environ:\n            user_account_info_path = os.environ[B2_ACCOUNT_INFO_ENV_VAR]\n        elif os.path.exists(os.path.expanduser(B2_ACCOUNT_INFO_DEFAULT_FILE)):\n            user_account_info_path = B2_ACCOUNT_INFO_DEFAULT_FILE\n        elif XDG_CONFIG_HOME_ENV_VAR in os.environ:\n            config_home = os.environ[XDG_CONFIG_HOME_ENV_VAR]\n            user_account_info_path = os.path.join(config_home, 'b2', 'account_info')\n            if not os.path.exists(os.path.join(config_home, 'b2')):\n                os.makedirs(os.path.join(config_home, 'b2'), mode=0o755)\n        else:\n            user_account_info_path = B2_ACCOUNT_INFO_DEFAULT_FILE\n\n        self.filename = os.path.expanduser(user_account_info_path)\n        logger.debug('%s file path to use: %s', self.__class__.__name__, self.filename)\n\n        self._validate_database(last_upgrade_to_run)\n        with self._get_connection() as conn:\n            self._create_tables(conn, last_upgrade_to_run)\n        super(SqliteAccountInfo, self).__init__()\n\n    # dirty trick to use parameters in the docstring\n    if getattr(__init__, '__doc__', None):  # don't break when using `python -oo`\n        __init__.__doc__ = __init__.__doc__.format(\n            **dict(\n                B2_ACCOUNT_INFO_ENV_VAR=B2_ACCOUNT_INFO_ENV_VAR,\n                B2_ACCOUNT_INFO_DEFAULT_FILE=B2_ACCOUNT_INFO_DEFAULT_FILE,\n                XDG_CONFIG_HOME_ENV_VAR=XDG_CONFIG_HOME_ENV_VAR,\n            )\n        )\n\n    def _validate_database(self, last_upgrade_to_run=None):\n        \"\"\"\n        Make sure that the database is openable.  Removes the file if it's not.\n        \"\"\"\n        # If there is no file there, that's fine.  It will get created when\n        # we connect.\n        if not os.path.exists(self.filename):\n            self._create_database(last_upgrade_to_run)\n            return\n\n        # If we can connect to the database, and do anything, then all is good.\n        try:\n            with self._connect() as conn:\n                self._create_tables(conn, last_upgrade_to_run)\n                return\n        except sqlite3.DatabaseError:\n            pass  # fall through to next case\n\n        # If the file contains JSON with the right stuff in it, convert from\n        # the old representation.\n        try:\n            with open(self.filename, 'rb') as f:\n                data = json.loads(f.read().decode('utf-8'))\n                keys = [\n                    'account_id', 'application_key', 'account_auth_token', 'api_url',\n                    'download_url', 'minimum_part_size', 'realm'\n                ]\n            if all(k in data for k in keys):\n                # remove the json file\n                os.unlink(self.filename)\n                # create a database\n                self._create_database(last_upgrade_to_run)\n                # add the data from the JSON file\n                with self._connect() as conn:\n                    self._create_tables(conn, last_upgrade_to_run)\n                    insert_statement = \"\"\"\n                        INSERT INTO account\n                        (account_id, application_key, account_auth_token, api_url, download_url, \n                         recommended_part_size, realm, absolute_minimum_part_size)\n                        values (?, ?, ?, ?, ?, ?, ?, ?);\n                    \"\"\"\n                    # Migrating from old schema is a little confusing, but the values change as:\n                    # minimum_part_size -> recommended_part_size\n                    # new column absolute_minimum_part_size = DEFAULT_ABSOLUTE_MINIMUM_PART_SIZE\n                    conn.execute(\n                        insert_statement,\n                        (*(data[k] for k in keys), DEFAULT_ABSOLUTE_MINIMUM_PART_SIZE)\n                    )\n                # all is happy now\n                return\n        except ValueError:  # includes json.decoder.JSONDecodeError\n            pass\n\n        # Remove the corrupted file and create a new database\n        raise CorruptAccountInfo(self.filename)\n\n    def _get_connection(self):\n        \"\"\"\n        Connections to sqlite cannot be shared across threads.\n        \"\"\"\n        try:\n            return self.thread_local.connection\n        except AttributeError:\n            self.thread_local.connection = self._connect()\n            return self.thread_local.connection\n\n    def _connect(self):\n        return sqlite3.connect(self.filename, isolation_level='EXCLUSIVE')\n\n    def _create_database(self, last_upgrade_to_run):\n        \"\"\"\n        Make sure that the database is created and sets the file permissions.\n        This should be done before storing any sensitive data in it.\n        \"\"\"\n        # Create the tables in the database\n        conn = self._connect()\n        try:\n            with conn:\n                self._create_tables(conn, last_upgrade_to_run)\n        finally:\n            conn.close()\n\n        # Set the file permissions\n        os.chmod(self.filename, stat.S_IRUSR | stat.S_IWUSR)\n\n    def _create_tables(self, conn, last_upgrade_to_run):\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS\n            update_done (\n                update_number INT NOT NULL\n            );\n        \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n           CREATE TABLE IF NOT EXISTS\n           account (\n               account_id TEXT NOT NULL,\n               application_key TEXT NOT NULL,\n               account_auth_token TEXT NOT NULL,\n               api_url TEXT NOT NULL,\n               download_url TEXT NOT NULL,\n               minimum_part_size INT NOT NULL,\n               realm TEXT NOT NULL\n           );\n        \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n           CREATE TABLE IF NOT EXISTS\n           bucket (\n               bucket_name TEXT NOT NULL,\n               bucket_id TEXT NOT NULL\n           );\n        \"\"\"\n        )\n        # This table is not used any more.  We may use it again\n        # someday if we save upload URLs across invocations of\n        # the command-line tool.\n        conn.execute(\n            \"\"\"\n           CREATE TABLE IF NOT EXISTS\n           bucket_upload_url (\n               bucket_id TEXT NOT NULL,\n               upload_url TEXT NOT NULL,\n               upload_auth_token TEXT NOT NULL\n           );\n        \"\"\"\n        )\n        # By default, we run all the upgrades\n        last_upgrade_to_run = 4 if last_upgrade_to_run is None else last_upgrade_to_run\n        # Add the 'allowed' column if it hasn't been yet.\n        if 1 <= last_upgrade_to_run:\n            self._ensure_update(1, ['ALTER TABLE account ADD COLUMN allowed TEXT;'])\n        # Add the 'account_id_or_app_key_id' column if it hasn't been yet\n        if 2 <= last_upgrade_to_run:\n            self._ensure_update(\n                2, ['ALTER TABLE account ADD COLUMN account_id_or_app_key_id TEXT;']\n            )\n        # Add the 's3_api_url' column if it hasn't been yet\n        if 3 <= last_upgrade_to_run:\n            self._ensure_update(3, ['ALTER TABLE account ADD COLUMN s3_api_url TEXT;'])\n        if 4 <= last_upgrade_to_run:\n            self._ensure_update(\n                4, [\n                    \"\"\"\n                    CREATE TABLE\n                    tmp_account (\n                        account_id TEXT NOT NULL,\n                        application_key TEXT NOT NULL,\n                        account_auth_token TEXT NOT NULL,\n                        api_url TEXT NOT NULL,\n                        download_url TEXT NOT NULL,\n                        absolute_minimum_part_size INT NOT NULL DEFAULT {},\n                        recommended_part_size INT NOT NULL,\n                        realm TEXT NOT NULL,\n                        allowed TEXT,\n                        account_id_or_app_key_id TEXT,\n                        s3_api_url TEXT    \n                    );\n                    \"\"\".format(DEFAULT_ABSOLUTE_MINIMUM_PART_SIZE),\n                    \"\"\"INSERT INTO tmp_account(\n                        account_id,\n                        application_key,\n                        account_auth_token,\n                        api_url,\n                        download_url,\n                        recommended_part_size,\n                        realm,\n                        allowed,\n                        account_id_or_app_key_id,\n                        s3_api_url\n                    ) \n                    SELECT\n                        account_id,\n                        application_key,\n                        account_auth_token,\n                        api_url,\n                        download_url,\n                        minimum_part_size,\n                        realm,\n                        allowed,\n                        account_id_or_app_key_id,\n                        s3_api_url\n                    FROM account;\n                    \"\"\",\n                    'DROP TABLE account;',\n                    \"\"\"\n                    CREATE TABLE account (\n                        account_id TEXT NOT NULL,\n                        application_key TEXT NOT NULL,\n                        account_auth_token TEXT NOT NULL,\n                        api_url TEXT NOT NULL,\n                        download_url TEXT NOT NULL,\n                        absolute_minimum_part_size INT NOT NULL,\n                        recommended_part_size INT NOT NULL,\n                        realm TEXT NOT NULL,\n                        allowed TEXT,\n                        account_id_or_app_key_id TEXT,\n                        s3_api_url TEXT    \n                    );\n                    \"\"\",\n                    \"\"\"INSERT INTO account(\n                                    account_id,\n                                    application_key,\n                                    account_auth_token,\n                                    api_url,\n                                    download_url,\n                                    absolute_minimum_part_size,\n                                    recommended_part_size,\n                                    realm,\n                                    allowed,\n                                    account_id_or_app_key_id,\n                                    s3_api_url\n                                ) \n                                SELECT\n                                    account_id,\n                                    application_key,\n                                    account_auth_token,\n                                    api_url,\n                                    download_url,\n                                    absolute_minimum_part_size,\n                                    recommended_part_size,\n                                    realm,\n                                    allowed,\n                                    account_id_or_app_key_id,\n                                    s3_api_url\n                                FROM tmp_account;\n                                \"\"\",\n                    'DROP TABLE tmp_account;',\n                ]\n            )\n\n    def _ensure_update(self, update_number, update_commands: List[str]):\n        \"\"\"\n        Run the update with the given number if it hasn't been done yet.\n\n        Does the update and stores the number as a single transaction,\n        so they will always be in sync.\n        \"\"\"\n        with self._get_connection() as conn:\n            conn.execute('BEGIN')\n            cursor = conn.execute(\n                'SELECT COUNT(*) AS count FROM update_done WHERE update_number = ?;',\n                (update_number,)\n            )\n            update_count = cursor.fetchone()[0]\n            if update_count == 0:\n                for command in update_commands:\n                    conn.execute(command)\n                conn.execute(\n                    'INSERT INTO update_done (update_number) VALUES (?);', (update_number,)\n                )\n\n    def clear(self):\n        \"\"\"\n        Remove all info about accounts and buckets.\n        \"\"\"\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM account;')\n            conn.execute('DELETE FROM bucket;')\n            conn.execute('DELETE FROM bucket_upload_url;')\n\n    def _set_auth_data(\n        self,\n        account_id,\n        auth_token,\n        api_url,\n        download_url,\n        recommended_part_size,\n        absolute_minimum_part_size,\n        application_key,\n        realm,\n        s3_api_url,\n        allowed,\n        application_key_id,\n    ):\n        assert self.allowed_is_valid(allowed)\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM account;')\n            conn.execute('DELETE FROM bucket;')\n            conn.execute('DELETE FROM bucket_upload_url;')\n            insert_statement = \"\"\"\n                INSERT INTO account\n                (account_id, account_id_or_app_key_id, application_key, account_auth_token, api_url, download_url, \n                 recommended_part_size, absolute_minimum_part_size, realm, allowed, s3_api_url)\n                values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n            \"\"\"\n\n            conn.execute(\n                insert_statement, (\n                    account_id,\n                    application_key_id,\n                    application_key,\n                    auth_token,\n                    api_url,\n                    download_url,\n                    recommended_part_size,\n                    absolute_minimum_part_size,\n                    realm,\n                    json.dumps(allowed),\n                    s3_api_url,\n                )\n            )\n\n    def set_auth_data_with_schema_0_for_test(\n        self,\n        account_id,\n        auth_token,\n        api_url,\n        download_url,\n        minimum_part_size,\n        application_key,\n        realm,\n    ):\n        \"\"\"\n        Set authentication data for tests.\n\n        :param str account_id: an account ID\n        :param str auth_token: an authentication token\n        :param str api_url: an API URL\n        :param str download_url: a download URL\n        :param int minimum_part_size: a minimum part size\n        :param str application_key: an application key\n        :param str realm: a realm to authorize account in\n        \"\"\"\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM account;')\n            conn.execute('DELETE FROM bucket;')\n            conn.execute('DELETE FROM bucket_upload_url;')\n            insert_statement = \"\"\"\n                INSERT INTO account\n                (account_id, application_key, account_auth_token, api_url, download_url, minimum_part_size, realm)\n                values (?, ?, ?, ?, ?, ?, ?);\n            \"\"\"\n\n            conn.execute(\n                insert_statement, (\n                    account_id,\n                    application_key,\n                    auth_token,\n                    api_url,\n                    download_url,\n                    minimum_part_size,\n                    realm,\n                )\n            )\n\n    def get_application_key(self):\n        return self._get_account_info_or_raise('application_key')\n\n    def get_account_id(self):\n        return self._get_account_info_or_raise('account_id')\n\n    def get_application_key_id(self):\n        \"\"\"\n        Return an application key ID.\n        The 'account_id_or_app_key_id' column was not in the original schema, so it may be NULL.\n\n        Nota bene - this is the only place where we are not renaming account_id_or_app_key_id to application_key_id\n        because it requires a column change.\n\n        application_key_id == account_id_or_app_key_id\n\n        :rtype: str\n        \"\"\"\n        result = self._get_account_info_or_raise('account_id_or_app_key_id')\n        if result is None:\n            return self.get_account_id()\n        else:\n            return result\n\n    def get_api_url(self):\n        return self._get_account_info_or_raise('api_url')\n\n    def get_account_auth_token(self):\n        return self._get_account_info_or_raise('account_auth_token')\n\n    def get_download_url(self):\n        return self._get_account_info_or_raise('download_url')\n\n    def get_realm(self):\n        return self._get_account_info_or_raise('realm')\n\n    def get_recommended_part_size(self):\n        return self._get_account_info_or_raise('recommended_part_size')\n\n    def get_absolute_minimum_part_size(self):\n        return self._get_account_info_or_raise('absolute_minimum_part_size')\n\n    def get_allowed(self):\n        \"\"\"\n        Return 'allowed' dictionary info.\n        Example:\n\n        .. code-block:: python\n\n            {\n                \"bucketId\": null,\n                \"bucketName\": null,\n                \"capabilities\": [\n                    \"listKeys\",\n                    \"writeKeys\"\n                ],\n                \"namePrefix\": null\n            }\n\n        The 'allowed' column was not in the original schema, so it may be NULL.\n\n        :rtype: dict\n        \"\"\"\n        allowed_json = self._get_account_info_or_raise('allowed')\n        if allowed_json is None:\n            return self.DEFAULT_ALLOWED\n        else:\n            return json.loads(allowed_json)\n\n    def get_s3_api_url(self):\n        result = self._get_account_info_or_raise('s3_api_url')\n        if result is None:\n            return ''\n        else:\n            return result\n\n    def _get_account_info_or_raise(self, column_name):\n        try:\n            with self._get_connection() as conn:\n                cursor = conn.execute('SELECT %s FROM account;' % (column_name,))\n                value = cursor.fetchone()[0]\n                return value\n        except Exception as e:\n            logger.exception(\n                '_get_account_info_or_raise encountered a problem while trying to retrieve \"%s\"',\n                column_name\n            )\n            raise MissingAccountData(str(e))\n\n    def refresh_entire_bucket_name_cache(self, name_id_iterable):\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM bucket;')\n            for (bucket_name, bucket_id) in name_id_iterable:\n                conn.execute(\n                    'INSERT INTO bucket (bucket_name, bucket_id) VALUES (?, ?);',\n                    (bucket_name, bucket_id)\n                )\n\n    def save_bucket(self, bucket):\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM bucket WHERE bucket_id = ?;', (bucket.id_,))\n            conn.execute(\n                'INSERT INTO bucket (bucket_id, bucket_name) VALUES (?, ?);',\n                (bucket.id_, bucket.name)\n            )\n\n    def remove_bucket_name(self, bucket_name):\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM bucket WHERE bucket_name = ?;', (bucket_name,))\n\n    def get_bucket_id_or_none_from_bucket_name(self, bucket_name):\n        return self._safe_query(\n            'SELECT bucket_id FROM bucket WHERE bucket_name = ?;', (bucket_name,)\n        )\n\n    def get_bucket_name_or_none_from_bucket_id(self, bucket_id: str) -> Optional[str]:\n        return self._safe_query('SELECT bucket_name FROM bucket WHERE bucket_id = ?;', (bucket_id,))\n\n    def _safe_query(self, query, params):\n        try:\n            with self._get_connection() as conn:\n                cursor = conn.execute(query, params)\n                return cursor.fetchone()[0]\n        except TypeError:  # TypeError: 'NoneType' object is unsubscriptable\n            return None\n        except sqlite3.Error:\n            return None\n", "######################################################################\n#\n# File: test/unit/account_info/test_account_info.py\n#\n# Copyright 2021 Backblaze Inc. All Rights Reserved.\n#\n# License https://www.backblaze.com/using_b2_code.html\n#\n######################################################################\n\nfrom abc import ABCMeta, abstractmethod\nimport json\nimport unittest.mock as mock\nimport os\nimport platform\nimport shutil\nimport tempfile\n\nimport pytest\n\nfrom apiver_deps import (\n    ALL_CAPABILITIES,\n    AbstractAccountInfo,\n    InMemoryAccountInfo,\n    UploadUrlPool,\n    SqliteAccountInfo,\n    TempDir,\n    B2_ACCOUNT_INFO_ENV_VAR,\n    XDG_CONFIG_HOME_ENV_VAR,\n)\nfrom apiver_deps_exception import CorruptAccountInfo, MissingAccountData\n\nfrom .fixtures import *\n\n\nclass WindowsSafeTempDir(TempDir):\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        try:\n            super().__exit__(exc_type, exc_val, exc_tb)\n        except OSError:\n            pass\n\n\nclass TestAccountInfo:\n    @pytest.fixture(autouse=True)\n    def setup(self, account_info_factory, account_info_default_data):\n        self.account_info_factory = account_info_factory\n        self.account_info_default_data = account_info_default_data\n\n    @pytest.mark.parametrize(\n        'application_key_id,realm,expected',\n        (\n            ('application_key_id', 'dev', True),\n            ('application_key_id', 'test', False),\n            ('different_application_key_id', 'dev', False),\n            ('different_application_key_id', 'test', False),\n        ),\n    )\n    def test_is_same_key(self, application_key_id, realm, expected):\n        account_info = self.account_info_factory()\n        account_info.set_auth_data(**self.account_info_default_data)\n\n        assert account_info.is_same_key(application_key_id, realm) is expected\n\n    @pytest.mark.parametrize(\n        'account_id,realm,expected',\n        (\n            ('account_id', 'dev', True),\n            ('account_id', 'test', False),\n            ('different_account_id', 'dev', False),\n            ('different_account_id', 'test', False),\n        ),\n    )\n    def test_is_same_account(self, account_id, realm, expected):\n        account_info = self.account_info_factory()\n        account_info.set_auth_data(**self.account_info_default_data)\n\n        assert account_info.is_same_account(account_id, realm) is expected\n\n    @pytest.mark.parametrize(\n        's3_api_url',\n        ('https://s3.us-east-123.backblazeb2.com', 'https://s3.us-west-321.backblazeb2.com')\n    )\n    def test_s3_api_url(self, s3_api_url):\n        account_info = self.account_info_factory()\n        account_info_default_data = {\n            **self.account_info_default_data,\n            's3_api_url': s3_api_url,\n        }\n        account_info.set_auth_data(**account_info_default_data)\n        assert s3_api_url == account_info.get_s3_api_url()\n\n    def test_getting_all_capabilities(self):\n        account_info = self.account_info_factory()\n        assert account_info.all_capabilities() == ALL_CAPABILITIES\n\n\nclass TestUploadUrlPool:\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.pool = UploadUrlPool()\n\n    def test_take_empty(self):\n        assert (None, None) == self.pool.take('a')\n\n    def test_put_and_take(self):\n        self.pool.put('a', 'url_a1', 'auth_token_a1')\n        self.pool.put('a', 'url_a2', 'auth_token_a2')\n        self.pool.put('b', 'url_b1', 'auth_token_b1')\n        assert ('url_a2', 'auth_token_a2') == self.pool.take('a')\n        assert ('url_a1', 'auth_token_a1') == self.pool.take('a')\n        assert (None, None) == self.pool.take('a')\n        assert ('url_b1', 'auth_token_b1') == self.pool.take('b')\n        assert (None, None) == self.pool.take('b')\n\n    def test_clear(self):\n        self.pool.put('a', 'url_a1', 'auth_token_a1')\n        self.pool.clear_for_key('a')\n        self.pool.put('b', 'url_b1', 'auth_token_b1')\n        assert (None, None) == self.pool.take('a')\n        assert ('url_b1', 'auth_token_b1') == self.pool.take('b')\n        assert (None, None) == self.pool.take('b')\n\n\nclass AccountInfoBase(metaclass=ABCMeta):\n    # it is a mixin to avoid running the tests directly (without inheritance)\n    PERSISTENCE = NotImplemented  # subclass should override this\n\n    @abstractmethod\n    def _make_info(self):\n        \"\"\"\n        returns a new object of AccountInfo class which should be tested\n        \"\"\"\n\n    def test_clear(self, account_info_default_data, apiver):\n        account_info = self._make_info()\n        account_info.set_auth_data(**account_info_default_data)\n        account_info.clear()\n\n        with pytest.raises(MissingAccountData):\n            account_info.get_account_id()\n        with pytest.raises(MissingAccountData):\n            account_info.get_account_auth_token()\n        with pytest.raises(MissingAccountData):\n            account_info.get_api_url()\n        with pytest.raises(MissingAccountData):\n            account_info.get_application_key()\n        with pytest.raises(MissingAccountData):\n            account_info.get_download_url()\n        with pytest.raises(MissingAccountData):\n            account_info.get_realm()\n        with pytest.raises(MissingAccountData):\n            account_info.get_application_key_id()\n        assert not account_info.is_same_key('key_id', 'realm')\n\n        if apiver in ['v0', 'v1']:\n            with pytest.raises(MissingAccountData):\n                account_info.get_minimum_part_size()\n        else:\n            with pytest.raises(MissingAccountData):\n                account_info.get_recommended_part_size()\n            with pytest.raises(MissingAccountData):\n                account_info.get_absolute_minimum_part_size()\n\n    def test_set_auth_data_compatibility(self, account_info_default_data):\n        account_info = self._make_info()\n\n        # The original set_auth_data\n        account_info.set_auth_data(**account_info_default_data)\n        actual = account_info.get_allowed()\n        assert AbstractAccountInfo.DEFAULT_ALLOWED == actual, 'default allowed'\n\n        # allowed was added later\n        allowed = dict(\n            bucketId=None,\n            bucketName=None,\n            capabilities=['readFiles'],\n            namePrefix=None,\n        )\n        account_info.set_auth_data(**{\n            **account_info_default_data,\n            'allowed': allowed,\n        })\n        assert allowed == account_info.get_allowed()\n\n    def test_clear_bucket_upload_data(self):\n        account_info = self._make_info()\n        account_info.put_bucket_upload_url('bucket-0', 'http://bucket-0', 'bucket-0_auth')\n        account_info.clear_bucket_upload_data('bucket-0')\n        assert (None, None) == account_info.take_bucket_upload_url('bucket-0')\n\n    def test_large_file_upload_urls(self):\n        account_info = self._make_info()\n        account_info.put_large_file_upload_url('file_0', 'http://file_0', 'auth_0')\n        assert ('http://file_0', 'auth_0') == account_info.take_large_file_upload_url('file_0')\n        assert (None, None) == account_info.take_large_file_upload_url('file_0')\n\n    def test_clear_large_file_upload_urls(self):\n        account_info = self._make_info()\n        account_info.put_large_file_upload_url('file_0', 'http://file_0', 'auth_0')\n        account_info.clear_large_file_upload_urls('file_0')\n        assert (None, None) == account_info.take_large_file_upload_url('file_0')\n\n    def test_bucket(self):\n        account_info = self._make_info()\n        bucket = mock.MagicMock()\n        bucket.name = 'my-bucket'\n        bucket.id_ = 'bucket-0'\n        assert account_info.get_bucket_id_or_none_from_bucket_name('my-bucket') is None\n        assert account_info.get_bucket_name_or_none_from_bucket_id('bucket-0') is None\n        account_info.save_bucket(bucket)\n        assert 'bucket-0' == account_info.get_bucket_id_or_none_from_bucket_name('my-bucket')\n        assert 'my-bucket' == account_info.get_bucket_name_or_none_from_bucket_id('bucket-0')\n        if self.PERSISTENCE:\n            assert 'bucket-0' == self._make_info(\n            ).get_bucket_id_or_none_from_bucket_name('my-bucket')\n            assert 'my-bucket' == self._make_info(\n            ).get_bucket_name_or_none_from_bucket_id('bucket-0')\n        account_info.remove_bucket_name('my-bucket')\n        assert account_info.get_bucket_id_or_none_from_bucket_name('my-bucket') is None\n        assert account_info.get_bucket_name_or_none_from_bucket_id('bucket-0') is None\n        if self.PERSISTENCE:\n            assert self._make_info().get_bucket_id_or_none_from_bucket_name('my-bucket') is None\n            assert self._make_info().get_bucket_name_or_none_from_bucket_id('bucket-0') is None\n\n    def test_refresh_bucket(self):\n        account_info = self._make_info()\n        assert account_info.get_bucket_id_or_none_from_bucket_name('my-bucket') is None\n        assert account_info.get_bucket_name_or_none_from_bucket_id('a') is None\n        bucket_names = {'a': 'bucket-0', 'b': 'bucket-1'}\n        account_info.refresh_entire_bucket_name_cache(bucket_names.items())\n        assert 'bucket-0' == account_info.get_bucket_id_or_none_from_bucket_name('a')\n        assert 'a' == account_info.get_bucket_name_or_none_from_bucket_id('bucket-0')\n        if self.PERSISTENCE:\n            assert 'bucket-0' == self._make_info().get_bucket_id_or_none_from_bucket_name('a')\n            assert 'a' == self._make_info().get_bucket_name_or_none_from_bucket_id('bucket-0')\n\n    @pytest.mark.apiver(to_ver=1)\n    def test_account_info_up_to_v1(self):\n        account_info = self._make_info()\n        account_info.set_auth_data(\n            'account_id',\n            'account_auth',\n            'https://api.backblazeb2.com',\n            'download_url',\n            100,\n            'app_key',\n            'realm',\n            application_key_id='key_id'\n        )\n\n        object_instances = [account_info]\n        if self.PERSISTENCE:\n            object_instances.append(self._make_info())\n        for info2 in object_instances:\n            assert 'account_id' == info2.get_account_id()\n            assert 'account_auth' == info2.get_account_auth_token()\n            assert 'https://api.backblazeb2.com' == info2.get_api_url()\n            assert 'app_key' == info2.get_application_key()\n            assert 'key_id' == info2.get_application_key_id()\n            assert 'realm' == info2.get_realm()\n            assert 100 == info2.get_minimum_part_size()\n            assert info2.is_same_key('key_id', 'realm')\n            assert not info2.is_same_key('key_id', 'another_realm')\n            assert not info2.is_same_key('another_key_id', 'realm')\n            assert not info2.is_same_key('another_key_id', 'another_realm')\n\n    @pytest.mark.apiver(from_ver=2)\n    def test_account_info_v2(self):\n        account_info = self._make_info()\n        account_info.set_auth_data(\n            account_id='account_id',\n            auth_token='account_auth',\n            api_url='https://api.backblazeb2.com',\n            download_url='download_url',\n            recommended_part_size=100,\n            absolute_minimum_part_size=50,\n            application_key='app_key',\n            realm='realm',\n            s3_api_url='s3_api_url',\n            allowed=None,\n            application_key_id='key_id',\n        )\n\n        object_instances = [account_info]\n        if self.PERSISTENCE:\n            object_instances.append(self._make_info())\n        for info2 in object_instances:\n            assert 'account_id' == info2.get_account_id()\n            assert 'account_auth' == info2.get_account_auth_token()\n            assert 'https://api.backblazeb2.com' == info2.get_api_url()\n            assert 'app_key' == info2.get_application_key()\n            assert 'key_id' == info2.get_application_key_id()\n            assert 'realm' == info2.get_realm()\n            assert 100 == info2.get_recommended_part_size()\n            assert 50 == info2.get_absolute_minimum_part_size()\n            assert info2.is_same_key('key_id', 'realm')\n            assert not info2.is_same_key('key_id', 'another_realm')\n            assert not info2.is_same_key('another_key_id', 'realm')\n            assert not info2.is_same_key('another_key_id', 'another_realm')\n\n\nclass TestInMemoryAccountInfo(AccountInfoBase):\n    PERSISTENCE = False\n\n    def _make_info(self):\n        return InMemoryAccountInfo()\n\n\nclass TestSqliteAccountInfo(AccountInfoBase):\n    PERSISTENCE = True\n\n    @pytest.fixture(autouse=True)\n    def setUp(self, request):\n        self.db_path = tempfile.NamedTemporaryFile(\n            prefix='tmp_b2_tests_%s__' % (request.node.name,), delete=True\n        ).name\n        try:\n            os.unlink(self.db_path)\n        except OSError:\n            pass\n        self.home = tempfile.mkdtemp()\n\n        yield\n        for cleanup_method in [lambda: os.unlink(self.db_path), lambda: shutil.rmtree(self.home)]:\n            try:\n                cleanup_method\n            except OSError:\n                pass\n\n    def test_corrupted(self):\n        \"\"\"\n        Test that a corrupted file will be replaced with a blank file.\n        \"\"\"\n        with open(self.db_path, 'wb') as f:\n            f.write(b'not a valid database')\n\n        with pytest.raises(CorruptAccountInfo):\n            self._make_info()\n\n    @pytest.mark.skipif(\n        platform.system() == 'Windows',\n        reason='it fails to upgrade on Windows, not worth to fix it anymore'\n    )\n    def test_convert_from_json(self):\n        \"\"\"\n        Tests converting from a JSON account info file, which is what version\n        0.5.2 of the command-line tool used.\n        \"\"\"\n        data = dict(\n            account_auth_token='auth_token',\n            account_id='account_id',\n            api_url='api_url',\n            application_key='application_key',\n            download_url='download_url',\n            minimum_part_size=5000,\n            realm='production'\n        )\n        with open(self.db_path, 'wb') as f:\n            f.write(json.dumps(data).encode('utf-8'))\n        account_info = self._make_info()\n        assert 'auth_token' == account_info.get_account_auth_token()\n\n    def _make_info(self):\n        return self._make_sqlite_account_info()\n\n    def _make_sqlite_account_info(self, env=None, last_upgrade_to_run=None):\n        \"\"\"\n        Returns a new SqliteAccountInfo that has just read the data from the file.\n\n        :param dict env: Override Environment variables.\n        \"\"\"\n        # Override HOME to ensure hermetic tests\n        with mock.patch('os.environ', env or {'HOME': self.home}):\n            return SqliteAccountInfo(\n                file_name=self.db_path if not env else None,\n                last_upgrade_to_run=last_upgrade_to_run,\n            )\n\n    def test_uses_default(self):\n        account_info = self._make_sqlite_account_info(\n            env={\n                'HOME': self.home,\n                'USERPROFILE': self.home,\n            }\n        )\n        actual_path = os.path.abspath(account_info.filename)\n        assert os.path.join(self.home, '.b2_account_info') == actual_path\n\n    def test_uses_xdg_config_home(self, apiver):\n        with WindowsSafeTempDir() as d:\n            account_info = self._make_sqlite_account_info(\n                env={\n                    'HOME': self.home,\n                    'USERPROFILE': self.home,\n                    XDG_CONFIG_HOME_ENV_VAR: d,\n                }\n            )\n            if apiver in ['v0', 'v1']:\n                expected_path = os.path.abspath(os.path.join(self.home, '.b2_account_info'))\n            else:\n                assert os.path.exists(os.path.join(d, 'b2'))\n                expected_path = os.path.abspath(os.path.join(d, 'b2', 'account_info'))\n            actual_path = os.path.abspath(account_info.filename)\n            assert expected_path == actual_path\n\n    def test_uses_existing_file_and_ignores_xdg(self):\n        with WindowsSafeTempDir() as d:\n            default_db_file_location = os.path.join(self.home, '.b2_account_info')\n            open(default_db_file_location, 'a').close()\n            account_info = self._make_sqlite_account_info(\n                env={\n                    'HOME': self.home,\n                    'USERPROFILE': self.home,\n                    XDG_CONFIG_HOME_ENV_VAR: d,\n                }\n            )\n            actual_path = os.path.abspath(account_info.filename)\n            assert default_db_file_location == actual_path\n            assert not os.path.exists(os.path.join(d, 'b2'))\n\n    def test_account_info_env_var_overrides_xdg_config_home(self):\n        with WindowsSafeTempDir() as d:\n            account_info = self._make_sqlite_account_info(\n                env={\n                    'HOME': self.home,\n                    'USERPROFILE': self.home,\n                    XDG_CONFIG_HOME_ENV_VAR: d,\n                    B2_ACCOUNT_INFO_ENV_VAR: os.path.join(d, 'b2_account_info'),\n                }\n            )\n            expected_path = os.path.abspath(os.path.join(d, 'b2_account_info'))\n            actual_path = os.path.abspath(account_info.filename)\n            assert expected_path == actual_path\n"], "fixing_code": ["# Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n* Add pypy-3.8 to test matrix\n* Add support for unverified checksum upload mode\n* Add dedicated exception for unverified email\n\n### Fixed\n* Fix downloading files with unverified checksum\n* Fix setting permissions for local sqlite database (thanks to Jan Schejbal for responsible disclosure!)\n\n## [1.14.0] - 2021-12-23\n\n### Fixed\n* Relax constraint on arrow to allow for versions >= 1.0.2\n\n## [1.13.0] - 2021-10-24\n\n### Added\n* Add support for Python 3.10\n\n### Changed\n* Update a list with all capabilities\n\n### Fixed\n* Fix pypy selector in CI\n\n## [1.12.0] - 2021-08-06\n\n### Changed\n* The `importlib-metadata` requirement is less strictly bound now (just >=3.3.0 for python > 3.5).\n* `B2Api` `update_file_legal_hold` and `update_file_retention_setting` now return the set values \n\n### Added\n* `BucketIdNotFound` thrown based on B2 cloud response\n* `_clone` method to `FileVersion` and `DownloadVersion`\n* `delete`, `update_legal_hold`, `update_retention` and `download` methods added to `FileVersion`\n\n### Fixed\n* FileSimulator returns special file info headers properly\n\n### Removed\n* One unused import.\n\n## [1.11.0] - 2021-06-24\n\n### Changed\n* apiver `v2` interface released. `from b2sdk.v2 import ...` is now the recommended import, \n  but `from b2sdk.v1 import ...` works as before\n\n## [1.10.0] - 2021-06-23\n\n### Added\n* `get_fresh_state` method added to `FileVersion` and `Bucket`\n\n### Changed\n* `download_file_*` methods refactored to allow for inspecting DownloadVersion before downloading the whole file\n* `B2Api.get_file_info` returns a `FileVersion` object in v2\n* `B2RawApi` renamed to `B2RawHTTPApi`\n* `B2HTTP` tests are now common\n* `B2HttpApiConfig` class introduced to provide parameters like `user_agent_append` to `B2Api` without using internal classes in v2\n* `Bucket.update` returns a `Bucket` object in v2\n* `Bucket.ls` argument `show_versions` renamed to `latest_only` in v2\n* `B2Api` application key methods refactored to operate with dataclasses instead of dicts in v2\n* `B2Api.list_keys` is a generator lazily fetching all keys in v2\n* `account_id` and `bucket_id` added to FileVersion\n\n### Fixed\n* Fix EncryptionSetting.from_response_headers\n* Fix FileVersion.size and FileVersion.mod_time_millis type ambiguity\n* Old buckets (from past tests) are cleaned up before running integration tests in a single thread\n\n### Removed\n* Remove deprecated `SyncReport` methods \n\n## [1.9.0] - 2021-06-07\n\n### Added\n* `ScanPoliciesManager` is able to filter b2 files by upload timestamp\n\n### Changed\n* `Synchronizer.make_file_sync_actions` and `Synchronizer.make_folder_sync_actions` were made private in v2 interface\n* Refactored `sync.file.*File` and `sync.file.*FileVersion` to `sync.path.*SyncPath` in v2\n* Refactored `FileVersionInfo` to `FileVersion` in v2\n* `ScanPoliciesManager` exclusion interface changed in v2\n* `B2Api` unittests for v0, v1 and v2 are now common\n* `B2Api.cancel_large_file` returns a `FileIdAndName` object instead of a `FileVersion` object in v2\n* `FileVersion` has a mandatory `api` parameter in v2\n* `B2Folder` holds a handle to B2Api \n* `Bucket` unit tests for v1 and v2 are now common\n\n### Fixed\n* Fix call to incorrect internal api in `B2Api.get_download_url_for_file_name`\n\n## [1.8.0] - 2021-05-21\n\n### Added\n* Add `get_bucket_name_or_none_from_bucket_id` to `AccountInfo` and `Cache`\n* Add possibility to change realm during integration tests\n* Add support for \"file locks\": file retention, legal hold and default bucket retention\n\n### Fixed\n* Cleanup sync errors related to directories\n* Use proper error handling in `ScanPoliciesManager`\n* Application key restriction message reverted to previous form\n* Added missing apiver wrappers for FileVersionInfo\n* Fix crash when Content-Range header is missing\n* Pin dependency versions appropriately\n\n### Changed\n* `b2sdk.v1.sync` refactored to reflect `b2sdk.sync` structure\n* Make `B2Api.get_bucket_by_id` return populated bucket objects in v2\n* Add proper support of `recommended_part_size` and `absolute_minimum_part_size` in `AccountInfo`\n* Refactored `minimum_part_size` to `recommended_part_size` (the value used stays the same)\n* Encryption settings, types and providers are now part of the public API\n\n### Removed\n* Remove `Bucket.copy_file` and `Bucket.start_large_file` \n* Remove `FileVersionInfo.format_ls_entry` and `FileVersionInfo.format_folder_ls_entry`\n\n## [1.7.0] - 2021-04-22\n\n### Added\n* Add `__slots__` and `__eq__` to `FileVersionInfo` for memory usage optimization and ease of testing\n* Add support for SSE-C server-side encryption mode\n* Add support for `XDG_CONFIG_HOME` for determining the location of `SqliteAccountInfo` db file\n\n### Changed\n* `BasicSyncEncryptionSettingsProvider` supports different settings sets for reading and writing\n* Refactored AccountInfo tests to a single file using pytest\n\n### Fixed\n* Fix clearing cache during `authorize_account`\n* Fix `ChainedStream` (needed in `Bucket.create_file` etc.)\n* Make tqdm-based progress reporters less jumpy and easier to read\n* Fix emerger examples in docs\n\n## [1.6.0] - 2021-04-08\n\n### Added\n* Fetch S3-compatible API URL from `authorize_account`\n\n### Fixed\n* Exclude packages inside the test package when installing\n* Fix for server response change regarding SSE\n\n## [1.5.0] - 2021-03-25\n\n### Added\n* Add `dependabot.yml`\n* Add support for SSE-B2 server-side encryption mode\n\n### Changed\n* Add upper version limit for the requirements\n\n### Fixed\n* Pin `setuptools-scm<6.0` as `>=6.0` doesn't support Python 3.5\n\n## [1.4.0] - 2021-03-03\n\n### Changed\n* Add an ability to provide `bucket_id` filter parameter for `list_buckets`\n* Add `is_same_key` method to `AccountInfo`\n* Add upper version limit for arrow dependency, because of a breaking change\n\n### Fixed\n* Fix docs autogen\n\n## [1.3.0] - 2021-01-13\n\n### Added\n* Add custom exception for `403 transaction_cap_exceeded`\n* Add `get_file_info_by_id` and `get_file_info_by_name` to `Bucket`\n* `FileNotPresent` and `NonExistentBucket` now subclass new exceptions `FileOrBucketNotFound` and `ResourceNotFound`\n\n### Changed\n* Fix missing import in the synchronization example\n* Use `setuptools-scm` for versioning\n* Clean up CI steps\n\n## [1.2.0] - 2020-11-03\n\n### Added\n* Add support for Python 3.9\n* Support for bucket to bucket sync\n* Add a possibility to append a string to the User-Agent in `B2Http`\n\n### Changed\n* Change default fetch count for `ls` to 10000\n\n### Removed\n* Drop Python 2 and Python 3.4 support :tada:\n* Remove `--prefix` from `ls` (it didn't really work, use `folderName` argument)\n\n### Fixed\n* Allow to set an empty bucket info during the update\n* Fix docs generation in CI\n\n## [1.1.4] - 2020-07-15\n\n### Added\n* Allow specifying custom realm in B2Session.authorize_account\n\n## [1.1.2] - 2020-07-06\n\n### Fixed\n* Fix upload part for file range on Python 2.7\n\n## [1.1.0] - 2020-06-24\n\n### Added\n* Add `list_file_versions` method to buckets.\n* Add server-side copy support for large files\n* Add ability to synthesize objects from local and remote sources\n* Add AuthInfoCache, InMemoryCache and AbstractCache to public interface\n* Add ability to filter in ScanPoliciesManager based on modification time\n* Add ScanPoliciesManager and SyncReport to public interface\n* Add md5 checksum to FileVersionInfo\n* Add more keys to dicts returned by as_dict() methods\n\n### Changed\n* Make sync treat hidden files as deleted\n* Ignore urllib3 \"connection pool is full\" warning\n\n### Removed\n* Remove arrow warnings caused by https://github.com/crsmithdev/arrow/issues/612\n\n### Fixed\n* Fix handling of modification time of files\n\n## [1.0.2] - 2019-10-15\n\n### Changed\n* Remove upper version limit for arrow dependency\n\n## [1.0.0] - 2019-10-03\n\n### Fixed\n* Minor bug fix.\n\n## [1.0.0-rc1] - 2019-07-09\n\n### Deprecated\n* Deprecate some transitional method names to v0 in preparation for v1.0.0.\n\n## [0.1.10] - 2019-07-09\n\n### Removed\n* Remove a parameter (which did nothing, really) from `b2sdk.v1.Bucket.copy_file` signature\n\n## [0.1.8] - 2019-06-28\n\n### Added\n* Add support for b2_copy_file\n* Add support for `prefix` parameter on ls-like calls\n\n## [0.1.6] - 2019-04-24\n\n### Changed\n* Rename account ID for authentication to application key ID.\nAccount ID is still backwards compatible, only the terminology\nhas changed.\n\n### Fixed\n* Fix transferer crashing on empty file download attempt\n\n## [0.1.4] - 2019-04-04\n\n### Added\nInitial official release of SDK as a separate package (until now it was a part of B2 CLI)\n\n[Unreleased]: https://github.com/Backblaze/b2-sdk-python/compare/v1.14.0...HEAD\n[1.14.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.13.0...v1.14.0\n[1.13.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.12.0...v1.13.0\n[1.12.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.11.0...v1.12.0\n[1.11.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.10.0...v1.11.0\n[1.10.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.9.0...v1.10.0\n[1.9.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.8.0...v1.9.0\n[1.8.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.7.0...v1.8.0\n[1.7.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.6.0...v1.7.0\n[1.6.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.5.0...v1.6.0\n[1.5.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.4.0...v1.5.0\n[1.4.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.3.0...v1.4.0\n[1.3.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.2.0...v1.3.0\n[1.2.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.1.4...v1.2.0\n[1.1.4]: https://github.com/Backblaze/b2-sdk-python/compare/v1.1.2...v1.1.4\n[1.1.2]: https://github.com/Backblaze/b2-sdk-python/compare/v1.1.0...v1.1.2\n[1.1.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.0.2...v1.1.0\n[1.0.2]: https://github.com/Backblaze/b2-sdk-python/compare/v1.0.0...v1.0.2\n[1.0.0]: https://github.com/Backblaze/b2-sdk-python/compare/v1.0.0-rc1...v1.0.0\n[1.0.0-rc1]: https://github.com/Backblaze/b2-sdk-python/compare/v0.1.10...v1.0.0-rc1\n[0.1.10]: https://github.com/Backblaze/b2-sdk-python/compare/v0.1.8...v0.1.10\n[0.1.8]: https://github.com/Backblaze/b2-sdk-python/compare/v0.1.6...v0.1.8\n[0.1.6]: https://github.com/Backblaze/b2-sdk-python/compare/v0.1.4...v0.1.6\n[0.1.4]: https://github.com/Backblaze/b2-sdk-python/compare/4fd290c...v0.1.4\n", "######################################################################\n#\n# File: b2sdk/account_info/sqlite_account_info.py\n#\n# Copyright 2019 Backblaze Inc. All Rights Reserved.\n#\n# License https://www.backblaze.com/using_b2_code.html\n#\n######################################################################\n\nimport json\nimport logging\nimport os\nimport stat\nimport threading\nfrom typing import Optional, List\n\nfrom .exception import (CorruptAccountInfo, MissingAccountData)\nfrom .upload_url_pool import UrlPoolAccountInfo\n\nimport sqlite3\n\nlogger = logging.getLogger(__name__)\n\nB2_ACCOUNT_INFO_ENV_VAR = 'B2_ACCOUNT_INFO'\nB2_ACCOUNT_INFO_DEFAULT_FILE = '~/.b2_account_info'\nXDG_CONFIG_HOME_ENV_VAR = 'XDG_CONFIG_HOME'\n\nDEFAULT_ABSOLUTE_MINIMUM_PART_SIZE = 5000000  # this value is used ONLY in migrating db, and in v1 wrapper, it is not\n# meant to be a default for other applications\n\n\nclass SqliteAccountInfo(UrlPoolAccountInfo):\n    \"\"\"\n    Store account information in an `sqlite3 <https://www.sqlite.org>`_ database which is\n    used to manage concurrent access to the data.\n\n    The ``update_done`` table tracks the schema updates that have been\n    completed.\n    \"\"\"\n\n    def __init__(self, file_name=None, last_upgrade_to_run=None):\n        \"\"\"\n        Initialize SqliteAccountInfo.\n\n        The exact algorithm used to determine the location of the database file is not API in any sense.\n        If the location of the database file is required (for cleanup, etc), do not assume a specific resolution:\n        instead, use ``self.filename`` to get the actual resolved location.\n\n        SqliteAccountInfo currently checks locations in the following order:\n\n        * ``file_name``, if truthy\n        * ``{B2_ACCOUNT_INFO_ENV_VAR}`` env var's value, if set\n        * ``{B2_ACCOUNT_INFO_DEFAULT_FILE}``, if it exists\n        * ``{XDG_CONFIG_HOME_ENV_VAR}/b2/account_info``, if ``{XDG_CONFIG_HOME_ENV_VAR}`` env var is set\n        * ``{B2_ACCOUNT_INFO_DEFAULT_FILE}``, as default\n\n        If the directory ``{XDG_CONFIG_HOME_ENV_VAR}/b2`` does not exist (and is needed), it is created.\n\n        :param str file_name: The sqlite file to use; overrides the default.\n        :param int last_upgrade_to_run: For testing only, override the auto-update on the db.\n        \"\"\"\n        self.thread_local = threading.local()\n\n        if file_name:\n            user_account_info_path = file_name\n        elif B2_ACCOUNT_INFO_ENV_VAR in os.environ:\n            user_account_info_path = os.environ[B2_ACCOUNT_INFO_ENV_VAR]\n        elif os.path.exists(os.path.expanduser(B2_ACCOUNT_INFO_DEFAULT_FILE)):\n            user_account_info_path = B2_ACCOUNT_INFO_DEFAULT_FILE\n        elif XDG_CONFIG_HOME_ENV_VAR in os.environ:\n            config_home = os.environ[XDG_CONFIG_HOME_ENV_VAR]\n            user_account_info_path = os.path.join(config_home, 'b2', 'account_info')\n            if not os.path.exists(os.path.join(config_home, 'b2')):\n                os.makedirs(os.path.join(config_home, 'b2'), mode=0o755)\n        else:\n            user_account_info_path = B2_ACCOUNT_INFO_DEFAULT_FILE\n\n        self.filename = os.path.expanduser(user_account_info_path)\n        logger.debug('%s file path to use: %s', self.__class__.__name__, self.filename)\n\n        self._validate_database(last_upgrade_to_run)\n        with self._get_connection() as conn:\n            self._create_tables(conn, last_upgrade_to_run)\n        super(SqliteAccountInfo, self).__init__()\n\n    # dirty trick to use parameters in the docstring\n    if getattr(__init__, '__doc__', None):  # don't break when using `python -oo`\n        __init__.__doc__ = __init__.__doc__.format(\n            **dict(\n                B2_ACCOUNT_INFO_ENV_VAR=B2_ACCOUNT_INFO_ENV_VAR,\n                B2_ACCOUNT_INFO_DEFAULT_FILE=B2_ACCOUNT_INFO_DEFAULT_FILE,\n                XDG_CONFIG_HOME_ENV_VAR=XDG_CONFIG_HOME_ENV_VAR,\n            )\n        )\n\n    def _validate_database(self, last_upgrade_to_run=None):\n        \"\"\"\n        Make sure that the database is openable.  Removes the file if it's not.\n        \"\"\"\n        # If there is no file there, that's fine.  It will get created when\n        # we connect.\n        if not os.path.exists(self.filename):\n            self._create_database(last_upgrade_to_run)\n            return\n\n        # If we can connect to the database, and do anything, then all is good.\n        try:\n            with self._connect() as conn:\n                self._create_tables(conn, last_upgrade_to_run)\n                return\n        except sqlite3.DatabaseError:\n            pass  # fall through to next case\n\n        # If the file contains JSON with the right stuff in it, convert from\n        # the old representation.\n        try:\n            with open(self.filename, 'rb') as f:\n                data = json.loads(f.read().decode('utf-8'))\n                keys = [\n                    'account_id', 'application_key', 'account_auth_token', 'api_url',\n                    'download_url', 'minimum_part_size', 'realm'\n                ]\n            if all(k in data for k in keys):\n                # remove the json file\n                os.unlink(self.filename)\n                # create a database\n                self._create_database(last_upgrade_to_run)\n                # add the data from the JSON file\n                with self._connect() as conn:\n                    self._create_tables(conn, last_upgrade_to_run)\n                    insert_statement = \"\"\"\n                        INSERT INTO account\n                        (account_id, application_key, account_auth_token, api_url, download_url, \n                         recommended_part_size, realm, absolute_minimum_part_size)\n                        values (?, ?, ?, ?, ?, ?, ?, ?);\n                    \"\"\"\n                    # Migrating from old schema is a little confusing, but the values change as:\n                    # minimum_part_size -> recommended_part_size\n                    # new column absolute_minimum_part_size = DEFAULT_ABSOLUTE_MINIMUM_PART_SIZE\n                    conn.execute(\n                        insert_statement,\n                        (*(data[k] for k in keys), DEFAULT_ABSOLUTE_MINIMUM_PART_SIZE)\n                    )\n                # all is happy now\n                return\n        except ValueError:  # includes json.decoder.JSONDecodeError\n            pass\n\n        # Remove the corrupted file and create a new database\n        raise CorruptAccountInfo(self.filename)\n\n    def _get_connection(self):\n        \"\"\"\n        Connections to sqlite cannot be shared across threads.\n        \"\"\"\n        try:\n            return self.thread_local.connection\n        except AttributeError:\n            self.thread_local.connection = self._connect()\n            return self.thread_local.connection\n\n    def _connect(self):\n        return sqlite3.connect(self.filename, isolation_level='EXCLUSIVE')\n\n    def _create_database(self, last_upgrade_to_run):\n        \"\"\"\n        Make sure that the database is created and has appropriate file permissions.\n        This should be done before storing any sensitive data in it.\n        \"\"\"\n        # Prepare a file\n        fd = os.open(\n            self.filename,\n            flags=os.O_RDWR | os.O_CREAT,\n            mode=stat.S_IRUSR | stat.S_IWUSR,\n        )\n        os.close(fd)\n        # Create the tables in the database\n        conn = self._connect()\n        try:\n            with conn:\n                self._create_tables(conn, last_upgrade_to_run)\n        finally:\n            conn.close()\n\n    def _create_tables(self, conn, last_upgrade_to_run):\n        conn.execute(\n            \"\"\"\n            CREATE TABLE IF NOT EXISTS\n            update_done (\n                update_number INT NOT NULL\n            );\n        \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n           CREATE TABLE IF NOT EXISTS\n           account (\n               account_id TEXT NOT NULL,\n               application_key TEXT NOT NULL,\n               account_auth_token TEXT NOT NULL,\n               api_url TEXT NOT NULL,\n               download_url TEXT NOT NULL,\n               minimum_part_size INT NOT NULL,\n               realm TEXT NOT NULL\n           );\n        \"\"\"\n        )\n        conn.execute(\n            \"\"\"\n           CREATE TABLE IF NOT EXISTS\n           bucket (\n               bucket_name TEXT NOT NULL,\n               bucket_id TEXT NOT NULL\n           );\n        \"\"\"\n        )\n        # This table is not used any more.  We may use it again\n        # someday if we save upload URLs across invocations of\n        # the command-line tool.\n        conn.execute(\n            \"\"\"\n           CREATE TABLE IF NOT EXISTS\n           bucket_upload_url (\n               bucket_id TEXT NOT NULL,\n               upload_url TEXT NOT NULL,\n               upload_auth_token TEXT NOT NULL\n           );\n        \"\"\"\n        )\n        # By default, we run all the upgrades\n        last_upgrade_to_run = 4 if last_upgrade_to_run is None else last_upgrade_to_run\n        # Add the 'allowed' column if it hasn't been yet.\n        if 1 <= last_upgrade_to_run:\n            self._ensure_update(1, ['ALTER TABLE account ADD COLUMN allowed TEXT;'])\n        # Add the 'account_id_or_app_key_id' column if it hasn't been yet\n        if 2 <= last_upgrade_to_run:\n            self._ensure_update(\n                2, ['ALTER TABLE account ADD COLUMN account_id_or_app_key_id TEXT;']\n            )\n        # Add the 's3_api_url' column if it hasn't been yet\n        if 3 <= last_upgrade_to_run:\n            self._ensure_update(3, ['ALTER TABLE account ADD COLUMN s3_api_url TEXT;'])\n        if 4 <= last_upgrade_to_run:\n            self._ensure_update(\n                4, [\n                    \"\"\"\n                    CREATE TABLE\n                    tmp_account (\n                        account_id TEXT NOT NULL,\n                        application_key TEXT NOT NULL,\n                        account_auth_token TEXT NOT NULL,\n                        api_url TEXT NOT NULL,\n                        download_url TEXT NOT NULL,\n                        absolute_minimum_part_size INT NOT NULL DEFAULT {},\n                        recommended_part_size INT NOT NULL,\n                        realm TEXT NOT NULL,\n                        allowed TEXT,\n                        account_id_or_app_key_id TEXT,\n                        s3_api_url TEXT    \n                    );\n                    \"\"\".format(DEFAULT_ABSOLUTE_MINIMUM_PART_SIZE),\n                    \"\"\"INSERT INTO tmp_account(\n                        account_id,\n                        application_key,\n                        account_auth_token,\n                        api_url,\n                        download_url,\n                        recommended_part_size,\n                        realm,\n                        allowed,\n                        account_id_or_app_key_id,\n                        s3_api_url\n                    ) \n                    SELECT\n                        account_id,\n                        application_key,\n                        account_auth_token,\n                        api_url,\n                        download_url,\n                        minimum_part_size,\n                        realm,\n                        allowed,\n                        account_id_or_app_key_id,\n                        s3_api_url\n                    FROM account;\n                    \"\"\",\n                    'DROP TABLE account;',\n                    \"\"\"\n                    CREATE TABLE account (\n                        account_id TEXT NOT NULL,\n                        application_key TEXT NOT NULL,\n                        account_auth_token TEXT NOT NULL,\n                        api_url TEXT NOT NULL,\n                        download_url TEXT NOT NULL,\n                        absolute_minimum_part_size INT NOT NULL,\n                        recommended_part_size INT NOT NULL,\n                        realm TEXT NOT NULL,\n                        allowed TEXT,\n                        account_id_or_app_key_id TEXT,\n                        s3_api_url TEXT    \n                    );\n                    \"\"\",\n                    \"\"\"INSERT INTO account(\n                                    account_id,\n                                    application_key,\n                                    account_auth_token,\n                                    api_url,\n                                    download_url,\n                                    absolute_minimum_part_size,\n                                    recommended_part_size,\n                                    realm,\n                                    allowed,\n                                    account_id_or_app_key_id,\n                                    s3_api_url\n                                ) \n                                SELECT\n                                    account_id,\n                                    application_key,\n                                    account_auth_token,\n                                    api_url,\n                                    download_url,\n                                    absolute_minimum_part_size,\n                                    recommended_part_size,\n                                    realm,\n                                    allowed,\n                                    account_id_or_app_key_id,\n                                    s3_api_url\n                                FROM tmp_account;\n                                \"\"\",\n                    'DROP TABLE tmp_account;',\n                ]\n            )\n\n    def _ensure_update(self, update_number, update_commands: List[str]):\n        \"\"\"\n        Run the update with the given number if it hasn't been done yet.\n\n        Does the update and stores the number as a single transaction,\n        so they will always be in sync.\n        \"\"\"\n        with self._get_connection() as conn:\n            conn.execute('BEGIN')\n            cursor = conn.execute(\n                'SELECT COUNT(*) AS count FROM update_done WHERE update_number = ?;',\n                (update_number,)\n            )\n            update_count = cursor.fetchone()[0]\n            if update_count == 0:\n                for command in update_commands:\n                    conn.execute(command)\n                conn.execute(\n                    'INSERT INTO update_done (update_number) VALUES (?);', (update_number,)\n                )\n\n    def clear(self):\n        \"\"\"\n        Remove all info about accounts and buckets.\n        \"\"\"\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM account;')\n            conn.execute('DELETE FROM bucket;')\n            conn.execute('DELETE FROM bucket_upload_url;')\n\n    def _set_auth_data(\n        self,\n        account_id,\n        auth_token,\n        api_url,\n        download_url,\n        recommended_part_size,\n        absolute_minimum_part_size,\n        application_key,\n        realm,\n        s3_api_url,\n        allowed,\n        application_key_id,\n    ):\n        assert self.allowed_is_valid(allowed)\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM account;')\n            conn.execute('DELETE FROM bucket;')\n            conn.execute('DELETE FROM bucket_upload_url;')\n            insert_statement = \"\"\"\n                INSERT INTO account\n                (account_id, account_id_or_app_key_id, application_key, account_auth_token, api_url, download_url, \n                 recommended_part_size, absolute_minimum_part_size, realm, allowed, s3_api_url)\n                values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n            \"\"\"\n\n            conn.execute(\n                insert_statement, (\n                    account_id,\n                    application_key_id,\n                    application_key,\n                    auth_token,\n                    api_url,\n                    download_url,\n                    recommended_part_size,\n                    absolute_minimum_part_size,\n                    realm,\n                    json.dumps(allowed),\n                    s3_api_url,\n                )\n            )\n\n    def set_auth_data_with_schema_0_for_test(\n        self,\n        account_id,\n        auth_token,\n        api_url,\n        download_url,\n        minimum_part_size,\n        application_key,\n        realm,\n    ):\n        \"\"\"\n        Set authentication data for tests.\n\n        :param str account_id: an account ID\n        :param str auth_token: an authentication token\n        :param str api_url: an API URL\n        :param str download_url: a download URL\n        :param int minimum_part_size: a minimum part size\n        :param str application_key: an application key\n        :param str realm: a realm to authorize account in\n        \"\"\"\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM account;')\n            conn.execute('DELETE FROM bucket;')\n            conn.execute('DELETE FROM bucket_upload_url;')\n            insert_statement = \"\"\"\n                INSERT INTO account\n                (account_id, application_key, account_auth_token, api_url, download_url, minimum_part_size, realm)\n                values (?, ?, ?, ?, ?, ?, ?);\n            \"\"\"\n\n            conn.execute(\n                insert_statement, (\n                    account_id,\n                    application_key,\n                    auth_token,\n                    api_url,\n                    download_url,\n                    minimum_part_size,\n                    realm,\n                )\n            )\n\n    def get_application_key(self):\n        return self._get_account_info_or_raise('application_key')\n\n    def get_account_id(self):\n        return self._get_account_info_or_raise('account_id')\n\n    def get_application_key_id(self):\n        \"\"\"\n        Return an application key ID.\n        The 'account_id_or_app_key_id' column was not in the original schema, so it may be NULL.\n\n        Nota bene - this is the only place where we are not renaming account_id_or_app_key_id to application_key_id\n        because it requires a column change.\n\n        application_key_id == account_id_or_app_key_id\n\n        :rtype: str\n        \"\"\"\n        result = self._get_account_info_or_raise('account_id_or_app_key_id')\n        if result is None:\n            return self.get_account_id()\n        else:\n            return result\n\n    def get_api_url(self):\n        return self._get_account_info_or_raise('api_url')\n\n    def get_account_auth_token(self):\n        return self._get_account_info_or_raise('account_auth_token')\n\n    def get_download_url(self):\n        return self._get_account_info_or_raise('download_url')\n\n    def get_realm(self):\n        return self._get_account_info_or_raise('realm')\n\n    def get_recommended_part_size(self):\n        return self._get_account_info_or_raise('recommended_part_size')\n\n    def get_absolute_minimum_part_size(self):\n        return self._get_account_info_or_raise('absolute_minimum_part_size')\n\n    def get_allowed(self):\n        \"\"\"\n        Return 'allowed' dictionary info.\n        Example:\n\n        .. code-block:: python\n\n            {\n                \"bucketId\": null,\n                \"bucketName\": null,\n                \"capabilities\": [\n                    \"listKeys\",\n                    \"writeKeys\"\n                ],\n                \"namePrefix\": null\n            }\n\n        The 'allowed' column was not in the original schema, so it may be NULL.\n\n        :rtype: dict\n        \"\"\"\n        allowed_json = self._get_account_info_or_raise('allowed')\n        if allowed_json is None:\n            return self.DEFAULT_ALLOWED\n        else:\n            return json.loads(allowed_json)\n\n    def get_s3_api_url(self):\n        result = self._get_account_info_or_raise('s3_api_url')\n        if result is None:\n            return ''\n        else:\n            return result\n\n    def _get_account_info_or_raise(self, column_name):\n        try:\n            with self._get_connection() as conn:\n                cursor = conn.execute('SELECT %s FROM account;' % (column_name,))\n                value = cursor.fetchone()[0]\n                return value\n        except Exception as e:\n            logger.exception(\n                '_get_account_info_or_raise encountered a problem while trying to retrieve \"%s\"',\n                column_name\n            )\n            raise MissingAccountData(str(e))\n\n    def refresh_entire_bucket_name_cache(self, name_id_iterable):\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM bucket;')\n            for (bucket_name, bucket_id) in name_id_iterable:\n                conn.execute(\n                    'INSERT INTO bucket (bucket_name, bucket_id) VALUES (?, ?);',\n                    (bucket_name, bucket_id)\n                )\n\n    def save_bucket(self, bucket):\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM bucket WHERE bucket_id = ?;', (bucket.id_,))\n            conn.execute(\n                'INSERT INTO bucket (bucket_id, bucket_name) VALUES (?, ?);',\n                (bucket.id_, bucket.name)\n            )\n\n    def remove_bucket_name(self, bucket_name):\n        with self._get_connection() as conn:\n            conn.execute('DELETE FROM bucket WHERE bucket_name = ?;', (bucket_name,))\n\n    def get_bucket_id_or_none_from_bucket_name(self, bucket_name):\n        return self._safe_query(\n            'SELECT bucket_id FROM bucket WHERE bucket_name = ?;', (bucket_name,)\n        )\n\n    def get_bucket_name_or_none_from_bucket_id(self, bucket_id: str) -> Optional[str]:\n        return self._safe_query('SELECT bucket_name FROM bucket WHERE bucket_id = ?;', (bucket_id,))\n\n    def _safe_query(self, query, params):\n        try:\n            with self._get_connection() as conn:\n                cursor = conn.execute(query, params)\n                return cursor.fetchone()[0]\n        except TypeError:  # TypeError: 'NoneType' object is unsubscriptable\n            return None\n        except sqlite3.Error:\n            return None\n", "######################################################################\n#\n# File: test/unit/account_info/test_account_info.py\n#\n# Copyright 2021 Backblaze Inc. All Rights Reserved.\n#\n# License https://www.backblaze.com/using_b2_code.html\n#\n######################################################################\n\nfrom abc import ABCMeta, abstractmethod\nimport json\nimport unittest.mock as mock\nimport os\nimport platform\nimport shutil\nimport stat\nimport tempfile\n\nimport pytest\n\nfrom apiver_deps import (\n    ALL_CAPABILITIES,\n    AbstractAccountInfo,\n    InMemoryAccountInfo,\n    UploadUrlPool,\n    SqliteAccountInfo,\n    TempDir,\n    B2_ACCOUNT_INFO_ENV_VAR,\n    XDG_CONFIG_HOME_ENV_VAR,\n)\nfrom apiver_deps_exception import CorruptAccountInfo, MissingAccountData\n\nfrom .fixtures import *\n\n\nclass WindowsSafeTempDir(TempDir):\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        try:\n            super().__exit__(exc_type, exc_val, exc_tb)\n        except OSError:\n            pass\n\n\nclass TestAccountInfo:\n    @pytest.fixture(autouse=True)\n    def setup(self, account_info_factory, account_info_default_data):\n        self.account_info_factory = account_info_factory\n        self.account_info_default_data = account_info_default_data\n\n    @pytest.mark.parametrize(\n        'application_key_id,realm,expected',\n        (\n            ('application_key_id', 'dev', True),\n            ('application_key_id', 'test', False),\n            ('different_application_key_id', 'dev', False),\n            ('different_application_key_id', 'test', False),\n        ),\n    )\n    def test_is_same_key(self, application_key_id, realm, expected):\n        account_info = self.account_info_factory()\n        account_info.set_auth_data(**self.account_info_default_data)\n\n        assert account_info.is_same_key(application_key_id, realm) is expected\n\n    @pytest.mark.parametrize(\n        'account_id,realm,expected',\n        (\n            ('account_id', 'dev', True),\n            ('account_id', 'test', False),\n            ('different_account_id', 'dev', False),\n            ('different_account_id', 'test', False),\n        ),\n    )\n    def test_is_same_account(self, account_id, realm, expected):\n        account_info = self.account_info_factory()\n        account_info.set_auth_data(**self.account_info_default_data)\n\n        assert account_info.is_same_account(account_id, realm) is expected\n\n    @pytest.mark.parametrize(\n        's3_api_url',\n        ('https://s3.us-east-123.backblazeb2.com', 'https://s3.us-west-321.backblazeb2.com')\n    )\n    def test_s3_api_url(self, s3_api_url):\n        account_info = self.account_info_factory()\n        account_info_default_data = {\n            **self.account_info_default_data,\n            's3_api_url': s3_api_url,\n        }\n        account_info.set_auth_data(**account_info_default_data)\n        assert s3_api_url == account_info.get_s3_api_url()\n\n    def test_getting_all_capabilities(self):\n        account_info = self.account_info_factory()\n        assert account_info.all_capabilities() == ALL_CAPABILITIES\n\n\nclass TestUploadUrlPool:\n    @pytest.fixture(autouse=True)\n    def setUp(self):\n        self.pool = UploadUrlPool()\n\n    def test_take_empty(self):\n        assert (None, None) == self.pool.take('a')\n\n    def test_put_and_take(self):\n        self.pool.put('a', 'url_a1', 'auth_token_a1')\n        self.pool.put('a', 'url_a2', 'auth_token_a2')\n        self.pool.put('b', 'url_b1', 'auth_token_b1')\n        assert ('url_a2', 'auth_token_a2') == self.pool.take('a')\n        assert ('url_a1', 'auth_token_a1') == self.pool.take('a')\n        assert (None, None) == self.pool.take('a')\n        assert ('url_b1', 'auth_token_b1') == self.pool.take('b')\n        assert (None, None) == self.pool.take('b')\n\n    def test_clear(self):\n        self.pool.put('a', 'url_a1', 'auth_token_a1')\n        self.pool.clear_for_key('a')\n        self.pool.put('b', 'url_b1', 'auth_token_b1')\n        assert (None, None) == self.pool.take('a')\n        assert ('url_b1', 'auth_token_b1') == self.pool.take('b')\n        assert (None, None) == self.pool.take('b')\n\n\nclass AccountInfoBase(metaclass=ABCMeta):\n    # it is a mixin to avoid running the tests directly (without inheritance)\n    PERSISTENCE = NotImplemented  # subclass should override this\n\n    @abstractmethod\n    def _make_info(self):\n        \"\"\"\n        returns a new object of AccountInfo class which should be tested\n        \"\"\"\n\n    def test_clear(self, account_info_default_data, apiver):\n        account_info = self._make_info()\n        account_info.set_auth_data(**account_info_default_data)\n        account_info.clear()\n\n        with pytest.raises(MissingAccountData):\n            account_info.get_account_id()\n        with pytest.raises(MissingAccountData):\n            account_info.get_account_auth_token()\n        with pytest.raises(MissingAccountData):\n            account_info.get_api_url()\n        with pytest.raises(MissingAccountData):\n            account_info.get_application_key()\n        with pytest.raises(MissingAccountData):\n            account_info.get_download_url()\n        with pytest.raises(MissingAccountData):\n            account_info.get_realm()\n        with pytest.raises(MissingAccountData):\n            account_info.get_application_key_id()\n        assert not account_info.is_same_key('key_id', 'realm')\n\n        if apiver in ['v0', 'v1']:\n            with pytest.raises(MissingAccountData):\n                account_info.get_minimum_part_size()\n        else:\n            with pytest.raises(MissingAccountData):\n                account_info.get_recommended_part_size()\n            with pytest.raises(MissingAccountData):\n                account_info.get_absolute_minimum_part_size()\n\n    def test_set_auth_data_compatibility(self, account_info_default_data):\n        account_info = self._make_info()\n\n        # The original set_auth_data\n        account_info.set_auth_data(**account_info_default_data)\n        actual = account_info.get_allowed()\n        assert AbstractAccountInfo.DEFAULT_ALLOWED == actual, 'default allowed'\n\n        # allowed was added later\n        allowed = dict(\n            bucketId=None,\n            bucketName=None,\n            capabilities=['readFiles'],\n            namePrefix=None,\n        )\n        account_info.set_auth_data(**{\n            **account_info_default_data,\n            'allowed': allowed,\n        })\n        assert allowed == account_info.get_allowed()\n\n    def test_clear_bucket_upload_data(self):\n        account_info = self._make_info()\n        account_info.put_bucket_upload_url('bucket-0', 'http://bucket-0', 'bucket-0_auth')\n        account_info.clear_bucket_upload_data('bucket-0')\n        assert (None, None) == account_info.take_bucket_upload_url('bucket-0')\n\n    def test_large_file_upload_urls(self):\n        account_info = self._make_info()\n        account_info.put_large_file_upload_url('file_0', 'http://file_0', 'auth_0')\n        assert ('http://file_0', 'auth_0') == account_info.take_large_file_upload_url('file_0')\n        assert (None, None) == account_info.take_large_file_upload_url('file_0')\n\n    def test_clear_large_file_upload_urls(self):\n        account_info = self._make_info()\n        account_info.put_large_file_upload_url('file_0', 'http://file_0', 'auth_0')\n        account_info.clear_large_file_upload_urls('file_0')\n        assert (None, None) == account_info.take_large_file_upload_url('file_0')\n\n    def test_bucket(self):\n        account_info = self._make_info()\n        bucket = mock.MagicMock()\n        bucket.name = 'my-bucket'\n        bucket.id_ = 'bucket-0'\n        assert account_info.get_bucket_id_or_none_from_bucket_name('my-bucket') is None\n        assert account_info.get_bucket_name_or_none_from_bucket_id('bucket-0') is None\n        account_info.save_bucket(bucket)\n        assert 'bucket-0' == account_info.get_bucket_id_or_none_from_bucket_name('my-bucket')\n        assert 'my-bucket' == account_info.get_bucket_name_or_none_from_bucket_id('bucket-0')\n        if self.PERSISTENCE:\n            assert 'bucket-0' == self._make_info(\n            ).get_bucket_id_or_none_from_bucket_name('my-bucket')\n            assert 'my-bucket' == self._make_info(\n            ).get_bucket_name_or_none_from_bucket_id('bucket-0')\n        account_info.remove_bucket_name('my-bucket')\n        assert account_info.get_bucket_id_or_none_from_bucket_name('my-bucket') is None\n        assert account_info.get_bucket_name_or_none_from_bucket_id('bucket-0') is None\n        if self.PERSISTENCE:\n            assert self._make_info().get_bucket_id_or_none_from_bucket_name('my-bucket') is None\n            assert self._make_info().get_bucket_name_or_none_from_bucket_id('bucket-0') is None\n\n    def test_refresh_bucket(self):\n        account_info = self._make_info()\n        assert account_info.get_bucket_id_or_none_from_bucket_name('my-bucket') is None\n        assert account_info.get_bucket_name_or_none_from_bucket_id('a') is None\n        bucket_names = {'a': 'bucket-0', 'b': 'bucket-1'}\n        account_info.refresh_entire_bucket_name_cache(bucket_names.items())\n        assert 'bucket-0' == account_info.get_bucket_id_or_none_from_bucket_name('a')\n        assert 'a' == account_info.get_bucket_name_or_none_from_bucket_id('bucket-0')\n        if self.PERSISTENCE:\n            assert 'bucket-0' == self._make_info().get_bucket_id_or_none_from_bucket_name('a')\n            assert 'a' == self._make_info().get_bucket_name_or_none_from_bucket_id('bucket-0')\n\n    @pytest.mark.apiver(to_ver=1)\n    def test_account_info_up_to_v1(self):\n        account_info = self._make_info()\n        account_info.set_auth_data(\n            'account_id',\n            'account_auth',\n            'https://api.backblazeb2.com',\n            'download_url',\n            100,\n            'app_key',\n            'realm',\n            application_key_id='key_id'\n        )\n\n        object_instances = [account_info]\n        if self.PERSISTENCE:\n            object_instances.append(self._make_info())\n        for info2 in object_instances:\n            assert 'account_id' == info2.get_account_id()\n            assert 'account_auth' == info2.get_account_auth_token()\n            assert 'https://api.backblazeb2.com' == info2.get_api_url()\n            assert 'app_key' == info2.get_application_key()\n            assert 'key_id' == info2.get_application_key_id()\n            assert 'realm' == info2.get_realm()\n            assert 100 == info2.get_minimum_part_size()\n            assert info2.is_same_key('key_id', 'realm')\n            assert not info2.is_same_key('key_id', 'another_realm')\n            assert not info2.is_same_key('another_key_id', 'realm')\n            assert not info2.is_same_key('another_key_id', 'another_realm')\n\n    @pytest.mark.apiver(from_ver=2)\n    def test_account_info_v2(self):\n        account_info = self._make_info()\n        account_info.set_auth_data(\n            account_id='account_id',\n            auth_token='account_auth',\n            api_url='https://api.backblazeb2.com',\n            download_url='download_url',\n            recommended_part_size=100,\n            absolute_minimum_part_size=50,\n            application_key='app_key',\n            realm='realm',\n            s3_api_url='s3_api_url',\n            allowed=None,\n            application_key_id='key_id',\n        )\n\n        object_instances = [account_info]\n        if self.PERSISTENCE:\n            object_instances.append(self._make_info())\n        for info2 in object_instances:\n            assert 'account_id' == info2.get_account_id()\n            assert 'account_auth' == info2.get_account_auth_token()\n            assert 'https://api.backblazeb2.com' == info2.get_api_url()\n            assert 'app_key' == info2.get_application_key()\n            assert 'key_id' == info2.get_application_key_id()\n            assert 'realm' == info2.get_realm()\n            assert 100 == info2.get_recommended_part_size()\n            assert 50 == info2.get_absolute_minimum_part_size()\n            assert info2.is_same_key('key_id', 'realm')\n            assert not info2.is_same_key('key_id', 'another_realm')\n            assert not info2.is_same_key('another_key_id', 'realm')\n            assert not info2.is_same_key('another_key_id', 'another_realm')\n\n\nclass TestInMemoryAccountInfo(AccountInfoBase):\n    PERSISTENCE = False\n\n    def _make_info(self):\n        return InMemoryAccountInfo()\n\n\nclass TestSqliteAccountInfo(AccountInfoBase):\n    PERSISTENCE = True\n\n    @pytest.fixture(autouse=True)\n    def setUp(self, request):\n        self.db_path = tempfile.NamedTemporaryFile(\n            prefix='tmp_b2_tests_%s__' % (request.node.name,), delete=True\n        ).name\n        try:\n            os.unlink(self.db_path)\n        except OSError:\n            pass\n        self.test_home = tempfile.mkdtemp()\n\n        yield\n        for cleanup_method in [\n            lambda: os.unlink(self.db_path), lambda: shutil.rmtree(self.test_home)\n        ]:\n            try:\n                cleanup_method()\n            except OSError:\n                pass\n\n    @pytest.mark.skipif(\n        platform.system() == 'Windows',\n        reason='different permission system on Windows'\n    )\n    def test_permissions(self):\n        \"\"\"\n        Test that a new database won't be readable by just any user\n        \"\"\"\n        s = SqliteAccountInfo(file_name=self.db_path,)\n        mode = os.stat(self.db_path).st_mode\n        assert stat.filemode(mode) == '-rw-------'\n\n    def test_corrupted(self):\n        \"\"\"\n        Test that a corrupted file will be replaced with a blank file.\n        \"\"\"\n        with open(self.db_path, 'wb') as f:\n            f.write(b'not a valid database')\n\n        with pytest.raises(CorruptAccountInfo):\n            self._make_info()\n\n    @pytest.mark.skipif(\n        platform.system() == 'Windows',\n        reason='it fails to upgrade on Windows, not worth to fix it anymore'\n    )\n    def test_convert_from_json(self):\n        \"\"\"\n        Tests converting from a JSON account info file, which is what version\n        0.5.2 of the command-line tool used.\n        \"\"\"\n        data = dict(\n            account_auth_token='auth_token',\n            account_id='account_id',\n            api_url='api_url',\n            application_key='application_key',\n            download_url='download_url',\n            minimum_part_size=5000,\n            realm='production'\n        )\n        with open(self.db_path, 'wb') as f:\n            f.write(json.dumps(data).encode('utf-8'))\n        account_info = self._make_info()\n        assert 'auth_token' == account_info.get_account_auth_token()\n\n    def _make_info(self):\n        return self._make_sqlite_account_info()\n\n    def _make_sqlite_account_info(self, env=None, last_upgrade_to_run=None):\n        \"\"\"\n        Returns a new SqliteAccountInfo that has just read the data from the file.\n\n        :param dict env: Override Environment variables.\n        \"\"\"\n        # Override HOME to ensure hermetic tests\n        with mock.patch('os.environ', env or {'HOME': self.test_home}):\n            return SqliteAccountInfo(\n                file_name=self.db_path if not env else None,\n                last_upgrade_to_run=last_upgrade_to_run,\n            )\n\n    def test_uses_default(self):\n        account_info = self._make_sqlite_account_info(\n            env={\n                'HOME': self.test_home,\n                'USERPROFILE': self.test_home,\n            }\n        )\n        actual_path = os.path.abspath(account_info.filename)\n        assert os.path.join(self.test_home, '.b2_account_info') == actual_path\n\n    def test_uses_xdg_config_home(self, apiver):\n        with WindowsSafeTempDir() as d:\n            account_info = self._make_sqlite_account_info(\n                env={\n                    'HOME': self.test_home,\n                    'USERPROFILE': self.test_home,\n                    XDG_CONFIG_HOME_ENV_VAR: d,\n                }\n            )\n            if apiver in ['v0', 'v1']:\n                expected_path = os.path.abspath(os.path.join(self.test_home, '.b2_account_info'))\n            else:\n                assert os.path.exists(os.path.join(d, 'b2'))\n                expected_path = os.path.abspath(os.path.join(d, 'b2', 'account_info'))\n            actual_path = os.path.abspath(account_info.filename)\n            assert expected_path == actual_path\n\n    def test_uses_existing_file_and_ignores_xdg(self):\n        with WindowsSafeTempDir() as d:\n            default_db_file_location = os.path.join(self.test_home, '.b2_account_info')\n            open(default_db_file_location, 'a').close()\n            account_info = self._make_sqlite_account_info(\n                env={\n                    'HOME': self.test_home,\n                    'USERPROFILE': self.test_home,\n                    XDG_CONFIG_HOME_ENV_VAR: d,\n                }\n            )\n            actual_path = os.path.abspath(account_info.filename)\n            assert default_db_file_location == actual_path\n            assert not os.path.exists(os.path.join(d, 'b2'))\n\n    def test_account_info_env_var_overrides_xdg_config_home(self):\n        with WindowsSafeTempDir() as d:\n            account_info = self._make_sqlite_account_info(\n                env={\n                    'HOME': self.test_home,\n                    'USERPROFILE': self.test_home,\n                    XDG_CONFIG_HOME_ENV_VAR: d,\n                    B2_ACCOUNT_INFO_ENV_VAR: os.path.join(d, 'b2_account_info'),\n                }\n            )\n            expected_path = os.path.abspath(os.path.join(d, 'b2_account_info'))\n            actual_path = os.path.abspath(account_info.filename)\n            assert expected_path == actual_path\n"], "filenames": ["CHANGELOG.md", "b2sdk/account_info/sqlite_account_info.py", "test/unit/account_info/test_account_info.py"], "buggy_code_start_loc": [15, 168, 16], "buggy_code_end_loc": [15, 181, 428], "fixing_code_start_loc": [16, 168, 17], "fixing_code_end_loc": [17, 184, 443], "type": "CWE-367", "message": "b2-sdk-python is a python library to access cloud storage provided by backblaze. Linux and Mac releases of the SDK version 1.14.0 and below contain a key disclosure vulnerability that, in certain conditions, can be exploited by local attackers through a time-of-check-time-of-use (TOCTOU) race condition. SDK users of the SqliteAccountInfo format are vulnerable while users of the InMemoryAccountInfo format are safe. The SqliteAccountInfo saves API keys (and bucket name-to-id mapping) in a local database file ($XDG_CONFIG_HOME/b2/account_info, ~/.b2_account_info or a user-defined path). When first created, the file is world readable and is (typically a few milliseconds) later altered to be private to the user. If the directory containing the file is readable by a local attacker then during the brief period between file creation and permission modification, a local attacker can race to open the file and maintain a handle to it. This allows the local attacker to read the contents after the file after the sensitive information has been saved to it. Consumers of this SDK who rely on it to save data using SqliteAccountInfo class should upgrade to the latest version of the SDK. Those who believe a local user might have opened a handle using this race condition, should remove the affected database files and regenerate all application keys. Users should upgrade to b2-sdk-python 1.14.1 or later.", "other": {"cve": {"id": "CVE-2022-23651", "sourceIdentifier": "security-advisories@github.com", "published": "2022-02-23T23:15:07.837", "lastModified": "2022-03-07T16:11:38.233", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "b2-sdk-python is a python library to access cloud storage provided by backblaze. Linux and Mac releases of the SDK version 1.14.0 and below contain a key disclosure vulnerability that, in certain conditions, can be exploited by local attackers through a time-of-check-time-of-use (TOCTOU) race condition. SDK users of the SqliteAccountInfo format are vulnerable while users of the InMemoryAccountInfo format are safe. The SqliteAccountInfo saves API keys (and bucket name-to-id mapping) in a local database file ($XDG_CONFIG_HOME/b2/account_info, ~/.b2_account_info or a user-defined path). When first created, the file is world readable and is (typically a few milliseconds) later altered to be private to the user. If the directory containing the file is readable by a local attacker then during the brief period between file creation and permission modification, a local attacker can race to open the file and maintain a handle to it. This allows the local attacker to read the contents after the file after the sensitive information has been saved to it. Consumers of this SDK who rely on it to save data using SqliteAccountInfo class should upgrade to the latest version of the SDK. Those who believe a local user might have opened a handle using this race condition, should remove the affected database files and regenerate all application keys. Users should upgrade to b2-sdk-python 1.14.1 or later."}, {"lang": "es", "value": "b2-sdk-python es una biblioteca python para acceder al almacenamiento en la nube proporcionado por backblaze. Las versiones para Linux y Mac del SDK versi\u00f3n 1.14.0 y anteriores contienen una vulnerabilidad de divulgaci\u00f3n de claves que, en determinadas condiciones, puede ser explotada por atacantes locales a mediante una condici\u00f3n de carrera de tiempo de comprobaci\u00f3n (TOCTOU). Los usuarios del SDK del formato SqliteAccountInfo son vulnerables mientras que usuarios del formato InMemoryAccountInfo est\u00e1n a salvo. El formato SqliteAccountInfo guarda las claves de la API (y el mapeo de nombres de cubos a identificadores) en un archivo de base de datos local ($XDG_CONFIG_HOME/b2/account_info, ~/.b2_account_info o una ruta definida por el usuario). Cuando es creado por primera vez, el archivo es legible para todo el mundo y es (normalmente unos pocos milisegundos) alterado m\u00e1s tarde para ser privado para el usuario. Si el directorio que contiene el archivo es legible por un atacante local, entonces durante el breve per\u00edodo entre la creaci\u00f3n del archivo y la modificaci\u00f3n de los permisos, un atacante local puede correr para abrir el archivo y mantener un manejo a \u00e9l. Esto permite al atacante local leer el contenido del archivo despu\u00e9s de que la informaci\u00f3n confidencial haya sido guardado en \u00e9l. Los consumidores de este SDK que conf\u00eden en \u00e9l para guardar datos usando la clase SqliteAccountInfo deber\u00edan actualizar a la \u00faltima versi\u00f3n del SDK. Aquellos que crean que un usuario local puede haber abierto una cuenta usando esta condici\u00f3n de carrera, deber\u00edan eliminar los archivos de base de datos afectados y regenerar todas las claves de la aplicaci\u00f3n. Los usuarios deben actualizar a b2-sdk-python versi\u00f3n 1.14.1 o posterior"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.0, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 4.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.0, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 1.9}, "baseSeverity": "LOW", "exploitabilityScore": 3.4, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-367"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:backblaze:b2_python_software_development_kit:*:*:*:*:*:linux:*:*", "versionEndIncluding": "1.14.0", "matchCriteriaId": "BC616D13-892D-4FB1-A217-41B775ACD9E9"}, {"vulnerable": true, "criteria": "cpe:2.3:a:backblaze:b2_python_software_development_kit:*:*:*:*:*:mac:*:*", "versionEndIncluding": "1.14.0", "matchCriteriaId": "0203F027-571C-407C-A643-439AAEDDC78B"}]}]}], "references": [{"url": "https://github.com/Backblaze/b2-sdk-python/commit/62476638986e5b6d7459aca5ef8ce220760226e0", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/Backblaze/b2-sdk-python/security/advisories/GHSA-p867-fxfr-ph2w", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://pypi.org/project/b2sdk/", "source": "security-advisories@github.com", "tags": ["Product"]}]}, "github_commit_url": "https://github.com/Backblaze/b2-sdk-python/commit/62476638986e5b6d7459aca5ef8ce220760226e0"}}