{"buggy_code": ["/*\n * Copyright (C) 2012,2013 - ARM Ltd\n * Author: Marc Zyngier <marc.zyngier@arm.com>\n *\n * Derived from arch/arm/kvm/guest.c:\n * Copyright (C) 2012 - Virtual Open Systems and Columbia University\n * Author: Christoffer Dall <c.dall@virtualopensystems.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n */\n\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/kvm_host.h>\n#include <linux/module.h>\n#include <linux/vmalloc.h>\n#include <linux/fs.h>\n#include <kvm/arm_psci.h>\n#include <asm/cputype.h>\n#include <linux/uaccess.h>\n#include <asm/kvm.h>\n#include <asm/kvm_emulate.h>\n#include <asm/kvm_coproc.h>\n\n#include \"trace.h\"\n\n#define VM_STAT(x) { #x, offsetof(struct kvm, stat.x), KVM_STAT_VM }\n#define VCPU_STAT(x) { #x, offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU }\n\nstruct kvm_stats_debugfs_item debugfs_entries[] = {\n\tVCPU_STAT(hvc_exit_stat),\n\tVCPU_STAT(wfe_exit_stat),\n\tVCPU_STAT(wfi_exit_stat),\n\tVCPU_STAT(mmio_exit_user),\n\tVCPU_STAT(mmio_exit_kernel),\n\tVCPU_STAT(exits),\n\t{ NULL }\n};\n\nint kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n\nstatic u64 core_reg_offset_from_id(u64 id)\n{\n\treturn id & ~(KVM_REG_ARCH_MASK | KVM_REG_SIZE_MASK | KVM_REG_ARM_CORE);\n}\n\nstatic int validate_core_offset(const struct kvm_one_reg *reg)\n{\n\tu64 off = core_reg_offset_from_id(reg->id);\n\tint size;\n\n\tswitch (off) {\n\tcase KVM_REG_ARM_CORE_REG(regs.regs[0]) ...\n\t     KVM_REG_ARM_CORE_REG(regs.regs[30]):\n\tcase KVM_REG_ARM_CORE_REG(regs.sp):\n\tcase KVM_REG_ARM_CORE_REG(regs.pc):\n\tcase KVM_REG_ARM_CORE_REG(regs.pstate):\n\tcase KVM_REG_ARM_CORE_REG(sp_el1):\n\tcase KVM_REG_ARM_CORE_REG(elr_el1):\n\tcase KVM_REG_ARM_CORE_REG(spsr[0]) ...\n\t     KVM_REG_ARM_CORE_REG(spsr[KVM_NR_SPSR - 1]):\n\t\tsize = sizeof(__u64);\n\t\tbreak;\n\n\tcase KVM_REG_ARM_CORE_REG(fp_regs.vregs[0]) ...\n\t     KVM_REG_ARM_CORE_REG(fp_regs.vregs[31]):\n\t\tsize = sizeof(__uint128_t);\n\t\tbreak;\n\n\tcase KVM_REG_ARM_CORE_REG(fp_regs.fpsr):\n\tcase KVM_REG_ARM_CORE_REG(fp_regs.fpcr):\n\t\tsize = sizeof(__u32);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (KVM_REG_SIZE(reg->id) == size &&\n\t    IS_ALIGNED(off, size / sizeof(__u32)))\n\t\treturn 0;\n\n\treturn -EINVAL;\n}\n\nstatic int get_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t/*\n\t * Because the kvm_regs structure is a mix of 32, 64 and\n\t * 128bit fields, we index it as if it was a 32bit\n\t * array. Hence below, nr_regs is the number of entries, and\n\t * off the index in the \"array\".\n\t */\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\tu32 off;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (validate_core_offset(reg))\n\t\treturn -EINVAL;\n\n\tif (copy_to_user(uaddr, ((u32 *)regs) + off, KVM_REG_SIZE(reg->id)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int set_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\t__uint128_t tmp;\n\tvoid *valp = &tmp;\n\tu64 off;\n\tint err = 0;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (validate_core_offset(reg))\n\t\treturn -EINVAL;\n\n\tif (KVM_REG_SIZE(reg->id) > sizeof(tmp))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(valp, uaddr, KVM_REG_SIZE(reg->id))) {\n\t\terr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (off == KVM_REG_ARM_CORE_REG(regs.pstate)) {\n\t\tu32 mode = (*(u32 *)valp) & PSR_AA32_MODE_MASK;\n\t\tswitch (mode) {\n\t\tcase PSR_AA32_MODE_USR:\n\t\tcase PSR_AA32_MODE_FIQ:\n\t\tcase PSR_AA32_MODE_IRQ:\n\t\tcase PSR_AA32_MODE_SVC:\n\t\tcase PSR_AA32_MODE_ABT:\n\t\tcase PSR_AA32_MODE_UND:\n\t\tcase PSR_MODE_EL0t:\n\t\tcase PSR_MODE_EL1t:\n\t\tcase PSR_MODE_EL1h:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmemcpy((u32 *)regs + off, valp, KVM_REG_SIZE(reg->id));\nout:\n\treturn err;\n}\n\nint kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\treturn -EINVAL;\n}\n\nstatic unsigned long num_core_regs(void)\n{\n\treturn sizeof(struct kvm_regs) / sizeof(__u32);\n}\n\n/**\n * ARM64 versions of the TIMER registers, always available on arm64\n */\n\n#define NUM_TIMER_REGS 3\n\nstatic bool is_timer_reg(u64 index)\n{\n\tswitch (index) {\n\tcase KVM_REG_ARM_TIMER_CTL:\n\tcase KVM_REG_ARM_TIMER_CNT:\n\tcase KVM_REG_ARM_TIMER_CVAL:\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int copy_timer_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)\n{\n\tif (put_user(KVM_REG_ARM_TIMER_CTL, uindices))\n\t\treturn -EFAULT;\n\tuindices++;\n\tif (put_user(KVM_REG_ARM_TIMER_CNT, uindices))\n\t\treturn -EFAULT;\n\tuindices++;\n\tif (put_user(KVM_REG_ARM_TIMER_CVAL, uindices))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int set_timer_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\tvoid __user *uaddr = (void __user *)(long)reg->addr;\n\tu64 val;\n\tint ret;\n\n\tret = copy_from_user(&val, uaddr, KVM_REG_SIZE(reg->id));\n\tif (ret != 0)\n\t\treturn -EFAULT;\n\n\treturn kvm_arm_timer_set_reg(vcpu, reg->id, val);\n}\n\nstatic int get_timer_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\tvoid __user *uaddr = (void __user *)(long)reg->addr;\n\tu64 val;\n\n\tval = kvm_arm_timer_get_reg(vcpu, reg->id);\n\treturn copy_to_user(uaddr, &val, KVM_REG_SIZE(reg->id)) ? -EFAULT : 0;\n}\n\n/**\n * kvm_arm_num_regs - how many registers do we present via KVM_GET_ONE_REG\n *\n * This is for all registers.\n */\nunsigned long kvm_arm_num_regs(struct kvm_vcpu *vcpu)\n{\n\treturn num_core_regs() + kvm_arm_num_sys_reg_descs(vcpu)\n\t\t+ kvm_arm_get_fw_num_regs(vcpu)\t+ NUM_TIMER_REGS;\n}\n\n/**\n * kvm_arm_copy_reg_indices - get indices of all registers.\n *\n * We do core registers right here, then we append system regs.\n */\nint kvm_arm_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)\n{\n\tunsigned int i;\n\tconst u64 core_reg = KVM_REG_ARM64 | KVM_REG_SIZE_U64 | KVM_REG_ARM_CORE;\n\tint ret;\n\n\tfor (i = 0; i < sizeof(struct kvm_regs) / sizeof(__u32); i++) {\n\t\tif (put_user(core_reg | i, uindices))\n\t\t\treturn -EFAULT;\n\t\tuindices++;\n\t}\n\n\tret = kvm_arm_copy_fw_reg_indices(vcpu, uindices);\n\tif (ret)\n\t\treturn ret;\n\tuindices += kvm_arm_get_fw_num_regs(vcpu);\n\n\tret = copy_timer_indices(vcpu, uindices);\n\tif (ret)\n\t\treturn ret;\n\tuindices += NUM_TIMER_REGS;\n\n\treturn kvm_arm_copy_sys_reg_indices(vcpu, uindices);\n}\n\nint kvm_arm_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t/* We currently use nothing arch-specific in upper 32 bits */\n\tif ((reg->id & ~KVM_REG_SIZE_MASK) >> 32 != KVM_REG_ARM64 >> 32)\n\t\treturn -EINVAL;\n\n\t/* Register group 16 means we want a core register. */\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_CORE)\n\t\treturn get_core_reg(vcpu, reg);\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_FW)\n\t\treturn kvm_arm_get_fw_reg(vcpu, reg);\n\n\tif (is_timer_reg(reg->id))\n\t\treturn get_timer_reg(vcpu, reg);\n\n\treturn kvm_arm_sys_reg_get_reg(vcpu, reg);\n}\n\nint kvm_arm_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t/* We currently use nothing arch-specific in upper 32 bits */\n\tif ((reg->id & ~KVM_REG_SIZE_MASK) >> 32 != KVM_REG_ARM64 >> 32)\n\t\treturn -EINVAL;\n\n\t/* Register group 16 means we set a core register. */\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_CORE)\n\t\treturn set_core_reg(vcpu, reg);\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_FW)\n\t\treturn kvm_arm_set_fw_reg(vcpu, reg);\n\n\tif (is_timer_reg(reg->id))\n\t\treturn set_timer_reg(vcpu, reg);\n\n\treturn kvm_arm_sys_reg_set_reg(vcpu, reg);\n}\n\nint kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs)\n{\n\treturn -EINVAL;\n}\n\nint __kvm_arm_vcpu_get_events(struct kvm_vcpu *vcpu,\n\t\t\t      struct kvm_vcpu_events *events)\n{\n\tevents->exception.serror_pending = !!(vcpu->arch.hcr_el2 & HCR_VSE);\n\tevents->exception.serror_has_esr = cpus_have_const_cap(ARM64_HAS_RAS_EXTN);\n\n\tif (events->exception.serror_pending && events->exception.serror_has_esr)\n\t\tevents->exception.serror_esr = vcpu_get_vsesr(vcpu);\n\n\treturn 0;\n}\n\nint __kvm_arm_vcpu_set_events(struct kvm_vcpu *vcpu,\n\t\t\t      struct kvm_vcpu_events *events)\n{\n\tbool serror_pending = events->exception.serror_pending;\n\tbool has_esr = events->exception.serror_has_esr;\n\n\tif (serror_pending && has_esr) {\n\t\tif (!cpus_have_const_cap(ARM64_HAS_RAS_EXTN))\n\t\t\treturn -EINVAL;\n\n\t\tif (!((events->exception.serror_esr) & ~ESR_ELx_ISS_MASK))\n\t\t\tkvm_set_sei_esr(vcpu, events->exception.serror_esr);\n\t\telse\n\t\t\treturn -EINVAL;\n\t} else if (serror_pending) {\n\t\tkvm_inject_vabt(vcpu);\n\t}\n\n\treturn 0;\n}\n\nint __attribute_const__ kvm_target_cpu(void)\n{\n\tunsigned long implementor = read_cpuid_implementor();\n\tunsigned long part_number = read_cpuid_part_number();\n\n\tswitch (implementor) {\n\tcase ARM_CPU_IMP_ARM:\n\t\tswitch (part_number) {\n\t\tcase ARM_CPU_PART_AEM_V8:\n\t\t\treturn KVM_ARM_TARGET_AEM_V8;\n\t\tcase ARM_CPU_PART_FOUNDATION:\n\t\t\treturn KVM_ARM_TARGET_FOUNDATION_V8;\n\t\tcase ARM_CPU_PART_CORTEX_A53:\n\t\t\treturn KVM_ARM_TARGET_CORTEX_A53;\n\t\tcase ARM_CPU_PART_CORTEX_A57:\n\t\t\treturn KVM_ARM_TARGET_CORTEX_A57;\n\t\t};\n\t\tbreak;\n\tcase ARM_CPU_IMP_APM:\n\t\tswitch (part_number) {\n\t\tcase APM_CPU_PART_POTENZA:\n\t\t\treturn KVM_ARM_TARGET_XGENE_POTENZA;\n\t\t};\n\t\tbreak;\n\t};\n\n\t/* Return a default generic target */\n\treturn KVM_ARM_TARGET_GENERIC_V8;\n}\n\nint kvm_vcpu_preferred_target(struct kvm_vcpu_init *init)\n{\n\tint target = kvm_target_cpu();\n\n\tif (target < 0)\n\t\treturn -ENODEV;\n\n\tmemset(init, 0, sizeof(*init));\n\n\t/*\n\t * For now, we don't return any features.\n\t * In future, we might use features to return target\n\t * specific features available for the preferred\n\t * target type.\n\t */\n\tinit->target = (__u32)target;\n\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_translation *tr)\n{\n\treturn -EINVAL;\n}\n\n#define KVM_GUESTDBG_VALID_MASK (KVM_GUESTDBG_ENABLE |    \\\n\t\t\t    KVM_GUESTDBG_USE_SW_BP | \\\n\t\t\t    KVM_GUESTDBG_USE_HW | \\\n\t\t\t    KVM_GUESTDBG_SINGLESTEP)\n\n/**\n * kvm_arch_vcpu_ioctl_set_guest_debug - set up guest debugging\n * @kvm:\tpointer to the KVM struct\n * @kvm_guest_debug: the ioctl data buffer\n *\n * This sets up and enables the VM for guest debugging. Userspace\n * passes in a control flag to enable different debug types and\n * potentially other architecture specific information in the rest of\n * the structure.\n */\nint kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_guest_debug *dbg)\n{\n\tint ret = 0;\n\n\ttrace_kvm_set_guest_debug(vcpu, dbg->control);\n\n\tif (dbg->control & ~KVM_GUESTDBG_VALID_MASK) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (dbg->control & KVM_GUESTDBG_ENABLE) {\n\t\tvcpu->guest_debug = dbg->control;\n\n\t\t/* Hardware assisted Break and Watch points */\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW) {\n\t\t\tvcpu->arch.external_debug_state = dbg->arch;\n\t\t}\n\n\t} else {\n\t\t/* If not enabled clear all flags */\n\t\tvcpu->guest_debug = 0;\n\t}\n\nout:\n\treturn ret;\n}\n\nint kvm_arm_vcpu_arch_set_attr(struct kvm_vcpu *vcpu,\n\t\t\t       struct kvm_device_attr *attr)\n{\n\tint ret;\n\n\tswitch (attr->group) {\n\tcase KVM_ARM_VCPU_PMU_V3_CTRL:\n\t\tret = kvm_arm_pmu_v3_set_attr(vcpu, attr);\n\t\tbreak;\n\tcase KVM_ARM_VCPU_TIMER_CTRL:\n\t\tret = kvm_arm_timer_set_attr(vcpu, attr);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENXIO;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nint kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,\n\t\t\t       struct kvm_device_attr *attr)\n{\n\tint ret;\n\n\tswitch (attr->group) {\n\tcase KVM_ARM_VCPU_PMU_V3_CTRL:\n\t\tret = kvm_arm_pmu_v3_get_attr(vcpu, attr);\n\t\tbreak;\n\tcase KVM_ARM_VCPU_TIMER_CTRL:\n\t\tret = kvm_arm_timer_get_attr(vcpu, attr);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENXIO;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nint kvm_arm_vcpu_arch_has_attr(struct kvm_vcpu *vcpu,\n\t\t\t       struct kvm_device_attr *attr)\n{\n\tint ret;\n\n\tswitch (attr->group) {\n\tcase KVM_ARM_VCPU_PMU_V3_CTRL:\n\t\tret = kvm_arm_pmu_v3_has_attr(vcpu, attr);\n\t\tbreak;\n\tcase KVM_ARM_VCPU_TIMER_CTRL:\n\t\tret = kvm_arm_timer_has_attr(vcpu, attr);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENXIO;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n"], "fixing_code": ["/*\n * Copyright (C) 2012,2013 - ARM Ltd\n * Author: Marc Zyngier <marc.zyngier@arm.com>\n *\n * Derived from arch/arm/kvm/guest.c:\n * Copyright (C) 2012 - Virtual Open Systems and Columbia University\n * Author: Christoffer Dall <c.dall@virtualopensystems.com>\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n */\n\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/kvm_host.h>\n#include <linux/module.h>\n#include <linux/vmalloc.h>\n#include <linux/fs.h>\n#include <kvm/arm_psci.h>\n#include <asm/cputype.h>\n#include <linux/uaccess.h>\n#include <asm/kvm.h>\n#include <asm/kvm_emulate.h>\n#include <asm/kvm_coproc.h>\n\n#include \"trace.h\"\n\n#define VM_STAT(x) { #x, offsetof(struct kvm, stat.x), KVM_STAT_VM }\n#define VCPU_STAT(x) { #x, offsetof(struct kvm_vcpu, stat.x), KVM_STAT_VCPU }\n\nstruct kvm_stats_debugfs_item debugfs_entries[] = {\n\tVCPU_STAT(hvc_exit_stat),\n\tVCPU_STAT(wfe_exit_stat),\n\tVCPU_STAT(wfi_exit_stat),\n\tVCPU_STAT(mmio_exit_user),\n\tVCPU_STAT(mmio_exit_kernel),\n\tVCPU_STAT(exits),\n\t{ NULL }\n};\n\nint kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n\nstatic u64 core_reg_offset_from_id(u64 id)\n{\n\treturn id & ~(KVM_REG_ARCH_MASK | KVM_REG_SIZE_MASK | KVM_REG_ARM_CORE);\n}\n\nstatic int validate_core_offset(const struct kvm_one_reg *reg)\n{\n\tu64 off = core_reg_offset_from_id(reg->id);\n\tint size;\n\n\tswitch (off) {\n\tcase KVM_REG_ARM_CORE_REG(regs.regs[0]) ...\n\t     KVM_REG_ARM_CORE_REG(regs.regs[30]):\n\tcase KVM_REG_ARM_CORE_REG(regs.sp):\n\tcase KVM_REG_ARM_CORE_REG(regs.pc):\n\tcase KVM_REG_ARM_CORE_REG(regs.pstate):\n\tcase KVM_REG_ARM_CORE_REG(sp_el1):\n\tcase KVM_REG_ARM_CORE_REG(elr_el1):\n\tcase KVM_REG_ARM_CORE_REG(spsr[0]) ...\n\t     KVM_REG_ARM_CORE_REG(spsr[KVM_NR_SPSR - 1]):\n\t\tsize = sizeof(__u64);\n\t\tbreak;\n\n\tcase KVM_REG_ARM_CORE_REG(fp_regs.vregs[0]) ...\n\t     KVM_REG_ARM_CORE_REG(fp_regs.vregs[31]):\n\t\tsize = sizeof(__uint128_t);\n\t\tbreak;\n\n\tcase KVM_REG_ARM_CORE_REG(fp_regs.fpsr):\n\tcase KVM_REG_ARM_CORE_REG(fp_regs.fpcr):\n\t\tsize = sizeof(__u32);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (KVM_REG_SIZE(reg->id) == size &&\n\t    IS_ALIGNED(off, size / sizeof(__u32)))\n\t\treturn 0;\n\n\treturn -EINVAL;\n}\n\nstatic int get_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t/*\n\t * Because the kvm_regs structure is a mix of 32, 64 and\n\t * 128bit fields, we index it as if it was a 32bit\n\t * array. Hence below, nr_regs is the number of entries, and\n\t * off the index in the \"array\".\n\t */\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\tu32 off;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (validate_core_offset(reg))\n\t\treturn -EINVAL;\n\n\tif (copy_to_user(uaddr, ((u32 *)regs) + off, KVM_REG_SIZE(reg->id)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int set_core_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t__u32 __user *uaddr = (__u32 __user *)(unsigned long)reg->addr;\n\tstruct kvm_regs *regs = vcpu_gp_regs(vcpu);\n\tint nr_regs = sizeof(*regs) / sizeof(__u32);\n\t__uint128_t tmp;\n\tvoid *valp = &tmp;\n\tu64 off;\n\tint err = 0;\n\n\t/* Our ID is an index into the kvm_regs struct. */\n\toff = core_reg_offset_from_id(reg->id);\n\tif (off >= nr_regs ||\n\t    (off + (KVM_REG_SIZE(reg->id) / sizeof(__u32))) >= nr_regs)\n\t\treturn -ENOENT;\n\n\tif (validate_core_offset(reg))\n\t\treturn -EINVAL;\n\n\tif (KVM_REG_SIZE(reg->id) > sizeof(tmp))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(valp, uaddr, KVM_REG_SIZE(reg->id))) {\n\t\terr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (off == KVM_REG_ARM_CORE_REG(regs.pstate)) {\n\t\tu64 mode = (*(u64 *)valp) & PSR_AA32_MODE_MASK;\n\t\tswitch (mode) {\n\t\tcase PSR_AA32_MODE_USR:\n\t\t\tif (!system_supports_32bit_el0())\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase PSR_AA32_MODE_FIQ:\n\t\tcase PSR_AA32_MODE_IRQ:\n\t\tcase PSR_AA32_MODE_SVC:\n\t\tcase PSR_AA32_MODE_ABT:\n\t\tcase PSR_AA32_MODE_UND:\n\t\t\tif (!vcpu_el1_is_32bit(vcpu))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tcase PSR_MODE_EL0t:\n\t\tcase PSR_MODE_EL1t:\n\t\tcase PSR_MODE_EL1h:\n\t\t\tif (vcpu_el1_is_32bit(vcpu))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmemcpy((u32 *)regs + off, valp, KVM_REG_SIZE(reg->id));\nout:\n\treturn err;\n}\n\nint kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\n{\n\treturn -EINVAL;\n}\n\nstatic unsigned long num_core_regs(void)\n{\n\treturn sizeof(struct kvm_regs) / sizeof(__u32);\n}\n\n/**\n * ARM64 versions of the TIMER registers, always available on arm64\n */\n\n#define NUM_TIMER_REGS 3\n\nstatic bool is_timer_reg(u64 index)\n{\n\tswitch (index) {\n\tcase KVM_REG_ARM_TIMER_CTL:\n\tcase KVM_REG_ARM_TIMER_CNT:\n\tcase KVM_REG_ARM_TIMER_CVAL:\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int copy_timer_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)\n{\n\tif (put_user(KVM_REG_ARM_TIMER_CTL, uindices))\n\t\treturn -EFAULT;\n\tuindices++;\n\tif (put_user(KVM_REG_ARM_TIMER_CNT, uindices))\n\t\treturn -EFAULT;\n\tuindices++;\n\tif (put_user(KVM_REG_ARM_TIMER_CVAL, uindices))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int set_timer_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\tvoid __user *uaddr = (void __user *)(long)reg->addr;\n\tu64 val;\n\tint ret;\n\n\tret = copy_from_user(&val, uaddr, KVM_REG_SIZE(reg->id));\n\tif (ret != 0)\n\t\treturn -EFAULT;\n\n\treturn kvm_arm_timer_set_reg(vcpu, reg->id, val);\n}\n\nstatic int get_timer_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\tvoid __user *uaddr = (void __user *)(long)reg->addr;\n\tu64 val;\n\n\tval = kvm_arm_timer_get_reg(vcpu, reg->id);\n\treturn copy_to_user(uaddr, &val, KVM_REG_SIZE(reg->id)) ? -EFAULT : 0;\n}\n\n/**\n * kvm_arm_num_regs - how many registers do we present via KVM_GET_ONE_REG\n *\n * This is for all registers.\n */\nunsigned long kvm_arm_num_regs(struct kvm_vcpu *vcpu)\n{\n\treturn num_core_regs() + kvm_arm_num_sys_reg_descs(vcpu)\n\t\t+ kvm_arm_get_fw_num_regs(vcpu)\t+ NUM_TIMER_REGS;\n}\n\n/**\n * kvm_arm_copy_reg_indices - get indices of all registers.\n *\n * We do core registers right here, then we append system regs.\n */\nint kvm_arm_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *uindices)\n{\n\tunsigned int i;\n\tconst u64 core_reg = KVM_REG_ARM64 | KVM_REG_SIZE_U64 | KVM_REG_ARM_CORE;\n\tint ret;\n\n\tfor (i = 0; i < sizeof(struct kvm_regs) / sizeof(__u32); i++) {\n\t\tif (put_user(core_reg | i, uindices))\n\t\t\treturn -EFAULT;\n\t\tuindices++;\n\t}\n\n\tret = kvm_arm_copy_fw_reg_indices(vcpu, uindices);\n\tif (ret)\n\t\treturn ret;\n\tuindices += kvm_arm_get_fw_num_regs(vcpu);\n\n\tret = copy_timer_indices(vcpu, uindices);\n\tif (ret)\n\t\treturn ret;\n\tuindices += NUM_TIMER_REGS;\n\n\treturn kvm_arm_copy_sys_reg_indices(vcpu, uindices);\n}\n\nint kvm_arm_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t/* We currently use nothing arch-specific in upper 32 bits */\n\tif ((reg->id & ~KVM_REG_SIZE_MASK) >> 32 != KVM_REG_ARM64 >> 32)\n\t\treturn -EINVAL;\n\n\t/* Register group 16 means we want a core register. */\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_CORE)\n\t\treturn get_core_reg(vcpu, reg);\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_FW)\n\t\treturn kvm_arm_get_fw_reg(vcpu, reg);\n\n\tif (is_timer_reg(reg->id))\n\t\treturn get_timer_reg(vcpu, reg);\n\n\treturn kvm_arm_sys_reg_get_reg(vcpu, reg);\n}\n\nint kvm_arm_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)\n{\n\t/* We currently use nothing arch-specific in upper 32 bits */\n\tif ((reg->id & ~KVM_REG_SIZE_MASK) >> 32 != KVM_REG_ARM64 >> 32)\n\t\treturn -EINVAL;\n\n\t/* Register group 16 means we set a core register. */\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_CORE)\n\t\treturn set_core_reg(vcpu, reg);\n\n\tif ((reg->id & KVM_REG_ARM_COPROC_MASK) == KVM_REG_ARM_FW)\n\t\treturn kvm_arm_set_fw_reg(vcpu, reg);\n\n\tif (is_timer_reg(reg->id))\n\t\treturn set_timer_reg(vcpu, reg);\n\n\treturn kvm_arm_sys_reg_set_reg(vcpu, reg);\n}\n\nint kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs)\n{\n\treturn -EINVAL;\n}\n\nint __kvm_arm_vcpu_get_events(struct kvm_vcpu *vcpu,\n\t\t\t      struct kvm_vcpu_events *events)\n{\n\tevents->exception.serror_pending = !!(vcpu->arch.hcr_el2 & HCR_VSE);\n\tevents->exception.serror_has_esr = cpus_have_const_cap(ARM64_HAS_RAS_EXTN);\n\n\tif (events->exception.serror_pending && events->exception.serror_has_esr)\n\t\tevents->exception.serror_esr = vcpu_get_vsesr(vcpu);\n\n\treturn 0;\n}\n\nint __kvm_arm_vcpu_set_events(struct kvm_vcpu *vcpu,\n\t\t\t      struct kvm_vcpu_events *events)\n{\n\tbool serror_pending = events->exception.serror_pending;\n\tbool has_esr = events->exception.serror_has_esr;\n\n\tif (serror_pending && has_esr) {\n\t\tif (!cpus_have_const_cap(ARM64_HAS_RAS_EXTN))\n\t\t\treturn -EINVAL;\n\n\t\tif (!((events->exception.serror_esr) & ~ESR_ELx_ISS_MASK))\n\t\t\tkvm_set_sei_esr(vcpu, events->exception.serror_esr);\n\t\telse\n\t\t\treturn -EINVAL;\n\t} else if (serror_pending) {\n\t\tkvm_inject_vabt(vcpu);\n\t}\n\n\treturn 0;\n}\n\nint __attribute_const__ kvm_target_cpu(void)\n{\n\tunsigned long implementor = read_cpuid_implementor();\n\tunsigned long part_number = read_cpuid_part_number();\n\n\tswitch (implementor) {\n\tcase ARM_CPU_IMP_ARM:\n\t\tswitch (part_number) {\n\t\tcase ARM_CPU_PART_AEM_V8:\n\t\t\treturn KVM_ARM_TARGET_AEM_V8;\n\t\tcase ARM_CPU_PART_FOUNDATION:\n\t\t\treturn KVM_ARM_TARGET_FOUNDATION_V8;\n\t\tcase ARM_CPU_PART_CORTEX_A53:\n\t\t\treturn KVM_ARM_TARGET_CORTEX_A53;\n\t\tcase ARM_CPU_PART_CORTEX_A57:\n\t\t\treturn KVM_ARM_TARGET_CORTEX_A57;\n\t\t};\n\t\tbreak;\n\tcase ARM_CPU_IMP_APM:\n\t\tswitch (part_number) {\n\t\tcase APM_CPU_PART_POTENZA:\n\t\t\treturn KVM_ARM_TARGET_XGENE_POTENZA;\n\t\t};\n\t\tbreak;\n\t};\n\n\t/* Return a default generic target */\n\treturn KVM_ARM_TARGET_GENERIC_V8;\n}\n\nint kvm_vcpu_preferred_target(struct kvm_vcpu_init *init)\n{\n\tint target = kvm_target_cpu();\n\n\tif (target < 0)\n\t\treturn -ENODEV;\n\n\tmemset(init, 0, sizeof(*init));\n\n\t/*\n\t * For now, we don't return any features.\n\t * In future, we might use features to return target\n\t * specific features available for the preferred\n\t * target type.\n\t */\n\tinit->target = (__u32)target;\n\n\treturn 0;\n}\n\nint kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\n{\n\treturn -EINVAL;\n}\n\nint kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_translation *tr)\n{\n\treturn -EINVAL;\n}\n\n#define KVM_GUESTDBG_VALID_MASK (KVM_GUESTDBG_ENABLE |    \\\n\t\t\t    KVM_GUESTDBG_USE_SW_BP | \\\n\t\t\t    KVM_GUESTDBG_USE_HW | \\\n\t\t\t    KVM_GUESTDBG_SINGLESTEP)\n\n/**\n * kvm_arch_vcpu_ioctl_set_guest_debug - set up guest debugging\n * @kvm:\tpointer to the KVM struct\n * @kvm_guest_debug: the ioctl data buffer\n *\n * This sets up and enables the VM for guest debugging. Userspace\n * passes in a control flag to enable different debug types and\n * potentially other architecture specific information in the rest of\n * the structure.\n */\nint kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_guest_debug *dbg)\n{\n\tint ret = 0;\n\n\ttrace_kvm_set_guest_debug(vcpu, dbg->control);\n\n\tif (dbg->control & ~KVM_GUESTDBG_VALID_MASK) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (dbg->control & KVM_GUESTDBG_ENABLE) {\n\t\tvcpu->guest_debug = dbg->control;\n\n\t\t/* Hardware assisted Break and Watch points */\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW) {\n\t\t\tvcpu->arch.external_debug_state = dbg->arch;\n\t\t}\n\n\t} else {\n\t\t/* If not enabled clear all flags */\n\t\tvcpu->guest_debug = 0;\n\t}\n\nout:\n\treturn ret;\n}\n\nint kvm_arm_vcpu_arch_set_attr(struct kvm_vcpu *vcpu,\n\t\t\t       struct kvm_device_attr *attr)\n{\n\tint ret;\n\n\tswitch (attr->group) {\n\tcase KVM_ARM_VCPU_PMU_V3_CTRL:\n\t\tret = kvm_arm_pmu_v3_set_attr(vcpu, attr);\n\t\tbreak;\n\tcase KVM_ARM_VCPU_TIMER_CTRL:\n\t\tret = kvm_arm_timer_set_attr(vcpu, attr);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENXIO;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nint kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,\n\t\t\t       struct kvm_device_attr *attr)\n{\n\tint ret;\n\n\tswitch (attr->group) {\n\tcase KVM_ARM_VCPU_PMU_V3_CTRL:\n\t\tret = kvm_arm_pmu_v3_get_attr(vcpu, attr);\n\t\tbreak;\n\tcase KVM_ARM_VCPU_TIMER_CTRL:\n\t\tret = kvm_arm_timer_get_attr(vcpu, attr);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENXIO;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nint kvm_arm_vcpu_arch_has_attr(struct kvm_vcpu *vcpu,\n\t\t\t       struct kvm_device_attr *attr)\n{\n\tint ret;\n\n\tswitch (attr->group) {\n\tcase KVM_ARM_VCPU_PMU_V3_CTRL:\n\t\tret = kvm_arm_pmu_v3_has_attr(vcpu, attr);\n\t\tbreak;\n\tcase KVM_ARM_VCPU_TIMER_CTRL:\n\t\tret = kvm_arm_timer_has_attr(vcpu, attr);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENXIO;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n"], "filenames": ["arch/arm64/kvm/guest.c"], "buggy_code_start_loc": [155], "buggy_code_end_loc": [165], "fixing_code_start_loc": [155], "fixing_code_end_loc": [174], "type": "CWE-20", "message": "arch/arm64/kvm/guest.c in KVM in the Linux kernel before 4.18.12 on the arm64 platform mishandles the KVM_SET_ON_REG ioctl. This is exploitable by attackers who can create virtual machines. An attacker can arbitrarily redirect the hypervisor flow of control (with full register control). An attacker can also cause a denial of service (hypervisor panic) via an illegal exception return. This occurs because of insufficient restrictions on userspace access to the core register file, and because PSTATE.M validation does not prevent unintended execution modes.", "other": {"cve": {"id": "CVE-2018-18021", "sourceIdentifier": "cve@mitre.org", "published": "2018-10-07T06:29:00.273", "lastModified": "2019-04-03T01:29:03.190", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "arch/arm64/kvm/guest.c in KVM in the Linux kernel before 4.18.12 on the arm64 platform mishandles the KVM_SET_ON_REG ioctl. This is exploitable by attackers who can create virtual machines. An attacker can arbitrarily redirect the hypervisor flow of control (with full register control). An attacker can also cause a denial of service (hypervisor panic) via an illegal exception return. This occurs because of insufficient restrictions on userspace access to the core register file, and because PSTATE.M validation does not prevent unintended execution modes."}, {"lang": "es", "value": "arch/arm64/kvm/guest.c en KVM en el kernel de Linux en versiones anteriores a la 4.18.12 en la plataforma arm64 gestiona de manera incorrecta la llamada IOCTL KVM_SET_ON_REG. Esto puede ser explotado por atacantes que puedan crear m\u00e1quinas virtuales. Un atacante puede redireccionar de forma arbitraria el flujo de control del hipervisor (con control de registro total). Un atacante tambi\u00e9n puede provocar una denegaci\u00f3n de servicio (p\u00e1nico del hipervisor) mediante una devoluci\u00f3n de excepci\u00f3n ilegal. Esto ocurre debido a las restricciones insuficientes sobre el acceso del espacio de usuario al archivo de registro core y, adem\u00e1s, debido a que la validaci\u00f3n PSTATE.M no evita los modos de ejecuci\u00f3n no planeados."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.1, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 3.6}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.18.12", "matchCriteriaId": "2E3BA887-5175-4A15-A927-E802ACD97543"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=2a3f93459d689d990b3ecfbe782fec89b97d3279", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=d26c25a9d19b5976b319af528886f89cf455692d", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "http://www.securityfocus.com/bid/105550", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3656", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.18.12", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://github.com/torvalds/linux/commit/2a3f93459d689d990b3ecfbe782fec89b97d3279", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://github.com/torvalds/linux/commit/d26c25a9d19b5976b319af528886f89cf455692d", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://usn.ubuntu.com/3821-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3821-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3931-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3931-2/", "source": "cve@mitre.org"}, {"url": "https://www.debian.org/security/2018/dsa-4313", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.openwall.com/lists/oss-security/2018/10/02/2", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/2a3f93459d689d990b3ecfbe782fec89b97d3279"}}