{"buggy_code": ["package service\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/http\"\n\n\t\"github.com/go-kit/kit/endpoint\"\n\tkitlog \"github.com/go-kit/kit/log\"\n\t\"github.com/igm/sockjs-go/v3/sockjs\"\n\t\"github.com/fleetdm/fleet/server/contexts/viewer\"\n\t\"github.com/fleetdm/fleet/server/kolide\"\n\t\"github.com/fleetdm/fleet/server/websocket\"\n)\n\n////////////////////////////////////////////////////////////////////////////////\n// Create Distributed Query Campaign\n////////////////////////////////////////////////////////////////////////////////\n\ntype createDistributedQueryCampaignRequest struct {\n\tQuery    string                          `json:\"query\"`\n\tSelected distributedQueryCampaignTargets `json:\"selected\"`\n}\n\ntype distributedQueryCampaignTargets struct {\n\tLabels []uint `json:\"labels\"`\n\tHosts  []uint `json:\"hosts\"`\n}\n\ntype createDistributedQueryCampaignResponse struct {\n\tCampaign *kolide.DistributedQueryCampaign `json:\"campaign,omitempty\"`\n\tErr      error                            `json:\"error,omitempty\"`\n}\n\nfunc (r createDistributedQueryCampaignResponse) error() error { return r.Err }\n\nfunc makeCreateDistributedQueryCampaignEndpoint(svc kolide.Service) endpoint.Endpoint {\n\treturn func(ctx context.Context, request interface{}) (interface{}, error) {\n\t\treq := request.(createDistributedQueryCampaignRequest)\n\t\tcampaign, err := svc.NewDistributedQueryCampaign(ctx, req.Query, req.Selected.Hosts, req.Selected.Labels)\n\t\tif err != nil {\n\t\t\treturn createDistributedQueryCampaignResponse{Err: err}, nil\n\t\t}\n\t\treturn createDistributedQueryCampaignResponse{Campaign: campaign}, nil\n\t}\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Create Distributed Query Campaign By Names\n////////////////////////////////////////////////////////////////////////////////\n\ntype createDistributedQueryCampaignByNamesRequest struct {\n\tQuery    string                                 `json:\"query\"`\n\tSelected distributedQueryCampaignTargetsByNames `json:\"selected\"`\n}\n\ntype distributedQueryCampaignTargetsByNames struct {\n\tLabels []string `json:\"labels\"`\n\tHosts  []string `json:\"hosts\"`\n}\n\nfunc makeCreateDistributedQueryCampaignByNamesEndpoint(svc kolide.Service) endpoint.Endpoint {\n\treturn func(ctx context.Context, request interface{}) (interface{}, error) {\n\t\treq := request.(createDistributedQueryCampaignByNamesRequest)\n\t\tcampaign, err := svc.NewDistributedQueryCampaignByNames(ctx, req.Query, req.Selected.Hosts, req.Selected.Labels)\n\t\tif err != nil {\n\t\t\treturn createDistributedQueryCampaignResponse{Err: err}, nil\n\t\t}\n\t\treturn createDistributedQueryCampaignResponse{Campaign: campaign}, nil\n\t}\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Stream Distributed Query Campaign Results and Metadata\n////////////////////////////////////////////////////////////////////////////////\n\nfunc makeStreamDistributedQueryCampaignResultsHandler(svc kolide.Service, jwtKey string, logger kitlog.Logger) http.Handler {\n\topt := sockjs.DefaultOptions\n\topt.Websocket = true\n\topt.RawWebsocket = true\n\treturn sockjs.NewHandler(\"/api/v1/kolide/results\", opt, func(session sockjs.Session) {\n\t\tdefer session.Close(0, \"none\")\n\n\t\tconn := &websocket.Conn{Session: session}\n\n\t\t// Receive the auth bearer token\n\t\ttoken, err := conn.ReadAuthToken()\n\t\tif err != nil {\n\t\t\tlogger.Log(\"err\", err, \"msg\", \"failed to read auth token\")\n\t\t\treturn\n\t\t}\n\n\t\t// Authenticate with the token\n\t\tvc, err := authViewer(context.Background(), jwtKey, token, svc)\n\t\tif err != nil || !vc.CanPerformActions() {\n\t\t\tlogger.Log(\"err\", err, \"msg\", \"unauthorized viewer\")\n\t\t\tconn.WriteJSONError(\"unauthorized\")\n\t\t\treturn\n\t\t}\n\n\t\tctx := viewer.NewContext(context.Background(), *vc)\n\n\t\tmsg, err := conn.ReadJSONMessage()\n\t\tif err != nil {\n\t\t\tlogger.Log(\"err\", err, \"msg\", \"reading select_campaign JSON\")\n\t\t\tconn.WriteJSONError(\"error reading select_campaign\")\n\t\t\treturn\n\t\t}\n\t\tif msg.Type != \"select_campaign\" {\n\t\t\tlogger.Log(\"err\", \"unexpected msg type, expected select_campaign\", \"msg-type\", msg.Type)\n\t\t\tconn.WriteJSONError(\"expected select_campaign\")\n\t\t\treturn\n\t\t}\n\n\t\tvar info struct {\n\t\t\tCampaignID uint `json:\"campaign_id\"`\n\t\t}\n\t\terr = json.Unmarshal(*(msg.Data.(*json.RawMessage)), &info)\n\t\tif err != nil {\n\t\t\tlogger.Log(\"err\", err, \"msg\", \"unmarshaling select_campaign data\")\n\t\t\tconn.WriteJSONError(\"error unmarshaling select_campaign data\")\n\t\t\treturn\n\t\t}\n\t\tif info.CampaignID == 0 {\n\t\t\tlogger.Log(\"err\", \"campaign ID not set\")\n\t\t\tconn.WriteJSONError(\"0 is not a valid campaign ID\")\n\t\t\treturn\n\t\t}\n\n\t\tsvc.StreamCampaignResults(ctx, conn, info.CampaignID)\n\n\t})\n}\n", "package service\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/fleetdm/fleet/server/contexts/viewer\"\n\t\"github.com/fleetdm/fleet/server/kolide\"\n\t\"github.com/fleetdm/fleet/server/websocket\"\n\t\"github.com/igm/sockjs-go/v3/sockjs\"\n\t\"github.com/pkg/errors\"\n)\n\nfunc (svc service) NewDistributedQueryCampaignByNames(ctx context.Context, queryString string, hosts []string, labels []string) (*kolide.DistributedQueryCampaign, error) {\n\thostIDs, err := svc.ds.HostIDsByName(hosts)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"finding host IDs\")\n\t}\n\n\tlabelIDs, err := svc.ds.LabelIDsByName(labels)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"finding label IDs\")\n\t}\n\n\treturn svc.NewDistributedQueryCampaign(ctx, queryString, hostIDs, labelIDs)\n}\n\nfunc uintPtr(n uint) *uint {\n\treturn &n\n}\n\nfunc (svc service) NewDistributedQueryCampaign(ctx context.Context, queryString string, hosts []uint, labels []uint) (*kolide.DistributedQueryCampaign, error) {\n\tif err := svc.StatusLiveQuery(ctx); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvc, ok := viewer.FromContext(ctx)\n\tif !ok {\n\t\treturn nil, errNoContext\n\t}\n\n\tquery := &kolide.Query{\n\t\tName:     fmt.Sprintf(\"distributed_%s_%d\", vc.Username(), time.Now().Unix()),\n\t\tQuery:    queryString,\n\t\tSaved:    false,\n\t\tAuthorID: uintPtr(vc.UserID()),\n\t}\n\tif err := query.ValidateSQL(); err != nil {\n\t\treturn nil, err\n\t}\n\tquery, err := svc.ds.NewQuery(query)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"new query\")\n\t}\n\n\tcampaign, err := svc.ds.NewDistributedQueryCampaign(&kolide.DistributedQueryCampaign{\n\t\tQueryID: query.ID,\n\t\tStatus:  kolide.QueryWaiting,\n\t\tUserID:  vc.UserID(),\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"new campaign\")\n\t}\n\n\t// Add host targets\n\tfor _, hid := range hosts {\n\t\t_, err = svc.ds.NewDistributedQueryCampaignTarget(&kolide.DistributedQueryCampaignTarget{\n\t\t\tType:                       kolide.TargetHost,\n\t\t\tDistributedQueryCampaignID: campaign.ID,\n\t\t\tTargetID:                   hid,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"adding host target\")\n\t\t}\n\t}\n\n\t// Add label targets\n\tfor _, lid := range labels {\n\t\t_, err = svc.ds.NewDistributedQueryCampaignTarget(&kolide.DistributedQueryCampaignTarget{\n\t\t\tType:                       kolide.TargetLabel,\n\t\t\tDistributedQueryCampaignID: campaign.ID,\n\t\t\tTargetID:                   lid,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"adding label target\")\n\t\t}\n\t}\n\n\thostIDs, err := svc.ds.HostIDsInTargets(hosts, labels)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"get target IDs\")\n\t}\n\n\terr = svc.liveQueryStore.RunQuery(strconv.Itoa(int(campaign.ID)), queryString, hostIDs)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"run query\")\n\t}\n\n\tcampaign.Metrics, err = svc.ds.CountHostsInTargets(hosts, labels, time.Now())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"counting hosts\")\n\t}\n\treturn campaign, nil\n}\n\ntype targetTotals struct {\n\tTotal           uint `json:\"count\"`\n\tOnline          uint `json:\"online\"`\n\tOffline         uint `json:\"offline\"`\n\tMissingInAction uint `json:\"missing_in_action\"`\n}\n\nconst (\n\tcampaignStatusPending  = \"pending\"\n\tcampaignStatusFinished = \"finished\"\n)\n\ntype campaignStatus struct {\n\tExpectedResults uint   `json:\"expected_results\"`\n\tActualResults   uint   `json:\"actual_results\"`\n\tStatus          string `json:\"status\"`\n}\n\nfunc (svc service) StreamCampaignResults(ctx context.Context, conn *websocket.Conn, campaignID uint) {\n\t// Find the campaign and ensure it is active\n\tcampaign, err := svc.ds.DistributedQueryCampaign(campaignID)\n\tif err != nil {\n\t\tconn.WriteJSONError(fmt.Sprintf(\"cannot find campaign for ID %d\", campaignID))\n\t\treturn\n\t}\n\n\t// Open the channel from which we will receive incoming query results\n\t// (probably from the redis pubsub implementation)\n\treadChan, err := svc.resultStore.ReadChannel(context.Background(), *campaign)\n\tif err != nil {\n\t\tconn.WriteJSONError(fmt.Sprintf(\"cannot open read channel for campaign %d \", campaignID))\n\t\treturn\n\t}\n\n\t// Setting status to running will cause the query to be returned to the\n\t// targets when they check in for their queries\n\tcampaign.Status = kolide.QueryRunning\n\tif err := svc.ds.SaveDistributedQueryCampaign(campaign); err != nil {\n\t\tconn.WriteJSONError(\"error saving campaign state\")\n\t\treturn\n\t}\n\n\t// Setting the status to completed stops the query from being sent to\n\t// targets. If this fails, there is a background job that will clean up\n\t// this campaign.\n\tdefer func() {\n\t\tcampaign.Status = kolide.QueryComplete\n\t\t_ = svc.ds.SaveDistributedQueryCampaign(campaign)\n\t\t_ = svc.liveQueryStore.StopQuery(strconv.Itoa(int(campaign.ID)))\n\t}()\n\n\tstatus := campaignStatus{\n\t\tStatus: campaignStatusPending,\n\t}\n\tlastStatus := status\n\tlastTotals := targetTotals{}\n\n\t// to improve performance of the frontend rendering the results table, we\n\t// add the \"host_hostname\" field to every row.\n\tmapHostnameRows := func(hostname string, rows []map[string]string) {\n\t\tfor _, row := range rows {\n\t\t\trow[\"host_hostname\"] = hostname\n\t\t}\n\t}\n\n\thostIDs, labelIDs, err := svc.ds.DistributedQueryCampaignTargetIDs(campaign.ID)\n\tif err != nil {\n\t\tconn.WriteJSONError(\"error retrieving campaign targets: \" + err.Error())\n\t\treturn\n\t}\n\n\tupdateStatus := func() error {\n\t\tmetrics, err := svc.CountHostsInTargets(context.Background(), hostIDs, labelIDs)\n\t\tif err != nil {\n\t\t\tif err = conn.WriteJSONError(\"error retrieving target counts\"); err != nil {\n\t\t\t\treturn errors.New(\"retrieve target counts\")\n\t\t\t}\n\t\t}\n\n\t\ttotals := targetTotals{\n\t\t\tTotal:           metrics.TotalHosts,\n\t\t\tOnline:          metrics.OnlineHosts,\n\t\t\tOffline:         metrics.OfflineHosts,\n\t\t\tMissingInAction: metrics.MissingInActionHosts,\n\t\t}\n\t\tif lastTotals != totals {\n\t\t\tlastTotals = totals\n\t\t\tif err = conn.WriteJSONMessage(\"totals\", totals); err != nil {\n\t\t\t\treturn errors.New(\"write totals\")\n\t\t\t}\n\t\t}\n\n\t\tstatus.ExpectedResults = totals.Online\n\t\tif status.ActualResults >= status.ExpectedResults {\n\t\t\tstatus.Status = campaignStatusFinished\n\t\t}\n\t\t// only write status message if status has changed\n\t\tif lastStatus != status {\n\t\t\tlastStatus = status\n\t\t\tif err = conn.WriteJSONMessage(\"status\", status); err != nil {\n\t\t\t\treturn errors.New(\"write status\")\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}\n\n\tif err := updateStatus(); err != nil {\n\t\t_ = svc.logger.Log(\"msg\", \"error updating status\", \"err\", err)\n\t\treturn\n\t}\n\n\t// Push status updates every 5 seconds at most\n\tticker := time.NewTicker(5 * time.Second)\n\tdefer ticker.Stop()\n\t// Loop, pushing updates to results and expected totals\n\tfor {\n\t\t// Update the expected hosts total (Should happen before\n\t\t// any results are written, to avoid the frontend showing \"x of\n\t\t// 0 Hosts Returning y Records\")\n\t\tselect {\n\t\tcase res := <-readChan:\n\t\t\t// Receive a result and push it over the websocket\n\t\t\tswitch res := res.(type) {\n\t\t\tcase kolide.DistributedQueryResult:\n\t\t\t\tmapHostnameRows(res.Host.HostName, res.Rows)\n\t\t\t\terr = conn.WriteJSONMessage(\"result\", res)\n\t\t\t\tif errors.Cause(err) == sockjs.ErrSessionNotOpen {\n\t\t\t\t\t// return and stop sending the query if the session was closed\n\t\t\t\t\t// by the client\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\t_ = svc.logger.Log(\"msg\", \"error writing to channel\", \"err\", err)\n\t\t\t\t}\n\t\t\t\tstatus.ActualResults++\n\t\t\t}\n\n\t\tcase <-ticker.C:\n\t\t\tif conn.GetSessionState() == sockjs.SessionClosed {\n\t\t\t\t// return and stop sending the query if the session was closed\n\t\t\t\t// by the client\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Update status\n\t\t\tif err := updateStatus(); err != nil {\n\t\t\t\tsvc.logger.Log(\"msg\", \"error updating status\", \"err\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n"], "fixing_code": ["package service\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/http\"\n\n\t\"github.com/fleetdm/fleet/server/contexts/viewer\"\n\t\"github.com/fleetdm/fleet/server/kolide\"\n\t\"github.com/fleetdm/fleet/server/websocket\"\n\t\"github.com/go-kit/kit/endpoint\"\n\tkitlog \"github.com/go-kit/kit/log\"\n\t\"github.com/igm/sockjs-go/v3/sockjs\"\n)\n\n////////////////////////////////////////////////////////////////////////////////\n// Create Distributed Query Campaign\n////////////////////////////////////////////////////////////////////////////////\n\ntype createDistributedQueryCampaignRequest struct {\n\tQuery    string                          `json:\"query\"`\n\tSelected distributedQueryCampaignTargets `json:\"selected\"`\n}\n\ntype distributedQueryCampaignTargets struct {\n\tLabels []uint `json:\"labels\"`\n\tHosts  []uint `json:\"hosts\"`\n}\n\ntype createDistributedQueryCampaignResponse struct {\n\tCampaign *kolide.DistributedQueryCampaign `json:\"campaign,omitempty\"`\n\tErr      error                            `json:\"error,omitempty\"`\n}\n\nfunc (r createDistributedQueryCampaignResponse) error() error { return r.Err }\n\nfunc makeCreateDistributedQueryCampaignEndpoint(svc kolide.Service) endpoint.Endpoint {\n\treturn func(ctx context.Context, request interface{}) (interface{}, error) {\n\t\treq := request.(createDistributedQueryCampaignRequest)\n\t\tcampaign, err := svc.NewDistributedQueryCampaign(ctx, req.Query, req.Selected.Hosts, req.Selected.Labels)\n\t\tif err != nil {\n\t\t\treturn createDistributedQueryCampaignResponse{Err: err}, nil\n\t\t}\n\t\treturn createDistributedQueryCampaignResponse{Campaign: campaign}, nil\n\t}\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Create Distributed Query Campaign By Names\n////////////////////////////////////////////////////////////////////////////////\n\ntype createDistributedQueryCampaignByNamesRequest struct {\n\tQuery    string                                 `json:\"query\"`\n\tSelected distributedQueryCampaignTargetsByNames `json:\"selected\"`\n}\n\ntype distributedQueryCampaignTargetsByNames struct {\n\tLabels []string `json:\"labels\"`\n\tHosts  []string `json:\"hosts\"`\n}\n\nfunc makeCreateDistributedQueryCampaignByNamesEndpoint(svc kolide.Service) endpoint.Endpoint {\n\treturn func(ctx context.Context, request interface{}) (interface{}, error) {\n\t\treq := request.(createDistributedQueryCampaignByNamesRequest)\n\t\tcampaign, err := svc.NewDistributedQueryCampaignByNames(ctx, req.Query, req.Selected.Hosts, req.Selected.Labels)\n\t\tif err != nil {\n\t\t\treturn createDistributedQueryCampaignResponse{Err: err}, nil\n\t\t}\n\t\treturn createDistributedQueryCampaignResponse{Campaign: campaign}, nil\n\t}\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Stream Distributed Query Campaign Results and Metadata\n////////////////////////////////////////////////////////////////////////////////\n\nfunc makeStreamDistributedQueryCampaignResultsHandler(svc kolide.Service, jwtKey string, logger kitlog.Logger) http.Handler {\n\topt := sockjs.DefaultOptions\n\topt.Websocket = true\n\topt.RawWebsocket = true\n\treturn sockjs.NewHandler(\"/api/v1/kolide/results\", opt, func(session sockjs.Session) {\n\t\tconn := &websocket.Conn{Session: session}\n\t\tdefer func() {\n\t\t\tif p := recover(); p != nil {\n\t\t\t\tlogger.Log(\"err\", p, \"msg\", \"panic in result handler\")\n\t\t\t\tconn.WriteJSONError(\"panic in result handler\")\n\t\t\t}\n\t\t\tsession.Close(0, \"none\")\n\t\t}()\n\n\t\t// Receive the auth bearer token\n\t\ttoken, err := conn.ReadAuthToken()\n\t\tif err != nil {\n\t\t\tlogger.Log(\"err\", err, \"msg\", \"failed to read auth token\")\n\t\t\treturn\n\t\t}\n\n\t\t// Authenticate with the token\n\t\tvc, err := authViewer(context.Background(), jwtKey, token, svc)\n\t\tif err != nil || !vc.CanPerformActions() {\n\t\t\tlogger.Log(\"err\", err, \"msg\", \"unauthorized viewer\")\n\t\t\tconn.WriteJSONError(\"unauthorized\")\n\t\t\treturn\n\t\t}\n\n\t\tctx := viewer.NewContext(context.Background(), *vc)\n\n\t\tmsg, err := conn.ReadJSONMessage()\n\t\tif err != nil {\n\t\t\tlogger.Log(\"err\", err, \"msg\", \"reading select_campaign JSON\")\n\t\t\tconn.WriteJSONError(\"error reading select_campaign\")\n\t\t\treturn\n\t\t}\n\t\tif msg.Type != \"select_campaign\" {\n\t\t\tlogger.Log(\"err\", \"unexpected msg type, expected select_campaign\", \"msg-type\", msg.Type)\n\t\t\tconn.WriteJSONError(\"expected select_campaign\")\n\t\t\treturn\n\t\t}\n\n\t\tvar info struct {\n\t\t\tCampaignID uint `json:\"campaign_id\"`\n\t\t}\n\t\terr = json.Unmarshal(*(msg.Data.(*json.RawMessage)), &info)\n\t\tif err != nil {\n\t\t\tlogger.Log(\"err\", err, \"msg\", \"unmarshaling select_campaign data\")\n\t\t\tconn.WriteJSONError(\"error unmarshaling select_campaign data\")\n\t\t\treturn\n\t\t}\n\t\tif info.CampaignID == 0 {\n\t\t\tlogger.Log(\"err\", \"campaign ID not set\")\n\t\t\tconn.WriteJSONError(\"0 is not a valid campaign ID\")\n\t\t\treturn\n\t\t}\n\n\t\tsvc.StreamCampaignResults(ctx, conn, info.CampaignID)\n\n\t})\n}\n", "package service\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/fleetdm/fleet/server/contexts/viewer\"\n\t\"github.com/fleetdm/fleet/server/kolide\"\n\t\"github.com/fleetdm/fleet/server/websocket\"\n\t\"github.com/igm/sockjs-go/v3/sockjs\"\n\t\"github.com/pkg/errors\"\n)\n\nfunc (svc service) NewDistributedQueryCampaignByNames(ctx context.Context, queryString string, hosts []string, labels []string) (*kolide.DistributedQueryCampaign, error) {\n\thostIDs, err := svc.ds.HostIDsByName(hosts)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"finding host IDs\")\n\t}\n\n\tlabelIDs, err := svc.ds.LabelIDsByName(labels)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"finding label IDs\")\n\t}\n\n\treturn svc.NewDistributedQueryCampaign(ctx, queryString, hostIDs, labelIDs)\n}\n\nfunc uintPtr(n uint) *uint {\n\treturn &n\n}\n\nfunc (svc service) NewDistributedQueryCampaign(ctx context.Context, queryString string, hosts []uint, labels []uint) (*kolide.DistributedQueryCampaign, error) {\n\tif err := svc.StatusLiveQuery(ctx); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvc, ok := viewer.FromContext(ctx)\n\tif !ok {\n\t\treturn nil, errNoContext\n\t}\n\n\tquery := &kolide.Query{\n\t\tName:     fmt.Sprintf(\"distributed_%s_%d\", vc.Username(), time.Now().Unix()),\n\t\tQuery:    queryString,\n\t\tSaved:    false,\n\t\tAuthorID: uintPtr(vc.UserID()),\n\t}\n\tif err := query.ValidateSQL(); err != nil {\n\t\treturn nil, err\n\t}\n\tquery, err := svc.ds.NewQuery(query)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"new query\")\n\t}\n\n\tcampaign, err := svc.ds.NewDistributedQueryCampaign(&kolide.DistributedQueryCampaign{\n\t\tQueryID: query.ID,\n\t\tStatus:  kolide.QueryWaiting,\n\t\tUserID:  vc.UserID(),\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"new campaign\")\n\t}\n\n\t// Add host targets\n\tfor _, hid := range hosts {\n\t\t_, err = svc.ds.NewDistributedQueryCampaignTarget(&kolide.DistributedQueryCampaignTarget{\n\t\t\tType:                       kolide.TargetHost,\n\t\t\tDistributedQueryCampaignID: campaign.ID,\n\t\t\tTargetID:                   hid,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"adding host target\")\n\t\t}\n\t}\n\n\t// Add label targets\n\tfor _, lid := range labels {\n\t\t_, err = svc.ds.NewDistributedQueryCampaignTarget(&kolide.DistributedQueryCampaignTarget{\n\t\t\tType:                       kolide.TargetLabel,\n\t\t\tDistributedQueryCampaignID: campaign.ID,\n\t\t\tTargetID:                   lid,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"adding label target\")\n\t\t}\n\t}\n\n\thostIDs, err := svc.ds.HostIDsInTargets(hosts, labels)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"get target IDs\")\n\t}\n\n\terr = svc.liveQueryStore.RunQuery(strconv.Itoa(int(campaign.ID)), queryString, hostIDs)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"run query\")\n\t}\n\n\tcampaign.Metrics, err = svc.ds.CountHostsInTargets(hosts, labels, time.Now())\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"counting hosts\")\n\t}\n\treturn campaign, nil\n}\n\ntype targetTotals struct {\n\tTotal           uint `json:\"count\"`\n\tOnline          uint `json:\"online\"`\n\tOffline         uint `json:\"offline\"`\n\tMissingInAction uint `json:\"missing_in_action\"`\n}\n\nconst (\n\tcampaignStatusPending  = \"pending\"\n\tcampaignStatusFinished = \"finished\"\n)\n\ntype campaignStatus struct {\n\tExpectedResults uint   `json:\"expected_results\"`\n\tActualResults   uint   `json:\"actual_results\"`\n\tStatus          string `json:\"status\"`\n}\n\nfunc (svc service) StreamCampaignResults(ctx context.Context, conn *websocket.Conn, campaignID uint) {\n\t// Find the campaign and ensure it is active\n\tcampaign, err := svc.ds.DistributedQueryCampaign(campaignID)\n\tif err != nil {\n\t\tconn.WriteJSONError(fmt.Sprintf(\"cannot find campaign for ID %d\", campaignID))\n\t\treturn\n\t}\n\n\t// Open the channel from which we will receive incoming query results\n\t// (probably from the redis pubsub implementation)\n\treadChan, err := svc.resultStore.ReadChannel(context.Background(), *campaign)\n\tif err != nil {\n\t\tconn.WriteJSONError(fmt.Sprintf(\"cannot open read channel for campaign %d \", campaignID))\n\t\treturn\n\t}\n\n\t// Setting status to running will cause the query to be returned to the\n\t// targets when they check in for their queries\n\tcampaign.Status = kolide.QueryRunning\n\tif err := svc.ds.SaveDistributedQueryCampaign(campaign); err != nil {\n\t\tconn.WriteJSONError(\"error saving campaign state\")\n\t\treturn\n\t}\n\n\t// Setting the status to completed stops the query from being sent to\n\t// targets. If this fails, there is a background job that will clean up\n\t// this campaign.\n\tdefer func() {\n\t\tcampaign.Status = kolide.QueryComplete\n\t\t_ = svc.ds.SaveDistributedQueryCampaign(campaign)\n\t\t_ = svc.liveQueryStore.StopQuery(strconv.Itoa(int(campaign.ID)))\n\t}()\n\n\tstatus := campaignStatus{\n\t\tStatus: campaignStatusPending,\n\t}\n\tlastStatus := status\n\tlastTotals := targetTotals{}\n\n\t// to improve performance of the frontend rendering the results table, we\n\t// add the \"host_hostname\" field to every row and clean null rows.\n\tmapHostnameRows := func(res *kolide.DistributedQueryResult) {\n\t\tfilteredRows := []map[string]string{}\n\t\tfor _, row := range res.Rows {\n\t\t\tif row == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trow[\"host_hostname\"] = res.Host.HostName\n\t\t\tfilteredRows = append(filteredRows, row)\n\t\t}\n\n\t\tres.Rows = filteredRows\n\t}\n\n\thostIDs, labelIDs, err := svc.ds.DistributedQueryCampaignTargetIDs(campaign.ID)\n\tif err != nil {\n\t\tconn.WriteJSONError(\"error retrieving campaign targets: \" + err.Error())\n\t\treturn\n\t}\n\n\tupdateStatus := func() error {\n\t\tmetrics, err := svc.CountHostsInTargets(context.Background(), hostIDs, labelIDs)\n\t\tif err != nil {\n\t\t\tif err = conn.WriteJSONError(\"error retrieving target counts\"); err != nil {\n\t\t\t\treturn errors.New(\"retrieve target counts\")\n\t\t\t}\n\t\t}\n\n\t\ttotals := targetTotals{\n\t\t\tTotal:           metrics.TotalHosts,\n\t\t\tOnline:          metrics.OnlineHosts,\n\t\t\tOffline:         metrics.OfflineHosts,\n\t\t\tMissingInAction: metrics.MissingInActionHosts,\n\t\t}\n\t\tif lastTotals != totals {\n\t\t\tlastTotals = totals\n\t\t\tif err = conn.WriteJSONMessage(\"totals\", totals); err != nil {\n\t\t\t\treturn errors.New(\"write totals\")\n\t\t\t}\n\t\t}\n\n\t\tstatus.ExpectedResults = totals.Online\n\t\tif status.ActualResults >= status.ExpectedResults {\n\t\t\tstatus.Status = campaignStatusFinished\n\t\t}\n\t\t// only write status message if status has changed\n\t\tif lastStatus != status {\n\t\t\tlastStatus = status\n\t\t\tif err = conn.WriteJSONMessage(\"status\", status); err != nil {\n\t\t\t\treturn errors.New(\"write status\")\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}\n\n\tif err := updateStatus(); err != nil {\n\t\t_ = svc.logger.Log(\"msg\", \"error updating status\", \"err\", err)\n\t\treturn\n\t}\n\n\t// Push status updates every 5 seconds at most\n\tticker := time.NewTicker(5 * time.Second)\n\tdefer ticker.Stop()\n\t// Loop, pushing updates to results and expected totals\n\tfor {\n\t\t// Update the expected hosts total (Should happen before\n\t\t// any results are written, to avoid the frontend showing \"x of\n\t\t// 0 Hosts Returning y Records\")\n\t\tselect {\n\t\tcase res := <-readChan:\n\t\t\t// Receive a result and push it over the websocket\n\t\t\tswitch res := res.(type) {\n\t\t\tcase kolide.DistributedQueryResult:\n\t\t\t\tmapHostnameRows(&res)\n\t\t\t\terr = conn.WriteJSONMessage(\"result\", res)\n\t\t\t\tif errors.Cause(err) == sockjs.ErrSessionNotOpen {\n\t\t\t\t\t// return and stop sending the query if the session was closed\n\t\t\t\t\t// by the client\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\t_ = svc.logger.Log(\"msg\", \"error writing to channel\", \"err\", err)\n\t\t\t\t}\n\t\t\t\tstatus.ActualResults++\n\t\t\t}\n\n\t\tcase <-ticker.C:\n\t\t\tif conn.GetSessionState() == sockjs.SessionClosed {\n\t\t\t\t// return and stop sending the query if the session was closed\n\t\t\t\t// by the client\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Update status\n\t\t\tif err := updateStatus(); err != nil {\n\t\t\t\tsvc.logger.Log(\"msg\", \"error updating status\", \"err\", err)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n"], "filenames": ["server/service/endpoint_campaigns.go", "server/service/service_campaigns.go"], "buggy_code_start_loc": [7, 166], "buggy_code_end_loc": [84, 234], "fixing_code_start_loc": [8, 166], "fixing_code_end_loc": [90, 241], "type": "NVD-CWE-Other", "message": "Fleet is an open source osquery manager. In Fleet before version 3.7.0 a malicious actor with a valid node key can send a badly formatted request that causes the Fleet server to exit, resulting in denial of service. This is possible only while a live query is currently ongoing. We believe the impact of this vulnerability to be low given the requirement that the actor has a valid node key. There is no information disclosure, privilege escalation, or code execution. The issue is fixed in Fleet 3.7.0.", "other": {"cve": {"id": "CVE-2021-21296", "sourceIdentifier": "security-advisories@github.com", "published": "2021-02-10T20:15:15.353", "lastModified": "2022-10-21T22:40:05.033", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Fleet is an open source osquery manager. In Fleet before version 3.7.0 a malicious actor with a valid node key can send a badly formatted request that causes the Fleet server to exit, resulting in denial of service. This is possible only while a live query is currently ongoing. We believe the impact of this vulnerability to be low given the requirement that the actor has a valid node key. There is no information disclosure, privilege escalation, or code execution. The issue is fixed in Fleet 3.7.0."}, {"lang": "es", "value": "Fleet es un administrador osquery de c\u00f3digo abierto.&#xa0;En Fleet versiones anteriores a 3.7.0, un actor malicioso con una clave de nodo v\u00e1lida puede enviar una petici\u00f3n mal formateada que hace que el servidor Fleet salga, lo que resulta en la denegaci\u00f3n del servicio.&#xa0;Esto solo es posible mientras se presenta una consulta en vivo en curso.&#xa0;Creemos que el impacto de esta vulnerabilidad es bajo el requerimiento dado de que el actor tenga una clave de nodo v\u00e1lida.&#xa0;No existe una divulgaci\u00f3n de informaci\u00f3n, escalada de privilegios ni ejecuci\u00f3n de c\u00f3digo.&#xa0;El problema es corregido en Fleet versi\u00f3n 3.7.0"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:N/I:N/A:L", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 2.7, "baseSeverity": "LOW"}, "exploitabilityScore": 1.2, "impactScore": 1.4}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:N/I:N/A:L", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 2.7, "baseSeverity": "LOW"}, "exploitabilityScore": 1.2, "impactScore": 1.4}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:S/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 4.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-Other"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:fleetdm:fleet:*:*:*:*:*:node.js:*:*", "versionEndExcluding": "3.7.0", "matchCriteriaId": "36E3F4B9-FBF0-4F41-A8A0-20F7BB120A9A"}]}]}], "references": [{"url": "https://github.com/fleetdm/fleet/commit/f68f4238e83b45b2164e4ed05df14af0f06eaf40", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/fleetdm/fleet/security/advisories/GHSA-xwh8-9p3f-3x45", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://www.npmjs.com/package/fleetctl", "source": "security-advisories@github.com", "tags": ["Product", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/fleetdm/fleet/commit/f68f4238e83b45b2164e4ed05df14af0f06eaf40"}}