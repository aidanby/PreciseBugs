{"buggy_code": ["/*\n * Copyright (c) 2000-2005 Silicon Graphics, Inc.\n * All Rights Reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it would be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write the Free Software Foundation,\n * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n */\n#include \"xfs.h\"\n#include \"xfs_shared.h\"\n#include \"xfs_format.h\"\n#include \"xfs_log_format.h\"\n#include \"xfs_trans_resv.h\"\n#include \"xfs_mount.h\"\n#include \"xfs_inode.h\"\n#include \"xfs_trans.h\"\n#include \"xfs_inode_item.h\"\n#include \"xfs_alloc.h\"\n#include \"xfs_error.h\"\n#include \"xfs_iomap.h\"\n#include \"xfs_trace.h\"\n#include \"xfs_bmap.h\"\n#include \"xfs_bmap_util.h\"\n#include \"xfs_bmap_btree.h\"\n#include \"xfs_reflink.h\"\n#include <linux/gfp.h>\n#include <linux/mpage.h>\n#include <linux/pagevec.h>\n#include <linux/writeback.h>\n\n/* flags for direct write completions */\n#define XFS_DIO_FLAG_UNWRITTEN\t(1 << 0)\n#define XFS_DIO_FLAG_APPEND\t(1 << 1)\n#define XFS_DIO_FLAG_COW\t(1 << 2)\n\n/*\n * structure owned by writepages passed to individual writepage calls\n */\nstruct xfs_writepage_ctx {\n\tstruct xfs_bmbt_irec    imap;\n\tbool\t\t\timap_valid;\n\tunsigned int\t\tio_type;\n\tstruct xfs_ioend\t*ioend;\n\tsector_t\t\tlast_block;\n};\n\nvoid\nxfs_count_page_state(\n\tstruct page\t\t*page,\n\tint\t\t\t*delalloc,\n\tint\t\t\t*unwritten)\n{\n\tstruct buffer_head\t*bh, *head;\n\n\t*delalloc = *unwritten = 0;\n\n\tbh = head = page_buffers(page);\n\tdo {\n\t\tif (buffer_unwritten(bh))\n\t\t\t(*unwritten) = 1;\n\t\telse if (buffer_delay(bh))\n\t\t\t(*delalloc) = 1;\n\t} while ((bh = bh->b_this_page) != head);\n}\n\nstruct block_device *\nxfs_find_bdev_for_inode(\n\tstruct inode\t\t*inode)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\n\tif (XFS_IS_REALTIME_INODE(ip))\n\t\treturn mp->m_rtdev_targp->bt_bdev;\n\telse\n\t\treturn mp->m_ddev_targp->bt_bdev;\n}\n\n/*\n * We're now finished for good with this page.  Update the page state via the\n * associated buffer_heads, paying attention to the start and end offsets that\n * we need to process on the page.\n *\n * Landmine Warning: bh->b_end_io() will call end_page_writeback() on the last\n * buffer in the IO. Once it does this, it is unsafe to access the bufferhead or\n * the page at all, as we may be racing with memory reclaim and it can free both\n * the bufferhead chain and the page as it will see the page as clean and\n * unused.\n */\nstatic void\nxfs_finish_page_writeback(\n\tstruct inode\t\t*inode,\n\tstruct bio_vec\t\t*bvec,\n\tint\t\t\terror)\n{\n\tunsigned int\t\tend = bvec->bv_offset + bvec->bv_len - 1;\n\tstruct buffer_head\t*head, *bh, *next;\n\tunsigned int\t\toff = 0;\n\tunsigned int\t\tbsize;\n\n\tASSERT(bvec->bv_offset < PAGE_SIZE);\n\tASSERT((bvec->bv_offset & ((1 << inode->i_blkbits) - 1)) == 0);\n\tASSERT(end < PAGE_SIZE);\n\tASSERT((bvec->bv_len & ((1 << inode->i_blkbits) - 1)) == 0);\n\n\tbh = head = page_buffers(bvec->bv_page);\n\n\tbsize = bh->b_size;\n\tdo {\n\t\tnext = bh->b_this_page;\n\t\tif (off < bvec->bv_offset)\n\t\t\tgoto next_bh;\n\t\tif (off > end)\n\t\t\tbreak;\n\t\tbh->b_end_io(bh, !error);\nnext_bh:\n\t\toff += bsize;\n\t} while ((bh = next) != head);\n}\n\n/*\n * We're now finished for good with this ioend structure.  Update the page\n * state, release holds on bios, and finally free up memory.  Do not use the\n * ioend after this.\n */\nSTATIC void\nxfs_destroy_ioend(\n\tstruct xfs_ioend\t*ioend,\n\tint\t\t\terror)\n{\n\tstruct inode\t\t*inode = ioend->io_inode;\n\tstruct bio\t\t*last = ioend->io_bio;\n\tstruct bio\t\t*bio, *next;\n\n\tfor (bio = &ioend->io_inline_bio; bio; bio = next) {\n\t\tstruct bio_vec\t*bvec;\n\t\tint\t\ti;\n\n\t\t/*\n\t\t * For the last bio, bi_private points to the ioend, so we\n\t\t * need to explicitly end the iteration here.\n\t\t */\n\t\tif (bio == last)\n\t\t\tnext = NULL;\n\t\telse\n\t\t\tnext = bio->bi_private;\n\n\t\t/* walk each page on bio, ending page IO on them */\n\t\tbio_for_each_segment_all(bvec, bio, i)\n\t\t\txfs_finish_page_writeback(inode, bvec, error);\n\n\t\tbio_put(bio);\n\t}\n}\n\n/*\n * Fast and loose check if this write could update the on-disk inode size.\n */\nstatic inline bool xfs_ioend_is_append(struct xfs_ioend *ioend)\n{\n\treturn ioend->io_offset + ioend->io_size >\n\t\tXFS_I(ioend->io_inode)->i_d.di_size;\n}\n\nSTATIC int\nxfs_setfilesize_trans_alloc(\n\tstruct xfs_ioend\t*ioend)\n{\n\tstruct xfs_mount\t*mp = XFS_I(ioend->io_inode)->i_mount;\n\tstruct xfs_trans\t*tp;\n\tint\t\t\terror;\n\n\terror = xfs_trans_alloc(mp, &M_RES(mp)->tr_fsyncts, 0, 0, 0, &tp);\n\tif (error)\n\t\treturn error;\n\n\tioend->io_append_trans = tp;\n\n\t/*\n\t * We may pass freeze protection with a transaction.  So tell lockdep\n\t * we released it.\n\t */\n\t__sb_writers_release(ioend->io_inode->i_sb, SB_FREEZE_FS);\n\t/*\n\t * We hand off the transaction to the completion thread now, so\n\t * clear the flag here.\n\t */\n\tcurrent_restore_flags_nested(&tp->t_pflags, PF_FSTRANS);\n\treturn 0;\n}\n\n/*\n * Update on-disk file size now that data has been written to disk.\n */\nSTATIC int\n__xfs_setfilesize(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_trans\t*tp,\n\txfs_off_t\t\toffset,\n\tsize_t\t\t\tsize)\n{\n\txfs_fsize_t\t\tisize;\n\n\txfs_ilock(ip, XFS_ILOCK_EXCL);\n\tisize = xfs_new_eof(ip, offset + size);\n\tif (!isize) {\n\t\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\t\txfs_trans_cancel(tp);\n\t\treturn 0;\n\t}\n\n\ttrace_xfs_setfilesize(ip, offset, size);\n\n\tip->i_d.di_size = isize;\n\txfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\treturn xfs_trans_commit(tp);\n}\n\nint\nxfs_setfilesize(\n\tstruct xfs_inode\t*ip,\n\txfs_off_t\t\toffset,\n\tsize_t\t\t\tsize)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tint\t\t\terror;\n\n\terror = xfs_trans_alloc(mp, &M_RES(mp)->tr_fsyncts, 0, 0, 0, &tp);\n\tif (error)\n\t\treturn error;\n\n\treturn __xfs_setfilesize(ip, tp, offset, size);\n}\n\nSTATIC int\nxfs_setfilesize_ioend(\n\tstruct xfs_ioend\t*ioend,\n\tint\t\t\terror)\n{\n\tstruct xfs_inode\t*ip = XFS_I(ioend->io_inode);\n\tstruct xfs_trans\t*tp = ioend->io_append_trans;\n\n\t/*\n\t * The transaction may have been allocated in the I/O submission thread,\n\t * thus we need to mark ourselves as being in a transaction manually.\n\t * Similarly for freeze protection.\n\t */\n\tcurrent_set_flags_nested(&tp->t_pflags, PF_FSTRANS);\n\t__sb_writers_acquired(VFS_I(ip)->i_sb, SB_FREEZE_FS);\n\n\t/* we abort the update if there was an IO error */\n\tif (error) {\n\t\txfs_trans_cancel(tp);\n\t\treturn error;\n\t}\n\n\treturn __xfs_setfilesize(ip, tp, ioend->io_offset, ioend->io_size);\n}\n\n/*\n * IO write completion.\n */\nSTATIC void\nxfs_end_io(\n\tstruct work_struct *work)\n{\n\tstruct xfs_ioend\t*ioend =\n\t\tcontainer_of(work, struct xfs_ioend, io_work);\n\tstruct xfs_inode\t*ip = XFS_I(ioend->io_inode);\n\tint\t\t\terror = ioend->io_bio->bi_error;\n\n\t/*\n\t * Set an error if the mount has shut down and proceed with end I/O\n\t * processing so it can perform whatever cleanups are necessary.\n\t */\n\tif (XFS_FORCED_SHUTDOWN(ip->i_mount))\n\t\terror = -EIO;\n\n\t/*\n\t * For a CoW extent, we need to move the mapping from the CoW fork\n\t * to the data fork.  If instead an error happened, just dump the\n\t * new blocks.\n\t */\n\tif (ioend->io_type == XFS_IO_COW) {\n\t\tif (error)\n\t\t\tgoto done;\n\t\tif (ioend->io_bio->bi_error) {\n\t\t\terror = xfs_reflink_cancel_cow_range(ip,\n\t\t\t\t\tioend->io_offset, ioend->io_size);\n\t\t\tgoto done;\n\t\t}\n\t\terror = xfs_reflink_end_cow(ip, ioend->io_offset,\n\t\t\t\tioend->io_size);\n\t\tif (error)\n\t\t\tgoto done;\n\t}\n\n\t/*\n\t * For unwritten extents we need to issue transactions to convert a\n\t * range to normal written extens after the data I/O has finished.\n\t * Detecting and handling completion IO errors is done individually\n\t * for each case as different cleanup operations need to be performed\n\t * on error.\n\t */\n\tif (ioend->io_type == XFS_IO_UNWRITTEN) {\n\t\tif (error)\n\t\t\tgoto done;\n\t\terror = xfs_iomap_write_unwritten(ip, ioend->io_offset,\n\t\t\t\t\t\t  ioend->io_size);\n\t} else if (ioend->io_append_trans) {\n\t\terror = xfs_setfilesize_ioend(ioend, error);\n\t} else {\n\t\tASSERT(!xfs_ioend_is_append(ioend) ||\n\t\t       ioend->io_type == XFS_IO_COW);\n\t}\n\ndone:\n\txfs_destroy_ioend(ioend, error);\n}\n\nSTATIC void\nxfs_end_bio(\n\tstruct bio\t\t*bio)\n{\n\tstruct xfs_ioend\t*ioend = bio->bi_private;\n\tstruct xfs_mount\t*mp = XFS_I(ioend->io_inode)->i_mount;\n\n\tif (ioend->io_type == XFS_IO_UNWRITTEN || ioend->io_type == XFS_IO_COW)\n\t\tqueue_work(mp->m_unwritten_workqueue, &ioend->io_work);\n\telse if (ioend->io_append_trans)\n\t\tqueue_work(mp->m_data_workqueue, &ioend->io_work);\n\telse\n\t\txfs_destroy_ioend(ioend, bio->bi_error);\n}\n\nSTATIC int\nxfs_map_blocks(\n\tstruct inode\t\t*inode,\n\tloff_t\t\t\toffset,\n\tstruct xfs_bmbt_irec\t*imap,\n\tint\t\t\ttype)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tssize_t\t\t\tcount = 1 << inode->i_blkbits;\n\txfs_fileoff_t\t\toffset_fsb, end_fsb;\n\tint\t\t\terror = 0;\n\tint\t\t\tbmapi_flags = XFS_BMAPI_ENTIRE;\n\tint\t\t\tnimaps = 1;\n\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn -EIO;\n\n\tASSERT(type != XFS_IO_COW);\n\tif (type == XFS_IO_UNWRITTEN)\n\t\tbmapi_flags |= XFS_BMAPI_IGSTATE;\n\n\txfs_ilock(ip, XFS_ILOCK_SHARED);\n\tASSERT(ip->i_d.di_format != XFS_DINODE_FMT_BTREE ||\n\t       (ip->i_df.if_flags & XFS_IFEXTENTS));\n\tASSERT(offset <= mp->m_super->s_maxbytes);\n\n\tif (offset + count > mp->m_super->s_maxbytes)\n\t\tcount = mp->m_super->s_maxbytes - offset;\n\tend_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + count);\n\toffset_fsb = XFS_B_TO_FSBT(mp, offset);\n\terror = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,\n\t\t\t\timap, &nimaps, bmapi_flags);\n\t/*\n\t * Truncate an overwrite extent if there's a pending CoW\n\t * reservation before the end of this extent.  This forces us\n\t * to come back to writepage to take care of the CoW.\n\t */\n\tif (nimaps && type == XFS_IO_OVERWRITE)\n\t\txfs_reflink_trim_irec_to_next_cow(ip, offset_fsb, imap);\n\txfs_iunlock(ip, XFS_ILOCK_SHARED);\n\n\tif (error)\n\t\treturn error;\n\n\tif (type == XFS_IO_DELALLOC &&\n\t    (!nimaps || isnullstartblock(imap->br_startblock))) {\n\t\terror = xfs_iomap_write_allocate(ip, XFS_DATA_FORK, offset,\n\t\t\t\timap);\n\t\tif (!error)\n\t\t\ttrace_xfs_map_blocks_alloc(ip, offset, count, type, imap);\n\t\treturn error;\n\t}\n\n#ifdef DEBUG\n\tif (type == XFS_IO_UNWRITTEN) {\n\t\tASSERT(nimaps);\n\t\tASSERT(imap->br_startblock != HOLESTARTBLOCK);\n\t\tASSERT(imap->br_startblock != DELAYSTARTBLOCK);\n\t}\n#endif\n\tif (nimaps)\n\t\ttrace_xfs_map_blocks_found(ip, offset, count, type, imap);\n\treturn 0;\n}\n\nSTATIC bool\nxfs_imap_valid(\n\tstruct inode\t\t*inode,\n\tstruct xfs_bmbt_irec\t*imap,\n\txfs_off_t\t\toffset)\n{\n\toffset >>= inode->i_blkbits;\n\n\treturn offset >= imap->br_startoff &&\n\t\toffset < imap->br_startoff + imap->br_blockcount;\n}\n\nSTATIC void\nxfs_start_buffer_writeback(\n\tstruct buffer_head\t*bh)\n{\n\tASSERT(buffer_mapped(bh));\n\tASSERT(buffer_locked(bh));\n\tASSERT(!buffer_delay(bh));\n\tASSERT(!buffer_unwritten(bh));\n\n\tmark_buffer_async_write(bh);\n\tset_buffer_uptodate(bh);\n\tclear_buffer_dirty(bh);\n}\n\nSTATIC void\nxfs_start_page_writeback(\n\tstruct page\t\t*page,\n\tint\t\t\tclear_dirty)\n{\n\tASSERT(PageLocked(page));\n\tASSERT(!PageWriteback(page));\n\n\t/*\n\t * if the page was not fully cleaned, we need to ensure that the higher\n\t * layers come back to it correctly. That means we need to keep the page\n\t * dirty, and for WB_SYNC_ALL writeback we need to ensure the\n\t * PAGECACHE_TAG_TOWRITE index mark is not removed so another attempt to\n\t * write this page in this writeback sweep will be made.\n\t */\n\tif (clear_dirty) {\n\t\tclear_page_dirty_for_io(page);\n\t\tset_page_writeback(page);\n\t} else\n\t\tset_page_writeback_keepwrite(page);\n\n\tunlock_page(page);\n}\n\nstatic inline int xfs_bio_add_buffer(struct bio *bio, struct buffer_head *bh)\n{\n\treturn bio_add_page(bio, bh->b_page, bh->b_size, bh_offset(bh));\n}\n\n/*\n * Submit the bio for an ioend. We are passed an ioend with a bio attached to\n * it, and we submit that bio. The ioend may be used for multiple bio\n * submissions, so we only want to allocate an append transaction for the ioend\n * once. In the case of multiple bio submission, each bio will take an IO\n * reference to the ioend to ensure that the ioend completion is only done once\n * all bios have been submitted and the ioend is really done.\n *\n * If @fail is non-zero, it means that we have a situation where some part of\n * the submission process has failed after we have marked paged for writeback\n * and unlocked them. In this situation, we need to fail the bio and ioend\n * rather than submit it to IO. This typically only happens on a filesystem\n * shutdown.\n */\nSTATIC int\nxfs_submit_ioend(\n\tstruct writeback_control *wbc,\n\tstruct xfs_ioend\t*ioend,\n\tint\t\t\tstatus)\n{\n\t/* Reserve log space if we might write beyond the on-disk inode size. */\n\tif (!status &&\n\t    ioend->io_type != XFS_IO_UNWRITTEN &&\n\t    xfs_ioend_is_append(ioend) &&\n\t    !ioend->io_append_trans)\n\t\tstatus = xfs_setfilesize_trans_alloc(ioend);\n\n\tioend->io_bio->bi_private = ioend;\n\tioend->io_bio->bi_end_io = xfs_end_bio;\n\tbio_set_op_attrs(ioend->io_bio, REQ_OP_WRITE,\n\t\t\t (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : 0);\n\t/*\n\t * If we are failing the IO now, just mark the ioend with an\n\t * error and finish it. This will run IO completion immediately\n\t * as there is only one reference to the ioend at this point in\n\t * time.\n\t */\n\tif (status) {\n\t\tioend->io_bio->bi_error = status;\n\t\tbio_endio(ioend->io_bio);\n\t\treturn status;\n\t}\n\n\tsubmit_bio(ioend->io_bio);\n\treturn 0;\n}\n\nstatic void\nxfs_init_bio_from_bh(\n\tstruct bio\t\t*bio,\n\tstruct buffer_head\t*bh)\n{\n\tbio->bi_iter.bi_sector = bh->b_blocknr * (bh->b_size >> 9);\n\tbio->bi_bdev = bh->b_bdev;\n}\n\nstatic struct xfs_ioend *\nxfs_alloc_ioend(\n\tstruct inode\t\t*inode,\n\tunsigned int\t\ttype,\n\txfs_off_t\t\toffset,\n\tstruct buffer_head\t*bh)\n{\n\tstruct xfs_ioend\t*ioend;\n\tstruct bio\t\t*bio;\n\n\tbio = bio_alloc_bioset(GFP_NOFS, BIO_MAX_PAGES, xfs_ioend_bioset);\n\txfs_init_bio_from_bh(bio, bh);\n\n\tioend = container_of(bio, struct xfs_ioend, io_inline_bio);\n\tINIT_LIST_HEAD(&ioend->io_list);\n\tioend->io_type = type;\n\tioend->io_inode = inode;\n\tioend->io_size = 0;\n\tioend->io_offset = offset;\n\tINIT_WORK(&ioend->io_work, xfs_end_io);\n\tioend->io_append_trans = NULL;\n\tioend->io_bio = bio;\n\treturn ioend;\n}\n\n/*\n * Allocate a new bio, and chain the old bio to the new one.\n *\n * Note that we have to do perform the chaining in this unintuitive order\n * so that the bi_private linkage is set up in the right direction for the\n * traversal in xfs_destroy_ioend().\n */\nstatic void\nxfs_chain_bio(\n\tstruct xfs_ioend\t*ioend,\n\tstruct writeback_control *wbc,\n\tstruct buffer_head\t*bh)\n{\n\tstruct bio *new;\n\n\tnew = bio_alloc(GFP_NOFS, BIO_MAX_PAGES);\n\txfs_init_bio_from_bh(new, bh);\n\n\tbio_chain(ioend->io_bio, new);\n\tbio_get(ioend->io_bio);\t\t/* for xfs_destroy_ioend */\n\tbio_set_op_attrs(ioend->io_bio, REQ_OP_WRITE,\n\t\t\t  (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : 0);\n\tsubmit_bio(ioend->io_bio);\n\tioend->io_bio = new;\n}\n\n/*\n * Test to see if we've been building up a completion structure for\n * earlier buffers -- if so, we try to append to this ioend if we\n * can, otherwise we finish off any current ioend and start another.\n * Return the ioend we finished off so that the caller can submit it\n * once it has finished processing the dirty page.\n */\nSTATIC void\nxfs_add_to_ioend(\n\tstruct inode\t\t*inode,\n\tstruct buffer_head\t*bh,\n\txfs_off_t\t\toffset,\n\tstruct xfs_writepage_ctx *wpc,\n\tstruct writeback_control *wbc,\n\tstruct list_head\t*iolist)\n{\n\tif (!wpc->ioend || wpc->io_type != wpc->ioend->io_type ||\n\t    bh->b_blocknr != wpc->last_block + 1 ||\n\t    offset != wpc->ioend->io_offset + wpc->ioend->io_size) {\n\t\tif (wpc->ioend)\n\t\t\tlist_add(&wpc->ioend->io_list, iolist);\n\t\twpc->ioend = xfs_alloc_ioend(inode, wpc->io_type, offset, bh);\n\t}\n\n\t/*\n\t * If the buffer doesn't fit into the bio we need to allocate a new\n\t * one.  This shouldn't happen more than once for a given buffer.\n\t */\n\twhile (xfs_bio_add_buffer(wpc->ioend->io_bio, bh) != bh->b_size)\n\t\txfs_chain_bio(wpc->ioend, wbc, bh);\n\n\twpc->ioend->io_size += bh->b_size;\n\twpc->last_block = bh->b_blocknr;\n\txfs_start_buffer_writeback(bh);\n}\n\nSTATIC void\nxfs_map_buffer(\n\tstruct inode\t\t*inode,\n\tstruct buffer_head\t*bh,\n\tstruct xfs_bmbt_irec\t*imap,\n\txfs_off_t\t\toffset)\n{\n\tsector_t\t\tbn;\n\tstruct xfs_mount\t*m = XFS_I(inode)->i_mount;\n\txfs_off_t\t\tiomap_offset = XFS_FSB_TO_B(m, imap->br_startoff);\n\txfs_daddr_t\t\tiomap_bn = xfs_fsb_to_db(XFS_I(inode), imap->br_startblock);\n\n\tASSERT(imap->br_startblock != HOLESTARTBLOCK);\n\tASSERT(imap->br_startblock != DELAYSTARTBLOCK);\n\n\tbn = (iomap_bn >> (inode->i_blkbits - BBSHIFT)) +\n\t      ((offset - iomap_offset) >> inode->i_blkbits);\n\n\tASSERT(bn || XFS_IS_REALTIME_INODE(XFS_I(inode)));\n\n\tbh->b_blocknr = bn;\n\tset_buffer_mapped(bh);\n}\n\nSTATIC void\nxfs_map_at_offset(\n\tstruct inode\t\t*inode,\n\tstruct buffer_head\t*bh,\n\tstruct xfs_bmbt_irec\t*imap,\n\txfs_off_t\t\toffset)\n{\n\tASSERT(imap->br_startblock != HOLESTARTBLOCK);\n\tASSERT(imap->br_startblock != DELAYSTARTBLOCK);\n\n\txfs_map_buffer(inode, bh, imap, offset);\n\tset_buffer_mapped(bh);\n\tclear_buffer_delay(bh);\n\tclear_buffer_unwritten(bh);\n}\n\n/*\n * Test if a given page contains at least one buffer of a given @type.\n * If @check_all_buffers is true, then we walk all the buffers in the page to\n * try to find one of the type passed in. If it is not set, then the caller only\n * needs to check the first buffer on the page for a match.\n */\nSTATIC bool\nxfs_check_page_type(\n\tstruct page\t\t*page,\n\tunsigned int\t\ttype,\n\tbool\t\t\tcheck_all_buffers)\n{\n\tstruct buffer_head\t*bh;\n\tstruct buffer_head\t*head;\n\n\tif (PageWriteback(page))\n\t\treturn false;\n\tif (!page->mapping)\n\t\treturn false;\n\tif (!page_has_buffers(page))\n\t\treturn false;\n\n\tbh = head = page_buffers(page);\n\tdo {\n\t\tif (buffer_unwritten(bh)) {\n\t\t\tif (type == XFS_IO_UNWRITTEN)\n\t\t\t\treturn true;\n\t\t} else if (buffer_delay(bh)) {\n\t\t\tif (type == XFS_IO_DELALLOC)\n\t\t\t\treturn true;\n\t\t} else if (buffer_dirty(bh) && buffer_mapped(bh)) {\n\t\t\tif (type == XFS_IO_OVERWRITE)\n\t\t\t\treturn true;\n\t\t}\n\n\t\t/* If we are only checking the first buffer, we are done now. */\n\t\tif (!check_all_buffers)\n\t\t\tbreak;\n\t} while ((bh = bh->b_this_page) != head);\n\n\treturn false;\n}\n\nSTATIC void\nxfs_vm_invalidatepage(\n\tstruct page\t\t*page,\n\tunsigned int\t\toffset,\n\tunsigned int\t\tlength)\n{\n\ttrace_xfs_invalidatepage(page->mapping->host, page, offset,\n\t\t\t\t length);\n\tblock_invalidatepage(page, offset, length);\n}\n\n/*\n * If the page has delalloc buffers on it, we need to punch them out before we\n * invalidate the page. If we don't, we leave a stale delalloc mapping on the\n * inode that can trip a BUG() in xfs_get_blocks() later on if a direct IO read\n * is done on that same region - the delalloc extent is returned when none is\n * supposed to be there.\n *\n * We prevent this by truncating away the delalloc regions on the page before\n * invalidating it. Because they are delalloc, we can do this without needing a\n * transaction. Indeed - if we get ENOSPC errors, we have to be able to do this\n * truncation without a transaction as there is no space left for block\n * reservation (typically why we see a ENOSPC in writeback).\n *\n * This is not a performance critical path, so for now just do the punching a\n * buffer head at a time.\n */\nSTATIC void\nxfs_aops_discard_page(\n\tstruct page\t\t*page)\n{\n\tstruct inode\t\t*inode = page->mapping->host;\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct buffer_head\t*bh, *head;\n\tloff_t\t\t\toffset = page_offset(page);\n\n\tif (!xfs_check_page_type(page, XFS_IO_DELALLOC, true))\n\t\tgoto out_invalidate;\n\n\tif (XFS_FORCED_SHUTDOWN(ip->i_mount))\n\t\tgoto out_invalidate;\n\n\txfs_alert(ip->i_mount,\n\t\t\"page discard on page %p, inode 0x%llx, offset %llu.\",\n\t\t\tpage, ip->i_ino, offset);\n\n\txfs_ilock(ip, XFS_ILOCK_EXCL);\n\tbh = head = page_buffers(page);\n\tdo {\n\t\tint\t\terror;\n\t\txfs_fileoff_t\tstart_fsb;\n\n\t\tif (!buffer_delay(bh))\n\t\t\tgoto next_buffer;\n\n\t\tstart_fsb = XFS_B_TO_FSBT(ip->i_mount, offset);\n\t\terror = xfs_bmap_punch_delalloc_range(ip, start_fsb, 1);\n\t\tif (error) {\n\t\t\t/* something screwed, just bail */\n\t\t\tif (!XFS_FORCED_SHUTDOWN(ip->i_mount)) {\n\t\t\t\txfs_alert(ip->i_mount,\n\t\t\t\"page discard unable to remove delalloc mapping.\");\n\t\t\t}\n\t\t\tbreak;\n\t\t}\nnext_buffer:\n\t\toffset += 1 << inode->i_blkbits;\n\n\t} while ((bh = bh->b_this_page) != head);\n\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\nout_invalidate:\n\txfs_vm_invalidatepage(page, 0, PAGE_SIZE);\n\treturn;\n}\n\nstatic int\nxfs_map_cow(\n\tstruct xfs_writepage_ctx *wpc,\n\tstruct inode\t\t*inode,\n\tloff_t\t\t\toffset,\n\tunsigned int\t\t*new_type)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_bmbt_irec\timap;\n\tbool\t\t\tis_cow = false, need_alloc = false;\n\tint\t\t\terror;\n\n\t/*\n\t * If we already have a valid COW mapping keep using it.\n\t */\n\tif (wpc->io_type == XFS_IO_COW) {\n\t\twpc->imap_valid = xfs_imap_valid(inode, &wpc->imap, offset);\n\t\tif (wpc->imap_valid) {\n\t\t\t*new_type = XFS_IO_COW;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * Else we need to check if there is a COW mapping at this offset.\n\t */\n\txfs_ilock(ip, XFS_ILOCK_SHARED);\n\tis_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap, &need_alloc);\n\txfs_iunlock(ip, XFS_ILOCK_SHARED);\n\n\tif (!is_cow)\n\t\treturn 0;\n\n\t/*\n\t * And if the COW mapping has a delayed extent here we need to\n\t * allocate real space for it now.\n\t */\n\tif (need_alloc) {\n\t\terror = xfs_iomap_write_allocate(ip, XFS_COW_FORK, offset,\n\t\t\t\t&imap);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\twpc->io_type = *new_type = XFS_IO_COW;\n\twpc->imap_valid = true;\n\twpc->imap = imap;\n\treturn 0;\n}\n\n/*\n * We implement an immediate ioend submission policy here to avoid needing to\n * chain multiple ioends and hence nest mempool allocations which can violate\n * forward progress guarantees we need to provide. The current ioend we are\n * adding buffers to is cached on the writepage context, and if the new buffer\n * does not append to the cached ioend it will create a new ioend and cache that\n * instead.\n *\n * If a new ioend is created and cached, the old ioend is returned and queued\n * locally for submission once the entire page is processed or an error has been\n * detected.  While ioends are submitted immediately after they are completed,\n * batching optimisations are provided by higher level block plugging.\n *\n * At the end of a writeback pass, there will be a cached ioend remaining on the\n * writepage context that the caller will need to submit.\n */\nstatic int\nxfs_writepage_map(\n\tstruct xfs_writepage_ctx *wpc,\n\tstruct writeback_control *wbc,\n\tstruct inode\t\t*inode,\n\tstruct page\t\t*page,\n\tloff_t\t\t\toffset,\n\t__uint64_t              end_offset)\n{\n\tLIST_HEAD(submit_list);\n\tstruct xfs_ioend\t*ioend, *next;\n\tstruct buffer_head\t*bh, *head;\n\tssize_t\t\t\tlen = 1 << inode->i_blkbits;\n\tint\t\t\terror = 0;\n\tint\t\t\tcount = 0;\n\tint\t\t\tuptodate = 1;\n\tunsigned int\t\tnew_type;\n\n\tbh = head = page_buffers(page);\n\toffset = page_offset(page);\n\tdo {\n\t\tif (offset >= end_offset)\n\t\t\tbreak;\n\t\tif (!buffer_uptodate(bh))\n\t\t\tuptodate = 0;\n\n\t\t/*\n\t\t * set_page_dirty dirties all buffers in a page, independent\n\t\t * of their state.  The dirty state however is entirely\n\t\t * meaningless for holes (!mapped && uptodate), so skip\n\t\t * buffers covering holes here.\n\t\t */\n\t\tif (!buffer_mapped(bh) && buffer_uptodate(bh)) {\n\t\t\twpc->imap_valid = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (buffer_unwritten(bh))\n\t\t\tnew_type = XFS_IO_UNWRITTEN;\n\t\telse if (buffer_delay(bh))\n\t\t\tnew_type = XFS_IO_DELALLOC;\n\t\telse if (buffer_uptodate(bh))\n\t\t\tnew_type = XFS_IO_OVERWRITE;\n\t\telse {\n\t\t\tif (PageUptodate(page))\n\t\t\t\tASSERT(buffer_mapped(bh));\n\t\t\t/*\n\t\t\t * This buffer is not uptodate and will not be\n\t\t\t * written to disk.  Ensure that we will put any\n\t\t\t * subsequent writeable buffers into a new\n\t\t\t * ioend.\n\t\t\t */\n\t\t\twpc->imap_valid = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (xfs_is_reflink_inode(XFS_I(inode))) {\n\t\t\terror = xfs_map_cow(wpc, inode, offset, &new_type);\n\t\t\tif (error)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (wpc->io_type != new_type) {\n\t\t\twpc->io_type = new_type;\n\t\t\twpc->imap_valid = false;\n\t\t}\n\n\t\tif (wpc->imap_valid)\n\t\t\twpc->imap_valid = xfs_imap_valid(inode, &wpc->imap,\n\t\t\t\t\t\t\t offset);\n\t\tif (!wpc->imap_valid) {\n\t\t\terror = xfs_map_blocks(inode, offset, &wpc->imap,\n\t\t\t\t\t     wpc->io_type);\n\t\t\tif (error)\n\t\t\t\tgoto out;\n\t\t\twpc->imap_valid = xfs_imap_valid(inode, &wpc->imap,\n\t\t\t\t\t\t\t offset);\n\t\t}\n\t\tif (wpc->imap_valid) {\n\t\t\tlock_buffer(bh);\n\t\t\tif (wpc->io_type != XFS_IO_OVERWRITE)\n\t\t\t\txfs_map_at_offset(inode, bh, &wpc->imap, offset);\n\t\t\txfs_add_to_ioend(inode, bh, offset, wpc, wbc, &submit_list);\n\t\t\tcount++;\n\t\t}\n\n\t} while (offset += len, ((bh = bh->b_this_page) != head));\n\n\tif (uptodate && bh == head)\n\t\tSetPageUptodate(page);\n\n\tASSERT(wpc->ioend || list_empty(&submit_list));\n\nout:\n\t/*\n\t * On error, we have to fail the ioend here because we have locked\n\t * buffers in the ioend. If we don't do this, we'll deadlock\n\t * invalidating the page as that tries to lock the buffers on the page.\n\t * Also, because we may have set pages under writeback, we have to make\n\t * sure we run IO completion to mark the error state of the IO\n\t * appropriately, so we can't cancel the ioend directly here. That means\n\t * we have to mark this page as under writeback if we included any\n\t * buffers from it in the ioend chain so that completion treats it\n\t * correctly.\n\t *\n\t * If we didn't include the page in the ioend, the on error we can\n\t * simply discard and unlock it as there are no other users of the page\n\t * or it's buffers right now. The caller will still need to trigger\n\t * submission of outstanding ioends on the writepage context so they are\n\t * treated correctly on error.\n\t */\n\tif (count) {\n\t\txfs_start_page_writeback(page, !error);\n\n\t\t/*\n\t\t * Preserve the original error if there was one, otherwise catch\n\t\t * submission errors here and propagate into subsequent ioend\n\t\t * submissions.\n\t\t */\n\t\tlist_for_each_entry_safe(ioend, next, &submit_list, io_list) {\n\t\t\tint error2;\n\n\t\t\tlist_del_init(&ioend->io_list);\n\t\t\terror2 = xfs_submit_ioend(wbc, ioend, error);\n\t\t\tif (error2 && !error)\n\t\t\t\terror = error2;\n\t\t}\n\t} else if (error) {\n\t\txfs_aops_discard_page(page);\n\t\tClearPageUptodate(page);\n\t\tunlock_page(page);\n\t} else {\n\t\t/*\n\t\t * We can end up here with no error and nothing to write if we\n\t\t * race with a partial page truncate on a sub-page block sized\n\t\t * filesystem. In that case we need to mark the page clean.\n\t\t */\n\t\txfs_start_page_writeback(page, 1);\n\t\tend_page_writeback(page);\n\t}\n\n\tmapping_set_error(page->mapping, error);\n\treturn error;\n}\n\n/*\n * Write out a dirty page.\n *\n * For delalloc space on the page we need to allocate space and flush it.\n * For unwritten space on the page we need to start the conversion to\n * regular allocated space.\n * For any other dirty buffer heads on the page we should flush them.\n */\nSTATIC int\nxfs_do_writepage(\n\tstruct page\t\t*page,\n\tstruct writeback_control *wbc,\n\tvoid\t\t\t*data)\n{\n\tstruct xfs_writepage_ctx *wpc = data;\n\tstruct inode\t\t*inode = page->mapping->host;\n\tloff_t\t\t\toffset;\n\t__uint64_t              end_offset;\n\tpgoff_t                 end_index;\n\n\ttrace_xfs_writepage(inode, page, 0, 0);\n\n\tASSERT(page_has_buffers(page));\n\n\t/*\n\t * Refuse to write the page out if we are called from reclaim context.\n\t *\n\t * This avoids stack overflows when called from deeply used stacks in\n\t * random callers for direct reclaim or memcg reclaim.  We explicitly\n\t * allow reclaim from kswapd as the stack usage there is relatively low.\n\t *\n\t * This should never happen except in the case of a VM regression so\n\t * warn about it.\n\t */\n\tif (WARN_ON_ONCE((current->flags & (PF_MEMALLOC|PF_KSWAPD)) ==\n\t\t\tPF_MEMALLOC))\n\t\tgoto redirty;\n\n\t/*\n\t * Given that we do not allow direct reclaim to call us, we should\n\t * never be called while in a filesystem transaction.\n\t */\n\tif (WARN_ON_ONCE(current->flags & PF_FSTRANS))\n\t\tgoto redirty;\n\n\t/*\n\t * Is this page beyond the end of the file?\n\t *\n\t * The page index is less than the end_index, adjust the end_offset\n\t * to the highest offset that this page should represent.\n\t * -----------------------------------------------------\n\t * |\t\t\tfile mapping\t       | <EOF> |\n\t * -----------------------------------------------------\n\t * | Page ... | Page N-2 | Page N-1 |  Page N  |       |\n\t * ^--------------------------------^----------|--------\n\t * |     desired writeback range    |      see else    |\n\t * ---------------------------------^------------------|\n\t */\n\toffset = i_size_read(inode);\n\tend_index = offset >> PAGE_SHIFT;\n\tif (page->index < end_index)\n\t\tend_offset = (xfs_off_t)(page->index + 1) << PAGE_SHIFT;\n\telse {\n\t\t/*\n\t\t * Check whether the page to write out is beyond or straddles\n\t\t * i_size or not.\n\t\t * -------------------------------------------------------\n\t\t * |\t\tfile mapping\t\t        | <EOF>  |\n\t\t * -------------------------------------------------------\n\t\t * | Page ... | Page N-2 | Page N-1 |  Page N   | Beyond |\n\t\t * ^--------------------------------^-----------|---------\n\t\t * |\t\t\t\t    |      Straddles     |\n\t\t * ---------------------------------^-----------|--------|\n\t\t */\n\t\tunsigned offset_into_page = offset & (PAGE_SIZE - 1);\n\n\t\t/*\n\t\t * Skip the page if it is fully outside i_size, e.g. due to a\n\t\t * truncate operation that is in progress. We must redirty the\n\t\t * page so that reclaim stops reclaiming it. Otherwise\n\t\t * xfs_vm_releasepage() is called on it and gets confused.\n\t\t *\n\t\t * Note that the end_index is unsigned long, it would overflow\n\t\t * if the given offset is greater than 16TB on 32-bit system\n\t\t * and if we do check the page is fully outside i_size or not\n\t\t * via \"if (page->index >= end_index + 1)\" as \"end_index + 1\"\n\t\t * will be evaluated to 0.  Hence this page will be redirtied\n\t\t * and be written out repeatedly which would result in an\n\t\t * infinite loop, the user program that perform this operation\n\t\t * will hang.  Instead, we can verify this situation by checking\n\t\t * if the page to write is totally beyond the i_size or if it's\n\t\t * offset is just equal to the EOF.\n\t\t */\n\t\tif (page->index > end_index ||\n\t\t    (page->index == end_index && offset_into_page == 0))\n\t\t\tgoto redirty;\n\n\t\t/*\n\t\t * The page straddles i_size.  It must be zeroed out on each\n\t\t * and every writepage invocation because it may be mmapped.\n\t\t * \"A file is mapped in multiples of the page size.  For a file\n\t\t * that is not a multiple of the page size, the remaining\n\t\t * memory is zeroed when mapped, and writes to that region are\n\t\t * not written out to the file.\"\n\t\t */\n\t\tzero_user_segment(page, offset_into_page, PAGE_SIZE);\n\n\t\t/* Adjust the end_offset to the end of file */\n\t\tend_offset = offset;\n\t}\n\n\treturn xfs_writepage_map(wpc, wbc, inode, page, offset, end_offset);\n\nredirty:\n\tredirty_page_for_writepage(wbc, page);\n\tunlock_page(page);\n\treturn 0;\n}\n\nSTATIC int\nxfs_vm_writepage(\n\tstruct page\t\t*page,\n\tstruct writeback_control *wbc)\n{\n\tstruct xfs_writepage_ctx wpc = {\n\t\t.io_type = XFS_IO_INVALID,\n\t};\n\tint\t\t\tret;\n\n\tret = xfs_do_writepage(page, wbc, &wpc);\n\tif (wpc.ioend)\n\t\tret = xfs_submit_ioend(wbc, wpc.ioend, ret);\n\treturn ret;\n}\n\nSTATIC int\nxfs_vm_writepages(\n\tstruct address_space\t*mapping,\n\tstruct writeback_control *wbc)\n{\n\tstruct xfs_writepage_ctx wpc = {\n\t\t.io_type = XFS_IO_INVALID,\n\t};\n\tint\t\t\tret;\n\n\txfs_iflags_clear(XFS_I(mapping->host), XFS_ITRUNCATED);\n\tif (dax_mapping(mapping))\n\t\treturn dax_writeback_mapping_range(mapping,\n\t\t\t\txfs_find_bdev_for_inode(mapping->host), wbc);\n\n\tret = write_cache_pages(mapping, wbc, xfs_do_writepage, &wpc);\n\tif (wpc.ioend)\n\t\tret = xfs_submit_ioend(wbc, wpc.ioend, ret);\n\treturn ret;\n}\n\n/*\n * Called to move a page into cleanable state - and from there\n * to be released. The page should already be clean. We always\n * have buffer heads in this call.\n *\n * Returns 1 if the page is ok to release, 0 otherwise.\n */\nSTATIC int\nxfs_vm_releasepage(\n\tstruct page\t\t*page,\n\tgfp_t\t\t\tgfp_mask)\n{\n\tint\t\t\tdelalloc, unwritten;\n\n\ttrace_xfs_releasepage(page->mapping->host, page, 0, 0);\n\n\t/*\n\t * mm accommodates an old ext3 case where clean pages might not have had\n\t * the dirty bit cleared. Thus, it can send actual dirty pages to\n\t * ->releasepage() via shrink_active_list(). Conversely,\n\t * block_invalidatepage() can send pages that are still marked dirty\n\t * but otherwise have invalidated buffers.\n\t *\n\t * We've historically freed buffers on the latter. Instead, quietly\n\t * filter out all dirty pages to avoid spurious buffer state warnings.\n\t * This can likely be removed once shrink_active_list() is fixed.\n\t */\n\tif (PageDirty(page))\n\t\treturn 0;\n\n\txfs_count_page_state(page, &delalloc, &unwritten);\n\n\tif (WARN_ON_ONCE(delalloc))\n\t\treturn 0;\n\tif (WARN_ON_ONCE(unwritten))\n\t\treturn 0;\n\n\treturn try_to_free_buffers(page);\n}\n\n/*\n * When we map a DIO buffer, we may need to pass flags to\n * xfs_end_io_direct_write to tell it what kind of write IO we are doing.\n *\n * Note that for DIO, an IO to the highest supported file block offset (i.e.\n * 2^63 - 1FSB bytes) will result in the offset + count overflowing a signed 64\n * bit variable. Hence if we see this overflow, we have to assume that the IO is\n * extending the file size. We won't know for sure until IO completion is run\n * and the actual max write offset is communicated to the IO completion\n * routine.\n */\nstatic void\nxfs_map_direct(\n\tstruct inode\t\t*inode,\n\tstruct buffer_head\t*bh_result,\n\tstruct xfs_bmbt_irec\t*imap,\n\txfs_off_t\t\toffset,\n\tbool\t\t\tis_cow)\n{\n\tuintptr_t\t\t*flags = (uintptr_t *)&bh_result->b_private;\n\txfs_off_t\t\tsize = bh_result->b_size;\n\n\ttrace_xfs_get_blocks_map_direct(XFS_I(inode), offset, size,\n\t\tISUNWRITTEN(imap) ? XFS_IO_UNWRITTEN : is_cow ? XFS_IO_COW :\n\t\tXFS_IO_OVERWRITE, imap);\n\n\tif (ISUNWRITTEN(imap)) {\n\t\t*flags |= XFS_DIO_FLAG_UNWRITTEN;\n\t\tset_buffer_defer_completion(bh_result);\n\t} else if (is_cow) {\n\t\t*flags |= XFS_DIO_FLAG_COW;\n\t\tset_buffer_defer_completion(bh_result);\n\t}\n\tif (offset + size > i_size_read(inode) || offset + size < 0) {\n\t\t*flags |= XFS_DIO_FLAG_APPEND;\n\t\tset_buffer_defer_completion(bh_result);\n\t}\n}\n\n/*\n * If this is O_DIRECT or the mpage code calling tell them how large the mapping\n * is, so that we can avoid repeated get_blocks calls.\n *\n * If the mapping spans EOF, then we have to break the mapping up as the mapping\n * for blocks beyond EOF must be marked new so that sub block regions can be\n * correctly zeroed. We can't do this for mappings within EOF unless the mapping\n * was just allocated or is unwritten, otherwise the callers would overwrite\n * existing data with zeros. Hence we have to split the mapping into a range up\n * to and including EOF, and a second mapping for beyond EOF.\n */\nstatic void\nxfs_map_trim_size(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tstruct xfs_bmbt_irec\t*imap,\n\txfs_off_t\t\toffset,\n\tssize_t\t\t\tsize)\n{\n\txfs_off_t\t\tmapping_size;\n\n\tmapping_size = imap->br_startoff + imap->br_blockcount - iblock;\n\tmapping_size <<= inode->i_blkbits;\n\n\tASSERT(mapping_size > 0);\n\tif (mapping_size > size)\n\t\tmapping_size = size;\n\tif (offset < i_size_read(inode) &&\n\t    offset + mapping_size >= i_size_read(inode)) {\n\t\t/* limit mapping to block that spans EOF */\n\t\tmapping_size = roundup_64(i_size_read(inode) - offset,\n\t\t\t\t\t  1 << inode->i_blkbits);\n\t}\n\tif (mapping_size > LONG_MAX)\n\t\tmapping_size = LONG_MAX;\n\n\tbh_result->b_size = mapping_size;\n}\n\n/* Bounce unaligned directio writes to the page cache. */\nstatic int\nxfs_bounce_unaligned_dio_write(\n\tstruct xfs_inode\t*ip,\n\txfs_fileoff_t\t\toffset_fsb,\n\tstruct xfs_bmbt_irec\t*imap)\n{\n\tstruct xfs_bmbt_irec\tirec;\n\txfs_fileoff_t\t\tdelta;\n\tbool\t\t\tshared;\n\tbool\t\t\tx;\n\tint\t\t\terror;\n\n\tirec = *imap;\n\tif (offset_fsb > irec.br_startoff) {\n\t\tdelta = offset_fsb - irec.br_startoff;\n\t\tirec.br_blockcount -= delta;\n\t\tirec.br_startblock += delta;\n\t\tirec.br_startoff = offset_fsb;\n\t}\n\terror = xfs_reflink_trim_around_shared(ip, &irec, &shared, &x);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * We're here because we're trying to do a directio write to a\n\t * region that isn't aligned to a filesystem block.  If any part\n\t * of the extent is shared, fall back to buffered mode to handle\n\t * the RMW.  This is done by returning -EREMCHG (\"remote addr\n\t * changed\"), which is caught further up the call stack.\n\t */\n\tif (shared) {\n\t\ttrace_xfs_reflink_bounce_dio_write(ip, imap);\n\t\treturn -EREMCHG;\n\t}\n\treturn 0;\n}\n\nSTATIC int\n__xfs_get_blocks(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate,\n\tbool\t\t\tdirect,\n\tbool\t\t\tdax_fault)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\txfs_fileoff_t\t\toffset_fsb, end_fsb;\n\tint\t\t\terror = 0;\n\tint\t\t\tlockmode = 0;\n\tstruct xfs_bmbt_irec\timap;\n\tint\t\t\tnimaps = 1;\n\txfs_off_t\t\toffset;\n\tssize_t\t\t\tsize;\n\tint\t\t\tnew = 0;\n\tbool\t\t\tis_cow = false;\n\tbool\t\t\tneed_alloc = false;\n\n\tBUG_ON(create && !direct);\n\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn -EIO;\n\n\toffset = (xfs_off_t)iblock << inode->i_blkbits;\n\tASSERT(bh_result->b_size >= (1 << inode->i_blkbits));\n\tsize = bh_result->b_size;\n\n\tif (!create && offset >= i_size_read(inode))\n\t\treturn 0;\n\n\t/*\n\t * Direct I/O is usually done on preallocated files, so try getting\n\t * a block mapping without an exclusive lock first.\n\t */\n\tlockmode = xfs_ilock_data_map_shared(ip);\n\n\tASSERT(offset <= mp->m_super->s_maxbytes);\n\tif (offset + size > mp->m_super->s_maxbytes)\n\t\tsize = mp->m_super->s_maxbytes - offset;\n\tend_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);\n\toffset_fsb = XFS_B_TO_FSBT(mp, offset);\n\n\tif (create && direct && xfs_is_reflink_inode(ip))\n\t\tis_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,\n\t\t\t\t\t&need_alloc);\n\tif (!is_cow) {\n\t\terror = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,\n\t\t\t\t\t&imap, &nimaps, XFS_BMAPI_ENTIRE);\n\t\t/*\n\t\t * Truncate an overwrite extent if there's a pending CoW\n\t\t * reservation before the end of this extent.  This\n\t\t * forces us to come back to get_blocks to take care of\n\t\t * the CoW.\n\t\t */\n\t\tif (create && direct && nimaps &&\n\t\t    imap.br_startblock != HOLESTARTBLOCK &&\n\t\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t\t    !ISUNWRITTEN(&imap))\n\t\t\txfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t}\n\tASSERT(!need_alloc);\n\tif (error)\n\t\tgoto out_unlock;\n\n\t/* for DAX, we convert unwritten extents directly */\n\tif (create &&\n\t    (!nimaps ||\n\t     (imap.br_startblock == HOLESTARTBLOCK ||\n\t      imap.br_startblock == DELAYSTARTBLOCK) ||\n\t     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {\n\t\t/*\n\t\t * xfs_iomap_write_direct() expects the shared lock. It\n\t\t * is unlocked on return.\n\t\t */\n\t\tif (lockmode == XFS_ILOCK_EXCL)\n\t\t\txfs_ilock_demote(ip, lockmode);\n\n\t\terror = xfs_iomap_write_direct(ip, offset, size,\n\t\t\t\t\t       &imap, nimaps);\n\t\tif (error)\n\t\t\treturn error;\n\t\tnew = 1;\n\n\t\ttrace_xfs_get_blocks_alloc(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_DELALLOC, &imap);\n\t} else if (nimaps) {\n\t\ttrace_xfs_get_blocks_found(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_OVERWRITE, &imap);\n\t\txfs_iunlock(ip, lockmode);\n\t} else {\n\t\ttrace_xfs_get_blocks_notfound(ip, offset, size);\n\t\tgoto out_unlock;\n\t}\n\n\tif (IS_DAX(inode) && create) {\n\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t/* zeroing is not needed at a higher layer */\n\t\tnew = 0;\n\t}\n\n\t/* trim mapping down to size requested */\n\txfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);\n\n\t/*\n\t * For unwritten extents do not report a disk address in the buffered\n\t * read case (treat as if we're reading into a hole).\n\t */\n\tif (imap.br_startblock != HOLESTARTBLOCK &&\n\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t    (create || !ISUNWRITTEN(&imap))) {\n\t\tif (create && direct && !is_cow) {\n\t\t\terror = xfs_bounce_unaligned_dio_write(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\n\t\txfs_map_buffer(inode, bh_result, &imap, offset);\n\t\tif (ISUNWRITTEN(&imap))\n\t\t\tset_buffer_unwritten(bh_result);\n\t\t/* direct IO needs special help */\n\t\tif (create) {\n\t\t\tif (dax_fault)\n\t\t\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t\telse\n\t\t\t\txfs_map_direct(inode, bh_result, &imap, offset,\n\t\t\t\t\t\tis_cow);\n\t\t}\n\t}\n\n\t/*\n\t * If this is a realtime file, data may be on a different device.\n\t * to that pointed to from the buffer_head b_bdev currently.\n\t */\n\tbh_result->b_bdev = xfs_find_bdev_for_inode(inode);\n\n\t/*\n\t * If we previously allocated a block out beyond eof and we are now\n\t * coming back to use it then we will need to flag it as new even if it\n\t * has a disk address.\n\t *\n\t * With sub-block writes into unwritten extents we also need to mark\n\t * the buffer as new so that the unwritten parts of the buffer gets\n\t * correctly zeroed.\n\t */\n\tif (create &&\n\t    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||\n\t     (offset >= i_size_read(inode)) ||\n\t     (new || ISUNWRITTEN(&imap))))\n\t\tset_buffer_new(bh_result);\n\n\tBUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);\n\n\treturn 0;\n\nout_unlock:\n\txfs_iunlock(ip, lockmode);\n\treturn error;\n}\n\nint\nxfs_get_blocks(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate)\n{\n\treturn __xfs_get_blocks(inode, iblock, bh_result, create, false, false);\n}\n\nint\nxfs_get_blocks_direct(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate)\n{\n\treturn __xfs_get_blocks(inode, iblock, bh_result, create, true, false);\n}\n\nint\nxfs_get_blocks_dax_fault(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate)\n{\n\treturn __xfs_get_blocks(inode, iblock, bh_result, create, true, true);\n}\n\n/*\n * Complete a direct I/O write request.\n *\n * xfs_map_direct passes us some flags in the private data to tell us what to\n * do.  If no flags are set, then the write IO is an overwrite wholly within\n * the existing allocated file size and so there is nothing for us to do.\n *\n * Note that in this case the completion can be called in interrupt context,\n * whereas if we have flags set we will always be called in task context\n * (i.e. from a workqueue).\n */\nint\nxfs_end_io_direct_write(\n\tstruct kiocb\t\t*iocb,\n\tloff_t\t\t\toffset,\n\tssize_t\t\t\tsize,\n\tvoid\t\t\t*private)\n{\n\tstruct inode\t\t*inode = file_inode(iocb->ki_filp);\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tuintptr_t\t\tflags = (uintptr_t)private;\n\tint\t\t\terror = 0;\n\n\ttrace_xfs_end_io_direct_write(ip, offset, size);\n\n\tif (XFS_FORCED_SHUTDOWN(ip->i_mount))\n\t\treturn -EIO;\n\n\tif (size <= 0)\n\t\treturn size;\n\n\t/*\n\t * The flags tell us whether we are doing unwritten extent conversions\n\t * or an append transaction that updates the on-disk file size. These\n\t * cases are the only cases where we should *potentially* be needing\n\t * to update the VFS inode size.\n\t */\n\tif (flags == 0) {\n\t\tASSERT(offset + size <= i_size_read(inode));\n\t\treturn 0;\n\t}\n\n\t/*\n\t * We need to update the in-core inode size here so that we don't end up\n\t * with the on-disk inode size being outside the in-core inode size. We\n\t * have no other method of updating EOF for AIO, so always do it here\n\t * if necessary.\n\t *\n\t * We need to lock the test/set EOF update as we can be racing with\n\t * other IO completions here to update the EOF. Failing to serialise\n\t * here can result in EOF moving backwards and Bad Things Happen when\n\t * that occurs.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (offset + size > i_size_read(inode))\n\t\ti_size_write(inode, offset + size);\n\tspin_unlock(&ip->i_flags_lock);\n\n\tif (flags & XFS_DIO_FLAG_COW)\n\t\terror = xfs_reflink_end_cow(ip, offset, size);\n\tif (flags & XFS_DIO_FLAG_UNWRITTEN) {\n\t\ttrace_xfs_end_io_direct_write_unwritten(ip, offset, size);\n\n\t\terror = xfs_iomap_write_unwritten(ip, offset, size);\n\t}\n\tif (flags & XFS_DIO_FLAG_APPEND) {\n\t\ttrace_xfs_end_io_direct_write_append(ip, offset, size);\n\n\t\terror = xfs_setfilesize(ip, offset, size);\n\t}\n\n\treturn error;\n}\n\nSTATIC ssize_t\nxfs_vm_direct_IO(\n\tstruct kiocb\t\t*iocb,\n\tstruct iov_iter\t\t*iter)\n{\n\t/*\n\t * We just need the method present so that open/fcntl allow direct I/O.\n\t */\n\treturn -EINVAL;\n}\n\nSTATIC sector_t\nxfs_vm_bmap(\n\tstruct address_space\t*mapping,\n\tsector_t\t\tblock)\n{\n\tstruct inode\t\t*inode = (struct inode *)mapping->host;\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\n\ttrace_xfs_vm_bmap(XFS_I(inode));\n\txfs_ilock(ip, XFS_IOLOCK_SHARED);\n\n\t/*\n\t * The swap code (ab-)uses ->bmap to get a block mapping and then\n\t * bypasse\u0455 the file system for actual I/O.  We really can't allow\n\t * that on reflinks inodes, so we have to skip out here.  And yes,\n\t * 0 is the magic code for a bmap error..\n\t */\n\tif (xfs_is_reflink_inode(ip)) {\n\t\txfs_iunlock(ip, XFS_IOLOCK_SHARED);\n\t\treturn 0;\n\t}\n\tfilemap_write_and_wait(mapping);\n\txfs_iunlock(ip, XFS_IOLOCK_SHARED);\n\treturn generic_block_bmap(mapping, block, xfs_get_blocks);\n}\n\nSTATIC int\nxfs_vm_readpage(\n\tstruct file\t\t*unused,\n\tstruct page\t\t*page)\n{\n\ttrace_xfs_vm_readpage(page->mapping->host, 1);\n\treturn mpage_readpage(page, xfs_get_blocks);\n}\n\nSTATIC int\nxfs_vm_readpages(\n\tstruct file\t\t*unused,\n\tstruct address_space\t*mapping,\n\tstruct list_head\t*pages,\n\tunsigned\t\tnr_pages)\n{\n\ttrace_xfs_vm_readpages(mapping->host, nr_pages);\n\treturn mpage_readpages(mapping, pages, nr_pages, xfs_get_blocks);\n}\n\n/*\n * This is basically a copy of __set_page_dirty_buffers() with one\n * small tweak: buffers beyond EOF do not get marked dirty. If we mark them\n * dirty, we'll never be able to clean them because we don't write buffers\n * beyond EOF, and that means we can't invalidate pages that span EOF\n * that have been marked dirty. Further, the dirty state can leak into\n * the file interior if the file is extended, resulting in all sorts of\n * bad things happening as the state does not match the underlying data.\n *\n * XXX: this really indicates that bufferheads in XFS need to die. Warts like\n * this only exist because of bufferheads and how the generic code manages them.\n */\nSTATIC int\nxfs_vm_set_page_dirty(\n\tstruct page\t\t*page)\n{\n\tstruct address_space\t*mapping = page->mapping;\n\tstruct inode\t\t*inode = mapping->host;\n\tloff_t\t\t\tend_offset;\n\tloff_t\t\t\toffset;\n\tint\t\t\tnewly_dirty;\n\n\tif (unlikely(!mapping))\n\t\treturn !TestSetPageDirty(page);\n\n\tend_offset = i_size_read(inode);\n\toffset = page_offset(page);\n\n\tspin_lock(&mapping->private_lock);\n\tif (page_has_buffers(page)) {\n\t\tstruct buffer_head *head = page_buffers(page);\n\t\tstruct buffer_head *bh = head;\n\n\t\tdo {\n\t\t\tif (offset < end_offset)\n\t\t\t\tset_buffer_dirty(bh);\n\t\t\tbh = bh->b_this_page;\n\t\t\toffset += 1 << inode->i_blkbits;\n\t\t} while (bh != head);\n\t}\n\t/*\n\t * Lock out page->mem_cgroup migration to keep PageDirty\n\t * synchronized with per-memcg dirty page counters.\n\t */\n\tlock_page_memcg(page);\n\tnewly_dirty = !TestSetPageDirty(page);\n\tspin_unlock(&mapping->private_lock);\n\n\tif (newly_dirty) {\n\t\t/* sigh - __set_page_dirty() is static, so copy it here, too */\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&mapping->tree_lock, flags);\n\t\tif (page->mapping) {\t/* Race with truncate? */\n\t\t\tWARN_ON_ONCE(!PageUptodate(page));\n\t\t\taccount_page_dirtied(page, mapping);\n\t\t\tradix_tree_tag_set(&mapping->page_tree,\n\t\t\t\t\tpage_index(page), PAGECACHE_TAG_DIRTY);\n\t\t}\n\t\tspin_unlock_irqrestore(&mapping->tree_lock, flags);\n\t}\n\tunlock_page_memcg(page);\n\tif (newly_dirty)\n\t\t__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);\n\treturn newly_dirty;\n}\n\nconst struct address_space_operations xfs_address_space_operations = {\n\t.readpage\t\t= xfs_vm_readpage,\n\t.readpages\t\t= xfs_vm_readpages,\n\t.writepage\t\t= xfs_vm_writepage,\n\t.writepages\t\t= xfs_vm_writepages,\n\t.set_page_dirty\t\t= xfs_vm_set_page_dirty,\n\t.releasepage\t\t= xfs_vm_releasepage,\n\t.invalidatepage\t\t= xfs_vm_invalidatepage,\n\t.bmap\t\t\t= xfs_vm_bmap,\n\t.direct_IO\t\t= xfs_vm_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n"], "fixing_code": ["/*\n * Copyright (c) 2000-2005 Silicon Graphics, Inc.\n * All Rights Reserved.\n *\n * This program is free software; you can redistribute it and/or\n * modify it under the terms of the GNU General Public License as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it would be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program; if not, write the Free Software Foundation,\n * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n */\n#include \"xfs.h\"\n#include \"xfs_shared.h\"\n#include \"xfs_format.h\"\n#include \"xfs_log_format.h\"\n#include \"xfs_trans_resv.h\"\n#include \"xfs_mount.h\"\n#include \"xfs_inode.h\"\n#include \"xfs_trans.h\"\n#include \"xfs_inode_item.h\"\n#include \"xfs_alloc.h\"\n#include \"xfs_error.h\"\n#include \"xfs_iomap.h\"\n#include \"xfs_trace.h\"\n#include \"xfs_bmap.h\"\n#include \"xfs_bmap_util.h\"\n#include \"xfs_bmap_btree.h\"\n#include \"xfs_reflink.h\"\n#include <linux/gfp.h>\n#include <linux/mpage.h>\n#include <linux/pagevec.h>\n#include <linux/writeback.h>\n\n/* flags for direct write completions */\n#define XFS_DIO_FLAG_UNWRITTEN\t(1 << 0)\n#define XFS_DIO_FLAG_APPEND\t(1 << 1)\n#define XFS_DIO_FLAG_COW\t(1 << 2)\n\n/*\n * structure owned by writepages passed to individual writepage calls\n */\nstruct xfs_writepage_ctx {\n\tstruct xfs_bmbt_irec    imap;\n\tbool\t\t\timap_valid;\n\tunsigned int\t\tio_type;\n\tstruct xfs_ioend\t*ioend;\n\tsector_t\t\tlast_block;\n};\n\nvoid\nxfs_count_page_state(\n\tstruct page\t\t*page,\n\tint\t\t\t*delalloc,\n\tint\t\t\t*unwritten)\n{\n\tstruct buffer_head\t*bh, *head;\n\n\t*delalloc = *unwritten = 0;\n\n\tbh = head = page_buffers(page);\n\tdo {\n\t\tif (buffer_unwritten(bh))\n\t\t\t(*unwritten) = 1;\n\t\telse if (buffer_delay(bh))\n\t\t\t(*delalloc) = 1;\n\t} while ((bh = bh->b_this_page) != head);\n}\n\nstruct block_device *\nxfs_find_bdev_for_inode(\n\tstruct inode\t\t*inode)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\n\tif (XFS_IS_REALTIME_INODE(ip))\n\t\treturn mp->m_rtdev_targp->bt_bdev;\n\telse\n\t\treturn mp->m_ddev_targp->bt_bdev;\n}\n\n/*\n * We're now finished for good with this page.  Update the page state via the\n * associated buffer_heads, paying attention to the start and end offsets that\n * we need to process on the page.\n *\n * Landmine Warning: bh->b_end_io() will call end_page_writeback() on the last\n * buffer in the IO. Once it does this, it is unsafe to access the bufferhead or\n * the page at all, as we may be racing with memory reclaim and it can free both\n * the bufferhead chain and the page as it will see the page as clean and\n * unused.\n */\nstatic void\nxfs_finish_page_writeback(\n\tstruct inode\t\t*inode,\n\tstruct bio_vec\t\t*bvec,\n\tint\t\t\terror)\n{\n\tunsigned int\t\tend = bvec->bv_offset + bvec->bv_len - 1;\n\tstruct buffer_head\t*head, *bh, *next;\n\tunsigned int\t\toff = 0;\n\tunsigned int\t\tbsize;\n\n\tASSERT(bvec->bv_offset < PAGE_SIZE);\n\tASSERT((bvec->bv_offset & ((1 << inode->i_blkbits) - 1)) == 0);\n\tASSERT(end < PAGE_SIZE);\n\tASSERT((bvec->bv_len & ((1 << inode->i_blkbits) - 1)) == 0);\n\n\tbh = head = page_buffers(bvec->bv_page);\n\n\tbsize = bh->b_size;\n\tdo {\n\t\tnext = bh->b_this_page;\n\t\tif (off < bvec->bv_offset)\n\t\t\tgoto next_bh;\n\t\tif (off > end)\n\t\t\tbreak;\n\t\tbh->b_end_io(bh, !error);\nnext_bh:\n\t\toff += bsize;\n\t} while ((bh = next) != head);\n}\n\n/*\n * We're now finished for good with this ioend structure.  Update the page\n * state, release holds on bios, and finally free up memory.  Do not use the\n * ioend after this.\n */\nSTATIC void\nxfs_destroy_ioend(\n\tstruct xfs_ioend\t*ioend,\n\tint\t\t\terror)\n{\n\tstruct inode\t\t*inode = ioend->io_inode;\n\tstruct bio\t\t*last = ioend->io_bio;\n\tstruct bio\t\t*bio, *next;\n\n\tfor (bio = &ioend->io_inline_bio; bio; bio = next) {\n\t\tstruct bio_vec\t*bvec;\n\t\tint\t\ti;\n\n\t\t/*\n\t\t * For the last bio, bi_private points to the ioend, so we\n\t\t * need to explicitly end the iteration here.\n\t\t */\n\t\tif (bio == last)\n\t\t\tnext = NULL;\n\t\telse\n\t\t\tnext = bio->bi_private;\n\n\t\t/* walk each page on bio, ending page IO on them */\n\t\tbio_for_each_segment_all(bvec, bio, i)\n\t\t\txfs_finish_page_writeback(inode, bvec, error);\n\n\t\tbio_put(bio);\n\t}\n}\n\n/*\n * Fast and loose check if this write could update the on-disk inode size.\n */\nstatic inline bool xfs_ioend_is_append(struct xfs_ioend *ioend)\n{\n\treturn ioend->io_offset + ioend->io_size >\n\t\tXFS_I(ioend->io_inode)->i_d.di_size;\n}\n\nSTATIC int\nxfs_setfilesize_trans_alloc(\n\tstruct xfs_ioend\t*ioend)\n{\n\tstruct xfs_mount\t*mp = XFS_I(ioend->io_inode)->i_mount;\n\tstruct xfs_trans\t*tp;\n\tint\t\t\terror;\n\n\terror = xfs_trans_alloc(mp, &M_RES(mp)->tr_fsyncts, 0, 0, 0, &tp);\n\tif (error)\n\t\treturn error;\n\n\tioend->io_append_trans = tp;\n\n\t/*\n\t * We may pass freeze protection with a transaction.  So tell lockdep\n\t * we released it.\n\t */\n\t__sb_writers_release(ioend->io_inode->i_sb, SB_FREEZE_FS);\n\t/*\n\t * We hand off the transaction to the completion thread now, so\n\t * clear the flag here.\n\t */\n\tcurrent_restore_flags_nested(&tp->t_pflags, PF_FSTRANS);\n\treturn 0;\n}\n\n/*\n * Update on-disk file size now that data has been written to disk.\n */\nSTATIC int\n__xfs_setfilesize(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_trans\t*tp,\n\txfs_off_t\t\toffset,\n\tsize_t\t\t\tsize)\n{\n\txfs_fsize_t\t\tisize;\n\n\txfs_ilock(ip, XFS_ILOCK_EXCL);\n\tisize = xfs_new_eof(ip, offset + size);\n\tif (!isize) {\n\t\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\t\txfs_trans_cancel(tp);\n\t\treturn 0;\n\t}\n\n\ttrace_xfs_setfilesize(ip, offset, size);\n\n\tip->i_d.di_size = isize;\n\txfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL);\n\txfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);\n\n\treturn xfs_trans_commit(tp);\n}\n\nint\nxfs_setfilesize(\n\tstruct xfs_inode\t*ip,\n\txfs_off_t\t\toffset,\n\tsize_t\t\t\tsize)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_trans\t*tp;\n\tint\t\t\terror;\n\n\terror = xfs_trans_alloc(mp, &M_RES(mp)->tr_fsyncts, 0, 0, 0, &tp);\n\tif (error)\n\t\treturn error;\n\n\treturn __xfs_setfilesize(ip, tp, offset, size);\n}\n\nSTATIC int\nxfs_setfilesize_ioend(\n\tstruct xfs_ioend\t*ioend,\n\tint\t\t\terror)\n{\n\tstruct xfs_inode\t*ip = XFS_I(ioend->io_inode);\n\tstruct xfs_trans\t*tp = ioend->io_append_trans;\n\n\t/*\n\t * The transaction may have been allocated in the I/O submission thread,\n\t * thus we need to mark ourselves as being in a transaction manually.\n\t * Similarly for freeze protection.\n\t */\n\tcurrent_set_flags_nested(&tp->t_pflags, PF_FSTRANS);\n\t__sb_writers_acquired(VFS_I(ip)->i_sb, SB_FREEZE_FS);\n\n\t/* we abort the update if there was an IO error */\n\tif (error) {\n\t\txfs_trans_cancel(tp);\n\t\treturn error;\n\t}\n\n\treturn __xfs_setfilesize(ip, tp, ioend->io_offset, ioend->io_size);\n}\n\n/*\n * IO write completion.\n */\nSTATIC void\nxfs_end_io(\n\tstruct work_struct *work)\n{\n\tstruct xfs_ioend\t*ioend =\n\t\tcontainer_of(work, struct xfs_ioend, io_work);\n\tstruct xfs_inode\t*ip = XFS_I(ioend->io_inode);\n\tint\t\t\terror = ioend->io_bio->bi_error;\n\n\t/*\n\t * Set an error if the mount has shut down and proceed with end I/O\n\t * processing so it can perform whatever cleanups are necessary.\n\t */\n\tif (XFS_FORCED_SHUTDOWN(ip->i_mount))\n\t\terror = -EIO;\n\n\t/*\n\t * For a CoW extent, we need to move the mapping from the CoW fork\n\t * to the data fork.  If instead an error happened, just dump the\n\t * new blocks.\n\t */\n\tif (ioend->io_type == XFS_IO_COW) {\n\t\tif (error)\n\t\t\tgoto done;\n\t\tif (ioend->io_bio->bi_error) {\n\t\t\terror = xfs_reflink_cancel_cow_range(ip,\n\t\t\t\t\tioend->io_offset, ioend->io_size);\n\t\t\tgoto done;\n\t\t}\n\t\terror = xfs_reflink_end_cow(ip, ioend->io_offset,\n\t\t\t\tioend->io_size);\n\t\tif (error)\n\t\t\tgoto done;\n\t}\n\n\t/*\n\t * For unwritten extents we need to issue transactions to convert a\n\t * range to normal written extens after the data I/O has finished.\n\t * Detecting and handling completion IO errors is done individually\n\t * for each case as different cleanup operations need to be performed\n\t * on error.\n\t */\n\tif (ioend->io_type == XFS_IO_UNWRITTEN) {\n\t\tif (error)\n\t\t\tgoto done;\n\t\terror = xfs_iomap_write_unwritten(ip, ioend->io_offset,\n\t\t\t\t\t\t  ioend->io_size);\n\t} else if (ioend->io_append_trans) {\n\t\terror = xfs_setfilesize_ioend(ioend, error);\n\t} else {\n\t\tASSERT(!xfs_ioend_is_append(ioend) ||\n\t\t       ioend->io_type == XFS_IO_COW);\n\t}\n\ndone:\n\txfs_destroy_ioend(ioend, error);\n}\n\nSTATIC void\nxfs_end_bio(\n\tstruct bio\t\t*bio)\n{\n\tstruct xfs_ioend\t*ioend = bio->bi_private;\n\tstruct xfs_mount\t*mp = XFS_I(ioend->io_inode)->i_mount;\n\n\tif (ioend->io_type == XFS_IO_UNWRITTEN || ioend->io_type == XFS_IO_COW)\n\t\tqueue_work(mp->m_unwritten_workqueue, &ioend->io_work);\n\telse if (ioend->io_append_trans)\n\t\tqueue_work(mp->m_data_workqueue, &ioend->io_work);\n\telse\n\t\txfs_destroy_ioend(ioend, bio->bi_error);\n}\n\nSTATIC int\nxfs_map_blocks(\n\tstruct inode\t\t*inode,\n\tloff_t\t\t\toffset,\n\tstruct xfs_bmbt_irec\t*imap,\n\tint\t\t\ttype)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tssize_t\t\t\tcount = 1 << inode->i_blkbits;\n\txfs_fileoff_t\t\toffset_fsb, end_fsb;\n\tint\t\t\terror = 0;\n\tint\t\t\tbmapi_flags = XFS_BMAPI_ENTIRE;\n\tint\t\t\tnimaps = 1;\n\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn -EIO;\n\n\tASSERT(type != XFS_IO_COW);\n\tif (type == XFS_IO_UNWRITTEN)\n\t\tbmapi_flags |= XFS_BMAPI_IGSTATE;\n\n\txfs_ilock(ip, XFS_ILOCK_SHARED);\n\tASSERT(ip->i_d.di_format != XFS_DINODE_FMT_BTREE ||\n\t       (ip->i_df.if_flags & XFS_IFEXTENTS));\n\tASSERT(offset <= mp->m_super->s_maxbytes);\n\n\tif (offset + count > mp->m_super->s_maxbytes)\n\t\tcount = mp->m_super->s_maxbytes - offset;\n\tend_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + count);\n\toffset_fsb = XFS_B_TO_FSBT(mp, offset);\n\terror = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,\n\t\t\t\timap, &nimaps, bmapi_flags);\n\t/*\n\t * Truncate an overwrite extent if there's a pending CoW\n\t * reservation before the end of this extent.  This forces us\n\t * to come back to writepage to take care of the CoW.\n\t */\n\tif (nimaps && type == XFS_IO_OVERWRITE)\n\t\txfs_reflink_trim_irec_to_next_cow(ip, offset_fsb, imap);\n\txfs_iunlock(ip, XFS_ILOCK_SHARED);\n\n\tif (error)\n\t\treturn error;\n\n\tif (type == XFS_IO_DELALLOC &&\n\t    (!nimaps || isnullstartblock(imap->br_startblock))) {\n\t\terror = xfs_iomap_write_allocate(ip, XFS_DATA_FORK, offset,\n\t\t\t\timap);\n\t\tif (!error)\n\t\t\ttrace_xfs_map_blocks_alloc(ip, offset, count, type, imap);\n\t\treturn error;\n\t}\n\n#ifdef DEBUG\n\tif (type == XFS_IO_UNWRITTEN) {\n\t\tASSERT(nimaps);\n\t\tASSERT(imap->br_startblock != HOLESTARTBLOCK);\n\t\tASSERT(imap->br_startblock != DELAYSTARTBLOCK);\n\t}\n#endif\n\tif (nimaps)\n\t\ttrace_xfs_map_blocks_found(ip, offset, count, type, imap);\n\treturn 0;\n}\n\nSTATIC bool\nxfs_imap_valid(\n\tstruct inode\t\t*inode,\n\tstruct xfs_bmbt_irec\t*imap,\n\txfs_off_t\t\toffset)\n{\n\toffset >>= inode->i_blkbits;\n\n\treturn offset >= imap->br_startoff &&\n\t\toffset < imap->br_startoff + imap->br_blockcount;\n}\n\nSTATIC void\nxfs_start_buffer_writeback(\n\tstruct buffer_head\t*bh)\n{\n\tASSERT(buffer_mapped(bh));\n\tASSERT(buffer_locked(bh));\n\tASSERT(!buffer_delay(bh));\n\tASSERT(!buffer_unwritten(bh));\n\n\tmark_buffer_async_write(bh);\n\tset_buffer_uptodate(bh);\n\tclear_buffer_dirty(bh);\n}\n\nSTATIC void\nxfs_start_page_writeback(\n\tstruct page\t\t*page,\n\tint\t\t\tclear_dirty)\n{\n\tASSERT(PageLocked(page));\n\tASSERT(!PageWriteback(page));\n\n\t/*\n\t * if the page was not fully cleaned, we need to ensure that the higher\n\t * layers come back to it correctly. That means we need to keep the page\n\t * dirty, and for WB_SYNC_ALL writeback we need to ensure the\n\t * PAGECACHE_TAG_TOWRITE index mark is not removed so another attempt to\n\t * write this page in this writeback sweep will be made.\n\t */\n\tif (clear_dirty) {\n\t\tclear_page_dirty_for_io(page);\n\t\tset_page_writeback(page);\n\t} else\n\t\tset_page_writeback_keepwrite(page);\n\n\tunlock_page(page);\n}\n\nstatic inline int xfs_bio_add_buffer(struct bio *bio, struct buffer_head *bh)\n{\n\treturn bio_add_page(bio, bh->b_page, bh->b_size, bh_offset(bh));\n}\n\n/*\n * Submit the bio for an ioend. We are passed an ioend with a bio attached to\n * it, and we submit that bio. The ioend may be used for multiple bio\n * submissions, so we only want to allocate an append transaction for the ioend\n * once. In the case of multiple bio submission, each bio will take an IO\n * reference to the ioend to ensure that the ioend completion is only done once\n * all bios have been submitted and the ioend is really done.\n *\n * If @fail is non-zero, it means that we have a situation where some part of\n * the submission process has failed after we have marked paged for writeback\n * and unlocked them. In this situation, we need to fail the bio and ioend\n * rather than submit it to IO. This typically only happens on a filesystem\n * shutdown.\n */\nSTATIC int\nxfs_submit_ioend(\n\tstruct writeback_control *wbc,\n\tstruct xfs_ioend\t*ioend,\n\tint\t\t\tstatus)\n{\n\t/* Reserve log space if we might write beyond the on-disk inode size. */\n\tif (!status &&\n\t    ioend->io_type != XFS_IO_UNWRITTEN &&\n\t    xfs_ioend_is_append(ioend) &&\n\t    !ioend->io_append_trans)\n\t\tstatus = xfs_setfilesize_trans_alloc(ioend);\n\n\tioend->io_bio->bi_private = ioend;\n\tioend->io_bio->bi_end_io = xfs_end_bio;\n\tbio_set_op_attrs(ioend->io_bio, REQ_OP_WRITE,\n\t\t\t (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : 0);\n\t/*\n\t * If we are failing the IO now, just mark the ioend with an\n\t * error and finish it. This will run IO completion immediately\n\t * as there is only one reference to the ioend at this point in\n\t * time.\n\t */\n\tif (status) {\n\t\tioend->io_bio->bi_error = status;\n\t\tbio_endio(ioend->io_bio);\n\t\treturn status;\n\t}\n\n\tsubmit_bio(ioend->io_bio);\n\treturn 0;\n}\n\nstatic void\nxfs_init_bio_from_bh(\n\tstruct bio\t\t*bio,\n\tstruct buffer_head\t*bh)\n{\n\tbio->bi_iter.bi_sector = bh->b_blocknr * (bh->b_size >> 9);\n\tbio->bi_bdev = bh->b_bdev;\n}\n\nstatic struct xfs_ioend *\nxfs_alloc_ioend(\n\tstruct inode\t\t*inode,\n\tunsigned int\t\ttype,\n\txfs_off_t\t\toffset,\n\tstruct buffer_head\t*bh)\n{\n\tstruct xfs_ioend\t*ioend;\n\tstruct bio\t\t*bio;\n\n\tbio = bio_alloc_bioset(GFP_NOFS, BIO_MAX_PAGES, xfs_ioend_bioset);\n\txfs_init_bio_from_bh(bio, bh);\n\n\tioend = container_of(bio, struct xfs_ioend, io_inline_bio);\n\tINIT_LIST_HEAD(&ioend->io_list);\n\tioend->io_type = type;\n\tioend->io_inode = inode;\n\tioend->io_size = 0;\n\tioend->io_offset = offset;\n\tINIT_WORK(&ioend->io_work, xfs_end_io);\n\tioend->io_append_trans = NULL;\n\tioend->io_bio = bio;\n\treturn ioend;\n}\n\n/*\n * Allocate a new bio, and chain the old bio to the new one.\n *\n * Note that we have to do perform the chaining in this unintuitive order\n * so that the bi_private linkage is set up in the right direction for the\n * traversal in xfs_destroy_ioend().\n */\nstatic void\nxfs_chain_bio(\n\tstruct xfs_ioend\t*ioend,\n\tstruct writeback_control *wbc,\n\tstruct buffer_head\t*bh)\n{\n\tstruct bio *new;\n\n\tnew = bio_alloc(GFP_NOFS, BIO_MAX_PAGES);\n\txfs_init_bio_from_bh(new, bh);\n\n\tbio_chain(ioend->io_bio, new);\n\tbio_get(ioend->io_bio);\t\t/* for xfs_destroy_ioend */\n\tbio_set_op_attrs(ioend->io_bio, REQ_OP_WRITE,\n\t\t\t  (wbc->sync_mode == WB_SYNC_ALL) ? WRITE_SYNC : 0);\n\tsubmit_bio(ioend->io_bio);\n\tioend->io_bio = new;\n}\n\n/*\n * Test to see if we've been building up a completion structure for\n * earlier buffers -- if so, we try to append to this ioend if we\n * can, otherwise we finish off any current ioend and start another.\n * Return the ioend we finished off so that the caller can submit it\n * once it has finished processing the dirty page.\n */\nSTATIC void\nxfs_add_to_ioend(\n\tstruct inode\t\t*inode,\n\tstruct buffer_head\t*bh,\n\txfs_off_t\t\toffset,\n\tstruct xfs_writepage_ctx *wpc,\n\tstruct writeback_control *wbc,\n\tstruct list_head\t*iolist)\n{\n\tif (!wpc->ioend || wpc->io_type != wpc->ioend->io_type ||\n\t    bh->b_blocknr != wpc->last_block + 1 ||\n\t    offset != wpc->ioend->io_offset + wpc->ioend->io_size) {\n\t\tif (wpc->ioend)\n\t\t\tlist_add(&wpc->ioend->io_list, iolist);\n\t\twpc->ioend = xfs_alloc_ioend(inode, wpc->io_type, offset, bh);\n\t}\n\n\t/*\n\t * If the buffer doesn't fit into the bio we need to allocate a new\n\t * one.  This shouldn't happen more than once for a given buffer.\n\t */\n\twhile (xfs_bio_add_buffer(wpc->ioend->io_bio, bh) != bh->b_size)\n\t\txfs_chain_bio(wpc->ioend, wbc, bh);\n\n\twpc->ioend->io_size += bh->b_size;\n\twpc->last_block = bh->b_blocknr;\n\txfs_start_buffer_writeback(bh);\n}\n\nSTATIC void\nxfs_map_buffer(\n\tstruct inode\t\t*inode,\n\tstruct buffer_head\t*bh,\n\tstruct xfs_bmbt_irec\t*imap,\n\txfs_off_t\t\toffset)\n{\n\tsector_t\t\tbn;\n\tstruct xfs_mount\t*m = XFS_I(inode)->i_mount;\n\txfs_off_t\t\tiomap_offset = XFS_FSB_TO_B(m, imap->br_startoff);\n\txfs_daddr_t\t\tiomap_bn = xfs_fsb_to_db(XFS_I(inode), imap->br_startblock);\n\n\tASSERT(imap->br_startblock != HOLESTARTBLOCK);\n\tASSERT(imap->br_startblock != DELAYSTARTBLOCK);\n\n\tbn = (iomap_bn >> (inode->i_blkbits - BBSHIFT)) +\n\t      ((offset - iomap_offset) >> inode->i_blkbits);\n\n\tASSERT(bn || XFS_IS_REALTIME_INODE(XFS_I(inode)));\n\n\tbh->b_blocknr = bn;\n\tset_buffer_mapped(bh);\n}\n\nSTATIC void\nxfs_map_at_offset(\n\tstruct inode\t\t*inode,\n\tstruct buffer_head\t*bh,\n\tstruct xfs_bmbt_irec\t*imap,\n\txfs_off_t\t\toffset)\n{\n\tASSERT(imap->br_startblock != HOLESTARTBLOCK);\n\tASSERT(imap->br_startblock != DELAYSTARTBLOCK);\n\n\txfs_map_buffer(inode, bh, imap, offset);\n\tset_buffer_mapped(bh);\n\tclear_buffer_delay(bh);\n\tclear_buffer_unwritten(bh);\n}\n\n/*\n * Test if a given page contains at least one buffer of a given @type.\n * If @check_all_buffers is true, then we walk all the buffers in the page to\n * try to find one of the type passed in. If it is not set, then the caller only\n * needs to check the first buffer on the page for a match.\n */\nSTATIC bool\nxfs_check_page_type(\n\tstruct page\t\t*page,\n\tunsigned int\t\ttype,\n\tbool\t\t\tcheck_all_buffers)\n{\n\tstruct buffer_head\t*bh;\n\tstruct buffer_head\t*head;\n\n\tif (PageWriteback(page))\n\t\treturn false;\n\tif (!page->mapping)\n\t\treturn false;\n\tif (!page_has_buffers(page))\n\t\treturn false;\n\n\tbh = head = page_buffers(page);\n\tdo {\n\t\tif (buffer_unwritten(bh)) {\n\t\t\tif (type == XFS_IO_UNWRITTEN)\n\t\t\t\treturn true;\n\t\t} else if (buffer_delay(bh)) {\n\t\t\tif (type == XFS_IO_DELALLOC)\n\t\t\t\treturn true;\n\t\t} else if (buffer_dirty(bh) && buffer_mapped(bh)) {\n\t\t\tif (type == XFS_IO_OVERWRITE)\n\t\t\t\treturn true;\n\t\t}\n\n\t\t/* If we are only checking the first buffer, we are done now. */\n\t\tif (!check_all_buffers)\n\t\t\tbreak;\n\t} while ((bh = bh->b_this_page) != head);\n\n\treturn false;\n}\n\nSTATIC void\nxfs_vm_invalidatepage(\n\tstruct page\t\t*page,\n\tunsigned int\t\toffset,\n\tunsigned int\t\tlength)\n{\n\ttrace_xfs_invalidatepage(page->mapping->host, page, offset,\n\t\t\t\t length);\n\tblock_invalidatepage(page, offset, length);\n}\n\n/*\n * If the page has delalloc buffers on it, we need to punch them out before we\n * invalidate the page. If we don't, we leave a stale delalloc mapping on the\n * inode that can trip a BUG() in xfs_get_blocks() later on if a direct IO read\n * is done on that same region - the delalloc extent is returned when none is\n * supposed to be there.\n *\n * We prevent this by truncating away the delalloc regions on the page before\n * invalidating it. Because they are delalloc, we can do this without needing a\n * transaction. Indeed - if we get ENOSPC errors, we have to be able to do this\n * truncation without a transaction as there is no space left for block\n * reservation (typically why we see a ENOSPC in writeback).\n *\n * This is not a performance critical path, so for now just do the punching a\n * buffer head at a time.\n */\nSTATIC void\nxfs_aops_discard_page(\n\tstruct page\t\t*page)\n{\n\tstruct inode\t\t*inode = page->mapping->host;\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct buffer_head\t*bh, *head;\n\tloff_t\t\t\toffset = page_offset(page);\n\n\tif (!xfs_check_page_type(page, XFS_IO_DELALLOC, true))\n\t\tgoto out_invalidate;\n\n\tif (XFS_FORCED_SHUTDOWN(ip->i_mount))\n\t\tgoto out_invalidate;\n\n\txfs_alert(ip->i_mount,\n\t\t\"page discard on page %p, inode 0x%llx, offset %llu.\",\n\t\t\tpage, ip->i_ino, offset);\n\n\txfs_ilock(ip, XFS_ILOCK_EXCL);\n\tbh = head = page_buffers(page);\n\tdo {\n\t\tint\t\terror;\n\t\txfs_fileoff_t\tstart_fsb;\n\n\t\tif (!buffer_delay(bh))\n\t\t\tgoto next_buffer;\n\n\t\tstart_fsb = XFS_B_TO_FSBT(ip->i_mount, offset);\n\t\terror = xfs_bmap_punch_delalloc_range(ip, start_fsb, 1);\n\t\tif (error) {\n\t\t\t/* something screwed, just bail */\n\t\t\tif (!XFS_FORCED_SHUTDOWN(ip->i_mount)) {\n\t\t\t\txfs_alert(ip->i_mount,\n\t\t\t\"page discard unable to remove delalloc mapping.\");\n\t\t\t}\n\t\t\tbreak;\n\t\t}\nnext_buffer:\n\t\toffset += 1 << inode->i_blkbits;\n\n\t} while ((bh = bh->b_this_page) != head);\n\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\nout_invalidate:\n\txfs_vm_invalidatepage(page, 0, PAGE_SIZE);\n\treturn;\n}\n\nstatic int\nxfs_map_cow(\n\tstruct xfs_writepage_ctx *wpc,\n\tstruct inode\t\t*inode,\n\tloff_t\t\t\toffset,\n\tunsigned int\t\t*new_type)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_bmbt_irec\timap;\n\tbool\t\t\tis_cow = false, need_alloc = false;\n\tint\t\t\terror;\n\n\t/*\n\t * If we already have a valid COW mapping keep using it.\n\t */\n\tif (wpc->io_type == XFS_IO_COW) {\n\t\twpc->imap_valid = xfs_imap_valid(inode, &wpc->imap, offset);\n\t\tif (wpc->imap_valid) {\n\t\t\t*new_type = XFS_IO_COW;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/*\n\t * Else we need to check if there is a COW mapping at this offset.\n\t */\n\txfs_ilock(ip, XFS_ILOCK_SHARED);\n\tis_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap, &need_alloc);\n\txfs_iunlock(ip, XFS_ILOCK_SHARED);\n\n\tif (!is_cow)\n\t\treturn 0;\n\n\t/*\n\t * And if the COW mapping has a delayed extent here we need to\n\t * allocate real space for it now.\n\t */\n\tif (need_alloc) {\n\t\terror = xfs_iomap_write_allocate(ip, XFS_COW_FORK, offset,\n\t\t\t\t&imap);\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\n\twpc->io_type = *new_type = XFS_IO_COW;\n\twpc->imap_valid = true;\n\twpc->imap = imap;\n\treturn 0;\n}\n\n/*\n * We implement an immediate ioend submission policy here to avoid needing to\n * chain multiple ioends and hence nest mempool allocations which can violate\n * forward progress guarantees we need to provide. The current ioend we are\n * adding buffers to is cached on the writepage context, and if the new buffer\n * does not append to the cached ioend it will create a new ioend and cache that\n * instead.\n *\n * If a new ioend is created and cached, the old ioend is returned and queued\n * locally for submission once the entire page is processed or an error has been\n * detected.  While ioends are submitted immediately after they are completed,\n * batching optimisations are provided by higher level block plugging.\n *\n * At the end of a writeback pass, there will be a cached ioend remaining on the\n * writepage context that the caller will need to submit.\n */\nstatic int\nxfs_writepage_map(\n\tstruct xfs_writepage_ctx *wpc,\n\tstruct writeback_control *wbc,\n\tstruct inode\t\t*inode,\n\tstruct page\t\t*page,\n\tloff_t\t\t\toffset,\n\t__uint64_t              end_offset)\n{\n\tLIST_HEAD(submit_list);\n\tstruct xfs_ioend\t*ioend, *next;\n\tstruct buffer_head\t*bh, *head;\n\tssize_t\t\t\tlen = 1 << inode->i_blkbits;\n\tint\t\t\terror = 0;\n\tint\t\t\tcount = 0;\n\tint\t\t\tuptodate = 1;\n\tunsigned int\t\tnew_type;\n\n\tbh = head = page_buffers(page);\n\toffset = page_offset(page);\n\tdo {\n\t\tif (offset >= end_offset)\n\t\t\tbreak;\n\t\tif (!buffer_uptodate(bh))\n\t\t\tuptodate = 0;\n\n\t\t/*\n\t\t * set_page_dirty dirties all buffers in a page, independent\n\t\t * of their state.  The dirty state however is entirely\n\t\t * meaningless for holes (!mapped && uptodate), so skip\n\t\t * buffers covering holes here.\n\t\t */\n\t\tif (!buffer_mapped(bh) && buffer_uptodate(bh)) {\n\t\t\twpc->imap_valid = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (buffer_unwritten(bh))\n\t\t\tnew_type = XFS_IO_UNWRITTEN;\n\t\telse if (buffer_delay(bh))\n\t\t\tnew_type = XFS_IO_DELALLOC;\n\t\telse if (buffer_uptodate(bh))\n\t\t\tnew_type = XFS_IO_OVERWRITE;\n\t\telse {\n\t\t\tif (PageUptodate(page))\n\t\t\t\tASSERT(buffer_mapped(bh));\n\t\t\t/*\n\t\t\t * This buffer is not uptodate and will not be\n\t\t\t * written to disk.  Ensure that we will put any\n\t\t\t * subsequent writeable buffers into a new\n\t\t\t * ioend.\n\t\t\t */\n\t\t\twpc->imap_valid = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (xfs_is_reflink_inode(XFS_I(inode))) {\n\t\t\terror = xfs_map_cow(wpc, inode, offset, &new_type);\n\t\t\tif (error)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (wpc->io_type != new_type) {\n\t\t\twpc->io_type = new_type;\n\t\t\twpc->imap_valid = false;\n\t\t}\n\n\t\tif (wpc->imap_valid)\n\t\t\twpc->imap_valid = xfs_imap_valid(inode, &wpc->imap,\n\t\t\t\t\t\t\t offset);\n\t\tif (!wpc->imap_valid) {\n\t\t\terror = xfs_map_blocks(inode, offset, &wpc->imap,\n\t\t\t\t\t     wpc->io_type);\n\t\t\tif (error)\n\t\t\t\tgoto out;\n\t\t\twpc->imap_valid = xfs_imap_valid(inode, &wpc->imap,\n\t\t\t\t\t\t\t offset);\n\t\t}\n\t\tif (wpc->imap_valid) {\n\t\t\tlock_buffer(bh);\n\t\t\tif (wpc->io_type != XFS_IO_OVERWRITE)\n\t\t\t\txfs_map_at_offset(inode, bh, &wpc->imap, offset);\n\t\t\txfs_add_to_ioend(inode, bh, offset, wpc, wbc, &submit_list);\n\t\t\tcount++;\n\t\t}\n\n\t} while (offset += len, ((bh = bh->b_this_page) != head));\n\n\tif (uptodate && bh == head)\n\t\tSetPageUptodate(page);\n\n\tASSERT(wpc->ioend || list_empty(&submit_list));\n\nout:\n\t/*\n\t * On error, we have to fail the ioend here because we have locked\n\t * buffers in the ioend. If we don't do this, we'll deadlock\n\t * invalidating the page as that tries to lock the buffers on the page.\n\t * Also, because we may have set pages under writeback, we have to make\n\t * sure we run IO completion to mark the error state of the IO\n\t * appropriately, so we can't cancel the ioend directly here. That means\n\t * we have to mark this page as under writeback if we included any\n\t * buffers from it in the ioend chain so that completion treats it\n\t * correctly.\n\t *\n\t * If we didn't include the page in the ioend, the on error we can\n\t * simply discard and unlock it as there are no other users of the page\n\t * or it's buffers right now. The caller will still need to trigger\n\t * submission of outstanding ioends on the writepage context so they are\n\t * treated correctly on error.\n\t */\n\tif (count) {\n\t\txfs_start_page_writeback(page, !error);\n\n\t\t/*\n\t\t * Preserve the original error if there was one, otherwise catch\n\t\t * submission errors here and propagate into subsequent ioend\n\t\t * submissions.\n\t\t */\n\t\tlist_for_each_entry_safe(ioend, next, &submit_list, io_list) {\n\t\t\tint error2;\n\n\t\t\tlist_del_init(&ioend->io_list);\n\t\t\terror2 = xfs_submit_ioend(wbc, ioend, error);\n\t\t\tif (error2 && !error)\n\t\t\t\terror = error2;\n\t\t}\n\t} else if (error) {\n\t\txfs_aops_discard_page(page);\n\t\tClearPageUptodate(page);\n\t\tunlock_page(page);\n\t} else {\n\t\t/*\n\t\t * We can end up here with no error and nothing to write if we\n\t\t * race with a partial page truncate on a sub-page block sized\n\t\t * filesystem. In that case we need to mark the page clean.\n\t\t */\n\t\txfs_start_page_writeback(page, 1);\n\t\tend_page_writeback(page);\n\t}\n\n\tmapping_set_error(page->mapping, error);\n\treturn error;\n}\n\n/*\n * Write out a dirty page.\n *\n * For delalloc space on the page we need to allocate space and flush it.\n * For unwritten space on the page we need to start the conversion to\n * regular allocated space.\n * For any other dirty buffer heads on the page we should flush them.\n */\nSTATIC int\nxfs_do_writepage(\n\tstruct page\t\t*page,\n\tstruct writeback_control *wbc,\n\tvoid\t\t\t*data)\n{\n\tstruct xfs_writepage_ctx *wpc = data;\n\tstruct inode\t\t*inode = page->mapping->host;\n\tloff_t\t\t\toffset;\n\t__uint64_t              end_offset;\n\tpgoff_t                 end_index;\n\n\ttrace_xfs_writepage(inode, page, 0, 0);\n\n\tASSERT(page_has_buffers(page));\n\n\t/*\n\t * Refuse to write the page out if we are called from reclaim context.\n\t *\n\t * This avoids stack overflows when called from deeply used stacks in\n\t * random callers for direct reclaim or memcg reclaim.  We explicitly\n\t * allow reclaim from kswapd as the stack usage there is relatively low.\n\t *\n\t * This should never happen except in the case of a VM regression so\n\t * warn about it.\n\t */\n\tif (WARN_ON_ONCE((current->flags & (PF_MEMALLOC|PF_KSWAPD)) ==\n\t\t\tPF_MEMALLOC))\n\t\tgoto redirty;\n\n\t/*\n\t * Given that we do not allow direct reclaim to call us, we should\n\t * never be called while in a filesystem transaction.\n\t */\n\tif (WARN_ON_ONCE(current->flags & PF_FSTRANS))\n\t\tgoto redirty;\n\n\t/*\n\t * Is this page beyond the end of the file?\n\t *\n\t * The page index is less than the end_index, adjust the end_offset\n\t * to the highest offset that this page should represent.\n\t * -----------------------------------------------------\n\t * |\t\t\tfile mapping\t       | <EOF> |\n\t * -----------------------------------------------------\n\t * | Page ... | Page N-2 | Page N-1 |  Page N  |       |\n\t * ^--------------------------------^----------|--------\n\t * |     desired writeback range    |      see else    |\n\t * ---------------------------------^------------------|\n\t */\n\toffset = i_size_read(inode);\n\tend_index = offset >> PAGE_SHIFT;\n\tif (page->index < end_index)\n\t\tend_offset = (xfs_off_t)(page->index + 1) << PAGE_SHIFT;\n\telse {\n\t\t/*\n\t\t * Check whether the page to write out is beyond or straddles\n\t\t * i_size or not.\n\t\t * -------------------------------------------------------\n\t\t * |\t\tfile mapping\t\t        | <EOF>  |\n\t\t * -------------------------------------------------------\n\t\t * | Page ... | Page N-2 | Page N-1 |  Page N   | Beyond |\n\t\t * ^--------------------------------^-----------|---------\n\t\t * |\t\t\t\t    |      Straddles     |\n\t\t * ---------------------------------^-----------|--------|\n\t\t */\n\t\tunsigned offset_into_page = offset & (PAGE_SIZE - 1);\n\n\t\t/*\n\t\t * Skip the page if it is fully outside i_size, e.g. due to a\n\t\t * truncate operation that is in progress. We must redirty the\n\t\t * page so that reclaim stops reclaiming it. Otherwise\n\t\t * xfs_vm_releasepage() is called on it and gets confused.\n\t\t *\n\t\t * Note that the end_index is unsigned long, it would overflow\n\t\t * if the given offset is greater than 16TB on 32-bit system\n\t\t * and if we do check the page is fully outside i_size or not\n\t\t * via \"if (page->index >= end_index + 1)\" as \"end_index + 1\"\n\t\t * will be evaluated to 0.  Hence this page will be redirtied\n\t\t * and be written out repeatedly which would result in an\n\t\t * infinite loop, the user program that perform this operation\n\t\t * will hang.  Instead, we can verify this situation by checking\n\t\t * if the page to write is totally beyond the i_size or if it's\n\t\t * offset is just equal to the EOF.\n\t\t */\n\t\tif (page->index > end_index ||\n\t\t    (page->index == end_index && offset_into_page == 0))\n\t\t\tgoto redirty;\n\n\t\t/*\n\t\t * The page straddles i_size.  It must be zeroed out on each\n\t\t * and every writepage invocation because it may be mmapped.\n\t\t * \"A file is mapped in multiples of the page size.  For a file\n\t\t * that is not a multiple of the page size, the remaining\n\t\t * memory is zeroed when mapped, and writes to that region are\n\t\t * not written out to the file.\"\n\t\t */\n\t\tzero_user_segment(page, offset_into_page, PAGE_SIZE);\n\n\t\t/* Adjust the end_offset to the end of file */\n\t\tend_offset = offset;\n\t}\n\n\treturn xfs_writepage_map(wpc, wbc, inode, page, offset, end_offset);\n\nredirty:\n\tredirty_page_for_writepage(wbc, page);\n\tunlock_page(page);\n\treturn 0;\n}\n\nSTATIC int\nxfs_vm_writepage(\n\tstruct page\t\t*page,\n\tstruct writeback_control *wbc)\n{\n\tstruct xfs_writepage_ctx wpc = {\n\t\t.io_type = XFS_IO_INVALID,\n\t};\n\tint\t\t\tret;\n\n\tret = xfs_do_writepage(page, wbc, &wpc);\n\tif (wpc.ioend)\n\t\tret = xfs_submit_ioend(wbc, wpc.ioend, ret);\n\treturn ret;\n}\n\nSTATIC int\nxfs_vm_writepages(\n\tstruct address_space\t*mapping,\n\tstruct writeback_control *wbc)\n{\n\tstruct xfs_writepage_ctx wpc = {\n\t\t.io_type = XFS_IO_INVALID,\n\t};\n\tint\t\t\tret;\n\n\txfs_iflags_clear(XFS_I(mapping->host), XFS_ITRUNCATED);\n\tif (dax_mapping(mapping))\n\t\treturn dax_writeback_mapping_range(mapping,\n\t\t\t\txfs_find_bdev_for_inode(mapping->host), wbc);\n\n\tret = write_cache_pages(mapping, wbc, xfs_do_writepage, &wpc);\n\tif (wpc.ioend)\n\t\tret = xfs_submit_ioend(wbc, wpc.ioend, ret);\n\treturn ret;\n}\n\n/*\n * Called to move a page into cleanable state - and from there\n * to be released. The page should already be clean. We always\n * have buffer heads in this call.\n *\n * Returns 1 if the page is ok to release, 0 otherwise.\n */\nSTATIC int\nxfs_vm_releasepage(\n\tstruct page\t\t*page,\n\tgfp_t\t\t\tgfp_mask)\n{\n\tint\t\t\tdelalloc, unwritten;\n\n\ttrace_xfs_releasepage(page->mapping->host, page, 0, 0);\n\n\t/*\n\t * mm accommodates an old ext3 case where clean pages might not have had\n\t * the dirty bit cleared. Thus, it can send actual dirty pages to\n\t * ->releasepage() via shrink_active_list(). Conversely,\n\t * block_invalidatepage() can send pages that are still marked dirty\n\t * but otherwise have invalidated buffers.\n\t *\n\t * We've historically freed buffers on the latter. Instead, quietly\n\t * filter out all dirty pages to avoid spurious buffer state warnings.\n\t * This can likely be removed once shrink_active_list() is fixed.\n\t */\n\tif (PageDirty(page))\n\t\treturn 0;\n\n\txfs_count_page_state(page, &delalloc, &unwritten);\n\n\tif (WARN_ON_ONCE(delalloc))\n\t\treturn 0;\n\tif (WARN_ON_ONCE(unwritten))\n\t\treturn 0;\n\n\treturn try_to_free_buffers(page);\n}\n\n/*\n * When we map a DIO buffer, we may need to pass flags to\n * xfs_end_io_direct_write to tell it what kind of write IO we are doing.\n *\n * Note that for DIO, an IO to the highest supported file block offset (i.e.\n * 2^63 - 1FSB bytes) will result in the offset + count overflowing a signed 64\n * bit variable. Hence if we see this overflow, we have to assume that the IO is\n * extending the file size. We won't know for sure until IO completion is run\n * and the actual max write offset is communicated to the IO completion\n * routine.\n */\nstatic void\nxfs_map_direct(\n\tstruct inode\t\t*inode,\n\tstruct buffer_head\t*bh_result,\n\tstruct xfs_bmbt_irec\t*imap,\n\txfs_off_t\t\toffset,\n\tbool\t\t\tis_cow)\n{\n\tuintptr_t\t\t*flags = (uintptr_t *)&bh_result->b_private;\n\txfs_off_t\t\tsize = bh_result->b_size;\n\n\ttrace_xfs_get_blocks_map_direct(XFS_I(inode), offset, size,\n\t\tISUNWRITTEN(imap) ? XFS_IO_UNWRITTEN : is_cow ? XFS_IO_COW :\n\t\tXFS_IO_OVERWRITE, imap);\n\n\tif (ISUNWRITTEN(imap)) {\n\t\t*flags |= XFS_DIO_FLAG_UNWRITTEN;\n\t\tset_buffer_defer_completion(bh_result);\n\t} else if (is_cow) {\n\t\t*flags |= XFS_DIO_FLAG_COW;\n\t\tset_buffer_defer_completion(bh_result);\n\t}\n\tif (offset + size > i_size_read(inode) || offset + size < 0) {\n\t\t*flags |= XFS_DIO_FLAG_APPEND;\n\t\tset_buffer_defer_completion(bh_result);\n\t}\n}\n\n/*\n * If this is O_DIRECT or the mpage code calling tell them how large the mapping\n * is, so that we can avoid repeated get_blocks calls.\n *\n * If the mapping spans EOF, then we have to break the mapping up as the mapping\n * for blocks beyond EOF must be marked new so that sub block regions can be\n * correctly zeroed. We can't do this for mappings within EOF unless the mapping\n * was just allocated or is unwritten, otherwise the callers would overwrite\n * existing data with zeros. Hence we have to split the mapping into a range up\n * to and including EOF, and a second mapping for beyond EOF.\n */\nstatic void\nxfs_map_trim_size(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tstruct xfs_bmbt_irec\t*imap,\n\txfs_off_t\t\toffset,\n\tssize_t\t\t\tsize)\n{\n\txfs_off_t\t\tmapping_size;\n\n\tmapping_size = imap->br_startoff + imap->br_blockcount - iblock;\n\tmapping_size <<= inode->i_blkbits;\n\n\tASSERT(mapping_size > 0);\n\tif (mapping_size > size)\n\t\tmapping_size = size;\n\tif (offset < i_size_read(inode) &&\n\t    offset + mapping_size >= i_size_read(inode)) {\n\t\t/* limit mapping to block that spans EOF */\n\t\tmapping_size = roundup_64(i_size_read(inode) - offset,\n\t\t\t\t\t  1 << inode->i_blkbits);\n\t}\n\tif (mapping_size > LONG_MAX)\n\t\tmapping_size = LONG_MAX;\n\n\tbh_result->b_size = mapping_size;\n}\n\n/* Bounce unaligned directio writes to the page cache. */\nstatic int\nxfs_bounce_unaligned_dio_write(\n\tstruct xfs_inode\t*ip,\n\txfs_fileoff_t\t\toffset_fsb,\n\tstruct xfs_bmbt_irec\t*imap)\n{\n\tstruct xfs_bmbt_irec\tirec;\n\txfs_fileoff_t\t\tdelta;\n\tbool\t\t\tshared;\n\tbool\t\t\tx;\n\tint\t\t\terror;\n\n\tirec = *imap;\n\tif (offset_fsb > irec.br_startoff) {\n\t\tdelta = offset_fsb - irec.br_startoff;\n\t\tirec.br_blockcount -= delta;\n\t\tirec.br_startblock += delta;\n\t\tirec.br_startoff = offset_fsb;\n\t}\n\terror = xfs_reflink_trim_around_shared(ip, &irec, &shared, &x);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * We're here because we're trying to do a directio write to a\n\t * region that isn't aligned to a filesystem block.  If any part\n\t * of the extent is shared, fall back to buffered mode to handle\n\t * the RMW.  This is done by returning -EREMCHG (\"remote addr\n\t * changed\"), which is caught further up the call stack.\n\t */\n\tif (shared) {\n\t\ttrace_xfs_reflink_bounce_dio_write(ip, imap);\n\t\treturn -EREMCHG;\n\t}\n\treturn 0;\n}\n\nSTATIC int\n__xfs_get_blocks(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate,\n\tbool\t\t\tdirect,\n\tbool\t\t\tdax_fault)\n{\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\txfs_fileoff_t\t\toffset_fsb, end_fsb;\n\tint\t\t\terror = 0;\n\tint\t\t\tlockmode = 0;\n\tstruct xfs_bmbt_irec\timap;\n\tint\t\t\tnimaps = 1;\n\txfs_off_t\t\toffset;\n\tssize_t\t\t\tsize;\n\tint\t\t\tnew = 0;\n\tbool\t\t\tis_cow = false;\n\tbool\t\t\tneed_alloc = false;\n\n\tBUG_ON(create && !direct);\n\n\tif (XFS_FORCED_SHUTDOWN(mp))\n\t\treturn -EIO;\n\n\toffset = (xfs_off_t)iblock << inode->i_blkbits;\n\tASSERT(bh_result->b_size >= (1 << inode->i_blkbits));\n\tsize = bh_result->b_size;\n\n\tif (!create && offset >= i_size_read(inode))\n\t\treturn 0;\n\n\t/*\n\t * Direct I/O is usually done on preallocated files, so try getting\n\t * a block mapping without an exclusive lock first.\n\t */\n\tlockmode = xfs_ilock_data_map_shared(ip);\n\n\tASSERT(offset <= mp->m_super->s_maxbytes);\n\tif (offset + size > mp->m_super->s_maxbytes)\n\t\tsize = mp->m_super->s_maxbytes - offset;\n\tend_fsb = XFS_B_TO_FSB(mp, (xfs_ufsize_t)offset + size);\n\toffset_fsb = XFS_B_TO_FSBT(mp, offset);\n\n\tif (create && direct && xfs_is_reflink_inode(ip))\n\t\tis_cow = xfs_reflink_find_cow_mapping(ip, offset, &imap,\n\t\t\t\t\t&need_alloc);\n\tif (!is_cow) {\n\t\terror = xfs_bmapi_read(ip, offset_fsb, end_fsb - offset_fsb,\n\t\t\t\t\t&imap, &nimaps, XFS_BMAPI_ENTIRE);\n\t\t/*\n\t\t * Truncate an overwrite extent if there's a pending CoW\n\t\t * reservation before the end of this extent.  This\n\t\t * forces us to come back to get_blocks to take care of\n\t\t * the CoW.\n\t\t */\n\t\tif (create && direct && nimaps &&\n\t\t    imap.br_startblock != HOLESTARTBLOCK &&\n\t\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t\t    !ISUNWRITTEN(&imap))\n\t\t\txfs_reflink_trim_irec_to_next_cow(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t}\n\tASSERT(!need_alloc);\n\tif (error)\n\t\tgoto out_unlock;\n\n\t/*\n\t * The only time we can ever safely find delalloc blocks on direct I/O\n\t * is a dio write to post-eof speculative preallocation. All other\n\t * scenarios are indicative of a problem or misuse (such as mixing\n\t * direct and mapped I/O).\n\t *\n\t * The file may be unmapped by the time we get here so we cannot\n\t * reliably fail the I/O based on mapping. Instead, fail the I/O if this\n\t * is a read or a write within eof. Otherwise, carry on but warn as a\n\t * precuation if the file happens to be mapped.\n\t */\n\tif (direct && imap.br_startblock == DELAYSTARTBLOCK) {\n\t\tif (!create || offset < i_size_read(VFS_I(ip))) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\terror = -EIO;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tWARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));\n\t}\n\n\t/* for DAX, we convert unwritten extents directly */\n\tif (create &&\n\t    (!nimaps ||\n\t     (imap.br_startblock == HOLESTARTBLOCK ||\n\t      imap.br_startblock == DELAYSTARTBLOCK) ||\n\t     (IS_DAX(inode) && ISUNWRITTEN(&imap)))) {\n\t\t/*\n\t\t * xfs_iomap_write_direct() expects the shared lock. It\n\t\t * is unlocked on return.\n\t\t */\n\t\tif (lockmode == XFS_ILOCK_EXCL)\n\t\t\txfs_ilock_demote(ip, lockmode);\n\n\t\terror = xfs_iomap_write_direct(ip, offset, size,\n\t\t\t\t\t       &imap, nimaps);\n\t\tif (error)\n\t\t\treturn error;\n\t\tnew = 1;\n\n\t\ttrace_xfs_get_blocks_alloc(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_DELALLOC, &imap);\n\t} else if (nimaps) {\n\t\ttrace_xfs_get_blocks_found(ip, offset, size,\n\t\t\t\tISUNWRITTEN(&imap) ? XFS_IO_UNWRITTEN\n\t\t\t\t\t\t   : XFS_IO_OVERWRITE, &imap);\n\t\txfs_iunlock(ip, lockmode);\n\t} else {\n\t\ttrace_xfs_get_blocks_notfound(ip, offset, size);\n\t\tgoto out_unlock;\n\t}\n\n\tif (IS_DAX(inode) && create) {\n\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t/* zeroing is not needed at a higher layer */\n\t\tnew = 0;\n\t}\n\n\t/* trim mapping down to size requested */\n\txfs_map_trim_size(inode, iblock, bh_result, &imap, offset, size);\n\n\t/*\n\t * For unwritten extents do not report a disk address in the buffered\n\t * read case (treat as if we're reading into a hole).\n\t */\n\tif (imap.br_startblock != HOLESTARTBLOCK &&\n\t    imap.br_startblock != DELAYSTARTBLOCK &&\n\t    (create || !ISUNWRITTEN(&imap))) {\n\t\tif (create && direct && !is_cow) {\n\t\t\terror = xfs_bounce_unaligned_dio_write(ip, offset_fsb,\n\t\t\t\t\t&imap);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\n\t\txfs_map_buffer(inode, bh_result, &imap, offset);\n\t\tif (ISUNWRITTEN(&imap))\n\t\t\tset_buffer_unwritten(bh_result);\n\t\t/* direct IO needs special help */\n\t\tif (create) {\n\t\t\tif (dax_fault)\n\t\t\t\tASSERT(!ISUNWRITTEN(&imap));\n\t\t\telse\n\t\t\t\txfs_map_direct(inode, bh_result, &imap, offset,\n\t\t\t\t\t\tis_cow);\n\t\t}\n\t}\n\n\t/*\n\t * If this is a realtime file, data may be on a different device.\n\t * to that pointed to from the buffer_head b_bdev currently.\n\t */\n\tbh_result->b_bdev = xfs_find_bdev_for_inode(inode);\n\n\t/*\n\t * If we previously allocated a block out beyond eof and we are now\n\t * coming back to use it then we will need to flag it as new even if it\n\t * has a disk address.\n\t *\n\t * With sub-block writes into unwritten extents we also need to mark\n\t * the buffer as new so that the unwritten parts of the buffer gets\n\t * correctly zeroed.\n\t */\n\tif (create &&\n\t    ((!buffer_mapped(bh_result) && !buffer_uptodate(bh_result)) ||\n\t     (offset >= i_size_read(inode)) ||\n\t     (new || ISUNWRITTEN(&imap))))\n\t\tset_buffer_new(bh_result);\n\n\treturn 0;\n\nout_unlock:\n\txfs_iunlock(ip, lockmode);\n\treturn error;\n}\n\nint\nxfs_get_blocks(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate)\n{\n\treturn __xfs_get_blocks(inode, iblock, bh_result, create, false, false);\n}\n\nint\nxfs_get_blocks_direct(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate)\n{\n\treturn __xfs_get_blocks(inode, iblock, bh_result, create, true, false);\n}\n\nint\nxfs_get_blocks_dax_fault(\n\tstruct inode\t\t*inode,\n\tsector_t\t\tiblock,\n\tstruct buffer_head\t*bh_result,\n\tint\t\t\tcreate)\n{\n\treturn __xfs_get_blocks(inode, iblock, bh_result, create, true, true);\n}\n\n/*\n * Complete a direct I/O write request.\n *\n * xfs_map_direct passes us some flags in the private data to tell us what to\n * do.  If no flags are set, then the write IO is an overwrite wholly within\n * the existing allocated file size and so there is nothing for us to do.\n *\n * Note that in this case the completion can be called in interrupt context,\n * whereas if we have flags set we will always be called in task context\n * (i.e. from a workqueue).\n */\nint\nxfs_end_io_direct_write(\n\tstruct kiocb\t\t*iocb,\n\tloff_t\t\t\toffset,\n\tssize_t\t\t\tsize,\n\tvoid\t\t\t*private)\n{\n\tstruct inode\t\t*inode = file_inode(iocb->ki_filp);\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\tuintptr_t\t\tflags = (uintptr_t)private;\n\tint\t\t\terror = 0;\n\n\ttrace_xfs_end_io_direct_write(ip, offset, size);\n\n\tif (XFS_FORCED_SHUTDOWN(ip->i_mount))\n\t\treturn -EIO;\n\n\tif (size <= 0)\n\t\treturn size;\n\n\t/*\n\t * The flags tell us whether we are doing unwritten extent conversions\n\t * or an append transaction that updates the on-disk file size. These\n\t * cases are the only cases where we should *potentially* be needing\n\t * to update the VFS inode size.\n\t */\n\tif (flags == 0) {\n\t\tASSERT(offset + size <= i_size_read(inode));\n\t\treturn 0;\n\t}\n\n\t/*\n\t * We need to update the in-core inode size here so that we don't end up\n\t * with the on-disk inode size being outside the in-core inode size. We\n\t * have no other method of updating EOF for AIO, so always do it here\n\t * if necessary.\n\t *\n\t * We need to lock the test/set EOF update as we can be racing with\n\t * other IO completions here to update the EOF. Failing to serialise\n\t * here can result in EOF moving backwards and Bad Things Happen when\n\t * that occurs.\n\t */\n\tspin_lock(&ip->i_flags_lock);\n\tif (offset + size > i_size_read(inode))\n\t\ti_size_write(inode, offset + size);\n\tspin_unlock(&ip->i_flags_lock);\n\n\tif (flags & XFS_DIO_FLAG_COW)\n\t\terror = xfs_reflink_end_cow(ip, offset, size);\n\tif (flags & XFS_DIO_FLAG_UNWRITTEN) {\n\t\ttrace_xfs_end_io_direct_write_unwritten(ip, offset, size);\n\n\t\terror = xfs_iomap_write_unwritten(ip, offset, size);\n\t}\n\tif (flags & XFS_DIO_FLAG_APPEND) {\n\t\ttrace_xfs_end_io_direct_write_append(ip, offset, size);\n\n\t\terror = xfs_setfilesize(ip, offset, size);\n\t}\n\n\treturn error;\n}\n\nSTATIC ssize_t\nxfs_vm_direct_IO(\n\tstruct kiocb\t\t*iocb,\n\tstruct iov_iter\t\t*iter)\n{\n\t/*\n\t * We just need the method present so that open/fcntl allow direct I/O.\n\t */\n\treturn -EINVAL;\n}\n\nSTATIC sector_t\nxfs_vm_bmap(\n\tstruct address_space\t*mapping,\n\tsector_t\t\tblock)\n{\n\tstruct inode\t\t*inode = (struct inode *)mapping->host;\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\n\ttrace_xfs_vm_bmap(XFS_I(inode));\n\txfs_ilock(ip, XFS_IOLOCK_SHARED);\n\n\t/*\n\t * The swap code (ab-)uses ->bmap to get a block mapping and then\n\t * bypasse\u0455 the file system for actual I/O.  We really can't allow\n\t * that on reflinks inodes, so we have to skip out here.  And yes,\n\t * 0 is the magic code for a bmap error..\n\t */\n\tif (xfs_is_reflink_inode(ip)) {\n\t\txfs_iunlock(ip, XFS_IOLOCK_SHARED);\n\t\treturn 0;\n\t}\n\tfilemap_write_and_wait(mapping);\n\txfs_iunlock(ip, XFS_IOLOCK_SHARED);\n\treturn generic_block_bmap(mapping, block, xfs_get_blocks);\n}\n\nSTATIC int\nxfs_vm_readpage(\n\tstruct file\t\t*unused,\n\tstruct page\t\t*page)\n{\n\ttrace_xfs_vm_readpage(page->mapping->host, 1);\n\treturn mpage_readpage(page, xfs_get_blocks);\n}\n\nSTATIC int\nxfs_vm_readpages(\n\tstruct file\t\t*unused,\n\tstruct address_space\t*mapping,\n\tstruct list_head\t*pages,\n\tunsigned\t\tnr_pages)\n{\n\ttrace_xfs_vm_readpages(mapping->host, nr_pages);\n\treturn mpage_readpages(mapping, pages, nr_pages, xfs_get_blocks);\n}\n\n/*\n * This is basically a copy of __set_page_dirty_buffers() with one\n * small tweak: buffers beyond EOF do not get marked dirty. If we mark them\n * dirty, we'll never be able to clean them because we don't write buffers\n * beyond EOF, and that means we can't invalidate pages that span EOF\n * that have been marked dirty. Further, the dirty state can leak into\n * the file interior if the file is extended, resulting in all sorts of\n * bad things happening as the state does not match the underlying data.\n *\n * XXX: this really indicates that bufferheads in XFS need to die. Warts like\n * this only exist because of bufferheads and how the generic code manages them.\n */\nSTATIC int\nxfs_vm_set_page_dirty(\n\tstruct page\t\t*page)\n{\n\tstruct address_space\t*mapping = page->mapping;\n\tstruct inode\t\t*inode = mapping->host;\n\tloff_t\t\t\tend_offset;\n\tloff_t\t\t\toffset;\n\tint\t\t\tnewly_dirty;\n\n\tif (unlikely(!mapping))\n\t\treturn !TestSetPageDirty(page);\n\n\tend_offset = i_size_read(inode);\n\toffset = page_offset(page);\n\n\tspin_lock(&mapping->private_lock);\n\tif (page_has_buffers(page)) {\n\t\tstruct buffer_head *head = page_buffers(page);\n\t\tstruct buffer_head *bh = head;\n\n\t\tdo {\n\t\t\tif (offset < end_offset)\n\t\t\t\tset_buffer_dirty(bh);\n\t\t\tbh = bh->b_this_page;\n\t\t\toffset += 1 << inode->i_blkbits;\n\t\t} while (bh != head);\n\t}\n\t/*\n\t * Lock out page->mem_cgroup migration to keep PageDirty\n\t * synchronized with per-memcg dirty page counters.\n\t */\n\tlock_page_memcg(page);\n\tnewly_dirty = !TestSetPageDirty(page);\n\tspin_unlock(&mapping->private_lock);\n\n\tif (newly_dirty) {\n\t\t/* sigh - __set_page_dirty() is static, so copy it here, too */\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&mapping->tree_lock, flags);\n\t\tif (page->mapping) {\t/* Race with truncate? */\n\t\t\tWARN_ON_ONCE(!PageUptodate(page));\n\t\t\taccount_page_dirtied(page, mapping);\n\t\t\tradix_tree_tag_set(&mapping->page_tree,\n\t\t\t\t\tpage_index(page), PAGECACHE_TAG_DIRTY);\n\t\t}\n\t\tspin_unlock_irqrestore(&mapping->tree_lock, flags);\n\t}\n\tunlock_page_memcg(page);\n\tif (newly_dirty)\n\t\t__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);\n\treturn newly_dirty;\n}\n\nconst struct address_space_operations xfs_address_space_operations = {\n\t.readpage\t\t= xfs_vm_readpage,\n\t.readpages\t\t= xfs_vm_readpages,\n\t.writepage\t\t= xfs_vm_writepage,\n\t.writepages\t\t= xfs_vm_writepages,\n\t.set_page_dirty\t\t= xfs_vm_set_page_dirty,\n\t.releasepage\t\t= xfs_vm_releasepage,\n\t.invalidatepage\t\t= xfs_vm_invalidatepage,\n\t.bmap\t\t\t= xfs_vm_bmap,\n\t.direct_IO\t\t= xfs_vm_direct_IO,\n\t.migratepage\t\t= buffer_migrate_page,\n\t.is_partially_uptodate  = block_is_partially_uptodate,\n\t.error_remove_page\t= generic_error_remove_page,\n};\n"], "filenames": ["fs/xfs/xfs_aops.c"], "buggy_code_start_loc": [1363], "buggy_code_end_loc": [1454], "fixing_code_start_loc": [1364], "fixing_code_end_loc": [1471], "type": "CWE-362", "message": "In the Linux kernel before 4.9.3, fs/xfs/xfs_aops.c allows local users to cause a denial of service (system crash) because there is a race condition between direct and memory-mapped I/O (associated with a hole) that is handled with BUG_ON instead of an I/O failure.", "other": {"cve": {"id": "CVE-2016-10741", "sourceIdentifier": "cve@mitre.org", "published": "2019-02-01T16:29:00.317", "lastModified": "2019-04-18T15:20:01.537", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In the Linux kernel before 4.9.3, fs/xfs/xfs_aops.c allows local users to cause a denial of service (system crash) because there is a race condition between direct and memory-mapped I/O (associated with a hole) that is handled with BUG_ON instead of an I/O failure."}, {"lang": "es", "value": "En el kernel de Linux, en versiones anteriores a la 4.9.3, \"fs/xfs/xfs_aops.c\" permite a los usuarios locales provocar una denegaci\u00f3n de servicio (cierre inesperado del sistema) debido a que hay una condici\u00f3n de carrera entre el I/O directo y el mapeado con la memoria (asociado con un agujero) que se maneja con BUG_ON en vez de un fallo I/O."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.0, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.7}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.9.3", "matchCriteriaId": "35F648B0-DC13-4848-8DC8-F6133AFBF280"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=04197b341f23b908193308b8d63d17ff23232598", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/106822", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.suse.com/show_bug.cgi?id=1124010", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.9.3", "source": "cve@mitre.org", "tags": ["Release Notes"]}, {"url": "https://github.com/torvalds/linux/commit/04197b341f23b908193308b8d63d17ff23232598", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/03/msg00034.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/04/msg00004.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/04197b341f23b908193308b8d63d17ff23232598"}}