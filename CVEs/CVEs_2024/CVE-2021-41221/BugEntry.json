{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/lib/strings/strcat.h\"\n\nnamespace tensorflow {\nnamespace {\n\nconstexpr auto kRNNModeAttrs =\n    \"rnn_mode: {'rnn_relu', 'rnn_tanh', 'lstm', 'gru'} = 'lstm'\";\n\nconstexpr auto kRNNInputModeAttrs =\n    \"input_mode: {'linear_input', 'skip_input', 'auto_select'} = \"\n    \"'linear_input'\";\n\nconstexpr auto kRNNDirectionAttrs =\n    \"direction: {'unidirectional', 'bidirectional'} = 'unidirectional'\";\n\n}  // namespace\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\nREGISTER_OP(\"CudnnRNNParamsSize\")\n    .Input(\"num_layers: int32\")\n    .Input(\"num_units: int32\")\n    .Input(\"input_size: int32\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"S: {int32, int64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"num_proj: int = 0\")\n    .Output(\"params_size: S\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      // num_layers, num_units, and input_size should be scalars.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n\n      c->set_output(0, c->Vector(1));\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNN\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .SetIsStateful()\n    .Output(\"output: T\")\n    .Output(\"output_h: T\")\n    .Output(\"output_c: T\")\n    .Output(\"reserve_space: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      auto seq_length = c->Dim(input_shape, 0);\n      auto batch_size = c->Dim(input_shape, 1);\n      auto num_units = c->Dim(input_h_shape, 2);\n      string direction;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n      string rnn_mode;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n      int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n      DimensionHandle output_size;\n      TF_RETURN_IF_ERROR(c->Multiply(num_units, dir_count, &output_size));\n      auto output_shape = c->MakeShape({seq_length, batch_size, output_size});\n      auto output_h_shape = input_h_shape;\n      auto output_c_shape TF_ATTRIBUTE_UNUSED =\n          (rnn_mode == \"lstm\") ? output_h_shape : c->MakeShape({});\n      c->set_output(0, output_shape);\n      c->set_output(1, output_h_shape);\n      c->set_output(2, output_c_shape);\n      c->set_output(3, c->UnknownShape());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNV2\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .SetIsStateful()\n    .Output(\"output: T\")\n    .Output(\"output_h: T\")\n    .Output(\"output_c: T\")\n    .Output(\"reserve_space: T\")\n    .Output(\"host_reserved: int8\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      auto seq_length = c->Dim(input_shape, 0);\n      auto batch_size = c->Dim(input_shape, 1);\n      auto num_units = c->Dim(input_h_shape, 2);\n      string direction;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n      string rnn_mode;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n      int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n      DimensionHandle output_size;\n      TF_RETURN_IF_ERROR(c->Multiply(num_units, dir_count, &output_size));\n      auto output_shape = c->MakeShape({seq_length, batch_size, output_size});\n      auto output_h_shape = input_h_shape;\n      auto output_c_shape TF_ATTRIBUTE_UNUSED =\n          (rnn_mode == \"lstm\") ? output_h_shape : c->MakeShape({});\n      c->set_output(0, output_shape);\n      c->set_output(1, output_h_shape);\n      c->set_output(2, output_c_shape);\n      c->set_output(3, c->UnknownShape());\n      c->set_output(4, c->UnknownShape());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNV3\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .Input(\"sequence_lengths: int32\")\n    .SetIsStateful()\n    .Output(\"output: T\")\n    .Output(\"output_h: T\")\n    .Output(\"output_c: T\")\n    .Output(\"reserve_space: T\")\n    .Output(\"host_reserved: int8\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"num_proj: int = 0\")\n    .Attr(\"is_training: bool = true\")\n    .Attr(\"time_major: bool = true\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      auto input_c_shape = c->input(2);\n      auto max_seq_length = c->Dim(input_shape, 0);\n      auto batch_size = c->Dim(input_shape, 1);\n      auto num_units = c->Dim(input_h_shape, 2);\n      string direction;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n      string rnn_mode;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n      int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n      DimensionHandle output_size;\n      TF_RETURN_IF_ERROR(c->Multiply(num_units, dir_count, &output_size));\n      auto output_shape =\n          c->MakeShape({max_seq_length, batch_size, output_size});\n      auto output_h_shape = input_h_shape;\n      auto output_c_shape TF_ATTRIBUTE_UNUSED =\n          (rnn_mode == \"lstm\") ? input_c_shape : c->MakeShape({});\n      c->set_output(0, output_shape);\n      c->set_output(1, output_h_shape);\n      c->set_output(2, output_c_shape);\n      c->set_output(3, c->UnknownShape());\n      c->set_output(4, c->UnknownShape());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNBackprop\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .Input(\"output: T\")\n    .Input(\"output_h: T\")\n    .Input(\"output_c: T\")\n    .Input(\"output_backprop: T\")\n    .Input(\"output_h_backprop: T\")\n    .Input(\"output_c_backprop: T\")\n    .Input(\"reserve_space: T\")\n    .SetIsStateful()\n    .Output(\"input_backprop: T\")\n    .Output(\"input_h_backprop: T\")\n    .Output(\"input_c_backprop: T\")\n    .Output(\"params_backprop: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      auto input_c_shape = c->input(2);\n      auto params_shape = c->input(3);\n      c->set_output(0, input_shape);\n      c->set_output(1, input_h_shape);\n      c->set_output(2, input_c_shape);\n      c->set_output(3, params_shape);\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNBackpropV2\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .Input(\"output: T\")\n    .Input(\"output_h: T\")\n    .Input(\"output_c: T\")\n    .Input(\"output_backprop: T\")\n    .Input(\"output_h_backprop: T\")\n    .Input(\"output_c_backprop: T\")\n    .Input(\"reserve_space: T\")\n    .Input(\"host_reserved: int8\")\n    .SetIsStateful()\n    .Output(\"input_backprop: T\")\n    .Output(\"input_h_backprop: T\")\n    .Output(\"input_c_backprop: T\")\n    .Output(\"params_backprop: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      auto input_c_shape = c->input(2);\n      auto params_shape = c->input(3);\n      c->set_output(0, input_shape);\n      c->set_output(1, input_h_shape);\n      c->set_output(2, input_c_shape);\n      c->set_output(3, params_shape);\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNBackpropV3\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .Input(\"sequence_lengths: int32\")\n    .Input(\"output: T\")\n    .Input(\"output_h: T\")\n    .Input(\"output_c: T\")\n    .Input(\"output_backprop: T\")\n    .Input(\"output_h_backprop: T\")\n    .Input(\"output_c_backprop: T\")\n    .Input(\"reserve_space: T\")\n    .Input(\"host_reserved: int8\")\n    .SetIsStateful()\n    .Output(\"input_backprop: T\")\n    .Output(\"input_h_backprop: T\")\n    .Output(\"input_c_backprop: T\")\n    .Output(\"params_backprop: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"num_proj: int = 0\")\n    .Attr(\"time_major: bool = true\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      auto input_c_shape = c->input(2);\n      auto params_shape = c->input(3);\n      c->set_output(0, input_shape);\n      c->set_output(1, input_h_shape);\n      c->set_output(2, input_c_shape);\n      c->set_output(3, params_shape);\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNParamsToCanonical\")\n    .Input(\"num_layers: int32\")\n    .Input(\"num_units: int32\")\n    .Input(\"input_size: int32\")\n    .Input(\"params: T\")\n    .Output(\"weights: num_params * T\")\n    .Output(\"biases: num_params * T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"num_params: int\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &unused));\n      int num_params;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"num_params\", &num_params));\n      // Set shape for weight matrices\n      for (int i = 0; i < num_params; i++) {\n        c->set_output(i, c->Matrix(InferenceContext::kUnknownDim,\n                                   InferenceContext::kUnknownDim));\n      }\n      // Set shape for bias vectors\n      for (int i = 0; i < num_params; i++) {\n        c->set_output(num_params + i, c->Vector(InferenceContext::kUnknownDim));\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNParamsToCanonicalV2\")\n    .Input(\"num_layers: int32\")\n    .Input(\"num_units: int32\")\n    .Input(\"input_size: int32\")\n    .Input(\"params: T\")\n    .Output(\"weights: num_params_weights * T\")\n    .Output(\"biases: num_params_biases * T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"num_params_weights: int\")\n    .Attr(\"num_params_biases: int\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"num_proj: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &unused));\n      int num_params_weights;\n      int num_params_biases;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"num_params_weights\", &num_params_weights));\n      TF_RETURN_IF_ERROR(c->GetAttr(\"num_params_biases\", &num_params_biases));\n      // Set shape for weight matrices\n      for (int i = 0; i < num_params_weights; i++) {\n        c->set_output(i, c->Matrix(InferenceContext::kUnknownDim,\n                                   InferenceContext::kUnknownDim));\n      }\n      // Set shape for bias vectors\n      for (int i = 0; i < num_params_biases; i++) {\n        c->set_output(num_params_weights + i,\n                      c->Vector(InferenceContext::kUnknownDim));\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNCanonicalToParams\")\n    .Input(\"num_layers: int32\")\n    .Input(\"num_units: int32\")\n    .Input(\"input_size: int32\")\n    .Input(\"weights: num_params * T\")\n    .Input(\"biases: num_params * T\")\n    .Output(\"params: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"num_params: int\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNCanonicalToParamsV2\")\n    .Input(\"num_layers: int32\")\n    .Input(\"num_units: int32\")\n    .Input(\"input_size: int32\")\n    .Input(\"weights: num_params_weights * T\")\n    .Input(\"biases: num_params_biases * T\")\n    .Output(\"params: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"num_params_weights: int\")\n    .Attr(\"num_params_biases: int\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"num_proj: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n      return Status::OK();\n    });\n\n}  // namespace tensorflow\n", "/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/node_def_builder.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference_testutil.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_testutil.h\"\n#include \"tensorflow/core/lib/core/status_test_util.h\"\n#include \"tensorflow/core/lib/strings/str_util.h\"\n#include \"tensorflow/core/platform/test.h\"\n\nnamespace tensorflow {\n\nTEST(CudnnRNNOpsTest, ParamsSize_ShapeFn) {\n  ShapeInferenceTestOp op(\"CudnnRNNParamsSize\");\n  INFER_OK(op, \"[];[];[]\", \"[1]\");\n  INFER_OK(op, \"?;[];[]\", \"[1]\");\n  INFER_OK(op, \"[];?;[]\", \"[1]\");\n  INFER_OK(op, \"[];[];?\", \"[1]\");\n  INFER_OK(op, \"[];?;?\", \"[1]\");\n  INFER_OK(op, \"?;?;?\", \"[1]\");\n\n  INFER_ERROR(\"Shape must be rank 0 \", op, \"[1,2];?;[]\");\n  INFER_ERROR(\"Shape must be rank 0 \", op, \"?;[2];[]\");\n  INFER_ERROR(\"Shape must be rank 0 \", op, \"?;?;[1]\");\n}\n\nTEST(CudnnRNNOpsTest, ForwardLstm_ShapeFn) {\n  int seq_length = 2;\n  int batch_size = 3;\n  int num_units = 4;\n  int num_layers = 5;\n  int dir_count = 1;\n  std::vector<int> input_shape = {seq_length, batch_size, num_units};\n  std::vector<int> input_h_shape = {num_layers * dir_count, batch_size,\n                                    num_units};\n  std::vector<int> output_shape = {seq_length, batch_size,\n                                   num_units * dir_count};\n  auto shape_to_str = [](const std::vector<int>& v) {\n    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n  };\n  string input_shapes_desc = strings::StrCat(\n      shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n      shape_to_str(input_h_shape), \";\", \"[?]\");\n  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?\";\n\n  ShapeInferenceTestOp op(\"CudnnRNN\");\n  TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNN\")\n                   .Input({\"input\", 0, DT_FLOAT})\n                   .Input({\"input_h\", 0, DT_FLOAT})\n                   .Input({\"input_c\", 0, DT_FLOAT})\n                   .Input({\"params\", 0, DT_FLOAT})\n                   .Attr(\"rnn_mode\", \"lstm\")\n                   .Attr(\"input_mode\", \"auto_select\")\n                   .Attr(\"direction\", \"unidirectional\")\n                   .Finalize(&op.node_def));\n  INFER_OK(op, input_shapes_desc, output_shapes_desc);\n}\n\nTEST(CudnnRNNOpsTest, ForwardV2Lstm_ShapeFn) {\n  int seq_length = 2;\n  int batch_size = 3;\n  int num_units = 4;\n  int num_layers = 5;\n  int dir_count = 1;\n  std::vector<int> input_shape = {seq_length, batch_size, num_units};\n  std::vector<int> input_h_shape = {num_layers * dir_count, batch_size,\n                                    num_units};\n  std::vector<int> output_shape = {seq_length, batch_size,\n                                   num_units * dir_count};\n  auto shape_to_str = [](const std::vector<int>& v) {\n    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n  };\n  string input_shapes_desc = strings::StrCat(\n      shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n      shape_to_str(input_h_shape), \";\", \"[?]\");\n  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?;?\";\n\n  ShapeInferenceTestOp op(\"CudnnRNNV2\");\n  TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV2\")\n                   .Input({\"input\", 0, DT_FLOAT})\n                   .Input({\"input_h\", 0, DT_FLOAT})\n                   .Input({\"input_c\", 0, DT_FLOAT})\n                   .Input({\"params\", 0, DT_FLOAT})\n                   .Attr(\"rnn_mode\", \"lstm\")\n                   .Attr(\"input_mode\", \"auto_select\")\n                   .Attr(\"direction\", \"unidirectional\")\n                   .Finalize(&op.node_def));\n  INFER_OK(op, input_shapes_desc, output_shapes_desc);\n}\n\nTEST(CudnnRNNOpsTest, ForwardV3Lstm_ShapeFn) {\n  int max_seq_length = 2;\n  int batch_size = 3;\n  int num_units = 4;\n  int num_layers = 5;\n  int dir_count = 1;\n  std::vector<int> input_shape = {max_seq_length, batch_size, num_units};\n  std::vector<int> input_h_shape = {num_layers * dir_count, batch_size,\n                                    num_units};\n  std::vector<int> input_c_shape = {num_layers * dir_count, batch_size,\n                                    num_units};\n  std::vector<int> output_shape = {max_seq_length, batch_size,\n                                   num_units * dir_count};\n  std::vector<int> seq_lengths_shape = {batch_size};\n  auto shape_to_str = [](const std::vector<int>& v) {\n    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n  };\n  string input_shapes_desc = strings::StrCat(\n      shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n      shape_to_str(input_c_shape), \";\", \"[?]\", \";\",\n      shape_to_str(seq_lengths_shape));\n  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in2;?;?\";\n\n  ShapeInferenceTestOp op(\"CudnnRNNV3\");\n  TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV3\")\n                   .Input({\"input\", 0, DT_FLOAT})\n                   .Input({\"input_h\", 0, DT_FLOAT})\n                   .Input({\"input_c\", 0, DT_FLOAT})\n                   .Input({\"params\", 0, DT_FLOAT})\n                   .Input({\"sequence_lengths\", 0, DT_INT32})\n                   .Attr(\"rnn_mode\", \"lstm\")\n                   .Attr(\"input_mode\", \"auto_select\")\n                   .Attr(\"direction\", \"unidirectional\")\n                   .Finalize(&op.node_def));\n  INFER_OK(op, input_shapes_desc, output_shapes_desc);\n}\n\n}  // end namespace tensorflow\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/common_shape_fns.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/lib/strings/strcat.h\"\n\nnamespace tensorflow {\nnamespace {\n\nconstexpr auto kRNNModeAttrs =\n    \"rnn_mode: {'rnn_relu', 'rnn_tanh', 'lstm', 'gru'} = 'lstm'\";\n\nconstexpr auto kRNNInputModeAttrs =\n    \"input_mode: {'linear_input', 'skip_input', 'auto_select'} = \"\n    \"'linear_input'\";\n\nconstexpr auto kRNNDirectionAttrs =\n    \"direction: {'unidirectional', 'bidirectional'} = 'unidirectional'\";\n\n}  // namespace\n\nusing shape_inference::DimensionHandle;\nusing shape_inference::InferenceContext;\nusing shape_inference::ShapeHandle;\n\nREGISTER_OP(\"CudnnRNNParamsSize\")\n    .Input(\"num_layers: int32\")\n    .Input(\"num_units: int32\")\n    .Input(\"input_size: int32\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"S: {int32, int64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"num_proj: int = 0\")\n    .Output(\"params_size: S\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      // num_layers, num_units, and input_size should be scalars.\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n\n      c->set_output(0, c->Vector(1));\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNN\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .SetIsStateful()\n    .Output(\"output: T\")\n    .Output(\"output_h: T\")\n    .Output(\"output_c: T\")\n    .Output(\"reserve_space: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      TF_RETURN_IF_ERROR(c->WithRank(input_shape, 3, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(input_h_shape, 3, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &unused));\n\n      auto seq_length = c->Dim(input_shape, 0);\n      auto batch_size = c->Dim(input_shape, 1);\n      auto num_units = c->Dim(input_h_shape, 2);\n\n      string direction;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n      string rnn_mode;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n      int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n      DimensionHandle output_size;\n      TF_RETURN_IF_ERROR(c->Multiply(num_units, dir_count, &output_size));\n      auto output_shape = c->MakeShape({seq_length, batch_size, output_size});\n      auto output_h_shape = input_h_shape;\n      auto output_c_shape TF_ATTRIBUTE_UNUSED =\n          (rnn_mode == \"lstm\") ? output_h_shape : c->MakeShape({});\n      c->set_output(0, output_shape);\n      c->set_output(1, output_h_shape);\n      c->set_output(2, output_c_shape);\n      c->set_output(3, c->UnknownShape());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNV2\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .SetIsStateful()\n    .Output(\"output: T\")\n    .Output(\"output_h: T\")\n    .Output(\"output_c: T\")\n    .Output(\"reserve_space: T\")\n    .Output(\"host_reserved: int8\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"is_training: bool = true\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      TF_RETURN_IF_ERROR(c->WithRank(input_shape, 3, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(input_h_shape, 3, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &unused));\n\n      auto seq_length = c->Dim(input_shape, 0);\n      auto batch_size = c->Dim(input_shape, 1);\n      auto num_units = c->Dim(input_h_shape, 2);\n      string direction;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n      string rnn_mode;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n      int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n      DimensionHandle output_size;\n      TF_RETURN_IF_ERROR(c->Multiply(num_units, dir_count, &output_size));\n      auto output_shape = c->MakeShape({seq_length, batch_size, output_size});\n      auto output_h_shape = input_h_shape;\n      auto output_c_shape TF_ATTRIBUTE_UNUSED =\n          (rnn_mode == \"lstm\") ? output_h_shape : c->MakeShape({});\n      c->set_output(0, output_shape);\n      c->set_output(1, output_h_shape);\n      c->set_output(2, output_c_shape);\n      c->set_output(3, c->UnknownShape());\n      c->set_output(4, c->UnknownShape());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNV3\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .Input(\"sequence_lengths: int32\")\n    .SetIsStateful()\n    .Output(\"output: T\")\n    .Output(\"output_h: T\")\n    .Output(\"output_c: T\")\n    .Output(\"reserve_space: T\")\n    .Output(\"host_reserved: int8\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"num_proj: int = 0\")\n    .Attr(\"is_training: bool = true\")\n    .Attr(\"time_major: bool = true\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      auto input_c_shape = c->input(2);\n      TF_RETURN_IF_ERROR(c->WithRank(input_shape, 3, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(input_h_shape, 3, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &unused));\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 1, &unused));\n\n      auto max_seq_length = c->Dim(input_shape, 0);\n      auto batch_size = c->Dim(input_shape, 1);\n      auto num_units = c->Dim(input_h_shape, 2);\n\n      string direction;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"direction\", &direction));\n      string rnn_mode;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"rnn_mode\", &rnn_mode));\n      if (rnn_mode == \"lstm\") {\n        TF_RETURN_IF_ERROR(c->WithRank(input_c_shape, 3, &unused));\n      }\n      int dir_count = (direction == \"bidirectional\") ? 2 : 1;\n      DimensionHandle output_size;\n      TF_RETURN_IF_ERROR(c->Multiply(num_units, dir_count, &output_size));\n      auto output_shape =\n          c->MakeShape({max_seq_length, batch_size, output_size});\n      auto output_h_shape = input_h_shape;\n      auto output_c_shape TF_ATTRIBUTE_UNUSED =\n          (rnn_mode == \"lstm\") ? input_c_shape : c->MakeShape({});\n      c->set_output(0, output_shape);\n      c->set_output(1, output_h_shape);\n      c->set_output(2, output_c_shape);\n      c->set_output(3, c->UnknownShape());\n      c->set_output(4, c->UnknownShape());\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNBackprop\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .Input(\"output: T\")\n    .Input(\"output_h: T\")\n    .Input(\"output_c: T\")\n    .Input(\"output_backprop: T\")\n    .Input(\"output_h_backprop: T\")\n    .Input(\"output_c_backprop: T\")\n    .Input(\"reserve_space: T\")\n    .SetIsStateful()\n    .Output(\"input_backprop: T\")\n    .Output(\"input_h_backprop: T\")\n    .Output(\"input_c_backprop: T\")\n    .Output(\"params_backprop: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      auto input_c_shape = c->input(2);\n      auto params_shape = c->input(3);\n      c->set_output(0, input_shape);\n      c->set_output(1, input_h_shape);\n      c->set_output(2, input_c_shape);\n      c->set_output(3, params_shape);\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNBackpropV2\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .Input(\"output: T\")\n    .Input(\"output_h: T\")\n    .Input(\"output_c: T\")\n    .Input(\"output_backprop: T\")\n    .Input(\"output_h_backprop: T\")\n    .Input(\"output_c_backprop: T\")\n    .Input(\"reserve_space: T\")\n    .Input(\"host_reserved: int8\")\n    .SetIsStateful()\n    .Output(\"input_backprop: T\")\n    .Output(\"input_h_backprop: T\")\n    .Output(\"input_c_backprop: T\")\n    .Output(\"params_backprop: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      auto input_c_shape = c->input(2);\n      auto params_shape = c->input(3);\n      c->set_output(0, input_shape);\n      c->set_output(1, input_h_shape);\n      c->set_output(2, input_c_shape);\n      c->set_output(3, params_shape);\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNBackpropV3\")\n    .Input(\"input: T\")\n    .Input(\"input_h: T\")\n    .Input(\"input_c: T\")\n    .Input(\"params: T\")\n    .Input(\"sequence_lengths: int32\")\n    .Input(\"output: T\")\n    .Input(\"output_h: T\")\n    .Input(\"output_c: T\")\n    .Input(\"output_backprop: T\")\n    .Input(\"output_h_backprop: T\")\n    .Input(\"output_c_backprop: T\")\n    .Input(\"reserve_space: T\")\n    .Input(\"host_reserved: int8\")\n    .SetIsStateful()\n    .Output(\"input_backprop: T\")\n    .Output(\"input_h_backprop: T\")\n    .Output(\"input_c_backprop: T\")\n    .Output(\"params_backprop: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"num_proj: int = 0\")\n    .Attr(\"time_major: bool = true\")\n    .SetShapeFn([](InferenceContext* c) {\n      auto input_shape = c->input(0);\n      auto input_h_shape = c->input(1);\n      auto input_c_shape = c->input(2);\n      auto params_shape = c->input(3);\n      c->set_output(0, input_shape);\n      c->set_output(1, input_h_shape);\n      c->set_output(2, input_c_shape);\n      c->set_output(3, params_shape);\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNParamsToCanonical\")\n    .Input(\"num_layers: int32\")\n    .Input(\"num_units: int32\")\n    .Input(\"input_size: int32\")\n    .Input(\"params: T\")\n    .Output(\"weights: num_params * T\")\n    .Output(\"biases: num_params * T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"num_params: int\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &unused));\n      int num_params;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"num_params\", &num_params));\n      // Set shape for weight matrices\n      for (int i = 0; i < num_params; i++) {\n        c->set_output(i, c->Matrix(InferenceContext::kUnknownDim,\n                                   InferenceContext::kUnknownDim));\n      }\n      // Set shape for bias vectors\n      for (int i = 0; i < num_params; i++) {\n        c->set_output(num_params + i, c->Vector(InferenceContext::kUnknownDim));\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNParamsToCanonicalV2\")\n    .Input(\"num_layers: int32\")\n    .Input(\"num_units: int32\")\n    .Input(\"input_size: int32\")\n    .Input(\"params: T\")\n    .Output(\"weights: num_params_weights * T\")\n    .Output(\"biases: num_params_biases * T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"num_params_weights: int\")\n    .Attr(\"num_params_biases: int\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"num_proj: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      ShapeHandle unused;\n      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 1, &unused));\n      int num_params_weights;\n      int num_params_biases;\n      TF_RETURN_IF_ERROR(c->GetAttr(\"num_params_weights\", &num_params_weights));\n      TF_RETURN_IF_ERROR(c->GetAttr(\"num_params_biases\", &num_params_biases));\n      // Set shape for weight matrices\n      for (int i = 0; i < num_params_weights; i++) {\n        c->set_output(i, c->Matrix(InferenceContext::kUnknownDim,\n                                   InferenceContext::kUnknownDim));\n      }\n      // Set shape for bias vectors\n      for (int i = 0; i < num_params_biases; i++) {\n        c->set_output(num_params_weights + i,\n                      c->Vector(InferenceContext::kUnknownDim));\n      }\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNCanonicalToParams\")\n    .Input(\"num_layers: int32\")\n    .Input(\"num_units: int32\")\n    .Input(\"input_size: int32\")\n    .Input(\"weights: num_params * T\")\n    .Input(\"biases: num_params * T\")\n    .Output(\"params: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"num_params: int\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n      return Status::OK();\n    });\n\nREGISTER_OP(\"CudnnRNNCanonicalToParamsV2\")\n    .Input(\"num_layers: int32\")\n    .Input(\"num_units: int32\")\n    .Input(\"input_size: int32\")\n    .Input(\"weights: num_params_weights * T\")\n    .Input(\"biases: num_params_biases * T\")\n    .Output(\"params: T\")\n    .Attr(\"T: {float16, float32, float64}\")\n    .Attr(\"num_params_weights: int\")\n    .Attr(\"num_params_biases: int\")\n    .Attr(kRNNModeAttrs)\n    .Attr(kRNNInputModeAttrs)\n    .Attr(kRNNDirectionAttrs)\n    .Attr(\"dropout: float = 0.0\")\n    .Attr(\"seed: int = 0\")\n    .Attr(\"seed2: int = 0\")\n    .Attr(\"num_proj: int = 0\")\n    .SetShapeFn([](InferenceContext* c) {\n      c->set_output(0, c->Vector(InferenceContext::kUnknownDim));\n      return Status::OK();\n    });\n\n}  // namespace tensorflow\n", "/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\n\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/framework/node_def_builder.h\"\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference_testutil.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_testutil.h\"\n#include \"tensorflow/core/lib/core/status_test_util.h\"\n#include \"tensorflow/core/lib/strings/str_util.h\"\n#include \"tensorflow/core/platform/test.h\"\n\nnamespace tensorflow {\n\nTEST(CudnnRNNOpsTest, ParamsSize_ShapeFn) {\n  ShapeInferenceTestOp op(\"CudnnRNNParamsSize\");\n  INFER_OK(op, \"[];[];[]\", \"[1]\");\n  INFER_OK(op, \"?;[];[]\", \"[1]\");\n  INFER_OK(op, \"[];?;[]\", \"[1]\");\n  INFER_OK(op, \"[];[];?\", \"[1]\");\n  INFER_OK(op, \"[];?;?\", \"[1]\");\n  INFER_OK(op, \"?;?;?\", \"[1]\");\n\n  INFER_ERROR(\"Shape must be rank 0 \", op, \"[1,2];?;[]\");\n  INFER_ERROR(\"Shape must be rank 0 \", op, \"?;[2];[]\");\n  INFER_ERROR(\"Shape must be rank 0 \", op, \"?;?;[1]\");\n}\n\nTEST(CudnnRNNOpsTest, ForwardLstm_ShapeFn) {\n  int seq_length = 2;\n  int batch_size = 3;\n  int num_units = 4;\n  int num_layers = 5;\n  int dir_count = 1;\n  std::vector<int> input_shape = {seq_length, batch_size, num_units};\n  std::vector<int> input_h_shape = {num_layers * dir_count, batch_size,\n                                    num_units};\n  std::vector<int> output_shape = {seq_length, batch_size,\n                                   num_units * dir_count};\n  auto shape_to_str = [](const std::vector<int>& v) {\n    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n  };\n  string input_shapes_desc = strings::StrCat(\n      shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n      shape_to_str(input_h_shape), \";\", \"[?]\");\n  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?\";\n\n  ShapeInferenceTestOp op(\"CudnnRNN\");\n  TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNN\")\n                   .Input({\"input\", 0, DT_FLOAT})\n                   .Input({\"input_h\", 0, DT_FLOAT})\n                   .Input({\"input_c\", 0, DT_FLOAT})\n                   .Input({\"params\", 0, DT_FLOAT})\n                   .Attr(\"rnn_mode\", \"lstm\")\n                   .Attr(\"input_mode\", \"auto_select\")\n                   .Attr(\"direction\", \"unidirectional\")\n                   .Finalize(&op.node_def));\n  INFER_OK(op, input_shapes_desc, output_shapes_desc);\n  INFER_ERROR(\"Shape must be rank 3 \", op, \"[];[?,?,?];[?,?,?];[?]\");\n  INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[];[?,?,?];[?]\");\n  // Disabled because the kernel does not check shape of input_c.\n  // INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[?,?,?];[?];[?]\");\n  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[?,?,?];[]\");\n}\n\nTEST(CudnnRNNOpsTest, ForwardV2Lstm_ShapeFn) {\n  int seq_length = 2;\n  int batch_size = 3;\n  int num_units = 4;\n  int num_layers = 5;\n  int dir_count = 1;\n  std::vector<int> input_shape = {seq_length, batch_size, num_units};\n  std::vector<int> input_h_shape = {num_layers * dir_count, batch_size,\n                                    num_units};\n  std::vector<int> output_shape = {seq_length, batch_size,\n                                   num_units * dir_count};\n  auto shape_to_str = [](const std::vector<int>& v) {\n    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n  };\n  string input_shapes_desc = strings::StrCat(\n      shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n      shape_to_str(input_h_shape), \";\", \"[?]\");\n  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in1;?;?\";\n\n  ShapeInferenceTestOp op(\"CudnnRNNV2\");\n  TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV2\")\n                   .Input({\"input\", 0, DT_FLOAT})\n                   .Input({\"input_h\", 0, DT_FLOAT})\n                   .Input({\"input_c\", 0, DT_FLOAT})\n                   .Input({\"params\", 0, DT_FLOAT})\n                   .Attr(\"rnn_mode\", \"lstm\")\n                   .Attr(\"input_mode\", \"auto_select\")\n                   .Attr(\"direction\", \"unidirectional\")\n                   .Finalize(&op.node_def));\n  INFER_OK(op, input_shapes_desc, output_shapes_desc);\n  INFER_ERROR(\"Shape must be rank 3 \", op, \"[];[?,?,?];[?,?,?];[?]\");\n  INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[];[?,?,?];[?]\");\n  // Disabled because the kernel does not check shape of input_c.\n  // INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[?,?,?];[?];[?]\");\n  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[?,?,?];[]\");\n}\n\nTEST(CudnnRNNOpsTest, ForwardV3Lstm_ShapeFn) {\n  int max_seq_length = 2;\n  int batch_size = 3;\n  int num_units = 4;\n  int num_layers = 5;\n  int dir_count = 1;\n  std::vector<int> input_shape = {max_seq_length, batch_size, num_units};\n  std::vector<int> input_h_shape = {num_layers * dir_count, batch_size,\n                                    num_units};\n  std::vector<int> input_c_shape = {num_layers * dir_count, batch_size,\n                                    num_units};\n  std::vector<int> output_shape = {max_seq_length, batch_size,\n                                   num_units * dir_count};\n  std::vector<int> seq_lengths_shape = {batch_size};\n  auto shape_to_str = [](const std::vector<int>& v) {\n    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n  };\n  string input_shapes_desc = strings::StrCat(\n      shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n      shape_to_str(input_c_shape), \";\", \"[?]\", \";\",\n      shape_to_str(seq_lengths_shape));\n  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;in2;?;?\";\n\n  ShapeInferenceTestOp op(\"CudnnRNNV3\");\n  TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV3\")\n                   .Input({\"input\", 0, DT_FLOAT})\n                   .Input({\"input_h\", 0, DT_FLOAT})\n                   .Input({\"input_c\", 0, DT_FLOAT})\n                   .Input({\"params\", 0, DT_FLOAT})\n                   .Input({\"sequence_lengths\", 0, DT_INT32})\n                   .Attr(\"rnn_mode\", \"lstm\")\n                   .Attr(\"input_mode\", \"auto_select\")\n                   .Attr(\"direction\", \"unidirectional\")\n                   .Finalize(&op.node_def));\n  INFER_OK(op, input_shapes_desc, output_shapes_desc);\n  INFER_ERROR(\"Shape must be rank 3 \", op, \"[];[?,?,?];[?,?,?];[?];[?]\");\n  INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[];[?,?,?];[?];[?]\");\n  INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[?,?,?];[];[?];[?]\");\n  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[?,?,?];[];[?]\");\n  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[?,?,?];[?];[]\");\n}\n\nTEST(CudnnRNNOpsTest, ForwardV3Gru) {\n  int max_seq_length = 2;\n  int batch_size = 3;\n  int num_units = 4;\n  int num_layers = 5;\n  int dir_count = 1;\n  std::vector<int> input_shape = {max_seq_length, batch_size, num_units};\n  std::vector<int> input_h_shape = {num_layers * dir_count, batch_size,\n                                    num_units};\n  std::vector<int> input_c_shape = {num_layers * dir_count, batch_size,\n                                    num_units};\n  std::vector<int> output_shape = {max_seq_length, batch_size,\n                                   num_units * dir_count};\n  std::vector<int> seq_lengths_shape = {batch_size};\n  auto shape_to_str = [](const std::vector<int>& v) {\n    return strings::StrCat(\"[\", absl::StrJoin(v, \",\"), \"]\");\n  };\n  string input_shapes_desc = strings::StrCat(\n      shape_to_str(input_shape), \";\", shape_to_str(input_h_shape), \";\",\n      shape_to_str(input_c_shape), \";\", \"[?]\", \";\",\n      shape_to_str(seq_lengths_shape));\n  string output_shapes_desc = \"[d0_0,d0_1,d1_2];in1;[];?;?\";\n\n  ShapeInferenceTestOp op(\"CudnnRNNV3\");\n  TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNNV3\")\n                   .Input({\"input\", 0, DT_FLOAT})\n                   .Input({\"input_h\", 0, DT_FLOAT})\n                   .Input({\"input_c\", 0, DT_FLOAT})\n                   .Input({\"params\", 0, DT_FLOAT})\n                   .Input({\"sequence_lengths\", 0, DT_INT32})\n                   .Attr(\"rnn_mode\", \"gru\")\n                   .Attr(\"input_mode\", \"auto_select\")\n                   .Attr(\"direction\", \"unidirectional\")\n                   .Finalize(&op.node_def));\n  INFER_OK(op, input_shapes_desc, output_shapes_desc);\n  INFER_ERROR(\"Shape must be rank 3 \", op, \"[];[?,?,?];[];[?];[?]\");\n  INFER_ERROR(\"Shape must be rank 3 \", op, \"[?,?,?];[];[];[?];[?]\");\n  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[];[];[?]\");\n  INFER_ERROR(\"Shape must be rank 1 \", op, \"[?,?,?];[?,?,?];[];[?];[]\");\n}\n\n}  // end namespace tensorflow\n"], "filenames": ["tensorflow/core/ops/cudnn_rnn_ops.cc", "tensorflow/core/ops/cudnn_rnn_ops_test.cc"], "buggy_code_start_loc": [84, 70], "buggy_code_end_loc": [183, 139], "fixing_code_start_loc": [84, 71], "fixing_code_end_loc": [205, 196], "type": "CWE-787", "message": "TensorFlow is an open source platform for machine learning. In affected versions the shape inference code for the `Cudnn*` operations in TensorFlow can be tricked into accessing invalid memory, via a heap buffer overflow. This occurs because the ranks of the `input`, `input_h` and `input_c` parameters are not validated, but code assumes they have certain values. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-41221", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-05T23:15:08.413", "lastModified": "2021-11-10T13:19:23.930", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. In affected versions the shape inference code for the `Cudnn*` operations in TensorFlow can be tricked into accessing invalid memory, via a heap buffer overflow. This occurs because the ranks of the `input`, `input_h` and `input_c` parameters are not validated, but code assumes they have certain values. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En las versiones afectadas, el c\u00f3digo de inferencia de formas para las operaciones \"Cudnn*\" en TensorFlow puede ser enga\u00f1ado para acceder a memoria no v\u00e1lida, por medio de un desbordamiento del buffer de la pila. Esto ocurre porque los rangos de los par\u00e1metros \"input\", \"input_h\" y \"input_c\" no son comprobados, sino que el c\u00f3digo asume que presentan ciertos valores. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.7.0. Tambi\u00e9n ser\u00e1 incluida este commit en TensorFlow versi\u00f3n 2.6.1, TensorFlow versi\u00f3n 2.5.2, y TensorFlow versi\u00f3n 2.4.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "security-advisories@github.com", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-787"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-120"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.4", "matchCriteriaId": "0E596567-6F67-4880-8EC4-CB262BF02E0D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.5.0", "versionEndExcluding": "2.5.2", "matchCriteriaId": "035CDF63-1548-4FB4-B8A9-B8D328FAF910"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndExcluding": "2.6.1", "matchCriteriaId": "5D68D8D1-DB27-4395-9D3D-2BED901B852C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "A58EDA5C-66D6-46F1-962E-60AFB7C784A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "89522760-C2DF-400D-9624-626D8F160CBA"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/af5fcebb37c8b5d71c237f4e59c6477015c78ce6", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-cqv6-3phm-hcwx", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/af5fcebb37c8b5d71c237f4e59c6477015c78ce6"}}