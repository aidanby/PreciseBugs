{"buggy_code": ["/*\n *\tlinux/mm/mlock.c\n *\n *  (C) Copyright 1995 Linus Torvalds\n *  (C) Copyright 2002 Christoph Hellwig\n */\n\n#include <linux/capability.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/sched/user.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/pagemap.h>\n#include <linux/pagevec.h>\n#include <linux/mempolicy.h>\n#include <linux/syscalls.h>\n#include <linux/sched.h>\n#include <linux/export.h>\n#include <linux/rmap.h>\n#include <linux/mmzone.h>\n#include <linux/hugetlb.h>\n#include <linux/memcontrol.h>\n#include <linux/mm_inline.h>\n\n#include \"internal.h\"\n\nbool can_do_mlock(void)\n{\n\tif (rlimit(RLIMIT_MEMLOCK) != 0)\n\t\treturn true;\n\tif (capable(CAP_IPC_LOCK))\n\t\treturn true;\n\treturn false;\n}\nEXPORT_SYMBOL(can_do_mlock);\n\n/*\n * Mlocked pages are marked with PageMlocked() flag for efficient testing\n * in vmscan and, possibly, the fault path; and to support semi-accurate\n * statistics.\n *\n * An mlocked page [PageMlocked(page)] is unevictable.  As such, it will\n * be placed on the LRU \"unevictable\" list, rather than the [in]active lists.\n * The unevictable list is an LRU sibling list to the [in]active lists.\n * PageUnevictable is set to indicate the unevictable state.\n *\n * When lazy mlocking via vmscan, it is important to ensure that the\n * vma's VM_LOCKED status is not concurrently being modified, otherwise we\n * may have mlocked a page that is being munlocked. So lazy mlock must take\n * the mmap_sem for read, and verify that the vma really is locked\n * (see mm/rmap.c).\n */\n\n/*\n *  LRU accounting for clear_page_mlock()\n */\nvoid clear_page_mlock(struct page *page)\n{\n\tif (!TestClearPageMlocked(page))\n\t\treturn;\n\n\tmod_zone_page_state(page_zone(page), NR_MLOCK,\n\t\t\t    -hpage_nr_pages(page));\n\tcount_vm_event(UNEVICTABLE_PGCLEARED);\n\tif (!isolate_lru_page(page)) {\n\t\tputback_lru_page(page);\n\t} else {\n\t\t/*\n\t\t * We lost the race. the page already moved to evictable list.\n\t\t */\n\t\tif (PageUnevictable(page))\n\t\t\tcount_vm_event(UNEVICTABLE_PGSTRANDED);\n\t}\n}\n\n/*\n * Mark page as mlocked if not already.\n * If page on LRU, isolate and putback to move to unevictable list.\n */\nvoid mlock_vma_page(struct page *page)\n{\n\t/* Serialize with page migration */\n\tBUG_ON(!PageLocked(page));\n\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\tVM_BUG_ON_PAGE(PageCompound(page) && PageDoubleMap(page), page);\n\n\tif (!TestSetPageMlocked(page)) {\n\t\tmod_zone_page_state(page_zone(page), NR_MLOCK,\n\t\t\t\t    hpage_nr_pages(page));\n\t\tcount_vm_event(UNEVICTABLE_PGMLOCKED);\n\t\tif (!isolate_lru_page(page))\n\t\t\tputback_lru_page(page);\n\t}\n}\n\n/*\n * Isolate a page from LRU with optional get_page() pin.\n * Assumes lru_lock already held and page already pinned.\n */\nstatic bool __munlock_isolate_lru_page(struct page *page, bool getpage)\n{\n\tif (PageLRU(page)) {\n\t\tstruct lruvec *lruvec;\n\n\t\tlruvec = mem_cgroup_page_lruvec(page, page_pgdat(page));\n\t\tif (getpage)\n\t\t\tget_page(page);\n\t\tClearPageLRU(page);\n\t\tdel_page_from_lru_list(page, lruvec, page_lru(page));\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Finish munlock after successful page isolation\n *\n * Page must be locked. This is a wrapper for try_to_munlock()\n * and putback_lru_page() with munlock accounting.\n */\nstatic void __munlock_isolated_page(struct page *page)\n{\n\t/*\n\t * Optimization: if the page was mapped just once, that's our mapping\n\t * and we don't need to check all the other vmas.\n\t */\n\tif (page_mapcount(page) > 1)\n\t\ttry_to_munlock(page);\n\n\t/* Did try_to_unlock() succeed or punt? */\n\tif (!PageMlocked(page))\n\t\tcount_vm_event(UNEVICTABLE_PGMUNLOCKED);\n\n\tputback_lru_page(page);\n}\n\n/*\n * Accounting for page isolation fail during munlock\n *\n * Performs accounting when page isolation fails in munlock. There is nothing\n * else to do because it means some other task has already removed the page\n * from the LRU. putback_lru_page() will take care of removing the page from\n * the unevictable list, if necessary. vmscan [page_referenced()] will move\n * the page back to the unevictable list if some other vma has it mlocked.\n */\nstatic void __munlock_isolation_failed(struct page *page)\n{\n\tif (PageUnevictable(page))\n\t\t__count_vm_event(UNEVICTABLE_PGSTRANDED);\n\telse\n\t\t__count_vm_event(UNEVICTABLE_PGMUNLOCKED);\n}\n\n/**\n * munlock_vma_page - munlock a vma page\n * @page - page to be unlocked, either a normal page or THP page head\n *\n * returns the size of the page as a page mask (0 for normal page,\n *         HPAGE_PMD_NR - 1 for THP head page)\n *\n * called from munlock()/munmap() path with page supposedly on the LRU.\n * When we munlock a page, because the vma where we found the page is being\n * munlock()ed or munmap()ed, we want to check whether other vmas hold the\n * page locked so that we can leave it on the unevictable lru list and not\n * bother vmscan with it.  However, to walk the page's rmap list in\n * try_to_munlock() we must isolate the page from the LRU.  If some other\n * task has removed the page from the LRU, we won't be able to do that.\n * So we clear the PageMlocked as we might not get another chance.  If we\n * can't isolate the page, we leave it for putback_lru_page() and vmscan\n * [page_referenced()/try_to_unmap()] to deal with.\n */\nunsigned int munlock_vma_page(struct page *page)\n{\n\tint nr_pages;\n\tstruct zone *zone = page_zone(page);\n\n\t/* For try_to_munlock() and to serialize with page migration */\n\tBUG_ON(!PageLocked(page));\n\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\n\t/*\n\t * Serialize with any parallel __split_huge_page_refcount() which\n\t * might otherwise copy PageMlocked to part of the tail pages before\n\t * we clear it in the head page. It also stabilizes hpage_nr_pages().\n\t */\n\tspin_lock_irq(zone_lru_lock(zone));\n\n\tif (!TestClearPageMlocked(page)) {\n\t\t/* Potentially, PTE-mapped THP: do not skip the rest PTEs */\n\t\tnr_pages = 1;\n\t\tgoto unlock_out;\n\t}\n\n\tnr_pages = hpage_nr_pages(page);\n\t__mod_zone_page_state(zone, NR_MLOCK, -nr_pages);\n\n\tif (__munlock_isolate_lru_page(page, true)) {\n\t\tspin_unlock_irq(zone_lru_lock(zone));\n\t\t__munlock_isolated_page(page);\n\t\tgoto out;\n\t}\n\t__munlock_isolation_failed(page);\n\nunlock_out:\n\tspin_unlock_irq(zone_lru_lock(zone));\n\nout:\n\treturn nr_pages - 1;\n}\n\n/*\n * convert get_user_pages() return value to posix mlock() error\n */\nstatic int __mlock_posix_error_return(long retval)\n{\n\tif (retval == -EFAULT)\n\t\tretval = -ENOMEM;\n\telse if (retval == -ENOMEM)\n\t\tretval = -EAGAIN;\n\treturn retval;\n}\n\n/*\n * Prepare page for fast batched LRU putback via putback_lru_evictable_pagevec()\n *\n * The fast path is available only for evictable pages with single mapping.\n * Then we can bypass the per-cpu pvec and get better performance.\n * when mapcount > 1 we need try_to_munlock() which can fail.\n * when !page_evictable(), we need the full redo logic of putback_lru_page to\n * avoid leaving evictable page in unevictable list.\n *\n * In case of success, @page is added to @pvec and @pgrescued is incremented\n * in case that the page was previously unevictable. @page is also unlocked.\n */\nstatic bool __putback_lru_fast_prepare(struct page *page, struct pagevec *pvec,\n\t\tint *pgrescued)\n{\n\tVM_BUG_ON_PAGE(PageLRU(page), page);\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\n\tif (page_mapcount(page) <= 1 && page_evictable(page)) {\n\t\tpagevec_add(pvec, page);\n\t\tif (TestClearPageUnevictable(page))\n\t\t\t(*pgrescued)++;\n\t\tunlock_page(page);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Putback multiple evictable pages to the LRU\n *\n * Batched putback of evictable pages that bypasses the per-cpu pvec. Some of\n * the pages might have meanwhile become unevictable but that is OK.\n */\nstatic void __putback_lru_fast(struct pagevec *pvec, int pgrescued)\n{\n\tcount_vm_events(UNEVICTABLE_PGMUNLOCKED, pagevec_count(pvec));\n\t/*\n\t *__pagevec_lru_add() calls release_pages() so we don't call\n\t * put_page() explicitly\n\t */\n\t__pagevec_lru_add(pvec);\n\tcount_vm_events(UNEVICTABLE_PGRESCUED, pgrescued);\n}\n\n/*\n * Munlock a batch of pages from the same zone\n *\n * The work is split to two main phases. First phase clears the Mlocked flag\n * and attempts to isolate the pages, all under a single zone lru lock.\n * The second phase finishes the munlock only for pages where isolation\n * succeeded.\n *\n * Note that the pagevec may be modified during the process.\n */\nstatic void __munlock_pagevec(struct pagevec *pvec, struct zone *zone)\n{\n\tint i;\n\tint nr = pagevec_count(pvec);\n\tint delta_munlocked;\n\tstruct pagevec pvec_putback;\n\tint pgrescued = 0;\n\n\tpagevec_init(&pvec_putback, 0);\n\n\t/* Phase 1: page isolation */\n\tspin_lock_irq(zone_lru_lock(zone));\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (TestClearPageMlocked(page)) {\n\t\t\t/*\n\t\t\t * We already have pin from follow_page_mask()\n\t\t\t * so we can spare the get_page() here.\n\t\t\t */\n\t\t\tif (__munlock_isolate_lru_page(page, false))\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\t__munlock_isolation_failed(page);\n\t\t}\n\n\t\t/*\n\t\t * We won't be munlocking this page in the next phase\n\t\t * but we still need to release the follow_page_mask()\n\t\t * pin. We cannot do it under lru_lock however. If it's\n\t\t * the last pin, __page_cache_release() would deadlock.\n\t\t */\n\t\tpagevec_add(&pvec_putback, pvec->pages[i]);\n\t\tpvec->pages[i] = NULL;\n\t}\n\tdelta_munlocked = -nr + pagevec_count(&pvec_putback);\n\t__mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);\n\tspin_unlock_irq(zone_lru_lock(zone));\n\n\t/* Now we can release pins of pages that we are not munlocking */\n\tpagevec_release(&pvec_putback);\n\n\t/* Phase 2: page munlock */\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (page) {\n\t\t\tlock_page(page);\n\t\t\tif (!__putback_lru_fast_prepare(page, &pvec_putback,\n\t\t\t\t\t&pgrescued)) {\n\t\t\t\t/*\n\t\t\t\t * Slow path. We don't want to lose the last\n\t\t\t\t * pin before unlock_page()\n\t\t\t\t */\n\t\t\t\tget_page(page); /* for putback_lru_page() */\n\t\t\t\t__munlock_isolated_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* from follow_page_mask() */\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Phase 3: page putback for pages that qualified for the fast path\n\t * This will also call put_page() to return pin from follow_page_mask()\n\t */\n\tif (pagevec_count(&pvec_putback))\n\t\t__putback_lru_fast(&pvec_putback, pgrescued);\n}\n\n/*\n * Fill up pagevec for __munlock_pagevec using pte walk\n *\n * The function expects that the struct page corresponding to @start address is\n * a non-TPH page already pinned and in the @pvec, and that it belongs to @zone.\n *\n * The rest of @pvec is filled by subsequent pages within the same pmd and same\n * zone, as long as the pte's are present and vm_normal_page() succeeds. These\n * pages also get pinned.\n *\n * Returns the address of the next page that should be scanned. This equals\n * @start + PAGE_SIZE when no page could be added by the pte walk.\n */\nstatic unsigned long __munlock_pagevec_fill(struct pagevec *pvec,\n\t\tstruct vm_area_struct *vma, int zoneid,\tunsigned long start,\n\t\tunsigned long end)\n{\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\t/*\n\t * Initialize pte walk starting at the already pinned page where we\n\t * are sure that there is a pte, as it was pinned under the same\n\t * mmap_sem write op.\n\t */\n\tpte = get_locked_pte(vma->vm_mm, start,\t&ptl);\n\t/* Make sure we do not cross the page table boundary */\n\tend = pgd_addr_end(start, end);\n\tend = p4d_addr_end(start, end);\n\tend = pud_addr_end(start, end);\n\tend = pmd_addr_end(start, end);\n\n\t/* The page next to the pinned page is the first we will try to get */\n\tstart += PAGE_SIZE;\n\twhile (start < end) {\n\t\tstruct page *page = NULL;\n\t\tpte++;\n\t\tif (pte_present(*pte))\n\t\t\tpage = vm_normal_page(vma, start, *pte);\n\t\t/*\n\t\t * Break if page could not be obtained or the page's node+zone does not\n\t\t * match\n\t\t */\n\t\tif (!page || page_zone_id(page) != zoneid)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Do not use pagevec for PTE-mapped THP,\n\t\t * munlock_vma_pages_range() will handle them.\n\t\t */\n\t\tif (PageTransCompound(page))\n\t\t\tbreak;\n\n\t\tget_page(page);\n\t\t/*\n\t\t * Increase the address that will be returned *before* the\n\t\t * eventual break due to pvec becoming full by adding the page\n\t\t */\n\t\tstart += PAGE_SIZE;\n\t\tif (pagevec_add(pvec, page) == 0)\n\t\t\tbreak;\n\t}\n\tpte_unmap_unlock(pte, ptl);\n\treturn start;\n}\n\n/*\n * munlock_vma_pages_range() - munlock all pages in the vma range.'\n * @vma - vma containing range to be munlock()ed.\n * @start - start address in @vma of the range\n * @end - end of range in @vma.\n *\n *  For mremap(), munmap() and exit().\n *\n * Called with @vma VM_LOCKED.\n *\n * Returns with VM_LOCKED cleared.  Callers must be prepared to\n * deal with this.\n *\n * We don't save and restore VM_LOCKED here because pages are\n * still on lru.  In unmap path, pages might be scanned by reclaim\n * and re-mlocked by try_to_{munlock|unmap} before we unmap and\n * free them.  This will result in freeing mlocked pages.\n */\nvoid munlock_vma_pages_range(struct vm_area_struct *vma,\n\t\t\t     unsigned long start, unsigned long end)\n{\n\tvma->vm_flags &= VM_LOCKED_CLEAR_MASK;\n\n\twhile (start < end) {\n\t\tstruct page *page;\n\t\tunsigned int page_mask = 0;\n\t\tunsigned long page_increm;\n\t\tstruct pagevec pvec;\n\t\tstruct zone *zone;\n\t\tint zoneid;\n\n\t\tpagevec_init(&pvec, 0);\n\t\t/*\n\t\t * Although FOLL_DUMP is intended for get_dump_page(),\n\t\t * it just so happens that its special treatment of the\n\t\t * ZERO_PAGE (returning an error instead of doing get_page)\n\t\t * suits munlock very well (and if somehow an abnormal page\n\t\t * has sneaked into the range, we won't oops here: great).\n\t\t */\n\t\tpage = follow_page(vma, start, FOLL_GET | FOLL_DUMP);\n\n\t\tif (page && !IS_ERR(page)) {\n\t\t\tif (PageTransTail(page)) {\n\t\t\t\tVM_BUG_ON_PAGE(PageMlocked(page), page);\n\t\t\t\tput_page(page); /* follow_page_mask() */\n\t\t\t} else if (PageTransHuge(page)) {\n\t\t\t\tlock_page(page);\n\t\t\t\t/*\n\t\t\t\t * Any THP page found by follow_page_mask() may\n\t\t\t\t * have gotten split before reaching\n\t\t\t\t * munlock_vma_page(), so we need to compute\n\t\t\t\t * the page_mask here instead.\n\t\t\t\t */\n\t\t\t\tpage_mask = munlock_vma_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* follow_page_mask() */\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * Non-huge pages are handled in batches via\n\t\t\t\t * pagevec. The pin from follow_page_mask()\n\t\t\t\t * prevents them from collapsing by THP.\n\t\t\t\t */\n\t\t\t\tpagevec_add(&pvec, page);\n\t\t\t\tzone = page_zone(page);\n\t\t\t\tzoneid = page_zone_id(page);\n\n\t\t\t\t/*\n\t\t\t\t * Try to fill the rest of pagevec using fast\n\t\t\t\t * pte walk. This will also update start to\n\t\t\t\t * the next page to process. Then munlock the\n\t\t\t\t * pagevec.\n\t\t\t\t */\n\t\t\t\tstart = __munlock_pagevec_fill(&pvec, vma,\n\t\t\t\t\t\tzoneid, start, end);\n\t\t\t\t__munlock_pagevec(&pvec, zone);\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t}\n\t\tpage_increm = 1 + page_mask;\n\t\tstart += page_increm * PAGE_SIZE;\nnext:\n\t\tcond_resched();\n\t}\n}\n\n/*\n * mlock_fixup  - handle mlock[all]/munlock[all] requests.\n *\n * Filters out \"special\" vmas -- VM_LOCKED never gets set for these, and\n * munlock is a no-op.  However, for some special vmas, we go ahead and\n * populate the ptes.\n *\n * For vmas that pass the filters, merge/split as appropriate.\n */\nstatic int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,\n\tunsigned long start, unsigned long end, vm_flags_t newflags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpgoff_t pgoff;\n\tint nr_pages;\n\tint ret = 0;\n\tint lock = !!(newflags & VM_LOCKED);\n\tvm_flags_t old_flags = vma->vm_flags;\n\n\tif (newflags == vma->vm_flags || (vma->vm_flags & VM_SPECIAL) ||\n\t    is_vm_hugetlb_page(vma) || vma == get_gate_vma(current->mm))\n\t\t/* don't set VM_LOCKED or VM_LOCKONFAULT and don't count */\n\t\tgoto out;\n\n\tpgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\n\t*prev = vma_merge(mm, *prev, start, end, newflags, vma->anon_vma,\n\t\t\t  vma->vm_file, pgoff, vma_policy(vma),\n\t\t\t  vma->vm_userfaultfd_ctx);\n\tif (*prev) {\n\t\tvma = *prev;\n\t\tgoto success;\n\t}\n\n\tif (start != vma->vm_start) {\n\t\tret = split_vma(mm, vma, start, 1);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (end != vma->vm_end) {\n\t\tret = split_vma(mm, vma, end, 0);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nsuccess:\n\t/*\n\t * Keep track of amount of locked VM.\n\t */\n\tnr_pages = (end - start) >> PAGE_SHIFT;\n\tif (!lock)\n\t\tnr_pages = -nr_pages;\n\telse if (old_flags & VM_LOCKED)\n\t\tnr_pages = 0;\n\tmm->locked_vm += nr_pages;\n\n\t/*\n\t * vm_flags is protected by the mmap_sem held in write mode.\n\t * It's okay if try_to_unmap_one unmaps a page just after we\n\t * set VM_LOCKED, populate_vma_page_range will bring it back.\n\t */\n\n\tif (lock)\n\t\tvma->vm_flags = newflags;\n\telse\n\t\tmunlock_vma_pages_range(vma, start, end);\n\nout:\n\t*prev = vma;\n\treturn ret;\n}\n\nstatic int apply_vma_lock_flags(unsigned long start, size_t len,\n\t\t\t\tvm_flags_t flags)\n{\n\tunsigned long nstart, end, tmp;\n\tstruct vm_area_struct * vma, * prev;\n\tint error;\n\n\tVM_BUG_ON(offset_in_page(start));\n\tVM_BUG_ON(len != PAGE_ALIGN(len));\n\tend = start + len;\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\tvma = find_vma(current->mm, start);\n\tif (!vma || vma->vm_start > start)\n\t\treturn -ENOMEM;\n\n\tprev = vma->vm_prev;\n\tif (start > vma->vm_start)\n\t\tprev = vma;\n\n\tfor (nstart = start ; ; ) {\n\t\tvm_flags_t newflags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;\n\n\t\tnewflags |= flags;\n\n\t\t/* Here we know that  vma->vm_start <= nstart < vma->vm_end. */\n\t\ttmp = vma->vm_end;\n\t\tif (tmp > end)\n\t\t\ttmp = end;\n\t\terror = mlock_fixup(vma, &prev, nstart, tmp, newflags);\n\t\tif (error)\n\t\t\tbreak;\n\t\tnstart = tmp;\n\t\tif (nstart < prev->vm_end)\n\t\t\tnstart = prev->vm_end;\n\t\tif (nstart >= end)\n\t\t\tbreak;\n\n\t\tvma = prev->vm_next;\n\t\tif (!vma || vma->vm_start != nstart) {\n\t\t\terror = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn error;\n}\n\n/*\n * Go through vma areas and sum size of mlocked\n * vma pages, as return value.\n * Note deferred memory locking case(mlock2(,,MLOCK_ONFAULT)\n * is also counted.\n * Return value: previously mlocked page counts\n */\nstatic int count_mm_mlocked_page_nr(struct mm_struct *mm,\n\t\tunsigned long start, size_t len)\n{\n\tstruct vm_area_struct *vma;\n\tint count = 0;\n\n\tif (mm == NULL)\n\t\tmm = current->mm;\n\n\tvma = find_vma(mm, start);\n\tif (vma == NULL)\n\t\tvma = mm->mmap;\n\n\tfor (; vma ; vma = vma->vm_next) {\n\t\tif (start >= vma->vm_end)\n\t\t\tcontinue;\n\t\tif (start + len <=  vma->vm_start)\n\t\t\tbreak;\n\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\tif (start > vma->vm_start)\n\t\t\t\tcount -= (start - vma->vm_start);\n\t\t\tif (start + len < vma->vm_end) {\n\t\t\t\tcount += start + len - vma->vm_start;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcount += vma->vm_end - vma->vm_start;\n\t\t}\n\t}\n\n\treturn count >> PAGE_SHIFT;\n}\n\nstatic __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t flags)\n{\n\tunsigned long locked;\n\tunsigned long lock_limit;\n\tint error = -ENOMEM;\n\n\tif (!can_do_mlock())\n\t\treturn -EPERM;\n\n\tlru_add_drain_all();\t/* flush pagevec */\n\n\tlen = PAGE_ALIGN(len + (offset_in_page(start)));\n\tstart &= PAGE_MASK;\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tlock_limit >>= PAGE_SHIFT;\n\tlocked = len >> PAGE_SHIFT;\n\n\tif (down_write_killable(&current->mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tlocked += current->mm->locked_vm;\n\tif ((locked > lock_limit) && (!capable(CAP_IPC_LOCK))) {\n\t\t/*\n\t\t * It is possible that the regions requested intersect with\n\t\t * previously mlocked areas, that part area in \"mm->locked_vm\"\n\t\t * should not be counted to new mlock increment count. So check\n\t\t * and adjust locked count if necessary.\n\t\t */\n\t\tlocked -= count_mm_mlocked_page_nr(current->mm,\n\t\t\t\tstart, len);\n\t}\n\n\t/* check against resource limits */\n\tif ((locked <= lock_limit) || capable(CAP_IPC_LOCK))\n\t\terror = apply_vma_lock_flags(start, len, flags);\n\n\tup_write(&current->mm->mmap_sem);\n\tif (error)\n\t\treturn error;\n\n\terror = __mm_populate(start, len, 0);\n\tif (error)\n\t\treturn __mlock_posix_error_return(error);\n\treturn 0;\n}\n\nSYSCALL_DEFINE2(mlock, unsigned long, start, size_t, len)\n{\n\treturn do_mlock(start, len, VM_LOCKED);\n}\n\nSYSCALL_DEFINE3(mlock2, unsigned long, start, size_t, len, int, flags)\n{\n\tvm_flags_t vm_flags = VM_LOCKED;\n\n\tif (flags & ~MLOCK_ONFAULT)\n\t\treturn -EINVAL;\n\n\tif (flags & MLOCK_ONFAULT)\n\t\tvm_flags |= VM_LOCKONFAULT;\n\n\treturn do_mlock(start, len, vm_flags);\n}\n\nSYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)\n{\n\tint ret;\n\n\tlen = PAGE_ALIGN(len + (offset_in_page(start)));\n\tstart &= PAGE_MASK;\n\n\tif (down_write_killable(&current->mm->mmap_sem))\n\t\treturn -EINTR;\n\tret = apply_vma_lock_flags(start, len, 0);\n\tup_write(&current->mm->mmap_sem);\n\n\treturn ret;\n}\n\n/*\n * Take the MCL_* flags passed into mlockall (or 0 if called from munlockall)\n * and translate into the appropriate modifications to mm->def_flags and/or the\n * flags for all current VMAs.\n *\n * There are a couple of subtleties with this.  If mlockall() is called multiple\n * times with different flags, the values do not necessarily stack.  If mlockall\n * is called once including the MCL_FUTURE flag and then a second time without\n * it, VM_LOCKED and VM_LOCKONFAULT will be cleared from mm->def_flags.\n */\nstatic int apply_mlockall_flags(int flags)\n{\n\tstruct vm_area_struct * vma, * prev = NULL;\n\tvm_flags_t to_add = 0;\n\n\tcurrent->mm->def_flags &= VM_LOCKED_CLEAR_MASK;\n\tif (flags & MCL_FUTURE) {\n\t\tcurrent->mm->def_flags |= VM_LOCKED;\n\n\t\tif (flags & MCL_ONFAULT)\n\t\t\tcurrent->mm->def_flags |= VM_LOCKONFAULT;\n\n\t\tif (!(flags & MCL_CURRENT))\n\t\t\tgoto out;\n\t}\n\n\tif (flags & MCL_CURRENT) {\n\t\tto_add |= VM_LOCKED;\n\t\tif (flags & MCL_ONFAULT)\n\t\t\tto_add |= VM_LOCKONFAULT;\n\t}\n\n\tfor (vma = current->mm->mmap; vma ; vma = prev->vm_next) {\n\t\tvm_flags_t newflags;\n\n\t\tnewflags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;\n\t\tnewflags |= to_add;\n\n\t\t/* Ignore errors */\n\t\tmlock_fixup(vma, &prev, vma->vm_start, vma->vm_end, newflags);\n\t\tcond_resched_rcu_qs();\n\t}\nout:\n\treturn 0;\n}\n\nSYSCALL_DEFINE1(mlockall, int, flags)\n{\n\tunsigned long lock_limit;\n\tint ret;\n\n\tif (!flags || (flags & ~(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT)))\n\t\treturn -EINVAL;\n\n\tif (!can_do_mlock())\n\t\treturn -EPERM;\n\n\tif (flags & MCL_CURRENT)\n\t\tlru_add_drain_all();\t/* flush pagevec */\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tlock_limit >>= PAGE_SHIFT;\n\n\tif (down_write_killable(&current->mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tret = -ENOMEM;\n\tif (!(flags & MCL_CURRENT) || (current->mm->total_vm <= lock_limit) ||\n\t    capable(CAP_IPC_LOCK))\n\t\tret = apply_mlockall_flags(flags);\n\tup_write(&current->mm->mmap_sem);\n\tif (!ret && (flags & MCL_CURRENT))\n\t\tmm_populate(0, TASK_SIZE);\n\n\treturn ret;\n}\n\nSYSCALL_DEFINE0(munlockall)\n{\n\tint ret;\n\n\tif (down_write_killable(&current->mm->mmap_sem))\n\t\treturn -EINTR;\n\tret = apply_mlockall_flags(0);\n\tup_write(&current->mm->mmap_sem);\n\treturn ret;\n}\n\n/*\n * Objects with different lifetime than processes (SHM_LOCK and SHM_HUGETLB\n * shm segments) get accounted against the user_struct instead.\n */\nstatic DEFINE_SPINLOCK(shmlock_user_lock);\n\nint user_shm_lock(size_t size, struct user_struct *user)\n{\n\tunsigned long lock_limit, locked;\n\tint allowed = 0;\n\n\tlocked = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tif (lock_limit == RLIM_INFINITY)\n\t\tallowed = 1;\n\tlock_limit >>= PAGE_SHIFT;\n\tspin_lock(&shmlock_user_lock);\n\tif (!allowed &&\n\t    locked + user->locked_shm > lock_limit && !capable(CAP_IPC_LOCK))\n\t\tgoto out;\n\tget_uid(user);\n\tuser->locked_shm += locked;\n\tallowed = 1;\nout:\n\tspin_unlock(&shmlock_user_lock);\n\treturn allowed;\n}\n\nvoid user_shm_unlock(size_t size, struct user_struct *user)\n{\n\tspin_lock(&shmlock_user_lock);\n\tuser->locked_shm -= (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tspin_unlock(&shmlock_user_lock);\n\tfree_uid(user);\n}\n"], "fixing_code": ["/*\n *\tlinux/mm/mlock.c\n *\n *  (C) Copyright 1995 Linus Torvalds\n *  (C) Copyright 2002 Christoph Hellwig\n */\n\n#include <linux/capability.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/sched/user.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n#include <linux/pagemap.h>\n#include <linux/pagevec.h>\n#include <linux/mempolicy.h>\n#include <linux/syscalls.h>\n#include <linux/sched.h>\n#include <linux/export.h>\n#include <linux/rmap.h>\n#include <linux/mmzone.h>\n#include <linux/hugetlb.h>\n#include <linux/memcontrol.h>\n#include <linux/mm_inline.h>\n\n#include \"internal.h\"\n\nbool can_do_mlock(void)\n{\n\tif (rlimit(RLIMIT_MEMLOCK) != 0)\n\t\treturn true;\n\tif (capable(CAP_IPC_LOCK))\n\t\treturn true;\n\treturn false;\n}\nEXPORT_SYMBOL(can_do_mlock);\n\n/*\n * Mlocked pages are marked with PageMlocked() flag for efficient testing\n * in vmscan and, possibly, the fault path; and to support semi-accurate\n * statistics.\n *\n * An mlocked page [PageMlocked(page)] is unevictable.  As such, it will\n * be placed on the LRU \"unevictable\" list, rather than the [in]active lists.\n * The unevictable list is an LRU sibling list to the [in]active lists.\n * PageUnevictable is set to indicate the unevictable state.\n *\n * When lazy mlocking via vmscan, it is important to ensure that the\n * vma's VM_LOCKED status is not concurrently being modified, otherwise we\n * may have mlocked a page that is being munlocked. So lazy mlock must take\n * the mmap_sem for read, and verify that the vma really is locked\n * (see mm/rmap.c).\n */\n\n/*\n *  LRU accounting for clear_page_mlock()\n */\nvoid clear_page_mlock(struct page *page)\n{\n\tif (!TestClearPageMlocked(page))\n\t\treturn;\n\n\tmod_zone_page_state(page_zone(page), NR_MLOCK,\n\t\t\t    -hpage_nr_pages(page));\n\tcount_vm_event(UNEVICTABLE_PGCLEARED);\n\tif (!isolate_lru_page(page)) {\n\t\tputback_lru_page(page);\n\t} else {\n\t\t/*\n\t\t * We lost the race. the page already moved to evictable list.\n\t\t */\n\t\tif (PageUnevictable(page))\n\t\t\tcount_vm_event(UNEVICTABLE_PGSTRANDED);\n\t}\n}\n\n/*\n * Mark page as mlocked if not already.\n * If page on LRU, isolate and putback to move to unevictable list.\n */\nvoid mlock_vma_page(struct page *page)\n{\n\t/* Serialize with page migration */\n\tBUG_ON(!PageLocked(page));\n\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\tVM_BUG_ON_PAGE(PageCompound(page) && PageDoubleMap(page), page);\n\n\tif (!TestSetPageMlocked(page)) {\n\t\tmod_zone_page_state(page_zone(page), NR_MLOCK,\n\t\t\t\t    hpage_nr_pages(page));\n\t\tcount_vm_event(UNEVICTABLE_PGMLOCKED);\n\t\tif (!isolate_lru_page(page))\n\t\t\tputback_lru_page(page);\n\t}\n}\n\n/*\n * Isolate a page from LRU with optional get_page() pin.\n * Assumes lru_lock already held and page already pinned.\n */\nstatic bool __munlock_isolate_lru_page(struct page *page, bool getpage)\n{\n\tif (PageLRU(page)) {\n\t\tstruct lruvec *lruvec;\n\n\t\tlruvec = mem_cgroup_page_lruvec(page, page_pgdat(page));\n\t\tif (getpage)\n\t\t\tget_page(page);\n\t\tClearPageLRU(page);\n\t\tdel_page_from_lru_list(page, lruvec, page_lru(page));\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Finish munlock after successful page isolation\n *\n * Page must be locked. This is a wrapper for try_to_munlock()\n * and putback_lru_page() with munlock accounting.\n */\nstatic void __munlock_isolated_page(struct page *page)\n{\n\t/*\n\t * Optimization: if the page was mapped just once, that's our mapping\n\t * and we don't need to check all the other vmas.\n\t */\n\tif (page_mapcount(page) > 1)\n\t\ttry_to_munlock(page);\n\n\t/* Did try_to_unlock() succeed or punt? */\n\tif (!PageMlocked(page))\n\t\tcount_vm_event(UNEVICTABLE_PGMUNLOCKED);\n\n\tputback_lru_page(page);\n}\n\n/*\n * Accounting for page isolation fail during munlock\n *\n * Performs accounting when page isolation fails in munlock. There is nothing\n * else to do because it means some other task has already removed the page\n * from the LRU. putback_lru_page() will take care of removing the page from\n * the unevictable list, if necessary. vmscan [page_referenced()] will move\n * the page back to the unevictable list if some other vma has it mlocked.\n */\nstatic void __munlock_isolation_failed(struct page *page)\n{\n\tif (PageUnevictable(page))\n\t\t__count_vm_event(UNEVICTABLE_PGSTRANDED);\n\telse\n\t\t__count_vm_event(UNEVICTABLE_PGMUNLOCKED);\n}\n\n/**\n * munlock_vma_page - munlock a vma page\n * @page - page to be unlocked, either a normal page or THP page head\n *\n * returns the size of the page as a page mask (0 for normal page,\n *         HPAGE_PMD_NR - 1 for THP head page)\n *\n * called from munlock()/munmap() path with page supposedly on the LRU.\n * When we munlock a page, because the vma where we found the page is being\n * munlock()ed or munmap()ed, we want to check whether other vmas hold the\n * page locked so that we can leave it on the unevictable lru list and not\n * bother vmscan with it.  However, to walk the page's rmap list in\n * try_to_munlock() we must isolate the page from the LRU.  If some other\n * task has removed the page from the LRU, we won't be able to do that.\n * So we clear the PageMlocked as we might not get another chance.  If we\n * can't isolate the page, we leave it for putback_lru_page() and vmscan\n * [page_referenced()/try_to_unmap()] to deal with.\n */\nunsigned int munlock_vma_page(struct page *page)\n{\n\tint nr_pages;\n\tstruct zone *zone = page_zone(page);\n\n\t/* For try_to_munlock() and to serialize with page migration */\n\tBUG_ON(!PageLocked(page));\n\n\tVM_BUG_ON_PAGE(PageTail(page), page);\n\n\t/*\n\t * Serialize with any parallel __split_huge_page_refcount() which\n\t * might otherwise copy PageMlocked to part of the tail pages before\n\t * we clear it in the head page. It also stabilizes hpage_nr_pages().\n\t */\n\tspin_lock_irq(zone_lru_lock(zone));\n\n\tif (!TestClearPageMlocked(page)) {\n\t\t/* Potentially, PTE-mapped THP: do not skip the rest PTEs */\n\t\tnr_pages = 1;\n\t\tgoto unlock_out;\n\t}\n\n\tnr_pages = hpage_nr_pages(page);\n\t__mod_zone_page_state(zone, NR_MLOCK, -nr_pages);\n\n\tif (__munlock_isolate_lru_page(page, true)) {\n\t\tspin_unlock_irq(zone_lru_lock(zone));\n\t\t__munlock_isolated_page(page);\n\t\tgoto out;\n\t}\n\t__munlock_isolation_failed(page);\n\nunlock_out:\n\tspin_unlock_irq(zone_lru_lock(zone));\n\nout:\n\treturn nr_pages - 1;\n}\n\n/*\n * convert get_user_pages() return value to posix mlock() error\n */\nstatic int __mlock_posix_error_return(long retval)\n{\n\tif (retval == -EFAULT)\n\t\tretval = -ENOMEM;\n\telse if (retval == -ENOMEM)\n\t\tretval = -EAGAIN;\n\treturn retval;\n}\n\n/*\n * Prepare page for fast batched LRU putback via putback_lru_evictable_pagevec()\n *\n * The fast path is available only for evictable pages with single mapping.\n * Then we can bypass the per-cpu pvec and get better performance.\n * when mapcount > 1 we need try_to_munlock() which can fail.\n * when !page_evictable(), we need the full redo logic of putback_lru_page to\n * avoid leaving evictable page in unevictable list.\n *\n * In case of success, @page is added to @pvec and @pgrescued is incremented\n * in case that the page was previously unevictable. @page is also unlocked.\n */\nstatic bool __putback_lru_fast_prepare(struct page *page, struct pagevec *pvec,\n\t\tint *pgrescued)\n{\n\tVM_BUG_ON_PAGE(PageLRU(page), page);\n\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\n\tif (page_mapcount(page) <= 1 && page_evictable(page)) {\n\t\tpagevec_add(pvec, page);\n\t\tif (TestClearPageUnevictable(page))\n\t\t\t(*pgrescued)++;\n\t\tunlock_page(page);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n/*\n * Putback multiple evictable pages to the LRU\n *\n * Batched putback of evictable pages that bypasses the per-cpu pvec. Some of\n * the pages might have meanwhile become unevictable but that is OK.\n */\nstatic void __putback_lru_fast(struct pagevec *pvec, int pgrescued)\n{\n\tcount_vm_events(UNEVICTABLE_PGMUNLOCKED, pagevec_count(pvec));\n\t/*\n\t *__pagevec_lru_add() calls release_pages() so we don't call\n\t * put_page() explicitly\n\t */\n\t__pagevec_lru_add(pvec);\n\tcount_vm_events(UNEVICTABLE_PGRESCUED, pgrescued);\n}\n\n/*\n * Munlock a batch of pages from the same zone\n *\n * The work is split to two main phases. First phase clears the Mlocked flag\n * and attempts to isolate the pages, all under a single zone lru lock.\n * The second phase finishes the munlock only for pages where isolation\n * succeeded.\n *\n * Note that the pagevec may be modified during the process.\n */\nstatic void __munlock_pagevec(struct pagevec *pvec, struct zone *zone)\n{\n\tint i;\n\tint nr = pagevec_count(pvec);\n\tint delta_munlocked = -nr;\n\tstruct pagevec pvec_putback;\n\tint pgrescued = 0;\n\n\tpagevec_init(&pvec_putback, 0);\n\n\t/* Phase 1: page isolation */\n\tspin_lock_irq(zone_lru_lock(zone));\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (TestClearPageMlocked(page)) {\n\t\t\t/*\n\t\t\t * We already have pin from follow_page_mask()\n\t\t\t * so we can spare the get_page() here.\n\t\t\t */\n\t\t\tif (__munlock_isolate_lru_page(page, false))\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\t__munlock_isolation_failed(page);\n\t\t} else {\n\t\t\tdelta_munlocked++;\n\t\t}\n\n\t\t/*\n\t\t * We won't be munlocking this page in the next phase\n\t\t * but we still need to release the follow_page_mask()\n\t\t * pin. We cannot do it under lru_lock however. If it's\n\t\t * the last pin, __page_cache_release() would deadlock.\n\t\t */\n\t\tpagevec_add(&pvec_putback, pvec->pages[i]);\n\t\tpvec->pages[i] = NULL;\n\t}\n\t__mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);\n\tspin_unlock_irq(zone_lru_lock(zone));\n\n\t/* Now we can release pins of pages that we are not munlocking */\n\tpagevec_release(&pvec_putback);\n\n\t/* Phase 2: page munlock */\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct page *page = pvec->pages[i];\n\n\t\tif (page) {\n\t\t\tlock_page(page);\n\t\t\tif (!__putback_lru_fast_prepare(page, &pvec_putback,\n\t\t\t\t\t&pgrescued)) {\n\t\t\t\t/*\n\t\t\t\t * Slow path. We don't want to lose the last\n\t\t\t\t * pin before unlock_page()\n\t\t\t\t */\n\t\t\t\tget_page(page); /* for putback_lru_page() */\n\t\t\t\t__munlock_isolated_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* from follow_page_mask() */\n\t\t\t}\n\t\t}\n\t}\n\n\t/*\n\t * Phase 3: page putback for pages that qualified for the fast path\n\t * This will also call put_page() to return pin from follow_page_mask()\n\t */\n\tif (pagevec_count(&pvec_putback))\n\t\t__putback_lru_fast(&pvec_putback, pgrescued);\n}\n\n/*\n * Fill up pagevec for __munlock_pagevec using pte walk\n *\n * The function expects that the struct page corresponding to @start address is\n * a non-TPH page already pinned and in the @pvec, and that it belongs to @zone.\n *\n * The rest of @pvec is filled by subsequent pages within the same pmd and same\n * zone, as long as the pte's are present and vm_normal_page() succeeds. These\n * pages also get pinned.\n *\n * Returns the address of the next page that should be scanned. This equals\n * @start + PAGE_SIZE when no page could be added by the pte walk.\n */\nstatic unsigned long __munlock_pagevec_fill(struct pagevec *pvec,\n\t\tstruct vm_area_struct *vma, int zoneid,\tunsigned long start,\n\t\tunsigned long end)\n{\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\t/*\n\t * Initialize pte walk starting at the already pinned page where we\n\t * are sure that there is a pte, as it was pinned under the same\n\t * mmap_sem write op.\n\t */\n\tpte = get_locked_pte(vma->vm_mm, start,\t&ptl);\n\t/* Make sure we do not cross the page table boundary */\n\tend = pgd_addr_end(start, end);\n\tend = p4d_addr_end(start, end);\n\tend = pud_addr_end(start, end);\n\tend = pmd_addr_end(start, end);\n\n\t/* The page next to the pinned page is the first we will try to get */\n\tstart += PAGE_SIZE;\n\twhile (start < end) {\n\t\tstruct page *page = NULL;\n\t\tpte++;\n\t\tif (pte_present(*pte))\n\t\t\tpage = vm_normal_page(vma, start, *pte);\n\t\t/*\n\t\t * Break if page could not be obtained or the page's node+zone does not\n\t\t * match\n\t\t */\n\t\tif (!page || page_zone_id(page) != zoneid)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Do not use pagevec for PTE-mapped THP,\n\t\t * munlock_vma_pages_range() will handle them.\n\t\t */\n\t\tif (PageTransCompound(page))\n\t\t\tbreak;\n\n\t\tget_page(page);\n\t\t/*\n\t\t * Increase the address that will be returned *before* the\n\t\t * eventual break due to pvec becoming full by adding the page\n\t\t */\n\t\tstart += PAGE_SIZE;\n\t\tif (pagevec_add(pvec, page) == 0)\n\t\t\tbreak;\n\t}\n\tpte_unmap_unlock(pte, ptl);\n\treturn start;\n}\n\n/*\n * munlock_vma_pages_range() - munlock all pages in the vma range.'\n * @vma - vma containing range to be munlock()ed.\n * @start - start address in @vma of the range\n * @end - end of range in @vma.\n *\n *  For mremap(), munmap() and exit().\n *\n * Called with @vma VM_LOCKED.\n *\n * Returns with VM_LOCKED cleared.  Callers must be prepared to\n * deal with this.\n *\n * We don't save and restore VM_LOCKED here because pages are\n * still on lru.  In unmap path, pages might be scanned by reclaim\n * and re-mlocked by try_to_{munlock|unmap} before we unmap and\n * free them.  This will result in freeing mlocked pages.\n */\nvoid munlock_vma_pages_range(struct vm_area_struct *vma,\n\t\t\t     unsigned long start, unsigned long end)\n{\n\tvma->vm_flags &= VM_LOCKED_CLEAR_MASK;\n\n\twhile (start < end) {\n\t\tstruct page *page;\n\t\tunsigned int page_mask = 0;\n\t\tunsigned long page_increm;\n\t\tstruct pagevec pvec;\n\t\tstruct zone *zone;\n\t\tint zoneid;\n\n\t\tpagevec_init(&pvec, 0);\n\t\t/*\n\t\t * Although FOLL_DUMP is intended for get_dump_page(),\n\t\t * it just so happens that its special treatment of the\n\t\t * ZERO_PAGE (returning an error instead of doing get_page)\n\t\t * suits munlock very well (and if somehow an abnormal page\n\t\t * has sneaked into the range, we won't oops here: great).\n\t\t */\n\t\tpage = follow_page(vma, start, FOLL_GET | FOLL_DUMP);\n\n\t\tif (page && !IS_ERR(page)) {\n\t\t\tif (PageTransTail(page)) {\n\t\t\t\tVM_BUG_ON_PAGE(PageMlocked(page), page);\n\t\t\t\tput_page(page); /* follow_page_mask() */\n\t\t\t} else if (PageTransHuge(page)) {\n\t\t\t\tlock_page(page);\n\t\t\t\t/*\n\t\t\t\t * Any THP page found by follow_page_mask() may\n\t\t\t\t * have gotten split before reaching\n\t\t\t\t * munlock_vma_page(), so we need to compute\n\t\t\t\t * the page_mask here instead.\n\t\t\t\t */\n\t\t\t\tpage_mask = munlock_vma_page(page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tput_page(page); /* follow_page_mask() */\n\t\t\t} else {\n\t\t\t\t/*\n\t\t\t\t * Non-huge pages are handled in batches via\n\t\t\t\t * pagevec. The pin from follow_page_mask()\n\t\t\t\t * prevents them from collapsing by THP.\n\t\t\t\t */\n\t\t\t\tpagevec_add(&pvec, page);\n\t\t\t\tzone = page_zone(page);\n\t\t\t\tzoneid = page_zone_id(page);\n\n\t\t\t\t/*\n\t\t\t\t * Try to fill the rest of pagevec using fast\n\t\t\t\t * pte walk. This will also update start to\n\t\t\t\t * the next page to process. Then munlock the\n\t\t\t\t * pagevec.\n\t\t\t\t */\n\t\t\t\tstart = __munlock_pagevec_fill(&pvec, vma,\n\t\t\t\t\t\tzoneid, start, end);\n\t\t\t\t__munlock_pagevec(&pvec, zone);\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t}\n\t\tpage_increm = 1 + page_mask;\n\t\tstart += page_increm * PAGE_SIZE;\nnext:\n\t\tcond_resched();\n\t}\n}\n\n/*\n * mlock_fixup  - handle mlock[all]/munlock[all] requests.\n *\n * Filters out \"special\" vmas -- VM_LOCKED never gets set for these, and\n * munlock is a no-op.  However, for some special vmas, we go ahead and\n * populate the ptes.\n *\n * For vmas that pass the filters, merge/split as appropriate.\n */\nstatic int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,\n\tunsigned long start, unsigned long end, vm_flags_t newflags)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpgoff_t pgoff;\n\tint nr_pages;\n\tint ret = 0;\n\tint lock = !!(newflags & VM_LOCKED);\n\tvm_flags_t old_flags = vma->vm_flags;\n\n\tif (newflags == vma->vm_flags || (vma->vm_flags & VM_SPECIAL) ||\n\t    is_vm_hugetlb_page(vma) || vma == get_gate_vma(current->mm))\n\t\t/* don't set VM_LOCKED or VM_LOCKONFAULT and don't count */\n\t\tgoto out;\n\n\tpgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\n\t*prev = vma_merge(mm, *prev, start, end, newflags, vma->anon_vma,\n\t\t\t  vma->vm_file, pgoff, vma_policy(vma),\n\t\t\t  vma->vm_userfaultfd_ctx);\n\tif (*prev) {\n\t\tvma = *prev;\n\t\tgoto success;\n\t}\n\n\tif (start != vma->vm_start) {\n\t\tret = split_vma(mm, vma, start, 1);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (end != vma->vm_end) {\n\t\tret = split_vma(mm, vma, end, 0);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nsuccess:\n\t/*\n\t * Keep track of amount of locked VM.\n\t */\n\tnr_pages = (end - start) >> PAGE_SHIFT;\n\tif (!lock)\n\t\tnr_pages = -nr_pages;\n\telse if (old_flags & VM_LOCKED)\n\t\tnr_pages = 0;\n\tmm->locked_vm += nr_pages;\n\n\t/*\n\t * vm_flags is protected by the mmap_sem held in write mode.\n\t * It's okay if try_to_unmap_one unmaps a page just after we\n\t * set VM_LOCKED, populate_vma_page_range will bring it back.\n\t */\n\n\tif (lock)\n\t\tvma->vm_flags = newflags;\n\telse\n\t\tmunlock_vma_pages_range(vma, start, end);\n\nout:\n\t*prev = vma;\n\treturn ret;\n}\n\nstatic int apply_vma_lock_flags(unsigned long start, size_t len,\n\t\t\t\tvm_flags_t flags)\n{\n\tunsigned long nstart, end, tmp;\n\tstruct vm_area_struct * vma, * prev;\n\tint error;\n\n\tVM_BUG_ON(offset_in_page(start));\n\tVM_BUG_ON(len != PAGE_ALIGN(len));\n\tend = start + len;\n\tif (end < start)\n\t\treturn -EINVAL;\n\tif (end == start)\n\t\treturn 0;\n\tvma = find_vma(current->mm, start);\n\tif (!vma || vma->vm_start > start)\n\t\treturn -ENOMEM;\n\n\tprev = vma->vm_prev;\n\tif (start > vma->vm_start)\n\t\tprev = vma;\n\n\tfor (nstart = start ; ; ) {\n\t\tvm_flags_t newflags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;\n\n\t\tnewflags |= flags;\n\n\t\t/* Here we know that  vma->vm_start <= nstart < vma->vm_end. */\n\t\ttmp = vma->vm_end;\n\t\tif (tmp > end)\n\t\t\ttmp = end;\n\t\terror = mlock_fixup(vma, &prev, nstart, tmp, newflags);\n\t\tif (error)\n\t\t\tbreak;\n\t\tnstart = tmp;\n\t\tif (nstart < prev->vm_end)\n\t\t\tnstart = prev->vm_end;\n\t\tif (nstart >= end)\n\t\t\tbreak;\n\n\t\tvma = prev->vm_next;\n\t\tif (!vma || vma->vm_start != nstart) {\n\t\t\terror = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn error;\n}\n\n/*\n * Go through vma areas and sum size of mlocked\n * vma pages, as return value.\n * Note deferred memory locking case(mlock2(,,MLOCK_ONFAULT)\n * is also counted.\n * Return value: previously mlocked page counts\n */\nstatic int count_mm_mlocked_page_nr(struct mm_struct *mm,\n\t\tunsigned long start, size_t len)\n{\n\tstruct vm_area_struct *vma;\n\tint count = 0;\n\n\tif (mm == NULL)\n\t\tmm = current->mm;\n\n\tvma = find_vma(mm, start);\n\tif (vma == NULL)\n\t\tvma = mm->mmap;\n\n\tfor (; vma ; vma = vma->vm_next) {\n\t\tif (start >= vma->vm_end)\n\t\t\tcontinue;\n\t\tif (start + len <=  vma->vm_start)\n\t\t\tbreak;\n\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\tif (start > vma->vm_start)\n\t\t\t\tcount -= (start - vma->vm_start);\n\t\t\tif (start + len < vma->vm_end) {\n\t\t\t\tcount += start + len - vma->vm_start;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcount += vma->vm_end - vma->vm_start;\n\t\t}\n\t}\n\n\treturn count >> PAGE_SHIFT;\n}\n\nstatic __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t flags)\n{\n\tunsigned long locked;\n\tunsigned long lock_limit;\n\tint error = -ENOMEM;\n\n\tif (!can_do_mlock())\n\t\treturn -EPERM;\n\n\tlru_add_drain_all();\t/* flush pagevec */\n\n\tlen = PAGE_ALIGN(len + (offset_in_page(start)));\n\tstart &= PAGE_MASK;\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tlock_limit >>= PAGE_SHIFT;\n\tlocked = len >> PAGE_SHIFT;\n\n\tif (down_write_killable(&current->mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tlocked += current->mm->locked_vm;\n\tif ((locked > lock_limit) && (!capable(CAP_IPC_LOCK))) {\n\t\t/*\n\t\t * It is possible that the regions requested intersect with\n\t\t * previously mlocked areas, that part area in \"mm->locked_vm\"\n\t\t * should not be counted to new mlock increment count. So check\n\t\t * and adjust locked count if necessary.\n\t\t */\n\t\tlocked -= count_mm_mlocked_page_nr(current->mm,\n\t\t\t\tstart, len);\n\t}\n\n\t/* check against resource limits */\n\tif ((locked <= lock_limit) || capable(CAP_IPC_LOCK))\n\t\terror = apply_vma_lock_flags(start, len, flags);\n\n\tup_write(&current->mm->mmap_sem);\n\tif (error)\n\t\treturn error;\n\n\terror = __mm_populate(start, len, 0);\n\tif (error)\n\t\treturn __mlock_posix_error_return(error);\n\treturn 0;\n}\n\nSYSCALL_DEFINE2(mlock, unsigned long, start, size_t, len)\n{\n\treturn do_mlock(start, len, VM_LOCKED);\n}\n\nSYSCALL_DEFINE3(mlock2, unsigned long, start, size_t, len, int, flags)\n{\n\tvm_flags_t vm_flags = VM_LOCKED;\n\n\tif (flags & ~MLOCK_ONFAULT)\n\t\treturn -EINVAL;\n\n\tif (flags & MLOCK_ONFAULT)\n\t\tvm_flags |= VM_LOCKONFAULT;\n\n\treturn do_mlock(start, len, vm_flags);\n}\n\nSYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)\n{\n\tint ret;\n\n\tlen = PAGE_ALIGN(len + (offset_in_page(start)));\n\tstart &= PAGE_MASK;\n\n\tif (down_write_killable(&current->mm->mmap_sem))\n\t\treturn -EINTR;\n\tret = apply_vma_lock_flags(start, len, 0);\n\tup_write(&current->mm->mmap_sem);\n\n\treturn ret;\n}\n\n/*\n * Take the MCL_* flags passed into mlockall (or 0 if called from munlockall)\n * and translate into the appropriate modifications to mm->def_flags and/or the\n * flags for all current VMAs.\n *\n * There are a couple of subtleties with this.  If mlockall() is called multiple\n * times with different flags, the values do not necessarily stack.  If mlockall\n * is called once including the MCL_FUTURE flag and then a second time without\n * it, VM_LOCKED and VM_LOCKONFAULT will be cleared from mm->def_flags.\n */\nstatic int apply_mlockall_flags(int flags)\n{\n\tstruct vm_area_struct * vma, * prev = NULL;\n\tvm_flags_t to_add = 0;\n\n\tcurrent->mm->def_flags &= VM_LOCKED_CLEAR_MASK;\n\tif (flags & MCL_FUTURE) {\n\t\tcurrent->mm->def_flags |= VM_LOCKED;\n\n\t\tif (flags & MCL_ONFAULT)\n\t\t\tcurrent->mm->def_flags |= VM_LOCKONFAULT;\n\n\t\tif (!(flags & MCL_CURRENT))\n\t\t\tgoto out;\n\t}\n\n\tif (flags & MCL_CURRENT) {\n\t\tto_add |= VM_LOCKED;\n\t\tif (flags & MCL_ONFAULT)\n\t\t\tto_add |= VM_LOCKONFAULT;\n\t}\n\n\tfor (vma = current->mm->mmap; vma ; vma = prev->vm_next) {\n\t\tvm_flags_t newflags;\n\n\t\tnewflags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;\n\t\tnewflags |= to_add;\n\n\t\t/* Ignore errors */\n\t\tmlock_fixup(vma, &prev, vma->vm_start, vma->vm_end, newflags);\n\t\tcond_resched_rcu_qs();\n\t}\nout:\n\treturn 0;\n}\n\nSYSCALL_DEFINE1(mlockall, int, flags)\n{\n\tunsigned long lock_limit;\n\tint ret;\n\n\tif (!flags || (flags & ~(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT)))\n\t\treturn -EINVAL;\n\n\tif (!can_do_mlock())\n\t\treturn -EPERM;\n\n\tif (flags & MCL_CURRENT)\n\t\tlru_add_drain_all();\t/* flush pagevec */\n\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tlock_limit >>= PAGE_SHIFT;\n\n\tif (down_write_killable(&current->mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tret = -ENOMEM;\n\tif (!(flags & MCL_CURRENT) || (current->mm->total_vm <= lock_limit) ||\n\t    capable(CAP_IPC_LOCK))\n\t\tret = apply_mlockall_flags(flags);\n\tup_write(&current->mm->mmap_sem);\n\tif (!ret && (flags & MCL_CURRENT))\n\t\tmm_populate(0, TASK_SIZE);\n\n\treturn ret;\n}\n\nSYSCALL_DEFINE0(munlockall)\n{\n\tint ret;\n\n\tif (down_write_killable(&current->mm->mmap_sem))\n\t\treturn -EINTR;\n\tret = apply_mlockall_flags(0);\n\tup_write(&current->mm->mmap_sem);\n\treturn ret;\n}\n\n/*\n * Objects with different lifetime than processes (SHM_LOCK and SHM_HUGETLB\n * shm segments) get accounted against the user_struct instead.\n */\nstatic DEFINE_SPINLOCK(shmlock_user_lock);\n\nint user_shm_lock(size_t size, struct user_struct *user)\n{\n\tunsigned long lock_limit, locked;\n\tint allowed = 0;\n\n\tlocked = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\tif (lock_limit == RLIM_INFINITY)\n\t\tallowed = 1;\n\tlock_limit >>= PAGE_SHIFT;\n\tspin_lock(&shmlock_user_lock);\n\tif (!allowed &&\n\t    locked + user->locked_shm > lock_limit && !capable(CAP_IPC_LOCK))\n\t\tgoto out;\n\tget_uid(user);\n\tuser->locked_shm += locked;\n\tallowed = 1;\nout:\n\tspin_unlock(&shmlock_user_lock);\n\treturn allowed;\n}\n\nvoid user_shm_unlock(size_t size, struct user_struct *user)\n{\n\tspin_lock(&shmlock_user_lock);\n\tuser->locked_shm -= (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tspin_unlock(&shmlock_user_lock);\n\tfree_uid(user);\n}\n"], "filenames": ["mm/mlock.c"], "buggy_code_start_loc": [287], "buggy_code_end_loc": [319], "fixing_code_start_loc": [287], "fixing_code_end_loc": [319], "type": "CWE-20", "message": "The __munlock_pagevec function in mm/mlock.c in the Linux kernel before 4.11.4 allows local users to cause a denial of service (NR_MLOCK accounting corruption) via crafted use of mlockall and munlockall system calls.", "other": {"cve": {"id": "CVE-2017-18221", "sourceIdentifier": "cve@mitre.org", "published": "2018-03-07T08:29:00.263", "lastModified": "2018-05-31T01:29:01.207", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The __munlock_pagevec function in mm/mlock.c in the Linux kernel before 4.11.4 allows local users to cause a denial of service (NR_MLOCK accounting corruption) via crafted use of mlockall and munlockall system calls."}, {"lang": "es", "value": "La funci\u00f3n __munlock_pagevec en mm/mlock.c en el kernel de Linux, en versiones anteriores a la 4.11.4, permite que usuarios locales provoquen una denegaci\u00f3n de servicio (corrupci\u00f3n de contabilidad NR_MLOCK) mediante el uso manipulado de llamadas del sistema mlockall y munlockall."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.11.4", "matchCriteriaId": "D5585AF1-24A5-486E-B916-B18EBF5851A7"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=70feee0e1ef331b22cc51f383d532a0d043fbdcc", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/103321", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/70feee0e1ef331b22cc51f383d532a0d043fbdcc", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3655-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3655-2/", "source": "cve@mitre.org"}, {"url": "https://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.11.4", "source": "cve@mitre.org", "tags": ["Release Notes", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/70feee0e1ef331b22cc51f383d532a0d043fbdcc"}}