{"buggy_code": ["/*\n * random.c -- A strong random number generator\n *\n * Copyright Matt Mackall <mpm@selenic.com>, 2003, 2004, 2005\n *\n * Copyright Theodore Ts'o, 1994, 1995, 1996, 1997, 1998, 1999.  All\n * rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, and the entire permission notice in its entirety,\n *    including the disclaimer of warranties.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n * 3. The name of the author may not be used to endorse or promote\n *    products derived from this software without specific prior\n *    written permission.\n *\n * ALTERNATIVELY, this product may be distributed under the terms of\n * the GNU General Public License, in which case the provisions of the GPL are\n * required INSTEAD OF the above restrictions.  (This clause is\n * necessary due to a potential bad interaction between the GPL and\n * the restrictions contained in a BSD-style copyright.)\n *\n * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED\n * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\n * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, ALL OF\n * WHICH ARE HEREBY DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT\n * OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n * USE OF THIS SOFTWARE, EVEN IF NOT ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n */\n\n/*\n * (now, with legal B.S. out of the way.....)\n *\n * This routine gathers environmental noise from device drivers, etc.,\n * and returns good random numbers, suitable for cryptographic use.\n * Besides the obvious cryptographic uses, these numbers are also good\n * for seeding TCP sequence numbers, and other places where it is\n * desirable to have numbers which are not only random, but hard to\n * predict by an attacker.\n *\n * Theory of operation\n * ===================\n *\n * Computers are very predictable devices.  Hence it is extremely hard\n * to produce truly random numbers on a computer --- as opposed to\n * pseudo-random numbers, which can easily generated by using a\n * algorithm.  Unfortunately, it is very easy for attackers to guess\n * the sequence of pseudo-random number generators, and for some\n * applications this is not acceptable.  So instead, we must try to\n * gather \"environmental noise\" from the computer's environment, which\n * must be hard for outside attackers to observe, and use that to\n * generate random numbers.  In a Unix environment, this is best done\n * from inside the kernel.\n *\n * Sources of randomness from the environment include inter-keyboard\n * timings, inter-interrupt timings from some interrupts, and other\n * events which are both (a) non-deterministic and (b) hard for an\n * outside observer to measure.  Randomness from these sources are\n * added to an \"entropy pool\", which is mixed using a CRC-like function.\n * This is not cryptographically strong, but it is adequate assuming\n * the randomness is not chosen maliciously, and it is fast enough that\n * the overhead of doing it on every interrupt is very reasonable.\n * As random bytes are mixed into the entropy pool, the routines keep\n * an *estimate* of how many bits of randomness have been stored into\n * the random number generator's internal state.\n *\n * When random bytes are desired, they are obtained by taking the SHA\n * hash of the contents of the \"entropy pool\".  The SHA hash avoids\n * exposing the internal state of the entropy pool.  It is believed to\n * be computationally infeasible to derive any useful information\n * about the input of SHA from its output.  Even if it is possible to\n * analyze SHA in some clever way, as long as the amount of data\n * returned from the generator is less than the inherent entropy in\n * the pool, the output data is totally unpredictable.  For this\n * reason, the routine decreases its internal estimate of how many\n * bits of \"true randomness\" are contained in the entropy pool as it\n * outputs random numbers.\n *\n * If this estimate goes to zero, the routine can still generate\n * random numbers; however, an attacker may (at least in theory) be\n * able to infer the future output of the generator from prior\n * outputs.  This requires successful cryptanalysis of SHA, which is\n * not believed to be feasible, but there is a remote possibility.\n * Nonetheless, these numbers should be useful for the vast majority\n * of purposes.\n *\n * Exported interfaces ---- output\n * ===============================\n *\n * There are three exported interfaces; the first is one designed to\n * be used from within the kernel:\n *\n * \tvoid get_random_bytes(void *buf, int nbytes);\n *\n * This interface will return the requested number of random bytes,\n * and place it in the requested buffer.\n *\n * The two other interfaces are two character devices /dev/random and\n * /dev/urandom.  /dev/random is suitable for use when very high\n * quality randomness is desired (for example, for key generation or\n * one-time pads), as it will only return a maximum of the number of\n * bits of randomness (as estimated by the random number generator)\n * contained in the entropy pool.\n *\n * The /dev/urandom device does not have this limit, and will return\n * as many bytes as are requested.  As more and more random bytes are\n * requested without giving time for the entropy pool to recharge,\n * this will result in random numbers that are merely cryptographically\n * strong.  For many applications, however, this is acceptable.\n *\n * Exported interfaces ---- input\n * ==============================\n *\n * The current exported interfaces for gathering environmental noise\n * from the devices are:\n *\n * \tvoid add_input_randomness(unsigned int type, unsigned int code,\n *                                unsigned int value);\n * \tvoid add_interrupt_randomness(int irq);\n * \tvoid add_disk_randomness(struct gendisk *disk);\n *\n * add_input_randomness() uses the input layer interrupt timing, as well as\n * the event type information from the hardware.\n *\n * add_interrupt_randomness() uses the inter-interrupt timing as random\n * inputs to the entropy pool.  Note that not all interrupts are good\n * sources of randomness!  For example, the timer interrupts is not a\n * good choice, because the periodicity of the interrupts is too\n * regular, and hence predictable to an attacker.  Network Interface\n * Controller interrupts are a better measure, since the timing of the\n * NIC interrupts are more unpredictable.\n *\n * add_disk_randomness() uses what amounts to the seek time of block\n * layer request events, on a per-disk_devt basis, as input to the\n * entropy pool. Note that high-speed solid state drives with very low\n * seek times do not make for good sources of entropy, as their seek\n * times are usually fairly consistent.\n *\n * All of these routines try to estimate how many bits of randomness a\n * particular randomness source.  They do this by keeping track of the\n * first and second order deltas of the event timings.\n *\n * Ensuring unpredictability at system startup\n * ============================================\n *\n * When any operating system starts up, it will go through a sequence\n * of actions that are fairly predictable by an adversary, especially\n * if the start-up does not involve interaction with a human operator.\n * This reduces the actual number of bits of unpredictability in the\n * entropy pool below the value in entropy_count.  In order to\n * counteract this effect, it helps to carry information in the\n * entropy pool across shut-downs and start-ups.  To do this, put the\n * following lines an appropriate script which is run during the boot\n * sequence:\n *\n *\techo \"Initializing random number generator...\"\n *\trandom_seed=/var/run/random-seed\n *\t# Carry a random seed from start-up to start-up\n *\t# Load and then save the whole entropy pool\n *\tif [ -f $random_seed ]; then\n *\t\tcat $random_seed >/dev/urandom\n *\telse\n *\t\ttouch $random_seed\n *\tfi\n *\tchmod 600 $random_seed\n *\tdd if=/dev/urandom of=$random_seed count=1 bs=512\n *\n * and the following lines in an appropriate script which is run as\n * the system is shutdown:\n *\n *\t# Carry a random seed from shut-down to start-up\n *\t# Save the whole entropy pool\n *\techo \"Saving random seed...\"\n *\trandom_seed=/var/run/random-seed\n *\ttouch $random_seed\n *\tchmod 600 $random_seed\n *\tdd if=/dev/urandom of=$random_seed count=1 bs=512\n *\n * For example, on most modern systems using the System V init\n * scripts, such code fragments would be found in\n * /etc/rc.d/init.d/random.  On older Linux systems, the correct script\n * location might be in /etc/rcb.d/rc.local or /etc/rc.d/rc.0.\n *\n * Effectively, these commands cause the contents of the entropy pool\n * to be saved at shut-down time and reloaded into the entropy pool at\n * start-up.  (The 'dd' in the addition to the bootup script is to\n * make sure that /etc/random-seed is different for every start-up,\n * even if the system crashes without executing rc.0.)  Even with\n * complete knowledge of the start-up activities, predicting the state\n * of the entropy pool requires knowledge of the previous history of\n * the system.\n *\n * Configuring the /dev/random driver under Linux\n * ==============================================\n *\n * The /dev/random driver under Linux uses minor numbers 8 and 9 of\n * the /dev/mem major number (#1).  So if your system does not have\n * /dev/random and /dev/urandom created already, they can be created\n * by using the commands:\n *\n * \tmknod /dev/random c 1 8\n * \tmknod /dev/urandom c 1 9\n *\n * Acknowledgements:\n * =================\n *\n * Ideas for constructing this random number generator were derived\n * from Pretty Good Privacy's random number generator, and from private\n * discussions with Phil Karn.  Colin Plumb provided a faster random\n * number generator, which speed up the mixing function of the entropy\n * pool, taken from PGPfone.  Dale Worley has also contributed many\n * useful ideas and suggestions to improve this driver.\n *\n * Any flaws in the design are solely my responsibility, and should\n * not be attributed to the Phil, Colin, or any of authors of PGP.\n *\n * Further background information on this topic may be obtained from\n * RFC 1750, \"Randomness Recommendations for Security\", by Donald\n * Eastlake, Steve Crocker, and Jeff Schiller.\n */\n\n#include <linux/utsname.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/major.h>\n#include <linux/string.h>\n#include <linux/fcntl.h>\n#include <linux/slab.h>\n#include <linux/random.h>\n#include <linux/poll.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/genhd.h>\n#include <linux/interrupt.h>\n#include <linux/mm.h>\n#include <linux/spinlock.h>\n#include <linux/percpu.h>\n#include <linux/cryptohash.h>\n#include <linux/fips.h>\n\n#ifdef CONFIG_GENERIC_HARDIRQS\n# include <linux/irq.h>\n#endif\n\n#include <asm/processor.h>\n#include <asm/uaccess.h>\n#include <asm/irq.h>\n#include <asm/io.h>\n\n/*\n * Configuration information\n */\n#define INPUT_POOL_WORDS 128\n#define OUTPUT_POOL_WORDS 32\n#define SEC_XFER_SIZE 512\n#define EXTRACT_SIZE 10\n\n/*\n * The minimum number of bits of entropy before we wake up a read on\n * /dev/random.  Should be enough to do a significant reseed.\n */\nstatic int random_read_wakeup_thresh = 64;\n\n/*\n * If the entropy count falls under this number of bits, then we\n * should wake up processes which are selecting or polling on write\n * access to /dev/random.\n */\nstatic int random_write_wakeup_thresh = 128;\n\n/*\n * When the input pool goes over trickle_thresh, start dropping most\n * samples to avoid wasting CPU time and reduce lock contention.\n */\n\nstatic int trickle_thresh __read_mostly = INPUT_POOL_WORDS * 28;\n\nstatic DEFINE_PER_CPU(int, trickle_count);\n\n/*\n * A pool of size .poolwords is stirred with a primitive polynomial\n * of degree .poolwords over GF(2).  The taps for various sizes are\n * defined below.  They are chosen to be evenly spaced (minimum RMS\n * distance from evenly spaced; the numbers in the comments are a\n * scaled squared error sum) except for the last tap, which is 1 to\n * get the twisting happening as fast as possible.\n */\nstatic struct poolinfo {\n\tint poolwords;\n\tint tap1, tap2, tap3, tap4, tap5;\n} poolinfo_table[] = {\n\t/* x^128 + x^103 + x^76 + x^51 +x^25 + x + 1 -- 105 */\n\t{ 128,\t103,\t76,\t51,\t25,\t1 },\n\t/* x^32 + x^26 + x^20 + x^14 + x^7 + x + 1 -- 15 */\n\t{ 32,\t26,\t20,\t14,\t7,\t1 },\n#if 0\n\t/* x^2048 + x^1638 + x^1231 + x^819 + x^411 + x + 1  -- 115 */\n\t{ 2048,\t1638,\t1231,\t819,\t411,\t1 },\n\n\t/* x^1024 + x^817 + x^615 + x^412 + x^204 + x + 1 -- 290 */\n\t{ 1024,\t817,\t615,\t412,\t204,\t1 },\n\n\t/* x^1024 + x^819 + x^616 + x^410 + x^207 + x^2 + 1 -- 115 */\n\t{ 1024,\t819,\t616,\t410,\t207,\t2 },\n\n\t/* x^512 + x^411 + x^308 + x^208 + x^104 + x + 1 -- 225 */\n\t{ 512,\t411,\t308,\t208,\t104,\t1 },\n\n\t/* x^512 + x^409 + x^307 + x^206 + x^102 + x^2 + 1 -- 95 */\n\t{ 512,\t409,\t307,\t206,\t102,\t2 },\n\t/* x^512 + x^409 + x^309 + x^205 + x^103 + x^2 + 1 -- 95 */\n\t{ 512,\t409,\t309,\t205,\t103,\t2 },\n\n\t/* x^256 + x^205 + x^155 + x^101 + x^52 + x + 1 -- 125 */\n\t{ 256,\t205,\t155,\t101,\t52,\t1 },\n\n\t/* x^128 + x^103 + x^78 + x^51 + x^27 + x^2 + 1 -- 70 */\n\t{ 128,\t103,\t78,\t51,\t27,\t2 },\n\n\t/* x^64 + x^52 + x^39 + x^26 + x^14 + x + 1 -- 15 */\n\t{ 64,\t52,\t39,\t26,\t14,\t1 },\n#endif\n};\n\n#define POOLBITS\tpoolwords*32\n#define POOLBYTES\tpoolwords*4\n\n/*\n * For the purposes of better mixing, we use the CRC-32 polynomial as\n * well to make a twisted Generalized Feedback Shift Reigster\n *\n * (See M. Matsumoto & Y. Kurita, 1992.  Twisted GFSR generators.  ACM\n * Transactions on Modeling and Computer Simulation 2(3):179-194.\n * Also see M. Matsumoto & Y. Kurita, 1994.  Twisted GFSR generators\n * II.  ACM Transactions on Mdeling and Computer Simulation 4:254-266)\n *\n * Thanks to Colin Plumb for suggesting this.\n *\n * We have not analyzed the resultant polynomial to prove it primitive;\n * in fact it almost certainly isn't.  Nonetheless, the irreducible factors\n * of a random large-degree polynomial over GF(2) are more than large enough\n * that periodicity is not a concern.\n *\n * The input hash is much less sensitive than the output hash.  All\n * that we want of it is that it be a good non-cryptographic hash;\n * i.e. it not produce collisions when fed \"random\" data of the sort\n * we expect to see.  As long as the pool state differs for different\n * inputs, we have preserved the input entropy and done a good job.\n * The fact that an intelligent attacker can construct inputs that\n * will produce controlled alterations to the pool's state is not\n * important because we don't consider such inputs to contribute any\n * randomness.  The only property we need with respect to them is that\n * the attacker can't increase his/her knowledge of the pool's state.\n * Since all additions are reversible (knowing the final state and the\n * input, you can reconstruct the initial state), if an attacker has\n * any uncertainty about the initial state, he/she can only shuffle\n * that uncertainty about, but never cause any collisions (which would\n * decrease the uncertainty).\n *\n * The chosen system lets the state of the pool be (essentially) the input\n * modulo the generator polymnomial.  Now, for random primitive polynomials,\n * this is a universal class of hash functions, meaning that the chance\n * of a collision is limited by the attacker's knowledge of the generator\n * polynomail, so if it is chosen at random, an attacker can never force\n * a collision.  Here, we use a fixed polynomial, but we *can* assume that\n * ###--> it is unknown to the processes generating the input entropy. <-###\n * Because of this important property, this is a good, collision-resistant\n * hash; hash collisions will occur no more often than chance.\n */\n\n/*\n * Static global variables\n */\nstatic DECLARE_WAIT_QUEUE_HEAD(random_read_wait);\nstatic DECLARE_WAIT_QUEUE_HEAD(random_write_wait);\nstatic struct fasync_struct *fasync;\n\n#if 0\nstatic int debug;\nmodule_param(debug, bool, 0644);\n#define DEBUG_ENT(fmt, arg...) do { \\\n\tif (debug) \\\n\t\tprintk(KERN_DEBUG \"random %04d %04d %04d: \" \\\n\t\tfmt,\\\n\t\tinput_pool.entropy_count,\\\n\t\tblocking_pool.entropy_count,\\\n\t\tnonblocking_pool.entropy_count,\\\n\t\t## arg); } while (0)\n#else\n#define DEBUG_ENT(fmt, arg...) do {} while (0)\n#endif\n\n/**********************************************************************\n *\n * OS independent entropy store.   Here are the functions which handle\n * storing entropy in an entropy pool.\n *\n **********************************************************************/\n\nstruct entropy_store;\nstruct entropy_store {\n\t/* read-only data: */\n\tstruct poolinfo *poolinfo;\n\t__u32 *pool;\n\tconst char *name;\n\tstruct entropy_store *pull;\n\tint limit;\n\n\t/* read-write data: */\n\tspinlock_t lock;\n\tunsigned add_ptr;\n\tint entropy_count;\n\tint input_rotate;\n\t__u8 last_data[EXTRACT_SIZE];\n};\n\nstatic __u32 input_pool_data[INPUT_POOL_WORDS];\nstatic __u32 blocking_pool_data[OUTPUT_POOL_WORDS];\nstatic __u32 nonblocking_pool_data[OUTPUT_POOL_WORDS];\n\nstatic struct entropy_store input_pool = {\n\t.poolinfo = &poolinfo_table[0],\n\t.name = \"input\",\n\t.limit = 1,\n\t.lock = __SPIN_LOCK_UNLOCKED(&input_pool.lock),\n\t.pool = input_pool_data\n};\n\nstatic struct entropy_store blocking_pool = {\n\t.poolinfo = &poolinfo_table[1],\n\t.name = \"blocking\",\n\t.limit = 1,\n\t.pull = &input_pool,\n\t.lock = __SPIN_LOCK_UNLOCKED(&blocking_pool.lock),\n\t.pool = blocking_pool_data\n};\n\nstatic struct entropy_store nonblocking_pool = {\n\t.poolinfo = &poolinfo_table[1],\n\t.name = \"nonblocking\",\n\t.pull = &input_pool,\n\t.lock = __SPIN_LOCK_UNLOCKED(&nonblocking_pool.lock),\n\t.pool = nonblocking_pool_data\n};\n\n/*\n * This function adds bytes into the entropy \"pool\".  It does not\n * update the entropy estimate.  The caller should call\n * credit_entropy_bits if this is appropriate.\n *\n * The pool is stirred with a primitive polynomial of the appropriate\n * degree, and then twisted.  We twist by three bits at a time because\n * it's cheap to do so and helps slightly in the expected case where\n * the entropy is concentrated in the low-order bits.\n */\nstatic void mix_pool_bytes_extract(struct entropy_store *r, const void *in,\n\t\t\t\t   int nbytes, __u8 out[64])\n{\n\tstatic __u32 const twist_table[8] = {\n\t\t0x00000000, 0x3b6e20c8, 0x76dc4190, 0x4db26158,\n\t\t0xedb88320, 0xd6d6a3e8, 0x9b64c2b0, 0xa00ae278 };\n\tunsigned long i, j, tap1, tap2, tap3, tap4, tap5;\n\tint input_rotate;\n\tint wordmask = r->poolinfo->poolwords - 1;\n\tconst char *bytes = in;\n\t__u32 w;\n\tunsigned long flags;\n\n\t/* Taps are constant, so we can load them without holding r->lock.  */\n\ttap1 = r->poolinfo->tap1;\n\ttap2 = r->poolinfo->tap2;\n\ttap3 = r->poolinfo->tap3;\n\ttap4 = r->poolinfo->tap4;\n\ttap5 = r->poolinfo->tap5;\n\n\tspin_lock_irqsave(&r->lock, flags);\n\tinput_rotate = r->input_rotate;\n\ti = r->add_ptr;\n\n\t/* mix one byte at a time to simplify size handling and churn faster */\n\twhile (nbytes--) {\n\t\tw = rol32(*bytes++, input_rotate & 31);\n\t\ti = (i - 1) & wordmask;\n\n\t\t/* XOR in the various taps */\n\t\tw ^= r->pool[i];\n\t\tw ^= r->pool[(i + tap1) & wordmask];\n\t\tw ^= r->pool[(i + tap2) & wordmask];\n\t\tw ^= r->pool[(i + tap3) & wordmask];\n\t\tw ^= r->pool[(i + tap4) & wordmask];\n\t\tw ^= r->pool[(i + tap5) & wordmask];\n\n\t\t/* Mix the result back in with a twist */\n\t\tr->pool[i] = (w >> 3) ^ twist_table[w & 7];\n\n\t\t/*\n\t\t * Normally, we add 7 bits of rotation to the pool.\n\t\t * At the beginning of the pool, add an extra 7 bits\n\t\t * rotation, so that successive passes spread the\n\t\t * input bits across the pool evenly.\n\t\t */\n\t\tinput_rotate += i ? 7 : 14;\n\t}\n\n\tr->input_rotate = input_rotate;\n\tr->add_ptr = i;\n\n\tif (out)\n\t\tfor (j = 0; j < 16; j++)\n\t\t\t((__u32 *)out)[j] = r->pool[(i - j) & wordmask];\n\n\tspin_unlock_irqrestore(&r->lock, flags);\n}\n\nstatic void mix_pool_bytes(struct entropy_store *r, const void *in, int bytes)\n{\n       mix_pool_bytes_extract(r, in, bytes, NULL);\n}\n\n/*\n * Credit (or debit) the entropy store with n bits of entropy\n */\nstatic void credit_entropy_bits(struct entropy_store *r, int nbits)\n{\n\tunsigned long flags;\n\tint entropy_count;\n\n\tif (!nbits)\n\t\treturn;\n\n\tspin_lock_irqsave(&r->lock, flags);\n\n\tDEBUG_ENT(\"added %d entropy credits to %s\\n\", nbits, r->name);\n\tentropy_count = r->entropy_count;\n\tentropy_count += nbits;\n\tif (entropy_count < 0) {\n\t\tDEBUG_ENT(\"negative entropy/overflow\\n\");\n\t\tentropy_count = 0;\n\t} else if (entropy_count > r->poolinfo->POOLBITS)\n\t\tentropy_count = r->poolinfo->POOLBITS;\n\tr->entropy_count = entropy_count;\n\n\t/* should we wake readers? */\n\tif (r == &input_pool && entropy_count >= random_read_wakeup_thresh) {\n\t\twake_up_interruptible(&random_read_wait);\n\t\tkill_fasync(&fasync, SIGIO, POLL_IN);\n\t}\n\tspin_unlock_irqrestore(&r->lock, flags);\n}\n\n/*********************************************************************\n *\n * Entropy input management\n *\n *********************************************************************/\n\n/* There is one of these per entropy source */\nstruct timer_rand_state {\n\tcycles_t last_time;\n\tlong last_delta, last_delta2;\n\tunsigned dont_count_entropy:1;\n};\n\n#ifndef CONFIG_GENERIC_HARDIRQS\n\nstatic struct timer_rand_state *irq_timer_state[NR_IRQS];\n\nstatic struct timer_rand_state *get_timer_rand_state(unsigned int irq)\n{\n\treturn irq_timer_state[irq];\n}\n\nstatic void set_timer_rand_state(unsigned int irq,\n\t\t\t\t struct timer_rand_state *state)\n{\n\tirq_timer_state[irq] = state;\n}\n\n#else\n\nstatic struct timer_rand_state *get_timer_rand_state(unsigned int irq)\n{\n\tstruct irq_desc *desc;\n\n\tdesc = irq_to_desc(irq);\n\n\treturn desc->timer_rand_state;\n}\n\nstatic void set_timer_rand_state(unsigned int irq,\n\t\t\t\t struct timer_rand_state *state)\n{\n\tstruct irq_desc *desc;\n\n\tdesc = irq_to_desc(irq);\n\n\tdesc->timer_rand_state = state;\n}\n#endif\n\nstatic struct timer_rand_state input_timer_state;\n\n/*\n * This function adds entropy to the entropy \"pool\" by using timing\n * delays.  It uses the timer_rand_state structure to make an estimate\n * of how many bits of entropy this call has added to the pool.\n *\n * The number \"num\" is also added to the pool - it should somehow describe\n * the type of event which just happened.  This is currently 0-255 for\n * keyboard scan codes, and 256 upwards for interrupts.\n *\n */\nstatic void add_timer_randomness(struct timer_rand_state *state, unsigned num)\n{\n\tstruct {\n\t\tcycles_t cycles;\n\t\tlong jiffies;\n\t\tunsigned num;\n\t} sample;\n\tlong delta, delta2, delta3;\n\n\tpreempt_disable();\n\t/* if over the trickle threshold, use only 1 in 4096 samples */\n\tif (input_pool.entropy_count > trickle_thresh &&\n\t    ((__this_cpu_inc_return(trickle_count) - 1) & 0xfff))\n\t\tgoto out;\n\n\tsample.jiffies = jiffies;\n\tsample.cycles = get_cycles();\n\tsample.num = num;\n\tmix_pool_bytes(&input_pool, &sample, sizeof(sample));\n\n\t/*\n\t * Calculate number of bits of randomness we probably added.\n\t * We take into account the first, second and third-order deltas\n\t * in order to make our estimate.\n\t */\n\n\tif (!state->dont_count_entropy) {\n\t\tdelta = sample.jiffies - state->last_time;\n\t\tstate->last_time = sample.jiffies;\n\n\t\tdelta2 = delta - state->last_delta;\n\t\tstate->last_delta = delta;\n\n\t\tdelta3 = delta2 - state->last_delta2;\n\t\tstate->last_delta2 = delta2;\n\n\t\tif (delta < 0)\n\t\t\tdelta = -delta;\n\t\tif (delta2 < 0)\n\t\t\tdelta2 = -delta2;\n\t\tif (delta3 < 0)\n\t\t\tdelta3 = -delta3;\n\t\tif (delta > delta2)\n\t\t\tdelta = delta2;\n\t\tif (delta > delta3)\n\t\t\tdelta = delta3;\n\n\t\t/*\n\t\t * delta is now minimum absolute delta.\n\t\t * Round down by 1 bit on general principles,\n\t\t * and limit entropy entimate to 12 bits.\n\t\t */\n\t\tcredit_entropy_bits(&input_pool,\n\t\t\t\t    min_t(int, fls(delta>>1), 11));\n\t}\nout:\n\tpreempt_enable();\n}\n\nvoid add_input_randomness(unsigned int type, unsigned int code,\n\t\t\t\t unsigned int value)\n{\n\tstatic unsigned char last_value;\n\n\t/* ignore autorepeat and the like */\n\tif (value == last_value)\n\t\treturn;\n\n\tDEBUG_ENT(\"input event\\n\");\n\tlast_value = value;\n\tadd_timer_randomness(&input_timer_state,\n\t\t\t     (type << 4) ^ code ^ (code >> 4) ^ value);\n}\nEXPORT_SYMBOL_GPL(add_input_randomness);\n\nvoid add_interrupt_randomness(int irq)\n{\n\tstruct timer_rand_state *state;\n\n\tstate = get_timer_rand_state(irq);\n\n\tif (state == NULL)\n\t\treturn;\n\n\tDEBUG_ENT(\"irq event %d\\n\", irq);\n\tadd_timer_randomness(state, 0x100 + irq);\n}\n\n#ifdef CONFIG_BLOCK\nvoid add_disk_randomness(struct gendisk *disk)\n{\n\tif (!disk || !disk->random)\n\t\treturn;\n\t/* first major is 1, so we get >= 0x200 here */\n\tDEBUG_ENT(\"disk event %d:%d\\n\",\n\t\t  MAJOR(disk_devt(disk)), MINOR(disk_devt(disk)));\n\n\tadd_timer_randomness(disk->random, 0x100 + disk_devt(disk));\n}\n#endif\n\n/*********************************************************************\n *\n * Entropy extraction routines\n *\n *********************************************************************/\n\nstatic ssize_t extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t       size_t nbytes, int min, int rsvd);\n\n/*\n * This utility inline function is responsible for transferring entropy\n * from the primary pool to the secondary extraction pool. We make\n * sure we pull enough for a 'catastrophic reseed'.\n */\nstatic void xfer_secondary_pool(struct entropy_store *r, size_t nbytes)\n{\n\t__u32 tmp[OUTPUT_POOL_WORDS];\n\n\tif (r->pull && r->entropy_count < nbytes * 8 &&\n\t    r->entropy_count < r->poolinfo->POOLBITS) {\n\t\t/* If we're limited, always leave two wakeup worth's BITS */\n\t\tint rsvd = r->limit ? 0 : random_read_wakeup_thresh/4;\n\t\tint bytes = nbytes;\n\n\t\t/* pull at least as many as BYTES as wakeup BITS */\n\t\tbytes = max_t(int, bytes, random_read_wakeup_thresh / 8);\n\t\t/* but never more than the buffer size */\n\t\tbytes = min_t(int, bytes, sizeof(tmp));\n\n\t\tDEBUG_ENT(\"going to reseed %s with %d bits \"\n\t\t\t  \"(%d of %d requested)\\n\",\n\t\t\t  r->name, bytes * 8, nbytes * 8, r->entropy_count);\n\n\t\tbytes = extract_entropy(r->pull, tmp, bytes,\n\t\t\t\t\trandom_read_wakeup_thresh / 8, rsvd);\n\t\tmix_pool_bytes(r, tmp, bytes);\n\t\tcredit_entropy_bits(r, bytes*8);\n\t}\n}\n\n/*\n * These functions extracts randomness from the \"entropy pool\", and\n * returns it in a buffer.\n *\n * The min parameter specifies the minimum amount we can pull before\n * failing to avoid races that defeat catastrophic reseeding while the\n * reserved parameter indicates how much entropy we must leave in the\n * pool after each pull to avoid starving other readers.\n *\n * Note: extract_entropy() assumes that .poolwords is a multiple of 16 words.\n */\n\nstatic size_t account(struct entropy_store *r, size_t nbytes, int min,\n\t\t      int reserved)\n{\n\tunsigned long flags;\n\n\t/* Hold lock while accounting */\n\tspin_lock_irqsave(&r->lock, flags);\n\n\tBUG_ON(r->entropy_count > r->poolinfo->POOLBITS);\n\tDEBUG_ENT(\"trying to extract %d bits from %s\\n\",\n\t\t  nbytes * 8, r->name);\n\n\t/* Can we pull enough? */\n\tif (r->entropy_count / 8 < min + reserved) {\n\t\tnbytes = 0;\n\t} else {\n\t\t/* If limited, never pull more than available */\n\t\tif (r->limit && nbytes + reserved >= r->entropy_count / 8)\n\t\t\tnbytes = r->entropy_count/8 - reserved;\n\n\t\tif (r->entropy_count / 8 >= nbytes + reserved)\n\t\t\tr->entropy_count -= nbytes*8;\n\t\telse\n\t\t\tr->entropy_count = reserved;\n\n\t\tif (r->entropy_count < random_write_wakeup_thresh) {\n\t\t\twake_up_interruptible(&random_write_wait);\n\t\t\tkill_fasync(&fasync, SIGIO, POLL_OUT);\n\t\t}\n\t}\n\n\tDEBUG_ENT(\"debiting %d entropy credits from %s%s\\n\",\n\t\t  nbytes * 8, r->name, r->limit ? \"\" : \" (unlimited)\");\n\n\tspin_unlock_irqrestore(&r->lock, flags);\n\n\treturn nbytes;\n}\n\nstatic void extract_buf(struct entropy_store *r, __u8 *out)\n{\n\tint i;\n\t__u32 hash[5], workspace[SHA_WORKSPACE_WORDS];\n\t__u8 extract[64];\n\n\t/* Generate a hash across the pool, 16 words (512 bits) at a time */\n\tsha_init(hash);\n\tfor (i = 0; i < r->poolinfo->poolwords; i += 16)\n\t\tsha_transform(hash, (__u8 *)(r->pool + i), workspace);\n\n\t/*\n\t * We mix the hash back into the pool to prevent backtracking\n\t * attacks (where the attacker knows the state of the pool\n\t * plus the current outputs, and attempts to find previous\n\t * ouputs), unless the hash function can be inverted. By\n\t * mixing at least a SHA1 worth of hash data back, we make\n\t * brute-forcing the feedback as hard as brute-forcing the\n\t * hash.\n\t */\n\tmix_pool_bytes_extract(r, hash, sizeof(hash), extract);\n\n\t/*\n\t * To avoid duplicates, we atomically extract a portion of the\n\t * pool while mixing, and hash one final time.\n\t */\n\tsha_transform(hash, extract, workspace);\n\tmemset(extract, 0, sizeof(extract));\n\tmemset(workspace, 0, sizeof(workspace));\n\n\t/*\n\t * In case the hash function has some recognizable output\n\t * pattern, we fold it in half. Thus, we always feed back\n\t * twice as much data as we output.\n\t */\n\thash[0] ^= hash[3];\n\thash[1] ^= hash[4];\n\thash[2] ^= rol32(hash[2], 16);\n\tmemcpy(out, hash, EXTRACT_SIZE);\n\tmemset(hash, 0, sizeof(hash));\n}\n\nstatic ssize_t extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t       size_t nbytes, int min, int reserved)\n{\n\tssize_t ret = 0, i;\n\t__u8 tmp[EXTRACT_SIZE];\n\tunsigned long flags;\n\n\txfer_secondary_pool(r, nbytes);\n\tnbytes = account(r, nbytes, min, reserved);\n\n\twhile (nbytes) {\n\t\textract_buf(r, tmp);\n\n\t\tif (fips_enabled) {\n\t\t\tspin_lock_irqsave(&r->lock, flags);\n\t\t\tif (!memcmp(tmp, r->last_data, EXTRACT_SIZE))\n\t\t\t\tpanic(\"Hardware RNG duplicated output!\\n\");\n\t\t\tmemcpy(r->last_data, tmp, EXTRACT_SIZE);\n\t\t\tspin_unlock_irqrestore(&r->lock, flags);\n\t\t}\n\t\ti = min_t(int, nbytes, EXTRACT_SIZE);\n\t\tmemcpy(buf, tmp, i);\n\t\tnbytes -= i;\n\t\tbuf += i;\n\t\tret += i;\n\t}\n\n\t/* Wipe data just returned from memory */\n\tmemset(tmp, 0, sizeof(tmp));\n\n\treturn ret;\n}\n\nstatic ssize_t extract_entropy_user(struct entropy_store *r, void __user *buf,\n\t\t\t\t    size_t nbytes)\n{\n\tssize_t ret = 0, i;\n\t__u8 tmp[EXTRACT_SIZE];\n\n\txfer_secondary_pool(r, nbytes);\n\tnbytes = account(r, nbytes, 0, 0);\n\n\twhile (nbytes) {\n\t\tif (need_resched()) {\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -ERESTARTSYS;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tschedule();\n\t\t}\n\n\t\textract_buf(r, tmp);\n\t\ti = min_t(int, nbytes, EXTRACT_SIZE);\n\t\tif (copy_to_user(buf, tmp, i)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tnbytes -= i;\n\t\tbuf += i;\n\t\tret += i;\n\t}\n\n\t/* Wipe data just returned from memory */\n\tmemset(tmp, 0, sizeof(tmp));\n\n\treturn ret;\n}\n\n/*\n * This function is the exported kernel interface.  It returns some\n * number of good random numbers, suitable for seeding TCP sequence\n * numbers, etc.\n */\nvoid get_random_bytes(void *buf, int nbytes)\n{\n\textract_entropy(&nonblocking_pool, buf, nbytes, 0, 0);\n}\nEXPORT_SYMBOL(get_random_bytes);\n\n/*\n * init_std_data - initialize pool with system data\n *\n * @r: pool to initialize\n *\n * This function clears the pool's entropy count and mixes some system\n * data into the pool to prepare it for use. The pool is not cleared\n * as that can only decrease the entropy in the pool.\n */\nstatic void init_std_data(struct entropy_store *r)\n{\n\tktime_t now;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&r->lock, flags);\n\tr->entropy_count = 0;\n\tspin_unlock_irqrestore(&r->lock, flags);\n\n\tnow = ktime_get_real();\n\tmix_pool_bytes(r, &now, sizeof(now));\n\tmix_pool_bytes(r, utsname(), sizeof(*(utsname())));\n}\n\nstatic int rand_initialize(void)\n{\n\tinit_std_data(&input_pool);\n\tinit_std_data(&blocking_pool);\n\tinit_std_data(&nonblocking_pool);\n\treturn 0;\n}\nmodule_init(rand_initialize);\n\nvoid rand_initialize_irq(int irq)\n{\n\tstruct timer_rand_state *state;\n\n\tstate = get_timer_rand_state(irq);\n\n\tif (state)\n\t\treturn;\n\n\t/*\n\t * If kzalloc returns null, we just won't use that entropy\n\t * source.\n\t */\n\tstate = kzalloc(sizeof(struct timer_rand_state), GFP_KERNEL);\n\tif (state)\n\t\tset_timer_rand_state(irq, state);\n}\n\n#ifdef CONFIG_BLOCK\nvoid rand_initialize_disk(struct gendisk *disk)\n{\n\tstruct timer_rand_state *state;\n\n\t/*\n\t * If kzalloc returns null, we just won't use that entropy\n\t * source.\n\t */\n\tstate = kzalloc(sizeof(struct timer_rand_state), GFP_KERNEL);\n\tif (state)\n\t\tdisk->random = state;\n}\n#endif\n\nstatic ssize_t\nrandom_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)\n{\n\tssize_t n, retval = 0, count = 0;\n\n\tif (nbytes == 0)\n\t\treturn 0;\n\n\twhile (nbytes > 0) {\n\t\tn = nbytes;\n\t\tif (n > SEC_XFER_SIZE)\n\t\t\tn = SEC_XFER_SIZE;\n\n\t\tDEBUG_ENT(\"reading %d bits\\n\", n*8);\n\n\t\tn = extract_entropy_user(&blocking_pool, buf, n);\n\n\t\tDEBUG_ENT(\"read got %d bits (%d still needed)\\n\",\n\t\t\t  n*8, (nbytes-n)*8);\n\n\t\tif (n == 0) {\n\t\t\tif (file->f_flags & O_NONBLOCK) {\n\t\t\t\tretval = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tDEBUG_ENT(\"sleeping?\\n\");\n\n\t\t\twait_event_interruptible(random_read_wait,\n\t\t\t\tinput_pool.entropy_count >=\n\t\t\t\t\t\t random_read_wakeup_thresh);\n\n\t\t\tDEBUG_ENT(\"awake\\n\");\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tretval = -ERESTARTSYS;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (n < 0) {\n\t\t\tretval = n;\n\t\t\tbreak;\n\t\t}\n\t\tcount += n;\n\t\tbuf += n;\n\t\tnbytes -= n;\n\t\tbreak;\t\t/* This break makes the device work */\n\t\t\t\t/* like a named pipe */\n\t}\n\n\treturn (count ? count : retval);\n}\n\nstatic ssize_t\nurandom_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)\n{\n\treturn extract_entropy_user(&nonblocking_pool, buf, nbytes);\n}\n\nstatic unsigned int\nrandom_poll(struct file *file, poll_table * wait)\n{\n\tunsigned int mask;\n\n\tpoll_wait(file, &random_read_wait, wait);\n\tpoll_wait(file, &random_write_wait, wait);\n\tmask = 0;\n\tif (input_pool.entropy_count >= random_read_wakeup_thresh)\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (input_pool.entropy_count < random_write_wakeup_thresh)\n\t\tmask |= POLLOUT | POLLWRNORM;\n\treturn mask;\n}\n\nstatic int\nwrite_pool(struct entropy_store *r, const char __user *buffer, size_t count)\n{\n\tsize_t bytes;\n\t__u32 buf[16];\n\tconst char __user *p = buffer;\n\n\twhile (count > 0) {\n\t\tbytes = min(count, sizeof(buf));\n\t\tif (copy_from_user(&buf, p, bytes))\n\t\t\treturn -EFAULT;\n\n\t\tcount -= bytes;\n\t\tp += bytes;\n\n\t\tmix_pool_bytes(r, buf, bytes);\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t random_write(struct file *file, const char __user *buffer,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tsize_t ret;\n\n\tret = write_pool(&blocking_pool, buffer, count);\n\tif (ret)\n\t\treturn ret;\n\tret = write_pool(&nonblocking_pool, buffer, count);\n\tif (ret)\n\t\treturn ret;\n\n\treturn (ssize_t)count;\n}\n\nstatic long random_ioctl(struct file *f, unsigned int cmd, unsigned long arg)\n{\n\tint size, ent_count;\n\tint __user *p = (int __user *)arg;\n\tint retval;\n\n\tswitch (cmd) {\n\tcase RNDGETENTCNT:\n\t\t/* inherently racy, no point locking */\n\t\tif (put_user(input_pool.entropy_count, p))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase RNDADDTOENTCNT:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (get_user(ent_count, p))\n\t\t\treturn -EFAULT;\n\t\tcredit_entropy_bits(&input_pool, ent_count);\n\t\treturn 0;\n\tcase RNDADDENTROPY:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (get_user(ent_count, p++))\n\t\t\treturn -EFAULT;\n\t\tif (ent_count < 0)\n\t\t\treturn -EINVAL;\n\t\tif (get_user(size, p++))\n\t\t\treturn -EFAULT;\n\t\tretval = write_pool(&input_pool, (const char __user *)p,\n\t\t\t\t    size);\n\t\tif (retval < 0)\n\t\t\treturn retval;\n\t\tcredit_entropy_bits(&input_pool, ent_count);\n\t\treturn 0;\n\tcase RNDZAPENTCNT:\n\tcase RNDCLEARPOOL:\n\t\t/* Clear the entropy pool counters. */\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\trand_initialize();\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int random_fasync(int fd, struct file *filp, int on)\n{\n\treturn fasync_helper(fd, filp, on, &fasync);\n}\n\nconst struct file_operations random_fops = {\n\t.read  = random_read,\n\t.write = random_write,\n\t.poll  = random_poll,\n\t.unlocked_ioctl = random_ioctl,\n\t.fasync = random_fasync,\n\t.llseek = noop_llseek,\n};\n\nconst struct file_operations urandom_fops = {\n\t.read  = urandom_read,\n\t.write = random_write,\n\t.unlocked_ioctl = random_ioctl,\n\t.fasync = random_fasync,\n\t.llseek = noop_llseek,\n};\n\n/***************************************************************\n * Random UUID interface\n *\n * Used here for a Boot ID, but can be useful for other kernel\n * drivers.\n ***************************************************************/\n\n/*\n * Generate random UUID\n */\nvoid generate_random_uuid(unsigned char uuid_out[16])\n{\n\tget_random_bytes(uuid_out, 16);\n\t/* Set UUID version to 4 --- truly random generation */\n\tuuid_out[6] = (uuid_out[6] & 0x0F) | 0x40;\n\t/* Set the UUID variant to DCE */\n\tuuid_out[8] = (uuid_out[8] & 0x3F) | 0x80;\n}\nEXPORT_SYMBOL(generate_random_uuid);\n\n/********************************************************************\n *\n * Sysctl interface\n *\n ********************************************************************/\n\n#ifdef CONFIG_SYSCTL\n\n#include <linux/sysctl.h>\n\nstatic int min_read_thresh = 8, min_write_thresh;\nstatic int max_read_thresh = INPUT_POOL_WORDS * 32;\nstatic int max_write_thresh = INPUT_POOL_WORDS * 32;\nstatic char sysctl_bootid[16];\n\n/*\n * These functions is used to return both the bootid UUID, and random\n * UUID.  The difference is in whether table->data is NULL; if it is,\n * then a new UUID is generated and returned to the user.\n *\n * If the user accesses this via the proc interface, it will be returned\n * as an ASCII string in the standard UUID format.  If accesses via the\n * sysctl system call, it is returned as 16 bytes of binary data.\n */\nstatic int proc_do_uuid(ctl_table *table, int write,\n\t\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tctl_table fake_table;\n\tunsigned char buf[64], tmp_uuid[16], *uuid;\n\n\tuuid = table->data;\n\tif (!uuid) {\n\t\tuuid = tmp_uuid;\n\t\tuuid[8] = 0;\n\t}\n\tif (uuid[8] == 0)\n\t\tgenerate_random_uuid(uuid);\n\n\tsprintf(buf, \"%pU\", uuid);\n\n\tfake_table.data = buf;\n\tfake_table.maxlen = sizeof(buf);\n\n\treturn proc_dostring(&fake_table, write, buffer, lenp, ppos);\n}\n\nstatic int sysctl_poolsize = INPUT_POOL_WORDS * 32;\nctl_table random_table[] = {\n\t{\n\t\t.procname\t= \"poolsize\",\n\t\t.data\t\t= &sysctl_poolsize,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"entropy_avail\",\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.data\t\t= &input_pool.entropy_count,\n\t},\n\t{\n\t\t.procname\t= \"read_wakeup_threshold\",\n\t\t.data\t\t= &random_read_wakeup_thresh,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_read_thresh,\n\t\t.extra2\t\t= &max_read_thresh,\n\t},\n\t{\n\t\t.procname\t= \"write_wakeup_threshold\",\n\t\t.data\t\t= &random_write_wakeup_thresh,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_write_thresh,\n\t\t.extra2\t\t= &max_write_thresh,\n\t},\n\t{\n\t\t.procname\t= \"boot_id\",\n\t\t.data\t\t= &sysctl_bootid,\n\t\t.maxlen\t\t= 16,\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_do_uuid,\n\t},\n\t{\n\t\t.procname\t= \"uuid\",\n\t\t.maxlen\t\t= 16,\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_do_uuid,\n\t},\n\t{ }\n};\n#endif \t/* CONFIG_SYSCTL */\n\n/********************************************************************\n *\n * Random functions for networking\n *\n ********************************************************************/\n\n/*\n * TCP initial sequence number picking.  This uses the random number\n * generator to pick an initial secret value.  This value is hashed\n * along with the TCP endpoint information to provide a unique\n * starting point for each pair of TCP endpoints.  This defeats\n * attacks which rely on guessing the initial TCP sequence number.\n * This algorithm was suggested by Steve Bellovin.\n *\n * Using a very strong hash was taking an appreciable amount of the total\n * TCP connection establishment time, so this is a weaker hash,\n * compensated for by changing the secret periodically.\n */\n\n/* F, G and H are basic MD4 functions: selection, majority, parity */\n#define F(x, y, z) ((z) ^ ((x) & ((y) ^ (z))))\n#define G(x, y, z) (((x) & (y)) + (((x) ^ (y)) & (z)))\n#define H(x, y, z) ((x) ^ (y) ^ (z))\n\n/*\n * The generic round function.  The application is so specific that\n * we don't bother protecting all the arguments with parens, as is generally\n * good macro practice, in favor of extra legibility.\n * Rotation is separate from addition to prevent recomputation\n */\n#define ROUND(f, a, b, c, d, x, s)\t\\\n\t(a += f(b, c, d) + x, a = (a << s) | (a >> (32 - s)))\n#define K1 0\n#define K2 013240474631UL\n#define K3 015666365641UL\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\nstatic __u32 twothirdsMD4Transform(__u32 const buf[4], __u32 const in[12])\n{\n\t__u32 a = buf[0], b = buf[1], c = buf[2], d = buf[3];\n\n\t/* Round 1 */\n\tROUND(F, a, b, c, d, in[ 0] + K1,  3);\n\tROUND(F, d, a, b, c, in[ 1] + K1,  7);\n\tROUND(F, c, d, a, b, in[ 2] + K1, 11);\n\tROUND(F, b, c, d, a, in[ 3] + K1, 19);\n\tROUND(F, a, b, c, d, in[ 4] + K1,  3);\n\tROUND(F, d, a, b, c, in[ 5] + K1,  7);\n\tROUND(F, c, d, a, b, in[ 6] + K1, 11);\n\tROUND(F, b, c, d, a, in[ 7] + K1, 19);\n\tROUND(F, a, b, c, d, in[ 8] + K1,  3);\n\tROUND(F, d, a, b, c, in[ 9] + K1,  7);\n\tROUND(F, c, d, a, b, in[10] + K1, 11);\n\tROUND(F, b, c, d, a, in[11] + K1, 19);\n\n\t/* Round 2 */\n\tROUND(G, a, b, c, d, in[ 1] + K2,  3);\n\tROUND(G, d, a, b, c, in[ 3] + K2,  5);\n\tROUND(G, c, d, a, b, in[ 5] + K2,  9);\n\tROUND(G, b, c, d, a, in[ 7] + K2, 13);\n\tROUND(G, a, b, c, d, in[ 9] + K2,  3);\n\tROUND(G, d, a, b, c, in[11] + K2,  5);\n\tROUND(G, c, d, a, b, in[ 0] + K2,  9);\n\tROUND(G, b, c, d, a, in[ 2] + K2, 13);\n\tROUND(G, a, b, c, d, in[ 4] + K2,  3);\n\tROUND(G, d, a, b, c, in[ 6] + K2,  5);\n\tROUND(G, c, d, a, b, in[ 8] + K2,  9);\n\tROUND(G, b, c, d, a, in[10] + K2, 13);\n\n\t/* Round 3 */\n\tROUND(H, a, b, c, d, in[ 3] + K3,  3);\n\tROUND(H, d, a, b, c, in[ 7] + K3,  9);\n\tROUND(H, c, d, a, b, in[11] + K3, 11);\n\tROUND(H, b, c, d, a, in[ 2] + K3, 15);\n\tROUND(H, a, b, c, d, in[ 6] + K3,  3);\n\tROUND(H, d, a, b, c, in[10] + K3,  9);\n\tROUND(H, c, d, a, b, in[ 1] + K3, 11);\n\tROUND(H, b, c, d, a, in[ 5] + K3, 15);\n\tROUND(H, a, b, c, d, in[ 9] + K3,  3);\n\tROUND(H, d, a, b, c, in[ 0] + K3,  9);\n\tROUND(H, c, d, a, b, in[ 4] + K3, 11);\n\tROUND(H, b, c, d, a, in[ 8] + K3, 15);\n\n\treturn buf[1] + b; /* \"most hashed\" word */\n\t/* Alternative: return sum of all words? */\n}\n#endif\n\n#undef ROUND\n#undef F\n#undef G\n#undef H\n#undef K1\n#undef K2\n#undef K3\n\n/* This should not be decreased so low that ISNs wrap too fast. */\n#define REKEY_INTERVAL (300 * HZ)\n/*\n * Bit layout of the tcp sequence numbers (before adding current time):\n * bit 24-31: increased after every key exchange\n * bit 0-23: hash(source,dest)\n *\n * The implementation is similar to the algorithm described\n * in the Appendix of RFC 1185, except that\n * - it uses a 1 MHz clock instead of a 250 kHz clock\n * - it performs a rekey every 5 minutes, which is equivalent\n * \tto a (source,dest) tulple dependent forward jump of the\n * \tclock by 0..2^(HASH_BITS+1)\n *\n * Thus the average ISN wraparound time is 68 minutes instead of\n * 4.55 hours.\n *\n * SMP cleanup and lock avoidance with poor man's RCU.\n * \t\t\tManfred Spraul <manfred@colorfullife.com>\n *\n */\n#define COUNT_BITS 8\n#define COUNT_MASK ((1 << COUNT_BITS) - 1)\n#define HASH_BITS 24\n#define HASH_MASK ((1 << HASH_BITS) - 1)\n\nstatic struct keydata {\n\t__u32 count; /* already shifted to the final position */\n\t__u32 secret[12];\n} ____cacheline_aligned ip_keydata[2];\n\nstatic unsigned int ip_cnt;\n\nstatic void rekey_seq_generator(struct work_struct *work);\n\nstatic DECLARE_DELAYED_WORK(rekey_work, rekey_seq_generator);\n\n/*\n * Lock avoidance:\n * The ISN generation runs lockless - it's just a hash over random data.\n * State changes happen every 5 minutes when the random key is replaced.\n * Synchronization is performed by having two copies of the hash function\n * state and rekey_seq_generator always updates the inactive copy.\n * The copy is then activated by updating ip_cnt.\n * The implementation breaks down if someone blocks the thread\n * that processes SYN requests for more than 5 minutes. Should never\n * happen, and even if that happens only a not perfectly compliant\n * ISN is generated, nothing fatal.\n */\nstatic void rekey_seq_generator(struct work_struct *work)\n{\n\tstruct keydata *keyptr = &ip_keydata[1 ^ (ip_cnt & 1)];\n\n\tget_random_bytes(keyptr->secret, sizeof(keyptr->secret));\n\tkeyptr->count = (ip_cnt & COUNT_MASK) << HASH_BITS;\n\tsmp_wmb();\n\tip_cnt++;\n\tschedule_delayed_work(&rekey_work,\n\t\t\t      round_jiffies_relative(REKEY_INTERVAL));\n}\n\nstatic inline struct keydata *get_keyptr(void)\n{\n\tstruct keydata *keyptr = &ip_keydata[ip_cnt & 1];\n\n\tsmp_rmb();\n\n\treturn keyptr;\n}\n\nstatic __init int seqgen_init(void)\n{\n\trekey_seq_generator(NULL);\n\treturn 0;\n}\nlate_initcall(seqgen_init);\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n__u32 secure_tcpv6_sequence_number(__be32 *saddr, __be32 *daddr,\n\t\t\t\t   __be16 sport, __be16 dport)\n{\n\t__u32 seq;\n\t__u32 hash[12];\n\tstruct keydata *keyptr = get_keyptr();\n\n\t/* The procedure is the same as for IPv4, but addresses are longer.\n\t * Thus we must use twothirdsMD4Transform.\n\t */\n\n\tmemcpy(hash, saddr, 16);\n\thash[4] = ((__force u16)sport << 16) + (__force u16)dport;\n\tmemcpy(&hash[5], keyptr->secret, sizeof(__u32) * 7);\n\n\tseq = twothirdsMD4Transform((const __u32 *)daddr, hash) & HASH_MASK;\n\tseq += keyptr->count;\n\n\tseq += ktime_to_ns(ktime_get_real());\n\n\treturn seq;\n}\nEXPORT_SYMBOL(secure_tcpv6_sequence_number);\n#endif\n\n/*  The code below is shamelessly stolen from secure_tcp_sequence_number().\n *  All blames to Andrey V. Savochkin <saw@msu.ru>.\n */\n__u32 secure_ip_id(__be32 daddr)\n{\n\tstruct keydata *keyptr;\n\t__u32 hash[4];\n\n\tkeyptr = get_keyptr();\n\n\t/*\n\t *  Pick a unique starting offset for each IP destination.\n\t *  The dest ip address is placed in the starting vector,\n\t *  which is then hashed with random data.\n\t */\n\thash[0] = (__force __u32)daddr;\n\thash[1] = keyptr->secret[9];\n\thash[2] = keyptr->secret[10];\n\thash[3] = keyptr->secret[11];\n\n\treturn half_md4_transform(hash, keyptr->secret);\n}\n\n#ifdef CONFIG_INET\n\n__u32 secure_tcp_sequence_number(__be32 saddr, __be32 daddr,\n\t\t\t\t __be16 sport, __be16 dport)\n{\n\t__u32 seq;\n\t__u32 hash[4];\n\tstruct keydata *keyptr = get_keyptr();\n\n\t/*\n\t *  Pick a unique starting offset for each TCP connection endpoints\n\t *  (saddr, daddr, sport, dport).\n\t *  Note that the words are placed into the starting vector, which is\n\t *  then mixed with a partial MD4 over random data.\n\t */\n\thash[0] = (__force u32)saddr;\n\thash[1] = (__force u32)daddr;\n\thash[2] = ((__force u16)sport << 16) + (__force u16)dport;\n\thash[3] = keyptr->secret[11];\n\n\tseq = half_md4_transform(hash, keyptr->secret) & HASH_MASK;\n\tseq += keyptr->count;\n\t/*\n\t *\tAs close as possible to RFC 793, which\n\t *\tsuggests using a 250 kHz clock.\n\t *\tFurther reading shows this assumes 2 Mb/s networks.\n\t *\tFor 10 Mb/s Ethernet, a 1 MHz clock is appropriate.\n\t *\tFor 10 Gb/s Ethernet, a 1 GHz clock should be ok, but\n\t *\twe also need to limit the resolution so that the u32 seq\n\t *\toverlaps less than one time per MSL (2 minutes).\n\t *\tChoosing a clock of 64 ns period is OK. (period of 274 s)\n\t */\n\tseq += ktime_to_ns(ktime_get_real()) >> 6;\n\n\treturn seq;\n}\n\n/* Generate secure starting point for ephemeral IPV4 transport port search */\nu32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)\n{\n\tstruct keydata *keyptr = get_keyptr();\n\tu32 hash[4];\n\n\t/*\n\t *  Pick a unique starting offset for each ephemeral port search\n\t *  (saddr, daddr, dport) and 48bits of random data.\n\t */\n\thash[0] = (__force u32)saddr;\n\thash[1] = (__force u32)daddr;\n\thash[2] = (__force u32)dport ^ keyptr->secret[10];\n\thash[3] = keyptr->secret[11];\n\n\treturn half_md4_transform(hash, keyptr->secret);\n}\nEXPORT_SYMBOL_GPL(secure_ipv4_port_ephemeral);\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\nu32 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,\n\t\t\t       __be16 dport)\n{\n\tstruct keydata *keyptr = get_keyptr();\n\tu32 hash[12];\n\n\tmemcpy(hash, saddr, 16);\n\thash[4] = (__force u32)dport;\n\tmemcpy(&hash[5], keyptr->secret, sizeof(__u32) * 7);\n\n\treturn twothirdsMD4Transform((const __u32 *)daddr, hash);\n}\n#endif\n\n#if defined(CONFIG_IP_DCCP) || defined(CONFIG_IP_DCCP_MODULE)\n/* Similar to secure_tcp_sequence_number but generate a 48 bit value\n * bit's 32-47 increase every key exchange\n *       0-31  hash(source, dest)\n */\nu64 secure_dccp_sequence_number(__be32 saddr, __be32 daddr,\n\t\t\t\t__be16 sport, __be16 dport)\n{\n\tu64 seq;\n\t__u32 hash[4];\n\tstruct keydata *keyptr = get_keyptr();\n\n\thash[0] = (__force u32)saddr;\n\thash[1] = (__force u32)daddr;\n\thash[2] = ((__force u16)sport << 16) + (__force u16)dport;\n\thash[3] = keyptr->secret[11];\n\n\tseq = half_md4_transform(hash, keyptr->secret);\n\tseq |= ((u64)keyptr->count) << (32 - HASH_BITS);\n\n\tseq += ktime_to_ns(ktime_get_real());\n\tseq &= (1ull << 48) - 1;\n\n\treturn seq;\n}\nEXPORT_SYMBOL(secure_dccp_sequence_number);\n#endif\n\n#endif /* CONFIG_INET */\n\n\n/*\n * Get a random word for internal kernel use only. Similar to urandom but\n * with the goal of minimal entropy pool depletion. As a result, the random\n * value is not cryptographically secure but for several uses the cost of\n * depleting entropy is too high\n */\nDEFINE_PER_CPU(__u32 [4], get_random_int_hash);\nunsigned int get_random_int(void)\n{\n\tstruct keydata *keyptr;\n\t__u32 *hash = get_cpu_var(get_random_int_hash);\n\tint ret;\n\n\tkeyptr = get_keyptr();\n\thash[0] += current->pid + jiffies + get_cycles();\n\n\tret = half_md4_transform(hash, keyptr->secret);\n\tput_cpu_var(get_random_int_hash);\n\n\treturn ret;\n}\n\n/*\n * randomize_range() returns a start address such that\n *\n *    [...... <range> .....]\n *  start                  end\n *\n * a <range> with size \"len\" starting at the return value is inside in the\n * area defined by [start, end], but is otherwise randomized.\n */\nunsigned long\nrandomize_range(unsigned long start, unsigned long end, unsigned long len)\n{\n\tunsigned long range = end - len - start;\n\n\tif (end <= start + len)\n\t\treturn 0;\n\treturn PAGE_ALIGN(get_random_int() % range + start);\n}\n", "/*\n * include/linux/random.h\n *\n * Include file for the random number generator.\n */\n\n#ifndef _LINUX_RANDOM_H\n#define _LINUX_RANDOM_H\n\n#include <linux/types.h>\n#include <linux/ioctl.h>\n#include <linux/irqnr.h>\n\n/* ioctl()'s for the random number generator */\n\n/* Get the entropy count. */\n#define RNDGETENTCNT\t_IOR( 'R', 0x00, int )\n\n/* Add to (or subtract from) the entropy count.  (Superuser only.) */\n#define RNDADDTOENTCNT\t_IOW( 'R', 0x01, int )\n\n/* Get the contents of the entropy pool.  (Superuser only.) */\n#define RNDGETPOOL\t_IOR( 'R', 0x02, int [2] )\n\n/* \n * Write bytes into the entropy pool and add to the entropy count.\n * (Superuser only.)\n */\n#define RNDADDENTROPY\t_IOW( 'R', 0x03, int [2] )\n\n/* Clear entropy count to 0.  (Superuser only.) */\n#define RNDZAPENTCNT\t_IO( 'R', 0x04 )\n\n/* Clear the entropy pool and associated counters.  (Superuser only.) */\n#define RNDCLEARPOOL\t_IO( 'R', 0x06 )\n\nstruct rand_pool_info {\n\tint\tentropy_count;\n\tint\tbuf_size;\n\t__u32\tbuf[0];\n};\n\nstruct rnd_state {\n\t__u32 s1, s2, s3;\n};\n\n/* Exported functions */\n\n#ifdef __KERNEL__\n\nextern void rand_initialize_irq(int irq);\n\nextern void add_input_randomness(unsigned int type, unsigned int code,\n\t\t\t\t unsigned int value);\nextern void add_interrupt_randomness(int irq);\n\nextern void get_random_bytes(void *buf, int nbytes);\nvoid generate_random_uuid(unsigned char uuid_out[16]);\n\nextern __u32 secure_ip_id(__be32 daddr);\nextern u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport);\nextern u32 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,\n\t\t\t\t      __be16 dport);\nextern __u32 secure_tcp_sequence_number(__be32 saddr, __be32 daddr,\n\t\t\t\t\t__be16 sport, __be16 dport);\nextern __u32 secure_tcpv6_sequence_number(__be32 *saddr, __be32 *daddr,\n\t\t\t\t\t  __be16 sport, __be16 dport);\nextern u64 secure_dccp_sequence_number(__be32 saddr, __be32 daddr,\n\t\t\t\t       __be16 sport, __be16 dport);\n\n#ifndef MODULE\nextern const struct file_operations random_fops, urandom_fops;\n#endif\n\nunsigned int get_random_int(void);\nunsigned long randomize_range(unsigned long start, unsigned long end, unsigned long len);\n\nu32 random32(void);\nvoid srandom32(u32 seed);\n\nu32 prandom32(struct rnd_state *);\n\n/*\n * Handle minimum values for seeds\n */\nstatic inline u32 __seed(u32 x, u32 m)\n{\n\treturn (x < m) ? x + m : x;\n}\n\n/**\n * prandom32_seed - set seed for prandom32().\n * @state: pointer to state structure to receive the seed.\n * @seed: arbitrary 64-bit value to use as a seed.\n */\nstatic inline void prandom32_seed(struct rnd_state *state, u64 seed)\n{\n\tu32 i = (seed >> 32) ^ (seed << 10) ^ seed;\n\n\tstate->s1 = __seed(i, 1);\n\tstate->s2 = __seed(i, 7);\n\tstate->s3 = __seed(i, 15);\n}\n\n#endif /* __KERNEL___ */\n\n#endif /* _LINUX_RANDOM_H */\n", "/*\n *\t\tINETPEER - A storage for permanent information about peers\n *\n *  Authors:\tAndrey V. Savochkin <saw@msu.ru>\n */\n\n#ifndef _NET_INETPEER_H\n#define _NET_INETPEER_H\n\n#include <linux/types.h>\n#include <linux/init.h>\n#include <linux/jiffies.h>\n#include <linux/spinlock.h>\n#include <linux/rtnetlink.h>\n#include <net/ipv6.h>\n#include <asm/atomic.h>\n\nstruct inetpeer_addr_base {\n\tunion {\n\t\t__be32\t\t\ta4;\n\t\t__be32\t\t\ta6[4];\n\t};\n};\n\nstruct inetpeer_addr {\n\tstruct inetpeer_addr_base\taddr;\n\t__u16\t\t\t\tfamily;\n};\n\nstruct inet_peer {\n\t/* group together avl_left,avl_right,v4daddr to speedup lookups */\n\tstruct inet_peer __rcu\t*avl_left, *avl_right;\n\tstruct inetpeer_addr\tdaddr;\n\t__u32\t\t\tavl_height;\n\n\tu32\t\t\tmetrics[RTAX_MAX];\n\tu32\t\t\trate_tokens;\t/* rate limiting for ICMP */\n\tunsigned long\t\trate_last;\n\tunsigned long\t\tpmtu_expires;\n\tu32\t\t\tpmtu_orig;\n\tu32\t\t\tpmtu_learned;\n\tstruct inetpeer_addr_base redirect_learned;\n\t/*\n\t * Once inet_peer is queued for deletion (refcnt == -1), following fields\n\t * are not available: rid, ip_id_count, tcp_ts, tcp_ts_stamp\n\t * We can share memory with rcu_head to help keep inet_peer small.\n\t */\n\tunion {\n\t\tstruct {\n\t\t\tatomic_t\t\t\trid;\t\t/* Frag reception counter */\n\t\t\tatomic_t\t\t\tip_id_count;\t/* IP ID for the next packet */\n\t\t\t__u32\t\t\t\ttcp_ts;\n\t\t\t__u32\t\t\t\ttcp_ts_stamp;\n\t\t};\n\t\tstruct rcu_head         rcu;\n\t\tstruct inet_peer\t*gc_next;\n\t};\n\n\t/* following fields might be frequently dirtied */\n\t__u32\t\t\tdtime;\t/* the time of last use of not referenced entries */\n\tatomic_t\t\trefcnt;\n};\n\nvoid\t\t\tinet_initpeers(void) __init;\n\n#define INETPEER_METRICS_NEW\t(~(u32) 0)\n\nstatic inline bool inet_metrics_new(const struct inet_peer *p)\n{\n\treturn p->metrics[RTAX_LOCK-1] == INETPEER_METRICS_NEW;\n}\n\n/* can be called with or without local BH being disabled */\nstruct inet_peer\t*inet_getpeer(struct inetpeer_addr *daddr, int create);\n\nstatic inline struct inet_peer *inet_getpeer_v4(__be32 v4daddr, int create)\n{\n\tstruct inetpeer_addr daddr;\n\n\tdaddr.addr.a4 = v4daddr;\n\tdaddr.family = AF_INET;\n\treturn inet_getpeer(&daddr, create);\n}\n\nstatic inline struct inet_peer *inet_getpeer_v6(const struct in6_addr *v6daddr, int create)\n{\n\tstruct inetpeer_addr daddr;\n\n\tipv6_addr_copy((struct in6_addr *)daddr.addr.a6, v6daddr);\n\tdaddr.family = AF_INET6;\n\treturn inet_getpeer(&daddr, create);\n}\n\n/* can be called from BH context or outside */\nextern void inet_putpeer(struct inet_peer *p);\nextern bool inet_peer_xrlim_allow(struct inet_peer *peer, int timeout);\n\n/*\n * temporary check to make sure we dont access rid, ip_id_count, tcp_ts,\n * tcp_ts_stamp if no refcount is taken on inet_peer\n */\nstatic inline void inet_peer_refcheck(const struct inet_peer *p)\n{\n\tWARN_ON_ONCE(atomic_read(&p->refcnt) <= 0);\n}\n\n\n/* can be called with or without local BH being disabled */\nstatic inline __u16\tinet_getid(struct inet_peer *p, int more)\n{\n\tmore++;\n\tinet_peer_refcheck(p);\n\treturn atomic_add_return(more, &p->ip_id_count) - more;\n}\n\n#endif /* _NET_INETPEER_H */\n", "/*\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#ifndef _NET_IPV6_H\n#define _NET_IPV6_H\n\n#include <linux/ipv6.h>\n#include <linux/hardirq.h>\n#include <net/if_inet6.h>\n#include <net/ndisc.h>\n#include <net/flow.h>\n#include <net/snmp.h>\n\n#define SIN6_LEN_RFC2133\t24\n\n#define IPV6_MAXPLEN\t\t65535\n\n/*\n *\tNextHeader field of IPv6 header\n */\n\n#define NEXTHDR_HOP\t\t0\t/* Hop-by-hop option header. */\n#define NEXTHDR_TCP\t\t6\t/* TCP segment. */\n#define NEXTHDR_UDP\t\t17\t/* UDP message. */\n#define NEXTHDR_IPV6\t\t41\t/* IPv6 in IPv6 */\n#define NEXTHDR_ROUTING\t\t43\t/* Routing header. */\n#define NEXTHDR_FRAGMENT\t44\t/* Fragmentation/reassembly header. */\n#define NEXTHDR_ESP\t\t50\t/* Encapsulating security payload. */\n#define NEXTHDR_AUTH\t\t51\t/* Authentication header. */\n#define NEXTHDR_ICMP\t\t58\t/* ICMP for IPv6. */\n#define NEXTHDR_NONE\t\t59\t/* No next header */\n#define NEXTHDR_DEST\t\t60\t/* Destination options header. */\n#define NEXTHDR_MOBILITY\t135\t/* Mobility header. */\n\n#define NEXTHDR_MAX\t\t255\n\n\n\n#define IPV6_DEFAULT_HOPLIMIT   64\n#define IPV6_DEFAULT_MCASTHOPS\t1\n\n/*\n *\tAddr type\n *\t\n *\ttype\t-\tunicast | multicast\n *\tscope\t-\tlocal\t| site\t    | global\n *\tv4\t-\tcompat\n *\tv4mapped\n *\tany\n *\tloopback\n */\n\n#define IPV6_ADDR_ANY\t\t0x0000U\n\n#define IPV6_ADDR_UNICAST      \t0x0001U\t\n#define IPV6_ADDR_MULTICAST    \t0x0002U\t\n\n#define IPV6_ADDR_LOOPBACK\t0x0010U\n#define IPV6_ADDR_LINKLOCAL\t0x0020U\n#define IPV6_ADDR_SITELOCAL\t0x0040U\n\n#define IPV6_ADDR_COMPATv4\t0x0080U\n\n#define IPV6_ADDR_SCOPE_MASK\t0x00f0U\n\n#define IPV6_ADDR_MAPPED\t0x1000U\n\n/*\n *\tAddr scopes\n */\n#define IPV6_ADDR_MC_SCOPE(a)\t\\\n\t((a)->s6_addr[1] & 0x0f)\t/* nonstandard */\n#define __IPV6_ADDR_SCOPE_INVALID\t-1\n#define IPV6_ADDR_SCOPE_NODELOCAL\t0x01\n#define IPV6_ADDR_SCOPE_LINKLOCAL\t0x02\n#define IPV6_ADDR_SCOPE_SITELOCAL\t0x05\n#define IPV6_ADDR_SCOPE_ORGLOCAL\t0x08\n#define IPV6_ADDR_SCOPE_GLOBAL\t\t0x0e\n\n/*\n *\tAddr flags\n */\n#define IPV6_ADDR_MC_FLAG_TRANSIENT(a)\t\\\n\t((a)->s6_addr[1] & 0x10)\n#define IPV6_ADDR_MC_FLAG_PREFIX(a)\t\\\n\t((a)->s6_addr[1] & 0x20)\n#define IPV6_ADDR_MC_FLAG_RENDEZVOUS(a)\t\\\n\t((a)->s6_addr[1] & 0x40)\n\n/*\n *\tfragmentation header\n */\n\nstruct frag_hdr {\n\t__u8\tnexthdr;\n\t__u8\treserved;\n\t__be16\tfrag_off;\n\t__be32\tidentification;\n};\n\n#define\tIP6_MF\t0x0001\n\n#include <net/sock.h>\n\n/* sysctls */\nextern int sysctl_mld_max_msf;\nextern struct ctl_path net_ipv6_ctl_path[];\n\n#define _DEVINC(net, statname, modifier, idev, field)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_INC_STATS##modifier((_idev)->stats.statname, (field)); \\\n\tSNMP_INC_STATS##modifier((net)->mib.statname##_statistics, (field));\\\n})\n\n/* per device counters are atomic_long_t */\n#define _DEVINCATOMIC(net, statname, modifier, idev, field)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_INC_STATS_ATOMIC_LONG((_idev)->stats.statname##dev, (field)); \\\n\tSNMP_INC_STATS##modifier((net)->mib.statname##_statistics, (field));\\\n})\n\n#define _DEVADD(net, statname, modifier, idev, field, val)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_ADD_STATS##modifier((_idev)->stats.statname, (field), (val)); \\\n\tSNMP_ADD_STATS##modifier((net)->mib.statname##_statistics, (field), (val));\\\n})\n\n#define _DEVUPD(net, statname, modifier, idev, field, val)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_UPD_PO_STATS##modifier((_idev)->stats.statname, field, (val)); \\\n\tSNMP_UPD_PO_STATS##modifier((net)->mib.statname##_statistics, field, (val));\\\n})\n\n/* MIBs */\n\n#define IP6_INC_STATS(net, idev,field)\t\t\\\n\t\t_DEVINC(net, ipv6, 64, idev, field)\n#define IP6_INC_STATS_BH(net, idev,field)\t\\\n\t\t_DEVINC(net, ipv6, 64_BH, idev, field)\n#define IP6_ADD_STATS(net, idev,field,val)\t\\\n\t\t_DEVADD(net, ipv6, 64, idev, field, val)\n#define IP6_ADD_STATS_BH(net, idev,field,val)\t\\\n\t\t_DEVADD(net, ipv6, 64_BH, idev, field, val)\n#define IP6_UPD_PO_STATS(net, idev,field,val)   \\\n\t\t_DEVUPD(net, ipv6, 64, idev, field, val)\n#define IP6_UPD_PO_STATS_BH(net, idev,field,val)   \\\n\t\t_DEVUPD(net, ipv6, 64_BH, idev, field, val)\n#define ICMP6_INC_STATS(net, idev, field)\t\\\n\t\t_DEVINCATOMIC(net, icmpv6, , idev, field)\n#define ICMP6_INC_STATS_BH(net, idev, field)\t\\\n\t\t_DEVINCATOMIC(net, icmpv6, _BH, idev, field)\n\n#define ICMP6MSGOUT_INC_STATS(net, idev, field)\t\t\\\n\t_DEVINCATOMIC(net, icmpv6msg, , idev, field +256)\n#define ICMP6MSGOUT_INC_STATS_BH(net, idev, field)\t\\\n\t_DEVINCATOMIC(net, icmpv6msg, _BH, idev, field +256)\n#define ICMP6MSGIN_INC_STATS_BH(net, idev, field)\t\\\n\t_DEVINCATOMIC(net, icmpv6msg, _BH, idev, field)\n\nstruct ip6_ra_chain {\n\tstruct ip6_ra_chain\t*next;\n\tstruct sock\t\t*sk;\n\tint\t\t\tsel;\n\tvoid\t\t\t(*destructor)(struct sock *);\n};\n\nextern struct ip6_ra_chain\t*ip6_ra_chain;\nextern rwlock_t ip6_ra_lock;\n\n/*\n   This structure is prepared by protocol, when parsing\n   ancillary data and passed to IPv6.\n */\n\nstruct ipv6_txoptions {\n\t/* Length of this structure */\n\tint\t\t\ttot_len;\n\n\t/* length of extension headers   */\n\n\t__u16\t\t\topt_flen;\t/* after fragment hdr */\n\t__u16\t\t\topt_nflen;\t/* before fragment hdr */\n\n\tstruct ipv6_opt_hdr\t*hopopt;\n\tstruct ipv6_opt_hdr\t*dst0opt;\n\tstruct ipv6_rt_hdr\t*srcrt;\t/* Routing Header */\n\tstruct ipv6_opt_hdr\t*dst1opt;\n\n\t/* Option buffer, as read by IPV6_PKTOPTIONS, starts here. */\n};\n\nstruct ip6_flowlabel {\n\tstruct ip6_flowlabel\t*next;\n\t__be32\t\t\tlabel;\n\tatomic_t\t\tusers;\n\tstruct in6_addr\t\tdst;\n\tstruct ipv6_txoptions\t*opt;\n\tunsigned long\t\tlinger;\n\tu8\t\t\tshare;\n\tu32\t\t\towner;\n\tunsigned long\t\tlastuse;\n\tunsigned long\t\texpires;\n\tstruct net\t\t*fl_net;\n};\n\n#define IPV6_FLOWINFO_MASK\tcpu_to_be32(0x0FFFFFFF)\n#define IPV6_FLOWLABEL_MASK\tcpu_to_be32(0x000FFFFF)\n\nstruct ipv6_fl_socklist {\n\tstruct ipv6_fl_socklist\t*next;\n\tstruct ip6_flowlabel\t*fl;\n};\n\nextern struct ip6_flowlabel\t*fl6_sock_lookup(struct sock *sk, __be32 label);\nextern struct ipv6_txoptions\t*fl6_merge_options(struct ipv6_txoptions * opt_space,\n\t\t\t\t\t\t   struct ip6_flowlabel * fl,\n\t\t\t\t\t\t   struct ipv6_txoptions * fopt);\nextern void\t\t\tfl6_free_socklist(struct sock *sk);\nextern int\t\t\tipv6_flowlabel_opt(struct sock *sk, char __user *optval, int optlen);\nextern int\t\t\tip6_flowlabel_init(void);\nextern void\t\t\tip6_flowlabel_cleanup(void);\n\nstatic inline void fl6_sock_release(struct ip6_flowlabel *fl)\n{\n\tif (fl)\n\t\tatomic_dec(&fl->users);\n}\n\nextern int \t\t\tip6_ra_control(struct sock *sk, int sel);\n\nextern int\t\t\tipv6_parse_hopopts(struct sk_buff *skb);\n\nextern struct ipv6_txoptions *  ipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt);\nextern struct ipv6_txoptions *\tipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t\t\t\t\t   int newtype,\n\t\t\t\t\t\t   struct ipv6_opt_hdr __user *newopt,\n\t\t\t\t\t\t   int newoptlen);\nstruct ipv6_txoptions *ipv6_fixup_options(struct ipv6_txoptions *opt_space,\n\t\t\t\t\t  struct ipv6_txoptions *opt);\n\nextern int ipv6_opt_accepted(struct sock *sk, struct sk_buff *skb);\n\nint ip6_frag_nqueues(struct net *net);\nint ip6_frag_mem(struct net *net);\n\n#define IPV6_FRAG_HIGH_THRESH\t(256 * 1024)\t/* 262144 */\n#define IPV6_FRAG_LOW_THRESH\t(192 * 1024)\t/* 196608 */\n#define IPV6_FRAG_TIMEOUT\t(60 * HZ)\t/* 60 seconds */\n\nextern int __ipv6_addr_type(const struct in6_addr *addr);\nstatic inline int ipv6_addr_type(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_type(addr) & 0xffff;\n}\n\nstatic inline int ipv6_addr_scope(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_type(addr) & IPV6_ADDR_SCOPE_MASK;\n}\n\nstatic inline int __ipv6_addr_src_scope(int type)\n{\n\treturn (type == IPV6_ADDR_ANY) ? __IPV6_ADDR_SCOPE_INVALID : (type >> 16);\n}\n\nstatic inline int ipv6_addr_src_scope(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_src_scope(__ipv6_addr_type(addr));\n}\n\nstatic inline int ipv6_addr_cmp(const struct in6_addr *a1, const struct in6_addr *a2)\n{\n\treturn memcmp(a1, a2, sizeof(struct in6_addr));\n}\n\nstatic inline int\nipv6_masked_addr_cmp(const struct in6_addr *a1, const struct in6_addr *m,\n\t\t     const struct in6_addr *a2)\n{\n\treturn !!(((a1->s6_addr32[0] ^ a2->s6_addr32[0]) & m->s6_addr32[0]) |\n\t\t  ((a1->s6_addr32[1] ^ a2->s6_addr32[1]) & m->s6_addr32[1]) |\n\t\t  ((a1->s6_addr32[2] ^ a2->s6_addr32[2]) & m->s6_addr32[2]) |\n\t\t  ((a1->s6_addr32[3] ^ a2->s6_addr32[3]) & m->s6_addr32[3]));\n}\n\nstatic inline void ipv6_addr_copy(struct in6_addr *a1, const struct in6_addr *a2)\n{\n\tmemcpy(a1, a2, sizeof(struct in6_addr));\n}\n\nstatic inline void ipv6_addr_prefix(struct in6_addr *pfx, \n\t\t\t\t    const struct in6_addr *addr,\n\t\t\t\t    int plen)\n{\n\t/* caller must guarantee 0 <= plen <= 128 */\n\tint o = plen >> 3,\n\t    b = plen & 0x7;\n\n\tmemset(pfx->s6_addr, 0, sizeof(pfx->s6_addr));\n\tmemcpy(pfx->s6_addr, addr, o);\n\tif (b != 0)\n\t\tpfx->s6_addr[o] = addr->s6_addr[o] & (0xff00 >> b);\n}\n\nstatic inline void ipv6_addr_set(struct in6_addr *addr, \n\t\t\t\t     __be32 w1, __be32 w2,\n\t\t\t\t     __be32 w3, __be32 w4)\n{\n\taddr->s6_addr32[0] = w1;\n\taddr->s6_addr32[1] = w2;\n\taddr->s6_addr32[2] = w3;\n\taddr->s6_addr32[3] = w4;\n}\n\nstatic inline int ipv6_addr_equal(const struct in6_addr *a1,\n\t\t\t\t  const struct in6_addr *a2)\n{\n\treturn ((a1->s6_addr32[0] ^ a2->s6_addr32[0]) |\n\t\t(a1->s6_addr32[1] ^ a2->s6_addr32[1]) |\n\t\t(a1->s6_addr32[2] ^ a2->s6_addr32[2]) |\n\t\t(a1->s6_addr32[3] ^ a2->s6_addr32[3])) == 0;\n}\n\nstatic inline int __ipv6_prefix_equal(const __be32 *a1, const __be32 *a2,\n\t\t\t\t      unsigned int prefixlen)\n{\n\tunsigned pdw, pbi;\n\n\t/* check complete u32 in prefix */\n\tpdw = prefixlen >> 5;\n\tif (pdw && memcmp(a1, a2, pdw << 2))\n\t\treturn 0;\n\n\t/* check incomplete u32 in prefix */\n\tpbi = prefixlen & 0x1f;\n\tif (pbi && ((a1[pdw] ^ a2[pdw]) & htonl((0xffffffff) << (32 - pbi))))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic inline int ipv6_prefix_equal(const struct in6_addr *a1,\n\t\t\t\t    const struct in6_addr *a2,\n\t\t\t\t    unsigned int prefixlen)\n{\n\treturn __ipv6_prefix_equal(a1->s6_addr32, a2->s6_addr32,\n\t\t\t\t   prefixlen);\n}\n\nstruct inet_frag_queue;\n\nenum ip6_defrag_users {\n\tIP6_DEFRAG_LOCAL_DELIVER,\n\tIP6_DEFRAG_CONNTRACK_IN,\n\t__IP6_DEFRAG_CONNTRACK_IN\t= IP6_DEFRAG_CONNTRACK_IN + USHRT_MAX,\n\tIP6_DEFRAG_CONNTRACK_OUT,\n\t__IP6_DEFRAG_CONNTRACK_OUT\t= IP6_DEFRAG_CONNTRACK_OUT + USHRT_MAX,\n\tIP6_DEFRAG_CONNTRACK_BRIDGE_IN,\n\t__IP6_DEFRAG_CONNTRACK_BRIDGE_IN = IP6_DEFRAG_CONNTRACK_BRIDGE_IN + USHRT_MAX,\n};\n\nstruct ip6_create_arg {\n\t__be32 id;\n\tu32 user;\n\tconst struct in6_addr *src;\n\tconst struct in6_addr *dst;\n};\n\nvoid ip6_frag_init(struct inet_frag_queue *q, void *a);\nint ip6_frag_match(struct inet_frag_queue *q, void *a);\n\nstatic inline int ipv6_addr_any(const struct in6_addr *a)\n{\n\treturn (a->s6_addr32[0] | a->s6_addr32[1] |\n\t\ta->s6_addr32[2] | a->s6_addr32[3]) == 0;\n}\n\nstatic inline int ipv6_addr_loopback(const struct in6_addr *a)\n{\n\treturn (a->s6_addr32[0] | a->s6_addr32[1] |\n\t\ta->s6_addr32[2] | (a->s6_addr32[3] ^ htonl(1))) == 0;\n}\n\nstatic inline int ipv6_addr_v4mapped(const struct in6_addr *a)\n{\n\treturn (a->s6_addr32[0] | a->s6_addr32[1] |\n\t\t (a->s6_addr32[2] ^ htonl(0x0000ffff))) == 0;\n}\n\n/*\n * Check for a RFC 4843 ORCHID address\n * (Overlay Routable Cryptographic Hash Identifiers)\n */\nstatic inline int ipv6_addr_orchid(const struct in6_addr *a)\n{\n\treturn (a->s6_addr32[0] & htonl(0xfffffff0)) == htonl(0x20010010);\n}\n\nstatic inline void ipv6_addr_set_v4mapped(const __be32 addr,\n\t\t\t\t\t  struct in6_addr *v4mapped)\n{\n\tipv6_addr_set(v4mapped,\n\t\t\t0, 0,\n\t\t\thtonl(0x0000FFFF),\n\t\t\taddr);\n}\n\n/*\n * find the first different bit between two addresses\n * length of address must be a multiple of 32bits\n */\nstatic inline int __ipv6_addr_diff(const void *token1, const void *token2, int addrlen)\n{\n\tconst __be32 *a1 = token1, *a2 = token2;\n\tint i;\n\n\taddrlen >>= 2;\n\n\tfor (i = 0; i < addrlen; i++) {\n\t\t__be32 xb = a1[i] ^ a2[i];\n\t\tif (xb)\n\t\t\treturn i * 32 + 31 - __fls(ntohl(xb));\n\t}\n\n\t/*\n\t *\twe should *never* get to this point since that \n\t *\twould mean the addrs are equal\n\t *\n\t *\tHowever, we do get to it 8) And exacly, when\n\t *\taddresses are equal 8)\n\t *\n\t *\tip route add 1111::/128 via ...\n\t *\tip route add 1111::/64 via ...\n\t *\tand we are here.\n\t *\n\t *\tIdeally, this function should stop comparison\n\t *\tat prefix length. It does not, but it is still OK,\n\t *\tif returned value is greater than prefix length.\n\t *\t\t\t\t\t--ANK (980803)\n\t */\n\treturn addrlen << 5;\n}\n\nstatic inline int ipv6_addr_diff(const struct in6_addr *a1, const struct in6_addr *a2)\n{\n\treturn __ipv6_addr_diff(a1, a2, sizeof(struct in6_addr));\n}\n\nstatic __inline__ void ipv6_select_ident(struct frag_hdr *fhdr)\n{\n\tstatic u32 ipv6_fragmentation_id = 1;\n\tstatic DEFINE_SPINLOCK(ip6_id_lock);\n\n\tspin_lock_bh(&ip6_id_lock);\n\tfhdr->identification = htonl(ipv6_fragmentation_id);\n\tif (++ipv6_fragmentation_id == 0)\n\t\tipv6_fragmentation_id = 1;\n\tspin_unlock_bh(&ip6_id_lock);\n}\n\n/*\n *\tPrototypes exported by ipv6\n */\n\n/*\n *\trcv function (called from netdevice level)\n */\n\nextern int\t\t\tipv6_rcv(struct sk_buff *skb, \n\t\t\t\t\t struct net_device *dev, \n\t\t\t\t\t struct packet_type *pt,\n\t\t\t\t\t struct net_device *orig_dev);\n\nextern int\t\t\tip6_rcv_finish(struct sk_buff *skb);\n\n/*\n *\tupper-layer output functions\n */\nextern int\t\t\tip6_xmit(struct sock *sk,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct flowi6 *fl6,\n\t\t\t\t\t struct ipv6_txoptions *opt);\n\nextern int\t\t\tip6_nd_hdr(struct sock *sk,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   const struct in6_addr *saddr,\n\t\t\t\t\t   const struct in6_addr *daddr,\n\t\t\t\t\t   int proto, int len);\n\nextern int\t\t\tip6_find_1stfragopt(struct sk_buff *skb, u8 **nexthdr);\n\nextern int\t\t\tip6_append_data(struct sock *sk,\n\t\t\t\t\t\tint getfrag(void *from, char *to, int offset, int len, int odd, struct sk_buff *skb),\n\t\t    \t\t\t\tvoid *from,\n\t\t\t\t\t\tint length,\n\t\t\t\t\t\tint transhdrlen,\n\t\t      \t\t\t\tint hlimit,\n\t\t      \t\t\t\tint tclass,\n\t\t\t\t\t\tstruct ipv6_txoptions *opt,\n\t\t\t\t\t\tstruct flowi6 *fl6,\n\t\t\t\t\t\tstruct rt6_info *rt,\n\t\t\t\t\t\tunsigned int flags,\n\t\t\t\t\t\tint dontfrag);\n\nextern int\t\t\tip6_push_pending_frames(struct sock *sk);\n\nextern void\t\t\tip6_flush_pending_frames(struct sock *sk);\n\nextern int\t\t\tip6_dst_lookup(struct sock *sk,\n\t\t\t\t\t       struct dst_entry **dst,\n\t\t\t\t\t       struct flowi6 *fl6);\nextern struct dst_entry *\tip6_dst_lookup_flow(struct sock *sk,\n\t\t\t\t\t\t    struct flowi6 *fl6,\n\t\t\t\t\t\t    const struct in6_addr *final_dst,\n\t\t\t\t\t\t    bool can_sleep);\nextern struct dst_entry *\tip6_sk_dst_lookup_flow(struct sock *sk,\n\t\t\t\t\t\t       struct flowi6 *fl6,\n\t\t\t\t\t\t       const struct in6_addr *final_dst,\n\t\t\t\t\t\t       bool can_sleep);\nextern struct dst_entry *\tip6_blackhole_route(struct net *net,\n\t\t\t\t\t\t    struct dst_entry *orig_dst);\n\n/*\n *\tskb processing functions\n */\n\nextern int\t\t\tip6_output(struct sk_buff *skb);\nextern int\t\t\tip6_forward(struct sk_buff *skb);\nextern int\t\t\tip6_input(struct sk_buff *skb);\nextern int\t\t\tip6_mc_input(struct sk_buff *skb);\n\nextern int\t\t\t__ip6_local_out(struct sk_buff *skb);\nextern int\t\t\tip6_local_out(struct sk_buff *skb);\n\n/*\n *\tExtension header (options) processing\n */\n\nextern void \t\t\tipv6_push_nfrag_opts(struct sk_buff *skb,\n\t\t\t\t\t\t     struct ipv6_txoptions *opt,\n\t\t\t\t\t\t     u8 *proto,\n\t\t\t\t\t\t     struct in6_addr **daddr_p);\nextern void\t\t\tipv6_push_frag_opts(struct sk_buff *skb,\n\t\t\t\t\t\t    struct ipv6_txoptions *opt,\n\t\t\t\t\t\t    u8 *proto);\n\nextern int\t\t\tipv6_skip_exthdr(const struct sk_buff *, int start,\n\t\t\t\t\t         u8 *nexthdrp);\n\nextern int \t\t\tipv6_ext_hdr(u8 nexthdr);\n\nextern int ipv6_find_tlv(struct sk_buff *skb, int offset, int type);\n\nextern struct in6_addr *fl6_update_dst(struct flowi6 *fl6,\n\t\t\t\t       const struct ipv6_txoptions *opt,\n\t\t\t\t       struct in6_addr *orig);\n\n/*\n *\tsocket options (ipv6_sockglue.c)\n */\n\nextern int\t\t\tipv6_setsockopt(struct sock *sk, int level, \n\t\t\t\t\t\tint optname,\n\t\t\t\t\t\tchar __user *optval, \n\t\t\t\t\t\tunsigned int optlen);\nextern int\t\t\tipv6_getsockopt(struct sock *sk, int level, \n\t\t\t\t\t\tint optname,\n\t\t\t\t\t\tchar __user *optval, \n\t\t\t\t\t\tint __user *optlen);\nextern int\t\t\tcompat_ipv6_setsockopt(struct sock *sk,\n\t\t\t\t\t\tint level,\n\t\t\t\t\t\tint optname,\n\t\t\t\t\t\tchar __user *optval,\n\t\t\t\t\t\tunsigned int optlen);\nextern int\t\t\tcompat_ipv6_getsockopt(struct sock *sk,\n\t\t\t\t\t\tint level,\n\t\t\t\t\t\tint optname,\n\t\t\t\t\t\tchar __user *optval,\n\t\t\t\t\t\tint __user *optlen);\n\nextern int\t\t\tip6_datagram_connect(struct sock *sk, \n\t\t\t\t\t\t     struct sockaddr *addr, int addr_len);\n\nextern int \t\t\tipv6_recv_error(struct sock *sk, struct msghdr *msg, int len);\nextern int \t\t\tipv6_recv_rxpmtu(struct sock *sk, struct msghdr *msg, int len);\nextern void\t\t\tipv6_icmp_error(struct sock *sk, struct sk_buff *skb, int err, __be16 port,\n\t\t\t\t\t\tu32 info, u8 *payload);\nextern void\t\t\tipv6_local_error(struct sock *sk, int err, struct flowi6 *fl6, u32 info);\nextern void\t\t\tipv6_local_rxpmtu(struct sock *sk, struct flowi6 *fl6, u32 mtu);\n\nextern int inet6_release(struct socket *sock);\nextern int inet6_bind(struct socket *sock, struct sockaddr *uaddr, \n\t\t      int addr_len);\nextern int inet6_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t int *uaddr_len, int peer);\nextern int inet6_ioctl(struct socket *sock, unsigned int cmd, \n\t\t       unsigned long arg);\n\nextern int inet6_hash_connect(struct inet_timewait_death_row *death_row,\n\t\t\t      struct sock *sk);\n\n/*\n * reassembly.c\n */\nextern const struct proto_ops inet6_stream_ops;\nextern const struct proto_ops inet6_dgram_ops;\n\nstruct group_source_req;\nstruct group_filter;\n\nextern int ip6_mc_source(int add, int omode, struct sock *sk,\n\t\t\t struct group_source_req *pgsr);\nextern int ip6_mc_msfilter(struct sock *sk, struct group_filter *gsf);\nextern int ip6_mc_msfget(struct sock *sk, struct group_filter *gsf,\n\t\t\t struct group_filter __user *optval,\n\t\t\t int __user *optlen);\nextern unsigned int inet6_hash_frag(__be32 id, const struct in6_addr *saddr,\n\t\t\t\t    const struct in6_addr *daddr, u32 rnd);\n\n#ifdef CONFIG_PROC_FS\nextern int  ac6_proc_init(struct net *net);\nextern void ac6_proc_exit(struct net *net);\nextern int  raw6_proc_init(void);\nextern void raw6_proc_exit(void);\nextern int  tcp6_proc_init(struct net *net);\nextern void tcp6_proc_exit(struct net *net);\nextern int  udp6_proc_init(struct net *net);\nextern void udp6_proc_exit(struct net *net);\nextern int  udplite6_proc_init(void);\nextern void udplite6_proc_exit(void);\nextern int  ipv6_misc_proc_init(void);\nextern void ipv6_misc_proc_exit(void);\nextern int snmp6_register_dev(struct inet6_dev *idev);\nextern int snmp6_unregister_dev(struct inet6_dev *idev);\n\n#else\nstatic inline int ac6_proc_init(struct net *net) { return 0; }\nstatic inline void ac6_proc_exit(struct net *net) { }\nstatic inline int snmp6_register_dev(struct inet6_dev *idev) { return 0; }\nstatic inline int snmp6_unregister_dev(struct inet6_dev *idev) { return 0; }\n#endif\n\n#ifdef CONFIG_SYSCTL\nextern ctl_table ipv6_route_table_template[];\nextern ctl_table ipv6_icmp_table_template[];\n\nextern struct ctl_table *ipv6_icmp_sysctl_init(struct net *net);\nextern struct ctl_table *ipv6_route_sysctl_init(struct net *net);\nextern int ipv6_sysctl_register(void);\nextern void ipv6_sysctl_unregister(void);\nextern int ipv6_static_sysctl_register(void);\nextern void ipv6_static_sysctl_unregister(void);\n#endif\n\n#endif /* _NET_IPV6_H */\n", "/*\n *\t\tINETPEER - A storage for permanent information about peers\n *\n *  This source is covered by the GNU GPL, the same as all kernel sources.\n *\n *  Authors:\tAndrey V. Savochkin <saw@msu.ru>\n */\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/random.h>\n#include <linux/timer.h>\n#include <linux/time.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/net.h>\n#include <net/ip.h>\n#include <net/inetpeer.h>\n\n/*\n *  Theory of operations.\n *  We keep one entry for each peer IP address.  The nodes contains long-living\n *  information about the peer which doesn't depend on routes.\n *  At this moment this information consists only of ID field for the next\n *  outgoing IP packet.  This field is incremented with each packet as encoded\n *  in inet_getid() function (include/net/inetpeer.h).\n *  At the moment of writing this notes identifier of IP packets is generated\n *  to be unpredictable using this code only for packets subjected\n *  (actually or potentially) to defragmentation.  I.e. DF packets less than\n *  PMTU in size uses a constant ID and do not use this code (see\n *  ip_select_ident() in include/net/ip.h).\n *\n *  Route cache entries hold references to our nodes.\n *  New cache entries get references via lookup by destination IP address in\n *  the avl tree.  The reference is grabbed only when it's needed i.e. only\n *  when we try to output IP packet which needs an unpredictable ID (see\n *  __ip_select_ident() in net/ipv4/route.c).\n *  Nodes are removed only when reference counter goes to 0.\n *  When it's happened the node may be removed when a sufficient amount of\n *  time has been passed since its last use.  The less-recently-used entry can\n *  also be removed if the pool is overloaded i.e. if the total amount of\n *  entries is greater-or-equal than the threshold.\n *\n *  Node pool is organised as an AVL tree.\n *  Such an implementation has been chosen not just for fun.  It's a way to\n *  prevent easy and efficient DoS attacks by creating hash collisions.  A huge\n *  amount of long living nodes in a single hash slot would significantly delay\n *  lookups performed with disabled BHs.\n *\n *  Serialisation issues.\n *  1.  Nodes may appear in the tree only with the pool lock held.\n *  2.  Nodes may disappear from the tree only with the pool lock held\n *      AND reference count being 0.\n *  3.  Global variable peer_total is modified under the pool lock.\n *  4.  struct inet_peer fields modification:\n *\t\tavl_left, avl_right, avl_parent, avl_height: pool lock\n *\t\trefcnt: atomically against modifications on other CPU;\n *\t\t   usually under some other lock to prevent node disappearing\n *\t\tdaddr: unchangeable\n *\t\tip_id_count: atomic value (no lock needed)\n */\n\nstatic struct kmem_cache *peer_cachep __read_mostly;\n\n#define node_height(x) x->avl_height\n\n#define peer_avl_empty ((struct inet_peer *)&peer_fake_node)\n#define peer_avl_empty_rcu ((struct inet_peer __rcu __force *)&peer_fake_node)\nstatic const struct inet_peer peer_fake_node = {\n\t.avl_left\t= peer_avl_empty_rcu,\n\t.avl_right\t= peer_avl_empty_rcu,\n\t.avl_height\t= 0\n};\n\nstruct inet_peer_base {\n\tstruct inet_peer __rcu *root;\n\tseqlock_t\tlock;\n\tint\t\ttotal;\n};\n\nstatic struct inet_peer_base v4_peers = {\n\t.root\t\t= peer_avl_empty_rcu,\n\t.lock\t\t= __SEQLOCK_UNLOCKED(v4_peers.lock),\n\t.total\t\t= 0,\n};\n\nstatic struct inet_peer_base v6_peers = {\n\t.root\t\t= peer_avl_empty_rcu,\n\t.lock\t\t= __SEQLOCK_UNLOCKED(v6_peers.lock),\n\t.total\t\t= 0,\n};\n\n#define PEER_MAXDEPTH 40 /* sufficient for about 2^27 nodes */\n\n/* Exported for sysctl_net_ipv4.  */\nint inet_peer_threshold __read_mostly = 65536 + 128;\t/* start to throw entries more\n\t\t\t\t\t * aggressively at this stage */\nint inet_peer_minttl __read_mostly = 120 * HZ;\t/* TTL under high load: 120 sec */\nint inet_peer_maxttl __read_mostly = 10 * 60 * HZ;\t/* usual time to live: 10 min */\n\n\n/* Called from ip_output.c:ip_init  */\nvoid __init inet_initpeers(void)\n{\n\tstruct sysinfo si;\n\n\t/* Use the straight interface to information about memory. */\n\tsi_meminfo(&si);\n\t/* The values below were suggested by Alexey Kuznetsov\n\t * <kuznet@ms2.inr.ac.ru>.  I don't have any opinion about the values\n\t * myself.  --SAW\n\t */\n\tif (si.totalram <= (32768*1024)/PAGE_SIZE)\n\t\tinet_peer_threshold >>= 1; /* max pool size about 1MB on IA32 */\n\tif (si.totalram <= (16384*1024)/PAGE_SIZE)\n\t\tinet_peer_threshold >>= 1; /* about 512KB */\n\tif (si.totalram <= (8192*1024)/PAGE_SIZE)\n\t\tinet_peer_threshold >>= 2; /* about 128KB */\n\n\tpeer_cachep = kmem_cache_create(\"inet_peer_cache\",\n\t\t\tsizeof(struct inet_peer),\n\t\t\t0, SLAB_HWCACHE_ALIGN | SLAB_PANIC,\n\t\t\tNULL);\n\n}\n\nstatic int addr_compare(const struct inetpeer_addr *a,\n\t\t\tconst struct inetpeer_addr *b)\n{\n\tint i, n = (a->family == AF_INET ? 1 : 4);\n\n\tfor (i = 0; i < n; i++) {\n\t\tif (a->addr.a6[i] == b->addr.a6[i])\n\t\t\tcontinue;\n\t\tif (a->addr.a6[i] < b->addr.a6[i])\n\t\t\treturn -1;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n#define rcu_deref_locked(X, BASE)\t\t\t\t\\\n\trcu_dereference_protected(X, lockdep_is_held(&(BASE)->lock.lock))\n\n/*\n * Called with local BH disabled and the pool lock held.\n */\n#define lookup(_daddr, _stack, _base)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tstruct inet_peer *u;\t\t\t\t\t\\\n\tstruct inet_peer __rcu **v;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tstackptr = _stack;\t\t\t\t\t\\\n\t*stackptr++ = &_base->root;\t\t\t\t\\\n\tfor (u = rcu_deref_locked(_base->root, _base);\t\t\\\n\t     u != peer_avl_empty; ) {\t\t\t\t\\\n\t\tint cmp = addr_compare(_daddr, &u->daddr);\t\\\n\t\tif (cmp == 0)\t\t\t\t\t\\\n\t\t\tbreak;\t\t\t\t\t\\\n\t\tif (cmp == -1)\t\t\t\t\t\\\n\t\t\tv = &u->avl_left;\t\t\t\\\n\t\telse\t\t\t\t\t\t\\\n\t\t\tv = &u->avl_right;\t\t\t\\\n\t\t*stackptr++ = v;\t\t\t\t\\\n\t\tu = rcu_deref_locked(*v, _base);\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tu;\t\t\t\t\t\t\t\\\n})\n\n/*\n * Called with rcu_read_lock()\n * Because we hold no lock against a writer, its quite possible we fall\n * in an endless loop.\n * But every pointer we follow is guaranteed to be valid thanks to RCU.\n * We exit from this function if number of links exceeds PEER_MAXDEPTH\n */\nstatic struct inet_peer *lookup_rcu(const struct inetpeer_addr *daddr,\n\t\t\t\t    struct inet_peer_base *base)\n{\n\tstruct inet_peer *u = rcu_dereference(base->root);\n\tint count = 0;\n\n\twhile (u != peer_avl_empty) {\n\t\tint cmp = addr_compare(daddr, &u->daddr);\n\t\tif (cmp == 0) {\n\t\t\t/* Before taking a reference, check if this entry was\n\t\t\t * deleted (refcnt=-1)\n\t\t\t */\n\t\t\tif (!atomic_add_unless(&u->refcnt, 1, -1))\n\t\t\t\tu = NULL;\n\t\t\treturn u;\n\t\t}\n\t\tif (cmp == -1)\n\t\t\tu = rcu_dereference(u->avl_left);\n\t\telse\n\t\t\tu = rcu_dereference(u->avl_right);\n\t\tif (unlikely(++count == PEER_MAXDEPTH))\n\t\t\tbreak;\n\t}\n\treturn NULL;\n}\n\n/* Called with local BH disabled and the pool lock held. */\n#define lookup_rightempty(start, base)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tstruct inet_peer *u;\t\t\t\t\t\\\n\tstruct inet_peer __rcu **v;\t\t\t\t\\\n\t*stackptr++ = &start->avl_left;\t\t\t\t\\\n\tv = &start->avl_left;\t\t\t\t\t\\\n\tfor (u = rcu_deref_locked(*v, base);\t\t\t\\\n\t     u->avl_right != peer_avl_empty_rcu; ) {\t\t\\\n\t\tv = &u->avl_right;\t\t\t\t\\\n\t\t*stackptr++ = v;\t\t\t\t\\\n\t\tu = rcu_deref_locked(*v, base);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tu;\t\t\t\t\t\t\t\\\n})\n\n/* Called with local BH disabled and the pool lock held.\n * Variable names are the proof of operation correctness.\n * Look into mm/map_avl.c for more detail description of the ideas.\n */\nstatic void peer_avl_rebalance(struct inet_peer __rcu **stack[],\n\t\t\t       struct inet_peer __rcu ***stackend,\n\t\t\t       struct inet_peer_base *base)\n{\n\tstruct inet_peer __rcu **nodep;\n\tstruct inet_peer *node, *l, *r;\n\tint lh, rh;\n\n\twhile (stackend > stack) {\n\t\tnodep = *--stackend;\n\t\tnode = rcu_deref_locked(*nodep, base);\n\t\tl = rcu_deref_locked(node->avl_left, base);\n\t\tr = rcu_deref_locked(node->avl_right, base);\n\t\tlh = node_height(l);\n\t\trh = node_height(r);\n\t\tif (lh > rh + 1) { /* l: RH+2 */\n\t\t\tstruct inet_peer *ll, *lr, *lrl, *lrr;\n\t\t\tint lrh;\n\t\t\tll = rcu_deref_locked(l->avl_left, base);\n\t\t\tlr = rcu_deref_locked(l->avl_right, base);\n\t\t\tlrh = node_height(lr);\n\t\t\tif (lrh <= node_height(ll)) {\t/* ll: RH+1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_left, lr);\t/* lr: RH or RH+1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_right, r);\t/* r: RH */\n\t\t\t\tnode->avl_height = lrh + 1; /* RH+1 or RH+2 */\n\t\t\t\tRCU_INIT_POINTER(l->avl_left, ll);       /* ll: RH+1 */\n\t\t\t\tRCU_INIT_POINTER(l->avl_right, node);\t/* node: RH+1 or RH+2 */\n\t\t\t\tl->avl_height = node->avl_height + 1;\n\t\t\t\tRCU_INIT_POINTER(*nodep, l);\n\t\t\t} else { /* ll: RH, lr: RH+1 */\n\t\t\t\tlrl = rcu_deref_locked(lr->avl_left, base);/* lrl: RH or RH-1 */\n\t\t\t\tlrr = rcu_deref_locked(lr->avl_right, base);/* lrr: RH or RH-1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_left, lrr);\t/* lrr: RH or RH-1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_right, r);\t/* r: RH */\n\t\t\t\tnode->avl_height = rh + 1; /* node: RH+1 */\n\t\t\t\tRCU_INIT_POINTER(l->avl_left, ll);\t/* ll: RH */\n\t\t\t\tRCU_INIT_POINTER(l->avl_right, lrl);\t/* lrl: RH or RH-1 */\n\t\t\t\tl->avl_height = rh + 1;\t/* l: RH+1 */\n\t\t\t\tRCU_INIT_POINTER(lr->avl_left, l);\t/* l: RH+1 */\n\t\t\t\tRCU_INIT_POINTER(lr->avl_right, node);\t/* node: RH+1 */\n\t\t\t\tlr->avl_height = rh + 2;\n\t\t\t\tRCU_INIT_POINTER(*nodep, lr);\n\t\t\t}\n\t\t} else if (rh > lh + 1) { /* r: LH+2 */\n\t\t\tstruct inet_peer *rr, *rl, *rlr, *rll;\n\t\t\tint rlh;\n\t\t\trr = rcu_deref_locked(r->avl_right, base);\n\t\t\trl = rcu_deref_locked(r->avl_left, base);\n\t\t\trlh = node_height(rl);\n\t\t\tif (rlh <= node_height(rr)) {\t/* rr: LH+1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_right, rl);\t/* rl: LH or LH+1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_left, l);\t/* l: LH */\n\t\t\t\tnode->avl_height = rlh + 1; /* LH+1 or LH+2 */\n\t\t\t\tRCU_INIT_POINTER(r->avl_right, rr);\t/* rr: LH+1 */\n\t\t\t\tRCU_INIT_POINTER(r->avl_left, node);\t/* node: LH+1 or LH+2 */\n\t\t\t\tr->avl_height = node->avl_height + 1;\n\t\t\t\tRCU_INIT_POINTER(*nodep, r);\n\t\t\t} else { /* rr: RH, rl: RH+1 */\n\t\t\t\trlr = rcu_deref_locked(rl->avl_right, base);/* rlr: LH or LH-1 */\n\t\t\t\trll = rcu_deref_locked(rl->avl_left, base);/* rll: LH or LH-1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_right, rll);\t/* rll: LH or LH-1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_left, l);\t/* l: LH */\n\t\t\t\tnode->avl_height = lh + 1; /* node: LH+1 */\n\t\t\t\tRCU_INIT_POINTER(r->avl_right, rr);\t/* rr: LH */\n\t\t\t\tRCU_INIT_POINTER(r->avl_left, rlr);\t/* rlr: LH or LH-1 */\n\t\t\t\tr->avl_height = lh + 1;\t/* r: LH+1 */\n\t\t\t\tRCU_INIT_POINTER(rl->avl_right, r);\t/* r: LH+1 */\n\t\t\t\tRCU_INIT_POINTER(rl->avl_left, node);\t/* node: LH+1 */\n\t\t\t\trl->avl_height = lh + 2;\n\t\t\t\tRCU_INIT_POINTER(*nodep, rl);\n\t\t\t}\n\t\t} else {\n\t\t\tnode->avl_height = (lh > rh ? lh : rh) + 1;\n\t\t}\n\t}\n}\n\n/* Called with local BH disabled and the pool lock held. */\n#define link_to_pool(n, base)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tn->avl_height = 1;\t\t\t\t\t\\\n\tn->avl_left = peer_avl_empty_rcu;\t\t\t\\\n\tn->avl_right = peer_avl_empty_rcu;\t\t\t\\\n\t/* lockless readers can catch us now */\t\t\t\\\n\trcu_assign_pointer(**--stackptr, n);\t\t\t\\\n\tpeer_avl_rebalance(stack, stackptr, base);\t\t\\\n} while (0)\n\nstatic void inetpeer_free_rcu(struct rcu_head *head)\n{\n\tkmem_cache_free(peer_cachep, container_of(head, struct inet_peer, rcu));\n}\n\nstatic void unlink_from_pool(struct inet_peer *p, struct inet_peer_base *base,\n\t\t\t     struct inet_peer __rcu **stack[PEER_MAXDEPTH])\n{\n\tstruct inet_peer __rcu ***stackptr, ***delp;\n\n\tif (lookup(&p->daddr, stack, base) != p)\n\t\tBUG();\n\tdelp = stackptr - 1; /* *delp[0] == p */\n\tif (p->avl_left == peer_avl_empty_rcu) {\n\t\t*delp[0] = p->avl_right;\n\t\t--stackptr;\n\t} else {\n\t\t/* look for a node to insert instead of p */\n\t\tstruct inet_peer *t;\n\t\tt = lookup_rightempty(p, base);\n\t\tBUG_ON(rcu_deref_locked(*stackptr[-1], base) != t);\n\t\t**--stackptr = t->avl_left;\n\t\t/* t is removed, t->daddr > x->daddr for any\n\t\t * x in p->avl_left subtree.\n\t\t * Put t in the old place of p. */\n\t\tRCU_INIT_POINTER(*delp[0], t);\n\t\tt->avl_left = p->avl_left;\n\t\tt->avl_right = p->avl_right;\n\t\tt->avl_height = p->avl_height;\n\t\tBUG_ON(delp[1] != &p->avl_left);\n\t\tdelp[1] = &t->avl_left; /* was &p->avl_left */\n\t}\n\tpeer_avl_rebalance(stack, stackptr, base);\n\tbase->total--;\n\tcall_rcu(&p->rcu, inetpeer_free_rcu);\n}\n\nstatic struct inet_peer_base *family_to_base(int family)\n{\n\treturn family == AF_INET ? &v4_peers : &v6_peers;\n}\n\n/* perform garbage collect on all items stacked during a lookup */\nstatic int inet_peer_gc(struct inet_peer_base *base,\n\t\t\tstruct inet_peer __rcu **stack[PEER_MAXDEPTH],\n\t\t\tstruct inet_peer __rcu ***stackptr)\n{\n\tstruct inet_peer *p, *gchead = NULL;\n\t__u32 delta, ttl;\n\tint cnt = 0;\n\n\tif (base->total >= inet_peer_threshold)\n\t\tttl = 0; /* be aggressive */\n\telse\n\t\tttl = inet_peer_maxttl\n\t\t\t\t- (inet_peer_maxttl - inet_peer_minttl) / HZ *\n\t\t\t\t\tbase->total / inet_peer_threshold * HZ;\n\tstackptr--; /* last stack slot is peer_avl_empty */\n\twhile (stackptr > stack) {\n\t\tstackptr--;\n\t\tp = rcu_deref_locked(**stackptr, base);\n\t\tif (atomic_read(&p->refcnt) == 0) {\n\t\t\tsmp_rmb();\n\t\t\tdelta = (__u32)jiffies - p->dtime;\n\t\t\tif (delta >= ttl &&\n\t\t\t    atomic_cmpxchg(&p->refcnt, 0, -1) == 0) {\n\t\t\t\tp->gc_next = gchead;\n\t\t\t\tgchead = p;\n\t\t\t}\n\t\t}\n\t}\n\twhile ((p = gchead) != NULL) {\n\t\tgchead = p->gc_next;\n\t\tcnt++;\n\t\tunlink_from_pool(p, base, stack);\n\t}\n\treturn cnt;\n}\n\nstruct inet_peer *inet_getpeer(struct inetpeer_addr *daddr, int create)\n{\n\tstruct inet_peer __rcu **stack[PEER_MAXDEPTH], ***stackptr;\n\tstruct inet_peer_base *base = family_to_base(daddr->family);\n\tstruct inet_peer *p;\n\tunsigned int sequence;\n\tint invalidated, gccnt = 0;\n\n\t/* Attempt a lockless lookup first.\n\t * Because of a concurrent writer, we might not find an existing entry.\n\t */\n\trcu_read_lock();\n\tsequence = read_seqbegin(&base->lock);\n\tp = lookup_rcu(daddr, base);\n\tinvalidated = read_seqretry(&base->lock, sequence);\n\trcu_read_unlock();\n\n\tif (p)\n\t\treturn p;\n\n\t/* If no writer did a change during our lookup, we can return early. */\n\tif (!create && !invalidated)\n\t\treturn NULL;\n\n\t/* retry an exact lookup, taking the lock before.\n\t * At least, nodes should be hot in our cache.\n\t */\n\twrite_seqlock_bh(&base->lock);\nrelookup:\n\tp = lookup(daddr, stack, base);\n\tif (p != peer_avl_empty) {\n\t\tatomic_inc(&p->refcnt);\n\t\twrite_sequnlock_bh(&base->lock);\n\t\treturn p;\n\t}\n\tif (!gccnt) {\n\t\tgccnt = inet_peer_gc(base, stack, stackptr);\n\t\tif (gccnt && create)\n\t\t\tgoto relookup;\n\t}\n\tp = create ? kmem_cache_alloc(peer_cachep, GFP_ATOMIC) : NULL;\n\tif (p) {\n\t\tp->daddr = *daddr;\n\t\tatomic_set(&p->refcnt, 1);\n\t\tatomic_set(&p->rid, 0);\n\t\tatomic_set(&p->ip_id_count, secure_ip_id(daddr->addr.a4));\n\t\tp->tcp_ts_stamp = 0;\n\t\tp->metrics[RTAX_LOCK-1] = INETPEER_METRICS_NEW;\n\t\tp->rate_tokens = 0;\n\t\tp->rate_last = 0;\n\t\tp->pmtu_expires = 0;\n\t\tp->pmtu_orig = 0;\n\t\tmemset(&p->redirect_learned, 0, sizeof(p->redirect_learned));\n\n\n\t\t/* Link the node. */\n\t\tlink_to_pool(p, base);\n\t\tbase->total++;\n\t}\n\twrite_sequnlock_bh(&base->lock);\n\n\treturn p;\n}\nEXPORT_SYMBOL_GPL(inet_getpeer);\n\nvoid inet_putpeer(struct inet_peer *p)\n{\n\tp->dtime = (__u32)jiffies;\n\tsmp_mb__before_atomic_dec();\n\tatomic_dec(&p->refcnt);\n}\nEXPORT_SYMBOL_GPL(inet_putpeer);\n\n/*\n *\tCheck transmit rate limitation for given message.\n *\tThe rate information is held in the inet_peer entries now.\n *\tThis function is generic and could be used for other purposes\n *\ttoo. It uses a Token bucket filter as suggested by Alexey Kuznetsov.\n *\n *\tNote that the same inet_peer fields are modified by functions in\n *\troute.c too, but these work for packet destinations while xrlim_allow\n *\tworks for icmp destinations. This means the rate limiting information\n *\tfor one \"ip object\" is shared - and these ICMPs are twice limited:\n *\tby source and by destination.\n *\n *\tRFC 1812: 4.3.2.8 SHOULD be able to limit error message rate\n *\t\t\t  SHOULD allow setting of rate limits\n *\n * \tShared between ICMPv4 and ICMPv6.\n */\n#define XRLIM_BURST_FACTOR 6\nbool inet_peer_xrlim_allow(struct inet_peer *peer, int timeout)\n{\n\tunsigned long now, token;\n\tbool rc = false;\n\n\tif (!peer)\n\t\treturn true;\n\n\ttoken = peer->rate_tokens;\n\tnow = jiffies;\n\ttoken += now - peer->rate_last;\n\tpeer->rate_last = now;\n\tif (token > XRLIM_BURST_FACTOR * timeout)\n\t\ttoken = XRLIM_BURST_FACTOR * timeout;\n\tif (token >= timeout) {\n\t\ttoken -= timeout;\n\t\trc = true;\n\t}\n\tpeer->rate_tokens = token;\n\treturn rc;\n}\nEXPORT_SYMBOL(inet_peer_xrlim_allow);\n", "/*\n *\tIPv6 output functions\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on linux/net/ipv4/ip_output.c\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n *\n *\tChanges:\n *\tA.N.Kuznetsov\t:\tairthmetics in fragmentation.\n *\t\t\t\textension headers are implemented.\n *\t\t\t\troute changes now work.\n *\t\t\t\tip6_forward does not confuse sniffers.\n *\t\t\t\tetc.\n *\n *      H. von Brand    :       Added missing #include <linux/string.h>\n *\tImran Patel\t: \tfrag id should be in NBO\n *      Kazunori MIYAZAWA @USAGI\n *\t\t\t:       add ip6_append_data and related functions\n *\t\t\t\tfor datagram xmit\n */\n\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/socket.h>\n#include <linux/net.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/in6.h>\n#include <linux/tcp.h>\n#include <linux/route.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n\n#include <linux/netfilter.h>\n#include <linux/netfilter_ipv6.h>\n\n#include <net/sock.h>\n#include <net/snmp.h>\n\n#include <net/ipv6.h>\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/ip6_route.h>\n#include <net/addrconf.h>\n#include <net/rawv6.h>\n#include <net/icmp.h>\n#include <net/xfrm.h>\n#include <net/checksum.h>\n#include <linux/mroute6.h>\n\nint ip6_fragment(struct sk_buff *skb, int (*output)(struct sk_buff *));\n\nint __ip6_local_out(struct sk_buff *skb)\n{\n\tint len;\n\n\tlen = skb->len - sizeof(struct ipv6hdr);\n\tif (len > IPV6_MAXPLEN)\n\t\tlen = 0;\n\tipv6_hdr(skb)->payload_len = htons(len);\n\n\treturn nf_hook(NFPROTO_IPV6, NF_INET_LOCAL_OUT, skb, NULL,\n\t\t       skb_dst(skb)->dev, dst_output);\n}\n\nint ip6_local_out(struct sk_buff *skb)\n{\n\tint err;\n\n\terr = __ip6_local_out(skb);\n\tif (likely(err == 1))\n\t\terr = dst_output(skb);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ip6_local_out);\n\n/* dev_loopback_xmit for use with netfilter. */\nstatic int ip6_dev_loopback_xmit(struct sk_buff *newskb)\n{\n\tskb_reset_mac_header(newskb);\n\t__skb_pull(newskb, skb_network_offset(newskb));\n\tnewskb->pkt_type = PACKET_LOOPBACK;\n\tnewskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tWARN_ON(!skb_dst(newskb));\n\n\tnetif_rx_ni(newskb);\n\treturn 0;\n}\n\nstatic int ip6_finish_output2(struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct net_device *dev = dst->dev;\n\tstruct neighbour *neigh;\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb->dev = dev;\n\n\tif (ipv6_addr_is_multicast(&ipv6_hdr(skb)->daddr)) {\n\t\tstruct inet6_dev *idev = ip6_dst_idev(skb_dst(skb));\n\n\t\tif (!(dev->flags & IFF_LOOPBACK) && sk_mc_loop(skb->sk) &&\n\t\t    ((mroute6_socket(dev_net(dev), skb) &&\n\t\t     !(IP6CB(skb)->flags & IP6SKB_FORWARDED)) ||\n\t\t     ipv6_chk_mcast_addr(dev, &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t &ipv6_hdr(skb)->saddr))) {\n\t\t\tstruct sk_buff *newskb = skb_clone(skb, GFP_ATOMIC);\n\n\t\t\t/* Do not check for IFF_ALLMULTI; multicast routing\n\t\t\t   is not supported in any case.\n\t\t\t */\n\t\t\tif (newskb)\n\t\t\t\tNF_HOOK(NFPROTO_IPV6, NF_INET_POST_ROUTING,\n\t\t\t\t\tnewskb, NULL, newskb->dev,\n\t\t\t\t\tip6_dev_loopback_xmit);\n\n\t\t\tif (ipv6_hdr(skb)->hop_limit == 0) {\n\t\t\t\tIP6_INC_STATS(dev_net(dev), idev,\n\t\t\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\tIP6_UPD_PO_STATS(dev_net(dev), idev, IPSTATS_MIB_OUTMCAST,\n\t\t\t\tskb->len);\n\t}\n\n\tneigh = dst_get_neighbour(dst);\n\tif (neigh)\n\t\treturn neigh_output(neigh, skb);\n\n\tIP6_INC_STATS_BH(dev_net(dst->dev),\n\t\t\t ip6_dst_idev(dst), IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}\n\nstatic int ip6_finish_output(struct sk_buff *skb)\n{\n\tif ((skb->len > ip6_skb_dst_mtu(skb) && !skb_is_gso(skb)) ||\n\t    dst_allfrag(skb_dst(skb)))\n\t\treturn ip6_fragment(skb, ip6_finish_output2);\n\telse\n\t\treturn ip6_finish_output2(skb);\n}\n\nint ip6_output(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb_dst(skb)->dev;\n\tstruct inet6_dev *idev = ip6_dst_idev(skb_dst(skb));\n\tif (unlikely(idev->cnf.disable_ipv6)) {\n\t\tIP6_INC_STATS(dev_net(dev), idev,\n\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t\tkfree_skb(skb);\n\t\treturn 0;\n\t}\n\n\treturn NF_HOOK_COND(NFPROTO_IPV6, NF_INET_POST_ROUTING, skb, NULL, dev,\n\t\t\t    ip6_finish_output,\n\t\t\t    !(IP6CB(skb)->flags & IP6SKB_REROUTED));\n}\n\n/*\n *\txmit an sk_buff (used by TCP, SCTP and DCCP)\n */\n\nint ip6_xmit(struct sock *sk, struct sk_buff *skb, struct flowi6 *fl6,\n\t     struct ipv6_txoptions *opt)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *first_hop = &fl6->daddr;\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct ipv6hdr *hdr;\n\tu8  proto = fl6->flowi6_proto;\n\tint seg_len = skb->len;\n\tint hlimit = -1;\n\tint tclass = 0;\n\tu32 mtu;\n\n\tif (opt) {\n\t\tunsigned int head_room;\n\n\t\t/* First: exthdrs may take lots of space (~8K for now)\n\t\t   MAX_HEADER is not enough.\n\t\t */\n\t\thead_room = opt->opt_nflen + opt->opt_flen;\n\t\tseg_len += head_room;\n\t\thead_room += sizeof(struct ipv6hdr) + LL_RESERVED_SPACE(dst->dev);\n\n\t\tif (skb_headroom(skb) < head_room) {\n\t\t\tstruct sk_buff *skb2 = skb_realloc_headroom(skb, head_room);\n\t\t\tif (skb2 == NULL) {\n\t\t\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\treturn -ENOBUFS;\n\t\t\t}\n\t\t\tkfree_skb(skb);\n\t\t\tskb = skb2;\n\t\t\tskb_set_owner_w(skb, sk);\n\t\t}\n\t\tif (opt->opt_flen)\n\t\t\tipv6_push_frag_opts(skb, opt, &proto);\n\t\tif (opt->opt_nflen)\n\t\t\tipv6_push_nfrag_opts(skb, opt, &proto, &first_hop);\n\t}\n\n\tskb_push(skb, sizeof(struct ipv6hdr));\n\tskb_reset_network_header(skb);\n\thdr = ipv6_hdr(skb);\n\n\t/*\n\t *\tFill in the IPv6 header\n\t */\n\tif (np) {\n\t\ttclass = np->tclass;\n\t\thlimit = np->hop_limit;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_dst_hoplimit(dst);\n\n\t*(__be32 *)hdr = htonl(0x60000000 | (tclass << 20)) | fl6->flowlabel;\n\n\thdr->payload_len = htons(seg_len);\n\thdr->nexthdr = proto;\n\thdr->hop_limit = hlimit;\n\n\tipv6_addr_copy(&hdr->saddr, &fl6->saddr);\n\tipv6_addr_copy(&hdr->daddr, first_hop);\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tmtu = dst_mtu(dst);\n\tif ((skb->len <= mtu) || skb->local_df || skb_is_gso(skb)) {\n\t\tIP6_UPD_PO_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t      IPSTATS_MIB_OUT, skb->len);\n\t\treturn NF_HOOK(NFPROTO_IPV6, NF_INET_LOCAL_OUT, skb, NULL,\n\t\t\t       dst->dev, dst_output);\n\t}\n\n\tif (net_ratelimit())\n\t\tprintk(KERN_DEBUG \"IPv6: sending pkt_too_big to self\\n\");\n\tskb->dev = dst->dev;\n\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)), IPSTATS_MIB_FRAGFAILS);\n\tkfree_skb(skb);\n\treturn -EMSGSIZE;\n}\n\nEXPORT_SYMBOL(ip6_xmit);\n\n/*\n *\tTo avoid extra problems ND packets are send through this\n *\troutine. It's code duplication but I really want to avoid\n *\textra checks since ipv6_build_header is used by TCP (which\n *\tis for us performance critical)\n */\n\nint ip6_nd_hdr(struct sock *sk, struct sk_buff *skb, struct net_device *dev,\n\t       const struct in6_addr *saddr, const struct in6_addr *daddr,\n\t       int proto, int len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6hdr *hdr;\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb->dev = dev;\n\n\tskb_reset_network_header(skb);\n\tskb_put(skb, sizeof(struct ipv6hdr));\n\thdr = ipv6_hdr(skb);\n\n\t*(__be32*)hdr = htonl(0x60000000);\n\n\thdr->payload_len = htons(len);\n\thdr->nexthdr = proto;\n\thdr->hop_limit = np->hop_limit;\n\n\tipv6_addr_copy(&hdr->saddr, saddr);\n\tipv6_addr_copy(&hdr->daddr, daddr);\n\n\treturn 0;\n}\n\nstatic int ip6_call_ra_chain(struct sk_buff *skb, int sel)\n{\n\tstruct ip6_ra_chain *ra;\n\tstruct sock *last = NULL;\n\n\tread_lock(&ip6_ra_lock);\n\tfor (ra = ip6_ra_chain; ra; ra = ra->next) {\n\t\tstruct sock *sk = ra->sk;\n\t\tif (sk && ra->sel == sel &&\n\t\t    (!sk->sk_bound_dev_if ||\n\t\t     sk->sk_bound_dev_if == skb->dev->ifindex)) {\n\t\t\tif (last) {\n\t\t\t\tstruct sk_buff *skb2 = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\tif (skb2)\n\t\t\t\t\trawv6_rcv(last, skb2);\n\t\t\t}\n\t\t\tlast = sk;\n\t\t}\n\t}\n\n\tif (last) {\n\t\trawv6_rcv(last, skb);\n\t\tread_unlock(&ip6_ra_lock);\n\t\treturn 1;\n\t}\n\tread_unlock(&ip6_ra_lock);\n\treturn 0;\n}\n\nstatic int ip6_forward_proxy_check(struct sk_buff *skb)\n{\n\tstruct ipv6hdr *hdr = ipv6_hdr(skb);\n\tu8 nexthdr = hdr->nexthdr;\n\tint offset;\n\n\tif (ipv6_ext_hdr(nexthdr)) {\n\t\toffset = ipv6_skip_exthdr(skb, sizeof(*hdr), &nexthdr);\n\t\tif (offset < 0)\n\t\t\treturn 0;\n\t} else\n\t\toffset = sizeof(struct ipv6hdr);\n\n\tif (nexthdr == IPPROTO_ICMPV6) {\n\t\tstruct icmp6hdr *icmp6;\n\n\t\tif (!pskb_may_pull(skb, (skb_network_header(skb) +\n\t\t\t\t\t offset + 1 - skb->data)))\n\t\t\treturn 0;\n\n\t\ticmp6 = (struct icmp6hdr *)(skb_network_header(skb) + offset);\n\n\t\tswitch (icmp6->icmp6_type) {\n\t\tcase NDISC_ROUTER_SOLICITATION:\n\t\tcase NDISC_ROUTER_ADVERTISEMENT:\n\t\tcase NDISC_NEIGHBOUR_SOLICITATION:\n\t\tcase NDISC_NEIGHBOUR_ADVERTISEMENT:\n\t\tcase NDISC_REDIRECT:\n\t\t\t/* For reaction involving unicast neighbor discovery\n\t\t\t * message destined to the proxied address, pass it to\n\t\t\t * input function.\n\t\t\t */\n\t\t\treturn 1;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * The proxying router can't forward traffic sent to a link-local\n\t * address, so signal the sender and discard the packet. This\n\t * behavior is clarified by the MIPv6 specification.\n\t */\n\tif (ipv6_addr_type(&hdr->daddr) & IPV6_ADDR_LINKLOCAL) {\n\t\tdst_link_failure(skb);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline int ip6_forward_finish(struct sk_buff *skb)\n{\n\treturn dst_output(skb);\n}\n\nint ip6_forward(struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct ipv6hdr *hdr = ipv6_hdr(skb);\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tstruct net *net = dev_net(dst->dev);\n\tstruct neighbour *n;\n\tu32 mtu;\n\n\tif (net->ipv6.devconf_all->forwarding == 0)\n\t\tgoto error;\n\n\tif (skb_warn_if_lro(skb))\n\t\tgoto drop;\n\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_FWD, skb)) {\n\t\tIP6_INC_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_INDISCARDS);\n\t\tgoto drop;\n\t}\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto drop;\n\n\tskb_forward_csum(skb);\n\n\t/*\n\t *\tWe DO NOT make any processing on\n\t *\tRA packets, pushing them to user level AS IS\n\t *\twithout ane WARRANTY that application will be able\n\t *\tto interpret them. The reason is that we\n\t *\tcannot make anything clever here.\n\t *\n\t *\tWe are not end-node, so that if packet contains\n\t *\tAH/ESP, we cannot make anything.\n\t *\tDefragmentation also would be mistake, RA packets\n\t *\tcannot be fragmented, because there is no warranty\n\t *\tthat different fragments will go along one path. --ANK\n\t */\n\tif (opt->ra) {\n\t\tu8 *ptr = skb_network_header(skb) + opt->ra;\n\t\tif (ip6_call_ra_chain(skb, (ptr[2]<<8) + ptr[3]))\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t *\tcheck and decrement ttl\n\t */\n\tif (hdr->hop_limit <= 1) {\n\t\t/* Force OUTPUT device used as source address */\n\t\tskb->dev = dst->dev;\n\t\ticmpv6_send(skb, ICMPV6_TIME_EXCEED, ICMPV6_EXC_HOPLIMIT, 0);\n\t\tIP6_INC_STATS_BH(net,\n\t\t\t\t ip6_dst_idev(dst), IPSTATS_MIB_INHDRERRORS);\n\n\t\tkfree_skb(skb);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\t/* XXX: idev->cnf.proxy_ndp? */\n\tif (net->ipv6.devconf_all->proxy_ndp &&\n\t    pneigh_lookup(&nd_tbl, net, &hdr->daddr, skb->dev, 0)) {\n\t\tint proxied = ip6_forward_proxy_check(skb);\n\t\tif (proxied > 0)\n\t\t\treturn ip6_input(skb);\n\t\telse if (proxied < 0) {\n\t\t\tIP6_INC_STATS(net, ip6_dst_idev(dst),\n\t\t\t\t      IPSTATS_MIB_INDISCARDS);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (!xfrm6_route_forward(skb)) {\n\t\tIP6_INC_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_INDISCARDS);\n\t\tgoto drop;\n\t}\n\tdst = skb_dst(skb);\n\n\t/* IPv6 specs say nothing about it, but it is clear that we cannot\n\t   send redirects to source routed frames.\n\t   We don't send redirects to frames decapsulated from IPsec.\n\t */\n\tn = dst_get_neighbour(dst);\n\tif (skb->dev == dst->dev && n && opt->srcrt == 0 && !skb_sec_path(skb)) {\n\t\tstruct in6_addr *target = NULL;\n\t\tstruct rt6_info *rt;\n\n\t\t/*\n\t\t *\tincoming and outgoing devices are the same\n\t\t *\tsend a redirect.\n\t\t */\n\n\t\trt = (struct rt6_info *) dst;\n\t\tif ((rt->rt6i_flags & RTF_GATEWAY))\n\t\t\ttarget = (struct in6_addr*)&n->primary_key;\n\t\telse\n\t\t\ttarget = &hdr->daddr;\n\n\t\tif (!rt->rt6i_peer)\n\t\t\trt6_bind_peer(rt, 1);\n\n\t\t/* Limit redirects both by destination (here)\n\t\t   and by source (inside ndisc_send_redirect)\n\t\t */\n\t\tif (inet_peer_xrlim_allow(rt->rt6i_peer, 1*HZ))\n\t\t\tndisc_send_redirect(skb, n, target);\n\t} else {\n\t\tint addrtype = ipv6_addr_type(&hdr->saddr);\n\n\t\t/* This check is security critical. */\n\t\tif (addrtype == IPV6_ADDR_ANY ||\n\t\t    addrtype & (IPV6_ADDR_MULTICAST | IPV6_ADDR_LOOPBACK))\n\t\t\tgoto error;\n\t\tif (addrtype & IPV6_ADDR_LINKLOCAL) {\n\t\t\ticmpv6_send(skb, ICMPV6_DEST_UNREACH,\n\t\t\t\t    ICMPV6_NOT_NEIGHBOUR, 0);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tmtu = dst_mtu(dst);\n\tif (mtu < IPV6_MIN_MTU)\n\t\tmtu = IPV6_MIN_MTU;\n\n\tif (skb->len > mtu && !skb_is_gso(skb)) {\n\t\t/* Again, force OUTPUT device used as source address */\n\t\tskb->dev = dst->dev;\n\t\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\t\tIP6_INC_STATS_BH(net,\n\t\t\t\t ip6_dst_idev(dst), IPSTATS_MIB_INTOOBIGERRORS);\n\t\tIP6_INC_STATS_BH(net,\n\t\t\t\t ip6_dst_idev(dst), IPSTATS_MIB_FRAGFAILS);\n\t\tkfree_skb(skb);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif (skb_cow(skb, dst->dev->hard_header_len)) {\n\t\tIP6_INC_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_OUTDISCARDS);\n\t\tgoto drop;\n\t}\n\n\thdr = ipv6_hdr(skb);\n\n\t/* Mangling hops number delayed to point after skb COW */\n\n\thdr->hop_limit--;\n\n\tIP6_INC_STATS_BH(net, ip6_dst_idev(dst), IPSTATS_MIB_OUTFORWDATAGRAMS);\n\treturn NF_HOOK(NFPROTO_IPV6, NF_INET_FORWARD, skb, skb->dev, dst->dev,\n\t\t       ip6_forward_finish);\n\nerror:\n\tIP6_INC_STATS_BH(net, ip6_dst_idev(dst), IPSTATS_MIB_INADDRERRORS);\ndrop:\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}\n\nstatic void ip6_copy_metadata(struct sk_buff *to, struct sk_buff *from)\n{\n\tto->pkt_type = from->pkt_type;\n\tto->priority = from->priority;\n\tto->protocol = from->protocol;\n\tskb_dst_drop(to);\n\tskb_dst_set(to, dst_clone(skb_dst(from)));\n\tto->dev = from->dev;\n\tto->mark = from->mark;\n\n#ifdef CONFIG_NET_SCHED\n\tto->tc_index = from->tc_index;\n#endif\n\tnf_copy(to, from);\n#if defined(CONFIG_NETFILTER_XT_TARGET_TRACE) || \\\n    defined(CONFIG_NETFILTER_XT_TARGET_TRACE_MODULE)\n\tto->nf_trace = from->nf_trace;\n#endif\n\tskb_copy_secmark(to, from);\n}\n\nint ip6_find_1stfragopt(struct sk_buff *skb, u8 **nexthdr)\n{\n\tu16 offset = sizeof(struct ipv6hdr);\n\tstruct ipv6_opt_hdr *exthdr =\n\t\t\t\t(struct ipv6_opt_hdr *)(ipv6_hdr(skb) + 1);\n\tunsigned int packet_len = skb->tail - skb->network_header;\n\tint found_rhdr = 0;\n\t*nexthdr = &ipv6_hdr(skb)->nexthdr;\n\n\twhile (offset + 1 <= packet_len) {\n\n\t\tswitch (**nexthdr) {\n\n\t\tcase NEXTHDR_HOP:\n\t\t\tbreak;\n\t\tcase NEXTHDR_ROUTING:\n\t\t\tfound_rhdr = 1;\n\t\t\tbreak;\n\t\tcase NEXTHDR_DEST:\n#if defined(CONFIG_IPV6_MIP6) || defined(CONFIG_IPV6_MIP6_MODULE)\n\t\t\tif (ipv6_find_tlv(skb, offset, IPV6_TLV_HAO) >= 0)\n\t\t\t\tbreak;\n#endif\n\t\t\tif (found_rhdr)\n\t\t\t\treturn offset;\n\t\t\tbreak;\n\t\tdefault :\n\t\t\treturn offset;\n\t\t}\n\n\t\toffset += ipv6_optlen(exthdr);\n\t\t*nexthdr = &exthdr->nexthdr;\n\t\texthdr = (struct ipv6_opt_hdr *)(skb_network_header(skb) +\n\t\t\t\t\t\t offset);\n\t}\n\n\treturn offset;\n}\n\nint ip6_fragment(struct sk_buff *skb, int (*output)(struct sk_buff *))\n{\n\tstruct sk_buff *frag;\n\tstruct rt6_info *rt = (struct rt6_info*)skb_dst(skb);\n\tstruct ipv6_pinfo *np = skb->sk ? inet6_sk(skb->sk) : NULL;\n\tstruct ipv6hdr *tmp_hdr;\n\tstruct frag_hdr *fh;\n\tunsigned int mtu, hlen, left, len;\n\t__be32 frag_id = 0;\n\tint ptr, offset = 0, err=0;\n\tu8 *prevhdr, nexthdr = 0;\n\tstruct net *net = dev_net(skb_dst(skb)->dev);\n\n\thlen = ip6_find_1stfragopt(skb, &prevhdr);\n\tnexthdr = *prevhdr;\n\n\tmtu = ip6_skb_dst_mtu(skb);\n\n\t/* We must not fragment if the socket is set to force MTU discovery\n\t * or if the skb it not generated by a local socket.\n\t */\n\tif (!skb->local_df && skb->len > mtu) {\n\t\tskb->dev = skb_dst(skb)->dev;\n\t\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t      IPSTATS_MIB_FRAGFAILS);\n\t\tkfree_skb(skb);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif (np && np->frag_size < mtu) {\n\t\tif (np->frag_size)\n\t\t\tmtu = np->frag_size;\n\t}\n\tmtu -= hlen + sizeof(struct frag_hdr);\n\n\tif (skb_has_frag_list(skb)) {\n\t\tint first_len = skb_pagelen(skb);\n\t\tstruct sk_buff *frag2;\n\n\t\tif (first_len - hlen > mtu ||\n\t\t    ((first_len - hlen) & 7) ||\n\t\t    skb_cloned(skb))\n\t\t\tgoto slow_path;\n\n\t\tskb_walk_frags(skb, frag) {\n\t\t\t/* Correct geometry. */\n\t\t\tif (frag->len > mtu ||\n\t\t\t    ((frag->len & 7) && frag->next) ||\n\t\t\t    skb_headroom(frag) < hlen)\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\t/* Partially cloned skb? */\n\t\t\tif (skb_shared(frag))\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\tBUG_ON(frag->sk);\n\t\t\tif (skb->sk) {\n\t\t\t\tfrag->sk = skb->sk;\n\t\t\t\tfrag->destructor = sock_wfree;\n\t\t\t}\n\t\t\tskb->truesize -= frag->truesize;\n\t\t}\n\n\t\terr = 0;\n\t\toffset = 0;\n\t\tfrag = skb_shinfo(skb)->frag_list;\n\t\tskb_frag_list_init(skb);\n\t\t/* BUILD HEADER */\n\n\t\t*prevhdr = NEXTHDR_FRAGMENT;\n\t\ttmp_hdr = kmemdup(skb_network_header(skb), hlen, GFP_ATOMIC);\n\t\tif (!tmp_hdr) {\n\t\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t      IPSTATS_MIB_FRAGFAILS);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\t__skb_pull(skb, hlen);\n\t\tfh = (struct frag_hdr*)__skb_push(skb, sizeof(struct frag_hdr));\n\t\t__skb_push(skb, hlen);\n\t\tskb_reset_network_header(skb);\n\t\tmemcpy(skb_network_header(skb), tmp_hdr, hlen);\n\n\t\tipv6_select_ident(fh);\n\t\tfh->nexthdr = nexthdr;\n\t\tfh->reserved = 0;\n\t\tfh->frag_off = htons(IP6_MF);\n\t\tfrag_id = fh->identification;\n\n\t\tfirst_len = skb_pagelen(skb);\n\t\tskb->data_len = first_len - skb_headlen(skb);\n\t\tskb->len = first_len;\n\t\tipv6_hdr(skb)->payload_len = htons(first_len -\n\t\t\t\t\t\t   sizeof(struct ipv6hdr));\n\n\t\tdst_hold(&rt->dst);\n\n\t\tfor (;;) {\n\t\t\t/* Prepare header of the next frame,\n\t\t\t * before previous one went down. */\n\t\t\tif (frag) {\n\t\t\t\tfrag->ip_summed = CHECKSUM_NONE;\n\t\t\t\tskb_reset_transport_header(frag);\n\t\t\t\tfh = (struct frag_hdr*)__skb_push(frag, sizeof(struct frag_hdr));\n\t\t\t\t__skb_push(frag, hlen);\n\t\t\t\tskb_reset_network_header(frag);\n\t\t\t\tmemcpy(skb_network_header(frag), tmp_hdr,\n\t\t\t\t       hlen);\n\t\t\t\toffset += skb->len - hlen - sizeof(struct frag_hdr);\n\t\t\t\tfh->nexthdr = nexthdr;\n\t\t\t\tfh->reserved = 0;\n\t\t\t\tfh->frag_off = htons(offset);\n\t\t\t\tif (frag->next != NULL)\n\t\t\t\t\tfh->frag_off |= htons(IP6_MF);\n\t\t\t\tfh->identification = frag_id;\n\t\t\t\tipv6_hdr(frag)->payload_len =\n\t\t\t\t\t\thtons(frag->len -\n\t\t\t\t\t\t      sizeof(struct ipv6hdr));\n\t\t\t\tip6_copy_metadata(frag, skb);\n\t\t\t}\n\n\t\t\terr = output(skb);\n\t\t\tif(!err)\n\t\t\t\tIP6_INC_STATS(net, ip6_dst_idev(&rt->dst),\n\t\t\t\t\t      IPSTATS_MIB_FRAGCREATES);\n\n\t\t\tif (err || !frag)\n\t\t\t\tbreak;\n\n\t\t\tskb = frag;\n\t\t\tfrag = skb->next;\n\t\t\tskb->next = NULL;\n\t\t}\n\n\t\tkfree(tmp_hdr);\n\n\t\tif (err == 0) {\n\t\t\tIP6_INC_STATS(net, ip6_dst_idev(&rt->dst),\n\t\t\t\t      IPSTATS_MIB_FRAGOKS);\n\t\t\tdst_release(&rt->dst);\n\t\t\treturn 0;\n\t\t}\n\n\t\twhile (frag) {\n\t\t\tskb = frag->next;\n\t\t\tkfree_skb(frag);\n\t\t\tfrag = skb;\n\t\t}\n\n\t\tIP6_INC_STATS(net, ip6_dst_idev(&rt->dst),\n\t\t\t      IPSTATS_MIB_FRAGFAILS);\n\t\tdst_release(&rt->dst);\n\t\treturn err;\n\nslow_path_clean:\n\t\tskb_walk_frags(skb, frag2) {\n\t\t\tif (frag2 == frag)\n\t\t\t\tbreak;\n\t\t\tfrag2->sk = NULL;\n\t\t\tfrag2->destructor = NULL;\n\t\t\tskb->truesize += frag2->truesize;\n\t\t}\n\t}\n\nslow_path:\n\tleft = skb->len - hlen;\t\t/* Space per frame */\n\tptr = hlen;\t\t\t/* Where to start from */\n\n\t/*\n\t *\tFragment the datagram.\n\t */\n\n\t*prevhdr = NEXTHDR_FRAGMENT;\n\n\t/*\n\t *\tKeep copying data until we run out.\n\t */\n\twhile(left > 0)\t{\n\t\tlen = left;\n\t\t/* IF: it doesn't fit, use 'mtu' - the data space left */\n\t\tif (len > mtu)\n\t\t\tlen = mtu;\n\t\t/* IF: we are not sending up to and including the packet end\n\t\t   then align the next start on an eight byte boundary */\n\t\tif (len < left)\t{\n\t\t\tlen &= ~7;\n\t\t}\n\t\t/*\n\t\t *\tAllocate buffer.\n\t\t */\n\n\t\tif ((frag = alloc_skb(len+hlen+sizeof(struct frag_hdr)+LL_ALLOCATED_SPACE(rt->dst.dev), GFP_ATOMIC)) == NULL) {\n\t\t\tNETDEBUG(KERN_INFO \"IPv6: frag: no memory for new fragment!\\n\");\n\t\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t      IPSTATS_MIB_FRAGFAILS);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\n\t\t/*\n\t\t *\tSet up data on packet\n\t\t */\n\n\t\tip6_copy_metadata(frag, skb);\n\t\tskb_reserve(frag, LL_RESERVED_SPACE(rt->dst.dev));\n\t\tskb_put(frag, len + hlen + sizeof(struct frag_hdr));\n\t\tskb_reset_network_header(frag);\n\t\tfh = (struct frag_hdr *)(skb_network_header(frag) + hlen);\n\t\tfrag->transport_header = (frag->network_header + hlen +\n\t\t\t\t\t  sizeof(struct frag_hdr));\n\n\t\t/*\n\t\t *\tCharge the memory for the fragment to any owner\n\t\t *\tit might possess\n\t\t */\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(frag, skb->sk);\n\n\t\t/*\n\t\t *\tCopy the packet header into the new buffer.\n\t\t */\n\t\tskb_copy_from_linear_data(skb, skb_network_header(frag), hlen);\n\n\t\t/*\n\t\t *\tBuild fragment header.\n\t\t */\n\t\tfh->nexthdr = nexthdr;\n\t\tfh->reserved = 0;\n\t\tif (!frag_id) {\n\t\t\tipv6_select_ident(fh);\n\t\t\tfrag_id = fh->identification;\n\t\t} else\n\t\t\tfh->identification = frag_id;\n\n\t\t/*\n\t\t *\tCopy a block of the IP datagram.\n\t\t */\n\t\tif (skb_copy_bits(skb, ptr, skb_transport_header(frag), len))\n\t\t\tBUG();\n\t\tleft -= len;\n\n\t\tfh->frag_off = htons(offset);\n\t\tif (left > 0)\n\t\t\tfh->frag_off |= htons(IP6_MF);\n\t\tipv6_hdr(frag)->payload_len = htons(frag->len -\n\t\t\t\t\t\t    sizeof(struct ipv6hdr));\n\n\t\tptr += len;\n\t\toffset += len;\n\n\t\t/*\n\t\t *\tPut this fragment into the sending queue.\n\t\t */\n\t\terr = output(frag);\n\t\tif (err)\n\t\t\tgoto fail;\n\n\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t      IPSTATS_MIB_FRAGCREATES);\n\t}\n\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t      IPSTATS_MIB_FRAGOKS);\n\tkfree_skb(skb);\n\treturn err;\n\nfail:\n\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t      IPSTATS_MIB_FRAGFAILS);\n\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic inline int ip6_rt_check(const struct rt6key *rt_key,\n\t\t\t       const struct in6_addr *fl_addr,\n\t\t\t       const struct in6_addr *addr_cache)\n{\n\treturn (rt_key->plen != 128 || !ipv6_addr_equal(fl_addr, &rt_key->addr)) &&\n\t\t(addr_cache == NULL || !ipv6_addr_equal(fl_addr, addr_cache));\n}\n\nstatic struct dst_entry *ip6_sk_dst_check(struct sock *sk,\n\t\t\t\t\t  struct dst_entry *dst,\n\t\t\t\t\t  const struct flowi6 *fl6)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct rt6_info *rt = (struct rt6_info *)dst;\n\n\tif (!dst)\n\t\tgoto out;\n\n\t/* Yes, checking route validity in not connected\n\t * case is not very simple. Take into account,\n\t * that we do not support routing by source, TOS,\n\t * and MSG_DONTROUTE \t\t--ANK (980726)\n\t *\n\t * 1. ip6_rt_check(): If route was host route,\n\t *    check that cached destination is current.\n\t *    If it is network route, we still may\n\t *    check its validity using saved pointer\n\t *    to the last used address: daddr_cache.\n\t *    We do not want to save whole address now,\n\t *    (because main consumer of this service\n\t *    is tcp, which has not this problem),\n\t *    so that the last trick works only on connected\n\t *    sockets.\n\t * 2. oif also should be the same.\n\t */\n\tif (ip6_rt_check(&rt->rt6i_dst, &fl6->daddr, np->daddr_cache) ||\n#ifdef CONFIG_IPV6_SUBTREES\n\t    ip6_rt_check(&rt->rt6i_src, &fl6->saddr, np->saddr_cache) ||\n#endif\n\t    (fl6->flowi6_oif && fl6->flowi6_oif != dst->dev->ifindex)) {\n\t\tdst_release(dst);\n\t\tdst = NULL;\n\t}\n\nout:\n\treturn dst;\n}\n\nstatic int ip6_dst_lookup_tail(struct sock *sk,\n\t\t\t       struct dst_entry **dst, struct flowi6 *fl6)\n{\n\tstruct net *net = sock_net(sk);\n#ifdef CONFIG_IPV6_OPTIMISTIC_DAD\n\tstruct neighbour *n;\n#endif\n\tint err;\n\n\tif (*dst == NULL)\n\t\t*dst = ip6_route_output(net, sk, fl6);\n\n\tif ((err = (*dst)->error))\n\t\tgoto out_err_release;\n\n\tif (ipv6_addr_any(&fl6->saddr)) {\n\t\tstruct rt6_info *rt = (struct rt6_info *) *dst;\n\t\terr = ip6_route_get_saddr(net, rt, &fl6->daddr,\n\t\t\t\t\t  sk ? inet6_sk(sk)->srcprefs : 0,\n\t\t\t\t\t  &fl6->saddr);\n\t\tif (err)\n\t\t\tgoto out_err_release;\n\t}\n\n#ifdef CONFIG_IPV6_OPTIMISTIC_DAD\n\t/*\n\t * Here if the dst entry we've looked up\n\t * has a neighbour entry that is in the INCOMPLETE\n\t * state and the src address from the flow is\n\t * marked as OPTIMISTIC, we release the found\n\t * dst entry and replace it instead with the\n\t * dst entry of the nexthop router\n\t */\n\tn = dst_get_neighbour(*dst);\n\tif (n && !(n->nud_state & NUD_VALID)) {\n\t\tstruct inet6_ifaddr *ifp;\n\t\tstruct flowi6 fl_gw6;\n\t\tint redirect;\n\n\t\tifp = ipv6_get_ifaddr(net, &fl6->saddr,\n\t\t\t\t      (*dst)->dev, 1);\n\n\t\tredirect = (ifp && ifp->flags & IFA_F_OPTIMISTIC);\n\t\tif (ifp)\n\t\t\tin6_ifa_put(ifp);\n\n\t\tif (redirect) {\n\t\t\t/*\n\t\t\t * We need to get the dst entry for the\n\t\t\t * default router instead\n\t\t\t */\n\t\t\tdst_release(*dst);\n\t\t\tmemcpy(&fl_gw6, fl6, sizeof(struct flowi6));\n\t\t\tmemset(&fl_gw6.daddr, 0, sizeof(struct in6_addr));\n\t\t\t*dst = ip6_route_output(net, sk, &fl_gw6);\n\t\t\tif ((err = (*dst)->error))\n\t\t\t\tgoto out_err_release;\n\t\t}\n\t}\n#endif\n\n\treturn 0;\n\nout_err_release:\n\tif (err == -ENETUNREACH)\n\t\tIP6_INC_STATS_BH(net, NULL, IPSTATS_MIB_OUTNOROUTES);\n\tdst_release(*dst);\n\t*dst = NULL;\n\treturn err;\n}\n\n/**\n *\tip6_dst_lookup - perform route lookup on flow\n *\t@sk: socket which provides route info\n *\t@dst: pointer to dst_entry * for result\n *\t@fl6: flow to lookup\n *\n *\tThis function performs a route lookup on the given flow.\n *\n *\tIt returns zero on success, or a standard errno code on error.\n */\nint ip6_dst_lookup(struct sock *sk, struct dst_entry **dst, struct flowi6 *fl6)\n{\n\t*dst = NULL;\n\treturn ip6_dst_lookup_tail(sk, dst, fl6);\n}\nEXPORT_SYMBOL_GPL(ip6_dst_lookup);\n\n/**\n *\tip6_dst_lookup_flow - perform route lookup on flow with ipsec\n *\t@sk: socket which provides route info\n *\t@fl6: flow to lookup\n *\t@final_dst: final destination address for ipsec lookup\n *\t@can_sleep: we are in a sleepable context\n *\n *\tThis function performs a route lookup on the given flow.\n *\n *\tIt returns a valid dst pointer on success, or a pointer encoded\n *\terror code.\n */\nstruct dst_entry *ip6_dst_lookup_flow(struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t      const struct in6_addr *final_dst,\n\t\t\t\t      bool can_sleep)\n{\n\tstruct dst_entry *dst = NULL;\n\tint err;\n\n\terr = ip6_dst_lookup_tail(sk, &dst, fl6);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\tif (final_dst)\n\t\tipv6_addr_copy(&fl6->daddr, final_dst);\n\tif (can_sleep)\n\t\tfl6->flowi6_flags |= FLOWI_FLAG_CAN_SLEEP;\n\n\treturn xfrm_lookup(sock_net(sk), dst, flowi6_to_flowi(fl6), sk, 0);\n}\nEXPORT_SYMBOL_GPL(ip6_dst_lookup_flow);\n\n/**\n *\tip6_sk_dst_lookup_flow - perform socket cached route lookup on flow\n *\t@sk: socket which provides the dst cache and route info\n *\t@fl6: flow to lookup\n *\t@final_dst: final destination address for ipsec lookup\n *\t@can_sleep: we are in a sleepable context\n *\n *\tThis function performs a route lookup on the given flow with the\n *\tpossibility of using the cached route in the socket if it is valid.\n *\tIt will take the socket dst lock when operating on the dst cache.\n *\tAs a result, this function can only be used in process context.\n *\n *\tIt returns a valid dst pointer on success, or a pointer encoded\n *\terror code.\n */\nstruct dst_entry *ip6_sk_dst_lookup_flow(struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t\t const struct in6_addr *final_dst,\n\t\t\t\t\t bool can_sleep)\n{\n\tstruct dst_entry *dst = sk_dst_check(sk, inet6_sk(sk)->dst_cookie);\n\tint err;\n\n\tdst = ip6_sk_dst_check(sk, dst, fl6);\n\n\terr = ip6_dst_lookup_tail(sk, &dst, fl6);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\tif (final_dst)\n\t\tipv6_addr_copy(&fl6->daddr, final_dst);\n\tif (can_sleep)\n\t\tfl6->flowi6_flags |= FLOWI_FLAG_CAN_SLEEP;\n\n\treturn xfrm_lookup(sock_net(sk), dst, flowi6_to_flowi(fl6), sk, 0);\n}\nEXPORT_SYMBOL_GPL(ip6_sk_dst_lookup_flow);\n\nstatic inline int ip6_ufo_append_data(struct sock *sk,\n\t\t\tint getfrag(void *from, char *to, int offset, int len,\n\t\t\tint odd, struct sk_buff *skb),\n\t\t\tvoid *from, int length, int hh_len, int fragheaderlen,\n\t\t\tint transhdrlen, int mtu,unsigned int flags)\n\n{\n\tstruct sk_buff *skb;\n\tint err;\n\n\t/* There is support for UDP large send offload by network\n\t * device, so create one single skb packet containing complete\n\t * udp datagram\n\t */\n\tif ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL) {\n\t\tskb = sock_alloc_send_skb(sk,\n\t\t\thh_len + fragheaderlen + transhdrlen + 20,\n\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\tif (skb == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\t/* reserve space for Hardware header */\n\t\tskb_reserve(skb, hh_len);\n\n\t\t/* create space for UDP/IP header */\n\t\tskb_put(skb,fragheaderlen + transhdrlen);\n\n\t\t/* initialize network header pointer */\n\t\tskb_reset_network_header(skb);\n\n\t\t/* initialize protocol header pointer */\n\t\tskb->transport_header = skb->network_header + fragheaderlen;\n\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\tskb->csum = 0;\n\t}\n\n\terr = skb_append_datato_frags(sk,skb, getfrag, from,\n\t\t\t\t      (length - transhdrlen));\n\tif (!err) {\n\t\tstruct frag_hdr fhdr;\n\n\t\t/* Specify the length of each IPv6 datagram fragment.\n\t\t * It has to be a multiple of 8.\n\t\t */\n\t\tskb_shinfo(skb)->gso_size = (mtu - fragheaderlen -\n\t\t\t\t\t     sizeof(struct frag_hdr)) & ~7;\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\t\tipv6_select_ident(&fhdr);\n\t\tskb_shinfo(skb)->ip6_frag_id = fhdr.identification;\n\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\n\t\treturn 0;\n\t}\n\t/* There is not enough support do UPD LSO,\n\t * so follow normal path\n\t */\n\tkfree_skb(skb);\n\n\treturn err;\n}\n\nstatic inline struct ipv6_opt_hdr *ip6_opt_dup(struct ipv6_opt_hdr *src,\n\t\t\t\t\t       gfp_t gfp)\n{\n\treturn src ? kmemdup(src, (src->hdrlen + 1) * 8, gfp) : NULL;\n}\n\nstatic inline struct ipv6_rt_hdr *ip6_rthdr_dup(struct ipv6_rt_hdr *src,\n\t\t\t\t\t\tgfp_t gfp)\n{\n\treturn src ? kmemdup(src, (src->hdrlen + 1) * 8, gfp) : NULL;\n}\n\nint ip6_append_data(struct sock *sk, int getfrag(void *from, char *to,\n\tint offset, int len, int odd, struct sk_buff *skb),\n\tvoid *from, int length, int transhdrlen,\n\tint hlimit, int tclass, struct ipv6_txoptions *opt, struct flowi6 *fl6,\n\tstruct rt6_info *rt, unsigned int flags, int dontfrag)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_cork *cork;\n\tstruct sk_buff *skb;\n\tunsigned int maxfraglen, fragheaderlen;\n\tint exthdrlen;\n\tint hh_len;\n\tint mtu;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\tint csummode = CHECKSUM_NONE;\n\t__u8 tx_flags = 0;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\tcork = &inet->cork.base;\n\tif (skb_queue_empty(&sk->sk_write_queue)) {\n\t\t/*\n\t\t * setup for corking\n\t\t */\n\t\tif (opt) {\n\t\t\tif (WARN_ON(np->cork.opt))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tnp->cork.opt = kmalloc(opt->tot_len, sk->sk_allocation);\n\t\t\tif (unlikely(np->cork.opt == NULL))\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\tnp->cork.opt->tot_len = opt->tot_len;\n\t\t\tnp->cork.opt->opt_flen = opt->opt_flen;\n\t\t\tnp->cork.opt->opt_nflen = opt->opt_nflen;\n\n\t\t\tnp->cork.opt->dst0opt = ip6_opt_dup(opt->dst0opt,\n\t\t\t\t\t\t\t    sk->sk_allocation);\n\t\t\tif (opt->dst0opt && !np->cork.opt->dst0opt)\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\tnp->cork.opt->dst1opt = ip6_opt_dup(opt->dst1opt,\n\t\t\t\t\t\t\t    sk->sk_allocation);\n\t\t\tif (opt->dst1opt && !np->cork.opt->dst1opt)\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\tnp->cork.opt->hopopt = ip6_opt_dup(opt->hopopt,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\tif (opt->hopopt && !np->cork.opt->hopopt)\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\tnp->cork.opt->srcrt = ip6_rthdr_dup(opt->srcrt,\n\t\t\t\t\t\t\t    sk->sk_allocation);\n\t\t\tif (opt->srcrt && !np->cork.opt->srcrt)\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\t/* need source address above miyazawa*/\n\t\t}\n\t\tdst_hold(&rt->dst);\n\t\tcork->dst = &rt->dst;\n\t\tinet->cork.fl.u.ip6 = *fl6;\n\t\tnp->cork.hop_limit = hlimit;\n\t\tnp->cork.tclass = tclass;\n\t\tmtu = np->pmtudisc == IPV6_PMTUDISC_PROBE ?\n\t\t      rt->dst.dev->mtu : dst_mtu(rt->dst.path);\n\t\tif (np->frag_size < mtu) {\n\t\t\tif (np->frag_size)\n\t\t\t\tmtu = np->frag_size;\n\t\t}\n\t\tcork->fragsize = mtu;\n\t\tif (dst_allfrag(rt->dst.path))\n\t\t\tcork->flags |= IPCORK_ALLFRAG;\n\t\tcork->length = 0;\n\t\tsk->sk_sndmsg_page = NULL;\n\t\tsk->sk_sndmsg_off = 0;\n\t\texthdrlen = rt->dst.header_len + (opt ? opt->opt_flen : 0) -\n\t\t\t    rt->rt6i_nfheader_len;\n\t\tlength += exthdrlen;\n\t\ttranshdrlen += exthdrlen;\n\t} else {\n\t\trt = (struct rt6_info *)cork->dst;\n\t\tfl6 = &inet->cork.fl.u.ip6;\n\t\topt = np->cork.opt;\n\t\ttranshdrlen = 0;\n\t\texthdrlen = 0;\n\t\tmtu = cork->fragsize;\n\t}\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct ipv6hdr) + rt->rt6i_nfheader_len +\n\t\t\t(opt ? opt->opt_nflen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen - sizeof(struct frag_hdr);\n\n\tif (mtu <= sizeof(struct ipv6hdr) + IPV6_MAXPLEN) {\n\t\tif (cork->length + length > sizeof(struct ipv6hdr) + IPV6_MAXPLEN - fragheaderlen) {\n\t\t\tipv6_local_error(sk, EMSGSIZE, fl6, mtu-exthdrlen);\n\t\t\treturn -EMSGSIZE;\n\t\t}\n\t}\n\n\t/* For UDP, check if TX timestamp is enabled */\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\terr = sock_tx_timestamp(sk, &tx_flags);\n\t\tif (err)\n\t\t\tgoto error;\n\t}\n\n\t/*\n\t * Let's try using as much space as possible.\n\t * Use MTU if total length of the message fits into the MTU.\n\t * Otherwise, we need to reserve fragment header and\n\t * fragment alignment (= 8-15 octects, in total).\n\t *\n\t * Note that we may need to \"move\" the data from the tail of\n\t * of the buffer to the new fragment when we split\n\t * the message.\n\t *\n\t * FIXME: It may be fragmented into multiple chunks\n\t *        at once if non-fragmentable extension headers\n\t *        are too large.\n\t * --yoshfuji\n\t */\n\n\tcork->length += length;\n\tif (length > mtu) {\n\t\tint proto = sk->sk_protocol;\n\t\tif (dontfrag && (proto == IPPROTO_UDP || proto == IPPROTO_RAW)){\n\t\t\tipv6_local_rxpmtu(sk, fl6, mtu-exthdrlen);\n\t\t\treturn -EMSGSIZE;\n\t\t}\n\n\t\tif (proto == IPPROTO_UDP &&\n\t\t    (rt->dst.dev->features & NETIF_F_UFO)) {\n\n\t\t\terr = ip6_ufo_append_data(sk, getfrag, from, length,\n\t\t\t\t\t\t  hh_len, fragheaderlen,\n\t\t\t\t\t\t  transhdrlen, mtu, flags);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\n\t\t\tstruct sk_buff *skb_prev;\nalloc_new_skb:\n\t\t\tskb_prev = skb;\n\n\t\t\t/* There's no room in the current skb */\n\t\t\tif (skb_prev)\n\t\t\t\tfraggap = skb_prev->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\t\t\tif (datalen > (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen;\n\n\t\t\tfraglen = datalen + fragheaderlen;\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse\n\t\t\t\talloclen = datalen + fragheaderlen;\n\n\t\t\t/*\n\t\t\t * The last fragment gets additional space at tail.\n\t\t\t * Note: we overallocate on fragments with MSG_MODE\n\t\t\t * because we have no idea if we're the last one.\n\t\t\t */\n\t\t\tif (datalen == length + fraggap)\n\t\t\t\talloclen += rt->dst.trailer_len;\n\n\t\t\t/*\n\t\t\t * We just reserve space for fragment header.\n\t\t\t * Note: this may be overallocation if the message\n\t\t\t * (without MSG_MORE) fits into the MTU.\n\t\t\t */\n\t\t\talloclen += sizeof(struct frag_hdr);\n\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (atomic_read(&sk->sk_wmem_alloc) <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = sock_wmalloc(sk,\n\t\t\t\t\t\t\t   alloclen + hh_len, 1,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\t\tif (unlikely(skb == NULL))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t\telse {\n\t\t\t\t\t/* Only the initial fragment\n\t\t\t\t\t * is time stamped.\n\t\t\t\t\t */\n\t\t\t\t\ttx_flags = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (skb == NULL)\n\t\t\t\tgoto error;\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\t/* reserve for fragmentation */\n\t\t\tskb_reserve(skb, hh_len+sizeof(struct frag_hdr));\n\n\t\t\tif (sk->sk_type == SOCK_DGRAM)\n\t\t\t\tskb_shinfo(skb)->tx_flags = tx_flags;\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tdata += fragheaderlen;\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\t\t\tcopy = datalen - transhdrlen - fraggap;\n\t\t\tif (copy < 0) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t} else if (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= datalen - fraggap;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tcsummode = CHECKSUM_NONE;\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue\n\t\t\t */\n\t\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG)) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i-1];\n\t\t\tstruct page *page = sk->sk_sndmsg_page;\n\t\t\tint off = sk->sk_sndmsg_off;\n\t\t\tunsigned int left;\n\n\t\t\tif (page && (left = PAGE_SIZE - off) > 0) {\n\t\t\t\tif (copy >= left)\n\t\t\t\t\tcopy = left;\n\t\t\t\tif (page != frag->page) {\n\t\t\t\t\tif (i == MAX_SKB_FRAGS) {\n\t\t\t\t\t\terr = -EMSGSIZE;\n\t\t\t\t\t\tgoto error;\n\t\t\t\t\t}\n\t\t\t\t\tget_page(page);\n\t\t\t\t\tskb_fill_page_desc(skb, i, page, sk->sk_sndmsg_off, 0);\n\t\t\t\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\t\t\t}\n\t\t\t} else if(i < MAX_SKB_FRAGS) {\n\t\t\t\tif (copy > PAGE_SIZE)\n\t\t\t\t\tcopy = PAGE_SIZE;\n\t\t\t\tpage = alloc_pages(sk->sk_allocation, 0);\n\t\t\t\tif (page == NULL) {\n\t\t\t\t\terr = -ENOMEM;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tsk->sk_sndmsg_page = page;\n\t\t\t\tsk->sk_sndmsg_off = 0;\n\n\t\t\t\tskb_fill_page_desc(skb, i, page, 0, 0);\n\t\t\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\t\t} else {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (getfrag(from, page_address(frag->page)+frag->page_offset+frag->size, offset, copy, skb->len, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tsk->sk_sndmsg_off += copy;\n\t\t\tfrag->size += copy;\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\tatomic_add(copy, &sk->sk_wmem_alloc);\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\treturn 0;\nerror:\n\tcork->length -= length;\n\tIP6_INC_STATS(sock_net(sk), rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}\n\nstatic void ip6_cork_release(struct inet_sock *inet, struct ipv6_pinfo *np)\n{\n\tif (np->cork.opt) {\n\t\tkfree(np->cork.opt->dst0opt);\n\t\tkfree(np->cork.opt->dst1opt);\n\t\tkfree(np->cork.opt->hopopt);\n\t\tkfree(np->cork.opt->srcrt);\n\t\tkfree(np->cork.opt);\n\t\tnp->cork.opt = NULL;\n\t}\n\n\tif (inet->cork.base.dst) {\n\t\tdst_release(inet->cork.base.dst);\n\t\tinet->cork.base.dst = NULL;\n\t\tinet->cork.base.flags &= ~IPCORK_ALLFRAG;\n\t}\n\tmemset(&inet->cork.fl, 0, sizeof(inet->cork.fl));\n}\n\nint ip6_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb, *tmp_skb;\n\tstruct sk_buff **tail_skb;\n\tstruct in6_addr final_dst_buf, *final_dst = &final_dst_buf;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6hdr *hdr;\n\tstruct ipv6_txoptions *opt = np->cork.opt;\n\tstruct rt6_info *rt = (struct rt6_info *)inet->cork.base.dst;\n\tstruct flowi6 *fl6 = &inet->cork.fl.u.ip6;\n\tunsigned char proto = fl6->flowi6_proto;\n\tint err = 0;\n\n\tif ((skb = __skb_dequeue(&sk->sk_write_queue)) == NULL)\n\t\tgoto out;\n\ttail_skb = &(skb_shinfo(skb)->frag_list);\n\n\t/* move skb->data to ip header from ext header */\n\tif (skb->data < skb_network_header(skb))\n\t\t__skb_pull(skb, skb_network_offset(skb));\n\twhile ((tmp_skb = __skb_dequeue(&sk->sk_write_queue)) != NULL) {\n\t\t__skb_pull(tmp_skb, skb_network_header_len(skb));\n\t\t*tail_skb = tmp_skb;\n\t\ttail_skb = &(tmp_skb->next);\n\t\tskb->len += tmp_skb->len;\n\t\tskb->data_len += tmp_skb->len;\n\t\tskb->truesize += tmp_skb->truesize;\n\t\ttmp_skb->destructor = NULL;\n\t\ttmp_skb->sk = NULL;\n\t}\n\n\t/* Allow local fragmentation. */\n\tif (np->pmtudisc < IPV6_PMTUDISC_DO)\n\t\tskb->local_df = 1;\n\n\tipv6_addr_copy(final_dst, &fl6->daddr);\n\t__skb_pull(skb, skb_network_header_len(skb));\n\tif (opt && opt->opt_flen)\n\t\tipv6_push_frag_opts(skb, opt, &proto);\n\tif (opt && opt->opt_nflen)\n\t\tipv6_push_nfrag_opts(skb, opt, &proto, &final_dst);\n\n\tskb_push(skb, sizeof(struct ipv6hdr));\n\tskb_reset_network_header(skb);\n\thdr = ipv6_hdr(skb);\n\n\t*(__be32*)hdr = fl6->flowlabel |\n\t\t     htonl(0x60000000 | ((int)np->cork.tclass << 20));\n\n\thdr->hop_limit = np->cork.hop_limit;\n\thdr->nexthdr = proto;\n\tipv6_addr_copy(&hdr->saddr, &fl6->saddr);\n\tipv6_addr_copy(&hdr->daddr, final_dst);\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tskb_dst_set(skb, dst_clone(&rt->dst));\n\tIP6_UPD_PO_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUT, skb->len);\n\tif (proto == IPPROTO_ICMPV6) {\n\t\tstruct inet6_dev *idev = ip6_dst_idev(skb_dst(skb));\n\n\t\tICMP6MSGOUT_INC_STATS_BH(net, idev, icmp6_hdr(skb)->icmp6_type);\n\t\tICMP6_INC_STATS_BH(net, idev, ICMP6_MIB_OUTMSGS);\n\t}\n\n\terr = ip6_local_out(skb);\n\tif (err) {\n\t\tif (err > 0)\n\t\t\terr = net_xmit_errno(err);\n\t\tif (err)\n\t\t\tgoto error;\n\t}\n\nout:\n\tip6_cork_release(inet, np);\n\treturn err;\nerror:\n\tIP6_INC_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\tgoto out;\n}\n\nvoid ip6_flush_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue_tail(&sk->sk_write_queue)) != NULL) {\n\t\tif (skb_dst(skb))\n\t\t\tIP6_INC_STATS(sock_net(sk), ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t\tkfree_skb(skb);\n\t}\n\n\tip6_cork_release(inet_sk(sk), inet6_sk(sk));\n}\n", "/*\n *\tUDP over IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on linux/ipv4/udp.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *      Kazunori MIYAZAWA @USAGI:       change process style to use ip6_append_data\n *      YOSHIFUJI Hideaki @USAGI:\tconvert /proc/net/udp6 to seq_file.\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/ipv6.h>\n#include <linux/icmpv6.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/raw.h>\n#include <net/tcp_states.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include \"udp_impl.h\"\n\nint ipv6_rcv_saddr_equal(const struct sock *sk, const struct sock *sk2)\n{\n\tconst struct in6_addr *sk_rcv_saddr6 = &inet6_sk(sk)->rcv_saddr;\n\tconst struct in6_addr *sk2_rcv_saddr6 = inet6_rcv_saddr(sk2);\n\t__be32 sk1_rcv_saddr = sk_rcv_saddr(sk);\n\t__be32 sk2_rcv_saddr = sk_rcv_saddr(sk2);\n\tint sk_ipv6only = ipv6_only_sock(sk);\n\tint sk2_ipv6only = inet_v6_ipv6only(sk2);\n\tint addr_type = ipv6_addr_type(sk_rcv_saddr6);\n\tint addr_type2 = sk2_rcv_saddr6 ? ipv6_addr_type(sk2_rcv_saddr6) : IPV6_ADDR_MAPPED;\n\n\t/* if both are mapped, treat as IPv4 */\n\tif (addr_type == IPV6_ADDR_MAPPED && addr_type2 == IPV6_ADDR_MAPPED)\n\t\treturn (!sk2_ipv6only &&\n\t\t\t(!sk1_rcv_saddr || !sk2_rcv_saddr ||\n\t\t\t  sk1_rcv_saddr == sk2_rcv_saddr));\n\n\tif (addr_type2 == IPV6_ADDR_ANY &&\n\t    !(sk2_ipv6only && addr_type == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (addr_type == IPV6_ADDR_ANY &&\n\t    !(sk_ipv6only && addr_type2 == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (sk2_rcv_saddr6 &&\n\t    ipv6_addr_equal(sk_rcv_saddr6, sk2_rcv_saddr6))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic unsigned int udp6_portaddr_hash(struct net *net,\n\t\t\t\t       const struct in6_addr *addr6,\n\t\t\t\t       unsigned int port)\n{\n\tunsigned int hash, mix = net_hash_mix(net);\n\n\tif (ipv6_addr_any(addr6))\n\t\thash = jhash_1word(0, mix);\n\telse if (ipv6_addr_v4mapped(addr6))\n\t\thash = jhash_1word((__force u32)addr6->s6_addr32[3], mix);\n\telse\n\t\thash = jhash2((__force u32 *)addr6->s6_addr32, 4, mix);\n\n\treturn hash ^ port;\n}\n\n\nint udp_v6_get_port(struct sock *sk, unsigned short snum)\n{\n\tunsigned int hash2_nulladdr =\n\t\tudp6_portaddr_hash(sock_net(sk), &in6addr_any, snum);\n\tunsigned int hash2_partial = \n\t\tudp6_portaddr_hash(sock_net(sk), &inet6_sk(sk)->rcv_saddr, 0);\n\n\t/* precompute partial secondary hash */\n\tudp_sk(sk)->udp_portaddr_hash = hash2_partial;\n\treturn udp_lib_get_port(sk, snum, ipv6_rcv_saddr_equal, hash2_nulladdr);\n}\n\nstatic void udp_v6_rehash(struct sock *sk)\n{\n\tu16 new_hash = udp6_portaddr_hash(sock_net(sk),\n\t\t\t\t\t  &inet6_sk(sk)->rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_num);\n\n\tudp_lib_rehash(sk, new_hash);\n}\n\nstatic inline int compute_score(struct sock *sk, struct net *net,\n\t\t\t\tunsigned short hnum,\n\t\t\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\t\t\tconst struct in6_addr *daddr, __be16 dport,\n\t\t\t\tint dif)\n{\n\tint score = -1;\n\n\tif (net_eq(sock_net(sk), net) && udp_sk(sk)->udp_port_hash == hnum &&\n\t\t\tsk->sk_family == PF_INET6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tscore = 0;\n\t\tif (inet->inet_dport) {\n\t\t\tif (inet->inet_dport != sport)\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t\tif (!ipv6_addr_any(&np->rcv_saddr)) {\n\t\t\tif (!ipv6_addr_equal(&np->rcv_saddr, daddr))\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t\tif (!ipv6_addr_any(&np->daddr)) {\n\t\t\tif (!ipv6_addr_equal(&np->daddr, saddr))\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t\tif (sk->sk_bound_dev_if) {\n\t\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t}\n\treturn score;\n}\n\n#define SCORE2_MAX (1 + 1 + 1)\nstatic inline int compute_score2(struct sock *sk, struct net *net,\n\t\t\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\t\t\tconst struct in6_addr *daddr, unsigned short hnum,\n\t\t\t\tint dif)\n{\n\tint score = -1;\n\n\tif (net_eq(sock_net(sk), net) && udp_sk(sk)->udp_port_hash == hnum &&\n\t\t\tsk->sk_family == PF_INET6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tif (!ipv6_addr_equal(&np->rcv_saddr, daddr))\n\t\t\treturn -1;\n\t\tscore = 0;\n\t\tif (inet->inet_dport) {\n\t\t\tif (inet->inet_dport != sport)\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t\tif (!ipv6_addr_any(&np->daddr)) {\n\t\t\tif (!ipv6_addr_equal(&np->daddr, saddr))\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t\tif (sk->sk_bound_dev_if) {\n\t\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t}\n\treturn score;\n}\n\n\n/* called with read_rcu_lock() */\nstatic struct sock *udp6_lib_lookup2(struct net *net,\n\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\tconst struct in6_addr *daddr, unsigned int hnum, int dif,\n\t\tstruct udp_hslot *hslot2, unsigned int slot2)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tint score, badness;\n\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tudp_portaddr_for_each_entry_rcu(sk, node, &hslot2->head) {\n\t\tscore = compute_score2(sk, net, saddr, sport,\n\t\t\t\t      daddr, hnum, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\tif (score == SCORE2_MAX)\n\t\t\t\tgoto exact_match;\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot2)\n\t\tgoto begin;\n\n\tif (result) {\nexact_match:\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score2(result, net, saddr, sport,\n\t\t\t\t  daddr, hnum, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\treturn result;\n}\n\nstatic struct sock *__udp6_lib_lookup(struct net *net,\n\t\t\t\t      const struct in6_addr *saddr, __be16 sport,\n\t\t\t\t      const struct in6_addr *daddr, __be16 dport,\n\t\t\t\t      int dif, struct udp_table *udptable)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(dport);\n\tunsigned int hash2, slot2, slot = udp_hashfn(net, hnum, udptable->mask);\n\tstruct udp_hslot *hslot2, *hslot = &udptable->hash[slot];\n\tint score, badness;\n\n\trcu_read_lock();\n\tif (hslot->count > 10) {\n\t\thash2 = udp6_portaddr_hash(net, daddr, hnum);\n\t\tslot2 = hash2 & udptable->mask;\n\t\thslot2 = &udptable->hash2[slot2];\n\t\tif (hslot->count < hslot2->count)\n\t\t\tgoto begin;\n\n\t\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t  daddr, hnum, dif,\n\t\t\t\t\t  hslot2, slot2);\n\t\tif (!result) {\n\t\t\thash2 = udp6_portaddr_hash(net, &in6addr_any, hnum);\n\t\t\tslot2 = hash2 & udptable->mask;\n\t\t\thslot2 = &udptable->hash2[slot2];\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto begin;\n\n\t\t\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t\t  &in6addr_any, hnum, dif,\n\t\t\t\t\t\t  hslot2, slot2);\n\t\t}\n\t\trcu_read_unlock();\n\t\treturn result;\n\t}\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tsk_nulls_for_each_rcu(sk, node, &hslot->head) {\n\t\tscore = compute_score(sk, net, hnum, saddr, sport, daddr, dport, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score(result, net, hnum, saddr, sport,\n\t\t\t\t\tdaddr, dport, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\n\nstatic struct sock *__udp6_lib_lookup_skb(struct sk_buff *skb,\n\t\t\t\t\t  __be16 sport, __be16 dport,\n\t\t\t\t\t  struct udp_table *udptable)\n{\n\tstruct sock *sk;\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\n\tif (unlikely(sk = skb_steal_sock(skb)))\n\t\treturn sk;\n\treturn __udp6_lib_lookup(dev_net(skb_dst(skb)->dev), &iph->saddr, sport,\n\t\t\t\t &iph->daddr, dport, inet6_iif(skb),\n\t\t\t\t udptable);\n}\n\nstruct sock *udp6_lib_lookup(struct net *net, const struct in6_addr *saddr, __be16 sport,\n\t\t\t     const struct in6_addr *daddr, __be16 dport, int dif)\n{\n\treturn __udp6_lib_lookup(net, saddr, sport, daddr, dport, dif, &udp_table);\n}\nEXPORT_SYMBOL_GPL(udp6_lib_lookup);\n\n\n/*\n * \tThis should be easy, if there is something there we\n * \treturn it, otherwise we block.\n */\n\nint udpv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen;\n\tint peeked;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint is_udp4;\n\tbool slow;\n\n\tif (addr_len)\n\t\t*addr_len=sizeof(struct sockaddr_in6);\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tif (len > ulen)\n\t\tlen = ulen;\n\telse if (len < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (len < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov,len);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, sizeof(struct udphdr), msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (err)\n\t\tgoto out_free;\n\n\tif (!peeked) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in6 *sin6;\n\n\t\tsin6 = (struct sockaddr_in6 *) msg->msg_name;\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\t\tsin6->sin6_scope_id = 0;\n\n\t\tif (is_udp4)\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\telse {\n\t\t\tipv6_addr_copy(&sin6->sin6_addr,\n\t\t\t\t       &ipv6_hdr(skb)->saddr);\n\t\t\tif (ipv6_addr_type(&sin6->sin6_addr) & IPV6_ADDR_LINKLOCAL)\n\t\t\t\tsin6->sin6_scope_id = IP6CB(skb)->iif;\n\t\t}\n\n\t}\n\tif (is_udp4) {\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tdatagram_recv_ctl(sk, msg, skb);\n\t}\n\n\terr = len;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t}\n\tunlock_sock_fast(sk, slow);\n\n\tif (noblock)\n\t\treturn -EAGAIN;\n\n\t/* starting over for a new packet */\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}\n\nvoid __udp6_lib_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t    u8 type, u8 code, int offset, __be32 info,\n\t\t    struct udp_table *udptable)\n{\n\tstruct ipv6_pinfo *np;\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct in6_addr *saddr = &hdr->saddr;\n\tconst struct in6_addr *daddr = &hdr->daddr;\n\tstruct udphdr *uh = (struct udphdr*)(skb->data+offset);\n\tstruct sock *sk;\n\tint err;\n\n\tsk = __udp6_lib_lookup(dev_net(skb->dev), daddr, uh->dest,\n\t\t\t       saddr, uh->source, inet6_iif(skb), udptable);\n\tif (sk == NULL)\n\t\treturn;\n\n\tnp = inet6_sk(sk);\n\n\tif (!icmpv6_err_convert(type, code, &err) && !np->recverr)\n\t\tgoto out;\n\n\tif (sk->sk_state != TCP_ESTABLISHED && !np->recverr)\n\t\tgoto out;\n\n\tif (np->recverr)\n\t\tipv6_icmp_error(sk, skb, err, uh->dest, ntohl(info), (u8 *)(uh+1));\n\n\tsk->sk_err = err;\n\tsk->sk_error_report(sk);\nout:\n\tsock_put(sk);\n}\n\nstatic __inline__ void udpv6_err(struct sk_buff *skb,\n\t\t\t\t struct inet6_skb_parm *opt, u8 type,\n\t\t\t\t u8 code, int offset, __be32 info     )\n{\n\t__udp6_lib_err(skb, opt, type, code, offset, info, &udp_table);\n}\n\nint udpv6_queue_rcv_skb(struct sock * sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (!ipv6_addr_any(&inet6_sk(sk)->daddr))\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\n\t/*\n\t * UDP-Lite specific tests, ignored on UDP sockets (see net/ipv4/udp.c).\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING \"UDPLITE6: partial coverage\"\n\t\t\t\t\" %d while full coverage %d requested\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING \"UDPLITE6: coverage %d \"\n\t\t\t\t\t\t    \"too small, need min %d\\n\",\n\t\t\t\t       UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_dereference_raw(sk->sk_filter)) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto drop;\n\t}\n\n\tif ((rc = ip_queue_rcv_skb(sk, skb)) < 0) {\n\t\t/* Note that an ENOMEM error is charged twice */\n\t\tif (rc == -ENOMEM)\n\t\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\tUDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tgoto drop_no_sk_drops_inc;\n\t}\n\n\treturn 0;\ndrop:\n\tatomic_inc(&sk->sk_drops);\ndrop_no_sk_drops_inc:\n\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\nstatic struct sock *udp_v6_mcast_next(struct net *net, struct sock *sk,\n\t\t\t\t      __be16 loc_port, const struct in6_addr *loc_addr,\n\t\t\t\t      __be16 rmt_port, const struct in6_addr *rmt_addr,\n\t\t\t\t      int dif)\n{\n\tstruct hlist_nulls_node *node;\n\tstruct sock *s = sk;\n\tunsigned short num = ntohs(loc_port);\n\n\tsk_nulls_for_each_from(s, node) {\n\t\tstruct inet_sock *inet = inet_sk(s);\n\n\t\tif (!net_eq(sock_net(s), net))\n\t\t\tcontinue;\n\n\t\tif (udp_sk(s)->udp_port_hash == num &&\n\t\t    s->sk_family == PF_INET6) {\n\t\t\tstruct ipv6_pinfo *np = inet6_sk(s);\n\t\t\tif (inet->inet_dport) {\n\t\t\t\tif (inet->inet_dport != rmt_port)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!ipv6_addr_any(&np->daddr) &&\n\t\t\t    !ipv6_addr_equal(&np->daddr, rmt_addr))\n\t\t\t\tcontinue;\n\n\t\t\tif (s->sk_bound_dev_if && s->sk_bound_dev_if != dif)\n\t\t\t\tcontinue;\n\n\t\t\tif (!ipv6_addr_any(&np->rcv_saddr)) {\n\t\t\t\tif (!ipv6_addr_equal(&np->rcv_saddr, loc_addr))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!inet6_mc_check(s, loc_addr, rmt_addr))\n\t\t\t\tcontinue;\n\t\t\treturn s;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic void flush_stack(struct sock **stack, unsigned int count,\n\t\t\tstruct sk_buff *skb, unsigned int final)\n{\n\tunsigned int i;\n\tstruct sock *sk;\n\tstruct sk_buff *skb1;\n\n\tfor (i = 0; i < count; i++) {\n\t\tskb1 = (i == final) ? skb : skb_clone(skb, GFP_ATOMIC);\n\n\t\tsk = stack[i];\n\t\tif (skb1) {\n\t\t\tif (sk_rcvqueues_full(sk, skb1)) {\n\t\t\t\tkfree_skb(skb1);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t\tbh_lock_sock(sk);\n\t\t\tif (!sock_owned_by_user(sk))\n\t\t\t\tudpv6_queue_rcv_skb(sk, skb1);\n\t\t\telse if (sk_add_backlog(sk, skb1)) {\n\t\t\t\tkfree_skb(skb1);\n\t\t\t\tbh_unlock_sock(sk);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t\tbh_unlock_sock(sk);\n\t\t\tcontinue;\n\t\t}\ndrop:\n\t\tatomic_inc(&sk->sk_drops);\n\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\tUDP_MIB_RCVBUFERRORS, IS_UDPLITE(sk));\n\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\tUDP_MIB_INERRORS, IS_UDPLITE(sk));\n\t}\n}\n/*\n * Note: called only from the BH handler context,\n * so we don't need to lock the hashes.\n */\nstatic int __udp6_lib_mcast_deliver(struct net *net, struct sk_buff *skb,\n\t\tconst struct in6_addr *saddr, const struct in6_addr *daddr,\n\t\tstruct udp_table *udptable)\n{\n\tstruct sock *sk, *stack[256 / sizeof(struct sock *)];\n\tconst struct udphdr *uh = udp_hdr(skb);\n\tstruct udp_hslot *hslot = udp_hashslot(udptable, net, ntohs(uh->dest));\n\tint dif;\n\tunsigned int i, count = 0;\n\n\tspin_lock(&hslot->lock);\n\tsk = sk_nulls_head(&hslot->head);\n\tdif = inet6_iif(skb);\n\tsk = udp_v6_mcast_next(net, sk, uh->dest, daddr, uh->source, saddr, dif);\n\twhile (sk) {\n\t\tstack[count++] = sk;\n\t\tsk = udp_v6_mcast_next(net, sk_nulls_next(sk), uh->dest, daddr,\n\t\t\t\t       uh->source, saddr, dif);\n\t\tif (unlikely(count == ARRAY_SIZE(stack))) {\n\t\t\tif (!sk)\n\t\t\t\tbreak;\n\t\t\tflush_stack(stack, count, skb, ~0);\n\t\t\tcount = 0;\n\t\t}\n\t}\n\t/*\n\t * before releasing the lock, we must take reference on sockets\n\t */\n\tfor (i = 0; i < count; i++)\n\t\tsock_hold(stack[i]);\n\n\tspin_unlock(&hslot->lock);\n\n\tif (count) {\n\t\tflush_stack(stack, count, skb, count - 1);\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tsock_put(stack[i]);\n\t} else {\n\t\tkfree_skb(skb);\n\t}\n\treturn 0;\n}\n\nstatic inline int udp6_csum_init(struct sk_buff *skb, struct udphdr *uh,\n\t\t\t\t int proto)\n{\n\tint err;\n\n\tUDP_SKB_CB(skb)->partial_cov = 0;\n\tUDP_SKB_CB(skb)->cscov = skb->len;\n\n\tif (proto == IPPROTO_UDPLITE) {\n\t\terr = udplite_checksum_init(skb, uh);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (uh->check == 0) {\n\t\t/* RFC 2460 section 8.1 says that we SHOULD log\n\t\t   this error. Well, it is reasonable.\n\t\t */\n\t\tLIMIT_NETDEBUG(KERN_INFO \"IPv6: udp checksum is 0\\n\");\n\t\treturn 1;\n\t}\n\tif (skb->ip_summed == CHECKSUM_COMPLETE &&\n\t    !csum_ipv6_magic(&ipv6_hdr(skb)->saddr, &ipv6_hdr(skb)->daddr,\n\t\t\t     skb->len, proto, skb->csum))\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tif (!skb_csum_unnecessary(skb))\n\t\tskb->csum = ~csum_unfold(csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t\t skb->len, proto, 0));\n\n\treturn 0;\n}\n\nint __udp6_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,\n\t\t   int proto)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tstruct sock *sk;\n\tstruct udphdr *uh;\n\tconst struct in6_addr *saddr, *daddr;\n\tu32 ulen = 0;\n\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto discard;\n\n\tsaddr = &ipv6_hdr(skb)->saddr;\n\tdaddr = &ipv6_hdr(skb)->daddr;\n\tuh = udp_hdr(skb);\n\n\tulen = ntohs(uh->len);\n\tif (ulen > skb->len)\n\t\tgoto short_packet;\n\n\tif (proto == IPPROTO_UDP) {\n\t\t/* UDP validates ulen. */\n\n\t\t/* Check for jumbo payload */\n\t\tif (ulen == 0)\n\t\t\tulen = skb->len;\n\n\t\tif (ulen < sizeof(*uh))\n\t\t\tgoto short_packet;\n\n\t\tif (ulen < skb->len) {\n\t\t\tif (pskb_trim_rcsum(skb, ulen))\n\t\t\t\tgoto short_packet;\n\t\t\tsaddr = &ipv6_hdr(skb)->saddr;\n\t\t\tdaddr = &ipv6_hdr(skb)->daddr;\n\t\t\tuh = udp_hdr(skb);\n\t\t}\n\t}\n\n\tif (udp6_csum_init(skb, uh, proto))\n\t\tgoto discard;\n\n\t/*\n\t *\tMulticast receive code\n\t */\n\tif (ipv6_addr_is_multicast(daddr))\n\t\treturn __udp6_lib_mcast_deliver(net, skb,\n\t\t\t\tsaddr, daddr, udptable);\n\n\t/* Unicast */\n\n\t/*\n\t * check socket cache ... must talk to Alan about his plans\n\t * for sock caches... i'll skip this for now.\n\t */\n\tsk = __udp6_lib_lookup_skb(skb, uh->source, uh->dest, udptable);\n\n\tif (sk == NULL) {\n\t\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\t\tgoto discard;\n\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto discard;\n\t\tUDP6_INC_STATS_BH(net, UDP_MIB_NOPORTS,\n\t\t\t\tproto == IPPROTO_UDPLITE);\n\n\t\ticmpv6_send(skb, ICMPV6_DEST_UNREACH, ICMPV6_PORT_UNREACH, 0);\n\n\t\tkfree_skb(skb);\n\t\treturn 0;\n\t}\n\n\t/* deliver */\n\n\tif (sk_rcvqueues_full(sk, skb)) {\n\t\tsock_put(sk);\n\t\tgoto discard;\n\t}\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\tudpv6_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tbh_unlock_sock(sk);\n\t\tsock_put(sk);\n\t\tgoto discard;\n\t}\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n\treturn 0;\n\nshort_packet:\n\tLIMIT_NETDEBUG(KERN_DEBUG \"UDP%sv6: short packet: From [%pI6c]:%u %d/%d to [%pI6c]:%u\\n\",\n\t\t       proto == IPPROTO_UDPLITE ? \"-Lite\" : \"\",\n\t\t       saddr,\n\t\t       ntohs(uh->source),\n\t\t       ulen,\n\t\t       skb->len,\n\t\t       daddr,\n\t\t       ntohs(uh->dest));\n\ndiscard:\n\tUDP6_INC_STATS_BH(net, UDP_MIB_INERRORS, proto == IPPROTO_UDPLITE);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic __inline__ int udpv6_rcv(struct sk_buff *skb)\n{\n\treturn __udp6_lib_rcv(skb, &udp_table, IPPROTO_UDP);\n}\n\n/*\n * Throw away all pending data and cancel the corking. Socket is locked.\n */\nstatic void udp_v6_flush_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (up->pending == AF_INET)\n\t\tudp_flush_pending_frames(sk);\n\telse if (up->pending) {\n\t\tup->len = 0;\n\t\tup->pending = 0;\n\t\tip6_flush_pending_frames(sk);\n\t}\n}\n\n/**\n * \tudp6_hwcsum_outgoing  -  handle outgoing HW checksumming\n * \t@sk: \tsocket we are sending on\n * \t@skb: \tsk_buff containing the filled-in UDP header\n * \t        (checksum field must be zeroed out)\n */\nstatic void udp6_hwcsum_outgoing(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t const struct in6_addr *saddr,\n\t\t\t\t const struct in6_addr *daddr, int len)\n{\n\tunsigned int offset;\n\tstruct udphdr *uh = udp_hdr(skb);\n\t__wsum csum = 0;\n\n\tif (skb_queue_len(&sk->sk_write_queue) == 1) {\n\t\t/* Only one fragment on the socket.  */\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP, 0);\n\t} else {\n\t\t/*\n\t\t * HW-checksum won't work as there are two or more\n\t\t * fragments on the socket so that all csums of sk_buffs\n\t\t * should be together\n\t\t */\n\t\toffset = skb_transport_offset(skb);\n\t\tskb->csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\tskb_queue_walk(&sk->sk_write_queue, skb) {\n\t\t\tcsum = csum_add(csum, skb->csum);\n\t\t}\n\n\t\tuh->check = csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP,\n\t\t\t\t\t    csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\n\n/*\n *\tSending\n */\n\nstatic int udp_v6_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tstruct udphdr *uh;\n\tstruct udp_sock  *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct flowi6 *fl6 = &inet->cork.fl.u.ip6;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\t__wsum csum = 0;\n\n\t/* Grab the skbuff where UDP header space exists. */\n\tif ((skb = skb_peek(&sk->sk_write_queue)) == NULL)\n\t\tgoto out;\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = fl6->fl6_sport;\n\tuh->dest = fl6->fl6_dport;\n\tuh->len = htons(up->len);\n\tuh->check = 0;\n\n\tif (is_udplite)\n\t\tcsum = udplite_csum_outgoing(sk, skb);\n\telse if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\n\t\tudp6_hwcsum_outgoing(sk, skb, &fl6->saddr, &fl6->daddr,\n\t\t\t\t     up->len);\n\t\tgoto send;\n\t} else\n\t\tcsum = udp_csum_outgoing(sk, skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_ipv6_magic(&fl6->saddr, &fl6->daddr,\n\t\t\t\t    up->len, fl6->flowi6_proto, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip6_push_pending_frames(sk);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet6_sk(sk)->recverr) {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t    UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t    UDP_MIB_OUTDATAGRAMS, is_udplite);\nout:\n\tup->len = 0;\n\tup->pending = 0;\n\treturn err;\n}\n\nint udpv6_sendmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *) msg->msg_name;\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &np->daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(iocb, sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(iocb, sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tdaddr = &flowlabel->dst;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &np->daddr))\n\t\t\tdaddr = &np->daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    ipv6_addr_type(daddr)&IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &np->daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = datagram_send_ctl(sock_net(sk), msg, &fl6, opt, &hlimit,\n\t\t\t\t\t&tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tipv6_addr_copy(&fl6.daddr, daddr);\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tipv6_addr_copy(&fl6.saddr, &np->saddr);\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t}\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p, true);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0) {\n\t\tif (ipv6_addr_is_multicast(&fl6.daddr))\n\t\t\thlimit = np->mcast_hops;\n\t\telse\n\t\t\thlimit = np->hop_limit;\n\t\tif (hlimit < 0)\n\t\t\thlimit = ip6_dst_hoplimit(dst);\n\t}\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tLIMIT_NETDEBUG(KERN_DEBUG \"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tup->len += ulen;\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\terr = ip6_append_data(sk, getfrag, msg->msg_iov, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info*)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &np->daddr) ?\n\t\t\t\t      &np->daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\n\nvoid udpv6_destroy_sock(struct sock *sk)\n{\n\tlock_sock(sk);\n\tudp_v6_flush_pending_frames(sk);\n\trelease_sock(sk);\n\n\tinet6_destroy_sock(sk);\n}\n\n/*\n *\tSocket option code for UDP\n */\nint udpv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t     char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_v6_push_pending_frames);\n\treturn ipv6_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udpv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_v6_push_pending_frames);\n\treturn compat_ipv6_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nint udpv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t     char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn ipv6_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udpv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn compat_ipv6_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nstatic int udp6_ufo_send_check(struct sk_buff *skb)\n{\n\tconst struct ipv6hdr *ipv6h;\n\tstruct udphdr *uh;\n\n\tif (!pskb_may_pull(skb, sizeof(*uh)))\n\t\treturn -EINVAL;\n\n\tipv6h = ipv6_hdr(skb);\n\tuh = udp_hdr(skb);\n\n\tuh->check = ~csum_ipv6_magic(&ipv6h->saddr, &ipv6h->daddr, skb->len,\n\t\t\t\t     IPPROTO_UDP, 0);\n\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\tskb->csum_offset = offsetof(struct udphdr, check);\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\treturn 0;\n}\n\nstatic struct sk_buff *udp6_ufo_fragment(struct sk_buff *skb, u32 features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tunsigned int mss;\n\tunsigned int unfrag_ip6hlen, unfrag_len;\n\tstruct frag_hdr *fptr;\n\tu8 *mac_start, *prevhdr;\n\tu8 nexthdr;\n\tu8 frag_hdr_sz = sizeof(struct frag_hdr);\n\tint offset;\n\t__wsum csum;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type & ~(SKB_GSO_UDP | SKB_GSO_DODGY) ||\n\t\t\t     !(type & (SKB_GSO_UDP))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\t/* Do software UFO. Complete and fill in the UDP checksum as HW cannot\n\t * do checksum of UDP packets sent as multiple IP fragments.\n\t */\n\toffset = skb_checksum_start_offset(skb);\n\tcsum = skb_checksum(skb, offset, skb->len- offset, 0);\n\toffset += skb->csum_offset;\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\t/* Check if there is enough headroom to insert fragment header. */\n\tif ((skb_mac_header(skb) < skb->head + frag_hdr_sz) &&\n\t    pskb_expand_head(skb, frag_hdr_sz, 0, GFP_ATOMIC))\n\t\tgoto out;\n\n\t/* Find the unfragmentable header and shift it left by frag_hdr_sz\n\t * bytes to insert fragment header.\n\t */\n\tunfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);\n\tnexthdr = *prevhdr;\n\t*prevhdr = NEXTHDR_FRAGMENT;\n\tunfrag_len = skb_network_header(skb) - skb_mac_header(skb) +\n\t\t     unfrag_ip6hlen;\n\tmac_start = skb_mac_header(skb);\n\tmemmove(mac_start-frag_hdr_sz, mac_start, unfrag_len);\n\n\tskb->mac_header -= frag_hdr_sz;\n\tskb->network_header -= frag_hdr_sz;\n\n\tfptr = (struct frag_hdr *)(skb_network_header(skb) + unfrag_ip6hlen);\n\tfptr->nexthdr = nexthdr;\n\tfptr->reserved = 0;\n\tipv6_select_ident(fptr);\n\n\t/* Fragment the skb. ipv6 header and the remaining fields of the\n\t * fragment header are updated in ipv6_gso_segment()\n\t */\n\tsegs = skb_segment(skb, features);\n\nout:\n\treturn segs;\n}\n\nstatic const struct inet6_protocol udpv6_protocol = {\n\t.handler\t=\tudpv6_rcv,\n\t.err_handler\t=\tudpv6_err,\n\t.gso_send_check =\tudp6_ufo_send_check,\n\t.gso_segment\t=\tudp6_ufo_fragment,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY|INET6_PROTO_FINAL,\n};\n\n/* ------------------------------------------------------------------------ */\n#ifdef CONFIG_PROC_FS\n\nstatic void udp6_sock_seq_show(struct seq_file *seq, struct sock *sp, int bucket)\n{\n\tstruct inet_sock *inet = inet_sk(sp);\n\tstruct ipv6_pinfo *np = inet6_sk(sp);\n\tconst struct in6_addr *dest, *src;\n\t__u16 destp, srcp;\n\n\tdest  = &np->daddr;\n\tsrc   = &np->rcv_saddr;\n\tdestp = ntohs(inet->inet_dport);\n\tsrcp  = ntohs(inet->inet_sport);\n\tseq_printf(seq,\n\t\t   \"%5d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5d %8d %lu %d %pK %d\\n\",\n\t\t   bucket,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   sp->sk_state,\n\t\t   sk_wmem_alloc_get(sp),\n\t\t   sk_rmem_alloc_get(sp),\n\t\t   0, 0L, 0,\n\t\t   sock_i_uid(sp), 0,\n\t\t   sock_i_ino(sp),\n\t\t   atomic_read(&sp->sk_refcnt), sp,\n\t\t   atomic_read(&sp->sk_drops));\n}\n\nint udp6_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_printf(seq,\n\t\t\t   \"  sl  \"\n\t\t\t   \"local_address                         \"\n\t\t\t   \"remote_address                        \"\n\t\t\t   \"st tx_queue rx_queue tr tm->when retrnsmt\"\n\t\t\t   \"   uid  timeout inode ref pointer drops\\n\");\n\telse\n\t\tudp6_sock_seq_show(seq, v, ((struct udp_iter_state *)seq->private)->bucket);\n\treturn 0;\n}\n\nstatic struct udp_seq_afinfo udp6_seq_afinfo = {\n\t.name\t\t= \"udp6\",\n\t.family\t\t= AF_INET6,\n\t.udp_table\t= &udp_table,\n\t.seq_fops\t= {\n\t\t.owner\t=\tTHIS_MODULE,\n\t},\n\t.seq_ops\t= {\n\t\t.show\t\t= udp6_seq_show,\n\t},\n};\n\nint __net_init udp6_proc_init(struct net *net)\n{\n\treturn udp_proc_register(net, &udp6_seq_afinfo);\n}\n\nvoid udp6_proc_exit(struct net *net) {\n\tudp_proc_unregister(net, &udp6_seq_afinfo);\n}\n#endif /* CONFIG_PROC_FS */\n\n/* ------------------------------------------------------------------------ */\n\nstruct proto udpv6_prot = {\n\t.name\t\t   = \"UDPv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = udp_lib_close,\n\t.connect\t   = ip6_datagram_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = udpv6_destroy_sock,\n\t.setsockopt\t   = udpv6_setsockopt,\n\t.getsockopt\t   = udpv6_getsockopt,\n\t.sendmsg\t   = udpv6_sendmsg,\n\t.recvmsg\t   = udpv6_recvmsg,\n\t.backlog_rcv\t   = udpv6_queue_rcv_skb,\n\t.hash\t\t   = udp_lib_hash,\n\t.unhash\t\t   = udp_lib_unhash,\n\t.rehash\t\t   = udp_v6_rehash,\n\t.get_port\t   = udp_v6_get_port,\n\t.memory_allocated  = &udp_memory_allocated,\n\t.sysctl_mem\t   = sysctl_udp_mem,\n\t.sysctl_wmem\t   = &sysctl_udp_wmem_min,\n\t.sysctl_rmem\t   = &sysctl_udp_rmem_min,\n\t.obj_size\t   = sizeof(struct udp6_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.h.udp_table\t   = &udp_table,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_udpv6_setsockopt,\n\t.compat_getsockopt = compat_udpv6_getsockopt,\n#endif\n\t.clear_sk\t   = sk_prot_clear_portaddr_nulls,\n};\n\nstatic struct inet_protosw udpv6_protosw = {\n\t.type =      SOCK_DGRAM,\n\t.protocol =  IPPROTO_UDP,\n\t.prot =      &udpv6_prot,\n\t.ops =       &inet6_dgram_ops,\n\t.no_check =  UDP_CSUM_DEFAULT,\n\t.flags =     INET_PROTOSW_PERMANENT,\n};\n\n\nint __init udpv6_init(void)\n{\n\tint ret;\n\n\tret = inet6_add_protocol(&udpv6_protocol, IPPROTO_UDP);\n\tif (ret)\n\t\tgoto out;\n\n\tret = inet6_register_protosw(&udpv6_protosw);\n\tif (ret)\n\t\tgoto out_udpv6_protocol;\nout:\n\treturn ret;\n\nout_udpv6_protocol:\n\tinet6_del_protocol(&udpv6_protocol, IPPROTO_UDP);\n\tgoto out;\n}\n\nvoid udpv6_exit(void)\n{\n\tinet6_unregister_protosw(&udpv6_protosw);\n\tinet6_del_protocol(&udpv6_protocol, IPPROTO_UDP);\n}\n"], "fixing_code": ["/*\n * random.c -- A strong random number generator\n *\n * Copyright Matt Mackall <mpm@selenic.com>, 2003, 2004, 2005\n *\n * Copyright Theodore Ts'o, 1994, 1995, 1996, 1997, 1998, 1999.  All\n * rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, and the entire permission notice in its entirety,\n *    including the disclaimer of warranties.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n * 3. The name of the author may not be used to endorse or promote\n *    products derived from this software without specific prior\n *    written permission.\n *\n * ALTERNATIVELY, this product may be distributed under the terms of\n * the GNU General Public License, in which case the provisions of the GPL are\n * required INSTEAD OF the above restrictions.  (This clause is\n * necessary due to a potential bad interaction between the GPL and\n * the restrictions contained in a BSD-style copyright.)\n *\n * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED\n * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\n * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, ALL OF\n * WHICH ARE HEREBY DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE\n * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT\n * OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE\n * USE OF THIS SOFTWARE, EVEN IF NOT ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n */\n\n/*\n * (now, with legal B.S. out of the way.....)\n *\n * This routine gathers environmental noise from device drivers, etc.,\n * and returns good random numbers, suitable for cryptographic use.\n * Besides the obvious cryptographic uses, these numbers are also good\n * for seeding TCP sequence numbers, and other places where it is\n * desirable to have numbers which are not only random, but hard to\n * predict by an attacker.\n *\n * Theory of operation\n * ===================\n *\n * Computers are very predictable devices.  Hence it is extremely hard\n * to produce truly random numbers on a computer --- as opposed to\n * pseudo-random numbers, which can easily generated by using a\n * algorithm.  Unfortunately, it is very easy for attackers to guess\n * the sequence of pseudo-random number generators, and for some\n * applications this is not acceptable.  So instead, we must try to\n * gather \"environmental noise\" from the computer's environment, which\n * must be hard for outside attackers to observe, and use that to\n * generate random numbers.  In a Unix environment, this is best done\n * from inside the kernel.\n *\n * Sources of randomness from the environment include inter-keyboard\n * timings, inter-interrupt timings from some interrupts, and other\n * events which are both (a) non-deterministic and (b) hard for an\n * outside observer to measure.  Randomness from these sources are\n * added to an \"entropy pool\", which is mixed using a CRC-like function.\n * This is not cryptographically strong, but it is adequate assuming\n * the randomness is not chosen maliciously, and it is fast enough that\n * the overhead of doing it on every interrupt is very reasonable.\n * As random bytes are mixed into the entropy pool, the routines keep\n * an *estimate* of how many bits of randomness have been stored into\n * the random number generator's internal state.\n *\n * When random bytes are desired, they are obtained by taking the SHA\n * hash of the contents of the \"entropy pool\".  The SHA hash avoids\n * exposing the internal state of the entropy pool.  It is believed to\n * be computationally infeasible to derive any useful information\n * about the input of SHA from its output.  Even if it is possible to\n * analyze SHA in some clever way, as long as the amount of data\n * returned from the generator is less than the inherent entropy in\n * the pool, the output data is totally unpredictable.  For this\n * reason, the routine decreases its internal estimate of how many\n * bits of \"true randomness\" are contained in the entropy pool as it\n * outputs random numbers.\n *\n * If this estimate goes to zero, the routine can still generate\n * random numbers; however, an attacker may (at least in theory) be\n * able to infer the future output of the generator from prior\n * outputs.  This requires successful cryptanalysis of SHA, which is\n * not believed to be feasible, but there is a remote possibility.\n * Nonetheless, these numbers should be useful for the vast majority\n * of purposes.\n *\n * Exported interfaces ---- output\n * ===============================\n *\n * There are three exported interfaces; the first is one designed to\n * be used from within the kernel:\n *\n * \tvoid get_random_bytes(void *buf, int nbytes);\n *\n * This interface will return the requested number of random bytes,\n * and place it in the requested buffer.\n *\n * The two other interfaces are two character devices /dev/random and\n * /dev/urandom.  /dev/random is suitable for use when very high\n * quality randomness is desired (for example, for key generation or\n * one-time pads), as it will only return a maximum of the number of\n * bits of randomness (as estimated by the random number generator)\n * contained in the entropy pool.\n *\n * The /dev/urandom device does not have this limit, and will return\n * as many bytes as are requested.  As more and more random bytes are\n * requested without giving time for the entropy pool to recharge,\n * this will result in random numbers that are merely cryptographically\n * strong.  For many applications, however, this is acceptable.\n *\n * Exported interfaces ---- input\n * ==============================\n *\n * The current exported interfaces for gathering environmental noise\n * from the devices are:\n *\n * \tvoid add_input_randomness(unsigned int type, unsigned int code,\n *                                unsigned int value);\n * \tvoid add_interrupt_randomness(int irq);\n * \tvoid add_disk_randomness(struct gendisk *disk);\n *\n * add_input_randomness() uses the input layer interrupt timing, as well as\n * the event type information from the hardware.\n *\n * add_interrupt_randomness() uses the inter-interrupt timing as random\n * inputs to the entropy pool.  Note that not all interrupts are good\n * sources of randomness!  For example, the timer interrupts is not a\n * good choice, because the periodicity of the interrupts is too\n * regular, and hence predictable to an attacker.  Network Interface\n * Controller interrupts are a better measure, since the timing of the\n * NIC interrupts are more unpredictable.\n *\n * add_disk_randomness() uses what amounts to the seek time of block\n * layer request events, on a per-disk_devt basis, as input to the\n * entropy pool. Note that high-speed solid state drives with very low\n * seek times do not make for good sources of entropy, as their seek\n * times are usually fairly consistent.\n *\n * All of these routines try to estimate how many bits of randomness a\n * particular randomness source.  They do this by keeping track of the\n * first and second order deltas of the event timings.\n *\n * Ensuring unpredictability at system startup\n * ============================================\n *\n * When any operating system starts up, it will go through a sequence\n * of actions that are fairly predictable by an adversary, especially\n * if the start-up does not involve interaction with a human operator.\n * This reduces the actual number of bits of unpredictability in the\n * entropy pool below the value in entropy_count.  In order to\n * counteract this effect, it helps to carry information in the\n * entropy pool across shut-downs and start-ups.  To do this, put the\n * following lines an appropriate script which is run during the boot\n * sequence:\n *\n *\techo \"Initializing random number generator...\"\n *\trandom_seed=/var/run/random-seed\n *\t# Carry a random seed from start-up to start-up\n *\t# Load and then save the whole entropy pool\n *\tif [ -f $random_seed ]; then\n *\t\tcat $random_seed >/dev/urandom\n *\telse\n *\t\ttouch $random_seed\n *\tfi\n *\tchmod 600 $random_seed\n *\tdd if=/dev/urandom of=$random_seed count=1 bs=512\n *\n * and the following lines in an appropriate script which is run as\n * the system is shutdown:\n *\n *\t# Carry a random seed from shut-down to start-up\n *\t# Save the whole entropy pool\n *\techo \"Saving random seed...\"\n *\trandom_seed=/var/run/random-seed\n *\ttouch $random_seed\n *\tchmod 600 $random_seed\n *\tdd if=/dev/urandom of=$random_seed count=1 bs=512\n *\n * For example, on most modern systems using the System V init\n * scripts, such code fragments would be found in\n * /etc/rc.d/init.d/random.  On older Linux systems, the correct script\n * location might be in /etc/rcb.d/rc.local or /etc/rc.d/rc.0.\n *\n * Effectively, these commands cause the contents of the entropy pool\n * to be saved at shut-down time and reloaded into the entropy pool at\n * start-up.  (The 'dd' in the addition to the bootup script is to\n * make sure that /etc/random-seed is different for every start-up,\n * even if the system crashes without executing rc.0.)  Even with\n * complete knowledge of the start-up activities, predicting the state\n * of the entropy pool requires knowledge of the previous history of\n * the system.\n *\n * Configuring the /dev/random driver under Linux\n * ==============================================\n *\n * The /dev/random driver under Linux uses minor numbers 8 and 9 of\n * the /dev/mem major number (#1).  So if your system does not have\n * /dev/random and /dev/urandom created already, they can be created\n * by using the commands:\n *\n * \tmknod /dev/random c 1 8\n * \tmknod /dev/urandom c 1 9\n *\n * Acknowledgements:\n * =================\n *\n * Ideas for constructing this random number generator were derived\n * from Pretty Good Privacy's random number generator, and from private\n * discussions with Phil Karn.  Colin Plumb provided a faster random\n * number generator, which speed up the mixing function of the entropy\n * pool, taken from PGPfone.  Dale Worley has also contributed many\n * useful ideas and suggestions to improve this driver.\n *\n * Any flaws in the design are solely my responsibility, and should\n * not be attributed to the Phil, Colin, or any of authors of PGP.\n *\n * Further background information on this topic may be obtained from\n * RFC 1750, \"Randomness Recommendations for Security\", by Donald\n * Eastlake, Steve Crocker, and Jeff Schiller.\n */\n\n#include <linux/utsname.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/major.h>\n#include <linux/string.h>\n#include <linux/fcntl.h>\n#include <linux/slab.h>\n#include <linux/random.h>\n#include <linux/poll.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/genhd.h>\n#include <linux/interrupt.h>\n#include <linux/mm.h>\n#include <linux/spinlock.h>\n#include <linux/percpu.h>\n#include <linux/cryptohash.h>\n#include <linux/fips.h>\n\n#ifdef CONFIG_GENERIC_HARDIRQS\n# include <linux/irq.h>\n#endif\n\n#include <asm/processor.h>\n#include <asm/uaccess.h>\n#include <asm/irq.h>\n#include <asm/io.h>\n\n/*\n * Configuration information\n */\n#define INPUT_POOL_WORDS 128\n#define OUTPUT_POOL_WORDS 32\n#define SEC_XFER_SIZE 512\n#define EXTRACT_SIZE 10\n\n/*\n * The minimum number of bits of entropy before we wake up a read on\n * /dev/random.  Should be enough to do a significant reseed.\n */\nstatic int random_read_wakeup_thresh = 64;\n\n/*\n * If the entropy count falls under this number of bits, then we\n * should wake up processes which are selecting or polling on write\n * access to /dev/random.\n */\nstatic int random_write_wakeup_thresh = 128;\n\n/*\n * When the input pool goes over trickle_thresh, start dropping most\n * samples to avoid wasting CPU time and reduce lock contention.\n */\n\nstatic int trickle_thresh __read_mostly = INPUT_POOL_WORDS * 28;\n\nstatic DEFINE_PER_CPU(int, trickle_count);\n\n/*\n * A pool of size .poolwords is stirred with a primitive polynomial\n * of degree .poolwords over GF(2).  The taps for various sizes are\n * defined below.  They are chosen to be evenly spaced (minimum RMS\n * distance from evenly spaced; the numbers in the comments are a\n * scaled squared error sum) except for the last tap, which is 1 to\n * get the twisting happening as fast as possible.\n */\nstatic struct poolinfo {\n\tint poolwords;\n\tint tap1, tap2, tap3, tap4, tap5;\n} poolinfo_table[] = {\n\t/* x^128 + x^103 + x^76 + x^51 +x^25 + x + 1 -- 105 */\n\t{ 128,\t103,\t76,\t51,\t25,\t1 },\n\t/* x^32 + x^26 + x^20 + x^14 + x^7 + x + 1 -- 15 */\n\t{ 32,\t26,\t20,\t14,\t7,\t1 },\n#if 0\n\t/* x^2048 + x^1638 + x^1231 + x^819 + x^411 + x + 1  -- 115 */\n\t{ 2048,\t1638,\t1231,\t819,\t411,\t1 },\n\n\t/* x^1024 + x^817 + x^615 + x^412 + x^204 + x + 1 -- 290 */\n\t{ 1024,\t817,\t615,\t412,\t204,\t1 },\n\n\t/* x^1024 + x^819 + x^616 + x^410 + x^207 + x^2 + 1 -- 115 */\n\t{ 1024,\t819,\t616,\t410,\t207,\t2 },\n\n\t/* x^512 + x^411 + x^308 + x^208 + x^104 + x + 1 -- 225 */\n\t{ 512,\t411,\t308,\t208,\t104,\t1 },\n\n\t/* x^512 + x^409 + x^307 + x^206 + x^102 + x^2 + 1 -- 95 */\n\t{ 512,\t409,\t307,\t206,\t102,\t2 },\n\t/* x^512 + x^409 + x^309 + x^205 + x^103 + x^2 + 1 -- 95 */\n\t{ 512,\t409,\t309,\t205,\t103,\t2 },\n\n\t/* x^256 + x^205 + x^155 + x^101 + x^52 + x + 1 -- 125 */\n\t{ 256,\t205,\t155,\t101,\t52,\t1 },\n\n\t/* x^128 + x^103 + x^78 + x^51 + x^27 + x^2 + 1 -- 70 */\n\t{ 128,\t103,\t78,\t51,\t27,\t2 },\n\n\t/* x^64 + x^52 + x^39 + x^26 + x^14 + x + 1 -- 15 */\n\t{ 64,\t52,\t39,\t26,\t14,\t1 },\n#endif\n};\n\n#define POOLBITS\tpoolwords*32\n#define POOLBYTES\tpoolwords*4\n\n/*\n * For the purposes of better mixing, we use the CRC-32 polynomial as\n * well to make a twisted Generalized Feedback Shift Reigster\n *\n * (See M. Matsumoto & Y. Kurita, 1992.  Twisted GFSR generators.  ACM\n * Transactions on Modeling and Computer Simulation 2(3):179-194.\n * Also see M. Matsumoto & Y. Kurita, 1994.  Twisted GFSR generators\n * II.  ACM Transactions on Mdeling and Computer Simulation 4:254-266)\n *\n * Thanks to Colin Plumb for suggesting this.\n *\n * We have not analyzed the resultant polynomial to prove it primitive;\n * in fact it almost certainly isn't.  Nonetheless, the irreducible factors\n * of a random large-degree polynomial over GF(2) are more than large enough\n * that periodicity is not a concern.\n *\n * The input hash is much less sensitive than the output hash.  All\n * that we want of it is that it be a good non-cryptographic hash;\n * i.e. it not produce collisions when fed \"random\" data of the sort\n * we expect to see.  As long as the pool state differs for different\n * inputs, we have preserved the input entropy and done a good job.\n * The fact that an intelligent attacker can construct inputs that\n * will produce controlled alterations to the pool's state is not\n * important because we don't consider such inputs to contribute any\n * randomness.  The only property we need with respect to them is that\n * the attacker can't increase his/her knowledge of the pool's state.\n * Since all additions are reversible (knowing the final state and the\n * input, you can reconstruct the initial state), if an attacker has\n * any uncertainty about the initial state, he/she can only shuffle\n * that uncertainty about, but never cause any collisions (which would\n * decrease the uncertainty).\n *\n * The chosen system lets the state of the pool be (essentially) the input\n * modulo the generator polymnomial.  Now, for random primitive polynomials,\n * this is a universal class of hash functions, meaning that the chance\n * of a collision is limited by the attacker's knowledge of the generator\n * polynomail, so if it is chosen at random, an attacker can never force\n * a collision.  Here, we use a fixed polynomial, but we *can* assume that\n * ###--> it is unknown to the processes generating the input entropy. <-###\n * Because of this important property, this is a good, collision-resistant\n * hash; hash collisions will occur no more often than chance.\n */\n\n/*\n * Static global variables\n */\nstatic DECLARE_WAIT_QUEUE_HEAD(random_read_wait);\nstatic DECLARE_WAIT_QUEUE_HEAD(random_write_wait);\nstatic struct fasync_struct *fasync;\n\n#if 0\nstatic int debug;\nmodule_param(debug, bool, 0644);\n#define DEBUG_ENT(fmt, arg...) do { \\\n\tif (debug) \\\n\t\tprintk(KERN_DEBUG \"random %04d %04d %04d: \" \\\n\t\tfmt,\\\n\t\tinput_pool.entropy_count,\\\n\t\tblocking_pool.entropy_count,\\\n\t\tnonblocking_pool.entropy_count,\\\n\t\t## arg); } while (0)\n#else\n#define DEBUG_ENT(fmt, arg...) do {} while (0)\n#endif\n\n/**********************************************************************\n *\n * OS independent entropy store.   Here are the functions which handle\n * storing entropy in an entropy pool.\n *\n **********************************************************************/\n\nstruct entropy_store;\nstruct entropy_store {\n\t/* read-only data: */\n\tstruct poolinfo *poolinfo;\n\t__u32 *pool;\n\tconst char *name;\n\tstruct entropy_store *pull;\n\tint limit;\n\n\t/* read-write data: */\n\tspinlock_t lock;\n\tunsigned add_ptr;\n\tint entropy_count;\n\tint input_rotate;\n\t__u8 last_data[EXTRACT_SIZE];\n};\n\nstatic __u32 input_pool_data[INPUT_POOL_WORDS];\nstatic __u32 blocking_pool_data[OUTPUT_POOL_WORDS];\nstatic __u32 nonblocking_pool_data[OUTPUT_POOL_WORDS];\n\nstatic struct entropy_store input_pool = {\n\t.poolinfo = &poolinfo_table[0],\n\t.name = \"input\",\n\t.limit = 1,\n\t.lock = __SPIN_LOCK_UNLOCKED(&input_pool.lock),\n\t.pool = input_pool_data\n};\n\nstatic struct entropy_store blocking_pool = {\n\t.poolinfo = &poolinfo_table[1],\n\t.name = \"blocking\",\n\t.limit = 1,\n\t.pull = &input_pool,\n\t.lock = __SPIN_LOCK_UNLOCKED(&blocking_pool.lock),\n\t.pool = blocking_pool_data\n};\n\nstatic struct entropy_store nonblocking_pool = {\n\t.poolinfo = &poolinfo_table[1],\n\t.name = \"nonblocking\",\n\t.pull = &input_pool,\n\t.lock = __SPIN_LOCK_UNLOCKED(&nonblocking_pool.lock),\n\t.pool = nonblocking_pool_data\n};\n\n/*\n * This function adds bytes into the entropy \"pool\".  It does not\n * update the entropy estimate.  The caller should call\n * credit_entropy_bits if this is appropriate.\n *\n * The pool is stirred with a primitive polynomial of the appropriate\n * degree, and then twisted.  We twist by three bits at a time because\n * it's cheap to do so and helps slightly in the expected case where\n * the entropy is concentrated in the low-order bits.\n */\nstatic void mix_pool_bytes_extract(struct entropy_store *r, const void *in,\n\t\t\t\t   int nbytes, __u8 out[64])\n{\n\tstatic __u32 const twist_table[8] = {\n\t\t0x00000000, 0x3b6e20c8, 0x76dc4190, 0x4db26158,\n\t\t0xedb88320, 0xd6d6a3e8, 0x9b64c2b0, 0xa00ae278 };\n\tunsigned long i, j, tap1, tap2, tap3, tap4, tap5;\n\tint input_rotate;\n\tint wordmask = r->poolinfo->poolwords - 1;\n\tconst char *bytes = in;\n\t__u32 w;\n\tunsigned long flags;\n\n\t/* Taps are constant, so we can load them without holding r->lock.  */\n\ttap1 = r->poolinfo->tap1;\n\ttap2 = r->poolinfo->tap2;\n\ttap3 = r->poolinfo->tap3;\n\ttap4 = r->poolinfo->tap4;\n\ttap5 = r->poolinfo->tap5;\n\n\tspin_lock_irqsave(&r->lock, flags);\n\tinput_rotate = r->input_rotate;\n\ti = r->add_ptr;\n\n\t/* mix one byte at a time to simplify size handling and churn faster */\n\twhile (nbytes--) {\n\t\tw = rol32(*bytes++, input_rotate & 31);\n\t\ti = (i - 1) & wordmask;\n\n\t\t/* XOR in the various taps */\n\t\tw ^= r->pool[i];\n\t\tw ^= r->pool[(i + tap1) & wordmask];\n\t\tw ^= r->pool[(i + tap2) & wordmask];\n\t\tw ^= r->pool[(i + tap3) & wordmask];\n\t\tw ^= r->pool[(i + tap4) & wordmask];\n\t\tw ^= r->pool[(i + tap5) & wordmask];\n\n\t\t/* Mix the result back in with a twist */\n\t\tr->pool[i] = (w >> 3) ^ twist_table[w & 7];\n\n\t\t/*\n\t\t * Normally, we add 7 bits of rotation to the pool.\n\t\t * At the beginning of the pool, add an extra 7 bits\n\t\t * rotation, so that successive passes spread the\n\t\t * input bits across the pool evenly.\n\t\t */\n\t\tinput_rotate += i ? 7 : 14;\n\t}\n\n\tr->input_rotate = input_rotate;\n\tr->add_ptr = i;\n\n\tif (out)\n\t\tfor (j = 0; j < 16; j++)\n\t\t\t((__u32 *)out)[j] = r->pool[(i - j) & wordmask];\n\n\tspin_unlock_irqrestore(&r->lock, flags);\n}\n\nstatic void mix_pool_bytes(struct entropy_store *r, const void *in, int bytes)\n{\n       mix_pool_bytes_extract(r, in, bytes, NULL);\n}\n\n/*\n * Credit (or debit) the entropy store with n bits of entropy\n */\nstatic void credit_entropy_bits(struct entropy_store *r, int nbits)\n{\n\tunsigned long flags;\n\tint entropy_count;\n\n\tif (!nbits)\n\t\treturn;\n\n\tspin_lock_irqsave(&r->lock, flags);\n\n\tDEBUG_ENT(\"added %d entropy credits to %s\\n\", nbits, r->name);\n\tentropy_count = r->entropy_count;\n\tentropy_count += nbits;\n\tif (entropy_count < 0) {\n\t\tDEBUG_ENT(\"negative entropy/overflow\\n\");\n\t\tentropy_count = 0;\n\t} else if (entropy_count > r->poolinfo->POOLBITS)\n\t\tentropy_count = r->poolinfo->POOLBITS;\n\tr->entropy_count = entropy_count;\n\n\t/* should we wake readers? */\n\tif (r == &input_pool && entropy_count >= random_read_wakeup_thresh) {\n\t\twake_up_interruptible(&random_read_wait);\n\t\tkill_fasync(&fasync, SIGIO, POLL_IN);\n\t}\n\tspin_unlock_irqrestore(&r->lock, flags);\n}\n\n/*********************************************************************\n *\n * Entropy input management\n *\n *********************************************************************/\n\n/* There is one of these per entropy source */\nstruct timer_rand_state {\n\tcycles_t last_time;\n\tlong last_delta, last_delta2;\n\tunsigned dont_count_entropy:1;\n};\n\n#ifndef CONFIG_GENERIC_HARDIRQS\n\nstatic struct timer_rand_state *irq_timer_state[NR_IRQS];\n\nstatic struct timer_rand_state *get_timer_rand_state(unsigned int irq)\n{\n\treturn irq_timer_state[irq];\n}\n\nstatic void set_timer_rand_state(unsigned int irq,\n\t\t\t\t struct timer_rand_state *state)\n{\n\tirq_timer_state[irq] = state;\n}\n\n#else\n\nstatic struct timer_rand_state *get_timer_rand_state(unsigned int irq)\n{\n\tstruct irq_desc *desc;\n\n\tdesc = irq_to_desc(irq);\n\n\treturn desc->timer_rand_state;\n}\n\nstatic void set_timer_rand_state(unsigned int irq,\n\t\t\t\t struct timer_rand_state *state)\n{\n\tstruct irq_desc *desc;\n\n\tdesc = irq_to_desc(irq);\n\n\tdesc->timer_rand_state = state;\n}\n#endif\n\nstatic struct timer_rand_state input_timer_state;\n\n/*\n * This function adds entropy to the entropy \"pool\" by using timing\n * delays.  It uses the timer_rand_state structure to make an estimate\n * of how many bits of entropy this call has added to the pool.\n *\n * The number \"num\" is also added to the pool - it should somehow describe\n * the type of event which just happened.  This is currently 0-255 for\n * keyboard scan codes, and 256 upwards for interrupts.\n *\n */\nstatic void add_timer_randomness(struct timer_rand_state *state, unsigned num)\n{\n\tstruct {\n\t\tcycles_t cycles;\n\t\tlong jiffies;\n\t\tunsigned num;\n\t} sample;\n\tlong delta, delta2, delta3;\n\n\tpreempt_disable();\n\t/* if over the trickle threshold, use only 1 in 4096 samples */\n\tif (input_pool.entropy_count > trickle_thresh &&\n\t    ((__this_cpu_inc_return(trickle_count) - 1) & 0xfff))\n\t\tgoto out;\n\n\tsample.jiffies = jiffies;\n\tsample.cycles = get_cycles();\n\tsample.num = num;\n\tmix_pool_bytes(&input_pool, &sample, sizeof(sample));\n\n\t/*\n\t * Calculate number of bits of randomness we probably added.\n\t * We take into account the first, second and third-order deltas\n\t * in order to make our estimate.\n\t */\n\n\tif (!state->dont_count_entropy) {\n\t\tdelta = sample.jiffies - state->last_time;\n\t\tstate->last_time = sample.jiffies;\n\n\t\tdelta2 = delta - state->last_delta;\n\t\tstate->last_delta = delta;\n\n\t\tdelta3 = delta2 - state->last_delta2;\n\t\tstate->last_delta2 = delta2;\n\n\t\tif (delta < 0)\n\t\t\tdelta = -delta;\n\t\tif (delta2 < 0)\n\t\t\tdelta2 = -delta2;\n\t\tif (delta3 < 0)\n\t\t\tdelta3 = -delta3;\n\t\tif (delta > delta2)\n\t\t\tdelta = delta2;\n\t\tif (delta > delta3)\n\t\t\tdelta = delta3;\n\n\t\t/*\n\t\t * delta is now minimum absolute delta.\n\t\t * Round down by 1 bit on general principles,\n\t\t * and limit entropy entimate to 12 bits.\n\t\t */\n\t\tcredit_entropy_bits(&input_pool,\n\t\t\t\t    min_t(int, fls(delta>>1), 11));\n\t}\nout:\n\tpreempt_enable();\n}\n\nvoid add_input_randomness(unsigned int type, unsigned int code,\n\t\t\t\t unsigned int value)\n{\n\tstatic unsigned char last_value;\n\n\t/* ignore autorepeat and the like */\n\tif (value == last_value)\n\t\treturn;\n\n\tDEBUG_ENT(\"input event\\n\");\n\tlast_value = value;\n\tadd_timer_randomness(&input_timer_state,\n\t\t\t     (type << 4) ^ code ^ (code >> 4) ^ value);\n}\nEXPORT_SYMBOL_GPL(add_input_randomness);\n\nvoid add_interrupt_randomness(int irq)\n{\n\tstruct timer_rand_state *state;\n\n\tstate = get_timer_rand_state(irq);\n\n\tif (state == NULL)\n\t\treturn;\n\n\tDEBUG_ENT(\"irq event %d\\n\", irq);\n\tadd_timer_randomness(state, 0x100 + irq);\n}\n\n#ifdef CONFIG_BLOCK\nvoid add_disk_randomness(struct gendisk *disk)\n{\n\tif (!disk || !disk->random)\n\t\treturn;\n\t/* first major is 1, so we get >= 0x200 here */\n\tDEBUG_ENT(\"disk event %d:%d\\n\",\n\t\t  MAJOR(disk_devt(disk)), MINOR(disk_devt(disk)));\n\n\tadd_timer_randomness(disk->random, 0x100 + disk_devt(disk));\n}\n#endif\n\n/*********************************************************************\n *\n * Entropy extraction routines\n *\n *********************************************************************/\n\nstatic ssize_t extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t       size_t nbytes, int min, int rsvd);\n\n/*\n * This utility inline function is responsible for transferring entropy\n * from the primary pool to the secondary extraction pool. We make\n * sure we pull enough for a 'catastrophic reseed'.\n */\nstatic void xfer_secondary_pool(struct entropy_store *r, size_t nbytes)\n{\n\t__u32 tmp[OUTPUT_POOL_WORDS];\n\n\tif (r->pull && r->entropy_count < nbytes * 8 &&\n\t    r->entropy_count < r->poolinfo->POOLBITS) {\n\t\t/* If we're limited, always leave two wakeup worth's BITS */\n\t\tint rsvd = r->limit ? 0 : random_read_wakeup_thresh/4;\n\t\tint bytes = nbytes;\n\n\t\t/* pull at least as many as BYTES as wakeup BITS */\n\t\tbytes = max_t(int, bytes, random_read_wakeup_thresh / 8);\n\t\t/* but never more than the buffer size */\n\t\tbytes = min_t(int, bytes, sizeof(tmp));\n\n\t\tDEBUG_ENT(\"going to reseed %s with %d bits \"\n\t\t\t  \"(%d of %d requested)\\n\",\n\t\t\t  r->name, bytes * 8, nbytes * 8, r->entropy_count);\n\n\t\tbytes = extract_entropy(r->pull, tmp, bytes,\n\t\t\t\t\trandom_read_wakeup_thresh / 8, rsvd);\n\t\tmix_pool_bytes(r, tmp, bytes);\n\t\tcredit_entropy_bits(r, bytes*8);\n\t}\n}\n\n/*\n * These functions extracts randomness from the \"entropy pool\", and\n * returns it in a buffer.\n *\n * The min parameter specifies the minimum amount we can pull before\n * failing to avoid races that defeat catastrophic reseeding while the\n * reserved parameter indicates how much entropy we must leave in the\n * pool after each pull to avoid starving other readers.\n *\n * Note: extract_entropy() assumes that .poolwords is a multiple of 16 words.\n */\n\nstatic size_t account(struct entropy_store *r, size_t nbytes, int min,\n\t\t      int reserved)\n{\n\tunsigned long flags;\n\n\t/* Hold lock while accounting */\n\tspin_lock_irqsave(&r->lock, flags);\n\n\tBUG_ON(r->entropy_count > r->poolinfo->POOLBITS);\n\tDEBUG_ENT(\"trying to extract %d bits from %s\\n\",\n\t\t  nbytes * 8, r->name);\n\n\t/* Can we pull enough? */\n\tif (r->entropy_count / 8 < min + reserved) {\n\t\tnbytes = 0;\n\t} else {\n\t\t/* If limited, never pull more than available */\n\t\tif (r->limit && nbytes + reserved >= r->entropy_count / 8)\n\t\t\tnbytes = r->entropy_count/8 - reserved;\n\n\t\tif (r->entropy_count / 8 >= nbytes + reserved)\n\t\t\tr->entropy_count -= nbytes*8;\n\t\telse\n\t\t\tr->entropy_count = reserved;\n\n\t\tif (r->entropy_count < random_write_wakeup_thresh) {\n\t\t\twake_up_interruptible(&random_write_wait);\n\t\t\tkill_fasync(&fasync, SIGIO, POLL_OUT);\n\t\t}\n\t}\n\n\tDEBUG_ENT(\"debiting %d entropy credits from %s%s\\n\",\n\t\t  nbytes * 8, r->name, r->limit ? \"\" : \" (unlimited)\");\n\n\tspin_unlock_irqrestore(&r->lock, flags);\n\n\treturn nbytes;\n}\n\nstatic void extract_buf(struct entropy_store *r, __u8 *out)\n{\n\tint i;\n\t__u32 hash[5], workspace[SHA_WORKSPACE_WORDS];\n\t__u8 extract[64];\n\n\t/* Generate a hash across the pool, 16 words (512 bits) at a time */\n\tsha_init(hash);\n\tfor (i = 0; i < r->poolinfo->poolwords; i += 16)\n\t\tsha_transform(hash, (__u8 *)(r->pool + i), workspace);\n\n\t/*\n\t * We mix the hash back into the pool to prevent backtracking\n\t * attacks (where the attacker knows the state of the pool\n\t * plus the current outputs, and attempts to find previous\n\t * ouputs), unless the hash function can be inverted. By\n\t * mixing at least a SHA1 worth of hash data back, we make\n\t * brute-forcing the feedback as hard as brute-forcing the\n\t * hash.\n\t */\n\tmix_pool_bytes_extract(r, hash, sizeof(hash), extract);\n\n\t/*\n\t * To avoid duplicates, we atomically extract a portion of the\n\t * pool while mixing, and hash one final time.\n\t */\n\tsha_transform(hash, extract, workspace);\n\tmemset(extract, 0, sizeof(extract));\n\tmemset(workspace, 0, sizeof(workspace));\n\n\t/*\n\t * In case the hash function has some recognizable output\n\t * pattern, we fold it in half. Thus, we always feed back\n\t * twice as much data as we output.\n\t */\n\thash[0] ^= hash[3];\n\thash[1] ^= hash[4];\n\thash[2] ^= rol32(hash[2], 16);\n\tmemcpy(out, hash, EXTRACT_SIZE);\n\tmemset(hash, 0, sizeof(hash));\n}\n\nstatic ssize_t extract_entropy(struct entropy_store *r, void *buf,\n\t\t\t       size_t nbytes, int min, int reserved)\n{\n\tssize_t ret = 0, i;\n\t__u8 tmp[EXTRACT_SIZE];\n\tunsigned long flags;\n\n\txfer_secondary_pool(r, nbytes);\n\tnbytes = account(r, nbytes, min, reserved);\n\n\twhile (nbytes) {\n\t\textract_buf(r, tmp);\n\n\t\tif (fips_enabled) {\n\t\t\tspin_lock_irqsave(&r->lock, flags);\n\t\t\tif (!memcmp(tmp, r->last_data, EXTRACT_SIZE))\n\t\t\t\tpanic(\"Hardware RNG duplicated output!\\n\");\n\t\t\tmemcpy(r->last_data, tmp, EXTRACT_SIZE);\n\t\t\tspin_unlock_irqrestore(&r->lock, flags);\n\t\t}\n\t\ti = min_t(int, nbytes, EXTRACT_SIZE);\n\t\tmemcpy(buf, tmp, i);\n\t\tnbytes -= i;\n\t\tbuf += i;\n\t\tret += i;\n\t}\n\n\t/* Wipe data just returned from memory */\n\tmemset(tmp, 0, sizeof(tmp));\n\n\treturn ret;\n}\n\nstatic ssize_t extract_entropy_user(struct entropy_store *r, void __user *buf,\n\t\t\t\t    size_t nbytes)\n{\n\tssize_t ret = 0, i;\n\t__u8 tmp[EXTRACT_SIZE];\n\n\txfer_secondary_pool(r, nbytes);\n\tnbytes = account(r, nbytes, 0, 0);\n\n\twhile (nbytes) {\n\t\tif (need_resched()) {\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tif (ret == 0)\n\t\t\t\t\tret = -ERESTARTSYS;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tschedule();\n\t\t}\n\n\t\textract_buf(r, tmp);\n\t\ti = min_t(int, nbytes, EXTRACT_SIZE);\n\t\tif (copy_to_user(buf, tmp, i)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tnbytes -= i;\n\t\tbuf += i;\n\t\tret += i;\n\t}\n\n\t/* Wipe data just returned from memory */\n\tmemset(tmp, 0, sizeof(tmp));\n\n\treturn ret;\n}\n\n/*\n * This function is the exported kernel interface.  It returns some\n * number of good random numbers, suitable for seeding TCP sequence\n * numbers, etc.\n */\nvoid get_random_bytes(void *buf, int nbytes)\n{\n\textract_entropy(&nonblocking_pool, buf, nbytes, 0, 0);\n}\nEXPORT_SYMBOL(get_random_bytes);\n\n/*\n * init_std_data - initialize pool with system data\n *\n * @r: pool to initialize\n *\n * This function clears the pool's entropy count and mixes some system\n * data into the pool to prepare it for use. The pool is not cleared\n * as that can only decrease the entropy in the pool.\n */\nstatic void init_std_data(struct entropy_store *r)\n{\n\tktime_t now;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&r->lock, flags);\n\tr->entropy_count = 0;\n\tspin_unlock_irqrestore(&r->lock, flags);\n\n\tnow = ktime_get_real();\n\tmix_pool_bytes(r, &now, sizeof(now));\n\tmix_pool_bytes(r, utsname(), sizeof(*(utsname())));\n}\n\nstatic int rand_initialize(void)\n{\n\tinit_std_data(&input_pool);\n\tinit_std_data(&blocking_pool);\n\tinit_std_data(&nonblocking_pool);\n\treturn 0;\n}\nmodule_init(rand_initialize);\n\nvoid rand_initialize_irq(int irq)\n{\n\tstruct timer_rand_state *state;\n\n\tstate = get_timer_rand_state(irq);\n\n\tif (state)\n\t\treturn;\n\n\t/*\n\t * If kzalloc returns null, we just won't use that entropy\n\t * source.\n\t */\n\tstate = kzalloc(sizeof(struct timer_rand_state), GFP_KERNEL);\n\tif (state)\n\t\tset_timer_rand_state(irq, state);\n}\n\n#ifdef CONFIG_BLOCK\nvoid rand_initialize_disk(struct gendisk *disk)\n{\n\tstruct timer_rand_state *state;\n\n\t/*\n\t * If kzalloc returns null, we just won't use that entropy\n\t * source.\n\t */\n\tstate = kzalloc(sizeof(struct timer_rand_state), GFP_KERNEL);\n\tif (state)\n\t\tdisk->random = state;\n}\n#endif\n\nstatic ssize_t\nrandom_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)\n{\n\tssize_t n, retval = 0, count = 0;\n\n\tif (nbytes == 0)\n\t\treturn 0;\n\n\twhile (nbytes > 0) {\n\t\tn = nbytes;\n\t\tif (n > SEC_XFER_SIZE)\n\t\t\tn = SEC_XFER_SIZE;\n\n\t\tDEBUG_ENT(\"reading %d bits\\n\", n*8);\n\n\t\tn = extract_entropy_user(&blocking_pool, buf, n);\n\n\t\tDEBUG_ENT(\"read got %d bits (%d still needed)\\n\",\n\t\t\t  n*8, (nbytes-n)*8);\n\n\t\tif (n == 0) {\n\t\t\tif (file->f_flags & O_NONBLOCK) {\n\t\t\t\tretval = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tDEBUG_ENT(\"sleeping?\\n\");\n\n\t\t\twait_event_interruptible(random_read_wait,\n\t\t\t\tinput_pool.entropy_count >=\n\t\t\t\t\t\t random_read_wakeup_thresh);\n\n\t\t\tDEBUG_ENT(\"awake\\n\");\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tretval = -ERESTARTSYS;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (n < 0) {\n\t\t\tretval = n;\n\t\t\tbreak;\n\t\t}\n\t\tcount += n;\n\t\tbuf += n;\n\t\tnbytes -= n;\n\t\tbreak;\t\t/* This break makes the device work */\n\t\t\t\t/* like a named pipe */\n\t}\n\n\treturn (count ? count : retval);\n}\n\nstatic ssize_t\nurandom_read(struct file *file, char __user *buf, size_t nbytes, loff_t *ppos)\n{\n\treturn extract_entropy_user(&nonblocking_pool, buf, nbytes);\n}\n\nstatic unsigned int\nrandom_poll(struct file *file, poll_table * wait)\n{\n\tunsigned int mask;\n\n\tpoll_wait(file, &random_read_wait, wait);\n\tpoll_wait(file, &random_write_wait, wait);\n\tmask = 0;\n\tif (input_pool.entropy_count >= random_read_wakeup_thresh)\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (input_pool.entropy_count < random_write_wakeup_thresh)\n\t\tmask |= POLLOUT | POLLWRNORM;\n\treturn mask;\n}\n\nstatic int\nwrite_pool(struct entropy_store *r, const char __user *buffer, size_t count)\n{\n\tsize_t bytes;\n\t__u32 buf[16];\n\tconst char __user *p = buffer;\n\n\twhile (count > 0) {\n\t\tbytes = min(count, sizeof(buf));\n\t\tif (copy_from_user(&buf, p, bytes))\n\t\t\treturn -EFAULT;\n\n\t\tcount -= bytes;\n\t\tp += bytes;\n\n\t\tmix_pool_bytes(r, buf, bytes);\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t random_write(struct file *file, const char __user *buffer,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tsize_t ret;\n\n\tret = write_pool(&blocking_pool, buffer, count);\n\tif (ret)\n\t\treturn ret;\n\tret = write_pool(&nonblocking_pool, buffer, count);\n\tif (ret)\n\t\treturn ret;\n\n\treturn (ssize_t)count;\n}\n\nstatic long random_ioctl(struct file *f, unsigned int cmd, unsigned long arg)\n{\n\tint size, ent_count;\n\tint __user *p = (int __user *)arg;\n\tint retval;\n\n\tswitch (cmd) {\n\tcase RNDGETENTCNT:\n\t\t/* inherently racy, no point locking */\n\t\tif (put_user(input_pool.entropy_count, p))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase RNDADDTOENTCNT:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (get_user(ent_count, p))\n\t\t\treturn -EFAULT;\n\t\tcredit_entropy_bits(&input_pool, ent_count);\n\t\treturn 0;\n\tcase RNDADDENTROPY:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\tif (get_user(ent_count, p++))\n\t\t\treturn -EFAULT;\n\t\tif (ent_count < 0)\n\t\t\treturn -EINVAL;\n\t\tif (get_user(size, p++))\n\t\t\treturn -EFAULT;\n\t\tretval = write_pool(&input_pool, (const char __user *)p,\n\t\t\t\t    size);\n\t\tif (retval < 0)\n\t\t\treturn retval;\n\t\tcredit_entropy_bits(&input_pool, ent_count);\n\t\treturn 0;\n\tcase RNDZAPENTCNT:\n\tcase RNDCLEARPOOL:\n\t\t/* Clear the entropy pool counters. */\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\trand_initialize();\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int random_fasync(int fd, struct file *filp, int on)\n{\n\treturn fasync_helper(fd, filp, on, &fasync);\n}\n\nconst struct file_operations random_fops = {\n\t.read  = random_read,\n\t.write = random_write,\n\t.poll  = random_poll,\n\t.unlocked_ioctl = random_ioctl,\n\t.fasync = random_fasync,\n\t.llseek = noop_llseek,\n};\n\nconst struct file_operations urandom_fops = {\n\t.read  = urandom_read,\n\t.write = random_write,\n\t.unlocked_ioctl = random_ioctl,\n\t.fasync = random_fasync,\n\t.llseek = noop_llseek,\n};\n\n/***************************************************************\n * Random UUID interface\n *\n * Used here for a Boot ID, but can be useful for other kernel\n * drivers.\n ***************************************************************/\n\n/*\n * Generate random UUID\n */\nvoid generate_random_uuid(unsigned char uuid_out[16])\n{\n\tget_random_bytes(uuid_out, 16);\n\t/* Set UUID version to 4 --- truly random generation */\n\tuuid_out[6] = (uuid_out[6] & 0x0F) | 0x40;\n\t/* Set the UUID variant to DCE */\n\tuuid_out[8] = (uuid_out[8] & 0x3F) | 0x80;\n}\nEXPORT_SYMBOL(generate_random_uuid);\n\n/********************************************************************\n *\n * Sysctl interface\n *\n ********************************************************************/\n\n#ifdef CONFIG_SYSCTL\n\n#include <linux/sysctl.h>\n\nstatic int min_read_thresh = 8, min_write_thresh;\nstatic int max_read_thresh = INPUT_POOL_WORDS * 32;\nstatic int max_write_thresh = INPUT_POOL_WORDS * 32;\nstatic char sysctl_bootid[16];\n\n/*\n * These functions is used to return both the bootid UUID, and random\n * UUID.  The difference is in whether table->data is NULL; if it is,\n * then a new UUID is generated and returned to the user.\n *\n * If the user accesses this via the proc interface, it will be returned\n * as an ASCII string in the standard UUID format.  If accesses via the\n * sysctl system call, it is returned as 16 bytes of binary data.\n */\nstatic int proc_do_uuid(ctl_table *table, int write,\n\t\t\tvoid __user *buffer, size_t *lenp, loff_t *ppos)\n{\n\tctl_table fake_table;\n\tunsigned char buf[64], tmp_uuid[16], *uuid;\n\n\tuuid = table->data;\n\tif (!uuid) {\n\t\tuuid = tmp_uuid;\n\t\tuuid[8] = 0;\n\t}\n\tif (uuid[8] == 0)\n\t\tgenerate_random_uuid(uuid);\n\n\tsprintf(buf, \"%pU\", uuid);\n\n\tfake_table.data = buf;\n\tfake_table.maxlen = sizeof(buf);\n\n\treturn proc_dostring(&fake_table, write, buffer, lenp, ppos);\n}\n\nstatic int sysctl_poolsize = INPUT_POOL_WORDS * 32;\nctl_table random_table[] = {\n\t{\n\t\t.procname\t= \"poolsize\",\n\t\t.data\t\t= &sysctl_poolsize,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"entropy_avail\",\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_dointvec,\n\t\t.data\t\t= &input_pool.entropy_count,\n\t},\n\t{\n\t\t.procname\t= \"read_wakeup_threshold\",\n\t\t.data\t\t= &random_read_wakeup_thresh,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_read_thresh,\n\t\t.extra2\t\t= &max_read_thresh,\n\t},\n\t{\n\t\t.procname\t= \"write_wakeup_threshold\",\n\t\t.data\t\t= &random_write_wakeup_thresh,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_write_thresh,\n\t\t.extra2\t\t= &max_write_thresh,\n\t},\n\t{\n\t\t.procname\t= \"boot_id\",\n\t\t.data\t\t= &sysctl_bootid,\n\t\t.maxlen\t\t= 16,\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_do_uuid,\n\t},\n\t{\n\t\t.procname\t= \"uuid\",\n\t\t.maxlen\t\t= 16,\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_do_uuid,\n\t},\n\t{ }\n};\n#endif \t/* CONFIG_SYSCTL */\n\n/********************************************************************\n *\n * Random functions for networking\n *\n ********************************************************************/\n\n/*\n * TCP initial sequence number picking.  This uses the random number\n * generator to pick an initial secret value.  This value is hashed\n * along with the TCP endpoint information to provide a unique\n * starting point for each pair of TCP endpoints.  This defeats\n * attacks which rely on guessing the initial TCP sequence number.\n * This algorithm was suggested by Steve Bellovin.\n *\n * Using a very strong hash was taking an appreciable amount of the total\n * TCP connection establishment time, so this is a weaker hash,\n * compensated for by changing the secret periodically.\n */\n\n/* F, G and H are basic MD4 functions: selection, majority, parity */\n#define F(x, y, z) ((z) ^ ((x) & ((y) ^ (z))))\n#define G(x, y, z) (((x) & (y)) + (((x) ^ (y)) & (z)))\n#define H(x, y, z) ((x) ^ (y) ^ (z))\n\n/*\n * The generic round function.  The application is so specific that\n * we don't bother protecting all the arguments with parens, as is generally\n * good macro practice, in favor of extra legibility.\n * Rotation is separate from addition to prevent recomputation\n */\n#define ROUND(f, a, b, c, d, x, s)\t\\\n\t(a += f(b, c, d) + x, a = (a << s) | (a >> (32 - s)))\n#define K1 0\n#define K2 013240474631UL\n#define K3 015666365641UL\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n\nstatic __u32 twothirdsMD4Transform(__u32 const buf[4], __u32 const in[12])\n{\n\t__u32 a = buf[0], b = buf[1], c = buf[2], d = buf[3];\n\n\t/* Round 1 */\n\tROUND(F, a, b, c, d, in[ 0] + K1,  3);\n\tROUND(F, d, a, b, c, in[ 1] + K1,  7);\n\tROUND(F, c, d, a, b, in[ 2] + K1, 11);\n\tROUND(F, b, c, d, a, in[ 3] + K1, 19);\n\tROUND(F, a, b, c, d, in[ 4] + K1,  3);\n\tROUND(F, d, a, b, c, in[ 5] + K1,  7);\n\tROUND(F, c, d, a, b, in[ 6] + K1, 11);\n\tROUND(F, b, c, d, a, in[ 7] + K1, 19);\n\tROUND(F, a, b, c, d, in[ 8] + K1,  3);\n\tROUND(F, d, a, b, c, in[ 9] + K1,  7);\n\tROUND(F, c, d, a, b, in[10] + K1, 11);\n\tROUND(F, b, c, d, a, in[11] + K1, 19);\n\n\t/* Round 2 */\n\tROUND(G, a, b, c, d, in[ 1] + K2,  3);\n\tROUND(G, d, a, b, c, in[ 3] + K2,  5);\n\tROUND(G, c, d, a, b, in[ 5] + K2,  9);\n\tROUND(G, b, c, d, a, in[ 7] + K2, 13);\n\tROUND(G, a, b, c, d, in[ 9] + K2,  3);\n\tROUND(G, d, a, b, c, in[11] + K2,  5);\n\tROUND(G, c, d, a, b, in[ 0] + K2,  9);\n\tROUND(G, b, c, d, a, in[ 2] + K2, 13);\n\tROUND(G, a, b, c, d, in[ 4] + K2,  3);\n\tROUND(G, d, a, b, c, in[ 6] + K2,  5);\n\tROUND(G, c, d, a, b, in[ 8] + K2,  9);\n\tROUND(G, b, c, d, a, in[10] + K2, 13);\n\n\t/* Round 3 */\n\tROUND(H, a, b, c, d, in[ 3] + K3,  3);\n\tROUND(H, d, a, b, c, in[ 7] + K3,  9);\n\tROUND(H, c, d, a, b, in[11] + K3, 11);\n\tROUND(H, b, c, d, a, in[ 2] + K3, 15);\n\tROUND(H, a, b, c, d, in[ 6] + K3,  3);\n\tROUND(H, d, a, b, c, in[10] + K3,  9);\n\tROUND(H, c, d, a, b, in[ 1] + K3, 11);\n\tROUND(H, b, c, d, a, in[ 5] + K3, 15);\n\tROUND(H, a, b, c, d, in[ 9] + K3,  3);\n\tROUND(H, d, a, b, c, in[ 0] + K3,  9);\n\tROUND(H, c, d, a, b, in[ 4] + K3, 11);\n\tROUND(H, b, c, d, a, in[ 8] + K3, 15);\n\n\treturn buf[1] + b; /* \"most hashed\" word */\n\t/* Alternative: return sum of all words? */\n}\n#endif\n\n#undef ROUND\n#undef F\n#undef G\n#undef H\n#undef K1\n#undef K2\n#undef K3\n\n/* This should not be decreased so low that ISNs wrap too fast. */\n#define REKEY_INTERVAL (300 * HZ)\n/*\n * Bit layout of the tcp sequence numbers (before adding current time):\n * bit 24-31: increased after every key exchange\n * bit 0-23: hash(source,dest)\n *\n * The implementation is similar to the algorithm described\n * in the Appendix of RFC 1185, except that\n * - it uses a 1 MHz clock instead of a 250 kHz clock\n * - it performs a rekey every 5 minutes, which is equivalent\n * \tto a (source,dest) tulple dependent forward jump of the\n * \tclock by 0..2^(HASH_BITS+1)\n *\n * Thus the average ISN wraparound time is 68 minutes instead of\n * 4.55 hours.\n *\n * SMP cleanup and lock avoidance with poor man's RCU.\n * \t\t\tManfred Spraul <manfred@colorfullife.com>\n *\n */\n#define COUNT_BITS 8\n#define COUNT_MASK ((1 << COUNT_BITS) - 1)\n#define HASH_BITS 24\n#define HASH_MASK ((1 << HASH_BITS) - 1)\n\nstatic struct keydata {\n\t__u32 count; /* already shifted to the final position */\n\t__u32 secret[12];\n} ____cacheline_aligned ip_keydata[2];\n\nstatic unsigned int ip_cnt;\n\nstatic void rekey_seq_generator(struct work_struct *work);\n\nstatic DECLARE_DELAYED_WORK(rekey_work, rekey_seq_generator);\n\n/*\n * Lock avoidance:\n * The ISN generation runs lockless - it's just a hash over random data.\n * State changes happen every 5 minutes when the random key is replaced.\n * Synchronization is performed by having two copies of the hash function\n * state and rekey_seq_generator always updates the inactive copy.\n * The copy is then activated by updating ip_cnt.\n * The implementation breaks down if someone blocks the thread\n * that processes SYN requests for more than 5 minutes. Should never\n * happen, and even if that happens only a not perfectly compliant\n * ISN is generated, nothing fatal.\n */\nstatic void rekey_seq_generator(struct work_struct *work)\n{\n\tstruct keydata *keyptr = &ip_keydata[1 ^ (ip_cnt & 1)];\n\n\tget_random_bytes(keyptr->secret, sizeof(keyptr->secret));\n\tkeyptr->count = (ip_cnt & COUNT_MASK) << HASH_BITS;\n\tsmp_wmb();\n\tip_cnt++;\n\tschedule_delayed_work(&rekey_work,\n\t\t\t      round_jiffies_relative(REKEY_INTERVAL));\n}\n\nstatic inline struct keydata *get_keyptr(void)\n{\n\tstruct keydata *keyptr = &ip_keydata[ip_cnt & 1];\n\n\tsmp_rmb();\n\n\treturn keyptr;\n}\n\nstatic __init int seqgen_init(void)\n{\n\trekey_seq_generator(NULL);\n\treturn 0;\n}\nlate_initcall(seqgen_init);\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\n__u32 secure_tcpv6_sequence_number(__be32 *saddr, __be32 *daddr,\n\t\t\t\t   __be16 sport, __be16 dport)\n{\n\t__u32 seq;\n\t__u32 hash[12];\n\tstruct keydata *keyptr = get_keyptr();\n\n\t/* The procedure is the same as for IPv4, but addresses are longer.\n\t * Thus we must use twothirdsMD4Transform.\n\t */\n\n\tmemcpy(hash, saddr, 16);\n\thash[4] = ((__force u16)sport << 16) + (__force u16)dport;\n\tmemcpy(&hash[5], keyptr->secret, sizeof(__u32) * 7);\n\n\tseq = twothirdsMD4Transform((const __u32 *)daddr, hash) & HASH_MASK;\n\tseq += keyptr->count;\n\n\tseq += ktime_to_ns(ktime_get_real());\n\n\treturn seq;\n}\nEXPORT_SYMBOL(secure_tcpv6_sequence_number);\n#endif\n\n/*  The code below is shamelessly stolen from secure_tcp_sequence_number().\n *  All blames to Andrey V. Savochkin <saw@msu.ru>.\n */\n__u32 secure_ip_id(__be32 daddr)\n{\n\tstruct keydata *keyptr;\n\t__u32 hash[4];\n\n\tkeyptr = get_keyptr();\n\n\t/*\n\t *  Pick a unique starting offset for each IP destination.\n\t *  The dest ip address is placed in the starting vector,\n\t *  which is then hashed with random data.\n\t */\n\thash[0] = (__force __u32)daddr;\n\thash[1] = keyptr->secret[9];\n\thash[2] = keyptr->secret[10];\n\thash[3] = keyptr->secret[11];\n\n\treturn half_md4_transform(hash, keyptr->secret);\n}\n\n__u32 secure_ipv6_id(const __be32 daddr[4])\n{\n\tconst struct keydata *keyptr;\n\t__u32 hash[4];\n\n\tkeyptr = get_keyptr();\n\n\thash[0] = (__force __u32)daddr[0];\n\thash[1] = (__force __u32)daddr[1];\n\thash[2] = (__force __u32)daddr[2];\n\thash[3] = (__force __u32)daddr[3];\n\n\treturn half_md4_transform(hash, keyptr->secret);\n}\n\n#ifdef CONFIG_INET\n\n__u32 secure_tcp_sequence_number(__be32 saddr, __be32 daddr,\n\t\t\t\t __be16 sport, __be16 dport)\n{\n\t__u32 seq;\n\t__u32 hash[4];\n\tstruct keydata *keyptr = get_keyptr();\n\n\t/*\n\t *  Pick a unique starting offset for each TCP connection endpoints\n\t *  (saddr, daddr, sport, dport).\n\t *  Note that the words are placed into the starting vector, which is\n\t *  then mixed with a partial MD4 over random data.\n\t */\n\thash[0] = (__force u32)saddr;\n\thash[1] = (__force u32)daddr;\n\thash[2] = ((__force u16)sport << 16) + (__force u16)dport;\n\thash[3] = keyptr->secret[11];\n\n\tseq = half_md4_transform(hash, keyptr->secret) & HASH_MASK;\n\tseq += keyptr->count;\n\t/*\n\t *\tAs close as possible to RFC 793, which\n\t *\tsuggests using a 250 kHz clock.\n\t *\tFurther reading shows this assumes 2 Mb/s networks.\n\t *\tFor 10 Mb/s Ethernet, a 1 MHz clock is appropriate.\n\t *\tFor 10 Gb/s Ethernet, a 1 GHz clock should be ok, but\n\t *\twe also need to limit the resolution so that the u32 seq\n\t *\toverlaps less than one time per MSL (2 minutes).\n\t *\tChoosing a clock of 64 ns period is OK. (period of 274 s)\n\t */\n\tseq += ktime_to_ns(ktime_get_real()) >> 6;\n\n\treturn seq;\n}\n\n/* Generate secure starting point for ephemeral IPV4 transport port search */\nu32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)\n{\n\tstruct keydata *keyptr = get_keyptr();\n\tu32 hash[4];\n\n\t/*\n\t *  Pick a unique starting offset for each ephemeral port search\n\t *  (saddr, daddr, dport) and 48bits of random data.\n\t */\n\thash[0] = (__force u32)saddr;\n\thash[1] = (__force u32)daddr;\n\thash[2] = (__force u32)dport ^ keyptr->secret[10];\n\thash[3] = keyptr->secret[11];\n\n\treturn half_md4_transform(hash, keyptr->secret);\n}\nEXPORT_SYMBOL_GPL(secure_ipv4_port_ephemeral);\n\n#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)\nu32 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,\n\t\t\t       __be16 dport)\n{\n\tstruct keydata *keyptr = get_keyptr();\n\tu32 hash[12];\n\n\tmemcpy(hash, saddr, 16);\n\thash[4] = (__force u32)dport;\n\tmemcpy(&hash[5], keyptr->secret, sizeof(__u32) * 7);\n\n\treturn twothirdsMD4Transform((const __u32 *)daddr, hash);\n}\n#endif\n\n#if defined(CONFIG_IP_DCCP) || defined(CONFIG_IP_DCCP_MODULE)\n/* Similar to secure_tcp_sequence_number but generate a 48 bit value\n * bit's 32-47 increase every key exchange\n *       0-31  hash(source, dest)\n */\nu64 secure_dccp_sequence_number(__be32 saddr, __be32 daddr,\n\t\t\t\t__be16 sport, __be16 dport)\n{\n\tu64 seq;\n\t__u32 hash[4];\n\tstruct keydata *keyptr = get_keyptr();\n\n\thash[0] = (__force u32)saddr;\n\thash[1] = (__force u32)daddr;\n\thash[2] = ((__force u16)sport << 16) + (__force u16)dport;\n\thash[3] = keyptr->secret[11];\n\n\tseq = half_md4_transform(hash, keyptr->secret);\n\tseq |= ((u64)keyptr->count) << (32 - HASH_BITS);\n\n\tseq += ktime_to_ns(ktime_get_real());\n\tseq &= (1ull << 48) - 1;\n\n\treturn seq;\n}\nEXPORT_SYMBOL(secure_dccp_sequence_number);\n#endif\n\n#endif /* CONFIG_INET */\n\n\n/*\n * Get a random word for internal kernel use only. Similar to urandom but\n * with the goal of minimal entropy pool depletion. As a result, the random\n * value is not cryptographically secure but for several uses the cost of\n * depleting entropy is too high\n */\nDEFINE_PER_CPU(__u32 [4], get_random_int_hash);\nunsigned int get_random_int(void)\n{\n\tstruct keydata *keyptr;\n\t__u32 *hash = get_cpu_var(get_random_int_hash);\n\tint ret;\n\n\tkeyptr = get_keyptr();\n\thash[0] += current->pid + jiffies + get_cycles();\n\n\tret = half_md4_transform(hash, keyptr->secret);\n\tput_cpu_var(get_random_int_hash);\n\n\treturn ret;\n}\n\n/*\n * randomize_range() returns a start address such that\n *\n *    [...... <range> .....]\n *  start                  end\n *\n * a <range> with size \"len\" starting at the return value is inside in the\n * area defined by [start, end], but is otherwise randomized.\n */\nunsigned long\nrandomize_range(unsigned long start, unsigned long end, unsigned long len)\n{\n\tunsigned long range = end - len - start;\n\n\tif (end <= start + len)\n\t\treturn 0;\n\treturn PAGE_ALIGN(get_random_int() % range + start);\n}\n", "/*\n * include/linux/random.h\n *\n * Include file for the random number generator.\n */\n\n#ifndef _LINUX_RANDOM_H\n#define _LINUX_RANDOM_H\n\n#include <linux/types.h>\n#include <linux/ioctl.h>\n#include <linux/irqnr.h>\n\n/* ioctl()'s for the random number generator */\n\n/* Get the entropy count. */\n#define RNDGETENTCNT\t_IOR( 'R', 0x00, int )\n\n/* Add to (or subtract from) the entropy count.  (Superuser only.) */\n#define RNDADDTOENTCNT\t_IOW( 'R', 0x01, int )\n\n/* Get the contents of the entropy pool.  (Superuser only.) */\n#define RNDGETPOOL\t_IOR( 'R', 0x02, int [2] )\n\n/* \n * Write bytes into the entropy pool and add to the entropy count.\n * (Superuser only.)\n */\n#define RNDADDENTROPY\t_IOW( 'R', 0x03, int [2] )\n\n/* Clear entropy count to 0.  (Superuser only.) */\n#define RNDZAPENTCNT\t_IO( 'R', 0x04 )\n\n/* Clear the entropy pool and associated counters.  (Superuser only.) */\n#define RNDCLEARPOOL\t_IO( 'R', 0x06 )\n\nstruct rand_pool_info {\n\tint\tentropy_count;\n\tint\tbuf_size;\n\t__u32\tbuf[0];\n};\n\nstruct rnd_state {\n\t__u32 s1, s2, s3;\n};\n\n/* Exported functions */\n\n#ifdef __KERNEL__\n\nextern void rand_initialize_irq(int irq);\n\nextern void add_input_randomness(unsigned int type, unsigned int code,\n\t\t\t\t unsigned int value);\nextern void add_interrupt_randomness(int irq);\n\nextern void get_random_bytes(void *buf, int nbytes);\nvoid generate_random_uuid(unsigned char uuid_out[16]);\n\nextern __u32 secure_ip_id(__be32 daddr);\nextern __u32 secure_ipv6_id(const __be32 daddr[4]);\nextern u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport);\nextern u32 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,\n\t\t\t\t      __be16 dport);\nextern __u32 secure_tcp_sequence_number(__be32 saddr, __be32 daddr,\n\t\t\t\t\t__be16 sport, __be16 dport);\nextern __u32 secure_tcpv6_sequence_number(__be32 *saddr, __be32 *daddr,\n\t\t\t\t\t  __be16 sport, __be16 dport);\nextern u64 secure_dccp_sequence_number(__be32 saddr, __be32 daddr,\n\t\t\t\t       __be16 sport, __be16 dport);\n\n#ifndef MODULE\nextern const struct file_operations random_fops, urandom_fops;\n#endif\n\nunsigned int get_random_int(void);\nunsigned long randomize_range(unsigned long start, unsigned long end, unsigned long len);\n\nu32 random32(void);\nvoid srandom32(u32 seed);\n\nu32 prandom32(struct rnd_state *);\n\n/*\n * Handle minimum values for seeds\n */\nstatic inline u32 __seed(u32 x, u32 m)\n{\n\treturn (x < m) ? x + m : x;\n}\n\n/**\n * prandom32_seed - set seed for prandom32().\n * @state: pointer to state structure to receive the seed.\n * @seed: arbitrary 64-bit value to use as a seed.\n */\nstatic inline void prandom32_seed(struct rnd_state *state, u64 seed)\n{\n\tu32 i = (seed >> 32) ^ (seed << 10) ^ seed;\n\n\tstate->s1 = __seed(i, 1);\n\tstate->s2 = __seed(i, 7);\n\tstate->s3 = __seed(i, 15);\n}\n\n#endif /* __KERNEL___ */\n\n#endif /* _LINUX_RANDOM_H */\n", "/*\n *\t\tINETPEER - A storage for permanent information about peers\n *\n *  Authors:\tAndrey V. Savochkin <saw@msu.ru>\n */\n\n#ifndef _NET_INETPEER_H\n#define _NET_INETPEER_H\n\n#include <linux/types.h>\n#include <linux/init.h>\n#include <linux/jiffies.h>\n#include <linux/spinlock.h>\n#include <linux/rtnetlink.h>\n#include <net/ipv6.h>\n#include <asm/atomic.h>\n\nstruct inetpeer_addr_base {\n\tunion {\n\t\t__be32\t\t\ta4;\n\t\t__be32\t\t\ta6[4];\n\t};\n};\n\nstruct inetpeer_addr {\n\tstruct inetpeer_addr_base\taddr;\n\t__u16\t\t\t\tfamily;\n};\n\nstruct inet_peer {\n\t/* group together avl_left,avl_right,v4daddr to speedup lookups */\n\tstruct inet_peer __rcu\t*avl_left, *avl_right;\n\tstruct inetpeer_addr\tdaddr;\n\t__u32\t\t\tavl_height;\n\n\tu32\t\t\tmetrics[RTAX_MAX];\n\tu32\t\t\trate_tokens;\t/* rate limiting for ICMP */\n\tunsigned long\t\trate_last;\n\tunsigned long\t\tpmtu_expires;\n\tu32\t\t\tpmtu_orig;\n\tu32\t\t\tpmtu_learned;\n\tstruct inetpeer_addr_base redirect_learned;\n\t/*\n\t * Once inet_peer is queued for deletion (refcnt == -1), following fields\n\t * are not available: rid, ip_id_count, tcp_ts, tcp_ts_stamp\n\t * We can share memory with rcu_head to help keep inet_peer small.\n\t */\n\tunion {\n\t\tstruct {\n\t\t\tatomic_t\t\t\trid;\t\t/* Frag reception counter */\n\t\t\tatomic_t\t\t\tip_id_count;\t/* IP ID for the next packet */\n\t\t\t__u32\t\t\t\ttcp_ts;\n\t\t\t__u32\t\t\t\ttcp_ts_stamp;\n\t\t};\n\t\tstruct rcu_head         rcu;\n\t\tstruct inet_peer\t*gc_next;\n\t};\n\n\t/* following fields might be frequently dirtied */\n\t__u32\t\t\tdtime;\t/* the time of last use of not referenced entries */\n\tatomic_t\t\trefcnt;\n};\n\nvoid\t\t\tinet_initpeers(void) __init;\n\n#define INETPEER_METRICS_NEW\t(~(u32) 0)\n\nstatic inline bool inet_metrics_new(const struct inet_peer *p)\n{\n\treturn p->metrics[RTAX_LOCK-1] == INETPEER_METRICS_NEW;\n}\n\n/* can be called with or without local BH being disabled */\nstruct inet_peer\t*inet_getpeer(const struct inetpeer_addr *daddr, int create);\n\nstatic inline struct inet_peer *inet_getpeer_v4(__be32 v4daddr, int create)\n{\n\tstruct inetpeer_addr daddr;\n\n\tdaddr.addr.a4 = v4daddr;\n\tdaddr.family = AF_INET;\n\treturn inet_getpeer(&daddr, create);\n}\n\nstatic inline struct inet_peer *inet_getpeer_v6(const struct in6_addr *v6daddr, int create)\n{\n\tstruct inetpeer_addr daddr;\n\n\tipv6_addr_copy((struct in6_addr *)daddr.addr.a6, v6daddr);\n\tdaddr.family = AF_INET6;\n\treturn inet_getpeer(&daddr, create);\n}\n\n/* can be called from BH context or outside */\nextern void inet_putpeer(struct inet_peer *p);\nextern bool inet_peer_xrlim_allow(struct inet_peer *peer, int timeout);\n\n/*\n * temporary check to make sure we dont access rid, ip_id_count, tcp_ts,\n * tcp_ts_stamp if no refcount is taken on inet_peer\n */\nstatic inline void inet_peer_refcheck(const struct inet_peer *p)\n{\n\tWARN_ON_ONCE(atomic_read(&p->refcnt) <= 0);\n}\n\n\n/* can be called with or without local BH being disabled */\nstatic inline int inet_getid(struct inet_peer *p, int more)\n{\n\tint old, new;\n\tmore++;\n\tinet_peer_refcheck(p);\n\tdo {\n\t\told = atomic_read(&p->ip_id_count);\n\t\tnew = old + more;\n\t\tif (!new)\n\t\t\tnew = 1;\n\t} while (atomic_cmpxchg(&p->ip_id_count, old, new) != old);\n\treturn new;\n}\n\n#endif /* _NET_INETPEER_H */\n", "/*\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#ifndef _NET_IPV6_H\n#define _NET_IPV6_H\n\n#include <linux/ipv6.h>\n#include <linux/hardirq.h>\n#include <net/if_inet6.h>\n#include <net/ndisc.h>\n#include <net/flow.h>\n#include <net/snmp.h>\n\n#define SIN6_LEN_RFC2133\t24\n\n#define IPV6_MAXPLEN\t\t65535\n\n/*\n *\tNextHeader field of IPv6 header\n */\n\n#define NEXTHDR_HOP\t\t0\t/* Hop-by-hop option header. */\n#define NEXTHDR_TCP\t\t6\t/* TCP segment. */\n#define NEXTHDR_UDP\t\t17\t/* UDP message. */\n#define NEXTHDR_IPV6\t\t41\t/* IPv6 in IPv6 */\n#define NEXTHDR_ROUTING\t\t43\t/* Routing header. */\n#define NEXTHDR_FRAGMENT\t44\t/* Fragmentation/reassembly header. */\n#define NEXTHDR_ESP\t\t50\t/* Encapsulating security payload. */\n#define NEXTHDR_AUTH\t\t51\t/* Authentication header. */\n#define NEXTHDR_ICMP\t\t58\t/* ICMP for IPv6. */\n#define NEXTHDR_NONE\t\t59\t/* No next header */\n#define NEXTHDR_DEST\t\t60\t/* Destination options header. */\n#define NEXTHDR_MOBILITY\t135\t/* Mobility header. */\n\n#define NEXTHDR_MAX\t\t255\n\n\n\n#define IPV6_DEFAULT_HOPLIMIT   64\n#define IPV6_DEFAULT_MCASTHOPS\t1\n\n/*\n *\tAddr type\n *\t\n *\ttype\t-\tunicast | multicast\n *\tscope\t-\tlocal\t| site\t    | global\n *\tv4\t-\tcompat\n *\tv4mapped\n *\tany\n *\tloopback\n */\n\n#define IPV6_ADDR_ANY\t\t0x0000U\n\n#define IPV6_ADDR_UNICAST      \t0x0001U\t\n#define IPV6_ADDR_MULTICAST    \t0x0002U\t\n\n#define IPV6_ADDR_LOOPBACK\t0x0010U\n#define IPV6_ADDR_LINKLOCAL\t0x0020U\n#define IPV6_ADDR_SITELOCAL\t0x0040U\n\n#define IPV6_ADDR_COMPATv4\t0x0080U\n\n#define IPV6_ADDR_SCOPE_MASK\t0x00f0U\n\n#define IPV6_ADDR_MAPPED\t0x1000U\n\n/*\n *\tAddr scopes\n */\n#define IPV6_ADDR_MC_SCOPE(a)\t\\\n\t((a)->s6_addr[1] & 0x0f)\t/* nonstandard */\n#define __IPV6_ADDR_SCOPE_INVALID\t-1\n#define IPV6_ADDR_SCOPE_NODELOCAL\t0x01\n#define IPV6_ADDR_SCOPE_LINKLOCAL\t0x02\n#define IPV6_ADDR_SCOPE_SITELOCAL\t0x05\n#define IPV6_ADDR_SCOPE_ORGLOCAL\t0x08\n#define IPV6_ADDR_SCOPE_GLOBAL\t\t0x0e\n\n/*\n *\tAddr flags\n */\n#define IPV6_ADDR_MC_FLAG_TRANSIENT(a)\t\\\n\t((a)->s6_addr[1] & 0x10)\n#define IPV6_ADDR_MC_FLAG_PREFIX(a)\t\\\n\t((a)->s6_addr[1] & 0x20)\n#define IPV6_ADDR_MC_FLAG_RENDEZVOUS(a)\t\\\n\t((a)->s6_addr[1] & 0x40)\n\n/*\n *\tfragmentation header\n */\n\nstruct frag_hdr {\n\t__u8\tnexthdr;\n\t__u8\treserved;\n\t__be16\tfrag_off;\n\t__be32\tidentification;\n};\n\n#define\tIP6_MF\t0x0001\n\n#include <net/sock.h>\n\n/* sysctls */\nextern int sysctl_mld_max_msf;\nextern struct ctl_path net_ipv6_ctl_path[];\n\n#define _DEVINC(net, statname, modifier, idev, field)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_INC_STATS##modifier((_idev)->stats.statname, (field)); \\\n\tSNMP_INC_STATS##modifier((net)->mib.statname##_statistics, (field));\\\n})\n\n/* per device counters are atomic_long_t */\n#define _DEVINCATOMIC(net, statname, modifier, idev, field)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_INC_STATS_ATOMIC_LONG((_idev)->stats.statname##dev, (field)); \\\n\tSNMP_INC_STATS##modifier((net)->mib.statname##_statistics, (field));\\\n})\n\n#define _DEVADD(net, statname, modifier, idev, field, val)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_ADD_STATS##modifier((_idev)->stats.statname, (field), (val)); \\\n\tSNMP_ADD_STATS##modifier((net)->mib.statname##_statistics, (field), (val));\\\n})\n\n#define _DEVUPD(net, statname, modifier, idev, field, val)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct inet6_dev *_idev = (idev);\t\t\t\t\\\n\tif (likely(_idev != NULL))\t\t\t\t\t\\\n\t\tSNMP_UPD_PO_STATS##modifier((_idev)->stats.statname, field, (val)); \\\n\tSNMP_UPD_PO_STATS##modifier((net)->mib.statname##_statistics, field, (val));\\\n})\n\n/* MIBs */\n\n#define IP6_INC_STATS(net, idev,field)\t\t\\\n\t\t_DEVINC(net, ipv6, 64, idev, field)\n#define IP6_INC_STATS_BH(net, idev,field)\t\\\n\t\t_DEVINC(net, ipv6, 64_BH, idev, field)\n#define IP6_ADD_STATS(net, idev,field,val)\t\\\n\t\t_DEVADD(net, ipv6, 64, idev, field, val)\n#define IP6_ADD_STATS_BH(net, idev,field,val)\t\\\n\t\t_DEVADD(net, ipv6, 64_BH, idev, field, val)\n#define IP6_UPD_PO_STATS(net, idev,field,val)   \\\n\t\t_DEVUPD(net, ipv6, 64, idev, field, val)\n#define IP6_UPD_PO_STATS_BH(net, idev,field,val)   \\\n\t\t_DEVUPD(net, ipv6, 64_BH, idev, field, val)\n#define ICMP6_INC_STATS(net, idev, field)\t\\\n\t\t_DEVINCATOMIC(net, icmpv6, , idev, field)\n#define ICMP6_INC_STATS_BH(net, idev, field)\t\\\n\t\t_DEVINCATOMIC(net, icmpv6, _BH, idev, field)\n\n#define ICMP6MSGOUT_INC_STATS(net, idev, field)\t\t\\\n\t_DEVINCATOMIC(net, icmpv6msg, , idev, field +256)\n#define ICMP6MSGOUT_INC_STATS_BH(net, idev, field)\t\\\n\t_DEVINCATOMIC(net, icmpv6msg, _BH, idev, field +256)\n#define ICMP6MSGIN_INC_STATS_BH(net, idev, field)\t\\\n\t_DEVINCATOMIC(net, icmpv6msg, _BH, idev, field)\n\nstruct ip6_ra_chain {\n\tstruct ip6_ra_chain\t*next;\n\tstruct sock\t\t*sk;\n\tint\t\t\tsel;\n\tvoid\t\t\t(*destructor)(struct sock *);\n};\n\nextern struct ip6_ra_chain\t*ip6_ra_chain;\nextern rwlock_t ip6_ra_lock;\n\n/*\n   This structure is prepared by protocol, when parsing\n   ancillary data and passed to IPv6.\n */\n\nstruct ipv6_txoptions {\n\t/* Length of this structure */\n\tint\t\t\ttot_len;\n\n\t/* length of extension headers   */\n\n\t__u16\t\t\topt_flen;\t/* after fragment hdr */\n\t__u16\t\t\topt_nflen;\t/* before fragment hdr */\n\n\tstruct ipv6_opt_hdr\t*hopopt;\n\tstruct ipv6_opt_hdr\t*dst0opt;\n\tstruct ipv6_rt_hdr\t*srcrt;\t/* Routing Header */\n\tstruct ipv6_opt_hdr\t*dst1opt;\n\n\t/* Option buffer, as read by IPV6_PKTOPTIONS, starts here. */\n};\n\nstruct ip6_flowlabel {\n\tstruct ip6_flowlabel\t*next;\n\t__be32\t\t\tlabel;\n\tatomic_t\t\tusers;\n\tstruct in6_addr\t\tdst;\n\tstruct ipv6_txoptions\t*opt;\n\tunsigned long\t\tlinger;\n\tu8\t\t\tshare;\n\tu32\t\t\towner;\n\tunsigned long\t\tlastuse;\n\tunsigned long\t\texpires;\n\tstruct net\t\t*fl_net;\n};\n\n#define IPV6_FLOWINFO_MASK\tcpu_to_be32(0x0FFFFFFF)\n#define IPV6_FLOWLABEL_MASK\tcpu_to_be32(0x000FFFFF)\n\nstruct ipv6_fl_socklist {\n\tstruct ipv6_fl_socklist\t*next;\n\tstruct ip6_flowlabel\t*fl;\n};\n\nextern struct ip6_flowlabel\t*fl6_sock_lookup(struct sock *sk, __be32 label);\nextern struct ipv6_txoptions\t*fl6_merge_options(struct ipv6_txoptions * opt_space,\n\t\t\t\t\t\t   struct ip6_flowlabel * fl,\n\t\t\t\t\t\t   struct ipv6_txoptions * fopt);\nextern void\t\t\tfl6_free_socklist(struct sock *sk);\nextern int\t\t\tipv6_flowlabel_opt(struct sock *sk, char __user *optval, int optlen);\nextern int\t\t\tip6_flowlabel_init(void);\nextern void\t\t\tip6_flowlabel_cleanup(void);\n\nstatic inline void fl6_sock_release(struct ip6_flowlabel *fl)\n{\n\tif (fl)\n\t\tatomic_dec(&fl->users);\n}\n\nextern int \t\t\tip6_ra_control(struct sock *sk, int sel);\n\nextern int\t\t\tipv6_parse_hopopts(struct sk_buff *skb);\n\nextern struct ipv6_txoptions *  ipv6_dup_options(struct sock *sk, struct ipv6_txoptions *opt);\nextern struct ipv6_txoptions *\tipv6_renew_options(struct sock *sk, struct ipv6_txoptions *opt,\n\t\t\t\t\t\t   int newtype,\n\t\t\t\t\t\t   struct ipv6_opt_hdr __user *newopt,\n\t\t\t\t\t\t   int newoptlen);\nstruct ipv6_txoptions *ipv6_fixup_options(struct ipv6_txoptions *opt_space,\n\t\t\t\t\t  struct ipv6_txoptions *opt);\n\nextern int ipv6_opt_accepted(struct sock *sk, struct sk_buff *skb);\n\nint ip6_frag_nqueues(struct net *net);\nint ip6_frag_mem(struct net *net);\n\n#define IPV6_FRAG_HIGH_THRESH\t(256 * 1024)\t/* 262144 */\n#define IPV6_FRAG_LOW_THRESH\t(192 * 1024)\t/* 196608 */\n#define IPV6_FRAG_TIMEOUT\t(60 * HZ)\t/* 60 seconds */\n\nextern int __ipv6_addr_type(const struct in6_addr *addr);\nstatic inline int ipv6_addr_type(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_type(addr) & 0xffff;\n}\n\nstatic inline int ipv6_addr_scope(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_type(addr) & IPV6_ADDR_SCOPE_MASK;\n}\n\nstatic inline int __ipv6_addr_src_scope(int type)\n{\n\treturn (type == IPV6_ADDR_ANY) ? __IPV6_ADDR_SCOPE_INVALID : (type >> 16);\n}\n\nstatic inline int ipv6_addr_src_scope(const struct in6_addr *addr)\n{\n\treturn __ipv6_addr_src_scope(__ipv6_addr_type(addr));\n}\n\nstatic inline int ipv6_addr_cmp(const struct in6_addr *a1, const struct in6_addr *a2)\n{\n\treturn memcmp(a1, a2, sizeof(struct in6_addr));\n}\n\nstatic inline int\nipv6_masked_addr_cmp(const struct in6_addr *a1, const struct in6_addr *m,\n\t\t     const struct in6_addr *a2)\n{\n\treturn !!(((a1->s6_addr32[0] ^ a2->s6_addr32[0]) & m->s6_addr32[0]) |\n\t\t  ((a1->s6_addr32[1] ^ a2->s6_addr32[1]) & m->s6_addr32[1]) |\n\t\t  ((a1->s6_addr32[2] ^ a2->s6_addr32[2]) & m->s6_addr32[2]) |\n\t\t  ((a1->s6_addr32[3] ^ a2->s6_addr32[3]) & m->s6_addr32[3]));\n}\n\nstatic inline void ipv6_addr_copy(struct in6_addr *a1, const struct in6_addr *a2)\n{\n\tmemcpy(a1, a2, sizeof(struct in6_addr));\n}\n\nstatic inline void ipv6_addr_prefix(struct in6_addr *pfx, \n\t\t\t\t    const struct in6_addr *addr,\n\t\t\t\t    int plen)\n{\n\t/* caller must guarantee 0 <= plen <= 128 */\n\tint o = plen >> 3,\n\t    b = plen & 0x7;\n\n\tmemset(pfx->s6_addr, 0, sizeof(pfx->s6_addr));\n\tmemcpy(pfx->s6_addr, addr, o);\n\tif (b != 0)\n\t\tpfx->s6_addr[o] = addr->s6_addr[o] & (0xff00 >> b);\n}\n\nstatic inline void ipv6_addr_set(struct in6_addr *addr, \n\t\t\t\t     __be32 w1, __be32 w2,\n\t\t\t\t     __be32 w3, __be32 w4)\n{\n\taddr->s6_addr32[0] = w1;\n\taddr->s6_addr32[1] = w2;\n\taddr->s6_addr32[2] = w3;\n\taddr->s6_addr32[3] = w4;\n}\n\nstatic inline int ipv6_addr_equal(const struct in6_addr *a1,\n\t\t\t\t  const struct in6_addr *a2)\n{\n\treturn ((a1->s6_addr32[0] ^ a2->s6_addr32[0]) |\n\t\t(a1->s6_addr32[1] ^ a2->s6_addr32[1]) |\n\t\t(a1->s6_addr32[2] ^ a2->s6_addr32[2]) |\n\t\t(a1->s6_addr32[3] ^ a2->s6_addr32[3])) == 0;\n}\n\nstatic inline int __ipv6_prefix_equal(const __be32 *a1, const __be32 *a2,\n\t\t\t\t      unsigned int prefixlen)\n{\n\tunsigned pdw, pbi;\n\n\t/* check complete u32 in prefix */\n\tpdw = prefixlen >> 5;\n\tif (pdw && memcmp(a1, a2, pdw << 2))\n\t\treturn 0;\n\n\t/* check incomplete u32 in prefix */\n\tpbi = prefixlen & 0x1f;\n\tif (pbi && ((a1[pdw] ^ a2[pdw]) & htonl((0xffffffff) << (32 - pbi))))\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic inline int ipv6_prefix_equal(const struct in6_addr *a1,\n\t\t\t\t    const struct in6_addr *a2,\n\t\t\t\t    unsigned int prefixlen)\n{\n\treturn __ipv6_prefix_equal(a1->s6_addr32, a2->s6_addr32,\n\t\t\t\t   prefixlen);\n}\n\nstruct inet_frag_queue;\n\nenum ip6_defrag_users {\n\tIP6_DEFRAG_LOCAL_DELIVER,\n\tIP6_DEFRAG_CONNTRACK_IN,\n\t__IP6_DEFRAG_CONNTRACK_IN\t= IP6_DEFRAG_CONNTRACK_IN + USHRT_MAX,\n\tIP6_DEFRAG_CONNTRACK_OUT,\n\t__IP6_DEFRAG_CONNTRACK_OUT\t= IP6_DEFRAG_CONNTRACK_OUT + USHRT_MAX,\n\tIP6_DEFRAG_CONNTRACK_BRIDGE_IN,\n\t__IP6_DEFRAG_CONNTRACK_BRIDGE_IN = IP6_DEFRAG_CONNTRACK_BRIDGE_IN + USHRT_MAX,\n};\n\nstruct ip6_create_arg {\n\t__be32 id;\n\tu32 user;\n\tconst struct in6_addr *src;\n\tconst struct in6_addr *dst;\n};\n\nvoid ip6_frag_init(struct inet_frag_queue *q, void *a);\nint ip6_frag_match(struct inet_frag_queue *q, void *a);\n\nstatic inline int ipv6_addr_any(const struct in6_addr *a)\n{\n\treturn (a->s6_addr32[0] | a->s6_addr32[1] |\n\t\ta->s6_addr32[2] | a->s6_addr32[3]) == 0;\n}\n\nstatic inline int ipv6_addr_loopback(const struct in6_addr *a)\n{\n\treturn (a->s6_addr32[0] | a->s6_addr32[1] |\n\t\ta->s6_addr32[2] | (a->s6_addr32[3] ^ htonl(1))) == 0;\n}\n\nstatic inline int ipv6_addr_v4mapped(const struct in6_addr *a)\n{\n\treturn (a->s6_addr32[0] | a->s6_addr32[1] |\n\t\t (a->s6_addr32[2] ^ htonl(0x0000ffff))) == 0;\n}\n\n/*\n * Check for a RFC 4843 ORCHID address\n * (Overlay Routable Cryptographic Hash Identifiers)\n */\nstatic inline int ipv6_addr_orchid(const struct in6_addr *a)\n{\n\treturn (a->s6_addr32[0] & htonl(0xfffffff0)) == htonl(0x20010010);\n}\n\nstatic inline void ipv6_addr_set_v4mapped(const __be32 addr,\n\t\t\t\t\t  struct in6_addr *v4mapped)\n{\n\tipv6_addr_set(v4mapped,\n\t\t\t0, 0,\n\t\t\thtonl(0x0000FFFF),\n\t\t\taddr);\n}\n\n/*\n * find the first different bit between two addresses\n * length of address must be a multiple of 32bits\n */\nstatic inline int __ipv6_addr_diff(const void *token1, const void *token2, int addrlen)\n{\n\tconst __be32 *a1 = token1, *a2 = token2;\n\tint i;\n\n\taddrlen >>= 2;\n\n\tfor (i = 0; i < addrlen; i++) {\n\t\t__be32 xb = a1[i] ^ a2[i];\n\t\tif (xb)\n\t\t\treturn i * 32 + 31 - __fls(ntohl(xb));\n\t}\n\n\t/*\n\t *\twe should *never* get to this point since that \n\t *\twould mean the addrs are equal\n\t *\n\t *\tHowever, we do get to it 8) And exacly, when\n\t *\taddresses are equal 8)\n\t *\n\t *\tip route add 1111::/128 via ...\n\t *\tip route add 1111::/64 via ...\n\t *\tand we are here.\n\t *\n\t *\tIdeally, this function should stop comparison\n\t *\tat prefix length. It does not, but it is still OK,\n\t *\tif returned value is greater than prefix length.\n\t *\t\t\t\t\t--ANK (980803)\n\t */\n\treturn addrlen << 5;\n}\n\nstatic inline int ipv6_addr_diff(const struct in6_addr *a1, const struct in6_addr *a2)\n{\n\treturn __ipv6_addr_diff(a1, a2, sizeof(struct in6_addr));\n}\n\nextern void ipv6_select_ident(struct frag_hdr *fhdr, struct rt6_info *rt);\n\n/*\n *\tPrototypes exported by ipv6\n */\n\n/*\n *\trcv function (called from netdevice level)\n */\n\nextern int\t\t\tipv6_rcv(struct sk_buff *skb, \n\t\t\t\t\t struct net_device *dev, \n\t\t\t\t\t struct packet_type *pt,\n\t\t\t\t\t struct net_device *orig_dev);\n\nextern int\t\t\tip6_rcv_finish(struct sk_buff *skb);\n\n/*\n *\tupper-layer output functions\n */\nextern int\t\t\tip6_xmit(struct sock *sk,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct flowi6 *fl6,\n\t\t\t\t\t struct ipv6_txoptions *opt);\n\nextern int\t\t\tip6_nd_hdr(struct sock *sk,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   const struct in6_addr *saddr,\n\t\t\t\t\t   const struct in6_addr *daddr,\n\t\t\t\t\t   int proto, int len);\n\nextern int\t\t\tip6_find_1stfragopt(struct sk_buff *skb, u8 **nexthdr);\n\nextern int\t\t\tip6_append_data(struct sock *sk,\n\t\t\t\t\t\tint getfrag(void *from, char *to, int offset, int len, int odd, struct sk_buff *skb),\n\t\t    \t\t\t\tvoid *from,\n\t\t\t\t\t\tint length,\n\t\t\t\t\t\tint transhdrlen,\n\t\t      \t\t\t\tint hlimit,\n\t\t      \t\t\t\tint tclass,\n\t\t\t\t\t\tstruct ipv6_txoptions *opt,\n\t\t\t\t\t\tstruct flowi6 *fl6,\n\t\t\t\t\t\tstruct rt6_info *rt,\n\t\t\t\t\t\tunsigned int flags,\n\t\t\t\t\t\tint dontfrag);\n\nextern int\t\t\tip6_push_pending_frames(struct sock *sk);\n\nextern void\t\t\tip6_flush_pending_frames(struct sock *sk);\n\nextern int\t\t\tip6_dst_lookup(struct sock *sk,\n\t\t\t\t\t       struct dst_entry **dst,\n\t\t\t\t\t       struct flowi6 *fl6);\nextern struct dst_entry *\tip6_dst_lookup_flow(struct sock *sk,\n\t\t\t\t\t\t    struct flowi6 *fl6,\n\t\t\t\t\t\t    const struct in6_addr *final_dst,\n\t\t\t\t\t\t    bool can_sleep);\nextern struct dst_entry *\tip6_sk_dst_lookup_flow(struct sock *sk,\n\t\t\t\t\t\t       struct flowi6 *fl6,\n\t\t\t\t\t\t       const struct in6_addr *final_dst,\n\t\t\t\t\t\t       bool can_sleep);\nextern struct dst_entry *\tip6_blackhole_route(struct net *net,\n\t\t\t\t\t\t    struct dst_entry *orig_dst);\n\n/*\n *\tskb processing functions\n */\n\nextern int\t\t\tip6_output(struct sk_buff *skb);\nextern int\t\t\tip6_forward(struct sk_buff *skb);\nextern int\t\t\tip6_input(struct sk_buff *skb);\nextern int\t\t\tip6_mc_input(struct sk_buff *skb);\n\nextern int\t\t\t__ip6_local_out(struct sk_buff *skb);\nextern int\t\t\tip6_local_out(struct sk_buff *skb);\n\n/*\n *\tExtension header (options) processing\n */\n\nextern void \t\t\tipv6_push_nfrag_opts(struct sk_buff *skb,\n\t\t\t\t\t\t     struct ipv6_txoptions *opt,\n\t\t\t\t\t\t     u8 *proto,\n\t\t\t\t\t\t     struct in6_addr **daddr_p);\nextern void\t\t\tipv6_push_frag_opts(struct sk_buff *skb,\n\t\t\t\t\t\t    struct ipv6_txoptions *opt,\n\t\t\t\t\t\t    u8 *proto);\n\nextern int\t\t\tipv6_skip_exthdr(const struct sk_buff *, int start,\n\t\t\t\t\t         u8 *nexthdrp);\n\nextern int \t\t\tipv6_ext_hdr(u8 nexthdr);\n\nextern int ipv6_find_tlv(struct sk_buff *skb, int offset, int type);\n\nextern struct in6_addr *fl6_update_dst(struct flowi6 *fl6,\n\t\t\t\t       const struct ipv6_txoptions *opt,\n\t\t\t\t       struct in6_addr *orig);\n\n/*\n *\tsocket options (ipv6_sockglue.c)\n */\n\nextern int\t\t\tipv6_setsockopt(struct sock *sk, int level, \n\t\t\t\t\t\tint optname,\n\t\t\t\t\t\tchar __user *optval, \n\t\t\t\t\t\tunsigned int optlen);\nextern int\t\t\tipv6_getsockopt(struct sock *sk, int level, \n\t\t\t\t\t\tint optname,\n\t\t\t\t\t\tchar __user *optval, \n\t\t\t\t\t\tint __user *optlen);\nextern int\t\t\tcompat_ipv6_setsockopt(struct sock *sk,\n\t\t\t\t\t\tint level,\n\t\t\t\t\t\tint optname,\n\t\t\t\t\t\tchar __user *optval,\n\t\t\t\t\t\tunsigned int optlen);\nextern int\t\t\tcompat_ipv6_getsockopt(struct sock *sk,\n\t\t\t\t\t\tint level,\n\t\t\t\t\t\tint optname,\n\t\t\t\t\t\tchar __user *optval,\n\t\t\t\t\t\tint __user *optlen);\n\nextern int\t\t\tip6_datagram_connect(struct sock *sk, \n\t\t\t\t\t\t     struct sockaddr *addr, int addr_len);\n\nextern int \t\t\tipv6_recv_error(struct sock *sk, struct msghdr *msg, int len);\nextern int \t\t\tipv6_recv_rxpmtu(struct sock *sk, struct msghdr *msg, int len);\nextern void\t\t\tipv6_icmp_error(struct sock *sk, struct sk_buff *skb, int err, __be16 port,\n\t\t\t\t\t\tu32 info, u8 *payload);\nextern void\t\t\tipv6_local_error(struct sock *sk, int err, struct flowi6 *fl6, u32 info);\nextern void\t\t\tipv6_local_rxpmtu(struct sock *sk, struct flowi6 *fl6, u32 mtu);\n\nextern int inet6_release(struct socket *sock);\nextern int inet6_bind(struct socket *sock, struct sockaddr *uaddr, \n\t\t      int addr_len);\nextern int inet6_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t int *uaddr_len, int peer);\nextern int inet6_ioctl(struct socket *sock, unsigned int cmd, \n\t\t       unsigned long arg);\n\nextern int inet6_hash_connect(struct inet_timewait_death_row *death_row,\n\t\t\t      struct sock *sk);\n\n/*\n * reassembly.c\n */\nextern const struct proto_ops inet6_stream_ops;\nextern const struct proto_ops inet6_dgram_ops;\n\nstruct group_source_req;\nstruct group_filter;\n\nextern int ip6_mc_source(int add, int omode, struct sock *sk,\n\t\t\t struct group_source_req *pgsr);\nextern int ip6_mc_msfilter(struct sock *sk, struct group_filter *gsf);\nextern int ip6_mc_msfget(struct sock *sk, struct group_filter *gsf,\n\t\t\t struct group_filter __user *optval,\n\t\t\t int __user *optlen);\nextern unsigned int inet6_hash_frag(__be32 id, const struct in6_addr *saddr,\n\t\t\t\t    const struct in6_addr *daddr, u32 rnd);\n\n#ifdef CONFIG_PROC_FS\nextern int  ac6_proc_init(struct net *net);\nextern void ac6_proc_exit(struct net *net);\nextern int  raw6_proc_init(void);\nextern void raw6_proc_exit(void);\nextern int  tcp6_proc_init(struct net *net);\nextern void tcp6_proc_exit(struct net *net);\nextern int  udp6_proc_init(struct net *net);\nextern void udp6_proc_exit(struct net *net);\nextern int  udplite6_proc_init(void);\nextern void udplite6_proc_exit(void);\nextern int  ipv6_misc_proc_init(void);\nextern void ipv6_misc_proc_exit(void);\nextern int snmp6_register_dev(struct inet6_dev *idev);\nextern int snmp6_unregister_dev(struct inet6_dev *idev);\n\n#else\nstatic inline int ac6_proc_init(struct net *net) { return 0; }\nstatic inline void ac6_proc_exit(struct net *net) { }\nstatic inline int snmp6_register_dev(struct inet6_dev *idev) { return 0; }\nstatic inline int snmp6_unregister_dev(struct inet6_dev *idev) { return 0; }\n#endif\n\n#ifdef CONFIG_SYSCTL\nextern ctl_table ipv6_route_table_template[];\nextern ctl_table ipv6_icmp_table_template[];\n\nextern struct ctl_table *ipv6_icmp_sysctl_init(struct net *net);\nextern struct ctl_table *ipv6_route_sysctl_init(struct net *net);\nextern int ipv6_sysctl_register(void);\nextern void ipv6_sysctl_unregister(void);\nextern int ipv6_static_sysctl_register(void);\nextern void ipv6_static_sysctl_unregister(void);\n#endif\n\n#endif /* _NET_IPV6_H */\n", "/*\n *\t\tINETPEER - A storage for permanent information about peers\n *\n *  This source is covered by the GNU GPL, the same as all kernel sources.\n *\n *  Authors:\tAndrey V. Savochkin <saw@msu.ru>\n */\n\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/spinlock.h>\n#include <linux/random.h>\n#include <linux/timer.h>\n#include <linux/time.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/net.h>\n#include <net/ip.h>\n#include <net/inetpeer.h>\n\n/*\n *  Theory of operations.\n *  We keep one entry for each peer IP address.  The nodes contains long-living\n *  information about the peer which doesn't depend on routes.\n *  At this moment this information consists only of ID field for the next\n *  outgoing IP packet.  This field is incremented with each packet as encoded\n *  in inet_getid() function (include/net/inetpeer.h).\n *  At the moment of writing this notes identifier of IP packets is generated\n *  to be unpredictable using this code only for packets subjected\n *  (actually or potentially) to defragmentation.  I.e. DF packets less than\n *  PMTU in size uses a constant ID and do not use this code (see\n *  ip_select_ident() in include/net/ip.h).\n *\n *  Route cache entries hold references to our nodes.\n *  New cache entries get references via lookup by destination IP address in\n *  the avl tree.  The reference is grabbed only when it's needed i.e. only\n *  when we try to output IP packet which needs an unpredictable ID (see\n *  __ip_select_ident() in net/ipv4/route.c).\n *  Nodes are removed only when reference counter goes to 0.\n *  When it's happened the node may be removed when a sufficient amount of\n *  time has been passed since its last use.  The less-recently-used entry can\n *  also be removed if the pool is overloaded i.e. if the total amount of\n *  entries is greater-or-equal than the threshold.\n *\n *  Node pool is organised as an AVL tree.\n *  Such an implementation has been chosen not just for fun.  It's a way to\n *  prevent easy and efficient DoS attacks by creating hash collisions.  A huge\n *  amount of long living nodes in a single hash slot would significantly delay\n *  lookups performed with disabled BHs.\n *\n *  Serialisation issues.\n *  1.  Nodes may appear in the tree only with the pool lock held.\n *  2.  Nodes may disappear from the tree only with the pool lock held\n *      AND reference count being 0.\n *  3.  Global variable peer_total is modified under the pool lock.\n *  4.  struct inet_peer fields modification:\n *\t\tavl_left, avl_right, avl_parent, avl_height: pool lock\n *\t\trefcnt: atomically against modifications on other CPU;\n *\t\t   usually under some other lock to prevent node disappearing\n *\t\tdaddr: unchangeable\n *\t\tip_id_count: atomic value (no lock needed)\n */\n\nstatic struct kmem_cache *peer_cachep __read_mostly;\n\n#define node_height(x) x->avl_height\n\n#define peer_avl_empty ((struct inet_peer *)&peer_fake_node)\n#define peer_avl_empty_rcu ((struct inet_peer __rcu __force *)&peer_fake_node)\nstatic const struct inet_peer peer_fake_node = {\n\t.avl_left\t= peer_avl_empty_rcu,\n\t.avl_right\t= peer_avl_empty_rcu,\n\t.avl_height\t= 0\n};\n\nstruct inet_peer_base {\n\tstruct inet_peer __rcu *root;\n\tseqlock_t\tlock;\n\tint\t\ttotal;\n};\n\nstatic struct inet_peer_base v4_peers = {\n\t.root\t\t= peer_avl_empty_rcu,\n\t.lock\t\t= __SEQLOCK_UNLOCKED(v4_peers.lock),\n\t.total\t\t= 0,\n};\n\nstatic struct inet_peer_base v6_peers = {\n\t.root\t\t= peer_avl_empty_rcu,\n\t.lock\t\t= __SEQLOCK_UNLOCKED(v6_peers.lock),\n\t.total\t\t= 0,\n};\n\n#define PEER_MAXDEPTH 40 /* sufficient for about 2^27 nodes */\n\n/* Exported for sysctl_net_ipv4.  */\nint inet_peer_threshold __read_mostly = 65536 + 128;\t/* start to throw entries more\n\t\t\t\t\t * aggressively at this stage */\nint inet_peer_minttl __read_mostly = 120 * HZ;\t/* TTL under high load: 120 sec */\nint inet_peer_maxttl __read_mostly = 10 * 60 * HZ;\t/* usual time to live: 10 min */\n\n\n/* Called from ip_output.c:ip_init  */\nvoid __init inet_initpeers(void)\n{\n\tstruct sysinfo si;\n\n\t/* Use the straight interface to information about memory. */\n\tsi_meminfo(&si);\n\t/* The values below were suggested by Alexey Kuznetsov\n\t * <kuznet@ms2.inr.ac.ru>.  I don't have any opinion about the values\n\t * myself.  --SAW\n\t */\n\tif (si.totalram <= (32768*1024)/PAGE_SIZE)\n\t\tinet_peer_threshold >>= 1; /* max pool size about 1MB on IA32 */\n\tif (si.totalram <= (16384*1024)/PAGE_SIZE)\n\t\tinet_peer_threshold >>= 1; /* about 512KB */\n\tif (si.totalram <= (8192*1024)/PAGE_SIZE)\n\t\tinet_peer_threshold >>= 2; /* about 128KB */\n\n\tpeer_cachep = kmem_cache_create(\"inet_peer_cache\",\n\t\t\tsizeof(struct inet_peer),\n\t\t\t0, SLAB_HWCACHE_ALIGN | SLAB_PANIC,\n\t\t\tNULL);\n\n}\n\nstatic int addr_compare(const struct inetpeer_addr *a,\n\t\t\tconst struct inetpeer_addr *b)\n{\n\tint i, n = (a->family == AF_INET ? 1 : 4);\n\n\tfor (i = 0; i < n; i++) {\n\t\tif (a->addr.a6[i] == b->addr.a6[i])\n\t\t\tcontinue;\n\t\tif (a->addr.a6[i] < b->addr.a6[i])\n\t\t\treturn -1;\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n#define rcu_deref_locked(X, BASE)\t\t\t\t\\\n\trcu_dereference_protected(X, lockdep_is_held(&(BASE)->lock.lock))\n\n/*\n * Called with local BH disabled and the pool lock held.\n */\n#define lookup(_daddr, _stack, _base)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tstruct inet_peer *u;\t\t\t\t\t\\\n\tstruct inet_peer __rcu **v;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tstackptr = _stack;\t\t\t\t\t\\\n\t*stackptr++ = &_base->root;\t\t\t\t\\\n\tfor (u = rcu_deref_locked(_base->root, _base);\t\t\\\n\t     u != peer_avl_empty; ) {\t\t\t\t\\\n\t\tint cmp = addr_compare(_daddr, &u->daddr);\t\\\n\t\tif (cmp == 0)\t\t\t\t\t\\\n\t\t\tbreak;\t\t\t\t\t\\\n\t\tif (cmp == -1)\t\t\t\t\t\\\n\t\t\tv = &u->avl_left;\t\t\t\\\n\t\telse\t\t\t\t\t\t\\\n\t\t\tv = &u->avl_right;\t\t\t\\\n\t\t*stackptr++ = v;\t\t\t\t\\\n\t\tu = rcu_deref_locked(*v, _base);\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tu;\t\t\t\t\t\t\t\\\n})\n\n/*\n * Called with rcu_read_lock()\n * Because we hold no lock against a writer, its quite possible we fall\n * in an endless loop.\n * But every pointer we follow is guaranteed to be valid thanks to RCU.\n * We exit from this function if number of links exceeds PEER_MAXDEPTH\n */\nstatic struct inet_peer *lookup_rcu(const struct inetpeer_addr *daddr,\n\t\t\t\t    struct inet_peer_base *base)\n{\n\tstruct inet_peer *u = rcu_dereference(base->root);\n\tint count = 0;\n\n\twhile (u != peer_avl_empty) {\n\t\tint cmp = addr_compare(daddr, &u->daddr);\n\t\tif (cmp == 0) {\n\t\t\t/* Before taking a reference, check if this entry was\n\t\t\t * deleted (refcnt=-1)\n\t\t\t */\n\t\t\tif (!atomic_add_unless(&u->refcnt, 1, -1))\n\t\t\t\tu = NULL;\n\t\t\treturn u;\n\t\t}\n\t\tif (cmp == -1)\n\t\t\tu = rcu_dereference(u->avl_left);\n\t\telse\n\t\t\tu = rcu_dereference(u->avl_right);\n\t\tif (unlikely(++count == PEER_MAXDEPTH))\n\t\t\tbreak;\n\t}\n\treturn NULL;\n}\n\n/* Called with local BH disabled and the pool lock held. */\n#define lookup_rightempty(start, base)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tstruct inet_peer *u;\t\t\t\t\t\\\n\tstruct inet_peer __rcu **v;\t\t\t\t\\\n\t*stackptr++ = &start->avl_left;\t\t\t\t\\\n\tv = &start->avl_left;\t\t\t\t\t\\\n\tfor (u = rcu_deref_locked(*v, base);\t\t\t\\\n\t     u->avl_right != peer_avl_empty_rcu; ) {\t\t\\\n\t\tv = &u->avl_right;\t\t\t\t\\\n\t\t*stackptr++ = v;\t\t\t\t\\\n\t\tu = rcu_deref_locked(*v, base);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tu;\t\t\t\t\t\t\t\\\n})\n\n/* Called with local BH disabled and the pool lock held.\n * Variable names are the proof of operation correctness.\n * Look into mm/map_avl.c for more detail description of the ideas.\n */\nstatic void peer_avl_rebalance(struct inet_peer __rcu **stack[],\n\t\t\t       struct inet_peer __rcu ***stackend,\n\t\t\t       struct inet_peer_base *base)\n{\n\tstruct inet_peer __rcu **nodep;\n\tstruct inet_peer *node, *l, *r;\n\tint lh, rh;\n\n\twhile (stackend > stack) {\n\t\tnodep = *--stackend;\n\t\tnode = rcu_deref_locked(*nodep, base);\n\t\tl = rcu_deref_locked(node->avl_left, base);\n\t\tr = rcu_deref_locked(node->avl_right, base);\n\t\tlh = node_height(l);\n\t\trh = node_height(r);\n\t\tif (lh > rh + 1) { /* l: RH+2 */\n\t\t\tstruct inet_peer *ll, *lr, *lrl, *lrr;\n\t\t\tint lrh;\n\t\t\tll = rcu_deref_locked(l->avl_left, base);\n\t\t\tlr = rcu_deref_locked(l->avl_right, base);\n\t\t\tlrh = node_height(lr);\n\t\t\tif (lrh <= node_height(ll)) {\t/* ll: RH+1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_left, lr);\t/* lr: RH or RH+1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_right, r);\t/* r: RH */\n\t\t\t\tnode->avl_height = lrh + 1; /* RH+1 or RH+2 */\n\t\t\t\tRCU_INIT_POINTER(l->avl_left, ll);       /* ll: RH+1 */\n\t\t\t\tRCU_INIT_POINTER(l->avl_right, node);\t/* node: RH+1 or RH+2 */\n\t\t\t\tl->avl_height = node->avl_height + 1;\n\t\t\t\tRCU_INIT_POINTER(*nodep, l);\n\t\t\t} else { /* ll: RH, lr: RH+1 */\n\t\t\t\tlrl = rcu_deref_locked(lr->avl_left, base);/* lrl: RH or RH-1 */\n\t\t\t\tlrr = rcu_deref_locked(lr->avl_right, base);/* lrr: RH or RH-1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_left, lrr);\t/* lrr: RH or RH-1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_right, r);\t/* r: RH */\n\t\t\t\tnode->avl_height = rh + 1; /* node: RH+1 */\n\t\t\t\tRCU_INIT_POINTER(l->avl_left, ll);\t/* ll: RH */\n\t\t\t\tRCU_INIT_POINTER(l->avl_right, lrl);\t/* lrl: RH or RH-1 */\n\t\t\t\tl->avl_height = rh + 1;\t/* l: RH+1 */\n\t\t\t\tRCU_INIT_POINTER(lr->avl_left, l);\t/* l: RH+1 */\n\t\t\t\tRCU_INIT_POINTER(lr->avl_right, node);\t/* node: RH+1 */\n\t\t\t\tlr->avl_height = rh + 2;\n\t\t\t\tRCU_INIT_POINTER(*nodep, lr);\n\t\t\t}\n\t\t} else if (rh > lh + 1) { /* r: LH+2 */\n\t\t\tstruct inet_peer *rr, *rl, *rlr, *rll;\n\t\t\tint rlh;\n\t\t\trr = rcu_deref_locked(r->avl_right, base);\n\t\t\trl = rcu_deref_locked(r->avl_left, base);\n\t\t\trlh = node_height(rl);\n\t\t\tif (rlh <= node_height(rr)) {\t/* rr: LH+1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_right, rl);\t/* rl: LH or LH+1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_left, l);\t/* l: LH */\n\t\t\t\tnode->avl_height = rlh + 1; /* LH+1 or LH+2 */\n\t\t\t\tRCU_INIT_POINTER(r->avl_right, rr);\t/* rr: LH+1 */\n\t\t\t\tRCU_INIT_POINTER(r->avl_left, node);\t/* node: LH+1 or LH+2 */\n\t\t\t\tr->avl_height = node->avl_height + 1;\n\t\t\t\tRCU_INIT_POINTER(*nodep, r);\n\t\t\t} else { /* rr: RH, rl: RH+1 */\n\t\t\t\trlr = rcu_deref_locked(rl->avl_right, base);/* rlr: LH or LH-1 */\n\t\t\t\trll = rcu_deref_locked(rl->avl_left, base);/* rll: LH or LH-1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_right, rll);\t/* rll: LH or LH-1 */\n\t\t\t\tRCU_INIT_POINTER(node->avl_left, l);\t/* l: LH */\n\t\t\t\tnode->avl_height = lh + 1; /* node: LH+1 */\n\t\t\t\tRCU_INIT_POINTER(r->avl_right, rr);\t/* rr: LH */\n\t\t\t\tRCU_INIT_POINTER(r->avl_left, rlr);\t/* rlr: LH or LH-1 */\n\t\t\t\tr->avl_height = lh + 1;\t/* r: LH+1 */\n\t\t\t\tRCU_INIT_POINTER(rl->avl_right, r);\t/* r: LH+1 */\n\t\t\t\tRCU_INIT_POINTER(rl->avl_left, node);\t/* node: LH+1 */\n\t\t\t\trl->avl_height = lh + 2;\n\t\t\t\tRCU_INIT_POINTER(*nodep, rl);\n\t\t\t}\n\t\t} else {\n\t\t\tnode->avl_height = (lh > rh ? lh : rh) + 1;\n\t\t}\n\t}\n}\n\n/* Called with local BH disabled and the pool lock held. */\n#define link_to_pool(n, base)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tn->avl_height = 1;\t\t\t\t\t\\\n\tn->avl_left = peer_avl_empty_rcu;\t\t\t\\\n\tn->avl_right = peer_avl_empty_rcu;\t\t\t\\\n\t/* lockless readers can catch us now */\t\t\t\\\n\trcu_assign_pointer(**--stackptr, n);\t\t\t\\\n\tpeer_avl_rebalance(stack, stackptr, base);\t\t\\\n} while (0)\n\nstatic void inetpeer_free_rcu(struct rcu_head *head)\n{\n\tkmem_cache_free(peer_cachep, container_of(head, struct inet_peer, rcu));\n}\n\nstatic void unlink_from_pool(struct inet_peer *p, struct inet_peer_base *base,\n\t\t\t     struct inet_peer __rcu **stack[PEER_MAXDEPTH])\n{\n\tstruct inet_peer __rcu ***stackptr, ***delp;\n\n\tif (lookup(&p->daddr, stack, base) != p)\n\t\tBUG();\n\tdelp = stackptr - 1; /* *delp[0] == p */\n\tif (p->avl_left == peer_avl_empty_rcu) {\n\t\t*delp[0] = p->avl_right;\n\t\t--stackptr;\n\t} else {\n\t\t/* look for a node to insert instead of p */\n\t\tstruct inet_peer *t;\n\t\tt = lookup_rightempty(p, base);\n\t\tBUG_ON(rcu_deref_locked(*stackptr[-1], base) != t);\n\t\t**--stackptr = t->avl_left;\n\t\t/* t is removed, t->daddr > x->daddr for any\n\t\t * x in p->avl_left subtree.\n\t\t * Put t in the old place of p. */\n\t\tRCU_INIT_POINTER(*delp[0], t);\n\t\tt->avl_left = p->avl_left;\n\t\tt->avl_right = p->avl_right;\n\t\tt->avl_height = p->avl_height;\n\t\tBUG_ON(delp[1] != &p->avl_left);\n\t\tdelp[1] = &t->avl_left; /* was &p->avl_left */\n\t}\n\tpeer_avl_rebalance(stack, stackptr, base);\n\tbase->total--;\n\tcall_rcu(&p->rcu, inetpeer_free_rcu);\n}\n\nstatic struct inet_peer_base *family_to_base(int family)\n{\n\treturn family == AF_INET ? &v4_peers : &v6_peers;\n}\n\n/* perform garbage collect on all items stacked during a lookup */\nstatic int inet_peer_gc(struct inet_peer_base *base,\n\t\t\tstruct inet_peer __rcu **stack[PEER_MAXDEPTH],\n\t\t\tstruct inet_peer __rcu ***stackptr)\n{\n\tstruct inet_peer *p, *gchead = NULL;\n\t__u32 delta, ttl;\n\tint cnt = 0;\n\n\tif (base->total >= inet_peer_threshold)\n\t\tttl = 0; /* be aggressive */\n\telse\n\t\tttl = inet_peer_maxttl\n\t\t\t\t- (inet_peer_maxttl - inet_peer_minttl) / HZ *\n\t\t\t\t\tbase->total / inet_peer_threshold * HZ;\n\tstackptr--; /* last stack slot is peer_avl_empty */\n\twhile (stackptr > stack) {\n\t\tstackptr--;\n\t\tp = rcu_deref_locked(**stackptr, base);\n\t\tif (atomic_read(&p->refcnt) == 0) {\n\t\t\tsmp_rmb();\n\t\t\tdelta = (__u32)jiffies - p->dtime;\n\t\t\tif (delta >= ttl &&\n\t\t\t    atomic_cmpxchg(&p->refcnt, 0, -1) == 0) {\n\t\t\t\tp->gc_next = gchead;\n\t\t\t\tgchead = p;\n\t\t\t}\n\t\t}\n\t}\n\twhile ((p = gchead) != NULL) {\n\t\tgchead = p->gc_next;\n\t\tcnt++;\n\t\tunlink_from_pool(p, base, stack);\n\t}\n\treturn cnt;\n}\n\nstruct inet_peer *inet_getpeer(const struct inetpeer_addr *daddr, int create)\n{\n\tstruct inet_peer __rcu **stack[PEER_MAXDEPTH], ***stackptr;\n\tstruct inet_peer_base *base = family_to_base(daddr->family);\n\tstruct inet_peer *p;\n\tunsigned int sequence;\n\tint invalidated, gccnt = 0;\n\n\t/* Attempt a lockless lookup first.\n\t * Because of a concurrent writer, we might not find an existing entry.\n\t */\n\trcu_read_lock();\n\tsequence = read_seqbegin(&base->lock);\n\tp = lookup_rcu(daddr, base);\n\tinvalidated = read_seqretry(&base->lock, sequence);\n\trcu_read_unlock();\n\n\tif (p)\n\t\treturn p;\n\n\t/* If no writer did a change during our lookup, we can return early. */\n\tif (!create && !invalidated)\n\t\treturn NULL;\n\n\t/* retry an exact lookup, taking the lock before.\n\t * At least, nodes should be hot in our cache.\n\t */\n\twrite_seqlock_bh(&base->lock);\nrelookup:\n\tp = lookup(daddr, stack, base);\n\tif (p != peer_avl_empty) {\n\t\tatomic_inc(&p->refcnt);\n\t\twrite_sequnlock_bh(&base->lock);\n\t\treturn p;\n\t}\n\tif (!gccnt) {\n\t\tgccnt = inet_peer_gc(base, stack, stackptr);\n\t\tif (gccnt && create)\n\t\t\tgoto relookup;\n\t}\n\tp = create ? kmem_cache_alloc(peer_cachep, GFP_ATOMIC) : NULL;\n\tif (p) {\n\t\tp->daddr = *daddr;\n\t\tatomic_set(&p->refcnt, 1);\n\t\tatomic_set(&p->rid, 0);\n\t\tatomic_set(&p->ip_id_count,\n\t\t\t\t(daddr->family == AF_INET) ?\n\t\t\t\t\tsecure_ip_id(daddr->addr.a4) :\n\t\t\t\t\tsecure_ipv6_id(daddr->addr.a6));\n\t\tp->tcp_ts_stamp = 0;\n\t\tp->metrics[RTAX_LOCK-1] = INETPEER_METRICS_NEW;\n\t\tp->rate_tokens = 0;\n\t\tp->rate_last = 0;\n\t\tp->pmtu_expires = 0;\n\t\tp->pmtu_orig = 0;\n\t\tmemset(&p->redirect_learned, 0, sizeof(p->redirect_learned));\n\n\n\t\t/* Link the node. */\n\t\tlink_to_pool(p, base);\n\t\tbase->total++;\n\t}\n\twrite_sequnlock_bh(&base->lock);\n\n\treturn p;\n}\nEXPORT_SYMBOL_GPL(inet_getpeer);\n\nvoid inet_putpeer(struct inet_peer *p)\n{\n\tp->dtime = (__u32)jiffies;\n\tsmp_mb__before_atomic_dec();\n\tatomic_dec(&p->refcnt);\n}\nEXPORT_SYMBOL_GPL(inet_putpeer);\n\n/*\n *\tCheck transmit rate limitation for given message.\n *\tThe rate information is held in the inet_peer entries now.\n *\tThis function is generic and could be used for other purposes\n *\ttoo. It uses a Token bucket filter as suggested by Alexey Kuznetsov.\n *\n *\tNote that the same inet_peer fields are modified by functions in\n *\troute.c too, but these work for packet destinations while xrlim_allow\n *\tworks for icmp destinations. This means the rate limiting information\n *\tfor one \"ip object\" is shared - and these ICMPs are twice limited:\n *\tby source and by destination.\n *\n *\tRFC 1812: 4.3.2.8 SHOULD be able to limit error message rate\n *\t\t\t  SHOULD allow setting of rate limits\n *\n * \tShared between ICMPv4 and ICMPv6.\n */\n#define XRLIM_BURST_FACTOR 6\nbool inet_peer_xrlim_allow(struct inet_peer *peer, int timeout)\n{\n\tunsigned long now, token;\n\tbool rc = false;\n\n\tif (!peer)\n\t\treturn true;\n\n\ttoken = peer->rate_tokens;\n\tnow = jiffies;\n\ttoken += now - peer->rate_last;\n\tpeer->rate_last = now;\n\tif (token > XRLIM_BURST_FACTOR * timeout)\n\t\ttoken = XRLIM_BURST_FACTOR * timeout;\n\tif (token >= timeout) {\n\t\ttoken -= timeout;\n\t\trc = true;\n\t}\n\tpeer->rate_tokens = token;\n\treturn rc;\n}\nEXPORT_SYMBOL(inet_peer_xrlim_allow);\n", "/*\n *\tIPv6 output functions\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on linux/net/ipv4/ip_output.c\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n *\n *\tChanges:\n *\tA.N.Kuznetsov\t:\tairthmetics in fragmentation.\n *\t\t\t\textension headers are implemented.\n *\t\t\t\troute changes now work.\n *\t\t\t\tip6_forward does not confuse sniffers.\n *\t\t\t\tetc.\n *\n *      H. von Brand    :       Added missing #include <linux/string.h>\n *\tImran Patel\t: \tfrag id should be in NBO\n *      Kazunori MIYAZAWA @USAGI\n *\t\t\t:       add ip6_append_data and related functions\n *\t\t\t\tfor datagram xmit\n */\n\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/socket.h>\n#include <linux/net.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/in6.h>\n#include <linux/tcp.h>\n#include <linux/route.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n\n#include <linux/netfilter.h>\n#include <linux/netfilter_ipv6.h>\n\n#include <net/sock.h>\n#include <net/snmp.h>\n\n#include <net/ipv6.h>\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/ip6_route.h>\n#include <net/addrconf.h>\n#include <net/rawv6.h>\n#include <net/icmp.h>\n#include <net/xfrm.h>\n#include <net/checksum.h>\n#include <linux/mroute6.h>\n\nint ip6_fragment(struct sk_buff *skb, int (*output)(struct sk_buff *));\n\nint __ip6_local_out(struct sk_buff *skb)\n{\n\tint len;\n\n\tlen = skb->len - sizeof(struct ipv6hdr);\n\tif (len > IPV6_MAXPLEN)\n\t\tlen = 0;\n\tipv6_hdr(skb)->payload_len = htons(len);\n\n\treturn nf_hook(NFPROTO_IPV6, NF_INET_LOCAL_OUT, skb, NULL,\n\t\t       skb_dst(skb)->dev, dst_output);\n}\n\nint ip6_local_out(struct sk_buff *skb)\n{\n\tint err;\n\n\terr = __ip6_local_out(skb);\n\tif (likely(err == 1))\n\t\terr = dst_output(skb);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ip6_local_out);\n\n/* dev_loopback_xmit for use with netfilter. */\nstatic int ip6_dev_loopback_xmit(struct sk_buff *newskb)\n{\n\tskb_reset_mac_header(newskb);\n\t__skb_pull(newskb, skb_network_offset(newskb));\n\tnewskb->pkt_type = PACKET_LOOPBACK;\n\tnewskb->ip_summed = CHECKSUM_UNNECESSARY;\n\tWARN_ON(!skb_dst(newskb));\n\n\tnetif_rx_ni(newskb);\n\treturn 0;\n}\n\nstatic int ip6_finish_output2(struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct net_device *dev = dst->dev;\n\tstruct neighbour *neigh;\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb->dev = dev;\n\n\tif (ipv6_addr_is_multicast(&ipv6_hdr(skb)->daddr)) {\n\t\tstruct inet6_dev *idev = ip6_dst_idev(skb_dst(skb));\n\n\t\tif (!(dev->flags & IFF_LOOPBACK) && sk_mc_loop(skb->sk) &&\n\t\t    ((mroute6_socket(dev_net(dev), skb) &&\n\t\t     !(IP6CB(skb)->flags & IP6SKB_FORWARDED)) ||\n\t\t     ipv6_chk_mcast_addr(dev, &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t &ipv6_hdr(skb)->saddr))) {\n\t\t\tstruct sk_buff *newskb = skb_clone(skb, GFP_ATOMIC);\n\n\t\t\t/* Do not check for IFF_ALLMULTI; multicast routing\n\t\t\t   is not supported in any case.\n\t\t\t */\n\t\t\tif (newskb)\n\t\t\t\tNF_HOOK(NFPROTO_IPV6, NF_INET_POST_ROUTING,\n\t\t\t\t\tnewskb, NULL, newskb->dev,\n\t\t\t\t\tip6_dev_loopback_xmit);\n\n\t\t\tif (ipv6_hdr(skb)->hop_limit == 0) {\n\t\t\t\tIP6_INC_STATS(dev_net(dev), idev,\n\t\t\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\tIP6_UPD_PO_STATS(dev_net(dev), idev, IPSTATS_MIB_OUTMCAST,\n\t\t\t\tskb->len);\n\t}\n\n\tneigh = dst_get_neighbour(dst);\n\tif (neigh)\n\t\treturn neigh_output(neigh, skb);\n\n\tIP6_INC_STATS_BH(dev_net(dst->dev),\n\t\t\t ip6_dst_idev(dst), IPSTATS_MIB_OUTNOROUTES);\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}\n\nstatic int ip6_finish_output(struct sk_buff *skb)\n{\n\tif ((skb->len > ip6_skb_dst_mtu(skb) && !skb_is_gso(skb)) ||\n\t    dst_allfrag(skb_dst(skb)))\n\t\treturn ip6_fragment(skb, ip6_finish_output2);\n\telse\n\t\treturn ip6_finish_output2(skb);\n}\n\nint ip6_output(struct sk_buff *skb)\n{\n\tstruct net_device *dev = skb_dst(skb)->dev;\n\tstruct inet6_dev *idev = ip6_dst_idev(skb_dst(skb));\n\tif (unlikely(idev->cnf.disable_ipv6)) {\n\t\tIP6_INC_STATS(dev_net(dev), idev,\n\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t\tkfree_skb(skb);\n\t\treturn 0;\n\t}\n\n\treturn NF_HOOK_COND(NFPROTO_IPV6, NF_INET_POST_ROUTING, skb, NULL, dev,\n\t\t\t    ip6_finish_output,\n\t\t\t    !(IP6CB(skb)->flags & IP6SKB_REROUTED));\n}\n\n/*\n *\txmit an sk_buff (used by TCP, SCTP and DCCP)\n */\n\nint ip6_xmit(struct sock *sk, struct sk_buff *skb, struct flowi6 *fl6,\n\t     struct ipv6_txoptions *opt)\n{\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct in6_addr *first_hop = &fl6->daddr;\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct ipv6hdr *hdr;\n\tu8  proto = fl6->flowi6_proto;\n\tint seg_len = skb->len;\n\tint hlimit = -1;\n\tint tclass = 0;\n\tu32 mtu;\n\n\tif (opt) {\n\t\tunsigned int head_room;\n\n\t\t/* First: exthdrs may take lots of space (~8K for now)\n\t\t   MAX_HEADER is not enough.\n\t\t */\n\t\thead_room = opt->opt_nflen + opt->opt_flen;\n\t\tseg_len += head_room;\n\t\thead_room += sizeof(struct ipv6hdr) + LL_RESERVED_SPACE(dst->dev);\n\n\t\tif (skb_headroom(skb) < head_room) {\n\t\t\tstruct sk_buff *skb2 = skb_realloc_headroom(skb, head_room);\n\t\t\tif (skb2 == NULL) {\n\t\t\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\treturn -ENOBUFS;\n\t\t\t}\n\t\t\tkfree_skb(skb);\n\t\t\tskb = skb2;\n\t\t\tskb_set_owner_w(skb, sk);\n\t\t}\n\t\tif (opt->opt_flen)\n\t\t\tipv6_push_frag_opts(skb, opt, &proto);\n\t\tif (opt->opt_nflen)\n\t\t\tipv6_push_nfrag_opts(skb, opt, &proto, &first_hop);\n\t}\n\n\tskb_push(skb, sizeof(struct ipv6hdr));\n\tskb_reset_network_header(skb);\n\thdr = ipv6_hdr(skb);\n\n\t/*\n\t *\tFill in the IPv6 header\n\t */\n\tif (np) {\n\t\ttclass = np->tclass;\n\t\thlimit = np->hop_limit;\n\t}\n\tif (hlimit < 0)\n\t\thlimit = ip6_dst_hoplimit(dst);\n\n\t*(__be32 *)hdr = htonl(0x60000000 | (tclass << 20)) | fl6->flowlabel;\n\n\thdr->payload_len = htons(seg_len);\n\thdr->nexthdr = proto;\n\thdr->hop_limit = hlimit;\n\n\tipv6_addr_copy(&hdr->saddr, &fl6->saddr);\n\tipv6_addr_copy(&hdr->daddr, first_hop);\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tmtu = dst_mtu(dst);\n\tif ((skb->len <= mtu) || skb->local_df || skb_is_gso(skb)) {\n\t\tIP6_UPD_PO_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t      IPSTATS_MIB_OUT, skb->len);\n\t\treturn NF_HOOK(NFPROTO_IPV6, NF_INET_LOCAL_OUT, skb, NULL,\n\t\t\t       dst->dev, dst_output);\n\t}\n\n\tif (net_ratelimit())\n\t\tprintk(KERN_DEBUG \"IPv6: sending pkt_too_big to self\\n\");\n\tskb->dev = dst->dev;\n\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)), IPSTATS_MIB_FRAGFAILS);\n\tkfree_skb(skb);\n\treturn -EMSGSIZE;\n}\n\nEXPORT_SYMBOL(ip6_xmit);\n\n/*\n *\tTo avoid extra problems ND packets are send through this\n *\troutine. It's code duplication but I really want to avoid\n *\textra checks since ipv6_build_header is used by TCP (which\n *\tis for us performance critical)\n */\n\nint ip6_nd_hdr(struct sock *sk, struct sk_buff *skb, struct net_device *dev,\n\t       const struct in6_addr *saddr, const struct in6_addr *daddr,\n\t       int proto, int len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct ipv6hdr *hdr;\n\n\tskb->protocol = htons(ETH_P_IPV6);\n\tskb->dev = dev;\n\n\tskb_reset_network_header(skb);\n\tskb_put(skb, sizeof(struct ipv6hdr));\n\thdr = ipv6_hdr(skb);\n\n\t*(__be32*)hdr = htonl(0x60000000);\n\n\thdr->payload_len = htons(len);\n\thdr->nexthdr = proto;\n\thdr->hop_limit = np->hop_limit;\n\n\tipv6_addr_copy(&hdr->saddr, saddr);\n\tipv6_addr_copy(&hdr->daddr, daddr);\n\n\treturn 0;\n}\n\nstatic int ip6_call_ra_chain(struct sk_buff *skb, int sel)\n{\n\tstruct ip6_ra_chain *ra;\n\tstruct sock *last = NULL;\n\n\tread_lock(&ip6_ra_lock);\n\tfor (ra = ip6_ra_chain; ra; ra = ra->next) {\n\t\tstruct sock *sk = ra->sk;\n\t\tif (sk && ra->sel == sel &&\n\t\t    (!sk->sk_bound_dev_if ||\n\t\t     sk->sk_bound_dev_if == skb->dev->ifindex)) {\n\t\t\tif (last) {\n\t\t\t\tstruct sk_buff *skb2 = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\tif (skb2)\n\t\t\t\t\trawv6_rcv(last, skb2);\n\t\t\t}\n\t\t\tlast = sk;\n\t\t}\n\t}\n\n\tif (last) {\n\t\trawv6_rcv(last, skb);\n\t\tread_unlock(&ip6_ra_lock);\n\t\treturn 1;\n\t}\n\tread_unlock(&ip6_ra_lock);\n\treturn 0;\n}\n\nstatic int ip6_forward_proxy_check(struct sk_buff *skb)\n{\n\tstruct ipv6hdr *hdr = ipv6_hdr(skb);\n\tu8 nexthdr = hdr->nexthdr;\n\tint offset;\n\n\tif (ipv6_ext_hdr(nexthdr)) {\n\t\toffset = ipv6_skip_exthdr(skb, sizeof(*hdr), &nexthdr);\n\t\tif (offset < 0)\n\t\t\treturn 0;\n\t} else\n\t\toffset = sizeof(struct ipv6hdr);\n\n\tif (nexthdr == IPPROTO_ICMPV6) {\n\t\tstruct icmp6hdr *icmp6;\n\n\t\tif (!pskb_may_pull(skb, (skb_network_header(skb) +\n\t\t\t\t\t offset + 1 - skb->data)))\n\t\t\treturn 0;\n\n\t\ticmp6 = (struct icmp6hdr *)(skb_network_header(skb) + offset);\n\n\t\tswitch (icmp6->icmp6_type) {\n\t\tcase NDISC_ROUTER_SOLICITATION:\n\t\tcase NDISC_ROUTER_ADVERTISEMENT:\n\t\tcase NDISC_NEIGHBOUR_SOLICITATION:\n\t\tcase NDISC_NEIGHBOUR_ADVERTISEMENT:\n\t\tcase NDISC_REDIRECT:\n\t\t\t/* For reaction involving unicast neighbor discovery\n\t\t\t * message destined to the proxied address, pass it to\n\t\t\t * input function.\n\t\t\t */\n\t\t\treturn 1;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t/*\n\t * The proxying router can't forward traffic sent to a link-local\n\t * address, so signal the sender and discard the packet. This\n\t * behavior is clarified by the MIPv6 specification.\n\t */\n\tif (ipv6_addr_type(&hdr->daddr) & IPV6_ADDR_LINKLOCAL) {\n\t\tdst_link_failure(skb);\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline int ip6_forward_finish(struct sk_buff *skb)\n{\n\treturn dst_output(skb);\n}\n\nint ip6_forward(struct sk_buff *skb)\n{\n\tstruct dst_entry *dst = skb_dst(skb);\n\tstruct ipv6hdr *hdr = ipv6_hdr(skb);\n\tstruct inet6_skb_parm *opt = IP6CB(skb);\n\tstruct net *net = dev_net(dst->dev);\n\tstruct neighbour *n;\n\tu32 mtu;\n\n\tif (net->ipv6.devconf_all->forwarding == 0)\n\t\tgoto error;\n\n\tif (skb_warn_if_lro(skb))\n\t\tgoto drop;\n\n\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_FWD, skb)) {\n\t\tIP6_INC_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_INDISCARDS);\n\t\tgoto drop;\n\t}\n\n\tif (skb->pkt_type != PACKET_HOST)\n\t\tgoto drop;\n\n\tskb_forward_csum(skb);\n\n\t/*\n\t *\tWe DO NOT make any processing on\n\t *\tRA packets, pushing them to user level AS IS\n\t *\twithout ane WARRANTY that application will be able\n\t *\tto interpret them. The reason is that we\n\t *\tcannot make anything clever here.\n\t *\n\t *\tWe are not end-node, so that if packet contains\n\t *\tAH/ESP, we cannot make anything.\n\t *\tDefragmentation also would be mistake, RA packets\n\t *\tcannot be fragmented, because there is no warranty\n\t *\tthat different fragments will go along one path. --ANK\n\t */\n\tif (opt->ra) {\n\t\tu8 *ptr = skb_network_header(skb) + opt->ra;\n\t\tif (ip6_call_ra_chain(skb, (ptr[2]<<8) + ptr[3]))\n\t\t\treturn 0;\n\t}\n\n\t/*\n\t *\tcheck and decrement ttl\n\t */\n\tif (hdr->hop_limit <= 1) {\n\t\t/* Force OUTPUT device used as source address */\n\t\tskb->dev = dst->dev;\n\t\ticmpv6_send(skb, ICMPV6_TIME_EXCEED, ICMPV6_EXC_HOPLIMIT, 0);\n\t\tIP6_INC_STATS_BH(net,\n\t\t\t\t ip6_dst_idev(dst), IPSTATS_MIB_INHDRERRORS);\n\n\t\tkfree_skb(skb);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\t/* XXX: idev->cnf.proxy_ndp? */\n\tif (net->ipv6.devconf_all->proxy_ndp &&\n\t    pneigh_lookup(&nd_tbl, net, &hdr->daddr, skb->dev, 0)) {\n\t\tint proxied = ip6_forward_proxy_check(skb);\n\t\tif (proxied > 0)\n\t\t\treturn ip6_input(skb);\n\t\telse if (proxied < 0) {\n\t\t\tIP6_INC_STATS(net, ip6_dst_idev(dst),\n\t\t\t\t      IPSTATS_MIB_INDISCARDS);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (!xfrm6_route_forward(skb)) {\n\t\tIP6_INC_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_INDISCARDS);\n\t\tgoto drop;\n\t}\n\tdst = skb_dst(skb);\n\n\t/* IPv6 specs say nothing about it, but it is clear that we cannot\n\t   send redirects to source routed frames.\n\t   We don't send redirects to frames decapsulated from IPsec.\n\t */\n\tn = dst_get_neighbour(dst);\n\tif (skb->dev == dst->dev && n && opt->srcrt == 0 && !skb_sec_path(skb)) {\n\t\tstruct in6_addr *target = NULL;\n\t\tstruct rt6_info *rt;\n\n\t\t/*\n\t\t *\tincoming and outgoing devices are the same\n\t\t *\tsend a redirect.\n\t\t */\n\n\t\trt = (struct rt6_info *) dst;\n\t\tif ((rt->rt6i_flags & RTF_GATEWAY))\n\t\t\ttarget = (struct in6_addr*)&n->primary_key;\n\t\telse\n\t\t\ttarget = &hdr->daddr;\n\n\t\tif (!rt->rt6i_peer)\n\t\t\trt6_bind_peer(rt, 1);\n\n\t\t/* Limit redirects both by destination (here)\n\t\t   and by source (inside ndisc_send_redirect)\n\t\t */\n\t\tif (inet_peer_xrlim_allow(rt->rt6i_peer, 1*HZ))\n\t\t\tndisc_send_redirect(skb, n, target);\n\t} else {\n\t\tint addrtype = ipv6_addr_type(&hdr->saddr);\n\n\t\t/* This check is security critical. */\n\t\tif (addrtype == IPV6_ADDR_ANY ||\n\t\t    addrtype & (IPV6_ADDR_MULTICAST | IPV6_ADDR_LOOPBACK))\n\t\t\tgoto error;\n\t\tif (addrtype & IPV6_ADDR_LINKLOCAL) {\n\t\t\ticmpv6_send(skb, ICMPV6_DEST_UNREACH,\n\t\t\t\t    ICMPV6_NOT_NEIGHBOUR, 0);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tmtu = dst_mtu(dst);\n\tif (mtu < IPV6_MIN_MTU)\n\t\tmtu = IPV6_MIN_MTU;\n\n\tif (skb->len > mtu && !skb_is_gso(skb)) {\n\t\t/* Again, force OUTPUT device used as source address */\n\t\tskb->dev = dst->dev;\n\t\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\t\tIP6_INC_STATS_BH(net,\n\t\t\t\t ip6_dst_idev(dst), IPSTATS_MIB_INTOOBIGERRORS);\n\t\tIP6_INC_STATS_BH(net,\n\t\t\t\t ip6_dst_idev(dst), IPSTATS_MIB_FRAGFAILS);\n\t\tkfree_skb(skb);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif (skb_cow(skb, dst->dev->hard_header_len)) {\n\t\tIP6_INC_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_OUTDISCARDS);\n\t\tgoto drop;\n\t}\n\n\thdr = ipv6_hdr(skb);\n\n\t/* Mangling hops number delayed to point after skb COW */\n\n\thdr->hop_limit--;\n\n\tIP6_INC_STATS_BH(net, ip6_dst_idev(dst), IPSTATS_MIB_OUTFORWDATAGRAMS);\n\treturn NF_HOOK(NFPROTO_IPV6, NF_INET_FORWARD, skb, skb->dev, dst->dev,\n\t\t       ip6_forward_finish);\n\nerror:\n\tIP6_INC_STATS_BH(net, ip6_dst_idev(dst), IPSTATS_MIB_INADDRERRORS);\ndrop:\n\tkfree_skb(skb);\n\treturn -EINVAL;\n}\n\nstatic void ip6_copy_metadata(struct sk_buff *to, struct sk_buff *from)\n{\n\tto->pkt_type = from->pkt_type;\n\tto->priority = from->priority;\n\tto->protocol = from->protocol;\n\tskb_dst_drop(to);\n\tskb_dst_set(to, dst_clone(skb_dst(from)));\n\tto->dev = from->dev;\n\tto->mark = from->mark;\n\n#ifdef CONFIG_NET_SCHED\n\tto->tc_index = from->tc_index;\n#endif\n\tnf_copy(to, from);\n#if defined(CONFIG_NETFILTER_XT_TARGET_TRACE) || \\\n    defined(CONFIG_NETFILTER_XT_TARGET_TRACE_MODULE)\n\tto->nf_trace = from->nf_trace;\n#endif\n\tskb_copy_secmark(to, from);\n}\n\nint ip6_find_1stfragopt(struct sk_buff *skb, u8 **nexthdr)\n{\n\tu16 offset = sizeof(struct ipv6hdr);\n\tstruct ipv6_opt_hdr *exthdr =\n\t\t\t\t(struct ipv6_opt_hdr *)(ipv6_hdr(skb) + 1);\n\tunsigned int packet_len = skb->tail - skb->network_header;\n\tint found_rhdr = 0;\n\t*nexthdr = &ipv6_hdr(skb)->nexthdr;\n\n\twhile (offset + 1 <= packet_len) {\n\n\t\tswitch (**nexthdr) {\n\n\t\tcase NEXTHDR_HOP:\n\t\t\tbreak;\n\t\tcase NEXTHDR_ROUTING:\n\t\t\tfound_rhdr = 1;\n\t\t\tbreak;\n\t\tcase NEXTHDR_DEST:\n#if defined(CONFIG_IPV6_MIP6) || defined(CONFIG_IPV6_MIP6_MODULE)\n\t\t\tif (ipv6_find_tlv(skb, offset, IPV6_TLV_HAO) >= 0)\n\t\t\t\tbreak;\n#endif\n\t\t\tif (found_rhdr)\n\t\t\t\treturn offset;\n\t\t\tbreak;\n\t\tdefault :\n\t\t\treturn offset;\n\t\t}\n\n\t\toffset += ipv6_optlen(exthdr);\n\t\t*nexthdr = &exthdr->nexthdr;\n\t\texthdr = (struct ipv6_opt_hdr *)(skb_network_header(skb) +\n\t\t\t\t\t\t offset);\n\t}\n\n\treturn offset;\n}\n\nvoid ipv6_select_ident(struct frag_hdr *fhdr, struct rt6_info *rt)\n{\n\tstatic atomic_t ipv6_fragmentation_id;\n\tint old, new;\n\n\tif (rt) {\n\t\tstruct inet_peer *peer;\n\n\t\tif (!rt->rt6i_peer)\n\t\t\trt6_bind_peer(rt, 1);\n\t\tpeer = rt->rt6i_peer;\n\t\tif (peer) {\n\t\t\tfhdr->identification = htonl(inet_getid(peer, 0));\n\t\t\treturn;\n\t\t}\n\t}\n\tdo {\n\t\told = atomic_read(&ipv6_fragmentation_id);\n\t\tnew = old + 1;\n\t\tif (!new)\n\t\t\tnew = 1;\n\t} while (atomic_cmpxchg(&ipv6_fragmentation_id, old, new) != old);\n\tfhdr->identification = htonl(new);\n}\n\nint ip6_fragment(struct sk_buff *skb, int (*output)(struct sk_buff *))\n{\n\tstruct sk_buff *frag;\n\tstruct rt6_info *rt = (struct rt6_info*)skb_dst(skb);\n\tstruct ipv6_pinfo *np = skb->sk ? inet6_sk(skb->sk) : NULL;\n\tstruct ipv6hdr *tmp_hdr;\n\tstruct frag_hdr *fh;\n\tunsigned int mtu, hlen, left, len;\n\t__be32 frag_id = 0;\n\tint ptr, offset = 0, err=0;\n\tu8 *prevhdr, nexthdr = 0;\n\tstruct net *net = dev_net(skb_dst(skb)->dev);\n\n\thlen = ip6_find_1stfragopt(skb, &prevhdr);\n\tnexthdr = *prevhdr;\n\n\tmtu = ip6_skb_dst_mtu(skb);\n\n\t/* We must not fragment if the socket is set to force MTU discovery\n\t * or if the skb it not generated by a local socket.\n\t */\n\tif (!skb->local_df && skb->len > mtu) {\n\t\tskb->dev = skb_dst(skb)->dev;\n\t\ticmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);\n\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t      IPSTATS_MIB_FRAGFAILS);\n\t\tkfree_skb(skb);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tif (np && np->frag_size < mtu) {\n\t\tif (np->frag_size)\n\t\t\tmtu = np->frag_size;\n\t}\n\tmtu -= hlen + sizeof(struct frag_hdr);\n\n\tif (skb_has_frag_list(skb)) {\n\t\tint first_len = skb_pagelen(skb);\n\t\tstruct sk_buff *frag2;\n\n\t\tif (first_len - hlen > mtu ||\n\t\t    ((first_len - hlen) & 7) ||\n\t\t    skb_cloned(skb))\n\t\t\tgoto slow_path;\n\n\t\tskb_walk_frags(skb, frag) {\n\t\t\t/* Correct geometry. */\n\t\t\tif (frag->len > mtu ||\n\t\t\t    ((frag->len & 7) && frag->next) ||\n\t\t\t    skb_headroom(frag) < hlen)\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\t/* Partially cloned skb? */\n\t\t\tif (skb_shared(frag))\n\t\t\t\tgoto slow_path_clean;\n\n\t\t\tBUG_ON(frag->sk);\n\t\t\tif (skb->sk) {\n\t\t\t\tfrag->sk = skb->sk;\n\t\t\t\tfrag->destructor = sock_wfree;\n\t\t\t}\n\t\t\tskb->truesize -= frag->truesize;\n\t\t}\n\n\t\terr = 0;\n\t\toffset = 0;\n\t\tfrag = skb_shinfo(skb)->frag_list;\n\t\tskb_frag_list_init(skb);\n\t\t/* BUILD HEADER */\n\n\t\t*prevhdr = NEXTHDR_FRAGMENT;\n\t\ttmp_hdr = kmemdup(skb_network_header(skb), hlen, GFP_ATOMIC);\n\t\tif (!tmp_hdr) {\n\t\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t      IPSTATS_MIB_FRAGFAILS);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\t__skb_pull(skb, hlen);\n\t\tfh = (struct frag_hdr*)__skb_push(skb, sizeof(struct frag_hdr));\n\t\t__skb_push(skb, hlen);\n\t\tskb_reset_network_header(skb);\n\t\tmemcpy(skb_network_header(skb), tmp_hdr, hlen);\n\n\t\tipv6_select_ident(fh, rt);\n\t\tfh->nexthdr = nexthdr;\n\t\tfh->reserved = 0;\n\t\tfh->frag_off = htons(IP6_MF);\n\t\tfrag_id = fh->identification;\n\n\t\tfirst_len = skb_pagelen(skb);\n\t\tskb->data_len = first_len - skb_headlen(skb);\n\t\tskb->len = first_len;\n\t\tipv6_hdr(skb)->payload_len = htons(first_len -\n\t\t\t\t\t\t   sizeof(struct ipv6hdr));\n\n\t\tdst_hold(&rt->dst);\n\n\t\tfor (;;) {\n\t\t\t/* Prepare header of the next frame,\n\t\t\t * before previous one went down. */\n\t\t\tif (frag) {\n\t\t\t\tfrag->ip_summed = CHECKSUM_NONE;\n\t\t\t\tskb_reset_transport_header(frag);\n\t\t\t\tfh = (struct frag_hdr*)__skb_push(frag, sizeof(struct frag_hdr));\n\t\t\t\t__skb_push(frag, hlen);\n\t\t\t\tskb_reset_network_header(frag);\n\t\t\t\tmemcpy(skb_network_header(frag), tmp_hdr,\n\t\t\t\t       hlen);\n\t\t\t\toffset += skb->len - hlen - sizeof(struct frag_hdr);\n\t\t\t\tfh->nexthdr = nexthdr;\n\t\t\t\tfh->reserved = 0;\n\t\t\t\tfh->frag_off = htons(offset);\n\t\t\t\tif (frag->next != NULL)\n\t\t\t\t\tfh->frag_off |= htons(IP6_MF);\n\t\t\t\tfh->identification = frag_id;\n\t\t\t\tipv6_hdr(frag)->payload_len =\n\t\t\t\t\t\thtons(frag->len -\n\t\t\t\t\t\t      sizeof(struct ipv6hdr));\n\t\t\t\tip6_copy_metadata(frag, skb);\n\t\t\t}\n\n\t\t\terr = output(skb);\n\t\t\tif(!err)\n\t\t\t\tIP6_INC_STATS(net, ip6_dst_idev(&rt->dst),\n\t\t\t\t\t      IPSTATS_MIB_FRAGCREATES);\n\n\t\t\tif (err || !frag)\n\t\t\t\tbreak;\n\n\t\t\tskb = frag;\n\t\t\tfrag = skb->next;\n\t\t\tskb->next = NULL;\n\t\t}\n\n\t\tkfree(tmp_hdr);\n\n\t\tif (err == 0) {\n\t\t\tIP6_INC_STATS(net, ip6_dst_idev(&rt->dst),\n\t\t\t\t      IPSTATS_MIB_FRAGOKS);\n\t\t\tdst_release(&rt->dst);\n\t\t\treturn 0;\n\t\t}\n\n\t\twhile (frag) {\n\t\t\tskb = frag->next;\n\t\t\tkfree_skb(frag);\n\t\t\tfrag = skb;\n\t\t}\n\n\t\tIP6_INC_STATS(net, ip6_dst_idev(&rt->dst),\n\t\t\t      IPSTATS_MIB_FRAGFAILS);\n\t\tdst_release(&rt->dst);\n\t\treturn err;\n\nslow_path_clean:\n\t\tskb_walk_frags(skb, frag2) {\n\t\t\tif (frag2 == frag)\n\t\t\t\tbreak;\n\t\t\tfrag2->sk = NULL;\n\t\t\tfrag2->destructor = NULL;\n\t\t\tskb->truesize += frag2->truesize;\n\t\t}\n\t}\n\nslow_path:\n\tleft = skb->len - hlen;\t\t/* Space per frame */\n\tptr = hlen;\t\t\t/* Where to start from */\n\n\t/*\n\t *\tFragment the datagram.\n\t */\n\n\t*prevhdr = NEXTHDR_FRAGMENT;\n\n\t/*\n\t *\tKeep copying data until we run out.\n\t */\n\twhile(left > 0)\t{\n\t\tlen = left;\n\t\t/* IF: it doesn't fit, use 'mtu' - the data space left */\n\t\tif (len > mtu)\n\t\t\tlen = mtu;\n\t\t/* IF: we are not sending up to and including the packet end\n\t\t   then align the next start on an eight byte boundary */\n\t\tif (len < left)\t{\n\t\t\tlen &= ~7;\n\t\t}\n\t\t/*\n\t\t *\tAllocate buffer.\n\t\t */\n\n\t\tif ((frag = alloc_skb(len+hlen+sizeof(struct frag_hdr)+LL_ALLOCATED_SPACE(rt->dst.dev), GFP_ATOMIC)) == NULL) {\n\t\t\tNETDEBUG(KERN_INFO \"IPv6: frag: no memory for new fragment!\\n\");\n\t\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t      IPSTATS_MIB_FRAGFAILS);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\n\t\t/*\n\t\t *\tSet up data on packet\n\t\t */\n\n\t\tip6_copy_metadata(frag, skb);\n\t\tskb_reserve(frag, LL_RESERVED_SPACE(rt->dst.dev));\n\t\tskb_put(frag, len + hlen + sizeof(struct frag_hdr));\n\t\tskb_reset_network_header(frag);\n\t\tfh = (struct frag_hdr *)(skb_network_header(frag) + hlen);\n\t\tfrag->transport_header = (frag->network_header + hlen +\n\t\t\t\t\t  sizeof(struct frag_hdr));\n\n\t\t/*\n\t\t *\tCharge the memory for the fragment to any owner\n\t\t *\tit might possess\n\t\t */\n\t\tif (skb->sk)\n\t\t\tskb_set_owner_w(frag, skb->sk);\n\n\t\t/*\n\t\t *\tCopy the packet header into the new buffer.\n\t\t */\n\t\tskb_copy_from_linear_data(skb, skb_network_header(frag), hlen);\n\n\t\t/*\n\t\t *\tBuild fragment header.\n\t\t */\n\t\tfh->nexthdr = nexthdr;\n\t\tfh->reserved = 0;\n\t\tif (!frag_id) {\n\t\t\tipv6_select_ident(fh, rt);\n\t\t\tfrag_id = fh->identification;\n\t\t} else\n\t\t\tfh->identification = frag_id;\n\n\t\t/*\n\t\t *\tCopy a block of the IP datagram.\n\t\t */\n\t\tif (skb_copy_bits(skb, ptr, skb_transport_header(frag), len))\n\t\t\tBUG();\n\t\tleft -= len;\n\n\t\tfh->frag_off = htons(offset);\n\t\tif (left > 0)\n\t\t\tfh->frag_off |= htons(IP6_MF);\n\t\tipv6_hdr(frag)->payload_len = htons(frag->len -\n\t\t\t\t\t\t    sizeof(struct ipv6hdr));\n\n\t\tptr += len;\n\t\toffset += len;\n\n\t\t/*\n\t\t *\tPut this fragment into the sending queue.\n\t\t */\n\t\terr = output(frag);\n\t\tif (err)\n\t\t\tgoto fail;\n\n\t\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t\t      IPSTATS_MIB_FRAGCREATES);\n\t}\n\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t      IPSTATS_MIB_FRAGOKS);\n\tkfree_skb(skb);\n\treturn err;\n\nfail:\n\tIP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),\n\t\t      IPSTATS_MIB_FRAGFAILS);\n\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic inline int ip6_rt_check(const struct rt6key *rt_key,\n\t\t\t       const struct in6_addr *fl_addr,\n\t\t\t       const struct in6_addr *addr_cache)\n{\n\treturn (rt_key->plen != 128 || !ipv6_addr_equal(fl_addr, &rt_key->addr)) &&\n\t\t(addr_cache == NULL || !ipv6_addr_equal(fl_addr, addr_cache));\n}\n\nstatic struct dst_entry *ip6_sk_dst_check(struct sock *sk,\n\t\t\t\t\t  struct dst_entry *dst,\n\t\t\t\t\t  const struct flowi6 *fl6)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct rt6_info *rt = (struct rt6_info *)dst;\n\n\tif (!dst)\n\t\tgoto out;\n\n\t/* Yes, checking route validity in not connected\n\t * case is not very simple. Take into account,\n\t * that we do not support routing by source, TOS,\n\t * and MSG_DONTROUTE \t\t--ANK (980726)\n\t *\n\t * 1. ip6_rt_check(): If route was host route,\n\t *    check that cached destination is current.\n\t *    If it is network route, we still may\n\t *    check its validity using saved pointer\n\t *    to the last used address: daddr_cache.\n\t *    We do not want to save whole address now,\n\t *    (because main consumer of this service\n\t *    is tcp, which has not this problem),\n\t *    so that the last trick works only on connected\n\t *    sockets.\n\t * 2. oif also should be the same.\n\t */\n\tif (ip6_rt_check(&rt->rt6i_dst, &fl6->daddr, np->daddr_cache) ||\n#ifdef CONFIG_IPV6_SUBTREES\n\t    ip6_rt_check(&rt->rt6i_src, &fl6->saddr, np->saddr_cache) ||\n#endif\n\t    (fl6->flowi6_oif && fl6->flowi6_oif != dst->dev->ifindex)) {\n\t\tdst_release(dst);\n\t\tdst = NULL;\n\t}\n\nout:\n\treturn dst;\n}\n\nstatic int ip6_dst_lookup_tail(struct sock *sk,\n\t\t\t       struct dst_entry **dst, struct flowi6 *fl6)\n{\n\tstruct net *net = sock_net(sk);\n#ifdef CONFIG_IPV6_OPTIMISTIC_DAD\n\tstruct neighbour *n;\n#endif\n\tint err;\n\n\tif (*dst == NULL)\n\t\t*dst = ip6_route_output(net, sk, fl6);\n\n\tif ((err = (*dst)->error))\n\t\tgoto out_err_release;\n\n\tif (ipv6_addr_any(&fl6->saddr)) {\n\t\tstruct rt6_info *rt = (struct rt6_info *) *dst;\n\t\terr = ip6_route_get_saddr(net, rt, &fl6->daddr,\n\t\t\t\t\t  sk ? inet6_sk(sk)->srcprefs : 0,\n\t\t\t\t\t  &fl6->saddr);\n\t\tif (err)\n\t\t\tgoto out_err_release;\n\t}\n\n#ifdef CONFIG_IPV6_OPTIMISTIC_DAD\n\t/*\n\t * Here if the dst entry we've looked up\n\t * has a neighbour entry that is in the INCOMPLETE\n\t * state and the src address from the flow is\n\t * marked as OPTIMISTIC, we release the found\n\t * dst entry and replace it instead with the\n\t * dst entry of the nexthop router\n\t */\n\tn = dst_get_neighbour(*dst);\n\tif (n && !(n->nud_state & NUD_VALID)) {\n\t\tstruct inet6_ifaddr *ifp;\n\t\tstruct flowi6 fl_gw6;\n\t\tint redirect;\n\n\t\tifp = ipv6_get_ifaddr(net, &fl6->saddr,\n\t\t\t\t      (*dst)->dev, 1);\n\n\t\tredirect = (ifp && ifp->flags & IFA_F_OPTIMISTIC);\n\t\tif (ifp)\n\t\t\tin6_ifa_put(ifp);\n\n\t\tif (redirect) {\n\t\t\t/*\n\t\t\t * We need to get the dst entry for the\n\t\t\t * default router instead\n\t\t\t */\n\t\t\tdst_release(*dst);\n\t\t\tmemcpy(&fl_gw6, fl6, sizeof(struct flowi6));\n\t\t\tmemset(&fl_gw6.daddr, 0, sizeof(struct in6_addr));\n\t\t\t*dst = ip6_route_output(net, sk, &fl_gw6);\n\t\t\tif ((err = (*dst)->error))\n\t\t\t\tgoto out_err_release;\n\t\t}\n\t}\n#endif\n\n\treturn 0;\n\nout_err_release:\n\tif (err == -ENETUNREACH)\n\t\tIP6_INC_STATS_BH(net, NULL, IPSTATS_MIB_OUTNOROUTES);\n\tdst_release(*dst);\n\t*dst = NULL;\n\treturn err;\n}\n\n/**\n *\tip6_dst_lookup - perform route lookup on flow\n *\t@sk: socket which provides route info\n *\t@dst: pointer to dst_entry * for result\n *\t@fl6: flow to lookup\n *\n *\tThis function performs a route lookup on the given flow.\n *\n *\tIt returns zero on success, or a standard errno code on error.\n */\nint ip6_dst_lookup(struct sock *sk, struct dst_entry **dst, struct flowi6 *fl6)\n{\n\t*dst = NULL;\n\treturn ip6_dst_lookup_tail(sk, dst, fl6);\n}\nEXPORT_SYMBOL_GPL(ip6_dst_lookup);\n\n/**\n *\tip6_dst_lookup_flow - perform route lookup on flow with ipsec\n *\t@sk: socket which provides route info\n *\t@fl6: flow to lookup\n *\t@final_dst: final destination address for ipsec lookup\n *\t@can_sleep: we are in a sleepable context\n *\n *\tThis function performs a route lookup on the given flow.\n *\n *\tIt returns a valid dst pointer on success, or a pointer encoded\n *\terror code.\n */\nstruct dst_entry *ip6_dst_lookup_flow(struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t      const struct in6_addr *final_dst,\n\t\t\t\t      bool can_sleep)\n{\n\tstruct dst_entry *dst = NULL;\n\tint err;\n\n\terr = ip6_dst_lookup_tail(sk, &dst, fl6);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\tif (final_dst)\n\t\tipv6_addr_copy(&fl6->daddr, final_dst);\n\tif (can_sleep)\n\t\tfl6->flowi6_flags |= FLOWI_FLAG_CAN_SLEEP;\n\n\treturn xfrm_lookup(sock_net(sk), dst, flowi6_to_flowi(fl6), sk, 0);\n}\nEXPORT_SYMBOL_GPL(ip6_dst_lookup_flow);\n\n/**\n *\tip6_sk_dst_lookup_flow - perform socket cached route lookup on flow\n *\t@sk: socket which provides the dst cache and route info\n *\t@fl6: flow to lookup\n *\t@final_dst: final destination address for ipsec lookup\n *\t@can_sleep: we are in a sleepable context\n *\n *\tThis function performs a route lookup on the given flow with the\n *\tpossibility of using the cached route in the socket if it is valid.\n *\tIt will take the socket dst lock when operating on the dst cache.\n *\tAs a result, this function can only be used in process context.\n *\n *\tIt returns a valid dst pointer on success, or a pointer encoded\n *\terror code.\n */\nstruct dst_entry *ip6_sk_dst_lookup_flow(struct sock *sk, struct flowi6 *fl6,\n\t\t\t\t\t const struct in6_addr *final_dst,\n\t\t\t\t\t bool can_sleep)\n{\n\tstruct dst_entry *dst = sk_dst_check(sk, inet6_sk(sk)->dst_cookie);\n\tint err;\n\n\tdst = ip6_sk_dst_check(sk, dst, fl6);\n\n\terr = ip6_dst_lookup_tail(sk, &dst, fl6);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\tif (final_dst)\n\t\tipv6_addr_copy(&fl6->daddr, final_dst);\n\tif (can_sleep)\n\t\tfl6->flowi6_flags |= FLOWI_FLAG_CAN_SLEEP;\n\n\treturn xfrm_lookup(sock_net(sk), dst, flowi6_to_flowi(fl6), sk, 0);\n}\nEXPORT_SYMBOL_GPL(ip6_sk_dst_lookup_flow);\n\nstatic inline int ip6_ufo_append_data(struct sock *sk,\n\t\t\tint getfrag(void *from, char *to, int offset, int len,\n\t\t\tint odd, struct sk_buff *skb),\n\t\t\tvoid *from, int length, int hh_len, int fragheaderlen,\n\t\t\tint transhdrlen, int mtu,unsigned int flags,\n\t\t\tstruct rt6_info *rt)\n\n{\n\tstruct sk_buff *skb;\n\tint err;\n\n\t/* There is support for UDP large send offload by network\n\t * device, so create one single skb packet containing complete\n\t * udp datagram\n\t */\n\tif ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL) {\n\t\tskb = sock_alloc_send_skb(sk,\n\t\t\thh_len + fragheaderlen + transhdrlen + 20,\n\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\tif (skb == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\t/* reserve space for Hardware header */\n\t\tskb_reserve(skb, hh_len);\n\n\t\t/* create space for UDP/IP header */\n\t\tskb_put(skb,fragheaderlen + transhdrlen);\n\n\t\t/* initialize network header pointer */\n\t\tskb_reset_network_header(skb);\n\n\t\t/* initialize protocol header pointer */\n\t\tskb->transport_header = skb->network_header + fragheaderlen;\n\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\tskb->csum = 0;\n\t}\n\n\terr = skb_append_datato_frags(sk,skb, getfrag, from,\n\t\t\t\t      (length - transhdrlen));\n\tif (!err) {\n\t\tstruct frag_hdr fhdr;\n\n\t\t/* Specify the length of each IPv6 datagram fragment.\n\t\t * It has to be a multiple of 8.\n\t\t */\n\t\tskb_shinfo(skb)->gso_size = (mtu - fragheaderlen -\n\t\t\t\t\t     sizeof(struct frag_hdr)) & ~7;\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_UDP;\n\t\tipv6_select_ident(&fhdr, rt);\n\t\tskb_shinfo(skb)->ip6_frag_id = fhdr.identification;\n\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\n\t\treturn 0;\n\t}\n\t/* There is not enough support do UPD LSO,\n\t * so follow normal path\n\t */\n\tkfree_skb(skb);\n\n\treturn err;\n}\n\nstatic inline struct ipv6_opt_hdr *ip6_opt_dup(struct ipv6_opt_hdr *src,\n\t\t\t\t\t       gfp_t gfp)\n{\n\treturn src ? kmemdup(src, (src->hdrlen + 1) * 8, gfp) : NULL;\n}\n\nstatic inline struct ipv6_rt_hdr *ip6_rthdr_dup(struct ipv6_rt_hdr *src,\n\t\t\t\t\t\tgfp_t gfp)\n{\n\treturn src ? kmemdup(src, (src->hdrlen + 1) * 8, gfp) : NULL;\n}\n\nint ip6_append_data(struct sock *sk, int getfrag(void *from, char *to,\n\tint offset, int len, int odd, struct sk_buff *skb),\n\tvoid *from, int length, int transhdrlen,\n\tint hlimit, int tclass, struct ipv6_txoptions *opt, struct flowi6 *fl6,\n\tstruct rt6_info *rt, unsigned int flags, int dontfrag)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_cork *cork;\n\tstruct sk_buff *skb;\n\tunsigned int maxfraglen, fragheaderlen;\n\tint exthdrlen;\n\tint hh_len;\n\tint mtu;\n\tint copy;\n\tint err;\n\tint offset = 0;\n\tint csummode = CHECKSUM_NONE;\n\t__u8 tx_flags = 0;\n\n\tif (flags&MSG_PROBE)\n\t\treturn 0;\n\tcork = &inet->cork.base;\n\tif (skb_queue_empty(&sk->sk_write_queue)) {\n\t\t/*\n\t\t * setup for corking\n\t\t */\n\t\tif (opt) {\n\t\t\tif (WARN_ON(np->cork.opt))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tnp->cork.opt = kmalloc(opt->tot_len, sk->sk_allocation);\n\t\t\tif (unlikely(np->cork.opt == NULL))\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\tnp->cork.opt->tot_len = opt->tot_len;\n\t\t\tnp->cork.opt->opt_flen = opt->opt_flen;\n\t\t\tnp->cork.opt->opt_nflen = opt->opt_nflen;\n\n\t\t\tnp->cork.opt->dst0opt = ip6_opt_dup(opt->dst0opt,\n\t\t\t\t\t\t\t    sk->sk_allocation);\n\t\t\tif (opt->dst0opt && !np->cork.opt->dst0opt)\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\tnp->cork.opt->dst1opt = ip6_opt_dup(opt->dst1opt,\n\t\t\t\t\t\t\t    sk->sk_allocation);\n\t\t\tif (opt->dst1opt && !np->cork.opt->dst1opt)\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\tnp->cork.opt->hopopt = ip6_opt_dup(opt->hopopt,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\tif (opt->hopopt && !np->cork.opt->hopopt)\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\tnp->cork.opt->srcrt = ip6_rthdr_dup(opt->srcrt,\n\t\t\t\t\t\t\t    sk->sk_allocation);\n\t\t\tif (opt->srcrt && !np->cork.opt->srcrt)\n\t\t\t\treturn -ENOBUFS;\n\n\t\t\t/* need source address above miyazawa*/\n\t\t}\n\t\tdst_hold(&rt->dst);\n\t\tcork->dst = &rt->dst;\n\t\tinet->cork.fl.u.ip6 = *fl6;\n\t\tnp->cork.hop_limit = hlimit;\n\t\tnp->cork.tclass = tclass;\n\t\tmtu = np->pmtudisc == IPV6_PMTUDISC_PROBE ?\n\t\t      rt->dst.dev->mtu : dst_mtu(rt->dst.path);\n\t\tif (np->frag_size < mtu) {\n\t\t\tif (np->frag_size)\n\t\t\t\tmtu = np->frag_size;\n\t\t}\n\t\tcork->fragsize = mtu;\n\t\tif (dst_allfrag(rt->dst.path))\n\t\t\tcork->flags |= IPCORK_ALLFRAG;\n\t\tcork->length = 0;\n\t\tsk->sk_sndmsg_page = NULL;\n\t\tsk->sk_sndmsg_off = 0;\n\t\texthdrlen = rt->dst.header_len + (opt ? opt->opt_flen : 0) -\n\t\t\t    rt->rt6i_nfheader_len;\n\t\tlength += exthdrlen;\n\t\ttranshdrlen += exthdrlen;\n\t} else {\n\t\trt = (struct rt6_info *)cork->dst;\n\t\tfl6 = &inet->cork.fl.u.ip6;\n\t\topt = np->cork.opt;\n\t\ttranshdrlen = 0;\n\t\texthdrlen = 0;\n\t\tmtu = cork->fragsize;\n\t}\n\n\thh_len = LL_RESERVED_SPACE(rt->dst.dev);\n\n\tfragheaderlen = sizeof(struct ipv6hdr) + rt->rt6i_nfheader_len +\n\t\t\t(opt ? opt->opt_nflen : 0);\n\tmaxfraglen = ((mtu - fragheaderlen) & ~7) + fragheaderlen - sizeof(struct frag_hdr);\n\n\tif (mtu <= sizeof(struct ipv6hdr) + IPV6_MAXPLEN) {\n\t\tif (cork->length + length > sizeof(struct ipv6hdr) + IPV6_MAXPLEN - fragheaderlen) {\n\t\t\tipv6_local_error(sk, EMSGSIZE, fl6, mtu-exthdrlen);\n\t\t\treturn -EMSGSIZE;\n\t\t}\n\t}\n\n\t/* For UDP, check if TX timestamp is enabled */\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\terr = sock_tx_timestamp(sk, &tx_flags);\n\t\tif (err)\n\t\t\tgoto error;\n\t}\n\n\t/*\n\t * Let's try using as much space as possible.\n\t * Use MTU if total length of the message fits into the MTU.\n\t * Otherwise, we need to reserve fragment header and\n\t * fragment alignment (= 8-15 octects, in total).\n\t *\n\t * Note that we may need to \"move\" the data from the tail of\n\t * of the buffer to the new fragment when we split\n\t * the message.\n\t *\n\t * FIXME: It may be fragmented into multiple chunks\n\t *        at once if non-fragmentable extension headers\n\t *        are too large.\n\t * --yoshfuji\n\t */\n\n\tcork->length += length;\n\tif (length > mtu) {\n\t\tint proto = sk->sk_protocol;\n\t\tif (dontfrag && (proto == IPPROTO_UDP || proto == IPPROTO_RAW)){\n\t\t\tipv6_local_rxpmtu(sk, fl6, mtu-exthdrlen);\n\t\t\treturn -EMSGSIZE;\n\t\t}\n\n\t\tif (proto == IPPROTO_UDP &&\n\t\t    (rt->dst.dev->features & NETIF_F_UFO)) {\n\n\t\t\terr = ip6_ufo_append_data(sk, getfrag, from, length,\n\t\t\t\t\t\t  hh_len, fragheaderlen,\n\t\t\t\t\t\t  transhdrlen, mtu, flags, rt);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL)\n\t\tgoto alloc_new_skb;\n\n\twhile (length > 0) {\n\t\t/* Check if the remaining data fits into current packet. */\n\t\tcopy = (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - skb->len;\n\t\tif (copy < length)\n\t\t\tcopy = maxfraglen - skb->len;\n\n\t\tif (copy <= 0) {\n\t\t\tchar *data;\n\t\t\tunsigned int datalen;\n\t\t\tunsigned int fraglen;\n\t\t\tunsigned int fraggap;\n\t\t\tunsigned int alloclen;\n\t\t\tstruct sk_buff *skb_prev;\nalloc_new_skb:\n\t\t\tskb_prev = skb;\n\n\t\t\t/* There's no room in the current skb */\n\t\t\tif (skb_prev)\n\t\t\t\tfraggap = skb_prev->len - maxfraglen;\n\t\t\telse\n\t\t\t\tfraggap = 0;\n\n\t\t\t/*\n\t\t\t * If remaining data exceeds the mtu,\n\t\t\t * we know we need more fragment(s).\n\t\t\t */\n\t\t\tdatalen = length + fraggap;\n\t\t\tif (datalen > (cork->length <= mtu && !(cork->flags & IPCORK_ALLFRAG) ? mtu : maxfraglen) - fragheaderlen)\n\t\t\t\tdatalen = maxfraglen - fragheaderlen;\n\n\t\t\tfraglen = datalen + fragheaderlen;\n\t\t\tif ((flags & MSG_MORE) &&\n\t\t\t    !(rt->dst.dev->features&NETIF_F_SG))\n\t\t\t\talloclen = mtu;\n\t\t\telse\n\t\t\t\talloclen = datalen + fragheaderlen;\n\n\t\t\t/*\n\t\t\t * The last fragment gets additional space at tail.\n\t\t\t * Note: we overallocate on fragments with MSG_MODE\n\t\t\t * because we have no idea if we're the last one.\n\t\t\t */\n\t\t\tif (datalen == length + fraggap)\n\t\t\t\talloclen += rt->dst.trailer_len;\n\n\t\t\t/*\n\t\t\t * We just reserve space for fragment header.\n\t\t\t * Note: this may be overallocation if the message\n\t\t\t * (without MSG_MORE) fits into the MTU.\n\t\t\t */\n\t\t\talloclen += sizeof(struct frag_hdr);\n\n\t\t\tif (transhdrlen) {\n\t\t\t\tskb = sock_alloc_send_skb(sk,\n\t\t\t\t\t\talloclen + hh_len,\n\t\t\t\t\t\t(flags & MSG_DONTWAIT), &err);\n\t\t\t} else {\n\t\t\t\tskb = NULL;\n\t\t\t\tif (atomic_read(&sk->sk_wmem_alloc) <=\n\t\t\t\t    2 * sk->sk_sndbuf)\n\t\t\t\t\tskb = sock_wmalloc(sk,\n\t\t\t\t\t\t\t   alloclen + hh_len, 1,\n\t\t\t\t\t\t\t   sk->sk_allocation);\n\t\t\t\tif (unlikely(skb == NULL))\n\t\t\t\t\terr = -ENOBUFS;\n\t\t\t\telse {\n\t\t\t\t\t/* Only the initial fragment\n\t\t\t\t\t * is time stamped.\n\t\t\t\t\t */\n\t\t\t\t\ttx_flags = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (skb == NULL)\n\t\t\t\tgoto error;\n\t\t\t/*\n\t\t\t *\tFill in the control structures\n\t\t\t */\n\t\t\tskb->ip_summed = csummode;\n\t\t\tskb->csum = 0;\n\t\t\t/* reserve for fragmentation */\n\t\t\tskb_reserve(skb, hh_len+sizeof(struct frag_hdr));\n\n\t\t\tif (sk->sk_type == SOCK_DGRAM)\n\t\t\t\tskb_shinfo(skb)->tx_flags = tx_flags;\n\n\t\t\t/*\n\t\t\t *\tFind where to start putting bytes\n\t\t\t */\n\t\t\tdata = skb_put(skb, fraglen);\n\t\t\tskb_set_network_header(skb, exthdrlen);\n\t\t\tdata += fragheaderlen;\n\t\t\tskb->transport_header = (skb->network_header +\n\t\t\t\t\t\t fragheaderlen);\n\t\t\tif (fraggap) {\n\t\t\t\tskb->csum = skb_copy_and_csum_bits(\n\t\t\t\t\tskb_prev, maxfraglen,\n\t\t\t\t\tdata + transhdrlen, fraggap, 0);\n\t\t\t\tskb_prev->csum = csum_sub(skb_prev->csum,\n\t\t\t\t\t\t\t  skb->csum);\n\t\t\t\tdata += fraggap;\n\t\t\t\tpskb_trim_unique(skb_prev, maxfraglen);\n\t\t\t}\n\t\t\tcopy = datalen - transhdrlen - fraggap;\n\t\t\tif (copy < 0) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t} else if (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\toffset += copy;\n\t\t\tlength -= datalen - fraggap;\n\t\t\ttranshdrlen = 0;\n\t\t\texthdrlen = 0;\n\t\t\tcsummode = CHECKSUM_NONE;\n\n\t\t\t/*\n\t\t\t * Put the packet on the pending queue\n\t\t\t */\n\t\t\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (copy > length)\n\t\t\tcopy = length;\n\n\t\tif (!(rt->dst.dev->features&NETIF_F_SG)) {\n\t\t\tunsigned int off;\n\n\t\t\toff = skb->len;\n\t\t\tif (getfrag(from, skb_put(skb, copy),\n\t\t\t\t\t\toffset, copy, off, skb) < 0) {\n\t\t\t\t__skb_trim(skb, off);\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else {\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\t\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i-1];\n\t\t\tstruct page *page = sk->sk_sndmsg_page;\n\t\t\tint off = sk->sk_sndmsg_off;\n\t\t\tunsigned int left;\n\n\t\t\tif (page && (left = PAGE_SIZE - off) > 0) {\n\t\t\t\tif (copy >= left)\n\t\t\t\t\tcopy = left;\n\t\t\t\tif (page != frag->page) {\n\t\t\t\t\tif (i == MAX_SKB_FRAGS) {\n\t\t\t\t\t\terr = -EMSGSIZE;\n\t\t\t\t\t\tgoto error;\n\t\t\t\t\t}\n\t\t\t\t\tget_page(page);\n\t\t\t\t\tskb_fill_page_desc(skb, i, page, sk->sk_sndmsg_off, 0);\n\t\t\t\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\t\t\t}\n\t\t\t} else if(i < MAX_SKB_FRAGS) {\n\t\t\t\tif (copy > PAGE_SIZE)\n\t\t\t\t\tcopy = PAGE_SIZE;\n\t\t\t\tpage = alloc_pages(sk->sk_allocation, 0);\n\t\t\t\tif (page == NULL) {\n\t\t\t\t\terr = -ENOMEM;\n\t\t\t\t\tgoto error;\n\t\t\t\t}\n\t\t\t\tsk->sk_sndmsg_page = page;\n\t\t\t\tsk->sk_sndmsg_off = 0;\n\n\t\t\t\tskb_fill_page_desc(skb, i, page, 0, 0);\n\t\t\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\t\t} else {\n\t\t\t\terr = -EMSGSIZE;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tif (getfrag(from, page_address(frag->page)+frag->page_offset+frag->size, offset, copy, skb->len, skb) < 0) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t\tsk->sk_sndmsg_off += copy;\n\t\t\tfrag->size += copy;\n\t\t\tskb->len += copy;\n\t\t\tskb->data_len += copy;\n\t\t\tskb->truesize += copy;\n\t\t\tatomic_add(copy, &sk->sk_wmem_alloc);\n\t\t}\n\t\toffset += copy;\n\t\tlength -= copy;\n\t}\n\treturn 0;\nerror:\n\tcork->length -= length;\n\tIP6_INC_STATS(sock_net(sk), rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\treturn err;\n}\n\nstatic void ip6_cork_release(struct inet_sock *inet, struct ipv6_pinfo *np)\n{\n\tif (np->cork.opt) {\n\t\tkfree(np->cork.opt->dst0opt);\n\t\tkfree(np->cork.opt->dst1opt);\n\t\tkfree(np->cork.opt->hopopt);\n\t\tkfree(np->cork.opt->srcrt);\n\t\tkfree(np->cork.opt);\n\t\tnp->cork.opt = NULL;\n\t}\n\n\tif (inet->cork.base.dst) {\n\t\tdst_release(inet->cork.base.dst);\n\t\tinet->cork.base.dst = NULL;\n\t\tinet->cork.base.flags &= ~IPCORK_ALLFRAG;\n\t}\n\tmemset(&inet->cork.fl, 0, sizeof(inet->cork.fl));\n}\n\nint ip6_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb, *tmp_skb;\n\tstruct sk_buff **tail_skb;\n\tstruct in6_addr final_dst_buf, *final_dst = &final_dst_buf;\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tstruct ipv6hdr *hdr;\n\tstruct ipv6_txoptions *opt = np->cork.opt;\n\tstruct rt6_info *rt = (struct rt6_info *)inet->cork.base.dst;\n\tstruct flowi6 *fl6 = &inet->cork.fl.u.ip6;\n\tunsigned char proto = fl6->flowi6_proto;\n\tint err = 0;\n\n\tif ((skb = __skb_dequeue(&sk->sk_write_queue)) == NULL)\n\t\tgoto out;\n\ttail_skb = &(skb_shinfo(skb)->frag_list);\n\n\t/* move skb->data to ip header from ext header */\n\tif (skb->data < skb_network_header(skb))\n\t\t__skb_pull(skb, skb_network_offset(skb));\n\twhile ((tmp_skb = __skb_dequeue(&sk->sk_write_queue)) != NULL) {\n\t\t__skb_pull(tmp_skb, skb_network_header_len(skb));\n\t\t*tail_skb = tmp_skb;\n\t\ttail_skb = &(tmp_skb->next);\n\t\tskb->len += tmp_skb->len;\n\t\tskb->data_len += tmp_skb->len;\n\t\tskb->truesize += tmp_skb->truesize;\n\t\ttmp_skb->destructor = NULL;\n\t\ttmp_skb->sk = NULL;\n\t}\n\n\t/* Allow local fragmentation. */\n\tif (np->pmtudisc < IPV6_PMTUDISC_DO)\n\t\tskb->local_df = 1;\n\n\tipv6_addr_copy(final_dst, &fl6->daddr);\n\t__skb_pull(skb, skb_network_header_len(skb));\n\tif (opt && opt->opt_flen)\n\t\tipv6_push_frag_opts(skb, opt, &proto);\n\tif (opt && opt->opt_nflen)\n\t\tipv6_push_nfrag_opts(skb, opt, &proto, &final_dst);\n\n\tskb_push(skb, sizeof(struct ipv6hdr));\n\tskb_reset_network_header(skb);\n\thdr = ipv6_hdr(skb);\n\n\t*(__be32*)hdr = fl6->flowlabel |\n\t\t     htonl(0x60000000 | ((int)np->cork.tclass << 20));\n\n\thdr->hop_limit = np->cork.hop_limit;\n\thdr->nexthdr = proto;\n\tipv6_addr_copy(&hdr->saddr, &fl6->saddr);\n\tipv6_addr_copy(&hdr->daddr, final_dst);\n\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tskb_dst_set(skb, dst_clone(&rt->dst));\n\tIP6_UPD_PO_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUT, skb->len);\n\tif (proto == IPPROTO_ICMPV6) {\n\t\tstruct inet6_dev *idev = ip6_dst_idev(skb_dst(skb));\n\n\t\tICMP6MSGOUT_INC_STATS_BH(net, idev, icmp6_hdr(skb)->icmp6_type);\n\t\tICMP6_INC_STATS_BH(net, idev, ICMP6_MIB_OUTMSGS);\n\t}\n\n\terr = ip6_local_out(skb);\n\tif (err) {\n\t\tif (err > 0)\n\t\t\terr = net_xmit_errno(err);\n\t\tif (err)\n\t\t\tgoto error;\n\t}\n\nout:\n\tip6_cork_release(inet, np);\n\treturn err;\nerror:\n\tIP6_INC_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);\n\tgoto out;\n}\n\nvoid ip6_flush_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue_tail(&sk->sk_write_queue)) != NULL) {\n\t\tif (skb_dst(skb))\n\t\t\tIP6_INC_STATS(sock_net(sk), ip6_dst_idev(skb_dst(skb)),\n\t\t\t\t      IPSTATS_MIB_OUTDISCARDS);\n\t\tkfree_skb(skb);\n\t}\n\n\tip6_cork_release(inet_sk(sk), inet6_sk(sk));\n}\n", "/*\n *\tUDP over IPv6\n *\tLinux INET6 implementation\n *\n *\tAuthors:\n *\tPedro Roque\t\t<roque@di.fc.ul.pt>\n *\n *\tBased on linux/ipv4/udp.c\n *\n *\tFixes:\n *\tHideaki YOSHIFUJI\t:\tsin6_scope_id support\n *\tYOSHIFUJI Hideaki @USAGI and:\tSupport IPV6_V6ONLY socket option, which\n *\tAlexey Kuznetsov\t\tallow both IPv4 and IPv6 sockets to bind\n *\t\t\t\t\ta single port at the same time.\n *      Kazunori MIYAZAWA @USAGI:       change process style to use ip6_append_data\n *      YOSHIFUJI Hideaki @USAGI:\tconvert /proc/net/udp6 to seq_file.\n *\n *\tThis program is free software; you can redistribute it and/or\n *      modify it under the terms of the GNU General Public License\n *      as published by the Free Software Foundation; either version\n *      2 of the License, or (at your option) any later version.\n */\n\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in6.h>\n#include <linux/netdevice.h>\n#include <linux/if_arp.h>\n#include <linux/ipv6.h>\n#include <linux/icmpv6.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n\n#include <net/ndisc.h>\n#include <net/protocol.h>\n#include <net/transp_v6.h>\n#include <net/ip6_route.h>\n#include <net/raw.h>\n#include <net/tcp_states.h>\n#include <net/ip6_checksum.h>\n#include <net/xfrm.h>\n\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include \"udp_impl.h\"\n\nint ipv6_rcv_saddr_equal(const struct sock *sk, const struct sock *sk2)\n{\n\tconst struct in6_addr *sk_rcv_saddr6 = &inet6_sk(sk)->rcv_saddr;\n\tconst struct in6_addr *sk2_rcv_saddr6 = inet6_rcv_saddr(sk2);\n\t__be32 sk1_rcv_saddr = sk_rcv_saddr(sk);\n\t__be32 sk2_rcv_saddr = sk_rcv_saddr(sk2);\n\tint sk_ipv6only = ipv6_only_sock(sk);\n\tint sk2_ipv6only = inet_v6_ipv6only(sk2);\n\tint addr_type = ipv6_addr_type(sk_rcv_saddr6);\n\tint addr_type2 = sk2_rcv_saddr6 ? ipv6_addr_type(sk2_rcv_saddr6) : IPV6_ADDR_MAPPED;\n\n\t/* if both are mapped, treat as IPv4 */\n\tif (addr_type == IPV6_ADDR_MAPPED && addr_type2 == IPV6_ADDR_MAPPED)\n\t\treturn (!sk2_ipv6only &&\n\t\t\t(!sk1_rcv_saddr || !sk2_rcv_saddr ||\n\t\t\t  sk1_rcv_saddr == sk2_rcv_saddr));\n\n\tif (addr_type2 == IPV6_ADDR_ANY &&\n\t    !(sk2_ipv6only && addr_type == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (addr_type == IPV6_ADDR_ANY &&\n\t    !(sk_ipv6only && addr_type2 == IPV6_ADDR_MAPPED))\n\t\treturn 1;\n\n\tif (sk2_rcv_saddr6 &&\n\t    ipv6_addr_equal(sk_rcv_saddr6, sk2_rcv_saddr6))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic unsigned int udp6_portaddr_hash(struct net *net,\n\t\t\t\t       const struct in6_addr *addr6,\n\t\t\t\t       unsigned int port)\n{\n\tunsigned int hash, mix = net_hash_mix(net);\n\n\tif (ipv6_addr_any(addr6))\n\t\thash = jhash_1word(0, mix);\n\telse if (ipv6_addr_v4mapped(addr6))\n\t\thash = jhash_1word((__force u32)addr6->s6_addr32[3], mix);\n\telse\n\t\thash = jhash2((__force u32 *)addr6->s6_addr32, 4, mix);\n\n\treturn hash ^ port;\n}\n\n\nint udp_v6_get_port(struct sock *sk, unsigned short snum)\n{\n\tunsigned int hash2_nulladdr =\n\t\tudp6_portaddr_hash(sock_net(sk), &in6addr_any, snum);\n\tunsigned int hash2_partial = \n\t\tudp6_portaddr_hash(sock_net(sk), &inet6_sk(sk)->rcv_saddr, 0);\n\n\t/* precompute partial secondary hash */\n\tudp_sk(sk)->udp_portaddr_hash = hash2_partial;\n\treturn udp_lib_get_port(sk, snum, ipv6_rcv_saddr_equal, hash2_nulladdr);\n}\n\nstatic void udp_v6_rehash(struct sock *sk)\n{\n\tu16 new_hash = udp6_portaddr_hash(sock_net(sk),\n\t\t\t\t\t  &inet6_sk(sk)->rcv_saddr,\n\t\t\t\t\t  inet_sk(sk)->inet_num);\n\n\tudp_lib_rehash(sk, new_hash);\n}\n\nstatic inline int compute_score(struct sock *sk, struct net *net,\n\t\t\t\tunsigned short hnum,\n\t\t\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\t\t\tconst struct in6_addr *daddr, __be16 dport,\n\t\t\t\tint dif)\n{\n\tint score = -1;\n\n\tif (net_eq(sock_net(sk), net) && udp_sk(sk)->udp_port_hash == hnum &&\n\t\t\tsk->sk_family == PF_INET6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tscore = 0;\n\t\tif (inet->inet_dport) {\n\t\t\tif (inet->inet_dport != sport)\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t\tif (!ipv6_addr_any(&np->rcv_saddr)) {\n\t\t\tif (!ipv6_addr_equal(&np->rcv_saddr, daddr))\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t\tif (!ipv6_addr_any(&np->daddr)) {\n\t\t\tif (!ipv6_addr_equal(&np->daddr, saddr))\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t\tif (sk->sk_bound_dev_if) {\n\t\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t}\n\treturn score;\n}\n\n#define SCORE2_MAX (1 + 1 + 1)\nstatic inline int compute_score2(struct sock *sk, struct net *net,\n\t\t\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\t\t\tconst struct in6_addr *daddr, unsigned short hnum,\n\t\t\t\tint dif)\n{\n\tint score = -1;\n\n\tif (net_eq(sock_net(sk), net) && udp_sk(sk)->udp_port_hash == hnum &&\n\t\t\tsk->sk_family == PF_INET6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\t\tstruct inet_sock *inet = inet_sk(sk);\n\n\t\tif (!ipv6_addr_equal(&np->rcv_saddr, daddr))\n\t\t\treturn -1;\n\t\tscore = 0;\n\t\tif (inet->inet_dport) {\n\t\t\tif (inet->inet_dport != sport)\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t\tif (!ipv6_addr_any(&np->daddr)) {\n\t\t\tif (!ipv6_addr_equal(&np->daddr, saddr))\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t\tif (sk->sk_bound_dev_if) {\n\t\t\tif (sk->sk_bound_dev_if != dif)\n\t\t\t\treturn -1;\n\t\t\tscore++;\n\t\t}\n\t}\n\treturn score;\n}\n\n\n/* called with read_rcu_lock() */\nstatic struct sock *udp6_lib_lookup2(struct net *net,\n\t\tconst struct in6_addr *saddr, __be16 sport,\n\t\tconst struct in6_addr *daddr, unsigned int hnum, int dif,\n\t\tstruct udp_hslot *hslot2, unsigned int slot2)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tint score, badness;\n\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tudp_portaddr_for_each_entry_rcu(sk, node, &hslot2->head) {\n\t\tscore = compute_score2(sk, net, saddr, sport,\n\t\t\t\t      daddr, hnum, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t\tif (score == SCORE2_MAX)\n\t\t\t\tgoto exact_match;\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot2)\n\t\tgoto begin;\n\n\tif (result) {\nexact_match:\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score2(result, net, saddr, sport,\n\t\t\t\t  daddr, hnum, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\treturn result;\n}\n\nstatic struct sock *__udp6_lib_lookup(struct net *net,\n\t\t\t\t      const struct in6_addr *saddr, __be16 sport,\n\t\t\t\t      const struct in6_addr *daddr, __be16 dport,\n\t\t\t\t      int dif, struct udp_table *udptable)\n{\n\tstruct sock *sk, *result;\n\tstruct hlist_nulls_node *node;\n\tunsigned short hnum = ntohs(dport);\n\tunsigned int hash2, slot2, slot = udp_hashfn(net, hnum, udptable->mask);\n\tstruct udp_hslot *hslot2, *hslot = &udptable->hash[slot];\n\tint score, badness;\n\n\trcu_read_lock();\n\tif (hslot->count > 10) {\n\t\thash2 = udp6_portaddr_hash(net, daddr, hnum);\n\t\tslot2 = hash2 & udptable->mask;\n\t\thslot2 = &udptable->hash2[slot2];\n\t\tif (hslot->count < hslot2->count)\n\t\t\tgoto begin;\n\n\t\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t  daddr, hnum, dif,\n\t\t\t\t\t  hslot2, slot2);\n\t\tif (!result) {\n\t\t\thash2 = udp6_portaddr_hash(net, &in6addr_any, hnum);\n\t\t\tslot2 = hash2 & udptable->mask;\n\t\t\thslot2 = &udptable->hash2[slot2];\n\t\t\tif (hslot->count < hslot2->count)\n\t\t\t\tgoto begin;\n\n\t\t\tresult = udp6_lib_lookup2(net, saddr, sport,\n\t\t\t\t\t\t  &in6addr_any, hnum, dif,\n\t\t\t\t\t\t  hslot2, slot2);\n\t\t}\n\t\trcu_read_unlock();\n\t\treturn result;\n\t}\nbegin:\n\tresult = NULL;\n\tbadness = -1;\n\tsk_nulls_for_each_rcu(sk, node, &hslot->head) {\n\t\tscore = compute_score(sk, net, hnum, saddr, sport, daddr, dport, dif);\n\t\tif (score > badness) {\n\t\t\tresult = sk;\n\t\t\tbadness = score;\n\t\t}\n\t}\n\t/*\n\t * if the nulls value we got at the end of this lookup is\n\t * not the expected one, we must restart lookup.\n\t * We probably met an item that was moved to another chain.\n\t */\n\tif (get_nulls_value(node) != slot)\n\t\tgoto begin;\n\n\tif (result) {\n\t\tif (unlikely(!atomic_inc_not_zero_hint(&result->sk_refcnt, 2)))\n\t\t\tresult = NULL;\n\t\telse if (unlikely(compute_score(result, net, hnum, saddr, sport,\n\t\t\t\t\tdaddr, dport, dif) < badness)) {\n\t\t\tsock_put(result);\n\t\t\tgoto begin;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn result;\n}\n\nstatic struct sock *__udp6_lib_lookup_skb(struct sk_buff *skb,\n\t\t\t\t\t  __be16 sport, __be16 dport,\n\t\t\t\t\t  struct udp_table *udptable)\n{\n\tstruct sock *sk;\n\tconst struct ipv6hdr *iph = ipv6_hdr(skb);\n\n\tif (unlikely(sk = skb_steal_sock(skb)))\n\t\treturn sk;\n\treturn __udp6_lib_lookup(dev_net(skb_dst(skb)->dev), &iph->saddr, sport,\n\t\t\t\t &iph->daddr, dport, inet6_iif(skb),\n\t\t\t\t udptable);\n}\n\nstruct sock *udp6_lib_lookup(struct net *net, const struct in6_addr *saddr, __be16 sport,\n\t\t\t     const struct in6_addr *daddr, __be16 dport, int dif)\n{\n\treturn __udp6_lib_lookup(net, saddr, sport, daddr, dport, dif, &udp_table);\n}\nEXPORT_SYMBOL_GPL(udp6_lib_lookup);\n\n\n/*\n * \tThis should be easy, if there is something there we\n * \treturn it, otherwise we block.\n */\n\nint udpv6_recvmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len,\n\t\t  int noblock, int flags, int *addr_len)\n{\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct sk_buff *skb;\n\tunsigned int ulen;\n\tint peeked;\n\tint err;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint is_udp4;\n\tbool slow;\n\n\tif (addr_len)\n\t\t*addr_len=sizeof(struct sockaddr_in6);\n\n\tif (flags & MSG_ERRQUEUE)\n\t\treturn ipv6_recv_error(sk, msg, len);\n\n\tif (np->rxpmtu && np->rxopt.bits.rxpmtu)\n\t\treturn ipv6_recv_rxpmtu(sk, msg, len);\n\ntry_again:\n\tskb = __skb_recv_datagram(sk, flags | (noblock ? MSG_DONTWAIT : 0),\n\t\t\t\t  &peeked, &err);\n\tif (!skb)\n\t\tgoto out;\n\n\tulen = skb->len - sizeof(struct udphdr);\n\tif (len > ulen)\n\t\tlen = ulen;\n\telse if (len < ulen)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\tis_udp4 = (skb->protocol == htons(ETH_P_IP));\n\n\t/*\n\t * If checksum is needed at all, try to do it while copying the\n\t * data.  If the data is truncated, or if we only want a partial\n\t * coverage checksum (UDP-Lite), do it before the copy.\n\t */\n\n\tif (len < ulen || UDP_SKB_CB(skb)->partial_cov) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto csum_copy_err;\n\t}\n\n\tif (skb_csum_unnecessary(skb))\n\t\terr = skb_copy_datagram_iovec(skb, sizeof(struct udphdr),\n\t\t\t\t\t      msg->msg_iov,len);\n\telse {\n\t\terr = skb_copy_and_csum_datagram_iovec(skb, sizeof(struct udphdr), msg->msg_iov);\n\t\tif (err == -EINVAL)\n\t\t\tgoto csum_copy_err;\n\t}\n\tif (err)\n\t\tgoto out_free;\n\n\tif (!peeked) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INDATAGRAMS, is_udplite);\n\t}\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\t/* Copy the address. */\n\tif (msg->msg_name) {\n\t\tstruct sockaddr_in6 *sin6;\n\n\t\tsin6 = (struct sockaddr_in6 *) msg->msg_name;\n\t\tsin6->sin6_family = AF_INET6;\n\t\tsin6->sin6_port = udp_hdr(skb)->source;\n\t\tsin6->sin6_flowinfo = 0;\n\t\tsin6->sin6_scope_id = 0;\n\n\t\tif (is_udp4)\n\t\t\tipv6_addr_set_v4mapped(ip_hdr(skb)->saddr,\n\t\t\t\t\t       &sin6->sin6_addr);\n\t\telse {\n\t\t\tipv6_addr_copy(&sin6->sin6_addr,\n\t\t\t\t       &ipv6_hdr(skb)->saddr);\n\t\t\tif (ipv6_addr_type(&sin6->sin6_addr) & IPV6_ADDR_LINKLOCAL)\n\t\t\t\tsin6->sin6_scope_id = IP6CB(skb)->iif;\n\t\t}\n\n\t}\n\tif (is_udp4) {\n\t\tif (inet->cmsg_flags)\n\t\t\tip_cmsg_recv(msg, skb);\n\t} else {\n\t\tif (np->rxopt.all)\n\t\t\tdatagram_recv_ctl(sk, msg, skb);\n\t}\n\n\terr = len;\n\tif (flags & MSG_TRUNC)\n\t\terr = ulen;\n\nout_free:\n\tskb_free_datagram_locked(sk, skb);\nout:\n\treturn err;\n\ncsum_copy_err:\n\tslow = lock_sock_fast(sk);\n\tif (!skb_kill_datagram(sk, skb, flags)) {\n\t\tif (is_udp4)\n\t\t\tUDP_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t\telse\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\tUDP_MIB_INERRORS, is_udplite);\n\t}\n\tunlock_sock_fast(sk, slow);\n\n\tif (noblock)\n\t\treturn -EAGAIN;\n\n\t/* starting over for a new packet */\n\tmsg->msg_flags &= ~MSG_TRUNC;\n\tgoto try_again;\n}\n\nvoid __udp6_lib_err(struct sk_buff *skb, struct inet6_skb_parm *opt,\n\t\t    u8 type, u8 code, int offset, __be32 info,\n\t\t    struct udp_table *udptable)\n{\n\tstruct ipv6_pinfo *np;\n\tconst struct ipv6hdr *hdr = (const struct ipv6hdr *)skb->data;\n\tconst struct in6_addr *saddr = &hdr->saddr;\n\tconst struct in6_addr *daddr = &hdr->daddr;\n\tstruct udphdr *uh = (struct udphdr*)(skb->data+offset);\n\tstruct sock *sk;\n\tint err;\n\n\tsk = __udp6_lib_lookup(dev_net(skb->dev), daddr, uh->dest,\n\t\t\t       saddr, uh->source, inet6_iif(skb), udptable);\n\tif (sk == NULL)\n\t\treturn;\n\n\tnp = inet6_sk(sk);\n\n\tif (!icmpv6_err_convert(type, code, &err) && !np->recverr)\n\t\tgoto out;\n\n\tif (sk->sk_state != TCP_ESTABLISHED && !np->recverr)\n\t\tgoto out;\n\n\tif (np->recverr)\n\t\tipv6_icmp_error(sk, skb, err, uh->dest, ntohl(info), (u8 *)(uh+1));\n\n\tsk->sk_err = err;\n\tsk->sk_error_report(sk);\nout:\n\tsock_put(sk);\n}\n\nstatic __inline__ void udpv6_err(struct sk_buff *skb,\n\t\t\t\t struct inet6_skb_parm *opt, u8 type,\n\t\t\t\t u8 code, int offset, __be32 info     )\n{\n\t__udp6_lib_err(skb, opt, type, code, offset, info, &udp_table);\n}\n\nint udpv6_queue_rcv_skb(struct sock * sk, struct sk_buff *skb)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\tint rc;\n\tint is_udplite = IS_UDPLITE(sk);\n\n\tif (!ipv6_addr_any(&inet6_sk(sk)->daddr))\n\t\tsock_rps_save_rxhash(sk, skb->rxhash);\n\n\tif (!xfrm6_policy_check(sk, XFRM_POLICY_IN, skb))\n\t\tgoto drop;\n\n\t/*\n\t * UDP-Lite specific tests, ignored on UDP sockets (see net/ipv4/udp.c).\n\t */\n\tif ((is_udplite & UDPLITE_RECV_CC)  &&  UDP_SKB_CB(skb)->partial_cov) {\n\n\t\tif (up->pcrlen == 0) {          /* full coverage was set  */\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING \"UDPLITE6: partial coverage\"\n\t\t\t\t\" %d while full coverage %d requested\\n\",\n\t\t\t\tUDP_SKB_CB(skb)->cscov, skb->len);\n\t\t\tgoto drop;\n\t\t}\n\t\tif (UDP_SKB_CB(skb)->cscov  <  up->pcrlen) {\n\t\t\tLIMIT_NETDEBUG(KERN_WARNING \"UDPLITE6: coverage %d \"\n\t\t\t\t\t\t    \"too small, need min %d\\n\",\n\t\t\t\t       UDP_SKB_CB(skb)->cscov, up->pcrlen);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\n\tif (rcu_dereference_raw(sk->sk_filter)) {\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto drop;\n\t}\n\n\tif ((rc = ip_queue_rcv_skb(sk, skb)) < 0) {\n\t\t/* Note that an ENOMEM error is charged twice */\n\t\tif (rc == -ENOMEM)\n\t\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\t\tUDP_MIB_RCVBUFERRORS, is_udplite);\n\t\tgoto drop_no_sk_drops_inc;\n\t}\n\n\treturn 0;\ndrop:\n\tatomic_inc(&sk->sk_drops);\ndrop_no_sk_drops_inc:\n\tUDP6_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);\n\tkfree_skb(skb);\n\treturn -1;\n}\n\nstatic struct sock *udp_v6_mcast_next(struct net *net, struct sock *sk,\n\t\t\t\t      __be16 loc_port, const struct in6_addr *loc_addr,\n\t\t\t\t      __be16 rmt_port, const struct in6_addr *rmt_addr,\n\t\t\t\t      int dif)\n{\n\tstruct hlist_nulls_node *node;\n\tstruct sock *s = sk;\n\tunsigned short num = ntohs(loc_port);\n\n\tsk_nulls_for_each_from(s, node) {\n\t\tstruct inet_sock *inet = inet_sk(s);\n\n\t\tif (!net_eq(sock_net(s), net))\n\t\t\tcontinue;\n\n\t\tif (udp_sk(s)->udp_port_hash == num &&\n\t\t    s->sk_family == PF_INET6) {\n\t\t\tstruct ipv6_pinfo *np = inet6_sk(s);\n\t\t\tif (inet->inet_dport) {\n\t\t\t\tif (inet->inet_dport != rmt_port)\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!ipv6_addr_any(&np->daddr) &&\n\t\t\t    !ipv6_addr_equal(&np->daddr, rmt_addr))\n\t\t\t\tcontinue;\n\n\t\t\tif (s->sk_bound_dev_if && s->sk_bound_dev_if != dif)\n\t\t\t\tcontinue;\n\n\t\t\tif (!ipv6_addr_any(&np->rcv_saddr)) {\n\t\t\t\tif (!ipv6_addr_equal(&np->rcv_saddr, loc_addr))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!inet6_mc_check(s, loc_addr, rmt_addr))\n\t\t\t\tcontinue;\n\t\t\treturn s;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic void flush_stack(struct sock **stack, unsigned int count,\n\t\t\tstruct sk_buff *skb, unsigned int final)\n{\n\tunsigned int i;\n\tstruct sock *sk;\n\tstruct sk_buff *skb1;\n\n\tfor (i = 0; i < count; i++) {\n\t\tskb1 = (i == final) ? skb : skb_clone(skb, GFP_ATOMIC);\n\n\t\tsk = stack[i];\n\t\tif (skb1) {\n\t\t\tif (sk_rcvqueues_full(sk, skb1)) {\n\t\t\t\tkfree_skb(skb1);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t\tbh_lock_sock(sk);\n\t\t\tif (!sock_owned_by_user(sk))\n\t\t\t\tudpv6_queue_rcv_skb(sk, skb1);\n\t\t\telse if (sk_add_backlog(sk, skb1)) {\n\t\t\t\tkfree_skb(skb1);\n\t\t\t\tbh_unlock_sock(sk);\n\t\t\t\tgoto drop;\n\t\t\t}\n\t\t\tbh_unlock_sock(sk);\n\t\t\tcontinue;\n\t\t}\ndrop:\n\t\tatomic_inc(&sk->sk_drops);\n\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\tUDP_MIB_RCVBUFERRORS, IS_UDPLITE(sk));\n\t\tUDP6_INC_STATS_BH(sock_net(sk),\n\t\t\t\tUDP_MIB_INERRORS, IS_UDPLITE(sk));\n\t}\n}\n/*\n * Note: called only from the BH handler context,\n * so we don't need to lock the hashes.\n */\nstatic int __udp6_lib_mcast_deliver(struct net *net, struct sk_buff *skb,\n\t\tconst struct in6_addr *saddr, const struct in6_addr *daddr,\n\t\tstruct udp_table *udptable)\n{\n\tstruct sock *sk, *stack[256 / sizeof(struct sock *)];\n\tconst struct udphdr *uh = udp_hdr(skb);\n\tstruct udp_hslot *hslot = udp_hashslot(udptable, net, ntohs(uh->dest));\n\tint dif;\n\tunsigned int i, count = 0;\n\n\tspin_lock(&hslot->lock);\n\tsk = sk_nulls_head(&hslot->head);\n\tdif = inet6_iif(skb);\n\tsk = udp_v6_mcast_next(net, sk, uh->dest, daddr, uh->source, saddr, dif);\n\twhile (sk) {\n\t\tstack[count++] = sk;\n\t\tsk = udp_v6_mcast_next(net, sk_nulls_next(sk), uh->dest, daddr,\n\t\t\t\t       uh->source, saddr, dif);\n\t\tif (unlikely(count == ARRAY_SIZE(stack))) {\n\t\t\tif (!sk)\n\t\t\t\tbreak;\n\t\t\tflush_stack(stack, count, skb, ~0);\n\t\t\tcount = 0;\n\t\t}\n\t}\n\t/*\n\t * before releasing the lock, we must take reference on sockets\n\t */\n\tfor (i = 0; i < count; i++)\n\t\tsock_hold(stack[i]);\n\n\tspin_unlock(&hslot->lock);\n\n\tif (count) {\n\t\tflush_stack(stack, count, skb, count - 1);\n\n\t\tfor (i = 0; i < count; i++)\n\t\t\tsock_put(stack[i]);\n\t} else {\n\t\tkfree_skb(skb);\n\t}\n\treturn 0;\n}\n\nstatic inline int udp6_csum_init(struct sk_buff *skb, struct udphdr *uh,\n\t\t\t\t int proto)\n{\n\tint err;\n\n\tUDP_SKB_CB(skb)->partial_cov = 0;\n\tUDP_SKB_CB(skb)->cscov = skb->len;\n\n\tif (proto == IPPROTO_UDPLITE) {\n\t\terr = udplite_checksum_init(skb, uh);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (uh->check == 0) {\n\t\t/* RFC 2460 section 8.1 says that we SHOULD log\n\t\t   this error. Well, it is reasonable.\n\t\t */\n\t\tLIMIT_NETDEBUG(KERN_INFO \"IPv6: udp checksum is 0\\n\");\n\t\treturn 1;\n\t}\n\tif (skb->ip_summed == CHECKSUM_COMPLETE &&\n\t    !csum_ipv6_magic(&ipv6_hdr(skb)->saddr, &ipv6_hdr(skb)->daddr,\n\t\t\t     skb->len, proto, skb->csum))\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tif (!skb_csum_unnecessary(skb))\n\t\tskb->csum = ~csum_unfold(csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t\t &ipv6_hdr(skb)->daddr,\n\t\t\t\t\t\t\t skb->len, proto, 0));\n\n\treturn 0;\n}\n\nint __udp6_lib_rcv(struct sk_buff *skb, struct udp_table *udptable,\n\t\t   int proto)\n{\n\tstruct net *net = dev_net(skb->dev);\n\tstruct sock *sk;\n\tstruct udphdr *uh;\n\tconst struct in6_addr *saddr, *daddr;\n\tu32 ulen = 0;\n\n\tif (!pskb_may_pull(skb, sizeof(struct udphdr)))\n\t\tgoto discard;\n\n\tsaddr = &ipv6_hdr(skb)->saddr;\n\tdaddr = &ipv6_hdr(skb)->daddr;\n\tuh = udp_hdr(skb);\n\n\tulen = ntohs(uh->len);\n\tif (ulen > skb->len)\n\t\tgoto short_packet;\n\n\tif (proto == IPPROTO_UDP) {\n\t\t/* UDP validates ulen. */\n\n\t\t/* Check for jumbo payload */\n\t\tif (ulen == 0)\n\t\t\tulen = skb->len;\n\n\t\tif (ulen < sizeof(*uh))\n\t\t\tgoto short_packet;\n\n\t\tif (ulen < skb->len) {\n\t\t\tif (pskb_trim_rcsum(skb, ulen))\n\t\t\t\tgoto short_packet;\n\t\t\tsaddr = &ipv6_hdr(skb)->saddr;\n\t\t\tdaddr = &ipv6_hdr(skb)->daddr;\n\t\t\tuh = udp_hdr(skb);\n\t\t}\n\t}\n\n\tif (udp6_csum_init(skb, uh, proto))\n\t\tgoto discard;\n\n\t/*\n\t *\tMulticast receive code\n\t */\n\tif (ipv6_addr_is_multicast(daddr))\n\t\treturn __udp6_lib_mcast_deliver(net, skb,\n\t\t\t\tsaddr, daddr, udptable);\n\n\t/* Unicast */\n\n\t/*\n\t * check socket cache ... must talk to Alan about his plans\n\t * for sock caches... i'll skip this for now.\n\t */\n\tsk = __udp6_lib_lookup_skb(skb, uh->source, uh->dest, udptable);\n\n\tif (sk == NULL) {\n\t\tif (!xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))\n\t\t\tgoto discard;\n\n\t\tif (udp_lib_checksum_complete(skb))\n\t\t\tgoto discard;\n\t\tUDP6_INC_STATS_BH(net, UDP_MIB_NOPORTS,\n\t\t\t\tproto == IPPROTO_UDPLITE);\n\n\t\ticmpv6_send(skb, ICMPV6_DEST_UNREACH, ICMPV6_PORT_UNREACH, 0);\n\n\t\tkfree_skb(skb);\n\t\treturn 0;\n\t}\n\n\t/* deliver */\n\n\tif (sk_rcvqueues_full(sk, skb)) {\n\t\tsock_put(sk);\n\t\tgoto discard;\n\t}\n\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk))\n\t\tudpv6_queue_rcv_skb(sk, skb);\n\telse if (sk_add_backlog(sk, skb)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tbh_unlock_sock(sk);\n\t\tsock_put(sk);\n\t\tgoto discard;\n\t}\n\tbh_unlock_sock(sk);\n\tsock_put(sk);\n\treturn 0;\n\nshort_packet:\n\tLIMIT_NETDEBUG(KERN_DEBUG \"UDP%sv6: short packet: From [%pI6c]:%u %d/%d to [%pI6c]:%u\\n\",\n\t\t       proto == IPPROTO_UDPLITE ? \"-Lite\" : \"\",\n\t\t       saddr,\n\t\t       ntohs(uh->source),\n\t\t       ulen,\n\t\t       skb->len,\n\t\t       daddr,\n\t\t       ntohs(uh->dest));\n\ndiscard:\n\tUDP6_INC_STATS_BH(net, UDP_MIB_INERRORS, proto == IPPROTO_UDPLITE);\n\tkfree_skb(skb);\n\treturn 0;\n}\n\nstatic __inline__ int udpv6_rcv(struct sk_buff *skb)\n{\n\treturn __udp6_lib_rcv(skb, &udp_table, IPPROTO_UDP);\n}\n\n/*\n * Throw away all pending data and cancel the corking. Socket is locked.\n */\nstatic void udp_v6_flush_pending_frames(struct sock *sk)\n{\n\tstruct udp_sock *up = udp_sk(sk);\n\n\tif (up->pending == AF_INET)\n\t\tudp_flush_pending_frames(sk);\n\telse if (up->pending) {\n\t\tup->len = 0;\n\t\tup->pending = 0;\n\t\tip6_flush_pending_frames(sk);\n\t}\n}\n\n/**\n * \tudp6_hwcsum_outgoing  -  handle outgoing HW checksumming\n * \t@sk: \tsocket we are sending on\n * \t@skb: \tsk_buff containing the filled-in UDP header\n * \t        (checksum field must be zeroed out)\n */\nstatic void udp6_hwcsum_outgoing(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t const struct in6_addr *saddr,\n\t\t\t\t const struct in6_addr *daddr, int len)\n{\n\tunsigned int offset;\n\tstruct udphdr *uh = udp_hdr(skb);\n\t__wsum csum = 0;\n\n\tif (skb_queue_len(&sk->sk_write_queue) == 1) {\n\t\t/* Only one fragment on the socket.  */\n\t\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\t\tskb->csum_offset = offsetof(struct udphdr, check);\n\t\tuh->check = ~csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP, 0);\n\t} else {\n\t\t/*\n\t\t * HW-checksum won't work as there are two or more\n\t\t * fragments on the socket so that all csums of sk_buffs\n\t\t * should be together\n\t\t */\n\t\toffset = skb_transport_offset(skb);\n\t\tskb->csum = skb_checksum(skb, offset, skb->len - offset, 0);\n\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\n\t\tskb_queue_walk(&sk->sk_write_queue, skb) {\n\t\t\tcsum = csum_add(csum, skb->csum);\n\t\t}\n\n\t\tuh->check = csum_ipv6_magic(saddr, daddr, len, IPPROTO_UDP,\n\t\t\t\t\t    csum);\n\t\tif (uh->check == 0)\n\t\t\tuh->check = CSUM_MANGLED_0;\n\t}\n}\n\n/*\n *\tSending\n */\n\nstatic int udp_v6_push_pending_frames(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tstruct udphdr *uh;\n\tstruct udp_sock  *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct flowi6 *fl6 = &inet->cork.fl.u.ip6;\n\tint err = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\t__wsum csum = 0;\n\n\t/* Grab the skbuff where UDP header space exists. */\n\tif ((skb = skb_peek(&sk->sk_write_queue)) == NULL)\n\t\tgoto out;\n\n\t/*\n\t * Create a UDP header\n\t */\n\tuh = udp_hdr(skb);\n\tuh->source = fl6->fl6_sport;\n\tuh->dest = fl6->fl6_dport;\n\tuh->len = htons(up->len);\n\tuh->check = 0;\n\n\tif (is_udplite)\n\t\tcsum = udplite_csum_outgoing(sk, skb);\n\telse if (skb->ip_summed == CHECKSUM_PARTIAL) { /* UDP hardware csum */\n\t\tudp6_hwcsum_outgoing(sk, skb, &fl6->saddr, &fl6->daddr,\n\t\t\t\t     up->len);\n\t\tgoto send;\n\t} else\n\t\tcsum = udp_csum_outgoing(sk, skb);\n\n\t/* add protocol-dependent pseudo-header */\n\tuh->check = csum_ipv6_magic(&fl6->saddr, &fl6->daddr,\n\t\t\t\t    up->len, fl6->flowi6_proto, csum);\n\tif (uh->check == 0)\n\t\tuh->check = CSUM_MANGLED_0;\n\nsend:\n\terr = ip6_push_pending_frames(sk);\n\tif (err) {\n\t\tif (err == -ENOBUFS && !inet6_sk(sk)->recverr) {\n\t\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t\t    UDP_MIB_SNDBUFERRORS, is_udplite);\n\t\t\terr = 0;\n\t\t}\n\t} else\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\t    UDP_MIB_OUTDATAGRAMS, is_udplite);\nout:\n\tup->len = 0;\n\tup->pending = 0;\n\treturn err;\n}\n\nint udpv6_sendmsg(struct kiocb *iocb, struct sock *sk,\n\t\t  struct msghdr *msg, size_t len)\n{\n\tstruct ipv6_txoptions opt_space;\n\tstruct udp_sock *up = udp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\tstruct sockaddr_in6 *sin6 = (struct sockaddr_in6 *) msg->msg_name;\n\tstruct in6_addr *daddr, *final_p, final;\n\tstruct ipv6_txoptions *opt = NULL;\n\tstruct ip6_flowlabel *flowlabel = NULL;\n\tstruct flowi6 fl6;\n\tstruct dst_entry *dst;\n\tint addr_len = msg->msg_namelen;\n\tint ulen = len;\n\tint hlimit = -1;\n\tint tclass = -1;\n\tint dontfrag = -1;\n\tint corkreq = up->corkflag || msg->msg_flags&MSG_MORE;\n\tint err;\n\tint connected = 0;\n\tint is_udplite = IS_UDPLITE(sk);\n\tint (*getfrag)(void *, char *, int, int, int, struct sk_buff *);\n\n\t/* destination address check */\n\tif (sin6) {\n\t\tif (addr_len < offsetof(struct sockaddr, sa_data))\n\t\t\treturn -EINVAL;\n\n\t\tswitch (sin6->sin6_family) {\n\t\tcase AF_INET6:\n\t\t\tif (addr_len < SIN6_LEN_RFC2133)\n\t\t\t\treturn -EINVAL;\n\t\t\tdaddr = &sin6->sin6_addr;\n\t\t\tbreak;\n\t\tcase AF_INET:\n\t\t\tgoto do_udp_sendmsg;\n\t\tcase AF_UNSPEC:\n\t\t\tmsg->msg_name = sin6 = NULL;\n\t\t\tmsg->msg_namelen = addr_len = 0;\n\t\t\tdaddr = NULL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (!up->pending) {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\t\tdaddr = &np->daddr;\n\t} else\n\t\tdaddr = NULL;\n\n\tif (daddr) {\n\t\tif (ipv6_addr_v4mapped(daddr)) {\n\t\t\tstruct sockaddr_in sin;\n\t\t\tsin.sin_family = AF_INET;\n\t\t\tsin.sin_port = sin6 ? sin6->sin6_port : inet->inet_dport;\n\t\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\t\tmsg->msg_name = &sin;\n\t\t\tmsg->msg_namelen = sizeof(sin);\ndo_udp_sendmsg:\n\t\t\tif (__ipv6_only_sock(sk))\n\t\t\t\treturn -ENETUNREACH;\n\t\t\treturn udp_sendmsg(iocb, sk, msg, len);\n\t\t}\n\t}\n\n\tif (up->pending == AF_INET)\n\t\treturn udp_sendmsg(iocb, sk, msg, len);\n\n\t/* Rough check on arithmetic overflow,\n\t   better check is made in ip6_append_data().\n\t   */\n\tif (len > INT_MAX - sizeof(struct udphdr))\n\t\treturn -EMSGSIZE;\n\n\tif (up->pending) {\n\t\t/*\n\t\t * There are pending frames.\n\t\t * The socket lock must be held while it's corked.\n\t\t */\n\t\tlock_sock(sk);\n\t\tif (likely(up->pending)) {\n\t\t\tif (unlikely(up->pending != AF_INET6)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EAFNOSUPPORT;\n\t\t\t}\n\t\t\tdst = NULL;\n\t\t\tgoto do_append_data;\n\t\t}\n\t\trelease_sock(sk);\n\t}\n\tulen += sizeof(struct udphdr);\n\n\tmemset(&fl6, 0, sizeof(fl6));\n\n\tif (sin6) {\n\t\tif (sin6->sin6_port == 0)\n\t\t\treturn -EINVAL;\n\n\t\tfl6.fl6_dport = sin6->sin6_port;\n\t\tdaddr = &sin6->sin6_addr;\n\n\t\tif (np->sndflow) {\n\t\t\tfl6.flowlabel = sin6->sin6_flowinfo&IPV6_FLOWINFO_MASK;\n\t\t\tif (fl6.flowlabel&IPV6_FLOWLABEL_MASK) {\n\t\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\t\tif (flowlabel == NULL)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tdaddr = &flowlabel->dst;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * Otherwise it will be difficult to maintain\n\t\t * sk->sk_dst_cache.\n\t\t */\n\t\tif (sk->sk_state == TCP_ESTABLISHED &&\n\t\t    ipv6_addr_equal(daddr, &np->daddr))\n\t\t\tdaddr = &np->daddr;\n\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    sin6->sin6_scope_id &&\n\t\t    ipv6_addr_type(daddr)&IPV6_ADDR_LINKLOCAL)\n\t\t\tfl6.flowi6_oif = sin6->sin6_scope_id;\n\t} else {\n\t\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\t\treturn -EDESTADDRREQ;\n\n\t\tfl6.fl6_dport = inet->inet_dport;\n\t\tdaddr = &np->daddr;\n\t\tfl6.flowlabel = np->flow_label;\n\t\tconnected = 1;\n\t}\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = sk->sk_bound_dev_if;\n\n\tif (!fl6.flowi6_oif)\n\t\tfl6.flowi6_oif = np->sticky_pktinfo.ipi6_ifindex;\n\n\tfl6.flowi6_mark = sk->sk_mark;\n\n\tif (msg->msg_controllen) {\n\t\topt = &opt_space;\n\t\tmemset(opt, 0, sizeof(struct ipv6_txoptions));\n\t\topt->tot_len = sizeof(*opt);\n\n\t\terr = datagram_send_ctl(sock_net(sk), msg, &fl6, opt, &hlimit,\n\t\t\t\t\t&tclass, &dontfrag);\n\t\tif (err < 0) {\n\t\t\tfl6_sock_release(flowlabel);\n\t\t\treturn err;\n\t\t}\n\t\tif ((fl6.flowlabel&IPV6_FLOWLABEL_MASK) && !flowlabel) {\n\t\t\tflowlabel = fl6_sock_lookup(sk, fl6.flowlabel);\n\t\t\tif (flowlabel == NULL)\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!(opt->opt_nflen|opt->opt_flen))\n\t\t\topt = NULL;\n\t\tconnected = 0;\n\t}\n\tif (opt == NULL)\n\t\topt = np->opt;\n\tif (flowlabel)\n\t\topt = fl6_merge_options(&opt_space, flowlabel, opt);\n\topt = ipv6_fixup_options(&opt_space, opt);\n\n\tfl6.flowi6_proto = sk->sk_protocol;\n\tif (!ipv6_addr_any(daddr))\n\t\tipv6_addr_copy(&fl6.daddr, daddr);\n\telse\n\t\tfl6.daddr.s6_addr[15] = 0x1; /* :: means loopback (BSD'ism) */\n\tif (ipv6_addr_any(&fl6.saddr) && !ipv6_addr_any(&np->saddr))\n\t\tipv6_addr_copy(&fl6.saddr, &np->saddr);\n\tfl6.fl6_sport = inet->inet_sport;\n\n\tfinal_p = fl6_update_dst(&fl6, opt, &final);\n\tif (final_p)\n\t\tconnected = 0;\n\n\tif (!fl6.flowi6_oif && ipv6_addr_is_multicast(&fl6.daddr)) {\n\t\tfl6.flowi6_oif = np->mcast_oif;\n\t\tconnected = 0;\n\t}\n\n\tsecurity_sk_classify_flow(sk, flowi6_to_flowi(&fl6));\n\n\tdst = ip6_sk_dst_lookup_flow(sk, &fl6, final_p, true);\n\tif (IS_ERR(dst)) {\n\t\terr = PTR_ERR(dst);\n\t\tdst = NULL;\n\t\tgoto out;\n\t}\n\n\tif (hlimit < 0) {\n\t\tif (ipv6_addr_is_multicast(&fl6.daddr))\n\t\t\thlimit = np->mcast_hops;\n\t\telse\n\t\t\thlimit = np->hop_limit;\n\t\tif (hlimit < 0)\n\t\t\thlimit = ip6_dst_hoplimit(dst);\n\t}\n\n\tif (tclass < 0)\n\t\ttclass = np->tclass;\n\n\tif (dontfrag < 0)\n\t\tdontfrag = np->dontfrag;\n\n\tif (msg->msg_flags&MSG_CONFIRM)\n\t\tgoto do_confirm;\nback_from_confirm:\n\n\tlock_sock(sk);\n\tif (unlikely(up->pending)) {\n\t\t/* The socket is already corked while preparing it. */\n\t\t/* ... which is an evident application bug. --ANK */\n\t\trelease_sock(sk);\n\n\t\tLIMIT_NETDEBUG(KERN_DEBUG \"udp cork app bug 2\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tup->pending = AF_INET6;\n\ndo_append_data:\n\tup->len += ulen;\n\tgetfrag  =  is_udplite ?  udplite_getfrag : ip_generic_getfrag;\n\terr = ip6_append_data(sk, getfrag, msg->msg_iov, ulen,\n\t\tsizeof(struct udphdr), hlimit, tclass, opt, &fl6,\n\t\t(struct rt6_info*)dst,\n\t\tcorkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags, dontfrag);\n\tif (err)\n\t\tudp_v6_flush_pending_frames(sk);\n\telse if (!corkreq)\n\t\terr = udp_v6_push_pending_frames(sk);\n\telse if (unlikely(skb_queue_empty(&sk->sk_write_queue)))\n\t\tup->pending = 0;\n\n\tif (dst) {\n\t\tif (connected) {\n\t\t\tip6_dst_store(sk, dst,\n\t\t\t\t      ipv6_addr_equal(&fl6.daddr, &np->daddr) ?\n\t\t\t\t      &np->daddr : NULL,\n#ifdef CONFIG_IPV6_SUBTREES\n\t\t\t\t      ipv6_addr_equal(&fl6.saddr, &np->saddr) ?\n\t\t\t\t      &np->saddr :\n#endif\n\t\t\t\t      NULL);\n\t\t} else {\n\t\t\tdst_release(dst);\n\t\t}\n\t\tdst = NULL;\n\t}\n\n\tif (err > 0)\n\t\terr = np->recverr ? net_xmit_errno(err) : 0;\n\trelease_sock(sk);\nout:\n\tdst_release(dst);\n\tfl6_sock_release(flowlabel);\n\tif (!err)\n\t\treturn len;\n\t/*\n\t * ENOBUFS = no kernel mem, SOCK_NOSPACE = no sndbuf space.  Reporting\n\t * ENOBUFS might not be good (it's not tunable per se), but otherwise\n\t * we don't have a good statistic (IpOutDiscards but it can be too many\n\t * things).  We could add another new stat but at least for now that\n\t * seems like overkill.\n\t */\n\tif (err == -ENOBUFS || test_bit(SOCK_NOSPACE, &sk->sk_socket->flags)) {\n\t\tUDP6_INC_STATS_USER(sock_net(sk),\n\t\t\t\tUDP_MIB_SNDBUFERRORS, is_udplite);\n\t}\n\treturn err;\n\ndo_confirm:\n\tdst_confirm(dst);\n\tif (!(msg->msg_flags&MSG_PROBE) || len)\n\t\tgoto back_from_confirm;\n\terr = 0;\n\tgoto out;\n}\n\nvoid udpv6_destroy_sock(struct sock *sk)\n{\n\tlock_sock(sk);\n\tudp_v6_flush_pending_frames(sk);\n\trelease_sock(sk);\n\n\tinet6_destroy_sock(sk);\n}\n\n/*\n *\tSocket option code for UDP\n */\nint udpv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t     char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_v6_push_pending_frames);\n\treturn ipv6_setsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udpv6_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, unsigned int optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_setsockopt(sk, level, optname, optval, optlen,\n\t\t\t\t\t  udp_v6_push_pending_frames);\n\treturn compat_ipv6_setsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nint udpv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t     char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn ipv6_getsockopt(sk, level, optname, optval, optlen);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_udpv6_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t    char __user *optval, int __user *optlen)\n{\n\tif (level == SOL_UDP  ||  level == SOL_UDPLITE)\n\t\treturn udp_lib_getsockopt(sk, level, optname, optval, optlen);\n\treturn compat_ipv6_getsockopt(sk, level, optname, optval, optlen);\n}\n#endif\n\nstatic int udp6_ufo_send_check(struct sk_buff *skb)\n{\n\tconst struct ipv6hdr *ipv6h;\n\tstruct udphdr *uh;\n\n\tif (!pskb_may_pull(skb, sizeof(*uh)))\n\t\treturn -EINVAL;\n\n\tipv6h = ipv6_hdr(skb);\n\tuh = udp_hdr(skb);\n\n\tuh->check = ~csum_ipv6_magic(&ipv6h->saddr, &ipv6h->daddr, skb->len,\n\t\t\t\t     IPPROTO_UDP, 0);\n\tskb->csum_start = skb_transport_header(skb) - skb->head;\n\tskb->csum_offset = offsetof(struct udphdr, check);\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\treturn 0;\n}\n\nstatic struct sk_buff *udp6_ufo_fragment(struct sk_buff *skb, u32 features)\n{\n\tstruct sk_buff *segs = ERR_PTR(-EINVAL);\n\tunsigned int mss;\n\tunsigned int unfrag_ip6hlen, unfrag_len;\n\tstruct frag_hdr *fptr;\n\tu8 *mac_start, *prevhdr;\n\tu8 nexthdr;\n\tu8 frag_hdr_sz = sizeof(struct frag_hdr);\n\tint offset;\n\t__wsum csum;\n\n\tmss = skb_shinfo(skb)->gso_size;\n\tif (unlikely(skb->len <= mss))\n\t\tgoto out;\n\n\tif (skb_gso_ok(skb, features | NETIF_F_GSO_ROBUST)) {\n\t\t/* Packet is from an untrusted source, reset gso_segs. */\n\t\tint type = skb_shinfo(skb)->gso_type;\n\n\t\tif (unlikely(type & ~(SKB_GSO_UDP | SKB_GSO_DODGY) ||\n\t\t\t     !(type & (SKB_GSO_UDP))))\n\t\t\tgoto out;\n\n\t\tskb_shinfo(skb)->gso_segs = DIV_ROUND_UP(skb->len, mss);\n\n\t\tsegs = NULL;\n\t\tgoto out;\n\t}\n\n\t/* Do software UFO. Complete and fill in the UDP checksum as HW cannot\n\t * do checksum of UDP packets sent as multiple IP fragments.\n\t */\n\toffset = skb_checksum_start_offset(skb);\n\tcsum = skb_checksum(skb, offset, skb->len- offset, 0);\n\toffset += skb->csum_offset;\n\t*(__sum16 *)(skb->data + offset) = csum_fold(csum);\n\tskb->ip_summed = CHECKSUM_NONE;\n\n\t/* Check if there is enough headroom to insert fragment header. */\n\tif ((skb_mac_header(skb) < skb->head + frag_hdr_sz) &&\n\t    pskb_expand_head(skb, frag_hdr_sz, 0, GFP_ATOMIC))\n\t\tgoto out;\n\n\t/* Find the unfragmentable header and shift it left by frag_hdr_sz\n\t * bytes to insert fragment header.\n\t */\n\tunfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);\n\tnexthdr = *prevhdr;\n\t*prevhdr = NEXTHDR_FRAGMENT;\n\tunfrag_len = skb_network_header(skb) - skb_mac_header(skb) +\n\t\t     unfrag_ip6hlen;\n\tmac_start = skb_mac_header(skb);\n\tmemmove(mac_start-frag_hdr_sz, mac_start, unfrag_len);\n\n\tskb->mac_header -= frag_hdr_sz;\n\tskb->network_header -= frag_hdr_sz;\n\n\tfptr = (struct frag_hdr *)(skb_network_header(skb) + unfrag_ip6hlen);\n\tfptr->nexthdr = nexthdr;\n\tfptr->reserved = 0;\n\tipv6_select_ident(fptr, (struct rt6_info *)skb_dst(skb));\n\n\t/* Fragment the skb. ipv6 header and the remaining fields of the\n\t * fragment header are updated in ipv6_gso_segment()\n\t */\n\tsegs = skb_segment(skb, features);\n\nout:\n\treturn segs;\n}\n\nstatic const struct inet6_protocol udpv6_protocol = {\n\t.handler\t=\tudpv6_rcv,\n\t.err_handler\t=\tudpv6_err,\n\t.gso_send_check =\tudp6_ufo_send_check,\n\t.gso_segment\t=\tudp6_ufo_fragment,\n\t.flags\t\t=\tINET6_PROTO_NOPOLICY|INET6_PROTO_FINAL,\n};\n\n/* ------------------------------------------------------------------------ */\n#ifdef CONFIG_PROC_FS\n\nstatic void udp6_sock_seq_show(struct seq_file *seq, struct sock *sp, int bucket)\n{\n\tstruct inet_sock *inet = inet_sk(sp);\n\tstruct ipv6_pinfo *np = inet6_sk(sp);\n\tconst struct in6_addr *dest, *src;\n\t__u16 destp, srcp;\n\n\tdest  = &np->daddr;\n\tsrc   = &np->rcv_saddr;\n\tdestp = ntohs(inet->inet_dport);\n\tsrcp  = ntohs(inet->inet_sport);\n\tseq_printf(seq,\n\t\t   \"%5d: %08X%08X%08X%08X:%04X %08X%08X%08X%08X:%04X \"\n\t\t   \"%02X %08X:%08X %02X:%08lX %08X %5d %8d %lu %d %pK %d\\n\",\n\t\t   bucket,\n\t\t   src->s6_addr32[0], src->s6_addr32[1],\n\t\t   src->s6_addr32[2], src->s6_addr32[3], srcp,\n\t\t   dest->s6_addr32[0], dest->s6_addr32[1],\n\t\t   dest->s6_addr32[2], dest->s6_addr32[3], destp,\n\t\t   sp->sk_state,\n\t\t   sk_wmem_alloc_get(sp),\n\t\t   sk_rmem_alloc_get(sp),\n\t\t   0, 0L, 0,\n\t\t   sock_i_uid(sp), 0,\n\t\t   sock_i_ino(sp),\n\t\t   atomic_read(&sp->sk_refcnt), sp,\n\t\t   atomic_read(&sp->sk_drops));\n}\n\nint udp6_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_printf(seq,\n\t\t\t   \"  sl  \"\n\t\t\t   \"local_address                         \"\n\t\t\t   \"remote_address                        \"\n\t\t\t   \"st tx_queue rx_queue tr tm->when retrnsmt\"\n\t\t\t   \"   uid  timeout inode ref pointer drops\\n\");\n\telse\n\t\tudp6_sock_seq_show(seq, v, ((struct udp_iter_state *)seq->private)->bucket);\n\treturn 0;\n}\n\nstatic struct udp_seq_afinfo udp6_seq_afinfo = {\n\t.name\t\t= \"udp6\",\n\t.family\t\t= AF_INET6,\n\t.udp_table\t= &udp_table,\n\t.seq_fops\t= {\n\t\t.owner\t=\tTHIS_MODULE,\n\t},\n\t.seq_ops\t= {\n\t\t.show\t\t= udp6_seq_show,\n\t},\n};\n\nint __net_init udp6_proc_init(struct net *net)\n{\n\treturn udp_proc_register(net, &udp6_seq_afinfo);\n}\n\nvoid udp6_proc_exit(struct net *net) {\n\tudp_proc_unregister(net, &udp6_seq_afinfo);\n}\n#endif /* CONFIG_PROC_FS */\n\n/* ------------------------------------------------------------------------ */\n\nstruct proto udpv6_prot = {\n\t.name\t\t   = \"UDPv6\",\n\t.owner\t\t   = THIS_MODULE,\n\t.close\t\t   = udp_lib_close,\n\t.connect\t   = ip6_datagram_connect,\n\t.disconnect\t   = udp_disconnect,\n\t.ioctl\t\t   = udp_ioctl,\n\t.destroy\t   = udpv6_destroy_sock,\n\t.setsockopt\t   = udpv6_setsockopt,\n\t.getsockopt\t   = udpv6_getsockopt,\n\t.sendmsg\t   = udpv6_sendmsg,\n\t.recvmsg\t   = udpv6_recvmsg,\n\t.backlog_rcv\t   = udpv6_queue_rcv_skb,\n\t.hash\t\t   = udp_lib_hash,\n\t.unhash\t\t   = udp_lib_unhash,\n\t.rehash\t\t   = udp_v6_rehash,\n\t.get_port\t   = udp_v6_get_port,\n\t.memory_allocated  = &udp_memory_allocated,\n\t.sysctl_mem\t   = sysctl_udp_mem,\n\t.sysctl_wmem\t   = &sysctl_udp_wmem_min,\n\t.sysctl_rmem\t   = &sysctl_udp_rmem_min,\n\t.obj_size\t   = sizeof(struct udp6_sock),\n\t.slab_flags\t   = SLAB_DESTROY_BY_RCU,\n\t.h.udp_table\t   = &udp_table,\n#ifdef CONFIG_COMPAT\n\t.compat_setsockopt = compat_udpv6_setsockopt,\n\t.compat_getsockopt = compat_udpv6_getsockopt,\n#endif\n\t.clear_sk\t   = sk_prot_clear_portaddr_nulls,\n};\n\nstatic struct inet_protosw udpv6_protosw = {\n\t.type =      SOCK_DGRAM,\n\t.protocol =  IPPROTO_UDP,\n\t.prot =      &udpv6_prot,\n\t.ops =       &inet6_dgram_ops,\n\t.no_check =  UDP_CSUM_DEFAULT,\n\t.flags =     INET_PROTOSW_PERMANENT,\n};\n\n\nint __init udpv6_init(void)\n{\n\tint ret;\n\n\tret = inet6_add_protocol(&udpv6_protocol, IPPROTO_UDP);\n\tif (ret)\n\t\tgoto out;\n\n\tret = inet6_register_protosw(&udpv6_protosw);\n\tif (ret)\n\t\tgoto out_udpv6_protocol;\nout:\n\treturn ret;\n\nout_udpv6_protocol:\n\tinet6_del_protocol(&udpv6_protocol, IPPROTO_UDP);\n\tgoto out;\n}\n\nvoid udpv6_exit(void)\n{\n\tinet6_unregister_protosw(&udpv6_protosw);\n\tinet6_del_protocol(&udpv6_protocol, IPPROTO_UDP);\n}\n"], "filenames": ["drivers/char/random.c", "include/linux/random.h", "include/net/inetpeer.h", "include/net/ipv6.h", "net/ipv4/inetpeer.c", "net/ipv6/ip6_output.c", "net/ipv6/udp.c"], "buggy_code_start_loc": [1525, 60, 74, 466, 394, 598, 1362], "buggy_code_end_loc": [1525, 60, 114, 477, 440, 1290, 1363], "fixing_code_start_loc": [1526, 61, 74, 466, 394, 599, 1362], "fixing_code_end_loc": [1541, 62, 121, 467, 443, 1316, 1363], "type": "NVD-CWE-Other", "message": "The IPv6 implementation in the Linux kernel before 3.1 does not generate Fragment Identification values separately for each destination, which makes it easier for remote attackers to cause a denial of service (disrupted networking) by predicting these values and sending crafted packets.", "other": {"cve": {"id": "CVE-2011-2699", "sourceIdentifier": "secalert@redhat.com", "published": "2012-05-24T23:55:01.963", "lastModified": "2023-02-13T04:31:09.240", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The IPv6 implementation in the Linux kernel before 3.1 does not generate Fragment Identification values separately for each destination, which makes it easier for remote attackers to cause a denial of service (disrupted networking) by predicting these values and sending crafted packets."}, {"lang": "es", "value": "La implementaci\u00f3n de IPv6 en el kernel de Linux antes de v3.1 no genera valores de los fragmentos de identificaci\u00f3n por separado para cada destino, lo que hace que sea m\u00e1s f\u00e1cil para los atacantes remotos causar una denegaci\u00f3n de servicio (red interrumpida) mediante la predicci\u00f3n de estos valores y el env\u00edo de paquetes modificados."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 7.8}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-Other"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.1", "matchCriteriaId": "156989A4-23D9-434A-B512-9C0F3583D13D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:4.0:*:*:*:*:*:*:*", "matchCriteriaId": "6172AF57-B26D-45F8-BE3A-F75ABDF28F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_mrg:2.0:*:*:*:*:*:*:*", "matchCriteriaId": "C60FA8B1-1802-4522-A088-22171DCF7A93"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=87c48fa3b4630905f98268dde838ee43626a060c", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.1", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2011/07/20/5", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securitytracker.com/id?1027274", "source": "secalert@redhat.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=723429", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/87c48fa3b4630905f98268dde838ee43626a060c", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/87c48fa3b4630905f98268dde838ee43626a060c"}}