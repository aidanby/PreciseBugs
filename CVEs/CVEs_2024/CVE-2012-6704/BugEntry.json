{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tGeneric socket support routines. Memory allocators, socket lock/release\n *\t\thandler for protocols to use and generic option handler.\n *\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tAlan Cox, <A.Cox@swansea.ac.uk>\n *\n * Fixes:\n *\t\tAlan Cox\t: \tNumerous verify_area() problems\n *\t\tAlan Cox\t:\tConnecting on a connecting socket\n *\t\t\t\t\tnow returns an error for tcp.\n *\t\tAlan Cox\t:\tsock->protocol is set correctly.\n *\t\t\t\t\tand is not sometimes left as 0.\n *\t\tAlan Cox\t:\tconnect handles icmp errors on a\n *\t\t\t\t\tconnect properly. Unfortunately there\n *\t\t\t\t\tis a restart syscall nasty there. I\n *\t\t\t\t\tcan't match BSD without hacking the C\n *\t\t\t\t\tlibrary. Ideas urgently sought!\n *\t\tAlan Cox\t:\tDisallow bind() to addresses that are\n *\t\t\t\t\tnot ours - especially broadcast ones!!\n *\t\tAlan Cox\t:\tSocket 1024 _IS_ ok for users. (fencepost)\n *\t\tAlan Cox\t:\tsock_wfree/sock_rfree don't destroy sockets,\n *\t\t\t\t\tinstead they leave that for the DESTROY timer.\n *\t\tAlan Cox\t:\tClean up error flag in accept\n *\t\tAlan Cox\t:\tTCP ack handling is buggy, the DESTROY timer\n *\t\t\t\t\twas buggy. Put a remove_sock() in the handler\n *\t\t\t\t\tfor memory when we hit 0. Also altered the timer\n *\t\t\t\t\tcode. The ACK stuff can wait and needs major\n *\t\t\t\t\tTCP layer surgery.\n *\t\tAlan Cox\t:\tFixed TCP ack bug, removed remove sock\n *\t\t\t\t\tand fixed timer/inet_bh race.\n *\t\tAlan Cox\t:\tAdded zapped flag for TCP\n *\t\tAlan Cox\t:\tMove kfree_skb into skbuff.c and tidied up surplus code\n *\t\tAlan Cox\t:\tfor new sk_buff allocations wmalloc/rmalloc now call alloc_skb\n *\t\tAlan Cox\t:\tkfree_s calls now are kfree_skbmem so we can track skb resources\n *\t\tAlan Cox\t:\tSupports socket option broadcast now as does udp. Packet and raw need fixing.\n *\t\tAlan Cox\t:\tAdded RCVBUF,SNDBUF size setting. It suddenly occurred to me how easy it was so...\n *\t\tRick Sladkey\t:\tRelaxed UDP rules for matching packets.\n *\t\tC.E.Hawkins\t:\tIFF_PROMISC/SIOCGHWADDR support\n *\tPauline Middelink\t:\tidentd support\n *\t\tAlan Cox\t:\tFixed connect() taking signals I think.\n *\t\tAlan Cox\t:\tSO_LINGER supported\n *\t\tAlan Cox\t:\tError reporting fixes\n *\t\tAnonymous\t:\tinet_create tidied up (sk->reuse setting)\n *\t\tAlan Cox\t:\tinet sockets don't set sk->type!\n *\t\tAlan Cox\t:\tSplit socket option code\n *\t\tAlan Cox\t:\tCallbacks\n *\t\tAlan Cox\t:\tNagle flag for Charles & Johannes stuff\n *\t\tAlex\t\t:\tRemoved restriction on inet fioctl\n *\t\tAlan Cox\t:\tSplitting INET from NET core\n *\t\tAlan Cox\t:\tFixed bogus SO_TYPE handling in getsockopt()\n *\t\tAdam Caldwell\t:\tMissing return in SO_DONTROUTE/SO_DEBUG code\n *\t\tAlan Cox\t:\tSplit IP from generic code\n *\t\tAlan Cox\t:\tNew kfree_skbmem()\n *\t\tAlan Cox\t:\tMake SO_DEBUG superuser only.\n *\t\tAlan Cox\t:\tAllow anyone to clear SO_DEBUG\n *\t\t\t\t\t(compatibility fix)\n *\t\tAlan Cox\t:\tAdded optimistic memory grabbing for AF_UNIX throughput.\n *\t\tAlan Cox\t:\tAllocator for a socket is settable.\n *\t\tAlan Cox\t:\tSO_ERROR includes soft errors.\n *\t\tAlan Cox\t:\tAllow NULL arguments on some SO_ opts\n *\t\tAlan Cox\t: \tGeneric socket allocation to make hooks\n *\t\t\t\t\teasier (suggested by Craig Metz).\n *\t\tMichael Pall\t:\tSO_ERROR returns positive errno again\n *              Steve Whitehouse:       Added default destructor to free\n *                                      protocol private data.\n *              Steve Whitehouse:       Added various other default routines\n *                                      common to several socket families.\n *              Chris Evans     :       Call suser() check last on F_SETOWN\n *\t\tJay Schulist\t:\tAdded SO_ATTACH_FILTER and SO_DETACH_FILTER.\n *\t\tAndi Kleen\t:\tAdd sock_kmalloc()/sock_kfree_s()\n *\t\tAndi Kleen\t:\tFix write_space callback\n *\t\tChris Evans\t:\tSecurity fixes - signedness again\n *\t\tArnaldo C. Melo :       cleanups, use skb_queue_purge\n *\n * To Fix:\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/sched.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/poll.h>\n#include <linux/tcp.h>\n#include <linux/init.h>\n#include <linux/highmem.h>\n#include <linux/user_namespace.h>\n#include <linux/static_key.h>\n#include <linux/memcontrol.h>\n\n#include <asm/uaccess.h>\n\n#include <linux/netdevice.h>\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/net_namespace.h>\n#include <net/request_sock.h>\n#include <net/sock.h>\n#include <linux/net_tstamp.h>\n#include <net/xfrm.h>\n#include <linux/ipsec.h>\n#include <net/cls_cgroup.h>\n#include <net/netprio_cgroup.h>\n\n#include <linux/filter.h>\n\n#include <trace/events/sock.h>\n\n#ifdef CONFIG_INET\n#include <net/tcp.h>\n#endif\n\nstatic DEFINE_MUTEX(proto_list_mutex);\nstatic LIST_HEAD(proto_list);\n\n#ifdef CONFIG_CGROUP_MEM_RES_CTLR_KMEM\nint mem_cgroup_sockets_init(struct cgroup *cgrp, struct cgroup_subsys *ss)\n{\n\tstruct proto *proto;\n\tint ret = 0;\n\n\tmutex_lock(&proto_list_mutex);\n\tlist_for_each_entry(proto, &proto_list, node) {\n\t\tif (proto->init_cgroup) {\n\t\t\tret = proto->init_cgroup(cgrp, ss);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmutex_unlock(&proto_list_mutex);\n\treturn ret;\nout:\n\tlist_for_each_entry_continue_reverse(proto, &proto_list, node)\n\t\tif (proto->destroy_cgroup)\n\t\t\tproto->destroy_cgroup(cgrp);\n\tmutex_unlock(&proto_list_mutex);\n\treturn ret;\n}\n\nvoid mem_cgroup_sockets_destroy(struct cgroup *cgrp)\n{\n\tstruct proto *proto;\n\n\tmutex_lock(&proto_list_mutex);\n\tlist_for_each_entry_reverse(proto, &proto_list, node)\n\t\tif (proto->destroy_cgroup)\n\t\t\tproto->destroy_cgroup(cgrp);\n\tmutex_unlock(&proto_list_mutex);\n}\n#endif\n\n/*\n * Each address family might have different locking rules, so we have\n * one slock key per address family:\n */\nstatic struct lock_class_key af_family_keys[AF_MAX];\nstatic struct lock_class_key af_family_slock_keys[AF_MAX];\n\nstruct static_key memcg_socket_limit_enabled;\nEXPORT_SYMBOL(memcg_socket_limit_enabled);\n\n/*\n * Make lock validator output more readable. (we pre-construct these\n * strings build-time, so that runtime initialization of socket\n * locks is fast):\n */\nstatic const char *const af_family_key_strings[AF_MAX+1] = {\n  \"sk_lock-AF_UNSPEC\", \"sk_lock-AF_UNIX\"     , \"sk_lock-AF_INET\"     ,\n  \"sk_lock-AF_AX25\"  , \"sk_lock-AF_IPX\"      , \"sk_lock-AF_APPLETALK\",\n  \"sk_lock-AF_NETROM\", \"sk_lock-AF_BRIDGE\"   , \"sk_lock-AF_ATMPVC\"   ,\n  \"sk_lock-AF_X25\"   , \"sk_lock-AF_INET6\"    , \"sk_lock-AF_ROSE\"     ,\n  \"sk_lock-AF_DECnet\", \"sk_lock-AF_NETBEUI\"  , \"sk_lock-AF_SECURITY\" ,\n  \"sk_lock-AF_KEY\"   , \"sk_lock-AF_NETLINK\"  , \"sk_lock-AF_PACKET\"   ,\n  \"sk_lock-AF_ASH\"   , \"sk_lock-AF_ECONET\"   , \"sk_lock-AF_ATMSVC\"   ,\n  \"sk_lock-AF_RDS\"   , \"sk_lock-AF_SNA\"      , \"sk_lock-AF_IRDA\"     ,\n  \"sk_lock-AF_PPPOX\" , \"sk_lock-AF_WANPIPE\"  , \"sk_lock-AF_LLC\"      ,\n  \"sk_lock-27\"       , \"sk_lock-28\"          , \"sk_lock-AF_CAN\"      ,\n  \"sk_lock-AF_TIPC\"  , \"sk_lock-AF_BLUETOOTH\", \"sk_lock-IUCV\"        ,\n  \"sk_lock-AF_RXRPC\" , \"sk_lock-AF_ISDN\"     , \"sk_lock-AF_PHONET\"   ,\n  \"sk_lock-AF_IEEE802154\", \"sk_lock-AF_CAIF\" , \"sk_lock-AF_ALG\"      ,\n  \"sk_lock-AF_NFC\"   , \"sk_lock-AF_MAX\"\n};\nstatic const char *const af_family_slock_key_strings[AF_MAX+1] = {\n  \"slock-AF_UNSPEC\", \"slock-AF_UNIX\"     , \"slock-AF_INET\"     ,\n  \"slock-AF_AX25\"  , \"slock-AF_IPX\"      , \"slock-AF_APPLETALK\",\n  \"slock-AF_NETROM\", \"slock-AF_BRIDGE\"   , \"slock-AF_ATMPVC\"   ,\n  \"slock-AF_X25\"   , \"slock-AF_INET6\"    , \"slock-AF_ROSE\"     ,\n  \"slock-AF_DECnet\", \"slock-AF_NETBEUI\"  , \"slock-AF_SECURITY\" ,\n  \"slock-AF_KEY\"   , \"slock-AF_NETLINK\"  , \"slock-AF_PACKET\"   ,\n  \"slock-AF_ASH\"   , \"slock-AF_ECONET\"   , \"slock-AF_ATMSVC\"   ,\n  \"slock-AF_RDS\"   , \"slock-AF_SNA\"      , \"slock-AF_IRDA\"     ,\n  \"slock-AF_PPPOX\" , \"slock-AF_WANPIPE\"  , \"slock-AF_LLC\"      ,\n  \"slock-27\"       , \"slock-28\"          , \"slock-AF_CAN\"      ,\n  \"slock-AF_TIPC\"  , \"slock-AF_BLUETOOTH\", \"slock-AF_IUCV\"     ,\n  \"slock-AF_RXRPC\" , \"slock-AF_ISDN\"     , \"slock-AF_PHONET\"   ,\n  \"slock-AF_IEEE802154\", \"slock-AF_CAIF\" , \"slock-AF_ALG\"      ,\n  \"slock-AF_NFC\"   , \"slock-AF_MAX\"\n};\nstatic const char *const af_family_clock_key_strings[AF_MAX+1] = {\n  \"clock-AF_UNSPEC\", \"clock-AF_UNIX\"     , \"clock-AF_INET\"     ,\n  \"clock-AF_AX25\"  , \"clock-AF_IPX\"      , \"clock-AF_APPLETALK\",\n  \"clock-AF_NETROM\", \"clock-AF_BRIDGE\"   , \"clock-AF_ATMPVC\"   ,\n  \"clock-AF_X25\"   , \"clock-AF_INET6\"    , \"clock-AF_ROSE\"     ,\n  \"clock-AF_DECnet\", \"clock-AF_NETBEUI\"  , \"clock-AF_SECURITY\" ,\n  \"clock-AF_KEY\"   , \"clock-AF_NETLINK\"  , \"clock-AF_PACKET\"   ,\n  \"clock-AF_ASH\"   , \"clock-AF_ECONET\"   , \"clock-AF_ATMSVC\"   ,\n  \"clock-AF_RDS\"   , \"clock-AF_SNA\"      , \"clock-AF_IRDA\"     ,\n  \"clock-AF_PPPOX\" , \"clock-AF_WANPIPE\"  , \"clock-AF_LLC\"      ,\n  \"clock-27\"       , \"clock-28\"          , \"clock-AF_CAN\"      ,\n  \"clock-AF_TIPC\"  , \"clock-AF_BLUETOOTH\", \"clock-AF_IUCV\"     ,\n  \"clock-AF_RXRPC\" , \"clock-AF_ISDN\"     , \"clock-AF_PHONET\"   ,\n  \"clock-AF_IEEE802154\", \"clock-AF_CAIF\" , \"clock-AF_ALG\"      ,\n  \"clock-AF_NFC\"   , \"clock-AF_MAX\"\n};\n\n/*\n * sk_callback_lock locking rules are per-address-family,\n * so split the lock classes by using a per-AF key:\n */\nstatic struct lock_class_key af_callback_keys[AF_MAX];\n\n/* Take into consideration the size of the struct sk_buff overhead in the\n * determination of these values, since that is non-constant across\n * platforms.  This makes socket queueing behavior and performance\n * not depend upon such differences.\n */\n#define _SK_MEM_PACKETS\t\t256\n#define _SK_MEM_OVERHEAD\tSKB_TRUESIZE(256)\n#define SK_WMEM_MAX\t\t(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)\n#define SK_RMEM_MAX\t\t(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)\n\n/* Run time adjustable parameters. */\n__u32 sysctl_wmem_max __read_mostly = SK_WMEM_MAX;\n__u32 sysctl_rmem_max __read_mostly = SK_RMEM_MAX;\n__u32 sysctl_wmem_default __read_mostly = SK_WMEM_MAX;\n__u32 sysctl_rmem_default __read_mostly = SK_RMEM_MAX;\n\n/* Maximal space eaten by iovec or ancillary data plus some space */\nint sysctl_optmem_max __read_mostly = sizeof(unsigned long)*(2*UIO_MAXIOV+512);\nEXPORT_SYMBOL(sysctl_optmem_max);\n\n#if defined(CONFIG_CGROUPS)\n#if !defined(CONFIG_NET_CLS_CGROUP)\nint net_cls_subsys_id = -1;\nEXPORT_SYMBOL_GPL(net_cls_subsys_id);\n#endif\n#if !defined(CONFIG_NETPRIO_CGROUP)\nint net_prio_subsys_id = -1;\nEXPORT_SYMBOL_GPL(net_prio_subsys_id);\n#endif\n#endif\n\nstatic int sock_set_timeout(long *timeo_p, char __user *optval, int optlen)\n{\n\tstruct timeval tv;\n\n\tif (optlen < sizeof(tv))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&tv, optval, sizeof(tv)))\n\t\treturn -EFAULT;\n\tif (tv.tv_usec < 0 || tv.tv_usec >= USEC_PER_SEC)\n\t\treturn -EDOM;\n\n\tif (tv.tv_sec < 0) {\n\t\tstatic int warned __read_mostly;\n\n\t\t*timeo_p = 0;\n\t\tif (warned < 10 && net_ratelimit()) {\n\t\t\twarned++;\n\t\t\tprintk(KERN_INFO \"sock_set_timeout: `%s' (pid %d) \"\n\t\t\t       \"tries to set negative timeout\\n\",\n\t\t\t\tcurrent->comm, task_pid_nr(current));\n\t\t}\n\t\treturn 0;\n\t}\n\t*timeo_p = MAX_SCHEDULE_TIMEOUT;\n\tif (tv.tv_sec == 0 && tv.tv_usec == 0)\n\t\treturn 0;\n\tif (tv.tv_sec < (MAX_SCHEDULE_TIMEOUT/HZ - 1))\n\t\t*timeo_p = tv.tv_sec*HZ + (tv.tv_usec+(1000000/HZ-1))/(1000000/HZ);\n\treturn 0;\n}\n\nstatic void sock_warn_obsolete_bsdism(const char *name)\n{\n\tstatic int warned;\n\tstatic char warncomm[TASK_COMM_LEN];\n\tif (strcmp(warncomm, current->comm) && warned < 5) {\n\t\tstrcpy(warncomm,  current->comm);\n\t\tprintk(KERN_WARNING \"process `%s' is using obsolete \"\n\t\t       \"%s SO_BSDCOMPAT\\n\", warncomm, name);\n\t\twarned++;\n\t}\n}\n\n#define SK_FLAGS_TIMESTAMP ((1UL << SOCK_TIMESTAMP) | (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE))\n\nstatic void sock_disable_timestamp(struct sock *sk, unsigned long flags)\n{\n\tif (sk->sk_flags & flags) {\n\t\tsk->sk_flags &= ~flags;\n\t\tif (!(sk->sk_flags & SK_FLAGS_TIMESTAMP))\n\t\t\tnet_disable_timestamp();\n\t}\n}\n\n\nint sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint err;\n\tint skb_len;\n\tunsigned long flags;\n\tstruct sk_buff_head *list = &sk->sk_receive_queue;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\ttrace_sock_rcvqueue_full(sk, skb);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = sk_filter(sk, skb);\n\tif (err)\n\t\treturn err;\n\n\tif (!sk_rmem_schedule(sk, skb->truesize)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\treturn -ENOBUFS;\n\t}\n\n\tskb->dev = NULL;\n\tskb_set_owner_r(skb, sk);\n\n\t/* Cache the SKB length before we tack it onto the receive\n\t * queue.  Once it is added it no longer belongs to us and\n\t * may be freed by other threads of control pulling packets\n\t * from the queue.\n\t */\n\tskb_len = skb->len;\n\n\t/* we escape from rcu protected region, make sure we dont leak\n\t * a norefcounted dst\n\t */\n\tskb_dst_force(skb);\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tskb->dropcount = atomic_read(&sk->sk_drops);\n\t__skb_queue_tail(list, skb);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_data_ready(sk, skb_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_queue_rcv_skb);\n\nint sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested)\n{\n\tint rc = NET_RX_SUCCESS;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tif (sk_rcvqueues_full(sk, skb, sk->sk_rcvbuf)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\tif (nested)\n\t\tbh_lock_sock_nested(sk);\n\telse\n\t\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk)) {\n\t\t/*\n\t\t * trylock + unlock semantics:\n\t\t */\n\t\tmutex_acquire(&sk->sk_lock.dep_map, 0, 1, _RET_IP_);\n\n\t\trc = sk_backlog_rcv(sk, skb);\n\n\t\tmutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);\n\t} else if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\n\tbh_unlock_sock(sk);\nout:\n\tsock_put(sk);\n\treturn rc;\ndiscard_and_relse:\n\tkfree_skb(skb);\n\tgoto out;\n}\nEXPORT_SYMBOL(sk_receive_skb);\n\nvoid sk_reset_txq(struct sock *sk)\n{\n\tsk_tx_queue_clear(sk);\n}\nEXPORT_SYMBOL(sk_reset_txq);\n\nstruct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie)\n{\n\tstruct dst_entry *dst = __sk_dst_get(sk);\n\n\tif (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {\n\t\tsk_tx_queue_clear(sk);\n\t\tRCU_INIT_POINTER(sk->sk_dst_cache, NULL);\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\n\treturn dst;\n}\nEXPORT_SYMBOL(__sk_dst_check);\n\nstruct dst_entry *sk_dst_check(struct sock *sk, u32 cookie)\n{\n\tstruct dst_entry *dst = sk_dst_get(sk);\n\n\tif (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {\n\t\tsk_dst_reset(sk);\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\n\treturn dst;\n}\nEXPORT_SYMBOL(sk_dst_check);\n\nstatic int sock_bindtodevice(struct sock *sk, char __user *optval, int optlen)\n{\n\tint ret = -ENOPROTOOPT;\n#ifdef CONFIG_NETDEVICES\n\tstruct net *net = sock_net(sk);\n\tchar devname[IFNAMSIZ];\n\tint index;\n\n\t/* Sorry... */\n\tret = -EPERM;\n\tif (!capable(CAP_NET_RAW))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (optlen < 0)\n\t\tgoto out;\n\n\t/* Bind this socket to a particular device like \"eth0\",\n\t * as specified in the passed interface name. If the\n\t * name is \"\" or the option length is zero the socket\n\t * is not bound.\n\t */\n\tif (optlen > IFNAMSIZ - 1)\n\t\toptlen = IFNAMSIZ - 1;\n\tmemset(devname, 0, sizeof(devname));\n\n\tret = -EFAULT;\n\tif (copy_from_user(devname, optval, optlen))\n\t\tgoto out;\n\n\tindex = 0;\n\tif (devname[0] != '\\0') {\n\t\tstruct net_device *dev;\n\n\t\trcu_read_lock();\n\t\tdev = dev_get_by_name_rcu(net, devname);\n\t\tif (dev)\n\t\t\tindex = dev->ifindex;\n\t\trcu_read_unlock();\n\t\tret = -ENODEV;\n\t\tif (!dev)\n\t\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tsk->sk_bound_dev_if = index;\n\tsk_dst_reset(sk);\n\trelease_sock(sk);\n\n\tret = 0;\n\nout:\n#endif\n\n\treturn ret;\n}\n\nstatic inline void sock_valbool_flag(struct sock *sk, int bit, int valbool)\n{\n\tif (valbool)\n\t\tsock_set_flag(sk, bit);\n\telse\n\t\tsock_reset_flag(sk, bit);\n}\n\n/*\n *\tThis is meant for all protocols to use and covers goings on\n *\tat the socket level. Everything here is generic.\n */\n\nint sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_bindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t   about it this is right. Otherwise apps have to\n\t\t   play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t   are treated in BSD as hints */\n\n\t\tif (val > sysctl_wmem_max)\n\t\t\tval = sysctl_wmem_max;\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tif ((val * 2) < SOCK_MIN_SNDBUF)\n\t\t\tsk->sk_sndbuf = SOCK_MIN_SNDBUF;\n\t\telse\n\t\t\tsk->sk_sndbuf = val * 2;\n\n\t\t/*\n\t\t *\tWake up sending tasks if we\n\t\t *\tupped the value.\n\t\t */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t   about it this is right. Otherwise apps have to\n\t\t   play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t   are treated in BSD as hints */\n\n\t\tif (val > sysctl_rmem_max)\n\t\t\tval = sysctl_rmem_max;\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tif ((val * 2) < SOCK_MIN_RCVBUF)\n\t\t\tsk->sk_rcvbuf = SOCK_MIN_RCVBUF;\n\t\telse\n\t\t\tsk->sk_rcvbuf = val * 2;\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) || capable(CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RX_HARDWARE);\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SYS_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RAW_HARDWARE);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\t\t/* We implement the SO_SNDLOWAT etc to\n\t\t   not be settable (1003.1g 5.3) */\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tsock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}\nEXPORT_SYMBOL(sock_setsockopt);\n\n\nvoid cred_to_ucred(struct pid *pid, const struct cred *cred,\n\t\t   struct ucred *ucred)\n{\n\tucred->pid = pid_vnr(pid);\n\tucred->uid = ucred->gid = -1;\n\tif (cred) {\n\t\tstruct user_namespace *current_ns = current_user_ns();\n\n\t\tucred->uid = user_ns_map_uid(current_ns, cred, cred->euid);\n\t\tucred->gid = user_ns_map_gid(current_ns, cred, cred->egid);\n\t}\n}\nEXPORT_SYMBOL_GPL(cred_to_ucred);\n\nint sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tunion {\n\t\tint val;\n\t\tstruct linger ling;\n\t\tstruct timeval tm;\n\t} v;\n\n\tint lv = sizeof(int);\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tmemset(&v, 0, sizeof(v));\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tv.val = sock_flag(sk, SOCK_DBG);\n\t\tbreak;\n\n\tcase SO_DONTROUTE:\n\t\tv.val = sock_flag(sk, SOCK_LOCALROUTE);\n\t\tbreak;\n\n\tcase SO_BROADCAST:\n\t\tv.val = !!sock_flag(sk, SOCK_BROADCAST);\n\t\tbreak;\n\n\tcase SO_SNDBUF:\n\t\tv.val = sk->sk_sndbuf;\n\t\tbreak;\n\n\tcase SO_RCVBUF:\n\t\tv.val = sk->sk_rcvbuf;\n\t\tbreak;\n\n\tcase SO_REUSEADDR:\n\t\tv.val = sk->sk_reuse;\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tv.val = !!sock_flag(sk, SOCK_KEEPOPEN);\n\t\tbreak;\n\n\tcase SO_TYPE:\n\t\tv.val = sk->sk_type;\n\t\tbreak;\n\n\tcase SO_PROTOCOL:\n\t\tv.val = sk->sk_protocol;\n\t\tbreak;\n\n\tcase SO_DOMAIN:\n\t\tv.val = sk->sk_family;\n\t\tbreak;\n\n\tcase SO_ERROR:\n\t\tv.val = -sock_error(sk);\n\t\tif (v.val == 0)\n\t\t\tv.val = xchg(&sk->sk_err_soft, 0);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tv.val = !!sock_flag(sk, SOCK_URGINLINE);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tv.val = sk->sk_no_check;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tv.val = sk->sk_priority;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tlv\t\t= sizeof(v.ling);\n\t\tv.ling.l_onoff\t= !!sock_flag(sk, SOCK_LINGER);\n\t\tv.ling.l_linger\t= sk->sk_lingertime / HZ;\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"getsockopt\");\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) &&\n\t\t\t\t!sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tv.val = 0;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_TX_HARDWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_TX_SOFTWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_RX_HARDWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_RX_SOFTWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_RX_SOFTWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_SOFTWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_SOFTWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_SYS_HARDWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_RAW_HARDWARE;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tlv = sizeof(struct timeval);\n\t\tif (sk->sk_rcvtimeo == MAX_SCHEDULE_TIMEOUT) {\n\t\t\tv.tm.tv_sec = 0;\n\t\t\tv.tm.tv_usec = 0;\n\t\t} else {\n\t\t\tv.tm.tv_sec = sk->sk_rcvtimeo / HZ;\n\t\t\tv.tm.tv_usec = ((sk->sk_rcvtimeo % HZ) * 1000000) / HZ;\n\t\t}\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tlv = sizeof(struct timeval);\n\t\tif (sk->sk_sndtimeo == MAX_SCHEDULE_TIMEOUT) {\n\t\t\tv.tm.tv_sec = 0;\n\t\t\tv.tm.tv_usec = 0;\n\t\t} else {\n\t\t\tv.tm.tv_sec = sk->sk_sndtimeo / HZ;\n\t\t\tv.tm.tv_usec = ((sk->sk_sndtimeo % HZ) * 1000000) / HZ;\n\t\t}\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tv.val = sk->sk_rcvlowat;\n\t\tbreak;\n\n\tcase SO_SNDLOWAT:\n\t\tv.val = 1;\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tv.val = test_bit(SOCK_PASSCRED, &sock->flags) ? 1 : 0;\n\t\tbreak;\n\n\tcase SO_PEERCRED:\n\t{\n\t\tstruct ucred peercred;\n\t\tif (len > sizeof(peercred))\n\t\t\tlen = sizeof(peercred);\n\t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n\t\tif (copy_to_user(optval, &peercred, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERNAME:\n\t{\n\t\tchar address[128];\n\n\t\tif (sock->ops->getname(sock, (struct sockaddr *)address, &lv, 2))\n\t\t\treturn -ENOTCONN;\n\t\tif (lv < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(optval, address, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\t/* Dubious BSD thing... Probably nobody even uses it, but\n\t * the UNIX standard wants it for whatever reason... -DaveM\n\t */\n\tcase SO_ACCEPTCONN:\n\t\tv.val = sk->sk_state == TCP_LISTEN;\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tv.val = test_bit(SOCK_PASSSEC, &sock->flags) ? 1 : 0;\n\t\tbreak;\n\n\tcase SO_PEERSEC:\n\t\treturn security_socket_getpeersec_stream(sock, optval, optlen, len);\n\n\tcase SO_MARK:\n\t\tv.val = sk->sk_mark;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tv.val = !!sock_flag(sk, SOCK_RXQ_OVFL);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tv.val = !!sock_flag(sk, SOCK_WIFI_STATUS);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (!sock->ops->set_peek_off)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tv.val = sk->sk_peek_off;\n\t\tbreak;\n\tcase SO_NOFCS:\n\t\tv.val = !!sock_flag(sk, SOCK_NOFCS);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (copy_to_user(optval, &v, len))\n\t\treturn -EFAULT;\nlenout:\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/*\n * Initialize an sk_lock.\n *\n * (We also register the sk_lock with the lock validator.)\n */\nstatic inline void sock_lock_init(struct sock *sk)\n{\n\tsock_lock_init_class_and_name(sk,\n\t\t\taf_family_slock_key_strings[sk->sk_family],\n\t\t\taf_family_slock_keys + sk->sk_family,\n\t\t\taf_family_key_strings[sk->sk_family],\n\t\t\taf_family_keys + sk->sk_family);\n}\n\n/*\n * Copy all fields from osk to nsk but nsk->sk_refcnt must not change yet,\n * even temporarly, because of RCU lookups. sk_node should also be left as is.\n * We must not copy fields between sk_dontcopy_begin and sk_dontcopy_end\n */\nstatic void sock_copy(struct sock *nsk, const struct sock *osk)\n{\n#ifdef CONFIG_SECURITY_NETWORK\n\tvoid *sptr = nsk->sk_security;\n#endif\n\tmemcpy(nsk, osk, offsetof(struct sock, sk_dontcopy_begin));\n\n\tmemcpy(&nsk->sk_dontcopy_end, &osk->sk_dontcopy_end,\n\t       osk->sk_prot->obj_size - offsetof(struct sock, sk_dontcopy_end));\n\n#ifdef CONFIG_SECURITY_NETWORK\n\tnsk->sk_security = sptr;\n\tsecurity_sk_clone(osk, nsk);\n#endif\n}\n\n/*\n * caches using SLAB_DESTROY_BY_RCU should let .next pointer from nulls nodes\n * un-modified. Special care is taken when initializing object to zero.\n */\nstatic inline void sk_prot_clear_nulls(struct sock *sk, int size)\n{\n\tif (offsetof(struct sock, sk_node.next) != 0)\n\t\tmemset(sk, 0, offsetof(struct sock, sk_node.next));\n\tmemset(&sk->sk_node.pprev, 0,\n\t       size - offsetof(struct sock, sk_node.pprev));\n}\n\nvoid sk_prot_clear_portaddr_nulls(struct sock *sk, int size)\n{\n\tunsigned long nulls1, nulls2;\n\n\tnulls1 = offsetof(struct sock, __sk_common.skc_node.next);\n\tnulls2 = offsetof(struct sock, __sk_common.skc_portaddr_node.next);\n\tif (nulls1 > nulls2)\n\t\tswap(nulls1, nulls2);\n\n\tif (nulls1 != 0)\n\t\tmemset((char *)sk, 0, nulls1);\n\tmemset((char *)sk + nulls1 + sizeof(void *), 0,\n\t       nulls2 - nulls1 - sizeof(void *));\n\tmemset((char *)sk + nulls2 + sizeof(void *), 0,\n\t       size - nulls2 - sizeof(void *));\n}\nEXPORT_SYMBOL(sk_prot_clear_portaddr_nulls);\n\nstatic struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority,\n\t\tint family)\n{\n\tstruct sock *sk;\n\tstruct kmem_cache *slab;\n\n\tslab = prot->slab;\n\tif (slab != NULL) {\n\t\tsk = kmem_cache_alloc(slab, priority & ~__GFP_ZERO);\n\t\tif (!sk)\n\t\t\treturn sk;\n\t\tif (priority & __GFP_ZERO) {\n\t\t\tif (prot->clear_sk)\n\t\t\t\tprot->clear_sk(sk, prot->obj_size);\n\t\t\telse\n\t\t\t\tsk_prot_clear_nulls(sk, prot->obj_size);\n\t\t}\n\t} else\n\t\tsk = kmalloc(prot->obj_size, priority);\n\n\tif (sk != NULL) {\n\t\tkmemcheck_annotate_bitfield(sk, flags);\n\n\t\tif (security_sk_alloc(sk, family, priority))\n\t\t\tgoto out_free;\n\n\t\tif (!try_module_get(prot->owner))\n\t\t\tgoto out_free_sec;\n\t\tsk_tx_queue_clear(sk);\n\t}\n\n\treturn sk;\n\nout_free_sec:\n\tsecurity_sk_free(sk);\nout_free:\n\tif (slab != NULL)\n\t\tkmem_cache_free(slab, sk);\n\telse\n\t\tkfree(sk);\n\treturn NULL;\n}\n\nstatic void sk_prot_free(struct proto *prot, struct sock *sk)\n{\n\tstruct kmem_cache *slab;\n\tstruct module *owner;\n\n\towner = prot->owner;\n\tslab = prot->slab;\n\n\tsecurity_sk_free(sk);\n\tif (slab != NULL)\n\t\tkmem_cache_free(slab, sk);\n\telse\n\t\tkfree(sk);\n\tmodule_put(owner);\n}\n\n#ifdef CONFIG_CGROUPS\nvoid sock_update_classid(struct sock *sk)\n{\n\tu32 classid;\n\n\trcu_read_lock();  /* doing current task, which cannot vanish. */\n\tclassid = task_cls_classid(current);\n\trcu_read_unlock();\n\tif (classid && classid != sk->sk_classid)\n\t\tsk->sk_classid = classid;\n}\nEXPORT_SYMBOL(sock_update_classid);\n\nvoid sock_update_netprioidx(struct sock *sk)\n{\n\tif (in_interrupt())\n\t\treturn;\n\n\tsk->sk_cgrp_prioidx = task_netprioidx(current);\n}\nEXPORT_SYMBOL_GPL(sock_update_netprioidx);\n#endif\n\n/**\n *\tsk_alloc - All socket objects are allocated here\n *\t@net: the applicable net namespace\n *\t@family: protocol family\n *\t@priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)\n *\t@prot: struct proto associated with this new sock instance\n */\nstruct sock *sk_alloc(struct net *net, int family, gfp_t priority,\n\t\t      struct proto *prot)\n{\n\tstruct sock *sk;\n\n\tsk = sk_prot_alloc(prot, priority | __GFP_ZERO, family);\n\tif (sk) {\n\t\tsk->sk_family = family;\n\t\t/*\n\t\t * See comment in struct sock definition to understand\n\t\t * why we need sk_prot_creator -acme\n\t\t */\n\t\tsk->sk_prot = sk->sk_prot_creator = prot;\n\t\tsock_lock_init(sk);\n\t\tsock_net_set(sk, get_net(net));\n\t\tatomic_set(&sk->sk_wmem_alloc, 1);\n\n\t\tsock_update_classid(sk);\n\t\tsock_update_netprioidx(sk);\n\t}\n\n\treturn sk;\n}\nEXPORT_SYMBOL(sk_alloc);\n\nstatic void __sk_free(struct sock *sk)\n{\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       atomic_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tprintk(KERN_DEBUG \"%s: optmem leakage (%d bytes) detected.\\n\",\n\t\t       __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}\n\nvoid sk_free(struct sock *sk)\n{\n\t/*\n\t * We subtract one from sk_wmem_alloc and can know if\n\t * some packets are still in some tx queue.\n\t * If not null, sock_wfree() will call __sk_free(sk) later\n\t */\n\tif (atomic_dec_and_test(&sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\nEXPORT_SYMBOL(sk_free);\n\n/*\n * Last sock_put should drop reference to sk->sk_net. It has already\n * been dropped in sk_change_net. Taking reference to stopping namespace\n * is not an option.\n * Take reference to a socket to remove it from hash _alive_ and after that\n * destroy it in the context of init_net.\n */\nvoid sk_release_kernel(struct sock *sk)\n{\n\tif (sk == NULL || sk->sk_socket == NULL)\n\t\treturn;\n\n\tsock_hold(sk);\n\tsock_release(sk->sk_socket);\n\trelease_net(sock_net(sk));\n\tsock_net_set(sk, get_net(&init_net));\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(sk_release_kernel);\n\nstatic void sk_update_clone(const struct sock *sk, struct sock *newsk)\n{\n\tif (mem_cgroup_sockets_enabled && sk->sk_cgrp)\n\t\tsock_update_memcg(newsk);\n}\n\n/**\n *\tsk_clone_lock - clone a socket, and lock its clone\n *\t@sk: the socket to clone\n *\t@priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)\n *\n *\tCaller must unlock socket even in error path (bh_unlock_sock(newsk))\n */\nstruct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority)\n{\n\tstruct sock *newsk;\n\n\tnewsk = sk_prot_alloc(sk->sk_prot, priority, sk->sk_family);\n\tif (newsk != NULL) {\n\t\tstruct sk_filter *filter;\n\n\t\tsock_copy(newsk, sk);\n\n\t\t/* SANITY */\n\t\tget_net(sock_net(newsk));\n\t\tsk_node_init(&newsk->sk_node);\n\t\tsock_lock_init(newsk);\n\t\tbh_lock_sock(newsk);\n\t\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n\t\tnewsk->sk_backlog.len = 0;\n\n\t\tatomic_set(&newsk->sk_rmem_alloc, 0);\n\t\t/*\n\t\t * sk_wmem_alloc set to one (see sk_free() and sock_wfree())\n\t\t */\n\t\tatomic_set(&newsk->sk_wmem_alloc, 1);\n\t\tatomic_set(&newsk->sk_omem_alloc, 0);\n\t\tskb_queue_head_init(&newsk->sk_receive_queue);\n\t\tskb_queue_head_init(&newsk->sk_write_queue);\n#ifdef CONFIG_NET_DMA\n\t\tskb_queue_head_init(&newsk->sk_async_wait_queue);\n#endif\n\n\t\tspin_lock_init(&newsk->sk_dst_lock);\n\t\trwlock_init(&newsk->sk_callback_lock);\n\t\tlockdep_set_class_and_name(&newsk->sk_callback_lock,\n\t\t\t\taf_callback_keys + newsk->sk_family,\n\t\t\t\taf_family_clock_key_strings[newsk->sk_family]);\n\n\t\tnewsk->sk_dst_cache\t= NULL;\n\t\tnewsk->sk_wmem_queued\t= 0;\n\t\tnewsk->sk_forward_alloc = 0;\n\t\tnewsk->sk_send_head\t= NULL;\n\t\tnewsk->sk_userlocks\t= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;\n\n\t\tsock_reset_flag(newsk, SOCK_DONE);\n\t\tskb_queue_head_init(&newsk->sk_error_queue);\n\n\t\tfilter = rcu_dereference_protected(newsk->sk_filter, 1);\n\t\tif (filter != NULL)\n\t\t\tsk_filter_charge(newsk, filter);\n\n\t\tif (unlikely(xfrm_sk_clone_policy(newsk))) {\n\t\t\t/* It is still raw copy of parent, so invalidate\n\t\t\t * destructor and make plain sk_free() */\n\t\t\tnewsk->sk_destruct = NULL;\n\t\t\tbh_unlock_sock(newsk);\n\t\t\tsk_free(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnewsk->sk_err\t   = 0;\n\t\tnewsk->sk_priority = 0;\n\t\t/*\n\t\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t\t */\n\t\tsmp_wmb();\n\t\tatomic_set(&newsk->sk_refcnt, 2);\n\n\t\t/*\n\t\t * Increment the counter in the same struct proto as the master\n\t\t * sock (sk_refcnt_debug_inc uses newsk->sk_prot->socks, that\n\t\t * is the same as sk->sk_prot->socks, as this field was copied\n\t\t * with memcpy).\n\t\t *\n\t\t * This _changes_ the previous behaviour, where\n\t\t * tcp_create_openreq_child always was incrementing the\n\t\t * equivalent to tcp_prot->socks (inet_sock_nr), so this have\n\t\t * to be taken into account in all callers. -acme\n\t\t */\n\t\tsk_refcnt_debug_inc(newsk);\n\t\tsk_set_socket(newsk, NULL);\n\t\tnewsk->sk_wq = NULL;\n\n\t\tsk_update_clone(sk, newsk);\n\n\t\tif (newsk->sk_prot->sockets_allocated)\n\t\t\tsk_sockets_allocated_inc(newsk);\n\n\t\tif (newsk->sk_flags & SK_FLAGS_TIMESTAMP)\n\t\t\tnet_enable_timestamp();\n\t}\nout:\n\treturn newsk;\n}\nEXPORT_SYMBOL_GPL(sk_clone_lock);\n\nvoid sk_setup_caps(struct sock *sk, struct dst_entry *dst)\n{\n\t__sk_dst_set(sk, dst);\n\tsk->sk_route_caps = dst->dev->features;\n\tif (sk->sk_route_caps & NETIF_F_GSO)\n\t\tsk->sk_route_caps |= NETIF_F_GSO_SOFTWARE;\n\tsk->sk_route_caps &= ~sk->sk_route_nocaps;\n\tif (sk_can_gso(sk)) {\n\t\tif (dst->header_len) {\n\t\t\tsk->sk_route_caps &= ~NETIF_F_GSO_MASK;\n\t\t} else {\n\t\t\tsk->sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM;\n\t\t\tsk->sk_gso_max_size = dst->dev->gso_max_size;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(sk_setup_caps);\n\nvoid __init sk_init(void)\n{\n\tif (totalram_pages <= 4096) {\n\t\tsysctl_wmem_max = 32767;\n\t\tsysctl_rmem_max = 32767;\n\t\tsysctl_wmem_default = 32767;\n\t\tsysctl_rmem_default = 32767;\n\t} else if (totalram_pages >= 131072) {\n\t\tsysctl_wmem_max = 131071;\n\t\tsysctl_rmem_max = 131071;\n\t}\n}\n\n/*\n *\tSimple resource managers for sockets.\n */\n\n\n/*\n * Write buffer destructor automatically called from kfree_skb.\n */\nvoid sock_wfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tunsigned int len = skb->truesize;\n\n\tif (!sock_flag(sk, SOCK_USE_WRITE_QUEUE)) {\n\t\t/*\n\t\t * Keep a reference on sk_wmem_alloc, this will be released\n\t\t * after sk_write_space() call\n\t\t */\n\t\tatomic_sub(len - 1, &sk->sk_wmem_alloc);\n\t\tsk->sk_write_space(sk);\n\t\tlen = 1;\n\t}\n\t/*\n\t * if sk_wmem_alloc reaches 0, we must finish what sk_free()\n\t * could not do because of in-flight packets\n\t */\n\tif (atomic_sub_and_test(len, &sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\nEXPORT_SYMBOL(sock_wfree);\n\n/*\n * Read buffer destructor automatically called from kfree_skb.\n */\nvoid sock_rfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tunsigned int len = skb->truesize;\n\n\tatomic_sub(len, &sk->sk_rmem_alloc);\n\tsk_mem_uncharge(sk, len);\n}\nEXPORT_SYMBOL(sock_rfree);\n\n\nint sock_i_uid(struct sock *sk)\n{\n\tint uid;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tuid = sk->sk_socket ? SOCK_INODE(sk->sk_socket)->i_uid : 0;\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn uid;\n}\nEXPORT_SYMBOL(sock_i_uid);\n\nunsigned long sock_i_ino(struct sock *sk)\n{\n\tunsigned long ino;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tino = sk->sk_socket ? SOCK_INODE(sk->sk_socket)->i_ino : 0;\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn ino;\n}\nEXPORT_SYMBOL(sock_i_ino);\n\n/*\n * Allocate a skb from the socket's send buffer.\n */\nstruct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,\n\t\t\t     gfp_t priority)\n{\n\tif (force || atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {\n\t\tstruct sk_buff *skb = alloc_skb(size, priority);\n\t\tif (skb) {\n\t\t\tskb_set_owner_w(skb, sk);\n\t\t\treturn skb;\n\t\t}\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_wmalloc);\n\n/*\n * Allocate a skb from the socket's receive buffer.\n */\nstruct sk_buff *sock_rmalloc(struct sock *sk, unsigned long size, int force,\n\t\t\t     gfp_t priority)\n{\n\tif (force || atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\tstruct sk_buff *skb = alloc_skb(size, priority);\n\t\tif (skb) {\n\t\t\tskb_set_owner_r(skb, sk);\n\t\t\treturn skb;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n/*\n * Allocate a memory block from the socket's option memory buffer.\n */\nvoid *sock_kmalloc(struct sock *sk, int size, gfp_t priority)\n{\n\tif ((unsigned int)size <= sysctl_optmem_max &&\n\t    atomic_read(&sk->sk_omem_alloc) + size < sysctl_optmem_max) {\n\t\tvoid *mem;\n\t\t/* First do the add, to avoid the race if kmalloc\n\t\t * might sleep.\n\t\t */\n\t\tatomic_add(size, &sk->sk_omem_alloc);\n\t\tmem = kmalloc(size, priority);\n\t\tif (mem)\n\t\t\treturn mem;\n\t\tatomic_sub(size, &sk->sk_omem_alloc);\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_kmalloc);\n\n/*\n * Free an option memory block.\n */\nvoid sock_kfree_s(struct sock *sk, void *mem, int size)\n{\n\tkfree(mem);\n\tatomic_sub(size, &sk->sk_omem_alloc);\n}\nEXPORT_SYMBOL(sock_kfree_s);\n\n/* It is almost wait_for_tcp_memory minus release_sock/lock_sock.\n   I think, these locks should be removed for datagram sockets.\n */\nstatic long sock_wait_for_wmem(struct sock *sk, long timeo)\n{\n\tDEFINE_WAIT(wait);\n\n\tclear_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\n\tfor (;;) {\n\t\tif (!timeo)\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t\tif (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf)\n\t\t\tbreak;\n\t\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\t\tbreak;\n\t\tif (sk->sk_err)\n\t\t\tbreak;\n\t\ttimeo = schedule_timeout(timeo);\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn timeo;\n}\n\n\n/*\n *\tGeneric send/receive buffer handlers\n */\n\nstruct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,\n\t\t\t\t     unsigned long data_len, int noblock,\n\t\t\t\t     int *errcode)\n{\n\tstruct sk_buff *skb;\n\tgfp_t gfp_mask;\n\tlong timeo;\n\tint err;\n\n\tgfp_mask = sk->sk_allocation;\n\tif (gfp_mask & __GFP_WAIT)\n\t\tgfp_mask |= __GFP_REPEAT;\n\n\ttimeo = sock_sndtimeo(sk, noblock);\n\twhile (1) {\n\t\terr = sock_error(sk);\n\t\tif (err != 0)\n\t\t\tgoto failure;\n\n\t\terr = -EPIPE;\n\t\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\t\tgoto failure;\n\n\t\tif (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {\n\t\t\tskb = alloc_skb(header_len, gfp_mask);\n\t\t\tif (skb) {\n\t\t\t\tint npages;\n\t\t\t\tint i;\n\n\t\t\t\t/* No pages, we're done... */\n\t\t\t\tif (!data_len)\n\t\t\t\t\tbreak;\n\n\t\t\t\tnpages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;\n\t\t\t\tskb->truesize += data_len;\n\t\t\t\tskb_shinfo(skb)->nr_frags = npages;\n\t\t\t\tfor (i = 0; i < npages; i++) {\n\t\t\t\t\tstruct page *page;\n\n\t\t\t\t\tpage = alloc_pages(sk->sk_allocation, 0);\n\t\t\t\t\tif (!page) {\n\t\t\t\t\t\terr = -ENOBUFS;\n\t\t\t\t\t\tskb_shinfo(skb)->nr_frags = i;\n\t\t\t\t\t\tkfree_skb(skb);\n\t\t\t\t\t\tgoto failure;\n\t\t\t\t\t}\n\n\t\t\t\t\t__skb_fill_page_desc(skb, i,\n\t\t\t\t\t\t\tpage, 0,\n\t\t\t\t\t\t\t(data_len >= PAGE_SIZE ?\n\t\t\t\t\t\t\t PAGE_SIZE :\n\t\t\t\t\t\t\t data_len));\n\t\t\t\t\tdata_len -= PAGE_SIZE;\n\t\t\t\t}\n\n\t\t\t\t/* Full success... */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto failure;\n\t\t}\n\t\tset_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto failure;\n\t\tif (signal_pending(current))\n\t\t\tgoto interrupted;\n\t\ttimeo = sock_wait_for_wmem(sk, timeo);\n\t}\n\n\tskb_set_owner_w(skb, sk);\n\treturn skb;\n\ninterrupted:\n\terr = sock_intr_errno(timeo);\nfailure:\n\t*errcode = err;\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_alloc_send_pskb);\n\nstruct sk_buff *sock_alloc_send_skb(struct sock *sk, unsigned long size,\n\t\t\t\t    int noblock, int *errcode)\n{\n\treturn sock_alloc_send_pskb(sk, size, 0, noblock, errcode);\n}\nEXPORT_SYMBOL(sock_alloc_send_skb);\n\nstatic void __lock_sock(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n\t__acquires(&sk->sk_lock.slock)\n{\n\tDEFINE_WAIT(wait);\n\n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(&sk->sk_lock.wq, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tspin_unlock_bh(&sk->sk_lock.slock);\n\t\tschedule();\n\t\tspin_lock_bh(&sk->sk_lock.slock);\n\t\tif (!sock_owned_by_user(sk))\n\t\t\tbreak;\n\t}\n\tfinish_wait(&sk->sk_lock.wq, &wait);\n}\n\nstatic void __release_sock(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n\t__acquires(&sk->sk_lock.slock)\n{\n\tstruct sk_buff *skb = sk->sk_backlog.head;\n\n\tdo {\n\t\tsk->sk_backlog.head = sk->sk_backlog.tail = NULL;\n\t\tbh_unlock_sock(sk);\n\n\t\tdo {\n\t\t\tstruct sk_buff *next = skb->next;\n\n\t\t\tWARN_ON_ONCE(skb_dst_is_noref(skb));\n\t\t\tskb->next = NULL;\n\t\t\tsk_backlog_rcv(sk, skb);\n\n\t\t\t/*\n\t\t\t * We are in process context here with softirqs\n\t\t\t * disabled, use cond_resched_softirq() to preempt.\n\t\t\t * This is safe to do because we've taken the backlog\n\t\t\t * queue private:\n\t\t\t */\n\t\t\tcond_resched_softirq();\n\n\t\t\tskb = next;\n\t\t} while (skb != NULL);\n\n\t\tbh_lock_sock(sk);\n\t} while ((skb = sk->sk_backlog.head) != NULL);\n\n\t/*\n\t * Doing the zeroing here guarantee we can not loop forever\n\t * while a wild producer attempts to flood us.\n\t */\n\tsk->sk_backlog.len = 0;\n}\n\n/**\n * sk_wait_data - wait for data to arrive at sk_receive_queue\n * @sk:    sock to wait on\n * @timeo: for how long\n *\n * Now socket state including sk->sk_err is changed only under lock,\n * hence we may omit checks after joining wait queue.\n * We check receive queue before schedule() only as optimization;\n * it is very likely that release_sock() added new data.\n */\nint sk_wait_data(struct sock *sk, long *timeo)\n{\n\tint rc;\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\tset_bit(SOCK_ASYNC_WAITDATA, &sk->sk_socket->flags);\n\trc = sk_wait_event(sk, timeo, !skb_queue_empty(&sk->sk_receive_queue));\n\tclear_bit(SOCK_ASYNC_WAITDATA, &sk->sk_socket->flags);\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn rc;\n}\nEXPORT_SYMBOL(sk_wait_data);\n\n/**\n *\t__sk_mem_schedule - increase sk_forward_alloc and memory_allocated\n *\t@sk: socket\n *\t@size: memory size to allocate\n *\t@kind: allocation type\n *\n *\tIf kind is SK_MEM_SEND, it means wmem allocation. Otherwise it means\n *\trmem allocation. This function assumes that protocols which have\n *\tmemory_pressure use sk_wmem_queued as write buffer accounting.\n */\nint __sk_mem_schedule(struct sock *sk, int size, int kind)\n{\n\tstruct proto *prot = sk->sk_prot;\n\tint amt = sk_mem_pages(size);\n\tlong allocated;\n\tint parent_status = UNDER_LIMIT;\n\n\tsk->sk_forward_alloc += amt * SK_MEM_QUANTUM;\n\n\tallocated = sk_memory_allocated_add(sk, amt, &parent_status);\n\n\t/* Under limit. */\n\tif (parent_status == UNDER_LIMIT &&\n\t\t\tallocated <= sk_prot_mem_limits(sk, 0)) {\n\t\tsk_leave_memory_pressure(sk);\n\t\treturn 1;\n\t}\n\n\t/* Under pressure. (we or our parents) */\n\tif ((parent_status > SOFT_LIMIT) ||\n\t\t\tallocated > sk_prot_mem_limits(sk, 1))\n\t\tsk_enter_memory_pressure(sk);\n\n\t/* Over hard limit (we or our parents) */\n\tif ((parent_status == OVER_LIMIT) ||\n\t\t\t(allocated > sk_prot_mem_limits(sk, 2)))\n\t\tgoto suppress_allocation;\n\n\t/* guarantee minimum buffer size under pressure */\n\tif (kind == SK_MEM_RECV) {\n\t\tif (atomic_read(&sk->sk_rmem_alloc) < prot->sysctl_rmem[0])\n\t\t\treturn 1;\n\n\t} else { /* SK_MEM_SEND */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tif (sk->sk_wmem_queued < prot->sysctl_wmem[0])\n\t\t\t\treturn 1;\n\t\t} else if (atomic_read(&sk->sk_wmem_alloc) <\n\t\t\t   prot->sysctl_wmem[0])\n\t\t\t\treturn 1;\n\t}\n\n\tif (sk_has_memory_pressure(sk)) {\n\t\tint alloc;\n\n\t\tif (!sk_under_memory_pressure(sk))\n\t\t\treturn 1;\n\t\talloc = sk_sockets_allocated_read_positive(sk);\n\t\tif (sk_prot_mem_limits(sk, 2) > alloc *\n\t\t    sk_mem_pages(sk->sk_wmem_queued +\n\t\t\t\t atomic_read(&sk->sk_rmem_alloc) +\n\t\t\t\t sk->sk_forward_alloc))\n\t\t\treturn 1;\n\t}\n\nsuppress_allocation:\n\n\tif (kind == SK_MEM_SEND && sk->sk_type == SOCK_STREAM) {\n\t\tsk_stream_moderate_sndbuf(sk);\n\n\t\t/* Fail only if socket is _under_ its sndbuf.\n\t\t * In this case we cannot block, so that we have to fail.\n\t\t */\n\t\tif (sk->sk_wmem_queued + size >= sk->sk_sndbuf)\n\t\t\treturn 1;\n\t}\n\n\ttrace_sock_exceed_buf_limit(sk, prot, allocated);\n\n\t/* Alas. Undo changes. */\n\tsk->sk_forward_alloc -= amt * SK_MEM_QUANTUM;\n\n\tsk_memory_allocated_sub(sk, amt);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(__sk_mem_schedule);\n\n/**\n *\t__sk_reclaim - reclaim memory_allocated\n *\t@sk: socket\n */\nvoid __sk_mem_reclaim(struct sock *sk)\n{\n\tsk_memory_allocated_sub(sk,\n\t\t\t\tsk->sk_forward_alloc >> SK_MEM_QUANTUM_SHIFT);\n\tsk->sk_forward_alloc &= SK_MEM_QUANTUM - 1;\n\n\tif (sk_under_memory_pressure(sk) &&\n\t    (sk_memory_allocated(sk) < sk_prot_mem_limits(sk, 0)))\n\t\tsk_leave_memory_pressure(sk);\n}\nEXPORT_SYMBOL(__sk_mem_reclaim);\n\n\n/*\n * Set of default routines for initialising struct proto_ops when\n * the protocol does not support a particular function. In certain\n * cases where it makes no sense for a protocol to have a \"do nothing\"\n * function, some default processing is provided.\n */\n\nint sock_no_bind(struct socket *sock, struct sockaddr *saddr, int len)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_bind);\n\nint sock_no_connect(struct socket *sock, struct sockaddr *saddr,\n\t\t    int len, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_connect);\n\nint sock_no_socketpair(struct socket *sock1, struct socket *sock2)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_socketpair);\n\nint sock_no_accept(struct socket *sock, struct socket *newsock, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_accept);\n\nint sock_no_getname(struct socket *sock, struct sockaddr *saddr,\n\t\t    int *len, int peer)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_getname);\n\nunsigned int sock_no_poll(struct file *file, struct socket *sock, poll_table *pt)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_no_poll);\n\nint sock_no_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_ioctl);\n\nint sock_no_listen(struct socket *sock, int backlog)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_listen);\n\nint sock_no_shutdown(struct socket *sock, int how)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_shutdown);\n\nint sock_no_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_setsockopt);\n\nint sock_no_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_getsockopt);\n\nint sock_no_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *m,\n\t\t    size_t len)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_sendmsg);\n\nint sock_no_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *m,\n\t\t    size_t len, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_recvmsg);\n\nint sock_no_mmap(struct file *file, struct socket *sock, struct vm_area_struct *vma)\n{\n\t/* Mirror missing mmap method error code */\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL(sock_no_mmap);\n\nssize_t sock_no_sendpage(struct socket *sock, struct page *page, int offset, size_t size, int flags)\n{\n\tssize_t res;\n\tstruct msghdr msg = {.msg_flags = flags};\n\tstruct kvec iov;\n\tchar *kaddr = kmap(page);\n\tiov.iov_base = kaddr + offset;\n\tiov.iov_len = size;\n\tres = kernel_sendmsg(sock, &msg, &iov, 1, size);\n\tkunmap(page);\n\treturn res;\n}\nEXPORT_SYMBOL(sock_no_sendpage);\n\n/*\n *\tDefault Socket Callbacks\n */\n\nstatic void sock_def_wakeup(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (wq_has_sleeper(wq))\n\t\twake_up_interruptible_all(&wq->wait);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_error_report(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (wq_has_sleeper(wq))\n\t\twake_up_interruptible_poll(&wq->wait, POLLERR);\n\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_ERR);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_readable(struct sock *sk, int len)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (wq_has_sleeper(wq))\n\t\twake_up_interruptible_sync_poll(&wq->wait, POLLIN | POLLPRI |\n\t\t\t\t\t\tPOLLRDNORM | POLLRDBAND);\n\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_write_space(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\n\t/* Do not wake up a writer until he can make \"significant\"\n\t * progress.  --DaveM\n\t */\n\tif ((atomic_read(&sk->sk_wmem_alloc) << 1) <= sk->sk_sndbuf) {\n\t\twq = rcu_dereference(sk->sk_wq);\n\t\tif (wq_has_sleeper(wq))\n\t\t\twake_up_interruptible_sync_poll(&wq->wait, POLLOUT |\n\t\t\t\t\t\tPOLLWRNORM | POLLWRBAND);\n\n\t\t/* Should agree with poll, otherwise some programs break */\n\t\tif (sock_writeable(sk))\n\t\t\tsk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT);\n\t}\n\n\trcu_read_unlock();\n}\n\nstatic void sock_def_destruct(struct sock *sk)\n{\n\tkfree(sk->sk_protinfo);\n}\n\nvoid sk_send_sigurg(struct sock *sk)\n{\n\tif (sk->sk_socket && sk->sk_socket->file)\n\t\tif (send_sigurg(&sk->sk_socket->file->f_owner))\n\t\t\tsk_wake_async(sk, SOCK_WAKE_URG, POLL_PRI);\n}\nEXPORT_SYMBOL(sk_send_sigurg);\n\nvoid sk_reset_timer(struct sock *sk, struct timer_list* timer,\n\t\t    unsigned long expires)\n{\n\tif (!mod_timer(timer, expires))\n\t\tsock_hold(sk);\n}\nEXPORT_SYMBOL(sk_reset_timer);\n\nvoid sk_stop_timer(struct sock *sk, struct timer_list* timer)\n{\n\tif (timer_pending(timer) && del_timer(timer))\n\t\t__sock_put(sk);\n}\nEXPORT_SYMBOL(sk_stop_timer);\n\nvoid sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tskb_queue_head_init(&sk->sk_receive_queue);\n\tskb_queue_head_init(&sk->sk_write_queue);\n\tskb_queue_head_init(&sk->sk_error_queue);\n#ifdef CONFIG_NET_DMA\n\tskb_queue_head_init(&sk->sk_async_wait_queue);\n#endif\n\n\tsk->sk_send_head\t=\tNULL;\n\n\tinit_timer(&sk->sk_timer);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tsk->sk_wq\t=\tsock->wq;\n\t\tsock->sk\t=\tsk;\n\t} else\n\t\tsk->sk_wq\t=\tNULL;\n\n\tspin_lock_init(&sk->sk_dst_lock);\n\trwlock_init(&sk->sk_callback_lock);\n\tlockdep_set_class_and_name(&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_sndmsg_page\t=\tNULL;\n\tsk->sk_sndmsg_off\t=\t0;\n\tsk->sk_peek_off\t\t=\t-1;\n\n\tsk->sk_peer_pid \t=\tNULL;\n\tsk->sk_peer_cred\t=\tNULL;\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = ktime_set(-1L, 0);\n\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t */\n\tsmp_wmb();\n\tatomic_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}\nEXPORT_SYMBOL(sock_init_data);\n\nvoid lock_sock_nested(struct sock *sk, int subclass)\n{\n\tmight_sleep();\n\tspin_lock_bh(&sk->sk_lock.slock);\n\tif (sk->sk_lock.owned)\n\t\t__lock_sock(sk);\n\tsk->sk_lock.owned = 1;\n\tspin_unlock(&sk->sk_lock.slock);\n\t/*\n\t * The sk_lock has mutex_lock() semantics here:\n\t */\n\tmutex_acquire(&sk->sk_lock.dep_map, subclass, 0, _RET_IP_);\n\tlocal_bh_enable();\n}\nEXPORT_SYMBOL(lock_sock_nested);\n\nvoid release_sock(struct sock *sk)\n{\n\t/*\n\t * The sk_lock has mutex_unlock() semantics:\n\t */\n\tmutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);\n\n\tspin_lock_bh(&sk->sk_lock.slock);\n\tif (sk->sk_backlog.tail)\n\t\t__release_sock(sk);\n\tsk->sk_lock.owned = 0;\n\tif (waitqueue_active(&sk->sk_lock.wq))\n\t\twake_up(&sk->sk_lock.wq);\n\tspin_unlock_bh(&sk->sk_lock.slock);\n}\nEXPORT_SYMBOL(release_sock);\n\n/**\n * lock_sock_fast - fast version of lock_sock\n * @sk: socket\n *\n * This version should be used for very small section, where process wont block\n * return false if fast path is taken\n *   sk_lock.slock locked, owned = 0, BH disabled\n * return true if slow path is taken\n *   sk_lock.slock unlocked, owned = 1, BH enabled\n */\nbool lock_sock_fast(struct sock *sk)\n{\n\tmight_sleep();\n\tspin_lock_bh(&sk->sk_lock.slock);\n\n\tif (!sk->sk_lock.owned)\n\t\t/*\n\t\t * Note : We must disable BH\n\t\t */\n\t\treturn false;\n\n\t__lock_sock(sk);\n\tsk->sk_lock.owned = 1;\n\tspin_unlock(&sk->sk_lock.slock);\n\t/*\n\t * The sk_lock has mutex_lock() semantics here:\n\t */\n\tmutex_acquire(&sk->sk_lock.dep_map, 0, 0, _RET_IP_);\n\tlocal_bh_enable();\n\treturn true;\n}\nEXPORT_SYMBOL(lock_sock_fast);\n\nint sock_get_timestamp(struct sock *sk, struct timeval __user *userstamp)\n{\n\tstruct timeval tv;\n\tif (!sock_flag(sk, SOCK_TIMESTAMP))\n\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\ttv = ktime_to_timeval(sk->sk_stamp);\n\tif (tv.tv_sec == -1)\n\t\treturn -ENOENT;\n\tif (tv.tv_sec == 0) {\n\t\tsk->sk_stamp = ktime_get_real();\n\t\ttv = ktime_to_timeval(sk->sk_stamp);\n\t}\n\treturn copy_to_user(userstamp, &tv, sizeof(tv)) ? -EFAULT : 0;\n}\nEXPORT_SYMBOL(sock_get_timestamp);\n\nint sock_get_timestampns(struct sock *sk, struct timespec __user *userstamp)\n{\n\tstruct timespec ts;\n\tif (!sock_flag(sk, SOCK_TIMESTAMP))\n\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\tts = ktime_to_timespec(sk->sk_stamp);\n\tif (ts.tv_sec == -1)\n\t\treturn -ENOENT;\n\tif (ts.tv_sec == 0) {\n\t\tsk->sk_stamp = ktime_get_real();\n\t\tts = ktime_to_timespec(sk->sk_stamp);\n\t}\n\treturn copy_to_user(userstamp, &ts, sizeof(ts)) ? -EFAULT : 0;\n}\nEXPORT_SYMBOL(sock_get_timestampns);\n\nvoid sock_enable_timestamp(struct sock *sk, int flag)\n{\n\tif (!sock_flag(sk, flag)) {\n\t\tunsigned long previous_flags = sk->sk_flags;\n\n\t\tsock_set_flag(sk, flag);\n\t\t/*\n\t\t * we just set one of the two flags which require net\n\t\t * time stamping, but time stamping might have been on\n\t\t * already because of the other one\n\t\t */\n\t\tif (!(previous_flags & SK_FLAGS_TIMESTAMP))\n\t\t\tnet_enable_timestamp();\n\t}\n}\n\n/*\n *\tGet a socket option on an socket.\n *\n *\tFIX: POSIX 1003.1g is very ambiguous here. It states that\n *\tasynchronous errors should be reported by getsockopt. We assume\n *\tthis means if you specify SO_ERROR (otherwise whats the point of it).\n */\nint sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(sock_common_getsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t\t  char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk->sk_prot->compat_getsockopt != NULL)\n\t\treturn sk->sk_prot->compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t      optval, optlen);\n\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_sock_common_getsockopt);\n#endif\n\nint sock_common_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\tstruct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint addr_len = 0;\n\tint err;\n\n\terr = sk->sk_prot->recvmsg(iocb, sk, msg, size, flags & MSG_DONTWAIT,\n\t\t\t\t   flags & ~MSG_DONTWAIT, &addr_len);\n\tif (err >= 0)\n\t\tmsg->msg_namelen = addr_len;\n\treturn err;\n}\nEXPORT_SYMBOL(sock_common_recvmsg);\n\n/*\n *\tSet socket options on an inet socket.\n */\nint sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(sock_common_setsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk->sk_prot->compat_setsockopt != NULL)\n\t\treturn sk->sk_prot->compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t      optval, optlen);\n\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_sock_common_setsockopt);\n#endif\n\nvoid sk_common_release(struct sock *sk)\n{\n\tif (sk->sk_prot->destroy)\n\t\tsk->sk_prot->destroy(sk);\n\n\t/*\n\t * Observation: when sock_common_release is called, processes have\n\t * no access to socket. But net still has.\n\t * Step one, detach it from networking:\n\t *\n\t * A. Remove from hash tables.\n\t */\n\n\tsk->sk_prot->unhash(sk);\n\n\t/*\n\t * In this point socket cannot receive new packets, but it is possible\n\t * that some packets are in flight because some CPU runs receiver and\n\t * did hash table lookup before we unhashed socket. They will achieve\n\t * receive queue and will be purged by socket destructor.\n\t *\n\t * Also we still have packets pending on receive queue and probably,\n\t * our own packets waiting in device queues. sock_destroy will drain\n\t * receive queue, but transmitted packets will delay socket destruction\n\t * until the last reference will be released.\n\t */\n\n\tsock_orphan(sk);\n\n\txfrm_sk_free_policy(sk);\n\n\tsk_refcnt_debug_release(sk);\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(sk_common_release);\n\n#ifdef CONFIG_PROC_FS\n#define PROTO_INUSE_NR\t64\t/* should be enough for the first time */\nstruct prot_inuse {\n\tint val[PROTO_INUSE_NR];\n};\n\nstatic DECLARE_BITMAP(proto_inuse_idx, PROTO_INUSE_NR);\n\n#ifdef CONFIG_NET_NS\nvoid sock_prot_inuse_add(struct net *net, struct proto *prot, int val)\n{\n\t__this_cpu_add(net->core.inuse->val[prot->inuse_idx], val);\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_add);\n\nint sock_prot_inuse_get(struct net *net, struct proto *prot)\n{\n\tint cpu, idx = prot->inuse_idx;\n\tint res = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tres += per_cpu_ptr(net->core.inuse, cpu)->val[idx];\n\n\treturn res >= 0 ? res : 0;\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_get);\n\nstatic int __net_init sock_inuse_init_net(struct net *net)\n{\n\tnet->core.inuse = alloc_percpu(struct prot_inuse);\n\treturn net->core.inuse ? 0 : -ENOMEM;\n}\n\nstatic void __net_exit sock_inuse_exit_net(struct net *net)\n{\n\tfree_percpu(net->core.inuse);\n}\n\nstatic struct pernet_operations net_inuse_ops = {\n\t.init = sock_inuse_init_net,\n\t.exit = sock_inuse_exit_net,\n};\n\nstatic __init int net_inuse_init(void)\n{\n\tif (register_pernet_subsys(&net_inuse_ops))\n\t\tpanic(\"Cannot initialize net inuse counters\");\n\n\treturn 0;\n}\n\ncore_initcall(net_inuse_init);\n#else\nstatic DEFINE_PER_CPU(struct prot_inuse, prot_inuse);\n\nvoid sock_prot_inuse_add(struct net *net, struct proto *prot, int val)\n{\n\t__this_cpu_add(prot_inuse.val[prot->inuse_idx], val);\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_add);\n\nint sock_prot_inuse_get(struct net *net, struct proto *prot)\n{\n\tint cpu, idx = prot->inuse_idx;\n\tint res = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tres += per_cpu(prot_inuse, cpu).val[idx];\n\n\treturn res >= 0 ? res : 0;\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_get);\n#endif\n\nstatic void assign_proto_idx(struct proto *prot)\n{\n\tprot->inuse_idx = find_first_zero_bit(proto_inuse_idx, PROTO_INUSE_NR);\n\n\tif (unlikely(prot->inuse_idx == PROTO_INUSE_NR - 1)) {\n\t\tprintk(KERN_ERR \"PROTO_INUSE_NR exhausted\\n\");\n\t\treturn;\n\t}\n\n\tset_bit(prot->inuse_idx, proto_inuse_idx);\n}\n\nstatic void release_proto_idx(struct proto *prot)\n{\n\tif (prot->inuse_idx != PROTO_INUSE_NR - 1)\n\t\tclear_bit(prot->inuse_idx, proto_inuse_idx);\n}\n#else\nstatic inline void assign_proto_idx(struct proto *prot)\n{\n}\n\nstatic inline void release_proto_idx(struct proto *prot)\n{\n}\n#endif\n\nint proto_register(struct proto *prot, int alloc_slab)\n{\n\tif (alloc_slab) {\n\t\tprot->slab = kmem_cache_create(prot->name, prot->obj_size, 0,\n\t\t\t\t\tSLAB_HWCACHE_ALIGN | prot->slab_flags,\n\t\t\t\t\tNULL);\n\n\t\tif (prot->slab == NULL) {\n\t\t\tprintk(KERN_CRIT \"%s: Can't create sock SLAB cache!\\n\",\n\t\t\t       prot->name);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (prot->rsk_prot != NULL) {\n\t\t\tprot->rsk_prot->slab_name = kasprintf(GFP_KERNEL, \"request_sock_%s\", prot->name);\n\t\t\tif (prot->rsk_prot->slab_name == NULL)\n\t\t\t\tgoto out_free_sock_slab;\n\n\t\t\tprot->rsk_prot->slab = kmem_cache_create(prot->rsk_prot->slab_name,\n\t\t\t\t\t\t\t\t prot->rsk_prot->obj_size, 0,\n\t\t\t\t\t\t\t\t SLAB_HWCACHE_ALIGN, NULL);\n\n\t\t\tif (prot->rsk_prot->slab == NULL) {\n\t\t\t\tprintk(KERN_CRIT \"%s: Can't create request sock SLAB cache!\\n\",\n\t\t\t\t       prot->name);\n\t\t\t\tgoto out_free_request_sock_slab_name;\n\t\t\t}\n\t\t}\n\n\t\tif (prot->twsk_prot != NULL) {\n\t\t\tprot->twsk_prot->twsk_slab_name = kasprintf(GFP_KERNEL, \"tw_sock_%s\", prot->name);\n\n\t\t\tif (prot->twsk_prot->twsk_slab_name == NULL)\n\t\t\t\tgoto out_free_request_sock_slab;\n\n\t\t\tprot->twsk_prot->twsk_slab =\n\t\t\t\tkmem_cache_create(prot->twsk_prot->twsk_slab_name,\n\t\t\t\t\t\t  prot->twsk_prot->twsk_obj_size,\n\t\t\t\t\t\t  0,\n\t\t\t\t\t\t  SLAB_HWCACHE_ALIGN |\n\t\t\t\t\t\t\tprot->slab_flags,\n\t\t\t\t\t\t  NULL);\n\t\t\tif (prot->twsk_prot->twsk_slab == NULL)\n\t\t\t\tgoto out_free_timewait_sock_slab_name;\n\t\t}\n\t}\n\n\tmutex_lock(&proto_list_mutex);\n\tlist_add(&prot->node, &proto_list);\n\tassign_proto_idx(prot);\n\tmutex_unlock(&proto_list_mutex);\n\treturn 0;\n\nout_free_timewait_sock_slab_name:\n\tkfree(prot->twsk_prot->twsk_slab_name);\nout_free_request_sock_slab:\n\tif (prot->rsk_prot && prot->rsk_prot->slab) {\n\t\tkmem_cache_destroy(prot->rsk_prot->slab);\n\t\tprot->rsk_prot->slab = NULL;\n\t}\nout_free_request_sock_slab_name:\n\tif (prot->rsk_prot)\n\t\tkfree(prot->rsk_prot->slab_name);\nout_free_sock_slab:\n\tkmem_cache_destroy(prot->slab);\n\tprot->slab = NULL;\nout:\n\treturn -ENOBUFS;\n}\nEXPORT_SYMBOL(proto_register);\n\nvoid proto_unregister(struct proto *prot)\n{\n\tmutex_lock(&proto_list_mutex);\n\trelease_proto_idx(prot);\n\tlist_del(&prot->node);\n\tmutex_unlock(&proto_list_mutex);\n\n\tif (prot->slab != NULL) {\n\t\tkmem_cache_destroy(prot->slab);\n\t\tprot->slab = NULL;\n\t}\n\n\tif (prot->rsk_prot != NULL && prot->rsk_prot->slab != NULL) {\n\t\tkmem_cache_destroy(prot->rsk_prot->slab);\n\t\tkfree(prot->rsk_prot->slab_name);\n\t\tprot->rsk_prot->slab = NULL;\n\t}\n\n\tif (prot->twsk_prot != NULL && prot->twsk_prot->twsk_slab != NULL) {\n\t\tkmem_cache_destroy(prot->twsk_prot->twsk_slab);\n\t\tkfree(prot->twsk_prot->twsk_slab_name);\n\t\tprot->twsk_prot->twsk_slab = NULL;\n\t}\n}\nEXPORT_SYMBOL(proto_unregister);\n\n#ifdef CONFIG_PROC_FS\nstatic void *proto_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(proto_list_mutex)\n{\n\tmutex_lock(&proto_list_mutex);\n\treturn seq_list_start_head(&proto_list, *pos);\n}\n\nstatic void *proto_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\treturn seq_list_next(v, &proto_list, pos);\n}\n\nstatic void proto_seq_stop(struct seq_file *seq, void *v)\n\t__releases(proto_list_mutex)\n{\n\tmutex_unlock(&proto_list_mutex);\n}\n\nstatic char proto_method_implemented(const void *method)\n{\n\treturn method == NULL ? 'n' : 'y';\n}\nstatic long sock_prot_memory_allocated(struct proto *proto)\n{\n\treturn proto->memory_allocated != NULL ? proto_memory_allocated(proto): -1L;\n}\n\nstatic char *sock_prot_memory_pressure(struct proto *proto)\n{\n\treturn proto->memory_pressure != NULL ?\n\tproto_memory_pressure(proto) ? \"yes\" : \"no\" : \"NI\";\n}\n\nstatic void proto_seq_printf(struct seq_file *seq, struct proto *proto)\n{\n\n\tseq_printf(seq, \"%-9s %4u %6d  %6ld   %-3s %6u   %-3s  %-10s \"\n\t\t\t\"%2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c\\n\",\n\t\t   proto->name,\n\t\t   proto->obj_size,\n\t\t   sock_prot_inuse_get(seq_file_net(seq), proto),\n\t\t   sock_prot_memory_allocated(proto),\n\t\t   sock_prot_memory_pressure(proto),\n\t\t   proto->max_header,\n\t\t   proto->slab == NULL ? \"no\" : \"yes\",\n\t\t   module_name(proto->owner),\n\t\t   proto_method_implemented(proto->close),\n\t\t   proto_method_implemented(proto->connect),\n\t\t   proto_method_implemented(proto->disconnect),\n\t\t   proto_method_implemented(proto->accept),\n\t\t   proto_method_implemented(proto->ioctl),\n\t\t   proto_method_implemented(proto->init),\n\t\t   proto_method_implemented(proto->destroy),\n\t\t   proto_method_implemented(proto->shutdown),\n\t\t   proto_method_implemented(proto->setsockopt),\n\t\t   proto_method_implemented(proto->getsockopt),\n\t\t   proto_method_implemented(proto->sendmsg),\n\t\t   proto_method_implemented(proto->recvmsg),\n\t\t   proto_method_implemented(proto->sendpage),\n\t\t   proto_method_implemented(proto->bind),\n\t\t   proto_method_implemented(proto->backlog_rcv),\n\t\t   proto_method_implemented(proto->hash),\n\t\t   proto_method_implemented(proto->unhash),\n\t\t   proto_method_implemented(proto->get_port),\n\t\t   proto_method_implemented(proto->enter_memory_pressure));\n}\n\nstatic int proto_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == &proto_list)\n\t\tseq_printf(seq, \"%-9s %-4s %-8s %-6s %-5s %-7s %-4s %-10s %s\",\n\t\t\t   \"protocol\",\n\t\t\t   \"size\",\n\t\t\t   \"sockets\",\n\t\t\t   \"memory\",\n\t\t\t   \"press\",\n\t\t\t   \"maxhdr\",\n\t\t\t   \"slab\",\n\t\t\t   \"module\",\n\t\t\t   \"cl co di ac io in de sh ss gs se re sp bi br ha uh gp em\\n\");\n\telse\n\t\tproto_seq_printf(seq, list_entry(v, struct proto, node));\n\treturn 0;\n}\n\nstatic const struct seq_operations proto_seq_ops = {\n\t.start  = proto_seq_start,\n\t.next   = proto_seq_next,\n\t.stop   = proto_seq_stop,\n\t.show   = proto_seq_show,\n};\n\nstatic int proto_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &proto_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations proto_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= proto_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\nstatic __net_init int proto_init_net(struct net *net)\n{\n\tif (!proc_net_fops_create(net, \"protocols\", S_IRUGO, &proto_seq_fops))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic __net_exit void proto_exit_net(struct net *net)\n{\n\tproc_net_remove(net, \"protocols\");\n}\n\n\nstatic __net_initdata struct pernet_operations proto_net_ops = {\n\t.init = proto_init_net,\n\t.exit = proto_exit_net,\n};\n\nstatic int __init proto_init(void)\n{\n\treturn register_pernet_subsys(&proto_net_ops);\n}\n\nsubsys_initcall(proto_init);\n\n#endif /* PROC_FS */\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tGeneric socket support routines. Memory allocators, socket lock/release\n *\t\thandler for protocols to use and generic option handler.\n *\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tAlan Cox, <A.Cox@swansea.ac.uk>\n *\n * Fixes:\n *\t\tAlan Cox\t: \tNumerous verify_area() problems\n *\t\tAlan Cox\t:\tConnecting on a connecting socket\n *\t\t\t\t\tnow returns an error for tcp.\n *\t\tAlan Cox\t:\tsock->protocol is set correctly.\n *\t\t\t\t\tand is not sometimes left as 0.\n *\t\tAlan Cox\t:\tconnect handles icmp errors on a\n *\t\t\t\t\tconnect properly. Unfortunately there\n *\t\t\t\t\tis a restart syscall nasty there. I\n *\t\t\t\t\tcan't match BSD without hacking the C\n *\t\t\t\t\tlibrary. Ideas urgently sought!\n *\t\tAlan Cox\t:\tDisallow bind() to addresses that are\n *\t\t\t\t\tnot ours - especially broadcast ones!!\n *\t\tAlan Cox\t:\tSocket 1024 _IS_ ok for users. (fencepost)\n *\t\tAlan Cox\t:\tsock_wfree/sock_rfree don't destroy sockets,\n *\t\t\t\t\tinstead they leave that for the DESTROY timer.\n *\t\tAlan Cox\t:\tClean up error flag in accept\n *\t\tAlan Cox\t:\tTCP ack handling is buggy, the DESTROY timer\n *\t\t\t\t\twas buggy. Put a remove_sock() in the handler\n *\t\t\t\t\tfor memory when we hit 0. Also altered the timer\n *\t\t\t\t\tcode. The ACK stuff can wait and needs major\n *\t\t\t\t\tTCP layer surgery.\n *\t\tAlan Cox\t:\tFixed TCP ack bug, removed remove sock\n *\t\t\t\t\tand fixed timer/inet_bh race.\n *\t\tAlan Cox\t:\tAdded zapped flag for TCP\n *\t\tAlan Cox\t:\tMove kfree_skb into skbuff.c and tidied up surplus code\n *\t\tAlan Cox\t:\tfor new sk_buff allocations wmalloc/rmalloc now call alloc_skb\n *\t\tAlan Cox\t:\tkfree_s calls now are kfree_skbmem so we can track skb resources\n *\t\tAlan Cox\t:\tSupports socket option broadcast now as does udp. Packet and raw need fixing.\n *\t\tAlan Cox\t:\tAdded RCVBUF,SNDBUF size setting. It suddenly occurred to me how easy it was so...\n *\t\tRick Sladkey\t:\tRelaxed UDP rules for matching packets.\n *\t\tC.E.Hawkins\t:\tIFF_PROMISC/SIOCGHWADDR support\n *\tPauline Middelink\t:\tidentd support\n *\t\tAlan Cox\t:\tFixed connect() taking signals I think.\n *\t\tAlan Cox\t:\tSO_LINGER supported\n *\t\tAlan Cox\t:\tError reporting fixes\n *\t\tAnonymous\t:\tinet_create tidied up (sk->reuse setting)\n *\t\tAlan Cox\t:\tinet sockets don't set sk->type!\n *\t\tAlan Cox\t:\tSplit socket option code\n *\t\tAlan Cox\t:\tCallbacks\n *\t\tAlan Cox\t:\tNagle flag for Charles & Johannes stuff\n *\t\tAlex\t\t:\tRemoved restriction on inet fioctl\n *\t\tAlan Cox\t:\tSplitting INET from NET core\n *\t\tAlan Cox\t:\tFixed bogus SO_TYPE handling in getsockopt()\n *\t\tAdam Caldwell\t:\tMissing return in SO_DONTROUTE/SO_DEBUG code\n *\t\tAlan Cox\t:\tSplit IP from generic code\n *\t\tAlan Cox\t:\tNew kfree_skbmem()\n *\t\tAlan Cox\t:\tMake SO_DEBUG superuser only.\n *\t\tAlan Cox\t:\tAllow anyone to clear SO_DEBUG\n *\t\t\t\t\t(compatibility fix)\n *\t\tAlan Cox\t:\tAdded optimistic memory grabbing for AF_UNIX throughput.\n *\t\tAlan Cox\t:\tAllocator for a socket is settable.\n *\t\tAlan Cox\t:\tSO_ERROR includes soft errors.\n *\t\tAlan Cox\t:\tAllow NULL arguments on some SO_ opts\n *\t\tAlan Cox\t: \tGeneric socket allocation to make hooks\n *\t\t\t\t\teasier (suggested by Craig Metz).\n *\t\tMichael Pall\t:\tSO_ERROR returns positive errno again\n *              Steve Whitehouse:       Added default destructor to free\n *                                      protocol private data.\n *              Steve Whitehouse:       Added various other default routines\n *                                      common to several socket families.\n *              Chris Evans     :       Call suser() check last on F_SETOWN\n *\t\tJay Schulist\t:\tAdded SO_ATTACH_FILTER and SO_DETACH_FILTER.\n *\t\tAndi Kleen\t:\tAdd sock_kmalloc()/sock_kfree_s()\n *\t\tAndi Kleen\t:\tFix write_space callback\n *\t\tChris Evans\t:\tSecurity fixes - signedness again\n *\t\tArnaldo C. Melo :       cleanups, use skb_queue_purge\n *\n * To Fix:\n *\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/types.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/sched.h>\n#include <linux/timer.h>\n#include <linux/string.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/poll.h>\n#include <linux/tcp.h>\n#include <linux/init.h>\n#include <linux/highmem.h>\n#include <linux/user_namespace.h>\n#include <linux/static_key.h>\n#include <linux/memcontrol.h>\n\n#include <asm/uaccess.h>\n\n#include <linux/netdevice.h>\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/net_namespace.h>\n#include <net/request_sock.h>\n#include <net/sock.h>\n#include <linux/net_tstamp.h>\n#include <net/xfrm.h>\n#include <linux/ipsec.h>\n#include <net/cls_cgroup.h>\n#include <net/netprio_cgroup.h>\n\n#include <linux/filter.h>\n\n#include <trace/events/sock.h>\n\n#ifdef CONFIG_INET\n#include <net/tcp.h>\n#endif\n\nstatic DEFINE_MUTEX(proto_list_mutex);\nstatic LIST_HEAD(proto_list);\n\n#ifdef CONFIG_CGROUP_MEM_RES_CTLR_KMEM\nint mem_cgroup_sockets_init(struct cgroup *cgrp, struct cgroup_subsys *ss)\n{\n\tstruct proto *proto;\n\tint ret = 0;\n\n\tmutex_lock(&proto_list_mutex);\n\tlist_for_each_entry(proto, &proto_list, node) {\n\t\tif (proto->init_cgroup) {\n\t\t\tret = proto->init_cgroup(cgrp, ss);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmutex_unlock(&proto_list_mutex);\n\treturn ret;\nout:\n\tlist_for_each_entry_continue_reverse(proto, &proto_list, node)\n\t\tif (proto->destroy_cgroup)\n\t\t\tproto->destroy_cgroup(cgrp);\n\tmutex_unlock(&proto_list_mutex);\n\treturn ret;\n}\n\nvoid mem_cgroup_sockets_destroy(struct cgroup *cgrp)\n{\n\tstruct proto *proto;\n\n\tmutex_lock(&proto_list_mutex);\n\tlist_for_each_entry_reverse(proto, &proto_list, node)\n\t\tif (proto->destroy_cgroup)\n\t\t\tproto->destroy_cgroup(cgrp);\n\tmutex_unlock(&proto_list_mutex);\n}\n#endif\n\n/*\n * Each address family might have different locking rules, so we have\n * one slock key per address family:\n */\nstatic struct lock_class_key af_family_keys[AF_MAX];\nstatic struct lock_class_key af_family_slock_keys[AF_MAX];\n\nstruct static_key memcg_socket_limit_enabled;\nEXPORT_SYMBOL(memcg_socket_limit_enabled);\n\n/*\n * Make lock validator output more readable. (we pre-construct these\n * strings build-time, so that runtime initialization of socket\n * locks is fast):\n */\nstatic const char *const af_family_key_strings[AF_MAX+1] = {\n  \"sk_lock-AF_UNSPEC\", \"sk_lock-AF_UNIX\"     , \"sk_lock-AF_INET\"     ,\n  \"sk_lock-AF_AX25\"  , \"sk_lock-AF_IPX\"      , \"sk_lock-AF_APPLETALK\",\n  \"sk_lock-AF_NETROM\", \"sk_lock-AF_BRIDGE\"   , \"sk_lock-AF_ATMPVC\"   ,\n  \"sk_lock-AF_X25\"   , \"sk_lock-AF_INET6\"    , \"sk_lock-AF_ROSE\"     ,\n  \"sk_lock-AF_DECnet\", \"sk_lock-AF_NETBEUI\"  , \"sk_lock-AF_SECURITY\" ,\n  \"sk_lock-AF_KEY\"   , \"sk_lock-AF_NETLINK\"  , \"sk_lock-AF_PACKET\"   ,\n  \"sk_lock-AF_ASH\"   , \"sk_lock-AF_ECONET\"   , \"sk_lock-AF_ATMSVC\"   ,\n  \"sk_lock-AF_RDS\"   , \"sk_lock-AF_SNA\"      , \"sk_lock-AF_IRDA\"     ,\n  \"sk_lock-AF_PPPOX\" , \"sk_lock-AF_WANPIPE\"  , \"sk_lock-AF_LLC\"      ,\n  \"sk_lock-27\"       , \"sk_lock-28\"          , \"sk_lock-AF_CAN\"      ,\n  \"sk_lock-AF_TIPC\"  , \"sk_lock-AF_BLUETOOTH\", \"sk_lock-IUCV\"        ,\n  \"sk_lock-AF_RXRPC\" , \"sk_lock-AF_ISDN\"     , \"sk_lock-AF_PHONET\"   ,\n  \"sk_lock-AF_IEEE802154\", \"sk_lock-AF_CAIF\" , \"sk_lock-AF_ALG\"      ,\n  \"sk_lock-AF_NFC\"   , \"sk_lock-AF_MAX\"\n};\nstatic const char *const af_family_slock_key_strings[AF_MAX+1] = {\n  \"slock-AF_UNSPEC\", \"slock-AF_UNIX\"     , \"slock-AF_INET\"     ,\n  \"slock-AF_AX25\"  , \"slock-AF_IPX\"      , \"slock-AF_APPLETALK\",\n  \"slock-AF_NETROM\", \"slock-AF_BRIDGE\"   , \"slock-AF_ATMPVC\"   ,\n  \"slock-AF_X25\"   , \"slock-AF_INET6\"    , \"slock-AF_ROSE\"     ,\n  \"slock-AF_DECnet\", \"slock-AF_NETBEUI\"  , \"slock-AF_SECURITY\" ,\n  \"slock-AF_KEY\"   , \"slock-AF_NETLINK\"  , \"slock-AF_PACKET\"   ,\n  \"slock-AF_ASH\"   , \"slock-AF_ECONET\"   , \"slock-AF_ATMSVC\"   ,\n  \"slock-AF_RDS\"   , \"slock-AF_SNA\"      , \"slock-AF_IRDA\"     ,\n  \"slock-AF_PPPOX\" , \"slock-AF_WANPIPE\"  , \"slock-AF_LLC\"      ,\n  \"slock-27\"       , \"slock-28\"          , \"slock-AF_CAN\"      ,\n  \"slock-AF_TIPC\"  , \"slock-AF_BLUETOOTH\", \"slock-AF_IUCV\"     ,\n  \"slock-AF_RXRPC\" , \"slock-AF_ISDN\"     , \"slock-AF_PHONET\"   ,\n  \"slock-AF_IEEE802154\", \"slock-AF_CAIF\" , \"slock-AF_ALG\"      ,\n  \"slock-AF_NFC\"   , \"slock-AF_MAX\"\n};\nstatic const char *const af_family_clock_key_strings[AF_MAX+1] = {\n  \"clock-AF_UNSPEC\", \"clock-AF_UNIX\"     , \"clock-AF_INET\"     ,\n  \"clock-AF_AX25\"  , \"clock-AF_IPX\"      , \"clock-AF_APPLETALK\",\n  \"clock-AF_NETROM\", \"clock-AF_BRIDGE\"   , \"clock-AF_ATMPVC\"   ,\n  \"clock-AF_X25\"   , \"clock-AF_INET6\"    , \"clock-AF_ROSE\"     ,\n  \"clock-AF_DECnet\", \"clock-AF_NETBEUI\"  , \"clock-AF_SECURITY\" ,\n  \"clock-AF_KEY\"   , \"clock-AF_NETLINK\"  , \"clock-AF_PACKET\"   ,\n  \"clock-AF_ASH\"   , \"clock-AF_ECONET\"   , \"clock-AF_ATMSVC\"   ,\n  \"clock-AF_RDS\"   , \"clock-AF_SNA\"      , \"clock-AF_IRDA\"     ,\n  \"clock-AF_PPPOX\" , \"clock-AF_WANPIPE\"  , \"clock-AF_LLC\"      ,\n  \"clock-27\"       , \"clock-28\"          , \"clock-AF_CAN\"      ,\n  \"clock-AF_TIPC\"  , \"clock-AF_BLUETOOTH\", \"clock-AF_IUCV\"     ,\n  \"clock-AF_RXRPC\" , \"clock-AF_ISDN\"     , \"clock-AF_PHONET\"   ,\n  \"clock-AF_IEEE802154\", \"clock-AF_CAIF\" , \"clock-AF_ALG\"      ,\n  \"clock-AF_NFC\"   , \"clock-AF_MAX\"\n};\n\n/*\n * sk_callback_lock locking rules are per-address-family,\n * so split the lock classes by using a per-AF key:\n */\nstatic struct lock_class_key af_callback_keys[AF_MAX];\n\n/* Take into consideration the size of the struct sk_buff overhead in the\n * determination of these values, since that is non-constant across\n * platforms.  This makes socket queueing behavior and performance\n * not depend upon such differences.\n */\n#define _SK_MEM_PACKETS\t\t256\n#define _SK_MEM_OVERHEAD\tSKB_TRUESIZE(256)\n#define SK_WMEM_MAX\t\t(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)\n#define SK_RMEM_MAX\t\t(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)\n\n/* Run time adjustable parameters. */\n__u32 sysctl_wmem_max __read_mostly = SK_WMEM_MAX;\n__u32 sysctl_rmem_max __read_mostly = SK_RMEM_MAX;\n__u32 sysctl_wmem_default __read_mostly = SK_WMEM_MAX;\n__u32 sysctl_rmem_default __read_mostly = SK_RMEM_MAX;\n\n/* Maximal space eaten by iovec or ancillary data plus some space */\nint sysctl_optmem_max __read_mostly = sizeof(unsigned long)*(2*UIO_MAXIOV+512);\nEXPORT_SYMBOL(sysctl_optmem_max);\n\n#if defined(CONFIG_CGROUPS)\n#if !defined(CONFIG_NET_CLS_CGROUP)\nint net_cls_subsys_id = -1;\nEXPORT_SYMBOL_GPL(net_cls_subsys_id);\n#endif\n#if !defined(CONFIG_NETPRIO_CGROUP)\nint net_prio_subsys_id = -1;\nEXPORT_SYMBOL_GPL(net_prio_subsys_id);\n#endif\n#endif\n\nstatic int sock_set_timeout(long *timeo_p, char __user *optval, int optlen)\n{\n\tstruct timeval tv;\n\n\tif (optlen < sizeof(tv))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&tv, optval, sizeof(tv)))\n\t\treturn -EFAULT;\n\tif (tv.tv_usec < 0 || tv.tv_usec >= USEC_PER_SEC)\n\t\treturn -EDOM;\n\n\tif (tv.tv_sec < 0) {\n\t\tstatic int warned __read_mostly;\n\n\t\t*timeo_p = 0;\n\t\tif (warned < 10 && net_ratelimit()) {\n\t\t\twarned++;\n\t\t\tprintk(KERN_INFO \"sock_set_timeout: `%s' (pid %d) \"\n\t\t\t       \"tries to set negative timeout\\n\",\n\t\t\t\tcurrent->comm, task_pid_nr(current));\n\t\t}\n\t\treturn 0;\n\t}\n\t*timeo_p = MAX_SCHEDULE_TIMEOUT;\n\tif (tv.tv_sec == 0 && tv.tv_usec == 0)\n\t\treturn 0;\n\tif (tv.tv_sec < (MAX_SCHEDULE_TIMEOUT/HZ - 1))\n\t\t*timeo_p = tv.tv_sec*HZ + (tv.tv_usec+(1000000/HZ-1))/(1000000/HZ);\n\treturn 0;\n}\n\nstatic void sock_warn_obsolete_bsdism(const char *name)\n{\n\tstatic int warned;\n\tstatic char warncomm[TASK_COMM_LEN];\n\tif (strcmp(warncomm, current->comm) && warned < 5) {\n\t\tstrcpy(warncomm,  current->comm);\n\t\tprintk(KERN_WARNING \"process `%s' is using obsolete \"\n\t\t       \"%s SO_BSDCOMPAT\\n\", warncomm, name);\n\t\twarned++;\n\t}\n}\n\n#define SK_FLAGS_TIMESTAMP ((1UL << SOCK_TIMESTAMP) | (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE))\n\nstatic void sock_disable_timestamp(struct sock *sk, unsigned long flags)\n{\n\tif (sk->sk_flags & flags) {\n\t\tsk->sk_flags &= ~flags;\n\t\tif (!(sk->sk_flags & SK_FLAGS_TIMESTAMP))\n\t\t\tnet_disable_timestamp();\n\t}\n}\n\n\nint sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tint err;\n\tint skb_len;\n\tunsigned long flags;\n\tstruct sk_buff_head *list = &sk->sk_receive_queue;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\ttrace_sock_rcvqueue_full(sk, skb);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = sk_filter(sk, skb);\n\tif (err)\n\t\treturn err;\n\n\tif (!sk_rmem_schedule(sk, skb->truesize)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\treturn -ENOBUFS;\n\t}\n\n\tskb->dev = NULL;\n\tskb_set_owner_r(skb, sk);\n\n\t/* Cache the SKB length before we tack it onto the receive\n\t * queue.  Once it is added it no longer belongs to us and\n\t * may be freed by other threads of control pulling packets\n\t * from the queue.\n\t */\n\tskb_len = skb->len;\n\n\t/* we escape from rcu protected region, make sure we dont leak\n\t * a norefcounted dst\n\t */\n\tskb_dst_force(skb);\n\n\tspin_lock_irqsave(&list->lock, flags);\n\tskb->dropcount = atomic_read(&sk->sk_drops);\n\t__skb_queue_tail(list, skb);\n\tspin_unlock_irqrestore(&list->lock, flags);\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_data_ready(sk, skb_len);\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_queue_rcv_skb);\n\nint sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested)\n{\n\tint rc = NET_RX_SUCCESS;\n\n\tif (sk_filter(sk, skb))\n\t\tgoto discard_and_relse;\n\n\tskb->dev = NULL;\n\n\tif (sk_rcvqueues_full(sk, skb, sk->sk_rcvbuf)) {\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\tif (nested)\n\t\tbh_lock_sock_nested(sk);\n\telse\n\t\tbh_lock_sock(sk);\n\tif (!sock_owned_by_user(sk)) {\n\t\t/*\n\t\t * trylock + unlock semantics:\n\t\t */\n\t\tmutex_acquire(&sk->sk_lock.dep_map, 0, 1, _RET_IP_);\n\n\t\trc = sk_backlog_rcv(sk, skb);\n\n\t\tmutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);\n\t} else if (sk_add_backlog(sk, skb, sk->sk_rcvbuf)) {\n\t\tbh_unlock_sock(sk);\n\t\tatomic_inc(&sk->sk_drops);\n\t\tgoto discard_and_relse;\n\t}\n\n\tbh_unlock_sock(sk);\nout:\n\tsock_put(sk);\n\treturn rc;\ndiscard_and_relse:\n\tkfree_skb(skb);\n\tgoto out;\n}\nEXPORT_SYMBOL(sk_receive_skb);\n\nvoid sk_reset_txq(struct sock *sk)\n{\n\tsk_tx_queue_clear(sk);\n}\nEXPORT_SYMBOL(sk_reset_txq);\n\nstruct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie)\n{\n\tstruct dst_entry *dst = __sk_dst_get(sk);\n\n\tif (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {\n\t\tsk_tx_queue_clear(sk);\n\t\tRCU_INIT_POINTER(sk->sk_dst_cache, NULL);\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\n\treturn dst;\n}\nEXPORT_SYMBOL(__sk_dst_check);\n\nstruct dst_entry *sk_dst_check(struct sock *sk, u32 cookie)\n{\n\tstruct dst_entry *dst = sk_dst_get(sk);\n\n\tif (dst && dst->obsolete && dst->ops->check(dst, cookie) == NULL) {\n\t\tsk_dst_reset(sk);\n\t\tdst_release(dst);\n\t\treturn NULL;\n\t}\n\n\treturn dst;\n}\nEXPORT_SYMBOL(sk_dst_check);\n\nstatic int sock_bindtodevice(struct sock *sk, char __user *optval, int optlen)\n{\n\tint ret = -ENOPROTOOPT;\n#ifdef CONFIG_NETDEVICES\n\tstruct net *net = sock_net(sk);\n\tchar devname[IFNAMSIZ];\n\tint index;\n\n\t/* Sorry... */\n\tret = -EPERM;\n\tif (!capable(CAP_NET_RAW))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (optlen < 0)\n\t\tgoto out;\n\n\t/* Bind this socket to a particular device like \"eth0\",\n\t * as specified in the passed interface name. If the\n\t * name is \"\" or the option length is zero the socket\n\t * is not bound.\n\t */\n\tif (optlen > IFNAMSIZ - 1)\n\t\toptlen = IFNAMSIZ - 1;\n\tmemset(devname, 0, sizeof(devname));\n\n\tret = -EFAULT;\n\tif (copy_from_user(devname, optval, optlen))\n\t\tgoto out;\n\n\tindex = 0;\n\tif (devname[0] != '\\0') {\n\t\tstruct net_device *dev;\n\n\t\trcu_read_lock();\n\t\tdev = dev_get_by_name_rcu(net, devname);\n\t\tif (dev)\n\t\t\tindex = dev->ifindex;\n\t\trcu_read_unlock();\n\t\tret = -ENODEV;\n\t\tif (!dev)\n\t\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tsk->sk_bound_dev_if = index;\n\tsk_dst_reset(sk);\n\trelease_sock(sk);\n\n\tret = 0;\n\nout:\n#endif\n\n\treturn ret;\n}\n\nstatic inline void sock_valbool_flag(struct sock *sk, int bit, int valbool)\n{\n\tif (valbool)\n\t\tsock_set_flag(sk, bit);\n\telse\n\t\tsock_reset_flag(sk, bit);\n}\n\n/*\n *\tThis is meant for all protocols to use and covers goings on\n *\tat the socket level. Everything here is generic.\n */\n\nint sock_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tint val;\n\tint valbool;\n\tstruct linger ling;\n\tint ret = 0;\n\n\t/*\n\t *\tOptions without arguments\n\t */\n\n\tif (optname == SO_BINDTODEVICE)\n\t\treturn sock_bindtodevice(sk, optval, optlen);\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tvalbool = val ? 1 : 0;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tif (val && !capable(CAP_NET_ADMIN))\n\t\t\tret = -EACCES;\n\t\telse\n\t\t\tsock_valbool_flag(sk, SOCK_DBG, valbool);\n\t\tbreak;\n\tcase SO_REUSEADDR:\n\t\tsk->sk_reuse = (valbool ? SK_CAN_REUSE : SK_NO_REUSE);\n\t\tbreak;\n\tcase SO_TYPE:\n\tcase SO_PROTOCOL:\n\tcase SO_DOMAIN:\n\tcase SO_ERROR:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\tcase SO_DONTROUTE:\n\t\tsock_valbool_flag(sk, SOCK_LOCALROUTE, valbool);\n\t\tbreak;\n\tcase SO_BROADCAST:\n\t\tsock_valbool_flag(sk, SOCK_BROADCAST, valbool);\n\t\tbreak;\n\tcase SO_SNDBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_wmem_max);\nset_sndbuf:\n\t\tsk->sk_userlocks |= SOCK_SNDBUF_LOCK;\n\t\tsk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);\n\t\t/* Wake up sending tasks if we upped the value. */\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\n\tcase SO_SNDBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_sndbuf;\n\n\tcase SO_RCVBUF:\n\t\t/* Don't error on this BSD doesn't and if you think\n\t\t * about it this is right. Otherwise apps have to\n\t\t * play 'guess the biggest size' games. RCVBUF/SNDBUF\n\t\t * are treated in BSD as hints\n\t\t */\n\t\tval = min_t(u32, val, sysctl_rmem_max);\nset_rcvbuf:\n\t\tsk->sk_userlocks |= SOCK_RCVBUF_LOCK;\n\t\t/*\n\t\t * We double it on the way in to account for\n\t\t * \"struct sk_buff\" etc. overhead.   Applications\n\t\t * assume that the SO_RCVBUF setting they make will\n\t\t * allow that much actual data to be received on that\n\t\t * socket.\n\t\t *\n\t\t * Applications are unaware that \"struct sk_buff\" and\n\t\t * other overheads allocate from the receive buffer\n\t\t * during socket buffer allocation.\n\t\t *\n\t\t * And after considering the possible alternatives,\n\t\t * returning the value we actually used in getsockopt\n\t\t * is the most desirable behavior.\n\t\t */\n\t\tsk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);\n\t\tbreak;\n\n\tcase SO_RCVBUFFORCE:\n\t\tif (!capable(CAP_NET_ADMIN)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\t\tgoto set_rcvbuf;\n\n\tcase SO_KEEPALIVE:\n#ifdef CONFIG_INET\n\t\tif (sk->sk_protocol == IPPROTO_TCP)\n\t\t\ttcp_set_keepalive(sk, valbool);\n#endif\n\t\tsock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tsock_valbool_flag(sk, SOCK_URGINLINE, valbool);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tsk->sk_no_check = valbool;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tif ((val >= 0 && val <= 6) || capable(CAP_NET_ADMIN))\n\t\t\tsk->sk_priority = val;\n\t\telse\n\t\t\tret = -EPERM;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tif (optlen < sizeof(ling)) {\n\t\t\tret = -EINVAL;\t/* 1003.1g */\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&ling, optval, sizeof(ling))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ling.l_onoff)\n\t\t\tsock_reset_flag(sk, SOCK_LINGER);\n\t\telse {\n#if (BITS_PER_LONG == 32)\n\t\t\tif ((unsigned int)ling.l_linger >= MAX_SCHEDULE_TIMEOUT/HZ)\n\t\t\t\tsk->sk_lingertime = MAX_SCHEDULE_TIMEOUT;\n\t\t\telse\n#endif\n\t\t\t\tsk->sk_lingertime = (unsigned int)ling.l_linger * HZ;\n\t\t\tsock_set_flag(sk, SOCK_LINGER);\n\t\t}\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"setsockopt\");\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSCRED, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\tcase SO_TIMESTAMPNS:\n\t\tif (valbool)  {\n\t\t\tif (optname == SO_TIMESTAMP)\n\t\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\telse\n\t\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t\tsock_set_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\t\t} else {\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMP);\n\t\t\tsock_reset_flag(sk, SOCK_RCVTSTAMPNS);\n\t\t}\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tif (val & ~SOF_TIMESTAMPING_MASK) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_TX_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RX_HARDWARE);\n\t\tif (val & SOF_TIMESTAMPING_RX_SOFTWARE)\n\t\t\tsock_enable_timestamp(sk,\n\t\t\t\t\t      SOCK_TIMESTAMPING_RX_SOFTWARE);\n\t\telse\n\t\t\tsock_disable_timestamp(sk,\n\t\t\t\t\t       (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE));\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SOFTWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SOFTWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_SYS_HARDWARE);\n\t\tsock_valbool_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE,\n\t\t\t\t  val & SOF_TIMESTAMPING_RAW_HARDWARE);\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tif (val < 0)\n\t\t\tval = INT_MAX;\n\t\tsk->sk_rcvlowat = val ? : 1;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_rcvtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tret = sock_set_timeout(&sk->sk_sndtimeo, optval, optlen);\n\t\tbreak;\n\n\tcase SO_ATTACH_FILTER:\n\t\tret = -EINVAL;\n\t\tif (optlen == sizeof(struct sock_fprog)) {\n\t\t\tstruct sock_fprog fprog;\n\n\t\t\tret = -EFAULT;\n\t\t\tif (copy_from_user(&fprog, optval, sizeof(fprog)))\n\t\t\t\tbreak;\n\n\t\t\tret = sk_attach_filter(&fprog, sk);\n\t\t}\n\t\tbreak;\n\n\tcase SO_DETACH_FILTER:\n\t\tret = sk_detach_filter(sk);\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tif (valbool)\n\t\t\tset_bit(SOCK_PASSSEC, &sock->flags);\n\t\telse\n\t\t\tclear_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\tcase SO_MARK:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\tret = -EPERM;\n\t\telse\n\t\t\tsk->sk_mark = val;\n\t\tbreak;\n\n\t\t/* We implement the SO_SNDLOWAT etc to\n\t\t   not be settable (1003.1g 5.3) */\n\tcase SO_RXQ_OVFL:\n\t\tsock_valbool_flag(sk, SOCK_RXQ_OVFL, valbool);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tsock_valbool_flag(sk, SOCK_WIFI_STATUS, valbool);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (sock->ops->set_peek_off)\n\t\t\tsock->ops->set_peek_off(sk, val);\n\t\telse\n\t\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase SO_NOFCS:\n\t\tsock_valbool_flag(sk, SOCK_NOFCS, valbool);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\trelease_sock(sk);\n\treturn ret;\n}\nEXPORT_SYMBOL(sock_setsockopt);\n\n\nvoid cred_to_ucred(struct pid *pid, const struct cred *cred,\n\t\t   struct ucred *ucred)\n{\n\tucred->pid = pid_vnr(pid);\n\tucred->uid = ucred->gid = -1;\n\tif (cred) {\n\t\tstruct user_namespace *current_ns = current_user_ns();\n\n\t\tucred->uid = user_ns_map_uid(current_ns, cred, cred->euid);\n\t\tucred->gid = user_ns_map_gid(current_ns, cred, cred->egid);\n\t}\n}\nEXPORT_SYMBOL_GPL(cred_to_ucred);\n\nint sock_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tunion {\n\t\tint val;\n\t\tstruct linger ling;\n\t\tstruct timeval tm;\n\t} v;\n\n\tint lv = sizeof(int);\n\tint len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tmemset(&v, 0, sizeof(v));\n\n\tswitch (optname) {\n\tcase SO_DEBUG:\n\t\tv.val = sock_flag(sk, SOCK_DBG);\n\t\tbreak;\n\n\tcase SO_DONTROUTE:\n\t\tv.val = sock_flag(sk, SOCK_LOCALROUTE);\n\t\tbreak;\n\n\tcase SO_BROADCAST:\n\t\tv.val = !!sock_flag(sk, SOCK_BROADCAST);\n\t\tbreak;\n\n\tcase SO_SNDBUF:\n\t\tv.val = sk->sk_sndbuf;\n\t\tbreak;\n\n\tcase SO_RCVBUF:\n\t\tv.val = sk->sk_rcvbuf;\n\t\tbreak;\n\n\tcase SO_REUSEADDR:\n\t\tv.val = sk->sk_reuse;\n\t\tbreak;\n\n\tcase SO_KEEPALIVE:\n\t\tv.val = !!sock_flag(sk, SOCK_KEEPOPEN);\n\t\tbreak;\n\n\tcase SO_TYPE:\n\t\tv.val = sk->sk_type;\n\t\tbreak;\n\n\tcase SO_PROTOCOL:\n\t\tv.val = sk->sk_protocol;\n\t\tbreak;\n\n\tcase SO_DOMAIN:\n\t\tv.val = sk->sk_family;\n\t\tbreak;\n\n\tcase SO_ERROR:\n\t\tv.val = -sock_error(sk);\n\t\tif (v.val == 0)\n\t\t\tv.val = xchg(&sk->sk_err_soft, 0);\n\t\tbreak;\n\n\tcase SO_OOBINLINE:\n\t\tv.val = !!sock_flag(sk, SOCK_URGINLINE);\n\t\tbreak;\n\n\tcase SO_NO_CHECK:\n\t\tv.val = sk->sk_no_check;\n\t\tbreak;\n\n\tcase SO_PRIORITY:\n\t\tv.val = sk->sk_priority;\n\t\tbreak;\n\n\tcase SO_LINGER:\n\t\tlv\t\t= sizeof(v.ling);\n\t\tv.ling.l_onoff\t= !!sock_flag(sk, SOCK_LINGER);\n\t\tv.ling.l_linger\t= sk->sk_lingertime / HZ;\n\t\tbreak;\n\n\tcase SO_BSDCOMPAT:\n\t\tsock_warn_obsolete_bsdism(\"getsockopt\");\n\t\tbreak;\n\n\tcase SO_TIMESTAMP:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMP) &&\n\t\t\t\t!sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPNS:\n\t\tv.val = sock_flag(sk, SOCK_RCVTSTAMPNS);\n\t\tbreak;\n\n\tcase SO_TIMESTAMPING:\n\t\tv.val = 0;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_TX_HARDWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_TX_HARDWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_TX_SOFTWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_TX_SOFTWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_RX_HARDWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_RX_HARDWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_RX_SOFTWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_RX_SOFTWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_SOFTWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_SOFTWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_SYS_HARDWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_SYS_HARDWARE;\n\t\tif (sock_flag(sk, SOCK_TIMESTAMPING_RAW_HARDWARE))\n\t\t\tv.val |= SOF_TIMESTAMPING_RAW_HARDWARE;\n\t\tbreak;\n\n\tcase SO_RCVTIMEO:\n\t\tlv = sizeof(struct timeval);\n\t\tif (sk->sk_rcvtimeo == MAX_SCHEDULE_TIMEOUT) {\n\t\t\tv.tm.tv_sec = 0;\n\t\t\tv.tm.tv_usec = 0;\n\t\t} else {\n\t\t\tv.tm.tv_sec = sk->sk_rcvtimeo / HZ;\n\t\t\tv.tm.tv_usec = ((sk->sk_rcvtimeo % HZ) * 1000000) / HZ;\n\t\t}\n\t\tbreak;\n\n\tcase SO_SNDTIMEO:\n\t\tlv = sizeof(struct timeval);\n\t\tif (sk->sk_sndtimeo == MAX_SCHEDULE_TIMEOUT) {\n\t\t\tv.tm.tv_sec = 0;\n\t\t\tv.tm.tv_usec = 0;\n\t\t} else {\n\t\t\tv.tm.tv_sec = sk->sk_sndtimeo / HZ;\n\t\t\tv.tm.tv_usec = ((sk->sk_sndtimeo % HZ) * 1000000) / HZ;\n\t\t}\n\t\tbreak;\n\n\tcase SO_RCVLOWAT:\n\t\tv.val = sk->sk_rcvlowat;\n\t\tbreak;\n\n\tcase SO_SNDLOWAT:\n\t\tv.val = 1;\n\t\tbreak;\n\n\tcase SO_PASSCRED:\n\t\tv.val = !!test_bit(SOCK_PASSCRED, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERCRED:\n\t{\n\t\tstruct ucred peercred;\n\t\tif (len > sizeof(peercred))\n\t\t\tlen = sizeof(peercred);\n\t\tcred_to_ucred(sk->sk_peer_pid, sk->sk_peer_cred, &peercred);\n\t\tif (copy_to_user(optval, &peercred, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\tcase SO_PEERNAME:\n\t{\n\t\tchar address[128];\n\n\t\tif (sock->ops->getname(sock, (struct sockaddr *)address, &lv, 2))\n\t\t\treturn -ENOTCONN;\n\t\tif (lv < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_to_user(optval, address, len))\n\t\t\treturn -EFAULT;\n\t\tgoto lenout;\n\t}\n\n\t/* Dubious BSD thing... Probably nobody even uses it, but\n\t * the UNIX standard wants it for whatever reason... -DaveM\n\t */\n\tcase SO_ACCEPTCONN:\n\t\tv.val = sk->sk_state == TCP_LISTEN;\n\t\tbreak;\n\n\tcase SO_PASSSEC:\n\t\tv.val = !!test_bit(SOCK_PASSSEC, &sock->flags);\n\t\tbreak;\n\n\tcase SO_PEERSEC:\n\t\treturn security_socket_getpeersec_stream(sock, optval, optlen, len);\n\n\tcase SO_MARK:\n\t\tv.val = sk->sk_mark;\n\t\tbreak;\n\n\tcase SO_RXQ_OVFL:\n\t\tv.val = !!sock_flag(sk, SOCK_RXQ_OVFL);\n\t\tbreak;\n\n\tcase SO_WIFI_STATUS:\n\t\tv.val = !!sock_flag(sk, SOCK_WIFI_STATUS);\n\t\tbreak;\n\n\tcase SO_PEEK_OFF:\n\t\tif (!sock->ops->set_peek_off)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tv.val = sk->sk_peek_off;\n\t\tbreak;\n\tcase SO_NOFCS:\n\t\tv.val = !!sock_flag(sk, SOCK_NOFCS);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (len > lv)\n\t\tlen = lv;\n\tif (copy_to_user(optval, &v, len))\n\t\treturn -EFAULT;\nlenout:\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n/*\n * Initialize an sk_lock.\n *\n * (We also register the sk_lock with the lock validator.)\n */\nstatic inline void sock_lock_init(struct sock *sk)\n{\n\tsock_lock_init_class_and_name(sk,\n\t\t\taf_family_slock_key_strings[sk->sk_family],\n\t\t\taf_family_slock_keys + sk->sk_family,\n\t\t\taf_family_key_strings[sk->sk_family],\n\t\t\taf_family_keys + sk->sk_family);\n}\n\n/*\n * Copy all fields from osk to nsk but nsk->sk_refcnt must not change yet,\n * even temporarly, because of RCU lookups. sk_node should also be left as is.\n * We must not copy fields between sk_dontcopy_begin and sk_dontcopy_end\n */\nstatic void sock_copy(struct sock *nsk, const struct sock *osk)\n{\n#ifdef CONFIG_SECURITY_NETWORK\n\tvoid *sptr = nsk->sk_security;\n#endif\n\tmemcpy(nsk, osk, offsetof(struct sock, sk_dontcopy_begin));\n\n\tmemcpy(&nsk->sk_dontcopy_end, &osk->sk_dontcopy_end,\n\t       osk->sk_prot->obj_size - offsetof(struct sock, sk_dontcopy_end));\n\n#ifdef CONFIG_SECURITY_NETWORK\n\tnsk->sk_security = sptr;\n\tsecurity_sk_clone(osk, nsk);\n#endif\n}\n\n/*\n * caches using SLAB_DESTROY_BY_RCU should let .next pointer from nulls nodes\n * un-modified. Special care is taken when initializing object to zero.\n */\nstatic inline void sk_prot_clear_nulls(struct sock *sk, int size)\n{\n\tif (offsetof(struct sock, sk_node.next) != 0)\n\t\tmemset(sk, 0, offsetof(struct sock, sk_node.next));\n\tmemset(&sk->sk_node.pprev, 0,\n\t       size - offsetof(struct sock, sk_node.pprev));\n}\n\nvoid sk_prot_clear_portaddr_nulls(struct sock *sk, int size)\n{\n\tunsigned long nulls1, nulls2;\n\n\tnulls1 = offsetof(struct sock, __sk_common.skc_node.next);\n\tnulls2 = offsetof(struct sock, __sk_common.skc_portaddr_node.next);\n\tif (nulls1 > nulls2)\n\t\tswap(nulls1, nulls2);\n\n\tif (nulls1 != 0)\n\t\tmemset((char *)sk, 0, nulls1);\n\tmemset((char *)sk + nulls1 + sizeof(void *), 0,\n\t       nulls2 - nulls1 - sizeof(void *));\n\tmemset((char *)sk + nulls2 + sizeof(void *), 0,\n\t       size - nulls2 - sizeof(void *));\n}\nEXPORT_SYMBOL(sk_prot_clear_portaddr_nulls);\n\nstatic struct sock *sk_prot_alloc(struct proto *prot, gfp_t priority,\n\t\tint family)\n{\n\tstruct sock *sk;\n\tstruct kmem_cache *slab;\n\n\tslab = prot->slab;\n\tif (slab != NULL) {\n\t\tsk = kmem_cache_alloc(slab, priority & ~__GFP_ZERO);\n\t\tif (!sk)\n\t\t\treturn sk;\n\t\tif (priority & __GFP_ZERO) {\n\t\t\tif (prot->clear_sk)\n\t\t\t\tprot->clear_sk(sk, prot->obj_size);\n\t\t\telse\n\t\t\t\tsk_prot_clear_nulls(sk, prot->obj_size);\n\t\t}\n\t} else\n\t\tsk = kmalloc(prot->obj_size, priority);\n\n\tif (sk != NULL) {\n\t\tkmemcheck_annotate_bitfield(sk, flags);\n\n\t\tif (security_sk_alloc(sk, family, priority))\n\t\t\tgoto out_free;\n\n\t\tif (!try_module_get(prot->owner))\n\t\t\tgoto out_free_sec;\n\t\tsk_tx_queue_clear(sk);\n\t}\n\n\treturn sk;\n\nout_free_sec:\n\tsecurity_sk_free(sk);\nout_free:\n\tif (slab != NULL)\n\t\tkmem_cache_free(slab, sk);\n\telse\n\t\tkfree(sk);\n\treturn NULL;\n}\n\nstatic void sk_prot_free(struct proto *prot, struct sock *sk)\n{\n\tstruct kmem_cache *slab;\n\tstruct module *owner;\n\n\towner = prot->owner;\n\tslab = prot->slab;\n\n\tsecurity_sk_free(sk);\n\tif (slab != NULL)\n\t\tkmem_cache_free(slab, sk);\n\telse\n\t\tkfree(sk);\n\tmodule_put(owner);\n}\n\n#ifdef CONFIG_CGROUPS\nvoid sock_update_classid(struct sock *sk)\n{\n\tu32 classid;\n\n\trcu_read_lock();  /* doing current task, which cannot vanish. */\n\tclassid = task_cls_classid(current);\n\trcu_read_unlock();\n\tif (classid && classid != sk->sk_classid)\n\t\tsk->sk_classid = classid;\n}\nEXPORT_SYMBOL(sock_update_classid);\n\nvoid sock_update_netprioidx(struct sock *sk)\n{\n\tif (in_interrupt())\n\t\treturn;\n\n\tsk->sk_cgrp_prioidx = task_netprioidx(current);\n}\nEXPORT_SYMBOL_GPL(sock_update_netprioidx);\n#endif\n\n/**\n *\tsk_alloc - All socket objects are allocated here\n *\t@net: the applicable net namespace\n *\t@family: protocol family\n *\t@priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)\n *\t@prot: struct proto associated with this new sock instance\n */\nstruct sock *sk_alloc(struct net *net, int family, gfp_t priority,\n\t\t      struct proto *prot)\n{\n\tstruct sock *sk;\n\n\tsk = sk_prot_alloc(prot, priority | __GFP_ZERO, family);\n\tif (sk) {\n\t\tsk->sk_family = family;\n\t\t/*\n\t\t * See comment in struct sock definition to understand\n\t\t * why we need sk_prot_creator -acme\n\t\t */\n\t\tsk->sk_prot = sk->sk_prot_creator = prot;\n\t\tsock_lock_init(sk);\n\t\tsock_net_set(sk, get_net(net));\n\t\tatomic_set(&sk->sk_wmem_alloc, 1);\n\n\t\tsock_update_classid(sk);\n\t\tsock_update_netprioidx(sk);\n\t}\n\n\treturn sk;\n}\nEXPORT_SYMBOL(sk_alloc);\n\nstatic void __sk_free(struct sock *sk)\n{\n\tstruct sk_filter *filter;\n\n\tif (sk->sk_destruct)\n\t\tsk->sk_destruct(sk);\n\n\tfilter = rcu_dereference_check(sk->sk_filter,\n\t\t\t\t       atomic_read(&sk->sk_wmem_alloc) == 0);\n\tif (filter) {\n\t\tsk_filter_uncharge(sk, filter);\n\t\tRCU_INIT_POINTER(sk->sk_filter, NULL);\n\t}\n\n\tsock_disable_timestamp(sk, SK_FLAGS_TIMESTAMP);\n\n\tif (atomic_read(&sk->sk_omem_alloc))\n\t\tprintk(KERN_DEBUG \"%s: optmem leakage (%d bytes) detected.\\n\",\n\t\t       __func__, atomic_read(&sk->sk_omem_alloc));\n\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tput_pid(sk->sk_peer_pid);\n\tput_net(sock_net(sk));\n\tsk_prot_free(sk->sk_prot_creator, sk);\n}\n\nvoid sk_free(struct sock *sk)\n{\n\t/*\n\t * We subtract one from sk_wmem_alloc and can know if\n\t * some packets are still in some tx queue.\n\t * If not null, sock_wfree() will call __sk_free(sk) later\n\t */\n\tif (atomic_dec_and_test(&sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\nEXPORT_SYMBOL(sk_free);\n\n/*\n * Last sock_put should drop reference to sk->sk_net. It has already\n * been dropped in sk_change_net. Taking reference to stopping namespace\n * is not an option.\n * Take reference to a socket to remove it from hash _alive_ and after that\n * destroy it in the context of init_net.\n */\nvoid sk_release_kernel(struct sock *sk)\n{\n\tif (sk == NULL || sk->sk_socket == NULL)\n\t\treturn;\n\n\tsock_hold(sk);\n\tsock_release(sk->sk_socket);\n\trelease_net(sock_net(sk));\n\tsock_net_set(sk, get_net(&init_net));\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(sk_release_kernel);\n\nstatic void sk_update_clone(const struct sock *sk, struct sock *newsk)\n{\n\tif (mem_cgroup_sockets_enabled && sk->sk_cgrp)\n\t\tsock_update_memcg(newsk);\n}\n\n/**\n *\tsk_clone_lock - clone a socket, and lock its clone\n *\t@sk: the socket to clone\n *\t@priority: for allocation (%GFP_KERNEL, %GFP_ATOMIC, etc)\n *\n *\tCaller must unlock socket even in error path (bh_unlock_sock(newsk))\n */\nstruct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority)\n{\n\tstruct sock *newsk;\n\n\tnewsk = sk_prot_alloc(sk->sk_prot, priority, sk->sk_family);\n\tif (newsk != NULL) {\n\t\tstruct sk_filter *filter;\n\n\t\tsock_copy(newsk, sk);\n\n\t\t/* SANITY */\n\t\tget_net(sock_net(newsk));\n\t\tsk_node_init(&newsk->sk_node);\n\t\tsock_lock_init(newsk);\n\t\tbh_lock_sock(newsk);\n\t\tnewsk->sk_backlog.head\t= newsk->sk_backlog.tail = NULL;\n\t\tnewsk->sk_backlog.len = 0;\n\n\t\tatomic_set(&newsk->sk_rmem_alloc, 0);\n\t\t/*\n\t\t * sk_wmem_alloc set to one (see sk_free() and sock_wfree())\n\t\t */\n\t\tatomic_set(&newsk->sk_wmem_alloc, 1);\n\t\tatomic_set(&newsk->sk_omem_alloc, 0);\n\t\tskb_queue_head_init(&newsk->sk_receive_queue);\n\t\tskb_queue_head_init(&newsk->sk_write_queue);\n#ifdef CONFIG_NET_DMA\n\t\tskb_queue_head_init(&newsk->sk_async_wait_queue);\n#endif\n\n\t\tspin_lock_init(&newsk->sk_dst_lock);\n\t\trwlock_init(&newsk->sk_callback_lock);\n\t\tlockdep_set_class_and_name(&newsk->sk_callback_lock,\n\t\t\t\taf_callback_keys + newsk->sk_family,\n\t\t\t\taf_family_clock_key_strings[newsk->sk_family]);\n\n\t\tnewsk->sk_dst_cache\t= NULL;\n\t\tnewsk->sk_wmem_queued\t= 0;\n\t\tnewsk->sk_forward_alloc = 0;\n\t\tnewsk->sk_send_head\t= NULL;\n\t\tnewsk->sk_userlocks\t= sk->sk_userlocks & ~SOCK_BINDPORT_LOCK;\n\n\t\tsock_reset_flag(newsk, SOCK_DONE);\n\t\tskb_queue_head_init(&newsk->sk_error_queue);\n\n\t\tfilter = rcu_dereference_protected(newsk->sk_filter, 1);\n\t\tif (filter != NULL)\n\t\t\tsk_filter_charge(newsk, filter);\n\n\t\tif (unlikely(xfrm_sk_clone_policy(newsk))) {\n\t\t\t/* It is still raw copy of parent, so invalidate\n\t\t\t * destructor and make plain sk_free() */\n\t\t\tnewsk->sk_destruct = NULL;\n\t\t\tbh_unlock_sock(newsk);\n\t\t\tsk_free(newsk);\n\t\t\tnewsk = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnewsk->sk_err\t   = 0;\n\t\tnewsk->sk_priority = 0;\n\t\t/*\n\t\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t\t */\n\t\tsmp_wmb();\n\t\tatomic_set(&newsk->sk_refcnt, 2);\n\n\t\t/*\n\t\t * Increment the counter in the same struct proto as the master\n\t\t * sock (sk_refcnt_debug_inc uses newsk->sk_prot->socks, that\n\t\t * is the same as sk->sk_prot->socks, as this field was copied\n\t\t * with memcpy).\n\t\t *\n\t\t * This _changes_ the previous behaviour, where\n\t\t * tcp_create_openreq_child always was incrementing the\n\t\t * equivalent to tcp_prot->socks (inet_sock_nr), so this have\n\t\t * to be taken into account in all callers. -acme\n\t\t */\n\t\tsk_refcnt_debug_inc(newsk);\n\t\tsk_set_socket(newsk, NULL);\n\t\tnewsk->sk_wq = NULL;\n\n\t\tsk_update_clone(sk, newsk);\n\n\t\tif (newsk->sk_prot->sockets_allocated)\n\t\t\tsk_sockets_allocated_inc(newsk);\n\n\t\tif (newsk->sk_flags & SK_FLAGS_TIMESTAMP)\n\t\t\tnet_enable_timestamp();\n\t}\nout:\n\treturn newsk;\n}\nEXPORT_SYMBOL_GPL(sk_clone_lock);\n\nvoid sk_setup_caps(struct sock *sk, struct dst_entry *dst)\n{\n\t__sk_dst_set(sk, dst);\n\tsk->sk_route_caps = dst->dev->features;\n\tif (sk->sk_route_caps & NETIF_F_GSO)\n\t\tsk->sk_route_caps |= NETIF_F_GSO_SOFTWARE;\n\tsk->sk_route_caps &= ~sk->sk_route_nocaps;\n\tif (sk_can_gso(sk)) {\n\t\tif (dst->header_len) {\n\t\t\tsk->sk_route_caps &= ~NETIF_F_GSO_MASK;\n\t\t} else {\n\t\t\tsk->sk_route_caps |= NETIF_F_SG | NETIF_F_HW_CSUM;\n\t\t\tsk->sk_gso_max_size = dst->dev->gso_max_size;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(sk_setup_caps);\n\nvoid __init sk_init(void)\n{\n\tif (totalram_pages <= 4096) {\n\t\tsysctl_wmem_max = 32767;\n\t\tsysctl_rmem_max = 32767;\n\t\tsysctl_wmem_default = 32767;\n\t\tsysctl_rmem_default = 32767;\n\t} else if (totalram_pages >= 131072) {\n\t\tsysctl_wmem_max = 131071;\n\t\tsysctl_rmem_max = 131071;\n\t}\n}\n\n/*\n *\tSimple resource managers for sockets.\n */\n\n\n/*\n * Write buffer destructor automatically called from kfree_skb.\n */\nvoid sock_wfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tunsigned int len = skb->truesize;\n\n\tif (!sock_flag(sk, SOCK_USE_WRITE_QUEUE)) {\n\t\t/*\n\t\t * Keep a reference on sk_wmem_alloc, this will be released\n\t\t * after sk_write_space() call\n\t\t */\n\t\tatomic_sub(len - 1, &sk->sk_wmem_alloc);\n\t\tsk->sk_write_space(sk);\n\t\tlen = 1;\n\t}\n\t/*\n\t * if sk_wmem_alloc reaches 0, we must finish what sk_free()\n\t * could not do because of in-flight packets\n\t */\n\tif (atomic_sub_and_test(len, &sk->sk_wmem_alloc))\n\t\t__sk_free(sk);\n}\nEXPORT_SYMBOL(sock_wfree);\n\n/*\n * Read buffer destructor automatically called from kfree_skb.\n */\nvoid sock_rfree(struct sk_buff *skb)\n{\n\tstruct sock *sk = skb->sk;\n\tunsigned int len = skb->truesize;\n\n\tatomic_sub(len, &sk->sk_rmem_alloc);\n\tsk_mem_uncharge(sk, len);\n}\nEXPORT_SYMBOL(sock_rfree);\n\n\nint sock_i_uid(struct sock *sk)\n{\n\tint uid;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tuid = sk->sk_socket ? SOCK_INODE(sk->sk_socket)->i_uid : 0;\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn uid;\n}\nEXPORT_SYMBOL(sock_i_uid);\n\nunsigned long sock_i_ino(struct sock *sk)\n{\n\tunsigned long ino;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tino = sk->sk_socket ? SOCK_INODE(sk->sk_socket)->i_ino : 0;\n\tread_unlock_bh(&sk->sk_callback_lock);\n\treturn ino;\n}\nEXPORT_SYMBOL(sock_i_ino);\n\n/*\n * Allocate a skb from the socket's send buffer.\n */\nstruct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,\n\t\t\t     gfp_t priority)\n{\n\tif (force || atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {\n\t\tstruct sk_buff *skb = alloc_skb(size, priority);\n\t\tif (skb) {\n\t\t\tskb_set_owner_w(skb, sk);\n\t\t\treturn skb;\n\t\t}\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_wmalloc);\n\n/*\n * Allocate a skb from the socket's receive buffer.\n */\nstruct sk_buff *sock_rmalloc(struct sock *sk, unsigned long size, int force,\n\t\t\t     gfp_t priority)\n{\n\tif (force || atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\tstruct sk_buff *skb = alloc_skb(size, priority);\n\t\tif (skb) {\n\t\t\tskb_set_owner_r(skb, sk);\n\t\t\treturn skb;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n/*\n * Allocate a memory block from the socket's option memory buffer.\n */\nvoid *sock_kmalloc(struct sock *sk, int size, gfp_t priority)\n{\n\tif ((unsigned int)size <= sysctl_optmem_max &&\n\t    atomic_read(&sk->sk_omem_alloc) + size < sysctl_optmem_max) {\n\t\tvoid *mem;\n\t\t/* First do the add, to avoid the race if kmalloc\n\t\t * might sleep.\n\t\t */\n\t\tatomic_add(size, &sk->sk_omem_alloc);\n\t\tmem = kmalloc(size, priority);\n\t\tif (mem)\n\t\t\treturn mem;\n\t\tatomic_sub(size, &sk->sk_omem_alloc);\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_kmalloc);\n\n/*\n * Free an option memory block.\n */\nvoid sock_kfree_s(struct sock *sk, void *mem, int size)\n{\n\tkfree(mem);\n\tatomic_sub(size, &sk->sk_omem_alloc);\n}\nEXPORT_SYMBOL(sock_kfree_s);\n\n/* It is almost wait_for_tcp_memory minus release_sock/lock_sock.\n   I think, these locks should be removed for datagram sockets.\n */\nstatic long sock_wait_for_wmem(struct sock *sk, long timeo)\n{\n\tDEFINE_WAIT(wait);\n\n\tclear_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\n\tfor (;;) {\n\t\tif (!timeo)\n\t\t\tbreak;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\t\tif (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf)\n\t\t\tbreak;\n\t\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\t\tbreak;\n\t\tif (sk->sk_err)\n\t\t\tbreak;\n\t\ttimeo = schedule_timeout(timeo);\n\t}\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn timeo;\n}\n\n\n/*\n *\tGeneric send/receive buffer handlers\n */\n\nstruct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,\n\t\t\t\t     unsigned long data_len, int noblock,\n\t\t\t\t     int *errcode)\n{\n\tstruct sk_buff *skb;\n\tgfp_t gfp_mask;\n\tlong timeo;\n\tint err;\n\n\tgfp_mask = sk->sk_allocation;\n\tif (gfp_mask & __GFP_WAIT)\n\t\tgfp_mask |= __GFP_REPEAT;\n\n\ttimeo = sock_sndtimeo(sk, noblock);\n\twhile (1) {\n\t\terr = sock_error(sk);\n\t\tif (err != 0)\n\t\t\tgoto failure;\n\n\t\terr = -EPIPE;\n\t\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\t\tgoto failure;\n\n\t\tif (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {\n\t\t\tskb = alloc_skb(header_len, gfp_mask);\n\t\t\tif (skb) {\n\t\t\t\tint npages;\n\t\t\t\tint i;\n\n\t\t\t\t/* No pages, we're done... */\n\t\t\t\tif (!data_len)\n\t\t\t\t\tbreak;\n\n\t\t\t\tnpages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;\n\t\t\t\tskb->truesize += data_len;\n\t\t\t\tskb_shinfo(skb)->nr_frags = npages;\n\t\t\t\tfor (i = 0; i < npages; i++) {\n\t\t\t\t\tstruct page *page;\n\n\t\t\t\t\tpage = alloc_pages(sk->sk_allocation, 0);\n\t\t\t\t\tif (!page) {\n\t\t\t\t\t\terr = -ENOBUFS;\n\t\t\t\t\t\tskb_shinfo(skb)->nr_frags = i;\n\t\t\t\t\t\tkfree_skb(skb);\n\t\t\t\t\t\tgoto failure;\n\t\t\t\t\t}\n\n\t\t\t\t\t__skb_fill_page_desc(skb, i,\n\t\t\t\t\t\t\tpage, 0,\n\t\t\t\t\t\t\t(data_len >= PAGE_SIZE ?\n\t\t\t\t\t\t\t PAGE_SIZE :\n\t\t\t\t\t\t\t data_len));\n\t\t\t\t\tdata_len -= PAGE_SIZE;\n\t\t\t\t}\n\n\t\t\t\t/* Full success... */\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\terr = -ENOBUFS;\n\t\t\tgoto failure;\n\t\t}\n\t\tset_bit(SOCK_ASYNC_NOSPACE, &sk->sk_socket->flags);\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto failure;\n\t\tif (signal_pending(current))\n\t\t\tgoto interrupted;\n\t\ttimeo = sock_wait_for_wmem(sk, timeo);\n\t}\n\n\tskb_set_owner_w(skb, sk);\n\treturn skb;\n\ninterrupted:\n\terr = sock_intr_errno(timeo);\nfailure:\n\t*errcode = err;\n\treturn NULL;\n}\nEXPORT_SYMBOL(sock_alloc_send_pskb);\n\nstruct sk_buff *sock_alloc_send_skb(struct sock *sk, unsigned long size,\n\t\t\t\t    int noblock, int *errcode)\n{\n\treturn sock_alloc_send_pskb(sk, size, 0, noblock, errcode);\n}\nEXPORT_SYMBOL(sock_alloc_send_skb);\n\nstatic void __lock_sock(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n\t__acquires(&sk->sk_lock.slock)\n{\n\tDEFINE_WAIT(wait);\n\n\tfor (;;) {\n\t\tprepare_to_wait_exclusive(&sk->sk_lock.wq, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tspin_unlock_bh(&sk->sk_lock.slock);\n\t\tschedule();\n\t\tspin_lock_bh(&sk->sk_lock.slock);\n\t\tif (!sock_owned_by_user(sk))\n\t\t\tbreak;\n\t}\n\tfinish_wait(&sk->sk_lock.wq, &wait);\n}\n\nstatic void __release_sock(struct sock *sk)\n\t__releases(&sk->sk_lock.slock)\n\t__acquires(&sk->sk_lock.slock)\n{\n\tstruct sk_buff *skb = sk->sk_backlog.head;\n\n\tdo {\n\t\tsk->sk_backlog.head = sk->sk_backlog.tail = NULL;\n\t\tbh_unlock_sock(sk);\n\n\t\tdo {\n\t\t\tstruct sk_buff *next = skb->next;\n\n\t\t\tWARN_ON_ONCE(skb_dst_is_noref(skb));\n\t\t\tskb->next = NULL;\n\t\t\tsk_backlog_rcv(sk, skb);\n\n\t\t\t/*\n\t\t\t * We are in process context here with softirqs\n\t\t\t * disabled, use cond_resched_softirq() to preempt.\n\t\t\t * This is safe to do because we've taken the backlog\n\t\t\t * queue private:\n\t\t\t */\n\t\t\tcond_resched_softirq();\n\n\t\t\tskb = next;\n\t\t} while (skb != NULL);\n\n\t\tbh_lock_sock(sk);\n\t} while ((skb = sk->sk_backlog.head) != NULL);\n\n\t/*\n\t * Doing the zeroing here guarantee we can not loop forever\n\t * while a wild producer attempts to flood us.\n\t */\n\tsk->sk_backlog.len = 0;\n}\n\n/**\n * sk_wait_data - wait for data to arrive at sk_receive_queue\n * @sk:    sock to wait on\n * @timeo: for how long\n *\n * Now socket state including sk->sk_err is changed only under lock,\n * hence we may omit checks after joining wait queue.\n * We check receive queue before schedule() only as optimization;\n * it is very likely that release_sock() added new data.\n */\nint sk_wait_data(struct sock *sk, long *timeo)\n{\n\tint rc;\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\tset_bit(SOCK_ASYNC_WAITDATA, &sk->sk_socket->flags);\n\trc = sk_wait_event(sk, timeo, !skb_queue_empty(&sk->sk_receive_queue));\n\tclear_bit(SOCK_ASYNC_WAITDATA, &sk->sk_socket->flags);\n\tfinish_wait(sk_sleep(sk), &wait);\n\treturn rc;\n}\nEXPORT_SYMBOL(sk_wait_data);\n\n/**\n *\t__sk_mem_schedule - increase sk_forward_alloc and memory_allocated\n *\t@sk: socket\n *\t@size: memory size to allocate\n *\t@kind: allocation type\n *\n *\tIf kind is SK_MEM_SEND, it means wmem allocation. Otherwise it means\n *\trmem allocation. This function assumes that protocols which have\n *\tmemory_pressure use sk_wmem_queued as write buffer accounting.\n */\nint __sk_mem_schedule(struct sock *sk, int size, int kind)\n{\n\tstruct proto *prot = sk->sk_prot;\n\tint amt = sk_mem_pages(size);\n\tlong allocated;\n\tint parent_status = UNDER_LIMIT;\n\n\tsk->sk_forward_alloc += amt * SK_MEM_QUANTUM;\n\n\tallocated = sk_memory_allocated_add(sk, amt, &parent_status);\n\n\t/* Under limit. */\n\tif (parent_status == UNDER_LIMIT &&\n\t\t\tallocated <= sk_prot_mem_limits(sk, 0)) {\n\t\tsk_leave_memory_pressure(sk);\n\t\treturn 1;\n\t}\n\n\t/* Under pressure. (we or our parents) */\n\tif ((parent_status > SOFT_LIMIT) ||\n\t\t\tallocated > sk_prot_mem_limits(sk, 1))\n\t\tsk_enter_memory_pressure(sk);\n\n\t/* Over hard limit (we or our parents) */\n\tif ((parent_status == OVER_LIMIT) ||\n\t\t\t(allocated > sk_prot_mem_limits(sk, 2)))\n\t\tgoto suppress_allocation;\n\n\t/* guarantee minimum buffer size under pressure */\n\tif (kind == SK_MEM_RECV) {\n\t\tif (atomic_read(&sk->sk_rmem_alloc) < prot->sysctl_rmem[0])\n\t\t\treturn 1;\n\n\t} else { /* SK_MEM_SEND */\n\t\tif (sk->sk_type == SOCK_STREAM) {\n\t\t\tif (sk->sk_wmem_queued < prot->sysctl_wmem[0])\n\t\t\t\treturn 1;\n\t\t} else if (atomic_read(&sk->sk_wmem_alloc) <\n\t\t\t   prot->sysctl_wmem[0])\n\t\t\t\treturn 1;\n\t}\n\n\tif (sk_has_memory_pressure(sk)) {\n\t\tint alloc;\n\n\t\tif (!sk_under_memory_pressure(sk))\n\t\t\treturn 1;\n\t\talloc = sk_sockets_allocated_read_positive(sk);\n\t\tif (sk_prot_mem_limits(sk, 2) > alloc *\n\t\t    sk_mem_pages(sk->sk_wmem_queued +\n\t\t\t\t atomic_read(&sk->sk_rmem_alloc) +\n\t\t\t\t sk->sk_forward_alloc))\n\t\t\treturn 1;\n\t}\n\nsuppress_allocation:\n\n\tif (kind == SK_MEM_SEND && sk->sk_type == SOCK_STREAM) {\n\t\tsk_stream_moderate_sndbuf(sk);\n\n\t\t/* Fail only if socket is _under_ its sndbuf.\n\t\t * In this case we cannot block, so that we have to fail.\n\t\t */\n\t\tif (sk->sk_wmem_queued + size >= sk->sk_sndbuf)\n\t\t\treturn 1;\n\t}\n\n\ttrace_sock_exceed_buf_limit(sk, prot, allocated);\n\n\t/* Alas. Undo changes. */\n\tsk->sk_forward_alloc -= amt * SK_MEM_QUANTUM;\n\n\tsk_memory_allocated_sub(sk, amt);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(__sk_mem_schedule);\n\n/**\n *\t__sk_reclaim - reclaim memory_allocated\n *\t@sk: socket\n */\nvoid __sk_mem_reclaim(struct sock *sk)\n{\n\tsk_memory_allocated_sub(sk,\n\t\t\t\tsk->sk_forward_alloc >> SK_MEM_QUANTUM_SHIFT);\n\tsk->sk_forward_alloc &= SK_MEM_QUANTUM - 1;\n\n\tif (sk_under_memory_pressure(sk) &&\n\t    (sk_memory_allocated(sk) < sk_prot_mem_limits(sk, 0)))\n\t\tsk_leave_memory_pressure(sk);\n}\nEXPORT_SYMBOL(__sk_mem_reclaim);\n\n\n/*\n * Set of default routines for initialising struct proto_ops when\n * the protocol does not support a particular function. In certain\n * cases where it makes no sense for a protocol to have a \"do nothing\"\n * function, some default processing is provided.\n */\n\nint sock_no_bind(struct socket *sock, struct sockaddr *saddr, int len)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_bind);\n\nint sock_no_connect(struct socket *sock, struct sockaddr *saddr,\n\t\t    int len, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_connect);\n\nint sock_no_socketpair(struct socket *sock1, struct socket *sock2)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_socketpair);\n\nint sock_no_accept(struct socket *sock, struct socket *newsock, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_accept);\n\nint sock_no_getname(struct socket *sock, struct sockaddr *saddr,\n\t\t    int *len, int peer)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_getname);\n\nunsigned int sock_no_poll(struct file *file, struct socket *sock, poll_table *pt)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL(sock_no_poll);\n\nint sock_no_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_ioctl);\n\nint sock_no_listen(struct socket *sock, int backlog)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_listen);\n\nint sock_no_shutdown(struct socket *sock, int how)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_shutdown);\n\nint sock_no_setsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, unsigned int optlen)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_setsockopt);\n\nint sock_no_getsockopt(struct socket *sock, int level, int optname,\n\t\t    char __user *optval, int __user *optlen)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_getsockopt);\n\nint sock_no_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *m,\n\t\t    size_t len)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_sendmsg);\n\nint sock_no_recvmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *m,\n\t\t    size_t len, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\nEXPORT_SYMBOL(sock_no_recvmsg);\n\nint sock_no_mmap(struct file *file, struct socket *sock, struct vm_area_struct *vma)\n{\n\t/* Mirror missing mmap method error code */\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL(sock_no_mmap);\n\nssize_t sock_no_sendpage(struct socket *sock, struct page *page, int offset, size_t size, int flags)\n{\n\tssize_t res;\n\tstruct msghdr msg = {.msg_flags = flags};\n\tstruct kvec iov;\n\tchar *kaddr = kmap(page);\n\tiov.iov_base = kaddr + offset;\n\tiov.iov_len = size;\n\tres = kernel_sendmsg(sock, &msg, &iov, 1, size);\n\tkunmap(page);\n\treturn res;\n}\nEXPORT_SYMBOL(sock_no_sendpage);\n\n/*\n *\tDefault Socket Callbacks\n */\n\nstatic void sock_def_wakeup(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (wq_has_sleeper(wq))\n\t\twake_up_interruptible_all(&wq->wait);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_error_report(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (wq_has_sleeper(wq))\n\t\twake_up_interruptible_poll(&wq->wait, POLLERR);\n\tsk_wake_async(sk, SOCK_WAKE_IO, POLL_ERR);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_readable(struct sock *sk, int len)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\twq = rcu_dereference(sk->sk_wq);\n\tif (wq_has_sleeper(wq))\n\t\twake_up_interruptible_sync_poll(&wq->wait, POLLIN | POLLPRI |\n\t\t\t\t\t\tPOLLRDNORM | POLLRDBAND);\n\tsk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN);\n\trcu_read_unlock();\n}\n\nstatic void sock_def_write_space(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\n\t/* Do not wake up a writer until he can make \"significant\"\n\t * progress.  --DaveM\n\t */\n\tif ((atomic_read(&sk->sk_wmem_alloc) << 1) <= sk->sk_sndbuf) {\n\t\twq = rcu_dereference(sk->sk_wq);\n\t\tif (wq_has_sleeper(wq))\n\t\t\twake_up_interruptible_sync_poll(&wq->wait, POLLOUT |\n\t\t\t\t\t\tPOLLWRNORM | POLLWRBAND);\n\n\t\t/* Should agree with poll, otherwise some programs break */\n\t\tif (sock_writeable(sk))\n\t\t\tsk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT);\n\t}\n\n\trcu_read_unlock();\n}\n\nstatic void sock_def_destruct(struct sock *sk)\n{\n\tkfree(sk->sk_protinfo);\n}\n\nvoid sk_send_sigurg(struct sock *sk)\n{\n\tif (sk->sk_socket && sk->sk_socket->file)\n\t\tif (send_sigurg(&sk->sk_socket->file->f_owner))\n\t\t\tsk_wake_async(sk, SOCK_WAKE_URG, POLL_PRI);\n}\nEXPORT_SYMBOL(sk_send_sigurg);\n\nvoid sk_reset_timer(struct sock *sk, struct timer_list* timer,\n\t\t    unsigned long expires)\n{\n\tif (!mod_timer(timer, expires))\n\t\tsock_hold(sk);\n}\nEXPORT_SYMBOL(sk_reset_timer);\n\nvoid sk_stop_timer(struct sock *sk, struct timer_list* timer)\n{\n\tif (timer_pending(timer) && del_timer(timer))\n\t\t__sock_put(sk);\n}\nEXPORT_SYMBOL(sk_stop_timer);\n\nvoid sock_init_data(struct socket *sock, struct sock *sk)\n{\n\tskb_queue_head_init(&sk->sk_receive_queue);\n\tskb_queue_head_init(&sk->sk_write_queue);\n\tskb_queue_head_init(&sk->sk_error_queue);\n#ifdef CONFIG_NET_DMA\n\tskb_queue_head_init(&sk->sk_async_wait_queue);\n#endif\n\n\tsk->sk_send_head\t=\tNULL;\n\n\tinit_timer(&sk->sk_timer);\n\n\tsk->sk_allocation\t=\tGFP_KERNEL;\n\tsk->sk_rcvbuf\t\t=\tsysctl_rmem_default;\n\tsk->sk_sndbuf\t\t=\tsysctl_wmem_default;\n\tsk->sk_state\t\t=\tTCP_CLOSE;\n\tsk_set_socket(sk, sock);\n\n\tsock_set_flag(sk, SOCK_ZAPPED);\n\n\tif (sock) {\n\t\tsk->sk_type\t=\tsock->type;\n\t\tsk->sk_wq\t=\tsock->wq;\n\t\tsock->sk\t=\tsk;\n\t} else\n\t\tsk->sk_wq\t=\tNULL;\n\n\tspin_lock_init(&sk->sk_dst_lock);\n\trwlock_init(&sk->sk_callback_lock);\n\tlockdep_set_class_and_name(&sk->sk_callback_lock,\n\t\t\taf_callback_keys + sk->sk_family,\n\t\t\taf_family_clock_key_strings[sk->sk_family]);\n\n\tsk->sk_state_change\t=\tsock_def_wakeup;\n\tsk->sk_data_ready\t=\tsock_def_readable;\n\tsk->sk_write_space\t=\tsock_def_write_space;\n\tsk->sk_error_report\t=\tsock_def_error_report;\n\tsk->sk_destruct\t\t=\tsock_def_destruct;\n\n\tsk->sk_sndmsg_page\t=\tNULL;\n\tsk->sk_sndmsg_off\t=\t0;\n\tsk->sk_peek_off\t\t=\t-1;\n\n\tsk->sk_peer_pid \t=\tNULL;\n\tsk->sk_peer_cred\t=\tNULL;\n\tsk->sk_write_pending\t=\t0;\n\tsk->sk_rcvlowat\t\t=\t1;\n\tsk->sk_rcvtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\tsk->sk_sndtimeo\t\t=\tMAX_SCHEDULE_TIMEOUT;\n\n\tsk->sk_stamp = ktime_set(-1L, 0);\n\n\t/*\n\t * Before updating sk_refcnt, we must commit prior changes to memory\n\t * (Documentation/RCU/rculist_nulls.txt for details)\n\t */\n\tsmp_wmb();\n\tatomic_set(&sk->sk_refcnt, 1);\n\tatomic_set(&sk->sk_drops, 0);\n}\nEXPORT_SYMBOL(sock_init_data);\n\nvoid lock_sock_nested(struct sock *sk, int subclass)\n{\n\tmight_sleep();\n\tspin_lock_bh(&sk->sk_lock.slock);\n\tif (sk->sk_lock.owned)\n\t\t__lock_sock(sk);\n\tsk->sk_lock.owned = 1;\n\tspin_unlock(&sk->sk_lock.slock);\n\t/*\n\t * The sk_lock has mutex_lock() semantics here:\n\t */\n\tmutex_acquire(&sk->sk_lock.dep_map, subclass, 0, _RET_IP_);\n\tlocal_bh_enable();\n}\nEXPORT_SYMBOL(lock_sock_nested);\n\nvoid release_sock(struct sock *sk)\n{\n\t/*\n\t * The sk_lock has mutex_unlock() semantics:\n\t */\n\tmutex_release(&sk->sk_lock.dep_map, 1, _RET_IP_);\n\n\tspin_lock_bh(&sk->sk_lock.slock);\n\tif (sk->sk_backlog.tail)\n\t\t__release_sock(sk);\n\tsk->sk_lock.owned = 0;\n\tif (waitqueue_active(&sk->sk_lock.wq))\n\t\twake_up(&sk->sk_lock.wq);\n\tspin_unlock_bh(&sk->sk_lock.slock);\n}\nEXPORT_SYMBOL(release_sock);\n\n/**\n * lock_sock_fast - fast version of lock_sock\n * @sk: socket\n *\n * This version should be used for very small section, where process wont block\n * return false if fast path is taken\n *   sk_lock.slock locked, owned = 0, BH disabled\n * return true if slow path is taken\n *   sk_lock.slock unlocked, owned = 1, BH enabled\n */\nbool lock_sock_fast(struct sock *sk)\n{\n\tmight_sleep();\n\tspin_lock_bh(&sk->sk_lock.slock);\n\n\tif (!sk->sk_lock.owned)\n\t\t/*\n\t\t * Note : We must disable BH\n\t\t */\n\t\treturn false;\n\n\t__lock_sock(sk);\n\tsk->sk_lock.owned = 1;\n\tspin_unlock(&sk->sk_lock.slock);\n\t/*\n\t * The sk_lock has mutex_lock() semantics here:\n\t */\n\tmutex_acquire(&sk->sk_lock.dep_map, 0, 0, _RET_IP_);\n\tlocal_bh_enable();\n\treturn true;\n}\nEXPORT_SYMBOL(lock_sock_fast);\n\nint sock_get_timestamp(struct sock *sk, struct timeval __user *userstamp)\n{\n\tstruct timeval tv;\n\tif (!sock_flag(sk, SOCK_TIMESTAMP))\n\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\ttv = ktime_to_timeval(sk->sk_stamp);\n\tif (tv.tv_sec == -1)\n\t\treturn -ENOENT;\n\tif (tv.tv_sec == 0) {\n\t\tsk->sk_stamp = ktime_get_real();\n\t\ttv = ktime_to_timeval(sk->sk_stamp);\n\t}\n\treturn copy_to_user(userstamp, &tv, sizeof(tv)) ? -EFAULT : 0;\n}\nEXPORT_SYMBOL(sock_get_timestamp);\n\nint sock_get_timestampns(struct sock *sk, struct timespec __user *userstamp)\n{\n\tstruct timespec ts;\n\tif (!sock_flag(sk, SOCK_TIMESTAMP))\n\t\tsock_enable_timestamp(sk, SOCK_TIMESTAMP);\n\tts = ktime_to_timespec(sk->sk_stamp);\n\tif (ts.tv_sec == -1)\n\t\treturn -ENOENT;\n\tif (ts.tv_sec == 0) {\n\t\tsk->sk_stamp = ktime_get_real();\n\t\tts = ktime_to_timespec(sk->sk_stamp);\n\t}\n\treturn copy_to_user(userstamp, &ts, sizeof(ts)) ? -EFAULT : 0;\n}\nEXPORT_SYMBOL(sock_get_timestampns);\n\nvoid sock_enable_timestamp(struct sock *sk, int flag)\n{\n\tif (!sock_flag(sk, flag)) {\n\t\tunsigned long previous_flags = sk->sk_flags;\n\n\t\tsock_set_flag(sk, flag);\n\t\t/*\n\t\t * we just set one of the two flags which require net\n\t\t * time stamping, but time stamping might have been on\n\t\t * already because of the other one\n\t\t */\n\t\tif (!(previous_flags & SK_FLAGS_TIMESTAMP))\n\t\t\tnet_enable_timestamp();\n\t}\n}\n\n/*\n *\tGet a socket option on an socket.\n *\n *\tFIX: POSIX 1003.1g is very ambiguous here. It states that\n *\tasynchronous errors should be reported by getsockopt. We assume\n *\tthis means if you specify SO_ERROR (otherwise whats the point of it).\n */\nint sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(sock_common_getsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t\t  char __user *optval, int __user *optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk->sk_prot->compat_getsockopt != NULL)\n\t\treturn sk->sk_prot->compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t      optval, optlen);\n\treturn sk->sk_prot->getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_sock_common_getsockopt);\n#endif\n\nint sock_common_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\tstruct msghdr *msg, size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tint addr_len = 0;\n\tint err;\n\n\terr = sk->sk_prot->recvmsg(iocb, sk, msg, size, flags & MSG_DONTWAIT,\n\t\t\t\t   flags & ~MSG_DONTWAIT, &addr_len);\n\tif (err >= 0)\n\t\tmsg->msg_namelen = addr_len;\n\treturn err;\n}\nEXPORT_SYMBOL(sock_common_recvmsg);\n\n/*\n *\tSet socket options on an inet socket.\n */\nint sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t   char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(sock_common_setsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk->sk_prot->compat_setsockopt != NULL)\n\t\treturn sk->sk_prot->compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t      optval, optlen);\n\treturn sk->sk_prot->setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_sock_common_setsockopt);\n#endif\n\nvoid sk_common_release(struct sock *sk)\n{\n\tif (sk->sk_prot->destroy)\n\t\tsk->sk_prot->destroy(sk);\n\n\t/*\n\t * Observation: when sock_common_release is called, processes have\n\t * no access to socket. But net still has.\n\t * Step one, detach it from networking:\n\t *\n\t * A. Remove from hash tables.\n\t */\n\n\tsk->sk_prot->unhash(sk);\n\n\t/*\n\t * In this point socket cannot receive new packets, but it is possible\n\t * that some packets are in flight because some CPU runs receiver and\n\t * did hash table lookup before we unhashed socket. They will achieve\n\t * receive queue and will be purged by socket destructor.\n\t *\n\t * Also we still have packets pending on receive queue and probably,\n\t * our own packets waiting in device queues. sock_destroy will drain\n\t * receive queue, but transmitted packets will delay socket destruction\n\t * until the last reference will be released.\n\t */\n\n\tsock_orphan(sk);\n\n\txfrm_sk_free_policy(sk);\n\n\tsk_refcnt_debug_release(sk);\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(sk_common_release);\n\n#ifdef CONFIG_PROC_FS\n#define PROTO_INUSE_NR\t64\t/* should be enough for the first time */\nstruct prot_inuse {\n\tint val[PROTO_INUSE_NR];\n};\n\nstatic DECLARE_BITMAP(proto_inuse_idx, PROTO_INUSE_NR);\n\n#ifdef CONFIG_NET_NS\nvoid sock_prot_inuse_add(struct net *net, struct proto *prot, int val)\n{\n\t__this_cpu_add(net->core.inuse->val[prot->inuse_idx], val);\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_add);\n\nint sock_prot_inuse_get(struct net *net, struct proto *prot)\n{\n\tint cpu, idx = prot->inuse_idx;\n\tint res = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tres += per_cpu_ptr(net->core.inuse, cpu)->val[idx];\n\n\treturn res >= 0 ? res : 0;\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_get);\n\nstatic int __net_init sock_inuse_init_net(struct net *net)\n{\n\tnet->core.inuse = alloc_percpu(struct prot_inuse);\n\treturn net->core.inuse ? 0 : -ENOMEM;\n}\n\nstatic void __net_exit sock_inuse_exit_net(struct net *net)\n{\n\tfree_percpu(net->core.inuse);\n}\n\nstatic struct pernet_operations net_inuse_ops = {\n\t.init = sock_inuse_init_net,\n\t.exit = sock_inuse_exit_net,\n};\n\nstatic __init int net_inuse_init(void)\n{\n\tif (register_pernet_subsys(&net_inuse_ops))\n\t\tpanic(\"Cannot initialize net inuse counters\");\n\n\treturn 0;\n}\n\ncore_initcall(net_inuse_init);\n#else\nstatic DEFINE_PER_CPU(struct prot_inuse, prot_inuse);\n\nvoid sock_prot_inuse_add(struct net *net, struct proto *prot, int val)\n{\n\t__this_cpu_add(prot_inuse.val[prot->inuse_idx], val);\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_add);\n\nint sock_prot_inuse_get(struct net *net, struct proto *prot)\n{\n\tint cpu, idx = prot->inuse_idx;\n\tint res = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tres += per_cpu(prot_inuse, cpu).val[idx];\n\n\treturn res >= 0 ? res : 0;\n}\nEXPORT_SYMBOL_GPL(sock_prot_inuse_get);\n#endif\n\nstatic void assign_proto_idx(struct proto *prot)\n{\n\tprot->inuse_idx = find_first_zero_bit(proto_inuse_idx, PROTO_INUSE_NR);\n\n\tif (unlikely(prot->inuse_idx == PROTO_INUSE_NR - 1)) {\n\t\tprintk(KERN_ERR \"PROTO_INUSE_NR exhausted\\n\");\n\t\treturn;\n\t}\n\n\tset_bit(prot->inuse_idx, proto_inuse_idx);\n}\n\nstatic void release_proto_idx(struct proto *prot)\n{\n\tif (prot->inuse_idx != PROTO_INUSE_NR - 1)\n\t\tclear_bit(prot->inuse_idx, proto_inuse_idx);\n}\n#else\nstatic inline void assign_proto_idx(struct proto *prot)\n{\n}\n\nstatic inline void release_proto_idx(struct proto *prot)\n{\n}\n#endif\n\nint proto_register(struct proto *prot, int alloc_slab)\n{\n\tif (alloc_slab) {\n\t\tprot->slab = kmem_cache_create(prot->name, prot->obj_size, 0,\n\t\t\t\t\tSLAB_HWCACHE_ALIGN | prot->slab_flags,\n\t\t\t\t\tNULL);\n\n\t\tif (prot->slab == NULL) {\n\t\t\tprintk(KERN_CRIT \"%s: Can't create sock SLAB cache!\\n\",\n\t\t\t       prot->name);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (prot->rsk_prot != NULL) {\n\t\t\tprot->rsk_prot->slab_name = kasprintf(GFP_KERNEL, \"request_sock_%s\", prot->name);\n\t\t\tif (prot->rsk_prot->slab_name == NULL)\n\t\t\t\tgoto out_free_sock_slab;\n\n\t\t\tprot->rsk_prot->slab = kmem_cache_create(prot->rsk_prot->slab_name,\n\t\t\t\t\t\t\t\t prot->rsk_prot->obj_size, 0,\n\t\t\t\t\t\t\t\t SLAB_HWCACHE_ALIGN, NULL);\n\n\t\t\tif (prot->rsk_prot->slab == NULL) {\n\t\t\t\tprintk(KERN_CRIT \"%s: Can't create request sock SLAB cache!\\n\",\n\t\t\t\t       prot->name);\n\t\t\t\tgoto out_free_request_sock_slab_name;\n\t\t\t}\n\t\t}\n\n\t\tif (prot->twsk_prot != NULL) {\n\t\t\tprot->twsk_prot->twsk_slab_name = kasprintf(GFP_KERNEL, \"tw_sock_%s\", prot->name);\n\n\t\t\tif (prot->twsk_prot->twsk_slab_name == NULL)\n\t\t\t\tgoto out_free_request_sock_slab;\n\n\t\t\tprot->twsk_prot->twsk_slab =\n\t\t\t\tkmem_cache_create(prot->twsk_prot->twsk_slab_name,\n\t\t\t\t\t\t  prot->twsk_prot->twsk_obj_size,\n\t\t\t\t\t\t  0,\n\t\t\t\t\t\t  SLAB_HWCACHE_ALIGN |\n\t\t\t\t\t\t\tprot->slab_flags,\n\t\t\t\t\t\t  NULL);\n\t\t\tif (prot->twsk_prot->twsk_slab == NULL)\n\t\t\t\tgoto out_free_timewait_sock_slab_name;\n\t\t}\n\t}\n\n\tmutex_lock(&proto_list_mutex);\n\tlist_add(&prot->node, &proto_list);\n\tassign_proto_idx(prot);\n\tmutex_unlock(&proto_list_mutex);\n\treturn 0;\n\nout_free_timewait_sock_slab_name:\n\tkfree(prot->twsk_prot->twsk_slab_name);\nout_free_request_sock_slab:\n\tif (prot->rsk_prot && prot->rsk_prot->slab) {\n\t\tkmem_cache_destroy(prot->rsk_prot->slab);\n\t\tprot->rsk_prot->slab = NULL;\n\t}\nout_free_request_sock_slab_name:\n\tif (prot->rsk_prot)\n\t\tkfree(prot->rsk_prot->slab_name);\nout_free_sock_slab:\n\tkmem_cache_destroy(prot->slab);\n\tprot->slab = NULL;\nout:\n\treturn -ENOBUFS;\n}\nEXPORT_SYMBOL(proto_register);\n\nvoid proto_unregister(struct proto *prot)\n{\n\tmutex_lock(&proto_list_mutex);\n\trelease_proto_idx(prot);\n\tlist_del(&prot->node);\n\tmutex_unlock(&proto_list_mutex);\n\n\tif (prot->slab != NULL) {\n\t\tkmem_cache_destroy(prot->slab);\n\t\tprot->slab = NULL;\n\t}\n\n\tif (prot->rsk_prot != NULL && prot->rsk_prot->slab != NULL) {\n\t\tkmem_cache_destroy(prot->rsk_prot->slab);\n\t\tkfree(prot->rsk_prot->slab_name);\n\t\tprot->rsk_prot->slab = NULL;\n\t}\n\n\tif (prot->twsk_prot != NULL && prot->twsk_prot->twsk_slab != NULL) {\n\t\tkmem_cache_destroy(prot->twsk_prot->twsk_slab);\n\t\tkfree(prot->twsk_prot->twsk_slab_name);\n\t\tprot->twsk_prot->twsk_slab = NULL;\n\t}\n}\nEXPORT_SYMBOL(proto_unregister);\n\n#ifdef CONFIG_PROC_FS\nstatic void *proto_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(proto_list_mutex)\n{\n\tmutex_lock(&proto_list_mutex);\n\treturn seq_list_start_head(&proto_list, *pos);\n}\n\nstatic void *proto_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\treturn seq_list_next(v, &proto_list, pos);\n}\n\nstatic void proto_seq_stop(struct seq_file *seq, void *v)\n\t__releases(proto_list_mutex)\n{\n\tmutex_unlock(&proto_list_mutex);\n}\n\nstatic char proto_method_implemented(const void *method)\n{\n\treturn method == NULL ? 'n' : 'y';\n}\nstatic long sock_prot_memory_allocated(struct proto *proto)\n{\n\treturn proto->memory_allocated != NULL ? proto_memory_allocated(proto): -1L;\n}\n\nstatic char *sock_prot_memory_pressure(struct proto *proto)\n{\n\treturn proto->memory_pressure != NULL ?\n\tproto_memory_pressure(proto) ? \"yes\" : \"no\" : \"NI\";\n}\n\nstatic void proto_seq_printf(struct seq_file *seq, struct proto *proto)\n{\n\n\tseq_printf(seq, \"%-9s %4u %6d  %6ld   %-3s %6u   %-3s  %-10s \"\n\t\t\t\"%2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c %2c\\n\",\n\t\t   proto->name,\n\t\t   proto->obj_size,\n\t\t   sock_prot_inuse_get(seq_file_net(seq), proto),\n\t\t   sock_prot_memory_allocated(proto),\n\t\t   sock_prot_memory_pressure(proto),\n\t\t   proto->max_header,\n\t\t   proto->slab == NULL ? \"no\" : \"yes\",\n\t\t   module_name(proto->owner),\n\t\t   proto_method_implemented(proto->close),\n\t\t   proto_method_implemented(proto->connect),\n\t\t   proto_method_implemented(proto->disconnect),\n\t\t   proto_method_implemented(proto->accept),\n\t\t   proto_method_implemented(proto->ioctl),\n\t\t   proto_method_implemented(proto->init),\n\t\t   proto_method_implemented(proto->destroy),\n\t\t   proto_method_implemented(proto->shutdown),\n\t\t   proto_method_implemented(proto->setsockopt),\n\t\t   proto_method_implemented(proto->getsockopt),\n\t\t   proto_method_implemented(proto->sendmsg),\n\t\t   proto_method_implemented(proto->recvmsg),\n\t\t   proto_method_implemented(proto->sendpage),\n\t\t   proto_method_implemented(proto->bind),\n\t\t   proto_method_implemented(proto->backlog_rcv),\n\t\t   proto_method_implemented(proto->hash),\n\t\t   proto_method_implemented(proto->unhash),\n\t\t   proto_method_implemented(proto->get_port),\n\t\t   proto_method_implemented(proto->enter_memory_pressure));\n}\n\nstatic int proto_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == &proto_list)\n\t\tseq_printf(seq, \"%-9s %-4s %-8s %-6s %-5s %-7s %-4s %-10s %s\",\n\t\t\t   \"protocol\",\n\t\t\t   \"size\",\n\t\t\t   \"sockets\",\n\t\t\t   \"memory\",\n\t\t\t   \"press\",\n\t\t\t   \"maxhdr\",\n\t\t\t   \"slab\",\n\t\t\t   \"module\",\n\t\t\t   \"cl co di ac io in de sh ss gs se re sp bi br ha uh gp em\\n\");\n\telse\n\t\tproto_seq_printf(seq, list_entry(v, struct proto, node));\n\treturn 0;\n}\n\nstatic const struct seq_operations proto_seq_ops = {\n\t.start  = proto_seq_start,\n\t.next   = proto_seq_next,\n\t.stop   = proto_seq_stop,\n\t.show   = proto_seq_show,\n};\n\nstatic int proto_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &proto_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations proto_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= proto_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\nstatic __net_init int proto_init_net(struct net *net)\n{\n\tif (!proc_net_fops_create(net, \"protocols\", S_IRUGO, &proto_seq_fops))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic __net_exit void proto_exit_net(struct net *net)\n{\n\tproc_net_remove(net, \"protocols\");\n}\n\n\nstatic __net_initdata struct pernet_operations proto_net_ops = {\n\t.init = proto_init_net,\n\t.exit = proto_exit_net,\n};\n\nstatic int __init proto_init(void)\n{\n\treturn register_pernet_subsys(&proto_net_ops);\n}\n\nsubsys_initcall(proto_init);\n\n#endif /* PROC_FS */\n"], "filenames": ["net/core/sock.c"], "buggy_code_start_loc": [580], "buggy_code_end_loc": [1014], "fixing_code_start_loc": [580], "fixing_code_end_loc": [1002], "type": "CWE-119", "message": "The sock_setsockopt function in net/core/sock.c in the Linux kernel before 3.5 mishandles negative values of sk_sndbuf and sk_rcvbuf, which allows local users to cause a denial of service (memory corruption and system crash) or possibly have unspecified other impact by leveraging the CAP_NET_ADMIN capability for a crafted setsockopt system call with the (1) SO_SNDBUF or (2) SO_RCVBUF option.", "other": {"cve": {"id": "CVE-2012-6704", "sourceIdentifier": "cve@mitre.org", "published": "2016-12-28T07:59:00.150", "lastModified": "2023-01-17T21:45:17.923", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The sock_setsockopt function in net/core/sock.c in the Linux kernel before 3.5 mishandles negative values of sk_sndbuf and sk_rcvbuf, which allows local users to cause a denial of service (memory corruption and system crash) or possibly have unspecified other impact by leveraging the CAP_NET_ADMIN capability for a crafted setsockopt system call with the (1) SO_SNDBUF or (2) SO_RCVBUF option."}, {"lang": "es", "value": "La funci\u00f3n sock_setsockopt en net/core/sock.c en el kernel de Linux en versiones anteriores a 3.5 no maneja adecuadamente valores negativos de sk_sndbuf y sk_rcvbuf, lo que permite a usuarios locales provocar una denegaci\u00f3n de servicio (corrupci\u00f3n de memoria y ca\u00edda del sistema) o posiblemente tener otro impacto no especificado aprovechando la capacidad CAP_NET_ADMIN para una llamada al sistema setsockopt manipulada con la opci\u00f3n (1) SO_SNDBUF o (2) SO_RCVBUF."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.2.85", "matchCriteriaId": "9A5A178A-A60C-4053-AEE0-5164430206AD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.3", "versionEndExcluding": "3.5", "matchCriteriaId": "122A7DB7-4CDA-41A9-84AC-A877B6100AF9"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=82981930125abfd39d7c8378a9cfdf5e1be2002b", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/12/03/1", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securityfocus.com/bid/95135", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1402024", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/82981930125abfd39d7c8378a9cfdf5e1be2002b", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/82981930125abfd39d7c8378a9cfdf5e1be2002b"}}