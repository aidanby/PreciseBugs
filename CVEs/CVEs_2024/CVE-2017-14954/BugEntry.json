{"buggy_code": ["/*\n *  linux/kernel/exit.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/task.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/cputime.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/capability.h>\n#include <linux/completion.h>\n#include <linux/personality.h>\n#include <linux/tty.h>\n#include <linux/iocontext.h>\n#include <linux/key.h>\n#include <linux/cpu.h>\n#include <linux/acct.h>\n#include <linux/tsacct_kern.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/freezer.h>\n#include <linux/binfmts.h>\n#include <linux/nsproxy.h>\n#include <linux/pid_namespace.h>\n#include <linux/ptrace.h>\n#include <linux/profile.h>\n#include <linux/mount.h>\n#include <linux/proc_fs.h>\n#include <linux/kthread.h>\n#include <linux/mempolicy.h>\n#include <linux/taskstats_kern.h>\n#include <linux/delayacct.h>\n#include <linux/cgroup.h>\n#include <linux/syscalls.h>\n#include <linux/signal.h>\n#include <linux/posix-timers.h>\n#include <linux/cn_proc.h>\n#include <linux/mutex.h>\n#include <linux/futex.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/resource.h>\n#include <linux/blkdev.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/tracehook.h>\n#include <linux/fs_struct.h>\n#include <linux/init_task.h>\n#include <linux/perf_event.h>\n#include <trace/events/sched.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/oom.h>\n#include <linux/writeback.h>\n#include <linux/shm.h>\n#include <linux/kcov.h>\n#include <linux/random.h>\n#include <linux/rcuwait.h>\n#include <linux/compat.h>\n\n#include <linux/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/pgtable.h>\n#include <asm/mmu_context.h>\n\nstatic void __unhash_process(struct task_struct *p, bool group_dead)\n{\n\tnr_threads--;\n\tdetach_pid(p, PIDTYPE_PID);\n\tif (group_dead) {\n\t\tdetach_pid(p, PIDTYPE_PGID);\n\t\tdetach_pid(p, PIDTYPE_SID);\n\n\t\tlist_del_rcu(&p->tasks);\n\t\tlist_del_init(&p->sibling);\n\t\t__this_cpu_dec(process_counts);\n\t}\n\tlist_del_rcu(&p->thread_group);\n\tlist_del_rcu(&p->thread_node);\n}\n\n/*\n * This function expects the tasklist_lock write-locked.\n */\nstatic void __exit_signal(struct task_struct *tsk)\n{\n\tstruct signal_struct *sig = tsk->signal;\n\tbool group_dead = thread_group_leader(tsk);\n\tstruct sighand_struct *sighand;\n\tstruct tty_struct *uninitialized_var(tty);\n\tu64 utime, stime;\n\n\tsighand = rcu_dereference_check(tsk->sighand,\n\t\t\t\t\tlockdep_tasklist_lock_is_held());\n\tspin_lock(&sighand->siglock);\n\n#ifdef CONFIG_POSIX_TIMERS\n\tposix_cpu_timers_exit(tsk);\n\tif (group_dead) {\n\t\tposix_cpu_timers_exit_group(tsk);\n\t} else {\n\t\t/*\n\t\t * This can only happen if the caller is de_thread().\n\t\t * FIXME: this is the temporary hack, we should teach\n\t\t * posix-cpu-timers to handle this case correctly.\n\t\t */\n\t\tif (unlikely(has_group_leader_pid(tsk)))\n\t\t\tposix_cpu_timers_exit_group(tsk);\n\t}\n#endif\n\n\tif (group_dead) {\n\t\ttty = sig->tty;\n\t\tsig->tty = NULL;\n\t} else {\n\t\t/*\n\t\t * If there is any task waiting for the group exit\n\t\t * then notify it:\n\t\t */\n\t\tif (sig->notify_count > 0 && !--sig->notify_count)\n\t\t\twake_up_process(sig->group_exit_task);\n\n\t\tif (tsk == sig->curr_target)\n\t\t\tsig->curr_target = next_thread(tsk);\n\t}\n\n\tadd_device_randomness((const void*) &tsk->se.sum_exec_runtime,\n\t\t\t      sizeof(unsigned long long));\n\n\t/*\n\t * Accumulate here the counters for all threads as they die. We could\n\t * skip the group leader because it is the last user of signal_struct,\n\t * but we want to avoid the race with thread_group_cputime() which can\n\t * see the empty ->thread_head list.\n\t */\n\ttask_cputime(tsk, &utime, &stime);\n\twrite_seqlock(&sig->stats_lock);\n\tsig->utime += utime;\n\tsig->stime += stime;\n\tsig->gtime += task_gtime(tsk);\n\tsig->min_flt += tsk->min_flt;\n\tsig->maj_flt += tsk->maj_flt;\n\tsig->nvcsw += tsk->nvcsw;\n\tsig->nivcsw += tsk->nivcsw;\n\tsig->inblock += task_io_get_inblock(tsk);\n\tsig->oublock += task_io_get_oublock(tsk);\n\ttask_io_accounting_add(&sig->ioac, &tsk->ioac);\n\tsig->sum_sched_runtime += tsk->se.sum_exec_runtime;\n\tsig->nr_threads--;\n\t__unhash_process(tsk, group_dead);\n\twrite_sequnlock(&sig->stats_lock);\n\n\t/*\n\t * Do this under ->siglock, we can race with another thread\n\t * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.\n\t */\n\tflush_sigqueue(&tsk->pending);\n\ttsk->sighand = NULL;\n\tspin_unlock(&sighand->siglock);\n\n\t__cleanup_sighand(sighand);\n\tclear_tsk_thread_flag(tsk, TIF_SIGPENDING);\n\tif (group_dead) {\n\t\tflush_sigqueue(&sig->shared_pending);\n\t\ttty_kref_put(tty);\n\t}\n}\n\nstatic void delayed_put_task_struct(struct rcu_head *rhp)\n{\n\tstruct task_struct *tsk = container_of(rhp, struct task_struct, rcu);\n\n\tperf_event_delayed_put(tsk);\n\ttrace_sched_process_free(tsk);\n\tput_task_struct(tsk);\n}\n\n\nvoid release_task(struct task_struct *p)\n{\n\tstruct task_struct *leader;\n\tint zap_leader;\nrepeat:\n\t/* don't need to get the RCU readlock here - the process is dead and\n\t * can't be modifying its own credentials. But shut RCU-lockdep up */\n\trcu_read_lock();\n\tatomic_dec(&__task_cred(p)->user->processes);\n\trcu_read_unlock();\n\n\tproc_flush_task(p);\n\n\twrite_lock_irq(&tasklist_lock);\n\tptrace_release_task(p);\n\t__exit_signal(p);\n\n\t/*\n\t * If we are the last non-leader member of the thread\n\t * group, and the leader is zombie, then notify the\n\t * group leader's parent process. (if it wants notification.)\n\t */\n\tzap_leader = 0;\n\tleader = p->group_leader;\n\tif (leader != p && thread_group_empty(leader)\n\t\t\t&& leader->exit_state == EXIT_ZOMBIE) {\n\t\t/*\n\t\t * If we were the last child thread and the leader has\n\t\t * exited already, and the leader's parent ignores SIGCHLD,\n\t\t * then we are the one who should release the leader.\n\t\t */\n\t\tzap_leader = do_notify_parent(leader, leader->exit_signal);\n\t\tif (zap_leader)\n\t\t\tleader->exit_state = EXIT_DEAD;\n\t}\n\n\twrite_unlock_irq(&tasklist_lock);\n\trelease_thread(p);\n\tcall_rcu(&p->rcu, delayed_put_task_struct);\n\n\tp = leader;\n\tif (unlikely(zap_leader))\n\t\tgoto repeat;\n}\n\n/*\n * Note that if this function returns a valid task_struct pointer (!NULL)\n * task->usage must remain >0 for the duration of the RCU critical section.\n */\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}\n\nvoid rcuwait_wake_up(struct rcuwait *w)\n{\n\tstruct task_struct *task;\n\n\trcu_read_lock();\n\n\t/*\n\t * Order condition vs @task, such that everything prior to the load\n\t * of @task is visible. This is the condition as to why the user called\n\t * rcuwait_trywake() in the first place. Pairs with set_current_state()\n\t * barrier (A) in rcuwait_wait_event().\n\t *\n\t *    WAIT                WAKE\n\t *    [S] tsk = current\t  [S] cond = true\n\t *        MB (A)\t      MB (B)\n\t *    [L] cond\t\t  [L] tsk\n\t */\n\tsmp_rmb(); /* (B) */\n\n\t/*\n\t * Avoid using task_rcu_dereference() magic as long as we are careful,\n\t * see comment in rcuwait_wait_event() regarding ->exit_state.\n\t */\n\ttask = rcu_dereference(w->task);\n\tif (task)\n\t\twake_up_process(task);\n\trcu_read_unlock();\n}\n\n/*\n * Determine if a process group is \"orphaned\", according to the POSIX\n * definition in 2.2.2.52.  Orphaned process groups are not to be affected\n * by terminal-generated stop signals.  Newly orphaned process groups are\n * to receive a SIGHUP and a SIGCONT.\n *\n * \"I ask you, have you ever known what it is to be an orphan?\"\n */\nstatic int will_become_orphaned_pgrp(struct pid *pgrp,\n\t\t\t\t\tstruct task_struct *ignored_task)\n{\n\tstruct task_struct *p;\n\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tif ((p == ignored_task) ||\n\t\t    (p->exit_state && thread_group_empty(p)) ||\n\t\t    is_global_init(p->real_parent))\n\t\t\tcontinue;\n\n\t\tif (task_pgrp(p->real_parent) != pgrp &&\n\t\t    task_session(p->real_parent) == task_session(p))\n\t\t\treturn 0;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\n\treturn 1;\n}\n\nint is_current_pgrp_orphaned(void)\n{\n\tint retval;\n\n\tread_lock(&tasklist_lock);\n\tretval = will_become_orphaned_pgrp(task_pgrp(current), NULL);\n\tread_unlock(&tasklist_lock);\n\n\treturn retval;\n}\n\nstatic bool has_stopped_jobs(struct pid *pgrp)\n{\n\tstruct task_struct *p;\n\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tif (p->signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\treturn true;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\n\treturn false;\n}\n\n/*\n * Check to see if any process groups have become orphaned as\n * a result of our exiting, and if they have any stopped jobs,\n * send them a SIGHUP and then a SIGCONT. (POSIX 3.2.2.2)\n */\nstatic void\nkill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)\n{\n\tstruct pid *pgrp = task_pgrp(tsk);\n\tstruct task_struct *ignored_task = tsk;\n\n\tif (!parent)\n\t\t/* exit: our father is in a different pgrp than\n\t\t * we are and we were the only connection outside.\n\t\t */\n\t\tparent = tsk->real_parent;\n\telse\n\t\t/* reparent: our child is in a different pgrp than\n\t\t * we are, and it was the only connection outside.\n\t\t */\n\t\tignored_task = NULL;\n\n\tif (task_pgrp(parent) != pgrp &&\n\t    task_session(parent) == task_session(tsk) &&\n\t    will_become_orphaned_pgrp(pgrp, ignored_task) &&\n\t    has_stopped_jobs(pgrp)) {\n\t\t__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);\n\t\t__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);\n\t}\n}\n\n#ifdef CONFIG_MEMCG\n/*\n * A task is exiting.   If it owned this mm, find a new owner for the mm.\n */\nvoid mm_update_next_owner(struct mm_struct *mm)\n{\n\tstruct task_struct *c, *g, *p = current;\n\nretry:\n\t/*\n\t * If the exiting or execing task is not the owner, it's\n\t * someone else's problem.\n\t */\n\tif (mm->owner != p)\n\t\treturn;\n\t/*\n\t * The current owner is exiting/execing and there are no other\n\t * candidates.  Do not leave the mm pointing to a possibly\n\t * freed task structure.\n\t */\n\tif (atomic_read(&mm->mm_users) <= 1) {\n\t\tmm->owner = NULL;\n\t\treturn;\n\t}\n\n\tread_lock(&tasklist_lock);\n\t/*\n\t * Search in the children\n\t */\n\tlist_for_each_entry(c, &p->children, sibling) {\n\t\tif (c->mm == mm)\n\t\t\tgoto assign_new_owner;\n\t}\n\n\t/*\n\t * Search in the siblings\n\t */\n\tlist_for_each_entry(c, &p->real_parent->children, sibling) {\n\t\tif (c->mm == mm)\n\t\t\tgoto assign_new_owner;\n\t}\n\n\t/*\n\t * Search through everything else, we should not get here often.\n\t */\n\tfor_each_process(g) {\n\t\tif (g->flags & PF_KTHREAD)\n\t\t\tcontinue;\n\t\tfor_each_thread(g, c) {\n\t\t\tif (c->mm == mm)\n\t\t\t\tgoto assign_new_owner;\n\t\t\tif (c->mm)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tread_unlock(&tasklist_lock);\n\t/*\n\t * We found no owner yet mm_users > 1: this implies that we are\n\t * most likely racing with swapoff (try_to_unuse()) or /proc or\n\t * ptrace or page migration (get_task_mm()).  Mark owner as NULL.\n\t */\n\tmm->owner = NULL;\n\treturn;\n\nassign_new_owner:\n\tBUG_ON(c == p);\n\tget_task_struct(c);\n\t/*\n\t * The task_lock protects c->mm from changing.\n\t * We always want mm->owner->mm == mm\n\t */\n\ttask_lock(c);\n\t/*\n\t * Delay read_unlock() till we have the task_lock()\n\t * to ensure that c does not slip away underneath us\n\t */\n\tread_unlock(&tasklist_lock);\n\tif (c->mm != mm) {\n\t\ttask_unlock(c);\n\t\tput_task_struct(c);\n\t\tgoto retry;\n\t}\n\tmm->owner = c;\n\ttask_unlock(c);\n\tput_task_struct(c);\n}\n#endif /* CONFIG_MEMCG */\n\n/*\n * Turn us into a lazy TLB process if we\n * aren't already..\n */\nstatic void exit_mm(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct core_state *core_state;\n\n\tmm_release(current, mm);\n\tif (!mm)\n\t\treturn;\n\tsync_mm_rss(mm);\n\t/*\n\t * Serialize with any possible pending coredump.\n\t * We must hold mmap_sem around checking core_state\n\t * and clearing tsk->mm.  The core-inducing thread\n\t * will increment ->nr_threads for each thread in the\n\t * group with ->mm != NULL.\n\t */\n\tdown_read(&mm->mmap_sem);\n\tcore_state = mm->core_state;\n\tif (core_state) {\n\t\tstruct core_thread self;\n\n\t\tup_read(&mm->mmap_sem);\n\n\t\tself.task = current;\n\t\tself.next = xchg(&core_state->dumper.next, &self);\n\t\t/*\n\t\t * Implies mb(), the result of xchg() must be visible\n\t\t * to core_state->dumper.\n\t\t */\n\t\tif (atomic_dec_and_test(&core_state->nr_threads))\n\t\t\tcomplete(&core_state->startup);\n\n\t\tfor (;;) {\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tif (!self.task) /* see coredump_finish() */\n\t\t\t\tbreak;\n\t\t\tfreezable_schedule();\n\t\t}\n\t\t__set_current_state(TASK_RUNNING);\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\tmmgrab(mm);\n\tBUG_ON(mm != current->active_mm);\n\t/* more a memory barrier than a real lock */\n\ttask_lock(current);\n\tcurrent->mm = NULL;\n\tup_read(&mm->mmap_sem);\n\tenter_lazy_tlb(mm, current);\n\ttask_unlock(current);\n\tmm_update_next_owner(mm);\n\tmmput(mm);\n\tif (test_thread_flag(TIF_MEMDIE))\n\t\texit_oom_victim();\n}\n\nstatic struct task_struct *find_alive_thread(struct task_struct *p)\n{\n\tstruct task_struct *t;\n\n\tfor_each_thread(p, t) {\n\t\tif (!(t->flags & PF_EXITING))\n\t\t\treturn t;\n\t}\n\treturn NULL;\n}\n\nstatic struct task_struct *find_child_reaper(struct task_struct *father)\n\t__releases(&tasklist_lock)\n\t__acquires(&tasklist_lock)\n{\n\tstruct pid_namespace *pid_ns = task_active_pid_ns(father);\n\tstruct task_struct *reaper = pid_ns->child_reaper;\n\n\tif (likely(reaper != father))\n\t\treturn reaper;\n\n\treaper = find_alive_thread(father);\n\tif (reaper) {\n\t\tpid_ns->child_reaper = reaper;\n\t\treturn reaper;\n\t}\n\n\twrite_unlock_irq(&tasklist_lock);\n\tif (unlikely(pid_ns == &init_pid_ns)) {\n\t\tpanic(\"Attempted to kill init! exitcode=0x%08x\\n\",\n\t\t\tfather->signal->group_exit_code ?: father->exit_code);\n\t}\n\tzap_pid_ns_processes(pid_ns);\n\twrite_lock_irq(&tasklist_lock);\n\n\treturn father;\n}\n\n/*\n * When we die, we re-parent all our children, and try to:\n * 1. give them to another thread in our thread group, if such a member exists\n * 2. give it to the first ancestor process which prctl'd itself as a\n *    child_subreaper for its children (like a service manager)\n * 3. give it to the init process (PID 1) in our pid namespace\n */\nstatic struct task_struct *find_new_reaper(struct task_struct *father,\n\t\t\t\t\t   struct task_struct *child_reaper)\n{\n\tstruct task_struct *thread, *reaper;\n\n\tthread = find_alive_thread(father);\n\tif (thread)\n\t\treturn thread;\n\n\tif (father->signal->has_child_subreaper) {\n\t\tunsigned int ns_level = task_pid(father)->level;\n\t\t/*\n\t\t * Find the first ->is_child_subreaper ancestor in our pid_ns.\n\t\t * We can't check reaper != child_reaper to ensure we do not\n\t\t * cross the namespaces, the exiting parent could be injected\n\t\t * by setns() + fork().\n\t\t * We check pid->level, this is slightly more efficient than\n\t\t * task_active_pid_ns(reaper) != task_active_pid_ns(father).\n\t\t */\n\t\tfor (reaper = father->real_parent;\n\t\t     task_pid(reaper)->level == ns_level;\n\t\t     reaper = reaper->real_parent) {\n\t\t\tif (reaper == &init_task)\n\t\t\t\tbreak;\n\t\t\tif (!reaper->signal->is_child_subreaper)\n\t\t\t\tcontinue;\n\t\t\tthread = find_alive_thread(reaper);\n\t\t\tif (thread)\n\t\t\t\treturn thread;\n\t\t}\n\t}\n\n\treturn child_reaper;\n}\n\n/*\n* Any that need to be release_task'd are put on the @dead list.\n */\nstatic void reparent_leader(struct task_struct *father, struct task_struct *p,\n\t\t\t\tstruct list_head *dead)\n{\n\tif (unlikely(p->exit_state == EXIT_DEAD))\n\t\treturn;\n\n\t/* We don't want people slaying init. */\n\tp->exit_signal = SIGCHLD;\n\n\t/* If it has exited notify the new parent about this child's death. */\n\tif (!p->ptrace &&\n\t    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {\n\t\tif (do_notify_parent(p, p->exit_signal)) {\n\t\t\tp->exit_state = EXIT_DEAD;\n\t\t\tlist_add(&p->ptrace_entry, dead);\n\t\t}\n\t}\n\n\tkill_orphaned_pgrp(p, father);\n}\n\n/*\n * This does two things:\n *\n * A.  Make init inherit all the child processes\n * B.  Check to see if any process groups have become orphaned\n *\tas a result of our exiting, and if they have any stopped\n *\tjobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)\n */\nstatic void forget_original_parent(struct task_struct *father,\n\t\t\t\t\tstruct list_head *dead)\n{\n\tstruct task_struct *p, *t, *reaper;\n\n\tif (unlikely(!list_empty(&father->ptraced)))\n\t\texit_ptrace(father, dead);\n\n\t/* Can drop and reacquire tasklist_lock */\n\treaper = find_child_reaper(father);\n\tif (list_empty(&father->children))\n\t\treturn;\n\n\treaper = find_new_reaper(father, reaper);\n\tlist_for_each_entry(p, &father->children, sibling) {\n\t\tfor_each_thread(p, t) {\n\t\t\tt->real_parent = reaper;\n\t\t\tBUG_ON((!t->ptrace) != (t->parent == father));\n\t\t\tif (likely(!t->ptrace))\n\t\t\t\tt->parent = t->real_parent;\n\t\t\tif (t->pdeath_signal)\n\t\t\t\tgroup_send_sig_info(t->pdeath_signal,\n\t\t\t\t\t\t    SEND_SIG_NOINFO, t);\n\t\t}\n\t\t/*\n\t\t * If this is a threaded reparent there is no need to\n\t\t * notify anyone anything has happened.\n\t\t */\n\t\tif (!same_thread_group(reaper, father))\n\t\t\treparent_leader(father, p, dead);\n\t}\n\tlist_splice_tail_init(&father->children, &reaper->children);\n}\n\n/*\n * Send signals to all our closest relatives so that they know\n * to properly mourn us..\n */\nstatic void exit_notify(struct task_struct *tsk, int group_dead)\n{\n\tbool autoreap;\n\tstruct task_struct *p, *n;\n\tLIST_HEAD(dead);\n\n\twrite_lock_irq(&tasklist_lock);\n\tforget_original_parent(tsk, &dead);\n\n\tif (group_dead)\n\t\tkill_orphaned_pgrp(tsk->group_leader, NULL);\n\n\tif (unlikely(tsk->ptrace)) {\n\t\tint sig = thread_group_leader(tsk) &&\n\t\t\t\tthread_group_empty(tsk) &&\n\t\t\t\t!ptrace_reparented(tsk) ?\n\t\t\ttsk->exit_signal : SIGCHLD;\n\t\tautoreap = do_notify_parent(tsk, sig);\n\t} else if (thread_group_leader(tsk)) {\n\t\tautoreap = thread_group_empty(tsk) &&\n\t\t\tdo_notify_parent(tsk, tsk->exit_signal);\n\t} else {\n\t\tautoreap = true;\n\t}\n\n\ttsk->exit_state = autoreap ? EXIT_DEAD : EXIT_ZOMBIE;\n\tif (tsk->exit_state == EXIT_DEAD)\n\t\tlist_add(&tsk->ptrace_entry, &dead);\n\n\t/* mt-exec, de_thread() is waiting for group leader */\n\tif (unlikely(tsk->signal->notify_count < 0))\n\t\twake_up_process(tsk->signal->group_exit_task);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tlist_for_each_entry_safe(p, n, &dead, ptrace_entry) {\n\t\tlist_del_init(&p->ptrace_entry);\n\t\trelease_task(p);\n\t}\n}\n\n#ifdef CONFIG_DEBUG_STACK_USAGE\nstatic void check_stack_usage(void)\n{\n\tstatic DEFINE_SPINLOCK(low_water_lock);\n\tstatic int lowest_to_date = THREAD_SIZE;\n\tunsigned long free;\n\n\tfree = stack_not_used(current);\n\n\tif (free >= lowest_to_date)\n\t\treturn;\n\n\tspin_lock(&low_water_lock);\n\tif (free < lowest_to_date) {\n\t\tpr_info(\"%s (%d) used greatest stack depth: %lu bytes left\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current), free);\n\t\tlowest_to_date = free;\n\t}\n\tspin_unlock(&low_water_lock);\n}\n#else\nstatic inline void check_stack_usage(void) {}\n#endif\n\nvoid __noreturn do_exit(long code)\n{\n\tstruct task_struct *tsk = current;\n\tint group_dead;\n\n\tprofile_task_exit(tsk);\n\tkcov_task_exit(tsk);\n\n\tWARN_ON(blk_needs_flush_plug(tsk));\n\n\tif (unlikely(in_interrupt()))\n\t\tpanic(\"Aiee, killing interrupt handler!\");\n\tif (unlikely(!tsk->pid))\n\t\tpanic(\"Attempted to kill the idle task!\");\n\n\t/*\n\t * If do_exit is called because this processes oopsed, it's possible\n\t * that get_fs() was left as KERNEL_DS, so reset it to USER_DS before\n\t * continuing. Amongst other possible reasons, this is to prevent\n\t * mm_release()->clear_child_tid() from writing to a user-controlled\n\t * kernel address.\n\t */\n\tset_fs(USER_DS);\n\n\tptrace_event(PTRACE_EVENT_EXIT, code);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\t/*\n\t * We're taking recursive faults here in do_exit. Safest is to just\n\t * leave this task alone and wait for reboot.\n\t */\n\tif (unlikely(tsk->flags & PF_EXITING)) {\n\t\tpr_alert(\"Fixing recursive fault but reboot is needed!\\n\");\n\t\t/*\n\t\t * We can do this unlocked here. The futex code uses\n\t\t * this flag just to verify whether the pi state\n\t\t * cleanup has been done or not. In the worst case it\n\t\t * loops once more. We pretend that the cleanup was\n\t\t * done as there is no way to return. Either the\n\t\t * OWNER_DIED bit is set by now or we push the blocked\n\t\t * task into the wait for ever nirwana as well.\n\t\t */\n\t\ttsk->flags |= PF_EXITPIDONE;\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\tschedule();\n\t}\n\n\texit_signals(tsk);  /* sets PF_EXITING */\n\t/*\n\t * Ensure that all new tsk->pi_lock acquisitions must observe\n\t * PF_EXITING. Serializes against futex.c:attach_to_pi_owner().\n\t */\n\tsmp_mb();\n\t/*\n\t * Ensure that we must observe the pi_state in exit_mm() ->\n\t * mm_release() -> exit_pi_state_list().\n\t */\n\traw_spin_lock_irq(&tsk->pi_lock);\n\traw_spin_unlock_irq(&tsk->pi_lock);\n\n\tif (unlikely(in_atomic())) {\n\t\tpr_info(\"note: %s[%d] exited with preempt_count %d\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\tpreempt_count());\n\t\tpreempt_count_set(PREEMPT_ENABLED);\n\t}\n\n\t/* sync mm's RSS info before statistics gathering */\n\tif (tsk->mm)\n\t\tsync_mm_rss(tsk->mm);\n\tacct_update_integrals(tsk);\n\tgroup_dead = atomic_dec_and_test(&tsk->signal->live);\n\tif (group_dead) {\n#ifdef CONFIG_POSIX_TIMERS\n\t\thrtimer_cancel(&tsk->signal->real_timer);\n\t\texit_itimers(tsk->signal);\n#endif\n\t\tif (tsk->mm)\n\t\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);\n\t}\n\tacct_collect(code, group_dead);\n\tif (group_dead)\n\t\ttty_audit_exit();\n\taudit_free(tsk);\n\n\ttsk->exit_code = code;\n\ttaskstats_exit(tsk, group_dead);\n\n\texit_mm();\n\n\tif (group_dead)\n\t\tacct_process();\n\ttrace_sched_process_exit(tsk);\n\n\texit_sem(tsk);\n\texit_shm(tsk);\n\texit_files(tsk);\n\texit_fs(tsk);\n\tif (group_dead)\n\t\tdisassociate_ctty(1);\n\texit_task_namespaces(tsk);\n\texit_task_work(tsk);\n\texit_thread(tsk);\n\n\t/*\n\t * Flush inherited counters to the parent - before the parent\n\t * gets woken up by child-exit notifications.\n\t *\n\t * because of cgroup mode, must be called before cgroup_exit()\n\t */\n\tperf_event_exit_task(tsk);\n\n\tsched_autogroup_exit_task(tsk);\n\tcgroup_exit(tsk);\n\n\t/*\n\t * FIXME: do that only when needed, using sched_exit tracepoint\n\t */\n\tflush_ptrace_hw_breakpoint(tsk);\n\n\texit_tasks_rcu_start();\n\texit_notify(tsk, group_dead);\n\tproc_exit_connector(tsk);\n\tmpol_put_task_policy(tsk);\n#ifdef CONFIG_FUTEX\n\tif (unlikely(current->pi_state_cache))\n\t\tkfree(current->pi_state_cache);\n#endif\n\t/*\n\t * Make sure we are holding no locks:\n\t */\n\tdebug_check_no_locks_held();\n\t/*\n\t * We can do this unlocked here. The futex code uses this flag\n\t * just to verify whether the pi state cleanup has been done\n\t * or not. In the worst case it loops once more.\n\t */\n\ttsk->flags |= PF_EXITPIDONE;\n\n\tif (tsk->io_context)\n\t\texit_io_context(tsk);\n\n\tif (tsk->splice_pipe)\n\t\tfree_pipe_info(tsk->splice_pipe);\n\n\tif (tsk->task_frag.page)\n\t\tput_page(tsk->task_frag.page);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\tcheck_stack_usage();\n\tpreempt_disable();\n\tif (tsk->nr_dirtied)\n\t\t__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);\n\texit_rcu();\n\texit_tasks_rcu_finish();\n\n\tlockdep_free_task(tsk);\n\tdo_task_dead();\n}\nEXPORT_SYMBOL_GPL(do_exit);\n\nvoid complete_and_exit(struct completion *comp, long code)\n{\n\tif (comp)\n\t\tcomplete(comp);\n\n\tdo_exit(code);\n}\nEXPORT_SYMBOL(complete_and_exit);\n\nSYSCALL_DEFINE1(exit, int, error_code)\n{\n\tdo_exit((error_code&0xff)<<8);\n}\n\n/*\n * Take down every thread in the group.  This is called by fatal signals\n * as well as by sys_exit_group (below).\n */\nvoid\ndo_group_exit(int exit_code)\n{\n\tstruct signal_struct *sig = current->signal;\n\n\tBUG_ON(exit_code & 0x80); /* core dumps don't get here */\n\n\tif (signal_group_exit(sig))\n\t\texit_code = sig->group_exit_code;\n\telse if (!thread_group_empty(current)) {\n\t\tstruct sighand_struct *const sighand = current->sighand;\n\n\t\tspin_lock_irq(&sighand->siglock);\n\t\tif (signal_group_exit(sig))\n\t\t\t/* Another thread got here before we took the lock.  */\n\t\t\texit_code = sig->group_exit_code;\n\t\telse {\n\t\t\tsig->group_exit_code = exit_code;\n\t\t\tsig->flags = SIGNAL_GROUP_EXIT;\n\t\t\tzap_other_threads(current);\n\t\t}\n\t\tspin_unlock_irq(&sighand->siglock);\n\t}\n\n\tdo_exit(exit_code);\n\t/* NOTREACHED */\n}\n\n/*\n * this kills every thread in the thread group. Note that any externally\n * wait4()-ing process will get the correct exit code - even if this\n * thread is not the thread group leader.\n */\nSYSCALL_DEFINE1(exit_group, int, error_code)\n{\n\tdo_group_exit((error_code & 0xff) << 8);\n\t/* NOTREACHED */\n\treturn 0;\n}\n\nstruct waitid_info {\n\tpid_t pid;\n\tuid_t uid;\n\tint status;\n\tint cause;\n};\n\nstruct wait_opts {\n\tenum pid_type\t\two_type;\n\tint\t\t\two_flags;\n\tstruct pid\t\t*wo_pid;\n\n\tstruct waitid_info\t*wo_info;\n\tint\t\t\two_stat;\n\tstruct rusage\t\t*wo_rusage;\n\n\twait_queue_entry_t\t\tchild_wait;\n\tint\t\t\tnotask_error;\n};\n\nstatic inline\nstruct pid *task_pid_type(struct task_struct *task, enum pid_type type)\n{\n\tif (type != PIDTYPE_PID)\n\t\ttask = task->group_leader;\n\treturn task->pids[type].pid;\n}\n\nstatic int eligible_pid(struct wait_opts *wo, struct task_struct *p)\n{\n\treturn\two->wo_type == PIDTYPE_MAX ||\n\t\ttask_pid_type(p, wo->wo_type) == wo->wo_pid;\n}\n\nstatic int\neligible_child(struct wait_opts *wo, bool ptrace, struct task_struct *p)\n{\n\tif (!eligible_pid(wo, p))\n\t\treturn 0;\n\n\t/*\n\t * Wait for all children (clone and not) if __WALL is set or\n\t * if it is traced by us.\n\t */\n\tif (ptrace || (wo->wo_flags & __WALL))\n\t\treturn 1;\n\n\t/*\n\t * Otherwise, wait for clone children *only* if __WCLONE is set;\n\t * otherwise, wait for non-clone children *only*.\n\t *\n\t * Note: a \"clone\" child here is one that reports to its parent\n\t * using a signal other than SIGCHLD, or a non-leader thread which\n\t * we can only see if it is traced by us.\n\t */\n\tif ((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * Handle sys_wait4 work for one task in state EXIT_ZOMBIE.  We hold\n * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold\n * the lock and this task is uninteresting.  If we return nonzero, we have\n * released the lock and the system call should return.\n */\nstatic int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)\n{\n\tint state, status;\n\tpid_t pid = task_pid_vnr(p);\n\tuid_t uid = from_kuid_munged(current_user_ns(), task_uid(p));\n\tstruct waitid_info *infop;\n\n\tif (!likely(wo->wo_flags & WEXITED))\n\t\treturn 0;\n\n\tif (unlikely(wo->wo_flags & WNOWAIT)) {\n\t\tstatus = p->exit_code;\n\t\tget_task_struct(p);\n\t\tread_unlock(&tasklist_lock);\n\t\tsched_annotate_sleep();\n\t\tif (wo->wo_rusage)\n\t\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\t\tput_task_struct(p);\n\t\tgoto out_info;\n\t}\n\t/*\n\t * Move the task's state to DEAD/TRACE, only one thread can do this.\n\t */\n\tstate = (ptrace_reparented(p) && thread_group_leader(p)) ?\n\t\tEXIT_TRACE : EXIT_DEAD;\n\tif (cmpxchg(&p->exit_state, EXIT_ZOMBIE, state) != EXIT_ZOMBIE)\n\t\treturn 0;\n\t/*\n\t * We own this thread, nobody else can reap it.\n\t */\n\tread_unlock(&tasklist_lock);\n\tsched_annotate_sleep();\n\n\t/*\n\t * Check thread_group_leader() to exclude the traced sub-threads.\n\t */\n\tif (state == EXIT_DEAD && thread_group_leader(p)) {\n\t\tstruct signal_struct *sig = p->signal;\n\t\tstruct signal_struct *psig = current->signal;\n\t\tunsigned long maxrss;\n\t\tu64 tgutime, tgstime;\n\n\t\t/*\n\t\t * The resource counters for the group leader are in its\n\t\t * own task_struct.  Those for dead threads in the group\n\t\t * are in its signal_struct, as are those for the child\n\t\t * processes it has previously reaped.  All these\n\t\t * accumulate in the parent's signal_struct c* fields.\n\t\t *\n\t\t * We don't bother to take a lock here to protect these\n\t\t * p->signal fields because the whole thread group is dead\n\t\t * and nobody can change them.\n\t\t *\n\t\t * psig->stats_lock also protects us from our sub-theads\n\t\t * which can reap other children at the same time. Until\n\t\t * we change k_getrusage()-like users to rely on this lock\n\t\t * we have to take ->siglock as well.\n\t\t *\n\t\t * We use thread_group_cputime_adjusted() to get times for\n\t\t * the thread group, which consolidates times for all threads\n\t\t * in the group including the group leader.\n\t\t */\n\t\tthread_group_cputime_adjusted(p, &tgutime, &tgstime);\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\twrite_seqlock(&psig->stats_lock);\n\t\tpsig->cutime += tgutime + sig->cutime;\n\t\tpsig->cstime += tgstime + sig->cstime;\n\t\tpsig->cgtime += task_gtime(p) + sig->gtime + sig->cgtime;\n\t\tpsig->cmin_flt +=\n\t\t\tp->min_flt + sig->min_flt + sig->cmin_flt;\n\t\tpsig->cmaj_flt +=\n\t\t\tp->maj_flt + sig->maj_flt + sig->cmaj_flt;\n\t\tpsig->cnvcsw +=\n\t\t\tp->nvcsw + sig->nvcsw + sig->cnvcsw;\n\t\tpsig->cnivcsw +=\n\t\t\tp->nivcsw + sig->nivcsw + sig->cnivcsw;\n\t\tpsig->cinblock +=\n\t\t\ttask_io_get_inblock(p) +\n\t\t\tsig->inblock + sig->cinblock;\n\t\tpsig->coublock +=\n\t\t\ttask_io_get_oublock(p) +\n\t\t\tsig->oublock + sig->coublock;\n\t\tmaxrss = max(sig->maxrss, sig->cmaxrss);\n\t\tif (psig->cmaxrss < maxrss)\n\t\t\tpsig->cmaxrss = maxrss;\n\t\ttask_io_accounting_add(&psig->ioac, &p->ioac);\n\t\ttask_io_accounting_add(&psig->ioac, &sig->ioac);\n\t\twrite_sequnlock(&psig->stats_lock);\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t}\n\n\tif (wo->wo_rusage)\n\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\tstatus = (p->signal->flags & SIGNAL_GROUP_EXIT)\n\t\t? p->signal->group_exit_code : p->exit_code;\n\two->wo_stat = status;\n\n\tif (state == EXIT_TRACE) {\n\t\twrite_lock_irq(&tasklist_lock);\n\t\t/* We dropped tasklist, ptracer could die and untrace */\n\t\tptrace_unlink(p);\n\n\t\t/* If parent wants a zombie, don't release it now */\n\t\tstate = EXIT_ZOMBIE;\n\t\tif (do_notify_parent(p, p->exit_signal))\n\t\t\tstate = EXIT_DEAD;\n\t\tp->exit_state = state;\n\t\twrite_unlock_irq(&tasklist_lock);\n\t}\n\tif (state == EXIT_DEAD)\n\t\trelease_task(p);\n\nout_info:\n\tinfop = wo->wo_info;\n\tif (infop) {\n\t\tif ((status & 0x7f) == 0) {\n\t\t\tinfop->cause = CLD_EXITED;\n\t\t\tinfop->status = status >> 8;\n\t\t} else {\n\t\t\tinfop->cause = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;\n\t\t\tinfop->status = status & 0x7f;\n\t\t}\n\t\tinfop->pid = pid;\n\t\tinfop->uid = uid;\n\t}\n\n\treturn pid;\n}\n\nstatic int *task_stopped_code(struct task_struct *p, bool ptrace)\n{\n\tif (ptrace) {\n\t\tif (task_is_traced(p) && !(p->jobctl & JOBCTL_LISTENING))\n\t\t\treturn &p->exit_code;\n\t} else {\n\t\tif (p->signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\treturn &p->signal->group_exit_code;\n\t}\n\treturn NULL;\n}\n\n/**\n * wait_task_stopped - Wait for %TASK_STOPPED or %TASK_TRACED\n * @wo: wait options\n * @ptrace: is the wait for ptrace\n * @p: task to wait for\n *\n * Handle sys_wait4() work for %p in state %TASK_STOPPED or %TASK_TRACED.\n *\n * CONTEXT:\n * read_lock(&tasklist_lock), which is released if return value is\n * non-zero.  Also, grabs and releases @p->sighand->siglock.\n *\n * RETURNS:\n * 0 if wait condition didn't exist and search for other wait conditions\n * should continue.  Non-zero return, -errno on failure and @p's pid on\n * success, implies that tasklist_lock is released and wait condition\n * search should terminate.\n */\nstatic int wait_task_stopped(struct wait_opts *wo,\n\t\t\t\tint ptrace, struct task_struct *p)\n{\n\tstruct waitid_info *infop;\n\tint exit_code, *p_code, why;\n\tuid_t uid = 0; /* unneeded, required by compiler */\n\tpid_t pid;\n\n\t/*\n\t * Traditionally we see ptrace'd stopped tasks regardless of options.\n\t */\n\tif (!ptrace && !(wo->wo_flags & WUNTRACED))\n\t\treturn 0;\n\n\tif (!task_stopped_code(p, ptrace))\n\t\treturn 0;\n\n\texit_code = 0;\n\tspin_lock_irq(&p->sighand->siglock);\n\n\tp_code = task_stopped_code(p, ptrace);\n\tif (unlikely(!p_code))\n\t\tgoto unlock_sig;\n\n\texit_code = *p_code;\n\tif (!exit_code)\n\t\tgoto unlock_sig;\n\n\tif (!unlikely(wo->wo_flags & WNOWAIT))\n\t\t*p_code = 0;\n\n\tuid = from_kuid_munged(current_user_ns(), task_uid(p));\nunlock_sig:\n\tspin_unlock_irq(&p->sighand->siglock);\n\tif (!exit_code)\n\t\treturn 0;\n\n\t/*\n\t * Now we are pretty sure this task is interesting.\n\t * Make sure it doesn't get reaped out from under us while we\n\t * give up the lock and then examine it below.  We don't want to\n\t * keep holding onto the tasklist_lock while we call getrusage and\n\t * possibly take page faults for user memory.\n\t */\n\tget_task_struct(p);\n\tpid = task_pid_vnr(p);\n\twhy = ptrace ? CLD_TRAPPED : CLD_STOPPED;\n\tread_unlock(&tasklist_lock);\n\tsched_annotate_sleep();\n\tif (wo->wo_rusage)\n\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\tput_task_struct(p);\n\n\tif (likely(!(wo->wo_flags & WNOWAIT)))\n\t\two->wo_stat = (exit_code << 8) | 0x7f;\n\n\tinfop = wo->wo_info;\n\tif (infop) {\n\t\tinfop->cause = why;\n\t\tinfop->status = exit_code;\n\t\tinfop->pid = pid;\n\t\tinfop->uid = uid;\n\t}\n\treturn pid;\n}\n\n/*\n * Handle do_wait work for one task in a live, non-stopped state.\n * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold\n * the lock and this task is uninteresting.  If we return nonzero, we have\n * released the lock and the system call should return.\n */\nstatic int wait_task_continued(struct wait_opts *wo, struct task_struct *p)\n{\n\tstruct waitid_info *infop;\n\tpid_t pid;\n\tuid_t uid;\n\n\tif (!unlikely(wo->wo_flags & WCONTINUED))\n\t\treturn 0;\n\n\tif (!(p->signal->flags & SIGNAL_STOP_CONTINUED))\n\t\treturn 0;\n\n\tspin_lock_irq(&p->sighand->siglock);\n\t/* Re-check with the lock held.  */\n\tif (!(p->signal->flags & SIGNAL_STOP_CONTINUED)) {\n\t\tspin_unlock_irq(&p->sighand->siglock);\n\t\treturn 0;\n\t}\n\tif (!unlikely(wo->wo_flags & WNOWAIT))\n\t\tp->signal->flags &= ~SIGNAL_STOP_CONTINUED;\n\tuid = from_kuid_munged(current_user_ns(), task_uid(p));\n\tspin_unlock_irq(&p->sighand->siglock);\n\n\tpid = task_pid_vnr(p);\n\tget_task_struct(p);\n\tread_unlock(&tasklist_lock);\n\tsched_annotate_sleep();\n\tif (wo->wo_rusage)\n\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\tput_task_struct(p);\n\n\tinfop = wo->wo_info;\n\tif (!infop) {\n\t\two->wo_stat = 0xffff;\n\t} else {\n\t\tinfop->cause = CLD_CONTINUED;\n\t\tinfop->pid = pid;\n\t\tinfop->uid = uid;\n\t\tinfop->status = SIGCONT;\n\t}\n\treturn pid;\n}\n\n/*\n * Consider @p for a wait by @parent.\n *\n * -ECHILD should be in ->notask_error before the first call.\n * Returns nonzero for a final return, when we have unlocked tasklist_lock.\n * Returns zero if the search for a child should continue;\n * then ->notask_error is 0 if @p is an eligible child,\n * or still -ECHILD.\n */\nstatic int wait_consider_task(struct wait_opts *wo, int ptrace,\n\t\t\t\tstruct task_struct *p)\n{\n\t/*\n\t * We can race with wait_task_zombie() from another thread.\n\t * Ensure that EXIT_ZOMBIE -> EXIT_DEAD/EXIT_TRACE transition\n\t * can't confuse the checks below.\n\t */\n\tint exit_state = ACCESS_ONCE(p->exit_state);\n\tint ret;\n\n\tif (unlikely(exit_state == EXIT_DEAD))\n\t\treturn 0;\n\n\tret = eligible_child(wo, ptrace, p);\n\tif (!ret)\n\t\treturn ret;\n\n\tif (unlikely(exit_state == EXIT_TRACE)) {\n\t\t/*\n\t\t * ptrace == 0 means we are the natural parent. In this case\n\t\t * we should clear notask_error, debugger will notify us.\n\t\t */\n\t\tif (likely(!ptrace))\n\t\t\two->notask_error = 0;\n\t\treturn 0;\n\t}\n\n\tif (likely(!ptrace) && unlikely(p->ptrace)) {\n\t\t/*\n\t\t * If it is traced by its real parent's group, just pretend\n\t\t * the caller is ptrace_do_wait() and reap this child if it\n\t\t * is zombie.\n\t\t *\n\t\t * This also hides group stop state from real parent; otherwise\n\t\t * a single stop can be reported twice as group and ptrace stop.\n\t\t * If a ptracer wants to distinguish these two events for its\n\t\t * own children it should create a separate process which takes\n\t\t * the role of real parent.\n\t\t */\n\t\tif (!ptrace_reparented(p))\n\t\t\tptrace = 1;\n\t}\n\n\t/* slay zombie? */\n\tif (exit_state == EXIT_ZOMBIE) {\n\t\t/* we don't reap group leaders with subthreads */\n\t\tif (!delay_group_leader(p)) {\n\t\t\t/*\n\t\t\t * A zombie ptracee is only visible to its ptracer.\n\t\t\t * Notification and reaping will be cascaded to the\n\t\t\t * real parent when the ptracer detaches.\n\t\t\t */\n\t\t\tif (unlikely(ptrace) || likely(!p->ptrace))\n\t\t\t\treturn wait_task_zombie(wo, p);\n\t\t}\n\n\t\t/*\n\t\t * Allow access to stopped/continued state via zombie by\n\t\t * falling through.  Clearing of notask_error is complex.\n\t\t *\n\t\t * When !@ptrace:\n\t\t *\n\t\t * If WEXITED is set, notask_error should naturally be\n\t\t * cleared.  If not, subset of WSTOPPED|WCONTINUED is set,\n\t\t * so, if there are live subthreads, there are events to\n\t\t * wait for.  If all subthreads are dead, it's still safe\n\t\t * to clear - this function will be called again in finite\n\t\t * amount time once all the subthreads are released and\n\t\t * will then return without clearing.\n\t\t *\n\t\t * When @ptrace:\n\t\t *\n\t\t * Stopped state is per-task and thus can't change once the\n\t\t * target task dies.  Only continued and exited can happen.\n\t\t * Clear notask_error if WCONTINUED | WEXITED.\n\t\t */\n\t\tif (likely(!ptrace) || (wo->wo_flags & (WCONTINUED | WEXITED)))\n\t\t\two->notask_error = 0;\n\t} else {\n\t\t/*\n\t\t * @p is alive and it's gonna stop, continue or exit, so\n\t\t * there always is something to wait for.\n\t\t */\n\t\two->notask_error = 0;\n\t}\n\n\t/*\n\t * Wait for stopped.  Depending on @ptrace, different stopped state\n\t * is used and the two don't interact with each other.\n\t */\n\tret = wait_task_stopped(wo, ptrace, p);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Wait for continued.  There's only one continued state and the\n\t * ptracer can consume it which can confuse the real parent.  Don't\n\t * use WCONTINUED from ptracer.  You don't need or want it.\n\t */\n\treturn wait_task_continued(wo, p);\n}\n\n/*\n * Do the work of do_wait() for one thread in the group, @tsk.\n *\n * -ECHILD should be in ->notask_error before the first call.\n * Returns nonzero for a final return, when we have unlocked tasklist_lock.\n * Returns zero if the search for a child should continue; then\n * ->notask_error is 0 if there were any eligible children,\n * or still -ECHILD.\n */\nstatic int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)\n{\n\tstruct task_struct *p;\n\n\tlist_for_each_entry(p, &tsk->children, sibling) {\n\t\tint ret = wait_consider_task(wo, 0, p);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)\n{\n\tstruct task_struct *p;\n\n\tlist_for_each_entry(p, &tsk->ptraced, ptrace_entry) {\n\t\tint ret = wait_consider_task(wo, 1, p);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int child_wait_callback(wait_queue_entry_t *wait, unsigned mode,\n\t\t\t\tint sync, void *key)\n{\n\tstruct wait_opts *wo = container_of(wait, struct wait_opts,\n\t\t\t\t\t\tchild_wait);\n\tstruct task_struct *p = key;\n\n\tif (!eligible_pid(wo, p))\n\t\treturn 0;\n\n\tif ((wo->wo_flags & __WNOTHREAD) && wait->private != p->parent)\n\t\treturn 0;\n\n\treturn default_wake_function(wait, mode, sync, key);\n}\n\nvoid __wake_up_parent(struct task_struct *p, struct task_struct *parent)\n{\n\t__wake_up_sync_key(&parent->signal->wait_chldexit,\n\t\t\t\tTASK_INTERRUPTIBLE, 1, p);\n}\n\nstatic long do_wait(struct wait_opts *wo)\n{\n\tstruct task_struct *tsk;\n\tint retval;\n\n\ttrace_sched_process_wait(wo->wo_pid);\n\n\tinit_waitqueue_func_entry(&wo->child_wait, child_wait_callback);\n\two->child_wait.private = current;\n\tadd_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);\nrepeat:\n\t/*\n\t * If there is nothing that can match our criteria, just get out.\n\t * We will clear ->notask_error to zero if we see any child that\n\t * might later match our criteria, even if we are not able to reap\n\t * it yet.\n\t */\n\two->notask_error = -ECHILD;\n\tif ((wo->wo_type < PIDTYPE_MAX) &&\n\t   (!wo->wo_pid || hlist_empty(&wo->wo_pid->tasks[wo->wo_type])))\n\t\tgoto notask;\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tread_lock(&tasklist_lock);\n\ttsk = current;\n\tdo {\n\t\tretval = do_wait_thread(wo, tsk);\n\t\tif (retval)\n\t\t\tgoto end;\n\n\t\tretval = ptrace_do_wait(wo, tsk);\n\t\tif (retval)\n\t\t\tgoto end;\n\n\t\tif (wo->wo_flags & __WNOTHREAD)\n\t\t\tbreak;\n\t} while_each_thread(current, tsk);\n\tread_unlock(&tasklist_lock);\n\nnotask:\n\tretval = wo->notask_error;\n\tif (!retval && !(wo->wo_flags & WNOHANG)) {\n\t\tretval = -ERESTARTSYS;\n\t\tif (!signal_pending(current)) {\n\t\t\tschedule();\n\t\t\tgoto repeat;\n\t\t}\n\t}\nend:\n\t__set_current_state(TASK_RUNNING);\n\tremove_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);\n\treturn retval;\n}\n\nstatic long kernel_waitid(int which, pid_t upid, struct waitid_info *infop,\n\t\t\t  int options, struct rusage *ru)\n{\n\tstruct wait_opts wo;\n\tstruct pid *pid = NULL;\n\tenum pid_type type;\n\tlong ret;\n\n\tif (options & ~(WNOHANG|WNOWAIT|WEXITED|WSTOPPED|WCONTINUED|\n\t\t\t__WNOTHREAD|__WCLONE|__WALL))\n\t\treturn -EINVAL;\n\tif (!(options & (WEXITED|WSTOPPED|WCONTINUED)))\n\t\treturn -EINVAL;\n\n\tswitch (which) {\n\tcase P_ALL:\n\t\ttype = PIDTYPE_MAX;\n\t\tbreak;\n\tcase P_PID:\n\t\ttype = PIDTYPE_PID;\n\t\tif (upid <= 0)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase P_PGID:\n\t\ttype = PIDTYPE_PGID;\n\t\tif (upid <= 0)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (type < PIDTYPE_MAX)\n\t\tpid = find_get_pid(upid);\n\n\two.wo_type\t= type;\n\two.wo_pid\t= pid;\n\two.wo_flags\t= options;\n\two.wo_info\t= infop;\n\two.wo_rusage\t= ru;\n\tret = do_wait(&wo);\n\n\tput_pid(pid);\n\treturn ret;\n}\n\nSYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,\n\t\tinfop, int, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);\n\tint signo = 0;\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t}\n\n\tif (!err) {\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\tif (!infop)\n\t\treturn err;\n\n\tuser_access_begin();\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}\n\nlong kernel_wait4(pid_t upid, int __user *stat_addr, int options,\n\t\t  struct rusage *ru)\n{\n\tstruct wait_opts wo;\n\tstruct pid *pid = NULL;\n\tenum pid_type type;\n\tlong ret;\n\n\tif (options & ~(WNOHANG|WUNTRACED|WCONTINUED|\n\t\t\t__WNOTHREAD|__WCLONE|__WALL))\n\t\treturn -EINVAL;\n\n\t/* -INT_MIN is not defined */\n\tif (upid == INT_MIN)\n\t\treturn -ESRCH;\n\n\tif (upid == -1)\n\t\ttype = PIDTYPE_MAX;\n\telse if (upid < 0) {\n\t\ttype = PIDTYPE_PGID;\n\t\tpid = find_get_pid(-upid);\n\t} else if (upid == 0) {\n\t\ttype = PIDTYPE_PGID;\n\t\tpid = get_task_pid(current, PIDTYPE_PGID);\n\t} else /* upid > 0 */ {\n\t\ttype = PIDTYPE_PID;\n\t\tpid = find_get_pid(upid);\n\t}\n\n\two.wo_type\t= type;\n\two.wo_pid\t= pid;\n\two.wo_flags\t= options | WEXITED;\n\two.wo_info\t= NULL;\n\two.wo_stat\t= 0;\n\two.wo_rusage\t= ru;\n\tret = do_wait(&wo);\n\tput_pid(pid);\n\tif (ret > 0 && stat_addr && put_user(wo.wo_stat, stat_addr))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nSYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,\n\t\tint, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tlong err = kernel_wait4(upid, stat_addr, options, ru ? &r : NULL);\n\n\tif (err > 0) {\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\treturn err;\n}\n\n#ifdef __ARCH_WANT_SYS_WAITPID\n\n/*\n * sys_waitpid() remains for compatibility. waitpid() should be\n * implemented by calling sys_wait4() from libc.a.\n */\nSYSCALL_DEFINE3(waitpid, pid_t, pid, int __user *, stat_addr, int, options)\n{\n\treturn sys_wait4(pid, stat_addr, options, NULL);\n}\n\n#endif\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(wait4,\n\tcompat_pid_t, pid,\n\tcompat_uint_t __user *, stat_addr,\n\tint, options,\n\tstruct compat_rusage __user *, ru)\n{\n\tstruct rusage r;\n\tlong err = kernel_wait4(pid, stat_addr, options, ru ? &r : NULL);\n\tif (err > 0) {\n\t\tif (ru && put_compat_rusage(&r, ru))\n\t\t\treturn -EFAULT;\n\t}\n\treturn err;\n}\n\nCOMPAT_SYSCALL_DEFINE5(waitid,\n\t\tint, which, compat_pid_t, pid,\n\t\tstruct compat_siginfo __user *, infop, int, options,\n\t\tstruct compat_rusage __user *, uru)\n{\n\tstruct rusage ru;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, pid, &info, options, uru ? &ru : NULL);\n\tint signo = 0;\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t}\n\n\tif (!err && uru) {\n\t\t/* kernel_waitid() overwrites everything in ru */\n\t\tif (COMPAT_USE_64BIT_TIME)\n\t\t\terr = copy_to_user(uru, &ru, sizeof(ru));\n\t\telse\n\t\t\terr = put_compat_rusage(&ru, uru);\n\t\tif (err)\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (!infop)\n\t\treturn err;\n\n\tuser_access_begin();\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}\n#endif\n"], "fixing_code": ["/*\n *  linux/kernel/exit.c\n *\n *  Copyright (C) 1991, 1992  Linus Torvalds\n */\n\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/sched/autogroup.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/stat.h>\n#include <linux/sched/task.h>\n#include <linux/sched/task_stack.h>\n#include <linux/sched/cputime.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/capability.h>\n#include <linux/completion.h>\n#include <linux/personality.h>\n#include <linux/tty.h>\n#include <linux/iocontext.h>\n#include <linux/key.h>\n#include <linux/cpu.h>\n#include <linux/acct.h>\n#include <linux/tsacct_kern.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/freezer.h>\n#include <linux/binfmts.h>\n#include <linux/nsproxy.h>\n#include <linux/pid_namespace.h>\n#include <linux/ptrace.h>\n#include <linux/profile.h>\n#include <linux/mount.h>\n#include <linux/proc_fs.h>\n#include <linux/kthread.h>\n#include <linux/mempolicy.h>\n#include <linux/taskstats_kern.h>\n#include <linux/delayacct.h>\n#include <linux/cgroup.h>\n#include <linux/syscalls.h>\n#include <linux/signal.h>\n#include <linux/posix-timers.h>\n#include <linux/cn_proc.h>\n#include <linux/mutex.h>\n#include <linux/futex.h>\n#include <linux/pipe_fs_i.h>\n#include <linux/audit.h> /* for audit_free() */\n#include <linux/resource.h>\n#include <linux/blkdev.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/tracehook.h>\n#include <linux/fs_struct.h>\n#include <linux/init_task.h>\n#include <linux/perf_event.h>\n#include <trace/events/sched.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/oom.h>\n#include <linux/writeback.h>\n#include <linux/shm.h>\n#include <linux/kcov.h>\n#include <linux/random.h>\n#include <linux/rcuwait.h>\n#include <linux/compat.h>\n\n#include <linux/uaccess.h>\n#include <asm/unistd.h>\n#include <asm/pgtable.h>\n#include <asm/mmu_context.h>\n\nstatic void __unhash_process(struct task_struct *p, bool group_dead)\n{\n\tnr_threads--;\n\tdetach_pid(p, PIDTYPE_PID);\n\tif (group_dead) {\n\t\tdetach_pid(p, PIDTYPE_PGID);\n\t\tdetach_pid(p, PIDTYPE_SID);\n\n\t\tlist_del_rcu(&p->tasks);\n\t\tlist_del_init(&p->sibling);\n\t\t__this_cpu_dec(process_counts);\n\t}\n\tlist_del_rcu(&p->thread_group);\n\tlist_del_rcu(&p->thread_node);\n}\n\n/*\n * This function expects the tasklist_lock write-locked.\n */\nstatic void __exit_signal(struct task_struct *tsk)\n{\n\tstruct signal_struct *sig = tsk->signal;\n\tbool group_dead = thread_group_leader(tsk);\n\tstruct sighand_struct *sighand;\n\tstruct tty_struct *uninitialized_var(tty);\n\tu64 utime, stime;\n\n\tsighand = rcu_dereference_check(tsk->sighand,\n\t\t\t\t\tlockdep_tasklist_lock_is_held());\n\tspin_lock(&sighand->siglock);\n\n#ifdef CONFIG_POSIX_TIMERS\n\tposix_cpu_timers_exit(tsk);\n\tif (group_dead) {\n\t\tposix_cpu_timers_exit_group(tsk);\n\t} else {\n\t\t/*\n\t\t * This can only happen if the caller is de_thread().\n\t\t * FIXME: this is the temporary hack, we should teach\n\t\t * posix-cpu-timers to handle this case correctly.\n\t\t */\n\t\tif (unlikely(has_group_leader_pid(tsk)))\n\t\t\tposix_cpu_timers_exit_group(tsk);\n\t}\n#endif\n\n\tif (group_dead) {\n\t\ttty = sig->tty;\n\t\tsig->tty = NULL;\n\t} else {\n\t\t/*\n\t\t * If there is any task waiting for the group exit\n\t\t * then notify it:\n\t\t */\n\t\tif (sig->notify_count > 0 && !--sig->notify_count)\n\t\t\twake_up_process(sig->group_exit_task);\n\n\t\tif (tsk == sig->curr_target)\n\t\t\tsig->curr_target = next_thread(tsk);\n\t}\n\n\tadd_device_randomness((const void*) &tsk->se.sum_exec_runtime,\n\t\t\t      sizeof(unsigned long long));\n\n\t/*\n\t * Accumulate here the counters for all threads as they die. We could\n\t * skip the group leader because it is the last user of signal_struct,\n\t * but we want to avoid the race with thread_group_cputime() which can\n\t * see the empty ->thread_head list.\n\t */\n\ttask_cputime(tsk, &utime, &stime);\n\twrite_seqlock(&sig->stats_lock);\n\tsig->utime += utime;\n\tsig->stime += stime;\n\tsig->gtime += task_gtime(tsk);\n\tsig->min_flt += tsk->min_flt;\n\tsig->maj_flt += tsk->maj_flt;\n\tsig->nvcsw += tsk->nvcsw;\n\tsig->nivcsw += tsk->nivcsw;\n\tsig->inblock += task_io_get_inblock(tsk);\n\tsig->oublock += task_io_get_oublock(tsk);\n\ttask_io_accounting_add(&sig->ioac, &tsk->ioac);\n\tsig->sum_sched_runtime += tsk->se.sum_exec_runtime;\n\tsig->nr_threads--;\n\t__unhash_process(tsk, group_dead);\n\twrite_sequnlock(&sig->stats_lock);\n\n\t/*\n\t * Do this under ->siglock, we can race with another thread\n\t * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.\n\t */\n\tflush_sigqueue(&tsk->pending);\n\ttsk->sighand = NULL;\n\tspin_unlock(&sighand->siglock);\n\n\t__cleanup_sighand(sighand);\n\tclear_tsk_thread_flag(tsk, TIF_SIGPENDING);\n\tif (group_dead) {\n\t\tflush_sigqueue(&sig->shared_pending);\n\t\ttty_kref_put(tty);\n\t}\n}\n\nstatic void delayed_put_task_struct(struct rcu_head *rhp)\n{\n\tstruct task_struct *tsk = container_of(rhp, struct task_struct, rcu);\n\n\tperf_event_delayed_put(tsk);\n\ttrace_sched_process_free(tsk);\n\tput_task_struct(tsk);\n}\n\n\nvoid release_task(struct task_struct *p)\n{\n\tstruct task_struct *leader;\n\tint zap_leader;\nrepeat:\n\t/* don't need to get the RCU readlock here - the process is dead and\n\t * can't be modifying its own credentials. But shut RCU-lockdep up */\n\trcu_read_lock();\n\tatomic_dec(&__task_cred(p)->user->processes);\n\trcu_read_unlock();\n\n\tproc_flush_task(p);\n\n\twrite_lock_irq(&tasklist_lock);\n\tptrace_release_task(p);\n\t__exit_signal(p);\n\n\t/*\n\t * If we are the last non-leader member of the thread\n\t * group, and the leader is zombie, then notify the\n\t * group leader's parent process. (if it wants notification.)\n\t */\n\tzap_leader = 0;\n\tleader = p->group_leader;\n\tif (leader != p && thread_group_empty(leader)\n\t\t\t&& leader->exit_state == EXIT_ZOMBIE) {\n\t\t/*\n\t\t * If we were the last child thread and the leader has\n\t\t * exited already, and the leader's parent ignores SIGCHLD,\n\t\t * then we are the one who should release the leader.\n\t\t */\n\t\tzap_leader = do_notify_parent(leader, leader->exit_signal);\n\t\tif (zap_leader)\n\t\t\tleader->exit_state = EXIT_DEAD;\n\t}\n\n\twrite_unlock_irq(&tasklist_lock);\n\trelease_thread(p);\n\tcall_rcu(&p->rcu, delayed_put_task_struct);\n\n\tp = leader;\n\tif (unlikely(zap_leader))\n\t\tgoto repeat;\n}\n\n/*\n * Note that if this function returns a valid task_struct pointer (!NULL)\n * task->usage must remain >0 for the duration of the RCU critical section.\n */\nstruct task_struct *task_rcu_dereference(struct task_struct **ptask)\n{\n\tstruct sighand_struct *sighand;\n\tstruct task_struct *task;\n\n\t/*\n\t * We need to verify that release_task() was not called and thus\n\t * delayed_put_task_struct() can't run and drop the last reference\n\t * before rcu_read_unlock(). We check task->sighand != NULL,\n\t * but we can read the already freed and reused memory.\n\t */\nretry:\n\ttask = rcu_dereference(*ptask);\n\tif (!task)\n\t\treturn NULL;\n\n\tprobe_kernel_address(&task->sighand, sighand);\n\n\t/*\n\t * Pairs with atomic_dec_and_test() in put_task_struct(). If this task\n\t * was already freed we can not miss the preceding update of this\n\t * pointer.\n\t */\n\tsmp_rmb();\n\tif (unlikely(task != READ_ONCE(*ptask)))\n\t\tgoto retry;\n\n\t/*\n\t * We've re-checked that \"task == *ptask\", now we have two different\n\t * cases:\n\t *\n\t * 1. This is actually the same task/task_struct. In this case\n\t *    sighand != NULL tells us it is still alive.\n\t *\n\t * 2. This is another task which got the same memory for task_struct.\n\t *    We can't know this of course, and we can not trust\n\t *    sighand != NULL.\n\t *\n\t *    In this case we actually return a random value, but this is\n\t *    correct.\n\t *\n\t *    If we return NULL - we can pretend that we actually noticed that\n\t *    *ptask was updated when the previous task has exited. Or pretend\n\t *    that probe_slab_address(&sighand) reads NULL.\n\t *\n\t *    If we return the new task (because sighand is not NULL for any\n\t *    reason) - this is fine too. This (new) task can't go away before\n\t *    another gp pass.\n\t *\n\t *    And note: We could even eliminate the false positive if re-read\n\t *    task->sighand once again to avoid the falsely NULL. But this case\n\t *    is very unlikely so we don't care.\n\t */\n\tif (!sighand)\n\t\treturn NULL;\n\n\treturn task;\n}\n\nvoid rcuwait_wake_up(struct rcuwait *w)\n{\n\tstruct task_struct *task;\n\n\trcu_read_lock();\n\n\t/*\n\t * Order condition vs @task, such that everything prior to the load\n\t * of @task is visible. This is the condition as to why the user called\n\t * rcuwait_trywake() in the first place. Pairs with set_current_state()\n\t * barrier (A) in rcuwait_wait_event().\n\t *\n\t *    WAIT                WAKE\n\t *    [S] tsk = current\t  [S] cond = true\n\t *        MB (A)\t      MB (B)\n\t *    [L] cond\t\t  [L] tsk\n\t */\n\tsmp_rmb(); /* (B) */\n\n\t/*\n\t * Avoid using task_rcu_dereference() magic as long as we are careful,\n\t * see comment in rcuwait_wait_event() regarding ->exit_state.\n\t */\n\ttask = rcu_dereference(w->task);\n\tif (task)\n\t\twake_up_process(task);\n\trcu_read_unlock();\n}\n\n/*\n * Determine if a process group is \"orphaned\", according to the POSIX\n * definition in 2.2.2.52.  Orphaned process groups are not to be affected\n * by terminal-generated stop signals.  Newly orphaned process groups are\n * to receive a SIGHUP and a SIGCONT.\n *\n * \"I ask you, have you ever known what it is to be an orphan?\"\n */\nstatic int will_become_orphaned_pgrp(struct pid *pgrp,\n\t\t\t\t\tstruct task_struct *ignored_task)\n{\n\tstruct task_struct *p;\n\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tif ((p == ignored_task) ||\n\t\t    (p->exit_state && thread_group_empty(p)) ||\n\t\t    is_global_init(p->real_parent))\n\t\t\tcontinue;\n\n\t\tif (task_pgrp(p->real_parent) != pgrp &&\n\t\t    task_session(p->real_parent) == task_session(p))\n\t\t\treturn 0;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\n\treturn 1;\n}\n\nint is_current_pgrp_orphaned(void)\n{\n\tint retval;\n\n\tread_lock(&tasklist_lock);\n\tretval = will_become_orphaned_pgrp(task_pgrp(current), NULL);\n\tread_unlock(&tasklist_lock);\n\n\treturn retval;\n}\n\nstatic bool has_stopped_jobs(struct pid *pgrp)\n{\n\tstruct task_struct *p;\n\n\tdo_each_pid_task(pgrp, PIDTYPE_PGID, p) {\n\t\tif (p->signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\treturn true;\n\t} while_each_pid_task(pgrp, PIDTYPE_PGID, p);\n\n\treturn false;\n}\n\n/*\n * Check to see if any process groups have become orphaned as\n * a result of our exiting, and if they have any stopped jobs,\n * send them a SIGHUP and then a SIGCONT. (POSIX 3.2.2.2)\n */\nstatic void\nkill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)\n{\n\tstruct pid *pgrp = task_pgrp(tsk);\n\tstruct task_struct *ignored_task = tsk;\n\n\tif (!parent)\n\t\t/* exit: our father is in a different pgrp than\n\t\t * we are and we were the only connection outside.\n\t\t */\n\t\tparent = tsk->real_parent;\n\telse\n\t\t/* reparent: our child is in a different pgrp than\n\t\t * we are, and it was the only connection outside.\n\t\t */\n\t\tignored_task = NULL;\n\n\tif (task_pgrp(parent) != pgrp &&\n\t    task_session(parent) == task_session(tsk) &&\n\t    will_become_orphaned_pgrp(pgrp, ignored_task) &&\n\t    has_stopped_jobs(pgrp)) {\n\t\t__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);\n\t\t__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);\n\t}\n}\n\n#ifdef CONFIG_MEMCG\n/*\n * A task is exiting.   If it owned this mm, find a new owner for the mm.\n */\nvoid mm_update_next_owner(struct mm_struct *mm)\n{\n\tstruct task_struct *c, *g, *p = current;\n\nretry:\n\t/*\n\t * If the exiting or execing task is not the owner, it's\n\t * someone else's problem.\n\t */\n\tif (mm->owner != p)\n\t\treturn;\n\t/*\n\t * The current owner is exiting/execing and there are no other\n\t * candidates.  Do not leave the mm pointing to a possibly\n\t * freed task structure.\n\t */\n\tif (atomic_read(&mm->mm_users) <= 1) {\n\t\tmm->owner = NULL;\n\t\treturn;\n\t}\n\n\tread_lock(&tasklist_lock);\n\t/*\n\t * Search in the children\n\t */\n\tlist_for_each_entry(c, &p->children, sibling) {\n\t\tif (c->mm == mm)\n\t\t\tgoto assign_new_owner;\n\t}\n\n\t/*\n\t * Search in the siblings\n\t */\n\tlist_for_each_entry(c, &p->real_parent->children, sibling) {\n\t\tif (c->mm == mm)\n\t\t\tgoto assign_new_owner;\n\t}\n\n\t/*\n\t * Search through everything else, we should not get here often.\n\t */\n\tfor_each_process(g) {\n\t\tif (g->flags & PF_KTHREAD)\n\t\t\tcontinue;\n\t\tfor_each_thread(g, c) {\n\t\t\tif (c->mm == mm)\n\t\t\t\tgoto assign_new_owner;\n\t\t\tif (c->mm)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tread_unlock(&tasklist_lock);\n\t/*\n\t * We found no owner yet mm_users > 1: this implies that we are\n\t * most likely racing with swapoff (try_to_unuse()) or /proc or\n\t * ptrace or page migration (get_task_mm()).  Mark owner as NULL.\n\t */\n\tmm->owner = NULL;\n\treturn;\n\nassign_new_owner:\n\tBUG_ON(c == p);\n\tget_task_struct(c);\n\t/*\n\t * The task_lock protects c->mm from changing.\n\t * We always want mm->owner->mm == mm\n\t */\n\ttask_lock(c);\n\t/*\n\t * Delay read_unlock() till we have the task_lock()\n\t * to ensure that c does not slip away underneath us\n\t */\n\tread_unlock(&tasklist_lock);\n\tif (c->mm != mm) {\n\t\ttask_unlock(c);\n\t\tput_task_struct(c);\n\t\tgoto retry;\n\t}\n\tmm->owner = c;\n\ttask_unlock(c);\n\tput_task_struct(c);\n}\n#endif /* CONFIG_MEMCG */\n\n/*\n * Turn us into a lazy TLB process if we\n * aren't already..\n */\nstatic void exit_mm(void)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct core_state *core_state;\n\n\tmm_release(current, mm);\n\tif (!mm)\n\t\treturn;\n\tsync_mm_rss(mm);\n\t/*\n\t * Serialize with any possible pending coredump.\n\t * We must hold mmap_sem around checking core_state\n\t * and clearing tsk->mm.  The core-inducing thread\n\t * will increment ->nr_threads for each thread in the\n\t * group with ->mm != NULL.\n\t */\n\tdown_read(&mm->mmap_sem);\n\tcore_state = mm->core_state;\n\tif (core_state) {\n\t\tstruct core_thread self;\n\n\t\tup_read(&mm->mmap_sem);\n\n\t\tself.task = current;\n\t\tself.next = xchg(&core_state->dumper.next, &self);\n\t\t/*\n\t\t * Implies mb(), the result of xchg() must be visible\n\t\t * to core_state->dumper.\n\t\t */\n\t\tif (atomic_dec_and_test(&core_state->nr_threads))\n\t\t\tcomplete(&core_state->startup);\n\n\t\tfor (;;) {\n\t\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tif (!self.task) /* see coredump_finish() */\n\t\t\t\tbreak;\n\t\t\tfreezable_schedule();\n\t\t}\n\t\t__set_current_state(TASK_RUNNING);\n\t\tdown_read(&mm->mmap_sem);\n\t}\n\tmmgrab(mm);\n\tBUG_ON(mm != current->active_mm);\n\t/* more a memory barrier than a real lock */\n\ttask_lock(current);\n\tcurrent->mm = NULL;\n\tup_read(&mm->mmap_sem);\n\tenter_lazy_tlb(mm, current);\n\ttask_unlock(current);\n\tmm_update_next_owner(mm);\n\tmmput(mm);\n\tif (test_thread_flag(TIF_MEMDIE))\n\t\texit_oom_victim();\n}\n\nstatic struct task_struct *find_alive_thread(struct task_struct *p)\n{\n\tstruct task_struct *t;\n\n\tfor_each_thread(p, t) {\n\t\tif (!(t->flags & PF_EXITING))\n\t\t\treturn t;\n\t}\n\treturn NULL;\n}\n\nstatic struct task_struct *find_child_reaper(struct task_struct *father)\n\t__releases(&tasklist_lock)\n\t__acquires(&tasklist_lock)\n{\n\tstruct pid_namespace *pid_ns = task_active_pid_ns(father);\n\tstruct task_struct *reaper = pid_ns->child_reaper;\n\n\tif (likely(reaper != father))\n\t\treturn reaper;\n\n\treaper = find_alive_thread(father);\n\tif (reaper) {\n\t\tpid_ns->child_reaper = reaper;\n\t\treturn reaper;\n\t}\n\n\twrite_unlock_irq(&tasklist_lock);\n\tif (unlikely(pid_ns == &init_pid_ns)) {\n\t\tpanic(\"Attempted to kill init! exitcode=0x%08x\\n\",\n\t\t\tfather->signal->group_exit_code ?: father->exit_code);\n\t}\n\tzap_pid_ns_processes(pid_ns);\n\twrite_lock_irq(&tasklist_lock);\n\n\treturn father;\n}\n\n/*\n * When we die, we re-parent all our children, and try to:\n * 1. give them to another thread in our thread group, if such a member exists\n * 2. give it to the first ancestor process which prctl'd itself as a\n *    child_subreaper for its children (like a service manager)\n * 3. give it to the init process (PID 1) in our pid namespace\n */\nstatic struct task_struct *find_new_reaper(struct task_struct *father,\n\t\t\t\t\t   struct task_struct *child_reaper)\n{\n\tstruct task_struct *thread, *reaper;\n\n\tthread = find_alive_thread(father);\n\tif (thread)\n\t\treturn thread;\n\n\tif (father->signal->has_child_subreaper) {\n\t\tunsigned int ns_level = task_pid(father)->level;\n\t\t/*\n\t\t * Find the first ->is_child_subreaper ancestor in our pid_ns.\n\t\t * We can't check reaper != child_reaper to ensure we do not\n\t\t * cross the namespaces, the exiting parent could be injected\n\t\t * by setns() + fork().\n\t\t * We check pid->level, this is slightly more efficient than\n\t\t * task_active_pid_ns(reaper) != task_active_pid_ns(father).\n\t\t */\n\t\tfor (reaper = father->real_parent;\n\t\t     task_pid(reaper)->level == ns_level;\n\t\t     reaper = reaper->real_parent) {\n\t\t\tif (reaper == &init_task)\n\t\t\t\tbreak;\n\t\t\tif (!reaper->signal->is_child_subreaper)\n\t\t\t\tcontinue;\n\t\t\tthread = find_alive_thread(reaper);\n\t\t\tif (thread)\n\t\t\t\treturn thread;\n\t\t}\n\t}\n\n\treturn child_reaper;\n}\n\n/*\n* Any that need to be release_task'd are put on the @dead list.\n */\nstatic void reparent_leader(struct task_struct *father, struct task_struct *p,\n\t\t\t\tstruct list_head *dead)\n{\n\tif (unlikely(p->exit_state == EXIT_DEAD))\n\t\treturn;\n\n\t/* We don't want people slaying init. */\n\tp->exit_signal = SIGCHLD;\n\n\t/* If it has exited notify the new parent about this child's death. */\n\tif (!p->ptrace &&\n\t    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {\n\t\tif (do_notify_parent(p, p->exit_signal)) {\n\t\t\tp->exit_state = EXIT_DEAD;\n\t\t\tlist_add(&p->ptrace_entry, dead);\n\t\t}\n\t}\n\n\tkill_orphaned_pgrp(p, father);\n}\n\n/*\n * This does two things:\n *\n * A.  Make init inherit all the child processes\n * B.  Check to see if any process groups have become orphaned\n *\tas a result of our exiting, and if they have any stopped\n *\tjobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)\n */\nstatic void forget_original_parent(struct task_struct *father,\n\t\t\t\t\tstruct list_head *dead)\n{\n\tstruct task_struct *p, *t, *reaper;\n\n\tif (unlikely(!list_empty(&father->ptraced)))\n\t\texit_ptrace(father, dead);\n\n\t/* Can drop and reacquire tasklist_lock */\n\treaper = find_child_reaper(father);\n\tif (list_empty(&father->children))\n\t\treturn;\n\n\treaper = find_new_reaper(father, reaper);\n\tlist_for_each_entry(p, &father->children, sibling) {\n\t\tfor_each_thread(p, t) {\n\t\t\tt->real_parent = reaper;\n\t\t\tBUG_ON((!t->ptrace) != (t->parent == father));\n\t\t\tif (likely(!t->ptrace))\n\t\t\t\tt->parent = t->real_parent;\n\t\t\tif (t->pdeath_signal)\n\t\t\t\tgroup_send_sig_info(t->pdeath_signal,\n\t\t\t\t\t\t    SEND_SIG_NOINFO, t);\n\t\t}\n\t\t/*\n\t\t * If this is a threaded reparent there is no need to\n\t\t * notify anyone anything has happened.\n\t\t */\n\t\tif (!same_thread_group(reaper, father))\n\t\t\treparent_leader(father, p, dead);\n\t}\n\tlist_splice_tail_init(&father->children, &reaper->children);\n}\n\n/*\n * Send signals to all our closest relatives so that they know\n * to properly mourn us..\n */\nstatic void exit_notify(struct task_struct *tsk, int group_dead)\n{\n\tbool autoreap;\n\tstruct task_struct *p, *n;\n\tLIST_HEAD(dead);\n\n\twrite_lock_irq(&tasklist_lock);\n\tforget_original_parent(tsk, &dead);\n\n\tif (group_dead)\n\t\tkill_orphaned_pgrp(tsk->group_leader, NULL);\n\n\tif (unlikely(tsk->ptrace)) {\n\t\tint sig = thread_group_leader(tsk) &&\n\t\t\t\tthread_group_empty(tsk) &&\n\t\t\t\t!ptrace_reparented(tsk) ?\n\t\t\ttsk->exit_signal : SIGCHLD;\n\t\tautoreap = do_notify_parent(tsk, sig);\n\t} else if (thread_group_leader(tsk)) {\n\t\tautoreap = thread_group_empty(tsk) &&\n\t\t\tdo_notify_parent(tsk, tsk->exit_signal);\n\t} else {\n\t\tautoreap = true;\n\t}\n\n\ttsk->exit_state = autoreap ? EXIT_DEAD : EXIT_ZOMBIE;\n\tif (tsk->exit_state == EXIT_DEAD)\n\t\tlist_add(&tsk->ptrace_entry, &dead);\n\n\t/* mt-exec, de_thread() is waiting for group leader */\n\tif (unlikely(tsk->signal->notify_count < 0))\n\t\twake_up_process(tsk->signal->group_exit_task);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tlist_for_each_entry_safe(p, n, &dead, ptrace_entry) {\n\t\tlist_del_init(&p->ptrace_entry);\n\t\trelease_task(p);\n\t}\n}\n\n#ifdef CONFIG_DEBUG_STACK_USAGE\nstatic void check_stack_usage(void)\n{\n\tstatic DEFINE_SPINLOCK(low_water_lock);\n\tstatic int lowest_to_date = THREAD_SIZE;\n\tunsigned long free;\n\n\tfree = stack_not_used(current);\n\n\tif (free >= lowest_to_date)\n\t\treturn;\n\n\tspin_lock(&low_water_lock);\n\tif (free < lowest_to_date) {\n\t\tpr_info(\"%s (%d) used greatest stack depth: %lu bytes left\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current), free);\n\t\tlowest_to_date = free;\n\t}\n\tspin_unlock(&low_water_lock);\n}\n#else\nstatic inline void check_stack_usage(void) {}\n#endif\n\nvoid __noreturn do_exit(long code)\n{\n\tstruct task_struct *tsk = current;\n\tint group_dead;\n\n\tprofile_task_exit(tsk);\n\tkcov_task_exit(tsk);\n\n\tWARN_ON(blk_needs_flush_plug(tsk));\n\n\tif (unlikely(in_interrupt()))\n\t\tpanic(\"Aiee, killing interrupt handler!\");\n\tif (unlikely(!tsk->pid))\n\t\tpanic(\"Attempted to kill the idle task!\");\n\n\t/*\n\t * If do_exit is called because this processes oopsed, it's possible\n\t * that get_fs() was left as KERNEL_DS, so reset it to USER_DS before\n\t * continuing. Amongst other possible reasons, this is to prevent\n\t * mm_release()->clear_child_tid() from writing to a user-controlled\n\t * kernel address.\n\t */\n\tset_fs(USER_DS);\n\n\tptrace_event(PTRACE_EVENT_EXIT, code);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\t/*\n\t * We're taking recursive faults here in do_exit. Safest is to just\n\t * leave this task alone and wait for reboot.\n\t */\n\tif (unlikely(tsk->flags & PF_EXITING)) {\n\t\tpr_alert(\"Fixing recursive fault but reboot is needed!\\n\");\n\t\t/*\n\t\t * We can do this unlocked here. The futex code uses\n\t\t * this flag just to verify whether the pi state\n\t\t * cleanup has been done or not. In the worst case it\n\t\t * loops once more. We pretend that the cleanup was\n\t\t * done as there is no way to return. Either the\n\t\t * OWNER_DIED bit is set by now or we push the blocked\n\t\t * task into the wait for ever nirwana as well.\n\t\t */\n\t\ttsk->flags |= PF_EXITPIDONE;\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\tschedule();\n\t}\n\n\texit_signals(tsk);  /* sets PF_EXITING */\n\t/*\n\t * Ensure that all new tsk->pi_lock acquisitions must observe\n\t * PF_EXITING. Serializes against futex.c:attach_to_pi_owner().\n\t */\n\tsmp_mb();\n\t/*\n\t * Ensure that we must observe the pi_state in exit_mm() ->\n\t * mm_release() -> exit_pi_state_list().\n\t */\n\traw_spin_lock_irq(&tsk->pi_lock);\n\traw_spin_unlock_irq(&tsk->pi_lock);\n\n\tif (unlikely(in_atomic())) {\n\t\tpr_info(\"note: %s[%d] exited with preempt_count %d\\n\",\n\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\tpreempt_count());\n\t\tpreempt_count_set(PREEMPT_ENABLED);\n\t}\n\n\t/* sync mm's RSS info before statistics gathering */\n\tif (tsk->mm)\n\t\tsync_mm_rss(tsk->mm);\n\tacct_update_integrals(tsk);\n\tgroup_dead = atomic_dec_and_test(&tsk->signal->live);\n\tif (group_dead) {\n#ifdef CONFIG_POSIX_TIMERS\n\t\thrtimer_cancel(&tsk->signal->real_timer);\n\t\texit_itimers(tsk->signal);\n#endif\n\t\tif (tsk->mm)\n\t\t\tsetmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);\n\t}\n\tacct_collect(code, group_dead);\n\tif (group_dead)\n\t\ttty_audit_exit();\n\taudit_free(tsk);\n\n\ttsk->exit_code = code;\n\ttaskstats_exit(tsk, group_dead);\n\n\texit_mm();\n\n\tif (group_dead)\n\t\tacct_process();\n\ttrace_sched_process_exit(tsk);\n\n\texit_sem(tsk);\n\texit_shm(tsk);\n\texit_files(tsk);\n\texit_fs(tsk);\n\tif (group_dead)\n\t\tdisassociate_ctty(1);\n\texit_task_namespaces(tsk);\n\texit_task_work(tsk);\n\texit_thread(tsk);\n\n\t/*\n\t * Flush inherited counters to the parent - before the parent\n\t * gets woken up by child-exit notifications.\n\t *\n\t * because of cgroup mode, must be called before cgroup_exit()\n\t */\n\tperf_event_exit_task(tsk);\n\n\tsched_autogroup_exit_task(tsk);\n\tcgroup_exit(tsk);\n\n\t/*\n\t * FIXME: do that only when needed, using sched_exit tracepoint\n\t */\n\tflush_ptrace_hw_breakpoint(tsk);\n\n\texit_tasks_rcu_start();\n\texit_notify(tsk, group_dead);\n\tproc_exit_connector(tsk);\n\tmpol_put_task_policy(tsk);\n#ifdef CONFIG_FUTEX\n\tif (unlikely(current->pi_state_cache))\n\t\tkfree(current->pi_state_cache);\n#endif\n\t/*\n\t * Make sure we are holding no locks:\n\t */\n\tdebug_check_no_locks_held();\n\t/*\n\t * We can do this unlocked here. The futex code uses this flag\n\t * just to verify whether the pi state cleanup has been done\n\t * or not. In the worst case it loops once more.\n\t */\n\ttsk->flags |= PF_EXITPIDONE;\n\n\tif (tsk->io_context)\n\t\texit_io_context(tsk);\n\n\tif (tsk->splice_pipe)\n\t\tfree_pipe_info(tsk->splice_pipe);\n\n\tif (tsk->task_frag.page)\n\t\tput_page(tsk->task_frag.page);\n\n\tvalidate_creds_for_do_exit(tsk);\n\n\tcheck_stack_usage();\n\tpreempt_disable();\n\tif (tsk->nr_dirtied)\n\t\t__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);\n\texit_rcu();\n\texit_tasks_rcu_finish();\n\n\tlockdep_free_task(tsk);\n\tdo_task_dead();\n}\nEXPORT_SYMBOL_GPL(do_exit);\n\nvoid complete_and_exit(struct completion *comp, long code)\n{\n\tif (comp)\n\t\tcomplete(comp);\n\n\tdo_exit(code);\n}\nEXPORT_SYMBOL(complete_and_exit);\n\nSYSCALL_DEFINE1(exit, int, error_code)\n{\n\tdo_exit((error_code&0xff)<<8);\n}\n\n/*\n * Take down every thread in the group.  This is called by fatal signals\n * as well as by sys_exit_group (below).\n */\nvoid\ndo_group_exit(int exit_code)\n{\n\tstruct signal_struct *sig = current->signal;\n\n\tBUG_ON(exit_code & 0x80); /* core dumps don't get here */\n\n\tif (signal_group_exit(sig))\n\t\texit_code = sig->group_exit_code;\n\telse if (!thread_group_empty(current)) {\n\t\tstruct sighand_struct *const sighand = current->sighand;\n\n\t\tspin_lock_irq(&sighand->siglock);\n\t\tif (signal_group_exit(sig))\n\t\t\t/* Another thread got here before we took the lock.  */\n\t\t\texit_code = sig->group_exit_code;\n\t\telse {\n\t\t\tsig->group_exit_code = exit_code;\n\t\t\tsig->flags = SIGNAL_GROUP_EXIT;\n\t\t\tzap_other_threads(current);\n\t\t}\n\t\tspin_unlock_irq(&sighand->siglock);\n\t}\n\n\tdo_exit(exit_code);\n\t/* NOTREACHED */\n}\n\n/*\n * this kills every thread in the thread group. Note that any externally\n * wait4()-ing process will get the correct exit code - even if this\n * thread is not the thread group leader.\n */\nSYSCALL_DEFINE1(exit_group, int, error_code)\n{\n\tdo_group_exit((error_code & 0xff) << 8);\n\t/* NOTREACHED */\n\treturn 0;\n}\n\nstruct waitid_info {\n\tpid_t pid;\n\tuid_t uid;\n\tint status;\n\tint cause;\n};\n\nstruct wait_opts {\n\tenum pid_type\t\two_type;\n\tint\t\t\two_flags;\n\tstruct pid\t\t*wo_pid;\n\n\tstruct waitid_info\t*wo_info;\n\tint\t\t\two_stat;\n\tstruct rusage\t\t*wo_rusage;\n\n\twait_queue_entry_t\t\tchild_wait;\n\tint\t\t\tnotask_error;\n};\n\nstatic inline\nstruct pid *task_pid_type(struct task_struct *task, enum pid_type type)\n{\n\tif (type != PIDTYPE_PID)\n\t\ttask = task->group_leader;\n\treturn task->pids[type].pid;\n}\n\nstatic int eligible_pid(struct wait_opts *wo, struct task_struct *p)\n{\n\treturn\two->wo_type == PIDTYPE_MAX ||\n\t\ttask_pid_type(p, wo->wo_type) == wo->wo_pid;\n}\n\nstatic int\neligible_child(struct wait_opts *wo, bool ptrace, struct task_struct *p)\n{\n\tif (!eligible_pid(wo, p))\n\t\treturn 0;\n\n\t/*\n\t * Wait for all children (clone and not) if __WALL is set or\n\t * if it is traced by us.\n\t */\n\tif (ptrace || (wo->wo_flags & __WALL))\n\t\treturn 1;\n\n\t/*\n\t * Otherwise, wait for clone children *only* if __WCLONE is set;\n\t * otherwise, wait for non-clone children *only*.\n\t *\n\t * Note: a \"clone\" child here is one that reports to its parent\n\t * using a signal other than SIGCHLD, or a non-leader thread which\n\t * we can only see if it is traced by us.\n\t */\n\tif ((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n/*\n * Handle sys_wait4 work for one task in state EXIT_ZOMBIE.  We hold\n * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold\n * the lock and this task is uninteresting.  If we return nonzero, we have\n * released the lock and the system call should return.\n */\nstatic int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)\n{\n\tint state, status;\n\tpid_t pid = task_pid_vnr(p);\n\tuid_t uid = from_kuid_munged(current_user_ns(), task_uid(p));\n\tstruct waitid_info *infop;\n\n\tif (!likely(wo->wo_flags & WEXITED))\n\t\treturn 0;\n\n\tif (unlikely(wo->wo_flags & WNOWAIT)) {\n\t\tstatus = p->exit_code;\n\t\tget_task_struct(p);\n\t\tread_unlock(&tasklist_lock);\n\t\tsched_annotate_sleep();\n\t\tif (wo->wo_rusage)\n\t\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\t\tput_task_struct(p);\n\t\tgoto out_info;\n\t}\n\t/*\n\t * Move the task's state to DEAD/TRACE, only one thread can do this.\n\t */\n\tstate = (ptrace_reparented(p) && thread_group_leader(p)) ?\n\t\tEXIT_TRACE : EXIT_DEAD;\n\tif (cmpxchg(&p->exit_state, EXIT_ZOMBIE, state) != EXIT_ZOMBIE)\n\t\treturn 0;\n\t/*\n\t * We own this thread, nobody else can reap it.\n\t */\n\tread_unlock(&tasklist_lock);\n\tsched_annotate_sleep();\n\n\t/*\n\t * Check thread_group_leader() to exclude the traced sub-threads.\n\t */\n\tif (state == EXIT_DEAD && thread_group_leader(p)) {\n\t\tstruct signal_struct *sig = p->signal;\n\t\tstruct signal_struct *psig = current->signal;\n\t\tunsigned long maxrss;\n\t\tu64 tgutime, tgstime;\n\n\t\t/*\n\t\t * The resource counters for the group leader are in its\n\t\t * own task_struct.  Those for dead threads in the group\n\t\t * are in its signal_struct, as are those for the child\n\t\t * processes it has previously reaped.  All these\n\t\t * accumulate in the parent's signal_struct c* fields.\n\t\t *\n\t\t * We don't bother to take a lock here to protect these\n\t\t * p->signal fields because the whole thread group is dead\n\t\t * and nobody can change them.\n\t\t *\n\t\t * psig->stats_lock also protects us from our sub-theads\n\t\t * which can reap other children at the same time. Until\n\t\t * we change k_getrusage()-like users to rely on this lock\n\t\t * we have to take ->siglock as well.\n\t\t *\n\t\t * We use thread_group_cputime_adjusted() to get times for\n\t\t * the thread group, which consolidates times for all threads\n\t\t * in the group including the group leader.\n\t\t */\n\t\tthread_group_cputime_adjusted(p, &tgutime, &tgstime);\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\twrite_seqlock(&psig->stats_lock);\n\t\tpsig->cutime += tgutime + sig->cutime;\n\t\tpsig->cstime += tgstime + sig->cstime;\n\t\tpsig->cgtime += task_gtime(p) + sig->gtime + sig->cgtime;\n\t\tpsig->cmin_flt +=\n\t\t\tp->min_flt + sig->min_flt + sig->cmin_flt;\n\t\tpsig->cmaj_flt +=\n\t\t\tp->maj_flt + sig->maj_flt + sig->cmaj_flt;\n\t\tpsig->cnvcsw +=\n\t\t\tp->nvcsw + sig->nvcsw + sig->cnvcsw;\n\t\tpsig->cnivcsw +=\n\t\t\tp->nivcsw + sig->nivcsw + sig->cnivcsw;\n\t\tpsig->cinblock +=\n\t\t\ttask_io_get_inblock(p) +\n\t\t\tsig->inblock + sig->cinblock;\n\t\tpsig->coublock +=\n\t\t\ttask_io_get_oublock(p) +\n\t\t\tsig->oublock + sig->coublock;\n\t\tmaxrss = max(sig->maxrss, sig->cmaxrss);\n\t\tif (psig->cmaxrss < maxrss)\n\t\t\tpsig->cmaxrss = maxrss;\n\t\ttask_io_accounting_add(&psig->ioac, &p->ioac);\n\t\ttask_io_accounting_add(&psig->ioac, &sig->ioac);\n\t\twrite_sequnlock(&psig->stats_lock);\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t}\n\n\tif (wo->wo_rusage)\n\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\tstatus = (p->signal->flags & SIGNAL_GROUP_EXIT)\n\t\t? p->signal->group_exit_code : p->exit_code;\n\two->wo_stat = status;\n\n\tif (state == EXIT_TRACE) {\n\t\twrite_lock_irq(&tasklist_lock);\n\t\t/* We dropped tasklist, ptracer could die and untrace */\n\t\tptrace_unlink(p);\n\n\t\t/* If parent wants a zombie, don't release it now */\n\t\tstate = EXIT_ZOMBIE;\n\t\tif (do_notify_parent(p, p->exit_signal))\n\t\t\tstate = EXIT_DEAD;\n\t\tp->exit_state = state;\n\t\twrite_unlock_irq(&tasklist_lock);\n\t}\n\tif (state == EXIT_DEAD)\n\t\trelease_task(p);\n\nout_info:\n\tinfop = wo->wo_info;\n\tif (infop) {\n\t\tif ((status & 0x7f) == 0) {\n\t\t\tinfop->cause = CLD_EXITED;\n\t\t\tinfop->status = status >> 8;\n\t\t} else {\n\t\t\tinfop->cause = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;\n\t\t\tinfop->status = status & 0x7f;\n\t\t}\n\t\tinfop->pid = pid;\n\t\tinfop->uid = uid;\n\t}\n\n\treturn pid;\n}\n\nstatic int *task_stopped_code(struct task_struct *p, bool ptrace)\n{\n\tif (ptrace) {\n\t\tif (task_is_traced(p) && !(p->jobctl & JOBCTL_LISTENING))\n\t\t\treturn &p->exit_code;\n\t} else {\n\t\tif (p->signal->flags & SIGNAL_STOP_STOPPED)\n\t\t\treturn &p->signal->group_exit_code;\n\t}\n\treturn NULL;\n}\n\n/**\n * wait_task_stopped - Wait for %TASK_STOPPED or %TASK_TRACED\n * @wo: wait options\n * @ptrace: is the wait for ptrace\n * @p: task to wait for\n *\n * Handle sys_wait4() work for %p in state %TASK_STOPPED or %TASK_TRACED.\n *\n * CONTEXT:\n * read_lock(&tasklist_lock), which is released if return value is\n * non-zero.  Also, grabs and releases @p->sighand->siglock.\n *\n * RETURNS:\n * 0 if wait condition didn't exist and search for other wait conditions\n * should continue.  Non-zero return, -errno on failure and @p's pid on\n * success, implies that tasklist_lock is released and wait condition\n * search should terminate.\n */\nstatic int wait_task_stopped(struct wait_opts *wo,\n\t\t\t\tint ptrace, struct task_struct *p)\n{\n\tstruct waitid_info *infop;\n\tint exit_code, *p_code, why;\n\tuid_t uid = 0; /* unneeded, required by compiler */\n\tpid_t pid;\n\n\t/*\n\t * Traditionally we see ptrace'd stopped tasks regardless of options.\n\t */\n\tif (!ptrace && !(wo->wo_flags & WUNTRACED))\n\t\treturn 0;\n\n\tif (!task_stopped_code(p, ptrace))\n\t\treturn 0;\n\n\texit_code = 0;\n\tspin_lock_irq(&p->sighand->siglock);\n\n\tp_code = task_stopped_code(p, ptrace);\n\tif (unlikely(!p_code))\n\t\tgoto unlock_sig;\n\n\texit_code = *p_code;\n\tif (!exit_code)\n\t\tgoto unlock_sig;\n\n\tif (!unlikely(wo->wo_flags & WNOWAIT))\n\t\t*p_code = 0;\n\n\tuid = from_kuid_munged(current_user_ns(), task_uid(p));\nunlock_sig:\n\tspin_unlock_irq(&p->sighand->siglock);\n\tif (!exit_code)\n\t\treturn 0;\n\n\t/*\n\t * Now we are pretty sure this task is interesting.\n\t * Make sure it doesn't get reaped out from under us while we\n\t * give up the lock and then examine it below.  We don't want to\n\t * keep holding onto the tasklist_lock while we call getrusage and\n\t * possibly take page faults for user memory.\n\t */\n\tget_task_struct(p);\n\tpid = task_pid_vnr(p);\n\twhy = ptrace ? CLD_TRAPPED : CLD_STOPPED;\n\tread_unlock(&tasklist_lock);\n\tsched_annotate_sleep();\n\tif (wo->wo_rusage)\n\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\tput_task_struct(p);\n\n\tif (likely(!(wo->wo_flags & WNOWAIT)))\n\t\two->wo_stat = (exit_code << 8) | 0x7f;\n\n\tinfop = wo->wo_info;\n\tif (infop) {\n\t\tinfop->cause = why;\n\t\tinfop->status = exit_code;\n\t\tinfop->pid = pid;\n\t\tinfop->uid = uid;\n\t}\n\treturn pid;\n}\n\n/*\n * Handle do_wait work for one task in a live, non-stopped state.\n * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold\n * the lock and this task is uninteresting.  If we return nonzero, we have\n * released the lock and the system call should return.\n */\nstatic int wait_task_continued(struct wait_opts *wo, struct task_struct *p)\n{\n\tstruct waitid_info *infop;\n\tpid_t pid;\n\tuid_t uid;\n\n\tif (!unlikely(wo->wo_flags & WCONTINUED))\n\t\treturn 0;\n\n\tif (!(p->signal->flags & SIGNAL_STOP_CONTINUED))\n\t\treturn 0;\n\n\tspin_lock_irq(&p->sighand->siglock);\n\t/* Re-check with the lock held.  */\n\tif (!(p->signal->flags & SIGNAL_STOP_CONTINUED)) {\n\t\tspin_unlock_irq(&p->sighand->siglock);\n\t\treturn 0;\n\t}\n\tif (!unlikely(wo->wo_flags & WNOWAIT))\n\t\tp->signal->flags &= ~SIGNAL_STOP_CONTINUED;\n\tuid = from_kuid_munged(current_user_ns(), task_uid(p));\n\tspin_unlock_irq(&p->sighand->siglock);\n\n\tpid = task_pid_vnr(p);\n\tget_task_struct(p);\n\tread_unlock(&tasklist_lock);\n\tsched_annotate_sleep();\n\tif (wo->wo_rusage)\n\t\tgetrusage(p, RUSAGE_BOTH, wo->wo_rusage);\n\tput_task_struct(p);\n\n\tinfop = wo->wo_info;\n\tif (!infop) {\n\t\two->wo_stat = 0xffff;\n\t} else {\n\t\tinfop->cause = CLD_CONTINUED;\n\t\tinfop->pid = pid;\n\t\tinfop->uid = uid;\n\t\tinfop->status = SIGCONT;\n\t}\n\treturn pid;\n}\n\n/*\n * Consider @p for a wait by @parent.\n *\n * -ECHILD should be in ->notask_error before the first call.\n * Returns nonzero for a final return, when we have unlocked tasklist_lock.\n * Returns zero if the search for a child should continue;\n * then ->notask_error is 0 if @p is an eligible child,\n * or still -ECHILD.\n */\nstatic int wait_consider_task(struct wait_opts *wo, int ptrace,\n\t\t\t\tstruct task_struct *p)\n{\n\t/*\n\t * We can race with wait_task_zombie() from another thread.\n\t * Ensure that EXIT_ZOMBIE -> EXIT_DEAD/EXIT_TRACE transition\n\t * can't confuse the checks below.\n\t */\n\tint exit_state = ACCESS_ONCE(p->exit_state);\n\tint ret;\n\n\tif (unlikely(exit_state == EXIT_DEAD))\n\t\treturn 0;\n\n\tret = eligible_child(wo, ptrace, p);\n\tif (!ret)\n\t\treturn ret;\n\n\tif (unlikely(exit_state == EXIT_TRACE)) {\n\t\t/*\n\t\t * ptrace == 0 means we are the natural parent. In this case\n\t\t * we should clear notask_error, debugger will notify us.\n\t\t */\n\t\tif (likely(!ptrace))\n\t\t\two->notask_error = 0;\n\t\treturn 0;\n\t}\n\n\tif (likely(!ptrace) && unlikely(p->ptrace)) {\n\t\t/*\n\t\t * If it is traced by its real parent's group, just pretend\n\t\t * the caller is ptrace_do_wait() and reap this child if it\n\t\t * is zombie.\n\t\t *\n\t\t * This also hides group stop state from real parent; otherwise\n\t\t * a single stop can be reported twice as group and ptrace stop.\n\t\t * If a ptracer wants to distinguish these two events for its\n\t\t * own children it should create a separate process which takes\n\t\t * the role of real parent.\n\t\t */\n\t\tif (!ptrace_reparented(p))\n\t\t\tptrace = 1;\n\t}\n\n\t/* slay zombie? */\n\tif (exit_state == EXIT_ZOMBIE) {\n\t\t/* we don't reap group leaders with subthreads */\n\t\tif (!delay_group_leader(p)) {\n\t\t\t/*\n\t\t\t * A zombie ptracee is only visible to its ptracer.\n\t\t\t * Notification and reaping will be cascaded to the\n\t\t\t * real parent when the ptracer detaches.\n\t\t\t */\n\t\t\tif (unlikely(ptrace) || likely(!p->ptrace))\n\t\t\t\treturn wait_task_zombie(wo, p);\n\t\t}\n\n\t\t/*\n\t\t * Allow access to stopped/continued state via zombie by\n\t\t * falling through.  Clearing of notask_error is complex.\n\t\t *\n\t\t * When !@ptrace:\n\t\t *\n\t\t * If WEXITED is set, notask_error should naturally be\n\t\t * cleared.  If not, subset of WSTOPPED|WCONTINUED is set,\n\t\t * so, if there are live subthreads, there are events to\n\t\t * wait for.  If all subthreads are dead, it's still safe\n\t\t * to clear - this function will be called again in finite\n\t\t * amount time once all the subthreads are released and\n\t\t * will then return without clearing.\n\t\t *\n\t\t * When @ptrace:\n\t\t *\n\t\t * Stopped state is per-task and thus can't change once the\n\t\t * target task dies.  Only continued and exited can happen.\n\t\t * Clear notask_error if WCONTINUED | WEXITED.\n\t\t */\n\t\tif (likely(!ptrace) || (wo->wo_flags & (WCONTINUED | WEXITED)))\n\t\t\two->notask_error = 0;\n\t} else {\n\t\t/*\n\t\t * @p is alive and it's gonna stop, continue or exit, so\n\t\t * there always is something to wait for.\n\t\t */\n\t\two->notask_error = 0;\n\t}\n\n\t/*\n\t * Wait for stopped.  Depending on @ptrace, different stopped state\n\t * is used and the two don't interact with each other.\n\t */\n\tret = wait_task_stopped(wo, ptrace, p);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * Wait for continued.  There's only one continued state and the\n\t * ptracer can consume it which can confuse the real parent.  Don't\n\t * use WCONTINUED from ptracer.  You don't need or want it.\n\t */\n\treturn wait_task_continued(wo, p);\n}\n\n/*\n * Do the work of do_wait() for one thread in the group, @tsk.\n *\n * -ECHILD should be in ->notask_error before the first call.\n * Returns nonzero for a final return, when we have unlocked tasklist_lock.\n * Returns zero if the search for a child should continue; then\n * ->notask_error is 0 if there were any eligible children,\n * or still -ECHILD.\n */\nstatic int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)\n{\n\tstruct task_struct *p;\n\n\tlist_for_each_entry(p, &tsk->children, sibling) {\n\t\tint ret = wait_consider_task(wo, 0, p);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)\n{\n\tstruct task_struct *p;\n\n\tlist_for_each_entry(p, &tsk->ptraced, ptrace_entry) {\n\t\tint ret = wait_consider_task(wo, 1, p);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int child_wait_callback(wait_queue_entry_t *wait, unsigned mode,\n\t\t\t\tint sync, void *key)\n{\n\tstruct wait_opts *wo = container_of(wait, struct wait_opts,\n\t\t\t\t\t\tchild_wait);\n\tstruct task_struct *p = key;\n\n\tif (!eligible_pid(wo, p))\n\t\treturn 0;\n\n\tif ((wo->wo_flags & __WNOTHREAD) && wait->private != p->parent)\n\t\treturn 0;\n\n\treturn default_wake_function(wait, mode, sync, key);\n}\n\nvoid __wake_up_parent(struct task_struct *p, struct task_struct *parent)\n{\n\t__wake_up_sync_key(&parent->signal->wait_chldexit,\n\t\t\t\tTASK_INTERRUPTIBLE, 1, p);\n}\n\nstatic long do_wait(struct wait_opts *wo)\n{\n\tstruct task_struct *tsk;\n\tint retval;\n\n\ttrace_sched_process_wait(wo->wo_pid);\n\n\tinit_waitqueue_func_entry(&wo->child_wait, child_wait_callback);\n\two->child_wait.private = current;\n\tadd_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);\nrepeat:\n\t/*\n\t * If there is nothing that can match our criteria, just get out.\n\t * We will clear ->notask_error to zero if we see any child that\n\t * might later match our criteria, even if we are not able to reap\n\t * it yet.\n\t */\n\two->notask_error = -ECHILD;\n\tif ((wo->wo_type < PIDTYPE_MAX) &&\n\t   (!wo->wo_pid || hlist_empty(&wo->wo_pid->tasks[wo->wo_type])))\n\t\tgoto notask;\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\tread_lock(&tasklist_lock);\n\ttsk = current;\n\tdo {\n\t\tretval = do_wait_thread(wo, tsk);\n\t\tif (retval)\n\t\t\tgoto end;\n\n\t\tretval = ptrace_do_wait(wo, tsk);\n\t\tif (retval)\n\t\t\tgoto end;\n\n\t\tif (wo->wo_flags & __WNOTHREAD)\n\t\t\tbreak;\n\t} while_each_thread(current, tsk);\n\tread_unlock(&tasklist_lock);\n\nnotask:\n\tretval = wo->notask_error;\n\tif (!retval && !(wo->wo_flags & WNOHANG)) {\n\t\tretval = -ERESTARTSYS;\n\t\tif (!signal_pending(current)) {\n\t\t\tschedule();\n\t\t\tgoto repeat;\n\t\t}\n\t}\nend:\n\t__set_current_state(TASK_RUNNING);\n\tremove_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);\n\treturn retval;\n}\n\nstatic long kernel_waitid(int which, pid_t upid, struct waitid_info *infop,\n\t\t\t  int options, struct rusage *ru)\n{\n\tstruct wait_opts wo;\n\tstruct pid *pid = NULL;\n\tenum pid_type type;\n\tlong ret;\n\n\tif (options & ~(WNOHANG|WNOWAIT|WEXITED|WSTOPPED|WCONTINUED|\n\t\t\t__WNOTHREAD|__WCLONE|__WALL))\n\t\treturn -EINVAL;\n\tif (!(options & (WEXITED|WSTOPPED|WCONTINUED)))\n\t\treturn -EINVAL;\n\n\tswitch (which) {\n\tcase P_ALL:\n\t\ttype = PIDTYPE_MAX;\n\t\tbreak;\n\tcase P_PID:\n\t\ttype = PIDTYPE_PID;\n\t\tif (upid <= 0)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase P_PGID:\n\t\ttype = PIDTYPE_PGID;\n\t\tif (upid <= 0)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (type < PIDTYPE_MAX)\n\t\tpid = find_get_pid(upid);\n\n\two.wo_type\t= type;\n\two.wo_pid\t= pid;\n\two.wo_flags\t= options;\n\two.wo_info\t= infop;\n\two.wo_rusage\t= ru;\n\tret = do_wait(&wo);\n\n\tput_pid(pid);\n\treturn ret;\n}\n\nSYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,\n\t\tinfop, int, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);\n\tint signo = 0;\n\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\tif (!infop)\n\t\treturn err;\n\n\tuser_access_begin();\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}\n\nlong kernel_wait4(pid_t upid, int __user *stat_addr, int options,\n\t\t  struct rusage *ru)\n{\n\tstruct wait_opts wo;\n\tstruct pid *pid = NULL;\n\tenum pid_type type;\n\tlong ret;\n\n\tif (options & ~(WNOHANG|WUNTRACED|WCONTINUED|\n\t\t\t__WNOTHREAD|__WCLONE|__WALL))\n\t\treturn -EINVAL;\n\n\t/* -INT_MIN is not defined */\n\tif (upid == INT_MIN)\n\t\treturn -ESRCH;\n\n\tif (upid == -1)\n\t\ttype = PIDTYPE_MAX;\n\telse if (upid < 0) {\n\t\ttype = PIDTYPE_PGID;\n\t\tpid = find_get_pid(-upid);\n\t} else if (upid == 0) {\n\t\ttype = PIDTYPE_PGID;\n\t\tpid = get_task_pid(current, PIDTYPE_PGID);\n\t} else /* upid > 0 */ {\n\t\ttype = PIDTYPE_PID;\n\t\tpid = find_get_pid(upid);\n\t}\n\n\two.wo_type\t= type;\n\two.wo_pid\t= pid;\n\two.wo_flags\t= options | WEXITED;\n\two.wo_info\t= NULL;\n\two.wo_stat\t= 0;\n\two.wo_rusage\t= ru;\n\tret = do_wait(&wo);\n\tput_pid(pid);\n\tif (ret > 0 && stat_addr && put_user(wo.wo_stat, stat_addr))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nSYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,\n\t\tint, options, struct rusage __user *, ru)\n{\n\tstruct rusage r;\n\tlong err = kernel_wait4(upid, stat_addr, options, ru ? &r : NULL);\n\n\tif (err > 0) {\n\t\tif (ru && copy_to_user(ru, &r, sizeof(struct rusage)))\n\t\t\treturn -EFAULT;\n\t}\n\treturn err;\n}\n\n#ifdef __ARCH_WANT_SYS_WAITPID\n\n/*\n * sys_waitpid() remains for compatibility. waitpid() should be\n * implemented by calling sys_wait4() from libc.a.\n */\nSYSCALL_DEFINE3(waitpid, pid_t, pid, int __user *, stat_addr, int, options)\n{\n\treturn sys_wait4(pid, stat_addr, options, NULL);\n}\n\n#endif\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE4(wait4,\n\tcompat_pid_t, pid,\n\tcompat_uint_t __user *, stat_addr,\n\tint, options,\n\tstruct compat_rusage __user *, ru)\n{\n\tstruct rusage r;\n\tlong err = kernel_wait4(pid, stat_addr, options, ru ? &r : NULL);\n\tif (err > 0) {\n\t\tif (ru && put_compat_rusage(&r, ru))\n\t\t\treturn -EFAULT;\n\t}\n\treturn err;\n}\n\nCOMPAT_SYSCALL_DEFINE5(waitid,\n\t\tint, which, compat_pid_t, pid,\n\t\tstruct compat_siginfo __user *, infop, int, options,\n\t\tstruct compat_rusage __user *, uru)\n{\n\tstruct rusage ru;\n\tstruct waitid_info info = {.status = 0};\n\tlong err = kernel_waitid(which, pid, &info, options, uru ? &ru : NULL);\n\tint signo = 0;\n\tif (err > 0) {\n\t\tsigno = SIGCHLD;\n\t\terr = 0;\n\t\tif (uru) {\n\t\t\t/* kernel_waitid() overwrites everything in ru */\n\t\t\tif (COMPAT_USE_64BIT_TIME)\n\t\t\t\terr = copy_to_user(uru, &ru, sizeof(ru));\n\t\t\telse\n\t\t\t\terr = put_compat_rusage(&ru, uru);\n\t\t\tif (err)\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (!infop)\n\t\treturn err;\n\n\tuser_access_begin();\n\tunsafe_put_user(signo, &infop->si_signo, Efault);\n\tunsafe_put_user(0, &infop->si_errno, Efault);\n\tunsafe_put_user(info.cause, &infop->si_code, Efault);\n\tunsafe_put_user(info.pid, &infop->si_pid, Efault);\n\tunsafe_put_user(info.uid, &infop->si_uid, Efault);\n\tunsafe_put_user(info.status, &infop->si_status, Efault);\n\tuser_access_end();\n\treturn err;\nEfault:\n\tuser_access_end();\n\treturn -EFAULT;\n}\n#endif\n"], "filenames": ["kernel/exit.c"], "buggy_code_start_loc": [1602], "buggy_code_end_loc": [1736], "fixing_code_start_loc": [1603], "fixing_code_end_loc": [1733], "type": "CWE-200", "message": "The waitid implementation in kernel/exit.c in the Linux kernel through 4.13.4 accesses rusage data structures in unintended cases, which allows local users to obtain sensitive information, and bypass the KASLR protection mechanism, via a crafted system call.", "other": {"cve": {"id": "CVE-2017-14954", "sourceIdentifier": "cve@mitre.org", "published": "2017-10-02T01:29:00.343", "lastModified": "2017-10-06T14:06:02.363", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The waitid implementation in kernel/exit.c in the Linux kernel through 4.13.4 accesses rusage data structures in unintended cases, which allows local users to obtain sensitive information, and bypass the KASLR protection mechanism, via a crafted system call."}, {"lang": "es", "value": "La implementaci\u00f3n waitid en kernel/exit.c en el kernel de Linux hasta la versi\u00f3n 4.13.4 accede a estructuras de datos rusage en casos que no deber\u00eda, lo que permite a los usuarios locales obtener informaci\u00f3n sensible y omitir el mecanismo de protecci\u00f3n KASLR mediante una llamada al sistema manipulada."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.13.4", "matchCriteriaId": "928AB8DD-8573-4425-803D-1B164491BF77"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=6c85501f2fabcfc4fc6ed976543d252c4eaf4be9", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/6c85501f2fabcfc4fc6ed976543d252c4eaf4be9", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://grsecurity.net/~spender/exploits/wait_for_kaslr_to_be_effective.c", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://twitter.com/_argp/status/914021130712870912", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://twitter.com/grsecurity/status/914079864478666753", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/6c85501f2fabcfc4fc6ed976543d252c4eaf4be9"}}