{"buggy_code": ["/*\n *\tmm/mremap.c\n *\n *\t(C) Copyright 1996 Linus Torvalds\n *\n *\tAddress space accounting code\t<alan@lxorguk.ukuu.org.uk>\n *\t(C) Copyright 2002 Red Hat Inc, All Rights Reserved\n */\n\n#include <linux/mm.h>\n#include <linux/hugetlb.h>\n#include <linux/shm.h>\n#include <linux/ksm.h>\n#include <linux/mman.h>\n#include <linux/swap.h>\n#include <linux/capability.h>\n#include <linux/fs.h>\n#include <linux/highmem.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/mmu_notifier.h>\n\n#include <asm/uaccess.h>\n#include <asm/cacheflush.h>\n#include <asm/tlbflush.h>\n\n#include \"internal.h\"\n\nstatic pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpgd = pgd_offset(mm, addr);\n\tif (pgd_none_or_clear_bad(pgd))\n\t\treturn NULL;\n\n\tpud = pud_offset(pgd, addr);\n\tif (pud_none_or_clear_bad(pud))\n\t\treturn NULL;\n\n\tpmd = pmd_offset(pud, addr);\n\tsplit_huge_page_pmd(mm, pmd);\n\tif (pmd_none_or_clear_bad(pmd))\n\t\treturn NULL;\n\n\treturn pmd;\n}\n\nstatic pmd_t *alloc_new_pmd(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t    unsigned long addr)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpgd = pgd_offset(mm, addr);\n\tpud = pud_alloc(mm, pgd, addr);\n\tif (!pud)\n\t\treturn NULL;\n\n\tpmd = pmd_alloc(mm, pud, addr);\n\tif (!pmd)\n\t\treturn NULL;\n\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\tif (pmd_none(*pmd) && __pte_alloc(mm, vma, pmd, addr))\n\t\treturn NULL;\n\n\treturn pmd;\n}\n\nstatic void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,\n\t\tunsigned long old_addr, unsigned long old_end,\n\t\tstruct vm_area_struct *new_vma, pmd_t *new_pmd,\n\t\tunsigned long new_addr)\n{\n\tstruct address_space *mapping = NULL;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpte_t *old_pte, *new_pte, pte;\n\tspinlock_t *old_ptl, *new_ptl;\n\tunsigned long old_start;\n\n\told_start = old_addr;\n\tmmu_notifier_invalidate_range_start(vma->vm_mm,\n\t\t\t\t\t    old_start, old_end);\n\tif (vma->vm_file) {\n\t\t/*\n\t\t * Subtle point from Rajesh Venkatasubramanian: before\n\t\t * moving file-based ptes, we must lock truncate_pagecache\n\t\t * out, since it might clean the dst vma before the src vma,\n\t\t * and we propagate stale pages into the dst afterward.\n\t\t */\n\t\tmapping = vma->vm_file->f_mapping;\n\t\tspin_lock(&mapping->i_mmap_lock);\n\t\tnew_vma->vm_truncate_count = 0;\n\t}\n\n\t/*\n\t * We don't have to worry about the ordering of src and dst\n\t * pte locks because exclusive mmap_sem prevents deadlock.\n\t */\n\told_pte = pte_offset_map_lock(mm, old_pmd, old_addr, &old_ptl);\n\tnew_pte = pte_offset_map(new_pmd, new_addr);\n\tnew_ptl = pte_lockptr(mm, new_pmd);\n\tif (new_ptl != old_ptl)\n\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\n\tarch_enter_lazy_mmu_mode();\n\n\tfor (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE,\n\t\t\t\t   new_pte++, new_addr += PAGE_SIZE) {\n\t\tif (pte_none(*old_pte))\n\t\t\tcontinue;\n\t\tpte = ptep_clear_flush(vma, old_addr, old_pte);\n\t\tpte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);\n\t\tset_pte_at(mm, new_addr, new_pte, pte);\n\t}\n\n\tarch_leave_lazy_mmu_mode();\n\tif (new_ptl != old_ptl)\n\t\tspin_unlock(new_ptl);\n\tpte_unmap(new_pte - 1);\n\tpte_unmap_unlock(old_pte - 1, old_ptl);\n\tif (mapping)\n\t\tspin_unlock(&mapping->i_mmap_lock);\n\tmmu_notifier_invalidate_range_end(vma->vm_mm, old_start, old_end);\n}\n\n#define LATENCY_LIMIT\t(64 * PAGE_SIZE)\n\nunsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len)\n{\n\tunsigned long extent, next, old_end;\n\tpmd_t *old_pmd, *new_pmd;\n\n\told_end = old_addr + len;\n\tflush_cache_range(vma, old_addr, old_end);\n\n\tfor (; old_addr < old_end; old_addr += extent, new_addr += extent) {\n\t\tcond_resched();\n\t\tnext = (old_addr + PMD_SIZE) & PMD_MASK;\n\t\tif (next - 1 > old_end)\n\t\t\tnext = old_end;\n\t\textent = next - old_addr;\n\t\told_pmd = get_old_pmd(vma->vm_mm, old_addr);\n\t\tif (!old_pmd)\n\t\t\tcontinue;\n\t\tnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pmd)\n\t\t\tbreak;\n\t\tnext = (new_addr + PMD_SIZE) & PMD_MASK;\n\t\tif (extent > next - new_addr)\n\t\t\textent = next - new_addr;\n\t\tif (extent > LATENCY_LIMIT)\n\t\t\textent = LATENCY_LIMIT;\n\t\tmove_ptes(vma, old_pmd, old_addr, old_addr + extent,\n\t\t\t\tnew_vma, new_pmd, new_addr);\n\t}\n\n\treturn len + old_addr - old_end;\t/* how much done */\n}\n\nstatic unsigned long move_vma(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, unsigned long old_len,\n\t\tunsigned long new_len, unsigned long new_addr)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *new_vma;\n\tunsigned long vm_flags = vma->vm_flags;\n\tunsigned long new_pgoff;\n\tunsigned long moved_len;\n\tunsigned long excess = 0;\n\tunsigned long hiwater_vm;\n\tint split = 0;\n\tint err;\n\n\t/*\n\t * We'd prefer to avoid failure later on in do_munmap:\n\t * which may split one vma into three before unmapping.\n\t */\n\tif (mm->map_count >= sysctl_max_map_count - 3)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Advise KSM to break any KSM pages in the area to be moved:\n\t * it would be confusing if they were to turn up at the new\n\t * location, where they happen to coincide with different KSM\n\t * pages recently unmapped.  But leave vma->vm_flags as it was,\n\t * so KSM can come around to merge on vma and new_vma afterwards.\n\t */\n\terr = ksm_madvise(vma, old_addr, old_addr + old_len,\n\t\t\t\t\t\tMADV_UNMERGEABLE, &vm_flags);\n\tif (err)\n\t\treturn err;\n\n\tnew_pgoff = vma->vm_pgoff + ((old_addr - vma->vm_start) >> PAGE_SHIFT);\n\tnew_vma = copy_vma(&vma, new_addr, new_len, new_pgoff);\n\tif (!new_vma)\n\t\treturn -ENOMEM;\n\n\tmoved_len = move_page_tables(vma, old_addr, new_vma, new_addr, old_len);\n\tif (moved_len < old_len) {\n\t\t/*\n\t\t * On error, move entries back from new area to old,\n\t\t * which will succeed since page tables still there,\n\t\t * and then proceed to unmap new area instead of old.\n\t\t */\n\t\tmove_page_tables(new_vma, new_addr, vma, old_addr, moved_len);\n\t\tvma = new_vma;\n\t\told_len = new_len;\n\t\told_addr = new_addr;\n\t\tnew_addr = -ENOMEM;\n\t}\n\n\t/* Conceal VM_ACCOUNT so old reservation is not undone */\n\tif (vm_flags & VM_ACCOUNT) {\n\t\tvma->vm_flags &= ~VM_ACCOUNT;\n\t\texcess = vma->vm_end - vma->vm_start - old_len;\n\t\tif (old_addr > vma->vm_start &&\n\t\t    old_addr + old_len < vma->vm_end)\n\t\t\tsplit = 1;\n\t}\n\n\t/*\n\t * If we failed to move page tables we still do total_vm increment\n\t * since do_munmap() will decrement it by old_len == new_len.\n\t *\n\t * Since total_vm is about to be raised artificially high for a\n\t * moment, we need to restore high watermark afterwards: if stats\n\t * are taken meanwhile, total_vm and hiwater_vm appear too high.\n\t * If this were a serious issue, we'd add a flag to do_munmap().\n\t */\n\thiwater_vm = mm->hiwater_vm;\n\tmm->total_vm += new_len >> PAGE_SHIFT;\n\tvm_stat_account(mm, vma->vm_flags, vma->vm_file, new_len>>PAGE_SHIFT);\n\n\tif (do_munmap(mm, old_addr, old_len) < 0) {\n\t\t/* OOM: unable to split vma, just get accounts right */\n\t\tvm_unacct_memory(excess >> PAGE_SHIFT);\n\t\texcess = 0;\n\t}\n\tmm->hiwater_vm = hiwater_vm;\n\n\t/* Restore VM_ACCOUNT if one or two pieces of vma left */\n\tif (excess) {\n\t\tvma->vm_flags |= VM_ACCOUNT;\n\t\tif (split)\n\t\t\tvma->vm_next->vm_flags |= VM_ACCOUNT;\n\t}\n\n\tif (vm_flags & VM_LOCKED) {\n\t\tmm->locked_vm += new_len >> PAGE_SHIFT;\n\t\tif (new_len > old_len)\n\t\t\tmlock_vma_pages_range(new_vma, new_addr + old_len,\n\t\t\t\t\t\t       new_addr + new_len);\n\t}\n\n\treturn new_addr;\n}\n\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP)) {\n\t\tif (new_len > old_len)\n\t\t\tgoto Efault;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}\n\nstatic int vma_expandable(struct vm_area_struct *vma, unsigned long delta)\n{\n\tunsigned long end = vma->vm_end + delta;\n\tif (end < vma->vm_end) /* overflow */\n\t\treturn 0;\n\tif (vma->vm_next && vma->vm_next->vm_start < end) /* intersection */\n\t\treturn 0;\n\tif (get_unmapped_area(NULL, vma->vm_start, end - vma->vm_start,\n\t\t\t      0, MAP_FIXED) & ~PAGE_MASK)\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * Expand (or shrink) an existing mapping, potentially moving it at the\n * same time (controlled by the MREMAP_MAYMOVE flag and available VM space)\n *\n * MREMAP_FIXED option added 5-Dec-1999 by Benjamin LaHaise\n * This option implies MREMAP_MAYMOVE.\n */\nunsigned long do_mremap(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len,\n\tunsigned long flags, unsigned long new_addr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\n\tif (flags & ~(MREMAP_FIXED | MREMAP_MAYMOVE))\n\t\tgoto out;\n\n\tif (addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\told_len = PAGE_ALIGN(old_len);\n\tnew_len = PAGE_ALIGN(new_len);\n\n\t/*\n\t * We allow a zero old-len as a special case\n\t * for DOS-emu \"duplicate shm area\" thing. But\n\t * a zero new-len is nonsensical.\n\t */\n\tif (!new_len)\n\t\tgoto out;\n\n\tif (flags & MREMAP_FIXED) {\n\t\tif (flags & MREMAP_MAYMOVE)\n\t\t\tret = mremap_to(addr, old_len, new_addr, new_len);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Always allow a shrinking remap: that just unmaps\n\t * the unnecessary pages..\n\t * do_munmap does all the needed commit accounting\n\t */\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\tret = addr;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Ok, we need to grow..\n\t */\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\t/* old_len exactly to the end of the area..\n\t */\n\tif (old_len == vma->vm_end - addr) {\n\t\t/* can we just expand the current mapping? */\n\t\tif (vma_expandable(vma, new_len - old_len)) {\n\t\t\tint pages = (new_len - old_len) >> PAGE_SHIFT;\n\n\t\t\tif (vma_adjust(vma, vma->vm_start, addr + new_len,\n\t\t\t\t       vma->vm_pgoff, NULL)) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmm->total_vm += pages;\n\t\t\tvm_stat_account(mm, vma->vm_flags, vma->vm_file, pages);\n\t\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm += pages;\n\t\t\t\tmlock_vma_pages_range(vma, addr + old_len,\n\t\t\t\t\t\t   addr + new_len);\n\t\t\t}\n\t\t\tret = addr;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * We weren't able to just expand or shrink the area,\n\t * we need to create a new one and move it..\n\t */\n\tret = -ENOMEM;\n\tif (flags & MREMAP_MAYMOVE) {\n\t\tunsigned long map_flags = 0;\n\t\tif (vma->vm_flags & VM_MAYSHARE)\n\t\t\tmap_flags |= MAP_SHARED;\n\n\t\tnew_addr = get_unmapped_area(vma->vm_file, 0, new_len,\n\t\t\t\t\tvma->vm_pgoff +\n\t\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\t\tmap_flags);\n\t\tif (new_addr & ~PAGE_MASK) {\n\t\t\tret = new_addr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\t}\nout:\n\tif (ret & ~PAGE_MASK)\n\t\tvm_unacct_memory(charged);\n\treturn ret;\n}\n\nSYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,\n\t\tunsigned long, new_len, unsigned long, flags,\n\t\tunsigned long, new_addr)\n{\n\tunsigned long ret;\n\n\tdown_write(&current->mm->mmap_sem);\n\tret = do_mremap(addr, old_len, new_len, flags, new_addr);\n\tup_write(&current->mm->mmap_sem);\n\treturn ret;\n}\n"], "fixing_code": ["/*\n *\tmm/mremap.c\n *\n *\t(C) Copyright 1996 Linus Torvalds\n *\n *\tAddress space accounting code\t<alan@lxorguk.ukuu.org.uk>\n *\t(C) Copyright 2002 Red Hat Inc, All Rights Reserved\n */\n\n#include <linux/mm.h>\n#include <linux/hugetlb.h>\n#include <linux/shm.h>\n#include <linux/ksm.h>\n#include <linux/mman.h>\n#include <linux/swap.h>\n#include <linux/capability.h>\n#include <linux/fs.h>\n#include <linux/highmem.h>\n#include <linux/security.h>\n#include <linux/syscalls.h>\n#include <linux/mmu_notifier.h>\n\n#include <asm/uaccess.h>\n#include <asm/cacheflush.h>\n#include <asm/tlbflush.h>\n\n#include \"internal.h\"\n\nstatic pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpgd = pgd_offset(mm, addr);\n\tif (pgd_none_or_clear_bad(pgd))\n\t\treturn NULL;\n\n\tpud = pud_offset(pgd, addr);\n\tif (pud_none_or_clear_bad(pud))\n\t\treturn NULL;\n\n\tpmd = pmd_offset(pud, addr);\n\tsplit_huge_page_pmd(mm, pmd);\n\tif (pmd_none_or_clear_bad(pmd))\n\t\treturn NULL;\n\n\treturn pmd;\n}\n\nstatic pmd_t *alloc_new_pmd(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t    unsigned long addr)\n{\n\tpgd_t *pgd;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\tpgd = pgd_offset(mm, addr);\n\tpud = pud_alloc(mm, pgd, addr);\n\tif (!pud)\n\t\treturn NULL;\n\n\tpmd = pmd_alloc(mm, pud, addr);\n\tif (!pmd)\n\t\treturn NULL;\n\n\tVM_BUG_ON(pmd_trans_huge(*pmd));\n\tif (pmd_none(*pmd) && __pte_alloc(mm, vma, pmd, addr))\n\t\treturn NULL;\n\n\treturn pmd;\n}\n\nstatic void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,\n\t\tunsigned long old_addr, unsigned long old_end,\n\t\tstruct vm_area_struct *new_vma, pmd_t *new_pmd,\n\t\tunsigned long new_addr)\n{\n\tstruct address_space *mapping = NULL;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tpte_t *old_pte, *new_pte, pte;\n\tspinlock_t *old_ptl, *new_ptl;\n\tunsigned long old_start;\n\n\told_start = old_addr;\n\tmmu_notifier_invalidate_range_start(vma->vm_mm,\n\t\t\t\t\t    old_start, old_end);\n\tif (vma->vm_file) {\n\t\t/*\n\t\t * Subtle point from Rajesh Venkatasubramanian: before\n\t\t * moving file-based ptes, we must lock truncate_pagecache\n\t\t * out, since it might clean the dst vma before the src vma,\n\t\t * and we propagate stale pages into the dst afterward.\n\t\t */\n\t\tmapping = vma->vm_file->f_mapping;\n\t\tspin_lock(&mapping->i_mmap_lock);\n\t\tnew_vma->vm_truncate_count = 0;\n\t}\n\n\t/*\n\t * We don't have to worry about the ordering of src and dst\n\t * pte locks because exclusive mmap_sem prevents deadlock.\n\t */\n\told_pte = pte_offset_map_lock(mm, old_pmd, old_addr, &old_ptl);\n\tnew_pte = pte_offset_map(new_pmd, new_addr);\n\tnew_ptl = pte_lockptr(mm, new_pmd);\n\tif (new_ptl != old_ptl)\n\t\tspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\n\tarch_enter_lazy_mmu_mode();\n\n\tfor (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE,\n\t\t\t\t   new_pte++, new_addr += PAGE_SIZE) {\n\t\tif (pte_none(*old_pte))\n\t\t\tcontinue;\n\t\tpte = ptep_clear_flush(vma, old_addr, old_pte);\n\t\tpte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);\n\t\tset_pte_at(mm, new_addr, new_pte, pte);\n\t}\n\n\tarch_leave_lazy_mmu_mode();\n\tif (new_ptl != old_ptl)\n\t\tspin_unlock(new_ptl);\n\tpte_unmap(new_pte - 1);\n\tpte_unmap_unlock(old_pte - 1, old_ptl);\n\tif (mapping)\n\t\tspin_unlock(&mapping->i_mmap_lock);\n\tmmu_notifier_invalidate_range_end(vma->vm_mm, old_start, old_end);\n}\n\n#define LATENCY_LIMIT\t(64 * PAGE_SIZE)\n\nunsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len)\n{\n\tunsigned long extent, next, old_end;\n\tpmd_t *old_pmd, *new_pmd;\n\n\told_end = old_addr + len;\n\tflush_cache_range(vma, old_addr, old_end);\n\n\tfor (; old_addr < old_end; old_addr += extent, new_addr += extent) {\n\t\tcond_resched();\n\t\tnext = (old_addr + PMD_SIZE) & PMD_MASK;\n\t\tif (next - 1 > old_end)\n\t\t\tnext = old_end;\n\t\textent = next - old_addr;\n\t\told_pmd = get_old_pmd(vma->vm_mm, old_addr);\n\t\tif (!old_pmd)\n\t\t\tcontinue;\n\t\tnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pmd)\n\t\t\tbreak;\n\t\tnext = (new_addr + PMD_SIZE) & PMD_MASK;\n\t\tif (extent > next - new_addr)\n\t\t\textent = next - new_addr;\n\t\tif (extent > LATENCY_LIMIT)\n\t\t\textent = LATENCY_LIMIT;\n\t\tmove_ptes(vma, old_pmd, old_addr, old_addr + extent,\n\t\t\t\tnew_vma, new_pmd, new_addr);\n\t}\n\n\treturn len + old_addr - old_end;\t/* how much done */\n}\n\nstatic unsigned long move_vma(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, unsigned long old_len,\n\t\tunsigned long new_len, unsigned long new_addr)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct vm_area_struct *new_vma;\n\tunsigned long vm_flags = vma->vm_flags;\n\tunsigned long new_pgoff;\n\tunsigned long moved_len;\n\tunsigned long excess = 0;\n\tunsigned long hiwater_vm;\n\tint split = 0;\n\tint err;\n\n\t/*\n\t * We'd prefer to avoid failure later on in do_munmap:\n\t * which may split one vma into three before unmapping.\n\t */\n\tif (mm->map_count >= sysctl_max_map_count - 3)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Advise KSM to break any KSM pages in the area to be moved:\n\t * it would be confusing if they were to turn up at the new\n\t * location, where they happen to coincide with different KSM\n\t * pages recently unmapped.  But leave vma->vm_flags as it was,\n\t * so KSM can come around to merge on vma and new_vma afterwards.\n\t */\n\terr = ksm_madvise(vma, old_addr, old_addr + old_len,\n\t\t\t\t\t\tMADV_UNMERGEABLE, &vm_flags);\n\tif (err)\n\t\treturn err;\n\n\tnew_pgoff = vma->vm_pgoff + ((old_addr - vma->vm_start) >> PAGE_SHIFT);\n\tnew_vma = copy_vma(&vma, new_addr, new_len, new_pgoff);\n\tif (!new_vma)\n\t\treturn -ENOMEM;\n\n\tmoved_len = move_page_tables(vma, old_addr, new_vma, new_addr, old_len);\n\tif (moved_len < old_len) {\n\t\t/*\n\t\t * On error, move entries back from new area to old,\n\t\t * which will succeed since page tables still there,\n\t\t * and then proceed to unmap new area instead of old.\n\t\t */\n\t\tmove_page_tables(new_vma, new_addr, vma, old_addr, moved_len);\n\t\tvma = new_vma;\n\t\told_len = new_len;\n\t\told_addr = new_addr;\n\t\tnew_addr = -ENOMEM;\n\t}\n\n\t/* Conceal VM_ACCOUNT so old reservation is not undone */\n\tif (vm_flags & VM_ACCOUNT) {\n\t\tvma->vm_flags &= ~VM_ACCOUNT;\n\t\texcess = vma->vm_end - vma->vm_start - old_len;\n\t\tif (old_addr > vma->vm_start &&\n\t\t    old_addr + old_len < vma->vm_end)\n\t\t\tsplit = 1;\n\t}\n\n\t/*\n\t * If we failed to move page tables we still do total_vm increment\n\t * since do_munmap() will decrement it by old_len == new_len.\n\t *\n\t * Since total_vm is about to be raised artificially high for a\n\t * moment, we need to restore high watermark afterwards: if stats\n\t * are taken meanwhile, total_vm and hiwater_vm appear too high.\n\t * If this were a serious issue, we'd add a flag to do_munmap().\n\t */\n\thiwater_vm = mm->hiwater_vm;\n\tmm->total_vm += new_len >> PAGE_SHIFT;\n\tvm_stat_account(mm, vma->vm_flags, vma->vm_file, new_len>>PAGE_SHIFT);\n\n\tif (do_munmap(mm, old_addr, old_len) < 0) {\n\t\t/* OOM: unable to split vma, just get accounts right */\n\t\tvm_unacct_memory(excess >> PAGE_SHIFT);\n\t\texcess = 0;\n\t}\n\tmm->hiwater_vm = hiwater_vm;\n\n\t/* Restore VM_ACCOUNT if one or two pieces of vma left */\n\tif (excess) {\n\t\tvma->vm_flags |= VM_ACCOUNT;\n\t\tif (split)\n\t\t\tvma->vm_next->vm_flags |= VM_ACCOUNT;\n\t}\n\n\tif (vm_flags & VM_LOCKED) {\n\t\tmm->locked_vm += new_len >> PAGE_SHIFT;\n\t\tif (new_len > old_len)\n\t\t\tmlock_vma_pages_range(new_vma, new_addr + old_len,\n\t\t\t\t\t\t       new_addr + new_len);\n\t}\n\n\treturn new_addr;\n}\n\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len, unsigned long *p)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma = find_vma(mm, addr);\n\n\tif (!vma || vma->vm_start > addr)\n\t\tgoto Efault;\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tgoto Einval;\n\n\t/* We can't remap across vm area boundaries */\n\tif (old_len > vma->vm_end - addr)\n\t\tgoto Efault;\n\n\t/* Need to be careful about a growing mapping */\n\tif (new_len > old_len) {\n\t\tunsigned long pgoff;\n\n\t\tif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\n\t\t\tgoto Efault;\n\t\tpgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\n\t\tpgoff += vma->vm_pgoff;\n\t\tif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\n\t\t\tgoto Einval;\n\t}\n\n\tif (vma->vm_flags & VM_LOCKED) {\n\t\tunsigned long locked, lock_limit;\n\t\tlocked = mm->locked_vm << PAGE_SHIFT;\n\t\tlock_limit = rlimit(RLIMIT_MEMLOCK);\n\t\tlocked += new_len - old_len;\n\t\tif (locked > lock_limit && !capable(CAP_IPC_LOCK))\n\t\t\tgoto Eagain;\n\t}\n\n\tif (!may_expand_vm(mm, (new_len - old_len) >> PAGE_SHIFT))\n\t\tgoto Enomem;\n\n\tif (vma->vm_flags & VM_ACCOUNT) {\n\t\tunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\n\t\tif (security_vm_enough_memory(charged))\n\t\t\tgoto Efault;\n\t\t*p = charged;\n\t}\n\n\treturn vma;\n\nEfault:\t/* very odd choice for most of the cases, but... */\n\treturn ERR_PTR(-EFAULT);\nEinval:\n\treturn ERR_PTR(-EINVAL);\nEnomem:\n\treturn ERR_PTR(-ENOMEM);\nEagain:\n\treturn ERR_PTR(-EAGAIN);\n}\n\nstatic unsigned long mremap_to(unsigned long addr,\n\tunsigned long old_len, unsigned long new_addr,\n\tunsigned long new_len)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tunsigned long map_flags;\n\n\tif (new_addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\tif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\n\t\tgoto out;\n\n\t/* Check if the location we're moving into overlaps the\n\t * old location at all, and fail if it does.\n\t */\n\tif ((new_addr <= addr) && (new_addr+new_len) > addr)\n\t\tgoto out;\n\n\tif ((addr <= new_addr) && (addr+old_len) > new_addr)\n\t\tgoto out;\n\n\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = do_munmap(mm, new_addr, new_len);\n\tif (ret)\n\t\tgoto out;\n\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\told_len = new_len;\n\t}\n\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\tmap_flags = MAP_FIXED;\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\tmap_flags |= MAP_SHARED;\n\n\tret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\n\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\tmap_flags);\n\tif (ret & ~PAGE_MASK)\n\t\tgoto out1;\n\n\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\tif (!(ret & ~PAGE_MASK))\n\t\tgoto out;\nout1:\n\tvm_unacct_memory(charged);\n\nout:\n\treturn ret;\n}\n\nstatic int vma_expandable(struct vm_area_struct *vma, unsigned long delta)\n{\n\tunsigned long end = vma->vm_end + delta;\n\tif (end < vma->vm_end) /* overflow */\n\t\treturn 0;\n\tif (vma->vm_next && vma->vm_next->vm_start < end) /* intersection */\n\t\treturn 0;\n\tif (get_unmapped_area(NULL, vma->vm_start, end - vma->vm_start,\n\t\t\t      0, MAP_FIXED) & ~PAGE_MASK)\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * Expand (or shrink) an existing mapping, potentially moving it at the\n * same time (controlled by the MREMAP_MAYMOVE flag and available VM space)\n *\n * MREMAP_FIXED option added 5-Dec-1999 by Benjamin LaHaise\n * This option implies MREMAP_MAYMOVE.\n */\nunsigned long do_mremap(unsigned long addr,\n\tunsigned long old_len, unsigned long new_len,\n\tunsigned long flags, unsigned long new_addr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\n\tif (flags & ~(MREMAP_FIXED | MREMAP_MAYMOVE))\n\t\tgoto out;\n\n\tif (addr & ~PAGE_MASK)\n\t\tgoto out;\n\n\told_len = PAGE_ALIGN(old_len);\n\tnew_len = PAGE_ALIGN(new_len);\n\n\t/*\n\t * We allow a zero old-len as a special case\n\t * for DOS-emu \"duplicate shm area\" thing. But\n\t * a zero new-len is nonsensical.\n\t */\n\tif (!new_len)\n\t\tgoto out;\n\n\tif (flags & MREMAP_FIXED) {\n\t\tif (flags & MREMAP_MAYMOVE)\n\t\t\tret = mremap_to(addr, old_len, new_addr, new_len);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Always allow a shrinking remap: that just unmaps\n\t * the unnecessary pages..\n\t * do_munmap does all the needed commit accounting\n\t */\n\tif (old_len >= new_len) {\n\t\tret = do_munmap(mm, addr+new_len, old_len - new_len);\n\t\tif (ret && old_len != new_len)\n\t\t\tgoto out;\n\t\tret = addr;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Ok, we need to grow..\n\t */\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\t/* old_len exactly to the end of the area..\n\t */\n\tif (old_len == vma->vm_end - addr) {\n\t\t/* can we just expand the current mapping? */\n\t\tif (vma_expandable(vma, new_len - old_len)) {\n\t\t\tint pages = (new_len - old_len) >> PAGE_SHIFT;\n\n\t\t\tif (vma_adjust(vma, vma->vm_start, addr + new_len,\n\t\t\t\t       vma->vm_pgoff, NULL)) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmm->total_vm += pages;\n\t\t\tvm_stat_account(mm, vma->vm_flags, vma->vm_file, pages);\n\t\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm += pages;\n\t\t\t\tmlock_vma_pages_range(vma, addr + old_len,\n\t\t\t\t\t\t   addr + new_len);\n\t\t\t}\n\t\t\tret = addr;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * We weren't able to just expand or shrink the area,\n\t * we need to create a new one and move it..\n\t */\n\tret = -ENOMEM;\n\tif (flags & MREMAP_MAYMOVE) {\n\t\tunsigned long map_flags = 0;\n\t\tif (vma->vm_flags & VM_MAYSHARE)\n\t\t\tmap_flags |= MAP_SHARED;\n\n\t\tnew_addr = get_unmapped_area(vma->vm_file, 0, new_len,\n\t\t\t\t\tvma->vm_pgoff +\n\t\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\t\tmap_flags);\n\t\tif (new_addr & ~PAGE_MASK) {\n\t\t\tret = new_addr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = security_file_mmap(NULL, 0, 0, 0, new_addr, 1);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tret = move_vma(vma, addr, old_len, new_len, new_addr);\n\t}\nout:\n\tif (ret & ~PAGE_MASK)\n\t\tvm_unacct_memory(charged);\n\treturn ret;\n}\n\nSYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,\n\t\tunsigned long, new_len, unsigned long, flags,\n\t\tunsigned long, new_addr)\n{\n\tunsigned long ret;\n\n\tdown_write(&current->mm->mmap_sem);\n\tret = do_mremap(addr, old_len, new_len, flags, new_addr);\n\tup_write(&current->mm->mmap_sem);\n\treturn ret;\n}\n"], "filenames": ["mm/mremap.c"], "buggy_code_start_loc": [280], "buggy_code_end_loc": [282], "fixing_code_start_loc": [280], "fixing_code_end_loc": [290], "type": "CWE-189", "message": "Integer overflow in the vma_to_resize function in mm/mremap.c in the Linux kernel before 2.6.39 allows local users to cause a denial of service (BUG_ON and system crash) via a crafted mremap system call that expands a memory mapping.", "other": {"cve": {"id": "CVE-2011-2496", "sourceIdentifier": "secalert@redhat.com", "published": "2012-06-13T10:24:55.873", "lastModified": "2023-02-13T01:19:54.693", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Integer overflow in the vma_to_resize function in mm/mremap.c in the Linux kernel before 2.6.39 allows local users to cause a denial of service (BUG_ON and system crash) via a crafted mremap system call that expands a memory mapping."}, {"lang": "es", "value": "Desbordamiento de entero en la funci\u00f3n vma_to_resize de mm/mremap.c del kernel de Linux en versiones anteriores a la 2.6.39. Permite a usuarios locales provocar una denegaci\u00f3n de servicio (BUG_ON y ca\u00edda del sistema) a trav\u00e9s de una llamada del sistema  mremap modificada que expande el mapeo de memoria (\"memory mapping\")."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-189"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.6.38.8", "matchCriteriaId": "57A0A2B0-3B9F-40C2-8C7A-CD9590B51315"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38:*:*:*:*:*:*:*", "matchCriteriaId": "7462DB6D-E0A6-4DBB-8E21-66B875184FFC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38:rc1:*:*:*:*:*:*", "matchCriteriaId": "2DDCB342-4F5F-4BF1-9624-882BBC57330D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38:rc2:*:*:*:*:*:*", "matchCriteriaId": "C3AB4113-BF83-4587-8A85-0E4FECEE7D9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38:rc3:*:*:*:*:*:*", "matchCriteriaId": "4B57F5AD-A697-4090-89B9-81BC12993A1A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38:rc4:*:*:*:*:*:*", "matchCriteriaId": "CA141BCB-A705-4DF5-9EED-746B62C86111"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38:rc5:*:*:*:*:*:*", "matchCriteriaId": "E9ECE134-58A3-4B9D-B9B3-F836C0EDD64C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38:rc6:*:*:*:*:*:*", "matchCriteriaId": "56186720-6B4C-4D71-85C5-7EAC5C5D84A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38:rc7:*:*:*:*:*:*", "matchCriteriaId": "9BBB4630-CBED-43B9-B203-BE65BBF011AA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38:rc8:*:*:*:*:*:*", "matchCriteriaId": "FD375A78-63D7-441A-9FB0-7BC878AB4EDD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38.1:*:*:*:*:*:*:*", "matchCriteriaId": "A5BEFFDD-02BB-4A05-8372-891DBDB9AC5A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38.2:*:*:*:*:*:*:*", "matchCriteriaId": "766E193D-819C-42EA-8411-AE0013AC15FA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38.3:*:*:*:*:*:*:*", "matchCriteriaId": "3B39B6AF-6A83-48C2-BED2-79228F8513A6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38.4:*:*:*:*:*:*:*", "matchCriteriaId": "CD8A68D1-DFE9-4ADB-9FB8-4D69AB4CAFF8"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38.5:*:*:*:*:*:*:*", "matchCriteriaId": "0D6EF951-AF15-4C30-A3A5-3392AA61813C"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38.6:*:*:*:*:*:*:*", "matchCriteriaId": "15154FA0-65DC-4855-AC70-3ACF92313F49"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.38.7:*:*:*:*:*:*:*", "matchCriteriaId": "F4B3A9F4-A61F-4919-A173-3E459F0C5AF8"}]}]}], "references": [{"url": "http://ftp.osuosl.org/pub/linux/kernel/v2.6/ChangeLog-2.6.39", "source": "secalert@redhat.com"}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=982134ba62618c2d69fbbbd166d0a11ee3b7e3d8", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2011/06/27/2", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=716538", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/982134ba62618c2d69fbbbd166d0a11ee3b7e3d8", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/982134ba62618c2d69fbbbd166d0a11ee3b7e3d8"}}