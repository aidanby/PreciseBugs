{"buggy_code": ["/*\n * Generic ring buffer\n *\n * Copyright (C) 2008 Steven Rostedt <srostedt@redhat.com>\n */\n#include <linux/trace_events.h>\n#include <linux/ring_buffer.h>\n#include <linux/trace_clock.h>\n#include <linux/trace_seq.h>\n#include <linux/spinlock.h>\n#include <linux/irq_work.h>\n#include <linux/uaccess.h>\n#include <linux/hardirq.h>\n#include <linux/kthread.h>\t/* for self test */\n#include <linux/kmemcheck.h>\n#include <linux/module.h>\n#include <linux/percpu.h>\n#include <linux/mutex.h>\n#include <linux/delay.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/cpu.h>\n\n#include <asm/local.h>\n\nstatic void update_pages_handler(struct work_struct *work);\n\n/*\n * The ring buffer header is special. We must manually up keep it.\n */\nint ring_buffer_print_entry_header(struct trace_seq *s)\n{\n\ttrace_seq_puts(s, \"# compressed entry header\\n\");\n\ttrace_seq_puts(s, \"\\ttype_len    :    5 bits\\n\");\n\ttrace_seq_puts(s, \"\\ttime_delta  :   27 bits\\n\");\n\ttrace_seq_puts(s, \"\\tarray       :   32 bits\\n\");\n\ttrace_seq_putc(s, '\\n');\n\ttrace_seq_printf(s, \"\\tpadding     : type == %d\\n\",\n\t\t\t RINGBUF_TYPE_PADDING);\n\ttrace_seq_printf(s, \"\\ttime_extend : type == %d\\n\",\n\t\t\t RINGBUF_TYPE_TIME_EXTEND);\n\ttrace_seq_printf(s, \"\\tdata max type_len  == %d\\n\",\n\t\t\t RINGBUF_TYPE_DATA_TYPE_LEN_MAX);\n\n\treturn !trace_seq_has_overflowed(s);\n}\n\n/*\n * The ring buffer is made up of a list of pages. A separate list of pages is\n * allocated for each CPU. A writer may only write to a buffer that is\n * associated with the CPU it is currently executing on.  A reader may read\n * from any per cpu buffer.\n *\n * The reader is special. For each per cpu buffer, the reader has its own\n * reader page. When a reader has read the entire reader page, this reader\n * page is swapped with another page in the ring buffer.\n *\n * Now, as long as the writer is off the reader page, the reader can do what\n * ever it wants with that page. The writer will never write to that page\n * again (as long as it is out of the ring buffer).\n *\n * Here's some silly ASCII art.\n *\n *   +------+\n *   |reader|          RING BUFFER\n *   |page  |\n *   +------+        +---+   +---+   +---+\n *                   |   |-->|   |-->|   |\n *                   +---+   +---+   +---+\n *                     ^               |\n *                     |               |\n *                     +---------------+\n *\n *\n *   +------+\n *   |reader|          RING BUFFER\n *   |page  |------------------v\n *   +------+        +---+   +---+   +---+\n *                   |   |-->|   |-->|   |\n *                   +---+   +---+   +---+\n *                     ^               |\n *                     |               |\n *                     +---------------+\n *\n *\n *   +------+\n *   |reader|          RING BUFFER\n *   |page  |------------------v\n *   +------+        +---+   +---+   +---+\n *      ^            |   |-->|   |-->|   |\n *      |            +---+   +---+   +---+\n *      |                              |\n *      |                              |\n *      +------------------------------+\n *\n *\n *   +------+\n *   |buffer|          RING BUFFER\n *   |page  |------------------v\n *   +------+        +---+   +---+   +---+\n *      ^            |   |   |   |-->|   |\n *      |   New      +---+   +---+   +---+\n *      |  Reader------^               |\n *      |   page                       |\n *      +------------------------------+\n *\n *\n * After we make this swap, the reader can hand this page off to the splice\n * code and be done with it. It can even allocate a new page if it needs to\n * and swap that into the ring buffer.\n *\n * We will be using cmpxchg soon to make all this lockless.\n *\n */\n\n/* Used for individual buffers (after the counter) */\n#define RB_BUFFER_OFF\t\t(1 << 20)\n\n#define BUF_PAGE_HDR_SIZE offsetof(struct buffer_data_page, data)\n\n#define RB_EVNT_HDR_SIZE (offsetof(struct ring_buffer_event, array))\n#define RB_ALIGNMENT\t\t4U\n#define RB_MAX_SMALL_DATA\t(RB_ALIGNMENT * RINGBUF_TYPE_DATA_TYPE_LEN_MAX)\n#define RB_EVNT_MIN_SIZE\t8U\t/* two 32bit words */\n\n#ifndef CONFIG_HAVE_64BIT_ALIGNED_ACCESS\n# define RB_FORCE_8BYTE_ALIGNMENT\t0\n# define RB_ARCH_ALIGNMENT\t\tRB_ALIGNMENT\n#else\n# define RB_FORCE_8BYTE_ALIGNMENT\t1\n# define RB_ARCH_ALIGNMENT\t\t8U\n#endif\n\n#define RB_ALIGN_DATA\t\t__aligned(RB_ARCH_ALIGNMENT)\n\n/* define RINGBUF_TYPE_DATA for 'case RINGBUF_TYPE_DATA:' */\n#define RINGBUF_TYPE_DATA 0 ... RINGBUF_TYPE_DATA_TYPE_LEN_MAX\n\nenum {\n\tRB_LEN_TIME_EXTEND = 8,\n\tRB_LEN_TIME_STAMP = 16,\n};\n\n#define skip_time_extend(event) \\\n\t((struct ring_buffer_event *)((char *)event + RB_LEN_TIME_EXTEND))\n\nstatic inline int rb_null_event(struct ring_buffer_event *event)\n{\n\treturn event->type_len == RINGBUF_TYPE_PADDING && !event->time_delta;\n}\n\nstatic void rb_event_set_padding(struct ring_buffer_event *event)\n{\n\t/* padding has a NULL time_delta */\n\tevent->type_len = RINGBUF_TYPE_PADDING;\n\tevent->time_delta = 0;\n}\n\nstatic unsigned\nrb_event_data_length(struct ring_buffer_event *event)\n{\n\tunsigned length;\n\n\tif (event->type_len)\n\t\tlength = event->type_len * RB_ALIGNMENT;\n\telse\n\t\tlength = event->array[0];\n\treturn length + RB_EVNT_HDR_SIZE;\n}\n\n/*\n * Return the length of the given event. Will return\n * the length of the time extend if the event is a\n * time extend.\n */\nstatic inline unsigned\nrb_event_length(struct ring_buffer_event *event)\n{\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\tif (rb_null_event(event))\n\t\t\t/* undefined */\n\t\t\treturn -1;\n\t\treturn  event->array[0] + RB_EVNT_HDR_SIZE;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\treturn RB_LEN_TIME_EXTEND;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\treturn RB_LEN_TIME_STAMP;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\treturn rb_event_data_length(event);\n\tdefault:\n\t\tBUG();\n\t}\n\t/* not hit */\n\treturn 0;\n}\n\n/*\n * Return total length of time extend and data,\n *   or just the event length for all other events.\n */\nstatic inline unsigned\nrb_event_ts_length(struct ring_buffer_event *event)\n{\n\tunsigned len = 0;\n\n\tif (event->type_len == RINGBUF_TYPE_TIME_EXTEND) {\n\t\t/* time extends include the data event after it */\n\t\tlen = RB_LEN_TIME_EXTEND;\n\t\tevent = skip_time_extend(event);\n\t}\n\treturn len + rb_event_length(event);\n}\n\n/**\n * ring_buffer_event_length - return the length of the event\n * @event: the event to get the length of\n *\n * Returns the size of the data load of a data event.\n * If the event is something other than a data event, it\n * returns the size of the event itself. With the exception\n * of a TIME EXTEND, where it still returns the size of the\n * data load of the data event after it.\n */\nunsigned ring_buffer_event_length(struct ring_buffer_event *event)\n{\n\tunsigned length;\n\n\tif (event->type_len == RINGBUF_TYPE_TIME_EXTEND)\n\t\tevent = skip_time_extend(event);\n\n\tlength = rb_event_length(event);\n\tif (event->type_len > RINGBUF_TYPE_DATA_TYPE_LEN_MAX)\n\t\treturn length;\n\tlength -= RB_EVNT_HDR_SIZE;\n\tif (length > RB_MAX_SMALL_DATA + sizeof(event->array[0]))\n                length -= sizeof(event->array[0]);\n\treturn length;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_event_length);\n\n/* inline for ring buffer fast paths */\nstatic void *\nrb_event_data(struct ring_buffer_event *event)\n{\n\tif (event->type_len == RINGBUF_TYPE_TIME_EXTEND)\n\t\tevent = skip_time_extend(event);\n\tBUG_ON(event->type_len > RINGBUF_TYPE_DATA_TYPE_LEN_MAX);\n\t/* If length is in len field, then array[0] has the data */\n\tif (event->type_len)\n\t\treturn (void *)&event->array[0];\n\t/* Otherwise length is in array[0] and array[1] has the data */\n\treturn (void *)&event->array[1];\n}\n\n/**\n * ring_buffer_event_data - return the data of the event\n * @event: the event to get the data from\n */\nvoid *ring_buffer_event_data(struct ring_buffer_event *event)\n{\n\treturn rb_event_data(event);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_event_data);\n\n#define for_each_buffer_cpu(buffer, cpu)\t\t\\\n\tfor_each_cpu(cpu, buffer->cpumask)\n\n#define TS_SHIFT\t27\n#define TS_MASK\t\t((1ULL << TS_SHIFT) - 1)\n#define TS_DELTA_TEST\t(~TS_MASK)\n\n/* Flag when events were overwritten */\n#define RB_MISSED_EVENTS\t(1 << 31)\n/* Missed count stored at end */\n#define RB_MISSED_STORED\t(1 << 30)\n\nstruct buffer_data_page {\n\tu64\t\t time_stamp;\t/* page time stamp */\n\tlocal_t\t\t commit;\t/* write committed index */\n\tunsigned char\t data[] RB_ALIGN_DATA;\t/* data of buffer page */\n};\n\n/*\n * Note, the buffer_page list must be first. The buffer pages\n * are allocated in cache lines, which means that each buffer\n * page will be at the beginning of a cache line, and thus\n * the least significant bits will be zero. We use this to\n * add flags in the list struct pointers, to make the ring buffer\n * lockless.\n */\nstruct buffer_page {\n\tstruct list_head list;\t\t/* list of buffer pages */\n\tlocal_t\t\t write;\t\t/* index for next write */\n\tunsigned\t read;\t\t/* index for next read */\n\tlocal_t\t\t entries;\t/* entries on this page */\n\tunsigned long\t real_end;\t/* real end of data */\n\tstruct buffer_data_page *page;\t/* Actual data page */\n};\n\n/*\n * The buffer page counters, write and entries, must be reset\n * atomically when crossing page boundaries. To synchronize this\n * update, two counters are inserted into the number. One is\n * the actual counter for the write position or count on the page.\n *\n * The other is a counter of updaters. Before an update happens\n * the update partition of the counter is incremented. This will\n * allow the updater to update the counter atomically.\n *\n * The counter is 20 bits, and the state data is 12.\n */\n#define RB_WRITE_MASK\t\t0xfffff\n#define RB_WRITE_INTCNT\t\t(1 << 20)\n\nstatic void rb_init_page(struct buffer_data_page *bpage)\n{\n\tlocal_set(&bpage->commit, 0);\n}\n\n/**\n * ring_buffer_page_len - the size of data on the page.\n * @page: The page to read\n *\n * Returns the amount of data on the page, including buffer page header.\n */\nsize_t ring_buffer_page_len(void *page)\n{\n\treturn local_read(&((struct buffer_data_page *)page)->commit)\n\t\t+ BUF_PAGE_HDR_SIZE;\n}\n\n/*\n * Also stolen from mm/slob.c. Thanks to Mathieu Desnoyers for pointing\n * this issue out.\n */\nstatic void free_buffer_page(struct buffer_page *bpage)\n{\n\tfree_page((unsigned long)bpage->page);\n\tkfree(bpage);\n}\n\n/*\n * We need to fit the time_stamp delta into 27 bits.\n */\nstatic inline int test_time_stamp(u64 delta)\n{\n\tif (delta & TS_DELTA_TEST)\n\t\treturn 1;\n\treturn 0;\n}\n\n#define BUF_PAGE_SIZE (PAGE_SIZE - BUF_PAGE_HDR_SIZE)\n\n/* Max payload is BUF_PAGE_SIZE - header (8bytes) */\n#define BUF_MAX_DATA_SIZE (BUF_PAGE_SIZE - (sizeof(u32) * 2))\n\nint ring_buffer_print_page_header(struct trace_seq *s)\n{\n\tstruct buffer_data_page field;\n\n\ttrace_seq_printf(s, \"\\tfield: u64 timestamp;\\t\"\n\t\t\t \"offset:0;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)sizeof(field.time_stamp),\n\t\t\t (unsigned int)is_signed_type(u64));\n\n\ttrace_seq_printf(s, \"\\tfield: local_t commit;\\t\"\n\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)offsetof(typeof(field), commit),\n\t\t\t (unsigned int)sizeof(field.commit),\n\t\t\t (unsigned int)is_signed_type(long));\n\n\ttrace_seq_printf(s, \"\\tfield: int overwrite;\\t\"\n\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)offsetof(typeof(field), commit),\n\t\t\t 1,\n\t\t\t (unsigned int)is_signed_type(long));\n\n\ttrace_seq_printf(s, \"\\tfield: char data;\\t\"\n\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)offsetof(typeof(field), data),\n\t\t\t (unsigned int)BUF_PAGE_SIZE,\n\t\t\t (unsigned int)is_signed_type(char));\n\n\treturn !trace_seq_has_overflowed(s);\n}\n\nstruct rb_irq_work {\n\tstruct irq_work\t\t\twork;\n\twait_queue_head_t\t\twaiters;\n\twait_queue_head_t\t\tfull_waiters;\n\tbool\t\t\t\twaiters_pending;\n\tbool\t\t\t\tfull_waiters_pending;\n\tbool\t\t\t\twakeup_full;\n};\n\n/*\n * Structure to hold event state and handle nested events.\n */\nstruct rb_event_info {\n\tu64\t\t\tts;\n\tu64\t\t\tdelta;\n\tunsigned long\t\tlength;\n\tstruct buffer_page\t*tail_page;\n\tint\t\t\tadd_timestamp;\n};\n\n/*\n * Used for which event context the event is in.\n *  NMI     = 0\n *  IRQ     = 1\n *  SOFTIRQ = 2\n *  NORMAL  = 3\n *\n * See trace_recursive_lock() comment below for more details.\n */\nenum {\n\tRB_CTX_NMI,\n\tRB_CTX_IRQ,\n\tRB_CTX_SOFTIRQ,\n\tRB_CTX_NORMAL,\n\tRB_CTX_MAX\n};\n\n/*\n * head_page == tail_page && head == tail then buffer is empty.\n */\nstruct ring_buffer_per_cpu {\n\tint\t\t\t\tcpu;\n\tatomic_t\t\t\trecord_disabled;\n\tstruct ring_buffer\t\t*buffer;\n\traw_spinlock_t\t\t\treader_lock;\t/* serialize readers */\n\tarch_spinlock_t\t\t\tlock;\n\tstruct lock_class_key\t\tlock_key;\n\tunsigned long\t\t\tnr_pages;\n\tunsigned int\t\t\tcurrent_context;\n\tstruct list_head\t\t*pages;\n\tstruct buffer_page\t\t*head_page;\t/* read from head */\n\tstruct buffer_page\t\t*tail_page;\t/* write to tail */\n\tstruct buffer_page\t\t*commit_page;\t/* committed pages */\n\tstruct buffer_page\t\t*reader_page;\n\tunsigned long\t\t\tlost_events;\n\tunsigned long\t\t\tlast_overrun;\n\tlocal_t\t\t\t\tentries_bytes;\n\tlocal_t\t\t\t\tentries;\n\tlocal_t\t\t\t\toverrun;\n\tlocal_t\t\t\t\tcommit_overrun;\n\tlocal_t\t\t\t\tdropped_events;\n\tlocal_t\t\t\t\tcommitting;\n\tlocal_t\t\t\t\tcommits;\n\tunsigned long\t\t\tread;\n\tunsigned long\t\t\tread_bytes;\n\tu64\t\t\t\twrite_stamp;\n\tu64\t\t\t\tread_stamp;\n\t/* ring buffer pages to update, > 0 to add, < 0 to remove */\n\tlong\t\t\t\tnr_pages_to_update;\n\tstruct list_head\t\tnew_pages; /* new pages to add */\n\tstruct work_struct\t\tupdate_pages_work;\n\tstruct completion\t\tupdate_done;\n\n\tstruct rb_irq_work\t\tirq_work;\n};\n\nstruct ring_buffer {\n\tunsigned\t\t\tflags;\n\tint\t\t\t\tcpus;\n\tatomic_t\t\t\trecord_disabled;\n\tatomic_t\t\t\tresize_disabled;\n\tcpumask_var_t\t\t\tcpumask;\n\n\tstruct lock_class_key\t\t*reader_lock_key;\n\n\tstruct mutex\t\t\tmutex;\n\n\tstruct ring_buffer_per_cpu\t**buffers;\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tstruct notifier_block\t\tcpu_notify;\n#endif\n\tu64\t\t\t\t(*clock)(void);\n\n\tstruct rb_irq_work\t\tirq_work;\n};\n\nstruct ring_buffer_iter {\n\tstruct ring_buffer_per_cpu\t*cpu_buffer;\n\tunsigned long\t\t\thead;\n\tstruct buffer_page\t\t*head_page;\n\tstruct buffer_page\t\t*cache_reader_page;\n\tunsigned long\t\t\tcache_read;\n\tu64\t\t\t\tread_stamp;\n};\n\n/*\n * rb_wake_up_waiters - wake up tasks waiting for ring buffer input\n *\n * Schedules a delayed work to wake up any task that is blocked on the\n * ring buffer waiters queue.\n */\nstatic void rb_wake_up_waiters(struct irq_work *work)\n{\n\tstruct rb_irq_work *rbwork = container_of(work, struct rb_irq_work, work);\n\n\twake_up_all(&rbwork->waiters);\n\tif (rbwork->wakeup_full) {\n\t\trbwork->wakeup_full = false;\n\t\twake_up_all(&rbwork->full_waiters);\n\t}\n}\n\n/**\n * ring_buffer_wait - wait for input to the ring buffer\n * @buffer: buffer to wait on\n * @cpu: the cpu buffer to wait on\n * @full: wait until a full page is available, if @cpu != RING_BUFFER_ALL_CPUS\n *\n * If @cpu == RING_BUFFER_ALL_CPUS then the task will wake up as soon\n * as data is added to any of the @buffer's cpu buffers. Otherwise\n * it will wait for data to be added to a specific cpu buffer.\n */\nint ring_buffer_wait(struct ring_buffer *buffer, int cpu, bool full)\n{\n\tstruct ring_buffer_per_cpu *uninitialized_var(cpu_buffer);\n\tDEFINE_WAIT(wait);\n\tstruct rb_irq_work *work;\n\tint ret = 0;\n\n\t/*\n\t * Depending on what the caller is waiting for, either any\n\t * data in any cpu buffer, or a specific buffer, put the\n\t * caller on the appropriate wait queue.\n\t */\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\twork = &buffer->irq_work;\n\t\t/* Full only makes sense on per cpu reads */\n\t\tfull = false;\n\t} else {\n\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\t\treturn -ENODEV;\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\twork = &cpu_buffer->irq_work;\n\t}\n\n\n\twhile (true) {\n\t\tif (full)\n\t\t\tprepare_to_wait(&work->full_waiters, &wait, TASK_INTERRUPTIBLE);\n\t\telse\n\t\t\tprepare_to_wait(&work->waiters, &wait, TASK_INTERRUPTIBLE);\n\n\t\t/*\n\t\t * The events can happen in critical sections where\n\t\t * checking a work queue can cause deadlocks.\n\t\t * After adding a task to the queue, this flag is set\n\t\t * only to notify events to try to wake up the queue\n\t\t * using irq_work.\n\t\t *\n\t\t * We don't clear it even if the buffer is no longer\n\t\t * empty. The flag only causes the next event to run\n\t\t * irq_work to do the work queue wake up. The worse\n\t\t * that can happen if we race with !trace_empty() is that\n\t\t * an event will cause an irq_work to try to wake up\n\t\t * an empty queue.\n\t\t *\n\t\t * There's no reason to protect this flag either, as\n\t\t * the work queue and irq_work logic will do the necessary\n\t\t * synchronization for the wake ups. The only thing\n\t\t * that is necessary is that the wake up happens after\n\t\t * a task has been queued. It's OK for spurious wake ups.\n\t\t */\n\t\tif (full)\n\t\t\twork->full_waiters_pending = true;\n\t\telse\n\t\t\twork->waiters_pending = true;\n\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cpu == RING_BUFFER_ALL_CPUS && !ring_buffer_empty(buffer))\n\t\t\tbreak;\n\n\t\tif (cpu != RING_BUFFER_ALL_CPUS &&\n\t\t    !ring_buffer_empty_cpu(buffer, cpu)) {\n\t\t\tunsigned long flags;\n\t\t\tbool pagebusy;\n\n\t\t\tif (!full)\n\t\t\t\tbreak;\n\n\t\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\t\t\tpagebusy = cpu_buffer->reader_page == cpu_buffer->commit_page;\n\t\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\t\t\tif (!pagebusy)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tschedule();\n\t}\n\n\tif (full)\n\t\tfinish_wait(&work->full_waiters, &wait);\n\telse\n\t\tfinish_wait(&work->waiters, &wait);\n\n\treturn ret;\n}\n\n/**\n * ring_buffer_poll_wait - poll on buffer input\n * @buffer: buffer to wait on\n * @cpu: the cpu buffer to wait on\n * @filp: the file descriptor\n * @poll_table: The poll descriptor\n *\n * If @cpu == RING_BUFFER_ALL_CPUS then the task will wake up as soon\n * as data is added to any of the @buffer's cpu buffers. Otherwise\n * it will wait for data to be added to a specific cpu buffer.\n *\n * Returns POLLIN | POLLRDNORM if data exists in the buffers,\n * zero otherwise.\n */\nint ring_buffer_poll_wait(struct ring_buffer *buffer, int cpu,\n\t\t\t  struct file *filp, poll_table *poll_table)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct rb_irq_work *work;\n\n\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\twork = &buffer->irq_work;\n\telse {\n\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\t\treturn -EINVAL;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\twork = &cpu_buffer->irq_work;\n\t}\n\n\tpoll_wait(filp, &work->waiters, poll_table);\n\twork->waiters_pending = true;\n\t/*\n\t * There's a tight race between setting the waiters_pending and\n\t * checking if the ring buffer is empty.  Once the waiters_pending bit\n\t * is set, the next event will wake the task up, but we can get stuck\n\t * if there's only a single event in.\n\t *\n\t * FIXME: Ideally, we need a memory barrier on the writer side as well,\n\t * but adding a memory barrier to all events will cause too much of a\n\t * performance hit in the fast path.  We only need a memory barrier when\n\t * the buffer goes from empty to having content.  But as this race is\n\t * extremely small, and it's not a problem if another event comes in, we\n\t * will fix it later.\n\t */\n\tsmp_mb();\n\n\tif ((cpu == RING_BUFFER_ALL_CPUS && !ring_buffer_empty(buffer)) ||\n\t    (cpu != RING_BUFFER_ALL_CPUS && !ring_buffer_empty_cpu(buffer, cpu)))\n\t\treturn POLLIN | POLLRDNORM;\n\treturn 0;\n}\n\n/* buffer may be either ring_buffer or ring_buffer_per_cpu */\n#define RB_WARN_ON(b, cond)\t\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\tint _____ret = unlikely(cond);\t\t\t\t\\\n\t\tif (_____ret) {\t\t\t\t\t\t\\\n\t\t\tif (__same_type(*(b), struct ring_buffer_per_cpu)) { \\\n\t\t\t\tstruct ring_buffer_per_cpu *__b =\t\\\n\t\t\t\t\t(void *)b;\t\t\t\\\n\t\t\t\tatomic_inc(&__b->buffer->record_disabled); \\\n\t\t\t} else\t\t\t\t\t\t\\\n\t\t\t\tatomic_inc(&b->record_disabled);\t\\\n\t\t\tWARN_ON(1);\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t_____ret;\t\t\t\t\t\t\\\n\t})\n\n/* Up this if you want to test the TIME_EXTENTS and normalization */\n#define DEBUG_SHIFT 0\n\nstatic inline u64 rb_time_stamp(struct ring_buffer *buffer)\n{\n\t/* shift to debug/test normalization and TIME_EXTENTS */\n\treturn buffer->clock() << DEBUG_SHIFT;\n}\n\nu64 ring_buffer_time_stamp(struct ring_buffer *buffer, int cpu)\n{\n\tu64 time;\n\n\tpreempt_disable_notrace();\n\ttime = rb_time_stamp(buffer);\n\tpreempt_enable_no_resched_notrace();\n\n\treturn time;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_time_stamp);\n\nvoid ring_buffer_normalize_time_stamp(struct ring_buffer *buffer,\n\t\t\t\t      int cpu, u64 *ts)\n{\n\t/* Just stupid testing the normalize function and deltas */\n\t*ts >>= DEBUG_SHIFT;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_normalize_time_stamp);\n\n/*\n * Making the ring buffer lockless makes things tricky.\n * Although writes only happen on the CPU that they are on,\n * and they only need to worry about interrupts. Reads can\n * happen on any CPU.\n *\n * The reader page is always off the ring buffer, but when the\n * reader finishes with a page, it needs to swap its page with\n * a new one from the buffer. The reader needs to take from\n * the head (writes go to the tail). But if a writer is in overwrite\n * mode and wraps, it must push the head page forward.\n *\n * Here lies the problem.\n *\n * The reader must be careful to replace only the head page, and\n * not another one. As described at the top of the file in the\n * ASCII art, the reader sets its old page to point to the next\n * page after head. It then sets the page after head to point to\n * the old reader page. But if the writer moves the head page\n * during this operation, the reader could end up with the tail.\n *\n * We use cmpxchg to help prevent this race. We also do something\n * special with the page before head. We set the LSB to 1.\n *\n * When the writer must push the page forward, it will clear the\n * bit that points to the head page, move the head, and then set\n * the bit that points to the new head page.\n *\n * We also don't want an interrupt coming in and moving the head\n * page on another writer. Thus we use the second LSB to catch\n * that too. Thus:\n *\n * head->list->prev->next        bit 1          bit 0\n *                              -------        -------\n * Normal page                     0              0\n * Points to head page             0              1\n * New head page                   1              0\n *\n * Note we can not trust the prev pointer of the head page, because:\n *\n * +----+       +-----+        +-----+\n * |    |------>|  T  |---X--->|  N  |\n * |    |<------|     |        |     |\n * +----+       +-----+        +-----+\n *   ^                           ^ |\n *   |          +-----+          | |\n *   +----------|  R  |----------+ |\n *              |     |<-----------+\n *              +-----+\n *\n * Key:  ---X-->  HEAD flag set in pointer\n *         T      Tail page\n *         R      Reader page\n *         N      Next page\n *\n * (see __rb_reserve_next() to see where this happens)\n *\n *  What the above shows is that the reader just swapped out\n *  the reader page with a page in the buffer, but before it\n *  could make the new header point back to the new page added\n *  it was preempted by a writer. The writer moved forward onto\n *  the new page added by the reader and is about to move forward\n *  again.\n *\n *  You can see, it is legitimate for the previous pointer of\n *  the head (or any page) not to point back to itself. But only\n *  temporarially.\n */\n\n#define RB_PAGE_NORMAL\t\t0UL\n#define RB_PAGE_HEAD\t\t1UL\n#define RB_PAGE_UPDATE\t\t2UL\n\n\n#define RB_FLAG_MASK\t\t3UL\n\n/* PAGE_MOVED is not part of the mask */\n#define RB_PAGE_MOVED\t\t4UL\n\n/*\n * rb_list_head - remove any bit\n */\nstatic struct list_head *rb_list_head(struct list_head *list)\n{\n\tunsigned long val = (unsigned long)list;\n\n\treturn (struct list_head *)(val & ~RB_FLAG_MASK);\n}\n\n/*\n * rb_is_head_page - test if the given page is the head page\n *\n * Because the reader may move the head_page pointer, we can\n * not trust what the head page is (it may be pointing to\n * the reader page). But if the next page is a header page,\n * its flags will be non zero.\n */\nstatic inline int\nrb_is_head_page(struct ring_buffer_per_cpu *cpu_buffer,\n\t\tstruct buffer_page *page, struct list_head *list)\n{\n\tunsigned long val;\n\n\tval = (unsigned long)list->next;\n\n\tif ((val & ~RB_FLAG_MASK) != (unsigned long)&page->list)\n\t\treturn RB_PAGE_MOVED;\n\n\treturn val & RB_FLAG_MASK;\n}\n\n/*\n * rb_is_reader_page\n *\n * The unique thing about the reader page, is that, if the\n * writer is ever on it, the previous pointer never points\n * back to the reader page.\n */\nstatic bool rb_is_reader_page(struct buffer_page *page)\n{\n\tstruct list_head *list = page->list.prev;\n\n\treturn rb_list_head(list->next) != &page->list;\n}\n\n/*\n * rb_set_list_to_head - set a list_head to be pointing to head.\n */\nstatic void rb_set_list_to_head(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\tstruct list_head *list)\n{\n\tunsigned long *ptr;\n\n\tptr = (unsigned long *)&list->next;\n\t*ptr |= RB_PAGE_HEAD;\n\t*ptr &= ~RB_PAGE_UPDATE;\n}\n\n/*\n * rb_head_page_activate - sets up head page\n */\nstatic void rb_head_page_activate(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *head;\n\n\thead = cpu_buffer->head_page;\n\tif (!head)\n\t\treturn;\n\n\t/*\n\t * Set the previous list pointer to have the HEAD flag.\n\t */\n\trb_set_list_to_head(cpu_buffer, head->list.prev);\n}\n\nstatic void rb_list_head_clear(struct list_head *list)\n{\n\tunsigned long *ptr = (unsigned long *)&list->next;\n\n\t*ptr &= ~RB_FLAG_MASK;\n}\n\n/*\n * rb_head_page_dactivate - clears head page ptr (for free list)\n */\nstatic void\nrb_head_page_deactivate(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *hd;\n\n\t/* Go through the whole list and clear any pointers found. */\n\trb_list_head_clear(cpu_buffer->pages);\n\n\tlist_for_each(hd, cpu_buffer->pages)\n\t\trb_list_head_clear(hd);\n}\n\nstatic int rb_head_page_set(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t    struct buffer_page *head,\n\t\t\t    struct buffer_page *prev,\n\t\t\t    int old_flag, int new_flag)\n{\n\tstruct list_head *list;\n\tunsigned long val = (unsigned long)&head->list;\n\tunsigned long ret;\n\n\tlist = &prev->list;\n\n\tval &= ~RB_FLAG_MASK;\n\n\tret = cmpxchg((unsigned long *)&list->next,\n\t\t      val | old_flag, val | new_flag);\n\n\t/* check if the reader took the page */\n\tif ((ret & ~RB_FLAG_MASK) != val)\n\t\treturn RB_PAGE_MOVED;\n\n\treturn ret & RB_FLAG_MASK;\n}\n\nstatic int rb_head_page_set_update(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t   struct buffer_page *head,\n\t\t\t\t   struct buffer_page *prev,\n\t\t\t\t   int old_flag)\n{\n\treturn rb_head_page_set(cpu_buffer, head, prev,\n\t\t\t\told_flag, RB_PAGE_UPDATE);\n}\n\nstatic int rb_head_page_set_head(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t struct buffer_page *head,\n\t\t\t\t struct buffer_page *prev,\n\t\t\t\t int old_flag)\n{\n\treturn rb_head_page_set(cpu_buffer, head, prev,\n\t\t\t\told_flag, RB_PAGE_HEAD);\n}\n\nstatic int rb_head_page_set_normal(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t   struct buffer_page *head,\n\t\t\t\t   struct buffer_page *prev,\n\t\t\t\t   int old_flag)\n{\n\treturn rb_head_page_set(cpu_buffer, head, prev,\n\t\t\t\told_flag, RB_PAGE_NORMAL);\n}\n\nstatic inline void rb_inc_page(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t       struct buffer_page **bpage)\n{\n\tstruct list_head *p = rb_list_head((*bpage)->list.next);\n\n\t*bpage = list_entry(p, struct buffer_page, list);\n}\n\nstatic struct buffer_page *\nrb_set_head_page(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *head;\n\tstruct buffer_page *page;\n\tstruct list_head *list;\n\tint i;\n\n\tif (RB_WARN_ON(cpu_buffer, !cpu_buffer->head_page))\n\t\treturn NULL;\n\n\t/* sanity check */\n\tlist = cpu_buffer->pages;\n\tif (RB_WARN_ON(cpu_buffer, rb_list_head(list->prev->next) != list))\n\t\treturn NULL;\n\n\tpage = head = cpu_buffer->head_page;\n\t/*\n\t * It is possible that the writer moves the header behind\n\t * where we started, and we miss in one loop.\n\t * A second loop should grab the header, but we'll do\n\t * three loops just because I'm paranoid.\n\t */\n\tfor (i = 0; i < 3; i++) {\n\t\tdo {\n\t\t\tif (rb_is_head_page(cpu_buffer, page, page->list.prev)) {\n\t\t\t\tcpu_buffer->head_page = page;\n\t\t\t\treturn page;\n\t\t\t}\n\t\t\trb_inc_page(cpu_buffer, &page);\n\t\t} while (page != head);\n\t}\n\n\tRB_WARN_ON(cpu_buffer, 1);\n\n\treturn NULL;\n}\n\nstatic int rb_head_page_replace(struct buffer_page *old,\n\t\t\t\tstruct buffer_page *new)\n{\n\tunsigned long *ptr = (unsigned long *)&old->list.prev->next;\n\tunsigned long val;\n\tunsigned long ret;\n\n\tval = *ptr & ~RB_FLAG_MASK;\n\tval |= RB_PAGE_HEAD;\n\n\tret = cmpxchg(ptr, val, (unsigned long)&new->list);\n\n\treturn ret == val;\n}\n\n/*\n * rb_tail_page_update - move the tail page forward\n */\nstatic void rb_tail_page_update(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t       struct buffer_page *tail_page,\n\t\t\t       struct buffer_page *next_page)\n{\n\tunsigned long old_entries;\n\tunsigned long old_write;\n\n\t/*\n\t * The tail page now needs to be moved forward.\n\t *\n\t * We need to reset the tail page, but without messing\n\t * with possible erasing of data brought in by interrupts\n\t * that have moved the tail page and are currently on it.\n\t *\n\t * We add a counter to the write field to denote this.\n\t */\n\told_write = local_add_return(RB_WRITE_INTCNT, &next_page->write);\n\told_entries = local_add_return(RB_WRITE_INTCNT, &next_page->entries);\n\n\t/*\n\t * Just make sure we have seen our old_write and synchronize\n\t * with any interrupts that come in.\n\t */\n\tbarrier();\n\n\t/*\n\t * If the tail page is still the same as what we think\n\t * it is, then it is up to us to update the tail\n\t * pointer.\n\t */\n\tif (tail_page == READ_ONCE(cpu_buffer->tail_page)) {\n\t\t/* Zero the write counter */\n\t\tunsigned long val = old_write & ~RB_WRITE_MASK;\n\t\tunsigned long eval = old_entries & ~RB_WRITE_MASK;\n\n\t\t/*\n\t\t * This will only succeed if an interrupt did\n\t\t * not come in and change it. In which case, we\n\t\t * do not want to modify it.\n\t\t *\n\t\t * We add (void) to let the compiler know that we do not care\n\t\t * about the return value of these functions. We use the\n\t\t * cmpxchg to only update if an interrupt did not already\n\t\t * do it for us. If the cmpxchg fails, we don't care.\n\t\t */\n\t\t(void)local_cmpxchg(&next_page->write, old_write, val);\n\t\t(void)local_cmpxchg(&next_page->entries, old_entries, eval);\n\n\t\t/*\n\t\t * No need to worry about races with clearing out the commit.\n\t\t * it only can increment when a commit takes place. But that\n\t\t * only happens in the outer most nested commit.\n\t\t */\n\t\tlocal_set(&next_page->page->commit, 0);\n\n\t\t/* Again, either we update tail_page or an interrupt does */\n\t\t(void)cmpxchg(&cpu_buffer->tail_page, tail_page, next_page);\n\t}\n}\n\nstatic int rb_check_bpage(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t  struct buffer_page *bpage)\n{\n\tunsigned long val = (unsigned long)bpage;\n\n\tif (RB_WARN_ON(cpu_buffer, val & RB_FLAG_MASK))\n\t\treturn 1;\n\n\treturn 0;\n}\n\n/**\n * rb_check_list - make sure a pointer to a list has the last bits zero\n */\nstatic int rb_check_list(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t struct list_head *list)\n{\n\tif (RB_WARN_ON(cpu_buffer, rb_list_head(list->prev) != list->prev))\n\t\treturn 1;\n\tif (RB_WARN_ON(cpu_buffer, rb_list_head(list->next) != list->next))\n\t\treturn 1;\n\treturn 0;\n}\n\n/**\n * rb_check_pages - integrity check of buffer pages\n * @cpu_buffer: CPU buffer with pages to test\n *\n * As a safety measure we check to make sure the data pages have not\n * been corrupted.\n */\nstatic int rb_check_pages(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *head = cpu_buffer->pages;\n\tstruct buffer_page *bpage, *tmp;\n\n\t/* Reset the head page if it exists */\n\tif (cpu_buffer->head_page)\n\t\trb_set_head_page(cpu_buffer);\n\n\trb_head_page_deactivate(cpu_buffer);\n\n\tif (RB_WARN_ON(cpu_buffer, head->next->prev != head))\n\t\treturn -1;\n\tif (RB_WARN_ON(cpu_buffer, head->prev->next != head))\n\t\treturn -1;\n\n\tif (rb_check_list(cpu_buffer, head))\n\t\treturn -1;\n\n\tlist_for_each_entry_safe(bpage, tmp, head, list) {\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t       bpage->list.next->prev != &bpage->list))\n\t\t\treturn -1;\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t       bpage->list.prev->next != &bpage->list))\n\t\t\treturn -1;\n\t\tif (rb_check_list(cpu_buffer, &bpage->list))\n\t\t\treturn -1;\n\t}\n\n\trb_head_page_activate(cpu_buffer);\n\n\treturn 0;\n}\n\nstatic int __rb_allocate_pages(long nr_pages, struct list_head *pages, int cpu)\n{\n\tstruct buffer_page *bpage, *tmp;\n\tlong i;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tstruct page *page;\n\t\t/*\n\t\t * __GFP_NORETRY flag makes sure that the allocation fails\n\t\t * gracefully without invoking oom-killer and the system is\n\t\t * not destabilized.\n\t\t */\n\t\tbpage = kzalloc_node(ALIGN(sizeof(*bpage), cache_line_size()),\n\t\t\t\t    GFP_KERNEL | __GFP_NORETRY,\n\t\t\t\t    cpu_to_node(cpu));\n\t\tif (!bpage)\n\t\t\tgoto free_pages;\n\n\t\tlist_add(&bpage->list, pages);\n\n\t\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\t\tif (!page)\n\t\t\tgoto free_pages;\n\t\tbpage->page = page_address(page);\n\t\trb_init_page(bpage->page);\n\t}\n\n\treturn 0;\n\nfree_pages:\n\tlist_for_each_entry_safe(bpage, tmp, pages, list) {\n\t\tlist_del_init(&bpage->list);\n\t\tfree_buffer_page(bpage);\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic int rb_allocate_pages(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t     unsigned long nr_pages)\n{\n\tLIST_HEAD(pages);\n\n\tWARN_ON(!nr_pages);\n\n\tif (__rb_allocate_pages(nr_pages, &pages, cpu_buffer->cpu))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * The ring buffer page list is a circular list that does not\n\t * start and end with a list head. All page list items point to\n\t * other pages.\n\t */\n\tcpu_buffer->pages = pages.next;\n\tlist_del(&pages);\n\n\tcpu_buffer->nr_pages = nr_pages;\n\n\trb_check_pages(cpu_buffer);\n\n\treturn 0;\n}\n\nstatic struct ring_buffer_per_cpu *\nrb_allocate_cpu_buffer(struct ring_buffer *buffer, long nr_pages, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct buffer_page *bpage;\n\tstruct page *page;\n\tint ret;\n\n\tcpu_buffer = kzalloc_node(ALIGN(sizeof(*cpu_buffer), cache_line_size()),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(cpu));\n\tif (!cpu_buffer)\n\t\treturn NULL;\n\n\tcpu_buffer->cpu = cpu;\n\tcpu_buffer->buffer = buffer;\n\traw_spin_lock_init(&cpu_buffer->reader_lock);\n\tlockdep_set_class(&cpu_buffer->reader_lock, buffer->reader_lock_key);\n\tcpu_buffer->lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\tINIT_WORK(&cpu_buffer->update_pages_work, update_pages_handler);\n\tinit_completion(&cpu_buffer->update_done);\n\tinit_irq_work(&cpu_buffer->irq_work.work, rb_wake_up_waiters);\n\tinit_waitqueue_head(&cpu_buffer->irq_work.waiters);\n\tinit_waitqueue_head(&cpu_buffer->irq_work.full_waiters);\n\n\tbpage = kzalloc_node(ALIGN(sizeof(*bpage), cache_line_size()),\n\t\t\t    GFP_KERNEL, cpu_to_node(cpu));\n\tif (!bpage)\n\t\tgoto fail_free_buffer;\n\n\trb_check_bpage(cpu_buffer, bpage);\n\n\tcpu_buffer->reader_page = bpage;\n\tpage = alloc_pages_node(cpu_to_node(cpu), GFP_KERNEL, 0);\n\tif (!page)\n\t\tgoto fail_free_reader;\n\tbpage->page = page_address(page);\n\trb_init_page(bpage->page);\n\n\tINIT_LIST_HEAD(&cpu_buffer->reader_page->list);\n\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\n\tret = rb_allocate_pages(cpu_buffer, nr_pages);\n\tif (ret < 0)\n\t\tgoto fail_free_reader;\n\n\tcpu_buffer->head_page\n\t\t= list_entry(cpu_buffer->pages, struct buffer_page, list);\n\tcpu_buffer->tail_page = cpu_buffer->commit_page = cpu_buffer->head_page;\n\n\trb_head_page_activate(cpu_buffer);\n\n\treturn cpu_buffer;\n\n fail_free_reader:\n\tfree_buffer_page(cpu_buffer->reader_page);\n\n fail_free_buffer:\n\tkfree(cpu_buffer);\n\treturn NULL;\n}\n\nstatic void rb_free_cpu_buffer(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *head = cpu_buffer->pages;\n\tstruct buffer_page *bpage, *tmp;\n\n\tfree_buffer_page(cpu_buffer->reader_page);\n\n\trb_head_page_deactivate(cpu_buffer);\n\n\tif (head) {\n\t\tlist_for_each_entry_safe(bpage, tmp, head, list) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t\tbpage = list_entry(head, struct buffer_page, list);\n\t\tfree_buffer_page(bpage);\n\t}\n\n\tkfree(cpu_buffer);\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\nstatic int rb_cpu_notify(struct notifier_block *self,\n\t\t\t unsigned long action, void *hcpu);\n#endif\n\n/**\n * __ring_buffer_alloc - allocate a new ring_buffer\n * @size: the size in bytes per cpu that is needed.\n * @flags: attributes to set for the ring buffer.\n *\n * Currently the only flag that is available is the RB_FL_OVERWRITE\n * flag. This flag means that the buffer will overwrite old data\n * when the buffer wraps. If this flag is not set, the buffer will\n * drop data when the tail hits the head.\n */\nstruct ring_buffer *__ring_buffer_alloc(unsigned long size, unsigned flags,\n\t\t\t\t\tstruct lock_class_key *key)\n{\n\tstruct ring_buffer *buffer;\n\tlong nr_pages;\n\tint bsize;\n\tint cpu;\n\n\t/* keep it in its own cache line */\n\tbuffer = kzalloc(ALIGN(sizeof(*buffer), cache_line_size()),\n\t\t\t GFP_KERNEL);\n\tif (!buffer)\n\t\treturn NULL;\n\n\tif (!alloc_cpumask_var(&buffer->cpumask, GFP_KERNEL))\n\t\tgoto fail_free_buffer;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\tbuffer->flags = flags;\n\tbuffer->clock = trace_clock_local;\n\tbuffer->reader_lock_key = key;\n\n\tinit_irq_work(&buffer->irq_work.work, rb_wake_up_waiters);\n\tinit_waitqueue_head(&buffer->irq_work.waiters);\n\n\t/* need at least two pages */\n\tif (nr_pages < 2)\n\t\tnr_pages = 2;\n\n\t/*\n\t * In case of non-hotplug cpu, if the ring-buffer is allocated\n\t * in early initcall, it will not be notified of secondary cpus.\n\t * In that off case, we need to allocate for all possible cpus.\n\t */\n#ifdef CONFIG_HOTPLUG_CPU\n\tcpu_notifier_register_begin();\n\tcpumask_copy(buffer->cpumask, cpu_online_mask);\n#else\n\tcpumask_copy(buffer->cpumask, cpu_possible_mask);\n#endif\n\tbuffer->cpus = nr_cpu_ids;\n\n\tbsize = sizeof(void *) * nr_cpu_ids;\n\tbuffer->buffers = kzalloc(ALIGN(bsize, cache_line_size()),\n\t\t\t\t  GFP_KERNEL);\n\tif (!buffer->buffers)\n\t\tgoto fail_free_cpumask;\n\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tbuffer->buffers[cpu] =\n\t\t\trb_allocate_cpu_buffer(buffer, nr_pages, cpu);\n\t\tif (!buffer->buffers[cpu])\n\t\t\tgoto fail_free_buffers;\n\t}\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tbuffer->cpu_notify.notifier_call = rb_cpu_notify;\n\tbuffer->cpu_notify.priority = 0;\n\t__register_cpu_notifier(&buffer->cpu_notify);\n\tcpu_notifier_register_done();\n#endif\n\n\tmutex_init(&buffer->mutex);\n\n\treturn buffer;\n\n fail_free_buffers:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tif (buffer->buffers[cpu])\n\t\t\trb_free_cpu_buffer(buffer->buffers[cpu]);\n\t}\n\tkfree(buffer->buffers);\n\n fail_free_cpumask:\n\tfree_cpumask_var(buffer->cpumask);\n#ifdef CONFIG_HOTPLUG_CPU\n\tcpu_notifier_register_done();\n#endif\n\n fail_free_buffer:\n\tkfree(buffer);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(__ring_buffer_alloc);\n\n/**\n * ring_buffer_free - free a ring buffer.\n * @buffer: the buffer to free.\n */\nvoid\nring_buffer_free(struct ring_buffer *buffer)\n{\n\tint cpu;\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tcpu_notifier_register_begin();\n\t__unregister_cpu_notifier(&buffer->cpu_notify);\n#endif\n\n\tfor_each_buffer_cpu(buffer, cpu)\n\t\trb_free_cpu_buffer(buffer->buffers[cpu]);\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tcpu_notifier_register_done();\n#endif\n\n\tkfree(buffer->buffers);\n\tfree_cpumask_var(buffer->cpumask);\n\n\tkfree(buffer);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_free);\n\nvoid ring_buffer_set_clock(struct ring_buffer *buffer,\n\t\t\t   u64 (*clock)(void))\n{\n\tbuffer->clock = clock;\n}\n\nstatic void rb_reset_cpu(struct ring_buffer_per_cpu *cpu_buffer);\n\nstatic inline unsigned long rb_page_entries(struct buffer_page *bpage)\n{\n\treturn local_read(&bpage->entries) & RB_WRITE_MASK;\n}\n\nstatic inline unsigned long rb_page_write(struct buffer_page *bpage)\n{\n\treturn local_read(&bpage->write) & RB_WRITE_MASK;\n}\n\nstatic int\nrb_remove_pages(struct ring_buffer_per_cpu *cpu_buffer, unsigned long nr_pages)\n{\n\tstruct list_head *tail_page, *to_remove, *next_page;\n\tstruct buffer_page *to_remove_page, *tmp_iter_page;\n\tstruct buffer_page *last_page, *first_page;\n\tunsigned long nr_removed;\n\tunsigned long head_bit;\n\tint page_entries;\n\n\thead_bit = 0;\n\n\traw_spin_lock_irq(&cpu_buffer->reader_lock);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\t/*\n\t * We don't race with the readers since we have acquired the reader\n\t * lock. We also don't race with writers after disabling recording.\n\t * This makes it easy to figure out the first and the last page to be\n\t * removed from the list. We unlink all the pages in between including\n\t * the first and last pages. This is done in a busy loop so that we\n\t * lose the least number of traces.\n\t * The pages are freed after we restart recording and unlock readers.\n\t */\n\ttail_page = &cpu_buffer->tail_page->list;\n\n\t/*\n\t * tail page might be on reader page, we remove the next page\n\t * from the ring buffer\n\t */\n\tif (cpu_buffer->tail_page == cpu_buffer->reader_page)\n\t\ttail_page = rb_list_head(tail_page->next);\n\tto_remove = tail_page;\n\n\t/* start of pages to remove */\n\tfirst_page = list_entry(rb_list_head(to_remove->next),\n\t\t\t\tstruct buffer_page, list);\n\n\tfor (nr_removed = 0; nr_removed < nr_pages; nr_removed++) {\n\t\tto_remove = rb_list_head(to_remove)->next;\n\t\thead_bit |= (unsigned long)to_remove & RB_PAGE_HEAD;\n\t}\n\n\tnext_page = rb_list_head(to_remove)->next;\n\n\t/*\n\t * Now we remove all pages between tail_page and next_page.\n\t * Make sure that we have head_bit value preserved for the\n\t * next page\n\t */\n\ttail_page->next = (struct list_head *)((unsigned long)next_page |\n\t\t\t\t\t\thead_bit);\n\tnext_page = rb_list_head(next_page);\n\tnext_page->prev = tail_page;\n\n\t/* make sure pages points to a valid page in the ring buffer */\n\tcpu_buffer->pages = next_page;\n\n\t/* update head page */\n\tif (head_bit)\n\t\tcpu_buffer->head_page = list_entry(next_page,\n\t\t\t\t\t\tstruct buffer_page, list);\n\n\t/*\n\t * change read pointer to make sure any read iterators reset\n\t * themselves\n\t */\n\tcpu_buffer->read = 0;\n\n\t/* pages are removed, resume tracing and then free the pages */\n\tatomic_dec(&cpu_buffer->record_disabled);\n\traw_spin_unlock_irq(&cpu_buffer->reader_lock);\n\n\tRB_WARN_ON(cpu_buffer, list_empty(cpu_buffer->pages));\n\n\t/* last buffer page to remove */\n\tlast_page = list_entry(rb_list_head(to_remove), struct buffer_page,\n\t\t\t\tlist);\n\ttmp_iter_page = first_page;\n\n\tdo {\n\t\tto_remove_page = tmp_iter_page;\n\t\trb_inc_page(cpu_buffer, &tmp_iter_page);\n\n\t\t/* update the counters */\n\t\tpage_entries = rb_page_entries(to_remove_page);\n\t\tif (page_entries) {\n\t\t\t/*\n\t\t\t * If something was added to this page, it was full\n\t\t\t * since it is not the tail page. So we deduct the\n\t\t\t * bytes consumed in ring buffer from here.\n\t\t\t * Increment overrun to account for the lost events.\n\t\t\t */\n\t\t\tlocal_add(page_entries, &cpu_buffer->overrun);\n\t\t\tlocal_sub(BUF_PAGE_SIZE, &cpu_buffer->entries_bytes);\n\t\t}\n\n\t\t/*\n\t\t * We have already removed references to this list item, just\n\t\t * free up the buffer_page and its page\n\t\t */\n\t\tfree_buffer_page(to_remove_page);\n\t\tnr_removed--;\n\n\t} while (to_remove_page != last_page);\n\n\tRB_WARN_ON(cpu_buffer, nr_removed);\n\n\treturn nr_removed == 0;\n}\n\nstatic int\nrb_insert_pages(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *pages = &cpu_buffer->new_pages;\n\tint retries, success;\n\n\traw_spin_lock_irq(&cpu_buffer->reader_lock);\n\t/*\n\t * We are holding the reader lock, so the reader page won't be swapped\n\t * in the ring buffer. Now we are racing with the writer trying to\n\t * move head page and the tail page.\n\t * We are going to adapt the reader page update process where:\n\t * 1. We first splice the start and end of list of new pages between\n\t *    the head page and its previous page.\n\t * 2. We cmpxchg the prev_page->next to point from head page to the\n\t *    start of new pages list.\n\t * 3. Finally, we update the head->prev to the end of new list.\n\t *\n\t * We will try this process 10 times, to make sure that we don't keep\n\t * spinning.\n\t */\n\tretries = 10;\n\tsuccess = 0;\n\twhile (retries--) {\n\t\tstruct list_head *head_page, *prev_page, *r;\n\t\tstruct list_head *last_page, *first_page;\n\t\tstruct list_head *head_page_with_bit;\n\n\t\thead_page = &rb_set_head_page(cpu_buffer)->list;\n\t\tif (!head_page)\n\t\t\tbreak;\n\t\tprev_page = head_page->prev;\n\n\t\tfirst_page = pages->next;\n\t\tlast_page  = pages->prev;\n\n\t\thead_page_with_bit = (struct list_head *)\n\t\t\t\t     ((unsigned long)head_page | RB_PAGE_HEAD);\n\n\t\tlast_page->next = head_page_with_bit;\n\t\tfirst_page->prev = prev_page;\n\n\t\tr = cmpxchg(&prev_page->next, head_page_with_bit, first_page);\n\n\t\tif (r == head_page_with_bit) {\n\t\t\t/*\n\t\t\t * yay, we replaced the page pointer to our new list,\n\t\t\t * now, we just have to update to head page's prev\n\t\t\t * pointer to point to end of list\n\t\t\t */\n\t\t\thead_page->prev = last_page;\n\t\t\tsuccess = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (success)\n\t\tINIT_LIST_HEAD(pages);\n\t/*\n\t * If we weren't successful in adding in new pages, warn and stop\n\t * tracing\n\t */\n\tRB_WARN_ON(cpu_buffer, !success);\n\traw_spin_unlock_irq(&cpu_buffer->reader_lock);\n\n\t/* free pages if they weren't inserted */\n\tif (!success) {\n\t\tstruct buffer_page *bpage, *tmp;\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\t list) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\treturn success;\n}\n\nstatic void rb_update_pages(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tint success;\n\n\tif (cpu_buffer->nr_pages_to_update > 0)\n\t\tsuccess = rb_insert_pages(cpu_buffer);\n\telse\n\t\tsuccess = rb_remove_pages(cpu_buffer,\n\t\t\t\t\t-cpu_buffer->nr_pages_to_update);\n\n\tif (success)\n\t\tcpu_buffer->nr_pages += cpu_buffer->nr_pages_to_update;\n}\n\nstatic void update_pages_handler(struct work_struct *work)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = container_of(work,\n\t\t\tstruct ring_buffer_per_cpu, update_pages_work);\n\trb_update_pages(cpu_buffer);\n\tcomplete(&cpu_buffer->update_done);\n}\n\n/**\n * ring_buffer_resize - resize the ring buffer\n * @buffer: the buffer to resize.\n * @size: the new size.\n * @cpu_id: the cpu buffer to resize\n *\n * Minimum size is 2 * BUF_PAGE_SIZE.\n *\n * Returns 0 on success and < 0 on failure.\n */\nint ring_buffer_resize(struct ring_buffer *buffer, unsigned long size,\n\t\t\tint cpu_id)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long nr_pages;\n\tint cpu, err = 0;\n\n\t/*\n\t * Always succeed at resizing a non-existent buffer:\n\t */\n\tif (!buffer)\n\t\treturn size;\n\n\t/* Make sure the requested buffer exists */\n\tif (cpu_id != RING_BUFFER_ALL_CPUS &&\n\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\treturn size;\n\n\tsize = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\tsize *= BUF_PAGE_SIZE;\n\n\t/* we need a minimum of two pages */\n\tif (size < BUF_PAGE_SIZE * 2)\n\t\tsize = BUF_PAGE_SIZE * 2;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\n\t/*\n\t * Don't succeed if resizing is disabled, as a reader might be\n\t * manipulating the ring buffer and is expecting a sane state while\n\t * this is true.\n\t */\n\tif (atomic_read(&buffer->resize_disabled))\n\t\treturn -EBUSY;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\t/* calculate the pages to update */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\t\tcpu_buffer->nr_pages;\n\t\t\t/*\n\t\t\t * nothing more to do for removing pages or no update\n\t\t\t */\n\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * to add pages, make sure all new pages can be\n\t\t\t * allocated without receiving ENOMEM\n\t\t\t */\n\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\t\tif (__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t\t&cpu_buffer->new_pages, cpu)) {\n\t\t\t\t/* not enough memory for new pages */\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\n\t\tget_online_cpus();\n\t\t/*\n\t\t * Fire off all the required work handlers\n\t\t * We can't schedule on offline CPUs, but it's not necessary\n\t\t * since we can change their buffer sizes without any race.\n\t\t */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\t/* Can't run something on an offline CPU. */\n\t\t\tif (!cpu_online(cpu)) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t\t} else {\n\t\t\t\tschedule_work_on(cpu,\n\t\t\t\t\t\t&cpu_buffer->update_pages_work);\n\t\t\t}\n\t\t}\n\n\t\t/* wait for all the updates to complete */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\tif (cpu_online(cpu))\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t}\n\n\t\tput_online_cpus();\n\t} else {\n\t\t/* Make sure this CPU has been intitialized */\n\t\tif (!cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\t\tgoto out;\n\n\t\tcpu_buffer = buffer->buffers[cpu_id];\n\n\t\tif (nr_pages == cpu_buffer->nr_pages)\n\t\t\tgoto out;\n\n\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\tcpu_buffer->nr_pages;\n\n\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\tif (cpu_buffer->nr_pages_to_update > 0 &&\n\t\t\t__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t    &cpu_buffer->new_pages, cpu_id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tget_online_cpus();\n\n\t\t/* Can't run something on an offline CPU. */\n\t\tif (!cpu_online(cpu_id))\n\t\t\trb_update_pages(cpu_buffer);\n\t\telse {\n\t\t\tschedule_work_on(cpu_id,\n\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\tput_online_cpus();\n\t}\n\n out:\n\t/*\n\t * The ring buffer resize can happen with the ring buffer\n\t * enabled, so that the update disturbs the tracing as little\n\t * as possible. But if the buffer is disabled, we do not need\n\t * to worry about that, and we can take the time to verify\n\t * that the buffer is not corrupt.\n\t */\n\tif (atomic_read(&buffer->record_disabled)) {\n\t\tatomic_inc(&buffer->record_disabled);\n\t\t/*\n\t\t * Even though the buffer was disabled, we must make sure\n\t\t * that it is truly disabled before calling rb_check_pages.\n\t\t * There could have been a race between checking\n\t\t * record_disable and incrementing it.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\trb_check_pages(cpu_buffer);\n\t\t}\n\t\tatomic_dec(&buffer->record_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n\treturn size;\n\n out_err:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tstruct buffer_page *bpage, *tmp;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\n\t\tif (list_empty(&cpu_buffer->new_pages))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\tlist) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\tmutex_unlock(&buffer->mutex);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_resize);\n\nvoid ring_buffer_change_overwrite(struct ring_buffer *buffer, int val)\n{\n\tmutex_lock(&buffer->mutex);\n\tif (val)\n\t\tbuffer->flags |= RB_FL_OVERWRITE;\n\telse\n\t\tbuffer->flags &= ~RB_FL_OVERWRITE;\n\tmutex_unlock(&buffer->mutex);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_change_overwrite);\n\nstatic inline void *\n__rb_data_page_index(struct buffer_data_page *bpage, unsigned index)\n{\n\treturn bpage->data + index;\n}\n\nstatic inline void *__rb_page_index(struct buffer_page *bpage, unsigned index)\n{\n\treturn bpage->page->data + index;\n}\n\nstatic inline struct ring_buffer_event *\nrb_reader_event(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn __rb_page_index(cpu_buffer->reader_page,\n\t\t\t       cpu_buffer->reader_page->read);\n}\n\nstatic inline struct ring_buffer_event *\nrb_iter_head_event(struct ring_buffer_iter *iter)\n{\n\treturn __rb_page_index(iter->head_page, iter->head);\n}\n\nstatic inline unsigned rb_page_commit(struct buffer_page *bpage)\n{\n\treturn local_read(&bpage->page->commit);\n}\n\n/* Size is determined by what has been committed */\nstatic inline unsigned rb_page_size(struct buffer_page *bpage)\n{\n\treturn rb_page_commit(bpage);\n}\n\nstatic inline unsigned\nrb_commit_index(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn rb_page_commit(cpu_buffer->commit_page);\n}\n\nstatic inline unsigned\nrb_event_index(struct ring_buffer_event *event)\n{\n\tunsigned long addr = (unsigned long)event;\n\n\treturn (addr & ~PAGE_MASK) - BUF_PAGE_HDR_SIZE;\n}\n\nstatic void rb_inc_iter(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\n\t/*\n\t * The iterator could be on the reader page (it starts there).\n\t * But the head could have moved, since the reader was\n\t * found. Check for this case and assign the iterator\n\t * to the head page instead of next.\n\t */\n\tif (iter->head_page == cpu_buffer->reader_page)\n\t\titer->head_page = rb_set_head_page(cpu_buffer);\n\telse\n\t\trb_inc_page(cpu_buffer, &iter->head_page);\n\n\titer->read_stamp = iter->head_page->page->time_stamp;\n\titer->head = 0;\n}\n\n/*\n * rb_handle_head_page - writer hit the head page\n *\n * Returns: +1 to retry page\n *           0 to continue\n *          -1 on error\n */\nstatic int\nrb_handle_head_page(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t    struct buffer_page *tail_page,\n\t\t    struct buffer_page *next_page)\n{\n\tstruct buffer_page *new_head;\n\tint entries;\n\tint type;\n\tint ret;\n\n\tentries = rb_page_entries(next_page);\n\n\t/*\n\t * The hard part is here. We need to move the head\n\t * forward, and protect against both readers on\n\t * other CPUs and writers coming in via interrupts.\n\t */\n\ttype = rb_head_page_set_update(cpu_buffer, next_page, tail_page,\n\t\t\t\t       RB_PAGE_HEAD);\n\n\t/*\n\t * type can be one of four:\n\t *  NORMAL - an interrupt already moved it for us\n\t *  HEAD   - we are the first to get here.\n\t *  UPDATE - we are the interrupt interrupting\n\t *           a current move.\n\t *  MOVED  - a reader on another CPU moved the next\n\t *           pointer to its reader page. Give up\n\t *           and try again.\n\t */\n\n\tswitch (type) {\n\tcase RB_PAGE_HEAD:\n\t\t/*\n\t\t * We changed the head to UPDATE, thus\n\t\t * it is our responsibility to update\n\t\t * the counters.\n\t\t */\n\t\tlocal_add(entries, &cpu_buffer->overrun);\n\t\tlocal_sub(BUF_PAGE_SIZE, &cpu_buffer->entries_bytes);\n\n\t\t/*\n\t\t * The entries will be zeroed out when we move the\n\t\t * tail page.\n\t\t */\n\n\t\t/* still more to do */\n\t\tbreak;\n\n\tcase RB_PAGE_UPDATE:\n\t\t/*\n\t\t * This is an interrupt that interrupt the\n\t\t * previous update. Still more to do.\n\t\t */\n\t\tbreak;\n\tcase RB_PAGE_NORMAL:\n\t\t/*\n\t\t * An interrupt came in before the update\n\t\t * and processed this for us.\n\t\t * Nothing left to do.\n\t\t */\n\t\treturn 1;\n\tcase RB_PAGE_MOVED:\n\t\t/*\n\t\t * The reader is on another CPU and just did\n\t\t * a swap with our next_page.\n\t\t * Try again.\n\t\t */\n\t\treturn 1;\n\tdefault:\n\t\tRB_WARN_ON(cpu_buffer, 1); /* WTF??? */\n\t\treturn -1;\n\t}\n\n\t/*\n\t * Now that we are here, the old head pointer is\n\t * set to UPDATE. This will keep the reader from\n\t * swapping the head page with the reader page.\n\t * The reader (on another CPU) will spin till\n\t * we are finished.\n\t *\n\t * We just need to protect against interrupts\n\t * doing the job. We will set the next pointer\n\t * to HEAD. After that, we set the old pointer\n\t * to NORMAL, but only if it was HEAD before.\n\t * otherwise we are an interrupt, and only\n\t * want the outer most commit to reset it.\n\t */\n\tnew_head = next_page;\n\trb_inc_page(cpu_buffer, &new_head);\n\n\tret = rb_head_page_set_head(cpu_buffer, new_head, next_page,\n\t\t\t\t    RB_PAGE_NORMAL);\n\n\t/*\n\t * Valid returns are:\n\t *  HEAD   - an interrupt came in and already set it.\n\t *  NORMAL - One of two things:\n\t *            1) We really set it.\n\t *            2) A bunch of interrupts came in and moved\n\t *               the page forward again.\n\t */\n\tswitch (ret) {\n\tcase RB_PAGE_HEAD:\n\tcase RB_PAGE_NORMAL:\n\t\t/* OK */\n\t\tbreak;\n\tdefault:\n\t\tRB_WARN_ON(cpu_buffer, 1);\n\t\treturn -1;\n\t}\n\n\t/*\n\t * It is possible that an interrupt came in,\n\t * set the head up, then more interrupts came in\n\t * and moved it again. When we get back here,\n\t * the page would have been set to NORMAL but we\n\t * just set it back to HEAD.\n\t *\n\t * How do you detect this? Well, if that happened\n\t * the tail page would have moved.\n\t */\n\tif (ret == RB_PAGE_NORMAL) {\n\t\tstruct buffer_page *buffer_tail_page;\n\n\t\tbuffer_tail_page = READ_ONCE(cpu_buffer->tail_page);\n\t\t/*\n\t\t * If the tail had moved passed next, then we need\n\t\t * to reset the pointer.\n\t\t */\n\t\tif (buffer_tail_page != tail_page &&\n\t\t    buffer_tail_page != next_page)\n\t\t\trb_head_page_set_normal(cpu_buffer, new_head,\n\t\t\t\t\t\tnext_page,\n\t\t\t\t\t\tRB_PAGE_HEAD);\n\t}\n\n\t/*\n\t * If this was the outer most commit (the one that\n\t * changed the original pointer from HEAD to UPDATE),\n\t * then it is up to us to reset it to NORMAL.\n\t */\n\tif (type == RB_PAGE_HEAD) {\n\t\tret = rb_head_page_set_normal(cpu_buffer, next_page,\n\t\t\t\t\t      tail_page,\n\t\t\t\t\t      RB_PAGE_UPDATE);\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t       ret != RB_PAGE_UPDATE))\n\t\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline void\nrb_reset_tail(struct ring_buffer_per_cpu *cpu_buffer,\n\t      unsigned long tail, struct rb_event_info *info)\n{\n\tstruct buffer_page *tail_page = info->tail_page;\n\tstruct ring_buffer_event *event;\n\tunsigned long length = info->length;\n\n\t/*\n\t * Only the event that crossed the page boundary\n\t * must fill the old tail_page with padding.\n\t */\n\tif (tail >= BUF_PAGE_SIZE) {\n\t\t/*\n\t\t * If the page was filled, then we still need\n\t\t * to update the real_end. Reset it to zero\n\t\t * and the reader will ignore it.\n\t\t */\n\t\tif (tail == BUF_PAGE_SIZE)\n\t\t\ttail_page->real_end = 0;\n\n\t\tlocal_sub(length, &tail_page->write);\n\t\treturn;\n\t}\n\n\tevent = __rb_page_index(tail_page, tail);\n\tkmemcheck_annotate_bitfield(event, bitfield);\n\n\t/* account for padding bytes */\n\tlocal_add(BUF_PAGE_SIZE - tail, &cpu_buffer->entries_bytes);\n\n\t/*\n\t * Save the original length to the meta data.\n\t * This will be used by the reader to add lost event\n\t * counter.\n\t */\n\ttail_page->real_end = tail;\n\n\t/*\n\t * If this event is bigger than the minimum size, then\n\t * we need to be careful that we don't subtract the\n\t * write counter enough to allow another writer to slip\n\t * in on this page.\n\t * We put in a discarded commit instead, to make sure\n\t * that this space is not used again.\n\t *\n\t * If we are less than the minimum size, we don't need to\n\t * worry about it.\n\t */\n\tif (tail > (BUF_PAGE_SIZE - RB_EVNT_MIN_SIZE)) {\n\t\t/* No room for any events */\n\n\t\t/* Mark the rest of the page with padding */\n\t\trb_event_set_padding(event);\n\n\t\t/* Set the write back to the previous setting */\n\t\tlocal_sub(length, &tail_page->write);\n\t\treturn;\n\t}\n\n\t/* Put in a discarded event */\n\tevent->array[0] = (BUF_PAGE_SIZE - tail) - RB_EVNT_HDR_SIZE;\n\tevent->type_len = RINGBUF_TYPE_PADDING;\n\t/* time delta must be non zero */\n\tevent->time_delta = 1;\n\n\t/* Set write to end of buffer */\n\tlength = (tail + length) - BUF_PAGE_SIZE;\n\tlocal_sub(length, &tail_page->write);\n}\n\nstatic inline void rb_end_commit(struct ring_buffer_per_cpu *cpu_buffer);\n\n/*\n * This is the slow path, force gcc not to inline it.\n */\nstatic noinline struct ring_buffer_event *\nrb_move_tail(struct ring_buffer_per_cpu *cpu_buffer,\n\t     unsigned long tail, struct rb_event_info *info)\n{\n\tstruct buffer_page *tail_page = info->tail_page;\n\tstruct buffer_page *commit_page = cpu_buffer->commit_page;\n\tstruct ring_buffer *buffer = cpu_buffer->buffer;\n\tstruct buffer_page *next_page;\n\tint ret;\n\n\tnext_page = tail_page;\n\n\trb_inc_page(cpu_buffer, &next_page);\n\n\t/*\n\t * If for some reason, we had an interrupt storm that made\n\t * it all the way around the buffer, bail, and warn\n\t * about it.\n\t */\n\tif (unlikely(next_page == commit_page)) {\n\t\tlocal_inc(&cpu_buffer->commit_overrun);\n\t\tgoto out_reset;\n\t}\n\n\t/*\n\t * This is where the fun begins!\n\t *\n\t * We are fighting against races between a reader that\n\t * could be on another CPU trying to swap its reader\n\t * page with the buffer head.\n\t *\n\t * We are also fighting against interrupts coming in and\n\t * moving the head or tail on us as well.\n\t *\n\t * If the next page is the head page then we have filled\n\t * the buffer, unless the commit page is still on the\n\t * reader page.\n\t */\n\tif (rb_is_head_page(cpu_buffer, next_page, &tail_page->list)) {\n\n\t\t/*\n\t\t * If the commit is not on the reader page, then\n\t\t * move the header page.\n\t\t */\n\t\tif (!rb_is_reader_page(cpu_buffer->commit_page)) {\n\t\t\t/*\n\t\t\t * If we are not in overwrite mode,\n\t\t\t * this is easy, just stop here.\n\t\t\t */\n\t\t\tif (!(buffer->flags & RB_FL_OVERWRITE)) {\n\t\t\t\tlocal_inc(&cpu_buffer->dropped_events);\n\t\t\t\tgoto out_reset;\n\t\t\t}\n\n\t\t\tret = rb_handle_head_page(cpu_buffer,\n\t\t\t\t\t\t  tail_page,\n\t\t\t\t\t\t  next_page);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out_reset;\n\t\t\tif (ret)\n\t\t\t\tgoto out_again;\n\t\t} else {\n\t\t\t/*\n\t\t\t * We need to be careful here too. The\n\t\t\t * commit page could still be on the reader\n\t\t\t * page. We could have a small buffer, and\n\t\t\t * have filled up the buffer with events\n\t\t\t * from interrupts and such, and wrapped.\n\t\t\t *\n\t\t\t * Note, if the tail page is also the on the\n\t\t\t * reader_page, we let it move out.\n\t\t\t */\n\t\t\tif (unlikely((cpu_buffer->commit_page !=\n\t\t\t\t      cpu_buffer->tail_page) &&\n\t\t\t\t     (cpu_buffer->commit_page ==\n\t\t\t\t      cpu_buffer->reader_page))) {\n\t\t\t\tlocal_inc(&cpu_buffer->commit_overrun);\n\t\t\t\tgoto out_reset;\n\t\t\t}\n\t\t}\n\t}\n\n\trb_tail_page_update(cpu_buffer, tail_page, next_page);\n\n out_again:\n\n\trb_reset_tail(cpu_buffer, tail, info);\n\n\t/* Commit what we have for now. */\n\trb_end_commit(cpu_buffer);\n\t/* rb_end_commit() decs committing */\n\tlocal_inc(&cpu_buffer->committing);\n\n\t/* fail and let the caller try again */\n\treturn ERR_PTR(-EAGAIN);\n\n out_reset:\n\t/* reset write */\n\trb_reset_tail(cpu_buffer, tail, info);\n\n\treturn NULL;\n}\n\n/* Slow path, do not inline */\nstatic noinline struct ring_buffer_event *\nrb_add_time_stamp(struct ring_buffer_event *event, u64 delta)\n{\n\tevent->type_len = RINGBUF_TYPE_TIME_EXTEND;\n\n\t/* Not the first event on the page? */\n\tif (rb_event_index(event)) {\n\t\tevent->time_delta = delta & TS_MASK;\n\t\tevent->array[0] = delta >> TS_SHIFT;\n\t} else {\n\t\t/* nope, just zero it */\n\t\tevent->time_delta = 0;\n\t\tevent->array[0] = 0;\n\t}\n\n\treturn skip_time_extend(event);\n}\n\nstatic inline bool rb_event_is_commit(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t     struct ring_buffer_event *event);\n\n/**\n * rb_update_event - update event type and data\n * @event: the event to update\n * @type: the type of event\n * @length: the size of the event field in the ring buffer\n *\n * Update the type and data fields of the event. The length\n * is the actual size that is written to the ring buffer,\n * and with this, we can determine what to place into the\n * data field.\n */\nstatic void\nrb_update_event(struct ring_buffer_per_cpu *cpu_buffer,\n\t\tstruct ring_buffer_event *event,\n\t\tstruct rb_event_info *info)\n{\n\tunsigned length = info->length;\n\tu64 delta = info->delta;\n\n\t/* Only a commit updates the timestamp */\n\tif (unlikely(!rb_event_is_commit(cpu_buffer, event)))\n\t\tdelta = 0;\n\n\t/*\n\t * If we need to add a timestamp, then we\n\t * add it to the start of the resevered space.\n\t */\n\tif (unlikely(info->add_timestamp)) {\n\t\tevent = rb_add_time_stamp(event, delta);\n\t\tlength -= RB_LEN_TIME_EXTEND;\n\t\tdelta = 0;\n\t}\n\n\tevent->time_delta = delta;\n\tlength -= RB_EVNT_HDR_SIZE;\n\tif (length > RB_MAX_SMALL_DATA || RB_FORCE_8BYTE_ALIGNMENT) {\n\t\tevent->type_len = 0;\n\t\tevent->array[0] = length;\n\t} else\n\t\tevent->type_len = DIV_ROUND_UP(length, RB_ALIGNMENT);\n}\n\nstatic unsigned rb_calculate_event_length(unsigned length)\n{\n\tstruct ring_buffer_event event; /* Used only for sizeof array */\n\n\t/* zero length can cause confusions */\n\tif (!length)\n\t\tlength++;\n\n\tif (length > RB_MAX_SMALL_DATA || RB_FORCE_8BYTE_ALIGNMENT)\n\t\tlength += sizeof(event.array[0]);\n\n\tlength += RB_EVNT_HDR_SIZE;\n\tlength = ALIGN(length, RB_ARCH_ALIGNMENT);\n\n\t/*\n\t * In case the time delta is larger than the 27 bits for it\n\t * in the header, we need to add a timestamp. If another\n\t * event comes in when trying to discard this one to increase\n\t * the length, then the timestamp will be added in the allocated\n\t * space of this event. If length is bigger than the size needed\n\t * for the TIME_EXTEND, then padding has to be used. The events\n\t * length must be either RB_LEN_TIME_EXTEND, or greater than or equal\n\t * to RB_LEN_TIME_EXTEND + 8, as 8 is the minimum size for padding.\n\t * As length is a multiple of 4, we only need to worry if it\n\t * is 12 (RB_LEN_TIME_EXTEND + 4).\n\t */\n\tif (length == RB_LEN_TIME_EXTEND + RB_ALIGNMENT)\n\t\tlength += RB_ALIGNMENT;\n\n\treturn length;\n}\n\n#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\nstatic inline bool sched_clock_stable(void)\n{\n\treturn true;\n}\n#endif\n\nstatic inline int\nrb_try_to_discard(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t  struct ring_buffer_event *event)\n{\n\tunsigned long new_index, old_index;\n\tstruct buffer_page *bpage;\n\tunsigned long index;\n\tunsigned long addr;\n\n\tnew_index = rb_event_index(event);\n\told_index = new_index + rb_event_ts_length(event);\n\taddr = (unsigned long)event;\n\taddr &= PAGE_MASK;\n\n\tbpage = READ_ONCE(cpu_buffer->tail_page);\n\n\tif (bpage->page == (void *)addr && rb_page_write(bpage) == old_index) {\n\t\tunsigned long write_mask =\n\t\t\tlocal_read(&bpage->write) & ~RB_WRITE_MASK;\n\t\tunsigned long event_length = rb_event_length(event);\n\t\t/*\n\t\t * This is on the tail page. It is possible that\n\t\t * a write could come in and move the tail page\n\t\t * and write to the next page. That is fine\n\t\t * because we just shorten what is on this page.\n\t\t */\n\t\told_index += write_mask;\n\t\tnew_index += write_mask;\n\t\tindex = local_cmpxchg(&bpage->write, old_index, new_index);\n\t\tif (index == old_index) {\n\t\t\t/* update counters */\n\t\t\tlocal_sub(event_length, &cpu_buffer->entries_bytes);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t/* could not discard */\n\treturn 0;\n}\n\nstatic void rb_start_commit(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tlocal_inc(&cpu_buffer->committing);\n\tlocal_inc(&cpu_buffer->commits);\n}\n\nstatic void\nrb_set_commit_to_write(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tunsigned long max_count;\n\n\t/*\n\t * We only race with interrupts and NMIs on this CPU.\n\t * If we own the commit event, then we can commit\n\t * all others that interrupted us, since the interruptions\n\t * are in stack format (they finish before they come\n\t * back to us). This allows us to do a simple loop to\n\t * assign the commit to the tail.\n\t */\n again:\n\tmax_count = cpu_buffer->nr_pages * 100;\n\n\twhile (cpu_buffer->commit_page != READ_ONCE(cpu_buffer->tail_page)) {\n\t\tif (RB_WARN_ON(cpu_buffer, !(--max_count)))\n\t\t\treturn;\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t       rb_is_reader_page(cpu_buffer->tail_page)))\n\t\t\treturn;\n\t\tlocal_set(&cpu_buffer->commit_page->page->commit,\n\t\t\t  rb_page_write(cpu_buffer->commit_page));\n\t\trb_inc_page(cpu_buffer, &cpu_buffer->commit_page);\n\t\t/* Only update the write stamp if the page has an event */\n\t\tif (rb_page_write(cpu_buffer->commit_page))\n\t\t\tcpu_buffer->write_stamp =\n\t\t\t\tcpu_buffer->commit_page->page->time_stamp;\n\t\t/* add barrier to keep gcc from optimizing too much */\n\t\tbarrier();\n\t}\n\twhile (rb_commit_index(cpu_buffer) !=\n\t       rb_page_write(cpu_buffer->commit_page)) {\n\n\t\tlocal_set(&cpu_buffer->commit_page->page->commit,\n\t\t\t  rb_page_write(cpu_buffer->commit_page));\n\t\tRB_WARN_ON(cpu_buffer,\n\t\t\t   local_read(&cpu_buffer->commit_page->page->commit) &\n\t\t\t   ~RB_WRITE_MASK);\n\t\tbarrier();\n\t}\n\n\t/* again, keep gcc from optimizing */\n\tbarrier();\n\n\t/*\n\t * If an interrupt came in just after the first while loop\n\t * and pushed the tail page forward, we will be left with\n\t * a dangling commit that will never go forward.\n\t */\n\tif (unlikely(cpu_buffer->commit_page != READ_ONCE(cpu_buffer->tail_page)))\n\t\tgoto again;\n}\n\nstatic inline void rb_end_commit(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tunsigned long commits;\n\n\tif (RB_WARN_ON(cpu_buffer,\n\t\t       !local_read(&cpu_buffer->committing)))\n\t\treturn;\n\n again:\n\tcommits = local_read(&cpu_buffer->commits);\n\t/* synchronize with interrupts */\n\tbarrier();\n\tif (local_read(&cpu_buffer->committing) == 1)\n\t\trb_set_commit_to_write(cpu_buffer);\n\n\tlocal_dec(&cpu_buffer->committing);\n\n\t/* synchronize with interrupts */\n\tbarrier();\n\n\t/*\n\t * Need to account for interrupts coming in between the\n\t * updating of the commit page and the clearing of the\n\t * committing counter.\n\t */\n\tif (unlikely(local_read(&cpu_buffer->commits) != commits) &&\n\t    !local_read(&cpu_buffer->committing)) {\n\t\tlocal_inc(&cpu_buffer->committing);\n\t\tgoto again;\n\t}\n}\n\nstatic inline void rb_event_discard(struct ring_buffer_event *event)\n{\n\tif (event->type_len == RINGBUF_TYPE_TIME_EXTEND)\n\t\tevent = skip_time_extend(event);\n\n\t/* array[0] holds the actual length for the discarded event */\n\tevent->array[0] = rb_event_data_length(event) - RB_EVNT_HDR_SIZE;\n\tevent->type_len = RINGBUF_TYPE_PADDING;\n\t/* time delta must be non zero */\n\tif (!event->time_delta)\n\t\tevent->time_delta = 1;\n}\n\nstatic inline bool\nrb_event_is_commit(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t   struct ring_buffer_event *event)\n{\n\tunsigned long addr = (unsigned long)event;\n\tunsigned long index;\n\n\tindex = rb_event_index(event);\n\taddr &= PAGE_MASK;\n\n\treturn cpu_buffer->commit_page->page == (void *)addr &&\n\t\trb_commit_index(cpu_buffer) == index;\n}\n\nstatic void\nrb_update_write_stamp(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t      struct ring_buffer_event *event)\n{\n\tu64 delta;\n\n\t/*\n\t * The event first in the commit queue updates the\n\t * time stamp.\n\t */\n\tif (rb_event_is_commit(cpu_buffer, event)) {\n\t\t/*\n\t\t * A commit event that is first on a page\n\t\t * updates the write timestamp with the page stamp\n\t\t */\n\t\tif (!rb_event_index(event))\n\t\t\tcpu_buffer->write_stamp =\n\t\t\t\tcpu_buffer->commit_page->page->time_stamp;\n\t\telse if (event->type_len == RINGBUF_TYPE_TIME_EXTEND) {\n\t\t\tdelta = event->array[0];\n\t\t\tdelta <<= TS_SHIFT;\n\t\t\tdelta += event->time_delta;\n\t\t\tcpu_buffer->write_stamp += delta;\n\t\t} else\n\t\t\tcpu_buffer->write_stamp += event->time_delta;\n\t}\n}\n\nstatic void rb_commit(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t      struct ring_buffer_event *event)\n{\n\tlocal_inc(&cpu_buffer->entries);\n\trb_update_write_stamp(cpu_buffer, event);\n\trb_end_commit(cpu_buffer);\n}\n\nstatic __always_inline void\nrb_wakeups(struct ring_buffer *buffer, struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tbool pagebusy;\n\n\tif (buffer->irq_work.waiters_pending) {\n\t\tbuffer->irq_work.waiters_pending = false;\n\t\t/* irq_work_queue() supplies it's own memory barriers */\n\t\tirq_work_queue(&buffer->irq_work.work);\n\t}\n\n\tif (cpu_buffer->irq_work.waiters_pending) {\n\t\tcpu_buffer->irq_work.waiters_pending = false;\n\t\t/* irq_work_queue() supplies it's own memory barriers */\n\t\tirq_work_queue(&cpu_buffer->irq_work.work);\n\t}\n\n\tpagebusy = cpu_buffer->reader_page == cpu_buffer->commit_page;\n\n\tif (!pagebusy && cpu_buffer->irq_work.full_waiters_pending) {\n\t\tcpu_buffer->irq_work.wakeup_full = true;\n\t\tcpu_buffer->irq_work.full_waiters_pending = false;\n\t\t/* irq_work_queue() supplies it's own memory barriers */\n\t\tirq_work_queue(&cpu_buffer->irq_work.work);\n\t}\n}\n\n/*\n * The lock and unlock are done within a preempt disable section.\n * The current_context per_cpu variable can only be modified\n * by the current task between lock and unlock. But it can\n * be modified more than once via an interrupt. To pass this\n * information from the lock to the unlock without having to\n * access the 'in_interrupt()' functions again (which do show\n * a bit of overhead in something as critical as function tracing,\n * we use a bitmask trick.\n *\n *  bit 0 =  NMI context\n *  bit 1 =  IRQ context\n *  bit 2 =  SoftIRQ context\n *  bit 3 =  normal context.\n *\n * This works because this is the order of contexts that can\n * preempt other contexts. A SoftIRQ never preempts an IRQ\n * context.\n *\n * When the context is determined, the corresponding bit is\n * checked and set (if it was set, then a recursion of that context\n * happened).\n *\n * On unlock, we need to clear this bit. To do so, just subtract\n * 1 from the current_context and AND it to itself.\n *\n * (binary)\n *  101 - 1 = 100\n *  101 & 100 = 100 (clearing bit zero)\n *\n *  1010 - 1 = 1001\n *  1010 & 1001 = 1000 (clearing bit 1)\n *\n * The least significant bit can be cleared this way, and it\n * just so happens that it is the same bit corresponding to\n * the current context.\n */\n\nstatic __always_inline int\ntrace_recursive_lock(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tunsigned int val = cpu_buffer->current_context;\n\tint bit;\n\n\tif (in_interrupt()) {\n\t\tif (in_nmi())\n\t\t\tbit = RB_CTX_NMI;\n\t\telse if (in_irq())\n\t\t\tbit = RB_CTX_IRQ;\n\t\telse\n\t\t\tbit = RB_CTX_SOFTIRQ;\n\t} else\n\t\tbit = RB_CTX_NORMAL;\n\n\tif (unlikely(val & (1 << bit)))\n\t\treturn 1;\n\n\tval |= (1 << bit);\n\tcpu_buffer->current_context = val;\n\n\treturn 0;\n}\n\nstatic __always_inline void\ntrace_recursive_unlock(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tcpu_buffer->current_context &= cpu_buffer->current_context - 1;\n}\n\n/**\n * ring_buffer_unlock_commit - commit a reserved\n * @buffer: The buffer to commit to\n * @event: The event pointer to commit.\n *\n * This commits the data to the ring buffer, and releases any locks held.\n *\n * Must be paired with ring_buffer_lock_reserve.\n */\nint ring_buffer_unlock_commit(struct ring_buffer *buffer,\n\t\t\t      struct ring_buffer_event *event)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu = raw_smp_processor_id();\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\trb_commit(cpu_buffer, event);\n\n\trb_wakeups(buffer, cpu_buffer);\n\n\ttrace_recursive_unlock(cpu_buffer);\n\n\tpreempt_enable_notrace();\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_unlock_commit);\n\nstatic noinline void\nrb_handle_timestamp(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t    struct rb_event_info *info)\n{\n\tWARN_ONCE(info->delta > (1ULL << 59),\n\t\t  KERN_WARNING \"Delta way too big! %llu ts=%llu write stamp = %llu\\n%s\",\n\t\t  (unsigned long long)info->delta,\n\t\t  (unsigned long long)info->ts,\n\t\t  (unsigned long long)cpu_buffer->write_stamp,\n\t\t  sched_clock_stable() ? \"\" :\n\t\t  \"If you just came from a suspend/resume,\\n\"\n\t\t  \"please switch to the trace global clock:\\n\"\n\t\t  \"  echo global > /sys/kernel/debug/tracing/trace_clock\\n\");\n\tinfo->add_timestamp = 1;\n}\n\nstatic struct ring_buffer_event *\n__rb_reserve_next(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t  struct rb_event_info *info)\n{\n\tstruct ring_buffer_event *event;\n\tstruct buffer_page *tail_page;\n\tunsigned long tail, write;\n\n\t/*\n\t * If the time delta since the last event is too big to\n\t * hold in the time field of the event, then we append a\n\t * TIME EXTEND event ahead of the data event.\n\t */\n\tif (unlikely(info->add_timestamp))\n\t\tinfo->length += RB_LEN_TIME_EXTEND;\n\n\t/* Don't let the compiler play games with cpu_buffer->tail_page */\n\ttail_page = info->tail_page = READ_ONCE(cpu_buffer->tail_page);\n\twrite = local_add_return(info->length, &tail_page->write);\n\n\t/* set write to only the index of the write */\n\twrite &= RB_WRITE_MASK;\n\ttail = write - info->length;\n\n\t/*\n\t * If this is the first commit on the page, then it has the same\n\t * timestamp as the page itself.\n\t */\n\tif (!tail)\n\t\tinfo->delta = 0;\n\n\t/* See if we shot pass the end of this buffer page */\n\tif (unlikely(write > BUF_PAGE_SIZE))\n\t\treturn rb_move_tail(cpu_buffer, tail, info);\n\n\t/* We reserved something on the buffer */\n\n\tevent = __rb_page_index(tail_page, tail);\n\tkmemcheck_annotate_bitfield(event, bitfield);\n\trb_update_event(cpu_buffer, event, info);\n\n\tlocal_inc(&tail_page->entries);\n\n\t/*\n\t * If this is the first commit on the page, then update\n\t * its timestamp.\n\t */\n\tif (!tail)\n\t\ttail_page->page->time_stamp = info->ts;\n\n\t/* account for these added bytes */\n\tlocal_add(info->length, &cpu_buffer->entries_bytes);\n\n\treturn event;\n}\n\nstatic struct ring_buffer_event *\nrb_reserve_next_event(struct ring_buffer *buffer,\n\t\t      struct ring_buffer_per_cpu *cpu_buffer,\n\t\t      unsigned long length)\n{\n\tstruct ring_buffer_event *event;\n\tstruct rb_event_info info;\n\tint nr_loops = 0;\n\tu64 diff;\n\n\trb_start_commit(cpu_buffer);\n\n#ifdef CONFIG_RING_BUFFER_ALLOW_SWAP\n\t/*\n\t * Due to the ability to swap a cpu buffer from a buffer\n\t * it is possible it was swapped before we committed.\n\t * (committing stops a swap). We check for it here and\n\t * if it happened, we have to fail the write.\n\t */\n\tbarrier();\n\tif (unlikely(ACCESS_ONCE(cpu_buffer->buffer) != buffer)) {\n\t\tlocal_dec(&cpu_buffer->committing);\n\t\tlocal_dec(&cpu_buffer->commits);\n\t\treturn NULL;\n\t}\n#endif\n\n\tinfo.length = rb_calculate_event_length(length);\n again:\n\tinfo.add_timestamp = 0;\n\tinfo.delta = 0;\n\n\t/*\n\t * We allow for interrupts to reenter here and do a trace.\n\t * If one does, it will cause this original code to loop\n\t * back here. Even with heavy interrupts happening, this\n\t * should only happen a few times in a row. If this happens\n\t * 1000 times in a row, there must be either an interrupt\n\t * storm or we have something buggy.\n\t * Bail!\n\t */\n\tif (RB_WARN_ON(cpu_buffer, ++nr_loops > 1000))\n\t\tgoto out_fail;\n\n\tinfo.ts = rb_time_stamp(cpu_buffer->buffer);\n\tdiff = info.ts - cpu_buffer->write_stamp;\n\n\t/* make sure this diff is calculated here */\n\tbarrier();\n\n\t/* Did the write stamp get updated already? */\n\tif (likely(info.ts >= cpu_buffer->write_stamp)) {\n\t\tinfo.delta = diff;\n\t\tif (unlikely(test_time_stamp(info.delta)))\n\t\t\trb_handle_timestamp(cpu_buffer, &info);\n\t}\n\n\tevent = __rb_reserve_next(cpu_buffer, &info);\n\n\tif (unlikely(PTR_ERR(event) == -EAGAIN)) {\n\t\tif (info.add_timestamp)\n\t\t\tinfo.length -= RB_LEN_TIME_EXTEND;\n\t\tgoto again;\n\t}\n\n\tif (!event)\n\t\tgoto out_fail;\n\n\treturn event;\n\n out_fail:\n\trb_end_commit(cpu_buffer);\n\treturn NULL;\n}\n\n/**\n * ring_buffer_lock_reserve - reserve a part of the buffer\n * @buffer: the ring buffer to reserve from\n * @length: the length of the data to reserve (excluding event header)\n *\n * Returns a reseverd event on the ring buffer to copy directly to.\n * The user of this interface will need to get the body to write into\n * and can use the ring_buffer_event_data() interface.\n *\n * The length is the length of the data needed, not the event length\n * which also includes the event header.\n *\n * Must be paired with ring_buffer_unlock_commit, unless NULL is returned.\n * If NULL is returned, then nothing has been allocated or locked.\n */\nstruct ring_buffer_event *\nring_buffer_lock_reserve(struct ring_buffer *buffer, unsigned long length)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tint cpu;\n\n\t/* If we are tracing schedule, we don't want to recurse */\n\tpreempt_disable_notrace();\n\n\tif (unlikely(atomic_read(&buffer->record_disabled)))\n\t\tgoto out;\n\n\tcpu = raw_smp_processor_id();\n\n\tif (unlikely(!cpumask_test_cpu(cpu, buffer->cpumask)))\n\t\tgoto out;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\tif (unlikely(atomic_read(&cpu_buffer->record_disabled)))\n\t\tgoto out;\n\n\tif (unlikely(length > BUF_MAX_DATA_SIZE))\n\t\tgoto out;\n\n\tif (unlikely(trace_recursive_lock(cpu_buffer)))\n\t\tgoto out;\n\n\tevent = rb_reserve_next_event(buffer, cpu_buffer, length);\n\tif (!event)\n\t\tgoto out_unlock;\n\n\treturn event;\n\n out_unlock:\n\ttrace_recursive_unlock(cpu_buffer);\n out:\n\tpreempt_enable_notrace();\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_lock_reserve);\n\n/*\n * Decrement the entries to the page that an event is on.\n * The event does not even need to exist, only the pointer\n * to the page it is on. This may only be called before the commit\n * takes place.\n */\nstatic inline void\nrb_decrement_entry(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t   struct ring_buffer_event *event)\n{\n\tunsigned long addr = (unsigned long)event;\n\tstruct buffer_page *bpage = cpu_buffer->commit_page;\n\tstruct buffer_page *start;\n\n\taddr &= PAGE_MASK;\n\n\t/* Do the likely case first */\n\tif (likely(bpage->page == (void *)addr)) {\n\t\tlocal_dec(&bpage->entries);\n\t\treturn;\n\t}\n\n\t/*\n\t * Because the commit page may be on the reader page we\n\t * start with the next page and check the end loop there.\n\t */\n\trb_inc_page(cpu_buffer, &bpage);\n\tstart = bpage;\n\tdo {\n\t\tif (bpage->page == (void *)addr) {\n\t\t\tlocal_dec(&bpage->entries);\n\t\t\treturn;\n\t\t}\n\t\trb_inc_page(cpu_buffer, &bpage);\n\t} while (bpage != start);\n\n\t/* commit not part of this buffer?? */\n\tRB_WARN_ON(cpu_buffer, 1);\n}\n\n/**\n * ring_buffer_commit_discard - discard an event that has not been committed\n * @buffer: the ring buffer\n * @event: non committed event to discard\n *\n * Sometimes an event that is in the ring buffer needs to be ignored.\n * This function lets the user discard an event in the ring buffer\n * and then that event will not be read later.\n *\n * This function only works if it is called before the the item has been\n * committed. It will try to free the event from the ring buffer\n * if another event has not been added behind it.\n *\n * If another event has been added behind it, it will set the event\n * up as discarded, and perform the commit.\n *\n * If this function is called, do not call ring_buffer_unlock_commit on\n * the event.\n */\nvoid ring_buffer_discard_commit(struct ring_buffer *buffer,\n\t\t\t\tstruct ring_buffer_event *event)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\t/* The event is discarded regardless */\n\trb_event_discard(event);\n\n\tcpu = smp_processor_id();\n\tcpu_buffer = buffer->buffers[cpu];\n\n\t/*\n\t * This must only be called if the event has not been\n\t * committed yet. Thus we can assume that preemption\n\t * is still disabled.\n\t */\n\tRB_WARN_ON(buffer, !local_read(&cpu_buffer->committing));\n\n\trb_decrement_entry(cpu_buffer, event);\n\tif (rb_try_to_discard(cpu_buffer, event))\n\t\tgoto out;\n\n\t/*\n\t * The commit is still visible by the reader, so we\n\t * must still update the timestamp.\n\t */\n\trb_update_write_stamp(cpu_buffer, event);\n out:\n\trb_end_commit(cpu_buffer);\n\n\ttrace_recursive_unlock(cpu_buffer);\n\n\tpreempt_enable_notrace();\n\n}\nEXPORT_SYMBOL_GPL(ring_buffer_discard_commit);\n\n/**\n * ring_buffer_write - write data to the buffer without reserving\n * @buffer: The ring buffer to write to.\n * @length: The length of the data being written (excluding the event header)\n * @data: The data to write to the buffer.\n *\n * This is like ring_buffer_lock_reserve and ring_buffer_unlock_commit as\n * one function. If you already have the data to write to the buffer, it\n * may be easier to simply call this function.\n *\n * Note, like ring_buffer_lock_reserve, the length is the length of the data\n * and not the length of the event which would hold the header.\n */\nint ring_buffer_write(struct ring_buffer *buffer,\n\t\t      unsigned long length,\n\t\t      void *data)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tvoid *body;\n\tint ret = -EBUSY;\n\tint cpu;\n\n\tpreempt_disable_notrace();\n\n\tif (atomic_read(&buffer->record_disabled))\n\t\tgoto out;\n\n\tcpu = raw_smp_processor_id();\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\tgoto out;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\tif (atomic_read(&cpu_buffer->record_disabled))\n\t\tgoto out;\n\n\tif (length > BUF_MAX_DATA_SIZE)\n\t\tgoto out;\n\n\tif (unlikely(trace_recursive_lock(cpu_buffer)))\n\t\tgoto out;\n\n\tevent = rb_reserve_next_event(buffer, cpu_buffer, length);\n\tif (!event)\n\t\tgoto out_unlock;\n\n\tbody = rb_event_data(event);\n\n\tmemcpy(body, data, length);\n\n\trb_commit(cpu_buffer, event);\n\n\trb_wakeups(buffer, cpu_buffer);\n\n\tret = 0;\n\n out_unlock:\n\ttrace_recursive_unlock(cpu_buffer);\n\n out:\n\tpreempt_enable_notrace();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_write);\n\nstatic bool rb_per_cpu_empty(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *reader = cpu_buffer->reader_page;\n\tstruct buffer_page *head = rb_set_head_page(cpu_buffer);\n\tstruct buffer_page *commit = cpu_buffer->commit_page;\n\n\t/* In case of error, head will be NULL */\n\tif (unlikely(!head))\n\t\treturn true;\n\n\treturn reader->read == rb_page_commit(reader) &&\n\t\t(commit == reader ||\n\t\t (commit == head &&\n\t\t  head->read == rb_page_commit(commit)));\n}\n\n/**\n * ring_buffer_record_disable - stop all writes into the buffer\n * @buffer: The ring buffer to stop writes to.\n *\n * This prevents all writes to the buffer. Any attempt to write\n * to the buffer after this will fail and return NULL.\n *\n * The caller should call synchronize_sched() after this.\n */\nvoid ring_buffer_record_disable(struct ring_buffer *buffer)\n{\n\tatomic_inc(&buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_disable);\n\n/**\n * ring_buffer_record_enable - enable writes to the buffer\n * @buffer: The ring buffer to enable writes\n *\n * Note, multiple disables will need the same number of enables\n * to truly enable the writing (much like preempt_disable).\n */\nvoid ring_buffer_record_enable(struct ring_buffer *buffer)\n{\n\tatomic_dec(&buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_enable);\n\n/**\n * ring_buffer_record_off - stop all writes into the buffer\n * @buffer: The ring buffer to stop writes to.\n *\n * This prevents all writes to the buffer. Any attempt to write\n * to the buffer after this will fail and return NULL.\n *\n * This is different than ring_buffer_record_disable() as\n * it works like an on/off switch, where as the disable() version\n * must be paired with a enable().\n */\nvoid ring_buffer_record_off(struct ring_buffer *buffer)\n{\n\tunsigned int rd;\n\tunsigned int new_rd;\n\n\tdo {\n\t\trd = atomic_read(&buffer->record_disabled);\n\t\tnew_rd = rd | RB_BUFFER_OFF;\n\t} while (atomic_cmpxchg(&buffer->record_disabled, rd, new_rd) != rd);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_off);\n\n/**\n * ring_buffer_record_on - restart writes into the buffer\n * @buffer: The ring buffer to start writes to.\n *\n * This enables all writes to the buffer that was disabled by\n * ring_buffer_record_off().\n *\n * This is different than ring_buffer_record_enable() as\n * it works like an on/off switch, where as the enable() version\n * must be paired with a disable().\n */\nvoid ring_buffer_record_on(struct ring_buffer *buffer)\n{\n\tunsigned int rd;\n\tunsigned int new_rd;\n\n\tdo {\n\t\trd = atomic_read(&buffer->record_disabled);\n\t\tnew_rd = rd & ~RB_BUFFER_OFF;\n\t} while (atomic_cmpxchg(&buffer->record_disabled, rd, new_rd) != rd);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_on);\n\n/**\n * ring_buffer_record_is_on - return true if the ring buffer can write\n * @buffer: The ring buffer to see if write is enabled\n *\n * Returns true if the ring buffer is in a state that it accepts writes.\n */\nint ring_buffer_record_is_on(struct ring_buffer *buffer)\n{\n\treturn !atomic_read(&buffer->record_disabled);\n}\n\n/**\n * ring_buffer_record_disable_cpu - stop all writes into the cpu_buffer\n * @buffer: The ring buffer to stop writes to.\n * @cpu: The CPU buffer to stop\n *\n * This prevents all writes to the buffer. Any attempt to write\n * to the buffer after this will fail and return NULL.\n *\n * The caller should call synchronize_sched() after this.\n */\nvoid ring_buffer_record_disable_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tatomic_inc(&cpu_buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_disable_cpu);\n\n/**\n * ring_buffer_record_enable_cpu - enable writes to the buffer\n * @buffer: The ring buffer to enable writes\n * @cpu: The CPU to enable.\n *\n * Note, multiple disables will need the same number of enables\n * to truly enable the writing (much like preempt_disable).\n */\nvoid ring_buffer_record_enable_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tatomic_dec(&cpu_buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_enable_cpu);\n\n/*\n * The total entries in the ring buffer is the running counter\n * of entries entered into the ring buffer, minus the sum of\n * the entries read from the ring buffer and the number of\n * entries that were overwritten.\n */\nstatic inline unsigned long\nrb_num_of_entries(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn local_read(&cpu_buffer->entries) -\n\t\t(local_read(&cpu_buffer->overrun) + cpu_buffer->read);\n}\n\n/**\n * ring_buffer_oldest_event_ts - get the oldest event timestamp from the buffer\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to read from.\n */\nu64 ring_buffer_oldest_event_ts(struct ring_buffer *buffer, int cpu)\n{\n\tunsigned long flags;\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct buffer_page *bpage;\n\tu64 ret = 0;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\t/*\n\t * if the tail is on reader_page, oldest time stamp is on the reader\n\t * page\n\t */\n\tif (cpu_buffer->tail_page == cpu_buffer->reader_page)\n\t\tbpage = cpu_buffer->reader_page;\n\telse\n\t\tbpage = rb_set_head_page(cpu_buffer);\n\tif (bpage)\n\t\tret = bpage->page->time_stamp;\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_oldest_event_ts);\n\n/**\n * ring_buffer_bytes_cpu - get the number of bytes consumed in a cpu buffer\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to read from.\n */\nunsigned long ring_buffer_bytes_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->entries_bytes) - cpu_buffer->read_bytes;\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_bytes_cpu);\n\n/**\n * ring_buffer_entries_cpu - get the number of entries in a cpu buffer\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to get the entries from.\n */\nunsigned long ring_buffer_entries_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\treturn rb_num_of_entries(cpu_buffer);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_entries_cpu);\n\n/**\n * ring_buffer_overrun_cpu - get the number of overruns caused by the ring\n * buffer wrapping around (only if RB_FL_OVERWRITE is on).\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to get the number of overruns from\n */\nunsigned long ring_buffer_overrun_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->overrun);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_overrun_cpu);\n\n/**\n * ring_buffer_commit_overrun_cpu - get the number of overruns caused by\n * commits failing due to the buffer wrapping around while there are uncommitted\n * events, such as during an interrupt storm.\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to get the number of overruns from\n */\nunsigned long\nring_buffer_commit_overrun_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->commit_overrun);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_commit_overrun_cpu);\n\n/**\n * ring_buffer_dropped_events_cpu - get the number of dropped events caused by\n * the ring buffer filling up (only if RB_FL_OVERWRITE is off).\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to get the number of overruns from\n */\nunsigned long\nring_buffer_dropped_events_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->dropped_events);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_dropped_events_cpu);\n\n/**\n * ring_buffer_read_events_cpu - get the number of events successfully read\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to get the number of events read\n */\nunsigned long\nring_buffer_read_events_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\treturn cpu_buffer->read;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_events_cpu);\n\n/**\n * ring_buffer_entries - get the number of entries in a buffer\n * @buffer: The ring buffer\n *\n * Returns the total number of entries in the ring buffer\n * (all CPU entries)\n */\nunsigned long ring_buffer_entries(struct ring_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long entries = 0;\n\tint cpu;\n\n\t/* if you care about this being correct, lock the buffer */\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tentries += rb_num_of_entries(cpu_buffer);\n\t}\n\n\treturn entries;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_entries);\n\n/**\n * ring_buffer_overruns - get the number of overruns in buffer\n * @buffer: The ring buffer\n *\n * Returns the total number of overruns in the ring buffer\n * (all CPU entries)\n */\nunsigned long ring_buffer_overruns(struct ring_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long overruns = 0;\n\tint cpu;\n\n\t/* if you care about this being correct, lock the buffer */\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\toverruns += local_read(&cpu_buffer->overrun);\n\t}\n\n\treturn overruns;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_overruns);\n\nstatic void rb_iter_reset(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\n\t/* Iterator usage is expected to have record disabled */\n\titer->head_page = cpu_buffer->reader_page;\n\titer->head = cpu_buffer->reader_page->read;\n\n\titer->cache_reader_page = iter->head_page;\n\titer->cache_read = cpu_buffer->read;\n\n\tif (iter->head)\n\t\titer->read_stamp = cpu_buffer->read_stamp;\n\telse\n\t\titer->read_stamp = iter->head_page->page->time_stamp;\n}\n\n/**\n * ring_buffer_iter_reset - reset an iterator\n * @iter: The iterator to reset\n *\n * Resets the iterator, so that it will start from the beginning\n * again.\n */\nvoid ring_buffer_iter_reset(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\n\tif (!iter)\n\t\treturn;\n\n\tcpu_buffer = iter->cpu_buffer;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\trb_iter_reset(iter);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_iter_reset);\n\n/**\n * ring_buffer_iter_empty - check if an iterator has no more to read\n * @iter: The iterator to check\n */\nint ring_buffer_iter_empty(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tcpu_buffer = iter->cpu_buffer;\n\n\treturn iter->head_page == cpu_buffer->commit_page &&\n\t\titer->head == rb_commit_index(cpu_buffer);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_iter_empty);\n\nstatic void\nrb_update_read_stamp(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t     struct ring_buffer_event *event)\n{\n\tu64 delta;\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\tdelta = event->array[0];\n\t\tdelta <<= TS_SHIFT;\n\t\tdelta += event->time_delta;\n\t\tcpu_buffer->read_stamp += delta;\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\t/* FIXME: not implemented */\n\t\treturn;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\tcpu_buffer->read_stamp += event->time_delta;\n\t\treturn;\n\n\tdefault:\n\t\tBUG();\n\t}\n\treturn;\n}\n\nstatic void\nrb_update_iter_read_stamp(struct ring_buffer_iter *iter,\n\t\t\t  struct ring_buffer_event *event)\n{\n\tu64 delta;\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\tdelta = event->array[0];\n\t\tdelta <<= TS_SHIFT;\n\t\tdelta += event->time_delta;\n\t\titer->read_stamp += delta;\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\t/* FIXME: not implemented */\n\t\treturn;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\titer->read_stamp += event->time_delta;\n\t\treturn;\n\n\tdefault:\n\t\tBUG();\n\t}\n\treturn;\n}\n\nstatic struct buffer_page *\nrb_get_reader_page(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *reader = NULL;\n\tunsigned long overwrite;\n\tunsigned long flags;\n\tint nr_loops = 0;\n\tint ret;\n\n\tlocal_irq_save(flags);\n\tarch_spin_lock(&cpu_buffer->lock);\n\n again:\n\t/*\n\t * This should normally only loop twice. But because the\n\t * start of the reader inserts an empty page, it causes\n\t * a case where we will loop three times. There should be no\n\t * reason to loop four times (that I know of).\n\t */\n\tif (RB_WARN_ON(cpu_buffer, ++nr_loops > 3)) {\n\t\treader = NULL;\n\t\tgoto out;\n\t}\n\n\treader = cpu_buffer->reader_page;\n\n\t/* If there's more to read, return this page */\n\tif (cpu_buffer->reader_page->read < rb_page_size(reader))\n\t\tgoto out;\n\n\t/* Never should we have an index greater than the size */\n\tif (RB_WARN_ON(cpu_buffer,\n\t\t       cpu_buffer->reader_page->read > rb_page_size(reader)))\n\t\tgoto out;\n\n\t/* check if we caught up to the tail */\n\treader = NULL;\n\tif (cpu_buffer->commit_page == cpu_buffer->reader_page)\n\t\tgoto out;\n\n\t/* Don't bother swapping if the ring buffer is empty */\n\tif (rb_num_of_entries(cpu_buffer) == 0)\n\t\tgoto out;\n\n\t/*\n\t * Reset the reader page to size zero.\n\t */\n\tlocal_set(&cpu_buffer->reader_page->write, 0);\n\tlocal_set(&cpu_buffer->reader_page->entries, 0);\n\tlocal_set(&cpu_buffer->reader_page->page->commit, 0);\n\tcpu_buffer->reader_page->real_end = 0;\n\n spin:\n\t/*\n\t * Splice the empty reader page into the list around the head.\n\t */\n\treader = rb_set_head_page(cpu_buffer);\n\tif (!reader)\n\t\tgoto out;\n\tcpu_buffer->reader_page->list.next = rb_list_head(reader->list.next);\n\tcpu_buffer->reader_page->list.prev = reader->list.prev;\n\n\t/*\n\t * cpu_buffer->pages just needs to point to the buffer, it\n\t *  has no specific buffer page to point to. Lets move it out\n\t *  of our way so we don't accidentally swap it.\n\t */\n\tcpu_buffer->pages = reader->list.prev;\n\n\t/* The reader page will be pointing to the new head */\n\trb_set_list_to_head(cpu_buffer, &cpu_buffer->reader_page->list);\n\n\t/*\n\t * We want to make sure we read the overruns after we set up our\n\t * pointers to the next object. The writer side does a\n\t * cmpxchg to cross pages which acts as the mb on the writer\n\t * side. Note, the reader will constantly fail the swap\n\t * while the writer is updating the pointers, so this\n\t * guarantees that the overwrite recorded here is the one we\n\t * want to compare with the last_overrun.\n\t */\n\tsmp_mb();\n\toverwrite = local_read(&(cpu_buffer->overrun));\n\n\t/*\n\t * Here's the tricky part.\n\t *\n\t * We need to move the pointer past the header page.\n\t * But we can only do that if a writer is not currently\n\t * moving it. The page before the header page has the\n\t * flag bit '1' set if it is pointing to the page we want.\n\t * but if the writer is in the process of moving it\n\t * than it will be '2' or already moved '0'.\n\t */\n\n\tret = rb_head_page_replace(reader, cpu_buffer->reader_page);\n\n\t/*\n\t * If we did not convert it, then we must try again.\n\t */\n\tif (!ret)\n\t\tgoto spin;\n\n\t/*\n\t * Yeah! We succeeded in replacing the page.\n\t *\n\t * Now make the new head point back to the reader page.\n\t */\n\trb_list_head(reader->list.next)->prev = &cpu_buffer->reader_page->list;\n\trb_inc_page(cpu_buffer, &cpu_buffer->head_page);\n\n\t/* Finally update the reader page to the new head */\n\tcpu_buffer->reader_page = reader;\n\tcpu_buffer->reader_page->read = 0;\n\n\tif (overwrite != cpu_buffer->last_overrun) {\n\t\tcpu_buffer->lost_events = overwrite - cpu_buffer->last_overrun;\n\t\tcpu_buffer->last_overrun = overwrite;\n\t}\n\n\tgoto again;\n\n out:\n\t/* Update the read_stamp on the first event */\n\tif (reader && reader->read == 0)\n\t\tcpu_buffer->read_stamp = reader->page->time_stamp;\n\n\tarch_spin_unlock(&cpu_buffer->lock);\n\tlocal_irq_restore(flags);\n\n\treturn reader;\n}\n\nstatic void rb_advance_reader(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct ring_buffer_event *event;\n\tstruct buffer_page *reader;\n\tunsigned length;\n\n\treader = rb_get_reader_page(cpu_buffer);\n\n\t/* This function should not be called when buffer is empty */\n\tif (RB_WARN_ON(cpu_buffer, !reader))\n\t\treturn;\n\n\tevent = rb_reader_event(cpu_buffer);\n\n\tif (event->type_len <= RINGBUF_TYPE_DATA_TYPE_LEN_MAX)\n\t\tcpu_buffer->read++;\n\n\trb_update_read_stamp(cpu_buffer, event);\n\n\tlength = rb_event_length(event);\n\tcpu_buffer->reader_page->read += length;\n}\n\nstatic void rb_advance_iter(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tunsigned length;\n\n\tcpu_buffer = iter->cpu_buffer;\n\n\t/*\n\t * Check if we are at the end of the buffer.\n\t */\n\tif (iter->head >= rb_page_size(iter->head_page)) {\n\t\t/* discarded commits can make the page empty */\n\t\tif (iter->head_page == cpu_buffer->commit_page)\n\t\t\treturn;\n\t\trb_inc_iter(iter);\n\t\treturn;\n\t}\n\n\tevent = rb_iter_head_event(iter);\n\n\tlength = rb_event_length(event);\n\n\t/*\n\t * This should not be called to advance the header if we are\n\t * at the tail of the buffer.\n\t */\n\tif (RB_WARN_ON(cpu_buffer,\n\t\t       (iter->head_page == cpu_buffer->commit_page) &&\n\t\t       (iter->head + length > rb_commit_index(cpu_buffer))))\n\t\treturn;\n\n\trb_update_iter_read_stamp(iter, event);\n\n\titer->head += length;\n\n\t/* check for end of page padding */\n\tif ((iter->head >= rb_page_size(iter->head_page)) &&\n\t    (iter->head_page != cpu_buffer->commit_page))\n\t\trb_inc_iter(iter);\n}\n\nstatic int rb_lost_events(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn cpu_buffer->lost_events;\n}\n\nstatic struct ring_buffer_event *\nrb_buffer_peek(struct ring_buffer_per_cpu *cpu_buffer, u64 *ts,\n\t       unsigned long *lost_events)\n{\n\tstruct ring_buffer_event *event;\n\tstruct buffer_page *reader;\n\tint nr_loops = 0;\n\n again:\n\t/*\n\t * We repeat when a time extend is encountered.\n\t * Since the time extend is always attached to a data event,\n\t * we should never loop more than once.\n\t * (We never hit the following condition more than twice).\n\t */\n\tif (RB_WARN_ON(cpu_buffer, ++nr_loops > 2))\n\t\treturn NULL;\n\n\treader = rb_get_reader_page(cpu_buffer);\n\tif (!reader)\n\t\treturn NULL;\n\n\tevent = rb_reader_event(cpu_buffer);\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\tif (rb_null_event(event))\n\t\t\tRB_WARN_ON(cpu_buffer, 1);\n\t\t/*\n\t\t * Because the writer could be discarding every\n\t\t * event it creates (which would probably be bad)\n\t\t * if we were to go back to \"again\" then we may never\n\t\t * catch up, and will trigger the warn on, or lock\n\t\t * the box. Return the padding, and we will release\n\t\t * the current locks, and try again.\n\t\t */\n\t\treturn event;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\t/* Internal data, OK to advance */\n\t\trb_advance_reader(cpu_buffer);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\t/* FIXME: not implemented */\n\t\trb_advance_reader(cpu_buffer);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\tif (ts) {\n\t\t\t*ts = cpu_buffer->read_stamp + event->time_delta;\n\t\t\tring_buffer_normalize_time_stamp(cpu_buffer->buffer,\n\t\t\t\t\t\t\t cpu_buffer->cpu, ts);\n\t\t}\n\t\tif (lost_events)\n\t\t\t*lost_events = rb_lost_events(cpu_buffer);\n\t\treturn event;\n\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_peek);\n\nstatic struct ring_buffer_event *\nrb_iter_peek(struct ring_buffer_iter *iter, u64 *ts)\n{\n\tstruct ring_buffer *buffer;\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tint nr_loops = 0;\n\n\tcpu_buffer = iter->cpu_buffer;\n\tbuffer = cpu_buffer->buffer;\n\n\t/*\n\t * Check if someone performed a consuming read to\n\t * the buffer. A consuming read invalidates the iterator\n\t * and we need to reset the iterator in this case.\n\t */\n\tif (unlikely(iter->cache_read != cpu_buffer->read ||\n\t\t     iter->cache_reader_page != cpu_buffer->reader_page))\n\t\trb_iter_reset(iter);\n\n again:\n\tif (ring_buffer_iter_empty(iter))\n\t\treturn NULL;\n\n\t/*\n\t * We repeat when a time extend is encountered or we hit\n\t * the end of the page. Since the time extend is always attached\n\t * to a data event, we should never loop more than three times.\n\t * Once for going to next page, once on time extend, and\n\t * finally once to get the event.\n\t * (We never hit the following condition more than thrice).\n\t */\n\tif (RB_WARN_ON(cpu_buffer, ++nr_loops > 3))\n\t\treturn NULL;\n\n\tif (rb_per_cpu_empty(cpu_buffer))\n\t\treturn NULL;\n\n\tif (iter->head >= rb_page_size(iter->head_page)) {\n\t\trb_inc_iter(iter);\n\t\tgoto again;\n\t}\n\n\tevent = rb_iter_head_event(iter);\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\tif (rb_null_event(event)) {\n\t\t\trb_inc_iter(iter);\n\t\t\tgoto again;\n\t\t}\n\t\trb_advance_iter(iter);\n\t\treturn event;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\t/* Internal data, OK to advance */\n\t\trb_advance_iter(iter);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\t/* FIXME: not implemented */\n\t\trb_advance_iter(iter);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\tif (ts) {\n\t\t\t*ts = iter->read_stamp + event->time_delta;\n\t\t\tring_buffer_normalize_time_stamp(buffer,\n\t\t\t\t\t\t\t cpu_buffer->cpu, ts);\n\t\t}\n\t\treturn event;\n\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_iter_peek);\n\nstatic inline bool rb_reader_lock(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tif (likely(!in_nmi())) {\n\t\traw_spin_lock(&cpu_buffer->reader_lock);\n\t\treturn true;\n\t}\n\n\t/*\n\t * If an NMI die dumps out the content of the ring buffer\n\t * trylock must be used to prevent a deadlock if the NMI\n\t * preempted a task that holds the ring buffer locks. If\n\t * we get the lock then all is fine, if not, then continue\n\t * to do the read, but this can corrupt the ring buffer,\n\t * so it must be permanently disabled from future writes.\n\t * Reading from NMI is a oneshot deal.\n\t */\n\tif (raw_spin_trylock(&cpu_buffer->reader_lock))\n\t\treturn true;\n\n\t/* Continue without locking, but disable the ring buffer */\n\tatomic_inc(&cpu_buffer->record_disabled);\n\treturn false;\n}\n\nstatic inline void\nrb_reader_unlock(struct ring_buffer_per_cpu *cpu_buffer, bool locked)\n{\n\tif (likely(locked))\n\t\traw_spin_unlock(&cpu_buffer->reader_lock);\n\treturn;\n}\n\n/**\n * ring_buffer_peek - peek at the next event to be read\n * @buffer: The ring buffer to read\n * @cpu: The cpu to peak at\n * @ts: The timestamp counter of this event.\n * @lost_events: a variable to store if events were lost (may be NULL)\n *\n * This will return the event that will be read next, but does\n * not consume the data.\n */\nstruct ring_buffer_event *\nring_buffer_peek(struct ring_buffer *buffer, int cpu, u64 *ts,\n\t\t unsigned long *lost_events)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\tstruct ring_buffer_event *event;\n\tunsigned long flags;\n\tbool dolock;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn NULL;\n\n again:\n\tlocal_irq_save(flags);\n\tdolock = rb_reader_lock(cpu_buffer);\n\tevent = rb_buffer_peek(cpu_buffer, ts, lost_events);\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\trb_advance_reader(cpu_buffer);\n\trb_reader_unlock(cpu_buffer, dolock);\n\tlocal_irq_restore(flags);\n\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\tgoto again;\n\n\treturn event;\n}\n\n/**\n * ring_buffer_iter_peek - peek at the next event to be read\n * @iter: The ring buffer iterator\n * @ts: The timestamp counter of this event.\n *\n * This will return the event that will be read next, but does\n * not increment the iterator.\n */\nstruct ring_buffer_event *\nring_buffer_iter_peek(struct ring_buffer_iter *iter, u64 *ts)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tunsigned long flags;\n\n again:\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\tevent = rb_iter_peek(iter, ts);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\tgoto again;\n\n\treturn event;\n}\n\n/**\n * ring_buffer_consume - return an event and consume it\n * @buffer: The ring buffer to get the next event from\n * @cpu: the cpu to read the buffer from\n * @ts: a variable to store the timestamp (may be NULL)\n * @lost_events: a variable to store if events were lost (may be NULL)\n *\n * Returns the next event in the ring buffer, and that event is consumed.\n * Meaning, that sequential reads will keep returning a different event,\n * and eventually empty the ring buffer if the producer is slower.\n */\nstruct ring_buffer_event *\nring_buffer_consume(struct ring_buffer *buffer, int cpu, u64 *ts,\n\t\t    unsigned long *lost_events)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event = NULL;\n\tunsigned long flags;\n\tbool dolock;\n\n again:\n\t/* might be called in atomic */\n\tpreempt_disable();\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\tgoto out;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tlocal_irq_save(flags);\n\tdolock = rb_reader_lock(cpu_buffer);\n\n\tevent = rb_buffer_peek(cpu_buffer, ts, lost_events);\n\tif (event) {\n\t\tcpu_buffer->lost_events = 0;\n\t\trb_advance_reader(cpu_buffer);\n\t}\n\n\trb_reader_unlock(cpu_buffer, dolock);\n\tlocal_irq_restore(flags);\n\n out:\n\tpreempt_enable();\n\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\tgoto again;\n\n\treturn event;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_consume);\n\n/**\n * ring_buffer_read_prepare - Prepare for a non consuming read of the buffer\n * @buffer: The ring buffer to read from\n * @cpu: The cpu buffer to iterate over\n *\n * This performs the initial preparations necessary to iterate\n * through the buffer.  Memory is allocated, buffer recording\n * is disabled, and the iterator pointer is returned to the caller.\n *\n * Disabling buffer recordng prevents the reading from being\n * corrupted. This is not a consuming read, so a producer is not\n * expected.\n *\n * After a sequence of ring_buffer_read_prepare calls, the user is\n * expected to make at least one call to ring_buffer_read_prepare_sync.\n * Afterwards, ring_buffer_read_start is invoked to get things going\n * for real.\n *\n * This overall must be paired with ring_buffer_read_finish.\n */\nstruct ring_buffer_iter *\nring_buffer_read_prepare(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_iter *iter;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn NULL;\n\n\titer = kmalloc(sizeof(*iter), GFP_KERNEL);\n\tif (!iter)\n\t\treturn NULL;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\titer->cpu_buffer = cpu_buffer;\n\n\tatomic_inc(&buffer->resize_disabled);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\n\treturn iter;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_prepare);\n\n/**\n * ring_buffer_read_prepare_sync - Synchronize a set of prepare calls\n *\n * All previously invoked ring_buffer_read_prepare calls to prepare\n * iterators will be synchronized.  Afterwards, read_buffer_read_start\n * calls on those iterators are allowed.\n */\nvoid\nring_buffer_read_prepare_sync(void)\n{\n\tsynchronize_sched();\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_prepare_sync);\n\n/**\n * ring_buffer_read_start - start a non consuming read of the buffer\n * @iter: The iterator returned by ring_buffer_read_prepare\n *\n * This finalizes the startup of an iteration through the buffer.\n * The iterator comes from a call to ring_buffer_read_prepare and\n * an intervening ring_buffer_read_prepare_sync must have been\n * performed.\n *\n * Must be paired with ring_buffer_read_finish.\n */\nvoid\nring_buffer_read_start(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\n\tif (!iter)\n\t\treturn;\n\n\tcpu_buffer = iter->cpu_buffer;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\tarch_spin_lock(&cpu_buffer->lock);\n\trb_iter_reset(iter);\n\tarch_spin_unlock(&cpu_buffer->lock);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_start);\n\n/**\n * ring_buffer_read_finish - finish reading the iterator of the buffer\n * @iter: The iterator retrieved by ring_buffer_start\n *\n * This re-enables the recording to the buffer, and frees the\n * iterator.\n */\nvoid\nring_buffer_read_finish(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\tunsigned long flags;\n\n\t/*\n\t * Ring buffer is disabled from recording, here's a good place\n\t * to check the integrity of the ring buffer.\n\t * Must prevent readers from trying to read, as the check\n\t * clears the HEAD page and readers require it.\n\t */\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\trb_check_pages(cpu_buffer);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\tatomic_dec(&cpu_buffer->record_disabled);\n\tatomic_dec(&cpu_buffer->buffer->resize_disabled);\n\tkfree(iter);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_finish);\n\n/**\n * ring_buffer_read - read the next item in the ring buffer by the iterator\n * @iter: The ring buffer iterator\n * @ts: The time stamp of the event read.\n *\n * This reads the next event in the ring buffer and increments the iterator.\n */\nstruct ring_buffer_event *\nring_buffer_read(struct ring_buffer_iter *iter, u64 *ts)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n again:\n\tevent = rb_iter_peek(iter, ts);\n\tif (!event)\n\t\tgoto out;\n\n\tif (event->type_len == RINGBUF_TYPE_PADDING)\n\t\tgoto again;\n\n\trb_advance_iter(iter);\n out:\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\treturn event;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read);\n\n/**\n * ring_buffer_size - return the size of the ring buffer (in bytes)\n * @buffer: The ring buffer.\n */\nunsigned long ring_buffer_size(struct ring_buffer *buffer, int cpu)\n{\n\t/*\n\t * Earlier, this method returned\n\t *\tBUF_PAGE_SIZE * buffer->nr_pages\n\t * Since the nr_pages field is now removed, we have converted this to\n\t * return the per cpu buffer value.\n\t */\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\treturn BUF_PAGE_SIZE * buffer->buffers[cpu]->nr_pages;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_size);\n\nstatic void\nrb_reset_cpu(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\trb_head_page_deactivate(cpu_buffer);\n\n\tcpu_buffer->head_page\n\t\t= list_entry(cpu_buffer->pages, struct buffer_page, list);\n\tlocal_set(&cpu_buffer->head_page->write, 0);\n\tlocal_set(&cpu_buffer->head_page->entries, 0);\n\tlocal_set(&cpu_buffer->head_page->page->commit, 0);\n\n\tcpu_buffer->head_page->read = 0;\n\n\tcpu_buffer->tail_page = cpu_buffer->head_page;\n\tcpu_buffer->commit_page = cpu_buffer->head_page;\n\n\tINIT_LIST_HEAD(&cpu_buffer->reader_page->list);\n\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\tlocal_set(&cpu_buffer->reader_page->write, 0);\n\tlocal_set(&cpu_buffer->reader_page->entries, 0);\n\tlocal_set(&cpu_buffer->reader_page->page->commit, 0);\n\tcpu_buffer->reader_page->read = 0;\n\n\tlocal_set(&cpu_buffer->entries_bytes, 0);\n\tlocal_set(&cpu_buffer->overrun, 0);\n\tlocal_set(&cpu_buffer->commit_overrun, 0);\n\tlocal_set(&cpu_buffer->dropped_events, 0);\n\tlocal_set(&cpu_buffer->entries, 0);\n\tlocal_set(&cpu_buffer->committing, 0);\n\tlocal_set(&cpu_buffer->commits, 0);\n\tcpu_buffer->read = 0;\n\tcpu_buffer->read_bytes = 0;\n\n\tcpu_buffer->write_stamp = 0;\n\tcpu_buffer->read_stamp = 0;\n\n\tcpu_buffer->lost_events = 0;\n\tcpu_buffer->last_overrun = 0;\n\n\trb_head_page_activate(cpu_buffer);\n}\n\n/**\n * ring_buffer_reset_cpu - reset a ring buffer per CPU buffer\n * @buffer: The ring buffer to reset a per cpu buffer of\n * @cpu: The CPU buffer to be reset\n */\nvoid ring_buffer_reset_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\tunsigned long flags;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\tatomic_inc(&buffer->resize_disabled);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_sched();\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\n\tif (RB_WARN_ON(cpu_buffer, local_read(&cpu_buffer->committing)))\n\t\tgoto out;\n\n\tarch_spin_lock(&cpu_buffer->lock);\n\n\trb_reset_cpu(cpu_buffer);\n\n\tarch_spin_unlock(&cpu_buffer->lock);\n\n out:\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\tatomic_dec(&cpu_buffer->record_disabled);\n\tatomic_dec(&buffer->resize_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_reset_cpu);\n\n/**\n * ring_buffer_reset - reset a ring buffer\n * @buffer: The ring buffer to reset all cpu buffers\n */\nvoid ring_buffer_reset(struct ring_buffer *buffer)\n{\n\tint cpu;\n\n\tfor_each_buffer_cpu(buffer, cpu)\n\t\tring_buffer_reset_cpu(buffer, cpu);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_reset);\n\n/**\n * rind_buffer_empty - is the ring buffer empty?\n * @buffer: The ring buffer to test\n */\nbool ring_buffer_empty(struct ring_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\tbool dolock;\n\tint cpu;\n\tint ret;\n\n\t/* yes this is racy, but if you don't like the race, lock the buffer */\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tlocal_irq_save(flags);\n\t\tdolock = rb_reader_lock(cpu_buffer);\n\t\tret = rb_per_cpu_empty(cpu_buffer);\n\t\trb_reader_unlock(cpu_buffer, dolock);\n\t\tlocal_irq_restore(flags);\n\n\t\tif (!ret)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_empty);\n\n/**\n * ring_buffer_empty_cpu - is a cpu buffer of a ring buffer empty?\n * @buffer: The ring buffer\n * @cpu: The CPU buffer to test\n */\nbool ring_buffer_empty_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\tbool dolock;\n\tint ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn true;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tlocal_irq_save(flags);\n\tdolock = rb_reader_lock(cpu_buffer);\n\tret = rb_per_cpu_empty(cpu_buffer);\n\trb_reader_unlock(cpu_buffer, dolock);\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_empty_cpu);\n\n#ifdef CONFIG_RING_BUFFER_ALLOW_SWAP\n/**\n * ring_buffer_swap_cpu - swap a CPU buffer between two ring buffers\n * @buffer_a: One buffer to swap with\n * @buffer_b: The other buffer to swap with\n *\n * This function is useful for tracers that want to take a \"snapshot\"\n * of a CPU buffer and has another back up buffer lying around.\n * it is expected that the tracer handles the cpu buffer not being\n * used at the moment.\n */\nint ring_buffer_swap_cpu(struct ring_buffer *buffer_a,\n\t\t\t struct ring_buffer *buffer_b, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer_a;\n\tstruct ring_buffer_per_cpu *cpu_buffer_b;\n\tint ret = -EINVAL;\n\n\tif (!cpumask_test_cpu(cpu, buffer_a->cpumask) ||\n\t    !cpumask_test_cpu(cpu, buffer_b->cpumask))\n\t\tgoto out;\n\n\tcpu_buffer_a = buffer_a->buffers[cpu];\n\tcpu_buffer_b = buffer_b->buffers[cpu];\n\n\t/* At least make sure the two buffers are somewhat the same */\n\tif (cpu_buffer_a->nr_pages != cpu_buffer_b->nr_pages)\n\t\tgoto out;\n\n\tret = -EAGAIN;\n\n\tif (atomic_read(&buffer_a->record_disabled))\n\t\tgoto out;\n\n\tif (atomic_read(&buffer_b->record_disabled))\n\t\tgoto out;\n\n\tif (atomic_read(&cpu_buffer_a->record_disabled))\n\t\tgoto out;\n\n\tif (atomic_read(&cpu_buffer_b->record_disabled))\n\t\tgoto out;\n\n\t/*\n\t * We can't do a synchronize_sched here because this\n\t * function can be called in atomic context.\n\t * Normally this will be called from the same CPU as cpu.\n\t * If not it's up to the caller to protect this.\n\t */\n\tatomic_inc(&cpu_buffer_a->record_disabled);\n\tatomic_inc(&cpu_buffer_b->record_disabled);\n\n\tret = -EBUSY;\n\tif (local_read(&cpu_buffer_a->committing))\n\t\tgoto out_dec;\n\tif (local_read(&cpu_buffer_b->committing))\n\t\tgoto out_dec;\n\n\tbuffer_a->buffers[cpu] = cpu_buffer_b;\n\tbuffer_b->buffers[cpu] = cpu_buffer_a;\n\n\tcpu_buffer_b->buffer = buffer_a;\n\tcpu_buffer_a->buffer = buffer_b;\n\n\tret = 0;\n\nout_dec:\n\tatomic_dec(&cpu_buffer_a->record_disabled);\n\tatomic_dec(&cpu_buffer_b->record_disabled);\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_swap_cpu);\n#endif /* CONFIG_RING_BUFFER_ALLOW_SWAP */\n\n/**\n * ring_buffer_alloc_read_page - allocate a page to read from buffer\n * @buffer: the buffer to allocate for.\n * @cpu: the cpu buffer to allocate.\n *\n * This function is used in conjunction with ring_buffer_read_page.\n * When reading a full page from the ring buffer, these functions\n * can be used to speed up the process. The calling function should\n * allocate a few pages first with this function. Then when it\n * needs to get pages from the ring buffer, it passes the result\n * of this function into ring_buffer_read_page, which will swap\n * the page that was allocated, with the read page of the buffer.\n *\n * Returns:\n *  The page allocated, or NULL on error.\n */\nvoid *ring_buffer_alloc_read_page(struct ring_buffer *buffer, int cpu)\n{\n\tstruct buffer_data_page *bpage;\n\tstruct page *page;\n\n\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\tif (!page)\n\t\treturn NULL;\n\n\tbpage = page_address(page);\n\n\trb_init_page(bpage);\n\n\treturn bpage;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_alloc_read_page);\n\n/**\n * ring_buffer_free_read_page - free an allocated read page\n * @buffer: the buffer the page was allocate for\n * @data: the page to free\n *\n * Free a page allocated from ring_buffer_alloc_read_page.\n */\nvoid ring_buffer_free_read_page(struct ring_buffer *buffer, void *data)\n{\n\tfree_page((unsigned long)data);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_free_read_page);\n\n/**\n * ring_buffer_read_page - extract a page from the ring buffer\n * @buffer: buffer to extract from\n * @data_page: the page to use allocated from ring_buffer_alloc_read_page\n * @len: amount to extract\n * @cpu: the cpu of the buffer to extract\n * @full: should the extraction only happen when the page is full.\n *\n * This function will pull out a page from the ring buffer and consume it.\n * @data_page must be the address of the variable that was returned\n * from ring_buffer_alloc_read_page. This is because the page might be used\n * to swap with a page in the ring buffer.\n *\n * for example:\n *\trpage = ring_buffer_alloc_read_page(buffer, cpu);\n *\tif (!rpage)\n *\t\treturn error;\n *\tret = ring_buffer_read_page(buffer, &rpage, len, cpu, 0);\n *\tif (ret >= 0)\n *\t\tprocess_page(rpage, ret);\n *\n * When @full is set, the function will not return true unless\n * the writer is off the reader page.\n *\n * Note: it is up to the calling functions to handle sleeps and wakeups.\n *  The ring buffer can be used anywhere in the kernel and can not\n *  blindly call wake_up. The layer that uses the ring buffer must be\n *  responsible for that.\n *\n * Returns:\n *  >=0 if data has been transferred, returns the offset of consumed data.\n *  <0 if no data has been transferred.\n */\nint ring_buffer_read_page(struct ring_buffer *buffer,\n\t\t\t  void **data_page, size_t len, int cpu, int full)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\tstruct ring_buffer_event *event;\n\tstruct buffer_data_page *bpage;\n\tstruct buffer_page *reader;\n\tunsigned long missed_events;\n\tunsigned long flags;\n\tunsigned int commit;\n\tunsigned int read;\n\tu64 save_timestamp;\n\tint ret = -1;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\tgoto out;\n\n\t/*\n\t * If len is not big enough to hold the page header, then\n\t * we can not copy anything.\n\t */\n\tif (len <= BUF_PAGE_HDR_SIZE)\n\t\tgoto out;\n\n\tlen -= BUF_PAGE_HDR_SIZE;\n\n\tif (!data_page)\n\t\tgoto out;\n\n\tbpage = *data_page;\n\tif (!bpage)\n\t\tgoto out;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\n\treader = rb_get_reader_page(cpu_buffer);\n\tif (!reader)\n\t\tgoto out_unlock;\n\n\tevent = rb_reader_event(cpu_buffer);\n\n\tread = reader->read;\n\tcommit = rb_page_commit(reader);\n\n\t/* Check if any events were dropped */\n\tmissed_events = cpu_buffer->lost_events;\n\n\t/*\n\t * If this page has been partially read or\n\t * if len is not big enough to read the rest of the page or\n\t * a writer is still on the page, then\n\t * we must copy the data from the page to the buffer.\n\t * Otherwise, we can simply swap the page with the one passed in.\n\t */\n\tif (read || (len < (commit - read)) ||\n\t    cpu_buffer->reader_page == cpu_buffer->commit_page) {\n\t\tstruct buffer_data_page *rpage = cpu_buffer->reader_page->page;\n\t\tunsigned int rpos = read;\n\t\tunsigned int pos = 0;\n\t\tunsigned int size;\n\n\t\tif (full)\n\t\t\tgoto out_unlock;\n\n\t\tif (len > (commit - read))\n\t\t\tlen = (commit - read);\n\n\t\t/* Always keep the time extend and data together */\n\t\tsize = rb_event_ts_length(event);\n\n\t\tif (len < size)\n\t\t\tgoto out_unlock;\n\n\t\t/* save the current timestamp, since the user will need it */\n\t\tsave_timestamp = cpu_buffer->read_stamp;\n\n\t\t/* Need to copy one event at a time */\n\t\tdo {\n\t\t\t/* We need the size of one event, because\n\t\t\t * rb_advance_reader only advances by one event,\n\t\t\t * whereas rb_event_ts_length may include the size of\n\t\t\t * one or two events.\n\t\t\t * We have already ensured there's enough space if this\n\t\t\t * is a time extend. */\n\t\t\tsize = rb_event_length(event);\n\t\t\tmemcpy(bpage->data + pos, rpage->data + rpos, size);\n\n\t\t\tlen -= size;\n\n\t\t\trb_advance_reader(cpu_buffer);\n\t\t\trpos = reader->read;\n\t\t\tpos += size;\n\n\t\t\tif (rpos >= commit)\n\t\t\t\tbreak;\n\n\t\t\tevent = rb_reader_event(cpu_buffer);\n\t\t\t/* Always keep the time extend and data together */\n\t\t\tsize = rb_event_ts_length(event);\n\t\t} while (len >= size);\n\n\t\t/* update bpage */\n\t\tlocal_set(&bpage->commit, pos);\n\t\tbpage->time_stamp = save_timestamp;\n\n\t\t/* we copied everything to the beginning */\n\t\tread = 0;\n\t} else {\n\t\t/* update the entry counter */\n\t\tcpu_buffer->read += rb_page_entries(reader);\n\t\tcpu_buffer->read_bytes += BUF_PAGE_SIZE;\n\n\t\t/* swap the pages */\n\t\trb_init_page(bpage);\n\t\tbpage = reader->page;\n\t\treader->page = *data_page;\n\t\tlocal_set(&reader->write, 0);\n\t\tlocal_set(&reader->entries, 0);\n\t\treader->read = 0;\n\t\t*data_page = bpage;\n\n\t\t/*\n\t\t * Use the real_end for the data size,\n\t\t * This gives us a chance to store the lost events\n\t\t * on the page.\n\t\t */\n\t\tif (reader->real_end)\n\t\t\tlocal_set(&bpage->commit, reader->real_end);\n\t}\n\tret = read;\n\n\tcpu_buffer->lost_events = 0;\n\n\tcommit = local_read(&bpage->commit);\n\t/*\n\t * Set a flag in the commit field if we lost events\n\t */\n\tif (missed_events) {\n\t\t/* If there is room at the end of the page to save the\n\t\t * missed events, then record it there.\n\t\t */\n\t\tif (BUF_PAGE_SIZE - commit >= sizeof(missed_events)) {\n\t\t\tmemcpy(&bpage->data[commit], &missed_events,\n\t\t\t       sizeof(missed_events));\n\t\t\tlocal_add(RB_MISSED_STORED, &bpage->commit);\n\t\t\tcommit += sizeof(missed_events);\n\t\t}\n\t\tlocal_add(RB_MISSED_EVENTS, &bpage->commit);\n\t}\n\n\t/*\n\t * This page may be off to user land. Zero it out here.\n\t */\n\tif (commit < BUF_PAGE_SIZE)\n\t\tmemset(&bpage->data[commit], 0, BUF_PAGE_SIZE - commit);\n\n out_unlock:\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n out:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_page);\n\n#ifdef CONFIG_HOTPLUG_CPU\nstatic int rb_cpu_notify(struct notifier_block *self,\n\t\t\t unsigned long action, void *hcpu)\n{\n\tstruct ring_buffer *buffer =\n\t\tcontainer_of(self, struct ring_buffer, cpu_notify);\n\tlong cpu = (long)hcpu;\n\tlong nr_pages_same;\n\tint cpu_i;\n\tunsigned long nr_pages;\n\n\tswitch (action) {\n\tcase CPU_UP_PREPARE:\n\tcase CPU_UP_PREPARE_FROZEN:\n\t\tif (cpumask_test_cpu(cpu, buffer->cpumask))\n\t\t\treturn NOTIFY_OK;\n\n\t\tnr_pages = 0;\n\t\tnr_pages_same = 1;\n\t\t/* check if all cpu sizes are same */\n\t\tfor_each_buffer_cpu(buffer, cpu_i) {\n\t\t\t/* fill in the size from first enabled cpu */\n\t\t\tif (nr_pages == 0)\n\t\t\t\tnr_pages = buffer->buffers[cpu_i]->nr_pages;\n\t\t\tif (nr_pages != buffer->buffers[cpu_i]->nr_pages) {\n\t\t\t\tnr_pages_same = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t/* allocate minimum pages, user can later expand it */\n\t\tif (!nr_pages_same)\n\t\t\tnr_pages = 2;\n\t\tbuffer->buffers[cpu] =\n\t\t\trb_allocate_cpu_buffer(buffer, nr_pages, cpu);\n\t\tif (!buffer->buffers[cpu]) {\n\t\t\tWARN(1, \"failed to allocate ring buffer on CPU %ld\\n\",\n\t\t\t     cpu);\n\t\t\treturn NOTIFY_OK;\n\t\t}\n\t\tsmp_wmb();\n\t\tcpumask_set_cpu(cpu, buffer->cpumask);\n\t\tbreak;\n\tcase CPU_DOWN_PREPARE:\n\tcase CPU_DOWN_PREPARE_FROZEN:\n\t\t/*\n\t\t * Do nothing.\n\t\t *  If we were to free the buffer, then the user would\n\t\t *  lose any trace that was in the buffer.\n\t\t */\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n#endif\n\n#ifdef CONFIG_RING_BUFFER_STARTUP_TEST\n/*\n * This is a basic integrity check of the ring buffer.\n * Late in the boot cycle this test will run when configured in.\n * It will kick off a thread per CPU that will go into a loop\n * writing to the per cpu ring buffer various sizes of data.\n * Some of the data will be large items, some small.\n *\n * Another thread is created that goes into a spin, sending out\n * IPIs to the other CPUs to also write into the ring buffer.\n * this is to test the nesting ability of the buffer.\n *\n * Basic stats are recorded and reported. If something in the\n * ring buffer should happen that's not expected, a big warning\n * is displayed and all ring buffers are disabled.\n */\nstatic struct task_struct *rb_threads[NR_CPUS] __initdata;\n\nstruct rb_test_data {\n\tstruct ring_buffer\t*buffer;\n\tunsigned long\t\tevents;\n\tunsigned long\t\tbytes_written;\n\tunsigned long\t\tbytes_alloc;\n\tunsigned long\t\tbytes_dropped;\n\tunsigned long\t\tevents_nested;\n\tunsigned long\t\tbytes_written_nested;\n\tunsigned long\t\tbytes_alloc_nested;\n\tunsigned long\t\tbytes_dropped_nested;\n\tint\t\t\tmin_size_nested;\n\tint\t\t\tmax_size_nested;\n\tint\t\t\tmax_size;\n\tint\t\t\tmin_size;\n\tint\t\t\tcpu;\n\tint\t\t\tcnt;\n};\n\nstatic struct rb_test_data rb_data[NR_CPUS] __initdata;\n\n/* 1 meg per cpu */\n#define RB_TEST_BUFFER_SIZE\t1048576\n\nstatic char rb_string[] __initdata =\n\t\"abcdefghijklmnopqrstuvwxyz1234567890!@#$%^&*()?+\\\\\"\n\t\"?+|:';\\\",.<>/?abcdefghijklmnopqrstuvwxyz1234567890\"\n\t\"!@#$%^&*()?+\\\\?+|:';\\\",.<>/?abcdefghijklmnopqrstuv\";\n\nstatic bool rb_test_started __initdata;\n\nstruct rb_item {\n\tint size;\n\tchar str[];\n};\n\nstatic __init int rb_write_something(struct rb_test_data *data, bool nested)\n{\n\tstruct ring_buffer_event *event;\n\tstruct rb_item *item;\n\tbool started;\n\tint event_len;\n\tint size;\n\tint len;\n\tint cnt;\n\n\t/* Have nested writes different that what is written */\n\tcnt = data->cnt + (nested ? 27 : 0);\n\n\t/* Multiply cnt by ~e, to make some unique increment */\n\tsize = (data->cnt * 68 / 25) % (sizeof(rb_string) - 1);\n\n\tlen = size + sizeof(struct rb_item);\n\n\tstarted = rb_test_started;\n\t/* read rb_test_started before checking buffer enabled */\n\tsmp_rmb();\n\n\tevent = ring_buffer_lock_reserve(data->buffer, len);\n\tif (!event) {\n\t\t/* Ignore dropped events before test starts. */\n\t\tif (started) {\n\t\t\tif (nested)\n\t\t\t\tdata->bytes_dropped += len;\n\t\t\telse\n\t\t\t\tdata->bytes_dropped_nested += len;\n\t\t}\n\t\treturn len;\n\t}\n\n\tevent_len = ring_buffer_event_length(event);\n\n\tif (RB_WARN_ON(data->buffer, event_len < len))\n\t\tgoto out;\n\n\titem = ring_buffer_event_data(event);\n\titem->size = size;\n\tmemcpy(item->str, rb_string, size);\n\n\tif (nested) {\n\t\tdata->bytes_alloc_nested += event_len;\n\t\tdata->bytes_written_nested += len;\n\t\tdata->events_nested++;\n\t\tif (!data->min_size_nested || len < data->min_size_nested)\n\t\t\tdata->min_size_nested = len;\n\t\tif (len > data->max_size_nested)\n\t\t\tdata->max_size_nested = len;\n\t} else {\n\t\tdata->bytes_alloc += event_len;\n\t\tdata->bytes_written += len;\n\t\tdata->events++;\n\t\tif (!data->min_size || len < data->min_size)\n\t\t\tdata->max_size = len;\n\t\tif (len > data->max_size)\n\t\t\tdata->max_size = len;\n\t}\n\n out:\n\tring_buffer_unlock_commit(data->buffer, event);\n\n\treturn 0;\n}\n\nstatic __init int rb_test(void *arg)\n{\n\tstruct rb_test_data *data = arg;\n\n\twhile (!kthread_should_stop()) {\n\t\trb_write_something(data, false);\n\t\tdata->cnt++;\n\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t/* Now sleep between a min of 100-300us and a max of 1ms */\n\t\tusleep_range(((data->cnt % 3) + 1) * 100, 1000);\n\t}\n\n\treturn 0;\n}\n\nstatic __init void rb_ipi(void *ignore)\n{\n\tstruct rb_test_data *data;\n\tint cpu = smp_processor_id();\n\n\tdata = &rb_data[cpu];\n\trb_write_something(data, true);\n}\n\nstatic __init int rb_hammer_test(void *arg)\n{\n\twhile (!kthread_should_stop()) {\n\n\t\t/* Send an IPI to all cpus to write data! */\n\t\tsmp_call_function(rb_ipi, NULL, 1);\n\t\t/* No sleep, but for non preempt, let others run */\n\t\tschedule();\n\t}\n\n\treturn 0;\n}\n\nstatic __init int test_ringbuffer(void)\n{\n\tstruct task_struct *rb_hammer;\n\tstruct ring_buffer *buffer;\n\tint cpu;\n\tint ret = 0;\n\n\tpr_info(\"Running ring buffer tests...\\n\");\n\n\tbuffer = ring_buffer_alloc(RB_TEST_BUFFER_SIZE, RB_FL_OVERWRITE);\n\tif (WARN_ON(!buffer))\n\t\treturn 0;\n\n\t/* Disable buffer so that threads can't write to it yet */\n\tring_buffer_record_off(buffer);\n\n\tfor_each_online_cpu(cpu) {\n\t\trb_data[cpu].buffer = buffer;\n\t\trb_data[cpu].cpu = cpu;\n\t\trb_data[cpu].cnt = cpu;\n\t\trb_threads[cpu] = kthread_create(rb_test, &rb_data[cpu],\n\t\t\t\t\t\t \"rbtester/%d\", cpu);\n\t\tif (WARN_ON(!rb_threads[cpu])) {\n\t\t\tpr_cont(\"FAILED\\n\");\n\t\t\tret = -1;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tkthread_bind(rb_threads[cpu], cpu);\n \t\twake_up_process(rb_threads[cpu]);\n\t}\n\n\t/* Now create the rb hammer! */\n\trb_hammer = kthread_run(rb_hammer_test, NULL, \"rbhammer\");\n\tif (WARN_ON(!rb_hammer)) {\n\t\tpr_cont(\"FAILED\\n\");\n\t\tret = -1;\n\t\tgoto out_free;\n\t}\n\n\tring_buffer_record_on(buffer);\n\t/*\n\t * Show buffer is enabled before setting rb_test_started.\n\t * Yes there's a small race window where events could be\n\t * dropped and the thread wont catch it. But when a ring\n\t * buffer gets enabled, there will always be some kind of\n\t * delay before other CPUs see it. Thus, we don't care about\n\t * those dropped events. We care about events dropped after\n\t * the threads see that the buffer is active.\n\t */\n\tsmp_wmb();\n\trb_test_started = true;\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\t/* Just run for 10 seconds */;\n\tschedule_timeout(10 * HZ);\n\n\tkthread_stop(rb_hammer);\n\n out_free:\n\tfor_each_online_cpu(cpu) {\n\t\tif (!rb_threads[cpu])\n\t\t\tbreak;\n\t\tkthread_stop(rb_threads[cpu]);\n\t}\n\tif (ret) {\n\t\tring_buffer_free(buffer);\n\t\treturn ret;\n\t}\n\n\t/* Report! */\n\tpr_info(\"finished\\n\");\n\tfor_each_online_cpu(cpu) {\n\t\tstruct ring_buffer_event *event;\n\t\tstruct rb_test_data *data = &rb_data[cpu];\n\t\tstruct rb_item *item;\n\t\tunsigned long total_events;\n\t\tunsigned long total_dropped;\n\t\tunsigned long total_written;\n\t\tunsigned long total_alloc;\n\t\tunsigned long total_read = 0;\n\t\tunsigned long total_size = 0;\n\t\tunsigned long total_len = 0;\n\t\tunsigned long total_lost = 0;\n\t\tunsigned long lost;\n\t\tint big_event_size;\n\t\tint small_event_size;\n\n\t\tret = -1;\n\n\t\ttotal_events = data->events + data->events_nested;\n\t\ttotal_written = data->bytes_written + data->bytes_written_nested;\n\t\ttotal_alloc = data->bytes_alloc + data->bytes_alloc_nested;\n\t\ttotal_dropped = data->bytes_dropped + data->bytes_dropped_nested;\n\n\t\tbig_event_size = data->max_size + data->max_size_nested;\n\t\tsmall_event_size = data->min_size + data->min_size_nested;\n\n\t\tpr_info(\"CPU %d:\\n\", cpu);\n\t\tpr_info(\"              events:    %ld\\n\", total_events);\n\t\tpr_info(\"       dropped bytes:    %ld\\n\", total_dropped);\n\t\tpr_info(\"       alloced bytes:    %ld\\n\", total_alloc);\n\t\tpr_info(\"       written bytes:    %ld\\n\", total_written);\n\t\tpr_info(\"       biggest event:    %d\\n\", big_event_size);\n\t\tpr_info(\"      smallest event:    %d\\n\", small_event_size);\n\n\t\tif (RB_WARN_ON(buffer, total_dropped))\n\t\t\tbreak;\n\n\t\tret = 0;\n\n\t\twhile ((event = ring_buffer_consume(buffer, cpu, NULL, &lost))) {\n\t\t\ttotal_lost += lost;\n\t\t\titem = ring_buffer_event_data(event);\n\t\t\ttotal_len += ring_buffer_event_length(event);\n\t\t\ttotal_size += item->size + sizeof(struct rb_item);\n\t\t\tif (memcmp(&item->str[0], rb_string, item->size) != 0) {\n\t\t\t\tpr_info(\"FAILED!\\n\");\n\t\t\t\tpr_info(\"buffer had: %.*s\\n\", item->size, item->str);\n\t\t\t\tpr_info(\"expected:   %.*s\\n\", item->size, rb_string);\n\t\t\t\tRB_WARN_ON(buffer, 1);\n\t\t\t\tret = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttotal_read++;\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = -1;\n\n\t\tpr_info(\"         read events:   %ld\\n\", total_read);\n\t\tpr_info(\"         lost events:   %ld\\n\", total_lost);\n\t\tpr_info(\"        total events:   %ld\\n\", total_lost + total_read);\n\t\tpr_info(\"  recorded len bytes:   %ld\\n\", total_len);\n\t\tpr_info(\" recorded size bytes:   %ld\\n\", total_size);\n\t\tif (total_lost)\n\t\t\tpr_info(\" With dropped events, record len and size may not match\\n\"\n\t\t\t\t\" alloced and written from above\\n\");\n\t\tif (!total_lost) {\n\t\t\tif (RB_WARN_ON(buffer, total_len != total_alloc ||\n\t\t\t\t       total_size != total_written))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (RB_WARN_ON(buffer, total_lost + total_read != total_events))\n\t\t\tbreak;\n\n\t\tret = 0;\n\t}\n\tif (!ret)\n\t\tpr_info(\"Ring buffer PASSED!\\n\");\n\n\tring_buffer_free(buffer);\n\treturn 0;\n}\n\nlate_initcall(test_ringbuffer);\n#endif /* CONFIG_RING_BUFFER_STARTUP_TEST */\n"], "fixing_code": ["/*\n * Generic ring buffer\n *\n * Copyright (C) 2008 Steven Rostedt <srostedt@redhat.com>\n */\n#include <linux/trace_events.h>\n#include <linux/ring_buffer.h>\n#include <linux/trace_clock.h>\n#include <linux/trace_seq.h>\n#include <linux/spinlock.h>\n#include <linux/irq_work.h>\n#include <linux/uaccess.h>\n#include <linux/hardirq.h>\n#include <linux/kthread.h>\t/* for self test */\n#include <linux/kmemcheck.h>\n#include <linux/module.h>\n#include <linux/percpu.h>\n#include <linux/mutex.h>\n#include <linux/delay.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/cpu.h>\n\n#include <asm/local.h>\n\nstatic void update_pages_handler(struct work_struct *work);\n\n/*\n * The ring buffer header is special. We must manually up keep it.\n */\nint ring_buffer_print_entry_header(struct trace_seq *s)\n{\n\ttrace_seq_puts(s, \"# compressed entry header\\n\");\n\ttrace_seq_puts(s, \"\\ttype_len    :    5 bits\\n\");\n\ttrace_seq_puts(s, \"\\ttime_delta  :   27 bits\\n\");\n\ttrace_seq_puts(s, \"\\tarray       :   32 bits\\n\");\n\ttrace_seq_putc(s, '\\n');\n\ttrace_seq_printf(s, \"\\tpadding     : type == %d\\n\",\n\t\t\t RINGBUF_TYPE_PADDING);\n\ttrace_seq_printf(s, \"\\ttime_extend : type == %d\\n\",\n\t\t\t RINGBUF_TYPE_TIME_EXTEND);\n\ttrace_seq_printf(s, \"\\tdata max type_len  == %d\\n\",\n\t\t\t RINGBUF_TYPE_DATA_TYPE_LEN_MAX);\n\n\treturn !trace_seq_has_overflowed(s);\n}\n\n/*\n * The ring buffer is made up of a list of pages. A separate list of pages is\n * allocated for each CPU. A writer may only write to a buffer that is\n * associated with the CPU it is currently executing on.  A reader may read\n * from any per cpu buffer.\n *\n * The reader is special. For each per cpu buffer, the reader has its own\n * reader page. When a reader has read the entire reader page, this reader\n * page is swapped with another page in the ring buffer.\n *\n * Now, as long as the writer is off the reader page, the reader can do what\n * ever it wants with that page. The writer will never write to that page\n * again (as long as it is out of the ring buffer).\n *\n * Here's some silly ASCII art.\n *\n *   +------+\n *   |reader|          RING BUFFER\n *   |page  |\n *   +------+        +---+   +---+   +---+\n *                   |   |-->|   |-->|   |\n *                   +---+   +---+   +---+\n *                     ^               |\n *                     |               |\n *                     +---------------+\n *\n *\n *   +------+\n *   |reader|          RING BUFFER\n *   |page  |------------------v\n *   +------+        +---+   +---+   +---+\n *                   |   |-->|   |-->|   |\n *                   +---+   +---+   +---+\n *                     ^               |\n *                     |               |\n *                     +---------------+\n *\n *\n *   +------+\n *   |reader|          RING BUFFER\n *   |page  |------------------v\n *   +------+        +---+   +---+   +---+\n *      ^            |   |-->|   |-->|   |\n *      |            +---+   +---+   +---+\n *      |                              |\n *      |                              |\n *      +------------------------------+\n *\n *\n *   +------+\n *   |buffer|          RING BUFFER\n *   |page  |------------------v\n *   +------+        +---+   +---+   +---+\n *      ^            |   |   |   |-->|   |\n *      |   New      +---+   +---+   +---+\n *      |  Reader------^               |\n *      |   page                       |\n *      +------------------------------+\n *\n *\n * After we make this swap, the reader can hand this page off to the splice\n * code and be done with it. It can even allocate a new page if it needs to\n * and swap that into the ring buffer.\n *\n * We will be using cmpxchg soon to make all this lockless.\n *\n */\n\n/* Used for individual buffers (after the counter) */\n#define RB_BUFFER_OFF\t\t(1 << 20)\n\n#define BUF_PAGE_HDR_SIZE offsetof(struct buffer_data_page, data)\n\n#define RB_EVNT_HDR_SIZE (offsetof(struct ring_buffer_event, array))\n#define RB_ALIGNMENT\t\t4U\n#define RB_MAX_SMALL_DATA\t(RB_ALIGNMENT * RINGBUF_TYPE_DATA_TYPE_LEN_MAX)\n#define RB_EVNT_MIN_SIZE\t8U\t/* two 32bit words */\n\n#ifndef CONFIG_HAVE_64BIT_ALIGNED_ACCESS\n# define RB_FORCE_8BYTE_ALIGNMENT\t0\n# define RB_ARCH_ALIGNMENT\t\tRB_ALIGNMENT\n#else\n# define RB_FORCE_8BYTE_ALIGNMENT\t1\n# define RB_ARCH_ALIGNMENT\t\t8U\n#endif\n\n#define RB_ALIGN_DATA\t\t__aligned(RB_ARCH_ALIGNMENT)\n\n/* define RINGBUF_TYPE_DATA for 'case RINGBUF_TYPE_DATA:' */\n#define RINGBUF_TYPE_DATA 0 ... RINGBUF_TYPE_DATA_TYPE_LEN_MAX\n\nenum {\n\tRB_LEN_TIME_EXTEND = 8,\n\tRB_LEN_TIME_STAMP = 16,\n};\n\n#define skip_time_extend(event) \\\n\t((struct ring_buffer_event *)((char *)event + RB_LEN_TIME_EXTEND))\n\nstatic inline int rb_null_event(struct ring_buffer_event *event)\n{\n\treturn event->type_len == RINGBUF_TYPE_PADDING && !event->time_delta;\n}\n\nstatic void rb_event_set_padding(struct ring_buffer_event *event)\n{\n\t/* padding has a NULL time_delta */\n\tevent->type_len = RINGBUF_TYPE_PADDING;\n\tevent->time_delta = 0;\n}\n\nstatic unsigned\nrb_event_data_length(struct ring_buffer_event *event)\n{\n\tunsigned length;\n\n\tif (event->type_len)\n\t\tlength = event->type_len * RB_ALIGNMENT;\n\telse\n\t\tlength = event->array[0];\n\treturn length + RB_EVNT_HDR_SIZE;\n}\n\n/*\n * Return the length of the given event. Will return\n * the length of the time extend if the event is a\n * time extend.\n */\nstatic inline unsigned\nrb_event_length(struct ring_buffer_event *event)\n{\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\tif (rb_null_event(event))\n\t\t\t/* undefined */\n\t\t\treturn -1;\n\t\treturn  event->array[0] + RB_EVNT_HDR_SIZE;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\treturn RB_LEN_TIME_EXTEND;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\treturn RB_LEN_TIME_STAMP;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\treturn rb_event_data_length(event);\n\tdefault:\n\t\tBUG();\n\t}\n\t/* not hit */\n\treturn 0;\n}\n\n/*\n * Return total length of time extend and data,\n *   or just the event length for all other events.\n */\nstatic inline unsigned\nrb_event_ts_length(struct ring_buffer_event *event)\n{\n\tunsigned len = 0;\n\n\tif (event->type_len == RINGBUF_TYPE_TIME_EXTEND) {\n\t\t/* time extends include the data event after it */\n\t\tlen = RB_LEN_TIME_EXTEND;\n\t\tevent = skip_time_extend(event);\n\t}\n\treturn len + rb_event_length(event);\n}\n\n/**\n * ring_buffer_event_length - return the length of the event\n * @event: the event to get the length of\n *\n * Returns the size of the data load of a data event.\n * If the event is something other than a data event, it\n * returns the size of the event itself. With the exception\n * of a TIME EXTEND, where it still returns the size of the\n * data load of the data event after it.\n */\nunsigned ring_buffer_event_length(struct ring_buffer_event *event)\n{\n\tunsigned length;\n\n\tif (event->type_len == RINGBUF_TYPE_TIME_EXTEND)\n\t\tevent = skip_time_extend(event);\n\n\tlength = rb_event_length(event);\n\tif (event->type_len > RINGBUF_TYPE_DATA_TYPE_LEN_MAX)\n\t\treturn length;\n\tlength -= RB_EVNT_HDR_SIZE;\n\tif (length > RB_MAX_SMALL_DATA + sizeof(event->array[0]))\n                length -= sizeof(event->array[0]);\n\treturn length;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_event_length);\n\n/* inline for ring buffer fast paths */\nstatic void *\nrb_event_data(struct ring_buffer_event *event)\n{\n\tif (event->type_len == RINGBUF_TYPE_TIME_EXTEND)\n\t\tevent = skip_time_extend(event);\n\tBUG_ON(event->type_len > RINGBUF_TYPE_DATA_TYPE_LEN_MAX);\n\t/* If length is in len field, then array[0] has the data */\n\tif (event->type_len)\n\t\treturn (void *)&event->array[0];\n\t/* Otherwise length is in array[0] and array[1] has the data */\n\treturn (void *)&event->array[1];\n}\n\n/**\n * ring_buffer_event_data - return the data of the event\n * @event: the event to get the data from\n */\nvoid *ring_buffer_event_data(struct ring_buffer_event *event)\n{\n\treturn rb_event_data(event);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_event_data);\n\n#define for_each_buffer_cpu(buffer, cpu)\t\t\\\n\tfor_each_cpu(cpu, buffer->cpumask)\n\n#define TS_SHIFT\t27\n#define TS_MASK\t\t((1ULL << TS_SHIFT) - 1)\n#define TS_DELTA_TEST\t(~TS_MASK)\n\n/* Flag when events were overwritten */\n#define RB_MISSED_EVENTS\t(1 << 31)\n/* Missed count stored at end */\n#define RB_MISSED_STORED\t(1 << 30)\n\nstruct buffer_data_page {\n\tu64\t\t time_stamp;\t/* page time stamp */\n\tlocal_t\t\t commit;\t/* write committed index */\n\tunsigned char\t data[] RB_ALIGN_DATA;\t/* data of buffer page */\n};\n\n/*\n * Note, the buffer_page list must be first. The buffer pages\n * are allocated in cache lines, which means that each buffer\n * page will be at the beginning of a cache line, and thus\n * the least significant bits will be zero. We use this to\n * add flags in the list struct pointers, to make the ring buffer\n * lockless.\n */\nstruct buffer_page {\n\tstruct list_head list;\t\t/* list of buffer pages */\n\tlocal_t\t\t write;\t\t/* index for next write */\n\tunsigned\t read;\t\t/* index for next read */\n\tlocal_t\t\t entries;\t/* entries on this page */\n\tunsigned long\t real_end;\t/* real end of data */\n\tstruct buffer_data_page *page;\t/* Actual data page */\n};\n\n/*\n * The buffer page counters, write and entries, must be reset\n * atomically when crossing page boundaries. To synchronize this\n * update, two counters are inserted into the number. One is\n * the actual counter for the write position or count on the page.\n *\n * The other is a counter of updaters. Before an update happens\n * the update partition of the counter is incremented. This will\n * allow the updater to update the counter atomically.\n *\n * The counter is 20 bits, and the state data is 12.\n */\n#define RB_WRITE_MASK\t\t0xfffff\n#define RB_WRITE_INTCNT\t\t(1 << 20)\n\nstatic void rb_init_page(struct buffer_data_page *bpage)\n{\n\tlocal_set(&bpage->commit, 0);\n}\n\n/**\n * ring_buffer_page_len - the size of data on the page.\n * @page: The page to read\n *\n * Returns the amount of data on the page, including buffer page header.\n */\nsize_t ring_buffer_page_len(void *page)\n{\n\treturn local_read(&((struct buffer_data_page *)page)->commit)\n\t\t+ BUF_PAGE_HDR_SIZE;\n}\n\n/*\n * Also stolen from mm/slob.c. Thanks to Mathieu Desnoyers for pointing\n * this issue out.\n */\nstatic void free_buffer_page(struct buffer_page *bpage)\n{\n\tfree_page((unsigned long)bpage->page);\n\tkfree(bpage);\n}\n\n/*\n * We need to fit the time_stamp delta into 27 bits.\n */\nstatic inline int test_time_stamp(u64 delta)\n{\n\tif (delta & TS_DELTA_TEST)\n\t\treturn 1;\n\treturn 0;\n}\n\n#define BUF_PAGE_SIZE (PAGE_SIZE - BUF_PAGE_HDR_SIZE)\n\n/* Max payload is BUF_PAGE_SIZE - header (8bytes) */\n#define BUF_MAX_DATA_SIZE (BUF_PAGE_SIZE - (sizeof(u32) * 2))\n\nint ring_buffer_print_page_header(struct trace_seq *s)\n{\n\tstruct buffer_data_page field;\n\n\ttrace_seq_printf(s, \"\\tfield: u64 timestamp;\\t\"\n\t\t\t \"offset:0;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)sizeof(field.time_stamp),\n\t\t\t (unsigned int)is_signed_type(u64));\n\n\ttrace_seq_printf(s, \"\\tfield: local_t commit;\\t\"\n\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)offsetof(typeof(field), commit),\n\t\t\t (unsigned int)sizeof(field.commit),\n\t\t\t (unsigned int)is_signed_type(long));\n\n\ttrace_seq_printf(s, \"\\tfield: int overwrite;\\t\"\n\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)offsetof(typeof(field), commit),\n\t\t\t 1,\n\t\t\t (unsigned int)is_signed_type(long));\n\n\ttrace_seq_printf(s, \"\\tfield: char data;\\t\"\n\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)offsetof(typeof(field), data),\n\t\t\t (unsigned int)BUF_PAGE_SIZE,\n\t\t\t (unsigned int)is_signed_type(char));\n\n\treturn !trace_seq_has_overflowed(s);\n}\n\nstruct rb_irq_work {\n\tstruct irq_work\t\t\twork;\n\twait_queue_head_t\t\twaiters;\n\twait_queue_head_t\t\tfull_waiters;\n\tbool\t\t\t\twaiters_pending;\n\tbool\t\t\t\tfull_waiters_pending;\n\tbool\t\t\t\twakeup_full;\n};\n\n/*\n * Structure to hold event state and handle nested events.\n */\nstruct rb_event_info {\n\tu64\t\t\tts;\n\tu64\t\t\tdelta;\n\tunsigned long\t\tlength;\n\tstruct buffer_page\t*tail_page;\n\tint\t\t\tadd_timestamp;\n};\n\n/*\n * Used for which event context the event is in.\n *  NMI     = 0\n *  IRQ     = 1\n *  SOFTIRQ = 2\n *  NORMAL  = 3\n *\n * See trace_recursive_lock() comment below for more details.\n */\nenum {\n\tRB_CTX_NMI,\n\tRB_CTX_IRQ,\n\tRB_CTX_SOFTIRQ,\n\tRB_CTX_NORMAL,\n\tRB_CTX_MAX\n};\n\n/*\n * head_page == tail_page && head == tail then buffer is empty.\n */\nstruct ring_buffer_per_cpu {\n\tint\t\t\t\tcpu;\n\tatomic_t\t\t\trecord_disabled;\n\tstruct ring_buffer\t\t*buffer;\n\traw_spinlock_t\t\t\treader_lock;\t/* serialize readers */\n\tarch_spinlock_t\t\t\tlock;\n\tstruct lock_class_key\t\tlock_key;\n\tunsigned long\t\t\tnr_pages;\n\tunsigned int\t\t\tcurrent_context;\n\tstruct list_head\t\t*pages;\n\tstruct buffer_page\t\t*head_page;\t/* read from head */\n\tstruct buffer_page\t\t*tail_page;\t/* write to tail */\n\tstruct buffer_page\t\t*commit_page;\t/* committed pages */\n\tstruct buffer_page\t\t*reader_page;\n\tunsigned long\t\t\tlost_events;\n\tunsigned long\t\t\tlast_overrun;\n\tlocal_t\t\t\t\tentries_bytes;\n\tlocal_t\t\t\t\tentries;\n\tlocal_t\t\t\t\toverrun;\n\tlocal_t\t\t\t\tcommit_overrun;\n\tlocal_t\t\t\t\tdropped_events;\n\tlocal_t\t\t\t\tcommitting;\n\tlocal_t\t\t\t\tcommits;\n\tunsigned long\t\t\tread;\n\tunsigned long\t\t\tread_bytes;\n\tu64\t\t\t\twrite_stamp;\n\tu64\t\t\t\tread_stamp;\n\t/* ring buffer pages to update, > 0 to add, < 0 to remove */\n\tlong\t\t\t\tnr_pages_to_update;\n\tstruct list_head\t\tnew_pages; /* new pages to add */\n\tstruct work_struct\t\tupdate_pages_work;\n\tstruct completion\t\tupdate_done;\n\n\tstruct rb_irq_work\t\tirq_work;\n};\n\nstruct ring_buffer {\n\tunsigned\t\t\tflags;\n\tint\t\t\t\tcpus;\n\tatomic_t\t\t\trecord_disabled;\n\tatomic_t\t\t\tresize_disabled;\n\tcpumask_var_t\t\t\tcpumask;\n\n\tstruct lock_class_key\t\t*reader_lock_key;\n\n\tstruct mutex\t\t\tmutex;\n\n\tstruct ring_buffer_per_cpu\t**buffers;\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tstruct notifier_block\t\tcpu_notify;\n#endif\n\tu64\t\t\t\t(*clock)(void);\n\n\tstruct rb_irq_work\t\tirq_work;\n};\n\nstruct ring_buffer_iter {\n\tstruct ring_buffer_per_cpu\t*cpu_buffer;\n\tunsigned long\t\t\thead;\n\tstruct buffer_page\t\t*head_page;\n\tstruct buffer_page\t\t*cache_reader_page;\n\tunsigned long\t\t\tcache_read;\n\tu64\t\t\t\tread_stamp;\n};\n\n/*\n * rb_wake_up_waiters - wake up tasks waiting for ring buffer input\n *\n * Schedules a delayed work to wake up any task that is blocked on the\n * ring buffer waiters queue.\n */\nstatic void rb_wake_up_waiters(struct irq_work *work)\n{\n\tstruct rb_irq_work *rbwork = container_of(work, struct rb_irq_work, work);\n\n\twake_up_all(&rbwork->waiters);\n\tif (rbwork->wakeup_full) {\n\t\trbwork->wakeup_full = false;\n\t\twake_up_all(&rbwork->full_waiters);\n\t}\n}\n\n/**\n * ring_buffer_wait - wait for input to the ring buffer\n * @buffer: buffer to wait on\n * @cpu: the cpu buffer to wait on\n * @full: wait until a full page is available, if @cpu != RING_BUFFER_ALL_CPUS\n *\n * If @cpu == RING_BUFFER_ALL_CPUS then the task will wake up as soon\n * as data is added to any of the @buffer's cpu buffers. Otherwise\n * it will wait for data to be added to a specific cpu buffer.\n */\nint ring_buffer_wait(struct ring_buffer *buffer, int cpu, bool full)\n{\n\tstruct ring_buffer_per_cpu *uninitialized_var(cpu_buffer);\n\tDEFINE_WAIT(wait);\n\tstruct rb_irq_work *work;\n\tint ret = 0;\n\n\t/*\n\t * Depending on what the caller is waiting for, either any\n\t * data in any cpu buffer, or a specific buffer, put the\n\t * caller on the appropriate wait queue.\n\t */\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\twork = &buffer->irq_work;\n\t\t/* Full only makes sense on per cpu reads */\n\t\tfull = false;\n\t} else {\n\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\t\treturn -ENODEV;\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\twork = &cpu_buffer->irq_work;\n\t}\n\n\n\twhile (true) {\n\t\tif (full)\n\t\t\tprepare_to_wait(&work->full_waiters, &wait, TASK_INTERRUPTIBLE);\n\t\telse\n\t\t\tprepare_to_wait(&work->waiters, &wait, TASK_INTERRUPTIBLE);\n\n\t\t/*\n\t\t * The events can happen in critical sections where\n\t\t * checking a work queue can cause deadlocks.\n\t\t * After adding a task to the queue, this flag is set\n\t\t * only to notify events to try to wake up the queue\n\t\t * using irq_work.\n\t\t *\n\t\t * We don't clear it even if the buffer is no longer\n\t\t * empty. The flag only causes the next event to run\n\t\t * irq_work to do the work queue wake up. The worse\n\t\t * that can happen if we race with !trace_empty() is that\n\t\t * an event will cause an irq_work to try to wake up\n\t\t * an empty queue.\n\t\t *\n\t\t * There's no reason to protect this flag either, as\n\t\t * the work queue and irq_work logic will do the necessary\n\t\t * synchronization for the wake ups. The only thing\n\t\t * that is necessary is that the wake up happens after\n\t\t * a task has been queued. It's OK for spurious wake ups.\n\t\t */\n\t\tif (full)\n\t\t\twork->full_waiters_pending = true;\n\t\telse\n\t\t\twork->waiters_pending = true;\n\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cpu == RING_BUFFER_ALL_CPUS && !ring_buffer_empty(buffer))\n\t\t\tbreak;\n\n\t\tif (cpu != RING_BUFFER_ALL_CPUS &&\n\t\t    !ring_buffer_empty_cpu(buffer, cpu)) {\n\t\t\tunsigned long flags;\n\t\t\tbool pagebusy;\n\n\t\t\tif (!full)\n\t\t\t\tbreak;\n\n\t\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\t\t\tpagebusy = cpu_buffer->reader_page == cpu_buffer->commit_page;\n\t\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\t\t\tif (!pagebusy)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tschedule();\n\t}\n\n\tif (full)\n\t\tfinish_wait(&work->full_waiters, &wait);\n\telse\n\t\tfinish_wait(&work->waiters, &wait);\n\n\treturn ret;\n}\n\n/**\n * ring_buffer_poll_wait - poll on buffer input\n * @buffer: buffer to wait on\n * @cpu: the cpu buffer to wait on\n * @filp: the file descriptor\n * @poll_table: The poll descriptor\n *\n * If @cpu == RING_BUFFER_ALL_CPUS then the task will wake up as soon\n * as data is added to any of the @buffer's cpu buffers. Otherwise\n * it will wait for data to be added to a specific cpu buffer.\n *\n * Returns POLLIN | POLLRDNORM if data exists in the buffers,\n * zero otherwise.\n */\nint ring_buffer_poll_wait(struct ring_buffer *buffer, int cpu,\n\t\t\t  struct file *filp, poll_table *poll_table)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct rb_irq_work *work;\n\n\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\twork = &buffer->irq_work;\n\telse {\n\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\t\treturn -EINVAL;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\twork = &cpu_buffer->irq_work;\n\t}\n\n\tpoll_wait(filp, &work->waiters, poll_table);\n\twork->waiters_pending = true;\n\t/*\n\t * There's a tight race between setting the waiters_pending and\n\t * checking if the ring buffer is empty.  Once the waiters_pending bit\n\t * is set, the next event will wake the task up, but we can get stuck\n\t * if there's only a single event in.\n\t *\n\t * FIXME: Ideally, we need a memory barrier on the writer side as well,\n\t * but adding a memory barrier to all events will cause too much of a\n\t * performance hit in the fast path.  We only need a memory barrier when\n\t * the buffer goes from empty to having content.  But as this race is\n\t * extremely small, and it's not a problem if another event comes in, we\n\t * will fix it later.\n\t */\n\tsmp_mb();\n\n\tif ((cpu == RING_BUFFER_ALL_CPUS && !ring_buffer_empty(buffer)) ||\n\t    (cpu != RING_BUFFER_ALL_CPUS && !ring_buffer_empty_cpu(buffer, cpu)))\n\t\treturn POLLIN | POLLRDNORM;\n\treturn 0;\n}\n\n/* buffer may be either ring_buffer or ring_buffer_per_cpu */\n#define RB_WARN_ON(b, cond)\t\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\tint _____ret = unlikely(cond);\t\t\t\t\\\n\t\tif (_____ret) {\t\t\t\t\t\t\\\n\t\t\tif (__same_type(*(b), struct ring_buffer_per_cpu)) { \\\n\t\t\t\tstruct ring_buffer_per_cpu *__b =\t\\\n\t\t\t\t\t(void *)b;\t\t\t\\\n\t\t\t\tatomic_inc(&__b->buffer->record_disabled); \\\n\t\t\t} else\t\t\t\t\t\t\\\n\t\t\t\tatomic_inc(&b->record_disabled);\t\\\n\t\t\tWARN_ON(1);\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t_____ret;\t\t\t\t\t\t\\\n\t})\n\n/* Up this if you want to test the TIME_EXTENTS and normalization */\n#define DEBUG_SHIFT 0\n\nstatic inline u64 rb_time_stamp(struct ring_buffer *buffer)\n{\n\t/* shift to debug/test normalization and TIME_EXTENTS */\n\treturn buffer->clock() << DEBUG_SHIFT;\n}\n\nu64 ring_buffer_time_stamp(struct ring_buffer *buffer, int cpu)\n{\n\tu64 time;\n\n\tpreempt_disable_notrace();\n\ttime = rb_time_stamp(buffer);\n\tpreempt_enable_no_resched_notrace();\n\n\treturn time;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_time_stamp);\n\nvoid ring_buffer_normalize_time_stamp(struct ring_buffer *buffer,\n\t\t\t\t      int cpu, u64 *ts)\n{\n\t/* Just stupid testing the normalize function and deltas */\n\t*ts >>= DEBUG_SHIFT;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_normalize_time_stamp);\n\n/*\n * Making the ring buffer lockless makes things tricky.\n * Although writes only happen on the CPU that they are on,\n * and they only need to worry about interrupts. Reads can\n * happen on any CPU.\n *\n * The reader page is always off the ring buffer, but when the\n * reader finishes with a page, it needs to swap its page with\n * a new one from the buffer. The reader needs to take from\n * the head (writes go to the tail). But if a writer is in overwrite\n * mode and wraps, it must push the head page forward.\n *\n * Here lies the problem.\n *\n * The reader must be careful to replace only the head page, and\n * not another one. As described at the top of the file in the\n * ASCII art, the reader sets its old page to point to the next\n * page after head. It then sets the page after head to point to\n * the old reader page. But if the writer moves the head page\n * during this operation, the reader could end up with the tail.\n *\n * We use cmpxchg to help prevent this race. We also do something\n * special with the page before head. We set the LSB to 1.\n *\n * When the writer must push the page forward, it will clear the\n * bit that points to the head page, move the head, and then set\n * the bit that points to the new head page.\n *\n * We also don't want an interrupt coming in and moving the head\n * page on another writer. Thus we use the second LSB to catch\n * that too. Thus:\n *\n * head->list->prev->next        bit 1          bit 0\n *                              -------        -------\n * Normal page                     0              0\n * Points to head page             0              1\n * New head page                   1              0\n *\n * Note we can not trust the prev pointer of the head page, because:\n *\n * +----+       +-----+        +-----+\n * |    |------>|  T  |---X--->|  N  |\n * |    |<------|     |        |     |\n * +----+       +-----+        +-----+\n *   ^                           ^ |\n *   |          +-----+          | |\n *   +----------|  R  |----------+ |\n *              |     |<-----------+\n *              +-----+\n *\n * Key:  ---X-->  HEAD flag set in pointer\n *         T      Tail page\n *         R      Reader page\n *         N      Next page\n *\n * (see __rb_reserve_next() to see where this happens)\n *\n *  What the above shows is that the reader just swapped out\n *  the reader page with a page in the buffer, but before it\n *  could make the new header point back to the new page added\n *  it was preempted by a writer. The writer moved forward onto\n *  the new page added by the reader and is about to move forward\n *  again.\n *\n *  You can see, it is legitimate for the previous pointer of\n *  the head (or any page) not to point back to itself. But only\n *  temporarially.\n */\n\n#define RB_PAGE_NORMAL\t\t0UL\n#define RB_PAGE_HEAD\t\t1UL\n#define RB_PAGE_UPDATE\t\t2UL\n\n\n#define RB_FLAG_MASK\t\t3UL\n\n/* PAGE_MOVED is not part of the mask */\n#define RB_PAGE_MOVED\t\t4UL\n\n/*\n * rb_list_head - remove any bit\n */\nstatic struct list_head *rb_list_head(struct list_head *list)\n{\n\tunsigned long val = (unsigned long)list;\n\n\treturn (struct list_head *)(val & ~RB_FLAG_MASK);\n}\n\n/*\n * rb_is_head_page - test if the given page is the head page\n *\n * Because the reader may move the head_page pointer, we can\n * not trust what the head page is (it may be pointing to\n * the reader page). But if the next page is a header page,\n * its flags will be non zero.\n */\nstatic inline int\nrb_is_head_page(struct ring_buffer_per_cpu *cpu_buffer,\n\t\tstruct buffer_page *page, struct list_head *list)\n{\n\tunsigned long val;\n\n\tval = (unsigned long)list->next;\n\n\tif ((val & ~RB_FLAG_MASK) != (unsigned long)&page->list)\n\t\treturn RB_PAGE_MOVED;\n\n\treturn val & RB_FLAG_MASK;\n}\n\n/*\n * rb_is_reader_page\n *\n * The unique thing about the reader page, is that, if the\n * writer is ever on it, the previous pointer never points\n * back to the reader page.\n */\nstatic bool rb_is_reader_page(struct buffer_page *page)\n{\n\tstruct list_head *list = page->list.prev;\n\n\treturn rb_list_head(list->next) != &page->list;\n}\n\n/*\n * rb_set_list_to_head - set a list_head to be pointing to head.\n */\nstatic void rb_set_list_to_head(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\tstruct list_head *list)\n{\n\tunsigned long *ptr;\n\n\tptr = (unsigned long *)&list->next;\n\t*ptr |= RB_PAGE_HEAD;\n\t*ptr &= ~RB_PAGE_UPDATE;\n}\n\n/*\n * rb_head_page_activate - sets up head page\n */\nstatic void rb_head_page_activate(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *head;\n\n\thead = cpu_buffer->head_page;\n\tif (!head)\n\t\treturn;\n\n\t/*\n\t * Set the previous list pointer to have the HEAD flag.\n\t */\n\trb_set_list_to_head(cpu_buffer, head->list.prev);\n}\n\nstatic void rb_list_head_clear(struct list_head *list)\n{\n\tunsigned long *ptr = (unsigned long *)&list->next;\n\n\t*ptr &= ~RB_FLAG_MASK;\n}\n\n/*\n * rb_head_page_dactivate - clears head page ptr (for free list)\n */\nstatic void\nrb_head_page_deactivate(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *hd;\n\n\t/* Go through the whole list and clear any pointers found. */\n\trb_list_head_clear(cpu_buffer->pages);\n\n\tlist_for_each(hd, cpu_buffer->pages)\n\t\trb_list_head_clear(hd);\n}\n\nstatic int rb_head_page_set(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t    struct buffer_page *head,\n\t\t\t    struct buffer_page *prev,\n\t\t\t    int old_flag, int new_flag)\n{\n\tstruct list_head *list;\n\tunsigned long val = (unsigned long)&head->list;\n\tunsigned long ret;\n\n\tlist = &prev->list;\n\n\tval &= ~RB_FLAG_MASK;\n\n\tret = cmpxchg((unsigned long *)&list->next,\n\t\t      val | old_flag, val | new_flag);\n\n\t/* check if the reader took the page */\n\tif ((ret & ~RB_FLAG_MASK) != val)\n\t\treturn RB_PAGE_MOVED;\n\n\treturn ret & RB_FLAG_MASK;\n}\n\nstatic int rb_head_page_set_update(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t   struct buffer_page *head,\n\t\t\t\t   struct buffer_page *prev,\n\t\t\t\t   int old_flag)\n{\n\treturn rb_head_page_set(cpu_buffer, head, prev,\n\t\t\t\told_flag, RB_PAGE_UPDATE);\n}\n\nstatic int rb_head_page_set_head(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t struct buffer_page *head,\n\t\t\t\t struct buffer_page *prev,\n\t\t\t\t int old_flag)\n{\n\treturn rb_head_page_set(cpu_buffer, head, prev,\n\t\t\t\told_flag, RB_PAGE_HEAD);\n}\n\nstatic int rb_head_page_set_normal(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t   struct buffer_page *head,\n\t\t\t\t   struct buffer_page *prev,\n\t\t\t\t   int old_flag)\n{\n\treturn rb_head_page_set(cpu_buffer, head, prev,\n\t\t\t\told_flag, RB_PAGE_NORMAL);\n}\n\nstatic inline void rb_inc_page(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t       struct buffer_page **bpage)\n{\n\tstruct list_head *p = rb_list_head((*bpage)->list.next);\n\n\t*bpage = list_entry(p, struct buffer_page, list);\n}\n\nstatic struct buffer_page *\nrb_set_head_page(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *head;\n\tstruct buffer_page *page;\n\tstruct list_head *list;\n\tint i;\n\n\tif (RB_WARN_ON(cpu_buffer, !cpu_buffer->head_page))\n\t\treturn NULL;\n\n\t/* sanity check */\n\tlist = cpu_buffer->pages;\n\tif (RB_WARN_ON(cpu_buffer, rb_list_head(list->prev->next) != list))\n\t\treturn NULL;\n\n\tpage = head = cpu_buffer->head_page;\n\t/*\n\t * It is possible that the writer moves the header behind\n\t * where we started, and we miss in one loop.\n\t * A second loop should grab the header, but we'll do\n\t * three loops just because I'm paranoid.\n\t */\n\tfor (i = 0; i < 3; i++) {\n\t\tdo {\n\t\t\tif (rb_is_head_page(cpu_buffer, page, page->list.prev)) {\n\t\t\t\tcpu_buffer->head_page = page;\n\t\t\t\treturn page;\n\t\t\t}\n\t\t\trb_inc_page(cpu_buffer, &page);\n\t\t} while (page != head);\n\t}\n\n\tRB_WARN_ON(cpu_buffer, 1);\n\n\treturn NULL;\n}\n\nstatic int rb_head_page_replace(struct buffer_page *old,\n\t\t\t\tstruct buffer_page *new)\n{\n\tunsigned long *ptr = (unsigned long *)&old->list.prev->next;\n\tunsigned long val;\n\tunsigned long ret;\n\n\tval = *ptr & ~RB_FLAG_MASK;\n\tval |= RB_PAGE_HEAD;\n\n\tret = cmpxchg(ptr, val, (unsigned long)&new->list);\n\n\treturn ret == val;\n}\n\n/*\n * rb_tail_page_update - move the tail page forward\n */\nstatic void rb_tail_page_update(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t       struct buffer_page *tail_page,\n\t\t\t       struct buffer_page *next_page)\n{\n\tunsigned long old_entries;\n\tunsigned long old_write;\n\n\t/*\n\t * The tail page now needs to be moved forward.\n\t *\n\t * We need to reset the tail page, but without messing\n\t * with possible erasing of data brought in by interrupts\n\t * that have moved the tail page and are currently on it.\n\t *\n\t * We add a counter to the write field to denote this.\n\t */\n\told_write = local_add_return(RB_WRITE_INTCNT, &next_page->write);\n\told_entries = local_add_return(RB_WRITE_INTCNT, &next_page->entries);\n\n\t/*\n\t * Just make sure we have seen our old_write and synchronize\n\t * with any interrupts that come in.\n\t */\n\tbarrier();\n\n\t/*\n\t * If the tail page is still the same as what we think\n\t * it is, then it is up to us to update the tail\n\t * pointer.\n\t */\n\tif (tail_page == READ_ONCE(cpu_buffer->tail_page)) {\n\t\t/* Zero the write counter */\n\t\tunsigned long val = old_write & ~RB_WRITE_MASK;\n\t\tunsigned long eval = old_entries & ~RB_WRITE_MASK;\n\n\t\t/*\n\t\t * This will only succeed if an interrupt did\n\t\t * not come in and change it. In which case, we\n\t\t * do not want to modify it.\n\t\t *\n\t\t * We add (void) to let the compiler know that we do not care\n\t\t * about the return value of these functions. We use the\n\t\t * cmpxchg to only update if an interrupt did not already\n\t\t * do it for us. If the cmpxchg fails, we don't care.\n\t\t */\n\t\t(void)local_cmpxchg(&next_page->write, old_write, val);\n\t\t(void)local_cmpxchg(&next_page->entries, old_entries, eval);\n\n\t\t/*\n\t\t * No need to worry about races with clearing out the commit.\n\t\t * it only can increment when a commit takes place. But that\n\t\t * only happens in the outer most nested commit.\n\t\t */\n\t\tlocal_set(&next_page->page->commit, 0);\n\n\t\t/* Again, either we update tail_page or an interrupt does */\n\t\t(void)cmpxchg(&cpu_buffer->tail_page, tail_page, next_page);\n\t}\n}\n\nstatic int rb_check_bpage(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t  struct buffer_page *bpage)\n{\n\tunsigned long val = (unsigned long)bpage;\n\n\tif (RB_WARN_ON(cpu_buffer, val & RB_FLAG_MASK))\n\t\treturn 1;\n\n\treturn 0;\n}\n\n/**\n * rb_check_list - make sure a pointer to a list has the last bits zero\n */\nstatic int rb_check_list(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t struct list_head *list)\n{\n\tif (RB_WARN_ON(cpu_buffer, rb_list_head(list->prev) != list->prev))\n\t\treturn 1;\n\tif (RB_WARN_ON(cpu_buffer, rb_list_head(list->next) != list->next))\n\t\treturn 1;\n\treturn 0;\n}\n\n/**\n * rb_check_pages - integrity check of buffer pages\n * @cpu_buffer: CPU buffer with pages to test\n *\n * As a safety measure we check to make sure the data pages have not\n * been corrupted.\n */\nstatic int rb_check_pages(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *head = cpu_buffer->pages;\n\tstruct buffer_page *bpage, *tmp;\n\n\t/* Reset the head page if it exists */\n\tif (cpu_buffer->head_page)\n\t\trb_set_head_page(cpu_buffer);\n\n\trb_head_page_deactivate(cpu_buffer);\n\n\tif (RB_WARN_ON(cpu_buffer, head->next->prev != head))\n\t\treturn -1;\n\tif (RB_WARN_ON(cpu_buffer, head->prev->next != head))\n\t\treturn -1;\n\n\tif (rb_check_list(cpu_buffer, head))\n\t\treturn -1;\n\n\tlist_for_each_entry_safe(bpage, tmp, head, list) {\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t       bpage->list.next->prev != &bpage->list))\n\t\t\treturn -1;\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t       bpage->list.prev->next != &bpage->list))\n\t\t\treturn -1;\n\t\tif (rb_check_list(cpu_buffer, &bpage->list))\n\t\t\treturn -1;\n\t}\n\n\trb_head_page_activate(cpu_buffer);\n\n\treturn 0;\n}\n\nstatic int __rb_allocate_pages(long nr_pages, struct list_head *pages, int cpu)\n{\n\tstruct buffer_page *bpage, *tmp;\n\tlong i;\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tstruct page *page;\n\t\t/*\n\t\t * __GFP_NORETRY flag makes sure that the allocation fails\n\t\t * gracefully without invoking oom-killer and the system is\n\t\t * not destabilized.\n\t\t */\n\t\tbpage = kzalloc_node(ALIGN(sizeof(*bpage), cache_line_size()),\n\t\t\t\t    GFP_KERNEL | __GFP_NORETRY,\n\t\t\t\t    cpu_to_node(cpu));\n\t\tif (!bpage)\n\t\t\tgoto free_pages;\n\n\t\tlist_add(&bpage->list, pages);\n\n\t\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\t\tif (!page)\n\t\t\tgoto free_pages;\n\t\tbpage->page = page_address(page);\n\t\trb_init_page(bpage->page);\n\t}\n\n\treturn 0;\n\nfree_pages:\n\tlist_for_each_entry_safe(bpage, tmp, pages, list) {\n\t\tlist_del_init(&bpage->list);\n\t\tfree_buffer_page(bpage);\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic int rb_allocate_pages(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t     unsigned long nr_pages)\n{\n\tLIST_HEAD(pages);\n\n\tWARN_ON(!nr_pages);\n\n\tif (__rb_allocate_pages(nr_pages, &pages, cpu_buffer->cpu))\n\t\treturn -ENOMEM;\n\n\t/*\n\t * The ring buffer page list is a circular list that does not\n\t * start and end with a list head. All page list items point to\n\t * other pages.\n\t */\n\tcpu_buffer->pages = pages.next;\n\tlist_del(&pages);\n\n\tcpu_buffer->nr_pages = nr_pages;\n\n\trb_check_pages(cpu_buffer);\n\n\treturn 0;\n}\n\nstatic struct ring_buffer_per_cpu *\nrb_allocate_cpu_buffer(struct ring_buffer *buffer, long nr_pages, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct buffer_page *bpage;\n\tstruct page *page;\n\tint ret;\n\n\tcpu_buffer = kzalloc_node(ALIGN(sizeof(*cpu_buffer), cache_line_size()),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(cpu));\n\tif (!cpu_buffer)\n\t\treturn NULL;\n\n\tcpu_buffer->cpu = cpu;\n\tcpu_buffer->buffer = buffer;\n\traw_spin_lock_init(&cpu_buffer->reader_lock);\n\tlockdep_set_class(&cpu_buffer->reader_lock, buffer->reader_lock_key);\n\tcpu_buffer->lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\tINIT_WORK(&cpu_buffer->update_pages_work, update_pages_handler);\n\tinit_completion(&cpu_buffer->update_done);\n\tinit_irq_work(&cpu_buffer->irq_work.work, rb_wake_up_waiters);\n\tinit_waitqueue_head(&cpu_buffer->irq_work.waiters);\n\tinit_waitqueue_head(&cpu_buffer->irq_work.full_waiters);\n\n\tbpage = kzalloc_node(ALIGN(sizeof(*bpage), cache_line_size()),\n\t\t\t    GFP_KERNEL, cpu_to_node(cpu));\n\tif (!bpage)\n\t\tgoto fail_free_buffer;\n\n\trb_check_bpage(cpu_buffer, bpage);\n\n\tcpu_buffer->reader_page = bpage;\n\tpage = alloc_pages_node(cpu_to_node(cpu), GFP_KERNEL, 0);\n\tif (!page)\n\t\tgoto fail_free_reader;\n\tbpage->page = page_address(page);\n\trb_init_page(bpage->page);\n\n\tINIT_LIST_HEAD(&cpu_buffer->reader_page->list);\n\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\n\tret = rb_allocate_pages(cpu_buffer, nr_pages);\n\tif (ret < 0)\n\t\tgoto fail_free_reader;\n\n\tcpu_buffer->head_page\n\t\t= list_entry(cpu_buffer->pages, struct buffer_page, list);\n\tcpu_buffer->tail_page = cpu_buffer->commit_page = cpu_buffer->head_page;\n\n\trb_head_page_activate(cpu_buffer);\n\n\treturn cpu_buffer;\n\n fail_free_reader:\n\tfree_buffer_page(cpu_buffer->reader_page);\n\n fail_free_buffer:\n\tkfree(cpu_buffer);\n\treturn NULL;\n}\n\nstatic void rb_free_cpu_buffer(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *head = cpu_buffer->pages;\n\tstruct buffer_page *bpage, *tmp;\n\n\tfree_buffer_page(cpu_buffer->reader_page);\n\n\trb_head_page_deactivate(cpu_buffer);\n\n\tif (head) {\n\t\tlist_for_each_entry_safe(bpage, tmp, head, list) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t\tbpage = list_entry(head, struct buffer_page, list);\n\t\tfree_buffer_page(bpage);\n\t}\n\n\tkfree(cpu_buffer);\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\nstatic int rb_cpu_notify(struct notifier_block *self,\n\t\t\t unsigned long action, void *hcpu);\n#endif\n\n/**\n * __ring_buffer_alloc - allocate a new ring_buffer\n * @size: the size in bytes per cpu that is needed.\n * @flags: attributes to set for the ring buffer.\n *\n * Currently the only flag that is available is the RB_FL_OVERWRITE\n * flag. This flag means that the buffer will overwrite old data\n * when the buffer wraps. If this flag is not set, the buffer will\n * drop data when the tail hits the head.\n */\nstruct ring_buffer *__ring_buffer_alloc(unsigned long size, unsigned flags,\n\t\t\t\t\tstruct lock_class_key *key)\n{\n\tstruct ring_buffer *buffer;\n\tlong nr_pages;\n\tint bsize;\n\tint cpu;\n\n\t/* keep it in its own cache line */\n\tbuffer = kzalloc(ALIGN(sizeof(*buffer), cache_line_size()),\n\t\t\t GFP_KERNEL);\n\tif (!buffer)\n\t\treturn NULL;\n\n\tif (!alloc_cpumask_var(&buffer->cpumask, GFP_KERNEL))\n\t\tgoto fail_free_buffer;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\tbuffer->flags = flags;\n\tbuffer->clock = trace_clock_local;\n\tbuffer->reader_lock_key = key;\n\n\tinit_irq_work(&buffer->irq_work.work, rb_wake_up_waiters);\n\tinit_waitqueue_head(&buffer->irq_work.waiters);\n\n\t/* need at least two pages */\n\tif (nr_pages < 2)\n\t\tnr_pages = 2;\n\n\t/*\n\t * In case of non-hotplug cpu, if the ring-buffer is allocated\n\t * in early initcall, it will not be notified of secondary cpus.\n\t * In that off case, we need to allocate for all possible cpus.\n\t */\n#ifdef CONFIG_HOTPLUG_CPU\n\tcpu_notifier_register_begin();\n\tcpumask_copy(buffer->cpumask, cpu_online_mask);\n#else\n\tcpumask_copy(buffer->cpumask, cpu_possible_mask);\n#endif\n\tbuffer->cpus = nr_cpu_ids;\n\n\tbsize = sizeof(void *) * nr_cpu_ids;\n\tbuffer->buffers = kzalloc(ALIGN(bsize, cache_line_size()),\n\t\t\t\t  GFP_KERNEL);\n\tif (!buffer->buffers)\n\t\tgoto fail_free_cpumask;\n\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tbuffer->buffers[cpu] =\n\t\t\trb_allocate_cpu_buffer(buffer, nr_pages, cpu);\n\t\tif (!buffer->buffers[cpu])\n\t\t\tgoto fail_free_buffers;\n\t}\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tbuffer->cpu_notify.notifier_call = rb_cpu_notify;\n\tbuffer->cpu_notify.priority = 0;\n\t__register_cpu_notifier(&buffer->cpu_notify);\n\tcpu_notifier_register_done();\n#endif\n\n\tmutex_init(&buffer->mutex);\n\n\treturn buffer;\n\n fail_free_buffers:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tif (buffer->buffers[cpu])\n\t\t\trb_free_cpu_buffer(buffer->buffers[cpu]);\n\t}\n\tkfree(buffer->buffers);\n\n fail_free_cpumask:\n\tfree_cpumask_var(buffer->cpumask);\n#ifdef CONFIG_HOTPLUG_CPU\n\tcpu_notifier_register_done();\n#endif\n\n fail_free_buffer:\n\tkfree(buffer);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(__ring_buffer_alloc);\n\n/**\n * ring_buffer_free - free a ring buffer.\n * @buffer: the buffer to free.\n */\nvoid\nring_buffer_free(struct ring_buffer *buffer)\n{\n\tint cpu;\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tcpu_notifier_register_begin();\n\t__unregister_cpu_notifier(&buffer->cpu_notify);\n#endif\n\n\tfor_each_buffer_cpu(buffer, cpu)\n\t\trb_free_cpu_buffer(buffer->buffers[cpu]);\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tcpu_notifier_register_done();\n#endif\n\n\tkfree(buffer->buffers);\n\tfree_cpumask_var(buffer->cpumask);\n\n\tkfree(buffer);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_free);\n\nvoid ring_buffer_set_clock(struct ring_buffer *buffer,\n\t\t\t   u64 (*clock)(void))\n{\n\tbuffer->clock = clock;\n}\n\nstatic void rb_reset_cpu(struct ring_buffer_per_cpu *cpu_buffer);\n\nstatic inline unsigned long rb_page_entries(struct buffer_page *bpage)\n{\n\treturn local_read(&bpage->entries) & RB_WRITE_MASK;\n}\n\nstatic inline unsigned long rb_page_write(struct buffer_page *bpage)\n{\n\treturn local_read(&bpage->write) & RB_WRITE_MASK;\n}\n\nstatic int\nrb_remove_pages(struct ring_buffer_per_cpu *cpu_buffer, unsigned long nr_pages)\n{\n\tstruct list_head *tail_page, *to_remove, *next_page;\n\tstruct buffer_page *to_remove_page, *tmp_iter_page;\n\tstruct buffer_page *last_page, *first_page;\n\tunsigned long nr_removed;\n\tunsigned long head_bit;\n\tint page_entries;\n\n\thead_bit = 0;\n\n\traw_spin_lock_irq(&cpu_buffer->reader_lock);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\t/*\n\t * We don't race with the readers since we have acquired the reader\n\t * lock. We also don't race with writers after disabling recording.\n\t * This makes it easy to figure out the first and the last page to be\n\t * removed from the list. We unlink all the pages in between including\n\t * the first and last pages. This is done in a busy loop so that we\n\t * lose the least number of traces.\n\t * The pages are freed after we restart recording and unlock readers.\n\t */\n\ttail_page = &cpu_buffer->tail_page->list;\n\n\t/*\n\t * tail page might be on reader page, we remove the next page\n\t * from the ring buffer\n\t */\n\tif (cpu_buffer->tail_page == cpu_buffer->reader_page)\n\t\ttail_page = rb_list_head(tail_page->next);\n\tto_remove = tail_page;\n\n\t/* start of pages to remove */\n\tfirst_page = list_entry(rb_list_head(to_remove->next),\n\t\t\t\tstruct buffer_page, list);\n\n\tfor (nr_removed = 0; nr_removed < nr_pages; nr_removed++) {\n\t\tto_remove = rb_list_head(to_remove)->next;\n\t\thead_bit |= (unsigned long)to_remove & RB_PAGE_HEAD;\n\t}\n\n\tnext_page = rb_list_head(to_remove)->next;\n\n\t/*\n\t * Now we remove all pages between tail_page and next_page.\n\t * Make sure that we have head_bit value preserved for the\n\t * next page\n\t */\n\ttail_page->next = (struct list_head *)((unsigned long)next_page |\n\t\t\t\t\t\thead_bit);\n\tnext_page = rb_list_head(next_page);\n\tnext_page->prev = tail_page;\n\n\t/* make sure pages points to a valid page in the ring buffer */\n\tcpu_buffer->pages = next_page;\n\n\t/* update head page */\n\tif (head_bit)\n\t\tcpu_buffer->head_page = list_entry(next_page,\n\t\t\t\t\t\tstruct buffer_page, list);\n\n\t/*\n\t * change read pointer to make sure any read iterators reset\n\t * themselves\n\t */\n\tcpu_buffer->read = 0;\n\n\t/* pages are removed, resume tracing and then free the pages */\n\tatomic_dec(&cpu_buffer->record_disabled);\n\traw_spin_unlock_irq(&cpu_buffer->reader_lock);\n\n\tRB_WARN_ON(cpu_buffer, list_empty(cpu_buffer->pages));\n\n\t/* last buffer page to remove */\n\tlast_page = list_entry(rb_list_head(to_remove), struct buffer_page,\n\t\t\t\tlist);\n\ttmp_iter_page = first_page;\n\n\tdo {\n\t\tto_remove_page = tmp_iter_page;\n\t\trb_inc_page(cpu_buffer, &tmp_iter_page);\n\n\t\t/* update the counters */\n\t\tpage_entries = rb_page_entries(to_remove_page);\n\t\tif (page_entries) {\n\t\t\t/*\n\t\t\t * If something was added to this page, it was full\n\t\t\t * since it is not the tail page. So we deduct the\n\t\t\t * bytes consumed in ring buffer from here.\n\t\t\t * Increment overrun to account for the lost events.\n\t\t\t */\n\t\t\tlocal_add(page_entries, &cpu_buffer->overrun);\n\t\t\tlocal_sub(BUF_PAGE_SIZE, &cpu_buffer->entries_bytes);\n\t\t}\n\n\t\t/*\n\t\t * We have already removed references to this list item, just\n\t\t * free up the buffer_page and its page\n\t\t */\n\t\tfree_buffer_page(to_remove_page);\n\t\tnr_removed--;\n\n\t} while (to_remove_page != last_page);\n\n\tRB_WARN_ON(cpu_buffer, nr_removed);\n\n\treturn nr_removed == 0;\n}\n\nstatic int\nrb_insert_pages(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *pages = &cpu_buffer->new_pages;\n\tint retries, success;\n\n\traw_spin_lock_irq(&cpu_buffer->reader_lock);\n\t/*\n\t * We are holding the reader lock, so the reader page won't be swapped\n\t * in the ring buffer. Now we are racing with the writer trying to\n\t * move head page and the tail page.\n\t * We are going to adapt the reader page update process where:\n\t * 1. We first splice the start and end of list of new pages between\n\t *    the head page and its previous page.\n\t * 2. We cmpxchg the prev_page->next to point from head page to the\n\t *    start of new pages list.\n\t * 3. Finally, we update the head->prev to the end of new list.\n\t *\n\t * We will try this process 10 times, to make sure that we don't keep\n\t * spinning.\n\t */\n\tretries = 10;\n\tsuccess = 0;\n\twhile (retries--) {\n\t\tstruct list_head *head_page, *prev_page, *r;\n\t\tstruct list_head *last_page, *first_page;\n\t\tstruct list_head *head_page_with_bit;\n\n\t\thead_page = &rb_set_head_page(cpu_buffer)->list;\n\t\tif (!head_page)\n\t\t\tbreak;\n\t\tprev_page = head_page->prev;\n\n\t\tfirst_page = pages->next;\n\t\tlast_page  = pages->prev;\n\n\t\thead_page_with_bit = (struct list_head *)\n\t\t\t\t     ((unsigned long)head_page | RB_PAGE_HEAD);\n\n\t\tlast_page->next = head_page_with_bit;\n\t\tfirst_page->prev = prev_page;\n\n\t\tr = cmpxchg(&prev_page->next, head_page_with_bit, first_page);\n\n\t\tif (r == head_page_with_bit) {\n\t\t\t/*\n\t\t\t * yay, we replaced the page pointer to our new list,\n\t\t\t * now, we just have to update to head page's prev\n\t\t\t * pointer to point to end of list\n\t\t\t */\n\t\t\thead_page->prev = last_page;\n\t\t\tsuccess = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (success)\n\t\tINIT_LIST_HEAD(pages);\n\t/*\n\t * If we weren't successful in adding in new pages, warn and stop\n\t * tracing\n\t */\n\tRB_WARN_ON(cpu_buffer, !success);\n\traw_spin_unlock_irq(&cpu_buffer->reader_lock);\n\n\t/* free pages if they weren't inserted */\n\tif (!success) {\n\t\tstruct buffer_page *bpage, *tmp;\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\t list) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\treturn success;\n}\n\nstatic void rb_update_pages(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tint success;\n\n\tif (cpu_buffer->nr_pages_to_update > 0)\n\t\tsuccess = rb_insert_pages(cpu_buffer);\n\telse\n\t\tsuccess = rb_remove_pages(cpu_buffer,\n\t\t\t\t\t-cpu_buffer->nr_pages_to_update);\n\n\tif (success)\n\t\tcpu_buffer->nr_pages += cpu_buffer->nr_pages_to_update;\n}\n\nstatic void update_pages_handler(struct work_struct *work)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = container_of(work,\n\t\t\tstruct ring_buffer_per_cpu, update_pages_work);\n\trb_update_pages(cpu_buffer);\n\tcomplete(&cpu_buffer->update_done);\n}\n\n/**\n * ring_buffer_resize - resize the ring buffer\n * @buffer: the buffer to resize.\n * @size: the new size.\n * @cpu_id: the cpu buffer to resize\n *\n * Minimum size is 2 * BUF_PAGE_SIZE.\n *\n * Returns 0 on success and < 0 on failure.\n */\nint ring_buffer_resize(struct ring_buffer *buffer, unsigned long size,\n\t\t\tint cpu_id)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long nr_pages;\n\tint cpu, err = 0;\n\n\t/*\n\t * Always succeed at resizing a non-existent buffer:\n\t */\n\tif (!buffer)\n\t\treturn size;\n\n\t/* Make sure the requested buffer exists */\n\tif (cpu_id != RING_BUFFER_ALL_CPUS &&\n\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\treturn size;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\n\t/* we need a minimum of two pages */\n\tif (nr_pages < 2)\n\t\tnr_pages = 2;\n\n\tsize = nr_pages * BUF_PAGE_SIZE;\n\n\t/*\n\t * Don't succeed if resizing is disabled, as a reader might be\n\t * manipulating the ring buffer and is expecting a sane state while\n\t * this is true.\n\t */\n\tif (atomic_read(&buffer->resize_disabled))\n\t\treturn -EBUSY;\n\n\t/* prevent another thread from changing buffer sizes */\n\tmutex_lock(&buffer->mutex);\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\t/* calculate the pages to update */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\t\tcpu_buffer->nr_pages;\n\t\t\t/*\n\t\t\t * nothing more to do for removing pages or no update\n\t\t\t */\n\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * to add pages, make sure all new pages can be\n\t\t\t * allocated without receiving ENOMEM\n\t\t\t */\n\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\t\tif (__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t\t&cpu_buffer->new_pages, cpu)) {\n\t\t\t\t/* not enough memory for new pages */\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\n\t\tget_online_cpus();\n\t\t/*\n\t\t * Fire off all the required work handlers\n\t\t * We can't schedule on offline CPUs, but it's not necessary\n\t\t * since we can change their buffer sizes without any race.\n\t\t */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\t/* Can't run something on an offline CPU. */\n\t\t\tif (!cpu_online(cpu)) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t\t} else {\n\t\t\t\tschedule_work_on(cpu,\n\t\t\t\t\t\t&cpu_buffer->update_pages_work);\n\t\t\t}\n\t\t}\n\n\t\t/* wait for all the updates to complete */\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\tif (cpu_online(cpu))\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t}\n\n\t\tput_online_cpus();\n\t} else {\n\t\t/* Make sure this CPU has been intitialized */\n\t\tif (!cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\t\tgoto out;\n\n\t\tcpu_buffer = buffer->buffers[cpu_id];\n\n\t\tif (nr_pages == cpu_buffer->nr_pages)\n\t\t\tgoto out;\n\n\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\tcpu_buffer->nr_pages;\n\n\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\tif (cpu_buffer->nr_pages_to_update > 0 &&\n\t\t\t__rb_allocate_pages(cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t    &cpu_buffer->new_pages, cpu_id)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tget_online_cpus();\n\n\t\t/* Can't run something on an offline CPU. */\n\t\tif (!cpu_online(cpu_id))\n\t\t\trb_update_pages(cpu_buffer);\n\t\telse {\n\t\t\tschedule_work_on(cpu_id,\n\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\tput_online_cpus();\n\t}\n\n out:\n\t/*\n\t * The ring buffer resize can happen with the ring buffer\n\t * enabled, so that the update disturbs the tracing as little\n\t * as possible. But if the buffer is disabled, we do not need\n\t * to worry about that, and we can take the time to verify\n\t * that the buffer is not corrupt.\n\t */\n\tif (atomic_read(&buffer->record_disabled)) {\n\t\tatomic_inc(&buffer->record_disabled);\n\t\t/*\n\t\t * Even though the buffer was disabled, we must make sure\n\t\t * that it is truly disabled before calling rb_check_pages.\n\t\t * There could have been a race between checking\n\t\t * record_disable and incrementing it.\n\t\t */\n\t\tsynchronize_sched();\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\trb_check_pages(cpu_buffer);\n\t\t}\n\t\tatomic_dec(&buffer->record_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n\treturn size;\n\n out_err:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tstruct buffer_page *bpage, *tmp;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\n\t\tif (list_empty(&cpu_buffer->new_pages))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\tlist) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\tmutex_unlock(&buffer->mutex);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_resize);\n\nvoid ring_buffer_change_overwrite(struct ring_buffer *buffer, int val)\n{\n\tmutex_lock(&buffer->mutex);\n\tif (val)\n\t\tbuffer->flags |= RB_FL_OVERWRITE;\n\telse\n\t\tbuffer->flags &= ~RB_FL_OVERWRITE;\n\tmutex_unlock(&buffer->mutex);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_change_overwrite);\n\nstatic inline void *\n__rb_data_page_index(struct buffer_data_page *bpage, unsigned index)\n{\n\treturn bpage->data + index;\n}\n\nstatic inline void *__rb_page_index(struct buffer_page *bpage, unsigned index)\n{\n\treturn bpage->page->data + index;\n}\n\nstatic inline struct ring_buffer_event *\nrb_reader_event(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn __rb_page_index(cpu_buffer->reader_page,\n\t\t\t       cpu_buffer->reader_page->read);\n}\n\nstatic inline struct ring_buffer_event *\nrb_iter_head_event(struct ring_buffer_iter *iter)\n{\n\treturn __rb_page_index(iter->head_page, iter->head);\n}\n\nstatic inline unsigned rb_page_commit(struct buffer_page *bpage)\n{\n\treturn local_read(&bpage->page->commit);\n}\n\n/* Size is determined by what has been committed */\nstatic inline unsigned rb_page_size(struct buffer_page *bpage)\n{\n\treturn rb_page_commit(bpage);\n}\n\nstatic inline unsigned\nrb_commit_index(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn rb_page_commit(cpu_buffer->commit_page);\n}\n\nstatic inline unsigned\nrb_event_index(struct ring_buffer_event *event)\n{\n\tunsigned long addr = (unsigned long)event;\n\n\treturn (addr & ~PAGE_MASK) - BUF_PAGE_HDR_SIZE;\n}\n\nstatic void rb_inc_iter(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\n\t/*\n\t * The iterator could be on the reader page (it starts there).\n\t * But the head could have moved, since the reader was\n\t * found. Check for this case and assign the iterator\n\t * to the head page instead of next.\n\t */\n\tif (iter->head_page == cpu_buffer->reader_page)\n\t\titer->head_page = rb_set_head_page(cpu_buffer);\n\telse\n\t\trb_inc_page(cpu_buffer, &iter->head_page);\n\n\titer->read_stamp = iter->head_page->page->time_stamp;\n\titer->head = 0;\n}\n\n/*\n * rb_handle_head_page - writer hit the head page\n *\n * Returns: +1 to retry page\n *           0 to continue\n *          -1 on error\n */\nstatic int\nrb_handle_head_page(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t    struct buffer_page *tail_page,\n\t\t    struct buffer_page *next_page)\n{\n\tstruct buffer_page *new_head;\n\tint entries;\n\tint type;\n\tint ret;\n\n\tentries = rb_page_entries(next_page);\n\n\t/*\n\t * The hard part is here. We need to move the head\n\t * forward, and protect against both readers on\n\t * other CPUs and writers coming in via interrupts.\n\t */\n\ttype = rb_head_page_set_update(cpu_buffer, next_page, tail_page,\n\t\t\t\t       RB_PAGE_HEAD);\n\n\t/*\n\t * type can be one of four:\n\t *  NORMAL - an interrupt already moved it for us\n\t *  HEAD   - we are the first to get here.\n\t *  UPDATE - we are the interrupt interrupting\n\t *           a current move.\n\t *  MOVED  - a reader on another CPU moved the next\n\t *           pointer to its reader page. Give up\n\t *           and try again.\n\t */\n\n\tswitch (type) {\n\tcase RB_PAGE_HEAD:\n\t\t/*\n\t\t * We changed the head to UPDATE, thus\n\t\t * it is our responsibility to update\n\t\t * the counters.\n\t\t */\n\t\tlocal_add(entries, &cpu_buffer->overrun);\n\t\tlocal_sub(BUF_PAGE_SIZE, &cpu_buffer->entries_bytes);\n\n\t\t/*\n\t\t * The entries will be zeroed out when we move the\n\t\t * tail page.\n\t\t */\n\n\t\t/* still more to do */\n\t\tbreak;\n\n\tcase RB_PAGE_UPDATE:\n\t\t/*\n\t\t * This is an interrupt that interrupt the\n\t\t * previous update. Still more to do.\n\t\t */\n\t\tbreak;\n\tcase RB_PAGE_NORMAL:\n\t\t/*\n\t\t * An interrupt came in before the update\n\t\t * and processed this for us.\n\t\t * Nothing left to do.\n\t\t */\n\t\treturn 1;\n\tcase RB_PAGE_MOVED:\n\t\t/*\n\t\t * The reader is on another CPU and just did\n\t\t * a swap with our next_page.\n\t\t * Try again.\n\t\t */\n\t\treturn 1;\n\tdefault:\n\t\tRB_WARN_ON(cpu_buffer, 1); /* WTF??? */\n\t\treturn -1;\n\t}\n\n\t/*\n\t * Now that we are here, the old head pointer is\n\t * set to UPDATE. This will keep the reader from\n\t * swapping the head page with the reader page.\n\t * The reader (on another CPU) will spin till\n\t * we are finished.\n\t *\n\t * We just need to protect against interrupts\n\t * doing the job. We will set the next pointer\n\t * to HEAD. After that, we set the old pointer\n\t * to NORMAL, but only if it was HEAD before.\n\t * otherwise we are an interrupt, and only\n\t * want the outer most commit to reset it.\n\t */\n\tnew_head = next_page;\n\trb_inc_page(cpu_buffer, &new_head);\n\n\tret = rb_head_page_set_head(cpu_buffer, new_head, next_page,\n\t\t\t\t    RB_PAGE_NORMAL);\n\n\t/*\n\t * Valid returns are:\n\t *  HEAD   - an interrupt came in and already set it.\n\t *  NORMAL - One of two things:\n\t *            1) We really set it.\n\t *            2) A bunch of interrupts came in and moved\n\t *               the page forward again.\n\t */\n\tswitch (ret) {\n\tcase RB_PAGE_HEAD:\n\tcase RB_PAGE_NORMAL:\n\t\t/* OK */\n\t\tbreak;\n\tdefault:\n\t\tRB_WARN_ON(cpu_buffer, 1);\n\t\treturn -1;\n\t}\n\n\t/*\n\t * It is possible that an interrupt came in,\n\t * set the head up, then more interrupts came in\n\t * and moved it again. When we get back here,\n\t * the page would have been set to NORMAL but we\n\t * just set it back to HEAD.\n\t *\n\t * How do you detect this? Well, if that happened\n\t * the tail page would have moved.\n\t */\n\tif (ret == RB_PAGE_NORMAL) {\n\t\tstruct buffer_page *buffer_tail_page;\n\n\t\tbuffer_tail_page = READ_ONCE(cpu_buffer->tail_page);\n\t\t/*\n\t\t * If the tail had moved passed next, then we need\n\t\t * to reset the pointer.\n\t\t */\n\t\tif (buffer_tail_page != tail_page &&\n\t\t    buffer_tail_page != next_page)\n\t\t\trb_head_page_set_normal(cpu_buffer, new_head,\n\t\t\t\t\t\tnext_page,\n\t\t\t\t\t\tRB_PAGE_HEAD);\n\t}\n\n\t/*\n\t * If this was the outer most commit (the one that\n\t * changed the original pointer from HEAD to UPDATE),\n\t * then it is up to us to reset it to NORMAL.\n\t */\n\tif (type == RB_PAGE_HEAD) {\n\t\tret = rb_head_page_set_normal(cpu_buffer, next_page,\n\t\t\t\t\t      tail_page,\n\t\t\t\t\t      RB_PAGE_UPDATE);\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t       ret != RB_PAGE_UPDATE))\n\t\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline void\nrb_reset_tail(struct ring_buffer_per_cpu *cpu_buffer,\n\t      unsigned long tail, struct rb_event_info *info)\n{\n\tstruct buffer_page *tail_page = info->tail_page;\n\tstruct ring_buffer_event *event;\n\tunsigned long length = info->length;\n\n\t/*\n\t * Only the event that crossed the page boundary\n\t * must fill the old tail_page with padding.\n\t */\n\tif (tail >= BUF_PAGE_SIZE) {\n\t\t/*\n\t\t * If the page was filled, then we still need\n\t\t * to update the real_end. Reset it to zero\n\t\t * and the reader will ignore it.\n\t\t */\n\t\tif (tail == BUF_PAGE_SIZE)\n\t\t\ttail_page->real_end = 0;\n\n\t\tlocal_sub(length, &tail_page->write);\n\t\treturn;\n\t}\n\n\tevent = __rb_page_index(tail_page, tail);\n\tkmemcheck_annotate_bitfield(event, bitfield);\n\n\t/* account for padding bytes */\n\tlocal_add(BUF_PAGE_SIZE - tail, &cpu_buffer->entries_bytes);\n\n\t/*\n\t * Save the original length to the meta data.\n\t * This will be used by the reader to add lost event\n\t * counter.\n\t */\n\ttail_page->real_end = tail;\n\n\t/*\n\t * If this event is bigger than the minimum size, then\n\t * we need to be careful that we don't subtract the\n\t * write counter enough to allow another writer to slip\n\t * in on this page.\n\t * We put in a discarded commit instead, to make sure\n\t * that this space is not used again.\n\t *\n\t * If we are less than the minimum size, we don't need to\n\t * worry about it.\n\t */\n\tif (tail > (BUF_PAGE_SIZE - RB_EVNT_MIN_SIZE)) {\n\t\t/* No room for any events */\n\n\t\t/* Mark the rest of the page with padding */\n\t\trb_event_set_padding(event);\n\n\t\t/* Set the write back to the previous setting */\n\t\tlocal_sub(length, &tail_page->write);\n\t\treturn;\n\t}\n\n\t/* Put in a discarded event */\n\tevent->array[0] = (BUF_PAGE_SIZE - tail) - RB_EVNT_HDR_SIZE;\n\tevent->type_len = RINGBUF_TYPE_PADDING;\n\t/* time delta must be non zero */\n\tevent->time_delta = 1;\n\n\t/* Set write to end of buffer */\n\tlength = (tail + length) - BUF_PAGE_SIZE;\n\tlocal_sub(length, &tail_page->write);\n}\n\nstatic inline void rb_end_commit(struct ring_buffer_per_cpu *cpu_buffer);\n\n/*\n * This is the slow path, force gcc not to inline it.\n */\nstatic noinline struct ring_buffer_event *\nrb_move_tail(struct ring_buffer_per_cpu *cpu_buffer,\n\t     unsigned long tail, struct rb_event_info *info)\n{\n\tstruct buffer_page *tail_page = info->tail_page;\n\tstruct buffer_page *commit_page = cpu_buffer->commit_page;\n\tstruct ring_buffer *buffer = cpu_buffer->buffer;\n\tstruct buffer_page *next_page;\n\tint ret;\n\n\tnext_page = tail_page;\n\n\trb_inc_page(cpu_buffer, &next_page);\n\n\t/*\n\t * If for some reason, we had an interrupt storm that made\n\t * it all the way around the buffer, bail, and warn\n\t * about it.\n\t */\n\tif (unlikely(next_page == commit_page)) {\n\t\tlocal_inc(&cpu_buffer->commit_overrun);\n\t\tgoto out_reset;\n\t}\n\n\t/*\n\t * This is where the fun begins!\n\t *\n\t * We are fighting against races between a reader that\n\t * could be on another CPU trying to swap its reader\n\t * page with the buffer head.\n\t *\n\t * We are also fighting against interrupts coming in and\n\t * moving the head or tail on us as well.\n\t *\n\t * If the next page is the head page then we have filled\n\t * the buffer, unless the commit page is still on the\n\t * reader page.\n\t */\n\tif (rb_is_head_page(cpu_buffer, next_page, &tail_page->list)) {\n\n\t\t/*\n\t\t * If the commit is not on the reader page, then\n\t\t * move the header page.\n\t\t */\n\t\tif (!rb_is_reader_page(cpu_buffer->commit_page)) {\n\t\t\t/*\n\t\t\t * If we are not in overwrite mode,\n\t\t\t * this is easy, just stop here.\n\t\t\t */\n\t\t\tif (!(buffer->flags & RB_FL_OVERWRITE)) {\n\t\t\t\tlocal_inc(&cpu_buffer->dropped_events);\n\t\t\t\tgoto out_reset;\n\t\t\t}\n\n\t\t\tret = rb_handle_head_page(cpu_buffer,\n\t\t\t\t\t\t  tail_page,\n\t\t\t\t\t\t  next_page);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out_reset;\n\t\t\tif (ret)\n\t\t\t\tgoto out_again;\n\t\t} else {\n\t\t\t/*\n\t\t\t * We need to be careful here too. The\n\t\t\t * commit page could still be on the reader\n\t\t\t * page. We could have a small buffer, and\n\t\t\t * have filled up the buffer with events\n\t\t\t * from interrupts and such, and wrapped.\n\t\t\t *\n\t\t\t * Note, if the tail page is also the on the\n\t\t\t * reader_page, we let it move out.\n\t\t\t */\n\t\t\tif (unlikely((cpu_buffer->commit_page !=\n\t\t\t\t      cpu_buffer->tail_page) &&\n\t\t\t\t     (cpu_buffer->commit_page ==\n\t\t\t\t      cpu_buffer->reader_page))) {\n\t\t\t\tlocal_inc(&cpu_buffer->commit_overrun);\n\t\t\t\tgoto out_reset;\n\t\t\t}\n\t\t}\n\t}\n\n\trb_tail_page_update(cpu_buffer, tail_page, next_page);\n\n out_again:\n\n\trb_reset_tail(cpu_buffer, tail, info);\n\n\t/* Commit what we have for now. */\n\trb_end_commit(cpu_buffer);\n\t/* rb_end_commit() decs committing */\n\tlocal_inc(&cpu_buffer->committing);\n\n\t/* fail and let the caller try again */\n\treturn ERR_PTR(-EAGAIN);\n\n out_reset:\n\t/* reset write */\n\trb_reset_tail(cpu_buffer, tail, info);\n\n\treturn NULL;\n}\n\n/* Slow path, do not inline */\nstatic noinline struct ring_buffer_event *\nrb_add_time_stamp(struct ring_buffer_event *event, u64 delta)\n{\n\tevent->type_len = RINGBUF_TYPE_TIME_EXTEND;\n\n\t/* Not the first event on the page? */\n\tif (rb_event_index(event)) {\n\t\tevent->time_delta = delta & TS_MASK;\n\t\tevent->array[0] = delta >> TS_SHIFT;\n\t} else {\n\t\t/* nope, just zero it */\n\t\tevent->time_delta = 0;\n\t\tevent->array[0] = 0;\n\t}\n\n\treturn skip_time_extend(event);\n}\n\nstatic inline bool rb_event_is_commit(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t     struct ring_buffer_event *event);\n\n/**\n * rb_update_event - update event type and data\n * @event: the event to update\n * @type: the type of event\n * @length: the size of the event field in the ring buffer\n *\n * Update the type and data fields of the event. The length\n * is the actual size that is written to the ring buffer,\n * and with this, we can determine what to place into the\n * data field.\n */\nstatic void\nrb_update_event(struct ring_buffer_per_cpu *cpu_buffer,\n\t\tstruct ring_buffer_event *event,\n\t\tstruct rb_event_info *info)\n{\n\tunsigned length = info->length;\n\tu64 delta = info->delta;\n\n\t/* Only a commit updates the timestamp */\n\tif (unlikely(!rb_event_is_commit(cpu_buffer, event)))\n\t\tdelta = 0;\n\n\t/*\n\t * If we need to add a timestamp, then we\n\t * add it to the start of the resevered space.\n\t */\n\tif (unlikely(info->add_timestamp)) {\n\t\tevent = rb_add_time_stamp(event, delta);\n\t\tlength -= RB_LEN_TIME_EXTEND;\n\t\tdelta = 0;\n\t}\n\n\tevent->time_delta = delta;\n\tlength -= RB_EVNT_HDR_SIZE;\n\tif (length > RB_MAX_SMALL_DATA || RB_FORCE_8BYTE_ALIGNMENT) {\n\t\tevent->type_len = 0;\n\t\tevent->array[0] = length;\n\t} else\n\t\tevent->type_len = DIV_ROUND_UP(length, RB_ALIGNMENT);\n}\n\nstatic unsigned rb_calculate_event_length(unsigned length)\n{\n\tstruct ring_buffer_event event; /* Used only for sizeof array */\n\n\t/* zero length can cause confusions */\n\tif (!length)\n\t\tlength++;\n\n\tif (length > RB_MAX_SMALL_DATA || RB_FORCE_8BYTE_ALIGNMENT)\n\t\tlength += sizeof(event.array[0]);\n\n\tlength += RB_EVNT_HDR_SIZE;\n\tlength = ALIGN(length, RB_ARCH_ALIGNMENT);\n\n\t/*\n\t * In case the time delta is larger than the 27 bits for it\n\t * in the header, we need to add a timestamp. If another\n\t * event comes in when trying to discard this one to increase\n\t * the length, then the timestamp will be added in the allocated\n\t * space of this event. If length is bigger than the size needed\n\t * for the TIME_EXTEND, then padding has to be used. The events\n\t * length must be either RB_LEN_TIME_EXTEND, or greater than or equal\n\t * to RB_LEN_TIME_EXTEND + 8, as 8 is the minimum size for padding.\n\t * As length is a multiple of 4, we only need to worry if it\n\t * is 12 (RB_LEN_TIME_EXTEND + 4).\n\t */\n\tif (length == RB_LEN_TIME_EXTEND + RB_ALIGNMENT)\n\t\tlength += RB_ALIGNMENT;\n\n\treturn length;\n}\n\n#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\nstatic inline bool sched_clock_stable(void)\n{\n\treturn true;\n}\n#endif\n\nstatic inline int\nrb_try_to_discard(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t  struct ring_buffer_event *event)\n{\n\tunsigned long new_index, old_index;\n\tstruct buffer_page *bpage;\n\tunsigned long index;\n\tunsigned long addr;\n\n\tnew_index = rb_event_index(event);\n\told_index = new_index + rb_event_ts_length(event);\n\taddr = (unsigned long)event;\n\taddr &= PAGE_MASK;\n\n\tbpage = READ_ONCE(cpu_buffer->tail_page);\n\n\tif (bpage->page == (void *)addr && rb_page_write(bpage) == old_index) {\n\t\tunsigned long write_mask =\n\t\t\tlocal_read(&bpage->write) & ~RB_WRITE_MASK;\n\t\tunsigned long event_length = rb_event_length(event);\n\t\t/*\n\t\t * This is on the tail page. It is possible that\n\t\t * a write could come in and move the tail page\n\t\t * and write to the next page. That is fine\n\t\t * because we just shorten what is on this page.\n\t\t */\n\t\told_index += write_mask;\n\t\tnew_index += write_mask;\n\t\tindex = local_cmpxchg(&bpage->write, old_index, new_index);\n\t\tif (index == old_index) {\n\t\t\t/* update counters */\n\t\t\tlocal_sub(event_length, &cpu_buffer->entries_bytes);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t/* could not discard */\n\treturn 0;\n}\n\nstatic void rb_start_commit(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tlocal_inc(&cpu_buffer->committing);\n\tlocal_inc(&cpu_buffer->commits);\n}\n\nstatic void\nrb_set_commit_to_write(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tunsigned long max_count;\n\n\t/*\n\t * We only race with interrupts and NMIs on this CPU.\n\t * If we own the commit event, then we can commit\n\t * all others that interrupted us, since the interruptions\n\t * are in stack format (they finish before they come\n\t * back to us). This allows us to do a simple loop to\n\t * assign the commit to the tail.\n\t */\n again:\n\tmax_count = cpu_buffer->nr_pages * 100;\n\n\twhile (cpu_buffer->commit_page != READ_ONCE(cpu_buffer->tail_page)) {\n\t\tif (RB_WARN_ON(cpu_buffer, !(--max_count)))\n\t\t\treturn;\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t       rb_is_reader_page(cpu_buffer->tail_page)))\n\t\t\treturn;\n\t\tlocal_set(&cpu_buffer->commit_page->page->commit,\n\t\t\t  rb_page_write(cpu_buffer->commit_page));\n\t\trb_inc_page(cpu_buffer, &cpu_buffer->commit_page);\n\t\t/* Only update the write stamp if the page has an event */\n\t\tif (rb_page_write(cpu_buffer->commit_page))\n\t\t\tcpu_buffer->write_stamp =\n\t\t\t\tcpu_buffer->commit_page->page->time_stamp;\n\t\t/* add barrier to keep gcc from optimizing too much */\n\t\tbarrier();\n\t}\n\twhile (rb_commit_index(cpu_buffer) !=\n\t       rb_page_write(cpu_buffer->commit_page)) {\n\n\t\tlocal_set(&cpu_buffer->commit_page->page->commit,\n\t\t\t  rb_page_write(cpu_buffer->commit_page));\n\t\tRB_WARN_ON(cpu_buffer,\n\t\t\t   local_read(&cpu_buffer->commit_page->page->commit) &\n\t\t\t   ~RB_WRITE_MASK);\n\t\tbarrier();\n\t}\n\n\t/* again, keep gcc from optimizing */\n\tbarrier();\n\n\t/*\n\t * If an interrupt came in just after the first while loop\n\t * and pushed the tail page forward, we will be left with\n\t * a dangling commit that will never go forward.\n\t */\n\tif (unlikely(cpu_buffer->commit_page != READ_ONCE(cpu_buffer->tail_page)))\n\t\tgoto again;\n}\n\nstatic inline void rb_end_commit(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tunsigned long commits;\n\n\tif (RB_WARN_ON(cpu_buffer,\n\t\t       !local_read(&cpu_buffer->committing)))\n\t\treturn;\n\n again:\n\tcommits = local_read(&cpu_buffer->commits);\n\t/* synchronize with interrupts */\n\tbarrier();\n\tif (local_read(&cpu_buffer->committing) == 1)\n\t\trb_set_commit_to_write(cpu_buffer);\n\n\tlocal_dec(&cpu_buffer->committing);\n\n\t/* synchronize with interrupts */\n\tbarrier();\n\n\t/*\n\t * Need to account for interrupts coming in between the\n\t * updating of the commit page and the clearing of the\n\t * committing counter.\n\t */\n\tif (unlikely(local_read(&cpu_buffer->commits) != commits) &&\n\t    !local_read(&cpu_buffer->committing)) {\n\t\tlocal_inc(&cpu_buffer->committing);\n\t\tgoto again;\n\t}\n}\n\nstatic inline void rb_event_discard(struct ring_buffer_event *event)\n{\n\tif (event->type_len == RINGBUF_TYPE_TIME_EXTEND)\n\t\tevent = skip_time_extend(event);\n\n\t/* array[0] holds the actual length for the discarded event */\n\tevent->array[0] = rb_event_data_length(event) - RB_EVNT_HDR_SIZE;\n\tevent->type_len = RINGBUF_TYPE_PADDING;\n\t/* time delta must be non zero */\n\tif (!event->time_delta)\n\t\tevent->time_delta = 1;\n}\n\nstatic inline bool\nrb_event_is_commit(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t   struct ring_buffer_event *event)\n{\n\tunsigned long addr = (unsigned long)event;\n\tunsigned long index;\n\n\tindex = rb_event_index(event);\n\taddr &= PAGE_MASK;\n\n\treturn cpu_buffer->commit_page->page == (void *)addr &&\n\t\trb_commit_index(cpu_buffer) == index;\n}\n\nstatic void\nrb_update_write_stamp(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t      struct ring_buffer_event *event)\n{\n\tu64 delta;\n\n\t/*\n\t * The event first in the commit queue updates the\n\t * time stamp.\n\t */\n\tif (rb_event_is_commit(cpu_buffer, event)) {\n\t\t/*\n\t\t * A commit event that is first on a page\n\t\t * updates the write timestamp with the page stamp\n\t\t */\n\t\tif (!rb_event_index(event))\n\t\t\tcpu_buffer->write_stamp =\n\t\t\t\tcpu_buffer->commit_page->page->time_stamp;\n\t\telse if (event->type_len == RINGBUF_TYPE_TIME_EXTEND) {\n\t\t\tdelta = event->array[0];\n\t\t\tdelta <<= TS_SHIFT;\n\t\t\tdelta += event->time_delta;\n\t\t\tcpu_buffer->write_stamp += delta;\n\t\t} else\n\t\t\tcpu_buffer->write_stamp += event->time_delta;\n\t}\n}\n\nstatic void rb_commit(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t      struct ring_buffer_event *event)\n{\n\tlocal_inc(&cpu_buffer->entries);\n\trb_update_write_stamp(cpu_buffer, event);\n\trb_end_commit(cpu_buffer);\n}\n\nstatic __always_inline void\nrb_wakeups(struct ring_buffer *buffer, struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tbool pagebusy;\n\n\tif (buffer->irq_work.waiters_pending) {\n\t\tbuffer->irq_work.waiters_pending = false;\n\t\t/* irq_work_queue() supplies it's own memory barriers */\n\t\tirq_work_queue(&buffer->irq_work.work);\n\t}\n\n\tif (cpu_buffer->irq_work.waiters_pending) {\n\t\tcpu_buffer->irq_work.waiters_pending = false;\n\t\t/* irq_work_queue() supplies it's own memory barriers */\n\t\tirq_work_queue(&cpu_buffer->irq_work.work);\n\t}\n\n\tpagebusy = cpu_buffer->reader_page == cpu_buffer->commit_page;\n\n\tif (!pagebusy && cpu_buffer->irq_work.full_waiters_pending) {\n\t\tcpu_buffer->irq_work.wakeup_full = true;\n\t\tcpu_buffer->irq_work.full_waiters_pending = false;\n\t\t/* irq_work_queue() supplies it's own memory barriers */\n\t\tirq_work_queue(&cpu_buffer->irq_work.work);\n\t}\n}\n\n/*\n * The lock and unlock are done within a preempt disable section.\n * The current_context per_cpu variable can only be modified\n * by the current task between lock and unlock. But it can\n * be modified more than once via an interrupt. To pass this\n * information from the lock to the unlock without having to\n * access the 'in_interrupt()' functions again (which do show\n * a bit of overhead in something as critical as function tracing,\n * we use a bitmask trick.\n *\n *  bit 0 =  NMI context\n *  bit 1 =  IRQ context\n *  bit 2 =  SoftIRQ context\n *  bit 3 =  normal context.\n *\n * This works because this is the order of contexts that can\n * preempt other contexts. A SoftIRQ never preempts an IRQ\n * context.\n *\n * When the context is determined, the corresponding bit is\n * checked and set (if it was set, then a recursion of that context\n * happened).\n *\n * On unlock, we need to clear this bit. To do so, just subtract\n * 1 from the current_context and AND it to itself.\n *\n * (binary)\n *  101 - 1 = 100\n *  101 & 100 = 100 (clearing bit zero)\n *\n *  1010 - 1 = 1001\n *  1010 & 1001 = 1000 (clearing bit 1)\n *\n * The least significant bit can be cleared this way, and it\n * just so happens that it is the same bit corresponding to\n * the current context.\n */\n\nstatic __always_inline int\ntrace_recursive_lock(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tunsigned int val = cpu_buffer->current_context;\n\tint bit;\n\n\tif (in_interrupt()) {\n\t\tif (in_nmi())\n\t\t\tbit = RB_CTX_NMI;\n\t\telse if (in_irq())\n\t\t\tbit = RB_CTX_IRQ;\n\t\telse\n\t\t\tbit = RB_CTX_SOFTIRQ;\n\t} else\n\t\tbit = RB_CTX_NORMAL;\n\n\tif (unlikely(val & (1 << bit)))\n\t\treturn 1;\n\n\tval |= (1 << bit);\n\tcpu_buffer->current_context = val;\n\n\treturn 0;\n}\n\nstatic __always_inline void\ntrace_recursive_unlock(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tcpu_buffer->current_context &= cpu_buffer->current_context - 1;\n}\n\n/**\n * ring_buffer_unlock_commit - commit a reserved\n * @buffer: The buffer to commit to\n * @event: The event pointer to commit.\n *\n * This commits the data to the ring buffer, and releases any locks held.\n *\n * Must be paired with ring_buffer_lock_reserve.\n */\nint ring_buffer_unlock_commit(struct ring_buffer *buffer,\n\t\t\t      struct ring_buffer_event *event)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu = raw_smp_processor_id();\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\trb_commit(cpu_buffer, event);\n\n\trb_wakeups(buffer, cpu_buffer);\n\n\ttrace_recursive_unlock(cpu_buffer);\n\n\tpreempt_enable_notrace();\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_unlock_commit);\n\nstatic noinline void\nrb_handle_timestamp(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t    struct rb_event_info *info)\n{\n\tWARN_ONCE(info->delta > (1ULL << 59),\n\t\t  KERN_WARNING \"Delta way too big! %llu ts=%llu write stamp = %llu\\n%s\",\n\t\t  (unsigned long long)info->delta,\n\t\t  (unsigned long long)info->ts,\n\t\t  (unsigned long long)cpu_buffer->write_stamp,\n\t\t  sched_clock_stable() ? \"\" :\n\t\t  \"If you just came from a suspend/resume,\\n\"\n\t\t  \"please switch to the trace global clock:\\n\"\n\t\t  \"  echo global > /sys/kernel/debug/tracing/trace_clock\\n\");\n\tinfo->add_timestamp = 1;\n}\n\nstatic struct ring_buffer_event *\n__rb_reserve_next(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t  struct rb_event_info *info)\n{\n\tstruct ring_buffer_event *event;\n\tstruct buffer_page *tail_page;\n\tunsigned long tail, write;\n\n\t/*\n\t * If the time delta since the last event is too big to\n\t * hold in the time field of the event, then we append a\n\t * TIME EXTEND event ahead of the data event.\n\t */\n\tif (unlikely(info->add_timestamp))\n\t\tinfo->length += RB_LEN_TIME_EXTEND;\n\n\t/* Don't let the compiler play games with cpu_buffer->tail_page */\n\ttail_page = info->tail_page = READ_ONCE(cpu_buffer->tail_page);\n\twrite = local_add_return(info->length, &tail_page->write);\n\n\t/* set write to only the index of the write */\n\twrite &= RB_WRITE_MASK;\n\ttail = write - info->length;\n\n\t/*\n\t * If this is the first commit on the page, then it has the same\n\t * timestamp as the page itself.\n\t */\n\tif (!tail)\n\t\tinfo->delta = 0;\n\n\t/* See if we shot pass the end of this buffer page */\n\tif (unlikely(write > BUF_PAGE_SIZE))\n\t\treturn rb_move_tail(cpu_buffer, tail, info);\n\n\t/* We reserved something on the buffer */\n\n\tevent = __rb_page_index(tail_page, tail);\n\tkmemcheck_annotate_bitfield(event, bitfield);\n\trb_update_event(cpu_buffer, event, info);\n\n\tlocal_inc(&tail_page->entries);\n\n\t/*\n\t * If this is the first commit on the page, then update\n\t * its timestamp.\n\t */\n\tif (!tail)\n\t\ttail_page->page->time_stamp = info->ts;\n\n\t/* account for these added bytes */\n\tlocal_add(info->length, &cpu_buffer->entries_bytes);\n\n\treturn event;\n}\n\nstatic struct ring_buffer_event *\nrb_reserve_next_event(struct ring_buffer *buffer,\n\t\t      struct ring_buffer_per_cpu *cpu_buffer,\n\t\t      unsigned long length)\n{\n\tstruct ring_buffer_event *event;\n\tstruct rb_event_info info;\n\tint nr_loops = 0;\n\tu64 diff;\n\n\trb_start_commit(cpu_buffer);\n\n#ifdef CONFIG_RING_BUFFER_ALLOW_SWAP\n\t/*\n\t * Due to the ability to swap a cpu buffer from a buffer\n\t * it is possible it was swapped before we committed.\n\t * (committing stops a swap). We check for it here and\n\t * if it happened, we have to fail the write.\n\t */\n\tbarrier();\n\tif (unlikely(ACCESS_ONCE(cpu_buffer->buffer) != buffer)) {\n\t\tlocal_dec(&cpu_buffer->committing);\n\t\tlocal_dec(&cpu_buffer->commits);\n\t\treturn NULL;\n\t}\n#endif\n\n\tinfo.length = rb_calculate_event_length(length);\n again:\n\tinfo.add_timestamp = 0;\n\tinfo.delta = 0;\n\n\t/*\n\t * We allow for interrupts to reenter here and do a trace.\n\t * If one does, it will cause this original code to loop\n\t * back here. Even with heavy interrupts happening, this\n\t * should only happen a few times in a row. If this happens\n\t * 1000 times in a row, there must be either an interrupt\n\t * storm or we have something buggy.\n\t * Bail!\n\t */\n\tif (RB_WARN_ON(cpu_buffer, ++nr_loops > 1000))\n\t\tgoto out_fail;\n\n\tinfo.ts = rb_time_stamp(cpu_buffer->buffer);\n\tdiff = info.ts - cpu_buffer->write_stamp;\n\n\t/* make sure this diff is calculated here */\n\tbarrier();\n\n\t/* Did the write stamp get updated already? */\n\tif (likely(info.ts >= cpu_buffer->write_stamp)) {\n\t\tinfo.delta = diff;\n\t\tif (unlikely(test_time_stamp(info.delta)))\n\t\t\trb_handle_timestamp(cpu_buffer, &info);\n\t}\n\n\tevent = __rb_reserve_next(cpu_buffer, &info);\n\n\tif (unlikely(PTR_ERR(event) == -EAGAIN)) {\n\t\tif (info.add_timestamp)\n\t\t\tinfo.length -= RB_LEN_TIME_EXTEND;\n\t\tgoto again;\n\t}\n\n\tif (!event)\n\t\tgoto out_fail;\n\n\treturn event;\n\n out_fail:\n\trb_end_commit(cpu_buffer);\n\treturn NULL;\n}\n\n/**\n * ring_buffer_lock_reserve - reserve a part of the buffer\n * @buffer: the ring buffer to reserve from\n * @length: the length of the data to reserve (excluding event header)\n *\n * Returns a reseverd event on the ring buffer to copy directly to.\n * The user of this interface will need to get the body to write into\n * and can use the ring_buffer_event_data() interface.\n *\n * The length is the length of the data needed, not the event length\n * which also includes the event header.\n *\n * Must be paired with ring_buffer_unlock_commit, unless NULL is returned.\n * If NULL is returned, then nothing has been allocated or locked.\n */\nstruct ring_buffer_event *\nring_buffer_lock_reserve(struct ring_buffer *buffer, unsigned long length)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tint cpu;\n\n\t/* If we are tracing schedule, we don't want to recurse */\n\tpreempt_disable_notrace();\n\n\tif (unlikely(atomic_read(&buffer->record_disabled)))\n\t\tgoto out;\n\n\tcpu = raw_smp_processor_id();\n\n\tif (unlikely(!cpumask_test_cpu(cpu, buffer->cpumask)))\n\t\tgoto out;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\tif (unlikely(atomic_read(&cpu_buffer->record_disabled)))\n\t\tgoto out;\n\n\tif (unlikely(length > BUF_MAX_DATA_SIZE))\n\t\tgoto out;\n\n\tif (unlikely(trace_recursive_lock(cpu_buffer)))\n\t\tgoto out;\n\n\tevent = rb_reserve_next_event(buffer, cpu_buffer, length);\n\tif (!event)\n\t\tgoto out_unlock;\n\n\treturn event;\n\n out_unlock:\n\ttrace_recursive_unlock(cpu_buffer);\n out:\n\tpreempt_enable_notrace();\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_lock_reserve);\n\n/*\n * Decrement the entries to the page that an event is on.\n * The event does not even need to exist, only the pointer\n * to the page it is on. This may only be called before the commit\n * takes place.\n */\nstatic inline void\nrb_decrement_entry(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t   struct ring_buffer_event *event)\n{\n\tunsigned long addr = (unsigned long)event;\n\tstruct buffer_page *bpage = cpu_buffer->commit_page;\n\tstruct buffer_page *start;\n\n\taddr &= PAGE_MASK;\n\n\t/* Do the likely case first */\n\tif (likely(bpage->page == (void *)addr)) {\n\t\tlocal_dec(&bpage->entries);\n\t\treturn;\n\t}\n\n\t/*\n\t * Because the commit page may be on the reader page we\n\t * start with the next page and check the end loop there.\n\t */\n\trb_inc_page(cpu_buffer, &bpage);\n\tstart = bpage;\n\tdo {\n\t\tif (bpage->page == (void *)addr) {\n\t\t\tlocal_dec(&bpage->entries);\n\t\t\treturn;\n\t\t}\n\t\trb_inc_page(cpu_buffer, &bpage);\n\t} while (bpage != start);\n\n\t/* commit not part of this buffer?? */\n\tRB_WARN_ON(cpu_buffer, 1);\n}\n\n/**\n * ring_buffer_commit_discard - discard an event that has not been committed\n * @buffer: the ring buffer\n * @event: non committed event to discard\n *\n * Sometimes an event that is in the ring buffer needs to be ignored.\n * This function lets the user discard an event in the ring buffer\n * and then that event will not be read later.\n *\n * This function only works if it is called before the the item has been\n * committed. It will try to free the event from the ring buffer\n * if another event has not been added behind it.\n *\n * If another event has been added behind it, it will set the event\n * up as discarded, and perform the commit.\n *\n * If this function is called, do not call ring_buffer_unlock_commit on\n * the event.\n */\nvoid ring_buffer_discard_commit(struct ring_buffer *buffer,\n\t\t\t\tstruct ring_buffer_event *event)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\t/* The event is discarded regardless */\n\trb_event_discard(event);\n\n\tcpu = smp_processor_id();\n\tcpu_buffer = buffer->buffers[cpu];\n\n\t/*\n\t * This must only be called if the event has not been\n\t * committed yet. Thus we can assume that preemption\n\t * is still disabled.\n\t */\n\tRB_WARN_ON(buffer, !local_read(&cpu_buffer->committing));\n\n\trb_decrement_entry(cpu_buffer, event);\n\tif (rb_try_to_discard(cpu_buffer, event))\n\t\tgoto out;\n\n\t/*\n\t * The commit is still visible by the reader, so we\n\t * must still update the timestamp.\n\t */\n\trb_update_write_stamp(cpu_buffer, event);\n out:\n\trb_end_commit(cpu_buffer);\n\n\ttrace_recursive_unlock(cpu_buffer);\n\n\tpreempt_enable_notrace();\n\n}\nEXPORT_SYMBOL_GPL(ring_buffer_discard_commit);\n\n/**\n * ring_buffer_write - write data to the buffer without reserving\n * @buffer: The ring buffer to write to.\n * @length: The length of the data being written (excluding the event header)\n * @data: The data to write to the buffer.\n *\n * This is like ring_buffer_lock_reserve and ring_buffer_unlock_commit as\n * one function. If you already have the data to write to the buffer, it\n * may be easier to simply call this function.\n *\n * Note, like ring_buffer_lock_reserve, the length is the length of the data\n * and not the length of the event which would hold the header.\n */\nint ring_buffer_write(struct ring_buffer *buffer,\n\t\t      unsigned long length,\n\t\t      void *data)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tvoid *body;\n\tint ret = -EBUSY;\n\tint cpu;\n\n\tpreempt_disable_notrace();\n\n\tif (atomic_read(&buffer->record_disabled))\n\t\tgoto out;\n\n\tcpu = raw_smp_processor_id();\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\tgoto out;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\tif (atomic_read(&cpu_buffer->record_disabled))\n\t\tgoto out;\n\n\tif (length > BUF_MAX_DATA_SIZE)\n\t\tgoto out;\n\n\tif (unlikely(trace_recursive_lock(cpu_buffer)))\n\t\tgoto out;\n\n\tevent = rb_reserve_next_event(buffer, cpu_buffer, length);\n\tif (!event)\n\t\tgoto out_unlock;\n\n\tbody = rb_event_data(event);\n\n\tmemcpy(body, data, length);\n\n\trb_commit(cpu_buffer, event);\n\n\trb_wakeups(buffer, cpu_buffer);\n\n\tret = 0;\n\n out_unlock:\n\ttrace_recursive_unlock(cpu_buffer);\n\n out:\n\tpreempt_enable_notrace();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_write);\n\nstatic bool rb_per_cpu_empty(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *reader = cpu_buffer->reader_page;\n\tstruct buffer_page *head = rb_set_head_page(cpu_buffer);\n\tstruct buffer_page *commit = cpu_buffer->commit_page;\n\n\t/* In case of error, head will be NULL */\n\tif (unlikely(!head))\n\t\treturn true;\n\n\treturn reader->read == rb_page_commit(reader) &&\n\t\t(commit == reader ||\n\t\t (commit == head &&\n\t\t  head->read == rb_page_commit(commit)));\n}\n\n/**\n * ring_buffer_record_disable - stop all writes into the buffer\n * @buffer: The ring buffer to stop writes to.\n *\n * This prevents all writes to the buffer. Any attempt to write\n * to the buffer after this will fail and return NULL.\n *\n * The caller should call synchronize_sched() after this.\n */\nvoid ring_buffer_record_disable(struct ring_buffer *buffer)\n{\n\tatomic_inc(&buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_disable);\n\n/**\n * ring_buffer_record_enable - enable writes to the buffer\n * @buffer: The ring buffer to enable writes\n *\n * Note, multiple disables will need the same number of enables\n * to truly enable the writing (much like preempt_disable).\n */\nvoid ring_buffer_record_enable(struct ring_buffer *buffer)\n{\n\tatomic_dec(&buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_enable);\n\n/**\n * ring_buffer_record_off - stop all writes into the buffer\n * @buffer: The ring buffer to stop writes to.\n *\n * This prevents all writes to the buffer. Any attempt to write\n * to the buffer after this will fail and return NULL.\n *\n * This is different than ring_buffer_record_disable() as\n * it works like an on/off switch, where as the disable() version\n * must be paired with a enable().\n */\nvoid ring_buffer_record_off(struct ring_buffer *buffer)\n{\n\tunsigned int rd;\n\tunsigned int new_rd;\n\n\tdo {\n\t\trd = atomic_read(&buffer->record_disabled);\n\t\tnew_rd = rd | RB_BUFFER_OFF;\n\t} while (atomic_cmpxchg(&buffer->record_disabled, rd, new_rd) != rd);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_off);\n\n/**\n * ring_buffer_record_on - restart writes into the buffer\n * @buffer: The ring buffer to start writes to.\n *\n * This enables all writes to the buffer that was disabled by\n * ring_buffer_record_off().\n *\n * This is different than ring_buffer_record_enable() as\n * it works like an on/off switch, where as the enable() version\n * must be paired with a disable().\n */\nvoid ring_buffer_record_on(struct ring_buffer *buffer)\n{\n\tunsigned int rd;\n\tunsigned int new_rd;\n\n\tdo {\n\t\trd = atomic_read(&buffer->record_disabled);\n\t\tnew_rd = rd & ~RB_BUFFER_OFF;\n\t} while (atomic_cmpxchg(&buffer->record_disabled, rd, new_rd) != rd);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_on);\n\n/**\n * ring_buffer_record_is_on - return true if the ring buffer can write\n * @buffer: The ring buffer to see if write is enabled\n *\n * Returns true if the ring buffer is in a state that it accepts writes.\n */\nint ring_buffer_record_is_on(struct ring_buffer *buffer)\n{\n\treturn !atomic_read(&buffer->record_disabled);\n}\n\n/**\n * ring_buffer_record_disable_cpu - stop all writes into the cpu_buffer\n * @buffer: The ring buffer to stop writes to.\n * @cpu: The CPU buffer to stop\n *\n * This prevents all writes to the buffer. Any attempt to write\n * to the buffer after this will fail and return NULL.\n *\n * The caller should call synchronize_sched() after this.\n */\nvoid ring_buffer_record_disable_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tatomic_inc(&cpu_buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_disable_cpu);\n\n/**\n * ring_buffer_record_enable_cpu - enable writes to the buffer\n * @buffer: The ring buffer to enable writes\n * @cpu: The CPU to enable.\n *\n * Note, multiple disables will need the same number of enables\n * to truly enable the writing (much like preempt_disable).\n */\nvoid ring_buffer_record_enable_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tatomic_dec(&cpu_buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_enable_cpu);\n\n/*\n * The total entries in the ring buffer is the running counter\n * of entries entered into the ring buffer, minus the sum of\n * the entries read from the ring buffer and the number of\n * entries that were overwritten.\n */\nstatic inline unsigned long\nrb_num_of_entries(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn local_read(&cpu_buffer->entries) -\n\t\t(local_read(&cpu_buffer->overrun) + cpu_buffer->read);\n}\n\n/**\n * ring_buffer_oldest_event_ts - get the oldest event timestamp from the buffer\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to read from.\n */\nu64 ring_buffer_oldest_event_ts(struct ring_buffer *buffer, int cpu)\n{\n\tunsigned long flags;\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct buffer_page *bpage;\n\tu64 ret = 0;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\t/*\n\t * if the tail is on reader_page, oldest time stamp is on the reader\n\t * page\n\t */\n\tif (cpu_buffer->tail_page == cpu_buffer->reader_page)\n\t\tbpage = cpu_buffer->reader_page;\n\telse\n\t\tbpage = rb_set_head_page(cpu_buffer);\n\tif (bpage)\n\t\tret = bpage->page->time_stamp;\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_oldest_event_ts);\n\n/**\n * ring_buffer_bytes_cpu - get the number of bytes consumed in a cpu buffer\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to read from.\n */\nunsigned long ring_buffer_bytes_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->entries_bytes) - cpu_buffer->read_bytes;\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_bytes_cpu);\n\n/**\n * ring_buffer_entries_cpu - get the number of entries in a cpu buffer\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to get the entries from.\n */\nunsigned long ring_buffer_entries_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\treturn rb_num_of_entries(cpu_buffer);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_entries_cpu);\n\n/**\n * ring_buffer_overrun_cpu - get the number of overruns caused by the ring\n * buffer wrapping around (only if RB_FL_OVERWRITE is on).\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to get the number of overruns from\n */\nunsigned long ring_buffer_overrun_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->overrun);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_overrun_cpu);\n\n/**\n * ring_buffer_commit_overrun_cpu - get the number of overruns caused by\n * commits failing due to the buffer wrapping around while there are uncommitted\n * events, such as during an interrupt storm.\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to get the number of overruns from\n */\nunsigned long\nring_buffer_commit_overrun_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->commit_overrun);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_commit_overrun_cpu);\n\n/**\n * ring_buffer_dropped_events_cpu - get the number of dropped events caused by\n * the ring buffer filling up (only if RB_FL_OVERWRITE is off).\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to get the number of overruns from\n */\nunsigned long\nring_buffer_dropped_events_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->dropped_events);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_dropped_events_cpu);\n\n/**\n * ring_buffer_read_events_cpu - get the number of events successfully read\n * @buffer: The ring buffer\n * @cpu: The per CPU buffer to get the number of events read\n */\nunsigned long\nring_buffer_read_events_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\treturn cpu_buffer->read;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_events_cpu);\n\n/**\n * ring_buffer_entries - get the number of entries in a buffer\n * @buffer: The ring buffer\n *\n * Returns the total number of entries in the ring buffer\n * (all CPU entries)\n */\nunsigned long ring_buffer_entries(struct ring_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long entries = 0;\n\tint cpu;\n\n\t/* if you care about this being correct, lock the buffer */\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tentries += rb_num_of_entries(cpu_buffer);\n\t}\n\n\treturn entries;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_entries);\n\n/**\n * ring_buffer_overruns - get the number of overruns in buffer\n * @buffer: The ring buffer\n *\n * Returns the total number of overruns in the ring buffer\n * (all CPU entries)\n */\nunsigned long ring_buffer_overruns(struct ring_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long overruns = 0;\n\tint cpu;\n\n\t/* if you care about this being correct, lock the buffer */\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\toverruns += local_read(&cpu_buffer->overrun);\n\t}\n\n\treturn overruns;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_overruns);\n\nstatic void rb_iter_reset(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\n\t/* Iterator usage is expected to have record disabled */\n\titer->head_page = cpu_buffer->reader_page;\n\titer->head = cpu_buffer->reader_page->read;\n\n\titer->cache_reader_page = iter->head_page;\n\titer->cache_read = cpu_buffer->read;\n\n\tif (iter->head)\n\t\titer->read_stamp = cpu_buffer->read_stamp;\n\telse\n\t\titer->read_stamp = iter->head_page->page->time_stamp;\n}\n\n/**\n * ring_buffer_iter_reset - reset an iterator\n * @iter: The iterator to reset\n *\n * Resets the iterator, so that it will start from the beginning\n * again.\n */\nvoid ring_buffer_iter_reset(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\n\tif (!iter)\n\t\treturn;\n\n\tcpu_buffer = iter->cpu_buffer;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\trb_iter_reset(iter);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_iter_reset);\n\n/**\n * ring_buffer_iter_empty - check if an iterator has no more to read\n * @iter: The iterator to check\n */\nint ring_buffer_iter_empty(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tcpu_buffer = iter->cpu_buffer;\n\n\treturn iter->head_page == cpu_buffer->commit_page &&\n\t\titer->head == rb_commit_index(cpu_buffer);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_iter_empty);\n\nstatic void\nrb_update_read_stamp(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t     struct ring_buffer_event *event)\n{\n\tu64 delta;\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\tdelta = event->array[0];\n\t\tdelta <<= TS_SHIFT;\n\t\tdelta += event->time_delta;\n\t\tcpu_buffer->read_stamp += delta;\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\t/* FIXME: not implemented */\n\t\treturn;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\tcpu_buffer->read_stamp += event->time_delta;\n\t\treturn;\n\n\tdefault:\n\t\tBUG();\n\t}\n\treturn;\n}\n\nstatic void\nrb_update_iter_read_stamp(struct ring_buffer_iter *iter,\n\t\t\t  struct ring_buffer_event *event)\n{\n\tu64 delta;\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\tdelta = event->array[0];\n\t\tdelta <<= TS_SHIFT;\n\t\tdelta += event->time_delta;\n\t\titer->read_stamp += delta;\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\t/* FIXME: not implemented */\n\t\treturn;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\titer->read_stamp += event->time_delta;\n\t\treturn;\n\n\tdefault:\n\t\tBUG();\n\t}\n\treturn;\n}\n\nstatic struct buffer_page *\nrb_get_reader_page(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *reader = NULL;\n\tunsigned long overwrite;\n\tunsigned long flags;\n\tint nr_loops = 0;\n\tint ret;\n\n\tlocal_irq_save(flags);\n\tarch_spin_lock(&cpu_buffer->lock);\n\n again:\n\t/*\n\t * This should normally only loop twice. But because the\n\t * start of the reader inserts an empty page, it causes\n\t * a case where we will loop three times. There should be no\n\t * reason to loop four times (that I know of).\n\t */\n\tif (RB_WARN_ON(cpu_buffer, ++nr_loops > 3)) {\n\t\treader = NULL;\n\t\tgoto out;\n\t}\n\n\treader = cpu_buffer->reader_page;\n\n\t/* If there's more to read, return this page */\n\tif (cpu_buffer->reader_page->read < rb_page_size(reader))\n\t\tgoto out;\n\n\t/* Never should we have an index greater than the size */\n\tif (RB_WARN_ON(cpu_buffer,\n\t\t       cpu_buffer->reader_page->read > rb_page_size(reader)))\n\t\tgoto out;\n\n\t/* check if we caught up to the tail */\n\treader = NULL;\n\tif (cpu_buffer->commit_page == cpu_buffer->reader_page)\n\t\tgoto out;\n\n\t/* Don't bother swapping if the ring buffer is empty */\n\tif (rb_num_of_entries(cpu_buffer) == 0)\n\t\tgoto out;\n\n\t/*\n\t * Reset the reader page to size zero.\n\t */\n\tlocal_set(&cpu_buffer->reader_page->write, 0);\n\tlocal_set(&cpu_buffer->reader_page->entries, 0);\n\tlocal_set(&cpu_buffer->reader_page->page->commit, 0);\n\tcpu_buffer->reader_page->real_end = 0;\n\n spin:\n\t/*\n\t * Splice the empty reader page into the list around the head.\n\t */\n\treader = rb_set_head_page(cpu_buffer);\n\tif (!reader)\n\t\tgoto out;\n\tcpu_buffer->reader_page->list.next = rb_list_head(reader->list.next);\n\tcpu_buffer->reader_page->list.prev = reader->list.prev;\n\n\t/*\n\t * cpu_buffer->pages just needs to point to the buffer, it\n\t *  has no specific buffer page to point to. Lets move it out\n\t *  of our way so we don't accidentally swap it.\n\t */\n\tcpu_buffer->pages = reader->list.prev;\n\n\t/* The reader page will be pointing to the new head */\n\trb_set_list_to_head(cpu_buffer, &cpu_buffer->reader_page->list);\n\n\t/*\n\t * We want to make sure we read the overruns after we set up our\n\t * pointers to the next object. The writer side does a\n\t * cmpxchg to cross pages which acts as the mb on the writer\n\t * side. Note, the reader will constantly fail the swap\n\t * while the writer is updating the pointers, so this\n\t * guarantees that the overwrite recorded here is the one we\n\t * want to compare with the last_overrun.\n\t */\n\tsmp_mb();\n\toverwrite = local_read(&(cpu_buffer->overrun));\n\n\t/*\n\t * Here's the tricky part.\n\t *\n\t * We need to move the pointer past the header page.\n\t * But we can only do that if a writer is not currently\n\t * moving it. The page before the header page has the\n\t * flag bit '1' set if it is pointing to the page we want.\n\t * but if the writer is in the process of moving it\n\t * than it will be '2' or already moved '0'.\n\t */\n\n\tret = rb_head_page_replace(reader, cpu_buffer->reader_page);\n\n\t/*\n\t * If we did not convert it, then we must try again.\n\t */\n\tif (!ret)\n\t\tgoto spin;\n\n\t/*\n\t * Yeah! We succeeded in replacing the page.\n\t *\n\t * Now make the new head point back to the reader page.\n\t */\n\trb_list_head(reader->list.next)->prev = &cpu_buffer->reader_page->list;\n\trb_inc_page(cpu_buffer, &cpu_buffer->head_page);\n\n\t/* Finally update the reader page to the new head */\n\tcpu_buffer->reader_page = reader;\n\tcpu_buffer->reader_page->read = 0;\n\n\tif (overwrite != cpu_buffer->last_overrun) {\n\t\tcpu_buffer->lost_events = overwrite - cpu_buffer->last_overrun;\n\t\tcpu_buffer->last_overrun = overwrite;\n\t}\n\n\tgoto again;\n\n out:\n\t/* Update the read_stamp on the first event */\n\tif (reader && reader->read == 0)\n\t\tcpu_buffer->read_stamp = reader->page->time_stamp;\n\n\tarch_spin_unlock(&cpu_buffer->lock);\n\tlocal_irq_restore(flags);\n\n\treturn reader;\n}\n\nstatic void rb_advance_reader(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct ring_buffer_event *event;\n\tstruct buffer_page *reader;\n\tunsigned length;\n\n\treader = rb_get_reader_page(cpu_buffer);\n\n\t/* This function should not be called when buffer is empty */\n\tif (RB_WARN_ON(cpu_buffer, !reader))\n\t\treturn;\n\n\tevent = rb_reader_event(cpu_buffer);\n\n\tif (event->type_len <= RINGBUF_TYPE_DATA_TYPE_LEN_MAX)\n\t\tcpu_buffer->read++;\n\n\trb_update_read_stamp(cpu_buffer, event);\n\n\tlength = rb_event_length(event);\n\tcpu_buffer->reader_page->read += length;\n}\n\nstatic void rb_advance_iter(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tunsigned length;\n\n\tcpu_buffer = iter->cpu_buffer;\n\n\t/*\n\t * Check if we are at the end of the buffer.\n\t */\n\tif (iter->head >= rb_page_size(iter->head_page)) {\n\t\t/* discarded commits can make the page empty */\n\t\tif (iter->head_page == cpu_buffer->commit_page)\n\t\t\treturn;\n\t\trb_inc_iter(iter);\n\t\treturn;\n\t}\n\n\tevent = rb_iter_head_event(iter);\n\n\tlength = rb_event_length(event);\n\n\t/*\n\t * This should not be called to advance the header if we are\n\t * at the tail of the buffer.\n\t */\n\tif (RB_WARN_ON(cpu_buffer,\n\t\t       (iter->head_page == cpu_buffer->commit_page) &&\n\t\t       (iter->head + length > rb_commit_index(cpu_buffer))))\n\t\treturn;\n\n\trb_update_iter_read_stamp(iter, event);\n\n\titer->head += length;\n\n\t/* check for end of page padding */\n\tif ((iter->head >= rb_page_size(iter->head_page)) &&\n\t    (iter->head_page != cpu_buffer->commit_page))\n\t\trb_inc_iter(iter);\n}\n\nstatic int rb_lost_events(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn cpu_buffer->lost_events;\n}\n\nstatic struct ring_buffer_event *\nrb_buffer_peek(struct ring_buffer_per_cpu *cpu_buffer, u64 *ts,\n\t       unsigned long *lost_events)\n{\n\tstruct ring_buffer_event *event;\n\tstruct buffer_page *reader;\n\tint nr_loops = 0;\n\n again:\n\t/*\n\t * We repeat when a time extend is encountered.\n\t * Since the time extend is always attached to a data event,\n\t * we should never loop more than once.\n\t * (We never hit the following condition more than twice).\n\t */\n\tif (RB_WARN_ON(cpu_buffer, ++nr_loops > 2))\n\t\treturn NULL;\n\n\treader = rb_get_reader_page(cpu_buffer);\n\tif (!reader)\n\t\treturn NULL;\n\n\tevent = rb_reader_event(cpu_buffer);\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\tif (rb_null_event(event))\n\t\t\tRB_WARN_ON(cpu_buffer, 1);\n\t\t/*\n\t\t * Because the writer could be discarding every\n\t\t * event it creates (which would probably be bad)\n\t\t * if we were to go back to \"again\" then we may never\n\t\t * catch up, and will trigger the warn on, or lock\n\t\t * the box. Return the padding, and we will release\n\t\t * the current locks, and try again.\n\t\t */\n\t\treturn event;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\t/* Internal data, OK to advance */\n\t\trb_advance_reader(cpu_buffer);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\t/* FIXME: not implemented */\n\t\trb_advance_reader(cpu_buffer);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\tif (ts) {\n\t\t\t*ts = cpu_buffer->read_stamp + event->time_delta;\n\t\t\tring_buffer_normalize_time_stamp(cpu_buffer->buffer,\n\t\t\t\t\t\t\t cpu_buffer->cpu, ts);\n\t\t}\n\t\tif (lost_events)\n\t\t\t*lost_events = rb_lost_events(cpu_buffer);\n\t\treturn event;\n\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_peek);\n\nstatic struct ring_buffer_event *\nrb_iter_peek(struct ring_buffer_iter *iter, u64 *ts)\n{\n\tstruct ring_buffer *buffer;\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tint nr_loops = 0;\n\n\tcpu_buffer = iter->cpu_buffer;\n\tbuffer = cpu_buffer->buffer;\n\n\t/*\n\t * Check if someone performed a consuming read to\n\t * the buffer. A consuming read invalidates the iterator\n\t * and we need to reset the iterator in this case.\n\t */\n\tif (unlikely(iter->cache_read != cpu_buffer->read ||\n\t\t     iter->cache_reader_page != cpu_buffer->reader_page))\n\t\trb_iter_reset(iter);\n\n again:\n\tif (ring_buffer_iter_empty(iter))\n\t\treturn NULL;\n\n\t/*\n\t * We repeat when a time extend is encountered or we hit\n\t * the end of the page. Since the time extend is always attached\n\t * to a data event, we should never loop more than three times.\n\t * Once for going to next page, once on time extend, and\n\t * finally once to get the event.\n\t * (We never hit the following condition more than thrice).\n\t */\n\tif (RB_WARN_ON(cpu_buffer, ++nr_loops > 3))\n\t\treturn NULL;\n\n\tif (rb_per_cpu_empty(cpu_buffer))\n\t\treturn NULL;\n\n\tif (iter->head >= rb_page_size(iter->head_page)) {\n\t\trb_inc_iter(iter);\n\t\tgoto again;\n\t}\n\n\tevent = rb_iter_head_event(iter);\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\tif (rb_null_event(event)) {\n\t\t\trb_inc_iter(iter);\n\t\t\tgoto again;\n\t\t}\n\t\trb_advance_iter(iter);\n\t\treturn event;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\t/* Internal data, OK to advance */\n\t\trb_advance_iter(iter);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\t/* FIXME: not implemented */\n\t\trb_advance_iter(iter);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\tif (ts) {\n\t\t\t*ts = iter->read_stamp + event->time_delta;\n\t\t\tring_buffer_normalize_time_stamp(buffer,\n\t\t\t\t\t\t\t cpu_buffer->cpu, ts);\n\t\t}\n\t\treturn event;\n\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_iter_peek);\n\nstatic inline bool rb_reader_lock(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tif (likely(!in_nmi())) {\n\t\traw_spin_lock(&cpu_buffer->reader_lock);\n\t\treturn true;\n\t}\n\n\t/*\n\t * If an NMI die dumps out the content of the ring buffer\n\t * trylock must be used to prevent a deadlock if the NMI\n\t * preempted a task that holds the ring buffer locks. If\n\t * we get the lock then all is fine, if not, then continue\n\t * to do the read, but this can corrupt the ring buffer,\n\t * so it must be permanently disabled from future writes.\n\t * Reading from NMI is a oneshot deal.\n\t */\n\tif (raw_spin_trylock(&cpu_buffer->reader_lock))\n\t\treturn true;\n\n\t/* Continue without locking, but disable the ring buffer */\n\tatomic_inc(&cpu_buffer->record_disabled);\n\treturn false;\n}\n\nstatic inline void\nrb_reader_unlock(struct ring_buffer_per_cpu *cpu_buffer, bool locked)\n{\n\tif (likely(locked))\n\t\traw_spin_unlock(&cpu_buffer->reader_lock);\n\treturn;\n}\n\n/**\n * ring_buffer_peek - peek at the next event to be read\n * @buffer: The ring buffer to read\n * @cpu: The cpu to peak at\n * @ts: The timestamp counter of this event.\n * @lost_events: a variable to store if events were lost (may be NULL)\n *\n * This will return the event that will be read next, but does\n * not consume the data.\n */\nstruct ring_buffer_event *\nring_buffer_peek(struct ring_buffer *buffer, int cpu, u64 *ts,\n\t\t unsigned long *lost_events)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\tstruct ring_buffer_event *event;\n\tunsigned long flags;\n\tbool dolock;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn NULL;\n\n again:\n\tlocal_irq_save(flags);\n\tdolock = rb_reader_lock(cpu_buffer);\n\tevent = rb_buffer_peek(cpu_buffer, ts, lost_events);\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\trb_advance_reader(cpu_buffer);\n\trb_reader_unlock(cpu_buffer, dolock);\n\tlocal_irq_restore(flags);\n\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\tgoto again;\n\n\treturn event;\n}\n\n/**\n * ring_buffer_iter_peek - peek at the next event to be read\n * @iter: The ring buffer iterator\n * @ts: The timestamp counter of this event.\n *\n * This will return the event that will be read next, but does\n * not increment the iterator.\n */\nstruct ring_buffer_event *\nring_buffer_iter_peek(struct ring_buffer_iter *iter, u64 *ts)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tunsigned long flags;\n\n again:\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\tevent = rb_iter_peek(iter, ts);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\tgoto again;\n\n\treturn event;\n}\n\n/**\n * ring_buffer_consume - return an event and consume it\n * @buffer: The ring buffer to get the next event from\n * @cpu: the cpu to read the buffer from\n * @ts: a variable to store the timestamp (may be NULL)\n * @lost_events: a variable to store if events were lost (may be NULL)\n *\n * Returns the next event in the ring buffer, and that event is consumed.\n * Meaning, that sequential reads will keep returning a different event,\n * and eventually empty the ring buffer if the producer is slower.\n */\nstruct ring_buffer_event *\nring_buffer_consume(struct ring_buffer *buffer, int cpu, u64 *ts,\n\t\t    unsigned long *lost_events)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event = NULL;\n\tunsigned long flags;\n\tbool dolock;\n\n again:\n\t/* might be called in atomic */\n\tpreempt_disable();\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\tgoto out;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tlocal_irq_save(flags);\n\tdolock = rb_reader_lock(cpu_buffer);\n\n\tevent = rb_buffer_peek(cpu_buffer, ts, lost_events);\n\tif (event) {\n\t\tcpu_buffer->lost_events = 0;\n\t\trb_advance_reader(cpu_buffer);\n\t}\n\n\trb_reader_unlock(cpu_buffer, dolock);\n\tlocal_irq_restore(flags);\n\n out:\n\tpreempt_enable();\n\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\tgoto again;\n\n\treturn event;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_consume);\n\n/**\n * ring_buffer_read_prepare - Prepare for a non consuming read of the buffer\n * @buffer: The ring buffer to read from\n * @cpu: The cpu buffer to iterate over\n *\n * This performs the initial preparations necessary to iterate\n * through the buffer.  Memory is allocated, buffer recording\n * is disabled, and the iterator pointer is returned to the caller.\n *\n * Disabling buffer recordng prevents the reading from being\n * corrupted. This is not a consuming read, so a producer is not\n * expected.\n *\n * After a sequence of ring_buffer_read_prepare calls, the user is\n * expected to make at least one call to ring_buffer_read_prepare_sync.\n * Afterwards, ring_buffer_read_start is invoked to get things going\n * for real.\n *\n * This overall must be paired with ring_buffer_read_finish.\n */\nstruct ring_buffer_iter *\nring_buffer_read_prepare(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_iter *iter;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn NULL;\n\n\titer = kmalloc(sizeof(*iter), GFP_KERNEL);\n\tif (!iter)\n\t\treturn NULL;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\titer->cpu_buffer = cpu_buffer;\n\n\tatomic_inc(&buffer->resize_disabled);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\n\treturn iter;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_prepare);\n\n/**\n * ring_buffer_read_prepare_sync - Synchronize a set of prepare calls\n *\n * All previously invoked ring_buffer_read_prepare calls to prepare\n * iterators will be synchronized.  Afterwards, read_buffer_read_start\n * calls on those iterators are allowed.\n */\nvoid\nring_buffer_read_prepare_sync(void)\n{\n\tsynchronize_sched();\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_prepare_sync);\n\n/**\n * ring_buffer_read_start - start a non consuming read of the buffer\n * @iter: The iterator returned by ring_buffer_read_prepare\n *\n * This finalizes the startup of an iteration through the buffer.\n * The iterator comes from a call to ring_buffer_read_prepare and\n * an intervening ring_buffer_read_prepare_sync must have been\n * performed.\n *\n * Must be paired with ring_buffer_read_finish.\n */\nvoid\nring_buffer_read_start(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\n\tif (!iter)\n\t\treturn;\n\n\tcpu_buffer = iter->cpu_buffer;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\tarch_spin_lock(&cpu_buffer->lock);\n\trb_iter_reset(iter);\n\tarch_spin_unlock(&cpu_buffer->lock);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_start);\n\n/**\n * ring_buffer_read_finish - finish reading the iterator of the buffer\n * @iter: The iterator retrieved by ring_buffer_start\n *\n * This re-enables the recording to the buffer, and frees the\n * iterator.\n */\nvoid\nring_buffer_read_finish(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\tunsigned long flags;\n\n\t/*\n\t * Ring buffer is disabled from recording, here's a good place\n\t * to check the integrity of the ring buffer.\n\t * Must prevent readers from trying to read, as the check\n\t * clears the HEAD page and readers require it.\n\t */\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\trb_check_pages(cpu_buffer);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\tatomic_dec(&cpu_buffer->record_disabled);\n\tatomic_dec(&cpu_buffer->buffer->resize_disabled);\n\tkfree(iter);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_finish);\n\n/**\n * ring_buffer_read - read the next item in the ring buffer by the iterator\n * @iter: The ring buffer iterator\n * @ts: The time stamp of the event read.\n *\n * This reads the next event in the ring buffer and increments the iterator.\n */\nstruct ring_buffer_event *\nring_buffer_read(struct ring_buffer_iter *iter, u64 *ts)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n again:\n\tevent = rb_iter_peek(iter, ts);\n\tif (!event)\n\t\tgoto out;\n\n\tif (event->type_len == RINGBUF_TYPE_PADDING)\n\t\tgoto again;\n\n\trb_advance_iter(iter);\n out:\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\treturn event;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read);\n\n/**\n * ring_buffer_size - return the size of the ring buffer (in bytes)\n * @buffer: The ring buffer.\n */\nunsigned long ring_buffer_size(struct ring_buffer *buffer, int cpu)\n{\n\t/*\n\t * Earlier, this method returned\n\t *\tBUF_PAGE_SIZE * buffer->nr_pages\n\t * Since the nr_pages field is now removed, we have converted this to\n\t * return the per cpu buffer value.\n\t */\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\treturn BUF_PAGE_SIZE * buffer->buffers[cpu]->nr_pages;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_size);\n\nstatic void\nrb_reset_cpu(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\trb_head_page_deactivate(cpu_buffer);\n\n\tcpu_buffer->head_page\n\t\t= list_entry(cpu_buffer->pages, struct buffer_page, list);\n\tlocal_set(&cpu_buffer->head_page->write, 0);\n\tlocal_set(&cpu_buffer->head_page->entries, 0);\n\tlocal_set(&cpu_buffer->head_page->page->commit, 0);\n\n\tcpu_buffer->head_page->read = 0;\n\n\tcpu_buffer->tail_page = cpu_buffer->head_page;\n\tcpu_buffer->commit_page = cpu_buffer->head_page;\n\n\tINIT_LIST_HEAD(&cpu_buffer->reader_page->list);\n\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\tlocal_set(&cpu_buffer->reader_page->write, 0);\n\tlocal_set(&cpu_buffer->reader_page->entries, 0);\n\tlocal_set(&cpu_buffer->reader_page->page->commit, 0);\n\tcpu_buffer->reader_page->read = 0;\n\n\tlocal_set(&cpu_buffer->entries_bytes, 0);\n\tlocal_set(&cpu_buffer->overrun, 0);\n\tlocal_set(&cpu_buffer->commit_overrun, 0);\n\tlocal_set(&cpu_buffer->dropped_events, 0);\n\tlocal_set(&cpu_buffer->entries, 0);\n\tlocal_set(&cpu_buffer->committing, 0);\n\tlocal_set(&cpu_buffer->commits, 0);\n\tcpu_buffer->read = 0;\n\tcpu_buffer->read_bytes = 0;\n\n\tcpu_buffer->write_stamp = 0;\n\tcpu_buffer->read_stamp = 0;\n\n\tcpu_buffer->lost_events = 0;\n\tcpu_buffer->last_overrun = 0;\n\n\trb_head_page_activate(cpu_buffer);\n}\n\n/**\n * ring_buffer_reset_cpu - reset a ring buffer per CPU buffer\n * @buffer: The ring buffer to reset a per cpu buffer of\n * @cpu: The CPU buffer to be reset\n */\nvoid ring_buffer_reset_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\tunsigned long flags;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\tatomic_inc(&buffer->resize_disabled);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\n\t/* Make sure all commits have finished */\n\tsynchronize_sched();\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\n\tif (RB_WARN_ON(cpu_buffer, local_read(&cpu_buffer->committing)))\n\t\tgoto out;\n\n\tarch_spin_lock(&cpu_buffer->lock);\n\n\trb_reset_cpu(cpu_buffer);\n\n\tarch_spin_unlock(&cpu_buffer->lock);\n\n out:\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\tatomic_dec(&cpu_buffer->record_disabled);\n\tatomic_dec(&buffer->resize_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_reset_cpu);\n\n/**\n * ring_buffer_reset - reset a ring buffer\n * @buffer: The ring buffer to reset all cpu buffers\n */\nvoid ring_buffer_reset(struct ring_buffer *buffer)\n{\n\tint cpu;\n\n\tfor_each_buffer_cpu(buffer, cpu)\n\t\tring_buffer_reset_cpu(buffer, cpu);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_reset);\n\n/**\n * rind_buffer_empty - is the ring buffer empty?\n * @buffer: The ring buffer to test\n */\nbool ring_buffer_empty(struct ring_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\tbool dolock;\n\tint cpu;\n\tint ret;\n\n\t/* yes this is racy, but if you don't like the race, lock the buffer */\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tlocal_irq_save(flags);\n\t\tdolock = rb_reader_lock(cpu_buffer);\n\t\tret = rb_per_cpu_empty(cpu_buffer);\n\t\trb_reader_unlock(cpu_buffer, dolock);\n\t\tlocal_irq_restore(flags);\n\n\t\tif (!ret)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_empty);\n\n/**\n * ring_buffer_empty_cpu - is a cpu buffer of a ring buffer empty?\n * @buffer: The ring buffer\n * @cpu: The CPU buffer to test\n */\nbool ring_buffer_empty_cpu(struct ring_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\tbool dolock;\n\tint ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn true;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tlocal_irq_save(flags);\n\tdolock = rb_reader_lock(cpu_buffer);\n\tret = rb_per_cpu_empty(cpu_buffer);\n\trb_reader_unlock(cpu_buffer, dolock);\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_empty_cpu);\n\n#ifdef CONFIG_RING_BUFFER_ALLOW_SWAP\n/**\n * ring_buffer_swap_cpu - swap a CPU buffer between two ring buffers\n * @buffer_a: One buffer to swap with\n * @buffer_b: The other buffer to swap with\n *\n * This function is useful for tracers that want to take a \"snapshot\"\n * of a CPU buffer and has another back up buffer lying around.\n * it is expected that the tracer handles the cpu buffer not being\n * used at the moment.\n */\nint ring_buffer_swap_cpu(struct ring_buffer *buffer_a,\n\t\t\t struct ring_buffer *buffer_b, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer_a;\n\tstruct ring_buffer_per_cpu *cpu_buffer_b;\n\tint ret = -EINVAL;\n\n\tif (!cpumask_test_cpu(cpu, buffer_a->cpumask) ||\n\t    !cpumask_test_cpu(cpu, buffer_b->cpumask))\n\t\tgoto out;\n\n\tcpu_buffer_a = buffer_a->buffers[cpu];\n\tcpu_buffer_b = buffer_b->buffers[cpu];\n\n\t/* At least make sure the two buffers are somewhat the same */\n\tif (cpu_buffer_a->nr_pages != cpu_buffer_b->nr_pages)\n\t\tgoto out;\n\n\tret = -EAGAIN;\n\n\tif (atomic_read(&buffer_a->record_disabled))\n\t\tgoto out;\n\n\tif (atomic_read(&buffer_b->record_disabled))\n\t\tgoto out;\n\n\tif (atomic_read(&cpu_buffer_a->record_disabled))\n\t\tgoto out;\n\n\tif (atomic_read(&cpu_buffer_b->record_disabled))\n\t\tgoto out;\n\n\t/*\n\t * We can't do a synchronize_sched here because this\n\t * function can be called in atomic context.\n\t * Normally this will be called from the same CPU as cpu.\n\t * If not it's up to the caller to protect this.\n\t */\n\tatomic_inc(&cpu_buffer_a->record_disabled);\n\tatomic_inc(&cpu_buffer_b->record_disabled);\n\n\tret = -EBUSY;\n\tif (local_read(&cpu_buffer_a->committing))\n\t\tgoto out_dec;\n\tif (local_read(&cpu_buffer_b->committing))\n\t\tgoto out_dec;\n\n\tbuffer_a->buffers[cpu] = cpu_buffer_b;\n\tbuffer_b->buffers[cpu] = cpu_buffer_a;\n\n\tcpu_buffer_b->buffer = buffer_a;\n\tcpu_buffer_a->buffer = buffer_b;\n\n\tret = 0;\n\nout_dec:\n\tatomic_dec(&cpu_buffer_a->record_disabled);\n\tatomic_dec(&cpu_buffer_b->record_disabled);\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_swap_cpu);\n#endif /* CONFIG_RING_BUFFER_ALLOW_SWAP */\n\n/**\n * ring_buffer_alloc_read_page - allocate a page to read from buffer\n * @buffer: the buffer to allocate for.\n * @cpu: the cpu buffer to allocate.\n *\n * This function is used in conjunction with ring_buffer_read_page.\n * When reading a full page from the ring buffer, these functions\n * can be used to speed up the process. The calling function should\n * allocate a few pages first with this function. Then when it\n * needs to get pages from the ring buffer, it passes the result\n * of this function into ring_buffer_read_page, which will swap\n * the page that was allocated, with the read page of the buffer.\n *\n * Returns:\n *  The page allocated, or NULL on error.\n */\nvoid *ring_buffer_alloc_read_page(struct ring_buffer *buffer, int cpu)\n{\n\tstruct buffer_data_page *bpage;\n\tstruct page *page;\n\n\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\tif (!page)\n\t\treturn NULL;\n\n\tbpage = page_address(page);\n\n\trb_init_page(bpage);\n\n\treturn bpage;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_alloc_read_page);\n\n/**\n * ring_buffer_free_read_page - free an allocated read page\n * @buffer: the buffer the page was allocate for\n * @data: the page to free\n *\n * Free a page allocated from ring_buffer_alloc_read_page.\n */\nvoid ring_buffer_free_read_page(struct ring_buffer *buffer, void *data)\n{\n\tfree_page((unsigned long)data);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_free_read_page);\n\n/**\n * ring_buffer_read_page - extract a page from the ring buffer\n * @buffer: buffer to extract from\n * @data_page: the page to use allocated from ring_buffer_alloc_read_page\n * @len: amount to extract\n * @cpu: the cpu of the buffer to extract\n * @full: should the extraction only happen when the page is full.\n *\n * This function will pull out a page from the ring buffer and consume it.\n * @data_page must be the address of the variable that was returned\n * from ring_buffer_alloc_read_page. This is because the page might be used\n * to swap with a page in the ring buffer.\n *\n * for example:\n *\trpage = ring_buffer_alloc_read_page(buffer, cpu);\n *\tif (!rpage)\n *\t\treturn error;\n *\tret = ring_buffer_read_page(buffer, &rpage, len, cpu, 0);\n *\tif (ret >= 0)\n *\t\tprocess_page(rpage, ret);\n *\n * When @full is set, the function will not return true unless\n * the writer is off the reader page.\n *\n * Note: it is up to the calling functions to handle sleeps and wakeups.\n *  The ring buffer can be used anywhere in the kernel and can not\n *  blindly call wake_up. The layer that uses the ring buffer must be\n *  responsible for that.\n *\n * Returns:\n *  >=0 if data has been transferred, returns the offset of consumed data.\n *  <0 if no data has been transferred.\n */\nint ring_buffer_read_page(struct ring_buffer *buffer,\n\t\t\t  void **data_page, size_t len, int cpu, int full)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\tstruct ring_buffer_event *event;\n\tstruct buffer_data_page *bpage;\n\tstruct buffer_page *reader;\n\tunsigned long missed_events;\n\tunsigned long flags;\n\tunsigned int commit;\n\tunsigned int read;\n\tu64 save_timestamp;\n\tint ret = -1;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\tgoto out;\n\n\t/*\n\t * If len is not big enough to hold the page header, then\n\t * we can not copy anything.\n\t */\n\tif (len <= BUF_PAGE_HDR_SIZE)\n\t\tgoto out;\n\n\tlen -= BUF_PAGE_HDR_SIZE;\n\n\tif (!data_page)\n\t\tgoto out;\n\n\tbpage = *data_page;\n\tif (!bpage)\n\t\tgoto out;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\n\treader = rb_get_reader_page(cpu_buffer);\n\tif (!reader)\n\t\tgoto out_unlock;\n\n\tevent = rb_reader_event(cpu_buffer);\n\n\tread = reader->read;\n\tcommit = rb_page_commit(reader);\n\n\t/* Check if any events were dropped */\n\tmissed_events = cpu_buffer->lost_events;\n\n\t/*\n\t * If this page has been partially read or\n\t * if len is not big enough to read the rest of the page or\n\t * a writer is still on the page, then\n\t * we must copy the data from the page to the buffer.\n\t * Otherwise, we can simply swap the page with the one passed in.\n\t */\n\tif (read || (len < (commit - read)) ||\n\t    cpu_buffer->reader_page == cpu_buffer->commit_page) {\n\t\tstruct buffer_data_page *rpage = cpu_buffer->reader_page->page;\n\t\tunsigned int rpos = read;\n\t\tunsigned int pos = 0;\n\t\tunsigned int size;\n\n\t\tif (full)\n\t\t\tgoto out_unlock;\n\n\t\tif (len > (commit - read))\n\t\t\tlen = (commit - read);\n\n\t\t/* Always keep the time extend and data together */\n\t\tsize = rb_event_ts_length(event);\n\n\t\tif (len < size)\n\t\t\tgoto out_unlock;\n\n\t\t/* save the current timestamp, since the user will need it */\n\t\tsave_timestamp = cpu_buffer->read_stamp;\n\n\t\t/* Need to copy one event at a time */\n\t\tdo {\n\t\t\t/* We need the size of one event, because\n\t\t\t * rb_advance_reader only advances by one event,\n\t\t\t * whereas rb_event_ts_length may include the size of\n\t\t\t * one or two events.\n\t\t\t * We have already ensured there's enough space if this\n\t\t\t * is a time extend. */\n\t\t\tsize = rb_event_length(event);\n\t\t\tmemcpy(bpage->data + pos, rpage->data + rpos, size);\n\n\t\t\tlen -= size;\n\n\t\t\trb_advance_reader(cpu_buffer);\n\t\t\trpos = reader->read;\n\t\t\tpos += size;\n\n\t\t\tif (rpos >= commit)\n\t\t\t\tbreak;\n\n\t\t\tevent = rb_reader_event(cpu_buffer);\n\t\t\t/* Always keep the time extend and data together */\n\t\t\tsize = rb_event_ts_length(event);\n\t\t} while (len >= size);\n\n\t\t/* update bpage */\n\t\tlocal_set(&bpage->commit, pos);\n\t\tbpage->time_stamp = save_timestamp;\n\n\t\t/* we copied everything to the beginning */\n\t\tread = 0;\n\t} else {\n\t\t/* update the entry counter */\n\t\tcpu_buffer->read += rb_page_entries(reader);\n\t\tcpu_buffer->read_bytes += BUF_PAGE_SIZE;\n\n\t\t/* swap the pages */\n\t\trb_init_page(bpage);\n\t\tbpage = reader->page;\n\t\treader->page = *data_page;\n\t\tlocal_set(&reader->write, 0);\n\t\tlocal_set(&reader->entries, 0);\n\t\treader->read = 0;\n\t\t*data_page = bpage;\n\n\t\t/*\n\t\t * Use the real_end for the data size,\n\t\t * This gives us a chance to store the lost events\n\t\t * on the page.\n\t\t */\n\t\tif (reader->real_end)\n\t\t\tlocal_set(&bpage->commit, reader->real_end);\n\t}\n\tret = read;\n\n\tcpu_buffer->lost_events = 0;\n\n\tcommit = local_read(&bpage->commit);\n\t/*\n\t * Set a flag in the commit field if we lost events\n\t */\n\tif (missed_events) {\n\t\t/* If there is room at the end of the page to save the\n\t\t * missed events, then record it there.\n\t\t */\n\t\tif (BUF_PAGE_SIZE - commit >= sizeof(missed_events)) {\n\t\t\tmemcpy(&bpage->data[commit], &missed_events,\n\t\t\t       sizeof(missed_events));\n\t\t\tlocal_add(RB_MISSED_STORED, &bpage->commit);\n\t\t\tcommit += sizeof(missed_events);\n\t\t}\n\t\tlocal_add(RB_MISSED_EVENTS, &bpage->commit);\n\t}\n\n\t/*\n\t * This page may be off to user land. Zero it out here.\n\t */\n\tif (commit < BUF_PAGE_SIZE)\n\t\tmemset(&bpage->data[commit], 0, BUF_PAGE_SIZE - commit);\n\n out_unlock:\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n out:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_page);\n\n#ifdef CONFIG_HOTPLUG_CPU\nstatic int rb_cpu_notify(struct notifier_block *self,\n\t\t\t unsigned long action, void *hcpu)\n{\n\tstruct ring_buffer *buffer =\n\t\tcontainer_of(self, struct ring_buffer, cpu_notify);\n\tlong cpu = (long)hcpu;\n\tlong nr_pages_same;\n\tint cpu_i;\n\tunsigned long nr_pages;\n\n\tswitch (action) {\n\tcase CPU_UP_PREPARE:\n\tcase CPU_UP_PREPARE_FROZEN:\n\t\tif (cpumask_test_cpu(cpu, buffer->cpumask))\n\t\t\treturn NOTIFY_OK;\n\n\t\tnr_pages = 0;\n\t\tnr_pages_same = 1;\n\t\t/* check if all cpu sizes are same */\n\t\tfor_each_buffer_cpu(buffer, cpu_i) {\n\t\t\t/* fill in the size from first enabled cpu */\n\t\t\tif (nr_pages == 0)\n\t\t\t\tnr_pages = buffer->buffers[cpu_i]->nr_pages;\n\t\t\tif (nr_pages != buffer->buffers[cpu_i]->nr_pages) {\n\t\t\t\tnr_pages_same = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t/* allocate minimum pages, user can later expand it */\n\t\tif (!nr_pages_same)\n\t\t\tnr_pages = 2;\n\t\tbuffer->buffers[cpu] =\n\t\t\trb_allocate_cpu_buffer(buffer, nr_pages, cpu);\n\t\tif (!buffer->buffers[cpu]) {\n\t\t\tWARN(1, \"failed to allocate ring buffer on CPU %ld\\n\",\n\t\t\t     cpu);\n\t\t\treturn NOTIFY_OK;\n\t\t}\n\t\tsmp_wmb();\n\t\tcpumask_set_cpu(cpu, buffer->cpumask);\n\t\tbreak;\n\tcase CPU_DOWN_PREPARE:\n\tcase CPU_DOWN_PREPARE_FROZEN:\n\t\t/*\n\t\t * Do nothing.\n\t\t *  If we were to free the buffer, then the user would\n\t\t *  lose any trace that was in the buffer.\n\t\t */\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n#endif\n\n#ifdef CONFIG_RING_BUFFER_STARTUP_TEST\n/*\n * This is a basic integrity check of the ring buffer.\n * Late in the boot cycle this test will run when configured in.\n * It will kick off a thread per CPU that will go into a loop\n * writing to the per cpu ring buffer various sizes of data.\n * Some of the data will be large items, some small.\n *\n * Another thread is created that goes into a spin, sending out\n * IPIs to the other CPUs to also write into the ring buffer.\n * this is to test the nesting ability of the buffer.\n *\n * Basic stats are recorded and reported. If something in the\n * ring buffer should happen that's not expected, a big warning\n * is displayed and all ring buffers are disabled.\n */\nstatic struct task_struct *rb_threads[NR_CPUS] __initdata;\n\nstruct rb_test_data {\n\tstruct ring_buffer\t*buffer;\n\tunsigned long\t\tevents;\n\tunsigned long\t\tbytes_written;\n\tunsigned long\t\tbytes_alloc;\n\tunsigned long\t\tbytes_dropped;\n\tunsigned long\t\tevents_nested;\n\tunsigned long\t\tbytes_written_nested;\n\tunsigned long\t\tbytes_alloc_nested;\n\tunsigned long\t\tbytes_dropped_nested;\n\tint\t\t\tmin_size_nested;\n\tint\t\t\tmax_size_nested;\n\tint\t\t\tmax_size;\n\tint\t\t\tmin_size;\n\tint\t\t\tcpu;\n\tint\t\t\tcnt;\n};\n\nstatic struct rb_test_data rb_data[NR_CPUS] __initdata;\n\n/* 1 meg per cpu */\n#define RB_TEST_BUFFER_SIZE\t1048576\n\nstatic char rb_string[] __initdata =\n\t\"abcdefghijklmnopqrstuvwxyz1234567890!@#$%^&*()?+\\\\\"\n\t\"?+|:';\\\",.<>/?abcdefghijklmnopqrstuvwxyz1234567890\"\n\t\"!@#$%^&*()?+\\\\?+|:';\\\",.<>/?abcdefghijklmnopqrstuv\";\n\nstatic bool rb_test_started __initdata;\n\nstruct rb_item {\n\tint size;\n\tchar str[];\n};\n\nstatic __init int rb_write_something(struct rb_test_data *data, bool nested)\n{\n\tstruct ring_buffer_event *event;\n\tstruct rb_item *item;\n\tbool started;\n\tint event_len;\n\tint size;\n\tint len;\n\tint cnt;\n\n\t/* Have nested writes different that what is written */\n\tcnt = data->cnt + (nested ? 27 : 0);\n\n\t/* Multiply cnt by ~e, to make some unique increment */\n\tsize = (data->cnt * 68 / 25) % (sizeof(rb_string) - 1);\n\n\tlen = size + sizeof(struct rb_item);\n\n\tstarted = rb_test_started;\n\t/* read rb_test_started before checking buffer enabled */\n\tsmp_rmb();\n\n\tevent = ring_buffer_lock_reserve(data->buffer, len);\n\tif (!event) {\n\t\t/* Ignore dropped events before test starts. */\n\t\tif (started) {\n\t\t\tif (nested)\n\t\t\t\tdata->bytes_dropped += len;\n\t\t\telse\n\t\t\t\tdata->bytes_dropped_nested += len;\n\t\t}\n\t\treturn len;\n\t}\n\n\tevent_len = ring_buffer_event_length(event);\n\n\tif (RB_WARN_ON(data->buffer, event_len < len))\n\t\tgoto out;\n\n\titem = ring_buffer_event_data(event);\n\titem->size = size;\n\tmemcpy(item->str, rb_string, size);\n\n\tif (nested) {\n\t\tdata->bytes_alloc_nested += event_len;\n\t\tdata->bytes_written_nested += len;\n\t\tdata->events_nested++;\n\t\tif (!data->min_size_nested || len < data->min_size_nested)\n\t\t\tdata->min_size_nested = len;\n\t\tif (len > data->max_size_nested)\n\t\t\tdata->max_size_nested = len;\n\t} else {\n\t\tdata->bytes_alloc += event_len;\n\t\tdata->bytes_written += len;\n\t\tdata->events++;\n\t\tif (!data->min_size || len < data->min_size)\n\t\t\tdata->max_size = len;\n\t\tif (len > data->max_size)\n\t\t\tdata->max_size = len;\n\t}\n\n out:\n\tring_buffer_unlock_commit(data->buffer, event);\n\n\treturn 0;\n}\n\nstatic __init int rb_test(void *arg)\n{\n\tstruct rb_test_data *data = arg;\n\n\twhile (!kthread_should_stop()) {\n\t\trb_write_something(data, false);\n\t\tdata->cnt++;\n\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t/* Now sleep between a min of 100-300us and a max of 1ms */\n\t\tusleep_range(((data->cnt % 3) + 1) * 100, 1000);\n\t}\n\n\treturn 0;\n}\n\nstatic __init void rb_ipi(void *ignore)\n{\n\tstruct rb_test_data *data;\n\tint cpu = smp_processor_id();\n\n\tdata = &rb_data[cpu];\n\trb_write_something(data, true);\n}\n\nstatic __init int rb_hammer_test(void *arg)\n{\n\twhile (!kthread_should_stop()) {\n\n\t\t/* Send an IPI to all cpus to write data! */\n\t\tsmp_call_function(rb_ipi, NULL, 1);\n\t\t/* No sleep, but for non preempt, let others run */\n\t\tschedule();\n\t}\n\n\treturn 0;\n}\n\nstatic __init int test_ringbuffer(void)\n{\n\tstruct task_struct *rb_hammer;\n\tstruct ring_buffer *buffer;\n\tint cpu;\n\tint ret = 0;\n\n\tpr_info(\"Running ring buffer tests...\\n\");\n\n\tbuffer = ring_buffer_alloc(RB_TEST_BUFFER_SIZE, RB_FL_OVERWRITE);\n\tif (WARN_ON(!buffer))\n\t\treturn 0;\n\n\t/* Disable buffer so that threads can't write to it yet */\n\tring_buffer_record_off(buffer);\n\n\tfor_each_online_cpu(cpu) {\n\t\trb_data[cpu].buffer = buffer;\n\t\trb_data[cpu].cpu = cpu;\n\t\trb_data[cpu].cnt = cpu;\n\t\trb_threads[cpu] = kthread_create(rb_test, &rb_data[cpu],\n\t\t\t\t\t\t \"rbtester/%d\", cpu);\n\t\tif (WARN_ON(!rb_threads[cpu])) {\n\t\t\tpr_cont(\"FAILED\\n\");\n\t\t\tret = -1;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tkthread_bind(rb_threads[cpu], cpu);\n \t\twake_up_process(rb_threads[cpu]);\n\t}\n\n\t/* Now create the rb hammer! */\n\trb_hammer = kthread_run(rb_hammer_test, NULL, \"rbhammer\");\n\tif (WARN_ON(!rb_hammer)) {\n\t\tpr_cont(\"FAILED\\n\");\n\t\tret = -1;\n\t\tgoto out_free;\n\t}\n\n\tring_buffer_record_on(buffer);\n\t/*\n\t * Show buffer is enabled before setting rb_test_started.\n\t * Yes there's a small race window where events could be\n\t * dropped and the thread wont catch it. But when a ring\n\t * buffer gets enabled, there will always be some kind of\n\t * delay before other CPUs see it. Thus, we don't care about\n\t * those dropped events. We care about events dropped after\n\t * the threads see that the buffer is active.\n\t */\n\tsmp_wmb();\n\trb_test_started = true;\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\t/* Just run for 10 seconds */;\n\tschedule_timeout(10 * HZ);\n\n\tkthread_stop(rb_hammer);\n\n out_free:\n\tfor_each_online_cpu(cpu) {\n\t\tif (!rb_threads[cpu])\n\t\t\tbreak;\n\t\tkthread_stop(rb_threads[cpu]);\n\t}\n\tif (ret) {\n\t\tring_buffer_free(buffer);\n\t\treturn ret;\n\t}\n\n\t/* Report! */\n\tpr_info(\"finished\\n\");\n\tfor_each_online_cpu(cpu) {\n\t\tstruct ring_buffer_event *event;\n\t\tstruct rb_test_data *data = &rb_data[cpu];\n\t\tstruct rb_item *item;\n\t\tunsigned long total_events;\n\t\tunsigned long total_dropped;\n\t\tunsigned long total_written;\n\t\tunsigned long total_alloc;\n\t\tunsigned long total_read = 0;\n\t\tunsigned long total_size = 0;\n\t\tunsigned long total_len = 0;\n\t\tunsigned long total_lost = 0;\n\t\tunsigned long lost;\n\t\tint big_event_size;\n\t\tint small_event_size;\n\n\t\tret = -1;\n\n\t\ttotal_events = data->events + data->events_nested;\n\t\ttotal_written = data->bytes_written + data->bytes_written_nested;\n\t\ttotal_alloc = data->bytes_alloc + data->bytes_alloc_nested;\n\t\ttotal_dropped = data->bytes_dropped + data->bytes_dropped_nested;\n\n\t\tbig_event_size = data->max_size + data->max_size_nested;\n\t\tsmall_event_size = data->min_size + data->min_size_nested;\n\n\t\tpr_info(\"CPU %d:\\n\", cpu);\n\t\tpr_info(\"              events:    %ld\\n\", total_events);\n\t\tpr_info(\"       dropped bytes:    %ld\\n\", total_dropped);\n\t\tpr_info(\"       alloced bytes:    %ld\\n\", total_alloc);\n\t\tpr_info(\"       written bytes:    %ld\\n\", total_written);\n\t\tpr_info(\"       biggest event:    %d\\n\", big_event_size);\n\t\tpr_info(\"      smallest event:    %d\\n\", small_event_size);\n\n\t\tif (RB_WARN_ON(buffer, total_dropped))\n\t\t\tbreak;\n\n\t\tret = 0;\n\n\t\twhile ((event = ring_buffer_consume(buffer, cpu, NULL, &lost))) {\n\t\t\ttotal_lost += lost;\n\t\t\titem = ring_buffer_event_data(event);\n\t\t\ttotal_len += ring_buffer_event_length(event);\n\t\t\ttotal_size += item->size + sizeof(struct rb_item);\n\t\t\tif (memcmp(&item->str[0], rb_string, item->size) != 0) {\n\t\t\t\tpr_info(\"FAILED!\\n\");\n\t\t\t\tpr_info(\"buffer had: %.*s\\n\", item->size, item->str);\n\t\t\t\tpr_info(\"expected:   %.*s\\n\", item->size, rb_string);\n\t\t\t\tRB_WARN_ON(buffer, 1);\n\t\t\t\tret = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttotal_read++;\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = -1;\n\n\t\tpr_info(\"         read events:   %ld\\n\", total_read);\n\t\tpr_info(\"         lost events:   %ld\\n\", total_lost);\n\t\tpr_info(\"        total events:   %ld\\n\", total_lost + total_read);\n\t\tpr_info(\"  recorded len bytes:   %ld\\n\", total_len);\n\t\tpr_info(\" recorded size bytes:   %ld\\n\", total_size);\n\t\tif (total_lost)\n\t\t\tpr_info(\" With dropped events, record len and size may not match\\n\"\n\t\t\t\t\" alloced and written from above\\n\");\n\t\tif (!total_lost) {\n\t\t\tif (RB_WARN_ON(buffer, total_len != total_alloc ||\n\t\t\t\t       total_size != total_written))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (RB_WARN_ON(buffer, total_lost + total_read != total_events))\n\t\t\tbreak;\n\n\t\tret = 0;\n\t}\n\tif (!ret)\n\t\tpr_info(\"Ring buffer PASSED!\\n\");\n\n\tring_buffer_free(buffer);\n\treturn 0;\n}\n\nlate_initcall(test_ringbuffer);\n#endif /* CONFIG_RING_BUFFER_STARTUP_TEST */\n"], "filenames": ["kernel/trace/ring_buffer.c"], "buggy_code_start_loc": [1660], "buggy_code_end_loc": [1668], "fixing_code_start_loc": [1660], "fixing_code_end_loc": [1667], "type": "CWE-190", "message": "The ring_buffer_resize function in kernel/trace/ring_buffer.c in the profiling subsystem in the Linux kernel before 4.6.1 mishandles certain integer calculations, which allows local users to gain privileges by writing to the /sys/kernel/debug/tracing/buffer_size_kb file.", "other": {"cve": {"id": "CVE-2016-9754", "sourceIdentifier": "security@android.com", "published": "2017-01-05T11:59:00.193", "lastModified": "2023-01-17T21:05:25.067", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The ring_buffer_resize function in kernel/trace/ring_buffer.c in the profiling subsystem in the Linux kernel before 4.6.1 mishandles certain integer calculations, which allows local users to gain privileges by writing to the /sys/kernel/debug/tracing/buffer_size_kb file."}, {"lang": "es", "value": "La funci\u00f3n ring_buffer_resize en kernel/trace/ring_buffer.c en el subsistema de creaci\u00f3n de perfiles del kernel de Linux en versiones anteriores a 4.6.1 no maneja adecuadamente ciertos c\u00e1lculos de entero, lo que permite a usuarios locales obtener privilegios escribiendo en el archivo /sys/kernel/debug/tracing/buffer_size_kb."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 7.2}, "baseSeverity": "HIGH", "exploitabilityScore": 3.9, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-190"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.5", "versionEndExcluding": "3.10.102", "matchCriteriaId": "7B7694A6-D37E-46B2-91D2-C02D1E1D92FB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.11", "versionEndExcluding": "3.12.61", "matchCriteriaId": "1DE29121-4AF5-432B-834F-340E88D3467D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.13", "versionEndExcluding": "3.14.71", "matchCriteriaId": "67E46C10-5C97-4519-B660-433ED4B105F5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.15", "versionEndExcluding": "3.16.37", "matchCriteriaId": "7DEF7E2D-A1AA-4733-A573-11EE52A2B419"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.35", "matchCriteriaId": "0B4D585F-6BD3-46AB-8FD1-CC66EC1518AD"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.26", "matchCriteriaId": "D6ED15B4-3B4F-4D72-A3DD-F61E2E8B2400"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.12", "matchCriteriaId": "74710E0F-33B0-4DE1-9EAA-BF090FD39706"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.5.6", "matchCriteriaId": "C60F583A-70FF-49D0-B1E7-27AF98A60A2F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:4.6:*:*:*:*:*:*:*", "matchCriteriaId": "273E9BE2-7913-4FA4-9E2E-41AF562FA7B1"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=59643d1535eb220668692a5359de22545af579f6", "source": "security@android.com", "tags": ["Exploit", "Patch", "Vendor Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.6.1", "source": "security@android.com", "tags": ["Release Notes"]}, {"url": "http://www.securityfocus.com/bid/95278", "source": "security@android.com", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://github.com/torvalds/linux/commit/59643d1535eb220668692a5359de22545af579f6", "source": "security@android.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://source.android.com/security/bulletin/2017-01-01.html", "source": "security@android.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/59643d1535eb220668692a5359de22545af579f6"}}