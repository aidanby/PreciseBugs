{"buggy_code": ["/*\n * PMU support\n *\n * Copyright (C) 2012 ARM Limited\n * Author: Will Deacon <will.deacon@arm.com>\n *\n * This code is based heavily on the ARMv7 perf event code.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n */\n#define pr_fmt(fmt) \"hw perfevents: \" fmt\n\n#include <linux/bitmap.h>\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n#include <linux/perf_event.h>\n#include <linux/platform_device.h>\n#include <linux/spinlock.h>\n#include <linux/uaccess.h>\n\n#include <asm/cputype.h>\n#include <asm/irq.h>\n#include <asm/irq_regs.h>\n#include <asm/pmu.h>\n#include <asm/stacktrace.h>\n\n/*\n * ARMv8 supports a maximum of 32 events.\n * The cycle counter is included in this total.\n */\n#define ARMPMU_MAX_HWEVENTS\t\t32\n\nstatic DEFINE_PER_CPU(struct perf_event * [ARMPMU_MAX_HWEVENTS], hw_events);\nstatic DEFINE_PER_CPU(unsigned long [BITS_TO_LONGS(ARMPMU_MAX_HWEVENTS)], used_mask);\nstatic DEFINE_PER_CPU(struct pmu_hw_events, cpu_hw_events);\n\n#define to_arm_pmu(p) (container_of(p, struct arm_pmu, pmu))\n\n/* Set at runtime when we know what CPU type we are. */\nstatic struct arm_pmu *cpu_pmu;\n\nint\narmpmu_get_max_events(void)\n{\n\tint max_events = 0;\n\n\tif (cpu_pmu != NULL)\n\t\tmax_events = cpu_pmu->num_events;\n\n\treturn max_events;\n}\nEXPORT_SYMBOL_GPL(armpmu_get_max_events);\n\nint perf_num_counters(void)\n{\n\treturn armpmu_get_max_events();\n}\nEXPORT_SYMBOL_GPL(perf_num_counters);\n\n#define HW_OP_UNSUPPORTED\t\t0xFFFF\n\n#define C(_x) \\\n\tPERF_COUNT_HW_CACHE_##_x\n\n#define CACHE_OP_UNSUPPORTED\t\t0xFFFF\n\nstatic int\narmpmu_map_cache_event(const unsigned (*cache_map)\n\t\t\t\t      [PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t      [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t      [PERF_COUNT_HW_CACHE_RESULT_MAX],\n\t\t       u64 config)\n{\n\tunsigned int cache_type, cache_op, cache_result, ret;\n\n\tcache_type = (config >>  0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn -EINVAL;\n\n\tcache_op = (config >>  8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn -EINVAL;\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tret = (int)(*cache_map)[cache_type][cache_op][cache_result];\n\n\tif (ret == CACHE_OP_UNSUPPORTED)\n\t\treturn -ENOENT;\n\n\treturn ret;\n}\n\nstatic int\narmpmu_map_event(const unsigned (*event_map)[PERF_COUNT_HW_MAX], u64 config)\n{\n\tint mapping;\n\n\tif (config >= PERF_COUNT_HW_MAX)\n\t\treturn -EINVAL;\n\n\tmapping = (*event_map)[config];\n\treturn mapping == HW_OP_UNSUPPORTED ? -ENOENT : mapping;\n}\n\nstatic int\narmpmu_map_raw_event(u32 raw_event_mask, u64 config)\n{\n\treturn (int)(config & raw_event_mask);\n}\n\nstatic int map_cpu_event(struct perf_event *event,\n\t\t\t const unsigned (*event_map)[PERF_COUNT_HW_MAX],\n\t\t\t const unsigned (*cache_map)\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX],\n\t\t\t u32 raw_event_mask)\n{\n\tu64 config = event->attr.config;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\treturn armpmu_map_event(event_map, config);\n\tcase PERF_TYPE_HW_CACHE:\n\t\treturn armpmu_map_cache_event(cache_map, config);\n\tcase PERF_TYPE_RAW:\n\t\treturn armpmu_map_raw_event(raw_event_mask, config);\n\t}\n\n\treturn -ENOENT;\n}\n\nint\narmpmu_event_set_period(struct perf_event *event,\n\t\t\tstruct hw_perf_event *hwc,\n\t\t\tint idx)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint ret = 0;\n\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\t/*\n\t * Limit the maximum period to prevent the counter value\n\t * from overtaking the one we are about to program. In\n\t * effect we are reducing max_period to account for\n\t * interrupt latency (and we are being very conservative).\n\t */\n\tif (left > (armpmu->max_period >> 1))\n\t\tleft = armpmu->max_period >> 1;\n\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\tarmpmu->write_counter(idx, (u64)(-left) & 0xffffffff);\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\nu64\narmpmu_event_update(struct perf_event *event,\n\t\t    struct hw_perf_event *hwc,\n\t\t    int idx)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tu64 delta, prev_raw_count, new_raw_count;\n\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tnew_raw_count = armpmu->read_counter(idx);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t     new_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\tdelta = (new_raw_count - prev_raw_count) & armpmu->max_period;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\nstatic void\narmpmu_read(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/* Don't read disabled counters! */\n\tif (hwc->idx < 0)\n\t\treturn;\n\n\tarmpmu_event_update(event, hwc, hwc->idx);\n}\n\nstatic void\narmpmu_stop(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/*\n\t * ARM pmu always has to update the counter, so ignore\n\t * PERF_EF_UPDATE, see comments in armpmu_start().\n\t */\n\tif (!(hwc->state & PERF_HES_STOPPED)) {\n\t\tarmpmu->disable(hwc, hwc->idx);\n\t\tbarrier(); /* why? */\n\t\tarmpmu_event_update(event, hwc, hwc->idx);\n\t\thwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\t}\n}\n\nstatic void\narmpmu_start(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/*\n\t * ARM pmu always has to reprogram the period, so ignore\n\t * PERF_EF_RELOAD, see the comment below.\n\t */\n\tif (flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\n\thwc->state = 0;\n\t/*\n\t * Set the period again. Some counters can't be stopped, so when we\n\t * were stopped we simply disabled the IRQ source and the counter\n\t * may have been left counting. If we don't do this step then we may\n\t * get an interrupt too soon or *way* too late if the overflow has\n\t * happened since disabling.\n\t */\n\tarmpmu_event_set_period(event, hwc, hwc->idx);\n\tarmpmu->enable(hwc, hwc->idx);\n}\n\nstatic void\narmpmu_del(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tWARN_ON(idx < 0);\n\n\tarmpmu_stop(event, PERF_EF_UPDATE);\n\thw_events->events[idx] = NULL;\n\tclear_bit(idx, hw_events->used_mask);\n\n\tperf_event_update_userpage(event);\n}\n\nstatic int\narmpmu_add(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\tint err = 0;\n\n\tperf_pmu_disable(event->pmu);\n\n\t/* If we don't have a space for the counter then finish early. */\n\tidx = armpmu->get_event_idx(hw_events, hwc);\n\tif (idx < 0) {\n\t\terr = idx;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If there is an event in the counter we are going to use then make\n\t * sure it is disabled.\n\t */\n\tevent->hw.idx = idx;\n\tarmpmu->disable(hwc, idx);\n\thw_events->events[idx] = event;\n\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\tif (flags & PERF_EF_START)\n\t\tarmpmu_start(event, PERF_EF_RELOAD);\n\n\t/* Propagate our changes to the userspace mapping. */\n\tperf_event_update_userpage(event);\n\nout:\n\tperf_pmu_enable(event->pmu);\n\treturn err;\n}\n\nstatic int\nvalidate_event(struct pmu_hw_events *hw_events,\n\t       struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event fake_event = event->hw;\n\tstruct pmu *leader_pmu = event->group_leader->pmu;\n\n\tif (is_software_event(event))\n\t\treturn 1;\n\n\tif (event->pmu != leader_pmu || event->state < PERF_EVENT_STATE_OFF)\n\t\treturn 1;\n\n\tif (event->state == PERF_EVENT_STATE_OFF && !event->attr.enable_on_exec)\n\t\treturn 1;\n\n\treturn armpmu->get_event_idx(hw_events, &fake_event) >= 0;\n}\n\nstatic int\nvalidate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct pmu_hw_events fake_pmu;\n\tDECLARE_BITMAP(fake_used_mask, ARMPMU_MAX_HWEVENTS);\n\n\t/*\n\t * Initialise the fake PMU. We only need to populate the\n\t * used_mask for the purposes of validation.\n\t */\n\tmemset(fake_used_mask, 0, sizeof(fake_used_mask));\n\tfake_pmu.used_mask = fake_used_mask;\n\n\tif (!validate_event(&fake_pmu, leader))\n\t\treturn -EINVAL;\n\n\tlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\n\t\tif (!validate_event(&fake_pmu, sibling))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!validate_event(&fake_pmu, event))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void\narmpmu_disable_percpu_irq(void *data)\n{\n\tunsigned int irq = *(unsigned int *)data;\n\tdisable_percpu_irq(irq);\n}\n\nstatic void\narmpmu_release_hardware(struct arm_pmu *armpmu)\n{\n\tint irq;\n\tunsigned int i, irqs;\n\tstruct platform_device *pmu_device = armpmu->plat_device;\n\n\tirqs = min(pmu_device->num_resources, num_possible_cpus());\n\tif (!irqs)\n\t\treturn;\n\n\tirq = platform_get_irq(pmu_device, 0);\n\tif (irq <= 0)\n\t\treturn;\n\n\tif (irq_is_percpu(irq)) {\n\t\ton_each_cpu(armpmu_disable_percpu_irq, &irq, 1);\n\t\tfree_percpu_irq(irq, &cpu_hw_events);\n\t} else {\n\t\tfor (i = 0; i < irqs; ++i) {\n\t\t\tif (!cpumask_test_and_clear_cpu(i, &armpmu->active_irqs))\n\t\t\t\tcontinue;\n\t\t\tirq = platform_get_irq(pmu_device, i);\n\t\t\tif (irq > 0)\n\t\t\t\tfree_irq(irq, armpmu);\n\t\t}\n\t}\n}\n\nstatic void\narmpmu_enable_percpu_irq(void *data)\n{\n\tunsigned int irq = *(unsigned int *)data;\n\tenable_percpu_irq(irq, IRQ_TYPE_NONE);\n}\n\nstatic int\narmpmu_reserve_hardware(struct arm_pmu *armpmu)\n{\n\tint err, irq;\n\tunsigned int i, irqs;\n\tstruct platform_device *pmu_device = armpmu->plat_device;\n\n\tif (!pmu_device) {\n\t\tpr_err(\"no PMU device registered\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tirqs = min(pmu_device->num_resources, num_possible_cpus());\n\tif (!irqs) {\n\t\tpr_err(\"no irqs for PMUs defined\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tirq = platform_get_irq(pmu_device, 0);\n\tif (irq <= 0) {\n\t\tpr_err(\"failed to get valid irq for PMU device\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (irq_is_percpu(irq)) {\n\t\terr = request_percpu_irq(irq, armpmu->handle_irq,\n\t\t\t\t\"arm-pmu\", &cpu_hw_events);\n\n\t\tif (err) {\n\t\t\tpr_err(\"unable to request percpu IRQ%d for ARM PMU counters\\n\",\n\t\t\t\t\tirq);\n\t\t\tarmpmu_release_hardware(armpmu);\n\t\t\treturn err;\n\t\t}\n\n\t\ton_each_cpu(armpmu_enable_percpu_irq, &irq, 1);\n\t} else {\n\t\tfor (i = 0; i < irqs; ++i) {\n\t\t\terr = 0;\n\t\t\tirq = platform_get_irq(pmu_device, i);\n\t\t\tif (irq <= 0)\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * If we have a single PMU interrupt that we can't shift,\n\t\t\t * assume that we're running on a uniprocessor machine and\n\t\t\t * continue. Otherwise, continue without this interrupt.\n\t\t\t */\n\t\t\tif (irq_set_affinity(irq, cpumask_of(i)) && irqs > 1) {\n\t\t\t\tpr_warning(\"unable to set irq affinity (irq=%d, cpu=%u)\\n\",\n\t\t\t\t\t\tirq, i);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\terr = request_irq(irq, armpmu->handle_irq,\n\t\t\t\t\tIRQF_NOBALANCING,\n\t\t\t\t\t\"arm-pmu\", armpmu);\n\t\t\tif (err) {\n\t\t\t\tpr_err(\"unable to request IRQ%d for ARM PMU counters\\n\",\n\t\t\t\t\t\tirq);\n\t\t\t\tarmpmu_release_hardware(armpmu);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\tcpumask_set_cpu(i, &armpmu->active_irqs);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void\nhw_perf_event_destroy(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tatomic_t *active_events\t = &armpmu->active_events;\n\tstruct mutex *pmu_reserve_mutex = &armpmu->reserve_mutex;\n\n\tif (atomic_dec_and_mutex_lock(active_events, pmu_reserve_mutex)) {\n\t\tarmpmu_release_hardware(armpmu);\n\t\tmutex_unlock(pmu_reserve_mutex);\n\t}\n}\n\nstatic int\nevent_requires_mode_exclusion(struct perf_event_attr *attr)\n{\n\treturn attr->exclude_idle || attr->exclude_user ||\n\t       attr->exclude_kernel || attr->exclude_hv;\n}\n\nstatic int\n__hw_perf_event_init(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint mapping, err;\n\n\tmapping = armpmu->map_event(event);\n\n\tif (mapping < 0) {\n\t\tpr_debug(\"event %x:%llx not supported\\n\", event->attr.type,\n\t\t\t event->attr.config);\n\t\treturn mapping;\n\t}\n\n\t/*\n\t * We don't assign an index until we actually place the event onto\n\t * hardware. Use -1 to signify that we haven't decided where to put it\n\t * yet. For SMP systems, each core has it's own PMU so we can't do any\n\t * clever allocation or constraints checking at this point.\n\t */\n\thwc->idx\t\t= -1;\n\thwc->config_base\t= 0;\n\thwc->config\t\t= 0;\n\thwc->event_base\t\t= 0;\n\n\t/*\n\t * Check whether we need to exclude the counter from certain modes.\n\t */\n\tif ((!armpmu->set_event_filter ||\n\t     armpmu->set_event_filter(hwc, &event->attr)) &&\n\t     event_requires_mode_exclusion(&event->attr)) {\n\t\tpr_debug(\"ARM performance counters do not support mode exclusion\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * Store the event encoding into the config_base field.\n\t */\n\thwc->config_base\t    |= (unsigned long)mapping;\n\n\tif (!hwc->sample_period) {\n\t\t/*\n\t\t * For non-sampling runs, limit the sample_period to half\n\t\t * of the counter width. That way, the new counter value\n\t\t * is far less likely to overtake the previous one unless\n\t\t * you have some serious IRQ latency issues.\n\t\t */\n\t\thwc->sample_period  = armpmu->max_period >> 1;\n\t\thwc->last_period    = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t}\n\n\terr = 0;\n\tif (event->group_leader != event) {\n\t\terr = validate_group(event);\n\t\tif (err)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn err;\n}\n\nstatic int armpmu_event_init(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tint err = 0;\n\tatomic_t *active_events = &armpmu->active_events;\n\n\tif (armpmu->map_event(event) == -ENOENT)\n\t\treturn -ENOENT;\n\n\tevent->destroy = hw_perf_event_destroy;\n\n\tif (!atomic_inc_not_zero(active_events)) {\n\t\tmutex_lock(&armpmu->reserve_mutex);\n\t\tif (atomic_read(active_events) == 0)\n\t\t\terr = armpmu_reserve_hardware(armpmu);\n\n\t\tif (!err)\n\t\t\tatomic_inc(active_events);\n\t\tmutex_unlock(&armpmu->reserve_mutex);\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\terr = __hw_perf_event_init(event);\n\tif (err)\n\t\thw_perf_event_destroy(event);\n\n\treturn err;\n}\n\nstatic void armpmu_enable(struct pmu *pmu)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tint enabled = bitmap_weight(hw_events->used_mask, armpmu->num_events);\n\n\tif (enabled)\n\t\tarmpmu->start();\n}\n\nstatic void armpmu_disable(struct pmu *pmu)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(pmu);\n\tarmpmu->stop();\n}\n\nstatic void __init armpmu_init(struct arm_pmu *armpmu)\n{\n\tatomic_set(&armpmu->active_events, 0);\n\tmutex_init(&armpmu->reserve_mutex);\n\n\tarmpmu->pmu = (struct pmu) {\n\t\t.pmu_enable\t= armpmu_enable,\n\t\t.pmu_disable\t= armpmu_disable,\n\t\t.event_init\t= armpmu_event_init,\n\t\t.add\t\t= armpmu_add,\n\t\t.del\t\t= armpmu_del,\n\t\t.start\t\t= armpmu_start,\n\t\t.stop\t\t= armpmu_stop,\n\t\t.read\t\t= armpmu_read,\n\t};\n}\n\nint __init armpmu_register(struct arm_pmu *armpmu, char *name, int type)\n{\n\tarmpmu_init(armpmu);\n\treturn perf_pmu_register(&armpmu->pmu, name, type);\n}\n\n/*\n * ARMv8 PMUv3 Performance Events handling code.\n * Common event types.\n */\nenum armv8_pmuv3_perf_types {\n\t/* Required events. */\n\tARMV8_PMUV3_PERFCTR_PMNC_SW_INCR\t\t\t= 0x00,\n\tARMV8_PMUV3_PERFCTR_L1_DCACHE_REFILL\t\t\t= 0x03,\n\tARMV8_PMUV3_PERFCTR_L1_DCACHE_ACCESS\t\t\t= 0x04,\n\tARMV8_PMUV3_PERFCTR_PC_BRANCH_MIS_PRED\t\t\t= 0x10,\n\tARMV8_PMUV3_PERFCTR_CLOCK_CYCLES\t\t\t= 0x11,\n\tARMV8_PMUV3_PERFCTR_PC_BRANCH_PRED\t\t\t= 0x12,\n\n\t/* At least one of the following is required. */\n\tARMV8_PMUV3_PERFCTR_INSTR_EXECUTED\t\t\t= 0x08,\n\tARMV8_PMUV3_PERFCTR_OP_SPEC\t\t\t\t= 0x1B,\n\n\t/* Common architectural events. */\n\tARMV8_PMUV3_PERFCTR_MEM_READ\t\t\t\t= 0x06,\n\tARMV8_PMUV3_PERFCTR_MEM_WRITE\t\t\t\t= 0x07,\n\tARMV8_PMUV3_PERFCTR_EXC_TAKEN\t\t\t\t= 0x09,\n\tARMV8_PMUV3_PERFCTR_EXC_EXECUTED\t\t\t= 0x0A,\n\tARMV8_PMUV3_PERFCTR_CID_WRITE\t\t\t\t= 0x0B,\n\tARMV8_PMUV3_PERFCTR_PC_WRITE\t\t\t\t= 0x0C,\n\tARMV8_PMUV3_PERFCTR_PC_IMM_BRANCH\t\t\t= 0x0D,\n\tARMV8_PMUV3_PERFCTR_PC_PROC_RETURN\t\t\t= 0x0E,\n\tARMV8_PMUV3_PERFCTR_MEM_UNALIGNED_ACCESS\t\t= 0x0F,\n\tARMV8_PMUV3_PERFCTR_TTBR_WRITE\t\t\t\t= 0x1C,\n\n\t/* Common microarchitectural events. */\n\tARMV8_PMUV3_PERFCTR_L1_ICACHE_REFILL\t\t\t= 0x01,\n\tARMV8_PMUV3_PERFCTR_ITLB_REFILL\t\t\t\t= 0x02,\n\tARMV8_PMUV3_PERFCTR_DTLB_REFILL\t\t\t\t= 0x05,\n\tARMV8_PMUV3_PERFCTR_MEM_ACCESS\t\t\t\t= 0x13,\n\tARMV8_PMUV3_PERFCTR_L1_ICACHE_ACCESS\t\t\t= 0x14,\n\tARMV8_PMUV3_PERFCTR_L1_DCACHE_WB\t\t\t= 0x15,\n\tARMV8_PMUV3_PERFCTR_L2_CACHE_ACCESS\t\t\t= 0x16,\n\tARMV8_PMUV3_PERFCTR_L2_CACHE_REFILL\t\t\t= 0x17,\n\tARMV8_PMUV3_PERFCTR_L2_CACHE_WB\t\t\t\t= 0x18,\n\tARMV8_PMUV3_PERFCTR_BUS_ACCESS\t\t\t\t= 0x19,\n\tARMV8_PMUV3_PERFCTR_MEM_ERROR\t\t\t\t= 0x1A,\n\tARMV8_PMUV3_PERFCTR_BUS_CYCLES\t\t\t\t= 0x1D,\n};\n\n/* PMUv3 HW events mapping. */\nstatic const unsigned armv8_pmuv3_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t\t= ARMV8_PMUV3_PERFCTR_CLOCK_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t\t= ARMV8_PMUV3_PERFCTR_INSTR_EXECUTED,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_ACCESS,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_REFILL,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS]\t= HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t\t= ARMV8_PMUV3_PERFCTR_PC_BRANCH_MIS_PRED,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t\t= HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND]\t= HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_STALLED_CYCLES_BACKEND]\t= HW_OP_UNSUPPORTED,\n};\n\nstatic const unsigned armv8_pmuv3_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_PC_BRANCH_PRED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_PC_BRANCH_PRED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(NODE)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\n/*\n * Perf Events' indices\n */\n#define\tARMV8_IDX_CYCLE_COUNTER\t0\n#define\tARMV8_IDX_COUNTER0\t1\n#define\tARMV8_IDX_COUNTER_LAST\t(ARMV8_IDX_CYCLE_COUNTER + cpu_pmu->num_events - 1)\n\n#define\tARMV8_MAX_COUNTERS\t32\n#define\tARMV8_COUNTER_MASK\t(ARMV8_MAX_COUNTERS - 1)\n\n/*\n * ARMv8 low level PMU access\n */\n\n/*\n * Perf Event to low level counters mapping\n */\n#define\tARMV8_IDX_TO_COUNTER(x)\t\\\n\t(((x) - ARMV8_IDX_COUNTER0) & ARMV8_COUNTER_MASK)\n\n/*\n * Per-CPU PMCR: config reg\n */\n#define ARMV8_PMCR_E\t\t(1 << 0) /* Enable all counters */\n#define ARMV8_PMCR_P\t\t(1 << 1) /* Reset all counters */\n#define ARMV8_PMCR_C\t\t(1 << 2) /* Cycle counter reset */\n#define ARMV8_PMCR_D\t\t(1 << 3) /* CCNT counts every 64th cpu cycle */\n#define ARMV8_PMCR_X\t\t(1 << 4) /* Export to ETM */\n#define ARMV8_PMCR_DP\t\t(1 << 5) /* Disable CCNT if non-invasive debug*/\n#define\tARMV8_PMCR_N_SHIFT\t11\t /* Number of counters supported */\n#define\tARMV8_PMCR_N_MASK\t0x1f\n#define\tARMV8_PMCR_MASK\t\t0x3f\t /* Mask for writable bits */\n\n/*\n * PMOVSR: counters overflow flag status reg\n */\n#define\tARMV8_OVSR_MASK\t\t0xffffffff\t/* Mask for writable bits */\n#define\tARMV8_OVERFLOWED_MASK\tARMV8_OVSR_MASK\n\n/*\n * PMXEVTYPER: Event selection reg\n */\n#define\tARMV8_EVTYPE_MASK\t0xc80003ff\t/* Mask for writable bits */\n#define\tARMV8_EVTYPE_EVENT\t0x3ff\t\t/* Mask for EVENT bits */\n\n/*\n * Event filters for PMUv3\n */\n#define\tARMV8_EXCLUDE_EL1\t(1 << 31)\n#define\tARMV8_EXCLUDE_EL0\t(1 << 30)\n#define\tARMV8_INCLUDE_EL2\t(1 << 27)\n\nstatic inline u32 armv8pmu_pmcr_read(void)\n{\n\tu32 val;\n\tasm volatile(\"mrs %0, pmcr_el0\" : \"=r\" (val));\n\treturn val;\n}\n\nstatic inline void armv8pmu_pmcr_write(u32 val)\n{\n\tval &= ARMV8_PMCR_MASK;\n\tisb();\n\tasm volatile(\"msr pmcr_el0, %0\" :: \"r\" (val));\n}\n\nstatic inline int armv8pmu_has_overflowed(u32 pmovsr)\n{\n\treturn pmovsr & ARMV8_OVERFLOWED_MASK;\n}\n\nstatic inline int armv8pmu_counter_valid(int idx)\n{\n\treturn idx >= ARMV8_IDX_CYCLE_COUNTER && idx <= ARMV8_IDX_COUNTER_LAST;\n}\n\nstatic inline int armv8pmu_counter_has_overflowed(u32 pmnc, int idx)\n{\n\tint ret = 0;\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u checking wrong counter %d overflow status\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t} else {\n\t\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\t\tret = pmnc & BIT(counter);\n\t}\n\n\treturn ret;\n}\n\nstatic inline int armv8pmu_select_counter(int idx)\n{\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u selecting wrong PMNC counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t\treturn -EINVAL;\n\t}\n\n\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\tasm volatile(\"msr pmselr_el0, %0\" :: \"r\" (counter));\n\tisb();\n\n\treturn idx;\n}\n\nstatic inline u32 armv8pmu_read_counter(int idx)\n{\n\tu32 value = 0;\n\n\tif (!armv8pmu_counter_valid(idx))\n\t\tpr_err(\"CPU%u reading wrong counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\telse if (idx == ARMV8_IDX_CYCLE_COUNTER)\n\t\tasm volatile(\"mrs %0, pmccntr_el0\" : \"=r\" (value));\n\telse if (armv8pmu_select_counter(idx) == idx)\n\t\tasm volatile(\"mrs %0, pmxevcntr_el0\" : \"=r\" (value));\n\n\treturn value;\n}\n\nstatic inline void armv8pmu_write_counter(int idx, u32 value)\n{\n\tif (!armv8pmu_counter_valid(idx))\n\t\tpr_err(\"CPU%u writing wrong counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\telse if (idx == ARMV8_IDX_CYCLE_COUNTER)\n\t\tasm volatile(\"msr pmccntr_el0, %0\" :: \"r\" (value));\n\telse if (armv8pmu_select_counter(idx) == idx)\n\t\tasm volatile(\"msr pmxevcntr_el0, %0\" :: \"r\" (value));\n}\n\nstatic inline void armv8pmu_write_evtype(int idx, u32 val)\n{\n\tif (armv8pmu_select_counter(idx) == idx) {\n\t\tval &= ARMV8_EVTYPE_MASK;\n\t\tasm volatile(\"msr pmxevtyper_el0, %0\" :: \"r\" (val));\n\t}\n}\n\nstatic inline int armv8pmu_enable_counter(int idx)\n{\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u enabling wrong PMNC counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t\treturn -EINVAL;\n\t}\n\n\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\tasm volatile(\"msr pmcntenset_el0, %0\" :: \"r\" (BIT(counter)));\n\treturn idx;\n}\n\nstatic inline int armv8pmu_disable_counter(int idx)\n{\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u disabling wrong PMNC counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t\treturn -EINVAL;\n\t}\n\n\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\tasm volatile(\"msr pmcntenclr_el0, %0\" :: \"r\" (BIT(counter)));\n\treturn idx;\n}\n\nstatic inline int armv8pmu_enable_intens(int idx)\n{\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u enabling wrong PMNC counter IRQ enable %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t\treturn -EINVAL;\n\t}\n\n\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\tasm volatile(\"msr pmintenset_el1, %0\" :: \"r\" (BIT(counter)));\n\treturn idx;\n}\n\nstatic inline int armv8pmu_disable_intens(int idx)\n{\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u disabling wrong PMNC counter IRQ enable %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t\treturn -EINVAL;\n\t}\n\n\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\tasm volatile(\"msr pmintenclr_el1, %0\" :: \"r\" (BIT(counter)));\n\tisb();\n\t/* Clear the overflow flag in case an interrupt is pending. */\n\tasm volatile(\"msr pmovsclr_el0, %0\" :: \"r\" (BIT(counter)));\n\tisb();\n\treturn idx;\n}\n\nstatic inline u32 armv8pmu_getreset_flags(void)\n{\n\tu32 value;\n\n\t/* Read */\n\tasm volatile(\"mrs %0, pmovsclr_el0\" : \"=r\" (value));\n\n\t/* Write to clear flags */\n\tvalue &= ARMV8_OVSR_MASK;\n\tasm volatile(\"msr pmovsclr_el0, %0\" :: \"r\" (value));\n\n\treturn value;\n}\n\nstatic void armv8pmu_enable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags;\n\tstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\n\n\t/*\n\t * Enable counter and interrupt, and set the counter to count\n\t * the event that we're interested in.\n\t */\n\traw_spin_lock_irqsave(&events->pmu_lock, flags);\n\n\t/*\n\t * Disable counter\n\t */\n\tarmv8pmu_disable_counter(idx);\n\n\t/*\n\t * Set event (if destined for PMNx counters).\n\t */\n\tarmv8pmu_write_evtype(idx, hwc->config_base);\n\n\t/*\n\t * Enable interrupt for this counter\n\t */\n\tarmv8pmu_enable_intens(idx);\n\n\t/*\n\t * Enable counter\n\t */\n\tarmv8pmu_enable_counter(idx);\n\n\traw_spin_unlock_irqrestore(&events->pmu_lock, flags);\n}\n\nstatic void armv8pmu_disable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags;\n\tstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\n\n\t/*\n\t * Disable counter and interrupt\n\t */\n\traw_spin_lock_irqsave(&events->pmu_lock, flags);\n\n\t/*\n\t * Disable counter\n\t */\n\tarmv8pmu_disable_counter(idx);\n\n\t/*\n\t * Disable interrupt for this counter\n\t */\n\tarmv8pmu_disable_intens(idx);\n\n\traw_spin_unlock_irqrestore(&events->pmu_lock, flags);\n}\n\nstatic irqreturn_t armv8pmu_handle_irq(int irq_num, void *dev)\n{\n\tu32 pmovsr;\n\tstruct perf_sample_data data;\n\tstruct pmu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\t/*\n\t * Get and reset the IRQ flags\n\t */\n\tpmovsr = armv8pmu_getreset_flags();\n\n\t/*\n\t * Did an overflow occur?\n\t */\n\tif (!armv8pmu_has_overflowed(pmovsr))\n\t\treturn IRQ_NONE;\n\n\t/*\n\t * Handle the counter(s) overflow(s)\n\t */\n\tregs = get_irq_regs();\n\n\tcpuc = this_cpu_ptr(&cpu_hw_events);\n\tfor (idx = 0; idx < cpu_pmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\t/* Ignore if we don't have an event. */\n\t\tif (!event)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We have a single interrupt for all counters. Check that\n\t\t * each counter has overflowed before we process it.\n\t\t */\n\t\tif (!armv8pmu_counter_has_overflowed(pmovsr, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx);\n\t\tperf_sample_data_init(&data, 0, hwc->last_period);\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tcpu_pmu->disable(hwc, idx);\n\t}\n\n\t/*\n\t * Handle the pending perf events.\n\t *\n\t * Note: this call *must* be run with interrupts disabled. For\n\t * platforms that can have the PMU interrupts raised as an NMI, this\n\t * will not work.\n\t */\n\tirq_work_run();\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void armv8pmu_start(void)\n{\n\tunsigned long flags;\n\tstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\n\n\traw_spin_lock_irqsave(&events->pmu_lock, flags);\n\t/* Enable all counters */\n\tarmv8pmu_pmcr_write(armv8pmu_pmcr_read() | ARMV8_PMCR_E);\n\traw_spin_unlock_irqrestore(&events->pmu_lock, flags);\n}\n\nstatic void armv8pmu_stop(void)\n{\n\tunsigned long flags;\n\tstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\n\n\traw_spin_lock_irqsave(&events->pmu_lock, flags);\n\t/* Disable all counters */\n\tarmv8pmu_pmcr_write(armv8pmu_pmcr_read() & ~ARMV8_PMCR_E);\n\traw_spin_unlock_irqrestore(&events->pmu_lock, flags);\n}\n\nstatic int armv8pmu_get_event_idx(struct pmu_hw_events *cpuc,\n\t\t\t\t  struct hw_perf_event *event)\n{\n\tint idx;\n\tunsigned long evtype = event->config_base & ARMV8_EVTYPE_EVENT;\n\n\t/* Always place a cycle counter into the cycle counter. */\n\tif (evtype == ARMV8_PMUV3_PERFCTR_CLOCK_CYCLES) {\n\t\tif (test_and_set_bit(ARMV8_IDX_CYCLE_COUNTER, cpuc->used_mask))\n\t\t\treturn -EAGAIN;\n\n\t\treturn ARMV8_IDX_CYCLE_COUNTER;\n\t}\n\n\t/*\n\t * For anything other than a cycle counter, try and use\n\t * the events counters\n\t */\n\tfor (idx = ARMV8_IDX_COUNTER0; idx < cpu_pmu->num_events; ++idx) {\n\t\tif (!test_and_set_bit(idx, cpuc->used_mask))\n\t\t\treturn idx;\n\t}\n\n\t/* The counters are all in use. */\n\treturn -EAGAIN;\n}\n\n/*\n * Add an event filter to a given event. This will only work for PMUv2 PMUs.\n */\nstatic int armv8pmu_set_event_filter(struct hw_perf_event *event,\n\t\t\t\t     struct perf_event_attr *attr)\n{\n\tunsigned long config_base = 0;\n\n\tif (attr->exclude_idle)\n\t\treturn -EPERM;\n\tif (attr->exclude_user)\n\t\tconfig_base |= ARMV8_EXCLUDE_EL0;\n\tif (attr->exclude_kernel)\n\t\tconfig_base |= ARMV8_EXCLUDE_EL1;\n\tif (!attr->exclude_hv)\n\t\tconfig_base |= ARMV8_INCLUDE_EL2;\n\n\t/*\n\t * Install the filter into config_base as this is used to\n\t * construct the event type.\n\t */\n\tevent->config_base = config_base;\n\n\treturn 0;\n}\n\nstatic void armv8pmu_reset(void *info)\n{\n\tu32 idx, nb_cnt = cpu_pmu->num_events;\n\n\t/* The counter and interrupt enable registers are unknown at reset. */\n\tfor (idx = ARMV8_IDX_CYCLE_COUNTER; idx < nb_cnt; ++idx)\n\t\tarmv8pmu_disable_event(NULL, idx);\n\n\t/* Initialize & Reset PMNC: C and P bits. */\n\tarmv8pmu_pmcr_write(ARMV8_PMCR_P | ARMV8_PMCR_C);\n\n\t/* Disable access from userspace. */\n\tasm volatile(\"msr pmuserenr_el0, %0\" :: \"r\" (0));\n}\n\nstatic int armv8_pmuv3_map_event(struct perf_event *event)\n{\n\treturn map_cpu_event(event, &armv8_pmuv3_perf_map,\n\t\t\t\t&armv8_pmuv3_perf_cache_map,\n\t\t\t\tARMV8_EVTYPE_EVENT);\n}\n\nstatic struct arm_pmu armv8pmu = {\n\t.handle_irq\t\t= armv8pmu_handle_irq,\n\t.enable\t\t\t= armv8pmu_enable_event,\n\t.disable\t\t= armv8pmu_disable_event,\n\t.read_counter\t\t= armv8pmu_read_counter,\n\t.write_counter\t\t= armv8pmu_write_counter,\n\t.get_event_idx\t\t= armv8pmu_get_event_idx,\n\t.start\t\t\t= armv8pmu_start,\n\t.stop\t\t\t= armv8pmu_stop,\n\t.reset\t\t\t= armv8pmu_reset,\n\t.max_period\t\t= (1LLU << 32) - 1,\n};\n\nstatic u32 __init armv8pmu_read_num_pmnc_events(void)\n{\n\tu32 nb_cnt;\n\n\t/* Read the nb of CNTx counters supported from PMNC */\n\tnb_cnt = (armv8pmu_pmcr_read() >> ARMV8_PMCR_N_SHIFT) & ARMV8_PMCR_N_MASK;\n\n\t/* Add the CPU cycles counter and return */\n\treturn nb_cnt + 1;\n}\n\nstatic struct arm_pmu *__init armv8_pmuv3_pmu_init(void)\n{\n\tarmv8pmu.name\t\t\t= \"arm/armv8-pmuv3\";\n\tarmv8pmu.map_event\t\t= armv8_pmuv3_map_event;\n\tarmv8pmu.num_events\t\t= armv8pmu_read_num_pmnc_events();\n\tarmv8pmu.set_event_filter\t= armv8pmu_set_event_filter;\n\treturn &armv8pmu;\n}\n\n/*\n * Ensure the PMU has sane values out of reset.\n * This requires SMP to be available, so exists as a separate initcall.\n */\nstatic int __init\ncpu_pmu_reset(void)\n{\n\tif (cpu_pmu && cpu_pmu->reset)\n\t\treturn on_each_cpu(cpu_pmu->reset, NULL, 1);\n\treturn 0;\n}\narch_initcall(cpu_pmu_reset);\n\n/*\n * PMU platform driver and devicetree bindings.\n */\nstatic const struct of_device_id armpmu_of_device_ids[] = {\n\t{.compatible = \"arm,armv8-pmuv3\"},\n\t{},\n};\n\nstatic int armpmu_device_probe(struct platform_device *pdev)\n{\n\tif (!cpu_pmu)\n\t\treturn -ENODEV;\n\n\tcpu_pmu->plat_device = pdev;\n\treturn 0;\n}\n\nstatic struct platform_driver armpmu_driver = {\n\t.driver\t\t= {\n\t\t.name\t= \"arm-pmu\",\n\t\t.of_match_table = armpmu_of_device_ids,\n\t},\n\t.probe\t\t= armpmu_device_probe,\n};\n\nstatic int __init register_pmu_driver(void)\n{\n\treturn platform_driver_register(&armpmu_driver);\n}\ndevice_initcall(register_pmu_driver);\n\nstatic struct pmu_hw_events *armpmu_get_cpu_events(void)\n{\n\treturn this_cpu_ptr(&cpu_hw_events);\n}\n\nstatic void __init cpu_pmu_init(struct arm_pmu *armpmu)\n{\n\tint cpu;\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct pmu_hw_events *events = &per_cpu(cpu_hw_events, cpu);\n\t\tevents->events = per_cpu(hw_events, cpu);\n\t\tevents->used_mask = per_cpu(used_mask, cpu);\n\t\traw_spin_lock_init(&events->pmu_lock);\n\t}\n\tarmpmu->get_hw_events = armpmu_get_cpu_events;\n}\n\nstatic int __init init_hw_perf_events(void)\n{\n\tu64 dfr = read_cpuid(ID_AA64DFR0_EL1);\n\n\tswitch ((dfr >> 8) & 0xf) {\n\tcase 0x1:\t/* PMUv3 */\n\t\tcpu_pmu = armv8_pmuv3_pmu_init();\n\t\tbreak;\n\t}\n\n\tif (cpu_pmu) {\n\t\tpr_info(\"enabled with %s PMU driver, %d counters available\\n\",\n\t\t\tcpu_pmu->name, cpu_pmu->num_events);\n\t\tcpu_pmu_init(cpu_pmu);\n\t\tarmpmu_register(cpu_pmu, \"cpu\", PERF_TYPE_RAW);\n\t} else {\n\t\tpr_info(\"no hardware support available\\n\");\n\t}\n\n\treturn 0;\n}\nearly_initcall(init_hw_perf_events);\n\n/*\n * Callchain handling code.\n */\nstruct frame_tail {\n\tstruct frame_tail\t__user *fp;\n\tunsigned long\t\tlr;\n} __attribute__((packed));\n\n/*\n * Get the return address for a single stackframe and return a pointer to the\n * next frame tail.\n */\nstatic struct frame_tail __user *\nuser_backtrace(struct frame_tail __user *tail,\n\t       struct perf_callchain_entry *entry)\n{\n\tstruct frame_tail buftail;\n\tunsigned long err;\n\n\t/* Also check accessibility of one struct frame_tail beyond */\n\tif (!access_ok(VERIFY_READ, tail, sizeof(buftail)))\n\t\treturn NULL;\n\n\tpagefault_disable();\n\terr = __copy_from_user_inatomic(&buftail, tail, sizeof(buftail));\n\tpagefault_enable();\n\n\tif (err)\n\t\treturn NULL;\n\n\tperf_callchain_store(entry, buftail.lr);\n\n\t/*\n\t * Frame pointers should strictly progress back up the stack\n\t * (towards higher addresses).\n\t */\n\tif (tail >= buftail.fp)\n\t\treturn NULL;\n\n\treturn buftail.fp;\n}\n\n#ifdef CONFIG_COMPAT\n/*\n * The registers we're interested in are at the end of the variable\n * length saved register structure. The fp points at the end of this\n * structure so the address of this struct is:\n * (struct compat_frame_tail *)(xxx->fp)-1\n *\n * This code has been adapted from the ARM OProfile support.\n */\nstruct compat_frame_tail {\n\tcompat_uptr_t\tfp; /* a (struct compat_frame_tail *) in compat mode */\n\tu32\t\tsp;\n\tu32\t\tlr;\n} __attribute__((packed));\n\nstatic struct compat_frame_tail __user *\ncompat_user_backtrace(struct compat_frame_tail __user *tail,\n\t\t      struct perf_callchain_entry *entry)\n{\n\tstruct compat_frame_tail buftail;\n\tunsigned long err;\n\n\t/* Also check accessibility of one struct frame_tail beyond */\n\tif (!access_ok(VERIFY_READ, tail, sizeof(buftail)))\n\t\treturn NULL;\n\n\tpagefault_disable();\n\terr = __copy_from_user_inatomic(&buftail, tail, sizeof(buftail));\n\tpagefault_enable();\n\n\tif (err)\n\t\treturn NULL;\n\n\tperf_callchain_store(entry, buftail.lr);\n\n\t/*\n\t * Frame pointers should strictly progress back up the stack\n\t * (towards higher addresses).\n\t */\n\tif (tail + 1 >= (struct compat_frame_tail __user *)\n\t\t\tcompat_ptr(buftail.fp))\n\t\treturn NULL;\n\n\treturn (struct compat_frame_tail __user *)compat_ptr(buftail.fp) - 1;\n}\n#endif /* CONFIG_COMPAT */\n\nvoid perf_callchain_user(struct perf_callchain_entry *entry,\n\t\t\t struct pt_regs *regs)\n{\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tperf_callchain_store(entry, regs->pc);\n\n\tif (!compat_user_mode(regs)) {\n\t\t/* AARCH64 mode */\n\t\tstruct frame_tail __user *tail;\n\n\t\ttail = (struct frame_tail __user *)regs->regs[29];\n\n\t\twhile (entry->nr < PERF_MAX_STACK_DEPTH &&\n\t\t       tail && !((unsigned long)tail & 0xf))\n\t\t\ttail = user_backtrace(tail, entry);\n\t} else {\n#ifdef CONFIG_COMPAT\n\t\t/* AARCH32 compat mode */\n\t\tstruct compat_frame_tail __user *tail;\n\n\t\ttail = (struct compat_frame_tail __user *)regs->compat_fp - 1;\n\n\t\twhile ((entry->nr < PERF_MAX_STACK_DEPTH) &&\n\t\t\ttail && !((unsigned long)tail & 0x3))\n\t\t\ttail = compat_user_backtrace(tail, entry);\n#endif\n\t}\n}\n\n/*\n * Gets called by walk_stackframe() for every stackframe. This will be called\n * whist unwinding the stackframe and is like a subroutine return so we use\n * the PC.\n */\nstatic int callchain_trace(struct stackframe *frame, void *data)\n{\n\tstruct perf_callchain_entry *entry = data;\n\tperf_callchain_store(entry, frame->pc);\n\treturn 0;\n}\n\nvoid perf_callchain_kernel(struct perf_callchain_entry *entry,\n\t\t\t   struct pt_regs *regs)\n{\n\tstruct stackframe frame;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tframe.fp = regs->regs[29];\n\tframe.sp = regs->sp;\n\tframe.pc = regs->pc;\n\n\twalk_stackframe(&frame, callchain_trace, entry);\n}\n\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest())\n\t\treturn perf_guest_cbs->get_guest_ip();\n\n\treturn instruction_pointer(regs);\n}\n\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tint misc = 0;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\tif (perf_guest_cbs->is_user_mode())\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_KERNEL;\n\t} else {\n\t\tif (user_mode(regs))\n\t\t\tmisc |= PERF_RECORD_MISC_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_KERNEL;\n\t}\n\n\treturn misc;\n}\n"], "fixing_code": ["/*\n * PMU support\n *\n * Copyright (C) 2012 ARM Limited\n * Author: Will Deacon <will.deacon@arm.com>\n *\n * This code is based heavily on the ARMv7 perf event code.\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License version 2 as\n * published by the Free Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n */\n#define pr_fmt(fmt) \"hw perfevents: \" fmt\n\n#include <linux/bitmap.h>\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n#include <linux/perf_event.h>\n#include <linux/platform_device.h>\n#include <linux/spinlock.h>\n#include <linux/uaccess.h>\n\n#include <asm/cputype.h>\n#include <asm/irq.h>\n#include <asm/irq_regs.h>\n#include <asm/pmu.h>\n#include <asm/stacktrace.h>\n\n/*\n * ARMv8 supports a maximum of 32 events.\n * The cycle counter is included in this total.\n */\n#define ARMPMU_MAX_HWEVENTS\t\t32\n\nstatic DEFINE_PER_CPU(struct perf_event * [ARMPMU_MAX_HWEVENTS], hw_events);\nstatic DEFINE_PER_CPU(unsigned long [BITS_TO_LONGS(ARMPMU_MAX_HWEVENTS)], used_mask);\nstatic DEFINE_PER_CPU(struct pmu_hw_events, cpu_hw_events);\n\n#define to_arm_pmu(p) (container_of(p, struct arm_pmu, pmu))\n\n/* Set at runtime when we know what CPU type we are. */\nstatic struct arm_pmu *cpu_pmu;\n\nint\narmpmu_get_max_events(void)\n{\n\tint max_events = 0;\n\n\tif (cpu_pmu != NULL)\n\t\tmax_events = cpu_pmu->num_events;\n\n\treturn max_events;\n}\nEXPORT_SYMBOL_GPL(armpmu_get_max_events);\n\nint perf_num_counters(void)\n{\n\treturn armpmu_get_max_events();\n}\nEXPORT_SYMBOL_GPL(perf_num_counters);\n\n#define HW_OP_UNSUPPORTED\t\t0xFFFF\n\n#define C(_x) \\\n\tPERF_COUNT_HW_CACHE_##_x\n\n#define CACHE_OP_UNSUPPORTED\t\t0xFFFF\n\nstatic int\narmpmu_map_cache_event(const unsigned (*cache_map)\n\t\t\t\t      [PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t      [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t      [PERF_COUNT_HW_CACHE_RESULT_MAX],\n\t\t       u64 config)\n{\n\tunsigned int cache_type, cache_op, cache_result, ret;\n\n\tcache_type = (config >>  0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn -EINVAL;\n\n\tcache_op = (config >>  8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn -EINVAL;\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tret = (int)(*cache_map)[cache_type][cache_op][cache_result];\n\n\tif (ret == CACHE_OP_UNSUPPORTED)\n\t\treturn -ENOENT;\n\n\treturn ret;\n}\n\nstatic int\narmpmu_map_event(const unsigned (*event_map)[PERF_COUNT_HW_MAX], u64 config)\n{\n\tint mapping;\n\n\tif (config >= PERF_COUNT_HW_MAX)\n\t\treturn -EINVAL;\n\n\tmapping = (*event_map)[config];\n\treturn mapping == HW_OP_UNSUPPORTED ? -ENOENT : mapping;\n}\n\nstatic int\narmpmu_map_raw_event(u32 raw_event_mask, u64 config)\n{\n\treturn (int)(config & raw_event_mask);\n}\n\nstatic int map_cpu_event(struct perf_event *event,\n\t\t\t const unsigned (*event_map)[PERF_COUNT_HW_MAX],\n\t\t\t const unsigned (*cache_map)\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX],\n\t\t\t u32 raw_event_mask)\n{\n\tu64 config = event->attr.config;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\treturn armpmu_map_event(event_map, config);\n\tcase PERF_TYPE_HW_CACHE:\n\t\treturn armpmu_map_cache_event(cache_map, config);\n\tcase PERF_TYPE_RAW:\n\t\treturn armpmu_map_raw_event(raw_event_mask, config);\n\t}\n\n\treturn -ENOENT;\n}\n\nint\narmpmu_event_set_period(struct perf_event *event,\n\t\t\tstruct hw_perf_event *hwc,\n\t\t\tint idx)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint ret = 0;\n\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\t/*\n\t * Limit the maximum period to prevent the counter value\n\t * from overtaking the one we are about to program. In\n\t * effect we are reducing max_period to account for\n\t * interrupt latency (and we are being very conservative).\n\t */\n\tif (left > (armpmu->max_period >> 1))\n\t\tleft = armpmu->max_period >> 1;\n\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\tarmpmu->write_counter(idx, (u64)(-left) & 0xffffffff);\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\nu64\narmpmu_event_update(struct perf_event *event,\n\t\t    struct hw_perf_event *hwc,\n\t\t    int idx)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tu64 delta, prev_raw_count, new_raw_count;\n\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tnew_raw_count = armpmu->read_counter(idx);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t     new_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\tdelta = (new_raw_count - prev_raw_count) & armpmu->max_period;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\nstatic void\narmpmu_read(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/* Don't read disabled counters! */\n\tif (hwc->idx < 0)\n\t\treturn;\n\n\tarmpmu_event_update(event, hwc, hwc->idx);\n}\n\nstatic void\narmpmu_stop(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/*\n\t * ARM pmu always has to update the counter, so ignore\n\t * PERF_EF_UPDATE, see comments in armpmu_start().\n\t */\n\tif (!(hwc->state & PERF_HES_STOPPED)) {\n\t\tarmpmu->disable(hwc, hwc->idx);\n\t\tbarrier(); /* why? */\n\t\tarmpmu_event_update(event, hwc, hwc->idx);\n\t\thwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\t}\n}\n\nstatic void\narmpmu_start(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/*\n\t * ARM pmu always has to reprogram the period, so ignore\n\t * PERF_EF_RELOAD, see the comment below.\n\t */\n\tif (flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\n\thwc->state = 0;\n\t/*\n\t * Set the period again. Some counters can't be stopped, so when we\n\t * were stopped we simply disabled the IRQ source and the counter\n\t * may have been left counting. If we don't do this step then we may\n\t * get an interrupt too soon or *way* too late if the overflow has\n\t * happened since disabling.\n\t */\n\tarmpmu_event_set_period(event, hwc, hwc->idx);\n\tarmpmu->enable(hwc, hwc->idx);\n}\n\nstatic void\narmpmu_del(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tWARN_ON(idx < 0);\n\n\tarmpmu_stop(event, PERF_EF_UPDATE);\n\thw_events->events[idx] = NULL;\n\tclear_bit(idx, hw_events->used_mask);\n\n\tperf_event_update_userpage(event);\n}\n\nstatic int\narmpmu_add(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\tint err = 0;\n\n\tperf_pmu_disable(event->pmu);\n\n\t/* If we don't have a space for the counter then finish early. */\n\tidx = armpmu->get_event_idx(hw_events, hwc);\n\tif (idx < 0) {\n\t\terr = idx;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If there is an event in the counter we are going to use then make\n\t * sure it is disabled.\n\t */\n\tevent->hw.idx = idx;\n\tarmpmu->disable(hwc, idx);\n\thw_events->events[idx] = event;\n\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\tif (flags & PERF_EF_START)\n\t\tarmpmu_start(event, PERF_EF_RELOAD);\n\n\t/* Propagate our changes to the userspace mapping. */\n\tperf_event_update_userpage(event);\n\nout:\n\tperf_pmu_enable(event->pmu);\n\treturn err;\n}\n\nstatic int\nvalidate_event(struct pmu *pmu, struct pmu_hw_events *hw_events,\n\t\t\t\tstruct perf_event *event)\n{\n\tstruct arm_pmu *armpmu;\n\tstruct hw_perf_event fake_event = event->hw;\n\tstruct pmu *leader_pmu = event->group_leader->pmu;\n\n\tif (is_software_event(event))\n\t\treturn 1;\n\n\t/*\n\t * Reject groups spanning multiple HW PMUs (e.g. CPU + CCI). The\n\t * core perf code won't check that the pmu->ctx == leader->ctx\n\t * until after pmu->event_init(event).\n\t */\n\tif (event->pmu != pmu)\n\t\treturn 0;\n\n\tif (event->pmu != leader_pmu || event->state < PERF_EVENT_STATE_OFF)\n\t\treturn 1;\n\n\tif (event->state == PERF_EVENT_STATE_OFF && !event->attr.enable_on_exec)\n\t\treturn 1;\n\n\tarmpmu = to_arm_pmu(event->pmu);\n\treturn armpmu->get_event_idx(hw_events, &fake_event) >= 0;\n}\n\nstatic int\nvalidate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct pmu_hw_events fake_pmu;\n\tDECLARE_BITMAP(fake_used_mask, ARMPMU_MAX_HWEVENTS);\n\n\t/*\n\t * Initialise the fake PMU. We only need to populate the\n\t * used_mask for the purposes of validation.\n\t */\n\tmemset(fake_used_mask, 0, sizeof(fake_used_mask));\n\tfake_pmu.used_mask = fake_used_mask;\n\n\tif (!validate_event(event->pmu, &fake_pmu, leader))\n\t\treturn -EINVAL;\n\n\tlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\n\t\tif (!validate_event(event->pmu, &fake_pmu, sibling))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!validate_event(event->pmu, &fake_pmu, event))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void\narmpmu_disable_percpu_irq(void *data)\n{\n\tunsigned int irq = *(unsigned int *)data;\n\tdisable_percpu_irq(irq);\n}\n\nstatic void\narmpmu_release_hardware(struct arm_pmu *armpmu)\n{\n\tint irq;\n\tunsigned int i, irqs;\n\tstruct platform_device *pmu_device = armpmu->plat_device;\n\n\tirqs = min(pmu_device->num_resources, num_possible_cpus());\n\tif (!irqs)\n\t\treturn;\n\n\tirq = platform_get_irq(pmu_device, 0);\n\tif (irq <= 0)\n\t\treturn;\n\n\tif (irq_is_percpu(irq)) {\n\t\ton_each_cpu(armpmu_disable_percpu_irq, &irq, 1);\n\t\tfree_percpu_irq(irq, &cpu_hw_events);\n\t} else {\n\t\tfor (i = 0; i < irqs; ++i) {\n\t\t\tif (!cpumask_test_and_clear_cpu(i, &armpmu->active_irqs))\n\t\t\t\tcontinue;\n\t\t\tirq = platform_get_irq(pmu_device, i);\n\t\t\tif (irq > 0)\n\t\t\t\tfree_irq(irq, armpmu);\n\t\t}\n\t}\n}\n\nstatic void\narmpmu_enable_percpu_irq(void *data)\n{\n\tunsigned int irq = *(unsigned int *)data;\n\tenable_percpu_irq(irq, IRQ_TYPE_NONE);\n}\n\nstatic int\narmpmu_reserve_hardware(struct arm_pmu *armpmu)\n{\n\tint err, irq;\n\tunsigned int i, irqs;\n\tstruct platform_device *pmu_device = armpmu->plat_device;\n\n\tif (!pmu_device) {\n\t\tpr_err(\"no PMU device registered\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tirqs = min(pmu_device->num_resources, num_possible_cpus());\n\tif (!irqs) {\n\t\tpr_err(\"no irqs for PMUs defined\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tirq = platform_get_irq(pmu_device, 0);\n\tif (irq <= 0) {\n\t\tpr_err(\"failed to get valid irq for PMU device\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (irq_is_percpu(irq)) {\n\t\terr = request_percpu_irq(irq, armpmu->handle_irq,\n\t\t\t\t\"arm-pmu\", &cpu_hw_events);\n\n\t\tif (err) {\n\t\t\tpr_err(\"unable to request percpu IRQ%d for ARM PMU counters\\n\",\n\t\t\t\t\tirq);\n\t\t\tarmpmu_release_hardware(armpmu);\n\t\t\treturn err;\n\t\t}\n\n\t\ton_each_cpu(armpmu_enable_percpu_irq, &irq, 1);\n\t} else {\n\t\tfor (i = 0; i < irqs; ++i) {\n\t\t\terr = 0;\n\t\t\tirq = platform_get_irq(pmu_device, i);\n\t\t\tif (irq <= 0)\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * If we have a single PMU interrupt that we can't shift,\n\t\t\t * assume that we're running on a uniprocessor machine and\n\t\t\t * continue. Otherwise, continue without this interrupt.\n\t\t\t */\n\t\t\tif (irq_set_affinity(irq, cpumask_of(i)) && irqs > 1) {\n\t\t\t\tpr_warning(\"unable to set irq affinity (irq=%d, cpu=%u)\\n\",\n\t\t\t\t\t\tirq, i);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\terr = request_irq(irq, armpmu->handle_irq,\n\t\t\t\t\tIRQF_NOBALANCING,\n\t\t\t\t\t\"arm-pmu\", armpmu);\n\t\t\tif (err) {\n\t\t\t\tpr_err(\"unable to request IRQ%d for ARM PMU counters\\n\",\n\t\t\t\t\t\tirq);\n\t\t\t\tarmpmu_release_hardware(armpmu);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\tcpumask_set_cpu(i, &armpmu->active_irqs);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void\nhw_perf_event_destroy(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tatomic_t *active_events\t = &armpmu->active_events;\n\tstruct mutex *pmu_reserve_mutex = &armpmu->reserve_mutex;\n\n\tif (atomic_dec_and_mutex_lock(active_events, pmu_reserve_mutex)) {\n\t\tarmpmu_release_hardware(armpmu);\n\t\tmutex_unlock(pmu_reserve_mutex);\n\t}\n}\n\nstatic int\nevent_requires_mode_exclusion(struct perf_event_attr *attr)\n{\n\treturn attr->exclude_idle || attr->exclude_user ||\n\t       attr->exclude_kernel || attr->exclude_hv;\n}\n\nstatic int\n__hw_perf_event_init(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint mapping, err;\n\n\tmapping = armpmu->map_event(event);\n\n\tif (mapping < 0) {\n\t\tpr_debug(\"event %x:%llx not supported\\n\", event->attr.type,\n\t\t\t event->attr.config);\n\t\treturn mapping;\n\t}\n\n\t/*\n\t * We don't assign an index until we actually place the event onto\n\t * hardware. Use -1 to signify that we haven't decided where to put it\n\t * yet. For SMP systems, each core has it's own PMU so we can't do any\n\t * clever allocation or constraints checking at this point.\n\t */\n\thwc->idx\t\t= -1;\n\thwc->config_base\t= 0;\n\thwc->config\t\t= 0;\n\thwc->event_base\t\t= 0;\n\n\t/*\n\t * Check whether we need to exclude the counter from certain modes.\n\t */\n\tif ((!armpmu->set_event_filter ||\n\t     armpmu->set_event_filter(hwc, &event->attr)) &&\n\t     event_requires_mode_exclusion(&event->attr)) {\n\t\tpr_debug(\"ARM performance counters do not support mode exclusion\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t/*\n\t * Store the event encoding into the config_base field.\n\t */\n\thwc->config_base\t    |= (unsigned long)mapping;\n\n\tif (!hwc->sample_period) {\n\t\t/*\n\t\t * For non-sampling runs, limit the sample_period to half\n\t\t * of the counter width. That way, the new counter value\n\t\t * is far less likely to overtake the previous one unless\n\t\t * you have some serious IRQ latency issues.\n\t\t */\n\t\thwc->sample_period  = armpmu->max_period >> 1;\n\t\thwc->last_period    = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t}\n\n\terr = 0;\n\tif (event->group_leader != event) {\n\t\terr = validate_group(event);\n\t\tif (err)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn err;\n}\n\nstatic int armpmu_event_init(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tint err = 0;\n\tatomic_t *active_events = &armpmu->active_events;\n\n\tif (armpmu->map_event(event) == -ENOENT)\n\t\treturn -ENOENT;\n\n\tevent->destroy = hw_perf_event_destroy;\n\n\tif (!atomic_inc_not_zero(active_events)) {\n\t\tmutex_lock(&armpmu->reserve_mutex);\n\t\tif (atomic_read(active_events) == 0)\n\t\t\terr = armpmu_reserve_hardware(armpmu);\n\n\t\tif (!err)\n\t\t\tatomic_inc(active_events);\n\t\tmutex_unlock(&armpmu->reserve_mutex);\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\terr = __hw_perf_event_init(event);\n\tif (err)\n\t\thw_perf_event_destroy(event);\n\n\treturn err;\n}\n\nstatic void armpmu_enable(struct pmu *pmu)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tint enabled = bitmap_weight(hw_events->used_mask, armpmu->num_events);\n\n\tif (enabled)\n\t\tarmpmu->start();\n}\n\nstatic void armpmu_disable(struct pmu *pmu)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(pmu);\n\tarmpmu->stop();\n}\n\nstatic void __init armpmu_init(struct arm_pmu *armpmu)\n{\n\tatomic_set(&armpmu->active_events, 0);\n\tmutex_init(&armpmu->reserve_mutex);\n\n\tarmpmu->pmu = (struct pmu) {\n\t\t.pmu_enable\t= armpmu_enable,\n\t\t.pmu_disable\t= armpmu_disable,\n\t\t.event_init\t= armpmu_event_init,\n\t\t.add\t\t= armpmu_add,\n\t\t.del\t\t= armpmu_del,\n\t\t.start\t\t= armpmu_start,\n\t\t.stop\t\t= armpmu_stop,\n\t\t.read\t\t= armpmu_read,\n\t};\n}\n\nint __init armpmu_register(struct arm_pmu *armpmu, char *name, int type)\n{\n\tarmpmu_init(armpmu);\n\treturn perf_pmu_register(&armpmu->pmu, name, type);\n}\n\n/*\n * ARMv8 PMUv3 Performance Events handling code.\n * Common event types.\n */\nenum armv8_pmuv3_perf_types {\n\t/* Required events. */\n\tARMV8_PMUV3_PERFCTR_PMNC_SW_INCR\t\t\t= 0x00,\n\tARMV8_PMUV3_PERFCTR_L1_DCACHE_REFILL\t\t\t= 0x03,\n\tARMV8_PMUV3_PERFCTR_L1_DCACHE_ACCESS\t\t\t= 0x04,\n\tARMV8_PMUV3_PERFCTR_PC_BRANCH_MIS_PRED\t\t\t= 0x10,\n\tARMV8_PMUV3_PERFCTR_CLOCK_CYCLES\t\t\t= 0x11,\n\tARMV8_PMUV3_PERFCTR_PC_BRANCH_PRED\t\t\t= 0x12,\n\n\t/* At least one of the following is required. */\n\tARMV8_PMUV3_PERFCTR_INSTR_EXECUTED\t\t\t= 0x08,\n\tARMV8_PMUV3_PERFCTR_OP_SPEC\t\t\t\t= 0x1B,\n\n\t/* Common architectural events. */\n\tARMV8_PMUV3_PERFCTR_MEM_READ\t\t\t\t= 0x06,\n\tARMV8_PMUV3_PERFCTR_MEM_WRITE\t\t\t\t= 0x07,\n\tARMV8_PMUV3_PERFCTR_EXC_TAKEN\t\t\t\t= 0x09,\n\tARMV8_PMUV3_PERFCTR_EXC_EXECUTED\t\t\t= 0x0A,\n\tARMV8_PMUV3_PERFCTR_CID_WRITE\t\t\t\t= 0x0B,\n\tARMV8_PMUV3_PERFCTR_PC_WRITE\t\t\t\t= 0x0C,\n\tARMV8_PMUV3_PERFCTR_PC_IMM_BRANCH\t\t\t= 0x0D,\n\tARMV8_PMUV3_PERFCTR_PC_PROC_RETURN\t\t\t= 0x0E,\n\tARMV8_PMUV3_PERFCTR_MEM_UNALIGNED_ACCESS\t\t= 0x0F,\n\tARMV8_PMUV3_PERFCTR_TTBR_WRITE\t\t\t\t= 0x1C,\n\n\t/* Common microarchitectural events. */\n\tARMV8_PMUV3_PERFCTR_L1_ICACHE_REFILL\t\t\t= 0x01,\n\tARMV8_PMUV3_PERFCTR_ITLB_REFILL\t\t\t\t= 0x02,\n\tARMV8_PMUV3_PERFCTR_DTLB_REFILL\t\t\t\t= 0x05,\n\tARMV8_PMUV3_PERFCTR_MEM_ACCESS\t\t\t\t= 0x13,\n\tARMV8_PMUV3_PERFCTR_L1_ICACHE_ACCESS\t\t\t= 0x14,\n\tARMV8_PMUV3_PERFCTR_L1_DCACHE_WB\t\t\t= 0x15,\n\tARMV8_PMUV3_PERFCTR_L2_CACHE_ACCESS\t\t\t= 0x16,\n\tARMV8_PMUV3_PERFCTR_L2_CACHE_REFILL\t\t\t= 0x17,\n\tARMV8_PMUV3_PERFCTR_L2_CACHE_WB\t\t\t\t= 0x18,\n\tARMV8_PMUV3_PERFCTR_BUS_ACCESS\t\t\t\t= 0x19,\n\tARMV8_PMUV3_PERFCTR_MEM_ERROR\t\t\t\t= 0x1A,\n\tARMV8_PMUV3_PERFCTR_BUS_CYCLES\t\t\t\t= 0x1D,\n};\n\n/* PMUv3 HW events mapping. */\nstatic const unsigned armv8_pmuv3_perf_map[PERF_COUNT_HW_MAX] = {\n\t[PERF_COUNT_HW_CPU_CYCLES]\t\t= ARMV8_PMUV3_PERFCTR_CLOCK_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t\t= ARMV8_PMUV3_PERFCTR_INSTR_EXECUTED,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_ACCESS,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_REFILL,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS]\t= HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t\t= ARMV8_PMUV3_PERFCTR_PC_BRANCH_MIS_PRED,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t\t= HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND]\t= HW_OP_UNSUPPORTED,\n\t[PERF_COUNT_HW_STALLED_CYCLES_BACKEND]\t= HW_OP_UNSUPPORTED,\n};\n\nstatic const unsigned armv8_pmuv3_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_L1_DCACHE_REFILL,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_PC_BRANCH_PRED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_PC_BRANCH_PRED,\n\t\t\t[C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_PC_BRANCH_MIS_PRED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n\t[C(NODE)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t\t[C(RESULT_MISS)]\t= CACHE_OP_UNSUPPORTED,\n\t\t},\n\t},\n};\n\n/*\n * Perf Events' indices\n */\n#define\tARMV8_IDX_CYCLE_COUNTER\t0\n#define\tARMV8_IDX_COUNTER0\t1\n#define\tARMV8_IDX_COUNTER_LAST\t(ARMV8_IDX_CYCLE_COUNTER + cpu_pmu->num_events - 1)\n\n#define\tARMV8_MAX_COUNTERS\t32\n#define\tARMV8_COUNTER_MASK\t(ARMV8_MAX_COUNTERS - 1)\n\n/*\n * ARMv8 low level PMU access\n */\n\n/*\n * Perf Event to low level counters mapping\n */\n#define\tARMV8_IDX_TO_COUNTER(x)\t\\\n\t(((x) - ARMV8_IDX_COUNTER0) & ARMV8_COUNTER_MASK)\n\n/*\n * Per-CPU PMCR: config reg\n */\n#define ARMV8_PMCR_E\t\t(1 << 0) /* Enable all counters */\n#define ARMV8_PMCR_P\t\t(1 << 1) /* Reset all counters */\n#define ARMV8_PMCR_C\t\t(1 << 2) /* Cycle counter reset */\n#define ARMV8_PMCR_D\t\t(1 << 3) /* CCNT counts every 64th cpu cycle */\n#define ARMV8_PMCR_X\t\t(1 << 4) /* Export to ETM */\n#define ARMV8_PMCR_DP\t\t(1 << 5) /* Disable CCNT if non-invasive debug*/\n#define\tARMV8_PMCR_N_SHIFT\t11\t /* Number of counters supported */\n#define\tARMV8_PMCR_N_MASK\t0x1f\n#define\tARMV8_PMCR_MASK\t\t0x3f\t /* Mask for writable bits */\n\n/*\n * PMOVSR: counters overflow flag status reg\n */\n#define\tARMV8_OVSR_MASK\t\t0xffffffff\t/* Mask for writable bits */\n#define\tARMV8_OVERFLOWED_MASK\tARMV8_OVSR_MASK\n\n/*\n * PMXEVTYPER: Event selection reg\n */\n#define\tARMV8_EVTYPE_MASK\t0xc80003ff\t/* Mask for writable bits */\n#define\tARMV8_EVTYPE_EVENT\t0x3ff\t\t/* Mask for EVENT bits */\n\n/*\n * Event filters for PMUv3\n */\n#define\tARMV8_EXCLUDE_EL1\t(1 << 31)\n#define\tARMV8_EXCLUDE_EL0\t(1 << 30)\n#define\tARMV8_INCLUDE_EL2\t(1 << 27)\n\nstatic inline u32 armv8pmu_pmcr_read(void)\n{\n\tu32 val;\n\tasm volatile(\"mrs %0, pmcr_el0\" : \"=r\" (val));\n\treturn val;\n}\n\nstatic inline void armv8pmu_pmcr_write(u32 val)\n{\n\tval &= ARMV8_PMCR_MASK;\n\tisb();\n\tasm volatile(\"msr pmcr_el0, %0\" :: \"r\" (val));\n}\n\nstatic inline int armv8pmu_has_overflowed(u32 pmovsr)\n{\n\treturn pmovsr & ARMV8_OVERFLOWED_MASK;\n}\n\nstatic inline int armv8pmu_counter_valid(int idx)\n{\n\treturn idx >= ARMV8_IDX_CYCLE_COUNTER && idx <= ARMV8_IDX_COUNTER_LAST;\n}\n\nstatic inline int armv8pmu_counter_has_overflowed(u32 pmnc, int idx)\n{\n\tint ret = 0;\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u checking wrong counter %d overflow status\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t} else {\n\t\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\t\tret = pmnc & BIT(counter);\n\t}\n\n\treturn ret;\n}\n\nstatic inline int armv8pmu_select_counter(int idx)\n{\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u selecting wrong PMNC counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t\treturn -EINVAL;\n\t}\n\n\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\tasm volatile(\"msr pmselr_el0, %0\" :: \"r\" (counter));\n\tisb();\n\n\treturn idx;\n}\n\nstatic inline u32 armv8pmu_read_counter(int idx)\n{\n\tu32 value = 0;\n\n\tif (!armv8pmu_counter_valid(idx))\n\t\tpr_err(\"CPU%u reading wrong counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\telse if (idx == ARMV8_IDX_CYCLE_COUNTER)\n\t\tasm volatile(\"mrs %0, pmccntr_el0\" : \"=r\" (value));\n\telse if (armv8pmu_select_counter(idx) == idx)\n\t\tasm volatile(\"mrs %0, pmxevcntr_el0\" : \"=r\" (value));\n\n\treturn value;\n}\n\nstatic inline void armv8pmu_write_counter(int idx, u32 value)\n{\n\tif (!armv8pmu_counter_valid(idx))\n\t\tpr_err(\"CPU%u writing wrong counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\telse if (idx == ARMV8_IDX_CYCLE_COUNTER)\n\t\tasm volatile(\"msr pmccntr_el0, %0\" :: \"r\" (value));\n\telse if (armv8pmu_select_counter(idx) == idx)\n\t\tasm volatile(\"msr pmxevcntr_el0, %0\" :: \"r\" (value));\n}\n\nstatic inline void armv8pmu_write_evtype(int idx, u32 val)\n{\n\tif (armv8pmu_select_counter(idx) == idx) {\n\t\tval &= ARMV8_EVTYPE_MASK;\n\t\tasm volatile(\"msr pmxevtyper_el0, %0\" :: \"r\" (val));\n\t}\n}\n\nstatic inline int armv8pmu_enable_counter(int idx)\n{\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u enabling wrong PMNC counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t\treturn -EINVAL;\n\t}\n\n\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\tasm volatile(\"msr pmcntenset_el0, %0\" :: \"r\" (BIT(counter)));\n\treturn idx;\n}\n\nstatic inline int armv8pmu_disable_counter(int idx)\n{\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u disabling wrong PMNC counter %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t\treturn -EINVAL;\n\t}\n\n\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\tasm volatile(\"msr pmcntenclr_el0, %0\" :: \"r\" (BIT(counter)));\n\treturn idx;\n}\n\nstatic inline int armv8pmu_enable_intens(int idx)\n{\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u enabling wrong PMNC counter IRQ enable %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t\treturn -EINVAL;\n\t}\n\n\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\tasm volatile(\"msr pmintenset_el1, %0\" :: \"r\" (BIT(counter)));\n\treturn idx;\n}\n\nstatic inline int armv8pmu_disable_intens(int idx)\n{\n\tu32 counter;\n\n\tif (!armv8pmu_counter_valid(idx)) {\n\t\tpr_err(\"CPU%u disabling wrong PMNC counter IRQ enable %d\\n\",\n\t\t\tsmp_processor_id(), idx);\n\t\treturn -EINVAL;\n\t}\n\n\tcounter = ARMV8_IDX_TO_COUNTER(idx);\n\tasm volatile(\"msr pmintenclr_el1, %0\" :: \"r\" (BIT(counter)));\n\tisb();\n\t/* Clear the overflow flag in case an interrupt is pending. */\n\tasm volatile(\"msr pmovsclr_el0, %0\" :: \"r\" (BIT(counter)));\n\tisb();\n\treturn idx;\n}\n\nstatic inline u32 armv8pmu_getreset_flags(void)\n{\n\tu32 value;\n\n\t/* Read */\n\tasm volatile(\"mrs %0, pmovsclr_el0\" : \"=r\" (value));\n\n\t/* Write to clear flags */\n\tvalue &= ARMV8_OVSR_MASK;\n\tasm volatile(\"msr pmovsclr_el0, %0\" :: \"r\" (value));\n\n\treturn value;\n}\n\nstatic void armv8pmu_enable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags;\n\tstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\n\n\t/*\n\t * Enable counter and interrupt, and set the counter to count\n\t * the event that we're interested in.\n\t */\n\traw_spin_lock_irqsave(&events->pmu_lock, flags);\n\n\t/*\n\t * Disable counter\n\t */\n\tarmv8pmu_disable_counter(idx);\n\n\t/*\n\t * Set event (if destined for PMNx counters).\n\t */\n\tarmv8pmu_write_evtype(idx, hwc->config_base);\n\n\t/*\n\t * Enable interrupt for this counter\n\t */\n\tarmv8pmu_enable_intens(idx);\n\n\t/*\n\t * Enable counter\n\t */\n\tarmv8pmu_enable_counter(idx);\n\n\traw_spin_unlock_irqrestore(&events->pmu_lock, flags);\n}\n\nstatic void armv8pmu_disable_event(struct hw_perf_event *hwc, int idx)\n{\n\tunsigned long flags;\n\tstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\n\n\t/*\n\t * Disable counter and interrupt\n\t */\n\traw_spin_lock_irqsave(&events->pmu_lock, flags);\n\n\t/*\n\t * Disable counter\n\t */\n\tarmv8pmu_disable_counter(idx);\n\n\t/*\n\t * Disable interrupt for this counter\n\t */\n\tarmv8pmu_disable_intens(idx);\n\n\traw_spin_unlock_irqrestore(&events->pmu_lock, flags);\n}\n\nstatic irqreturn_t armv8pmu_handle_irq(int irq_num, void *dev)\n{\n\tu32 pmovsr;\n\tstruct perf_sample_data data;\n\tstruct pmu_hw_events *cpuc;\n\tstruct pt_regs *regs;\n\tint idx;\n\n\t/*\n\t * Get and reset the IRQ flags\n\t */\n\tpmovsr = armv8pmu_getreset_flags();\n\n\t/*\n\t * Did an overflow occur?\n\t */\n\tif (!armv8pmu_has_overflowed(pmovsr))\n\t\treturn IRQ_NONE;\n\n\t/*\n\t * Handle the counter(s) overflow(s)\n\t */\n\tregs = get_irq_regs();\n\n\tcpuc = this_cpu_ptr(&cpu_hw_events);\n\tfor (idx = 0; idx < cpu_pmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\t/* Ignore if we don't have an event. */\n\t\tif (!event)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * We have a single interrupt for all counters. Check that\n\t\t * each counter has overflowed before we process it.\n\t\t */\n\t\tif (!armv8pmu_counter_has_overflowed(pmovsr, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event, hwc, idx);\n\t\tperf_sample_data_init(&data, 0, hwc->last_period);\n\t\tif (!armpmu_event_set_period(event, hwc, idx))\n\t\t\tcontinue;\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tcpu_pmu->disable(hwc, idx);\n\t}\n\n\t/*\n\t * Handle the pending perf events.\n\t *\n\t * Note: this call *must* be run with interrupts disabled. For\n\t * platforms that can have the PMU interrupts raised as an NMI, this\n\t * will not work.\n\t */\n\tirq_work_run();\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void armv8pmu_start(void)\n{\n\tunsigned long flags;\n\tstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\n\n\traw_spin_lock_irqsave(&events->pmu_lock, flags);\n\t/* Enable all counters */\n\tarmv8pmu_pmcr_write(armv8pmu_pmcr_read() | ARMV8_PMCR_E);\n\traw_spin_unlock_irqrestore(&events->pmu_lock, flags);\n}\n\nstatic void armv8pmu_stop(void)\n{\n\tunsigned long flags;\n\tstruct pmu_hw_events *events = cpu_pmu->get_hw_events();\n\n\traw_spin_lock_irqsave(&events->pmu_lock, flags);\n\t/* Disable all counters */\n\tarmv8pmu_pmcr_write(armv8pmu_pmcr_read() & ~ARMV8_PMCR_E);\n\traw_spin_unlock_irqrestore(&events->pmu_lock, flags);\n}\n\nstatic int armv8pmu_get_event_idx(struct pmu_hw_events *cpuc,\n\t\t\t\t  struct hw_perf_event *event)\n{\n\tint idx;\n\tunsigned long evtype = event->config_base & ARMV8_EVTYPE_EVENT;\n\n\t/* Always place a cycle counter into the cycle counter. */\n\tif (evtype == ARMV8_PMUV3_PERFCTR_CLOCK_CYCLES) {\n\t\tif (test_and_set_bit(ARMV8_IDX_CYCLE_COUNTER, cpuc->used_mask))\n\t\t\treturn -EAGAIN;\n\n\t\treturn ARMV8_IDX_CYCLE_COUNTER;\n\t}\n\n\t/*\n\t * For anything other than a cycle counter, try and use\n\t * the events counters\n\t */\n\tfor (idx = ARMV8_IDX_COUNTER0; idx < cpu_pmu->num_events; ++idx) {\n\t\tif (!test_and_set_bit(idx, cpuc->used_mask))\n\t\t\treturn idx;\n\t}\n\n\t/* The counters are all in use. */\n\treturn -EAGAIN;\n}\n\n/*\n * Add an event filter to a given event. This will only work for PMUv2 PMUs.\n */\nstatic int armv8pmu_set_event_filter(struct hw_perf_event *event,\n\t\t\t\t     struct perf_event_attr *attr)\n{\n\tunsigned long config_base = 0;\n\n\tif (attr->exclude_idle)\n\t\treturn -EPERM;\n\tif (attr->exclude_user)\n\t\tconfig_base |= ARMV8_EXCLUDE_EL0;\n\tif (attr->exclude_kernel)\n\t\tconfig_base |= ARMV8_EXCLUDE_EL1;\n\tif (!attr->exclude_hv)\n\t\tconfig_base |= ARMV8_INCLUDE_EL2;\n\n\t/*\n\t * Install the filter into config_base as this is used to\n\t * construct the event type.\n\t */\n\tevent->config_base = config_base;\n\n\treturn 0;\n}\n\nstatic void armv8pmu_reset(void *info)\n{\n\tu32 idx, nb_cnt = cpu_pmu->num_events;\n\n\t/* The counter and interrupt enable registers are unknown at reset. */\n\tfor (idx = ARMV8_IDX_CYCLE_COUNTER; idx < nb_cnt; ++idx)\n\t\tarmv8pmu_disable_event(NULL, idx);\n\n\t/* Initialize & Reset PMNC: C and P bits. */\n\tarmv8pmu_pmcr_write(ARMV8_PMCR_P | ARMV8_PMCR_C);\n\n\t/* Disable access from userspace. */\n\tasm volatile(\"msr pmuserenr_el0, %0\" :: \"r\" (0));\n}\n\nstatic int armv8_pmuv3_map_event(struct perf_event *event)\n{\n\treturn map_cpu_event(event, &armv8_pmuv3_perf_map,\n\t\t\t\t&armv8_pmuv3_perf_cache_map,\n\t\t\t\tARMV8_EVTYPE_EVENT);\n}\n\nstatic struct arm_pmu armv8pmu = {\n\t.handle_irq\t\t= armv8pmu_handle_irq,\n\t.enable\t\t\t= armv8pmu_enable_event,\n\t.disable\t\t= armv8pmu_disable_event,\n\t.read_counter\t\t= armv8pmu_read_counter,\n\t.write_counter\t\t= armv8pmu_write_counter,\n\t.get_event_idx\t\t= armv8pmu_get_event_idx,\n\t.start\t\t\t= armv8pmu_start,\n\t.stop\t\t\t= armv8pmu_stop,\n\t.reset\t\t\t= armv8pmu_reset,\n\t.max_period\t\t= (1LLU << 32) - 1,\n};\n\nstatic u32 __init armv8pmu_read_num_pmnc_events(void)\n{\n\tu32 nb_cnt;\n\n\t/* Read the nb of CNTx counters supported from PMNC */\n\tnb_cnt = (armv8pmu_pmcr_read() >> ARMV8_PMCR_N_SHIFT) & ARMV8_PMCR_N_MASK;\n\n\t/* Add the CPU cycles counter and return */\n\treturn nb_cnt + 1;\n}\n\nstatic struct arm_pmu *__init armv8_pmuv3_pmu_init(void)\n{\n\tarmv8pmu.name\t\t\t= \"arm/armv8-pmuv3\";\n\tarmv8pmu.map_event\t\t= armv8_pmuv3_map_event;\n\tarmv8pmu.num_events\t\t= armv8pmu_read_num_pmnc_events();\n\tarmv8pmu.set_event_filter\t= armv8pmu_set_event_filter;\n\treturn &armv8pmu;\n}\n\n/*\n * Ensure the PMU has sane values out of reset.\n * This requires SMP to be available, so exists as a separate initcall.\n */\nstatic int __init\ncpu_pmu_reset(void)\n{\n\tif (cpu_pmu && cpu_pmu->reset)\n\t\treturn on_each_cpu(cpu_pmu->reset, NULL, 1);\n\treturn 0;\n}\narch_initcall(cpu_pmu_reset);\n\n/*\n * PMU platform driver and devicetree bindings.\n */\nstatic const struct of_device_id armpmu_of_device_ids[] = {\n\t{.compatible = \"arm,armv8-pmuv3\"},\n\t{},\n};\n\nstatic int armpmu_device_probe(struct platform_device *pdev)\n{\n\tif (!cpu_pmu)\n\t\treturn -ENODEV;\n\n\tcpu_pmu->plat_device = pdev;\n\treturn 0;\n}\n\nstatic struct platform_driver armpmu_driver = {\n\t.driver\t\t= {\n\t\t.name\t= \"arm-pmu\",\n\t\t.of_match_table = armpmu_of_device_ids,\n\t},\n\t.probe\t\t= armpmu_device_probe,\n};\n\nstatic int __init register_pmu_driver(void)\n{\n\treturn platform_driver_register(&armpmu_driver);\n}\ndevice_initcall(register_pmu_driver);\n\nstatic struct pmu_hw_events *armpmu_get_cpu_events(void)\n{\n\treturn this_cpu_ptr(&cpu_hw_events);\n}\n\nstatic void __init cpu_pmu_init(struct arm_pmu *armpmu)\n{\n\tint cpu;\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct pmu_hw_events *events = &per_cpu(cpu_hw_events, cpu);\n\t\tevents->events = per_cpu(hw_events, cpu);\n\t\tevents->used_mask = per_cpu(used_mask, cpu);\n\t\traw_spin_lock_init(&events->pmu_lock);\n\t}\n\tarmpmu->get_hw_events = armpmu_get_cpu_events;\n}\n\nstatic int __init init_hw_perf_events(void)\n{\n\tu64 dfr = read_cpuid(ID_AA64DFR0_EL1);\n\n\tswitch ((dfr >> 8) & 0xf) {\n\tcase 0x1:\t/* PMUv3 */\n\t\tcpu_pmu = armv8_pmuv3_pmu_init();\n\t\tbreak;\n\t}\n\n\tif (cpu_pmu) {\n\t\tpr_info(\"enabled with %s PMU driver, %d counters available\\n\",\n\t\t\tcpu_pmu->name, cpu_pmu->num_events);\n\t\tcpu_pmu_init(cpu_pmu);\n\t\tarmpmu_register(cpu_pmu, \"cpu\", PERF_TYPE_RAW);\n\t} else {\n\t\tpr_info(\"no hardware support available\\n\");\n\t}\n\n\treturn 0;\n}\nearly_initcall(init_hw_perf_events);\n\n/*\n * Callchain handling code.\n */\nstruct frame_tail {\n\tstruct frame_tail\t__user *fp;\n\tunsigned long\t\tlr;\n} __attribute__((packed));\n\n/*\n * Get the return address for a single stackframe and return a pointer to the\n * next frame tail.\n */\nstatic struct frame_tail __user *\nuser_backtrace(struct frame_tail __user *tail,\n\t       struct perf_callchain_entry *entry)\n{\n\tstruct frame_tail buftail;\n\tunsigned long err;\n\n\t/* Also check accessibility of one struct frame_tail beyond */\n\tif (!access_ok(VERIFY_READ, tail, sizeof(buftail)))\n\t\treturn NULL;\n\n\tpagefault_disable();\n\terr = __copy_from_user_inatomic(&buftail, tail, sizeof(buftail));\n\tpagefault_enable();\n\n\tif (err)\n\t\treturn NULL;\n\n\tperf_callchain_store(entry, buftail.lr);\n\n\t/*\n\t * Frame pointers should strictly progress back up the stack\n\t * (towards higher addresses).\n\t */\n\tif (tail >= buftail.fp)\n\t\treturn NULL;\n\n\treturn buftail.fp;\n}\n\n#ifdef CONFIG_COMPAT\n/*\n * The registers we're interested in are at the end of the variable\n * length saved register structure. The fp points at the end of this\n * structure so the address of this struct is:\n * (struct compat_frame_tail *)(xxx->fp)-1\n *\n * This code has been adapted from the ARM OProfile support.\n */\nstruct compat_frame_tail {\n\tcompat_uptr_t\tfp; /* a (struct compat_frame_tail *) in compat mode */\n\tu32\t\tsp;\n\tu32\t\tlr;\n} __attribute__((packed));\n\nstatic struct compat_frame_tail __user *\ncompat_user_backtrace(struct compat_frame_tail __user *tail,\n\t\t      struct perf_callchain_entry *entry)\n{\n\tstruct compat_frame_tail buftail;\n\tunsigned long err;\n\n\t/* Also check accessibility of one struct frame_tail beyond */\n\tif (!access_ok(VERIFY_READ, tail, sizeof(buftail)))\n\t\treturn NULL;\n\n\tpagefault_disable();\n\terr = __copy_from_user_inatomic(&buftail, tail, sizeof(buftail));\n\tpagefault_enable();\n\n\tif (err)\n\t\treturn NULL;\n\n\tperf_callchain_store(entry, buftail.lr);\n\n\t/*\n\t * Frame pointers should strictly progress back up the stack\n\t * (towards higher addresses).\n\t */\n\tif (tail + 1 >= (struct compat_frame_tail __user *)\n\t\t\tcompat_ptr(buftail.fp))\n\t\treturn NULL;\n\n\treturn (struct compat_frame_tail __user *)compat_ptr(buftail.fp) - 1;\n}\n#endif /* CONFIG_COMPAT */\n\nvoid perf_callchain_user(struct perf_callchain_entry *entry,\n\t\t\t struct pt_regs *regs)\n{\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tperf_callchain_store(entry, regs->pc);\n\n\tif (!compat_user_mode(regs)) {\n\t\t/* AARCH64 mode */\n\t\tstruct frame_tail __user *tail;\n\n\t\ttail = (struct frame_tail __user *)regs->regs[29];\n\n\t\twhile (entry->nr < PERF_MAX_STACK_DEPTH &&\n\t\t       tail && !((unsigned long)tail & 0xf))\n\t\t\ttail = user_backtrace(tail, entry);\n\t} else {\n#ifdef CONFIG_COMPAT\n\t\t/* AARCH32 compat mode */\n\t\tstruct compat_frame_tail __user *tail;\n\n\t\ttail = (struct compat_frame_tail __user *)regs->compat_fp - 1;\n\n\t\twhile ((entry->nr < PERF_MAX_STACK_DEPTH) &&\n\t\t\ttail && !((unsigned long)tail & 0x3))\n\t\t\ttail = compat_user_backtrace(tail, entry);\n#endif\n\t}\n}\n\n/*\n * Gets called by walk_stackframe() for every stackframe. This will be called\n * whist unwinding the stackframe and is like a subroutine return so we use\n * the PC.\n */\nstatic int callchain_trace(struct stackframe *frame, void *data)\n{\n\tstruct perf_callchain_entry *entry = data;\n\tperf_callchain_store(entry, frame->pc);\n\treturn 0;\n}\n\nvoid perf_callchain_kernel(struct perf_callchain_entry *entry,\n\t\t\t   struct pt_regs *regs)\n{\n\tstruct stackframe frame;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tframe.fp = regs->regs[29];\n\tframe.sp = regs->sp;\n\tframe.pc = regs->pc;\n\n\twalk_stackframe(&frame, callchain_trace, entry);\n}\n\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest())\n\t\treturn perf_guest_cbs->get_guest_ip();\n\n\treturn instruction_pointer(regs);\n}\n\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tint misc = 0;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\tif (perf_guest_cbs->is_user_mode())\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_KERNEL;\n\t} else {\n\t\tif (user_mode(regs))\n\t\t\tmisc |= PERF_RECORD_MISC_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_KERNEL;\n\t}\n\n\treturn misc;\n}\n"], "filenames": ["arch/arm64/kernel/perf_event.c"], "buggy_code_start_loc": [325], "buggy_code_end_loc": [367], "fixing_code_start_loc": [325], "fixing_code_end_loc": [376], "type": "CWE-264", "message": "arch/arm64/kernel/perf_event.c in the Linux kernel before 4.1 on arm64 platforms allows local users to gain privileges or cause a denial of service (invalid pointer dereference) via vectors involving events that are mishandled during a span of multiple HW PMUs.", "other": {"cve": {"id": "CVE-2015-8955", "sourceIdentifier": "security@android.com", "published": "2016-10-10T10:59:03.323", "lastModified": "2016-11-28T19:50:49.177", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "arch/arm64/kernel/perf_event.c in the Linux kernel before 4.1 on arm64 platforms allows local users to gain privileges or cause a denial of service (invalid pointer dereference) via vectors involving events that are mishandled during a span of multiple HW PMUs."}, {"lang": "es", "value": "arch/arm64/kernel/perf_event.c en el kernel de Linux en versiones anteriores a 4.1 en plataformas arm64 permite a usuarios locales obtener privilegios o provocar una denegaci\u00f3n de servicio (puntero de referencia no valido) a trav\u00e9s de vectores relacionados con eventos que son manejados incorrectamente durante un lapso de m\u00faltiples HW PMUs."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:R/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.3, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.3, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-264"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.0.9", "matchCriteriaId": "EFF016D5-BD0A-4CC2-BDFC-FF4F0A17A957"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:google:android:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "09E6085C-A61E-4A89-BF80-EDD9A7DF1E47"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=8fff105e13041e49b82f92eef034f363a6b1c071", "source": "security@android.com", "tags": ["Issue Tracking", "Patch"]}, {"url": "http://source.android.com/security/bulletin/2016-10-01.html", "source": "security@android.com", "tags": ["Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/93314", "source": "security@android.com"}, {"url": "https://github.com/torvalds/linux/commit/8fff105e13041e49b82f92eef034f363a6b1c071", "source": "security@android.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/8fff105e13041e49b82f92eef034f363a6b1c071"}}