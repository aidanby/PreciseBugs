{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n * net/sched/sch_qfq.c         Quick Fair Queueing Plus Scheduler.\n *\n * Copyright (c) 2009 Fabio Checconi, Luigi Rizzo, and Paolo Valente.\n * Copyright (c) 2012 Paolo Valente.\n */\n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/bitops.h>\n#include <linux/errno.h>\n#include <linux/netdevice.h>\n#include <linux/pkt_sched.h>\n#include <net/sch_generic.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n\n\n/*  Quick Fair Queueing Plus\n    ========================\n\n    Sources:\n\n    [1] Paolo Valente,\n    \"Reducing the Execution Time of Fair-Queueing Schedulers.\"\n    http://algo.ing.unimo.it/people/paolo/agg-sched/agg-sched.pdf\n\n    Sources for QFQ:\n\n    [2] Fabio Checconi, Luigi Rizzo, and Paolo Valente: \"QFQ: Efficient\n    Packet Scheduling with Tight Bandwidth Distribution Guarantees.\"\n\n    See also:\n    http://retis.sssup.it/~fabio/linux/qfq/\n */\n\n/*\n\n  QFQ+ divides classes into aggregates of at most MAX_AGG_CLASSES\n  classes. Each aggregate is timestamped with a virtual start time S\n  and a virtual finish time F, and scheduled according to its\n  timestamps. S and F are computed as a function of a system virtual\n  time function V. The classes within each aggregate are instead\n  scheduled with DRR.\n\n  To speed up operations, QFQ+ divides also aggregates into a limited\n  number of groups. Which group a class belongs to depends on the\n  ratio between the maximum packet length for the class and the weight\n  of the class. Groups have their own S and F. In the end, QFQ+\n  schedules groups, then aggregates within groups, then classes within\n  aggregates. See [1] and [2] for a full description.\n\n  Virtual time computations.\n\n  S, F and V are all computed in fixed point arithmetic with\n  FRAC_BITS decimal bits.\n\n  QFQ_MAX_INDEX is the maximum index allowed for a group. We need\n\tone bit per index.\n  QFQ_MAX_WSHIFT is the maximum power of two supported as a weight.\n\n  The layout of the bits is as below:\n\n                   [ MTU_SHIFT ][      FRAC_BITS    ]\n                   [ MAX_INDEX    ][ MIN_SLOT_SHIFT ]\n\t\t\t\t ^.__grp->index = 0\n\t\t\t\t *.__grp->slot_shift\n\n  where MIN_SLOT_SHIFT is derived by difference from the others.\n\n  The max group index corresponds to Lmax/w_min, where\n  Lmax=1<<MTU_SHIFT, w_min = 1 .\n  From this, and knowing how many groups (MAX_INDEX) we want,\n  we can derive the shift corresponding to each group.\n\n  Because we often need to compute\n\tF = S + len/w_i  and V = V + len/wsum\n  instead of storing w_i store the value\n\tinv_w = (1<<FRAC_BITS)/w_i\n  so we can do F = S + len * inv_w * wsum.\n  We use W_TOT in the formulas so we can easily move between\n  static and adaptive weight sum.\n\n  The per-scheduler-instance data contain all the data structures\n  for the scheduler: bitmaps and bucket lists.\n\n */\n\n/*\n * Maximum number of consecutive slots occupied by backlogged classes\n * inside a group.\n */\n#define QFQ_MAX_SLOTS\t32\n\n/*\n * Shifts used for aggregate<->group mapping.  We allow class weights that are\n * in the range [1, 2^MAX_WSHIFT], and we try to map each aggregate i to the\n * group with the smallest index that can support the L_i / r_i configured\n * for the classes in the aggregate.\n *\n * grp->index is the index of the group; and grp->slot_shift\n * is the shift for the corresponding (scaled) sigma_i.\n */\n#define QFQ_MAX_INDEX\t\t24\n#define QFQ_MAX_WSHIFT\t\t10\n\n#define\tQFQ_MAX_WEIGHT\t\t(1<<QFQ_MAX_WSHIFT) /* see qfq_slot_insert */\n#define QFQ_MAX_WSUM\t\t(64*QFQ_MAX_WEIGHT)\n\n#define FRAC_BITS\t\t30\t/* fixed point arithmetic */\n#define ONE_FP\t\t\t(1UL << FRAC_BITS)\n\n#define QFQ_MTU_SHIFT\t\t16\t/* to support TSO/GSO */\n#define QFQ_MIN_LMAX\t\t512\t/* see qfq_slot_insert */\n\n#define QFQ_MAX_AGG_CLASSES\t8 /* max num classes per aggregate allowed */\n\n/*\n * Possible group states.  These values are used as indexes for the bitmaps\n * array of struct qfq_queue.\n */\nenum qfq_state { ER, IR, EB, IB, QFQ_MAX_STATE };\n\nstruct qfq_group;\n\nstruct qfq_aggregate;\n\nstruct qfq_class {\n\tstruct Qdisc_class_common common;\n\n\tunsigned int filter_cnt;\n\n\tstruct gnet_stats_basic_sync bstats;\n\tstruct gnet_stats_queue qstats;\n\tstruct net_rate_estimator __rcu *rate_est;\n\tstruct Qdisc *qdisc;\n\tstruct list_head alist;\t\t/* Link for active-classes list. */\n\tstruct qfq_aggregate *agg;\t/* Parent aggregate. */\n\tint deficit;\t\t\t/* DRR deficit counter. */\n};\n\nstruct qfq_aggregate {\n\tstruct hlist_node next;\t/* Link for the slot list. */\n\tu64 S, F;\t\t/* flow timestamps (exact) */\n\n\t/* group we belong to. In principle we would need the index,\n\t * which is log_2(lmax/weight), but we never reference it\n\t * directly, only the group.\n\t */\n\tstruct qfq_group *grp;\n\n\t/* these are copied from the flowset. */\n\tu32\tclass_weight; /* Weight of each class in this aggregate. */\n\t/* Max pkt size for the classes in this aggregate, DRR quantum. */\n\tint\tlmax;\n\n\tu32\tinv_w;\t    /* ONE_FP/(sum of weights of classes in aggr.). */\n\tu32\tbudgetmax;  /* Max budget for this aggregate. */\n\tu32\tinitial_budget, budget;     /* Initial and current budget. */\n\n\tint\t\t  num_classes;\t/* Number of classes in this aggr. */\n\tstruct list_head  active;\t/* DRR queue of active classes. */\n\n\tstruct hlist_node nonfull_next;\t/* See nonfull_aggs in qfq_sched. */\n};\n\nstruct qfq_group {\n\tu64 S, F;\t\t\t/* group timestamps (approx). */\n\tunsigned int slot_shift;\t/* Slot shift. */\n\tunsigned int index;\t\t/* Group index. */\n\tunsigned int front;\t\t/* Index of the front slot. */\n\tunsigned long full_slots;\t/* non-empty slots */\n\n\t/* Array of RR lists of active aggregates. */\n\tstruct hlist_head slots[QFQ_MAX_SLOTS];\n};\n\nstruct qfq_sched {\n\tstruct tcf_proto __rcu *filter_list;\n\tstruct tcf_block\t*block;\n\tstruct Qdisc_class_hash clhash;\n\n\tu64\t\t\toldV, V;\t/* Precise virtual times. */\n\tstruct qfq_aggregate\t*in_serv_agg;   /* Aggregate being served. */\n\tu32\t\t\twsum;\t\t/* weight sum */\n\tu32\t\t\tiwsum;\t\t/* inverse weight sum */\n\n\tunsigned long bitmaps[QFQ_MAX_STATE];\t    /* Group bitmaps. */\n\tstruct qfq_group groups[QFQ_MAX_INDEX + 1]; /* The groups. */\n\tu32 min_slot_shift;\t/* Index of the group-0 bit in the bitmaps. */\n\n\tu32 max_agg_classes;\t\t/* Max number of classes per aggr. */\n\tstruct hlist_head nonfull_aggs; /* Aggs with room for more classes. */\n};\n\n/*\n * Possible reasons why the timestamps of an aggregate are updated\n * enqueue: the aggregate switches from idle to active and must scheduled\n *\t    for service\n * requeue: the aggregate finishes its budget, so it stops being served and\n *\t    must be rescheduled for service\n */\nenum update_reason {enqueue, requeue};\n\nstatic struct qfq_class *qfq_find_class(struct Qdisc *sch, u32 classid)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct Qdisc_class_common *clc;\n\n\tclc = qdisc_class_find(&q->clhash, classid);\n\tif (clc == NULL)\n\t\treturn NULL;\n\treturn container_of(clc, struct qfq_class, common);\n}\n\nstatic const struct nla_policy qfq_policy[TCA_QFQ_MAX + 1] = {\n\t[TCA_QFQ_WEIGHT] = { .type = NLA_U32 },\n\t[TCA_QFQ_LMAX] = { .type = NLA_U32 },\n};\n\n/*\n * Calculate a flow index, given its weight and maximum packet length.\n * index = log_2(maxlen/weight) but we need to apply the scaling.\n * This is used only once at flow creation.\n */\nstatic int qfq_calc_index(u32 inv_w, unsigned int maxlen, u32 min_slot_shift)\n{\n\tu64 slot_size = (u64)maxlen * inv_w;\n\tunsigned long size_map;\n\tint index = 0;\n\n\tsize_map = slot_size >> min_slot_shift;\n\tif (!size_map)\n\t\tgoto out;\n\n\tindex = __fls(size_map) + 1;\t/* basically a log_2 */\n\tindex -= !(slot_size - (1ULL << (index + min_slot_shift - 1)));\n\n\tif (index < 0)\n\t\tindex = 0;\nout:\n\tpr_debug(\"qfq calc_index: W = %lu, L = %u, I = %d\\n\",\n\t\t (unsigned long) ONE_FP/inv_w, maxlen, index);\n\n\treturn index;\n}\n\nstatic void qfq_deactivate_agg(struct qfq_sched *, struct qfq_aggregate *);\nstatic void qfq_activate_agg(struct qfq_sched *, struct qfq_aggregate *,\n\t\t\t     enum update_reason);\n\nstatic void qfq_init_agg(struct qfq_sched *q, struct qfq_aggregate *agg,\n\t\t\t u32 lmax, u32 weight)\n{\n\tINIT_LIST_HEAD(&agg->active);\n\thlist_add_head(&agg->nonfull_next, &q->nonfull_aggs);\n\n\tagg->lmax = lmax;\n\tagg->class_weight = weight;\n}\n\nstatic struct qfq_aggregate *qfq_find_agg(struct qfq_sched *q,\n\t\t\t\t\t  u32 lmax, u32 weight)\n{\n\tstruct qfq_aggregate *agg;\n\n\thlist_for_each_entry(agg, &q->nonfull_aggs, nonfull_next)\n\t\tif (agg->lmax == lmax && agg->class_weight == weight)\n\t\t\treturn agg;\n\n\treturn NULL;\n}\n\n\n/* Update aggregate as a function of the new number of classes. */\nstatic void qfq_update_agg(struct qfq_sched *q, struct qfq_aggregate *agg,\n\t\t\t   int new_num_classes)\n{\n\tu32 new_agg_weight;\n\n\tif (new_num_classes == q->max_agg_classes)\n\t\thlist_del_init(&agg->nonfull_next);\n\n\tif (agg->num_classes > new_num_classes &&\n\t    new_num_classes == q->max_agg_classes - 1) /* agg no more full */\n\t\thlist_add_head(&agg->nonfull_next, &q->nonfull_aggs);\n\n\t/* The next assignment may let\n\t * agg->initial_budget > agg->budgetmax\n\t * hold, we will take it into account in charge_actual_service().\n\t */\n\tagg->budgetmax = new_num_classes * agg->lmax;\n\tnew_agg_weight = agg->class_weight * new_num_classes;\n\tagg->inv_w = ONE_FP/new_agg_weight;\n\n\tif (agg->grp == NULL) {\n\t\tint i = qfq_calc_index(agg->inv_w, agg->budgetmax,\n\t\t\t\t       q->min_slot_shift);\n\t\tagg->grp = &q->groups[i];\n\t}\n\n\tq->wsum +=\n\t\t(int) agg->class_weight * (new_num_classes - agg->num_classes);\n\tq->iwsum = ONE_FP / q->wsum;\n\n\tagg->num_classes = new_num_classes;\n}\n\n/* Add class to aggregate. */\nstatic void qfq_add_to_agg(struct qfq_sched *q,\n\t\t\t   struct qfq_aggregate *agg,\n\t\t\t   struct qfq_class *cl)\n{\n\tcl->agg = agg;\n\n\tqfq_update_agg(q, agg, agg->num_classes+1);\n\tif (cl->qdisc->q.qlen > 0) { /* adding an active class */\n\t\tlist_add_tail(&cl->alist, &agg->active);\n\t\tif (list_first_entry(&agg->active, struct qfq_class, alist) ==\n\t\t    cl && q->in_serv_agg != agg) /* agg was inactive */\n\t\t\tqfq_activate_agg(q, agg, enqueue); /* schedule agg */\n\t}\n}\n\nstatic struct qfq_aggregate *qfq_choose_next_agg(struct qfq_sched *);\n\nstatic void qfq_destroy_agg(struct qfq_sched *q, struct qfq_aggregate *agg)\n{\n\thlist_del_init(&agg->nonfull_next);\n\tq->wsum -= agg->class_weight;\n\tif (q->wsum != 0)\n\t\tq->iwsum = ONE_FP / q->wsum;\n\n\tif (q->in_serv_agg == agg)\n\t\tq->in_serv_agg = qfq_choose_next_agg(q);\n\tkfree(agg);\n}\n\n/* Deschedule class from within its parent aggregate. */\nstatic void qfq_deactivate_class(struct qfq_sched *q, struct qfq_class *cl)\n{\n\tstruct qfq_aggregate *agg = cl->agg;\n\n\n\tlist_del(&cl->alist); /* remove from RR queue of the aggregate */\n\tif (list_empty(&agg->active)) /* agg is now inactive */\n\t\tqfq_deactivate_agg(q, agg);\n}\n\n/* Remove class from its parent aggregate. */\nstatic void qfq_rm_from_agg(struct qfq_sched *q, struct qfq_class *cl)\n{\n\tstruct qfq_aggregate *agg = cl->agg;\n\n\tcl->agg = NULL;\n\tif (agg->num_classes == 1) { /* agg being emptied, destroy it */\n\t\tqfq_destroy_agg(q, agg);\n\t\treturn;\n\t}\n\tqfq_update_agg(q, agg, agg->num_classes-1);\n}\n\n/* Deschedule class and remove it from its parent aggregate. */\nstatic void qfq_deact_rm_from_agg(struct qfq_sched *q, struct qfq_class *cl)\n{\n\tif (cl->qdisc->q.qlen > 0) /* class is active */\n\t\tqfq_deactivate_class(q, cl);\n\n\tqfq_rm_from_agg(q, cl);\n}\n\n/* Move class to a new aggregate, matching the new class weight and/or lmax */\nstatic int qfq_change_agg(struct Qdisc *sch, struct qfq_class *cl, u32 weight,\n\t\t\t   u32 lmax)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_aggregate *new_agg = qfq_find_agg(q, lmax, weight);\n\n\tif (new_agg == NULL) { /* create new aggregate */\n\t\tnew_agg = kzalloc(sizeof(*new_agg), GFP_ATOMIC);\n\t\tif (new_agg == NULL)\n\t\t\treturn -ENOBUFS;\n\t\tqfq_init_agg(q, new_agg, lmax, weight);\n\t}\n\tqfq_deact_rm_from_agg(q, cl);\n\tqfq_add_to_agg(q, new_agg, cl);\n\n\treturn 0;\n}\n\nstatic int qfq_change_class(struct Qdisc *sch, u32 classid, u32 parentid,\n\t\t\t    struct nlattr **tca, unsigned long *arg,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl = (struct qfq_class *)*arg;\n\tbool existing = false;\n\tstruct nlattr *tb[TCA_QFQ_MAX + 1];\n\tstruct qfq_aggregate *new_agg = NULL;\n\tu32 weight, lmax, inv_w;\n\tint err;\n\tint delta_w;\n\n\tif (tca[TCA_OPTIONS] == NULL) {\n\t\tpr_notice(\"qfq: no options\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = nla_parse_nested_deprecated(tb, TCA_QFQ_MAX, tca[TCA_OPTIONS],\n\t\t\t\t\t  qfq_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_QFQ_WEIGHT]) {\n\t\tweight = nla_get_u32(tb[TCA_QFQ_WEIGHT]);\n\t\tif (!weight || weight > (1UL << QFQ_MAX_WSHIFT)) {\n\t\t\tpr_notice(\"qfq: invalid weight %u\\n\", weight);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else\n\t\tweight = 1;\n\n\tif (tb[TCA_QFQ_LMAX]) {\n\t\tlmax = nla_get_u32(tb[TCA_QFQ_LMAX]);\n\t\tif (lmax < QFQ_MIN_LMAX || lmax > (1UL << QFQ_MTU_SHIFT)) {\n\t\t\tpr_notice(\"qfq: invalid max length %u\\n\", lmax);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else\n\t\tlmax = psched_mtu(qdisc_dev(sch));\n\n\tinv_w = ONE_FP / weight;\n\tweight = ONE_FP / inv_w;\n\n\tif (cl != NULL &&\n\t    lmax == cl->agg->lmax &&\n\t    weight == cl->agg->class_weight)\n\t\treturn 0; /* nothing to change */\n\n\tdelta_w = weight - (cl ? cl->agg->class_weight : 0);\n\n\tif (q->wsum + delta_w > QFQ_MAX_WSUM) {\n\t\tpr_notice(\"qfq: total weight out of range (%d + %u)\\n\",\n\t\t\t  delta_w, q->wsum);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cl != NULL) { /* modify existing class */\n\t\tif (tca[TCA_RATE]) {\n\t\t\terr = gen_replace_estimator(&cl->bstats, NULL,\n\t\t\t\t\t\t    &cl->rate_est,\n\t\t\t\t\t\t    NULL,\n\t\t\t\t\t\t    true,\n\t\t\t\t\t\t    tca[TCA_RATE]);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\texisting = true;\n\t\tgoto set_change_agg;\n\t}\n\n\t/* create and init new class */\n\tcl = kzalloc(sizeof(struct qfq_class), GFP_KERNEL);\n\tif (cl == NULL)\n\t\treturn -ENOBUFS;\n\n\tgnet_stats_basic_sync_init(&cl->bstats);\n\tcl->common.classid = classid;\n\tcl->deficit = lmax;\n\n\tcl->qdisc = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t      classid, NULL);\n\tif (cl->qdisc == NULL)\n\t\tcl->qdisc = &noop_qdisc;\n\n\tif (tca[TCA_RATE]) {\n\t\terr = gen_new_estimator(&cl->bstats, NULL,\n\t\t\t\t\t&cl->rate_est,\n\t\t\t\t\tNULL,\n\t\t\t\t\ttrue,\n\t\t\t\t\ttca[TCA_RATE]);\n\t\tif (err)\n\t\t\tgoto destroy_class;\n\t}\n\n\tif (cl->qdisc != &noop_qdisc)\n\t\tqdisc_hash_add(cl->qdisc, true);\n\nset_change_agg:\n\tsch_tree_lock(sch);\n\tnew_agg = qfq_find_agg(q, lmax, weight);\n\tif (new_agg == NULL) { /* create new aggregate */\n\t\tsch_tree_unlock(sch);\n\t\tnew_agg = kzalloc(sizeof(*new_agg), GFP_KERNEL);\n\t\tif (new_agg == NULL) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgen_kill_estimator(&cl->rate_est);\n\t\t\tgoto destroy_class;\n\t\t}\n\t\tsch_tree_lock(sch);\n\t\tqfq_init_agg(q, new_agg, lmax, weight);\n\t}\n\tif (existing)\n\t\tqfq_deact_rm_from_agg(q, cl);\n\telse\n\t\tqdisc_class_hash_insert(&q->clhash, &cl->common);\n\tqfq_add_to_agg(q, new_agg, cl);\n\tsch_tree_unlock(sch);\n\tqdisc_class_hash_grow(sch, &q->clhash);\n\n\t*arg = (unsigned long)cl;\n\treturn 0;\n\ndestroy_class:\n\tqdisc_put(cl->qdisc);\n\tkfree(cl);\n\treturn err;\n}\n\nstatic void qfq_destroy_class(struct Qdisc *sch, struct qfq_class *cl)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\n\tqfq_rm_from_agg(q, cl);\n\tgen_kill_estimator(&cl->rate_est);\n\tqdisc_put(cl->qdisc);\n\tkfree(cl);\n}\n\nstatic int qfq_delete_class(struct Qdisc *sch, unsigned long arg,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\n\tif (cl->filter_cnt > 0)\n\t\treturn -EBUSY;\n\n\tsch_tree_lock(sch);\n\n\tqdisc_purge_queue(cl->qdisc);\n\tqdisc_class_hash_remove(&q->clhash, &cl->common);\n\n\tsch_tree_unlock(sch);\n\n\tqfq_destroy_class(sch, cl);\n\treturn 0;\n}\n\nstatic unsigned long qfq_search_class(struct Qdisc *sch, u32 classid)\n{\n\treturn (unsigned long)qfq_find_class(sch, classid);\n}\n\nstatic struct tcf_block *qfq_tcf_block(struct Qdisc *sch, unsigned long cl,\n\t\t\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\n\tif (cl)\n\t\treturn NULL;\n\n\treturn q->block;\n}\n\nstatic unsigned long qfq_bind_tcf(struct Qdisc *sch, unsigned long parent,\n\t\t\t\t  u32 classid)\n{\n\tstruct qfq_class *cl = qfq_find_class(sch, classid);\n\n\tif (cl != NULL)\n\t\tcl->filter_cnt++;\n\n\treturn (unsigned long)cl;\n}\n\nstatic void qfq_unbind_tcf(struct Qdisc *sch, unsigned long arg)\n{\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\n\tcl->filter_cnt--;\n}\n\nstatic int qfq_graft_class(struct Qdisc *sch, unsigned long arg,\n\t\t\t   struct Qdisc *new, struct Qdisc **old,\n\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\n\tif (new == NULL) {\n\t\tnew = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t\tcl->common.classid, NULL);\n\t\tif (new == NULL)\n\t\t\tnew = &noop_qdisc;\n\t}\n\n\t*old = qdisc_replace(sch, new, &cl->qdisc);\n\treturn 0;\n}\n\nstatic struct Qdisc *qfq_class_leaf(struct Qdisc *sch, unsigned long arg)\n{\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\n\treturn cl->qdisc;\n}\n\nstatic int qfq_dump_class(struct Qdisc *sch, unsigned long arg,\n\t\t\t  struct sk_buff *skb, struct tcmsg *tcm)\n{\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\tstruct nlattr *nest;\n\n\ttcm->tcm_parent\t= TC_H_ROOT;\n\ttcm->tcm_handle\t= cl->common.classid;\n\ttcm->tcm_info\t= cl->qdisc->handle;\n\n\tnest = nla_nest_start_noflag(skb, TCA_OPTIONS);\n\tif (nest == NULL)\n\t\tgoto nla_put_failure;\n\tif (nla_put_u32(skb, TCA_QFQ_WEIGHT, cl->agg->class_weight) ||\n\t    nla_put_u32(skb, TCA_QFQ_LMAX, cl->agg->lmax))\n\t\tgoto nla_put_failure;\n\treturn nla_nest_end(skb, nest);\n\nnla_put_failure:\n\tnla_nest_cancel(skb, nest);\n\treturn -EMSGSIZE;\n}\n\nstatic int qfq_dump_class_stats(struct Qdisc *sch, unsigned long arg,\n\t\t\t\tstruct gnet_dump *d)\n{\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\tstruct tc_qfq_stats xstats;\n\n\tmemset(&xstats, 0, sizeof(xstats));\n\n\txstats.weight = cl->agg->class_weight;\n\txstats.lmax = cl->agg->lmax;\n\n\tif (gnet_stats_copy_basic(d, NULL, &cl->bstats, true) < 0 ||\n\t    gnet_stats_copy_rate_est(d, &cl->rate_est) < 0 ||\n\t    qdisc_qstats_copy(d, cl->qdisc) < 0)\n\t\treturn -1;\n\n\treturn gnet_stats_copy_app(d, &xstats, sizeof(xstats));\n}\n\nstatic void qfq_walk(struct Qdisc *sch, struct qdisc_walker *arg)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl;\n\tunsigned int i;\n\n\tif (arg->stop)\n\t\treturn;\n\n\tfor (i = 0; i < q->clhash.hashsize; i++) {\n\t\thlist_for_each_entry(cl, &q->clhash.hash[i], common.hnode) {\n\t\t\tif (!tc_qdisc_stats_dump(sch, (unsigned long)cl, arg))\n\t\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic struct qfq_class *qfq_classify(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t\t      int *qerr)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl;\n\tstruct tcf_result res;\n\tstruct tcf_proto *fl;\n\tint result;\n\n\tif (TC_H_MAJ(skb->priority ^ sch->handle) == 0) {\n\t\tpr_debug(\"qfq_classify: found %d\\n\", skb->priority);\n\t\tcl = qfq_find_class(sch, skb->priority);\n\t\tif (cl != NULL)\n\t\t\treturn cl;\n\t}\n\n\t*qerr = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\tfl = rcu_dereference_bh(q->filter_list);\n\tresult = tcf_classify(skb, NULL, fl, &res, false);\n\tif (result >= 0) {\n#ifdef CONFIG_NET_CLS_ACT\n\t\tswitch (result) {\n\t\tcase TC_ACT_QUEUED:\n\t\tcase TC_ACT_STOLEN:\n\t\tcase TC_ACT_TRAP:\n\t\t\t*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\n\t\t\tfallthrough;\n\t\tcase TC_ACT_SHOT:\n\t\t\treturn NULL;\n\t\t}\n#endif\n\t\tcl = (struct qfq_class *)res.class;\n\t\tif (cl == NULL)\n\t\t\tcl = qfq_find_class(sch, res.classid);\n\t\treturn cl;\n\t}\n\n\treturn NULL;\n}\n\n/* Generic comparison function, handling wraparound. */\nstatic inline int qfq_gt(u64 a, u64 b)\n{\n\treturn (s64)(a - b) > 0;\n}\n\n/* Round a precise timestamp to its slotted value. */\nstatic inline u64 qfq_round_down(u64 ts, unsigned int shift)\n{\n\treturn ts & ~((1ULL << shift) - 1);\n}\n\n/* return the pointer to the group with lowest index in the bitmap */\nstatic inline struct qfq_group *qfq_ffs(struct qfq_sched *q,\n\t\t\t\t\tunsigned long bitmap)\n{\n\tint index = __ffs(bitmap);\n\treturn &q->groups[index];\n}\n/* Calculate a mask to mimic what would be ffs_from(). */\nstatic inline unsigned long mask_from(unsigned long bitmap, int from)\n{\n\treturn bitmap & ~((1UL << from) - 1);\n}\n\n/*\n * The state computation relies on ER=0, IR=1, EB=2, IB=3\n * First compute eligibility comparing grp->S, q->V,\n * then check if someone is blocking us and possibly add EB\n */\nstatic int qfq_calc_state(struct qfq_sched *q, const struct qfq_group *grp)\n{\n\t/* if S > V we are not eligible */\n\tunsigned int state = qfq_gt(grp->S, q->V);\n\tunsigned long mask = mask_from(q->bitmaps[ER], grp->index);\n\tstruct qfq_group *next;\n\n\tif (mask) {\n\t\tnext = qfq_ffs(q, mask);\n\t\tif (qfq_gt(grp->F, next->F))\n\t\t\tstate |= EB;\n\t}\n\n\treturn state;\n}\n\n\n/*\n * In principle\n *\tq->bitmaps[dst] |= q->bitmaps[src] & mask;\n *\tq->bitmaps[src] &= ~mask;\n * but we should make sure that src != dst\n */\nstatic inline void qfq_move_groups(struct qfq_sched *q, unsigned long mask,\n\t\t\t\t   int src, int dst)\n{\n\tq->bitmaps[dst] |= q->bitmaps[src] & mask;\n\tq->bitmaps[src] &= ~mask;\n}\n\nstatic void qfq_unblock_groups(struct qfq_sched *q, int index, u64 old_F)\n{\n\tunsigned long mask = mask_from(q->bitmaps[ER], index + 1);\n\tstruct qfq_group *next;\n\n\tif (mask) {\n\t\tnext = qfq_ffs(q, mask);\n\t\tif (!qfq_gt(next->F, old_F))\n\t\t\treturn;\n\t}\n\n\tmask = (1UL << index) - 1;\n\tqfq_move_groups(q, mask, EB, ER);\n\tqfq_move_groups(q, mask, IB, IR);\n}\n\n/*\n * perhaps\n *\n\told_V ^= q->V;\n\told_V >>= q->min_slot_shift;\n\tif (old_V) {\n\t\t...\n\t}\n *\n */\nstatic void qfq_make_eligible(struct qfq_sched *q)\n{\n\tunsigned long vslot = q->V >> q->min_slot_shift;\n\tunsigned long old_vslot = q->oldV >> q->min_slot_shift;\n\n\tif (vslot != old_vslot) {\n\t\tunsigned long mask;\n\t\tint last_flip_pos = fls(vslot ^ old_vslot);\n\n\t\tif (last_flip_pos > 31) /* higher than the number of groups */\n\t\t\tmask = ~0UL;    /* make all groups eligible */\n\t\telse\n\t\t\tmask = (1UL << last_flip_pos) - 1;\n\n\t\tqfq_move_groups(q, mask, IR, ER);\n\t\tqfq_move_groups(q, mask, IB, EB);\n\t}\n}\n\n/*\n * The index of the slot in which the input aggregate agg is to be\n * inserted must not be higher than QFQ_MAX_SLOTS-2. There is a '-2'\n * and not a '-1' because the start time of the group may be moved\n * backward by one slot after the aggregate has been inserted, and\n * this would cause non-empty slots to be right-shifted by one\n * position.\n *\n * QFQ+ fully satisfies this bound to the slot index if the parameters\n * of the classes are not changed dynamically, and if QFQ+ never\n * happens to postpone the service of agg unjustly, i.e., it never\n * happens that the aggregate becomes backlogged and eligible, or just\n * eligible, while an aggregate with a higher approximated finish time\n * is being served. In particular, in this case QFQ+ guarantees that\n * the timestamps of agg are low enough that the slot index is never\n * higher than 2. Unfortunately, QFQ+ cannot provide the same\n * guarantee if it happens to unjustly postpone the service of agg, or\n * if the parameters of some class are changed.\n *\n * As for the first event, i.e., an out-of-order service, the\n * upper bound to the slot index guaranteed by QFQ+ grows to\n * 2 +\n * QFQ_MAX_AGG_CLASSES * ((1<<QFQ_MTU_SHIFT)/QFQ_MIN_LMAX) *\n * (current_max_weight/current_wsum) <= 2 + 8 * 128 * 1.\n *\n * The following function deals with this problem by backward-shifting\n * the timestamps of agg, if needed, so as to guarantee that the slot\n * index is never higher than QFQ_MAX_SLOTS-2. This backward-shift may\n * cause the service of other aggregates to be postponed, yet the\n * worst-case guarantees of these aggregates are not violated.  In\n * fact, in case of no out-of-order service, the timestamps of agg\n * would have been even lower than they are after the backward shift,\n * because QFQ+ would have guaranteed a maximum value equal to 2 for\n * the slot index, and 2 < QFQ_MAX_SLOTS-2. Hence the aggregates whose\n * service is postponed because of the backward-shift would have\n * however waited for the service of agg before being served.\n *\n * The other event that may cause the slot index to be higher than 2\n * for agg is a recent change of the parameters of some class. If the\n * weight of a class is increased or the lmax (max_pkt_size) of the\n * class is decreased, then a new aggregate with smaller slot size\n * than the original parent aggregate of the class may happen to be\n * activated. The activation of this aggregate should be properly\n * delayed to when the service of the class has finished in the ideal\n * system tracked by QFQ+. If the activation of the aggregate is not\n * delayed to this reference time instant, then this aggregate may be\n * unjustly served before other aggregates waiting for service. This\n * may cause the above bound to the slot index to be violated for some\n * of these unlucky aggregates.\n *\n * Instead of delaying the activation of the new aggregate, which is\n * quite complex, the above-discussed capping of the slot index is\n * used to handle also the consequences of a change of the parameters\n * of a class.\n */\nstatic void qfq_slot_insert(struct qfq_group *grp, struct qfq_aggregate *agg,\n\t\t\t    u64 roundedS)\n{\n\tu64 slot = (roundedS - grp->S) >> grp->slot_shift;\n\tunsigned int i; /* slot index in the bucket list */\n\n\tif (unlikely(slot > QFQ_MAX_SLOTS - 2)) {\n\t\tu64 deltaS = roundedS - grp->S -\n\t\t\t((u64)(QFQ_MAX_SLOTS - 2)<<grp->slot_shift);\n\t\tagg->S -= deltaS;\n\t\tagg->F -= deltaS;\n\t\tslot = QFQ_MAX_SLOTS - 2;\n\t}\n\n\ti = (grp->front + slot) % QFQ_MAX_SLOTS;\n\n\thlist_add_head(&agg->next, &grp->slots[i]);\n\t__set_bit(slot, &grp->full_slots);\n}\n\n/* Maybe introduce hlist_first_entry?? */\nstatic struct qfq_aggregate *qfq_slot_head(struct qfq_group *grp)\n{\n\treturn hlist_entry(grp->slots[grp->front].first,\n\t\t\t   struct qfq_aggregate, next);\n}\n\n/*\n * remove the entry from the slot\n */\nstatic void qfq_front_slot_remove(struct qfq_group *grp)\n{\n\tstruct qfq_aggregate *agg = qfq_slot_head(grp);\n\n\tBUG_ON(!agg);\n\thlist_del(&agg->next);\n\tif (hlist_empty(&grp->slots[grp->front]))\n\t\t__clear_bit(0, &grp->full_slots);\n}\n\n/*\n * Returns the first aggregate in the first non-empty bucket of the\n * group. As a side effect, adjusts the bucket list so the first\n * non-empty bucket is at position 0 in full_slots.\n */\nstatic struct qfq_aggregate *qfq_slot_scan(struct qfq_group *grp)\n{\n\tunsigned int i;\n\n\tpr_debug(\"qfq slot_scan: grp %u full %#lx\\n\",\n\t\t grp->index, grp->full_slots);\n\n\tif (grp->full_slots == 0)\n\t\treturn NULL;\n\n\ti = __ffs(grp->full_slots);  /* zero based */\n\tif (i > 0) {\n\t\tgrp->front = (grp->front + i) % QFQ_MAX_SLOTS;\n\t\tgrp->full_slots >>= i;\n\t}\n\n\treturn qfq_slot_head(grp);\n}\n\n/*\n * adjust the bucket list. When the start time of a group decreases,\n * we move the index down (modulo QFQ_MAX_SLOTS) so we don't need to\n * move the objects. The mask of occupied slots must be shifted\n * because we use ffs() to find the first non-empty slot.\n * This covers decreases in the group's start time, but what about\n * increases of the start time ?\n * Here too we should make sure that i is less than 32\n */\nstatic void qfq_slot_rotate(struct qfq_group *grp, u64 roundedS)\n{\n\tunsigned int i = (grp->S - roundedS) >> grp->slot_shift;\n\n\tgrp->full_slots <<= i;\n\tgrp->front = (grp->front - i) % QFQ_MAX_SLOTS;\n}\n\nstatic void qfq_update_eligible(struct qfq_sched *q)\n{\n\tstruct qfq_group *grp;\n\tunsigned long ineligible;\n\n\tineligible = q->bitmaps[IR] | q->bitmaps[IB];\n\tif (ineligible) {\n\t\tif (!q->bitmaps[ER]) {\n\t\t\tgrp = qfq_ffs(q, ineligible);\n\t\t\tif (qfq_gt(grp->S, q->V))\n\t\t\t\tq->V = grp->S;\n\t\t}\n\t\tqfq_make_eligible(q);\n\t}\n}\n\n/* Dequeue head packet of the head class in the DRR queue of the aggregate. */\nstatic void agg_dequeue(struct qfq_aggregate *agg,\n\t\t\tstruct qfq_class *cl, unsigned int len)\n{\n\tqdisc_dequeue_peeked(cl->qdisc);\n\n\tcl->deficit -= (int) len;\n\n\tif (cl->qdisc->q.qlen == 0) /* no more packets, remove from list */\n\t\tlist_del(&cl->alist);\n\telse if (cl->deficit < qdisc_pkt_len(cl->qdisc->ops->peek(cl->qdisc))) {\n\t\tcl->deficit += agg->lmax;\n\t\tlist_move_tail(&cl->alist, &agg->active);\n\t}\n}\n\nstatic inline struct sk_buff *qfq_peek_skb(struct qfq_aggregate *agg,\n\t\t\t\t\t   struct qfq_class **cl,\n\t\t\t\t\t   unsigned int *len)\n{\n\tstruct sk_buff *skb;\n\n\t*cl = list_first_entry(&agg->active, struct qfq_class, alist);\n\tskb = (*cl)->qdisc->ops->peek((*cl)->qdisc);\n\tif (skb == NULL)\n\t\tWARN_ONCE(1, \"qfq_dequeue: non-workconserving leaf\\n\");\n\telse\n\t\t*len = qdisc_pkt_len(skb);\n\n\treturn skb;\n}\n\n/* Update F according to the actual service received by the aggregate. */\nstatic inline void charge_actual_service(struct qfq_aggregate *agg)\n{\n\t/* Compute the service received by the aggregate, taking into\n\t * account that, after decreasing the number of classes in\n\t * agg, it may happen that\n\t * agg->initial_budget - agg->budget > agg->bugdetmax\n\t */\n\tu32 service_received = min(agg->budgetmax,\n\t\t\t\t   agg->initial_budget - agg->budget);\n\n\tagg->F = agg->S + (u64)service_received * agg->inv_w;\n}\n\n/* Assign a reasonable start time for a new aggregate in group i.\n * Admissible values for \\hat(F) are multiples of \\sigma_i\n * no greater than V+\\sigma_i . Larger values mean that\n * we had a wraparound so we consider the timestamp to be stale.\n *\n * If F is not stale and F >= V then we set S = F.\n * Otherwise we should assign S = V, but this may violate\n * the ordering in EB (see [2]). So, if we have groups in ER,\n * set S to the F_j of the first group j which would be blocking us.\n * We are guaranteed not to move S backward because\n * otherwise our group i would still be blocked.\n */\nstatic void qfq_update_start(struct qfq_sched *q, struct qfq_aggregate *agg)\n{\n\tunsigned long mask;\n\tu64 limit, roundedF;\n\tint slot_shift = agg->grp->slot_shift;\n\n\troundedF = qfq_round_down(agg->F, slot_shift);\n\tlimit = qfq_round_down(q->V, slot_shift) + (1ULL << slot_shift);\n\n\tif (!qfq_gt(agg->F, q->V) || qfq_gt(roundedF, limit)) {\n\t\t/* timestamp was stale */\n\t\tmask = mask_from(q->bitmaps[ER], agg->grp->index);\n\t\tif (mask) {\n\t\t\tstruct qfq_group *next = qfq_ffs(q, mask);\n\t\t\tif (qfq_gt(roundedF, next->F)) {\n\t\t\t\tif (qfq_gt(limit, next->F))\n\t\t\t\t\tagg->S = next->F;\n\t\t\t\telse /* preserve timestamp correctness */\n\t\t\t\t\tagg->S = limit;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\tagg->S = q->V;\n\t} else  /* timestamp is not stale */\n\t\tagg->S = agg->F;\n}\n\n/* Update the timestamps of agg before scheduling/rescheduling it for\n * service.  In particular, assign to agg->F its maximum possible\n * value, i.e., the virtual finish time with which the aggregate\n * should be labeled if it used all its budget once in service.\n */\nstatic inline void\nqfq_update_agg_ts(struct qfq_sched *q,\n\t\t    struct qfq_aggregate *agg, enum update_reason reason)\n{\n\tif (reason != requeue)\n\t\tqfq_update_start(q, agg);\n\telse /* just charge agg for the service received */\n\t\tagg->S = agg->F;\n\n\tagg->F = agg->S + (u64)agg->budgetmax * agg->inv_w;\n}\n\nstatic void qfq_schedule_agg(struct qfq_sched *q, struct qfq_aggregate *agg);\n\nstatic struct sk_buff *qfq_dequeue(struct Qdisc *sch)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_aggregate *in_serv_agg = q->in_serv_agg;\n\tstruct qfq_class *cl;\n\tstruct sk_buff *skb = NULL;\n\t/* next-packet len, 0 means no more active classes in in-service agg */\n\tunsigned int len = 0;\n\n\tif (in_serv_agg == NULL)\n\t\treturn NULL;\n\n\tif (!list_empty(&in_serv_agg->active))\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\n\t/*\n\t * If there are no active classes in the in-service aggregate,\n\t * or if the aggregate has not enough budget to serve its next\n\t * class, then choose the next aggregate to serve.\n\t */\n\tif (len == 0 || in_serv_agg->budget < len) {\n\t\tcharge_actual_service(in_serv_agg);\n\n\t\t/* recharge the budget of the aggregate */\n\t\tin_serv_agg->initial_budget = in_serv_agg->budget =\n\t\t\tin_serv_agg->budgetmax;\n\n\t\tif (!list_empty(&in_serv_agg->active)) {\n\t\t\t/*\n\t\t\t * Still active: reschedule for\n\t\t\t * service. Possible optimization: if no other\n\t\t\t * aggregate is active, then there is no point\n\t\t\t * in rescheduling this aggregate, and we can\n\t\t\t * just keep it as the in-service one. This\n\t\t\t * should be however a corner case, and to\n\t\t\t * handle it, we would need to maintain an\n\t\t\t * extra num_active_aggs field.\n\t\t\t*/\n\t\t\tqfq_update_agg_ts(q, in_serv_agg, requeue);\n\t\t\tqfq_schedule_agg(q, in_serv_agg);\n\t\t} else if (sch->q.qlen == 0) { /* no aggregate to serve */\n\t\t\tq->in_serv_agg = NULL;\n\t\t\treturn NULL;\n\t\t}\n\n\t\t/*\n\t\t * If we get here, there are other aggregates queued:\n\t\t * choose the new aggregate to serve.\n\t\t */\n\t\tin_serv_agg = q->in_serv_agg = qfq_choose_next_agg(q);\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\t}\n\tif (!skb)\n\t\treturn NULL;\n\n\tqdisc_qstats_backlog_dec(sch, skb);\n\tsch->q.qlen--;\n\tqdisc_bstats_update(sch, skb);\n\n\tagg_dequeue(in_serv_agg, cl, len);\n\t/* If lmax is lowered, through qfq_change_class, for a class\n\t * owning pending packets with larger size than the new value\n\t * of lmax, then the following condition may hold.\n\t */\n\tif (unlikely(in_serv_agg->budget < len))\n\t\tin_serv_agg->budget = 0;\n\telse\n\t\tin_serv_agg->budget -= len;\n\n\tq->V += (u64)len * q->iwsum;\n\tpr_debug(\"qfq dequeue: len %u F %lld now %lld\\n\",\n\t\t len, (unsigned long long) in_serv_agg->F,\n\t\t (unsigned long long) q->V);\n\n\treturn skb;\n}\n\nstatic struct qfq_aggregate *qfq_choose_next_agg(struct qfq_sched *q)\n{\n\tstruct qfq_group *grp;\n\tstruct qfq_aggregate *agg, *new_front_agg;\n\tu64 old_F;\n\n\tqfq_update_eligible(q);\n\tq->oldV = q->V;\n\n\tif (!q->bitmaps[ER])\n\t\treturn NULL;\n\n\tgrp = qfq_ffs(q, q->bitmaps[ER]);\n\told_F = grp->F;\n\n\tagg = qfq_slot_head(grp);\n\n\t/* agg starts to be served, remove it from schedule */\n\tqfq_front_slot_remove(grp);\n\n\tnew_front_agg = qfq_slot_scan(grp);\n\n\tif (new_front_agg == NULL) /* group is now inactive, remove from ER */\n\t\t__clear_bit(grp->index, &q->bitmaps[ER]);\n\telse {\n\t\tu64 roundedS = qfq_round_down(new_front_agg->S,\n\t\t\t\t\t      grp->slot_shift);\n\t\tunsigned int s;\n\n\t\tif (grp->S == roundedS)\n\t\t\treturn agg;\n\t\tgrp->S = roundedS;\n\t\tgrp->F = roundedS + (2ULL << grp->slot_shift);\n\t\t__clear_bit(grp->index, &q->bitmaps[ER]);\n\t\ts = qfq_calc_state(q, grp);\n\t\t__set_bit(grp->index, &q->bitmaps[s]);\n\t}\n\n\tqfq_unblock_groups(q, grp->index, old_F);\n\n\treturn agg;\n}\n\nstatic int qfq_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t       struct sk_buff **to_free)\n{\n\tunsigned int len = qdisc_pkt_len(skb), gso_segs;\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl;\n\tstruct qfq_aggregate *agg;\n\tint err = 0;\n\tbool first;\n\n\tcl = qfq_classify(skb, sch, &err);\n\tif (cl == NULL) {\n\t\tif (err & __NET_XMIT_BYPASS)\n\t\t\tqdisc_qstats_drop(sch);\n\t\t__qdisc_drop(skb, to_free);\n\t\treturn err;\n\t}\n\tpr_debug(\"qfq_enqueue: cl = %x\\n\", cl->common.classid);\n\n\tif (unlikely(cl->agg->lmax < len)) {\n\t\tpr_debug(\"qfq: increasing maxpkt from %u to %u for class %u\",\n\t\t\t cl->agg->lmax, len, cl->common.classid);\n\t\terr = qfq_change_agg(sch, cl, cl->agg->class_weight, len);\n\t\tif (err) {\n\t\t\tcl->qstats.drops++;\n\t\t\treturn qdisc_drop(skb, sch, to_free);\n\t\t}\n\t}\n\n\tgso_segs = skb_is_gso(skb) ? skb_shinfo(skb)->gso_segs : 1;\n\tfirst = !cl->qdisc->q.qlen;\n\terr = qdisc_enqueue(skb, cl->qdisc, to_free);\n\tif (unlikely(err != NET_XMIT_SUCCESS)) {\n\t\tpr_debug(\"qfq_enqueue: enqueue failed %d\\n\", err);\n\t\tif (net_xmit_drop_count(err)) {\n\t\t\tcl->qstats.drops++;\n\t\t\tqdisc_qstats_drop(sch);\n\t\t}\n\t\treturn err;\n\t}\n\n\t_bstats_update(&cl->bstats, len, gso_segs);\n\tsch->qstats.backlog += len;\n\t++sch->q.qlen;\n\n\tagg = cl->agg;\n\t/* if the queue was not empty, then done here */\n\tif (!first) {\n\t\tif (unlikely(skb == cl->qdisc->ops->peek(cl->qdisc)) &&\n\t\t    list_first_entry(&agg->active, struct qfq_class, alist)\n\t\t    == cl && cl->deficit < len)\n\t\t\tlist_move_tail(&cl->alist, &agg->active);\n\n\t\treturn err;\n\t}\n\n\t/* schedule class for service within the aggregate */\n\tcl->deficit = agg->lmax;\n\tlist_add_tail(&cl->alist, &agg->active);\n\n\tif (list_first_entry(&agg->active, struct qfq_class, alist) != cl ||\n\t    q->in_serv_agg == agg)\n\t\treturn err; /* non-empty or in service, nothing else to do */\n\n\tqfq_activate_agg(q, agg, enqueue);\n\n\treturn err;\n}\n\n/*\n * Schedule aggregate according to its timestamps.\n */\nstatic void qfq_schedule_agg(struct qfq_sched *q, struct qfq_aggregate *agg)\n{\n\tstruct qfq_group *grp = agg->grp;\n\tu64 roundedS;\n\tint s;\n\n\troundedS = qfq_round_down(agg->S, grp->slot_shift);\n\n\t/*\n\t * Insert agg in the correct bucket.\n\t * If agg->S >= grp->S we don't need to adjust the\n\t * bucket list and simply go to the insertion phase.\n\t * Otherwise grp->S is decreasing, we must make room\n\t * in the bucket list, and also recompute the group state.\n\t * Finally, if there were no flows in this group and nobody\n\t * was in ER make sure to adjust V.\n\t */\n\tif (grp->full_slots) {\n\t\tif (!qfq_gt(grp->S, agg->S))\n\t\t\tgoto skip_update;\n\n\t\t/* create a slot for this agg->S */\n\t\tqfq_slot_rotate(grp, roundedS);\n\t\t/* group was surely ineligible, remove */\n\t\t__clear_bit(grp->index, &q->bitmaps[IR]);\n\t\t__clear_bit(grp->index, &q->bitmaps[IB]);\n\t} else if (!q->bitmaps[ER] && qfq_gt(roundedS, q->V) &&\n\t\t   q->in_serv_agg == NULL)\n\t\tq->V = roundedS;\n\n\tgrp->S = roundedS;\n\tgrp->F = roundedS + (2ULL << grp->slot_shift);\n\ts = qfq_calc_state(q, grp);\n\t__set_bit(grp->index, &q->bitmaps[s]);\n\n\tpr_debug(\"qfq enqueue: new state %d %#lx S %lld F %lld V %lld\\n\",\n\t\t s, q->bitmaps[s],\n\t\t (unsigned long long) agg->S,\n\t\t (unsigned long long) agg->F,\n\t\t (unsigned long long) q->V);\n\nskip_update:\n\tqfq_slot_insert(grp, agg, roundedS);\n}\n\n\n/* Update agg ts and schedule agg for service */\nstatic void qfq_activate_agg(struct qfq_sched *q, struct qfq_aggregate *agg,\n\t\t\t     enum update_reason reason)\n{\n\tagg->initial_budget = agg->budget = agg->budgetmax; /* recharge budg. */\n\n\tqfq_update_agg_ts(q, agg, reason);\n\tif (q->in_serv_agg == NULL) { /* no aggr. in service or scheduled */\n\t\tq->in_serv_agg = agg; /* start serving this aggregate */\n\t\t /* update V: to be in service, agg must be eligible */\n\t\tq->oldV = q->V = agg->S;\n\t} else if (agg != q->in_serv_agg)\n\t\tqfq_schedule_agg(q, agg);\n}\n\nstatic void qfq_slot_remove(struct qfq_sched *q, struct qfq_group *grp,\n\t\t\t    struct qfq_aggregate *agg)\n{\n\tunsigned int i, offset;\n\tu64 roundedS;\n\n\troundedS = qfq_round_down(agg->S, grp->slot_shift);\n\toffset = (roundedS - grp->S) >> grp->slot_shift;\n\n\ti = (grp->front + offset) % QFQ_MAX_SLOTS;\n\n\thlist_del(&agg->next);\n\tif (hlist_empty(&grp->slots[i]))\n\t\t__clear_bit(offset, &grp->full_slots);\n}\n\n/*\n * Called to forcibly deschedule an aggregate.  If the aggregate is\n * not in the front bucket, or if the latter has other aggregates in\n * the front bucket, we can simply remove the aggregate with no other\n * side effects.\n * Otherwise we must propagate the event up.\n */\nstatic void qfq_deactivate_agg(struct qfq_sched *q, struct qfq_aggregate *agg)\n{\n\tstruct qfq_group *grp = agg->grp;\n\tunsigned long mask;\n\tu64 roundedS;\n\tint s;\n\n\tif (agg == q->in_serv_agg) {\n\t\tcharge_actual_service(agg);\n\t\tq->in_serv_agg = qfq_choose_next_agg(q);\n\t\treturn;\n\t}\n\n\tagg->F = agg->S;\n\tqfq_slot_remove(q, grp, agg);\n\n\tif (!grp->full_slots) {\n\t\t__clear_bit(grp->index, &q->bitmaps[IR]);\n\t\t__clear_bit(grp->index, &q->bitmaps[EB]);\n\t\t__clear_bit(grp->index, &q->bitmaps[IB]);\n\n\t\tif (test_bit(grp->index, &q->bitmaps[ER]) &&\n\t\t    !(q->bitmaps[ER] & ~((1UL << grp->index) - 1))) {\n\t\t\tmask = q->bitmaps[ER] & ((1UL << grp->index) - 1);\n\t\t\tif (mask)\n\t\t\t\tmask = ~((1UL << __fls(mask)) - 1);\n\t\t\telse\n\t\t\t\tmask = ~0UL;\n\t\t\tqfq_move_groups(q, mask, EB, ER);\n\t\t\tqfq_move_groups(q, mask, IB, IR);\n\t\t}\n\t\t__clear_bit(grp->index, &q->bitmaps[ER]);\n\t} else if (hlist_empty(&grp->slots[grp->front])) {\n\t\tagg = qfq_slot_scan(grp);\n\t\troundedS = qfq_round_down(agg->S, grp->slot_shift);\n\t\tif (grp->S != roundedS) {\n\t\t\t__clear_bit(grp->index, &q->bitmaps[ER]);\n\t\t\t__clear_bit(grp->index, &q->bitmaps[IR]);\n\t\t\t__clear_bit(grp->index, &q->bitmaps[EB]);\n\t\t\t__clear_bit(grp->index, &q->bitmaps[IB]);\n\t\t\tgrp->S = roundedS;\n\t\t\tgrp->F = roundedS + (2ULL << grp->slot_shift);\n\t\t\ts = qfq_calc_state(q, grp);\n\t\t\t__set_bit(grp->index, &q->bitmaps[s]);\n\t\t}\n\t}\n}\n\nstatic void qfq_qlen_notify(struct Qdisc *sch, unsigned long arg)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\n\tqfq_deactivate_class(q, cl);\n}\n\nstatic int qfq_init_qdisc(struct Qdisc *sch, struct nlattr *opt,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_group *grp;\n\tint i, j, err;\n\tu32 max_cl_shift, maxbudg_shift, max_classes;\n\n\terr = tcf_block_get(&q->block, &q->filter_list, sch, extack);\n\tif (err)\n\t\treturn err;\n\n\terr = qdisc_class_hash_init(&q->clhash);\n\tif (err < 0)\n\t\treturn err;\n\n\tmax_classes = min_t(u64, (u64)qdisc_dev(sch)->tx_queue_len + 1,\n\t\t\t    QFQ_MAX_AGG_CLASSES);\n\t/* max_cl_shift = floor(log_2(max_classes)) */\n\tmax_cl_shift = __fls(max_classes);\n\tq->max_agg_classes = 1<<max_cl_shift;\n\n\t/* maxbudg_shift = log2(max_len * max_classes_per_agg) */\n\tmaxbudg_shift = QFQ_MTU_SHIFT + max_cl_shift;\n\tq->min_slot_shift = FRAC_BITS + maxbudg_shift - QFQ_MAX_INDEX;\n\n\tfor (i = 0; i <= QFQ_MAX_INDEX; i++) {\n\t\tgrp = &q->groups[i];\n\t\tgrp->index = i;\n\t\tgrp->slot_shift = q->min_slot_shift + i;\n\t\tfor (j = 0; j < QFQ_MAX_SLOTS; j++)\n\t\t\tINIT_HLIST_HEAD(&grp->slots[j]);\n\t}\n\n\tINIT_HLIST_HEAD(&q->nonfull_aggs);\n\n\treturn 0;\n}\n\nstatic void qfq_reset_qdisc(struct Qdisc *sch)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl;\n\tunsigned int i;\n\n\tfor (i = 0; i < q->clhash.hashsize; i++) {\n\t\thlist_for_each_entry(cl, &q->clhash.hash[i], common.hnode) {\n\t\t\tif (cl->qdisc->q.qlen > 0)\n\t\t\t\tqfq_deactivate_class(q, cl);\n\n\t\t\tqdisc_reset(cl->qdisc);\n\t\t}\n\t}\n}\n\nstatic void qfq_destroy_qdisc(struct Qdisc *sch)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl;\n\tstruct hlist_node *next;\n\tunsigned int i;\n\n\ttcf_block_put(q->block);\n\n\tfor (i = 0; i < q->clhash.hashsize; i++) {\n\t\thlist_for_each_entry_safe(cl, next, &q->clhash.hash[i],\n\t\t\t\t\t  common.hnode) {\n\t\t\tqfq_destroy_class(sch, cl);\n\t\t}\n\t}\n\tqdisc_class_hash_destroy(&q->clhash);\n}\n\nstatic const struct Qdisc_class_ops qfq_class_ops = {\n\t.change\t\t= qfq_change_class,\n\t.delete\t\t= qfq_delete_class,\n\t.find\t\t= qfq_search_class,\n\t.tcf_block\t= qfq_tcf_block,\n\t.bind_tcf\t= qfq_bind_tcf,\n\t.unbind_tcf\t= qfq_unbind_tcf,\n\t.graft\t\t= qfq_graft_class,\n\t.leaf\t\t= qfq_class_leaf,\n\t.qlen_notify\t= qfq_qlen_notify,\n\t.dump\t\t= qfq_dump_class,\n\t.dump_stats\t= qfq_dump_class_stats,\n\t.walk\t\t= qfq_walk,\n};\n\nstatic struct Qdisc_ops qfq_qdisc_ops __read_mostly = {\n\t.cl_ops\t\t= &qfq_class_ops,\n\t.id\t\t= \"qfq\",\n\t.priv_size\t= sizeof(struct qfq_sched),\n\t.enqueue\t= qfq_enqueue,\n\t.dequeue\t= qfq_dequeue,\n\t.peek\t\t= qdisc_peek_dequeued,\n\t.init\t\t= qfq_init_qdisc,\n\t.reset\t\t= qfq_reset_qdisc,\n\t.destroy\t= qfq_destroy_qdisc,\n\t.owner\t\t= THIS_MODULE,\n};\n\nstatic int __init qfq_init(void)\n{\n\treturn register_qdisc(&qfq_qdisc_ops);\n}\n\nstatic void __exit qfq_exit(void)\n{\n\tunregister_qdisc(&qfq_qdisc_ops);\n}\n\nmodule_init(qfq_init);\nmodule_exit(qfq_exit);\nMODULE_LICENSE(\"GPL\");\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0-only\n/*\n * net/sched/sch_qfq.c         Quick Fair Queueing Plus Scheduler.\n *\n * Copyright (c) 2009 Fabio Checconi, Luigi Rizzo, and Paolo Valente.\n * Copyright (c) 2012 Paolo Valente.\n */\n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/bitops.h>\n#include <linux/errno.h>\n#include <linux/netdevice.h>\n#include <linux/pkt_sched.h>\n#include <net/sch_generic.h>\n#include <net/pkt_sched.h>\n#include <net/pkt_cls.h>\n\n\n/*  Quick Fair Queueing Plus\n    ========================\n\n    Sources:\n\n    [1] Paolo Valente,\n    \"Reducing the Execution Time of Fair-Queueing Schedulers.\"\n    http://algo.ing.unimo.it/people/paolo/agg-sched/agg-sched.pdf\n\n    Sources for QFQ:\n\n    [2] Fabio Checconi, Luigi Rizzo, and Paolo Valente: \"QFQ: Efficient\n    Packet Scheduling with Tight Bandwidth Distribution Guarantees.\"\n\n    See also:\n    http://retis.sssup.it/~fabio/linux/qfq/\n */\n\n/*\n\n  QFQ+ divides classes into aggregates of at most MAX_AGG_CLASSES\n  classes. Each aggregate is timestamped with a virtual start time S\n  and a virtual finish time F, and scheduled according to its\n  timestamps. S and F are computed as a function of a system virtual\n  time function V. The classes within each aggregate are instead\n  scheduled with DRR.\n\n  To speed up operations, QFQ+ divides also aggregates into a limited\n  number of groups. Which group a class belongs to depends on the\n  ratio between the maximum packet length for the class and the weight\n  of the class. Groups have their own S and F. In the end, QFQ+\n  schedules groups, then aggregates within groups, then classes within\n  aggregates. See [1] and [2] for a full description.\n\n  Virtual time computations.\n\n  S, F and V are all computed in fixed point arithmetic with\n  FRAC_BITS decimal bits.\n\n  QFQ_MAX_INDEX is the maximum index allowed for a group. We need\n\tone bit per index.\n  QFQ_MAX_WSHIFT is the maximum power of two supported as a weight.\n\n  The layout of the bits is as below:\n\n                   [ MTU_SHIFT ][      FRAC_BITS    ]\n                   [ MAX_INDEX    ][ MIN_SLOT_SHIFT ]\n\t\t\t\t ^.__grp->index = 0\n\t\t\t\t *.__grp->slot_shift\n\n  where MIN_SLOT_SHIFT is derived by difference from the others.\n\n  The max group index corresponds to Lmax/w_min, where\n  Lmax=1<<MTU_SHIFT, w_min = 1 .\n  From this, and knowing how many groups (MAX_INDEX) we want,\n  we can derive the shift corresponding to each group.\n\n  Because we often need to compute\n\tF = S + len/w_i  and V = V + len/wsum\n  instead of storing w_i store the value\n\tinv_w = (1<<FRAC_BITS)/w_i\n  so we can do F = S + len * inv_w * wsum.\n  We use W_TOT in the formulas so we can easily move between\n  static and adaptive weight sum.\n\n  The per-scheduler-instance data contain all the data structures\n  for the scheduler: bitmaps and bucket lists.\n\n */\n\n/*\n * Maximum number of consecutive slots occupied by backlogged classes\n * inside a group.\n */\n#define QFQ_MAX_SLOTS\t32\n\n/*\n * Shifts used for aggregate<->group mapping.  We allow class weights that are\n * in the range [1, 2^MAX_WSHIFT], and we try to map each aggregate i to the\n * group with the smallest index that can support the L_i / r_i configured\n * for the classes in the aggregate.\n *\n * grp->index is the index of the group; and grp->slot_shift\n * is the shift for the corresponding (scaled) sigma_i.\n */\n#define QFQ_MAX_INDEX\t\t24\n#define QFQ_MAX_WSHIFT\t\t10\n\n#define\tQFQ_MAX_WEIGHT\t\t(1<<QFQ_MAX_WSHIFT) /* see qfq_slot_insert */\n#define QFQ_MAX_WSUM\t\t(64*QFQ_MAX_WEIGHT)\n\n#define FRAC_BITS\t\t30\t/* fixed point arithmetic */\n#define ONE_FP\t\t\t(1UL << FRAC_BITS)\n\n#define QFQ_MTU_SHIFT\t\t16\t/* to support TSO/GSO */\n#define QFQ_MIN_LMAX\t\t512\t/* see qfq_slot_insert */\n\n#define QFQ_MAX_AGG_CLASSES\t8 /* max num classes per aggregate allowed */\n\n/*\n * Possible group states.  These values are used as indexes for the bitmaps\n * array of struct qfq_queue.\n */\nenum qfq_state { ER, IR, EB, IB, QFQ_MAX_STATE };\n\nstruct qfq_group;\n\nstruct qfq_aggregate;\n\nstruct qfq_class {\n\tstruct Qdisc_class_common common;\n\n\tunsigned int filter_cnt;\n\n\tstruct gnet_stats_basic_sync bstats;\n\tstruct gnet_stats_queue qstats;\n\tstruct net_rate_estimator __rcu *rate_est;\n\tstruct Qdisc *qdisc;\n\tstruct list_head alist;\t\t/* Link for active-classes list. */\n\tstruct qfq_aggregate *agg;\t/* Parent aggregate. */\n\tint deficit;\t\t\t/* DRR deficit counter. */\n};\n\nstruct qfq_aggregate {\n\tstruct hlist_node next;\t/* Link for the slot list. */\n\tu64 S, F;\t\t/* flow timestamps (exact) */\n\n\t/* group we belong to. In principle we would need the index,\n\t * which is log_2(lmax/weight), but we never reference it\n\t * directly, only the group.\n\t */\n\tstruct qfq_group *grp;\n\n\t/* these are copied from the flowset. */\n\tu32\tclass_weight; /* Weight of each class in this aggregate. */\n\t/* Max pkt size for the classes in this aggregate, DRR quantum. */\n\tint\tlmax;\n\n\tu32\tinv_w;\t    /* ONE_FP/(sum of weights of classes in aggr.). */\n\tu32\tbudgetmax;  /* Max budget for this aggregate. */\n\tu32\tinitial_budget, budget;     /* Initial and current budget. */\n\n\tint\t\t  num_classes;\t/* Number of classes in this aggr. */\n\tstruct list_head  active;\t/* DRR queue of active classes. */\n\n\tstruct hlist_node nonfull_next;\t/* See nonfull_aggs in qfq_sched. */\n};\n\nstruct qfq_group {\n\tu64 S, F;\t\t\t/* group timestamps (approx). */\n\tunsigned int slot_shift;\t/* Slot shift. */\n\tunsigned int index;\t\t/* Group index. */\n\tunsigned int front;\t\t/* Index of the front slot. */\n\tunsigned long full_slots;\t/* non-empty slots */\n\n\t/* Array of RR lists of active aggregates. */\n\tstruct hlist_head slots[QFQ_MAX_SLOTS];\n};\n\nstruct qfq_sched {\n\tstruct tcf_proto __rcu *filter_list;\n\tstruct tcf_block\t*block;\n\tstruct Qdisc_class_hash clhash;\n\n\tu64\t\t\toldV, V;\t/* Precise virtual times. */\n\tstruct qfq_aggregate\t*in_serv_agg;   /* Aggregate being served. */\n\tu32\t\t\twsum;\t\t/* weight sum */\n\tu32\t\t\tiwsum;\t\t/* inverse weight sum */\n\n\tunsigned long bitmaps[QFQ_MAX_STATE];\t    /* Group bitmaps. */\n\tstruct qfq_group groups[QFQ_MAX_INDEX + 1]; /* The groups. */\n\tu32 min_slot_shift;\t/* Index of the group-0 bit in the bitmaps. */\n\n\tu32 max_agg_classes;\t\t/* Max number of classes per aggr. */\n\tstruct hlist_head nonfull_aggs; /* Aggs with room for more classes. */\n};\n\n/*\n * Possible reasons why the timestamps of an aggregate are updated\n * enqueue: the aggregate switches from idle to active and must scheduled\n *\t    for service\n * requeue: the aggregate finishes its budget, so it stops being served and\n *\t    must be rescheduled for service\n */\nenum update_reason {enqueue, requeue};\n\nstatic struct qfq_class *qfq_find_class(struct Qdisc *sch, u32 classid)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct Qdisc_class_common *clc;\n\n\tclc = qdisc_class_find(&q->clhash, classid);\n\tif (clc == NULL)\n\t\treturn NULL;\n\treturn container_of(clc, struct qfq_class, common);\n}\n\nstatic const struct nla_policy qfq_policy[TCA_QFQ_MAX + 1] = {\n\t[TCA_QFQ_WEIGHT] = { .type = NLA_U32 },\n\t[TCA_QFQ_LMAX] = { .type = NLA_U32 },\n};\n\n/*\n * Calculate a flow index, given its weight and maximum packet length.\n * index = log_2(maxlen/weight) but we need to apply the scaling.\n * This is used only once at flow creation.\n */\nstatic int qfq_calc_index(u32 inv_w, unsigned int maxlen, u32 min_slot_shift)\n{\n\tu64 slot_size = (u64)maxlen * inv_w;\n\tunsigned long size_map;\n\tint index = 0;\n\n\tsize_map = slot_size >> min_slot_shift;\n\tif (!size_map)\n\t\tgoto out;\n\n\tindex = __fls(size_map) + 1;\t/* basically a log_2 */\n\tindex -= !(slot_size - (1ULL << (index + min_slot_shift - 1)));\n\n\tif (index < 0)\n\t\tindex = 0;\nout:\n\tpr_debug(\"qfq calc_index: W = %lu, L = %u, I = %d\\n\",\n\t\t (unsigned long) ONE_FP/inv_w, maxlen, index);\n\n\treturn index;\n}\n\nstatic void qfq_deactivate_agg(struct qfq_sched *, struct qfq_aggregate *);\nstatic void qfq_activate_agg(struct qfq_sched *, struct qfq_aggregate *,\n\t\t\t     enum update_reason);\n\nstatic void qfq_init_agg(struct qfq_sched *q, struct qfq_aggregate *agg,\n\t\t\t u32 lmax, u32 weight)\n{\n\tINIT_LIST_HEAD(&agg->active);\n\thlist_add_head(&agg->nonfull_next, &q->nonfull_aggs);\n\n\tagg->lmax = lmax;\n\tagg->class_weight = weight;\n}\n\nstatic struct qfq_aggregate *qfq_find_agg(struct qfq_sched *q,\n\t\t\t\t\t  u32 lmax, u32 weight)\n{\n\tstruct qfq_aggregate *agg;\n\n\thlist_for_each_entry(agg, &q->nonfull_aggs, nonfull_next)\n\t\tif (agg->lmax == lmax && agg->class_weight == weight)\n\t\t\treturn agg;\n\n\treturn NULL;\n}\n\n\n/* Update aggregate as a function of the new number of classes. */\nstatic void qfq_update_agg(struct qfq_sched *q, struct qfq_aggregate *agg,\n\t\t\t   int new_num_classes)\n{\n\tu32 new_agg_weight;\n\n\tif (new_num_classes == q->max_agg_classes)\n\t\thlist_del_init(&agg->nonfull_next);\n\n\tif (agg->num_classes > new_num_classes &&\n\t    new_num_classes == q->max_agg_classes - 1) /* agg no more full */\n\t\thlist_add_head(&agg->nonfull_next, &q->nonfull_aggs);\n\n\t/* The next assignment may let\n\t * agg->initial_budget > agg->budgetmax\n\t * hold, we will take it into account in charge_actual_service().\n\t */\n\tagg->budgetmax = new_num_classes * agg->lmax;\n\tnew_agg_weight = agg->class_weight * new_num_classes;\n\tagg->inv_w = ONE_FP/new_agg_weight;\n\n\tif (agg->grp == NULL) {\n\t\tint i = qfq_calc_index(agg->inv_w, agg->budgetmax,\n\t\t\t\t       q->min_slot_shift);\n\t\tagg->grp = &q->groups[i];\n\t}\n\n\tq->wsum +=\n\t\t(int) agg->class_weight * (new_num_classes - agg->num_classes);\n\tq->iwsum = ONE_FP / q->wsum;\n\n\tagg->num_classes = new_num_classes;\n}\n\n/* Add class to aggregate. */\nstatic void qfq_add_to_agg(struct qfq_sched *q,\n\t\t\t   struct qfq_aggregate *agg,\n\t\t\t   struct qfq_class *cl)\n{\n\tcl->agg = agg;\n\n\tqfq_update_agg(q, agg, agg->num_classes+1);\n\tif (cl->qdisc->q.qlen > 0) { /* adding an active class */\n\t\tlist_add_tail(&cl->alist, &agg->active);\n\t\tif (list_first_entry(&agg->active, struct qfq_class, alist) ==\n\t\t    cl && q->in_serv_agg != agg) /* agg was inactive */\n\t\t\tqfq_activate_agg(q, agg, enqueue); /* schedule agg */\n\t}\n}\n\nstatic struct qfq_aggregate *qfq_choose_next_agg(struct qfq_sched *);\n\nstatic void qfq_destroy_agg(struct qfq_sched *q, struct qfq_aggregate *agg)\n{\n\thlist_del_init(&agg->nonfull_next);\n\tq->wsum -= agg->class_weight;\n\tif (q->wsum != 0)\n\t\tq->iwsum = ONE_FP / q->wsum;\n\n\tif (q->in_serv_agg == agg)\n\t\tq->in_serv_agg = qfq_choose_next_agg(q);\n\tkfree(agg);\n}\n\n/* Deschedule class from within its parent aggregate. */\nstatic void qfq_deactivate_class(struct qfq_sched *q, struct qfq_class *cl)\n{\n\tstruct qfq_aggregate *agg = cl->agg;\n\n\n\tlist_del(&cl->alist); /* remove from RR queue of the aggregate */\n\tif (list_empty(&agg->active)) /* agg is now inactive */\n\t\tqfq_deactivate_agg(q, agg);\n}\n\n/* Remove class from its parent aggregate. */\nstatic void qfq_rm_from_agg(struct qfq_sched *q, struct qfq_class *cl)\n{\n\tstruct qfq_aggregate *agg = cl->agg;\n\n\tcl->agg = NULL;\n\tif (agg->num_classes == 1) { /* agg being emptied, destroy it */\n\t\tqfq_destroy_agg(q, agg);\n\t\treturn;\n\t}\n\tqfq_update_agg(q, agg, agg->num_classes-1);\n}\n\n/* Deschedule class and remove it from its parent aggregate. */\nstatic void qfq_deact_rm_from_agg(struct qfq_sched *q, struct qfq_class *cl)\n{\n\tif (cl->qdisc->q.qlen > 0) /* class is active */\n\t\tqfq_deactivate_class(q, cl);\n\n\tqfq_rm_from_agg(q, cl);\n}\n\n/* Move class to a new aggregate, matching the new class weight and/or lmax */\nstatic int qfq_change_agg(struct Qdisc *sch, struct qfq_class *cl, u32 weight,\n\t\t\t   u32 lmax)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_aggregate *new_agg = qfq_find_agg(q, lmax, weight);\n\n\tif (new_agg == NULL) { /* create new aggregate */\n\t\tnew_agg = kzalloc(sizeof(*new_agg), GFP_ATOMIC);\n\t\tif (new_agg == NULL)\n\t\t\treturn -ENOBUFS;\n\t\tqfq_init_agg(q, new_agg, lmax, weight);\n\t}\n\tqfq_deact_rm_from_agg(q, cl);\n\tqfq_add_to_agg(q, new_agg, cl);\n\n\treturn 0;\n}\n\nstatic int qfq_change_class(struct Qdisc *sch, u32 classid, u32 parentid,\n\t\t\t    struct nlattr **tca, unsigned long *arg,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl = (struct qfq_class *)*arg;\n\tbool existing = false;\n\tstruct nlattr *tb[TCA_QFQ_MAX + 1];\n\tstruct qfq_aggregate *new_agg = NULL;\n\tu32 weight, lmax, inv_w;\n\tint err;\n\tint delta_w;\n\n\tif (tca[TCA_OPTIONS] == NULL) {\n\t\tpr_notice(\"qfq: no options\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = nla_parse_nested_deprecated(tb, TCA_QFQ_MAX, tca[TCA_OPTIONS],\n\t\t\t\t\t  qfq_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_QFQ_WEIGHT]) {\n\t\tweight = nla_get_u32(tb[TCA_QFQ_WEIGHT]);\n\t\tif (!weight || weight > (1UL << QFQ_MAX_WSHIFT)) {\n\t\t\tpr_notice(\"qfq: invalid weight %u\\n\", weight);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else\n\t\tweight = 1;\n\n\tif (tb[TCA_QFQ_LMAX])\n\t\tlmax = nla_get_u32(tb[TCA_QFQ_LMAX]);\n\telse\n\t\tlmax = psched_mtu(qdisc_dev(sch));\n\n\tif (lmax < QFQ_MIN_LMAX || lmax > (1UL << QFQ_MTU_SHIFT)) {\n\t\tpr_notice(\"qfq: invalid max length %u\\n\", lmax);\n\t\treturn -EINVAL;\n\t}\n\n\tinv_w = ONE_FP / weight;\n\tweight = ONE_FP / inv_w;\n\n\tif (cl != NULL &&\n\t    lmax == cl->agg->lmax &&\n\t    weight == cl->agg->class_weight)\n\t\treturn 0; /* nothing to change */\n\n\tdelta_w = weight - (cl ? cl->agg->class_weight : 0);\n\n\tif (q->wsum + delta_w > QFQ_MAX_WSUM) {\n\t\tpr_notice(\"qfq: total weight out of range (%d + %u)\\n\",\n\t\t\t  delta_w, q->wsum);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cl != NULL) { /* modify existing class */\n\t\tif (tca[TCA_RATE]) {\n\t\t\terr = gen_replace_estimator(&cl->bstats, NULL,\n\t\t\t\t\t\t    &cl->rate_est,\n\t\t\t\t\t\t    NULL,\n\t\t\t\t\t\t    true,\n\t\t\t\t\t\t    tca[TCA_RATE]);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\texisting = true;\n\t\tgoto set_change_agg;\n\t}\n\n\t/* create and init new class */\n\tcl = kzalloc(sizeof(struct qfq_class), GFP_KERNEL);\n\tif (cl == NULL)\n\t\treturn -ENOBUFS;\n\n\tgnet_stats_basic_sync_init(&cl->bstats);\n\tcl->common.classid = classid;\n\tcl->deficit = lmax;\n\n\tcl->qdisc = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t      classid, NULL);\n\tif (cl->qdisc == NULL)\n\t\tcl->qdisc = &noop_qdisc;\n\n\tif (tca[TCA_RATE]) {\n\t\terr = gen_new_estimator(&cl->bstats, NULL,\n\t\t\t\t\t&cl->rate_est,\n\t\t\t\t\tNULL,\n\t\t\t\t\ttrue,\n\t\t\t\t\ttca[TCA_RATE]);\n\t\tif (err)\n\t\t\tgoto destroy_class;\n\t}\n\n\tif (cl->qdisc != &noop_qdisc)\n\t\tqdisc_hash_add(cl->qdisc, true);\n\nset_change_agg:\n\tsch_tree_lock(sch);\n\tnew_agg = qfq_find_agg(q, lmax, weight);\n\tif (new_agg == NULL) { /* create new aggregate */\n\t\tsch_tree_unlock(sch);\n\t\tnew_agg = kzalloc(sizeof(*new_agg), GFP_KERNEL);\n\t\tif (new_agg == NULL) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgen_kill_estimator(&cl->rate_est);\n\t\t\tgoto destroy_class;\n\t\t}\n\t\tsch_tree_lock(sch);\n\t\tqfq_init_agg(q, new_agg, lmax, weight);\n\t}\n\tif (existing)\n\t\tqfq_deact_rm_from_agg(q, cl);\n\telse\n\t\tqdisc_class_hash_insert(&q->clhash, &cl->common);\n\tqfq_add_to_agg(q, new_agg, cl);\n\tsch_tree_unlock(sch);\n\tqdisc_class_hash_grow(sch, &q->clhash);\n\n\t*arg = (unsigned long)cl;\n\treturn 0;\n\ndestroy_class:\n\tqdisc_put(cl->qdisc);\n\tkfree(cl);\n\treturn err;\n}\n\nstatic void qfq_destroy_class(struct Qdisc *sch, struct qfq_class *cl)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\n\tqfq_rm_from_agg(q, cl);\n\tgen_kill_estimator(&cl->rate_est);\n\tqdisc_put(cl->qdisc);\n\tkfree(cl);\n}\n\nstatic int qfq_delete_class(struct Qdisc *sch, unsigned long arg,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\n\tif (cl->filter_cnt > 0)\n\t\treturn -EBUSY;\n\n\tsch_tree_lock(sch);\n\n\tqdisc_purge_queue(cl->qdisc);\n\tqdisc_class_hash_remove(&q->clhash, &cl->common);\n\n\tsch_tree_unlock(sch);\n\n\tqfq_destroy_class(sch, cl);\n\treturn 0;\n}\n\nstatic unsigned long qfq_search_class(struct Qdisc *sch, u32 classid)\n{\n\treturn (unsigned long)qfq_find_class(sch, classid);\n}\n\nstatic struct tcf_block *qfq_tcf_block(struct Qdisc *sch, unsigned long cl,\n\t\t\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\n\tif (cl)\n\t\treturn NULL;\n\n\treturn q->block;\n}\n\nstatic unsigned long qfq_bind_tcf(struct Qdisc *sch, unsigned long parent,\n\t\t\t\t  u32 classid)\n{\n\tstruct qfq_class *cl = qfq_find_class(sch, classid);\n\n\tif (cl != NULL)\n\t\tcl->filter_cnt++;\n\n\treturn (unsigned long)cl;\n}\n\nstatic void qfq_unbind_tcf(struct Qdisc *sch, unsigned long arg)\n{\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\n\tcl->filter_cnt--;\n}\n\nstatic int qfq_graft_class(struct Qdisc *sch, unsigned long arg,\n\t\t\t   struct Qdisc *new, struct Qdisc **old,\n\t\t\t   struct netlink_ext_ack *extack)\n{\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\n\tif (new == NULL) {\n\t\tnew = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t\tcl->common.classid, NULL);\n\t\tif (new == NULL)\n\t\t\tnew = &noop_qdisc;\n\t}\n\n\t*old = qdisc_replace(sch, new, &cl->qdisc);\n\treturn 0;\n}\n\nstatic struct Qdisc *qfq_class_leaf(struct Qdisc *sch, unsigned long arg)\n{\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\n\treturn cl->qdisc;\n}\n\nstatic int qfq_dump_class(struct Qdisc *sch, unsigned long arg,\n\t\t\t  struct sk_buff *skb, struct tcmsg *tcm)\n{\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\tstruct nlattr *nest;\n\n\ttcm->tcm_parent\t= TC_H_ROOT;\n\ttcm->tcm_handle\t= cl->common.classid;\n\ttcm->tcm_info\t= cl->qdisc->handle;\n\n\tnest = nla_nest_start_noflag(skb, TCA_OPTIONS);\n\tif (nest == NULL)\n\t\tgoto nla_put_failure;\n\tif (nla_put_u32(skb, TCA_QFQ_WEIGHT, cl->agg->class_weight) ||\n\t    nla_put_u32(skb, TCA_QFQ_LMAX, cl->agg->lmax))\n\t\tgoto nla_put_failure;\n\treturn nla_nest_end(skb, nest);\n\nnla_put_failure:\n\tnla_nest_cancel(skb, nest);\n\treturn -EMSGSIZE;\n}\n\nstatic int qfq_dump_class_stats(struct Qdisc *sch, unsigned long arg,\n\t\t\t\tstruct gnet_dump *d)\n{\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\tstruct tc_qfq_stats xstats;\n\n\tmemset(&xstats, 0, sizeof(xstats));\n\n\txstats.weight = cl->agg->class_weight;\n\txstats.lmax = cl->agg->lmax;\n\n\tif (gnet_stats_copy_basic(d, NULL, &cl->bstats, true) < 0 ||\n\t    gnet_stats_copy_rate_est(d, &cl->rate_est) < 0 ||\n\t    qdisc_qstats_copy(d, cl->qdisc) < 0)\n\t\treturn -1;\n\n\treturn gnet_stats_copy_app(d, &xstats, sizeof(xstats));\n}\n\nstatic void qfq_walk(struct Qdisc *sch, struct qdisc_walker *arg)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl;\n\tunsigned int i;\n\n\tif (arg->stop)\n\t\treturn;\n\n\tfor (i = 0; i < q->clhash.hashsize; i++) {\n\t\thlist_for_each_entry(cl, &q->clhash.hash[i], common.hnode) {\n\t\t\tif (!tc_qdisc_stats_dump(sch, (unsigned long)cl, arg))\n\t\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic struct qfq_class *qfq_classify(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t\t      int *qerr)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl;\n\tstruct tcf_result res;\n\tstruct tcf_proto *fl;\n\tint result;\n\n\tif (TC_H_MAJ(skb->priority ^ sch->handle) == 0) {\n\t\tpr_debug(\"qfq_classify: found %d\\n\", skb->priority);\n\t\tcl = qfq_find_class(sch, skb->priority);\n\t\tif (cl != NULL)\n\t\t\treturn cl;\n\t}\n\n\t*qerr = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\n\tfl = rcu_dereference_bh(q->filter_list);\n\tresult = tcf_classify(skb, NULL, fl, &res, false);\n\tif (result >= 0) {\n#ifdef CONFIG_NET_CLS_ACT\n\t\tswitch (result) {\n\t\tcase TC_ACT_QUEUED:\n\t\tcase TC_ACT_STOLEN:\n\t\tcase TC_ACT_TRAP:\n\t\t\t*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\n\t\t\tfallthrough;\n\t\tcase TC_ACT_SHOT:\n\t\t\treturn NULL;\n\t\t}\n#endif\n\t\tcl = (struct qfq_class *)res.class;\n\t\tif (cl == NULL)\n\t\t\tcl = qfq_find_class(sch, res.classid);\n\t\treturn cl;\n\t}\n\n\treturn NULL;\n}\n\n/* Generic comparison function, handling wraparound. */\nstatic inline int qfq_gt(u64 a, u64 b)\n{\n\treturn (s64)(a - b) > 0;\n}\n\n/* Round a precise timestamp to its slotted value. */\nstatic inline u64 qfq_round_down(u64 ts, unsigned int shift)\n{\n\treturn ts & ~((1ULL << shift) - 1);\n}\n\n/* return the pointer to the group with lowest index in the bitmap */\nstatic inline struct qfq_group *qfq_ffs(struct qfq_sched *q,\n\t\t\t\t\tunsigned long bitmap)\n{\n\tint index = __ffs(bitmap);\n\treturn &q->groups[index];\n}\n/* Calculate a mask to mimic what would be ffs_from(). */\nstatic inline unsigned long mask_from(unsigned long bitmap, int from)\n{\n\treturn bitmap & ~((1UL << from) - 1);\n}\n\n/*\n * The state computation relies on ER=0, IR=1, EB=2, IB=3\n * First compute eligibility comparing grp->S, q->V,\n * then check if someone is blocking us and possibly add EB\n */\nstatic int qfq_calc_state(struct qfq_sched *q, const struct qfq_group *grp)\n{\n\t/* if S > V we are not eligible */\n\tunsigned int state = qfq_gt(grp->S, q->V);\n\tunsigned long mask = mask_from(q->bitmaps[ER], grp->index);\n\tstruct qfq_group *next;\n\n\tif (mask) {\n\t\tnext = qfq_ffs(q, mask);\n\t\tif (qfq_gt(grp->F, next->F))\n\t\t\tstate |= EB;\n\t}\n\n\treturn state;\n}\n\n\n/*\n * In principle\n *\tq->bitmaps[dst] |= q->bitmaps[src] & mask;\n *\tq->bitmaps[src] &= ~mask;\n * but we should make sure that src != dst\n */\nstatic inline void qfq_move_groups(struct qfq_sched *q, unsigned long mask,\n\t\t\t\t   int src, int dst)\n{\n\tq->bitmaps[dst] |= q->bitmaps[src] & mask;\n\tq->bitmaps[src] &= ~mask;\n}\n\nstatic void qfq_unblock_groups(struct qfq_sched *q, int index, u64 old_F)\n{\n\tunsigned long mask = mask_from(q->bitmaps[ER], index + 1);\n\tstruct qfq_group *next;\n\n\tif (mask) {\n\t\tnext = qfq_ffs(q, mask);\n\t\tif (!qfq_gt(next->F, old_F))\n\t\t\treturn;\n\t}\n\n\tmask = (1UL << index) - 1;\n\tqfq_move_groups(q, mask, EB, ER);\n\tqfq_move_groups(q, mask, IB, IR);\n}\n\n/*\n * perhaps\n *\n\told_V ^= q->V;\n\told_V >>= q->min_slot_shift;\n\tif (old_V) {\n\t\t...\n\t}\n *\n */\nstatic void qfq_make_eligible(struct qfq_sched *q)\n{\n\tunsigned long vslot = q->V >> q->min_slot_shift;\n\tunsigned long old_vslot = q->oldV >> q->min_slot_shift;\n\n\tif (vslot != old_vslot) {\n\t\tunsigned long mask;\n\t\tint last_flip_pos = fls(vslot ^ old_vslot);\n\n\t\tif (last_flip_pos > 31) /* higher than the number of groups */\n\t\t\tmask = ~0UL;    /* make all groups eligible */\n\t\telse\n\t\t\tmask = (1UL << last_flip_pos) - 1;\n\n\t\tqfq_move_groups(q, mask, IR, ER);\n\t\tqfq_move_groups(q, mask, IB, EB);\n\t}\n}\n\n/*\n * The index of the slot in which the input aggregate agg is to be\n * inserted must not be higher than QFQ_MAX_SLOTS-2. There is a '-2'\n * and not a '-1' because the start time of the group may be moved\n * backward by one slot after the aggregate has been inserted, and\n * this would cause non-empty slots to be right-shifted by one\n * position.\n *\n * QFQ+ fully satisfies this bound to the slot index if the parameters\n * of the classes are not changed dynamically, and if QFQ+ never\n * happens to postpone the service of agg unjustly, i.e., it never\n * happens that the aggregate becomes backlogged and eligible, or just\n * eligible, while an aggregate with a higher approximated finish time\n * is being served. In particular, in this case QFQ+ guarantees that\n * the timestamps of agg are low enough that the slot index is never\n * higher than 2. Unfortunately, QFQ+ cannot provide the same\n * guarantee if it happens to unjustly postpone the service of agg, or\n * if the parameters of some class are changed.\n *\n * As for the first event, i.e., an out-of-order service, the\n * upper bound to the slot index guaranteed by QFQ+ grows to\n * 2 +\n * QFQ_MAX_AGG_CLASSES * ((1<<QFQ_MTU_SHIFT)/QFQ_MIN_LMAX) *\n * (current_max_weight/current_wsum) <= 2 + 8 * 128 * 1.\n *\n * The following function deals with this problem by backward-shifting\n * the timestamps of agg, if needed, so as to guarantee that the slot\n * index is never higher than QFQ_MAX_SLOTS-2. This backward-shift may\n * cause the service of other aggregates to be postponed, yet the\n * worst-case guarantees of these aggregates are not violated.  In\n * fact, in case of no out-of-order service, the timestamps of agg\n * would have been even lower than they are after the backward shift,\n * because QFQ+ would have guaranteed a maximum value equal to 2 for\n * the slot index, and 2 < QFQ_MAX_SLOTS-2. Hence the aggregates whose\n * service is postponed because of the backward-shift would have\n * however waited for the service of agg before being served.\n *\n * The other event that may cause the slot index to be higher than 2\n * for agg is a recent change of the parameters of some class. If the\n * weight of a class is increased or the lmax (max_pkt_size) of the\n * class is decreased, then a new aggregate with smaller slot size\n * than the original parent aggregate of the class may happen to be\n * activated. The activation of this aggregate should be properly\n * delayed to when the service of the class has finished in the ideal\n * system tracked by QFQ+. If the activation of the aggregate is not\n * delayed to this reference time instant, then this aggregate may be\n * unjustly served before other aggregates waiting for service. This\n * may cause the above bound to the slot index to be violated for some\n * of these unlucky aggregates.\n *\n * Instead of delaying the activation of the new aggregate, which is\n * quite complex, the above-discussed capping of the slot index is\n * used to handle also the consequences of a change of the parameters\n * of a class.\n */\nstatic void qfq_slot_insert(struct qfq_group *grp, struct qfq_aggregate *agg,\n\t\t\t    u64 roundedS)\n{\n\tu64 slot = (roundedS - grp->S) >> grp->slot_shift;\n\tunsigned int i; /* slot index in the bucket list */\n\n\tif (unlikely(slot > QFQ_MAX_SLOTS - 2)) {\n\t\tu64 deltaS = roundedS - grp->S -\n\t\t\t((u64)(QFQ_MAX_SLOTS - 2)<<grp->slot_shift);\n\t\tagg->S -= deltaS;\n\t\tagg->F -= deltaS;\n\t\tslot = QFQ_MAX_SLOTS - 2;\n\t}\n\n\ti = (grp->front + slot) % QFQ_MAX_SLOTS;\n\n\thlist_add_head(&agg->next, &grp->slots[i]);\n\t__set_bit(slot, &grp->full_slots);\n}\n\n/* Maybe introduce hlist_first_entry?? */\nstatic struct qfq_aggregate *qfq_slot_head(struct qfq_group *grp)\n{\n\treturn hlist_entry(grp->slots[grp->front].first,\n\t\t\t   struct qfq_aggregate, next);\n}\n\n/*\n * remove the entry from the slot\n */\nstatic void qfq_front_slot_remove(struct qfq_group *grp)\n{\n\tstruct qfq_aggregate *agg = qfq_slot_head(grp);\n\n\tBUG_ON(!agg);\n\thlist_del(&agg->next);\n\tif (hlist_empty(&grp->slots[grp->front]))\n\t\t__clear_bit(0, &grp->full_slots);\n}\n\n/*\n * Returns the first aggregate in the first non-empty bucket of the\n * group. As a side effect, adjusts the bucket list so the first\n * non-empty bucket is at position 0 in full_slots.\n */\nstatic struct qfq_aggregate *qfq_slot_scan(struct qfq_group *grp)\n{\n\tunsigned int i;\n\n\tpr_debug(\"qfq slot_scan: grp %u full %#lx\\n\",\n\t\t grp->index, grp->full_slots);\n\n\tif (grp->full_slots == 0)\n\t\treturn NULL;\n\n\ti = __ffs(grp->full_slots);  /* zero based */\n\tif (i > 0) {\n\t\tgrp->front = (grp->front + i) % QFQ_MAX_SLOTS;\n\t\tgrp->full_slots >>= i;\n\t}\n\n\treturn qfq_slot_head(grp);\n}\n\n/*\n * adjust the bucket list. When the start time of a group decreases,\n * we move the index down (modulo QFQ_MAX_SLOTS) so we don't need to\n * move the objects. The mask of occupied slots must be shifted\n * because we use ffs() to find the first non-empty slot.\n * This covers decreases in the group's start time, but what about\n * increases of the start time ?\n * Here too we should make sure that i is less than 32\n */\nstatic void qfq_slot_rotate(struct qfq_group *grp, u64 roundedS)\n{\n\tunsigned int i = (grp->S - roundedS) >> grp->slot_shift;\n\n\tgrp->full_slots <<= i;\n\tgrp->front = (grp->front - i) % QFQ_MAX_SLOTS;\n}\n\nstatic void qfq_update_eligible(struct qfq_sched *q)\n{\n\tstruct qfq_group *grp;\n\tunsigned long ineligible;\n\n\tineligible = q->bitmaps[IR] | q->bitmaps[IB];\n\tif (ineligible) {\n\t\tif (!q->bitmaps[ER]) {\n\t\t\tgrp = qfq_ffs(q, ineligible);\n\t\t\tif (qfq_gt(grp->S, q->V))\n\t\t\t\tq->V = grp->S;\n\t\t}\n\t\tqfq_make_eligible(q);\n\t}\n}\n\n/* Dequeue head packet of the head class in the DRR queue of the aggregate. */\nstatic void agg_dequeue(struct qfq_aggregate *agg,\n\t\t\tstruct qfq_class *cl, unsigned int len)\n{\n\tqdisc_dequeue_peeked(cl->qdisc);\n\n\tcl->deficit -= (int) len;\n\n\tif (cl->qdisc->q.qlen == 0) /* no more packets, remove from list */\n\t\tlist_del(&cl->alist);\n\telse if (cl->deficit < qdisc_pkt_len(cl->qdisc->ops->peek(cl->qdisc))) {\n\t\tcl->deficit += agg->lmax;\n\t\tlist_move_tail(&cl->alist, &agg->active);\n\t}\n}\n\nstatic inline struct sk_buff *qfq_peek_skb(struct qfq_aggregate *agg,\n\t\t\t\t\t   struct qfq_class **cl,\n\t\t\t\t\t   unsigned int *len)\n{\n\tstruct sk_buff *skb;\n\n\t*cl = list_first_entry(&agg->active, struct qfq_class, alist);\n\tskb = (*cl)->qdisc->ops->peek((*cl)->qdisc);\n\tif (skb == NULL)\n\t\tWARN_ONCE(1, \"qfq_dequeue: non-workconserving leaf\\n\");\n\telse\n\t\t*len = qdisc_pkt_len(skb);\n\n\treturn skb;\n}\n\n/* Update F according to the actual service received by the aggregate. */\nstatic inline void charge_actual_service(struct qfq_aggregate *agg)\n{\n\t/* Compute the service received by the aggregate, taking into\n\t * account that, after decreasing the number of classes in\n\t * agg, it may happen that\n\t * agg->initial_budget - agg->budget > agg->bugdetmax\n\t */\n\tu32 service_received = min(agg->budgetmax,\n\t\t\t\t   agg->initial_budget - agg->budget);\n\n\tagg->F = agg->S + (u64)service_received * agg->inv_w;\n}\n\n/* Assign a reasonable start time for a new aggregate in group i.\n * Admissible values for \\hat(F) are multiples of \\sigma_i\n * no greater than V+\\sigma_i . Larger values mean that\n * we had a wraparound so we consider the timestamp to be stale.\n *\n * If F is not stale and F >= V then we set S = F.\n * Otherwise we should assign S = V, but this may violate\n * the ordering in EB (see [2]). So, if we have groups in ER,\n * set S to the F_j of the first group j which would be blocking us.\n * We are guaranteed not to move S backward because\n * otherwise our group i would still be blocked.\n */\nstatic void qfq_update_start(struct qfq_sched *q, struct qfq_aggregate *agg)\n{\n\tunsigned long mask;\n\tu64 limit, roundedF;\n\tint slot_shift = agg->grp->slot_shift;\n\n\troundedF = qfq_round_down(agg->F, slot_shift);\n\tlimit = qfq_round_down(q->V, slot_shift) + (1ULL << slot_shift);\n\n\tif (!qfq_gt(agg->F, q->V) || qfq_gt(roundedF, limit)) {\n\t\t/* timestamp was stale */\n\t\tmask = mask_from(q->bitmaps[ER], agg->grp->index);\n\t\tif (mask) {\n\t\t\tstruct qfq_group *next = qfq_ffs(q, mask);\n\t\t\tif (qfq_gt(roundedF, next->F)) {\n\t\t\t\tif (qfq_gt(limit, next->F))\n\t\t\t\t\tagg->S = next->F;\n\t\t\t\telse /* preserve timestamp correctness */\n\t\t\t\t\tagg->S = limit;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\tagg->S = q->V;\n\t} else  /* timestamp is not stale */\n\t\tagg->S = agg->F;\n}\n\n/* Update the timestamps of agg before scheduling/rescheduling it for\n * service.  In particular, assign to agg->F its maximum possible\n * value, i.e., the virtual finish time with which the aggregate\n * should be labeled if it used all its budget once in service.\n */\nstatic inline void\nqfq_update_agg_ts(struct qfq_sched *q,\n\t\t    struct qfq_aggregate *agg, enum update_reason reason)\n{\n\tif (reason != requeue)\n\t\tqfq_update_start(q, agg);\n\telse /* just charge agg for the service received */\n\t\tagg->S = agg->F;\n\n\tagg->F = agg->S + (u64)agg->budgetmax * agg->inv_w;\n}\n\nstatic void qfq_schedule_agg(struct qfq_sched *q, struct qfq_aggregate *agg);\n\nstatic struct sk_buff *qfq_dequeue(struct Qdisc *sch)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_aggregate *in_serv_agg = q->in_serv_agg;\n\tstruct qfq_class *cl;\n\tstruct sk_buff *skb = NULL;\n\t/* next-packet len, 0 means no more active classes in in-service agg */\n\tunsigned int len = 0;\n\n\tif (in_serv_agg == NULL)\n\t\treturn NULL;\n\n\tif (!list_empty(&in_serv_agg->active))\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\n\t/*\n\t * If there are no active classes in the in-service aggregate,\n\t * or if the aggregate has not enough budget to serve its next\n\t * class, then choose the next aggregate to serve.\n\t */\n\tif (len == 0 || in_serv_agg->budget < len) {\n\t\tcharge_actual_service(in_serv_agg);\n\n\t\t/* recharge the budget of the aggregate */\n\t\tin_serv_agg->initial_budget = in_serv_agg->budget =\n\t\t\tin_serv_agg->budgetmax;\n\n\t\tif (!list_empty(&in_serv_agg->active)) {\n\t\t\t/*\n\t\t\t * Still active: reschedule for\n\t\t\t * service. Possible optimization: if no other\n\t\t\t * aggregate is active, then there is no point\n\t\t\t * in rescheduling this aggregate, and we can\n\t\t\t * just keep it as the in-service one. This\n\t\t\t * should be however a corner case, and to\n\t\t\t * handle it, we would need to maintain an\n\t\t\t * extra num_active_aggs field.\n\t\t\t*/\n\t\t\tqfq_update_agg_ts(q, in_serv_agg, requeue);\n\t\t\tqfq_schedule_agg(q, in_serv_agg);\n\t\t} else if (sch->q.qlen == 0) { /* no aggregate to serve */\n\t\t\tq->in_serv_agg = NULL;\n\t\t\treturn NULL;\n\t\t}\n\n\t\t/*\n\t\t * If we get here, there are other aggregates queued:\n\t\t * choose the new aggregate to serve.\n\t\t */\n\t\tin_serv_agg = q->in_serv_agg = qfq_choose_next_agg(q);\n\t\tskb = qfq_peek_skb(in_serv_agg, &cl, &len);\n\t}\n\tif (!skb)\n\t\treturn NULL;\n\n\tqdisc_qstats_backlog_dec(sch, skb);\n\tsch->q.qlen--;\n\tqdisc_bstats_update(sch, skb);\n\n\tagg_dequeue(in_serv_agg, cl, len);\n\t/* If lmax is lowered, through qfq_change_class, for a class\n\t * owning pending packets with larger size than the new value\n\t * of lmax, then the following condition may hold.\n\t */\n\tif (unlikely(in_serv_agg->budget < len))\n\t\tin_serv_agg->budget = 0;\n\telse\n\t\tin_serv_agg->budget -= len;\n\n\tq->V += (u64)len * q->iwsum;\n\tpr_debug(\"qfq dequeue: len %u F %lld now %lld\\n\",\n\t\t len, (unsigned long long) in_serv_agg->F,\n\t\t (unsigned long long) q->V);\n\n\treturn skb;\n}\n\nstatic struct qfq_aggregate *qfq_choose_next_agg(struct qfq_sched *q)\n{\n\tstruct qfq_group *grp;\n\tstruct qfq_aggregate *agg, *new_front_agg;\n\tu64 old_F;\n\n\tqfq_update_eligible(q);\n\tq->oldV = q->V;\n\n\tif (!q->bitmaps[ER])\n\t\treturn NULL;\n\n\tgrp = qfq_ffs(q, q->bitmaps[ER]);\n\told_F = grp->F;\n\n\tagg = qfq_slot_head(grp);\n\n\t/* agg starts to be served, remove it from schedule */\n\tqfq_front_slot_remove(grp);\n\n\tnew_front_agg = qfq_slot_scan(grp);\n\n\tif (new_front_agg == NULL) /* group is now inactive, remove from ER */\n\t\t__clear_bit(grp->index, &q->bitmaps[ER]);\n\telse {\n\t\tu64 roundedS = qfq_round_down(new_front_agg->S,\n\t\t\t\t\t      grp->slot_shift);\n\t\tunsigned int s;\n\n\t\tif (grp->S == roundedS)\n\t\t\treturn agg;\n\t\tgrp->S = roundedS;\n\t\tgrp->F = roundedS + (2ULL << grp->slot_shift);\n\t\t__clear_bit(grp->index, &q->bitmaps[ER]);\n\t\ts = qfq_calc_state(q, grp);\n\t\t__set_bit(grp->index, &q->bitmaps[s]);\n\t}\n\n\tqfq_unblock_groups(q, grp->index, old_F);\n\n\treturn agg;\n}\n\nstatic int qfq_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t       struct sk_buff **to_free)\n{\n\tunsigned int len = qdisc_pkt_len(skb), gso_segs;\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl;\n\tstruct qfq_aggregate *agg;\n\tint err = 0;\n\tbool first;\n\n\tcl = qfq_classify(skb, sch, &err);\n\tif (cl == NULL) {\n\t\tif (err & __NET_XMIT_BYPASS)\n\t\t\tqdisc_qstats_drop(sch);\n\t\t__qdisc_drop(skb, to_free);\n\t\treturn err;\n\t}\n\tpr_debug(\"qfq_enqueue: cl = %x\\n\", cl->common.classid);\n\n\tif (unlikely(cl->agg->lmax < len)) {\n\t\tpr_debug(\"qfq: increasing maxpkt from %u to %u for class %u\",\n\t\t\t cl->agg->lmax, len, cl->common.classid);\n\t\terr = qfq_change_agg(sch, cl, cl->agg->class_weight, len);\n\t\tif (err) {\n\t\t\tcl->qstats.drops++;\n\t\t\treturn qdisc_drop(skb, sch, to_free);\n\t\t}\n\t}\n\n\tgso_segs = skb_is_gso(skb) ? skb_shinfo(skb)->gso_segs : 1;\n\tfirst = !cl->qdisc->q.qlen;\n\terr = qdisc_enqueue(skb, cl->qdisc, to_free);\n\tif (unlikely(err != NET_XMIT_SUCCESS)) {\n\t\tpr_debug(\"qfq_enqueue: enqueue failed %d\\n\", err);\n\t\tif (net_xmit_drop_count(err)) {\n\t\t\tcl->qstats.drops++;\n\t\t\tqdisc_qstats_drop(sch);\n\t\t}\n\t\treturn err;\n\t}\n\n\t_bstats_update(&cl->bstats, len, gso_segs);\n\tsch->qstats.backlog += len;\n\t++sch->q.qlen;\n\n\tagg = cl->agg;\n\t/* if the queue was not empty, then done here */\n\tif (!first) {\n\t\tif (unlikely(skb == cl->qdisc->ops->peek(cl->qdisc)) &&\n\t\t    list_first_entry(&agg->active, struct qfq_class, alist)\n\t\t    == cl && cl->deficit < len)\n\t\t\tlist_move_tail(&cl->alist, &agg->active);\n\n\t\treturn err;\n\t}\n\n\t/* schedule class for service within the aggregate */\n\tcl->deficit = agg->lmax;\n\tlist_add_tail(&cl->alist, &agg->active);\n\n\tif (list_first_entry(&agg->active, struct qfq_class, alist) != cl ||\n\t    q->in_serv_agg == agg)\n\t\treturn err; /* non-empty or in service, nothing else to do */\n\n\tqfq_activate_agg(q, agg, enqueue);\n\n\treturn err;\n}\n\n/*\n * Schedule aggregate according to its timestamps.\n */\nstatic void qfq_schedule_agg(struct qfq_sched *q, struct qfq_aggregate *agg)\n{\n\tstruct qfq_group *grp = agg->grp;\n\tu64 roundedS;\n\tint s;\n\n\troundedS = qfq_round_down(agg->S, grp->slot_shift);\n\n\t/*\n\t * Insert agg in the correct bucket.\n\t * If agg->S >= grp->S we don't need to adjust the\n\t * bucket list and simply go to the insertion phase.\n\t * Otherwise grp->S is decreasing, we must make room\n\t * in the bucket list, and also recompute the group state.\n\t * Finally, if there were no flows in this group and nobody\n\t * was in ER make sure to adjust V.\n\t */\n\tif (grp->full_slots) {\n\t\tif (!qfq_gt(grp->S, agg->S))\n\t\t\tgoto skip_update;\n\n\t\t/* create a slot for this agg->S */\n\t\tqfq_slot_rotate(grp, roundedS);\n\t\t/* group was surely ineligible, remove */\n\t\t__clear_bit(grp->index, &q->bitmaps[IR]);\n\t\t__clear_bit(grp->index, &q->bitmaps[IB]);\n\t} else if (!q->bitmaps[ER] && qfq_gt(roundedS, q->V) &&\n\t\t   q->in_serv_agg == NULL)\n\t\tq->V = roundedS;\n\n\tgrp->S = roundedS;\n\tgrp->F = roundedS + (2ULL << grp->slot_shift);\n\ts = qfq_calc_state(q, grp);\n\t__set_bit(grp->index, &q->bitmaps[s]);\n\n\tpr_debug(\"qfq enqueue: new state %d %#lx S %lld F %lld V %lld\\n\",\n\t\t s, q->bitmaps[s],\n\t\t (unsigned long long) agg->S,\n\t\t (unsigned long long) agg->F,\n\t\t (unsigned long long) q->V);\n\nskip_update:\n\tqfq_slot_insert(grp, agg, roundedS);\n}\n\n\n/* Update agg ts and schedule agg for service */\nstatic void qfq_activate_agg(struct qfq_sched *q, struct qfq_aggregate *agg,\n\t\t\t     enum update_reason reason)\n{\n\tagg->initial_budget = agg->budget = agg->budgetmax; /* recharge budg. */\n\n\tqfq_update_agg_ts(q, agg, reason);\n\tif (q->in_serv_agg == NULL) { /* no aggr. in service or scheduled */\n\t\tq->in_serv_agg = agg; /* start serving this aggregate */\n\t\t /* update V: to be in service, agg must be eligible */\n\t\tq->oldV = q->V = agg->S;\n\t} else if (agg != q->in_serv_agg)\n\t\tqfq_schedule_agg(q, agg);\n}\n\nstatic void qfq_slot_remove(struct qfq_sched *q, struct qfq_group *grp,\n\t\t\t    struct qfq_aggregate *agg)\n{\n\tunsigned int i, offset;\n\tu64 roundedS;\n\n\troundedS = qfq_round_down(agg->S, grp->slot_shift);\n\toffset = (roundedS - grp->S) >> grp->slot_shift;\n\n\ti = (grp->front + offset) % QFQ_MAX_SLOTS;\n\n\thlist_del(&agg->next);\n\tif (hlist_empty(&grp->slots[i]))\n\t\t__clear_bit(offset, &grp->full_slots);\n}\n\n/*\n * Called to forcibly deschedule an aggregate.  If the aggregate is\n * not in the front bucket, or if the latter has other aggregates in\n * the front bucket, we can simply remove the aggregate with no other\n * side effects.\n * Otherwise we must propagate the event up.\n */\nstatic void qfq_deactivate_agg(struct qfq_sched *q, struct qfq_aggregate *agg)\n{\n\tstruct qfq_group *grp = agg->grp;\n\tunsigned long mask;\n\tu64 roundedS;\n\tint s;\n\n\tif (agg == q->in_serv_agg) {\n\t\tcharge_actual_service(agg);\n\t\tq->in_serv_agg = qfq_choose_next_agg(q);\n\t\treturn;\n\t}\n\n\tagg->F = agg->S;\n\tqfq_slot_remove(q, grp, agg);\n\n\tif (!grp->full_slots) {\n\t\t__clear_bit(grp->index, &q->bitmaps[IR]);\n\t\t__clear_bit(grp->index, &q->bitmaps[EB]);\n\t\t__clear_bit(grp->index, &q->bitmaps[IB]);\n\n\t\tif (test_bit(grp->index, &q->bitmaps[ER]) &&\n\t\t    !(q->bitmaps[ER] & ~((1UL << grp->index) - 1))) {\n\t\t\tmask = q->bitmaps[ER] & ((1UL << grp->index) - 1);\n\t\t\tif (mask)\n\t\t\t\tmask = ~((1UL << __fls(mask)) - 1);\n\t\t\telse\n\t\t\t\tmask = ~0UL;\n\t\t\tqfq_move_groups(q, mask, EB, ER);\n\t\t\tqfq_move_groups(q, mask, IB, IR);\n\t\t}\n\t\t__clear_bit(grp->index, &q->bitmaps[ER]);\n\t} else if (hlist_empty(&grp->slots[grp->front])) {\n\t\tagg = qfq_slot_scan(grp);\n\t\troundedS = qfq_round_down(agg->S, grp->slot_shift);\n\t\tif (grp->S != roundedS) {\n\t\t\t__clear_bit(grp->index, &q->bitmaps[ER]);\n\t\t\t__clear_bit(grp->index, &q->bitmaps[IR]);\n\t\t\t__clear_bit(grp->index, &q->bitmaps[EB]);\n\t\t\t__clear_bit(grp->index, &q->bitmaps[IB]);\n\t\t\tgrp->S = roundedS;\n\t\t\tgrp->F = roundedS + (2ULL << grp->slot_shift);\n\t\t\ts = qfq_calc_state(q, grp);\n\t\t\t__set_bit(grp->index, &q->bitmaps[s]);\n\t\t}\n\t}\n}\n\nstatic void qfq_qlen_notify(struct Qdisc *sch, unsigned long arg)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl = (struct qfq_class *)arg;\n\n\tqfq_deactivate_class(q, cl);\n}\n\nstatic int qfq_init_qdisc(struct Qdisc *sch, struct nlattr *opt,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_group *grp;\n\tint i, j, err;\n\tu32 max_cl_shift, maxbudg_shift, max_classes;\n\n\terr = tcf_block_get(&q->block, &q->filter_list, sch, extack);\n\tif (err)\n\t\treturn err;\n\n\terr = qdisc_class_hash_init(&q->clhash);\n\tif (err < 0)\n\t\treturn err;\n\n\tmax_classes = min_t(u64, (u64)qdisc_dev(sch)->tx_queue_len + 1,\n\t\t\t    QFQ_MAX_AGG_CLASSES);\n\t/* max_cl_shift = floor(log_2(max_classes)) */\n\tmax_cl_shift = __fls(max_classes);\n\tq->max_agg_classes = 1<<max_cl_shift;\n\n\t/* maxbudg_shift = log2(max_len * max_classes_per_agg) */\n\tmaxbudg_shift = QFQ_MTU_SHIFT + max_cl_shift;\n\tq->min_slot_shift = FRAC_BITS + maxbudg_shift - QFQ_MAX_INDEX;\n\n\tfor (i = 0; i <= QFQ_MAX_INDEX; i++) {\n\t\tgrp = &q->groups[i];\n\t\tgrp->index = i;\n\t\tgrp->slot_shift = q->min_slot_shift + i;\n\t\tfor (j = 0; j < QFQ_MAX_SLOTS; j++)\n\t\t\tINIT_HLIST_HEAD(&grp->slots[j]);\n\t}\n\n\tINIT_HLIST_HEAD(&q->nonfull_aggs);\n\n\treturn 0;\n}\n\nstatic void qfq_reset_qdisc(struct Qdisc *sch)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl;\n\tunsigned int i;\n\n\tfor (i = 0; i < q->clhash.hashsize; i++) {\n\t\thlist_for_each_entry(cl, &q->clhash.hash[i], common.hnode) {\n\t\t\tif (cl->qdisc->q.qlen > 0)\n\t\t\t\tqfq_deactivate_class(q, cl);\n\n\t\t\tqdisc_reset(cl->qdisc);\n\t\t}\n\t}\n}\n\nstatic void qfq_destroy_qdisc(struct Qdisc *sch)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl;\n\tstruct hlist_node *next;\n\tunsigned int i;\n\n\ttcf_block_put(q->block);\n\n\tfor (i = 0; i < q->clhash.hashsize; i++) {\n\t\thlist_for_each_entry_safe(cl, next, &q->clhash.hash[i],\n\t\t\t\t\t  common.hnode) {\n\t\t\tqfq_destroy_class(sch, cl);\n\t\t}\n\t}\n\tqdisc_class_hash_destroy(&q->clhash);\n}\n\nstatic const struct Qdisc_class_ops qfq_class_ops = {\n\t.change\t\t= qfq_change_class,\n\t.delete\t\t= qfq_delete_class,\n\t.find\t\t= qfq_search_class,\n\t.tcf_block\t= qfq_tcf_block,\n\t.bind_tcf\t= qfq_bind_tcf,\n\t.unbind_tcf\t= qfq_unbind_tcf,\n\t.graft\t\t= qfq_graft_class,\n\t.leaf\t\t= qfq_class_leaf,\n\t.qlen_notify\t= qfq_qlen_notify,\n\t.dump\t\t= qfq_dump_class,\n\t.dump_stats\t= qfq_dump_class_stats,\n\t.walk\t\t= qfq_walk,\n};\n\nstatic struct Qdisc_ops qfq_qdisc_ops __read_mostly = {\n\t.cl_ops\t\t= &qfq_class_ops,\n\t.id\t\t= \"qfq\",\n\t.priv_size\t= sizeof(struct qfq_sched),\n\t.enqueue\t= qfq_enqueue,\n\t.dequeue\t= qfq_dequeue,\n\t.peek\t\t= qdisc_peek_dequeued,\n\t.init\t\t= qfq_init_qdisc,\n\t.reset\t\t= qfq_reset_qdisc,\n\t.destroy\t= qfq_destroy_qdisc,\n\t.owner\t\t= THIS_MODULE,\n};\n\nstatic int __init qfq_init(void)\n{\n\treturn register_qdisc(&qfq_qdisc_ops);\n}\n\nstatic void __exit qfq_exit(void)\n{\n\tunregister_qdisc(&qfq_qdisc_ops);\n}\n\nmodule_init(qfq_init);\nmodule_exit(qfq_exit);\nMODULE_LICENSE(\"GPL\");\n"], "filenames": ["net/sched/sch_qfq.c"], "buggy_code_start_loc": [424], "buggy_code_end_loc": [431], "fixing_code_start_loc": [424], "fixing_code_end_loc": [433], "type": "CWE-787", "message": "qfq_change_class in net/sched/sch_qfq.c in the Linux kernel before 6.2.13 allows an out-of-bounds write because lmax can exceed QFQ_MIN_LMAX.", "other": {"cve": {"id": "CVE-2023-31436", "sourceIdentifier": "cve@mitre.org", "published": "2023-04-28T02:15:09.007", "lastModified": "2023-06-09T08:15:11.370", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "qfq_change_class in net/sched/sch_qfq.c in the Linux kernel before 6.2.13 allows an out-of-bounds write because lmax can exceed QFQ_MIN_LMAX."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-787"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "6.2.13", "matchCriteriaId": "EA8B36B5-B3EC-431B-B425-1847BE9C994F"}]}]}], "references": [{"url": "https://cdn.kernel.org/pub/linux/kernel/v6.x/ChangeLog-6.2.13", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch"]}, {"url": "https://github.com/torvalds/linux/commit/3037933448f60f9acb705997eae62013ecb81e0d", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://lists.debian.org/debian-lts-announce/2023/06/msg00008.html", "source": "cve@mitre.org"}, {"url": "https://security.netapp.com/advisory/ntap-20230609-0001/", "source": "cve@mitre.org"}, {"url": "https://www.debian.org/security/2023/dsa-5402", "source": "cve@mitre.org"}, {"url": "https://www.spinics.net/lists/stable-commits/msg294885.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/3037933448f60f9acb705997eae62013ecb81e0d"}}