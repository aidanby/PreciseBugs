{"buggy_code": ["/*\n** gc.c - garbage collector for mruby\n**\n** See Copyright Notice in mruby.h\n*/\n\n#include <string.h>\n#include <stdlib.h>\n#include <mruby.h>\n#include <mruby/array.h>\n#include <mruby/class.h>\n#include <mruby/data.h>\n#include <mruby/istruct.h>\n#include <mruby/hash.h>\n#include <mruby/proc.h>\n#include <mruby/range.h>\n#include <mruby/string.h>\n#include <mruby/variable.h>\n#include <mruby/gc.h>\n#include <mruby/error.h>\n#include <mruby/throw.h>\n\n/*\n  = Tri-color Incremental Garbage Collection\n\n  mruby's GC is Tri-color Incremental GC with Mark & Sweep.\n  Algorithm details are omitted.\n  Instead, the implementation part is described below.\n\n  == Object's Color\n\n  Each object can be painted in three colors:\n\n    * White - Unmarked.\n    * Gray - Marked, But the child objects are unmarked.\n    * Black - Marked, the child objects are also marked.\n\n  == Two White Types\n\n  There're two white color types in a flip-flop fashion: White-A and White-B,\n  which respectively represent the Current White color (the newly allocated\n  objects in the current GC cycle) and the Sweep Target White color (the\n  dead objects to be swept).\n\n  A and B will be switched just at the beginning of the next GC cycle. At\n  that time, all the dead objects have been swept, while the newly created\n  objects in the current GC cycle which finally remains White are now\n  regarded as dead objects. Instead of traversing all the White-A objects and\n  painting them as White-B, just switch the meaning of White-A and White-B as\n  this will be much cheaper.\n\n  As a result, the objects we sweep in the current GC cycle are always\n  left from the previous GC cycle. This allows us to sweep objects\n  incrementally, without the disturbance of the newly created objects.\n\n  == Execution Timing\n\n  GC Execution Time and Each step interval are decided by live objects count.\n  List of Adjustment API:\n\n    * gc_interval_ratio_set\n    * gc_step_ratio_set\n\n  For details, see the comments for each function.\n\n  == Write Barrier\n\n  mruby implementer and C extension library writer must insert a write\n  barrier when updating a reference from a field of an object.\n  When updating a reference from a field of object A to object B,\n  two different types of write barrier are available:\n\n    * mrb_field_write_barrier - target B object for a mark.\n    * mrb_write_barrier       - target A object for a mark.\n\n  == Generational Mode\n\n  mruby's GC offers an Generational Mode while re-using the tri-color GC\n  infrastructure. It will treat the Black objects as Old objects after each\n  sweep phase, instead of painting them White. The key ideas are still the same\n  as traditional generational GC:\n\n    * Minor GC - just traverse the Young objects (Gray objects) in the mark\n                 phase, then only sweep the newly created objects, and leave\n                 the Old objects live.\n\n    * Major GC - same as a full regular GC cycle.\n\n  The difference from \"traditional\" generational GC is, that the major GC\n  in mruby is triggered incrementally in a tri-color manner.\n\n\n  For details, see the comments for each function.\n\n*/\n\nstruct free_obj {\n  MRB_OBJECT_HEADER;\n  struct RBasic *next;\n};\n\ntypedef struct {\n  union {\n    struct free_obj free;\n    struct RBasic basic;\n    struct RObject object;\n    struct RClass klass;\n    struct RString string;\n    struct RArray array;\n    struct RHash hash;\n    struct RRange range;\n    struct RData data;\n    struct RIStruct istruct;\n    struct RProc proc;\n    struct REnv env;\n    struct RFiber fiber;\n    struct RException exc;\n    struct RBreak brk;\n#ifdef MRB_WORD_BOXING\n#ifndef MRB_WITHOUT_FLOAT\n    struct RFloat floatv;\n#endif\n    struct RCptr cptr;\n#endif\n  } as;\n} RVALUE;\n\n#ifdef GC_PROFILE\n#include <stdio.h>\n#include <sys/time.h>\n\nstatic double program_invoke_time = 0;\nstatic double gc_time = 0;\nstatic double gc_total_time = 0;\n\nstatic double\ngettimeofday_time(void)\n{\n  struct timeval tv;\n  gettimeofday(&tv, NULL);\n  return tv.tv_sec + tv.tv_usec * 1e-6;\n}\n\n#define GC_INVOKE_TIME_REPORT(with) do {\\\n  fprintf(stderr, \"%s\\n\", with);\\\n  fprintf(stderr, \"gc_invoke: %19.3f\\n\", gettimeofday_time() - program_invoke_time);\\\n  fprintf(stderr, \"is_generational: %d\\n\", is_generational(gc));\\\n  fprintf(stderr, \"is_major_gc: %d\\n\", is_major_gc(gc));\\\n} while(0)\n\n#define GC_TIME_START do {\\\n  gc_time = gettimeofday_time();\\\n} while(0)\n\n#define GC_TIME_STOP_AND_REPORT do {\\\n  gc_time = gettimeofday_time() - gc_time;\\\n  gc_total_time += gc_time;\\\n  fprintf(stderr, \"gc_state: %d\\n\", gc->state);\\\n  fprintf(stderr, \"live: %zu\\n\", gc->live);\\\n  fprintf(stderr, \"majorgc_old_threshold: %zu\\n\", gc->majorgc_old_threshold);\\\n  fprintf(stderr, \"gc_threshold: %zu\\n\", gc->threshold);\\\n  fprintf(stderr, \"gc_time: %30.20f\\n\", gc_time);\\\n  fprintf(stderr, \"gc_total_time: %30.20f\\n\\n\", gc_total_time);\\\n} while(0)\n#else\n#define GC_INVOKE_TIME_REPORT(s)\n#define GC_TIME_START\n#define GC_TIME_STOP_AND_REPORT\n#endif\n\n#ifdef GC_DEBUG\n#define DEBUG(x) (x)\n#else\n#define DEBUG(x)\n#endif\n\n#ifndef MRB_HEAP_PAGE_SIZE\n#define MRB_HEAP_PAGE_SIZE 1024\n#endif\n\n#define GC_STEP_SIZE 1024\n\n/* white: 001 or 010, black: 100, gray: 000 */\n#define GC_GRAY 0\n#define GC_WHITE_A 1\n#define GC_WHITE_B (1 << 1)\n#define GC_BLACK (1 << 2)\n#define GC_WHITES (GC_WHITE_A | GC_WHITE_B)\n#define GC_COLOR_MASK 7\n\n#define paint_gray(o) ((o)->color = GC_GRAY)\n#define paint_black(o) ((o)->color = GC_BLACK)\n#define paint_white(o) ((o)->color = GC_WHITES)\n#define paint_partial_white(s, o) ((o)->color = (s)->current_white_part)\n#define is_gray(o) ((o)->color == GC_GRAY)\n#define is_white(o) ((o)->color & GC_WHITES)\n#define is_black(o) ((o)->color & GC_BLACK)\n#define flip_white_part(s) ((s)->current_white_part = other_white_part(s))\n#define other_white_part(s) ((s)->current_white_part ^ GC_WHITES)\n#define is_dead(s, o) (((o)->color & other_white_part(s) & GC_WHITES) || (o)->tt == MRB_TT_FREE)\n\n#define objects(p) ((RVALUE *)p->objects)\n\nmrb_noreturn void mrb_raise_nomemory(mrb_state *mrb);\n\nMRB_API void*\nmrb_realloc_simple(mrb_state *mrb, void *p,  size_t len)\n{\n  void *p2;\n\n  p2 = (mrb->allocf)(mrb, p, len, mrb->allocf_ud);\n  if (!p2 && len > 0 && mrb->gc.heaps) {\n    mrb_full_gc(mrb);\n    p2 = (mrb->allocf)(mrb, p, len, mrb->allocf_ud);\n  }\n\n  return p2;\n}\n\nMRB_API void*\nmrb_realloc(mrb_state *mrb, void *p, size_t len)\n{\n  void *p2;\n\n  p2 = mrb_realloc_simple(mrb, p, len);\n  if (len == 0) return p2;\n  if (p2 == NULL) {\n    mrb_free(mrb, p);\n    mrb->gc.out_of_memory = TRUE;\n    mrb_raise_nomemory(mrb);\n  }\n  else {\n    mrb->gc.out_of_memory = FALSE;\n  }\n\n  return p2;\n}\n\nMRB_API void*\nmrb_malloc(mrb_state *mrb, size_t len)\n{\n  return mrb_realloc(mrb, 0, len);\n}\n\nMRB_API void*\nmrb_malloc_simple(mrb_state *mrb, size_t len)\n{\n  return mrb_realloc_simple(mrb, 0, len);\n}\n\nMRB_API void*\nmrb_calloc(mrb_state *mrb, size_t nelem, size_t len)\n{\n  void *p;\n\n  if (nelem > 0 && len > 0 &&\n      nelem <= SIZE_MAX / len) {\n    size_t size;\n    size = nelem * len;\n    p = mrb_malloc(mrb, size);\n\n    memset(p, 0, size);\n  }\n  else {\n    p = NULL;\n  }\n\n  return p;\n}\n\nMRB_API void\nmrb_free(mrb_state *mrb, void *p)\n{\n  (mrb->allocf)(mrb, p, 0, mrb->allocf_ud);\n}\n\nMRB_API void*\nmrb_alloca(mrb_state *mrb, size_t size)\n{\n  struct RString *s;\n  s = (struct RString*)mrb_obj_alloc(mrb, MRB_TT_STRING, mrb->string_class);\n  return s->as.heap.ptr = (char*)mrb_malloc(mrb, size);\n}\n\nstatic mrb_bool\nheap_p(mrb_gc *gc, struct RBasic *object)\n{\n  mrb_heap_page* page;\n\n  page = gc->heaps;\n  while (page) {\n    RVALUE *p;\n\n    p = objects(page);\n    if (&p[0].as.basic <= object && object <= &p[MRB_HEAP_PAGE_SIZE].as.basic) {\n      return TRUE;\n    }\n    page = page->next;\n  }\n  return FALSE;\n}\n\nMRB_API mrb_bool\nmrb_object_dead_p(mrb_state *mrb, struct RBasic *object) {\n  mrb_gc *gc = &mrb->gc;\n  if (!heap_p(gc, object)) return TRUE;\n  return is_dead(gc, object);\n}\n\nstatic void\nlink_heap_page(mrb_gc *gc, mrb_heap_page *page)\n{\n  page->next = gc->heaps;\n  if (gc->heaps)\n    gc->heaps->prev = page;\n  gc->heaps = page;\n}\n\nstatic void\nunlink_heap_page(mrb_gc *gc, mrb_heap_page *page)\n{\n  if (page->prev)\n    page->prev->next = page->next;\n  if (page->next)\n    page->next->prev = page->prev;\n  if (gc->heaps == page)\n    gc->heaps = page->next;\n  page->prev = NULL;\n  page->next = NULL;\n}\n\nstatic void\nlink_free_heap_page(mrb_gc *gc, mrb_heap_page *page)\n{\n  page->free_next = gc->free_heaps;\n  if (gc->free_heaps) {\n    gc->free_heaps->free_prev = page;\n  }\n  gc->free_heaps = page;\n}\n\nstatic void\nunlink_free_heap_page(mrb_gc *gc, mrb_heap_page *page)\n{\n  if (page->free_prev)\n    page->free_prev->free_next = page->free_next;\n  if (page->free_next)\n    page->free_next->free_prev = page->free_prev;\n  if (gc->free_heaps == page)\n    gc->free_heaps = page->free_next;\n  page->free_prev = NULL;\n  page->free_next = NULL;\n}\n\nstatic void\nadd_heap(mrb_state *mrb, mrb_gc *gc)\n{\n  mrb_heap_page *page = (mrb_heap_page *)mrb_calloc(mrb, 1, sizeof(mrb_heap_page) + MRB_HEAP_PAGE_SIZE * sizeof(RVALUE));\n  RVALUE *p, *e;\n  struct RBasic *prev = NULL;\n\n  for (p = objects(page), e=p+MRB_HEAP_PAGE_SIZE; p<e; p++) {\n    p->as.free.tt = MRB_TT_FREE;\n    p->as.free.next = prev;\n    prev = &p->as.basic;\n  }\n  page->freelist = prev;\n\n  link_heap_page(gc, page);\n  link_free_heap_page(gc, page);\n}\n\n#define DEFAULT_GC_INTERVAL_RATIO 200\n#define DEFAULT_GC_STEP_RATIO 200\n#define MAJOR_GC_INC_RATIO 120\n#define MAJOR_GC_TOOMANY 10000\n#define is_generational(gc) ((gc)->generational)\n#define is_major_gc(gc) (is_generational(gc) && (gc)->full)\n#define is_minor_gc(gc) (is_generational(gc) && !(gc)->full)\n\nvoid\nmrb_gc_init(mrb_state *mrb, mrb_gc *gc)\n{\n#ifndef MRB_GC_FIXED_ARENA\n  gc->arena = (struct RBasic**)mrb_malloc(mrb, sizeof(struct RBasic*)*MRB_GC_ARENA_SIZE);\n  gc->arena_capa = MRB_GC_ARENA_SIZE;\n#endif\n\n  gc->current_white_part = GC_WHITE_A;\n  gc->heaps = NULL;\n  gc->free_heaps = NULL;\n  add_heap(mrb, gc);\n  gc->interval_ratio = DEFAULT_GC_INTERVAL_RATIO;\n  gc->step_ratio = DEFAULT_GC_STEP_RATIO;\n#ifndef MRB_GC_TURN_OFF_GENERATIONAL\n  gc->generational = TRUE;\n  gc->full = TRUE;\n#endif\n\n#ifdef GC_PROFILE\n  program_invoke_time = gettimeofday_time();\n#endif\n}\n\nstatic void obj_free(mrb_state *mrb, struct RBasic *obj, int end);\n\nstatic void\nfree_heap(mrb_state *mrb, mrb_gc *gc)\n{\n  mrb_heap_page *page = gc->heaps;\n  mrb_heap_page *tmp;\n  RVALUE *p, *e;\n\n  while (page) {\n    tmp = page;\n    page = page->next;\n    for (p = objects(tmp), e=p+MRB_HEAP_PAGE_SIZE; p<e; p++) {\n      if (p->as.free.tt != MRB_TT_FREE)\n        obj_free(mrb, &p->as.basic, TRUE);\n    }\n    mrb_free(mrb, tmp);\n  }\n}\n\nvoid\nmrb_gc_destroy(mrb_state *mrb, mrb_gc *gc)\n{\n  free_heap(mrb, gc);\n#ifndef MRB_GC_FIXED_ARENA\n  mrb_free(mrb, gc->arena);\n#endif\n}\n\nstatic void\ngc_protect(mrb_state *mrb, mrb_gc *gc, struct RBasic *p)\n{\n#ifdef MRB_GC_FIXED_ARENA\n  if (gc->arena_idx >= MRB_GC_ARENA_SIZE) {\n    /* arena overflow error */\n    gc->arena_idx = MRB_GC_ARENA_SIZE - 4; /* force room in arena */\n    mrb_exc_raise(mrb, mrb_obj_value(mrb->arena_err));\n  }\n#else\n  if (gc->arena_idx >= gc->arena_capa) {\n    /* extend arena */\n    gc->arena_capa = (int)(gc->arena_capa * 3 / 2);\n    gc->arena = (struct RBasic**)mrb_realloc(mrb, gc->arena, sizeof(struct RBasic*)*gc->arena_capa);\n  }\n#endif\n  gc->arena[gc->arena_idx++] = p;\n}\n\n/* mrb_gc_protect() leaves the object in the arena */\nMRB_API void\nmrb_gc_protect(mrb_state *mrb, mrb_value obj)\n{\n  if (mrb_immediate_p(obj)) return;\n  gc_protect(mrb, &mrb->gc, mrb_basic_ptr(obj));\n}\n\n#define GC_ROOT_NAME \"_gc_root_\"\n\n/* mrb_gc_register() keeps the object from GC.\n\n   Register your object when it's exported to C world,\n   without reference from Ruby world, e.g. callback\n   arguments.  Don't forget to remove the object using\n   mrb_gc_unregister, otherwise your object will leak.\n*/\n\nMRB_API void\nmrb_gc_register(mrb_state *mrb, mrb_value obj)\n{\n  mrb_sym root;\n  mrb_value table;\n\n  if (mrb_immediate_p(obj)) return;\n  root = mrb_intern_lit(mrb, GC_ROOT_NAME);\n  table = mrb_gv_get(mrb, root);\n  if (mrb_nil_p(table) || !mrb_array_p(table)) {\n    table = mrb_ary_new(mrb);\n    mrb_gv_set(mrb, root, table);\n  }\n  mrb_ary_push(mrb, table, obj);\n}\n\n/* mrb_gc_unregister() removes the object from GC root. */\nMRB_API void\nmrb_gc_unregister(mrb_state *mrb, mrb_value obj)\n{\n  mrb_sym root;\n  mrb_value table;\n  struct RArray *a;\n  mrb_int i;\n\n  if (mrb_immediate_p(obj)) return;\n  root = mrb_intern_lit(mrb, GC_ROOT_NAME);\n  table = mrb_gv_get(mrb, root);\n  if (mrb_nil_p(table)) return;\n  if (!mrb_array_p(table)) {\n    mrb_gv_set(mrb, root, mrb_nil_value());\n    return;\n  }\n  a = mrb_ary_ptr(table);\n  mrb_ary_modify(mrb, a);\n  for (i = 0; i < ARY_LEN(a); i++) {\n    if (mrb_ptr(ARY_PTR(a)[i]) == mrb_ptr(obj)) {\n      mrb_int len = ARY_LEN(a)-1;\n      mrb_value *ptr = ARY_PTR(a);\n\n      ARY_SET_LEN(a, len);\n      memmove(&ptr[i], &ptr[i + 1], (len - i) * sizeof(mrb_value));\n      break;\n    }\n  }\n}\n\nMRB_API struct RBasic*\nmrb_obj_alloc(mrb_state *mrb, enum mrb_vtype ttype, struct RClass *cls)\n{\n  struct RBasic *p;\n  static const RVALUE RVALUE_zero = { { { NULL, NULL, MRB_TT_FALSE } } };\n  mrb_gc *gc = &mrb->gc;\n\n  if (cls) {\n    enum mrb_vtype tt;\n\n    switch (cls->tt) {\n    case MRB_TT_CLASS:\n    case MRB_TT_SCLASS:\n    case MRB_TT_MODULE:\n    case MRB_TT_ENV:\n      break;\n    default:\n      mrb_raise(mrb, E_TYPE_ERROR, \"allocation failure\");\n    }\n    tt = MRB_INSTANCE_TT(cls);\n    if (tt != MRB_TT_FALSE &&\n        ttype != MRB_TT_SCLASS &&\n        ttype != MRB_TT_ICLASS &&\n        ttype != MRB_TT_ENV &&\n        ttype != tt) {\n      mrb_raisef(mrb, E_TYPE_ERROR, \"allocation failure of %C\", cls);\n    }\n  }\n\n#ifdef MRB_GC_STRESS\n  mrb_full_gc(mrb);\n#endif\n  if (gc->threshold < gc->live) {\n    mrb_incremental_gc(mrb);\n  }\n  if (gc->free_heaps == NULL) {\n    add_heap(mrb, gc);\n  }\n\n  p = gc->free_heaps->freelist;\n  gc->free_heaps->freelist = ((struct free_obj*)p)->next;\n  if (gc->free_heaps->freelist == NULL) {\n    unlink_free_heap_page(gc, gc->free_heaps);\n  }\n\n  gc->live++;\n  gc_protect(mrb, gc, p);\n  *(RVALUE *)p = RVALUE_zero;\n  p->tt = ttype;\n  p->c = cls;\n  paint_partial_white(gc, p);\n  return p;\n}\n\nstatic inline void\nadd_gray_list(mrb_state *mrb, mrb_gc *gc, struct RBasic *obj)\n{\n#ifdef MRB_GC_STRESS\n  if (obj->tt > MRB_TT_MAXDEFINE) {\n    abort();\n  }\n#endif\n  paint_gray(obj);\n  obj->gcnext = gc->gray_list;\n  gc->gray_list = obj;\n}\n\nstatic int\nci_nregs(mrb_callinfo *ci)\n{\n  struct RProc *p = ci->proc;\n  int n = 0;\n\n  if (!p) {\n    if (ci->argc < 0) return 3;\n    return ci->argc+2;\n  }\n  if (!MRB_PROC_CFUNC_P(p) && p->body.irep) {\n    n = p->body.irep->nregs;\n  }\n  if (ci->argc < 0) {\n    if (n < 3) n = 3; /* self + args + blk */\n  }\n  if (ci->argc > n) {\n    n = ci->argc + 2; /* self + blk */\n  }\n  return n;\n}\n\nstatic void\nmark_context_stack(mrb_state *mrb, struct mrb_context *c)\n{\n  size_t i;\n  size_t e;\n  mrb_value nil;\n\n  if (c->stack == NULL) return;\n  e = c->stack - c->stbase;\n  if (c->ci) {\n    e += ci_nregs(c->ci);\n  }\n  if (c->stbase + e > c->stend) e = c->stend - c->stbase;\n  for (i=0; i<e; i++) {\n    mrb_value v = c->stbase[i];\n\n    if (!mrb_immediate_p(v)) {\n      mrb_gc_mark(mrb, mrb_basic_ptr(v));\n    }\n  }\n  e = c->stend - c->stbase;\n  nil = mrb_nil_value();\n  for (; i<e; i++) {\n    c->stbase[i] = nil;\n  }\n}\n\nstatic void\nmark_context(mrb_state *mrb, struct mrb_context *c)\n{\n  int i;\n  mrb_callinfo *ci;\n\n start:\n  if (c->status == MRB_FIBER_TERMINATED) return;\n\n  /* mark VM stack */\n  mark_context_stack(mrb, c);\n\n  /* mark call stack */\n  if (c->cibase) {\n    for (ci = c->cibase; ci <= c->ci; ci++) {\n      mrb_gc_mark(mrb, (struct RBasic*)ci->env);\n      mrb_gc_mark(mrb, (struct RBasic*)ci->proc);\n      mrb_gc_mark(mrb, (struct RBasic*)ci->target_class);\n    }\n  }\n  /* mark ensure stack */\n  for (i=0; i<c->eidx; i++) {\n    mrb_gc_mark(mrb, (struct RBasic*)c->ensure[i]);\n  }\n  /* mark fibers */\n  mrb_gc_mark(mrb, (struct RBasic*)c->fib);\n  if (c->prev) {\n    c = c->prev;\n    goto start;\n  }\n}\n\nstatic void\ngc_mark_children(mrb_state *mrb, mrb_gc *gc, struct RBasic *obj)\n{\n  mrb_assert(is_gray(obj));\n  paint_black(obj);\n  gc->gray_list = obj->gcnext;\n  mrb_gc_mark(mrb, (struct RBasic*)obj->c);\n  switch (obj->tt) {\n  case MRB_TT_ICLASS:\n    {\n      struct RClass *c = (struct RClass*)obj;\n      if (MRB_FLAG_TEST(c, MRB_FL_CLASS_IS_ORIGIN))\n        mrb_gc_mark_mt(mrb, c);\n      mrb_gc_mark(mrb, (struct RBasic*)((struct RClass*)obj)->super);\n    }\n    break;\n\n  case MRB_TT_CLASS:\n  case MRB_TT_MODULE:\n  case MRB_TT_SCLASS:\n    {\n      struct RClass *c = (struct RClass*)obj;\n\n      mrb_gc_mark_mt(mrb, c);\n      mrb_gc_mark(mrb, (struct RBasic*)c->super);\n    }\n    /* fall through */\n\n  case MRB_TT_OBJECT:\n  case MRB_TT_DATA:\n  case MRB_TT_EXCEPTION:\n    mrb_gc_mark_iv(mrb, (struct RObject*)obj);\n    break;\n\n  case MRB_TT_PROC:\n    {\n      struct RProc *p = (struct RProc*)obj;\n\n      mrb_gc_mark(mrb, (struct RBasic*)p->upper);\n      mrb_gc_mark(mrb, (struct RBasic*)p->e.env);\n    }\n    break;\n\n  case MRB_TT_ENV:\n    {\n      struct REnv *e = (struct REnv*)obj;\n      mrb_int i, len;\n\n      if (MRB_ENV_ONSTACK_P(e) && e->cxt && e->cxt->fib) {\n        mrb_gc_mark(mrb, (struct RBasic*)e->cxt->fib);\n      }\n      len = MRB_ENV_LEN(e);\n      for (i=0; i<len; i++) {\n        mrb_gc_mark_value(mrb, e->stack[i]);\n      }\n    }\n    break;\n\n  case MRB_TT_FIBER:\n    {\n      struct mrb_context *c = ((struct RFiber*)obj)->cxt;\n\n      if (c) mark_context(mrb, c);\n    }\n    break;\n\n  case MRB_TT_ARRAY:\n    {\n      struct RArray *a = (struct RArray*)obj;\n      size_t i, e;\n\n      for (i=0,e=ARY_LEN(a); i<e; i++) {\n        mrb_gc_mark_value(mrb, ARY_PTR(a)[i]);\n      }\n    }\n    break;\n\n  case MRB_TT_HASH:\n    mrb_gc_mark_iv(mrb, (struct RObject*)obj);\n    mrb_gc_mark_hash(mrb, (struct RHash*)obj);\n    break;\n\n  case MRB_TT_STRING:\n    if (RSTR_FSHARED_P(obj)) {\n      struct RString *s = (struct RString*)obj;\n      mrb_gc_mark(mrb, (struct RBasic*)s->as.heap.aux.fshared);\n    }\n    break;\n\n  case MRB_TT_RANGE:\n    mrb_gc_mark_range(mrb, (struct RRange*)obj);\n    break;\n\n  default:\n    break;\n  }\n}\n\nMRB_API void\nmrb_gc_mark(mrb_state *mrb, struct RBasic *obj)\n{\n  if (obj == 0) return;\n  if (!is_white(obj)) return;\n  mrb_assert((obj)->tt != MRB_TT_FREE);\n  add_gray_list(mrb, &mrb->gc, obj);\n}\n\nstatic void\nobj_free(mrb_state *mrb, struct RBasic *obj, int end)\n{\n  DEBUG(fprintf(stderr, \"obj_free(%p,tt=%d)\\n\",obj,obj->tt));\n  switch (obj->tt) {\n    /* immediate - no mark */\n  case MRB_TT_TRUE:\n  case MRB_TT_FIXNUM:\n  case MRB_TT_SYMBOL:\n    /* cannot happen */\n    return;\n\n#ifndef MRB_WITHOUT_FLOAT\n  case MRB_TT_FLOAT:\n#ifdef MRB_WORD_BOXING\n    break;\n#else\n    return;\n#endif\n#endif\n\n  case MRB_TT_OBJECT:\n    mrb_gc_free_iv(mrb, (struct RObject*)obj);\n    break;\n\n  case MRB_TT_EXCEPTION:\n    mrb_gc_free_iv(mrb, (struct RObject*)obj);\n    break;\n\n  case MRB_TT_CLASS:\n  case MRB_TT_MODULE:\n  case MRB_TT_SCLASS:\n    mrb_gc_free_mt(mrb, (struct RClass*)obj);\n    mrb_gc_free_iv(mrb, (struct RObject*)obj);\n    mrb_mc_clear_by_class(mrb, (struct RClass*)obj);\n    break;\n  case MRB_TT_ICLASS:\n    if (MRB_FLAG_TEST(obj, MRB_FL_CLASS_IS_ORIGIN))\n      mrb_gc_free_mt(mrb, (struct RClass*)obj);\n    mrb_mc_clear_by_class(mrb, (struct RClass*)obj);\n    break;\n  case MRB_TT_ENV:\n    {\n      struct REnv *e = (struct REnv*)obj;\n\n      if (MRB_ENV_ONSTACK_P(e)) {\n        /* cannot be freed */\n        e->stack = NULL;\n        break;\n      }\n      mrb_free(mrb, e->stack);\n      e->stack = NULL;\n    }\n    break;\n\n  case MRB_TT_FIBER:\n    {\n      struct mrb_context *c = ((struct RFiber*)obj)->cxt;\n\n      if (c && c != mrb->root_c) {\n        if (!end && c->status != MRB_FIBER_TERMINATED) {\n          mrb_callinfo *ci = c->ci;\n          mrb_callinfo *ce = c->cibase;\n\n          while (ce <= ci) {\n            struct REnv *e = ci->env;\n            if (e && !mrb_object_dead_p(mrb, (struct RBasic*)e) &&\n                e->tt == MRB_TT_ENV && MRB_ENV_ONSTACK_P(e)) {\n              mrb_env_unshare(mrb, e);\n            }\n            ci--;\n          }\n        }\n        mrb_free_context(mrb, c);\n      }\n    }\n    break;\n\n  case MRB_TT_ARRAY:\n    if (ARY_SHARED_P(obj))\n      mrb_ary_decref(mrb, ((struct RArray*)obj)->as.heap.aux.shared);\n    else if (!ARY_EMBED_P(obj))\n      mrb_free(mrb, ((struct RArray*)obj)->as.heap.ptr);\n    break;\n\n  case MRB_TT_HASH:\n    mrb_gc_free_iv(mrb, (struct RObject*)obj);\n    mrb_gc_free_hash(mrb, (struct RHash*)obj);\n    break;\n\n  case MRB_TT_STRING:\n    mrb_gc_free_str(mrb, (struct RString*)obj);\n    break;\n\n  case MRB_TT_PROC:\n    {\n      struct RProc *p = (struct RProc*)obj;\n\n      if (!MRB_PROC_CFUNC_P(p) && p->body.irep) {\n        mrb_irep *irep = p->body.irep;\n        if (end) {\n          mrb_irep_cutref(mrb, irep);\n        }\n        mrb_irep_decref(mrb, irep);\n      }\n    }\n    break;\n\n  case MRB_TT_RANGE:\n    mrb_gc_free_range(mrb, ((struct RRange*)obj));\n    break;\n\n  case MRB_TT_DATA:\n    {\n      struct RData *d = (struct RData*)obj;\n      if (d->type && d->type->dfree) {\n        d->type->dfree(mrb, d->data);\n      }\n      mrb_gc_free_iv(mrb, (struct RObject*)obj);\n    }\n    break;\n\n  default:\n    break;\n  }\n  obj->tt = MRB_TT_FREE;\n}\n\nstatic void\nroot_scan_phase(mrb_state *mrb, mrb_gc *gc)\n{\n  int i, e;\n\n  if (!is_minor_gc(gc)) {\n    gc->gray_list = NULL;\n    gc->atomic_gray_list = NULL;\n  }\n\n  mrb_gc_mark_gv(mrb);\n  /* mark arena */\n  for (i=0,e=gc->arena_idx; i<e; i++) {\n    mrb_gc_mark(mrb, gc->arena[i]);\n  }\n  /* mark class hierarchy */\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->object_class);\n\n  /* mark built-in classes */\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->class_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->module_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->proc_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->string_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->array_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->hash_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->range_class);\n\n#ifndef MRB_WITHOUT_FLOAT\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->float_class);\n#endif\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->fixnum_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->true_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->false_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->nil_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->symbol_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->kernel_module);\n\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->eException_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->eStandardError_class);\n\n  /* mark top_self */\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->top_self);\n  /* mark exception */\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->exc);\n  /* mark pre-allocated exception */\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->nomem_err);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->stack_err);\n#ifdef MRB_GC_FIXED_ARENA\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->arena_err);\n#endif\n\n  mark_context(mrb, mrb->c);\n  if (mrb->root_c != mrb->c) {\n    mark_context(mrb, mrb->root_c);\n  }\n}\n\n/* rough estimation of number of GC marks (non recursive) */\nstatic size_t\ngc_gray_counts(mrb_state *mrb, mrb_gc *gc, struct RBasic *obj)\n{\n  size_t children = 0;\n\n  switch (obj->tt) {\n  case MRB_TT_ICLASS:\n    children++;\n    break;\n\n  case MRB_TT_CLASS:\n  case MRB_TT_SCLASS:\n  case MRB_TT_MODULE:\n    {\n      struct RClass *c = (struct RClass*)obj;\n\n      children += mrb_gc_mark_iv_size(mrb, (struct RObject*)obj);\n      children += mrb_gc_mark_mt_size(mrb, c);\n      children++;\n    }\n    break;\n\n  case MRB_TT_OBJECT:\n  case MRB_TT_DATA:\n  case MRB_TT_EXCEPTION:\n    children += mrb_gc_mark_iv_size(mrb, (struct RObject*)obj);\n    break;\n\n  case MRB_TT_ENV:\n    children += MRB_ENV_LEN(obj);\n    break;\n\n  case MRB_TT_FIBER:\n    {\n      struct mrb_context *c = ((struct RFiber*)obj)->cxt;\n      size_t i;\n      mrb_callinfo *ci;\n\n      if (!c || c->status == MRB_FIBER_TERMINATED) break;\n\n      /* mark stack */\n      i = c->stack - c->stbase;\n\n      if (c->ci) {\n        i += ci_nregs(c->ci);\n      }\n      if (c->stbase + i > c->stend) i = c->stend - c->stbase;\n      children += i;\n\n      /* mark ensure stack */\n      children += c->eidx;\n\n      /* mark closure */\n      if (c->cibase) {\n        for (i=0, ci = c->cibase; ci <= c->ci; i++, ci++)\n          ;\n      }\n      children += i;\n    }\n    break;\n\n  case MRB_TT_ARRAY:\n    {\n      struct RArray *a = (struct RArray*)obj;\n      children += ARY_LEN(a);\n    }\n    break;\n\n  case MRB_TT_HASH:\n    children += mrb_gc_mark_iv_size(mrb, (struct RObject*)obj);\n    children += mrb_gc_mark_hash_size(mrb, (struct RHash*)obj);\n    break;\n\n  case MRB_TT_PROC:\n  case MRB_TT_RANGE:\n    children+=2;\n    break;\n\n  default:\n    break;\n  }\n  return children;\n}\n\n\nstatic void\ngc_mark_gray_list(mrb_state *mrb, mrb_gc *gc) {\n  while (gc->gray_list) {\n    if (is_gray(gc->gray_list))\n      gc_mark_children(mrb, gc, gc->gray_list);\n    else\n      gc->gray_list = gc->gray_list->gcnext;\n  }\n}\n\n\nstatic size_t\nincremental_marking_phase(mrb_state *mrb, mrb_gc *gc, size_t limit)\n{\n  size_t tried_marks = 0;\n\n  while (gc->gray_list && tried_marks < limit) {\n    struct RBasic *obj = gc->gray_list;\n    gc_mark_children(mrb, gc, obj);\n    tried_marks += gc_gray_counts(mrb, gc, obj);\n  }\n\n  return tried_marks;\n}\n\nstatic void\nfinal_marking_phase(mrb_state *mrb, mrb_gc *gc)\n{\n  int i, e;\n\n  /* mark arena */\n  for (i=0,e=gc->arena_idx; i<e; i++) {\n    mrb_gc_mark(mrb, gc->arena[i]);\n  }\n  mrb_gc_mark_gv(mrb);\n  mark_context(mrb, mrb->c);\n  mark_context(mrb, mrb->root_c);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->exc);\n  gc_mark_gray_list(mrb, gc);\n  mrb_assert(gc->gray_list == NULL);\n  gc->gray_list = gc->atomic_gray_list;\n  gc->atomic_gray_list = NULL;\n  gc_mark_gray_list(mrb, gc);\n  mrb_assert(gc->gray_list == NULL);\n}\n\nstatic void\nprepare_incremental_sweep(mrb_state *mrb, mrb_gc *gc)\n{\n  gc->state = MRB_GC_STATE_SWEEP;\n  gc->sweeps = gc->heaps;\n  gc->live_after_mark = gc->live;\n}\n\nstatic size_t\nincremental_sweep_phase(mrb_state *mrb, mrb_gc *gc, size_t limit)\n{\n  mrb_heap_page *page = gc->sweeps;\n  size_t tried_sweep = 0;\n\n  while (page && (tried_sweep < limit)) {\n    RVALUE *p = objects(page);\n    RVALUE *e = p + MRB_HEAP_PAGE_SIZE;\n    size_t freed = 0;\n    mrb_bool dead_slot = TRUE;\n    mrb_bool full = (page->freelist == NULL);\n\n    if (is_minor_gc(gc) && page->old) {\n      /* skip a slot which doesn't contain any young object */\n      p = e;\n      dead_slot = FALSE;\n    }\n    while (p<e) {\n      if (is_dead(gc, &p->as.basic)) {\n        if (p->as.basic.tt != MRB_TT_FREE) {\n          obj_free(mrb, &p->as.basic, FALSE);\n          if (p->as.basic.tt == MRB_TT_FREE) {\n            p->as.free.next = page->freelist;\n            page->freelist = (struct RBasic*)p;\n            freed++;\n          }\n          else {\n            dead_slot = FALSE;\n          }\n        }\n      }\n      else {\n        if (!is_generational(gc))\n          paint_partial_white(gc, &p->as.basic); /* next gc target */\n        dead_slot = FALSE;\n      }\n      p++;\n    }\n\n    /* free dead slot */\n    if (dead_slot && freed < MRB_HEAP_PAGE_SIZE) {\n      mrb_heap_page *next = page->next;\n\n      unlink_heap_page(gc, page);\n      unlink_free_heap_page(gc, page);\n      mrb_free(mrb, page);\n      page = next;\n    }\n    else {\n      if (full && freed > 0) {\n        link_free_heap_page(gc, page);\n      }\n      if (page->freelist == NULL && is_minor_gc(gc))\n        page->old = TRUE;\n      else\n        page->old = FALSE;\n      page = page->next;\n    }\n    tried_sweep += MRB_HEAP_PAGE_SIZE;\n    gc->live -= freed;\n    gc->live_after_mark -= freed;\n  }\n  gc->sweeps = page;\n  return tried_sweep;\n}\n\nstatic size_t\nincremental_gc(mrb_state *mrb, mrb_gc *gc, size_t limit)\n{\n  switch (gc->state) {\n  case MRB_GC_STATE_ROOT:\n    root_scan_phase(mrb, gc);\n    gc->state = MRB_GC_STATE_MARK;\n    flip_white_part(gc);\n    return 0;\n  case MRB_GC_STATE_MARK:\n    if (gc->gray_list) {\n      return incremental_marking_phase(mrb, gc, limit);\n    }\n    else {\n      final_marking_phase(mrb, gc);\n      prepare_incremental_sweep(mrb, gc);\n      return 0;\n    }\n  case MRB_GC_STATE_SWEEP: {\n     size_t tried_sweep = 0;\n     tried_sweep = incremental_sweep_phase(mrb, gc, limit);\n     if (tried_sweep == 0)\n       gc->state = MRB_GC_STATE_ROOT;\n     return tried_sweep;\n  }\n  default:\n    /* unknown state */\n    mrb_assert(0);\n    return 0;\n  }\n}\n\nstatic void\nincremental_gc_until(mrb_state *mrb, mrb_gc *gc, mrb_gc_state to_state)\n{\n  do {\n    incremental_gc(mrb, gc, SIZE_MAX);\n  } while (gc->state != to_state);\n}\n\nstatic void\nincremental_gc_step(mrb_state *mrb, mrb_gc *gc)\n{\n  size_t limit = 0, result = 0;\n  limit = (GC_STEP_SIZE/100) * gc->step_ratio;\n  while (result < limit) {\n    result += incremental_gc(mrb, gc, limit);\n    if (gc->state == MRB_GC_STATE_ROOT)\n      break;\n  }\n\n  gc->threshold = gc->live + GC_STEP_SIZE;\n}\n\nstatic void\nclear_all_old(mrb_state *mrb, mrb_gc *gc)\n{\n  mrb_bool origin_mode = gc->generational;\n\n  mrb_assert(is_generational(gc));\n  if (is_major_gc(gc)) {\n    /* finish the half baked GC */\n    incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n  }\n\n  /* Sweep the dead objects, then reset all the live objects\n   * (including all the old objects, of course) to white. */\n  gc->generational = FALSE;\n  prepare_incremental_sweep(mrb, gc);\n  incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n  gc->generational = origin_mode;\n\n  /* The gray objects have already been painted as white */\n  gc->atomic_gray_list = gc->gray_list = NULL;\n}\n\nMRB_API void\nmrb_incremental_gc(mrb_state *mrb)\n{\n  mrb_gc *gc = &mrb->gc;\n\n  if (gc->disabled || gc->iterating) return;\n\n  GC_INVOKE_TIME_REPORT(\"mrb_incremental_gc()\");\n  GC_TIME_START;\n\n  if (is_minor_gc(gc)) {\n    incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n  }\n  else {\n    incremental_gc_step(mrb, gc);\n  }\n\n  if (gc->state == MRB_GC_STATE_ROOT) {\n    mrb_assert(gc->live >= gc->live_after_mark);\n    gc->threshold = (gc->live_after_mark/100) * gc->interval_ratio;\n    if (gc->threshold < GC_STEP_SIZE) {\n      gc->threshold = GC_STEP_SIZE;\n    }\n\n    if (is_major_gc(gc)) {\n      size_t threshold = gc->live_after_mark/100 * MAJOR_GC_INC_RATIO;\n\n      gc->full = FALSE;\n      if (threshold < MAJOR_GC_TOOMANY) {\n        gc->majorgc_old_threshold = threshold;\n      }\n      else {\n        /* too many objects allocated during incremental GC, */\n        /* instead of increasing threshold, invoke full GC. */\n        mrb_full_gc(mrb);\n      }\n    }\n    else if (is_minor_gc(gc)) {\n      if (gc->live > gc->majorgc_old_threshold) {\n        clear_all_old(mrb, gc);\n        gc->full = TRUE;\n      }\n    }\n  }\n\n  GC_TIME_STOP_AND_REPORT;\n}\n\n/* Perform a full gc cycle */\nMRB_API void\nmrb_full_gc(mrb_state *mrb)\n{\n  mrb_gc *gc = &mrb->gc;\n\n  if (!mrb->c) return;\n  if (gc->disabled || gc->iterating) return;\n\n  GC_INVOKE_TIME_REPORT(\"mrb_full_gc()\");\n  GC_TIME_START;\n\n  if (is_generational(gc)) {\n    /* clear all the old objects back to young */\n    clear_all_old(mrb, gc);\n    gc->full = TRUE;\n  }\n  else if (gc->state != MRB_GC_STATE_ROOT) {\n    /* finish half baked GC cycle */\n    incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n  }\n\n  incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n  gc->threshold = (gc->live_after_mark/100) * gc->interval_ratio;\n\n  if (is_generational(gc)) {\n    gc->majorgc_old_threshold = gc->live_after_mark/100 * MAJOR_GC_INC_RATIO;\n    gc->full = FALSE;\n  }\n\n  GC_TIME_STOP_AND_REPORT;\n}\n\nMRB_API void\nmrb_garbage_collect(mrb_state *mrb)\n{\n  mrb_full_gc(mrb);\n}\n\n/*\n * Field write barrier\n *   Paint obj(Black) -> value(White) to obj(Black) -> value(Gray).\n */\n\nMRB_API void\nmrb_field_write_barrier(mrb_state *mrb, struct RBasic *obj, struct RBasic *value)\n{\n  mrb_gc *gc = &mrb->gc;\n\n  if (!is_black(obj)) return;\n  if (!is_white(value)) return;\n\n  mrb_assert(gc->state == MRB_GC_STATE_MARK || (!is_dead(gc, value) && !is_dead(gc, obj)));\n  mrb_assert(is_generational(gc) || gc->state != MRB_GC_STATE_ROOT);\n\n  if (is_generational(gc) || gc->state == MRB_GC_STATE_MARK) {\n    add_gray_list(mrb, gc, value);\n  }\n  else {\n    mrb_assert(gc->state == MRB_GC_STATE_SWEEP);\n    paint_partial_white(gc, obj); /* for never write barriers */\n  }\n}\n\n/*\n * Write barrier\n *   Paint obj(Black) to obj(Gray).\n *\n *   The object that is painted gray will be traversed atomically in final\n *   mark phase. So you use this write barrier if it's frequency written spot.\n *   e.g. Set element on Array.\n */\n\nMRB_API void\nmrb_write_barrier(mrb_state *mrb, struct RBasic *obj)\n{\n  mrb_gc *gc = &mrb->gc;\n\n  if (!is_black(obj)) return;\n\n  mrb_assert(!is_dead(gc, obj));\n  mrb_assert(is_generational(gc) || gc->state != MRB_GC_STATE_ROOT);\n  paint_gray(obj);\n  obj->gcnext = gc->atomic_gray_list;\n  gc->atomic_gray_list = obj;\n}\n\n/*\n *  call-seq:\n *     GC.start                     -> nil\n *\n *  Initiates full garbage collection.\n *\n */\n\nstatic mrb_value\ngc_start(mrb_state *mrb, mrb_value obj)\n{\n  mrb_full_gc(mrb);\n  return mrb_nil_value();\n}\n\n/*\n *  call-seq:\n *     GC.enable    -> true or false\n *\n *  Enables garbage collection, returning <code>true</code> if garbage\n *  collection was previously disabled.\n *\n *     GC.disable   #=> false\n *     GC.enable    #=> true\n *     GC.enable    #=> false\n *\n */\n\nstatic mrb_value\ngc_enable(mrb_state *mrb, mrb_value obj)\n{\n  mrb_bool old = mrb->gc.disabled;\n\n  mrb->gc.disabled = FALSE;\n\n  return mrb_bool_value(old);\n}\n\n/*\n *  call-seq:\n *     GC.disable    -> true or false\n *\n *  Disables garbage collection, returning <code>true</code> if garbage\n *  collection was already disabled.\n *\n *     GC.disable   #=> false\n *     GC.disable   #=> true\n *\n */\n\nstatic mrb_value\ngc_disable(mrb_state *mrb, mrb_value obj)\n{\n  mrb_bool old = mrb->gc.disabled;\n\n  mrb->gc.disabled = TRUE;\n\n  return mrb_bool_value(old);\n}\n\n/*\n *  call-seq:\n *     GC.interval_ratio      -> fixnum\n *\n *  Returns ratio of GC interval. Default value is 200(%).\n *\n */\n\nstatic mrb_value\ngc_interval_ratio_get(mrb_state *mrb, mrb_value obj)\n{\n  return mrb_fixnum_value(mrb->gc.interval_ratio);\n}\n\n/*\n *  call-seq:\n *     GC.interval_ratio = fixnum    -> nil\n *\n *  Updates ratio of GC interval. Default value is 200(%).\n *  GC start as soon as after end all step of GC if you set 100(%).\n *\n */\n\nstatic mrb_value\ngc_interval_ratio_set(mrb_state *mrb, mrb_value obj)\n{\n  mrb_int ratio;\n\n  mrb_get_args(mrb, \"i\", &ratio);\n  mrb->gc.interval_ratio = (int)ratio;\n  return mrb_nil_value();\n}\n\n/*\n *  call-seq:\n *     GC.step_ratio    -> fixnum\n *\n *  Returns step span ratio of Incremental GC. Default value is 200(%).\n *\n */\n\nstatic mrb_value\ngc_step_ratio_get(mrb_state *mrb, mrb_value obj)\n{\n  return mrb_fixnum_value(mrb->gc.step_ratio);\n}\n\n/*\n *  call-seq:\n *     GC.step_ratio = fixnum   -> nil\n *\n *  Updates step span ratio of Incremental GC. Default value is 200(%).\n *  1 step of incrementalGC becomes long if a rate is big.\n *\n */\n\nstatic mrb_value\ngc_step_ratio_set(mrb_state *mrb, mrb_value obj)\n{\n  mrb_int ratio;\n\n  mrb_get_args(mrb, \"i\", &ratio);\n  mrb->gc.step_ratio = (int)ratio;\n  return mrb_nil_value();\n}\n\nstatic void\nchange_gen_gc_mode(mrb_state *mrb, mrb_gc *gc, mrb_bool enable)\n{\n  if (gc->disabled || gc->iterating) {\n    mrb_raise(mrb, E_RUNTIME_ERROR, \"generational mode changed when GC disabled\");\n    return;\n  }\n  if (is_generational(gc) && !enable) {\n    clear_all_old(mrb, gc);\n    mrb_assert(gc->state == MRB_GC_STATE_ROOT);\n    gc->full = FALSE;\n  }\n  else if (!is_generational(gc) && enable) {\n    incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n    gc->majorgc_old_threshold = gc->live_after_mark/100 * MAJOR_GC_INC_RATIO;\n    gc->full = FALSE;\n  }\n  gc->generational = enable;\n}\n\n/*\n *  call-seq:\n *     GC.generational_mode -> true or false\n *\n *  Returns generational or normal gc mode.\n *\n */\n\nstatic mrb_value\ngc_generational_mode_get(mrb_state *mrb, mrb_value self)\n{\n  return mrb_bool_value(mrb->gc.generational);\n}\n\n/*\n *  call-seq:\n *     GC.generational_mode = true or false -> true or false\n *\n *  Changes to generational or normal gc mode.\n *\n */\n\nstatic mrb_value\ngc_generational_mode_set(mrb_state *mrb, mrb_value self)\n{\n  mrb_bool enable;\n\n  mrb_get_args(mrb, \"b\", &enable);\n  if (mrb->gc.generational != enable)\n    change_gen_gc_mode(mrb, &mrb->gc, enable);\n\n  return mrb_bool_value(enable);\n}\n\n\nstatic void\ngc_each_objects(mrb_state *mrb, mrb_gc *gc, mrb_each_object_callback *callback, void *data)\n{\n  mrb_heap_page* page;\n\n  page = gc->heaps;\n  while (page != NULL) {\n    RVALUE *p;\n    int i;\n\n    p = objects(page);\n    for (i=0; i < MRB_HEAP_PAGE_SIZE; i++) {\n      if ((*callback)(mrb, &p[i].as.basic, data) == MRB_EACH_OBJ_BREAK)\n        return;\n    }\n    page = page->next;\n  }\n}\n\nvoid\nmrb_objspace_each_objects(mrb_state *mrb, mrb_each_object_callback *callback, void *data)\n{\n  mrb_bool iterating = mrb->gc.iterating;\n\n  mrb_full_gc(mrb);\n  mrb->gc.iterating = TRUE;\n  if (iterating) {\n    gc_each_objects(mrb, &mrb->gc, callback, data);\n  }\n  else {\n    struct mrb_jmpbuf *prev_jmp = mrb->jmp;\n    struct mrb_jmpbuf c_jmp;\n\n    MRB_TRY(&c_jmp) {\n      mrb->jmp = &c_jmp;\n      gc_each_objects(mrb, &mrb->gc, callback, data);\n      mrb->jmp = prev_jmp;\n      mrb->gc.iterating = iterating;\n   } MRB_CATCH(&c_jmp) {\n      mrb->gc.iterating = iterating;\n      mrb->jmp = prev_jmp;\n      MRB_THROW(prev_jmp);\n    } MRB_END_EXC(&c_jmp);\n  }\n}\n\n#ifdef GC_TEST\n#ifdef GC_DEBUG\nstatic mrb_value gc_test(mrb_state *, mrb_value);\n#endif\n#endif\n\nvoid\nmrb_init_gc(mrb_state *mrb)\n{\n  struct RClass *gc;\n\n  mrb_static_assert(sizeof(RVALUE) <= sizeof(void*) * 6,\n                    \"RVALUE size must be within 6 words\");\n\n  gc = mrb_define_module(mrb, \"GC\");\n\n  mrb_define_class_method(mrb, gc, \"start\", gc_start, MRB_ARGS_NONE());\n  mrb_define_class_method(mrb, gc, \"enable\", gc_enable, MRB_ARGS_NONE());\n  mrb_define_class_method(mrb, gc, \"disable\", gc_disable, MRB_ARGS_NONE());\n  mrb_define_class_method(mrb, gc, \"interval_ratio\", gc_interval_ratio_get, MRB_ARGS_NONE());\n  mrb_define_class_method(mrb, gc, \"interval_ratio=\", gc_interval_ratio_set, MRB_ARGS_REQ(1));\n  mrb_define_class_method(mrb, gc, \"step_ratio\", gc_step_ratio_get, MRB_ARGS_NONE());\n  mrb_define_class_method(mrb, gc, \"step_ratio=\", gc_step_ratio_set, MRB_ARGS_REQ(1));\n  mrb_define_class_method(mrb, gc, \"generational_mode=\", gc_generational_mode_set, MRB_ARGS_REQ(1));\n  mrb_define_class_method(mrb, gc, \"generational_mode\", gc_generational_mode_get, MRB_ARGS_NONE());\n#ifdef GC_TEST\n#ifdef GC_DEBUG\n  mrb_define_class_method(mrb, gc, \"test\", gc_test, MRB_ARGS_NONE());\n#endif\n#endif\n}\n"], "fixing_code": ["/*\n** gc.c - garbage collector for mruby\n**\n** See Copyright Notice in mruby.h\n*/\n\n#include <string.h>\n#include <stdlib.h>\n#include <mruby.h>\n#include <mruby/array.h>\n#include <mruby/class.h>\n#include <mruby/data.h>\n#include <mruby/istruct.h>\n#include <mruby/hash.h>\n#include <mruby/proc.h>\n#include <mruby/range.h>\n#include <mruby/string.h>\n#include <mruby/variable.h>\n#include <mruby/gc.h>\n#include <mruby/error.h>\n#include <mruby/throw.h>\n\n/*\n  = Tri-color Incremental Garbage Collection\n\n  mruby's GC is Tri-color Incremental GC with Mark & Sweep.\n  Algorithm details are omitted.\n  Instead, the implementation part is described below.\n\n  == Object's Color\n\n  Each object can be painted in three colors:\n\n    * White - Unmarked.\n    * Gray - Marked, But the child objects are unmarked.\n    * Black - Marked, the child objects are also marked.\n\n  == Two White Types\n\n  There're two white color types in a flip-flop fashion: White-A and White-B,\n  which respectively represent the Current White color (the newly allocated\n  objects in the current GC cycle) and the Sweep Target White color (the\n  dead objects to be swept).\n\n  A and B will be switched just at the beginning of the next GC cycle. At\n  that time, all the dead objects have been swept, while the newly created\n  objects in the current GC cycle which finally remains White are now\n  regarded as dead objects. Instead of traversing all the White-A objects and\n  painting them as White-B, just switch the meaning of White-A and White-B as\n  this will be much cheaper.\n\n  As a result, the objects we sweep in the current GC cycle are always\n  left from the previous GC cycle. This allows us to sweep objects\n  incrementally, without the disturbance of the newly created objects.\n\n  == Execution Timing\n\n  GC Execution Time and Each step interval are decided by live objects count.\n  List of Adjustment API:\n\n    * gc_interval_ratio_set\n    * gc_step_ratio_set\n\n  For details, see the comments for each function.\n\n  == Write Barrier\n\n  mruby implementer and C extension library writer must insert a write\n  barrier when updating a reference from a field of an object.\n  When updating a reference from a field of object A to object B,\n  two different types of write barrier are available:\n\n    * mrb_field_write_barrier - target B object for a mark.\n    * mrb_write_barrier       - target A object for a mark.\n\n  == Generational Mode\n\n  mruby's GC offers an Generational Mode while re-using the tri-color GC\n  infrastructure. It will treat the Black objects as Old objects after each\n  sweep phase, instead of painting them White. The key ideas are still the same\n  as traditional generational GC:\n\n    * Minor GC - just traverse the Young objects (Gray objects) in the mark\n                 phase, then only sweep the newly created objects, and leave\n                 the Old objects live.\n\n    * Major GC - same as a full regular GC cycle.\n\n  The difference from \"traditional\" generational GC is, that the major GC\n  in mruby is triggered incrementally in a tri-color manner.\n\n\n  For details, see the comments for each function.\n\n*/\n\nstruct free_obj {\n  MRB_OBJECT_HEADER;\n  struct RBasic *next;\n};\n\ntypedef struct {\n  union {\n    struct free_obj free;\n    struct RBasic basic;\n    struct RObject object;\n    struct RClass klass;\n    struct RString string;\n    struct RArray array;\n    struct RHash hash;\n    struct RRange range;\n    struct RData data;\n    struct RIStruct istruct;\n    struct RProc proc;\n    struct REnv env;\n    struct RFiber fiber;\n    struct RException exc;\n    struct RBreak brk;\n#ifdef MRB_WORD_BOXING\n#ifndef MRB_WITHOUT_FLOAT\n    struct RFloat floatv;\n#endif\n    struct RCptr cptr;\n#endif\n  } as;\n} RVALUE;\n\n#ifdef GC_PROFILE\n#include <stdio.h>\n#include <sys/time.h>\n\nstatic double program_invoke_time = 0;\nstatic double gc_time = 0;\nstatic double gc_total_time = 0;\n\nstatic double\ngettimeofday_time(void)\n{\n  struct timeval tv;\n  gettimeofday(&tv, NULL);\n  return tv.tv_sec + tv.tv_usec * 1e-6;\n}\n\n#define GC_INVOKE_TIME_REPORT(with) do {\\\n  fprintf(stderr, \"%s\\n\", with);\\\n  fprintf(stderr, \"gc_invoke: %19.3f\\n\", gettimeofday_time() - program_invoke_time);\\\n  fprintf(stderr, \"is_generational: %d\\n\", is_generational(gc));\\\n  fprintf(stderr, \"is_major_gc: %d\\n\", is_major_gc(gc));\\\n} while(0)\n\n#define GC_TIME_START do {\\\n  gc_time = gettimeofday_time();\\\n} while(0)\n\n#define GC_TIME_STOP_AND_REPORT do {\\\n  gc_time = gettimeofday_time() - gc_time;\\\n  gc_total_time += gc_time;\\\n  fprintf(stderr, \"gc_state: %d\\n\", gc->state);\\\n  fprintf(stderr, \"live: %zu\\n\", gc->live);\\\n  fprintf(stderr, \"majorgc_old_threshold: %zu\\n\", gc->majorgc_old_threshold);\\\n  fprintf(stderr, \"gc_threshold: %zu\\n\", gc->threshold);\\\n  fprintf(stderr, \"gc_time: %30.20f\\n\", gc_time);\\\n  fprintf(stderr, \"gc_total_time: %30.20f\\n\\n\", gc_total_time);\\\n} while(0)\n#else\n#define GC_INVOKE_TIME_REPORT(s)\n#define GC_TIME_START\n#define GC_TIME_STOP_AND_REPORT\n#endif\n\n#ifdef GC_DEBUG\n#define DEBUG(x) (x)\n#else\n#define DEBUG(x)\n#endif\n\n#ifndef MRB_HEAP_PAGE_SIZE\n#define MRB_HEAP_PAGE_SIZE 1024\n#endif\n\n#define GC_STEP_SIZE 1024\n\n/* white: 001 or 010, black: 100, gray: 000 */\n#define GC_GRAY 0\n#define GC_WHITE_A 1\n#define GC_WHITE_B (1 << 1)\n#define GC_BLACK (1 << 2)\n#define GC_WHITES (GC_WHITE_A | GC_WHITE_B)\n#define GC_COLOR_MASK 7\n\n#define paint_gray(o) ((o)->color = GC_GRAY)\n#define paint_black(o) ((o)->color = GC_BLACK)\n#define paint_white(o) ((o)->color = GC_WHITES)\n#define paint_partial_white(s, o) ((o)->color = (s)->current_white_part)\n#define is_gray(o) ((o)->color == GC_GRAY)\n#define is_white(o) ((o)->color & GC_WHITES)\n#define is_black(o) ((o)->color & GC_BLACK)\n#define flip_white_part(s) ((s)->current_white_part = other_white_part(s))\n#define other_white_part(s) ((s)->current_white_part ^ GC_WHITES)\n#define is_dead(s, o) (((o)->color & other_white_part(s) & GC_WHITES) || (o)->tt == MRB_TT_FREE)\n\n#define objects(p) ((RVALUE *)p->objects)\n\nmrb_noreturn void mrb_raise_nomemory(mrb_state *mrb);\n\nMRB_API void*\nmrb_realloc_simple(mrb_state *mrb, void *p,  size_t len)\n{\n  void *p2;\n\n  p2 = (mrb->allocf)(mrb, p, len, mrb->allocf_ud);\n  if (!p2 && len > 0 && mrb->gc.heaps) {\n    mrb_full_gc(mrb);\n    p2 = (mrb->allocf)(mrb, p, len, mrb->allocf_ud);\n  }\n\n  return p2;\n}\n\nMRB_API void*\nmrb_realloc(mrb_state *mrb, void *p, size_t len)\n{\n  void *p2;\n\n  p2 = mrb_realloc_simple(mrb, p, len);\n  if (len == 0) return p2;\n  if (p2 == NULL) {\n    mrb->gc.out_of_memory = TRUE;\n    mrb_raise_nomemory(mrb);\n  }\n  else {\n    mrb->gc.out_of_memory = FALSE;\n  }\n\n  return p2;\n}\n\nMRB_API void*\nmrb_malloc(mrb_state *mrb, size_t len)\n{\n  return mrb_realloc(mrb, 0, len);\n}\n\nMRB_API void*\nmrb_malloc_simple(mrb_state *mrb, size_t len)\n{\n  return mrb_realloc_simple(mrb, 0, len);\n}\n\nMRB_API void*\nmrb_calloc(mrb_state *mrb, size_t nelem, size_t len)\n{\n  void *p;\n\n  if (nelem > 0 && len > 0 &&\n      nelem <= SIZE_MAX / len) {\n    size_t size;\n    size = nelem * len;\n    p = mrb_malloc(mrb, size);\n\n    memset(p, 0, size);\n  }\n  else {\n    p = NULL;\n  }\n\n  return p;\n}\n\nMRB_API void\nmrb_free(mrb_state *mrb, void *p)\n{\n  (mrb->allocf)(mrb, p, 0, mrb->allocf_ud);\n}\n\nMRB_API void*\nmrb_alloca(mrb_state *mrb, size_t size)\n{\n  struct RString *s;\n  s = (struct RString*)mrb_obj_alloc(mrb, MRB_TT_STRING, mrb->string_class);\n  return s->as.heap.ptr = (char*)mrb_malloc(mrb, size);\n}\n\nstatic mrb_bool\nheap_p(mrb_gc *gc, struct RBasic *object)\n{\n  mrb_heap_page* page;\n\n  page = gc->heaps;\n  while (page) {\n    RVALUE *p;\n\n    p = objects(page);\n    if (&p[0].as.basic <= object && object <= &p[MRB_HEAP_PAGE_SIZE].as.basic) {\n      return TRUE;\n    }\n    page = page->next;\n  }\n  return FALSE;\n}\n\nMRB_API mrb_bool\nmrb_object_dead_p(mrb_state *mrb, struct RBasic *object) {\n  mrb_gc *gc = &mrb->gc;\n  if (!heap_p(gc, object)) return TRUE;\n  return is_dead(gc, object);\n}\n\nstatic void\nlink_heap_page(mrb_gc *gc, mrb_heap_page *page)\n{\n  page->next = gc->heaps;\n  if (gc->heaps)\n    gc->heaps->prev = page;\n  gc->heaps = page;\n}\n\nstatic void\nunlink_heap_page(mrb_gc *gc, mrb_heap_page *page)\n{\n  if (page->prev)\n    page->prev->next = page->next;\n  if (page->next)\n    page->next->prev = page->prev;\n  if (gc->heaps == page)\n    gc->heaps = page->next;\n  page->prev = NULL;\n  page->next = NULL;\n}\n\nstatic void\nlink_free_heap_page(mrb_gc *gc, mrb_heap_page *page)\n{\n  page->free_next = gc->free_heaps;\n  if (gc->free_heaps) {\n    gc->free_heaps->free_prev = page;\n  }\n  gc->free_heaps = page;\n}\n\nstatic void\nunlink_free_heap_page(mrb_gc *gc, mrb_heap_page *page)\n{\n  if (page->free_prev)\n    page->free_prev->free_next = page->free_next;\n  if (page->free_next)\n    page->free_next->free_prev = page->free_prev;\n  if (gc->free_heaps == page)\n    gc->free_heaps = page->free_next;\n  page->free_prev = NULL;\n  page->free_next = NULL;\n}\n\nstatic void\nadd_heap(mrb_state *mrb, mrb_gc *gc)\n{\n  mrb_heap_page *page = (mrb_heap_page *)mrb_calloc(mrb, 1, sizeof(mrb_heap_page) + MRB_HEAP_PAGE_SIZE * sizeof(RVALUE));\n  RVALUE *p, *e;\n  struct RBasic *prev = NULL;\n\n  for (p = objects(page), e=p+MRB_HEAP_PAGE_SIZE; p<e; p++) {\n    p->as.free.tt = MRB_TT_FREE;\n    p->as.free.next = prev;\n    prev = &p->as.basic;\n  }\n  page->freelist = prev;\n\n  link_heap_page(gc, page);\n  link_free_heap_page(gc, page);\n}\n\n#define DEFAULT_GC_INTERVAL_RATIO 200\n#define DEFAULT_GC_STEP_RATIO 200\n#define MAJOR_GC_INC_RATIO 120\n#define MAJOR_GC_TOOMANY 10000\n#define is_generational(gc) ((gc)->generational)\n#define is_major_gc(gc) (is_generational(gc) && (gc)->full)\n#define is_minor_gc(gc) (is_generational(gc) && !(gc)->full)\n\nvoid\nmrb_gc_init(mrb_state *mrb, mrb_gc *gc)\n{\n#ifndef MRB_GC_FIXED_ARENA\n  gc->arena = (struct RBasic**)mrb_malloc(mrb, sizeof(struct RBasic*)*MRB_GC_ARENA_SIZE);\n  gc->arena_capa = MRB_GC_ARENA_SIZE;\n#endif\n\n  gc->current_white_part = GC_WHITE_A;\n  gc->heaps = NULL;\n  gc->free_heaps = NULL;\n  add_heap(mrb, gc);\n  gc->interval_ratio = DEFAULT_GC_INTERVAL_RATIO;\n  gc->step_ratio = DEFAULT_GC_STEP_RATIO;\n#ifndef MRB_GC_TURN_OFF_GENERATIONAL\n  gc->generational = TRUE;\n  gc->full = TRUE;\n#endif\n\n#ifdef GC_PROFILE\n  program_invoke_time = gettimeofday_time();\n#endif\n}\n\nstatic void obj_free(mrb_state *mrb, struct RBasic *obj, int end);\n\nstatic void\nfree_heap(mrb_state *mrb, mrb_gc *gc)\n{\n  mrb_heap_page *page = gc->heaps;\n  mrb_heap_page *tmp;\n  RVALUE *p, *e;\n\n  while (page) {\n    tmp = page;\n    page = page->next;\n    for (p = objects(tmp), e=p+MRB_HEAP_PAGE_SIZE; p<e; p++) {\n      if (p->as.free.tt != MRB_TT_FREE)\n        obj_free(mrb, &p->as.basic, TRUE);\n    }\n    mrb_free(mrb, tmp);\n  }\n}\n\nvoid\nmrb_gc_destroy(mrb_state *mrb, mrb_gc *gc)\n{\n  free_heap(mrb, gc);\n#ifndef MRB_GC_FIXED_ARENA\n  mrb_free(mrb, gc->arena);\n#endif\n}\n\nstatic void\ngc_protect(mrb_state *mrb, mrb_gc *gc, struct RBasic *p)\n{\n#ifdef MRB_GC_FIXED_ARENA\n  if (gc->arena_idx >= MRB_GC_ARENA_SIZE) {\n    /* arena overflow error */\n    gc->arena_idx = MRB_GC_ARENA_SIZE - 4; /* force room in arena */\n    mrb_exc_raise(mrb, mrb_obj_value(mrb->arena_err));\n  }\n#else\n  if (gc->arena_idx >= gc->arena_capa) {\n    /* extend arena */\n    gc->arena_capa = (int)(gc->arena_capa * 3 / 2);\n    gc->arena = (struct RBasic**)mrb_realloc(mrb, gc->arena, sizeof(struct RBasic*)*gc->arena_capa);\n  }\n#endif\n  gc->arena[gc->arena_idx++] = p;\n}\n\n/* mrb_gc_protect() leaves the object in the arena */\nMRB_API void\nmrb_gc_protect(mrb_state *mrb, mrb_value obj)\n{\n  if (mrb_immediate_p(obj)) return;\n  gc_protect(mrb, &mrb->gc, mrb_basic_ptr(obj));\n}\n\n#define GC_ROOT_NAME \"_gc_root_\"\n\n/* mrb_gc_register() keeps the object from GC.\n\n   Register your object when it's exported to C world,\n   without reference from Ruby world, e.g. callback\n   arguments.  Don't forget to remove the object using\n   mrb_gc_unregister, otherwise your object will leak.\n*/\n\nMRB_API void\nmrb_gc_register(mrb_state *mrb, mrb_value obj)\n{\n  mrb_sym root;\n  mrb_value table;\n\n  if (mrb_immediate_p(obj)) return;\n  root = mrb_intern_lit(mrb, GC_ROOT_NAME);\n  table = mrb_gv_get(mrb, root);\n  if (mrb_nil_p(table) || !mrb_array_p(table)) {\n    table = mrb_ary_new(mrb);\n    mrb_gv_set(mrb, root, table);\n  }\n  mrb_ary_push(mrb, table, obj);\n}\n\n/* mrb_gc_unregister() removes the object from GC root. */\nMRB_API void\nmrb_gc_unregister(mrb_state *mrb, mrb_value obj)\n{\n  mrb_sym root;\n  mrb_value table;\n  struct RArray *a;\n  mrb_int i;\n\n  if (mrb_immediate_p(obj)) return;\n  root = mrb_intern_lit(mrb, GC_ROOT_NAME);\n  table = mrb_gv_get(mrb, root);\n  if (mrb_nil_p(table)) return;\n  if (!mrb_array_p(table)) {\n    mrb_gv_set(mrb, root, mrb_nil_value());\n    return;\n  }\n  a = mrb_ary_ptr(table);\n  mrb_ary_modify(mrb, a);\n  for (i = 0; i < ARY_LEN(a); i++) {\n    if (mrb_ptr(ARY_PTR(a)[i]) == mrb_ptr(obj)) {\n      mrb_int len = ARY_LEN(a)-1;\n      mrb_value *ptr = ARY_PTR(a);\n\n      ARY_SET_LEN(a, len);\n      memmove(&ptr[i], &ptr[i + 1], (len - i) * sizeof(mrb_value));\n      break;\n    }\n  }\n}\n\nMRB_API struct RBasic*\nmrb_obj_alloc(mrb_state *mrb, enum mrb_vtype ttype, struct RClass *cls)\n{\n  struct RBasic *p;\n  static const RVALUE RVALUE_zero = { { { NULL, NULL, MRB_TT_FALSE } } };\n  mrb_gc *gc = &mrb->gc;\n\n  if (cls) {\n    enum mrb_vtype tt;\n\n    switch (cls->tt) {\n    case MRB_TT_CLASS:\n    case MRB_TT_SCLASS:\n    case MRB_TT_MODULE:\n    case MRB_TT_ENV:\n      break;\n    default:\n      mrb_raise(mrb, E_TYPE_ERROR, \"allocation failure\");\n    }\n    tt = MRB_INSTANCE_TT(cls);\n    if (tt != MRB_TT_FALSE &&\n        ttype != MRB_TT_SCLASS &&\n        ttype != MRB_TT_ICLASS &&\n        ttype != MRB_TT_ENV &&\n        ttype != tt) {\n      mrb_raisef(mrb, E_TYPE_ERROR, \"allocation failure of %C\", cls);\n    }\n  }\n\n#ifdef MRB_GC_STRESS\n  mrb_full_gc(mrb);\n#endif\n  if (gc->threshold < gc->live) {\n    mrb_incremental_gc(mrb);\n  }\n  if (gc->free_heaps == NULL) {\n    add_heap(mrb, gc);\n  }\n\n  p = gc->free_heaps->freelist;\n  gc->free_heaps->freelist = ((struct free_obj*)p)->next;\n  if (gc->free_heaps->freelist == NULL) {\n    unlink_free_heap_page(gc, gc->free_heaps);\n  }\n\n  gc->live++;\n  gc_protect(mrb, gc, p);\n  *(RVALUE *)p = RVALUE_zero;\n  p->tt = ttype;\n  p->c = cls;\n  paint_partial_white(gc, p);\n  return p;\n}\n\nstatic inline void\nadd_gray_list(mrb_state *mrb, mrb_gc *gc, struct RBasic *obj)\n{\n#ifdef MRB_GC_STRESS\n  if (obj->tt > MRB_TT_MAXDEFINE) {\n    abort();\n  }\n#endif\n  paint_gray(obj);\n  obj->gcnext = gc->gray_list;\n  gc->gray_list = obj;\n}\n\nstatic int\nci_nregs(mrb_callinfo *ci)\n{\n  struct RProc *p = ci->proc;\n  int n = 0;\n\n  if (!p) {\n    if (ci->argc < 0) return 3;\n    return ci->argc+2;\n  }\n  if (!MRB_PROC_CFUNC_P(p) && p->body.irep) {\n    n = p->body.irep->nregs;\n  }\n  if (ci->argc < 0) {\n    if (n < 3) n = 3; /* self + args + blk */\n  }\n  if (ci->argc > n) {\n    n = ci->argc + 2; /* self + blk */\n  }\n  return n;\n}\n\nstatic void\nmark_context_stack(mrb_state *mrb, struct mrb_context *c)\n{\n  size_t i;\n  size_t e;\n  mrb_value nil;\n\n  if (c->stack == NULL) return;\n  e = c->stack - c->stbase;\n  if (c->ci) {\n    e += ci_nregs(c->ci);\n  }\n  if (c->stbase + e > c->stend) e = c->stend - c->stbase;\n  for (i=0; i<e; i++) {\n    mrb_value v = c->stbase[i];\n\n    if (!mrb_immediate_p(v)) {\n      mrb_gc_mark(mrb, mrb_basic_ptr(v));\n    }\n  }\n  e = c->stend - c->stbase;\n  nil = mrb_nil_value();\n  for (; i<e; i++) {\n    c->stbase[i] = nil;\n  }\n}\n\nstatic void\nmark_context(mrb_state *mrb, struct mrb_context *c)\n{\n  int i;\n  mrb_callinfo *ci;\n\n start:\n  if (c->status == MRB_FIBER_TERMINATED) return;\n\n  /* mark VM stack */\n  mark_context_stack(mrb, c);\n\n  /* mark call stack */\n  if (c->cibase) {\n    for (ci = c->cibase; ci <= c->ci; ci++) {\n      mrb_gc_mark(mrb, (struct RBasic*)ci->env);\n      mrb_gc_mark(mrb, (struct RBasic*)ci->proc);\n      mrb_gc_mark(mrb, (struct RBasic*)ci->target_class);\n    }\n  }\n  /* mark ensure stack */\n  for (i=0; i<c->eidx; i++) {\n    mrb_gc_mark(mrb, (struct RBasic*)c->ensure[i]);\n  }\n  /* mark fibers */\n  mrb_gc_mark(mrb, (struct RBasic*)c->fib);\n  if (c->prev) {\n    c = c->prev;\n    goto start;\n  }\n}\n\nstatic void\ngc_mark_children(mrb_state *mrb, mrb_gc *gc, struct RBasic *obj)\n{\n  mrb_assert(is_gray(obj));\n  paint_black(obj);\n  gc->gray_list = obj->gcnext;\n  mrb_gc_mark(mrb, (struct RBasic*)obj->c);\n  switch (obj->tt) {\n  case MRB_TT_ICLASS:\n    {\n      struct RClass *c = (struct RClass*)obj;\n      if (MRB_FLAG_TEST(c, MRB_FL_CLASS_IS_ORIGIN))\n        mrb_gc_mark_mt(mrb, c);\n      mrb_gc_mark(mrb, (struct RBasic*)((struct RClass*)obj)->super);\n    }\n    break;\n\n  case MRB_TT_CLASS:\n  case MRB_TT_MODULE:\n  case MRB_TT_SCLASS:\n    {\n      struct RClass *c = (struct RClass*)obj;\n\n      mrb_gc_mark_mt(mrb, c);\n      mrb_gc_mark(mrb, (struct RBasic*)c->super);\n    }\n    /* fall through */\n\n  case MRB_TT_OBJECT:\n  case MRB_TT_DATA:\n  case MRB_TT_EXCEPTION:\n    mrb_gc_mark_iv(mrb, (struct RObject*)obj);\n    break;\n\n  case MRB_TT_PROC:\n    {\n      struct RProc *p = (struct RProc*)obj;\n\n      mrb_gc_mark(mrb, (struct RBasic*)p->upper);\n      mrb_gc_mark(mrb, (struct RBasic*)p->e.env);\n    }\n    break;\n\n  case MRB_TT_ENV:\n    {\n      struct REnv *e = (struct REnv*)obj;\n      mrb_int i, len;\n\n      if (MRB_ENV_ONSTACK_P(e) && e->cxt && e->cxt->fib) {\n        mrb_gc_mark(mrb, (struct RBasic*)e->cxt->fib);\n      }\n      len = MRB_ENV_LEN(e);\n      for (i=0; i<len; i++) {\n        mrb_gc_mark_value(mrb, e->stack[i]);\n      }\n    }\n    break;\n\n  case MRB_TT_FIBER:\n    {\n      struct mrb_context *c = ((struct RFiber*)obj)->cxt;\n\n      if (c) mark_context(mrb, c);\n    }\n    break;\n\n  case MRB_TT_ARRAY:\n    {\n      struct RArray *a = (struct RArray*)obj;\n      size_t i, e;\n\n      for (i=0,e=ARY_LEN(a); i<e; i++) {\n        mrb_gc_mark_value(mrb, ARY_PTR(a)[i]);\n      }\n    }\n    break;\n\n  case MRB_TT_HASH:\n    mrb_gc_mark_iv(mrb, (struct RObject*)obj);\n    mrb_gc_mark_hash(mrb, (struct RHash*)obj);\n    break;\n\n  case MRB_TT_STRING:\n    if (RSTR_FSHARED_P(obj)) {\n      struct RString *s = (struct RString*)obj;\n      mrb_gc_mark(mrb, (struct RBasic*)s->as.heap.aux.fshared);\n    }\n    break;\n\n  case MRB_TT_RANGE:\n    mrb_gc_mark_range(mrb, (struct RRange*)obj);\n    break;\n\n  default:\n    break;\n  }\n}\n\nMRB_API void\nmrb_gc_mark(mrb_state *mrb, struct RBasic *obj)\n{\n  if (obj == 0) return;\n  if (!is_white(obj)) return;\n  mrb_assert((obj)->tt != MRB_TT_FREE);\n  add_gray_list(mrb, &mrb->gc, obj);\n}\n\nstatic void\nobj_free(mrb_state *mrb, struct RBasic *obj, int end)\n{\n  DEBUG(fprintf(stderr, \"obj_free(%p,tt=%d)\\n\",obj,obj->tt));\n  switch (obj->tt) {\n    /* immediate - no mark */\n  case MRB_TT_TRUE:\n  case MRB_TT_FIXNUM:\n  case MRB_TT_SYMBOL:\n    /* cannot happen */\n    return;\n\n#ifndef MRB_WITHOUT_FLOAT\n  case MRB_TT_FLOAT:\n#ifdef MRB_WORD_BOXING\n    break;\n#else\n    return;\n#endif\n#endif\n\n  case MRB_TT_OBJECT:\n    mrb_gc_free_iv(mrb, (struct RObject*)obj);\n    break;\n\n  case MRB_TT_EXCEPTION:\n    mrb_gc_free_iv(mrb, (struct RObject*)obj);\n    break;\n\n  case MRB_TT_CLASS:\n  case MRB_TT_MODULE:\n  case MRB_TT_SCLASS:\n    mrb_gc_free_mt(mrb, (struct RClass*)obj);\n    mrb_gc_free_iv(mrb, (struct RObject*)obj);\n    mrb_mc_clear_by_class(mrb, (struct RClass*)obj);\n    break;\n  case MRB_TT_ICLASS:\n    if (MRB_FLAG_TEST(obj, MRB_FL_CLASS_IS_ORIGIN))\n      mrb_gc_free_mt(mrb, (struct RClass*)obj);\n    mrb_mc_clear_by_class(mrb, (struct RClass*)obj);\n    break;\n  case MRB_TT_ENV:\n    {\n      struct REnv *e = (struct REnv*)obj;\n\n      if (MRB_ENV_ONSTACK_P(e)) {\n        /* cannot be freed */\n        e->stack = NULL;\n        break;\n      }\n      mrb_free(mrb, e->stack);\n      e->stack = NULL;\n    }\n    break;\n\n  case MRB_TT_FIBER:\n    {\n      struct mrb_context *c = ((struct RFiber*)obj)->cxt;\n\n      if (c && c != mrb->root_c) {\n        if (!end && c->status != MRB_FIBER_TERMINATED) {\n          mrb_callinfo *ci = c->ci;\n          mrb_callinfo *ce = c->cibase;\n\n          while (ce <= ci) {\n            struct REnv *e = ci->env;\n            if (e && !mrb_object_dead_p(mrb, (struct RBasic*)e) &&\n                e->tt == MRB_TT_ENV && MRB_ENV_ONSTACK_P(e)) {\n              mrb_env_unshare(mrb, e);\n            }\n            ci--;\n          }\n        }\n        mrb_free_context(mrb, c);\n      }\n    }\n    break;\n\n  case MRB_TT_ARRAY:\n    if (ARY_SHARED_P(obj))\n      mrb_ary_decref(mrb, ((struct RArray*)obj)->as.heap.aux.shared);\n    else if (!ARY_EMBED_P(obj))\n      mrb_free(mrb, ((struct RArray*)obj)->as.heap.ptr);\n    break;\n\n  case MRB_TT_HASH:\n    mrb_gc_free_iv(mrb, (struct RObject*)obj);\n    mrb_gc_free_hash(mrb, (struct RHash*)obj);\n    break;\n\n  case MRB_TT_STRING:\n    mrb_gc_free_str(mrb, (struct RString*)obj);\n    break;\n\n  case MRB_TT_PROC:\n    {\n      struct RProc *p = (struct RProc*)obj;\n\n      if (!MRB_PROC_CFUNC_P(p) && p->body.irep) {\n        mrb_irep *irep = p->body.irep;\n        if (end) {\n          mrb_irep_cutref(mrb, irep);\n        }\n        mrb_irep_decref(mrb, irep);\n      }\n    }\n    break;\n\n  case MRB_TT_RANGE:\n    mrb_gc_free_range(mrb, ((struct RRange*)obj));\n    break;\n\n  case MRB_TT_DATA:\n    {\n      struct RData *d = (struct RData*)obj;\n      if (d->type && d->type->dfree) {\n        d->type->dfree(mrb, d->data);\n      }\n      mrb_gc_free_iv(mrb, (struct RObject*)obj);\n    }\n    break;\n\n  default:\n    break;\n  }\n  obj->tt = MRB_TT_FREE;\n}\n\nstatic void\nroot_scan_phase(mrb_state *mrb, mrb_gc *gc)\n{\n  int i, e;\n\n  if (!is_minor_gc(gc)) {\n    gc->gray_list = NULL;\n    gc->atomic_gray_list = NULL;\n  }\n\n  mrb_gc_mark_gv(mrb);\n  /* mark arena */\n  for (i=0,e=gc->arena_idx; i<e; i++) {\n    mrb_gc_mark(mrb, gc->arena[i]);\n  }\n  /* mark class hierarchy */\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->object_class);\n\n  /* mark built-in classes */\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->class_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->module_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->proc_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->string_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->array_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->hash_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->range_class);\n\n#ifndef MRB_WITHOUT_FLOAT\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->float_class);\n#endif\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->fixnum_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->true_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->false_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->nil_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->symbol_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->kernel_module);\n\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->eException_class);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->eStandardError_class);\n\n  /* mark top_self */\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->top_self);\n  /* mark exception */\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->exc);\n  /* mark pre-allocated exception */\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->nomem_err);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->stack_err);\n#ifdef MRB_GC_FIXED_ARENA\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->arena_err);\n#endif\n\n  mark_context(mrb, mrb->c);\n  if (mrb->root_c != mrb->c) {\n    mark_context(mrb, mrb->root_c);\n  }\n}\n\n/* rough estimation of number of GC marks (non recursive) */\nstatic size_t\ngc_gray_counts(mrb_state *mrb, mrb_gc *gc, struct RBasic *obj)\n{\n  size_t children = 0;\n\n  switch (obj->tt) {\n  case MRB_TT_ICLASS:\n    children++;\n    break;\n\n  case MRB_TT_CLASS:\n  case MRB_TT_SCLASS:\n  case MRB_TT_MODULE:\n    {\n      struct RClass *c = (struct RClass*)obj;\n\n      children += mrb_gc_mark_iv_size(mrb, (struct RObject*)obj);\n      children += mrb_gc_mark_mt_size(mrb, c);\n      children++;\n    }\n    break;\n\n  case MRB_TT_OBJECT:\n  case MRB_TT_DATA:\n  case MRB_TT_EXCEPTION:\n    children += mrb_gc_mark_iv_size(mrb, (struct RObject*)obj);\n    break;\n\n  case MRB_TT_ENV:\n    children += MRB_ENV_LEN(obj);\n    break;\n\n  case MRB_TT_FIBER:\n    {\n      struct mrb_context *c = ((struct RFiber*)obj)->cxt;\n      size_t i;\n      mrb_callinfo *ci;\n\n      if (!c || c->status == MRB_FIBER_TERMINATED) break;\n\n      /* mark stack */\n      i = c->stack - c->stbase;\n\n      if (c->ci) {\n        i += ci_nregs(c->ci);\n      }\n      if (c->stbase + i > c->stend) i = c->stend - c->stbase;\n      children += i;\n\n      /* mark ensure stack */\n      children += c->eidx;\n\n      /* mark closure */\n      if (c->cibase) {\n        for (i=0, ci = c->cibase; ci <= c->ci; i++, ci++)\n          ;\n      }\n      children += i;\n    }\n    break;\n\n  case MRB_TT_ARRAY:\n    {\n      struct RArray *a = (struct RArray*)obj;\n      children += ARY_LEN(a);\n    }\n    break;\n\n  case MRB_TT_HASH:\n    children += mrb_gc_mark_iv_size(mrb, (struct RObject*)obj);\n    children += mrb_gc_mark_hash_size(mrb, (struct RHash*)obj);\n    break;\n\n  case MRB_TT_PROC:\n  case MRB_TT_RANGE:\n    children+=2;\n    break;\n\n  default:\n    break;\n  }\n  return children;\n}\n\n\nstatic void\ngc_mark_gray_list(mrb_state *mrb, mrb_gc *gc) {\n  while (gc->gray_list) {\n    if (is_gray(gc->gray_list))\n      gc_mark_children(mrb, gc, gc->gray_list);\n    else\n      gc->gray_list = gc->gray_list->gcnext;\n  }\n}\n\n\nstatic size_t\nincremental_marking_phase(mrb_state *mrb, mrb_gc *gc, size_t limit)\n{\n  size_t tried_marks = 0;\n\n  while (gc->gray_list && tried_marks < limit) {\n    struct RBasic *obj = gc->gray_list;\n    gc_mark_children(mrb, gc, obj);\n    tried_marks += gc_gray_counts(mrb, gc, obj);\n  }\n\n  return tried_marks;\n}\n\nstatic void\nfinal_marking_phase(mrb_state *mrb, mrb_gc *gc)\n{\n  int i, e;\n\n  /* mark arena */\n  for (i=0,e=gc->arena_idx; i<e; i++) {\n    mrb_gc_mark(mrb, gc->arena[i]);\n  }\n  mrb_gc_mark_gv(mrb);\n  mark_context(mrb, mrb->c);\n  mark_context(mrb, mrb->root_c);\n  mrb_gc_mark(mrb, (struct RBasic*)mrb->exc);\n  gc_mark_gray_list(mrb, gc);\n  mrb_assert(gc->gray_list == NULL);\n  gc->gray_list = gc->atomic_gray_list;\n  gc->atomic_gray_list = NULL;\n  gc_mark_gray_list(mrb, gc);\n  mrb_assert(gc->gray_list == NULL);\n}\n\nstatic void\nprepare_incremental_sweep(mrb_state *mrb, mrb_gc *gc)\n{\n  gc->state = MRB_GC_STATE_SWEEP;\n  gc->sweeps = gc->heaps;\n  gc->live_after_mark = gc->live;\n}\n\nstatic size_t\nincremental_sweep_phase(mrb_state *mrb, mrb_gc *gc, size_t limit)\n{\n  mrb_heap_page *page = gc->sweeps;\n  size_t tried_sweep = 0;\n\n  while (page && (tried_sweep < limit)) {\n    RVALUE *p = objects(page);\n    RVALUE *e = p + MRB_HEAP_PAGE_SIZE;\n    size_t freed = 0;\n    mrb_bool dead_slot = TRUE;\n    mrb_bool full = (page->freelist == NULL);\n\n    if (is_minor_gc(gc) && page->old) {\n      /* skip a slot which doesn't contain any young object */\n      p = e;\n      dead_slot = FALSE;\n    }\n    while (p<e) {\n      if (is_dead(gc, &p->as.basic)) {\n        if (p->as.basic.tt != MRB_TT_FREE) {\n          obj_free(mrb, &p->as.basic, FALSE);\n          if (p->as.basic.tt == MRB_TT_FREE) {\n            p->as.free.next = page->freelist;\n            page->freelist = (struct RBasic*)p;\n            freed++;\n          }\n          else {\n            dead_slot = FALSE;\n          }\n        }\n      }\n      else {\n        if (!is_generational(gc))\n          paint_partial_white(gc, &p->as.basic); /* next gc target */\n        dead_slot = FALSE;\n      }\n      p++;\n    }\n\n    /* free dead slot */\n    if (dead_slot && freed < MRB_HEAP_PAGE_SIZE) {\n      mrb_heap_page *next = page->next;\n\n      unlink_heap_page(gc, page);\n      unlink_free_heap_page(gc, page);\n      mrb_free(mrb, page);\n      page = next;\n    }\n    else {\n      if (full && freed > 0) {\n        link_free_heap_page(gc, page);\n      }\n      if (page->freelist == NULL && is_minor_gc(gc))\n        page->old = TRUE;\n      else\n        page->old = FALSE;\n      page = page->next;\n    }\n    tried_sweep += MRB_HEAP_PAGE_SIZE;\n    gc->live -= freed;\n    gc->live_after_mark -= freed;\n  }\n  gc->sweeps = page;\n  return tried_sweep;\n}\n\nstatic size_t\nincremental_gc(mrb_state *mrb, mrb_gc *gc, size_t limit)\n{\n  switch (gc->state) {\n  case MRB_GC_STATE_ROOT:\n    root_scan_phase(mrb, gc);\n    gc->state = MRB_GC_STATE_MARK;\n    flip_white_part(gc);\n    return 0;\n  case MRB_GC_STATE_MARK:\n    if (gc->gray_list) {\n      return incremental_marking_phase(mrb, gc, limit);\n    }\n    else {\n      final_marking_phase(mrb, gc);\n      prepare_incremental_sweep(mrb, gc);\n      return 0;\n    }\n  case MRB_GC_STATE_SWEEP: {\n     size_t tried_sweep = 0;\n     tried_sweep = incremental_sweep_phase(mrb, gc, limit);\n     if (tried_sweep == 0)\n       gc->state = MRB_GC_STATE_ROOT;\n     return tried_sweep;\n  }\n  default:\n    /* unknown state */\n    mrb_assert(0);\n    return 0;\n  }\n}\n\nstatic void\nincremental_gc_until(mrb_state *mrb, mrb_gc *gc, mrb_gc_state to_state)\n{\n  do {\n    incremental_gc(mrb, gc, SIZE_MAX);\n  } while (gc->state != to_state);\n}\n\nstatic void\nincremental_gc_step(mrb_state *mrb, mrb_gc *gc)\n{\n  size_t limit = 0, result = 0;\n  limit = (GC_STEP_SIZE/100) * gc->step_ratio;\n  while (result < limit) {\n    result += incremental_gc(mrb, gc, limit);\n    if (gc->state == MRB_GC_STATE_ROOT)\n      break;\n  }\n\n  gc->threshold = gc->live + GC_STEP_SIZE;\n}\n\nstatic void\nclear_all_old(mrb_state *mrb, mrb_gc *gc)\n{\n  mrb_bool origin_mode = gc->generational;\n\n  mrb_assert(is_generational(gc));\n  if (is_major_gc(gc)) {\n    /* finish the half baked GC */\n    incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n  }\n\n  /* Sweep the dead objects, then reset all the live objects\n   * (including all the old objects, of course) to white. */\n  gc->generational = FALSE;\n  prepare_incremental_sweep(mrb, gc);\n  incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n  gc->generational = origin_mode;\n\n  /* The gray objects have already been painted as white */\n  gc->atomic_gray_list = gc->gray_list = NULL;\n}\n\nMRB_API void\nmrb_incremental_gc(mrb_state *mrb)\n{\n  mrb_gc *gc = &mrb->gc;\n\n  if (gc->disabled || gc->iterating) return;\n\n  GC_INVOKE_TIME_REPORT(\"mrb_incremental_gc()\");\n  GC_TIME_START;\n\n  if (is_minor_gc(gc)) {\n    incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n  }\n  else {\n    incremental_gc_step(mrb, gc);\n  }\n\n  if (gc->state == MRB_GC_STATE_ROOT) {\n    mrb_assert(gc->live >= gc->live_after_mark);\n    gc->threshold = (gc->live_after_mark/100) * gc->interval_ratio;\n    if (gc->threshold < GC_STEP_SIZE) {\n      gc->threshold = GC_STEP_SIZE;\n    }\n\n    if (is_major_gc(gc)) {\n      size_t threshold = gc->live_after_mark/100 * MAJOR_GC_INC_RATIO;\n\n      gc->full = FALSE;\n      if (threshold < MAJOR_GC_TOOMANY) {\n        gc->majorgc_old_threshold = threshold;\n      }\n      else {\n        /* too many objects allocated during incremental GC, */\n        /* instead of increasing threshold, invoke full GC. */\n        mrb_full_gc(mrb);\n      }\n    }\n    else if (is_minor_gc(gc)) {\n      if (gc->live > gc->majorgc_old_threshold) {\n        clear_all_old(mrb, gc);\n        gc->full = TRUE;\n      }\n    }\n  }\n\n  GC_TIME_STOP_AND_REPORT;\n}\n\n/* Perform a full gc cycle */\nMRB_API void\nmrb_full_gc(mrb_state *mrb)\n{\n  mrb_gc *gc = &mrb->gc;\n\n  if (!mrb->c) return;\n  if (gc->disabled || gc->iterating) return;\n\n  GC_INVOKE_TIME_REPORT(\"mrb_full_gc()\");\n  GC_TIME_START;\n\n  if (is_generational(gc)) {\n    /* clear all the old objects back to young */\n    clear_all_old(mrb, gc);\n    gc->full = TRUE;\n  }\n  else if (gc->state != MRB_GC_STATE_ROOT) {\n    /* finish half baked GC cycle */\n    incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n  }\n\n  incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n  gc->threshold = (gc->live_after_mark/100) * gc->interval_ratio;\n\n  if (is_generational(gc)) {\n    gc->majorgc_old_threshold = gc->live_after_mark/100 * MAJOR_GC_INC_RATIO;\n    gc->full = FALSE;\n  }\n\n  GC_TIME_STOP_AND_REPORT;\n}\n\nMRB_API void\nmrb_garbage_collect(mrb_state *mrb)\n{\n  mrb_full_gc(mrb);\n}\n\n/*\n * Field write barrier\n *   Paint obj(Black) -> value(White) to obj(Black) -> value(Gray).\n */\n\nMRB_API void\nmrb_field_write_barrier(mrb_state *mrb, struct RBasic *obj, struct RBasic *value)\n{\n  mrb_gc *gc = &mrb->gc;\n\n  if (!is_black(obj)) return;\n  if (!is_white(value)) return;\n\n  mrb_assert(gc->state == MRB_GC_STATE_MARK || (!is_dead(gc, value) && !is_dead(gc, obj)));\n  mrb_assert(is_generational(gc) || gc->state != MRB_GC_STATE_ROOT);\n\n  if (is_generational(gc) || gc->state == MRB_GC_STATE_MARK) {\n    add_gray_list(mrb, gc, value);\n  }\n  else {\n    mrb_assert(gc->state == MRB_GC_STATE_SWEEP);\n    paint_partial_white(gc, obj); /* for never write barriers */\n  }\n}\n\n/*\n * Write barrier\n *   Paint obj(Black) to obj(Gray).\n *\n *   The object that is painted gray will be traversed atomically in final\n *   mark phase. So you use this write barrier if it's frequency written spot.\n *   e.g. Set element on Array.\n */\n\nMRB_API void\nmrb_write_barrier(mrb_state *mrb, struct RBasic *obj)\n{\n  mrb_gc *gc = &mrb->gc;\n\n  if (!is_black(obj)) return;\n\n  mrb_assert(!is_dead(gc, obj));\n  mrb_assert(is_generational(gc) || gc->state != MRB_GC_STATE_ROOT);\n  paint_gray(obj);\n  obj->gcnext = gc->atomic_gray_list;\n  gc->atomic_gray_list = obj;\n}\n\n/*\n *  call-seq:\n *     GC.start                     -> nil\n *\n *  Initiates full garbage collection.\n *\n */\n\nstatic mrb_value\ngc_start(mrb_state *mrb, mrb_value obj)\n{\n  mrb_full_gc(mrb);\n  return mrb_nil_value();\n}\n\n/*\n *  call-seq:\n *     GC.enable    -> true or false\n *\n *  Enables garbage collection, returning <code>true</code> if garbage\n *  collection was previously disabled.\n *\n *     GC.disable   #=> false\n *     GC.enable    #=> true\n *     GC.enable    #=> false\n *\n */\n\nstatic mrb_value\ngc_enable(mrb_state *mrb, mrb_value obj)\n{\n  mrb_bool old = mrb->gc.disabled;\n\n  mrb->gc.disabled = FALSE;\n\n  return mrb_bool_value(old);\n}\n\n/*\n *  call-seq:\n *     GC.disable    -> true or false\n *\n *  Disables garbage collection, returning <code>true</code> if garbage\n *  collection was already disabled.\n *\n *     GC.disable   #=> false\n *     GC.disable   #=> true\n *\n */\n\nstatic mrb_value\ngc_disable(mrb_state *mrb, mrb_value obj)\n{\n  mrb_bool old = mrb->gc.disabled;\n\n  mrb->gc.disabled = TRUE;\n\n  return mrb_bool_value(old);\n}\n\n/*\n *  call-seq:\n *     GC.interval_ratio      -> fixnum\n *\n *  Returns ratio of GC interval. Default value is 200(%).\n *\n */\n\nstatic mrb_value\ngc_interval_ratio_get(mrb_state *mrb, mrb_value obj)\n{\n  return mrb_fixnum_value(mrb->gc.interval_ratio);\n}\n\n/*\n *  call-seq:\n *     GC.interval_ratio = fixnum    -> nil\n *\n *  Updates ratio of GC interval. Default value is 200(%).\n *  GC start as soon as after end all step of GC if you set 100(%).\n *\n */\n\nstatic mrb_value\ngc_interval_ratio_set(mrb_state *mrb, mrb_value obj)\n{\n  mrb_int ratio;\n\n  mrb_get_args(mrb, \"i\", &ratio);\n  mrb->gc.interval_ratio = (int)ratio;\n  return mrb_nil_value();\n}\n\n/*\n *  call-seq:\n *     GC.step_ratio    -> fixnum\n *\n *  Returns step span ratio of Incremental GC. Default value is 200(%).\n *\n */\n\nstatic mrb_value\ngc_step_ratio_get(mrb_state *mrb, mrb_value obj)\n{\n  return mrb_fixnum_value(mrb->gc.step_ratio);\n}\n\n/*\n *  call-seq:\n *     GC.step_ratio = fixnum   -> nil\n *\n *  Updates step span ratio of Incremental GC. Default value is 200(%).\n *  1 step of incrementalGC becomes long if a rate is big.\n *\n */\n\nstatic mrb_value\ngc_step_ratio_set(mrb_state *mrb, mrb_value obj)\n{\n  mrb_int ratio;\n\n  mrb_get_args(mrb, \"i\", &ratio);\n  mrb->gc.step_ratio = (int)ratio;\n  return mrb_nil_value();\n}\n\nstatic void\nchange_gen_gc_mode(mrb_state *mrb, mrb_gc *gc, mrb_bool enable)\n{\n  if (gc->disabled || gc->iterating) {\n    mrb_raise(mrb, E_RUNTIME_ERROR, \"generational mode changed when GC disabled\");\n    return;\n  }\n  if (is_generational(gc) && !enable) {\n    clear_all_old(mrb, gc);\n    mrb_assert(gc->state == MRB_GC_STATE_ROOT);\n    gc->full = FALSE;\n  }\n  else if (!is_generational(gc) && enable) {\n    incremental_gc_until(mrb, gc, MRB_GC_STATE_ROOT);\n    gc->majorgc_old_threshold = gc->live_after_mark/100 * MAJOR_GC_INC_RATIO;\n    gc->full = FALSE;\n  }\n  gc->generational = enable;\n}\n\n/*\n *  call-seq:\n *     GC.generational_mode -> true or false\n *\n *  Returns generational or normal gc mode.\n *\n */\n\nstatic mrb_value\ngc_generational_mode_get(mrb_state *mrb, mrb_value self)\n{\n  return mrb_bool_value(mrb->gc.generational);\n}\n\n/*\n *  call-seq:\n *     GC.generational_mode = true or false -> true or false\n *\n *  Changes to generational or normal gc mode.\n *\n */\n\nstatic mrb_value\ngc_generational_mode_set(mrb_state *mrb, mrb_value self)\n{\n  mrb_bool enable;\n\n  mrb_get_args(mrb, \"b\", &enable);\n  if (mrb->gc.generational != enable)\n    change_gen_gc_mode(mrb, &mrb->gc, enable);\n\n  return mrb_bool_value(enable);\n}\n\n\nstatic void\ngc_each_objects(mrb_state *mrb, mrb_gc *gc, mrb_each_object_callback *callback, void *data)\n{\n  mrb_heap_page* page;\n\n  page = gc->heaps;\n  while (page != NULL) {\n    RVALUE *p;\n    int i;\n\n    p = objects(page);\n    for (i=0; i < MRB_HEAP_PAGE_SIZE; i++) {\n      if ((*callback)(mrb, &p[i].as.basic, data) == MRB_EACH_OBJ_BREAK)\n        return;\n    }\n    page = page->next;\n  }\n}\n\nvoid\nmrb_objspace_each_objects(mrb_state *mrb, mrb_each_object_callback *callback, void *data)\n{\n  mrb_bool iterating = mrb->gc.iterating;\n\n  mrb_full_gc(mrb);\n  mrb->gc.iterating = TRUE;\n  if (iterating) {\n    gc_each_objects(mrb, &mrb->gc, callback, data);\n  }\n  else {\n    struct mrb_jmpbuf *prev_jmp = mrb->jmp;\n    struct mrb_jmpbuf c_jmp;\n\n    MRB_TRY(&c_jmp) {\n      mrb->jmp = &c_jmp;\n      gc_each_objects(mrb, &mrb->gc, callback, data);\n      mrb->jmp = prev_jmp;\n      mrb->gc.iterating = iterating;\n   } MRB_CATCH(&c_jmp) {\n      mrb->gc.iterating = iterating;\n      mrb->jmp = prev_jmp;\n      MRB_THROW(prev_jmp);\n    } MRB_END_EXC(&c_jmp);\n  }\n}\n\n#ifdef GC_TEST\n#ifdef GC_DEBUG\nstatic mrb_value gc_test(mrb_state *, mrb_value);\n#endif\n#endif\n\nvoid\nmrb_init_gc(mrb_state *mrb)\n{\n  struct RClass *gc;\n\n  mrb_static_assert(sizeof(RVALUE) <= sizeof(void*) * 6,\n                    \"RVALUE size must be within 6 words\");\n\n  gc = mrb_define_module(mrb, \"GC\");\n\n  mrb_define_class_method(mrb, gc, \"start\", gc_start, MRB_ARGS_NONE());\n  mrb_define_class_method(mrb, gc, \"enable\", gc_enable, MRB_ARGS_NONE());\n  mrb_define_class_method(mrb, gc, \"disable\", gc_disable, MRB_ARGS_NONE());\n  mrb_define_class_method(mrb, gc, \"interval_ratio\", gc_interval_ratio_get, MRB_ARGS_NONE());\n  mrb_define_class_method(mrb, gc, \"interval_ratio=\", gc_interval_ratio_set, MRB_ARGS_REQ(1));\n  mrb_define_class_method(mrb, gc, \"step_ratio\", gc_step_ratio_get, MRB_ARGS_NONE());\n  mrb_define_class_method(mrb, gc, \"step_ratio=\", gc_step_ratio_set, MRB_ARGS_REQ(1));\n  mrb_define_class_method(mrb, gc, \"generational_mode=\", gc_generational_mode_set, MRB_ARGS_REQ(1));\n  mrb_define_class_method(mrb, gc, \"generational_mode\", gc_generational_mode_get, MRB_ARGS_NONE());\n#ifdef GC_TEST\n#ifdef GC_DEBUG\n  mrb_define_class_method(mrb, gc, \"test\", gc_test, MRB_ARGS_NONE());\n#endif\n#endif\n}\n"], "filenames": ["src/gc.c"], "buggy_code_start_loc": [228], "buggy_code_end_loc": [229], "fixing_code_start_loc": [227], "fixing_code_end_loc": [227], "type": "CWE-415", "message": "mruby 2.1.2 has a double free in mrb_default_allocf (called from mrb_free and obj_free).", "other": {"cve": {"id": "CVE-2020-36401", "sourceIdentifier": "cve@mitre.org", "published": "2021-07-01T03:15:07.820", "lastModified": "2021-07-06T21:11:19.353", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "mruby 2.1.2 has a double free in mrb_default_allocf (called from mrb_free and obj_free)."}, {"lang": "es", "value": "mruby versi\u00f3n 2.1.2, presenta una doble liberaci\u00f3n en la funci\u00f3n mrb_default_allocf (llamado desde mrb_free y obj_free)"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 6.8}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-415"}]}], "configurations": [{"operator": "AND", "nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:mruby:mruby:2.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "A6335051-1DC8-486F-AA93-79F8D40BF436"}]}, {"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": false, "criteria": "cpe:2.3:o:linux:linux_kernel:-:*:*:*:*:*:*:*", "matchCriteriaId": "703AF700-7A70-47E2-BC3A-7FD03B3CA9C1"}]}]}], "references": [{"url": "https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=23801", "source": "cve@mitre.org", "tags": ["Exploit", "Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/google/oss-fuzz-vulns/blob/main/vulns/mruby/OSV-2020-744.yaml", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/mruby/mruby/commit/97319697c8f9f6ff27b32589947e1918e3015503", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/mruby/mruby/commit/97319697c8f9f6ff27b32589947e1918e3015503"}}