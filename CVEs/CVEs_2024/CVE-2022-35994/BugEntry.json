{"buggy_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <string>\n#include <utility>\n\n#include \"absl/strings/str_cat.h\"\n#include \"absl/strings/str_format.h\"\n#include \"tensorflow/core/framework/attr_value.pb.h\"\n#include \"tensorflow/core/framework/collective.h\"\n#include \"tensorflow/core/framework/device_attributes.pb.h\"\n#include \"tensorflow/core/framework/node_def.pb.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/op_requires.h\"\n#include \"tensorflow/core/framework/resource_handle.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/tensor_util.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/framework/types.pb.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/refcount.h\"\n#include \"tensorflow/core/platform/status.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\n\nnamespace {\n\nstatic string CollectiveKey(OpKernelContext* ctx, int32_t group_key,\n                            int32_t instance_key) {\n  return strings::StrCat(group_key, \":\", instance_key, \":\",\n                         ctx->frame_iter().frame_id, \":\",\n                         ctx->frame_iter().iter_id);\n}\n\nstatic std::unique_ptr<OpKernel> BuildOpKernel(OpKernelConstruction* c,\n                                               const string& name,\n                                               NodeDef* sub_node) {\n  std::unique_ptr<OpKernel> k;\n  if (name.empty() || name == \"Id\") return k;\n  sub_node->set_name(name);\n  sub_node->set_op(name);\n  Status status;\n  k = CreateOpKernel(c->device_type(), c->device(),\n                     c->device()->GetAllocator(AllocatorAttributes()),\n                     *sub_node, c->graph_def_version(), &status);\n  if (!status.ok()) {\n    c->CtxFailureWithWarning(errors::Internal(\n        \"Failed to build OpKernel for \", name, \" : \", status.error_message()));\n  }\n  return k;\n}\n\nclass CollectiveOpV1Kernel : public AsyncOpKernel {\n public:\n  explicit CollectiveOpV1Kernel(OpKernelConstruction* c)\n      : AsyncOpKernel(c), name_(name()), col_params_(new CollectiveParams()) {}\n\n  ~CollectiveOpV1Kernel() override { col_params_->Unref(); }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    CollectiveExecutor* col_exec = c->collective_executor();\n    OP_REQUIRES_ASYNC(\n        c, col_exec,\n        errors::Internal(\n            \"Failed to get CollectiveExecutor from OpKernelContext for Op \",\n            name_),\n        done);\n    const CancellationToken token =\n        c->cancellation_manager()->get_cancellation_token();\n    const bool already_cancelled =\n        !c->cancellation_manager()->RegisterCallback(token, [col_exec]() {\n          // We must call StartAbort() within the callback. StartAbort() relies\n          // on resources that may be deallocated if all execution of a graph is\n          // finished.\n          col_exec->StartAbort(errors::Cancelled(\"op cancelled\"));\n        });\n    OP_REQUIRES_ASYNC(c, !already_cancelled,\n                      errors::Cancelled(\"op cancelled \", name_), done);\n\n    auto deregister_and_done = [c, token, done = std::move(done)]() {\n      // Once done() is called, StartAbort() won't have any effect, so we\n      // don't need to block on the deregistration. Also StartAbort() may call\n      // done() and DeregisterCallback may deadlock.\n      c->cancellation_manager()->TryDeregisterCallback(token);\n      done();\n    };\n    ComputeAsyncImpl(c, col_exec, std::move(deregister_and_done));\n  }\n\n  // A string encoding instance, frame and iter to be handed off to\n  // the implementation for use in generating RecvBuf keys.\n  string GetCollectiveKey(OpKernelContext* c) {\n    return CollectiveKey(c, col_params_->group.group_key,\n                         col_params_->instance.instance_key);\n  }\n\n  // Returns false if calling invocation of ComputeAsync should return\n  // immediately.\n  bool CanProceedWithCompute(OpKernelContext* c, CollectiveExecutor* col_exec,\n                             const DoneCallback& done) {\n    if (col_params_->group.group_size > col_params_->group.members.size()) {\n      // This is the first invocation: Finish initializing col_params_.\n      // Schedule the `CompleteParamsAsync` call on a work queue that can handle\n      // blocking work because it's not guaranteed that this call cannot block.\n      c->collective_executor()->RunClosure([this, c, col_exec, done]() {\n        VLOG(1) << \"CollectiveOpKernel CompleteParams for collective \"\n                << col_params_->name << \" device \" << c->device()->name()\n                << \" group \" << col_params_->group.group_key << \" instance \"\n                << col_params_->instance.instance_key;\n        col_exec->CompleteParamsAsync(\n            c->device()->attributes(), col_params_, c->cancellation_manager(),\n            [this, c, done](const Status& s) {\n              if (s.ok()) {\n                col_params_->instance.impl_details.dependencies = dependencies_;\n                ComputeAsync(c, done);\n              } else {\n                c->SetStatus(s);\n                done();\n              }\n            });\n      });\n      return false;\n    }\n    return true;\n  }\n\n protected:\n  virtual void ComputeAsyncImpl(OpKernelContext* c,\n                                CollectiveExecutor* col_exec,\n                                DoneCallback done) = 0;\n\n  string name_;\n  CollectiveParams* col_params_;\n  std::vector<int32> dependencies_;\n};\n\nclass CollectiveGatherOpKernel : public CollectiveOpV1Kernel {\n public:\n  explicit CollectiveGatherOpKernel(OpKernelConstruction* c)\n      : CollectiveOpV1Kernel(c) {\n    col_params_->instance.type = GATHER_COLLECTIVE;\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_->group.group_size));\n    OP_REQUIRES(\n        c, col_params_->group.group_size > 0,\n        errors::InvalidArgument(\"group_size must be positive integer but got \",\n                                col_params_->group.group_size));\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_key\", &col_params_->group.group_key));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"instance_key\", &col_params_->instance.instance_key));\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"communication_hint\",\n                      &col_params_->instance.impl_details.communication_hint));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"timeout_seconds\",\n                      &col_params_->instance.impl_details.timeout_seconds));\n    const NodeDef& real_node = c->def();\n    col_params_->name = strings::StrCat(real_node.name(), \": Gather\");\n    col_params_->group.device_type = c->device_type();\n  }\n\n protected:\n  void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,\n                        DoneCallback done) override {\n    auto output_shape = c->input(0).shape();\n    output_shape.set_dim(\n        0, output_shape.dim_size(0) * col_params_->group.group_size);\n    col_params_->instance.shape = output_shape;\n\n    // Allocate output on the first pass through this function.  This must be\n    // done immediately, while we're still in the executor thread.  Otherwise\n    // the memory is not guaranteed to be unused by any concurrently executing\n    // GPU kernel.\n    if (c->mutable_output(0) == nullptr) {\n      // Allocate the output tensor.\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK_ASYNC(\n          c, c->allocate_output(0, col_params_->instance.shape, &output), done);\n    }\n    if (!CanProceedWithCompute(c, col_exec, done)) return;\n\n    auto actual_done = [c, col_params = col_params_, done](const Status& s) {\n      VLOG(1) << \"CollectiveGatherOpKernel ExecuteAsync done for collective \"\n              << c->op_kernel().name() << \" device \" << c->device()->name()\n              << \" group \" << col_params->group.group_key << \" instance \"\n              << col_params->instance.instance_key << \" status \" << s;\n      col_params->Unref();\n      OP_REQUIRES_OK_ASYNC(c, s, done);\n      done();\n    };\n    VLOG(1) << \"CollectiveGatherOpKernel ExecuteAsync start for collective \"\n            << col_params_->name << \" device \" << c->device()->name()\n            << \" group \" << col_params_->group.group_key << \" instance \"\n            << col_params_->instance.instance_key;\n    col_params_->Ref();\n    col_exec->ExecuteAsync(c, col_params_, GetCollectiveKey(c), actual_done);\n  }\n\n private:\n  TF_DISALLOW_COPY_AND_ASSIGN(CollectiveGatherOpKernel);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveGather\").Device(DEVICE_CPU),\n                        CollectiveGatherOpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveGather\").Device(DEVICE_GPU),\n                        CollectiveGatherOpKernel);\n\nclass CollectiveReduceOpKernel : public CollectiveOpV1Kernel {\n public:\n  explicit CollectiveReduceOpKernel(OpKernelConstruction* c)\n      : CollectiveOpV1Kernel(c) {\n    col_params_->instance.type = REDUCTION_COLLECTIVE;\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_->group.group_size));\n    OP_REQUIRES(\n        c, col_params_->group.group_size > 0,\n        errors::InvalidArgument(\"group_size must be positive integer but got \",\n                                col_params_->group.group_size));\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_key\", &col_params_->group.group_key));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"instance_key\", &col_params_->instance.instance_key));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"subdiv_offsets\",\n                      &col_params_->instance.impl_details.subdiv_offsets));\n    string merge_op_name;\n    OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));\n    if (merge_op_name == \"Max\") {\n      merge_op_name = \"Maximum\";\n    } else if (merge_op_name == \"Min\") {\n      merge_op_name = \"Minimum\";\n    }\n    string final_op_name;\n    OP_REQUIRES_OK(c, c->GetAttr(\"final_op\", &final_op_name));\n    OP_REQUIRES(c, final_op_name == \"Id\" || final_op_name == \"Div\",\n                errors::InvalidArgument(\n                    \"final_op must be one of {\\\"Id\\\", \\\"Div\\\"} but got \",\n                    final_op_name));\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));\n    OP_REQUIRES_OK(c, c->GetAttr(\"wait_for\", &dependencies_));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"communication_hint\",\n                      &col_params_->instance.impl_details.communication_hint));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"timeout_seconds\",\n                      &col_params_->instance.impl_details.timeout_seconds));\n    VLOG(2) << \"CollectiveReduce instance \"\n            << col_params_->instance.instance_key << \" merge_op \"\n            << merge_op_name << \" final_op \" << final_op_name\n            << \" communication_hint \"\n            << col_params_->instance.impl_details.communication_hint\n            << \" timeout \"\n            << col_params_->instance.impl_details.timeout_seconds;\n\n    const NodeDef& real_node = c->def();\n    col_params_->name = strings::StrCat(real_node.name(), \": Reduce(\",\n                                        merge_op_name, \",\", final_op_name, \")\");\n    col_params_->group.device_type = c->device_type();\n\n    // Find the OpKernels by name, type and device type.\n    NodeDef sub_node;\n    // The merge_op takes two inputs\n    sub_node.add_input(real_node.input(0));\n    sub_node.add_input(real_node.input(0));\n    sub_node.set_device(real_node.device());\n    SetAttrValue(col_params_->instance.data_type,\n                 &(*sub_node.mutable_attr())[\"T\"]);\n    merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);\n    final_op_ = BuildOpKernel(c, final_op_name, &sub_node);\n    col_params_->merge_op = merge_op_.get();\n    col_params_->final_op = final_op_.get();\n  }\n\n protected:\n  void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,\n                        DoneCallback done) override {\n    // Allocate output on the first pass through this function.  This must be\n    // done immediately, while we're still in the executor thread.  Otherwise\n    // the memory is not guaranteed to be unused by any concurrently executing\n    // GPU kernel.\n    if (c->mutable_output(0) == nullptr) {\n      // Allocate the output tensor, trying to reuse the input.\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK_ASYNC(c,\n                           c->forward_input_or_allocate_output(\n                               {0}, 0, c->input(0).shape(), &output),\n                           done);\n      col_params_->instance.shape = c->input(0).shape();\n    }\n    if (!CanProceedWithCompute(c, col_exec, done)) return;\n\n    auto actual_done = [c, col_params = col_params_, done](const Status& s) {\n      VLOG(1) << \"CollectiveReduceOpKernel ExecuteAsync done for collective \"\n              << c->op_kernel().name() << \" device \" << c->device()->name()\n              << \" group \" << col_params->group.group_key << \" instance \"\n              << col_params->instance.instance_key << \" status \" << s;\n      col_params->Unref();\n      OP_REQUIRES_OK_ASYNC(c, s, done);\n      done();\n    };\n    VLOG(1) << \"CollectiveReduceOpKernel ExecuteAsync start for collective \"\n            << col_params_->name << \" device \" << c->device()->name()\n            << \" group \" << col_params_->group.group_key << \" instance \"\n            << col_params_->instance.instance_key;\n    col_params_->Ref();\n    col_exec->ExecuteAsync(c, col_params_, GetCollectiveKey(c), actual_done);\n  }\n\n private:\n  std::unique_ptr<OpKernel> merge_op_;\n  std::unique_ptr<OpKernel> final_op_;\n  TF_DISALLOW_COPY_AND_ASSIGN(CollectiveReduceOpKernel);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduce\").Device(DEVICE_CPU),\n                        CollectiveReduceOpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduce\").Device(DEVICE_GPU),\n                        CollectiveReduceOpKernel);\n\nclass CollectiveBcastSendOpKernel : public CollectiveOpV1Kernel {\n public:\n  explicit CollectiveBcastSendOpKernel(OpKernelConstruction* c)\n      : CollectiveOpV1Kernel(c) {\n    col_params_->instance.type = BROADCAST_COLLECTIVE;\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_->group.group_size));\n    OP_REQUIRES(\n        c, col_params_->group.group_size > 0,\n        errors::InvalidArgument(\"group_size must be positive integer but got \",\n                                col_params_->group.group_size));\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_key\", &col_params_->group.group_key));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"instance_key\", &col_params_->instance.instance_key));\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));\n    OP_REQUIRES_OK(c, c->GetAttr(\"shape\", &col_params_->instance.shape));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"communication_hint\",\n                      &col_params_->instance.impl_details.communication_hint));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"timeout_seconds\",\n                      &col_params_->instance.impl_details.timeout_seconds));\n    col_params_->is_source = true;\n    col_params_->instance.impl_details.subdiv_offsets = {0};\n\n    col_params_->name =\n        strings::StrCat(name(), \": Broadcast(\", col_params_->is_source, \")\");\n    col_params_->group.device_type = c->device_type();\n  }\n\n protected:\n  void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,\n                        DoneCallback done) override {\n    // Allocate output on the first pass through this function.  This must be\n    // done immediately, while we're still in the executor thread.  Otherwise\n    // the memory is not guaranteed to be unused by any concurrently executing\n    // GPU kernel.\n    if (c->mutable_output(0) == nullptr) {\n      // Allocate the output tensor, trying to reuse the input.\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK_ASYNC(c,\n                           c->forward_input_or_allocate_output(\n                               {0}, 0, col_params_->instance.shape, &output),\n                           done);\n    }\n    if (!CanProceedWithCompute(c, col_exec, done)) return;\n    OP_REQUIRES_ASYNC(\n        c, col_params_->instance.shape.IsSameSize(c->input(0).shape()),\n        errors::Internal(\"Declared shape of op \", col_params_->name,\n                         \" does not match shape of input\"),\n        done);\n\n    auto actual_done = [c, col_params = col_params_, done](const Status& s) {\n      VLOG(1) << \"CollectiveBcastSendOpKernel ExecuteAsync done for collective \"\n              << c->op_kernel().name() << \" device \" << c->device()->name()\n              << \" group \" << col_params->group.group_key << \" instance \"\n              << col_params->instance.instance_key << \" status \" << s;\n      col_params->Unref();\n      OP_REQUIRES_OK_ASYNC(c, s, done);\n      done();\n    };\n    VLOG(1) << \"CollectiveBcastSendOpKernel ExecuteAsync start for collective \"\n            << col_params_->name << \" device \" << c->device()->name()\n            << \" group \" << col_params_->group.group_key << \" instance \"\n            << col_params_->instance.instance_key;\n    col_params_->Ref();\n    col_exec->ExecuteAsync(c, col_params_, GetCollectiveKey(c), actual_done);\n  }\n\n private:\n  TF_DISALLOW_COPY_AND_ASSIGN(CollectiveBcastSendOpKernel);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSend\").Device(DEVICE_CPU),\n                        CollectiveBcastSendOpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSend\").Device(DEVICE_DEFAULT),\n                        CollectiveBcastSendOpKernel);\n\nclass CollectiveBcastRecvOpKernel : public CollectiveOpV1Kernel {\n public:\n  explicit CollectiveBcastRecvOpKernel(OpKernelConstruction* c)\n      : CollectiveOpV1Kernel(c) {\n    col_params_->instance.type = BROADCAST_COLLECTIVE;\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_->group.group_size));\n    OP_REQUIRES(\n        c, col_params_->group.group_size > 0,\n        errors::InvalidArgument(\"group_size must be positive integer but got \",\n                                col_params_->group.group_size));\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_key\", &col_params_->group.group_key));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"instance_key\", &col_params_->instance.instance_key));\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));\n    OP_REQUIRES_OK(c, c->GetAttr(\"shape\", &col_params_->instance.shape));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"communication_hint\",\n                      &col_params_->instance.impl_details.communication_hint));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"timeout_seconds\",\n                      &col_params_->instance.impl_details.timeout_seconds));\n    col_params_->is_source = false;\n    col_params_->instance.impl_details.subdiv_offsets = {0};\n\n    col_params_->name =\n        strings::StrCat(name(), \": Broadcast(\", col_params_->is_source, \")\");\n    col_params_->group.device_type = c->device_type();\n  }\n\n protected:\n  void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,\n                        DoneCallback done) override {\n    // Allocate output on the first pass through this function.  This must be\n    // done immediately, while we're still in the executor thread.  Otherwise\n    // the memory is not guaranteed to be unused by any concurrently executing\n    // GPU kernel.\n    if (c->mutable_output(0) == nullptr) {\n      // No input, so must allocate output.\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK_ASYNC(\n          c, c->allocate_output(0, col_params_->instance.shape, &output), done);\n    }\n    if (!CanProceedWithCompute(c, col_exec, done)) return;\n\n    auto actual_done = [c, col_params = col_params_, done](const Status& s) {\n      VLOG(1) << \"CollectiveBcastRecvOpKernel ExecuteAsync done for collective \"\n              << c->op_kernel().name() << \" device \" << c->device()->name()\n              << \" group \" << col_params->group.group_key << \" instance_key \"\n              << col_params->instance.instance_key << \" status  \" << s;\n      col_params->Unref();\n      OP_REQUIRES_OK_ASYNC(c, s, done);\n      done();\n    };\n    VLOG(1) << \"CollectiveBcastRecvOpKernel ExecuteAsync start for collective \"\n            << col_params_->name << \" device \" << c->device()->name()\n            << \" group \" << col_params_->group.group_key << \" instance \"\n            << col_params_->instance.instance_key;\n    col_params_->Ref();\n    col_exec->ExecuteAsync(c, col_params_, GetCollectiveKey(c), actual_done);\n  }\n\n private:\n  TF_DISALLOW_COPY_AND_ASSIGN(CollectiveBcastRecvOpKernel);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecv\").Device(DEVICE_CPU),\n                        CollectiveBcastRecvOpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecv\").Device(DEVICE_DEFAULT),\n                        CollectiveBcastRecvOpKernel);\n\nclass CollectiveAssignGroupV2OpKernel : public OpKernel {\n public:\n  explicit CollectiveAssignGroupV2OpKernel(OpKernelConstruction* c)\n      : OpKernel(c) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& group_assignment = context->input(0);\n    const Tensor& device_index = context->input(1);\n    const Tensor& base_key = context->input(2);\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(device_index.shape()),\n        errors::InvalidArgument(\n            \"device_index must be a scalar, but received tensor of shape: \",\n            device_index.shape().DebugString()));\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsMatrix(group_assignment.shape()),\n        errors::InvalidArgument(\"group_assignment must be a 2-d Tensor, but \"\n                                \"received tensor of shape: \",\n                                group_assignment.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(base_key.shape()),\n                errors::InvalidArgument(\n                    \"base_key must be a scalar, but received tensor of shape: \",\n                    base_key.shape().DebugString()));\n\n    Tensor* group_key = nullptr;\n    Tensor* group_size = nullptr;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}),\n                                                     &group_size, attr));\n\n    OP_REQUIRES_OK(context, context->allocate_output(1, TensorShape({}),\n                                                     &group_key, attr));\n\n    OP_REQUIRES_OK(\n        context,\n        ComputeGroupKey(group_assignment, device_index.scalar<int32_t>()(),\n                        base_key.scalar<int32_t>()(), group_size, group_key));\n  }\n\n private:\n  static Status ComputeGroupKey(const Tensor& group_assignment,\n                                const int32_t device_index,\n                                const int32_t base_key, Tensor* group_size,\n                                Tensor* group_key) {\n    group_size->flat<int32_t>()(0) = group_assignment.dim_size(1);\n\n    for (int group_id = 0; group_id < group_assignment.dim_size(0);\n         group_id++) {\n      int32_t key = static_cast<int32_t>(static_cast<uint32_t>(base_key) +\n                                         static_cast<uint32_t>(group_id));\n      if (key == 0) {\n        return errors::InvalidArgument(\n            \"Using the reserved group_key = 0 is not allowed: group_id = \",\n            group_id, \", base_key = \", base_key);\n      }\n      for (int color = 0; color < group_assignment.dim_size(1); color++) {\n        const auto index = group_assignment.matrix<int32>()(group_id, color);\n        if (index < 0 || index >= group_assignment.shape().num_elements()) {\n          return errors::InvalidArgument(\"Not all items in group_assignment \",\n                                         group_assignment.DebugString(),\n                                         \" is within [0, number of devices)\");\n        }\n        if (index == device_index) {\n          group_key->flat<int32_t>()(0) = key;\n          VLOG(2) << \" group_assignment = \" << group_assignment.DebugString()\n                  << \" device_index = \" << index\n                  << \" group_key = \" << group_key->DebugString()\n                  << \" group_size = \" << group_size->DebugString();\n          return OkStatus();\n        }\n      }\n    }\n    return errors::InvalidArgument(\"device_index \", device_index,\n                                   \" is not found in group_assignment \",\n                                   group_assignment.DebugString());\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveAssignGroupV2\").Device(DEVICE_CPU),\n                        CollectiveAssignGroupV2OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveAssignGroupV2\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"device_index\")\n                            .HostMemory(\"group_assignment\")\n                            .HostMemory(\"base_key\")\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\"),\n                        CollectiveAssignGroupV2OpKernel);\n\nclass CollectiveOpV2Kernel : public AsyncOpKernel {\n public:\n  explicit CollectiveOpV2Kernel(OpKernelConstruction* c)\n      : AsyncOpKernel(c), name_(name()), device_type_(DEVICE_DEFAULT) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));\n    OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));\n    OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));\n    device_type_ = c->device_type();\n  }\n\n protected:\n  // Fills common parts of CollectiveParams according to the Op, *excluding\n  // output_shape*. Kernels should further work on the CollectiveParams if they\n  // need to set additional fields.\n  Status FillCollectiveParams(CollectiveParams* col_params,\n                              CollectiveType collective_type,\n                              const Tensor& group_size, const Tensor& group_key,\n                              const Tensor& instance_key) {\n    if (group_size.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input group_size, got \",\n          group_size.shape().DebugString());\n    }\n    if (group_key.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input group_key, got \",\n          group_key.shape().DebugString());\n    }\n    if (instance_key.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input instance_key, got \",\n          instance_key.shape().DebugString());\n    }\n    col_params->name = name_;\n    col_params->group.device_type = device_type_;\n    col_params->group.group_size = group_size.unaligned_flat<int32>()(0);\n    if (col_params->group.group_size <= 0) {\n      return errors::InvalidArgument(\n          \"group_size must be positive integer but got \",\n          col_params->group.group_size);\n    }\n    col_params->group.group_key = group_key.unaligned_flat<int32>()(0);\n    col_params->instance.type = collective_type;\n    col_params->instance.instance_key = instance_key.unaligned_flat<int32>()(0);\n    col_params->instance.data_type = data_type_;\n    col_params->instance.impl_details.communication_hint = communication_hint_;\n    col_params->instance.impl_details.timeout_seconds = timeout_seconds_;\n    return OkStatus();\n  }\n\n  // Runs a collective. The output tensor must be allocated before calling this\n  // method. col_params must live until done is called.\n  void Run(OpKernelContext* c, CollectiveParams* col_params,\n           DoneCallback done) {\n    CollectiveExecutor* col_exec = c->collective_executor();\n    OP_REQUIRES_ASYNC(\n        c, col_exec,\n        errors::Internal(\n            \"Failed to get CollectiveExecutor from OpKernelContext for Op \",\n            name_),\n        done);\n    // Resolve the collective params.\n    // Schedule the `CompleteParamsAsync` call on a work queue that can handle\n    // blocking work because it's not guaranteed that this call cannot block.\n    c->collective_executor()->RunClosure([c, done = std::move(done), col_params,\n                                          col_exec]() {\n      VLOG(1) << \"Collective CompleteParams for \" << col_params->name\n              << \" device \" << c->device()->name() << \" group \"\n              << col_params->group.group_key << \" instance \"\n              << col_params->instance.instance_key;\n      col_exec->CompleteParamsAsync(\n          c->device()->attributes(), col_params, c->cancellation_manager(),\n          [c, done = std::move(done), col_params, col_exec](const Status& s) {\n            if (s.ok()) {\n              auto actual_done = [c, col_params,\n                                  done = std::move(done)](const Status& s) {\n                VLOG(1) << \"Collective ExecuteAsync done for \"\n                        << col_params->name << \" device \" << c->device()->name()\n                        << \" group \" << col_params->group.group_key\n                        << \" instance \" << col_params->instance.instance_key\n                        << \" status \" << s;\n                if (!s.ok()) {\n                  c->SetStatus(s);\n                }\n                done();\n              };\n              VLOG(1) << \"Collective ExecuteAsync start for \"\n                      << col_params->name << \" device \" << c->device()->name()\n                      << \" group \" << col_params->group.group_key\n                      << \" instance \" << col_params->instance.instance_key;\n              col_exec->ExecuteAsync(\n                  c, col_params,\n                  CollectiveKey(c, col_params->group.group_key,\n                                col_params->instance.instance_key),\n                  actual_done);\n            } else {\n              c->SetStatus(s);\n              done();\n            }\n          });\n    });\n  }\n\n protected:\n  string name_;\n  DataType data_type_ = DT_INVALID;\n  string communication_hint_;\n  float timeout_seconds_ = 0;\n  DeviceType device_type_;\n};\n\nclass CollectiveReduceV2OpKernel : public CollectiveOpV2Kernel {\n public:\n  explicit CollectiveReduceV2OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV2Kernel(c) {\n    string merge_op_name;\n    OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));\n    if (merge_op_name == \"Max\") {\n      merge_op_name = \"Maximum\";\n    } else if (merge_op_name == \"Min\") {\n      merge_op_name = \"Minimum\";\n    }\n    string final_op_name;\n    OP_REQUIRES_OK(c, c->GetAttr(\"final_op\", &final_op_name));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"max_subdivs_per_device\", &max_subdivs_per_device_));\n    // Prepare OpKernels for reduction and final operations.\n    // The merge_op takes two inputs\n    NodeDef sub_node;\n    sub_node.add_input(c->def().input(0));\n    sub_node.add_input(c->def().input(0));\n    sub_node.set_device(c->def().device());\n    SetAttrValue(data_type_, &(*sub_node.mutable_attr())[\"T\"]);\n    merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);\n    final_op_ = BuildOpKernel(c, final_op_name, &sub_node);\n    name_ = strings::StrCat(c->def().name(), \": ReduceV2(\", merge_op_name, \",\",\n                            final_op_name, \")\");\n    VLOG(2) << \"CollectiveReduceV2 \" << this << \" name \" << name_\n            << \" communication_hint \" << communication_hint_;\n  }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    OP_REQUIRES_OK_ASYNC(c,\n                         FillCollectiveParams(col_params, REDUCTION_COLLECTIVE,\n                                              /*group_size*/ c->input(1),\n                                              /*group_key*/ c->input(2),\n                                              /*instance_key*/ c->input(3)),\n                         done_with_cleanup);\n    col_params->instance.shape = c->input(0).shape();\n    col_params->merge_op = merge_op_.get();\n    col_params->final_op = final_op_.get();\n    VLOG(1) << \"CollectiveReduceV2 group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n\n private:\n  int max_subdivs_per_device_;\n  std::unique_ptr<OpKernel> merge_op_;\n  std::unique_ptr<OpKernel> final_op_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV2\").Device(DEVICE_CPU),\n                        CollectiveReduceV2OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV2\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\")\n                            .HostMemory(\"instance_key\"),\n                        CollectiveReduceV2OpKernel);\n\nclass CollectiveGatherV2OpKernel : public CollectiveOpV2Kernel {\n public:\n  explicit CollectiveGatherV2OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV2Kernel(c) {\n    name_ = strings::StrCat(c->def().name(), \": GatherV2\");\n    VLOG(2) << \"CollectiveGatherV2 \" << this << \" name \" << name_\n            << \" communication_hint \" << communication_hint_;\n  }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    OP_REQUIRES_OK_ASYNC(c,\n                         FillCollectiveParams(col_params, GATHER_COLLECTIVE,\n                                              /*group_size*/ c->input(1),\n                                              /*group_key*/ c->input(2),\n                                              /*instance_key*/\n                                              c->input(3)),\n                         done_with_cleanup);\n    auto output_shape = c->input(0).shape();\n    output_shape.set_dim(\n        0, output_shape.dim_size(0) * col_params->group.group_size);\n    col_params->instance.shape = output_shape;\n    VLOG(1) << \"CollectiveGatherV2 group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(\n        c, c->allocate_output(0, col_params->instance.shape, &output),\n        done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveGatherV2\").Device(DEVICE_CPU),\n                        CollectiveGatherV2OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveGatherV2\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\")\n                            .HostMemory(\"instance_key\"),\n                        CollectiveGatherV2OpKernel);\n\nclass CollectiveBcastSendV2OpKernel : public CollectiveOpV2Kernel {\n public:\n  explicit CollectiveBcastSendV2OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV2Kernel(c) {\n    const bool is_source = true;\n    name_ = strings::StrCat(name(), \": Broadcast(\", is_source, \")\");\n  }\n\n protected:\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    OP_REQUIRES_OK_ASYNC(c,\n                         FillCollectiveParams(col_params, BROADCAST_COLLECTIVE,\n                                              /*group_size*/ c->input(1),\n                                              /*group_key*/ c->input(2),\n                                              /*instance_key*/ c->input(3)),\n                         done_with_cleanup);\n    col_params->is_source = true;\n    col_params->instance.shape = c->input(0).shape();\n    // Add a default value for subdiv offsets, which is the same as the default\n    // value in the V1 op's attribute.\n    col_params->instance.impl_details.subdiv_offsets.push_back(0);\n    VLOG(1) << \"CollectiveBcastSendV2 group_size \"\n            << col_params->group.group_size << \" group_key \"\n            << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSendV2\").Device(DEVICE_CPU),\n                        CollectiveBcastSendV2OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSendV2\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\")\n                            .HostMemory(\"instance_key\"),\n                        CollectiveBcastSendV2OpKernel);\n\nclass CollectiveBcastRecvV2OpKernel : public CollectiveOpV2Kernel {\n public:\n  explicit CollectiveBcastRecvV2OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV2Kernel(c) {\n    const bool is_source = false;\n    name_ = strings::StrCat(name(), \": Broadcast(\", is_source, \")\");\n  }\n\n protected:\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    OP_REQUIRES_OK_ASYNC(c,\n                         FillCollectiveParams(col_params, BROADCAST_COLLECTIVE,\n                                              /*group_size*/ c->input(0),\n                                              /*group_key*/ c->input(1),\n                                              /*instance_key*/ c->input(2)),\n                         done_with_cleanup);\n    col_params->is_source = false;\n    TensorShape output_shape;\n    OP_REQUIRES_OK_ASYNC(c, tensor::MakeShape(c->input(3), &output_shape),\n                         done_with_cleanup);\n    col_params->instance.shape = output_shape;\n    // Add a default value for subdiv offsets, which is the same as the default\n    // value in the V1 op's attribute.\n    col_params->instance.impl_details.subdiv_offsets.push_back(0);\n    VLOG(1) << \"CollectiveBcastRecvV2 group_size \"\n            << col_params->group.group_size << \" group_key \"\n            << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(\n        c, c->allocate_output(0, col_params->instance.shape, &output),\n        done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecvV2\").Device(DEVICE_CPU),\n                        CollectiveBcastRecvV2OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecvV2\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\")\n                            .HostMemory(\"instance_key\")\n                            .HostMemory(\"shape\"),\n                        CollectiveBcastRecvV2OpKernel);\n\n/*\n * Resource for holding group for CollectiveOps.\n * This resource is returned from CollectiveInitializeCommunicatorOpKernel\n * It generates next instance key for the group for each collective operation.\n */\nclass CollectiveGroupResource : public ResourceBase {\n public:\n  CollectiveGroupResource(int32 group_key, int32 rank, int32 group_size,\n                          string communication_hint, float timeout_seconds)\n      : group_key_(group_key),\n        rank_(rank),\n        group_size_(group_size),\n        communication_hint_(communication_hint),\n        timeout_seconds_(timeout_seconds) {}\n\n  std::string DebugString() const override {\n    return absl::StrFormat(\n        \"Collective Group with group_key = %d, group_size = %d, rank = %d\",\n        group_key_, group_size_, rank_);\n  }\n\n  int get_next_instance_key() {\n    return instance_key_.fetch_add(1, std::memory_order_relaxed);\n  }\n\n  int32 group_key() const { return group_key_; }\n\n  int32 rank() const { return rank_; }\n\n  int32 group_size() const { return group_size_; }\n\n  string communication_hint() const { return communication_hint_; }\n\n  float timeout_seconds() const { return timeout_seconds_; }\n\n private:\n  int32 group_key_, rank_, group_size_;\n  string communication_hint_;\n  std::atomic<int> instance_key_{0};\n  float timeout_seconds_ = 0;\n};\n\nclass CollectiveInitializeCommunicatorOpKernel : public AsyncOpKernel {\n public:\n  explicit CollectiveInitializeCommunicatorOpKernel(OpKernelConstruction* c)\n      : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));\n    OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));\n    device_type_ = c->device_type();\n  }\n\n  Status CheckInputs(Tensor group_size_t, Tensor group_key_t) {\n    if (group_size_t.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input group_size. \"\n          \"It shoulbe a scalar, got tensor with shape \",\n          group_size_t.shape().DebugString());\n    }\n    if (group_key_t.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input group_key, got \",\n          group_key_t.shape().DebugString());\n    }\n\n    auto group_size = group_size_t.unaligned_flat<int32>()(0);\n    if (group_size <= 0) {\n      return errors::InvalidArgument(\n          \"group_size must be positive integer but got \", group_size);\n    }\n    return OkStatus();\n  }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto group_key_t = c->input(0);\n    auto rank_t = c->input(1);\n    auto group_size_t = c->input(2);\n\n    OP_REQUIRES_OK_ASYNC(c, CheckInputs(group_size_t, group_key_t), done);\n\n    auto group_size = group_size_t.unaligned_flat<int32>()(0);\n    auto group_key = group_key_t.unaligned_flat<int32>()(0);\n    auto rank = rank_t.unaligned_flat<int32>()(0);\n\n    ResourceHandle resource_handle =\n        MakeResourceHandle<CollectiveGroupResource>(\n            c, \"collective_op_group\",\n            absl::StrFormat(\"%d:r%04d\", group_key, rank));\n\n    Tensor* output_handle = nullptr;\n    OP_REQUIRES_OK_ASYNC(\n        c, c->allocate_output(0, TensorShape({}), &output_handle), done);\n    output_handle->scalar<ResourceHandle>()() = resource_handle;\n\n    CollectiveGroupResource* resource = new CollectiveGroupResource(\n        group_key, rank, group_size, this->communication_hint_,\n        this->timeout_seconds_);\n    OP_REQUIRES_OK_ASYNC(\n        c,\n        CreateResource<CollectiveGroupResource>(c, resource_handle, resource),\n        done);\n    auto group_params = new CollGroupParams();\n    group_params->device_type = device_type_;\n    group_params->group_size = resource->group_size();\n    group_params->group_key = resource->group_key();\n    group_params->user_specified_rank = resource->rank();\n\n    auto* col_exec = c->collective_executor();\n\n    c->collective_executor()->RunClosure([c, done = std::move(done),\n                                          group_params, col_exec]() {\n      VLOG(1) << \"Collective Group initialization for \"\n              << \" device \" << c->device()->name() << \" group \"\n              << group_params->group_key;\n      col_exec->CompleteGroupAsync(\n          c->device()->attributes(), group_params, c->cancellation_manager(),\n          [c, done = std::move(done), group_params](const Status& s) {\n            if (s.ok()) {\n              VLOG(1) << \"Collective Group initialization done for device \"\n                      << c->device()->name() << \" group \"\n                      << group_params->group_key << \" status \" << s;\n            } else {\n              c->SetStatus(s);\n            }\n            delete group_params;\n            done();\n          });\n    });\n  }\n\n private:\n  string communication_hint_;\n  DeviceType device_type_;\n  float timeout_seconds_ = 0;\n};\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"CollectiveInitializeCommunicator\").Device(DEVICE_CPU),\n    CollectiveInitializeCommunicatorOpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveInitializeCommunicator\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\")\n                            .HostMemory(\"rank\"),\n                        CollectiveInitializeCommunicatorOpKernel);\n\nclass CollectiveOpV3Kernel : public AsyncOpKernel {\n public:\n  explicit CollectiveOpV3Kernel(OpKernelConstruction* c)\n      : AsyncOpKernel(c), name_(name()), device_type_(DEVICE_DEFAULT) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));\n    if (c->HasAttr(\"timeout_seconds\")) {\n      OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));\n    } else {\n      timeout_seconds_ = -1;\n    }\n    device_type_ = c->device_type();\n  }\n\n protected:\n  // Fills common parts of CollectiveParams according to the Op, *excluding\n  // output_shape*. Kernels should further work on the CollectiveParams if they\n  // need to set additional fields.\n  Status FillCollectiveParams(CollectiveParams* col_params,\n                              const Tensor& group_assignment,\n                              CollectiveType collective_type,\n                              CollectiveGroupResource* resource) {\n    int64 group_id;\n    int64 group_size;\n    if (group_assignment.NumElements() == 0) {\n      // No group assignments, perform collective as a single group.\n      group_id = 0;\n      group_size = resource->group_size();\n    } else {\n      return errors::Unimplemented(\"Group assignments are not supported yet.\");\n    }\n\n    // Construct instance key with format:\n    // <11 bits for group><21 bits for atomic incremented instance key>\n    int32 instance_key = group_id << 21 | resource->get_next_instance_key();\n    col_params->name = name_;\n    col_params->group.device_type = device_type_;\n    col_params->group.group_size = group_size;\n    col_params->group.group_key = resource->group_key();\n    col_params->group.user_specified_rank = resource->rank();\n    col_params->instance.type = collective_type;\n    col_params->instance.instance_key = instance_key;\n    col_params->instance.data_type = data_type_;\n    col_params->instance.impl_details.communication_hint =\n        resource->communication_hint();\n    col_params->instance.impl_details.timeout_seconds =\n        timeout_seconds_ > 0 ? resource->timeout_seconds() : timeout_seconds_;\n    col_params->run_group_initialization = false;\n    return OkStatus();\n  }\n\n  // Runs a collective. The output tensor must be allocated before calling this\n  // method. col_params must live until done is called.\n  void Run(OpKernelContext* c, CollectiveParams* col_params,\n           DoneCallback done) {\n    CollectiveExecutor* col_exec = c->collective_executor();\n    OP_REQUIRES_ASYNC(\n        c, col_exec,\n        errors::Internal(\n            \"Failed to get CollectiveExecutor from OpKernelContext for Op \",\n            name_),\n        done);\n    // Resolve the collective params.\n    // Schedule the `CompleteParamsAsync` call on a work queue that can handle\n    // blocking work because it's not guaranteed that this call cannot block.\n    col_exec->RunClosure([c, done = std::move(done), col_params, col_exec]() {\n      VLOG(1) << \"Collective CompleteParams for \" << col_params->name\n              << \" device \" << c->device()->name() << \" group \"\n              << col_params->group.group_key << \" instance \"\n              << col_params->instance.instance_key;\n      col_exec->CompleteParamsAsync(\n          c->device()->attributes(), col_params, c->cancellation_manager(),\n          [c, done = std::move(done), col_params, col_exec](const Status& s) {\n            if (s.ok()) {\n              auto actual_done = [c, col_params,\n                                  done = std::move(done)](const Status& s) {\n                VLOG(1) << \"Collective ExecuteAsync done for \"\n                        << col_params->name << \" device \" << c->device()->name()\n                        << \" group \" << col_params->group.group_key\n                        << \" instance \" << col_params->instance.instance_key\n                        << \" status \" << s;\n                if (!s.ok()) {\n                  c->SetStatus(s);\n                }\n                done();\n              };\n              VLOG(1) << \"Collective ExecuteAsync start for \"\n                      << col_params->name << \" device \" << c->device()->name()\n                      << \" group \" << col_params->group.group_key\n                      << \" instance \" << col_params->instance.instance_key;\n              col_exec->ExecuteAsync(\n                  c, col_params,\n                  CollectiveKey(c, col_params->group.group_key,\n                                col_params->instance.instance_key),\n                  actual_done);\n            } else {\n              c->SetStatus(s);\n              done();\n            }\n          });\n    });\n  }\n\n protected:\n  string name_;\n  DataType data_type_ = DT_INVALID;\n  DeviceType device_type_;\n  float timeout_seconds_ = 0;\n};\n\nclass CollectiveReduceV3OpKernel : public CollectiveOpV3Kernel {\n public:\n  explicit CollectiveReduceV3OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV3Kernel(c) {\n    string reduction;\n    OP_REQUIRES_OK(c, c->GetAttr(\"reduction\", &reduction));\n    if (reduction == \"Max\") {\n      reduction = \"Maximum\";\n    } else if (reduction == \"Min\") {\n      reduction = \"Minimum\";\n    }\n    // Prepare OpKernels for reduction and final operations.\n    // The merge_op takes two inputs\n    NodeDef sub_node;\n    sub_node.add_input(c->def().input(0));\n    sub_node.add_input(c->def().input(0));\n    sub_node.set_device(c->def().device());\n    SetAttrValue(data_type_, &(*sub_node.mutable_attr())[\"T\"]);\n    merge_op_ = BuildOpKernel(c, reduction, &sub_node);\n    final_op_ = BuildOpKernel(c, \"Id\", &sub_node);\n    name_ = strings::StrCat(c->def().name(), \": ReduceV3(\", reduction, \")\");\n    VLOG(2) << \"CollectiveReduceV3 \" << this << \" name \" << name_;\n  }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    core::RefCountPtr<CollectiveGroupResource> resource;\n    OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),\n                         done_with_cleanup);\n\n    Tensor group_assignment = c->input(2);\n\n    OP_REQUIRES_OK_ASYNC(\n        c,\n        FillCollectiveParams(col_params, group_assignment, REDUCTION_COLLECTIVE,\n                             resource.get()),\n        done_with_cleanup);\n    col_params->instance.shape = c->input(0).shape();\n    col_params->merge_op = merge_op_.get();\n    col_params->final_op = final_op_.get();\n    VLOG(1) << \"CollectiveReduceV3 group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n\n private:\n  std::unique_ptr<OpKernel> merge_op_;\n  std::unique_ptr<OpKernel> final_op_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV3\").Device(DEVICE_CPU),\n                        CollectiveReduceV3OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV3\").Device(DEVICE_GPU),\n                        CollectiveReduceV3OpKernel);\n\nclass CollectiveAllToAllV3OpKernel : public CollectiveOpV3Kernel {\n public:\n  explicit CollectiveAllToAllV3OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV3Kernel(c) {\n    name_ = strings::StrCat(c->def().name(), \": AllToAllV3\");\n    VLOG(2) << \"CollectiveAllToAllV3 \" << this << \" name \" << name_;\n  }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    core::RefCountPtr<CollectiveGroupResource> resource;\n    OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),\n                         done_with_cleanup);\n\n    Tensor group_assignment = c->input(2);\n\n    OP_REQUIRES_OK_ASYNC(\n        c,\n        FillCollectiveParams(col_params, group_assignment,\n                             ALL_TO_ALL_COLLECTIVE, resource.get()),\n        done_with_cleanup);\n    col_params->instance.shape = c->input(0).shape();\n    VLOG(1) << \"CollectiveAllToAll group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveAllToAllV3\").Device(DEVICE_CPU),\n                        CollectiveAllToAllV3OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveAllToAllV3\").Device(DEVICE_GPU),\n                        CollectiveAllToAllV3OpKernel);\n}  // namespace\n}  // namespace tensorflow\n", "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Collective Operations.\"\"\"\n\nimport time\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.core.protobuf import rewriter_config_pb2\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import config\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import kernels\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import collective_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging as logging\n\n\nclass CollectiveOpTest(test.TestCase):\n\n  def setUp(self):\n    context._reset_context()  # pylint: disable=protected-access\n    super(CollectiveOpTest, self).setUp()\n\n  def _testCollectiveReduce(self,\n                            inputs,\n                            expected,\n                            set_graph_key,\n                            communication_hint='auto',\n                            fp16=False,\n                            instance_key=1,\n                            merge_op='Add',\n                            final_op='Div',\n                            timeout=0,\n                            reported_group_size=None):\n    group_key = 1\n    group_size = len(inputs)\n    if reported_group_size is None:\n      reported_group_size = group_size\n    device_type = 'CPU'\n    config = config_pb2.ConfigProto(device_count={device_type: group_size})\n    devices = ['/{}:{}'.format(device_type, i) for i in range(group_size)]\n\n    with self.session(config=config) as sess:\n      colred = []\n      for i in range(group_size):\n        with ops.device(devices[i]):\n          tensor = constant_op.constant(inputs[i], dtype=(\n              dtypes.float16 if fp16 else dtypes.float32))\n          colred.append(\n              collective_ops.all_reduce(\n                  tensor,\n                  reported_group_size,\n                  group_key,\n                  instance_key,\n                  merge_op,\n                  final_op,\n                  communication_hint=communication_hint,\n                  timeout=timeout))\n      run_options = config_pb2.RunOptions()\n      if set_graph_key:\n        run_options.experimental.collective_graph_key = 1\n      results = sess.run(colred, options=run_options)\n    tolerance = 1e-3 if fp16 else 1e-5\n    for i in range(group_size):\n      logging.info('i {} result {} expected {}'.format(i, results[i], expected))\n      self.assertAllClose(results[i], expected, rtol=tolerance, atol=tolerance)\n\n  def _testMultipleConcurrentCollectiveReduce(self, t0, t1, expected):\n    group_key = 1\n    group_size = 2\n    num_instances = 2\n    all_reduces = []\n    config = config_pb2.ConfigProto(device_count={'CPU': group_size})\n    config.experimental.collective_deterministic_sequential_execution = True\n    with self.session(config=config) as sess:\n      for cpu in range(group_size):\n        with ops.device('/CPU:%d' % cpu):\n          in_tensor = constant_op.constant(t0 if cpu == 0 else t1)\n          for instance in range(num_instances):\n            all_reduces.append(collective_ops.all_reduce(\n                in_tensor, group_size, group_key, instance, 'Add', 'Div'))\n      results = sess.run(all_reduces)\n    for i in range(group_size * num_instances):\n      self.assertAllClose(results[i], expected, rtol=1e-5, atol=1e-5)\n\n  def testCollectiveReduce(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1],\n                  [0.3, 1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3]],\n          expected=[0.2, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2],\n          set_graph_key=True)\n\n  def testCollectiveAutoGraphKey(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1],\n                  [0.3, 1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3]],\n          expected=[0.2, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2],\n          set_graph_key=False)\n\n  def testFp16Reduce(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1],\n                  [0.3, 1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3]],\n          expected=[0.2, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2],\n          set_graph_key=True,\n          fp16=True)\n\n  def testCollectiveMultipleConcurrentReduce(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testMultipleConcurrentCollectiveReduce(\n          [0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1],\n          [0.3, 1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3],\n          [0.2, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2])\n\n  def testCollectiveTimeoutV1(self):\n    timeout = 4.5\n    kwargs = dict(\n        inputs=[[i + j + 0.1 for i in range(8)] for j in range(3)],\n        expected=[1 + i + 0.1 for i in range(8)],\n        set_graph_key=True,\n        timeout=timeout)\n\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(**kwargs)\n\n    start_time = time.time()\n    with ops.Graph().as_default():\n      with self.assertRaisesRegex(\n          errors.DeadlineExceededError,\n          'Collective has timed out waiting for other workers'):\n        self._testCollectiveReduce(\n            reported_group_size=len(kwargs['inputs']) + 1, **kwargs)\n    elapsed = time.time() - start_time\n    self.assertAllGreaterEqual(elapsed, timeout)\n\n  def testNcclHintFallbackToRingReduce(self):\n    \"\"\"Tests that setting `communication_hint=nccl` works on non-GPU builds.\"\"\"\n    if kernels.get_registered_kernels_for_op('NcclAllReduce'):\n      self.skipTest('Run only on non-GPU environments')\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1],\n                  [0.3, 1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3]],\n          expected=[0.2, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2],\n          set_graph_key=False,\n          communication_hint='nccl')\n\n  def _testWhile(self, num_vars, num_iterations, key_base):\n    group_size = 2\n    group_key = 1\n    instances = [(key_base + i) for i in range(num_vars)]\n    devices = ['CPU:{}'.format(i) for i in range(group_size)]\n\n    config = config_pb2.ConfigProto(device_count={'CPU': group_size})\n    rewrite_options = config.graph_options.rewrite_options\n    rewrite_options.scoped_allocator_optimization = (\n        rewriter_config_pb2.RewriterConfig.ON)\n    del rewrite_options.scoped_allocator_opts.enable_op[:]\n    rewrite_options.scoped_allocator_opts.enable_op.append('CollectiveReduce')\n\n    with self.session(config=config) as sess:\n      loop_vars = []\n      for device in devices:\n        with ops.device(device):\n          loop_vars.append(\n              [variables.VariableV1((1 << i) * 1.) for i in range(num_vars)])\n      # This variable controls number of iterations.\n      loop_vars.append(variables.VariableV1(0.))\n      def loop_body(dev0_tensors, dev1_tensors, loop_tensor):\n        return_ops = []\n        for i in range(len(devices)):\n          device = devices[i]\n          device_tensors = dev0_tensors if i == 0 else dev1_tensors\n          with ops.device(device):\n            device_collectives = []\n            for j in range(num_vars):\n              # NOTE(ayushd): we need the `cast` here to ensure that the input\n              # to `all_reduce` has an explicit device string.  We don't use\n              # `identity` because `cast` is more resilient to getting optimized\n              # away by various optimization passes.\n              input_tensor = math_ops.cast(device_tensors[j], dtypes.float16)\n              collective_op = collective_ops.all_reduce(\n                  input_tensor, group_size, group_key, instances[j],\n                  'Add', 'Id')\n              output_tensor = math_ops.cast(collective_op, dtypes.float32)\n              device_collectives.append(output_tensor)\n            return_ops.append(device_collectives)\n        return_ops.append(math_ops.add(loop_tensor, 1.))\n        return return_ops\n      # Run until last variable exceeds number of iterations.\n      loop_cond = lambda d0, d1, i: math_ops.less(i, num_iterations)\n      sess.run(variables.global_variables_initializer())\n      results = sess.run(control_flow_ops.while_loop(loop_cond, loop_body,\n                                                     loop_vars))\n      self.assertEqual(results[:-1], [\n          [((1 << (num_iterations + v)) * 1.) for v in range(num_vars)]\n          for _ in range(group_size)])\n\n  def testSimpleWhile(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testWhile(num_vars=1, num_iterations=4, key_base=20)\n\n  def testWhileMultipleAllReduce(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testWhile(num_vars=2, num_iterations=4, key_base=20)\n\n  def testWhileWithScopedAllocator(self):\n    group_size = 2\n    group_key = 1\n    instance_key0 = 1\n    instance_key1 = 2\n\n    config = config_pb2.ConfigProto(device_count={'CPU': group_size})\n    rewrite_options = config.graph_options.rewrite_options\n    rewrite_options.scoped_allocator_optimization = (\n        rewriter_config_pb2.RewriterConfig.ON)\n    del rewrite_options.scoped_allocator_opts.enable_op[:]\n    rewrite_options.scoped_allocator_opts.enable_op.append('CollectiveReduce')\n\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      with self.session(config=config) as sess:\n        run_ops = []\n        for i in range(group_size):\n          with ops.device('CPU:%d' % i):\n            constant = constant_op.constant(0.)\n            cond = lambda i: math_ops.less(i, 10.)\n            body = lambda i: math_ops.add(i, 1.)\n            input0 = control_flow_ops.while_loop(cond, body, [constant])\n            input1 = math_ops.add(constant, 5)\n            colred0 = collective_ops.all_reduce(input0, group_size, group_key,\n                                                instance_key0, 'Add', 'Id')\n            colred1 = collective_ops.all_reduce(input1, group_size, group_key,\n                                                instance_key1, 'Add', 'Id')\n            run_ops.append(math_ops.add_n([colred0, colred1]))\n        results = sess.run(run_ops)\n      self.assertEqual(results, [30., 30.])\n\n  def testCollectiveReduceScalar(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(inputs=[0.1, 0.3], expected=0.2,\n                                 set_graph_key=True)\n\n  def testCollectiveReduceMaximum(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[1., 20., 3., 40., 5.], [10., 2., 30., 4., 50.]],\n          expected=[10., 20., 30., 40., 50.],\n          set_graph_key=True,\n          instance_key=30,\n          merge_op='Max',\n          final_op='Id')\n\n  def testCollectiveReduceMinimum(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[1., 20., 3., 40., 5.], [10., 2., 30., 4., 50.]],\n          expected=[1., 2., 3., 4., 5.],\n          set_graph_key=True,\n          instance_key=40,\n          merge_op='Min',\n          final_op='Id')\n\n  def _testCollectiveBroadcast(self, in_val):\n    group_key = 1\n    instance_key = 1\n    with self.session(\n        config=config_pb2.ConfigProto(device_count={'CPU': 2})) as sess:\n      with ops.device('/CPU:0'):\n        in0 = constant_op.constant(in_val)\n        out0 = collective_ops.broadcast_send(in0, in0.shape, in0.dtype,\n                                             2, group_key, instance_key)\n      with ops.device('/CPU:1'):\n        c1 = constant_op.constant(in_val)\n        out1 = collective_ops.broadcast_recv(c1.shape, c1.dtype,\n                                             2, group_key, instance_key)\n      run_options = config_pb2.RunOptions()\n      run_options.experimental.collective_graph_key = 1\n      results = sess.run([out0, out1], options=run_options)\n    self.assertAllClose(results[0], in_val, rtol=1e-5, atol=1e-5)\n    self.assertAllClose(results[1], in_val, rtol=1e-5, atol=1e-5)\n\n  def testCollectiveBroadcast(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveBroadcast([0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1])\n\n  def testCollectiveBroadcastBool(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveBroadcast([True, False])\n\n  def _testCollectiveGather(self, t0, t1, expected, set_graph_key):\n    group_key = 1\n    instance_key = 1\n    with self.session(\n        config=config_pb2.ConfigProto(device_count={'CPU': 2})) as sess:\n      with ops.device('/CPU:0'):\n        in0 = constant_op.constant(t0)\n        c0 = collective_ops.all_gather(in0, 2, group_key, instance_key)\n      with ops.device('/CPU:1'):\n        in1 = constant_op.constant(t1)\n        c1 = collective_ops.all_gather(in1, 2, group_key, instance_key)\n      run_options = config_pb2.RunOptions()\n      if set_graph_key:\n        run_options.experimental.collective_graph_key = 1\n      results = sess.run([c0, c1], options=run_options)\n    self.assertAllClose(results[0], expected, rtol=1e-5, atol=1e-5)\n    self.assertAllClose(results[1], expected, rtol=1e-5, atol=1e-5)\n\n  def testCollectiveGather(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveGather([0, 1, 2, 3, 4, 5, 6, 7],\n                                 [10, 11, 12, 13, 14, 15, 16, 17],\n                                 [0, 1, 2, 3, 4, 5, 6, 7,\n                                  10, 11, 12, 13, 14, 15, 16, 17],\n                                 True)\n      self._testCollectiveGather([[0, 1, 2, 3], [4, 5, 6, 7]],\n                                 [[10, 11, 12, 13], [14, 15, 16, 17]],\n                                 [[0, 1, 2, 3], [4, 5, 6, 7],\n                                  [10, 11, 12, 13], [14, 15, 16, 17]],\n                                 True)\n      self._testCollectiveGather([[[0, 1], [2, 3]], [[4, 5], [6, 7]]],\n                                 [[[10, 11], [12, 13]], [[14, 15], [16, 17]]],\n                                 [[[0, 1], [2, 3]], [[4, 5], [6, 7]],\n                                  [[10, 11], [12, 13]], [[14, 15], [16, 17]]],\n                                 True)\n\n  def testCollectiveGatherShapeMismatch(self):\n    group_key = 1\n    instance_key = 1\n    t0 = [1, 2, 3, 4]\n    t1 = [5, 6, 7, 8]\n    t2 = [9, 10]\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      with self.session(\n          config=config_pb2.ConfigProto(device_count={'CPU': 2})) as sess:\n        with ops.device('/CPU:0'):\n          in0 = constant_op.constant(t0)\n          c0 = collective_ops.all_gather(in0, 2, group_key, instance_key)\n        with ops.device('/CPU:1'):\n          in1 = constant_op.constant(t1)\n          in2 = constant_op.constant(t2)\n          c1 = collective_ops.all_gather(in1, 2, group_key, instance_key)\n          c2 = collective_ops.all_gather(in2, 2, group_key, instance_key)\n        run_options = config_pb2.RunOptions()\n        run_options.experimental.collective_graph_key = 1\n        sess.run([c0, c1], options=run_options)\n        with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                    'Shape mismatch'):\n          sess.run([c0, c2], options=run_options)\n\n  def testCollectiveGatherShapeMismatchAcrossDevices(self):\n    group_key = 1\n    instance_key = 1\n    t0 = [1, 2, 3, 4]\n    t1 = [5, 6]\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      with self.session(\n          config=config_pb2.ConfigProto(device_count={'CPU': 2})) as sess:\n        with ops.device('/CPU:0'):\n          in0 = constant_op.constant(t0)\n          c0 = collective_ops.all_gather(in0, 2, group_key, instance_key)\n        with ops.device('/CPU:1'):\n          in1 = constant_op.constant(t1)\n          c1 = collective_ops.all_gather(in1, 2, group_key, instance_key)\n        run_options = config_pb2.RunOptions()\n        run_options.experimental.collective_graph_key = 1\n        with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                    'Shape mismatch'):\n          sess.run([c0, c1], options=run_options)\n\n  def testCollectiveGatherPolymorphicShape(self):\n    t0 = [0, 1, 2, 3, 4, 5, 6, 7]\n    t1 = [10, 11, 12, 13, 14, 15, 16, 17]\n    group_size = 2\n    group_key = 1\n    instance_key = 123\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      with self.session(\n          config=config_pb2.ConfigProto(\n              device_count={'CPU': group_size})) as sess:\n        with ops.device('/CPU:0'):\n          in0 = array_ops.placeholder(dtype=dtypes.int32, shape=[None])\n          c0 = collective_ops.all_gather(in0, group_size, group_key,\n                                         instance_key)\n        with ops.device('/CPU:1'):\n          in1 = array_ops.placeholder(dtype=dtypes.int32, shape=[None])\n          c1 = collective_ops.all_gather(in1, group_size, group_key,\n                                         instance_key)\n\n        results = sess.run([c0, c1], feed_dict={in0: t0, in1: t1})\n        results_ = sess.run([c0, c1], feed_dict={in0: t0[1:], in1: t1[1:]})\n\n    expected_output = [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17]\n    self.assertAllClose(results[0], expected_output, rtol=1e-5, atol=1e-5)\n    self.assertAllClose(results[1], expected_output, rtol=1e-5, atol=1e-5)\n\n    expected_output_ = [1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 14, 15, 16, 17]\n    self.assertAllClose(results_[0], expected_output_, rtol=1e-5, atol=1e-5)\n    self.assertAllClose(results_[1], expected_output_, rtol=1e-5, atol=1e-5)\n\n  @test_util.run_v2_only\n  @test_util.disable_tfrt(\n      'b/177270918: TFRT has dead lock when executing collective ops.')\n  def testCollectiveGroupSizeMismatch(self):\n    cpus = config.list_physical_devices('CPU')\n    self.assertEqual(len(cpus), 1)\n    config.set_logical_device_configuration(cpus[0], [\n        context.LogicalDeviceConfiguration(),\n        context.LogicalDeviceConfiguration()\n    ])\n    context.ensure_initialized()\n\n    @def_function.function\n    def run_all_reduce():\n      group_key = 10\n      instance_key = 20\n      t0 = [1, 2, 3, 4]\n      t1 = [5, 6, 7, 8]\n      with ops.device('/CPU:0'):\n        in0 = constant_op.constant(t0)\n        c0 = collective_ops.all_reduce(\n            in0, group_size=2, group_key=group_key, instance_key=instance_key,\n            merge_op='Add', final_op='Id')\n      with ops.device('/CPU:1'):\n        in1 = constant_op.constant(t1)\n        c1 = collective_ops.all_reduce(\n            in1, group_size=3, group_key=group_key, instance_key=instance_key,\n            merge_op='Add', final_op='Id')\n      return c0, c1\n\n    with self.assertRaisesRegex(errors.InternalError,\n                                'but that group has size'):\n      run_all_reduce()\n\n  @test_util.run_v2_only\n  def testCollectiveTensorsHaveNoDeviceSpecified(self):\n    cpus = config.list_physical_devices('CPU')\n    self.assertEqual(len(cpus), 1)\n    config.set_logical_device_configuration(cpus[0], [\n        context.LogicalDeviceConfiguration(),\n        context.LogicalDeviceConfiguration()\n    ])\n    context.ensure_initialized()\n\n    group_size = 2\n    group_key = 1\n    instance_key = 1\n\n    @def_function.function\n    def fn(all_args):\n      results = []\n      # The inputs have no devices set. This is expected to be a trace-time\n      # check only.\n      self.assertEqual(all_args[0].device, '')\n      self.assertEqual(all_args[1].device, '')\n\n      with ops.device('/CPU:0'):\n        results.append(\n            collective_ops.all_reduce(all_args[0], group_size, group_key,\n                                      instance_key, 'Add', 'Div'))\n      with ops.device('/CPU:1'):\n        results.append(\n            collective_ops.all_reduce(all_args[1], group_size, group_key,\n                                      instance_key, 'Add', 'Div'))\n\n      return results\n\n    with ops.device('/CPU:0'):\n      in0 = constant_op.constant(1)\n    with ops.device('/CPU:1'):\n      in1 = constant_op.constant(3)\n    result = fn([in0, in1])\n    self.assertAllClose(result, [2, 2])\n\n  def testConstantWithScopedAllocator(self):\n    group_size = 2\n    group_key = 1\n    instance_key1 = 1\n    instance_key2 = 2\n\n    graph_options = config_pb2.GraphOptions(\n        optimizer_options=config_pb2.OptimizerOptions(do_constant_folding=True))\n    cfg = config_pb2.ConfigProto(device_count={'CPU': group_size},\n                                 graph_options=graph_options)\n    rewrite_options = cfg.graph_options.rewrite_options\n    rewrite_options.scoped_allocator_optimization = (\n        rewriter_config_pb2.RewriterConfig.ON)\n    del rewrite_options.scoped_allocator_opts.enable_op[:]\n    rewrite_options.scoped_allocator_opts.enable_op.append('CollectiveReduce')\n\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      with self.session(config=cfg) as sess:\n        run_ops = []\n        for i in range(group_size):\n          with ops.device('CPU:%d' % i):\n            constant = constant_op.constant(i + 1.)\n            input_tensor1 = array_ops.identity(constant)\n            input_tensor2 = array_ops.identity(constant)\n            reduced_tensor1 = collective_ops.all_reduce(\n                input_tensor1, group_size, group_key, instance_key1, 'Add',\n                'Id')\n            reduced_tensor2 = collective_ops.all_reduce(\n                input_tensor2, group_size, group_key, instance_key2, 'Add',\n                'Id')\n            run_ops.append(array_ops.identity(reduced_tensor1))\n            run_ops.append(array_ops.identity(reduced_tensor2))\n        results = sess.run(run_ops)\n    self.assertEqual(results, [3., 3., 3., 3.])\n\n\nif __name__ == '__main__':\n  test.main()\n"], "fixing_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n#include <string>\n#include <utility>\n\n#include \"absl/strings/str_cat.h\"\n#include \"absl/strings/str_format.h\"\n#include \"tensorflow/core/framework/attr_value.pb.h\"\n#include \"tensorflow/core/framework/collective.h\"\n#include \"tensorflow/core/framework/device_attributes.pb.h\"\n#include \"tensorflow/core/framework/node_def.pb.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/op_requires.h\"\n#include \"tensorflow/core/framework/resource_handle.h\"\n#include \"tensorflow/core/framework/resource_mgr.h\"\n#include \"tensorflow/core/framework/tensor_util.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/framework/types.pb.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/platform/errors.h\"\n#include \"tensorflow/core/platform/refcount.h\"\n#include \"tensorflow/core/platform/status.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\n\nnamespace {\n\nstatic string CollectiveKey(OpKernelContext* ctx, int32_t group_key,\n                            int32_t instance_key) {\n  return strings::StrCat(group_key, \":\", instance_key, \":\",\n                         ctx->frame_iter().frame_id, \":\",\n                         ctx->frame_iter().iter_id);\n}\n\nstatic std::unique_ptr<OpKernel> BuildOpKernel(OpKernelConstruction* c,\n                                               const string& name,\n                                               NodeDef* sub_node) {\n  std::unique_ptr<OpKernel> k;\n  if (name.empty() || name == \"Id\") return k;\n  sub_node->set_name(name);\n  sub_node->set_op(name);\n  Status status;\n  k = CreateOpKernel(c->device_type(), c->device(),\n                     c->device()->GetAllocator(AllocatorAttributes()),\n                     *sub_node, c->graph_def_version(), &status);\n  if (!status.ok()) {\n    c->CtxFailureWithWarning(errors::Internal(\n        \"Failed to build OpKernel for \", name, \" : \", status.error_message()));\n  }\n  return k;\n}\n\nclass CollectiveOpV1Kernel : public AsyncOpKernel {\n public:\n  explicit CollectiveOpV1Kernel(OpKernelConstruction* c)\n      : AsyncOpKernel(c), name_(name()), col_params_(new CollectiveParams()) {}\n\n  ~CollectiveOpV1Kernel() override { col_params_->Unref(); }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    CollectiveExecutor* col_exec = c->collective_executor();\n    OP_REQUIRES_ASYNC(\n        c, col_exec,\n        errors::Internal(\n            \"Failed to get CollectiveExecutor from OpKernelContext for Op \",\n            name_),\n        done);\n    const CancellationToken token =\n        c->cancellation_manager()->get_cancellation_token();\n    const bool already_cancelled =\n        !c->cancellation_manager()->RegisterCallback(token, [col_exec]() {\n          // We must call StartAbort() within the callback. StartAbort() relies\n          // on resources that may be deallocated if all execution of a graph is\n          // finished.\n          col_exec->StartAbort(errors::Cancelled(\"op cancelled\"));\n        });\n    OP_REQUIRES_ASYNC(c, !already_cancelled,\n                      errors::Cancelled(\"op cancelled \", name_), done);\n\n    auto deregister_and_done = [c, token, done = std::move(done)]() {\n      // Once done() is called, StartAbort() won't have any effect, so we\n      // don't need to block on the deregistration. Also StartAbort() may call\n      // done() and DeregisterCallback may deadlock.\n      c->cancellation_manager()->TryDeregisterCallback(token);\n      done();\n    };\n    ComputeAsyncImpl(c, col_exec, std::move(deregister_and_done));\n  }\n\n  // A string encoding instance, frame and iter to be handed off to\n  // the implementation for use in generating RecvBuf keys.\n  string GetCollectiveKey(OpKernelContext* c) {\n    return CollectiveKey(c, col_params_->group.group_key,\n                         col_params_->instance.instance_key);\n  }\n\n  // Returns false if calling invocation of ComputeAsync should return\n  // immediately.\n  bool CanProceedWithCompute(OpKernelContext* c, CollectiveExecutor* col_exec,\n                             const DoneCallback& done) {\n    if (col_params_->group.group_size > col_params_->group.members.size()) {\n      // This is the first invocation: Finish initializing col_params_.\n      // Schedule the `CompleteParamsAsync` call on a work queue that can handle\n      // blocking work because it's not guaranteed that this call cannot block.\n      c->collective_executor()->RunClosure([this, c, col_exec, done]() {\n        VLOG(1) << \"CollectiveOpKernel CompleteParams for collective \"\n                << col_params_->name << \" device \" << c->device()->name()\n                << \" group \" << col_params_->group.group_key << \" instance \"\n                << col_params_->instance.instance_key;\n        col_exec->CompleteParamsAsync(\n            c->device()->attributes(), col_params_, c->cancellation_manager(),\n            [this, c, done](const Status& s) {\n              if (s.ok()) {\n                col_params_->instance.impl_details.dependencies = dependencies_;\n                ComputeAsync(c, done);\n              } else {\n                c->SetStatus(s);\n                done();\n              }\n            });\n      });\n      return false;\n    }\n    return true;\n  }\n\n protected:\n  virtual void ComputeAsyncImpl(OpKernelContext* c,\n                                CollectiveExecutor* col_exec,\n                                DoneCallback done) = 0;\n\n  string name_;\n  CollectiveParams* col_params_;\n  std::vector<int32> dependencies_;\n};\n\nclass CollectiveGatherOpKernel : public CollectiveOpV1Kernel {\n public:\n  explicit CollectiveGatherOpKernel(OpKernelConstruction* c)\n      : CollectiveOpV1Kernel(c) {\n    col_params_->instance.type = GATHER_COLLECTIVE;\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_->group.group_size));\n    OP_REQUIRES(\n        c, col_params_->group.group_size > 0,\n        errors::InvalidArgument(\"group_size must be positive integer but got \",\n                                col_params_->group.group_size));\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_key\", &col_params_->group.group_key));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"instance_key\", &col_params_->instance.instance_key));\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"communication_hint\",\n                      &col_params_->instance.impl_details.communication_hint));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"timeout_seconds\",\n                      &col_params_->instance.impl_details.timeout_seconds));\n    const NodeDef& real_node = c->def();\n    col_params_->name = strings::StrCat(real_node.name(), \": Gather\");\n    col_params_->group.device_type = c->device_type();\n  }\n\n protected:\n  void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,\n                        DoneCallback done) override {\n    auto output_shape = c->input(0).shape();\n    OP_REQUIRES_ASYNC(c, output_shape.dims() > 0,\n                      errors::InvalidArgument(\"input should have rank > 0, \",\n                                              \"recieved \", output_shape.dims()),\n                      done);\n    output_shape.set_dim(\n        0, output_shape.dim_size(0) * col_params_->group.group_size);\n    col_params_->instance.shape = output_shape;\n\n    // Allocate output on the first pass through this function.  This must be\n    // done immediately, while we're still in the executor thread.  Otherwise\n    // the memory is not guaranteed to be unused by any concurrently executing\n    // GPU kernel.\n    if (c->mutable_output(0) == nullptr) {\n      // Allocate the output tensor.\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK_ASYNC(\n          c, c->allocate_output(0, col_params_->instance.shape, &output), done);\n    }\n    if (!CanProceedWithCompute(c, col_exec, done)) return;\n\n    auto actual_done = [c, col_params = col_params_, done](const Status& s) {\n      VLOG(1) << \"CollectiveGatherOpKernel ExecuteAsync done for collective \"\n              << c->op_kernel().name() << \" device \" << c->device()->name()\n              << \" group \" << col_params->group.group_key << \" instance \"\n              << col_params->instance.instance_key << \" status \" << s;\n      col_params->Unref();\n      OP_REQUIRES_OK_ASYNC(c, s, done);\n      done();\n    };\n    VLOG(1) << \"CollectiveGatherOpKernel ExecuteAsync start for collective \"\n            << col_params_->name << \" device \" << c->device()->name()\n            << \" group \" << col_params_->group.group_key << \" instance \"\n            << col_params_->instance.instance_key;\n    col_params_->Ref();\n    col_exec->ExecuteAsync(c, col_params_, GetCollectiveKey(c), actual_done);\n  }\n\n private:\n  TF_DISALLOW_COPY_AND_ASSIGN(CollectiveGatherOpKernel);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveGather\").Device(DEVICE_CPU),\n                        CollectiveGatherOpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveGather\").Device(DEVICE_GPU),\n                        CollectiveGatherOpKernel);\n\nclass CollectiveReduceOpKernel : public CollectiveOpV1Kernel {\n public:\n  explicit CollectiveReduceOpKernel(OpKernelConstruction* c)\n      : CollectiveOpV1Kernel(c) {\n    col_params_->instance.type = REDUCTION_COLLECTIVE;\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_->group.group_size));\n    OP_REQUIRES(\n        c, col_params_->group.group_size > 0,\n        errors::InvalidArgument(\"group_size must be positive integer but got \",\n                                col_params_->group.group_size));\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_key\", &col_params_->group.group_key));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"instance_key\", &col_params_->instance.instance_key));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"subdiv_offsets\",\n                      &col_params_->instance.impl_details.subdiv_offsets));\n    string merge_op_name;\n    OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));\n    if (merge_op_name == \"Max\") {\n      merge_op_name = \"Maximum\";\n    } else if (merge_op_name == \"Min\") {\n      merge_op_name = \"Minimum\";\n    }\n    string final_op_name;\n    OP_REQUIRES_OK(c, c->GetAttr(\"final_op\", &final_op_name));\n    OP_REQUIRES(c, final_op_name == \"Id\" || final_op_name == \"Div\",\n                errors::InvalidArgument(\n                    \"final_op must be one of {\\\"Id\\\", \\\"Div\\\"} but got \",\n                    final_op_name));\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));\n    OP_REQUIRES_OK(c, c->GetAttr(\"wait_for\", &dependencies_));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"communication_hint\",\n                      &col_params_->instance.impl_details.communication_hint));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"timeout_seconds\",\n                      &col_params_->instance.impl_details.timeout_seconds));\n    VLOG(2) << \"CollectiveReduce instance \"\n            << col_params_->instance.instance_key << \" merge_op \"\n            << merge_op_name << \" final_op \" << final_op_name\n            << \" communication_hint \"\n            << col_params_->instance.impl_details.communication_hint\n            << \" timeout \"\n            << col_params_->instance.impl_details.timeout_seconds;\n\n    const NodeDef& real_node = c->def();\n    col_params_->name = strings::StrCat(real_node.name(), \": Reduce(\",\n                                        merge_op_name, \",\", final_op_name, \")\");\n    col_params_->group.device_type = c->device_type();\n\n    // Find the OpKernels by name, type and device type.\n    NodeDef sub_node;\n    // The merge_op takes two inputs\n    sub_node.add_input(real_node.input(0));\n    sub_node.add_input(real_node.input(0));\n    sub_node.set_device(real_node.device());\n    SetAttrValue(col_params_->instance.data_type,\n                 &(*sub_node.mutable_attr())[\"T\"]);\n    merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);\n    final_op_ = BuildOpKernel(c, final_op_name, &sub_node);\n    col_params_->merge_op = merge_op_.get();\n    col_params_->final_op = final_op_.get();\n  }\n\n protected:\n  void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,\n                        DoneCallback done) override {\n    // Allocate output on the first pass through this function.  This must be\n    // done immediately, while we're still in the executor thread.  Otherwise\n    // the memory is not guaranteed to be unused by any concurrently executing\n    // GPU kernel.\n    if (c->mutable_output(0) == nullptr) {\n      // Allocate the output tensor, trying to reuse the input.\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK_ASYNC(c,\n                           c->forward_input_or_allocate_output(\n                               {0}, 0, c->input(0).shape(), &output),\n                           done);\n      col_params_->instance.shape = c->input(0).shape();\n    }\n    if (!CanProceedWithCompute(c, col_exec, done)) return;\n\n    auto actual_done = [c, col_params = col_params_, done](const Status& s) {\n      VLOG(1) << \"CollectiveReduceOpKernel ExecuteAsync done for collective \"\n              << c->op_kernel().name() << \" device \" << c->device()->name()\n              << \" group \" << col_params->group.group_key << \" instance \"\n              << col_params->instance.instance_key << \" status \" << s;\n      col_params->Unref();\n      OP_REQUIRES_OK_ASYNC(c, s, done);\n      done();\n    };\n    VLOG(1) << \"CollectiveReduceOpKernel ExecuteAsync start for collective \"\n            << col_params_->name << \" device \" << c->device()->name()\n            << \" group \" << col_params_->group.group_key << \" instance \"\n            << col_params_->instance.instance_key;\n    col_params_->Ref();\n    col_exec->ExecuteAsync(c, col_params_, GetCollectiveKey(c), actual_done);\n  }\n\n private:\n  std::unique_ptr<OpKernel> merge_op_;\n  std::unique_ptr<OpKernel> final_op_;\n  TF_DISALLOW_COPY_AND_ASSIGN(CollectiveReduceOpKernel);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduce\").Device(DEVICE_CPU),\n                        CollectiveReduceOpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduce\").Device(DEVICE_GPU),\n                        CollectiveReduceOpKernel);\n\nclass CollectiveBcastSendOpKernel : public CollectiveOpV1Kernel {\n public:\n  explicit CollectiveBcastSendOpKernel(OpKernelConstruction* c)\n      : CollectiveOpV1Kernel(c) {\n    col_params_->instance.type = BROADCAST_COLLECTIVE;\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_->group.group_size));\n    OP_REQUIRES(\n        c, col_params_->group.group_size > 0,\n        errors::InvalidArgument(\"group_size must be positive integer but got \",\n                                col_params_->group.group_size));\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_key\", &col_params_->group.group_key));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"instance_key\", &col_params_->instance.instance_key));\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));\n    OP_REQUIRES_OK(c, c->GetAttr(\"shape\", &col_params_->instance.shape));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"communication_hint\",\n                      &col_params_->instance.impl_details.communication_hint));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"timeout_seconds\",\n                      &col_params_->instance.impl_details.timeout_seconds));\n    col_params_->is_source = true;\n    col_params_->instance.impl_details.subdiv_offsets = {0};\n\n    col_params_->name =\n        strings::StrCat(name(), \": Broadcast(\", col_params_->is_source, \")\");\n    col_params_->group.device_type = c->device_type();\n  }\n\n protected:\n  void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,\n                        DoneCallback done) override {\n    // Allocate output on the first pass through this function.  This must be\n    // done immediately, while we're still in the executor thread.  Otherwise\n    // the memory is not guaranteed to be unused by any concurrently executing\n    // GPU kernel.\n    if (c->mutable_output(0) == nullptr) {\n      // Allocate the output tensor, trying to reuse the input.\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK_ASYNC(c,\n                           c->forward_input_or_allocate_output(\n                               {0}, 0, col_params_->instance.shape, &output),\n                           done);\n    }\n    if (!CanProceedWithCompute(c, col_exec, done)) return;\n    OP_REQUIRES_ASYNC(\n        c, col_params_->instance.shape.IsSameSize(c->input(0).shape()),\n        errors::Internal(\"Declared shape of op \", col_params_->name,\n                         \" does not match shape of input\"),\n        done);\n\n    auto actual_done = [c, col_params = col_params_, done](const Status& s) {\n      VLOG(1) << \"CollectiveBcastSendOpKernel ExecuteAsync done for collective \"\n              << c->op_kernel().name() << \" device \" << c->device()->name()\n              << \" group \" << col_params->group.group_key << \" instance \"\n              << col_params->instance.instance_key << \" status \" << s;\n      col_params->Unref();\n      OP_REQUIRES_OK_ASYNC(c, s, done);\n      done();\n    };\n    VLOG(1) << \"CollectiveBcastSendOpKernel ExecuteAsync start for collective \"\n            << col_params_->name << \" device \" << c->device()->name()\n            << \" group \" << col_params_->group.group_key << \" instance \"\n            << col_params_->instance.instance_key;\n    col_params_->Ref();\n    col_exec->ExecuteAsync(c, col_params_, GetCollectiveKey(c), actual_done);\n  }\n\n private:\n  TF_DISALLOW_COPY_AND_ASSIGN(CollectiveBcastSendOpKernel);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSend\").Device(DEVICE_CPU),\n                        CollectiveBcastSendOpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSend\").Device(DEVICE_DEFAULT),\n                        CollectiveBcastSendOpKernel);\n\nclass CollectiveBcastRecvOpKernel : public CollectiveOpV1Kernel {\n public:\n  explicit CollectiveBcastRecvOpKernel(OpKernelConstruction* c)\n      : CollectiveOpV1Kernel(c) {\n    col_params_->instance.type = BROADCAST_COLLECTIVE;\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_size\", &col_params_->group.group_size));\n    OP_REQUIRES(\n        c, col_params_->group.group_size > 0,\n        errors::InvalidArgument(\"group_size must be positive integer but got \",\n                                col_params_->group.group_size));\n    OP_REQUIRES_OK(c, c->GetAttr(\"group_key\", &col_params_->group.group_key));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"instance_key\", &col_params_->instance.instance_key));\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &col_params_->instance.data_type));\n    OP_REQUIRES_OK(c, c->GetAttr(\"shape\", &col_params_->instance.shape));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"communication_hint\",\n                      &col_params_->instance.impl_details.communication_hint));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"timeout_seconds\",\n                      &col_params_->instance.impl_details.timeout_seconds));\n    col_params_->is_source = false;\n    col_params_->instance.impl_details.subdiv_offsets = {0};\n\n    col_params_->name =\n        strings::StrCat(name(), \": Broadcast(\", col_params_->is_source, \")\");\n    col_params_->group.device_type = c->device_type();\n  }\n\n protected:\n  void ComputeAsyncImpl(OpKernelContext* c, CollectiveExecutor* col_exec,\n                        DoneCallback done) override {\n    // Allocate output on the first pass through this function.  This must be\n    // done immediately, while we're still in the executor thread.  Otherwise\n    // the memory is not guaranteed to be unused by any concurrently executing\n    // GPU kernel.\n    if (c->mutable_output(0) == nullptr) {\n      // No input, so must allocate output.\n      Tensor* output = nullptr;\n      OP_REQUIRES_OK_ASYNC(\n          c, c->allocate_output(0, col_params_->instance.shape, &output), done);\n    }\n    if (!CanProceedWithCompute(c, col_exec, done)) return;\n\n    auto actual_done = [c, col_params = col_params_, done](const Status& s) {\n      VLOG(1) << \"CollectiveBcastRecvOpKernel ExecuteAsync done for collective \"\n              << c->op_kernel().name() << \" device \" << c->device()->name()\n              << \" group \" << col_params->group.group_key << \" instance_key \"\n              << col_params->instance.instance_key << \" status  \" << s;\n      col_params->Unref();\n      OP_REQUIRES_OK_ASYNC(c, s, done);\n      done();\n    };\n    VLOG(1) << \"CollectiveBcastRecvOpKernel ExecuteAsync start for collective \"\n            << col_params_->name << \" device \" << c->device()->name()\n            << \" group \" << col_params_->group.group_key << \" instance \"\n            << col_params_->instance.instance_key;\n    col_params_->Ref();\n    col_exec->ExecuteAsync(c, col_params_, GetCollectiveKey(c), actual_done);\n  }\n\n private:\n  TF_DISALLOW_COPY_AND_ASSIGN(CollectiveBcastRecvOpKernel);\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecv\").Device(DEVICE_CPU),\n                        CollectiveBcastRecvOpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecv\").Device(DEVICE_DEFAULT),\n                        CollectiveBcastRecvOpKernel);\n\nclass CollectiveAssignGroupV2OpKernel : public OpKernel {\n public:\n  explicit CollectiveAssignGroupV2OpKernel(OpKernelConstruction* c)\n      : OpKernel(c) {}\n\n  void Compute(OpKernelContext* context) override {\n    const Tensor& group_assignment = context->input(0);\n    const Tensor& device_index = context->input(1);\n    const Tensor& base_key = context->input(2);\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsScalar(device_index.shape()),\n        errors::InvalidArgument(\n            \"device_index must be a scalar, but received tensor of shape: \",\n            device_index.shape().DebugString()));\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsMatrix(group_assignment.shape()),\n        errors::InvalidArgument(\"group_assignment must be a 2-d Tensor, but \"\n                                \"received tensor of shape: \",\n                                group_assignment.shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(base_key.shape()),\n                errors::InvalidArgument(\n                    \"base_key must be a scalar, but received tensor of shape: \",\n                    base_key.shape().DebugString()));\n\n    Tensor* group_key = nullptr;\n    Tensor* group_size = nullptr;\n    AllocatorAttributes attr;\n    attr.set_on_host(true);\n    OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({}),\n                                                     &group_size, attr));\n\n    OP_REQUIRES_OK(context, context->allocate_output(1, TensorShape({}),\n                                                     &group_key, attr));\n\n    OP_REQUIRES_OK(\n        context,\n        ComputeGroupKey(group_assignment, device_index.scalar<int32_t>()(),\n                        base_key.scalar<int32_t>()(), group_size, group_key));\n  }\n\n private:\n  static Status ComputeGroupKey(const Tensor& group_assignment,\n                                const int32_t device_index,\n                                const int32_t base_key, Tensor* group_size,\n                                Tensor* group_key) {\n    group_size->flat<int32_t>()(0) = group_assignment.dim_size(1);\n\n    for (int group_id = 0; group_id < group_assignment.dim_size(0);\n         group_id++) {\n      int32_t key = static_cast<int32_t>(static_cast<uint32_t>(base_key) +\n                                         static_cast<uint32_t>(group_id));\n      if (key == 0) {\n        return errors::InvalidArgument(\n            \"Using the reserved group_key = 0 is not allowed: group_id = \",\n            group_id, \", base_key = \", base_key);\n      }\n      for (int color = 0; color < group_assignment.dim_size(1); color++) {\n        const auto index = group_assignment.matrix<int32>()(group_id, color);\n        if (index < 0 || index >= group_assignment.shape().num_elements()) {\n          return errors::InvalidArgument(\"Not all items in group_assignment \",\n                                         group_assignment.DebugString(),\n                                         \" is within [0, number of devices)\");\n        }\n        if (index == device_index) {\n          group_key->flat<int32_t>()(0) = key;\n          VLOG(2) << \" group_assignment = \" << group_assignment.DebugString()\n                  << \" device_index = \" << index\n                  << \" group_key = \" << group_key->DebugString()\n                  << \" group_size = \" << group_size->DebugString();\n          return OkStatus();\n        }\n      }\n    }\n    return errors::InvalidArgument(\"device_index \", device_index,\n                                   \" is not found in group_assignment \",\n                                   group_assignment.DebugString());\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveAssignGroupV2\").Device(DEVICE_CPU),\n                        CollectiveAssignGroupV2OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveAssignGroupV2\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"device_index\")\n                            .HostMemory(\"group_assignment\")\n                            .HostMemory(\"base_key\")\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\"),\n                        CollectiveAssignGroupV2OpKernel);\n\nclass CollectiveOpV2Kernel : public AsyncOpKernel {\n public:\n  explicit CollectiveOpV2Kernel(OpKernelConstruction* c)\n      : AsyncOpKernel(c), name_(name()), device_type_(DEVICE_DEFAULT) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));\n    OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));\n    OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));\n    device_type_ = c->device_type();\n  }\n\n protected:\n  // Fills common parts of CollectiveParams according to the Op, *excluding\n  // output_shape*. Kernels should further work on the CollectiveParams if they\n  // need to set additional fields.\n  Status FillCollectiveParams(CollectiveParams* col_params,\n                              CollectiveType collective_type,\n                              const Tensor& group_size, const Tensor& group_key,\n                              const Tensor& instance_key) {\n    if (group_size.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input group_size, got \",\n          group_size.shape().DebugString());\n    }\n    if (group_key.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input group_key, got \",\n          group_key.shape().DebugString());\n    }\n    if (instance_key.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input instance_key, got \",\n          instance_key.shape().DebugString());\n    }\n    col_params->name = name_;\n    col_params->group.device_type = device_type_;\n    col_params->group.group_size = group_size.unaligned_flat<int32>()(0);\n    if (col_params->group.group_size <= 0) {\n      return errors::InvalidArgument(\n          \"group_size must be positive integer but got \",\n          col_params->group.group_size);\n    }\n    col_params->group.group_key = group_key.unaligned_flat<int32>()(0);\n    col_params->instance.type = collective_type;\n    col_params->instance.instance_key = instance_key.unaligned_flat<int32>()(0);\n    col_params->instance.data_type = data_type_;\n    col_params->instance.impl_details.communication_hint = communication_hint_;\n    col_params->instance.impl_details.timeout_seconds = timeout_seconds_;\n    return OkStatus();\n  }\n\n  // Runs a collective. The output tensor must be allocated before calling this\n  // method. col_params must live until done is called.\n  void Run(OpKernelContext* c, CollectiveParams* col_params,\n           DoneCallback done) {\n    CollectiveExecutor* col_exec = c->collective_executor();\n    OP_REQUIRES_ASYNC(\n        c, col_exec,\n        errors::Internal(\n            \"Failed to get CollectiveExecutor from OpKernelContext for Op \",\n            name_),\n        done);\n    // Resolve the collective params.\n    // Schedule the `CompleteParamsAsync` call on a work queue that can handle\n    // blocking work because it's not guaranteed that this call cannot block.\n    c->collective_executor()->RunClosure([c, done = std::move(done), col_params,\n                                          col_exec]() {\n      VLOG(1) << \"Collective CompleteParams for \" << col_params->name\n              << \" device \" << c->device()->name() << \" group \"\n              << col_params->group.group_key << \" instance \"\n              << col_params->instance.instance_key;\n      col_exec->CompleteParamsAsync(\n          c->device()->attributes(), col_params, c->cancellation_manager(),\n          [c, done = std::move(done), col_params, col_exec](const Status& s) {\n            if (s.ok()) {\n              auto actual_done = [c, col_params,\n                                  done = std::move(done)](const Status& s) {\n                VLOG(1) << \"Collective ExecuteAsync done for \"\n                        << col_params->name << \" device \" << c->device()->name()\n                        << \" group \" << col_params->group.group_key\n                        << \" instance \" << col_params->instance.instance_key\n                        << \" status \" << s;\n                if (!s.ok()) {\n                  c->SetStatus(s);\n                }\n                done();\n              };\n              VLOG(1) << \"Collective ExecuteAsync start for \"\n                      << col_params->name << \" device \" << c->device()->name()\n                      << \" group \" << col_params->group.group_key\n                      << \" instance \" << col_params->instance.instance_key;\n              col_exec->ExecuteAsync(\n                  c, col_params,\n                  CollectiveKey(c, col_params->group.group_key,\n                                col_params->instance.instance_key),\n                  actual_done);\n            } else {\n              c->SetStatus(s);\n              done();\n            }\n          });\n    });\n  }\n\n protected:\n  string name_;\n  DataType data_type_ = DT_INVALID;\n  string communication_hint_;\n  float timeout_seconds_ = 0;\n  DeviceType device_type_;\n};\n\nclass CollectiveReduceV2OpKernel : public CollectiveOpV2Kernel {\n public:\n  explicit CollectiveReduceV2OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV2Kernel(c) {\n    string merge_op_name;\n    OP_REQUIRES_OK(c, c->GetAttr(\"merge_op\", &merge_op_name));\n    if (merge_op_name == \"Max\") {\n      merge_op_name = \"Maximum\";\n    } else if (merge_op_name == \"Min\") {\n      merge_op_name = \"Minimum\";\n    }\n    string final_op_name;\n    OP_REQUIRES_OK(c, c->GetAttr(\"final_op\", &final_op_name));\n    OP_REQUIRES_OK(\n        c, c->GetAttr(\"max_subdivs_per_device\", &max_subdivs_per_device_));\n    // Prepare OpKernels for reduction and final operations.\n    // The merge_op takes two inputs\n    NodeDef sub_node;\n    sub_node.add_input(c->def().input(0));\n    sub_node.add_input(c->def().input(0));\n    sub_node.set_device(c->def().device());\n    SetAttrValue(data_type_, &(*sub_node.mutable_attr())[\"T\"]);\n    merge_op_ = BuildOpKernel(c, merge_op_name, &sub_node);\n    final_op_ = BuildOpKernel(c, final_op_name, &sub_node);\n    name_ = strings::StrCat(c->def().name(), \": ReduceV2(\", merge_op_name, \",\",\n                            final_op_name, \")\");\n    VLOG(2) << \"CollectiveReduceV2 \" << this << \" name \" << name_\n            << \" communication_hint \" << communication_hint_;\n  }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    OP_REQUIRES_OK_ASYNC(c,\n                         FillCollectiveParams(col_params, REDUCTION_COLLECTIVE,\n                                              /*group_size*/ c->input(1),\n                                              /*group_key*/ c->input(2),\n                                              /*instance_key*/ c->input(3)),\n                         done_with_cleanup);\n    col_params->instance.shape = c->input(0).shape();\n    col_params->merge_op = merge_op_.get();\n    col_params->final_op = final_op_.get();\n    VLOG(1) << \"CollectiveReduceV2 group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n\n private:\n  int max_subdivs_per_device_;\n  std::unique_ptr<OpKernel> merge_op_;\n  std::unique_ptr<OpKernel> final_op_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV2\").Device(DEVICE_CPU),\n                        CollectiveReduceV2OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV2\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\")\n                            .HostMemory(\"instance_key\"),\n                        CollectiveReduceV2OpKernel);\n\nclass CollectiveGatherV2OpKernel : public CollectiveOpV2Kernel {\n public:\n  explicit CollectiveGatherV2OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV2Kernel(c) {\n    name_ = strings::StrCat(c->def().name(), \": GatherV2\");\n    VLOG(2) << \"CollectiveGatherV2 \" << this << \" name \" << name_\n            << \" communication_hint \" << communication_hint_;\n  }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    OP_REQUIRES_OK_ASYNC(c,\n                         FillCollectiveParams(col_params, GATHER_COLLECTIVE,\n                                              /*group_size*/ c->input(1),\n                                              /*group_key*/ c->input(2),\n                                              /*instance_key*/\n                                              c->input(3)),\n                         done_with_cleanup);\n    auto output_shape = c->input(0).shape();\n    output_shape.set_dim(\n        0, output_shape.dim_size(0) * col_params->group.group_size);\n    col_params->instance.shape = output_shape;\n    VLOG(1) << \"CollectiveGatherV2 group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(\n        c, c->allocate_output(0, col_params->instance.shape, &output),\n        done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveGatherV2\").Device(DEVICE_CPU),\n                        CollectiveGatherV2OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveGatherV2\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\")\n                            .HostMemory(\"instance_key\"),\n                        CollectiveGatherV2OpKernel);\n\nclass CollectiveBcastSendV2OpKernel : public CollectiveOpV2Kernel {\n public:\n  explicit CollectiveBcastSendV2OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV2Kernel(c) {\n    const bool is_source = true;\n    name_ = strings::StrCat(name(), \": Broadcast(\", is_source, \")\");\n  }\n\n protected:\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    OP_REQUIRES_OK_ASYNC(c,\n                         FillCollectiveParams(col_params, BROADCAST_COLLECTIVE,\n                                              /*group_size*/ c->input(1),\n                                              /*group_key*/ c->input(2),\n                                              /*instance_key*/ c->input(3)),\n                         done_with_cleanup);\n    col_params->is_source = true;\n    col_params->instance.shape = c->input(0).shape();\n    // Add a default value for subdiv offsets, which is the same as the default\n    // value in the V1 op's attribute.\n    col_params->instance.impl_details.subdiv_offsets.push_back(0);\n    VLOG(1) << \"CollectiveBcastSendV2 group_size \"\n            << col_params->group.group_size << \" group_key \"\n            << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSendV2\").Device(DEVICE_CPU),\n                        CollectiveBcastSendV2OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastSendV2\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\")\n                            .HostMemory(\"instance_key\"),\n                        CollectiveBcastSendV2OpKernel);\n\nclass CollectiveBcastRecvV2OpKernel : public CollectiveOpV2Kernel {\n public:\n  explicit CollectiveBcastRecvV2OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV2Kernel(c) {\n    const bool is_source = false;\n    name_ = strings::StrCat(name(), \": Broadcast(\", is_source, \")\");\n  }\n\n protected:\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    OP_REQUIRES_OK_ASYNC(c,\n                         FillCollectiveParams(col_params, BROADCAST_COLLECTIVE,\n                                              /*group_size*/ c->input(0),\n                                              /*group_key*/ c->input(1),\n                                              /*instance_key*/ c->input(2)),\n                         done_with_cleanup);\n    col_params->is_source = false;\n    TensorShape output_shape;\n    OP_REQUIRES_OK_ASYNC(c, tensor::MakeShape(c->input(3), &output_shape),\n                         done_with_cleanup);\n    col_params->instance.shape = output_shape;\n    // Add a default value for subdiv offsets, which is the same as the default\n    // value in the V1 op's attribute.\n    col_params->instance.impl_details.subdiv_offsets.push_back(0);\n    VLOG(1) << \"CollectiveBcastRecvV2 group_size \"\n            << col_params->group.group_size << \" group_key \"\n            << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(\n        c, c->allocate_output(0, col_params->instance.shape, &output),\n        done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecvV2\").Device(DEVICE_CPU),\n                        CollectiveBcastRecvV2OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveBcastRecvV2\")\n                            .Device(DEVICE_DEFAULT)\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\")\n                            .HostMemory(\"instance_key\")\n                            .HostMemory(\"shape\"),\n                        CollectiveBcastRecvV2OpKernel);\n\n/*\n * Resource for holding group for CollectiveOps.\n * This resource is returned from CollectiveInitializeCommunicatorOpKernel\n * It generates next instance key for the group for each collective operation.\n */\nclass CollectiveGroupResource : public ResourceBase {\n public:\n  CollectiveGroupResource(int32 group_key, int32 rank, int32 group_size,\n                          string communication_hint, float timeout_seconds)\n      : group_key_(group_key),\n        rank_(rank),\n        group_size_(group_size),\n        communication_hint_(communication_hint),\n        timeout_seconds_(timeout_seconds) {}\n\n  std::string DebugString() const override {\n    return absl::StrFormat(\n        \"Collective Group with group_key = %d, group_size = %d, rank = %d\",\n        group_key_, group_size_, rank_);\n  }\n\n  int get_next_instance_key() {\n    return instance_key_.fetch_add(1, std::memory_order_relaxed);\n  }\n\n  int32 group_key() const { return group_key_; }\n\n  int32 rank() const { return rank_; }\n\n  int32 group_size() const { return group_size_; }\n\n  string communication_hint() const { return communication_hint_; }\n\n  float timeout_seconds() const { return timeout_seconds_; }\n\n private:\n  int32 group_key_, rank_, group_size_;\n  string communication_hint_;\n  std::atomic<int> instance_key_{0};\n  float timeout_seconds_ = 0;\n};\n\nclass CollectiveInitializeCommunicatorOpKernel : public AsyncOpKernel {\n public:\n  explicit CollectiveInitializeCommunicatorOpKernel(OpKernelConstruction* c)\n      : AsyncOpKernel(c), device_type_(DEVICE_DEFAULT) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"communication_hint\", &communication_hint_));\n    OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));\n    device_type_ = c->device_type();\n  }\n\n  Status CheckInputs(Tensor group_size_t, Tensor group_key_t) {\n    if (group_size_t.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input group_size. \"\n          \"It shoulbe a scalar, got tensor with shape \",\n          group_size_t.shape().DebugString());\n    }\n    if (group_key_t.dims() > 0) {\n      return errors::InvalidArgument(\n          \"Unexpected dimensions on input group_key, got \",\n          group_key_t.shape().DebugString());\n    }\n\n    auto group_size = group_size_t.unaligned_flat<int32>()(0);\n    if (group_size <= 0) {\n      return errors::InvalidArgument(\n          \"group_size must be positive integer but got \", group_size);\n    }\n    return OkStatus();\n  }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto group_key_t = c->input(0);\n    auto rank_t = c->input(1);\n    auto group_size_t = c->input(2);\n\n    OP_REQUIRES_OK_ASYNC(c, CheckInputs(group_size_t, group_key_t), done);\n\n    auto group_size = group_size_t.unaligned_flat<int32>()(0);\n    auto group_key = group_key_t.unaligned_flat<int32>()(0);\n    auto rank = rank_t.unaligned_flat<int32>()(0);\n\n    ResourceHandle resource_handle =\n        MakeResourceHandle<CollectiveGroupResource>(\n            c, \"collective_op_group\",\n            absl::StrFormat(\"%d:r%04d\", group_key, rank));\n\n    Tensor* output_handle = nullptr;\n    OP_REQUIRES_OK_ASYNC(\n        c, c->allocate_output(0, TensorShape({}), &output_handle), done);\n    output_handle->scalar<ResourceHandle>()() = resource_handle;\n\n    CollectiveGroupResource* resource = new CollectiveGroupResource(\n        group_key, rank, group_size, this->communication_hint_,\n        this->timeout_seconds_);\n    OP_REQUIRES_OK_ASYNC(\n        c,\n        CreateResource<CollectiveGroupResource>(c, resource_handle, resource),\n        done);\n    auto group_params = new CollGroupParams();\n    group_params->device_type = device_type_;\n    group_params->group_size = resource->group_size();\n    group_params->group_key = resource->group_key();\n    group_params->user_specified_rank = resource->rank();\n\n    auto* col_exec = c->collective_executor();\n\n    c->collective_executor()->RunClosure([c, done = std::move(done),\n                                          group_params, col_exec]() {\n      VLOG(1) << \"Collective Group initialization for \"\n              << \" device \" << c->device()->name() << \" group \"\n              << group_params->group_key;\n      col_exec->CompleteGroupAsync(\n          c->device()->attributes(), group_params, c->cancellation_manager(),\n          [c, done = std::move(done), group_params](const Status& s) {\n            if (s.ok()) {\n              VLOG(1) << \"Collective Group initialization done for device \"\n                      << c->device()->name() << \" group \"\n                      << group_params->group_key << \" status \" << s;\n            } else {\n              c->SetStatus(s);\n            }\n            delete group_params;\n            done();\n          });\n    });\n  }\n\n private:\n  string communication_hint_;\n  DeviceType device_type_;\n  float timeout_seconds_ = 0;\n};\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"CollectiveInitializeCommunicator\").Device(DEVICE_CPU),\n    CollectiveInitializeCommunicatorOpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveInitializeCommunicator\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"group_size\")\n                            .HostMemory(\"group_key\")\n                            .HostMemory(\"rank\"),\n                        CollectiveInitializeCommunicatorOpKernel);\n\nclass CollectiveOpV3Kernel : public AsyncOpKernel {\n public:\n  explicit CollectiveOpV3Kernel(OpKernelConstruction* c)\n      : AsyncOpKernel(c), name_(name()), device_type_(DEVICE_DEFAULT) {\n    OP_REQUIRES_OK(c, c->GetAttr(\"T\", &data_type_));\n    if (c->HasAttr(\"timeout_seconds\")) {\n      OP_REQUIRES_OK(c, c->GetAttr(\"timeout_seconds\", &timeout_seconds_));\n    } else {\n      timeout_seconds_ = -1;\n    }\n    device_type_ = c->device_type();\n  }\n\n protected:\n  // Fills common parts of CollectiveParams according to the Op, *excluding\n  // output_shape*. Kernels should further work on the CollectiveParams if they\n  // need to set additional fields.\n  Status FillCollectiveParams(CollectiveParams* col_params,\n                              const Tensor& group_assignment,\n                              CollectiveType collective_type,\n                              CollectiveGroupResource* resource) {\n    int64 group_id;\n    int64 group_size;\n    if (group_assignment.NumElements() == 0) {\n      // No group assignments, perform collective as a single group.\n      group_id = 0;\n      group_size = resource->group_size();\n    } else {\n      return errors::Unimplemented(\"Group assignments are not supported yet.\");\n    }\n\n    // Construct instance key with format:\n    // <11 bits for group><21 bits for atomic incremented instance key>\n    int32 instance_key = group_id << 21 | resource->get_next_instance_key();\n    col_params->name = name_;\n    col_params->group.device_type = device_type_;\n    col_params->group.group_size = group_size;\n    col_params->group.group_key = resource->group_key();\n    col_params->group.user_specified_rank = resource->rank();\n    col_params->instance.type = collective_type;\n    col_params->instance.instance_key = instance_key;\n    col_params->instance.data_type = data_type_;\n    col_params->instance.impl_details.communication_hint =\n        resource->communication_hint();\n    col_params->instance.impl_details.timeout_seconds =\n        timeout_seconds_ > 0 ? resource->timeout_seconds() : timeout_seconds_;\n    col_params->run_group_initialization = false;\n    return OkStatus();\n  }\n\n  // Runs a collective. The output tensor must be allocated before calling this\n  // method. col_params must live until done is called.\n  void Run(OpKernelContext* c, CollectiveParams* col_params,\n           DoneCallback done) {\n    CollectiveExecutor* col_exec = c->collective_executor();\n    OP_REQUIRES_ASYNC(\n        c, col_exec,\n        errors::Internal(\n            \"Failed to get CollectiveExecutor from OpKernelContext for Op \",\n            name_),\n        done);\n    // Resolve the collective params.\n    // Schedule the `CompleteParamsAsync` call on a work queue that can handle\n    // blocking work because it's not guaranteed that this call cannot block.\n    col_exec->RunClosure([c, done = std::move(done), col_params, col_exec]() {\n      VLOG(1) << \"Collective CompleteParams for \" << col_params->name\n              << \" device \" << c->device()->name() << \" group \"\n              << col_params->group.group_key << \" instance \"\n              << col_params->instance.instance_key;\n      col_exec->CompleteParamsAsync(\n          c->device()->attributes(), col_params, c->cancellation_manager(),\n          [c, done = std::move(done), col_params, col_exec](const Status& s) {\n            if (s.ok()) {\n              auto actual_done = [c, col_params,\n                                  done = std::move(done)](const Status& s) {\n                VLOG(1) << \"Collective ExecuteAsync done for \"\n                        << col_params->name << \" device \" << c->device()->name()\n                        << \" group \" << col_params->group.group_key\n                        << \" instance \" << col_params->instance.instance_key\n                        << \" status \" << s;\n                if (!s.ok()) {\n                  c->SetStatus(s);\n                }\n                done();\n              };\n              VLOG(1) << \"Collective ExecuteAsync start for \"\n                      << col_params->name << \" device \" << c->device()->name()\n                      << \" group \" << col_params->group.group_key\n                      << \" instance \" << col_params->instance.instance_key;\n              col_exec->ExecuteAsync(\n                  c, col_params,\n                  CollectiveKey(c, col_params->group.group_key,\n                                col_params->instance.instance_key),\n                  actual_done);\n            } else {\n              c->SetStatus(s);\n              done();\n            }\n          });\n    });\n  }\n\n protected:\n  string name_;\n  DataType data_type_ = DT_INVALID;\n  DeviceType device_type_;\n  float timeout_seconds_ = 0;\n};\n\nclass CollectiveReduceV3OpKernel : public CollectiveOpV3Kernel {\n public:\n  explicit CollectiveReduceV3OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV3Kernel(c) {\n    string reduction;\n    OP_REQUIRES_OK(c, c->GetAttr(\"reduction\", &reduction));\n    if (reduction == \"Max\") {\n      reduction = \"Maximum\";\n    } else if (reduction == \"Min\") {\n      reduction = \"Minimum\";\n    }\n    // Prepare OpKernels for reduction and final operations.\n    // The merge_op takes two inputs\n    NodeDef sub_node;\n    sub_node.add_input(c->def().input(0));\n    sub_node.add_input(c->def().input(0));\n    sub_node.set_device(c->def().device());\n    SetAttrValue(data_type_, &(*sub_node.mutable_attr())[\"T\"]);\n    merge_op_ = BuildOpKernel(c, reduction, &sub_node);\n    final_op_ = BuildOpKernel(c, \"Id\", &sub_node);\n    name_ = strings::StrCat(c->def().name(), \": ReduceV3(\", reduction, \")\");\n    VLOG(2) << \"CollectiveReduceV3 \" << this << \" name \" << name_;\n  }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    core::RefCountPtr<CollectiveGroupResource> resource;\n    OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),\n                         done_with_cleanup);\n\n    Tensor group_assignment = c->input(2);\n\n    OP_REQUIRES_OK_ASYNC(\n        c,\n        FillCollectiveParams(col_params, group_assignment, REDUCTION_COLLECTIVE,\n                             resource.get()),\n        done_with_cleanup);\n    col_params->instance.shape = c->input(0).shape();\n    col_params->merge_op = merge_op_.get();\n    col_params->final_op = final_op_.get();\n    VLOG(1) << \"CollectiveReduceV3 group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n\n private:\n  std::unique_ptr<OpKernel> merge_op_;\n  std::unique_ptr<OpKernel> final_op_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV3\").Device(DEVICE_CPU),\n                        CollectiveReduceV3OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveReduceV3\").Device(DEVICE_GPU),\n                        CollectiveReduceV3OpKernel);\n\nclass CollectiveAllToAllV3OpKernel : public CollectiveOpV3Kernel {\n public:\n  explicit CollectiveAllToAllV3OpKernel(OpKernelConstruction* c)\n      : CollectiveOpV3Kernel(c) {\n    name_ = strings::StrCat(c->def().name(), \": AllToAllV3\");\n    VLOG(2) << \"CollectiveAllToAllV3 \" << this << \" name \" << name_;\n  }\n\n  void ComputeAsync(OpKernelContext* c, DoneCallback done) override {\n    auto col_params = new CollectiveParams();\n    auto done_with_cleanup = [col_params, done = std::move(done)]() {\n      done();\n      col_params->Unref();\n    };\n    core::RefCountPtr<CollectiveGroupResource> resource;\n    OP_REQUIRES_OK_ASYNC(c, LookupResource(c, HandleFromInput(c, 1), &resource),\n                         done_with_cleanup);\n\n    Tensor group_assignment = c->input(2);\n\n    OP_REQUIRES_OK_ASYNC(\n        c,\n        FillCollectiveParams(col_params, group_assignment,\n                             ALL_TO_ALL_COLLECTIVE, resource.get()),\n        done_with_cleanup);\n    col_params->instance.shape = c->input(0).shape();\n    VLOG(1) << \"CollectiveAllToAll group_size \" << col_params->group.group_size\n            << \" group_key \" << col_params->group.group_key << \" instance_key \"\n            << col_params->instance.instance_key;\n    // Allocate the output tensor, trying to reuse the input.\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK_ASYNC(c,\n                         c->forward_input_or_allocate_output(\n                             {0}, 0, col_params->instance.shape, &output),\n                         done_with_cleanup);\n    Run(c, col_params, std::move(done_with_cleanup));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveAllToAllV3\").Device(DEVICE_CPU),\n                        CollectiveAllToAllV3OpKernel);\nREGISTER_KERNEL_BUILDER(Name(\"CollectiveAllToAllV3\").Device(DEVICE_GPU),\n                        CollectiveAllToAllV3OpKernel);\n}  // namespace\n}  // namespace tensorflow\n", "# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Collective Operations.\"\"\"\n\nimport time\n\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.core.protobuf import rewriter_config_pb2\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import config\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import kernels\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import collective_ops\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging as logging\n\n\nclass CollectiveOpTest(test.TestCase):\n\n  def setUp(self):\n    context._reset_context()  # pylint: disable=protected-access\n    super(CollectiveOpTest, self).setUp()\n\n  def _testCollectiveReduce(self,\n                            inputs,\n                            expected,\n                            set_graph_key,\n                            communication_hint='auto',\n                            fp16=False,\n                            instance_key=1,\n                            merge_op='Add',\n                            final_op='Div',\n                            timeout=0,\n                            reported_group_size=None):\n    group_key = 1\n    group_size = len(inputs)\n    if reported_group_size is None:\n      reported_group_size = group_size\n    device_type = 'CPU'\n    config = config_pb2.ConfigProto(device_count={device_type: group_size})\n    devices = ['/{}:{}'.format(device_type, i) for i in range(group_size)]\n\n    with self.session(config=config) as sess:\n      colred = []\n      for i in range(group_size):\n        with ops.device(devices[i]):\n          tensor = constant_op.constant(inputs[i], dtype=(\n              dtypes.float16 if fp16 else dtypes.float32))\n          colred.append(\n              collective_ops.all_reduce(\n                  tensor,\n                  reported_group_size,\n                  group_key,\n                  instance_key,\n                  merge_op,\n                  final_op,\n                  communication_hint=communication_hint,\n                  timeout=timeout))\n      run_options = config_pb2.RunOptions()\n      if set_graph_key:\n        run_options.experimental.collective_graph_key = 1\n      results = sess.run(colred, options=run_options)\n    tolerance = 1e-3 if fp16 else 1e-5\n    for i in range(group_size):\n      logging.info('i {} result {} expected {}'.format(i, results[i], expected))\n      self.assertAllClose(results[i], expected, rtol=tolerance, atol=tolerance)\n\n  def _testMultipleConcurrentCollectiveReduce(self, t0, t1, expected):\n    group_key = 1\n    group_size = 2\n    num_instances = 2\n    all_reduces = []\n    config = config_pb2.ConfigProto(device_count={'CPU': group_size})\n    config.experimental.collective_deterministic_sequential_execution = True\n    with self.session(config=config) as sess:\n      for cpu in range(group_size):\n        with ops.device('/CPU:%d' % cpu):\n          in_tensor = constant_op.constant(t0 if cpu == 0 else t1)\n          for instance in range(num_instances):\n            all_reduces.append(collective_ops.all_reduce(\n                in_tensor, group_size, group_key, instance, 'Add', 'Div'))\n      results = sess.run(all_reduces)\n    for i in range(group_size * num_instances):\n      self.assertAllClose(results[i], expected, rtol=1e-5, atol=1e-5)\n\n  def testCollectiveReduce(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1],\n                  [0.3, 1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3]],\n          expected=[0.2, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2],\n          set_graph_key=True)\n\n  def testCollectiveAutoGraphKey(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1],\n                  [0.3, 1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3]],\n          expected=[0.2, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2],\n          set_graph_key=False)\n\n  def testFp16Reduce(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1],\n                  [0.3, 1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3]],\n          expected=[0.2, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2],\n          set_graph_key=True,\n          fp16=True)\n\n  def testCollectiveMultipleConcurrentReduce(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testMultipleConcurrentCollectiveReduce(\n          [0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1],\n          [0.3, 1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3],\n          [0.2, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2])\n\n  def testCollectiveTimeoutV1(self):\n    timeout = 4.5\n    kwargs = dict(\n        inputs=[[i + j + 0.1 for i in range(8)] for j in range(3)],\n        expected=[1 + i + 0.1 for i in range(8)],\n        set_graph_key=True,\n        timeout=timeout)\n\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(**kwargs)\n\n    start_time = time.time()\n    with ops.Graph().as_default():\n      with self.assertRaisesRegex(\n          errors.DeadlineExceededError,\n          'Collective has timed out waiting for other workers'):\n        self._testCollectiveReduce(\n            reported_group_size=len(kwargs['inputs']) + 1, **kwargs)\n    elapsed = time.time() - start_time\n    self.assertAllGreaterEqual(elapsed, timeout)\n\n  def testNcclHintFallbackToRingReduce(self):\n    \"\"\"Tests that setting `communication_hint=nccl` works on non-GPU builds.\"\"\"\n    if kernels.get_registered_kernels_for_op('NcclAllReduce'):\n      self.skipTest('Run only on non-GPU environments')\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1],\n                  [0.3, 1.3, 2.3, 3.3, 4.3, 5.3, 6.3, 7.3]],\n          expected=[0.2, 1.2, 2.2, 3.2, 4.2, 5.2, 6.2, 7.2],\n          set_graph_key=False,\n          communication_hint='nccl')\n\n  def _testWhile(self, num_vars, num_iterations, key_base):\n    group_size = 2\n    group_key = 1\n    instances = [(key_base + i) for i in range(num_vars)]\n    devices = ['CPU:{}'.format(i) for i in range(group_size)]\n\n    config = config_pb2.ConfigProto(device_count={'CPU': group_size})\n    rewrite_options = config.graph_options.rewrite_options\n    rewrite_options.scoped_allocator_optimization = (\n        rewriter_config_pb2.RewriterConfig.ON)\n    del rewrite_options.scoped_allocator_opts.enable_op[:]\n    rewrite_options.scoped_allocator_opts.enable_op.append('CollectiveReduce')\n\n    with self.session(config=config) as sess:\n      loop_vars = []\n      for device in devices:\n        with ops.device(device):\n          loop_vars.append(\n              [variables.VariableV1((1 << i) * 1.) for i in range(num_vars)])\n      # This variable controls number of iterations.\n      loop_vars.append(variables.VariableV1(0.))\n      def loop_body(dev0_tensors, dev1_tensors, loop_tensor):\n        return_ops = []\n        for i in range(len(devices)):\n          device = devices[i]\n          device_tensors = dev0_tensors if i == 0 else dev1_tensors\n          with ops.device(device):\n            device_collectives = []\n            for j in range(num_vars):\n              # NOTE(ayushd): we need the `cast` here to ensure that the input\n              # to `all_reduce` has an explicit device string.  We don't use\n              # `identity` because `cast` is more resilient to getting optimized\n              # away by various optimization passes.\n              input_tensor = math_ops.cast(device_tensors[j], dtypes.float16)\n              collective_op = collective_ops.all_reduce(\n                  input_tensor, group_size, group_key, instances[j],\n                  'Add', 'Id')\n              output_tensor = math_ops.cast(collective_op, dtypes.float32)\n              device_collectives.append(output_tensor)\n            return_ops.append(device_collectives)\n        return_ops.append(math_ops.add(loop_tensor, 1.))\n        return return_ops\n      # Run until last variable exceeds number of iterations.\n      loop_cond = lambda d0, d1, i: math_ops.less(i, num_iterations)\n      sess.run(variables.global_variables_initializer())\n      results = sess.run(control_flow_ops.while_loop(loop_cond, loop_body,\n                                                     loop_vars))\n      self.assertEqual(results[:-1], [\n          [((1 << (num_iterations + v)) * 1.) for v in range(num_vars)]\n          for _ in range(group_size)])\n\n  def testSimpleWhile(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testWhile(num_vars=1, num_iterations=4, key_base=20)\n\n  def testWhileMultipleAllReduce(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testWhile(num_vars=2, num_iterations=4, key_base=20)\n\n  def testWhileWithScopedAllocator(self):\n    group_size = 2\n    group_key = 1\n    instance_key0 = 1\n    instance_key1 = 2\n\n    config = config_pb2.ConfigProto(device_count={'CPU': group_size})\n    rewrite_options = config.graph_options.rewrite_options\n    rewrite_options.scoped_allocator_optimization = (\n        rewriter_config_pb2.RewriterConfig.ON)\n    del rewrite_options.scoped_allocator_opts.enable_op[:]\n    rewrite_options.scoped_allocator_opts.enable_op.append('CollectiveReduce')\n\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      with self.session(config=config) as sess:\n        run_ops = []\n        for i in range(group_size):\n          with ops.device('CPU:%d' % i):\n            constant = constant_op.constant(0.)\n            cond = lambda i: math_ops.less(i, 10.)\n            body = lambda i: math_ops.add(i, 1.)\n            input0 = control_flow_ops.while_loop(cond, body, [constant])\n            input1 = math_ops.add(constant, 5)\n            colred0 = collective_ops.all_reduce(input0, group_size, group_key,\n                                                instance_key0, 'Add', 'Id')\n            colred1 = collective_ops.all_reduce(input1, group_size, group_key,\n                                                instance_key1, 'Add', 'Id')\n            run_ops.append(math_ops.add_n([colred0, colred1]))\n        results = sess.run(run_ops)\n      self.assertEqual(results, [30., 30.])\n\n  def testCollectiveReduceScalar(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(inputs=[0.1, 0.3], expected=0.2,\n                                 set_graph_key=True)\n\n  def testCollectiveReduceMaximum(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[1., 20., 3., 40., 5.], [10., 2., 30., 4., 50.]],\n          expected=[10., 20., 30., 40., 50.],\n          set_graph_key=True,\n          instance_key=30,\n          merge_op='Max',\n          final_op='Id')\n\n  def testCollectiveReduceMinimum(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveReduce(\n          inputs=[[1., 20., 3., 40., 5.], [10., 2., 30., 4., 50.]],\n          expected=[1., 2., 3., 4., 5.],\n          set_graph_key=True,\n          instance_key=40,\n          merge_op='Min',\n          final_op='Id')\n\n  def _testCollectiveBroadcast(self, in_val):\n    group_key = 1\n    instance_key = 1\n    with self.session(\n        config=config_pb2.ConfigProto(device_count={'CPU': 2})) as sess:\n      with ops.device('/CPU:0'):\n        in0 = constant_op.constant(in_val)\n        out0 = collective_ops.broadcast_send(in0, in0.shape, in0.dtype,\n                                             2, group_key, instance_key)\n      with ops.device('/CPU:1'):\n        c1 = constant_op.constant(in_val)\n        out1 = collective_ops.broadcast_recv(c1.shape, c1.dtype,\n                                             2, group_key, instance_key)\n      run_options = config_pb2.RunOptions()\n      run_options.experimental.collective_graph_key = 1\n      results = sess.run([out0, out1], options=run_options)\n    self.assertAllClose(results[0], in_val, rtol=1e-5, atol=1e-5)\n    self.assertAllClose(results[1], in_val, rtol=1e-5, atol=1e-5)\n\n  def testCollectiveBroadcast(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveBroadcast([0.1, 1.1, 2.1, 3.1, 4.1, 5.1, 6.1, 7.1])\n\n  def testCollectiveBroadcastBool(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveBroadcast([True, False])\n\n  def _testCollectiveGather(self, t0, t1, expected, set_graph_key):\n    group_key = 1\n    instance_key = 1\n    with self.session(\n        config=config_pb2.ConfigProto(device_count={'CPU': 2})) as sess:\n      with ops.device('/CPU:0'):\n        in0 = constant_op.constant(t0)\n        c0 = collective_ops.all_gather(in0, 2, group_key, instance_key)\n      with ops.device('/CPU:1'):\n        in1 = constant_op.constant(t1)\n        c1 = collective_ops.all_gather(in1, 2, group_key, instance_key)\n      run_options = config_pb2.RunOptions()\n      if set_graph_key:\n        run_options.experimental.collective_graph_key = 1\n      results = sess.run([c0, c1], options=run_options)\n    self.assertAllClose(results[0], expected, rtol=1e-5, atol=1e-5)\n    self.assertAllClose(results[1], expected, rtol=1e-5, atol=1e-5)\n\n  def testCollectiveGather(self):\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      self._testCollectiveGather([0, 1, 2, 3, 4, 5, 6, 7],\n                                 [10, 11, 12, 13, 14, 15, 16, 17],\n                                 [0, 1, 2, 3, 4, 5, 6, 7,\n                                  10, 11, 12, 13, 14, 15, 16, 17],\n                                 True)\n      self._testCollectiveGather([[0, 1, 2, 3], [4, 5, 6, 7]],\n                                 [[10, 11, 12, 13], [14, 15, 16, 17]],\n                                 [[0, 1, 2, 3], [4, 5, 6, 7],\n                                  [10, 11, 12, 13], [14, 15, 16, 17]],\n                                 True)\n      self._testCollectiveGather([[[0, 1], [2, 3]], [[4, 5], [6, 7]]],\n                                 [[[10, 11], [12, 13]], [[14, 15], [16, 17]]],\n                                 [[[0, 1], [2, 3]], [[4, 5], [6, 7]],\n                                  [[10, 11], [12, 13]], [[14, 15], [16, 17]]],\n                                 True)\n\n  def testCollectiveGatherShapeMismatch(self):\n    group_key = 1\n    instance_key = 1\n    t0 = [1, 2, 3, 4]\n    t1 = [5, 6, 7, 8]\n    t2 = [9, 10]\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      with self.session(\n          config=config_pb2.ConfigProto(device_count={'CPU': 2})) as sess:\n        with ops.device('/CPU:0'):\n          in0 = constant_op.constant(t0)\n          c0 = collective_ops.all_gather(in0, 2, group_key, instance_key)\n        with ops.device('/CPU:1'):\n          in1 = constant_op.constant(t1)\n          in2 = constant_op.constant(t2)\n          c1 = collective_ops.all_gather(in1, 2, group_key, instance_key)\n          c2 = collective_ops.all_gather(in2, 2, group_key, instance_key)\n        run_options = config_pb2.RunOptions()\n        run_options.experimental.collective_graph_key = 1\n        sess.run([c0, c1], options=run_options)\n        with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                    'Shape mismatch'):\n          sess.run([c0, c2], options=run_options)\n\n  def testCollectiveGatherShapeMismatchAcrossDevices(self):\n    group_key = 1\n    instance_key = 1\n    t0 = [1, 2, 3, 4]\n    t1 = [5, 6]\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      with self.session(\n          config=config_pb2.ConfigProto(device_count={'CPU': 2})) as sess:\n        with ops.device('/CPU:0'):\n          in0 = constant_op.constant(t0)\n          c0 = collective_ops.all_gather(in0, 2, group_key, instance_key)\n        with ops.device('/CPU:1'):\n          in1 = constant_op.constant(t1)\n          c1 = collective_ops.all_gather(in1, 2, group_key, instance_key)\n        run_options = config_pb2.RunOptions()\n        run_options.experimental.collective_graph_key = 1\n        with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                    'Shape mismatch'):\n          sess.run([c0, c1], options=run_options)\n\n  def testCollectiveGatherPolymorphicShape(self):\n    t0 = [0, 1, 2, 3, 4, 5, 6, 7]\n    t1 = [10, 11, 12, 13, 14, 15, 16, 17]\n    group_size = 2\n    group_key = 1\n    instance_key = 123\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      with self.session(\n          config=config_pb2.ConfigProto(\n              device_count={'CPU': group_size})) as sess:\n        with ops.device('/CPU:0'):\n          in0 = array_ops.placeholder(dtype=dtypes.int32, shape=[None])\n          c0 = collective_ops.all_gather(in0, group_size, group_key,\n                                         instance_key)\n        with ops.device('/CPU:1'):\n          in1 = array_ops.placeholder(dtype=dtypes.int32, shape=[None])\n          c1 = collective_ops.all_gather(in1, group_size, group_key,\n                                         instance_key)\n\n        results = sess.run([c0, c1], feed_dict={in0: t0, in1: t1})\n        results_ = sess.run([c0, c1], feed_dict={in0: t0[1:], in1: t1[1:]})\n\n    expected_output = [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 17]\n    self.assertAllClose(results[0], expected_output, rtol=1e-5, atol=1e-5)\n    self.assertAllClose(results[1], expected_output, rtol=1e-5, atol=1e-5)\n\n    expected_output_ = [1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 14, 15, 16, 17]\n    self.assertAllClose(results_[0], expected_output_, rtol=1e-5, atol=1e-5)\n    self.assertAllClose(results_[1], expected_output_, rtol=1e-5, atol=1e-5)\n\n  @test_util.run_v2_only\n  @test_util.disable_tfrt(\n      'b/177270918: TFRT has dead lock when executing collective ops.')\n  def testCollectiveGroupSizeMismatch(self):\n    cpus = config.list_physical_devices('CPU')\n    self.assertEqual(len(cpus), 1)\n    config.set_logical_device_configuration(cpus[0], [\n        context.LogicalDeviceConfiguration(),\n        context.LogicalDeviceConfiguration()\n    ])\n    context.ensure_initialized()\n\n  @test_util.run_v2_only\n  def testCollectiveGatherShapeCheckFailure(self):\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                'input should have rank > 0'):\n      collective_ops.gen_collective_ops.CollectiveGather(\n          input=1,\n          group_size=1,\n          group_key=1,\n          instance_key=1,\n          shape=(3, 3, 3),\n          communication_hint='auto',\n          timeout_seconds=0,\n          name='')\n\n    @def_function.function\n    def run_all_reduce():\n      group_key = 10\n      instance_key = 20\n      t0 = [1, 2, 3, 4]\n      t1 = [5, 6, 7, 8]\n      with ops.device('/CPU:0'):\n        in0 = constant_op.constant(t0)\n        c0 = collective_ops.all_reduce(\n            in0, group_size=2, group_key=group_key, instance_key=instance_key,\n            merge_op='Add', final_op='Id')\n      with ops.device('/CPU:1'):\n        in1 = constant_op.constant(t1)\n        c1 = collective_ops.all_reduce(\n            in1, group_size=3, group_key=group_key, instance_key=instance_key,\n            merge_op='Add', final_op='Id')\n      return c0, c1\n\n    with self.assertRaisesRegex(errors.InternalError,\n                                'but that group has size'):\n      run_all_reduce()\n\n  @test_util.run_v2_only\n  def testCollectiveTensorsHaveNoDeviceSpecified(self):\n    cpus = config.list_physical_devices('CPU')\n    self.assertEqual(len(cpus), 1)\n    config.set_logical_device_configuration(cpus[0], [\n        context.LogicalDeviceConfiguration(),\n        context.LogicalDeviceConfiguration()\n    ])\n    context.ensure_initialized()\n\n    group_size = 2\n    group_key = 1\n    instance_key = 1\n\n    @def_function.function\n    def fn(all_args):\n      results = []\n      # The inputs have no devices set. This is expected to be a trace-time\n      # check only.\n      self.assertEqual(all_args[0].device, '')\n      self.assertEqual(all_args[1].device, '')\n\n      with ops.device('/CPU:0'):\n        results.append(\n            collective_ops.all_reduce(all_args[0], group_size, group_key,\n                                      instance_key, 'Add', 'Div'))\n      with ops.device('/CPU:1'):\n        results.append(\n            collective_ops.all_reduce(all_args[1], group_size, group_key,\n                                      instance_key, 'Add', 'Div'))\n\n      return results\n\n    with ops.device('/CPU:0'):\n      in0 = constant_op.constant(1)\n    with ops.device('/CPU:1'):\n      in1 = constant_op.constant(3)\n    result = fn([in0, in1])\n    self.assertAllClose(result, [2, 2])\n\n  def testConstantWithScopedAllocator(self):\n    group_size = 2\n    group_key = 1\n    instance_key1 = 1\n    instance_key2 = 2\n\n    graph_options = config_pb2.GraphOptions(\n        optimizer_options=config_pb2.OptimizerOptions(do_constant_folding=True))\n    cfg = config_pb2.ConfigProto(device_count={'CPU': group_size},\n                                 graph_options=graph_options)\n    rewrite_options = cfg.graph_options.rewrite_options\n    rewrite_options.scoped_allocator_optimization = (\n        rewriter_config_pb2.RewriterConfig.ON)\n    del rewrite_options.scoped_allocator_opts.enable_op[:]\n    rewrite_options.scoped_allocator_opts.enable_op.append('CollectiveReduce')\n\n    # Tests that execute collectives need to be enclosed in graph or tf.function\n    with ops.Graph().as_default():\n      with self.session(config=cfg) as sess:\n        run_ops = []\n        for i in range(group_size):\n          with ops.device('CPU:%d' % i):\n            constant = constant_op.constant(i + 1.)\n            input_tensor1 = array_ops.identity(constant)\n            input_tensor2 = array_ops.identity(constant)\n            reduced_tensor1 = collective_ops.all_reduce(\n                input_tensor1, group_size, group_key, instance_key1, 'Add',\n                'Id')\n            reduced_tensor2 = collective_ops.all_reduce(\n                input_tensor2, group_size, group_key, instance_key2, 'Add',\n                'Id')\n            run_ops.append(array_ops.identity(reduced_tensor1))\n            run_ops.append(array_ops.identity(reduced_tensor2))\n        results = sess.run(run_ops)\n    self.assertEqual(results, [3., 3., 3., 3.])\n\n\nif __name__ == '__main__':\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/collective_ops.cc", "tensorflow/python/ops/collective_ops_test.py"], "buggy_code_start_loc": [178, 453], "buggy_code_end_loc": [178, 453], "fixing_code_start_loc": [179, 454], "fixing_code_end_loc": [183, 468], "type": "CWE-617", "message": "TensorFlow is an open source platform for machine learning. When `CollectiveGather` receives an scalar input `input`, it gives a `CHECK` fails that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit c1f491817dec39a26be3c574e86a88c30f3c4770. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue.", "other": {"cve": {"id": "CVE-2022-35994", "sourceIdentifier": "security-advisories@github.com", "published": "2022-09-16T23:15:10.290", "lastModified": "2022-09-20T14:50:21.257", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. When `CollectiveGather` receives an scalar input `input`, it gives a `CHECK` fails that can be used to trigger a denial of service attack. We have patched the issue in GitHub commit c1f491817dec39a26be3c574e86a88c30f3c4770. The fix will be included in TensorFlow 2.10.0. We will also cherrypick this commit on TensorFlow 2.9.1, TensorFlow 2.8.1, and TensorFlow 2.7.2, as these are also affected and still in supported range. There are no known workarounds for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. Cuando \"CollectiveGather\" recibe una entrada escalar \"input\", da un fallo \"CHECK\" que puede ser usado para desencadenar un ataque de denegaci\u00f3n de servicio. Hemos parcheado el problema en el commit c1f491817dec39a26be3c574e86a88c30f3c4770 de GitHub. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.10.0. Tambi\u00e9n seleccionaremos este compromiso en TensorFlow versi\u00f3n 2.9.1, TensorFlow versi\u00f3n 2.8.1, y TensorFlow versi\u00f3n 2.7.2, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda est\u00e1n en el rango admitido. No se presentan mitigaciones conocidas para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.9, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.2, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-617"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C6622D95-1C86-45C5-AB55-E6EEEA0996DF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.8.0", "versionEndExcluding": "2.8.1", "matchCriteriaId": "0F9D273D-02DC-441E-AA91-EAC8DEAA4B44"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.9.0", "versionEndExcluding": "2.9.1", "matchCriteriaId": "FE4F8A81-6CC2-4F7F-9602-C170FDD926E7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc0:*:*:*:*:*:*", "matchCriteriaId": "1DBFBCE2-0A01-4575-BE45-6775ABFB8B28"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc1:*:*:*:*:*:*", "matchCriteriaId": "89806CF9-E423-4CA6-A01A-8175C260CB24"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc2:*:*:*:*:*:*", "matchCriteriaId": "F2B80690-A257-4E16-BD27-9AE045BC56ED"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.10:rc3:*:*:*:*:*:*", "matchCriteriaId": "F335F9A4-5AB8-4E53-BC18-E01F7C653E5E"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/c1f491817dec39a26be3c574e86a88c30f3c4770", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-fhfc-2q7x-929f", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/c1f491817dec39a26be3c574e86a88c30f3c4770"}}