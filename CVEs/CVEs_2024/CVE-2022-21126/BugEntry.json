{"buggy_code": ["/*\n * The MIT License\n *\n * Copyright (c) 2010 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools;\n\nimport htsjdk.samtools.util.CloseableIterator;\nimport htsjdk.samtools.util.CloserUtil;\nimport htsjdk.samtools.util.FileAppendStreamLRUCache;\nimport htsjdk.samtools.util.IOUtil;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\n\n/**\n * Holds info about a mate pair for use when processing a coordinate sorted file.  When one read of a pair is encountered,\n * the caller should add a record to this map.  When the other read of a pair is encountered, the record should be removed.\n * This class assumes that reads will be processed in order of reference sequence index.  When the map is queried for\n * a record for a given reference sequence index, all the records for that sequence are loaded from temp file into RAM, so there\n * must be sufficient RAM to hold all the records for one reference sequence.  If the records are not processed in\n * reference sequence order, loading and unloading of records will cause performance to be terrible.\n * @param <KEY> KEY + reference sequence index are used to identify the record being stored or retrieved.\n * @param <REC> The type of record being retrieved.\n */\npublic class CoordinateSortedPairInfoMap<KEY, REC> implements Iterable<Map.Entry<KEY, REC>> {\n    // -1 is a valid sequence index in this case\n    private final int INVALID_SEQUENCE_INDEX = -2;\n    /**\n     * directory where files will go\n     */\n    private final File workDir = IOUtil.createTempDir(\"CSPI.\", null);\n    private int sequenceIndexOfMapInRam = INVALID_SEQUENCE_INDEX;\n    private Map<KEY, REC> mapInRam = null;\n    private final FileAppendStreamLRUCache outputStreams;\n    private final Codec<KEY, REC> elementCodec;\n    // Key is reference index (which is in the range [-1 .. max sequence index].\n    // Value is the number of records on disk for this index.\n    private final Map<Integer, Integer> sizeOfMapOnDisk = new HashMap<Integer, Integer>();\n\n    // No other methods may be called when iteration is in progress, because iteration depends on and changes\n    // internal state.\n    private boolean iterationInProgress = false;\n\n    public CoordinateSortedPairInfoMap(final int maxOpenFiles, final Codec<KEY, REC> elementCodec) {\n        this.elementCodec = elementCodec;\n        workDir.deleteOnExit();\n        outputStreams = new FileAppendStreamLRUCache(maxOpenFiles);\n    }\n\n    /**\n     *\n     * @param sequenceIndex\n     * @param key\n     * @return The record corresponding to the given sequenceIndex and key, or null if it is not present.\n     */\n    public REC remove(final int sequenceIndex, final KEY key) {\n        if (iterationInProgress) throw new IllegalStateException(\"Cannot be called when iteration is in progress\");\n        ensureSequenceLoaded(sequenceIndex);\n        return mapInRam.remove(key);\n    }\n\n    private void ensureSequenceLoaded(final int sequenceIndex) {\n        try {\n            if (sequenceIndexOfMapInRam == sequenceIndex) {\n                return;\n            }\n\n            // Spill map in RAM to disk\n            if (mapInRam != null) {\n                final File spillFile = makeFileForSequence(sequenceIndexOfMapInRam);\n                if (spillFile.exists()) throw new IllegalStateException(spillFile + \" should not exist.\");\n                if (!mapInRam.isEmpty()) {\n                    // Do not create file or entry in sizeOfMapOnDisk if there is nothing to write.\n                    final OutputStream os = getOutputStreamForSequence(sequenceIndexOfMapInRam);\n                    elementCodec.setOutputStream(os);\n                    for (final Map.Entry<KEY, REC> entry : mapInRam.entrySet()) {\n                        elementCodec.encode(entry.getKey(), entry.getValue());\n                    }\n                    sizeOfMapOnDisk.put(sequenceIndexOfMapInRam, mapInRam.size());\n                    mapInRam.clear();\n                }\n            } else {\n                mapInRam = new HashMap<KEY, REC>();\n            }\n\n            sequenceIndexOfMapInRam = sequenceIndex;\n\n            // Load map from disk if it existed\n            File mapOnDisk = makeFileForSequence(sequenceIndex);\n            if (outputStreams.containsKey(mapOnDisk)) {\n                outputStreams.remove(mapOnDisk).close();\n            }\n            final Integer numRecords = sizeOfMapOnDisk.remove(sequenceIndex);\n            if (mapOnDisk.exists()) {\n                if (numRecords == null)\n                    throw new IllegalStateException(\"null numRecords for \" + mapOnDisk);\n                FileInputStream is = null;\n                try {\n                    is = new FileInputStream(mapOnDisk);\n                    elementCodec.setInputStream(is);\n                    for (int i = 0; i < numRecords; ++i) {\n                        final Map.Entry<KEY, REC> keyAndRecord = elementCodec.decode();\n                        if (mapInRam.containsKey(keyAndRecord.getKey()))\n                            throw new SAMException(\"Value was put into PairInfoMap more than once.  \" +\n                                    sequenceIndex + \": \" + keyAndRecord.getKey());\n                        mapInRam.put(keyAndRecord.getKey(), keyAndRecord.getValue());\n                    }\n                } finally {\n                    CloserUtil.close(is);\n                }\n                htsjdk.samtools.util.IOUtil.deleteFiles(mapOnDisk);\n            } else if (numRecords != null && numRecords > 0)\n                throw new IllegalStateException(\"Non-zero numRecords but \" + mapOnDisk + \" does not exist\");\n        } catch (IOException e) {\n            throw new SAMException(\"Error loading new map from disk.\", e);\n        }\n    }\n\n    /**\n     * Store the record with the given sequence index and key.  It is assumed that value did not previously exist\n     * in the map, and an exception is thrown (possibly at a later time) if that is not the case.\n     * @param sequenceIndex\n     * @param key\n     * @param record\n     */\n    public void put(final int sequenceIndex, final KEY key, final REC record) {\n        if (iterationInProgress) throw new IllegalStateException(\"Cannot be called when iteration is in progress\");\n        if (sequenceIndex == sequenceIndexOfMapInRam) {\n            // Store in RAM map\n            if (mapInRam.containsKey(key))\n                throw new IllegalArgumentException(\"Putting value into PairInfoMap that already existed. \" +\n                        sequenceIndex + \": \" + key);\n            mapInRam.put(key, record);\n        } else {\n            // Append to file\n            final OutputStream os = getOutputStreamForSequence(sequenceIndex);\n            elementCodec.setOutputStream(os);\n            elementCodec.encode(key, record);\n            Integer prevCount = sizeOfMapOnDisk.get(sequenceIndex);\n            if (prevCount == null) prevCount = 0;\n            sizeOfMapOnDisk.put(sequenceIndex,  prevCount + 1);\n        }\n    }\n\n    private File makeFileForSequence(final int index) {\n        final File file = new File(workDir, index + \".tmp\");\n        file.deleteOnExit();\n        return file;\n    }\n\n    private OutputStream getOutputStreamForSequence(final int mateSequenceIndex) {\n        return outputStreams.get(makeFileForSequence(mateSequenceIndex));\n    }\n\n    public int size() {\n        int total = sizeInRam();\n        for (final Integer mapSize : sizeOfMapOnDisk.values()) {\n            if (mapSize != null) {\n                total += mapSize;\n            }\n        }\n        return total;\n    }\n\n    /**\n     * @return number of elements stored in RAM.  Always <= size()\n     */\n    public int sizeInRam() {\n        return mapInRam != null? mapInRam.size(): 0;\n    }\n\n    /**\n     * Creates an iterator over all elements in map, in arbitrary order.  Elements may not be added\n     * or removed from map when iteration is in progress, nor may a second iteration be started.\n     * Iterator must be closed in order to allow normal access to the map.\n     */\n    @Override\n    public CloseableIterator<Map.Entry<KEY, REC>> iterator() {\n        if (iterationInProgress) throw new IllegalStateException(\"Cannot be called when iteration is in progress\");\n        iterationInProgress = true;\n        return new MapIterator();\n    }\n\n    private class MapIterator implements CloseableIterator<Map.Entry<KEY, REC>> {\n        private boolean closed = false;\n        private Set<Integer> referenceIndices = new HashSet<Integer>(sizeOfMapOnDisk.keySet());\n        private final Iterator<Integer> referenceIndexIterator;\n        private Iterator<Map.Entry<KEY, REC>> currentReferenceIterator = null;\n\n        private MapIterator() {\n            if (sequenceIndexOfMapInRam != INVALID_SEQUENCE_INDEX)\n                referenceIndices.add(sequenceIndexOfMapInRam);\n            referenceIndexIterator = referenceIndices.iterator();\n            advanceToNextNonEmptyReferenceIndex();\n        }\n\n        private void advanceToNextNonEmptyReferenceIndex() {\n            while (referenceIndexIterator.hasNext()) {\n                int nextReferenceIndex = referenceIndexIterator.next();\n                ensureSequenceLoaded(nextReferenceIndex);\n                if (!mapInRam.isEmpty()) {\n                    createIteratorForMapInRam();\n                    return;\n                }\n            }\n            // no more.\n            currentReferenceIterator = null;\n        }\n\n        private void createIteratorForMapInRam() {\n            currentReferenceIterator = mapInRam.entrySet().iterator();\n        }\n\n        @Override\n        public void close() {\n            closed = true;\n            iterationInProgress = false;\n        }\n\n        @Override\n        public boolean hasNext() {\n            if (closed) throw new IllegalStateException(\"Iterator has been closed\");\n            if (currentReferenceIterator != null && !currentReferenceIterator.hasNext())\n                throw new IllegalStateException(\"Should not happen\");\n            return currentReferenceIterator != null;\n        }\n\n        @Override\n        public Map.Entry<KEY, REC> next() {\n            if (closed) throw new IllegalStateException(\"Iterator has been closed\");\n            if (!hasNext()) throw new NoSuchElementException();\n            final Map.Entry<KEY, REC> ret = currentReferenceIterator.next();\n            if (!currentReferenceIterator.hasNext()) advanceToNextNonEmptyReferenceIndex();\n            return ret;\n        }\n\n        @Override\n        public void remove() {\n            throw new UnsupportedOperationException();\n        }\n    }\n\n    /**\n     * Client must implement this class, which defines the way in which records are written to and\n     * read from file.\n     */\n    public interface Codec<KEY, REC> {\n        /**\n         * Where to write encoded output\n         * @param os\n         */\n        void setOutputStream(OutputStream os);\n\n        /**\n         * Where to read encoded input from\n         * @param is\n         */\n        void setInputStream(InputStream is);\n\n        /**\n         * Write object to output stream.  If the key is part of the record, then there is no need to write\n         * it separately.\n         */\n        void encode(KEY key, REC record);\n\n        /**\n         * Read the next key and record from the input stream and convert into a java object.\n         * @return null if no more records.  Should throw exception if EOF is encountered in the middle of\n         * a record.\n         */\n        Map.Entry<KEY, REC> decode();\n\n    }\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2009 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools.util;\n\nimport htsjdk.samtools.Defaults;\nimport htsjdk.samtools.SAMException;\nimport htsjdk.samtools.seekablestream.SeekableBufferedStream;\nimport htsjdk.samtools.seekablestream.SeekableFileStream;\nimport htsjdk.samtools.seekablestream.SeekableHTTPStream;\nimport htsjdk.samtools.seekablestream.SeekableStream;\nimport htsjdk.samtools.util.nio.DeleteOnExitPathHook;\n\nimport java.io.BufferedInputStream;\nimport java.io.BufferedOutputStream;\nimport java.io.BufferedReader;\nimport java.io.BufferedWriter;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileNotFoundException;\nimport java.io.FileOutputStream;\nimport java.io.FilenameFilter;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.io.OutputStream;\nimport java.io.OutputStreamWriter;\nimport java.io.Reader;\nimport java.io.Writer;\nimport java.net.MalformedURLException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.net.URL;\nimport java.nio.charset.Charset;\nimport java.nio.file.FileSystemNotFoundException;\nimport java.nio.file.FileSystems;\nimport java.nio.file.FileVisitResult;\nimport java.nio.file.Files;\nimport java.nio.file.OpenOption;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.nio.file.SimpleFileVisitor;\nimport java.nio.file.StandardOpenOption;\nimport java.nio.file.attribute.BasicFileAttributes;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.Scanner;\nimport java.util.Set;\nimport java.util.Stack;\nimport java.util.regex.Pattern;\nimport java.util.stream.Collectors;\nimport java.util.zip.Deflater;\nimport java.util.zip.GZIPInputStream;\n\n/**\n * Miscellaneous stateless static IO-oriented methods.\n *  Also used for utility methods that wrap or aggregate functionality in Java IO.\n */\npublic class IOUtil {\n    /**\n     * @deprecated Use {@link Defaults#NON_ZERO_BUFFER_SIZE} instead.\n     */\n    @Deprecated\n    public static final int STANDARD_BUFFER_SIZE = Defaults.NON_ZERO_BUFFER_SIZE;\n\n    public static final long ONE_GB = 1024 * 1024 * 1024;\n    public static final long TWO_GBS = 2 * ONE_GB;\n    public static final long FIVE_GBS = 5 * ONE_GB;\n\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#VCF} instead.\n     */\n    @Deprecated\n    public static final String VCF_FILE_EXTENSION = FileExtensions.VCF;\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#VCF_INDEX} instead.\n     */\n    @Deprecated\n    public static final String VCF_INDEX_EXTENSION = FileExtensions.VCF_INDEX;\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#BCF} instead.\n     */\n    @Deprecated\n    public static final String BCF_FILE_EXTENSION = FileExtensions.BCF;\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#COMPRESSED_VCF} instead.\n     */\n    @Deprecated\n    public static final String COMPRESSED_VCF_FILE_EXTENSION = FileExtensions.COMPRESSED_VCF;\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#COMPRESSED_VCF_INDEX} instead.\n     */\n    @Deprecated\n    public static final String COMPRESSED_VCF_INDEX_EXTENSION = FileExtensions.COMPRESSED_VCF_INDEX;\n\n    /** Possible extensions for VCF files and related formats. */\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#VCF_LIST} instead.\n     */\n    @Deprecated\n    public static final List<String> VCF_EXTENSIONS_LIST = FileExtensions.VCF_LIST;\n\n    /**\n     * Possible extensions for VCF files and related formats.\n     * @deprecated since June 2019 Use {@link FileExtensions#VCF_LIST} instead.\n     */\n    @Deprecated\n    public static final String[] VCF_EXTENSIONS = FileExtensions.VCF_LIST.toArray(new String[0]);\n\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#INTERVAL_LIST} instead.\n     */\n    @Deprecated\n    public static final String INTERVAL_LIST_FILE_EXTENSION = FileExtensions.INTERVAL_LIST;\n\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#SAM} instead.\n     */\n    @Deprecated\n    public static final String SAM_FILE_EXTENSION = FileExtensions.SAM;\n\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#DICT} instead.\n     */\n    @Deprecated\n    public static final String DICT_FILE_EXTENSION = FileExtensions.DICT;\n\n    /**\n     * @deprecated Use since June 2019 {@link FileExtensions#BLOCK_COMPRESSED} instead.\n     */\n    @Deprecated\n    public static final Set<String> BLOCK_COMPRESSED_EXTENSIONS = FileExtensions.BLOCK_COMPRESSED;\n\n    /** number of bytes that will be read for the GZIP-header in the function {@link #isGZIPInputStream(InputStream)} */\n    public static final int GZIP_HEADER_READ_LENGTH = 8000;\n\n    private static final OpenOption[] EMPTY_OPEN_OPTIONS = new OpenOption[0];\n\n    private static int compressionLevel = Defaults.COMPRESSION_LEVEL;\n    /**\n     * Sets the GZip compression level for subsequent GZIPOutputStream object creation.\n     * @param compressionLevel 0 <= compressionLevel <= 9\n     */\n    public static void setCompressionLevel(final int compressionLevel) {\n        if (compressionLevel < Deflater.NO_COMPRESSION || compressionLevel > Deflater.BEST_COMPRESSION) {\n            throw new IllegalArgumentException(\"Invalid compression level: \" + compressionLevel);\n        }\n        IOUtil.compressionLevel = compressionLevel;\n    }\n\n    public static int getCompressionLevel() {\n        return compressionLevel;\n    }\n\n    /**\n     * Wrap the given stream in a BufferedInputStream, if it isn't already wrapper\n     *\n     * @param stream stream to be wrapped\n     * @return A BufferedInputStream wrapping stream, or stream itself if stream instanceof BufferedInputStream.\n     */\n    public static BufferedInputStream toBufferedStream(final InputStream stream) {\n        if (stream instanceof BufferedInputStream) {\n            return (BufferedInputStream) stream;\n        } else {\n            return new BufferedInputStream(stream, Defaults.NON_ZERO_BUFFER_SIZE);\n        }\n    }\n\n    /**\n     * Transfers from the input stream to the output stream using stream operations and a buffer.\n     */\n    public static void transferByStream(final InputStream in, final OutputStream out, final long bytes) {\n        final byte[] buffer = new byte[Defaults.NON_ZERO_BUFFER_SIZE];\n        long remaining = bytes;\n\n        try {\n            while (remaining > 0) {\n                final int read = in.read(buffer, 0, (int) Math.min(buffer.length, remaining));\n                out.write(buffer, 0, read);\n                remaining -= read;\n            }\n        }\n        catch (final IOException ioe) {\n            throw new RuntimeIOException(ioe);\n        }\n    }\n\n    /**\n     * @return If Defaults.BUFFER_SIZE > 0, wrap os in BufferedOutputStream, else return os itself.\n     */\n    public static OutputStream maybeBufferOutputStream(final OutputStream os) {\n        return maybeBufferOutputStream(os, Defaults.BUFFER_SIZE);\n    }\n\n    /**\n     * @return If bufferSize > 0, wrap os in BufferedOutputStream, else return os itself.\n     */\n    public static OutputStream maybeBufferOutputStream(final OutputStream os, final int bufferSize) {\n        if (bufferSize > 0) return new BufferedOutputStream(os, bufferSize);\n        else return os;\n    }\n\n    public static SeekableStream maybeBufferedSeekableStream(final SeekableStream stream, final int bufferSize) {\n        return bufferSize > 0 ? new SeekableBufferedStream(stream, bufferSize) : stream; \n    }\n    \n    public static SeekableStream maybeBufferedSeekableStream(final SeekableStream stream) {\n        return maybeBufferedSeekableStream(stream, Defaults.BUFFER_SIZE);\n    }\n    \n    public static SeekableStream maybeBufferedSeekableStream(final File file) {\n        try {\n            return maybeBufferedSeekableStream(new SeekableFileStream(file));\n        } catch (final FileNotFoundException e) {\n            throw new RuntimeIOException(e);\n        }\n    }\n\n    public static SeekableStream maybeBufferedSeekableStream(final URL url) {\n        return maybeBufferedSeekableStream(new SeekableHTTPStream(url));\n    }\n\n    /**\n     * @return If Defaults.BUFFER_SIZE > 0, wrap is in BufferedInputStream, else return is itself.\n     */\n    public static InputStream maybeBufferInputStream(final InputStream is) {\n        return maybeBufferInputStream(is, Defaults.BUFFER_SIZE);\n    }\n\n    /**\n     * @return If bufferSize > 0, wrap is in BufferedInputStream, else return is itself.\n     */\n    public static InputStream maybeBufferInputStream(final InputStream is, final int bufferSize) {\n        if (bufferSize > 0) return new BufferedInputStream(is, bufferSize);\n        else return is;\n    }\n\n    public static Reader maybeBufferReader(Reader reader, final int bufferSize) {\n        if (bufferSize > 0) reader = new BufferedReader(reader, bufferSize);\n        return reader;\n    }\n\n    public static Reader maybeBufferReader(final Reader reader) {\n        return maybeBufferReader(reader, Defaults.BUFFER_SIZE);\n    }\n\n    public static Writer maybeBufferWriter(Writer writer, final int bufferSize) {\n        if (bufferSize > 0) writer = new BufferedWriter(writer, bufferSize);\n        return writer;\n    }\n\n    public static Writer maybeBufferWriter(final Writer writer) {\n        return maybeBufferWriter(writer, Defaults.BUFFER_SIZE);\n    }\n\n\n    /**\n     * Delete a list of files, and write a warning message if one could not be deleted.\n     *\n     * @param files Files to be deleted.\n     */\n    public static void deleteFiles(final File... files) {\n        for (final File f : files) {\n            if (!f.delete()) {\n                System.err.println(\"Could not delete file \" + f);\n            }\n        }\n    }\n\n    public static void deleteFiles(final Iterable<File> files) {\n        for (final File f : files) {\n            if (!f.delete()) {\n                System.err.println(\"Could not delete file \" + f);\n            }\n        }\n    }\n\n    public static void deletePaths(final Path... paths) {\n        for(Path path: paths){\n            deletePath(path);\n        }\n    }\n\n    /**\n     * Iterate through Paths and delete each one.\n     * Note: Path is itself an Iterable<Path>.  This method special cases that and deletes the single Path rather than\n     * Iterating the Path for targets to delete.\n     * @param paths an iterable of Paths to delete\n     */\n    public static void deletePaths(final Iterable<Path> paths) {\n        //Path is itself an Iterable<Path> which causes very confusing behavior if we don't explicitly check here.\n        if( paths instanceof Path){\n            deletePath((Path)paths);\n        }\n        paths.forEach(IOUtil::deletePath);\n    }\n\n    /**\n     * Attempt to delete a single path and log an error if it is not deleted.\n     */\n    public static void deletePath(Path path){\n        try {\n            Files.delete(path);\n        } catch (IOException e) {\n            System.err.println(\"Could not delete file \" + path);\n        }\n    }\n\n    /**\n     * @return true if the path is not a device (e.g. /dev/null or /dev/stdin), and is not\n     * an existing directory.  I.e. is is a regular path that may correspond to an existing\n     * file, or a path that could be a regular output file.\n     */\n    public static boolean isRegularPath(final File file) {\n        return !file.exists() || file.isFile();\n    }\n\n    /**\n     * @return true if the path is not a device (e.g. /dev/null or /dev/stdin), and is not\n     * an existing directory.  I.e. is is a regular path that may correspond to an existing\n     * file, or a path that could be a regular output file.\n     */\n    public static boolean isRegularPath(final Path path) {\n        return !Files.exists(path) || Files.isRegularFile(path);\n    }\n\n    /**\n     * Creates a new tmp file on one of the available temp filesystems, registers it for deletion\n     * on JVM exit and then returns it.\n     */\n    public static File newTempFile(final String prefix, final String suffix,\n                                   final File[] tmpDirs, final long minBytesFree) throws IOException {\n        File f = null;\n\n        for (int i = 0; i < tmpDirs.length; ++i) {\n            if (i == tmpDirs.length - 1 || tmpDirs[i].getUsableSpace() > minBytesFree) {\n                f = File.createTempFile(prefix, suffix, tmpDirs[i]);\n                f.deleteOnExit();\n                break;\n            }\n        }\n\n        return f;\n    }\n\n    /** Creates a new tmp file on one of the potential filesystems that has at least 5GB free. */\n    public static File newTempFile(final String prefix, final String suffix,\n                                   final File[] tmpDirs) throws IOException {\n        return newTempFile(prefix, suffix, tmpDirs, FIVE_GBS);\n    }\n\n    /** Returns a default tmp directory. */\n    public static File getDefaultTmpDir() {\n        final String user = System.getProperty(\"user.name\");\n        final String tmp = System.getProperty(\"java.io.tmpdir\");\n\n        if (tmp.endsWith(File.separatorChar + user)) return new File(tmp);\n        else return new File(tmp, user);\n    }\n\n    /**\n     * Creates a new tmp path on one of the available temp filesystems, registers it for deletion\n     * on JVM exit and then returns it.\n     */\n    public static Path newTempPath(final String prefix, final String suffix,\n            final Path[] tmpDirs, final long minBytesFree) throws IOException {\n        Path p = null;\n\n        for (int i = 0; i < tmpDirs.length; ++i) {\n            if (i == tmpDirs.length - 1 || Files.getFileStore(tmpDirs[i]).getUsableSpace() > minBytesFree) {\n                p = Files.createTempFile(tmpDirs[i], prefix, suffix);\n                deleteOnExit(p);\n                break;\n            }\n        }\n\n        return p;\n    }\n\n    /** Creates a new tmp file on one of the potential filesystems that has at least 5GB free. */\n    public static Path newTempPath(final String prefix, final String suffix,\n            final Path[] tmpDirs) throws IOException {\n        return newTempPath(prefix, suffix, tmpDirs, FIVE_GBS);\n    }\n\n    /** Returns a default tmp directory as a Path. */\n    public static Path getDefaultTmpDirPath() {\n        try {\n            final String user = System.getProperty(\"user.name\");\n            final String tmp = System.getProperty(\"java.io.tmpdir\");\n\n            final Path tmpParent = getPath(tmp);\n            if (tmpParent.endsWith(tmpParent.getFileSystem().getSeparator() + user)) {\n                return tmpParent;\n            } else {\n                return tmpParent.resolve(user);\n            }\n        } catch (IOException e) {\n            throw new RuntimeIOException(e);\n        }\n    }\n\n    /**\n     * Register a {@link Path} for deletion on JVM exit.\n     *\n     * @see DeleteOnExitPathHook\n     */\n    public static void deleteOnExit(final Path path) {\n        DeleteOnExitPathHook.add(path);\n    }\n\n    /** Returns the name of the file minus the extension (i.e. text after the last \".\" in the filename). */\n    public static String basename(final File f) {\n        final String full = f.getName();\n        final int index = full.lastIndexOf('.');\n        if (index > 0  && index > full.lastIndexOf(File.separator)) {\n            return full.substring(0, index);\n        }\n        else {\n            return full;\n        }\n    }\n    \n    /**\n     * Checks that an input is  is non-null, a URL or a file, exists, \n     * and if its a file then it is not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param input the input to check for validity\n     */\n    public static void assertInputIsValid(final String input) {\n      if (input == null) {\n        throw new IllegalArgumentException(\"Cannot check validity of null input.\");\n      }\n      if (!isUrl(input)) {\n        assertFileIsReadable(new File(input));\n      }\n    }\n    \n    /** \n     * Returns true iff the string is a url. \n     * Helps distinguish url inputs form file path inputs.\n     */\n    public static boolean isUrl(final String input) {\n      try {\n        new URL(input);\n        return true;\n      } catch (MalformedURLException e) {\n        return false;\n      }\n    }\n\n    /**\n     * Checks that a file is non-null, exists, is not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param file the file to check for readability\n     */\n    public static void assertFileIsReadable(final File file) {\n        assertFileIsReadable(toPath(file));\n    }\n\n    /**\n     * Checks that a file is non-null, exists, is not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param path the file to check for readability\n     */\n    public static void assertFileIsReadable(final Path path) {\n        if (path == null) {\n            throw new IllegalArgumentException(\"Cannot check readability of null file.\");\n        } else if (!Files.exists(path)) {\n            throw new SAMException(\"Cannot read non-existent file: \" + path.toUri().toString());\n        }\n        else if (Files.isDirectory(path)) {\n            throw new SAMException(\"Cannot read file because it is a directory: \" + path.toUri().toString());\n        }\n        else if (!Files.isReadable(path)) {\n            throw new SAMException(\"File exists but is not readable: \" + path.toUri().toString());\n        }\n    }\n\n    /**\n     * Checks that each file is non-null, exists, is not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param files the list of files to check for readability\n     */\n    public static void assertFilesAreReadable(final List<File> files) {\n        for (final File file : files) assertFileIsReadable(file);\n    }\n\n    /**\n     * Checks that each path is non-null, exists, is not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param paths the list of paths to check for readability\n     */\n    public static void assertPathsAreReadable(final List<Path> paths) {\n        for (final Path path: paths) assertFileIsReadable(path);\n    }\n\n\n    /**\n     * Checks that each string is non-null, exists or is a URL, \n     * and if it is a file then not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param inputs the list of files to check for readability\n     */\n    public static void assertInputsAreValid(final List<String> inputs) {\n        for (final String input : inputs) assertInputIsValid(input);\n    }\n\n    /**\n     * Checks that a file is non-null, and is either extent and writable, or non-existent but\n     * that the parent directory exists and is writable. If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param file the file to check for writability\n     */\n    public static void assertFileIsWritable(final File file) {\n        if (file == null) {\n            throw new IllegalArgumentException(\"Cannot check readability of null file.\");\n        } else if (!file.exists()) {\n            // If the file doesn't exist, check that it's parent directory does and is writable\n            final File parent = file.getAbsoluteFile().getParentFile();\n            if (!parent.exists()) {\n                throw new SAMException(\"Cannot write file: \" + file.getAbsolutePath() + \". \" +\n                        \"Neither file nor parent directory exist.\");\n            }\n            else if (!parent.isDirectory()) {\n                throw new SAMException(\"Cannot write file: \" + file.getAbsolutePath() + \". \" +\n                        \"File does not exist and parent is not a directory.\");\n            }\n            else if (!parent.canWrite()) {\n                throw new SAMException(\"Cannot write file: \" + file.getAbsolutePath() + \". \" +\n                        \"File does not exist and parent directory is not writable..\");\n            }\n        }\n        else if (file.isDirectory()) {\n            throw new SAMException(\"Cannot write file because it is a directory: \" + file.getAbsolutePath());\n        }\n        else if (!file.canWrite()) {\n            throw new SAMException(\"File exists but is not writable: \" + file.getAbsolutePath());\n        }\n    }\n\n    /**\n     * Checks that each file is non-null, and is either extent and writable, or non-existent but\n     * that the parent directory exists and is writable. If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param files the list of files to check for writability\n     */\n    public static void assertFilesAreWritable(final List<File> files) {\n        for (final File file : files) assertFileIsWritable(file);\n    }\n\n    /**\n     * Checks that a directory is non-null, extent, writable and a directory\n     * otherwise a runtime exception is thrown.\n     *\n     * @param dir the dir to check for writability\n     */\n    public static void assertDirectoryIsWritable(final File dir) {\n        final Path asPath = IOUtil.toPath(dir);\n        assertDirectoryIsWritable(asPath);\n    }\n\n    /**\n     * Checks that a directory is non-null, extent, writable and a directory\n     * otherwise a runtime exception is thrown.\n     *\n     * @param dir the dir to check for writability\n     */\n    public static void assertDirectoryIsWritable(final Path dir) {\n        if (dir == null) {\n            throw new IllegalArgumentException(\"Cannot check readability of null file.\");\n        }\n        else if (!Files.exists(dir)) {\n            throw new SAMException(\"Directory does not exist: \" + dir.toUri().toString());\n        }\n        else if (!Files.isDirectory(dir)) {\n            throw new SAMException(\"Cannot write to directory because it is not a directory: \" + dir.toUri().toString());\n        }\n        else if (!Files.isWritable(dir)) {\n            throw new SAMException(\"Directory exists but is not writable: \" + dir.toUri().toString());\n        }\n    }\n\n    /**\n     * Checks that a directory is non-null, extent, readable and a directory\n     * otherwise a runtime exception is thrown.\n     *\n     * @param dir the dir to check for writability\n     */\n    public static void assertDirectoryIsReadable(final File dir) {\n        if (dir == null) {\n            throw new IllegalArgumentException(\"Cannot check readability of null file.\");\n        }\n        else if (!dir.exists()) {\n            throw new SAMException(\"Directory does not exist: \" + dir.getAbsolutePath());\n        }\n        else if (!dir.isDirectory()) {\n            throw new SAMException(\"Cannot read from directory because it is not a directory: \" + dir.getAbsolutePath());\n        }\n        else if (!dir.canRead()) {\n            throw new SAMException(\"Directory exists but is not readable: \" + dir.getAbsolutePath());\n        }\n    }\n\n    /**\n     * Checks that the two files are the same length, and have the same content, otherwise throws a runtime exception.\n     */\n    public static void assertFilesEqual(final File f1, final File f2) {\n        if (f1.length() != f2.length()) {\n            throw new SAMException(\"File \" + f1 + \" is \" + f1.length() + \" bytes but file \" + f2 + \" is \" + f2.length() + \" bytes.\");\n        }\n        try (\n            final FileInputStream s1 = new FileInputStream(f1);\n            final FileInputStream s2 = new FileInputStream(f2);\n            ) {\n            final byte[] buf1 = new byte[1024 * 1024];\n            final byte[] buf2 = new byte[1024 * 1024];\n            int len1;\n            while ((len1 = s1.read(buf1)) != -1) {\n                final int len2 = s2.read(buf2);\n                if (len1 != len2) {\n                    throw new SAMException(\"Unexpected EOF comparing files that are supposed to be the same length.\");\n                }\n                if (!Arrays.equals(buf1, buf2)) {\n                    throw new SAMException(\"Files \" + f1 + \" and \" + f2 + \" differ.\");\n                }\n            }\n        } catch (final IOException e) {\n            throw new SAMException(\"Exception comparing files \" + f1 + \" and \" + f2, e);\n        }\n    }\n\n    /**\n     * Checks that a file is of non-zero length\n     */\n    public static void assertFileSizeNonZero(final File file) {\n        if (file.length() == 0) {\n            throw new SAMException(file.getAbsolutePath() + \" has length 0\");\n        }\n    }\n\n    /**\n     * Opens a file for reading, decompressing it if necessary\n     *\n     * @param file  The file to open\n     * @return the input stream to read from\n     */\n    public static InputStream openFileForReading(final File file) {\n        return openFileForReading(toPath(file));\n    }\n\n    /**\n     * Opens a file for reading, decompressing it if necessary\n     *\n     * @param path  The file to open\n     * @return the input stream to read from\n     */\n    public static InputStream openFileForReading(final Path path) {\n\n        try {\n            if (hasGzipFileExtension(path))  {\n                return openGzipFileForReading(path);\n            }\n            else {\n                return Files.newInputStream(path);\n            }\n        }\n        catch (IOException ioe) {\n            throw new SAMException(\"Error opening file: \" + path, ioe);\n        }\n\n    }\n\n    /**\n     * Opens a GZIP-encoded file for reading, decompressing it if necessary\n     *\n     * @param file  The file to open\n     * @return the input stream to read from\n     */\n    public static InputStream openGzipFileForReading(final File file) {\n        return openGzipFileForReading(toPath(file));\n    }\n\n    /**\n     * Opens a GZIP-encoded file for reading, decompressing it if necessary\n     *\n     * @param path  The file to open\n     * @return the input stream to read from\n     */\n    public static InputStream openGzipFileForReading(final Path path) {\n\n        try {\n            return new GZIPInputStream(Files.newInputStream(path));\n        }\n        catch (IOException ioe) {\n            throw new SAMException(\"Error opening file: \" + path, ioe);\n        }\n    }\n\n    /**\n     * Opens a file for writing, overwriting the file if it already exists\n     *\n     * @param file  the file to write to\n     * @return the output stream to write to\n     */\n    public static OutputStream openFileForWriting(final File file) {\n        return openFileForWriting(toPath(file));\n    }\n\n    /**\n     * Opens a file for writing, gzip it if it ends with \".gz\" or \"bfq\"\n     *\n     * @param file  the file to write to\n     * @param append    whether to append to the file if it already exists (we overwrite it if false)\n     * @return the output stream to write to\n     */\n    public static OutputStream openFileForWriting(final File file, final boolean append) {\n        return openFileForWriting(toPath(file), getAppendOpenOption(append));\n    }\n\n    /**\n     * Opens a file for writing, gzip it if it ends with \".gz\" or \"bfq\"\n     *\n     * @param path  the file to write to\n     * @param openOptions options to use when opening the file\n     * @return the output stream to write to\n     */\n    public static OutputStream openFileForWriting(final Path path, OpenOption... openOptions) {\n        try {\n            if (hasGzipFileExtension(path)) {\n                return openGzipFileForWriting(path, openOptions);\n            } else {\n                return Files.newOutputStream(path, openOptions);\n            }\n        } catch (final IOException ioe) {\n            throw new SAMException(\"Error opening file for writing: \" + path.toUri().toString(), ioe);\n        }\n    }\n\n    /**\n     * check if the file name ends with .gz, .gzip, or .bfq\n     */\n    public static boolean hasGzipFileExtension(Path path) {\n        final List<String> gzippedEndings = Arrays.asList(\".gz\", \".gzip\", \".bfq\");\n        final String fileName = path.getFileName().toString();\n        return gzippedEndings.stream().anyMatch(fileName::endsWith);\n    }\n\n    /**\n     * Preferred over PrintStream and PrintWriter because an exception is thrown on I/O error\n     */\n    public static BufferedWriter openFileForBufferedWriting(final File file, final boolean append) {\n        return new BufferedWriter(new OutputStreamWriter(openFileForWriting(file, append)), Defaults.NON_ZERO_BUFFER_SIZE);\n    }\n\n    /**\n     * Preferred over PrintStream and PrintWriter because an exception is thrown on I/O error\n     */\n    public static BufferedWriter openFileForBufferedWriting(final Path path, final OpenOption ... openOptions) {\n        return new BufferedWriter(new OutputStreamWriter(openFileForWriting(path, openOptions)), Defaults.NON_ZERO_BUFFER_SIZE);\n    }\n\n    /**\n     * Preferred over PrintStream and PrintWriter because an exception is thrown on I/O error\n     */\n    public static BufferedWriter openFileForBufferedWriting(final File file) {\n        return openFileForBufferedWriting(IOUtil.toPath(file));\n    }\n\n    /**\n     * Preferred over PrintStream and PrintWriter because an exception is thrown on I/O error\n     */\n    public static BufferedWriter openFileForBufferedUtf8Writing(final File file) {\n        return openFileForBufferedUtf8Writing(IOUtil.toPath(file));\n    }\n\n    /**\n     * Preferred over PrintStream and PrintWriter because an exception is thrown on I/O error\n     */\n    public static BufferedWriter openFileForBufferedUtf8Writing(final Path path) {\n        return new BufferedWriter(new OutputStreamWriter(openFileForWriting(path), Charset.forName(\"UTF-8\")), Defaults.NON_ZERO_BUFFER_SIZE);\n    }\n\n    /**\n     * Opens a file for reading, decompressing it if necessary\n     *\n     * @param file  The file to open\n     * @return the input stream to read from\n     */\n    public static BufferedReader openFileForBufferedUtf8Reading(final File file) {\n        return new BufferedReader(new InputStreamReader(openFileForReading(file), Charset.forName(\"UTF-8\")));\n    }\n\n    /**\n     * Opens a GZIP encoded file for writing\n     *\n     * @param file  the file to write to\n     * @param append    whether to append to the file if it already exists (we overwrite it if false)\n     * @return the output stream to write to\n     */\n    public static OutputStream openGzipFileForWriting(final File file, final boolean append) {\n        return openGzipFileForWriting(IOUtil.toPath(file), getAppendOpenOption(append));\n    }\n\n    /**\n     * converts a boolean into an array containing either the append option or nothing\n     */\n    private static OpenOption[] getAppendOpenOption(boolean append) {\n        return append ? new OpenOption[]{StandardOpenOption.APPEND} : EMPTY_OPEN_OPTIONS;\n    }\n\n    /**\n     * Opens a GZIP encoded file for writing\n     *\n     * @param path the file to write to\n     * @param openOptions options to control how the file is opened\n     * @return the output stream to write to\n     */\n    public static OutputStream openGzipFileForWriting(final Path path, final OpenOption ... openOptions) {\n        try {\n            final OutputStream out = Files.newOutputStream(path, openOptions);\n            if (Defaults.BUFFER_SIZE > 0) {\n                return new CustomGzipOutputStream(out, Defaults.BUFFER_SIZE, compressionLevel);\n            } else {\n                return new CustomGzipOutputStream(out, compressionLevel);\n            }\n        } catch (final IOException ioe) {\n            throw new SAMException(\"Error opening file for writing: \" + path.toUri().toString(), ioe);\n        }\n    }\n\n    public static OutputStream openFileForMd5CalculatingWriting(final File file) {\n        return openFileForMd5CalculatingWriting(toPath(file));\n    }\n\n    public static OutputStream openFileForMd5CalculatingWriting(final Path file) {\n        return new Md5CalculatingOutputStream(IOUtil.openFileForWriting(file), file.resolve(\".md5\"));\n    }\n\n    /**\n     * Utility method to copy the contents of input to output. The caller is responsible for\n     * opening and closing both streams.\n     *\n     * @param input contents to be copied\n     * @param output destination\n     */\n    public static void copyStream(final InputStream input, final OutputStream output) {\n        try {\n            final byte[] buffer = new byte[Defaults.NON_ZERO_BUFFER_SIZE];\n            int bytesRead = 0;\n            while((bytesRead = input.read(buffer)) > 0) {\n                output.write(buffer, 0, bytesRead);\n            }\n        } catch (IOException e) {\n            throw new SAMException(\"Exception copying stream\", e);\n        }\n    }\n\n    /**\n     * Copy input to output, overwriting output if it already exists.\n     */\n    public static void copyFile(final File input, final File output) {\n        try {\n            final InputStream is = new FileInputStream(input);\n            final OutputStream os = new FileOutputStream(output);\n            copyStream(is, os);\n            os.close();\n            is.close();\n        } catch (IOException e) {\n            throw new SAMException(\"Error copying \" + input + \" to \" + output, e);\n        }\n    }\n\n    /**\n     *\n     * @param directory\n     * @param regexp\n     * @return list of files matching regexp.\n     */\n    public static File[] getFilesMatchingRegexp(final File directory, final String regexp) {\n        final Pattern pattern = Pattern.compile(regexp);\n        return getFilesMatchingRegexp(directory, pattern);\n    }\n\n    public static File[] getFilesMatchingRegexp(final File directory, final Pattern regexp) {\n        return directory.listFiles( new FilenameFilter() {\n            @Override\n            public boolean accept(final File dir, final String name) {\n                return regexp.matcher(name).matches();\n            }\n        });\n    }\n\n    /**\n     * Delete the given file or directory.  If a directory, all enclosing files and subdirs are also deleted.\n     */\n    public static boolean deleteDirectoryTree(final File fileOrDirectory) {\n        boolean success = true;\n\n        if (fileOrDirectory.isDirectory()) {\n            for (final File child : fileOrDirectory.listFiles()) {\n                success = success && deleteDirectoryTree(child);\n            }\n        }\n\n        success = success && fileOrDirectory.delete();\n        return success;\n    }\n\n    /**\n     * Returns the size (in bytes) of the file or directory and all it's children.\n     */\n    public static long sizeOfTree(final File fileOrDirectory) {\n        long total = fileOrDirectory.length();\n        if (fileOrDirectory.isDirectory()) {\n            for (final File f : fileOrDirectory.listFiles()) {\n                total += sizeOfTree(f);\n            }\n        }\n\n        return total;\n    }\n\n    /**\n     *\n     * Copies a directory tree (all subdirectories and files) recursively to a destination\n     */\n    public static void copyDirectoryTree(final File fileOrDirectory, final File destination) {\n        if (fileOrDirectory.isDirectory()) {\n            destination.mkdir();\n            for(final File f : fileOrDirectory.listFiles()) {\n                final File destinationFileOrDirectory =  new File(destination.getPath(),f.getName());\n                if (f.isDirectory()){\n                    copyDirectoryTree(f,destinationFileOrDirectory);\n                }\n                else {\n                    copyFile(f,destinationFileOrDirectory);\n                }\n            }\n        }\n    }\n\n    /**\n     * Create a temporary subdirectory in the default temporary-file directory, using the given prefix and suffix to generate the name.\n     * Note that this method is not completely safe, because it create a temporary file, deletes it, and then creates\n     * a directory with the same name as the file.  Should be good enough.\n     *\n     * @param prefix The prefix string to be used in generating the file's name; must be at least three characters long\n     * @param suffix The suffix string to be used in generating the file's name; may be null, in which case the suffix \".tmp\" will be used\n     * @return File object for new directory\n     */\n    public static File createTempDir(final String prefix, final String suffix) {\n        try {\n            final File tmp = File.createTempFile(prefix, suffix);\n            if (!tmp.delete()) {\n                throw new SAMException(\"Could not delete temporary file \" + tmp);\n            }\n            if (!tmp.mkdir()) {\n                throw new SAMException(\"Could not create temporary directory \" + tmp);\n            }\n            return tmp;\n        } catch (IOException e) {\n            throw new SAMException(\"Exception creating temporary directory.\", e);\n        }\n    }\n\n    /** Checks that a file exists and is readable, and then returns a buffered reader for it. */\n    public static BufferedReader openFileForBufferedReading(final File file) {\n        return openFileForBufferedReading(toPath(file));\n    }\n\n    /** Checks that a path exists and is readable, and then returns a buffered reader for it. */\n    public static BufferedReader openFileForBufferedReading(final Path path) {\n        return new BufferedReader(new InputStreamReader(openFileForReading(path)), Defaults.NON_ZERO_BUFFER_SIZE);\n    }\n\n    /** Takes a string and replaces any characters that are not safe for filenames with an underscore */\n    public static String makeFileNameSafe(final String str) {\n        return str.trim().replaceAll(\"[\\\\s!\\\"#$%&'()*/:;<=>?@\\\\[\\\\]\\\\\\\\^`{|}~]\", \"_\");\n    }\n\n    /** Returns the name of the file extension (i.e. text after the last \".\" in the filename) including the . */\n    public static String fileSuffix(final File f) {\n        final String full = f.getName();\n        final int index = full.lastIndexOf('.');\n        if (index > 0 && index > full.lastIndexOf(File.separator)) {\n            return full.substring(index);\n        } else {\n            return null;\n        }\n    }\n\n    /** Returns the full path to the file with all symbolic links resolved **/\n    public static String getFullCanonicalPath(final File file) {\n        try {\n            File f = file.getCanonicalFile();\n            String canonicalPath = \"\";\n            while (f != null  && !f.getName().equals(\"\")) {\n                canonicalPath = \"/\" + f.getName() + canonicalPath;\n                f = f.getParentFile();\n                if (f != null) f = f.getCanonicalFile();\n            }\n            return canonicalPath;\n        } catch (final IOException ioe) {\n            throw new RuntimeIOException(\"Error getting full canonical path for \" +\n                    file + \": \" + ioe.getMessage(), ioe);\n        }\n   }\n\n    /**\n     * Reads everything from an input stream as characters and returns a single String.\n     */\n    public static String readFully(final InputStream in) {\n        try {\n            final BufferedReader r = new BufferedReader(new InputStreamReader(in), Defaults.NON_ZERO_BUFFER_SIZE);\n            final StringBuilder builder = new StringBuilder(512);\n            String line = null;\n\n            while ((line = r.readLine()) != null) {\n                if (builder.length() > 0) builder.append('\\n');\n                builder.append(line);\n            }\n\n            return builder.toString();\n        }\n        catch (final IOException ioe) {\n            throw new RuntimeIOException(\"Error reading stream\", ioe);\n        }\n    }\n\n    /**\n     * Returns an iterator over the lines in a text file. The underlying resources are automatically\n     * closed when the iterator hits the end of the input, or manually by calling close().\n     *\n     * @param f a file that is to be read in as text\n     * @return an iterator over the lines in the text file\n     */\n    public static IterableOnceIterator<String> readLines(final File f) {\n        try {\n            final BufferedReader in = IOUtil.openFileForBufferedReading(f);\n\n            return new IterableOnceIterator<String>() {\n                private String next = in.readLine();\n\n                /** Returns true if there is another line to read or false otherwise. */\n                @Override public boolean hasNext() { return next != null; }\n\n                /** Returns the next line in the file or null if there are no more lines. */\n                @Override public String next() {\n                    try {\n                        final String tmp = next;\n                        next = in.readLine();\n                        if (next == null) in.close();\n                        return tmp;\n                    }\n                    catch (final IOException ioe) { throw new RuntimeIOException(ioe); }\n                }\n\n                /** Closes the underlying input stream. Not required if end of stream has already been hit. */\n                @Override public void close() throws IOException { CloserUtil.close(in); }\n            };\n        }\n        catch (final IOException e) {\n            throw new RuntimeIOException(e);\n        }\n    }\n\n    /** Returns all of the untrimmed lines in the provided file. */\n    public static List<String> slurpLines(final File file) throws FileNotFoundException {\n        return slurpLines(new FileInputStream(file));\n    }\n\n    public static List<String> slurpLines(final InputStream is) throws FileNotFoundException {\n        /** See {@link java.util.Scanner} source for origin of delimiter used here.  */\n        return tokenSlurp(is, Charset.defaultCharset(), \"\\r\\n|[\\n\\r\\u2028\\u2029\\u0085]\");\n    }\n\n    /** Convenience overload for {@link #slurp(java.io.InputStream, java.nio.charset.Charset)} using the default charset {@link java.nio.charset.Charset#defaultCharset()}. */\n    public static String slurp(final File file) throws FileNotFoundException {\n        return slurp(new FileInputStream(file));\n    }\n\n    /** Convenience overload for {@link #slurp(java.io.InputStream, java.nio.charset.Charset)} using the default charset {@link java.nio.charset.Charset#defaultCharset()}. */\n    public static String slurp(final InputStream is) {\n        return slurp(is, Charset.defaultCharset());\n    }\n\n    /** Reads all of the stream into a String, decoding with the provided {@link java.nio.charset.Charset} then closes the stream quietly. */\n    public static String slurp(final InputStream is, final Charset charSet) {\n        final List<String> tokenOrEmpty = tokenSlurp(is, charSet, \"\\\\A\");\n        return tokenOrEmpty.isEmpty() ? StringUtil.EMPTY_STRING : CollectionUtil.getSoleElement(tokenOrEmpty);\n    }\n\n    /** Tokenizes the provided input stream into memory using the given delimiter. */\n    private static List<String> tokenSlurp(final InputStream is, final Charset charSet, final String delimiterPattern) {\n        try {\n            final Scanner s = new Scanner(is, charSet.toString()).useDelimiter(delimiterPattern);\n            final LinkedList<String> tokens = new LinkedList<>();\n            while (s.hasNext()) {\n                tokens.add(s.next());\n            }\n            return tokens;\n        } finally {\n            CloserUtil.close(is);\n        }\n    }\n\n    /**\n     * Go through the files provided and if they have one of the provided file extensions pass the file into the output\n     * otherwise assume that file is a list of filenames and unfold it into the output.\n     */\n    public static List<File> unrollFiles(final Collection<File> inputs, final String... extensions) {\n        Collection<Path> paths = unrollPaths(filesToPaths(inputs), extensions);\n        return paths.stream().map(Path::toFile).collect(Collectors.toList());\n    }\n\n    /**\n     * Go through the files provided and if they have one of the provided file extensions pass the file to the output\n     * otherwise assume that file is a list of filenames and unfold it into the output (recursively).\n     */\n    public static List<Path> unrollPaths(final Collection<Path> inputs, final String... extensions) {\n        if (extensions.length < 1) throw new IllegalArgumentException(\"Must provide at least one extension.\");\n\n        final Stack<Path> stack = new Stack<>();\n        final List<Path> output = new ArrayList<>();\n        stack.addAll(inputs);\n\n        while (!stack.empty()) {\n            final Path p = stack.pop();\n            final String name = p.toString();\n            boolean matched = false;\n\n            for (final String ext : extensions) {\n                if (!matched && name.endsWith(ext)) {\n                    output.add(p);\n                    matched = true;\n                }\n            }\n\n            // If the file didn't match a given extension, treat it as a list of files\n            if (!matched) {\n                try {\n                    Files.lines(p)\n                            .map(String::trim)\n                            .filter(s -> !s.isEmpty())\n                            .forEach(s -> {\n                                        final Path innerPath;\n                                        try {\n                                            innerPath = getPath(s);\n                                            stack.push(innerPath);\n                                        } catch (IOException e) {\n                                            throw new IllegalArgumentException(\"cannot convert \" + s + \" to a Path.\", e);\n                                        }\n                                    }\n                            );\n\n                } catch (IOException e) {\n                    throw new IllegalArgumentException(\"had trouble reading from \" + p.toUri().toString(), e);\n                }\n            }\n        }\n\n        // Preserve input order (since we're using a stack above) for things that care\n        Collections.reverse(output);\n\n        return output;\n    }\n\n\n    /**\n     * Check if the given URI has a scheme.\n     *\n     * @param uriString the URI to check\n     * @return <code>true</code> if the given URI has a scheme, <code>false</code> if\n     * not, or if the URI is malformed.\n     */\n    public static boolean hasScheme(String uriString) {\n        try {\n            return new URI(uriString).getScheme() != null;\n        } catch (URISyntaxException e) {\n            return false;\n        }\n    }\n\n    /**\n     * Converts the given URI to a {@link Path} object. If the filesystem cannot be found in the usual way, then attempt\n     * to load the filesystem provider using the thread context classloader. This is needed when the filesystem\n     * provider is loaded using a URL classloader (e.g. in spark-submit).\n     *\n     * @param uriString the URI to convert\n     * @return the resulting {@code Path}\n     * @throws IOException an I/O error occurs creating the file system\n     */\n    public static Path getPath(String uriString) throws IOException {\n        URI uri = URI.create(uriString);\n        try {\n            // if the URI has no scheme, then treat as a local file, otherwise use the scheme to determine the filesystem to use\n            return uri.getScheme() == null ? Paths.get(uriString) : Paths.get(uri);\n        } catch (FileSystemNotFoundException e) {\n            ClassLoader cl = Thread.currentThread().getContextClassLoader();\n            if (cl == null) {\n                throw e;\n            }\n            return FileSystems.newFileSystem(uri, new HashMap<>(), cl).provider().getPath(uri);\n        }\n    }\n\n    public static List<Path> getPaths(List<String> uriStrings) throws RuntimeIOException {\n        return uriStrings.stream().map(s -> {\n            try {\n                return IOUtil.getPath(s);\n            } catch (IOException e) {\n                throw new RuntimeIOException(e);\n            }\n        }).collect(Collectors.toList());\n    }\n\n    /*\n     * Converts the File to a Path, preserving nullness.\n     *\n     * @param fileOrNull a File, or null\n     * @return           the corresponding Path (or null)\n     */\n    public static Path toPath(File fileOrNull) {\n        return (null == fileOrNull ? null : fileOrNull.toPath());\n    }\n\n    /** Takes a list of Files and converts them to a list of Paths\n     * Runs .toPath() on the contents of the input.\n     *\n     * @param files a {@link List} of {@link File}s to convert to {@link Path}s\n     * @return a new List containing the results of running toPath on the elements of the input\n     */\n    public static List<Path> filesToPaths(Collection<File> files){\n        return files.stream().map(File::toPath).collect(Collectors.toList());\n    }\n\n    /**\n     * Test whether a input stream looks like a GZIP input.\n     * This identifies both gzip and bgzip streams as being GZIP.\n     * @param stream the input stream.\n     * @return true if `stream` starts with a gzip signature.\n     * @throws IllegalArgumentException if `stream` cannot mark or reset the stream\n     */\n    public static boolean isGZIPInputStream(final InputStream stream) {\n        if (!stream.markSupported()) {\n            throw new IllegalArgumentException(\"isGZIPInputStream() : Cannot test a stream that doesn't support marking.\");\n        }\n        stream.mark(GZIP_HEADER_READ_LENGTH);\n\n        try {\n            final GZIPInputStream gunzip = new GZIPInputStream(stream);\n            final int ch = gunzip.read();\n            return true;\n        } catch (final IOException ioe) {\n            return false;\n        } finally {\n            try {\n                stream.reset();\n            } catch (final IOException ioe) {\n                throw new IllegalStateException(\"isGZIPInputStream(): Could not reset stream.\");\n            }\n        }\n    }\n\n    /**\n     * Adds the extension to the given path.\n     *\n     * @param path       the path to start from, eg. \"/folder/file.jpg\"\n     * @param extension  the extension to add, eg. \".bak\"\n     * @return           \"/folder/file.jpg.bak\"\n     */\n    public static Path addExtension(Path path, String extension) {\n        return path.resolveSibling(path.getFileName() + extension);\n    }\n\n    /**\n     * Checks if the provided path is block-compressed.\n     *\n     * <p>Note that using {@code checkExtension=true} would avoid the cost of opening the file, but\n     * if {@link #hasBlockCompressedExtension(String)} returns {@code false} this would not detect\n     * block-compressed files such BAM.\n     *\n     * @param path file to check if it is block-compressed.\n     * @param checkExtension if {@code true}, checks the extension before opening the file.\n     * @return {@code true} if the file is block-compressed; {@code false} otherwise.\n     * @throws IOException if there is an I/O error.\n     */\n    public static boolean isBlockCompressed(final Path path, final boolean checkExtension) throws IOException {\n        if (checkExtension && !hasBlockCompressedExtension(path)) {\n            return false;\n        }\n        try (final InputStream stream = new BufferedInputStream(Files.newInputStream(path), Math.max(Defaults.BUFFER_SIZE, BlockCompressedStreamConstants.MAX_COMPRESSED_BLOCK_SIZE))) {\n            return BlockCompressedInputStream.isValidFile(stream);\n        }\n    }\n\n    /**\n     * Checks if the provided path is block-compressed (including extension).\n     *\n     * <p>Note that block-compressed file extensions {@link FileExtensions#BLOCK_COMPRESSED} are not\n     * checked by this method.\n     *\n     * @param path file to check if it is block-compressed.\n     * @return {@code true} if the file is block-compressed; {@code false} otherwise.\n     * @throws IOException if there is an I/O error.\n     */\n    public static boolean isBlockCompressed(final Path path) throws IOException {\n        return isBlockCompressed(path, false);\n    }\n\n    /**\n     * Checks if a file ends in one of the {@link FileExtensions#BLOCK_COMPRESSED}.\n     *\n     * @param fileName string name for the file. May be an HTTP/S url.\n     *\n     * @return {@code true} if the file has a block-compressed extension; {@code false} otherwise.\n     */\n    public static boolean hasBlockCompressedExtension (final String fileName) {\n        String cleanedPath = stripQueryStringIfPathIsAnHttpUrl(fileName);\n        for (final String extension : FileExtensions.BLOCK_COMPRESSED) {\n            if (cleanedPath.toLowerCase().endsWith(extension))\n                return true;\n        }\n        return false;\n    }\n\n    /**\n     * Checks if a path ends in one of the {@link FileExtensions#BLOCK_COMPRESSED}.\n     *\n     * @param path object to extract the name from.\n     *\n     * @return {@code true} if the path has a block-compressed extension; {@code false} otherwise.\n     */\n    public static boolean hasBlockCompressedExtension(final Path path) {\n        return hasBlockCompressedExtension(path.getFileName().toString());\n    }\n\n    /**\n     * Checks if a file ends in one of the {@link FileExtensions#BLOCK_COMPRESSED}.\n     *\n     * @param file object to extract the name from.\n     *\n     * @return {@code true} if the file has a block-compressed extension; {@code false} otherwise.\n     */\n    public static boolean hasBlockCompressedExtension (final File file) {\n        return hasBlockCompressedExtension(file.getName());\n    }\n\n    /**\n     * Checks if a file ends in one of the {@link FileExtensions#BLOCK_COMPRESSED}.\n     *\n     * @param uri file as an URI.\n     *\n     * @return {@code true} if the file has a block-compressed extension; {@code false} otherwise.\n     */\n    public static boolean hasBlockCompressedExtension (final URI uri) {\n        String path = uri.getPath();\n        return hasBlockCompressedExtension(path);\n    }\n\n    /**\n     * Remove http query before checking extension\n     * Path might be a local file, in which case a '?' is a legal part of the filename.\n     * @param path a string representing some sort of path, potentially an http url\n     * @return path with no trailing queryString (ex: http://something.com/path.vcf?stuff=something => http://something.com/path.vcf)\n     */\n    private static String stripQueryStringIfPathIsAnHttpUrl(String path) {\n        if(path.startsWith(\"http://\") || path.startsWith(\"https://\")) {\n            int qIdx = path.indexOf('?');\n            if (qIdx > 0) {\n                return path.substring(0, qIdx);\n            }\n        }\n        return path;\n    }\n\n    /**\n     * Delete a directory and all files in it.\n     *\n     * @param directory The directory to be deleted (along with its subdirectories)\n     */\n    public static void recursiveDelete(final Path directory) {\n        \n        final SimpleFileVisitor<Path> simpleFileVisitor = new SimpleFileVisitor<Path>() {\n            @Override\n            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {\n                super.visitFile(file, attrs);\n                Files.deleteIfExists(file);\n                return FileVisitResult.CONTINUE;\n            }\n\n            @Override\n            public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException {\n                super.postVisitDirectory(dir, exc);\n                Files.deleteIfExists(dir);\n                return FileVisitResult.CONTINUE;\n            }\n        };\n\n        try {\n            Files.walkFileTree(directory, simpleFileVisitor);\n        } catch (final IOException e){\n            throw new RuntimeIOException(e);\n        }\n    }\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2019 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.cram.io.InputStreamUtils;\nimport htsjdk.samtools.seekablestream.ByteArraySeekableStream;\nimport htsjdk.samtools.seekablestream.SeekableStream;\nimport htsjdk.samtools.util.BlockCompressedStreamConstants;\nimport htsjdk.samtools.util.FileExtensions;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.ProgressLoggerInterface;\nimport htsjdk.samtools.util.RuntimeIOException;\nimport htsjdk.utils.ValidationUtils;\nimport org.testng.Assert;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class BAMMergerTest extends HtsjdkTest {\n\n    private final static Path BAM_FILE = new File(\"src/test/resources/htsjdk/samtools/BAMFileIndexTest/index_test.bam\").toPath();\n\n    /**\n     * Writes a <i>partitioned BAM</i>.\n     *\n     * Note that this writer is only for single-threaded use. Consider using the implementation in Disq for a partitioned BAM writer\n     * that works with multiple threads or in a distributed setting.\n     *\n     * @see BAMIndexMerger\n     */\n    static class PartitionedBAMFileWriter implements SAMFileWriter {\n        private final Path outputDir;\n        private final SAMFileHeader header;\n        private int recordsPerPart;\n        private long recordCount = 0;\n        private int partNumber = -1;\n        private BAMStreamWriter samStreamWriter;\n        private ProgressLoggerInterface progressLogger;\n\n        public PartitionedBAMFileWriter(Path outputDir, SAMFileHeader header, int recordsPerPart) {\n            this.outputDir = outputDir;\n            this.header = header;\n            this.recordsPerPart = recordsPerPart;\n        }\n\n        @Override\n        public void addAlignment(SAMRecord alignment) {\n            if (recordCount == 0) {\n                // write header\n                try (OutputStream out = Files.newOutputStream(outputDir.resolve(\"header\"))) {\n                    new BAMStreamWriter(out, null, null, -1, header).writeHeader(header);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n            if (recordCount % recordsPerPart == 0) {\n                // start a new part\n                try {\n                    if (samStreamWriter != null) {\n                        samStreamWriter.finish(false);\n                    }\n                    partNumber++;\n                    String partName = String.format(\"part-%05d\", partNumber);\n                    OutputStream out = Files.newOutputStream(outputDir.resolve(partName));\n                    OutputStream indexOut = Files.newOutputStream(outputDir.resolve(\".\" + partName + FileExtensions.BAI_INDEX));\n                    OutputStream sbiOut = Files.newOutputStream(outputDir.resolve(\".\" + partName + FileExtensions.SBI));\n                    long sbiGranularity = 1; // set to one so we can test merging\n                    samStreamWriter = new BAMStreamWriter(out, indexOut, sbiOut, sbiGranularity, header);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n            recordCount++;\n            samStreamWriter.writeAlignment(alignment);\n            if (progressLogger != null) {\n                progressLogger.record(alignment);\n            }\n        }\n\n        @Override\n        public SAMFileHeader getFileHeader() {\n            return header;\n        }\n\n        @Override\n        public void setProgressLogger(ProgressLoggerInterface progressLogger) {\n            this.progressLogger = progressLogger;\n        }\n\n        @Override\n        public void close() {\n            if (samStreamWriter != null) {\n                samStreamWriter.finish(false);\n            }\n            // write terminator\n            try (OutputStream out = Files.newOutputStream(outputDir.resolve(\"terminator\"))) {\n                out.write(BlockCompressedStreamConstants.EMPTY_GZIP_BLOCK);\n            } catch (IOException e) {\n                throw new RuntimeIOException(e);\n            }\n        }\n    }\n\n    /**\n     * Merge the files created by {@link PartitionedBAMFileWriter} into a single BAM file and index.\n     */\n    static class PartitionedBAMFileMerger {\n        public void merge(Path dir, Path outputBam, Path outputBai, Path outputSbi) throws IOException {\n            Path headerPath = dir.resolve(\"header\");\n            List<Path> bamParts = Files.list(dir)\n                    .filter(path -> !path.toString().endsWith(FileExtensions.BAI_INDEX) && !path.toString().endsWith(FileExtensions.SBI)) // include header and terminator\n                    .sorted()\n                    .collect(Collectors.toList());\n            List<Path> baiParts = Files.list(dir)\n                    .filter(path -> path.toString().endsWith(FileExtensions.BAI_INDEX))\n                    .sorted()\n                    .collect(Collectors.toList());\n            List<Path> sbiParts = Files.list(dir)\n                    .filter(path -> path.toString().endsWith(FileExtensions.SBI))\n                    .sorted()\n                    .collect(Collectors.toList());\n\n            Assert.assertTrue(baiParts.size() > 1);\n\n            ValidationUtils.validateArg(bamParts.size() - 2 == baiParts.size(), \"Number of BAM part files does not match number of BAI files (\" + baiParts.size() + \")\");\n\n            SAMFileHeader header = SamReaderFactory.makeDefault().open(headerPath).getFileHeader();\n\n            // merge BAM parts\n            try (OutputStream out = Files.newOutputStream(outputBam)) {\n                for (Path bamPart : bamParts) {\n                    Files.copy(bamPart, out);\n                }\n            }\n\n            // merge index parts\n            try (OutputStream out = Files.newOutputStream(outputBai)) {\n                BAMIndexMerger bamIndexMerger = new BAMIndexMerger(out, Files.size(headerPath));\n                int i = 1; // start from 1 since we ignore the header\n                for (Path baiPart : baiParts) {\n                    try (InputStream in = Files.newInputStream(baiPart)) {\n                        // read all bytes into memory since AbstractBAMFileIndex reads lazily\n                        byte[] bytes = InputStreamUtils.readFully(in);\n                        SeekableStream allIn = new ByteArraySeekableStream(bytes);\n                        AbstractBAMFileIndex index = BAMIndexMerger.openIndex(allIn, header.getSequenceDictionary());\n                        bamIndexMerger.processIndex(index, Files.size(bamParts.get(i++)));\n                    }\n                }\n                bamIndexMerger.finish(Files.size(outputBam));\n            }\n\n            // merge SBI index parts\n            try (OutputStream out = Files.newOutputStream(outputSbi)) {\n                SBIIndexMerger sbiIndexMerger = new SBIIndexMerger(out, Files.size(headerPath));\n                int i = 1; // start from 1 since we ignore the header\n                for (Path sbiPart : sbiParts) {\n                    try (InputStream in = Files.newInputStream(sbiPart)) {\n                        SBIIndex index = SBIIndex.load(in);\n                        sbiIndexMerger.processIndex(index, Files.size(bamParts.get(i++)));\n                    }\n                }\n                sbiIndexMerger.finish(Files.size(outputBam));\n            }\n        }\n    }\n\n    // index a BAM file\n    private static Path indexBam(Path bam, Path bai) throws IOException {\n        try (SamReader in =\n                     SamReaderFactory.makeDefault()\n                             .validationStringency(ValidationStringency.SILENT)\n                             .enable(SamReaderFactory.Option.INCLUDE_SOURCE_IN_RECORDS)\n                             .disable(SamReaderFactory.Option.VALIDATE_CRC_CHECKSUMS)\n                             .open(SamInputResource.of(bam))) {\n\n            final BAMIndexer indexer = new BAMIndexer(bai, in.getFileHeader());\n            for (final SAMRecord rec : in) {\n                indexer.processAlignment(rec);\n            }\n            indexer.finish();\n        }\n        BAMSBIIndexer.createIndex(bam, 1);\n        textIndexBai(bai);\n        return bai;\n    }\n\n    // create a human-readable BAI\n    private static Path textIndexBai(Path bai) {\n        Path textBai = bai.resolveSibling(bai.getFileName().toString() + \".txt\");\n        BAMIndexer.createAndWriteIndex(bai.toFile(), textBai.toFile(), true);\n        return textBai;\n    }\n\n    @Test\n    public void test() throws IOException {\n        final Path outputDir = IOUtil.createTempDir(this.getClass().getSimpleName() + \".\", \".tmp\").toPath();\n        IOUtil.deleteOnExit(outputDir);\n\n        final Path outputBam = File.createTempFile(this.getClass().getSimpleName() + \".\", \".bam\").toPath();\n        IOUtil.deleteOnExit(outputBam);\n\n        final Path outputBai = IOUtil.addExtension(outputBam, FileExtensions.BAI_INDEX);\n        IOUtil.deleteOnExit(outputBai);\n\n        final Path outputSbi = IOUtil.addExtension(outputBam, FileExtensions.SBI);\n        IOUtil.deleteOnExit(outputSbi);\n\n        final Path outputBaiMerged = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.BAI_INDEX).toPath();\n        IOUtil.deleteOnExit(outputBaiMerged);\n\n        final Path outputSbiMerged = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.SBI).toPath();\n        IOUtil.deleteOnExit(outputBaiMerged);\n\n        // 1. Read an input BAM and write it out in partitioned form (header, parts, terminator)\n        try (SamReader samReader = SamReaderFactory.makeDefault().open(BAM_FILE);\n            PartitionedBAMFileWriter partitionedBAMFileWriter = new PartitionedBAMFileWriter(outputDir, samReader.getFileHeader(), 2500)) { // BAM file has 10000 reads\n            for (SAMRecord samRecord : samReader) {\n                partitionedBAMFileWriter.addAlignment(samRecord);\n            }\n        }\n\n        // 2. Merge the partitioned BAM and index\n        new PartitionedBAMFileMerger().merge(outputDir, outputBam, outputBaiMerged, outputSbiMerged);\n        textIndexBai(outputBaiMerged); // for debugging\n\n        // 3. Index the merged BAM (using regular indexing)\n        indexBam(outputBam, outputBai);\n\n        // 4. Assert that the merged index is the same as the index produced from the merged file\n        // Check equality on object before comparing file contents to get a better indication\n        // of the difference in case they are not equal.\n        BaiEqualityChecker.assertEquals(outputBam, outputBai, outputBaiMerged);\n        Assert.assertEquals(\n                com.google.common.io.Files.toByteArray(outputBai.toFile()),\n                com.google.common.io.Files.toByteArray(outputBaiMerged.toFile()));\n\n        // 5. Assert that the merged SBI index is the same as the SBI index produced from the merged file\n        Assert.assertEquals(\n                com.google.common.io.Files.toByteArray(outputSbi.toFile()),\n                com.google.common.io.Files.toByteArray(outputSbiMerged.toFile()));\n    }\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2010 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.cram.ref.ReferenceSource;\nimport htsjdk.samtools.reference.InMemoryReferenceSequenceFile;\nimport htsjdk.samtools.reference.ReferenceSequenceFileFactory;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.Log;\nimport htsjdk.samtools.util.Log.LogLevel;\nimport org.testng.Assert;\nimport org.testng.annotations.AfterClass;\nimport org.testng.annotations.BeforeClass;\nimport org.testng.annotations.Test;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.ByteArrayOutputStream;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Path;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class CRAMFileWriterTest extends HtsjdkTest {\n\n    LogLevel globalLogLevel;\n    final File SAM_TOOLS_TEST_DIR = new File(\"src/test/resources/htsjdk/samtools\");\n\n    @Test(description = \"Test for lossy CRAM compression invariants.\")\n    public void lossyCramInvariantsTest() {\n        doTest(createRecords(1000));\n    }\n\n    @Test(description = \"Tests a writing records with null SAMFileHeaders\")\n    public void writeRecordsWithNullHeader() {\n        final List<SAMRecord> samRecs = createRecords(50);\n        for (final SAMRecord rec : samRecs) {\n            rec.setHeader(null);\n        }\n        doTest(samRecs);\n    }\n\n    @Test(description = \"Tests a unmapped record with sequence and quality fields\")\n    public void unmappedWithSequenceAndQualityField() throws Exception {\n        unmappedSequenceAndQualityFieldHelper(true);\n    }\n\n    @Test(description = \"Tests a unmapped record with no sequence or quality fields\")\n    public void unmappedWithNoSequenceAndQualityField() throws Exception {\n        unmappedSequenceAndQualityFieldHelper(false);\n    }\n\n    private void unmappedSequenceAndQualityFieldHelper(boolean unmappedHasBasesAndQualities) throws Exception {\n        final List<SAMRecord> list = new ArrayList<>(2);\n        final SAMRecordSetBuilder builder = new SAMRecordSetBuilder();\n\n        if (builder.getHeader().getReadGroups().isEmpty()) {\n            throw new Exception(\"Read group expected in the header\");\n        }\n\n        builder.setUnmappedHasBasesAndQualities(unmappedHasBasesAndQualities);\n        builder.addUnmappedFragment(\"test1\");\n        builder.addUnmappedPair(\"test2\");\n\n        list.addAll(builder.getRecords());\n        list.sort(new SAMRecordCoordinateComparator());\n\n        doTest(list);\n    }\n\n    private List<SAMRecord> createRecords(int count) {\n        final List<SAMRecord> list = new ArrayList<>(count);\n        final SAMRecordSetBuilder builder = new SAMRecordSetBuilder();\n        if (builder.getHeader().getReadGroups().isEmpty()) {\n            throw new IllegalStateException(\"Read group expected in the header\");\n        }\n\n        int posInRef = 1;\n        for (int i = 0; i < count / 2; i++) {\n            builder.addPair(Integer.toString(i), 0, posInRef += 1, posInRef += 3);\n        }\n        list.addAll(builder.getRecords());\n        list.sort(new SAMRecordCoordinateComparator());\n\n        return list;\n    }\n\n    private SAMFileHeader createSAMHeader(SAMFileHeader.SortOrder sortOrder) {\n        final SAMFileHeader header = new SAMFileHeader();\n        header.setSortOrder(sortOrder);\n        header.addSequence(new SAMSequenceRecord(\"chr1\", 123));\n        SAMReadGroupRecord readGroupRecord = new SAMReadGroupRecord(\"1\");\n        header.addReadGroup(readGroupRecord);\n        return header;\n    }\n\n    private ReferenceSource createReferenceSource() {\n        final byte[] refBases = new byte[1024 * 1024];\n        Arrays.fill(refBases, (byte) 'A');\n        InMemoryReferenceSequenceFile rsf = new InMemoryReferenceSequenceFile();\n        rsf.add(\"chr1\", refBases);\n        return new ReferenceSource(rsf);\n    }\n\n    private void writeRecordsToCRAM(CRAMFileWriter writer, List<SAMRecord> samRecords) {\n        for (SAMRecord record : samRecords) {\n            writer.addAlignment(record);\n        }\n    }\n\n    private void validateRecords(final List<SAMRecord> expectedRecords, ByteArrayInputStream is, ReferenceSource referenceSource) {\n        try (CRAMFileReader cReader = new CRAMFileReader(null, is, referenceSource)) {\n\n            SAMRecordIterator iterator2 = cReader.getIterator();\n            int index = 0;\n            while (iterator2.hasNext()) {\n                SAMRecord actualRecord = iterator2.next();\n                SAMRecord expectedRecord = expectedRecords.get(index++);\n\n                Assert.assertEquals(actualRecord.getReadName(), expectedRecord.getReadName());\n                Assert.assertEquals(actualRecord.getFlags(), expectedRecord.getFlags());\n                Assert.assertEquals(actualRecord.getAlignmentStart(), expectedRecord.getAlignmentStart());\n                Assert.assertEquals(actualRecord.getAlignmentEnd(), expectedRecord.getAlignmentEnd());\n                Assert.assertEquals(actualRecord.getReferenceName(), expectedRecord.getReferenceName());\n                Assert.assertEquals(actualRecord.getMateAlignmentStart(),\n                        expectedRecord.getMateAlignmentStart());\n                Assert.assertEquals(actualRecord.getMateReferenceName(),\n                        expectedRecord.getMateReferenceName());\n                Assert.assertEquals(actualRecord.getReadBases(), expectedRecord.getReadBases());\n                Assert.assertEquals(actualRecord.getBaseQualities(), expectedRecord.getBaseQualities());\n\n                Assert.assertEquals(\n                        actualRecord.getAttributes().stream().map(s -> s.tag).collect(Collectors.toSet()),\n                        expectedRecord.getAttributes().stream().map(s -> s.tag).collect(Collectors.toSet()), expectedRecord.getReadName());\n\n                actualRecord.getAttributes().forEach(tv -> {\n                    Assert.assertEquals(tv.value, expectedRecord.getAttribute(tv.tag));\n                });\n\n            }\n        }\n    }\n\n    private void doTest(final List<SAMRecord> samRecords) {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ReferenceSource refSource = createReferenceSource();\n        final ByteArrayOutputStream os = new ByteArrayOutputStream();\n\n        try (CRAMFileWriter writer = new CRAMFileWriter(os, refSource, header, null)) {\n            writeRecordsToCRAM(writer, samRecords);\n        }\n\n        validateRecords(samRecords, new ByteArrayInputStream(os.toByteArray()), refSource);\n    }\n\n    @Test(description = \"Test CRAMWriter constructor with index stream\")\n    public void testCRAMWriterWithIndex() {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ReferenceSource refSource = createReferenceSource();\n        final ByteArrayOutputStream outStream = new ByteArrayOutputStream();\n        final ByteArrayOutputStream indexStream = new ByteArrayOutputStream();\n\n        final List<SAMRecord> samRecords = createRecords(100);\n        try (CRAMFileWriter writer = new CRAMFileWriter(outStream, indexStream, refSource, header, null)) {\n            writeRecordsToCRAM(writer, samRecords);\n        }\n\n        validateRecords(samRecords, new ByteArrayInputStream(outStream.toByteArray()), refSource);\n        Assert.assertTrue(indexStream.size() != 0);\n    }\n\n    @Test(description = \"Test CRAMWriter constructor with presorted==false\")\n    public void testCRAMWriterNotPresorted() {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ReferenceSource refSource = createReferenceSource();\n        final ByteArrayOutputStream outStream = new ByteArrayOutputStream();\n        final ByteArrayOutputStream indexStream = new ByteArrayOutputStream();\n\n        final List<SAMRecord> samRecords = createRecords(100);\n        try (CRAMFileWriter writer = new CRAMFileWriter(outStream, indexStream, false, refSource, header, null)) {\n\n            // force records to not be coordinate sorted to ensure we're relying on presorted=false\n            samRecords.sort(new SAMRecordCoordinateComparator().reversed());\n            writeRecordsToCRAM(writer, samRecords);\n        }\n        // for validation, restore the sort order of the expected records so they match the order of the written records\n        samRecords.sort(new SAMRecordCoordinateComparator());\n        validateRecords(samRecords, new ByteArrayInputStream(outStream.toByteArray()), refSource);\n        Assert.assertTrue(indexStream.size() != 0);\n    }\n\n    @Test(description = \"Test CRAMWriter constructor reference required 1\", expectedExceptions = IllegalArgumentException.class)\n    public void testCRAMWriterConstructorRequiredReference_1() {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ByteArrayOutputStream outStream = new ByteArrayOutputStream();\n        try (CRAMFileWriter ignored = new CRAMFileWriter(outStream, null, header, null)) {\n        }\n    }\n\n    @Test(description = \"Test CRAMWriter constructor reference required 2\", expectedExceptions = IllegalArgumentException.class)\n    public void testCRAMWriterConstructorRequiredReference_2() {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ByteArrayOutputStream outStream = new ByteArrayOutputStream();\n        try (CRAMFileWriter ignored = new CRAMFileWriter(outStream, null, null, header, null)) {\n        }\n    }\n\n    @Test(description = \"Test CRAMWriter constructor reference required 3\", expectedExceptions = IllegalArgumentException.class)\n    public void testCRAMWriterConstructorRequiredReference_3() {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ByteArrayOutputStream outStream = new ByteArrayOutputStream();\n        try (CRAMFileWriter ignored = new CRAMFileWriter(outStream, null, true, null, header, null)) {\n        }\n    }\n\n\n\n    @Test\n    public void test_roundtrip_tlen_preserved() throws IOException {\n        final ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        final ReferenceSource source = new ReferenceSource(new File(SAM_TOOLS_TEST_DIR, \"cram_tlen.fasta\"));\n        final List<SAMRecord> records = new ArrayList<>();\n\n        try (SamReader reader = SamReaderFactory.make().open(new File(SAM_TOOLS_TEST_DIR, \"cram_tlen_reads.sorted.sam\"));\n             CRAMFileWriter writer = new CRAMFileWriter(baos, source, reader.getFileHeader(), \"test.cram\")) {\n            for (final SAMRecord record : reader) {\n                writer.addAlignment(record);\n                records.add(record);\n            }\n\n        }\n\n        try (CRAMFileReader cramReader = new CRAMFileReader(new ByteArrayInputStream(baos.toByteArray()), (File) null, source, ValidationStringency.STRICT)) {\n            final SAMRecordIterator iterator = cramReader.getIterator();\n            int i = 0;\n            while (iterator.hasNext()) {\n                final SAMRecord record1 = iterator.next();\n                final SAMRecord record2 = records.get(i++);\n                Assert.assertEquals(record1.getInferredInsertSize(), record2.getInferredInsertSize(), record1.getReadName());\n                Assert.assertEquals(record1, record2, record1.getReadName());\n            }\n            Assert.assertEquals(records.size(), i);\n        }\n    }\n\n    @Test\n    public void test_roundtrip_many_reads() throws IOException {\n\n        // Create the input file\n        final File outputDir = IOUtil.createTempDir(this.getClass().getSimpleName() + \".\", \".tmp\");\n        outputDir.deleteOnExit();\n        final Path output = new File(outputDir, \"input.cram\").toPath();\n        IOUtil.deleteOnExit(output);\n        final Path fastaDir = IOUtil.createTempDir(\"CRAMFileWriterTest\", \"\").toPath();\n        IOUtil.deleteOnExit(fastaDir);\n        final Path newFasta = fastaDir.resolve(\"input.fasta\");\n        IOUtil.deleteOnExit(newFasta);\n        IOUtil.deleteOnExit(ReferenceSequenceFileFactory.getFastaIndexFileName(newFasta));\n        IOUtil.deleteOnExit(ReferenceSequenceFileFactory.getDefaultDictionaryForReferenceSequence(newFasta));\n\n        final SAMRecordSetBuilder samRecordSetBuilder = new SAMRecordSetBuilder();\n        samRecordSetBuilder.setHeader(SAMRecordSetBuilder.makeDefaultHeader(SAMFileHeader.SortOrder.coordinate, 10_000, true));\n        samRecordSetBuilder.writeRandomReference(newFasta);\n\n        final List<SAMRecord> records = new ArrayList<>();\n\n        try (SAMFileWriter writer = new SAMFileWriterFactory().makeWriter(samRecordSetBuilder.getHeader(), true, output, newFasta)) {\n\n            // make sure we don't write reads that go off the end of the reference, which is 10,000 bases\n            for (int position = 1; position <= 9900; position += 1) {\n                samRecordSetBuilder.addFrag(\"read_\" + position, 0, position, false);\n            }\n            samRecordSetBuilder.getRecords().forEach(r -> {\n                writer.addAlignment(r);\n                records.add(r);\n            });\n        }\n\n        try (SamReader cramReader = SamReaderFactory.make().referenceSequence(newFasta).open(output)) {\n            final SAMRecordIterator iterator = cramReader.iterator();\n            int i = 0;\n            while (iterator.hasNext()) {\n                final SAMRecord record1 = iterator.next();\n                final SAMRecord record2 = records.get(i++);\n                Assert.assertEquals(record1.getInferredInsertSize(), record2.getInferredInsertSize(), record1.getReadName());\n                Assert.assertEquals(record1, record2, record1.getReadName());\n            }\n            Assert.assertEquals(records.size(), i);\n        }\n    }\n\n    @Test\n    public void testCRAMQuerySort() throws IOException {\n        final File input = new File(SAM_TOOLS_TEST_DIR, \"cram_query_sorted.cram\");\n        final File reference = new File(SAM_TOOLS_TEST_DIR, \"cram_query_sorted.fasta\");\n        final File outputFile = File.createTempFile(\"tmp.\", \".cram\");\n        final SamReaderFactory samReaderFactory = SamReaderFactory.makeDefault().referenceSequence(reference);\n\n        try (final SamReader reader = samReaderFactory.open(input);\n             final SAMFileWriter writer = new SAMFileWriterFactory().makeWriter(reader.getFileHeader().clone(), false, outputFile, reference)) {\n            for (final SAMRecord rec : reader) {\n                writer.addAlignment(rec);\n            }\n        }\n\n        try (final SamReader outReader = samReaderFactory.open(outputFile)) {\n            String prevName = null;\n            for (final SAMRecord rec : outReader) {\n                if (prevName == null) {\n                    prevName = rec.getReadName();\n                    continue;\n                }\n                // test if the read names are sorted alphabetically:\n                Assert.assertTrue(rec.getReadName().compareTo(prevName) >= 0);\n            }\n        }\n    }\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2019 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.cram.CRAIIndex;\nimport htsjdk.samtools.cram.CRAIIndexMerger;\nimport htsjdk.samtools.cram.build.CramIO;\nimport htsjdk.samtools.cram.common.CramVersions;\nimport htsjdk.samtools.cram.ref.CRAMReferenceSource;\nimport htsjdk.samtools.cram.ref.ReferenceSource;\nimport htsjdk.samtools.seekablestream.SeekablePathStream;\nimport htsjdk.samtools.seekablestream.SeekableStream;\nimport htsjdk.samtools.util.FileExtensions;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.ProgressLoggerInterface;\nimport htsjdk.samtools.util.RuntimeIOException;\nimport htsjdk.utils.ValidationUtils;\nimport org.testng.Assert;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class CRAMMergerTest extends HtsjdkTest {\n\n    private final static Path CRAM_FILE = new File(\"src/test/resources/htsjdk/samtools/cram/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram\").toPath();\n    private final static Path CRAM_REF = new File(\"src/test/resources/htsjdk/samtools/reference/human_g1k_v37.20.21.fasta.gz\").toPath();\n\n    /**\n     * Writes a <i>partitioned CRAM</i>.\n     *\n     * Technically, it is not valid to concatenate non-hidden files to create a CRAM file from a partitioned CRAM file, since each slice header has a record\n     * counter acting a a sequential index of records in the file, which would be incorrect if the file was created by concatenation. The implementation\n     * here ignores this complication.\n     *\n     * Note that this writer is only for single-threaded use. Consider using the implementation in Disq for a partitioned CRAM writer\n     * that works with multiple threads or in a distributed setting.\n     *\n     * @see CRAIIndexMerger\n     */\n    static class PartitionedCRAMFileWriter implements SAMFileWriter {\n        private final Path outputDir;\n        private final CRAMReferenceSource referenceSource;\n        private final SAMFileHeader header;\n        private int recordsPerPart;\n        private long recordCount = 0;\n        private int partNumber = -1;\n        private CRAMContainerStreamWriter samStreamWriter;\n        private ProgressLoggerInterface progressLogger;\n\n        public PartitionedCRAMFileWriter(Path outputDir, CRAMReferenceSource referenceSource, SAMFileHeader header, int recordsPerPart) {\n            this.outputDir = outputDir;\n            this.referenceSource = referenceSource;\n            this.header = header;\n            this.recordsPerPart = recordsPerPart;\n        }\n\n        @Override\n        public void addAlignment(SAMRecord alignment) {\n            if (recordCount == 0) {\n                // write header\n                try (OutputStream out = Files.newOutputStream(outputDir.resolve(\"header\"))) {\n                    CRAMContainerStreamWriter cramWriter = new CRAMContainerStreamWriter(out, null, referenceSource, this.header, \"header\");\n                    cramWriter.writeHeader(this.header);\n                    cramWriter.finish(false);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n            if (recordCount % recordsPerPart == 0) {\n                // start a new part\n                try {\n                    if (samStreamWriter != null) {\n                        samStreamWriter.finish(false);\n                    }\n                    partNumber++;\n                    String partName = String.format(\"part-%05d\", partNumber);\n                    OutputStream out = Files.newOutputStream(outputDir.resolve(partName));\n                    OutputStream indexOut = Files.newOutputStream(outputDir.resolve(\".\" + partName + FileExtensions.CRAM_INDEX));\n                    CRAMCRAIIndexer indexer = indexOut == null ? null : new CRAMCRAIIndexer(indexOut, header);\n                    samStreamWriter = new CRAMContainerStreamWriter(out, referenceSource, header, partName, indexer);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n            recordCount++;\n            samStreamWriter.writeAlignment(alignment);\n            if (progressLogger != null) {\n                progressLogger.record(alignment);\n            }\n        }\n\n        @Override\n        public SAMFileHeader getFileHeader() {\n            return header;\n        }\n\n        @Override\n        public void setProgressLogger(ProgressLoggerInterface progressLogger) {\n            this.progressLogger = progressLogger;\n        }\n\n        @Override\n        public void close() {\n            if (samStreamWriter != null) {\n                samStreamWriter.finish(false);\n            }\n            // write terminator\n            try (OutputStream out = Files.newOutputStream(outputDir.resolve(\"terminator\"))) {\n                CramIO.writeCramEOF(CramVersions.DEFAULT_CRAM_VERSION, out);\n            } catch (IOException e) {\n                throw new RuntimeIOException(e);\n            }\n        }\n    }\n\n    /**\n     * Merge the files created by {@link PartitionedCRAMFileWriter} into a single CRAM file and index.\n     */\n    static class PartitionedCRAMFileMerger {\n        public void merge(Path dir, Path outputCram, Path outputCrai) throws IOException {\n            Path headerPath = dir.resolve(\"header\");\n            List<Path> cramParts = Files.list(dir)\n                    .filter(path -> !path.toString().endsWith(FileExtensions.CRAM_INDEX)) // include header and terminator\n                    .sorted()\n                    .collect(Collectors.toList());\n            List<Path> craiParts = Files.list(dir)\n                    .filter(path -> path.toString().endsWith(FileExtensions.CRAM_INDEX))\n                    .sorted()\n                    .collect(Collectors.toList());\n\n            Assert.assertTrue(craiParts.size() > 1);\n\n            ValidationUtils.validateArg(cramParts.size() - 2 == craiParts.size(), \"Number of CRAM part files does not match number of CRAI files (\" + craiParts.size() + \")\");\n\n            // merge CRAM parts\n            try (OutputStream out = Files.newOutputStream(outputCram)) {\n                for (Path cramPart : cramParts) {\n                    Files.copy(cramPart, out);\n                }\n            }\n\n            // merge index parts\n            try (OutputStream out = Files.newOutputStream(outputCrai)) {\n                CRAIIndexMerger craiIndexMerger = new CRAIIndexMerger(out, Files.size(headerPath));\n                int i = 1; // start from 1 since we ignore the header\n                for (Path craiPart : craiParts) {\n                    try (InputStream in = Files.newInputStream(craiPart)) {\n                        CRAIIndex index = CRAMCRAIIndexer.readIndex(in);\n                        craiIndexMerger.processIndex(index, Files.size(cramParts.get(i++)));\n                    }\n                }\n                craiIndexMerger.finish(Files.size(outputCram));\n            }\n        }\n    }\n\n    private static Path indexCram(Path cram, Path crai) throws IOException {\n        try (SeekableStream in = new SeekablePathStream(cram);\n             OutputStream out = Files.newOutputStream(crai)) {\n            CRAMCRAIIndexer.writeIndex(in, out);\n        }\n        return crai;\n    }\n\n    @Test\n    public void test() throws IOException {\n        final Path outputDir = IOUtil.createTempDir(this.getClass().getSimpleName() + \".\", \".tmp\").toPath();\n        IOUtil.deleteOnExit(outputDir);\n\n        final Path outputCram = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.CRAM).toPath();\n        IOUtil.deleteOnExit(outputCram);\n\n        final Path outputCrai = IOUtil.addExtension(outputCram, FileExtensions.CRAM_INDEX);\n        IOUtil.deleteOnExit(outputCrai);\n\n        final Path outputCraiMerged = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.CRAM_INDEX).toPath();\n        IOUtil.deleteOnExit(outputCraiMerged);\n\n        // 1. Read an input CRAM and write it out in partitioned form (header, parts, terminator)\n        ReferenceSource referenceSource = new ReferenceSource(CRAM_REF);\n        try (SamReader samReader = SamReaderFactory.makeDefault().referenceSource(referenceSource).open(CRAM_FILE);\n             PartitionedCRAMFileWriter partitionedCRAMFileWriter = new PartitionedCRAMFileWriter(outputDir, referenceSource, samReader.getFileHeader(), 250)) {\n            for (SAMRecord samRecord : samReader) {\n                partitionedCRAMFileWriter.addAlignment(samRecord);\n            }\n        }\n\n        // 2. Merge the partitioned CRAM and index\n        new PartitionedCRAMFileMerger().merge(outputDir, outputCram, outputCraiMerged);\n\n        // 3. Index the merged CRAM (using regular indexing)\n        indexCram(outputCram, outputCrai);\n\n        // 4. Assert that the merged index is the same as the index produced from the merged file\n        Assert.assertEquals(\n                com.google.common.io.Files.toByteArray(outputCrai.toFile()),\n                com.google.common.io.Files.toByteArray(outputCraiMerged.toFile()));\n    }\n}\n", "/*\n * The MIT License (MIT)\n *\n * Copyright (c) 2017 Daniel Gomez-Sanchez\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in all\n * copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\npackage htsjdk.samtools.reference;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.util.IOUtil;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.util.List;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\n/**\n * @author Daniel Gomez-Sanchez (magicDGS)\n */\npublic class FastaSequenceIndexCreatorTest extends HtsjdkTest {\n    private static File TEST_DATA_DIR = new File(\"src/test/resources/htsjdk/samtools/reference\");\n\n\n    @DataProvider(name = \"indexedSequences\")\n    public Object[][] getIndexedSequences() {\n        return new Object[][]{\n                {new File(TEST_DATA_DIR, \"Homo_sapiens_assembly18.trimmed.fasta\")},\n                {new File(TEST_DATA_DIR, \"Homo_sapiens_assembly18.trimmed.fasta.gz\")},\n                {new File(TEST_DATA_DIR, \"header_with_white_space.fasta\")},\n                {new File(TEST_DATA_DIR, \"crlf.fasta\")}\n        };\n    }\n\n    @Test(dataProvider = \"indexedSequences\")\n    public void testBuildFromFasta(final File indexedFile) throws Exception {\n        final FastaSequenceIndex original = new FastaSequenceIndex(new File(indexedFile.getAbsolutePath() + \".fai\"));\n        final FastaSequenceIndex build = FastaSequenceIndexCreator.buildFromFasta(indexedFile.toPath());\n        Assert.assertEquals(original, build);\n    }\n\n    @Test(dataProvider = \"indexedSequences\")\n    public void testCreate(final File indexedFile) throws Exception {\n        // copy the file to index\n        final File tempDir = IOUtil.createTempDir(\"FastaSequenceIndexCreatorTest\", \"testCreate\");\n        final File copied = new File(tempDir, indexedFile.getName());\n        copied.deleteOnExit();\n        Files.copy(indexedFile.toPath(), copied.toPath());\n\n        // create the index for the copied file\n        FastaSequenceIndexCreator.create(copied.toPath(), false);\n\n        // test if the expected .fai and the created one are the same\n        final File expectedFai = new File(indexedFile.getAbsolutePath() +  \".fai\");\n        final File createdFai = new File(copied.getAbsolutePath() + \".fai\");\n\n        // read all the files and compare line by line\n        try(final Stream<String> expected = Files.lines(expectedFai.toPath());\n                final Stream<String> created = Files.lines(createdFai.toPath())) {\n            final List<String> expectedLines = expected.filter(String::isEmpty).collect(Collectors.toList());\n            final List<String> createdLines = created.filter(String::isEmpty).collect(Collectors.toList());\n            Assert.assertEquals(expectedLines, createdLines);\n        }\n\n        // load the tmp index and check that both are the same\n        Assert.assertEquals(new FastaSequenceIndex(createdFai), new FastaSequenceIndex(expectedFai));\n    }\n\n}", "package htsjdk.samtools.seekablestream;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.TestUtil;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.net.MalformedURLException;\nimport java.net.URISyntaxException;\nimport java.net.URL;\nimport java.nio.file.Paths;\n\npublic class SeekableStreamFactoryTest extends HtsjdkTest {\n    private static final File TEST_DATA_DIR = new File(\"src/test/resources/htsjdk/samtools\");\n\n    @Test\n    public void testIsFilePath() {\n        Assert.assertEquals(SeekableStreamFactory.isFilePath(\"x\"), true);\n        Assert.assertEquals(SeekableStreamFactory.isFilePath(\"\"), true);\n        Assert.assertEquals(SeekableStreamFactory.isFilePath(\"http://broadinstitute.org\"), false);\n        Assert.assertEquals(SeekableStreamFactory.isFilePath(\"https://broadinstitute.org\"), false);\n        Assert.assertEquals(SeekableStreamFactory.isFilePath(\"ftp://broadinstitute.org\"), false);\n    }\n\n    @DataProvider(name=\"getStreamForData\")\n    public Object[][] getStreamForData() throws MalformedURLException {\n        return new Object[][] {\n                { new File(TEST_DATA_DIR, \"BAMFileIndexTest/index_test.bam\").getAbsolutePath(),\n                        new File(TEST_DATA_DIR, \"BAMFileIndexTest/index_test.bam\").getAbsolutePath() },\n                { new File(TEST_DATA_DIR, \"cram_with_bai_index.cram\").getAbsolutePath(),\n                        new File(TEST_DATA_DIR, \"cram_with_bai_index.cram\").getAbsolutePath() },\n                { new URL(\"file://\" + new File(TEST_DATA_DIR, \"cram_with_bai_index.cram\").getAbsolutePath()).toExternalForm(),\n                        new File(TEST_DATA_DIR, \"cram_with_bai_index.cram\").getAbsolutePath() },\n                { new URL(TestUtil.BASE_URL_FOR_HTTP_TESTS + \"index_test.bam\").toExternalForm(),\n                        new URL(TestUtil.BASE_URL_FOR_HTTP_TESTS + \"index_test.bam\").toExternalForm() },\n                { new URL(TestUtil.BASE_URL_FOR_HTTP_TESTS + \"index_test.bam.bai\").toExternalForm(),\n                       new URL(TestUtil.BASE_URL_FOR_HTTP_TESTS + \"index_test.bam.bai\").toExternalForm() }\n        };\n    }\n\n    @Test(dataProvider = \"getStreamForData\")\n    public void testGetStreamFor(final String path, final String expectedPath) throws IOException {\n        Assert.assertEquals(SeekableStreamFactory.getInstance().getStreamFor(path).getSource(), expectedPath);\n    }\n\n    @Test\n    public void testPathWithEmbeddedSpace() throws IOException {\n        final File testBam =  new File(TEST_DATA_DIR, \"BAMFileIndexTest/index_test.bam\");\n\n        //create a temp dir with a space in the name and copy the test file there\n        final File tempDir = IOUtil.createTempDir(\"test spaces\", \"\");\n        Assert.assertTrue(tempDir.getAbsolutePath().contains(\" \"));\n        tempDir.deleteOnExit();\n        final File inputBam = new File(tempDir, \"index_test.bam\");\n        inputBam.deleteOnExit();\n        IOUtil.copyFile(testBam, inputBam);\n\n        // make sure the input string we use is URL-encoded\n        final String inputString = Paths.get(inputBam.getAbsolutePath()).toUri().toString();\n        Assert.assertFalse(inputString.contains(\" \"));\n        Assert.assertTrue(inputString.contains(\"%20\"));\n\n        try (final SeekableStream seekableStream =\n                     SeekableStreamFactory.getInstance().getStreamFor(inputString)) {\n            final int BYTES_TO_READ = 10;\n            Assert.assertEquals(seekableStream.read(new byte[BYTES_TO_READ], 0,BYTES_TO_READ), BYTES_TO_READ);\n        }\n\n    }\n\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2009 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools.util;\n\nimport htsjdk.HtsjdkTest;\nimport com.google.common.jimfs.Configuration;\nimport com.google.common.jimfs.Jimfs;\n\nimport java.io.*;\nimport java.net.URI;\nimport java.nio.file.FileSystem;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.nio.file.spi.FileSystemProvider;\n\nimport htsjdk.samtools.SAMException;\nimport org.testng.Assert;\nimport org.testng.annotations.AfterClass;\nimport org.testng.annotations.BeforeClass;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.lang.IllegalArgumentException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.stream.Collectors;\nimport java.util.Random;\nimport java.util.zip.GZIPOutputStream;\n\n\npublic class IOUtilTest extends HtsjdkTest {\n\n\n    private static final Path TEST_DATA_DIR = Paths.get (\"src/test/resources/htsjdk/samtools/io/\");\n    private static final Path TEST_VARIANT_DIR = Paths.get(\"src/test/resources/htsjdk/variant/\");\n    private static final Path SLURP_TEST_FILE = TEST_DATA_DIR.resolve(\"slurptest.txt\");\n    private static final Path EMPTY_FILE = TEST_DATA_DIR.resolve(\"empty.txt\");\n    private static final Path FIVE_SPACES_THEN_A_NEWLINE_THEN_FIVE_SPACES_FILE = TEST_DATA_DIR.resolve(\"5newline5.txt\");\n    private static final List<String> SLURP_TEST_LINES = Arrays.asList(\"bacon   and rice   \", \"for breakfast  \", \"wont you join me\");\n    private static final String SLURP_TEST_LINE_SEPARATOR = \"\\n\";\n    private static final String TEST_FILE_PREFIX = \"htsjdk-IOUtilTest\";\n    private static final String[] TEST_FILE_EXTENSIONS = {\".txt\", \".txt.gz\"};\n    private static final String TEST_STRING = \"bar!\";\n\n    private File existingTempFile;\n    private String systemUser;\n    private String systemTempDir;\n    private FileSystem inMemoryFileSystem;\n    private static Path WORDS_LONG;\n\n    @BeforeClass\n    public void setUp() throws IOException {\n        existingTempFile = File.createTempFile(\"FiletypeTest.\", \".tmp\");\n        existingTempFile.deleteOnExit();\n        systemTempDir = System.getProperty(\"java.io.tmpdir\");\n        final File tmpDir = new File(systemTempDir);\n        inMemoryFileSystem = Jimfs.newFileSystem(Configuration.unix());;\n        if (!tmpDir.isDirectory()) tmpDir.mkdir();\n        if (!tmpDir.isDirectory())\n            throw new RuntimeException(\"java.io.tmpdir (\" + systemTempDir + \") is not a directory\");\n        systemUser = System.getProperty(\"user.name\");\n        //build long file of random words for compression testing\n        WORDS_LONG = Files.createTempFile(\"words_long\", \".txt\");\n        WORDS_LONG.toFile().deleteOnExit();\n        final List<String> wordsList = Files.lines(TEST_DATA_DIR.resolve(\"dictionary_english_short.dic\")).collect(Collectors.toList());\n        final int numberOfWords = 300000;\n        final int seed = 345987345;\n        final Random rand = new Random(seed);\n        try (final BufferedWriter writer = Files.newBufferedWriter(WORDS_LONG)) {\n            for (int i = 0; i < numberOfWords; i++) {\n                writer.write(wordsList.get(rand.nextInt(wordsList.size())));\n            }\n        }\n    }\n\n    @AfterClass\n    public void tearDown() throws IOException {\n        // reset java properties to original\n        System.setProperty(\"java.io.tmpdir\", systemTempDir);\n        System.setProperty(\"user.name\", systemUser);\n        inMemoryFileSystem.close();\n    }\n\n    @Test\n    public void testFileReadingAndWriting() throws IOException {\n        String randomizedTestString = TEST_STRING + System.currentTimeMillis();\n        for (String ext : TEST_FILE_EXTENSIONS) {\n            File f = File.createTempFile(TEST_FILE_PREFIX, ext);\n            f.deleteOnExit();\n\n            OutputStream os = IOUtil.openFileForWriting(f);\n            BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(os));\n            writer.write(randomizedTestString);\n            writer.close();\n\n            InputStream is = IOUtil.openFileForReading(f);\n            BufferedReader reader = new BufferedReader(new InputStreamReader(is));\n            String line = reader.readLine();\n            Assert.assertEquals(randomizedTestString, line);\n        }\n    }\n\n    @Test(groups = {\"unix\"})\n    public void testGetCanonicalPath() throws IOException {\n        String tmpPath = System.getProperty(\"java.io.tmpdir\");\n        String userName = System.getProperty(\"user.name\");\n\n        if (tmpPath.endsWith(userName)) {\n            tmpPath = tmpPath.substring(0, tmpPath.length() - userName.length());\n        }\n\n        File tmpDir = new File(tmpPath, userName);\n        tmpDir.mkdir();\n        tmpDir.deleteOnExit();\n        File actual = new File(tmpDir, \"actual.txt\");\n        actual.deleteOnExit();\n        ProcessExecutor.execute(new String[]{\"touch\", actual.getAbsolutePath()});\n        File symlink = new File(tmpDir, \"symlink.txt\");\n        symlink.deleteOnExit();\n        ProcessExecutor.execute(new String[]{\"ln\", \"-s\", actual.getAbsolutePath(), symlink.getAbsolutePath()});\n        File lnDir = new File(tmpDir, \"symLinkDir\");\n        lnDir.deleteOnExit();\n        ProcessExecutor.execute(new String[]{\"ln\", \"-s\", tmpDir.getAbsolutePath(), lnDir.getAbsolutePath()});\n        File lnToActual = new File(lnDir, \"actual.txt\");\n        lnToActual.deleteOnExit();\n        File lnToSymlink = new File(lnDir, \"symlink.txt\");\n        lnToSymlink.deleteOnExit();\n\n        File[] files = {actual, symlink, lnToActual, lnToSymlink};\n        for (File f : files) {\n            Assert.assertEquals(IOUtil.getFullCanonicalPath(f), actual.getCanonicalPath());\n        }\n    }\n\n    @Test\n    public void testUtfWriting() throws IOException {\n        final String utf8 = new StringWriter().append((char) 168).append((char) 197).toString();\n        for (String ext : TEST_FILE_EXTENSIONS) {\n            final File f = File.createTempFile(TEST_FILE_PREFIX, ext);\n            f.deleteOnExit();\n\n            final BufferedWriter writer = IOUtil.openFileForBufferedUtf8Writing(f);\n            writer.write(utf8);\n            CloserUtil.close(writer);\n\n            final BufferedReader reader = IOUtil.openFileForBufferedUtf8Reading(f);\n            final String line = reader.readLine();\n            Assert.assertEquals(utf8, line, f.getAbsolutePath());\n\n            CloserUtil.close(reader);\n\n        }\n    }\n\n    @Test\n    public void slurpLinesTest() throws FileNotFoundException {\n        Assert.assertEquals(IOUtil.slurpLines(SLURP_TEST_FILE.toFile()), SLURP_TEST_LINES);\n    }\n\n    @Test\n    public void slurpWhitespaceOnlyFileTest() throws FileNotFoundException {\n        Assert.assertEquals(IOUtil.slurp(FIVE_SPACES_THEN_A_NEWLINE_THEN_FIVE_SPACES_FILE.toFile()), \"     \\n     \");\n    }\n\n    @Test\n    public void slurpEmptyFileTest() throws FileNotFoundException {\n        Assert.assertEquals(IOUtil.slurp(EMPTY_FILE.toFile()), \"\");\n    }\n\n    @Test\n    public void slurpTest() throws FileNotFoundException {\n        Assert.assertEquals(IOUtil.slurp(SLURP_TEST_FILE.toFile()), CollectionUtil.join(SLURP_TEST_LINES, SLURP_TEST_LINE_SEPARATOR));\n    }\n\n    @Test(dataProvider = \"fileTypeTestCases\")\n    public void testFileType(final String path, boolean expectedIsRegularFile) {\n        final File file = new File(path);\n        Assert.assertEquals(IOUtil.isRegularPath(file), expectedIsRegularFile);\n        Assert.assertEquals(IOUtil.isRegularPath(file.toPath()), expectedIsRegularFile);\n    }\n\n    @Test(dataProvider = \"unixFileTypeTestCases\", groups = {\"unix\"})\n    public void testFileTypeUnix(final String path, boolean expectedIsRegularFile) {\n        final File file = new File(path);\n        Assert.assertEquals(IOUtil.isRegularPath(file), expectedIsRegularFile);\n        Assert.assertEquals(IOUtil.isRegularPath(file.toPath()), expectedIsRegularFile);\n    }\n\n    @Test\n    public void testAddExtension() throws IOException {\n        Path p = IOUtil.getPath(\"/folder/file\");\n        Assert.assertEquals(IOUtil.addExtension(p, \".ext\"), IOUtil.getPath(\"/folder/file.ext\"));\n        p = IOUtil.getPath(\"folder/file\");\n        Assert.assertEquals(IOUtil.addExtension(p, \".ext\"), IOUtil.getPath(\"folder/file.ext\"));\n        try (FileSystem jimfs = Jimfs.newFileSystem(Configuration.unix())) {\n            p = jimfs.getPath(\"folder/sub/file\");\n            Assert.assertEquals(IOUtil.addExtension(p, \".ext\"), jimfs.getPath(\"folder/sub/file.ext\"));\n            p = jimfs.getPath(\"folder/file\");\n            Assert.assertEquals(IOUtil.addExtension(p, \".ext\"), jimfs.getPath(\"folder/file.ext\"));\n            p = jimfs.getPath(\"file\");\n            Assert.assertEquals(IOUtil.addExtension(p, \".ext\"), jimfs.getPath(\"file.ext\"));\n        }\n    }\n\n    @Test\n    public void testAddExtensionOnList() throws IOException {\n        Path p = IOUtil.getPath(\"/folder/file\");\n        List<FileSystemProvider> fileSystemProviders = FileSystemProvider.installedProviders();\n\n        List<Path> paths = new ArrayList<>();\n        List<String> strings = new ArrayList<>();\n\n        paths.add(IOUtil.addExtension(p, \".ext\"));\n        strings.add(\"/folder/file.ext\");\n\n        p = IOUtil.getPath(\"folder/file\");\n        paths.add(IOUtil.addExtension(p, \".ext\"));\n        strings.add(\"folder/file.ext\");\n\n        List<Path> expectedPaths = IOUtil.getPaths(strings);\n\n        Assert.assertEquals(paths, expectedPaths);\n    }\n\n\n    @DataProvider(name = \"fileTypeTestCases\")\n    private Object[][] fileTypeTestCases() {\n        return new Object[][]{\n                {existingTempFile.getAbsolutePath(), Boolean.TRUE},\n                {systemTempDir, Boolean.FALSE}\n\n        };\n    }\n\n    @DataProvider(name = \"unixFileTypeTestCases\")\n    private Object[][] unixFileTypeTestCases() {\n        return new Object[][]{\n                {\"/dev/null\", Boolean.FALSE},\n                {\"/dev/stdout\", Boolean.FALSE},\n                {\"/non/existent/file\", Boolean.TRUE},\n        };\n    }\n\n    @DataProvider\n    public Object[][] getFiles(){\n        final File file = new File(\"someFile\");\n        return new Object[][] {\n                {null, null},\n                {file, file.toPath()}\n        };\n    }\n\n    @Test(dataProvider = \"getFiles\")\n    public void testToPath(final File file, final Path expected){\n        Assert.assertEquals(IOUtil.toPath(file), expected);\n    }\n\n\n    @DataProvider(name = \"fileNamesForDelete\")\n    public Object[][] fileNamesForDelete() {\n        return new Object[][] {\n                {Collections.emptyList()},\n                {Collections.singletonList(\"file1\")},\n                {Arrays.asList(\"file1\", \"file2\")}\n        };\n    }\n\n    @Test\n    public void testGetDefaultTmpDirPath() throws Exception {\n        try {\n            Path testPath = IOUtil.getDefaultTmpDirPath();\n            Assert.assertEquals(testPath.toFile().getAbsolutePath(), new File(systemTempDir).getAbsolutePath() + \"/\" + systemUser);\n\n            // change the properties to test others\n            final String newTempPath = Files.createTempDirectory(\"testGetDefaultTmpDirPath\").toString();\n            final String newUser = \"my_user\";\n            System.setProperty(\"java.io.tmpdir\", newTempPath);\n            System.setProperty(\"user.name\", newUser);\n            testPath = IOUtil.getDefaultTmpDirPath();\n            Assert.assertEquals(testPath.toFile().getAbsolutePath(), new File(newTempPath).getAbsolutePath() + \"/\" + newUser);\n\n        } finally {\n            // reset system properties\n            System.setProperty(\"java.io.tmpdir\", systemTempDir);\n            System.setProperty(\"user.name\", systemUser);\n        }\n    }\n\n    @Test(dataProvider = \"fileNamesForDelete\")\n    public void testDeletePathLocal(final List<String> fileNames) throws Exception {\n        final File tmpDir = IOUtil.createTempDir(\"testDeletePath\", \"\");\n        final List<Path> paths = createLocalFiles(tmpDir, fileNames);\n        testDeletePaths(paths);\n    }\n\n    @Test\n    public void testDeleteSinglePath() throws Exception {\n        final Path toDelete = Files.createTempFile(\"file\",\".bad\");\n        Assert.assertTrue(Files.exists(toDelete));\n        IOUtil.deletePath(toDelete);\n        Assert.assertFalse(Files.exists(toDelete));\n    }\n\n    @Test\n    public void testDeleteSingleWithDeletePaths() throws Exception {\n        final Path toDelete = Files.createTempFile(\"file\",\".bad\");\n        Assert.assertTrue(Files.exists(toDelete));\n        IOUtil.deletePaths(toDelete);\n        Assert.assertFalse(Files.exists(toDelete));\n    }\n\n    @Test(dataProvider = \"fileNamesForDelete\")\n    public void testDeletePathJims(final List<String> fileNames) throws Exception {\n        final List<Path> paths = createJimfsFiles(\"testDeletePath\", fileNames);\n        testDeletePaths(paths);\n    }\n\n    @Test(dataProvider = \"fileNamesForDelete\")\n    public void testDeleteArrayPathLocal(final List<String> fileNames) throws Exception {\n        final File tmpDir = IOUtil.createTempDir(\"testDeletePath\", \"\");\n        final List<Path> paths = createLocalFiles(tmpDir, fileNames);\n        testDeletePathArray(paths);\n    }\n\n    @Test(dataProvider = \"fileNamesForDelete\")\n    public void testDeleteArrayPathJims(final List<String> fileNames) throws Exception {\n        final List<Path> paths = createJimfsFiles(\"testDeletePath\", fileNames);\n        testDeletePathArray(paths);\n    }\n\n\n    private static void testDeletePaths(final List<Path> paths) {\n        paths.forEach(p -> Assert.assertTrue(Files.exists(p)));\n        IOUtil.deletePaths(paths);\n        paths.forEach(p -> Assert.assertFalse(Files.exists(p)));\n    }\n\n    private static void testDeletePathArray(final List<Path> paths) {\n        paths.forEach(p -> Assert.assertTrue(Files.exists(p)));\n        IOUtil.deletePaths(paths.toArray(new Path[paths.size()]));\n        paths.forEach(p -> Assert.assertFalse(Files.exists(p)));\n    }\n\n    private static List<Path> createLocalFiles(final File tmpDir, final List<String> fileNames) throws Exception {\n        final List<Path> paths = new ArrayList<>(fileNames.size());\n        for (final String f: fileNames) {\n            final File file = new File(tmpDir, f);\n            Assert.assertTrue(file.createNewFile(), \"failed to create test file\" +file);\n            paths.add(file.toPath());\n        }\n        return paths;\n    }\n\n    private List<Path> createJimfsFiles(final String folderName, final List<String> fileNames) throws Exception {\n        final List<Path> paths = new ArrayList<>(fileNames.size());\n        final Path folder = inMemoryFileSystem.getPath(folderName);\n        if (Files.notExists(folder)) Files.createDirectory(folder);\n\n        for (final String f: fileNames) {\n            final Path p = inMemoryFileSystem.getPath(folderName, f);\n            Files.createFile(p);\n            paths.add(p);\n        }\n\n        return paths;\n    }\n\n    @DataProvider\n    public Object[][] pathsForWritableDirectory() throws Exception {\n        return new Object[][] {\n                // non existent\n                {inMemoryFileSystem.getPath(\"no_exists\"), false},\n                // non directory\n                {Files.createFile(inMemoryFileSystem.getPath(\"testAssertDirectoryIsWritable_file\")), false},\n                // TODO - how to do in inMemoryFileSystem a non-writable directory?\n                // writable directory\n                {Files.createDirectory(inMemoryFileSystem.getPath(\"testAssertDirectoryIsWritable_directory\")), true}\n        };\n    }\n\n    @Test(dataProvider = \"pathsForWritableDirectory\")\n    public void testAssertDirectoryIsWritablePath(final Path path, final boolean writable) {\n        try {\n            IOUtil.assertDirectoryIsWritable(path);\n        } catch (SAMException e) {\n            if (writable) {\n                Assert.fail(e.getMessage());\n            }\n        }\n    }\n\n    @DataProvider\n    public Object[][] filesForWritableDirectory() throws Exception {\n        final File nonWritableFile = new File(systemTempDir, \"testAssertDirectoryIsWritable_non_writable_dir\");\n        nonWritableFile.mkdir();\n        nonWritableFile.setWritable(false);\n\n        return new Object[][] {\n                // non existent\n                {new File(\"no_exists\"), false},\n                // non directory\n                {existingTempFile, false},\n                // non-writable directory\n                {nonWritableFile, false},\n                // writable directory\n                {new File(systemTempDir), true},\n        };\n    }\n\n    @Test(dataProvider = \"filesForWritableDirectory\")\n    public void testAssertDirectoryIsWritableFile(final File file, final boolean writable) {\n        try {\n            IOUtil.assertDirectoryIsWritable(file);\n        } catch (SAMException e) {\n            if (writable) {\n                Assert.fail(e.getMessage());\n            }\n        }\n    }\n\n    static final String level1 = \"Level1.fofn\";\n    static final String level2 = \"Level2.fofn\";\n\n    @DataProvider\n    public Object[][] fofnData() throws IOException {\n\n        Path fofnPath1 = inMemoryFileSystem.getPath(level1);\n        Files.copy(TEST_DATA_DIR.resolve(level1), fofnPath1);\n\n        Path fofnPath2 = inMemoryFileSystem.getPath(level2);\n        Files.copy(TEST_DATA_DIR.resolve(level2), fofnPath2);\n\n        return new Object[][]{\n                {TEST_DATA_DIR + \"/\" + level1, new String[]{\".vcf\", \".vcf.gz\"}, 2},\n                {TEST_DATA_DIR + \"/\" + level2, new String[]{\".vcf\", \".vcf.gz\"}, 4},\n                {fofnPath1.toUri().toString(), new String[]{\".vcf\", \".vcf.gz\"}, 2},\n                {fofnPath2.toUri().toString(), new String[]{\".vcf\", \".vcf.gz\"}, 4}\n        };\n    }\n\n    @Test(dataProvider = \"fofnData\")\n    public void testUnrollPaths(final String pathUri, final String[] extensions, final int expectedNumberOfUnrolledPaths) throws IOException {\n        Path p = IOUtil.getPath(pathUri);\n        List<Path> paths = IOUtil.unrollPaths(Collections.singleton(p), extensions);\n\n        Assert.assertEquals(paths.size(), expectedNumberOfUnrolledPaths);\n    }\n\n    @DataProvider(name = \"blockCompressedExtensionExtensionStrings\")\n    public static Object[][] createBlockCompressedExtensionStrings() {\n        return new Object[][] {\n                { \"testzip.gz\", true },\n                { \"test.gzip\", true },\n                { \"test.bgz\", true },\n                { \"test.bgzf\", true },\n                { \"test.bzip2\", false }\n        };\n    }\n\n    @Test(dataProvider = \"blockCompressedExtensionExtensionStrings\")\n    public void testBlockCompressionExtensionString(final String testString, final boolean expected) {\n        Assert.assertEquals(IOUtil.hasBlockCompressedExtension(testString), expected);\n    }\n\n    @Test(dataProvider = \"blockCompressedExtensionExtensionStrings\")\n    public void testBlockCompressionExtensionFile(final String testString, final boolean expected) {\n        Assert.assertEquals(IOUtil.hasBlockCompressedExtension(new File(testString)), expected);\n    }\n\n    @DataProvider(name = \"blockCompressedExtensionExtensionURIStrings\")\n    public static Object[][] createBlockCompressedExtensionURIs() {\n        return new Object[][]{\n                {\"testzip.gz\", true},\n                {\"test.gzip\", true},\n                {\"test.bgz\", true},\n                {\"test.bgzf\", true},\n                {\"test\", false},\n                {\"test.bzip2\", false},\n\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.gz\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.gzip\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bgz\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bgzf\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bzip2\", false},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877\", false},\n\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.gz?alt=media\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.gzip?alt=media\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bgz?alt=media\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bgzf?alt=media\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bzip2?alt=media\", false},\n\n                {\"ftp://ftp.broadinstitute.org/distribution/igv/TEST/cpgIslands.hg18.gz\", true},\n                {\"ftp://ftp.broadinstitute.org/distribution/igv/TEST/cpgIslands.hg18.bed\", false}\n        };\n    }\n\n    @Test(dataProvider = \"blockCompressedExtensionExtensionURIStrings\")\n    public void testBlockCompressionExtension(final String testURIString, final boolean expected) {\n        URI testURI = URI.create(testURIString);\n        Assert.assertEquals(IOUtil.hasBlockCompressedExtension(testURI), expected);\n    }\n\n    @Test(dataProvider = \"blockCompressedExtensionExtensionURIStrings\")\n    public void testBlockCompressionExtensionStringVersion(final String testURIString, final boolean expected) {\n        Assert.assertEquals(IOUtil.hasBlockCompressedExtension(testURIString), expected);\n    }\n\n    @DataProvider\n    public static Object[][] blockCompressedFiles() {\n        return new Object[][]{\n                {TEST_DATA_DIR.resolve(\"ipsum.txt\"), true, false},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt\"), false, false},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.gz\"), true, false},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.gz\"), false, false},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz\"), true, true},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz\"), false, true},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz.wrongextension\"), true, false},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz.wrongextension\"), false, true},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgzipped_with_gzextension.gz\"), true, true},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgzipped_with_gzextension.gz\"), false, true},\n                {TEST_DATA_DIR.resolve(\"example.bam\"), true, false},\n                {TEST_DATA_DIR.resolve(\"example.bam\"), false, true}\n        };\n    }\n\n    @Test(dataProvider = \"blockCompressedFiles\")\n    public void testIsBlockCompressed(Path file, boolean checkExtension, boolean expected) throws IOException {\n        Assert.assertEquals(IOUtil.isBlockCompressed(file, checkExtension), expected);\n    }\n\n    @Test(dataProvider = \"blockCompressedFiles\")\n    public void testIsBlockCompressedOnJimfs(Path file, boolean checkExtension, boolean expected) throws IOException {\n         try (FileSystem jimfs = Jimfs.newFileSystem(Configuration.unix())) {\n             final Path jimfsRoot = jimfs.getRootDirectories().iterator().next();\n             final Path jimfsFile = Files.copy(file, jimfsRoot.resolve(file.getFileName().toString()));\n             Assert.assertEquals(IOUtil.isBlockCompressed(jimfsFile, checkExtension), expected);\n         }\n    }\n\n    @DataProvider\n    public static Object[][] filesToCompress() {\n        return new Object[][]{\n                {WORDS_LONG, \".gz\", 8},\n                {WORDS_LONG, \".bfq\", 8},\n                {TEST_VARIANT_DIR.resolve(\"test1.vcf\"), \".gz\", 7},\n                {TEST_VARIANT_DIR.resolve(\"test1.vcf\"), \".bfq\", 7}\n        };\n    }\n\n    @Test(dataProvider = \"filesToCompress\")\n    public void testCompressionLevel(final Path file, final String extension, final int lastDifference) throws IOException {\n        final long origSize = Files.size(file);\n        long previousSize = origSize;\n        for (int compressionLevel = 1; compressionLevel <= 9; compressionLevel++) {\n            final Path outFile = Files.createTempFile(\"tmp\", extension);\n            outFile.toFile().deleteOnExit();\n            IOUtil.setCompressionLevel(compressionLevel);\n            Assert.assertEquals(IOUtil.getCompressionLevel(), compressionLevel);\n            final InputStream inStream = IOUtil.openFileForReading(file);\n            try (final OutputStream outStream = IOUtil.openFileForWriting(outFile.toFile())) {\n                IOUtil.transferByStream(inStream, outStream, origSize);\n            }\n            final long newSize = Files.size(outFile);\n            if (compressionLevel <= lastDifference) {\n                Assert.assertTrue(previousSize > newSize);\n            } else {\n                Assert.assertTrue(previousSize >= newSize);\n            }\n            previousSize = newSize;\n        }\n    }\n\n    @DataProvider\n    public Object[][] getExtensions(){\n        return new Object[][]{\n                {\".gz\", true},\n                {\".bfq\", true},\n                {\".txt\", false}};\n    }\n\n    @Test(dataProvider = \"getExtensions\")\n    public void testReadWriteJimfs(String extension, boolean gzipped) throws IOException {\n        final Path jmfsRoot = inMemoryFileSystem.getRootDirectories().iterator().next();\n        final Path tmp = Files.createTempFile(jmfsRoot, \"test\", extension);\n        final String expected = \"lorem ipswitch, nantucket, bucket\";\n        try (Writer out = IOUtil.openFileForBufferedWriting(tmp)){\n            out.write(expected);\n        }\n\n        try (InputStream in = new BufferedInputStream(Files.newInputStream(tmp))){\n               Assert.assertEquals(IOUtil.isGZIPInputStream(in), gzipped);\n        }\n\n        try (BufferedReader in = IOUtil.openFileForBufferedReading(tmp)){\n            final String actual = in.readLine();\n            Assert.assertEquals(actual, expected);\n        }\n    }\n\n    @DataProvider\n    public static Object[][] badCompressionLevels() {\n        return new Object[][]{\n                {-1},\n                {10}\n        };\n    }\n\n    @Test(dataProvider = \"badCompressionLevels\", expectedExceptions = {IllegalArgumentException.class})\n    public void testCompressionLevelExceptions(final int compressionLevel) {\n        IOUtil.setCompressionLevel(compressionLevel);\n    }\n\n    @DataProvider\n    public static Object[][] filesToCopy() {\n        return new Object[][]{\n                {TEST_VARIANT_DIR.resolve(\"test1.vcf\")},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt\")}\n        };\n    }\n\n    @Test(dataProvider = \"filesToCopy\")\n    public void testCopyFile(final Path file) throws IOException {\n        final Path outFile = Files.createTempFile(\"tmp\", \".tmp\");\n        outFile.toFile().deleteOnExit();\n        IOUtil.copyFile(file.toFile(), outFile.toFile());\n        Assert.assertEquals(Files.lines(file).collect(Collectors.toList()), Files.lines(outFile).collect(Collectors.toList()));\n    }\n\n    @Test(dataProvider = \"filesToCopy\", expectedExceptions = {SAMException.class})\n    public void testCopyFileReadException(final Path file) throws IOException {\n        final Path outFile = Files.createTempFile(\"tmp\", \".tmp\");\n        outFile.toFile().deleteOnExit();\n        file.toFile().setReadable(false);\n        try {\n            IOUtil.copyFile(file.toFile(), outFile.toFile());\n        } finally { //need to set input file permission back to readable so other unit tests can access it\n            file.toFile().setReadable(true);\n        }\n    }\n\n    @Test(dataProvider = \"filesToCopy\", expectedExceptions = {SAMException.class})\n    public void testCopyFileWriteException(final Path file) throws IOException {\n        final Path outFile = Files.createTempFile(\"tmp\", \".tmp\");\n        outFile.toFile().deleteOnExit();\n        outFile.toFile().setWritable(false);\n        IOUtil.copyFile(file.toFile(), outFile.toFile());\n    }\n\n    @DataProvider\n    public static Object[][] baseNameTests() {\n        return new Object[][]{\n                {TEST_DATA_DIR.resolve(\"ipsum.txt\"), \"ipsum\"},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz.wrongextension\"), \"ipsum.txt.bgz\"},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgzipped_with_gzextension.gz\"), \"ipsum.txt.bgzipped_with_gzextension\"},\n                {TEST_VARIANT_DIR.resolve(\"utils\"), \"utils\"},\n                {TEST_VARIANT_DIR.resolve(\"not_real_file.txt\"), \"not_real_file\"}\n        };\n    }\n\n    @Test(dataProvider = \"baseNameTests\")\n    public void testBasename(final Path file, final String expected) {\n        final String result = IOUtil.basename(file.toFile());\n        Assert.assertEquals(result, expected);\n    }\n\n    @DataProvider\n    public static Object[][] regExpTests() {\n        return new Object[][]{\n                {\"\\\\w+\\\\.txt\", new String[]{\"5newline5.txt\", \"empty.txt\", \"ipsum.txt\", \"slurptest.txt\"}},\n                {\"^((?!txt).)*$\", new String[]{\"Level1.fofn\", \"Level2.fofn\", \"example.bam\"}},\n                {\"^\\\\d+.*\", new String[]{\"5newline5.txt\"}}\n        };\n    }\n\n    @Test(dataProvider = \"regExpTests\")\n    public void testRegExp(final String regexp, final String[] expected) throws IOException {\n        final String[] allNames = {\"5newline5.txt\", \"Level2.fofn\", \"example.bam\", \"ipsum.txt.bgz\", \"ipsum.txt.bgzipped_with_gzextension.gz\", \"slurptest.txt\", \"Level1.fofn\", \"empty.txt\", \"ipsum.txt\", \"ipsum.txt.bgz.wrongextension\", \"ipsum.txt.gz\"};\n        final Path regExpDir = Files.createTempDirectory(\"regExpDir\");\n        regExpDir.toFile().deleteOnExit();\n        final List<String> listExpected = Arrays.asList(expected);\n        final List<File> expectedFiles = new ArrayList<File>();\n        for (String name : allNames) {\n            final Path file = regExpDir.resolve(name);\n            file.toFile().deleteOnExit();\n            file.toFile().createNewFile();\n            if (listExpected.contains(name)) {\n                expectedFiles.add(file.toFile());\n            }\n        }\n        final File[] result = IOUtil.getFilesMatchingRegexp(regExpDir.toFile(), regexp);\n        Assert.assertEqualsNoOrder(result, expectedFiles.toArray());\n    }\n\n    @Test()\n    public void testReadLines() throws IOException {\n        final Path file = Files.createTempFile(\"tmp\", \".txt\");\n        file.toFile().deleteOnExit();\n        final int seed = 12394738;\n        final Random rand = new Random(seed);\n        final int nLines = 5;\n        final List<String> lines = new ArrayList<String>();\n        try (final PrintWriter writer = new PrintWriter(Files.newBufferedWriter(file))) {\n            for (int i = 0; i < nLines; i++) {\n                final String line = TEST_STRING + Integer.toString(rand.nextInt(100000000));\n                lines.add(line);\n                writer.println(line);\n            }\n        }\n        final List<String> retLines = new ArrayList<String>();\n        IOUtil.readLines(file.toFile()).forEachRemaining(retLines::add);\n        Assert.assertEquals(retLines, lines);\n    }\n\n    @DataProvider\n    public static Object[][] fileSuffixTests() {\n        return new Object[][]{\n                {TEST_DATA_DIR.resolve(\"ipsum.txt\"), \".txt\"},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz\"), \".bgz\"},\n                {TEST_DATA_DIR, null}\n        };\n    }\n\n    @Test(dataProvider = \"fileSuffixTests\")\n    public void testSuffixTest(final Path file, final String expected) {\n        final String ret = IOUtil.fileSuffix(file.toFile());\n        Assert.assertEquals(ret, expected);\n    }\n\n    @Test\n    public void testCopyDirectoryTree() throws IOException {\n        final Path copyToDir = Files.createTempDirectory(\"copyToDir\");\n        copyToDir.toFile().deleteOnExit();\n        IOUtil.copyDirectoryTree(TEST_VARIANT_DIR.toFile(), copyToDir.toFile());\n        final List<Path> collect = Files.walk(TEST_VARIANT_DIR).filter(f -> !f.equals(TEST_VARIANT_DIR)).map(p -> p.getFileName()).collect(Collectors.toList());\n        final List<Path> collectCopy = Files.walk(copyToDir).filter(f -> !f.equals(copyToDir)).map(p -> p.getFileName()).collect(Collectors.toList());\n        Assert.assertEqualsNoOrder(collect.toArray(), collectCopy.toArray());\n    }\n\n    // Little utility to gzip a byte array\n    private static byte[] gzipMessage(final byte[] message) throws IOException {\n        ByteArrayOutputStream bos = new ByteArrayOutputStream();\n        GZIPOutputStream gzout = new GZIPOutputStream(bos);\n        gzout.write(message);\n        gzout.finish();\n        gzout.close();\n        return bos.toByteArray();\n    }\n\n    @DataProvider\n    public static Object[][] gzipTests() throws IOException {\n        final byte[] emptyMessage = \"\".getBytes();\n        final byte[] message = \"Hello World\".getBytes();\n\n        // Compressed version of the messages\n        final byte[] gzippedMessage = gzipMessage(message);\n        final byte[] gzippedEmptyMessage = gzipMessage(emptyMessage);\n\n        return new Object[][]{\n                {emptyMessage, false},\n                {message, false},\n                {gzippedMessage, true},\n                {gzippedEmptyMessage, true}\n        };\n    }\n\n    @Test(dataProvider = \"gzipTests\")\n    public void isGZIPInputStreamTest(byte[] data, boolean isGzipped) throws IOException {\n        try(ByteArrayInputStream inputStream = new ByteArrayInputStream(data)) {\n            // test string without compression\n            Assert.assertEquals(IOUtil.isGZIPInputStream(inputStream), isGzipped);\n            // call twice to verify 'in.reset()' was called\n            Assert.assertEquals(IOUtil.isGZIPInputStream(inputStream), isGzipped);\n        }\n    }\n}\n\n", "/*\n * The MIT License\n *\n * Copyright (c) 2013 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.tribble.index;\n\nimport com.google.common.io.Files;\nimport com.google.common.jimfs.Configuration;\nimport com.google.common.jimfs.Jimfs;\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.SAMSequenceDictionary;\nimport htsjdk.samtools.SAMSequenceRecord;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.Interval;\nimport htsjdk.tribble.AbstractFeatureReader;\nimport htsjdk.tribble.TestUtils;\nimport htsjdk.tribble.Tribble;\nimport htsjdk.tribble.TribbleException;\nimport htsjdk.tribble.VCFRedirectCodec;\nimport htsjdk.tribble.bed.BEDCodec;\nimport htsjdk.tribble.index.tabix.TabixFormat;\nimport htsjdk.tribble.index.tabix.TabixIndex;\nimport htsjdk.tribble.readers.LineIterator;\nimport htsjdk.variant.bcf2.BCF2Codec;\nimport htsjdk.variant.variantcontext.VariantContext;\nimport htsjdk.variant.vcf.VCFCodec;\nimport htsjdk.variant.vcf.VCFFileReader;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.FileSystem;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.Iterator;\nimport java.util.List;\n\n/**\n * User: jacob\n * Date: 2012-Aug-23\n */\npublic class IndexFactoryTest extends HtsjdkTest {\n\n    @DataProvider(name = \"bedDataProvider\")\n    public Object[][] getLinearIndexFactoryTypes() {\n        return new Object[][] {\n                { new File(TestUtils.DATA_DIR, \"bed/Unigene.sample.bed\") },\n                { new File(TestUtils.DATA_DIR, \"bed/Unigene.sample.bed.gz\") }\n        };\n    }\n\n    @Test(dataProvider = \"bedDataProvider\")\n    public void testCreateLinearIndexFromBED(final File inputBEDFIle) {\n        Index index = IndexFactory.createLinearIndex(inputBEDFIle, new BEDCodec());\n        String chr = \"chr2\";\n\n        Assert.assertTrue(index.getSequenceNames().contains(chr));\n        Assert.assertTrue(index.containsChromosome(chr));\n        Assert.assertEquals(1, index.getSequenceNames().size());\n        List<Block> blocks = index.getBlocks(chr, 1, 50);\n        Assert.assertEquals(1, blocks.size());\n\n        Block block = blocks.get(0);\n        Assert.assertEquals(78, block.getSize());\n    }\n\n    @Test(expectedExceptions = TribbleException.MalformedFeatureFile.class, dataProvider = \"indexFactoryProvider\")\n    public void testCreateIndexUnsorted(IndexFactory.IndexType type) {\n        final File unsortedBedFile = new File(TestUtils.DATA_DIR, \"bed/unsorted.bed\");\n        IndexFactory.createIndex(unsortedBedFile, new BEDCodec(), type);\n    }\n\n    @Test(expectedExceptions = TribbleException.MalformedFeatureFile.class, dataProvider = \"indexFactoryProvider\")\n    public void testCreateIndexDiscontinuousContigs(IndexFactory.IndexType type) throws Exception{\n        final File discontinuousFile = new File(TestUtils.DATA_DIR,\"bed/disconcontigs.bed\");\n        IndexFactory.createIndex(discontinuousFile, new BEDCodec(), type);\n    }\n\n    @DataProvider(name = \"indexFactoryProvider\")\n    public Object[][] getIndexFactoryTypes(){\n        return new Object[][] {\n                new Object[] { IndexFactory.IndexType.TABIX },\n                new Object[] { IndexFactory.IndexType.LINEAR },\n                new Object[] { IndexFactory.IndexType.INTERVAL_TREE }\n        };\n    }\n\n    @Test\n    public void testCreateTabixIndexOnBlockCompressed() {\n        // index a VCF\n        final Path inputFileVcf = Paths.get(\"src/test/resources/htsjdk/tribble/tabix/testTabixIndex.vcf\");\n        final VCFFileReader readerVcf = new VCFFileReader(inputFileVcf, false);\n        final SAMSequenceDictionary vcfDict = readerVcf.getFileHeader().getSequenceDictionary();\n        final TabixIndex tabixIndexVcf =\n                IndexFactory.createTabixIndex(inputFileVcf, new VCFCodec(), TabixFormat.VCF, vcfDict);\n\n        // index the same bgzipped VCF\n        final Path inputFileVcfGz = Paths.get(\"src/test/resources/htsjdk/tribble/tabix/testTabixIndex.vcf.gz\");\n        final VCFFileReader readerVcfGz = new VCFFileReader(inputFileVcfGz, false);\n        final TabixIndex tabixIndexVcfGz =\n                IndexFactory.createTabixIndex(inputFileVcfGz, new VCFCodec(), TabixFormat.VCF,\n                        readerVcfGz.getFileHeader().getSequenceDictionary());\n\n        // assert that each sequence in the header that represents some VCF row ended up in the index\n        // for both the VCF and bgzipped VCF\n        for (SAMSequenceRecord samSequenceRecord : vcfDict.getSequences()) {\n            Assert.assertTrue(\n                    tabixIndexVcf.containsChromosome(samSequenceRecord.getSequenceName()),\n                    \"Tabix indexed VCF does not contain sequence: \" + samSequenceRecord.getSequenceName());\n\n            Assert.assertTrue(\n                    tabixIndexVcfGz.containsChromosome(samSequenceRecord.getSequenceName()),\n                    \"Tabix indexed (bgzipped) VCF does not contain sequence: \" + samSequenceRecord.getSequenceName());\n        }\n\n    }\n\n    @Test\n    public void testTabixOnNonDefaultFileSystem() throws IOException {\n        try (final FileSystem fs = Jimfs.newFileSystem(\"test\", Configuration.unix())) {\n            final Path vcfInJimfs = TestUtils.getTribbleFileInJimfs(\n                    \"src/test/resources/htsjdk/tribble/tabix/testTabixIndex.vcf.gz\",\n                    null, fs);\n            // IndexFactory doesn't write the output, so we just want to make sure it doesn't throw\n            IndexFactory.createTabixIndex(vcfInJimfs, new VCFCodec(), TabixFormat.VCF, null);\n        }\n    }\n\n    @DataProvider(name = \"vcfDataProvider\")\n    public Object[][] getVCFIndexData(){\n        return new Object[][] {\n                new Object[] {\n                        new File(TestUtils.DATA_DIR, \"tabix/4featuresHG38Header.vcf.gz\"),\n                        new Interval(\"chr6\", 33414233, 118314029)\n                },\n                new Object[] {\n                        new File(TestUtils.DATA_DIR, \"tabix/4featuresHG38Header.vcf\"),\n                        new Interval(\"chr6\", 33414233, 118314029)\n                },\n        };\n    }\n\n    @Test(dataProvider = \"vcfDataProvider\")\n    public void testCreateTabixIndexFromVCF(\n            final File inputVCF,\n            final Interval queryInterval) throws IOException {\n        // copy the original file and create the index for the copy\n        final File tempDir = IOUtil.createTempDir(\"testCreateTabixIndexFromVCF\", null);\n        tempDir.deleteOnExit();\n        final File tmpVCF = new File(tempDir, inputVCF.getName());\n        Files.copy(inputVCF, tmpVCF);\n        tmpVCF.deleteOnExit();\n\n        // this test creates a TABIX index (.tbi)\n        final TabixIndex tabixIndexGz = IndexFactory.createTabixIndex(tmpVCF, new VCFCodec(), null);\n        tabixIndexGz.writeBasedOnFeatureFile(tmpVCF);\n        final File tmpIndex = Tribble.tabixIndexFile(tmpVCF);\n        tmpIndex.deleteOnExit();\n\n        try (final VCFFileReader originalReader = new VCFFileReader(inputVCF,false);\n            final VCFFileReader tmpReader = new VCFFileReader(tmpVCF, tmpIndex,true)) {\n            Iterator<VariantContext> originalIt = originalReader.iterator();\n            Iterator<VariantContext> tmpIt = tmpReader.query(queryInterval.getContig(), queryInterval.getStart(), queryInterval.getEnd());\n            while (originalIt.hasNext()) {\n                Assert.assertTrue(tmpIt.hasNext(), \"variants missing from gzip query\");\n                VariantContext vcTmp = tmpIt.next();\n                VariantContext vcOrig = originalIt.next();\n                Assert.assertEquals(vcOrig.getContig(), vcTmp.getContig());\n                Assert.assertEquals(vcOrig.getStart(), vcTmp.getStart());\n                Assert.assertEquals(vcOrig.getEnd(), vcTmp.getEnd());\n            }\n        }\n    }\n\n    @DataProvider(name = \"bcfDataFactory\")\n    public Object[][] getBCFData(){\n        return new Object[][] {\n                //TODO: this needs more test cases, including block compressed and indexed, but bcftools can't\n                // generate indices for BCF2.1 files, which is all HTSJDK can read, and htsjdk also can't read/write\n                // block compressed BCFs (https://github.com/samtools/htsjdk/issues/946)\n                new Object[] {\n                        new File(\"src/test/resources/htsjdk/variant/serialization_test.bcf\")\n                }\n        };\n    }\n\n    @Test(dataProvider = \"bcfDataFactory\")\n    public void testCreateLinearIndexFromBCF(final File inputBCF) throws IOException {\n        // copy the original file and create the index for the copy\n        final File tempDir = IOUtil.createTempDir(\"testCreateIndexFromBCF\", null);\n        tempDir.deleteOnExit();\n        final File tmpBCF = new File(tempDir, inputBCF.getName());\n        Files.copy(inputBCF, tmpBCF);\n        tmpBCF.deleteOnExit();\n\n        // NOTE: this test creates a LINEAR index (.idx)\n        final Index index = IndexFactory.createIndex(tmpBCF, new BCF2Codec(), IndexFactory.IndexType.LINEAR);\n        index.writeBasedOnFeatureFile(tmpBCF);\n        final File tempIndex = Tribble.indexFile(tmpBCF);\n        tempIndex.deleteOnExit();\n\n        try (final VCFFileReader originalReader = new VCFFileReader(inputBCF,false);\n            final VCFFileReader tmpReader = new VCFFileReader(tmpBCF, tempIndex,true)) {\n            final Iterator<VariantContext> originalIt = originalReader.iterator();\n            while (originalIt.hasNext()) {\n                // we don't have an externally generated index file for the original input, so iterate through each variant\n                // and use the generated index to query for the same variant in the indexed copy of the input\n                final VariantContext vcOrig = originalIt.next();\n                final Iterator<VariantContext> tmpIt = tmpReader.query(vcOrig);\n                Assert.assertTrue(tmpIt.hasNext(), \"Variant not returned from indexed file\");\n                final VariantContext vcTmp = tmpIt.next();\n                Assert.assertEquals(vcOrig.getContig(), vcTmp.getContig());\n                Assert.assertEquals(vcOrig.getStart(), vcTmp.getStart());\n                Assert.assertEquals(vcOrig.getEnd(), vcTmp.getEnd());\n                Assert.assertFalse(tmpIt.hasNext()); // make sure there is only one matching variant\n            }\n        }\n    }\n\n    @DataProvider\n    public Object[][] getRedirectFiles(){\n        return new Object[][] {\n                {VCFRedirectCodec.REDIRECTING_CODEC_TEST_FILE_ROOT + \"vcf.gz.redirect\", IndexFactory.IndexType.TABIX},\n                {VCFRedirectCodec.REDIRECTING_CODEC_TEST_FILE_ROOT + \"vcf.redirect\", IndexFactory.IndexType.INTERVAL_TREE},\n                {VCFRedirectCodec.REDIRECTING_CODEC_TEST_FILE_ROOT + \"vcf.redirect\", IndexFactory.IndexType.LINEAR}\n        };\n    }\n\n    @Test(dataProvider = \"getRedirectFiles\")\n    public void testIndexRedirectedFiles(String input, IndexFactory.IndexType type) throws IOException {\n        final VCFRedirectCodec codec = new VCFRedirectCodec();\n        final File dir = IOUtil.createTempDir(\"redirec-test\", \"dir\");\n        try {\n            final File tmpInput = new File(dir, new File(input).getName());\n            Files.copy(new File(input), tmpInput);\n            final File tmpDataFile = new File(codec.getPathToDataFile(tmpInput.toString()));\n            Assert.assertTrue(new File(tmpDataFile.getAbsoluteFile().getParent()).mkdir());\n            final File originalDataFile = new File(codec.getPathToDataFile(input));\n            Files.copy(originalDataFile, tmpDataFile);\n\n            try(final AbstractFeatureReader<VariantContext, LineIterator> featureReader = AbstractFeatureReader.getFeatureReader(tmpInput.getAbsolutePath(), codec, false)) {\n                Assert.assertFalse(featureReader.hasIndex());\n            }\n            final Index index = IndexFactory.createIndex(tmpInput, codec, type);\n            index.writeBasedOnFeatureFile(tmpDataFile);\n\n            try(final AbstractFeatureReader<VariantContext, LineIterator> featureReader = AbstractFeatureReader.getFeatureReader(tmpInput.getAbsolutePath(), codec)) {\n                Assert.assertTrue(featureReader.hasIndex());\n                Assert.assertEquals(featureReader.query(\"20\",1110696,1230237).stream().count(), 2);\n            }\n        } finally {\n            IOUtil.recursiveDelete(dir.toPath());\n        }\n    }\n}\n", "package htsjdk.tribble.index;\n\nimport com.google.common.jimfs.Configuration;\nimport com.google.common.jimfs.Jimfs;\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.util.FileExtensions;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.tribble.FeatureCodec;\nimport htsjdk.tribble.TestUtils;\nimport htsjdk.tribble.Tribble;\nimport htsjdk.tribble.bed.BEDCodec;\nimport htsjdk.tribble.index.interval.IntervalTreeIndex;\nimport htsjdk.tribble.index.linear.LinearIndex;\nimport htsjdk.tribble.index.tabix.TabixIndex;\nimport htsjdk.tribble.util.LittleEndianOutputStream;\nimport htsjdk.tribble.util.TabixUtils;\nimport htsjdk.variant.vcf.VCFCodec;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.OutputStream;\nimport java.nio.file.FileSystem;\nimport java.nio.file.Path;\nimport java.util.ArrayList;\nimport java.util.List;\n\n\npublic class IndexTest extends HtsjdkTest {\n    private final static String CHR = \"1\";\n    private final static File MassiveIndexFile = new File(TestUtils.DATA_DIR + \"Tb.vcf.idx\");\n\n    @DataProvider(name = \"StartProvider\")\n    public Object[][] makeStartProvider() {\n        List<Object[]> tests = new ArrayList<>();\n\n        tests.add(new Object[]{1226943, 1226943, 1226943, 2000000});\n\n        return tests.toArray(new Object[][]{});\n    }\n\n    @Test(dataProvider = \"StartProvider\")\n    public void testMassiveQuery(final int start, final int mid, final int mid2, final int end) throws IOException {\n        LinearIndex index = (LinearIndex)IndexFactory.loadIndex(MassiveIndexFile.getAbsolutePath());\n\n        final List<Block> leftBlocks = index.getBlocks(CHR, start, mid);\n        final List<Block> rightBlocks = index.getBlocks(CHR, mid2, end); // gap must be big to avoid overlaps\n        final List<Block> allBlocks = index.getBlocks(CHR, start, end);\n\n        final long leftSize = leftBlocks.isEmpty() ? 0 : leftBlocks.get(0).getSize();\n        final long rightSize = rightBlocks.isEmpty() ? 0 : rightBlocks.get(0).getSize();\n        final long allSize = allBlocks.isEmpty() ? 0 : allBlocks.get(0).getSize();\n\n        Assert.assertTrue(leftSize >= 0, \"Expected leftSize to be positive \" + leftSize);\n        Assert.assertTrue(rightSize >= 0, \"Expected rightSize to be positive \" + rightSize);\n        Assert.assertTrue(allSize >= 0, \"Expected allSize to be positive \" + allSize);\n\n        Assert.assertTrue(allSize >= Math.max(leftSize,rightSize), \"Expected size of joint query \" + allSize + \" to be at least >= max of left \" + leftSize + \" and right queries \" + rightSize);\n    }\n\n    @Test()\n    public void testLoadFromStream() throws IOException {\n        LinearIndex index = (LinearIndex)IndexFactory.loadIndex(MassiveIndexFile.getAbsolutePath(), new FileInputStream(MassiveIndexFile));\n        List<String> sequenceNames = index.getSequenceNames();\n        Assert.assertEquals(sequenceNames.size(), 1);\n        Assert.assertEquals(sequenceNames.get(0), CHR);\n    }\n\n    @DataProvider(name = \"writeIndexData\")\n    public Object[][] writeIndexData() {\n        return new Object[][]{\n                {new File(\"src/test/resources/htsjdk/tribble/tabix/testTabixIndex.vcf\"), IndexFactory.IndexType.LINEAR, new VCFCodec()},\n                {new File(\"src/test/resources/htsjdk/tribble/tabix/testTabixIndex.vcf.gz\"), IndexFactory.IndexType.TABIX, new VCFCodec()},\n                {new File(\"src/test/resources/htsjdk/tribble/test.bed\"), IndexFactory.IndexType.LINEAR, new BEDCodec()}\n        };\n    }\n\n    private final static OutputStream nullOutputStrem = new OutputStream() {\n        @Override\n        public void write(int b) throws IOException { }\n    };\n\n    @Test(dataProvider = \"writeIndexData\")\n    public void testWriteIndex(final File inputFile, final IndexFactory.IndexType type, final  FeatureCodec codec) throws Exception {\n        // temp index file for this test\n        final File tempIndex = File.createTempFile(\"index\", (type == IndexFactory.IndexType.TABIX) ? FileExtensions.TABIX_INDEX : FileExtensions.TRIBBLE_INDEX);\n        tempIndex.delete();\n        tempIndex.deleteOnExit();\n        // create the index\n        final Index index = IndexFactory.createIndex(inputFile, codec, type);\n        Assert.assertFalse(tempIndex.exists());\n        // write the index to a file\n        index.write(tempIndex);\n        Assert.assertTrue(tempIndex.exists());\n        // load the generated index\n        final Index loadedIndex = IndexFactory.loadIndex(tempIndex.getAbsolutePath());\n        //TODO: This is just a smoke test; it can pass even if the generated index is unusable for queries.\n        // test that the sequences and properties are the same\n        Assert.assertEquals(loadedIndex.getSequenceNames(), index.getSequenceNames());\n        Assert.assertEquals(loadedIndex.getProperties(), index.getProperties());\n        // test that write to a stream does not blows ip\n        index.write(new LittleEndianOutputStream(nullOutputStrem));\n    }\n\n    @Test(dataProvider = \"writeIndexData\")\n    public void testWritePathIndex(final File inputFile, final IndexFactory.IndexType type, final  FeatureCodec codec) throws Exception {\n        try (final FileSystem fs = Jimfs.newFileSystem(\"test\", Configuration.unix())) {\n            // create the index\n            final Index index = IndexFactory.createIndex(inputFile, codec, type);\n            final Path path = fs.getPath(inputFile.getName() + \".index\");\n            // write the index to a file\n            index.write(path);\n\n            // test if the index does not blow up with the path constructor\n            switch (type) {\n                case TABIX:\n                    new TabixIndex(path);\n                    break;\n                case LINEAR:\n                    new LinearIndex(path);\n                    break;\n                case INTERVAL_TREE:\n                    new IntervalTreeIndex(path);\n                    break;\n            }\n        }\n    }\n\n    @Test(dataProvider = \"writeIndexData\")\n    public void testWriteBasedOnNonRegularFeatureFile(final File inputFile, final IndexFactory.IndexType type, final  FeatureCodec codec) throws Exception {\n        final File tmpFolder = IOUtil.createTempDir(\"NonRegultarFeatureFile\", null);\n        // create the index\n        final Index index = IndexFactory.createIndex(inputFile, codec, type);\n        // try to write based on the tmpFolder\n        Assert.assertThrows(IOException.class, () -> index.writeBasedOnFeatureFile(tmpFolder));\n    }\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2014 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.tribble.index.tabix;\n\nimport htsjdk.HtsjdkTest;\nimport com.google.common.io.Files;\nimport htsjdk.samtools.util.BlockCompressedOutputStream;\nimport htsjdk.samtools.util.FileExtensions;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.Interval;\nimport htsjdk.tribble.AbstractFeatureReader;\nimport htsjdk.tribble.FeatureReader;\nimport htsjdk.tribble.TestUtils;\nimport htsjdk.tribble.Tribble;\nimport htsjdk.tribble.bed.BEDCodec;\nimport htsjdk.tribble.bed.BEDFeature;\nimport htsjdk.tribble.index.IndexFactory;\nimport htsjdk.tribble.util.LittleEndianOutputStream;\nimport htsjdk.variant.variantcontext.VariantContext;\nimport htsjdk.variant.variantcontext.writer.VariantContextWriter;\nimport htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder;\nimport htsjdk.variant.vcf.VCFCodec;\nimport htsjdk.variant.vcf.VCFFileReader;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.Iterator;\nimport java.util.List;\n\npublic class TabixIndexTest extends HtsjdkTest {\n    private static final File SMALL_TABIX_FILE = new File(TestUtils.DATA_DIR, \"tabix/trioDup.vcf.gz.tbi\");\n    private static final File BIGGER_TABIX_FILE = new File(TestUtils.DATA_DIR, \"tabix/bigger.vcf.gz.tbi\");\n\n    /**\n     * Read an existing index from disk, write it to a temp file, read that in, and assert that both in-memory\n     * representations are identical.  Disk representations may not be identical due to arbitrary bin order and\n     * compression differences.\n     */\n    @Test(dataProvider = \"readWriteTestDataProvider\")\n    public void readWriteTest(final File tabixFile) throws Exception {\n        final TabixIndex index = new TabixIndex(tabixFile);\n        final File indexFile = File.createTempFile(\"TabixIndexTest.\", FileExtensions.TABIX_INDEX);\n        indexFile.deleteOnExit();\n        final LittleEndianOutputStream los = new LittleEndianOutputStream(new BlockCompressedOutputStream(indexFile));\n        index.write(los);\n        los.close();\n        final TabixIndex index2 = new TabixIndex(indexFile);\n        Assert.assertEquals(index, index2);\n        // Unfortunately, can't do byte comparison of original file and temp file, because 1) different compression\n        // levels; and more importantly, arbitrary order of bins in bin list.\n    }\n\n    @DataProvider(name = \"readWriteTestDataProvider\")\n    public Object[][] readWriteTestDataProvider() {\n        return new Object[][]{\n                {SMALL_TABIX_FILE},\n                {BIGGER_TABIX_FILE}\n        };\n    }\n\n    //TODO: This test reads an existing .tbi on a .gz, but only writes a .tbi index for a plain vcf, which\n    // tabix doesn't appear to even allow.\n    @Test\n    public void testQueryProvidedItemsAmount() throws IOException {\n        final String VCF = \"src/test/resources/htsjdk/tribble/tabix/YRI.trio.2010_07.indel.sites.vcf\";\n        // Note that we store only compressed files\n        final File plainTextVcfInputFile = new File(VCF);\n        plainTextVcfInputFile.deleteOnExit();\n        final File plainTextVcfIndexFile = new File(VCF + \".tbi\");\n        plainTextVcfIndexFile.deleteOnExit();\n        final File compressedVcfInputFile = new File(VCF + \".gz\");\n        final File compressedTbiIndexFile = new File(VCF + \".gz.tbi\");\n        final VCFFileReader compressedVcfReader = new VCFFileReader(compressedVcfInputFile, compressedTbiIndexFile);\n\n        //create plain text VCF without \"index on the fly\" option\n        final VariantContextWriter plainTextVcfWriter = new VariantContextWriterBuilder()\n                .setOptions(VariantContextWriterBuilder.NO_OPTIONS)\n                .setOutputFile(VCF)\n                .build();\n        plainTextVcfWriter.writeHeader(compressedVcfReader.getFileHeader());\n        for (VariantContext vc : compressedVcfReader) {\n            if (vc != null) plainTextVcfWriter.add(vc);\n        }\n        plainTextVcfWriter.close();\n\n        IndexFactory.createTabixIndex(plainTextVcfInputFile,\n                new VCFCodec(),\n                TabixFormat.VCF,\n                new VCFFileReader(plainTextVcfInputFile, false).getFileHeader().getSequenceDictionary()\n        ) // create TabixIndex straight from plaintext VCF\n                .write(plainTextVcfIndexFile); // write it\n\n        //TODO: you can pass in a .tbi file as the index for a plain .vcf, but if you *don't* pass in the file name and\n        //just require an index, VCFFleREader will only look for a .idx on a plain vcf\n        final VCFFileReader plainTextVcfReader = new VCFFileReader(plainTextVcfInputFile, plainTextVcfIndexFile);\n        // Now we have both plaintext and compressed VCFs with provided TabixIndex-es and could test their \"queryability\"\n\n        // magic numbers chosen from just looking in provided VCF file\n        try {\n            // just somewhere in middle of chromosome\n            Assert.assertEquals(42, countIteratedElements(compressedVcfReader.query(\"1\", 868379 - 1, 1006891 + 1)));\n            Assert.assertEquals(42, countIteratedElements(plainTextVcfReader.query(\"1\", 868379 - 1, 1006891 + 1)));\n            // chromosome start\n            Assert.assertEquals(13, countIteratedElements(compressedVcfReader.query(\"1\", 1, 836463 + 1)));\n            Assert.assertEquals(13, countIteratedElements(plainTextVcfReader.query(\"1\", 1, 836463 + 1)));\n            // chromosome end\n            Assert.assertEquals(36, countIteratedElements(compressedVcfReader.query(\"1\", 76690833 - 1, 76837502 + 11111111)));\n            Assert.assertEquals(36, countIteratedElements(plainTextVcfReader.query(\"1\", 76690833 - 1, 76837502 + 11111111)));\n            // where's no one feature in the middle of chromosome\n            Assert.assertEquals(0, countIteratedElements(compressedVcfReader.query(\"1\", 36606472 + 1, 36623523 - 1)));\n            Assert.assertEquals(0, countIteratedElements(plainTextVcfReader.query(\"1\", 36606472 + 1, 36623523 - 1)));\n            // before chromosome\n            Assert.assertEquals(0, countIteratedElements(compressedVcfReader.query(\"1\", 1, 10)));\n            Assert.assertEquals(0, countIteratedElements(plainTextVcfReader.query(\"1\", 1, 10)));\n            // after chromosome\n            Assert.assertEquals(0, countIteratedElements(compressedVcfReader.query(\"1\", 76837502 * 15, 76837502 * 16)));\n            Assert.assertEquals(0, countIteratedElements(plainTextVcfReader.query(\"1\", 76837502 * 15, 76837502 * 16)));\n        } catch (NullPointerException e) {\n            Assert.fail(\"Exception caught on querying: \", e);\n            // before fix exception was thrown from 'TabixIndex.getBlocks()' on 'chunks.size()' while 'chunks == null' for plain files\n        } finally {\n            plainTextVcfReader.close();\n            compressedVcfReader.close();\n        }\n    }\n\n    @DataProvider(name = \"bedTabixIndexTestData\")\n    public Object[][] getBedIndexFactory(){\n        // These files have accompanying .tbi files created with tabix.\n        return new Object[][] {\n                new Object[] {\n                        // BGZP BED file with no header, 2 features\n                        new File(TestUtils.DATA_DIR, \"bed/2featuresNoHeader.bed.gz\"),\n                        Arrays.asList(\n                                new Interval(\"chr1\", 1, 10),\n                                new Interval(\"chr1\", 100, 1000000))\n                },\n                new Object[] {\n                        // BGZP BED file with no header, 3 features; one feature falls in between the query intervals\n                        new File(TestUtils.DATA_DIR, \"bed/3featuresNoHeader.bed.gz\"),\n                        Arrays.asList(\n                                new Interval(\"chr1\", 1, 10),\n                                new Interval(\"chr1\", 100, 1000000))\n                },\n                new Object[] {\n                        // same file as above (BGZP BED file with no header, 3 features), but change the query to return\n                        // only the single interval for the feature that falls in between the other features\n                        new File(TestUtils.DATA_DIR, \"bed/3featuresNoHeader.bed.gz\"),\n                        Arrays.asList(\n                                new Interval(\"chr1\", 15, 20))\n                },\n                new Object[] {\n                        // BGZP BED file with one line header, 2 features\n                        new File(TestUtils.DATA_DIR, \"bed/2featuresWithHeader.bed.gz\"),\n                        Arrays.asList(\n                                new Interval(\"chr1\", 1, 10),\n                                new Interval(\"chr1\", 100, 1000000))\n                },\n        };\n    }\n\n    @Test(dataProvider = \"bedTabixIndexTestData\")\n    public void testBedTabixIndex(\n            final File inputBed,\n            final List<Interval> queryIntervals\n    ) throws Exception {\n        // copy the input file and create an index for the copy\n        final File tempDir = IOUtil.createTempDir(\"testBedTabixIndex\", null);\n        tempDir.deleteOnExit();\n        final File tmpBed = new File(tempDir, inputBed.getName());\n        Files.copy(inputBed, tmpBed);\n        tmpBed.deleteOnExit();\n        final TabixIndex tabixIndexGz = IndexFactory.createTabixIndex(tmpBed, new BEDCodec(), null);\n        tabixIndexGz.writeBasedOnFeatureFile(tmpBed);\n        final File tmpIndex = Tribble.tabixIndexFile(tmpBed);\n        tmpIndex.deleteOnExit();\n\n        // iterate over the query intervals and validate the query results\n        try(final FeatureReader<BEDFeature> originalReader =\n                    AbstractFeatureReader.getFeatureReader(inputBed.getAbsolutePath(), new BEDCodec());\n            final FeatureReader<BEDFeature> createdReader =\n                    AbstractFeatureReader.getFeatureReader(tmpBed.getAbsolutePath(), new BEDCodec()))\n        {\n            for (final Interval interval: queryIntervals) {\n                final Iterator<BEDFeature> originalIt = originalReader.query(interval.getContig(), interval.getStart(), interval.getEnd());\n                final Iterator<BEDFeature> createdIt = createdReader.query(interval.getContig(), interval.getStart(), interval.getEnd());\n                while(originalIt.hasNext()) {\n                    Assert.assertTrue(createdIt.hasNext(), \"some features not returned from query\");\n                    BEDFeature bedOrig = originalIt.next();\n                    BEDFeature bedTmp = createdIt.next();\n                    Assert.assertEquals(bedOrig.getContig(), bedTmp.getContig());\n                    Assert.assertEquals(bedOrig.getStart(), bedTmp.getStart());\n                    Assert.assertEquals(bedOrig.getEnd(), bedTmp.getEnd());\n                }\n            }\n        }\n    }\n\n    private static int countIteratedElements(final Iterator iterator) {\n        int counter = 0;\n        while (iterator.hasNext()) {\n            iterator.next();\n            counter++;\n        }\n        return counter;\n    }\n}\n", "package htsjdk.variant.vcf;\n\nimport com.google.common.jimfs.Configuration;\nimport com.google.common.jimfs.Jimfs;\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.seekablestream.SeekableStream;\nimport htsjdk.samtools.seekablestream.SeekableStreamFactory;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.tribble.TestUtils;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.FileSystem;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\n\n/**\n * Created by farjoun on 10/12/17.\n */\npublic class VCFFileReaderTest extends HtsjdkTest {\n    private static final File TEST_DATA_DIR = new File(\"src/test/resources/htsjdk/variant/\");\n\n    @DataProvider(name = \"queryableData\")\n    public Iterator<Object[]> queryableData() throws IOException {\n        List<Object[]> tests = new ArrayList<>();\n        tests.add(new Object[]{new File(TEST_DATA_DIR, \"NA12891.fp.vcf\"), false});\n        tests.add(new Object[]{new File(TEST_DATA_DIR, \"NA12891.vcf\"), false});\n        tests.add(new Object[]{VCFUtils.createTemporaryIndexedVcfFromInput(new File(TEST_DATA_DIR, \"NA12891.vcf\"), \"fingerprintcheckertest.tmp.\"), true});\n        tests.add(new Object[]{VCFUtils.createTemporaryIndexedVcfFromInput(new File(TEST_DATA_DIR, \"NA12891.vcf.gz\"), \"fingerprintcheckertest.tmp.\"), true});\n\n        return tests.iterator();\n    }\n\n    @Test(dataProvider = \"queryableData\")\n    public void testIsQueriable(final File vcf, final boolean expectedQueryable) throws Exception {\n        Assert.assertEquals(new VCFFileReader(vcf, false).isQueryable(), expectedQueryable);\n    }\n\n    @DataProvider(name = \"pathsData\")\n    Object[][] pathsData() {\n\n        final String TEST_DATA_DIR = \"src/test/resources/htsjdk/variant/\";\n        return new Object[][]{\n                // various ways to refer to a local file\n                {TEST_DATA_DIR + \"VCF4HeaderTest.vcf\", null, false, true},\n\n//                // this is almost a vcf, but not quite it's missing the #CHROM line and it has no content...\n                {TEST_DATA_DIR + \"Homo_sapiens_assembly38.tile_db_header.vcf\", null, false, false},\n\n//                // test that have indexes\n                {TEST_DATA_DIR + \"test.vcf.bgz\", TEST_DATA_DIR + \"test.vcf.bgz.tbi\", true, true},\n                {TEST_DATA_DIR + \"serialization_test.bcf\", TEST_DATA_DIR + \"serialization_test.bcf.idx\", true, true},\n                {TEST_DATA_DIR + \"test1.vcf\", TEST_DATA_DIR + \"test1.vcf.idx\", true, true},\n//\n//                // test that lack indexes (should succeed)\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf.gz\", null, false, true},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf\", null, false, true},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex but has a space.vcf\", null, false, true},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.bcf\", null, false, true},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf.bgz\", null, false, true},\n//\n//                // test that lack indexes should fail)\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf.gz\", null, true, false},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf\", null, true, false},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.bcf\", null, true, false},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf.bgz\", null, true, false},\n//\n//                // testing that v4.2 parses Source/Version fields, see issue #517\n                {TEST_DATA_DIR + \"Vcf4.2WithSourceVersionInfoFields.vcf\", null, false, true},\n//\n//                // should reject bcf v2.2 on read, see issue https://github.com/samtools/htsjdk/issues/1323\n                {TEST_DATA_DIR + \"BCFVersion22Uncompressed.bcf\", null, false, false}\n        };\n    }\n\n    @Test(dataProvider = \"pathsData\", timeOut = 60_000)\n    public void testCanOpenVCFPathReader(final String file, final String index, final boolean requiresIndex, final boolean shouldSucceed) throws Exception {\n        try (FileSystem fs = Jimfs.newFileSystem(\"test\", Configuration.unix())) {\n            final Path tribbleFileInJimfs = TestUtils.getTribbleFileInJimfs(file, index, fs);\n            try (final VCFFileReader reader = new VCFFileReader(tribbleFileInJimfs, requiresIndex)) {\n\n                final VCFHeader header = reader.getFileHeader();\n                reader.iterator().stream().forEach(\n                        v->v.getGenotypes()\n                                .stream()\n                                .count());\n            } catch (Exception e) {\n                if (shouldSucceed) {\n                    throw e;\n                } else {\n                    return;\n                }\n            }\n        }\n        // fail if a test that should have thrown didn't\n        Assert.assertTrue(shouldSucceed, \"Test should have failed but succeeded\");\n    }\n\n    @Test\n    public void testTabixFileWithEmbeddedSpaces() throws IOException {\n        final File testVCF =  new File(TEST_DATA_DIR, \"HiSeq.10000.vcf.bgz\");\n        final File testTBI =  new File(TEST_DATA_DIR, \"HiSeq.10000.vcf.bgz.tbi\");\n\n        // Copy the input files into a temporary directory with embedded spaces in the name.\n        // This test needs to include the associated .tbi file because we want to force execution\n        // of the tabix code path.\n        final File tempDir = IOUtil.createTempDir(\"test spaces\", \"\");\n        Assert.assertTrue(tempDir.getAbsolutePath().contains(\" \"));\n        tempDir.deleteOnExit();\n        final File inputVCF = new File(tempDir, \"HiSeq.10000.vcf.bgz\");\n        inputVCF.deleteOnExit();\n        final File inputTBI = new File(tempDir, \"HiSeq.10000.vcf.bgz.tbi\");\n        inputTBI.deleteOnExit();\n        IOUtil.copyFile(testVCF, inputVCF);\n        IOUtil.copyFile(testTBI, inputTBI);\n\n        try (final VCFFileReader vcfFileReader = new VCFFileReader(inputVCF)) {\n            Assert.assertNotNull(vcfFileReader.getFileHeader());\n        }\n\n    }\n\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2019 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.variant.vcf;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.util.BlockCompressedInputStream;\nimport htsjdk.samtools.util.BlockCompressedOutputStream;\nimport htsjdk.samtools.util.BlockCompressedStreamConstants;\nimport htsjdk.samtools.util.FileExtensions;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.RuntimeIOException;\nimport htsjdk.samtools.util.TerminatorlessBlockCompressedOutputStream;\nimport htsjdk.tribble.index.IndexCreator;\nimport htsjdk.tribble.index.IndexFactory;\nimport htsjdk.tribble.index.tabix.StreamBasedTabixIndexCreator;\nimport htsjdk.tribble.index.tabix.TabixFormat;\nimport htsjdk.tribble.index.tabix.TabixIndex;\nimport htsjdk.tribble.index.tabix.TabixIndexMerger;\nimport htsjdk.tribble.index.tabix.TbiEqualityChecker;\nimport htsjdk.utils.ValidationUtils;\nimport htsjdk.variant.variantcontext.VariantContext;\nimport htsjdk.variant.variantcontext.writer.Options;\nimport htsjdk.variant.variantcontext.writer.VariantContextWriter;\nimport htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder;\nimport org.testng.Assert;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class VCFMergerTest extends HtsjdkTest {\n\n    private final static Path VCF_FILE = new File(\"src/test/resources/htsjdk/variant/HiSeq.10000.vcf.bgz\").toPath();\n\n    /**\n     * Writes a <i>partitioned VCF</i>.\n     *\n     * Note that this writer is only for single-threaded use. Consider using the implementation in Disq for a partitioned VCF writer\n     * that works with multiple threads or in a distributed setting.\n     *\n     * @see TabixIndexMerger\n     */\n    static class PartitionedVCFFileWriter implements VariantContextWriter {\n        private final Path outputDir;\n        private VCFHeader header;\n        private final int recordsPerPart;\n        private final boolean compressed;\n        private long recordCount = 0;\n        private int partNumber = -1;\n        private VariantContextWriter variantContextWriter;\n\n        public PartitionedVCFFileWriter(Path outputDir, int recordsPerPart, boolean compressed) {\n            this.outputDir = outputDir;\n            this.recordsPerPart = recordsPerPart;\n            this.compressed = compressed;\n        }\n\n        @Override\n        public void writeHeader(VCFHeader header) {\n            ValidationUtils.nonNull(header.getSequenceDictionary(), \"VCF header must have a sequence dictionary\");\n            this.header = header;\n            try (OutputStream headerOut = Files.newOutputStream(outputDir.resolve(\"header\"))) {\n                OutputStream out =\n                        compressed ? new BlockCompressedOutputStream(headerOut, (Path) null) : headerOut;\n                VariantContextWriter writer =\n                        new VariantContextWriterBuilder().clearOptions().setOutputVCFStream(out).build();\n                writer.writeHeader(header);\n                out.flush(); // don't close BlockCompressedOutputStream since we don't want to write the terminator after the header\n            } catch (IOException e) {\n                throw new RuntimeIOException(e);\n            }\n        }\n\n        @Override\n        public void add(VariantContext vc) {\n            if (recordCount % recordsPerPart == 0) {\n                // start a new part\n                try {\n                    if (variantContextWriter != null) {\n                        variantContextWriter.close();\n                    }\n                    partNumber++;\n                    String partName = String.format(\"part-%05d\", partNumber);\n                    OutputStream out = Files.newOutputStream(outputDir.resolve(partName));\n                    OutputStream indexOut = Files.newOutputStream(outputDir.resolve(\".\" + partName + FileExtensions.TABIX_INDEX));\n                    IndexCreator tabixIndexCreator = new StreamBasedTabixIndexCreator(\n                                        header.getSequenceDictionary(), TabixFormat.VCF, indexOut);\n                    this.variantContextWriter = new VariantContextWriterBuilder()\n                            .setOutputStream(\n                                    compressed ? new TerminatorlessBlockCompressedOutputStream(out) : out)\n                            .setReferenceDictionary(header.getSequenceDictionary())\n                            .setIndexCreator(tabixIndexCreator)\n                            .modifyOption(Options.INDEX_ON_THE_FLY, tabixIndexCreator != null)\n                            .unsetOption(Options.DO_NOT_WRITE_GENOTYPES)\n                            .unsetOption(Options.ALLOW_MISSING_FIELDS_IN_HEADER)\n                            .unsetOption(Options.WRITE_FULL_FORMAT_FIELD)\n                            .build();\n                    variantContextWriter.setHeader(header);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n            recordCount++;\n            variantContextWriter.add(vc);\n        }\n\n        @Override\n        public void setHeader(VCFHeader header) {\n            // ignore\n        }\n\n        @Override\n        public void close() {\n            if (variantContextWriter != null) {\n                variantContextWriter.close();\n            }\n            if (compressed) {\n                // write terminator\n                try (OutputStream out = Files.newOutputStream(outputDir.resolve(\"terminator\"))) {\n                    out.write(BlockCompressedStreamConstants.EMPTY_GZIP_BLOCK);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n        }\n\n        @Override\n        public boolean checkError() {\n            return variantContextWriter != null && variantContextWriter.checkError();\n        }\n    }\n\n    /**\n     * Merge the files created by {@link PartitionedVCFFileWriter} into a single VCF file and index.\n     */\n    static class PartitionedVCFFileMerger {\n        public void merge(Path dir, Path outputVcf, Path outputTbi) throws IOException {\n            Path headerPath = dir.resolve(\"header\");\n            List<Path> vcfParts = Files.list(dir)\n                    .filter(path -> !path.toString().endsWith(FileExtensions.TABIX_INDEX)) // include header and terminator\n                    .sorted()\n                    .collect(Collectors.toList());\n            List<Path> tbiParts = Files.list(dir)\n                    .filter(path -> path.toString().endsWith(FileExtensions.TABIX_INDEX))\n                    .sorted()\n                    .collect(Collectors.toList());\n\n            Assert.assertTrue(tbiParts.size() > 1);\n\n            ValidationUtils.validateArg(vcfParts.size() - 2 == tbiParts.size(), \"Number of VCF part files does not match number of TBI files (\" + tbiParts.size() + \")\");\n\n            // merge VCF parts\n            try (OutputStream out = Files.newOutputStream(outputVcf)) {\n                for (Path vcfPart : vcfParts) {\n                    Files.copy(vcfPart, out);\n                }\n            }\n\n            // merge index parts\n            try (OutputStream out = Files.newOutputStream(outputTbi)) {\n                TabixIndexMerger tabixIndexMerger = new TabixIndexMerger(out, Files.size(headerPath));\n                int i = 1; // start from 1 since we ignore the header\n                for (Path tbiPart : tbiParts) {\n                    try (InputStream in = Files.newInputStream(tbiPart)) {\n                        TabixIndex index = new TabixIndex(new BlockCompressedInputStream(in));\n                        tabixIndexMerger.processIndex(index, Files.size(vcfParts.get(i++)));\n                    }\n                }\n                tabixIndexMerger.finish(Files.size(outputVcf));\n            }\n        }\n    }\n\n    private static Path indexVcf(Path vcf, Path tbi) throws IOException {\n        TabixIndex tabixIndex = IndexFactory.createTabixIndex(vcf.toFile(), new VCFCodec(), null);\n        tabixIndex.write(tbi);\n        return tbi;\n    }\n\n    @Test\n    public void test() throws IOException {\n        final Path outputDir = IOUtil.createTempDir(this.getClass().getSimpleName() + \".\", \".tmp\").toPath();\n        IOUtil.deleteOnExit(outputDir);\n\n        final Path outputVcf = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.COMPRESSED_VCF).toPath();\n        IOUtil.deleteOnExit(outputVcf);\n\n        final Path outputTbi = IOUtil.addExtension(outputVcf, FileExtensions.TABIX_INDEX);\n        IOUtil.deleteOnExit(outputTbi);\n\n        final Path outputTbiMerged = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.TABIX_INDEX).toPath();\n        IOUtil.deleteOnExit(outputTbiMerged);\n\n        // 1. Read an input VCF and write it out in partitioned form (header, parts, terminator)\n        try (VCFFileReader vcfReader = new VCFFileReader(VCF_FILE, false);\n             PartitionedVCFFileWriter partitionedVCFFileWriter = new PartitionedVCFFileWriter(outputDir, 2500, true)) {\n            partitionedVCFFileWriter.writeHeader(vcfReader.getFileHeader());\n            for (VariantContext vc : vcfReader) {\n                partitionedVCFFileWriter.add(vc);\n            }\n        }\n\n        // 2. Merge the partitioned VCF and index\n        new PartitionedVCFFileMerger().merge(outputDir, outputVcf, outputTbiMerged);\n\n        // 3. Index the merged VCF (using regular indexing)\n        indexVcf(outputVcf, outputTbi);\n\n        // 4. Assert that the merged index is the same as the index produced from the merged file\n\n        // Don't check for strict equality (byte identical), since the TBI files\n        // generated by the two methods have one difference: the final virtual\n        // file position in the last bin is at the end of the empty BGZF block\n        // in TBI files generated by in the usual way, and is at the start of the empty block\n        // for those generated by merging. These represent the same concrete file position, even though\n        // the virtual file positions are different.\n        TbiEqualityChecker.assertEquals(outputVcf, outputTbi, outputTbiMerged, false);\n    }\n}\n"], "fixing_code": ["/*\n * The MIT License\n *\n * Copyright (c) 2010 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools;\n\nimport htsjdk.samtools.util.CloseableIterator;\nimport htsjdk.samtools.util.CloserUtil;\nimport htsjdk.samtools.util.FileAppendStreamLRUCache;\nimport htsjdk.samtools.util.IOUtil;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.NoSuchElementException;\nimport java.util.Set;\n\n/**\n * Holds info about a mate pair for use when processing a coordinate sorted file.  When one read of a pair is encountered,\n * the caller should add a record to this map.  When the other read of a pair is encountered, the record should be removed.\n * This class assumes that reads will be processed in order of reference sequence index.  When the map is queried for\n * a record for a given reference sequence index, all the records for that sequence are loaded from temp file into RAM, so there\n * must be sufficient RAM to hold all the records for one reference sequence.  If the records are not processed in\n * reference sequence order, loading and unloading of records will cause performance to be terrible.\n * @param <KEY> KEY + reference sequence index are used to identify the record being stored or retrieved.\n * @param <REC> The type of record being retrieved.\n */\npublic class CoordinateSortedPairInfoMap<KEY, REC> implements Iterable<Map.Entry<KEY, REC>> {\n    // -1 is a valid sequence index in this case\n    private final int INVALID_SEQUENCE_INDEX = -2;\n    /**\n     * directory where files will go\n     */\n    private final File workDir = IOUtil.createTempDir(\"CSPI.tmp\").toFile();\n    private int sequenceIndexOfMapInRam = INVALID_SEQUENCE_INDEX;\n    private Map<KEY, REC> mapInRam = null;\n    private final FileAppendStreamLRUCache outputStreams;\n    private final Codec<KEY, REC> elementCodec;\n    // Key is reference index (which is in the range [-1 .. max sequence index].\n    // Value is the number of records on disk for this index.\n    private final Map<Integer, Integer> sizeOfMapOnDisk = new HashMap<Integer, Integer>();\n\n    // No other methods may be called when iteration is in progress, because iteration depends on and changes\n    // internal state.\n    private boolean iterationInProgress = false;\n\n    public CoordinateSortedPairInfoMap(final int maxOpenFiles, final Codec<KEY, REC> elementCodec) {\n        this.elementCodec = elementCodec;\n        workDir.deleteOnExit();\n        outputStreams = new FileAppendStreamLRUCache(maxOpenFiles);\n    }\n\n    /**\n     *\n     * @param sequenceIndex\n     * @param key\n     * @return The record corresponding to the given sequenceIndex and key, or null if it is not present.\n     */\n    public REC remove(final int sequenceIndex, final KEY key) {\n        if (iterationInProgress) throw new IllegalStateException(\"Cannot be called when iteration is in progress\");\n        ensureSequenceLoaded(sequenceIndex);\n        return mapInRam.remove(key);\n    }\n\n    private void ensureSequenceLoaded(final int sequenceIndex) {\n        try {\n            if (sequenceIndexOfMapInRam == sequenceIndex) {\n                return;\n            }\n\n            // Spill map in RAM to disk\n            if (mapInRam != null) {\n                final File spillFile = makeFileForSequence(sequenceIndexOfMapInRam);\n                if (spillFile.exists()) throw new IllegalStateException(spillFile + \" should not exist.\");\n                if (!mapInRam.isEmpty()) {\n                    // Do not create file or entry in sizeOfMapOnDisk if there is nothing to write.\n                    final OutputStream os = getOutputStreamForSequence(sequenceIndexOfMapInRam);\n                    elementCodec.setOutputStream(os);\n                    for (final Map.Entry<KEY, REC> entry : mapInRam.entrySet()) {\n                        elementCodec.encode(entry.getKey(), entry.getValue());\n                    }\n                    sizeOfMapOnDisk.put(sequenceIndexOfMapInRam, mapInRam.size());\n                    mapInRam.clear();\n                }\n            } else {\n                mapInRam = new HashMap<KEY, REC>();\n            }\n\n            sequenceIndexOfMapInRam = sequenceIndex;\n\n            // Load map from disk if it existed\n            File mapOnDisk = makeFileForSequence(sequenceIndex);\n            if (outputStreams.containsKey(mapOnDisk)) {\n                outputStreams.remove(mapOnDisk).close();\n            }\n            final Integer numRecords = sizeOfMapOnDisk.remove(sequenceIndex);\n            if (mapOnDisk.exists()) {\n                if (numRecords == null)\n                    throw new IllegalStateException(\"null numRecords for \" + mapOnDisk);\n                FileInputStream is = null;\n                try {\n                    is = new FileInputStream(mapOnDisk);\n                    elementCodec.setInputStream(is);\n                    for (int i = 0; i < numRecords; ++i) {\n                        final Map.Entry<KEY, REC> keyAndRecord = elementCodec.decode();\n                        if (mapInRam.containsKey(keyAndRecord.getKey()))\n                            throw new SAMException(\"Value was put into PairInfoMap more than once.  \" +\n                                    sequenceIndex + \": \" + keyAndRecord.getKey());\n                        mapInRam.put(keyAndRecord.getKey(), keyAndRecord.getValue());\n                    }\n                } finally {\n                    CloserUtil.close(is);\n                }\n                htsjdk.samtools.util.IOUtil.deleteFiles(mapOnDisk);\n            } else if (numRecords != null && numRecords > 0)\n                throw new IllegalStateException(\"Non-zero numRecords but \" + mapOnDisk + \" does not exist\");\n        } catch (IOException e) {\n            throw new SAMException(\"Error loading new map from disk.\", e);\n        }\n    }\n\n    /**\n     * Store the record with the given sequence index and key.  It is assumed that value did not previously exist\n     * in the map, and an exception is thrown (possibly at a later time) if that is not the case.\n     * @param sequenceIndex\n     * @param key\n     * @param record\n     */\n    public void put(final int sequenceIndex, final KEY key, final REC record) {\n        if (iterationInProgress) throw new IllegalStateException(\"Cannot be called when iteration is in progress\");\n        if (sequenceIndex == sequenceIndexOfMapInRam) {\n            // Store in RAM map\n            if (mapInRam.containsKey(key))\n                throw new IllegalArgumentException(\"Putting value into PairInfoMap that already existed. \" +\n                        sequenceIndex + \": \" + key);\n            mapInRam.put(key, record);\n        } else {\n            // Append to file\n            final OutputStream os = getOutputStreamForSequence(sequenceIndex);\n            elementCodec.setOutputStream(os);\n            elementCodec.encode(key, record);\n            Integer prevCount = sizeOfMapOnDisk.get(sequenceIndex);\n            if (prevCount == null) prevCount = 0;\n            sizeOfMapOnDisk.put(sequenceIndex,  prevCount + 1);\n        }\n    }\n\n    private File makeFileForSequence(final int index) {\n        final File file = new File(workDir, index + \".tmp\");\n        file.deleteOnExit();\n        return file;\n    }\n\n    private OutputStream getOutputStreamForSequence(final int mateSequenceIndex) {\n        return outputStreams.get(makeFileForSequence(mateSequenceIndex));\n    }\n\n    public int size() {\n        int total = sizeInRam();\n        for (final Integer mapSize : sizeOfMapOnDisk.values()) {\n            if (mapSize != null) {\n                total += mapSize;\n            }\n        }\n        return total;\n    }\n\n    /**\n     * @return number of elements stored in RAM.  Always <= size()\n     */\n    public int sizeInRam() {\n        return mapInRam != null? mapInRam.size(): 0;\n    }\n\n    /**\n     * Creates an iterator over all elements in map, in arbitrary order.  Elements may not be added\n     * or removed from map when iteration is in progress, nor may a second iteration be started.\n     * Iterator must be closed in order to allow normal access to the map.\n     */\n    @Override\n    public CloseableIterator<Map.Entry<KEY, REC>> iterator() {\n        if (iterationInProgress) throw new IllegalStateException(\"Cannot be called when iteration is in progress\");\n        iterationInProgress = true;\n        return new MapIterator();\n    }\n\n    private class MapIterator implements CloseableIterator<Map.Entry<KEY, REC>> {\n        private boolean closed = false;\n        private Set<Integer> referenceIndices = new HashSet<Integer>(sizeOfMapOnDisk.keySet());\n        private final Iterator<Integer> referenceIndexIterator;\n        private Iterator<Map.Entry<KEY, REC>> currentReferenceIterator = null;\n\n        private MapIterator() {\n            if (sequenceIndexOfMapInRam != INVALID_SEQUENCE_INDEX)\n                referenceIndices.add(sequenceIndexOfMapInRam);\n            referenceIndexIterator = referenceIndices.iterator();\n            advanceToNextNonEmptyReferenceIndex();\n        }\n\n        private void advanceToNextNonEmptyReferenceIndex() {\n            while (referenceIndexIterator.hasNext()) {\n                int nextReferenceIndex = referenceIndexIterator.next();\n                ensureSequenceLoaded(nextReferenceIndex);\n                if (!mapInRam.isEmpty()) {\n                    createIteratorForMapInRam();\n                    return;\n                }\n            }\n            // no more.\n            currentReferenceIterator = null;\n        }\n\n        private void createIteratorForMapInRam() {\n            currentReferenceIterator = mapInRam.entrySet().iterator();\n        }\n\n        @Override\n        public void close() {\n            closed = true;\n            iterationInProgress = false;\n        }\n\n        @Override\n        public boolean hasNext() {\n            if (closed) throw new IllegalStateException(\"Iterator has been closed\");\n            if (currentReferenceIterator != null && !currentReferenceIterator.hasNext())\n                throw new IllegalStateException(\"Should not happen\");\n            return currentReferenceIterator != null;\n        }\n\n        @Override\n        public Map.Entry<KEY, REC> next() {\n            if (closed) throw new IllegalStateException(\"Iterator has been closed\");\n            if (!hasNext()) throw new NoSuchElementException();\n            final Map.Entry<KEY, REC> ret = currentReferenceIterator.next();\n            if (!currentReferenceIterator.hasNext()) advanceToNextNonEmptyReferenceIndex();\n            return ret;\n        }\n\n        @Override\n        public void remove() {\n            throw new UnsupportedOperationException();\n        }\n    }\n\n    /**\n     * Client must implement this class, which defines the way in which records are written to and\n     * read from file.\n     */\n    public interface Codec<KEY, REC> {\n        /**\n         * Where to write encoded output\n         * @param os\n         */\n        void setOutputStream(OutputStream os);\n\n        /**\n         * Where to read encoded input from\n         * @param is\n         */\n        void setInputStream(InputStream is);\n\n        /**\n         * Write object to output stream.  If the key is part of the record, then there is no need to write\n         * it separately.\n         */\n        void encode(KEY key, REC record);\n\n        /**\n         * Read the next key and record from the input stream and convert into a java object.\n         * @return null if no more records.  Should throw exception if EOF is encountered in the middle of\n         * a record.\n         */\n        Map.Entry<KEY, REC> decode();\n\n    }\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2009 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools.util;\n\nimport htsjdk.samtools.Defaults;\nimport htsjdk.samtools.SAMException;\nimport htsjdk.samtools.seekablestream.SeekableBufferedStream;\nimport htsjdk.samtools.seekablestream.SeekableFileStream;\nimport htsjdk.samtools.seekablestream.SeekableHTTPStream;\nimport htsjdk.samtools.seekablestream.SeekableStream;\nimport htsjdk.samtools.util.nio.DeleteOnExitPathHook;\n\nimport java.io.BufferedInputStream;\nimport java.io.BufferedOutputStream;\nimport java.io.BufferedReader;\nimport java.io.BufferedWriter;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileNotFoundException;\nimport java.io.FileOutputStream;\nimport java.io.FilenameFilter;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.io.OutputStream;\nimport java.io.OutputStreamWriter;\nimport java.io.Reader;\nimport java.io.Writer;\nimport java.net.MalformedURLException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.net.URL;\nimport java.nio.charset.Charset;\nimport java.nio.file.FileSystemNotFoundException;\nimport java.nio.file.FileSystems;\nimport java.nio.file.FileVisitResult;\nimport java.nio.file.Files;\nimport java.nio.file.OpenOption;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.nio.file.SimpleFileVisitor;\nimport java.nio.file.StandardOpenOption;\nimport java.nio.file.attribute.BasicFileAttributes;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.Scanner;\nimport java.util.Set;\nimport java.util.Stack;\nimport java.util.regex.Pattern;\nimport java.util.stream.Collectors;\nimport java.util.zip.Deflater;\nimport java.util.zip.GZIPInputStream;\n\n/**\n * Miscellaneous stateless static IO-oriented methods.\n *  Also used for utility methods that wrap or aggregate functionality in Java IO.\n */\npublic class IOUtil {\n    /**\n     * @deprecated Use {@link Defaults#NON_ZERO_BUFFER_SIZE} instead.\n     */\n    @Deprecated\n    public static final int STANDARD_BUFFER_SIZE = Defaults.NON_ZERO_BUFFER_SIZE;\n\n    public static final long ONE_GB = 1024 * 1024 * 1024;\n    public static final long TWO_GBS = 2 * ONE_GB;\n    public static final long FIVE_GBS = 5 * ONE_GB;\n\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#VCF} instead.\n     */\n    @Deprecated\n    public static final String VCF_FILE_EXTENSION = FileExtensions.VCF;\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#VCF_INDEX} instead.\n     */\n    @Deprecated\n    public static final String VCF_INDEX_EXTENSION = FileExtensions.VCF_INDEX;\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#BCF} instead.\n     */\n    @Deprecated\n    public static final String BCF_FILE_EXTENSION = FileExtensions.BCF;\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#COMPRESSED_VCF} instead.\n     */\n    @Deprecated\n    public static final String COMPRESSED_VCF_FILE_EXTENSION = FileExtensions.COMPRESSED_VCF;\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#COMPRESSED_VCF_INDEX} instead.\n     */\n    @Deprecated\n    public static final String COMPRESSED_VCF_INDEX_EXTENSION = FileExtensions.COMPRESSED_VCF_INDEX;\n\n    /** Possible extensions for VCF files and related formats. */\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#VCF_LIST} instead.\n     */\n    @Deprecated\n    public static final List<String> VCF_EXTENSIONS_LIST = FileExtensions.VCF_LIST;\n\n    /**\n     * Possible extensions for VCF files and related formats.\n     * @deprecated since June 2019 Use {@link FileExtensions#VCF_LIST} instead.\n     */\n    @Deprecated\n    public static final String[] VCF_EXTENSIONS = FileExtensions.VCF_LIST.toArray(new String[0]);\n\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#INTERVAL_LIST} instead.\n     */\n    @Deprecated\n    public static final String INTERVAL_LIST_FILE_EXTENSION = FileExtensions.INTERVAL_LIST;\n\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#SAM} instead.\n     */\n    @Deprecated\n    public static final String SAM_FILE_EXTENSION = FileExtensions.SAM;\n\n    /**\n     * @deprecated since June 2019 Use {@link FileExtensions#DICT} instead.\n     */\n    @Deprecated\n    public static final String DICT_FILE_EXTENSION = FileExtensions.DICT;\n\n    /**\n     * @deprecated Use since June 2019 {@link FileExtensions#BLOCK_COMPRESSED} instead.\n     */\n    @Deprecated\n    public static final Set<String> BLOCK_COMPRESSED_EXTENSIONS = FileExtensions.BLOCK_COMPRESSED;\n\n    /** number of bytes that will be read for the GZIP-header in the function {@link #isGZIPInputStream(InputStream)} */\n    public static final int GZIP_HEADER_READ_LENGTH = 8000;\n\n    private static final OpenOption[] EMPTY_OPEN_OPTIONS = new OpenOption[0];\n\n    private static int compressionLevel = Defaults.COMPRESSION_LEVEL;\n    /**\n     * Sets the GZip compression level for subsequent GZIPOutputStream object creation.\n     * @param compressionLevel 0 <= compressionLevel <= 9\n     */\n    public static void setCompressionLevel(final int compressionLevel) {\n        if (compressionLevel < Deflater.NO_COMPRESSION || compressionLevel > Deflater.BEST_COMPRESSION) {\n            throw new IllegalArgumentException(\"Invalid compression level: \" + compressionLevel);\n        }\n        IOUtil.compressionLevel = compressionLevel;\n    }\n\n    public static int getCompressionLevel() {\n        return compressionLevel;\n    }\n\n    /**\n     * Wrap the given stream in a BufferedInputStream, if it isn't already wrapper\n     *\n     * @param stream stream to be wrapped\n     * @return A BufferedInputStream wrapping stream, or stream itself if stream instanceof BufferedInputStream.\n     */\n    public static BufferedInputStream toBufferedStream(final InputStream stream) {\n        if (stream instanceof BufferedInputStream) {\n            return (BufferedInputStream) stream;\n        } else {\n            return new BufferedInputStream(stream, Defaults.NON_ZERO_BUFFER_SIZE);\n        }\n    }\n\n    /**\n     * Transfers from the input stream to the output stream using stream operations and a buffer.\n     */\n    public static void transferByStream(final InputStream in, final OutputStream out, final long bytes) {\n        final byte[] buffer = new byte[Defaults.NON_ZERO_BUFFER_SIZE];\n        long remaining = bytes;\n\n        try {\n            while (remaining > 0) {\n                final int read = in.read(buffer, 0, (int) Math.min(buffer.length, remaining));\n                out.write(buffer, 0, read);\n                remaining -= read;\n            }\n        }\n        catch (final IOException ioe) {\n            throw new RuntimeIOException(ioe);\n        }\n    }\n\n    /**\n     * @return If Defaults.BUFFER_SIZE > 0, wrap os in BufferedOutputStream, else return os itself.\n     */\n    public static OutputStream maybeBufferOutputStream(final OutputStream os) {\n        return maybeBufferOutputStream(os, Defaults.BUFFER_SIZE);\n    }\n\n    /**\n     * @return If bufferSize > 0, wrap os in BufferedOutputStream, else return os itself.\n     */\n    public static OutputStream maybeBufferOutputStream(final OutputStream os, final int bufferSize) {\n        if (bufferSize > 0) return new BufferedOutputStream(os, bufferSize);\n        else return os;\n    }\n\n    public static SeekableStream maybeBufferedSeekableStream(final SeekableStream stream, final int bufferSize) {\n        return bufferSize > 0 ? new SeekableBufferedStream(stream, bufferSize) : stream; \n    }\n    \n    public static SeekableStream maybeBufferedSeekableStream(final SeekableStream stream) {\n        return maybeBufferedSeekableStream(stream, Defaults.BUFFER_SIZE);\n    }\n    \n    public static SeekableStream maybeBufferedSeekableStream(final File file) {\n        try {\n            return maybeBufferedSeekableStream(new SeekableFileStream(file));\n        } catch (final FileNotFoundException e) {\n            throw new RuntimeIOException(e);\n        }\n    }\n\n    public static SeekableStream maybeBufferedSeekableStream(final URL url) {\n        return maybeBufferedSeekableStream(new SeekableHTTPStream(url));\n    }\n\n    /**\n     * @return If Defaults.BUFFER_SIZE > 0, wrap is in BufferedInputStream, else return is itself.\n     */\n    public static InputStream maybeBufferInputStream(final InputStream is) {\n        return maybeBufferInputStream(is, Defaults.BUFFER_SIZE);\n    }\n\n    /**\n     * @return If bufferSize > 0, wrap is in BufferedInputStream, else return is itself.\n     */\n    public static InputStream maybeBufferInputStream(final InputStream is, final int bufferSize) {\n        if (bufferSize > 0) return new BufferedInputStream(is, bufferSize);\n        else return is;\n    }\n\n    public static Reader maybeBufferReader(Reader reader, final int bufferSize) {\n        if (bufferSize > 0) reader = new BufferedReader(reader, bufferSize);\n        return reader;\n    }\n\n    public static Reader maybeBufferReader(final Reader reader) {\n        return maybeBufferReader(reader, Defaults.BUFFER_SIZE);\n    }\n\n    public static Writer maybeBufferWriter(Writer writer, final int bufferSize) {\n        if (bufferSize > 0) writer = new BufferedWriter(writer, bufferSize);\n        return writer;\n    }\n\n    public static Writer maybeBufferWriter(final Writer writer) {\n        return maybeBufferWriter(writer, Defaults.BUFFER_SIZE);\n    }\n\n\n    /**\n     * Delete a list of files, and write a warning message if one could not be deleted.\n     *\n     * @param files Files to be deleted.\n     */\n    public static void deleteFiles(final File... files) {\n        for (final File f : files) {\n            if (!f.delete()) {\n                System.err.println(\"Could not delete file \" + f);\n            }\n        }\n    }\n\n    public static void deleteFiles(final Iterable<File> files) {\n        for (final File f : files) {\n            if (!f.delete()) {\n                System.err.println(\"Could not delete file \" + f);\n            }\n        }\n    }\n\n    public static void deletePaths(final Path... paths) {\n        for(Path path: paths){\n            deletePath(path);\n        }\n    }\n\n    /**\n     * Iterate through Paths and delete each one.\n     * Note: Path is itself an Iterable<Path>.  This method special cases that and deletes the single Path rather than\n     * Iterating the Path for targets to delete.\n     * @param paths an iterable of Paths to delete\n     */\n    public static void deletePaths(final Iterable<Path> paths) {\n        //Path is itself an Iterable<Path> which causes very confusing behavior if we don't explicitly check here.\n        if( paths instanceof Path){\n            deletePath((Path)paths);\n        }\n        paths.forEach(IOUtil::deletePath);\n    }\n\n    /**\n     * Attempt to delete a single path and log an error if it is not deleted.\n     */\n    public static void deletePath(Path path){\n        try {\n            Files.delete(path);\n        } catch (IOException e) {\n            System.err.println(\"Could not delete file \" + path);\n        }\n    }\n\n    /**\n     * @return true if the path is not a device (e.g. /dev/null or /dev/stdin), and is not\n     * an existing directory.  I.e. is is a regular path that may correspond to an existing\n     * file, or a path that could be a regular output file.\n     */\n    public static boolean isRegularPath(final File file) {\n        return !file.exists() || file.isFile();\n    }\n\n    /**\n     * @return true if the path is not a device (e.g. /dev/null or /dev/stdin), and is not\n     * an existing directory.  I.e. is is a regular path that may correspond to an existing\n     * file, or a path that could be a regular output file.\n     */\n    public static boolean isRegularPath(final Path path) {\n        return !Files.exists(path) || Files.isRegularFile(path);\n    }\n\n    /**\n     * Creates a new tmp file on one of the available temp filesystems, registers it for deletion\n     * on JVM exit and then returns it.\n     */\n    public static File newTempFile(final String prefix, final String suffix,\n                                   final File[] tmpDirs, final long minBytesFree) throws IOException {\n        File f = null;\n\n        for (int i = 0; i < tmpDirs.length; ++i) {\n            if (i == tmpDirs.length - 1 || tmpDirs[i].getUsableSpace() > minBytesFree) {\n                f = File.createTempFile(prefix, suffix, tmpDirs[i]);\n                f.deleteOnExit();\n                break;\n            }\n        }\n\n        return f;\n    }\n\n    /** Creates a new tmp file on one of the potential filesystems that has at least 5GB free. */\n    public static File newTempFile(final String prefix, final String suffix,\n                                   final File[] tmpDirs) throws IOException {\n        return newTempFile(prefix, suffix, tmpDirs, FIVE_GBS);\n    }\n\n    /** Returns a default tmp directory. */\n    public static File getDefaultTmpDir() {\n        final String user = System.getProperty(\"user.name\");\n        final String tmp = System.getProperty(\"java.io.tmpdir\");\n\n        if (tmp.endsWith(File.separatorChar + user)) return new File(tmp);\n        else return new File(tmp, user);\n    }\n\n    /**\n     * Creates a new tmp path on one of the available temp filesystems, registers it for deletion\n     * on JVM exit and then returns it.\n     */\n    public static Path newTempPath(final String prefix, final String suffix,\n            final Path[] tmpDirs, final long minBytesFree) throws IOException {\n        Path p = null;\n\n        for (int i = 0; i < tmpDirs.length; ++i) {\n            if (i == tmpDirs.length - 1 || Files.getFileStore(tmpDirs[i]).getUsableSpace() > minBytesFree) {\n                p = Files.createTempFile(tmpDirs[i], prefix, suffix);\n                deleteOnExit(p);\n                break;\n            }\n        }\n\n        return p;\n    }\n\n    /** Creates a new tmp file on one of the potential filesystems that has at least 5GB free. */\n    public static Path newTempPath(final String prefix, final String suffix,\n            final Path[] tmpDirs) throws IOException {\n        return newTempPath(prefix, suffix, tmpDirs, FIVE_GBS);\n    }\n\n    /** Returns a default tmp directory as a Path. */\n    public static Path getDefaultTmpDirPath() {\n        try {\n            final String user = System.getProperty(\"user.name\");\n            final String tmp = System.getProperty(\"java.io.tmpdir\");\n\n            final Path tmpParent = getPath(tmp);\n            if (tmpParent.endsWith(tmpParent.getFileSystem().getSeparator() + user)) {\n                return tmpParent;\n            } else {\n                return tmpParent.resolve(user);\n            }\n        } catch (IOException e) {\n            throw new RuntimeIOException(e);\n        }\n    }\n\n    /**\n     * Register a {@link Path} for deletion on JVM exit.\n     *\n     * @see DeleteOnExitPathHook\n     */\n    public static void deleteOnExit(final Path path) {\n        DeleteOnExitPathHook.add(path);\n    }\n\n    /** Returns the name of the file minus the extension (i.e. text after the last \".\" in the filename). */\n    public static String basename(final File f) {\n        final String full = f.getName();\n        final int index = full.lastIndexOf('.');\n        if (index > 0  && index > full.lastIndexOf(File.separator)) {\n            return full.substring(0, index);\n        }\n        else {\n            return full;\n        }\n    }\n    \n    /**\n     * Checks that an input is  is non-null, a URL or a file, exists, \n     * and if its a file then it is not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param input the input to check for validity\n     */\n    public static void assertInputIsValid(final String input) {\n      if (input == null) {\n        throw new IllegalArgumentException(\"Cannot check validity of null input.\");\n      }\n      if (!isUrl(input)) {\n        assertFileIsReadable(new File(input));\n      }\n    }\n    \n    /** \n     * Returns true iff the string is a url. \n     * Helps distinguish url inputs form file path inputs.\n     */\n    public static boolean isUrl(final String input) {\n      try {\n        new URL(input);\n        return true;\n      } catch (MalformedURLException e) {\n        return false;\n      }\n    }\n\n    /**\n     * Checks that a file is non-null, exists, is not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param file the file to check for readability\n     */\n    public static void assertFileIsReadable(final File file) {\n        assertFileIsReadable(toPath(file));\n    }\n\n    /**\n     * Checks that a file is non-null, exists, is not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param path the file to check for readability\n     */\n    public static void assertFileIsReadable(final Path path) {\n        if (path == null) {\n            throw new IllegalArgumentException(\"Cannot check readability of null file.\");\n        } else if (!Files.exists(path)) {\n            throw new SAMException(\"Cannot read non-existent file: \" + path.toUri().toString());\n        }\n        else if (Files.isDirectory(path)) {\n            throw new SAMException(\"Cannot read file because it is a directory: \" + path.toUri().toString());\n        }\n        else if (!Files.isReadable(path)) {\n            throw new SAMException(\"File exists but is not readable: \" + path.toUri().toString());\n        }\n    }\n\n    /**\n     * Checks that each file is non-null, exists, is not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param files the list of files to check for readability\n     */\n    public static void assertFilesAreReadable(final List<File> files) {\n        for (final File file : files) assertFileIsReadable(file);\n    }\n\n    /**\n     * Checks that each path is non-null, exists, is not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param paths the list of paths to check for readability\n     */\n    public static void assertPathsAreReadable(final List<Path> paths) {\n        for (final Path path: paths) assertFileIsReadable(path);\n    }\n\n\n    /**\n     * Checks that each string is non-null, exists or is a URL, \n     * and if it is a file then not a directory and is readable.  If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param inputs the list of files to check for readability\n     */\n    public static void assertInputsAreValid(final List<String> inputs) {\n        for (final String input : inputs) assertInputIsValid(input);\n    }\n\n    /**\n     * Checks that a file is non-null, and is either extent and writable, or non-existent but\n     * that the parent directory exists and is writable. If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param file the file to check for writability\n     */\n    public static void assertFileIsWritable(final File file) {\n        if (file == null) {\n            throw new IllegalArgumentException(\"Cannot check readability of null file.\");\n        } else if (!file.exists()) {\n            // If the file doesn't exist, check that it's parent directory does and is writable\n            final File parent = file.getAbsoluteFile().getParentFile();\n            if (!parent.exists()) {\n                throw new SAMException(\"Cannot write file: \" + file.getAbsolutePath() + \". \" +\n                        \"Neither file nor parent directory exist.\");\n            }\n            else if (!parent.isDirectory()) {\n                throw new SAMException(\"Cannot write file: \" + file.getAbsolutePath() + \". \" +\n                        \"File does not exist and parent is not a directory.\");\n            }\n            else if (!parent.canWrite()) {\n                throw new SAMException(\"Cannot write file: \" + file.getAbsolutePath() + \". \" +\n                        \"File does not exist and parent directory is not writable..\");\n            }\n        }\n        else if (file.isDirectory()) {\n            throw new SAMException(\"Cannot write file because it is a directory: \" + file.getAbsolutePath());\n        }\n        else if (!file.canWrite()) {\n            throw new SAMException(\"File exists but is not writable: \" + file.getAbsolutePath());\n        }\n    }\n\n    /**\n     * Checks that each file is non-null, and is either extent and writable, or non-existent but\n     * that the parent directory exists and is writable. If any\n     * condition is false then a runtime exception is thrown.\n     *\n     * @param files the list of files to check for writability\n     */\n    public static void assertFilesAreWritable(final List<File> files) {\n        for (final File file : files) assertFileIsWritable(file);\n    }\n\n    /**\n     * Checks that a directory is non-null, extent, writable and a directory\n     * otherwise a runtime exception is thrown.\n     *\n     * @param dir the dir to check for writability\n     */\n    public static void assertDirectoryIsWritable(final File dir) {\n        final Path asPath = IOUtil.toPath(dir);\n        assertDirectoryIsWritable(asPath);\n    }\n\n    /**\n     * Checks that a directory is non-null, extent, writable and a directory\n     * otherwise a runtime exception is thrown.\n     *\n     * @param dir the dir to check for writability\n     */\n    public static void assertDirectoryIsWritable(final Path dir) {\n        if (dir == null) {\n            throw new IllegalArgumentException(\"Cannot check readability of null file.\");\n        }\n        else if (!Files.exists(dir)) {\n            throw new SAMException(\"Directory does not exist: \" + dir.toUri().toString());\n        }\n        else if (!Files.isDirectory(dir)) {\n            throw new SAMException(\"Cannot write to directory because it is not a directory: \" + dir.toUri().toString());\n        }\n        else if (!Files.isWritable(dir)) {\n            throw new SAMException(\"Directory exists but is not writable: \" + dir.toUri().toString());\n        }\n    }\n\n    /**\n     * Checks that a directory is non-null, extent, readable and a directory\n     * otherwise a runtime exception is thrown.\n     *\n     * @param dir the dir to check for writability\n     */\n    public static void assertDirectoryIsReadable(final File dir) {\n        if (dir == null) {\n            throw new IllegalArgumentException(\"Cannot check readability of null file.\");\n        }\n        else if (!dir.exists()) {\n            throw new SAMException(\"Directory does not exist: \" + dir.getAbsolutePath());\n        }\n        else if (!dir.isDirectory()) {\n            throw new SAMException(\"Cannot read from directory because it is not a directory: \" + dir.getAbsolutePath());\n        }\n        else if (!dir.canRead()) {\n            throw new SAMException(\"Directory exists but is not readable: \" + dir.getAbsolutePath());\n        }\n    }\n\n    /**\n     * Checks that the two files are the same length, and have the same content, otherwise throws a runtime exception.\n     */\n    public static void assertFilesEqual(final File f1, final File f2) {\n        if (f1.length() != f2.length()) {\n            throw new SAMException(\"File \" + f1 + \" is \" + f1.length() + \" bytes but file \" + f2 + \" is \" + f2.length() + \" bytes.\");\n        }\n        try (\n            final FileInputStream s1 = new FileInputStream(f1);\n            final FileInputStream s2 = new FileInputStream(f2);\n            ) {\n            final byte[] buf1 = new byte[1024 * 1024];\n            final byte[] buf2 = new byte[1024 * 1024];\n            int len1;\n            while ((len1 = s1.read(buf1)) != -1) {\n                final int len2 = s2.read(buf2);\n                if (len1 != len2) {\n                    throw new SAMException(\"Unexpected EOF comparing files that are supposed to be the same length.\");\n                }\n                if (!Arrays.equals(buf1, buf2)) {\n                    throw new SAMException(\"Files \" + f1 + \" and \" + f2 + \" differ.\");\n                }\n            }\n        } catch (final IOException e) {\n            throw new SAMException(\"Exception comparing files \" + f1 + \" and \" + f2, e);\n        }\n    }\n\n    /**\n     * Checks that a file is of non-zero length\n     */\n    public static void assertFileSizeNonZero(final File file) {\n        if (file.length() == 0) {\n            throw new SAMException(file.getAbsolutePath() + \" has length 0\");\n        }\n    }\n\n    /**\n     * Opens a file for reading, decompressing it if necessary\n     *\n     * @param file  The file to open\n     * @return the input stream to read from\n     */\n    public static InputStream openFileForReading(final File file) {\n        return openFileForReading(toPath(file));\n    }\n\n    /**\n     * Opens a file for reading, decompressing it if necessary\n     *\n     * @param path  The file to open\n     * @return the input stream to read from\n     */\n    public static InputStream openFileForReading(final Path path) {\n\n        try {\n            if (hasGzipFileExtension(path))  {\n                return openGzipFileForReading(path);\n            }\n            else {\n                return Files.newInputStream(path);\n            }\n        }\n        catch (IOException ioe) {\n            throw new SAMException(\"Error opening file: \" + path, ioe);\n        }\n\n    }\n\n    /**\n     * Opens a GZIP-encoded file for reading, decompressing it if necessary\n     *\n     * @param file  The file to open\n     * @return the input stream to read from\n     */\n    public static InputStream openGzipFileForReading(final File file) {\n        return openGzipFileForReading(toPath(file));\n    }\n\n    /**\n     * Opens a GZIP-encoded file for reading, decompressing it if necessary\n     *\n     * @param path  The file to open\n     * @return the input stream to read from\n     */\n    public static InputStream openGzipFileForReading(final Path path) {\n\n        try {\n            return new GZIPInputStream(Files.newInputStream(path));\n        }\n        catch (IOException ioe) {\n            throw new SAMException(\"Error opening file: \" + path, ioe);\n        }\n    }\n\n    /**\n     * Opens a file for writing, overwriting the file if it already exists\n     *\n     * @param file  the file to write to\n     * @return the output stream to write to\n     */\n    public static OutputStream openFileForWriting(final File file) {\n        return openFileForWriting(toPath(file));\n    }\n\n    /**\n     * Opens a file for writing, gzip it if it ends with \".gz\" or \"bfq\"\n     *\n     * @param file  the file to write to\n     * @param append    whether to append to the file if it already exists (we overwrite it if false)\n     * @return the output stream to write to\n     */\n    public static OutputStream openFileForWriting(final File file, final boolean append) {\n        return openFileForWriting(toPath(file), getAppendOpenOption(append));\n    }\n\n    /**\n     * Opens a file for writing, gzip it if it ends with \".gz\" or \"bfq\"\n     *\n     * @param path  the file to write to\n     * @param openOptions options to use when opening the file\n     * @return the output stream to write to\n     */\n    public static OutputStream openFileForWriting(final Path path, OpenOption... openOptions) {\n        try {\n            if (hasGzipFileExtension(path)) {\n                return openGzipFileForWriting(path, openOptions);\n            } else {\n                return Files.newOutputStream(path, openOptions);\n            }\n        } catch (final IOException ioe) {\n            throw new SAMException(\"Error opening file for writing: \" + path.toUri().toString(), ioe);\n        }\n    }\n\n    /**\n     * check if the file name ends with .gz, .gzip, or .bfq\n     */\n    public static boolean hasGzipFileExtension(Path path) {\n        final List<String> gzippedEndings = Arrays.asList(\".gz\", \".gzip\", \".bfq\");\n        final String fileName = path.getFileName().toString();\n        return gzippedEndings.stream().anyMatch(fileName::endsWith);\n    }\n\n    /**\n     * Preferred over PrintStream and PrintWriter because an exception is thrown on I/O error\n     */\n    public static BufferedWriter openFileForBufferedWriting(final File file, final boolean append) {\n        return new BufferedWriter(new OutputStreamWriter(openFileForWriting(file, append)), Defaults.NON_ZERO_BUFFER_SIZE);\n    }\n\n    /**\n     * Preferred over PrintStream and PrintWriter because an exception is thrown on I/O error\n     */\n    public static BufferedWriter openFileForBufferedWriting(final Path path, final OpenOption ... openOptions) {\n        return new BufferedWriter(new OutputStreamWriter(openFileForWriting(path, openOptions)), Defaults.NON_ZERO_BUFFER_SIZE);\n    }\n\n    /**\n     * Preferred over PrintStream and PrintWriter because an exception is thrown on I/O error\n     */\n    public static BufferedWriter openFileForBufferedWriting(final File file) {\n        return openFileForBufferedWriting(IOUtil.toPath(file));\n    }\n\n    /**\n     * Preferred over PrintStream and PrintWriter because an exception is thrown on I/O error\n     */\n    public static BufferedWriter openFileForBufferedUtf8Writing(final File file) {\n        return openFileForBufferedUtf8Writing(IOUtil.toPath(file));\n    }\n\n    /**\n     * Preferred over PrintStream and PrintWriter because an exception is thrown on I/O error\n     */\n    public static BufferedWriter openFileForBufferedUtf8Writing(final Path path) {\n        return new BufferedWriter(new OutputStreamWriter(openFileForWriting(path), Charset.forName(\"UTF-8\")), Defaults.NON_ZERO_BUFFER_SIZE);\n    }\n\n    /**\n     * Opens a file for reading, decompressing it if necessary\n     *\n     * @param file  The file to open\n     * @return the input stream to read from\n     */\n    public static BufferedReader openFileForBufferedUtf8Reading(final File file) {\n        return new BufferedReader(new InputStreamReader(openFileForReading(file), Charset.forName(\"UTF-8\")));\n    }\n\n    /**\n     * Opens a GZIP encoded file for writing\n     *\n     * @param file  the file to write to\n     * @param append    whether to append to the file if it already exists (we overwrite it if false)\n     * @return the output stream to write to\n     */\n    public static OutputStream openGzipFileForWriting(final File file, final boolean append) {\n        return openGzipFileForWriting(IOUtil.toPath(file), getAppendOpenOption(append));\n    }\n\n    /**\n     * converts a boolean into an array containing either the append option or nothing\n     */\n    private static OpenOption[] getAppendOpenOption(boolean append) {\n        return append ? new OpenOption[]{StandardOpenOption.APPEND} : EMPTY_OPEN_OPTIONS;\n    }\n\n    /**\n     * Opens a GZIP encoded file for writing\n     *\n     * @param path the file to write to\n     * @param openOptions options to control how the file is opened\n     * @return the output stream to write to\n     */\n    public static OutputStream openGzipFileForWriting(final Path path, final OpenOption ... openOptions) {\n        try {\n            final OutputStream out = Files.newOutputStream(path, openOptions);\n            if (Defaults.BUFFER_SIZE > 0) {\n                return new CustomGzipOutputStream(out, Defaults.BUFFER_SIZE, compressionLevel);\n            } else {\n                return new CustomGzipOutputStream(out, compressionLevel);\n            }\n        } catch (final IOException ioe) {\n            throw new SAMException(\"Error opening file for writing: \" + path.toUri().toString(), ioe);\n        }\n    }\n\n    public static OutputStream openFileForMd5CalculatingWriting(final File file) {\n        return openFileForMd5CalculatingWriting(toPath(file));\n    }\n\n    public static OutputStream openFileForMd5CalculatingWriting(final Path file) {\n        return new Md5CalculatingOutputStream(IOUtil.openFileForWriting(file), file.resolve(\".md5\"));\n    }\n\n    /**\n     * Utility method to copy the contents of input to output. The caller is responsible for\n     * opening and closing both streams.\n     *\n     * @param input contents to be copied\n     * @param output destination\n     */\n    public static void copyStream(final InputStream input, final OutputStream output) {\n        try {\n            final byte[] buffer = new byte[Defaults.NON_ZERO_BUFFER_SIZE];\n            int bytesRead = 0;\n            while((bytesRead = input.read(buffer)) > 0) {\n                output.write(buffer, 0, bytesRead);\n            }\n        } catch (IOException e) {\n            throw new SAMException(\"Exception copying stream\", e);\n        }\n    }\n\n    /**\n     * Copy input to output, overwriting output if it already exists.\n     */\n    public static void copyFile(final File input, final File output) {\n        try {\n            final InputStream is = new FileInputStream(input);\n            final OutputStream os = new FileOutputStream(output);\n            copyStream(is, os);\n            os.close();\n            is.close();\n        } catch (IOException e) {\n            throw new SAMException(\"Error copying \" + input + \" to \" + output, e);\n        }\n    }\n\n    /**\n     *\n     * @param directory\n     * @param regexp\n     * @return list of files matching regexp.\n     */\n    public static File[] getFilesMatchingRegexp(final File directory, final String regexp) {\n        final Pattern pattern = Pattern.compile(regexp);\n        return getFilesMatchingRegexp(directory, pattern);\n    }\n\n    public static File[] getFilesMatchingRegexp(final File directory, final Pattern regexp) {\n        return directory.listFiles( new FilenameFilter() {\n            @Override\n            public boolean accept(final File dir, final String name) {\n                return regexp.matcher(name).matches();\n            }\n        });\n    }\n\n    /**\n     * Delete the given file or directory.  If a directory, all enclosing files and subdirs are also deleted.\n     */\n    public static boolean deleteDirectoryTree(final File fileOrDirectory) {\n        boolean success = true;\n\n        if (fileOrDirectory.isDirectory()) {\n            for (final File child : fileOrDirectory.listFiles()) {\n                success = success && deleteDirectoryTree(child);\n            }\n        }\n\n        success = success && fileOrDirectory.delete();\n        return success;\n    }\n\n    /**\n     * Returns the size (in bytes) of the file or directory and all it's children.\n     */\n    public static long sizeOfTree(final File fileOrDirectory) {\n        long total = fileOrDirectory.length();\n        if (fileOrDirectory.isDirectory()) {\n            for (final File f : fileOrDirectory.listFiles()) {\n                total += sizeOfTree(f);\n            }\n        }\n\n        return total;\n    }\n\n    /**\n     *\n     * Copies a directory tree (all subdirectories and files) recursively to a destination\n     */\n    public static void copyDirectoryTree(final File fileOrDirectory, final File destination) {\n        if (fileOrDirectory.isDirectory()) {\n            destination.mkdir();\n            for(final File f : fileOrDirectory.listFiles()) {\n                final File destinationFileOrDirectory =  new File(destination.getPath(),f.getName());\n                if (f.isDirectory()){\n                    copyDirectoryTree(f,destinationFileOrDirectory);\n                }\n                else {\n                    copyFile(f,destinationFileOrDirectory);\n                }\n            }\n        }\n    }\n\n    /**\n     * Create a temporary subdirectory in the default temporary-file directory, using the given prefix and morePrefix to generate the name.\n     * Note that this method is not completely safe, because it create a temporary file, deletes it, and then creates\n     * a directory with the same name as the file.  Should be good enough.\n     *\n     * This has been updated to avoid the problem in https://github.com/samtools/htsjdk/pull/1617\n     *\n     * @param prefix The prefix string to be used in generating the file's name;\n     * @param morePrefix This was previously a suffix but the implementation changed; may be null, in which case the morePrefix \".tmp\" will be used\n     * @return File object for new directory\n     * @deprecated  Use {@link #createTempDir(String)} instead.\n     *              It turns out the mechanism was not \"good enough\" and caused security issues, the new implementation\n     *              combines the prefix/morePrefix into a single prefix.  The security flaw is fixed but due to the now\n     *              extraneous morePrefix argument it is recommended to use the 1 argument form.\n     */\n    @Deprecated\n    public static File createTempDir(final String prefix, final String morePrefix) {\n        final String dotSeparatedSuffix = morePrefix == null ? \".tmp\" : morePrefix.startsWith(\".\") ? morePrefix : \".\" + morePrefix;\n        return createTempDir(prefix + dotSeparatedSuffix).toFile() ;\n    }\n\n    /*\n     * Create a temporary subdirectory in the default temporary-file directory, using the given prefix and suffix to generate the name.\n     * @param prefix The prefix string to be used in generating the file's name, may be null\n     */\n    public static Path createTempDir(final String prefix) {\n        try {\n            return Files.createTempDirectory(prefix);\n        } catch (final IOException e) {\n            throw new SAMException(\"Exception creating temporary directory.\", e);\n        }\n    }\n\n    /** Checks that a file exists and is readable, and then returns a buffered reader for it. */\n    public static BufferedReader openFileForBufferedReading(final File file) {\n        return openFileForBufferedReading(toPath(file));\n    }\n\n    /** Checks that a path exists and is readable, and then returns a buffered reader for it. */\n    public static BufferedReader openFileForBufferedReading(final Path path) {\n        return new BufferedReader(new InputStreamReader(openFileForReading(path)), Defaults.NON_ZERO_BUFFER_SIZE);\n    }\n\n    /** Takes a string and replaces any characters that are not safe for filenames with an underscore */\n    public static String makeFileNameSafe(final String str) {\n        return str.trim().replaceAll(\"[\\\\s!\\\"#$%&'()*/:;<=>?@\\\\[\\\\]\\\\\\\\^`{|}~]\", \"_\");\n    }\n\n    /** Returns the name of the file extension (i.e. text after the last \".\" in the filename) including the . */\n    public static String fileSuffix(final File f) {\n        final String full = f.getName();\n        final int index = full.lastIndexOf('.');\n        if (index > 0 && index > full.lastIndexOf(File.separator)) {\n            return full.substring(index);\n        } else {\n            return null;\n        }\n    }\n\n    /** Returns the full path to the file with all symbolic links resolved **/\n    public static String getFullCanonicalPath(final File file) {\n        try {\n            File f = file.getCanonicalFile();\n            String canonicalPath = \"\";\n            while (f != null  && !f.getName().equals(\"\")) {\n                canonicalPath = \"/\" + f.getName() + canonicalPath;\n                f = f.getParentFile();\n                if (f != null) f = f.getCanonicalFile();\n            }\n            return canonicalPath;\n        } catch (final IOException ioe) {\n            throw new RuntimeIOException(\"Error getting full canonical path for \" +\n                    file + \": \" + ioe.getMessage(), ioe);\n        }\n   }\n\n    /**\n     * Reads everything from an input stream as characters and returns a single String.\n     */\n    public static String readFully(final InputStream in) {\n        try {\n            final BufferedReader r = new BufferedReader(new InputStreamReader(in), Defaults.NON_ZERO_BUFFER_SIZE);\n            final StringBuilder builder = new StringBuilder(512);\n            String line = null;\n\n            while ((line = r.readLine()) != null) {\n                if (builder.length() > 0) builder.append('\\n');\n                builder.append(line);\n            }\n\n            return builder.toString();\n        }\n        catch (final IOException ioe) {\n            throw new RuntimeIOException(\"Error reading stream\", ioe);\n        }\n    }\n\n    /**\n     * Returns an iterator over the lines in a text file. The underlying resources are automatically\n     * closed when the iterator hits the end of the input, or manually by calling close().\n     *\n     * @param f a file that is to be read in as text\n     * @return an iterator over the lines in the text file\n     */\n    public static IterableOnceIterator<String> readLines(final File f) {\n        try {\n            final BufferedReader in = IOUtil.openFileForBufferedReading(f);\n\n            return new IterableOnceIterator<String>() {\n                private String next = in.readLine();\n\n                /** Returns true if there is another line to read or false otherwise. */\n                @Override public boolean hasNext() { return next != null; }\n\n                /** Returns the next line in the file or null if there are no more lines. */\n                @Override public String next() {\n                    try {\n                        final String tmp = next;\n                        next = in.readLine();\n                        if (next == null) in.close();\n                        return tmp;\n                    }\n                    catch (final IOException ioe) { throw new RuntimeIOException(ioe); }\n                }\n\n                /** Closes the underlying input stream. Not required if end of stream has already been hit. */\n                @Override public void close() throws IOException { CloserUtil.close(in); }\n            };\n        }\n        catch (final IOException e) {\n            throw new RuntimeIOException(e);\n        }\n    }\n\n    /** Returns all of the untrimmed lines in the provided file. */\n    public static List<String> slurpLines(final File file) throws FileNotFoundException {\n        return slurpLines(new FileInputStream(file));\n    }\n\n    public static List<String> slurpLines(final InputStream is) throws FileNotFoundException {\n        /** See {@link java.util.Scanner} source for origin of delimiter used here.  */\n        return tokenSlurp(is, Charset.defaultCharset(), \"\\r\\n|[\\n\\r\\u2028\\u2029\\u0085]\");\n    }\n\n    /** Convenience overload for {@link #slurp(java.io.InputStream, java.nio.charset.Charset)} using the default charset {@link java.nio.charset.Charset#defaultCharset()}. */\n    public static String slurp(final File file) throws FileNotFoundException {\n        return slurp(new FileInputStream(file));\n    }\n\n    /** Convenience overload for {@link #slurp(java.io.InputStream, java.nio.charset.Charset)} using the default charset {@link java.nio.charset.Charset#defaultCharset()}. */\n    public static String slurp(final InputStream is) {\n        return slurp(is, Charset.defaultCharset());\n    }\n\n    /** Reads all of the stream into a String, decoding with the provided {@link java.nio.charset.Charset} then closes the stream quietly. */\n    public static String slurp(final InputStream is, final Charset charSet) {\n        final List<String> tokenOrEmpty = tokenSlurp(is, charSet, \"\\\\A\");\n        return tokenOrEmpty.isEmpty() ? StringUtil.EMPTY_STRING : CollectionUtil.getSoleElement(tokenOrEmpty);\n    }\n\n    /** Tokenizes the provided input stream into memory using the given delimiter. */\n    private static List<String> tokenSlurp(final InputStream is, final Charset charSet, final String delimiterPattern) {\n        try {\n            final Scanner s = new Scanner(is, charSet.toString()).useDelimiter(delimiterPattern);\n            final LinkedList<String> tokens = new LinkedList<>();\n            while (s.hasNext()) {\n                tokens.add(s.next());\n            }\n            return tokens;\n        } finally {\n            CloserUtil.close(is);\n        }\n    }\n\n    /**\n     * Go through the files provided and if they have one of the provided file extensions pass the file into the output\n     * otherwise assume that file is a list of filenames and unfold it into the output.\n     */\n    public static List<File> unrollFiles(final Collection<File> inputs, final String... extensions) {\n        Collection<Path> paths = unrollPaths(filesToPaths(inputs), extensions);\n        return paths.stream().map(Path::toFile).collect(Collectors.toList());\n    }\n\n    /**\n     * Go through the files provided and if they have one of the provided file extensions pass the file to the output\n     * otherwise assume that file is a list of filenames and unfold it into the output (recursively).\n     */\n    public static List<Path> unrollPaths(final Collection<Path> inputs, final String... extensions) {\n        if (extensions.length < 1) throw new IllegalArgumentException(\"Must provide at least one extension.\");\n\n        final Stack<Path> stack = new Stack<>();\n        final List<Path> output = new ArrayList<>();\n        stack.addAll(inputs);\n\n        while (!stack.empty()) {\n            final Path p = stack.pop();\n            final String name = p.toString();\n            boolean matched = false;\n\n            for (final String ext : extensions) {\n                if (!matched && name.endsWith(ext)) {\n                    output.add(p);\n                    matched = true;\n                }\n            }\n\n            // If the file didn't match a given extension, treat it as a list of files\n            if (!matched) {\n                try {\n                    Files.lines(p)\n                            .map(String::trim)\n                            .filter(s -> !s.isEmpty())\n                            .forEach(s -> {\n                                        final Path innerPath;\n                                        try {\n                                            innerPath = getPath(s);\n                                            stack.push(innerPath);\n                                        } catch (IOException e) {\n                                            throw new IllegalArgumentException(\"cannot convert \" + s + \" to a Path.\", e);\n                                        }\n                                    }\n                            );\n\n                } catch (IOException e) {\n                    throw new IllegalArgumentException(\"had trouble reading from \" + p.toUri().toString(), e);\n                }\n            }\n        }\n\n        // Preserve input order (since we're using a stack above) for things that care\n        Collections.reverse(output);\n\n        return output;\n    }\n\n\n    /**\n     * Check if the given URI has a scheme.\n     *\n     * @param uriString the URI to check\n     * @return <code>true</code> if the given URI has a scheme, <code>false</code> if\n     * not, or if the URI is malformed.\n     */\n    public static boolean hasScheme(String uriString) {\n        try {\n            return new URI(uriString).getScheme() != null;\n        } catch (URISyntaxException e) {\n            return false;\n        }\n    }\n\n    /**\n     * Converts the given URI to a {@link Path} object. If the filesystem cannot be found in the usual way, then attempt\n     * to load the filesystem provider using the thread context classloader. This is needed when the filesystem\n     * provider is loaded using a URL classloader (e.g. in spark-submit).\n     *\n     * @param uriString the URI to convert\n     * @return the resulting {@code Path}\n     * @throws IOException an I/O error occurs creating the file system\n     */\n    public static Path getPath(String uriString) throws IOException {\n        URI uri = URI.create(uriString);\n        try {\n            // if the URI has no scheme, then treat as a local file, otherwise use the scheme to determine the filesystem to use\n            return uri.getScheme() == null ? Paths.get(uriString) : Paths.get(uri);\n        } catch (FileSystemNotFoundException e) {\n            ClassLoader cl = Thread.currentThread().getContextClassLoader();\n            if (cl == null) {\n                throw e;\n            }\n            return FileSystems.newFileSystem(uri, new HashMap<>(), cl).provider().getPath(uri);\n        }\n    }\n\n    public static List<Path> getPaths(List<String> uriStrings) throws RuntimeIOException {\n        return uriStrings.stream().map(s -> {\n            try {\n                return IOUtil.getPath(s);\n            } catch (IOException e) {\n                throw new RuntimeIOException(e);\n            }\n        }).collect(Collectors.toList());\n    }\n\n    /*\n     * Converts the File to a Path, preserving nullness.\n     *\n     * @param fileOrNull a File, or null\n     * @return           the corresponding Path (or null)\n     */\n    public static Path toPath(File fileOrNull) {\n        return (null == fileOrNull ? null : fileOrNull.toPath());\n    }\n\n    /** Takes a list of Files and converts them to a list of Paths\n     * Runs .toPath() on the contents of the input.\n     *\n     * @param files a {@link List} of {@link File}s to convert to {@link Path}s\n     * @return a new List containing the results of running toPath on the elements of the input\n     */\n    public static List<Path> filesToPaths(Collection<File> files){\n        return files.stream().map(File::toPath).collect(Collectors.toList());\n    }\n\n    /**\n     * Test whether a input stream looks like a GZIP input.\n     * This identifies both gzip and bgzip streams as being GZIP.\n     * @param stream the input stream.\n     * @return true if `stream` starts with a gzip signature.\n     * @throws IllegalArgumentException if `stream` cannot mark or reset the stream\n     */\n    public static boolean isGZIPInputStream(final InputStream stream) {\n        if (!stream.markSupported()) {\n            throw new IllegalArgumentException(\"isGZIPInputStream() : Cannot test a stream that doesn't support marking.\");\n        }\n        stream.mark(GZIP_HEADER_READ_LENGTH);\n\n        try {\n            final GZIPInputStream gunzip = new GZIPInputStream(stream);\n            final int ch = gunzip.read();\n            return true;\n        } catch (final IOException ioe) {\n            return false;\n        } finally {\n            try {\n                stream.reset();\n            } catch (final IOException ioe) {\n                throw new IllegalStateException(\"isGZIPInputStream(): Could not reset stream.\");\n            }\n        }\n    }\n\n    /**\n     * Adds the extension to the given path.\n     *\n     * @param path       the path to start from, eg. \"/folder/file.jpg\"\n     * @param extension  the extension to add, eg. \".bak\"\n     * @return           \"/folder/file.jpg.bak\"\n     */\n    public static Path addExtension(Path path, String extension) {\n        return path.resolveSibling(path.getFileName() + extension);\n    }\n\n    /**\n     * Checks if the provided path is block-compressed.\n     *\n     * <p>Note that using {@code checkExtension=true} would avoid the cost of opening the file, but\n     * if {@link #hasBlockCompressedExtension(String)} returns {@code false} this would not detect\n     * block-compressed files such BAM.\n     *\n     * @param path file to check if it is block-compressed.\n     * @param checkExtension if {@code true}, checks the extension before opening the file.\n     * @return {@code true} if the file is block-compressed; {@code false} otherwise.\n     * @throws IOException if there is an I/O error.\n     */\n    public static boolean isBlockCompressed(final Path path, final boolean checkExtension) throws IOException {\n        if (checkExtension && !hasBlockCompressedExtension(path)) {\n            return false;\n        }\n        try (final InputStream stream = new BufferedInputStream(Files.newInputStream(path), Math.max(Defaults.BUFFER_SIZE, BlockCompressedStreamConstants.MAX_COMPRESSED_BLOCK_SIZE))) {\n            return BlockCompressedInputStream.isValidFile(stream);\n        }\n    }\n\n    /**\n     * Checks if the provided path is block-compressed (including extension).\n     *\n     * <p>Note that block-compressed file extensions {@link FileExtensions#BLOCK_COMPRESSED} are not\n     * checked by this method.\n     *\n     * @param path file to check if it is block-compressed.\n     * @return {@code true} if the file is block-compressed; {@code false} otherwise.\n     * @throws IOException if there is an I/O error.\n     */\n    public static boolean isBlockCompressed(final Path path) throws IOException {\n        return isBlockCompressed(path, false);\n    }\n\n    /**\n     * Checks if a file ends in one of the {@link FileExtensions#BLOCK_COMPRESSED}.\n     *\n     * @param fileName string name for the file. May be an HTTP/S url.\n     *\n     * @return {@code true} if the file has a block-compressed extension; {@code false} otherwise.\n     */\n    public static boolean hasBlockCompressedExtension (final String fileName) {\n        String cleanedPath = stripQueryStringIfPathIsAnHttpUrl(fileName);\n        for (final String extension : FileExtensions.BLOCK_COMPRESSED) {\n            if (cleanedPath.toLowerCase().endsWith(extension))\n                return true;\n        }\n        return false;\n    }\n\n    /**\n     * Checks if a path ends in one of the {@link FileExtensions#BLOCK_COMPRESSED}.\n     *\n     * @param path object to extract the name from.\n     *\n     * @return {@code true} if the path has a block-compressed extension; {@code false} otherwise.\n     */\n    public static boolean hasBlockCompressedExtension(final Path path) {\n        return hasBlockCompressedExtension(path.getFileName().toString());\n    }\n\n    /**\n     * Checks if a file ends in one of the {@link FileExtensions#BLOCK_COMPRESSED}.\n     *\n     * @param file object to extract the name from.\n     *\n     * @return {@code true} if the file has a block-compressed extension; {@code false} otherwise.\n     */\n    public static boolean hasBlockCompressedExtension (final File file) {\n        return hasBlockCompressedExtension(file.getName());\n    }\n\n    /**\n     * Checks if a file ends in one of the {@link FileExtensions#BLOCK_COMPRESSED}.\n     *\n     * @param uri file as an URI.\n     *\n     * @return {@code true} if the file has a block-compressed extension; {@code false} otherwise.\n     */\n    public static boolean hasBlockCompressedExtension (final URI uri) {\n        String path = uri.getPath();\n        return hasBlockCompressedExtension(path);\n    }\n\n    /**\n     * Remove http query before checking extension\n     * Path might be a local file, in which case a '?' is a legal part of the filename.\n     * @param path a string representing some sort of path, potentially an http url\n     * @return path with no trailing queryString (ex: http://something.com/path.vcf?stuff=something => http://something.com/path.vcf)\n     */\n    private static String stripQueryStringIfPathIsAnHttpUrl(String path) {\n        if(path.startsWith(\"http://\") || path.startsWith(\"https://\")) {\n            int qIdx = path.indexOf('?');\n            if (qIdx > 0) {\n                return path.substring(0, qIdx);\n            }\n        }\n        return path;\n    }\n\n    /**\n     * Delete a directory and all files in it.\n     *\n     * @param directory The directory to be deleted (along with its subdirectories)\n     */\n    public static void recursiveDelete(final Path directory) {\n        \n        final SimpleFileVisitor<Path> simpleFileVisitor = new SimpleFileVisitor<Path>() {\n            @Override\n            public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException {\n                super.visitFile(file, attrs);\n                Files.deleteIfExists(file);\n                return FileVisitResult.CONTINUE;\n            }\n\n            @Override\n            public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException {\n                super.postVisitDirectory(dir, exc);\n                Files.deleteIfExists(dir);\n                return FileVisitResult.CONTINUE;\n            }\n        };\n\n        try {\n            Files.walkFileTree(directory, simpleFileVisitor);\n        } catch (final IOException e){\n            throw new RuntimeIOException(e);\n        }\n    }\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2019 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.cram.io.InputStreamUtils;\nimport htsjdk.samtools.seekablestream.ByteArraySeekableStream;\nimport htsjdk.samtools.seekablestream.SeekableStream;\nimport htsjdk.samtools.util.BlockCompressedStreamConstants;\nimport htsjdk.samtools.util.FileExtensions;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.ProgressLoggerInterface;\nimport htsjdk.samtools.util.RuntimeIOException;\nimport htsjdk.utils.ValidationUtils;\nimport org.testng.Assert;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class BAMMergerTest extends HtsjdkTest {\n\n    private final static Path BAM_FILE = new File(\"src/test/resources/htsjdk/samtools/BAMFileIndexTest/index_test.bam\").toPath();\n\n    /**\n     * Writes a <i>partitioned BAM</i>.\n     *\n     * Note that this writer is only for single-threaded use. Consider using the implementation in Disq for a partitioned BAM writer\n     * that works with multiple threads or in a distributed setting.\n     *\n     * @see BAMIndexMerger\n     */\n    static class PartitionedBAMFileWriter implements SAMFileWriter {\n        private final Path outputDir;\n        private final SAMFileHeader header;\n        private int recordsPerPart;\n        private long recordCount = 0;\n        private int partNumber = -1;\n        private BAMStreamWriter samStreamWriter;\n        private ProgressLoggerInterface progressLogger;\n\n        public PartitionedBAMFileWriter(Path outputDir, SAMFileHeader header, int recordsPerPart) {\n            this.outputDir = outputDir;\n            this.header = header;\n            this.recordsPerPart = recordsPerPart;\n        }\n\n        @Override\n        public void addAlignment(SAMRecord alignment) {\n            if (recordCount == 0) {\n                // write header\n                try (OutputStream out = Files.newOutputStream(outputDir.resolve(\"header\"))) {\n                    new BAMStreamWriter(out, null, null, -1, header).writeHeader(header);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n            if (recordCount % recordsPerPart == 0) {\n                // start a new part\n                try {\n                    if (samStreamWriter != null) {\n                        samStreamWriter.finish(false);\n                    }\n                    partNumber++;\n                    String partName = String.format(\"part-%05d\", partNumber);\n                    OutputStream out = Files.newOutputStream(outputDir.resolve(partName));\n                    OutputStream indexOut = Files.newOutputStream(outputDir.resolve(\".\" + partName + FileExtensions.BAI_INDEX));\n                    OutputStream sbiOut = Files.newOutputStream(outputDir.resolve(\".\" + partName + FileExtensions.SBI));\n                    long sbiGranularity = 1; // set to one so we can test merging\n                    samStreamWriter = new BAMStreamWriter(out, indexOut, sbiOut, sbiGranularity, header);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n            recordCount++;\n            samStreamWriter.writeAlignment(alignment);\n            if (progressLogger != null) {\n                progressLogger.record(alignment);\n            }\n        }\n\n        @Override\n        public SAMFileHeader getFileHeader() {\n            return header;\n        }\n\n        @Override\n        public void setProgressLogger(ProgressLoggerInterface progressLogger) {\n            this.progressLogger = progressLogger;\n        }\n\n        @Override\n        public void close() {\n            if (samStreamWriter != null) {\n                samStreamWriter.finish(false);\n            }\n            // write terminator\n            try (OutputStream out = Files.newOutputStream(outputDir.resolve(\"terminator\"))) {\n                out.write(BlockCompressedStreamConstants.EMPTY_GZIP_BLOCK);\n            } catch (IOException e) {\n                throw new RuntimeIOException(e);\n            }\n        }\n    }\n\n    /**\n     * Merge the files created by {@link PartitionedBAMFileWriter} into a single BAM file and index.\n     */\n    static class PartitionedBAMFileMerger {\n        public void merge(Path dir, Path outputBam, Path outputBai, Path outputSbi) throws IOException {\n            Path headerPath = dir.resolve(\"header\");\n            List<Path> bamParts = Files.list(dir)\n                    .filter(path -> !path.toString().endsWith(FileExtensions.BAI_INDEX) && !path.toString().endsWith(FileExtensions.SBI)) // include header and terminator\n                    .sorted()\n                    .collect(Collectors.toList());\n            List<Path> baiParts = Files.list(dir)\n                    .filter(path -> path.toString().endsWith(FileExtensions.BAI_INDEX))\n                    .sorted()\n                    .collect(Collectors.toList());\n            List<Path> sbiParts = Files.list(dir)\n                    .filter(path -> path.toString().endsWith(FileExtensions.SBI))\n                    .sorted()\n                    .collect(Collectors.toList());\n\n            Assert.assertTrue(baiParts.size() > 1);\n\n            ValidationUtils.validateArg(bamParts.size() - 2 == baiParts.size(), \"Number of BAM part files does not match number of BAI files (\" + baiParts.size() + \")\");\n\n            SAMFileHeader header = SamReaderFactory.makeDefault().open(headerPath).getFileHeader();\n\n            // merge BAM parts\n            try (OutputStream out = Files.newOutputStream(outputBam)) {\n                for (Path bamPart : bamParts) {\n                    Files.copy(bamPart, out);\n                }\n            }\n\n            // merge index parts\n            try (OutputStream out = Files.newOutputStream(outputBai)) {\n                BAMIndexMerger bamIndexMerger = new BAMIndexMerger(out, Files.size(headerPath));\n                int i = 1; // start from 1 since we ignore the header\n                for (Path baiPart : baiParts) {\n                    try (InputStream in = Files.newInputStream(baiPart)) {\n                        // read all bytes into memory since AbstractBAMFileIndex reads lazily\n                        byte[] bytes = InputStreamUtils.readFully(in);\n                        SeekableStream allIn = new ByteArraySeekableStream(bytes);\n                        AbstractBAMFileIndex index = BAMIndexMerger.openIndex(allIn, header.getSequenceDictionary());\n                        bamIndexMerger.processIndex(index, Files.size(bamParts.get(i++)));\n                    }\n                }\n                bamIndexMerger.finish(Files.size(outputBam));\n            }\n\n            // merge SBI index parts\n            try (OutputStream out = Files.newOutputStream(outputSbi)) {\n                SBIIndexMerger sbiIndexMerger = new SBIIndexMerger(out, Files.size(headerPath));\n                int i = 1; // start from 1 since we ignore the header\n                for (Path sbiPart : sbiParts) {\n                    try (InputStream in = Files.newInputStream(sbiPart)) {\n                        SBIIndex index = SBIIndex.load(in);\n                        sbiIndexMerger.processIndex(index, Files.size(bamParts.get(i++)));\n                    }\n                }\n                sbiIndexMerger.finish(Files.size(outputBam));\n            }\n        }\n    }\n\n    // index a BAM file\n    private static Path indexBam(Path bam, Path bai) throws IOException {\n        try (SamReader in =\n                     SamReaderFactory.makeDefault()\n                             .validationStringency(ValidationStringency.SILENT)\n                             .enable(SamReaderFactory.Option.INCLUDE_SOURCE_IN_RECORDS)\n                             .disable(SamReaderFactory.Option.VALIDATE_CRC_CHECKSUMS)\n                             .open(SamInputResource.of(bam))) {\n\n            final BAMIndexer indexer = new BAMIndexer(bai, in.getFileHeader());\n            for (final SAMRecord rec : in) {\n                indexer.processAlignment(rec);\n            }\n            indexer.finish();\n        }\n        BAMSBIIndexer.createIndex(bam, 1);\n        textIndexBai(bai);\n        return bai;\n    }\n\n    // create a human-readable BAI\n    private static Path textIndexBai(Path bai) {\n        Path textBai = bai.resolveSibling(bai.getFileName().toString() + \".txt\");\n        BAMIndexer.createAndWriteIndex(bai.toFile(), textBai.toFile(), true);\n        return textBai;\n    }\n\n    @Test\n    public void test() throws IOException {\n        final Path outputDir = IOUtil.createTempDir(this.getClass().getSimpleName() + \".tmp\");\n        IOUtil.deleteOnExit(outputDir);\n\n        final Path outputBam = File.createTempFile(this.getClass().getSimpleName() + \".\", \".bam\").toPath();\n        IOUtil.deleteOnExit(outputBam);\n\n        final Path outputBai = IOUtil.addExtension(outputBam, FileExtensions.BAI_INDEX);\n        IOUtil.deleteOnExit(outputBai);\n\n        final Path outputSbi = IOUtil.addExtension(outputBam, FileExtensions.SBI);\n        IOUtil.deleteOnExit(outputSbi);\n\n        final Path outputBaiMerged = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.BAI_INDEX).toPath();\n        IOUtil.deleteOnExit(outputBaiMerged);\n\n        final Path outputSbiMerged = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.SBI).toPath();\n        IOUtil.deleteOnExit(outputBaiMerged);\n\n        // 1. Read an input BAM and write it out in partitioned form (header, parts, terminator)\n        try (SamReader samReader = SamReaderFactory.makeDefault().open(BAM_FILE);\n            PartitionedBAMFileWriter partitionedBAMFileWriter = new PartitionedBAMFileWriter(outputDir, samReader.getFileHeader(), 2500)) { // BAM file has 10000 reads\n            for (SAMRecord samRecord : samReader) {\n                partitionedBAMFileWriter.addAlignment(samRecord);\n            }\n        }\n\n        // 2. Merge the partitioned BAM and index\n        new PartitionedBAMFileMerger().merge(outputDir, outputBam, outputBaiMerged, outputSbiMerged);\n        textIndexBai(outputBaiMerged); // for debugging\n\n        // 3. Index the merged BAM (using regular indexing)\n        indexBam(outputBam, outputBai);\n\n        // 4. Assert that the merged index is the same as the index produced from the merged file\n        // Check equality on object before comparing file contents to get a better indication\n        // of the difference in case they are not equal.\n        BaiEqualityChecker.assertEquals(outputBam, outputBai, outputBaiMerged);\n        Assert.assertEquals(\n                com.google.common.io.Files.toByteArray(outputBai.toFile()),\n                com.google.common.io.Files.toByteArray(outputBaiMerged.toFile()));\n\n        // 5. Assert that the merged SBI index is the same as the SBI index produced from the merged file\n        Assert.assertEquals(\n                com.google.common.io.Files.toByteArray(outputSbi.toFile()),\n                com.google.common.io.Files.toByteArray(outputSbiMerged.toFile()));\n    }\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2010 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.cram.ref.ReferenceSource;\nimport htsjdk.samtools.reference.InMemoryReferenceSequenceFile;\nimport htsjdk.samtools.reference.ReferenceSequenceFileFactory;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.Log;\nimport htsjdk.samtools.util.Log.LogLevel;\nimport org.testng.Assert;\nimport org.testng.annotations.AfterClass;\nimport org.testng.annotations.BeforeClass;\nimport org.testng.annotations.Test;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.ByteArrayOutputStream;\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.Path;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class CRAMFileWriterTest extends HtsjdkTest {\n\n    LogLevel globalLogLevel;\n    final File SAM_TOOLS_TEST_DIR = new File(\"src/test/resources/htsjdk/samtools\");\n\n    @Test(description = \"Test for lossy CRAM compression invariants.\")\n    public void lossyCramInvariantsTest() {\n        doTest(createRecords(1000));\n    }\n\n    @Test(description = \"Tests a writing records with null SAMFileHeaders\")\n    public void writeRecordsWithNullHeader() {\n        final List<SAMRecord> samRecs = createRecords(50);\n        for (final SAMRecord rec : samRecs) {\n            rec.setHeader(null);\n        }\n        doTest(samRecs);\n    }\n\n    @Test(description = \"Tests a unmapped record with sequence and quality fields\")\n    public void unmappedWithSequenceAndQualityField() throws Exception {\n        unmappedSequenceAndQualityFieldHelper(true);\n    }\n\n    @Test(description = \"Tests a unmapped record with no sequence or quality fields\")\n    public void unmappedWithNoSequenceAndQualityField() throws Exception {\n        unmappedSequenceAndQualityFieldHelper(false);\n    }\n\n    private void unmappedSequenceAndQualityFieldHelper(boolean unmappedHasBasesAndQualities) throws Exception {\n        final List<SAMRecord> list = new ArrayList<>(2);\n        final SAMRecordSetBuilder builder = new SAMRecordSetBuilder();\n\n        if (builder.getHeader().getReadGroups().isEmpty()) {\n            throw new Exception(\"Read group expected in the header\");\n        }\n\n        builder.setUnmappedHasBasesAndQualities(unmappedHasBasesAndQualities);\n        builder.addUnmappedFragment(\"test1\");\n        builder.addUnmappedPair(\"test2\");\n\n        list.addAll(builder.getRecords());\n        list.sort(new SAMRecordCoordinateComparator());\n\n        doTest(list);\n    }\n\n    private List<SAMRecord> createRecords(int count) {\n        final List<SAMRecord> list = new ArrayList<>(count);\n        final SAMRecordSetBuilder builder = new SAMRecordSetBuilder();\n        if (builder.getHeader().getReadGroups().isEmpty()) {\n            throw new IllegalStateException(\"Read group expected in the header\");\n        }\n\n        int posInRef = 1;\n        for (int i = 0; i < count / 2; i++) {\n            builder.addPair(Integer.toString(i), 0, posInRef += 1, posInRef += 3);\n        }\n        list.addAll(builder.getRecords());\n        list.sort(new SAMRecordCoordinateComparator());\n\n        return list;\n    }\n\n    private SAMFileHeader createSAMHeader(SAMFileHeader.SortOrder sortOrder) {\n        final SAMFileHeader header = new SAMFileHeader();\n        header.setSortOrder(sortOrder);\n        header.addSequence(new SAMSequenceRecord(\"chr1\", 123));\n        SAMReadGroupRecord readGroupRecord = new SAMReadGroupRecord(\"1\");\n        header.addReadGroup(readGroupRecord);\n        return header;\n    }\n\n    private ReferenceSource createReferenceSource() {\n        final byte[] refBases = new byte[1024 * 1024];\n        Arrays.fill(refBases, (byte) 'A');\n        InMemoryReferenceSequenceFile rsf = new InMemoryReferenceSequenceFile();\n        rsf.add(\"chr1\", refBases);\n        return new ReferenceSource(rsf);\n    }\n\n    private void writeRecordsToCRAM(CRAMFileWriter writer, List<SAMRecord> samRecords) {\n        for (SAMRecord record : samRecords) {\n            writer.addAlignment(record);\n        }\n    }\n\n    private void validateRecords(final List<SAMRecord> expectedRecords, ByteArrayInputStream is, ReferenceSource referenceSource) {\n        try (CRAMFileReader cReader = new CRAMFileReader(null, is, referenceSource)) {\n\n            SAMRecordIterator iterator2 = cReader.getIterator();\n            int index = 0;\n            while (iterator2.hasNext()) {\n                SAMRecord actualRecord = iterator2.next();\n                SAMRecord expectedRecord = expectedRecords.get(index++);\n\n                Assert.assertEquals(actualRecord.getReadName(), expectedRecord.getReadName());\n                Assert.assertEquals(actualRecord.getFlags(), expectedRecord.getFlags());\n                Assert.assertEquals(actualRecord.getAlignmentStart(), expectedRecord.getAlignmentStart());\n                Assert.assertEquals(actualRecord.getAlignmentEnd(), expectedRecord.getAlignmentEnd());\n                Assert.assertEquals(actualRecord.getReferenceName(), expectedRecord.getReferenceName());\n                Assert.assertEquals(actualRecord.getMateAlignmentStart(),\n                        expectedRecord.getMateAlignmentStart());\n                Assert.assertEquals(actualRecord.getMateReferenceName(),\n                        expectedRecord.getMateReferenceName());\n                Assert.assertEquals(actualRecord.getReadBases(), expectedRecord.getReadBases());\n                Assert.assertEquals(actualRecord.getBaseQualities(), expectedRecord.getBaseQualities());\n\n                Assert.assertEquals(\n                        actualRecord.getAttributes().stream().map(s -> s.tag).collect(Collectors.toSet()),\n                        expectedRecord.getAttributes().stream().map(s -> s.tag).collect(Collectors.toSet()), expectedRecord.getReadName());\n\n                actualRecord.getAttributes().forEach(tv -> {\n                    Assert.assertEquals(tv.value, expectedRecord.getAttribute(tv.tag));\n                });\n\n            }\n        }\n    }\n\n    private void doTest(final List<SAMRecord> samRecords) {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ReferenceSource refSource = createReferenceSource();\n        final ByteArrayOutputStream os = new ByteArrayOutputStream();\n\n        try (CRAMFileWriter writer = new CRAMFileWriter(os, refSource, header, null)) {\n            writeRecordsToCRAM(writer, samRecords);\n        }\n\n        validateRecords(samRecords, new ByteArrayInputStream(os.toByteArray()), refSource);\n    }\n\n    @Test(description = \"Test CRAMWriter constructor with index stream\")\n    public void testCRAMWriterWithIndex() {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ReferenceSource refSource = createReferenceSource();\n        final ByteArrayOutputStream outStream = new ByteArrayOutputStream();\n        final ByteArrayOutputStream indexStream = new ByteArrayOutputStream();\n\n        final List<SAMRecord> samRecords = createRecords(100);\n        try (CRAMFileWriter writer = new CRAMFileWriter(outStream, indexStream, refSource, header, null)) {\n            writeRecordsToCRAM(writer, samRecords);\n        }\n\n        validateRecords(samRecords, new ByteArrayInputStream(outStream.toByteArray()), refSource);\n        Assert.assertTrue(indexStream.size() != 0);\n    }\n\n    @Test(description = \"Test CRAMWriter constructor with presorted==false\")\n    public void testCRAMWriterNotPresorted() {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ReferenceSource refSource = createReferenceSource();\n        final ByteArrayOutputStream outStream = new ByteArrayOutputStream();\n        final ByteArrayOutputStream indexStream = new ByteArrayOutputStream();\n\n        final List<SAMRecord> samRecords = createRecords(100);\n        try (CRAMFileWriter writer = new CRAMFileWriter(outStream, indexStream, false, refSource, header, null)) {\n\n            // force records to not be coordinate sorted to ensure we're relying on presorted=false\n            samRecords.sort(new SAMRecordCoordinateComparator().reversed());\n            writeRecordsToCRAM(writer, samRecords);\n        }\n        // for validation, restore the sort order of the expected records so they match the order of the written records\n        samRecords.sort(new SAMRecordCoordinateComparator());\n        validateRecords(samRecords, new ByteArrayInputStream(outStream.toByteArray()), refSource);\n        Assert.assertTrue(indexStream.size() != 0);\n    }\n\n    @Test(description = \"Test CRAMWriter constructor reference required 1\", expectedExceptions = IllegalArgumentException.class)\n    public void testCRAMWriterConstructorRequiredReference_1() {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ByteArrayOutputStream outStream = new ByteArrayOutputStream();\n        try (CRAMFileWriter ignored = new CRAMFileWriter(outStream, null, header, null)) {\n        }\n    }\n\n    @Test(description = \"Test CRAMWriter constructor reference required 2\", expectedExceptions = IllegalArgumentException.class)\n    public void testCRAMWriterConstructorRequiredReference_2() {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ByteArrayOutputStream outStream = new ByteArrayOutputStream();\n        try (CRAMFileWriter ignored = new CRAMFileWriter(outStream, null, null, header, null)) {\n        }\n    }\n\n    @Test(description = \"Test CRAMWriter constructor reference required 3\", expectedExceptions = IllegalArgumentException.class)\n    public void testCRAMWriterConstructorRequiredReference_3() {\n        final SAMFileHeader header = createSAMHeader(SAMFileHeader.SortOrder.coordinate);\n        final ByteArrayOutputStream outStream = new ByteArrayOutputStream();\n        try (CRAMFileWriter ignored = new CRAMFileWriter(outStream, null, true, null, header, null)) {\n        }\n    }\n\n\n\n    @Test\n    public void test_roundtrip_tlen_preserved() throws IOException {\n        final ByteArrayOutputStream baos = new ByteArrayOutputStream();\n        final ReferenceSource source = new ReferenceSource(new File(SAM_TOOLS_TEST_DIR, \"cram_tlen.fasta\"));\n        final List<SAMRecord> records = new ArrayList<>();\n\n        try (SamReader reader = SamReaderFactory.make().open(new File(SAM_TOOLS_TEST_DIR, \"cram_tlen_reads.sorted.sam\"));\n             CRAMFileWriter writer = new CRAMFileWriter(baos, source, reader.getFileHeader(), \"test.cram\")) {\n            for (final SAMRecord record : reader) {\n                writer.addAlignment(record);\n                records.add(record);\n            }\n\n        }\n\n        try (CRAMFileReader cramReader = new CRAMFileReader(new ByteArrayInputStream(baos.toByteArray()), (File) null, source, ValidationStringency.STRICT)) {\n            final SAMRecordIterator iterator = cramReader.getIterator();\n            int i = 0;\n            while (iterator.hasNext()) {\n                final SAMRecord record1 = iterator.next();\n                final SAMRecord record2 = records.get(i++);\n                Assert.assertEquals(record1.getInferredInsertSize(), record2.getInferredInsertSize(), record1.getReadName());\n                Assert.assertEquals(record1, record2, record1.getReadName());\n            }\n            Assert.assertEquals(records.size(), i);\n        }\n    }\n\n    @Test\n    public void test_roundtrip_many_reads() throws IOException {\n\n        // Create the input file\n        final File outputDir = IOUtil.createTempDir(this.getClass().getSimpleName() + \".tmp\").toFile();\n        outputDir.deleteOnExit();\n        final Path output = new File(outputDir, \"input.cram\").toPath();\n        IOUtil.deleteOnExit(output);\n        final Path fastaDir = IOUtil.createTempDir(\"CRAMFileWriterTest\");\n        IOUtil.deleteOnExit(fastaDir);\n        final Path newFasta = fastaDir.resolve(\"input.fasta\");\n        IOUtil.deleteOnExit(newFasta);\n        IOUtil.deleteOnExit(ReferenceSequenceFileFactory.getFastaIndexFileName(newFasta));\n        IOUtil.deleteOnExit(ReferenceSequenceFileFactory.getDefaultDictionaryForReferenceSequence(newFasta));\n\n        final SAMRecordSetBuilder samRecordSetBuilder = new SAMRecordSetBuilder();\n        samRecordSetBuilder.setHeader(SAMRecordSetBuilder.makeDefaultHeader(SAMFileHeader.SortOrder.coordinate, 10_000, true));\n        samRecordSetBuilder.writeRandomReference(newFasta);\n\n        final List<SAMRecord> records = new ArrayList<>();\n\n        try (SAMFileWriter writer = new SAMFileWriterFactory().makeWriter(samRecordSetBuilder.getHeader(), true, output, newFasta)) {\n\n            // make sure we don't write reads that go off the end of the reference, which is 10,000 bases\n            for (int position = 1; position <= 9900; position += 1) {\n                samRecordSetBuilder.addFrag(\"read_\" + position, 0, position, false);\n            }\n            samRecordSetBuilder.getRecords().forEach(r -> {\n                writer.addAlignment(r);\n                records.add(r);\n            });\n        }\n\n        try (SamReader cramReader = SamReaderFactory.make().referenceSequence(newFasta).open(output)) {\n            final SAMRecordIterator iterator = cramReader.iterator();\n            int i = 0;\n            while (iterator.hasNext()) {\n                final SAMRecord record1 = iterator.next();\n                final SAMRecord record2 = records.get(i++);\n                Assert.assertEquals(record1.getInferredInsertSize(), record2.getInferredInsertSize(), record1.getReadName());\n                Assert.assertEquals(record1, record2, record1.getReadName());\n            }\n            Assert.assertEquals(records.size(), i);\n        }\n    }\n\n    @Test\n    public void testCRAMQuerySort() throws IOException {\n        final File input = new File(SAM_TOOLS_TEST_DIR, \"cram_query_sorted.cram\");\n        final File reference = new File(SAM_TOOLS_TEST_DIR, \"cram_query_sorted.fasta\");\n        final File outputFile = File.createTempFile(\"tmp.\", \".cram\");\n        final SamReaderFactory samReaderFactory = SamReaderFactory.makeDefault().referenceSequence(reference);\n\n        try (final SamReader reader = samReaderFactory.open(input);\n             final SAMFileWriter writer = new SAMFileWriterFactory().makeWriter(reader.getFileHeader().clone(), false, outputFile, reference)) {\n            for (final SAMRecord rec : reader) {\n                writer.addAlignment(rec);\n            }\n        }\n\n        try (final SamReader outReader = samReaderFactory.open(outputFile)) {\n            String prevName = null;\n            for (final SAMRecord rec : outReader) {\n                if (prevName == null) {\n                    prevName = rec.getReadName();\n                    continue;\n                }\n                // test if the read names are sorted alphabetically:\n                Assert.assertTrue(rec.getReadName().compareTo(prevName) >= 0);\n            }\n        }\n    }\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2019 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.cram.CRAIIndex;\nimport htsjdk.samtools.cram.CRAIIndexMerger;\nimport htsjdk.samtools.cram.build.CramIO;\nimport htsjdk.samtools.cram.common.CramVersions;\nimport htsjdk.samtools.cram.ref.CRAMReferenceSource;\nimport htsjdk.samtools.cram.ref.ReferenceSource;\nimport htsjdk.samtools.seekablestream.SeekablePathStream;\nimport htsjdk.samtools.seekablestream.SeekableStream;\nimport htsjdk.samtools.util.FileExtensions;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.ProgressLoggerInterface;\nimport htsjdk.samtools.util.RuntimeIOException;\nimport htsjdk.utils.ValidationUtils;\nimport org.testng.Assert;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class CRAMMergerTest extends HtsjdkTest {\n\n    private final static Path CRAM_FILE = new File(\"src/test/resources/htsjdk/samtools/cram/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram\").toPath();\n    private final static Path CRAM_REF = new File(\"src/test/resources/htsjdk/samtools/reference/human_g1k_v37.20.21.fasta.gz\").toPath();\n\n    /**\n     * Writes a <i>partitioned CRAM</i>.\n     *\n     * Technically, it is not valid to concatenate non-hidden files to create a CRAM file from a partitioned CRAM file, since each slice header has a record\n     * counter acting a a sequential index of records in the file, which would be incorrect if the file was created by concatenation. The implementation\n     * here ignores this complication.\n     *\n     * Note that this writer is only for single-threaded use. Consider using the implementation in Disq for a partitioned CRAM writer\n     * that works with multiple threads or in a distributed setting.\n     *\n     * @see CRAIIndexMerger\n     */\n    static class PartitionedCRAMFileWriter implements SAMFileWriter {\n        private final Path outputDir;\n        private final CRAMReferenceSource referenceSource;\n        private final SAMFileHeader header;\n        private int recordsPerPart;\n        private long recordCount = 0;\n        private int partNumber = -1;\n        private CRAMContainerStreamWriter samStreamWriter;\n        private ProgressLoggerInterface progressLogger;\n\n        public PartitionedCRAMFileWriter(Path outputDir, CRAMReferenceSource referenceSource, SAMFileHeader header, int recordsPerPart) {\n            this.outputDir = outputDir;\n            this.referenceSource = referenceSource;\n            this.header = header;\n            this.recordsPerPart = recordsPerPart;\n        }\n\n        @Override\n        public void addAlignment(SAMRecord alignment) {\n            if (recordCount == 0) {\n                // write header\n                try (OutputStream out = Files.newOutputStream(outputDir.resolve(\"header\"))) {\n                    CRAMContainerStreamWriter cramWriter = new CRAMContainerStreamWriter(out, null, referenceSource, this.header, \"header\");\n                    cramWriter.writeHeader(this.header);\n                    cramWriter.finish(false);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n            if (recordCount % recordsPerPart == 0) {\n                // start a new part\n                try {\n                    if (samStreamWriter != null) {\n                        samStreamWriter.finish(false);\n                    }\n                    partNumber++;\n                    String partName = String.format(\"part-%05d\", partNumber);\n                    OutputStream out = Files.newOutputStream(outputDir.resolve(partName));\n                    OutputStream indexOut = Files.newOutputStream(outputDir.resolve(\".\" + partName + FileExtensions.CRAM_INDEX));\n                    CRAMCRAIIndexer indexer = indexOut == null ? null : new CRAMCRAIIndexer(indexOut, header);\n                    samStreamWriter = new CRAMContainerStreamWriter(out, referenceSource, header, partName, indexer);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n            recordCount++;\n            samStreamWriter.writeAlignment(alignment);\n            if (progressLogger != null) {\n                progressLogger.record(alignment);\n            }\n        }\n\n        @Override\n        public SAMFileHeader getFileHeader() {\n            return header;\n        }\n\n        @Override\n        public void setProgressLogger(ProgressLoggerInterface progressLogger) {\n            this.progressLogger = progressLogger;\n        }\n\n        @Override\n        public void close() {\n            if (samStreamWriter != null) {\n                samStreamWriter.finish(false);\n            }\n            // write terminator\n            try (OutputStream out = Files.newOutputStream(outputDir.resolve(\"terminator\"))) {\n                CramIO.writeCramEOF(CramVersions.DEFAULT_CRAM_VERSION, out);\n            } catch (IOException e) {\n                throw new RuntimeIOException(e);\n            }\n        }\n    }\n\n    /**\n     * Merge the files created by {@link PartitionedCRAMFileWriter} into a single CRAM file and index.\n     */\n    static class PartitionedCRAMFileMerger {\n        public void merge(Path dir, Path outputCram, Path outputCrai) throws IOException {\n            Path headerPath = dir.resolve(\"header\");\n            List<Path> cramParts = Files.list(dir)\n                    .filter(path -> !path.toString().endsWith(FileExtensions.CRAM_INDEX)) // include header and terminator\n                    .sorted()\n                    .collect(Collectors.toList());\n            List<Path> craiParts = Files.list(dir)\n                    .filter(path -> path.toString().endsWith(FileExtensions.CRAM_INDEX))\n                    .sorted()\n                    .collect(Collectors.toList());\n\n            Assert.assertTrue(craiParts.size() > 1);\n\n            ValidationUtils.validateArg(cramParts.size() - 2 == craiParts.size(), \"Number of CRAM part files does not match number of CRAI files (\" + craiParts.size() + \")\");\n\n            // merge CRAM parts\n            try (OutputStream out = Files.newOutputStream(outputCram)) {\n                for (Path cramPart : cramParts) {\n                    Files.copy(cramPart, out);\n                }\n            }\n\n            // merge index parts\n            try (OutputStream out = Files.newOutputStream(outputCrai)) {\n                CRAIIndexMerger craiIndexMerger = new CRAIIndexMerger(out, Files.size(headerPath));\n                int i = 1; // start from 1 since we ignore the header\n                for (Path craiPart : craiParts) {\n                    try (InputStream in = Files.newInputStream(craiPart)) {\n                        CRAIIndex index = CRAMCRAIIndexer.readIndex(in);\n                        craiIndexMerger.processIndex(index, Files.size(cramParts.get(i++)));\n                    }\n                }\n                craiIndexMerger.finish(Files.size(outputCram));\n            }\n        }\n    }\n\n    private static Path indexCram(Path cram, Path crai) throws IOException {\n        try (SeekableStream in = new SeekablePathStream(cram);\n             OutputStream out = Files.newOutputStream(crai)) {\n            CRAMCRAIIndexer.writeIndex(in, out);\n        }\n        return crai;\n    }\n\n    @Test\n    public void test() throws IOException {\n        final Path outputDir = IOUtil.createTempDir(this.getClass().getSimpleName() + \".tmp\");\n        IOUtil.deleteOnExit(outputDir);\n\n        final Path outputCram = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.CRAM).toPath();\n        IOUtil.deleteOnExit(outputCram);\n\n        final Path outputCrai = IOUtil.addExtension(outputCram, FileExtensions.CRAM_INDEX);\n        IOUtil.deleteOnExit(outputCrai);\n\n        final Path outputCraiMerged = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.CRAM_INDEX).toPath();\n        IOUtil.deleteOnExit(outputCraiMerged);\n\n        // 1. Read an input CRAM and write it out in partitioned form (header, parts, terminator)\n        ReferenceSource referenceSource = new ReferenceSource(CRAM_REF);\n        try (SamReader samReader = SamReaderFactory.makeDefault().referenceSource(referenceSource).open(CRAM_FILE);\n             PartitionedCRAMFileWriter partitionedCRAMFileWriter = new PartitionedCRAMFileWriter(outputDir, referenceSource, samReader.getFileHeader(), 250)) {\n            for (SAMRecord samRecord : samReader) {\n                partitionedCRAMFileWriter.addAlignment(samRecord);\n            }\n        }\n\n        // 2. Merge the partitioned CRAM and index\n        new PartitionedCRAMFileMerger().merge(outputDir, outputCram, outputCraiMerged);\n\n        // 3. Index the merged CRAM (using regular indexing)\n        indexCram(outputCram, outputCrai);\n\n        // 4. Assert that the merged index is the same as the index produced from the merged file\n        Assert.assertEquals(\n                com.google.common.io.Files.toByteArray(outputCrai.toFile()),\n                com.google.common.io.Files.toByteArray(outputCraiMerged.toFile()));\n    }\n}\n", "/*\n * The MIT License (MIT)\n *\n * Copyright (c) 2017 Daniel Gomez-Sanchez\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in all\n * copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n */\n\npackage htsjdk.samtools.reference;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.util.IOUtil;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.util.List;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\n/**\n * @author Daniel Gomez-Sanchez (magicDGS)\n */\npublic class FastaSequenceIndexCreatorTest extends HtsjdkTest {\n    private static File TEST_DATA_DIR = new File(\"src/test/resources/htsjdk/samtools/reference\");\n\n\n    @DataProvider(name = \"indexedSequences\")\n    public Object[][] getIndexedSequences() {\n        return new Object[][]{\n                {new File(TEST_DATA_DIR, \"Homo_sapiens_assembly18.trimmed.fasta\")},\n                {new File(TEST_DATA_DIR, \"Homo_sapiens_assembly18.trimmed.fasta.gz\")},\n                {new File(TEST_DATA_DIR, \"header_with_white_space.fasta\")},\n                {new File(TEST_DATA_DIR, \"crlf.fasta\")}\n        };\n    }\n\n    @Test(dataProvider = \"indexedSequences\")\n    public void testBuildFromFasta(final File indexedFile) throws Exception {\n        final FastaSequenceIndex original = new FastaSequenceIndex(new File(indexedFile.getAbsolutePath() + \".fai\"));\n        final FastaSequenceIndex build = FastaSequenceIndexCreator.buildFromFasta(indexedFile.toPath());\n        Assert.assertEquals(original, build);\n    }\n\n    @Test(dataProvider = \"indexedSequences\")\n    public void testCreate(final File indexedFile) throws Exception {\n        // copy the file to index\n        final File tempDir = IOUtil.createTempDir(\"FastaSequenceIndexCreatorTest.testCreate\").toFile();\n        final File copied = new File(tempDir, indexedFile.getName());\n        copied.deleteOnExit();\n        Files.copy(indexedFile.toPath(), copied.toPath());\n\n        // create the index for the copied file\n        FastaSequenceIndexCreator.create(copied.toPath(), false);\n\n        // test if the expected .fai and the created one are the same\n        final File expectedFai = new File(indexedFile.getAbsolutePath() +  \".fai\");\n        final File createdFai = new File(copied.getAbsolutePath() + \".fai\");\n\n        // read all the files and compare line by line\n        try(final Stream<String> expected = Files.lines(expectedFai.toPath());\n                final Stream<String> created = Files.lines(createdFai.toPath())) {\n            final List<String> expectedLines = expected.filter(String::isEmpty).collect(Collectors.toList());\n            final List<String> createdLines = created.filter(String::isEmpty).collect(Collectors.toList());\n            Assert.assertEquals(expectedLines, createdLines);\n        }\n\n        // load the tmp index and check that both are the same\n        Assert.assertEquals(new FastaSequenceIndex(createdFai), new FastaSequenceIndex(expectedFai));\n    }\n\n}", "package htsjdk.samtools.seekablestream;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.TestUtil;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.net.MalformedURLException;\nimport java.net.URISyntaxException;\nimport java.net.URL;\nimport java.nio.file.Paths;\n\npublic class SeekableStreamFactoryTest extends HtsjdkTest {\n    private static final File TEST_DATA_DIR = new File(\"src/test/resources/htsjdk/samtools\");\n\n    @Test\n    public void testIsFilePath() {\n        Assert.assertEquals(SeekableStreamFactory.isFilePath(\"x\"), true);\n        Assert.assertEquals(SeekableStreamFactory.isFilePath(\"\"), true);\n        Assert.assertEquals(SeekableStreamFactory.isFilePath(\"http://broadinstitute.org\"), false);\n        Assert.assertEquals(SeekableStreamFactory.isFilePath(\"https://broadinstitute.org\"), false);\n        Assert.assertEquals(SeekableStreamFactory.isFilePath(\"ftp://broadinstitute.org\"), false);\n    }\n\n    @DataProvider(name=\"getStreamForData\")\n    public Object[][] getStreamForData() throws MalformedURLException {\n        return new Object[][] {\n                { new File(TEST_DATA_DIR, \"BAMFileIndexTest/index_test.bam\").getAbsolutePath(),\n                        new File(TEST_DATA_DIR, \"BAMFileIndexTest/index_test.bam\").getAbsolutePath() },\n                { new File(TEST_DATA_DIR, \"cram_with_bai_index.cram\").getAbsolutePath(),\n                        new File(TEST_DATA_DIR, \"cram_with_bai_index.cram\").getAbsolutePath() },\n                { new URL(\"file://\" + new File(TEST_DATA_DIR, \"cram_with_bai_index.cram\").getAbsolutePath()).toExternalForm(),\n                        new File(TEST_DATA_DIR, \"cram_with_bai_index.cram\").getAbsolutePath() },\n                { new URL(TestUtil.BASE_URL_FOR_HTTP_TESTS + \"index_test.bam\").toExternalForm(),\n                        new URL(TestUtil.BASE_URL_FOR_HTTP_TESTS + \"index_test.bam\").toExternalForm() },\n                { new URL(TestUtil.BASE_URL_FOR_HTTP_TESTS + \"index_test.bam.bai\").toExternalForm(),\n                       new URL(TestUtil.BASE_URL_FOR_HTTP_TESTS + \"index_test.bam.bai\").toExternalForm() }\n        };\n    }\n\n    @Test(dataProvider = \"getStreamForData\")\n    public void testGetStreamFor(final String path, final String expectedPath) throws IOException {\n        Assert.assertEquals(SeekableStreamFactory.getInstance().getStreamFor(path).getSource(), expectedPath);\n    }\n\n    @Test\n    public void testPathWithEmbeddedSpace() throws IOException {\n        final File testBam =  new File(TEST_DATA_DIR, \"BAMFileIndexTest/index_test.bam\");\n\n        //create a temp dir with a space in the name and copy the test file there\n        final File tempDir = IOUtil.createTempDir(\"test spaces\").toFile();\n        Assert.assertTrue(tempDir.getAbsolutePath().contains(\" \"));\n        tempDir.deleteOnExit();\n        final File inputBam = new File(tempDir, \"index_test.bam\");\n        inputBam.deleteOnExit();\n        IOUtil.copyFile(testBam, inputBam);\n\n        // make sure the input string we use is URL-encoded\n        final String inputString = Paths.get(inputBam.getAbsolutePath()).toUri().toString();\n        Assert.assertFalse(inputString.contains(\" \"));\n        Assert.assertTrue(inputString.contains(\"%20\"));\n\n        try (final SeekableStream seekableStream =\n                     SeekableStreamFactory.getInstance().getStreamFor(inputString)) {\n            final int BYTES_TO_READ = 10;\n            Assert.assertEquals(seekableStream.read(new byte[BYTES_TO_READ], 0,BYTES_TO_READ), BYTES_TO_READ);\n        }\n\n    }\n\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2009 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.samtools.util;\n\nimport htsjdk.HtsjdkTest;\nimport com.google.common.jimfs.Configuration;\nimport com.google.common.jimfs.Jimfs;\n\nimport java.io.*;\nimport java.net.URI;\nimport java.nio.file.FileSystem;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.nio.file.spi.FileSystemProvider;\n\nimport htsjdk.samtools.SAMException;\nimport org.testng.Assert;\nimport org.testng.annotations.AfterClass;\nimport org.testng.annotations.BeforeClass;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.lang.IllegalArgumentException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.stream.Collectors;\nimport java.util.Random;\nimport java.util.zip.GZIPOutputStream;\n\n\npublic class IOUtilTest extends HtsjdkTest {\n\n\n    private static final Path TEST_DATA_DIR = Paths.get (\"src/test/resources/htsjdk/samtools/io/\");\n    private static final Path TEST_VARIANT_DIR = Paths.get(\"src/test/resources/htsjdk/variant/\");\n    private static final Path SLURP_TEST_FILE = TEST_DATA_DIR.resolve(\"slurptest.txt\");\n    private static final Path EMPTY_FILE = TEST_DATA_DIR.resolve(\"empty.txt\");\n    private static final Path FIVE_SPACES_THEN_A_NEWLINE_THEN_FIVE_SPACES_FILE = TEST_DATA_DIR.resolve(\"5newline5.txt\");\n    private static final List<String> SLURP_TEST_LINES = Arrays.asList(\"bacon   and rice   \", \"for breakfast  \", \"wont you join me\");\n    private static final String SLURP_TEST_LINE_SEPARATOR = \"\\n\";\n    private static final String TEST_FILE_PREFIX = \"htsjdk-IOUtilTest\";\n    private static final String[] TEST_FILE_EXTENSIONS = {\".txt\", \".txt.gz\"};\n    private static final String TEST_STRING = \"bar!\";\n\n    private File existingTempFile;\n    private String systemUser;\n    private String systemTempDir;\n    private FileSystem inMemoryFileSystem;\n    private static Path WORDS_LONG;\n\n    @BeforeClass\n    public void setUp() throws IOException {\n        existingTempFile = File.createTempFile(\"FiletypeTest.\", \".tmp\");\n        existingTempFile.deleteOnExit();\n        systemTempDir = System.getProperty(\"java.io.tmpdir\");\n        final File tmpDir = new File(systemTempDir);\n        inMemoryFileSystem = Jimfs.newFileSystem(Configuration.unix());;\n        if (!tmpDir.isDirectory()) tmpDir.mkdir();\n        if (!tmpDir.isDirectory())\n            throw new RuntimeException(\"java.io.tmpdir (\" + systemTempDir + \") is not a directory\");\n        systemUser = System.getProperty(\"user.name\");\n        //build long file of random words for compression testing\n        WORDS_LONG = Files.createTempFile(\"words_long\", \".txt\");\n        WORDS_LONG.toFile().deleteOnExit();\n        final List<String> wordsList = Files.lines(TEST_DATA_DIR.resolve(\"dictionary_english_short.dic\")).collect(Collectors.toList());\n        final int numberOfWords = 300000;\n        final int seed = 345987345;\n        final Random rand = new Random(seed);\n        try (final BufferedWriter writer = Files.newBufferedWriter(WORDS_LONG)) {\n            for (int i = 0; i < numberOfWords; i++) {\n                writer.write(wordsList.get(rand.nextInt(wordsList.size())));\n            }\n        }\n    }\n\n    @AfterClass\n    public void tearDown() throws IOException {\n        // reset java properties to original\n        System.setProperty(\"java.io.tmpdir\", systemTempDir);\n        System.setProperty(\"user.name\", systemUser);\n        inMemoryFileSystem.close();\n    }\n\n    @Test\n    public void testFileReadingAndWriting() throws IOException {\n        String randomizedTestString = TEST_STRING + System.currentTimeMillis();\n        for (String ext : TEST_FILE_EXTENSIONS) {\n            File f = File.createTempFile(TEST_FILE_PREFIX, ext);\n            f.deleteOnExit();\n\n            OutputStream os = IOUtil.openFileForWriting(f);\n            BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(os));\n            writer.write(randomizedTestString);\n            writer.close();\n\n            InputStream is = IOUtil.openFileForReading(f);\n            BufferedReader reader = new BufferedReader(new InputStreamReader(is));\n            String line = reader.readLine();\n            Assert.assertEquals(randomizedTestString, line);\n        }\n    }\n\n    @Test(groups = {\"unix\"})\n    public void testGetCanonicalPath() throws IOException {\n        String tmpPath = System.getProperty(\"java.io.tmpdir\");\n        String userName = System.getProperty(\"user.name\");\n\n        if (tmpPath.endsWith(userName)) {\n            tmpPath = tmpPath.substring(0, tmpPath.length() - userName.length());\n        }\n\n        File tmpDir = new File(tmpPath, userName);\n        tmpDir.mkdir();\n        tmpDir.deleteOnExit();\n        File actual = new File(tmpDir, \"actual.txt\");\n        actual.deleteOnExit();\n        ProcessExecutor.execute(new String[]{\"touch\", actual.getAbsolutePath()});\n        File symlink = new File(tmpDir, \"symlink.txt\");\n        symlink.deleteOnExit();\n        ProcessExecutor.execute(new String[]{\"ln\", \"-s\", actual.getAbsolutePath(), symlink.getAbsolutePath()});\n        File lnDir = new File(tmpDir, \"symLinkDir\");\n        lnDir.deleteOnExit();\n        ProcessExecutor.execute(new String[]{\"ln\", \"-s\", tmpDir.getAbsolutePath(), lnDir.getAbsolutePath()});\n        File lnToActual = new File(lnDir, \"actual.txt\");\n        lnToActual.deleteOnExit();\n        File lnToSymlink = new File(lnDir, \"symlink.txt\");\n        lnToSymlink.deleteOnExit();\n\n        File[] files = {actual, symlink, lnToActual, lnToSymlink};\n        for (File f : files) {\n            Assert.assertEquals(IOUtil.getFullCanonicalPath(f), actual.getCanonicalPath());\n        }\n    }\n\n    @Test\n    public void testUtfWriting() throws IOException {\n        final String utf8 = new StringWriter().append((char) 168).append((char) 197).toString();\n        for (String ext : TEST_FILE_EXTENSIONS) {\n            final File f = File.createTempFile(TEST_FILE_PREFIX, ext);\n            f.deleteOnExit();\n\n            final BufferedWriter writer = IOUtil.openFileForBufferedUtf8Writing(f);\n            writer.write(utf8);\n            CloserUtil.close(writer);\n\n            final BufferedReader reader = IOUtil.openFileForBufferedUtf8Reading(f);\n            final String line = reader.readLine();\n            Assert.assertEquals(utf8, line, f.getAbsolutePath());\n\n            CloserUtil.close(reader);\n\n        }\n    }\n\n    @Test\n    public void slurpLinesTest() throws FileNotFoundException {\n        Assert.assertEquals(IOUtil.slurpLines(SLURP_TEST_FILE.toFile()), SLURP_TEST_LINES);\n    }\n\n    @Test\n    public void slurpWhitespaceOnlyFileTest() throws FileNotFoundException {\n        Assert.assertEquals(IOUtil.slurp(FIVE_SPACES_THEN_A_NEWLINE_THEN_FIVE_SPACES_FILE.toFile()), \"     \\n     \");\n    }\n\n    @Test\n    public void slurpEmptyFileTest() throws FileNotFoundException {\n        Assert.assertEquals(IOUtil.slurp(EMPTY_FILE.toFile()), \"\");\n    }\n\n    @Test\n    public void slurpTest() throws FileNotFoundException {\n        Assert.assertEquals(IOUtil.slurp(SLURP_TEST_FILE.toFile()), CollectionUtil.join(SLURP_TEST_LINES, SLURP_TEST_LINE_SEPARATOR));\n    }\n\n    @Test(dataProvider = \"fileTypeTestCases\")\n    public void testFileType(final String path, boolean expectedIsRegularFile) {\n        final File file = new File(path);\n        Assert.assertEquals(IOUtil.isRegularPath(file), expectedIsRegularFile);\n        Assert.assertEquals(IOUtil.isRegularPath(file.toPath()), expectedIsRegularFile);\n    }\n\n    @Test(dataProvider = \"unixFileTypeTestCases\", groups = {\"unix\"})\n    public void testFileTypeUnix(final String path, boolean expectedIsRegularFile) {\n        final File file = new File(path);\n        Assert.assertEquals(IOUtil.isRegularPath(file), expectedIsRegularFile);\n        Assert.assertEquals(IOUtil.isRegularPath(file.toPath()), expectedIsRegularFile);\n    }\n\n    @Test\n    public void testAddExtension() throws IOException {\n        Path p = IOUtil.getPath(\"/folder/file\");\n        Assert.assertEquals(IOUtil.addExtension(p, \".ext\"), IOUtil.getPath(\"/folder/file.ext\"));\n        p = IOUtil.getPath(\"folder/file\");\n        Assert.assertEquals(IOUtil.addExtension(p, \".ext\"), IOUtil.getPath(\"folder/file.ext\"));\n        try (FileSystem jimfs = Jimfs.newFileSystem(Configuration.unix())) {\n            p = jimfs.getPath(\"folder/sub/file\");\n            Assert.assertEquals(IOUtil.addExtension(p, \".ext\"), jimfs.getPath(\"folder/sub/file.ext\"));\n            p = jimfs.getPath(\"folder/file\");\n            Assert.assertEquals(IOUtil.addExtension(p, \".ext\"), jimfs.getPath(\"folder/file.ext\"));\n            p = jimfs.getPath(\"file\");\n            Assert.assertEquals(IOUtil.addExtension(p, \".ext\"), jimfs.getPath(\"file.ext\"));\n        }\n    }\n\n    @Test\n    public void testAddExtensionOnList() throws IOException {\n        Path p = IOUtil.getPath(\"/folder/file\");\n        List<FileSystemProvider> fileSystemProviders = FileSystemProvider.installedProviders();\n\n        List<Path> paths = new ArrayList<>();\n        List<String> strings = new ArrayList<>();\n\n        paths.add(IOUtil.addExtension(p, \".ext\"));\n        strings.add(\"/folder/file.ext\");\n\n        p = IOUtil.getPath(\"folder/file\");\n        paths.add(IOUtil.addExtension(p, \".ext\"));\n        strings.add(\"folder/file.ext\");\n\n        List<Path> expectedPaths = IOUtil.getPaths(strings);\n\n        Assert.assertEquals(paths, expectedPaths);\n    }\n\n\n    @DataProvider(name = \"fileTypeTestCases\")\n    private Object[][] fileTypeTestCases() {\n        return new Object[][]{\n                {existingTempFile.getAbsolutePath(), Boolean.TRUE},\n                {systemTempDir, Boolean.FALSE}\n\n        };\n    }\n\n    @DataProvider(name = \"unixFileTypeTestCases\")\n    private Object[][] unixFileTypeTestCases() {\n        return new Object[][]{\n                {\"/dev/null\", Boolean.FALSE},\n                {\"/dev/stdout\", Boolean.FALSE},\n                {\"/non/existent/file\", Boolean.TRUE},\n        };\n    }\n\n    @DataProvider\n    public Object[][] getFiles(){\n        final File file = new File(\"someFile\");\n        return new Object[][] {\n                {null, null},\n                {file, file.toPath()}\n        };\n    }\n\n    @Test(dataProvider = \"getFiles\")\n    public void testToPath(final File file, final Path expected){\n        Assert.assertEquals(IOUtil.toPath(file), expected);\n    }\n\n\n    @DataProvider(name = \"fileNamesForDelete\")\n    public Object[][] fileNamesForDelete() {\n        return new Object[][] {\n                {Collections.emptyList()},\n                {Collections.singletonList(\"file1\")},\n                {Arrays.asList(\"file1\", \"file2\")}\n        };\n    }\n\n    @Test\n    public void testGetDefaultTmpDirPath() throws Exception {\n        try {\n            Path testPath = IOUtil.getDefaultTmpDirPath();\n            Assert.assertEquals(testPath.toFile().getAbsolutePath(), new File(systemTempDir).getAbsolutePath() + \"/\" + systemUser);\n\n            // change the properties to test others\n            final String newTempPath = Files.createTempDirectory(\"testGetDefaultTmpDirPath\").toString();\n            final String newUser = \"my_user\";\n            System.setProperty(\"java.io.tmpdir\", newTempPath);\n            System.setProperty(\"user.name\", newUser);\n            testPath = IOUtil.getDefaultTmpDirPath();\n            Assert.assertEquals(testPath.toFile().getAbsolutePath(), new File(newTempPath).getAbsolutePath() + \"/\" + newUser);\n\n        } finally {\n            // reset system properties\n            System.setProperty(\"java.io.tmpdir\", systemTempDir);\n            System.setProperty(\"user.name\", systemUser);\n        }\n    }\n\n    @Test(dataProvider = \"fileNamesForDelete\")\n    public void testDeletePathLocal(final List<String> fileNames) throws Exception {\n        final Path tmpDir = IOUtil.createTempDir(\"testDeletePath\");\n        final List<Path> paths = createLocalFiles(tmpDir, fileNames);\n        testDeletePaths(paths);\n    }\n\n    @Test\n    public void testDeleteSinglePath() throws Exception {\n        final Path toDelete = Files.createTempFile(\"file\",\".bad\");\n        Assert.assertTrue(Files.exists(toDelete));\n        IOUtil.deletePath(toDelete);\n        Assert.assertFalse(Files.exists(toDelete));\n    }\n\n    @Test\n    public void testDeleteSingleWithDeletePaths() throws Exception {\n        final Path toDelete = Files.createTempFile(\"file\",\".bad\");\n        Assert.assertTrue(Files.exists(toDelete));\n        IOUtil.deletePaths(toDelete);\n        Assert.assertFalse(Files.exists(toDelete));\n    }\n\n    @Test(dataProvider = \"fileNamesForDelete\")\n    public void testDeletePathJims(final List<String> fileNames) throws Exception {\n        final List<Path> paths = createJimfsFiles(\"testDeletePath\", fileNames);\n        testDeletePaths(paths);\n    }\n\n    @Test(dataProvider = \"fileNamesForDelete\")\n    public void testDeleteArrayPathLocal(final List<String> fileNames) throws Exception {\n        final Path tmpDir = IOUtil.createTempDir(\"testDeletePath\");\n        final List<Path> paths = createLocalFiles(tmpDir, fileNames);\n        testDeletePathArray(paths);\n    }\n\n    @Test(dataProvider = \"fileNamesForDelete\")\n    public void testDeleteArrayPathJims(final List<String> fileNames) throws Exception {\n        final List<Path> paths = createJimfsFiles(\"testDeletePath\", fileNames);\n        testDeletePathArray(paths);\n    }\n\n\n    private static void testDeletePaths(final List<Path> paths) {\n        paths.forEach(p -> Assert.assertTrue(Files.exists(p)));\n        IOUtil.deletePaths(paths);\n        paths.forEach(p -> Assert.assertFalse(Files.exists(p)));\n    }\n\n    private static void testDeletePathArray(final List<Path> paths) {\n        paths.forEach(p -> Assert.assertTrue(Files.exists(p)));\n        IOUtil.deletePaths(paths.toArray(new Path[paths.size()]));\n        paths.forEach(p -> Assert.assertFalse(Files.exists(p)));\n    }\n\n    private static List<Path> createLocalFiles(final Path tmpDir, final List<String> fileNames) throws Exception {\n        final List<Path> paths = new ArrayList<>(fileNames.size());\n        for (final String f: fileNames) {\n            final Path file = Files.createFile(tmpDir.resolve(f));\n            paths.add(file);\n        }\n        return paths;\n    }\n\n    private List<Path> createJimfsFiles(final String folderName, final List<String> fileNames) throws Exception {\n        final List<Path> paths = new ArrayList<>(fileNames.size());\n        final Path folder = inMemoryFileSystem.getPath(folderName);\n        if (Files.notExists(folder)) Files.createDirectory(folder);\n\n        for (final String f: fileNames) {\n            final Path p = inMemoryFileSystem.getPath(folderName, f);\n            Files.createFile(p);\n            paths.add(p);\n        }\n\n        return paths;\n    }\n\n    @DataProvider\n    public Object[][] pathsForWritableDirectory() throws Exception {\n        return new Object[][] {\n                // non existent\n                {inMemoryFileSystem.getPath(\"no_exists\"), false},\n                // non directory\n                {Files.createFile(inMemoryFileSystem.getPath(\"testAssertDirectoryIsWritable_file\")), false},\n                // TODO - how to do in inMemoryFileSystem a non-writable directory?\n                // writable directory\n                {Files.createDirectory(inMemoryFileSystem.getPath(\"testAssertDirectoryIsWritable_directory\")), true}\n        };\n    }\n\n    @Test(dataProvider = \"pathsForWritableDirectory\")\n    public void testAssertDirectoryIsWritablePath(final Path path, final boolean writable) {\n        try {\n            IOUtil.assertDirectoryIsWritable(path);\n        } catch (SAMException e) {\n            if (writable) {\n                Assert.fail(e.getMessage());\n            }\n        }\n    }\n\n    @DataProvider\n    public Object[][] filesForWritableDirectory() throws Exception {\n        final File nonWritableFile = new File(systemTempDir, \"testAssertDirectoryIsWritable_non_writable_dir\");\n        nonWritableFile.mkdir();\n        nonWritableFile.setWritable(false);\n\n        return new Object[][] {\n                // non existent\n                {new File(\"no_exists\"), false},\n                // non directory\n                {existingTempFile, false},\n                // non-writable directory\n                {nonWritableFile, false},\n                // writable directory\n                {new File(systemTempDir), true},\n        };\n    }\n\n    @Test(dataProvider = \"filesForWritableDirectory\")\n    public void testAssertDirectoryIsWritableFile(final File file, final boolean writable) {\n        try {\n            IOUtil.assertDirectoryIsWritable(file);\n        } catch (SAMException e) {\n            if (writable) {\n                Assert.fail(e.getMessage());\n            }\n        }\n    }\n\n    static final String level1 = \"Level1.fofn\";\n    static final String level2 = \"Level2.fofn\";\n\n    @DataProvider\n    public Object[][] fofnData() throws IOException {\n\n        Path fofnPath1 = inMemoryFileSystem.getPath(level1);\n        Files.copy(TEST_DATA_DIR.resolve(level1), fofnPath1);\n\n        Path fofnPath2 = inMemoryFileSystem.getPath(level2);\n        Files.copy(TEST_DATA_DIR.resolve(level2), fofnPath2);\n\n        return new Object[][]{\n                {TEST_DATA_DIR + \"/\" + level1, new String[]{\".vcf\", \".vcf.gz\"}, 2},\n                {TEST_DATA_DIR + \"/\" + level2, new String[]{\".vcf\", \".vcf.gz\"}, 4},\n                {fofnPath1.toUri().toString(), new String[]{\".vcf\", \".vcf.gz\"}, 2},\n                {fofnPath2.toUri().toString(), new String[]{\".vcf\", \".vcf.gz\"}, 4}\n        };\n    }\n\n    @Test(dataProvider = \"fofnData\")\n    public void testUnrollPaths(final String pathUri, final String[] extensions, final int expectedNumberOfUnrolledPaths) throws IOException {\n        Path p = IOUtil.getPath(pathUri);\n        List<Path> paths = IOUtil.unrollPaths(Collections.singleton(p), extensions);\n\n        Assert.assertEquals(paths.size(), expectedNumberOfUnrolledPaths);\n    }\n\n    @DataProvider(name = \"blockCompressedExtensionExtensionStrings\")\n    public static Object[][] createBlockCompressedExtensionStrings() {\n        return new Object[][] {\n                { \"testzip.gz\", true },\n                { \"test.gzip\", true },\n                { \"test.bgz\", true },\n                { \"test.bgzf\", true },\n                { \"test.bzip2\", false }\n        };\n    }\n\n    @Test(dataProvider = \"blockCompressedExtensionExtensionStrings\")\n    public void testBlockCompressionExtensionString(final String testString, final boolean expected) {\n        Assert.assertEquals(IOUtil.hasBlockCompressedExtension(testString), expected);\n    }\n\n    @Test(dataProvider = \"blockCompressedExtensionExtensionStrings\")\n    public void testBlockCompressionExtensionFile(final String testString, final boolean expected) {\n        Assert.assertEquals(IOUtil.hasBlockCompressedExtension(new File(testString)), expected);\n    }\n\n    @DataProvider(name = \"blockCompressedExtensionExtensionURIStrings\")\n    public static Object[][] createBlockCompressedExtensionURIs() {\n        return new Object[][]{\n                {\"testzip.gz\", true},\n                {\"test.gzip\", true},\n                {\"test.bgz\", true},\n                {\"test.bgzf\", true},\n                {\"test\", false},\n                {\"test.bzip2\", false},\n\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.gz\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.gzip\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bgz\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bgzf\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bzip2\", false},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877\", false},\n\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.gz?alt=media\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.gzip?alt=media\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bgz?alt=media\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bgzf?alt=media\", true},\n                {\"https://www.googleapis.com/download/storage/v1/b/deflaux-public-test/o/NA12877.vcf.bzip2?alt=media\", false},\n\n                {\"ftp://ftp.broadinstitute.org/distribution/igv/TEST/cpgIslands.hg18.gz\", true},\n                {\"ftp://ftp.broadinstitute.org/distribution/igv/TEST/cpgIslands.hg18.bed\", false}\n        };\n    }\n\n    @Test(dataProvider = \"blockCompressedExtensionExtensionURIStrings\")\n    public void testBlockCompressionExtension(final String testURIString, final boolean expected) {\n        URI testURI = URI.create(testURIString);\n        Assert.assertEquals(IOUtil.hasBlockCompressedExtension(testURI), expected);\n    }\n\n    @Test(dataProvider = \"blockCompressedExtensionExtensionURIStrings\")\n    public void testBlockCompressionExtensionStringVersion(final String testURIString, final boolean expected) {\n        Assert.assertEquals(IOUtil.hasBlockCompressedExtension(testURIString), expected);\n    }\n\n    @DataProvider\n    public static Object[][] blockCompressedFiles() {\n        return new Object[][]{\n                {TEST_DATA_DIR.resolve(\"ipsum.txt\"), true, false},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt\"), false, false},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.gz\"), true, false},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.gz\"), false, false},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz\"), true, true},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz\"), false, true},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz.wrongextension\"), true, false},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz.wrongextension\"), false, true},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgzipped_with_gzextension.gz\"), true, true},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgzipped_with_gzextension.gz\"), false, true},\n                {TEST_DATA_DIR.resolve(\"example.bam\"), true, false},\n                {TEST_DATA_DIR.resolve(\"example.bam\"), false, true}\n        };\n    }\n\n    @Test(dataProvider = \"blockCompressedFiles\")\n    public void testIsBlockCompressed(Path file, boolean checkExtension, boolean expected) throws IOException {\n        Assert.assertEquals(IOUtil.isBlockCompressed(file, checkExtension), expected);\n    }\n\n    @Test(dataProvider = \"blockCompressedFiles\")\n    public void testIsBlockCompressedOnJimfs(Path file, boolean checkExtension, boolean expected) throws IOException {\n         try (FileSystem jimfs = Jimfs.newFileSystem(Configuration.unix())) {\n             final Path jimfsRoot = jimfs.getRootDirectories().iterator().next();\n             final Path jimfsFile = Files.copy(file, jimfsRoot.resolve(file.getFileName().toString()));\n             Assert.assertEquals(IOUtil.isBlockCompressed(jimfsFile, checkExtension), expected);\n         }\n    }\n\n    @DataProvider\n    public static Object[][] filesToCompress() {\n        return new Object[][]{\n                {WORDS_LONG, \".gz\", 8},\n                {WORDS_LONG, \".bfq\", 8},\n                {TEST_VARIANT_DIR.resolve(\"test1.vcf\"), \".gz\", 7},\n                {TEST_VARIANT_DIR.resolve(\"test1.vcf\"), \".bfq\", 7}\n        };\n    }\n\n    @Test(dataProvider = \"filesToCompress\")\n    public void testCompressionLevel(final Path file, final String extension, final int lastDifference) throws IOException {\n        final long origSize = Files.size(file);\n        long previousSize = origSize;\n        for (int compressionLevel = 1; compressionLevel <= 9; compressionLevel++) {\n            final Path outFile = Files.createTempFile(\"tmp\", extension);\n            outFile.toFile().deleteOnExit();\n            IOUtil.setCompressionLevel(compressionLevel);\n            Assert.assertEquals(IOUtil.getCompressionLevel(), compressionLevel);\n            final InputStream inStream = IOUtil.openFileForReading(file);\n            try (final OutputStream outStream = IOUtil.openFileForWriting(outFile.toFile())) {\n                IOUtil.transferByStream(inStream, outStream, origSize);\n            }\n            final long newSize = Files.size(outFile);\n            if (compressionLevel <= lastDifference) {\n                Assert.assertTrue(previousSize > newSize);\n            } else {\n                Assert.assertTrue(previousSize >= newSize);\n            }\n            previousSize = newSize;\n        }\n    }\n\n    @DataProvider\n    public Object[][] getExtensions(){\n        return new Object[][]{\n                {\".gz\", true},\n                {\".bfq\", true},\n                {\".txt\", false}};\n    }\n\n    @Test(dataProvider = \"getExtensions\")\n    public void testReadWriteJimfs(String extension, boolean gzipped) throws IOException {\n        final Path jmfsRoot = inMemoryFileSystem.getRootDirectories().iterator().next();\n        final Path tmp = Files.createTempFile(jmfsRoot, \"test\", extension);\n        final String expected = \"lorem ipswitch, nantucket, bucket\";\n        try (Writer out = IOUtil.openFileForBufferedWriting(tmp)){\n            out.write(expected);\n        }\n\n        try (InputStream in = new BufferedInputStream(Files.newInputStream(tmp))){\n               Assert.assertEquals(IOUtil.isGZIPInputStream(in), gzipped);\n        }\n\n        try (BufferedReader in = IOUtil.openFileForBufferedReading(tmp)){\n            final String actual = in.readLine();\n            Assert.assertEquals(actual, expected);\n        }\n    }\n\n    @DataProvider\n    public static Object[][] badCompressionLevels() {\n        return new Object[][]{\n                {-1},\n                {10}\n        };\n    }\n\n    @Test(dataProvider = \"badCompressionLevels\", expectedExceptions = {IllegalArgumentException.class})\n    public void testCompressionLevelExceptions(final int compressionLevel) {\n        IOUtil.setCompressionLevel(compressionLevel);\n    }\n\n    @DataProvider\n    public static Object[][] filesToCopy() {\n        return new Object[][]{\n                {TEST_VARIANT_DIR.resolve(\"test1.vcf\")},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt\")}\n        };\n    }\n\n    @Test(dataProvider = \"filesToCopy\")\n    public void testCopyFile(final Path file) throws IOException {\n        final Path outFile = Files.createTempFile(\"tmp\", \".tmp\");\n        outFile.toFile().deleteOnExit();\n        IOUtil.copyFile(file.toFile(), outFile.toFile());\n        Assert.assertEquals(Files.lines(file).collect(Collectors.toList()), Files.lines(outFile).collect(Collectors.toList()));\n    }\n\n    @Test(dataProvider = \"filesToCopy\", expectedExceptions = {SAMException.class})\n    public void testCopyFileReadException(final Path file) throws IOException {\n        final Path outFile = Files.createTempFile(\"tmp\", \".tmp\");\n        outFile.toFile().deleteOnExit();\n        file.toFile().setReadable(false);\n        try {\n            IOUtil.copyFile(file.toFile(), outFile.toFile());\n        } finally { //need to set input file permission back to readable so other unit tests can access it\n            file.toFile().setReadable(true);\n        }\n    }\n\n    @Test(dataProvider = \"filesToCopy\", expectedExceptions = {SAMException.class})\n    public void testCopyFileWriteException(final Path file) throws IOException {\n        final Path outFile = Files.createTempFile(\"tmp\", \".tmp\");\n        outFile.toFile().deleteOnExit();\n        outFile.toFile().setWritable(false);\n        IOUtil.copyFile(file.toFile(), outFile.toFile());\n    }\n\n    @DataProvider\n    public static Object[][] baseNameTests() {\n        return new Object[][]{\n                {TEST_DATA_DIR.resolve(\"ipsum.txt\"), \"ipsum\"},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz.wrongextension\"), \"ipsum.txt.bgz\"},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgzipped_with_gzextension.gz\"), \"ipsum.txt.bgzipped_with_gzextension\"},\n                {TEST_VARIANT_DIR.resolve(\"utils\"), \"utils\"},\n                {TEST_VARIANT_DIR.resolve(\"not_real_file.txt\"), \"not_real_file\"}\n        };\n    }\n\n    @Test(dataProvider = \"baseNameTests\")\n    public void testBasename(final Path file, final String expected) {\n        final String result = IOUtil.basename(file.toFile());\n        Assert.assertEquals(result, expected);\n    }\n\n    @DataProvider\n    public static Object[][] regExpTests() {\n        return new Object[][]{\n                {\"\\\\w+\\\\.txt\", new String[]{\"5newline5.txt\", \"empty.txt\", \"ipsum.txt\", \"slurptest.txt\"}},\n                {\"^((?!txt).)*$\", new String[]{\"Level1.fofn\", \"Level2.fofn\", \"example.bam\"}},\n                {\"^\\\\d+.*\", new String[]{\"5newline5.txt\"}}\n        };\n    }\n\n    @Test(dataProvider = \"regExpTests\")\n    public void testRegExp(final String regexp, final String[] expected) throws IOException {\n        final String[] allNames = {\"5newline5.txt\", \"Level2.fofn\", \"example.bam\", \"ipsum.txt.bgz\", \"ipsum.txt.bgzipped_with_gzextension.gz\", \"slurptest.txt\", \"Level1.fofn\", \"empty.txt\", \"ipsum.txt\", \"ipsum.txt.bgz.wrongextension\", \"ipsum.txt.gz\"};\n        final Path regExpDir = Files.createTempDirectory(\"regExpDir\");\n        regExpDir.toFile().deleteOnExit();\n        final List<String> listExpected = Arrays.asList(expected);\n        final List<File> expectedFiles = new ArrayList<File>();\n        for (String name : allNames) {\n            final Path file = regExpDir.resolve(name);\n            file.toFile().deleteOnExit();\n            file.toFile().createNewFile();\n            if (listExpected.contains(name)) {\n                expectedFiles.add(file.toFile());\n            }\n        }\n        final File[] result = IOUtil.getFilesMatchingRegexp(regExpDir.toFile(), regexp);\n        Assert.assertEqualsNoOrder(result, expectedFiles.toArray());\n    }\n\n    @Test()\n    public void testReadLines() throws IOException {\n        final Path file = Files.createTempFile(\"tmp\", \".txt\");\n        file.toFile().deleteOnExit();\n        final int seed = 12394738;\n        final Random rand = new Random(seed);\n        final int nLines = 5;\n        final List<String> lines = new ArrayList<String>();\n        try (final PrintWriter writer = new PrintWriter(Files.newBufferedWriter(file))) {\n            for (int i = 0; i < nLines; i++) {\n                final String line = TEST_STRING + Integer.toString(rand.nextInt(100000000));\n                lines.add(line);\n                writer.println(line);\n            }\n        }\n        final List<String> retLines = new ArrayList<String>();\n        IOUtil.readLines(file.toFile()).forEachRemaining(retLines::add);\n        Assert.assertEquals(retLines, lines);\n    }\n\n    @DataProvider\n    public static Object[][] fileSuffixTests() {\n        return new Object[][]{\n                {TEST_DATA_DIR.resolve(\"ipsum.txt\"), \".txt\"},\n                {TEST_DATA_DIR.resolve(\"ipsum.txt.bgz\"), \".bgz\"},\n                {TEST_DATA_DIR, null}\n        };\n    }\n\n    @Test(dataProvider = \"fileSuffixTests\")\n    public void testSuffixTest(final Path file, final String expected) {\n        final String ret = IOUtil.fileSuffix(file.toFile());\n        Assert.assertEquals(ret, expected);\n    }\n\n    @Test\n    public void testCopyDirectoryTree() throws IOException {\n        final Path copyToDir = Files.createTempDirectory(\"copyToDir\");\n        copyToDir.toFile().deleteOnExit();\n        IOUtil.copyDirectoryTree(TEST_VARIANT_DIR.toFile(), copyToDir.toFile());\n        final List<Path> collect = Files.walk(TEST_VARIANT_DIR).filter(f -> !f.equals(TEST_VARIANT_DIR)).map(p -> p.getFileName()).collect(Collectors.toList());\n        final List<Path> collectCopy = Files.walk(copyToDir).filter(f -> !f.equals(copyToDir)).map(p -> p.getFileName()).collect(Collectors.toList());\n        Assert.assertEqualsNoOrder(collect.toArray(), collectCopy.toArray());\n    }\n\n    // Little utility to gzip a byte array\n    private static byte[] gzipMessage(final byte[] message) throws IOException {\n        ByteArrayOutputStream bos = new ByteArrayOutputStream();\n        GZIPOutputStream gzout = new GZIPOutputStream(bos);\n        gzout.write(message);\n        gzout.finish();\n        gzout.close();\n        return bos.toByteArray();\n    }\n\n    @DataProvider\n    public static Object[][] gzipTests() throws IOException {\n        final byte[] emptyMessage = \"\".getBytes();\n        final byte[] message = \"Hello World\".getBytes();\n\n        // Compressed version of the messages\n        final byte[] gzippedMessage = gzipMessage(message);\n        final byte[] gzippedEmptyMessage = gzipMessage(emptyMessage);\n\n        return new Object[][]{\n                {emptyMessage, false},\n                {message, false},\n                {gzippedMessage, true},\n                {gzippedEmptyMessage, true}\n        };\n    }\n\n    @Test(dataProvider = \"gzipTests\")\n    public void isGZIPInputStreamTest(byte[] data, boolean isGzipped) throws IOException {\n        try(ByteArrayInputStream inputStream = new ByteArrayInputStream(data)) {\n            // test string without compression\n            Assert.assertEquals(IOUtil.isGZIPInputStream(inputStream), isGzipped);\n            // call twice to verify 'in.reset()' was called\n            Assert.assertEquals(IOUtil.isGZIPInputStream(inputStream), isGzipped);\n        }\n    }\n}\n\n", "/*\n * The MIT License\n *\n * Copyright (c) 2013 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.tribble.index;\n\nimport com.google.common.io.Files;\nimport com.google.common.jimfs.Configuration;\nimport com.google.common.jimfs.Jimfs;\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.SAMSequenceDictionary;\nimport htsjdk.samtools.SAMSequenceRecord;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.Interval;\nimport htsjdk.tribble.AbstractFeatureReader;\nimport htsjdk.tribble.TestUtils;\nimport htsjdk.tribble.Tribble;\nimport htsjdk.tribble.TribbleException;\nimport htsjdk.tribble.VCFRedirectCodec;\nimport htsjdk.tribble.bed.BEDCodec;\nimport htsjdk.tribble.index.tabix.TabixFormat;\nimport htsjdk.tribble.index.tabix.TabixIndex;\nimport htsjdk.tribble.readers.LineIterator;\nimport htsjdk.variant.bcf2.BCF2Codec;\nimport htsjdk.variant.variantcontext.VariantContext;\nimport htsjdk.variant.vcf.VCFCodec;\nimport htsjdk.variant.vcf.VCFFileReader;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.FileSystem;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.Iterator;\nimport java.util.List;\n\n/**\n * User: jacob\n * Date: 2012-Aug-23\n */\npublic class IndexFactoryTest extends HtsjdkTest {\n\n    @DataProvider(name = \"bedDataProvider\")\n    public Object[][] getLinearIndexFactoryTypes() {\n        return new Object[][] {\n                { new File(TestUtils.DATA_DIR, \"bed/Unigene.sample.bed\") },\n                { new File(TestUtils.DATA_DIR, \"bed/Unigene.sample.bed.gz\") }\n        };\n    }\n\n    @Test(dataProvider = \"bedDataProvider\")\n    public void testCreateLinearIndexFromBED(final File inputBEDFIle) {\n        Index index = IndexFactory.createLinearIndex(inputBEDFIle, new BEDCodec());\n        String chr = \"chr2\";\n\n        Assert.assertTrue(index.getSequenceNames().contains(chr));\n        Assert.assertTrue(index.containsChromosome(chr));\n        Assert.assertEquals(1, index.getSequenceNames().size());\n        List<Block> blocks = index.getBlocks(chr, 1, 50);\n        Assert.assertEquals(1, blocks.size());\n\n        Block block = blocks.get(0);\n        Assert.assertEquals(78, block.getSize());\n    }\n\n    @Test(expectedExceptions = TribbleException.MalformedFeatureFile.class, dataProvider = \"indexFactoryProvider\")\n    public void testCreateIndexUnsorted(IndexFactory.IndexType type) {\n        final File unsortedBedFile = new File(TestUtils.DATA_DIR, \"bed/unsorted.bed\");\n        IndexFactory.createIndex(unsortedBedFile, new BEDCodec(), type);\n    }\n\n    @Test(expectedExceptions = TribbleException.MalformedFeatureFile.class, dataProvider = \"indexFactoryProvider\")\n    public void testCreateIndexDiscontinuousContigs(IndexFactory.IndexType type) throws Exception{\n        final File discontinuousFile = new File(TestUtils.DATA_DIR,\"bed/disconcontigs.bed\");\n        IndexFactory.createIndex(discontinuousFile, new BEDCodec(), type);\n    }\n\n    @DataProvider(name = \"indexFactoryProvider\")\n    public Object[][] getIndexFactoryTypes(){\n        return new Object[][] {\n                new Object[] { IndexFactory.IndexType.TABIX },\n                new Object[] { IndexFactory.IndexType.LINEAR },\n                new Object[] { IndexFactory.IndexType.INTERVAL_TREE }\n        };\n    }\n\n    @Test\n    public void testCreateTabixIndexOnBlockCompressed() {\n        // index a VCF\n        final Path inputFileVcf = Paths.get(\"src/test/resources/htsjdk/tribble/tabix/testTabixIndex.vcf\");\n        final VCFFileReader readerVcf = new VCFFileReader(inputFileVcf, false);\n        final SAMSequenceDictionary vcfDict = readerVcf.getFileHeader().getSequenceDictionary();\n        final TabixIndex tabixIndexVcf =\n                IndexFactory.createTabixIndex(inputFileVcf, new VCFCodec(), TabixFormat.VCF, vcfDict);\n\n        // index the same bgzipped VCF\n        final Path inputFileVcfGz = Paths.get(\"src/test/resources/htsjdk/tribble/tabix/testTabixIndex.vcf.gz\");\n        final VCFFileReader readerVcfGz = new VCFFileReader(inputFileVcfGz, false);\n        final TabixIndex tabixIndexVcfGz =\n                IndexFactory.createTabixIndex(inputFileVcfGz, new VCFCodec(), TabixFormat.VCF,\n                        readerVcfGz.getFileHeader().getSequenceDictionary());\n\n        // assert that each sequence in the header that represents some VCF row ended up in the index\n        // for both the VCF and bgzipped VCF\n        for (SAMSequenceRecord samSequenceRecord : vcfDict.getSequences()) {\n            Assert.assertTrue(\n                    tabixIndexVcf.containsChromosome(samSequenceRecord.getSequenceName()),\n                    \"Tabix indexed VCF does not contain sequence: \" + samSequenceRecord.getSequenceName());\n\n            Assert.assertTrue(\n                    tabixIndexVcfGz.containsChromosome(samSequenceRecord.getSequenceName()),\n                    \"Tabix indexed (bgzipped) VCF does not contain sequence: \" + samSequenceRecord.getSequenceName());\n        }\n\n    }\n\n    @Test\n    public void testTabixOnNonDefaultFileSystem() throws IOException {\n        try (final FileSystem fs = Jimfs.newFileSystem(\"test\", Configuration.unix())) {\n            final Path vcfInJimfs = TestUtils.getTribbleFileInJimfs(\n                    \"src/test/resources/htsjdk/tribble/tabix/testTabixIndex.vcf.gz\",\n                    null, fs);\n            // IndexFactory doesn't write the output, so we just want to make sure it doesn't throw\n            IndexFactory.createTabixIndex(vcfInJimfs, new VCFCodec(), TabixFormat.VCF, null);\n        }\n    }\n\n    @DataProvider(name = \"vcfDataProvider\")\n    public Object[][] getVCFIndexData(){\n        return new Object[][] {\n                new Object[] {\n                        new File(TestUtils.DATA_DIR, \"tabix/4featuresHG38Header.vcf.gz\"),\n                        new Interval(\"chr6\", 33414233, 118314029)\n                },\n                new Object[] {\n                        new File(TestUtils.DATA_DIR, \"tabix/4featuresHG38Header.vcf\"),\n                        new Interval(\"chr6\", 33414233, 118314029)\n                },\n        };\n    }\n\n    @Test(dataProvider = \"vcfDataProvider\")\n    public void testCreateTabixIndexFromVCF(\n            final File inputVCF,\n            final Interval queryInterval) throws IOException {\n        // copy the original file and create the index for the copy\n        final File tempDir = IOUtil.createTempDir(\"testCreateTabixIndexFromVCF\").toFile();\n        tempDir.deleteOnExit();\n        final File tmpVCF = new File(tempDir, inputVCF.getName());\n        Files.copy(inputVCF, tmpVCF);\n        tmpVCF.deleteOnExit();\n\n        // this test creates a TABIX index (.tbi)\n        final TabixIndex tabixIndexGz = IndexFactory.createTabixIndex(tmpVCF, new VCFCodec(), null);\n        tabixIndexGz.writeBasedOnFeatureFile(tmpVCF);\n        final File tmpIndex = Tribble.tabixIndexFile(tmpVCF);\n        tmpIndex.deleteOnExit();\n\n        try (final VCFFileReader originalReader = new VCFFileReader(inputVCF,false);\n            final VCFFileReader tmpReader = new VCFFileReader(tmpVCF, tmpIndex,true)) {\n            Iterator<VariantContext> originalIt = originalReader.iterator();\n            Iterator<VariantContext> tmpIt = tmpReader.query(queryInterval.getContig(), queryInterval.getStart(), queryInterval.getEnd());\n            while (originalIt.hasNext()) {\n                Assert.assertTrue(tmpIt.hasNext(), \"variants missing from gzip query\");\n                VariantContext vcTmp = tmpIt.next();\n                VariantContext vcOrig = originalIt.next();\n                Assert.assertEquals(vcOrig.getContig(), vcTmp.getContig());\n                Assert.assertEquals(vcOrig.getStart(), vcTmp.getStart());\n                Assert.assertEquals(vcOrig.getEnd(), vcTmp.getEnd());\n            }\n        }\n    }\n\n    @DataProvider(name = \"bcfDataFactory\")\n    public Object[][] getBCFData(){\n        return new Object[][] {\n                //TODO: this needs more test cases, including block compressed and indexed, but bcftools can't\n                // generate indices for BCF2.1 files, which is all HTSJDK can read, and htsjdk also can't read/write\n                // block compressed BCFs (https://github.com/samtools/htsjdk/issues/946)\n                new Object[] {\n                        new File(\"src/test/resources/htsjdk/variant/serialization_test.bcf\")\n                }\n        };\n    }\n\n    @Test(dataProvider = \"bcfDataFactory\")\n    public void testCreateLinearIndexFromBCF(final File inputBCF) throws IOException {\n        // copy the original file and create the index for the copy\n        final File tempDir = IOUtil.createTempDir(\"testCreateIndexFromBCF\").toFile();\n        tempDir.deleteOnExit();\n        final File tmpBCF = new File(tempDir, inputBCF.getName());\n        Files.copy(inputBCF, tmpBCF);\n        tmpBCF.deleteOnExit();\n\n        // NOTE: this test creates a LINEAR index (.idx)\n        final Index index = IndexFactory.createIndex(tmpBCF, new BCF2Codec(), IndexFactory.IndexType.LINEAR);\n        index.writeBasedOnFeatureFile(tmpBCF);\n        final File tempIndex = Tribble.indexFile(tmpBCF);\n        tempIndex.deleteOnExit();\n\n        try (final VCFFileReader originalReader = new VCFFileReader(inputBCF,false);\n            final VCFFileReader tmpReader = new VCFFileReader(tmpBCF, tempIndex,true)) {\n            final Iterator<VariantContext> originalIt = originalReader.iterator();\n            while (originalIt.hasNext()) {\n                // we don't have an externally generated index file for the original input, so iterate through each variant\n                // and use the generated index to query for the same variant in the indexed copy of the input\n                final VariantContext vcOrig = originalIt.next();\n                final Iterator<VariantContext> tmpIt = tmpReader.query(vcOrig);\n                Assert.assertTrue(tmpIt.hasNext(), \"Variant not returned from indexed file\");\n                final VariantContext vcTmp = tmpIt.next();\n                Assert.assertEquals(vcOrig.getContig(), vcTmp.getContig());\n                Assert.assertEquals(vcOrig.getStart(), vcTmp.getStart());\n                Assert.assertEquals(vcOrig.getEnd(), vcTmp.getEnd());\n                Assert.assertFalse(tmpIt.hasNext()); // make sure there is only one matching variant\n            }\n        }\n    }\n\n    @DataProvider\n    public Object[][] getRedirectFiles(){\n        return new Object[][] {\n                {VCFRedirectCodec.REDIRECTING_CODEC_TEST_FILE_ROOT + \"vcf.gz.redirect\", IndexFactory.IndexType.TABIX},\n                {VCFRedirectCodec.REDIRECTING_CODEC_TEST_FILE_ROOT + \"vcf.redirect\", IndexFactory.IndexType.INTERVAL_TREE},\n                {VCFRedirectCodec.REDIRECTING_CODEC_TEST_FILE_ROOT + \"vcf.redirect\", IndexFactory.IndexType.LINEAR}\n        };\n    }\n\n    @Test(dataProvider = \"getRedirectFiles\")\n    public void testIndexRedirectedFiles(String input, IndexFactory.IndexType type) throws IOException {\n        final VCFRedirectCodec codec = new VCFRedirectCodec();\n        final File dir = IOUtil.createTempDir(\"redirec-test.dir\").toFile();\n        try {\n            final File tmpInput = new File(dir, new File(input).getName());\n            Files.copy(new File(input), tmpInput);\n            final File tmpDataFile = new File(codec.getPathToDataFile(tmpInput.toString()));\n            Assert.assertTrue(new File(tmpDataFile.getAbsoluteFile().getParent()).mkdir());\n            final File originalDataFile = new File(codec.getPathToDataFile(input));\n            Files.copy(originalDataFile, tmpDataFile);\n\n            try(final AbstractFeatureReader<VariantContext, LineIterator> featureReader = AbstractFeatureReader.getFeatureReader(tmpInput.getAbsolutePath(), codec, false)) {\n                Assert.assertFalse(featureReader.hasIndex());\n            }\n            final Index index = IndexFactory.createIndex(tmpInput, codec, type);\n            index.writeBasedOnFeatureFile(tmpDataFile);\n\n            try(final AbstractFeatureReader<VariantContext, LineIterator> featureReader = AbstractFeatureReader.getFeatureReader(tmpInput.getAbsolutePath(), codec)) {\n                Assert.assertTrue(featureReader.hasIndex());\n                Assert.assertEquals(featureReader.query(\"20\",1110696,1230237).stream().count(), 2);\n            }\n        } finally {\n            IOUtil.recursiveDelete(dir.toPath());\n        }\n    }\n}\n", "package htsjdk.tribble.index;\n\nimport com.google.common.jimfs.Configuration;\nimport com.google.common.jimfs.Jimfs;\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.util.FileExtensions;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.tribble.FeatureCodec;\nimport htsjdk.tribble.TestUtils;\nimport htsjdk.tribble.Tribble;\nimport htsjdk.tribble.bed.BEDCodec;\nimport htsjdk.tribble.index.interval.IntervalTreeIndex;\nimport htsjdk.tribble.index.linear.LinearIndex;\nimport htsjdk.tribble.index.tabix.TabixIndex;\nimport htsjdk.tribble.util.LittleEndianOutputStream;\nimport htsjdk.tribble.util.TabixUtils;\nimport htsjdk.variant.vcf.VCFCodec;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.OutputStream;\nimport java.nio.file.FileSystem;\nimport java.nio.file.Path;\nimport java.util.ArrayList;\nimport java.util.List;\n\n\npublic class IndexTest extends HtsjdkTest {\n    private final static String CHR = \"1\";\n    private final static File MassiveIndexFile = new File(TestUtils.DATA_DIR + \"Tb.vcf.idx\");\n\n    @DataProvider(name = \"StartProvider\")\n    public Object[][] makeStartProvider() {\n        List<Object[]> tests = new ArrayList<>();\n\n        tests.add(new Object[]{1226943, 1226943, 1226943, 2000000});\n\n        return tests.toArray(new Object[][]{});\n    }\n\n    @Test(dataProvider = \"StartProvider\")\n    public void testMassiveQuery(final int start, final int mid, final int mid2, final int end) throws IOException {\n        LinearIndex index = (LinearIndex)IndexFactory.loadIndex(MassiveIndexFile.getAbsolutePath());\n\n        final List<Block> leftBlocks = index.getBlocks(CHR, start, mid);\n        final List<Block> rightBlocks = index.getBlocks(CHR, mid2, end); // gap must be big to avoid overlaps\n        final List<Block> allBlocks = index.getBlocks(CHR, start, end);\n\n        final long leftSize = leftBlocks.isEmpty() ? 0 : leftBlocks.get(0).getSize();\n        final long rightSize = rightBlocks.isEmpty() ? 0 : rightBlocks.get(0).getSize();\n        final long allSize = allBlocks.isEmpty() ? 0 : allBlocks.get(0).getSize();\n\n        Assert.assertTrue(leftSize >= 0, \"Expected leftSize to be positive \" + leftSize);\n        Assert.assertTrue(rightSize >= 0, \"Expected rightSize to be positive \" + rightSize);\n        Assert.assertTrue(allSize >= 0, \"Expected allSize to be positive \" + allSize);\n\n        Assert.assertTrue(allSize >= Math.max(leftSize,rightSize), \"Expected size of joint query \" + allSize + \" to be at least >= max of left \" + leftSize + \" and right queries \" + rightSize);\n    }\n\n    @Test()\n    public void testLoadFromStream() throws IOException {\n        LinearIndex index = (LinearIndex)IndexFactory.loadIndex(MassiveIndexFile.getAbsolutePath(), new FileInputStream(MassiveIndexFile));\n        List<String> sequenceNames = index.getSequenceNames();\n        Assert.assertEquals(sequenceNames.size(), 1);\n        Assert.assertEquals(sequenceNames.get(0), CHR);\n    }\n\n    @DataProvider(name = \"writeIndexData\")\n    public Object[][] writeIndexData() {\n        return new Object[][]{\n                {new File(\"src/test/resources/htsjdk/tribble/tabix/testTabixIndex.vcf\"), IndexFactory.IndexType.LINEAR, new VCFCodec()},\n                {new File(\"src/test/resources/htsjdk/tribble/tabix/testTabixIndex.vcf.gz\"), IndexFactory.IndexType.TABIX, new VCFCodec()},\n                {new File(\"src/test/resources/htsjdk/tribble/test.bed\"), IndexFactory.IndexType.LINEAR, new BEDCodec()}\n        };\n    }\n\n    private final static OutputStream nullOutputStrem = new OutputStream() {\n        @Override\n        public void write(int b) throws IOException { }\n    };\n\n    @Test(dataProvider = \"writeIndexData\")\n    public void testWriteIndex(final File inputFile, final IndexFactory.IndexType type, final  FeatureCodec codec) throws Exception {\n        // temp index file for this test\n        final File tempIndex = File.createTempFile(\"index\", (type == IndexFactory.IndexType.TABIX) ? FileExtensions.TABIX_INDEX : FileExtensions.TRIBBLE_INDEX);\n        tempIndex.delete();\n        tempIndex.deleteOnExit();\n        // create the index\n        final Index index = IndexFactory.createIndex(inputFile, codec, type);\n        Assert.assertFalse(tempIndex.exists());\n        // write the index to a file\n        index.write(tempIndex);\n        Assert.assertTrue(tempIndex.exists());\n        // load the generated index\n        final Index loadedIndex = IndexFactory.loadIndex(tempIndex.getAbsolutePath());\n        //TODO: This is just a smoke test; it can pass even if the generated index is unusable for queries.\n        // test that the sequences and properties are the same\n        Assert.assertEquals(loadedIndex.getSequenceNames(), index.getSequenceNames());\n        Assert.assertEquals(loadedIndex.getProperties(), index.getProperties());\n        // test that write to a stream does not blows ip\n        index.write(new LittleEndianOutputStream(nullOutputStrem));\n    }\n\n    @Test(dataProvider = \"writeIndexData\")\n    public void testWritePathIndex(final File inputFile, final IndexFactory.IndexType type, final  FeatureCodec codec) throws Exception {\n        try (final FileSystem fs = Jimfs.newFileSystem(\"test\", Configuration.unix())) {\n            // create the index\n            final Index index = IndexFactory.createIndex(inputFile, codec, type);\n            final Path path = fs.getPath(inputFile.getName() + \".index\");\n            // write the index to a file\n            index.write(path);\n\n            // test if the index does not blow up with the path constructor\n            switch (type) {\n                case TABIX:\n                    new TabixIndex(path);\n                    break;\n                case LINEAR:\n                    new LinearIndex(path);\n                    break;\n                case INTERVAL_TREE:\n                    new IntervalTreeIndex(path);\n                    break;\n            }\n        }\n    }\n\n    @Test(dataProvider = \"writeIndexData\")\n    public void testWriteBasedOnNonRegularFeatureFile(final File inputFile, final IndexFactory.IndexType type, final  FeatureCodec codec) throws Exception {\n        final File tmpFolder = IOUtil.createTempDir(\"NonRegultarFeatureFile\").toFile();\n        // create the index\n        final Index index = IndexFactory.createIndex(inputFile, codec, type);\n        // try to write based on the tmpFolder\n        Assert.assertThrows(IOException.class, () -> index.writeBasedOnFeatureFile(tmpFolder));\n    }\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2014 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.tribble.index.tabix;\n\nimport htsjdk.HtsjdkTest;\nimport com.google.common.io.Files;\nimport htsjdk.samtools.util.BlockCompressedOutputStream;\nimport htsjdk.samtools.util.FileExtensions;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.Interval;\nimport htsjdk.tribble.AbstractFeatureReader;\nimport htsjdk.tribble.FeatureReader;\nimport htsjdk.tribble.TestUtils;\nimport htsjdk.tribble.Tribble;\nimport htsjdk.tribble.bed.BEDCodec;\nimport htsjdk.tribble.bed.BEDFeature;\nimport htsjdk.tribble.index.IndexFactory;\nimport htsjdk.tribble.util.LittleEndianOutputStream;\nimport htsjdk.variant.variantcontext.VariantContext;\nimport htsjdk.variant.variantcontext.writer.VariantContextWriter;\nimport htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder;\nimport htsjdk.variant.vcf.VCFCodec;\nimport htsjdk.variant.vcf.VCFFileReader;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.Iterator;\nimport java.util.List;\n\npublic class TabixIndexTest extends HtsjdkTest {\n    private static final File SMALL_TABIX_FILE = new File(TestUtils.DATA_DIR, \"tabix/trioDup.vcf.gz.tbi\");\n    private static final File BIGGER_TABIX_FILE = new File(TestUtils.DATA_DIR, \"tabix/bigger.vcf.gz.tbi\");\n\n    /**\n     * Read an existing index from disk, write it to a temp file, read that in, and assert that both in-memory\n     * representations are identical.  Disk representations may not be identical due to arbitrary bin order and\n     * compression differences.\n     */\n    @Test(dataProvider = \"readWriteTestDataProvider\")\n    public void readWriteTest(final File tabixFile) throws Exception {\n        final TabixIndex index = new TabixIndex(tabixFile);\n        final File indexFile = File.createTempFile(\"TabixIndexTest.\", FileExtensions.TABIX_INDEX);\n        indexFile.deleteOnExit();\n        final LittleEndianOutputStream los = new LittleEndianOutputStream(new BlockCompressedOutputStream(indexFile));\n        index.write(los);\n        los.close();\n        final TabixIndex index2 = new TabixIndex(indexFile);\n        Assert.assertEquals(index, index2);\n        // Unfortunately, can't do byte comparison of original file and temp file, because 1) different compression\n        // levels; and more importantly, arbitrary order of bins in bin list.\n    }\n\n    @DataProvider(name = \"readWriteTestDataProvider\")\n    public Object[][] readWriteTestDataProvider() {\n        return new Object[][]{\n                {SMALL_TABIX_FILE},\n                {BIGGER_TABIX_FILE}\n        };\n    }\n\n    //TODO: This test reads an existing .tbi on a .gz, but only writes a .tbi index for a plain vcf, which\n    // tabix doesn't appear to even allow.\n    @Test\n    public void testQueryProvidedItemsAmount() throws IOException {\n        final String VCF = \"src/test/resources/htsjdk/tribble/tabix/YRI.trio.2010_07.indel.sites.vcf\";\n        // Note that we store only compressed files\n        final File plainTextVcfInputFile = new File(VCF);\n        plainTextVcfInputFile.deleteOnExit();\n        final File plainTextVcfIndexFile = new File(VCF + \".tbi\");\n        plainTextVcfIndexFile.deleteOnExit();\n        final File compressedVcfInputFile = new File(VCF + \".gz\");\n        final File compressedTbiIndexFile = new File(VCF + \".gz.tbi\");\n        final VCFFileReader compressedVcfReader = new VCFFileReader(compressedVcfInputFile, compressedTbiIndexFile);\n\n        //create plain text VCF without \"index on the fly\" option\n        final VariantContextWriter plainTextVcfWriter = new VariantContextWriterBuilder()\n                .setOptions(VariantContextWriterBuilder.NO_OPTIONS)\n                .setOutputFile(VCF)\n                .build();\n        plainTextVcfWriter.writeHeader(compressedVcfReader.getFileHeader());\n        for (VariantContext vc : compressedVcfReader) {\n            if (vc != null) plainTextVcfWriter.add(vc);\n        }\n        plainTextVcfWriter.close();\n\n        IndexFactory.createTabixIndex(plainTextVcfInputFile,\n                new VCFCodec(),\n                TabixFormat.VCF,\n                new VCFFileReader(plainTextVcfInputFile, false).getFileHeader().getSequenceDictionary()\n        ) // create TabixIndex straight from plaintext VCF\n                .write(plainTextVcfIndexFile); // write it\n\n        //TODO: you can pass in a .tbi file as the index for a plain .vcf, but if you *don't* pass in the file name and\n        //just require an index, VCFFleREader will only look for a .idx on a plain vcf\n        final VCFFileReader plainTextVcfReader = new VCFFileReader(plainTextVcfInputFile, plainTextVcfIndexFile);\n        // Now we have both plaintext and compressed VCFs with provided TabixIndex-es and could test their \"queryability\"\n\n        // magic numbers chosen from just looking in provided VCF file\n        try {\n            // just somewhere in middle of chromosome\n            Assert.assertEquals(42, countIteratedElements(compressedVcfReader.query(\"1\", 868379 - 1, 1006891 + 1)));\n            Assert.assertEquals(42, countIteratedElements(plainTextVcfReader.query(\"1\", 868379 - 1, 1006891 + 1)));\n            // chromosome start\n            Assert.assertEquals(13, countIteratedElements(compressedVcfReader.query(\"1\", 1, 836463 + 1)));\n            Assert.assertEquals(13, countIteratedElements(plainTextVcfReader.query(\"1\", 1, 836463 + 1)));\n            // chromosome end\n            Assert.assertEquals(36, countIteratedElements(compressedVcfReader.query(\"1\", 76690833 - 1, 76837502 + 11111111)));\n            Assert.assertEquals(36, countIteratedElements(plainTextVcfReader.query(\"1\", 76690833 - 1, 76837502 + 11111111)));\n            // where's no one feature in the middle of chromosome\n            Assert.assertEquals(0, countIteratedElements(compressedVcfReader.query(\"1\", 36606472 + 1, 36623523 - 1)));\n            Assert.assertEquals(0, countIteratedElements(plainTextVcfReader.query(\"1\", 36606472 + 1, 36623523 - 1)));\n            // before chromosome\n            Assert.assertEquals(0, countIteratedElements(compressedVcfReader.query(\"1\", 1, 10)));\n            Assert.assertEquals(0, countIteratedElements(plainTextVcfReader.query(\"1\", 1, 10)));\n            // after chromosome\n            Assert.assertEquals(0, countIteratedElements(compressedVcfReader.query(\"1\", 76837502 * 15, 76837502 * 16)));\n            Assert.assertEquals(0, countIteratedElements(plainTextVcfReader.query(\"1\", 76837502 * 15, 76837502 * 16)));\n        } catch (NullPointerException e) {\n            Assert.fail(\"Exception caught on querying: \", e);\n            // before fix exception was thrown from 'TabixIndex.getBlocks()' on 'chunks.size()' while 'chunks == null' for plain files\n        } finally {\n            plainTextVcfReader.close();\n            compressedVcfReader.close();\n        }\n    }\n\n    @DataProvider(name = \"bedTabixIndexTestData\")\n    public Object[][] getBedIndexFactory(){\n        // These files have accompanying .tbi files created with tabix.\n        return new Object[][] {\n                new Object[] {\n                        // BGZP BED file with no header, 2 features\n                        new File(TestUtils.DATA_DIR, \"bed/2featuresNoHeader.bed.gz\"),\n                        Arrays.asList(\n                                new Interval(\"chr1\", 1, 10),\n                                new Interval(\"chr1\", 100, 1000000))\n                },\n                new Object[] {\n                        // BGZP BED file with no header, 3 features; one feature falls in between the query intervals\n                        new File(TestUtils.DATA_DIR, \"bed/3featuresNoHeader.bed.gz\"),\n                        Arrays.asList(\n                                new Interval(\"chr1\", 1, 10),\n                                new Interval(\"chr1\", 100, 1000000))\n                },\n                new Object[] {\n                        // same file as above (BGZP BED file with no header, 3 features), but change the query to return\n                        // only the single interval for the feature that falls in between the other features\n                        new File(TestUtils.DATA_DIR, \"bed/3featuresNoHeader.bed.gz\"),\n                        Arrays.asList(\n                                new Interval(\"chr1\", 15, 20))\n                },\n                new Object[] {\n                        // BGZP BED file with one line header, 2 features\n                        new File(TestUtils.DATA_DIR, \"bed/2featuresWithHeader.bed.gz\"),\n                        Arrays.asList(\n                                new Interval(\"chr1\", 1, 10),\n                                new Interval(\"chr1\", 100, 1000000))\n                },\n        };\n    }\n\n    @Test(dataProvider = \"bedTabixIndexTestData\")\n    public void testBedTabixIndex(\n            final File inputBed,\n            final List<Interval> queryIntervals\n    ) throws Exception {\n        // copy the input file and create an index for the copy\n        final File tempDir = IOUtil.createTempDir(\"testBedTabixIndex.tmp\").toFile();\n        tempDir.deleteOnExit();\n        final File tmpBed = new File(tempDir, inputBed.getName());\n        Files.copy(inputBed, tmpBed);\n        tmpBed.deleteOnExit();\n        final TabixIndex tabixIndexGz = IndexFactory.createTabixIndex(tmpBed, new BEDCodec(), null);\n        tabixIndexGz.writeBasedOnFeatureFile(tmpBed);\n        final File tmpIndex = Tribble.tabixIndexFile(tmpBed);\n        tmpIndex.deleteOnExit();\n\n        // iterate over the query intervals and validate the query results\n        try(final FeatureReader<BEDFeature> originalReader =\n                    AbstractFeatureReader.getFeatureReader(inputBed.getAbsolutePath(), new BEDCodec());\n            final FeatureReader<BEDFeature> createdReader =\n                    AbstractFeatureReader.getFeatureReader(tmpBed.getAbsolutePath(), new BEDCodec()))\n        {\n            for (final Interval interval: queryIntervals) {\n                final Iterator<BEDFeature> originalIt = originalReader.query(interval.getContig(), interval.getStart(), interval.getEnd());\n                final Iterator<BEDFeature> createdIt = createdReader.query(interval.getContig(), interval.getStart(), interval.getEnd());\n                while(originalIt.hasNext()) {\n                    Assert.assertTrue(createdIt.hasNext(), \"some features not returned from query\");\n                    BEDFeature bedOrig = originalIt.next();\n                    BEDFeature bedTmp = createdIt.next();\n                    Assert.assertEquals(bedOrig.getContig(), bedTmp.getContig());\n                    Assert.assertEquals(bedOrig.getStart(), bedTmp.getStart());\n                    Assert.assertEquals(bedOrig.getEnd(), bedTmp.getEnd());\n                }\n            }\n        }\n    }\n\n    private static int countIteratedElements(final Iterator iterator) {\n        int counter = 0;\n        while (iterator.hasNext()) {\n            iterator.next();\n            counter++;\n        }\n        return counter;\n    }\n}\n", "package htsjdk.variant.vcf;\n\nimport com.google.common.jimfs.Configuration;\nimport com.google.common.jimfs.Jimfs;\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.seekablestream.SeekableStream;\nimport htsjdk.samtools.seekablestream.SeekableStreamFactory;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.tribble.TestUtils;\nimport org.testng.Assert;\nimport org.testng.annotations.DataProvider;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.nio.file.FileSystem;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\n\n/**\n * Created by farjoun on 10/12/17.\n */\npublic class VCFFileReaderTest extends HtsjdkTest {\n    private static final File TEST_DATA_DIR = new File(\"src/test/resources/htsjdk/variant/\");\n\n    @DataProvider(name = \"queryableData\")\n    public Iterator<Object[]> queryableData() throws IOException {\n        List<Object[]> tests = new ArrayList<>();\n        tests.add(new Object[]{new File(TEST_DATA_DIR, \"NA12891.fp.vcf\"), false});\n        tests.add(new Object[]{new File(TEST_DATA_DIR, \"NA12891.vcf\"), false});\n        tests.add(new Object[]{VCFUtils.createTemporaryIndexedVcfFromInput(new File(TEST_DATA_DIR, \"NA12891.vcf\"), \"fingerprintcheckertest.tmp.\"), true});\n        tests.add(new Object[]{VCFUtils.createTemporaryIndexedVcfFromInput(new File(TEST_DATA_DIR, \"NA12891.vcf.gz\"), \"fingerprintcheckertest.tmp.\"), true});\n\n        return tests.iterator();\n    }\n\n    @Test(dataProvider = \"queryableData\")\n    public void testIsQueriable(final File vcf, final boolean expectedQueryable) throws Exception {\n        Assert.assertEquals(new VCFFileReader(vcf, false).isQueryable(), expectedQueryable);\n    }\n\n    @DataProvider(name = \"pathsData\")\n    Object[][] pathsData() {\n\n        final String TEST_DATA_DIR = \"src/test/resources/htsjdk/variant/\";\n        return new Object[][]{\n                // various ways to refer to a local file\n                {TEST_DATA_DIR + \"VCF4HeaderTest.vcf\", null, false, true},\n\n//                // this is almost a vcf, but not quite it's missing the #CHROM line and it has no content...\n                {TEST_DATA_DIR + \"Homo_sapiens_assembly38.tile_db_header.vcf\", null, false, false},\n\n//                // test that have indexes\n                {TEST_DATA_DIR + \"test.vcf.bgz\", TEST_DATA_DIR + \"test.vcf.bgz.tbi\", true, true},\n                {TEST_DATA_DIR + \"serialization_test.bcf\", TEST_DATA_DIR + \"serialization_test.bcf.idx\", true, true},\n                {TEST_DATA_DIR + \"test1.vcf\", TEST_DATA_DIR + \"test1.vcf.idx\", true, true},\n//\n//                // test that lack indexes (should succeed)\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf.gz\", null, false, true},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf\", null, false, true},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex but has a space.vcf\", null, false, true},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.bcf\", null, false, true},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf.bgz\", null, false, true},\n//\n//                // test that lack indexes should fail)\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf.gz\", null, true, false},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf\", null, true, false},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.bcf\", null, true, false},\n                {TEST_DATA_DIR + \"VcfThatLacksAnIndex.vcf.bgz\", null, true, false},\n//\n//                // testing that v4.2 parses Source/Version fields, see issue #517\n                {TEST_DATA_DIR + \"Vcf4.2WithSourceVersionInfoFields.vcf\", null, false, true},\n//\n//                // should reject bcf v2.2 on read, see issue https://github.com/samtools/htsjdk/issues/1323\n                {TEST_DATA_DIR + \"BCFVersion22Uncompressed.bcf\", null, false, false}\n        };\n    }\n\n    @Test(dataProvider = \"pathsData\", timeOut = 60_000)\n    public void testCanOpenVCFPathReader(final String file, final String index, final boolean requiresIndex, final boolean shouldSucceed) throws Exception {\n        try (FileSystem fs = Jimfs.newFileSystem(\"test\", Configuration.unix())) {\n            final Path tribbleFileInJimfs = TestUtils.getTribbleFileInJimfs(file, index, fs);\n            try (final VCFFileReader reader = new VCFFileReader(tribbleFileInJimfs, requiresIndex)) {\n\n                final VCFHeader header = reader.getFileHeader();\n                reader.iterator().stream().forEach(\n                        v->v.getGenotypes()\n                                .stream()\n                                .count());\n            } catch (Exception e) {\n                if (shouldSucceed) {\n                    throw e;\n                } else {\n                    return;\n                }\n            }\n        }\n        // fail if a test that should have thrown didn't\n        Assert.assertTrue(shouldSucceed, \"Test should have failed but succeeded\");\n    }\n\n    @Test\n    public void testTabixFileWithEmbeddedSpaces() throws IOException {\n        final File testVCF =  new File(TEST_DATA_DIR, \"HiSeq.10000.vcf.bgz\");\n        final File testTBI =  new File(TEST_DATA_DIR, \"HiSeq.10000.vcf.bgz.tbi\");\n\n        // Copy the input files into a temporary directory with embedded spaces in the name.\n        // This test needs to include the associated .tbi file because we want to force execution\n        // of the tabix code path.\n        final File tempDir = IOUtil.createTempDir(\"test spaces\").toFile();\n        Assert.assertTrue(tempDir.getAbsolutePath().contains(\" \"));\n        tempDir.deleteOnExit();\n        final File inputVCF = new File(tempDir, \"HiSeq.10000.vcf.bgz\");\n        inputVCF.deleteOnExit();\n        final File inputTBI = new File(tempDir, \"HiSeq.10000.vcf.bgz.tbi\");\n        inputTBI.deleteOnExit();\n        IOUtil.copyFile(testVCF, inputVCF);\n        IOUtil.copyFile(testTBI, inputTBI);\n\n        try (final VCFFileReader vcfFileReader = new VCFFileReader(inputVCF)) {\n            Assert.assertNotNull(vcfFileReader.getFileHeader());\n        }\n\n    }\n\n}\n", "/*\n * The MIT License\n *\n * Copyright (c) 2019 The Broad Institute\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n * THE SOFTWARE.\n */\npackage htsjdk.variant.vcf;\n\nimport htsjdk.HtsjdkTest;\nimport htsjdk.samtools.util.BlockCompressedInputStream;\nimport htsjdk.samtools.util.BlockCompressedOutputStream;\nimport htsjdk.samtools.util.BlockCompressedStreamConstants;\nimport htsjdk.samtools.util.FileExtensions;\nimport htsjdk.samtools.util.IOUtil;\nimport htsjdk.samtools.util.RuntimeIOException;\nimport htsjdk.samtools.util.TerminatorlessBlockCompressedOutputStream;\nimport htsjdk.tribble.index.IndexCreator;\nimport htsjdk.tribble.index.IndexFactory;\nimport htsjdk.tribble.index.tabix.StreamBasedTabixIndexCreator;\nimport htsjdk.tribble.index.tabix.TabixFormat;\nimport htsjdk.tribble.index.tabix.TabixIndex;\nimport htsjdk.tribble.index.tabix.TabixIndexMerger;\nimport htsjdk.tribble.index.tabix.TbiEqualityChecker;\nimport htsjdk.utils.ValidationUtils;\nimport htsjdk.variant.variantcontext.VariantContext;\nimport htsjdk.variant.variantcontext.writer.Options;\nimport htsjdk.variant.variantcontext.writer.VariantContextWriter;\nimport htsjdk.variant.variantcontext.writer.VariantContextWriterBuilder;\nimport org.testng.Assert;\nimport org.testng.annotations.Test;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.List;\nimport java.util.stream.Collectors;\n\npublic class VCFMergerTest extends HtsjdkTest {\n\n    private final static Path VCF_FILE = new File(\"src/test/resources/htsjdk/variant/HiSeq.10000.vcf.bgz\").toPath();\n\n    /**\n     * Writes a <i>partitioned VCF</i>.\n     *\n     * Note that this writer is only for single-threaded use. Consider using the implementation in Disq for a partitioned VCF writer\n     * that works with multiple threads or in a distributed setting.\n     *\n     * @see TabixIndexMerger\n     */\n    static class PartitionedVCFFileWriter implements VariantContextWriter {\n        private final Path outputDir;\n        private VCFHeader header;\n        private final int recordsPerPart;\n        private final boolean compressed;\n        private long recordCount = 0;\n        private int partNumber = -1;\n        private VariantContextWriter variantContextWriter;\n\n        public PartitionedVCFFileWriter(Path outputDir, int recordsPerPart, boolean compressed) {\n            this.outputDir = outputDir;\n            this.recordsPerPart = recordsPerPart;\n            this.compressed = compressed;\n        }\n\n        @Override\n        public void writeHeader(VCFHeader header) {\n            ValidationUtils.nonNull(header.getSequenceDictionary(), \"VCF header must have a sequence dictionary\");\n            this.header = header;\n            try (OutputStream headerOut = Files.newOutputStream(outputDir.resolve(\"header\"))) {\n                OutputStream out =\n                        compressed ? new BlockCompressedOutputStream(headerOut, (Path) null) : headerOut;\n                VariantContextWriter writer =\n                        new VariantContextWriterBuilder().clearOptions().setOutputVCFStream(out).build();\n                writer.writeHeader(header);\n                out.flush(); // don't close BlockCompressedOutputStream since we don't want to write the terminator after the header\n            } catch (IOException e) {\n                throw new RuntimeIOException(e);\n            }\n        }\n\n        @Override\n        public void add(VariantContext vc) {\n            if (recordCount % recordsPerPart == 0) {\n                // start a new part\n                try {\n                    if (variantContextWriter != null) {\n                        variantContextWriter.close();\n                    }\n                    partNumber++;\n                    String partName = String.format(\"part-%05d\", partNumber);\n                    OutputStream out = Files.newOutputStream(outputDir.resolve(partName));\n                    OutputStream indexOut = Files.newOutputStream(outputDir.resolve(\".\" + partName + FileExtensions.TABIX_INDEX));\n                    IndexCreator tabixIndexCreator = new StreamBasedTabixIndexCreator(\n                                        header.getSequenceDictionary(), TabixFormat.VCF, indexOut);\n                    this.variantContextWriter = new VariantContextWriterBuilder()\n                            .setOutputStream(\n                                    compressed ? new TerminatorlessBlockCompressedOutputStream(out) : out)\n                            .setReferenceDictionary(header.getSequenceDictionary())\n                            .setIndexCreator(tabixIndexCreator)\n                            .modifyOption(Options.INDEX_ON_THE_FLY, tabixIndexCreator != null)\n                            .unsetOption(Options.DO_NOT_WRITE_GENOTYPES)\n                            .unsetOption(Options.ALLOW_MISSING_FIELDS_IN_HEADER)\n                            .unsetOption(Options.WRITE_FULL_FORMAT_FIELD)\n                            .build();\n                    variantContextWriter.setHeader(header);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n            recordCount++;\n            variantContextWriter.add(vc);\n        }\n\n        @Override\n        public void setHeader(VCFHeader header) {\n            // ignore\n        }\n\n        @Override\n        public void close() {\n            if (variantContextWriter != null) {\n                variantContextWriter.close();\n            }\n            if (compressed) {\n                // write terminator\n                try (OutputStream out = Files.newOutputStream(outputDir.resolve(\"terminator\"))) {\n                    out.write(BlockCompressedStreamConstants.EMPTY_GZIP_BLOCK);\n                } catch (IOException e) {\n                    throw new RuntimeIOException(e);\n                }\n            }\n        }\n\n        @Override\n        public boolean checkError() {\n            return variantContextWriter != null && variantContextWriter.checkError();\n        }\n    }\n\n    /**\n     * Merge the files created by {@link PartitionedVCFFileWriter} into a single VCF file and index.\n     */\n    static class PartitionedVCFFileMerger {\n        public void merge(Path dir, Path outputVcf, Path outputTbi) throws IOException {\n            Path headerPath = dir.resolve(\"header\");\n            List<Path> vcfParts = Files.list(dir)\n                    .filter(path -> !path.toString().endsWith(FileExtensions.TABIX_INDEX)) // include header and terminator\n                    .sorted()\n                    .collect(Collectors.toList());\n            List<Path> tbiParts = Files.list(dir)\n                    .filter(path -> path.toString().endsWith(FileExtensions.TABIX_INDEX))\n                    .sorted()\n                    .collect(Collectors.toList());\n\n            Assert.assertTrue(tbiParts.size() > 1);\n\n            ValidationUtils.validateArg(vcfParts.size() - 2 == tbiParts.size(), \"Number of VCF part files does not match number of TBI files (\" + tbiParts.size() + \")\");\n\n            // merge VCF parts\n            try (OutputStream out = Files.newOutputStream(outputVcf)) {\n                for (Path vcfPart : vcfParts) {\n                    Files.copy(vcfPart, out);\n                }\n            }\n\n            // merge index parts\n            try (OutputStream out = Files.newOutputStream(outputTbi)) {\n                TabixIndexMerger tabixIndexMerger = new TabixIndexMerger(out, Files.size(headerPath));\n                int i = 1; // start from 1 since we ignore the header\n                for (Path tbiPart : tbiParts) {\n                    try (InputStream in = Files.newInputStream(tbiPart)) {\n                        TabixIndex index = new TabixIndex(new BlockCompressedInputStream(in));\n                        tabixIndexMerger.processIndex(index, Files.size(vcfParts.get(i++)));\n                    }\n                }\n                tabixIndexMerger.finish(Files.size(outputVcf));\n            }\n        }\n    }\n\n    private static Path indexVcf(Path vcf, Path tbi) throws IOException {\n        TabixIndex tabixIndex = IndexFactory.createTabixIndex(vcf.toFile(), new VCFCodec(), null);\n        tabixIndex.write(tbi);\n        return tbi;\n    }\n\n    @Test\n    public void test() throws IOException {\n        final Path outputDir = IOUtil.createTempDir(this.getClass().getSimpleName() + \".tmp\");\n        IOUtil.deleteOnExit(outputDir);\n\n        final Path outputVcf = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.COMPRESSED_VCF).toPath();\n        IOUtil.deleteOnExit(outputVcf);\n\n        final Path outputTbi = IOUtil.addExtension(outputVcf, FileExtensions.TABIX_INDEX);\n        IOUtil.deleteOnExit(outputTbi);\n\n        final Path outputTbiMerged = File.createTempFile(this.getClass().getSimpleName() + \".\", FileExtensions.TABIX_INDEX).toPath();\n        IOUtil.deleteOnExit(outputTbiMerged);\n\n        // 1. Read an input VCF and write it out in partitioned form (header, parts, terminator)\n        try (VCFFileReader vcfReader = new VCFFileReader(VCF_FILE, false);\n             PartitionedVCFFileWriter partitionedVCFFileWriter = new PartitionedVCFFileWriter(outputDir, 2500, true)) {\n            partitionedVCFFileWriter.writeHeader(vcfReader.getFileHeader());\n            for (VariantContext vc : vcfReader) {\n                partitionedVCFFileWriter.add(vc);\n            }\n        }\n\n        // 2. Merge the partitioned VCF and index\n        new PartitionedVCFFileMerger().merge(outputDir, outputVcf, outputTbiMerged);\n\n        // 3. Index the merged VCF (using regular indexing)\n        indexVcf(outputVcf, outputTbi);\n\n        // 4. Assert that the merged index is the same as the index produced from the merged file\n\n        // Don't check for strict equality (byte identical), since the TBI files\n        // generated by the two methods have one difference: the final virtual\n        // file position in the last bin is at the end of the empty BGZF block\n        // in TBI files generated by in the usual way, and is at the start of the empty block\n        // for those generated by merging. These represent the same concrete file position, even though\n        // the virtual file positions are different.\n        TbiEqualityChecker.assertEquals(outputVcf, outputTbi, outputTbiMerged, false);\n    }\n}\n"], "filenames": ["src/main/java/htsjdk/samtools/CoordinateSortedPairInfoMap.java", "src/main/java/htsjdk/samtools/util/IOUtil.java", "src/test/java/htsjdk/samtools/BAMMergerTest.java", "src/test/java/htsjdk/samtools/CRAMFileWriterTest.java", "src/test/java/htsjdk/samtools/CRAMMergerTest.java", "src/test/java/htsjdk/samtools/reference/FastaSequenceIndexCreatorTest.java", "src/test/java/htsjdk/samtools/seekablestream/SeekableStreamFactoryTest.java", "src/test/java/htsjdk/samtools/util/IOUtilTest.java", "src/test/java/htsjdk/tribble/index/IndexFactoryTest.java", "src/test/java/htsjdk/tribble/index/IndexTest.java", "src/test/java/htsjdk/tribble/index/tabix/TabixIndexTest.java", "src/test/java/htsjdk/variant/vcf/VCFFileReaderTest.java", "src/test/java/htsjdk/variant/vcf/VCFMergerTest.java"], "buggy_code_start_loc": [59, 977, 225, 274, 196, 66, 55, 315, 169, 134, 193, 113, 209], "buggy_code_end_loc": [60, 996, 226, 279, 197, 67, 56, 374, 254, 135, 194, 114, 210], "fixing_code_start_loc": [59, 977, 225, 274, 196, 66, 55, 315, 169, 134, 193, 113, 209], "fixing_code_end_loc": [60, 1005, 226, 279, 197, 67, 56, 373, 254, 135, 194, 114, 210], "type": "CWE-668", "message": "The package com.github.samtools:htsjdk before 3.0.1 are vulnerable to Creation of Temporary File in Directory with Insecure Permissions due to the createTempDir() function in util/IOUtil.java not checking for the existence of the temporary directory before attempting to create it.", "other": {"cve": {"id": "CVE-2022-21126", "sourceIdentifier": "report@snyk.io", "published": "2022-11-29T17:15:11.043", "lastModified": "2022-12-01T21:08:12.207", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The package com.github.samtools:htsjdk before 3.0.1 are vulnerable to Creation of Temporary File in Directory with Insecure Permissions due to the createTempDir() function in util/IOUtil.java not checking for the existence of the temporary directory before attempting to create it."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}, {"source": "report@snyk.io", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:L", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "LOW", "integrityImpact": "LOW", "availabilityImpact": "LOW", "baseScore": 7.3, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.4}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-668"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:samtools:htsjdk:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.0.1", "matchCriteriaId": "33B69CF9-290D-415B-90F3-E5232574726B"}]}]}], "references": [{"url": "https://github.com/samtools/htsjdk/commit/4a4024a97ee3e87096df6ad9b22c8260bd527772", "source": "report@snyk.io", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/samtools/htsjdk/pull/1617", "source": "report@snyk.io", "tags": ["Exploit", "Patch", "Third Party Advisory"]}, {"url": "https://security.snyk.io/vuln/SNYK-JAVA-COMGITHUBSAMTOOLS-3149901", "source": "report@snyk.io", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/samtools/htsjdk/commit/4a4024a97ee3e87096df6ad9b22c8260bd527772"}}