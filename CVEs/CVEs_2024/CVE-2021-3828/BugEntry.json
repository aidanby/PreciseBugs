{"buggy_code": ["# Natural Language Toolkit: Comparative Sentence Corpus Reader\n#\n# Copyright (C) 2001-2021 NLTK Project\n# Author: Pierpaolo Pantone <24alsecondo@gmail.com>\n# URL: <http://nltk.org/>\n# For license information, see LICENSE.TXT\n\n\"\"\"\nCorpusReader for the Comparative Sentence Dataset.\n\n- Comparative Sentence Dataset information -\n\nAnnotated by: Nitin Jindal and Bing Liu, 2006.\n              Department of Computer Sicence\n              University of Illinois at Chicago\n\nContact: Nitin Jindal, njindal@cs.uic.edu\n         Bing Liu, liub@cs.uic.edu (http://www.cs.uic.edu/~liub)\n\nDistributed with permission.\n\nRelated papers:\n\n- Nitin Jindal and Bing Liu. \"Identifying Comparative Sentences in Text Documents\".\n   Proceedings of the ACM SIGIR International Conference on Information Retrieval\n   (SIGIR-06), 2006.\n\n- Nitin Jindal and Bing Liu. \"Mining Comprative Sentences and Relations\".\n   Proceedings of Twenty First National Conference on Artificial Intelligence\n   (AAAI-2006), 2006.\n\n- Murthy Ganapathibhotla and Bing Liu. \"Mining Opinions in Comparative Sentences\".\n    Proceedings of the 22nd International Conference on Computational Linguistics\n    (Coling-2008), Manchester, 18-22 August, 2008.\n\"\"\"\nimport re\n\nfrom nltk.corpus.reader.api import *\nfrom nltk.tokenize import *\n\n# Regular expressions for dataset components\nSTARS = re.compile(r\"^\\*+$\")\nCOMPARISON = re.compile(r\"<cs-[1234]>\")\nCLOSE_COMPARISON = re.compile(r\"</cs-[1234]>\")\nGRAD_COMPARISON = re.compile(r\"<cs-[123]>\")\nNON_GRAD_COMPARISON = re.compile(r\"<cs-4>\")\nENTITIES_FEATS = re.compile(r\"(\\d)_((?:[\\.\\w\\s/-](?!\\d_))+)\")\nKEYWORD = re.compile(r\"\\((?!.*\\()(.*)\\)$\")\n\n\nclass Comparison:\n    \"\"\"\n    A Comparison represents a comparative sentence and its constituents.\n    \"\"\"\n\n    def __init__(\n        self,\n        text=None,\n        comp_type=None,\n        entity_1=None,\n        entity_2=None,\n        feature=None,\n        keyword=None,\n    ):\n        \"\"\"\n        :param text: a string (optionally tokenized) containing a comparison.\n        :param comp_type: an integer defining the type of comparison expressed.\n            Values can be: 1 (Non-equal gradable), 2 (Equative), 3 (Superlative),\n            4 (Non-gradable).\n        :param entity_1: the first entity considered in the comparison relation.\n        :param entity_2: the second entity considered in the comparison relation.\n        :param feature: the feature considered in the comparison relation.\n        :param keyword: the word or phrase which is used for that comparative relation.\n        \"\"\"\n        self.text = text\n        self.comp_type = comp_type\n        self.entity_1 = entity_1\n        self.entity_2 = entity_2\n        self.feature = feature\n        self.keyword = keyword\n\n    def __repr__(self):\n        return (\n            'Comparison(text=\"{}\", comp_type={}, entity_1=\"{}\", entity_2=\"{}\", '\n            'feature=\"{}\", keyword=\"{}\")'\n        ).format(\n            self.text,\n            self.comp_type,\n            self.entity_1,\n            self.entity_2,\n            self.feature,\n            self.keyword,\n        )\n\n\nclass ComparativeSentencesCorpusReader(CorpusReader):\n    \"\"\"\n    Reader for the Comparative Sentence Dataset by Jindal and Liu (2006).\n\n        >>> from nltk.corpus import comparative_sentences\n        >>> comparison = comparative_sentences.comparisons()[0]\n        >>> comparison.text\n        ['its', 'fast-forward', 'and', 'rewind', 'work', 'much', 'more', 'smoothly',\n        'and', 'consistently', 'than', 'those', 'of', 'other', 'models', 'i', \"'ve\",\n        'had', '.']\n        >>> comparison.entity_2\n        'models'\n        >>> (comparison.feature, comparison.keyword)\n        ('rewind', 'more')\n        >>> len(comparative_sentences.comparisons())\n        853\n    \"\"\"\n\n    CorpusView = StreamBackedCorpusView\n\n    def __init__(\n        self,\n        root,\n        fileids,\n        word_tokenizer=WhitespaceTokenizer(),\n        sent_tokenizer=None,\n        encoding=\"utf8\",\n    ):\n        \"\"\"\n        :param root: The root directory for this corpus.\n        :param fileids: a list or regexp specifying the fileids in this corpus.\n        :param word_tokenizer: tokenizer for breaking sentences or paragraphs\n            into words. Default: `WhitespaceTokenizer`\n        :param sent_tokenizer: tokenizer for breaking paragraphs into sentences.\n        :param encoding: the encoding that should be used to read the corpus.\n        \"\"\"\n\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._word_tokenizer = word_tokenizer\n        self._sent_tokenizer = sent_tokenizer\n        self._readme = \"README.txt\"\n\n    def comparisons(self, fileids=None):\n        \"\"\"\n        Return all comparisons in the corpus.\n\n        :param fileids: a list or regexp specifying the ids of the files whose\n            comparisons have to be returned.\n        :return: the given file(s) as a list of Comparison objects.\n        :rtype: list(Comparison)\n        \"\"\"\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, str):\n            fileids = [fileids]\n        return concat(\n            [\n                self.CorpusView(path, self._read_comparison_block, encoding=enc)\n                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n            ]\n        )\n\n    def keywords(self, fileids=None):\n        \"\"\"\n        Return a set of all keywords used in the corpus.\n\n        :param fileids: a list or regexp specifying the ids of the files whose\n            keywords have to be returned.\n        :return: the set of keywords and comparative phrases used in the corpus.\n        :rtype: set(str)\n        \"\"\"\n        all_keywords = concat(\n            [\n                self.CorpusView(path, self._read_keyword_block, encoding=enc)\n                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n            ]\n        )\n\n        keywords_set = {keyword.lower() for keyword in all_keywords if keyword}\n        return keywords_set\n\n    def keywords_readme(self):\n        \"\"\"\n        Return the list of words and constituents considered as clues of a\n        comparison (from listOfkeywords.txt).\n        \"\"\"\n        keywords = []\n        with self.open(\"listOfkeywords.txt\") as fp:\n            raw_text = fp.read()\n        for line in raw_text.split(\"\\n\"):\n            if not line or line.startswith(\"//\"):\n                continue\n            keywords.append(line.strip())\n        return keywords\n\n    def sents(self, fileids=None):\n        \"\"\"\n        Return all sentences in the corpus.\n\n        :param fileids: a list or regexp specifying the ids of the files whose\n            sentences have to be returned.\n        :return: all sentences of the corpus as lists of tokens (or as plain\n            strings, if no word tokenizer is specified).\n        :rtype: list(list(str)) or list(str)\n        \"\"\"\n        return concat(\n            [\n                self.CorpusView(path, self._read_sent_block, encoding=enc)\n                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n            ]\n        )\n\n    def words(self, fileids=None):\n        \"\"\"\n        Return all words and punctuation symbols in the corpus.\n\n        :param fileids: a list or regexp specifying the ids of the files whose\n            words have to be returned.\n        :return: the given file(s) as a list of words and punctuation symbols.\n        :rtype: list(str)\n        \"\"\"\n        return concat(\n            [\n                self.CorpusView(path, self._read_word_block, encoding=enc)\n                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n            ]\n        )\n\n    def _read_comparison_block(self, stream):\n        while True:\n            line = stream.readline()\n            if not line:\n                return []  # end of file.\n            comparison_tags = re.findall(COMPARISON, line)\n            if comparison_tags:\n                grad_comparisons = re.findall(GRAD_COMPARISON, line)\n                non_grad_comparisons = re.findall(NON_GRAD_COMPARISON, line)\n                # Advance to the next line (it contains the comparative sentence)\n                comparison_text = stream.readline().strip()\n                if self._word_tokenizer:\n                    comparison_text = self._word_tokenizer.tokenize(comparison_text)\n                # Skip the next line (it contains closing comparison tags)\n                stream.readline()\n                # If gradable comparisons are found, create Comparison instances\n                # and populate their fields\n                comparison_bundle = []\n                if grad_comparisons:\n                    # Each comparison tag has its own relations on a separate line\n                    for comp in grad_comparisons:\n                        comp_type = int(re.match(r\"<cs-(\\d)>\", comp).group(1))\n                        comparison = Comparison(\n                            text=comparison_text, comp_type=comp_type\n                        )\n                        line = stream.readline()\n                        entities_feats = ENTITIES_FEATS.findall(line)\n                        if entities_feats:\n                            for (code, entity_feat) in entities_feats:\n                                if code == \"1\":\n                                    comparison.entity_1 = entity_feat.strip()\n                                elif code == \"2\":\n                                    comparison.entity_2 = entity_feat.strip()\n                                elif code == \"3\":\n                                    comparison.feature = entity_feat.strip()\n                        keyword = KEYWORD.findall(line)\n                        if keyword:\n                            comparison.keyword = keyword[0]\n                        comparison_bundle.append(comparison)\n                # If non-gradable comparisons are found, create a simple Comparison\n                # instance for each one\n                if non_grad_comparisons:\n                    for comp in non_grad_comparisons:\n                        # comp_type in this case should always be 4.\n                        comp_type = int(re.match(r\"<cs-(\\d)>\", comp).group(1))\n                        comparison = Comparison(\n                            text=comparison_text, comp_type=comp_type\n                        )\n                        comparison_bundle.append(comparison)\n                # Flatten the list of comparisons before returning them\n                # return concat([comparison_bundle])\n                return comparison_bundle\n\n    def _read_keyword_block(self, stream):\n        keywords = []\n        for comparison in self._read_comparison_block(stream):\n            keywords.append(comparison.keyword)\n        return keywords\n\n    def _read_sent_block(self, stream):\n        while True:\n            line = stream.readline()\n            if re.match(STARS, line):\n                while True:\n                    line = stream.readline()\n                    if re.match(STARS, line):\n                        break\n                continue\n            if (\n                not re.findall(COMPARISON, line)\n                and not ENTITIES_FEATS.findall(line)\n                and not re.findall(CLOSE_COMPARISON, line)\n            ):\n                if self._sent_tokenizer:\n                    return [\n                        self._word_tokenizer.tokenize(sent)\n                        for sent in self._sent_tokenizer.tokenize(line)\n                    ]\n                else:\n                    return [self._word_tokenizer.tokenize(line)]\n\n    def _read_word_block(self, stream):\n        words = []\n        for sent in self._read_sent_block(stream):\n            words.extend(sent)\n        return words\n", ".. Copyright (C) 2001-2021 NLTK Project\n.. For license information, see LICENSE.TXT\n\n================\n Corpus Readers\n================\n\nThe `nltk.corpus` package defines a collection of *corpus reader*\nclasses, which can be used to access the contents of a diverse set of\ncorpora.  The list of available corpora is given at:\n\nhttp://www.nltk.org/nltk_data/\n\nEach corpus reader class is specialized to handle a specific\ncorpus format.  In addition, the `nltk.corpus` package automatically\ncreates a set of corpus reader instances that can be used to access\nthe corpora in the NLTK data package.\nSection `Corpus Reader Objects`_ (\"Corpus Reader Objects\") describes\nthe corpus reader instances that can be used to read the corpora in\nthe NLTK data package.  Section `Corpus Reader Classes`_ (\"Corpus\nReader Classes\") describes the corpus reader classes themselves, and\ndiscusses the issues involved in creating new corpus reader objects\nand new corpus reader classes.  Section `Regression Tests`_\n(\"Regression Tests\") contains regression tests for the corpus readers\nand associated functions and classes.\n\n.. contents:: **Table of Contents**\n  :depth: 2\n  :backlinks: none\n\n---------------------\nCorpus Reader Objects\n---------------------\n\nOverview\n========\n\nNLTK includes a diverse set of corpora which can be\nread using the ``nltk.corpus`` package.  Each corpus is accessed by\nmeans of a \"corpus reader\" object from ``nltk.corpus``:\n\n    >>> import nltk.corpus\n    >>> # The Brown corpus:\n    >>> print(str(nltk.corpus.brown).replace('\\\\\\\\','/'))\n    <CategorizedTaggedCorpusReader in '.../corpora/brown'...>\n    >>> # The Penn Treebank Corpus:\n    >>> print(str(nltk.corpus.treebank).replace('\\\\\\\\','/'))\n    <BracketParseCorpusReader in '.../corpora/treebank/combined'...>\n    >>> # The Name Genders Corpus:\n    >>> print(str(nltk.corpus.names).replace('\\\\\\\\','/'))\n    <WordListCorpusReader in '.../corpora/names'...>\n    >>> # The Inaugural Address Corpus:\n    >>> print(str(nltk.corpus.inaugural).replace('\\\\\\\\','/'))\n    <PlaintextCorpusReader in '.../corpora/inaugural'...>\n\nMost corpora consist of a set of files, each containing a document (or\nother pieces of text).  A list of identifiers for these files is\naccessed via the ``fileids()`` method of the corpus reader:\n\n    >>> nltk.corpus.treebank.fileids()\n    ['wsj_0001.mrg', 'wsj_0002.mrg', 'wsj_0003.mrg', 'wsj_0004.mrg', ...]\n    >>> nltk.corpus.inaugural.fileids()\n    ['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...]\n\nEach corpus reader provides a variety of methods to read data from the\ncorpus, depending on the format of the corpus.  For example, plaintext\ncorpora support methods to read the corpus as raw text, a list of\nwords, a list of sentences, or a list of paragraphs.\n\n    >>> from nltk.corpus import inaugural\n    >>> inaugural.raw('1789-Washington.txt')\n    'Fellow-Citizens of the Senate ...'\n    >>> inaugural.words('1789-Washington.txt')\n    ['Fellow', '-', 'Citizens', 'of', 'the', ...]\n    >>> inaugural.sents('1789-Washington.txt')\n    [['Fellow', '-', 'Citizens'...], ['Among', 'the', 'vicissitudes'...]...]\n    >>> inaugural.paras('1789-Washington.txt')\n    [[['Fellow', '-', 'Citizens'...]],\n     [['Among', 'the', 'vicissitudes'...],\n      ['On', 'the', 'one', 'hand', ',', 'I'...]...]...]\n\nEach of these reader methods may be given a single document's item\nname or a list of document item names.  When given a list of document\nitem names, the reader methods will concatenate together the contents\nof the individual documents.\n\n    >>> l1 = len(inaugural.words('1789-Washington.txt'))\n    >>> l2 = len(inaugural.words('1793-Washington.txt'))\n    >>> l3 = len(inaugural.words(['1789-Washington.txt', '1793-Washington.txt']))\n    >>> print('%s+%s == %s' % (l1, l2, l3))\n    1538+147 == 1685\n\nIf the reader methods are called without any arguments, they will\ntypically load all documents in the corpus.\n\n    >>> len(inaugural.words())\n    149797\n\nIf a corpus contains a README file, it can be accessed with a ``readme()`` method:\n\n    >>> inaugural.readme()[:32]\n    'C-Span Inaugural Address Corpus\\n'\n\nPlaintext Corpora\n=================\n\nHere are the first few words from each of NLTK's plaintext corpora:\n\n    >>> nltk.corpus.abc.words()\n    ['PM', 'denies', 'knowledge', 'of', 'AWB', ...]\n    >>> nltk.corpus.genesis.words()\n    ['In', 'the', 'beginning', 'God', 'created', ...]\n    >>> nltk.corpus.gutenberg.words(fileids='austen-emma.txt')\n    ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ...]\n    >>> nltk.corpus.inaugural.words()\n    ['Fellow', '-', 'Citizens', 'of', 'the', ...]\n    >>> nltk.corpus.state_union.words()\n    ['PRESIDENT', 'HARRY', 'S', '.', 'TRUMAN', \"'\", ...]\n    >>> nltk.corpus.webtext.words()\n    ['Cookie', 'Manager', ':', '\"', 'Don', \"'\", 't', ...]\n\nTagged Corpora\n==============\n\nIn addition to the plaintext corpora, NLTK's data package also\ncontains a wide variety of annotated corpora.  For example, the Brown\nCorpus is annotated with part-of-speech tags, and defines additional\nmethods ``tagged_*()`` which words as `(word,tag)` tuples, rather\nthan just bare word strings.\n\n    >>> from nltk.corpus import brown\n    >>> print(brown.words())\n    ['The', 'Fulton', 'County', 'Grand', 'Jury', ...]\n    >>> print(brown.tagged_words())\n    [('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n    >>> print(brown.sents())\n    [['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]\n    >>> print(brown.tagged_sents())\n    [[('The', 'AT'), ('Fulton', 'NP-TL')...],\n     [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR')...]...]\n    >>> print(brown.paras(categories='reviews'))\n    [[['It', 'is', 'not', 'news', 'that', 'Nathan', 'Milstein'...],\n      ['Certainly', 'not', 'in', 'Orchestra', 'Hall', 'where'...]],\n     [['There', 'was', 'about', 'that', 'song', 'something', ...],\n      ['Not', 'the', 'noblest', 'performance', 'we', 'have', ...], ...], ...]\n    >>> print(brown.tagged_paras(categories='reviews'))\n    [[[('It', 'PPS'), ('is', 'BEZ'), ('not', '*'), ...],\n      [('Certainly', 'RB'), ('not', '*'), ('in', 'IN'), ...]],\n     [[('There', 'EX'), ('was', 'BEDZ'), ('about', 'IN'), ...],\n      [('Not', '*'), ('the', 'AT'), ('noblest', 'JJT'), ...], ...], ...]\n\nSimilarly, the Indian Language POS-Tagged Corpus includes samples of\nIndian text annotated with part-of-speech tags:\n\n    >>> from nltk.corpus import indian\n    >>> print(indian.words()) # doctest: +SKIP\n    ['\\xe0\\xa6\\xae\\xe0\\xa6\\xb9\\xe0\\xa6\\xbf\\...',\n     '\\xe0\\xa6\\xb8\\xe0\\xa6\\xa8\\xe0\\xa7\\x8d\\xe0...', ...]\n    >>> print(indian.tagged_words()) # doctest: +SKIP\n    [('\\xe0\\xa6\\xae\\xe0\\xa6\\xb9\\xe0\\xa6\\xbf...', 'NN'),\n     ('\\xe0\\xa6\\xb8\\xe0\\xa6\\xa8\\xe0\\xa7\\x8d\\xe0...', 'NN'), ...]\n\nSeveral tagged corpora support access to a simplified, universal tagset, e.g. where all nouns\ntags are collapsed to a single category ``NOUN``:\n\n    >>> print(brown.tagged_sents(tagset='universal'))\n    [[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ...],\n     [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ...]...]\n    >>> from nltk.corpus import conll2000, switchboard\n    >>> print(conll2000.tagged_words(tagset='universal'))\n    [('Confidence', 'NOUN'), ('in', 'ADP'), ...]\n\nUse ``nltk.app.pos_concordance()`` to access a GUI for searching tagged corpora.\n\nChunked Corpora\n===============\n\nThe CoNLL corpora also provide chunk structures, which are encoded as\nflat trees.  The CoNLL 2000 Corpus includes phrasal chunks; and the\nCoNLL 2002 Corpus includes named entity chunks.\n\n    >>> from nltk.corpus import conll2000, conll2002\n    >>> print(conll2000.sents())\n    [['Confidence', 'in', 'the', 'pound', 'is', 'widely', ...],\n     ['Chancellor', 'of', 'the', 'Exchequer', ...], ...]\n    >>> for tree in conll2000.chunked_sents()[:2]:\n    ...     print(tree)\n    (S\n      (NP Confidence/NN)\n      (PP in/IN)\n      (NP the/DT pound/NN)\n      (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n      (NP another/DT sharp/JJ dive/NN)\n      if/IN\n      ...)\n    (S\n      Chancellor/NNP\n      (PP of/IN)\n      (NP the/DT Exchequer/NNP)\n      ...)\n    >>> print(conll2002.sents())\n    [['Sao', 'Paulo', '(', 'Brasil', ')', ',', ...], ['-'], ...]\n    >>> for tree in conll2002.chunked_sents()[:2]:\n    ...     print(tree)\n    (S\n      (LOC Sao/NC Paulo/VMI)\n      (/Fpa\n      (LOC Brasil/NC)\n      )/Fpt\n      ...)\n    (S -/Fg)\n\n.. note:: Since the CONLL corpora do not contain paragraph break\n   information, these readers do not support the ``para()`` method.)\n\n.. warning:: if you call the conll corpora reader methods without any\n   arguments, they will return the contents of the entire corpus,\n   *including* the 'test' portions of the corpus.)\n\nSemCor is a subset of the Brown corpus tagged with WordNet senses and\nnamed entities. Both kinds of lexical items include multiword units,\nwhich are encoded as chunks (senses and part-of-speech tags pertain\nto the entire chunk).\n\n    >>> from nltk.corpus import semcor\n    >>> semcor.words()\n    ['The', 'Fulton', 'County', 'Grand', 'Jury', ...]\n    >>> semcor.chunks()\n    [['The'], ['Fulton', 'County', 'Grand', 'Jury'], ...]\n    >>> semcor.sents()\n    [['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...],\n    ['The', 'jury', 'further', 'said', ...], ...]\n    >>> semcor.chunk_sents()\n    [[['The'], ['Fulton', 'County', 'Grand', 'Jury'], ['said'], ...\n    ['.']], [['The'], ['jury'], ['further'], ['said'], ... ['.']], ...]\n    >>> list(map(str, semcor.tagged_chunks(tag='both')[:3]))\n    ['(DT The)', \"(Lemma('group.n.01.group') (NE (NNP Fulton County Grand Jury)))\", \"(Lemma('state.v.01.say') (VB said))\"]\n    >>> [[str(c) for c in s] for s in semcor.tagged_sents(tag='both')[:2]]\n    [['(DT The)', \"(Lemma('group.n.01.group') (NE (NNP Fulton County Grand Jury)))\", ...\n     '(None .)'], ['(DT The)', ... '(None .)']]\n\n\nThe IEER corpus is another chunked corpus.  This corpus is unusual in\nthat each corpus item contains multiple documents.  (This reflects the\nfact that each corpus file contains multiple documents.)  The IEER\ncorpus defines the `parsed_docs` method, which returns the documents\nin a given item as `IEERDocument` objects:\n\n    >>> from nltk.corpus import ieer\n    >>> ieer.fileids()\n    ['APW_19980314', 'APW_19980424', 'APW_19980429',\n     'NYT_19980315', 'NYT_19980403', 'NYT_19980407']\n    >>> docs = ieer.parsed_docs('APW_19980314')\n    >>> print(docs[0])\n    <IEERDocument APW19980314.0391: 'Kenyans protest tax hikes'>\n    >>> print(docs[0].docno)\n    APW19980314.0391\n    >>> print(docs[0].doctype)\n    NEWS STORY\n    >>> print(docs[0].date_time)\n    03/14/1998 10:36:00\n    >>> print(docs[0].headline)\n    (DOCUMENT Kenyans protest tax hikes)\n    >>> print(docs[0].text)\n    (DOCUMENT\n      (LOCATION NAIROBI)\n      ,\n      (LOCATION Kenya)\n      (\n      (ORGANIZATION AP)\n      )\n      _\n      (CARDINAL Thousands)\n      of\n      laborers,\n      ...\n      on\n      (DATE Saturday)\n      ...)\n\nParsed Corpora\n==============\n\nThe Treebank corpora provide a syntactic parse for each sentence.  The\nNLTK data package includes a 10% sample of the Penn Treebank (in\n``treebank``), as well as the Sinica Treebank (in ``sinica_treebank``).\n\nReading the Penn Treebank (Wall Street Journal sample):\n\n    >>> from nltk.corpus import treebank\n    >>> print(treebank.fileids())\n    ['wsj_0001.mrg', 'wsj_0002.mrg', 'wsj_0003.mrg', 'wsj_0004.mrg', ...]\n    >>> print(treebank.words('wsj_0003.mrg'))\n    ['A', 'form', 'of', 'asbestos', 'once', 'used', ...]\n    >>> print(treebank.tagged_words('wsj_0003.mrg'))\n    [('A', 'DT'), ('form', 'NN'), ('of', 'IN'), ...]\n    >>> print(treebank.parsed_sents('wsj_0003.mrg')[0])\n    (S\n      (S-TPC-1\n        (NP-SBJ\n          (NP (NP (DT A) (NN form)) (PP (IN of) (NP (NN asbestos))))\n          (RRC ...)...)...)\n      ...\n      (VP (VBD reported) (SBAR (-NONE- 0) (S (-NONE- *T*-1))))\n      (. .))\n\nIf you have access to a full installation of the Penn Treebank, NLTK\ncan be configured to load it as well. Download the ``ptb`` package,\nand in the directory ``nltk_data/corpora/ptb`` place the ``BROWN``\nand ``WSJ`` directories of the Treebank installation (symlinks work\nas well). Then use the ``ptb`` module instead of ``treebank``:\n\n   >>> from nltk.corpus import ptb\n   >>> print(ptb.fileids()) # doctest: +SKIP\n   ['BROWN/CF/CF01.MRG', 'BROWN/CF/CF02.MRG', 'BROWN/CF/CF03.MRG', 'BROWN/CF/CF04.MRG', ...]\n   >>> print(ptb.words('WSJ/00/WSJ_0003.MRG')) # doctest: +SKIP\n   ['A', 'form', 'of', 'asbestos', 'once', 'used', '*', ...]\n   >>> print(ptb.tagged_words('WSJ/00/WSJ_0003.MRG')) # doctest: +SKIP\n   [('A', 'DT'), ('form', 'NN'), ('of', 'IN'), ...]\n\n...and so forth, like ``treebank`` but with extended fileids. Categories\nspecified in ``allcats.txt`` can be used to filter by genre; they consist\nof ``news`` (for WSJ articles) and names of the Brown subcategories\n(``fiction``, ``humor``, ``romance``, etc.):\n\n   >>> ptb.categories() # doctest: +SKIP\n   ['adventure', 'belles_lettres', 'fiction', 'humor', 'lore', 'mystery', 'news', 'romance', 'science_fiction']\n   >>> print(ptb.fileids('news')) # doctest: +SKIP\n   ['WSJ/00/WSJ_0001.MRG', 'WSJ/00/WSJ_0002.MRG', 'WSJ/00/WSJ_0003.MRG', ...]\n   >>> print(ptb.words(categories=['humor','fiction'])) # doctest: +SKIP\n   ['Thirty-three', 'Scotty', 'did', 'not', 'go', 'back', ...]\n\nAs PropBank and NomBank depend on the (WSJ portion of the) Penn Treebank,\nthe modules ``propbank_ptb`` and ``nombank_ptb`` are provided for access\nto a full PTB installation.\n\nReading the Sinica Treebank:\n\n    >>> from nltk.corpus import sinica_treebank\n    >>> print(sinica_treebank.sents()) # doctest: +SKIP\n    [['\\xe4\\xb8\\x80'], ['\\xe5\\x8f\\x8b\\xe6\\x83\\x85'], ...]\n    >>> sinica_treebank.parsed_sents()[25] # doctest: +SKIP\n    Tree('S',\n        [Tree('NP',\n            [Tree('Nba', ['\\xe5\\x98\\x89\\xe7\\x8f\\x8d'])]),\n         Tree('V\\xe2\\x80\\xa7\\xe5\\x9c\\xb0',\n            [Tree('VA11', ['\\xe4\\xb8\\x8d\\xe5\\x81\\x9c']),\n             Tree('DE', ['\\xe7\\x9a\\x84'])]),\n         Tree('VA4', ['\\xe5\\x93\\xad\\xe6\\xb3\\xa3'])])\n\nReading the CoNLL 2007 Dependency Treebanks:\n\n    >>> from nltk.corpus import conll2007\n    >>> conll2007.sents('esp.train')[0] # doctest: +SKIP\n    ['El', 'aumento', 'del', '\u00edndice', 'de', 'desempleo', ...]\n    >>> conll2007.parsed_sents('esp.train')[0] # doctest: +SKIP\n    <DependencyGraph with 38 nodes>\n    >>> print(conll2007.parsed_sents('esp.train')[0].tree()) # doctest: +SKIP\n    (fortaleci\u00f3\n      (aumento El (del (\u00edndice (de (desempleo estadounidense)))))\n      hoy\n      considerablemente\n      (al\n        (euro\n          (cotizaba\n            ,\n            que\n            (a (15.35 las GMT))\n            se\n            (en (mercado el (de divisas) (de Fr\u00e1ncfort)))\n            (a 0,9452_d\u00f3lares)\n            (frente_a , (0,9349_d\u00f3lares los (de (ma\u00f1ana esta)))))))\n      .)\n\nWord Lists and Lexicons\n=======================\n\nThe NLTK data package also includes a number of lexicons and word\nlists.  These are accessed just like text corpora.  The following\nexamples illustrate the use of the wordlist corpora:\n\n    >>> from nltk.corpus import names, stopwords, words\n    >>> words.fileids()\n    ['en', 'en-basic']\n    >>> words.words('en')\n    ['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', ...]\n\n    >>> stopwords.fileids()\n    ['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', ...]\n    >>> sorted(stopwords.words('portuguese'))\n    ['a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', ...]\n    >>> names.fileids()\n    ['female.txt', 'male.txt']\n    >>> names.words('male.txt')\n    ['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', ...]\n    >>> names.words('female.txt')\n    ['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', ...]\n\nThe CMU Pronunciation Dictionary corpus contains pronunciation\ntranscriptions for over 100,000 words.  It can be accessed as a list\nof entries (where each entry consists of a word, an identifier, and a\ntranscription) or as a dictionary from words to lists of\ntranscriptions.  Transcriptions are encoded as tuples of phoneme\nstrings.\n\n    >>> from nltk.corpus import cmudict\n    >>> print(cmudict.entries()[653:659])\n    [('acetate', ['AE1', 'S', 'AH0', 'T', 'EY2', 'T']),\n    ('acetic', ['AH0', 'S', 'EH1', 'T', 'IH0', 'K']),\n    ('acetic', ['AH0', 'S', 'IY1', 'T', 'IH0', 'K']),\n    ('aceto', ['AA0', 'S', 'EH1', 'T', 'OW0']),\n    ('acetochlor', ['AA0', 'S', 'EH1', 'T', 'OW0', 'K', 'L', 'AO2', 'R']),\n    ('acetone', ['AE1', 'S', 'AH0', 'T', 'OW2', 'N'])]\n    >>> # Load the entire cmudict corpus into a Python dictionary:\n    >>> transcr = cmudict.dict()\n    >>> print([transcr[w][0] for w in 'Natural Language Tool Kit'.lower().split()])\n    [['N', 'AE1', 'CH', 'ER0', 'AH0', 'L'],\n     ['L', 'AE1', 'NG', 'G', 'W', 'AH0', 'JH'],\n     ['T', 'UW1', 'L'],\n     ['K', 'IH1', 'T']]\n\n\nWordNet\n=======\n\nPlease see the separate WordNet howto.\n\nFrameNet\n========\n\nPlease see the separate FrameNet howto.\n\nPropBank\n========\n\nPlease see the separate PropBank howto.\n\nSentiWordNet\n============\n\nPlease see the separate SentiWordNet howto.\n\nCategorized Corpora\n===================\n\nSeveral corpora included with NLTK contain documents that have been categorized for\ntopic, genre, polarity, etc.  In addition to the standard corpus interface, these\ncorpora provide access to the list of categories and the mapping between the documents\nand their categories (in both directions).  Access the categories using the ``categories()``\nmethod, e.g.:\n\n    >>> from nltk.corpus import brown, movie_reviews, reuters\n    >>> brown.categories()\n    ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor',\n    'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n    >>> movie_reviews.categories()\n    ['neg', 'pos']\n    >>> reuters.categories()\n    ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',\n    'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',\n    'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]\n\nThis method has an optional argument that specifies a document or a list\nof documents, allowing us to map from (one or more) documents to (one or more) categories:\n\n    >>> brown.categories('ca01')\n    ['news']\n    >>> brown.categories(['ca01','cb01'])\n    ['editorial', 'news']\n    >>> reuters.categories('training/9865')\n    ['barley', 'corn', 'grain', 'wheat']\n    >>> reuters.categories(['training/9865', 'training/9880'])\n    ['barley', 'corn', 'grain', 'money-fx', 'wheat']\n\nWe can go back the other way using the optional argument of the ``fileids()`` method:\n\n    >>> reuters.fileids('barley')\n    ['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]\n\nBoth the ``categories()`` and ``fileids()`` methods return a sorted list containing\nno duplicates.\n\nIn addition to mapping between categories and documents, these corpora permit\ndirect access to their contents via the categories.  Instead of accessing a subset\nof a corpus by specifying one or more fileids, we can identify one or more categories, e.g.:\n\n    >>> brown.tagged_words(categories='news')\n    [('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n    >>> brown.sents(categories=['editorial','reviews'])\n    [['Assembly', 'session', 'brought', 'much', 'good'], ['The', 'General',\n    'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed',\n    'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from',\n    'the', 'day', 'it', 'convened', '.'], ...]\n\nNote that it is an error to specify both documents and categories.\n\nIn the context of a text categorization system, we can easily test if the\ncategory assigned to a document is correct as follows:\n\n    >>> def classify(doc): return 'news'   # Trivial classifier\n    >>> doc = 'ca01'\n    >>> classify(doc) in brown.categories(doc)\n    True\n\n\nOther Corpora\n=============\n\ncomparative_sentences\n---------------------\nA list of sentences from various sources, especially reviews and articles. Each\nline contains one sentence; sentences were separated by using a sentence tokenizer.\nComparative sentences have been annotated with their type, entities, features and\nkeywords.\n\n    >>> from nltk.corpus import comparative_sentences\n    >>> comparison = comparative_sentences.comparisons()[0]\n    >>> comparison.text\n    ['its', 'fast-forward', 'and', 'rewind', 'work', 'much', 'more', 'smoothly',\n    'and', 'consistently', 'than', 'those', 'of', 'other', 'models', 'i', \"'ve\",\n    'had', '.']\n    >>> comparison.entity_2\n    'models'\n    >>> (comparison.feature, comparison.keyword)\n    ('rewind', 'more')\n    >>> len(comparative_sentences.comparisons())\n    853\n\nopinion_lexicon\n---------------\nA list of positive and negative opinion words or sentiment words for English.\n\n    >>> from nltk.corpus import opinion_lexicon\n    >>> opinion_lexicon.words()[:4]\n        ['2-faced', '2-faces', 'abnormal', 'abolish']\n\nThe OpinionLexiconCorpusReader also provides shortcuts to retrieve positive/negative\nwords:\n\n    >>> opinion_lexicon.negative()[:4]\n    ['2-faced', '2-faces', 'abnormal', 'abolish']\n\nNote that words from `words()` method in opinion_lexicon are sorted by file id,\nnot alphabetically:\n\n    >>> opinion_lexicon.words()[0:10]\n    ['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably',\n    'abominate', 'abomination', 'abort', 'aborted']\n    >>> sorted(opinion_lexicon.words())[0:10]\n    ['2-faced', '2-faces', 'a+', 'abnormal', 'abolish', 'abominable', 'abominably',\n    'abominate', 'abomination', 'abort']\n\nppattach\n--------\nThe Prepositional Phrase Attachment corpus is a corpus of\nprepositional phrase attachment decisions.  Each instance in the\ncorpus is encoded as a ``PPAttachment`` object:\n\n    >>> from nltk.corpus import ppattach\n    >>> ppattach.attachments('training')\n    [PPAttachment(sent='0', verb='join', noun1='board',\n                  prep='as', noun2='director', attachment='V'),\n     PPAttachment(sent='1', verb='is', noun1='chairman',\n                  prep='of', noun2='N.V.', attachment='N'),\n     ...]\n    >>> inst = ppattach.attachments('training')[0]\n    >>> (inst.sent, inst.verb, inst.noun1, inst.prep, inst.noun2)\n    ('0', 'join', 'board', 'as', 'director')\n    >>> inst.attachment\n    'V'\n\nproduct_reviews_1 and product_reviews_2\n---------------------------------------\nThese two datasets respectively contain annotated customer reviews of 5 and 9\nproducts from amazon.com.\n\n    >>> from nltk.corpus import product_reviews_1\n    >>> camera_reviews = product_reviews_1.reviews('Canon_G3.txt')\n    >>> review = camera_reviews[0]\n    >>> review.sents()[0]\n    ['i', 'recently', 'purchased', 'the', 'canon', 'powershot', 'g3', 'and', 'am',\n    'extremely', 'satisfied', 'with', 'the', 'purchase', '.']\n    >>> review.features()\n    [('canon powershot g3', '+3'), ('use', '+2'), ('picture', '+2'),\n    ('picture quality', '+1'), ('picture quality', '+1'), ('camera', '+2'),\n    ('use', '+2'), ('feature', '+1'), ('picture quality', '+3'), ('use', '+1'),\n    ('option', '+1')]\n\nIt is also possible to reach the same information directly from the stream:\n\n    >>> product_reviews_1.features('Canon_G3.txt')\n    [('canon powershot g3', '+3'), ('use', '+2'), ...]\n\nWe can compute stats for specific product features:\n\n    >>> n_reviews = len([(feat,score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])\n    >>> tot = sum([int(score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])\n    >>> mean = tot / n_reviews\n    >>> print(n_reviews, tot, mean)\n    15 24 1.6\n\npros_cons\n---------\nA list of pros/cons sentences for determining context (aspect) dependent\nsentiment words, which are then applied to sentiment analysis of comparative\nsentences.\n\n    >>> from nltk.corpus import pros_cons\n    >>> pros_cons.sents(categories='Cons')\n    [['East', 'batteries', '!', 'On', '-', 'off', 'switch', 'too', 'easy',\n    'to', 'maneuver', '.'], ['Eats', '...', 'no', ',', 'GULPS', 'batteries'],\n    ...]\n    >>> pros_cons.words('IntegratedPros.txt')\n    ['Easy', 'to', 'use', ',', 'economical', '!', ...]\n\nsemcor\n------\nThe Brown Corpus, annotated with WordNet senses.\n\n    >>> from nltk.corpus import semcor\n    >>> semcor.words('brown2/tagfiles/br-n12.xml')\n    ['When', 'several', 'minutes', 'had', 'passed', ...]\n\nsenseval\n--------\nThe Senseval 2 corpus is a word sense disambiguation corpus.  Each\nitem in the corpus corresponds to a single ambiguous word.  For each\nof these words, the corpus contains a list of instances, corresponding\nto occurrences of that word.  Each instance provides the word; a list\nof word senses that apply to the word occurrence; and the word's\ncontext.\n\n    >>> from nltk.corpus import senseval\n    >>> senseval.fileids()\n    ['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']\n    >>> senseval.instances('hard.pos')\n    ...\n    [SensevalInstance(word='hard-a',\n        position=20,\n        context=[('``', '``'), ('he', 'PRP'), ...('hard', 'JJ'), ...],\n        senses=('HARD1',)),\n     SensevalInstance(word='hard-a',\n        position=10,\n        context=[('clever', 'NNP'), ...('hard', 'JJ'), ('time', 'NN'), ...],\n        senses=('HARD1',)), ...]\n\nThe following code looks at instances of the word 'interest', and\ndisplays their local context (2 words on each side) and word sense(s):\n\n    >>> for inst in senseval.instances('interest.pos')[:10]:\n    ...     p = inst.position\n    ...     left = ' '.join(w for (w,t) in inst.context[p-2:p])\n    ...     word = ' '.join(w for (w,t) in inst.context[p:p+1])\n    ...     right = ' '.join(w for (w,t) in inst.context[p+1:p+3])\n    ...     senses = ' '.join(inst.senses)\n    ...     print('%20s |%10s | %-15s -> %s' % (left, word, right, senses))\n             declines in |  interest | rates .         -> interest_6\n      indicate declining |  interest | rates because   -> interest_6\n           in short-term |  interest | rates .         -> interest_6\n                     4 % |  interest | in this         -> interest_5\n            company with | interests | in the          -> interest_5\n                  , plus |  interest | .               -> interest_6\n                 set the |  interest | rate on         -> interest_6\n                  's own |  interest | , prompted      -> interest_4\n           principal and |  interest | is the          -> interest_6\n            increase its |  interest | to 70           -> interest_5\n\nsentence_polarity\n-----------------\nThe Sentence Polarity dataset contains 5331 positive and 5331 negative processed\nsentences.\n\n    >>> from nltk.corpus import sentence_polarity\n    >>> sentence_polarity.sents()\n    [['simplistic', ',', 'silly', 'and', 'tedious', '.'], [\"it's\", 'so', 'laddish',\n    'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find',\n    'it', 'funny', '.'], ...]\n    >>> sentence_polarity.categories()\n    ['neg', 'pos']\n    >>> sentence_polarity.sents()[1]\n    [\"it's\", 'so', 'laddish', 'and', 'juvenile', ',', 'only', 'teenage', 'boys',\n    'could', 'possibly', 'find', 'it', 'funny', '.']\n\nshakespeare\n-----------\nThe Shakespeare corpus contains a set of Shakespeare plays, formatted\nas XML files.  These corpora are returned as ElementTree objects:\n\n    >>> from nltk.corpus import shakespeare\n    >>> from xml.etree import ElementTree\n    >>> shakespeare.fileids()\n    ['a_and_c.xml', 'dream.xml', 'hamlet.xml', 'j_caesar.xml', ...]\n    >>> play = shakespeare.xml('dream.xml')\n    >>> print(play)\n    <Element 'PLAY' at ...>\n    >>> print('%s: %s' % (play[0].tag, play[0].text))\n    TITLE: A Midsummer Night's Dream\n    >>> personae = [persona.text for persona in\n    ...             play.findall('PERSONAE/PERSONA')]\n    >>> print(personae)\n    ['THESEUS, Duke of Athens.', 'EGEUS, father to Hermia.', ...]\n    >>> # Find and print speakers not listed as personae\n    >>> names = [persona.split(',')[0] for persona in personae]\n    >>> speakers = set(speaker.text for speaker in\n    ...                play.findall('*/*/*/SPEAKER'))\n    >>> print(sorted(speakers.difference(names)))\n    ['ALL', 'COBWEB', 'DEMETRIUS', 'Fairy', 'HERNIA', 'LYSANDER',\n     'Lion', 'MOTH', 'MUSTARDSEED', 'Moonshine', 'PEASEBLOSSOM',\n     'Prologue', 'Pyramus', 'Thisbe', 'Wall']\n\nsubjectivity\n-----------\nThe Subjectivity Dataset contains 5000 subjective and 5000 objective processed\nsentences.\n\n    >>> from nltk.corpus import subjectivity\n    >>> subjectivity.categories()\n    ['obj', 'subj']\n    >>> subjectivity.sents()[23]\n    ['television', 'made', 'him', 'famous', ',', 'but', 'his', 'biggest', 'hits',\n    'happened', 'off', 'screen', '.']\n    >>> subjectivity.words(categories='subj')\n    ['smart', 'and', 'alert', ',', 'thirteen', ...]\n\ntoolbox\n-------\nThe Toolbox corpus distributed with NLTK contains a sample lexicon and\nseveral sample texts from the Rotokas language.  The Toolbox corpus\nreader returns Toolbox files as XML ElementTree objects.  The\nfollowing example loads the Rotokas dictionary, and figures out the\ndistribution of part-of-speech tags for reduplicated words.\n\n.. doctest: +SKIP\n\n    >>> from nltk.corpus import toolbox\n    >>> from nltk.probability import FreqDist\n    >>> from xml.etree import ElementTree\n    >>> import re\n    >>> rotokas = toolbox.xml('rotokas.dic')\n    >>> redup_pos_freqdist = FreqDist()\n    >>> # Note: we skip over the first record, which is actually\n    >>> # the header.\n    >>> for record in rotokas[1:]:\n    ...     lexeme = record.find('lx').text\n    ...     if re.match(r'(.*)\\1$', lexeme):\n    ...         redup_pos_freqdist[record.find('ps').text] += 1\n    >>> for item, count in redup_pos_freqdist.most_common():\n    ...     print(item, count)\n    V 41\n    N 14\n    ??? 4\n\nThis example displays some records from a Rotokas text:\n\n.. doctest: +SKIP\n\n    >>> river = toolbox.xml('rotokas/river.txt', key='ref')\n    >>> for record in river.findall('record')[:3]:\n    ...     for piece in record:\n    ...         if len(piece.text) > 60:\n    ...             print('%-6s %s...' % (piece.tag, piece.text[:57]))\n    ...         else:\n    ...             print('%-6s %s' % (piece.tag, piece.text))\n    ref    Paragraph 1\n    t      ``Viapau oisio              ra   ovaupasi                ...\n    m      viapau   oisio              ra   ovau   -pa       -si    ...\n    g      NEG      this way/like this and  forget -PROG     -2/3.DL...\n    p      NEG      ???                CONJ V.I    -SUFF.V.3 -SUFF.V...\n    f      ``No ken lus tingting wanema samting papa i bin tok,'' Na...\n    fe     ``Don't forget what Dad said,'' yelled Naomi.\n    ref    2\n    t      Osa     Ira  ora  Reviti viapau uvupasiva.\n    m      osa     Ira  ora  Reviti viapau uvu        -pa       -si ...\n    g      as/like name and  name   NEG    hear/smell -PROG     -2/3...\n    p      CONJ    N.PN CONJ N.PN   NEG    V.T        -SUFF.V.3 -SUF...\n    f      Tasol Ila na David no bin harim toktok.\n    fe     But Ila and David took no notice.\n    ref    3\n    t      Ikaupaoro                     rokosiva                   ...\n    m      ikau      -pa       -oro      roko    -si       -va      ...\n    g      run/hurry -PROG     -SIM      go down -2/3.DL.M -RP      ...\n    p      V.T       -SUFF.V.3 -SUFF.V.4 ADV     -SUFF.V.4 -SUFF.VT....\n    f      Tupela i bin hariap i go long wara .\n    fe     They raced to the river.\n\ntimit\n-----\nThe NLTK data package includes a fragment of the TIMIT\nAcoustic-Phonetic Continuous Speech Corpus.  This corpus is broken\ndown into small speech samples, each of which is available as a wave\nfile, a phonetic transcription, and a tokenized word list.\n\n    >>> from nltk.corpus import timit\n    >>> print(timit.utteranceids())\n    ['dr1-fvmh0/sa1', 'dr1-fvmh0/sa2', 'dr1-fvmh0/si1466',\n    'dr1-fvmh0/si2096', 'dr1-fvmh0/si836', 'dr1-fvmh0/sx116',\n    'dr1-fvmh0/sx206', 'dr1-fvmh0/sx26', 'dr1-fvmh0/sx296', ...]\n\n    >>> item = timit.utteranceids()[5]\n    >>> print(timit.phones(item))\n    ['h#', 'k', 'l', 'ae', 's', 'pcl', 'p', 'dh', 'ax',\n     's', 'kcl', 'k', 'r', 'ux', 'ix', 'nx', 'y', 'ax',\n     'l', 'eh', 'f', 'tcl', 't', 'hh', 'ae', 'n', 'dcl',\n     'd', 'h#']\n    >>> print(timit.words(item))\n    ['clasp', 'the', 'screw', 'in', 'your', 'left', 'hand']\n    >>> timit.play(item) # doctest: +SKIP\n\nThe corpus reader can combine the word segmentation information with\nthe phonemes to produce a single tree structure:\n\n    >>> for tree in timit.phone_trees(item):\n    ...     print(tree)\n    (S\n      h#\n      (clasp k l ae s pcl p)\n      (the dh ax)\n      (screw s kcl k r ux)\n      (in ix nx)\n      (your y ax)\n      (left l eh f tcl t)\n      (hand hh ae n dcl d)\n      h#)\n\nThe start time and stop time of each phoneme, word, and sentence are\nalso available:\n\n    >>> print(timit.phone_times(item))\n    [('h#', 0, 2190), ('k', 2190, 3430), ('l', 3430, 4326), ...]\n    >>> print(timit.word_times(item))\n    [('clasp', 2190, 8804), ('the', 8804, 9734), ...]\n    >>> print(timit.sent_times(item))\n    [('Clasp the screw in your left hand.', 0, 32154)]\n\nWe can use these times to play selected pieces of a speech sample:\n\n    >>> timit.play(item, 2190, 8804) # 'clasp'  # doctest: +SKIP\n\nThe corpus reader can also be queried for information about the\nspeaker and sentence identifier for a given speech sample:\n\n    >>> print(timit.spkrid(item))\n    dr1-fvmh0\n    >>> print(timit.sentid(item))\n    sx116\n    >>> print(timit.spkrinfo(timit.spkrid(item)))\n    SpeakerInfo(id='VMH0',\n                sex='F',\n                dr='1',\n                use='TRN',\n                recdate='03/11/86',\n                birthdate='01/08/60',\n                ht='5\\'05\"',\n                race='WHT',\n                edu='BS',\n                comments='BEST NEW ENGLAND ACCENT SO FAR')\n\n    >>> # List the speech samples from the same speaker:\n    >>> timit.utteranceids(spkrid=timit.spkrid(item))\n    ['dr1-fvmh0/sa1', 'dr1-fvmh0/sa2', 'dr1-fvmh0/si1466', ...]\n\ntwitter_samples\n---------------\n\nTwitter is well-known microblog service that allows public data to be\ncollected via APIs. NLTK's twitter corpus currently contains a sample of 20k Tweets\nretrieved from the Twitter Streaming API.\n\n    >>> from nltk.corpus import twitter_samples\n    >>> twitter_samples.fileids()\n    ['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n\nWe follow standard practice in storing full Tweets as line-separated\nJSON. These data structures can be accessed via `tweets.docs()`. However, in general it\nis more practical to focus just on the text field of the Tweets, which\nare accessed via the `strings()` method.\n\n    >>> twitter_samples.strings('tweets.20150430-223406.json')[:5]\n    ['RT @KirkKus: Indirect cost of the UK being in the EU is estimated to be costing Britain \\xa3170 billion per year! #BetterOffOut #UKIP', ...]\n\nThe default tokenizer for Tweets is specialised for 'casual' text, and\nthe `tokenized()` method returns a list of lists of tokens.\n\n    >>> twitter_samples.tokenized('tweets.20150430-223406.json')[:5]\n    [['RT', '@KirkKus', ':', 'Indirect', 'cost', 'of', 'the', 'UK', 'being', 'in', ...],\n     ['VIDEO', ':', 'Sturgeon', 'on', 'post-election', 'deals', 'http://t.co/BTJwrpbmOY'], ...]\n\nrte\n---\nThe RTE (Recognizing Textual Entailment) corpus was derived from the\nRTE1, RTE2 and RTE3 datasets (dev and test data), and consists of a\nlist of XML-formatted 'text'/'hypothesis' pairs.\n\n    >>> from nltk.corpus import rte\n    >>> print(rte.fileids())\n    ['rte1_dev.xml', 'rte1_test.xml', 'rte2_dev.xml', ..., 'rte3_test.xml']\n    >>> rtepairs = rte.pairs(['rte2_test.xml', 'rte3_test.xml'])\n    >>> print(rtepairs)\n    [<RTEPair: gid=2-8>, <RTEPair: gid=2-9>, <RTEPair: gid=2-15>, ...]\n\nIn the gold standard test sets, each pair is labeled according to\nwhether or not the text 'entails' the hypothesis; the\nentailment value is mapped to an integer 1 (True) or 0 (False).\n\n    >>> rtepairs[5]\n    <RTEPair: gid=2-23>\n    >>> rtepairs[5].text\n    'His wife Strida won a seat in parliament after forging an alliance\n    with the main anti-Syrian coalition in the recent election.'\n    >>> rtepairs[5].hyp\n    'Strida elected to parliament.'\n    >>> rtepairs[5].value\n    1\n\nThe RTE corpus also supports an ``xml()`` method which produces ElementTrees.\n\n    >>> xmltree = rte.xml('rte3_dev.xml')\n    >>> xmltree # doctest: +SKIP\n    <Element entailment-corpus at ...>\n    >>> xmltree[7].findtext('t')\n    \"Mrs. Bush's approval ratings have remained very high, above 80%,\n    even as her husband's have recently dropped below 50%.\"\n\nverbnet\n-------\nThe VerbNet corpus is a lexicon that divides verbs into classes, based\non their syntax-semantics linking behavior.  The basic elements in the\nlexicon are verb lemmas, such as 'abandon' and 'accept', and verb\nclasses, which have identifiers such as 'remove-10.1' and\n'admire-31.2-1'.  These class identifiers consist of a representative\nverb selected from the class, followed by a numerical identifier.  The\nlist of verb lemmas, and the list of class identifiers, can be\nretrieved with the following methods:\n\n    >>> from nltk.corpus import verbnet\n    >>> verbnet.lemmas()[20:25]\n    ['accelerate', 'accept', 'acclaim', 'accompany', 'accrue']\n    >>> verbnet.classids()[:5]\n    ['accompany-51.7', 'admire-31.2', 'admire-31.2-1', 'admit-65', 'adopt-93']\n\nThe `classids()` method may also be used to retrieve the classes that\na given lemma belongs to:\n\n    >>> verbnet.classids('accept')\n    ['approve-77', 'characterize-29.2-1-1', 'obtain-13.5.2']\n\nThe `classids()` method may additionally be used to retrieve all classes\nwithin verbnet if nothing is passed:\n\n    >>> verbnet.classids()\n    ['accompany-51.7', 'admire-31.2', 'admire-31.2-1', 'admit-65', 'adopt-93', 'advise-37.9', 'advise-37.9-1', 'allow-64', 'amalgamate-22.2', 'amalgamate-22.2-1', 'amalgamate-22.2-1-1', 'amalgamate-22.2-2', 'amalgamate-22.2-2-1', 'amalgamate-22.2-3', 'amalgamate-22.2-3-1', 'amalgamate-22.2-3-1-1', 'amalgamate-22.2-3-2', 'amuse-31.1', 'animal_sounds-38', 'appeal-31.4', 'appeal-31.4-1', 'appeal-31.4-2', 'appeal-31.4-3', 'appear-48.1.1', 'appoint-29.1', 'approve-77', 'assessment-34', 'assuming_position-50', 'avoid-52', 'banish-10.2', 'battle-36.4', 'battle-36.4-1', 'begin-55.1', 'begin-55.1-1', 'being_dressed-41.3.3', 'bend-45.2', 'berry-13.7', 'bill-54.5', 'body_internal_motion-49', 'body_internal_states-40.6', 'braid-41.2.2', 'break-45.1', 'breathe-40.1.2', 'breathe-40.1.2-1', 'bring-11.3', 'bring-11.3-1', 'build-26.1', 'build-26.1-1', 'bulge-47.5.3', 'bump-18.4', 'bump-18.4-1', 'butter-9.9', 'calibratable_cos-45.6', 'calibratable_cos-45.6-1', 'calve-28', 'captain-29.8', 'captain-29.8-1', 'captain-29.8-1-1', 'care-88', 'care-88-1', 'carry-11.4', 'carry-11.4-1', 'carry-11.4-1-1', 'carve-21.2', 'carve-21.2-1', 'carve-21.2-2', 'change_bodily_state-40.8.4', 'characterize-29.2', 'characterize-29.2-1', 'characterize-29.2-1-1', 'characterize-29.2-1-2', 'chase-51.6', 'cheat-10.6', 'cheat-10.6-1', 'cheat-10.6-1-1', 'chew-39.2', 'chew-39.2-1', 'chew-39.2-2', 'chit_chat-37.6', 'clear-10.3', 'clear-10.3-1', 'cling-22.5', 'coil-9.6', 'coil-9.6-1', 'coloring-24', 'complain-37.8', 'complete-55.2', 'concealment-16', 'concealment-16-1', 'confess-37.10', 'confine-92', 'confine-92-1', 'conjecture-29.5', 'conjecture-29.5-1', 'conjecture-29.5-2', 'consider-29.9', 'consider-29.9-1', 'consider-29.9-1-1', 'consider-29.9-1-1-1', 'consider-29.9-2', 'conspire-71', 'consume-66', 'consume-66-1', 'contiguous_location-47.8', 'contiguous_location-47.8-1', 'contiguous_location-47.8-2', 'continue-55.3', 'contribute-13.2', 'contribute-13.2-1', 'contribute-13.2-1-1', 'contribute-13.2-1-1-1', 'contribute-13.2-2', 'contribute-13.2-2-1', 'convert-26.6.2', 'convert-26.6.2-1', 'cooking-45.3', 'cooperate-73', 'cooperate-73-1', 'cooperate-73-2', 'cooperate-73-3', 'cope-83', 'cope-83-1', 'cope-83-1-1', 'correlate-86', 'correspond-36.1', 'correspond-36.1-1', 'correspond-36.1-1-1', 'cost-54.2', 'crane-40.3.2', 'create-26.4', 'create-26.4-1', 'curtsey-40.3.3', 'cut-21.1', 'cut-21.1-1', 'debone-10.8', 'declare-29.4', 'declare-29.4-1', 'declare-29.4-1-1', 'declare-29.4-1-1-1', 'declare-29.4-1-1-2', 'declare-29.4-1-1-3', 'declare-29.4-2', 'dedicate-79', 'defend-85', 'destroy-44', 'devour-39.4', 'devour-39.4-1', 'devour-39.4-2', 'differ-23.4', 'dine-39.5', 'disappearance-48.2', 'disassemble-23.3', 'discover-84', 'discover-84-1', 'discover-84-1-1', 'dress-41.1.1', 'dressing_well-41.3.2', 'drive-11.5', 'drive-11.5-1', 'dub-29.3', 'dub-29.3-1', 'eat-39.1', 'eat-39.1-1', 'eat-39.1-2', 'enforce-63', 'engender-27', 'entity_specific_cos-45.5', 'entity_specific_modes_being-47.2', 'equip-13.4.2', 'equip-13.4.2-1', 'equip-13.4.2-1-1', 'escape-51.1', 'escape-51.1-1', 'escape-51.1-2', 'escape-51.1-2-1', 'exceed-90', 'exchange-13.6', 'exchange-13.6-1', 'exchange-13.6-1-1', 'exhale-40.1.3', 'exhale-40.1.3-1', 'exhale-40.1.3-2', 'exist-47.1', 'exist-47.1-1', 'exist-47.1-1-1', 'feeding-39.7', 'ferret-35.6', 'fill-9.8', 'fill-9.8-1', 'fit-54.3', 'flinch-40.5', 'floss-41.2.1', 'focus-87', 'forbid-67', 'force-59', 'force-59-1', 'free-80', 'free-80-1', 'fulfilling-13.4.1', 'fulfilling-13.4.1-1', 'fulfilling-13.4.1-2', 'funnel-9.3', 'funnel-9.3-1', 'funnel-9.3-2', 'funnel-9.3-2-1', 'future_having-13.3', 'get-13.5.1', 'get-13.5.1-1', 'give-13.1', 'give-13.1-1', 'gobble-39.3', 'gobble-39.3-1', 'gobble-39.3-2', 'gorge-39.6', 'groom-41.1.2', 'grow-26.2', 'help-72', 'help-72-1', 'herd-47.5.2', 'hiccup-40.1.1', 'hit-18.1', 'hit-18.1-1', 'hold-15.1', 'hold-15.1-1', 'hunt-35.1', 'hurt-40.8.3', 'hurt-40.8.3-1', 'hurt-40.8.3-1-1', 'hurt-40.8.3-2', 'illustrate-25.3', 'image_impression-25.1', 'indicate-78', 'indicate-78-1', 'indicate-78-1-1', 'inquire-37.1.2', 'instr_communication-37.4', 'investigate-35.4', 'judgement-33', 'keep-15.2', 'knead-26.5', 'learn-14', 'learn-14-1', 'learn-14-2', 'learn-14-2-1', 'leave-51.2', 'leave-51.2-1', 'lecture-37.11', 'lecture-37.11-1', 'lecture-37.11-1-1', 'lecture-37.11-2', 'light_emission-43.1', 'limit-76', 'linger-53.1', 'linger-53.1-1', 'lodge-46', 'long-32.2', 'long-32.2-1', 'long-32.2-2', 'manner_speaking-37.3', 'marry-36.2', 'marvel-31.3', 'marvel-31.3-1', 'marvel-31.3-2', 'marvel-31.3-3', 'marvel-31.3-4', 'marvel-31.3-5', 'marvel-31.3-6', 'marvel-31.3-7', 'marvel-31.3-8', 'marvel-31.3-9', 'masquerade-29.6', 'masquerade-29.6-1', 'masquerade-29.6-2', 'matter-91', 'meander-47.7', 'meet-36.3', 'meet-36.3-1', 'meet-36.3-2', 'mine-10.9', 'mix-22.1', 'mix-22.1-1', 'mix-22.1-1-1', 'mix-22.1-2', 'mix-22.1-2-1', 'modes_of_being_with_motion-47.3', 'murder-42.1', 'murder-42.1-1', 'neglect-75', 'neglect-75-1', 'neglect-75-1-1', 'neglect-75-2', 'nonvehicle-51.4.2', 'nonverbal_expression-40.2', 'obtain-13.5.2', 'obtain-13.5.2-1', 'occurrence-48.3', 'order-60', 'order-60-1', 'orphan-29.7', 'other_cos-45.4', 'pain-40.8.1', 'pay-68', 'peer-30.3', 'pelt-17.2', 'performance-26.7', 'performance-26.7-1', 'performance-26.7-1-1', 'performance-26.7-2', 'performance-26.7-2-1', 'pit-10.7', 'pocket-9.10', 'pocket-9.10-1', 'poison-42.2', 'poke-19', 'pour-9.5', 'preparing-26.3', 'preparing-26.3-1', 'preparing-26.3-2', 'price-54.4', 'push-12', 'push-12-1', 'push-12-1-1', 'put-9.1', 'put-9.1-1', 'put-9.1-2', 'put_direction-9.4', 'put_spatial-9.2', 'put_spatial-9.2-1', 'reach-51.8', 'reflexive_appearance-48.1.2', 'refrain-69', 'register-54.1', 'rely-70', 'remove-10.1', 'risk-94', 'risk-94-1', 'roll-51.3.1', 'rummage-35.5', 'run-51.3.2', 'rush-53.2', 'say-37.7', 'say-37.7-1', 'say-37.7-1-1', 'say-37.7-2', 'scribble-25.2', 'search-35.2', 'see-30.1', 'see-30.1-1', 'see-30.1-1-1', 'send-11.1', 'send-11.1-1', 'separate-23.1', 'separate-23.1-1', 'separate-23.1-2', 'settle-89', 'shake-22.3', 'shake-22.3-1', 'shake-22.3-1-1', 'shake-22.3-2', 'shake-22.3-2-1', 'sight-30.2', 'simple_dressing-41.3.1', 'slide-11.2', 'slide-11.2-1-1', 'smell_emission-43.3', 'snooze-40.4', 'sound_emission-43.2', 'sound_existence-47.4', 'spank-18.3', 'spatial_configuration-47.6', 'split-23.2', 'spray-9.7', 'spray-9.7-1', 'spray-9.7-1-1', 'spray-9.7-2', 'stalk-35.3', 'steal-10.5', 'stimulus_subject-30.4', 'stop-55.4', 'stop-55.4-1', 'substance_emission-43.4', 'succeed-74', 'succeed-74-1', 'succeed-74-1-1', 'succeed-74-2', 'suffocate-40.7', 'suspect-81', 'swarm-47.5.1', 'swarm-47.5.1-1', 'swarm-47.5.1-2', 'swarm-47.5.1-2-1', 'swat-18.2', 'talk-37.5', 'tape-22.4', 'tape-22.4-1', 'tell-37.2', 'throw-17.1', 'throw-17.1-1', 'throw-17.1-1-1', 'tingle-40.8.2', 'touch-20', 'touch-20-1', 'transcribe-25.4', 'transfer_mesg-37.1.1', 'transfer_mesg-37.1.1-1', 'transfer_mesg-37.1.1-1-1', 'try-61', 'turn-26.6.1', 'turn-26.6.1-1', 'urge-58', 'vehicle-51.4.1', 'vehicle-51.4.1-1', 'waltz-51.5', 'want-32.1', 'want-32.1-1', 'want-32.1-1-1', 'weather-57', 'weekend-56', 'wink-40.3.1', 'wink-40.3.1-1', 'wipe_instr-10.4.2', 'wipe_instr-10.4.2-1', 'wipe_manner-10.4.1', 'wipe_manner-10.4.1-1', 'wish-62', 'withdraw-82', 'withdraw-82-1', 'withdraw-82-2', 'withdraw-82-3']\n\nThe primary object in the lexicon is a class record, which is stored\nas an ElementTree xml object.  The class record for a given class\nidentifier is returned by the `vnclass()` method:\n\n    >>> verbnet.vnclass('remove-10.1')\n    <Element 'VNCLASS' at ...>\n\nThe `vnclass()` method also accepts \"short\" identifiers, such as '10.1':\n\n    >>> verbnet.vnclass('10.1')\n    <Element 'VNCLASS' at ...>\n\nSee the Verbnet documentation, or the Verbnet files, for information\nabout the structure of this xml.  As an example, we can retrieve a\nlist of thematic roles for a given Verbnet class:\n\n    >>> vn_31_2 = verbnet.vnclass('admire-31.2')\n    >>> for themrole in vn_31_2.findall('THEMROLES/THEMROLE'):\n    ...     print(themrole.attrib['type'], end=' ')\n    ...     for selrestr in themrole.findall('SELRESTRS/SELRESTR'):\n    ...         print('[%(Value)s%(type)s]' % selrestr.attrib, end=' ')\n    ...     print()\n    Theme\n    Experiencer [+animate]\n    Predicate\n\nThe Verbnet corpus also provides a variety of pretty printing\nfunctions that can be used to display the xml contents in a more\nconcise form.  The simplest such method is `pprint()`:\n\n    >>> print(verbnet.pprint('57'))\n    weather-57\n      Subclasses: (none)\n      Members: blow clear drizzle fog freeze gust hail howl lightning mist\n        mizzle pelt pour precipitate rain roar shower sleet snow spit spot\n        sprinkle storm swelter teem thaw thunder\n      Thematic roles:\n        * Theme[+concrete +force]\n      Frames:\n        Intransitive (Expletive Subject)\n          Example: It's raining.\n          Syntax: LEX[it] LEX[[+be]] VERB\n          Semantics:\n            * weather(during(E), Weather_type, ?Theme)\n        NP (Expletive Subject, Theme Object)\n          Example: It's raining cats and dogs.\n          Syntax: LEX[it] LEX[[+be]] VERB NP[Theme]\n          Semantics:\n            * weather(during(E), Weather_type, Theme)\n        PP (Expletive Subject, Theme-PP)\n          Example: It was pelting with rain.\n          Syntax: LEX[it[+be]] VERB PREP[with] NP[Theme]\n          Semantics:\n            * weather(during(E), Weather_type, Theme)\n\nVerbnet gives us frames that link the syntax and semantics using an example.\nThese frames are part of the corpus and we can use `frames()` to get a frame\nfor a given verbnet class.\n\n    >>> frame = verbnet.frames('57')\n    >>> frame == [{'semantics': [{'arguments': [{'value': 'during(E)', 'type': 'Event'}, {'value': 'Weather_type', 'type': 'VerbSpecific'}, {'value': '?Theme', 'type': 'ThemRole'}], 'predicate_value': 'weather'}], 'example': \"It's raining.\", 'syntax': [{'pos_tag': 'LEX', 'modifiers': {'value': 'it', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'LEX', 'modifiers': {'value': '[+be]', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'VERB', 'modifiers': {'value': '', 'synrestrs': [], 'selrestrs': []}}], 'description': {'primary': 'Intransitive', 'secondary': 'Expletive Subject'}}, {'semantics': [{'arguments': [{'value': 'during(E)', 'type': 'Event'}, {'value': 'Weather_type', 'type': 'VerbSpecific'}, {'value': 'Theme', 'type': 'ThemRole'}], 'predicate_value': 'weather'}], 'example': \"It's raining cats and dogs.\", 'syntax': [{'pos_tag': 'LEX', 'modifiers': {'value': 'it', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'LEX', 'modifiers': {'value': '[+be]', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'VERB', 'modifiers': {'value': '', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'NP', 'modifiers': {'value': 'Theme', 'synrestrs': [], 'selrestrs': []}}], 'description': {'primary': 'NP', 'secondary': 'Expletive Subject, Theme Object'}}, {'semantics': [{'arguments': [{'value': 'during(E)', 'type': 'Event'}, {'value': 'Weather_type', 'type': 'VerbSpecific'}, {'value': 'Theme', 'type': 'ThemRole'}], 'predicate_value': 'weather'}], 'example': 'It was pelting with rain.', 'syntax': [{'pos_tag': 'LEX', 'modifiers': {'value': 'it[+be]', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'VERB', 'modifiers': {'value': '', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'PREP', 'modifiers': {'value': 'with', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'NP', 'modifiers': {'value': 'Theme', 'synrestrs': [], 'selrestrs': []}}], 'description': {'primary': 'PP', 'secondary': 'Expletive Subject, Theme-PP'}}]\n    True\n\nVerbnet corpus lets us access thematic roles individually using `themroles()`.\n\n    >>> themroles = verbnet.themroles('57')\n    >>> themroles == [{'modifiers': [{'type': 'concrete', 'value': '+'}, {'type': 'force', 'value': '+'}], 'type': 'Theme'}]\n    True\n\nVerbnet classes may also have subclasses sharing similar syntactic and semantic properties\nwhile having differences with the superclass. The Verbnet corpus allows us to access these\nsubclasses using `subclasses()`.\n\n    >>> print(verbnet.subclasses('9.1')) #Testing for 9.1 since '57' does not have subclasses\n    ['put-9.1-1', 'put-9.1-2']\n\n\nnps_chat\n--------\n\nThe NPS Chat Corpus, Release 1.0 consists of over 10,000 posts in age-specific\nchat rooms, which have been anonymized, POS-tagged and dialogue-act tagged.\n\n    >>> print(nltk.corpus.nps_chat.words())\n    ['now', 'im', 'left', 'with', 'this', 'gay', ...]\n    >>> print(nltk.corpus.nps_chat.tagged_words())\n    [('now', 'RB'), ('im', 'PRP'), ('left', 'VBD'), ...]\n    >>> print(nltk.corpus.nps_chat.tagged_posts())\n    [[('now', 'RB'), ('im', 'PRP'), ('left', 'VBD'), ('with', 'IN'),\n    ('this', 'DT'), ('gay', 'JJ'), ('name', 'NN')], [(':P', 'UH')], ...]\n\nWe can access the XML elements corresponding to individual posts.  These elements\nhave ``class`` and ``user`` attributes that we can access using ``p.attrib['class']``\nand ``p.attrib['user']``.  They also have text content, accessed using ``p.text``.\n\n    >>> print(nltk.corpus.nps_chat.xml_posts())\n    [<Element 'Post' at 0...>, <Element 'Post' at 0...>, ...]\n    >>> posts = nltk.corpus.nps_chat.xml_posts()\n    >>> sorted(nltk.FreqDist(p.attrib['class'] for p in posts).keys())\n    ['Accept', 'Bye', 'Clarify', 'Continuer', 'Emotion', 'Emphasis',\n    'Greet', 'Other', 'Reject', 'Statement', 'System', 'nAnswer',\n    'whQuestion', 'yAnswer', 'ynQuestion']\n    >>> posts[0].text\n    'now im left with this gay name'\n\nIn addition to the above methods for accessing tagged text, we can navigate\nthe XML structure directly, as follows:\n\n    >>> tokens = posts[0].findall('terminals/t')\n    >>> [t.attrib['pos'] + \"/\" + t.attrib['word'] for t in tokens]\n    ['RB/now', 'PRP/im', 'VBD/left', 'IN/with', 'DT/this', 'JJ/gay', 'NN/name']\n\nmultext_east\n------------\n\nThe Multext-East Corpus consists of POS-tagged versions of George Orwell's book\n1984 in 12 languages: English, Czech, Hungarian, Macedonian, Slovenian, Serbian,\nSlovak, Romanian, Estonian, Farsi, Bulgarian and Polish.\nThe corpus can be accessed using the usual methods for tagged corpora. The tagset\ncan be transformed from the Multext-East specific MSD tags to the Universal tagset\nusing the \"tagset\" parameter of all functions returning tagged parts of the corpus.\n\n    >>> print(nltk.corpus.multext_east.words(\"oana-en.xml\"))\n    ['It', 'was', 'a', 'bright', ...]\n    >>> print(nltk.corpus.multext_east.tagged_words(\"oana-en.xml\"))\n    [('It', '#Pp3ns'), ('was', '#Vmis3s'), ('a', '#Di'), ...]\n    >>> print(nltk.corpus.multext_east.tagged_sents(\"oana-en.xml\", \"universal\"))\n    [[('It', 'PRON'), ('was', 'VERB'), ('a', 'DET'), ...]\n\n\n\n---------------------\nCorpus Reader Classes\n---------------------\n\nNLTK's *corpus reader* classes are used to access the contents of a\ndiverse set of corpora.  Each corpus reader class is specialized to\nhandle a specific corpus format.  Examples include the\n`PlaintextCorpusReader`, which handles corpora that consist of a set\nof unannotated text files, and the `BracketParseCorpusReader`, which\nhandles corpora that consist of files containing\nparenthesis-delineated parse trees.\n\nAutomatically Created Corpus Reader Instances\n=============================================\n\nWhen the `nltk.corpus` module is imported, it automatically creates a\nset of corpus reader instances that can be used to access the corpora\nin the NLTK data distribution.  Here is a small sample of those\ncorpus reader instances:\n\n    >>> import nltk\n    >>> nltk.corpus.brown\n    <CategorizedTaggedCorpusReader ...>\n    >>> nltk.corpus.treebank\n    <BracketParseCorpusReader ...>\n    >>> nltk.corpus.names\n    <WordListCorpusReader ...>\n    >>> nltk.corpus.genesis\n    <PlaintextCorpusReader ...>\n    >>> nltk.corpus.inaugural\n    <PlaintextCorpusReader ...>\n\nThis sample illustrates that different corpus reader classes are used\nto read different corpora; but that the same corpus reader class may\nbe used for more than one corpus (e.g., ``genesis`` and ``inaugural``).\n\nCreating New Corpus Reader Instances\n====================================\n\nAlthough the `nltk.corpus` module automatically creates corpus reader\ninstances for the corpora in the NLTK data distribution, you may\nsometimes need to create your own corpus reader.  In particular, you\nwould need to create your own corpus reader if you want...\n\n- To access a corpus that is not included in the NLTK data\n  distribution.\n\n- To access a full copy of a corpus for which the NLTK data\n  distribution only provides a sample.\n\n- To access a corpus using a customized corpus reader (e.g., with\n  a customized tokenizer).\n\nTo create a new corpus reader, you will first need to look up the\nsignature for that corpus reader's constructor.  Different corpus\nreaders have different constructor signatures, but most of the\nconstructor signatures have the basic form::\n\n    SomeCorpusReader(root, files, ...options...)\n\nWhere ``root`` is an absolute path to the directory containing the\ncorpus data files; ``files`` is either a list of file names (relative\nto ``root``) or a regexp specifying which files should be included;\nand ``options`` are additional reader-specific options.  For example,\nwe can create a customized corpus reader for the genesis corpus that\nuses a different sentence tokenizer as follows:\n\n    >>> # Find the directory where the corpus lives.\n    >>> genesis_dir = nltk.data.find('corpora/genesis')\n    >>> # Create our custom sentence tokenizer.\n    >>> my_sent_tokenizer = nltk.RegexpTokenizer('[^.!?]+')\n    >>> # Create the new corpus reader object.\n    >>> my_genesis = nltk.corpus.PlaintextCorpusReader(\n    ...     genesis_dir, r'.*\\.txt', sent_tokenizer=my_sent_tokenizer)\n    >>> # Use the new corpus reader object.\n    >>> print(my_genesis.sents('english-kjv.txt')[0])\n    ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',\n     'and', 'the', 'earth']\n\nIf you wish to read your own plaintext corpus, which is stored in the\ndirectory '/usr/share/some-corpus', then you can create a corpus\nreader for it with::\n\n    >>> my_corpus = nltk.corpus.PlaintextCorpusReader(\n    ...     '/usr/share/some-corpus', r'.*\\.txt') # doctest: +SKIP\n\nFor a complete list of corpus reader subclasses, see the API\ndocumentation for `nltk.corpus.reader`.\n\nCorpus Types\n============\n\nCorpora vary widely in the types of content they include.  This is\nreflected in the fact that the base class `CorpusReader` only defines\na few general-purpose methods for listing and accessing the files that\nmake up a corpus.  It is up to the subclasses to define *data access\nmethods* that provide access to the information in the corpus.\nHowever, corpus reader subclasses should be consistent in their\ndefinitions of these data access methods wherever possible.\n\nAt a high level, corpora can be divided into three basic types:\n\n- A *token corpus* contains information about specific occurrences of\n  language use (or linguistic tokens), such as dialogues or written\n  texts.  Examples of token corpora are collections of written text\n  and collections of speech.\n\n- A *type corpus*, or *lexicon*, contains information about a coherent\n  set of lexical items (or linguistic types).  Examples of lexicons\n  are dictionaries and word lists.\n\n- A *language description corpus* contains information about a set of\n  non-lexical linguistic constructs, such as grammar rules.\n\nHowever, many individual corpora blur the distinctions between these\ntypes.  For example, corpora that are primarily lexicons may include\ntoken data in the form of example sentences; and corpora that are\nprimarily token corpora may be accompanied by one or more word lists\nor other lexical data sets.\n\nBecause corpora vary so widely in their information content, we have\ndecided that it would not be wise to use separate corpus reader base\nclasses for different corpus types.  Instead, we simply try to make\nthe corpus readers consistent wherever possible, but let them differ\nwhere the underlying data itself differs.\n\nCommon Corpus Reader Methods\n============================\n\nAs mentioned above, there are only a handful of methods that all\ncorpus readers are guaranteed to implement.  These methods provide\naccess to the files that contain the corpus data.  Every corpus is\nassumed to consist of one or more files, all located in a common root\ndirectory (or in subdirectories of that root directory).  The absolute\npath to the root directory is stored in the ``root`` property:\n\n    >>> import os\n    >>> str(nltk.corpus.genesis.root).replace(os.path.sep,'/')\n    '.../nltk_data/corpora/genesis'\n\nEach file within the corpus is identified by a platform-independent\nidentifier, which is basically a path string that uses ``/`` as the\npath separator.  I.e., this identifier can be converted to a relative\npath as follows:\n\n    >>> some_corpus_file_id = nltk.corpus.reuters.fileids()[0]\n    >>> import os.path\n    >>> os.path.normpath(some_corpus_file_id).replace(os.path.sep,'/')\n    'test/14826'\n\nTo get a list of all data files that make up a corpus, use the\n``fileids()`` method.  In some corpora, these files will not all contain\nthe same type of data; for example, for the ``nltk.corpus.timit``\ncorpus, ``fileids()`` will return a list including text files, word\nsegmentation files, phonetic transcription files, sound files, and\nmetadata files.  For corpora with diverse file types, the ``fileids()``\nmethod will often take one or more optional arguments, which can be\nused to get a list of the files with a specific file type:\n\n    >>> nltk.corpus.timit.fileids()\n    ['dr1-fvmh0/sa1.phn', 'dr1-fvmh0/sa1.txt', 'dr1-fvmh0/sa1.wav', ...]\n    >>> nltk.corpus.timit.fileids('phn')\n    ['dr1-fvmh0/sa1.phn', 'dr1-fvmh0/sa2.phn', 'dr1-fvmh0/si1466.phn', ...]\n\nIn some corpora, the files are divided into distinct categories.  For\nthese corpora, the ``fileids()`` method takes an optional argument,\nwhich can be used to get a list of the files within a specific category:\n\n    >>> nltk.corpus.brown.fileids('hobbies')\n    ['ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', ...]\n\nThe ``abspath()`` method can be used to find the absolute path to a\ncorpus file, given its file identifier:\n\n    >>> str(nltk.corpus.brown.abspath('ce06')).replace(os.path.sep,'/')\n    '.../corpora/brown/ce06'\n\nThe ``abspaths()`` method can be used to find the absolute paths for\none corpus file, a list of corpus files, or (if no fileids are specified),\nall corpus files.\n\nThis method is mainly useful as a helper method when defining corpus\ndata access methods, since data access methods can usually be called\nwith a string argument (to get a view for a specific file), with a\nlist argument (to get a view for a specific list of files), or with no\nargument (to get a view for the whole corpus).\n\nData Access Methods\n===================\n\nIndividual corpus reader subclasses typically extend this basic set of\nfile-access methods with one or more *data access methods*, which provide\neasy access to the data contained in the corpus.  The signatures for\ndata access methods often have the basic form::\n\n    corpus_reader.some_data access(fileids=None, ...options...)\n\nWhere ``fileids`` can be a single file identifier string (to get a view\nfor a specific file); a list of file identifier strings (to get a view\nfor a specific list of files); or None (to get a view for the entire\ncorpus).  Some of the common data access methods, and their return\ntypes, are:\n\n  - I{corpus}.words(): list of str\n  - I{corpus}.sents(): list of (list of str)\n  - I{corpus}.paras(): list of (list of (list of str))\n  - I{corpus}.tagged_words(): list of (str,str) tuple\n  - I{corpus}.tagged_sents(): list of (list of (str,str))\n  - I{corpus}.tagged_paras(): list of (list of (list of (str,str)))\n  - I{corpus}.chunked_sents(): list of (Tree w/ (str,str) leaves)\n  - I{corpus}.parsed_sents(): list of (Tree with str leaves)\n  - I{corpus}.parsed_paras(): list of (list of (Tree with str leaves))\n  - I{corpus}.xml(): A single xml ElementTree\n  - I{corpus}.raw(): str (unprocessed corpus contents)\n\nFor example, the `words()` method is supported by many different\ncorpora, and returns a flat list of word strings:\n\n    >>> nltk.corpus.brown.words()\n    ['The', 'Fulton', 'County', 'Grand', 'Jury', ...]\n    >>> nltk.corpus.treebank.words()\n    ['Pierre', 'Vinken', ',', '61', 'years', 'old', ...]\n    >>> nltk.corpus.conll2002.words()\n    ['Sao', 'Paulo', '(', 'Brasil', ')', ',', '23', ...]\n    >>> nltk.corpus.genesis.words()\n    ['In', 'the', 'beginning', 'God', 'created', ...]\n\nOn the other hand, the `tagged_words()` method is only supported by\ncorpora that include part-of-speech annotations:\n\n    >>> nltk.corpus.brown.tagged_words()\n    [('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n    >>> nltk.corpus.treebank.tagged_words()\n    [('Pierre', 'NNP'), ('Vinken', 'NNP'), ...]\n    >>> nltk.corpus.conll2002.tagged_words()\n    [('Sao', 'NC'), ('Paulo', 'VMI'), ('(', 'Fpa'), ...]\n    >>> nltk.corpus.genesis.tagged_words()\n    Traceback (most recent call last):\n      ...\n    AttributeError: 'PlaintextCorpusReader' object has no attribute 'tagged_words'\n\nAlthough most corpus readers use file identifiers to index their\ncontent, some corpora use different identifiers instead.  For example,\nthe data access methods for the ``timit`` corpus uses *utterance\nidentifiers* to select which corpus items should be returned:\n\n    >>> nltk.corpus.timit.utteranceids()\n    ['dr1-fvmh0/sa1', 'dr1-fvmh0/sa2', 'dr1-fvmh0/si1466', ...]\n    >>> nltk.corpus.timit.words('dr1-fvmh0/sa2')\n    [\"don't\", 'ask', 'me', 'to', 'carry', 'an', 'oily', 'rag', 'like', 'that']\n\nAttempting to call ``timit``\\ 's data access methods with a file\nidentifier will result in an exception:\n\n    >>> nltk.corpus.timit.fileids()\n    ['dr1-fvmh0/sa1.phn', 'dr1-fvmh0/sa1.txt', 'dr1-fvmh0/sa1.wav', ...]\n    >>> nltk.corpus.timit.words('dr1-fvmh0/sa1.txt') # doctest: +SKIP\n    Traceback (most recent call last):\n      ...\n    IOError: No such file or directory: '.../dr1-fvmh0/sa1.txt.wrd'\n\nAs another example, the ``propbank`` corpus defines the ``roleset()``\nmethod, which expects a roleset identifier, not a file identifier:\n\n    >>> roleset = nltk.corpus.propbank.roleset('eat.01')\n    >>> from xml.etree import ElementTree as ET\n    >>> print(ET.tostring(roleset).decode('utf8'))\n    <roleset id=\"eat.01\" name=\"consume\" vncls=\"39.1\">\n      <roles>\n        <role descr=\"consumer, eater\" n=\"0\">...</role>...\n      </roles>...\n    </roleset>...\n\nStream Backed Corpus Views\n==========================\nAn important feature of NLTK's corpus readers is that many of them\naccess the underlying data files using \"corpus views.\"  A *corpus\nview* is an object that acts like a simple data structure (such as a\nlist), but does not store the data elements in memory; instead, data\nelements are read from the underlying data files on an as-needed\nbasis.\n\nBy only loading items from the file on an as-needed basis, corpus\nviews maintain both memory efficiency and responsiveness.  The memory\nefficiency of corpus readers is important because some corpora contain\nvery large amounts of data, and storing the entire data set in memory\ncould overwhelm many machines.  The responsiveness is important when\nexperimenting with corpora in interactive sessions and in in-class\ndemonstrations.\n\nThe most common corpus view is the `StreamBackedCorpusView`, which\nacts as a read-only list of tokens.  Two additional corpus view\nclasses, `ConcatenatedCorpusView` and `LazySubsequence`, make it\npossible to create concatenations and take slices of\n`StreamBackedCorpusView` objects without actually storing the\nresulting list-like object's elements in memory.\n\nIn the future, we may add additional corpus views that act like other\nbasic data structures, such as dictionaries.\n\nWriting New Corpus Readers\n==========================\n\nIn order to add support for new corpus formats, it is necessary to\ndefine new corpus reader classes.  For many corpus formats, writing\nnew corpus readers is relatively straight-forward.  In this section,\nwe'll describe what's involved in creating a new corpus reader.  If\nyou do create a new corpus reader, we encourage you to contribute it\nback to the NLTK project.\n\nDon't Reinvent the Wheel\n------------------------\nBefore you start writing a new corpus reader, you should check to be\nsure that the desired format can't be read using an existing corpus\nreader with appropriate constructor arguments.  For example, although\nthe `TaggedCorpusReader` assumes that words and tags are separated by\n``/`` characters by default, an alternative tag-separation character\ncan be specified via the ``sep`` constructor argument.  You should\nalso check whether the new corpus format can be handled by subclassing\nan existing corpus reader, and tweaking a few methods or variables.\n\nDesign\n------\nIf you decide to write a new corpus reader from scratch, then you\nshould first decide which data access methods you want the reader to\nprovide, and what their signatures should be.  You should look at\nexisting corpus readers that process corpora with similar data\ncontents, and try to be consistent with those corpus readers whenever\npossible.\n\nYou should also consider what sets of identifiers are appropriate for\nthe corpus format.  Where it's practical, file identifiers should be\nused.  However, for some corpora, it may make sense to use additional\nsets of identifiers.  Each set of identifiers should have a distinct\nname (e.g., fileids, utteranceids, rolesets); and you should be consistent\nin using that name to refer to that identifier.  Do not use parameter\nnames like ``id``, which leave it unclear what type of identifier is\nrequired.\n\nOnce you've decided what data access methods and identifiers are\nappropriate for your corpus, you should decide if there are any\ncustomizable parameters that you'd like the corpus reader to handle.\nThese parameters make it possible to use a single corpus reader to\nhandle a wider variety of corpora.  The ``sep`` argument for\n`TaggedCorpusReader`, mentioned above, is an example of a customizable\ncorpus reader parameter.\n\nImplementation\n--------------\n\nConstructor\n~~~~~~~~~~~\nIf your corpus reader implements any customizable parameters, then\nyou'll need to override the constructor.  Typically, the new\nconstructor will first call its base class's constructor, and then\nstore the customizable parameters.  For example, the\n`ConllChunkCorpusReader`\\ 's constructor is defined as follows:\n\n    def __init__(self, root, fileids, chunk_types, encoding='utf8',\n                 tagset=None, separator=None):\n        ConllCorpusReader.__init__(\n                self, root, fileids, ('words', 'pos', 'chunk'),\n                chunk_types=chunk_types, encoding=encoding,\n                tagset=tagset, separator=separator)\n\nIf your corpus reader does not implement any customization parameters,\nthen you can often just inherit the base class's constructor.\n\nData Access Methods\n~~~~~~~~~~~~~~~~~~~\n\nThe most common type of data access method takes an argument\nidentifying which files to access, and returns a view covering those\nfiles.  This argument may be a single file identifier string (to get a\nview for a specific file); a list of file identifier strings (to get a\nview for a specific list of files); or None (to get a view for the\nentire corpus).  The method's implementation converts this argument to\na list of path names using the `abspaths()` method, which handles all\nthree value types (string, list, and None):\n\n    >>> print(str(nltk.corpus.brown.abspaths()).replace('\\\\\\\\','/'))\n    [FileSystemPathPointer('.../corpora/brown/ca01'),\n     FileSystemPathPointer('.../corpora/brown/ca02'), ...]\n    >>> print(str(nltk.corpus.brown.abspaths('ce06')).replace('\\\\\\\\','/'))\n    [FileSystemPathPointer('.../corpora/brown/ce06')]\n    >>> print(str(nltk.corpus.brown.abspaths(['ce06', 'ce07'])).replace('\\\\\\\\','/'))\n    [FileSystemPathPointer('.../corpora/brown/ce06'),\n     FileSystemPathPointer('.../corpora/brown/ce07')]\n\nAn example of this type of method is the `words()` method, defined by\nthe `PlaintextCorpusReader` as follows:\n\n    >>> def words(self, fileids=None):\n    ...     return concat([self.CorpusView(fileid, self._read_word_block)\n    ...                    for fileid in self.abspaths(fileids)])\n\nThis method first uses `abspaths()` to convert ``fileids`` to a list of\nabsolute paths.  It then creates a corpus view for each file, using\nthe `PlaintextCorpusReader._read_word_block()` method to read elements\nfrom the data file (see the discussion of corpus views below).\nFinally, it combines these corpus views using the\n`nltk.corpus.reader.util.concat()` function.\n\nWhen writing a corpus reader for a corpus that is never expected to be\nvery large, it can sometimes be appropriate to read the files\ndirectly, rather than using a corpus view.  For example, the\n`WordListCorpusView` class defines its `words()` method as follows:\n\n    >>> def words(self, fileids=None):\n    ...     return concat([[w for w in open(fileid).read().split('\\n') if w]\n    ...                    for fileid in self.abspaths(fileids)])\n\n(This is usually more appropriate for lexicons than for token corpora.)\n\nIf the type of data returned by a data access method is one for which\nNLTK has a conventional representation (e.g., words, tagged words, and\nparse trees), then you should use that representation.  Otherwise, you\nmay find it necessary to define your own representation.  For data\nstructures that are relatively corpus-specific, it's usually best to\ndefine new classes for these elements.  For example, the ``propbank``\ncorpus defines the `PropbankInstance` class to store the semantic role\nlabeling instances described by the corpus; and the ``ppattach``\ncorpus defines the `PPAttachment` class to store the prepositional\nattachment instances described by the corpus.\n\nCorpus Views\n~~~~~~~~~~~~\n.. (Much of the content for this section is taken from the\n   StreamBackedCorpusView docstring.)\n\nThe heart of a `StreamBackedCorpusView` is its *block reader*\nfunction, which reads zero or more tokens from a stream, and returns\nthem as a list.  A very simple example of a block reader is:\n\n    >>> def simple_block_reader(stream):\n    ...     return stream.readline().split()\n\nThis simple block reader reads a single line at a time, and returns a\nsingle token (consisting of a string) for each whitespace-separated\nsubstring on the line.  A `StreamBackedCorpusView` built from this\nblock reader will act like a read-only list of all the\nwhitespace-separated tokens in an underlying file.\n\nWhen deciding how to define the block reader for a given corpus,\ncareful consideration should be given to the size of blocks handled by\nthe block reader.  Smaller block sizes will increase the memory\nrequirements of the corpus view's internal data structures (by 2\nintegers per block).  On the other hand, larger block sizes may\ndecrease performance for random access to the corpus.  (But note that\nlarger block sizes will *not* decrease performance for iteration.)\n\nInternally, the `StreamBackedCorpusView` class maintains a partial\nmapping from token index to file position, with one entry per block.\nWhen a token with a given index *i* is requested, the corpus view\nconstructs it as follows:\n\n1. First, it searches the toknum/filepos mapping for the token index\n   closest to (but less than or equal to) *i*.\n\n2. Then, starting at the file position corresponding to that index, it\n   reads one block at a time using the block reader until it reaches\n   the requested token.\n\nThe toknum/filepos mapping is created lazily: it is initially empty,\nbut every time a new block is read, the block's initial token is added\nto the mapping.  (Thus, the toknum/filepos map has one entry per\nblock.)\n\nYou can create your own corpus view in one of two ways:\n\n1. Call the `StreamBackedCorpusView` constructor, and provide your\n   block reader function via the ``block_reader`` argument.\n\n2. Subclass `StreamBackedCorpusView`, and override the\n   `read_block()` method.\n\nThe first option is usually easier, but the second option can allow\nyou to write a single `read_block` method whose behavior can be\ncustomized by different parameters to the subclass's constructor.  For\nan example of this design pattern, see the `TaggedCorpusView` class,\nwhich is used by `TaggedCorpusView`.\n\n----------------\nRegression Tests\n----------------\n\nThe following helper functions are used to create and then delete\ntesting corpora that are stored in temporary directories.  These\ntesting corpora are used to make sure the readers work correctly.\n\n    >>> import tempfile, os.path, textwrap\n    >>> def make_testcorpus(ext='', **fileids):\n    ...     root = tempfile.mkdtemp()\n    ...     for fileid, contents in fileids.items():\n    ...         fileid += ext\n    ...         f = open(os.path.join(root, fileid), 'w')\n    ...         f.write(textwrap.dedent(contents))\n    ...         f.close()\n    ...     return root\n    >>> def del_testcorpus(root):\n    ...     for fileid in os.listdir(root):\n    ...         os.remove(os.path.join(root, fileid))\n    ...     os.rmdir(root)\n\nPlaintext Corpus Reader\n=======================\nThe plaintext corpus reader is used to access corpora that consist of\nunprocessed plaintext data.  It assumes that paragraph breaks are\nindicated by blank lines.  Sentences and words can be tokenized using\nthe default tokenizers, or by custom tokenizers specified as\nparameters to the constructor.\n\n    >>> root = make_testcorpus(ext='.txt',\n    ...     a=\"\"\"\\\n    ...     This is the first sentence.  Here is another\n    ...     sentence!  And here's a third sentence.\n    ...\n    ...     This is the second paragraph.  Tokenization is currently\n    ...     fairly simple, so the period in Mr. gets tokenized.\n    ...     \"\"\",\n    ...     b=\"\"\"This is the second file.\"\"\")\n\n    >>> from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\nThe list of documents can be specified explicitly, or implicitly (using a\nregexp).  The ``ext`` argument specifies a file extension.\n\n    >>> corpus = PlaintextCorpusReader(root, ['a.txt', 'b.txt'])\n    >>> corpus.fileids()\n    ['a.txt', 'b.txt']\n    >>> corpus = PlaintextCorpusReader(root, r'.*\\.txt')\n    >>> corpus.fileids()\n    ['a.txt', 'b.txt']\n\nThe directory containing the corpus is corpus.root:\n\n    >>> str(corpus.root) == str(root)\n    True\n\nWe can get a list of words, or the raw string:\n\n    >>> corpus.words()\n    ['This', 'is', 'the', 'first', 'sentence', '.', ...]\n    >>> corpus.raw()[:40]\n    'This is the first sentence.  Here is ano'\n\nCheck that reading individual documents works, and reading all documents at\nonce works:\n\n    >>> len(corpus.words()), [len(corpus.words(d)) for d in corpus.fileids()]\n    (46, [40, 6])\n    >>> corpus.words('a.txt')\n    ['This', 'is', 'the', 'first', 'sentence', '.', ...]\n    >>> corpus.words('b.txt')\n    ['This', 'is', 'the', 'second', 'file', '.']\n    >>> corpus.words()[:4], corpus.words()[-4:]\n    (['This', 'is', 'the', 'first'], ['the', 'second', 'file', '.'])\n\nWe're done with the test corpus:\n\n    >>> del_testcorpus(root)\n\nTest the plaintext corpora that come with nltk:\n\n    >>> from nltk.corpus import abc, genesis, inaugural\n    >>> from nltk.corpus import state_union, webtext\n    >>> for corpus in (abc, genesis, inaugural, state_union,\n    ...                webtext):\n    ...     print(str(corpus).replace('\\\\\\\\','/'))\n    ...     print('  ', repr(corpus.fileids())[:60])\n    ...     print('  ', repr(corpus.words()[:10])[:60])\n    <PlaintextCorpusReader in '.../nltk_data/corpora/ab...'>\n       ['rural.txt', 'science.txt']\n       ['PM', 'denies', 'knowledge', 'of', 'AWB', ...\n    <PlaintextCorpusReader in '.../nltk_data/corpora/genesi...'>\n       ['english-kjv.txt', 'english-web.txt', 'finnish.txt', ...\n       ['In', 'the', 'beginning', 'God', 'created', 'the', ...\n    <PlaintextCorpusReader in '.../nltk_data/corpora/inaugura...'>\n       ['1789-Washington.txt', '1793-Washington.txt', ...\n       ['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...\n    <PlaintextCorpusReader in '.../nltk_data/corpora/state_unio...'>\n       ['1945-Truman.txt', '1946-Truman.txt', ...\n       ['PRESIDENT', 'HARRY', 'S', '.', 'TRUMAN', \"'\", ...\n    <PlaintextCorpusReader in '.../nltk_data/corpora/webtex...'>\n       ['firefox.txt', 'grail.txt', 'overheard.txt', ...\n       ['Cookie', 'Manager', ':', '\"', 'Don', \"'\", 't', ...\n\n\nTagged Corpus Reader\n====================\nThe Tagged Corpus reader can give us words, sentences, and paragraphs,\neach tagged or untagged.  All of the read methods can take one item\n(in which case they return the contents of that file) or a list of\ndocuments (in which case they concatenate the contents of those files).\nBy default, they apply to all documents in the corpus.\n\n    >>> root = make_testcorpus(\n    ...     a=\"\"\"\\\n    ...     This/det is/verb the/det first/adj sentence/noun ./punc\n    ...     Here/det  is/verb  another/adj    sentence/noun ./punc\n    ...     Note/verb that/comp you/pron can/verb use/verb \\\n    ...           any/noun tag/noun set/noun\n    ...\n    ...     This/det is/verb the/det second/adj paragraph/noun ./punc\n    ...     word/n without/adj a/det tag/noun :/: hello ./punc\n    ...     \"\"\",\n    ...     b=\"\"\"\\\n    ...     This/det is/verb the/det second/adj file/noun ./punc\n    ...     \"\"\")\n\n    >>> from nltk.corpus.reader.tagged import TaggedCorpusReader\n    >>> corpus = TaggedCorpusReader(root, list('ab'))\n    >>> corpus.fileids()\n    ['a', 'b']\n    >>> str(corpus.root) == str(root)\n    True\n    >>> corpus.words()\n    ['This', 'is', 'the', 'first', 'sentence', '.', ...]\n    >>> corpus.sents()\n    [['This', 'is', 'the', 'first', ...], ['Here', 'is', 'another'...], ...]\n    >>> corpus.paras()\n    [[['This', ...], ['Here', ...], ...], [['This', ...], ...], ...]\n    >>> corpus.tagged_words()\n    [('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ...]\n    >>> corpus.tagged_sents()\n    [[('This', 'DET'), ('is', 'VERB'), ...], [('Here', 'DET'), ...], ...]\n    >>> corpus.tagged_paras()\n    [[[('This', 'DET'), ...], ...], [[('This', 'DET'), ...], ...], ...]\n    >>> corpus.raw()[:40]\n    'This/det is/verb the/det first/adj sente'\n    >>> len(corpus.words()), [len(corpus.words(d)) for d in corpus.fileids()]\n    (38, [32, 6])\n    >>> len(corpus.sents()), [len(corpus.sents(d)) for d in corpus.fileids()]\n    (6, [5, 1])\n    >>> len(corpus.paras()), [len(corpus.paras(d)) for d in corpus.fileids()]\n    (3, [2, 1])\n    >>> print(corpus.words('a'))\n    ['This', 'is', 'the', 'first', 'sentence', '.', ...]\n    >>> print(corpus.words('b'))\n    ['This', 'is', 'the', 'second', 'file', '.']\n    >>> del_testcorpus(root)\n\nThe Brown Corpus uses the tagged corpus reader:\n\n    >>> from nltk.corpus import brown\n    >>> brown.fileids()\n    ['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', ...]\n    >>> brown.categories()\n    ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor',\n    'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n    >>> print(repr(brown.root).replace('\\\\\\\\','/'))\n    FileSystemPathPointer('.../corpora/brown')\n    >>> brown.words()\n    ['The', 'Fulton', 'County', 'Grand', 'Jury', ...]\n    >>> brown.sents()\n    [['The', 'Fulton', 'County', 'Grand', ...], ...]\n    >>> brown.paras()\n    [[['The', 'Fulton', 'County', ...]], [['The', 'jury', ...]], ...]\n    >>> brown.tagged_words()\n    [('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n    >>> brown.tagged_sents()\n    [[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ...], ...]\n    >>> brown.tagged_paras()\n    [[[('The', 'AT'), ...]], [[('The', 'AT'), ...]], ...]\n\nVerbnet Corpus Reader\n=====================\n\nMake sure we're picking up the right number of elements:\n\n    >>> from nltk.corpus import verbnet\n    >>> len(verbnet.lemmas())\n    3621\n    >>> len(verbnet.wordnetids())\n    4953\n    >>> len(verbnet.classids())\n    429\n\nSelecting classids based on various selectors:\n\n    >>> verbnet.classids(lemma='take')\n    ['bring-11.3', 'characterize-29.2', 'convert-26.6.2', 'cost-54.2',\n    'fit-54.3', 'performance-26.7-2', 'steal-10.5']\n    >>> verbnet.classids(wordnetid='lead%2:38:01')\n    ['accompany-51.7']\n    >>> verbnet.classids(fileid='approve-77.xml')\n    ['approve-77']\n    >>> verbnet.classids(classid='admire-31.2') # subclasses\n    ['admire-31.2-1']\n\nvnclass() accepts filenames, long ids, and short ids:\n\n    >>> a = ElementTree.tostring(verbnet.vnclass('admire-31.2.xml'))\n    >>> b = ElementTree.tostring(verbnet.vnclass('admire-31.2'))\n    >>> c = ElementTree.tostring(verbnet.vnclass('31.2'))\n    >>> a == b == c\n    True\n\nfileids() can be used to get files based on verbnet class ids:\n\n    >>> verbnet.fileids('admire-31.2')\n    ['admire-31.2.xml']\n    >>> verbnet.fileids(['admire-31.2', 'obtain-13.5.2'])\n    ['admire-31.2.xml', 'obtain-13.5.2.xml']\n    >>> verbnet.fileids('badidentifier')\n    Traceback (most recent call last):\n      . . .\n    ValueError: vnclass identifier 'badidentifier' not found\n\nlongid() and shortid() can be used to convert identifiers:\n\n    >>> verbnet.longid('31.2')\n    'admire-31.2'\n    >>> verbnet.longid('admire-31.2')\n    'admire-31.2'\n    >>> verbnet.shortid('31.2')\n    '31.2'\n    >>> verbnet.shortid('admire-31.2')\n    '31.2'\n    >>> verbnet.longid('badidentifier')\n    Traceback (most recent call last):\n      . . .\n    ValueError: vnclass identifier 'badidentifier' not found\n    >>> verbnet.shortid('badidentifier')\n    Traceback (most recent call last):\n      . . .\n    ValueError: vnclass identifier 'badidentifier' not found\n\nCorpus View Regression Tests\n============================\n\nSelect some corpus files to play with:\n\n    >>> import nltk.data\n    >>> # A very short file (160 chars):\n    >>> f1 = nltk.data.find('corpora/inaugural/README')\n    >>> # A relatively short file (791 chars):\n    >>> f2 = nltk.data.find('corpora/inaugural/1793-Washington.txt')\n    >>> # A longer file (32k chars):\n    >>> f3 = nltk.data.find('corpora/inaugural/1909-Taft.txt')\n    >>> fileids = [f1, f2, f3]\n\n\nConcatenation\n-------------\nCheck that concatenation works as intended.\n\n    >>> from nltk.corpus.reader.util import *\n\n    >>> c1 = StreamBackedCorpusView(f1, read_whitespace_block, encoding='utf-8')\n    >>> c2 = StreamBackedCorpusView(f2, read_whitespace_block, encoding='utf-8')\n    >>> c3 = StreamBackedCorpusView(f3, read_whitespace_block, encoding='utf-8')\n    >>> c123 = c1+c2+c3\n    >>> print(c123)\n    ['C-Span', 'Inaugural', 'Address', 'Corpus', 'US', ...]\n\n    >>> l1 = f1.open(encoding='utf-8').read().split()\n    >>> l2 = f2.open(encoding='utf-8').read().split()\n    >>> l3 = f3.open(encoding='utf-8').read().split()\n    >>> l123 = l1+l2+l3\n\n    >>> list(c123) == l123\n    True\n\n    >>> (c1+c2+c3)[100] == l123[100]\n    True\n\nSlicing\n-------\nFirst, do some tests with fairly small slices.  These will all\ngenerate tuple values.\n\n    >>> from nltk.util import LazySubsequence\n    >>> c1 = StreamBackedCorpusView(f1, read_whitespace_block, encoding='utf-8')\n    >>> l1 = f1.open(encoding='utf-8').read().split()\n    >>> print(len(c1))\n    21\n    >>> len(c1) < LazySubsequence.MIN_SIZE\n    True\n\nChoose a list of indices, based on the length, that covers the\nimportant corner cases:\n\n    >>> indices = [-60, -30, -22, -21, -20, -1,\n    ...            0, 1, 10, 20, 21, 22, 30, 60]\n\nTest slicing with explicit start & stop value:\n\n    >>> for s in indices:\n    ...     for e in indices:\n    ...         assert list(c1[s:e]) == l1[s:e]\n\nTest slicing with stop=None:\n\n    >>> for s in indices:\n    ...     assert list(c1[s:]) == l1[s:]\n\nTest slicing with start=None:\n\n    >>> for e in indices:\n    ...     assert list(c1[:e]) == l1[:e]\n\nTest slicing with start=stop=None:\n\n    >>> list(c1[:]) == list(l1[:])\n    True\n\nNext, we'll do some tests with much longer slices.  These will\ngenerate LazySubsequence objects.\n\n    >>> c3 = StreamBackedCorpusView(f3, read_whitespace_block, encoding='utf-8')\n    >>> l3 = f3.open(encoding='utf-8').read().split()\n    >>> print(len(c3))\n    5430\n    >>> len(c3) > LazySubsequence.MIN_SIZE*2\n    True\n\nChoose a list of indices, based on the length, that covers the\nimportant corner cases:\n\n    >>> indices = [-12000, -6000, -5431, -5430, -5429, -3000, -200, -1,\n    ...            0, 1, 200, 3000, 5000, 5429, 5430, 5431, 6000, 12000]\n\nTest slicing with explicit start & stop value:\n\n    >>> for s in indices:\n    ...     for e in indices:\n    ...         assert list(c3[s:e]) == l3[s:e]\n\nTest slicing with stop=None:\n\n    >>> for s in indices:\n    ...     assert list(c3[s:]) == l3[s:]\n\nTest slicing with start=None:\n\n    >>> for e in indices:\n    ...     assert list(c3[:e]) == l3[:e]\n\nTest slicing with start=stop=None:\n\n    >>> list(c3[:]) == list(l3[:])\n    True\n\nMultiple Iterators\n------------------\nIf multiple iterators are created for the same corpus view, their\niteration can be interleaved:\n\n    >>> c3 = StreamBackedCorpusView(f3, read_whitespace_block)\n    >>> iterators = [c3.iterate_from(n) for n in [0,15,30,45]]\n    >>> for i in range(15):\n    ...     for iterator in iterators:\n    ...         print('%-15s' % next(iterator), end=' ')\n    ...     print()\n    My              a               duties          in\n    fellow          heavy           of              a\n    citizens:       weight          the             proper\n    Anyone          of              office          sense\n    who             responsibility. upon            of\n    has             If              which           the\n    taken           not,            he              obligation\n    the             he              is              which\n    oath            has             about           the\n    I               no              to              oath\n    have            conception      enter,          imposes.\n    just            of              or              The\n    taken           the             he              office\n    must            powers          is              of\n    feel            and             lacking         an\n\nSeekableUnicodeStreamReader\n===========================\n\nThe file-like objects provided by the ``codecs`` module unfortunately\nsuffer from a bug that prevents them from working correctly with\ncorpus view objects.  In particular, although the expose ``seek()``\nand ``tell()`` methods, those methods do not exhibit the expected\nbehavior, because they are not synchronized with the internal buffers\nthat are kept by the file-like objects.  For example, the ``tell()``\nmethod will return the file position at the end of the buffers (whose\ncontents have not yet been returned by the stream); and therefore this\nfile position can not be used to return to the 'current' location in\nthe stream (since ``seek()`` has no way to reconstruct the buffers).\n\nTo get around these problems, we define a new class,\n`SeekableUnicodeStreamReader`, to act as a file-like interface to\nfiles containing encoded unicode data.  This class is loosely based on\nthe ``codecs.StreamReader`` class.  To construct a new reader, we call\nthe constructor with an underlying stream and an encoding name:\n\n    >>> from io import StringIO, BytesIO\n    >>> from nltk.data import SeekableUnicodeStreamReader\n    >>> stream = BytesIO(b\"\"\"\\\n    ... This is a test file.\n    ... It is encoded in ascii.\n    ... \"\"\".decode('ascii').encode('ascii'))\n    >>> reader = SeekableUnicodeStreamReader(stream, 'ascii')\n\n`SeekableUnicodeStreamReader`\\ s support all of the normal operations\nsupplied by a read-only stream.  Note that all of the read operations\nreturn ``unicode`` objects (not ``str`` objects).\n\n    >>> reader.read()         # read the entire file.\n    'This is a test file.\\nIt is encoded in ascii.\\n'\n    >>> reader.seek(0)        # rewind to the start.\n    >>> reader.read(5)        # read at most 5 bytes.\n    'This '\n    >>> reader.readline()     # read to the end of the line.\n    'is a test file.\\n'\n    >>> reader.seek(0)        # rewind to the start.\n    >>> for line in reader:\n    ...     print(repr(line))      # iterate over lines\n    'This is a test file.\\n'\n    'It is encoded in ascii.\\n'\n    >>> reader.seek(0)        # rewind to the start.\n    >>> reader.readlines()    # read a list of line strings\n    ['This is a test file.\\n', 'It is encoded in ascii.\\n']\n    >>> reader.close()\n\nSize argument to ``read()``\n---------------------------\nThe ``size`` argument to ``read()`` specifies the maximum number of\n*bytes* to read, not the maximum number of *characters*.  Thus, for\nencodings that use multiple bytes per character, it may return fewer\ncharacters than the ``size`` argument:\n\n    >>> stream = BytesIO(b\"\"\"\\\n    ... This is a test file.\n    ... It is encoded in utf-16.\n    ... \"\"\".decode('ascii').encode('utf-16'))\n    >>> reader = SeekableUnicodeStreamReader(stream, 'utf-16')\n    >>> reader.read(10)\n    'This '\n\nIf a read block ends in the middle of the byte string encoding a\nsingle character, then that byte string is stored in an internal\nbuffer, and re-used on the next call to ``read()``.  However, if the\nsize argument is too small to read even a single character, even\nthough at least one character is available, then the ``read()`` method\nwill read additional bytes until it can return a single character.\nThis ensures that the ``read()`` method does not return an empty\nstring, which could be mistaken for indicating the end of the file.\n\n    >>> reader.seek(0)            # rewind to the start.\n    >>> reader.read(1)            # we actually need to read 4 bytes\n    'T'\n    >>> int(reader.tell())\n    4\n\nThe ``readline()`` method may read more than a single line of text, in\nwhich case it stores the text that it does not return in a buffer.  If\nthis buffer is not empty, then its contents will be included in the\nvalue returned by the next call to ``read()``, regardless of the\n``size`` argument, since they are available without reading any new\nbytes from the stream:\n\n    >>> reader.seek(0)            # rewind to the start.\n    >>> reader.readline()         # stores extra text in a buffer\n    'This is a test file.\\n'\n    >>> print(reader.linebuffer)   # examine the buffer contents\n    ['It is encoded i']\n    >>> reader.read(0)            # returns the contents of the buffer\n    'It is encoded i'\n    >>> print(reader.linebuffer)   # examine the buffer contents\n    None\n\nSeek and Tell\n-------------\nIn addition to these basic read operations,\n`SeekableUnicodeStreamReader` also supports the ``seek()`` and\n``tell()`` operations.  However, some care must still be taken when\nusing these operations.  In particular, the only file offsets that\nshould be passed to ``seek()`` are ``0`` and any offset that has been\nreturned by ``tell``.\n\n    >>> stream = BytesIO(b\"\"\"\\\n    ... This is a test file.\n    ... It is encoded in utf-16.\n    ... \"\"\".decode('ascii').encode('utf-16'))\n    >>> reader = SeekableUnicodeStreamReader(stream, 'utf-16')\n    >>> reader.read(20)\n    'This is a '\n    >>> pos = reader.tell(); print(pos)\n    22\n    >>> reader.read(20)\n    'test file.'\n    >>> reader.seek(pos)     # rewind to the position from tell.\n    >>> reader.read(20)\n    'test file.'\n\nThe ``seek()`` and ``tell()`` methods work property even when\n``readline()`` is used.\n\n    >>> stream = BytesIO(b\"\"\"\\\n    ... This is a test file.\n    ... It is encoded in utf-16.\n    ... \"\"\".decode('ascii').encode('utf-16'))\n    >>> reader = SeekableUnicodeStreamReader(stream, 'utf-16')\n    >>> reader.readline()\n    'This is a test file.\\n'\n    >>> pos = reader.tell(); print(pos)\n    44\n    >>> reader.readline()\n    'It is encoded in utf-16.\\n'\n    >>> reader.seek(pos)     # rewind to the position from tell.\n    >>> reader.readline()\n    'It is encoded in utf-16.\\n'\n\n\nSquashed Bugs\n=============\n\nsvn 5276 fixed a bug in the comment-stripping behavior of\nparse_sexpr_block.\n\n    >>> from io import StringIO\n    >>> from nltk.corpus.reader.util import read_sexpr_block\n    >>> f = StringIO(b\"\"\"\n    ... (a b c)\n    ... # This line is a comment.\n    ... (d e f\\ng h)\"\"\".decode('ascii'))\n    >>> print(read_sexpr_block(f, block_size=38, comment_char='#'))\n    ['(a b c)']\n    >>> print(read_sexpr_block(f, block_size=38, comment_char='#'))\n    ['(d e f\\ng h)']\n\nsvn 5277 fixed a bug in parse_sexpr_block, which would cause it to\nenter an infinite loop if a file ended mid-sexpr, or ended with a\ntoken that was not followed by whitespace.  A related bug caused\nan infinite loop if the corpus ended in an unmatched close paren --\nthis was fixed in svn 5279\n\n    >>> f = StringIO(b\"\"\"\n    ... This file ends mid-sexpr\n    ... (hello (world\"\"\".decode('ascii'))\n    >>> for i in range(3): print(read_sexpr_block(f))\n    ['This', 'file', 'ends', 'mid-sexpr']\n    ['(hello (world']\n    []\n\n    >>> f = StringIO(b\"This file has no trailing whitespace.\".decode('ascii'))\n    >>> for i in range(3): print(read_sexpr_block(f))\n    ['This', 'file', 'has', 'no', 'trailing']\n    ['whitespace.']\n    []\n\n    >>> # Bug fixed in 5279:\n    >>> f = StringIO(b\"a b c)\".decode('ascii'))\n    >>> for i in range(3): print(read_sexpr_block(f))\n    ['a', 'b']\n    ['c)']\n    []\n\n\nsvn 5624 & 5265 fixed a bug in ConcatenatedCorpusView, which caused it\nto return the wrong items when indexed starting at any index beyond\nthe first file.\n\n    >>> import nltk\n    >>> sents = nltk.corpus.brown.sents()\n    >>> print(sents[6000])\n    ['Cholesterol', 'and', 'thyroid']\n    >>> print(sents[6000])\n    ['Cholesterol', 'and', 'thyroid']\n\nsvn 5728 fixed a bug in Categorized*CorpusReader, which caused them\nto return words from *all* files when just one file was specified.\n\n    >>> from nltk.corpus import reuters\n    >>> reuters.words('training/13085')\n    ['SNYDER', '&', 'lt', ';', 'SOI', '>', 'MAKES', ...]\n    >>> reuters.words('training/5082')\n    ['SHEPPARD', 'RESOURCES', 'TO', 'MERGE', 'WITH', ...]\n\nsvn 7227 fixed a bug in the qc corpus reader, which prevented\naccess to its tuples() method\n\n    >>> from nltk.corpus import qc\n    >>> qc.tuples('test.txt')\n    [('NUM:dist', 'How far is it from Denver to Aspen ?'), ('LOC:city', 'What county is Modesto , California in ?'), ...]\n"], "fixing_code": ["# Natural Language Toolkit: Comparative Sentence Corpus Reader\n#\n# Copyright (C) 2001-2021 NLTK Project\n# Author: Pierpaolo Pantone <24alsecondo@gmail.com>\n# URL: <http://nltk.org/>\n# For license information, see LICENSE.TXT\n\n\"\"\"\nCorpusReader for the Comparative Sentence Dataset.\n\n- Comparative Sentence Dataset information -\n\nAnnotated by: Nitin Jindal and Bing Liu, 2006.\n              Department of Computer Sicence\n              University of Illinois at Chicago\n\nContact: Nitin Jindal, njindal@cs.uic.edu\n         Bing Liu, liub@cs.uic.edu (http://www.cs.uic.edu/~liub)\n\nDistributed with permission.\n\nRelated papers:\n\n- Nitin Jindal and Bing Liu. \"Identifying Comparative Sentences in Text Documents\".\n   Proceedings of the ACM SIGIR International Conference on Information Retrieval\n   (SIGIR-06), 2006.\n\n- Nitin Jindal and Bing Liu. \"Mining Comprative Sentences and Relations\".\n   Proceedings of Twenty First National Conference on Artificial Intelligence\n   (AAAI-2006), 2006.\n\n- Murthy Ganapathibhotla and Bing Liu. \"Mining Opinions in Comparative Sentences\".\n    Proceedings of the 22nd International Conference on Computational Linguistics\n    (Coling-2008), Manchester, 18-22 August, 2008.\n\"\"\"\nimport re\n\nfrom nltk.corpus.reader.api import *\nfrom nltk.tokenize import *\n\n# Regular expressions for dataset components\nSTARS = re.compile(r\"^\\*+$\")\nCOMPARISON = re.compile(r\"<cs-[1234]>\")\nCLOSE_COMPARISON = re.compile(r\"</cs-[1234]>\")\nGRAD_COMPARISON = re.compile(r\"<cs-[123]>\")\nNON_GRAD_COMPARISON = re.compile(r\"<cs-4>\")\nENTITIES_FEATS = re.compile(r\"(\\d)_((?:[\\.\\w\\s/-](?!\\d_))+)\")\nKEYWORD = re.compile(r\"\\(([^\\(]*)\\)$\")\n\n\nclass Comparison:\n    \"\"\"\n    A Comparison represents a comparative sentence and its constituents.\n    \"\"\"\n\n    def __init__(\n        self,\n        text=None,\n        comp_type=None,\n        entity_1=None,\n        entity_2=None,\n        feature=None,\n        keyword=None,\n    ):\n        \"\"\"\n        :param text: a string (optionally tokenized) containing a comparison.\n        :param comp_type: an integer defining the type of comparison expressed.\n            Values can be: 1 (Non-equal gradable), 2 (Equative), 3 (Superlative),\n            4 (Non-gradable).\n        :param entity_1: the first entity considered in the comparison relation.\n        :param entity_2: the second entity considered in the comparison relation.\n        :param feature: the feature considered in the comparison relation.\n        :param keyword: the word or phrase which is used for that comparative relation.\n        \"\"\"\n        self.text = text\n        self.comp_type = comp_type\n        self.entity_1 = entity_1\n        self.entity_2 = entity_2\n        self.feature = feature\n        self.keyword = keyword\n\n    def __repr__(self):\n        return (\n            'Comparison(text=\"{}\", comp_type={}, entity_1=\"{}\", entity_2=\"{}\", '\n            'feature=\"{}\", keyword=\"{}\")'\n        ).format(\n            self.text,\n            self.comp_type,\n            self.entity_1,\n            self.entity_2,\n            self.feature,\n            self.keyword,\n        )\n\n\nclass ComparativeSentencesCorpusReader(CorpusReader):\n    \"\"\"\n    Reader for the Comparative Sentence Dataset by Jindal and Liu (2006).\n\n        >>> from nltk.corpus import comparative_sentences\n        >>> comparison = comparative_sentences.comparisons()[0]\n        >>> comparison.text\n        ['its', 'fast-forward', 'and', 'rewind', 'work', 'much', 'more', 'smoothly',\n        'and', 'consistently', 'than', 'those', 'of', 'other', 'models', 'i', \"'ve\",\n        'had', '.']\n        >>> comparison.entity_2\n        'models'\n        >>> (comparison.feature, comparison.keyword)\n        ('rewind', 'more')\n        >>> len(comparative_sentences.comparisons())\n        853\n    \"\"\"\n\n    CorpusView = StreamBackedCorpusView\n\n    def __init__(\n        self,\n        root,\n        fileids,\n        word_tokenizer=WhitespaceTokenizer(),\n        sent_tokenizer=None,\n        encoding=\"utf8\",\n    ):\n        \"\"\"\n        :param root: The root directory for this corpus.\n        :param fileids: a list or regexp specifying the fileids in this corpus.\n        :param word_tokenizer: tokenizer for breaking sentences or paragraphs\n            into words. Default: `WhitespaceTokenizer`\n        :param sent_tokenizer: tokenizer for breaking paragraphs into sentences.\n        :param encoding: the encoding that should be used to read the corpus.\n        \"\"\"\n\n        CorpusReader.__init__(self, root, fileids, encoding)\n        self._word_tokenizer = word_tokenizer\n        self._sent_tokenizer = sent_tokenizer\n        self._readme = \"README.txt\"\n\n    def comparisons(self, fileids=None):\n        \"\"\"\n        Return all comparisons in the corpus.\n\n        :param fileids: a list or regexp specifying the ids of the files whose\n            comparisons have to be returned.\n        :return: the given file(s) as a list of Comparison objects.\n        :rtype: list(Comparison)\n        \"\"\"\n        if fileids is None:\n            fileids = self._fileids\n        elif isinstance(fileids, str):\n            fileids = [fileids]\n        return concat(\n            [\n                self.CorpusView(path, self._read_comparison_block, encoding=enc)\n                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n            ]\n        )\n\n    def keywords(self, fileids=None):\n        \"\"\"\n        Return a set of all keywords used in the corpus.\n\n        :param fileids: a list or regexp specifying the ids of the files whose\n            keywords have to be returned.\n        :return: the set of keywords and comparative phrases used in the corpus.\n        :rtype: set(str)\n        \"\"\"\n        all_keywords = concat(\n            [\n                self.CorpusView(path, self._read_keyword_block, encoding=enc)\n                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n            ]\n        )\n\n        keywords_set = {keyword.lower() for keyword in all_keywords if keyword}\n        return keywords_set\n\n    def keywords_readme(self):\n        \"\"\"\n        Return the list of words and constituents considered as clues of a\n        comparison (from listOfkeywords.txt).\n        \"\"\"\n        keywords = []\n        with self.open(\"listOfkeywords.txt\") as fp:\n            raw_text = fp.read()\n        for line in raw_text.split(\"\\n\"):\n            if not line or line.startswith(\"//\"):\n                continue\n            keywords.append(line.strip())\n        return keywords\n\n    def sents(self, fileids=None):\n        \"\"\"\n        Return all sentences in the corpus.\n\n        :param fileids: a list or regexp specifying the ids of the files whose\n            sentences have to be returned.\n        :return: all sentences of the corpus as lists of tokens (or as plain\n            strings, if no word tokenizer is specified).\n        :rtype: list(list(str)) or list(str)\n        \"\"\"\n        return concat(\n            [\n                self.CorpusView(path, self._read_sent_block, encoding=enc)\n                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n            ]\n        )\n\n    def words(self, fileids=None):\n        \"\"\"\n        Return all words and punctuation symbols in the corpus.\n\n        :param fileids: a list or regexp specifying the ids of the files whose\n            words have to be returned.\n        :return: the given file(s) as a list of words and punctuation symbols.\n        :rtype: list(str)\n        \"\"\"\n        return concat(\n            [\n                self.CorpusView(path, self._read_word_block, encoding=enc)\n                for (path, enc, fileid) in self.abspaths(fileids, True, True)\n            ]\n        )\n\n    def _read_comparison_block(self, stream):\n        while True:\n            line = stream.readline()\n            if not line:\n                return []  # end of file.\n            comparison_tags = re.findall(COMPARISON, line)\n            if comparison_tags:\n                grad_comparisons = re.findall(GRAD_COMPARISON, line)\n                non_grad_comparisons = re.findall(NON_GRAD_COMPARISON, line)\n                # Advance to the next line (it contains the comparative sentence)\n                comparison_text = stream.readline().strip()\n                if self._word_tokenizer:\n                    comparison_text = self._word_tokenizer.tokenize(comparison_text)\n                # Skip the next line (it contains closing comparison tags)\n                stream.readline()\n                # If gradable comparisons are found, create Comparison instances\n                # and populate their fields\n                comparison_bundle = []\n                if grad_comparisons:\n                    # Each comparison tag has its own relations on a separate line\n                    for comp in grad_comparisons:\n                        comp_type = int(re.match(r\"<cs-(\\d)>\", comp).group(1))\n                        comparison = Comparison(\n                            text=comparison_text, comp_type=comp_type\n                        )\n                        line = stream.readline()\n                        entities_feats = ENTITIES_FEATS.findall(line)\n                        if entities_feats:\n                            for (code, entity_feat) in entities_feats:\n                                if code == \"1\":\n                                    comparison.entity_1 = entity_feat.strip()\n                                elif code == \"2\":\n                                    comparison.entity_2 = entity_feat.strip()\n                                elif code == \"3\":\n                                    comparison.feature = entity_feat.strip()\n                        keyword = KEYWORD.findall(line)\n                        if keyword:\n                            comparison.keyword = keyword[0]\n                        comparison_bundle.append(comparison)\n                # If non-gradable comparisons are found, create a simple Comparison\n                # instance for each one\n                if non_grad_comparisons:\n                    for comp in non_grad_comparisons:\n                        # comp_type in this case should always be 4.\n                        comp_type = int(re.match(r\"<cs-(\\d)>\", comp).group(1))\n                        comparison = Comparison(\n                            text=comparison_text, comp_type=comp_type\n                        )\n                        comparison_bundle.append(comparison)\n                # Flatten the list of comparisons before returning them\n                # return concat([comparison_bundle])\n                return comparison_bundle\n\n    def _read_keyword_block(self, stream):\n        keywords = []\n        for comparison in self._read_comparison_block(stream):\n            keywords.append(comparison.keyword)\n        return keywords\n\n    def _read_sent_block(self, stream):\n        while True:\n            line = stream.readline()\n            if re.match(STARS, line):\n                while True:\n                    line = stream.readline()\n                    if re.match(STARS, line):\n                        break\n                continue\n            if (\n                not re.findall(COMPARISON, line)\n                and not ENTITIES_FEATS.findall(line)\n                and not re.findall(CLOSE_COMPARISON, line)\n            ):\n                if self._sent_tokenizer:\n                    return [\n                        self._word_tokenizer.tokenize(sent)\n                        for sent in self._sent_tokenizer.tokenize(line)\n                    ]\n                else:\n                    return [self._word_tokenizer.tokenize(line)]\n\n    def _read_word_block(self, stream):\n        words = []\n        for sent in self._read_sent_block(stream):\n            words.extend(sent)\n        return words\n", ".. Copyright (C) 2001-2021 NLTK Project\n.. For license information, see LICENSE.TXT\n\n================\n Corpus Readers\n================\n\nThe `nltk.corpus` package defines a collection of *corpus reader*\nclasses, which can be used to access the contents of a diverse set of\ncorpora.  The list of available corpora is given at:\n\nhttp://www.nltk.org/nltk_data/\n\nEach corpus reader class is specialized to handle a specific\ncorpus format.  In addition, the `nltk.corpus` package automatically\ncreates a set of corpus reader instances that can be used to access\nthe corpora in the NLTK data package.\nSection `Corpus Reader Objects`_ (\"Corpus Reader Objects\") describes\nthe corpus reader instances that can be used to read the corpora in\nthe NLTK data package.  Section `Corpus Reader Classes`_ (\"Corpus\nReader Classes\") describes the corpus reader classes themselves, and\ndiscusses the issues involved in creating new corpus reader objects\nand new corpus reader classes.  Section `Regression Tests`_\n(\"Regression Tests\") contains regression tests for the corpus readers\nand associated functions and classes.\n\n.. contents:: **Table of Contents**\n  :depth: 2\n  :backlinks: none\n\n---------------------\nCorpus Reader Objects\n---------------------\n\nOverview\n========\n\nNLTK includes a diverse set of corpora which can be\nread using the ``nltk.corpus`` package.  Each corpus is accessed by\nmeans of a \"corpus reader\" object from ``nltk.corpus``:\n\n    >>> import nltk.corpus\n    >>> # The Brown corpus:\n    >>> print(str(nltk.corpus.brown).replace('\\\\\\\\','/'))\n    <CategorizedTaggedCorpusReader in '.../corpora/brown'...>\n    >>> # The Penn Treebank Corpus:\n    >>> print(str(nltk.corpus.treebank).replace('\\\\\\\\','/'))\n    <BracketParseCorpusReader in '.../corpora/treebank/combined'...>\n    >>> # The Name Genders Corpus:\n    >>> print(str(nltk.corpus.names).replace('\\\\\\\\','/'))\n    <WordListCorpusReader in '.../corpora/names'...>\n    >>> # The Inaugural Address Corpus:\n    >>> print(str(nltk.corpus.inaugural).replace('\\\\\\\\','/'))\n    <PlaintextCorpusReader in '.../corpora/inaugural'...>\n\nMost corpora consist of a set of files, each containing a document (or\nother pieces of text).  A list of identifiers for these files is\naccessed via the ``fileids()`` method of the corpus reader:\n\n    >>> nltk.corpus.treebank.fileids()\n    ['wsj_0001.mrg', 'wsj_0002.mrg', 'wsj_0003.mrg', 'wsj_0004.mrg', ...]\n    >>> nltk.corpus.inaugural.fileids()\n    ['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...]\n\nEach corpus reader provides a variety of methods to read data from the\ncorpus, depending on the format of the corpus.  For example, plaintext\ncorpora support methods to read the corpus as raw text, a list of\nwords, a list of sentences, or a list of paragraphs.\n\n    >>> from nltk.corpus import inaugural\n    >>> inaugural.raw('1789-Washington.txt')\n    'Fellow-Citizens of the Senate ...'\n    >>> inaugural.words('1789-Washington.txt')\n    ['Fellow', '-', 'Citizens', 'of', 'the', ...]\n    >>> inaugural.sents('1789-Washington.txt')\n    [['Fellow', '-', 'Citizens'...], ['Among', 'the', 'vicissitudes'...]...]\n    >>> inaugural.paras('1789-Washington.txt')\n    [[['Fellow', '-', 'Citizens'...]],\n     [['Among', 'the', 'vicissitudes'...],\n      ['On', 'the', 'one', 'hand', ',', 'I'...]...]...]\n\nEach of these reader methods may be given a single document's item\nname or a list of document item names.  When given a list of document\nitem names, the reader methods will concatenate together the contents\nof the individual documents.\n\n    >>> l1 = len(inaugural.words('1789-Washington.txt'))\n    >>> l2 = len(inaugural.words('1793-Washington.txt'))\n    >>> l3 = len(inaugural.words(['1789-Washington.txt', '1793-Washington.txt']))\n    >>> print('%s+%s == %s' % (l1, l2, l3))\n    1538+147 == 1685\n\nIf the reader methods are called without any arguments, they will\ntypically load all documents in the corpus.\n\n    >>> len(inaugural.words())\n    149797\n\nIf a corpus contains a README file, it can be accessed with a ``readme()`` method:\n\n    >>> inaugural.readme()[:32]\n    'C-Span Inaugural Address Corpus\\n'\n\nPlaintext Corpora\n=================\n\nHere are the first few words from each of NLTK's plaintext corpora:\n\n    >>> nltk.corpus.abc.words()\n    ['PM', 'denies', 'knowledge', 'of', 'AWB', ...]\n    >>> nltk.corpus.genesis.words()\n    ['In', 'the', 'beginning', 'God', 'created', ...]\n    >>> nltk.corpus.gutenberg.words(fileids='austen-emma.txt')\n    ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ...]\n    >>> nltk.corpus.inaugural.words()\n    ['Fellow', '-', 'Citizens', 'of', 'the', ...]\n    >>> nltk.corpus.state_union.words()\n    ['PRESIDENT', 'HARRY', 'S', '.', 'TRUMAN', \"'\", ...]\n    >>> nltk.corpus.webtext.words()\n    ['Cookie', 'Manager', ':', '\"', 'Don', \"'\", 't', ...]\n\nTagged Corpora\n==============\n\nIn addition to the plaintext corpora, NLTK's data package also\ncontains a wide variety of annotated corpora.  For example, the Brown\nCorpus is annotated with part-of-speech tags, and defines additional\nmethods ``tagged_*()`` which words as `(word,tag)` tuples, rather\nthan just bare word strings.\n\n    >>> from nltk.corpus import brown\n    >>> print(brown.words())\n    ['The', 'Fulton', 'County', 'Grand', 'Jury', ...]\n    >>> print(brown.tagged_words())\n    [('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n    >>> print(brown.sents())\n    [['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]\n    >>> print(brown.tagged_sents())\n    [[('The', 'AT'), ('Fulton', 'NP-TL')...],\n     [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR')...]...]\n    >>> print(brown.paras(categories='reviews'))\n    [[['It', 'is', 'not', 'news', 'that', 'Nathan', 'Milstein'...],\n      ['Certainly', 'not', 'in', 'Orchestra', 'Hall', 'where'...]],\n     [['There', 'was', 'about', 'that', 'song', 'something', ...],\n      ['Not', 'the', 'noblest', 'performance', 'we', 'have', ...], ...], ...]\n    >>> print(brown.tagged_paras(categories='reviews'))\n    [[[('It', 'PPS'), ('is', 'BEZ'), ('not', '*'), ...],\n      [('Certainly', 'RB'), ('not', '*'), ('in', 'IN'), ...]],\n     [[('There', 'EX'), ('was', 'BEDZ'), ('about', 'IN'), ...],\n      [('Not', '*'), ('the', 'AT'), ('noblest', 'JJT'), ...], ...], ...]\n\nSimilarly, the Indian Language POS-Tagged Corpus includes samples of\nIndian text annotated with part-of-speech tags:\n\n    >>> from nltk.corpus import indian\n    >>> print(indian.words()) # doctest: +SKIP\n    ['\\xe0\\xa6\\xae\\xe0\\xa6\\xb9\\xe0\\xa6\\xbf\\...',\n     '\\xe0\\xa6\\xb8\\xe0\\xa6\\xa8\\xe0\\xa7\\x8d\\xe0...', ...]\n    >>> print(indian.tagged_words()) # doctest: +SKIP\n    [('\\xe0\\xa6\\xae\\xe0\\xa6\\xb9\\xe0\\xa6\\xbf...', 'NN'),\n     ('\\xe0\\xa6\\xb8\\xe0\\xa6\\xa8\\xe0\\xa7\\x8d\\xe0...', 'NN'), ...]\n\nSeveral tagged corpora support access to a simplified, universal tagset, e.g. where all nouns\ntags are collapsed to a single category ``NOUN``:\n\n    >>> print(brown.tagged_sents(tagset='universal'))\n    [[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ...],\n     [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ...]...]\n    >>> from nltk.corpus import conll2000, switchboard\n    >>> print(conll2000.tagged_words(tagset='universal'))\n    [('Confidence', 'NOUN'), ('in', 'ADP'), ...]\n\nUse ``nltk.app.pos_concordance()`` to access a GUI for searching tagged corpora.\n\nChunked Corpora\n===============\n\nThe CoNLL corpora also provide chunk structures, which are encoded as\nflat trees.  The CoNLL 2000 Corpus includes phrasal chunks; and the\nCoNLL 2002 Corpus includes named entity chunks.\n\n    >>> from nltk.corpus import conll2000, conll2002\n    >>> print(conll2000.sents())\n    [['Confidence', 'in', 'the', 'pound', 'is', 'widely', ...],\n     ['Chancellor', 'of', 'the', 'Exchequer', ...], ...]\n    >>> for tree in conll2000.chunked_sents()[:2]:\n    ...     print(tree)\n    (S\n      (NP Confidence/NN)\n      (PP in/IN)\n      (NP the/DT pound/NN)\n      (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n      (NP another/DT sharp/JJ dive/NN)\n      if/IN\n      ...)\n    (S\n      Chancellor/NNP\n      (PP of/IN)\n      (NP the/DT Exchequer/NNP)\n      ...)\n    >>> print(conll2002.sents())\n    [['Sao', 'Paulo', '(', 'Brasil', ')', ',', ...], ['-'], ...]\n    >>> for tree in conll2002.chunked_sents()[:2]:\n    ...     print(tree)\n    (S\n      (LOC Sao/NC Paulo/VMI)\n      (/Fpa\n      (LOC Brasil/NC)\n      )/Fpt\n      ...)\n    (S -/Fg)\n\n.. note:: Since the CONLL corpora do not contain paragraph break\n   information, these readers do not support the ``para()`` method.)\n\n.. warning:: if you call the conll corpora reader methods without any\n   arguments, they will return the contents of the entire corpus,\n   *including* the 'test' portions of the corpus.)\n\nSemCor is a subset of the Brown corpus tagged with WordNet senses and\nnamed entities. Both kinds of lexical items include multiword units,\nwhich are encoded as chunks (senses and part-of-speech tags pertain\nto the entire chunk).\n\n    >>> from nltk.corpus import semcor\n    >>> semcor.words()\n    ['The', 'Fulton', 'County', 'Grand', 'Jury', ...]\n    >>> semcor.chunks()\n    [['The'], ['Fulton', 'County', 'Grand', 'Jury'], ...]\n    >>> semcor.sents()\n    [['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...],\n    ['The', 'jury', 'further', 'said', ...], ...]\n    >>> semcor.chunk_sents()\n    [[['The'], ['Fulton', 'County', 'Grand', 'Jury'], ['said'], ...\n    ['.']], [['The'], ['jury'], ['further'], ['said'], ... ['.']], ...]\n    >>> list(map(str, semcor.tagged_chunks(tag='both')[:3]))\n    ['(DT The)', \"(Lemma('group.n.01.group') (NE (NNP Fulton County Grand Jury)))\", \"(Lemma('state.v.01.say') (VB said))\"]\n    >>> [[str(c) for c in s] for s in semcor.tagged_sents(tag='both')[:2]]\n    [['(DT The)', \"(Lemma('group.n.01.group') (NE (NNP Fulton County Grand Jury)))\", ...\n     '(None .)'], ['(DT The)', ... '(None .)']]\n\n\nThe IEER corpus is another chunked corpus.  This corpus is unusual in\nthat each corpus item contains multiple documents.  (This reflects the\nfact that each corpus file contains multiple documents.)  The IEER\ncorpus defines the `parsed_docs` method, which returns the documents\nin a given item as `IEERDocument` objects:\n\n    >>> from nltk.corpus import ieer\n    >>> ieer.fileids()\n    ['APW_19980314', 'APW_19980424', 'APW_19980429',\n     'NYT_19980315', 'NYT_19980403', 'NYT_19980407']\n    >>> docs = ieer.parsed_docs('APW_19980314')\n    >>> print(docs[0])\n    <IEERDocument APW19980314.0391: 'Kenyans protest tax hikes'>\n    >>> print(docs[0].docno)\n    APW19980314.0391\n    >>> print(docs[0].doctype)\n    NEWS STORY\n    >>> print(docs[0].date_time)\n    03/14/1998 10:36:00\n    >>> print(docs[0].headline)\n    (DOCUMENT Kenyans protest tax hikes)\n    >>> print(docs[0].text)\n    (DOCUMENT\n      (LOCATION NAIROBI)\n      ,\n      (LOCATION Kenya)\n      (\n      (ORGANIZATION AP)\n      )\n      _\n      (CARDINAL Thousands)\n      of\n      laborers,\n      ...\n      on\n      (DATE Saturday)\n      ...)\n\nParsed Corpora\n==============\n\nThe Treebank corpora provide a syntactic parse for each sentence.  The\nNLTK data package includes a 10% sample of the Penn Treebank (in\n``treebank``), as well as the Sinica Treebank (in ``sinica_treebank``).\n\nReading the Penn Treebank (Wall Street Journal sample):\n\n    >>> from nltk.corpus import treebank\n    >>> print(treebank.fileids())\n    ['wsj_0001.mrg', 'wsj_0002.mrg', 'wsj_0003.mrg', 'wsj_0004.mrg', ...]\n    >>> print(treebank.words('wsj_0003.mrg'))\n    ['A', 'form', 'of', 'asbestos', 'once', 'used', ...]\n    >>> print(treebank.tagged_words('wsj_0003.mrg'))\n    [('A', 'DT'), ('form', 'NN'), ('of', 'IN'), ...]\n    >>> print(treebank.parsed_sents('wsj_0003.mrg')[0])\n    (S\n      (S-TPC-1\n        (NP-SBJ\n          (NP (NP (DT A) (NN form)) (PP (IN of) (NP (NN asbestos))))\n          (RRC ...)...)...)\n      ...\n      (VP (VBD reported) (SBAR (-NONE- 0) (S (-NONE- *T*-1))))\n      (. .))\n\nIf you have access to a full installation of the Penn Treebank, NLTK\ncan be configured to load it as well. Download the ``ptb`` package,\nand in the directory ``nltk_data/corpora/ptb`` place the ``BROWN``\nand ``WSJ`` directories of the Treebank installation (symlinks work\nas well). Then use the ``ptb`` module instead of ``treebank``:\n\n   >>> from nltk.corpus import ptb\n   >>> print(ptb.fileids()) # doctest: +SKIP\n   ['BROWN/CF/CF01.MRG', 'BROWN/CF/CF02.MRG', 'BROWN/CF/CF03.MRG', 'BROWN/CF/CF04.MRG', ...]\n   >>> print(ptb.words('WSJ/00/WSJ_0003.MRG')) # doctest: +SKIP\n   ['A', 'form', 'of', 'asbestos', 'once', 'used', '*', ...]\n   >>> print(ptb.tagged_words('WSJ/00/WSJ_0003.MRG')) # doctest: +SKIP\n   [('A', 'DT'), ('form', 'NN'), ('of', 'IN'), ...]\n\n...and so forth, like ``treebank`` but with extended fileids. Categories\nspecified in ``allcats.txt`` can be used to filter by genre; they consist\nof ``news`` (for WSJ articles) and names of the Brown subcategories\n(``fiction``, ``humor``, ``romance``, etc.):\n\n   >>> ptb.categories() # doctest: +SKIP\n   ['adventure', 'belles_lettres', 'fiction', 'humor', 'lore', 'mystery', 'news', 'romance', 'science_fiction']\n   >>> print(ptb.fileids('news')) # doctest: +SKIP\n   ['WSJ/00/WSJ_0001.MRG', 'WSJ/00/WSJ_0002.MRG', 'WSJ/00/WSJ_0003.MRG', ...]\n   >>> print(ptb.words(categories=['humor','fiction'])) # doctest: +SKIP\n   ['Thirty-three', 'Scotty', 'did', 'not', 'go', 'back', ...]\n\nAs PropBank and NomBank depend on the (WSJ portion of the) Penn Treebank,\nthe modules ``propbank_ptb`` and ``nombank_ptb`` are provided for access\nto a full PTB installation.\n\nReading the Sinica Treebank:\n\n    >>> from nltk.corpus import sinica_treebank\n    >>> print(sinica_treebank.sents()) # doctest: +SKIP\n    [['\\xe4\\xb8\\x80'], ['\\xe5\\x8f\\x8b\\xe6\\x83\\x85'], ...]\n    >>> sinica_treebank.parsed_sents()[25] # doctest: +SKIP\n    Tree('S',\n        [Tree('NP',\n            [Tree('Nba', ['\\xe5\\x98\\x89\\xe7\\x8f\\x8d'])]),\n         Tree('V\\xe2\\x80\\xa7\\xe5\\x9c\\xb0',\n            [Tree('VA11', ['\\xe4\\xb8\\x8d\\xe5\\x81\\x9c']),\n             Tree('DE', ['\\xe7\\x9a\\x84'])]),\n         Tree('VA4', ['\\xe5\\x93\\xad\\xe6\\xb3\\xa3'])])\n\nReading the CoNLL 2007 Dependency Treebanks:\n\n    >>> from nltk.corpus import conll2007\n    >>> conll2007.sents('esp.train')[0] # doctest: +SKIP\n    ['El', 'aumento', 'del', '\u00edndice', 'de', 'desempleo', ...]\n    >>> conll2007.parsed_sents('esp.train')[0] # doctest: +SKIP\n    <DependencyGraph with 38 nodes>\n    >>> print(conll2007.parsed_sents('esp.train')[0].tree()) # doctest: +SKIP\n    (fortaleci\u00f3\n      (aumento El (del (\u00edndice (de (desempleo estadounidense)))))\n      hoy\n      considerablemente\n      (al\n        (euro\n          (cotizaba\n            ,\n            que\n            (a (15.35 las GMT))\n            se\n            (en (mercado el (de divisas) (de Fr\u00e1ncfort)))\n            (a 0,9452_d\u00f3lares)\n            (frente_a , (0,9349_d\u00f3lares los (de (ma\u00f1ana esta)))))))\n      .)\n\nWord Lists and Lexicons\n=======================\n\nThe NLTK data package also includes a number of lexicons and word\nlists.  These are accessed just like text corpora.  The following\nexamples illustrate the use of the wordlist corpora:\n\n    >>> from nltk.corpus import names, stopwords, words\n    >>> words.fileids()\n    ['en', 'en-basic']\n    >>> words.words('en')\n    ['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', ...]\n\n    >>> stopwords.fileids()\n    ['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', ...]\n    >>> sorted(stopwords.words('portuguese'))\n    ['a', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', ...]\n    >>> names.fileids()\n    ['female.txt', 'male.txt']\n    >>> names.words('male.txt')\n    ['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', ...]\n    >>> names.words('female.txt')\n    ['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', ...]\n\nThe CMU Pronunciation Dictionary corpus contains pronunciation\ntranscriptions for over 100,000 words.  It can be accessed as a list\nof entries (where each entry consists of a word, an identifier, and a\ntranscription) or as a dictionary from words to lists of\ntranscriptions.  Transcriptions are encoded as tuples of phoneme\nstrings.\n\n    >>> from nltk.corpus import cmudict\n    >>> print(cmudict.entries()[653:659])\n    [('acetate', ['AE1', 'S', 'AH0', 'T', 'EY2', 'T']),\n    ('acetic', ['AH0', 'S', 'EH1', 'T', 'IH0', 'K']),\n    ('acetic', ['AH0', 'S', 'IY1', 'T', 'IH0', 'K']),\n    ('aceto', ['AA0', 'S', 'EH1', 'T', 'OW0']),\n    ('acetochlor', ['AA0', 'S', 'EH1', 'T', 'OW0', 'K', 'L', 'AO2', 'R']),\n    ('acetone', ['AE1', 'S', 'AH0', 'T', 'OW2', 'N'])]\n    >>> # Load the entire cmudict corpus into a Python dictionary:\n    >>> transcr = cmudict.dict()\n    >>> print([transcr[w][0] for w in 'Natural Language Tool Kit'.lower().split()])\n    [['N', 'AE1', 'CH', 'ER0', 'AH0', 'L'],\n     ['L', 'AE1', 'NG', 'G', 'W', 'AH0', 'JH'],\n     ['T', 'UW1', 'L'],\n     ['K', 'IH1', 'T']]\n\n\nWordNet\n=======\n\nPlease see the separate WordNet howto.\n\nFrameNet\n========\n\nPlease see the separate FrameNet howto.\n\nPropBank\n========\n\nPlease see the separate PropBank howto.\n\nSentiWordNet\n============\n\nPlease see the separate SentiWordNet howto.\n\nCategorized Corpora\n===================\n\nSeveral corpora included with NLTK contain documents that have been categorized for\ntopic, genre, polarity, etc.  In addition to the standard corpus interface, these\ncorpora provide access to the list of categories and the mapping between the documents\nand their categories (in both directions).  Access the categories using the ``categories()``\nmethod, e.g.:\n\n    >>> from nltk.corpus import brown, movie_reviews, reuters\n    >>> brown.categories()\n    ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor',\n    'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n    >>> movie_reviews.categories()\n    ['neg', 'pos']\n    >>> reuters.categories()\n    ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',\n    'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',\n    'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]\n\nThis method has an optional argument that specifies a document or a list\nof documents, allowing us to map from (one or more) documents to (one or more) categories:\n\n    >>> brown.categories('ca01')\n    ['news']\n    >>> brown.categories(['ca01','cb01'])\n    ['editorial', 'news']\n    >>> reuters.categories('training/9865')\n    ['barley', 'corn', 'grain', 'wheat']\n    >>> reuters.categories(['training/9865', 'training/9880'])\n    ['barley', 'corn', 'grain', 'money-fx', 'wheat']\n\nWe can go back the other way using the optional argument of the ``fileids()`` method:\n\n    >>> reuters.fileids('barley')\n    ['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]\n\nBoth the ``categories()`` and ``fileids()`` methods return a sorted list containing\nno duplicates.\n\nIn addition to mapping between categories and documents, these corpora permit\ndirect access to their contents via the categories.  Instead of accessing a subset\nof a corpus by specifying one or more fileids, we can identify one or more categories, e.g.:\n\n    >>> brown.tagged_words(categories='news')\n    [('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n    >>> brown.sents(categories=['editorial','reviews'])\n    [['Assembly', 'session', 'brought', 'much', 'good'], ['The', 'General',\n    'Assembly', ',', 'which', 'adjourns', 'today', ',', 'has', 'performed',\n    'in', 'an', 'atmosphere', 'of', 'crisis', 'and', 'struggle', 'from',\n    'the', 'day', 'it', 'convened', '.'], ...]\n\nNote that it is an error to specify both documents and categories.\n\nIn the context of a text categorization system, we can easily test if the\ncategory assigned to a document is correct as follows:\n\n    >>> def classify(doc): return 'news'   # Trivial classifier\n    >>> doc = 'ca01'\n    >>> classify(doc) in brown.categories(doc)\n    True\n\n\nOther Corpora\n=============\n\ncomparative_sentences\n---------------------\nA list of sentences from various sources, especially reviews and articles. Each\nline contains one sentence; sentences were separated by using a sentence tokenizer.\nComparative sentences have been annotated with their type, entities, features and\nkeywords.\n\n    >>> from nltk.corpus import comparative_sentences\n    >>> comparison = comparative_sentences.comparisons()[0]\n    >>> comparison.text\n    ['its', 'fast-forward', 'and', 'rewind', 'work', 'much', 'more', 'smoothly',\n    'and', 'consistently', 'than', 'those', 'of', 'other', 'models', 'i', \"'ve\",\n    'had', '.']\n    >>> comparison.entity_2\n    'models'\n    >>> (comparison.feature, comparison.keyword)\n    ('rewind', 'more')\n    >>> len(comparative_sentences.comparisons())\n    853\n\nopinion_lexicon\n---------------\nA list of positive and negative opinion words or sentiment words for English.\n\n    >>> from nltk.corpus import opinion_lexicon\n    >>> opinion_lexicon.words()[:4]\n        ['2-faced', '2-faces', 'abnormal', 'abolish']\n\nThe OpinionLexiconCorpusReader also provides shortcuts to retrieve positive/negative\nwords:\n\n    >>> opinion_lexicon.negative()[:4]\n    ['2-faced', '2-faces', 'abnormal', 'abolish']\n\nNote that words from `words()` method in opinion_lexicon are sorted by file id,\nnot alphabetically:\n\n    >>> opinion_lexicon.words()[0:10]\n    ['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably',\n    'abominate', 'abomination', 'abort', 'aborted']\n    >>> sorted(opinion_lexicon.words())[0:10]\n    ['2-faced', '2-faces', 'a+', 'abnormal', 'abolish', 'abominable', 'abominably',\n    'abominate', 'abomination', 'abort']\n\nppattach\n--------\nThe Prepositional Phrase Attachment corpus is a corpus of\nprepositional phrase attachment decisions.  Each instance in the\ncorpus is encoded as a ``PPAttachment`` object:\n\n    >>> from nltk.corpus import ppattach\n    >>> ppattach.attachments('training')\n    [PPAttachment(sent='0', verb='join', noun1='board',\n                  prep='as', noun2='director', attachment='V'),\n     PPAttachment(sent='1', verb='is', noun1='chairman',\n                  prep='of', noun2='N.V.', attachment='N'),\n     ...]\n    >>> inst = ppattach.attachments('training')[0]\n    >>> (inst.sent, inst.verb, inst.noun1, inst.prep, inst.noun2)\n    ('0', 'join', 'board', 'as', 'director')\n    >>> inst.attachment\n    'V'\n\nproduct_reviews_1 and product_reviews_2\n---------------------------------------\nThese two datasets respectively contain annotated customer reviews of 5 and 9\nproducts from amazon.com.\n\n    >>> from nltk.corpus import product_reviews_1\n    >>> camera_reviews = product_reviews_1.reviews('Canon_G3.txt')\n    >>> review = camera_reviews[0]\n    >>> review.sents()[0]\n    ['i', 'recently', 'purchased', 'the', 'canon', 'powershot', 'g3', 'and', 'am',\n    'extremely', 'satisfied', 'with', 'the', 'purchase', '.']\n    >>> review.features()\n    [('canon powershot g3', '+3'), ('use', '+2'), ('picture', '+2'),\n    ('picture quality', '+1'), ('picture quality', '+1'), ('camera', '+2'),\n    ('use', '+2'), ('feature', '+1'), ('picture quality', '+3'), ('use', '+1'),\n    ('option', '+1')]\n\nIt is also possible to reach the same information directly from the stream:\n\n    >>> product_reviews_1.features('Canon_G3.txt')\n    [('canon powershot g3', '+3'), ('use', '+2'), ...]\n\nWe can compute stats for specific product features:\n\n    >>> n_reviews = len([(feat,score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])\n    >>> tot = sum([int(score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])\n    >>> mean = tot / n_reviews\n    >>> print(n_reviews, tot, mean)\n    15 24 1.6\n\npros_cons\n---------\nA list of pros/cons sentences for determining context (aspect) dependent\nsentiment words, which are then applied to sentiment analysis of comparative\nsentences.\n\n    >>> from nltk.corpus import pros_cons\n    >>> pros_cons.sents(categories='Cons')\n    [['East', 'batteries', '!', 'On', '-', 'off', 'switch', 'too', 'easy',\n    'to', 'maneuver', '.'], ['Eats', '...', 'no', ',', 'GULPS', 'batteries'],\n    ...]\n    >>> pros_cons.words('IntegratedPros.txt')\n    ['Easy', 'to', 'use', ',', 'economical', '!', ...]\n\nsemcor\n------\nThe Brown Corpus, annotated with WordNet senses.\n\n    >>> from nltk.corpus import semcor\n    >>> semcor.words('brown2/tagfiles/br-n12.xml')\n    ['When', 'several', 'minutes', 'had', 'passed', ...]\n\nsenseval\n--------\nThe Senseval 2 corpus is a word sense disambiguation corpus.  Each\nitem in the corpus corresponds to a single ambiguous word.  For each\nof these words, the corpus contains a list of instances, corresponding\nto occurrences of that word.  Each instance provides the word; a list\nof word senses that apply to the word occurrence; and the word's\ncontext.\n\n    >>> from nltk.corpus import senseval\n    >>> senseval.fileids()\n    ['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']\n    >>> senseval.instances('hard.pos')\n    ...\n    [SensevalInstance(word='hard-a',\n        position=20,\n        context=[('``', '``'), ('he', 'PRP'), ...('hard', 'JJ'), ...],\n        senses=('HARD1',)),\n     SensevalInstance(word='hard-a',\n        position=10,\n        context=[('clever', 'NNP'), ...('hard', 'JJ'), ('time', 'NN'), ...],\n        senses=('HARD1',)), ...]\n\nThe following code looks at instances of the word 'interest', and\ndisplays their local context (2 words on each side) and word sense(s):\n\n    >>> for inst in senseval.instances('interest.pos')[:10]:\n    ...     p = inst.position\n    ...     left = ' '.join(w for (w,t) in inst.context[p-2:p])\n    ...     word = ' '.join(w for (w,t) in inst.context[p:p+1])\n    ...     right = ' '.join(w for (w,t) in inst.context[p+1:p+3])\n    ...     senses = ' '.join(inst.senses)\n    ...     print('%20s |%10s | %-15s -> %s' % (left, word, right, senses))\n             declines in |  interest | rates .         -> interest_6\n      indicate declining |  interest | rates because   -> interest_6\n           in short-term |  interest | rates .         -> interest_6\n                     4 % |  interest | in this         -> interest_5\n            company with | interests | in the          -> interest_5\n                  , plus |  interest | .               -> interest_6\n                 set the |  interest | rate on         -> interest_6\n                  's own |  interest | , prompted      -> interest_4\n           principal and |  interest | is the          -> interest_6\n            increase its |  interest | to 70           -> interest_5\n\nsentence_polarity\n-----------------\nThe Sentence Polarity dataset contains 5331 positive and 5331 negative processed\nsentences.\n\n    >>> from nltk.corpus import sentence_polarity\n    >>> sentence_polarity.sents()\n    [['simplistic', ',', 'silly', 'and', 'tedious', '.'], [\"it's\", 'so', 'laddish',\n    'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find',\n    'it', 'funny', '.'], ...]\n    >>> sentence_polarity.categories()\n    ['neg', 'pos']\n    >>> sentence_polarity.sents()[1]\n    [\"it's\", 'so', 'laddish', 'and', 'juvenile', ',', 'only', 'teenage', 'boys',\n    'could', 'possibly', 'find', 'it', 'funny', '.']\n\nshakespeare\n-----------\nThe Shakespeare corpus contains a set of Shakespeare plays, formatted\nas XML files.  These corpora are returned as ElementTree objects:\n\n    >>> from nltk.corpus import shakespeare\n    >>> from xml.etree import ElementTree\n    >>> shakespeare.fileids()\n    ['a_and_c.xml', 'dream.xml', 'hamlet.xml', 'j_caesar.xml', ...]\n    >>> play = shakespeare.xml('dream.xml')\n    >>> print(play)\n    <Element 'PLAY' at ...>\n    >>> print('%s: %s' % (play[0].tag, play[0].text))\n    TITLE: A Midsummer Night's Dream\n    >>> personae = [persona.text for persona in\n    ...             play.findall('PERSONAE/PERSONA')]\n    >>> print(personae)\n    ['THESEUS, Duke of Athens.', 'EGEUS, father to Hermia.', ...]\n    >>> # Find and print speakers not listed as personae\n    >>> names = [persona.split(',')[0] for persona in personae]\n    >>> speakers = set(speaker.text for speaker in\n    ...                play.findall('*/*/*/SPEAKER'))\n    >>> print(sorted(speakers.difference(names)))\n    ['ALL', 'COBWEB', 'DEMETRIUS', 'Fairy', 'HERNIA', 'LYSANDER',\n     'Lion', 'MOTH', 'MUSTARDSEED', 'Moonshine', 'PEASEBLOSSOM',\n     'Prologue', 'Pyramus', 'Thisbe', 'Wall']\n\nsubjectivity\n-----------\nThe Subjectivity Dataset contains 5000 subjective and 5000 objective processed\nsentences.\n\n    >>> from nltk.corpus import subjectivity\n    >>> subjectivity.categories()\n    ['obj', 'subj']\n    >>> subjectivity.sents()[23]\n    ['television', 'made', 'him', 'famous', ',', 'but', 'his', 'biggest', 'hits',\n    'happened', 'off', 'screen', '.']\n    >>> subjectivity.words(categories='subj')\n    ['smart', 'and', 'alert', ',', 'thirteen', ...]\n\ntoolbox\n-------\nThe Toolbox corpus distributed with NLTK contains a sample lexicon and\nseveral sample texts from the Rotokas language.  The Toolbox corpus\nreader returns Toolbox files as XML ElementTree objects.  The\nfollowing example loads the Rotokas dictionary, and figures out the\ndistribution of part-of-speech tags for reduplicated words.\n\n.. doctest: +SKIP\n\n    >>> from nltk.corpus import toolbox\n    >>> from nltk.probability import FreqDist\n    >>> from xml.etree import ElementTree\n    >>> import re\n    >>> rotokas = toolbox.xml('rotokas.dic')\n    >>> redup_pos_freqdist = FreqDist()\n    >>> # Note: we skip over the first record, which is actually\n    >>> # the header.\n    >>> for record in rotokas[1:]:\n    ...     lexeme = record.find('lx').text\n    ...     if re.match(r'(.*)\\1$', lexeme):\n    ...         redup_pos_freqdist[record.find('ps').text] += 1\n    >>> for item, count in redup_pos_freqdist.most_common():\n    ...     print(item, count)\n    V 41\n    N 14\n    ??? 4\n\nThis example displays some records from a Rotokas text:\n\n.. doctest: +SKIP\n\n    >>> river = toolbox.xml('rotokas/river.txt', key='ref')\n    >>> for record in river.findall('record')[:3]:\n    ...     for piece in record:\n    ...         if len(piece.text) > 60:\n    ...             print('%-6s %s...' % (piece.tag, piece.text[:57]))\n    ...         else:\n    ...             print('%-6s %s' % (piece.tag, piece.text))\n    ref    Paragraph 1\n    t      ``Viapau oisio              ra   ovaupasi                ...\n    m      viapau   oisio              ra   ovau   -pa       -si    ...\n    g      NEG      this way/like this and  forget -PROG     -2/3.DL...\n    p      NEG      ???                CONJ V.I    -SUFF.V.3 -SUFF.V...\n    f      ``No ken lus tingting wanema samting papa i bin tok,'' Na...\n    fe     ``Don't forget what Dad said,'' yelled Naomi.\n    ref    2\n    t      Osa     Ira  ora  Reviti viapau uvupasiva.\n    m      osa     Ira  ora  Reviti viapau uvu        -pa       -si ...\n    g      as/like name and  name   NEG    hear/smell -PROG     -2/3...\n    p      CONJ    N.PN CONJ N.PN   NEG    V.T        -SUFF.V.3 -SUF...\n    f      Tasol Ila na David no bin harim toktok.\n    fe     But Ila and David took no notice.\n    ref    3\n    t      Ikaupaoro                     rokosiva                   ...\n    m      ikau      -pa       -oro      roko    -si       -va      ...\n    g      run/hurry -PROG     -SIM      go down -2/3.DL.M -RP      ...\n    p      V.T       -SUFF.V.3 -SUFF.V.4 ADV     -SUFF.V.4 -SUFF.VT....\n    f      Tupela i bin hariap i go long wara .\n    fe     They raced to the river.\n\ntimit\n-----\nThe NLTK data package includes a fragment of the TIMIT\nAcoustic-Phonetic Continuous Speech Corpus.  This corpus is broken\ndown into small speech samples, each of which is available as a wave\nfile, a phonetic transcription, and a tokenized word list.\n\n    >>> from nltk.corpus import timit\n    >>> print(timit.utteranceids())\n    ['dr1-fvmh0/sa1', 'dr1-fvmh0/sa2', 'dr1-fvmh0/si1466',\n    'dr1-fvmh0/si2096', 'dr1-fvmh0/si836', 'dr1-fvmh0/sx116',\n    'dr1-fvmh0/sx206', 'dr1-fvmh0/sx26', 'dr1-fvmh0/sx296', ...]\n\n    >>> item = timit.utteranceids()[5]\n    >>> print(timit.phones(item))\n    ['h#', 'k', 'l', 'ae', 's', 'pcl', 'p', 'dh', 'ax',\n     's', 'kcl', 'k', 'r', 'ux', 'ix', 'nx', 'y', 'ax',\n     'l', 'eh', 'f', 'tcl', 't', 'hh', 'ae', 'n', 'dcl',\n     'd', 'h#']\n    >>> print(timit.words(item))\n    ['clasp', 'the', 'screw', 'in', 'your', 'left', 'hand']\n    >>> timit.play(item) # doctest: +SKIP\n\nThe corpus reader can combine the word segmentation information with\nthe phonemes to produce a single tree structure:\n\n    >>> for tree in timit.phone_trees(item):\n    ...     print(tree)\n    (S\n      h#\n      (clasp k l ae s pcl p)\n      (the dh ax)\n      (screw s kcl k r ux)\n      (in ix nx)\n      (your y ax)\n      (left l eh f tcl t)\n      (hand hh ae n dcl d)\n      h#)\n\nThe start time and stop time of each phoneme, word, and sentence are\nalso available:\n\n    >>> print(timit.phone_times(item))\n    [('h#', 0, 2190), ('k', 2190, 3430), ('l', 3430, 4326), ...]\n    >>> print(timit.word_times(item))\n    [('clasp', 2190, 8804), ('the', 8804, 9734), ...]\n    >>> print(timit.sent_times(item))\n    [('Clasp the screw in your left hand.', 0, 32154)]\n\nWe can use these times to play selected pieces of a speech sample:\n\n    >>> timit.play(item, 2190, 8804) # 'clasp'  # doctest: +SKIP\n\nThe corpus reader can also be queried for information about the\nspeaker and sentence identifier for a given speech sample:\n\n    >>> print(timit.spkrid(item))\n    dr1-fvmh0\n    >>> print(timit.sentid(item))\n    sx116\n    >>> print(timit.spkrinfo(timit.spkrid(item)))\n    SpeakerInfo(id='VMH0',\n                sex='F',\n                dr='1',\n                use='TRN',\n                recdate='03/11/86',\n                birthdate='01/08/60',\n                ht='5\\'05\"',\n                race='WHT',\n                edu='BS',\n                comments='BEST NEW ENGLAND ACCENT SO FAR')\n\n    >>> # List the speech samples from the same speaker:\n    >>> timit.utteranceids(spkrid=timit.spkrid(item))\n    ['dr1-fvmh0/sa1', 'dr1-fvmh0/sa2', 'dr1-fvmh0/si1466', ...]\n\ntwitter_samples\n---------------\n\nTwitter is well-known microblog service that allows public data to be\ncollected via APIs. NLTK's twitter corpus currently contains a sample of 20k Tweets\nretrieved from the Twitter Streaming API.\n\n    >>> from nltk.corpus import twitter_samples\n    >>> twitter_samples.fileids()\n    ['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n\nWe follow standard practice in storing full Tweets as line-separated\nJSON. These data structures can be accessed via `tweets.docs()`. However, in general it\nis more practical to focus just on the text field of the Tweets, which\nare accessed via the `strings()` method.\n\n    >>> twitter_samples.strings('tweets.20150430-223406.json')[:5]\n    ['RT @KirkKus: Indirect cost of the UK being in the EU is estimated to be costing Britain \\xa3170 billion per year! #BetterOffOut #UKIP', ...]\n\nThe default tokenizer for Tweets is specialised for 'casual' text, and\nthe `tokenized()` method returns a list of lists of tokens.\n\n    >>> twitter_samples.tokenized('tweets.20150430-223406.json')[:5]\n    [['RT', '@KirkKus', ':', 'Indirect', 'cost', 'of', 'the', 'UK', 'being', 'in', ...],\n     ['VIDEO', ':', 'Sturgeon', 'on', 'post-election', 'deals', 'http://t.co/BTJwrpbmOY'], ...]\n\nrte\n---\nThe RTE (Recognizing Textual Entailment) corpus was derived from the\nRTE1, RTE2 and RTE3 datasets (dev and test data), and consists of a\nlist of XML-formatted 'text'/'hypothesis' pairs.\n\n    >>> from nltk.corpus import rte\n    >>> print(rte.fileids())\n    ['rte1_dev.xml', 'rte1_test.xml', 'rte2_dev.xml', ..., 'rte3_test.xml']\n    >>> rtepairs = rte.pairs(['rte2_test.xml', 'rte3_test.xml'])\n    >>> print(rtepairs)\n    [<RTEPair: gid=2-8>, <RTEPair: gid=2-9>, <RTEPair: gid=2-15>, ...]\n\nIn the gold standard test sets, each pair is labeled according to\nwhether or not the text 'entails' the hypothesis; the\nentailment value is mapped to an integer 1 (True) or 0 (False).\n\n    >>> rtepairs[5]\n    <RTEPair: gid=2-23>\n    >>> rtepairs[5].text\n    'His wife Strida won a seat in parliament after forging an alliance\n    with the main anti-Syrian coalition in the recent election.'\n    >>> rtepairs[5].hyp\n    'Strida elected to parliament.'\n    >>> rtepairs[5].value\n    1\n\nThe RTE corpus also supports an ``xml()`` method which produces ElementTrees.\n\n    >>> xmltree = rte.xml('rte3_dev.xml')\n    >>> xmltree # doctest: +SKIP\n    <Element entailment-corpus at ...>\n    >>> xmltree[7].findtext('t')\n    \"Mrs. Bush's approval ratings have remained very high, above 80%,\n    even as her husband's have recently dropped below 50%.\"\n\nverbnet\n-------\nThe VerbNet corpus is a lexicon that divides verbs into classes, based\non their syntax-semantics linking behavior.  The basic elements in the\nlexicon are verb lemmas, such as 'abandon' and 'accept', and verb\nclasses, which have identifiers such as 'remove-10.1' and\n'admire-31.2-1'.  These class identifiers consist of a representative\nverb selected from the class, followed by a numerical identifier.  The\nlist of verb lemmas, and the list of class identifiers, can be\nretrieved with the following methods:\n\n    >>> from nltk.corpus import verbnet\n    >>> verbnet.lemmas()[20:25]\n    ['accelerate', 'accept', 'acclaim', 'accompany', 'accrue']\n    >>> verbnet.classids()[:5]\n    ['accompany-51.7', 'admire-31.2', 'admire-31.2-1', 'admit-65', 'adopt-93']\n\nThe `classids()` method may also be used to retrieve the classes that\na given lemma belongs to:\n\n    >>> verbnet.classids('accept')\n    ['approve-77', 'characterize-29.2-1-1', 'obtain-13.5.2']\n\nThe `classids()` method may additionally be used to retrieve all classes\nwithin verbnet if nothing is passed:\n\n    >>> verbnet.classids()\n    ['accompany-51.7', 'admire-31.2', 'admire-31.2-1', 'admit-65', 'adopt-93', 'advise-37.9', 'advise-37.9-1', 'allow-64', 'amalgamate-22.2', 'amalgamate-22.2-1', 'amalgamate-22.2-1-1', 'amalgamate-22.2-2', 'amalgamate-22.2-2-1', 'amalgamate-22.2-3', 'amalgamate-22.2-3-1', 'amalgamate-22.2-3-1-1', 'amalgamate-22.2-3-2', 'amuse-31.1', 'animal_sounds-38', 'appeal-31.4', 'appeal-31.4-1', 'appeal-31.4-2', 'appeal-31.4-3', 'appear-48.1.1', 'appoint-29.1', 'approve-77', 'assessment-34', 'assuming_position-50', 'avoid-52', 'banish-10.2', 'battle-36.4', 'battle-36.4-1', 'begin-55.1', 'begin-55.1-1', 'being_dressed-41.3.3', 'bend-45.2', 'berry-13.7', 'bill-54.5', 'body_internal_motion-49', 'body_internal_states-40.6', 'braid-41.2.2', 'break-45.1', 'breathe-40.1.2', 'breathe-40.1.2-1', 'bring-11.3', 'bring-11.3-1', 'build-26.1', 'build-26.1-1', 'bulge-47.5.3', 'bump-18.4', 'bump-18.4-1', 'butter-9.9', 'calibratable_cos-45.6', 'calibratable_cos-45.6-1', 'calve-28', 'captain-29.8', 'captain-29.8-1', 'captain-29.8-1-1', 'care-88', 'care-88-1', 'carry-11.4', 'carry-11.4-1', 'carry-11.4-1-1', 'carve-21.2', 'carve-21.2-1', 'carve-21.2-2', 'change_bodily_state-40.8.4', 'characterize-29.2', 'characterize-29.2-1', 'characterize-29.2-1-1', 'characterize-29.2-1-2', 'chase-51.6', 'cheat-10.6', 'cheat-10.6-1', 'cheat-10.6-1-1', 'chew-39.2', 'chew-39.2-1', 'chew-39.2-2', 'chit_chat-37.6', 'clear-10.3', 'clear-10.3-1', 'cling-22.5', 'coil-9.6', 'coil-9.6-1', 'coloring-24', 'complain-37.8', 'complete-55.2', 'concealment-16', 'concealment-16-1', 'confess-37.10', 'confine-92', 'confine-92-1', 'conjecture-29.5', 'conjecture-29.5-1', 'conjecture-29.5-2', 'consider-29.9', 'consider-29.9-1', 'consider-29.9-1-1', 'consider-29.9-1-1-1', 'consider-29.9-2', 'conspire-71', 'consume-66', 'consume-66-1', 'contiguous_location-47.8', 'contiguous_location-47.8-1', 'contiguous_location-47.8-2', 'continue-55.3', 'contribute-13.2', 'contribute-13.2-1', 'contribute-13.2-1-1', 'contribute-13.2-1-1-1', 'contribute-13.2-2', 'contribute-13.2-2-1', 'convert-26.6.2', 'convert-26.6.2-1', 'cooking-45.3', 'cooperate-73', 'cooperate-73-1', 'cooperate-73-2', 'cooperate-73-3', 'cope-83', 'cope-83-1', 'cope-83-1-1', 'correlate-86', 'correspond-36.1', 'correspond-36.1-1', 'correspond-36.1-1-1', 'cost-54.2', 'crane-40.3.2', 'create-26.4', 'create-26.4-1', 'curtsey-40.3.3', 'cut-21.1', 'cut-21.1-1', 'debone-10.8', 'declare-29.4', 'declare-29.4-1', 'declare-29.4-1-1', 'declare-29.4-1-1-1', 'declare-29.4-1-1-2', 'declare-29.4-1-1-3', 'declare-29.4-2', 'dedicate-79', 'defend-85', 'destroy-44', 'devour-39.4', 'devour-39.4-1', 'devour-39.4-2', 'differ-23.4', 'dine-39.5', 'disappearance-48.2', 'disassemble-23.3', 'discover-84', 'discover-84-1', 'discover-84-1-1', 'dress-41.1.1', 'dressing_well-41.3.2', 'drive-11.5', 'drive-11.5-1', 'dub-29.3', 'dub-29.3-1', 'eat-39.1', 'eat-39.1-1', 'eat-39.1-2', 'enforce-63', 'engender-27', 'entity_specific_cos-45.5', 'entity_specific_modes_being-47.2', 'equip-13.4.2', 'equip-13.4.2-1', 'equip-13.4.2-1-1', 'escape-51.1', 'escape-51.1-1', 'escape-51.1-2', 'escape-51.1-2-1', 'exceed-90', 'exchange-13.6', 'exchange-13.6-1', 'exchange-13.6-1-1', 'exhale-40.1.3', 'exhale-40.1.3-1', 'exhale-40.1.3-2', 'exist-47.1', 'exist-47.1-1', 'exist-47.1-1-1', 'feeding-39.7', 'ferret-35.6', 'fill-9.8', 'fill-9.8-1', 'fit-54.3', 'flinch-40.5', 'floss-41.2.1', 'focus-87', 'forbid-67', 'force-59', 'force-59-1', 'free-80', 'free-80-1', 'fulfilling-13.4.1', 'fulfilling-13.4.1-1', 'fulfilling-13.4.1-2', 'funnel-9.3', 'funnel-9.3-1', 'funnel-9.3-2', 'funnel-9.3-2-1', 'future_having-13.3', 'get-13.5.1', 'get-13.5.1-1', 'give-13.1', 'give-13.1-1', 'gobble-39.3', 'gobble-39.3-1', 'gobble-39.3-2', 'gorge-39.6', 'groom-41.1.2', 'grow-26.2', 'help-72', 'help-72-1', 'herd-47.5.2', 'hiccup-40.1.1', 'hit-18.1', 'hit-18.1-1', 'hold-15.1', 'hold-15.1-1', 'hunt-35.1', 'hurt-40.8.3', 'hurt-40.8.3-1', 'hurt-40.8.3-1-1', 'hurt-40.8.3-2', 'illustrate-25.3', 'image_impression-25.1', 'indicate-78', 'indicate-78-1', 'indicate-78-1-1', 'inquire-37.1.2', 'instr_communication-37.4', 'investigate-35.4', 'judgement-33', 'keep-15.2', 'knead-26.5', 'learn-14', 'learn-14-1', 'learn-14-2', 'learn-14-2-1', 'leave-51.2', 'leave-51.2-1', 'lecture-37.11', 'lecture-37.11-1', 'lecture-37.11-1-1', 'lecture-37.11-2', 'light_emission-43.1', 'limit-76', 'linger-53.1', 'linger-53.1-1', 'lodge-46', 'long-32.2', 'long-32.2-1', 'long-32.2-2', 'manner_speaking-37.3', 'marry-36.2', 'marvel-31.3', 'marvel-31.3-1', 'marvel-31.3-2', 'marvel-31.3-3', 'marvel-31.3-4', 'marvel-31.3-5', 'marvel-31.3-6', 'marvel-31.3-7', 'marvel-31.3-8', 'marvel-31.3-9', 'masquerade-29.6', 'masquerade-29.6-1', 'masquerade-29.6-2', 'matter-91', 'meander-47.7', 'meet-36.3', 'meet-36.3-1', 'meet-36.3-2', 'mine-10.9', 'mix-22.1', 'mix-22.1-1', 'mix-22.1-1-1', 'mix-22.1-2', 'mix-22.1-2-1', 'modes_of_being_with_motion-47.3', 'murder-42.1', 'murder-42.1-1', 'neglect-75', 'neglect-75-1', 'neglect-75-1-1', 'neglect-75-2', 'nonvehicle-51.4.2', 'nonverbal_expression-40.2', 'obtain-13.5.2', 'obtain-13.5.2-1', 'occurrence-48.3', 'order-60', 'order-60-1', 'orphan-29.7', 'other_cos-45.4', 'pain-40.8.1', 'pay-68', 'peer-30.3', 'pelt-17.2', 'performance-26.7', 'performance-26.7-1', 'performance-26.7-1-1', 'performance-26.7-2', 'performance-26.7-2-1', 'pit-10.7', 'pocket-9.10', 'pocket-9.10-1', 'poison-42.2', 'poke-19', 'pour-9.5', 'preparing-26.3', 'preparing-26.3-1', 'preparing-26.3-2', 'price-54.4', 'push-12', 'push-12-1', 'push-12-1-1', 'put-9.1', 'put-9.1-1', 'put-9.1-2', 'put_direction-9.4', 'put_spatial-9.2', 'put_spatial-9.2-1', 'reach-51.8', 'reflexive_appearance-48.1.2', 'refrain-69', 'register-54.1', 'rely-70', 'remove-10.1', 'risk-94', 'risk-94-1', 'roll-51.3.1', 'rummage-35.5', 'run-51.3.2', 'rush-53.2', 'say-37.7', 'say-37.7-1', 'say-37.7-1-1', 'say-37.7-2', 'scribble-25.2', 'search-35.2', 'see-30.1', 'see-30.1-1', 'see-30.1-1-1', 'send-11.1', 'send-11.1-1', 'separate-23.1', 'separate-23.1-1', 'separate-23.1-2', 'settle-89', 'shake-22.3', 'shake-22.3-1', 'shake-22.3-1-1', 'shake-22.3-2', 'shake-22.3-2-1', 'sight-30.2', 'simple_dressing-41.3.1', 'slide-11.2', 'slide-11.2-1-1', 'smell_emission-43.3', 'snooze-40.4', 'sound_emission-43.2', 'sound_existence-47.4', 'spank-18.3', 'spatial_configuration-47.6', 'split-23.2', 'spray-9.7', 'spray-9.7-1', 'spray-9.7-1-1', 'spray-9.7-2', 'stalk-35.3', 'steal-10.5', 'stimulus_subject-30.4', 'stop-55.4', 'stop-55.4-1', 'substance_emission-43.4', 'succeed-74', 'succeed-74-1', 'succeed-74-1-1', 'succeed-74-2', 'suffocate-40.7', 'suspect-81', 'swarm-47.5.1', 'swarm-47.5.1-1', 'swarm-47.5.1-2', 'swarm-47.5.1-2-1', 'swat-18.2', 'talk-37.5', 'tape-22.4', 'tape-22.4-1', 'tell-37.2', 'throw-17.1', 'throw-17.1-1', 'throw-17.1-1-1', 'tingle-40.8.2', 'touch-20', 'touch-20-1', 'transcribe-25.4', 'transfer_mesg-37.1.1', 'transfer_mesg-37.1.1-1', 'transfer_mesg-37.1.1-1-1', 'try-61', 'turn-26.6.1', 'turn-26.6.1-1', 'urge-58', 'vehicle-51.4.1', 'vehicle-51.4.1-1', 'waltz-51.5', 'want-32.1', 'want-32.1-1', 'want-32.1-1-1', 'weather-57', 'weekend-56', 'wink-40.3.1', 'wink-40.3.1-1', 'wipe_instr-10.4.2', 'wipe_instr-10.4.2-1', 'wipe_manner-10.4.1', 'wipe_manner-10.4.1-1', 'wish-62', 'withdraw-82', 'withdraw-82-1', 'withdraw-82-2', 'withdraw-82-3']\n\nThe primary object in the lexicon is a class record, which is stored\nas an ElementTree xml object.  The class record for a given class\nidentifier is returned by the `vnclass()` method:\n\n    >>> verbnet.vnclass('remove-10.1')\n    <Element 'VNCLASS' at ...>\n\nThe `vnclass()` method also accepts \"short\" identifiers, such as '10.1':\n\n    >>> verbnet.vnclass('10.1')\n    <Element 'VNCLASS' at ...>\n\nSee the Verbnet documentation, or the Verbnet files, for information\nabout the structure of this xml.  As an example, we can retrieve a\nlist of thematic roles for a given Verbnet class:\n\n    >>> vn_31_2 = verbnet.vnclass('admire-31.2')\n    >>> for themrole in vn_31_2.findall('THEMROLES/THEMROLE'):\n    ...     print(themrole.attrib['type'], end=' ')\n    ...     for selrestr in themrole.findall('SELRESTRS/SELRESTR'):\n    ...         print('[%(Value)s%(type)s]' % selrestr.attrib, end=' ')\n    ...     print()\n    Theme\n    Experiencer [+animate]\n    Predicate\n\nThe Verbnet corpus also provides a variety of pretty printing\nfunctions that can be used to display the xml contents in a more\nconcise form.  The simplest such method is `pprint()`:\n\n    >>> print(verbnet.pprint('57'))\n    weather-57\n      Subclasses: (none)\n      Members: blow clear drizzle fog freeze gust hail howl lightning mist\n        mizzle pelt pour precipitate rain roar shower sleet snow spit spot\n        sprinkle storm swelter teem thaw thunder\n      Thematic roles:\n        * Theme[+concrete +force]\n      Frames:\n        Intransitive (Expletive Subject)\n          Example: It's raining.\n          Syntax: LEX[it] LEX[[+be]] VERB\n          Semantics:\n            * weather(during(E), Weather_type, ?Theme)\n        NP (Expletive Subject, Theme Object)\n          Example: It's raining cats and dogs.\n          Syntax: LEX[it] LEX[[+be]] VERB NP[Theme]\n          Semantics:\n            * weather(during(E), Weather_type, Theme)\n        PP (Expletive Subject, Theme-PP)\n          Example: It was pelting with rain.\n          Syntax: LEX[it[+be]] VERB PREP[with] NP[Theme]\n          Semantics:\n            * weather(during(E), Weather_type, Theme)\n\nVerbnet gives us frames that link the syntax and semantics using an example.\nThese frames are part of the corpus and we can use `frames()` to get a frame\nfor a given verbnet class.\n\n    >>> frame = verbnet.frames('57')\n    >>> frame == [{'semantics': [{'arguments': [{'value': 'during(E)', 'type': 'Event'}, {'value': 'Weather_type', 'type': 'VerbSpecific'}, {'value': '?Theme', 'type': 'ThemRole'}], 'predicate_value': 'weather'}], 'example': \"It's raining.\", 'syntax': [{'pos_tag': 'LEX', 'modifiers': {'value': 'it', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'LEX', 'modifiers': {'value': '[+be]', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'VERB', 'modifiers': {'value': '', 'synrestrs': [], 'selrestrs': []}}], 'description': {'primary': 'Intransitive', 'secondary': 'Expletive Subject'}}, {'semantics': [{'arguments': [{'value': 'during(E)', 'type': 'Event'}, {'value': 'Weather_type', 'type': 'VerbSpecific'}, {'value': 'Theme', 'type': 'ThemRole'}], 'predicate_value': 'weather'}], 'example': \"It's raining cats and dogs.\", 'syntax': [{'pos_tag': 'LEX', 'modifiers': {'value': 'it', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'LEX', 'modifiers': {'value': '[+be]', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'VERB', 'modifiers': {'value': '', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'NP', 'modifiers': {'value': 'Theme', 'synrestrs': [], 'selrestrs': []}}], 'description': {'primary': 'NP', 'secondary': 'Expletive Subject, Theme Object'}}, {'semantics': [{'arguments': [{'value': 'during(E)', 'type': 'Event'}, {'value': 'Weather_type', 'type': 'VerbSpecific'}, {'value': 'Theme', 'type': 'ThemRole'}], 'predicate_value': 'weather'}], 'example': 'It was pelting with rain.', 'syntax': [{'pos_tag': 'LEX', 'modifiers': {'value': 'it[+be]', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'VERB', 'modifiers': {'value': '', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'PREP', 'modifiers': {'value': 'with', 'synrestrs': [], 'selrestrs': []}}, {'pos_tag': 'NP', 'modifiers': {'value': 'Theme', 'synrestrs': [], 'selrestrs': []}}], 'description': {'primary': 'PP', 'secondary': 'Expletive Subject, Theme-PP'}}]\n    True\n\nVerbnet corpus lets us access thematic roles individually using `themroles()`.\n\n    >>> themroles = verbnet.themroles('57')\n    >>> themroles == [{'modifiers': [{'type': 'concrete', 'value': '+'}, {'type': 'force', 'value': '+'}], 'type': 'Theme'}]\n    True\n\nVerbnet classes may also have subclasses sharing similar syntactic and semantic properties\nwhile having differences with the superclass. The Verbnet corpus allows us to access these\nsubclasses using `subclasses()`.\n\n    >>> print(verbnet.subclasses('9.1')) #Testing for 9.1 since '57' does not have subclasses\n    ['put-9.1-1', 'put-9.1-2']\n\n\nnps_chat\n--------\n\nThe NPS Chat Corpus, Release 1.0 consists of over 10,000 posts in age-specific\nchat rooms, which have been anonymized, POS-tagged and dialogue-act tagged.\n\n    >>> print(nltk.corpus.nps_chat.words())\n    ['now', 'im', 'left', 'with', 'this', 'gay', ...]\n    >>> print(nltk.corpus.nps_chat.tagged_words())\n    [('now', 'RB'), ('im', 'PRP'), ('left', 'VBD'), ...]\n    >>> print(nltk.corpus.nps_chat.tagged_posts())\n    [[('now', 'RB'), ('im', 'PRP'), ('left', 'VBD'), ('with', 'IN'),\n    ('this', 'DT'), ('gay', 'JJ'), ('name', 'NN')], [(':P', 'UH')], ...]\n\nWe can access the XML elements corresponding to individual posts.  These elements\nhave ``class`` and ``user`` attributes that we can access using ``p.attrib['class']``\nand ``p.attrib['user']``.  They also have text content, accessed using ``p.text``.\n\n    >>> print(nltk.corpus.nps_chat.xml_posts())\n    [<Element 'Post' at 0...>, <Element 'Post' at 0...>, ...]\n    >>> posts = nltk.corpus.nps_chat.xml_posts()\n    >>> sorted(nltk.FreqDist(p.attrib['class'] for p in posts).keys())\n    ['Accept', 'Bye', 'Clarify', 'Continuer', 'Emotion', 'Emphasis',\n    'Greet', 'Other', 'Reject', 'Statement', 'System', 'nAnswer',\n    'whQuestion', 'yAnswer', 'ynQuestion']\n    >>> posts[0].text\n    'now im left with this gay name'\n\nIn addition to the above methods for accessing tagged text, we can navigate\nthe XML structure directly, as follows:\n\n    >>> tokens = posts[0].findall('terminals/t')\n    >>> [t.attrib['pos'] + \"/\" + t.attrib['word'] for t in tokens]\n    ['RB/now', 'PRP/im', 'VBD/left', 'IN/with', 'DT/this', 'JJ/gay', 'NN/name']\n\nmultext_east\n------------\n\nThe Multext-East Corpus consists of POS-tagged versions of George Orwell's book\n1984 in 12 languages: English, Czech, Hungarian, Macedonian, Slovenian, Serbian,\nSlovak, Romanian, Estonian, Farsi, Bulgarian and Polish.\nThe corpus can be accessed using the usual methods for tagged corpora. The tagset\ncan be transformed from the Multext-East specific MSD tags to the Universal tagset\nusing the \"tagset\" parameter of all functions returning tagged parts of the corpus.\n\n    >>> print(nltk.corpus.multext_east.words(\"oana-en.xml\"))\n    ['It', 'was', 'a', 'bright', ...]\n    >>> print(nltk.corpus.multext_east.tagged_words(\"oana-en.xml\"))\n    [('It', '#Pp3ns'), ('was', '#Vmis3s'), ('a', '#Di'), ...]\n    >>> print(nltk.corpus.multext_east.tagged_sents(\"oana-en.xml\", \"universal\"))\n    [[('It', 'PRON'), ('was', 'VERB'), ('a', 'DET'), ...]\n\n\n\n---------------------\nCorpus Reader Classes\n---------------------\n\nNLTK's *corpus reader* classes are used to access the contents of a\ndiverse set of corpora.  Each corpus reader class is specialized to\nhandle a specific corpus format.  Examples include the\n`PlaintextCorpusReader`, which handles corpora that consist of a set\nof unannotated text files, and the `BracketParseCorpusReader`, which\nhandles corpora that consist of files containing\nparenthesis-delineated parse trees.\n\nAutomatically Created Corpus Reader Instances\n=============================================\n\nWhen the `nltk.corpus` module is imported, it automatically creates a\nset of corpus reader instances that can be used to access the corpora\nin the NLTK data distribution.  Here is a small sample of those\ncorpus reader instances:\n\n    >>> import nltk\n    >>> nltk.corpus.brown\n    <CategorizedTaggedCorpusReader ...>\n    >>> nltk.corpus.treebank\n    <BracketParseCorpusReader ...>\n    >>> nltk.corpus.names\n    <WordListCorpusReader ...>\n    >>> nltk.corpus.genesis\n    <PlaintextCorpusReader ...>\n    >>> nltk.corpus.inaugural\n    <PlaintextCorpusReader ...>\n\nThis sample illustrates that different corpus reader classes are used\nto read different corpora; but that the same corpus reader class may\nbe used for more than one corpus (e.g., ``genesis`` and ``inaugural``).\n\nCreating New Corpus Reader Instances\n====================================\n\nAlthough the `nltk.corpus` module automatically creates corpus reader\ninstances for the corpora in the NLTK data distribution, you may\nsometimes need to create your own corpus reader.  In particular, you\nwould need to create your own corpus reader if you want...\n\n- To access a corpus that is not included in the NLTK data\n  distribution.\n\n- To access a full copy of a corpus for which the NLTK data\n  distribution only provides a sample.\n\n- To access a corpus using a customized corpus reader (e.g., with\n  a customized tokenizer).\n\nTo create a new corpus reader, you will first need to look up the\nsignature for that corpus reader's constructor.  Different corpus\nreaders have different constructor signatures, but most of the\nconstructor signatures have the basic form::\n\n    SomeCorpusReader(root, files, ...options...)\n\nWhere ``root`` is an absolute path to the directory containing the\ncorpus data files; ``files`` is either a list of file names (relative\nto ``root``) or a regexp specifying which files should be included;\nand ``options`` are additional reader-specific options.  For example,\nwe can create a customized corpus reader for the genesis corpus that\nuses a different sentence tokenizer as follows:\n\n    >>> # Find the directory where the corpus lives.\n    >>> genesis_dir = nltk.data.find('corpora/genesis')\n    >>> # Create our custom sentence tokenizer.\n    >>> my_sent_tokenizer = nltk.RegexpTokenizer('[^.!?]+')\n    >>> # Create the new corpus reader object.\n    >>> my_genesis = nltk.corpus.PlaintextCorpusReader(\n    ...     genesis_dir, r'.*\\.txt', sent_tokenizer=my_sent_tokenizer)\n    >>> # Use the new corpus reader object.\n    >>> print(my_genesis.sents('english-kjv.txt')[0])\n    ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',\n     'and', 'the', 'earth']\n\nIf you wish to read your own plaintext corpus, which is stored in the\ndirectory '/usr/share/some-corpus', then you can create a corpus\nreader for it with::\n\n    >>> my_corpus = nltk.corpus.PlaintextCorpusReader(\n    ...     '/usr/share/some-corpus', r'.*\\.txt') # doctest: +SKIP\n\nFor a complete list of corpus reader subclasses, see the API\ndocumentation for `nltk.corpus.reader`.\n\nCorpus Types\n============\n\nCorpora vary widely in the types of content they include.  This is\nreflected in the fact that the base class `CorpusReader` only defines\na few general-purpose methods for listing and accessing the files that\nmake up a corpus.  It is up to the subclasses to define *data access\nmethods* that provide access to the information in the corpus.\nHowever, corpus reader subclasses should be consistent in their\ndefinitions of these data access methods wherever possible.\n\nAt a high level, corpora can be divided into three basic types:\n\n- A *token corpus* contains information about specific occurrences of\n  language use (or linguistic tokens), such as dialogues or written\n  texts.  Examples of token corpora are collections of written text\n  and collections of speech.\n\n- A *type corpus*, or *lexicon*, contains information about a coherent\n  set of lexical items (or linguistic types).  Examples of lexicons\n  are dictionaries and word lists.\n\n- A *language description corpus* contains information about a set of\n  non-lexical linguistic constructs, such as grammar rules.\n\nHowever, many individual corpora blur the distinctions between these\ntypes.  For example, corpora that are primarily lexicons may include\ntoken data in the form of example sentences; and corpora that are\nprimarily token corpora may be accompanied by one or more word lists\nor other lexical data sets.\n\nBecause corpora vary so widely in their information content, we have\ndecided that it would not be wise to use separate corpus reader base\nclasses for different corpus types.  Instead, we simply try to make\nthe corpus readers consistent wherever possible, but let them differ\nwhere the underlying data itself differs.\n\nCommon Corpus Reader Methods\n============================\n\nAs mentioned above, there are only a handful of methods that all\ncorpus readers are guaranteed to implement.  These methods provide\naccess to the files that contain the corpus data.  Every corpus is\nassumed to consist of one or more files, all located in a common root\ndirectory (or in subdirectories of that root directory).  The absolute\npath to the root directory is stored in the ``root`` property:\n\n    >>> import os\n    >>> str(nltk.corpus.genesis.root).replace(os.path.sep,'/')\n    '.../nltk_data/corpora/genesis'\n\nEach file within the corpus is identified by a platform-independent\nidentifier, which is basically a path string that uses ``/`` as the\npath separator.  I.e., this identifier can be converted to a relative\npath as follows:\n\n    >>> some_corpus_file_id = nltk.corpus.reuters.fileids()[0]\n    >>> import os.path\n    >>> os.path.normpath(some_corpus_file_id).replace(os.path.sep,'/')\n    'test/14826'\n\nTo get a list of all data files that make up a corpus, use the\n``fileids()`` method.  In some corpora, these files will not all contain\nthe same type of data; for example, for the ``nltk.corpus.timit``\ncorpus, ``fileids()`` will return a list including text files, word\nsegmentation files, phonetic transcription files, sound files, and\nmetadata files.  For corpora with diverse file types, the ``fileids()``\nmethod will often take one or more optional arguments, which can be\nused to get a list of the files with a specific file type:\n\n    >>> nltk.corpus.timit.fileids()\n    ['dr1-fvmh0/sa1.phn', 'dr1-fvmh0/sa1.txt', 'dr1-fvmh0/sa1.wav', ...]\n    >>> nltk.corpus.timit.fileids('phn')\n    ['dr1-fvmh0/sa1.phn', 'dr1-fvmh0/sa2.phn', 'dr1-fvmh0/si1466.phn', ...]\n\nIn some corpora, the files are divided into distinct categories.  For\nthese corpora, the ``fileids()`` method takes an optional argument,\nwhich can be used to get a list of the files within a specific category:\n\n    >>> nltk.corpus.brown.fileids('hobbies')\n    ['ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', ...]\n\nThe ``abspath()`` method can be used to find the absolute path to a\ncorpus file, given its file identifier:\n\n    >>> str(nltk.corpus.brown.abspath('ce06')).replace(os.path.sep,'/')\n    '.../corpora/brown/ce06'\n\nThe ``abspaths()`` method can be used to find the absolute paths for\none corpus file, a list of corpus files, or (if no fileids are specified),\nall corpus files.\n\nThis method is mainly useful as a helper method when defining corpus\ndata access methods, since data access methods can usually be called\nwith a string argument (to get a view for a specific file), with a\nlist argument (to get a view for a specific list of files), or with no\nargument (to get a view for the whole corpus).\n\nData Access Methods\n===================\n\nIndividual corpus reader subclasses typically extend this basic set of\nfile-access methods with one or more *data access methods*, which provide\neasy access to the data contained in the corpus.  The signatures for\ndata access methods often have the basic form::\n\n    corpus_reader.some_data access(fileids=None, ...options...)\n\nWhere ``fileids`` can be a single file identifier string (to get a view\nfor a specific file); a list of file identifier strings (to get a view\nfor a specific list of files); or None (to get a view for the entire\ncorpus).  Some of the common data access methods, and their return\ntypes, are:\n\n  - I{corpus}.words(): list of str\n  - I{corpus}.sents(): list of (list of str)\n  - I{corpus}.paras(): list of (list of (list of str))\n  - I{corpus}.tagged_words(): list of (str,str) tuple\n  - I{corpus}.tagged_sents(): list of (list of (str,str))\n  - I{corpus}.tagged_paras(): list of (list of (list of (str,str)))\n  - I{corpus}.chunked_sents(): list of (Tree w/ (str,str) leaves)\n  - I{corpus}.parsed_sents(): list of (Tree with str leaves)\n  - I{corpus}.parsed_paras(): list of (list of (Tree with str leaves))\n  - I{corpus}.xml(): A single xml ElementTree\n  - I{corpus}.raw(): str (unprocessed corpus contents)\n\nFor example, the `words()` method is supported by many different\ncorpora, and returns a flat list of word strings:\n\n    >>> nltk.corpus.brown.words()\n    ['The', 'Fulton', 'County', 'Grand', 'Jury', ...]\n    >>> nltk.corpus.treebank.words()\n    ['Pierre', 'Vinken', ',', '61', 'years', 'old', ...]\n    >>> nltk.corpus.conll2002.words()\n    ['Sao', 'Paulo', '(', 'Brasil', ')', ',', '23', ...]\n    >>> nltk.corpus.genesis.words()\n    ['In', 'the', 'beginning', 'God', 'created', ...]\n\nOn the other hand, the `tagged_words()` method is only supported by\ncorpora that include part-of-speech annotations:\n\n    >>> nltk.corpus.brown.tagged_words()\n    [('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n    >>> nltk.corpus.treebank.tagged_words()\n    [('Pierre', 'NNP'), ('Vinken', 'NNP'), ...]\n    >>> nltk.corpus.conll2002.tagged_words()\n    [('Sao', 'NC'), ('Paulo', 'VMI'), ('(', 'Fpa'), ...]\n    >>> nltk.corpus.genesis.tagged_words()\n    Traceback (most recent call last):\n      ...\n    AttributeError: 'PlaintextCorpusReader' object has no attribute 'tagged_words'\n\nAlthough most corpus readers use file identifiers to index their\ncontent, some corpora use different identifiers instead.  For example,\nthe data access methods for the ``timit`` corpus uses *utterance\nidentifiers* to select which corpus items should be returned:\n\n    >>> nltk.corpus.timit.utteranceids()\n    ['dr1-fvmh0/sa1', 'dr1-fvmh0/sa2', 'dr1-fvmh0/si1466', ...]\n    >>> nltk.corpus.timit.words('dr1-fvmh0/sa2')\n    [\"don't\", 'ask', 'me', 'to', 'carry', 'an', 'oily', 'rag', 'like', 'that']\n\nAttempting to call ``timit``\\ 's data access methods with a file\nidentifier will result in an exception:\n\n    >>> nltk.corpus.timit.fileids()\n    ['dr1-fvmh0/sa1.phn', 'dr1-fvmh0/sa1.txt', 'dr1-fvmh0/sa1.wav', ...]\n    >>> nltk.corpus.timit.words('dr1-fvmh0/sa1.txt') # doctest: +SKIP\n    Traceback (most recent call last):\n      ...\n    IOError: No such file or directory: '.../dr1-fvmh0/sa1.txt.wrd'\n\nAs another example, the ``propbank`` corpus defines the ``roleset()``\nmethod, which expects a roleset identifier, not a file identifier:\n\n    >>> roleset = nltk.corpus.propbank.roleset('eat.01')\n    >>> from xml.etree import ElementTree as ET\n    >>> print(ET.tostring(roleset).decode('utf8'))\n    <roleset id=\"eat.01\" name=\"consume\" vncls=\"39.1\">\n      <roles>\n        <role descr=\"consumer, eater\" n=\"0\">...</role>...\n      </roles>...\n    </roleset>...\n\nStream Backed Corpus Views\n==========================\nAn important feature of NLTK's corpus readers is that many of them\naccess the underlying data files using \"corpus views.\"  A *corpus\nview* is an object that acts like a simple data structure (such as a\nlist), but does not store the data elements in memory; instead, data\nelements are read from the underlying data files on an as-needed\nbasis.\n\nBy only loading items from the file on an as-needed basis, corpus\nviews maintain both memory efficiency and responsiveness.  The memory\nefficiency of corpus readers is important because some corpora contain\nvery large amounts of data, and storing the entire data set in memory\ncould overwhelm many machines.  The responsiveness is important when\nexperimenting with corpora in interactive sessions and in in-class\ndemonstrations.\n\nThe most common corpus view is the `StreamBackedCorpusView`, which\nacts as a read-only list of tokens.  Two additional corpus view\nclasses, `ConcatenatedCorpusView` and `LazySubsequence`, make it\npossible to create concatenations and take slices of\n`StreamBackedCorpusView` objects without actually storing the\nresulting list-like object's elements in memory.\n\nIn the future, we may add additional corpus views that act like other\nbasic data structures, such as dictionaries.\n\nWriting New Corpus Readers\n==========================\n\nIn order to add support for new corpus formats, it is necessary to\ndefine new corpus reader classes.  For many corpus formats, writing\nnew corpus readers is relatively straight-forward.  In this section,\nwe'll describe what's involved in creating a new corpus reader.  If\nyou do create a new corpus reader, we encourage you to contribute it\nback to the NLTK project.\n\nDon't Reinvent the Wheel\n------------------------\nBefore you start writing a new corpus reader, you should check to be\nsure that the desired format can't be read using an existing corpus\nreader with appropriate constructor arguments.  For example, although\nthe `TaggedCorpusReader` assumes that words and tags are separated by\n``/`` characters by default, an alternative tag-separation character\ncan be specified via the ``sep`` constructor argument.  You should\nalso check whether the new corpus format can be handled by subclassing\nan existing corpus reader, and tweaking a few methods or variables.\n\nDesign\n------\nIf you decide to write a new corpus reader from scratch, then you\nshould first decide which data access methods you want the reader to\nprovide, and what their signatures should be.  You should look at\nexisting corpus readers that process corpora with similar data\ncontents, and try to be consistent with those corpus readers whenever\npossible.\n\nYou should also consider what sets of identifiers are appropriate for\nthe corpus format.  Where it's practical, file identifiers should be\nused.  However, for some corpora, it may make sense to use additional\nsets of identifiers.  Each set of identifiers should have a distinct\nname (e.g., fileids, utteranceids, rolesets); and you should be consistent\nin using that name to refer to that identifier.  Do not use parameter\nnames like ``id``, which leave it unclear what type of identifier is\nrequired.\n\nOnce you've decided what data access methods and identifiers are\nappropriate for your corpus, you should decide if there are any\ncustomizable parameters that you'd like the corpus reader to handle.\nThese parameters make it possible to use a single corpus reader to\nhandle a wider variety of corpora.  The ``sep`` argument for\n`TaggedCorpusReader`, mentioned above, is an example of a customizable\ncorpus reader parameter.\n\nImplementation\n--------------\n\nConstructor\n~~~~~~~~~~~\nIf your corpus reader implements any customizable parameters, then\nyou'll need to override the constructor.  Typically, the new\nconstructor will first call its base class's constructor, and then\nstore the customizable parameters.  For example, the\n`ConllChunkCorpusReader`\\ 's constructor is defined as follows:\n\n    def __init__(self, root, fileids, chunk_types, encoding='utf8',\n                 tagset=None, separator=None):\n        ConllCorpusReader.__init__(\n                self, root, fileids, ('words', 'pos', 'chunk'),\n                chunk_types=chunk_types, encoding=encoding,\n                tagset=tagset, separator=separator)\n\nIf your corpus reader does not implement any customization parameters,\nthen you can often just inherit the base class's constructor.\n\nData Access Methods\n~~~~~~~~~~~~~~~~~~~\n\nThe most common type of data access method takes an argument\nidentifying which files to access, and returns a view covering those\nfiles.  This argument may be a single file identifier string (to get a\nview for a specific file); a list of file identifier strings (to get a\nview for a specific list of files); or None (to get a view for the\nentire corpus).  The method's implementation converts this argument to\na list of path names using the `abspaths()` method, which handles all\nthree value types (string, list, and None):\n\n    >>> print(str(nltk.corpus.brown.abspaths()).replace('\\\\\\\\','/'))\n    [FileSystemPathPointer('.../corpora/brown/ca01'),\n     FileSystemPathPointer('.../corpora/brown/ca02'), ...]\n    >>> print(str(nltk.corpus.brown.abspaths('ce06')).replace('\\\\\\\\','/'))\n    [FileSystemPathPointer('.../corpora/brown/ce06')]\n    >>> print(str(nltk.corpus.brown.abspaths(['ce06', 'ce07'])).replace('\\\\\\\\','/'))\n    [FileSystemPathPointer('.../corpora/brown/ce06'),\n     FileSystemPathPointer('.../corpora/brown/ce07')]\n\nAn example of this type of method is the `words()` method, defined by\nthe `PlaintextCorpusReader` as follows:\n\n    >>> def words(self, fileids=None):\n    ...     return concat([self.CorpusView(fileid, self._read_word_block)\n    ...                    for fileid in self.abspaths(fileids)])\n\nThis method first uses `abspaths()` to convert ``fileids`` to a list of\nabsolute paths.  It then creates a corpus view for each file, using\nthe `PlaintextCorpusReader._read_word_block()` method to read elements\nfrom the data file (see the discussion of corpus views below).\nFinally, it combines these corpus views using the\n`nltk.corpus.reader.util.concat()` function.\n\nWhen writing a corpus reader for a corpus that is never expected to be\nvery large, it can sometimes be appropriate to read the files\ndirectly, rather than using a corpus view.  For example, the\n`WordListCorpusView` class defines its `words()` method as follows:\n\n    >>> def words(self, fileids=None):\n    ...     return concat([[w for w in open(fileid).read().split('\\n') if w]\n    ...                    for fileid in self.abspaths(fileids)])\n\n(This is usually more appropriate for lexicons than for token corpora.)\n\nIf the type of data returned by a data access method is one for which\nNLTK has a conventional representation (e.g., words, tagged words, and\nparse trees), then you should use that representation.  Otherwise, you\nmay find it necessary to define your own representation.  For data\nstructures that are relatively corpus-specific, it's usually best to\ndefine new classes for these elements.  For example, the ``propbank``\ncorpus defines the `PropbankInstance` class to store the semantic role\nlabeling instances described by the corpus; and the ``ppattach``\ncorpus defines the `PPAttachment` class to store the prepositional\nattachment instances described by the corpus.\n\nCorpus Views\n~~~~~~~~~~~~\n.. (Much of the content for this section is taken from the\n   StreamBackedCorpusView docstring.)\n\nThe heart of a `StreamBackedCorpusView` is its *block reader*\nfunction, which reads zero or more tokens from a stream, and returns\nthem as a list.  A very simple example of a block reader is:\n\n    >>> def simple_block_reader(stream):\n    ...     return stream.readline().split()\n\nThis simple block reader reads a single line at a time, and returns a\nsingle token (consisting of a string) for each whitespace-separated\nsubstring on the line.  A `StreamBackedCorpusView` built from this\nblock reader will act like a read-only list of all the\nwhitespace-separated tokens in an underlying file.\n\nWhen deciding how to define the block reader for a given corpus,\ncareful consideration should be given to the size of blocks handled by\nthe block reader.  Smaller block sizes will increase the memory\nrequirements of the corpus view's internal data structures (by 2\nintegers per block).  On the other hand, larger block sizes may\ndecrease performance for random access to the corpus.  (But note that\nlarger block sizes will *not* decrease performance for iteration.)\n\nInternally, the `StreamBackedCorpusView` class maintains a partial\nmapping from token index to file position, with one entry per block.\nWhen a token with a given index *i* is requested, the corpus view\nconstructs it as follows:\n\n1. First, it searches the toknum/filepos mapping for the token index\n   closest to (but less than or equal to) *i*.\n\n2. Then, starting at the file position corresponding to that index, it\n   reads one block at a time using the block reader until it reaches\n   the requested token.\n\nThe toknum/filepos mapping is created lazily: it is initially empty,\nbut every time a new block is read, the block's initial token is added\nto the mapping.  (Thus, the toknum/filepos map has one entry per\nblock.)\n\nYou can create your own corpus view in one of two ways:\n\n1. Call the `StreamBackedCorpusView` constructor, and provide your\n   block reader function via the ``block_reader`` argument.\n\n2. Subclass `StreamBackedCorpusView`, and override the\n   `read_block()` method.\n\nThe first option is usually easier, but the second option can allow\nyou to write a single `read_block` method whose behavior can be\ncustomized by different parameters to the subclass's constructor.  For\nan example of this design pattern, see the `TaggedCorpusView` class,\nwhich is used by `TaggedCorpusView`.\n\n----------------\nRegression Tests\n----------------\n\nThe following helper functions are used to create and then delete\ntesting corpora that are stored in temporary directories.  These\ntesting corpora are used to make sure the readers work correctly.\n\n    >>> import tempfile, os.path, textwrap\n    >>> def make_testcorpus(ext='', **fileids):\n    ...     root = tempfile.mkdtemp()\n    ...     for fileid, contents in fileids.items():\n    ...         fileid += ext\n    ...         f = open(os.path.join(root, fileid), 'w')\n    ...         f.write(textwrap.dedent(contents))\n    ...         f.close()\n    ...     return root\n    >>> def del_testcorpus(root):\n    ...     for fileid in os.listdir(root):\n    ...         os.remove(os.path.join(root, fileid))\n    ...     os.rmdir(root)\n\nPlaintext Corpus Reader\n=======================\nThe plaintext corpus reader is used to access corpora that consist of\nunprocessed plaintext data.  It assumes that paragraph breaks are\nindicated by blank lines.  Sentences and words can be tokenized using\nthe default tokenizers, or by custom tokenizers specified as\nparameters to the constructor.\n\n    >>> root = make_testcorpus(ext='.txt',\n    ...     a=\"\"\"\\\n    ...     This is the first sentence.  Here is another\n    ...     sentence!  And here's a third sentence.\n    ...\n    ...     This is the second paragraph.  Tokenization is currently\n    ...     fairly simple, so the period in Mr. gets tokenized.\n    ...     \"\"\",\n    ...     b=\"\"\"This is the second file.\"\"\")\n\n    >>> from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n\nThe list of documents can be specified explicitly, or implicitly (using a\nregexp).  The ``ext`` argument specifies a file extension.\n\n    >>> corpus = PlaintextCorpusReader(root, ['a.txt', 'b.txt'])\n    >>> corpus.fileids()\n    ['a.txt', 'b.txt']\n    >>> corpus = PlaintextCorpusReader(root, r'.*\\.txt')\n    >>> corpus.fileids()\n    ['a.txt', 'b.txt']\n\nThe directory containing the corpus is corpus.root:\n\n    >>> str(corpus.root) == str(root)\n    True\n\nWe can get a list of words, or the raw string:\n\n    >>> corpus.words()\n    ['This', 'is', 'the', 'first', 'sentence', '.', ...]\n    >>> corpus.raw()[:40]\n    'This is the first sentence.  Here is ano'\n\nCheck that reading individual documents works, and reading all documents at\nonce works:\n\n    >>> len(corpus.words()), [len(corpus.words(d)) for d in corpus.fileids()]\n    (46, [40, 6])\n    >>> corpus.words('a.txt')\n    ['This', 'is', 'the', 'first', 'sentence', '.', ...]\n    >>> corpus.words('b.txt')\n    ['This', 'is', 'the', 'second', 'file', '.']\n    >>> corpus.words()[:4], corpus.words()[-4:]\n    (['This', 'is', 'the', 'first'], ['the', 'second', 'file', '.'])\n\nWe're done with the test corpus:\n\n    >>> del_testcorpus(root)\n\nTest the plaintext corpora that come with nltk:\n\n    >>> from nltk.corpus import abc, genesis, inaugural\n    >>> from nltk.corpus import state_union, webtext\n    >>> for corpus in (abc, genesis, inaugural, state_union,\n    ...                webtext):\n    ...     print(str(corpus).replace('\\\\\\\\','/'))\n    ...     print('  ', repr(corpus.fileids())[:60])\n    ...     print('  ', repr(corpus.words()[:10])[:60])\n    <PlaintextCorpusReader in '.../nltk_data/corpora/ab...'>\n       ['rural.txt', 'science.txt']\n       ['PM', 'denies', 'knowledge', 'of', 'AWB', ...\n    <PlaintextCorpusReader in '.../nltk_data/corpora/genesi...'>\n       ['english-kjv.txt', 'english-web.txt', 'finnish.txt', ...\n       ['In', 'the', 'beginning', 'God', 'created', 'the', ...\n    <PlaintextCorpusReader in '.../nltk_data/corpora/inaugura...'>\n       ['1789-Washington.txt', '1793-Washington.txt', ...\n       ['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...\n    <PlaintextCorpusReader in '.../nltk_data/corpora/state_unio...'>\n       ['1945-Truman.txt', '1946-Truman.txt', ...\n       ['PRESIDENT', 'HARRY', 'S', '.', 'TRUMAN', \"'\", ...\n    <PlaintextCorpusReader in '.../nltk_data/corpora/webtex...'>\n       ['firefox.txt', 'grail.txt', 'overheard.txt', ...\n       ['Cookie', 'Manager', ':', '\"', 'Don', \"'\", 't', ...\n\n\nTagged Corpus Reader\n====================\nThe Tagged Corpus reader can give us words, sentences, and paragraphs,\neach tagged or untagged.  All of the read methods can take one item\n(in which case they return the contents of that file) or a list of\ndocuments (in which case they concatenate the contents of those files).\nBy default, they apply to all documents in the corpus.\n\n    >>> root = make_testcorpus(\n    ...     a=\"\"\"\\\n    ...     This/det is/verb the/det first/adj sentence/noun ./punc\n    ...     Here/det  is/verb  another/adj    sentence/noun ./punc\n    ...     Note/verb that/comp you/pron can/verb use/verb \\\n    ...           any/noun tag/noun set/noun\n    ...\n    ...     This/det is/verb the/det second/adj paragraph/noun ./punc\n    ...     word/n without/adj a/det tag/noun :/: hello ./punc\n    ...     \"\"\",\n    ...     b=\"\"\"\\\n    ...     This/det is/verb the/det second/adj file/noun ./punc\n    ...     \"\"\")\n\n    >>> from nltk.corpus.reader.tagged import TaggedCorpusReader\n    >>> corpus = TaggedCorpusReader(root, list('ab'))\n    >>> corpus.fileids()\n    ['a', 'b']\n    >>> str(corpus.root) == str(root)\n    True\n    >>> corpus.words()\n    ['This', 'is', 'the', 'first', 'sentence', '.', ...]\n    >>> corpus.sents()\n    [['This', 'is', 'the', 'first', ...], ['Here', 'is', 'another'...], ...]\n    >>> corpus.paras()\n    [[['This', ...], ['Here', ...], ...], [['This', ...], ...], ...]\n    >>> corpus.tagged_words()\n    [('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ...]\n    >>> corpus.tagged_sents()\n    [[('This', 'DET'), ('is', 'VERB'), ...], [('Here', 'DET'), ...], ...]\n    >>> corpus.tagged_paras()\n    [[[('This', 'DET'), ...], ...], [[('This', 'DET'), ...], ...], ...]\n    >>> corpus.raw()[:40]\n    'This/det is/verb the/det first/adj sente'\n    >>> len(corpus.words()), [len(corpus.words(d)) for d in corpus.fileids()]\n    (38, [32, 6])\n    >>> len(corpus.sents()), [len(corpus.sents(d)) for d in corpus.fileids()]\n    (6, [5, 1])\n    >>> len(corpus.paras()), [len(corpus.paras(d)) for d in corpus.fileids()]\n    (3, [2, 1])\n    >>> print(corpus.words('a'))\n    ['This', 'is', 'the', 'first', 'sentence', '.', ...]\n    >>> print(corpus.words('b'))\n    ['This', 'is', 'the', 'second', 'file', '.']\n    >>> del_testcorpus(root)\n\nThe Brown Corpus uses the tagged corpus reader:\n\n    >>> from nltk.corpus import brown\n    >>> brown.fileids()\n    ['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', ...]\n    >>> brown.categories()\n    ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor',\n    'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n    >>> print(repr(brown.root).replace('\\\\\\\\','/'))\n    FileSystemPathPointer('.../corpora/brown')\n    >>> brown.words()\n    ['The', 'Fulton', 'County', 'Grand', 'Jury', ...]\n    >>> brown.sents()\n    [['The', 'Fulton', 'County', 'Grand', ...], ...]\n    >>> brown.paras()\n    [[['The', 'Fulton', 'County', ...]], [['The', 'jury', ...]], ...]\n    >>> brown.tagged_words()\n    [('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n    >>> brown.tagged_sents()\n    [[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ...], ...]\n    >>> brown.tagged_paras()\n    [[[('The', 'AT'), ...]], [[('The', 'AT'), ...]], ...]\n\nVerbnet Corpus Reader\n=====================\n\nMake sure we're picking up the right number of elements:\n\n    >>> from nltk.corpus import verbnet\n    >>> len(verbnet.lemmas())\n    3621\n    >>> len(verbnet.wordnetids())\n    4953\n    >>> len(verbnet.classids())\n    429\n\nSelecting classids based on various selectors:\n\n    >>> verbnet.classids(lemma='take')\n    ['bring-11.3', 'characterize-29.2', 'convert-26.6.2', 'cost-54.2',\n    'fit-54.3', 'performance-26.7-2', 'steal-10.5']\n    >>> verbnet.classids(wordnetid='lead%2:38:01')\n    ['accompany-51.7']\n    >>> verbnet.classids(fileid='approve-77.xml')\n    ['approve-77']\n    >>> verbnet.classids(classid='admire-31.2') # subclasses\n    ['admire-31.2-1']\n\nvnclass() accepts filenames, long ids, and short ids:\n\n    >>> a = ElementTree.tostring(verbnet.vnclass('admire-31.2.xml'))\n    >>> b = ElementTree.tostring(verbnet.vnclass('admire-31.2'))\n    >>> c = ElementTree.tostring(verbnet.vnclass('31.2'))\n    >>> a == b == c\n    True\n\nfileids() can be used to get files based on verbnet class ids:\n\n    >>> verbnet.fileids('admire-31.2')\n    ['admire-31.2.xml']\n    >>> verbnet.fileids(['admire-31.2', 'obtain-13.5.2'])\n    ['admire-31.2.xml', 'obtain-13.5.2.xml']\n    >>> verbnet.fileids('badidentifier')\n    Traceback (most recent call last):\n      . . .\n    ValueError: vnclass identifier 'badidentifier' not found\n\nlongid() and shortid() can be used to convert identifiers:\n\n    >>> verbnet.longid('31.2')\n    'admire-31.2'\n    >>> verbnet.longid('admire-31.2')\n    'admire-31.2'\n    >>> verbnet.shortid('31.2')\n    '31.2'\n    >>> verbnet.shortid('admire-31.2')\n    '31.2'\n    >>> verbnet.longid('badidentifier')\n    Traceback (most recent call last):\n      . . .\n    ValueError: vnclass identifier 'badidentifier' not found\n    >>> verbnet.shortid('badidentifier')\n    Traceback (most recent call last):\n      . . .\n    ValueError: vnclass identifier 'badidentifier' not found\n\nCorpus View Regression Tests\n============================\n\nSelect some corpus files to play with:\n\n    >>> import nltk.data\n    >>> # A very short file (160 chars):\n    >>> f1 = nltk.data.find('corpora/inaugural/README')\n    >>> # A relatively short file (791 chars):\n    >>> f2 = nltk.data.find('corpora/inaugural/1793-Washington.txt')\n    >>> # A longer file (32k chars):\n    >>> f3 = nltk.data.find('corpora/inaugural/1909-Taft.txt')\n    >>> fileids = [f1, f2, f3]\n\n\nConcatenation\n-------------\nCheck that concatenation works as intended.\n\n    >>> from nltk.corpus.reader.util import *\n\n    >>> c1 = StreamBackedCorpusView(f1, read_whitespace_block, encoding='utf-8')\n    >>> c2 = StreamBackedCorpusView(f2, read_whitespace_block, encoding='utf-8')\n    >>> c3 = StreamBackedCorpusView(f3, read_whitespace_block, encoding='utf-8')\n    >>> c123 = c1+c2+c3\n    >>> print(c123)\n    ['C-Span', 'Inaugural', 'Address', 'Corpus', 'US', ...]\n\n    >>> l1 = f1.open(encoding='utf-8').read().split()\n    >>> l2 = f2.open(encoding='utf-8').read().split()\n    >>> l3 = f3.open(encoding='utf-8').read().split()\n    >>> l123 = l1+l2+l3\n\n    >>> list(c123) == l123\n    True\n\n    >>> (c1+c2+c3)[100] == l123[100]\n    True\n\nSlicing\n-------\nFirst, do some tests with fairly small slices.  These will all\ngenerate tuple values.\n\n    >>> from nltk.util import LazySubsequence\n    >>> c1 = StreamBackedCorpusView(f1, read_whitespace_block, encoding='utf-8')\n    >>> l1 = f1.open(encoding='utf-8').read().split()\n    >>> print(len(c1))\n    21\n    >>> len(c1) < LazySubsequence.MIN_SIZE\n    True\n\nChoose a list of indices, based on the length, that covers the\nimportant corner cases:\n\n    >>> indices = [-60, -30, -22, -21, -20, -1,\n    ...            0, 1, 10, 20, 21, 22, 30, 60]\n\nTest slicing with explicit start & stop value:\n\n    >>> for s in indices:\n    ...     for e in indices:\n    ...         assert list(c1[s:e]) == l1[s:e]\n\nTest slicing with stop=None:\n\n    >>> for s in indices:\n    ...     assert list(c1[s:]) == l1[s:]\n\nTest slicing with start=None:\n\n    >>> for e in indices:\n    ...     assert list(c1[:e]) == l1[:e]\n\nTest slicing with start=stop=None:\n\n    >>> list(c1[:]) == list(l1[:])\n    True\n\nNext, we'll do some tests with much longer slices.  These will\ngenerate LazySubsequence objects.\n\n    >>> c3 = StreamBackedCorpusView(f3, read_whitespace_block, encoding='utf-8')\n    >>> l3 = f3.open(encoding='utf-8').read().split()\n    >>> print(len(c3))\n    5430\n    >>> len(c3) > LazySubsequence.MIN_SIZE*2\n    True\n\nChoose a list of indices, based on the length, that covers the\nimportant corner cases:\n\n    >>> indices = [-12000, -6000, -5431, -5430, -5429, -3000, -200, -1,\n    ...            0, 1, 200, 3000, 5000, 5429, 5430, 5431, 6000, 12000]\n\nTest slicing with explicit start & stop value:\n\n    >>> for s in indices:\n    ...     for e in indices:\n    ...         assert list(c3[s:e]) == l3[s:e]\n\nTest slicing with stop=None:\n\n    >>> for s in indices:\n    ...     assert list(c3[s:]) == l3[s:]\n\nTest slicing with start=None:\n\n    >>> for e in indices:\n    ...     assert list(c3[:e]) == l3[:e]\n\nTest slicing with start=stop=None:\n\n    >>> list(c3[:]) == list(l3[:])\n    True\n\nMultiple Iterators\n------------------\nIf multiple iterators are created for the same corpus view, their\niteration can be interleaved:\n\n    >>> c3 = StreamBackedCorpusView(f3, read_whitespace_block)\n    >>> iterators = [c3.iterate_from(n) for n in [0,15,30,45]]\n    >>> for i in range(15):\n    ...     for iterator in iterators:\n    ...         print('%-15s' % next(iterator), end=' ')\n    ...     print()\n    My              a               duties          in\n    fellow          heavy           of              a\n    citizens:       weight          the             proper\n    Anyone          of              office          sense\n    who             responsibility. upon            of\n    has             If              which           the\n    taken           not,            he              obligation\n    the             he              is              which\n    oath            has             about           the\n    I               no              to              oath\n    have            conception      enter,          imposes.\n    just            of              or              The\n    taken           the             he              office\n    must            powers          is              of\n    feel            and             lacking         an\n\nSeekableUnicodeStreamReader\n===========================\n\nThe file-like objects provided by the ``codecs`` module unfortunately\nsuffer from a bug that prevents them from working correctly with\ncorpus view objects.  In particular, although the expose ``seek()``\nand ``tell()`` methods, those methods do not exhibit the expected\nbehavior, because they are not synchronized with the internal buffers\nthat are kept by the file-like objects.  For example, the ``tell()``\nmethod will return the file position at the end of the buffers (whose\ncontents have not yet been returned by the stream); and therefore this\nfile position can not be used to return to the 'current' location in\nthe stream (since ``seek()`` has no way to reconstruct the buffers).\n\nTo get around these problems, we define a new class,\n`SeekableUnicodeStreamReader`, to act as a file-like interface to\nfiles containing encoded unicode data.  This class is loosely based on\nthe ``codecs.StreamReader`` class.  To construct a new reader, we call\nthe constructor with an underlying stream and an encoding name:\n\n    >>> from io import StringIO, BytesIO\n    >>> from nltk.data import SeekableUnicodeStreamReader\n    >>> stream = BytesIO(b\"\"\"\\\n    ... This is a test file.\n    ... It is encoded in ascii.\n    ... \"\"\".decode('ascii').encode('ascii'))\n    >>> reader = SeekableUnicodeStreamReader(stream, 'ascii')\n\n`SeekableUnicodeStreamReader`\\ s support all of the normal operations\nsupplied by a read-only stream.  Note that all of the read operations\nreturn ``unicode`` objects (not ``str`` objects).\n\n    >>> reader.read()         # read the entire file.\n    'This is a test file.\\nIt is encoded in ascii.\\n'\n    >>> reader.seek(0)        # rewind to the start.\n    >>> reader.read(5)        # read at most 5 bytes.\n    'This '\n    >>> reader.readline()     # read to the end of the line.\n    'is a test file.\\n'\n    >>> reader.seek(0)        # rewind to the start.\n    >>> for line in reader:\n    ...     print(repr(line))      # iterate over lines\n    'This is a test file.\\n'\n    'It is encoded in ascii.\\n'\n    >>> reader.seek(0)        # rewind to the start.\n    >>> reader.readlines()    # read a list of line strings\n    ['This is a test file.\\n', 'It is encoded in ascii.\\n']\n    >>> reader.close()\n\nSize argument to ``read()``\n---------------------------\nThe ``size`` argument to ``read()`` specifies the maximum number of\n*bytes* to read, not the maximum number of *characters*.  Thus, for\nencodings that use multiple bytes per character, it may return fewer\ncharacters than the ``size`` argument:\n\n    >>> stream = BytesIO(b\"\"\"\\\n    ... This is a test file.\n    ... It is encoded in utf-16.\n    ... \"\"\".decode('ascii').encode('utf-16'))\n    >>> reader = SeekableUnicodeStreamReader(stream, 'utf-16')\n    >>> reader.read(10)\n    'This '\n\nIf a read block ends in the middle of the byte string encoding a\nsingle character, then that byte string is stored in an internal\nbuffer, and re-used on the next call to ``read()``.  However, if the\nsize argument is too small to read even a single character, even\nthough at least one character is available, then the ``read()`` method\nwill read additional bytes until it can return a single character.\nThis ensures that the ``read()`` method does not return an empty\nstring, which could be mistaken for indicating the end of the file.\n\n    >>> reader.seek(0)            # rewind to the start.\n    >>> reader.read(1)            # we actually need to read 4 bytes\n    'T'\n    >>> int(reader.tell())\n    4\n\nThe ``readline()`` method may read more than a single line of text, in\nwhich case it stores the text that it does not return in a buffer.  If\nthis buffer is not empty, then its contents will be included in the\nvalue returned by the next call to ``read()``, regardless of the\n``size`` argument, since they are available without reading any new\nbytes from the stream:\n\n    >>> reader.seek(0)            # rewind to the start.\n    >>> reader.readline()         # stores extra text in a buffer\n    'This is a test file.\\n'\n    >>> print(reader.linebuffer)   # examine the buffer contents\n    ['It is encoded i']\n    >>> reader.read(0)            # returns the contents of the buffer\n    'It is encoded i'\n    >>> print(reader.linebuffer)   # examine the buffer contents\n    None\n\nSeek and Tell\n-------------\nIn addition to these basic read operations,\n`SeekableUnicodeStreamReader` also supports the ``seek()`` and\n``tell()`` operations.  However, some care must still be taken when\nusing these operations.  In particular, the only file offsets that\nshould be passed to ``seek()`` are ``0`` and any offset that has been\nreturned by ``tell``.\n\n    >>> stream = BytesIO(b\"\"\"\\\n    ... This is a test file.\n    ... It is encoded in utf-16.\n    ... \"\"\".decode('ascii').encode('utf-16'))\n    >>> reader = SeekableUnicodeStreamReader(stream, 'utf-16')\n    >>> reader.read(20)\n    'This is a '\n    >>> pos = reader.tell(); print(pos)\n    22\n    >>> reader.read(20)\n    'test file.'\n    >>> reader.seek(pos)     # rewind to the position from tell.\n    >>> reader.read(20)\n    'test file.'\n\nThe ``seek()`` and ``tell()`` methods work property even when\n``readline()`` is used.\n\n    >>> stream = BytesIO(b\"\"\"\\\n    ... This is a test file.\n    ... It is encoded in utf-16.\n    ... \"\"\".decode('ascii').encode('utf-16'))\n    >>> reader = SeekableUnicodeStreamReader(stream, 'utf-16')\n    >>> reader.readline()\n    'This is a test file.\\n'\n    >>> pos = reader.tell(); print(pos)\n    44\n    >>> reader.readline()\n    'It is encoded in utf-16.\\n'\n    >>> reader.seek(pos)     # rewind to the position from tell.\n    >>> reader.readline()\n    'It is encoded in utf-16.\\n'\n\n\nSquashed Bugs\n=============\n\nsvn 5276 fixed a bug in the comment-stripping behavior of\nparse_sexpr_block.\n\n    >>> from io import StringIO\n    >>> from nltk.corpus.reader.util import read_sexpr_block\n    >>> f = StringIO(b\"\"\"\n    ... (a b c)\n    ... # This line is a comment.\n    ... (d e f\\ng h)\"\"\".decode('ascii'))\n    >>> print(read_sexpr_block(f, block_size=38, comment_char='#'))\n    ['(a b c)']\n    >>> print(read_sexpr_block(f, block_size=38, comment_char='#'))\n    ['(d e f\\ng h)']\n\nsvn 5277 fixed a bug in parse_sexpr_block, which would cause it to\nenter an infinite loop if a file ended mid-sexpr, or ended with a\ntoken that was not followed by whitespace.  A related bug caused\nan infinite loop if the corpus ended in an unmatched close paren --\nthis was fixed in svn 5279\n\n    >>> f = StringIO(b\"\"\"\n    ... This file ends mid-sexpr\n    ... (hello (world\"\"\".decode('ascii'))\n    >>> for i in range(3): print(read_sexpr_block(f))\n    ['This', 'file', 'ends', 'mid-sexpr']\n    ['(hello (world']\n    []\n\n    >>> f = StringIO(b\"This file has no trailing whitespace.\".decode('ascii'))\n    >>> for i in range(3): print(read_sexpr_block(f))\n    ['This', 'file', 'has', 'no', 'trailing']\n    ['whitespace.']\n    []\n\n    >>> # Bug fixed in 5279:\n    >>> f = StringIO(b\"a b c)\".decode('ascii'))\n    >>> for i in range(3): print(read_sexpr_block(f))\n    ['a', 'b']\n    ['c)']\n    []\n\n\nsvn 5624 & 5265 fixed a bug in ConcatenatedCorpusView, which caused it\nto return the wrong items when indexed starting at any index beyond\nthe first file.\n\n    >>> import nltk\n    >>> sents = nltk.corpus.brown.sents()\n    >>> print(sents[6000])\n    ['Cholesterol', 'and', 'thyroid']\n    >>> print(sents[6000])\n    ['Cholesterol', 'and', 'thyroid']\n\nsvn 5728 fixed a bug in Categorized*CorpusReader, which caused them\nto return words from *all* files when just one file was specified.\n\n    >>> from nltk.corpus import reuters\n    >>> reuters.words('training/13085')\n    ['SNYDER', '&', 'lt', ';', 'SOI', '>', 'MAKES', ...]\n    >>> reuters.words('training/5082')\n    ['SHEPPARD', 'RESOURCES', 'TO', 'MERGE', 'WITH', ...]\n\nsvn 7227 fixed a bug in the qc corpus reader, which prevented\naccess to its tuples() method\n\n    >>> from nltk.corpus import qc\n    >>> qc.tuples('test.txt')\n    [('NUM:dist', 'How far is it from Denver to Aspen ?'), ('LOC:city', 'What county is Modesto , California in ?'), ...]\n\nEnsure that KEYWORD from `comparative_sents.py` no longer contains a ReDoS vulnerability.\n\n    >>> import re\n    >>> import time\n    >>> from nltk.corpus.reader.comparative_sents import KEYWORD\n    >>> sizes = {\n    ...     \"short\": 4000,\n    ...     \"long\": 40000\n    ... }\n    >>> exec_times = {\n    ...     \"short\": [],\n    ...     \"long\": [],\n    ... }\n    >>> for size_name, size in sizes.items():\n    ...     for j in range(9):\n    ...         start_t = time.perf_counter()\n    ...         payload = \"( \" + \"(\" * size\n    ...         output = KEYWORD.findall(payload)\n    ...         exec_times[size_name].append(time.perf_counter() - start_t)\n    ...     exec_times[size_name] = sorted(exec_times[size_name])[4] # Get the mean\n\nIdeally, the execution time of such a regular expression is linear\nin the length of the input. As such, we would expect exec_times[\"long\"]\nto be roughly 10 times as big as exec_times[\"short\"].\nWith the ReDoS in place, it took roughly 80 times as long.\nFor now, we accept values below 30 (times as long), due to the potential\nfor variance. This ensures that the ReDoS has certainly been reduced,\nif not removed.\n\n    >>> exec_times[\"long\"] / exec_times[\"short\"] < 30\n    True\n"], "filenames": ["nltk/corpus/reader/comparative_sents.py", "nltk/test/corpus.doctest"], "buggy_code_start_loc": [48, 2164], "buggy_code_end_loc": [49, 2164], "fixing_code_start_loc": [48, 2165], "fixing_code_end_loc": [49, 2197], "type": "CWE-697", "message": "nltk is vulnerable to Inefficient Regular Expression Complexity", "other": {"cve": {"id": "CVE-2021-3828", "sourceIdentifier": "security@huntr.dev", "published": "2021-09-27T13:15:07.993", "lastModified": "2022-04-25T18:18:14.673", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "nltk is vulnerable to Inefficient Regular Expression Complexity"}, {"lang": "es", "value": "nltk es vulnerable a una Complejidad de Expresi\u00f3n Regular Ineficiente"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV30": [{"source": "security@huntr.dev", "type": "Secondary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-697"}]}, {"source": "security@huntr.dev", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-1333"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:nltk:nltk:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.6.3", "matchCriteriaId": "18A51778-0184-4262-A014-08D077E6BE93"}]}]}], "references": [{"url": "https://github.com/nltk/nltk/commit/277711ab1dec729e626b27aab6fa35ea5efbd7e6", "source": "security@huntr.dev", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://huntr.dev/bounties/d19aed43-75bc-4a03-91a0-4d0bb516bc32", "source": "security@huntr.dev", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/nltk/nltk/commit/277711ab1dec729e626b27aab6fa35ea5efbd7e6"}}