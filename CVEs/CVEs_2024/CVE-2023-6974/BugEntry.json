{"buggy_code": ["\"\"\"\nThis module defines environment variables used in MLflow.\n\"\"\"\nimport os\nfrom pathlib import Path\n\n\nclass _EnvironmentVariable:\n    \"\"\"\n    Represents an environment variable.\n    \"\"\"\n\n    def __init__(self, name, type_, default):\n        self.name = name\n        self.type = type_\n        self.default = default\n\n    @property\n    def defined(self):\n        return self.name in os.environ\n\n    def get_raw(self):\n        return os.getenv(self.name)\n\n    def set(self, value):\n        os.environ[self.name] = str(value)\n\n    def unset(self):\n        os.environ.pop(self.name, None)\n\n    def get(self):\n        \"\"\"\n        Reads the value of the environment variable if it exists and converts it to the desired\n        type. Otherwise, returns the default value.\n        \"\"\"\n        if (val := self.get_raw()) is not None:\n            try:\n                return self.type(val)\n            except Exception as e:\n                raise ValueError(f\"Failed to convert {val!r} to {self.type} for {self.name}: {e}\")\n        return self.default\n\n    def __str__(self):\n        return f\"{self.name} (default: {self.default}, type: {self.type.__name__})\"\n\n    def __repr__(self):\n        return repr(self.name)\n\n    def __format__(self, format_spec: str) -> str:\n        return self.name.__format__(format_spec)\n\n\nclass _BooleanEnvironmentVariable(_EnvironmentVariable):\n    \"\"\"\n    Represents a boolean environment variable.\n    \"\"\"\n\n    def __init__(self, name, default):\n        # `default not in [True, False, None]` doesn't work because `1 in [True]`\n        # (or `0 in [False]`) returns True.\n        if not (default is True or default is False or default is None):\n            raise ValueError(f\"{name} default value must be one of [True, False, None]\")\n        super().__init__(name, bool, default)\n\n    def get(self):\n        if not self.defined:\n            return self.default\n\n        val = os.getenv(self.name)\n        lowercased = val.lower()\n        if lowercased not in [\"true\", \"false\", \"1\", \"0\"]:\n            raise ValueError(\n                f\"{self.name} value must be one of ['true', 'false', '1', '0'] (case-insensitive), \"\n                f\"but got {val}\"\n            )\n        return lowercased in [\"true\", \"1\"]\n\n\n#: Specifies the tracking URI.\n#: (default: ``None``)\nMLFLOW_TRACKING_URI = _EnvironmentVariable(\"MLFLOW_TRACKING_URI\", str, None)\n\n#: Specifies the registry URI.\n#: (default: ``None``)\nMLFLOW_REGISTRY_URI = _EnvironmentVariable(\"MLFLOW_REGISTRY_URI\", str, None)\n\n#: Specifies the ``dfs_tmpdir`` parameter to use for ``mlflow.spark.save_model``,\n#: ``mlflow.spark.log_model`` and ``mlflow.spark.load_model``. See\n#: https://www.mlflow.org/docs/latest/python_api/mlflow.spark.html#mlflow.spark.save_model\n#: for more information.\n#: (default: ``/tmp/mlflow``)\nMLFLOW_DFS_TMP = _EnvironmentVariable(\"MLFLOW_DFS_TMP\", str, \"/tmp/mlflow\")\n\n#: Specifies the maximum number of retries for MLflow HTTP requests\n#: (default: ``5``)\nMLFLOW_HTTP_REQUEST_MAX_RETRIES = _EnvironmentVariable(\"MLFLOW_HTTP_REQUEST_MAX_RETRIES\", int, 5)\n\n#: Specifies the backoff increase factor between MLflow HTTP request failures\n#: (default: ``2``)\nMLFLOW_HTTP_REQUEST_BACKOFF_FACTOR = _EnvironmentVariable(\n    \"MLFLOW_HTTP_REQUEST_BACKOFF_FACTOR\", int, 2\n)\n\n#: Specifies the backoff jitter between MLflow HTTP request failures\n#: (default: ``1.0``)\nMLFLOW_HTTP_REQUEST_BACKOFF_JITTER = _EnvironmentVariable(\n    \"MLFLOW_HTTP_REQUEST_BACKOFF_JITTER\", float, 1.0\n)\n\n#: Specifies the timeout in seconds for MLflow HTTP requests\n#: (default: ``120``)\nMLFLOW_HTTP_REQUEST_TIMEOUT = _EnvironmentVariable(\"MLFLOW_HTTP_REQUEST_TIMEOUT\", int, 120)\n\n#: Specifies whether MLflow HTTP requests should be signed using AWS signature V4. It will overwrite\n#: (default: ``False``). When set, it will overwrite the \"Authorization\" HTTP header.\n#: See https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html for more information.\nMLFLOW_TRACKING_AWS_SIGV4 = _BooleanEnvironmentVariable(\"MLFLOW_TRACKING_AWS_SIGV4\", False)\n\n#: Specifies the auth provider to sign the MLflow HTTP request\n#: (default: ``None``). When set, it will overwrite the \"Authorization\" HTTP header.\nMLFLOW_TRACKING_AUTH = _EnvironmentVariable(\"MLFLOW_TRACKING_AUTH\", str, None)\n\n#: Specifies the chunk size to use when downloading a file from GCS\n#: (default: ``None``). If None, the chunk size is automatically determined by the\n#: ``google-cloud-storage`` package.\nMLFLOW_GCS_DOWNLOAD_CHUNK_SIZE = _EnvironmentVariable(\"MLFLOW_GCS_DOWNLOAD_CHUNK_SIZE\", int, None)\n\n#: Specifies the chunk size to use when uploading a file to GCS.\n#: (default: ``None``). If None, the chunk size is automatically determined by the\n#: ``google-cloud-storage`` package.\nMLFLOW_GCS_UPLOAD_CHUNK_SIZE = _EnvironmentVariable(\"MLFLOW_GCS_UPLOAD_CHUNK_SIZE\", int, None)\n\n#: (Deprecated, please use ``MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT``)\n#: Specifies the default timeout to use when downloading/uploading a file from/to GCS\n#: (default: ``None``). If None, ``google.cloud.storage.constants._DEFAULT_TIMEOUT`` is used.\nMLFLOW_GCS_DEFAULT_TIMEOUT = _EnvironmentVariable(\"MLFLOW_GCS_DEFAULT_TIMEOUT\", int, None)\n\n#: Specifies whether to disable model logging and loading via mlflowdbfs.\n#: (default: ``None``)\n_DISABLE_MLFLOWDBFS = _EnvironmentVariable(\"DISABLE_MLFLOWDBFS\", str, None)\n\n#: Specifies the S3 endpoint URL to use for S3 artifact operations.\n#: (default: ``None``)\nMLFLOW_S3_ENDPOINT_URL = _EnvironmentVariable(\"MLFLOW_S3_ENDPOINT_URL\", str, None)\n\n#: Specifies whether or not to skip TLS certificate verification for S3 artifact operations.\n#: (default: ``False``)\nMLFLOW_S3_IGNORE_TLS = _BooleanEnvironmentVariable(\"MLFLOW_S3_IGNORE_TLS\", False)\n\n#: Specifies extra arguments for S3 artifact uploads.\n#: (default: ``None``)\nMLFLOW_S3_UPLOAD_EXTRA_ARGS = _EnvironmentVariable(\"MLFLOW_S3_UPLOAD_EXTRA_ARGS\", str, None)\n\n#: Specifies the location of a Kerberos ticket cache to use for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_KERBEROS_TICKET_CACHE = _EnvironmentVariable(\"MLFLOW_KERBEROS_TICKET_CACHE\", str, None)\n\n#: Specifies a Kerberos user for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_KERBEROS_USER = _EnvironmentVariable(\"MLFLOW_KERBEROS_USER\", str, None)\n\n#: Specifies extra pyarrow configurations for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_PYARROW_EXTRA_CONF = _EnvironmentVariable(\"MLFLOW_PYARROW_EXTRA_CONF\", str, None)\n\n#: Specifies the ``pool_size`` parameter to use for ``sqlalchemy.create_engine`` in the SQLAlchemy\n#: tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_size\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOL_SIZE = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOL_SIZE\", int, None\n)\n\n#: Specifies the ``pool_recycle`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_recycle\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE\", int, None\n)\n\n#: Specifies the ``max_overflow`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.max_overflow\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_MAX_OVERFLOW = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_MAX_OVERFLOW\", int, None\n)\n\n#: Specifies the ``echo`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.echo\n#: for more information.\n#: (default: ``False``)\nMLFLOW_SQLALCHEMYSTORE_ECHO = _BooleanEnvironmentVariable(\"MLFLOW_SQLALCHEMYSTORE_ECHO\", False)\n\n#: Specifies whether or not to print a warning when `--env-manager=conda` is specified.\n#: (default: ``False``)\nMLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING = _BooleanEnvironmentVariable(\n    \"MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING\", False\n)\n#: Specifies the ``poolclass`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.poolclass\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOLCLASS = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOLCLASS\", str, None\n)\n\n#: Specifies the ``timeout_seconds`` for MLflow Model dependency inference operations.\n#: (default: ``120``)\nMLFLOW_REQUIREMENTS_INFERENCE_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_REQUIREMENTS_INFERENCE_TIMEOUT\", int, 120\n)\n\n#: Specifies the MLflow Model Scoring server request timeout in seconds\n#: (default: ``60``)\nMLFLOW_SCORING_SERVER_REQUEST_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT\", int, 60\n)\n\n#: (Experimental, may be changed or removed)\n#: Specifies the timeout to use when uploading or downloading a file\n#: (default: ``None``). If None, individual artifact stores will choose defaults.\nMLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT\", int, None\n)\n\n#: Specifies the device intended for use in the predict function - can be used\n#: to override behavior where the GPU is used by default when available by\n#: setting this environment variable to be ``cpu``. Currently, this\n#: variable is only supported for the MLflow PyTorch and HuggingFace flavors.\n#: For the HuggingFace flavor, note that device must be parseable as an integer.\nMLFLOW_DEFAULT_PREDICTION_DEVICE = _EnvironmentVariable(\n    \"MLFLOW_DEFAULT_PREDICTION_DEVICE\", str, None\n)\n\n#: Specifies to Huggingface whether to use the automatic device placement logic of\n# HuggingFace accelerate. If it's set to false, the low_cpu_mem_usage flag will not be\n# set to True and device_map will not be set to \"auto\".\nMLFLOW_HUGGINGFACE_DISABLE_ACCELERATE_FEATURES = _BooleanEnvironmentVariable(\n    \"MLFLOW_DISABLE_HUGGINGFACE_ACCELERATE_FEATURES\", False\n)\n\n#: Specifies to Huggingface whether to use the automatic device placement logic of\n# HuggingFace accelerate. If it's set to false, the low_cpu_mem_usage flag will not be\n# set to True and device_map will not be set to \"auto\".\nMLFLOW_HUGGINGFACE_USE_DEVICE_MAP = _BooleanEnvironmentVariable(\n    \"MLFLOW_HUGGINGFACE_USE_DEVICE_MAP\", True\n)\n\n#: Specifies to Huggingface to use the automatic device placement logic of HuggingFace accelerate.\n#: This can be set to values supported by the version of HuggingFace Accelerate being installed.\nMLFLOW_HUGGINGFACE_DEVICE_MAP_STRATEGY = _EnvironmentVariable(\n    \"MLFLOW_HUGGINGFACE_DEVICE_MAP_STRATEGY\", str, \"auto\"\n)\n\n#: Specifies to Huggingface to use the low_cpu_mem_usage flag powered by HuggingFace accelerate.\n#: If it's set to false, the low_cpu_mem_usage flag will be set to False.\nMLFLOW_HUGGINGFACE_USE_LOW_CPU_MEM_USAGE = _BooleanEnvironmentVariable(\n    \"MLFLOW_HUGGINGFACE_USE_LOW_CPU_MEM_USAGE\", True\n)\n\n#: Specifies the max_shard_size to use when mlflow transformers flavor saves the model checkpoint.\n#: This can be set to override the 500MB default.\nMLFLOW_HUGGINGFACE_MODEL_MAX_SHARD_SIZE = _EnvironmentVariable(\n    \"MLFLOW_HUGGINGFACE_MODEL_MAX_SHARD_SIZE\", str, \"500MB\"\n)\n\n#: Specifies whether or not to allow using a file URI as a model version source.\n#: Please be aware that setting this environment variable to True is potentially risky\n#: because it can allow access to arbitrary files on the specified filesystem\n#: (default: ``False``).\nMLFLOW_ALLOW_FILE_URI_AS_MODEL_VERSION_SOURCE = _BooleanEnvironmentVariable(\n    \"MLFLOW_ALLOW_FILE_URI_AS_MODEL_VERSION_SOURCE\", False\n)\n\n\n#: Specifies the name of the Databricks secret scope to use for storing OpenAI API keys.\nMLFLOW_OPENAI_SECRET_SCOPE = _EnvironmentVariable(\"MLFLOW_OPENAI_SECRET_SCOPE\", str, None)\n\n#: Specifier whether or not to retry OpenAI API calls.\nMLFLOW_OPENAI_RETRIES_ENABLED = _BooleanEnvironmentVariable(\"MLFLOW_OPENAI_RETRIES_ENABLED\", True)\n\n#: (Experimental, may be changed or removed)\n#: Specifies the download options to be used by pip wheel when `add_libraries_to_model` is used to\n#: create and log model dependencies as model artifacts. The default behavior only uses dependency\n#: binaries and no source packages.\n#: (default: ``--only-binary=:all:``).\nMLFLOW_WHEELED_MODEL_PIP_DOWNLOAD_OPTIONS = _EnvironmentVariable(\n    \"MLFLOW_WHEELED_MODEL_PIP_DOWNLOAD_OPTIONS\", str, \"--only-binary=:all:\"\n)\n\n# Specifies whether or not to use multipart download when downloading a large file on Databricks.\nMLFLOW_ENABLE_MULTIPART_DOWNLOAD = _BooleanEnvironmentVariable(\n    \"MLFLOW_ENABLE_MULTIPART_DOWNLOAD\", True\n)\n\n# Specifies whether or not to use multipart upload when uploading large artifacts.\nMLFLOW_ENABLE_MULTIPART_UPLOAD = _BooleanEnvironmentVariable(\"MLFLOW_ENABLE_MULTIPART_UPLOAD\", True)\n\n#: Specifies whether or not to use multipart upload for proxied artifact access.\n#: (default: ``False``)\nMLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD = _BooleanEnvironmentVariable(\n    \"MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD\", False\n)\n\n#: Private environment variable that's set to ``True`` while running tests.\n_MLFLOW_TESTING = _BooleanEnvironmentVariable(\"MLFLOW_TESTING\", False)\n\n#: Specifies the username used to authenticate with a tracking server.\n#: (default: ``None``)\nMLFLOW_TRACKING_USERNAME = _EnvironmentVariable(\"MLFLOW_TRACKING_USERNAME\", str, None)\n\n#: Specifies the password used to authenticate with a tracking server.\n#: (default: ``None``)\nMLFLOW_TRACKING_PASSWORD = _EnvironmentVariable(\"MLFLOW_TRACKING_PASSWORD\", str, None)\n\n#: Specifies and takes precedence for setting the basic/bearer auth on http requests.\n#: (default: ``None``)\nMLFLOW_TRACKING_TOKEN = _EnvironmentVariable(\"MLFLOW_TRACKING_TOKEN\", str, None)\n\n#: Specifies whether to verify TLS connection in ``requests.request`` function,\n#: see https://requests.readthedocs.io/en/master/api/\n#: (default: ``False``).\nMLFLOW_TRACKING_INSECURE_TLS = _BooleanEnvironmentVariable(\"MLFLOW_TRACKING_INSECURE_TLS\", False)\n\n#: Sets the ``verify`` param in ``requests.request`` function,\n#: see https://requests.readthedocs.io/en/master/api/\n#: (default: ``None``)\nMLFLOW_TRACKING_SERVER_CERT_PATH = _EnvironmentVariable(\n    \"MLFLOW_TRACKING_SERVER_CERT_PATH\", str, None\n)\n\n#: Sets the ``cert`` param in ``requests.request`` function,\n#: see https://requests.readthedocs.io/en/master/api/\n#: (default: ``None``)\nMLFLOW_TRACKING_CLIENT_CERT_PATH = _EnvironmentVariable(\n    \"MLFLOW_TRACKING_CLIENT_CERT_PATH\", str, None\n)\n\n#: Specified the ID of the run to log data to.\n#: (default: ``None``)\nMLFLOW_RUN_ID = _EnvironmentVariable(\"MLFLOW_RUN_ID\", str, None)\n\n#: Specifies the default root directory for tracking `FileStore`.\n#: (default: ``None``)\nMLFLOW_TRACKING_DIR = _EnvironmentVariable(\"MLFLOW_TRACKING_DIR\", str, None)\n\n#: Specifies the default root directory for registry `FileStore`.\n#: (default: ``None``)\nMLFLOW_REGISTRY_DIR = _EnvironmentVariable(\"MLFLOW_REGISTRY_DIR\", str, None)\n\n#: Specifies the default experiment ID to create run to.\n#: (default: ``None``)\nMLFLOW_EXPERIMENT_ID = _EnvironmentVariable(\"MLFLOW_EXPERIMENT_ID\", str, None)\n\n#: Specifies the default experiment name to create run to.\n#: (default: ``None``)\nMLFLOW_EXPERIMENT_NAME = _EnvironmentVariable(\"MLFLOW_EXPERIMENT_NAME\", str, None)\n\n#: Specified the path to the configuration file for MLflow Authentication.\n#: (default: ``None``)\nMLFLOW_AUTH_CONFIG_PATH = _EnvironmentVariable(\"MLFLOW_AUTH_CONFIG_PATH\", str, None)\n\n#: Specifies the root directory to create Python virtual environments in.\n#: (default: ``~/.mlflow/envs``)\nMLFLOW_ENV_ROOT = _EnvironmentVariable(\n    \"MLFLOW_ENV_ROOT\", str, str(Path.home().joinpath(\".mlflow\", \"envs\"))\n)\n\n#: Specifies whether or not to use DBFS FUSE mount to store artifacts on Databricks\n#: (default: ``False``)\nMLFLOW_ENABLE_DBFS_FUSE_ARTIFACT_REPO = _BooleanEnvironmentVariable(\n    \"MLFLOW_ENABLE_DBFS_FUSE_ARTIFACT_REPO\", True\n)\n\n#: Private environment variable that should be set to ``True`` when running autologging tests.\n#: (default: ``False``)\n_MLFLOW_AUTOLOGGING_TESTING = _BooleanEnvironmentVariable(\"MLFLOW_AUTOLOGGING_TESTING\", False)\n\n#: (Experimental, may be changed or removed)\n#: Specifies the uri of a MLflow Gateway Server instance to be used with the Gateway Client APIs\n#: (default: ``None``)\nMLFLOW_GATEWAY_URI = _EnvironmentVariable(\"MLFLOW_GATEWAY_URI\", str, None)\n\n#: (Experimental, may be changed or removed)\n#: Specifies the uri of a MLflow Deployments Server instance to be used with the Deployments\n#: Client APIs\n#: (default: ``None``)\nMLFLOW_DEPLOYMENTS_TARGET = _EnvironmentVariable(\"MLFLOW_DEPLOYMENTS_TARGET\", str, None)\n\n#: Specifies the path of the config file for MLflow AI Gateway.\n#: (default: ``None``)\nMLFLOW_GATEWAY_CONFIG = _EnvironmentVariable(\"MLFLOW_GATEWAY_CONFIG\", str, None)\n\n#: Specifies the path of the config file for the MLflow Deployments server.\n#: (default: ``None``)\nMLFLOW_DEPLOYMENTS_CONFIG = _EnvironmentVariable(\"MLFLOW_DEPLOYMENTS_CONFIG\", str, None)\n\n#: Specifies whether to display the progress bar when uploading/downloading artifacts.\n#: (default: ``True``)\nMLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR = _BooleanEnvironmentVariable(\n    \"MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR\", True\n)\n\n#: Specifies the conda home directory to use.\n#: (default: ``conda``)\nMLFLOW_CONDA_HOME = _EnvironmentVariable(\"MLFLOW_CONDA_HOME\", str, None)\n\n#: Specifies the name of the command to use when creating the environments.\n#: For example, let's say we want to use mamba (https://github.com/mamba-org/mamba)\n#: instead of conda to create environments.\n#: Then: > conda install mamba -n base -c conda-forge\n#: If not set, use the same as conda_path\n#: (default: ``conda``)\nMLFLOW_CONDA_CREATE_ENV_CMD = _EnvironmentVariable(\"MLFLOW_CONDA_CREATE_ENV_CMD\", str, \"conda\")\n\n#: Specifies the execution directory for recipes.\n#: (default: ``None``)\nMLFLOW_RECIPES_EXECUTION_DIRECTORY = _EnvironmentVariable(\n    \"MLFLOW_RECIPES_EXECUTION_DIRECTORY\", str, None\n)\n\n#: Specifies the target step to execute for recipes.\n#: (default: ``None``)\nMLFLOW_RECIPES_EXECUTION_TARGET_STEP_NAME = _EnvironmentVariable(\n    \"MLFLOW_RECIPES_EXECUTION_TARGET_STEP_NAME\", str, None\n)\n\n#: Specifies the flavor to serve in the scoring server.\n#: (default ``None``)\nMLFLOW_DEPLOYMENT_FLAVOR_NAME = _EnvironmentVariable(\"MLFLOW_DEPLOYMENT_FLAVOR_NAME\", str, None)\n\n#: Specifies the profile to use for recipes.\n#: (default: ``None``)\nMLFLOW_RECIPES_PROFILE = _EnvironmentVariable(\"MLFLOW_RECIPES_PROFILE\", str, None)\n\n#: Specifies the MLflow Run context\n#: (default: ``None``)\nMLFLOW_RUN_CONTEXT = _EnvironmentVariable(\"MLFLOW_RUN_CONTEXT\", str, None)\n\n#: Specifies the URL of the ECR-hosted Docker image a model is deployed into for SageMaker.\n# (default: ``None``)\nMLFLOW_SAGEMAKER_DEPLOY_IMG_URL = _EnvironmentVariable(\"MLFLOW_SAGEMAKER_DEPLOY_IMG_URL\", str, None)\n\n#: Specifies whether to disable creating a new conda environment for `mlflow models build-docker`.\n#: (default: ``False``)\nMLFLOW_DISABLE_ENV_CREATION = _BooleanEnvironmentVariable(\"MLFLOW_DISABLE_ENV_CREATION\", False)\n\n#: Specifies the timeout value for downloading chunks of mlflow artifacts.\n#: (default: ``300``)\nMLFLOW_DOWNLOAD_CHUNK_TIMEOUT = _EnvironmentVariable(\"MLFLOW_DOWNLOAD_CHUNK_TIMEOUT\", int, 300)\n\n#: Specifies if system metrics logging should be enabled.\nMLFLOW_ENABLE_SYSTEM_METRICS_LOGGING = _BooleanEnvironmentVariable(\n    \"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\", False\n)\n\n#: Specifies the sampling interval for system metrics logging.\nMLFLOW_SYSTEM_METRICS_SAMPLING_INTERVAL = _EnvironmentVariable(\n    \"MLFLOW_SYSTEM_METRICS_SAMPLING_INTERVAL\", float, None\n)\n\n#: Specifies the number of samples before logging system metrics.\nMLFLOW_SYSTEM_METRICS_SAMPLES_BEFORE_LOGGING = _EnvironmentVariable(\n    \"MLFLOW_SYSTEM_METRICS_SAMPLES_BEFORE_LOGGING\", int, None\n)\n\n# Private environment variable to specify the number of chunk download retries for multipart\n# download.\n_MLFLOW_MPD_NUM_RETRIES = _EnvironmentVariable(\"_MLFLOW_MPD_NUM_RETRIES\", int, 3)\n\n# Private environment variable to specify the interval between chunk download retries for multipart\n# download.\n_MLFLOW_MPD_RETRY_INTERVAL_SECONDS = _EnvironmentVariable(\n    \"_MLFLOW_MPD_RETRY_INTERVAL_SECONDS\", int, 1\n)\n\n#: Specifies the minimum file size in bytes to use multipart upload when logging artifacts\n#: (default: ``524_288_000`` (500 MB))\nMLFLOW_MULTIPART_UPLOAD_MINIMUM_FILE_SIZE = _EnvironmentVariable(\n    \"MLFLOW_MULTIPART_UPLOAD_MINIMUM_FILE_SIZE\", int, 500 * 1024**2\n)\n\n#: Specifies the chunk size in bytes to use when performing multipart upload\n#: (default: ``104_857_60`` (10 MB))\nMLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE = _EnvironmentVariable(\n    \"MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE\", int, 10 * 1024**2\n)\n\n#: Specifies the chunk size in bytes to use when performing multipart download\n#: (default: ``104_857_600`` (100 MB))\nMLFLOW_MULTIPART_DOWNLOAD_CHUNK_SIZE = _EnvironmentVariable(\n    \"MLFLOW_MULTIPART_DOWNLOAD_CHUNK_SIZE\", int, 100 * 1024**2\n)\n", "# DO NO IMPORT MLFLOW IN THIS FILE.\n# This file is imported by download_cloud_file_chunk.py.\n# Importing mlflow is time-consuming and we want to avoid that in artifact download subprocesses.\nimport os\nimport random\nfrom functools import lru_cache\n\nimport requests\nimport urllib3\nfrom packaging.version import Version\nfrom requests.adapters import HTTPAdapter\nfrom requests.exceptions import HTTPError\nfrom urllib3.util import Retry\n\n# Response codes that generally indicate transient network failures and merit client retries,\n# based on guidance from cloud service providers\n# (https://docs.microsoft.com/en-us/azure/architecture/best-practices/retry-service-specific#general-rest-and-retry-guidelines)\n_TRANSIENT_FAILURE_RESPONSE_CODES = frozenset(\n    [\n        408,  # Request Timeout\n        429,  # Too Many Requests\n        500,  # Internal Server Error\n        502,  # Bad Gateway\n        503,  # Service Unavailable\n        504,  # Gateway Timeout\n    ]\n)\n\n\nclass JitteredRetry(Retry):\n    \"\"\"\n    urllib3 < 2 doesn't support `backoff_jitter`. This class is a workaround for that.\n    \"\"\"\n\n    def __init__(self, *args, backoff_jitter=0.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.backoff_jitter = backoff_jitter\n\n    def get_backoff_time(self):\n        \"\"\"\n        Source: https://github.com/urllib3/urllib3/commit/214b184923388328919b0a4b0c15bff603aa51be\n        \"\"\"\n        backoff_value = super().get_backoff_time()\n        if self.backoff_jitter != 0.0:\n            backoff_value += random.random() * self.backoff_jitter\n        return float(max(0, min(Retry.DEFAULT_BACKOFF_MAX, backoff_value)))\n\n\ndef augmented_raise_for_status(response):\n    \"\"\"Wrap the standard `requests.response.raise_for_status()` method and return reason\"\"\"\n    try:\n        response.raise_for_status()\n    except HTTPError as e:\n        if response.text:\n            raise HTTPError(\n                f\"{e}. Response text: {response.text}\", request=e.request, response=e.response\n            )\n        else:\n            raise e\n\n\ndef download_chunk(*, range_start, range_end, headers, download_path, http_uri):\n    combined_headers = {**headers, \"Range\": f\"bytes={range_start}-{range_end}\"}\n\n    with cloud_storage_http_request(\n        \"get\",\n        http_uri,\n        stream=False,\n        headers=combined_headers,\n        timeout=10,\n    ) as response:\n        expected_length = response.headers.get(\"Content-Length\")\n        if expected_length is not None:\n            actual_length = response.raw.tell()\n            expected_length = int(expected_length)\n            if actual_length < expected_length:\n                raise IOError(\n                    \"Incomplete read ({} bytes read, {} more expected)\".format(\n                        actual_length, expected_length - actual_length\n                    )\n                )\n        # File will have been created upstream. Use r+b to ensure chunks\n        # don't overwrite the entire file.\n        augmented_raise_for_status(response)\n        with open(download_path, \"r+b\") as f:\n            f.seek(range_start)\n            f.write(response.content)\n\n\n@lru_cache(maxsize=64)\ndef _cached_get_request_session(\n    max_retries,\n    backoff_factor,\n    backoff_jitter,\n    retry_codes,\n    raise_on_status,\n    # To create a new Session object for each process, we use the process id as the cache key.\n    # This is to avoid sharing the same Session object across processes, which can lead to issues\n    # such as https://stackoverflow.com/q/3724900.\n    _pid,\n):\n    \"\"\"\n    This function should not be called directly. Instead, use `_get_request_session` below.\n    \"\"\"\n    assert 0 <= max_retries < 10\n    assert 0 <= backoff_factor < 120\n\n    retry_kwargs = {\n        \"total\": max_retries,\n        \"connect\": max_retries,\n        \"read\": max_retries,\n        \"redirect\": max_retries,\n        \"status\": max_retries,\n        \"status_forcelist\": retry_codes,\n        \"backoff_factor\": backoff_factor,\n        \"backoff_jitter\": backoff_jitter,\n        \"raise_on_status\": raise_on_status,\n    }\n    urllib3_version = Version(urllib3.__version__)\n    if urllib3_version >= Version(\"1.26.0\"):\n        retry_kwargs[\"allowed_methods\"] = None\n    else:\n        retry_kwargs[\"method_whitelist\"] = None\n\n    if urllib3_version < Version(\"2.0\"):\n        retry = JitteredRetry(**retry_kwargs)\n    else:\n        retry = Retry(**retry_kwargs)\n    adapter = HTTPAdapter(max_retries=retry)\n    session = requests.Session()\n    session.mount(\"https://\", adapter)\n    session.mount(\"http://\", adapter)\n    return session\n\n\ndef _get_request_session(max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status):\n    \"\"\"\n    Returns a `Requests.Session` object for making an HTTP request.\n\n    :param max_retries: Maximum total number of retries.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param raise_on_status: whether to raise an exception, or return a response, if status falls\n      in retry_codes range and retries have been exhausted.\n    :return: requests.Session object.\n    \"\"\"\n    return _cached_get_request_session(\n        max_retries,\n        backoff_factor,\n        backoff_jitter,\n        retry_codes,\n        raise_on_status,\n        _pid=os.getpid(),\n    )\n\n\ndef _get_http_response_with_retries(\n    method,\n    url,\n    max_retries,\n    backoff_factor,\n    backoff_jitter,\n    retry_codes,\n    raise_on_status=True,\n    **kwargs,\n):\n    \"\"\"\n    Performs an HTTP request using Python's `requests` module with an automatic retry policy.\n\n    :param method: a string indicating the method to use, e.g. \"GET\", \"POST\", \"PUT\".\n    :param url: the target URL address for the HTTP request.\n    :param max_retries: Maximum total number of retries.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param raise_on_status: whether to raise an exception, or return a response, if status falls\n      in retry_codes range and retries have been exhausted.\n    :param kwargs: Additional keyword arguments to pass to `requests.Session.request()`\n\n    :return: requests.Response object.\n    \"\"\"\n    session = _get_request_session(\n        max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status\n    )\n    return session.request(method, url, **kwargs)\n\n\ndef cloud_storage_http_request(\n    method,\n    url,\n    max_retries=5,\n    backoff_factor=2,\n    backoff_jitter=1.0,\n    retry_codes=_TRANSIENT_FAILURE_RESPONSE_CODES,\n    timeout=None,\n    **kwargs,\n):\n    \"\"\"\n    Performs an HTTP PUT/GET/PATCH request using Python's `requests` module with automatic retry.\n\n    :param method: string of 'PUT' or 'GET' or 'PATCH', specify to do http PUT or GET or PATCH\n    :param url: the target URL address for the HTTP request.\n    :param max_retries: maximum number of retries before throwing an exception.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param timeout: wait for timeout seconds for response from remote server for connect and\n      read request. Default to None owing to long duration operation in read / write.\n    :param kwargs: Additional keyword arguments to pass to `requests.Session.request()`\n\n    :return requests.Response object.\n    \"\"\"\n    if method.lower() not in (\"put\", \"get\", \"patch\", \"delete\"):\n        raise ValueError(\"Illegal http method: \" + method)\n    return _get_http_response_with_retries(\n        method,\n        url,\n        max_retries,\n        backoff_factor,\n        backoff_jitter,\n        retry_codes,\n        timeout=timeout,\n        **kwargs,\n    )\n", "import filecmp\nimport json\nimport os\nimport shutil\nfrom unittest import mock\n\nimport databricks_cli\nimport pytest\nfrom databricks_cli.configure.provider import DatabricksConfig\n\nimport mlflow\nfrom mlflow import MlflowClient, cli\nfrom mlflow.entities import RunStatus\nfrom mlflow.environment_variables import MLFLOW_TRACKING_URI\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.projects import ExecutionException, databricks\nfrom mlflow.projects.databricks import DatabricksJobRunner, _get_cluster_mlflow_run_cmd\nfrom mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE, ErrorCode\nfrom mlflow.store.tracking.file_store import FileStore\nfrom mlflow.tracking.request_header.default_request_header_provider import (\n    DefaultRequestHeaderProvider,\n)\nfrom mlflow.utils import file_utils\nfrom mlflow.utils.mlflow_tags import (\n    MLFLOW_DATABRICKS_RUN_URL,\n    MLFLOW_DATABRICKS_SHELL_JOB_RUN_ID,\n    MLFLOW_DATABRICKS_WEBAPP_URL,\n)\nfrom mlflow.utils.uri import construct_db_uri_from_profile\n\nfrom tests import helper_functions\nfrom tests.integration.utils import invoke_cli_runner\nfrom tests.projects.utils import TEST_PROJECT_DIR, validate_exit_status\n\n\n@pytest.fixture\ndef runs_cancel_mock():\n    \"\"\"Mocks the Jobs Runs Cancel API request\"\"\"\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner.jobs_runs_cancel\"\n    ) as runs_cancel_mock:\n        runs_cancel_mock.return_value = None\n        yield runs_cancel_mock\n\n\n@pytest.fixture\ndef runs_submit_mock():\n    \"\"\"Mocks the Jobs Runs Submit API request\"\"\"\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner._jobs_runs_submit\",\n        return_value={\"run_id\": \"-1\"},\n    ) as runs_submit_mock:\n        yield runs_submit_mock\n\n\n@pytest.fixture\ndef runs_get_mock():\n    \"\"\"Mocks the Jobs Runs Get API request\"\"\"\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner.jobs_runs_get\"\n    ) as runs_get_mock:\n        yield runs_get_mock\n\n\n@pytest.fixture\ndef databricks_cluster_mlflow_run_cmd_mock():\n    \"\"\"Mocks the Jobs Runs Get API request\"\"\"\n    with mock.patch(\n        \"mlflow.projects.databricks._get_cluster_mlflow_run_cmd\"\n    ) as mlflow_run_cmd_mock:\n        yield mlflow_run_cmd_mock\n\n\n@pytest.fixture\ndef cluster_spec_mock(tmp_path):\n    cluster_spec_handle = tmp_path.joinpath(\"cluster_spec.json\")\n    cluster_spec_handle.write_text(\"{}\")\n    return str(cluster_spec_handle)\n\n\n@pytest.fixture\ndef dbfs_root_mock(tmp_path):\n    return str(tmp_path.joinpath(\"dbfs-root\"))\n\n\n@pytest.fixture\ndef upload_to_dbfs_mock(dbfs_root_mock):\n    def upload_mock_fn(_, src_path, dbfs_uri):\n        mock_dbfs_dst = os.path.join(dbfs_root_mock, dbfs_uri.split(\"/dbfs/\")[1])\n        os.makedirs(os.path.dirname(mock_dbfs_dst))\n        shutil.copy(src_path, mock_dbfs_dst)\n\n    with mock.patch.object(\n        mlflow.projects.databricks.DatabricksJobRunner, \"_upload_to_dbfs\", new=upload_mock_fn\n    ) as upload_mock:\n        yield upload_mock\n\n\n@pytest.fixture\ndef dbfs_path_exists_mock(dbfs_root_mock):  # pylint: disable=unused-argument\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner._dbfs_path_exists\"\n    ) as path_exists_mock:\n        yield path_exists_mock\n\n\n@pytest.fixture\ndef dbfs_mocks(dbfs_path_exists_mock, upload_to_dbfs_mock):  # pylint: disable=unused-argument\n    return\n\n\n@pytest.fixture\ndef before_run_validations_mock():\n    with mock.patch(\"mlflow.projects.databricks.before_run_validations\"):\n        yield\n\n\n@pytest.fixture\ndef set_tag_mock():\n    with mock.patch(\"mlflow.projects.databricks.tracking.MlflowClient\") as m:\n        mlflow_service_mock = mock.Mock(wraps=MlflowClient())\n        m.return_value = mlflow_service_mock\n        yield mlflow_service_mock.set_tag\n\n\ndef _get_mock_run_state(succeeded):\n    if succeeded is None:\n        return {\"life_cycle_state\": \"RUNNING\", \"state_message\": \"\"}\n    run_result_state = \"SUCCESS\" if succeeded else \"FAILED\"\n    return {\"life_cycle_state\": \"TERMINATED\", \"state_message\": \"\", \"result_state\": run_result_state}\n\n\ndef mock_runs_get_result(succeeded):\n    run_state = _get_mock_run_state(succeeded)\n    return {\"state\": run_state, \"run_page_url\": \"test_url\"}\n\n\ndef run_databricks_project(cluster_spec, **kwargs):\n    return mlflow.projects.run(\n        uri=TEST_PROJECT_DIR,\n        backend=\"databricks\",\n        backend_config=cluster_spec,\n        parameters={\"alpha\": \"0.4\"},\n        **kwargs,\n    )\n\n\ndef test_upload_project_to_dbfs(\n    dbfs_root_mock, tmp_path, dbfs_path_exists_mock, upload_to_dbfs_mock\n):  # pylint: disable=unused-argument\n    # Upload project to a mock directory\n    dbfs_path_exists_mock.return_value = False\n    runner = DatabricksJobRunner(databricks_profile_uri=construct_db_uri_from_profile(\"DEFAULT\"))\n    dbfs_uri = runner._upload_project_to_dbfs(\n        project_dir=TEST_PROJECT_DIR, experiment_id=FileStore.DEFAULT_EXPERIMENT_ID\n    )\n    # Get expected tar\n    local_tar_path = os.path.join(dbfs_root_mock, dbfs_uri.split(\"/dbfs/\")[1])\n    expected_tar_path = str(tmp_path.joinpath(\"expected.tar.gz\"))\n    file_utils.make_tarfile(\n        output_filename=expected_tar_path,\n        source_dir=TEST_PROJECT_DIR,\n        archive_name=databricks.DB_TARFILE_ARCHIVE_NAME,\n    )\n    # Extract the tarred project, verify its contents\n    assert filecmp.cmp(local_tar_path, expected_tar_path, shallow=False)\n\n\ndef test_upload_existing_project_to_dbfs(dbfs_path_exists_mock):  # pylint: disable=unused-argument\n    # Check that we don't upload the project if it already exists on DBFS\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner._upload_to_dbfs\"\n    ) as upload_to_dbfs_mock:\n        dbfs_path_exists_mock.return_value = True\n        runner = DatabricksJobRunner(\n            databricks_profile_uri=construct_db_uri_from_profile(\"DEFAULT\")\n        )\n        runner._upload_project_to_dbfs(\n            project_dir=TEST_PROJECT_DIR, experiment_id=FileStore.DEFAULT_EXPERIMENT_ID\n        )\n        assert upload_to_dbfs_mock.call_count == 0\n\n\n@pytest.mark.parametrize(\n    \"response_mock\",\n    [\n        helper_functions.create_mock_response(400, \"Error message but not a JSON string\"),\n        helper_functions.create_mock_response(400, \"\"),\n        helper_functions.create_mock_response(400, None),\n    ],\n)\ndef test_dbfs_path_exists_error_response_handling(response_mock):\n    with mock.patch(\n        \"mlflow.utils.databricks_utils.get_databricks_host_creds\"\n    ) as get_databricks_host_creds_mock, mock.patch(\n        \"mlflow.utils.rest_utils.http_request\"\n    ) as http_request_mock:\n        # given a well formed DatabricksJobRunner\n        # note: databricks_profile is None needed because clients using profile are mocked\n        job_runner = DatabricksJobRunner(databricks_profile_uri=None)\n\n        # when the http request to validate the dbfs path returns a 400 response with an\n        # error message that is either well-formed JSON or not\n        get_databricks_host_creds_mock.return_value = None\n        http_request_mock.return_value = response_mock\n\n        # then _dbfs_path_exists should return a MlflowException\n        with pytest.raises(MlflowException, match=\"API request to check existence of file at DBFS\"):\n            job_runner._dbfs_path_exists(\"some/path\")\n\n\ndef test_run_databricks_validations(\n    tmp_path,\n    monkeypatch,\n    cluster_spec_mock,\n    dbfs_mocks,\n    set_tag_mock,\n):  # pylint: disable=unused-argument\n    \"\"\"\n    Tests that running on Databricks fails before making any API requests if validations fail.\n    \"\"\"\n    monkeypatch.setenvs({\"DATABRICKS_HOST\": \"test-host\", \"DATABRICKS_TOKEN\": \"foo\"})\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner._databricks_api_request\"\n    ) as db_api_req_mock:\n        # Test bad tracking URI\n        mlflow.set_tracking_uri(tmp_path.as_uri())\n        with pytest.raises(ExecutionException, match=\"MLflow tracking URI must be of\"):\n            run_databricks_project(cluster_spec_mock, synchronous=True)\n        assert db_api_req_mock.call_count == 0\n        db_api_req_mock.reset_mock()\n        mlflow_service = MlflowClient()\n        assert len(mlflow_service.search_runs([FileStore.DEFAULT_EXPERIMENT_ID])) == 0\n        mlflow.set_tracking_uri(\"databricks\")\n        # Test misspecified parameters\n        with pytest.raises(\n            ExecutionException, match=\"No value given for missing parameters: 'name'\"\n        ):\n            mlflow.projects.run(\n                TEST_PROJECT_DIR,\n                backend=\"databricks\",\n                entry_point=\"greeter\",\n                backend_config=cluster_spec_mock,\n            )\n        assert db_api_req_mock.call_count == 0\n        db_api_req_mock.reset_mock()\n        # Test bad cluster spec\n        with pytest.raises(ExecutionException, match=\"Backend spec must be provided\"):\n            mlflow.projects.run(\n                TEST_PROJECT_DIR, backend=\"databricks\", synchronous=True, backend_config=None\n            )\n        assert db_api_req_mock.call_count == 0\n        db_api_req_mock.reset_mock()\n        # Test that validations pass with good tracking URIs\n        databricks.before_run_validations(\"http://\", cluster_spec_mock)\n        databricks.before_run_validations(\"databricks\", cluster_spec_mock)\n\n\n@pytest.mark.usefixtures(\n    \"before_run_validations_mock\",\n    \"runs_cancel_mock\",\n    \"dbfs_mocks\",\n    \"databricks_cluster_mlflow_run_cmd_mock\",\n)\ndef test_run_databricks(\n    runs_submit_mock,\n    runs_get_mock,\n    cluster_spec_mock,\n    set_tag_mock,\n    databricks_cluster_mlflow_run_cmd_mock,\n    monkeypatch,\n):\n    \"\"\"Test running on Databricks with mocks.\"\"\"\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    # Test that MLflow gets the correct run status when performing a Databricks run\n    for run_succeeded, expect_status in [(True, RunStatus.FINISHED), (False, RunStatus.FAILED)]:\n        runs_get_mock.return_value = mock_runs_get_result(succeeded=run_succeeded)\n        submitted_run = run_databricks_project(cluster_spec_mock, synchronous=False)\n        assert submitted_run.wait() == run_succeeded\n        assert submitted_run.run_id is not None\n        assert runs_submit_mock.call_count == 1\n        assert databricks_cluster_mlflow_run_cmd_mock.call_count == 1\n        tags = {}\n        for call_args, _ in set_tag_mock.call_args_list:\n            tags[call_args[1]] = call_args[2]\n        assert tags[MLFLOW_DATABRICKS_RUN_URL] == \"test_url\"\n        assert tags[MLFLOW_DATABRICKS_SHELL_JOB_RUN_ID] == \"-1\"\n        assert tags[MLFLOW_DATABRICKS_WEBAPP_URL] == \"test-host\"\n        set_tag_mock.reset_mock()\n        runs_submit_mock.reset_mock()\n        databricks_cluster_mlflow_run_cmd_mock.reset_mock()\n        validate_exit_status(submitted_run.get_status(), expect_status)\n\n\n@pytest.mark.usefixtures(\n    \"before_run_validations_mock\",\n    \"runs_cancel_mock\",\n    \"dbfs_mocks\",\n    \"cluster_spec_mock\",\n    \"set_tag_mock\",\n)\ndef test_run_databricks_cluster_spec_json(runs_submit_mock, runs_get_mock, monkeypatch):\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    runs_get_mock.return_value = mock_runs_get_result(succeeded=True)\n    cluster_spec = {\n        \"spark_version\": \"5.0.x-scala2.11\",\n        \"num_workers\": 2,\n        \"node_type_id\": \"i3.xlarge\",\n    }\n    # Run project synchronously, verify that it succeeds (doesn't throw)\n    run_databricks_project(cluster_spec=cluster_spec, synchronous=True)\n    assert runs_submit_mock.call_count == 1\n    runs_submit_args, _ = runs_submit_mock.call_args_list[0]\n    req_body = runs_submit_args[0]\n    assert req_body[\"new_cluster\"] == cluster_spec\n\n\n@pytest.mark.usefixtures(\n    \"before_run_validations_mock\",\n    \"runs_cancel_mock\",\n    \"dbfs_mocks\",\n    \"cluster_spec_mock\",\n    \"set_tag_mock\",\n)\ndef test_run_databricks_extended_cluster_spec_json(runs_submit_mock, runs_get_mock, monkeypatch):\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    runs_get_mock.return_value = mock_runs_get_result(succeeded=True)\n    new_cluster_spec = {\n        \"spark_version\": \"6.5.x-scala2.11\",\n        \"num_workers\": 2,\n        \"node_type_id\": \"i3.xlarge\",\n    }\n    extra_library = {\"pypi\": {\"package\": \"tensorflow\"}}\n\n    cluster_spec = {\"new_cluster\": new_cluster_spec, \"libraries\": [extra_library]}\n\n    # Run project synchronously, verify that it succeeds (doesn't throw)\n    run_databricks_project(cluster_spec=cluster_spec, synchronous=True)\n    assert runs_submit_mock.call_count == 1\n    runs_submit_args, _ = runs_submit_mock.call_args_list[0]\n    req_body = runs_submit_args[0]\n    assert req_body[\"new_cluster\"] == new_cluster_spec\n    # This does test deep object equivalence\n    assert extra_library in req_body[\"libraries\"]\n\n\n@pytest.mark.usefixtures(\n    \"before_run_validations_mock\",\n    \"runs_cancel_mock\",\n    \"dbfs_mocks\",\n    \"cluster_spec_mock\",\n    \"set_tag_mock\",\n)\ndef test_run_databricks_extended_cluster_spec_json_without_libraries(\n    runs_submit_mock, runs_get_mock, monkeypatch\n):\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    runs_get_mock.return_value = mock_runs_get_result(succeeded=True)\n    new_cluster_spec = {\n        \"spark_version\": \"6.5.x-scala2.11\",\n        \"num_workers\": 2,\n        \"node_type_id\": \"i3.xlarge\",\n    }\n\n    cluster_spec = {\n        \"new_cluster\": new_cluster_spec,\n    }\n\n    # Run project synchronously, verify that it succeeds (doesn't throw)\n    run_databricks_project(cluster_spec=cluster_spec, synchronous=True)\n    assert runs_submit_mock.call_count == 1\n    runs_submit_args, _ = runs_submit_mock.call_args_list[0]\n    req_body = runs_submit_args[0]\n    assert req_body[\"new_cluster\"] == new_cluster_spec\n\n\ndef test_run_databricks_throws_exception_when_spec_uses_existing_cluster(monkeypatch):\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    existing_cluster_spec = {\n        \"existing_cluster_id\": \"1000-123456-clust1\",\n    }\n    with pytest.raises(\n        MlflowException, match=\"execution against existing clusters is not currently supported\"\n    ) as exc:\n        run_databricks_project(cluster_spec=existing_cluster_spec)\n    assert exc.value.error_code == ErrorCode.Name(INVALID_PARAMETER_VALUE)\n\n\ndef test_run_databricks_cancel(\n    before_run_validations_mock,\n    runs_submit_mock,\n    dbfs_mocks,\n    set_tag_mock,\n    runs_cancel_mock,\n    runs_get_mock,\n    cluster_spec_mock,\n    monkeypatch,\n):\n    # pylint: disable=unused-argument\n    # Test that MLflow properly handles Databricks run cancellation. We mock the result of\n    # the runs-get API to indicate run failure so that cancel() exits instead of blocking while\n    # waiting for run status.\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    runs_get_mock.return_value = mock_runs_get_result(succeeded=False)\n    submitted_run = run_databricks_project(cluster_spec_mock, synchronous=False)\n    submitted_run.cancel()\n    validate_exit_status(submitted_run.get_status(), RunStatus.FAILED)\n    assert runs_cancel_mock.call_count == 1\n    # Test that we raise an exception when a blocking Databricks run fails\n    runs_get_mock.return_value = mock_runs_get_result(succeeded=False)\n    with pytest.raises(mlflow.projects.ExecutionException, match=r\"Run \\(ID '.+'\\) failed\"):\n        run_databricks_project(cluster_spec_mock, synchronous=True)\n\n\ndef test_get_tracking_uri_for_run(monkeypatch):\n    mlflow.set_tracking_uri(\"http://some-uri\")\n    assert databricks._get_tracking_uri_for_run() == \"http://some-uri\"\n    mlflow.set_tracking_uri(\"databricks://profile\")\n    assert databricks._get_tracking_uri_for_run() == \"databricks\"\n    mlflow.set_tracking_uri(None)\n    monkeypatch.setenv(MLFLOW_TRACKING_URI.name, \"http://some-uri\")\n    assert mlflow.tracking._tracking_service.utils.get_tracking_uri() == \"http://some-uri\"\n\n\nclass MockProfileConfigProvider:\n    def __init__(self, profile):\n        assert profile == \"my-profile\"\n\n    def get_config(self):\n        return DatabricksConfig.from_password(\"host\", \"user\", \"pass\", insecure=False)\n\n\n@mock.patch(\"requests.Session.request\")\n@mock.patch(\"databricks_cli.configure.provider.get_config\")\n@mock.patch.object(\n    databricks_cli.configure.provider, \"ProfileConfigProvider\", MockProfileConfigProvider\n)\ndef test_databricks_http_request_integration(get_config, request):\n    \"\"\"Confirms that the databricks http request params can in fact be used as an HTTP request\"\"\"\n\n    def confirm_request_params(*args, **kwargs):\n        headers = DefaultRequestHeaderProvider().request_headers()\n        headers[\"Authorization\"] = \"Basic dXNlcjpwYXNz\"\n        assert args == (\"PUT\", \"host/clusters/list\")\n        assert kwargs == {\n            \"headers\": headers,\n            \"verify\": True,\n            \"json\": {\"a\": \"b\"},\n            \"timeout\": 120,\n        }\n        http_response = mock.MagicMock()\n        http_response.status_code = 200\n        http_response.text = '{\"OK\": \"woo\"}'\n        return http_response\n\n    request.side_effect = confirm_request_params\n    get_config.return_value = DatabricksConfig.from_password(\"host\", \"user\", \"pass\", insecure=False)\n\n    response = DatabricksJobRunner(databricks_profile_uri=None)._databricks_api_request(\n        \"/clusters/list\", \"PUT\", json={\"a\": \"b\"}\n    )\n    assert json.loads(response.text) == {\"OK\": \"woo\"}\n    get_config.reset_mock()\n    response = DatabricksJobRunner(\n        databricks_profile_uri=construct_db_uri_from_profile(\"my-profile\")\n    )._databricks_api_request(\"/clusters/list\", \"PUT\", json={\"a\": \"b\"})\n    assert json.loads(response.text) == {\"OK\": \"woo\"}\n    assert get_config.call_count == 0\n\n\n@mock.patch(\"mlflow.utils.databricks_utils.get_databricks_host_creds\")\ndef test_run_databricks_failed(_):\n    text = '{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Node type not supported\"}'\n    with mock.patch(\n        \"mlflow.utils.rest_utils.http_request\", return_value=mock.Mock(text=text, status_code=400)\n    ):\n        runner = DatabricksJobRunner(construct_db_uri_from_profile(\"profile\"))\n        with pytest.raises(\n            MlflowException, match=\"RESOURCE_DOES_NOT_EXIST: Node type not supported\"\n        ):\n            runner._run_shell_command_job(\"/project\", \"command\", {}, {})\n\n\ndef test_run_databricks_generates_valid_mlflow_run_cmd():\n    cmd = _get_cluster_mlflow_run_cmd(\n        project_dir=\"my_project_dir\",\n        run_id=\"hi\",\n        entry_point=\"main\",\n        parameters={\"a\": \"b\"},\n        env_manager=\"conda\",\n    )\n    assert cmd[0] == \"mlflow\"\n    with mock.patch(\"mlflow.projects.run\"):\n        invoke_cli_runner(cli.cli, cmd[1:])\n", "import json\nimport os\nimport posixpath\nimport re\nimport shutil\nimport time\nfrom unittest import mock\nfrom unittest.mock import ANY\n\nimport pytest\nfrom requests.models import Response\n\nfrom mlflow.entities.file_info import FileInfo as FileInfoEntity\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.protos.databricks_artifacts_pb2 import (\n    ArtifactCredentialInfo,\n    ArtifactCredentialType,\n    CompleteMultipartUpload,\n    CreateMultipartUpload,\n    GetCredentialsForRead,\n    GetCredentialsForWrite,\n    GetPresignedUploadPartUrl,\n)\nfrom mlflow.protos.service_pb2 import FileInfo, ListArtifacts\nfrom mlflow.store.artifact.artifact_repository_registry import get_artifact_repository\nfrom mlflow.store.artifact.databricks_artifact_repo import (\n    _MAX_CREDENTIALS_REQUEST_SIZE,\n    DatabricksArtifactRepository,\n)\n\nDATABRICKS_ARTIFACT_REPOSITORY_PACKAGE = \"mlflow.store.artifact.databricks_artifact_repo\"\nCLOUD_ARTIFACT_REPOSITORY_PACKAGE = \"mlflow.store.artifact.cloud_artifact_repo\"\nDATABRICKS_ARTIFACT_REPOSITORY = (\n    f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.DatabricksArtifactRepository\"\n)\nCLOUD_ARTIFACT_REPOSITORY = f\"{CLOUD_ARTIFACT_REPOSITORY_PACKAGE}.CloudArtifactRepository\"\n\nMOCK_AZURE_SIGNED_URI = \"http://this_is_a_mock_sas_for_azure\"\nMOCK_ADLS_GEN2_SIGNED_URI = \"http://this_is_a_mock_sas_for_adls_gen2\"\nMOCK_AWS_SIGNED_URI = \"http://this_is_a_mock_presigned_uri_for_aws?\"\nMOCK_GCP_SIGNED_URL = \"http://this_is_a_mock_signed_url_for_gcp?\"\nMOCK_RUN_ID = \"MOCK-RUN-ID\"\nMOCK_HEADERS = [\n    ArtifactCredentialInfo.HttpHeader(name=\"Mock-Name1\", value=\"Mock-Value1\"),\n    ArtifactCredentialInfo.HttpHeader(name=\"Mock-Name2\", value=\"Mock-Value2\"),\n]\nMOCK_RUN_ROOT_URI = \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\"\nMOCK_SUBDIR = \"subdir/path\"\nMOCK_SUBDIR_ROOT_URI = posixpath.join(MOCK_RUN_ROOT_URI, MOCK_SUBDIR)\n\n\n@pytest.fixture\ndef databricks_artifact_repo():\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\", return_value=MOCK_RUN_ROOT_URI\n    ):\n        return get_artifact_repository(\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\"\n        )\n\n\n@pytest.fixture\ndef test_file(tmp_path):\n    test_file_content = \"Hello \ud83c\udf46\ud83c\udf54\".encode()\n    p = tmp_path.joinpath(\"test.txt\")\n    p.write_bytes(test_file_content)\n    return str(p)\n\n\n@pytest.fixture\ndef test_dir(tmp_path):\n    test_file_content = \"World \ud83c\udf46\ud83c\udf54\ud83c\udf46\".encode()\n    p = tmp_path.joinpath(\"subdir\").joinpath(\"test.txt\")\n    p.parent.mkdir()\n    p.write_bytes(test_file_content)\n    p = tmp_path.joinpath(\"test.txt\")\n    p.write_bytes(test_file_content)\n    p = tmp_path.joinpath(\"empty-file.txt\")\n    p.write_text(\"\")\n    return str(tmp_path)\n\n\ndef test_init_validation_and_cleaning():\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ):\n        # Basic artifact uri\n        repo = get_artifact_repository(\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\"\n        )\n        assert (\n            repo.artifact_uri == \"dbfs:/databricks/mlflow-tracking/\"\n            \"MOCK-EXP/MOCK-RUN-ID/artifacts\"\n        )\n        assert repo.run_id == MOCK_RUN_ID\n        assert repo.run_relative_artifact_repo_root_path == \"\"\n\n        with pytest.raises(MlflowException, match=\"DBFS URI must be of the form dbfs\"):\n            DatabricksArtifactRepository(\"s3://test\")\n        with pytest.raises(MlflowException, match=\"Artifact URI incorrect\"):\n            DatabricksArtifactRepository(\"dbfs:/databricks/mlflow/EXP/RUN/artifact\")\n        with pytest.raises(MlflowException, match=\"DBFS URI must be of the form dbfs\"):\n            DatabricksArtifactRepository(\n                \"dbfs://scope:key@notdatabricks/databricks/mlflow-tracking/experiment/1/run/2\"\n            )\n\n\n@pytest.mark.parametrize(\n    (\"artifact_uri\", \"expected_uri\", \"expected_db_uri\"),\n    [\n        (\n            \"dbfs:/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"dbfs:/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"databricks://getTrackingUriDefault\",\n        ),  # see test body for the mock\n        (\n            \"dbfs://@databricks/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"dbfs:/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"databricks\",\n        ),\n        (\n            \"dbfs://someProfile@databricks/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"dbfs:/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"databricks://someProfile\",\n        ),\n        (\n            \"dbfs://scope:key@databricks/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"dbfs:/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"databricks://scope:key\",\n        ),\n        (\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\",\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\",\n            \"databricks://getTrackingUriDefault\",\n        ),\n        (\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/awesome/path\",\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/awesome/path\",\n            \"databricks://getTrackingUriDefault\",\n        ),\n    ],\n)\ndef test_init_artifact_uri(artifact_uri, expected_uri, expected_db_uri):\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.get_databricks_host_creds\", return_value=None\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\", return_value=\"whatever\"\n    ), mock.patch(\n        \"mlflow.tracking.get_tracking_uri\", return_value=\"databricks://getTrackingUriDefault\"\n    ):\n        repo = DatabricksArtifactRepository(artifact_uri)\n        assert repo.artifact_uri == expected_uri\n        assert repo.databricks_profile_uri == expected_db_uri\n\n\n@pytest.mark.parametrize(\n    (\"artifact_uri\", \"expected_relative_path\"),\n    [\n        (\"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\", \"\"),\n        (\"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts/arty\", \"arty\"),\n        (\n            \"dbfs://prof@databricks/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts/arty\",  # pylint: disable=line-too-long\n            \"arty\",\n        ),\n        (\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/awesome/path\",\n            \"../awesome/path\",\n        ),\n    ],\n)\ndef test_run_relative_artifact_repo_root_path(artifact_uri, expected_relative_path):\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ):\n        # Basic artifact uri\n        repo = get_artifact_repository(artifact_uri)\n        assert repo.run_id == MOCK_RUN_ID\n        assert repo.run_relative_artifact_repo_root_path == expected_relative_path\n\n\ndef test_extract_run_id():\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ):\n        expected_run_id = \"RUN_ID\"\n        repo = get_artifact_repository(\"dbfs:/databricks/mlflow-tracking/EXP/RUN_ID/artifact\")\n        assert repo.run_id == expected_run_id\n        repo = get_artifact_repository(\"dbfs:/databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\")\n        assert repo.run_id == expected_run_id\n        repo = get_artifact_repository(\n            \"dbfs:/databricks///mlflow-tracking///EXP_ID///RUN_ID///artifacts/\"\n        )\n        assert repo.run_id == expected_run_id\n        repo = get_artifact_repository(\n            \"dbfs:/databricks///mlflow-tracking//EXP_ID//RUN_ID///artifacts//\"\n        )\n        assert repo.run_id == expected_run_id\n\n\n@pytest.mark.parametrize(\n    (\"artifact_path\", \"expected_location\"),\n    [(None, \"test.txt\"), (\"output\", \"output/test.txt\"), (\"\", \"test.txt\")],\n)\ndef test_log_artifact_azure(databricks_artifact_repo, test_file, artifact_path, expected_location):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._azure_upload_file\", return_value=None\n    ) as azure_upload_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        azure_upload_mock.assert_called_with(mock_credential_info, test_file, expected_location)\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_azure_with_headers(\n    databricks_artifact_repo, test_file, artifact_path, expected_location\n):\n    mock_azure_headers = {\n        \"x-ms-encryption-scope\": \"test-scope\",\n        \"x-ms-tags\": \"some-tags\",\n        \"x-ms-blob-type\": \"some-type\",\n    }\n    filtered_azure_headers = {\n        \"x-ms-encryption-scope\": \"test-scope\",\n        \"x-ms-tags\": \"some-tags\",\n    }\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI,\n        type=ArtifactCredentialType.AZURE_SAS_URI,\n        headers=[\n            ArtifactCredentialInfo.HttpHeader(name=header_name, value=header_value)\n            for header_name, header_value in mock_azure_headers.items()\n        ],\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        request_mock.assert_called_with(\n            \"put\",\n            f\"{MOCK_AZURE_SIGNED_URI}?comp=blocklist\",\n            data=ANY,\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n\n\ndef test_log_artifact_azure_blob_client_sas_error(databricks_artifact_repo, test_file):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", side_effect=MlflowException(\"MOCK ERROR\")\n    ):\n        with pytest.raises(MlflowException, match=r\"MOCK ERROR\"):\n            databricks_artifact_repo.log_artifact(test_file)\n        get_credential_infos_mock.assert_called_with(GetCredentialsForWrite, MOCK_RUN_ID, ANY)\n\n\n@pytest.mark.parametrize(\n    (\"artifact_path\", \"expected_location\"),\n    [(None, \"test.txt\"), (\"output\", \"output/test.txt\"), (\"\", \"test.txt\")],\n)\ndef test_log_artifact_adls_gen2(\n    databricks_artifact_repo, test_file, artifact_path, expected_location\n):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_ADLS_GEN2_SIGNED_URI,\n        type=ArtifactCredentialType.AZURE_ADLS_GEN2_SAS_URI,\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._azure_adls_gen2_upload_file\", return_value=None\n    ) as azure_adls_gen2_upload_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        azure_adls_gen2_upload_mock.assert_called_with(\n            mock_credential_info, test_file, expected_location\n        )\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_adls_gen2_with_headers(\n    databricks_artifact_repo, test_file, artifact_path, expected_location, monkeypatch\n):\n    mock_azure_headers = {\n        \"x-ms-content-type\": \"test-type\",\n        \"x-ms-owner\": \"some-owner\",\n        \"x-ms-something_not_supported\": \"some-value\",\n    }\n    filtered_azure_headers = {\n        \"x-ms-content-type\": \"test-type\",\n        \"x-ms-owner\": \"some-owner\",\n    }\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_ADLS_GEN2_SIGNED_URI,\n        type=ArtifactCredentialType.AZURE_ADLS_GEN2_SAS_URI,\n        headers=[\n            ArtifactCredentialInfo.HttpHeader(name=header_name, value=header_value)\n            for header_name, header_value in mock_azure_headers.items()\n        ],\n    )\n    monkeypatch.setenv(\"MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE\", \"5\")\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        # test with block size 5\n        request_mock.assert_any_call(\n            \"put\",\n            f\"{MOCK_ADLS_GEN2_SIGNED_URI}?resource=file\",\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n        request_mock.assert_any_call(\n            \"patch\",\n            f\"{MOCK_ADLS_GEN2_SIGNED_URI}?action=append&position=0\",\n            data=ANY,\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n        request_mock.assert_any_call(\n            \"patch\",\n            f\"{MOCK_ADLS_GEN2_SIGNED_URI}?action=append&position=5\",\n            data=ANY,\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n        request_mock.assert_any_call(\n            \"patch\",\n            f\"{MOCK_ADLS_GEN2_SIGNED_URI}?action=append&position=10\",\n            data=ANY,\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n        request_mock.assert_called_with(\n            \"patch\",\n            f\"{MOCK_ADLS_GEN2_SIGNED_URI}?action=flush&position=14\",\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n\n\ndef test_log_artifact_adls_gen2_flush_error(databricks_artifact_repo, test_file):\n    mock_successful_response = Response()\n    mock_successful_response.status_code = 200\n    mock_successful_response.close = lambda: None\n    mock_error_response = MlflowException(\"MOCK ERROR\")\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_ADLS_GEN2_SIGNED_URI,\n        type=ArtifactCredentialType.AZURE_ADLS_GEN2_SAS_URI,\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", side_effect=[mock_successful_response, mock_error_response]\n    ) as request_mock:\n        mock_credential_info = ArtifactCredentialInfo(\n            signed_uri=MOCK_ADLS_GEN2_SIGNED_URI,\n            type=ArtifactCredentialType.AZURE_ADLS_GEN2_SAS_URI,\n        )\n        with pytest.raises(MlflowException, match=r\"MOCK ERROR\"):\n            databricks_artifact_repo.log_artifact(test_file)\n        get_credential_infos_mock.assert_called_with(GetCredentialsForWrite, MOCK_RUN_ID, ANY)\n        assert request_mock.mock_calls == [\n            mock.call(\n                \"put\",\n                f\"{MOCK_ADLS_GEN2_SIGNED_URI}?resource=file\",\n                headers={},\n                timeout=None,\n            ),\n            mock.call(\n                \"patch\",\n                f\"{MOCK_ADLS_GEN2_SIGNED_URI}?action=append&position=0&flush=true\",\n                data=ANY,\n                headers={},\n                timeout=None,\n            ),\n        ]\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_aws(databricks_artifact_repo, test_file, artifact_path, expected_location):\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI, type=ArtifactCredentialType.AWS_PRESIGNED_URL\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        request_mock.assert_called_with(\n            \"put\", MOCK_AWS_SIGNED_URI, data=ANY, headers={}, timeout=None\n        )\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_aws_with_headers(\n    databricks_artifact_repo, test_file, artifact_path, expected_location\n):\n    expected_headers = {header.name: header.value for header in MOCK_HEADERS}\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI,\n        type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n        headers=MOCK_HEADERS,\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        request_mock.assert_called_with(\n            \"put\", MOCK_AWS_SIGNED_URI, data=ANY, headers=expected_headers, timeout=None\n        )\n\n\ndef test_log_artifact_aws_presigned_url_error(databricks_artifact_repo, test_file):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI, type=ArtifactCredentialType.AWS_PRESIGNED_URL\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", side_effect=MlflowException(\"MOCK ERROR\")\n    ):\n        with pytest.raises(MlflowException, match=\"MOCK ERROR\"):\n            databricks_artifact_repo.log_artifact(test_file)\n        get_credential_infos_mock.assert_called_with(GetCredentialsForWrite, MOCK_RUN_ID, ANY)\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_gcp(databricks_artifact_repo, test_file, artifact_path, expected_location):\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_GCP_SIGNED_URL, type=ArtifactCredentialType.GCP_SIGNED_URL\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        request_mock.assert_called_with(\n            \"put\", MOCK_GCP_SIGNED_URL, data=ANY, headers={}, timeout=None\n        )\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_gcp_with_headers(\n    databricks_artifact_repo, test_file, artifact_path, expected_location\n):\n    expected_headers = {header.name: header.value for header in MOCK_HEADERS}\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_GCP_SIGNED_URL,\n        type=ArtifactCredentialType.GCP_SIGNED_URL,\n        headers=MOCK_HEADERS,\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        request_mock.assert_called_with(\n            \"put\", MOCK_GCP_SIGNED_URL, data=ANY, headers=expected_headers, timeout=None\n        )\n\n\ndef test_log_artifact_gcp_presigned_url_error(databricks_artifact_repo, test_file):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_GCP_SIGNED_URL, type=ArtifactCredentialType.GCP_SIGNED_URL\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", side_effect=MlflowException(\"MOCK ERROR\")\n    ):\n        with pytest.raises(MlflowException, match=\"MOCK ERROR\"):\n            databricks_artifact_repo.log_artifact(test_file)\n        get_credential_infos_mock.assert_called_with(GetCredentialsForWrite, MOCK_RUN_ID, ANY)\n\n\n@pytest.mark.parametrize(\n    (\"artifact_path\", \"expected_location\"),\n    [\n        (None, posixpath.join(MOCK_SUBDIR, \"test.txt\")),\n        (\"test_path\", posixpath.join(MOCK_SUBDIR, \"test_path/test.txt\")),\n    ],\n)\ndef test_log_artifact_with_relative_path(test_file, artifact_path, expected_location):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._upload_to_cloud\", return_value=None\n    ) as upload_mock:\n        databricks_artifact_repo = get_artifact_repository(MOCK_SUBDIR_ROOT_URI)\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        artifact_file_path = posixpath.join(artifact_path or \"\", os.path.basename(test_file))\n        upload_mock.assert_called_with(\n            cloud_credential_info=mock_credential_info,\n            src_file_path=test_file,\n            artifact_file_path=artifact_file_path,\n        )\n\n\ndef test_list_artifacts(databricks_artifact_repo):\n    list_artifact_file_proto_mock = [FileInfo(path=\"a.txt\", is_dir=False, file_size=0)]\n    list_artifacts_dir_proto_mock = [\n        FileInfo(path=\"test/a.txt\", is_dir=False, file_size=100),\n        FileInfo(path=\"test/dir\", is_dir=True, file_size=0),\n    ]\n    list_artifact_response_proto = ListArtifacts.Response(\n        root_uri=\"\", files=list_artifacts_dir_proto_mock, next_page_token=None\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        return_value=list_artifact_response_proto,\n    ):\n        artifacts = databricks_artifact_repo.list_artifacts(\"test/\")\n        assert isinstance(artifacts, list)\n        assert isinstance(artifacts[0], FileInfoEntity)\n        assert len(artifacts) == 2\n        assert artifacts[0].path == \"test/a.txt\"\n        assert artifacts[0].is_dir is False\n        assert artifacts[0].file_size == 100\n        assert artifacts[1].path == \"test/dir\"\n        assert artifacts[1].is_dir is True\n        assert artifacts[1].file_size is None\n\n        # Calling list_artifacts() on a path that's a file should return an empty list\n        list_artifact_response_proto = ListArtifacts.Response(\n            root_uri=\"\", files=list_artifact_file_proto_mock\n        )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        return_value=list_artifact_response_proto,\n    ):\n        artifacts = databricks_artifact_repo.list_artifacts(\"a.txt\")\n        assert len(artifacts) == 0\n\n\ndef test_list_artifacts_with_relative_path():\n    list_artifact_file_proto_mock = [\n        FileInfo(path=posixpath.join(MOCK_SUBDIR, \"a.txt\"), is_dir=False, file_size=0)\n    ]\n    list_artifacts_dir_proto_mock = [\n        FileInfo(path=posixpath.join(MOCK_SUBDIR, \"test/a.txt\"), is_dir=False, file_size=100),\n        FileInfo(path=posixpath.join(MOCK_SUBDIR, \"test/dir\"), is_dir=True, file_size=0),\n    ]\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\", return_value=None\n    ) as message_mock:\n        list_artifact_response_proto = ListArtifacts.Response(\n            root_uri=\"\", files=list_artifacts_dir_proto_mock, next_page_token=None\n        )\n        with mock.patch(\n            f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n            return_value=list_artifact_response_proto,\n        ):\n            databricks_artifact_repo = get_artifact_repository(MOCK_SUBDIR_ROOT_URI)\n            artifacts = databricks_artifact_repo.list_artifacts(\"test\")\n            assert isinstance(artifacts, list)\n            assert isinstance(artifacts[0], FileInfoEntity)\n            assert len(artifacts) == 2\n            assert artifacts[0].path == \"test/a.txt\"\n            assert artifacts[0].is_dir is False\n            assert artifacts[0].file_size == 100\n            assert artifacts[1].path == \"test/dir\"\n            assert artifacts[1].is_dir is True\n            assert artifacts[1].file_size is None\n            message_mock.assert_called_with(\n                ListArtifacts(run_id=MOCK_RUN_ID, path=posixpath.join(MOCK_SUBDIR, \"test\"))\n            )\n\n        # Calling list_artifacts() on a relative path that's a file should return an empty list\n        with mock.patch(\n            f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n            return_value=ListArtifacts.Response(\n                root_uri=\"\", files=list_artifact_file_proto_mock, next_page_token=None\n            ),\n        ):\n            artifacts = databricks_artifact_repo.list_artifacts(\"a.txt\")\n            assert len(artifacts) == 0\n\n\ndef test_list_artifacts_handles_pagination(databricks_artifact_repo):\n    list_artifacts_proto_mock_1 = [\n        FileInfo(path=\"a.txt\", is_dir=False, file_size=100),\n        FileInfo(path=\"b\", is_dir=True, file_size=0),\n    ]\n    list_artifacts_proto_mock_2 = [\n        FileInfo(path=\"c.txt\", is_dir=False, file_size=100),\n        FileInfo(path=\"d\", is_dir=True, file_size=0),\n    ]\n    list_artifacts_proto_mock_3 = [\n        FileInfo(path=\"e.txt\", is_dir=False, file_size=100),\n        FileInfo(path=\"f\", is_dir=True, file_size=0),\n    ]\n    list_artifacts_proto_mock_4 = []\n    list_artifact_paginated_response_protos = [\n        ListArtifacts.Response(root_uri=\"\", files=list_artifacts_proto_mock_1, next_page_token=\"2\"),\n        ListArtifacts.Response(root_uri=\"\", files=list_artifacts_proto_mock_2, next_page_token=\"4\"),\n        ListArtifacts.Response(root_uri=\"\", files=list_artifacts_proto_mock_3, next_page_token=\"6\"),\n        ListArtifacts.Response(root_uri=\"\", files=list_artifacts_proto_mock_4, next_page_token=\"8\"),\n    ]\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\", return_value=None\n    ) as message_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=list_artifact_paginated_response_protos,\n    ):\n        artifacts = databricks_artifact_repo.list_artifacts()\n        assert {file.path for file in artifacts} == {\"a.txt\", \"b\", \"c.txt\", \"d\", \"e.txt\", \"f\"}\n        calls = [\n            mock.call(ListArtifacts(run_id=MOCK_RUN_ID, path=\"\")),\n            mock.call(ListArtifacts(run_id=MOCK_RUN_ID, path=\"\", page_token=\"2\")),\n            mock.call(ListArtifacts(run_id=MOCK_RUN_ID, path=\"\", page_token=\"4\")),\n            mock.call(ListArtifacts(run_id=MOCK_RUN_ID, path=\"\", page_token=\"6\")),\n        ]\n        message_mock.assert_has_calls(calls)\n\n\ndef test_get_read_credential_infos_handles_pagination(databricks_artifact_repo):\n    \"\"\"\n    Verifies that the `get_read_credential_infos` method, which is used to resolve read access\n    credentials for a collection of artifacts, handles paginated responses properly, issuing\n    incremental requests until all pages have been consumed\n    \"\"\"\n    credential_infos_mock_1 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_1\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        ),\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_2\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        ),\n    ]\n    credential_infos_mock_2 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_3\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n    ]\n    credential_infos_mock_3 = []\n    get_credentials_for_read_responses = [\n        GetCredentialsForRead.Response(\n            credential_infos=credential_infos_mock_1, next_page_token=\"2\"\n        ),\n        GetCredentialsForRead.Response(\n            credential_infos=credential_infos_mock_2, next_page_token=\"3\"\n        ),\n        GetCredentialsForRead.Response(credential_infos=credential_infos_mock_3),\n    ]\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\"\n    ) as message_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=get_credentials_for_read_responses,\n    ) as call_endpoint_mock:\n        read_credential_infos = databricks_artifact_repo._get_read_credential_infos([\"testpath\"])\n        assert read_credential_infos == credential_infos_mock_1 + credential_infos_mock_2\n        message_mock.assert_has_calls(\n            [\n                mock.call(GetCredentialsForRead(run_id=MOCK_RUN_ID, path=[\"testpath\"])),\n                mock.call(\n                    GetCredentialsForRead(run_id=MOCK_RUN_ID, path=[\"testpath\"], page_token=\"2\")\n                ),\n                mock.call(\n                    GetCredentialsForRead(run_id=MOCK_RUN_ID, path=[\"testpath\"], page_token=\"3\")\n                ),\n            ]\n        )\n        assert call_endpoint_mock.call_count == 3\n\n\ndef test_get_read_credential_infos_respects_max_request_size(databricks_artifact_repo):\n    \"\"\"\n    Verifies that the `_get_read_credential_infos` method, which is used to resolve read access\n    credentials for a collection of artifacts, handles paginated responses properly, issuing\n    incremental requests until all pages have been consumed\n    \"\"\"\n    assert _MAX_CREDENTIALS_REQUEST_SIZE == 2000, (\n        \"The maximum request size configured by the client should be consistent with the\"\n        \" Databricks backend. Only update this value of the backend limit has changed.\"\n    )\n\n    # Create 3 chunks of paths, two of which have the maximum request size and one of which\n    # is smaller than the maximum chunk size. Aggregate and pass these to\n    # `_get_read_credential_infos`, validating that this method decomposes the aggregate\n    # list into these expected chunks and makes 3 separate requests\n    paths_chunk_1 = [\"path1\"] * _MAX_CREDENTIALS_REQUEST_SIZE\n    paths_chunk_2 = [\"path2\"] * _MAX_CREDENTIALS_REQUEST_SIZE\n    paths_chunk_3 = [\"path3\"] * 5\n    credential_infos_mock_1 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_1\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(_MAX_CREDENTIALS_REQUEST_SIZE)\n    ]\n    credential_infos_mock_2 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_2\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(_MAX_CREDENTIALS_REQUEST_SIZE)\n    ]\n    credential_infos_mock_3 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_3\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(5)\n    ]\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\"\n    ) as message_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=[\n            GetCredentialsForRead.Response(credential_infos=credential_infos_mock_1),\n            GetCredentialsForRead.Response(credential_infos=credential_infos_mock_2),\n            GetCredentialsForRead.Response(credential_infos=credential_infos_mock_3),\n        ],\n    ) as call_endpoint_mock:\n        databricks_artifact_repo._get_read_credential_infos(\n            paths_chunk_1 + paths_chunk_2 + paths_chunk_3\n        )\n        assert call_endpoint_mock.call_count == 3\n        assert message_mock.call_count == 3\n        message_mock.assert_has_calls(\n            [\n                mock.call(GetCredentialsForRead(run_id=MOCK_RUN_ID, path=paths_chunk_1)),\n                mock.call(GetCredentialsForRead(run_id=MOCK_RUN_ID, path=paths_chunk_2)),\n                mock.call(GetCredentialsForRead(run_id=MOCK_RUN_ID, path=paths_chunk_3)),\n            ]\n        )\n\n\ndef test_get_write_credential_infos_handles_pagination(databricks_artifact_repo):\n    \"\"\"\n    Verifies that the `_get_write_credential_infos` method, which is used to resolve write\n    access credentials for a collection of artifacts, handles paginated responses properly,\n    issuing incremental requests until all pages have been consumed\n    \"\"\"\n    credential_infos_mock_1 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_1\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        ),\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_2\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        ),\n    ]\n    credential_infos_mock_2 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_3\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n    ]\n    credential_infos_mock_3 = []\n    get_credentials_for_write_responses = [\n        GetCredentialsForWrite.Response(\n            credential_infos=credential_infos_mock_1, next_page_token=\"2\"\n        ),\n        GetCredentialsForWrite.Response(\n            credential_infos=credential_infos_mock_2, next_page_token=\"3\"\n        ),\n        GetCredentialsForWrite.Response(credential_infos=credential_infos_mock_3),\n    ]\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\"\n    ) as message_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=get_credentials_for_write_responses,\n    ) as call_endpoint_mock:\n        write_credential_infos = databricks_artifact_repo._get_write_credential_infos([\"testpath\"])\n        assert write_credential_infos == credential_infos_mock_1 + credential_infos_mock_2\n        message_mock.assert_has_calls(\n            [\n                mock.call(GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=[\"testpath\"])),\n                mock.call(\n                    GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=[\"testpath\"], page_token=\"2\")\n                ),\n                mock.call(\n                    GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=[\"testpath\"], page_token=\"3\")\n                ),\n            ]\n        )\n        assert call_endpoint_mock.call_count == 3\n\n\ndef test_get_write_credential_infos_respects_max_request_size(databricks_artifact_repo):\n    \"\"\"\n    Verifies that the `_get_write_credential_infos` method, which is used to resolve write\n    access credentials for a collection of artifacts, batches requests according to a maximum\n    request size configured by the backend\n    \"\"\"\n    # Create 3 chunks of paths, two of which have the maximum request size and one of which\n    # is smaller than the maximum chunk size. Aggregate and pass these to\n    # `_get_write_credential_infos`, validating that this method decomposes the aggregate\n    # list into these expected chunks and makes 3 separate requests\n    paths_chunk_1 = [\"path1\"] * 2000\n    paths_chunk_2 = [\"path2\"] * 2000\n    paths_chunk_3 = [\"path3\"] * 5\n    credential_infos_mock_1 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_1\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(2000)\n    ]\n    credential_infos_mock_2 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_2\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(2000)\n    ]\n    credential_infos_mock_3 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_3\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(5)\n    ]\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\"\n    ) as message_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=[\n            GetCredentialsForWrite.Response(credential_infos=credential_infos_mock_1),\n            GetCredentialsForWrite.Response(credential_infos=credential_infos_mock_2),\n            GetCredentialsForWrite.Response(credential_infos=credential_infos_mock_3),\n        ],\n    ) as call_endpoint_mock:\n        databricks_artifact_repo._get_write_credential_infos(\n            paths_chunk_1 + paths_chunk_2 + paths_chunk_3\n        )\n        assert call_endpoint_mock.call_count == message_mock.call_count == 3\n        message_mock.assert_has_calls(\n            [\n                mock.call(GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=paths_chunk_1)),\n                mock.call(GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=paths_chunk_2)),\n                mock.call(GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=paths_chunk_3)),\n            ]\n        )\n\n\n@pytest.mark.parametrize(\n    (\"remote_file_path\", \"local_path\"),\n    [\n        (\"test_file.txt\", \"\"),\n        (\"test_file.txt\", None),\n        (\"output/test_file\", None),\n    ],\n)\ndef test_databricks_download_file(databricks_artifact_repo, remote_file_path, local_path):\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_read_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            )\n        ],\n    ) as read_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\", return_value=[]\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.download_file_using_http_uri\", return_value=None\n    ) as download_mock:\n        databricks_artifact_repo.download_artifacts(remote_file_path, local_path)\n        read_credential_infos_mock.assert_called_with(remote_file_path)\n        download_mock.assert_called_with(MOCK_AZURE_SIGNED_URI, ANY, ANY, {})\n\n\n@pytest.mark.parametrize(\n    (\"remote_file_path\", \"local_path\"), [(\"test_file.txt\", \"\"), (\"test_file.txt\", None)]\n)\ndef test_databricks_download_file_with_relative_path(remote_file_path, local_path):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\", return_value=[]\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.download_file_using_http_uri\", return_value=None\n    ) as download_mock:\n        databricks_artifact_repo = get_artifact_repository(MOCK_SUBDIR_ROOT_URI)\n        databricks_artifact_repo.download_artifacts(remote_file_path, local_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForRead, MOCK_RUN_ID, [posixpath.join(MOCK_SUBDIR, remote_file_path)]\n        )\n        download_mock.assert_called_with(MOCK_AZURE_SIGNED_URI, ANY, ANY, ANY)\n\n\n@pytest.mark.parametrize(\n    (\"file_size\", \"is_parallel_download\"),\n    [(None, False), (100, False), (500 * 1024**2 - 1, False), (500 * 1024**2, True)],\n)\ndef test_databricks_download_file_in_parallel_when_necessary(\n    databricks_artifact_repo, file_size, is_parallel_download\n):\n    remote_file_path = \"file_1.txt\"\n    list_artifacts_result = (\n        [FileInfo(path=remote_file_path, is_dir=False, file_size=file_size)] if file_size else []\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\",\n        return_value=list_artifacts_result,\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._download_from_cloud\", return_value=None\n    ) as download_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._parallelized_download_from_cloud\", return_value=None\n    ) as parallel_download_mock:\n        databricks_artifact_repo.download_artifacts(\"\")\n        if is_parallel_download:\n            parallel_download_mock.assert_called_with(file_size, remote_file_path, ANY)\n        else:\n            download_mock.assert_called()\n\n\ndef test_databricks_download_file_get_request_fail(databricks_artifact_repo, test_file):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_read_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as read_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\", return_value=[]\n    ), mock.patch(\n        \"requests.Session.request\", side_effect=MlflowException(\"MOCK ERROR\")\n    ):\n        with pytest.raises(MlflowException, match=r\"MOCK ERROR\"):\n            databricks_artifact_repo.download_artifacts(test_file)\n        read_credential_infos_mock.assert_called_with(test_file)\n\n\ndef test_download_artifacts_awaits_download_completion(databricks_artifact_repo, tmp_path):\n    \"\"\"\n    Verifies that all asynchronous artifact downloads are joined before `download_artifacts()`\n    returns a result to the caller\n    \"\"\"\n\n    def mock_download_from_cloud(\n        cloud_credential_info,  # pylint: disable=unused-argument\n        dst_local_file_path,\n    ):\n        # Sleep in order to simulate a longer-running asynchronous download\n        time.sleep(2)\n        with open(dst_local_file_path, \"w\") as f:\n            f.write(\"content\")\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_read_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\",\n        return_value=[\n            FileInfo(path=\"file_1.txt\", is_dir=False, file_size=100),\n            FileInfo(path=\"file_2.txt\", is_dir=False, file_size=0),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._download_from_cloud\",\n        side_effect=mock_download_from_cloud,\n    ):\n        databricks_artifact_repo.download_artifacts(\"test_path\", str(tmp_path))\n        expected_file1_path = os.path.join(str(tmp_path), \"file_1.txt\")\n        expected_file2_path = os.path.join(str(tmp_path), \"file_2.txt\")\n        for path in [expected_file1_path, expected_file2_path]:\n            assert os.path.exists(path)\n            with open(path) as f:\n                assert f.read() == \"content\"\n\n\ndef test_artifact_logging(databricks_artifact_repo, tmp_path):\n    \"\"\"\n    Verifies that `log_artifact()` and `log_artifacts()` initiate all expected asynchronous\n    artifact uploads and await their completion before returning results to the caller\n    \"\"\"\n    src_dir = tmp_path.joinpath(\"src\")\n    src_dir.mkdir()\n    src_file1_path = src_dir.joinpath(\"file_1.txt\")\n    src_file1_path.write_text(\"file1\")\n    src_file2_path = src_dir.joinpath(\"file_2.txt\")\n    src_file2_path.write_text(\"file2\")\n\n    dst_dir = tmp_path.joinpath(\"dst\")\n    dst_dir.mkdir()\n\n    def mock_upload_to_cloud(\n        cloud_credential_info,  # pylint: disable=unused-argument\n        src_file_path,\n        artifact_file_path,\n    ):\n        # Sleep in order to simulate a longer-running asynchronous upload\n        time.sleep(2)\n        dst_run_relative_artifact_path = dst_dir.joinpath(artifact_file_path)\n        dst_run_relative_artifact_path.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copy2(src=src_file_path, dst=dst_run_relative_artifact_path)\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._upload_to_cloud\", side_effect=mock_upload_to_cloud\n    ):\n        databricks_artifact_repo.log_artifacts(src_dir, \"dir_artifact\")\n\n        expected_dst_dir_file1_path = os.path.join(dst_dir, \"dir_artifact\", \"file_1.txt\")\n        expected_dst_dir_file2_path = os.path.join(dst_dir, \"dir_artifact\", \"file_2.txt\")\n        assert os.path.exists(expected_dst_dir_file1_path)\n        assert os.path.exists(expected_dst_dir_file2_path)\n        with open(expected_dst_dir_file1_path) as f:\n            assert f.read() == \"file1\"\n        with open(expected_dst_dir_file2_path) as f:\n            assert f.read() == \"file2\"\n\n        databricks_artifact_repo.log_artifact(src_file1_path)\n\n        expected_dst_file_path = os.path.join(dst_dir, \"file_1.txt\")\n        assert os.path.exists(expected_dst_file_path)\n        with open(expected_dst_file_path) as f:\n            assert f.read() == \"file1\"\n\n\ndef test_artifact_logging_chunks_upload_list(databricks_artifact_repo, tmp_path):\n    \"\"\"\n    Verifies that write credentials are fetched in chunks rather than all at once.\n    \"\"\"\n    src_dir = tmp_path.joinpath(\"src\")\n    src_dir.mkdir()\n    for i in range(10):\n        src_dir.joinpath(f\"file_{i}.txt\").write_text(f\"file{i}\")\n    dst_dir = tmp_path.joinpath(\"dst\")\n    dst_dir.mkdir()\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n        ],\n    ) as mock_get_write_creds, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._upload_to_cloud\"\n    ), mock.patch(\n        f\"{CLOUD_ARTIFACT_REPOSITORY_PACKAGE}._ARTIFACT_UPLOAD_BATCH_SIZE\", 2\n    ):\n        databricks_artifact_repo.log_artifacts(src_dir, \"dir_artifact\")\n\n        assert mock_get_write_creds.call_count == 5\n        assert all(\n            len(call[1][\"remote_file_paths\"]) == 2 for call in mock_get_write_creds.call_args_list\n        )\n\n\ndef test_download_artifacts_provides_failure_info(databricks_artifact_repo):\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_read_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\",\n        return_value=[\n            FileInfo(path=\"file_1.txt\", is_dir=False, file_size=100),\n            FileInfo(path=\"file_2.txt\", is_dir=False, file_size=0),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._download_from_cloud\",\n        side_effect=[\n            MlflowException(\"MOCK ERROR 1\"),\n            MlflowException(\"MOCK ERROR 2\"),\n        ],\n    ):\n        match = r\"The following failures occurred while downloading one or more artifacts.+\"\n        with pytest.raises(MlflowException, match=match) as exc:\n            databricks_artifact_repo.download_artifacts(\"test_path\")\n\n        err_msg = str(exc.value)\n        assert MOCK_RUN_ROOT_URI in err_msg\n        assert \"file_1.txt\" in err_msg\n        assert \"MOCK ERROR 1\" in err_msg\n        assert \"file_2.txt\" in err_msg\n        assert \"MOCK ERROR 2\" in err_msg\n\n\ndef test_log_artifacts_provides_failure_info(databricks_artifact_repo, tmp_path):\n    src_file1_path = os.path.join(tmp_path, \"file_1.txt\")\n    with open(src_file1_path, \"w\") as f:\n        f.write(\"file1\")\n    src_file2_path = os.path.join(tmp_path, \"file_2.txt\")\n    with open(src_file2_path, \"w\") as f:\n        f.write(\"file2\")\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._upload_to_cloud\",\n        side_effect=[\n            MlflowException(\"MOCK ERROR 1\"),\n            MlflowException(\"MOCK ERROR 2\"),\n        ],\n    ):\n        match = (\n            r\"The following failures occurred while uploading one or more artifacts.+\"\n            r\"MOCK ERROR 1.+\"\n            r\"MOCK ERROR 2\"\n        )\n        with pytest.raises(MlflowException, match=match) as exc:\n            databricks_artifact_repo.log_artifacts(str(tmp_path), \"test_artifacts\")\n\n        err_msg = str(exc.value)\n        assert MOCK_RUN_ROOT_URI in err_msg\n        assert \"file_1.txt\" in err_msg\n        assert \"MOCK ERROR 1\" in err_msg\n        assert \"file_2.txt\" in err_msg\n        assert \"MOCK ERROR 2\" in err_msg\n\n\n@pytest.fixture\ndef mock_chunk_size(monkeypatch):\n    # Use a smaller chunk size for faster comparison\n    chunk_size = 10\n    monkeypatch.setenv(\"MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE\", str(chunk_size))\n    return chunk_size\n\n\n@pytest.fixture\ndef large_file(tmp_path, mock_chunk_size):\n    path = tmp_path.joinpath(\"large_file\")\n    with path.open(\"a\") as f:\n        f.write(\"a\" * mock_chunk_size)\n        f.write(\"b\" * mock_chunk_size)\n    return path\n\n\ndef extract_part_number(url):\n    return int(re.search(r\"partNumber=(\\d+)\", url).group(1))\n\n\ndef mock_request(method, url, *_args, **_kwargs):\n    resp = Response()\n    resp.status_code = 200\n    resp.close = lambda: None\n    if method.lower() == \"delete\":\n        # Abort-multipart-upload request\n        return resp\n    elif method.lower() == \"put\":\n        # Upload-part request\n        part_number = extract_part_number(url)\n        resp.headers = {\"ETag\": f\"etag-{part_number}\"}\n        return resp\n    else:\n        raise Exception(\"Unreachable\")\n\n\ndef test_multipart_upload(databricks_artifact_repo, large_file, mock_chunk_size):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI, type=ArtifactCredentialType.AWS_PRESIGNED_URL\n    )\n    mock_upload_id = \"upload_id\"\n    create_mpu_response = CreateMultipartUpload.Response(\n        upload_id=mock_upload_id,\n        upload_credential_infos=[\n            ArtifactCredentialInfo(\n                signed_uri=f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n                headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=f\"part-{i + 1}\")],\n            )\n            for i in range(2)\n        ],\n        abort_credential_info=ArtifactCredentialInfo(\n            signed_uri=f\"{MOCK_AWS_SIGNED_URI}uploadId=abort\",\n            type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n            headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=\"abort\")],\n        ),\n    )\n    complete_mpu_response = CompleteMultipartUpload.Response()\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[mock_credential_info],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=[create_mpu_response, complete_mpu_response],\n    ) as call_endpoint_mock, mock.patch(\n        \"requests.Session.request\", side_effect=mock_request\n    ) as http_request_mock:\n        databricks_artifact_repo.log_artifact(large_file)\n        with large_file.open(\"rb\") as f:\n            expected_calls = [\n                mock.call(\n                    \"put\",\n                    f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                    data=f.read(mock_chunk_size),\n                    headers={\"header\": f\"part-{i + 1}\"},\n                    timeout=None,\n                )\n                for i in range(2)\n            ]\n        # The upload-part requests are sent in parallel, so the order of the calls is not\n        # deterministic\n        assert sorted(http_request_mock.call_args_list, key=lambda c: c.args[1]) == expected_calls\n        complete_request_body = json.loads(call_endpoint_mock.call_args_list[-1].args[-1])\n        assert complete_request_body[\"upload_id\"] == mock_upload_id\n        assert complete_request_body[\"part_etags\"] == [\n            {\"part_number\": 1, \"etag\": \"etag-1\"},\n            {\"part_number\": 2, \"etag\": \"etag-2\"},\n        ]\n\n\n# The first request will fail with a 403, and the second will succeed\nSTATUS_CODE_GENERATOR = (s for s in (403, 200))\n\n\ndef mock_request_retry(method, url, *_args, **_kwargs):\n    resp = Response()\n    resp.status_code = 200\n    resp.close = lambda: None\n    if method.lower() == \"delete\":\n        # Abort-multipart-upload request\n        return resp\n    elif method.lower() == \"put\":\n        # Upload-part request\n        part_number = extract_part_number(url)\n        resp.headers = {\"ETag\": f\"etag-{part_number}\"}\n        # To ensure the upload-part retry logic works correctly,\n        # make the first attempt of the second part upload fail by responding with a 403,\n        # then make the second attempt succeed by responding with a 200\n        if part_number == 2:\n            status_code = next(STATUS_CODE_GENERATOR)\n            resp.status_code = status_code\n        return resp\n    else:\n        raise Exception(\"Unreachable\")\n\n\ndef test_multipart_upload_retry_part_upload(databricks_artifact_repo, large_file, mock_chunk_size):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI, type=ArtifactCredentialType.AWS_PRESIGNED_URL\n    )\n    mock_upload_id = \"upload_id\"\n    create_mpu_response = CreateMultipartUpload.Response(\n        upload_id=mock_upload_id,\n        upload_credential_infos=[\n            ArtifactCredentialInfo(\n                signed_uri=f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n                headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=f\"part-{i + 1}\")],\n            )\n            for i in range(2)\n        ],\n        abort_credential_info=ArtifactCredentialInfo(\n            signed_uri=f\"{MOCK_AWS_SIGNED_URI}uploadId=abort\",\n            type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n            headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=\"abort\")],\n        ),\n    )\n    part_upload_url_response = GetPresignedUploadPartUrl.Response(\n        upload_credential_info=ArtifactCredentialInfo(\n            signed_uri=f\"{MOCK_AWS_SIGNED_URI}partNumber=2\",\n            type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n            headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=\"part-2\")],\n        ),\n    )\n    complete_mpu_response = CompleteMultipartUpload.Response()\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[mock_credential_info],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=[create_mpu_response, part_upload_url_response, complete_mpu_response],\n    ) as call_endpoint_mock, mock.patch(\n        \"requests.Session.request\", side_effect=mock_request_retry\n    ) as http_request_mock:\n        databricks_artifact_repo.log_artifact(large_file)\n\n        with large_file.open(\"rb\") as f:\n            expected_calls = [\n                mock.call(\n                    \"put\",\n                    f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                    data=f.read(mock_chunk_size),\n                    headers={\"header\": f\"part-{i + 1}\"},\n                    timeout=None,\n                )\n                for i in range(2)\n            ]\n        expected_calls += expected_calls[-1:]  # Append the second part upload call\n        # The upload-part requests are sent in parallel, so the order of the calls is not\n        # deterministic\n        assert sorted(http_request_mock.call_args_list, key=lambda c: c.args[1]) == expected_calls\n        complete_request_body = json.loads(call_endpoint_mock.call_args_list[-1].args[-1])\n        assert complete_request_body[\"upload_id\"] == mock_upload_id\n        assert complete_request_body[\"part_etags\"] == [\n            {\"part_number\": 1, \"etag\": \"etag-1\"},\n            {\"part_number\": 2, \"etag\": \"etag-2\"},\n        ]\n\n\ndef test_multipart_upload_abort(databricks_artifact_repo, large_file, mock_chunk_size):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI,\n        type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n    )\n    mock_upload_id = \"upload_id\"\n    create_mpu_response = CreateMultipartUpload.Response(\n        upload_id=mock_upload_id,\n        upload_credential_infos=[\n            ArtifactCredentialInfo(\n                signed_uri=f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n                headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=f\"part-{i + 1}\")],\n            )\n            for i in range(2)\n        ],\n        abort_credential_info=ArtifactCredentialInfo(\n            signed_uri=f\"{MOCK_AWS_SIGNED_URI}uploadId=abort\",\n            type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n            headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=\"abort\")],\n        ),\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[mock_credential_info],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=[create_mpu_response, Exception(\"Failed to complete multipart upload\")],\n    ) as call_endpoint_mock, mock.patch(\n        \"requests.Session.request\", side_effect=mock_request\n    ) as http_request_mock:\n        with pytest.raises(Exception, match=\"Failed to complete multipart upload\"):\n            databricks_artifact_repo.log_artifact(large_file)\n\n        (*part_upload_calls, abort_call) = http_request_mock.call_args_list\n        with large_file.open(\"rb\") as f:\n            expected_calls = [\n                mock.call(\n                    \"put\",\n                    f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                    data=f.read(mock_chunk_size),\n                    headers={\"header\": f\"part-{i + 1}\"},\n                    timeout=None,\n                )\n                for i in range(2)\n            ]\n        # The upload-part requests are sent in parallel, so the order of the calls is not\n        # deterministic\n        assert sorted(part_upload_calls, key=lambda c: c.args[1]) == expected_calls\n        complete_request_body = json.loads(call_endpoint_mock.call_args_list[-1].args[-1])\n        assert complete_request_body[\"upload_id\"] == mock_upload_id\n        assert complete_request_body[\"part_etags\"] == [\n            {\"part_number\": 1, \"etag\": \"etag-1\"},\n            {\"part_number\": 2, \"etag\": \"etag-2\"},\n        ]\n        assert abort_call == mock.call(\n            \"delete\",\n            f\"{MOCK_AWS_SIGNED_URI}uploadId=abort\",\n            headers={\"header\": \"abort\"},\n            timeout=None,\n        )\n", "import json\nfrom unittest import mock\n\nimport pytest\n\nimport mlflow\nfrom mlflow.entities import (\n    Dataset,\n    DatasetInput,\n    Experiment,\n    ExperimentTag,\n    InputTag,\n    LifecycleStage,\n    Metric,\n    Param,\n    RunTag,\n    SourceType,\n    ViewType,\n)\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.models import Model\nfrom mlflow.protos.databricks_pb2 import RESOURCE_DOES_NOT_EXIST\nfrom mlflow.protos.service_pb2 import (\n    CreateRun,\n    DeleteExperiment,\n    DeleteRun,\n    DeleteTag,\n    GetExperimentByName,\n    LogBatch,\n    LogInputs,\n    LogMetric,\n    LogModel,\n    LogParam,\n    RestoreExperiment,\n    RestoreRun,\n    SearchExperiments,\n    SearchRuns,\n    SetExperimentTag,\n    SetTag,\n)\nfrom mlflow.protos.service_pb2 import RunTag as ProtoRunTag\nfrom mlflow.store.tracking.rest_store import RestStore\nfrom mlflow.tracking.request_header.default_request_header_provider import (\n    DefaultRequestHeaderProvider,\n)\nfrom mlflow.utils.proto_json_utils import message_to_json\nfrom mlflow.utils.rest_utils import MlflowHostCreds\n\n\nclass MyCoolException(Exception):\n    pass\n\n\nclass CustomErrorHandlingRestStore(RestStore):\n    def _call_endpoint(self, api, json_body):\n        raise MyCoolException(\"cool\")\n\n\ndef mock_http_request():\n    return mock.patch(\n        \"mlflow.utils.rest_utils.http_request\",\n        return_value=mock.MagicMock(status_code=200, text=\"{}\"),\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_successful_http_request(request):\n    def mock_request(*args, **kwargs):\n        # Filter out None arguments\n        assert args == (\"POST\", \"https://hello/api/2.0/mlflow/experiments/search\")\n        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n        assert kwargs == {\n            \"json\": {\"view_type\": \"ACTIVE_ONLY\"},\n            \"headers\": DefaultRequestHeaderProvider().request_headers(),\n            \"verify\": True,\n            \"timeout\": 120,\n        }\n        response = mock.MagicMock()\n        response.status_code = 200\n        response.text = '{\"experiments\": [{\"name\": \"Exp!\", \"lifecycle_stage\": \"active\"}]}'\n        return response\n\n    request.side_effect = mock_request\n\n    store = RestStore(lambda: MlflowHostCreds(\"https://hello\"))\n    experiments = store.search_experiments()\n    assert experiments[0].name == \"Exp!\"\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_failed_http_request(request):\n    response = mock.MagicMock()\n    response.status_code = 404\n    response.text = '{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"No experiment\"}'\n    request.return_value = response\n\n    store = RestStore(lambda: MlflowHostCreds(\"https://hello\"))\n    with pytest.raises(MlflowException, match=\"RESOURCE_DOES_NOT_EXIST: No experiment\"):\n        store.search_experiments()\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_failed_http_request_custom_handler(request):\n    response = mock.MagicMock()\n    response.status_code = 404\n    response.text = '{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"No experiment\"}'\n    request.return_value = response\n\n    store = CustomErrorHandlingRestStore(lambda: MlflowHostCreds(\"https://hello\"))\n    with pytest.raises(MyCoolException, match=\"cool\"):\n        store.search_experiments()\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_response_with_unknown_fields(request):\n    experiment_json = {\n        \"experiment_id\": \"1\",\n        \"name\": \"My experiment\",\n        \"artifact_location\": \"foo\",\n        \"lifecycle_stage\": \"deleted\",\n        \"OMG_WHAT_IS_THIS_FIELD\": \"Hooly cow\",\n    }\n\n    response = mock.MagicMock()\n    response.status_code = 200\n    experiments = {\"experiments\": [experiment_json]}\n    response.text = json.dumps(experiments)\n    request.return_value = response\n\n    store = RestStore(lambda: MlflowHostCreds(\"https://hello\"))\n    experiments = store.search_experiments()\n    assert len(experiments) == 1\n    assert experiments[0].name == \"My experiment\"\n\n\ndef _args(host_creds, endpoint, method, json_body):\n    res = {\n        \"host_creds\": host_creds,\n        \"endpoint\": f\"/api/2.0/mlflow/{endpoint}\",\n        \"method\": method,\n    }\n    if method == \"GET\":\n        res[\"params\"] = json.loads(json_body)\n    else:\n        res[\"json\"] = json.loads(json_body)\n    return res\n\n\ndef _verify_requests(http_request, host_creds, endpoint, method, json_body):\n    http_request.assert_any_call(**(_args(host_creds, endpoint, method, json_body)))\n\n\ndef test_requestor():\n    creds = MlflowHostCreds(\"https://hello\")\n    store = RestStore(lambda: creds)\n\n    user_name = \"mock user\"\n    source_name = \"rest test\"\n    run_name = \"my name\"\n\n    source_name_patch = mock.patch(\n        \"mlflow.tracking.context.default_context._get_source_name\", return_value=source_name\n    )\n    source_type_patch = mock.patch(\n        \"mlflow.tracking.context.default_context._get_source_type\",\n        return_value=SourceType.LOCAL,\n    )\n    with mock_http_request() as mock_http, mock.patch(\n        \"mlflow.tracking._tracking_service.utils._get_store\", return_value=store\n    ), mock.patch(\n        \"mlflow.tracking.context.default_context._get_user\", return_value=user_name\n    ), mock.patch(\n        \"time.time\", return_value=13579\n    ), source_name_patch, source_type_patch:\n        with mlflow.start_run(experiment_id=\"43\", run_name=run_name):\n            cr_body = message_to_json(\n                CreateRun(\n                    experiment_id=\"43\",\n                    user_id=user_name,\n                    run_name=run_name,\n                    start_time=13579000,\n                    tags=[\n                        ProtoRunTag(key=\"mlflow.source.name\", value=source_name),\n                        ProtoRunTag(key=\"mlflow.source.type\", value=\"LOCAL\"),\n                        ProtoRunTag(key=\"mlflow.user\", value=user_name),\n                        ProtoRunTag(key=\"mlflow.runName\", value=run_name),\n                    ],\n                )\n            )\n            expected_kwargs = _args(creds, \"runs/create\", \"POST\", cr_body)\n\n            assert mock_http.call_count == 1\n            actual_kwargs = mock_http.call_args[1]\n\n            # Test the passed tag values separately from the rest of the request\n            # Tag order is inconsistent on Python 2 and 3, but the order does not matter\n            expected_tags = expected_kwargs[\"json\"].pop(\"tags\")\n            actual_tags = actual_kwargs[\"json\"].pop(\"tags\")\n\n            assert sorted(expected_tags, key=lambda t: t[\"key\"]) == sorted(\n                actual_tags, key=lambda t: t[\"key\"]\n            )\n            assert expected_kwargs == actual_kwargs\n\n    with mock_http_request() as mock_http:\n        store.log_param(\"some_uuid\", Param(\"k1\", \"v1\"))\n        body = message_to_json(\n            LogParam(run_uuid=\"some_uuid\", run_id=\"some_uuid\", key=\"k1\", value=\"v1\")\n        )\n        _verify_requests(mock_http, creds, \"runs/log-parameter\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        store.set_experiment_tag(\"some_id\", ExperimentTag(\"t1\", \"abcd\" * 1000))\n        body = message_to_json(\n            SetExperimentTag(experiment_id=\"some_id\", key=\"t1\", value=\"abcd\" * 1000)\n        )\n        _verify_requests(mock_http, creds, \"experiments/set-experiment-tag\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        store.set_tag(\"some_uuid\", RunTag(\"t1\", \"abcd\" * 1000))\n        body = message_to_json(\n            SetTag(run_uuid=\"some_uuid\", run_id=\"some_uuid\", key=\"t1\", value=\"abcd\" * 1000)\n        )\n        _verify_requests(mock_http, creds, \"runs/set-tag\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        store.delete_tag(\"some_uuid\", \"t1\")\n        body = message_to_json(DeleteTag(run_id=\"some_uuid\", key=\"t1\"))\n        _verify_requests(mock_http, creds, \"runs/delete-tag\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        store.log_metric(\"u2\", Metric(\"m1\", 0.87, 12345, 3))\n        body = message_to_json(\n            LogMetric(run_uuid=\"u2\", run_id=\"u2\", key=\"m1\", value=0.87, timestamp=12345, step=3)\n        )\n        _verify_requests(mock_http, creds, \"runs/log-metric\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        metrics = [\n            Metric(\"m1\", 0.87, 12345, 0),\n            Metric(\"m2\", 0.49, 12345, -1),\n            Metric(\"m3\", 0.58, 12345, 2),\n        ]\n        params = [Param(\"p1\", \"p1val\"), Param(\"p2\", \"p2val\")]\n        tags = [RunTag(\"t1\", \"t1val\"), RunTag(\"t2\", \"t2val\")]\n        store.log_batch(run_id=\"u2\", metrics=metrics, params=params, tags=tags)\n        metric_protos = [metric.to_proto() for metric in metrics]\n        param_protos = [param.to_proto() for param in params]\n        tag_protos = [tag.to_proto() for tag in tags]\n        body = message_to_json(\n            LogBatch(run_id=\"u2\", metrics=metric_protos, params=param_protos, tags=tag_protos)\n        )\n        _verify_requests(mock_http, creds, \"runs/log-batch\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        dataset = Dataset(name=\"name\", digest=\"digest\", source_type=\"st\", source=\"source\")\n        tag = InputTag(key=\"k1\", value=\"v1\")\n        dataset_input = DatasetInput(dataset=dataset, tags=[tag])\n        store.log_inputs(\"some_uuid\", [dataset_input])\n        body = message_to_json(LogInputs(run_id=\"some_uuid\", datasets=[dataset_input.to_proto()]))\n        _verify_requests(mock_http, creds, \"runs/log-inputs\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        store.delete_run(\"u25\")\n        _verify_requests(\n            mock_http, creds, \"runs/delete\", \"POST\", message_to_json(DeleteRun(run_id=\"u25\"))\n        )\n\n    with mock_http_request() as mock_http:\n        store.restore_run(\"u76\")\n        _verify_requests(\n            mock_http, creds, \"runs/restore\", \"POST\", message_to_json(RestoreRun(run_id=\"u76\"))\n        )\n\n    with mock_http_request() as mock_http:\n        store.delete_experiment(\"0\")\n        _verify_requests(\n            mock_http,\n            creds,\n            \"experiments/delete\",\n            \"POST\",\n            message_to_json(DeleteExperiment(experiment_id=\"0\")),\n        )\n\n    with mock_http_request() as mock_http:\n        store.restore_experiment(\"0\")\n        _verify_requests(\n            mock_http,\n            creds,\n            \"experiments/restore\",\n            \"POST\",\n            message_to_json(RestoreExperiment(experiment_id=\"0\")),\n        )\n\n    with mock.patch(\"mlflow.utils.rest_utils.http_request\") as mock_http:\n        response = mock.MagicMock()\n        response.status_code = 200\n        response.text = '{\"runs\": [\"1a\", \"2b\", \"3c\"], \"next_page_token\": \"67890fghij\"}'\n        mock_http.return_value = response\n        result = store.search_runs(\n            [\"0\", \"1\"],\n            \"params.p1 = 'a'\",\n            ViewType.ACTIVE_ONLY,\n            max_results=10,\n            order_by=[\"a\"],\n            page_token=\"12345abcde\",\n        )\n\n        expected_message = SearchRuns(\n            experiment_ids=[\"0\", \"1\"],\n            filter=\"params.p1 = 'a'\",\n            run_view_type=ViewType.to_proto(ViewType.ACTIVE_ONLY),\n            max_results=10,\n            order_by=[\"a\"],\n            page_token=\"12345abcde\",\n        )\n        _verify_requests(mock_http, creds, \"runs/search\", \"POST\", message_to_json(expected_message))\n        assert result.token == \"67890fghij\"\n\n    with mock_http_request() as mock_http:\n        run_id = \"run_id\"\n        m = Model(artifact_path=\"model/path\", run_id=\"run_id\", flavors={\"tf\": \"flavor body\"})\n        store.record_logged_model(\"run_id\", m)\n        expected_message = LogModel(run_id=run_id, model_json=m.to_json())\n        _verify_requests(\n            mock_http, creds, \"runs/log-model\", \"POST\", message_to_json(expected_message)\n        )\n\n\ndef test_get_experiment_by_name():\n    creds = MlflowHostCreds(\"https://hello\")\n    store = RestStore(lambda: creds)\n    with mock.patch(\"mlflow.utils.rest_utils.http_request\") as mock_http:\n        response = mock.MagicMock()\n        response.status_code = 200\n        experiment = Experiment(\n            experiment_id=\"123\",\n            name=\"abc\",\n            artifact_location=\"/abc\",\n            lifecycle_stage=LifecycleStage.ACTIVE,\n        )\n        response.text = json.dumps(\n            {\"experiment\": json.loads(message_to_json(experiment.to_proto()))}\n        )\n        mock_http.return_value = response\n        result = store.get_experiment_by_name(\"abc\")\n        expected_message0 = GetExperimentByName(experiment_name=\"abc\")\n        _verify_requests(\n            mock_http,\n            creds,\n            \"experiments/get-by-name\",\n            \"GET\",\n            message_to_json(expected_message0),\n        )\n        assert result.experiment_id == experiment.experiment_id\n        assert result.name == experiment.name\n        assert result.artifact_location == experiment.artifact_location\n        assert result.lifecycle_stage == experiment.lifecycle_stage\n        # Test GetExperimentByName against nonexistent experiment\n        mock_http.reset_mock()\n        nonexistent_exp_response = mock.MagicMock()\n        nonexistent_exp_response.status_code = 404\n        nonexistent_exp_response.text = MlflowException(\n            \"Exp doesn't exist!\", RESOURCE_DOES_NOT_EXIST\n        ).serialize_as_json()\n        mock_http.return_value = nonexistent_exp_response\n        assert store.get_experiment_by_name(\"nonexistent-experiment\") is None\n        expected_message1 = GetExperimentByName(experiment_name=\"nonexistent-experiment\")\n        _verify_requests(\n            mock_http,\n            creds,\n            \"experiments/get-by-name\",\n            \"GET\",\n            message_to_json(expected_message1),\n        )\n        assert mock_http.call_count == 1\n\n\ndef test_search_experiments():\n    creds = MlflowHostCreds(\"https://hello\")\n    store = RestStore(lambda: creds)\n\n    with mock_http_request() as mock_http:\n        store.search_experiments(\n            view_type=ViewType.DELETED_ONLY,\n            max_results=5,\n            filter_string=\"name\",\n            order_by=[\"name\"],\n            page_token=\"abc\",\n        )\n        _verify_requests(\n            mock_http,\n            creds,\n            \"experiments/search\",\n            \"POST\",\n            message_to_json(\n                SearchExperiments(\n                    view_type=ViewType.DELETED_ONLY,\n                    max_results=5,\n                    filter=\"name\",\n                    order_by=[\"name\"],\n                    page_token=\"abc\",\n                )\n            ),\n        )\n\n\ndef _mock_response_with_200_status_code():\n    mock_response = mock.MagicMock()\n    mock_response.status_code = 200\n    return mock_response\n\n\ndef test_get_metric_history_paginated():\n    creds = MlflowHostCreds(\"https://hello\")\n    store = RestStore(lambda: creds)\n\n    response_1 = _mock_response_with_200_status_code()\n    response_2 = _mock_response_with_200_status_code()\n    response_payload_1 = {\n        \"metrics\": [\n            {\"key\": \"a_metric\", \"value\": 42, \"timestamp\": 123456777, \"step\": 0},\n            {\"key\": \"a_metric\", \"value\": 46, \"timestamp\": 123456797, \"step\": 1},\n        ],\n        \"next_page_token\": \"AcursorForTheRestofTheData\",\n    }\n    response_1.text = json.dumps(response_payload_1)\n    response_payload_2 = {\n        \"metrics\": [\n            {\"key\": \"a_metric\", \"value\": 40, \"timestamp\": 123456877, \"step\": 2},\n            {\"key\": \"a_metric\", \"value\": 56, \"timestamp\": 123456897, \"step\": 3},\n        ],\n        \"next_page_token\": \"\",\n    }\n    response_2.text = json.dumps(response_payload_2)\n    with mock.patch(\n        \"requests.Session.request\", side_effect=[response_1, response_2]\n    ) as mock_request:\n        # Fetch the first page\n        metrics = store.get_metric_history(\n            run_id=\"2\", metric_key=\"a_metric\", max_results=2, page_token=None\n        )\n        mock_request.assert_called_once()\n        assert mock_request.call_args.kwargs[\"params\"] == {\n            \"max_results\": 2,\n            \"metric_key\": \"a_metric\",\n            \"run_id\": \"2\",\n            \"run_uuid\": \"2\",\n        }\n        assert len(metrics) == 2\n        assert metrics[0] == Metric(key=\"a_metric\", value=42, timestamp=123456777, step=0)\n        assert metrics[1] == Metric(key=\"a_metric\", value=46, timestamp=123456797, step=1)\n        assert metrics.token == \"AcursorForTheRestofTheData\"\n        # Fetch the second page\n        mock_request.reset_mock()\n        metrics = store.get_metric_history(\n            run_id=\"2\", metric_key=\"a_metric\", max_results=2, page_token=metrics.token\n        )\n        mock_request.assert_called_once()\n        assert mock_request.call_args.kwargs[\"params\"] == {\n            \"max_results\": 2,\n            \"page_token\": \"AcursorForTheRestofTheData\",\n            \"metric_key\": \"a_metric\",\n            \"run_id\": \"2\",\n            \"run_uuid\": \"2\",\n        }\n        assert len(metrics) == 2\n        assert metrics[0] == Metric(key=\"a_metric\", value=40, timestamp=123456877, step=2)\n        assert metrics[1] == Metric(key=\"a_metric\", value=56, timestamp=123456897, step=3)\n        assert metrics.token is None\n\n\ndef test_get_metric_history_on_non_existent_metric_key():\n    creds = MlflowHostCreds(\"https://hello\")\n    rest_store = RestStore(lambda: creds)\n    empty_metric_response = _mock_response_with_200_status_code()\n    empty_metric_response.text = json.dumps({})\n    with mock.patch(\n        \"requests.Session.request\", side_effect=[empty_metric_response]\n    ) as mock_request:\n        metrics = rest_store.get_metric_history(run_id=\"1\", metric_key=\"test_metric\")\n        mock_request.assert_called_once()\n        assert metrics == []\n", "import subprocess\nimport sys\nfrom unittest import mock\n\nimport pytest\n\nfrom mlflow.utils import request_utils\n\n\ndef test_request_utils_does_not_import_mlflow(tmp_path):\n    file_content = f\"\"\"\nimport importlib.util\nimport os\nimport sys\n\nfile_path = r\"{request_utils.__file__}\"\nmodule_name = \"mlflow.utils.request_utils\"\n\nspec = importlib.util.spec_from_file_location(module_name, file_path)\nmodule = importlib.util.module_from_spec(spec)\nsys.modules[module_name] = module\nspec.loader.exec_module(module)\n\nassert \"mlflow\" not in sys.modules\nassert \"mlflow.utils.request_utils\" in sys.modules\n\"\"\"\n    test_file = tmp_path.joinpath(\"test_request_utils_does_not_import_mlflow.py\")\n    test_file.write_text(file_content)\n\n    subprocess.run([sys.executable, str(test_file)], check=True)\n\n\nclass IncompleteResponse:\n    def __init__(self):\n        self.headers = {\"Content-Length\": \"100\"}\n        raw = mock.MagicMock()\n        raw.tell.return_value = 50\n        self.raw = raw\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        pass\n\n\ndef test_download_chunk_incomplete_read(tmp_path):\n    with mock.patch.object(\n        request_utils, \"cloud_storage_http_request\", return_value=IncompleteResponse()\n    ):\n        download_path = tmp_path / \"chunk\"\n        download_path.touch()\n        with pytest.raises(IOError, match=\"Incomplete read\"):\n            request_utils.download_chunk(\n                range_start=0,\n                range_end=999,\n                headers={},\n                download_path=download_path,\n                http_uri=\"https://example.com\",\n            )\n", "import re\nfrom unittest import mock\n\nimport numpy\nimport pytest\nimport requests\n\nfrom mlflow.environment_variables import MLFLOW_HTTP_REQUEST_TIMEOUT\nfrom mlflow.exceptions import InvalidUrlException, MlflowException, RestException\nfrom mlflow.protos.databricks_pb2 import ENDPOINT_NOT_FOUND, ErrorCode\nfrom mlflow.protos.service_pb2 import GetRun\nfrom mlflow.pyfunc.scoring_server import NumpyEncoder\nfrom mlflow.tracking.request_header.default_request_header_provider import (\n    _USER_AGENT,\n    DefaultRequestHeaderProvider,\n)\nfrom mlflow.utils.rest_utils import (\n    MlflowHostCreds,\n    _can_parse_as_json_object,\n    augmented_raise_for_status,\n    call_endpoint,\n    call_endpoints,\n    http_request,\n    http_request_safe,\n)\n\nfrom tests import helper_functions\n\n\ndef test_well_formed_json_error_response():\n    with mock.patch(\n        \"requests.Session.request\", return_value=mock.MagicMock(status_code=400, text=\"{}\")\n    ):\n        host_only = MlflowHostCreds(\"http://my-host\")\n        response_proto = GetRun.Response()\n        with pytest.raises(RestException, match=\"INTERNAL_ERROR\"):\n            call_endpoint(host_only, \"/my/endpoint\", \"GET\", \"\", response_proto)\n\n\ndef test_non_json_ok_response():\n    with mock.patch(\n        \"requests.Session.request\",\n        return_value=mock.MagicMock(status_code=200, text=\"<html></html>\"),\n    ):\n        host_only = MlflowHostCreds(\"http://my-host\")\n        response_proto = GetRun.Response()\n        with pytest.raises(\n            MlflowException,\n            match=\"API request to endpoint was successful but the response body was not \"\n            \"in a valid JSON format\",\n        ):\n            call_endpoint(host_only, \"/api/2.0/fetch-model\", \"GET\", \"\", response_proto)\n\n\n@pytest.mark.parametrize(\n    \"response_mock\",\n    [\n        helper_functions.create_mock_response(400, \"Error message but not a JSON string\"),\n        helper_functions.create_mock_response(400, \"\"),\n        helper_functions.create_mock_response(400, None),\n    ],\n)\ndef test_malformed_json_error_response(response_mock):\n    with mock.patch(\"requests.Session.request\", return_value=response_mock):\n        host_only = MlflowHostCreds(\"http://my-host\")\n\n        response_proto = GetRun.Response()\n        with pytest.raises(\n            MlflowException, match=\"API request to endpoint /my/endpoint failed with error code 400\"\n        ):\n            call_endpoint(host_only, \"/my/endpoint\", \"GET\", \"\", response_proto)\n\n\ndef test_call_endpoints():\n    with mock.patch(\"mlflow.utils.rest_utils.call_endpoint\") as mock_call_endpoint:\n        response_proto = GetRun.Response()\n        mock_call_endpoint.side_effect = [\n            RestException({\"error_code\": ErrorCode.Name(ENDPOINT_NOT_FOUND)}),\n            None,\n        ]\n        host_only = MlflowHostCreds(\"http://my-host\")\n        endpoints = [(\"/my/endpoint\", \"POST\"), (\"/my/endpoint\", \"GET\")]\n        resp = call_endpoints(host_only, endpoints, \"\", response_proto)\n        mock_call_endpoint.assert_has_calls(\n            [\n                mock.call(host_only, endpoint, method, \"\", response_proto, None)\n                for endpoint, method in endpoints\n            ]\n        )\n        assert resp is None\n\n\ndef test_call_endpoints_raises_exceptions():\n    with mock.patch(\"mlflow.utils.rest_utils.call_endpoint\") as mock_call_endpoint:\n        response_proto = GetRun.Response()\n        mock_call_endpoint.side_effect = [\n            RestException({\"error_code\": ErrorCode.Name(ENDPOINT_NOT_FOUND)}),\n            RestException({\"error_code\": ErrorCode.Name(ENDPOINT_NOT_FOUND)}),\n        ]\n        host_only = MlflowHostCreds(\"http://my-host\")\n        endpoints = [(\"/my/endpoint\", \"POST\"), (\"/my/endpoint\", \"GET\")]\n        with pytest.raises(RestException, match=\"ENDPOINT_NOT_FOUND\"):\n            call_endpoints(host_only, endpoints, \"\", response_proto)\n        mock_call_endpoint.side_effect = [RestException({}), None]\n        with pytest.raises(RestException, match=\"INTERNAL_ERROR\"):\n            call_endpoints(host_only, endpoints, \"\", response_proto)\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_hostonly(request):\n    host_only = MlflowHostCreds(\"http://my-host\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=True,\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_cleans_hostname(request):\n    # Add a trailing slash, should be removed.\n    host_only = MlflowHostCreds(\"http://my-host/\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=True,\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_with_basic_auth(request):\n    host_only = MlflowHostCreds(\"http://my-host\", username=\"user\", password=\"pass\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    headers = DefaultRequestHeaderProvider().request_headers()\n    headers[\"Authorization\"] = \"Basic dXNlcjpwYXNz\"\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=True,\n        headers=headers,\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_with_aws_sigv4(request, monkeypatch):\n    \"\"\"This test requires the \"requests_auth_aws_sigv4\" package to be installed\"\"\"\n\n    from requests_auth_aws_sigv4 import AWSSigV4\n\n    monkeypatch.setenvs(\n        {\n            \"AWS_ACCESS_KEY_ID\": \"access-key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret-key\",\n            \"AWS_DEFAULT_REGION\": \"eu-west-1\",\n        }\n    )\n    aws_sigv4 = MlflowHostCreds(\"http://my-host\", aws_sigv4=True)\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(aws_sigv4, \"/my/endpoint\", \"GET\")\n\n    class AuthMatcher:\n        def __eq__(self, other):\n            return isinstance(other, AWSSigV4)\n\n    request.assert_called_once_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=mock.ANY,\n        headers=mock.ANY,\n        timeout=mock.ANY,\n        auth=AuthMatcher(),\n    )\n\n\n@mock.patch(\"requests.Session.request\")\n@mock.patch(\"mlflow.tracking.request_auth.registry.fetch_auth\")\ndef test_http_request_with_auth(fetch_auth, request):\n    mock_fetch_auth = {\"test_name\": \"test_auth_value\"}\n    fetch_auth.return_value = mock_fetch_auth\n    auth = \"test_auth_name\"\n    host_only = MlflowHostCreds(\"http://my-host\", auth=auth)\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n\n    fetch_auth.assert_called_with(auth)\n\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=mock.ANY,\n        headers=mock.ANY,\n        timeout=mock.ANY,\n        auth=mock_fetch_auth,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_with_token(request):\n    host_only = MlflowHostCreds(\"http://my-host\", token=\"my-token\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    headers = DefaultRequestHeaderProvider().request_headers()\n    headers[\"Authorization\"] = \"Bearer my-token\"\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=True,\n        headers=headers,\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_with_insecure(request):\n    host_only = MlflowHostCreds(\"http://my-host\", ignore_tls_verification=True)\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=False,\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_client_cert_path(request):\n    host_only = MlflowHostCreds(\"http://my-host\", client_cert_path=\"/some/path\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=True,\n        cert=\"/some/path\",\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_server_cert_path(request):\n    host_only = MlflowHostCreds(\"http://my-host\", server_cert_path=\"/some/path\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=\"/some/path\",\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_with_content_type_header(request):\n    host_only = MlflowHostCreds(\"http://my-host\", token=\"my-token\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    extra_headers = {\"Content-Type\": \"text/plain\"}\n    http_request(host_only, \"/my/endpoint\", \"GET\", extra_headers=extra_headers)\n    headers = DefaultRequestHeaderProvider().request_headers()\n    headers[\"Authorization\"] = \"Bearer my-token\"\n    headers[\"Content-Type\"] = \"text/plain\"\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=True,\n        headers=headers,\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_request_headers(request):\n    \"\"\"This test requires the package in tests/resources/mlflow-test-plugin to be installed\"\"\"\n\n    from mlflow_test_plugin.request_header_provider import PluginRequestHeaderProvider\n\n    # The test plugin's request header provider always returns False from in_context to avoid\n    # polluting request headers in developers' environments. The following mock overrides this to\n    # perform the integration test.\n    with mock.patch.object(PluginRequestHeaderProvider, \"in_context\", return_value=True):\n        host_only = MlflowHostCreds(\"http://my-host\", server_cert_path=\"/some/path\")\n\n        response = mock.MagicMock()\n        response.status_code = 200\n        request.return_value = response\n        http_request(host_only, \"/my/endpoint\", \"GET\")\n        request.assert_called_with(\n            \"GET\",\n            \"http://my-host/my/endpoint\",\n            verify=\"/some/path\",\n            headers={**DefaultRequestHeaderProvider().request_headers(), \"test\": \"header\"},\n            timeout=120,\n        )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_request_headers_user_agent(request):\n    \"\"\"This test requires the package in tests/resources/mlflow-test-plugin to be installed\"\"\"\n\n    from mlflow_test_plugin.request_header_provider import PluginRequestHeaderProvider\n\n    # The test plugin's request header provider always returns False from in_context to avoid\n    # polluting request headers in developers' environments. The following mock overrides this to\n    # perform the integration test.\n    with mock.patch.object(\n        PluginRequestHeaderProvider, \"in_context\", return_value=True\n    ), mock.patch.object(\n        PluginRequestHeaderProvider,\n        \"request_headers\",\n        return_value={_USER_AGENT: \"test_user_agent\"},\n    ):\n        host_only = MlflowHostCreds(\"http://my-host\", server_cert_path=\"/some/path\")\n        expected_headers = {\n            _USER_AGENT: \"{} {}\".format(\n                DefaultRequestHeaderProvider().request_headers()[_USER_AGENT], \"test_user_agent\"\n            )\n        }\n\n        response = mock.MagicMock()\n        response.status_code = 200\n        request.return_value = response\n        http_request(host_only, \"/my/endpoint\", \"GET\")\n        request.assert_called_with(\n            \"GET\",\n            \"http://my-host/my/endpoint\",\n            verify=\"/some/path\",\n            headers=expected_headers,\n            timeout=120,\n        )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_request_headers_user_agent_and_extra_header(request):\n    \"\"\"This test requires the package in tests/resources/mlflow-test-plugin to be installed\"\"\"\n\n    from mlflow_test_plugin.request_header_provider import PluginRequestHeaderProvider\n\n    # The test plugin's request header provider always returns False from in_context to avoid\n    # polluting request headers in developers' environments. The following mock overrides this to\n    # perform the integration test.\n    with mock.patch.object(\n        PluginRequestHeaderProvider, \"in_context\", return_value=True\n    ), mock.patch.object(\n        PluginRequestHeaderProvider,\n        \"request_headers\",\n        return_value={_USER_AGENT: \"test_user_agent\", \"header\": \"value\"},\n    ):\n        host_only = MlflowHostCreds(\"http://my-host\", server_cert_path=\"/some/path\")\n        expected_headers = {\n            _USER_AGENT: \"{} {}\".format(\n                DefaultRequestHeaderProvider().request_headers()[_USER_AGENT], \"test_user_agent\"\n            ),\n            \"header\": \"value\",\n        }\n\n        response = mock.MagicMock()\n        response.status_code = 200\n        request.return_value = response\n        http_request(host_only, \"/my/endpoint\", \"GET\")\n        request.assert_called_with(\n            \"GET\",\n            \"http://my-host/my/endpoint\",\n            verify=\"/some/path\",\n            headers=expected_headers,\n            timeout=120,\n        )\n\n\ndef test_http_request_with_invalid_url_raise_invalid_url_exception():\n    \"\"\"InvalidURL exception can be caught by a custom InvalidUrlException\"\"\"\n    host_only = MlflowHostCreds(\"http://my-host\")\n\n    with pytest.raises(InvalidUrlException, match=\"Invalid url: http://my-host/invalid_url\"):\n        with mock.patch(\"requests.Session.request\", side_effect=requests.exceptions.InvalidURL):\n            http_request(host_only, \"/invalid_url\", \"GET\")\n\n\ndef test_http_request_with_invalid_url_raise_mlflow_exception():\n    \"\"\"The InvalidUrlException can be caught by the MlflowException\"\"\"\n    host_only = MlflowHostCreds(\"http://my-host\")\n\n    with pytest.raises(MlflowException, match=\"Invalid url: http://my-host/invalid_url\"):\n        with mock.patch(\"requests.Session.request\", side_effect=requests.exceptions.InvalidURL):\n            http_request(host_only, \"/invalid_url\", \"GET\")\n\n\ndef test_ignore_tls_verification_not_server_cert_path():\n    with pytest.raises(\n        MlflowException,\n        match=\"When 'ignore_tls_verification' is true then 'server_cert_path' must not be set\",\n    ):\n        MlflowHostCreds(\n            \"http://my-host\",\n            ignore_tls_verification=True,\n            server_cert_path=\"/some/path\",\n        )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_wrapper(request):\n    host_only = MlflowHostCreds(\"http://my-host\", ignore_tls_verification=True)\n    response = mock.MagicMock()\n    response.status_code = 200\n    response.text = \"{}\"\n    request.return_value = response\n    http_request_safe(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=False,\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n    response.text = \"non json\"\n    request.return_value = response\n    http_request_safe(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        verify=False,\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n    response.status_code = 400\n    response.text = \"\"\n    request.return_value = response\n    with pytest.raises(MlflowException, match=\"Response body\"):\n        http_request_safe(host_only, \"/my/endpoint\", \"GET\")\n    response.text = (\n        '{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Node type not supported\"}'\n    )\n    request.return_value = response\n    with pytest.raises(RestException, match=\"RESOURCE_DOES_NOT_EXIST: Node type not supported\"):\n        http_request_safe(host_only, \"/my/endpoint\", \"GET\")\n\n\ndef test_numpy_encoder():\n    test_number = numpy.int64(42)\n    ne = NumpyEncoder()\n    defaulted_val = ne.default(test_number)\n    assert defaulted_val == 42\n\n\ndef test_numpy_encoder_fail():\n    if not hasattr(numpy, \"float128\"):\n        pytest.skip(\"numpy on exit this platform has no float128\")\n    test_number = numpy.float128\n    ne = NumpyEncoder()\n    with pytest.raises(TypeError, match=\"not JSON serializable\"):\n        ne.default(test_number)\n\n\ndef test_can_parse_as_json_object():\n    assert _can_parse_as_json_object(\"{}\")\n    assert _can_parse_as_json_object('{\"a\": \"b\"}')\n    assert _can_parse_as_json_object('{\"a\": {\"b\": \"c\"}}')\n    assert not _can_parse_as_json_object(\"[0, 1, 2]\")\n    assert not _can_parse_as_json_object('\"abc\"')\n    assert not _can_parse_as_json_object(\"123\")\n\n\ndef test_http_request_customize_config(monkeypatch):\n    with mock.patch(\n        \"mlflow.utils.rest_utils._get_http_response_with_retries\"\n    ) as mock_get_http_response_with_retries:\n        host_only = MlflowHostCreds(\"http://my-host\")\n        monkeypatch.delenv(\"MLFLOW_HTTP_REQUEST_MAX_RETRIES\", raising=False)\n        monkeypatch.delenv(\"MLFLOW_HTTP_REQUEST_BACKOFF_FACTOR\", raising=False)\n        monkeypatch.delenv(\"MLFLOW_HTTP_REQUEST_TIMEOUT\", raising=False)\n        http_request(host_only, \"/my/endpoint\", \"GET\")\n        mock_get_http_response_with_retries.assert_called_with(\n            mock.ANY,\n            mock.ANY,\n            5,\n            2,\n            1.0,\n            mock.ANY,\n            True,\n            headers=mock.ANY,\n            verify=mock.ANY,\n            timeout=120,\n        )\n        mock_get_http_response_with_retries.reset_mock()\n        monkeypatch.setenv(\"MLFLOW_HTTP_REQUEST_MAX_RETRIES\", \"8\")\n        monkeypatch.setenv(\"MLFLOW_HTTP_REQUEST_BACKOFF_FACTOR\", \"3\")\n        monkeypatch.setenv(\"MLFLOW_HTTP_REQUEST_BACKOFF_JITTER\", \"1.0\")\n        monkeypatch.setenv(\"MLFLOW_HTTP_REQUEST_TIMEOUT\", \"300\")\n        http_request(host_only, \"/my/endpoint\", \"GET\")\n        mock_get_http_response_with_retries.assert_called_with(\n            mock.ANY,\n            mock.ANY,\n            8,\n            3,\n            1.0,\n            mock.ANY,\n            True,\n            headers=mock.ANY,\n            verify=mock.ANY,\n            timeout=300,\n        )\n\n\ndef test_http_request_explains_how_to_increase_timeout_in_error_message():\n    with mock.patch(\"requests.Session.request\", side_effect=requests.exceptions.Timeout):\n        with pytest.raises(\n            MlflowException,\n            match=(\n                r\"To increase the timeout, set the environment variable \"\n                + re.escape(str(MLFLOW_HTTP_REQUEST_TIMEOUT))\n            ),\n        ):\n            http_request(MlflowHostCreds(\"http://my-host\"), \"/my/endpoint\", \"GET\")\n\n\ndef test_augmented_raise_for_status():\n    response = requests.Response()\n    response.status_code = 403\n    response._content = b\"Token expired\"\n\n    with mock.patch(\"requests.Session.request\", return_value=response) as mock_request:\n        response = requests.get(\"https://github.com/mlflow/mlflow.git\")\n        mock_request.assert_called_once()\n\n    with pytest.raises(requests.HTTPError, match=\"Token expired\") as e:\n        augmented_raise_for_status(response)\n\n    assert e.value.response == response\n    assert e.value.request == response.request\n    assert response.text in str(e.value)\n"], "fixing_code": ["\"\"\"\nThis module defines environment variables used in MLflow.\n\"\"\"\nimport os\nfrom pathlib import Path\n\n\nclass _EnvironmentVariable:\n    \"\"\"\n    Represents an environment variable.\n    \"\"\"\n\n    def __init__(self, name, type_, default):\n        self.name = name\n        self.type = type_\n        self.default = default\n\n    @property\n    def defined(self):\n        return self.name in os.environ\n\n    def get_raw(self):\n        return os.getenv(self.name)\n\n    def set(self, value):\n        os.environ[self.name] = str(value)\n\n    def unset(self):\n        os.environ.pop(self.name, None)\n\n    def get(self):\n        \"\"\"\n        Reads the value of the environment variable if it exists and converts it to the desired\n        type. Otherwise, returns the default value.\n        \"\"\"\n        if (val := self.get_raw()) is not None:\n            try:\n                return self.type(val)\n            except Exception as e:\n                raise ValueError(f\"Failed to convert {val!r} to {self.type} for {self.name}: {e}\")\n        return self.default\n\n    def __str__(self):\n        return f\"{self.name} (default: {self.default}, type: {self.type.__name__})\"\n\n    def __repr__(self):\n        return repr(self.name)\n\n    def __format__(self, format_spec: str) -> str:\n        return self.name.__format__(format_spec)\n\n\nclass _BooleanEnvironmentVariable(_EnvironmentVariable):\n    \"\"\"\n    Represents a boolean environment variable.\n    \"\"\"\n\n    def __init__(self, name, default):\n        # `default not in [True, False, None]` doesn't work because `1 in [True]`\n        # (or `0 in [False]`) returns True.\n        if not (default is True or default is False or default is None):\n            raise ValueError(f\"{name} default value must be one of [True, False, None]\")\n        super().__init__(name, bool, default)\n\n    def get(self):\n        if not self.defined:\n            return self.default\n\n        val = os.getenv(self.name)\n        lowercased = val.lower()\n        if lowercased not in [\"true\", \"false\", \"1\", \"0\"]:\n            raise ValueError(\n                f\"{self.name} value must be one of ['true', 'false', '1', '0'] (case-insensitive), \"\n                f\"but got {val}\"\n            )\n        return lowercased in [\"true\", \"1\"]\n\n\n#: Specifies the tracking URI.\n#: (default: ``None``)\nMLFLOW_TRACKING_URI = _EnvironmentVariable(\"MLFLOW_TRACKING_URI\", str, None)\n\n#: Specifies the registry URI.\n#: (default: ``None``)\nMLFLOW_REGISTRY_URI = _EnvironmentVariable(\"MLFLOW_REGISTRY_URI\", str, None)\n\n#: Specifies the ``dfs_tmpdir`` parameter to use for ``mlflow.spark.save_model``,\n#: ``mlflow.spark.log_model`` and ``mlflow.spark.load_model``. See\n#: https://www.mlflow.org/docs/latest/python_api/mlflow.spark.html#mlflow.spark.save_model\n#: for more information.\n#: (default: ``/tmp/mlflow``)\nMLFLOW_DFS_TMP = _EnvironmentVariable(\"MLFLOW_DFS_TMP\", str, \"/tmp/mlflow\")\n\n#: Specifies the maximum number of retries for MLflow HTTP requests\n#: (default: ``5``)\nMLFLOW_HTTP_REQUEST_MAX_RETRIES = _EnvironmentVariable(\"MLFLOW_HTTP_REQUEST_MAX_RETRIES\", int, 5)\n\n#: Specifies the backoff increase factor between MLflow HTTP request failures\n#: (default: ``2``)\nMLFLOW_HTTP_REQUEST_BACKOFF_FACTOR = _EnvironmentVariable(\n    \"MLFLOW_HTTP_REQUEST_BACKOFF_FACTOR\", int, 2\n)\n\n#: Specifies the backoff jitter between MLflow HTTP request failures\n#: (default: ``1.0``)\nMLFLOW_HTTP_REQUEST_BACKOFF_JITTER = _EnvironmentVariable(\n    \"MLFLOW_HTTP_REQUEST_BACKOFF_JITTER\", float, 1.0\n)\n\n#: Specifies the timeout in seconds for MLflow HTTP requests\n#: (default: ``120``)\nMLFLOW_HTTP_REQUEST_TIMEOUT = _EnvironmentVariable(\"MLFLOW_HTTP_REQUEST_TIMEOUT\", int, 120)\n\n#: Specifies whether MLflow HTTP requests should be signed using AWS signature V4. It will overwrite\n#: (default: ``False``). When set, it will overwrite the \"Authorization\" HTTP header.\n#: See https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html for more information.\nMLFLOW_TRACKING_AWS_SIGV4 = _BooleanEnvironmentVariable(\"MLFLOW_TRACKING_AWS_SIGV4\", False)\n\n#: Specifies the auth provider to sign the MLflow HTTP request\n#: (default: ``None``). When set, it will overwrite the \"Authorization\" HTTP header.\nMLFLOW_TRACKING_AUTH = _EnvironmentVariable(\"MLFLOW_TRACKING_AUTH\", str, None)\n\n#: Specifies the chunk size to use when downloading a file from GCS\n#: (default: ``None``). If None, the chunk size is automatically determined by the\n#: ``google-cloud-storage`` package.\nMLFLOW_GCS_DOWNLOAD_CHUNK_SIZE = _EnvironmentVariable(\"MLFLOW_GCS_DOWNLOAD_CHUNK_SIZE\", int, None)\n\n#: Specifies the chunk size to use when uploading a file to GCS.\n#: (default: ``None``). If None, the chunk size is automatically determined by the\n#: ``google-cloud-storage`` package.\nMLFLOW_GCS_UPLOAD_CHUNK_SIZE = _EnvironmentVariable(\"MLFLOW_GCS_UPLOAD_CHUNK_SIZE\", int, None)\n\n#: (Deprecated, please use ``MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT``)\n#: Specifies the default timeout to use when downloading/uploading a file from/to GCS\n#: (default: ``None``). If None, ``google.cloud.storage.constants._DEFAULT_TIMEOUT`` is used.\nMLFLOW_GCS_DEFAULT_TIMEOUT = _EnvironmentVariable(\"MLFLOW_GCS_DEFAULT_TIMEOUT\", int, None)\n\n#: Specifies whether to disable model logging and loading via mlflowdbfs.\n#: (default: ``None``)\n_DISABLE_MLFLOWDBFS = _EnvironmentVariable(\"DISABLE_MLFLOWDBFS\", str, None)\n\n#: Specifies the S3 endpoint URL to use for S3 artifact operations.\n#: (default: ``None``)\nMLFLOW_S3_ENDPOINT_URL = _EnvironmentVariable(\"MLFLOW_S3_ENDPOINT_URL\", str, None)\n\n#: Specifies whether or not to skip TLS certificate verification for S3 artifact operations.\n#: (default: ``False``)\nMLFLOW_S3_IGNORE_TLS = _BooleanEnvironmentVariable(\"MLFLOW_S3_IGNORE_TLS\", False)\n\n#: Specifies extra arguments for S3 artifact uploads.\n#: (default: ``None``)\nMLFLOW_S3_UPLOAD_EXTRA_ARGS = _EnvironmentVariable(\"MLFLOW_S3_UPLOAD_EXTRA_ARGS\", str, None)\n\n#: Specifies the location of a Kerberos ticket cache to use for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_KERBEROS_TICKET_CACHE = _EnvironmentVariable(\"MLFLOW_KERBEROS_TICKET_CACHE\", str, None)\n\n#: Specifies a Kerberos user for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_KERBEROS_USER = _EnvironmentVariable(\"MLFLOW_KERBEROS_USER\", str, None)\n\n#: Specifies extra pyarrow configurations for HDFS artifact operations.\n#: (default: ``None``)\nMLFLOW_PYARROW_EXTRA_CONF = _EnvironmentVariable(\"MLFLOW_PYARROW_EXTRA_CONF\", str, None)\n\n#: Specifies the ``pool_size`` parameter to use for ``sqlalchemy.create_engine`` in the SQLAlchemy\n#: tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_size\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOL_SIZE = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOL_SIZE\", int, None\n)\n\n#: Specifies the ``pool_recycle`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.pool_recycle\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOL_RECYCLE\", int, None\n)\n\n#: Specifies the ``max_overflow`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.max_overflow\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_MAX_OVERFLOW = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_MAX_OVERFLOW\", int, None\n)\n\n#: Specifies the ``echo`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.echo\n#: for more information.\n#: (default: ``False``)\nMLFLOW_SQLALCHEMYSTORE_ECHO = _BooleanEnvironmentVariable(\"MLFLOW_SQLALCHEMYSTORE_ECHO\", False)\n\n#: Specifies whether or not to print a warning when `--env-manager=conda` is specified.\n#: (default: ``False``)\nMLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING = _BooleanEnvironmentVariable(\n    \"MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING\", False\n)\n#: Specifies the ``poolclass`` parameter to use for ``sqlalchemy.create_engine`` in the\n#: SQLAlchemy tracking store. See https://docs.sqlalchemy.org/en/14/core/engines.html#sqlalchemy.create_engine.params.poolclass\n#: for more information.\n#: (default: ``None``)\nMLFLOW_SQLALCHEMYSTORE_POOLCLASS = _EnvironmentVariable(\n    \"MLFLOW_SQLALCHEMYSTORE_POOLCLASS\", str, None\n)\n\n#: Specifies the ``timeout_seconds`` for MLflow Model dependency inference operations.\n#: (default: ``120``)\nMLFLOW_REQUIREMENTS_INFERENCE_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_REQUIREMENTS_INFERENCE_TIMEOUT\", int, 120\n)\n\n#: Specifies the MLflow Model Scoring server request timeout in seconds\n#: (default: ``60``)\nMLFLOW_SCORING_SERVER_REQUEST_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT\", int, 60\n)\n\n#: (Experimental, may be changed or removed)\n#: Specifies the timeout to use when uploading or downloading a file\n#: (default: ``None``). If None, individual artifact stores will choose defaults.\nMLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT = _EnvironmentVariable(\n    \"MLFLOW_ARTIFACT_UPLOAD_DOWNLOAD_TIMEOUT\", int, None\n)\n\n#: Specifies the device intended for use in the predict function - can be used\n#: to override behavior where the GPU is used by default when available by\n#: setting this environment variable to be ``cpu``. Currently, this\n#: variable is only supported for the MLflow PyTorch and HuggingFace flavors.\n#: For the HuggingFace flavor, note that device must be parseable as an integer.\nMLFLOW_DEFAULT_PREDICTION_DEVICE = _EnvironmentVariable(\n    \"MLFLOW_DEFAULT_PREDICTION_DEVICE\", str, None\n)\n\n#: Specifies to Huggingface whether to use the automatic device placement logic of\n# HuggingFace accelerate. If it's set to false, the low_cpu_mem_usage flag will not be\n# set to True and device_map will not be set to \"auto\".\nMLFLOW_HUGGINGFACE_DISABLE_ACCELERATE_FEATURES = _BooleanEnvironmentVariable(\n    \"MLFLOW_DISABLE_HUGGINGFACE_ACCELERATE_FEATURES\", False\n)\n\n#: Specifies to Huggingface whether to use the automatic device placement logic of\n# HuggingFace accelerate. If it's set to false, the low_cpu_mem_usage flag will not be\n# set to True and device_map will not be set to \"auto\".\nMLFLOW_HUGGINGFACE_USE_DEVICE_MAP = _BooleanEnvironmentVariable(\n    \"MLFLOW_HUGGINGFACE_USE_DEVICE_MAP\", True\n)\n\n#: Specifies to Huggingface to use the automatic device placement logic of HuggingFace accelerate.\n#: This can be set to values supported by the version of HuggingFace Accelerate being installed.\nMLFLOW_HUGGINGFACE_DEVICE_MAP_STRATEGY = _EnvironmentVariable(\n    \"MLFLOW_HUGGINGFACE_DEVICE_MAP_STRATEGY\", str, \"auto\"\n)\n\n#: Specifies to Huggingface to use the low_cpu_mem_usage flag powered by HuggingFace accelerate.\n#: If it's set to false, the low_cpu_mem_usage flag will be set to False.\nMLFLOW_HUGGINGFACE_USE_LOW_CPU_MEM_USAGE = _BooleanEnvironmentVariable(\n    \"MLFLOW_HUGGINGFACE_USE_LOW_CPU_MEM_USAGE\", True\n)\n\n#: Specifies the max_shard_size to use when mlflow transformers flavor saves the model checkpoint.\n#: This can be set to override the 500MB default.\nMLFLOW_HUGGINGFACE_MODEL_MAX_SHARD_SIZE = _EnvironmentVariable(\n    \"MLFLOW_HUGGINGFACE_MODEL_MAX_SHARD_SIZE\", str, \"500MB\"\n)\n\n#: Specifies whether or not to allow using a file URI as a model version source.\n#: Please be aware that setting this environment variable to True is potentially risky\n#: because it can allow access to arbitrary files on the specified filesystem\n#: (default: ``False``).\nMLFLOW_ALLOW_FILE_URI_AS_MODEL_VERSION_SOURCE = _BooleanEnvironmentVariable(\n    \"MLFLOW_ALLOW_FILE_URI_AS_MODEL_VERSION_SOURCE\", False\n)\n\n\n#: Specifies the name of the Databricks secret scope to use for storing OpenAI API keys.\nMLFLOW_OPENAI_SECRET_SCOPE = _EnvironmentVariable(\"MLFLOW_OPENAI_SECRET_SCOPE\", str, None)\n\n#: Specifier whether or not to retry OpenAI API calls.\nMLFLOW_OPENAI_RETRIES_ENABLED = _BooleanEnvironmentVariable(\"MLFLOW_OPENAI_RETRIES_ENABLED\", True)\n\n#: (Experimental, may be changed or removed)\n#: Specifies the download options to be used by pip wheel when `add_libraries_to_model` is used to\n#: create and log model dependencies as model artifacts. The default behavior only uses dependency\n#: binaries and no source packages.\n#: (default: ``--only-binary=:all:``).\nMLFLOW_WHEELED_MODEL_PIP_DOWNLOAD_OPTIONS = _EnvironmentVariable(\n    \"MLFLOW_WHEELED_MODEL_PIP_DOWNLOAD_OPTIONS\", str, \"--only-binary=:all:\"\n)\n\n# Specifies whether or not to use multipart download when downloading a large file on Databricks.\nMLFLOW_ENABLE_MULTIPART_DOWNLOAD = _BooleanEnvironmentVariable(\n    \"MLFLOW_ENABLE_MULTIPART_DOWNLOAD\", True\n)\n\n# Specifies whether or not to use multipart upload when uploading large artifacts.\nMLFLOW_ENABLE_MULTIPART_UPLOAD = _BooleanEnvironmentVariable(\"MLFLOW_ENABLE_MULTIPART_UPLOAD\", True)\n\n#: Specifies whether or not to use multipart upload for proxied artifact access.\n#: (default: ``False``)\nMLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD = _BooleanEnvironmentVariable(\n    \"MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD\", False\n)\n\n#: Private environment variable that's set to ``True`` while running tests.\n_MLFLOW_TESTING = _BooleanEnvironmentVariable(\"MLFLOW_TESTING\", False)\n\n#: Specifies the username used to authenticate with a tracking server.\n#: (default: ``None``)\nMLFLOW_TRACKING_USERNAME = _EnvironmentVariable(\"MLFLOW_TRACKING_USERNAME\", str, None)\n\n#: Specifies the password used to authenticate with a tracking server.\n#: (default: ``None``)\nMLFLOW_TRACKING_PASSWORD = _EnvironmentVariable(\"MLFLOW_TRACKING_PASSWORD\", str, None)\n\n#: Specifies and takes precedence for setting the basic/bearer auth on http requests.\n#: (default: ``None``)\nMLFLOW_TRACKING_TOKEN = _EnvironmentVariable(\"MLFLOW_TRACKING_TOKEN\", str, None)\n\n#: Specifies whether to verify TLS connection in ``requests.request`` function,\n#: see https://requests.readthedocs.io/en/master/api/\n#: (default: ``False``).\nMLFLOW_TRACKING_INSECURE_TLS = _BooleanEnvironmentVariable(\"MLFLOW_TRACKING_INSECURE_TLS\", False)\n\n#: Sets the ``verify`` param in ``requests.request`` function,\n#: see https://requests.readthedocs.io/en/master/api/\n#: (default: ``None``)\nMLFLOW_TRACKING_SERVER_CERT_PATH = _EnvironmentVariable(\n    \"MLFLOW_TRACKING_SERVER_CERT_PATH\", str, None\n)\n\n#: Sets the ``cert`` param in ``requests.request`` function,\n#: see https://requests.readthedocs.io/en/master/api/\n#: (default: ``None``)\nMLFLOW_TRACKING_CLIENT_CERT_PATH = _EnvironmentVariable(\n    \"MLFLOW_TRACKING_CLIENT_CERT_PATH\", str, None\n)\n\n#: Specified the ID of the run to log data to.\n#: (default: ``None``)\nMLFLOW_RUN_ID = _EnvironmentVariable(\"MLFLOW_RUN_ID\", str, None)\n\n#: Specifies the default root directory for tracking `FileStore`.\n#: (default: ``None``)\nMLFLOW_TRACKING_DIR = _EnvironmentVariable(\"MLFLOW_TRACKING_DIR\", str, None)\n\n#: Specifies the default root directory for registry `FileStore`.\n#: (default: ``None``)\nMLFLOW_REGISTRY_DIR = _EnvironmentVariable(\"MLFLOW_REGISTRY_DIR\", str, None)\n\n#: Specifies the default experiment ID to create run to.\n#: (default: ``None``)\nMLFLOW_EXPERIMENT_ID = _EnvironmentVariable(\"MLFLOW_EXPERIMENT_ID\", str, None)\n\n#: Specifies the default experiment name to create run to.\n#: (default: ``None``)\nMLFLOW_EXPERIMENT_NAME = _EnvironmentVariable(\"MLFLOW_EXPERIMENT_NAME\", str, None)\n\n#: Specified the path to the configuration file for MLflow Authentication.\n#: (default: ``None``)\nMLFLOW_AUTH_CONFIG_PATH = _EnvironmentVariable(\"MLFLOW_AUTH_CONFIG_PATH\", str, None)\n\n#: Specifies the root directory to create Python virtual environments in.\n#: (default: ``~/.mlflow/envs``)\nMLFLOW_ENV_ROOT = _EnvironmentVariable(\n    \"MLFLOW_ENV_ROOT\", str, str(Path.home().joinpath(\".mlflow\", \"envs\"))\n)\n\n#: Specifies whether or not to use DBFS FUSE mount to store artifacts on Databricks\n#: (default: ``False``)\nMLFLOW_ENABLE_DBFS_FUSE_ARTIFACT_REPO = _BooleanEnvironmentVariable(\n    \"MLFLOW_ENABLE_DBFS_FUSE_ARTIFACT_REPO\", True\n)\n\n#: Private environment variable that should be set to ``True`` when running autologging tests.\n#: (default: ``False``)\n_MLFLOW_AUTOLOGGING_TESTING = _BooleanEnvironmentVariable(\"MLFLOW_AUTOLOGGING_TESTING\", False)\n\n#: (Experimental, may be changed or removed)\n#: Specifies the uri of a MLflow Gateway Server instance to be used with the Gateway Client APIs\n#: (default: ``None``)\nMLFLOW_GATEWAY_URI = _EnvironmentVariable(\"MLFLOW_GATEWAY_URI\", str, None)\n\n#: (Experimental, may be changed or removed)\n#: Specifies the uri of a MLflow Deployments Server instance to be used with the Deployments\n#: Client APIs\n#: (default: ``None``)\nMLFLOW_DEPLOYMENTS_TARGET = _EnvironmentVariable(\"MLFLOW_DEPLOYMENTS_TARGET\", str, None)\n\n#: Specifies the path of the config file for MLflow AI Gateway.\n#: (default: ``None``)\nMLFLOW_GATEWAY_CONFIG = _EnvironmentVariable(\"MLFLOW_GATEWAY_CONFIG\", str, None)\n\n#: Specifies the path of the config file for the MLflow Deployments server.\n#: (default: ``None``)\nMLFLOW_DEPLOYMENTS_CONFIG = _EnvironmentVariable(\"MLFLOW_DEPLOYMENTS_CONFIG\", str, None)\n\n#: Specifies whether to display the progress bar when uploading/downloading artifacts.\n#: (default: ``True``)\nMLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR = _BooleanEnvironmentVariable(\n    \"MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR\", True\n)\n\n#: Specifies the conda home directory to use.\n#: (default: ``conda``)\nMLFLOW_CONDA_HOME = _EnvironmentVariable(\"MLFLOW_CONDA_HOME\", str, None)\n\n#: Specifies the name of the command to use when creating the environments.\n#: For example, let's say we want to use mamba (https://github.com/mamba-org/mamba)\n#: instead of conda to create environments.\n#: Then: > conda install mamba -n base -c conda-forge\n#: If not set, use the same as conda_path\n#: (default: ``conda``)\nMLFLOW_CONDA_CREATE_ENV_CMD = _EnvironmentVariable(\"MLFLOW_CONDA_CREATE_ENV_CMD\", str, \"conda\")\n\n#: Specifies the execution directory for recipes.\n#: (default: ``None``)\nMLFLOW_RECIPES_EXECUTION_DIRECTORY = _EnvironmentVariable(\n    \"MLFLOW_RECIPES_EXECUTION_DIRECTORY\", str, None\n)\n\n#: Specifies the target step to execute for recipes.\n#: (default: ``None``)\nMLFLOW_RECIPES_EXECUTION_TARGET_STEP_NAME = _EnvironmentVariable(\n    \"MLFLOW_RECIPES_EXECUTION_TARGET_STEP_NAME\", str, None\n)\n\n#: Specifies the flavor to serve in the scoring server.\n#: (default ``None``)\nMLFLOW_DEPLOYMENT_FLAVOR_NAME = _EnvironmentVariable(\"MLFLOW_DEPLOYMENT_FLAVOR_NAME\", str, None)\n\n#: Specifies the profile to use for recipes.\n#: (default: ``None``)\nMLFLOW_RECIPES_PROFILE = _EnvironmentVariable(\"MLFLOW_RECIPES_PROFILE\", str, None)\n\n#: Specifies the MLflow Run context\n#: (default: ``None``)\nMLFLOW_RUN_CONTEXT = _EnvironmentVariable(\"MLFLOW_RUN_CONTEXT\", str, None)\n\n#: Specifies the URL of the ECR-hosted Docker image a model is deployed into for SageMaker.\n# (default: ``None``)\nMLFLOW_SAGEMAKER_DEPLOY_IMG_URL = _EnvironmentVariable(\"MLFLOW_SAGEMAKER_DEPLOY_IMG_URL\", str, None)\n\n#: Specifies whether to disable creating a new conda environment for `mlflow models build-docker`.\n#: (default: ``False``)\nMLFLOW_DISABLE_ENV_CREATION = _BooleanEnvironmentVariable(\"MLFLOW_DISABLE_ENV_CREATION\", False)\n\n#: Specifies the timeout value for downloading chunks of mlflow artifacts.\n#: (default: ``300``)\nMLFLOW_DOWNLOAD_CHUNK_TIMEOUT = _EnvironmentVariable(\"MLFLOW_DOWNLOAD_CHUNK_TIMEOUT\", int, 300)\n\n#: Specifies if system metrics logging should be enabled.\nMLFLOW_ENABLE_SYSTEM_METRICS_LOGGING = _BooleanEnvironmentVariable(\n    \"MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING\", False\n)\n\n#: Specifies the sampling interval for system metrics logging.\nMLFLOW_SYSTEM_METRICS_SAMPLING_INTERVAL = _EnvironmentVariable(\n    \"MLFLOW_SYSTEM_METRICS_SAMPLING_INTERVAL\", float, None\n)\n\n#: Specifies the number of samples before logging system metrics.\nMLFLOW_SYSTEM_METRICS_SAMPLES_BEFORE_LOGGING = _EnvironmentVariable(\n    \"MLFLOW_SYSTEM_METRICS_SAMPLES_BEFORE_LOGGING\", int, None\n)\n\n# Private environment variable to specify the number of chunk download retries for multipart\n# download.\n_MLFLOW_MPD_NUM_RETRIES = _EnvironmentVariable(\"_MLFLOW_MPD_NUM_RETRIES\", int, 3)\n\n# Private environment variable to specify the interval between chunk download retries for multipart\n# download.\n_MLFLOW_MPD_RETRY_INTERVAL_SECONDS = _EnvironmentVariable(\n    \"_MLFLOW_MPD_RETRY_INTERVAL_SECONDS\", int, 1\n)\n\n#: Specifies the minimum file size in bytes to use multipart upload when logging artifacts\n#: (default: ``524_288_000`` (500 MB))\nMLFLOW_MULTIPART_UPLOAD_MINIMUM_FILE_SIZE = _EnvironmentVariable(\n    \"MLFLOW_MULTIPART_UPLOAD_MINIMUM_FILE_SIZE\", int, 500 * 1024**2\n)\n\n#: Specifies the chunk size in bytes to use when performing multipart upload\n#: (default: ``104_857_60`` (10 MB))\nMLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE = _EnvironmentVariable(\n    \"MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE\", int, 10 * 1024**2\n)\n\n#: Specifies the chunk size in bytes to use when performing multipart download\n#: (default: ``104_857_600`` (100 MB))\nMLFLOW_MULTIPART_DOWNLOAD_CHUNK_SIZE = _EnvironmentVariable(\n    \"MLFLOW_MULTIPART_DOWNLOAD_CHUNK_SIZE\", int, 100 * 1024**2\n)\n\n#: Specifies whether or not to allow the MLflow server to follow redirects when\n#: making HTTP requests. If set to False, the server will throw an exception if it\n#: encounters a redirect response.\n#: (default: ``True``)\nMLFLOW_ALLOW_HTTP_REDIRECTS = _BooleanEnvironmentVariable(\"MLFLOW_ALLOW_HTTP_REDIRECTS\", True)\n", "# DO NO IMPORT MLFLOW IN THIS FILE.\n# This file is imported by download_cloud_file_chunk.py.\n# Importing mlflow is time-consuming and we want to avoid that in artifact download subprocesses.\nimport os\nimport random\nfrom functools import lru_cache\n\nimport requests\nimport urllib3\nfrom packaging.version import Version\nfrom requests.adapters import HTTPAdapter\nfrom requests.exceptions import HTTPError\nfrom urllib3.util import Retry\n\n# Response codes that generally indicate transient network failures and merit client retries,\n# based on guidance from cloud service providers\n# (https://docs.microsoft.com/en-us/azure/architecture/best-practices/retry-service-specific#general-rest-and-retry-guidelines)\n_TRANSIENT_FAILURE_RESPONSE_CODES = frozenset(\n    [\n        408,  # Request Timeout\n        429,  # Too Many Requests\n        500,  # Internal Server Error\n        502,  # Bad Gateway\n        503,  # Service Unavailable\n        504,  # Gateway Timeout\n    ]\n)\n\n\nclass JitteredRetry(Retry):\n    \"\"\"\n    urllib3 < 2 doesn't support `backoff_jitter`. This class is a workaround for that.\n    \"\"\"\n\n    def __init__(self, *args, backoff_jitter=0.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.backoff_jitter = backoff_jitter\n\n    def get_backoff_time(self):\n        \"\"\"\n        Source: https://github.com/urllib3/urllib3/commit/214b184923388328919b0a4b0c15bff603aa51be\n        \"\"\"\n        backoff_value = super().get_backoff_time()\n        if self.backoff_jitter != 0.0:\n            backoff_value += random.random() * self.backoff_jitter\n        return float(max(0, min(Retry.DEFAULT_BACKOFF_MAX, backoff_value)))\n\n\ndef augmented_raise_for_status(response):\n    \"\"\"Wrap the standard `requests.response.raise_for_status()` method and return reason\"\"\"\n    try:\n        response.raise_for_status()\n    except HTTPError as e:\n        if response.text:\n            raise HTTPError(\n                f\"{e}. Response text: {response.text}\", request=e.request, response=e.response\n            )\n        else:\n            raise e\n\n\ndef download_chunk(*, range_start, range_end, headers, download_path, http_uri):\n    combined_headers = {**headers, \"Range\": f\"bytes={range_start}-{range_end}\"}\n\n    with cloud_storage_http_request(\n        \"get\",\n        http_uri,\n        stream=False,\n        headers=combined_headers,\n        timeout=10,\n    ) as response:\n        expected_length = response.headers.get(\"Content-Length\")\n        if expected_length is not None:\n            actual_length = response.raw.tell()\n            expected_length = int(expected_length)\n            if actual_length < expected_length:\n                raise IOError(\n                    \"Incomplete read ({} bytes read, {} more expected)\".format(\n                        actual_length, expected_length - actual_length\n                    )\n                )\n        # File will have been created upstream. Use r+b to ensure chunks\n        # don't overwrite the entire file.\n        augmented_raise_for_status(response)\n        with open(download_path, \"r+b\") as f:\n            f.seek(range_start)\n            f.write(response.content)\n\n\n@lru_cache(maxsize=64)\ndef _cached_get_request_session(\n    max_retries,\n    backoff_factor,\n    backoff_jitter,\n    retry_codes,\n    raise_on_status,\n    # To create a new Session object for each process, we use the process id as the cache key.\n    # This is to avoid sharing the same Session object across processes, which can lead to issues\n    # such as https://stackoverflow.com/q/3724900.\n    _pid,\n):\n    \"\"\"\n    This function should not be called directly. Instead, use `_get_request_session` below.\n    \"\"\"\n    assert 0 <= max_retries < 10\n    assert 0 <= backoff_factor < 120\n\n    retry_kwargs = {\n        \"total\": max_retries,\n        \"connect\": max_retries,\n        \"read\": max_retries,\n        \"redirect\": max_retries,\n        \"status\": max_retries,\n        \"status_forcelist\": retry_codes,\n        \"backoff_factor\": backoff_factor,\n        \"backoff_jitter\": backoff_jitter,\n        \"raise_on_status\": raise_on_status,\n    }\n    urllib3_version = Version(urllib3.__version__)\n    if urllib3_version >= Version(\"1.26.0\"):\n        retry_kwargs[\"allowed_methods\"] = None\n    else:\n        retry_kwargs[\"method_whitelist\"] = None\n\n    if urllib3_version < Version(\"2.0\"):\n        retry = JitteredRetry(**retry_kwargs)\n    else:\n        retry = Retry(**retry_kwargs)\n    adapter = HTTPAdapter(max_retries=retry)\n    session = requests.Session()\n    session.mount(\"https://\", adapter)\n    session.mount(\"http://\", adapter)\n    return session\n\n\ndef _get_request_session(max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status):\n    \"\"\"\n    Returns a `Requests.Session` object for making an HTTP request.\n\n    :param max_retries: Maximum total number of retries.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param raise_on_status: whether to raise an exception, or return a response, if status falls\n      in retry_codes range and retries have been exhausted.\n    :return: requests.Session object.\n    \"\"\"\n    return _cached_get_request_session(\n        max_retries,\n        backoff_factor,\n        backoff_jitter,\n        retry_codes,\n        raise_on_status,\n        _pid=os.getpid(),\n    )\n\n\ndef _get_http_response_with_retries(\n    method,\n    url,\n    max_retries,\n    backoff_factor,\n    backoff_jitter,\n    retry_codes,\n    raise_on_status=True,\n    allow_redirects=None,\n    **kwargs,\n):\n    \"\"\"\n    Performs an HTTP request using Python's `requests` module with an automatic retry policy.\n\n    :param method: a string indicating the method to use, e.g. \"GET\", \"POST\", \"PUT\".\n    :param url: the target URL address for the HTTP request.\n    :param max_retries: Maximum total number of retries.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param raise_on_status: whether to raise an exception, or return a response, if status falls\n      in retry_codes range and retries have been exhausted.\n    :param kwargs: Additional keyword arguments to pass to `requests.Session.request()`\n\n    :return: requests.Response object.\n    \"\"\"\n    session = _get_request_session(\n        max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status\n    )\n\n    # the environment variable is hardcoded here to avoid importing mlflow.\n    # however, documentation is available in environment_variables.py\n    env_value = os.getenv(\"MLFLOW_ALLOW_HTTP_REDIRECTS\", \"true\").lower() in [\"true\", \"1\"]\n    allow_redirects = env_value if allow_redirects is None else allow_redirects\n\n    return session.request(method, url, allow_redirects=allow_redirects, **kwargs)\n\n\ndef cloud_storage_http_request(\n    method,\n    url,\n    max_retries=5,\n    backoff_factor=2,\n    backoff_jitter=1.0,\n    retry_codes=_TRANSIENT_FAILURE_RESPONSE_CODES,\n    timeout=None,\n    **kwargs,\n):\n    \"\"\"\n    Performs an HTTP PUT/GET/PATCH request using Python's `requests` module with automatic retry.\n\n    :param method: string of 'PUT' or 'GET' or 'PATCH', specify to do http PUT or GET or PATCH\n    :param url: the target URL address for the HTTP request.\n    :param max_retries: maximum number of retries before throwing an exception.\n    :param backoff_factor: a time factor for exponential backoff. e.g. value 5 means the HTTP\n      request will be retried with interval 5, 10, 20... seconds. A value of 0 turns off the\n      exponential backoff.\n    :param backoff_jitter: A random jitter to add to the backoff interval.\n    :param retry_codes: a list of HTTP response error codes that qualifies for retry.\n    :param timeout: wait for timeout seconds for response from remote server for connect and\n      read request. Default to None owing to long duration operation in read / write.\n    :param kwargs: Additional keyword arguments to pass to `requests.Session.request()`\n\n    :return requests.Response object.\n    \"\"\"\n    if method.lower() not in (\"put\", \"get\", \"patch\", \"delete\"):\n        raise ValueError(\"Illegal http method: \" + method)\n    return _get_http_response_with_retries(\n        method,\n        url,\n        max_retries,\n        backoff_factor,\n        backoff_jitter,\n        retry_codes,\n        timeout=timeout,\n        **kwargs,\n    )\n", "import filecmp\nimport json\nimport os\nimport shutil\nfrom unittest import mock\n\nimport databricks_cli\nimport pytest\nfrom databricks_cli.configure.provider import DatabricksConfig\n\nimport mlflow\nfrom mlflow import MlflowClient, cli\nfrom mlflow.entities import RunStatus\nfrom mlflow.environment_variables import MLFLOW_TRACKING_URI\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.projects import ExecutionException, databricks\nfrom mlflow.projects.databricks import DatabricksJobRunner, _get_cluster_mlflow_run_cmd\nfrom mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE, ErrorCode\nfrom mlflow.store.tracking.file_store import FileStore\nfrom mlflow.tracking.request_header.default_request_header_provider import (\n    DefaultRequestHeaderProvider,\n)\nfrom mlflow.utils import file_utils\nfrom mlflow.utils.mlflow_tags import (\n    MLFLOW_DATABRICKS_RUN_URL,\n    MLFLOW_DATABRICKS_SHELL_JOB_RUN_ID,\n    MLFLOW_DATABRICKS_WEBAPP_URL,\n)\nfrom mlflow.utils.uri import construct_db_uri_from_profile\n\nfrom tests import helper_functions\nfrom tests.integration.utils import invoke_cli_runner\nfrom tests.projects.utils import TEST_PROJECT_DIR, validate_exit_status\n\n\n@pytest.fixture\ndef runs_cancel_mock():\n    \"\"\"Mocks the Jobs Runs Cancel API request\"\"\"\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner.jobs_runs_cancel\"\n    ) as runs_cancel_mock:\n        runs_cancel_mock.return_value = None\n        yield runs_cancel_mock\n\n\n@pytest.fixture\ndef runs_submit_mock():\n    \"\"\"Mocks the Jobs Runs Submit API request\"\"\"\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner._jobs_runs_submit\",\n        return_value={\"run_id\": \"-1\"},\n    ) as runs_submit_mock:\n        yield runs_submit_mock\n\n\n@pytest.fixture\ndef runs_get_mock():\n    \"\"\"Mocks the Jobs Runs Get API request\"\"\"\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner.jobs_runs_get\"\n    ) as runs_get_mock:\n        yield runs_get_mock\n\n\n@pytest.fixture\ndef databricks_cluster_mlflow_run_cmd_mock():\n    \"\"\"Mocks the Jobs Runs Get API request\"\"\"\n    with mock.patch(\n        \"mlflow.projects.databricks._get_cluster_mlflow_run_cmd\"\n    ) as mlflow_run_cmd_mock:\n        yield mlflow_run_cmd_mock\n\n\n@pytest.fixture\ndef cluster_spec_mock(tmp_path):\n    cluster_spec_handle = tmp_path.joinpath(\"cluster_spec.json\")\n    cluster_spec_handle.write_text(\"{}\")\n    return str(cluster_spec_handle)\n\n\n@pytest.fixture\ndef dbfs_root_mock(tmp_path):\n    return str(tmp_path.joinpath(\"dbfs-root\"))\n\n\n@pytest.fixture\ndef upload_to_dbfs_mock(dbfs_root_mock):\n    def upload_mock_fn(_, src_path, dbfs_uri):\n        mock_dbfs_dst = os.path.join(dbfs_root_mock, dbfs_uri.split(\"/dbfs/\")[1])\n        os.makedirs(os.path.dirname(mock_dbfs_dst))\n        shutil.copy(src_path, mock_dbfs_dst)\n\n    with mock.patch.object(\n        mlflow.projects.databricks.DatabricksJobRunner, \"_upload_to_dbfs\", new=upload_mock_fn\n    ) as upload_mock:\n        yield upload_mock\n\n\n@pytest.fixture\ndef dbfs_path_exists_mock(dbfs_root_mock):  # pylint: disable=unused-argument\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner._dbfs_path_exists\"\n    ) as path_exists_mock:\n        yield path_exists_mock\n\n\n@pytest.fixture\ndef dbfs_mocks(dbfs_path_exists_mock, upload_to_dbfs_mock):  # pylint: disable=unused-argument\n    return\n\n\n@pytest.fixture\ndef before_run_validations_mock():\n    with mock.patch(\"mlflow.projects.databricks.before_run_validations\"):\n        yield\n\n\n@pytest.fixture\ndef set_tag_mock():\n    with mock.patch(\"mlflow.projects.databricks.tracking.MlflowClient\") as m:\n        mlflow_service_mock = mock.Mock(wraps=MlflowClient())\n        m.return_value = mlflow_service_mock\n        yield mlflow_service_mock.set_tag\n\n\ndef _get_mock_run_state(succeeded):\n    if succeeded is None:\n        return {\"life_cycle_state\": \"RUNNING\", \"state_message\": \"\"}\n    run_result_state = \"SUCCESS\" if succeeded else \"FAILED\"\n    return {\"life_cycle_state\": \"TERMINATED\", \"state_message\": \"\", \"result_state\": run_result_state}\n\n\ndef mock_runs_get_result(succeeded):\n    run_state = _get_mock_run_state(succeeded)\n    return {\"state\": run_state, \"run_page_url\": \"test_url\"}\n\n\ndef run_databricks_project(cluster_spec, **kwargs):\n    return mlflow.projects.run(\n        uri=TEST_PROJECT_DIR,\n        backend=\"databricks\",\n        backend_config=cluster_spec,\n        parameters={\"alpha\": \"0.4\"},\n        **kwargs,\n    )\n\n\ndef test_upload_project_to_dbfs(\n    dbfs_root_mock, tmp_path, dbfs_path_exists_mock, upload_to_dbfs_mock\n):  # pylint: disable=unused-argument\n    # Upload project to a mock directory\n    dbfs_path_exists_mock.return_value = False\n    runner = DatabricksJobRunner(databricks_profile_uri=construct_db_uri_from_profile(\"DEFAULT\"))\n    dbfs_uri = runner._upload_project_to_dbfs(\n        project_dir=TEST_PROJECT_DIR, experiment_id=FileStore.DEFAULT_EXPERIMENT_ID\n    )\n    # Get expected tar\n    local_tar_path = os.path.join(dbfs_root_mock, dbfs_uri.split(\"/dbfs/\")[1])\n    expected_tar_path = str(tmp_path.joinpath(\"expected.tar.gz\"))\n    file_utils.make_tarfile(\n        output_filename=expected_tar_path,\n        source_dir=TEST_PROJECT_DIR,\n        archive_name=databricks.DB_TARFILE_ARCHIVE_NAME,\n    )\n    # Extract the tarred project, verify its contents\n    assert filecmp.cmp(local_tar_path, expected_tar_path, shallow=False)\n\n\ndef test_upload_existing_project_to_dbfs(dbfs_path_exists_mock):  # pylint: disable=unused-argument\n    # Check that we don't upload the project if it already exists on DBFS\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner._upload_to_dbfs\"\n    ) as upload_to_dbfs_mock:\n        dbfs_path_exists_mock.return_value = True\n        runner = DatabricksJobRunner(\n            databricks_profile_uri=construct_db_uri_from_profile(\"DEFAULT\")\n        )\n        runner._upload_project_to_dbfs(\n            project_dir=TEST_PROJECT_DIR, experiment_id=FileStore.DEFAULT_EXPERIMENT_ID\n        )\n        assert upload_to_dbfs_mock.call_count == 0\n\n\n@pytest.mark.parametrize(\n    \"response_mock\",\n    [\n        helper_functions.create_mock_response(400, \"Error message but not a JSON string\"),\n        helper_functions.create_mock_response(400, \"\"),\n        helper_functions.create_mock_response(400, None),\n    ],\n)\ndef test_dbfs_path_exists_error_response_handling(response_mock):\n    with mock.patch(\n        \"mlflow.utils.databricks_utils.get_databricks_host_creds\"\n    ) as get_databricks_host_creds_mock, mock.patch(\n        \"mlflow.utils.rest_utils.http_request\"\n    ) as http_request_mock:\n        # given a well formed DatabricksJobRunner\n        # note: databricks_profile is None needed because clients using profile are mocked\n        job_runner = DatabricksJobRunner(databricks_profile_uri=None)\n\n        # when the http request to validate the dbfs path returns a 400 response with an\n        # error message that is either well-formed JSON or not\n        get_databricks_host_creds_mock.return_value = None\n        http_request_mock.return_value = response_mock\n\n        # then _dbfs_path_exists should return a MlflowException\n        with pytest.raises(MlflowException, match=\"API request to check existence of file at DBFS\"):\n            job_runner._dbfs_path_exists(\"some/path\")\n\n\ndef test_run_databricks_validations(\n    tmp_path,\n    monkeypatch,\n    cluster_spec_mock,\n    dbfs_mocks,\n    set_tag_mock,\n):  # pylint: disable=unused-argument\n    \"\"\"\n    Tests that running on Databricks fails before making any API requests if validations fail.\n    \"\"\"\n    monkeypatch.setenvs({\"DATABRICKS_HOST\": \"test-host\", \"DATABRICKS_TOKEN\": \"foo\"})\n    with mock.patch(\n        \"mlflow.projects.databricks.DatabricksJobRunner._databricks_api_request\"\n    ) as db_api_req_mock:\n        # Test bad tracking URI\n        mlflow.set_tracking_uri(tmp_path.as_uri())\n        with pytest.raises(ExecutionException, match=\"MLflow tracking URI must be of\"):\n            run_databricks_project(cluster_spec_mock, synchronous=True)\n        assert db_api_req_mock.call_count == 0\n        db_api_req_mock.reset_mock()\n        mlflow_service = MlflowClient()\n        assert len(mlflow_service.search_runs([FileStore.DEFAULT_EXPERIMENT_ID])) == 0\n        mlflow.set_tracking_uri(\"databricks\")\n        # Test misspecified parameters\n        with pytest.raises(\n            ExecutionException, match=\"No value given for missing parameters: 'name'\"\n        ):\n            mlflow.projects.run(\n                TEST_PROJECT_DIR,\n                backend=\"databricks\",\n                entry_point=\"greeter\",\n                backend_config=cluster_spec_mock,\n            )\n        assert db_api_req_mock.call_count == 0\n        db_api_req_mock.reset_mock()\n        # Test bad cluster spec\n        with pytest.raises(ExecutionException, match=\"Backend spec must be provided\"):\n            mlflow.projects.run(\n                TEST_PROJECT_DIR, backend=\"databricks\", synchronous=True, backend_config=None\n            )\n        assert db_api_req_mock.call_count == 0\n        db_api_req_mock.reset_mock()\n        # Test that validations pass with good tracking URIs\n        databricks.before_run_validations(\"http://\", cluster_spec_mock)\n        databricks.before_run_validations(\"databricks\", cluster_spec_mock)\n\n\n@pytest.mark.usefixtures(\n    \"before_run_validations_mock\",\n    \"runs_cancel_mock\",\n    \"dbfs_mocks\",\n    \"databricks_cluster_mlflow_run_cmd_mock\",\n)\ndef test_run_databricks(\n    runs_submit_mock,\n    runs_get_mock,\n    cluster_spec_mock,\n    set_tag_mock,\n    databricks_cluster_mlflow_run_cmd_mock,\n    monkeypatch,\n):\n    \"\"\"Test running on Databricks with mocks.\"\"\"\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    # Test that MLflow gets the correct run status when performing a Databricks run\n    for run_succeeded, expect_status in [(True, RunStatus.FINISHED), (False, RunStatus.FAILED)]:\n        runs_get_mock.return_value = mock_runs_get_result(succeeded=run_succeeded)\n        submitted_run = run_databricks_project(cluster_spec_mock, synchronous=False)\n        assert submitted_run.wait() == run_succeeded\n        assert submitted_run.run_id is not None\n        assert runs_submit_mock.call_count == 1\n        assert databricks_cluster_mlflow_run_cmd_mock.call_count == 1\n        tags = {}\n        for call_args, _ in set_tag_mock.call_args_list:\n            tags[call_args[1]] = call_args[2]\n        assert tags[MLFLOW_DATABRICKS_RUN_URL] == \"test_url\"\n        assert tags[MLFLOW_DATABRICKS_SHELL_JOB_RUN_ID] == \"-1\"\n        assert tags[MLFLOW_DATABRICKS_WEBAPP_URL] == \"test-host\"\n        set_tag_mock.reset_mock()\n        runs_submit_mock.reset_mock()\n        databricks_cluster_mlflow_run_cmd_mock.reset_mock()\n        validate_exit_status(submitted_run.get_status(), expect_status)\n\n\n@pytest.mark.usefixtures(\n    \"before_run_validations_mock\",\n    \"runs_cancel_mock\",\n    \"dbfs_mocks\",\n    \"cluster_spec_mock\",\n    \"set_tag_mock\",\n)\ndef test_run_databricks_cluster_spec_json(runs_submit_mock, runs_get_mock, monkeypatch):\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    runs_get_mock.return_value = mock_runs_get_result(succeeded=True)\n    cluster_spec = {\n        \"spark_version\": \"5.0.x-scala2.11\",\n        \"num_workers\": 2,\n        \"node_type_id\": \"i3.xlarge\",\n    }\n    # Run project synchronously, verify that it succeeds (doesn't throw)\n    run_databricks_project(cluster_spec=cluster_spec, synchronous=True)\n    assert runs_submit_mock.call_count == 1\n    runs_submit_args, _ = runs_submit_mock.call_args_list[0]\n    req_body = runs_submit_args[0]\n    assert req_body[\"new_cluster\"] == cluster_spec\n\n\n@pytest.mark.usefixtures(\n    \"before_run_validations_mock\",\n    \"runs_cancel_mock\",\n    \"dbfs_mocks\",\n    \"cluster_spec_mock\",\n    \"set_tag_mock\",\n)\ndef test_run_databricks_extended_cluster_spec_json(runs_submit_mock, runs_get_mock, monkeypatch):\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    runs_get_mock.return_value = mock_runs_get_result(succeeded=True)\n    new_cluster_spec = {\n        \"spark_version\": \"6.5.x-scala2.11\",\n        \"num_workers\": 2,\n        \"node_type_id\": \"i3.xlarge\",\n    }\n    extra_library = {\"pypi\": {\"package\": \"tensorflow\"}}\n\n    cluster_spec = {\"new_cluster\": new_cluster_spec, \"libraries\": [extra_library]}\n\n    # Run project synchronously, verify that it succeeds (doesn't throw)\n    run_databricks_project(cluster_spec=cluster_spec, synchronous=True)\n    assert runs_submit_mock.call_count == 1\n    runs_submit_args, _ = runs_submit_mock.call_args_list[0]\n    req_body = runs_submit_args[0]\n    assert req_body[\"new_cluster\"] == new_cluster_spec\n    # This does test deep object equivalence\n    assert extra_library in req_body[\"libraries\"]\n\n\n@pytest.mark.usefixtures(\n    \"before_run_validations_mock\",\n    \"runs_cancel_mock\",\n    \"dbfs_mocks\",\n    \"cluster_spec_mock\",\n    \"set_tag_mock\",\n)\ndef test_run_databricks_extended_cluster_spec_json_without_libraries(\n    runs_submit_mock, runs_get_mock, monkeypatch\n):\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    runs_get_mock.return_value = mock_runs_get_result(succeeded=True)\n    new_cluster_spec = {\n        \"spark_version\": \"6.5.x-scala2.11\",\n        \"num_workers\": 2,\n        \"node_type_id\": \"i3.xlarge\",\n    }\n\n    cluster_spec = {\n        \"new_cluster\": new_cluster_spec,\n    }\n\n    # Run project synchronously, verify that it succeeds (doesn't throw)\n    run_databricks_project(cluster_spec=cluster_spec, synchronous=True)\n    assert runs_submit_mock.call_count == 1\n    runs_submit_args, _ = runs_submit_mock.call_args_list[0]\n    req_body = runs_submit_args[0]\n    assert req_body[\"new_cluster\"] == new_cluster_spec\n\n\ndef test_run_databricks_throws_exception_when_spec_uses_existing_cluster(monkeypatch):\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    existing_cluster_spec = {\n        \"existing_cluster_id\": \"1000-123456-clust1\",\n    }\n    with pytest.raises(\n        MlflowException, match=\"execution against existing clusters is not currently supported\"\n    ) as exc:\n        run_databricks_project(cluster_spec=existing_cluster_spec)\n    assert exc.value.error_code == ErrorCode.Name(INVALID_PARAMETER_VALUE)\n\n\ndef test_run_databricks_cancel(\n    before_run_validations_mock,\n    runs_submit_mock,\n    dbfs_mocks,\n    set_tag_mock,\n    runs_cancel_mock,\n    runs_get_mock,\n    cluster_spec_mock,\n    monkeypatch,\n):\n    # pylint: disable=unused-argument\n    # Test that MLflow properly handles Databricks run cancellation. We mock the result of\n    # the runs-get API to indicate run failure so that cancel() exits instead of blocking while\n    # waiting for run status.\n    monkeypatch.setenv(\"DATABRICKS_HOST\", \"test-host\")\n    monkeypatch.setenv(\"DATABRICKS_TOKEN\", \"foo\")\n    runs_get_mock.return_value = mock_runs_get_result(succeeded=False)\n    submitted_run = run_databricks_project(cluster_spec_mock, synchronous=False)\n    submitted_run.cancel()\n    validate_exit_status(submitted_run.get_status(), RunStatus.FAILED)\n    assert runs_cancel_mock.call_count == 1\n    # Test that we raise an exception when a blocking Databricks run fails\n    runs_get_mock.return_value = mock_runs_get_result(succeeded=False)\n    with pytest.raises(mlflow.projects.ExecutionException, match=r\"Run \\(ID '.+'\\) failed\"):\n        run_databricks_project(cluster_spec_mock, synchronous=True)\n\n\ndef test_get_tracking_uri_for_run(monkeypatch):\n    mlflow.set_tracking_uri(\"http://some-uri\")\n    assert databricks._get_tracking_uri_for_run() == \"http://some-uri\"\n    mlflow.set_tracking_uri(\"databricks://profile\")\n    assert databricks._get_tracking_uri_for_run() == \"databricks\"\n    mlflow.set_tracking_uri(None)\n    monkeypatch.setenv(MLFLOW_TRACKING_URI.name, \"http://some-uri\")\n    assert mlflow.tracking._tracking_service.utils.get_tracking_uri() == \"http://some-uri\"\n\n\nclass MockProfileConfigProvider:\n    def __init__(self, profile):\n        assert profile == \"my-profile\"\n\n    def get_config(self):\n        return DatabricksConfig.from_password(\"host\", \"user\", \"pass\", insecure=False)\n\n\n@mock.patch(\"requests.Session.request\")\n@mock.patch(\"databricks_cli.configure.provider.get_config\")\n@mock.patch.object(\n    databricks_cli.configure.provider, \"ProfileConfigProvider\", MockProfileConfigProvider\n)\ndef test_databricks_http_request_integration(get_config, request):\n    \"\"\"Confirms that the databricks http request params can in fact be used as an HTTP request\"\"\"\n\n    def confirm_request_params(*args, **kwargs):\n        headers = DefaultRequestHeaderProvider().request_headers()\n        headers[\"Authorization\"] = \"Basic dXNlcjpwYXNz\"\n        assert args == (\"PUT\", \"host/clusters/list\")\n        assert kwargs == {\n            \"allow_redirects\": True,\n            \"headers\": headers,\n            \"verify\": True,\n            \"json\": {\"a\": \"b\"},\n            \"timeout\": 120,\n        }\n        http_response = mock.MagicMock()\n        http_response.status_code = 200\n        http_response.text = '{\"OK\": \"woo\"}'\n        return http_response\n\n    request.side_effect = confirm_request_params\n    get_config.return_value = DatabricksConfig.from_password(\"host\", \"user\", \"pass\", insecure=False)\n\n    response = DatabricksJobRunner(databricks_profile_uri=None)._databricks_api_request(\n        \"/clusters/list\", \"PUT\", json={\"a\": \"b\"}\n    )\n    assert json.loads(response.text) == {\"OK\": \"woo\"}\n    get_config.reset_mock()\n    response = DatabricksJobRunner(\n        databricks_profile_uri=construct_db_uri_from_profile(\"my-profile\")\n    )._databricks_api_request(\"/clusters/list\", \"PUT\", json={\"a\": \"b\"})\n    assert json.loads(response.text) == {\"OK\": \"woo\"}\n    assert get_config.call_count == 0\n\n\n@mock.patch(\"mlflow.utils.databricks_utils.get_databricks_host_creds\")\ndef test_run_databricks_failed(_):\n    text = '{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Node type not supported\"}'\n    with mock.patch(\n        \"mlflow.utils.rest_utils.http_request\", return_value=mock.Mock(text=text, status_code=400)\n    ):\n        runner = DatabricksJobRunner(construct_db_uri_from_profile(\"profile\"))\n        with pytest.raises(\n            MlflowException, match=\"RESOURCE_DOES_NOT_EXIST: Node type not supported\"\n        ):\n            runner._run_shell_command_job(\"/project\", \"command\", {}, {})\n\n\ndef test_run_databricks_generates_valid_mlflow_run_cmd():\n    cmd = _get_cluster_mlflow_run_cmd(\n        project_dir=\"my_project_dir\",\n        run_id=\"hi\",\n        entry_point=\"main\",\n        parameters={\"a\": \"b\"},\n        env_manager=\"conda\",\n    )\n    assert cmd[0] == \"mlflow\"\n    with mock.patch(\"mlflow.projects.run\"):\n        invoke_cli_runner(cli.cli, cmd[1:])\n", "import json\nimport os\nimport posixpath\nimport re\nimport shutil\nimport time\nfrom unittest import mock\nfrom unittest.mock import ANY\n\nimport pytest\nfrom requests.models import Response\n\nfrom mlflow.entities.file_info import FileInfo as FileInfoEntity\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.protos.databricks_artifacts_pb2 import (\n    ArtifactCredentialInfo,\n    ArtifactCredentialType,\n    CompleteMultipartUpload,\n    CreateMultipartUpload,\n    GetCredentialsForRead,\n    GetCredentialsForWrite,\n    GetPresignedUploadPartUrl,\n)\nfrom mlflow.protos.service_pb2 import FileInfo, ListArtifacts\nfrom mlflow.store.artifact.artifact_repository_registry import get_artifact_repository\nfrom mlflow.store.artifact.databricks_artifact_repo import (\n    _MAX_CREDENTIALS_REQUEST_SIZE,\n    DatabricksArtifactRepository,\n)\n\nDATABRICKS_ARTIFACT_REPOSITORY_PACKAGE = \"mlflow.store.artifact.databricks_artifact_repo\"\nCLOUD_ARTIFACT_REPOSITORY_PACKAGE = \"mlflow.store.artifact.cloud_artifact_repo\"\nDATABRICKS_ARTIFACT_REPOSITORY = (\n    f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.DatabricksArtifactRepository\"\n)\nCLOUD_ARTIFACT_REPOSITORY = f\"{CLOUD_ARTIFACT_REPOSITORY_PACKAGE}.CloudArtifactRepository\"\n\nMOCK_AZURE_SIGNED_URI = \"http://this_is_a_mock_sas_for_azure\"\nMOCK_ADLS_GEN2_SIGNED_URI = \"http://this_is_a_mock_sas_for_adls_gen2\"\nMOCK_AWS_SIGNED_URI = \"http://this_is_a_mock_presigned_uri_for_aws?\"\nMOCK_GCP_SIGNED_URL = \"http://this_is_a_mock_signed_url_for_gcp?\"\nMOCK_RUN_ID = \"MOCK-RUN-ID\"\nMOCK_HEADERS = [\n    ArtifactCredentialInfo.HttpHeader(name=\"Mock-Name1\", value=\"Mock-Value1\"),\n    ArtifactCredentialInfo.HttpHeader(name=\"Mock-Name2\", value=\"Mock-Value2\"),\n]\nMOCK_RUN_ROOT_URI = \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\"\nMOCK_SUBDIR = \"subdir/path\"\nMOCK_SUBDIR_ROOT_URI = posixpath.join(MOCK_RUN_ROOT_URI, MOCK_SUBDIR)\n\n\n@pytest.fixture\ndef databricks_artifact_repo():\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\", return_value=MOCK_RUN_ROOT_URI\n    ):\n        return get_artifact_repository(\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\"\n        )\n\n\n@pytest.fixture\ndef test_file(tmp_path):\n    test_file_content = \"Hello \ud83c\udf46\ud83c\udf54\".encode()\n    p = tmp_path.joinpath(\"test.txt\")\n    p.write_bytes(test_file_content)\n    return str(p)\n\n\n@pytest.fixture\ndef test_dir(tmp_path):\n    test_file_content = \"World \ud83c\udf46\ud83c\udf54\ud83c\udf46\".encode()\n    p = tmp_path.joinpath(\"subdir\").joinpath(\"test.txt\")\n    p.parent.mkdir()\n    p.write_bytes(test_file_content)\n    p = tmp_path.joinpath(\"test.txt\")\n    p.write_bytes(test_file_content)\n    p = tmp_path.joinpath(\"empty-file.txt\")\n    p.write_text(\"\")\n    return str(tmp_path)\n\n\ndef test_init_validation_and_cleaning():\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ):\n        # Basic artifact uri\n        repo = get_artifact_repository(\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\"\n        )\n        assert (\n            repo.artifact_uri == \"dbfs:/databricks/mlflow-tracking/\"\n            \"MOCK-EXP/MOCK-RUN-ID/artifacts\"\n        )\n        assert repo.run_id == MOCK_RUN_ID\n        assert repo.run_relative_artifact_repo_root_path == \"\"\n\n        with pytest.raises(MlflowException, match=\"DBFS URI must be of the form dbfs\"):\n            DatabricksArtifactRepository(\"s3://test\")\n        with pytest.raises(MlflowException, match=\"Artifact URI incorrect\"):\n            DatabricksArtifactRepository(\"dbfs:/databricks/mlflow/EXP/RUN/artifact\")\n        with pytest.raises(MlflowException, match=\"DBFS URI must be of the form dbfs\"):\n            DatabricksArtifactRepository(\n                \"dbfs://scope:key@notdatabricks/databricks/mlflow-tracking/experiment/1/run/2\"\n            )\n\n\n@pytest.mark.parametrize(\n    (\"artifact_uri\", \"expected_uri\", \"expected_db_uri\"),\n    [\n        (\n            \"dbfs:/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"dbfs:/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"databricks://getTrackingUriDefault\",\n        ),  # see test body for the mock\n        (\n            \"dbfs://@databricks/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"dbfs:/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"databricks\",\n        ),\n        (\n            \"dbfs://someProfile@databricks/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"dbfs:/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"databricks://someProfile\",\n        ),\n        (\n            \"dbfs://scope:key@databricks/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"dbfs:/databricks/mlflow-tracking/experiment/1/run/2\",\n            \"databricks://scope:key\",\n        ),\n        (\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\",\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\",\n            \"databricks://getTrackingUriDefault\",\n        ),\n        (\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/awesome/path\",\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/awesome/path\",\n            \"databricks://getTrackingUriDefault\",\n        ),\n    ],\n)\ndef test_init_artifact_uri(artifact_uri, expected_uri, expected_db_uri):\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.get_databricks_host_creds\", return_value=None\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\", return_value=\"whatever\"\n    ), mock.patch(\n        \"mlflow.tracking.get_tracking_uri\", return_value=\"databricks://getTrackingUriDefault\"\n    ):\n        repo = DatabricksArtifactRepository(artifact_uri)\n        assert repo.artifact_uri == expected_uri\n        assert repo.databricks_profile_uri == expected_db_uri\n\n\n@pytest.mark.parametrize(\n    (\"artifact_uri\", \"expected_relative_path\"),\n    [\n        (\"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts\", \"\"),\n        (\"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts/arty\", \"arty\"),\n        (\n            \"dbfs://prof@databricks/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/artifacts/arty\",  # pylint: disable=line-too-long\n            \"arty\",\n        ),\n        (\n            \"dbfs:/databricks/mlflow-tracking/MOCK-EXP/MOCK-RUN-ID/awesome/path\",\n            \"../awesome/path\",\n        ),\n    ],\n)\ndef test_run_relative_artifact_repo_root_path(artifact_uri, expected_relative_path):\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ):\n        # Basic artifact uri\n        repo = get_artifact_repository(artifact_uri)\n        assert repo.run_id == MOCK_RUN_ID\n        assert repo.run_relative_artifact_repo_root_path == expected_relative_path\n\n\ndef test_extract_run_id():\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ):\n        expected_run_id = \"RUN_ID\"\n        repo = get_artifact_repository(\"dbfs:/databricks/mlflow-tracking/EXP/RUN_ID/artifact\")\n        assert repo.run_id == expected_run_id\n        repo = get_artifact_repository(\"dbfs:/databricks/mlflow-tracking/EXP_ID/RUN_ID/artifacts\")\n        assert repo.run_id == expected_run_id\n        repo = get_artifact_repository(\n            \"dbfs:/databricks///mlflow-tracking///EXP_ID///RUN_ID///artifacts/\"\n        )\n        assert repo.run_id == expected_run_id\n        repo = get_artifact_repository(\n            \"dbfs:/databricks///mlflow-tracking//EXP_ID//RUN_ID///artifacts//\"\n        )\n        assert repo.run_id == expected_run_id\n\n\n@pytest.mark.parametrize(\n    (\"artifact_path\", \"expected_location\"),\n    [(None, \"test.txt\"), (\"output\", \"output/test.txt\"), (\"\", \"test.txt\")],\n)\ndef test_log_artifact_azure(databricks_artifact_repo, test_file, artifact_path, expected_location):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._azure_upload_file\", return_value=None\n    ) as azure_upload_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        azure_upload_mock.assert_called_with(mock_credential_info, test_file, expected_location)\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_azure_with_headers(\n    databricks_artifact_repo, test_file, artifact_path, expected_location\n):\n    mock_azure_headers = {\n        \"x-ms-encryption-scope\": \"test-scope\",\n        \"x-ms-tags\": \"some-tags\",\n        \"x-ms-blob-type\": \"some-type\",\n    }\n    filtered_azure_headers = {\n        \"x-ms-encryption-scope\": \"test-scope\",\n        \"x-ms-tags\": \"some-tags\",\n    }\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI,\n        type=ArtifactCredentialType.AZURE_SAS_URI,\n        headers=[\n            ArtifactCredentialInfo.HttpHeader(name=header_name, value=header_value)\n            for header_name, header_value in mock_azure_headers.items()\n        ],\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        request_mock.assert_called_with(\n            \"put\",\n            f\"{MOCK_AZURE_SIGNED_URI}?comp=blocklist\",\n            allow_redirects=True,\n            data=ANY,\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n\n\ndef test_log_artifact_azure_blob_client_sas_error(databricks_artifact_repo, test_file):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", side_effect=MlflowException(\"MOCK ERROR\")\n    ):\n        with pytest.raises(MlflowException, match=r\"MOCK ERROR\"):\n            databricks_artifact_repo.log_artifact(test_file)\n        get_credential_infos_mock.assert_called_with(GetCredentialsForWrite, MOCK_RUN_ID, ANY)\n\n\n@pytest.mark.parametrize(\n    (\"artifact_path\", \"expected_location\"),\n    [(None, \"test.txt\"), (\"output\", \"output/test.txt\"), (\"\", \"test.txt\")],\n)\ndef test_log_artifact_adls_gen2(\n    databricks_artifact_repo, test_file, artifact_path, expected_location\n):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_ADLS_GEN2_SIGNED_URI,\n        type=ArtifactCredentialType.AZURE_ADLS_GEN2_SAS_URI,\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._azure_adls_gen2_upload_file\", return_value=None\n    ) as azure_adls_gen2_upload_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        azure_adls_gen2_upload_mock.assert_called_with(\n            mock_credential_info, test_file, expected_location\n        )\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_adls_gen2_with_headers(\n    databricks_artifact_repo, test_file, artifact_path, expected_location, monkeypatch\n):\n    mock_azure_headers = {\n        \"x-ms-content-type\": \"test-type\",\n        \"x-ms-owner\": \"some-owner\",\n        \"x-ms-something_not_supported\": \"some-value\",\n    }\n    filtered_azure_headers = {\n        \"x-ms-content-type\": \"test-type\",\n        \"x-ms-owner\": \"some-owner\",\n    }\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_ADLS_GEN2_SIGNED_URI,\n        type=ArtifactCredentialType.AZURE_ADLS_GEN2_SAS_URI,\n        headers=[\n            ArtifactCredentialInfo.HttpHeader(name=header_name, value=header_value)\n            for header_name, header_value in mock_azure_headers.items()\n        ],\n    )\n    monkeypatch.setenv(\"MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE\", \"5\")\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        # test with block size 5\n        request_mock.assert_any_call(\n            \"put\",\n            f\"{MOCK_ADLS_GEN2_SIGNED_URI}?resource=file\",\n            allow_redirects=True,\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n        request_mock.assert_any_call(\n            \"patch\",\n            f\"{MOCK_ADLS_GEN2_SIGNED_URI}?action=append&position=0\",\n            allow_redirects=True,\n            data=ANY,\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n        request_mock.assert_any_call(\n            \"patch\",\n            f\"{MOCK_ADLS_GEN2_SIGNED_URI}?action=append&position=5\",\n            allow_redirects=True,\n            data=ANY,\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n        request_mock.assert_any_call(\n            \"patch\",\n            f\"{MOCK_ADLS_GEN2_SIGNED_URI}?action=append&position=10\",\n            allow_redirects=True,\n            data=ANY,\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n        request_mock.assert_called_with(\n            \"patch\",\n            f\"{MOCK_ADLS_GEN2_SIGNED_URI}?action=flush&position=14\",\n            allow_redirects=True,\n            headers=filtered_azure_headers,\n            timeout=None,\n        )\n\n\ndef test_log_artifact_adls_gen2_flush_error(databricks_artifact_repo, test_file):\n    mock_successful_response = Response()\n    mock_successful_response.status_code = 200\n    mock_successful_response.close = lambda: None\n    mock_error_response = MlflowException(\"MOCK ERROR\")\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_ADLS_GEN2_SIGNED_URI,\n        type=ArtifactCredentialType.AZURE_ADLS_GEN2_SAS_URI,\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", side_effect=[mock_successful_response, mock_error_response]\n    ) as request_mock:\n        mock_credential_info = ArtifactCredentialInfo(\n            signed_uri=MOCK_ADLS_GEN2_SIGNED_URI,\n            type=ArtifactCredentialType.AZURE_ADLS_GEN2_SAS_URI,\n        )\n        with pytest.raises(MlflowException, match=r\"MOCK ERROR\"):\n            databricks_artifact_repo.log_artifact(test_file)\n        get_credential_infos_mock.assert_called_with(GetCredentialsForWrite, MOCK_RUN_ID, ANY)\n        assert request_mock.mock_calls == [\n            mock.call(\n                \"put\",\n                f\"{MOCK_ADLS_GEN2_SIGNED_URI}?resource=file\",\n                allow_redirects=True,\n                headers={},\n                timeout=None,\n            ),\n            mock.call(\n                \"patch\",\n                f\"{MOCK_ADLS_GEN2_SIGNED_URI}?action=append&position=0&flush=true\",\n                allow_redirects=True,\n                data=ANY,\n                headers={},\n                timeout=None,\n            ),\n        ]\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_aws(databricks_artifact_repo, test_file, artifact_path, expected_location):\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI, type=ArtifactCredentialType.AWS_PRESIGNED_URL\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        request_mock.assert_called_with(\n            \"put\", MOCK_AWS_SIGNED_URI, allow_redirects=True, data=ANY, headers={}, timeout=None\n        )\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_aws_with_headers(\n    databricks_artifact_repo, test_file, artifact_path, expected_location\n):\n    expected_headers = {header.name: header.value for header in MOCK_HEADERS}\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI,\n        type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n        headers=MOCK_HEADERS,\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        request_mock.assert_called_with(\n            \"put\",\n            MOCK_AWS_SIGNED_URI,\n            allow_redirects=True,\n            data=ANY,\n            headers=expected_headers,\n            timeout=None,\n        )\n\n\ndef test_log_artifact_aws_presigned_url_error(databricks_artifact_repo, test_file):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI, type=ArtifactCredentialType.AWS_PRESIGNED_URL\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", side_effect=MlflowException(\"MOCK ERROR\")\n    ):\n        with pytest.raises(MlflowException, match=\"MOCK ERROR\"):\n            databricks_artifact_repo.log_artifact(test_file)\n        get_credential_infos_mock.assert_called_with(GetCredentialsForWrite, MOCK_RUN_ID, ANY)\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_gcp(databricks_artifact_repo, test_file, artifact_path, expected_location):\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_GCP_SIGNED_URL, type=ArtifactCredentialType.GCP_SIGNED_URL\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        request_mock.assert_called_with(\n            \"put\", MOCK_GCP_SIGNED_URL, allow_redirects=True, data=ANY, headers={}, timeout=None\n        )\n\n\n@pytest.mark.parametrize((\"artifact_path\", \"expected_location\"), [(None, \"test.txt\")])\ndef test_log_artifact_gcp_with_headers(\n    databricks_artifact_repo, test_file, artifact_path, expected_location\n):\n    expected_headers = {header.name: header.value for header in MOCK_HEADERS}\n    mock_response = Response()\n    mock_response.status_code = 200\n    mock_response.close = lambda: None\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_GCP_SIGNED_URL,\n        type=ArtifactCredentialType.GCP_SIGNED_URL,\n        headers=MOCK_HEADERS,\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", return_value=mock_response\n    ) as request_mock:\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        request_mock.assert_called_with(\n            \"put\",\n            MOCK_GCP_SIGNED_URL,\n            allow_redirects=True,\n            data=ANY,\n            headers=expected_headers,\n            timeout=None,\n        )\n\n\ndef test_log_artifact_gcp_presigned_url_error(databricks_artifact_repo, test_file):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_GCP_SIGNED_URL, type=ArtifactCredentialType.GCP_SIGNED_URL\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        \"requests.Session.request\", side_effect=MlflowException(\"MOCK ERROR\")\n    ):\n        with pytest.raises(MlflowException, match=\"MOCK ERROR\"):\n            databricks_artifact_repo.log_artifact(test_file)\n        get_credential_infos_mock.assert_called_with(GetCredentialsForWrite, MOCK_RUN_ID, ANY)\n\n\n@pytest.mark.parametrize(\n    (\"artifact_path\", \"expected_location\"),\n    [\n        (None, posixpath.join(MOCK_SUBDIR, \"test.txt\")),\n        (\"test_path\", posixpath.join(MOCK_SUBDIR, \"test_path/test.txt\")),\n    ],\n)\ndef test_log_artifact_with_relative_path(test_file, artifact_path, expected_location):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._upload_to_cloud\", return_value=None\n    ) as upload_mock:\n        databricks_artifact_repo = get_artifact_repository(MOCK_SUBDIR_ROOT_URI)\n        databricks_artifact_repo.log_artifact(test_file, artifact_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForWrite, MOCK_RUN_ID, [expected_location]\n        )\n        artifact_file_path = posixpath.join(artifact_path or \"\", os.path.basename(test_file))\n        upload_mock.assert_called_with(\n            cloud_credential_info=mock_credential_info,\n            src_file_path=test_file,\n            artifact_file_path=artifact_file_path,\n        )\n\n\ndef test_list_artifacts(databricks_artifact_repo):\n    list_artifact_file_proto_mock = [FileInfo(path=\"a.txt\", is_dir=False, file_size=0)]\n    list_artifacts_dir_proto_mock = [\n        FileInfo(path=\"test/a.txt\", is_dir=False, file_size=100),\n        FileInfo(path=\"test/dir\", is_dir=True, file_size=0),\n    ]\n    list_artifact_response_proto = ListArtifacts.Response(\n        root_uri=\"\", files=list_artifacts_dir_proto_mock, next_page_token=None\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        return_value=list_artifact_response_proto,\n    ):\n        artifacts = databricks_artifact_repo.list_artifacts(\"test/\")\n        assert isinstance(artifacts, list)\n        assert isinstance(artifacts[0], FileInfoEntity)\n        assert len(artifacts) == 2\n        assert artifacts[0].path == \"test/a.txt\"\n        assert artifacts[0].is_dir is False\n        assert artifacts[0].file_size == 100\n        assert artifacts[1].path == \"test/dir\"\n        assert artifacts[1].is_dir is True\n        assert artifacts[1].file_size is None\n\n        # Calling list_artifacts() on a path that's a file should return an empty list\n        list_artifact_response_proto = ListArtifacts.Response(\n            root_uri=\"\", files=list_artifact_file_proto_mock\n        )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        return_value=list_artifact_response_proto,\n    ):\n        artifacts = databricks_artifact_repo.list_artifacts(\"a.txt\")\n        assert len(artifacts) == 0\n\n\ndef test_list_artifacts_with_relative_path():\n    list_artifact_file_proto_mock = [\n        FileInfo(path=posixpath.join(MOCK_SUBDIR, \"a.txt\"), is_dir=False, file_size=0)\n    ]\n    list_artifacts_dir_proto_mock = [\n        FileInfo(path=posixpath.join(MOCK_SUBDIR, \"test/a.txt\"), is_dir=False, file_size=100),\n        FileInfo(path=posixpath.join(MOCK_SUBDIR, \"test/dir\"), is_dir=True, file_size=0),\n    ]\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\", return_value=None\n    ) as message_mock:\n        list_artifact_response_proto = ListArtifacts.Response(\n            root_uri=\"\", files=list_artifacts_dir_proto_mock, next_page_token=None\n        )\n        with mock.patch(\n            f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n            return_value=list_artifact_response_proto,\n        ):\n            databricks_artifact_repo = get_artifact_repository(MOCK_SUBDIR_ROOT_URI)\n            artifacts = databricks_artifact_repo.list_artifacts(\"test\")\n            assert isinstance(artifacts, list)\n            assert isinstance(artifacts[0], FileInfoEntity)\n            assert len(artifacts) == 2\n            assert artifacts[0].path == \"test/a.txt\"\n            assert artifacts[0].is_dir is False\n            assert artifacts[0].file_size == 100\n            assert artifacts[1].path == \"test/dir\"\n            assert artifacts[1].is_dir is True\n            assert artifacts[1].file_size is None\n            message_mock.assert_called_with(\n                ListArtifacts(run_id=MOCK_RUN_ID, path=posixpath.join(MOCK_SUBDIR, \"test\"))\n            )\n\n        # Calling list_artifacts() on a relative path that's a file should return an empty list\n        with mock.patch(\n            f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n            return_value=ListArtifacts.Response(\n                root_uri=\"\", files=list_artifact_file_proto_mock, next_page_token=None\n            ),\n        ):\n            artifacts = databricks_artifact_repo.list_artifacts(\"a.txt\")\n            assert len(artifacts) == 0\n\n\ndef test_list_artifacts_handles_pagination(databricks_artifact_repo):\n    list_artifacts_proto_mock_1 = [\n        FileInfo(path=\"a.txt\", is_dir=False, file_size=100),\n        FileInfo(path=\"b\", is_dir=True, file_size=0),\n    ]\n    list_artifacts_proto_mock_2 = [\n        FileInfo(path=\"c.txt\", is_dir=False, file_size=100),\n        FileInfo(path=\"d\", is_dir=True, file_size=0),\n    ]\n    list_artifacts_proto_mock_3 = [\n        FileInfo(path=\"e.txt\", is_dir=False, file_size=100),\n        FileInfo(path=\"f\", is_dir=True, file_size=0),\n    ]\n    list_artifacts_proto_mock_4 = []\n    list_artifact_paginated_response_protos = [\n        ListArtifacts.Response(root_uri=\"\", files=list_artifacts_proto_mock_1, next_page_token=\"2\"),\n        ListArtifacts.Response(root_uri=\"\", files=list_artifacts_proto_mock_2, next_page_token=\"4\"),\n        ListArtifacts.Response(root_uri=\"\", files=list_artifacts_proto_mock_3, next_page_token=\"6\"),\n        ListArtifacts.Response(root_uri=\"\", files=list_artifacts_proto_mock_4, next_page_token=\"8\"),\n    ]\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\", return_value=None\n    ) as message_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=list_artifact_paginated_response_protos,\n    ):\n        artifacts = databricks_artifact_repo.list_artifacts()\n        assert {file.path for file in artifacts} == {\"a.txt\", \"b\", \"c.txt\", \"d\", \"e.txt\", \"f\"}\n        calls = [\n            mock.call(ListArtifacts(run_id=MOCK_RUN_ID, path=\"\")),\n            mock.call(ListArtifacts(run_id=MOCK_RUN_ID, path=\"\", page_token=\"2\")),\n            mock.call(ListArtifacts(run_id=MOCK_RUN_ID, path=\"\", page_token=\"4\")),\n            mock.call(ListArtifacts(run_id=MOCK_RUN_ID, path=\"\", page_token=\"6\")),\n        ]\n        message_mock.assert_has_calls(calls)\n\n\ndef test_get_read_credential_infos_handles_pagination(databricks_artifact_repo):\n    \"\"\"\n    Verifies that the `get_read_credential_infos` method, which is used to resolve read access\n    credentials for a collection of artifacts, handles paginated responses properly, issuing\n    incremental requests until all pages have been consumed\n    \"\"\"\n    credential_infos_mock_1 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_1\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        ),\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_2\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        ),\n    ]\n    credential_infos_mock_2 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_3\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n    ]\n    credential_infos_mock_3 = []\n    get_credentials_for_read_responses = [\n        GetCredentialsForRead.Response(\n            credential_infos=credential_infos_mock_1, next_page_token=\"2\"\n        ),\n        GetCredentialsForRead.Response(\n            credential_infos=credential_infos_mock_2, next_page_token=\"3\"\n        ),\n        GetCredentialsForRead.Response(credential_infos=credential_infos_mock_3),\n    ]\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\"\n    ) as message_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=get_credentials_for_read_responses,\n    ) as call_endpoint_mock:\n        read_credential_infos = databricks_artifact_repo._get_read_credential_infos([\"testpath\"])\n        assert read_credential_infos == credential_infos_mock_1 + credential_infos_mock_2\n        message_mock.assert_has_calls(\n            [\n                mock.call(GetCredentialsForRead(run_id=MOCK_RUN_ID, path=[\"testpath\"])),\n                mock.call(\n                    GetCredentialsForRead(run_id=MOCK_RUN_ID, path=[\"testpath\"], page_token=\"2\")\n                ),\n                mock.call(\n                    GetCredentialsForRead(run_id=MOCK_RUN_ID, path=[\"testpath\"], page_token=\"3\")\n                ),\n            ]\n        )\n        assert call_endpoint_mock.call_count == 3\n\n\ndef test_get_read_credential_infos_respects_max_request_size(databricks_artifact_repo):\n    \"\"\"\n    Verifies that the `_get_read_credential_infos` method, which is used to resolve read access\n    credentials for a collection of artifacts, handles paginated responses properly, issuing\n    incremental requests until all pages have been consumed\n    \"\"\"\n    assert _MAX_CREDENTIALS_REQUEST_SIZE == 2000, (\n        \"The maximum request size configured by the client should be consistent with the\"\n        \" Databricks backend. Only update this value of the backend limit has changed.\"\n    )\n\n    # Create 3 chunks of paths, two of which have the maximum request size and one of which\n    # is smaller than the maximum chunk size. Aggregate and pass these to\n    # `_get_read_credential_infos`, validating that this method decomposes the aggregate\n    # list into these expected chunks and makes 3 separate requests\n    paths_chunk_1 = [\"path1\"] * _MAX_CREDENTIALS_REQUEST_SIZE\n    paths_chunk_2 = [\"path2\"] * _MAX_CREDENTIALS_REQUEST_SIZE\n    paths_chunk_3 = [\"path3\"] * 5\n    credential_infos_mock_1 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_1\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(_MAX_CREDENTIALS_REQUEST_SIZE)\n    ]\n    credential_infos_mock_2 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_2\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(_MAX_CREDENTIALS_REQUEST_SIZE)\n    ]\n    credential_infos_mock_3 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_3\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(5)\n    ]\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\"\n    ) as message_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=[\n            GetCredentialsForRead.Response(credential_infos=credential_infos_mock_1),\n            GetCredentialsForRead.Response(credential_infos=credential_infos_mock_2),\n            GetCredentialsForRead.Response(credential_infos=credential_infos_mock_3),\n        ],\n    ) as call_endpoint_mock:\n        databricks_artifact_repo._get_read_credential_infos(\n            paths_chunk_1 + paths_chunk_2 + paths_chunk_3\n        )\n        assert call_endpoint_mock.call_count == 3\n        assert message_mock.call_count == 3\n        message_mock.assert_has_calls(\n            [\n                mock.call(GetCredentialsForRead(run_id=MOCK_RUN_ID, path=paths_chunk_1)),\n                mock.call(GetCredentialsForRead(run_id=MOCK_RUN_ID, path=paths_chunk_2)),\n                mock.call(GetCredentialsForRead(run_id=MOCK_RUN_ID, path=paths_chunk_3)),\n            ]\n        )\n\n\ndef test_get_write_credential_infos_handles_pagination(databricks_artifact_repo):\n    \"\"\"\n    Verifies that the `_get_write_credential_infos` method, which is used to resolve write\n    access credentials for a collection of artifacts, handles paginated responses properly,\n    issuing incremental requests until all pages have been consumed\n    \"\"\"\n    credential_infos_mock_1 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_1\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        ),\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_2\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        ),\n    ]\n    credential_infos_mock_2 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_3\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n    ]\n    credential_infos_mock_3 = []\n    get_credentials_for_write_responses = [\n        GetCredentialsForWrite.Response(\n            credential_infos=credential_infos_mock_1, next_page_token=\"2\"\n        ),\n        GetCredentialsForWrite.Response(\n            credential_infos=credential_infos_mock_2, next_page_token=\"3\"\n        ),\n        GetCredentialsForWrite.Response(credential_infos=credential_infos_mock_3),\n    ]\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\"\n    ) as message_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=get_credentials_for_write_responses,\n    ) as call_endpoint_mock:\n        write_credential_infos = databricks_artifact_repo._get_write_credential_infos([\"testpath\"])\n        assert write_credential_infos == credential_infos_mock_1 + credential_infos_mock_2\n        message_mock.assert_has_calls(\n            [\n                mock.call(GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=[\"testpath\"])),\n                mock.call(\n                    GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=[\"testpath\"], page_token=\"2\")\n                ),\n                mock.call(\n                    GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=[\"testpath\"], page_token=\"3\")\n                ),\n            ]\n        )\n        assert call_endpoint_mock.call_count == 3\n\n\ndef test_get_write_credential_infos_respects_max_request_size(databricks_artifact_repo):\n    \"\"\"\n    Verifies that the `_get_write_credential_infos` method, which is used to resolve write\n    access credentials for a collection of artifacts, batches requests according to a maximum\n    request size configured by the backend\n    \"\"\"\n    # Create 3 chunks of paths, two of which have the maximum request size and one of which\n    # is smaller than the maximum chunk size. Aggregate and pass these to\n    # `_get_write_credential_infos`, validating that this method decomposes the aggregate\n    # list into these expected chunks and makes 3 separate requests\n    paths_chunk_1 = [\"path1\"] * 2000\n    paths_chunk_2 = [\"path2\"] * 2000\n    paths_chunk_3 = [\"path3\"] * 5\n    credential_infos_mock_1 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_1\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(2000)\n    ]\n    credential_infos_mock_2 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_2\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(2000)\n    ]\n    credential_infos_mock_3 = [\n        ArtifactCredentialInfo(\n            signed_uri=\"http://mock_url_3\", type=ArtifactCredentialType.AWS_PRESIGNED_URL\n        )\n        for _ in range(5)\n    ]\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.message_to_json\"\n    ) as message_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=[\n            GetCredentialsForWrite.Response(credential_infos=credential_infos_mock_1),\n            GetCredentialsForWrite.Response(credential_infos=credential_infos_mock_2),\n            GetCredentialsForWrite.Response(credential_infos=credential_infos_mock_3),\n        ],\n    ) as call_endpoint_mock:\n        databricks_artifact_repo._get_write_credential_infos(\n            paths_chunk_1 + paths_chunk_2 + paths_chunk_3\n        )\n        assert call_endpoint_mock.call_count == message_mock.call_count == 3\n        message_mock.assert_has_calls(\n            [\n                mock.call(GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=paths_chunk_1)),\n                mock.call(GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=paths_chunk_2)),\n                mock.call(GetCredentialsForWrite(run_id=MOCK_RUN_ID, path=paths_chunk_3)),\n            ]\n        )\n\n\n@pytest.mark.parametrize(\n    (\"remote_file_path\", \"local_path\"),\n    [\n        (\"test_file.txt\", \"\"),\n        (\"test_file.txt\", None),\n        (\"output/test_file\", None),\n    ],\n)\ndef test_databricks_download_file(databricks_artifact_repo, remote_file_path, local_path):\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_read_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            )\n        ],\n    ) as read_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\", return_value=[]\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.download_file_using_http_uri\", return_value=None\n    ) as download_mock:\n        databricks_artifact_repo.download_artifacts(remote_file_path, local_path)\n        read_credential_infos_mock.assert_called_with(remote_file_path)\n        download_mock.assert_called_with(MOCK_AZURE_SIGNED_URI, ANY, ANY, {})\n\n\n@pytest.mark.parametrize(\n    (\"remote_file_path\", \"local_path\"), [(\"test_file.txt\", \"\"), (\"test_file.txt\", None)]\n)\ndef test_databricks_download_file_with_relative_path(remote_file_path, local_path):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_run_artifact_root\",\n        return_value=MOCK_RUN_ROOT_URI,\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as get_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\", return_value=[]\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY_PACKAGE}.download_file_using_http_uri\", return_value=None\n    ) as download_mock:\n        databricks_artifact_repo = get_artifact_repository(MOCK_SUBDIR_ROOT_URI)\n        databricks_artifact_repo.download_artifacts(remote_file_path, local_path)\n        get_credential_infos_mock.assert_called_with(\n            GetCredentialsForRead, MOCK_RUN_ID, [posixpath.join(MOCK_SUBDIR, remote_file_path)]\n        )\n        download_mock.assert_called_with(MOCK_AZURE_SIGNED_URI, ANY, ANY, ANY)\n\n\n@pytest.mark.parametrize(\n    (\"file_size\", \"is_parallel_download\"),\n    [(None, False), (100, False), (500 * 1024**2 - 1, False), (500 * 1024**2, True)],\n)\ndef test_databricks_download_file_in_parallel_when_necessary(\n    databricks_artifact_repo, file_size, is_parallel_download\n):\n    remote_file_path = \"file_1.txt\"\n    list_artifacts_result = (\n        [FileInfo(path=remote_file_path, is_dir=False, file_size=file_size)] if file_size else []\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\",\n        return_value=list_artifacts_result,\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._download_from_cloud\", return_value=None\n    ) as download_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._parallelized_download_from_cloud\", return_value=None\n    ) as parallel_download_mock:\n        databricks_artifact_repo.download_artifacts(\"\")\n        if is_parallel_download:\n            parallel_download_mock.assert_called_with(file_size, remote_file_path, ANY)\n        else:\n            download_mock.assert_called()\n\n\ndef test_databricks_download_file_get_request_fail(databricks_artifact_repo, test_file):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_read_credential_infos\",\n        return_value=[mock_credential_info],\n    ) as read_credential_infos_mock, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\", return_value=[]\n    ), mock.patch(\n        \"requests.Session.request\", side_effect=MlflowException(\"MOCK ERROR\")\n    ):\n        with pytest.raises(MlflowException, match=r\"MOCK ERROR\"):\n            databricks_artifact_repo.download_artifacts(test_file)\n        read_credential_infos_mock.assert_called_with(test_file)\n\n\ndef test_download_artifacts_awaits_download_completion(databricks_artifact_repo, tmp_path):\n    \"\"\"\n    Verifies that all asynchronous artifact downloads are joined before `download_artifacts()`\n    returns a result to the caller\n    \"\"\"\n\n    def mock_download_from_cloud(\n        cloud_credential_info,  # pylint: disable=unused-argument\n        dst_local_file_path,\n    ):\n        # Sleep in order to simulate a longer-running asynchronous download\n        time.sleep(2)\n        with open(dst_local_file_path, \"w\") as f:\n            f.write(\"content\")\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_read_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\",\n        return_value=[\n            FileInfo(path=\"file_1.txt\", is_dir=False, file_size=100),\n            FileInfo(path=\"file_2.txt\", is_dir=False, file_size=0),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._download_from_cloud\",\n        side_effect=mock_download_from_cloud,\n    ):\n        databricks_artifact_repo.download_artifacts(\"test_path\", str(tmp_path))\n        expected_file1_path = os.path.join(str(tmp_path), \"file_1.txt\")\n        expected_file2_path = os.path.join(str(tmp_path), \"file_2.txt\")\n        for path in [expected_file1_path, expected_file2_path]:\n            assert os.path.exists(path)\n            with open(path) as f:\n                assert f.read() == \"content\"\n\n\ndef test_artifact_logging(databricks_artifact_repo, tmp_path):\n    \"\"\"\n    Verifies that `log_artifact()` and `log_artifacts()` initiate all expected asynchronous\n    artifact uploads and await their completion before returning results to the caller\n    \"\"\"\n    src_dir = tmp_path.joinpath(\"src\")\n    src_dir.mkdir()\n    src_file1_path = src_dir.joinpath(\"file_1.txt\")\n    src_file1_path.write_text(\"file1\")\n    src_file2_path = src_dir.joinpath(\"file_2.txt\")\n    src_file2_path.write_text(\"file2\")\n\n    dst_dir = tmp_path.joinpath(\"dst\")\n    dst_dir.mkdir()\n\n    def mock_upload_to_cloud(\n        cloud_credential_info,  # pylint: disable=unused-argument\n        src_file_path,\n        artifact_file_path,\n    ):\n        # Sleep in order to simulate a longer-running asynchronous upload\n        time.sleep(2)\n        dst_run_relative_artifact_path = dst_dir.joinpath(artifact_file_path)\n        dst_run_relative_artifact_path.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copy2(src=src_file_path, dst=dst_run_relative_artifact_path)\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._upload_to_cloud\", side_effect=mock_upload_to_cloud\n    ):\n        databricks_artifact_repo.log_artifacts(src_dir, \"dir_artifact\")\n\n        expected_dst_dir_file1_path = os.path.join(dst_dir, \"dir_artifact\", \"file_1.txt\")\n        expected_dst_dir_file2_path = os.path.join(dst_dir, \"dir_artifact\", \"file_2.txt\")\n        assert os.path.exists(expected_dst_dir_file1_path)\n        assert os.path.exists(expected_dst_dir_file2_path)\n        with open(expected_dst_dir_file1_path) as f:\n            assert f.read() == \"file1\"\n        with open(expected_dst_dir_file2_path) as f:\n            assert f.read() == \"file2\"\n\n        databricks_artifact_repo.log_artifact(src_file1_path)\n\n        expected_dst_file_path = os.path.join(dst_dir, \"file_1.txt\")\n        assert os.path.exists(expected_dst_file_path)\n        with open(expected_dst_file_path) as f:\n            assert f.read() == \"file1\"\n\n\ndef test_artifact_logging_chunks_upload_list(databricks_artifact_repo, tmp_path):\n    \"\"\"\n    Verifies that write credentials are fetched in chunks rather than all at once.\n    \"\"\"\n    src_dir = tmp_path.joinpath(\"src\")\n    src_dir.mkdir()\n    for i in range(10):\n        src_dir.joinpath(f\"file_{i}.txt\").write_text(f\"file{i}\")\n    dst_dir = tmp_path.joinpath(\"dst\")\n    dst_dir.mkdir()\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n        ],\n    ) as mock_get_write_creds, mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._upload_to_cloud\"\n    ), mock.patch(\n        f\"{CLOUD_ARTIFACT_REPOSITORY_PACKAGE}._ARTIFACT_UPLOAD_BATCH_SIZE\", 2\n    ):\n        databricks_artifact_repo.log_artifacts(src_dir, \"dir_artifact\")\n\n        assert mock_get_write_creds.call_count == 5\n        assert all(\n            len(call[1][\"remote_file_paths\"]) == 2 for call in mock_get_write_creds.call_args_list\n        )\n\n\ndef test_download_artifacts_provides_failure_info(databricks_artifact_repo):\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_read_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}.list_artifacts\",\n        return_value=[\n            FileInfo(path=\"file_1.txt\", is_dir=False, file_size=100),\n            FileInfo(path=\"file_2.txt\", is_dir=False, file_size=0),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._download_from_cloud\",\n        side_effect=[\n            MlflowException(\"MOCK ERROR 1\"),\n            MlflowException(\"MOCK ERROR 2\"),\n        ],\n    ):\n        match = r\"The following failures occurred while downloading one or more artifacts.+\"\n        with pytest.raises(MlflowException, match=match) as exc:\n            databricks_artifact_repo.download_artifacts(\"test_path\")\n\n        err_msg = str(exc.value)\n        assert MOCK_RUN_ROOT_URI in err_msg\n        assert \"file_1.txt\" in err_msg\n        assert \"MOCK ERROR 1\" in err_msg\n        assert \"file_2.txt\" in err_msg\n        assert \"MOCK ERROR 2\" in err_msg\n\n\ndef test_log_artifacts_provides_failure_info(databricks_artifact_repo, tmp_path):\n    src_file1_path = os.path.join(tmp_path, \"file_1.txt\")\n    with open(src_file1_path, \"w\") as f:\n        f.write(\"file1\")\n    src_file2_path = os.path.join(tmp_path, \"file_2.txt\")\n    with open(src_file2_path, \"w\") as f:\n        f.write(\"file2\")\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n            ArtifactCredentialInfo(\n                signed_uri=MOCK_AZURE_SIGNED_URI, type=ArtifactCredentialType.AZURE_SAS_URI\n            ),\n        ],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._upload_to_cloud\",\n        side_effect=[\n            MlflowException(\"MOCK ERROR 1\"),\n            MlflowException(\"MOCK ERROR 2\"),\n        ],\n    ):\n        match = (\n            r\"The following failures occurred while uploading one or more artifacts.+\"\n            r\"MOCK ERROR 1.+\"\n            r\"MOCK ERROR 2\"\n        )\n        with pytest.raises(MlflowException, match=match) as exc:\n            databricks_artifact_repo.log_artifacts(str(tmp_path), \"test_artifacts\")\n\n        err_msg = str(exc.value)\n        assert MOCK_RUN_ROOT_URI in err_msg\n        assert \"file_1.txt\" in err_msg\n        assert \"MOCK ERROR 1\" in err_msg\n        assert \"file_2.txt\" in err_msg\n        assert \"MOCK ERROR 2\" in err_msg\n\n\n@pytest.fixture\ndef mock_chunk_size(monkeypatch):\n    # Use a smaller chunk size for faster comparison\n    chunk_size = 10\n    monkeypatch.setenv(\"MLFLOW_MULTIPART_UPLOAD_CHUNK_SIZE\", str(chunk_size))\n    return chunk_size\n\n\n@pytest.fixture\ndef large_file(tmp_path, mock_chunk_size):\n    path = tmp_path.joinpath(\"large_file\")\n    with path.open(\"a\") as f:\n        f.write(\"a\" * mock_chunk_size)\n        f.write(\"b\" * mock_chunk_size)\n    return path\n\n\ndef extract_part_number(url):\n    return int(re.search(r\"partNumber=(\\d+)\", url).group(1))\n\n\ndef mock_request(method, url, *_args, **_kwargs):\n    resp = Response()\n    resp.status_code = 200\n    resp.close = lambda: None\n    if method.lower() == \"delete\":\n        # Abort-multipart-upload request\n        return resp\n    elif method.lower() == \"put\":\n        # Upload-part request\n        part_number = extract_part_number(url)\n        resp.headers = {\"ETag\": f\"etag-{part_number}\"}\n        return resp\n    else:\n        raise Exception(\"Unreachable\")\n\n\ndef test_multipart_upload(databricks_artifact_repo, large_file, mock_chunk_size):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI, type=ArtifactCredentialType.AWS_PRESIGNED_URL\n    )\n    mock_upload_id = \"upload_id\"\n    create_mpu_response = CreateMultipartUpload.Response(\n        upload_id=mock_upload_id,\n        upload_credential_infos=[\n            ArtifactCredentialInfo(\n                signed_uri=f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n                headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=f\"part-{i + 1}\")],\n            )\n            for i in range(2)\n        ],\n        abort_credential_info=ArtifactCredentialInfo(\n            signed_uri=f\"{MOCK_AWS_SIGNED_URI}uploadId=abort\",\n            type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n            headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=\"abort\")],\n        ),\n    )\n    complete_mpu_response = CompleteMultipartUpload.Response()\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[mock_credential_info],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=[create_mpu_response, complete_mpu_response],\n    ) as call_endpoint_mock, mock.patch(\n        \"requests.Session.request\", side_effect=mock_request\n    ) as http_request_mock:\n        databricks_artifact_repo.log_artifact(large_file)\n        with large_file.open(\"rb\") as f:\n            expected_calls = [\n                mock.call(\n                    \"put\",\n                    f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                    allow_redirects=True,\n                    data=f.read(mock_chunk_size),\n                    headers={\"header\": f\"part-{i + 1}\"},\n                    timeout=None,\n                )\n                for i in range(2)\n            ]\n        # The upload-part requests are sent in parallel, so the order of the calls is not\n        # deterministic\n        assert sorted(http_request_mock.call_args_list, key=lambda c: c.args[1]) == expected_calls\n        complete_request_body = json.loads(call_endpoint_mock.call_args_list[-1].args[-1])\n        assert complete_request_body[\"upload_id\"] == mock_upload_id\n        assert complete_request_body[\"part_etags\"] == [\n            {\"part_number\": 1, \"etag\": \"etag-1\"},\n            {\"part_number\": 2, \"etag\": \"etag-2\"},\n        ]\n\n\n# The first request will fail with a 403, and the second will succeed\nSTATUS_CODE_GENERATOR = (s for s in (403, 200))\n\n\ndef mock_request_retry(method, url, *_args, **_kwargs):\n    resp = Response()\n    resp.status_code = 200\n    resp.close = lambda: None\n    if method.lower() == \"delete\":\n        # Abort-multipart-upload request\n        return resp\n    elif method.lower() == \"put\":\n        # Upload-part request\n        part_number = extract_part_number(url)\n        resp.headers = {\"ETag\": f\"etag-{part_number}\"}\n        # To ensure the upload-part retry logic works correctly,\n        # make the first attempt of the second part upload fail by responding with a 403,\n        # then make the second attempt succeed by responding with a 200\n        if part_number == 2:\n            status_code = next(STATUS_CODE_GENERATOR)\n            resp.status_code = status_code\n        return resp\n    else:\n        raise Exception(\"Unreachable\")\n\n\ndef test_multipart_upload_retry_part_upload(databricks_artifact_repo, large_file, mock_chunk_size):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI, type=ArtifactCredentialType.AWS_PRESIGNED_URL\n    )\n    mock_upload_id = \"upload_id\"\n    create_mpu_response = CreateMultipartUpload.Response(\n        upload_id=mock_upload_id,\n        upload_credential_infos=[\n            ArtifactCredentialInfo(\n                signed_uri=f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n                headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=f\"part-{i + 1}\")],\n            )\n            for i in range(2)\n        ],\n        abort_credential_info=ArtifactCredentialInfo(\n            signed_uri=f\"{MOCK_AWS_SIGNED_URI}uploadId=abort\",\n            type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n            headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=\"abort\")],\n        ),\n    )\n    part_upload_url_response = GetPresignedUploadPartUrl.Response(\n        upload_credential_info=ArtifactCredentialInfo(\n            signed_uri=f\"{MOCK_AWS_SIGNED_URI}partNumber=2\",\n            type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n            headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=\"part-2\")],\n        ),\n    )\n    complete_mpu_response = CompleteMultipartUpload.Response()\n\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[mock_credential_info],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=[create_mpu_response, part_upload_url_response, complete_mpu_response],\n    ) as call_endpoint_mock, mock.patch(\n        \"requests.Session.request\", side_effect=mock_request_retry\n    ) as http_request_mock:\n        databricks_artifact_repo.log_artifact(large_file)\n\n        with large_file.open(\"rb\") as f:\n            expected_calls = [\n                mock.call(\n                    \"put\",\n                    f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                    allow_redirects=True,\n                    data=f.read(mock_chunk_size),\n                    headers={\"header\": f\"part-{i + 1}\"},\n                    timeout=None,\n                )\n                for i in range(2)\n            ]\n        expected_calls += expected_calls[-1:]  # Append the second part upload call\n        # The upload-part requests are sent in parallel, so the order of the calls is not\n        # deterministic\n        assert sorted(http_request_mock.call_args_list, key=lambda c: c.args[1]) == expected_calls\n        complete_request_body = json.loads(call_endpoint_mock.call_args_list[-1].args[-1])\n        assert complete_request_body[\"upload_id\"] == mock_upload_id\n        assert complete_request_body[\"part_etags\"] == [\n            {\"part_number\": 1, \"etag\": \"etag-1\"},\n            {\"part_number\": 2, \"etag\": \"etag-2\"},\n        ]\n\n\ndef test_multipart_upload_abort(databricks_artifact_repo, large_file, mock_chunk_size):\n    mock_credential_info = ArtifactCredentialInfo(\n        signed_uri=MOCK_AWS_SIGNED_URI,\n        type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n    )\n    mock_upload_id = \"upload_id\"\n    create_mpu_response = CreateMultipartUpload.Response(\n        upload_id=mock_upload_id,\n        upload_credential_infos=[\n            ArtifactCredentialInfo(\n                signed_uri=f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n                headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=f\"part-{i + 1}\")],\n            )\n            for i in range(2)\n        ],\n        abort_credential_info=ArtifactCredentialInfo(\n            signed_uri=f\"{MOCK_AWS_SIGNED_URI}uploadId=abort\",\n            type=ArtifactCredentialType.AWS_PRESIGNED_URL,\n            headers=[ArtifactCredentialInfo.HttpHeader(name=\"header\", value=\"abort\")],\n        ),\n    )\n    with mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._get_write_credential_infos\",\n        return_value=[mock_credential_info],\n    ), mock.patch(\n        f\"{DATABRICKS_ARTIFACT_REPOSITORY}._call_endpoint\",\n        side_effect=[create_mpu_response, Exception(\"Failed to complete multipart upload\")],\n    ) as call_endpoint_mock, mock.patch(\n        \"requests.Session.request\", side_effect=mock_request\n    ) as http_request_mock:\n        with pytest.raises(Exception, match=\"Failed to complete multipart upload\"):\n            databricks_artifact_repo.log_artifact(large_file)\n\n        (*part_upload_calls, abort_call) = http_request_mock.call_args_list\n        with large_file.open(\"rb\") as f:\n            expected_calls = [\n                mock.call(\n                    \"put\",\n                    f\"{MOCK_AWS_SIGNED_URI}partNumber={i + 1}\",\n                    allow_redirects=True,\n                    data=f.read(mock_chunk_size),\n                    headers={\"header\": f\"part-{i + 1}\"},\n                    timeout=None,\n                )\n                for i in range(2)\n            ]\n        # The upload-part requests are sent in parallel, so the order of the calls is not\n        # deterministic\n        assert sorted(part_upload_calls, key=lambda c: c.args[1]) == expected_calls\n        complete_request_body = json.loads(call_endpoint_mock.call_args_list[-1].args[-1])\n        assert complete_request_body[\"upload_id\"] == mock_upload_id\n        assert complete_request_body[\"part_etags\"] == [\n            {\"part_number\": 1, \"etag\": \"etag-1\"},\n            {\"part_number\": 2, \"etag\": \"etag-2\"},\n        ]\n        assert abort_call == mock.call(\n            \"delete\",\n            f\"{MOCK_AWS_SIGNED_URI}uploadId=abort\",\n            allow_redirects=True,\n            headers={\"header\": \"abort\"},\n            timeout=None,\n        )\n", "import json\nfrom unittest import mock\n\nimport pytest\n\nimport mlflow\nfrom mlflow.entities import (\n    Dataset,\n    DatasetInput,\n    Experiment,\n    ExperimentTag,\n    InputTag,\n    LifecycleStage,\n    Metric,\n    Param,\n    RunTag,\n    SourceType,\n    ViewType,\n)\nfrom mlflow.exceptions import MlflowException\nfrom mlflow.models import Model\nfrom mlflow.protos.databricks_pb2 import RESOURCE_DOES_NOT_EXIST\nfrom mlflow.protos.service_pb2 import (\n    CreateRun,\n    DeleteExperiment,\n    DeleteRun,\n    DeleteTag,\n    GetExperimentByName,\n    LogBatch,\n    LogInputs,\n    LogMetric,\n    LogModel,\n    LogParam,\n    RestoreExperiment,\n    RestoreRun,\n    SearchExperiments,\n    SearchRuns,\n    SetExperimentTag,\n    SetTag,\n)\nfrom mlflow.protos.service_pb2 import RunTag as ProtoRunTag\nfrom mlflow.store.tracking.rest_store import RestStore\nfrom mlflow.tracking.request_header.default_request_header_provider import (\n    DefaultRequestHeaderProvider,\n)\nfrom mlflow.utils.proto_json_utils import message_to_json\nfrom mlflow.utils.rest_utils import MlflowHostCreds\n\n\nclass MyCoolException(Exception):\n    pass\n\n\nclass CustomErrorHandlingRestStore(RestStore):\n    def _call_endpoint(self, api, json_body):\n        raise MyCoolException(\"cool\")\n\n\ndef mock_http_request():\n    return mock.patch(\n        \"mlflow.utils.rest_utils.http_request\",\n        return_value=mock.MagicMock(status_code=200, text=\"{}\"),\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_successful_http_request(request):\n    def mock_request(*args, **kwargs):\n        # Filter out None arguments\n        assert args == (\"POST\", \"https://hello/api/2.0/mlflow/experiments/search\")\n        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n        assert kwargs == {\n            \"allow_redirects\": True,\n            \"json\": {\"view_type\": \"ACTIVE_ONLY\"},\n            \"headers\": DefaultRequestHeaderProvider().request_headers(),\n            \"verify\": True,\n            \"timeout\": 120,\n        }\n        response = mock.MagicMock()\n        response.status_code = 200\n        response.text = '{\"experiments\": [{\"name\": \"Exp!\", \"lifecycle_stage\": \"active\"}]}'\n        return response\n\n    request.side_effect = mock_request\n\n    store = RestStore(lambda: MlflowHostCreds(\"https://hello\"))\n    experiments = store.search_experiments()\n    assert experiments[0].name == \"Exp!\"\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_failed_http_request(request):\n    response = mock.MagicMock()\n    response.status_code = 404\n    response.text = '{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"No experiment\"}'\n    request.return_value = response\n\n    store = RestStore(lambda: MlflowHostCreds(\"https://hello\"))\n    with pytest.raises(MlflowException, match=\"RESOURCE_DOES_NOT_EXIST: No experiment\"):\n        store.search_experiments()\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_failed_http_request_custom_handler(request):\n    response = mock.MagicMock()\n    response.status_code = 404\n    response.text = '{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"No experiment\"}'\n    request.return_value = response\n\n    store = CustomErrorHandlingRestStore(lambda: MlflowHostCreds(\"https://hello\"))\n    with pytest.raises(MyCoolException, match=\"cool\"):\n        store.search_experiments()\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_response_with_unknown_fields(request):\n    experiment_json = {\n        \"experiment_id\": \"1\",\n        \"name\": \"My experiment\",\n        \"artifact_location\": \"foo\",\n        \"lifecycle_stage\": \"deleted\",\n        \"OMG_WHAT_IS_THIS_FIELD\": \"Hooly cow\",\n    }\n\n    response = mock.MagicMock()\n    response.status_code = 200\n    experiments = {\"experiments\": [experiment_json]}\n    response.text = json.dumps(experiments)\n    request.return_value = response\n\n    store = RestStore(lambda: MlflowHostCreds(\"https://hello\"))\n    experiments = store.search_experiments()\n    assert len(experiments) == 1\n    assert experiments[0].name == \"My experiment\"\n\n\ndef _args(host_creds, endpoint, method, json_body):\n    res = {\n        \"host_creds\": host_creds,\n        \"endpoint\": f\"/api/2.0/mlflow/{endpoint}\",\n        \"method\": method,\n    }\n    if method == \"GET\":\n        res[\"params\"] = json.loads(json_body)\n    else:\n        res[\"json\"] = json.loads(json_body)\n    return res\n\n\ndef _verify_requests(http_request, host_creds, endpoint, method, json_body):\n    http_request.assert_any_call(**(_args(host_creds, endpoint, method, json_body)))\n\n\ndef test_requestor():\n    creds = MlflowHostCreds(\"https://hello\")\n    store = RestStore(lambda: creds)\n\n    user_name = \"mock user\"\n    source_name = \"rest test\"\n    run_name = \"my name\"\n\n    source_name_patch = mock.patch(\n        \"mlflow.tracking.context.default_context._get_source_name\", return_value=source_name\n    )\n    source_type_patch = mock.patch(\n        \"mlflow.tracking.context.default_context._get_source_type\",\n        return_value=SourceType.LOCAL,\n    )\n    with mock_http_request() as mock_http, mock.patch(\n        \"mlflow.tracking._tracking_service.utils._get_store\", return_value=store\n    ), mock.patch(\n        \"mlflow.tracking.context.default_context._get_user\", return_value=user_name\n    ), mock.patch(\n        \"time.time\", return_value=13579\n    ), source_name_patch, source_type_patch:\n        with mlflow.start_run(experiment_id=\"43\", run_name=run_name):\n            cr_body = message_to_json(\n                CreateRun(\n                    experiment_id=\"43\",\n                    user_id=user_name,\n                    run_name=run_name,\n                    start_time=13579000,\n                    tags=[\n                        ProtoRunTag(key=\"mlflow.source.name\", value=source_name),\n                        ProtoRunTag(key=\"mlflow.source.type\", value=\"LOCAL\"),\n                        ProtoRunTag(key=\"mlflow.user\", value=user_name),\n                        ProtoRunTag(key=\"mlflow.runName\", value=run_name),\n                    ],\n                )\n            )\n            expected_kwargs = _args(creds, \"runs/create\", \"POST\", cr_body)\n\n            assert mock_http.call_count == 1\n            actual_kwargs = mock_http.call_args[1]\n\n            # Test the passed tag values separately from the rest of the request\n            # Tag order is inconsistent on Python 2 and 3, but the order does not matter\n            expected_tags = expected_kwargs[\"json\"].pop(\"tags\")\n            actual_tags = actual_kwargs[\"json\"].pop(\"tags\")\n\n            assert sorted(expected_tags, key=lambda t: t[\"key\"]) == sorted(\n                actual_tags, key=lambda t: t[\"key\"]\n            )\n            assert expected_kwargs == actual_kwargs\n\n    with mock_http_request() as mock_http:\n        store.log_param(\"some_uuid\", Param(\"k1\", \"v1\"))\n        body = message_to_json(\n            LogParam(run_uuid=\"some_uuid\", run_id=\"some_uuid\", key=\"k1\", value=\"v1\")\n        )\n        _verify_requests(mock_http, creds, \"runs/log-parameter\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        store.set_experiment_tag(\"some_id\", ExperimentTag(\"t1\", \"abcd\" * 1000))\n        body = message_to_json(\n            SetExperimentTag(experiment_id=\"some_id\", key=\"t1\", value=\"abcd\" * 1000)\n        )\n        _verify_requests(mock_http, creds, \"experiments/set-experiment-tag\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        store.set_tag(\"some_uuid\", RunTag(\"t1\", \"abcd\" * 1000))\n        body = message_to_json(\n            SetTag(run_uuid=\"some_uuid\", run_id=\"some_uuid\", key=\"t1\", value=\"abcd\" * 1000)\n        )\n        _verify_requests(mock_http, creds, \"runs/set-tag\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        store.delete_tag(\"some_uuid\", \"t1\")\n        body = message_to_json(DeleteTag(run_id=\"some_uuid\", key=\"t1\"))\n        _verify_requests(mock_http, creds, \"runs/delete-tag\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        store.log_metric(\"u2\", Metric(\"m1\", 0.87, 12345, 3))\n        body = message_to_json(\n            LogMetric(run_uuid=\"u2\", run_id=\"u2\", key=\"m1\", value=0.87, timestamp=12345, step=3)\n        )\n        _verify_requests(mock_http, creds, \"runs/log-metric\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        metrics = [\n            Metric(\"m1\", 0.87, 12345, 0),\n            Metric(\"m2\", 0.49, 12345, -1),\n            Metric(\"m3\", 0.58, 12345, 2),\n        ]\n        params = [Param(\"p1\", \"p1val\"), Param(\"p2\", \"p2val\")]\n        tags = [RunTag(\"t1\", \"t1val\"), RunTag(\"t2\", \"t2val\")]\n        store.log_batch(run_id=\"u2\", metrics=metrics, params=params, tags=tags)\n        metric_protos = [metric.to_proto() for metric in metrics]\n        param_protos = [param.to_proto() for param in params]\n        tag_protos = [tag.to_proto() for tag in tags]\n        body = message_to_json(\n            LogBatch(run_id=\"u2\", metrics=metric_protos, params=param_protos, tags=tag_protos)\n        )\n        _verify_requests(mock_http, creds, \"runs/log-batch\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        dataset = Dataset(name=\"name\", digest=\"digest\", source_type=\"st\", source=\"source\")\n        tag = InputTag(key=\"k1\", value=\"v1\")\n        dataset_input = DatasetInput(dataset=dataset, tags=[tag])\n        store.log_inputs(\"some_uuid\", [dataset_input])\n        body = message_to_json(LogInputs(run_id=\"some_uuid\", datasets=[dataset_input.to_proto()]))\n        _verify_requests(mock_http, creds, \"runs/log-inputs\", \"POST\", body)\n\n    with mock_http_request() as mock_http:\n        store.delete_run(\"u25\")\n        _verify_requests(\n            mock_http, creds, \"runs/delete\", \"POST\", message_to_json(DeleteRun(run_id=\"u25\"))\n        )\n\n    with mock_http_request() as mock_http:\n        store.restore_run(\"u76\")\n        _verify_requests(\n            mock_http, creds, \"runs/restore\", \"POST\", message_to_json(RestoreRun(run_id=\"u76\"))\n        )\n\n    with mock_http_request() as mock_http:\n        store.delete_experiment(\"0\")\n        _verify_requests(\n            mock_http,\n            creds,\n            \"experiments/delete\",\n            \"POST\",\n            message_to_json(DeleteExperiment(experiment_id=\"0\")),\n        )\n\n    with mock_http_request() as mock_http:\n        store.restore_experiment(\"0\")\n        _verify_requests(\n            mock_http,\n            creds,\n            \"experiments/restore\",\n            \"POST\",\n            message_to_json(RestoreExperiment(experiment_id=\"0\")),\n        )\n\n    with mock.patch(\"mlflow.utils.rest_utils.http_request\") as mock_http:\n        response = mock.MagicMock()\n        response.status_code = 200\n        response.text = '{\"runs\": [\"1a\", \"2b\", \"3c\"], \"next_page_token\": \"67890fghij\"}'\n        mock_http.return_value = response\n        result = store.search_runs(\n            [\"0\", \"1\"],\n            \"params.p1 = 'a'\",\n            ViewType.ACTIVE_ONLY,\n            max_results=10,\n            order_by=[\"a\"],\n            page_token=\"12345abcde\",\n        )\n\n        expected_message = SearchRuns(\n            experiment_ids=[\"0\", \"1\"],\n            filter=\"params.p1 = 'a'\",\n            run_view_type=ViewType.to_proto(ViewType.ACTIVE_ONLY),\n            max_results=10,\n            order_by=[\"a\"],\n            page_token=\"12345abcde\",\n        )\n        _verify_requests(mock_http, creds, \"runs/search\", \"POST\", message_to_json(expected_message))\n        assert result.token == \"67890fghij\"\n\n    with mock_http_request() as mock_http:\n        run_id = \"run_id\"\n        m = Model(artifact_path=\"model/path\", run_id=\"run_id\", flavors={\"tf\": \"flavor body\"})\n        store.record_logged_model(\"run_id\", m)\n        expected_message = LogModel(run_id=run_id, model_json=m.to_json())\n        _verify_requests(\n            mock_http, creds, \"runs/log-model\", \"POST\", message_to_json(expected_message)\n        )\n\n\ndef test_get_experiment_by_name():\n    creds = MlflowHostCreds(\"https://hello\")\n    store = RestStore(lambda: creds)\n    with mock.patch(\"mlflow.utils.rest_utils.http_request\") as mock_http:\n        response = mock.MagicMock()\n        response.status_code = 200\n        experiment = Experiment(\n            experiment_id=\"123\",\n            name=\"abc\",\n            artifact_location=\"/abc\",\n            lifecycle_stage=LifecycleStage.ACTIVE,\n        )\n        response.text = json.dumps(\n            {\"experiment\": json.loads(message_to_json(experiment.to_proto()))}\n        )\n        mock_http.return_value = response\n        result = store.get_experiment_by_name(\"abc\")\n        expected_message0 = GetExperimentByName(experiment_name=\"abc\")\n        _verify_requests(\n            mock_http,\n            creds,\n            \"experiments/get-by-name\",\n            \"GET\",\n            message_to_json(expected_message0),\n        )\n        assert result.experiment_id == experiment.experiment_id\n        assert result.name == experiment.name\n        assert result.artifact_location == experiment.artifact_location\n        assert result.lifecycle_stage == experiment.lifecycle_stage\n        # Test GetExperimentByName against nonexistent experiment\n        mock_http.reset_mock()\n        nonexistent_exp_response = mock.MagicMock()\n        nonexistent_exp_response.status_code = 404\n        nonexistent_exp_response.text = MlflowException(\n            \"Exp doesn't exist!\", RESOURCE_DOES_NOT_EXIST\n        ).serialize_as_json()\n        mock_http.return_value = nonexistent_exp_response\n        assert store.get_experiment_by_name(\"nonexistent-experiment\") is None\n        expected_message1 = GetExperimentByName(experiment_name=\"nonexistent-experiment\")\n        _verify_requests(\n            mock_http,\n            creds,\n            \"experiments/get-by-name\",\n            \"GET\",\n            message_to_json(expected_message1),\n        )\n        assert mock_http.call_count == 1\n\n\ndef test_search_experiments():\n    creds = MlflowHostCreds(\"https://hello\")\n    store = RestStore(lambda: creds)\n\n    with mock_http_request() as mock_http:\n        store.search_experiments(\n            view_type=ViewType.DELETED_ONLY,\n            max_results=5,\n            filter_string=\"name\",\n            order_by=[\"name\"],\n            page_token=\"abc\",\n        )\n        _verify_requests(\n            mock_http,\n            creds,\n            \"experiments/search\",\n            \"POST\",\n            message_to_json(\n                SearchExperiments(\n                    view_type=ViewType.DELETED_ONLY,\n                    max_results=5,\n                    filter=\"name\",\n                    order_by=[\"name\"],\n                    page_token=\"abc\",\n                )\n            ),\n        )\n\n\ndef _mock_response_with_200_status_code():\n    mock_response = mock.MagicMock()\n    mock_response.status_code = 200\n    return mock_response\n\n\ndef test_get_metric_history_paginated():\n    creds = MlflowHostCreds(\"https://hello\")\n    store = RestStore(lambda: creds)\n\n    response_1 = _mock_response_with_200_status_code()\n    response_2 = _mock_response_with_200_status_code()\n    response_payload_1 = {\n        \"metrics\": [\n            {\"key\": \"a_metric\", \"value\": 42, \"timestamp\": 123456777, \"step\": 0},\n            {\"key\": \"a_metric\", \"value\": 46, \"timestamp\": 123456797, \"step\": 1},\n        ],\n        \"next_page_token\": \"AcursorForTheRestofTheData\",\n    }\n    response_1.text = json.dumps(response_payload_1)\n    response_payload_2 = {\n        \"metrics\": [\n            {\"key\": \"a_metric\", \"value\": 40, \"timestamp\": 123456877, \"step\": 2},\n            {\"key\": \"a_metric\", \"value\": 56, \"timestamp\": 123456897, \"step\": 3},\n        ],\n        \"next_page_token\": \"\",\n    }\n    response_2.text = json.dumps(response_payload_2)\n    with mock.patch(\n        \"requests.Session.request\", side_effect=[response_1, response_2]\n    ) as mock_request:\n        # Fetch the first page\n        metrics = store.get_metric_history(\n            run_id=\"2\", metric_key=\"a_metric\", max_results=2, page_token=None\n        )\n        mock_request.assert_called_once()\n        assert mock_request.call_args.kwargs[\"params\"] == {\n            \"max_results\": 2,\n            \"metric_key\": \"a_metric\",\n            \"run_id\": \"2\",\n            \"run_uuid\": \"2\",\n        }\n        assert len(metrics) == 2\n        assert metrics[0] == Metric(key=\"a_metric\", value=42, timestamp=123456777, step=0)\n        assert metrics[1] == Metric(key=\"a_metric\", value=46, timestamp=123456797, step=1)\n        assert metrics.token == \"AcursorForTheRestofTheData\"\n        # Fetch the second page\n        mock_request.reset_mock()\n        metrics = store.get_metric_history(\n            run_id=\"2\", metric_key=\"a_metric\", max_results=2, page_token=metrics.token\n        )\n        mock_request.assert_called_once()\n        assert mock_request.call_args.kwargs[\"params\"] == {\n            \"max_results\": 2,\n            \"page_token\": \"AcursorForTheRestofTheData\",\n            \"metric_key\": \"a_metric\",\n            \"run_id\": \"2\",\n            \"run_uuid\": \"2\",\n        }\n        assert len(metrics) == 2\n        assert metrics[0] == Metric(key=\"a_metric\", value=40, timestamp=123456877, step=2)\n        assert metrics[1] == Metric(key=\"a_metric\", value=56, timestamp=123456897, step=3)\n        assert metrics.token is None\n\n\ndef test_get_metric_history_on_non_existent_metric_key():\n    creds = MlflowHostCreds(\"https://hello\")\n    rest_store = RestStore(lambda: creds)\n    empty_metric_response = _mock_response_with_200_status_code()\n    empty_metric_response.text = json.dumps({})\n    with mock.patch(\n        \"requests.Session.request\", side_effect=[empty_metric_response]\n    ) as mock_request:\n        metrics = rest_store.get_metric_history(run_id=\"1\", metric_key=\"test_metric\")\n        mock_request.assert_called_once()\n        assert metrics == []\n", "import subprocess\nimport sys\nfrom unittest import mock\n\nimport pytest\n\nfrom mlflow.utils import request_utils\n\n\ndef test_request_utils_does_not_import_mlflow(tmp_path):\n    file_content = f\"\"\"\nimport importlib.util\nimport os\nimport sys\n\nfile_path = r\"{request_utils.__file__}\"\nmodule_name = \"mlflow.utils.request_utils\"\n\nspec = importlib.util.spec_from_file_location(module_name, file_path)\nmodule = importlib.util.module_from_spec(spec)\nsys.modules[module_name] = module\nspec.loader.exec_module(module)\n\nassert \"mlflow\" not in sys.modules\nassert \"mlflow.utils.request_utils\" in sys.modules\n\"\"\"\n    test_file = tmp_path.joinpath(\"test_request_utils_does_not_import_mlflow.py\")\n    test_file.write_text(file_content)\n\n    subprocess.run([sys.executable, str(test_file)], check=True)\n\n\nclass IncompleteResponse:\n    def __init__(self):\n        self.headers = {\"Content-Length\": \"100\"}\n        raw = mock.MagicMock()\n        raw.tell.return_value = 50\n        self.raw = raw\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        pass\n\n\ndef test_download_chunk_incomplete_read(tmp_path):\n    with mock.patch.object(\n        request_utils, \"cloud_storage_http_request\", return_value=IncompleteResponse()\n    ):\n        download_path = tmp_path / \"chunk\"\n        download_path.touch()\n        with pytest.raises(IOError, match=\"Incomplete read\"):\n            request_utils.download_chunk(\n                range_start=0,\n                range_end=999,\n                headers={},\n                download_path=download_path,\n                http_uri=\"https://example.com\",\n            )\n\n\n@pytest.mark.parametrize(\"env_value\", [\"0\", \"false\", \"False\", \"FALSE\"])\ndef test_redirects_disabled_if_env_var_set(monkeypatch, env_value):\n    monkeypatch.setenv(\"MLFLOW_ALLOW_HTTP_REDIRECTS\", env_value)\n\n    with mock.patch(\"requests.Session.request\") as mock_request:\n        mock_request.return_value.status_code = 302\n        mock_request.return_value.text = \"mock response\"\n\n        response = request_utils.cloud_storage_http_request(\"GET\", \"http://localhost:5000\")\n\n        assert response.text == \"mock response\"\n        mock_request.assert_called_once_with(\n            \"GET\",\n            \"http://localhost:5000\",\n            allow_redirects=False,\n            timeout=None,\n        )\n\n\n@pytest.mark.parametrize(\"env_value\", [\"1\", \"true\", \"True\", \"TRUE\"])\ndef test_redirects_enabled_if_env_var_set(monkeypatch, env_value):\n    monkeypatch.setenv(\"MLFLOW_ALLOW_HTTP_REDIRECTS\", env_value)\n\n    with mock.patch(\"requests.Session.request\") as mock_request:\n        mock_request.return_value.status_code = 302\n        mock_request.return_value.text = \"mock response\"\n\n        response = request_utils.cloud_storage_http_request(\n            \"GET\",\n            \"http://localhost:5000\",\n        )\n\n        assert response.text == \"mock response\"\n        mock_request.assert_called_once_with(\n            \"GET\",\n            \"http://localhost:5000\",\n            allow_redirects=True,\n            timeout=None,\n        )\n\n\n@pytest.mark.parametrize(\"env_value\", [\"0\", \"false\", \"False\", \"FALSE\"])\ndef test_redirect_kwarg_overrides_env_value_false(monkeypatch, env_value):\n    monkeypatch.setenv(\"MLFLOW_ALLOW_HTTP_REDIRECTS\", env_value)\n\n    with mock.patch(\"requests.Session.request\") as mock_request:\n        mock_request.return_value.status_code = 302\n        mock_request.return_value.text = \"mock response\"\n\n        response = request_utils.cloud_storage_http_request(\n            \"GET\", \"http://localhost:5000\", allow_redirects=True\n        )\n\n        assert response.text == \"mock response\"\n        mock_request.assert_called_once_with(\n            \"GET\",\n            \"http://localhost:5000\",\n            allow_redirects=True,\n            timeout=None,\n        )\n\n\n@pytest.mark.parametrize(\"env_value\", [\"1\", \"true\", \"True\", \"TRUE\"])\ndef test_redirect_kwarg_overrides_env_value_true(monkeypatch, env_value):\n    monkeypatch.setenv(\"MLFLOW_ALLOW_HTTP_REDIRECTS\", env_value)\n\n    with mock.patch(\"requests.Session.request\") as mock_request:\n        mock_request.return_value.status_code = 302\n        mock_request.return_value.text = \"mock response\"\n\n        response = request_utils.cloud_storage_http_request(\n            \"GET\", \"http://localhost:5000\", allow_redirects=False\n        )\n\n        assert response.text == \"mock response\"\n        mock_request.assert_called_once_with(\n            \"GET\",\n            \"http://localhost:5000\",\n            allow_redirects=False,\n            timeout=None,\n        )\n\n\ndef test_redirects_enabled_by_default():\n    with mock.patch(\"requests.Session.request\") as mock_request:\n        mock_request.return_value.status_code = 302\n        mock_request.return_value.text = \"mock response\"\n\n        response = request_utils.cloud_storage_http_request(\n            \"GET\",\n            \"http://localhost:5000\",\n        )\n\n        assert response.text == \"mock response\"\n        mock_request.assert_called_once_with(\n            \"GET\",\n            \"http://localhost:5000\",\n            allow_redirects=True,\n            timeout=None,\n        )\n", "import re\nfrom unittest import mock\n\nimport numpy\nimport pytest\nimport requests\n\nfrom mlflow.environment_variables import MLFLOW_HTTP_REQUEST_TIMEOUT\nfrom mlflow.exceptions import InvalidUrlException, MlflowException, RestException\nfrom mlflow.protos.databricks_pb2 import ENDPOINT_NOT_FOUND, ErrorCode\nfrom mlflow.protos.service_pb2 import GetRun\nfrom mlflow.pyfunc.scoring_server import NumpyEncoder\nfrom mlflow.tracking.request_header.default_request_header_provider import (\n    _USER_AGENT,\n    DefaultRequestHeaderProvider,\n)\nfrom mlflow.utils.rest_utils import (\n    MlflowHostCreds,\n    _can_parse_as_json_object,\n    augmented_raise_for_status,\n    call_endpoint,\n    call_endpoints,\n    http_request,\n    http_request_safe,\n)\n\nfrom tests import helper_functions\n\n\ndef test_well_formed_json_error_response():\n    with mock.patch(\n        \"requests.Session.request\", return_value=mock.MagicMock(status_code=400, text=\"{}\")\n    ):\n        host_only = MlflowHostCreds(\"http://my-host\")\n        response_proto = GetRun.Response()\n        with pytest.raises(RestException, match=\"INTERNAL_ERROR\"):\n            call_endpoint(host_only, \"/my/endpoint\", \"GET\", \"\", response_proto)\n\n\ndef test_non_json_ok_response():\n    with mock.patch(\n        \"requests.Session.request\",\n        return_value=mock.MagicMock(status_code=200, text=\"<html></html>\"),\n    ):\n        host_only = MlflowHostCreds(\"http://my-host\")\n        response_proto = GetRun.Response()\n        with pytest.raises(\n            MlflowException,\n            match=\"API request to endpoint was successful but the response body was not \"\n            \"in a valid JSON format\",\n        ):\n            call_endpoint(host_only, \"/api/2.0/fetch-model\", \"GET\", \"\", response_proto)\n\n\n@pytest.mark.parametrize(\n    \"response_mock\",\n    [\n        helper_functions.create_mock_response(400, \"Error message but not a JSON string\"),\n        helper_functions.create_mock_response(400, \"\"),\n        helper_functions.create_mock_response(400, None),\n    ],\n)\ndef test_malformed_json_error_response(response_mock):\n    with mock.patch(\"requests.Session.request\", return_value=response_mock):\n        host_only = MlflowHostCreds(\"http://my-host\")\n\n        response_proto = GetRun.Response()\n        with pytest.raises(\n            MlflowException, match=\"API request to endpoint /my/endpoint failed with error code 400\"\n        ):\n            call_endpoint(host_only, \"/my/endpoint\", \"GET\", \"\", response_proto)\n\n\ndef test_call_endpoints():\n    with mock.patch(\"mlflow.utils.rest_utils.call_endpoint\") as mock_call_endpoint:\n        response_proto = GetRun.Response()\n        mock_call_endpoint.side_effect = [\n            RestException({\"error_code\": ErrorCode.Name(ENDPOINT_NOT_FOUND)}),\n            None,\n        ]\n        host_only = MlflowHostCreds(\"http://my-host\")\n        endpoints = [(\"/my/endpoint\", \"POST\"), (\"/my/endpoint\", \"GET\")]\n        resp = call_endpoints(host_only, endpoints, \"\", response_proto)\n        mock_call_endpoint.assert_has_calls(\n            [\n                mock.call(host_only, endpoint, method, \"\", response_proto, None)\n                for endpoint, method in endpoints\n            ]\n        )\n        assert resp is None\n\n\ndef test_call_endpoints_raises_exceptions():\n    with mock.patch(\"mlflow.utils.rest_utils.call_endpoint\") as mock_call_endpoint:\n        response_proto = GetRun.Response()\n        mock_call_endpoint.side_effect = [\n            RestException({\"error_code\": ErrorCode.Name(ENDPOINT_NOT_FOUND)}),\n            RestException({\"error_code\": ErrorCode.Name(ENDPOINT_NOT_FOUND)}),\n        ]\n        host_only = MlflowHostCreds(\"http://my-host\")\n        endpoints = [(\"/my/endpoint\", \"POST\"), (\"/my/endpoint\", \"GET\")]\n        with pytest.raises(RestException, match=\"ENDPOINT_NOT_FOUND\"):\n            call_endpoints(host_only, endpoints, \"\", response_proto)\n        mock_call_endpoint.side_effect = [RestException({}), None]\n        with pytest.raises(RestException, match=\"INTERNAL_ERROR\"):\n            call_endpoints(host_only, endpoints, \"\", response_proto)\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_hostonly(request):\n    host_only = MlflowHostCreds(\"http://my-host\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=True,\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_cleans_hostname(request):\n    # Add a trailing slash, should be removed.\n    host_only = MlflowHostCreds(\"http://my-host/\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=True,\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_with_basic_auth(request):\n    host_only = MlflowHostCreds(\"http://my-host\", username=\"user\", password=\"pass\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    headers = DefaultRequestHeaderProvider().request_headers()\n    headers[\"Authorization\"] = \"Basic dXNlcjpwYXNz\"\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=True,\n        headers=headers,\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_with_aws_sigv4(request, monkeypatch):\n    \"\"\"This test requires the \"requests_auth_aws_sigv4\" package to be installed\"\"\"\n\n    from requests_auth_aws_sigv4 import AWSSigV4\n\n    monkeypatch.setenvs(\n        {\n            \"AWS_ACCESS_KEY_ID\": \"access-key\",\n            \"AWS_SECRET_ACCESS_KEY\": \"secret-key\",\n            \"AWS_DEFAULT_REGION\": \"eu-west-1\",\n        }\n    )\n    aws_sigv4 = MlflowHostCreds(\"http://my-host\", aws_sigv4=True)\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(aws_sigv4, \"/my/endpoint\", \"GET\")\n\n    class AuthMatcher:\n        def __eq__(self, other):\n            return isinstance(other, AWSSigV4)\n\n    request.assert_called_once_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=mock.ANY,\n        headers=mock.ANY,\n        timeout=mock.ANY,\n        auth=AuthMatcher(),\n    )\n\n\n@mock.patch(\"requests.Session.request\")\n@mock.patch(\"mlflow.tracking.request_auth.registry.fetch_auth\")\ndef test_http_request_with_auth(fetch_auth, request):\n    mock_fetch_auth = {\"test_name\": \"test_auth_value\"}\n    fetch_auth.return_value = mock_fetch_auth\n    auth = \"test_auth_name\"\n    host_only = MlflowHostCreds(\"http://my-host\", auth=auth)\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n\n    fetch_auth.assert_called_with(auth)\n\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=mock.ANY,\n        headers=mock.ANY,\n        timeout=mock.ANY,\n        auth=mock_fetch_auth,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_with_token(request):\n    host_only = MlflowHostCreds(\"http://my-host\", token=\"my-token\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    headers = DefaultRequestHeaderProvider().request_headers()\n    headers[\"Authorization\"] = \"Bearer my-token\"\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=True,\n        headers=headers,\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_with_insecure(request):\n    host_only = MlflowHostCreds(\"http://my-host\", ignore_tls_verification=True)\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=False,\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_client_cert_path(request):\n    host_only = MlflowHostCreds(\"http://my-host\", client_cert_path=\"/some/path\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=True,\n        cert=\"/some/path\",\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_server_cert_path(request):\n    host_only = MlflowHostCreds(\"http://my-host\", server_cert_path=\"/some/path\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    http_request(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=\"/some/path\",\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_with_content_type_header(request):\n    host_only = MlflowHostCreds(\"http://my-host\", token=\"my-token\")\n    response = mock.MagicMock()\n    response.status_code = 200\n    request.return_value = response\n    extra_headers = {\"Content-Type\": \"text/plain\"}\n    http_request(host_only, \"/my/endpoint\", \"GET\", extra_headers=extra_headers)\n    headers = DefaultRequestHeaderProvider().request_headers()\n    headers[\"Authorization\"] = \"Bearer my-token\"\n    headers[\"Content-Type\"] = \"text/plain\"\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=True,\n        headers=headers,\n        timeout=120,\n    )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_request_headers(request):\n    \"\"\"This test requires the package in tests/resources/mlflow-test-plugin to be installed\"\"\"\n\n    from mlflow_test_plugin.request_header_provider import PluginRequestHeaderProvider\n\n    # The test plugin's request header provider always returns False from in_context to avoid\n    # polluting request headers in developers' environments. The following mock overrides this to\n    # perform the integration test.\n    with mock.patch.object(PluginRequestHeaderProvider, \"in_context\", return_value=True):\n        host_only = MlflowHostCreds(\"http://my-host\", server_cert_path=\"/some/path\")\n\n        response = mock.MagicMock()\n        response.status_code = 200\n        request.return_value = response\n        http_request(host_only, \"/my/endpoint\", \"GET\")\n        request.assert_called_with(\n            \"GET\",\n            \"http://my-host/my/endpoint\",\n            allow_redirects=True,\n            verify=\"/some/path\",\n            headers={**DefaultRequestHeaderProvider().request_headers(), \"test\": \"header\"},\n            timeout=120,\n        )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_request_headers_user_agent(request):\n    \"\"\"This test requires the package in tests/resources/mlflow-test-plugin to be installed\"\"\"\n\n    from mlflow_test_plugin.request_header_provider import PluginRequestHeaderProvider\n\n    # The test plugin's request header provider always returns False from in_context to avoid\n    # polluting request headers in developers' environments. The following mock overrides this to\n    # perform the integration test.\n    with mock.patch.object(\n        PluginRequestHeaderProvider, \"in_context\", return_value=True\n    ), mock.patch.object(\n        PluginRequestHeaderProvider,\n        \"request_headers\",\n        return_value={_USER_AGENT: \"test_user_agent\"},\n    ):\n        host_only = MlflowHostCreds(\"http://my-host\", server_cert_path=\"/some/path\")\n        expected_headers = {\n            _USER_AGENT: \"{} {}\".format(\n                DefaultRequestHeaderProvider().request_headers()[_USER_AGENT], \"test_user_agent\"\n            )\n        }\n\n        response = mock.MagicMock()\n        response.status_code = 200\n        request.return_value = response\n        http_request(host_only, \"/my/endpoint\", \"GET\")\n        request.assert_called_with(\n            \"GET\",\n            \"http://my-host/my/endpoint\",\n            allow_redirects=True,\n            verify=\"/some/path\",\n            headers=expected_headers,\n            timeout=120,\n        )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_request_headers_user_agent_and_extra_header(request):\n    \"\"\"This test requires the package in tests/resources/mlflow-test-plugin to be installed\"\"\"\n\n    from mlflow_test_plugin.request_header_provider import PluginRequestHeaderProvider\n\n    # The test plugin's request header provider always returns False from in_context to avoid\n    # polluting request headers in developers' environments. The following mock overrides this to\n    # perform the integration test.\n    with mock.patch.object(\n        PluginRequestHeaderProvider, \"in_context\", return_value=True\n    ), mock.patch.object(\n        PluginRequestHeaderProvider,\n        \"request_headers\",\n        return_value={_USER_AGENT: \"test_user_agent\", \"header\": \"value\"},\n    ):\n        host_only = MlflowHostCreds(\"http://my-host\", server_cert_path=\"/some/path\")\n        expected_headers = {\n            _USER_AGENT: \"{} {}\".format(\n                DefaultRequestHeaderProvider().request_headers()[_USER_AGENT], \"test_user_agent\"\n            ),\n            \"header\": \"value\",\n        }\n\n        response = mock.MagicMock()\n        response.status_code = 200\n        request.return_value = response\n        http_request(host_only, \"/my/endpoint\", \"GET\")\n        request.assert_called_with(\n            \"GET\",\n            \"http://my-host/my/endpoint\",\n            allow_redirects=True,\n            verify=\"/some/path\",\n            headers=expected_headers,\n            timeout=120,\n        )\n\n\ndef test_http_request_with_invalid_url_raise_invalid_url_exception():\n    \"\"\"InvalidURL exception can be caught by a custom InvalidUrlException\"\"\"\n    host_only = MlflowHostCreds(\"http://my-host\")\n\n    with pytest.raises(InvalidUrlException, match=\"Invalid url: http://my-host/invalid_url\"):\n        with mock.patch(\"requests.Session.request\", side_effect=requests.exceptions.InvalidURL):\n            http_request(host_only, \"/invalid_url\", \"GET\")\n\n\ndef test_http_request_with_invalid_url_raise_mlflow_exception():\n    \"\"\"The InvalidUrlException can be caught by the MlflowException\"\"\"\n    host_only = MlflowHostCreds(\"http://my-host\")\n\n    with pytest.raises(MlflowException, match=\"Invalid url: http://my-host/invalid_url\"):\n        with mock.patch(\"requests.Session.request\", side_effect=requests.exceptions.InvalidURL):\n            http_request(host_only, \"/invalid_url\", \"GET\")\n\n\ndef test_ignore_tls_verification_not_server_cert_path():\n    with pytest.raises(\n        MlflowException,\n        match=\"When 'ignore_tls_verification' is true then 'server_cert_path' must not be set\",\n    ):\n        MlflowHostCreds(\n            \"http://my-host\",\n            ignore_tls_verification=True,\n            server_cert_path=\"/some/path\",\n        )\n\n\n@mock.patch(\"requests.Session.request\")\ndef test_http_request_wrapper(request):\n    host_only = MlflowHostCreds(\"http://my-host\", ignore_tls_verification=True)\n    response = mock.MagicMock()\n    response.status_code = 200\n    response.text = \"{}\"\n    request.return_value = response\n    http_request_safe(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=False,\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n    response.text = \"non json\"\n    request.return_value = response\n    http_request_safe(host_only, \"/my/endpoint\", \"GET\")\n    request.assert_called_with(\n        \"GET\",\n        \"http://my-host/my/endpoint\",\n        allow_redirects=True,\n        verify=False,\n        headers=DefaultRequestHeaderProvider().request_headers(),\n        timeout=120,\n    )\n    response.status_code = 400\n    response.text = \"\"\n    request.return_value = response\n    with pytest.raises(MlflowException, match=\"Response body\"):\n        http_request_safe(host_only, \"/my/endpoint\", \"GET\")\n    response.text = (\n        '{\"error_code\": \"RESOURCE_DOES_NOT_EXIST\", \"message\": \"Node type not supported\"}'\n    )\n    request.return_value = response\n    with pytest.raises(RestException, match=\"RESOURCE_DOES_NOT_EXIST: Node type not supported\"):\n        http_request_safe(host_only, \"/my/endpoint\", \"GET\")\n\n\ndef test_numpy_encoder():\n    test_number = numpy.int64(42)\n    ne = NumpyEncoder()\n    defaulted_val = ne.default(test_number)\n    assert defaulted_val == 42\n\n\ndef test_numpy_encoder_fail():\n    if not hasattr(numpy, \"float128\"):\n        pytest.skip(\"numpy on exit this platform has no float128\")\n    test_number = numpy.float128\n    ne = NumpyEncoder()\n    with pytest.raises(TypeError, match=\"not JSON serializable\"):\n        ne.default(test_number)\n\n\ndef test_can_parse_as_json_object():\n    assert _can_parse_as_json_object(\"{}\")\n    assert _can_parse_as_json_object('{\"a\": \"b\"}')\n    assert _can_parse_as_json_object('{\"a\": {\"b\": \"c\"}}')\n    assert not _can_parse_as_json_object(\"[0, 1, 2]\")\n    assert not _can_parse_as_json_object('\"abc\"')\n    assert not _can_parse_as_json_object(\"123\")\n\n\ndef test_http_request_customize_config(monkeypatch):\n    with mock.patch(\n        \"mlflow.utils.rest_utils._get_http_response_with_retries\"\n    ) as mock_get_http_response_with_retries:\n        host_only = MlflowHostCreds(\"http://my-host\")\n        monkeypatch.delenv(\"MLFLOW_HTTP_REQUEST_MAX_RETRIES\", raising=False)\n        monkeypatch.delenv(\"MLFLOW_HTTP_REQUEST_BACKOFF_FACTOR\", raising=False)\n        monkeypatch.delenv(\"MLFLOW_HTTP_REQUEST_TIMEOUT\", raising=False)\n        http_request(host_only, \"/my/endpoint\", \"GET\")\n        mock_get_http_response_with_retries.assert_called_with(\n            mock.ANY,\n            mock.ANY,\n            5,\n            2,\n            1.0,\n            mock.ANY,\n            True,\n            headers=mock.ANY,\n            verify=mock.ANY,\n            timeout=120,\n        )\n        mock_get_http_response_with_retries.reset_mock()\n        monkeypatch.setenv(\"MLFLOW_HTTP_REQUEST_MAX_RETRIES\", \"8\")\n        monkeypatch.setenv(\"MLFLOW_HTTP_REQUEST_BACKOFF_FACTOR\", \"3\")\n        monkeypatch.setenv(\"MLFLOW_HTTP_REQUEST_BACKOFF_JITTER\", \"1.0\")\n        monkeypatch.setenv(\"MLFLOW_HTTP_REQUEST_TIMEOUT\", \"300\")\n        http_request(host_only, \"/my/endpoint\", \"GET\")\n        mock_get_http_response_with_retries.assert_called_with(\n            mock.ANY,\n            mock.ANY,\n            8,\n            3,\n            1.0,\n            mock.ANY,\n            True,\n            headers=mock.ANY,\n            verify=mock.ANY,\n            timeout=300,\n        )\n\n\ndef test_http_request_explains_how_to_increase_timeout_in_error_message():\n    with mock.patch(\"requests.Session.request\", side_effect=requests.exceptions.Timeout):\n        with pytest.raises(\n            MlflowException,\n            match=(\n                r\"To increase the timeout, set the environment variable \"\n                + re.escape(str(MLFLOW_HTTP_REQUEST_TIMEOUT))\n            ),\n        ):\n            http_request(MlflowHostCreds(\"http://my-host\"), \"/my/endpoint\", \"GET\")\n\n\ndef test_augmented_raise_for_status():\n    response = requests.Response()\n    response.status_code = 403\n    response._content = b\"Token expired\"\n\n    with mock.patch(\"requests.Session.request\", return_value=response) as mock_request:\n        response = requests.get(\"https://github.com/mlflow/mlflow.git\")\n        mock_request.assert_called_once()\n\n    with pytest.raises(requests.HTTPError, match=\"Token expired\") as e:\n        augmented_raise_for_status(response)\n\n    assert e.value.response == response\n    assert e.value.request == response.request\n    assert response.text in str(e.value)\n\n\ndef test_provide_redirect_kwarg():\n    with mock.patch(\"requests.Session.request\") as mock_request:\n        mock_request.return_value.status_code = 302\n        mock_request.return_value.text = \"mock response\"\n\n        response = http_request(\n            MlflowHostCreds(\"http://my-host\"),\n            \"/my/endpoint\",\n            \"GET\",\n            allow_redirects=False,\n        )\n\n        assert response.text == \"mock response\"\n        mock_request.assert_called_with(\n            \"GET\",\n            \"http://my-host/my/endpoint\",\n            allow_redirects=False,\n            headers=mock.ANY,\n            verify=mock.ANY,\n            timeout=120,\n        )\n"], "filenames": ["mlflow/environment_variables.py", "mlflow/utils/request_utils.py", "tests/projects/test_databricks.py", "tests/store/artifact/test_databricks_artifact_repo.py", "tests/store/tracking/test_rest_store.py", "tests/utils/test_request_utils.py", "tests/utils/test_rest_utils.py"], "buggy_code_start_loc": [495, 167, 451, 260, 72, 60, 118], "buggy_code_end_loc": [495, 191, 451, 1461, 72, 60, 562], "fixing_code_start_loc": [496, 168, 452, 261, 73, 61, 119], "fixing_code_end_loc": [502, 198, 453, 1484, 74, 163, 601], "type": "CWE-918", "message": "A malicious user could use this issue to access internal HTTP(s) servers and in the worst case (ie: aws instance) it could be abuse to get a remote code execution on the victim machine.", "other": {"cve": {"id": "CVE-2023-6974", "sourceIdentifier": "security@huntr.dev", "published": "2023-12-20T06:15:45.160", "lastModified": "2023-12-29T14:13:47.377", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A malicious user could use this issue to access internal HTTP(s) servers and in the worst case (ie: aws instance) it could be abuse to get a remote code execution on the victim machine."}, {"lang": "es", "value": "Un usuario malintencionado podr\u00eda utilizar este problema para acceder a servidores HTTP internos y, en el peor de los casos (es decir, instancia de AWS), podr\u00eda ser un abuso obtener una ejecuci\u00f3n remota de c\u00f3digo en la m\u00e1quina v\u00edctima."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV30": [{"source": "security@huntr.dev", "type": "Secondary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:C/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 8.6, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 4.0}]}, "weaknesses": [{"source": "security@huntr.dev", "type": "Primary", "description": [{"lang": "en", "value": "CWE-918"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:lfprojects:mlflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.9.2", "matchCriteriaId": "6B5585E2-CC70-4BED-AA89-B791F081ACFC"}]}]}], "references": [{"url": "https://github.com/mlflow/mlflow/commit/8174250f83352a04c2d42079f414759060458555", "source": "security@huntr.dev", "tags": ["Patch"]}, {"url": "https://huntr.com/bounties/438b0524-da0e-4d08-976a-6f270c688393", "source": "security@huntr.dev", "tags": ["Exploit", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/mlflow/mlflow/commit/8174250f83352a04c2d42079f414759060458555"}}