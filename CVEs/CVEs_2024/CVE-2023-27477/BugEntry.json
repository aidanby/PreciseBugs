{"buggy_code": ["//! ISLE integration glue code for x64 lowering.\n\n// Pull in the ISLE generated code.\npub(crate) mod generated_code;\nuse crate::{\n    ir::types,\n    ir::AtomicRmwOp,\n    machinst::{InputSourceInst, Reg, Writable},\n};\nuse crate::{isle_common_prelude_methods, isle_lower_prelude_methods};\nuse generated_code::{Context, MInst, RegisterClass};\n\n// Types that the generated ISLE code uses via `use super::*`.\nuse super::{is_int_or_ref_ty, is_mergeable_load, lower_to_amode};\nuse crate::ir::LibCall;\nuse crate::isa::x64::lower::emit_vm_call;\nuse crate::isa::x64::X64Backend;\nuse crate::{\n    ir::{\n        condcodes::{CondCode, FloatCC, IntCC},\n        immediates::*,\n        types::*,\n        BlockCall, Inst, InstructionData, MemFlags, Opcode, TrapCode, Value, ValueList,\n    },\n    isa::{\n        unwind::UnwindInst,\n        x64::{\n            abi::X64Caller,\n            inst::{args::*, regs, CallInfo},\n        },\n    },\n    machinst::{\n        isle::*, valueregs, ArgPair, InsnInput, InstOutput, Lower, MachAtomicRmwOp, MachInst,\n        VCodeConstant, VCodeConstantData,\n    },\n};\nuse alloc::vec::Vec;\nuse regalloc2::PReg;\nuse smallvec::SmallVec;\nuse std::boxed::Box;\nuse std::convert::TryFrom;\n\ntype BoxCallInfo = Box<CallInfo>;\ntype BoxVecMachLabel = Box<SmallVec<[MachLabel; 4]>>;\ntype MachLabelSlice = [MachLabel];\ntype VecArgPair = Vec<ArgPair>;\n\npub struct SinkableLoad {\n    inst: Inst,\n    addr_input: InsnInput,\n    offset: i32,\n}\n\n/// The main entry point for lowering with ISLE.\npub(crate) fn lower(\n    lower_ctx: &mut Lower<MInst>,\n    backend: &X64Backend,\n    inst: Inst,\n) -> Option<InstOutput> {\n    // TODO: reuse the ISLE context across lowerings so we can reuse its\n    // internal heap allocations.\n    let mut isle_ctx = IsleContext { lower_ctx, backend };\n    generated_code::constructor_lower(&mut isle_ctx, inst)\n}\n\npub(crate) fn lower_branch(\n    lower_ctx: &mut Lower<MInst>,\n    backend: &X64Backend,\n    branch: Inst,\n    targets: &[MachLabel],\n) -> Option<()> {\n    // TODO: reuse the ISLE context across lowerings so we can reuse its\n    // internal heap allocations.\n    let mut isle_ctx = IsleContext { lower_ctx, backend };\n    generated_code::constructor_lower_branch(&mut isle_ctx, branch, &targets.to_vec())\n}\n\nimpl Context for IsleContext<'_, '_, MInst, X64Backend> {\n    isle_lower_prelude_methods!();\n    isle_prelude_caller_methods!(X64ABIMachineSpec, X64Caller);\n\n    #[inline]\n    fn operand_size_of_type_32_64(&mut self, ty: Type) -> OperandSize {\n        if ty.bits() == 64 {\n            OperandSize::Size64\n        } else {\n            OperandSize::Size32\n        }\n    }\n\n    #[inline]\n    fn raw_operand_size_of_type(&mut self, ty: Type) -> OperandSize {\n        OperandSize::from_ty(ty)\n    }\n\n    fn put_in_reg_mem_imm(&mut self, val: Value) -> RegMemImm {\n        let inputs = self.lower_ctx.get_value_as_source_or_const(val);\n\n        if let Some(c) = inputs.constant {\n            if let Some(imm) = to_simm32(c as i64) {\n                return imm.to_reg_mem_imm();\n            }\n        }\n\n        self.put_in_reg_mem(val).into()\n    }\n\n    fn put_in_xmm_mem_imm(&mut self, val: Value) -> XmmMemImm {\n        let inputs = self.lower_ctx.get_value_as_source_or_const(val);\n\n        if let Some(c) = inputs.constant {\n            if let Some(imm) = to_simm32(c as i64) {\n                return XmmMemImm::new(imm.to_reg_mem_imm()).unwrap();\n            }\n        }\n\n        let res = match self.put_in_xmm_mem(val).to_reg_mem() {\n            RegMem::Reg { reg } => RegMemImm::Reg { reg },\n            RegMem::Mem { addr } => RegMemImm::Mem { addr },\n        };\n\n        XmmMemImm::new(res).unwrap()\n    }\n\n    fn put_in_xmm_mem(&mut self, val: Value) -> XmmMem {\n        let inputs = self.lower_ctx.get_value_as_source_or_const(val);\n\n        if let Some(c) = inputs.constant {\n            // A load from the constant pool is better than a rematerialization into a register,\n            // because it reduces register pressure.\n            //\n            // NOTE: this is where behavior differs from `put_in_reg_mem`, as we always force\n            // constants to be 16 bytes when a constant will be used in place of an xmm register.\n            let vcode_constant = self.emit_u128_le_const(c as u128);\n            return XmmMem::new(RegMem::mem(SyntheticAmode::ConstantOffset(vcode_constant)))\n                .unwrap();\n        }\n\n        XmmMem::new(self.put_in_reg_mem(val)).unwrap()\n    }\n\n    fn put_in_reg_mem(&mut self, val: Value) -> RegMem {\n        let inputs = self.lower_ctx.get_value_as_source_or_const(val);\n\n        if let Some(c) = inputs.constant {\n            // A load from the constant pool is better than a\n            // rematerialization into a register, because it reduces\n            // register pressure.\n            let vcode_constant = self.emit_u64_le_const(c);\n            return RegMem::mem(SyntheticAmode::ConstantOffset(vcode_constant));\n        }\n\n        if let Some(load) = self.sinkable_load(val) {\n            return self.sink_load(&load);\n        }\n\n        RegMem::reg(self.put_in_reg(val))\n    }\n\n    #[inline]\n    fn encode_fcmp_imm(&mut self, imm: &FcmpImm) -> u8 {\n        imm.encode()\n    }\n\n    #[inline]\n    fn encode_round_imm(&mut self, imm: &RoundImm) -> u8 {\n        imm.encode()\n    }\n\n    #[inline]\n    fn has_avx(&mut self) -> bool {\n        self.backend.x64_flags.has_avx()\n    }\n\n    #[inline]\n    fn avx512vl_enabled(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_avx512vl_simd()\n    }\n\n    #[inline]\n    fn avx512dq_enabled(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_avx512dq_simd()\n    }\n\n    #[inline]\n    fn avx512f_enabled(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_avx512f_simd()\n    }\n\n    #[inline]\n    fn avx512bitalg_enabled(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_avx512bitalg_simd()\n    }\n\n    #[inline]\n    fn avx512vbmi_enabled(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_avx512vbmi_simd()\n    }\n\n    #[inline]\n    fn use_lzcnt(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_lzcnt()\n    }\n\n    #[inline]\n    fn use_bmi1(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_bmi1()\n    }\n\n    #[inline]\n    fn use_popcnt(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_popcnt()\n    }\n\n    #[inline]\n    fn use_fma(&mut self) -> bool {\n        self.backend.x64_flags.use_fma()\n    }\n\n    #[inline]\n    fn use_sse41(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_sse41()\n    }\n\n    #[inline]\n    fn imm8_from_value(&mut self, val: Value) -> Option<Imm8Reg> {\n        let inst = self.lower_ctx.dfg().value_def(val).inst()?;\n        let constant = self.lower_ctx.get_constant(inst)?;\n        let imm = u8::try_from(constant).ok()?;\n        Some(Imm8Reg::Imm8 { imm })\n    }\n\n    #[inline]\n    fn const_to_type_masked_imm8(&mut self, c: u64, ty: Type) -> Imm8Gpr {\n        let mask = self.shift_mask(ty) as u64;\n        Imm8Gpr::new(Imm8Reg::Imm8 {\n            imm: (c & mask) as u8,\n        })\n        .unwrap()\n    }\n\n    #[inline]\n    fn shift_mask(&mut self, ty: Type) -> u32 {\n        debug_assert!(ty.lane_bits().is_power_of_two());\n\n        ty.lane_bits() - 1\n    }\n\n    fn shift_amount_masked(&mut self, ty: Type, val: Imm64) -> u32 {\n        (val.bits() as u32) & self.shift_mask(ty)\n    }\n\n    #[inline]\n    fn simm32_from_value(&mut self, val: Value) -> Option<GprMemImm> {\n        let inst = self.lower_ctx.dfg().value_def(val).inst()?;\n        let constant: u64 = self.lower_ctx.get_constant(inst)?;\n        let constant = constant as i64;\n        to_simm32(constant)\n    }\n\n    #[inline]\n    fn simm32_from_imm64(&mut self, imm: Imm64) -> Option<GprMemImm> {\n        to_simm32(imm.bits())\n    }\n\n    fn sinkable_load(&mut self, val: Value) -> Option<SinkableLoad> {\n        let input = self.lower_ctx.get_value_as_source_or_const(val);\n        if let InputSourceInst::UniqueUse(inst, 0) = input.inst {\n            if let Some((addr_input, offset)) = is_mergeable_load(self.lower_ctx, inst) {\n                return Some(SinkableLoad {\n                    inst,\n                    addr_input,\n                    offset,\n                });\n            }\n        }\n        None\n    }\n\n    fn sink_load(&mut self, load: &SinkableLoad) -> RegMem {\n        self.lower_ctx.sink_inst(load.inst);\n        let addr = lower_to_amode(self.lower_ctx, load.addr_input, load.offset);\n        RegMem::Mem {\n            addr: SyntheticAmode::Real(addr),\n        }\n    }\n\n    #[inline]\n    fn ext_mode(&mut self, from_bits: u16, to_bits: u16) -> ExtMode {\n        ExtMode::new(from_bits, to_bits).unwrap()\n    }\n\n    fn emit(&mut self, inst: &MInst) -> Unit {\n        self.lower_ctx.emit(inst.clone());\n    }\n\n    #[inline]\n    fn nonzero_u64_fits_in_u32(&mut self, x: u64) -> Option<u64> {\n        if x != 0 && x < u64::from(u32::MAX) {\n            Some(x)\n        } else {\n            None\n        }\n    }\n\n    #[inline]\n    fn sse_insertps_lane_imm(&mut self, lane: u8) -> u8 {\n        // Insert 32-bits from replacement (at index 00, bits 7:8) to vector (lane\n        // shifted into bits 5:6).\n        0b00_00_00_00 | lane << 4\n    }\n\n    #[inline]\n    fn synthetic_amode_to_reg_mem(&mut self, addr: &SyntheticAmode) -> RegMem {\n        RegMem::mem(addr.clone())\n    }\n\n    #[inline]\n    fn amode_imm_reg_reg_shift(&mut self, simm32: u32, base: Gpr, index: Gpr, shift: u8) -> Amode {\n        Amode::imm_reg_reg_shift(simm32, base, index, shift)\n    }\n\n    #[inline]\n    fn amode_imm_reg(&mut self, simm32: u32, base: Gpr) -> Amode {\n        Amode::imm_reg(simm32, base.to_reg())\n    }\n\n    #[inline]\n    fn amode_with_flags(&mut self, amode: &Amode, flags: MemFlags) -> Amode {\n        amode.with_flags(flags)\n    }\n\n    #[inline]\n    fn amode_to_synthetic_amode(&mut self, amode: &Amode) -> SyntheticAmode {\n        amode.clone().into()\n    }\n\n    #[inline]\n    fn const_to_synthetic_amode(&mut self, c: VCodeConstant) -> SyntheticAmode {\n        SyntheticAmode::ConstantOffset(c)\n    }\n\n    #[inline]\n    fn writable_gpr_to_reg(&mut self, r: WritableGpr) -> WritableReg {\n        r.to_writable_reg()\n    }\n\n    #[inline]\n    fn writable_xmm_to_reg(&mut self, r: WritableXmm) -> WritableReg {\n        r.to_writable_reg()\n    }\n\n    fn ishl_i8x16_mask_for_const(&mut self, amt: u32) -> SyntheticAmode {\n        // When the shift amount is known, we can statically (i.e. at compile\n        // time) determine the mask to use and only emit that.\n        debug_assert!(amt < 8);\n        let mask_offset = amt as usize * 16;\n        let mask_constant = self.lower_ctx.use_constant(VCodeConstantData::WellKnown(\n            &I8X16_ISHL_MASKS[mask_offset..mask_offset + 16],\n        ));\n        SyntheticAmode::ConstantOffset(mask_constant)\n    }\n\n    fn ishl_i8x16_mask_table(&mut self) -> SyntheticAmode {\n        let mask_table = self\n            .lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&I8X16_ISHL_MASKS));\n        SyntheticAmode::ConstantOffset(mask_table)\n    }\n\n    fn ushr_i8x16_mask_for_const(&mut self, amt: u32) -> SyntheticAmode {\n        // When the shift amount is known, we can statically (i.e. at compile\n        // time) determine the mask to use and only emit that.\n        debug_assert!(amt < 8);\n        let mask_offset = amt as usize * 16;\n        let mask_constant = self.lower_ctx.use_constant(VCodeConstantData::WellKnown(\n            &I8X16_USHR_MASKS[mask_offset..mask_offset + 16],\n        ));\n        SyntheticAmode::ConstantOffset(mask_constant)\n    }\n\n    fn ushr_i8x16_mask_table(&mut self) -> SyntheticAmode {\n        let mask_table = self\n            .lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&I8X16_USHR_MASKS));\n        SyntheticAmode::ConstantOffset(mask_table)\n    }\n\n    fn popcount_4bit_table(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&POPCOUNT_4BIT_TABLE))\n    }\n\n    fn popcount_low_mask(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&POPCOUNT_LOW_MASK))\n    }\n\n    #[inline]\n    fn writable_reg_to_xmm(&mut self, r: WritableReg) -> WritableXmm {\n        Writable::from_reg(Xmm::new(r.to_reg()).unwrap())\n    }\n\n    #[inline]\n    fn writable_xmm_to_xmm(&mut self, r: WritableXmm) -> Xmm {\n        r.to_reg()\n    }\n\n    #[inline]\n    fn writable_gpr_to_gpr(&mut self, r: WritableGpr) -> Gpr {\n        r.to_reg()\n    }\n\n    #[inline]\n    fn gpr_to_reg(&mut self, r: Gpr) -> Reg {\n        r.into()\n    }\n\n    #[inline]\n    fn xmm_to_reg(&mut self, r: Xmm) -> Reg {\n        r.into()\n    }\n\n    #[inline]\n    fn xmm_to_xmm_mem_imm(&mut self, r: Xmm) -> XmmMemImm {\n        r.into()\n    }\n\n    #[inline]\n    fn xmm_mem_to_xmm_mem_imm(&mut self, r: &XmmMem) -> XmmMemImm {\n        XmmMemImm::new(r.clone().to_reg_mem().into()).unwrap()\n    }\n\n    #[inline]\n    fn temp_writable_gpr(&mut self) -> WritableGpr {\n        Writable::from_reg(Gpr::new(self.temp_writable_reg(I64).to_reg()).unwrap())\n    }\n\n    #[inline]\n    fn temp_writable_xmm(&mut self) -> WritableXmm {\n        Writable::from_reg(Xmm::new(self.temp_writable_reg(I8X16).to_reg()).unwrap())\n    }\n\n    #[inline]\n    fn reg_to_reg_mem_imm(&mut self, reg: Reg) -> RegMemImm {\n        RegMemImm::Reg { reg }\n    }\n\n    #[inline]\n    fn reg_mem_to_xmm_mem(&mut self, rm: &RegMem) -> XmmMem {\n        XmmMem::new(rm.clone()).unwrap()\n    }\n\n    #[inline]\n    fn gpr_mem_imm_new(&mut self, rmi: &RegMemImm) -> GprMemImm {\n        GprMemImm::new(rmi.clone()).unwrap()\n    }\n\n    #[inline]\n    fn xmm_mem_imm_new(&mut self, rmi: &RegMemImm) -> XmmMemImm {\n        XmmMemImm::new(rmi.clone()).unwrap()\n    }\n\n    #[inline]\n    fn xmm_to_xmm_mem(&mut self, r: Xmm) -> XmmMem {\n        r.into()\n    }\n\n    #[inline]\n    fn xmm_mem_to_reg_mem(&mut self, xm: &XmmMem) -> RegMem {\n        xm.clone().into()\n    }\n\n    #[inline]\n    fn gpr_mem_to_reg_mem(&mut self, gm: &GprMem) -> RegMem {\n        gm.clone().into()\n    }\n\n    #[inline]\n    fn xmm_new(&mut self, r: Reg) -> Xmm {\n        Xmm::new(r).unwrap()\n    }\n\n    #[inline]\n    fn gpr_new(&mut self, r: Reg) -> Gpr {\n        Gpr::new(r).unwrap()\n    }\n\n    #[inline]\n    fn reg_mem_to_gpr_mem(&mut self, rm: &RegMem) -> GprMem {\n        GprMem::new(rm.clone()).unwrap()\n    }\n\n    #[inline]\n    fn reg_to_gpr_mem(&mut self, r: Reg) -> GprMem {\n        GprMem::new(RegMem::reg(r)).unwrap()\n    }\n\n    #[inline]\n    fn imm8_reg_to_imm8_gpr(&mut self, ir: &Imm8Reg) -> Imm8Gpr {\n        Imm8Gpr::new(ir.clone()).unwrap()\n    }\n\n    #[inline]\n    fn gpr_to_gpr_mem(&mut self, gpr: Gpr) -> GprMem {\n        GprMem::from(gpr)\n    }\n\n    #[inline]\n    fn gpr_to_gpr_mem_imm(&mut self, gpr: Gpr) -> GprMemImm {\n        GprMemImm::from(gpr)\n    }\n\n    #[inline]\n    fn gpr_to_imm8_gpr(&mut self, gpr: Gpr) -> Imm8Gpr {\n        Imm8Gpr::from(gpr)\n    }\n\n    #[inline]\n    fn imm8_to_imm8_gpr(&mut self, imm: u8) -> Imm8Gpr {\n        Imm8Gpr::new(Imm8Reg::Imm8 { imm }).unwrap()\n    }\n\n    #[inline]\n    fn type_register_class(&mut self, ty: Type) -> Option<RegisterClass> {\n        if is_int_or_ref_ty(ty) || ty == I128 {\n            Some(RegisterClass::Gpr {\n                single_register: ty != I128,\n            })\n        } else if ty == F32 || ty == F64 || (ty.is_vector() && ty.bits() == 128) {\n            Some(RegisterClass::Xmm)\n        } else {\n            None\n        }\n    }\n\n    #[inline]\n    fn ty_int_bool_or_ref(&mut self, ty: Type) -> Option<()> {\n        match ty {\n            types::I8 | types::I16 | types::I32 | types::I64 | types::R64 => Some(()),\n            types::R32 => panic!(\"shouldn't have 32-bits refs on x64\"),\n            _ => None,\n        }\n    }\n\n    #[inline]\n    fn intcc_without_eq(&mut self, x: &IntCC) -> IntCC {\n        x.without_equal()\n    }\n\n    #[inline]\n    fn intcc_to_cc(&mut self, intcc: &IntCC) -> CC {\n        CC::from_intcc(*intcc)\n    }\n\n    #[inline]\n    fn cc_invert(&mut self, cc: &CC) -> CC {\n        cc.invert()\n    }\n\n    #[inline]\n    fn cc_nz_or_z(&mut self, cc: &CC) -> Option<CC> {\n        match cc {\n            CC::Z => Some(*cc),\n            CC::NZ => Some(*cc),\n            _ => None,\n        }\n    }\n\n    #[inline]\n    fn sum_extend_fits_in_32_bits(\n        &mut self,\n        extend_from_ty: Type,\n        constant_value: Imm64,\n        offset: Offset32,\n    ) -> Option<u32> {\n        let offset: i64 = offset.into();\n        let constant_value: u64 = constant_value.bits() as u64;\n        // If necessary, zero extend `constant_value` up to 64 bits.\n        let shift = 64 - extend_from_ty.bits();\n        let zero_extended_constant_value = (constant_value << shift) >> shift;\n        // Sum up the two operands.\n        let sum = offset.wrapping_add(zero_extended_constant_value as i64);\n        // Check that the sum will fit in 32-bits.\n        if sum == ((sum << 32) >> 32) {\n            Some(sum as u32)\n        } else {\n            None\n        }\n    }\n\n    #[inline]\n    fn amode_offset(&mut self, addr: &Amode, offset: u32) -> Amode {\n        addr.offset(offset)\n    }\n\n    #[inline]\n    fn zero_offset(&mut self) -> Offset32 {\n        Offset32::new(0)\n    }\n\n    #[inline]\n    fn atomic_rmw_op_to_mach_atomic_rmw_op(&mut self, op: &AtomicRmwOp) -> MachAtomicRmwOp {\n        MachAtomicRmwOp::from(*op)\n    }\n\n    #[inline]\n    fn preg_rbp(&mut self) -> PReg {\n        regs::rbp().to_real_reg().unwrap().into()\n    }\n\n    #[inline]\n    fn preg_rsp(&mut self) -> PReg {\n        regs::rsp().to_real_reg().unwrap().into()\n    }\n\n    #[inline]\n    fn preg_pinned(&mut self) -> PReg {\n        regs::pinned_reg().to_real_reg().unwrap().into()\n    }\n\n    fn libcall_1(&mut self, libcall: &LibCall, a: Reg) -> Reg {\n        let call_conv = self.lower_ctx.abi().call_conv(self.lower_ctx.sigs());\n        let ret_ty = libcall.signature(call_conv).returns[0].value_type;\n        let output_reg = self.lower_ctx.alloc_tmp(ret_ty).only_reg().unwrap();\n\n        emit_vm_call(\n            self.lower_ctx,\n            &self.backend.flags,\n            &self.backend.triple,\n            libcall.clone(),\n            &[a],\n            &[output_reg],\n        )\n        .expect(\"Failed to emit LibCall\");\n\n        output_reg.to_reg()\n    }\n\n    fn libcall_3(&mut self, libcall: &LibCall, a: Reg, b: Reg, c: Reg) -> Reg {\n        let call_conv = self.lower_ctx.abi().call_conv(self.lower_ctx.sigs());\n        let ret_ty = libcall.signature(call_conv).returns[0].value_type;\n        let output_reg = self.lower_ctx.alloc_tmp(ret_ty).only_reg().unwrap();\n\n        emit_vm_call(\n            self.lower_ctx,\n            &self.backend.flags,\n            &self.backend.triple,\n            libcall.clone(),\n            &[a, b, c],\n            &[output_reg],\n        )\n        .expect(\"Failed to emit LibCall\");\n\n        output_reg.to_reg()\n    }\n\n    #[inline]\n    fn single_target(&mut self, targets: &MachLabelSlice) -> Option<MachLabel> {\n        if targets.len() == 1 {\n            Some(targets[0])\n        } else {\n            None\n        }\n    }\n\n    #[inline]\n    fn two_targets(&mut self, targets: &MachLabelSlice) -> Option<(MachLabel, MachLabel)> {\n        if targets.len() == 2 {\n            Some((targets[0], targets[1]))\n        } else {\n            None\n        }\n    }\n\n    #[inline]\n    fn jump_table_targets(\n        &mut self,\n        targets: &MachLabelSlice,\n    ) -> Option<(MachLabel, BoxVecMachLabel)> {\n        if targets.is_empty() {\n            return None;\n        }\n\n        let default_label = targets[0];\n        let jt_targets = Box::new(SmallVec::from(&targets[1..]));\n        Some((default_label, jt_targets))\n    }\n\n    #[inline]\n    fn jump_table_size(&mut self, targets: &BoxVecMachLabel) -> u32 {\n        targets.len() as u32\n    }\n\n    #[inline]\n    fn vconst_all_ones_or_all_zeros(&mut self, constant: Constant) -> Option<()> {\n        let const_data = self.lower_ctx.get_constant_data(constant);\n        if const_data.iter().all(|&b| b == 0 || b == 0xFF) {\n            return Some(());\n        }\n        None\n    }\n\n    #[inline]\n    fn fcvt_uint_mask_const(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&UINT_MASK))\n    }\n\n    #[inline]\n    fn fcvt_uint_mask_high_const(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&UINT_MASK_HIGH))\n    }\n\n    #[inline]\n    fn iadd_pairwise_mul_const_16(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&IADD_PAIRWISE_MUL_CONST_16))\n    }\n\n    #[inline]\n    fn iadd_pairwise_mul_const_32(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&IADD_PAIRWISE_MUL_CONST_32))\n    }\n\n    #[inline]\n    fn iadd_pairwise_xor_const_32(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&IADD_PAIRWISE_XOR_CONST_32))\n    }\n\n    #[inline]\n    fn iadd_pairwise_addd_const_32(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&IADD_PAIRWISE_ADDD_CONST_32))\n    }\n\n    #[inline]\n    fn snarrow_umax_mask(&mut self) -> VCodeConstant {\n        // 2147483647.0 is equivalent to 0x41DFFFFFFFC00000\n        static UMAX_MASK: [u8; 16] = [\n            0x00, 0x00, 0xC0, 0xFF, 0xFF, 0xFF, 0xDF, 0x41, 0x00, 0x00, 0xC0, 0xFF, 0xFF, 0xFF,\n            0xDF, 0x41,\n        ];\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&UMAX_MASK))\n    }\n\n    #[inline]\n    fn shuffle_0_31_mask(&mut self, mask: &VecMask) -> VCodeConstant {\n        let mask = mask\n            .iter()\n            .map(|&b| if b > 15 { b.wrapping_sub(15) } else { b })\n            .map(|b| if b > 15 { 0b10000000 } else { b })\n            .collect();\n        self.lower_ctx\n            .use_constant(VCodeConstantData::Generated(mask))\n    }\n\n    #[inline]\n    fn shuffle_0_15_mask(&mut self, mask: &VecMask) -> VCodeConstant {\n        let mask = mask\n            .iter()\n            .map(|&b| if b > 15 { 0b10000000 } else { b })\n            .collect();\n        self.lower_ctx\n            .use_constant(VCodeConstantData::Generated(mask))\n    }\n\n    #[inline]\n    fn shuffle_16_31_mask(&mut self, mask: &VecMask) -> VCodeConstant {\n        let mask = mask\n            .iter()\n            .map(|&b| b.wrapping_sub(16))\n            .map(|b| if b > 15 { 0b10000000 } else { b })\n            .collect();\n        self.lower_ctx\n            .use_constant(VCodeConstantData::Generated(mask))\n    }\n\n    #[inline]\n    fn perm_from_mask_with_zeros(\n        &mut self,\n        mask: &VecMask,\n    ) -> Option<(VCodeConstant, VCodeConstant)> {\n        if !mask.iter().any(|&b| b > 31) {\n            return None;\n        }\n\n        let zeros = mask\n            .iter()\n            .map(|&b| if b > 31 { 0x00 } else { 0xff })\n            .collect();\n\n        Some((\n            self.perm_from_mask(mask),\n            self.lower_ctx\n                .use_constant(VCodeConstantData::Generated(zeros)),\n        ))\n    }\n\n    #[inline]\n    fn perm_from_mask(&mut self, mask: &VecMask) -> VCodeConstant {\n        let mask = mask.iter().cloned().collect();\n        self.lower_ctx\n            .use_constant(VCodeConstantData::Generated(mask))\n    }\n\n    #[inline]\n    fn swizzle_zero_mask(&mut self) -> VCodeConstant {\n        static ZERO_MASK_VALUE: [u8; 16] = [0x70; 16];\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&ZERO_MASK_VALUE))\n    }\n\n    #[inline]\n    fn sqmul_round_sat_mask(&mut self) -> VCodeConstant {\n        static SAT_MASK: [u8; 16] = [\n            0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80,\n            0x00, 0x80,\n        ];\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&SAT_MASK))\n    }\n\n    #[inline]\n    fn uunarrow_umax_mask(&mut self) -> VCodeConstant {\n        // 4294967295.0 is equivalent to 0x41EFFFFFFFE00000\n        static UMAX_MASK: [u8; 16] = [\n            0x00, 0x00, 0xE0, 0xFF, 0xFF, 0xFF, 0xEF, 0x41, 0x00, 0x00, 0xE0, 0xFF, 0xFF, 0xFF,\n            0xEF, 0x41,\n        ];\n\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&UMAX_MASK))\n    }\n\n    #[inline]\n    fn uunarrow_uint_mask(&mut self) -> VCodeConstant {\n        static UINT_MASK: [u8; 16] = [\n            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n            0x30, 0x43,\n        ];\n\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&UINT_MASK))\n    }\n\n    fn emit_div_or_rem(\n        &mut self,\n        kind: &DivOrRemKind,\n        ty: Type,\n        dst: WritableGpr,\n        dividend: Gpr,\n        divisor: Gpr,\n    ) {\n        let is_div = kind.is_div();\n        let size = OperandSize::from_ty(ty);\n\n        let dst_quotient = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n        let dst_remainder = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n\n        // Always do explicit checks for `srem`: otherwise, INT_MIN % -1 is not handled properly.\n        if self.backend.flags.avoid_div_traps() || *kind == DivOrRemKind::SignedRem {\n            // A vcode meta-instruction is used to lower the inline checks, since they embed\n            // pc-relative offsets that must not change, thus requiring regalloc to not\n            // interfere by introducing spills and reloads.\n            let tmp = if *kind == DivOrRemKind::SignedDiv && size == OperandSize::Size64 {\n                Some(self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap())\n            } else {\n                None\n            };\n            let dividend_hi = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n            self.lower_ctx.emit(MInst::AluConstOp {\n                op: AluRmiROpcode::Xor,\n                size: OperandSize::Size32,\n                dst: WritableGpr::from_reg(Gpr::new(dividend_hi.to_reg()).unwrap()),\n            });\n            self.lower_ctx.emit(MInst::checked_div_or_rem_seq(\n                kind.clone(),\n                size,\n                divisor.to_reg(),\n                Gpr::new(dividend.to_reg()).unwrap(),\n                Gpr::new(dividend_hi.to_reg()).unwrap(),\n                WritableGpr::from_reg(Gpr::new(dst_quotient.to_reg()).unwrap()),\n                WritableGpr::from_reg(Gpr::new(dst_remainder.to_reg()).unwrap()),\n                tmp,\n            ));\n        } else {\n            // We don't want more than one trap record for a single instruction,\n            // so let's not allow the \"mem\" case (load-op merging) here; force\n            // divisor into a register instead.\n            let divisor = RegMem::reg(divisor.to_reg());\n\n            let dividend_hi = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n\n            // Fill in the high parts:\n            let dividend_lo = if kind.is_signed() && ty == types::I8 {\n                let dividend_lo = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n                // 8-bit div takes its dividend in only the `lo` reg.\n                self.lower_ctx.emit(MInst::sign_extend_data(\n                    size,\n                    Gpr::new(dividend.to_reg()).unwrap(),\n                    WritableGpr::from_reg(Gpr::new(dividend_lo.to_reg()).unwrap()),\n                ));\n                // `dividend_hi` is not used by the Div below, so we\n                // don't def it here.\n\n                dividend_lo.to_reg()\n            } else if kind.is_signed() {\n                // 16-bit and higher div takes its operand in hi:lo\n                // with half in each (64:64, 32:32 or 16:16).\n                self.lower_ctx.emit(MInst::sign_extend_data(\n                    size,\n                    Gpr::new(dividend.to_reg()).unwrap(),\n                    WritableGpr::from_reg(Gpr::new(dividend_hi.to_reg()).unwrap()),\n                ));\n\n                dividend.to_reg()\n            } else if ty == types::I8 {\n                let dividend_lo = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n                self.lower_ctx.emit(MInst::movzx_rm_r(\n                    ExtMode::BL,\n                    RegMem::reg(dividend.to_reg()),\n                    dividend_lo,\n                ));\n\n                dividend_lo.to_reg()\n            } else {\n                // zero for unsigned opcodes.\n                self.lower_ctx\n                    .emit(MInst::imm(OperandSize::Size64, 0, dividend_hi));\n\n                dividend.to_reg()\n            };\n\n            // Emit the actual idiv.\n            self.lower_ctx.emit(MInst::div(\n                size,\n                kind.is_signed(),\n                divisor,\n                Gpr::new(dividend_lo).unwrap(),\n                Gpr::new(dividend_hi.to_reg()).unwrap(),\n                WritableGpr::from_reg(Gpr::new(dst_quotient.to_reg()).unwrap()),\n                WritableGpr::from_reg(Gpr::new(dst_remainder.to_reg()).unwrap()),\n            ));\n        }\n\n        // Move the result back into the destination reg.\n        if is_div {\n            // The quotient is in rax.\n            self.lower_ctx.emit(MInst::gen_move(\n                dst.to_writable_reg(),\n                dst_quotient.to_reg(),\n                ty,\n            ));\n        } else {\n            if size == OperandSize::Size8 {\n                let tmp = self.temp_writable_reg(ty);\n                // The remainder is in AH. Right-shift by 8 bits then move from rax.\n                self.lower_ctx.emit(MInst::shift_r(\n                    OperandSize::Size64,\n                    ShiftKind::ShiftRightLogical,\n                    Imm8Gpr::new(Imm8Reg::Imm8 { imm: 8 }).unwrap(),\n                    dst_quotient.to_reg(),\n                    tmp,\n                ));\n                self.lower_ctx\n                    .emit(MInst::gen_move(dst.to_writable_reg(), tmp.to_reg(), ty));\n            } else {\n                // The remainder is in rdx.\n                self.lower_ctx.emit(MInst::gen_move(\n                    dst.to_writable_reg(),\n                    dst_remainder.to_reg(),\n                    ty,\n                ));\n            }\n        }\n    }\n\n    fn xmm_mem_to_xmm_mem_aligned(&mut self, arg: &XmmMem) -> XmmMemAligned {\n        match XmmMemAligned::new(arg.clone().into()) {\n            Some(aligned) => aligned,\n            None => match arg.clone().into() {\n                RegMem::Mem { addr } => self.load_xmm_unaligned(addr).into(),\n                _ => unreachable!(),\n            },\n        }\n    }\n\n    fn xmm_mem_imm_to_xmm_mem_aligned_imm(&mut self, arg: &XmmMemImm) -> XmmMemAlignedImm {\n        match XmmMemAlignedImm::new(arg.clone().into()) {\n            Some(aligned) => aligned,\n            None => match arg.clone().into() {\n                RegMemImm::Mem { addr } => self.load_xmm_unaligned(addr).into(),\n                _ => unreachable!(),\n            },\n        }\n    }\n}\n\nimpl IsleContext<'_, '_, MInst, X64Backend> {\n    isle_prelude_method_helpers!(X64Caller);\n\n    fn load_xmm_unaligned(&mut self, addr: SyntheticAmode) -> Xmm {\n        let tmp = self.lower_ctx.alloc_tmp(types::F32X4).only_reg().unwrap();\n        self.lower_ctx.emit(MInst::XmmUnaryRmRUnaligned {\n            op: SseOpcode::Movdqu,\n            src: XmmMem::new(RegMem::mem(addr)).unwrap(),\n            dst: Writable::from_reg(Xmm::new(tmp.to_reg()).unwrap()),\n        });\n        Xmm::new(tmp.to_reg()).unwrap()\n    }\n}\n\n// Since x64 doesn't have 8x16 shifts and we must use a 16x8 shift instead, we\n// need to fix up the bits that migrate from one half of the lane to the\n// other. Each 16-byte mask is indexed by the shift amount: e.g. if we shift\n// right by 0 (no movement), we want to retain all the bits so we mask with\n// `0xff`; if we shift right by 1, we want to retain all bits except the MSB so\n// we mask with `0x7f`; etc.\n\n#[rustfmt::skip] // Preserve 16 bytes (i.e. one mask) per row.\nconst I8X16_ISHL_MASKS: [u8; 128] = [\n    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n    0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe,\n    0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc,\n    0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8,\n    0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0,\n    0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0,\n    0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0,\n    0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80,\n];\n\n#[rustfmt::skip] // Preserve 16 bytes (i.e. one mask) per row.\nconst I8X16_USHR_MASKS: [u8; 128] = [\n    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n    0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f,\n    0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f,\n    0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f,\n    0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f,\n    0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07,\n    0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03,\n    0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01,\n];\n\n/// Number of bits set in a given nibble (4-bit value). Used in the\n/// vector implementation of popcount.\n#[rustfmt::skip] // Preserve 4x4 layout.\nconst POPCOUNT_4BIT_TABLE: [u8; 16] = [\n    0x00, 0x01, 0x01, 0x02,\n    0x01, 0x02, 0x02, 0x03,\n    0x01, 0x02, 0x02, 0x03,\n    0x02, 0x03, 0x03, 0x04,\n];\n\nconst POPCOUNT_LOW_MASK: [u8; 16] = [0x0f; 16];\n\n#[inline]\nfn to_simm32(constant: i64) -> Option<GprMemImm> {\n    if constant == ((constant << 32) >> 32) {\n        Some(\n            GprMemImm::new(RegMemImm::Imm {\n                simm32: constant as u32,\n            })\n            .unwrap(),\n        )\n    } else {\n        None\n    }\n}\n\nconst UINT_MASK: [u8; 16] = [\n    0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n\nconst UINT_MASK_HIGH: [u8; 16] = [\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x30, 0x43,\n];\n\nconst IADD_PAIRWISE_MUL_CONST_16: [u8; 16] = [0x01; 16];\n\nconst IADD_PAIRWISE_MUL_CONST_32: [u8; 16] = [\n    0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00,\n];\n\nconst IADD_PAIRWISE_XOR_CONST_32: [u8; 16] = [\n    0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80,\n];\n\nconst IADD_PAIRWISE_ADDD_CONST_32: [u8; 16] = [\n    0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00,\n];\n", "test compile precise-output\nset enable_simd\ntarget x86_64 has_sse3 has_ssse3 has_sse41\n\n;; shuffle\n\nfunction %shuffle_different_ssa_values() -> i8x16 {\nblock0:\n    v0 = vconst.i8x16 0x00\n    v1 = vconst.i8x16 0x01\n    v2 = shuffle v0, v1, 0x11000000000000000000000000000000     ;; pick the second lane of v1, the rest use the first lane of v0\n    return v2\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movdqu  const(3), %xmm0\n;   movdqu  const(2), %xmm2\n;   pshufb  %xmm0, const(0), %xmm0\n;   pshufb  %xmm2, const(1), %xmm2\n;   por     %xmm0, %xmm2, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movdqu 0x54(%rip), %xmm0\n;   movdqu 0x3c(%rip), %xmm2\n;   pshufb 0x13(%rip), %xmm0\n;   pshufb 0x1a(%rip), %xmm2\n;   por %xmm2, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb $0x80, -0x7f7f7f80(%rax)\n;   addb $0x80, -0x7f7f7f80(%rax)\n;   addb $0, 0x101(%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n\nfunction %shuffle_same_ssa_value() -> i8x16 {\nblock0:\n    v1 = vconst.i8x16 0x01\n    v2 = shuffle v1, v1, 0x13000000000000000000000000000000     ;; pick the fourth lane of v1 and the rest from the first lane of v1\n    return v2\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movdqu  const(1), %xmm0\n;   pshufb  %xmm0, const(0), %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movdqu 0x24(%rip), %xmm0\n;   pshufb 0xb(%rip), %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rcx, %rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n\nfunction %swizzle() -> i8x16 {\nblock0:\n    v0 = vconst.i8x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]\n    v1 = vconst.i8x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]\n    v2 = swizzle v0, v1\n    return v2\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movdqu  const(1), %xmm0\n;   movdqu  const(1), %xmm1\n;   paddusb %xmm1, const(0), %xmm1\n;   pshufb  %xmm0, %xmm1, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movdqu 0x34(%rip), %xmm0\n;   movdqu 0x2c(%rip), %xmm1\n;   paddusb 0x14(%rip), %xmm1\n;   pshufb %xmm1, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   jo 0xa2\n;   jo 0xa4\n;   jo 0xa6\n;   jo 0xa8\n;   jo 0xaa\n;   jo 0xac\n;   jo 0xae\n;   jo 0xb0\n;   addb %al, (%rcx)\n;   addb (%rbx), %al\n;   addb $5, %al\n\nfunction %splat_i8(i8) -> i8x16 {\nblock0(v0: i8):\n    v1 = splat.i8x16 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   uninit  %xmm0\n;   pinsrb  $0, %xmm0, %rdi, %xmm0\n;   pxor    %xmm6, %xmm6, %xmm6\n;   pshufb  %xmm0, %xmm6, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   pinsrb $0, %edi, %xmm0\n;   pxor %xmm6, %xmm6\n;   pshufb %xmm6, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %splat_i16() -> i16x8 {\nblock0:\n    v0 = iconst.i16 -1\n    v1 = splat.i16x8 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movl    $-1, %esi\n;   uninit  %xmm4\n;   pinsrw  $0, %xmm4, %rsi, %xmm4\n;   pinsrw  $1, %xmm4, %rsi, %xmm4\n;   pshufd  $0, %xmm4, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movl $0xffffffff, %esi\n;   pinsrw $0, %esi, %xmm4\n;   pinsrw $1, %esi, %xmm4\n;   pshufd $0, %xmm4, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %splat_i32(i32) -> i32x4 {\nblock0(v0: i32):\n    v1 = splat.i32x4 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   uninit  %xmm3\n;   pinsrd  $0, %xmm3, %rdi, %xmm3\n;   pshufd  $0, %xmm3, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   pinsrd $0, %edi, %xmm3\n;   pshufd $0, %xmm3, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %splat_f64(f64) -> f64x2 {\nblock0(v0: f64):\n    v1 = splat.f64x2 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movdqa  %xmm0, %xmm5\n;   uninit  %xmm0\n;   movdqa  %xmm5, %xmm6\n;   movsd   %xmm0, %xmm6, %xmm0\n;   movlhps %xmm0, %xmm6, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movdqa %xmm0, %xmm5\n;   movdqa %xmm5, %xmm6\n;   movsd %xmm6, %xmm0\n;   movlhps %xmm6, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %load32_zero_coalesced(i64) -> i32x4 {\nblock0(v0: i64):\n    v1 = load.i32 v0\n    v2 = scalar_to_vector.i32x4 v1\n    return v2\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movss   0(%rdi), %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movss (%rdi), %xmm0 ; trap: heap_oob\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %load32_zero_int(i32) -> i32x4 {\nblock0(v0: i32):\n    v1 = scalar_to_vector.i32x4 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movd    %edi, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movd %edi, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %load32_zero_float(f32) -> f32x4 {\nblock0(v0: f32):\n    v1 = scalar_to_vector.f32x4 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\n", "test interpret\ntest run\ntarget aarch64\ntarget s390x\nset enable_simd\ntarget x86_64 has_sse3 has_ssse3 has_sse41\ntarget x86_64 has_sse3 has_ssse3 has_sse41 has_avx512vl has_avx512vbmi\n\nfunction %shuffle_i8x16(i8x16, i8x16) -> i8x16 {\nblock0(v0: i8x16, v1: i8x16):\n    v2 = shuffle v0, v1, [3 0 31 26 4 6 12 11 23 13 24 4 2 15 17 5]\n    return v2\n}\n; run: %shuffle_i8x16([1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16], [17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32]) == [4 1 32 27 5 7 13 12 24 14 25 5 3 16 18 6]\n\nfunction %shuffle_zeros(i8x16, i8x16) -> i8x16 {\nblock0(v0: i8x16, v1: i8x16):\n    v2 = shuffle v0, v1, [3 0 32 255 4 6 12 11 23 13 24 4 2 97 17 5]\n    return v2\n}\n; run: %shuffle_zeros([1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16], [17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32]) == [4 1 0 0 5 7 13 12 24 14 25 5 3 0 18 6]\n"], "fixing_code": ["//! ISLE integration glue code for x64 lowering.\n\n// Pull in the ISLE generated code.\npub(crate) mod generated_code;\nuse crate::{\n    ir::types,\n    ir::AtomicRmwOp,\n    machinst::{InputSourceInst, Reg, Writable},\n};\nuse crate::{isle_common_prelude_methods, isle_lower_prelude_methods};\nuse generated_code::{Context, MInst, RegisterClass};\n\n// Types that the generated ISLE code uses via `use super::*`.\nuse super::{is_int_or_ref_ty, is_mergeable_load, lower_to_amode};\nuse crate::ir::LibCall;\nuse crate::isa::x64::lower::emit_vm_call;\nuse crate::isa::x64::X64Backend;\nuse crate::{\n    ir::{\n        condcodes::{CondCode, FloatCC, IntCC},\n        immediates::*,\n        types::*,\n        BlockCall, Inst, InstructionData, MemFlags, Opcode, TrapCode, Value, ValueList,\n    },\n    isa::{\n        unwind::UnwindInst,\n        x64::{\n            abi::X64Caller,\n            inst::{args::*, regs, CallInfo},\n        },\n    },\n    machinst::{\n        isle::*, valueregs, ArgPair, InsnInput, InstOutput, Lower, MachAtomicRmwOp, MachInst,\n        VCodeConstant, VCodeConstantData,\n    },\n};\nuse alloc::vec::Vec;\nuse regalloc2::PReg;\nuse smallvec::SmallVec;\nuse std::boxed::Box;\nuse std::convert::TryFrom;\n\ntype BoxCallInfo = Box<CallInfo>;\ntype BoxVecMachLabel = Box<SmallVec<[MachLabel; 4]>>;\ntype MachLabelSlice = [MachLabel];\ntype VecArgPair = Vec<ArgPair>;\n\npub struct SinkableLoad {\n    inst: Inst,\n    addr_input: InsnInput,\n    offset: i32,\n}\n\n/// The main entry point for lowering with ISLE.\npub(crate) fn lower(\n    lower_ctx: &mut Lower<MInst>,\n    backend: &X64Backend,\n    inst: Inst,\n) -> Option<InstOutput> {\n    // TODO: reuse the ISLE context across lowerings so we can reuse its\n    // internal heap allocations.\n    let mut isle_ctx = IsleContext { lower_ctx, backend };\n    generated_code::constructor_lower(&mut isle_ctx, inst)\n}\n\npub(crate) fn lower_branch(\n    lower_ctx: &mut Lower<MInst>,\n    backend: &X64Backend,\n    branch: Inst,\n    targets: &[MachLabel],\n) -> Option<()> {\n    // TODO: reuse the ISLE context across lowerings so we can reuse its\n    // internal heap allocations.\n    let mut isle_ctx = IsleContext { lower_ctx, backend };\n    generated_code::constructor_lower_branch(&mut isle_ctx, branch, &targets.to_vec())\n}\n\nimpl Context for IsleContext<'_, '_, MInst, X64Backend> {\n    isle_lower_prelude_methods!();\n    isle_prelude_caller_methods!(X64ABIMachineSpec, X64Caller);\n\n    #[inline]\n    fn operand_size_of_type_32_64(&mut self, ty: Type) -> OperandSize {\n        if ty.bits() == 64 {\n            OperandSize::Size64\n        } else {\n            OperandSize::Size32\n        }\n    }\n\n    #[inline]\n    fn raw_operand_size_of_type(&mut self, ty: Type) -> OperandSize {\n        OperandSize::from_ty(ty)\n    }\n\n    fn put_in_reg_mem_imm(&mut self, val: Value) -> RegMemImm {\n        let inputs = self.lower_ctx.get_value_as_source_or_const(val);\n\n        if let Some(c) = inputs.constant {\n            if let Some(imm) = to_simm32(c as i64) {\n                return imm.to_reg_mem_imm();\n            }\n        }\n\n        self.put_in_reg_mem(val).into()\n    }\n\n    fn put_in_xmm_mem_imm(&mut self, val: Value) -> XmmMemImm {\n        let inputs = self.lower_ctx.get_value_as_source_or_const(val);\n\n        if let Some(c) = inputs.constant {\n            if let Some(imm) = to_simm32(c as i64) {\n                return XmmMemImm::new(imm.to_reg_mem_imm()).unwrap();\n            }\n        }\n\n        let res = match self.put_in_xmm_mem(val).to_reg_mem() {\n            RegMem::Reg { reg } => RegMemImm::Reg { reg },\n            RegMem::Mem { addr } => RegMemImm::Mem { addr },\n        };\n\n        XmmMemImm::new(res).unwrap()\n    }\n\n    fn put_in_xmm_mem(&mut self, val: Value) -> XmmMem {\n        let inputs = self.lower_ctx.get_value_as_source_or_const(val);\n\n        if let Some(c) = inputs.constant {\n            // A load from the constant pool is better than a rematerialization into a register,\n            // because it reduces register pressure.\n            //\n            // NOTE: this is where behavior differs from `put_in_reg_mem`, as we always force\n            // constants to be 16 bytes when a constant will be used in place of an xmm register.\n            let vcode_constant = self.emit_u128_le_const(c as u128);\n            return XmmMem::new(RegMem::mem(SyntheticAmode::ConstantOffset(vcode_constant)))\n                .unwrap();\n        }\n\n        XmmMem::new(self.put_in_reg_mem(val)).unwrap()\n    }\n\n    fn put_in_reg_mem(&mut self, val: Value) -> RegMem {\n        let inputs = self.lower_ctx.get_value_as_source_or_const(val);\n\n        if let Some(c) = inputs.constant {\n            // A load from the constant pool is better than a\n            // rematerialization into a register, because it reduces\n            // register pressure.\n            let vcode_constant = self.emit_u64_le_const(c);\n            return RegMem::mem(SyntheticAmode::ConstantOffset(vcode_constant));\n        }\n\n        if let Some(load) = self.sinkable_load(val) {\n            return self.sink_load(&load);\n        }\n\n        RegMem::reg(self.put_in_reg(val))\n    }\n\n    #[inline]\n    fn encode_fcmp_imm(&mut self, imm: &FcmpImm) -> u8 {\n        imm.encode()\n    }\n\n    #[inline]\n    fn encode_round_imm(&mut self, imm: &RoundImm) -> u8 {\n        imm.encode()\n    }\n\n    #[inline]\n    fn has_avx(&mut self) -> bool {\n        self.backend.x64_flags.has_avx()\n    }\n\n    #[inline]\n    fn avx512vl_enabled(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_avx512vl_simd()\n    }\n\n    #[inline]\n    fn avx512dq_enabled(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_avx512dq_simd()\n    }\n\n    #[inline]\n    fn avx512f_enabled(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_avx512f_simd()\n    }\n\n    #[inline]\n    fn avx512bitalg_enabled(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_avx512bitalg_simd()\n    }\n\n    #[inline]\n    fn avx512vbmi_enabled(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_avx512vbmi_simd()\n    }\n\n    #[inline]\n    fn use_lzcnt(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_lzcnt()\n    }\n\n    #[inline]\n    fn use_bmi1(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_bmi1()\n    }\n\n    #[inline]\n    fn use_popcnt(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_popcnt()\n    }\n\n    #[inline]\n    fn use_fma(&mut self) -> bool {\n        self.backend.x64_flags.use_fma()\n    }\n\n    #[inline]\n    fn use_sse41(&mut self, _: Type) -> bool {\n        self.backend.x64_flags.use_sse41()\n    }\n\n    #[inline]\n    fn imm8_from_value(&mut self, val: Value) -> Option<Imm8Reg> {\n        let inst = self.lower_ctx.dfg().value_def(val).inst()?;\n        let constant = self.lower_ctx.get_constant(inst)?;\n        let imm = u8::try_from(constant).ok()?;\n        Some(Imm8Reg::Imm8 { imm })\n    }\n\n    #[inline]\n    fn const_to_type_masked_imm8(&mut self, c: u64, ty: Type) -> Imm8Gpr {\n        let mask = self.shift_mask(ty) as u64;\n        Imm8Gpr::new(Imm8Reg::Imm8 {\n            imm: (c & mask) as u8,\n        })\n        .unwrap()\n    }\n\n    #[inline]\n    fn shift_mask(&mut self, ty: Type) -> u32 {\n        debug_assert!(ty.lane_bits().is_power_of_two());\n\n        ty.lane_bits() - 1\n    }\n\n    fn shift_amount_masked(&mut self, ty: Type, val: Imm64) -> u32 {\n        (val.bits() as u32) & self.shift_mask(ty)\n    }\n\n    #[inline]\n    fn simm32_from_value(&mut self, val: Value) -> Option<GprMemImm> {\n        let inst = self.lower_ctx.dfg().value_def(val).inst()?;\n        let constant: u64 = self.lower_ctx.get_constant(inst)?;\n        let constant = constant as i64;\n        to_simm32(constant)\n    }\n\n    #[inline]\n    fn simm32_from_imm64(&mut self, imm: Imm64) -> Option<GprMemImm> {\n        to_simm32(imm.bits())\n    }\n\n    fn sinkable_load(&mut self, val: Value) -> Option<SinkableLoad> {\n        let input = self.lower_ctx.get_value_as_source_or_const(val);\n        if let InputSourceInst::UniqueUse(inst, 0) = input.inst {\n            if let Some((addr_input, offset)) = is_mergeable_load(self.lower_ctx, inst) {\n                return Some(SinkableLoad {\n                    inst,\n                    addr_input,\n                    offset,\n                });\n            }\n        }\n        None\n    }\n\n    fn sink_load(&mut self, load: &SinkableLoad) -> RegMem {\n        self.lower_ctx.sink_inst(load.inst);\n        let addr = lower_to_amode(self.lower_ctx, load.addr_input, load.offset);\n        RegMem::Mem {\n            addr: SyntheticAmode::Real(addr),\n        }\n    }\n\n    #[inline]\n    fn ext_mode(&mut self, from_bits: u16, to_bits: u16) -> ExtMode {\n        ExtMode::new(from_bits, to_bits).unwrap()\n    }\n\n    fn emit(&mut self, inst: &MInst) -> Unit {\n        self.lower_ctx.emit(inst.clone());\n    }\n\n    #[inline]\n    fn nonzero_u64_fits_in_u32(&mut self, x: u64) -> Option<u64> {\n        if x != 0 && x < u64::from(u32::MAX) {\n            Some(x)\n        } else {\n            None\n        }\n    }\n\n    #[inline]\n    fn sse_insertps_lane_imm(&mut self, lane: u8) -> u8 {\n        // Insert 32-bits from replacement (at index 00, bits 7:8) to vector (lane\n        // shifted into bits 5:6).\n        0b00_00_00_00 | lane << 4\n    }\n\n    #[inline]\n    fn synthetic_amode_to_reg_mem(&mut self, addr: &SyntheticAmode) -> RegMem {\n        RegMem::mem(addr.clone())\n    }\n\n    #[inline]\n    fn amode_imm_reg_reg_shift(&mut self, simm32: u32, base: Gpr, index: Gpr, shift: u8) -> Amode {\n        Amode::imm_reg_reg_shift(simm32, base, index, shift)\n    }\n\n    #[inline]\n    fn amode_imm_reg(&mut self, simm32: u32, base: Gpr) -> Amode {\n        Amode::imm_reg(simm32, base.to_reg())\n    }\n\n    #[inline]\n    fn amode_with_flags(&mut self, amode: &Amode, flags: MemFlags) -> Amode {\n        amode.with_flags(flags)\n    }\n\n    #[inline]\n    fn amode_to_synthetic_amode(&mut self, amode: &Amode) -> SyntheticAmode {\n        amode.clone().into()\n    }\n\n    #[inline]\n    fn const_to_synthetic_amode(&mut self, c: VCodeConstant) -> SyntheticAmode {\n        SyntheticAmode::ConstantOffset(c)\n    }\n\n    #[inline]\n    fn writable_gpr_to_reg(&mut self, r: WritableGpr) -> WritableReg {\n        r.to_writable_reg()\n    }\n\n    #[inline]\n    fn writable_xmm_to_reg(&mut self, r: WritableXmm) -> WritableReg {\n        r.to_writable_reg()\n    }\n\n    fn ishl_i8x16_mask_for_const(&mut self, amt: u32) -> SyntheticAmode {\n        // When the shift amount is known, we can statically (i.e. at compile\n        // time) determine the mask to use and only emit that.\n        debug_assert!(amt < 8);\n        let mask_offset = amt as usize * 16;\n        let mask_constant = self.lower_ctx.use_constant(VCodeConstantData::WellKnown(\n            &I8X16_ISHL_MASKS[mask_offset..mask_offset + 16],\n        ));\n        SyntheticAmode::ConstantOffset(mask_constant)\n    }\n\n    fn ishl_i8x16_mask_table(&mut self) -> SyntheticAmode {\n        let mask_table = self\n            .lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&I8X16_ISHL_MASKS));\n        SyntheticAmode::ConstantOffset(mask_table)\n    }\n\n    fn ushr_i8x16_mask_for_const(&mut self, amt: u32) -> SyntheticAmode {\n        // When the shift amount is known, we can statically (i.e. at compile\n        // time) determine the mask to use and only emit that.\n        debug_assert!(amt < 8);\n        let mask_offset = amt as usize * 16;\n        let mask_constant = self.lower_ctx.use_constant(VCodeConstantData::WellKnown(\n            &I8X16_USHR_MASKS[mask_offset..mask_offset + 16],\n        ));\n        SyntheticAmode::ConstantOffset(mask_constant)\n    }\n\n    fn ushr_i8x16_mask_table(&mut self) -> SyntheticAmode {\n        let mask_table = self\n            .lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&I8X16_USHR_MASKS));\n        SyntheticAmode::ConstantOffset(mask_table)\n    }\n\n    fn popcount_4bit_table(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&POPCOUNT_4BIT_TABLE))\n    }\n\n    fn popcount_low_mask(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&POPCOUNT_LOW_MASK))\n    }\n\n    #[inline]\n    fn writable_reg_to_xmm(&mut self, r: WritableReg) -> WritableXmm {\n        Writable::from_reg(Xmm::new(r.to_reg()).unwrap())\n    }\n\n    #[inline]\n    fn writable_xmm_to_xmm(&mut self, r: WritableXmm) -> Xmm {\n        r.to_reg()\n    }\n\n    #[inline]\n    fn writable_gpr_to_gpr(&mut self, r: WritableGpr) -> Gpr {\n        r.to_reg()\n    }\n\n    #[inline]\n    fn gpr_to_reg(&mut self, r: Gpr) -> Reg {\n        r.into()\n    }\n\n    #[inline]\n    fn xmm_to_reg(&mut self, r: Xmm) -> Reg {\n        r.into()\n    }\n\n    #[inline]\n    fn xmm_to_xmm_mem_imm(&mut self, r: Xmm) -> XmmMemImm {\n        r.into()\n    }\n\n    #[inline]\n    fn xmm_mem_to_xmm_mem_imm(&mut self, r: &XmmMem) -> XmmMemImm {\n        XmmMemImm::new(r.clone().to_reg_mem().into()).unwrap()\n    }\n\n    #[inline]\n    fn temp_writable_gpr(&mut self) -> WritableGpr {\n        Writable::from_reg(Gpr::new(self.temp_writable_reg(I64).to_reg()).unwrap())\n    }\n\n    #[inline]\n    fn temp_writable_xmm(&mut self) -> WritableXmm {\n        Writable::from_reg(Xmm::new(self.temp_writable_reg(I8X16).to_reg()).unwrap())\n    }\n\n    #[inline]\n    fn reg_to_reg_mem_imm(&mut self, reg: Reg) -> RegMemImm {\n        RegMemImm::Reg { reg }\n    }\n\n    #[inline]\n    fn reg_mem_to_xmm_mem(&mut self, rm: &RegMem) -> XmmMem {\n        XmmMem::new(rm.clone()).unwrap()\n    }\n\n    #[inline]\n    fn gpr_mem_imm_new(&mut self, rmi: &RegMemImm) -> GprMemImm {\n        GprMemImm::new(rmi.clone()).unwrap()\n    }\n\n    #[inline]\n    fn xmm_mem_imm_new(&mut self, rmi: &RegMemImm) -> XmmMemImm {\n        XmmMemImm::new(rmi.clone()).unwrap()\n    }\n\n    #[inline]\n    fn xmm_to_xmm_mem(&mut self, r: Xmm) -> XmmMem {\n        r.into()\n    }\n\n    #[inline]\n    fn xmm_mem_to_reg_mem(&mut self, xm: &XmmMem) -> RegMem {\n        xm.clone().into()\n    }\n\n    #[inline]\n    fn gpr_mem_to_reg_mem(&mut self, gm: &GprMem) -> RegMem {\n        gm.clone().into()\n    }\n\n    #[inline]\n    fn xmm_new(&mut self, r: Reg) -> Xmm {\n        Xmm::new(r).unwrap()\n    }\n\n    #[inline]\n    fn gpr_new(&mut self, r: Reg) -> Gpr {\n        Gpr::new(r).unwrap()\n    }\n\n    #[inline]\n    fn reg_mem_to_gpr_mem(&mut self, rm: &RegMem) -> GprMem {\n        GprMem::new(rm.clone()).unwrap()\n    }\n\n    #[inline]\n    fn reg_to_gpr_mem(&mut self, r: Reg) -> GprMem {\n        GprMem::new(RegMem::reg(r)).unwrap()\n    }\n\n    #[inline]\n    fn imm8_reg_to_imm8_gpr(&mut self, ir: &Imm8Reg) -> Imm8Gpr {\n        Imm8Gpr::new(ir.clone()).unwrap()\n    }\n\n    #[inline]\n    fn gpr_to_gpr_mem(&mut self, gpr: Gpr) -> GprMem {\n        GprMem::from(gpr)\n    }\n\n    #[inline]\n    fn gpr_to_gpr_mem_imm(&mut self, gpr: Gpr) -> GprMemImm {\n        GprMemImm::from(gpr)\n    }\n\n    #[inline]\n    fn gpr_to_imm8_gpr(&mut self, gpr: Gpr) -> Imm8Gpr {\n        Imm8Gpr::from(gpr)\n    }\n\n    #[inline]\n    fn imm8_to_imm8_gpr(&mut self, imm: u8) -> Imm8Gpr {\n        Imm8Gpr::new(Imm8Reg::Imm8 { imm }).unwrap()\n    }\n\n    #[inline]\n    fn type_register_class(&mut self, ty: Type) -> Option<RegisterClass> {\n        if is_int_or_ref_ty(ty) || ty == I128 {\n            Some(RegisterClass::Gpr {\n                single_register: ty != I128,\n            })\n        } else if ty == F32 || ty == F64 || (ty.is_vector() && ty.bits() == 128) {\n            Some(RegisterClass::Xmm)\n        } else {\n            None\n        }\n    }\n\n    #[inline]\n    fn ty_int_bool_or_ref(&mut self, ty: Type) -> Option<()> {\n        match ty {\n            types::I8 | types::I16 | types::I32 | types::I64 | types::R64 => Some(()),\n            types::R32 => panic!(\"shouldn't have 32-bits refs on x64\"),\n            _ => None,\n        }\n    }\n\n    #[inline]\n    fn intcc_without_eq(&mut self, x: &IntCC) -> IntCC {\n        x.without_equal()\n    }\n\n    #[inline]\n    fn intcc_to_cc(&mut self, intcc: &IntCC) -> CC {\n        CC::from_intcc(*intcc)\n    }\n\n    #[inline]\n    fn cc_invert(&mut self, cc: &CC) -> CC {\n        cc.invert()\n    }\n\n    #[inline]\n    fn cc_nz_or_z(&mut self, cc: &CC) -> Option<CC> {\n        match cc {\n            CC::Z => Some(*cc),\n            CC::NZ => Some(*cc),\n            _ => None,\n        }\n    }\n\n    #[inline]\n    fn sum_extend_fits_in_32_bits(\n        &mut self,\n        extend_from_ty: Type,\n        constant_value: Imm64,\n        offset: Offset32,\n    ) -> Option<u32> {\n        let offset: i64 = offset.into();\n        let constant_value: u64 = constant_value.bits() as u64;\n        // If necessary, zero extend `constant_value` up to 64 bits.\n        let shift = 64 - extend_from_ty.bits();\n        let zero_extended_constant_value = (constant_value << shift) >> shift;\n        // Sum up the two operands.\n        let sum = offset.wrapping_add(zero_extended_constant_value as i64);\n        // Check that the sum will fit in 32-bits.\n        if sum == ((sum << 32) >> 32) {\n            Some(sum as u32)\n        } else {\n            None\n        }\n    }\n\n    #[inline]\n    fn amode_offset(&mut self, addr: &Amode, offset: u32) -> Amode {\n        addr.offset(offset)\n    }\n\n    #[inline]\n    fn zero_offset(&mut self) -> Offset32 {\n        Offset32::new(0)\n    }\n\n    #[inline]\n    fn atomic_rmw_op_to_mach_atomic_rmw_op(&mut self, op: &AtomicRmwOp) -> MachAtomicRmwOp {\n        MachAtomicRmwOp::from(*op)\n    }\n\n    #[inline]\n    fn preg_rbp(&mut self) -> PReg {\n        regs::rbp().to_real_reg().unwrap().into()\n    }\n\n    #[inline]\n    fn preg_rsp(&mut self) -> PReg {\n        regs::rsp().to_real_reg().unwrap().into()\n    }\n\n    #[inline]\n    fn preg_pinned(&mut self) -> PReg {\n        regs::pinned_reg().to_real_reg().unwrap().into()\n    }\n\n    fn libcall_1(&mut self, libcall: &LibCall, a: Reg) -> Reg {\n        let call_conv = self.lower_ctx.abi().call_conv(self.lower_ctx.sigs());\n        let ret_ty = libcall.signature(call_conv).returns[0].value_type;\n        let output_reg = self.lower_ctx.alloc_tmp(ret_ty).only_reg().unwrap();\n\n        emit_vm_call(\n            self.lower_ctx,\n            &self.backend.flags,\n            &self.backend.triple,\n            libcall.clone(),\n            &[a],\n            &[output_reg],\n        )\n        .expect(\"Failed to emit LibCall\");\n\n        output_reg.to_reg()\n    }\n\n    fn libcall_3(&mut self, libcall: &LibCall, a: Reg, b: Reg, c: Reg) -> Reg {\n        let call_conv = self.lower_ctx.abi().call_conv(self.lower_ctx.sigs());\n        let ret_ty = libcall.signature(call_conv).returns[0].value_type;\n        let output_reg = self.lower_ctx.alloc_tmp(ret_ty).only_reg().unwrap();\n\n        emit_vm_call(\n            self.lower_ctx,\n            &self.backend.flags,\n            &self.backend.triple,\n            libcall.clone(),\n            &[a, b, c],\n            &[output_reg],\n        )\n        .expect(\"Failed to emit LibCall\");\n\n        output_reg.to_reg()\n    }\n\n    #[inline]\n    fn single_target(&mut self, targets: &MachLabelSlice) -> Option<MachLabel> {\n        if targets.len() == 1 {\n            Some(targets[0])\n        } else {\n            None\n        }\n    }\n\n    #[inline]\n    fn two_targets(&mut self, targets: &MachLabelSlice) -> Option<(MachLabel, MachLabel)> {\n        if targets.len() == 2 {\n            Some((targets[0], targets[1]))\n        } else {\n            None\n        }\n    }\n\n    #[inline]\n    fn jump_table_targets(\n        &mut self,\n        targets: &MachLabelSlice,\n    ) -> Option<(MachLabel, BoxVecMachLabel)> {\n        if targets.is_empty() {\n            return None;\n        }\n\n        let default_label = targets[0];\n        let jt_targets = Box::new(SmallVec::from(&targets[1..]));\n        Some((default_label, jt_targets))\n    }\n\n    #[inline]\n    fn jump_table_size(&mut self, targets: &BoxVecMachLabel) -> u32 {\n        targets.len() as u32\n    }\n\n    #[inline]\n    fn vconst_all_ones_or_all_zeros(&mut self, constant: Constant) -> Option<()> {\n        let const_data = self.lower_ctx.get_constant_data(constant);\n        if const_data.iter().all(|&b| b == 0 || b == 0xFF) {\n            return Some(());\n        }\n        None\n    }\n\n    #[inline]\n    fn fcvt_uint_mask_const(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&UINT_MASK))\n    }\n\n    #[inline]\n    fn fcvt_uint_mask_high_const(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&UINT_MASK_HIGH))\n    }\n\n    #[inline]\n    fn iadd_pairwise_mul_const_16(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&IADD_PAIRWISE_MUL_CONST_16))\n    }\n\n    #[inline]\n    fn iadd_pairwise_mul_const_32(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&IADD_PAIRWISE_MUL_CONST_32))\n    }\n\n    #[inline]\n    fn iadd_pairwise_xor_const_32(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&IADD_PAIRWISE_XOR_CONST_32))\n    }\n\n    #[inline]\n    fn iadd_pairwise_addd_const_32(&mut self) -> VCodeConstant {\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&IADD_PAIRWISE_ADDD_CONST_32))\n    }\n\n    #[inline]\n    fn snarrow_umax_mask(&mut self) -> VCodeConstant {\n        // 2147483647.0 is equivalent to 0x41DFFFFFFFC00000\n        static UMAX_MASK: [u8; 16] = [\n            0x00, 0x00, 0xC0, 0xFF, 0xFF, 0xFF, 0xDF, 0x41, 0x00, 0x00, 0xC0, 0xFF, 0xFF, 0xFF,\n            0xDF, 0x41,\n        ];\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&UMAX_MASK))\n    }\n\n    #[inline]\n    fn shuffle_0_31_mask(&mut self, mask: &VecMask) -> VCodeConstant {\n        let mask = mask\n            .iter()\n            .map(|&b| if b > 15 { b.wrapping_sub(16) } else { b })\n            .map(|b| if b > 15 { 0b10000000 } else { b })\n            .collect();\n        self.lower_ctx\n            .use_constant(VCodeConstantData::Generated(mask))\n    }\n\n    #[inline]\n    fn shuffle_0_15_mask(&mut self, mask: &VecMask) -> VCodeConstant {\n        let mask = mask\n            .iter()\n            .map(|&b| if b > 15 { 0b10000000 } else { b })\n            .collect();\n        self.lower_ctx\n            .use_constant(VCodeConstantData::Generated(mask))\n    }\n\n    #[inline]\n    fn shuffle_16_31_mask(&mut self, mask: &VecMask) -> VCodeConstant {\n        let mask = mask\n            .iter()\n            .map(|&b| b.wrapping_sub(16))\n            .map(|b| if b > 15 { 0b10000000 } else { b })\n            .collect();\n        self.lower_ctx\n            .use_constant(VCodeConstantData::Generated(mask))\n    }\n\n    #[inline]\n    fn perm_from_mask_with_zeros(\n        &mut self,\n        mask: &VecMask,\n    ) -> Option<(VCodeConstant, VCodeConstant)> {\n        if !mask.iter().any(|&b| b > 31) {\n            return None;\n        }\n\n        let zeros = mask\n            .iter()\n            .map(|&b| if b > 31 { 0x00 } else { 0xff })\n            .collect();\n\n        Some((\n            self.perm_from_mask(mask),\n            self.lower_ctx\n                .use_constant(VCodeConstantData::Generated(zeros)),\n        ))\n    }\n\n    #[inline]\n    fn perm_from_mask(&mut self, mask: &VecMask) -> VCodeConstant {\n        let mask = mask.iter().cloned().collect();\n        self.lower_ctx\n            .use_constant(VCodeConstantData::Generated(mask))\n    }\n\n    #[inline]\n    fn swizzle_zero_mask(&mut self) -> VCodeConstant {\n        static ZERO_MASK_VALUE: [u8; 16] = [0x70; 16];\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&ZERO_MASK_VALUE))\n    }\n\n    #[inline]\n    fn sqmul_round_sat_mask(&mut self) -> VCodeConstant {\n        static SAT_MASK: [u8; 16] = [\n            0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80,\n            0x00, 0x80,\n        ];\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&SAT_MASK))\n    }\n\n    #[inline]\n    fn uunarrow_umax_mask(&mut self) -> VCodeConstant {\n        // 4294967295.0 is equivalent to 0x41EFFFFFFFE00000\n        static UMAX_MASK: [u8; 16] = [\n            0x00, 0x00, 0xE0, 0xFF, 0xFF, 0xFF, 0xEF, 0x41, 0x00, 0x00, 0xE0, 0xFF, 0xFF, 0xFF,\n            0xEF, 0x41,\n        ];\n\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&UMAX_MASK))\n    }\n\n    #[inline]\n    fn uunarrow_uint_mask(&mut self) -> VCodeConstant {\n        static UINT_MASK: [u8; 16] = [\n            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n            0x30, 0x43,\n        ];\n\n        self.lower_ctx\n            .use_constant(VCodeConstantData::WellKnown(&UINT_MASK))\n    }\n\n    fn emit_div_or_rem(\n        &mut self,\n        kind: &DivOrRemKind,\n        ty: Type,\n        dst: WritableGpr,\n        dividend: Gpr,\n        divisor: Gpr,\n    ) {\n        let is_div = kind.is_div();\n        let size = OperandSize::from_ty(ty);\n\n        let dst_quotient = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n        let dst_remainder = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n\n        // Always do explicit checks for `srem`: otherwise, INT_MIN % -1 is not handled properly.\n        if self.backend.flags.avoid_div_traps() || *kind == DivOrRemKind::SignedRem {\n            // A vcode meta-instruction is used to lower the inline checks, since they embed\n            // pc-relative offsets that must not change, thus requiring regalloc to not\n            // interfere by introducing spills and reloads.\n            let tmp = if *kind == DivOrRemKind::SignedDiv && size == OperandSize::Size64 {\n                Some(self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap())\n            } else {\n                None\n            };\n            let dividend_hi = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n            self.lower_ctx.emit(MInst::AluConstOp {\n                op: AluRmiROpcode::Xor,\n                size: OperandSize::Size32,\n                dst: WritableGpr::from_reg(Gpr::new(dividend_hi.to_reg()).unwrap()),\n            });\n            self.lower_ctx.emit(MInst::checked_div_or_rem_seq(\n                kind.clone(),\n                size,\n                divisor.to_reg(),\n                Gpr::new(dividend.to_reg()).unwrap(),\n                Gpr::new(dividend_hi.to_reg()).unwrap(),\n                WritableGpr::from_reg(Gpr::new(dst_quotient.to_reg()).unwrap()),\n                WritableGpr::from_reg(Gpr::new(dst_remainder.to_reg()).unwrap()),\n                tmp,\n            ));\n        } else {\n            // We don't want more than one trap record for a single instruction,\n            // so let's not allow the \"mem\" case (load-op merging) here; force\n            // divisor into a register instead.\n            let divisor = RegMem::reg(divisor.to_reg());\n\n            let dividend_hi = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n\n            // Fill in the high parts:\n            let dividend_lo = if kind.is_signed() && ty == types::I8 {\n                let dividend_lo = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n                // 8-bit div takes its dividend in only the `lo` reg.\n                self.lower_ctx.emit(MInst::sign_extend_data(\n                    size,\n                    Gpr::new(dividend.to_reg()).unwrap(),\n                    WritableGpr::from_reg(Gpr::new(dividend_lo.to_reg()).unwrap()),\n                ));\n                // `dividend_hi` is not used by the Div below, so we\n                // don't def it here.\n\n                dividend_lo.to_reg()\n            } else if kind.is_signed() {\n                // 16-bit and higher div takes its operand in hi:lo\n                // with half in each (64:64, 32:32 or 16:16).\n                self.lower_ctx.emit(MInst::sign_extend_data(\n                    size,\n                    Gpr::new(dividend.to_reg()).unwrap(),\n                    WritableGpr::from_reg(Gpr::new(dividend_hi.to_reg()).unwrap()),\n                ));\n\n                dividend.to_reg()\n            } else if ty == types::I8 {\n                let dividend_lo = self.lower_ctx.alloc_tmp(types::I64).only_reg().unwrap();\n                self.lower_ctx.emit(MInst::movzx_rm_r(\n                    ExtMode::BL,\n                    RegMem::reg(dividend.to_reg()),\n                    dividend_lo,\n                ));\n\n                dividend_lo.to_reg()\n            } else {\n                // zero for unsigned opcodes.\n                self.lower_ctx\n                    .emit(MInst::imm(OperandSize::Size64, 0, dividend_hi));\n\n                dividend.to_reg()\n            };\n\n            // Emit the actual idiv.\n            self.lower_ctx.emit(MInst::div(\n                size,\n                kind.is_signed(),\n                divisor,\n                Gpr::new(dividend_lo).unwrap(),\n                Gpr::new(dividend_hi.to_reg()).unwrap(),\n                WritableGpr::from_reg(Gpr::new(dst_quotient.to_reg()).unwrap()),\n                WritableGpr::from_reg(Gpr::new(dst_remainder.to_reg()).unwrap()),\n            ));\n        }\n\n        // Move the result back into the destination reg.\n        if is_div {\n            // The quotient is in rax.\n            self.lower_ctx.emit(MInst::gen_move(\n                dst.to_writable_reg(),\n                dst_quotient.to_reg(),\n                ty,\n            ));\n        } else {\n            if size == OperandSize::Size8 {\n                let tmp = self.temp_writable_reg(ty);\n                // The remainder is in AH. Right-shift by 8 bits then move from rax.\n                self.lower_ctx.emit(MInst::shift_r(\n                    OperandSize::Size64,\n                    ShiftKind::ShiftRightLogical,\n                    Imm8Gpr::new(Imm8Reg::Imm8 { imm: 8 }).unwrap(),\n                    dst_quotient.to_reg(),\n                    tmp,\n                ));\n                self.lower_ctx\n                    .emit(MInst::gen_move(dst.to_writable_reg(), tmp.to_reg(), ty));\n            } else {\n                // The remainder is in rdx.\n                self.lower_ctx.emit(MInst::gen_move(\n                    dst.to_writable_reg(),\n                    dst_remainder.to_reg(),\n                    ty,\n                ));\n            }\n        }\n    }\n\n    fn xmm_mem_to_xmm_mem_aligned(&mut self, arg: &XmmMem) -> XmmMemAligned {\n        match XmmMemAligned::new(arg.clone().into()) {\n            Some(aligned) => aligned,\n            None => match arg.clone().into() {\n                RegMem::Mem { addr } => self.load_xmm_unaligned(addr).into(),\n                _ => unreachable!(),\n            },\n        }\n    }\n\n    fn xmm_mem_imm_to_xmm_mem_aligned_imm(&mut self, arg: &XmmMemImm) -> XmmMemAlignedImm {\n        match XmmMemAlignedImm::new(arg.clone().into()) {\n            Some(aligned) => aligned,\n            None => match arg.clone().into() {\n                RegMemImm::Mem { addr } => self.load_xmm_unaligned(addr).into(),\n                _ => unreachable!(),\n            },\n        }\n    }\n}\n\nimpl IsleContext<'_, '_, MInst, X64Backend> {\n    isle_prelude_method_helpers!(X64Caller);\n\n    fn load_xmm_unaligned(&mut self, addr: SyntheticAmode) -> Xmm {\n        let tmp = self.lower_ctx.alloc_tmp(types::F32X4).only_reg().unwrap();\n        self.lower_ctx.emit(MInst::XmmUnaryRmRUnaligned {\n            op: SseOpcode::Movdqu,\n            src: XmmMem::new(RegMem::mem(addr)).unwrap(),\n            dst: Writable::from_reg(Xmm::new(tmp.to_reg()).unwrap()),\n        });\n        Xmm::new(tmp.to_reg()).unwrap()\n    }\n}\n\n// Since x64 doesn't have 8x16 shifts and we must use a 16x8 shift instead, we\n// need to fix up the bits that migrate from one half of the lane to the\n// other. Each 16-byte mask is indexed by the shift amount: e.g. if we shift\n// right by 0 (no movement), we want to retain all the bits so we mask with\n// `0xff`; if we shift right by 1, we want to retain all bits except the MSB so\n// we mask with `0x7f`; etc.\n\n#[rustfmt::skip] // Preserve 16 bytes (i.e. one mask) per row.\nconst I8X16_ISHL_MASKS: [u8; 128] = [\n    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n    0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe, 0xfe,\n    0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc, 0xfc,\n    0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8, 0xf8,\n    0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0, 0xf0,\n    0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0, 0xe0,\n    0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0, 0xc0,\n    0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80,\n];\n\n#[rustfmt::skip] // Preserve 16 bytes (i.e. one mask) per row.\nconst I8X16_USHR_MASKS: [u8; 128] = [\n    0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n    0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f, 0x7f,\n    0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f, 0x3f,\n    0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f, 0x1f,\n    0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f, 0x0f,\n    0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07, 0x07,\n    0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03, 0x03,\n    0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01, 0x01,\n];\n\n/// Number of bits set in a given nibble (4-bit value). Used in the\n/// vector implementation of popcount.\n#[rustfmt::skip] // Preserve 4x4 layout.\nconst POPCOUNT_4BIT_TABLE: [u8; 16] = [\n    0x00, 0x01, 0x01, 0x02,\n    0x01, 0x02, 0x02, 0x03,\n    0x01, 0x02, 0x02, 0x03,\n    0x02, 0x03, 0x03, 0x04,\n];\n\nconst POPCOUNT_LOW_MASK: [u8; 16] = [0x0f; 16];\n\n#[inline]\nfn to_simm32(constant: i64) -> Option<GprMemImm> {\n    if constant == ((constant << 32) >> 32) {\n        Some(\n            GprMemImm::new(RegMemImm::Imm {\n                simm32: constant as u32,\n            })\n            .unwrap(),\n        )\n    } else {\n        None\n    }\n}\n\nconst UINT_MASK: [u8; 16] = [\n    0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n];\n\nconst UINT_MASK_HIGH: [u8; 16] = [\n    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x30, 0x43, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x30, 0x43,\n];\n\nconst IADD_PAIRWISE_MUL_CONST_16: [u8; 16] = [0x01; 16];\n\nconst IADD_PAIRWISE_MUL_CONST_32: [u8; 16] = [\n    0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00, 0x01, 0x00,\n];\n\nconst IADD_PAIRWISE_XOR_CONST_32: [u8; 16] = [\n    0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80, 0x00, 0x80,\n];\n\nconst IADD_PAIRWISE_ADDD_CONST_32: [u8; 16] = [\n    0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00,\n];\n", "test compile precise-output\nset enable_simd\ntarget x86_64 has_sse3 has_ssse3 has_sse41\n\n;; shuffle\n\nfunction %shuffle_different_ssa_values() -> i8x16 {\nblock0:\n    v0 = vconst.i8x16 0x00\n    v1 = vconst.i8x16 0x01\n    v2 = shuffle v0, v1, 0x11000000000000000000000000000000     ;; pick the second lane of v1, the rest use the first lane of v0\n    return v2\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movdqu  const(3), %xmm0\n;   movdqu  const(2), %xmm2\n;   pshufb  %xmm0, const(0), %xmm0\n;   pshufb  %xmm2, const(1), %xmm2\n;   por     %xmm0, %xmm2, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movdqu 0x54(%rip), %xmm0\n;   movdqu 0x3c(%rip), %xmm2\n;   pshufb 0x13(%rip), %xmm0\n;   pshufb 0x1a(%rip), %xmm2\n;   por %xmm2, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb $0x80, -0x7f7f7f80(%rax)\n;   addb $0x80, -0x7f7f7f80(%rax)\n;   addb $0, 0x101(%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n\nfunction %shuffle_same_ssa_value() -> i8x16 {\nblock0:\n    v1 = vconst.i8x16 0x01\n    v2 = shuffle v1, v1, 0x13000000000000000000000000000000     ;; pick the fourth lane of v1 and the rest from the first lane of v1\n    return v2\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movdqu  const(1), %xmm0\n;   pshufb  %xmm0, const(0), %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movdqu 0x24(%rip), %xmm0\n;   pshufb 0xb(%rip), %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rbx)\n;   addl %eax, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n\nfunction %swizzle() -> i8x16 {\nblock0:\n    v0 = vconst.i8x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]\n    v1 = vconst.i8x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]\n    v2 = swizzle v0, v1\n    return v2\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movdqu  const(1), %xmm0\n;   movdqu  const(1), %xmm1\n;   paddusb %xmm1, const(0), %xmm1\n;   pshufb  %xmm0, %xmm1, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movdqu 0x34(%rip), %xmm0\n;   movdqu 0x2c(%rip), %xmm1\n;   paddusb 0x14(%rip), %xmm1\n;   pshufb %xmm1, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   addb %al, (%rax)\n;   jo 0xa2\n;   jo 0xa4\n;   jo 0xa6\n;   jo 0xa8\n;   jo 0xaa\n;   jo 0xac\n;   jo 0xae\n;   jo 0xb0\n;   addb %al, (%rcx)\n;   addb (%rbx), %al\n;   addb $5, %al\n\nfunction %splat_i8(i8) -> i8x16 {\nblock0(v0: i8):\n    v1 = splat.i8x16 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   uninit  %xmm0\n;   pinsrb  $0, %xmm0, %rdi, %xmm0\n;   pxor    %xmm6, %xmm6, %xmm6\n;   pshufb  %xmm0, %xmm6, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   pinsrb $0, %edi, %xmm0\n;   pxor %xmm6, %xmm6\n;   pshufb %xmm6, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %splat_i16() -> i16x8 {\nblock0:\n    v0 = iconst.i16 -1\n    v1 = splat.i16x8 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movl    $-1, %esi\n;   uninit  %xmm4\n;   pinsrw  $0, %xmm4, %rsi, %xmm4\n;   pinsrw  $1, %xmm4, %rsi, %xmm4\n;   pshufd  $0, %xmm4, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movl $0xffffffff, %esi\n;   pinsrw $0, %esi, %xmm4\n;   pinsrw $1, %esi, %xmm4\n;   pshufd $0, %xmm4, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %splat_i32(i32) -> i32x4 {\nblock0(v0: i32):\n    v1 = splat.i32x4 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   uninit  %xmm3\n;   pinsrd  $0, %xmm3, %rdi, %xmm3\n;   pshufd  $0, %xmm3, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   pinsrd $0, %edi, %xmm3\n;   pshufd $0, %xmm3, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %splat_f64(f64) -> f64x2 {\nblock0(v0: f64):\n    v1 = splat.f64x2 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movdqa  %xmm0, %xmm5\n;   uninit  %xmm0\n;   movdqa  %xmm5, %xmm6\n;   movsd   %xmm0, %xmm6, %xmm0\n;   movlhps %xmm0, %xmm6, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movdqa %xmm0, %xmm5\n;   movdqa %xmm5, %xmm6\n;   movsd %xmm6, %xmm0\n;   movlhps %xmm6, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %load32_zero_coalesced(i64) -> i32x4 {\nblock0(v0: i64):\n    v1 = load.i32 v0\n    v2 = scalar_to_vector.i32x4 v1\n    return v2\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movss   0(%rdi), %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movss (%rdi), %xmm0 ; trap: heap_oob\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %load32_zero_int(i32) -> i32x4 {\nblock0(v0: i32):\n    v1 = scalar_to_vector.i32x4 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movd    %edi, %xmm0\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movd %edi, %xmm0\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\nfunction %load32_zero_float(f32) -> f32x4 {\nblock0(v0: f32):\n    v1 = scalar_to_vector.f32x4 v0\n    return v1\n}\n\n; VCode:\n;   pushq   %rbp\n;   movq    %rsp, %rbp\n; block0:\n;   movq    %rbp, %rsp\n;   popq    %rbp\n;   ret\n; \n; Disassembled:\n; block0: ; offset 0x0\n;   pushq %rbp\n;   movq %rsp, %rbp\n; block1: ; offset 0x4\n;   movq %rbp, %rsp\n;   popq %rbp\n;   retq\n\n", "test interpret\ntest run\ntarget aarch64\ntarget s390x\nset enable_simd\ntarget x86_64 has_sse3 has_ssse3 has_sse41\ntarget x86_64 has_sse3 has_ssse3 has_sse41 has_avx512vl has_avx512vbmi\n\nfunction %shuffle_i8x16(i8x16, i8x16) -> i8x16 {\nblock0(v0: i8x16, v1: i8x16):\n    v2 = shuffle v0, v1, [3 0 31 26 4 6 12 11 23 13 24 4 2 15 17 5]\n    return v2\n}\n; run: %shuffle_i8x16([1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16], [17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32]) == [4 1 32 27 5 7 13 12 24 14 25 5 3 16 18 6]\n\nfunction %shuffle_zeros(i8x16, i8x16) -> i8x16 {\nblock0(v0: i8x16, v1: i8x16):\n    v2 = shuffle v0, v1, [3 0 32 255 4 6 12 11 23 13 24 4 2 97 17 5]\n    return v2\n}\n; run: %shuffle_zeros([1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16], [17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32]) == [4 1 0 0 5 7 13 12 24 14 25 5 3 0 18 6]\n\nfunction %shuffle1(i8x16) -> i8x16 {\nblock0(v0: i8x16):\n    v1 = shuffle v0, v0, [8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n    return v1\n}\n; run: %shuffle1([0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]) == [8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7]\n"], "filenames": ["cranelift/codegen/src/isa/x64/lower/isle.rs", "cranelift/filetests/filetests/isa/x64/simd-lane-access-compile.clif", "cranelift/filetests/filetests/runtests/simd-shuffle.clif"], "buggy_code_start_loc": [755, 104, 21], "buggy_code_end_loc": [756, 105, 21], "fixing_code_start_loc": [755, 104, 22], "fixing_code_end_loc": [756, 106, 29], "type": "CWE-193", "message": "wasmtime is a fast and secure runtime for WebAssembly. Wasmtime's code generation backend, Cranelift, has a bug on x86_64 platforms for the WebAssembly `i8x16.select` instruction which will produce the wrong results when the same operand is provided to the instruction and some of the selected indices are greater than 16. There is an off-by-one error in the calculation of the mask to the `pshufb` instruction which causes incorrect results to be returned if lanes are selected from the second vector. This codegen bug has been fixed in Wasmtiem 6.0.1, 5.0.1, and 4.0.1. Users are recommended to upgrade to these updated versions. If upgrading is not an option for you at this time, you can avoid this miscompilation by disabling the Wasm simd proposal. Additionally the bug is only present on x86_64 hosts. Other platforms such as AArch64 and s390x are not affected.", "other": {"cve": {"id": "CVE-2023-27477", "sourceIdentifier": "security-advisories@github.com", "published": "2023-03-08T21:15:11.010", "lastModified": "2023-03-15T16:56:04.477", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "wasmtime is a fast and secure runtime for WebAssembly. Wasmtime's code generation backend, Cranelift, has a bug on x86_64 platforms for the WebAssembly `i8x16.select` instruction which will produce the wrong results when the same operand is provided to the instruction and some of the selected indices are greater than 16. There is an off-by-one error in the calculation of the mask to the `pshufb` instruction which causes incorrect results to be returned if lanes are selected from the second vector. This codegen bug has been fixed in Wasmtiem 6.0.1, 5.0.1, and 4.0.1. Users are recommended to upgrade to these updated versions. If upgrading is not an option for you at this time, you can avoid this miscompilation by disabling the Wasm simd proposal. Additionally the bug is only present on x86_64 hosts. Other platforms such as AArch64 and s390x are not affected."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 4.3, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 1.4}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:U/C:N/I:L/A:N", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "LOW", "availabilityImpact": "NONE", "baseScore": 3.1, "baseSeverity": "LOW"}, "exploitabilityScore": 1.6, "impactScore": 1.4}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-193"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:cranelift-codegen:*:*:*:*:*:rust:*:*", "versionStartIncluding": "0.84.0", "versionEndExcluding": "0.91.1", "matchCriteriaId": "DBCFE132-FEDA-4A43-AE02-08E150E411D7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:cranelift-codegen:0.92.0:*:*:*:*:rust:*:*", "matchCriteriaId": "7D7B0DE4-0071-4056-A243-D0BB71963F93"}, {"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:cranelift-codegen:0.93.0:*:*:*:*:rust:*:*", "matchCriteriaId": "0EC7D2FC-6423-45AE-8F82-A37F93C17285"}, {"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:wasmtime:*:*:*:*:*:rust:*:*", "versionStartIncluding": "0.37.0", "versionEndExcluding": "4.0.1", "matchCriteriaId": "DD965E87-91DE-4FD2-8AED-37274050D01F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:wasmtime:5.0.0:*:*:*:*:rust:*:*", "matchCriteriaId": "87B4AC93-F648-4E7B-9FC7-53D885110E35"}, {"vulnerable": true, "criteria": "cpe:2.3:a:bytecodealliance:wasmtime:6.0.0:*:*:*:*:rust:*:*", "matchCriteriaId": "A21E7233-1DAD-449F-A48F-05BE3DE8E2E0"}]}]}], "references": [{"url": "https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.wasm_simd", "source": "security-advisories@github.com", "tags": ["Product"]}, {"url": "https://github.com/bytecodealliance/wasmtime/commit/5dc2bbccbb363e474d2c9a1b8e38a89a43bbd5d1", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/bytecodealliance/wasmtime/security/advisories/GHSA-xm67-587q-r2vw", "source": "security-advisories@github.com", "tags": ["Mitigation", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/webassembly/simd", "source": "security-advisories@github.com", "tags": ["Not Applicable"]}, {"url": "https://groups.google.com/a/bytecodealliance.org/g/sec-announce/c/Mov-ItrNJsQ", "source": "security-advisories@github.com", "tags": ["Mailing List", "Release Notes", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/bytecodealliance/wasmtime/commit/5dc2bbccbb363e474d2c9a1b8e38a89a43bbd5d1"}}