{"buggy_code": ["/*\n\n  Broadcom B43 wireless driver\n\n  DMA ringbuffer and descriptor allocation/management\n\n  Copyright (c) 2005, 2006 Michael Buesch <mb@bu3sch.de>\n\n  Some code in this file is derived from the b44.c driver\n  Copyright (C) 2002 David S. Miller\n  Copyright (C) Pekka Pietikainen\n\n  This program is free software; you can redistribute it and/or modify\n  it under the terms of the GNU General Public License as published by\n  the Free Software Foundation; either version 2 of the License, or\n  (at your option) any later version.\n\n  This program is distributed in the hope that it will be useful,\n  but WITHOUT ANY WARRANTY; without even the implied warranty of\n  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n  GNU General Public License for more details.\n\n  You should have received a copy of the GNU General Public License\n  along with this program; see the file COPYING.  If not, write to\n  the Free Software Foundation, Inc., 51 Franklin Steet, Fifth Floor,\n  Boston, MA 02110-1301, USA.\n\n*/\n\n#include \"b43.h\"\n#include \"dma.h\"\n#include \"main.h\"\n#include \"debugfs.h\"\n#include \"xmit.h\"\n\n#include <linux/dma-mapping.h>\n#include <linux/pci.h>\n#include <linux/delay.h>\n#include <linux/skbuff.h>\n#include <linux/etherdevice.h>\n#include <linux/slab.h>\n#include <asm/div64.h>\n\n\n/* Required number of TX DMA slots per TX frame.\n * This currently is 2, because we put the header and the ieee80211 frame\n * into separate slots. */\n#define TX_SLOTS_PER_FRAME\t2\n\n\n/* 32bit DMA ops. */\nstatic\nstruct b43_dmadesc_generic *op32_idx2desc(struct b43_dmaring *ring,\n\t\t\t\t\t  int slot,\n\t\t\t\t\t  struct b43_dmadesc_meta **meta)\n{\n\tstruct b43_dmadesc32 *desc;\n\n\t*meta = &(ring->meta[slot]);\n\tdesc = ring->descbase;\n\tdesc = &(desc[slot]);\n\n\treturn (struct b43_dmadesc_generic *)desc;\n}\n\nstatic void op32_fill_descriptor(struct b43_dmaring *ring,\n\t\t\t\t struct b43_dmadesc_generic *desc,\n\t\t\t\t dma_addr_t dmaaddr, u16 bufsize,\n\t\t\t\t int start, int end, int irq)\n{\n\tstruct b43_dmadesc32 *descbase = ring->descbase;\n\tint slot;\n\tu32 ctl;\n\tu32 addr;\n\tu32 addrext;\n\n\tslot = (int)(&(desc->dma32) - descbase);\n\tB43_WARN_ON(!(slot >= 0 && slot < ring->nr_slots));\n\n\taddr = (u32) (dmaaddr & ~SSB_DMA_TRANSLATION_MASK);\n\taddrext = (u32) (dmaaddr & SSB_DMA_TRANSLATION_MASK)\n\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\taddr |= ssb_dma_translation(ring->dev->dev);\n\tctl = bufsize & B43_DMA32_DCTL_BYTECNT;\n\tif (slot == ring->nr_slots - 1)\n\t\tctl |= B43_DMA32_DCTL_DTABLEEND;\n\tif (start)\n\t\tctl |= B43_DMA32_DCTL_FRAMESTART;\n\tif (end)\n\t\tctl |= B43_DMA32_DCTL_FRAMEEND;\n\tif (irq)\n\t\tctl |= B43_DMA32_DCTL_IRQ;\n\tctl |= (addrext << B43_DMA32_DCTL_ADDREXT_SHIFT)\n\t    & B43_DMA32_DCTL_ADDREXT_MASK;\n\n\tdesc->dma32.control = cpu_to_le32(ctl);\n\tdesc->dma32.address = cpu_to_le32(addr);\n}\n\nstatic void op32_poke_tx(struct b43_dmaring *ring, int slot)\n{\n\tb43_dma_write(ring, B43_DMA32_TXINDEX,\n\t\t      (u32) (slot * sizeof(struct b43_dmadesc32)));\n}\n\nstatic void op32_tx_suspend(struct b43_dmaring *ring)\n{\n\tb43_dma_write(ring, B43_DMA32_TXCTL, b43_dma_read(ring, B43_DMA32_TXCTL)\n\t\t      | B43_DMA32_TXSUSPEND);\n}\n\nstatic void op32_tx_resume(struct b43_dmaring *ring)\n{\n\tb43_dma_write(ring, B43_DMA32_TXCTL, b43_dma_read(ring, B43_DMA32_TXCTL)\n\t\t      & ~B43_DMA32_TXSUSPEND);\n}\n\nstatic int op32_get_current_rxslot(struct b43_dmaring *ring)\n{\n\tu32 val;\n\n\tval = b43_dma_read(ring, B43_DMA32_RXSTATUS);\n\tval &= B43_DMA32_RXDPTR;\n\n\treturn (val / sizeof(struct b43_dmadesc32));\n}\n\nstatic void op32_set_current_rxslot(struct b43_dmaring *ring, int slot)\n{\n\tb43_dma_write(ring, B43_DMA32_RXINDEX,\n\t\t      (u32) (slot * sizeof(struct b43_dmadesc32)));\n}\n\nstatic const struct b43_dma_ops dma32_ops = {\n\t.idx2desc = op32_idx2desc,\n\t.fill_descriptor = op32_fill_descriptor,\n\t.poke_tx = op32_poke_tx,\n\t.tx_suspend = op32_tx_suspend,\n\t.tx_resume = op32_tx_resume,\n\t.get_current_rxslot = op32_get_current_rxslot,\n\t.set_current_rxslot = op32_set_current_rxslot,\n};\n\n/* 64bit DMA ops. */\nstatic\nstruct b43_dmadesc_generic *op64_idx2desc(struct b43_dmaring *ring,\n\t\t\t\t\t  int slot,\n\t\t\t\t\t  struct b43_dmadesc_meta **meta)\n{\n\tstruct b43_dmadesc64 *desc;\n\n\t*meta = &(ring->meta[slot]);\n\tdesc = ring->descbase;\n\tdesc = &(desc[slot]);\n\n\treturn (struct b43_dmadesc_generic *)desc;\n}\n\nstatic void op64_fill_descriptor(struct b43_dmaring *ring,\n\t\t\t\t struct b43_dmadesc_generic *desc,\n\t\t\t\t dma_addr_t dmaaddr, u16 bufsize,\n\t\t\t\t int start, int end, int irq)\n{\n\tstruct b43_dmadesc64 *descbase = ring->descbase;\n\tint slot;\n\tu32 ctl0 = 0, ctl1 = 0;\n\tu32 addrlo, addrhi;\n\tu32 addrext;\n\n\tslot = (int)(&(desc->dma64) - descbase);\n\tB43_WARN_ON(!(slot >= 0 && slot < ring->nr_slots));\n\n\taddrlo = (u32) (dmaaddr & 0xFFFFFFFF);\n\taddrhi = (((u64) dmaaddr >> 32) & ~SSB_DMA_TRANSLATION_MASK);\n\taddrext = (((u64) dmaaddr >> 32) & SSB_DMA_TRANSLATION_MASK)\n\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\taddrhi |= (ssb_dma_translation(ring->dev->dev) << 1);\n\tif (slot == ring->nr_slots - 1)\n\t\tctl0 |= B43_DMA64_DCTL0_DTABLEEND;\n\tif (start)\n\t\tctl0 |= B43_DMA64_DCTL0_FRAMESTART;\n\tif (end)\n\t\tctl0 |= B43_DMA64_DCTL0_FRAMEEND;\n\tif (irq)\n\t\tctl0 |= B43_DMA64_DCTL0_IRQ;\n\tctl1 |= bufsize & B43_DMA64_DCTL1_BYTECNT;\n\tctl1 |= (addrext << B43_DMA64_DCTL1_ADDREXT_SHIFT)\n\t    & B43_DMA64_DCTL1_ADDREXT_MASK;\n\n\tdesc->dma64.control0 = cpu_to_le32(ctl0);\n\tdesc->dma64.control1 = cpu_to_le32(ctl1);\n\tdesc->dma64.address_low = cpu_to_le32(addrlo);\n\tdesc->dma64.address_high = cpu_to_le32(addrhi);\n}\n\nstatic void op64_poke_tx(struct b43_dmaring *ring, int slot)\n{\n\tb43_dma_write(ring, B43_DMA64_TXINDEX,\n\t\t      (u32) (slot * sizeof(struct b43_dmadesc64)));\n}\n\nstatic void op64_tx_suspend(struct b43_dmaring *ring)\n{\n\tb43_dma_write(ring, B43_DMA64_TXCTL, b43_dma_read(ring, B43_DMA64_TXCTL)\n\t\t      | B43_DMA64_TXSUSPEND);\n}\n\nstatic void op64_tx_resume(struct b43_dmaring *ring)\n{\n\tb43_dma_write(ring, B43_DMA64_TXCTL, b43_dma_read(ring, B43_DMA64_TXCTL)\n\t\t      & ~B43_DMA64_TXSUSPEND);\n}\n\nstatic int op64_get_current_rxslot(struct b43_dmaring *ring)\n{\n\tu32 val;\n\n\tval = b43_dma_read(ring, B43_DMA64_RXSTATUS);\n\tval &= B43_DMA64_RXSTATDPTR;\n\n\treturn (val / sizeof(struct b43_dmadesc64));\n}\n\nstatic void op64_set_current_rxslot(struct b43_dmaring *ring, int slot)\n{\n\tb43_dma_write(ring, B43_DMA64_RXINDEX,\n\t\t      (u32) (slot * sizeof(struct b43_dmadesc64)));\n}\n\nstatic const struct b43_dma_ops dma64_ops = {\n\t.idx2desc = op64_idx2desc,\n\t.fill_descriptor = op64_fill_descriptor,\n\t.poke_tx = op64_poke_tx,\n\t.tx_suspend = op64_tx_suspend,\n\t.tx_resume = op64_tx_resume,\n\t.get_current_rxslot = op64_get_current_rxslot,\n\t.set_current_rxslot = op64_set_current_rxslot,\n};\n\nstatic inline int free_slots(struct b43_dmaring *ring)\n{\n\treturn (ring->nr_slots - ring->used_slots);\n}\n\nstatic inline int next_slot(struct b43_dmaring *ring, int slot)\n{\n\tB43_WARN_ON(!(slot >= -1 && slot <= ring->nr_slots - 1));\n\tif (slot == ring->nr_slots - 1)\n\t\treturn 0;\n\treturn slot + 1;\n}\n\nstatic inline int prev_slot(struct b43_dmaring *ring, int slot)\n{\n\tB43_WARN_ON(!(slot >= 0 && slot <= ring->nr_slots - 1));\n\tif (slot == 0)\n\t\treturn ring->nr_slots - 1;\n\treturn slot - 1;\n}\n\n#ifdef CONFIG_B43_DEBUG\nstatic void update_max_used_slots(struct b43_dmaring *ring,\n\t\t\t\t  int current_used_slots)\n{\n\tif (current_used_slots <= ring->max_used_slots)\n\t\treturn;\n\tring->max_used_slots = current_used_slots;\n\tif (b43_debug(ring->dev, B43_DBG_DMAVERBOSE)) {\n\t\tb43dbg(ring->dev->wl,\n\t\t       \"max_used_slots increased to %d on %s ring %d\\n\",\n\t\t       ring->max_used_slots,\n\t\t       ring->tx ? \"TX\" : \"RX\", ring->index);\n\t}\n}\n#else\nstatic inline\n    void update_max_used_slots(struct b43_dmaring *ring, int current_used_slots)\n{\n}\n#endif /* DEBUG */\n\n/* Request a slot for usage. */\nstatic inline int request_slot(struct b43_dmaring *ring)\n{\n\tint slot;\n\n\tB43_WARN_ON(!ring->tx);\n\tB43_WARN_ON(ring->stopped);\n\tB43_WARN_ON(free_slots(ring) == 0);\n\n\tslot = next_slot(ring, ring->current_slot);\n\tring->current_slot = slot;\n\tring->used_slots++;\n\n\tupdate_max_used_slots(ring, ring->used_slots);\n\n\treturn slot;\n}\n\nstatic u16 b43_dmacontroller_base(enum b43_dmatype type, int controller_idx)\n{\n\tstatic const u16 map64[] = {\n\t\tB43_MMIO_DMA64_BASE0,\n\t\tB43_MMIO_DMA64_BASE1,\n\t\tB43_MMIO_DMA64_BASE2,\n\t\tB43_MMIO_DMA64_BASE3,\n\t\tB43_MMIO_DMA64_BASE4,\n\t\tB43_MMIO_DMA64_BASE5,\n\t};\n\tstatic const u16 map32[] = {\n\t\tB43_MMIO_DMA32_BASE0,\n\t\tB43_MMIO_DMA32_BASE1,\n\t\tB43_MMIO_DMA32_BASE2,\n\t\tB43_MMIO_DMA32_BASE3,\n\t\tB43_MMIO_DMA32_BASE4,\n\t\tB43_MMIO_DMA32_BASE5,\n\t};\n\n\tif (type == B43_DMA_64BIT) {\n\t\tB43_WARN_ON(!(controller_idx >= 0 &&\n\t\t\t      controller_idx < ARRAY_SIZE(map64)));\n\t\treturn map64[controller_idx];\n\t}\n\tB43_WARN_ON(!(controller_idx >= 0 &&\n\t\t      controller_idx < ARRAY_SIZE(map32)));\n\treturn map32[controller_idx];\n}\n\nstatic inline\n    dma_addr_t map_descbuffer(struct b43_dmaring *ring,\n\t\t\t      unsigned char *buf, size_t len, int tx)\n{\n\tdma_addr_t dmaaddr;\n\n\tif (tx) {\n\t\tdmaaddr = dma_map_single(ring->dev->dev->dma_dev,\n\t\t\t\t\t buf, len, DMA_TO_DEVICE);\n\t} else {\n\t\tdmaaddr = dma_map_single(ring->dev->dev->dma_dev,\n\t\t\t\t\t buf, len, DMA_FROM_DEVICE);\n\t}\n\n\treturn dmaaddr;\n}\n\nstatic inline\n    void unmap_descbuffer(struct b43_dmaring *ring,\n\t\t\t  dma_addr_t addr, size_t len, int tx)\n{\n\tif (tx) {\n\t\tdma_unmap_single(ring->dev->dev->dma_dev,\n\t\t\t\t addr, len, DMA_TO_DEVICE);\n\t} else {\n\t\tdma_unmap_single(ring->dev->dev->dma_dev,\n\t\t\t\t addr, len, DMA_FROM_DEVICE);\n\t}\n}\n\nstatic inline\n    void sync_descbuffer_for_cpu(struct b43_dmaring *ring,\n\t\t\t\t dma_addr_t addr, size_t len)\n{\n\tB43_WARN_ON(ring->tx);\n\tdma_sync_single_for_cpu(ring->dev->dev->dma_dev,\n\t\t\t\t    addr, len, DMA_FROM_DEVICE);\n}\n\nstatic inline\n    void sync_descbuffer_for_device(struct b43_dmaring *ring,\n\t\t\t\t    dma_addr_t addr, size_t len)\n{\n\tB43_WARN_ON(ring->tx);\n\tdma_sync_single_for_device(ring->dev->dev->dma_dev,\n\t\t\t\t   addr, len, DMA_FROM_DEVICE);\n}\n\nstatic inline\n    void free_descriptor_buffer(struct b43_dmaring *ring,\n\t\t\t\tstruct b43_dmadesc_meta *meta)\n{\n\tif (meta->skb) {\n\t\tdev_kfree_skb_any(meta->skb);\n\t\tmeta->skb = NULL;\n\t}\n}\n\nstatic int alloc_ringmemory(struct b43_dmaring *ring)\n{\n\tgfp_t flags = GFP_KERNEL;\n\n\t/* The specs call for 4K buffers for 30- and 32-bit DMA with 4K\n\t * alignment and 8K buffers for 64-bit DMA with 8K alignment. Testing\n\t * has shown that 4K is sufficient for the latter as long as the buffer\n\t * does not cross an 8K boundary.\n\t *\n\t * For unknown reasons - possibly a hardware error - the BCM4311 rev\n\t * 02, which uses 64-bit DMA, needs the ring buffer in very low memory,\n\t * which accounts for the GFP_DMA flag below.\n\t *\n\t * The flags here must match the flags in free_ringmemory below!\n\t */\n\tif (ring->type == B43_DMA_64BIT)\n\t\tflags |= GFP_DMA;\n\tring->descbase = dma_alloc_coherent(ring->dev->dev->dma_dev,\n\t\t\t\t\t    B43_DMA_RINGMEMSIZE,\n\t\t\t\t\t    &(ring->dmabase), flags);\n\tif (!ring->descbase) {\n\t\tb43err(ring->dev->wl, \"DMA ringmemory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tmemset(ring->descbase, 0, B43_DMA_RINGMEMSIZE);\n\n\treturn 0;\n}\n\nstatic void free_ringmemory(struct b43_dmaring *ring)\n{\n\tdma_free_coherent(ring->dev->dev->dma_dev, B43_DMA_RINGMEMSIZE,\n\t\t\t  ring->descbase, ring->dmabase);\n}\n\n/* Reset the RX DMA channel */\nstatic int b43_dmacontroller_rx_reset(struct b43_wldev *dev, u16 mmio_base,\n\t\t\t\t      enum b43_dmatype type)\n{\n\tint i;\n\tu32 value;\n\tu16 offset;\n\n\tmight_sleep();\n\n\toffset = (type == B43_DMA_64BIT) ? B43_DMA64_RXCTL : B43_DMA32_RXCTL;\n\tb43_write32(dev, mmio_base + offset, 0);\n\tfor (i = 0; i < 10; i++) {\n\t\toffset = (type == B43_DMA_64BIT) ? B43_DMA64_RXSTATUS :\n\t\t\t\t\t\t   B43_DMA32_RXSTATUS;\n\t\tvalue = b43_read32(dev, mmio_base + offset);\n\t\tif (type == B43_DMA_64BIT) {\n\t\t\tvalue &= B43_DMA64_RXSTAT;\n\t\t\tif (value == B43_DMA64_RXSTAT_DISABLED) {\n\t\t\t\ti = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tvalue &= B43_DMA32_RXSTATE;\n\t\t\tif (value == B43_DMA32_RXSTAT_DISABLED) {\n\t\t\t\ti = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tmsleep(1);\n\t}\n\tif (i != -1) {\n\t\tb43err(dev->wl, \"DMA RX reset timed out\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\n/* Reset the TX DMA channel */\nstatic int b43_dmacontroller_tx_reset(struct b43_wldev *dev, u16 mmio_base,\n\t\t\t\t      enum b43_dmatype type)\n{\n\tint i;\n\tu32 value;\n\tu16 offset;\n\n\tmight_sleep();\n\n\tfor (i = 0; i < 10; i++) {\n\t\toffset = (type == B43_DMA_64BIT) ? B43_DMA64_TXSTATUS :\n\t\t\t\t\t\t   B43_DMA32_TXSTATUS;\n\t\tvalue = b43_read32(dev, mmio_base + offset);\n\t\tif (type == B43_DMA_64BIT) {\n\t\t\tvalue &= B43_DMA64_TXSTAT;\n\t\t\tif (value == B43_DMA64_TXSTAT_DISABLED ||\n\t\t\t    value == B43_DMA64_TXSTAT_IDLEWAIT ||\n\t\t\t    value == B43_DMA64_TXSTAT_STOPPED)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tvalue &= B43_DMA32_TXSTATE;\n\t\t\tif (value == B43_DMA32_TXSTAT_DISABLED ||\n\t\t\t    value == B43_DMA32_TXSTAT_IDLEWAIT ||\n\t\t\t    value == B43_DMA32_TXSTAT_STOPPED)\n\t\t\t\tbreak;\n\t\t}\n\t\tmsleep(1);\n\t}\n\toffset = (type == B43_DMA_64BIT) ? B43_DMA64_TXCTL : B43_DMA32_TXCTL;\n\tb43_write32(dev, mmio_base + offset, 0);\n\tfor (i = 0; i < 10; i++) {\n\t\toffset = (type == B43_DMA_64BIT) ? B43_DMA64_TXSTATUS :\n\t\t\t\t\t\t   B43_DMA32_TXSTATUS;\n\t\tvalue = b43_read32(dev, mmio_base + offset);\n\t\tif (type == B43_DMA_64BIT) {\n\t\t\tvalue &= B43_DMA64_TXSTAT;\n\t\t\tif (value == B43_DMA64_TXSTAT_DISABLED) {\n\t\t\t\ti = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tvalue &= B43_DMA32_TXSTATE;\n\t\t\tif (value == B43_DMA32_TXSTAT_DISABLED) {\n\t\t\t\ti = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tmsleep(1);\n\t}\n\tif (i != -1) {\n\t\tb43err(dev->wl, \"DMA TX reset timed out\\n\");\n\t\treturn -ENODEV;\n\t}\n\t/* ensure the reset is completed. */\n\tmsleep(1);\n\n\treturn 0;\n}\n\n/* Check if a DMA mapping address is invalid. */\nstatic bool b43_dma_mapping_error(struct b43_dmaring *ring,\n\t\t\t\t  dma_addr_t addr,\n\t\t\t\t  size_t buffersize, bool dma_to_device)\n{\n\tif (unlikely(dma_mapping_error(ring->dev->dev->dma_dev, addr)))\n\t\treturn 1;\n\n\tswitch (ring->type) {\n\tcase B43_DMA_30BIT:\n\t\tif ((u64)addr + buffersize > (1ULL << 30))\n\t\t\tgoto address_error;\n\t\tbreak;\n\tcase B43_DMA_32BIT:\n\t\tif ((u64)addr + buffersize > (1ULL << 32))\n\t\t\tgoto address_error;\n\t\tbreak;\n\tcase B43_DMA_64BIT:\n\t\t/* Currently we can't have addresses beyond\n\t\t * 64bit in the kernel. */\n\t\tbreak;\n\t}\n\n\t/* The address is OK. */\n\treturn 0;\n\naddress_error:\n\t/* We can't support this address. Unmap it again. */\n\tunmap_descbuffer(ring, addr, buffersize, dma_to_device);\n\n\treturn 1;\n}\n\nstatic bool b43_rx_buffer_is_poisoned(struct b43_dmaring *ring, struct sk_buff *skb)\n{\n\tunsigned char *f = skb->data + ring->frameoffset;\n\n\treturn ((f[0] & f[1] & f[2] & f[3] & f[4] & f[5] & f[6] & f[7]) == 0xFF);\n}\n\nstatic void b43_poison_rx_buffer(struct b43_dmaring *ring, struct sk_buff *skb)\n{\n\tstruct b43_rxhdr_fw4 *rxhdr;\n\tunsigned char *frame;\n\n\t/* This poisons the RX buffer to detect DMA failures. */\n\n\trxhdr = (struct b43_rxhdr_fw4 *)(skb->data);\n\trxhdr->frame_len = 0;\n\n\tB43_WARN_ON(ring->rx_buffersize < ring->frameoffset + sizeof(struct b43_plcp_hdr6) + 2);\n\tframe = skb->data + ring->frameoffset;\n\tmemset(frame, 0xFF, sizeof(struct b43_plcp_hdr6) + 2 /* padding */);\n}\n\nstatic int setup_rx_descbuffer(struct b43_dmaring *ring,\n\t\t\t       struct b43_dmadesc_generic *desc,\n\t\t\t       struct b43_dmadesc_meta *meta, gfp_t gfp_flags)\n{\n\tdma_addr_t dmaaddr;\n\tstruct sk_buff *skb;\n\n\tB43_WARN_ON(ring->tx);\n\n\tskb = __dev_alloc_skb(ring->rx_buffersize, gfp_flags);\n\tif (unlikely(!skb))\n\t\treturn -ENOMEM;\n\tb43_poison_rx_buffer(ring, skb);\n\tdmaaddr = map_descbuffer(ring, skb->data, ring->rx_buffersize, 0);\n\tif (b43_dma_mapping_error(ring, dmaaddr, ring->rx_buffersize, 0)) {\n\t\t/* ugh. try to realloc in zone_dma */\n\t\tgfp_flags |= GFP_DMA;\n\n\t\tdev_kfree_skb_any(skb);\n\n\t\tskb = __dev_alloc_skb(ring->rx_buffersize, gfp_flags);\n\t\tif (unlikely(!skb))\n\t\t\treturn -ENOMEM;\n\t\tb43_poison_rx_buffer(ring, skb);\n\t\tdmaaddr = map_descbuffer(ring, skb->data,\n\t\t\t\t\t ring->rx_buffersize, 0);\n\t\tif (b43_dma_mapping_error(ring, dmaaddr, ring->rx_buffersize, 0)) {\n\t\t\tb43err(ring->dev->wl, \"RX DMA buffer allocation failed\\n\");\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\tmeta->skb = skb;\n\tmeta->dmaaddr = dmaaddr;\n\tring->ops->fill_descriptor(ring, desc, dmaaddr,\n\t\t\t\t   ring->rx_buffersize, 0, 0, 0);\n\n\treturn 0;\n}\n\n/* Allocate the initial descbuffers.\n * This is used for an RX ring only.\n */\nstatic int alloc_initial_descbuffers(struct b43_dmaring *ring)\n{\n\tint i, err = -ENOMEM;\n\tstruct b43_dmadesc_generic *desc;\n\tstruct b43_dmadesc_meta *meta;\n\n\tfor (i = 0; i < ring->nr_slots; i++) {\n\t\tdesc = ring->ops->idx2desc(ring, i, &meta);\n\n\t\terr = setup_rx_descbuffer(ring, desc, meta, GFP_KERNEL);\n\t\tif (err) {\n\t\t\tb43err(ring->dev->wl,\n\t\t\t       \"Failed to allocate initial descbuffers\\n\");\n\t\t\tgoto err_unwind;\n\t\t}\n\t}\n\tmb();\n\tring->used_slots = ring->nr_slots;\n\terr = 0;\n      out:\n\treturn err;\n\n      err_unwind:\n\tfor (i--; i >= 0; i--) {\n\t\tdesc = ring->ops->idx2desc(ring, i, &meta);\n\n\t\tunmap_descbuffer(ring, meta->dmaaddr, ring->rx_buffersize, 0);\n\t\tdev_kfree_skb(meta->skb);\n\t}\n\tgoto out;\n}\n\n/* Do initial setup of the DMA controller.\n * Reset the controller, write the ring busaddress\n * and switch the \"enable\" bit on.\n */\nstatic int dmacontroller_setup(struct b43_dmaring *ring)\n{\n\tint err = 0;\n\tu32 value;\n\tu32 addrext;\n\tu32 trans = ssb_dma_translation(ring->dev->dev);\n\n\tif (ring->tx) {\n\t\tif (ring->type == B43_DMA_64BIT) {\n\t\t\tu64 ringbase = (u64) (ring->dmabase);\n\n\t\t\taddrext = ((ringbase >> 32) & SSB_DMA_TRANSLATION_MASK)\n\t\t\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\t\t\tvalue = B43_DMA64_TXENABLE;\n\t\t\tvalue |= (addrext << B43_DMA64_TXADDREXT_SHIFT)\n\t\t\t    & B43_DMA64_TXADDREXT_MASK;\n\t\t\tb43_dma_write(ring, B43_DMA64_TXCTL, value);\n\t\t\tb43_dma_write(ring, B43_DMA64_TXRINGLO,\n\t\t\t\t      (ringbase & 0xFFFFFFFF));\n\t\t\tb43_dma_write(ring, B43_DMA64_TXRINGHI,\n\t\t\t\t      ((ringbase >> 32) &\n\t\t\t\t       ~SSB_DMA_TRANSLATION_MASK)\n\t\t\t\t      | (trans << 1));\n\t\t} else {\n\t\t\tu32 ringbase = (u32) (ring->dmabase);\n\n\t\t\taddrext = (ringbase & SSB_DMA_TRANSLATION_MASK)\n\t\t\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\t\t\tvalue = B43_DMA32_TXENABLE;\n\t\t\tvalue |= (addrext << B43_DMA32_TXADDREXT_SHIFT)\n\t\t\t    & B43_DMA32_TXADDREXT_MASK;\n\t\t\tb43_dma_write(ring, B43_DMA32_TXCTL, value);\n\t\t\tb43_dma_write(ring, B43_DMA32_TXRING,\n\t\t\t\t      (ringbase & ~SSB_DMA_TRANSLATION_MASK)\n\t\t\t\t      | trans);\n\t\t}\n\t} else {\n\t\terr = alloc_initial_descbuffers(ring);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (ring->type == B43_DMA_64BIT) {\n\t\t\tu64 ringbase = (u64) (ring->dmabase);\n\n\t\t\taddrext = ((ringbase >> 32) & SSB_DMA_TRANSLATION_MASK)\n\t\t\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\t\t\tvalue = (ring->frameoffset << B43_DMA64_RXFROFF_SHIFT);\n\t\t\tvalue |= B43_DMA64_RXENABLE;\n\t\t\tvalue |= (addrext << B43_DMA64_RXADDREXT_SHIFT)\n\t\t\t    & B43_DMA64_RXADDREXT_MASK;\n\t\t\tb43_dma_write(ring, B43_DMA64_RXCTL, value);\n\t\t\tb43_dma_write(ring, B43_DMA64_RXRINGLO,\n\t\t\t\t      (ringbase & 0xFFFFFFFF));\n\t\t\tb43_dma_write(ring, B43_DMA64_RXRINGHI,\n\t\t\t\t      ((ringbase >> 32) &\n\t\t\t\t       ~SSB_DMA_TRANSLATION_MASK)\n\t\t\t\t      | (trans << 1));\n\t\t\tb43_dma_write(ring, B43_DMA64_RXINDEX, ring->nr_slots *\n\t\t\t\t      sizeof(struct b43_dmadesc64));\n\t\t} else {\n\t\t\tu32 ringbase = (u32) (ring->dmabase);\n\n\t\t\taddrext = (ringbase & SSB_DMA_TRANSLATION_MASK)\n\t\t\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\t\t\tvalue = (ring->frameoffset << B43_DMA32_RXFROFF_SHIFT);\n\t\t\tvalue |= B43_DMA32_RXENABLE;\n\t\t\tvalue |= (addrext << B43_DMA32_RXADDREXT_SHIFT)\n\t\t\t    & B43_DMA32_RXADDREXT_MASK;\n\t\t\tb43_dma_write(ring, B43_DMA32_RXCTL, value);\n\t\t\tb43_dma_write(ring, B43_DMA32_RXRING,\n\t\t\t\t      (ringbase & ~SSB_DMA_TRANSLATION_MASK)\n\t\t\t\t      | trans);\n\t\t\tb43_dma_write(ring, B43_DMA32_RXINDEX, ring->nr_slots *\n\t\t\t\t      sizeof(struct b43_dmadesc32));\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/* Shutdown the DMA controller. */\nstatic void dmacontroller_cleanup(struct b43_dmaring *ring)\n{\n\tif (ring->tx) {\n\t\tb43_dmacontroller_tx_reset(ring->dev, ring->mmio_base,\n\t\t\t\t\t   ring->type);\n\t\tif (ring->type == B43_DMA_64BIT) {\n\t\t\tb43_dma_write(ring, B43_DMA64_TXRINGLO, 0);\n\t\t\tb43_dma_write(ring, B43_DMA64_TXRINGHI, 0);\n\t\t} else\n\t\t\tb43_dma_write(ring, B43_DMA32_TXRING, 0);\n\t} else {\n\t\tb43_dmacontroller_rx_reset(ring->dev, ring->mmio_base,\n\t\t\t\t\t   ring->type);\n\t\tif (ring->type == B43_DMA_64BIT) {\n\t\t\tb43_dma_write(ring, B43_DMA64_RXRINGLO, 0);\n\t\t\tb43_dma_write(ring, B43_DMA64_RXRINGHI, 0);\n\t\t} else\n\t\t\tb43_dma_write(ring, B43_DMA32_RXRING, 0);\n\t}\n}\n\nstatic void free_all_descbuffers(struct b43_dmaring *ring)\n{\n\tstruct b43_dmadesc_generic *desc;\n\tstruct b43_dmadesc_meta *meta;\n\tint i;\n\n\tif (!ring->used_slots)\n\t\treturn;\n\tfor (i = 0; i < ring->nr_slots; i++) {\n\t\tdesc = ring->ops->idx2desc(ring, i, &meta);\n\n\t\tif (!meta->skb || b43_dma_ptr_is_poisoned(meta->skb)) {\n\t\t\tB43_WARN_ON(!ring->tx);\n\t\t\tcontinue;\n\t\t}\n\t\tif (ring->tx) {\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr,\n\t\t\t\t\t meta->skb->len, 1);\n\t\t} else {\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr,\n\t\t\t\t\t ring->rx_buffersize, 0);\n\t\t}\n\t\tfree_descriptor_buffer(ring, meta);\n\t}\n}\n\nstatic u64 supported_dma_mask(struct b43_wldev *dev)\n{\n\tu32 tmp;\n\tu16 mmio_base;\n\n\ttmp = b43_read32(dev, SSB_TMSHIGH);\n\tif (tmp & SSB_TMSHIGH_DMA64)\n\t\treturn DMA_BIT_MASK(64);\n\tmmio_base = b43_dmacontroller_base(0, 0);\n\tb43_write32(dev, mmio_base + B43_DMA32_TXCTL, B43_DMA32_TXADDREXT_MASK);\n\ttmp = b43_read32(dev, mmio_base + B43_DMA32_TXCTL);\n\tif (tmp & B43_DMA32_TXADDREXT_MASK)\n\t\treturn DMA_BIT_MASK(32);\n\n\treturn DMA_BIT_MASK(30);\n}\n\nstatic enum b43_dmatype dma_mask_to_engine_type(u64 dmamask)\n{\n\tif (dmamask == DMA_BIT_MASK(30))\n\t\treturn B43_DMA_30BIT;\n\tif (dmamask == DMA_BIT_MASK(32))\n\t\treturn B43_DMA_32BIT;\n\tif (dmamask == DMA_BIT_MASK(64))\n\t\treturn B43_DMA_64BIT;\n\tB43_WARN_ON(1);\n\treturn B43_DMA_30BIT;\n}\n\n/* Main initialization function. */\nstatic\nstruct b43_dmaring *b43_setup_dmaring(struct b43_wldev *dev,\n\t\t\t\t      int controller_index,\n\t\t\t\t      int for_tx,\n\t\t\t\t      enum b43_dmatype type)\n{\n\tstruct b43_dmaring *ring;\n\tint i, err;\n\tdma_addr_t dma_test;\n\n\tring = kzalloc(sizeof(*ring), GFP_KERNEL);\n\tif (!ring)\n\t\tgoto out;\n\n\tring->nr_slots = B43_RXRING_SLOTS;\n\tif (for_tx)\n\t\tring->nr_slots = B43_TXRING_SLOTS;\n\n\tring->meta = kcalloc(ring->nr_slots, sizeof(struct b43_dmadesc_meta),\n\t\t\t     GFP_KERNEL);\n\tif (!ring->meta)\n\t\tgoto err_kfree_ring;\n\tfor (i = 0; i < ring->nr_slots; i++)\n\t\tring->meta->skb = B43_DMA_PTR_POISON;\n\n\tring->type = type;\n\tring->dev = dev;\n\tring->mmio_base = b43_dmacontroller_base(type, controller_index);\n\tring->index = controller_index;\n\tif (type == B43_DMA_64BIT)\n\t\tring->ops = &dma64_ops;\n\telse\n\t\tring->ops = &dma32_ops;\n\tif (for_tx) {\n\t\tring->tx = 1;\n\t\tring->current_slot = -1;\n\t} else {\n\t\tif (ring->index == 0) {\n\t\t\tring->rx_buffersize = B43_DMA0_RX_BUFFERSIZE;\n\t\t\tring->frameoffset = B43_DMA0_RX_FRAMEOFFSET;\n\t\t} else\n\t\t\tB43_WARN_ON(1);\n\t}\n#ifdef CONFIG_B43_DEBUG\n\tring->last_injected_overflow = jiffies;\n#endif\n\n\tif (for_tx) {\n\t\t/* Assumption: B43_TXRING_SLOTS can be divided by TX_SLOTS_PER_FRAME */\n\t\tBUILD_BUG_ON(B43_TXRING_SLOTS % TX_SLOTS_PER_FRAME != 0);\n\n\t\tring->txhdr_cache = kcalloc(ring->nr_slots / TX_SLOTS_PER_FRAME,\n\t\t\t\t\t    b43_txhdr_size(dev),\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!ring->txhdr_cache)\n\t\t\tgoto err_kfree_meta;\n\n\t\t/* test for ability to dma to txhdr_cache */\n\t\tdma_test = dma_map_single(dev->dev->dma_dev,\n\t\t\t\t\t  ring->txhdr_cache,\n\t\t\t\t\t  b43_txhdr_size(dev),\n\t\t\t\t\t  DMA_TO_DEVICE);\n\n\t\tif (b43_dma_mapping_error(ring, dma_test,\n\t\t\t\t\t  b43_txhdr_size(dev), 1)) {\n\t\t\t/* ugh realloc */\n\t\t\tkfree(ring->txhdr_cache);\n\t\t\tring->txhdr_cache = kcalloc(ring->nr_slots / TX_SLOTS_PER_FRAME,\n\t\t\t\t\t\t    b43_txhdr_size(dev),\n\t\t\t\t\t\t    GFP_KERNEL | GFP_DMA);\n\t\t\tif (!ring->txhdr_cache)\n\t\t\t\tgoto err_kfree_meta;\n\n\t\t\tdma_test = dma_map_single(dev->dev->dma_dev,\n\t\t\t\t\t\t  ring->txhdr_cache,\n\t\t\t\t\t\t  b43_txhdr_size(dev),\n\t\t\t\t\t\t  DMA_TO_DEVICE);\n\n\t\t\tif (b43_dma_mapping_error(ring, dma_test,\n\t\t\t\t\t\t  b43_txhdr_size(dev), 1)) {\n\n\t\t\t\tb43err(dev->wl,\n\t\t\t\t       \"TXHDR DMA allocation failed\\n\");\n\t\t\t\tgoto err_kfree_txhdr_cache;\n\t\t\t}\n\t\t}\n\n\t\tdma_unmap_single(dev->dev->dma_dev,\n\t\t\t\t dma_test, b43_txhdr_size(dev),\n\t\t\t\t DMA_TO_DEVICE);\n\t}\n\n\terr = alloc_ringmemory(ring);\n\tif (err)\n\t\tgoto err_kfree_txhdr_cache;\n\terr = dmacontroller_setup(ring);\n\tif (err)\n\t\tgoto err_free_ringmemory;\n\n      out:\n\treturn ring;\n\n      err_free_ringmemory:\n\tfree_ringmemory(ring);\n      err_kfree_txhdr_cache:\n\tkfree(ring->txhdr_cache);\n      err_kfree_meta:\n\tkfree(ring->meta);\n      err_kfree_ring:\n\tkfree(ring);\n\tring = NULL;\n\tgoto out;\n}\n\n#define divide(a, b)\t({\t\\\n\ttypeof(a) __a = a;\t\\\n\tdo_div(__a, b);\t\t\\\n\t__a;\t\t\t\\\n  })\n\n#define modulo(a, b)\t({\t\\\n\ttypeof(a) __a = a;\t\\\n\tdo_div(__a, b);\t\t\\\n  })\n\n/* Main cleanup function. */\nstatic void b43_destroy_dmaring(struct b43_dmaring *ring,\n\t\t\t\tconst char *ringname)\n{\n\tif (!ring)\n\t\treturn;\n\n#ifdef CONFIG_B43_DEBUG\n\t{\n\t\t/* Print some statistics. */\n\t\tu64 failed_packets = ring->nr_failed_tx_packets;\n\t\tu64 succeed_packets = ring->nr_succeed_tx_packets;\n\t\tu64 nr_packets = failed_packets + succeed_packets;\n\t\tu64 permille_failed = 0, average_tries = 0;\n\n\t\tif (nr_packets)\n\t\t\tpermille_failed = divide(failed_packets * 1000, nr_packets);\n\t\tif (nr_packets)\n\t\t\taverage_tries = divide(ring->nr_total_packet_tries * 100, nr_packets);\n\n\t\tb43dbg(ring->dev->wl, \"DMA-%u %s: \"\n\t\t       \"Used slots %d/%d, Failed frames %llu/%llu = %llu.%01llu%%, \"\n\t\t       \"Average tries %llu.%02llu\\n\",\n\t\t       (unsigned int)(ring->type), ringname,\n\t\t       ring->max_used_slots,\n\t\t       ring->nr_slots,\n\t\t       (unsigned long long)failed_packets,\n\t\t       (unsigned long long)nr_packets,\n\t\t       (unsigned long long)divide(permille_failed, 10),\n\t\t       (unsigned long long)modulo(permille_failed, 10),\n\t\t       (unsigned long long)divide(average_tries, 100),\n\t\t       (unsigned long long)modulo(average_tries, 100));\n\t}\n#endif /* DEBUG */\n\n\t/* Device IRQs are disabled prior entering this function,\n\t * so no need to take care of concurrency with rx handler stuff.\n\t */\n\tdmacontroller_cleanup(ring);\n\tfree_all_descbuffers(ring);\n\tfree_ringmemory(ring);\n\n\tkfree(ring->txhdr_cache);\n\tkfree(ring->meta);\n\tkfree(ring);\n}\n\n#define destroy_ring(dma, ring) do {\t\t\t\t\\\n\tb43_destroy_dmaring((dma)->ring, __stringify(ring));\t\\\n\t(dma)->ring = NULL;\t\t\t\t\t\\\n    } while (0)\n\nvoid b43_dma_free(struct b43_wldev *dev)\n{\n\tstruct b43_dma *dma;\n\n\tif (b43_using_pio_transfers(dev))\n\t\treturn;\n\tdma = &dev->dma;\n\n\tdestroy_ring(dma, rx_ring);\n\tdestroy_ring(dma, tx_ring_AC_BK);\n\tdestroy_ring(dma, tx_ring_AC_BE);\n\tdestroy_ring(dma, tx_ring_AC_VI);\n\tdestroy_ring(dma, tx_ring_AC_VO);\n\tdestroy_ring(dma, tx_ring_mcast);\n}\n\nstatic int b43_dma_set_mask(struct b43_wldev *dev, u64 mask)\n{\n\tu64 orig_mask = mask;\n\tbool fallback = 0;\n\tint err;\n\n\t/* Try to set the DMA mask. If it fails, try falling back to a\n\t * lower mask, as we can always also support a lower one. */\n\twhile (1) {\n\t\terr = dma_set_mask(dev->dev->dma_dev, mask);\n\t\tif (!err) {\n\t\t\terr = dma_set_coherent_mask(dev->dev->dma_dev, mask);\n\t\t\tif (!err)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (mask == DMA_BIT_MASK(64)) {\n\t\t\tmask = DMA_BIT_MASK(32);\n\t\t\tfallback = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (mask == DMA_BIT_MASK(32)) {\n\t\t\tmask = DMA_BIT_MASK(30);\n\t\t\tfallback = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tb43err(dev->wl, \"The machine/kernel does not support \"\n\t\t       \"the required %u-bit DMA mask\\n\",\n\t\t       (unsigned int)dma_mask_to_engine_type(orig_mask));\n\t\treturn -EOPNOTSUPP;\n\t}\n\tif (fallback) {\n\t\tb43info(dev->wl, \"DMA mask fallback from %u-bit to %u-bit\\n\",\n\t\t\t(unsigned int)dma_mask_to_engine_type(orig_mask),\n\t\t\t(unsigned int)dma_mask_to_engine_type(mask));\n\t}\n\n\treturn 0;\n}\n\nint b43_dma_init(struct b43_wldev *dev)\n{\n\tstruct b43_dma *dma = &dev->dma;\n\tint err;\n\tu64 dmamask;\n\tenum b43_dmatype type;\n\n\tdmamask = supported_dma_mask(dev);\n\ttype = dma_mask_to_engine_type(dmamask);\n\terr = b43_dma_set_mask(dev, dmamask);\n\tif (err)\n\t\treturn err;\n\n\terr = -ENOMEM;\n\t/* setup TX DMA channels. */\n\tdma->tx_ring_AC_BK = b43_setup_dmaring(dev, 0, 1, type);\n\tif (!dma->tx_ring_AC_BK)\n\t\tgoto out;\n\n\tdma->tx_ring_AC_BE = b43_setup_dmaring(dev, 1, 1, type);\n\tif (!dma->tx_ring_AC_BE)\n\t\tgoto err_destroy_bk;\n\n\tdma->tx_ring_AC_VI = b43_setup_dmaring(dev, 2, 1, type);\n\tif (!dma->tx_ring_AC_VI)\n\t\tgoto err_destroy_be;\n\n\tdma->tx_ring_AC_VO = b43_setup_dmaring(dev, 3, 1, type);\n\tif (!dma->tx_ring_AC_VO)\n\t\tgoto err_destroy_vi;\n\n\tdma->tx_ring_mcast = b43_setup_dmaring(dev, 4, 1, type);\n\tif (!dma->tx_ring_mcast)\n\t\tgoto err_destroy_vo;\n\n\t/* setup RX DMA channel. */\n\tdma->rx_ring = b43_setup_dmaring(dev, 0, 0, type);\n\tif (!dma->rx_ring)\n\t\tgoto err_destroy_mcast;\n\n\t/* No support for the TX status DMA ring. */\n\tB43_WARN_ON(dev->dev->id.revision < 5);\n\n\tb43dbg(dev->wl, \"%u-bit DMA initialized\\n\",\n\t       (unsigned int)type);\n\terr = 0;\nout:\n\treturn err;\n\nerr_destroy_mcast:\n\tdestroy_ring(dma, tx_ring_mcast);\nerr_destroy_vo:\n\tdestroy_ring(dma, tx_ring_AC_VO);\nerr_destroy_vi:\n\tdestroy_ring(dma, tx_ring_AC_VI);\nerr_destroy_be:\n\tdestroy_ring(dma, tx_ring_AC_BE);\nerr_destroy_bk:\n\tdestroy_ring(dma, tx_ring_AC_BK);\n\treturn err;\n}\n\n/* Generate a cookie for the TX header. */\nstatic u16 generate_cookie(struct b43_dmaring *ring, int slot)\n{\n\tu16 cookie;\n\n\t/* Use the upper 4 bits of the cookie as\n\t * DMA controller ID and store the slot number\n\t * in the lower 12 bits.\n\t * Note that the cookie must never be 0, as this\n\t * is a special value used in RX path.\n\t * It can also not be 0xFFFF because that is special\n\t * for multicast frames.\n\t */\n\tcookie = (((u16)ring->index + 1) << 12);\n\tB43_WARN_ON(slot & ~0x0FFF);\n\tcookie |= (u16)slot;\n\n\treturn cookie;\n}\n\n/* Inspect a cookie and find out to which controller/slot it belongs. */\nstatic\nstruct b43_dmaring *parse_cookie(struct b43_wldev *dev, u16 cookie, int *slot)\n{\n\tstruct b43_dma *dma = &dev->dma;\n\tstruct b43_dmaring *ring = NULL;\n\n\tswitch (cookie & 0xF000) {\n\tcase 0x1000:\n\t\tring = dma->tx_ring_AC_BK;\n\t\tbreak;\n\tcase 0x2000:\n\t\tring = dma->tx_ring_AC_BE;\n\t\tbreak;\n\tcase 0x3000:\n\t\tring = dma->tx_ring_AC_VI;\n\t\tbreak;\n\tcase 0x4000:\n\t\tring = dma->tx_ring_AC_VO;\n\t\tbreak;\n\tcase 0x5000:\n\t\tring = dma->tx_ring_mcast;\n\t\tbreak;\n\t}\n\t*slot = (cookie & 0x0FFF);\n\tif (unlikely(!ring || *slot < 0 || *slot >= ring->nr_slots)) {\n\t\tb43dbg(dev->wl, \"TX-status contains \"\n\t\t       \"invalid cookie: 0x%04X\\n\", cookie);\n\t\treturn NULL;\n\t}\n\n\treturn ring;\n}\n\nstatic int dma_tx_fragment(struct b43_dmaring *ring,\n\t\t\t   struct sk_buff *skb)\n{\n\tconst struct b43_dma_ops *ops = ring->ops;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct b43_private_tx_info *priv_info = b43_get_priv_tx_info(info);\n\tu8 *header;\n\tint slot, old_top_slot, old_used_slots;\n\tint err;\n\tstruct b43_dmadesc_generic *desc;\n\tstruct b43_dmadesc_meta *meta;\n\tstruct b43_dmadesc_meta *meta_hdr;\n\tu16 cookie;\n\tsize_t hdrsize = b43_txhdr_size(ring->dev);\n\n\t/* Important note: If the number of used DMA slots per TX frame\n\t * is changed here, the TX_SLOTS_PER_FRAME definition at the top of\n\t * the file has to be updated, too!\n\t */\n\n\told_top_slot = ring->current_slot;\n\told_used_slots = ring->used_slots;\n\n\t/* Get a slot for the header. */\n\tslot = request_slot(ring);\n\tdesc = ops->idx2desc(ring, slot, &meta_hdr);\n\tmemset(meta_hdr, 0, sizeof(*meta_hdr));\n\n\theader = &(ring->txhdr_cache[(slot / TX_SLOTS_PER_FRAME) * hdrsize]);\n\tcookie = generate_cookie(ring, slot);\n\terr = b43_generate_txhdr(ring->dev, header,\n\t\t\t\t skb, info, cookie);\n\tif (unlikely(err)) {\n\t\tring->current_slot = old_top_slot;\n\t\tring->used_slots = old_used_slots;\n\t\treturn err;\n\t}\n\n\tmeta_hdr->dmaaddr = map_descbuffer(ring, (unsigned char *)header,\n\t\t\t\t\t   hdrsize, 1);\n\tif (b43_dma_mapping_error(ring, meta_hdr->dmaaddr, hdrsize, 1)) {\n\t\tring->current_slot = old_top_slot;\n\t\tring->used_slots = old_used_slots;\n\t\treturn -EIO;\n\t}\n\tops->fill_descriptor(ring, desc, meta_hdr->dmaaddr,\n\t\t\t     hdrsize, 1, 0, 0);\n\n\t/* Get a slot for the payload. */\n\tslot = request_slot(ring);\n\tdesc = ops->idx2desc(ring, slot, &meta);\n\tmemset(meta, 0, sizeof(*meta));\n\n\tmeta->skb = skb;\n\tmeta->is_last_fragment = 1;\n\tpriv_info->bouncebuffer = NULL;\n\n\tmeta->dmaaddr = map_descbuffer(ring, skb->data, skb->len, 1);\n\t/* create a bounce buffer in zone_dma on mapping failure. */\n\tif (b43_dma_mapping_error(ring, meta->dmaaddr, skb->len, 1)) {\n\t\tpriv_info->bouncebuffer = kmemdup(skb->data, skb->len,\n\t\t\t\t\t\t  GFP_ATOMIC | GFP_DMA);\n\t\tif (!priv_info->bouncebuffer) {\n\t\t\tring->current_slot = old_top_slot;\n\t\t\tring->used_slots = old_used_slots;\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_unmap_hdr;\n\t\t}\n\n\t\tmeta->dmaaddr = map_descbuffer(ring, priv_info->bouncebuffer, skb->len, 1);\n\t\tif (b43_dma_mapping_error(ring, meta->dmaaddr, skb->len, 1)) {\n\t\t\tkfree(priv_info->bouncebuffer);\n\t\t\tpriv_info->bouncebuffer = NULL;\n\t\t\tring->current_slot = old_top_slot;\n\t\t\tring->used_slots = old_used_slots;\n\t\t\terr = -EIO;\n\t\t\tgoto out_unmap_hdr;\n\t\t}\n\t}\n\n\tops->fill_descriptor(ring, desc, meta->dmaaddr, skb->len, 0, 1, 1);\n\n\tif (info->flags & IEEE80211_TX_CTL_SEND_AFTER_DTIM) {\n\t\t/* Tell the firmware about the cookie of the last\n\t\t * mcast frame, so it can clear the more-data bit in it. */\n\t\tb43_shm_write16(ring->dev, B43_SHM_SHARED,\n\t\t\t\tB43_SHM_SH_MCASTCOOKIE, cookie);\n\t}\n\t/* Now transfer the whole frame. */\n\twmb();\n\tops->poke_tx(ring, next_slot(ring, slot));\n\treturn 0;\n\nout_unmap_hdr:\n\tunmap_descbuffer(ring, meta_hdr->dmaaddr,\n\t\t\t hdrsize, 1);\n\treturn err;\n}\n\nstatic inline int should_inject_overflow(struct b43_dmaring *ring)\n{\n#ifdef CONFIG_B43_DEBUG\n\tif (unlikely(b43_debug(ring->dev, B43_DBG_DMAOVERFLOW))) {\n\t\t/* Check if we should inject another ringbuffer overflow\n\t\t * to test handling of this situation in the stack. */\n\t\tunsigned long next_overflow;\n\n\t\tnext_overflow = ring->last_injected_overflow + HZ;\n\t\tif (time_after(jiffies, next_overflow)) {\n\t\t\tring->last_injected_overflow = jiffies;\n\t\t\tb43dbg(ring->dev->wl,\n\t\t\t       \"Injecting TX ring overflow on \"\n\t\t\t       \"DMA controller %d\\n\", ring->index);\n\t\t\treturn 1;\n\t\t}\n\t}\n#endif /* CONFIG_B43_DEBUG */\n\treturn 0;\n}\n\n/* Static mapping of mac80211's queues (priorities) to b43 DMA rings. */\nstatic struct b43_dmaring *select_ring_by_priority(struct b43_wldev *dev,\n\t\t\t\t\t\t   u8 queue_prio)\n{\n\tstruct b43_dmaring *ring;\n\n\tif (dev->qos_enabled) {\n\t\t/* 0 = highest priority */\n\t\tswitch (queue_prio) {\n\t\tdefault:\n\t\t\tB43_WARN_ON(1);\n\t\t\t/* fallthrough */\n\t\tcase 0:\n\t\t\tring = dev->dma.tx_ring_AC_VO;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tring = dev->dma.tx_ring_AC_VI;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tring = dev->dma.tx_ring_AC_BE;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tring = dev->dma.tx_ring_AC_BK;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\tring = dev->dma.tx_ring_AC_BE;\n\n\treturn ring;\n}\n\nint b43_dma_tx(struct b43_wldev *dev, struct sk_buff *skb)\n{\n\tstruct b43_dmaring *ring;\n\tstruct ieee80211_hdr *hdr;\n\tint err = 0;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\n\thdr = (struct ieee80211_hdr *)skb->data;\n\tif (info->flags & IEEE80211_TX_CTL_SEND_AFTER_DTIM) {\n\t\t/* The multicast ring will be sent after the DTIM */\n\t\tring = dev->dma.tx_ring_mcast;\n\t\t/* Set the more-data bit. Ucode will clear it on\n\t\t * the last frame for us. */\n\t\thdr->frame_control |= cpu_to_le16(IEEE80211_FCTL_MOREDATA);\n\t} else {\n\t\t/* Decide by priority where to put this frame. */\n\t\tring = select_ring_by_priority(\n\t\t\tdev, skb_get_queue_mapping(skb));\n\t}\n\n\tB43_WARN_ON(!ring->tx);\n\n\tif (unlikely(ring->stopped)) {\n\t\t/* We get here only because of a bug in mac80211.\n\t\t * Because of a race, one packet may be queued after\n\t\t * the queue is stopped, thus we got called when we shouldn't.\n\t\t * For now, just refuse the transmit. */\n\t\tif (b43_debug(dev, B43_DBG_DMAVERBOSE))\n\t\t\tb43err(dev->wl, \"Packet after queue stopped\\n\");\n\t\terr = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tif (unlikely(WARN_ON(free_slots(ring) < TX_SLOTS_PER_FRAME))) {\n\t\t/* If we get here, we have a real error with the queue\n\t\t * full, but queues not stopped. */\n\t\tb43err(dev->wl, \"DMA queue overflow\\n\");\n\t\terr = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\t/* Assign the queue number to the ring (if not already done before)\n\t * so TX status handling can use it. The queue to ring mapping is\n\t * static, so we don't need to store it per frame. */\n\tring->queue_prio = skb_get_queue_mapping(skb);\n\n\terr = dma_tx_fragment(ring, skb);\n\tif (unlikely(err == -ENOKEY)) {\n\t\t/* Drop this packet, as we don't have the encryption key\n\t\t * anymore and must not transmit it unencrypted. */\n\t\tdev_kfree_skb_any(skb);\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\tif (unlikely(err)) {\n\t\tb43err(dev->wl, \"DMA tx mapping failure\\n\");\n\t\tgoto out;\n\t}\n\tif ((free_slots(ring) < TX_SLOTS_PER_FRAME) ||\n\t    should_inject_overflow(ring)) {\n\t\t/* This TX ring is full. */\n\t\tieee80211_stop_queue(dev->wl->hw, skb_get_queue_mapping(skb));\n\t\tring->stopped = 1;\n\t\tif (b43_debug(dev, B43_DBG_DMAVERBOSE)) {\n\t\t\tb43dbg(dev->wl, \"Stopped TX ring %d\\n\", ring->index);\n\t\t}\n\t}\nout:\n\n\treturn err;\n}\n\nvoid b43_dma_handle_txstatus(struct b43_wldev *dev,\n\t\t\t     const struct b43_txstatus *status)\n{\n\tconst struct b43_dma_ops *ops;\n\tstruct b43_dmaring *ring;\n\tstruct b43_dmadesc_generic *desc;\n\tstruct b43_dmadesc_meta *meta;\n\tint slot, firstused;\n\tbool frame_succeed;\n\n\tring = parse_cookie(dev, status->cookie, &slot);\n\tif (unlikely(!ring))\n\t\treturn;\n\tB43_WARN_ON(!ring->tx);\n\n\t/* Sanity check: TX packets are processed in-order on one ring.\n\t * Check if the slot deduced from the cookie really is the first\n\t * used slot. */\n\tfirstused = ring->current_slot - ring->used_slots + 1;\n\tif (firstused < 0)\n\t\tfirstused = ring->nr_slots + firstused;\n\tif (unlikely(slot != firstused)) {\n\t\t/* This possibly is a firmware bug and will result in\n\t\t * malfunction, memory leaks and/or stall of DMA functionality. */\n\t\tb43dbg(dev->wl, \"Out of order TX status report on DMA ring %d. \"\n\t\t       \"Expected %d, but got %d\\n\",\n\t\t       ring->index, firstused, slot);\n\t\treturn;\n\t}\n\n\tops = ring->ops;\n\twhile (1) {\n\t\tB43_WARN_ON(slot < 0 || slot >= ring->nr_slots);\n\t\tdesc = ops->idx2desc(ring, slot, &meta);\n\n\t\tif (b43_dma_ptr_is_poisoned(meta->skb)) {\n\t\t\tb43dbg(dev->wl, \"Poisoned TX slot %d (first=%d) \"\n\t\t\t       \"on ring %d\\n\",\n\t\t\t       slot, firstused, ring->index);\n\t\t\tbreak;\n\t\t}\n\t\tif (meta->skb) {\n\t\t\tstruct b43_private_tx_info *priv_info =\n\t\t\t\tb43_get_priv_tx_info(IEEE80211_SKB_CB(meta->skb));\n\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr, meta->skb->len, 1);\n\t\t\tkfree(priv_info->bouncebuffer);\n\t\t\tpriv_info->bouncebuffer = NULL;\n\t\t} else {\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr,\n\t\t\t\t\t b43_txhdr_size(dev), 1);\n\t\t}\n\n\t\tif (meta->is_last_fragment) {\n\t\t\tstruct ieee80211_tx_info *info;\n\n\t\t\tif (unlikely(!meta->skb)) {\n\t\t\t\t/* This is a scatter-gather fragment of a frame, so\n\t\t\t\t * the skb pointer must not be NULL. */\n\t\t\t\tb43dbg(dev->wl, \"TX status unexpected NULL skb \"\n\t\t\t\t       \"at slot %d (first=%d) on ring %d\\n\",\n\t\t\t\t       slot, firstused, ring->index);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tinfo = IEEE80211_SKB_CB(meta->skb);\n\n\t\t\t/*\n\t\t\t * Call back to inform the ieee80211 subsystem about\n\t\t\t * the status of the transmission.\n\t\t\t */\n\t\t\tframe_succeed = b43_fill_txstatus_report(dev, info, status);\n#ifdef CONFIG_B43_DEBUG\n\t\t\tif (frame_succeed)\n\t\t\t\tring->nr_succeed_tx_packets++;\n\t\t\telse\n\t\t\t\tring->nr_failed_tx_packets++;\n\t\t\tring->nr_total_packet_tries += status->frame_count;\n#endif /* DEBUG */\n\t\t\tieee80211_tx_status(dev->wl->hw, meta->skb);\n\n\t\t\t/* skb will be freed by ieee80211_tx_status().\n\t\t\t * Poison our pointer. */\n\t\t\tmeta->skb = B43_DMA_PTR_POISON;\n\t\t} else {\n\t\t\t/* No need to call free_descriptor_buffer here, as\n\t\t\t * this is only the txhdr, which is not allocated.\n\t\t\t */\n\t\t\tif (unlikely(meta->skb)) {\n\t\t\t\tb43dbg(dev->wl, \"TX status unexpected non-NULL skb \"\n\t\t\t\t       \"at slot %d (first=%d) on ring %d\\n\",\n\t\t\t\t       slot, firstused, ring->index);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* Everything unmapped and free'd. So it's not used anymore. */\n\t\tring->used_slots--;\n\n\t\tif (meta->is_last_fragment) {\n\t\t\t/* This is the last scatter-gather\n\t\t\t * fragment of the frame. We are done. */\n\t\t\tbreak;\n\t\t}\n\t\tslot = next_slot(ring, slot);\n\t}\n\tif (ring->stopped) {\n\t\tB43_WARN_ON(free_slots(ring) < TX_SLOTS_PER_FRAME);\n\t\tieee80211_wake_queue(dev->wl->hw, ring->queue_prio);\n\t\tring->stopped = 0;\n\t\tif (b43_debug(dev, B43_DBG_DMAVERBOSE)) {\n\t\t\tb43dbg(dev->wl, \"Woke up TX ring %d\\n\", ring->index);\n\t\t}\n\t}\n}\n\nstatic void dma_rx(struct b43_dmaring *ring, int *slot)\n{\n\tconst struct b43_dma_ops *ops = ring->ops;\n\tstruct b43_dmadesc_generic *desc;\n\tstruct b43_dmadesc_meta *meta;\n\tstruct b43_rxhdr_fw4 *rxhdr;\n\tstruct sk_buff *skb;\n\tu16 len;\n\tint err;\n\tdma_addr_t dmaaddr;\n\n\tdesc = ops->idx2desc(ring, *slot, &meta);\n\n\tsync_descbuffer_for_cpu(ring, meta->dmaaddr, ring->rx_buffersize);\n\tskb = meta->skb;\n\n\trxhdr = (struct b43_rxhdr_fw4 *)skb->data;\n\tlen = le16_to_cpu(rxhdr->frame_len);\n\tif (len == 0) {\n\t\tint i = 0;\n\n\t\tdo {\n\t\t\tudelay(2);\n\t\t\tbarrier();\n\t\t\tlen = le16_to_cpu(rxhdr->frame_len);\n\t\t} while (len == 0 && i++ < 5);\n\t\tif (unlikely(len == 0)) {\n\t\t\tdmaaddr = meta->dmaaddr;\n\t\t\tgoto drop_recycle_buffer;\n\t\t}\n\t}\n\tif (unlikely(b43_rx_buffer_is_poisoned(ring, skb))) {\n\t\t/* Something went wrong with the DMA.\n\t\t * The device did not touch the buffer and did not overwrite the poison. */\n\t\tb43dbg(ring->dev->wl, \"DMA RX: Dropping poisoned buffer.\\n\");\n\t\tdmaaddr = meta->dmaaddr;\n\t\tgoto drop_recycle_buffer;\n\t}\n\tif (unlikely(len > ring->rx_buffersize)) {\n\t\t/* The data did not fit into one descriptor buffer\n\t\t * and is split over multiple buffers.\n\t\t * This should never happen, as we try to allocate buffers\n\t\t * big enough. So simply ignore this packet.\n\t\t */\n\t\tint cnt = 0;\n\t\ts32 tmp = len;\n\n\t\twhile (1) {\n\t\t\tdesc = ops->idx2desc(ring, *slot, &meta);\n\t\t\t/* recycle the descriptor buffer. */\n\t\t\tb43_poison_rx_buffer(ring, meta->skb);\n\t\t\tsync_descbuffer_for_device(ring, meta->dmaaddr,\n\t\t\t\t\t\t   ring->rx_buffersize);\n\t\t\t*slot = next_slot(ring, *slot);\n\t\t\tcnt++;\n\t\t\ttmp -= ring->rx_buffersize;\n\t\t\tif (tmp <= 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tb43err(ring->dev->wl, \"DMA RX buffer too small \"\n\t\t       \"(len: %u, buffer: %u, nr-dropped: %d)\\n\",\n\t\t       len, ring->rx_buffersize, cnt);\n\t\tgoto drop;\n\t}\n\n\tdmaaddr = meta->dmaaddr;\n\terr = setup_rx_descbuffer(ring, desc, meta, GFP_ATOMIC);\n\tif (unlikely(err)) {\n\t\tb43dbg(ring->dev->wl, \"DMA RX: setup_rx_descbuffer() failed\\n\");\n\t\tgoto drop_recycle_buffer;\n\t}\n\n\tunmap_descbuffer(ring, dmaaddr, ring->rx_buffersize, 0);\n\tskb_put(skb, len + ring->frameoffset);\n\tskb_pull(skb, ring->frameoffset);\n\n\tb43_rx(ring->dev, skb, rxhdr);\ndrop:\n\treturn;\n\ndrop_recycle_buffer:\n\t/* Poison and recycle the RX buffer. */\n\tb43_poison_rx_buffer(ring, skb);\n\tsync_descbuffer_for_device(ring, dmaaddr, ring->rx_buffersize);\n}\n\nvoid b43_dma_rx(struct b43_dmaring *ring)\n{\n\tconst struct b43_dma_ops *ops = ring->ops;\n\tint slot, current_slot;\n\tint used_slots = 0;\n\n\tB43_WARN_ON(ring->tx);\n\tcurrent_slot = ops->get_current_rxslot(ring);\n\tB43_WARN_ON(!(current_slot >= 0 && current_slot < ring->nr_slots));\n\n\tslot = ring->current_slot;\n\tfor (; slot != current_slot; slot = next_slot(ring, slot)) {\n\t\tdma_rx(ring, &slot);\n\t\tupdate_max_used_slots(ring, ++used_slots);\n\t}\n\tops->set_current_rxslot(ring, slot);\n\tring->current_slot = slot;\n}\n\nstatic void b43_dma_tx_suspend_ring(struct b43_dmaring *ring)\n{\n\tB43_WARN_ON(!ring->tx);\n\tring->ops->tx_suspend(ring);\n}\n\nstatic void b43_dma_tx_resume_ring(struct b43_dmaring *ring)\n{\n\tB43_WARN_ON(!ring->tx);\n\tring->ops->tx_resume(ring);\n}\n\nvoid b43_dma_tx_suspend(struct b43_wldev *dev)\n{\n\tb43_power_saving_ctl_bits(dev, B43_PS_AWAKE);\n\tb43_dma_tx_suspend_ring(dev->dma.tx_ring_AC_BK);\n\tb43_dma_tx_suspend_ring(dev->dma.tx_ring_AC_BE);\n\tb43_dma_tx_suspend_ring(dev->dma.tx_ring_AC_VI);\n\tb43_dma_tx_suspend_ring(dev->dma.tx_ring_AC_VO);\n\tb43_dma_tx_suspend_ring(dev->dma.tx_ring_mcast);\n}\n\nvoid b43_dma_tx_resume(struct b43_wldev *dev)\n{\n\tb43_dma_tx_resume_ring(dev->dma.tx_ring_mcast);\n\tb43_dma_tx_resume_ring(dev->dma.tx_ring_AC_VO);\n\tb43_dma_tx_resume_ring(dev->dma.tx_ring_AC_VI);\n\tb43_dma_tx_resume_ring(dev->dma.tx_ring_AC_BE);\n\tb43_dma_tx_resume_ring(dev->dma.tx_ring_AC_BK);\n\tb43_power_saving_ctl_bits(dev, 0);\n}\n\nstatic void direct_fifo_rx(struct b43_wldev *dev, enum b43_dmatype type,\n\t\t\t   u16 mmio_base, bool enable)\n{\n\tu32 ctl;\n\n\tif (type == B43_DMA_64BIT) {\n\t\tctl = b43_read32(dev, mmio_base + B43_DMA64_RXCTL);\n\t\tctl &= ~B43_DMA64_RXDIRECTFIFO;\n\t\tif (enable)\n\t\t\tctl |= B43_DMA64_RXDIRECTFIFO;\n\t\tb43_write32(dev, mmio_base + B43_DMA64_RXCTL, ctl);\n\t} else {\n\t\tctl = b43_read32(dev, mmio_base + B43_DMA32_RXCTL);\n\t\tctl &= ~B43_DMA32_RXDIRECTFIFO;\n\t\tif (enable)\n\t\t\tctl |= B43_DMA32_RXDIRECTFIFO;\n\t\tb43_write32(dev, mmio_base + B43_DMA32_RXCTL, ctl);\n\t}\n}\n\n/* Enable/Disable Direct FIFO Receive Mode (PIO) on a RX engine.\n * This is called from PIO code, so DMA structures are not available. */\nvoid b43_dma_direct_fifo_rx(struct b43_wldev *dev,\n\t\t\t    unsigned int engine_index, bool enable)\n{\n\tenum b43_dmatype type;\n\tu16 mmio_base;\n\n\ttype = dma_mask_to_engine_type(supported_dma_mask(dev));\n\n\tmmio_base = b43_dmacontroller_base(type, engine_index);\n\tdirect_fifo_rx(dev, type, mmio_base, enable);\n}\n", "#ifndef B43_DMA_H_\n#define B43_DMA_H_\n\n#include <linux/err.h>\n\n#include \"b43.h\"\n\n\n/* DMA-Interrupt reasons. */\n#define B43_DMAIRQ_FATALMASK\t((1 << 10) | (1 << 11) | (1 << 12) \\\n\t\t\t\t\t | (1 << 14) | (1 << 15))\n#define B43_DMAIRQ_NONFATALMASK\t(1 << 13)\n#define B43_DMAIRQ_RX_DONE\t\t(1 << 16)\n\n/*** 32-bit DMA Engine. ***/\n\n/* 32-bit DMA controller registers. */\n#define B43_DMA32_TXCTL\t\t\t\t0x00\n#define\t\tB43_DMA32_TXENABLE\t\t\t0x00000001\n#define\t\tB43_DMA32_TXSUSPEND\t\t\t0x00000002\n#define\t\tB43_DMA32_TXLOOPBACK\t\t0x00000004\n#define\t\tB43_DMA32_TXFLUSH\t\t\t0x00000010\n#define\t\tB43_DMA32_TXADDREXT_MASK\t\t0x00030000\n#define\t\tB43_DMA32_TXADDREXT_SHIFT\t\t16\n#define B43_DMA32_TXRING\t\t\t\t0x04\n#define B43_DMA32_TXINDEX\t\t\t\t0x08\n#define B43_DMA32_TXSTATUS\t\t\t\t0x0C\n#define\t\tB43_DMA32_TXDPTR\t\t\t0x00000FFF\n#define\t\tB43_DMA32_TXSTATE\t\t\t0x0000F000\n#define\t\t\tB43_DMA32_TXSTAT_DISABLED\t0x00000000\n#define\t\t\tB43_DMA32_TXSTAT_ACTIVE\t0x00001000\n#define\t\t\tB43_DMA32_TXSTAT_IDLEWAIT\t0x00002000\n#define\t\t\tB43_DMA32_TXSTAT_STOPPED\t0x00003000\n#define\t\t\tB43_DMA32_TXSTAT_SUSP\t0x00004000\n#define\t\tB43_DMA32_TXERROR\t\t\t0x000F0000\n#define\t\t\tB43_DMA32_TXERR_NOERR\t0x00000000\n#define\t\t\tB43_DMA32_TXERR_PROT\t0x00010000\n#define\t\t\tB43_DMA32_TXERR_UNDERRUN\t0x00020000\n#define\t\t\tB43_DMA32_TXERR_BUFREAD\t0x00030000\n#define\t\t\tB43_DMA32_TXERR_DESCREAD\t0x00040000\n#define\t\tB43_DMA32_TXACTIVE\t\t\t0xFFF00000\n#define B43_DMA32_RXCTL\t\t\t\t0x10\n#define\t\tB43_DMA32_RXENABLE\t\t\t0x00000001\n#define\t\tB43_DMA32_RXFROFF_MASK\t\t0x000000FE\n#define\t\tB43_DMA32_RXFROFF_SHIFT\t\t1\n#define\t\tB43_DMA32_RXDIRECTFIFO\t\t0x00000100\n#define\t\tB43_DMA32_RXADDREXT_MASK\t\t0x00030000\n#define\t\tB43_DMA32_RXADDREXT_SHIFT\t\t16\n#define B43_DMA32_RXRING\t\t\t\t0x14\n#define B43_DMA32_RXINDEX\t\t\t\t0x18\n#define B43_DMA32_RXSTATUS\t\t\t\t0x1C\n#define\t\tB43_DMA32_RXDPTR\t\t\t0x00000FFF\n#define\t\tB43_DMA32_RXSTATE\t\t\t0x0000F000\n#define\t\t\tB43_DMA32_RXSTAT_DISABLED\t0x00000000\n#define\t\t\tB43_DMA32_RXSTAT_ACTIVE\t0x00001000\n#define\t\t\tB43_DMA32_RXSTAT_IDLEWAIT\t0x00002000\n#define\t\t\tB43_DMA32_RXSTAT_STOPPED\t0x00003000\n#define\t\tB43_DMA32_RXERROR\t\t\t0x000F0000\n#define\t\t\tB43_DMA32_RXERR_NOERR\t0x00000000\n#define\t\t\tB43_DMA32_RXERR_PROT\t0x00010000\n#define\t\t\tB43_DMA32_RXERR_OVERFLOW\t0x00020000\n#define\t\t\tB43_DMA32_RXERR_BUFWRITE\t0x00030000\n#define\t\t\tB43_DMA32_RXERR_DESCREAD\t0x00040000\n#define\t\tB43_DMA32_RXACTIVE\t\t\t0xFFF00000\n\n/* 32-bit DMA descriptor. */\nstruct b43_dmadesc32 {\n\t__le32 control;\n\t__le32 address;\n} __packed;\n#define B43_DMA32_DCTL_BYTECNT\t\t0x00001FFF\n#define B43_DMA32_DCTL_ADDREXT_MASK\t\t0x00030000\n#define B43_DMA32_DCTL_ADDREXT_SHIFT\t16\n#define B43_DMA32_DCTL_DTABLEEND\t\t0x10000000\n#define B43_DMA32_DCTL_IRQ\t\t\t0x20000000\n#define B43_DMA32_DCTL_FRAMEEND\t\t0x40000000\n#define B43_DMA32_DCTL_FRAMESTART\t\t0x80000000\n\n/*** 64-bit DMA Engine. ***/\n\n/* 64-bit DMA controller registers. */\n#define B43_DMA64_TXCTL\t\t\t\t0x00\n#define\t\tB43_DMA64_TXENABLE\t\t\t0x00000001\n#define\t\tB43_DMA64_TXSUSPEND\t\t\t0x00000002\n#define\t\tB43_DMA64_TXLOOPBACK\t\t0x00000004\n#define\t\tB43_DMA64_TXFLUSH\t\t\t0x00000010\n#define\t\tB43_DMA64_TXADDREXT_MASK\t\t0x00030000\n#define\t\tB43_DMA64_TXADDREXT_SHIFT\t\t16\n#define B43_DMA64_TXINDEX\t\t\t\t0x04\n#define B43_DMA64_TXRINGLO\t\t\t\t0x08\n#define B43_DMA64_TXRINGHI\t\t\t\t0x0C\n#define B43_DMA64_TXSTATUS\t\t\t\t0x10\n#define\t\tB43_DMA64_TXSTATDPTR\t\t0x00001FFF\n#define\t\tB43_DMA64_TXSTAT\t\t\t0xF0000000\n#define\t\t\tB43_DMA64_TXSTAT_DISABLED\t0x00000000\n#define\t\t\tB43_DMA64_TXSTAT_ACTIVE\t0x10000000\n#define\t\t\tB43_DMA64_TXSTAT_IDLEWAIT\t0x20000000\n#define\t\t\tB43_DMA64_TXSTAT_STOPPED\t0x30000000\n#define\t\t\tB43_DMA64_TXSTAT_SUSP\t0x40000000\n#define B43_DMA64_TXERROR\t\t\t\t0x14\n#define\t\tB43_DMA64_TXERRDPTR\t\t\t0x0001FFFF\n#define\t\tB43_DMA64_TXERR\t\t\t0xF0000000\n#define\t\t\tB43_DMA64_TXERR_NOERR\t0x00000000\n#define\t\t\tB43_DMA64_TXERR_PROT\t0x10000000\n#define\t\t\tB43_DMA64_TXERR_UNDERRUN\t0x20000000\n#define\t\t\tB43_DMA64_TXERR_TRANSFER\t0x30000000\n#define\t\t\tB43_DMA64_TXERR_DESCREAD\t0x40000000\n#define\t\t\tB43_DMA64_TXERR_CORE\t0x50000000\n#define B43_DMA64_RXCTL\t\t\t\t0x20\n#define\t\tB43_DMA64_RXENABLE\t\t\t0x00000001\n#define\t\tB43_DMA64_RXFROFF_MASK\t\t0x000000FE\n#define\t\tB43_DMA64_RXFROFF_SHIFT\t\t1\n#define\t\tB43_DMA64_RXDIRECTFIFO\t\t0x00000100\n#define\t\tB43_DMA64_RXADDREXT_MASK\t\t0x00030000\n#define\t\tB43_DMA64_RXADDREXT_SHIFT\t\t16\n#define B43_DMA64_RXINDEX\t\t\t\t0x24\n#define B43_DMA64_RXRINGLO\t\t\t\t0x28\n#define B43_DMA64_RXRINGHI\t\t\t\t0x2C\n#define B43_DMA64_RXSTATUS\t\t\t\t0x30\n#define\t\tB43_DMA64_RXSTATDPTR\t\t0x00001FFF\n#define\t\tB43_DMA64_RXSTAT\t\t\t0xF0000000\n#define\t\t\tB43_DMA64_RXSTAT_DISABLED\t0x00000000\n#define\t\t\tB43_DMA64_RXSTAT_ACTIVE\t0x10000000\n#define\t\t\tB43_DMA64_RXSTAT_IDLEWAIT\t0x20000000\n#define\t\t\tB43_DMA64_RXSTAT_STOPPED\t0x30000000\n#define\t\t\tB43_DMA64_RXSTAT_SUSP\t0x40000000\n#define B43_DMA64_RXERROR\t\t\t\t0x34\n#define\t\tB43_DMA64_RXERRDPTR\t\t\t0x0001FFFF\n#define\t\tB43_DMA64_RXERR\t\t\t0xF0000000\n#define\t\t\tB43_DMA64_RXERR_NOERR\t0x00000000\n#define\t\t\tB43_DMA64_RXERR_PROT\t0x10000000\n#define\t\t\tB43_DMA64_RXERR_UNDERRUN\t0x20000000\n#define\t\t\tB43_DMA64_RXERR_TRANSFER\t0x30000000\n#define\t\t\tB43_DMA64_RXERR_DESCREAD\t0x40000000\n#define\t\t\tB43_DMA64_RXERR_CORE\t0x50000000\n\n/* 64-bit DMA descriptor. */\nstruct b43_dmadesc64 {\n\t__le32 control0;\n\t__le32 control1;\n\t__le32 address_low;\n\t__le32 address_high;\n} __packed;\n#define B43_DMA64_DCTL0_DTABLEEND\t\t0x10000000\n#define B43_DMA64_DCTL0_IRQ\t\t\t0x20000000\n#define B43_DMA64_DCTL0_FRAMEEND\t\t0x40000000\n#define B43_DMA64_DCTL0_FRAMESTART\t\t0x80000000\n#define B43_DMA64_DCTL1_BYTECNT\t\t0x00001FFF\n#define B43_DMA64_DCTL1_ADDREXT_MASK\t0x00030000\n#define B43_DMA64_DCTL1_ADDREXT_SHIFT\t16\n\nstruct b43_dmadesc_generic {\n\tunion {\n\t\tstruct b43_dmadesc32 dma32;\n\t\tstruct b43_dmadesc64 dma64;\n\t} __packed;\n} __packed;\n\n/* Misc DMA constants */\n#define B43_DMA_RINGMEMSIZE\t\tPAGE_SIZE\n#define B43_DMA0_RX_FRAMEOFFSET\t\t30\n\n/* DMA engine tuning knobs */\n#define B43_TXRING_SLOTS\t\t256\n#define B43_RXRING_SLOTS\t\t64\n#define B43_DMA0_RX_BUFFERSIZE\t\tIEEE80211_MAX_FRAME_LEN\n\n/* Pointer poison */\n#define B43_DMA_PTR_POISON\t\t((void *)ERR_PTR(-ENOMEM))\n#define b43_dma_ptr_is_poisoned(ptr)\t(unlikely((ptr) == B43_DMA_PTR_POISON))\n\n\nstruct sk_buff;\nstruct b43_private;\nstruct b43_txstatus;\n\nstruct b43_dmadesc_meta {\n\t/* The kernel DMA-able buffer. */\n\tstruct sk_buff *skb;\n\t/* DMA base bus-address of the descriptor buffer. */\n\tdma_addr_t dmaaddr;\n\t/* ieee80211 TX status. Only used once per 802.11 frag. */\n\tbool is_last_fragment;\n};\n\nstruct b43_dmaring;\n\n/* Lowlevel DMA operations that differ between 32bit and 64bit DMA. */\nstruct b43_dma_ops {\n\tstruct b43_dmadesc_generic *(*idx2desc) (struct b43_dmaring * ring,\n\t\t\t\t\t\t int slot,\n\t\t\t\t\t\t struct b43_dmadesc_meta **\n\t\t\t\t\t\t meta);\n\tvoid (*fill_descriptor) (struct b43_dmaring * ring,\n\t\t\t\t struct b43_dmadesc_generic * desc,\n\t\t\t\t dma_addr_t dmaaddr, u16 bufsize, int start,\n\t\t\t\t int end, int irq);\n\tvoid (*poke_tx) (struct b43_dmaring * ring, int slot);\n\tvoid (*tx_suspend) (struct b43_dmaring * ring);\n\tvoid (*tx_resume) (struct b43_dmaring * ring);\n\tint (*get_current_rxslot) (struct b43_dmaring * ring);\n\tvoid (*set_current_rxslot) (struct b43_dmaring * ring, int slot);\n};\n\nenum b43_dmatype {\n\tB43_DMA_30BIT\t= 30,\n\tB43_DMA_32BIT\t= 32,\n\tB43_DMA_64BIT\t= 64,\n};\n\nstruct b43_dmaring {\n\t/* Lowlevel DMA ops. */\n\tconst struct b43_dma_ops *ops;\n\t/* Kernel virtual base address of the ring memory. */\n\tvoid *descbase;\n\t/* Meta data about all descriptors. */\n\tstruct b43_dmadesc_meta *meta;\n\t/* Cache of TX headers for each TX frame.\n\t * This is to avoid an allocation on each TX.\n\t * This is NULL for an RX ring.\n\t */\n\tu8 *txhdr_cache;\n\t/* (Unadjusted) DMA base bus-address of the ring memory. */\n\tdma_addr_t dmabase;\n\t/* Number of descriptor slots in the ring. */\n\tint nr_slots;\n\t/* Number of used descriptor slots. */\n\tint used_slots;\n\t/* Currently used slot in the ring. */\n\tint current_slot;\n\t/* Frameoffset in octets. */\n\tu32 frameoffset;\n\t/* Descriptor buffer size. */\n\tu16 rx_buffersize;\n\t/* The MMIO base register of the DMA controller. */\n\tu16 mmio_base;\n\t/* DMA controller index number (0-5). */\n\tint index;\n\t/* Boolean. Is this a TX ring? */\n\tbool tx;\n\t/* The type of DMA engine used. */\n\tenum b43_dmatype type;\n\t/* Boolean. Is this ring stopped at ieee80211 level? */\n\tbool stopped;\n\t/* The QOS priority assigned to this ring. Only used for TX rings.\n\t * This is the mac80211 \"queue\" value. */\n\tu8 queue_prio;\n\tstruct b43_wldev *dev;\n#ifdef CONFIG_B43_DEBUG\n\t/* Maximum number of used slots. */\n\tint max_used_slots;\n\t/* Last time we injected a ring overflow. */\n\tunsigned long last_injected_overflow;\n\t/* Statistics: Number of successfully transmitted packets */\n\tu64 nr_succeed_tx_packets;\n\t/* Statistics: Number of failed TX packets */\n\tu64 nr_failed_tx_packets;\n\t/* Statistics: Total number of TX plus all retries. */\n\tu64 nr_total_packet_tries;\n#endif /* CONFIG_B43_DEBUG */\n};\n\nstatic inline u32 b43_dma_read(struct b43_dmaring *ring, u16 offset)\n{\n\treturn b43_read32(ring->dev, ring->mmio_base + offset);\n}\n\nstatic inline void b43_dma_write(struct b43_dmaring *ring, u16 offset, u32 value)\n{\n\tb43_write32(ring->dev, ring->mmio_base + offset, value);\n}\n\nint b43_dma_init(struct b43_wldev *dev);\nvoid b43_dma_free(struct b43_wldev *dev);\n\nvoid b43_dma_tx_suspend(struct b43_wldev *dev);\nvoid b43_dma_tx_resume(struct b43_wldev *dev);\n\nint b43_dma_tx(struct b43_wldev *dev,\n\t       struct sk_buff *skb);\nvoid b43_dma_handle_txstatus(struct b43_wldev *dev,\n\t\t\t     const struct b43_txstatus *status);\n\nvoid b43_dma_rx(struct b43_dmaring *ring);\n\nvoid b43_dma_direct_fifo_rx(struct b43_wldev *dev,\n\t\t\t    unsigned int engine_index, bool enable);\n\n#endif /* B43_DMA_H_ */\n"], "fixing_code": ["/*\n\n  Broadcom B43 wireless driver\n\n  DMA ringbuffer and descriptor allocation/management\n\n  Copyright (c) 2005, 2006 Michael Buesch <mb@bu3sch.de>\n\n  Some code in this file is derived from the b44.c driver\n  Copyright (C) 2002 David S. Miller\n  Copyright (C) Pekka Pietikainen\n\n  This program is free software; you can redistribute it and/or modify\n  it under the terms of the GNU General Public License as published by\n  the Free Software Foundation; either version 2 of the License, or\n  (at your option) any later version.\n\n  This program is distributed in the hope that it will be useful,\n  but WITHOUT ANY WARRANTY; without even the implied warranty of\n  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n  GNU General Public License for more details.\n\n  You should have received a copy of the GNU General Public License\n  along with this program; see the file COPYING.  If not, write to\n  the Free Software Foundation, Inc., 51 Franklin Steet, Fifth Floor,\n  Boston, MA 02110-1301, USA.\n\n*/\n\n#include \"b43.h\"\n#include \"dma.h\"\n#include \"main.h\"\n#include \"debugfs.h\"\n#include \"xmit.h\"\n\n#include <linux/dma-mapping.h>\n#include <linux/pci.h>\n#include <linux/delay.h>\n#include <linux/skbuff.h>\n#include <linux/etherdevice.h>\n#include <linux/slab.h>\n#include <asm/div64.h>\n\n\n/* Required number of TX DMA slots per TX frame.\n * This currently is 2, because we put the header and the ieee80211 frame\n * into separate slots. */\n#define TX_SLOTS_PER_FRAME\t2\n\n\n/* 32bit DMA ops. */\nstatic\nstruct b43_dmadesc_generic *op32_idx2desc(struct b43_dmaring *ring,\n\t\t\t\t\t  int slot,\n\t\t\t\t\t  struct b43_dmadesc_meta **meta)\n{\n\tstruct b43_dmadesc32 *desc;\n\n\t*meta = &(ring->meta[slot]);\n\tdesc = ring->descbase;\n\tdesc = &(desc[slot]);\n\n\treturn (struct b43_dmadesc_generic *)desc;\n}\n\nstatic void op32_fill_descriptor(struct b43_dmaring *ring,\n\t\t\t\t struct b43_dmadesc_generic *desc,\n\t\t\t\t dma_addr_t dmaaddr, u16 bufsize,\n\t\t\t\t int start, int end, int irq)\n{\n\tstruct b43_dmadesc32 *descbase = ring->descbase;\n\tint slot;\n\tu32 ctl;\n\tu32 addr;\n\tu32 addrext;\n\n\tslot = (int)(&(desc->dma32) - descbase);\n\tB43_WARN_ON(!(slot >= 0 && slot < ring->nr_slots));\n\n\taddr = (u32) (dmaaddr & ~SSB_DMA_TRANSLATION_MASK);\n\taddrext = (u32) (dmaaddr & SSB_DMA_TRANSLATION_MASK)\n\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\taddr |= ssb_dma_translation(ring->dev->dev);\n\tctl = bufsize & B43_DMA32_DCTL_BYTECNT;\n\tif (slot == ring->nr_slots - 1)\n\t\tctl |= B43_DMA32_DCTL_DTABLEEND;\n\tif (start)\n\t\tctl |= B43_DMA32_DCTL_FRAMESTART;\n\tif (end)\n\t\tctl |= B43_DMA32_DCTL_FRAMEEND;\n\tif (irq)\n\t\tctl |= B43_DMA32_DCTL_IRQ;\n\tctl |= (addrext << B43_DMA32_DCTL_ADDREXT_SHIFT)\n\t    & B43_DMA32_DCTL_ADDREXT_MASK;\n\n\tdesc->dma32.control = cpu_to_le32(ctl);\n\tdesc->dma32.address = cpu_to_le32(addr);\n}\n\nstatic void op32_poke_tx(struct b43_dmaring *ring, int slot)\n{\n\tb43_dma_write(ring, B43_DMA32_TXINDEX,\n\t\t      (u32) (slot * sizeof(struct b43_dmadesc32)));\n}\n\nstatic void op32_tx_suspend(struct b43_dmaring *ring)\n{\n\tb43_dma_write(ring, B43_DMA32_TXCTL, b43_dma_read(ring, B43_DMA32_TXCTL)\n\t\t      | B43_DMA32_TXSUSPEND);\n}\n\nstatic void op32_tx_resume(struct b43_dmaring *ring)\n{\n\tb43_dma_write(ring, B43_DMA32_TXCTL, b43_dma_read(ring, B43_DMA32_TXCTL)\n\t\t      & ~B43_DMA32_TXSUSPEND);\n}\n\nstatic int op32_get_current_rxslot(struct b43_dmaring *ring)\n{\n\tu32 val;\n\n\tval = b43_dma_read(ring, B43_DMA32_RXSTATUS);\n\tval &= B43_DMA32_RXDPTR;\n\n\treturn (val / sizeof(struct b43_dmadesc32));\n}\n\nstatic void op32_set_current_rxslot(struct b43_dmaring *ring, int slot)\n{\n\tb43_dma_write(ring, B43_DMA32_RXINDEX,\n\t\t      (u32) (slot * sizeof(struct b43_dmadesc32)));\n}\n\nstatic const struct b43_dma_ops dma32_ops = {\n\t.idx2desc = op32_idx2desc,\n\t.fill_descriptor = op32_fill_descriptor,\n\t.poke_tx = op32_poke_tx,\n\t.tx_suspend = op32_tx_suspend,\n\t.tx_resume = op32_tx_resume,\n\t.get_current_rxslot = op32_get_current_rxslot,\n\t.set_current_rxslot = op32_set_current_rxslot,\n};\n\n/* 64bit DMA ops. */\nstatic\nstruct b43_dmadesc_generic *op64_idx2desc(struct b43_dmaring *ring,\n\t\t\t\t\t  int slot,\n\t\t\t\t\t  struct b43_dmadesc_meta **meta)\n{\n\tstruct b43_dmadesc64 *desc;\n\n\t*meta = &(ring->meta[slot]);\n\tdesc = ring->descbase;\n\tdesc = &(desc[slot]);\n\n\treturn (struct b43_dmadesc_generic *)desc;\n}\n\nstatic void op64_fill_descriptor(struct b43_dmaring *ring,\n\t\t\t\t struct b43_dmadesc_generic *desc,\n\t\t\t\t dma_addr_t dmaaddr, u16 bufsize,\n\t\t\t\t int start, int end, int irq)\n{\n\tstruct b43_dmadesc64 *descbase = ring->descbase;\n\tint slot;\n\tu32 ctl0 = 0, ctl1 = 0;\n\tu32 addrlo, addrhi;\n\tu32 addrext;\n\n\tslot = (int)(&(desc->dma64) - descbase);\n\tB43_WARN_ON(!(slot >= 0 && slot < ring->nr_slots));\n\n\taddrlo = (u32) (dmaaddr & 0xFFFFFFFF);\n\taddrhi = (((u64) dmaaddr >> 32) & ~SSB_DMA_TRANSLATION_MASK);\n\taddrext = (((u64) dmaaddr >> 32) & SSB_DMA_TRANSLATION_MASK)\n\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\taddrhi |= (ssb_dma_translation(ring->dev->dev) << 1);\n\tif (slot == ring->nr_slots - 1)\n\t\tctl0 |= B43_DMA64_DCTL0_DTABLEEND;\n\tif (start)\n\t\tctl0 |= B43_DMA64_DCTL0_FRAMESTART;\n\tif (end)\n\t\tctl0 |= B43_DMA64_DCTL0_FRAMEEND;\n\tif (irq)\n\t\tctl0 |= B43_DMA64_DCTL0_IRQ;\n\tctl1 |= bufsize & B43_DMA64_DCTL1_BYTECNT;\n\tctl1 |= (addrext << B43_DMA64_DCTL1_ADDREXT_SHIFT)\n\t    & B43_DMA64_DCTL1_ADDREXT_MASK;\n\n\tdesc->dma64.control0 = cpu_to_le32(ctl0);\n\tdesc->dma64.control1 = cpu_to_le32(ctl1);\n\tdesc->dma64.address_low = cpu_to_le32(addrlo);\n\tdesc->dma64.address_high = cpu_to_le32(addrhi);\n}\n\nstatic void op64_poke_tx(struct b43_dmaring *ring, int slot)\n{\n\tb43_dma_write(ring, B43_DMA64_TXINDEX,\n\t\t      (u32) (slot * sizeof(struct b43_dmadesc64)));\n}\n\nstatic void op64_tx_suspend(struct b43_dmaring *ring)\n{\n\tb43_dma_write(ring, B43_DMA64_TXCTL, b43_dma_read(ring, B43_DMA64_TXCTL)\n\t\t      | B43_DMA64_TXSUSPEND);\n}\n\nstatic void op64_tx_resume(struct b43_dmaring *ring)\n{\n\tb43_dma_write(ring, B43_DMA64_TXCTL, b43_dma_read(ring, B43_DMA64_TXCTL)\n\t\t      & ~B43_DMA64_TXSUSPEND);\n}\n\nstatic int op64_get_current_rxslot(struct b43_dmaring *ring)\n{\n\tu32 val;\n\n\tval = b43_dma_read(ring, B43_DMA64_RXSTATUS);\n\tval &= B43_DMA64_RXSTATDPTR;\n\n\treturn (val / sizeof(struct b43_dmadesc64));\n}\n\nstatic void op64_set_current_rxslot(struct b43_dmaring *ring, int slot)\n{\n\tb43_dma_write(ring, B43_DMA64_RXINDEX,\n\t\t      (u32) (slot * sizeof(struct b43_dmadesc64)));\n}\n\nstatic const struct b43_dma_ops dma64_ops = {\n\t.idx2desc = op64_idx2desc,\n\t.fill_descriptor = op64_fill_descriptor,\n\t.poke_tx = op64_poke_tx,\n\t.tx_suspend = op64_tx_suspend,\n\t.tx_resume = op64_tx_resume,\n\t.get_current_rxslot = op64_get_current_rxslot,\n\t.set_current_rxslot = op64_set_current_rxslot,\n};\n\nstatic inline int free_slots(struct b43_dmaring *ring)\n{\n\treturn (ring->nr_slots - ring->used_slots);\n}\n\nstatic inline int next_slot(struct b43_dmaring *ring, int slot)\n{\n\tB43_WARN_ON(!(slot >= -1 && slot <= ring->nr_slots - 1));\n\tif (slot == ring->nr_slots - 1)\n\t\treturn 0;\n\treturn slot + 1;\n}\n\nstatic inline int prev_slot(struct b43_dmaring *ring, int slot)\n{\n\tB43_WARN_ON(!(slot >= 0 && slot <= ring->nr_slots - 1));\n\tif (slot == 0)\n\t\treturn ring->nr_slots - 1;\n\treturn slot - 1;\n}\n\n#ifdef CONFIG_B43_DEBUG\nstatic void update_max_used_slots(struct b43_dmaring *ring,\n\t\t\t\t  int current_used_slots)\n{\n\tif (current_used_slots <= ring->max_used_slots)\n\t\treturn;\n\tring->max_used_slots = current_used_slots;\n\tif (b43_debug(ring->dev, B43_DBG_DMAVERBOSE)) {\n\t\tb43dbg(ring->dev->wl,\n\t\t       \"max_used_slots increased to %d on %s ring %d\\n\",\n\t\t       ring->max_used_slots,\n\t\t       ring->tx ? \"TX\" : \"RX\", ring->index);\n\t}\n}\n#else\nstatic inline\n    void update_max_used_slots(struct b43_dmaring *ring, int current_used_slots)\n{\n}\n#endif /* DEBUG */\n\n/* Request a slot for usage. */\nstatic inline int request_slot(struct b43_dmaring *ring)\n{\n\tint slot;\n\n\tB43_WARN_ON(!ring->tx);\n\tB43_WARN_ON(ring->stopped);\n\tB43_WARN_ON(free_slots(ring) == 0);\n\n\tslot = next_slot(ring, ring->current_slot);\n\tring->current_slot = slot;\n\tring->used_slots++;\n\n\tupdate_max_used_slots(ring, ring->used_slots);\n\n\treturn slot;\n}\n\nstatic u16 b43_dmacontroller_base(enum b43_dmatype type, int controller_idx)\n{\n\tstatic const u16 map64[] = {\n\t\tB43_MMIO_DMA64_BASE0,\n\t\tB43_MMIO_DMA64_BASE1,\n\t\tB43_MMIO_DMA64_BASE2,\n\t\tB43_MMIO_DMA64_BASE3,\n\t\tB43_MMIO_DMA64_BASE4,\n\t\tB43_MMIO_DMA64_BASE5,\n\t};\n\tstatic const u16 map32[] = {\n\t\tB43_MMIO_DMA32_BASE0,\n\t\tB43_MMIO_DMA32_BASE1,\n\t\tB43_MMIO_DMA32_BASE2,\n\t\tB43_MMIO_DMA32_BASE3,\n\t\tB43_MMIO_DMA32_BASE4,\n\t\tB43_MMIO_DMA32_BASE5,\n\t};\n\n\tif (type == B43_DMA_64BIT) {\n\t\tB43_WARN_ON(!(controller_idx >= 0 &&\n\t\t\t      controller_idx < ARRAY_SIZE(map64)));\n\t\treturn map64[controller_idx];\n\t}\n\tB43_WARN_ON(!(controller_idx >= 0 &&\n\t\t      controller_idx < ARRAY_SIZE(map32)));\n\treturn map32[controller_idx];\n}\n\nstatic inline\n    dma_addr_t map_descbuffer(struct b43_dmaring *ring,\n\t\t\t      unsigned char *buf, size_t len, int tx)\n{\n\tdma_addr_t dmaaddr;\n\n\tif (tx) {\n\t\tdmaaddr = dma_map_single(ring->dev->dev->dma_dev,\n\t\t\t\t\t buf, len, DMA_TO_DEVICE);\n\t} else {\n\t\tdmaaddr = dma_map_single(ring->dev->dev->dma_dev,\n\t\t\t\t\t buf, len, DMA_FROM_DEVICE);\n\t}\n\n\treturn dmaaddr;\n}\n\nstatic inline\n    void unmap_descbuffer(struct b43_dmaring *ring,\n\t\t\t  dma_addr_t addr, size_t len, int tx)\n{\n\tif (tx) {\n\t\tdma_unmap_single(ring->dev->dev->dma_dev,\n\t\t\t\t addr, len, DMA_TO_DEVICE);\n\t} else {\n\t\tdma_unmap_single(ring->dev->dev->dma_dev,\n\t\t\t\t addr, len, DMA_FROM_DEVICE);\n\t}\n}\n\nstatic inline\n    void sync_descbuffer_for_cpu(struct b43_dmaring *ring,\n\t\t\t\t dma_addr_t addr, size_t len)\n{\n\tB43_WARN_ON(ring->tx);\n\tdma_sync_single_for_cpu(ring->dev->dev->dma_dev,\n\t\t\t\t    addr, len, DMA_FROM_DEVICE);\n}\n\nstatic inline\n    void sync_descbuffer_for_device(struct b43_dmaring *ring,\n\t\t\t\t    dma_addr_t addr, size_t len)\n{\n\tB43_WARN_ON(ring->tx);\n\tdma_sync_single_for_device(ring->dev->dev->dma_dev,\n\t\t\t\t   addr, len, DMA_FROM_DEVICE);\n}\n\nstatic inline\n    void free_descriptor_buffer(struct b43_dmaring *ring,\n\t\t\t\tstruct b43_dmadesc_meta *meta)\n{\n\tif (meta->skb) {\n\t\tdev_kfree_skb_any(meta->skb);\n\t\tmeta->skb = NULL;\n\t}\n}\n\nstatic int alloc_ringmemory(struct b43_dmaring *ring)\n{\n\tgfp_t flags = GFP_KERNEL;\n\n\t/* The specs call for 4K buffers for 30- and 32-bit DMA with 4K\n\t * alignment and 8K buffers for 64-bit DMA with 8K alignment. Testing\n\t * has shown that 4K is sufficient for the latter as long as the buffer\n\t * does not cross an 8K boundary.\n\t *\n\t * For unknown reasons - possibly a hardware error - the BCM4311 rev\n\t * 02, which uses 64-bit DMA, needs the ring buffer in very low memory,\n\t * which accounts for the GFP_DMA flag below.\n\t *\n\t * The flags here must match the flags in free_ringmemory below!\n\t */\n\tif (ring->type == B43_DMA_64BIT)\n\t\tflags |= GFP_DMA;\n\tring->descbase = dma_alloc_coherent(ring->dev->dev->dma_dev,\n\t\t\t\t\t    B43_DMA_RINGMEMSIZE,\n\t\t\t\t\t    &(ring->dmabase), flags);\n\tif (!ring->descbase) {\n\t\tb43err(ring->dev->wl, \"DMA ringmemory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tmemset(ring->descbase, 0, B43_DMA_RINGMEMSIZE);\n\n\treturn 0;\n}\n\nstatic void free_ringmemory(struct b43_dmaring *ring)\n{\n\tdma_free_coherent(ring->dev->dev->dma_dev, B43_DMA_RINGMEMSIZE,\n\t\t\t  ring->descbase, ring->dmabase);\n}\n\n/* Reset the RX DMA channel */\nstatic int b43_dmacontroller_rx_reset(struct b43_wldev *dev, u16 mmio_base,\n\t\t\t\t      enum b43_dmatype type)\n{\n\tint i;\n\tu32 value;\n\tu16 offset;\n\n\tmight_sleep();\n\n\toffset = (type == B43_DMA_64BIT) ? B43_DMA64_RXCTL : B43_DMA32_RXCTL;\n\tb43_write32(dev, mmio_base + offset, 0);\n\tfor (i = 0; i < 10; i++) {\n\t\toffset = (type == B43_DMA_64BIT) ? B43_DMA64_RXSTATUS :\n\t\t\t\t\t\t   B43_DMA32_RXSTATUS;\n\t\tvalue = b43_read32(dev, mmio_base + offset);\n\t\tif (type == B43_DMA_64BIT) {\n\t\t\tvalue &= B43_DMA64_RXSTAT;\n\t\t\tif (value == B43_DMA64_RXSTAT_DISABLED) {\n\t\t\t\ti = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tvalue &= B43_DMA32_RXSTATE;\n\t\t\tif (value == B43_DMA32_RXSTAT_DISABLED) {\n\t\t\t\ti = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tmsleep(1);\n\t}\n\tif (i != -1) {\n\t\tb43err(dev->wl, \"DMA RX reset timed out\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\n/* Reset the TX DMA channel */\nstatic int b43_dmacontroller_tx_reset(struct b43_wldev *dev, u16 mmio_base,\n\t\t\t\t      enum b43_dmatype type)\n{\n\tint i;\n\tu32 value;\n\tu16 offset;\n\n\tmight_sleep();\n\n\tfor (i = 0; i < 10; i++) {\n\t\toffset = (type == B43_DMA_64BIT) ? B43_DMA64_TXSTATUS :\n\t\t\t\t\t\t   B43_DMA32_TXSTATUS;\n\t\tvalue = b43_read32(dev, mmio_base + offset);\n\t\tif (type == B43_DMA_64BIT) {\n\t\t\tvalue &= B43_DMA64_TXSTAT;\n\t\t\tif (value == B43_DMA64_TXSTAT_DISABLED ||\n\t\t\t    value == B43_DMA64_TXSTAT_IDLEWAIT ||\n\t\t\t    value == B43_DMA64_TXSTAT_STOPPED)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tvalue &= B43_DMA32_TXSTATE;\n\t\t\tif (value == B43_DMA32_TXSTAT_DISABLED ||\n\t\t\t    value == B43_DMA32_TXSTAT_IDLEWAIT ||\n\t\t\t    value == B43_DMA32_TXSTAT_STOPPED)\n\t\t\t\tbreak;\n\t\t}\n\t\tmsleep(1);\n\t}\n\toffset = (type == B43_DMA_64BIT) ? B43_DMA64_TXCTL : B43_DMA32_TXCTL;\n\tb43_write32(dev, mmio_base + offset, 0);\n\tfor (i = 0; i < 10; i++) {\n\t\toffset = (type == B43_DMA_64BIT) ? B43_DMA64_TXSTATUS :\n\t\t\t\t\t\t   B43_DMA32_TXSTATUS;\n\t\tvalue = b43_read32(dev, mmio_base + offset);\n\t\tif (type == B43_DMA_64BIT) {\n\t\t\tvalue &= B43_DMA64_TXSTAT;\n\t\t\tif (value == B43_DMA64_TXSTAT_DISABLED) {\n\t\t\t\ti = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tvalue &= B43_DMA32_TXSTATE;\n\t\t\tif (value == B43_DMA32_TXSTAT_DISABLED) {\n\t\t\t\ti = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tmsleep(1);\n\t}\n\tif (i != -1) {\n\t\tb43err(dev->wl, \"DMA TX reset timed out\\n\");\n\t\treturn -ENODEV;\n\t}\n\t/* ensure the reset is completed. */\n\tmsleep(1);\n\n\treturn 0;\n}\n\n/* Check if a DMA mapping address is invalid. */\nstatic bool b43_dma_mapping_error(struct b43_dmaring *ring,\n\t\t\t\t  dma_addr_t addr,\n\t\t\t\t  size_t buffersize, bool dma_to_device)\n{\n\tif (unlikely(dma_mapping_error(ring->dev->dev->dma_dev, addr)))\n\t\treturn 1;\n\n\tswitch (ring->type) {\n\tcase B43_DMA_30BIT:\n\t\tif ((u64)addr + buffersize > (1ULL << 30))\n\t\t\tgoto address_error;\n\t\tbreak;\n\tcase B43_DMA_32BIT:\n\t\tif ((u64)addr + buffersize > (1ULL << 32))\n\t\t\tgoto address_error;\n\t\tbreak;\n\tcase B43_DMA_64BIT:\n\t\t/* Currently we can't have addresses beyond\n\t\t * 64bit in the kernel. */\n\t\tbreak;\n\t}\n\n\t/* The address is OK. */\n\treturn 0;\n\naddress_error:\n\t/* We can't support this address. Unmap it again. */\n\tunmap_descbuffer(ring, addr, buffersize, dma_to_device);\n\n\treturn 1;\n}\n\nstatic bool b43_rx_buffer_is_poisoned(struct b43_dmaring *ring, struct sk_buff *skb)\n{\n\tunsigned char *f = skb->data + ring->frameoffset;\n\n\treturn ((f[0] & f[1] & f[2] & f[3] & f[4] & f[5] & f[6] & f[7]) == 0xFF);\n}\n\nstatic void b43_poison_rx_buffer(struct b43_dmaring *ring, struct sk_buff *skb)\n{\n\tstruct b43_rxhdr_fw4 *rxhdr;\n\tunsigned char *frame;\n\n\t/* This poisons the RX buffer to detect DMA failures. */\n\n\trxhdr = (struct b43_rxhdr_fw4 *)(skb->data);\n\trxhdr->frame_len = 0;\n\n\tB43_WARN_ON(ring->rx_buffersize < ring->frameoffset + sizeof(struct b43_plcp_hdr6) + 2);\n\tframe = skb->data + ring->frameoffset;\n\tmemset(frame, 0xFF, sizeof(struct b43_plcp_hdr6) + 2 /* padding */);\n}\n\nstatic int setup_rx_descbuffer(struct b43_dmaring *ring,\n\t\t\t       struct b43_dmadesc_generic *desc,\n\t\t\t       struct b43_dmadesc_meta *meta, gfp_t gfp_flags)\n{\n\tdma_addr_t dmaaddr;\n\tstruct sk_buff *skb;\n\n\tB43_WARN_ON(ring->tx);\n\n\tskb = __dev_alloc_skb(ring->rx_buffersize, gfp_flags);\n\tif (unlikely(!skb))\n\t\treturn -ENOMEM;\n\tb43_poison_rx_buffer(ring, skb);\n\tdmaaddr = map_descbuffer(ring, skb->data, ring->rx_buffersize, 0);\n\tif (b43_dma_mapping_error(ring, dmaaddr, ring->rx_buffersize, 0)) {\n\t\t/* ugh. try to realloc in zone_dma */\n\t\tgfp_flags |= GFP_DMA;\n\n\t\tdev_kfree_skb_any(skb);\n\n\t\tskb = __dev_alloc_skb(ring->rx_buffersize, gfp_flags);\n\t\tif (unlikely(!skb))\n\t\t\treturn -ENOMEM;\n\t\tb43_poison_rx_buffer(ring, skb);\n\t\tdmaaddr = map_descbuffer(ring, skb->data,\n\t\t\t\t\t ring->rx_buffersize, 0);\n\t\tif (b43_dma_mapping_error(ring, dmaaddr, ring->rx_buffersize, 0)) {\n\t\t\tb43err(ring->dev->wl, \"RX DMA buffer allocation failed\\n\");\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\tmeta->skb = skb;\n\tmeta->dmaaddr = dmaaddr;\n\tring->ops->fill_descriptor(ring, desc, dmaaddr,\n\t\t\t\t   ring->rx_buffersize, 0, 0, 0);\n\n\treturn 0;\n}\n\n/* Allocate the initial descbuffers.\n * This is used for an RX ring only.\n */\nstatic int alloc_initial_descbuffers(struct b43_dmaring *ring)\n{\n\tint i, err = -ENOMEM;\n\tstruct b43_dmadesc_generic *desc;\n\tstruct b43_dmadesc_meta *meta;\n\n\tfor (i = 0; i < ring->nr_slots; i++) {\n\t\tdesc = ring->ops->idx2desc(ring, i, &meta);\n\n\t\terr = setup_rx_descbuffer(ring, desc, meta, GFP_KERNEL);\n\t\tif (err) {\n\t\t\tb43err(ring->dev->wl,\n\t\t\t       \"Failed to allocate initial descbuffers\\n\");\n\t\t\tgoto err_unwind;\n\t\t}\n\t}\n\tmb();\n\tring->used_slots = ring->nr_slots;\n\terr = 0;\n      out:\n\treturn err;\n\n      err_unwind:\n\tfor (i--; i >= 0; i--) {\n\t\tdesc = ring->ops->idx2desc(ring, i, &meta);\n\n\t\tunmap_descbuffer(ring, meta->dmaaddr, ring->rx_buffersize, 0);\n\t\tdev_kfree_skb(meta->skb);\n\t}\n\tgoto out;\n}\n\n/* Do initial setup of the DMA controller.\n * Reset the controller, write the ring busaddress\n * and switch the \"enable\" bit on.\n */\nstatic int dmacontroller_setup(struct b43_dmaring *ring)\n{\n\tint err = 0;\n\tu32 value;\n\tu32 addrext;\n\tu32 trans = ssb_dma_translation(ring->dev->dev);\n\n\tif (ring->tx) {\n\t\tif (ring->type == B43_DMA_64BIT) {\n\t\t\tu64 ringbase = (u64) (ring->dmabase);\n\n\t\t\taddrext = ((ringbase >> 32) & SSB_DMA_TRANSLATION_MASK)\n\t\t\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\t\t\tvalue = B43_DMA64_TXENABLE;\n\t\t\tvalue |= (addrext << B43_DMA64_TXADDREXT_SHIFT)\n\t\t\t    & B43_DMA64_TXADDREXT_MASK;\n\t\t\tb43_dma_write(ring, B43_DMA64_TXCTL, value);\n\t\t\tb43_dma_write(ring, B43_DMA64_TXRINGLO,\n\t\t\t\t      (ringbase & 0xFFFFFFFF));\n\t\t\tb43_dma_write(ring, B43_DMA64_TXRINGHI,\n\t\t\t\t      ((ringbase >> 32) &\n\t\t\t\t       ~SSB_DMA_TRANSLATION_MASK)\n\t\t\t\t      | (trans << 1));\n\t\t} else {\n\t\t\tu32 ringbase = (u32) (ring->dmabase);\n\n\t\t\taddrext = (ringbase & SSB_DMA_TRANSLATION_MASK)\n\t\t\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\t\t\tvalue = B43_DMA32_TXENABLE;\n\t\t\tvalue |= (addrext << B43_DMA32_TXADDREXT_SHIFT)\n\t\t\t    & B43_DMA32_TXADDREXT_MASK;\n\t\t\tb43_dma_write(ring, B43_DMA32_TXCTL, value);\n\t\t\tb43_dma_write(ring, B43_DMA32_TXRING,\n\t\t\t\t      (ringbase & ~SSB_DMA_TRANSLATION_MASK)\n\t\t\t\t      | trans);\n\t\t}\n\t} else {\n\t\terr = alloc_initial_descbuffers(ring);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tif (ring->type == B43_DMA_64BIT) {\n\t\t\tu64 ringbase = (u64) (ring->dmabase);\n\n\t\t\taddrext = ((ringbase >> 32) & SSB_DMA_TRANSLATION_MASK)\n\t\t\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\t\t\tvalue = (ring->frameoffset << B43_DMA64_RXFROFF_SHIFT);\n\t\t\tvalue |= B43_DMA64_RXENABLE;\n\t\t\tvalue |= (addrext << B43_DMA64_RXADDREXT_SHIFT)\n\t\t\t    & B43_DMA64_RXADDREXT_MASK;\n\t\t\tb43_dma_write(ring, B43_DMA64_RXCTL, value);\n\t\t\tb43_dma_write(ring, B43_DMA64_RXRINGLO,\n\t\t\t\t      (ringbase & 0xFFFFFFFF));\n\t\t\tb43_dma_write(ring, B43_DMA64_RXRINGHI,\n\t\t\t\t      ((ringbase >> 32) &\n\t\t\t\t       ~SSB_DMA_TRANSLATION_MASK)\n\t\t\t\t      | (trans << 1));\n\t\t\tb43_dma_write(ring, B43_DMA64_RXINDEX, ring->nr_slots *\n\t\t\t\t      sizeof(struct b43_dmadesc64));\n\t\t} else {\n\t\t\tu32 ringbase = (u32) (ring->dmabase);\n\n\t\t\taddrext = (ringbase & SSB_DMA_TRANSLATION_MASK)\n\t\t\t    >> SSB_DMA_TRANSLATION_SHIFT;\n\t\t\tvalue = (ring->frameoffset << B43_DMA32_RXFROFF_SHIFT);\n\t\t\tvalue |= B43_DMA32_RXENABLE;\n\t\t\tvalue |= (addrext << B43_DMA32_RXADDREXT_SHIFT)\n\t\t\t    & B43_DMA32_RXADDREXT_MASK;\n\t\t\tb43_dma_write(ring, B43_DMA32_RXCTL, value);\n\t\t\tb43_dma_write(ring, B43_DMA32_RXRING,\n\t\t\t\t      (ringbase & ~SSB_DMA_TRANSLATION_MASK)\n\t\t\t\t      | trans);\n\t\t\tb43_dma_write(ring, B43_DMA32_RXINDEX, ring->nr_slots *\n\t\t\t\t      sizeof(struct b43_dmadesc32));\n\t\t}\n\t}\n\nout:\n\treturn err;\n}\n\n/* Shutdown the DMA controller. */\nstatic void dmacontroller_cleanup(struct b43_dmaring *ring)\n{\n\tif (ring->tx) {\n\t\tb43_dmacontroller_tx_reset(ring->dev, ring->mmio_base,\n\t\t\t\t\t   ring->type);\n\t\tif (ring->type == B43_DMA_64BIT) {\n\t\t\tb43_dma_write(ring, B43_DMA64_TXRINGLO, 0);\n\t\t\tb43_dma_write(ring, B43_DMA64_TXRINGHI, 0);\n\t\t} else\n\t\t\tb43_dma_write(ring, B43_DMA32_TXRING, 0);\n\t} else {\n\t\tb43_dmacontroller_rx_reset(ring->dev, ring->mmio_base,\n\t\t\t\t\t   ring->type);\n\t\tif (ring->type == B43_DMA_64BIT) {\n\t\t\tb43_dma_write(ring, B43_DMA64_RXRINGLO, 0);\n\t\t\tb43_dma_write(ring, B43_DMA64_RXRINGHI, 0);\n\t\t} else\n\t\t\tb43_dma_write(ring, B43_DMA32_RXRING, 0);\n\t}\n}\n\nstatic void free_all_descbuffers(struct b43_dmaring *ring)\n{\n\tstruct b43_dmadesc_generic *desc;\n\tstruct b43_dmadesc_meta *meta;\n\tint i;\n\n\tif (!ring->used_slots)\n\t\treturn;\n\tfor (i = 0; i < ring->nr_slots; i++) {\n\t\tdesc = ring->ops->idx2desc(ring, i, &meta);\n\n\t\tif (!meta->skb || b43_dma_ptr_is_poisoned(meta->skb)) {\n\t\t\tB43_WARN_ON(!ring->tx);\n\t\t\tcontinue;\n\t\t}\n\t\tif (ring->tx) {\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr,\n\t\t\t\t\t meta->skb->len, 1);\n\t\t} else {\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr,\n\t\t\t\t\t ring->rx_buffersize, 0);\n\t\t}\n\t\tfree_descriptor_buffer(ring, meta);\n\t}\n}\n\nstatic u64 supported_dma_mask(struct b43_wldev *dev)\n{\n\tu32 tmp;\n\tu16 mmio_base;\n\n\ttmp = b43_read32(dev, SSB_TMSHIGH);\n\tif (tmp & SSB_TMSHIGH_DMA64)\n\t\treturn DMA_BIT_MASK(64);\n\tmmio_base = b43_dmacontroller_base(0, 0);\n\tb43_write32(dev, mmio_base + B43_DMA32_TXCTL, B43_DMA32_TXADDREXT_MASK);\n\ttmp = b43_read32(dev, mmio_base + B43_DMA32_TXCTL);\n\tif (tmp & B43_DMA32_TXADDREXT_MASK)\n\t\treturn DMA_BIT_MASK(32);\n\n\treturn DMA_BIT_MASK(30);\n}\n\nstatic enum b43_dmatype dma_mask_to_engine_type(u64 dmamask)\n{\n\tif (dmamask == DMA_BIT_MASK(30))\n\t\treturn B43_DMA_30BIT;\n\tif (dmamask == DMA_BIT_MASK(32))\n\t\treturn B43_DMA_32BIT;\n\tif (dmamask == DMA_BIT_MASK(64))\n\t\treturn B43_DMA_64BIT;\n\tB43_WARN_ON(1);\n\treturn B43_DMA_30BIT;\n}\n\n/* Main initialization function. */\nstatic\nstruct b43_dmaring *b43_setup_dmaring(struct b43_wldev *dev,\n\t\t\t\t      int controller_index,\n\t\t\t\t      int for_tx,\n\t\t\t\t      enum b43_dmatype type)\n{\n\tstruct b43_dmaring *ring;\n\tint i, err;\n\tdma_addr_t dma_test;\n\n\tring = kzalloc(sizeof(*ring), GFP_KERNEL);\n\tif (!ring)\n\t\tgoto out;\n\n\tring->nr_slots = B43_RXRING_SLOTS;\n\tif (for_tx)\n\t\tring->nr_slots = B43_TXRING_SLOTS;\n\n\tring->meta = kcalloc(ring->nr_slots, sizeof(struct b43_dmadesc_meta),\n\t\t\t     GFP_KERNEL);\n\tif (!ring->meta)\n\t\tgoto err_kfree_ring;\n\tfor (i = 0; i < ring->nr_slots; i++)\n\t\tring->meta->skb = B43_DMA_PTR_POISON;\n\n\tring->type = type;\n\tring->dev = dev;\n\tring->mmio_base = b43_dmacontroller_base(type, controller_index);\n\tring->index = controller_index;\n\tif (type == B43_DMA_64BIT)\n\t\tring->ops = &dma64_ops;\n\telse\n\t\tring->ops = &dma32_ops;\n\tif (for_tx) {\n\t\tring->tx = 1;\n\t\tring->current_slot = -1;\n\t} else {\n\t\tif (ring->index == 0) {\n\t\t\tring->rx_buffersize = B43_DMA0_RX_BUFFERSIZE;\n\t\t\tring->frameoffset = B43_DMA0_RX_FRAMEOFFSET;\n\t\t} else\n\t\t\tB43_WARN_ON(1);\n\t}\n#ifdef CONFIG_B43_DEBUG\n\tring->last_injected_overflow = jiffies;\n#endif\n\n\tif (for_tx) {\n\t\t/* Assumption: B43_TXRING_SLOTS can be divided by TX_SLOTS_PER_FRAME */\n\t\tBUILD_BUG_ON(B43_TXRING_SLOTS % TX_SLOTS_PER_FRAME != 0);\n\n\t\tring->txhdr_cache = kcalloc(ring->nr_slots / TX_SLOTS_PER_FRAME,\n\t\t\t\t\t    b43_txhdr_size(dev),\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!ring->txhdr_cache)\n\t\t\tgoto err_kfree_meta;\n\n\t\t/* test for ability to dma to txhdr_cache */\n\t\tdma_test = dma_map_single(dev->dev->dma_dev,\n\t\t\t\t\t  ring->txhdr_cache,\n\t\t\t\t\t  b43_txhdr_size(dev),\n\t\t\t\t\t  DMA_TO_DEVICE);\n\n\t\tif (b43_dma_mapping_error(ring, dma_test,\n\t\t\t\t\t  b43_txhdr_size(dev), 1)) {\n\t\t\t/* ugh realloc */\n\t\t\tkfree(ring->txhdr_cache);\n\t\t\tring->txhdr_cache = kcalloc(ring->nr_slots / TX_SLOTS_PER_FRAME,\n\t\t\t\t\t\t    b43_txhdr_size(dev),\n\t\t\t\t\t\t    GFP_KERNEL | GFP_DMA);\n\t\t\tif (!ring->txhdr_cache)\n\t\t\t\tgoto err_kfree_meta;\n\n\t\t\tdma_test = dma_map_single(dev->dev->dma_dev,\n\t\t\t\t\t\t  ring->txhdr_cache,\n\t\t\t\t\t\t  b43_txhdr_size(dev),\n\t\t\t\t\t\t  DMA_TO_DEVICE);\n\n\t\t\tif (b43_dma_mapping_error(ring, dma_test,\n\t\t\t\t\t\t  b43_txhdr_size(dev), 1)) {\n\n\t\t\t\tb43err(dev->wl,\n\t\t\t\t       \"TXHDR DMA allocation failed\\n\");\n\t\t\t\tgoto err_kfree_txhdr_cache;\n\t\t\t}\n\t\t}\n\n\t\tdma_unmap_single(dev->dev->dma_dev,\n\t\t\t\t dma_test, b43_txhdr_size(dev),\n\t\t\t\t DMA_TO_DEVICE);\n\t}\n\n\terr = alloc_ringmemory(ring);\n\tif (err)\n\t\tgoto err_kfree_txhdr_cache;\n\terr = dmacontroller_setup(ring);\n\tif (err)\n\t\tgoto err_free_ringmemory;\n\n      out:\n\treturn ring;\n\n      err_free_ringmemory:\n\tfree_ringmemory(ring);\n      err_kfree_txhdr_cache:\n\tkfree(ring->txhdr_cache);\n      err_kfree_meta:\n\tkfree(ring->meta);\n      err_kfree_ring:\n\tkfree(ring);\n\tring = NULL;\n\tgoto out;\n}\n\n#define divide(a, b)\t({\t\\\n\ttypeof(a) __a = a;\t\\\n\tdo_div(__a, b);\t\t\\\n\t__a;\t\t\t\\\n  })\n\n#define modulo(a, b)\t({\t\\\n\ttypeof(a) __a = a;\t\\\n\tdo_div(__a, b);\t\t\\\n  })\n\n/* Main cleanup function. */\nstatic void b43_destroy_dmaring(struct b43_dmaring *ring,\n\t\t\t\tconst char *ringname)\n{\n\tif (!ring)\n\t\treturn;\n\n#ifdef CONFIG_B43_DEBUG\n\t{\n\t\t/* Print some statistics. */\n\t\tu64 failed_packets = ring->nr_failed_tx_packets;\n\t\tu64 succeed_packets = ring->nr_succeed_tx_packets;\n\t\tu64 nr_packets = failed_packets + succeed_packets;\n\t\tu64 permille_failed = 0, average_tries = 0;\n\n\t\tif (nr_packets)\n\t\t\tpermille_failed = divide(failed_packets * 1000, nr_packets);\n\t\tif (nr_packets)\n\t\t\taverage_tries = divide(ring->nr_total_packet_tries * 100, nr_packets);\n\n\t\tb43dbg(ring->dev->wl, \"DMA-%u %s: \"\n\t\t       \"Used slots %d/%d, Failed frames %llu/%llu = %llu.%01llu%%, \"\n\t\t       \"Average tries %llu.%02llu\\n\",\n\t\t       (unsigned int)(ring->type), ringname,\n\t\t       ring->max_used_slots,\n\t\t       ring->nr_slots,\n\t\t       (unsigned long long)failed_packets,\n\t\t       (unsigned long long)nr_packets,\n\t\t       (unsigned long long)divide(permille_failed, 10),\n\t\t       (unsigned long long)modulo(permille_failed, 10),\n\t\t       (unsigned long long)divide(average_tries, 100),\n\t\t       (unsigned long long)modulo(average_tries, 100));\n\t}\n#endif /* DEBUG */\n\n\t/* Device IRQs are disabled prior entering this function,\n\t * so no need to take care of concurrency with rx handler stuff.\n\t */\n\tdmacontroller_cleanup(ring);\n\tfree_all_descbuffers(ring);\n\tfree_ringmemory(ring);\n\n\tkfree(ring->txhdr_cache);\n\tkfree(ring->meta);\n\tkfree(ring);\n}\n\n#define destroy_ring(dma, ring) do {\t\t\t\t\\\n\tb43_destroy_dmaring((dma)->ring, __stringify(ring));\t\\\n\t(dma)->ring = NULL;\t\t\t\t\t\\\n    } while (0)\n\nvoid b43_dma_free(struct b43_wldev *dev)\n{\n\tstruct b43_dma *dma;\n\n\tif (b43_using_pio_transfers(dev))\n\t\treturn;\n\tdma = &dev->dma;\n\n\tdestroy_ring(dma, rx_ring);\n\tdestroy_ring(dma, tx_ring_AC_BK);\n\tdestroy_ring(dma, tx_ring_AC_BE);\n\tdestroy_ring(dma, tx_ring_AC_VI);\n\tdestroy_ring(dma, tx_ring_AC_VO);\n\tdestroy_ring(dma, tx_ring_mcast);\n}\n\nstatic int b43_dma_set_mask(struct b43_wldev *dev, u64 mask)\n{\n\tu64 orig_mask = mask;\n\tbool fallback = 0;\n\tint err;\n\n\t/* Try to set the DMA mask. If it fails, try falling back to a\n\t * lower mask, as we can always also support a lower one. */\n\twhile (1) {\n\t\terr = dma_set_mask(dev->dev->dma_dev, mask);\n\t\tif (!err) {\n\t\t\terr = dma_set_coherent_mask(dev->dev->dma_dev, mask);\n\t\t\tif (!err)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (mask == DMA_BIT_MASK(64)) {\n\t\t\tmask = DMA_BIT_MASK(32);\n\t\t\tfallback = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (mask == DMA_BIT_MASK(32)) {\n\t\t\tmask = DMA_BIT_MASK(30);\n\t\t\tfallback = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tb43err(dev->wl, \"The machine/kernel does not support \"\n\t\t       \"the required %u-bit DMA mask\\n\",\n\t\t       (unsigned int)dma_mask_to_engine_type(orig_mask));\n\t\treturn -EOPNOTSUPP;\n\t}\n\tif (fallback) {\n\t\tb43info(dev->wl, \"DMA mask fallback from %u-bit to %u-bit\\n\",\n\t\t\t(unsigned int)dma_mask_to_engine_type(orig_mask),\n\t\t\t(unsigned int)dma_mask_to_engine_type(mask));\n\t}\n\n\treturn 0;\n}\n\nint b43_dma_init(struct b43_wldev *dev)\n{\n\tstruct b43_dma *dma = &dev->dma;\n\tint err;\n\tu64 dmamask;\n\tenum b43_dmatype type;\n\n\tdmamask = supported_dma_mask(dev);\n\ttype = dma_mask_to_engine_type(dmamask);\n\terr = b43_dma_set_mask(dev, dmamask);\n\tif (err)\n\t\treturn err;\n\n\terr = -ENOMEM;\n\t/* setup TX DMA channels. */\n\tdma->tx_ring_AC_BK = b43_setup_dmaring(dev, 0, 1, type);\n\tif (!dma->tx_ring_AC_BK)\n\t\tgoto out;\n\n\tdma->tx_ring_AC_BE = b43_setup_dmaring(dev, 1, 1, type);\n\tif (!dma->tx_ring_AC_BE)\n\t\tgoto err_destroy_bk;\n\n\tdma->tx_ring_AC_VI = b43_setup_dmaring(dev, 2, 1, type);\n\tif (!dma->tx_ring_AC_VI)\n\t\tgoto err_destroy_be;\n\n\tdma->tx_ring_AC_VO = b43_setup_dmaring(dev, 3, 1, type);\n\tif (!dma->tx_ring_AC_VO)\n\t\tgoto err_destroy_vi;\n\n\tdma->tx_ring_mcast = b43_setup_dmaring(dev, 4, 1, type);\n\tif (!dma->tx_ring_mcast)\n\t\tgoto err_destroy_vo;\n\n\t/* setup RX DMA channel. */\n\tdma->rx_ring = b43_setup_dmaring(dev, 0, 0, type);\n\tif (!dma->rx_ring)\n\t\tgoto err_destroy_mcast;\n\n\t/* No support for the TX status DMA ring. */\n\tB43_WARN_ON(dev->dev->id.revision < 5);\n\n\tb43dbg(dev->wl, \"%u-bit DMA initialized\\n\",\n\t       (unsigned int)type);\n\terr = 0;\nout:\n\treturn err;\n\nerr_destroy_mcast:\n\tdestroy_ring(dma, tx_ring_mcast);\nerr_destroy_vo:\n\tdestroy_ring(dma, tx_ring_AC_VO);\nerr_destroy_vi:\n\tdestroy_ring(dma, tx_ring_AC_VI);\nerr_destroy_be:\n\tdestroy_ring(dma, tx_ring_AC_BE);\nerr_destroy_bk:\n\tdestroy_ring(dma, tx_ring_AC_BK);\n\treturn err;\n}\n\n/* Generate a cookie for the TX header. */\nstatic u16 generate_cookie(struct b43_dmaring *ring, int slot)\n{\n\tu16 cookie;\n\n\t/* Use the upper 4 bits of the cookie as\n\t * DMA controller ID and store the slot number\n\t * in the lower 12 bits.\n\t * Note that the cookie must never be 0, as this\n\t * is a special value used in RX path.\n\t * It can also not be 0xFFFF because that is special\n\t * for multicast frames.\n\t */\n\tcookie = (((u16)ring->index + 1) << 12);\n\tB43_WARN_ON(slot & ~0x0FFF);\n\tcookie |= (u16)slot;\n\n\treturn cookie;\n}\n\n/* Inspect a cookie and find out to which controller/slot it belongs. */\nstatic\nstruct b43_dmaring *parse_cookie(struct b43_wldev *dev, u16 cookie, int *slot)\n{\n\tstruct b43_dma *dma = &dev->dma;\n\tstruct b43_dmaring *ring = NULL;\n\n\tswitch (cookie & 0xF000) {\n\tcase 0x1000:\n\t\tring = dma->tx_ring_AC_BK;\n\t\tbreak;\n\tcase 0x2000:\n\t\tring = dma->tx_ring_AC_BE;\n\t\tbreak;\n\tcase 0x3000:\n\t\tring = dma->tx_ring_AC_VI;\n\t\tbreak;\n\tcase 0x4000:\n\t\tring = dma->tx_ring_AC_VO;\n\t\tbreak;\n\tcase 0x5000:\n\t\tring = dma->tx_ring_mcast;\n\t\tbreak;\n\t}\n\t*slot = (cookie & 0x0FFF);\n\tif (unlikely(!ring || *slot < 0 || *slot >= ring->nr_slots)) {\n\t\tb43dbg(dev->wl, \"TX-status contains \"\n\t\t       \"invalid cookie: 0x%04X\\n\", cookie);\n\t\treturn NULL;\n\t}\n\n\treturn ring;\n}\n\nstatic int dma_tx_fragment(struct b43_dmaring *ring,\n\t\t\t   struct sk_buff *skb)\n{\n\tconst struct b43_dma_ops *ops = ring->ops;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct b43_private_tx_info *priv_info = b43_get_priv_tx_info(info);\n\tu8 *header;\n\tint slot, old_top_slot, old_used_slots;\n\tint err;\n\tstruct b43_dmadesc_generic *desc;\n\tstruct b43_dmadesc_meta *meta;\n\tstruct b43_dmadesc_meta *meta_hdr;\n\tu16 cookie;\n\tsize_t hdrsize = b43_txhdr_size(ring->dev);\n\n\t/* Important note: If the number of used DMA slots per TX frame\n\t * is changed here, the TX_SLOTS_PER_FRAME definition at the top of\n\t * the file has to be updated, too!\n\t */\n\n\told_top_slot = ring->current_slot;\n\told_used_slots = ring->used_slots;\n\n\t/* Get a slot for the header. */\n\tslot = request_slot(ring);\n\tdesc = ops->idx2desc(ring, slot, &meta_hdr);\n\tmemset(meta_hdr, 0, sizeof(*meta_hdr));\n\n\theader = &(ring->txhdr_cache[(slot / TX_SLOTS_PER_FRAME) * hdrsize]);\n\tcookie = generate_cookie(ring, slot);\n\terr = b43_generate_txhdr(ring->dev, header,\n\t\t\t\t skb, info, cookie);\n\tif (unlikely(err)) {\n\t\tring->current_slot = old_top_slot;\n\t\tring->used_slots = old_used_slots;\n\t\treturn err;\n\t}\n\n\tmeta_hdr->dmaaddr = map_descbuffer(ring, (unsigned char *)header,\n\t\t\t\t\t   hdrsize, 1);\n\tif (b43_dma_mapping_error(ring, meta_hdr->dmaaddr, hdrsize, 1)) {\n\t\tring->current_slot = old_top_slot;\n\t\tring->used_slots = old_used_slots;\n\t\treturn -EIO;\n\t}\n\tops->fill_descriptor(ring, desc, meta_hdr->dmaaddr,\n\t\t\t     hdrsize, 1, 0, 0);\n\n\t/* Get a slot for the payload. */\n\tslot = request_slot(ring);\n\tdesc = ops->idx2desc(ring, slot, &meta);\n\tmemset(meta, 0, sizeof(*meta));\n\n\tmeta->skb = skb;\n\tmeta->is_last_fragment = 1;\n\tpriv_info->bouncebuffer = NULL;\n\n\tmeta->dmaaddr = map_descbuffer(ring, skb->data, skb->len, 1);\n\t/* create a bounce buffer in zone_dma on mapping failure. */\n\tif (b43_dma_mapping_error(ring, meta->dmaaddr, skb->len, 1)) {\n\t\tpriv_info->bouncebuffer = kmemdup(skb->data, skb->len,\n\t\t\t\t\t\t  GFP_ATOMIC | GFP_DMA);\n\t\tif (!priv_info->bouncebuffer) {\n\t\t\tring->current_slot = old_top_slot;\n\t\t\tring->used_slots = old_used_slots;\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_unmap_hdr;\n\t\t}\n\n\t\tmeta->dmaaddr = map_descbuffer(ring, priv_info->bouncebuffer, skb->len, 1);\n\t\tif (b43_dma_mapping_error(ring, meta->dmaaddr, skb->len, 1)) {\n\t\t\tkfree(priv_info->bouncebuffer);\n\t\t\tpriv_info->bouncebuffer = NULL;\n\t\t\tring->current_slot = old_top_slot;\n\t\t\tring->used_slots = old_used_slots;\n\t\t\terr = -EIO;\n\t\t\tgoto out_unmap_hdr;\n\t\t}\n\t}\n\n\tops->fill_descriptor(ring, desc, meta->dmaaddr, skb->len, 0, 1, 1);\n\n\tif (info->flags & IEEE80211_TX_CTL_SEND_AFTER_DTIM) {\n\t\t/* Tell the firmware about the cookie of the last\n\t\t * mcast frame, so it can clear the more-data bit in it. */\n\t\tb43_shm_write16(ring->dev, B43_SHM_SHARED,\n\t\t\t\tB43_SHM_SH_MCASTCOOKIE, cookie);\n\t}\n\t/* Now transfer the whole frame. */\n\twmb();\n\tops->poke_tx(ring, next_slot(ring, slot));\n\treturn 0;\n\nout_unmap_hdr:\n\tunmap_descbuffer(ring, meta_hdr->dmaaddr,\n\t\t\t hdrsize, 1);\n\treturn err;\n}\n\nstatic inline int should_inject_overflow(struct b43_dmaring *ring)\n{\n#ifdef CONFIG_B43_DEBUG\n\tif (unlikely(b43_debug(ring->dev, B43_DBG_DMAOVERFLOW))) {\n\t\t/* Check if we should inject another ringbuffer overflow\n\t\t * to test handling of this situation in the stack. */\n\t\tunsigned long next_overflow;\n\n\t\tnext_overflow = ring->last_injected_overflow + HZ;\n\t\tif (time_after(jiffies, next_overflow)) {\n\t\t\tring->last_injected_overflow = jiffies;\n\t\t\tb43dbg(ring->dev->wl,\n\t\t\t       \"Injecting TX ring overflow on \"\n\t\t\t       \"DMA controller %d\\n\", ring->index);\n\t\t\treturn 1;\n\t\t}\n\t}\n#endif /* CONFIG_B43_DEBUG */\n\treturn 0;\n}\n\n/* Static mapping of mac80211's queues (priorities) to b43 DMA rings. */\nstatic struct b43_dmaring *select_ring_by_priority(struct b43_wldev *dev,\n\t\t\t\t\t\t   u8 queue_prio)\n{\n\tstruct b43_dmaring *ring;\n\n\tif (dev->qos_enabled) {\n\t\t/* 0 = highest priority */\n\t\tswitch (queue_prio) {\n\t\tdefault:\n\t\t\tB43_WARN_ON(1);\n\t\t\t/* fallthrough */\n\t\tcase 0:\n\t\t\tring = dev->dma.tx_ring_AC_VO;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tring = dev->dma.tx_ring_AC_VI;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tring = dev->dma.tx_ring_AC_BE;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tring = dev->dma.tx_ring_AC_BK;\n\t\t\tbreak;\n\t\t}\n\t} else\n\t\tring = dev->dma.tx_ring_AC_BE;\n\n\treturn ring;\n}\n\nint b43_dma_tx(struct b43_wldev *dev, struct sk_buff *skb)\n{\n\tstruct b43_dmaring *ring;\n\tstruct ieee80211_hdr *hdr;\n\tint err = 0;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\n\thdr = (struct ieee80211_hdr *)skb->data;\n\tif (info->flags & IEEE80211_TX_CTL_SEND_AFTER_DTIM) {\n\t\t/* The multicast ring will be sent after the DTIM */\n\t\tring = dev->dma.tx_ring_mcast;\n\t\t/* Set the more-data bit. Ucode will clear it on\n\t\t * the last frame for us. */\n\t\thdr->frame_control |= cpu_to_le16(IEEE80211_FCTL_MOREDATA);\n\t} else {\n\t\t/* Decide by priority where to put this frame. */\n\t\tring = select_ring_by_priority(\n\t\t\tdev, skb_get_queue_mapping(skb));\n\t}\n\n\tB43_WARN_ON(!ring->tx);\n\n\tif (unlikely(ring->stopped)) {\n\t\t/* We get here only because of a bug in mac80211.\n\t\t * Because of a race, one packet may be queued after\n\t\t * the queue is stopped, thus we got called when we shouldn't.\n\t\t * For now, just refuse the transmit. */\n\t\tif (b43_debug(dev, B43_DBG_DMAVERBOSE))\n\t\t\tb43err(dev->wl, \"Packet after queue stopped\\n\");\n\t\terr = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tif (unlikely(WARN_ON(free_slots(ring) < TX_SLOTS_PER_FRAME))) {\n\t\t/* If we get here, we have a real error with the queue\n\t\t * full, but queues not stopped. */\n\t\tb43err(dev->wl, \"DMA queue overflow\\n\");\n\t\terr = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\t/* Assign the queue number to the ring (if not already done before)\n\t * so TX status handling can use it. The queue to ring mapping is\n\t * static, so we don't need to store it per frame. */\n\tring->queue_prio = skb_get_queue_mapping(skb);\n\n\terr = dma_tx_fragment(ring, skb);\n\tif (unlikely(err == -ENOKEY)) {\n\t\t/* Drop this packet, as we don't have the encryption key\n\t\t * anymore and must not transmit it unencrypted. */\n\t\tdev_kfree_skb_any(skb);\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\tif (unlikely(err)) {\n\t\tb43err(dev->wl, \"DMA tx mapping failure\\n\");\n\t\tgoto out;\n\t}\n\tif ((free_slots(ring) < TX_SLOTS_PER_FRAME) ||\n\t    should_inject_overflow(ring)) {\n\t\t/* This TX ring is full. */\n\t\tieee80211_stop_queue(dev->wl->hw, skb_get_queue_mapping(skb));\n\t\tring->stopped = 1;\n\t\tif (b43_debug(dev, B43_DBG_DMAVERBOSE)) {\n\t\t\tb43dbg(dev->wl, \"Stopped TX ring %d\\n\", ring->index);\n\t\t}\n\t}\nout:\n\n\treturn err;\n}\n\nvoid b43_dma_handle_txstatus(struct b43_wldev *dev,\n\t\t\t     const struct b43_txstatus *status)\n{\n\tconst struct b43_dma_ops *ops;\n\tstruct b43_dmaring *ring;\n\tstruct b43_dmadesc_generic *desc;\n\tstruct b43_dmadesc_meta *meta;\n\tint slot, firstused;\n\tbool frame_succeed;\n\n\tring = parse_cookie(dev, status->cookie, &slot);\n\tif (unlikely(!ring))\n\t\treturn;\n\tB43_WARN_ON(!ring->tx);\n\n\t/* Sanity check: TX packets are processed in-order on one ring.\n\t * Check if the slot deduced from the cookie really is the first\n\t * used slot. */\n\tfirstused = ring->current_slot - ring->used_slots + 1;\n\tif (firstused < 0)\n\t\tfirstused = ring->nr_slots + firstused;\n\tif (unlikely(slot != firstused)) {\n\t\t/* This possibly is a firmware bug and will result in\n\t\t * malfunction, memory leaks and/or stall of DMA functionality. */\n\t\tb43dbg(dev->wl, \"Out of order TX status report on DMA ring %d. \"\n\t\t       \"Expected %d, but got %d\\n\",\n\t\t       ring->index, firstused, slot);\n\t\treturn;\n\t}\n\n\tops = ring->ops;\n\twhile (1) {\n\t\tB43_WARN_ON(slot < 0 || slot >= ring->nr_slots);\n\t\tdesc = ops->idx2desc(ring, slot, &meta);\n\n\t\tif (b43_dma_ptr_is_poisoned(meta->skb)) {\n\t\t\tb43dbg(dev->wl, \"Poisoned TX slot %d (first=%d) \"\n\t\t\t       \"on ring %d\\n\",\n\t\t\t       slot, firstused, ring->index);\n\t\t\tbreak;\n\t\t}\n\t\tif (meta->skb) {\n\t\t\tstruct b43_private_tx_info *priv_info =\n\t\t\t\tb43_get_priv_tx_info(IEEE80211_SKB_CB(meta->skb));\n\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr, meta->skb->len, 1);\n\t\t\tkfree(priv_info->bouncebuffer);\n\t\t\tpriv_info->bouncebuffer = NULL;\n\t\t} else {\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr,\n\t\t\t\t\t b43_txhdr_size(dev), 1);\n\t\t}\n\n\t\tif (meta->is_last_fragment) {\n\t\t\tstruct ieee80211_tx_info *info;\n\n\t\t\tif (unlikely(!meta->skb)) {\n\t\t\t\t/* This is a scatter-gather fragment of a frame, so\n\t\t\t\t * the skb pointer must not be NULL. */\n\t\t\t\tb43dbg(dev->wl, \"TX status unexpected NULL skb \"\n\t\t\t\t       \"at slot %d (first=%d) on ring %d\\n\",\n\t\t\t\t       slot, firstused, ring->index);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tinfo = IEEE80211_SKB_CB(meta->skb);\n\n\t\t\t/*\n\t\t\t * Call back to inform the ieee80211 subsystem about\n\t\t\t * the status of the transmission.\n\t\t\t */\n\t\t\tframe_succeed = b43_fill_txstatus_report(dev, info, status);\n#ifdef CONFIG_B43_DEBUG\n\t\t\tif (frame_succeed)\n\t\t\t\tring->nr_succeed_tx_packets++;\n\t\t\telse\n\t\t\t\tring->nr_failed_tx_packets++;\n\t\t\tring->nr_total_packet_tries += status->frame_count;\n#endif /* DEBUG */\n\t\t\tieee80211_tx_status(dev->wl->hw, meta->skb);\n\n\t\t\t/* skb will be freed by ieee80211_tx_status().\n\t\t\t * Poison our pointer. */\n\t\t\tmeta->skb = B43_DMA_PTR_POISON;\n\t\t} else {\n\t\t\t/* No need to call free_descriptor_buffer here, as\n\t\t\t * this is only the txhdr, which is not allocated.\n\t\t\t */\n\t\t\tif (unlikely(meta->skb)) {\n\t\t\t\tb43dbg(dev->wl, \"TX status unexpected non-NULL skb \"\n\t\t\t\t       \"at slot %d (first=%d) on ring %d\\n\",\n\t\t\t\t       slot, firstused, ring->index);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* Everything unmapped and free'd. So it's not used anymore. */\n\t\tring->used_slots--;\n\n\t\tif (meta->is_last_fragment) {\n\t\t\t/* This is the last scatter-gather\n\t\t\t * fragment of the frame. We are done. */\n\t\t\tbreak;\n\t\t}\n\t\tslot = next_slot(ring, slot);\n\t}\n\tif (ring->stopped) {\n\t\tB43_WARN_ON(free_slots(ring) < TX_SLOTS_PER_FRAME);\n\t\tieee80211_wake_queue(dev->wl->hw, ring->queue_prio);\n\t\tring->stopped = 0;\n\t\tif (b43_debug(dev, B43_DBG_DMAVERBOSE)) {\n\t\t\tb43dbg(dev->wl, \"Woke up TX ring %d\\n\", ring->index);\n\t\t}\n\t}\n}\n\nstatic void dma_rx(struct b43_dmaring *ring, int *slot)\n{\n\tconst struct b43_dma_ops *ops = ring->ops;\n\tstruct b43_dmadesc_generic *desc;\n\tstruct b43_dmadesc_meta *meta;\n\tstruct b43_rxhdr_fw4 *rxhdr;\n\tstruct sk_buff *skb;\n\tu16 len;\n\tint err;\n\tdma_addr_t dmaaddr;\n\n\tdesc = ops->idx2desc(ring, *slot, &meta);\n\n\tsync_descbuffer_for_cpu(ring, meta->dmaaddr, ring->rx_buffersize);\n\tskb = meta->skb;\n\n\trxhdr = (struct b43_rxhdr_fw4 *)skb->data;\n\tlen = le16_to_cpu(rxhdr->frame_len);\n\tif (len == 0) {\n\t\tint i = 0;\n\n\t\tdo {\n\t\t\tudelay(2);\n\t\t\tbarrier();\n\t\t\tlen = le16_to_cpu(rxhdr->frame_len);\n\t\t} while (len == 0 && i++ < 5);\n\t\tif (unlikely(len == 0)) {\n\t\t\tdmaaddr = meta->dmaaddr;\n\t\t\tgoto drop_recycle_buffer;\n\t\t}\n\t}\n\tif (unlikely(b43_rx_buffer_is_poisoned(ring, skb))) {\n\t\t/* Something went wrong with the DMA.\n\t\t * The device did not touch the buffer and did not overwrite the poison. */\n\t\tb43dbg(ring->dev->wl, \"DMA RX: Dropping poisoned buffer.\\n\");\n\t\tdmaaddr = meta->dmaaddr;\n\t\tgoto drop_recycle_buffer;\n\t}\n\tif (unlikely(len + ring->frameoffset > ring->rx_buffersize)) {\n\t\t/* The data did not fit into one descriptor buffer\n\t\t * and is split over multiple buffers.\n\t\t * This should never happen, as we try to allocate buffers\n\t\t * big enough. So simply ignore this packet.\n\t\t */\n\t\tint cnt = 0;\n\t\ts32 tmp = len;\n\n\t\twhile (1) {\n\t\t\tdesc = ops->idx2desc(ring, *slot, &meta);\n\t\t\t/* recycle the descriptor buffer. */\n\t\t\tb43_poison_rx_buffer(ring, meta->skb);\n\t\t\tsync_descbuffer_for_device(ring, meta->dmaaddr,\n\t\t\t\t\t\t   ring->rx_buffersize);\n\t\t\t*slot = next_slot(ring, *slot);\n\t\t\tcnt++;\n\t\t\ttmp -= ring->rx_buffersize;\n\t\t\tif (tmp <= 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tb43err(ring->dev->wl, \"DMA RX buffer too small \"\n\t\t       \"(len: %u, buffer: %u, nr-dropped: %d)\\n\",\n\t\t       len, ring->rx_buffersize, cnt);\n\t\tgoto drop;\n\t}\n\n\tdmaaddr = meta->dmaaddr;\n\terr = setup_rx_descbuffer(ring, desc, meta, GFP_ATOMIC);\n\tif (unlikely(err)) {\n\t\tb43dbg(ring->dev->wl, \"DMA RX: setup_rx_descbuffer() failed\\n\");\n\t\tgoto drop_recycle_buffer;\n\t}\n\n\tunmap_descbuffer(ring, dmaaddr, ring->rx_buffersize, 0);\n\tskb_put(skb, len + ring->frameoffset);\n\tskb_pull(skb, ring->frameoffset);\n\n\tb43_rx(ring->dev, skb, rxhdr);\ndrop:\n\treturn;\n\ndrop_recycle_buffer:\n\t/* Poison and recycle the RX buffer. */\n\tb43_poison_rx_buffer(ring, skb);\n\tsync_descbuffer_for_device(ring, dmaaddr, ring->rx_buffersize);\n}\n\nvoid b43_dma_rx(struct b43_dmaring *ring)\n{\n\tconst struct b43_dma_ops *ops = ring->ops;\n\tint slot, current_slot;\n\tint used_slots = 0;\n\n\tB43_WARN_ON(ring->tx);\n\tcurrent_slot = ops->get_current_rxslot(ring);\n\tB43_WARN_ON(!(current_slot >= 0 && current_slot < ring->nr_slots));\n\n\tslot = ring->current_slot;\n\tfor (; slot != current_slot; slot = next_slot(ring, slot)) {\n\t\tdma_rx(ring, &slot);\n\t\tupdate_max_used_slots(ring, ++used_slots);\n\t}\n\tops->set_current_rxslot(ring, slot);\n\tring->current_slot = slot;\n}\n\nstatic void b43_dma_tx_suspend_ring(struct b43_dmaring *ring)\n{\n\tB43_WARN_ON(!ring->tx);\n\tring->ops->tx_suspend(ring);\n}\n\nstatic void b43_dma_tx_resume_ring(struct b43_dmaring *ring)\n{\n\tB43_WARN_ON(!ring->tx);\n\tring->ops->tx_resume(ring);\n}\n\nvoid b43_dma_tx_suspend(struct b43_wldev *dev)\n{\n\tb43_power_saving_ctl_bits(dev, B43_PS_AWAKE);\n\tb43_dma_tx_suspend_ring(dev->dma.tx_ring_AC_BK);\n\tb43_dma_tx_suspend_ring(dev->dma.tx_ring_AC_BE);\n\tb43_dma_tx_suspend_ring(dev->dma.tx_ring_AC_VI);\n\tb43_dma_tx_suspend_ring(dev->dma.tx_ring_AC_VO);\n\tb43_dma_tx_suspend_ring(dev->dma.tx_ring_mcast);\n}\n\nvoid b43_dma_tx_resume(struct b43_wldev *dev)\n{\n\tb43_dma_tx_resume_ring(dev->dma.tx_ring_mcast);\n\tb43_dma_tx_resume_ring(dev->dma.tx_ring_AC_VO);\n\tb43_dma_tx_resume_ring(dev->dma.tx_ring_AC_VI);\n\tb43_dma_tx_resume_ring(dev->dma.tx_ring_AC_BE);\n\tb43_dma_tx_resume_ring(dev->dma.tx_ring_AC_BK);\n\tb43_power_saving_ctl_bits(dev, 0);\n}\n\nstatic void direct_fifo_rx(struct b43_wldev *dev, enum b43_dmatype type,\n\t\t\t   u16 mmio_base, bool enable)\n{\n\tu32 ctl;\n\n\tif (type == B43_DMA_64BIT) {\n\t\tctl = b43_read32(dev, mmio_base + B43_DMA64_RXCTL);\n\t\tctl &= ~B43_DMA64_RXDIRECTFIFO;\n\t\tif (enable)\n\t\t\tctl |= B43_DMA64_RXDIRECTFIFO;\n\t\tb43_write32(dev, mmio_base + B43_DMA64_RXCTL, ctl);\n\t} else {\n\t\tctl = b43_read32(dev, mmio_base + B43_DMA32_RXCTL);\n\t\tctl &= ~B43_DMA32_RXDIRECTFIFO;\n\t\tif (enable)\n\t\t\tctl |= B43_DMA32_RXDIRECTFIFO;\n\t\tb43_write32(dev, mmio_base + B43_DMA32_RXCTL, ctl);\n\t}\n}\n\n/* Enable/Disable Direct FIFO Receive Mode (PIO) on a RX engine.\n * This is called from PIO code, so DMA structures are not available. */\nvoid b43_dma_direct_fifo_rx(struct b43_wldev *dev,\n\t\t\t    unsigned int engine_index, bool enable)\n{\n\tenum b43_dmatype type;\n\tu16 mmio_base;\n\n\ttype = dma_mask_to_engine_type(supported_dma_mask(dev));\n\n\tmmio_base = b43_dmacontroller_base(type, engine_index);\n\tdirect_fifo_rx(dev, type, mmio_base, enable);\n}\n", "#ifndef B43_DMA_H_\n#define B43_DMA_H_\n\n#include <linux/err.h>\n\n#include \"b43.h\"\n\n\n/* DMA-Interrupt reasons. */\n#define B43_DMAIRQ_FATALMASK\t((1 << 10) | (1 << 11) | (1 << 12) \\\n\t\t\t\t\t | (1 << 14) | (1 << 15))\n#define B43_DMAIRQ_NONFATALMASK\t(1 << 13)\n#define B43_DMAIRQ_RX_DONE\t\t(1 << 16)\n\n/*** 32-bit DMA Engine. ***/\n\n/* 32-bit DMA controller registers. */\n#define B43_DMA32_TXCTL\t\t\t\t0x00\n#define\t\tB43_DMA32_TXENABLE\t\t\t0x00000001\n#define\t\tB43_DMA32_TXSUSPEND\t\t\t0x00000002\n#define\t\tB43_DMA32_TXLOOPBACK\t\t0x00000004\n#define\t\tB43_DMA32_TXFLUSH\t\t\t0x00000010\n#define\t\tB43_DMA32_TXADDREXT_MASK\t\t0x00030000\n#define\t\tB43_DMA32_TXADDREXT_SHIFT\t\t16\n#define B43_DMA32_TXRING\t\t\t\t0x04\n#define B43_DMA32_TXINDEX\t\t\t\t0x08\n#define B43_DMA32_TXSTATUS\t\t\t\t0x0C\n#define\t\tB43_DMA32_TXDPTR\t\t\t0x00000FFF\n#define\t\tB43_DMA32_TXSTATE\t\t\t0x0000F000\n#define\t\t\tB43_DMA32_TXSTAT_DISABLED\t0x00000000\n#define\t\t\tB43_DMA32_TXSTAT_ACTIVE\t0x00001000\n#define\t\t\tB43_DMA32_TXSTAT_IDLEWAIT\t0x00002000\n#define\t\t\tB43_DMA32_TXSTAT_STOPPED\t0x00003000\n#define\t\t\tB43_DMA32_TXSTAT_SUSP\t0x00004000\n#define\t\tB43_DMA32_TXERROR\t\t\t0x000F0000\n#define\t\t\tB43_DMA32_TXERR_NOERR\t0x00000000\n#define\t\t\tB43_DMA32_TXERR_PROT\t0x00010000\n#define\t\t\tB43_DMA32_TXERR_UNDERRUN\t0x00020000\n#define\t\t\tB43_DMA32_TXERR_BUFREAD\t0x00030000\n#define\t\t\tB43_DMA32_TXERR_DESCREAD\t0x00040000\n#define\t\tB43_DMA32_TXACTIVE\t\t\t0xFFF00000\n#define B43_DMA32_RXCTL\t\t\t\t0x10\n#define\t\tB43_DMA32_RXENABLE\t\t\t0x00000001\n#define\t\tB43_DMA32_RXFROFF_MASK\t\t0x000000FE\n#define\t\tB43_DMA32_RXFROFF_SHIFT\t\t1\n#define\t\tB43_DMA32_RXDIRECTFIFO\t\t0x00000100\n#define\t\tB43_DMA32_RXADDREXT_MASK\t\t0x00030000\n#define\t\tB43_DMA32_RXADDREXT_SHIFT\t\t16\n#define B43_DMA32_RXRING\t\t\t\t0x14\n#define B43_DMA32_RXINDEX\t\t\t\t0x18\n#define B43_DMA32_RXSTATUS\t\t\t\t0x1C\n#define\t\tB43_DMA32_RXDPTR\t\t\t0x00000FFF\n#define\t\tB43_DMA32_RXSTATE\t\t\t0x0000F000\n#define\t\t\tB43_DMA32_RXSTAT_DISABLED\t0x00000000\n#define\t\t\tB43_DMA32_RXSTAT_ACTIVE\t0x00001000\n#define\t\t\tB43_DMA32_RXSTAT_IDLEWAIT\t0x00002000\n#define\t\t\tB43_DMA32_RXSTAT_STOPPED\t0x00003000\n#define\t\tB43_DMA32_RXERROR\t\t\t0x000F0000\n#define\t\t\tB43_DMA32_RXERR_NOERR\t0x00000000\n#define\t\t\tB43_DMA32_RXERR_PROT\t0x00010000\n#define\t\t\tB43_DMA32_RXERR_OVERFLOW\t0x00020000\n#define\t\t\tB43_DMA32_RXERR_BUFWRITE\t0x00030000\n#define\t\t\tB43_DMA32_RXERR_DESCREAD\t0x00040000\n#define\t\tB43_DMA32_RXACTIVE\t\t\t0xFFF00000\n\n/* 32-bit DMA descriptor. */\nstruct b43_dmadesc32 {\n\t__le32 control;\n\t__le32 address;\n} __packed;\n#define B43_DMA32_DCTL_BYTECNT\t\t0x00001FFF\n#define B43_DMA32_DCTL_ADDREXT_MASK\t\t0x00030000\n#define B43_DMA32_DCTL_ADDREXT_SHIFT\t16\n#define B43_DMA32_DCTL_DTABLEEND\t\t0x10000000\n#define B43_DMA32_DCTL_IRQ\t\t\t0x20000000\n#define B43_DMA32_DCTL_FRAMEEND\t\t0x40000000\n#define B43_DMA32_DCTL_FRAMESTART\t\t0x80000000\n\n/*** 64-bit DMA Engine. ***/\n\n/* 64-bit DMA controller registers. */\n#define B43_DMA64_TXCTL\t\t\t\t0x00\n#define\t\tB43_DMA64_TXENABLE\t\t\t0x00000001\n#define\t\tB43_DMA64_TXSUSPEND\t\t\t0x00000002\n#define\t\tB43_DMA64_TXLOOPBACK\t\t0x00000004\n#define\t\tB43_DMA64_TXFLUSH\t\t\t0x00000010\n#define\t\tB43_DMA64_TXADDREXT_MASK\t\t0x00030000\n#define\t\tB43_DMA64_TXADDREXT_SHIFT\t\t16\n#define B43_DMA64_TXINDEX\t\t\t\t0x04\n#define B43_DMA64_TXRINGLO\t\t\t\t0x08\n#define B43_DMA64_TXRINGHI\t\t\t\t0x0C\n#define B43_DMA64_TXSTATUS\t\t\t\t0x10\n#define\t\tB43_DMA64_TXSTATDPTR\t\t0x00001FFF\n#define\t\tB43_DMA64_TXSTAT\t\t\t0xF0000000\n#define\t\t\tB43_DMA64_TXSTAT_DISABLED\t0x00000000\n#define\t\t\tB43_DMA64_TXSTAT_ACTIVE\t0x10000000\n#define\t\t\tB43_DMA64_TXSTAT_IDLEWAIT\t0x20000000\n#define\t\t\tB43_DMA64_TXSTAT_STOPPED\t0x30000000\n#define\t\t\tB43_DMA64_TXSTAT_SUSP\t0x40000000\n#define B43_DMA64_TXERROR\t\t\t\t0x14\n#define\t\tB43_DMA64_TXERRDPTR\t\t\t0x0001FFFF\n#define\t\tB43_DMA64_TXERR\t\t\t0xF0000000\n#define\t\t\tB43_DMA64_TXERR_NOERR\t0x00000000\n#define\t\t\tB43_DMA64_TXERR_PROT\t0x10000000\n#define\t\t\tB43_DMA64_TXERR_UNDERRUN\t0x20000000\n#define\t\t\tB43_DMA64_TXERR_TRANSFER\t0x30000000\n#define\t\t\tB43_DMA64_TXERR_DESCREAD\t0x40000000\n#define\t\t\tB43_DMA64_TXERR_CORE\t0x50000000\n#define B43_DMA64_RXCTL\t\t\t\t0x20\n#define\t\tB43_DMA64_RXENABLE\t\t\t0x00000001\n#define\t\tB43_DMA64_RXFROFF_MASK\t\t0x000000FE\n#define\t\tB43_DMA64_RXFROFF_SHIFT\t\t1\n#define\t\tB43_DMA64_RXDIRECTFIFO\t\t0x00000100\n#define\t\tB43_DMA64_RXADDREXT_MASK\t\t0x00030000\n#define\t\tB43_DMA64_RXADDREXT_SHIFT\t\t16\n#define B43_DMA64_RXINDEX\t\t\t\t0x24\n#define B43_DMA64_RXRINGLO\t\t\t\t0x28\n#define B43_DMA64_RXRINGHI\t\t\t\t0x2C\n#define B43_DMA64_RXSTATUS\t\t\t\t0x30\n#define\t\tB43_DMA64_RXSTATDPTR\t\t0x00001FFF\n#define\t\tB43_DMA64_RXSTAT\t\t\t0xF0000000\n#define\t\t\tB43_DMA64_RXSTAT_DISABLED\t0x00000000\n#define\t\t\tB43_DMA64_RXSTAT_ACTIVE\t0x10000000\n#define\t\t\tB43_DMA64_RXSTAT_IDLEWAIT\t0x20000000\n#define\t\t\tB43_DMA64_RXSTAT_STOPPED\t0x30000000\n#define\t\t\tB43_DMA64_RXSTAT_SUSP\t0x40000000\n#define B43_DMA64_RXERROR\t\t\t\t0x34\n#define\t\tB43_DMA64_RXERRDPTR\t\t\t0x0001FFFF\n#define\t\tB43_DMA64_RXERR\t\t\t0xF0000000\n#define\t\t\tB43_DMA64_RXERR_NOERR\t0x00000000\n#define\t\t\tB43_DMA64_RXERR_PROT\t0x10000000\n#define\t\t\tB43_DMA64_RXERR_UNDERRUN\t0x20000000\n#define\t\t\tB43_DMA64_RXERR_TRANSFER\t0x30000000\n#define\t\t\tB43_DMA64_RXERR_DESCREAD\t0x40000000\n#define\t\t\tB43_DMA64_RXERR_CORE\t0x50000000\n\n/* 64-bit DMA descriptor. */\nstruct b43_dmadesc64 {\n\t__le32 control0;\n\t__le32 control1;\n\t__le32 address_low;\n\t__le32 address_high;\n} __packed;\n#define B43_DMA64_DCTL0_DTABLEEND\t\t0x10000000\n#define B43_DMA64_DCTL0_IRQ\t\t\t0x20000000\n#define B43_DMA64_DCTL0_FRAMEEND\t\t0x40000000\n#define B43_DMA64_DCTL0_FRAMESTART\t\t0x80000000\n#define B43_DMA64_DCTL1_BYTECNT\t\t0x00001FFF\n#define B43_DMA64_DCTL1_ADDREXT_MASK\t0x00030000\n#define B43_DMA64_DCTL1_ADDREXT_SHIFT\t16\n\nstruct b43_dmadesc_generic {\n\tunion {\n\t\tstruct b43_dmadesc32 dma32;\n\t\tstruct b43_dmadesc64 dma64;\n\t} __packed;\n} __packed;\n\n/* Misc DMA constants */\n#define B43_DMA_RINGMEMSIZE\t\tPAGE_SIZE\n#define B43_DMA0_RX_FRAMEOFFSET\t\t30\n\n/* DMA engine tuning knobs */\n#define B43_TXRING_SLOTS\t\t256\n#define B43_RXRING_SLOTS\t\t64\n#define B43_DMA0_RX_BUFFERSIZE\t\t(B43_DMA0_RX_FRAMEOFFSET + IEEE80211_MAX_FRAME_LEN)\n\n/* Pointer poison */\n#define B43_DMA_PTR_POISON\t\t((void *)ERR_PTR(-ENOMEM))\n#define b43_dma_ptr_is_poisoned(ptr)\t(unlikely((ptr) == B43_DMA_PTR_POISON))\n\n\nstruct sk_buff;\nstruct b43_private;\nstruct b43_txstatus;\n\nstruct b43_dmadesc_meta {\n\t/* The kernel DMA-able buffer. */\n\tstruct sk_buff *skb;\n\t/* DMA base bus-address of the descriptor buffer. */\n\tdma_addr_t dmaaddr;\n\t/* ieee80211 TX status. Only used once per 802.11 frag. */\n\tbool is_last_fragment;\n};\n\nstruct b43_dmaring;\n\n/* Lowlevel DMA operations that differ between 32bit and 64bit DMA. */\nstruct b43_dma_ops {\n\tstruct b43_dmadesc_generic *(*idx2desc) (struct b43_dmaring * ring,\n\t\t\t\t\t\t int slot,\n\t\t\t\t\t\t struct b43_dmadesc_meta **\n\t\t\t\t\t\t meta);\n\tvoid (*fill_descriptor) (struct b43_dmaring * ring,\n\t\t\t\t struct b43_dmadesc_generic * desc,\n\t\t\t\t dma_addr_t dmaaddr, u16 bufsize, int start,\n\t\t\t\t int end, int irq);\n\tvoid (*poke_tx) (struct b43_dmaring * ring, int slot);\n\tvoid (*tx_suspend) (struct b43_dmaring * ring);\n\tvoid (*tx_resume) (struct b43_dmaring * ring);\n\tint (*get_current_rxslot) (struct b43_dmaring * ring);\n\tvoid (*set_current_rxslot) (struct b43_dmaring * ring, int slot);\n};\n\nenum b43_dmatype {\n\tB43_DMA_30BIT\t= 30,\n\tB43_DMA_32BIT\t= 32,\n\tB43_DMA_64BIT\t= 64,\n};\n\nstruct b43_dmaring {\n\t/* Lowlevel DMA ops. */\n\tconst struct b43_dma_ops *ops;\n\t/* Kernel virtual base address of the ring memory. */\n\tvoid *descbase;\n\t/* Meta data about all descriptors. */\n\tstruct b43_dmadesc_meta *meta;\n\t/* Cache of TX headers for each TX frame.\n\t * This is to avoid an allocation on each TX.\n\t * This is NULL for an RX ring.\n\t */\n\tu8 *txhdr_cache;\n\t/* (Unadjusted) DMA base bus-address of the ring memory. */\n\tdma_addr_t dmabase;\n\t/* Number of descriptor slots in the ring. */\n\tint nr_slots;\n\t/* Number of used descriptor slots. */\n\tint used_slots;\n\t/* Currently used slot in the ring. */\n\tint current_slot;\n\t/* Frameoffset in octets. */\n\tu32 frameoffset;\n\t/* Descriptor buffer size. */\n\tu16 rx_buffersize;\n\t/* The MMIO base register of the DMA controller. */\n\tu16 mmio_base;\n\t/* DMA controller index number (0-5). */\n\tint index;\n\t/* Boolean. Is this a TX ring? */\n\tbool tx;\n\t/* The type of DMA engine used. */\n\tenum b43_dmatype type;\n\t/* Boolean. Is this ring stopped at ieee80211 level? */\n\tbool stopped;\n\t/* The QOS priority assigned to this ring. Only used for TX rings.\n\t * This is the mac80211 \"queue\" value. */\n\tu8 queue_prio;\n\tstruct b43_wldev *dev;\n#ifdef CONFIG_B43_DEBUG\n\t/* Maximum number of used slots. */\n\tint max_used_slots;\n\t/* Last time we injected a ring overflow. */\n\tunsigned long last_injected_overflow;\n\t/* Statistics: Number of successfully transmitted packets */\n\tu64 nr_succeed_tx_packets;\n\t/* Statistics: Number of failed TX packets */\n\tu64 nr_failed_tx_packets;\n\t/* Statistics: Total number of TX plus all retries. */\n\tu64 nr_total_packet_tries;\n#endif /* CONFIG_B43_DEBUG */\n};\n\nstatic inline u32 b43_dma_read(struct b43_dmaring *ring, u16 offset)\n{\n\treturn b43_read32(ring->dev, ring->mmio_base + offset);\n}\n\nstatic inline void b43_dma_write(struct b43_dmaring *ring, u16 offset, u32 value)\n{\n\tb43_write32(ring->dev, ring->mmio_base + offset, value);\n}\n\nint b43_dma_init(struct b43_wldev *dev);\nvoid b43_dma_free(struct b43_wldev *dev);\n\nvoid b43_dma_tx_suspend(struct b43_wldev *dev);\nvoid b43_dma_tx_resume(struct b43_wldev *dev);\n\nint b43_dma_tx(struct b43_wldev *dev,\n\t       struct sk_buff *skb);\nvoid b43_dma_handle_txstatus(struct b43_wldev *dev,\n\t\t\t     const struct b43_txstatus *status);\n\nvoid b43_dma_rx(struct b43_dmaring *ring);\n\nvoid b43_dma_direct_fifo_rx(struct b43_wldev *dev,\n\t\t\t    unsigned int engine_index, bool enable);\n\n#endif /* B43_DMA_H_ */\n"], "filenames": ["drivers/net/wireless/b43/dma.c", "drivers/net/wireless/b43/dma.h"], "buggy_code_start_loc": [1539, 166], "buggy_code_end_loc": [1540, 167], "fixing_code_start_loc": [1539, 166], "fixing_code_end_loc": [1540, 167], "type": "CWE-119", "message": "The dma_rx function in drivers/net/wireless/b43/dma.c in the Linux kernel before 2.6.39 does not properly allocate receive buffers, which allows remote attackers to cause a denial of service (system crash) via a crafted frame.", "other": {"cve": {"id": "CVE-2011-3359", "sourceIdentifier": "secalert@redhat.com", "published": "2012-05-24T23:55:02.340", "lastModified": "2023-02-13T04:32:38.440", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The dma_rx function in drivers/net/wireless/b43/dma.c in the Linux kernel before 2.6.39 does not properly allocate receive buffers, which allows remote attackers to cause a denial of service (system crash) via a crafted frame."}, {"lang": "es", "value": "La funci\u00f3n de dma_rx drivers/net/wireless/b43/dma.c en el kernel de Linux antes de 2.6.39 no asigna correctamente los b\u00faferes de recepci\u00f3n, lo que permite a atacantes remotos provocar una denegaci\u00f3n de servicio (ca\u00edda del sistema) a trav\u00e9s de un marco dise\u00f1ado."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 7.8}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-119"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.39", "matchCriteriaId": "176353CE-F17E-4776-AD9F-19014DA75B76"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=c85ce65ecac078ab1a1835c87c4a6319cf74660a", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2011/09/14/2", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=738202", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/c85ce65ecac078ab1a1835c87c4a6319cf74660a", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/c85ce65ecac078ab1a1835c87c4a6319cf74660a"}}