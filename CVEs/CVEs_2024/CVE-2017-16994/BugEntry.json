{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/sched.h>\n#include <linux/hugetlb.h>\n\nstatic int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpte_t *pte;\n\tint err = 0;\n\n\tpte = pte_offset_map(pmd, addr);\n\tfor (;;) {\n\t\terr = walk->pte_entry(pte, addr, addr + PAGE_SIZE, walk);\n\t\tif (err)\n\t\t       break;\n\t\taddr += PAGE_SIZE;\n\t\tif (addr == end)\n\t\t\tbreak;\n\t\tpte++;\n\t}\n\n\tpte_unmap(pte);\n\treturn err;\n}\n\nstatic int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tint err = 0;\n\n\tpmd = pmd_offset(pud, addr);\n\tdo {\nagain:\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (pmd_none(*pmd) || !walk->vma) {\n\t\t\tif (walk->pte_hole)\n\t\t\t\terr = walk->pte_hole(addr, next, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * This implies that each ->pmd_entry() handler\n\t\t * needs to know about pmd_trans_huge() pmds\n\t\t */\n\t\tif (walk->pmd_entry)\n\t\t\terr = walk->pmd_entry(pmd, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Check this here so we only break down trans_huge\n\t\t * pages when we _need_ to\n\t\t */\n\t\tif (!walk->pte_entry)\n\t\t\tcontinue;\n\n\t\tsplit_huge_pmd(walk->vma, pmd, addr);\n\t\tif (pmd_trans_unstable(pmd))\n\t\t\tgoto again;\n\t\terr = walk_pte_range(pmd, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pmd++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tint err = 0;\n\n\tpud = pud_offset(p4d, addr);\n\tdo {\n again:\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_none(*pud) || !walk->vma) {\n\t\t\tif (walk->pte_hole)\n\t\t\t\terr = walk->pte_hole(addr, next, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (walk->pud_entry) {\n\t\t\tspinlock_t *ptl = pud_trans_huge_lock(pud, walk->vma);\n\n\t\t\tif (ptl) {\n\t\t\t\terr = walk->pud_entry(pud, addr, next, walk);\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tsplit_huge_pud(walk->vma, pud, addr);\n\t\tif (pud_none(*pud))\n\t\t\tgoto again;\n\n\t\tif (walk->pmd_entry || walk->pte_entry)\n\t\t\terr = walk_pmd_range(pud, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pud++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\tint err = 0;\n\n\tp4d = p4d_offset(pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none_or_clear_bad(p4d)) {\n\t\t\tif (walk->pte_hole)\n\t\t\t\terr = walk->pte_hole(addr, next, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\t\tif (walk->pmd_entry || walk->pte_entry)\n\t\t\terr = walk_pud_range(p4d, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (p4d++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int walk_pgd_range(unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpgd_t *pgd;\n\tunsigned long next;\n\tint err = 0;\n\n\tpgd = pgd_offset(walk->mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none_or_clear_bad(pgd)) {\n\t\t\tif (walk->pte_hole)\n\t\t\t\terr = walk->pte_hole(addr, next, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\t\tif (walk->pmd_entry || walk->pte_entry)\n\t\t\terr = walk_p4d_range(pgd, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pgd++, addr = next, addr != end);\n\n\treturn err;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic unsigned long hugetlb_entry_end(struct hstate *h, unsigned long addr,\n\t\t\t\t       unsigned long end)\n{\n\tunsigned long boundary = (addr & huge_page_mask(h)) + huge_page_size(h);\n\treturn boundary < end ? boundary : end;\n}\n\nstatic int walk_hugetlb_range(unsigned long addr, unsigned long end,\n\t\t\t      struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long next;\n\tunsigned long hmask = huge_page_mask(h);\n\tunsigned long sz = huge_page_size(h);\n\tpte_t *pte;\n\tint err = 0;\n\n\tdo {\n\t\tnext = hugetlb_entry_end(h, addr, end);\n\t\tpte = huge_pte_offset(walk->mm, addr & hmask, sz);\n\t\tif (pte && walk->hugetlb_entry)\n\t\t\terr = walk->hugetlb_entry(pte, hmask, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (addr = next, addr != end);\n\n\treturn err;\n}\n\n#else /* CONFIG_HUGETLB_PAGE */\nstatic int walk_hugetlb_range(unsigned long addr, unsigned long end,\n\t\t\t      struct mm_walk *walk)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_HUGETLB_PAGE */\n\n/*\n * Decide whether we really walk over the current vma on [@start, @end)\n * or skip it via the returned value. Return 0 if we do walk over the\n * current vma, and return 1 if we skip the vma. Negative values means\n * error, where we abort the current walk.\n */\nstatic int walk_page_test(unsigned long start, unsigned long end,\n\t\t\tstruct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\n\tif (walk->test_walk)\n\t\treturn walk->test_walk(start, end, walk);\n\n\t/*\n\t * vma(VM_PFNMAP) doesn't have any valid struct pages behind VM_PFNMAP\n\t * range, so we don't walk over it as we do for normal vmas. However,\n\t * Some callers are interested in handling hole range and they don't\n\t * want to just ignore any single address range. Such users certainly\n\t * define their ->pte_hole() callbacks, so let's delegate them to handle\n\t * vma(VM_PFNMAP).\n\t */\n\tif (vma->vm_flags & VM_PFNMAP) {\n\t\tint err = 1;\n\t\tif (walk->pte_hole)\n\t\t\terr = walk->pte_hole(start, end, walk);\n\t\treturn err ? err : 1;\n\t}\n\treturn 0;\n}\n\nstatic int __walk_page_range(unsigned long start, unsigned long end,\n\t\t\tstruct mm_walk *walk)\n{\n\tint err = 0;\n\tstruct vm_area_struct *vma = walk->vma;\n\n\tif (vma && is_vm_hugetlb_page(vma)) {\n\t\tif (walk->hugetlb_entry)\n\t\t\terr = walk_hugetlb_range(start, end, walk);\n\t} else\n\t\terr = walk_pgd_range(start, end, walk);\n\n\treturn err;\n}\n\n/**\n * walk_page_range - walk page table with caller specific callbacks\n *\n * Recursively walk the page table tree of the process represented by @walk->mm\n * within the virtual address range [@start, @end). During walking, we can do\n * some caller-specific works for each entry, by setting up pmd_entry(),\n * pte_entry(), and/or hugetlb_entry(). If you don't set up for some of these\n * callbacks, the associated entries/pages are just ignored.\n * The return values of these callbacks are commonly defined like below:\n *  - 0  : succeeded to handle the current entry, and if you don't reach the\n *         end address yet, continue to walk.\n *  - >0 : succeeded to handle the current entry, and return to the caller\n *         with caller specific value.\n *  - <0 : failed to handle the current entry, and return to the caller\n *         with error code.\n *\n * Before starting to walk page table, some callers want to check whether\n * they really want to walk over the current vma, typically by checking\n * its vm_flags. walk_page_test() and @walk->test_walk() are used for this\n * purpose.\n *\n * struct mm_walk keeps current values of some common data like vma and pmd,\n * which are useful for the access from callbacks. If you want to pass some\n * caller-specific data to callbacks, @walk->private should be helpful.\n *\n * Locking:\n *   Callers of walk_page_range() and walk_page_vma() should hold\n *   @walk->mm->mmap_sem, because these function traverse vma list and/or\n *   access to vma's data.\n */\nint walk_page_range(unsigned long start, unsigned long end,\n\t\t    struct mm_walk *walk)\n{\n\tint err = 0;\n\tunsigned long next;\n\tstruct vm_area_struct *vma;\n\n\tif (start >= end)\n\t\treturn -EINVAL;\n\n\tif (!walk->mm)\n\t\treturn -EINVAL;\n\n\tVM_BUG_ON_MM(!rwsem_is_locked(&walk->mm->mmap_sem), walk->mm);\n\n\tvma = find_vma(walk->mm, start);\n\tdo {\n\t\tif (!vma) { /* after the last vma */\n\t\t\twalk->vma = NULL;\n\t\t\tnext = end;\n\t\t} else if (start < vma->vm_start) { /* outside vma */\n\t\t\twalk->vma = NULL;\n\t\t\tnext = min(end, vma->vm_start);\n\t\t} else { /* inside vma */\n\t\t\twalk->vma = vma;\n\t\t\tnext = min(end, vma->vm_end);\n\t\t\tvma = vma->vm_next;\n\n\t\t\terr = walk_page_test(start, next, walk);\n\t\t\tif (err > 0) {\n\t\t\t\t/*\n\t\t\t\t * positive return values are purely for\n\t\t\t\t * controlling the pagewalk, so should never\n\t\t\t\t * be passed to the callers.\n\t\t\t\t */\n\t\t\t\terr = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (walk->vma || walk->pte_hole)\n\t\t\terr = __walk_page_range(start, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (start = next, start < end);\n\treturn err;\n}\n\nint walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk)\n{\n\tint err;\n\n\tif (!walk->mm)\n\t\treturn -EINVAL;\n\n\tVM_BUG_ON(!rwsem_is_locked(&walk->mm->mmap_sem));\n\tVM_BUG_ON(!vma);\n\twalk->vma = vma;\n\terr = walk_page_test(vma->vm_start, vma->vm_end, walk);\n\tif (err > 0)\n\t\treturn 0;\n\tif (err < 0)\n\t\treturn err;\n\treturn __walk_page_range(vma->vm_start, vma->vm_end, walk);\n}\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/sched.h>\n#include <linux/hugetlb.h>\n\nstatic int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpte_t *pte;\n\tint err = 0;\n\n\tpte = pte_offset_map(pmd, addr);\n\tfor (;;) {\n\t\terr = walk->pte_entry(pte, addr, addr + PAGE_SIZE, walk);\n\t\tif (err)\n\t\t       break;\n\t\taddr += PAGE_SIZE;\n\t\tif (addr == end)\n\t\t\tbreak;\n\t\tpte++;\n\t}\n\n\tpte_unmap(pte);\n\treturn err;\n}\n\nstatic int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpmd_t *pmd;\n\tunsigned long next;\n\tint err = 0;\n\n\tpmd = pmd_offset(pud, addr);\n\tdo {\nagain:\n\t\tnext = pmd_addr_end(addr, end);\n\t\tif (pmd_none(*pmd) || !walk->vma) {\n\t\t\tif (walk->pte_hole)\n\t\t\t\terr = walk->pte_hole(addr, next, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * This implies that each ->pmd_entry() handler\n\t\t * needs to know about pmd_trans_huge() pmds\n\t\t */\n\t\tif (walk->pmd_entry)\n\t\t\terr = walk->pmd_entry(pmd, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * Check this here so we only break down trans_huge\n\t\t * pages when we _need_ to\n\t\t */\n\t\tif (!walk->pte_entry)\n\t\t\tcontinue;\n\n\t\tsplit_huge_pmd(walk->vma, pmd, addr);\n\t\tif (pmd_trans_unstable(pmd))\n\t\t\tgoto again;\n\t\terr = walk_pte_range(pmd, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pmd++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpud_t *pud;\n\tunsigned long next;\n\tint err = 0;\n\n\tpud = pud_offset(p4d, addr);\n\tdo {\n again:\n\t\tnext = pud_addr_end(addr, end);\n\t\tif (pud_none(*pud) || !walk->vma) {\n\t\t\tif (walk->pte_hole)\n\t\t\t\terr = walk->pte_hole(addr, next, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (walk->pud_entry) {\n\t\t\tspinlock_t *ptl = pud_trans_huge_lock(pud, walk->vma);\n\n\t\t\tif (ptl) {\n\t\t\t\terr = walk->pud_entry(pud, addr, next, walk);\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tif (err)\n\t\t\t\t\tbreak;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tsplit_huge_pud(walk->vma, pud, addr);\n\t\tif (pud_none(*pud))\n\t\t\tgoto again;\n\n\t\tif (walk->pmd_entry || walk->pte_entry)\n\t\t\terr = walk_pmd_range(pud, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pud++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tp4d_t *p4d;\n\tunsigned long next;\n\tint err = 0;\n\n\tp4d = p4d_offset(pgd, addr);\n\tdo {\n\t\tnext = p4d_addr_end(addr, end);\n\t\tif (p4d_none_or_clear_bad(p4d)) {\n\t\t\tif (walk->pte_hole)\n\t\t\t\terr = walk->pte_hole(addr, next, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\t\tif (walk->pmd_entry || walk->pte_entry)\n\t\t\terr = walk_pud_range(p4d, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (p4d++, addr = next, addr != end);\n\n\treturn err;\n}\n\nstatic int walk_pgd_range(unsigned long addr, unsigned long end,\n\t\t\t  struct mm_walk *walk)\n{\n\tpgd_t *pgd;\n\tunsigned long next;\n\tint err = 0;\n\n\tpgd = pgd_offset(walk->mm, addr);\n\tdo {\n\t\tnext = pgd_addr_end(addr, end);\n\t\tif (pgd_none_or_clear_bad(pgd)) {\n\t\t\tif (walk->pte_hole)\n\t\t\t\terr = walk->pte_hole(addr, next, walk);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\t\tif (walk->pmd_entry || walk->pte_entry)\n\t\t\terr = walk_p4d_range(pgd, addr, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (pgd++, addr = next, addr != end);\n\n\treturn err;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic unsigned long hugetlb_entry_end(struct hstate *h, unsigned long addr,\n\t\t\t\t       unsigned long end)\n{\n\tunsigned long boundary = (addr & huge_page_mask(h)) + huge_page_size(h);\n\treturn boundary < end ? boundary : end;\n}\n\nstatic int walk_hugetlb_range(unsigned long addr, unsigned long end,\n\t\t\t      struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long next;\n\tunsigned long hmask = huge_page_mask(h);\n\tunsigned long sz = huge_page_size(h);\n\tpte_t *pte;\n\tint err = 0;\n\n\tdo {\n\t\tnext = hugetlb_entry_end(h, addr, end);\n\t\tpte = huge_pte_offset(walk->mm, addr & hmask, sz);\n\n\t\tif (pte)\n\t\t\terr = walk->hugetlb_entry(pte, hmask, addr, next, walk);\n\t\telse if (walk->pte_hole)\n\t\t\terr = walk->pte_hole(addr, next, walk);\n\n\t\tif (err)\n\t\t\tbreak;\n\t} while (addr = next, addr != end);\n\n\treturn err;\n}\n\n#else /* CONFIG_HUGETLB_PAGE */\nstatic int walk_hugetlb_range(unsigned long addr, unsigned long end,\n\t\t\t      struct mm_walk *walk)\n{\n\treturn 0;\n}\n\n#endif /* CONFIG_HUGETLB_PAGE */\n\n/*\n * Decide whether we really walk over the current vma on [@start, @end)\n * or skip it via the returned value. Return 0 if we do walk over the\n * current vma, and return 1 if we skip the vma. Negative values means\n * error, where we abort the current walk.\n */\nstatic int walk_page_test(unsigned long start, unsigned long end,\n\t\t\tstruct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\n\tif (walk->test_walk)\n\t\treturn walk->test_walk(start, end, walk);\n\n\t/*\n\t * vma(VM_PFNMAP) doesn't have any valid struct pages behind VM_PFNMAP\n\t * range, so we don't walk over it as we do for normal vmas. However,\n\t * Some callers are interested in handling hole range and they don't\n\t * want to just ignore any single address range. Such users certainly\n\t * define their ->pte_hole() callbacks, so let's delegate them to handle\n\t * vma(VM_PFNMAP).\n\t */\n\tif (vma->vm_flags & VM_PFNMAP) {\n\t\tint err = 1;\n\t\tif (walk->pte_hole)\n\t\t\terr = walk->pte_hole(start, end, walk);\n\t\treturn err ? err : 1;\n\t}\n\treturn 0;\n}\n\nstatic int __walk_page_range(unsigned long start, unsigned long end,\n\t\t\tstruct mm_walk *walk)\n{\n\tint err = 0;\n\tstruct vm_area_struct *vma = walk->vma;\n\n\tif (vma && is_vm_hugetlb_page(vma)) {\n\t\tif (walk->hugetlb_entry)\n\t\t\terr = walk_hugetlb_range(start, end, walk);\n\t} else\n\t\terr = walk_pgd_range(start, end, walk);\n\n\treturn err;\n}\n\n/**\n * walk_page_range - walk page table with caller specific callbacks\n *\n * Recursively walk the page table tree of the process represented by @walk->mm\n * within the virtual address range [@start, @end). During walking, we can do\n * some caller-specific works for each entry, by setting up pmd_entry(),\n * pte_entry(), and/or hugetlb_entry(). If you don't set up for some of these\n * callbacks, the associated entries/pages are just ignored.\n * The return values of these callbacks are commonly defined like below:\n *  - 0  : succeeded to handle the current entry, and if you don't reach the\n *         end address yet, continue to walk.\n *  - >0 : succeeded to handle the current entry, and return to the caller\n *         with caller specific value.\n *  - <0 : failed to handle the current entry, and return to the caller\n *         with error code.\n *\n * Before starting to walk page table, some callers want to check whether\n * they really want to walk over the current vma, typically by checking\n * its vm_flags. walk_page_test() and @walk->test_walk() are used for this\n * purpose.\n *\n * struct mm_walk keeps current values of some common data like vma and pmd,\n * which are useful for the access from callbacks. If you want to pass some\n * caller-specific data to callbacks, @walk->private should be helpful.\n *\n * Locking:\n *   Callers of walk_page_range() and walk_page_vma() should hold\n *   @walk->mm->mmap_sem, because these function traverse vma list and/or\n *   access to vma's data.\n */\nint walk_page_range(unsigned long start, unsigned long end,\n\t\t    struct mm_walk *walk)\n{\n\tint err = 0;\n\tunsigned long next;\n\tstruct vm_area_struct *vma;\n\n\tif (start >= end)\n\t\treturn -EINVAL;\n\n\tif (!walk->mm)\n\t\treturn -EINVAL;\n\n\tVM_BUG_ON_MM(!rwsem_is_locked(&walk->mm->mmap_sem), walk->mm);\n\n\tvma = find_vma(walk->mm, start);\n\tdo {\n\t\tif (!vma) { /* after the last vma */\n\t\t\twalk->vma = NULL;\n\t\t\tnext = end;\n\t\t} else if (start < vma->vm_start) { /* outside vma */\n\t\t\twalk->vma = NULL;\n\t\t\tnext = min(end, vma->vm_start);\n\t\t} else { /* inside vma */\n\t\t\twalk->vma = vma;\n\t\t\tnext = min(end, vma->vm_end);\n\t\t\tvma = vma->vm_next;\n\n\t\t\terr = walk_page_test(start, next, walk);\n\t\t\tif (err > 0) {\n\t\t\t\t/*\n\t\t\t\t * positive return values are purely for\n\t\t\t\t * controlling the pagewalk, so should never\n\t\t\t\t * be passed to the callers.\n\t\t\t\t */\n\t\t\t\terr = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (walk->vma || walk->pte_hole)\n\t\t\terr = __walk_page_range(start, next, walk);\n\t\tif (err)\n\t\t\tbreak;\n\t} while (start = next, start < end);\n\treturn err;\n}\n\nint walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk)\n{\n\tint err;\n\n\tif (!walk->mm)\n\t\treturn -EINVAL;\n\n\tVM_BUG_ON(!rwsem_is_locked(&walk->mm->mmap_sem));\n\tVM_BUG_ON(!vma);\n\twalk->vma = vma;\n\terr = walk_page_test(vma->vm_start, vma->vm_end, walk);\n\tif (err > 0)\n\t\treturn 0;\n\tif (err < 0)\n\t\treturn err;\n\treturn __walk_page_range(vma->vm_start, vma->vm_end, walk);\n}\n"], "filenames": ["mm/pagewalk.c"], "buggy_code_start_loc": [191], "buggy_code_end_loc": [192], "fixing_code_start_loc": [191], "fixing_code_end_loc": [197], "type": "CWE-200", "message": "The walk_hugetlb_range function in mm/pagewalk.c in the Linux kernel before 4.14.2 mishandles holes in hugetlb ranges, which allows local users to obtain sensitive information from uninitialized kernel memory via crafted use of the mincore() system call.", "other": {"cve": {"id": "CVE-2017-16994", "sourceIdentifier": "cve@mitre.org", "published": "2017-11-27T19:29:00.423", "lastModified": "2018-04-25T01:29:02.240", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The walk_hugetlb_range function in mm/pagewalk.c in the Linux kernel before 4.14.2 mishandles holes in hugetlb ranges, which allows local users to obtain sensitive information from uninitialized kernel memory via crafted use of the mincore() system call."}, {"lang": "es", "value": "La funci\u00f3n walk_hugetlb_range en mm/pagewalk.c en el kernel de Linux en versiones anteriores a la 4.14.2 gestiona de manera incorrecta los agujeros en los rangos hugetlb, lo que permite que usuarios locales obtengan informaci\u00f3n sensible de la memoria del kernel no inicializada mediante el uso manipulado de la llamada del sistema mincore()."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.14.2", "matchCriteriaId": "8BFBFB82-9C5E-45C1-AD36-05E379DAE87F"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=373c4557d2aa362702c4c2d41288fb1e54990b7c", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.14.2", "source": "cve@mitre.org", "tags": ["Release Notes"]}, {"url": "http://www.securityfocus.com/bid/101969", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:0502", "source": "cve@mitre.org"}, {"url": "https://bugs.chromium.org/p/project-zero/issues/detail?id=1431", "source": "cve@mitre.org", "tags": ["Exploit", "Issue Tracking", "Patch"]}, {"url": "https://github.com/torvalds/linux/commit/373c4557d2aa362702c4c2d41288fb1e54990b7c", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://usn.ubuntu.com/3617-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3617-2/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3617-3/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3619-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3619-2/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/3632-1/", "source": "cve@mitre.org"}, {"url": "https://www.exploit-db.com/exploits/43178/", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory", "VDB Entry"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/373c4557d2aa362702c4c2d41288fb1e54990b7c"}}