{"buggy_code": ["application: {}\n  # you may also add your configuration here and leave environments empty\n  ...\nenvironments:\n  # name of the environment (should be 'test', 'prod', 'acc' or 'dev')\n  test:\n\n    # Human readable description of the server instance. This is to help\n    # your peers to identify the server\n    description: Test\n\n    # Should be prod, acc, test or dev. In case the type is set to test\n    # the JWT-tokens expiration is set to 1 day (default is 6 hours). The\n    # other types can be used in future releases of vantage6\n    type: test\n\n    # IP adress to which the server binds. In case you specify 0.0.0.0\n    # the server listens on all interfaces\n    ip: 0.0.0.0\n\n    # Port to which the server binds\n    port: 5000\n\n    # API path prefix. (i.e. https://yourdomain.org/api_path/<endpoint>). In the\n    # case you use a referse proxy and use a subpath, make sure to include it\n    # here also.\n    api_path: /api\n\n    # The URI to the server database. This should be a valid SQLAlchemy URI,\n    # e.g. for an Sqlite database: sqlite:///database-name.sqlite,\n    # or Postgres: postgresql://username:password@172.17.0.1/database).\n    uri: sqlite:///test.sqlite\n\n    # This should be set to false in production as this allows to completely\n    # wipe the database in a single command. Useful to set to true when\n    # testing/developing.\n    allow_drop_all: True\n\n    # The secret key used to generate JWT authorization tokens. This should\n    # be kept secret as others are able to generate access tokens if they\n    # know this secret. This parameter is optional. In case it is not\n    # provided in the configuration it is generated each time the server\n    # starts. Thereby invalidating all previous distributed keys.\n    # OPTIONAL\n    jwt_secret_key: super-secret-key! # recommended but optional\n\n    # Settings for the logger\n    logging:\n\n      # Controls the logging output level. Could be one of the following\n      # levels: CRITICAL, ERROR, WARNING, INFO, DEBUG, NOTSET\n      level:        DEBUG\n\n      # Filename of the log-file, used by RotatingFileHandler\n      file:         test.log\n\n      # Whether the output is shown in the console or not\n      use_console:  True\n\n      # The number of log files that are kept, used by RotatingFileHandler\n      backup_count: 5\n\n      # Size in kB of a single log file, used by RotatingFileHandler\n      max_size:     1024\n\n      # format: input for logging.Formatter,\n      format:       \"%(asctime)s - %(name)-14s - %(levelname)-8s - %(message)s\"\n      datefmt:      \"%Y-%m-%d %H:%M:%S\"\n\n    # Configure a smtp mail server for the server to use for administrative\n    # purposes. e.g. allowing users to reset their password.\n    # OPTIONAL\n    smtp:\n      port: 587\n      server: smtp.yourmailserver.com\n      username: your-username\n      password: super-secret-password\n\n    # Set an email address you want to direct your users to for support\n    # (defaults to the address you set above in the SMTP server or otherwise\n    # to support@vantage6.ai)\n    support_email: your-support@email.com\n\n    # set how long reset token provided via email are valid (default 1 hour)\n    email_token_validity_minutes: 60\n\n    # If algorithm containers need direct communication between each other\n    # the server also requires a VPN server. (!) This must be a EduVPN\n    # instance as vantage6 makes use of their API (!)\n    # OPTIONAL\n    vpn_server:\n        # the URL of your VPN server\n        url: https://your-vpn-server.ext\n\n        # OATH2 settings, make sure these are the same as in the\n        # configuration file of your EduVPN instance\n        redirect_url: http://localhost\n        client_id: your_VPN_client_user_name\n        client_secret: your_VPN_client_user_password\n\n        # Username and password to acccess the EduVPN portal\n        portal_username: your_eduvpn_portal_user_name\n        portal_userpass: your_eduvpn_portal_user_password\n\n  prod:\n    ...", "\"\"\"\nvantage6 clients\n\nThis module is contains a base client. From this base client the container\nclient (client used by master algorithms) and the user client are derived.\n\"\"\"\nimport logging\nimport pickle\nimport time\nimport typing\nimport jwt\nimport requests\nimport pyfiglet\nimport json as json_lib\nimport itertools\nimport sys\nimport traceback\n\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Union\n\nfrom vantage6.common.exceptions import AuthenticationException\nfrom vantage6.common import bytes_to_base64s, base64s_to_bytes\nfrom vantage6.common.globals import APPNAME\nfrom vantage6.common.encryption import RSACryptor, DummyCryptor\nfrom vantage6.common import WhoAmI\nfrom vantage6.client import serialization, deserialization\nfrom vantage6.client.filter import post_filtering\nfrom vantage6.client.utils import print_qr_code, LogLevel\n\n\nmodule_name = __name__.split('.')[1]\n\nLEGACY = 'legacy'\n\n\nclass ServerInfo(typing.NamedTuple):\n    \"\"\"Data-class to store the server info.\"\"\"\n    host: str\n    port: int\n    path: str\n\n\nclass ClientBase(object):\n    \"\"\"Common interface to the central server.\n\n    Contains the basis for all other clients. This includes a basic interface\n    to authenticate, generic request, creating tasks and result retrieval.\n    \"\"\"\n\n    def __init__(self, host: str, port: int, path: str = '/api'):\n        \"\"\"Basic setup for the client\n\n        Parameters\n        ----------\n        host : str\n            Adress (including protocol, e.g. `https://`) of the vantage6 server\n        port : int\n            port numer to which the server listens\n        path : str, optional\n            path of the api, by default '/api'\n        \"\"\"\n\n        self.log = logging.getLogger(module_name)\n\n        # server settings\n        self.__host = host\n        self.__port = port\n        self.__api_path = path\n\n        # tokens\n        self._access_token = None\n        self.__refresh_token = None\n        self.__refresh_url = None\n\n        self.cryptor = None\n        self.whoami = None\n\n    @property\n    def name(self) -> str:\n        \"\"\"Return the node's/client's name\"\"\"\n        return self.whoami.name\n\n    @property\n    def headers(self) -> dict:\n        \"\"\"Headers that are send with each request\"\"\"\n        if self._access_token:\n            return {'Authorization': 'Bearer ' + self._access_token}\n        else:\n            return {}\n\n    @property\n    def token(self) -> str:\n        \"\"\"JWT Authorization token\"\"\"\n        return self._access_token\n\n    @property\n    def host(self) -> str:\n        \"\"\"Host including protocol (HTTP/HTTPS)\"\"\"\n        return self.__host\n\n    @property\n    def port(self) -> int:\n        \"\"\"Port to vantage6-server listens\"\"\"\n        return self.__port\n\n    @property\n    def path(self) -> str:\n        \"\"\"Path/endpoint at the server where the api resides\"\"\"\n        return self.__api_path\n\n    @property\n    def base_path(self) -> str:\n        \"\"\"Combination of host, port and api-path\"\"\"\n        if self.__port:\n            return f\"{self.host}:{self.port}{self.__api_path}\"\n\n        return f\"{self.host}{self.__api_path}\"\n\n    def generate_path_to(self, endpoint: str) -> str:\n        \"\"\"Generate URL to endpoint using host, port and endpoint\n\n        Parameters\n        ----------\n        endpoint : str\n            endpoint to which a fullpath needs to be generated\n\n        Returns\n        -------\n        str\n            URL to the endpoint\n        \"\"\"\n        if endpoint.startswith('/'):\n            path = self.base_path + endpoint\n        else:\n            path = self.base_path + '/' + endpoint\n\n        return path\n\n    def request(self, endpoint: str, json: dict = None, method: str = 'get',\n                params: dict = None, first_try: bool = True,\n                retry: bool = True) -> dict:\n        \"\"\"Create http(s) request to the vantage6 server\n\n        Parameters\n        ----------\n        endpoint : str\n            Endpoint of the server\n        json : dict, optional\n            payload, by default None\n        method : str, optional\n            Http verb, by default 'get'\n        params : dict, optional\n            URL parameters, by default None\n        first_try : bool, optional\n            Whether this is the first attempt of this request. Default True.\n        retry: bool, optional\n            Try request again after refreshing the token. Default True.\n\n        Returns\n        -------\n        dict\n            Response of the server\n        \"\"\"\n\n        # get appropiate method\n        rest_method = {\n            'get': requests.get,\n            'post': requests.post,\n            'put': requests.put,\n            'patch': requests.patch,\n            'delete': requests.delete\n        }.get(method.lower(), requests.get)\n\n        # send request to server\n        url = self.generate_path_to(endpoint)\n        self.log.debug(f'Making request: {method.upper()} | {url} | {params}')\n\n        try:\n            response = rest_method(url, json=json, headers=self.headers,\n                                   params=params)\n        except requests.exceptions.ConnectionError as e:\n            # we can safely retry as this is a connection error. And we\n            # keep trying!\n            self.log.error('Connection error... Retrying')\n            self.log.debug(e)\n            time.sleep(1)\n            return self.request(endpoint, json, method, params)\n\n        # TODO: should check for a non 2xx response\n        if response.status_code > 210:\n            self.log.error(\n                f'Server responded with error code: {response.status_code}')\n            try:\n                self.log.error(\"msg:\"+response.json().get(\"msg\", \"\"))\n            except json_lib.JSONDecodeError:\n                self.log.error('Did not find a message from the server')\n                self.log.debug(response.content)\n\n            if retry:\n                if first_try:\n                    self.refresh_token()\n                    return self.request(endpoint, json, method, params,\n                                        first_try=False)\n                else:\n                    self.log.error(\"Nope, refreshing the token didn't fix it.\")\n\n        return response.json()\n\n    def setup_encryption(self, private_key_file: str) -> None:\n        \"\"\"Enable the encryption module fot the communication\n\n        This will attach a Crypter object to the client. It will also\n        verify that the public key at the server matches the local\n        private key. In case they differ, the local public key is uploaded\n        to the server.\n\n        Parameters\n        ----------\n        private_key_file : str\n            File path of the private key file\n\n        \"\"\"\n        assert self._access_token, \\\n            \"Encryption can only be setup after authentication\"\n        assert self.whoami.organization_id, \\\n            \"Organization unknown... Did you authenticate?\"\n\n        if private_key_file is None:\n            self.cryptor = DummyCryptor()\n            return\n\n        if isinstance(private_key_file, str):\n            private_key_file = Path(private_key_file)\n\n        cryptor = RSACryptor(private_key_file)\n\n        # check if the public-key is the same on the server. If this is\n        # not the case, this node will not be able to read any messages\n        # that are send to him! If this is the case, the new public_key\n        # will be uploaded to the central server\n        organization = self.request(\n            f\"organization/{self.whoami.organization_id}\")\n        pub_key = organization.get(\"public_key\")\n        upload_pub_key = False\n\n        if pub_key:\n            if cryptor.verify_public_key(pub_key):\n                self.log.info(\"Public key matches the server key! Good to go!\")\n\n            else:\n                self.log.critical(\n                    \"Local public key does not match server public key. \"\n                    \"You will not able to read any messages that are intended \"\n                    \"for you!\"\n                )\n                upload_pub_key = True\n        else:\n            upload_pub_key = True\n\n        # upload public key if required\n        if upload_pub_key:\n            self.request(\n                f\"organization/{self.whoami.organization_id}\",\n                method=\"patch\",\n                json={\"public_key\": cryptor.public_key_str}\n            )\n            self.log.info(\"The public key on the server is updated!\")\n\n        self.cryptor = cryptor\n\n    def authenticate(self, credentials: dict,\n                     path: str = \"token/user\") -> bool:\n        \"\"\"Authenticate to the vantage6-server\n\n        It allows users, nodes and containers to sign in. Credentials can\n        either be a username/password combination or a JWT authorization\n        token.\n\n        Parameters\n        ----------\n        credentials : dict\n            Credentials used to authenticate\n        path : str, optional\n            Endpoint used for authentication. This differs for users, nodes and\n            containers, by default \"token/user\"\n\n        Raises\n        ------\n        Exception\n            Failed to authenticate\n\n        Returns\n        -------\n        Bool\n            Whether or not user is authenticated. Alternative is that user is\n            redirected to set up two-factor authentication\n        \"\"\"\n        if 'username' in credentials:\n            self.log.debug(\n                f\"Authenticating user {credentials['username']}...\")\n        elif 'api_key' in credentials:\n            self.log.debug('Authenticating node...')\n\n        # authenticate to the central server\n        url = self.generate_path_to(path)\n        response = requests.post(url, json=credentials)\n        data = response.json()\n\n        # handle negative responses\n        if response.status_code > 200:\n            self.log.critical(f\"Failed to authenticate: {data.get('msg')}\")\n            if response.status_code == 401:\n                raise AuthenticationException(\"Failed to authenticate\")\n            else:\n                raise Exception(\"Failed to authenticate\")\n\n        if 'qr_uri' in data:\n            print_qr_code(data)\n            return False\n        else:\n            # Check if there is an access token. If not, there is a problem\n            # with authenticating\n            if 'access_token' not in data:\n                if 'msg' in data:\n                    raise Exception(data['msg'])\n                else:\n                    raise Exception(\n                        \"No access token in authentication response!\")\n\n            # store tokens in object\n            self.log.info(\"Successfully authenticated\")\n            self._access_token = data.get(\"access_token\")\n            self.__refresh_token = data.get(\"refresh_token\")\n            self.__refresh_url = data.get(\"refresh_url\")\n            return True\n\n    def refresh_token(self) -> None:\n        \"\"\"Refresh an expired token using the refresh token\n\n        Raises\n        ------\n        Exception\n            Authentication Error!\n        \"\"\"\n        self.log.info(\"Refreshing token\")\n        assert self.__refresh_url, \\\n            \"Refresh URL not found, did you authenticate?\"\n\n        # if no port is specified explicit, then it should be omnit the\n        # colon : in the path. Similar (but different) to the property\n        # base_path\n        if self.__port:\n            url = f\"{self.__host}:{self.__port}{self.__refresh_url}\"\n        else:\n            url = f\"{self.__host}{self.__refresh_url}\"\n\n        # send request to server\n        response = requests.post(url, headers={\n            'Authorization': 'Bearer ' + self.__refresh_token\n        })\n\n        # server says no!\n        if response.status_code != 200:\n            self.log.critical(\"Could not refresh token\")\n            raise Exception(\"Authentication Error!\")\n\n        self._access_token = response.json()[\"access_token\"]\n\n    # TODO BvB 23-01-23 remove this method in v4+. It is only here for\n    # backwards compatibility\n    def post_task(self, name: str, image: str, collaboration_id: int,\n                  input_='', description='',\n                  organization_ids: list = None,\n                  data_format=LEGACY, database: str = 'default') -> dict:\n        \"\"\"Post a new task at the server\n\n        It will also encrypt `input_` for each receiving organization.\n\n        Parameters\n        ----------\n        name : str\n            Human readable name for the task\n        image : str\n            Docker image name containing the algorithm\n        collaboration_id : int\n            Collaboration `id` of the collaboration for which the task is\n            intended\n        input_ : str, optional\n            Task input, by default ''\n        description : str, optional\n            Human readable description of the task, by default ''\n        organization_ids : list, optional\n            Ids of organizations (within the collaboration) that need to\n            execute this task, by default None\n        data_format : str, optional\n            Type of data format to use to send and receive\n            data. possible values: 'json', 'pickle', 'legacy'. 'legacy'\n            will use pickle serialization. Default is 'legacy'., by default\n            LEGACY\n\n        Returns\n        -------\n        dict\n            Containing the task meta-data\n        \"\"\"\n        assert self.cryptor, \"Encryption has not yet been setup!\"\n\n        if organization_ids is None:\n            organization_ids = []\n\n        if data_format == LEGACY:\n            serialized_input = pickle.dumps(input_)\n        else:\n            # Data will be serialized to bytes in the specified data format.\n            # It will be prepended with 'DATA_FORMAT.' in unicode.\n            serialized_input = data_format.encode() + b'.' \\\n                + serialization.serialize(input_, data_format)\n\n        organization_json_list = []\n        for org_id in organization_ids:\n            pub_key = self.request(f\"organization/{org_id}\").get(\"public_key\")\n            # pub_key = base64s_to_bytes(pub_key)\n            # self.log.debug(pub_key)\n\n            organization_json_list.append({\n                \"id\": org_id,\n                \"input\": self.cryptor.encrypt_bytes_to_str(serialized_input,\n                                                           pub_key)\n            })\n\n        return self.request('task', method='post', json={\n            \"name\": name,\n            \"image\": image,\n            \"collaboration_id\": collaboration_id,\n            \"description\": description,\n            \"organizations\": organization_json_list,\n            'database': database\n        })\n\n    # TODO BvB 23-01-23 remove this method in v4+ (or make it private?). It is\n    # only here for backwards compatibility.\n    def get_results(self, id: int = None, state: str = None,\n                    include_task: bool = False, task_id: int = None,\n                    node_id: int = None, params: dict = {}) -> dict:\n        \"\"\"Get task result(s) from the central server\n\n        Depending if a `id` is specified or not, either a single or a\n        list of results is returned. The input and result field of the\n        result are attempted te be decrypted. This fails if the public\n        key at the server is not derived from the currently private key\n        or when the result is not from your organization.\n\n        Parameters\n        ----------\n        id : int, optional\n            Id of the result, by default None\n        state : str, optional\n            The state of the task (e.g. `open`), by default None\n        include_task : bool, optional\n            Whenever to include the orginating task, by default False\n        task_id : int, optional\n            The id of the originating task, this will return all results\n            belonging to this task, by default None\n        node_id : int, optional\n            The id of the node at which this result has been produced,\n            this will return all results from this node, by default None\n\n        Returns\n        -------\n        dict\n            Containing the result(s)\n        \"\"\"\n        # Determine endpoint and create dict with query parameters\n        endpoint = 'result' if not id else f'result/{id}'\n\n        if state:\n            params['state'] = state\n        if include_task:\n            params['include'] = 'task'\n        if task_id:\n            params['task_id'] = task_id\n        if node_id:\n            params['node_id'] = node_id\n\n        # self.log.debug(f\"Retrieving results using query parameters:{params}\")\n        results = self.request(endpoint=endpoint, params=params)\n\n        if isinstance(results, str):\n            self.log.warn(\"Requesting results failed\")\n            self.log.debug(f\"Results message: {results}\")\n            return {}\n\n        # hack: in the case that the pagination metadata is included we\n        # need to strip that for decrypting\n        if isinstance(results, dict) and 'data' in results:\n            wrapper = results\n            results = results['data']\n\n        if id:\n            # Single result\n            self._decrypt_result(results)\n\n        else:\n            # Multiple results\n            for result in results:\n                self._decrypt_result(result)\n\n        if 'wrapper' in locals():\n            wrapper['data'] = results\n            results = wrapper\n\n        return results\n\n    def _decrypt_result(self, result):\n        \"\"\"Helper to decrypt the keys 'input' and 'result' in dict.\n\n        Keys are replaced, but object reference remains intact: changes are\n        made *in-place*.\n        \"\"\"\n        assert self.cryptor, \"Encryption has not been initialized\"\n        cryptor = self.cryptor\n        try:\n            self.log.info('Decrypting input')\n            # TODO this only works when the results belong to the\n            # same organization... We should make different implementation\n            # of get_results\n            result[\"input\"] = cryptor.decrypt_str_to_bytes(result[\"input\"])\n\n        except Exception as e:\n            self.log.debug(e)\n\n        try:\n            if result[\"result\"]:\n                self.log.info('Decrypting result')\n                result[\"result\"] = \\\n                    cryptor.decrypt_str_to_bytes(result[\"result\"])\n\n        except ValueError as e:\n            self.log.error(\"Could not decrypt/decode input or result.\")\n            self.log.error(e)\n            # raise\n\n    class SubClient:\n        \"\"\"Create sub groups of commands using this SubClient\"\"\"\n        def __init__(self, parent):\n            self.parent: UserClient = parent\n\n\nclass UserClient(ClientBase):\n    \"\"\"User interface to the vantage6-server\"\"\"\n\n    def __init__(self, *args, verbose=False, log_level='debug', **kwargs):\n        \"\"\"Create user client\n\n        All paramters from `ClientBase` can be used here.\n\n        Parameters\n        ----------\n        verbose : bool, optional\n            Whenever to print (info) messages, by default False\n        \"\"\"\n        super(UserClient, self).__init__(*args, **kwargs)\n\n        # Replace logger by print logger\n        # TODO in v4+, remove the verbose option and only keep log_level\n        self.log = self.get_logger(verbose, log_level)\n\n        # attach sub-clients\n        self.util = self.Util(self)\n        self.collaboration = self.Collaboration(self)\n        self.organization = self.Organization(self)\n        self.user = self.User(self)\n        self.result = self.Result(self)\n        self.task = self.Task(self)\n        self.role = self.Role(self)\n        self.node = self.Node(self)\n        self.rule = self.Rule(self)\n\n        # Display welcome message\n        self.log.info(\" Welcome to\")\n        for line in pyfiglet.figlet_format(APPNAME, font='big').split('\\n'):\n            self.log.info(line)\n        self.log.info(\" --> Join us on Discord! https://discord.gg/rwRvwyK\")\n        self.log.info(\" --> Docs: https://docs.vantage6.ai\")\n        self.log.info(\" --> Blog: https://vantage6.ai\")\n        self.log.info(\"-\" * 60)\n        self.log.info(\"Cite us!\")\n        self.log.info(\"If you publish your findings obtained using vantage6, \")\n        self.log.info(\"please cite the proper sources as mentioned in:\")\n        self.log.info(\"https://vantage6.ai/vantage6/references\")\n        self.log.info(\"-\" * 60)\n\n    @staticmethod\n    def get_logger(enabled: bool, level: str) -> logging.Logger:\n        \"\"\"\n        Create print-logger\n\n        Parameters\n        ----------\n        enabled: bool\n            If true, logging at most detailed level\n        level: str\n            Desired logging level\n\n        Returns\n        -------\n        logging.Logger\n            Logger object\n        \"\"\"\n        # get logger that prints to console\n        logger = logging.getLogger()\n        logger.handlers.clear()\n        logger.addHandler(logging.StreamHandler(sys.stdout))\n\n        # set log level\n        level = level.upper()\n        if enabled:\n            logger.setLevel(LogLevel.DEBUG.value)\n        elif level not in [lvl.value for lvl in LogLevel]:\n            default_lvl = LogLevel.DEBUG.value\n            logger.setLevel(default_lvl)\n            logger.warn(\n                f\"You set unknown log level {level}. Available levels are: \"\n                f\"{', '.join([lvl.value for lvl in LogLevel])}. \")\n            logger.warn(f\"Log level now set to {default_lvl}.\")\n        else:\n            logger.setLevel(level)\n        return logger\n\n    def authenticate(self, username: str, password: str,\n                     mfa_code: Union[int, str] = None) -> None:\n        \"\"\"Authenticate as a user\n\n        It also collects some additional info about your user.\n\n        Parameters\n        ----------\n        username : str\n            Username used to authenticate\n        password : str\n            Password used to authenticate\n        mfa_token: str or int\n            Six-digit two-factor authentication code\n        \"\"\"\n        auth_json = {\n            \"username\": username,\n            \"password\": password,\n        }\n        if mfa_code:\n            auth_json[\"mfa_code\"] = mfa_code\n        auth = super(UserClient, self).authenticate(auth_json,\n                                                    path=\"token/user\")\n        if not auth:\n            # user is not authenticated. The super function is responsible for\n            # printing useful output\n            return\n\n        # identify the user and the organization to which this user\n        # belongs. This is usefull for some client side checks\n        try:\n            type_ = \"user\"\n            jwt_payload = jwt.decode(self.token,\n                                     options={\"verify_signature\": False})\n\n            # FIXME: 'identity' is no longer needed in version 4+. So this if\n            # statement can be removed\n            if 'sub' in jwt_payload:\n                id_ = jwt_payload['sub']\n            elif 'identity' in jwt_payload:\n                id_ = jwt_payload['identity']\n\n            user = self.request(f\"user/{id_}\")\n            name = user.get(\"firstname\")\n            organization_id = user.get(\"organization\").get(\"id\")\n            organization = self.request(f\"organization/{organization_id}\")\n            organization_name = organization.get(\"name\")\n\n            self.whoami = WhoAmI(\n                type_=type_,\n                id_=id_,\n                name=name,\n                organization_id=organization_id,\n                organization_name=organization_name\n            )\n\n            self.log.info(\" --> Succesfully authenticated\")\n            self.log.info(f\" --> Name: {name} (id={id_})\")\n            self.log.info(f\" --> Organization: {organization_name} \"\n                          f\"(id={organization_id})\")\n        except Exception:\n            self.log.info('--> Retrieving additional user info failed!')\n            self.log.error(traceback.format_exc())\n\n    def wait_for_results(self, task_id: int, sleep: float = 1) -> Dict:\n        \"\"\"\n        Polls the server to check when results are ready, and returns the\n        results when the task is completed.\n\n        Parameters\n        ----------\n        task_id: int\n            ID of the task that you are waiting for\n        sleep: float\n            Interval in seconds between checks if task is finished. Default 1.\n\n        Returns\n        -------\n        Dict\n            A dictionary with the results of the task, after it has completed.\n        \"\"\"\n        # Disable logging (additional logging would prevent the 'wait' message\n        # from being printed on a single line)\n        if isinstance(self.log, logging.Logger):\n            prev_level = self.log.level\n            self.log.setLevel(logging.WARN)\n        elif isinstance(self.log, UserClient.Log):\n            prev_level = self.log.enabled\n            self.log.enabled = False\n\n        animation = itertools.cycle(['|', '/', '-', '\\\\'])\n        t = time.time()\n\n        while not self.task.get(task_id)['complete']:\n            frame = next(animation)\n            sys.stdout.write(\n                f'\\r{frame} Waiting for task {task_id} ({int(time.time()-t)}s)'\n            )\n            sys.stdout.flush()\n            time.sleep(sleep)\n        sys.stdout.write('\\rDone!                  ')\n\n        # Re-enable logging\n        if isinstance(self.log, logging.Logger):\n            self.log.setLevel(prev_level)\n        elif isinstance(self.log, UserClient.Log):\n            self.log.enabled = prev_level\n\n        return self.get_results(task_id=task_id)\n\n    class Util(ClientBase.SubClient):\n        \"\"\"Collection of general utilities\"\"\"\n\n        def get_server_version(self) -> dict:\n            r\"\"\"View the version number of the vantage6-server\n\n            Returns\n            -------\n            dict\n                A dict containing the version number\n            \"\"\"\n            return self.parent.request('version')\n\n        def get_server_health(self) -> dict:\n            \"\"\"View the health of the vantage6-server\n\n            Returns\n            -------\n            dict\n                Containing the server health information\n            \"\"\"\n            return self.parent.request('health')\n\n        def change_my_password(self, current_password: str,\n                               new_password: str) -> dict:\n            \"\"\"Change your own password by providing your current password\n\n            Parameters\n            ----------\n            current_password : str\n                Your current password\n            new_password : str\n                Your new password\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            result = self.parent.request(\n                'password/change', method='patch', json={\n                    'current_password': current_password,\n                    'new_password': new_password\n                }\n            )\n            msg = result.get('msg')\n            self.parent.log.info(f'--> {msg}')\n            return result\n\n        def reset_my_password(self, email: str = None,\n                              username: str = None) -> dict:\n            \"\"\"Start reset password procedure\n\n            Either a username of email needs to be provided.\n\n            Parameters\n            ----------\n            email : str, optional\n                Email address of your account, by default None\n            username : str, optional\n                Username of your account, by default None\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            assert email or username, \"You need to provide username or email!\"\n            result = self.parent.request('recover/lost', method='post', json={\n                'username': username,\n                'email': email\n            })\n            msg = result.get('msg')\n            self.parent.log.info(f'--> {msg}')\n            return result\n\n        def set_my_password(self, token: str, password: str) -> dict:\n            \"\"\"Set a new password using a recovery token\n\n            Token can be obtained through `.reset_password(...)`\n\n            Parameters\n            ----------\n            token : str\n                Token obtained from `reset_password`\n            password : str\n                New password\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            result = self.parent.request('recover/reset', method='post', json={\n                'reset_token': token,\n                'password': password\n            })\n            msg = result.get('msg')\n            self.parent.log.info(f'--> {msg}')\n            return result\n\n        def reset_two_factor_auth(\n            self, password: str, email: str = None, username: str = None\n        ) -> dict:\n            \"\"\"Start reset procedure for two-factor authentication\n\n            The password and either username of email must be provided.\n\n            Parameters\n            ----------\n            password: str\n                Password of your account\n            email : str, optional\n                Email address of your account, by default None\n            username : str, optional\n                Username of your account, by default None\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            assert email or username, \"You need to provide username or email!\"\n            result = self.parent.request(\n                'recover/2fa/lost', method='post', json={\n                    'username': username,\n                    'email': email,\n                    \"password\": password\n                }, retry=False)\n            msg = result.get('msg')\n            self.parent.log.info(f'--> {msg}')\n            return result\n\n        def set_two_factor_auth(self, token: str) -> dict:\n            \"\"\"\n            Setup two-factor authentication using a recovery token after you\n            have lost access.\n\n            Token can be obtained through `.reset_two_factor_auth(...)`\n\n            Parameters\n            ----------\n            token : str\n                Token obtained from `reset_two_factor_auth`\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            result = self.parent.request(\n                'recover/2fa/reset', method='post', json={\n                    'reset_token': token,\n                }, retry=False)\n            if 'qr_uri' in result:\n                print_qr_code(result)\n            else:\n                msg = result.get('msg')\n                self.parent.log.info(f'--> {msg}')\n            return result\n\n        def generate_private_key(self, file_: str = None) -> None:\n            \"\"\"Generate new private key\n\n            ....\n\n            Parameters\n            ----------\n            file_ : str, optional\n                Path where to store the private key, by default None\n            \"\"\"\n\n            if not file_:\n                self.parent.log.info('--> Using current directory')\n                file_ = \"private_key.pem\"\n\n            if isinstance(file_, str):\n                file_ = Path(file_).absolute()\n\n            self.parent.log.info(f'--> Generating private key file: {file_}')\n            private_key = RSACryptor.create_new_rsa_key(file_)\n\n            self.parent.log.info('--> Assigning private key to client')\n            self.parent.cryptor.private_key = private_key\n\n            self.parent.log.info('--> Encrypting the client and uploading '\n                                 'the public key')\n            self.parent.setup_encryption(file_)\n\n    class Collaboration(ClientBase.SubClient):\n        \"\"\"Collection of collaboration requests\"\"\"\n\n        @post_filtering()\n        def list(self, scope: str = 'organization',\n                 name: str = None, encrypted: bool = None,\n                 organization: int = None, page: int = 1,\n                 per_page: int = 20, include_metadata: bool = True,\n                 ) -> dict:\n            \"\"\"View your collaborations\n\n            Parameters\n            ----------\n            scope : str, optional\n                Scope of the list, accepted values are `organization` and\n                `global`. In case of `organization` you get the collaborations\n                in which your organization participates. If you specify global\n                you get the collaborations which you are allowed to see.\n            name: str, optional (with LIKE operator)\n                Filter collaborations by name\n            organization: int, optional\n                Filter collaborations by organization id\n            encrypted: bool, optional\n                Filter collaborations by whether or not they are encrypted\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            list of dicts\n                Containing collabotation information\n\n            Notes\n            -----\n            - pagination does not work in combination with scope\n              `organization` as pagination is missing at endpoint\n              /organization/<id>/collaboration\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'name': name, 'encrypted': encrypted,\n                'organization_id': organization,\n            }\n            if scope == 'organization':\n                self.parent.log.info('pagination for scope `organization` '\n                                     'not available')\n                org_id = self.parent.whoami.organization_id\n                return self.parent.request(\n                    f'organization/{org_id}/collaboration'\n                )\n            elif scope == 'global':\n                return self.parent.request('collaboration', params=params)\n            else:\n                self.parent.log.info('--> Unrecognized `scope`. Needs to be '\n                                     '`organization` or `global`')\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int) -> dict:\n            \"\"\"View specific collaboration\n\n            Parameters\n            ----------\n            id_ : int\n                Id from the collaboration you want to view\n\n            Returns\n            -------\n            dict\n                Containing the collaboration information\n            \"\"\"\n            return self.parent.request(f'collaboration/{id_}')\n\n        @post_filtering(iterable=False)\n        def create(self, name: str, organizations: list,\n                   encrypted: bool = False) -> dict:\n            \"\"\"Create new collaboration\n\n            Parameters\n            ----------\n            name : str\n                Name of the collaboration\n            organizations : list\n                List of organization ids which participate in the\n                collaboration\n            encrypted : bool, optional\n                Whenever the collaboration should be encrypted or not,\n                by default False\n\n            Returns\n            -------\n            dict\n                Containing the new collaboration meta-data\n            \"\"\"\n            return self.parent.request('collaboration', method='post', json={\n                'name': name,\n                'organization_ids': organizations,\n                'encrypted': encrypted\n            })\n\n    class Node(ClientBase.SubClient):\n        \"\"\"Collection of node requests\"\"\"\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int) -> dict:\n            \"\"\"View specific node\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the node you want to inspect\n\n            Returns\n            -------\n            dict\n                Containing the node meta-data\n            \"\"\"\n            return self.parent.request(f'node/{id_}')\n\n        @post_filtering()\n        def list(self, name: str = None, organization: int = None,\n                 collaboration: int = None, is_online: bool = None,\n                 ip: str = None, last_seen_from: str = None,\n                 last_seen_till: str = None, page: int = 1, per_page: int = 20,\n                 include_metadata: bool = True,\n                 ) -> list:\n            \"\"\"List nodes\n\n            Parameters\n            ----------\n            name: str, optional\n                Filter by name (with LIKE operator)\n            organization: int, optional\n                Filter by organization id\n            collaboration: int, optional\n                Filter by collaboration id\n            is_online: bool, optional\n                Filter on whether nodes are online or not\n            ip: str, optional\n                Filter by node VPN IP address\n            last_seen_from: str, optional\n                Filter if node has been online since date (format: yyyy-mm-dd)\n            last_seen_till: str, optional\n                Filter if node has been online until date (format: yyyy-mm-dd)\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n\n            list of dicts\n                Containing meta-data of the nodes\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'name': name, 'organization_id': organization,\n                'collaboration_id': collaboration, 'ip': ip,\n                'last_seen_from': last_seen_from,\n                'last_seen_till': last_seen_till\n            }\n            if is_online is not None:\n                params['status'] = 'online' if is_online else 'offline'\n            return self.parent.request('node', params=params)\n\n        @post_filtering(iterable=False)\n        def create(self, collaboration: int, organization: int = None,\n                   name: str = None) -> dict:\n            \"\"\"Register new node\n\n            Parameters\n            ----------\n            collaboration : int\n                Collaboration id to which this node belongs\n            organization : int, optional\n                Organization id to which this node belongs. If no id provided\n                the users organization is used. Default value is None\n            name : str, optional\n                Name of the node. If no name is provided the server will\n                generate one. Default value is None\n\n            Returns\n            -------\n            dict\n                Containing the meta-data of the new node\n            \"\"\"\n            if not organization:\n                organization = self.parent.whoami.organization_id\n\n            return self.parent.request('node', method='post', json={\n                'organization_id': organization,\n                'collaboration_id': collaboration,\n                'name': name\n            })\n\n        @post_filtering(iterable=False)\n        def update(self, id_: int, name: str = None, organization: int = None,\n                   collaboration: int = None) -> dict:\n            \"\"\"Update node information\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the node you want to update\n            name : str, optional\n                New node name, by default None\n            organization : int, optional\n                Change the owning organization of the node, by default\n                None\n            collaboration : int, optional\n                Changes the collaboration to which the node belongs, by\n                default None\n\n            Returns\n            -------\n            dict\n                Containing the meta-data of the updated node\n            \"\"\"\n            return self.parent.request(f'node/{id_}', method='patch', json={\n                'name': name,\n                'organization_id': organization,\n                'collaboration_id': collaboration\n            })\n\n        def delete(self, id_: int) -> dict:\n            \"\"\"Deletes a node\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the node you want to delete\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            return self.parent.request(f'node/{id_}', method='delete')\n\n        def kill_tasks(self, id_: int) -> dict:\n            \"\"\"\n            Kill all tasks currently running on a node\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the node of which you want to kill the tasks\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            return self.parent.request(\n                'kill/node/tasks', method='post', json={'id': id_}\n            )\n\n    class Organization(ClientBase.SubClient):\n        \"\"\"Collection of organization requests\"\"\"\n\n        @post_filtering()\n        def list(self, name: str = None, country: int = None,\n                 collaboration: int = None, page: int = None,\n                 per_page: int = None, include_metadata: bool = True) -> list:\n            \"\"\"List organizations\n\n            Parameters\n            ----------\n            name: str, optional\n                Filter by name (with LIKE operator)\n            country: str, optional\n                Filter by country\n            collaboration: int, optional\n                Filter by collaboration id\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            list of dicts\n                Containing meta-data information of the organizations\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'name': name, 'country': country,\n                'collaboration_id': collaboration\n            }\n            return self.parent.request('organization', params=params)\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int = None) -> dict:\n            \"\"\"View specific organization\n\n            Parameters\n            ----------\n            id_ : int, optional\n                Organization `id` of the organization you want to view.\n                In case no `id` is provided it will display your own\n                organization, default value is None.\n\n            Returns\n            -------\n            dict\n                Containing the organization meta-data\n            \"\"\"\n            if not id_:\n                id_ = self.parent.whoami.organization_id\n\n            return self.parent.request(f'organization/{id_}')\n\n        @post_filtering(iterable=False)\n        def update(self, id_: int = None, name: str = None,\n                   address1: str = None, address2: str = None,\n                   zipcode: str = None, country: str = None,\n                   domain: str = None, public_key: str = None) -> dict:\n            \"\"\"Update organization information\n\n            Parameters\n            ----------\n            id_ : int, optional\n                Organization id, by default None\n            name : str, optional\n                New organization name, by default None\n            address1 : str, optional\n                Address line 1, by default None\n            address2 : str, optional\n                Address line 2, by default None\n            zipcode : str, optional\n                Zipcode, by default None\n            country : str, optional\n                Country, by default None\n            domain : str, optional\n                Domain of the organization (e.g. `iknl.nl`), by default None\n            public_key : str, optional\n                public key, by default None\n\n            Returns\n            -------\n            dict\n                The meta-data of the updated organization\n            \"\"\"\n            if not id_:\n                id_ = self.parent.whoami.organization_id\n\n            return self.parent.request(\n                f'organization/{id_}',\n                method='patch',\n                json={\n                    'name': name,\n                    'address1': address1,\n                    'address2': address2,\n                    'zipcode': zipcode,\n                    'country': country,\n                    'domain': domain,\n                    'public_key': public_key\n                }\n            )\n\n        def create(self, name: str, address1: str, address2: str, zipcode: str,\n                   country: str, domain: str, public_key: str = None) -> dict:\n            \"\"\"Create new organization\n\n            Parameters\n            ----------\n            name : str\n                Name of the organization\n            address1 : str\n                Street and number\n            address2 : str\n                City\n            zipcode : str\n                Zip or postal code\n            country : str\n                Country\n            domain : str\n                Domain of the organization (e.g. vantage6.ai)\n            public_key : str, optional\n                Public key of the organization. This can be set later,\n                by default None\n\n            Returns\n            -------\n            dict\n                Containing the information of the new organization\n            \"\"\"\n            json_data = {\n                'name': name,\n                'address1': address1,\n                'address2': address2,\n                'zipcode': zipcode,\n                'country': country,\n                'domain': domain,\n            }\n\n            if public_key:\n                json_data['public_key'] = public_key\n\n            return self.parent.request(\n                'organization',\n                method='post',\n                json=json_data\n            )\n\n    class User(ClientBase.SubClient):\n\n        @post_filtering()\n        def list(self, username: str = None, organization: int = None,\n                 firstname: str = None, lastname: str = None,\n                 email: str = None, role: int = None, rule: int = None,\n                 last_seen_from: str = None, last_seen_till: str = None,\n                 page: int = 1, per_page: int = 20,\n                 include_metadata: bool = True) -> list:\n            \"\"\"List users\n\n            Parameters\n            ----------\n            username: str, optional\n                Filter by username (with LIKE operator)\n            organization: int, optional\n                Filter by organization id\n            firstname: str, optional\n                Filter by firstname (with LIKE operator)\n            lastname: str, optional\n                Filter by lastname (with LIKE operator)\n            email: str, optional\n                Filter by email (with LIKE operator)\n            role: int, optional\n                Show only users that have this role id\n            rule: int, optional\n                Show only users that have this rule id\n            last_seen_from: str, optional\n                Filter users that have logged on since (format yyyy-mm-dd)\n            last_seen_till: str, optional\n                Filter users that have logged on until (format yyyy-mm-dd)\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            list of dicts\n                Containing the meta-data of the users\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'username': username, 'organization_id': organization,\n                'firstname': firstname, 'lastname': lastname, 'email': email,\n                'role_id': role, 'rule_id': rule,\n                'last_seen_from': last_seen_from,\n                'last_seen_till': last_seen_till,\n            }\n            return self.parent.request('user', params=params)\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int = None) -> dict:\n            \"\"\"View user information\n\n            Parameters\n            ----------\n            id_ : int, optional\n                User `id`, by default None. When no `id` is provided\n                your own user information is displayed\n\n            Returns\n            -------\n            dict\n                Containing user information\n            \"\"\"\n            if not id_:\n                id_ = self.parent.whoami.id_\n            return self.parent.request(f'user/{id_}')\n\n        @post_filtering(iterable=False)\n        def update(self, id_: int = None, firstname: str = None,\n                   lastname: str = None, organization: int = None,\n                   rules: list = None, roles: list = None, email: str = None\n                   ) -> dict:\n            \"\"\"Update user details\n\n            In case you do not supply a user_id, your user is being\n            updated.\n\n            Parameters\n            ----------\n            id_ : int\n                User `id` from the user you want to update\n            firstname : str\n                Your first name\n            lastname : str\n                Your last name\n            organization : int\n                Organization id of the organization you want to be part\n                of. This can only done by super-users.\n            rules : list of ints\n                USE WITH CAUTION! Rule ids that should be assigned to\n                this user. All previous assigned rules will be removed!\n            roles : list of ints\n                USE WITH CAUTION! Role ids that should be assigned to\n                this user. All previous assigned roles will be removed!\n            email : str\n                New email from the user\n\n            Returns\n            -------\n            dict\n                A dict containing the updated user data\n            \"\"\"\n            if not id_:\n                id_ = self.parent.whoami.id_\n\n            json_body = {\n                \"firstname\": firstname,\n                \"lastname\": lastname,\n                \"organization_id\": organization,\n                \"rules\": rules,\n                \"roles\": roles,\n                \"email\": email\n            }\n\n            # only submit supplied keys\n            json_body = {k: v for k, v in json_body.items() if v is not None}\n\n            user = self.parent.request(f'user/{id_}', method='patch',\n                                       json=json_body)\n            return user\n\n        @post_filtering(iterable=False)\n        def create(self, username: str, firstname: str, lastname: str,\n                   password: str, email: str, organization: int = None,\n                   roles: list = [], rules: list = []) -> dict:\n            \"\"\"Create new user\n\n            Parameters\n            ----------\n            username : str\n                Used to login to the service. This can not be changed\n                later.\n            firstname : str\n                Firstname of the new user\n            lastname : str\n                Lastname of the new user\n            password : str\n                Password of the new user\n            organization : int\n                Organization `id` this user should belong to\n            roles : list of ints\n                Role ids that are assigned to this user. Note that you\n                can only assign roles if you own the rules within this\n                role.\n            rules : list of ints\n                Rule ids that are assigned to this user. Note that you\n                can only assign rules that you own\n\n            Return\n            ----------\n            dict\n                Containing data of the new user\n            \"\"\"\n            user_data = {\n                'username': username,\n                'firstname': firstname,\n                'lastname': lastname,\n                'password': password,\n                'email': email,\n                'organization_id': organization,\n                'roles': roles,\n                'rules': rules\n            }\n            return self.parent.request('user', json=user_data, method='post')\n\n    class Role(ClientBase.SubClient):\n\n        @post_filtering()\n        def list(self, name: str = None, description: str = None,\n                 organization: int = None, rule: int = None, user: int = None,\n                 include_root: bool = None, page: int = 1, per_page: int = 20,\n                 include_metadata: bool = True) -> list:\n            \"\"\"List of roles\n\n            Parameters\n            ----------\n            name: str, optional\n                Filter by name (with LIKE operator)\n            description: str, optional\n                Filter by description (with LIKE operator)\n            organization: int, optional\n                Filter by organization id\n            rule: int, optional\n                Only show roles that contain this rule id\n            user: int, optional\n                Only show roles that belong to a particular user id\n            include_root: bool, optional\n                Include roles that are not assigned to any particular\n                organization\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            list of dicts\n                Containing roles meta-data\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'name': name, 'description': description,\n                'organization_id': organization, 'rule_id': rule,\n                'include_root': include_root, 'user_id': user,\n            }\n            return self.parent.request('role', params=params)\n\n        @post_filtering(iterable=True)\n        def get(self, id_: int) -> dict:\n            \"\"\"View specific role\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the role you want to insepct\n\n            Returns\n            -------\n            dict\n                Containing meta-data of the role\n            \"\"\"\n            return self.parent.request(f'role/{id_}')\n\n        @post_filtering(iterable=True)\n        def create(self, name: str, description: str, rules: list,\n                   organization: int = None) -> dict:\n            \"\"\"Register new role\n\n            Parameters\n            ----------\n            name : str\n                Role name\n            description : str\n                Human readable description of the role\n            rules : list\n                Rules that this role contains\n            organization : int, optional\n                Organization to which this role belongs. In case this is\n                not provided the users organization is used. By default\n                None\n\n            Returns\n            -------\n            dict\n                Containing meta-data of the new role\n            \"\"\"\n            if not organization:\n                organization = self.parent.whoami.organization_id\n            return self.parent.request('role', method='post', json={\n                'name': name,\n                'description': description,\n                'rules': rules,\n                'organization_id': organization\n            })\n\n        @post_filtering(iterable=True)\n        def update(self, role: int, name: str = None, description: str = None,\n                   rules: list = None) -> dict:\n            \"\"\"Update role\n\n            Parameters\n            ----------\n            role : int\n                Id of the role that updated\n            name : str, optional\n                New name of the role, by default None\n            description : str, optional\n                New description of the role, by default None\n            rules : list, optional\n                CAUTION! This will not *add* rules but replace them. If\n                you remove rules from your own role you lose access. By\n                default None\n\n            Returns\n            -------\n            dict\n                Containing the updated role data\n            \"\"\"\n            return self.parent.request(f'role/{role}', method='patch', json={\n                'name': name,\n                'description': description,\n                'rules': rules\n            })\n\n        def delete(self, role: int) -> dict:\n            \"\"\"Delete role\n\n            Parameters\n            ----------\n            role : int\n                CAUTION! Id of the role to be deleted. If you remove\n                roles that are attached to you, you might lose access!\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            res = self.parent.request(f'role/{role}', method='delete')\n            self.parent.log.info(f'--> {res.get(\"msg\")}')\n\n    class Task(ClientBase.SubClient):\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int, include_results: bool = False) -> dict:\n            \"\"\"View specific task\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the task you want to view\n            include_results : bool, optional\n                Whenever to include the results or not, by default False\n\n            Returns\n            -------\n            dict\n                Containing the task data\n            \"\"\"\n            params = {}\n            params['include'] = 'results' if include_results else None\n            return self.parent.request(f'task/{id_}', params=params)\n\n        @post_filtering()\n        def list(self, initiator: int = None, initiating_user: int = None,\n                 collaboration: int = None, image: str = None,\n                 parent: int = None, run: int = None,\n                 name: str = None, include_results: bool = False,\n                 description: str = None, database: str = None,\n                 result: int = None, status: str = None, page: int = 1,\n                 per_page: int = 20, include_metadata: bool = True) -> dict:\n            \"\"\"List tasks\n\n            Parameters\n            ----------\n            name: str, optional\n                Filter by the name of the task. It will match with a\n                Like operator. I.e. E% will search for task names that\n                start with an 'E'.\n            initiator: int, optional\n                Filter by initiating organization\n            initiating_user: int, optional\n                Filter by initiating user\n            collaboration: int, optional\n                Filter by collaboration\n            image: str, optional\n                Filter by Docker image name (with LIKE operator)\n            parent: int, optional\n                Filter by parent task\n            run: int, optional\n                Filter by run\n            include_results : bool, optional\n                Whenever to include the results in the tasks, by default\n                False\n            description: str, optional\n                Filter by description (with LIKE operator)\n            database: str, optional\n                Filter by database (with LIKE operator)\n            result: int, optional\n                Only show task that contains this result id\n            status: str, optional\n                Filter by task status (e.g. 'active', 'pending', 'completed',\n                'crashed')\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            dict\n                dictonairy containing the key 'data' which contains the\n                tasks and a key 'links' containing the pagination\n                metadata\n\n            OR\n\n            list\n                when 'include_metadata' is set to false, it removes the\n                metadata wrapper. I.e. directly returning the 'data'\n                key.\n            \"\"\"\n            # if the param is None, it will not be passed on to the\n            # request\n            # TODO in v4+, we should change the 'initiator' argument to\n            # a name that distinguishes it better from the initiating user.\n            # Then, we should also change it in the server\n            params = {\n                'initiator_id': initiator, 'init_user_id': initiating_user,\n                'collaboration_id': collaboration,\n                'image': image, 'parent_id': parent, 'run_id': run,\n                'name': name, 'page': page, 'per_page': per_page,\n                'description': description, 'database': database,\n                'result_id': result, 'status': status,\n            }\n            includes = []\n            if include_results:\n                includes.append('results')\n            if include_metadata:\n                includes.append('metadata')\n            params['include'] = includes\n\n            return self.parent.request('task', params=params)\n\n        @post_filtering(iterable=False)\n        def create(self, collaboration: int, organizations: list, name: str,\n                   image: str, description: str, input: dict,\n                   data_format: str = LEGACY,\n                   database: str = 'default') -> dict:\n            \"\"\"Create a new task\n\n            Parameters\n            ----------\n            collaboration : int\n                Id of the collaboration to which this task belongs\n            organizations : list\n                Organization ids (within the collaboration) which need\n                to execute this task\n            name : str\n                Human readable name\n            image : str\n                Docker image name which contains the algorithm\n            description : str\n                Human readable description\n            input : dict\n                Algorithm input\n            data_format : str, optional\n                IO data format used, by default LEGACY\n            database: str, optional\n                Database name to be used at the node\n\n            Returns\n            -------\n            dict\n                [description]\n            \"\"\"\n            return self.parent.post_task(name, image, collaboration, input,\n                                         description, organizations,\n                                         data_format, database)\n\n        def delete(self, id_: int) -> dict:\n            \"\"\"Delete a task\n\n            Also removes the related results.\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the task to be removed\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            msg = self.parent.request(f'task/{id_}', method='delete')\n            self.parent.log.info(f'--> {msg}')\n\n        def kill(self, id_: int) -> dict:\n            \"\"\"Kill a task running on one or more nodes\n\n            Note that this does not remove the task from the database, but\n            merely halts its execution (and prevents it from being restarted).\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the task to be killed\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            msg = self.parent.request('/kill/task', method='post', json={\n                'id': id_\n            })\n            self.parent.log.info(f'--> {msg}')\n\n    class Result(ClientBase.SubClient):\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int, include_task: bool = False) -> dict:\n            \"\"\"View a specific result\n\n            Parameters\n            ----------\n            id_ : int\n                id of the result you want to inspect\n            include_task : bool, optional\n                Whenever to include the task or not, by default False\n\n            Returns\n            -------\n            dict\n                Containing the result data\n            \"\"\"\n            self.parent.log.info('--> Attempting to decrypt results!')\n\n            # get_results also handles decryption\n            result = self.parent.get_results(id=id_, include_task=include_task)\n            result_data = result.get('result')\n            if result_data:\n                try:\n                    result['result'] = deserialization.load_data(result_data)\n                except Exception as e:\n                    self.parent.log.warn('--> Failed to deserialize')\n                    self.parent.log.debug(e)\n\n            return result\n\n        @post_filtering()\n        def list(self, task: int = None, organization: int = None,\n                 state: str = None, node: int = None,\n                 include_task: bool = False, started: Tuple[str, str] = None,\n                 assigned: Tuple[str, str] = None,\n                 finished: Tuple[str, str] = None, port: int = None,\n                 page: int = None, per_page: int = None,\n                 include_metadata: bool = True) -> list:\n            \"\"\"List results\n\n            Parameters\n            ----------\n            task: int, optional\n                Filter by task id\n            organization: int, optional\n                Filter by organization id\n            state: int, optional\n                Filter by state: ('open',)\n            node: int, optional\n                Filter by node id\n            include_task : bool, optional\n                Whenever to include the task or not, by default False\n            started: Tuple[str, str], optional\n                Filter on a range of start times (format: yyyy-mm-dd)\n            assigned: Tuple[str, str], optional\n                Filter on a range of assign times (format: yyyy-mm-dd)\n            finished: Tuple[str, str], optional\n                Filter on a range of finished times (format: yyyy-mm-dd)\n            port: int, optional\n                Port on which result was computed\n            page: int, optional\n                Pagination page number, defaults to 1\n            per_page: int, optional\n                Number of items per page, defaults to 20\n            include_metedata: bool, optional\n                Whenevet to include pagination metadata, defaults to\n                True\n\n            Returns\n            -------\n            dict\n                Containing the key 'data' which contains a list of\n                results, and a key 'links' which contains the pagination\n                metadata\n\n            OR\n\n            list of dicts\n                When include_metadata is set to False, the metadata wrapper\n                is stripped and only a list of results is returned\n            \"\"\"\n            includes = []\n            if include_metadata:\n                includes.append('metadata')\n            if include_task:\n                includes.append('task')\n\n            s_from, s_till = started if started else (None, None)\n            a_from, a_till = assigned if assigned else (None, None)\n            f_from, f_till = finished if finished else (None, None)\n\n            params = {\n                'task_id': task, 'organization_id': organization,\n                'state': state, 'node_id': node, 'page': page,\n                'per_page': per_page, 'include': includes,\n                'started_from': s_from, 'started_till': s_till,\n                'assigned_from': a_from, 'assigned_till': a_till,\n                'finished_from': f_from, 'finished_till': f_till,\n                'port': port\n            }\n\n            results = self.parent.get_results(params=params)\n\n            if isinstance(results, dict):\n                wrapper = results\n                results = results['data']\n\n            cleaned_results = []\n            for result in results:\n                if result.get('result'):\n                    try:\n                        des_res = deserialization.load_data(\n                            result.get('result')\n                        )\n                    except Exception as e:\n                        id_ = result.get('id')\n                        self.parent.log.warn('Could not deserialize result id='\n                                             f'{id_}')\n                        self.parent.log.debug(e)\n                        continue\n                    result['result'] = des_res\n                cleaned_results.append(result)\n\n            if 'wrapper' in locals():\n                wrapper['data'] = cleaned_results\n                cleaned_results = wrapper\n\n            return cleaned_results\n\n        def from_task(self, task_id: int, include_task: bool = False):\n            self.parent.log.info('--> Attempting to decrypt results!')\n\n            # get_results also handles decryption\n            results = self.parent.get_results(task_id=task_id,\n                                              include_task=include_task)\n            cleaned_results = []\n            for result in results:\n                if result.get('result'):\n                    des_res = deserialization.load_data(result.get('result'))\n                    result['result'] = des_res\n                cleaned_results.append(result)\n\n            return cleaned_results\n\n    class Rule(ClientBase.SubClient):\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int) -> dict:\n            \"\"\"View specific rule\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the rule you want to view\n\n            Returns\n            -------\n            dict\n                Containing the information about this rule\n            \"\"\"\n            return self.parent.request(f'rule/{id_}')\n\n        @post_filtering()\n        def list(self, name: str = None, operation: str = None,\n                 scope: str = None, role: int = None, page: int = 1,\n                 per_page: int = 20, include_metadata: bool = True) -> list:\n            \"\"\"List of all available rules\n\n            Parameters\n            ----------\n            name: str, optional\n                Filter by rule name\n            operation: str, optional\n                Filter by operation\n            scope: str, optional\n                Filter by scope\n            role: int, optional\n                Only show rules that belong to this role id\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            list of dicts\n                Containing all the rules from the vantage6 server\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'name': name, 'operation': operation, 'scope': scope,\n                'role_id': role\n            }\n            return self.parent.request('rule', params=params)\n\n\n# TODO remove in v4+ (deprecated for AlgorithmClient but still kept for\n# backwards compatibility)\nclass ContainerClient(ClientBase):\n    \"\"\" Container interface to the local proxy server (central server).\n\n        An algorithm container should never communicate directly to the\n        central server. Therefore the algorithm container has no\n        internet connection. The algorithm can, however, talk to a local\n        proxy server which has interface to the central server. This way\n        we make sure that the algorithm container does not share stuff\n        with others, and we also can encrypt the results for a specific\n        receiver. Thus this not a interface to the central server but to\n        the local proxy server. However the interface is identical thus\n        we are happy that we can ignore this detail.\n    \"\"\"\n\n    def __init__(self, token: str, *args, **kwargs):\n        \"\"\"Container client.\n        A client which can be used by algorithms. All permissions of the\n        container are derived from the token.\n\n        Parameters\n        ----------\n        token : str\n            JWT (container) token, generated by the node\n                the algorithm container runs on\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        # obtain the identity from the token\n        jwt_payload = jwt.decode(\n            token, options={\"verify_signature\": False})\n\n        # FIXME: 'identity' is no longer needed in version 4+. So this if\n        # statement can be removed\n        if 'sub' in jwt_payload:\n            container_identity = jwt_payload['sub']\n        elif 'identity' in jwt_payload:\n            container_identity = jwt_payload['identity']\n\n        self.image = container_identity.get(\"image\")\n        self.database = container_identity.get('database')\n        self.host_node_id = container_identity.get(\"node_id\")\n        self.collaboration_id = container_identity.get(\"collaboration_id\")\n        self.log.info(\n            f\"Container in collaboration_id={self.collaboration_id} \\n\"\n            f\"Key created by node_id {self.host_node_id} \\n\"\n            f\"Can only use image={self.image}\"\n        )\n\n        self._access_token = token\n        self.log.debug(f\"Access token={self._access_token}\")\n\n    def authenticate(self):\n        \"\"\" Containers obtain their key via their host Node.\"\"\"\n        self.log.warn(\"Containers do not authenticate?!\")\n        return\n\n    def refresh_token(self):\n        \"\"\" Containers cannot refresh their token.\n\n            TODO we might want to notify node/server about this...\n            TODO make a more usefull exception\n        \"\"\"\n        raise Exception(\"Containers cannot refresh!\")\n\n    def get_results(self, task_id: int):\n        \"\"\" Obtain results from a specific task at the server\n\n            Containers are allowed to obtain the results of their\n            children (having the same run_id at the server). The\n            permissions are checked at te central server.\n\n            :param task_id: id of the task from which you want to obtain\n                the results\n        \"\"\"\n        results = self.request(\n            f\"task/{task_id}/result\"\n        )\n\n        res = []\n        # Encryption is not done at the client level for the container.\n        # Although I am not completely sure that the format is always\n        # a pickle.\n        # for result in results:\n        #     self._decrypt_result(result)\n        #     res.append(result.get(\"result\"))\n        #\n        try:\n            res = [pickle.loads(base64s_to_bytes(result.get(\"result\")))\n                   for result in results if result.get(\"result\")]\n        except Exception as e:\n            self.log.error('Unable to unpickle result')\n            self.log.debug(e)\n\n        return res\n\n    def get_algorithm_addresses(self, task_id: int):\n        \"\"\"\n        Return IP address and port number of other algorithm containers\n        involved in a task so that VPN can be used for communication\n        \"\"\"\n        results = self.request(f\"task/{task_id}/result\")\n\n        algorithm_addresses = []\n        for result in results:\n            for port in result['ports']:\n                algorithm_addresses.append({\n                    'ip': result['node']['ip'],\n                    'port': port['port'],\n                    'label': port['label']\n                })\n        return algorithm_addresses\n\n    def get_algorithm_address_by_label(self, task_id: int, label: str) -> str:\n        \"\"\"\n        Return the IP address plus port number of a given port label\n        \"\"\"\n        algorithm_addresses = self.get_algorithm_addresses(task_id=task_id)\n        for address in algorithm_addresses:\n            if address['label'] == label:\n                return f\"{address['ip']}:{address['port']}\"\n        return None\n\n    def get_task(self, task_id: int):\n        return self.request(\n            f\"task/{task_id}\"\n        )\n\n    def create_new_task(self, input_, organization_ids=[]):\n        \"\"\" Create a new (child) task at the central server.\n\n            Containers are allowed to create child tasks (having the\n            same run_id) at the central server. The docker image must\n            be the same as the docker image of this container self.\n\n            :param input_: input to the task\n            :param organization_ids: organization ids which need to\n                execute this task\n        \"\"\"\n        self.log.debug(f\"create new task for {organization_ids}\")\n\n        return self.post_task(\n            name=\"subtask\",\n            description=f\"task from container on node_id={self.host_node_id}\",\n            collaboration_id=self.collaboration_id,\n            organization_ids=organization_ids,\n            input_=input_,\n            image=self.image,\n            database=self.database\n        )\n\n    def get_organizations_in_my_collaboration(self):\n        \"\"\" Obtain all organization in the collaboration.\n\n            The container runs in a Node which is part of a single\n            collaboration. This method retrieves all organization data\n            that are within that collaboration. This can be used to\n            target specific organizations in a collaboration.\n        \"\"\"\n        organizations = self.request(\n            f\"collaboration/{self.collaboration_id}/organization\")\n        return organizations\n\n    def post_task(self, name: str, image: str, collaboration_id: int,\n                  input_: str = '', description='',\n                  organization_ids: list = [], database='default') -> dict:\n        \"\"\" Post a new task at the central server.\n\n            ! To create a new task from the algorithm container you\n            should use the `create_new_task` function !\n\n            Creating a task from a container does need to be encrypted.\n            This is done because the container should never have access\n            to the private key of this organization. The encryption\n            takes place in the local proxy server to which the algorithm\n            communicates (indirectly to the central server). Therefore\n            we needed to overload the post_task function.\n\n            :param name: human-readable name\n            :param image: docker image name of the task\n            :param collaboration_id: id of the collaboration in which\n                the task should run\n            :param input_: input to the task\n            :param description: human-readable description\n            :param organization_ids: ids of the organizations where this\n                task should run\n        \"\"\"\n        self.log.debug(\"post task without encryption (is handled by proxy)\")\n\n        serialized_input = bytes_to_base64s(pickle.dumps(input_))\n\n        organization_json_list = []\n        for org_id in organization_ids:\n            organization_json_list.append(\n                {\n                    \"id\": org_id,\n                    \"input\": serialized_input\n                }\n            )\n\n        return self.request('task', method='post', json={\n            \"name\": name,\n            \"image\": image,\n            \"collaboration_id\": collaboration_id,\n            \"description\": description,\n            \"organizations\": organization_json_list,\n            \"database\": database\n        })\n\n\n# For backwards compatibility\nClient = UserClient\n", "\"\"\"\nA node in its simplest would retrieve a task from the central server by\nan API call, run this task and finally return the results to the central\nserver again.\n\nThe node application runs four threads:\n\n*Main thread*\n    Checks the task queue and run the next task if there is one available.\n*Listening thread*\n    Listens for incoming websocket messages. Among other functionality, it adds\n    new tasks to the task queue.\n*Speaking thread*\n    Waits for tasks to finish. When they do, return the results to the central\n    server.\n*Proxy server thread*\n    Algorithm containers are isolated from the internet for security reasons.\n    The local proxy server provides an interface to the central server for\n    *master* containers to create subtasks and retrieve their results.\n\nThe node connects to the server using a websocket connection. This connection\nis mainly used for sharing status updates. This avoids the need for polling to\nsee if there are new tasks available.\n\"\"\"\nimport sys\nimport os\nimport random\nimport time\nimport datetime\nimport logging\nimport queue\nimport json\nimport shutil\nimport requests.exceptions\n\nfrom pathlib import Path\nfrom threading import Thread\nfrom typing import Dict, List, Union, Type\nfrom socketio import Client as SocketIO\nfrom gevent.pywsgi import WSGIServer\nfrom enum import Enum\n\nfrom vantage6.common.docker.addons import (\n    ContainerKillListener, check_docker_running, running_in_docker\n)\nfrom vantage6.common.globals import VPN_CONFIG_FILE\nfrom vantage6.common.exceptions import AuthenticationException\nfrom vantage6.common.docker.network_manager import NetworkManager\nfrom vantage6.common.task_status import TaskStatus\nfrom vantage6.cli.context import NodeContext\nfrom vantage6.node.context import DockerNodeContext\nfrom vantage6.node.globals import (\n    NODE_PROXY_SERVER_HOSTNAME, SLEEP_BTWN_NODE_LOGIN_TRIES,\n    TIME_LIMIT_RETRY_CONNECT_NODE, TIME_LIMIT_INITIAL_CONNECTION_WEBSOCKET\n)\nfrom vantage6.node.server_io import NodeClient\nfrom vantage6.node import proxy_server\nfrom vantage6.node.util import logger_name, get_parent_id\nfrom vantage6.node.docker.docker_manager import DockerManager\nfrom vantage6.node.docker.vpn_manager import VPNManager\nfrom vantage6.node.socket import NodeTaskNamespace\nfrom vantage6.node.docker.ssh_tunnel import SSHTunnel\n\n\nclass VPNConnectMode(Enum):\n    FIRST_TRY = 1\n    REFRESH_KEYPAIR = 2\n    REFRESH_COMPLETE = 3\n\n\n# ------------------------------------------------------------------------------\nclass Node(object):\n    \"\"\"\n    Authenticates to the central server, setup encryption, a\n    websocket connection, retrieving task that were posted while\n    offline, preparing dataset for usage and finally setup a\n    local proxy server..\n\n    Parameters\n    ----------\n    ctx: Union[NodeContext, DockerNodeContext]\n        Application context object.\n\n    \"\"\"\n    def __init__(self, ctx: Union[NodeContext, DockerNodeContext]):\n\n        self.log = logging.getLogger(logger_name(__name__))\n        self.ctx = ctx\n\n        # Initialize the node. If it crashes, shut down the parts that started\n        # already\n        try:\n            self.initialize()\n        except Exception:\n            self.cleanup()\n            raise\n\n    def initialize(self) -> None:\n        \"\"\"Initialization of the node\"\"\"\n        # check if docker is running, otherwise exit with error\n        check_docker_running()\n\n        self.config = self.ctx.config\n        self.queue = queue.Queue()\n        self._using_encryption = None\n\n        # initialize Node connection to the server\n        self.server_io = NodeClient(\n            host=self.config.get('server_url'),\n            port=self.config.get('port'),\n            path=self.config.get('api_path')\n        )\n\n        self.log.info(f\"Connecting server: {self.server_io.base_path}\")\n\n        # Authenticate with the server, obtaining a JSON Web Token.\n        # Note that self.authenticate() blocks until it succeeds.\n        self.log.debug(\"Authenticating\")\n        self.authenticate()\n\n        # Setup encryption\n        self.setup_encryption()\n\n        # Thread for proxy server for algorithm containers, so they can\n        # communicate with the central server.\n        self.log.info(\"Setting up proxy server\")\n        t = Thread(target=self.__proxy_server_worker, daemon=True)\n        t.start()\n\n        # Create a long-lasting websocket connection.\n        self.log.debug(\"Creating websocket connection with the server\")\n        self.connect_to_socket()\n\n        # setup docker isolated network manager\n        internal_ = running_in_docker()\n        if not internal_:\n            self.log.warn(\n                \"Algorithms have internet connection! \"\n                \"This happens because you use 'vnode-local'!\"\n            )\n        isolated_network_mgr = NetworkManager(self.ctx.docker_network_name)\n        isolated_network_mgr.create_network(is_internal=internal_)\n\n        # Setup tasks dir\n        self._set_task_dir(self.ctx)\n\n        # Setup VPN connection\n        self.vpn_manager = self.setup_vpn_connection(\n            isolated_network_mgr, self.ctx)\n\n        # Create SSH tunnel according to the node configuration\n        self.ssh_tunnels = self.setup_ssh_tunnels(isolated_network_mgr)\n\n        # setup the docker manager\n        self.log.debug(\"Setting up the docker manager\")\n        self.__docker = DockerManager(\n            ctx=self.ctx,\n            isolated_network_mgr=isolated_network_mgr,\n            vpn_manager=self.vpn_manager,\n            tasks_dir=self.__tasks_dir,\n            client=self.server_io,\n        )\n\n        # Connect the node to the isolated algorithm network *only* if we're\n        # running in a docker container.\n        if self.ctx.running_in_docker:\n            isolated_network_mgr.connect(\n                container_name=self.ctx.docker_container_name,\n                aliases=[NODE_PROXY_SERVER_HOSTNAME]\n            )\n\n        # Connect any docker services specified in the configuration file to\n        # the node container\n        self.link_docker_services()\n\n        # Thread for sending results to the server when they come available.\n        self.log.debug(\"Start thread for sending messages (results)\")\n        t = Thread(target=self.__speaking_worker, daemon=True)\n        t.start()\n\n        # listen forever for incoming messages, tasks are stored in\n        # the queue.\n        self.log.debug(\"Starting thread for incoming messages (tasks)\")\n        t = Thread(target=self.__listening_worker, daemon=True)\n        t.start()\n\n        self.log.info('Init complete')\n\n    def __proxy_server_worker(self) -> None:\n        \"\"\"\n        Proxy algorithm container communcation.\n\n        A proxy for communication between algorithms and central\n        server.\n        \"\"\"\n        if self.ctx.running_in_docker:\n            # NODE_PROXY_SERVER_HOSTNAME points to the name of the proxy\n            # when running in the isolated docker network.\n            default_proxy_host = NODE_PROXY_SERVER_HOSTNAME\n        else:\n            # If we're running non-dockerized, assume that the proxy is\n            # accessible from within the docker algorithm container on\n            # host.docker.internal.\n            default_proxy_host = 'host.docker.internal'\n\n        # If PROXY_SERVER_HOST was set in the environment, it overrides our\n        # value.\n        proxy_host = os.environ.get(\"PROXY_SERVER_HOST\", default_proxy_host)\n        os.environ[\"PROXY_SERVER_HOST\"] = proxy_host\n\n        proxy_port = int(os.environ.get(\"PROXY_SERVER_PORT\", 8080))\n\n        # 'app' is defined in vantage6.node.proxy_server\n        # app.debug = True\n        proxy_server.app.config[\"SERVER_IO\"] = self.server_io\n        proxy_server.server_url = self.server_io.base_path\n\n        # this is where we try to find a port for the proxyserver\n        for try_number in range(5):\n            self.log.info(\n                f\"Starting proxyserver at '{proxy_host}:{proxy_port}'\")\n            http_server = WSGIServer(('0.0.0.0', proxy_port), proxy_server.app)\n\n            try:\n                http_server.serve_forever()\n\n            except OSError as e:\n                self.log.debug(f'Error during attempt {try_number}')\n                self.log.debug(f'{type(e)}: {e}')\n\n                if e.errno == 48:\n                    proxy_port = random.randint(2048, 16384)\n                    self.log.critical(\n                        f\"Retrying with a different port: {proxy_port}\")\n                    os.environ['PROXY_SERVER_PORT'] = str(proxy_port)\n\n                else:\n                    raise\n\n            except Exception as e:\n                self.log.error('Proxyserver could not be started or crashed!')\n                self.log.error(e)\n\n    def sync_task_queue_with_server(self) -> None:\n        \"\"\" Get all unprocessed tasks from the server for this node.\"\"\"\n        assert self.server_io.cryptor, \"Encrpytion has not been setup\"\n\n        # request open tasks from the server\n        tasks = self.server_io.get_results(state=\"open\", include_task=True)\n        self.log.debug(tasks)\n        for task in tasks:\n            self.queue.put(task)\n\n        self.log.info(f\"received {self.queue._qsize()} tasks\")\n\n    def __start_task(self, taskresult: dict) -> None:\n        \"\"\"\n        Start the docker image and notify the server that the task has been\n        started.\n\n        Parameters\n        ----------\n        taskresult : dict\n            A dictionary with information required to run the algorithm\n        \"\"\"\n        task = taskresult['task']\n        self.log.info(\"Starting task {id} - {name}\".format(**task))\n\n        # notify that we are processing this task\n        self.server_io.set_task_start_time(taskresult[\"id\"])\n\n        token = self.server_io.request_token_for_container(\n            task[\"id\"],\n            task[\"image\"]\n        )\n        token = token[\"container_token\"]\n\n        # create a temporary volume for each run_id\n        vol_name = self.ctx.docker_temporary_volume_name(task[\"run_id\"])\n        self.__docker.create_volume(vol_name)\n\n        # For some reason, if the key 'input' consists of JSON, it is\n        # automatically marshalled? This causes trouble, so we'll serialize it\n        # again.\n        # FIXME: should probably find & fix the root cause?\n        if type(taskresult['input']) == dict:\n            taskresult['input'] = json.dumps(taskresult['input'])\n\n        # Run the container. This adds the created container/task to the list\n        # __docker.active_tasks\n        task_status, vpn_ports = self.__docker.run(\n            result_id=taskresult[\"id\"],\n            task_info=task,\n            image=task[\"image\"],\n            docker_input=taskresult['input'],\n            tmp_vol_name=vol_name,\n            token=token,\n            database=task.get('database', 'default')\n        )\n\n        # save task status to the server and send socket event to update others\n        self.server_io.patch_results(\n            id=taskresult['id'], result={'status': task_status}\n        )\n        self.socketIO.emit(\n            'algorithm_status_change',\n            data={\n                'node_id': self.server_io.whoami.id_,\n                'status': task_status,\n                'result_id': taskresult['id'],\n                'task_id': task['id'],\n                'collaboration_id': self.server_io.collaboration_id,\n                'organization_id': self.server_io.whoami.organization_id,\n                'parent_id': get_parent_id(task),\n            },\n            namespace='/tasks',\n        )\n\n        if vpn_ports:\n            # Save port of VPN client container at which it redirects traffic\n            # to the algorithm container. First delete any existing port\n            # assignments in case algorithm has crashed\n            self.server_io.request(\n                'port', params={'result_id': taskresult['id']}, method=\"DELETE\"\n            )\n            for port in vpn_ports:\n                port['result_id'] = taskresult['id']\n                self.server_io.request('port', method='POST', json=port)\n\n            # Save IP address of VPN container\n            # FIXME BvB 2023-02-21: node IP is now updated when task is started\n            # but this should be done when VPN connection is established\n            node_id = self.server_io.whoami.id_\n            node_ip = self.vpn_manager.get_vpn_ip()\n            self.server_io.request(\n                f\"node/{node_id}\", json={\"ip\": node_ip}, method=\"PATCH\"\n            )\n\n    def __listening_worker(self) -> None:\n        \"\"\"\n        Listen for incoming (websocket) messages from the server.\n\n        Runs in a separate thread. Received events are handled by the\n        appropriate action handler.\n        \"\"\"\n        self.log.debug(\"Listening for incoming messages\")\n\n        # FIXME: while True in combination with a wait() call that never exits\n        #   makes joining the tread (to terminate) difficult?\n        while True:\n            # incoming messages are handled by the action_handler instance\n            # which is attached when the socket connection was made. wait()\n            # is blocks forever (if no time is specified).\n            try:\n                self.socketIO.wait()\n            except Exception as e:\n                self.log.error('Listening thread had an exception')\n                self.log.debug(e)\n\n    def __speaking_worker(self) -> None:\n        \"\"\"\n        Sending messages to central server.\n\n        Routine that is in a seperate thread sending results\n        to the server when they come available.\n        \"\"\"\n        # TODO change to a single request, might need to reconsider\n        #     the flow\n        self.log.debug(\"Waiting for results to send to the server\")\n\n        while True:\n            try:\n                results = self.__docker.get_result()\n\n                # notify socket channel of algorithm status change\n                self.socketIO.emit(\n                    'algorithm_status_change',\n                    data={\n                        'node_id': self.server_io.whoami.id_,\n                        'status': results.status,\n                        'result_id': results.result_id,\n                        'task_id': results.task_id,\n                        'collaboration_id': self.server_io.collaboration_id,\n                        'organization_id':\n                            self.server_io.whoami.organization_id,\n                        'parent_id': results.parent_id,\n                    },\n                    namespace='/tasks',\n                )\n\n                self.log.info(\n                    f\"Sending result (id={results.result_id}) to the server!\")\n\n                # FIXME: why are we retrieving the result *again*? Shouldn't we\n                # just store the task_id when retrieving the task the first\n                # time?\n                response = self.server_io.request(\n                    f\"result/{results.result_id}\"\n                )\n                task_id = response.get(\"task\").get(\"id\")\n\n                if not task_id:\n                    self.log.error(\n                        f\"task_id of result (id={results.result_id}) \"\n                        f\"could not be retrieved\"\n                    )\n                    return\n\n                response = self.server_io.request(f\"task/{task_id}\")\n\n                init_org_id = response.get(\"initiator\")\n                if not init_org_id:\n                    self.log.error(\n                        f\"Initiator organization from task (id={task_id})could\"\n                        \" not be retrieved!\"\n                    )\n\n                self.server_io.patch_results(\n                    id=results.result_id,\n                    result={\n                        'result': results.data,\n                        'log': results.logs,\n                        'status': results.status,\n                        'finished_at': datetime.datetime.now().isoformat(),\n                    },\n                    init_org_id=init_org_id,\n                )\n            except Exception:\n                self.log.exception('Speaking thread had an exception')\n\n    def __print_connection_error_logs(self):\n        \"\"\" Print error message when node cannot find the server \"\"\"\n        self.log.warning(\n            \"Could not connect to the server. Retrying in 10 seconds\")\n        if self.server_io.host == 'http://localhost' and running_in_docker():\n            self.log.warn(\n                f\"You are trying to reach the server at {self.server_io.host}.\"\n                \" As your node is running inside a Docker container, it cannot\"\n                \" reach localhost on your host system. Probably, you have to \"\n                \"change your serverl URL to http://host.docker.internal \"\n                \"(Windows/MacOS) or http://172.17.0.1 (Linux).\"\n            )\n        else:\n            self.log.debug(\"Are you sure the server can be reached at \"\n                           f\"{self.server_io.base_path}?\")\n\n    def authenticate(self) -> None:\n        \"\"\"\n        Authenticate with the server using the api-key from the configuration\n        file. If the server rejects for any reason -other than a wrong API key-\n        serveral attempts are taken to retry.\n        \"\"\"\n\n        api_key = self.config.get(\"api_key\")\n\n        success = False\n        i = 0\n        while i < TIME_LIMIT_RETRY_CONNECT_NODE / SLEEP_BTWN_NODE_LOGIN_TRIES:\n            i = i + 1\n            try:\n                self.server_io.authenticate(api_key)\n\n            except AuthenticationException as e:\n                msg = \"Authentication failed: API key is wrong!\"\n                self.log.warning(msg)\n                self.log.debug(e)\n                break\n            except requests.exceptions.ConnectionError:\n                self.__print_connection_error_logs()\n                time.sleep(SLEEP_BTWN_NODE_LOGIN_TRIES)\n            except Exception as e:\n                msg = ('Authentication failed. Retrying in '\n                       f'{SLEEP_BTWN_NODE_LOGIN_TRIES} seconds!')\n                self.log.warning(msg)\n                self.log.debug(e)\n                time.sleep(SLEEP_BTWN_NODE_LOGIN_TRIES)\n\n            else:\n                # This is only executed if try-block executed without error.\n                success = True\n                break\n\n        if success:\n            self.log.info(f\"Node name: {self.server_io.name}\")\n        else:\n            self.log.critical('Unable to authenticate. Exiting')\n            exit(1)\n\n    def private_key_filename(self) -> Path:\n        \"\"\"Get the path to the private key.\"\"\"\n\n        # FIXME: Code duplication: vantage6/cli/node.py uses a lot of the same\n        #   logic. Suggest moving this to ctx.get_private_key()\n        filename = self.config['encryption'][\"private_key\"]\n\n        # filename may be set to an empty string\n        if not filename:\n            filename = 'private_key.pem'\n\n        # If we're running dockerized, the location may have been overridden\n        filename = os.environ.get('PRIVATE_KEY', filename)\n\n        # If ctx.get_data_file() receives an absolute path, its returned as-is\n        fullpath = Path(self.ctx.get_data_file(filename))\n\n        return fullpath\n\n    def setup_encryption(self) -> None:\n        \"\"\" Setup encryption if the node is part of encrypted collaboration \"\"\"\n        encrypted_collaboration = self.server_io.is_encrypted_collaboration()\n        encrypted_node = self.config['encryption'][\"enabled\"]\n\n        if encrypted_collaboration != encrypted_node:\n            # You can't force it if it just ain't right, you know?\n            raise Exception(\"Expectations on encryption don't match?!\")\n\n        if encrypted_collaboration:\n            self.log.warn('Enabling encryption!')\n            private_key_file = self.private_key_filename()\n            self.server_io.setup_encryption(private_key_file)\n\n        else:\n            self.log.warn('Disabling encryption!')\n            self.server_io.setup_encryption(None)\n\n    def _set_task_dir(self, ctx) -> None:\n        \"\"\"\n        Set the task dir\n\n        Parameters\n        ----------\n        ctx: DockerNodeContext or NodeContext\n            Context object containing settings\n        \"\"\"\n        # If we're in a 'regular' context, we'll copy the dataset to our data\n        # dir and mount it in any algorithm container that's run; bind mounts\n        # on a folder will work just fine.\n        #\n        # If we're running in dockerized mode we *cannot* bind mount a folder,\n        # because the folder is in the container and not in the host. We'll\n        # have to use a docker volume instead. This means:\n        #  1. we need to know the name of the volume so we can pass it along\n        #  2. need to have this volume mounted so we can copy files to it.\n        #\n        #  Ad 1: We'll use a default name that can be overridden by an\n        #        environment variable.\n        #  Ad 2: We'll expect `ctx.data_dir` to point to the right place. This\n        #        is OK, since ctx will be a DockerNodeContext.\n        #\n        #  This also means that the volume will have to be created & mounted\n        #  *before* this node is started, so we won't do anything with it here.\n\n        # We'll create a subfolder in the data_dir. We need this subfolder so\n        # we can easily mount it in the algorithm containers; the root folder\n        # may contain the private key, which which we don't want to share.\n        # We'll only do this if we're running outside docker, otherwise we\n        # would create '/data' on the data volume.\n        if not ctx.running_in_docker:\n            self.__tasks_dir = ctx.data_dir / 'data'\n            os.makedirs(self.__tasks_dir, exist_ok=True)\n            self.__vpn_dir = ctx.data_dir / 'vpn'\n            os.makedirs(self.__vpn_dir, exist_ok=True)\n        else:\n            self.__tasks_dir = ctx.data_dir\n            self.__vpn_dir = ctx.vpn_dir\n\n    def setup_ssh_tunnels(self, isolated_network_mgr: Type[NetworkManager]) \\\n            -> List[SSHTunnel]:\n        \"\"\"\n        Create a SSH tunnels when they are defined in the configuration file.\n        For each tunnel a new container is created. The image used can be\n        specified in the configuration file as `ssh-tunnel` in the `images`\n        section, else the default image is used.\n\n        Parameters\n        ----------\n        isolated_network_mgr: NetworkManager\n            Manager for the isolated network\n        \"\"\"\n        if 'ssh-tunnels' not in self.config:\n            self.log.info(\"No SSH tunnels configured\")\n            return\n\n        custom_tunnel_image = self.config.get('images', {}).get('ssh-tunnel') \\\n            if 'images' in self.config else None\n\n        configs = self.config['ssh-tunnels']\n        self.log.info(f\"Setting up {len(configs)} SSH tunnels\")\n\n        tunnels: List[SSHTunnel] = []\n        for config in configs:\n            self.log.debug(f\"SSH tunnel config: {config}\")\n\n            # copy (rename) the ssh key to the correct name, this is done so\n            # that the file is in the volume (somehow we can not file mount\n            # within a volume)\n            if self.ctx.running_in_docker:\n                ssh_key = f\"/mnt/ssh/{config['hostname']}.pem.tmp\"\n                key_path = shutil.copy(ssh_key,\n                                       f\"/mnt/ssh/{config['hostname']}.pem\")\n                volume = self.ctx.docker_ssh_volume_name\n\n            else:\n                ssh_key = config['ssh']['identity']['key']\n\n                volume = str(Path(ssh_key).parent)\n                key_path = shutil.copy(ssh_key,\n                                       f\"{volume}/{config['hostname']}.pem\")\n\n            os.chmod(key_path, 0o600)\n\n            try:\n                new_tunnel = SSHTunnel(isolated_network_mgr, config,\n                                       self.ctx.name, volume,\n                                       custom_tunnel_image)\n            except Exception as e:\n                self.log.error(\"Error setting up SSH tunnel\")\n                self.log.debug(e, exc_info=True)\n                continue\n\n            tunnels.append(new_tunnel)\n\n        return tunnels\n\n    def setup_vpn_connection(self, isolated_network_mgr: NetworkManager,\n                             ctx: Union[DockerNodeContext, NodeContext]\n                             ) -> VPNManager:\n        \"\"\"\n        Setup container which has a VPN connection\n\n        Parameters\n        ----------\n        isolated_network_mgr: NetworkManager\n            Manager for the isolated Docker network\n        ctx: NodeContext\n            Context object for the node\n\n        Returns\n        -------\n        VPNManager\n            Manages the VPN connection\n        \"\"\"\n        ovpn_file = os.path.join(self.__vpn_dir, VPN_CONFIG_FILE)\n\n        self.log.info(\"Setting up VPN client container\")\n        vpn_volume_name = self.ctx.docker_vpn_volume_name \\\n            if ctx.running_in_docker else self.__vpn_dir\n\n        # FIXME: remove me in 4+. alpine image has been moved into the `images`\n        # key. This is to support older configuration files.\n        legacy_alpine = self.config.get('alpine')\n\n        # user can specify custom images in the configuration file\n        custom_alpine = self.config['images'].get('alpine') \\\n            if 'images' in self.config else None\n        custom_vpn_client = self.config['images'].get('vpn_client') \\\n            if 'images' in self.config else None\n        custom_network = self.config['images'].get('network_config') \\\n            if 'images' in self.config else None\n\n        vpn_manager = VPNManager(\n            isolated_network_mgr=isolated_network_mgr,\n            node_name=self.ctx.name,\n            vpn_volume_name=vpn_volume_name,\n            vpn_subnet=self.config.get('vpn_subnet'),\n            alpine_image=custom_alpine or legacy_alpine,\n            vpn_client_image=custom_vpn_client,\n            network_config_image=custom_network\n        )\n\n        if not self.config.get('vpn_subnet'):\n            self.log.warn(\"VPN subnet is not defined! VPN disabled.\")\n        elif not os.path.isfile(ovpn_file):\n            # if vpn config doesn't exist, get it and write to disk\n            self._connect_vpn(vpn_manager, VPNConnectMode.REFRESH_COMPLETE,\n                              ovpn_file)\n        else:\n            self._connect_vpn(vpn_manager, VPNConnectMode.FIRST_TRY, ovpn_file)\n\n        return vpn_manager\n\n    def _connect_vpn(self, vpn_manager: VPNManager,\n                     connect_mode: VPNConnectMode, ovpn_file: str) -> None:\n        \"\"\"\n        Connect to the VPN by starting up a VPN client container. If no VPN\n        config file exists, we only try once after first obtaining a config\n        file. If a VPN config file already exists, we first try to connect,\n        then try to refresh the keypair, and finally try to renew the entire\n        config file, until a connection is established.\n\n        Parameters\n        ----------\n        vpn_manager: VPNManager\n            Manages the VPN connection\n        connect_mode: VPNConnectMode\n            Specifies which parts of a config file to refresh before attempting\n            to connect\n        ovpn_file: str\n            Path to the VPN configuration file\n        \"\"\"\n        do_try = True\n        if connect_mode == VPNConnectMode.FIRST_TRY:\n            self.log.debug(\"Using existing config file to connect to VPN\")\n            next_mode = VPNConnectMode.REFRESH_KEYPAIR\n        elif connect_mode == VPNConnectMode.REFRESH_KEYPAIR:\n            self.log.debug(\"Refreshing VPN keypair...\")\n            do_try = self.server_io.refresh_vpn_keypair(ovpn_file=ovpn_file)\n            next_mode = VPNConnectMode.REFRESH_COMPLETE\n        elif connect_mode == VPNConnectMode.REFRESH_COMPLETE:\n            self.log.debug(\"Requesting new VPN configuration file...\")\n            do_try = self._get_vpn_config_file(ovpn_file)\n            next_mode = None  # if new config file doesn't work, give up\n\n        if do_try:\n            # try connecting to VPN\n            try:\n                vpn_manager.connect_vpn()\n            except Exception as e:\n                self.log.debug(\"Could not connect to VPN.\")\n                self.log.debug(f\"Exception: {e}\")\n                # try again in another fashion\n                if next_mode:\n                    self._connect_vpn(vpn_manager, next_mode, ovpn_file)\n\n    def _get_vpn_config_file(self, ovpn_file: str) -> bool:\n        \"\"\"\n        Obtain VPN configuration file from the server\n\n        Parameters\n        ----------\n        ovpn_file: str\n            Path to the VPN configuration file\n\n        Returns\n        -------\n        bool\n            Whether or not configuration file was successfully obtained\n        \"\"\"\n        # get the ovpn configuration from the server\n        success, ovpn_config = self.server_io.get_vpn_config()\n        if not success:\n            self.log.warn(\"Obtaining VPN configuration file not successful!\")\n            self.log.warn(\"Disabling node-to-node communication via VPN\")\n            return False\n\n        # write ovpn config to node docker volume\n        with open(ovpn_file, 'w') as f:\n            f.write(ovpn_config)\n        return True\n\n    def link_docker_services(self) -> None:\n        docker_services = self.ctx.config.get(\"docker_services\")\n        if not docker_services:\n            return\n        self.log.info(\"Linking docker services specified in the configuration\")\n        for alias, container_name in docker_services.items():\n            self.__docker.link_container_to_network(\n                container_name=container_name, config_alias=alias\n            )\n\n    def connect_to_socket(self) -> None:\n        \"\"\"\n        Create long-lasting websocket connection with the server. The\n        connection is used to receive status updates, such as new tasks.\n        \"\"\"\n        self.socketIO = SocketIO(request_timeout=60)\n\n        self.socketIO.register_namespace(NodeTaskNamespace('/tasks'))\n        NodeTaskNamespace.node_worker_ref = self\n\n        self.socketIO.connect(\n            url=f'{self.server_io.host}:{self.server_io.port}',\n            headers=self.server_io.headers,\n            wait=False\n        )\n\n        # Log the outcome\n        i = 0\n        while not self.socketIO.connected:\n            if i > TIME_LIMIT_INITIAL_CONNECTION_WEBSOCKET:\n                self.log.critical('Could not connect to the websocket '\n                                  'channels, do you have a slow connection?')\n                exit(1)\n            self.log.debug('Waiting for socket connection...')\n            time.sleep(1)\n            i += 1\n\n        self.log.info(f'Connected to host={self.server_io.host} on port='\n                      f'{self.server_io.port}')\n\n    def get_task_and_add_to_queue(self, task_id: int) -> None:\n        \"\"\"\n        Fetches (open) task with task_id from the server. The `task_id` is\n        delivered by the websocket-connection.\n\n        Parameters\n        ----------\n        task_id : int\n            Task identifier\n        \"\"\"\n        # fetch (open) result for the node with the task_id\n        tasks = self.server_io.get_results(\n            include_task=True,\n            state='open',\n            task_id=task_id\n        )\n\n        # in the current setup, only a single result for a single node\n        # in a task exists.\n        for task in tasks:\n            self.queue.put(task)\n\n    def run_forever(self) -> None:\n        \"\"\"Keep checking queue for incoming tasks (and execute them).\"\"\"\n        kill_listener = ContainerKillListener()\n        try:\n            while True:\n                # blocking untill a task comes available\n                # timeout specified, else Keyboard interupts are ignored\n                self.log.info(\"Waiting for new tasks....\")\n\n                while not kill_listener.kill_now:\n                    try:\n                        task = self.queue.get(timeout=1)\n                        # if no item is returned, the Empty exception is\n                        # triggered, thus break statement is not reached\n                        break\n\n                    except queue.Empty:\n                        pass\n\n                    except Exception as e:\n                        self.log.debug(e)\n\n                if kill_listener.kill_now:\n                    raise InterruptedError\n\n                # if task comes available, attempt to execute it\n                try:\n                    self.__start_task(task)\n                except Exception as e:\n                    self.log.exception(e)\n\n        except (KeyboardInterrupt, InterruptedError):\n            self.log.info(\"Vnode is interrupted, shutting down...\")\n            self.cleanup()\n            sys.exit()\n\n    def kill_containers(self, kill_info: Dict) -> List[Dict]:\n        \"\"\"\n        Kill containers on instruction from socket event\n\n        Parameters\n        ----------\n        kill_info: Dict\n            Dictionary received over websocket with instructions for which\n            tasks to kill\n\n        Returns\n        -------\n        List[Dict]:\n            List of dictionaries with information on killed task (keys:\n            result_id, task_id and parent_id)\n        \"\"\"\n        if kill_info['collaboration_id'] != self.server_io.collaboration_id:\n            self.log.debug(\n                \"Not killing tasks as this node is in another collaboration.\"\n            )\n            return []\n        elif 'node_id' in kill_info and \\\n                kill_info['node_id'] != self.server_io.whoami.id_:\n            self.log.debug(\n                \"Not killing tasks as instructions to kill tasks were directed\"\n                \" at another node in this collaboration.\")\n            return []\n\n        # kill specific task if specified, else kill all algorithms\n        kill_list = kill_info.get('kill_list')\n        killed_algos = self.__docker.kill_tasks(\n            org_id=self.server_io.whoami.organization_id, kill_list=kill_list\n        )\n        # update status of killed tasks\n        for killed_algo in killed_algos:\n            self.server_io.patch_results(\n                id=killed_algo.result_id, result={'status': TaskStatus.KILLED}\n            )\n        return killed_algos\n\n    def share_node_details(self) -> None:\n        \"\"\"\n        Share part of the node's configuration with the server.\n\n        This helps the other parties in a collaboration to see e.g. which\n        algorithms they are allowed to run on this node.\n        \"\"\"\n        # check if node allows to share node details, otherwise return\n        if not self.config.get('share_config', True):\n            self.log.debug(\"Not sharing node configuration in accordance with \"\n                           \"the configuration setting.\")\n            return\n\n        config_to_share = {}\n\n        encryption_config = self.config.get('encryption')\n        if encryption_config:\n            if encryption_config.get('enabled') is not None:\n                config_to_share['encryption'] = \\\n                    encryption_config.get('enabled')\n\n        allowed_algos = self.config.get('allowed_images')\n        config_to_share['allowed_images'] = allowed_algos if allowed_algos \\\n            else 'all'\n\n        self.log.debug(f\"Sharing node configuration: {config_to_share}\")\n        self.socketIO.emit(\n            'node_info_update', config_to_share, namespace='/tasks'\n        )\n\n    def cleanup(self) -> None:\n\n        if hasattr(self, 'socketIO') and self.socketIO:\n            self.socketIO.disconnect()\n        if hasattr(self, 'vpn_manager') and self.vpn_manager:\n            self.vpn_manager.exit_vpn()\n        if hasattr(self, 'ssh_tunnels') and self.ssh_tunnels:\n            for tunnel in self.ssh_tunnels:\n                tunnel.stop()\n        if hasattr(self, '_Node__docker') and self.__docker:\n            self.__docker.cleanup()\n\n        self.log.info(\"Bye!\")\n\n\n# ------------------------------------------------------------------------------\ndef run(ctx):\n    \"\"\" Start the node.\"\"\"\n    logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n    logging.getLogger(\"requests\").setLevel(logging.WARNING)\n    logging.getLogger(\"engineio.client\").setLevel(logging.WARNING)\n\n    # initialize node, connect to the server using websockets\n    node = Node(ctx)\n\n    # put the node to work, executing tasks that are in the que\n    node.run_forever()\n", "from pathlib import Path\nfrom vantage6.common.globals import APPNAME\n\n#\n#   NODE SETTINGS\n#\nDEFAULT_NODE_SYSTEM_FOLDERS = False\n\nDEFAULT_NODE_ENVIRONMENT = \"application\"\n\n\n#\n#   INSTALLATION SETTINGS\n#\nPACKAGE_FOLDER = Path(__file__).parent.parent.parent\n\nNODE_PROXY_SERVER_HOSTNAME = \"proxyserver\"\n\nDATA_FOLDER = PACKAGE_FOLDER / APPNAME / \"_data\"\n\n# with open(Path(PACKAGE_FOLDER) / APPNAME / \"node\" / \"VERSION\") as f:\n#     VERSION = f.read()\n\n\n# constants for retrying node login\nSLEEP_BTWN_NODE_LOGIN_TRIES = 10  # retry every 10s\nTIME_LIMIT_RETRY_CONNECT_NODE = 60 * 60 * 24 * 7  # i.e. 1 week\n\n# constant for waiting for the initial websocket connection\nTIME_LIMIT_INITIAL_CONNECTION_WEBSOCKET = 60\n\n#\n#    VPN CONFIGURATION RELATED CONSTANTS\n#\n# TODO move part of these constants elsewhere?! Or make context?\nVPN_CLIENT_IMAGE = 'harbor2.vantage6.ai/infrastructure/vpn-client'\nNETWORK_CONFIG_IMAGE = 'harbor2.vantage6.ai/infrastructure/vpn-configurator'\nALPINE_IMAGE = 'harbor2.vantage6.ai/infrastructure/alpine'\nMAX_CHECK_VPN_ATTEMPTS = 60   # max attempts to obtain VPN IP (1 second apart)\nFREE_PORT_RANGE = range(49152, 65535)\nDEFAULT_ALGO_VPN_PORT = '8888'  # default VPN port for algorithm container\n\n#\n#   SSH TUNNEL RELATED CONSTANTS\n#\nSSH_TUNNEL_IMAGE = \"harbor2.vantage6.ai/infrastructure/ssh-tunnel\"\n", "\"\"\"\nThis module provides a client interface for the node to communicate with the\ncentral server.\n\"\"\"\nimport jwt\nimport datetime\nfrom typing import Dict, Tuple\n\nfrom vantage6.common import WhoAmI\nfrom vantage6.client import ClientBase\n\n\nclass NodeClient(ClientBase):\n    \"\"\" Node interface to the central server.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # self.name = None\n        self.collaboration_id = None\n        self.whoami = None\n\n    def authenticate(self, api_key: str):\n        \"\"\" Nodes authentication at the central server.\n\n            It also identifies itself by retrieving the collaboration\n            and organization to which this node belongs. The server\n            returns a JWT-token that is used in all succeeding requests.\n\n            :param api_key: api-key used to authenticate to the central\n                server\n        \"\"\"\n        super().authenticate({\"api_key\": api_key}, path=\"token/node\")\n\n        # obtain the server authenticatable id\n        jwt_payload = jwt.decode(self.token,\n                                 options={\"verify_signature\": False})\n\n        # FIXME: 'identity' is no longer needed in version 4+. So this if\n        # statement can be removed\n        if 'sub' in jwt_payload:\n            id_ = jwt_payload['sub']\n        elif 'identity' in jwt_payload:\n            id_ = jwt_payload['identity']\n\n        # get info on how the server sees me\n        node = self.request(f\"node/{id_}\")\n\n        name = node.get(\"name\")\n        self.collaboration_id = node.get(\"collaboration\").get(\"id\")\n\n        organization_id = node.get(\"organization\").get(\"id\")\n        organization = self.request(f\"organization/{organization_id}\")\n        organization_name = organization.get(\"name\")\n\n        self.whoami = WhoAmI(\n            type_=\"node\",\n            id_=id_,\n            name=name,\n            organization_id=organization_id,\n            organization_name=organization_name\n        )\n\n    def request_token_for_container(self, task_id: int, image: str):\n        \"\"\" Request a container-token at the central server.\n\n            This token is used by algorithm containers that run on this\n            node. These algorithms can then post tasks and retrieve\n            child-results (usually refered to as a master container).\n            The server performs a few checks (e.g. if the task you\n            request the key for is still open) before handing out this\n            token.\n\n            :param task_id: id from the task, which is going to use this\n                container-token (a task results in a algorithm-\n                container at the node)\n            :param image: image-name of the task\n        \"\"\"\n        self.log.debug(\n            f\"requesting container token for task_id={task_id} \"\n            f\"and image={image}\"\n        )\n        return self.request('/token/container', method=\"post\", json={\n            \"task_id\": task_id,\n            \"image\": image\n        })\n\n    def get_results(self, id=None, state=None, include_task=False,\n                    task_id=None):\n        \"\"\" Obtain the results for a specific task.\n\n            Overload the definition of the parent by entering the\n            task_id automatically.\n        \"\"\"\n        return super().get_results(\n            id=id,\n            state=state,\n            include_task=include_task,\n            task_id=task_id,\n            node_id=self.whoami.id_\n        )\n\n    def is_encrypted_collaboration(self):\n        \"\"\" Boolean whenever the encryption is enabled.\n\n            End-to-end encryption is per collaboration managed at the\n            central server. It is important to note that the local\n            configuration-file should allow explicitly for unencrpyted\n            messages. This function returns the setting from the server.\n        \"\"\"\n        response = self.request(f\"collaboration/{self.collaboration_id}\")\n        return response.get(\"encrypted\") == 1\n\n    def set_task_start_time(self, id: int):\n        \"\"\" Sets the start time of the task at the central server.\n\n            This is important as this will note that the task has been\n            started, and is waiting for restuls.\n\n            :param id: id of the task to set the start-time of\n\n            TODO the initiator_id does not make sens here...\n        \"\"\"\n        self.patch_results(id, result={\n            \"started_at\": datetime.datetime.now().isoformat()\n        })\n\n    def patch_results(self, id: int, result: Dict,\n                      init_org_id: int = None) -> None:\n        \"\"\"\n        Update the results at the central server.\n\n        Typically used when to algorithm container is finished or\n        when a status-update is posted (started, finished)\n\n        Parameters\n        ----------\n        id: int\n            ID of the result to patch\n        result: Dict\n            Dictionary of fields that are to be patched\n        init_org_id: int, optional\n            Organization id of the origin of the task. This is required\n            when the result dict includes results, because then results have\n            to be encrypted specifically for them\n        \"\"\"\n        # TODO: the key `result` is not always present, e.g. when\n        #     only the timestamps are updated\n        # FIXME: public keys should be cached\n        if \"result\" in result:\n            if not init_org_id:\n                self.log.critical(\n                    \"Organization id is not provided: cannot send results to \"\n                    \"server as they cannot be encrypted\")\n            msg = f\"Retrieving public key from organization={init_org_id}\"\n            self.log.debug(msg)\n\n            org = self.request(f\"organization/{init_org_id}\")\n            public_key = None\n            try:\n                public_key = org[\"public_key\"]\n            except KeyError:\n                self.log.critical('Public key could not be retrieved...')\n                self.log.critical('Does the initiating organization belong to '\n                                  'your organization?')\n\n            result[\"result\"] = self.cryptor.encrypt_bytes_to_str(\n                result[\"result\"],\n                public_key\n            )\n\n            self.log.debug(\"Sending results to server\")\n        else:\n            self.log.debug(\"Just patchin'\")\n\n        return self.request(f\"result/{id}\", json=result, method='patch')\n\n    def get_vpn_config(self) -> Tuple[bool, str]:\n        \"\"\"\n        Obtain VPN configuration from the server\n\n        Returns\n        -------\n        bool\n            Whether or not obtaining VPN config was successful\n        str\n            OVPN configuration file content\n        \"\"\"\n        response = self.request(\"vpn\")\n\n        ovpn_config = response.get(\"ovpn_config\")\n        if ovpn_config is None:\n            return False, ''\n\n        # replace windows line endings to linux style to prevent extra\n        # whitespace in writing the file\n        ovpn_config = ovpn_config.replace(\"\\r\\n\", \"\\n\")\n\n        return True, ovpn_config\n\n    def refresh_vpn_keypair(self, ovpn_file: str) -> bool:\n        \"\"\"\n        Refresh the client's keypair in an ovpn configuration file\n\n        Parameters\n        ----------\n        ovpn_file: str\n            The path to the current ovpn configuration on disk\n        \"\"\"\n        # Extract the contents of the VPN file\n        with open(ovpn_file, 'r') as file:\n            ovpn_config = file.read()\n\n        response = self.request(\n            \"vpn/update\",\n            method=\"POST\",\n            json={'vpn_config': ovpn_config},\n        )\n        ovpn_config = response.get(\"ovpn_config\")\n        if not ovpn_config:\n            self.log.warn(\"Refreshing VPN keypair not successful!\")\n            self.log.warn(\"Disabling node-to-node communication via VPN\")\n            return False\n\n        # write new configuration back to file\n        with open(ovpn_file, 'w') as f:\n            f.write(ovpn_config)\n        return True\n", "# -*- coding: utf-8 -*-\nfrom gevent import monkey\n\n# flake8: noqa: E402 (ignore import error)\nmonkey.patch_all()\n\nimport importlib\nimport logging\nimport os\nimport uuid\nimport json\nimport time\nimport datetime as dt\nimport traceback\n\nfrom http import HTTPStatus\nfrom werkzeug.exceptions import HTTPException\nfrom flasgger import Swagger\nfrom flask import (\n    Flask, make_response, current_app, request, send_from_directory, Request\n)\nfrom flask_cors import CORS\nfrom flask_jwt_extended import JWTManager\nfrom flask_marshmallow import Marshmallow\nfrom flask_restful import Api\nfrom flask_mail import Mail\nfrom flask_principal import Principal, Identity, identity_changed\nfrom flask_socketio import SocketIO\nfrom threading import Thread\n\nfrom vantage6.server import db\nfrom vantage6.cli.context import ServerContext\nfrom vantage6.server.model.base import DatabaseSessionManager, Database\nfrom vantage6.server.resource.common._schema import HATEOASModelSchema\nfrom vantage6.common import logger_name\nfrom vantage6.server.permission import RuleNeed, PermissionManager\nfrom vantage6.server.globals import (\n    APPNAME,\n    JWT_ACCESS_TOKEN_EXPIRES,\n    JWT_TEST_ACCESS_TOKEN_EXPIRES,\n    RESOURCES,\n    SUPER_USER_INFO,\n    REFRESH_TOKENS_EXPIRE,\n    DEFAULT_SUPPORT_EMAIL_ADDRESS,\n    MAX_RESPONSE_TIME_PING\n)\nfrom vantage6.server.resource.common.swagger_templates import swagger_template\nfrom vantage6.server._version import __version__\nfrom vantage6.server.mail_service import MailService\nfrom vantage6.server.websockets import DefaultSocketNamespace\nfrom vantage6.server.default_roles import get_default_roles, DefaultRole\n\n\nmodule_name = logger_name(__name__)\nlog = logging.getLogger(module_name)\n\n\nclass ServerApp:\n    \"\"\"Vantage6 server instance.\"\"\"\n\n    def __init__(self, ctx):\n        \"\"\"Create a vantage6-server application.\"\"\"\n\n        self.ctx = ctx\n\n        # initialize, configure Flask\n        self.app = Flask(APPNAME, root_path=os.path.dirname(__file__))\n        self.configure_flask()\n\n        # Setup SQLAlchemy and Marshmallow for marshalling/serializing\n        self.ma = Marshmallow(self.app)\n\n        # Setup the Flask-JWT-Extended extension (JWT: JSON Web Token)\n        self.jwt = JWTManager(self.app)\n        self.configure_jwt()\n\n        # Setup Principal, granular API access manegement\n        self.principal = Principal(self.app, use_sessions=False)\n\n        # Enable cross-origin resource sharing\n        self.cors = CORS(self.app)\n\n        # SWAGGER documentation\n        self.swagger = Swagger(self.app, template=swagger_template)\n\n        # Setup the Flask-Mail client\n        self.mail = MailService(self.app, Mail(self.app))\n\n        # Setup websocket channel\n        self.socketio = self.setup_socket_connection()\n\n        # setup the permission manager for the API endpoints\n        self.permissions = PermissionManager()\n\n        # Api - REST JSON-rpc\n        self.api = Api(self.app)\n        self.configure_api()\n        self.load_resources()\n\n        # make specific log settings (muting etc)\n        self.configure_logging()\n\n        # set the server version\n        self.__version__ = __version__\n\n        # set up socket ping/pong\n        log.debug(\n            \"Starting thread for socket ping/pong between server and nodes\")\n        self.socketio.start_background_task(self.__socket_pingpong_worker)\n\n        log.info(\"Initialization done\")\n\n    def setup_socket_connection(self):\n\n        msg_queue = self.ctx.config.get('rabbitmq_uri')\n        if msg_queue:\n            log.debug(f'Connecting to msg queue: {msg_queue}')\n\n        try:\n            socketio = SocketIO(\n                self.app,\n                async_mode='gevent_uwsgi',\n                message_queue=msg_queue,\n                ping_timeout=60,\n                cors_allowed_origins='*'\n            )\n        except Exception as e:\n            log.warning('Default socketio settings failed, attempt to run '\n                        'without gevent_uwsgi packages! This leads to '\n                        'performance issues and possible issues concerning '\n                        'the websocket channels!')\n            log.debug(e)\n            socketio = SocketIO(\n                self.app,\n                message_queue=msg_queue,\n                ping_timeout=60,\n                cors_allowed_origins='*'\n            )\n\n        # FIXME: temporary fix to get socket object into the namespace class\n        DefaultSocketNamespace.socketio = socketio\n        socketio.on_namespace(DefaultSocketNamespace(\"/tasks\"))\n\n        return socketio\n\n    @staticmethod\n    def configure_logging():\n        \"\"\"Turn 3rd party loggers off.\"\"\"\n\n        # Prevent logging from urllib3\n        logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n        logging.getLogger(\"socketIO-client\").setLevel(logging.WARNING)\n        logging.getLogger(\"engineio.server\").setLevel(logging.WARNING)\n        logging.getLogger(\"socketio.server\").setLevel(logging.WARNING)\n        logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)\n        logging.getLogger('requests_oauthlib.oauth2_session')\\\n            .setLevel(logging.WARNING)\n\n    def configure_flask(self):\n        \"\"\"All flask config settings should go here.\"\"\"\n\n        # let us handle exceptions\n        self.app.config['PROPAGATE_EXCEPTIONS'] = True\n\n        # patch where to obtain token\n        self.app.config['JWT_AUTH_URL_RULE'] = '/api/token'\n\n        # False means refresh tokens never expire\n        self.app.config['JWT_REFRESH_TOKEN_EXPIRES'] = REFRESH_TOKENS_EXPIRE\n\n        # If no secret is set in the config file, one is generated. This\n        # implies that all (even refresh) tokens will be invalidated on restart\n        self.app.config['JWT_SECRET_KEY'] = self.ctx.config.get(\n            'jwt_secret_key',\n            str(uuid.uuid1())\n        )\n\n        # Default expiration time\n        self.app.config['JWT_ACCESS_TOKEN_EXPIRES'] = JWT_ACCESS_TOKEN_EXPIRES\n\n        # Set an extra long expiration time on access tokens for testing\n        # TODO: this does not seem needed...\n        environment = self.ctx.config.get('type')\n        self.app.config['environment'] = environment\n        if environment == 'test':\n            log.warning(\"Setting 'JWT_ACCESS_TOKEN_EXPIRES' to one day!\")\n            self.app.config['JWT_ACCESS_TOKEN_EXPIRES'] = \\\n                JWT_TEST_ACCESS_TOKEN_EXPIRES\n\n        # Open Api Specification (f.k.a. swagger)\n        self.app.config['SWAGGER'] = {\n            'title': APPNAME,\n            'uiversion': \"3\",\n            'openapi': '3.0.0',\n            'version': __version__\n        }\n\n        # Mail settings\n        mail_config = self.ctx.config.get(\"smtp\", {})\n        self.app.config[\"MAIL_PORT\"] = mail_config.get(\"port\", 1025)\n        self.app.config[\"MAIL_SERVER\"] = mail_config.get(\"server\", \"localhost\")\n        self.app.config[\"MAIL_USERNAME\"] = mail_config.get(\n            \"username\",\n            DEFAULT_SUPPORT_EMAIL_ADDRESS\n        )\n        self.app.config[\"MAIL_PASSWORD\"] = mail_config.get(\"password\", \"\")\n        self.app.config[\"MAIL_USE_TLS\"] = mail_config.get(\"MAIL_USE_TLS\",\n                                                          True)\n        self.app.config[\"MAIL_USE_SSL\"] = mail_config.get(\"MAIL_USE_SSL\",\n                                                          False)\n\n        def _get_request_path(request: Request) -> str:\n            \"\"\"\n            Return request extension of request URL, e.g.\n            http://localhost:5000/api/task/1 -> api/task/1\n\n            Parameters\n            ----------\n            request: Request\n                Flask request object\n\n            Returns\n            -------\n            string:\n                The endpoint path of the request\n            \"\"\"\n            return request.url.replace(request.url_root, '')\n\n        # before request\n        @self.app.before_request\n        def do_before_request():\n            \"\"\"Before every flask request method.\"\"\"\n            # Add log message before each request\n            log.debug(f\"Received request: {request.method} \"\n                      f\"{_get_request_path(request)}\")\n\n            # This will obtain a (scoped) db session from the session factory\n            # that is linked to the flask request global `g`. In every endpoint\n            # we then can access the database by using this session. We ensure\n            # that the session is removed (and uncommited changes are rolled\n            # back) at the end of every request.\n            DatabaseSessionManager.new_session()\n\n        @self.app.after_request\n        def remove_db_session(response):\n            \"\"\"After every flask request.\n\n            This will close the database session created by the\n            `before_request`.\n            \"\"\"\n            DatabaseSessionManager.clear_session()\n            return response\n\n        @self.app.errorhandler(HTTPException)\n        def error_remove_db_session(error: HTTPException):\n            \"\"\"In case an HTTP-exception occurs during the request.\n\n            It is important to close the db session to avoid having dangling\n            sessions.\n            \"\"\"\n            if error.code == 404:\n                log.debug(\n                    f\"404 error for route '{_get_request_path(request)}'\")\n            else:\n                log.warn('HTTP Exception occured during request')\n                log.debug(traceback.format_exc())\n            DatabaseSessionManager.clear_session()\n            return error.get_response()\n\n        @self.app.errorhandler(Exception)\n        def error2_remove_db_session(error):\n            \"\"\"In case an exception occurs during the request.\n\n            It is important to close the db session to avoid having dangling\n            sessions.\n            \"\"\"\n            log.exception('Exception occured during request')\n            DatabaseSessionManager.clear_session()\n            return {'msg': f'An unexpected error occurred on the server!'}, \\\n                HTTPStatus.INTERNAL_SERVER_ERROR\n\n        @self.app.route('/robots.txt')\n        def static_from_root():\n            return send_from_directory(self.app.static_folder,\n                                       request.path[1:])\n\n    def configure_api(self):\n        \"\"\"\"Define global API output.\"\"\"\n\n        # helper to create HATEOAS schemas\n        HATEOASModelSchema.api = self.api\n\n        # whatever you get try to json it\n        @self.api.representation('application/json')\n        def output_json(data, code, headers=None):\n\n            if isinstance(data, db.Base):\n                data = db.jsonable(data)\n            elif isinstance(data, list) and len(data) and \\\n                    isinstance(data[0], db.Base):\n                data = db.jsonable(data)\n\n            resp = make_response(json.dumps(data), code)\n            resp.headers.extend(headers or {})\n            return resp\n\n    def configure_jwt(self):\n        \"\"\"Load user and its claims.\"\"\"\n\n        @self.jwt.additional_claims_loader\n        def additional_claims_loader(identity):\n            roles = []\n            if isinstance(identity, db.User):\n                type_ = 'user'\n                roles = [role.name for role in identity.roles]\n\n            elif isinstance(identity, db.Node):\n                type_ = 'node'\n            elif isinstance(identity, dict):\n                type_ = 'container'\n            else:\n                log.error(f\"could not create claims from {str(identity)}\")\n                return\n\n            claims = {\n                'client_type': type_,\n                'roles': roles,\n            }\n\n            return claims\n\n        @self.jwt.user_identity_loader\n        def user_identity_loader(identity):\n            \"\"\"\"JSON serializing identity to be used by create_access_token.\"\"\"\n            if isinstance(identity, db.Authenticatable):\n                return identity.id\n            if isinstance(identity, dict):\n                return identity\n\n            log.error(f\"Could not create a JSON serializable identity \\\n                        from '{str(identity)}'\")\n\n        @self.jwt.user_lookup_loader\n        def user_lookup_loader(jwt_payload, jwt_headers):\n            identity = jwt_headers['sub']\n            auth_identity = Identity(identity)\n\n            # in case of a user or node an auth id is shared as identity\n            if isinstance(identity, int):\n\n                # auth_identity = Identity(identity)\n\n                auth = db.Authenticatable.get(identity)\n\n                if isinstance(auth, db.Node):\n\n                    for rule in db.Role.get_by_name(DefaultRole.NODE).rules:\n                        auth_identity.provides.add(\n                                RuleNeed(\n                                    name=rule.name,\n                                    scope=rule.scope,\n                                    operation=rule.operation\n                                )\n                            )\n\n                if isinstance(auth, db.User):\n\n                    # add role permissions\n                    for role in auth.roles:\n                        for rule in role.rules:\n                            auth_identity.provides.add(\n                                RuleNeed(\n                                    name=rule.name,\n                                    scope=rule.scope,\n                                    operation=rule.operation\n                                )\n                            )\n\n                    # add 'extra' permissions\n                    for rule in auth.rules:\n                        auth_identity.provides.add(\n                            RuleNeed(\n                                name=rule.name,\n                                scope=rule.scope,\n                                operation=rule.operation\n                            )\n                        )\n\n                identity_changed.send(current_app._get_current_object(),\n                                      identity=auth_identity)\n\n                return auth\n            else:\n\n                for rule in db.Role.get_by_name(DefaultRole.CONTAINER).rules:\n                    auth_identity.provides.add(\n                        RuleNeed(\n                            name=rule.name,\n                            scope=rule.scope,\n                            operation=rule.operation\n                        )\n                    )\n                identity_changed.send(current_app._get_current_object(),\n                                      identity=auth_identity)\n                log.debug(identity)\n                return identity\n\n    def load_resources(self):\n        \"\"\"Import the modules containing Resources.\"\"\"\n\n        # make services available to the endpoints, this way each endpoint can\n        # make use of 'em.\n        services = {\n            \"socketio\": self.socketio,\n            \"mail\": self.mail,\n            \"api\": self.api,\n            \"permissions\": self.permissions,\n            \"config\": self.ctx.config\n        }\n\n        for res in RESOURCES:\n            module = importlib.import_module('vantage6.server.resource.' + res)\n            module.setup(self.api, self.ctx.config['api_path'], services)\n\n    # TODO consider moving this method elsewhere. This is not trivial at the\n    # moment because of the circular import issue with `db`, see\n    # https://github.com/vantage6/vantage6/issues/53\n    @staticmethod\n    def _add_default_roles():\n        for role in get_default_roles(db):\n            if not db.Role.get_by_name(role['name']):\n                log.warn(f\"Creating new default role {role['name']}...\")\n                new_role = db.Role(\n                    name=role['name'],\n                    description=role['description'],\n                    rules=role['rules']\n                )\n                new_role.save()\n\n    def start(self):\n        \"\"\"Start the server.\n        \"\"\"\n\n        # add default roles (if they don't exist yet)\n        self._add_default_roles()\n\n        # create root user if it is not in the DB yet\n        try:\n            db.User.get_by_username(SUPER_USER_INFO['username'])\n        except Exception:\n            log.warn(\"No root user found! Is this the first run?\")\n\n            log.debug(\"Creating organization for root user\")\n            org = db.Organization(name=\"root\")\n\n            # TODO use constant instead of 'Root' literal\n            root = db.Role.get_by_name(\"Root\")\n\n            log.warn(f\"Creating root user: \"\n                     f\"username={SUPER_USER_INFO['username']}, \"\n                     f\"password={SUPER_USER_INFO['password']}\")\n\n            user = db.User(username=SUPER_USER_INFO['username'], roles=[root],\n                           organization=org, email=\"root@domain.ext\",\n                           password=SUPER_USER_INFO['password'],\n                           failed_login_attempts=0,\n                           last_login_attempt=None)\n            user.save()\n        return self\n\n    def __socket_pingpong_worker(self) -> None:\n        \"\"\"\n        Send ping messages periodically to nodes over the socketIO connection\n        and set node status online/offline depending on whether they respond\n        or not.\n        \"\"\"\n        # when starting up the server, wait a few seconds to allow nodes that\n        # are already online to connect back to the server (otherwise they\n        # would be incorrectly set to offline for one period)\n        time.sleep(5)\n\n        # start periodic check if nodes are responsive\n        while True:\n            # Send ping event\n            try:\n                ping_time = dt.datetime.utcnow()\n                self.socketio.emit(\n                    'ping', namespace='/tasks', room='all_nodes',\n                    callback=self.__pong_response\n                )\n\n                # Wait a while to give nodes opportunity to pong\n                time.sleep(MAX_RESPONSE_TIME_PING)\n\n                # Check for each node that is online if they have responded.\n                # Otherwise set them to offline.\n                online_status_nodes = db.Node.get_online_nodes()\n                for node in online_status_nodes:\n                    if node.last_seen < ping_time:\n                        node.status = 'offline'\n                        node.save()\n\n                # we need to sleep here for a bit to make sure that there is a\n                # delay between setting nodes offline and pinging again - this\n                # prevents a racing condition in setting status\n                time.sleep(5)\n            except Exception:\n                log.exception('Pingpong thread had an exception')\n                time.sleep(MAX_RESPONSE_TIME_PING)\n\n    def __pong_response(self, node_id) -> None:\n        node = db.Node.get(node_id)\n        node.status = 'online'\n        node.last_seen = dt.datetime.utcnow()\n        node.save()\n\n\ndef run_server(config: str, environment: str = 'prod',\n               system_folders: bool = True):\n    ctx = ServerContext.from_external_config_file(\n        config,\n        environment,\n        system_folders\n    )\n    allow_drop_all = ctx.config[\"allow_drop_all\"]\n    Database().connect(uri=ctx.get_database_uri(),\n                       allow_drop_all=allow_drop_all)\n    return ServerApp(ctx).start()\n\n\ndef run_dev_server(server_app: ServerApp, *args, **kwargs):\n    log.warn('*'*80)\n    log.warn(' DEVELOPMENT SERVER '.center(80, '*'))\n    log.warn('*'*80)\n    kwargs.setdefault('log_output', False)\n    server_app.socketio.run(server_app.app, *args, **kwargs)\n", "import datetime\n\nfrom pathlib import Path\n\nfrom vantage6.common.globals import APPNAME\n\n#\n#   INSTALLATION SETTINGS\n#\nPACKAGE_FOLDER = Path(__file__).parent.parent.parent\n\nDATA_FOLDER = PACKAGE_FOLDER / APPNAME / \"server\" / \"_data\"\n\n#\n#   RUNTIME SETTINGS\n#\n\n# Expiretime of JWT tokens\nJWT_ACCESS_TOKEN_EXPIRES = datetime.timedelta(hours=6)\n\n# Expiretime of JWT token in a test environment\nJWT_TEST_ACCESS_TOKEN_EXPIRES = datetime.timedelta(days=1)\n\n# Which resources should be initialized. These names correspond to the\n# file-names in the resource directory\nRESOURCES = ['node', 'collaboration', 'organization', 'task', 'result',\n             'token', 'user', 'version', 'recover', 'role',\n             'rule', 'health', 'vpn', 'port', 'event']\n\n# Super user information. This user is only created if it is not in the\n# database yet at startup time.\nSUPER_USER_INFO = {\n    \"username\": \"root\",\n    \"password\": \"root\"\n}\n\n# Whenever the refresh tokens should expire. Note that setting this to true\n# would mean that nodes will disconnect after some time\nREFRESH_TOKENS_EXPIRE = False\n\n# default support email address\nDEFAULT_SUPPORT_EMAIL_ADDRESS = 'support@vantage6.ai'\n\n# default time that token is valid in minutes\nDEFAULT_EMAILED_TOKEN_VALIDITY_MINUTES = 60\n\n# maximum time given to nodes to respond to ping in seconds\nMAX_RESPONSE_TIME_PING = 60\n", "# -*- coding: utf-8 -*-\n\"\"\"\nResources below '/<api_base>/token'\n\"\"\"\nimport logging\nimport datetime as dt\nimport pyotp\n\nfrom typing import Union\nfrom flask import request, g, render_template\nfrom flask_jwt_extended import (\n    jwt_required,\n    create_access_token,\n    create_refresh_token,\n    get_jwt_identity\n)\nfrom http import HTTPStatus\n\nfrom vantage6 import server\nfrom vantage6.server import db\nfrom vantage6.server.model.user import User\nfrom vantage6.server.resource import (\n    with_node,\n    ServicesResources\n)\nfrom vantage6.server.resource.common.auth_helper import (\n  user_login, create_qr_uri\n)\n\nmodule_name = __name__.split('.')[-1]\nlog = logging.getLogger(module_name)\n\n\ndef setup(api, api_base, services):\n\n    path = \"/\".join([api_base, module_name])\n    log.info('Setting up \"{}\" and subdirectories'.format(path))\n\n    api.add_resource(\n        UserToken,\n        path+'/user',\n        endpoint='user_token',\n        methods=('POST',),\n        resource_class_kwargs=services\n    )\n\n    api.add_resource(\n        NodeToken,\n        path+'/node',\n        endpoint='node_token',\n        methods=('POST',),\n        resource_class_kwargs=services\n    )\n\n    api.add_resource(\n        ContainerToken,\n        path+'/container',\n        endpoint='container_token',\n        methods=('POST',),\n        resource_class_kwargs=services\n    )\n\n    api.add_resource(\n        RefreshToken,\n        path+'/refresh',\n        endpoint='refresh_token',\n        methods=('POST',),\n        resource_class_kwargs=services\n    )\n\n\n# ------------------------------------------------------------------------------\n# Resources / API's\n# ------------------------------------------------------------------------------\nclass UserToken(ServicesResources):\n    \"\"\"resource for api/token\"\"\"\n\n    def post(self):\n        \"\"\"Login user\n        ---\n        description: >-\n          Allow user to sign in by supplying a username and password. When MFA\n          is enabled on the server, a code is also required\n\n        requestBody:\n          content:\n            application/json:\n              schema:\n                properties:\n                  username:\n                    type: string\n                    description: Username of user that is logging in\n                  password:\n                    type: string\n                    description: User password\n                  mfa_code:\n                    type: string\n                    description: Two-factor authentication code. Only required\n                      if two-factor authentication is mandatory.\n\n        responses:\n          200:\n            description: Ok, authenticated\n          400:\n            description: Username/password combination unknown, or missing from\n              request body.\n          401:\n            description: Password and/or two-factor authentication token\n              incorrect.\n\n        tags: [\"Authentication\"]\n        \"\"\"\n        log.debug(\"Authenticate user using username and password\")\n\n        if not request.is_json:\n            log.warning('Authentication failed because no JSON body was '\n                        'provided!')\n            return {\"msg\": \"Missing JSON in request\"}, HTTPStatus.BAD_REQUEST\n\n        # Check JSON body\n        username = request.json.get('username', None)\n        password = request.json.get('password', None)\n        if not username and password:\n            msg = \"Username and/or password missing in JSON body\"\n            log.error(msg)\n            return {\"msg\": msg}, HTTPStatus.BAD_REQUEST\n\n        user, code = user_login(self.config.get(\"password_policy\", {}),\n                                username, password)\n        if code != HTTPStatus.OK:  # login failed\n            log.error(f\"Failed to login for user='{username}'\")\n            return user, code\n\n        is_mfa_enabled = self.config.get('two_factor_auth', False)\n        if is_mfa_enabled:\n            if user.otp_secret is None:\n                # server requires mfa but user hasn't set it up yet. Return\n                # an URI to generate a QR code\n                log.info(f'Redirecting user {username} to setup MFA')\n                return create_qr_uri(user), HTTPStatus.OK\n            else:\n                # 2nd authentication factor: check the OTP secret of the user\n                mfa_code = request.json.get('mfa_code')\n                if not mfa_code:\n                    # note: this is not treated as error, but simply guide\n                    # user to also fill in second factor\n                    return {\"msg\": \"Please include your two-factor \"\n                            \"authentication code\"}, HTTPStatus.OK\n                elif not self.validate_2fa_token(user, mfa_code):\n                    return {\n                        \"msg\": \"Your two-factor authentication code is \"\n                               \"incorrect!\"\n                    }, HTTPStatus.UNAUTHORIZED\n\n        token = create_access_token(user)\n\n        ret = {\n            'access_token': token,\n            'refresh_token': create_refresh_token(user),\n            'user_url': self.api.url_for(server.resource.user.User,\n                                         id=user.id),\n            'refresh_url': self.api.url_for(RefreshToken),\n        }\n\n        log.info(f\"Succesfull login from {username}\")\n        return ret, HTTPStatus.OK, {'jwt-token': token}\n\n    def user_login(self, username: str, password: str) -> Union[dict, db.User]:\n        \"\"\"Returns user a message in case of failed login attempt.\"\"\"\n        log.info(f\"Trying to login '{username}'\")\n        failed_login_msg = \"Failed to login\"\n        if db.User.username_exists(username):\n            user = db.User.get_by_username(username)\n            pw_policy = self.config.get('password_policy', {})\n            max_failed_attempts = pw_policy.get('max_failed_attempts', 5)\n            inactivation_time = pw_policy.get('inactivation_minutes', 15)\n\n            is_blocked, min_rem = user.is_blocked(max_failed_attempts,\n                                                  inactivation_time)\n            if is_blocked:\n                self.notify_user_blocked(user, max_failed_attempts, min_rem)\n                return {\"msg\": failed_login_msg}, HTTPStatus.UNAUTHORIZED\n            elif user.check_password(password):\n                user.failed_login_attempts = 0\n                user.save()\n                return user, HTTPStatus.OK\n            else:\n                # update the number of failed login attempts\n                user.failed_login_attempts = 1 \\\n                    if (\n                        not user.failed_login_attempts or\n                        user.failed_login_attempts >= max_failed_attempts\n                    ) else user.failed_login_attempts + 1\n                user.last_login_attempt = dt.datetime.now()\n                user.save()\n\n        return {\"msg\": failed_login_msg}, HTTPStatus.UNAUTHORIZED\n\n    def notify_user_blocked(self, user: db.User, max_n_attempts: int,\n                            min_rem: int) -> None:\n        \"\"\"Sends an email to the user when his or her account is locked\"\"\"\n        if not user.email:\n            log.warning(f'User {user.username} is locked, but does not have'\n                        'an email registered. So no message has been sent.')\n\n        log.info(f'User {user.username} is locked')\n\n        template_vars = {'firstname': user.firstname,\n                         'number_of_allowed_attempts': max_n_attempts,\n                         'ip': request.access_route[-1],\n                         'time': dt.datetime.now(dt.timezone.utc),\n                         'time_remaining': min_rem}\n\n        self.mail.send_email(\n            \"Your account has been temporary suspended\",\n            sender=\"support@vantage6.ai\",\n            recipients=[user.email],\n            text_body=render_template(\"mail/blocked_account.txt\",\n                                      **template_vars),\n            html_body=render_template(\"mail/blocked_account.html\",\n                                      **template_vars)\n        )\n\n    @staticmethod\n    def validate_2fa_token(user: User, mfa_code: Union[int, str]) -> bool:\n        \"\"\"\n        Check whether the 6-digit two-factor authentication code is valid\n\n        Parameters\n        ----------\n        user: User\n            The SQLAlchemy model of the user who is authenticating\n        mfa_code:\n            A six-digit TOTP code from an authenticator app\n\n        Returns\n        -------\n        bool\n          Whether six-digit code is valid or not\n        \"\"\"\n        # the option `valid_window=1` means the code from 1 time window (30s)\n        # ago, is also valid. This prevents that users around the edge of\n        # the time window can still login successfully if server is a bit slow\n        return pyotp.TOTP(user.otp_secret).verify(str(mfa_code),\n                                                  valid_window=1)\n\n\nclass NodeToken(ServicesResources):\n\n    def post(self):\n        \"\"\"Login node\n        ---\n        description: >-\n          Allows node to sign in using a unique API key. If the login is\n          successful this returns a dictionary with access and refresh tokens\n          for the node as well as a node_url and a refresh_url.\n\n        requestBody:\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Node'\n\n        responses:\n          200:\n            description: Ok, authenticated\n          400:\n            description: No API key provided in request body.\n          401:\n            description: Invalid API key\n\n        tags: [\"Authentication\"]\n        \"\"\"\n        log.debug(\"Authenticate Node using api key\")\n\n        if not request.is_json:\n            log.warning('Authentication failed because no JSON body was '\n                        'provided!')\n            return {\"msg\": \"Missing JSON in request\"}, HTTPStatus.BAD_REQUEST\n\n        # Check JSON body\n        api_key = request.json.get('api_key', None)\n        if not api_key:\n            msg = \"api_key missing in JSON body\"\n            log.error(msg)\n            return {\"msg\": msg}, HTTPStatus.BAD_REQUEST\n\n        node = db.Node.get_by_api_key(api_key)\n\n        if not node:  # login failed\n            log.error(\"Api key is not recognized\")\n            return {\"msg\": \"Api key is not recognized!\"}, \\\n                HTTPStatus.UNAUTHORIZED\n\n        token = create_access_token(node)\n        ret = {\n            'access_token': token,\n            'refresh_token': create_refresh_token(node),\n            'node_url': self.api.url_for(server.resource.node.Node,\n                                         id=node.id),\n            'refresh_url': self.api.url_for(RefreshToken),\n        }\n\n        log.info(f\"Succesfull login as node '{node.id}' ({node.name})\")\n        return ret, HTTPStatus.OK, {'jwt-token': token}\n\n\nclass ContainerToken(ServicesResources):\n\n    @with_node\n    def post(self):\n        \"\"\"Algorithm container login\n        ---\n        description: >-\n          Generate token for the algorithm container of a specific task.\\n\n\n          Not available to users; only for authenticated nodes.\n\n        requestBody:\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ContainerToken'\n\n        responses:\n          200:\n            description: Container token generated\n          400:\n            description: Task does not exist or is already completed\n          401:\n            description: Key request for invalid image or task\n\n        tags: [\"Authentication\"]\n        \"\"\"\n        log.debug(\"Creating a token for a container running on a node\")\n\n        data = request.get_json()\n\n        task_id = data.get(\"task_id\")\n        claim_image = data.get(\"image\")\n\n        db_task = db.Task.get(task_id)\n        if not db_task:\n            log.warning(f\"Node {g.node.id} attempts to generate key for task \"\n                        f\"{task_id} that does not exist\")\n            return {\"msg\": \"Master task does not exist!\"}, \\\n                HTTPStatus.BAD_REQUEST\n\n        # verify that task the token is requested for exists\n        if claim_image != db_task.image:\n            log.warning(\n                f\"Node {g.node.id} attempts to generate key for image \"\n                f\"{claim_image} that does not belong to task {task_id}.\"\n            )\n            return {\"msg\": \"Image and task do no match\"}, \\\n                HTTPStatus.UNAUTHORIZED\n\n        # check if the node is in the collaboration to which the task is\n        # enlisted\n        if g.node.collaboration_id != db_task.collaboration_id:\n            log.warning(\n                f\"Node {g.node.id} attempts to generate key for task {task_id}\"\n                f\" which is outside its collaboration \"\n                f\"({g.node.collaboration_id}/{db_task.collaboration_id}).\"\n            )\n            return {\"msg\": \"You are not within the collaboration\"}, \\\n                HTTPStatus.UNAUTHORIZED\n\n        # validate that the task not has been finished yet\n        if db_task.complete:\n            log.warning(f\"Node {g.node.id} attempts to generate a key for \"\n                        f\"completed task {task_id}\")\n            return {\"msg\": \"Task is already finished!\"}, HTTPStatus.BAD_REQUEST\n\n        # container identity consists of its node_id,\n        # task_id, collaboration_id and image_id\n        container = {\n            \"client_type\": \"container\",\n            \"node_id\": g.node.id,\n            \"organization_id\": g.node.organization_id,\n            \"collaboration_id\": g.node.collaboration_id,\n            \"task_id\": task_id,\n            \"image\": claim_image,\n            \"database\": db_task.database\n        }\n        token = create_access_token(container, expires_delta=False)\n\n        return {'container_token': token}, HTTPStatus.OK\n\n\nclass RefreshToken(ServicesResources):\n\n    @jwt_required(refresh=True)\n    def post(self):\n        \"\"\"Refresh token\n        ---\n        description: >-\n          Refresh access token if the previous one is expired.\\n\n\n          Your refresh token must be present in the request headers to use\n          this endpoint.\n\n        responses:\n          200:\n            description: Token refreshed\n\n        security:\n          - bearerAuth: []\n\n        tags: [\"Authentication\"]\n        \"\"\"\n        user_or_node_id = get_jwt_identity()\n        log.info(f'Refreshing token for user or node \"{user_or_node_id}\"')\n        user_or_node = db.Authenticatable.get(user_or_node_id)\n        ret = {'access_token': create_access_token(user_or_node)}\n\n        return ret, HTTPStatus.OK\n"], "fixing_code": ["application: {}\n  # you may also add your configuration here and leave environments empty\n\nenvironments:\n  # name of the environment (should be 'test', 'prod', 'acc' or 'dev')\n  test:\n\n    # Human readable description of the server instance. This is to help\n    # your peers to identify the server\n    description: Test\n\n    # Should be prod, acc, test or dev. In case the type is set to test\n    # the JWT-tokens expiration is set to 1 day (default is 6 hours). The\n    # other types can be used in future releases of vantage6\n    type: test\n\n    # IP adress to which the server binds. In case you specify 0.0.0.0\n    # the server listens on all interfaces\n    ip: 0.0.0.0\n\n    # Port to which the server binds\n    port: 5000\n\n    # API path prefix. (i.e. https://yourdomain.org/api_path/<endpoint>). In the\n    # case you use a referse proxy and use a subpath, make sure to include it\n    # here also.\n    api_path: /api\n\n    # The URI to the server database. This should be a valid SQLAlchemy URI,\n    # e.g. for an Sqlite database: sqlite:///database-name.sqlite,\n    # or Postgres: postgresql://username:password@172.17.0.1/database).\n    uri: sqlite:///test.sqlite\n\n    # This should be set to false in production as this allows to completely\n    # wipe the database in a single command. Useful to set to true when\n    # testing/developing.\n    allow_drop_all: True\n\n    # The secret key used to generate JWT authorization tokens. This should\n    # be kept secret as others are able to generate access tokens if they\n    # know this secret. This parameter is optional. In case it is not\n    # provided in the configuration it is generated each time the server\n    # starts. Thereby invalidating all previous distributed keys.\n    # OPTIONAL\n    jwt_secret_key: super-secret-key! # recommended but optional\n\n    # Settings for the logger\n    logging:\n\n      # Controls the logging output level. Could be one of the following\n      # levels: CRITICAL, ERROR, WARNING, INFO, DEBUG, NOTSET\n      level:        DEBUG\n\n      # Filename of the log-file, used by RotatingFileHandler\n      file:         test.log\n\n      # Whether the output is shown in the console or not\n      use_console:  True\n\n      # The number of log files that are kept, used by RotatingFileHandler\n      backup_count: 5\n\n      # Size in kB of a single log file, used by RotatingFileHandler\n      max_size:     1024\n\n      # format: input for logging.Formatter,\n      format:       \"%(asctime)s - %(name)-14s - %(levelname)-8s - %(message)s\"\n      datefmt:      \"%Y-%m-%d %H:%M:%S\"\n\n    # Configure a smtp mail server for the server to use for administrative\n    # purposes. e.g. allowing users to reset their password.\n    # OPTIONAL\n    smtp:\n      port: 587\n      server: smtp.yourmailserver.com\n      username: your-username\n      password: super-secret-password\n\n    # Set an email address you want to direct your users to for support\n    # (defaults to the address you set above in the SMTP server or otherwise\n    # to support@vantage6.ai)\n    support_email: your-support@email.com\n\n    # set how long reset token provided via email are valid (default 1 hour)\n    email_token_validity_minutes: 60\n\n    # set how long tokens and refresh tokens are valid (default 6 and 48\n    # hours, respectively)\n    token_expires_hours: 6\n    refresh_token_expires_hours: 48\n\n    # If algorithm containers need direct communication between each other\n    # the server also requires a VPN server. (!) This must be a EduVPN\n    # instance as vantage6 makes use of their API (!)\n    # OPTIONAL\n    vpn_server:\n        # the URL of your VPN server\n        url: https://your-vpn-server.ext\n\n        # OATH2 settings, make sure these are the same as in the\n        # configuration file of your EduVPN instance\n        redirect_url: http://localhost\n        client_id: your_VPN_client_user_name\n        client_secret: your_VPN_client_user_password\n\n        # Username and password to acccess the EduVPN portal\n        portal_username: your_eduvpn_portal_user_name\n        portal_userpass: your_eduvpn_portal_user_password\n\n  prod: {}\n  acc: {}\n  dev: {}", "\"\"\"\nvantage6 clients\n\nThis module is contains a base client. From this base client the container\nclient (client used by master algorithms) and the user client are derived.\n\"\"\"\nimport logging\nimport pickle\nimport time\nimport typing\nimport jwt\nimport requests\nimport pyfiglet\nimport json as json_lib\nimport itertools\nimport sys\nimport traceback\n\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Union\n\nfrom vantage6.common.exceptions import AuthenticationException\nfrom vantage6.common import bytes_to_base64s, base64s_to_bytes\nfrom vantage6.common.globals import APPNAME\nfrom vantage6.common.encryption import RSACryptor, DummyCryptor\nfrom vantage6.common import WhoAmI\nfrom vantage6.client import serialization, deserialization\nfrom vantage6.client.filter import post_filtering\nfrom vantage6.client.utils import print_qr_code, LogLevel\n\n\nmodule_name = __name__.split('.')[1]\n\nLEGACY = 'legacy'\n\n\nclass ServerInfo(typing.NamedTuple):\n    \"\"\"Data-class to store the server info.\"\"\"\n    host: str\n    port: int\n    path: str\n\n\nclass ClientBase(object):\n    \"\"\"Common interface to the central server.\n\n    Contains the basis for all other clients. This includes a basic interface\n    to authenticate, generic request, creating tasks and result retrieval.\n    \"\"\"\n\n    def __init__(self, host: str, port: int, path: str = '/api'):\n        \"\"\"Basic setup for the client\n\n        Parameters\n        ----------\n        host : str\n            Adress (including protocol, e.g. `https://`) of the vantage6 server\n        port : int\n            port numer to which the server listens\n        path : str, optional\n            path of the api, by default '/api'\n        \"\"\"\n\n        self.log = logging.getLogger(module_name)\n\n        # server settings\n        self.__host = host\n        self.__port = port\n        self.__api_path = path\n\n        # tokens\n        self._access_token = None\n        self.__refresh_token = None\n        self.__refresh_url = None\n\n        self.cryptor = None\n        self.whoami = None\n\n    @property\n    def name(self) -> str:\n        \"\"\"Return the node's/client's name\"\"\"\n        return self.whoami.name\n\n    @property\n    def headers(self) -> dict:\n        \"\"\"Headers that are send with each request\"\"\"\n        if self._access_token:\n            return {'Authorization': 'Bearer ' + self._access_token}\n        else:\n            return {}\n\n    @property\n    def token(self) -> str:\n        \"\"\"JWT Authorization token\"\"\"\n        return self._access_token\n\n    @property\n    def host(self) -> str:\n        \"\"\"Host including protocol (HTTP/HTTPS)\"\"\"\n        return self.__host\n\n    @property\n    def port(self) -> int:\n        \"\"\"Port to vantage6-server listens\"\"\"\n        return self.__port\n\n    @property\n    def path(self) -> str:\n        \"\"\"Path/endpoint at the server where the api resides\"\"\"\n        return self.__api_path\n\n    @property\n    def base_path(self) -> str:\n        \"\"\"Combination of host, port and api-path\"\"\"\n        if self.__port:\n            return f\"{self.host}:{self.port}{self.__api_path}\"\n\n        return f\"{self.host}{self.__api_path}\"\n\n    def generate_path_to(self, endpoint: str) -> str:\n        \"\"\"Generate URL to endpoint using host, port and endpoint\n\n        Parameters\n        ----------\n        endpoint : str\n            endpoint to which a fullpath needs to be generated\n\n        Returns\n        -------\n        str\n            URL to the endpoint\n        \"\"\"\n        if endpoint.startswith('/'):\n            path = self.base_path + endpoint\n        else:\n            path = self.base_path + '/' + endpoint\n\n        return path\n\n    def request(self, endpoint: str, json: dict = None, method: str = 'get',\n                params: dict = None, first_try: bool = True,\n                retry: bool = True) -> dict:\n        \"\"\"Create http(s) request to the vantage6 server\n\n        Parameters\n        ----------\n        endpoint : str\n            Endpoint of the server\n        json : dict, optional\n            payload, by default None\n        method : str, optional\n            Http verb, by default 'get'\n        params : dict, optional\n            URL parameters, by default None\n        first_try : bool, optional\n            Whether this is the first attempt of this request. Default True.\n        retry: bool, optional\n            Try request again after refreshing the token. Default True.\n\n        Returns\n        -------\n        dict\n            Response of the server\n        \"\"\"\n\n        # get appropiate method\n        rest_method = {\n            'get': requests.get,\n            'post': requests.post,\n            'put': requests.put,\n            'patch': requests.patch,\n            'delete': requests.delete\n        }.get(method.lower(), requests.get)\n\n        # send request to server\n        url = self.generate_path_to(endpoint)\n        self.log.debug(f'Making request: {method.upper()} | {url} | {params}')\n\n        try:\n            response = rest_method(url, json=json, headers=self.headers,\n                                   params=params)\n        except requests.exceptions.ConnectionError as e:\n            # we can safely retry as this is a connection error. And we\n            # keep trying!\n            self.log.error('Connection error... Retrying')\n            self.log.debug(e)\n            time.sleep(1)\n            return self.request(endpoint, json, method, params)\n\n        # TODO: should check for a non 2xx response\n        if response.status_code > 210:\n            self.log.error(\n                f'Server responded with error code: {response.status_code}')\n            try:\n                self.log.error(\"msg:\"+response.json().get(\"msg\", \"\"))\n            except json_lib.JSONDecodeError:\n                self.log.error('Did not find a message from the server')\n                self.log.debug(response.content)\n\n            if retry:\n                if first_try:\n                    self.refresh_token()\n                    return self.request(endpoint, json, method, params,\n                                        first_try=False)\n                else:\n                    self.log.error(\"Nope, refreshing the token didn't fix it.\")\n\n        return response.json()\n\n    def setup_encryption(self, private_key_file: str) -> None:\n        \"\"\"Enable the encryption module fot the communication\n\n        This will attach a Crypter object to the client. It will also\n        verify that the public key at the server matches the local\n        private key. In case they differ, the local public key is uploaded\n        to the server.\n\n        Parameters\n        ----------\n        private_key_file : str\n            File path of the private key file\n\n        \"\"\"\n        assert self._access_token, \\\n            \"Encryption can only be setup after authentication\"\n        assert self.whoami.organization_id, \\\n            \"Organization unknown... Did you authenticate?\"\n\n        if private_key_file is None:\n            self.cryptor = DummyCryptor()\n            return\n\n        if isinstance(private_key_file, str):\n            private_key_file = Path(private_key_file)\n\n        cryptor = RSACryptor(private_key_file)\n\n        # check if the public-key is the same on the server. If this is\n        # not the case, this node will not be able to read any messages\n        # that are send to him! If this is the case, the new public_key\n        # will be uploaded to the central server\n        organization = self.request(\n            f\"organization/{self.whoami.organization_id}\")\n        pub_key = organization.get(\"public_key\")\n        upload_pub_key = False\n\n        if pub_key:\n            if cryptor.verify_public_key(pub_key):\n                self.log.info(\"Public key matches the server key! Good to go!\")\n\n            else:\n                self.log.critical(\n                    \"Local public key does not match server public key. \"\n                    \"You will not able to read any messages that are intended \"\n                    \"for you!\"\n                )\n                upload_pub_key = True\n        else:\n            upload_pub_key = True\n\n        # upload public key if required\n        if upload_pub_key:\n            self.request(\n                f\"organization/{self.whoami.organization_id}\",\n                method=\"patch\",\n                json={\"public_key\": cryptor.public_key_str}\n            )\n            self.log.info(\"The public key on the server is updated!\")\n\n        self.cryptor = cryptor\n\n    def authenticate(self, credentials: dict,\n                     path: str = \"token/user\") -> bool:\n        \"\"\"Authenticate to the vantage6-server\n\n        It allows users, nodes and containers to sign in. Credentials can\n        either be a username/password combination or a JWT authorization\n        token.\n\n        Parameters\n        ----------\n        credentials : dict\n            Credentials used to authenticate\n        path : str, optional\n            Endpoint used for authentication. This differs for users, nodes and\n            containers, by default \"token/user\"\n\n        Raises\n        ------\n        Exception\n            Failed to authenticate\n\n        Returns\n        -------\n        Bool\n            Whether or not user is authenticated. Alternative is that user is\n            redirected to set up two-factor authentication\n        \"\"\"\n        if 'username' in credentials:\n            self.log.debug(\n                f\"Authenticating user {credentials['username']}...\")\n        elif 'api_key' in credentials:\n            self.log.debug('Authenticating node...')\n\n        # authenticate to the central server\n        url = self.generate_path_to(path)\n        response = requests.post(url, json=credentials)\n        data = response.json()\n\n        # handle negative responses\n        if response.status_code > 200:\n            self.log.critical(f\"Failed to authenticate: {data.get('msg')}\")\n            if response.status_code == 401:\n                raise AuthenticationException(\"Failed to authenticate\")\n            else:\n                raise Exception(\"Failed to authenticate\")\n\n        if 'qr_uri' in data:\n            print_qr_code(data)\n            return False\n        else:\n            # Check if there is an access token. If not, there is a problem\n            # with authenticating\n            if 'access_token' not in data:\n                if 'msg' in data:\n                    raise Exception(data['msg'])\n                else:\n                    raise Exception(\n                        \"No access token in authentication response!\")\n\n            # store tokens in object\n            self.log.info(\"Successfully authenticated\")\n            self._access_token = data.get(\"access_token\")\n            self.__refresh_token = data.get(\"refresh_token\")\n            self.__refresh_url = data.get(\"refresh_url\")\n            return True\n\n    def refresh_token(self) -> None:\n        \"\"\"Refresh an expired token using the refresh token\n\n        Raises\n        ------\n        Exception\n            Authentication Error!\n        \"\"\"\n        self.log.info(\"Refreshing token\")\n        assert self.__refresh_url, \\\n            \"Refresh URL not found, did you authenticate?\"\n\n        # if no port is specified explicit, then it should be omnit the\n        # colon : in the path. Similar (but different) to the property\n        # base_path\n        if self.__port:\n            url = f\"{self.__host}:{self.__port}{self.__refresh_url}\"\n        else:\n            url = f\"{self.__host}{self.__refresh_url}\"\n\n        # send request to server\n        response = requests.post(url, headers={\n            'Authorization': 'Bearer ' + self.__refresh_token\n        })\n\n        # server says no!\n        if response.status_code != 200:\n            self.log.critical(\"Could not refresh token\")\n            raise Exception(\"Authentication Error!\")\n\n        self._access_token = response.json()[\"access_token\"]\n        self.__refresh_token = response.json()[\"refresh_token\"]\n\n    # TODO BvB 23-01-23 remove this method in v4+. It is only here for\n    # backwards compatibility\n    def post_task(self, name: str, image: str, collaboration_id: int,\n                  input_='', description='',\n                  organization_ids: list = None,\n                  data_format=LEGACY, database: str = 'default') -> dict:\n        \"\"\"Post a new task at the server\n\n        It will also encrypt `input_` for each receiving organization.\n\n        Parameters\n        ----------\n        name : str\n            Human readable name for the task\n        image : str\n            Docker image name containing the algorithm\n        collaboration_id : int\n            Collaboration `id` of the collaboration for which the task is\n            intended\n        input_ : str, optional\n            Task input, by default ''\n        description : str, optional\n            Human readable description of the task, by default ''\n        organization_ids : list, optional\n            Ids of organizations (within the collaboration) that need to\n            execute this task, by default None\n        data_format : str, optional\n            Type of data format to use to send and receive\n            data. possible values: 'json', 'pickle', 'legacy'. 'legacy'\n            will use pickle serialization. Default is 'legacy'., by default\n            LEGACY\n\n        Returns\n        -------\n        dict\n            Containing the task meta-data\n        \"\"\"\n        assert self.cryptor, \"Encryption has not yet been setup!\"\n\n        if organization_ids is None:\n            organization_ids = []\n\n        if data_format == LEGACY:\n            serialized_input = pickle.dumps(input_)\n        else:\n            # Data will be serialized to bytes in the specified data format.\n            # It will be prepended with 'DATA_FORMAT.' in unicode.\n            serialized_input = data_format.encode() + b'.' \\\n                + serialization.serialize(input_, data_format)\n\n        organization_json_list = []\n        for org_id in organization_ids:\n            pub_key = self.request(f\"organization/{org_id}\").get(\"public_key\")\n            # pub_key = base64s_to_bytes(pub_key)\n            # self.log.debug(pub_key)\n\n            organization_json_list.append({\n                \"id\": org_id,\n                \"input\": self.cryptor.encrypt_bytes_to_str(serialized_input,\n                                                           pub_key)\n            })\n\n        return self.request('task', method='post', json={\n            \"name\": name,\n            \"image\": image,\n            \"collaboration_id\": collaboration_id,\n            \"description\": description,\n            \"organizations\": organization_json_list,\n            'database': database\n        })\n\n    # TODO BvB 23-01-23 remove this method in v4+ (or make it private?). It is\n    # only here for backwards compatibility.\n    def get_results(self, id: int = None, state: str = None,\n                    include_task: bool = False, task_id: int = None,\n                    node_id: int = None, params: dict = {}) -> dict:\n        \"\"\"Get task result(s) from the central server\n\n        Depending if a `id` is specified or not, either a single or a\n        list of results is returned. The input and result field of the\n        result are attempted te be decrypted. This fails if the public\n        key at the server is not derived from the currently private key\n        or when the result is not from your organization.\n\n        Parameters\n        ----------\n        id : int, optional\n            Id of the result, by default None\n        state : str, optional\n            The state of the task (e.g. `open`), by default None\n        include_task : bool, optional\n            Whenever to include the orginating task, by default False\n        task_id : int, optional\n            The id of the originating task, this will return all results\n            belonging to this task, by default None\n        node_id : int, optional\n            The id of the node at which this result has been produced,\n            this will return all results from this node, by default None\n\n        Returns\n        -------\n        dict\n            Containing the result(s)\n        \"\"\"\n        # Determine endpoint and create dict with query parameters\n        endpoint = 'result' if not id else f'result/{id}'\n\n        if state:\n            params['state'] = state\n        if include_task:\n            params['include'] = 'task'\n        if task_id:\n            params['task_id'] = task_id\n        if node_id:\n            params['node_id'] = node_id\n\n        # self.log.debug(f\"Retrieving results using query parameters:{params}\")\n        results = self.request(endpoint=endpoint, params=params)\n\n        if isinstance(results, str):\n            self.log.warn(\"Requesting results failed\")\n            self.log.debug(f\"Results message: {results}\")\n            return {}\n\n        # hack: in the case that the pagination metadata is included we\n        # need to strip that for decrypting\n        if isinstance(results, dict) and 'data' in results:\n            wrapper = results\n            results = results['data']\n\n        if id:\n            # Single result\n            self._decrypt_result(results)\n\n        else:\n            # Multiple results\n            for result in results:\n                self._decrypt_result(result)\n\n        if 'wrapper' in locals():\n            wrapper['data'] = results\n            results = wrapper\n\n        return results\n\n    def _decrypt_result(self, result):\n        \"\"\"Helper to decrypt the keys 'input' and 'result' in dict.\n\n        Keys are replaced, but object reference remains intact: changes are\n        made *in-place*.\n        \"\"\"\n        assert self.cryptor, \"Encryption has not been initialized\"\n        cryptor = self.cryptor\n        try:\n            self.log.info('Decrypting input')\n            # TODO this only works when the results belong to the\n            # same organization... We should make different implementation\n            # of get_results\n            result[\"input\"] = cryptor.decrypt_str_to_bytes(result[\"input\"])\n\n        except Exception as e:\n            self.log.debug(e)\n\n        try:\n            if result[\"result\"]:\n                self.log.info('Decrypting result')\n                result[\"result\"] = \\\n                    cryptor.decrypt_str_to_bytes(result[\"result\"])\n\n        except ValueError as e:\n            self.log.error(\"Could not decrypt/decode input or result.\")\n            self.log.error(e)\n            # raise\n\n    class SubClient:\n        \"\"\"Create sub groups of commands using this SubClient\"\"\"\n        def __init__(self, parent):\n            self.parent: UserClient = parent\n\n\nclass UserClient(ClientBase):\n    \"\"\"User interface to the vantage6-server\"\"\"\n\n    def __init__(self, *args, verbose=False, log_level='debug', **kwargs):\n        \"\"\"Create user client\n\n        All paramters from `ClientBase` can be used here.\n\n        Parameters\n        ----------\n        verbose : bool, optional\n            Whenever to print (info) messages, by default False\n        \"\"\"\n        super(UserClient, self).__init__(*args, **kwargs)\n\n        # Replace logger by print logger\n        # TODO in v4+, remove the verbose option and only keep log_level\n        self.log = self.get_logger(verbose, log_level)\n\n        # attach sub-clients\n        self.util = self.Util(self)\n        self.collaboration = self.Collaboration(self)\n        self.organization = self.Organization(self)\n        self.user = self.User(self)\n        self.result = self.Result(self)\n        self.task = self.Task(self)\n        self.role = self.Role(self)\n        self.node = self.Node(self)\n        self.rule = self.Rule(self)\n\n        # Display welcome message\n        self.log.info(\" Welcome to\")\n        for line in pyfiglet.figlet_format(APPNAME, font='big').split('\\n'):\n            self.log.info(line)\n        self.log.info(\" --> Join us on Discord! https://discord.gg/rwRvwyK\")\n        self.log.info(\" --> Docs: https://docs.vantage6.ai\")\n        self.log.info(\" --> Blog: https://vantage6.ai\")\n        self.log.info(\"-\" * 60)\n        self.log.info(\"Cite us!\")\n        self.log.info(\"If you publish your findings obtained using vantage6, \")\n        self.log.info(\"please cite the proper sources as mentioned in:\")\n        self.log.info(\"https://vantage6.ai/vantage6/references\")\n        self.log.info(\"-\" * 60)\n\n    @staticmethod\n    def get_logger(enabled: bool, level: str) -> logging.Logger:\n        \"\"\"\n        Create print-logger\n\n        Parameters\n        ----------\n        enabled: bool\n            If true, logging at most detailed level\n        level: str\n            Desired logging level\n\n        Returns\n        -------\n        logging.Logger\n            Logger object\n        \"\"\"\n        # get logger that prints to console\n        logger = logging.getLogger()\n        logger.handlers.clear()\n        logger.addHandler(logging.StreamHandler(sys.stdout))\n\n        # set log level\n        level = level.upper()\n        if enabled:\n            logger.setLevel(LogLevel.DEBUG.value)\n        elif level not in [lvl.value for lvl in LogLevel]:\n            default_lvl = LogLevel.DEBUG.value\n            logger.setLevel(default_lvl)\n            logger.warn(\n                f\"You set unknown log level {level}. Available levels are: \"\n                f\"{', '.join([lvl.value for lvl in LogLevel])}. \")\n            logger.warn(f\"Log level now set to {default_lvl}.\")\n        else:\n            logger.setLevel(level)\n        return logger\n\n    def authenticate(self, username: str, password: str,\n                     mfa_code: Union[int, str] = None) -> None:\n        \"\"\"Authenticate as a user\n\n        It also collects some additional info about your user.\n\n        Parameters\n        ----------\n        username : str\n            Username used to authenticate\n        password : str\n            Password used to authenticate\n        mfa_token: str or int\n            Six-digit two-factor authentication code\n        \"\"\"\n        auth_json = {\n            \"username\": username,\n            \"password\": password,\n        }\n        if mfa_code:\n            auth_json[\"mfa_code\"] = mfa_code\n        auth = super(UserClient, self).authenticate(auth_json,\n                                                    path=\"token/user\")\n        if not auth:\n            # user is not authenticated. The super function is responsible for\n            # printing useful output\n            return\n\n        # identify the user and the organization to which this user\n        # belongs. This is usefull for some client side checks\n        try:\n            type_ = \"user\"\n            jwt_payload = jwt.decode(self.token,\n                                     options={\"verify_signature\": False})\n\n            # FIXME: 'identity' is no longer needed in version 4+. So this if\n            # statement can be removed\n            if 'sub' in jwt_payload:\n                id_ = jwt_payload['sub']\n            elif 'identity' in jwt_payload:\n                id_ = jwt_payload['identity']\n\n            user = self.request(f\"user/{id_}\")\n            name = user.get(\"firstname\")\n            organization_id = user.get(\"organization\").get(\"id\")\n            organization = self.request(f\"organization/{organization_id}\")\n            organization_name = organization.get(\"name\")\n\n            self.whoami = WhoAmI(\n                type_=type_,\n                id_=id_,\n                name=name,\n                organization_id=organization_id,\n                organization_name=organization_name\n            )\n\n            self.log.info(\" --> Succesfully authenticated\")\n            self.log.info(f\" --> Name: {name} (id={id_})\")\n            self.log.info(f\" --> Organization: {organization_name} \"\n                          f\"(id={organization_id})\")\n        except Exception:\n            self.log.info('--> Retrieving additional user info failed!')\n            self.log.error(traceback.format_exc())\n\n    def wait_for_results(self, task_id: int, sleep: float = 1) -> Dict:\n        \"\"\"\n        Polls the server to check when results are ready, and returns the\n        results when the task is completed.\n\n        Parameters\n        ----------\n        task_id: int\n            ID of the task that you are waiting for\n        sleep: float\n            Interval in seconds between checks if task is finished. Default 1.\n\n        Returns\n        -------\n        Dict\n            A dictionary with the results of the task, after it has completed.\n        \"\"\"\n        # Disable logging (additional logging would prevent the 'wait' message\n        # from being printed on a single line)\n        if isinstance(self.log, logging.Logger):\n            prev_level = self.log.level\n            self.log.setLevel(logging.WARN)\n        elif isinstance(self.log, UserClient.Log):\n            prev_level = self.log.enabled\n            self.log.enabled = False\n\n        animation = itertools.cycle(['|', '/', '-', '\\\\'])\n        t = time.time()\n\n        while not self.task.get(task_id)['complete']:\n            frame = next(animation)\n            sys.stdout.write(\n                f'\\r{frame} Waiting for task {task_id} ({int(time.time()-t)}s)'\n            )\n            sys.stdout.flush()\n            time.sleep(sleep)\n        sys.stdout.write('\\rDone!                  ')\n\n        # Re-enable logging\n        if isinstance(self.log, logging.Logger):\n            self.log.setLevel(prev_level)\n        elif isinstance(self.log, UserClient.Log):\n            self.log.enabled = prev_level\n\n        return self.get_results(task_id=task_id)\n\n    class Util(ClientBase.SubClient):\n        \"\"\"Collection of general utilities\"\"\"\n\n        def get_server_version(self) -> dict:\n            r\"\"\"View the version number of the vantage6-server\n\n            Returns\n            -------\n            dict\n                A dict containing the version number\n            \"\"\"\n            return self.parent.request('version')\n\n        def get_server_health(self) -> dict:\n            \"\"\"View the health of the vantage6-server\n\n            Returns\n            -------\n            dict\n                Containing the server health information\n            \"\"\"\n            return self.parent.request('health')\n\n        def change_my_password(self, current_password: str,\n                               new_password: str) -> dict:\n            \"\"\"Change your own password by providing your current password\n\n            Parameters\n            ----------\n            current_password : str\n                Your current password\n            new_password : str\n                Your new password\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            result = self.parent.request(\n                'password/change', method='patch', json={\n                    'current_password': current_password,\n                    'new_password': new_password\n                }\n            )\n            msg = result.get('msg')\n            self.parent.log.info(f'--> {msg}')\n            return result\n\n        def reset_my_password(self, email: str = None,\n                              username: str = None) -> dict:\n            \"\"\"Start reset password procedure\n\n            Either a username of email needs to be provided.\n\n            Parameters\n            ----------\n            email : str, optional\n                Email address of your account, by default None\n            username : str, optional\n                Username of your account, by default None\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            assert email or username, \"You need to provide username or email!\"\n            result = self.parent.request('recover/lost', method='post', json={\n                'username': username,\n                'email': email\n            })\n            msg = result.get('msg')\n            self.parent.log.info(f'--> {msg}')\n            return result\n\n        def set_my_password(self, token: str, password: str) -> dict:\n            \"\"\"Set a new password using a recovery token\n\n            Token can be obtained through `.reset_password(...)`\n\n            Parameters\n            ----------\n            token : str\n                Token obtained from `reset_password`\n            password : str\n                New password\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            result = self.parent.request('recover/reset', method='post', json={\n                'reset_token': token,\n                'password': password\n            })\n            msg = result.get('msg')\n            self.parent.log.info(f'--> {msg}')\n            return result\n\n        def reset_two_factor_auth(\n            self, password: str, email: str = None, username: str = None\n        ) -> dict:\n            \"\"\"Start reset procedure for two-factor authentication\n\n            The password and either username of email must be provided.\n\n            Parameters\n            ----------\n            password: str\n                Password of your account\n            email : str, optional\n                Email address of your account, by default None\n            username : str, optional\n                Username of your account, by default None\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            assert email or username, \"You need to provide username or email!\"\n            result = self.parent.request(\n                'recover/2fa/lost', method='post', json={\n                    'username': username,\n                    'email': email,\n                    \"password\": password\n                }, retry=False)\n            msg = result.get('msg')\n            self.parent.log.info(f'--> {msg}')\n            return result\n\n        def set_two_factor_auth(self, token: str) -> dict:\n            \"\"\"\n            Setup two-factor authentication using a recovery token after you\n            have lost access.\n\n            Token can be obtained through `.reset_two_factor_auth(...)`\n\n            Parameters\n            ----------\n            token : str\n                Token obtained from `reset_two_factor_auth`\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            result = self.parent.request(\n                'recover/2fa/reset', method='post', json={\n                    'reset_token': token,\n                }, retry=False)\n            if 'qr_uri' in result:\n                print_qr_code(result)\n            else:\n                msg = result.get('msg')\n                self.parent.log.info(f'--> {msg}')\n            return result\n\n        def generate_private_key(self, file_: str = None) -> None:\n            \"\"\"Generate new private key\n\n            ....\n\n            Parameters\n            ----------\n            file_ : str, optional\n                Path where to store the private key, by default None\n            \"\"\"\n\n            if not file_:\n                self.parent.log.info('--> Using current directory')\n                file_ = \"private_key.pem\"\n\n            if isinstance(file_, str):\n                file_ = Path(file_).absolute()\n\n            self.parent.log.info(f'--> Generating private key file: {file_}')\n            private_key = RSACryptor.create_new_rsa_key(file_)\n\n            self.parent.log.info('--> Assigning private key to client')\n            self.parent.cryptor.private_key = private_key\n\n            self.parent.log.info('--> Encrypting the client and uploading '\n                                 'the public key')\n            self.parent.setup_encryption(file_)\n\n    class Collaboration(ClientBase.SubClient):\n        \"\"\"Collection of collaboration requests\"\"\"\n\n        @post_filtering()\n        def list(self, scope: str = 'organization',\n                 name: str = None, encrypted: bool = None,\n                 organization: int = None, page: int = 1,\n                 per_page: int = 20, include_metadata: bool = True,\n                 ) -> dict:\n            \"\"\"View your collaborations\n\n            Parameters\n            ----------\n            scope : str, optional\n                Scope of the list, accepted values are `organization` and\n                `global`. In case of `organization` you get the collaborations\n                in which your organization participates. If you specify global\n                you get the collaborations which you are allowed to see.\n            name: str, optional (with LIKE operator)\n                Filter collaborations by name\n            organization: int, optional\n                Filter collaborations by organization id\n            encrypted: bool, optional\n                Filter collaborations by whether or not they are encrypted\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            list of dicts\n                Containing collabotation information\n\n            Notes\n            -----\n            - pagination does not work in combination with scope\n              `organization` as pagination is missing at endpoint\n              /organization/<id>/collaboration\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'name': name, 'encrypted': encrypted,\n                'organization_id': organization,\n            }\n            if scope == 'organization':\n                self.parent.log.info('pagination for scope `organization` '\n                                     'not available')\n                org_id = self.parent.whoami.organization_id\n                return self.parent.request(\n                    f'organization/{org_id}/collaboration'\n                )\n            elif scope == 'global':\n                return self.parent.request('collaboration', params=params)\n            else:\n                self.parent.log.info('--> Unrecognized `scope`. Needs to be '\n                                     '`organization` or `global`')\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int) -> dict:\n            \"\"\"View specific collaboration\n\n            Parameters\n            ----------\n            id_ : int\n                Id from the collaboration you want to view\n\n            Returns\n            -------\n            dict\n                Containing the collaboration information\n            \"\"\"\n            return self.parent.request(f'collaboration/{id_}')\n\n        @post_filtering(iterable=False)\n        def create(self, name: str, organizations: list,\n                   encrypted: bool = False) -> dict:\n            \"\"\"Create new collaboration\n\n            Parameters\n            ----------\n            name : str\n                Name of the collaboration\n            organizations : list\n                List of organization ids which participate in the\n                collaboration\n            encrypted : bool, optional\n                Whenever the collaboration should be encrypted or not,\n                by default False\n\n            Returns\n            -------\n            dict\n                Containing the new collaboration meta-data\n            \"\"\"\n            return self.parent.request('collaboration', method='post', json={\n                'name': name,\n                'organization_ids': organizations,\n                'encrypted': encrypted\n            })\n\n    class Node(ClientBase.SubClient):\n        \"\"\"Collection of node requests\"\"\"\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int) -> dict:\n            \"\"\"View specific node\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the node you want to inspect\n\n            Returns\n            -------\n            dict\n                Containing the node meta-data\n            \"\"\"\n            return self.parent.request(f'node/{id_}')\n\n        @post_filtering()\n        def list(self, name: str = None, organization: int = None,\n                 collaboration: int = None, is_online: bool = None,\n                 ip: str = None, last_seen_from: str = None,\n                 last_seen_till: str = None, page: int = 1, per_page: int = 20,\n                 include_metadata: bool = True,\n                 ) -> list:\n            \"\"\"List nodes\n\n            Parameters\n            ----------\n            name: str, optional\n                Filter by name (with LIKE operator)\n            organization: int, optional\n                Filter by organization id\n            collaboration: int, optional\n                Filter by collaboration id\n            is_online: bool, optional\n                Filter on whether nodes are online or not\n            ip: str, optional\n                Filter by node VPN IP address\n            last_seen_from: str, optional\n                Filter if node has been online since date (format: yyyy-mm-dd)\n            last_seen_till: str, optional\n                Filter if node has been online until date (format: yyyy-mm-dd)\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n\n            list of dicts\n                Containing meta-data of the nodes\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'name': name, 'organization_id': organization,\n                'collaboration_id': collaboration, 'ip': ip,\n                'last_seen_from': last_seen_from,\n                'last_seen_till': last_seen_till\n            }\n            if is_online is not None:\n                params['status'] = 'online' if is_online else 'offline'\n            return self.parent.request('node', params=params)\n\n        @post_filtering(iterable=False)\n        def create(self, collaboration: int, organization: int = None,\n                   name: str = None) -> dict:\n            \"\"\"Register new node\n\n            Parameters\n            ----------\n            collaboration : int\n                Collaboration id to which this node belongs\n            organization : int, optional\n                Organization id to which this node belongs. If no id provided\n                the users organization is used. Default value is None\n            name : str, optional\n                Name of the node. If no name is provided the server will\n                generate one. Default value is None\n\n            Returns\n            -------\n            dict\n                Containing the meta-data of the new node\n            \"\"\"\n            if not organization:\n                organization = self.parent.whoami.organization_id\n\n            return self.parent.request('node', method='post', json={\n                'organization_id': organization,\n                'collaboration_id': collaboration,\n                'name': name\n            })\n\n        @post_filtering(iterable=False)\n        def update(self, id_: int, name: str = None, organization: int = None,\n                   collaboration: int = None) -> dict:\n            \"\"\"Update node information\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the node you want to update\n            name : str, optional\n                New node name, by default None\n            organization : int, optional\n                Change the owning organization of the node, by default\n                None\n            collaboration : int, optional\n                Changes the collaboration to which the node belongs, by\n                default None\n\n            Returns\n            -------\n            dict\n                Containing the meta-data of the updated node\n            \"\"\"\n            return self.parent.request(f'node/{id_}', method='patch', json={\n                'name': name,\n                'organization_id': organization,\n                'collaboration_id': collaboration\n            })\n\n        def delete(self, id_: int) -> dict:\n            \"\"\"Deletes a node\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the node you want to delete\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            return self.parent.request(f'node/{id_}', method='delete')\n\n        def kill_tasks(self, id_: int) -> dict:\n            \"\"\"\n            Kill all tasks currently running on a node\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the node of which you want to kill the tasks\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            return self.parent.request(\n                'kill/node/tasks', method='post', json={'id': id_}\n            )\n\n    class Organization(ClientBase.SubClient):\n        \"\"\"Collection of organization requests\"\"\"\n\n        @post_filtering()\n        def list(self, name: str = None, country: int = None,\n                 collaboration: int = None, page: int = None,\n                 per_page: int = None, include_metadata: bool = True) -> list:\n            \"\"\"List organizations\n\n            Parameters\n            ----------\n            name: str, optional\n                Filter by name (with LIKE operator)\n            country: str, optional\n                Filter by country\n            collaboration: int, optional\n                Filter by collaboration id\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            list of dicts\n                Containing meta-data information of the organizations\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'name': name, 'country': country,\n                'collaboration_id': collaboration\n            }\n            return self.parent.request('organization', params=params)\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int = None) -> dict:\n            \"\"\"View specific organization\n\n            Parameters\n            ----------\n            id_ : int, optional\n                Organization `id` of the organization you want to view.\n                In case no `id` is provided it will display your own\n                organization, default value is None.\n\n            Returns\n            -------\n            dict\n                Containing the organization meta-data\n            \"\"\"\n            if not id_:\n                id_ = self.parent.whoami.organization_id\n\n            return self.parent.request(f'organization/{id_}')\n\n        @post_filtering(iterable=False)\n        def update(self, id_: int = None, name: str = None,\n                   address1: str = None, address2: str = None,\n                   zipcode: str = None, country: str = None,\n                   domain: str = None, public_key: str = None) -> dict:\n            \"\"\"Update organization information\n\n            Parameters\n            ----------\n            id_ : int, optional\n                Organization id, by default None\n            name : str, optional\n                New organization name, by default None\n            address1 : str, optional\n                Address line 1, by default None\n            address2 : str, optional\n                Address line 2, by default None\n            zipcode : str, optional\n                Zipcode, by default None\n            country : str, optional\n                Country, by default None\n            domain : str, optional\n                Domain of the organization (e.g. `iknl.nl`), by default None\n            public_key : str, optional\n                public key, by default None\n\n            Returns\n            -------\n            dict\n                The meta-data of the updated organization\n            \"\"\"\n            if not id_:\n                id_ = self.parent.whoami.organization_id\n\n            return self.parent.request(\n                f'organization/{id_}',\n                method='patch',\n                json={\n                    'name': name,\n                    'address1': address1,\n                    'address2': address2,\n                    'zipcode': zipcode,\n                    'country': country,\n                    'domain': domain,\n                    'public_key': public_key\n                }\n            )\n\n        def create(self, name: str, address1: str, address2: str, zipcode: str,\n                   country: str, domain: str, public_key: str = None) -> dict:\n            \"\"\"Create new organization\n\n            Parameters\n            ----------\n            name : str\n                Name of the organization\n            address1 : str\n                Street and number\n            address2 : str\n                City\n            zipcode : str\n                Zip or postal code\n            country : str\n                Country\n            domain : str\n                Domain of the organization (e.g. vantage6.ai)\n            public_key : str, optional\n                Public key of the organization. This can be set later,\n                by default None\n\n            Returns\n            -------\n            dict\n                Containing the information of the new organization\n            \"\"\"\n            json_data = {\n                'name': name,\n                'address1': address1,\n                'address2': address2,\n                'zipcode': zipcode,\n                'country': country,\n                'domain': domain,\n            }\n\n            if public_key:\n                json_data['public_key'] = public_key\n\n            return self.parent.request(\n                'organization',\n                method='post',\n                json=json_data\n            )\n\n    class User(ClientBase.SubClient):\n\n        @post_filtering()\n        def list(self, username: str = None, organization: int = None,\n                 firstname: str = None, lastname: str = None,\n                 email: str = None, role: int = None, rule: int = None,\n                 last_seen_from: str = None, last_seen_till: str = None,\n                 page: int = 1, per_page: int = 20,\n                 include_metadata: bool = True) -> list:\n            \"\"\"List users\n\n            Parameters\n            ----------\n            username: str, optional\n                Filter by username (with LIKE operator)\n            organization: int, optional\n                Filter by organization id\n            firstname: str, optional\n                Filter by firstname (with LIKE operator)\n            lastname: str, optional\n                Filter by lastname (with LIKE operator)\n            email: str, optional\n                Filter by email (with LIKE operator)\n            role: int, optional\n                Show only users that have this role id\n            rule: int, optional\n                Show only users that have this rule id\n            last_seen_from: str, optional\n                Filter users that have logged on since (format yyyy-mm-dd)\n            last_seen_till: str, optional\n                Filter users that have logged on until (format yyyy-mm-dd)\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            list of dicts\n                Containing the meta-data of the users\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'username': username, 'organization_id': organization,\n                'firstname': firstname, 'lastname': lastname, 'email': email,\n                'role_id': role, 'rule_id': rule,\n                'last_seen_from': last_seen_from,\n                'last_seen_till': last_seen_till,\n            }\n            return self.parent.request('user', params=params)\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int = None) -> dict:\n            \"\"\"View user information\n\n            Parameters\n            ----------\n            id_ : int, optional\n                User `id`, by default None. When no `id` is provided\n                your own user information is displayed\n\n            Returns\n            -------\n            dict\n                Containing user information\n            \"\"\"\n            if not id_:\n                id_ = self.parent.whoami.id_\n            return self.parent.request(f'user/{id_}')\n\n        @post_filtering(iterable=False)\n        def update(self, id_: int = None, firstname: str = None,\n                   lastname: str = None, organization: int = None,\n                   rules: list = None, roles: list = None, email: str = None\n                   ) -> dict:\n            \"\"\"Update user details\n\n            In case you do not supply a user_id, your user is being\n            updated.\n\n            Parameters\n            ----------\n            id_ : int\n                User `id` from the user you want to update\n            firstname : str\n                Your first name\n            lastname : str\n                Your last name\n            organization : int\n                Organization id of the organization you want to be part\n                of. This can only done by super-users.\n            rules : list of ints\n                USE WITH CAUTION! Rule ids that should be assigned to\n                this user. All previous assigned rules will be removed!\n            roles : list of ints\n                USE WITH CAUTION! Role ids that should be assigned to\n                this user. All previous assigned roles will be removed!\n            email : str\n                New email from the user\n\n            Returns\n            -------\n            dict\n                A dict containing the updated user data\n            \"\"\"\n            if not id_:\n                id_ = self.parent.whoami.id_\n\n            json_body = {\n                \"firstname\": firstname,\n                \"lastname\": lastname,\n                \"organization_id\": organization,\n                \"rules\": rules,\n                \"roles\": roles,\n                \"email\": email\n            }\n\n            # only submit supplied keys\n            json_body = {k: v for k, v in json_body.items() if v is not None}\n\n            user = self.parent.request(f'user/{id_}', method='patch',\n                                       json=json_body)\n            return user\n\n        @post_filtering(iterable=False)\n        def create(self, username: str, firstname: str, lastname: str,\n                   password: str, email: str, organization: int = None,\n                   roles: list = [], rules: list = []) -> dict:\n            \"\"\"Create new user\n\n            Parameters\n            ----------\n            username : str\n                Used to login to the service. This can not be changed\n                later.\n            firstname : str\n                Firstname of the new user\n            lastname : str\n                Lastname of the new user\n            password : str\n                Password of the new user\n            organization : int\n                Organization `id` this user should belong to\n            roles : list of ints\n                Role ids that are assigned to this user. Note that you\n                can only assign roles if you own the rules within this\n                role.\n            rules : list of ints\n                Rule ids that are assigned to this user. Note that you\n                can only assign rules that you own\n\n            Return\n            ----------\n            dict\n                Containing data of the new user\n            \"\"\"\n            user_data = {\n                'username': username,\n                'firstname': firstname,\n                'lastname': lastname,\n                'password': password,\n                'email': email,\n                'organization_id': organization,\n                'roles': roles,\n                'rules': rules\n            }\n            return self.parent.request('user', json=user_data, method='post')\n\n    class Role(ClientBase.SubClient):\n\n        @post_filtering()\n        def list(self, name: str = None, description: str = None,\n                 organization: int = None, rule: int = None, user: int = None,\n                 include_root: bool = None, page: int = 1, per_page: int = 20,\n                 include_metadata: bool = True) -> list:\n            \"\"\"List of roles\n\n            Parameters\n            ----------\n            name: str, optional\n                Filter by name (with LIKE operator)\n            description: str, optional\n                Filter by description (with LIKE operator)\n            organization: int, optional\n                Filter by organization id\n            rule: int, optional\n                Only show roles that contain this rule id\n            user: int, optional\n                Only show roles that belong to a particular user id\n            include_root: bool, optional\n                Include roles that are not assigned to any particular\n                organization\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            list of dicts\n                Containing roles meta-data\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'name': name, 'description': description,\n                'organization_id': organization, 'rule_id': rule,\n                'include_root': include_root, 'user_id': user,\n            }\n            return self.parent.request('role', params=params)\n\n        @post_filtering(iterable=True)\n        def get(self, id_: int) -> dict:\n            \"\"\"View specific role\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the role you want to insepct\n\n            Returns\n            -------\n            dict\n                Containing meta-data of the role\n            \"\"\"\n            return self.parent.request(f'role/{id_}')\n\n        @post_filtering(iterable=True)\n        def create(self, name: str, description: str, rules: list,\n                   organization: int = None) -> dict:\n            \"\"\"Register new role\n\n            Parameters\n            ----------\n            name : str\n                Role name\n            description : str\n                Human readable description of the role\n            rules : list\n                Rules that this role contains\n            organization : int, optional\n                Organization to which this role belongs. In case this is\n                not provided the users organization is used. By default\n                None\n\n            Returns\n            -------\n            dict\n                Containing meta-data of the new role\n            \"\"\"\n            if not organization:\n                organization = self.parent.whoami.organization_id\n            return self.parent.request('role', method='post', json={\n                'name': name,\n                'description': description,\n                'rules': rules,\n                'organization_id': organization\n            })\n\n        @post_filtering(iterable=True)\n        def update(self, role: int, name: str = None, description: str = None,\n                   rules: list = None) -> dict:\n            \"\"\"Update role\n\n            Parameters\n            ----------\n            role : int\n                Id of the role that updated\n            name : str, optional\n                New name of the role, by default None\n            description : str, optional\n                New description of the role, by default None\n            rules : list, optional\n                CAUTION! This will not *add* rules but replace them. If\n                you remove rules from your own role you lose access. By\n                default None\n\n            Returns\n            -------\n            dict\n                Containing the updated role data\n            \"\"\"\n            return self.parent.request(f'role/{role}', method='patch', json={\n                'name': name,\n                'description': description,\n                'rules': rules\n            })\n\n        def delete(self, role: int) -> dict:\n            \"\"\"Delete role\n\n            Parameters\n            ----------\n            role : int\n                CAUTION! Id of the role to be deleted. If you remove\n                roles that are attached to you, you might lose access!\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            res = self.parent.request(f'role/{role}', method='delete')\n            self.parent.log.info(f'--> {res.get(\"msg\")}')\n\n    class Task(ClientBase.SubClient):\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int, include_results: bool = False) -> dict:\n            \"\"\"View specific task\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the task you want to view\n            include_results : bool, optional\n                Whenever to include the results or not, by default False\n\n            Returns\n            -------\n            dict\n                Containing the task data\n            \"\"\"\n            params = {}\n            params['include'] = 'results' if include_results else None\n            return self.parent.request(f'task/{id_}', params=params)\n\n        @post_filtering()\n        def list(self, initiator: int = None, initiating_user: int = None,\n                 collaboration: int = None, image: str = None,\n                 parent: int = None, run: int = None,\n                 name: str = None, include_results: bool = False,\n                 description: str = None, database: str = None,\n                 result: int = None, status: str = None, page: int = 1,\n                 per_page: int = 20, include_metadata: bool = True) -> dict:\n            \"\"\"List tasks\n\n            Parameters\n            ----------\n            name: str, optional\n                Filter by the name of the task. It will match with a\n                Like operator. I.e. E% will search for task names that\n                start with an 'E'.\n            initiator: int, optional\n                Filter by initiating organization\n            initiating_user: int, optional\n                Filter by initiating user\n            collaboration: int, optional\n                Filter by collaboration\n            image: str, optional\n                Filter by Docker image name (with LIKE operator)\n            parent: int, optional\n                Filter by parent task\n            run: int, optional\n                Filter by run\n            include_results : bool, optional\n                Whenever to include the results in the tasks, by default\n                False\n            description: str, optional\n                Filter by description (with LIKE operator)\n            database: str, optional\n                Filter by database (with LIKE operator)\n            result: int, optional\n                Only show task that contains this result id\n            status: str, optional\n                Filter by task status (e.g. 'active', 'pending', 'completed',\n                'crashed')\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            dict\n                dictonairy containing the key 'data' which contains the\n                tasks and a key 'links' containing the pagination\n                metadata\n\n            OR\n\n            list\n                when 'include_metadata' is set to false, it removes the\n                metadata wrapper. I.e. directly returning the 'data'\n                key.\n            \"\"\"\n            # if the param is None, it will not be passed on to the\n            # request\n            # TODO in v4+, we should change the 'initiator' argument to\n            # a name that distinguishes it better from the initiating user.\n            # Then, we should also change it in the server\n            params = {\n                'initiator_id': initiator, 'init_user_id': initiating_user,\n                'collaboration_id': collaboration,\n                'image': image, 'parent_id': parent, 'run_id': run,\n                'name': name, 'page': page, 'per_page': per_page,\n                'description': description, 'database': database,\n                'result_id': result, 'status': status,\n            }\n            includes = []\n            if include_results:\n                includes.append('results')\n            if include_metadata:\n                includes.append('metadata')\n            params['include'] = includes\n\n            return self.parent.request('task', params=params)\n\n        @post_filtering(iterable=False)\n        def create(self, collaboration: int, organizations: list, name: str,\n                   image: str, description: str, input: dict,\n                   data_format: str = LEGACY,\n                   database: str = 'default') -> dict:\n            \"\"\"Create a new task\n\n            Parameters\n            ----------\n            collaboration : int\n                Id of the collaboration to which this task belongs\n            organizations : list\n                Organization ids (within the collaboration) which need\n                to execute this task\n            name : str\n                Human readable name\n            image : str\n                Docker image name which contains the algorithm\n            description : str\n                Human readable description\n            input : dict\n                Algorithm input\n            data_format : str, optional\n                IO data format used, by default LEGACY\n            database: str, optional\n                Database name to be used at the node\n\n            Returns\n            -------\n            dict\n                [description]\n            \"\"\"\n            return self.parent.post_task(name, image, collaboration, input,\n                                         description, organizations,\n                                         data_format, database)\n\n        def delete(self, id_: int) -> dict:\n            \"\"\"Delete a task\n\n            Also removes the related results.\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the task to be removed\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            msg = self.parent.request(f'task/{id_}', method='delete')\n            self.parent.log.info(f'--> {msg}')\n\n        def kill(self, id_: int) -> dict:\n            \"\"\"Kill a task running on one or more nodes\n\n            Note that this does not remove the task from the database, but\n            merely halts its execution (and prevents it from being restarted).\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the task to be killed\n\n            Returns\n            -------\n            dict\n                Message from the server\n            \"\"\"\n            msg = self.parent.request('/kill/task', method='post', json={\n                'id': id_\n            })\n            self.parent.log.info(f'--> {msg}')\n\n    class Result(ClientBase.SubClient):\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int, include_task: bool = False) -> dict:\n            \"\"\"View a specific result\n\n            Parameters\n            ----------\n            id_ : int\n                id of the result you want to inspect\n            include_task : bool, optional\n                Whenever to include the task or not, by default False\n\n            Returns\n            -------\n            dict\n                Containing the result data\n            \"\"\"\n            self.parent.log.info('--> Attempting to decrypt results!')\n\n            # get_results also handles decryption\n            result = self.parent.get_results(id=id_, include_task=include_task)\n            result_data = result.get('result')\n            if result_data:\n                try:\n                    result['result'] = deserialization.load_data(result_data)\n                except Exception as e:\n                    self.parent.log.warn('--> Failed to deserialize')\n                    self.parent.log.debug(e)\n\n            return result\n\n        @post_filtering()\n        def list(self, task: int = None, organization: int = None,\n                 state: str = None, node: int = None,\n                 include_task: bool = False, started: Tuple[str, str] = None,\n                 assigned: Tuple[str, str] = None,\n                 finished: Tuple[str, str] = None, port: int = None,\n                 page: int = None, per_page: int = None,\n                 include_metadata: bool = True) -> list:\n            \"\"\"List results\n\n            Parameters\n            ----------\n            task: int, optional\n                Filter by task id\n            organization: int, optional\n                Filter by organization id\n            state: int, optional\n                Filter by state: ('open',)\n            node: int, optional\n                Filter by node id\n            include_task : bool, optional\n                Whenever to include the task or not, by default False\n            started: Tuple[str, str], optional\n                Filter on a range of start times (format: yyyy-mm-dd)\n            assigned: Tuple[str, str], optional\n                Filter on a range of assign times (format: yyyy-mm-dd)\n            finished: Tuple[str, str], optional\n                Filter on a range of finished times (format: yyyy-mm-dd)\n            port: int, optional\n                Port on which result was computed\n            page: int, optional\n                Pagination page number, defaults to 1\n            per_page: int, optional\n                Number of items per page, defaults to 20\n            include_metedata: bool, optional\n                Whenevet to include pagination metadata, defaults to\n                True\n\n            Returns\n            -------\n            dict\n                Containing the key 'data' which contains a list of\n                results, and a key 'links' which contains the pagination\n                metadata\n\n            OR\n\n            list of dicts\n                When include_metadata is set to False, the metadata wrapper\n                is stripped and only a list of results is returned\n            \"\"\"\n            includes = []\n            if include_metadata:\n                includes.append('metadata')\n            if include_task:\n                includes.append('task')\n\n            s_from, s_till = started if started else (None, None)\n            a_from, a_till = assigned if assigned else (None, None)\n            f_from, f_till = finished if finished else (None, None)\n\n            params = {\n                'task_id': task, 'organization_id': organization,\n                'state': state, 'node_id': node, 'page': page,\n                'per_page': per_page, 'include': includes,\n                'started_from': s_from, 'started_till': s_till,\n                'assigned_from': a_from, 'assigned_till': a_till,\n                'finished_from': f_from, 'finished_till': f_till,\n                'port': port\n            }\n\n            results = self.parent.get_results(params=params)\n\n            if isinstance(results, dict):\n                wrapper = results\n                results = results['data']\n\n            cleaned_results = []\n            for result in results:\n                if result.get('result'):\n                    try:\n                        des_res = deserialization.load_data(\n                            result.get('result')\n                        )\n                    except Exception as e:\n                        id_ = result.get('id')\n                        self.parent.log.warn('Could not deserialize result id='\n                                             f'{id_}')\n                        self.parent.log.debug(e)\n                        continue\n                    result['result'] = des_res\n                cleaned_results.append(result)\n\n            if 'wrapper' in locals():\n                wrapper['data'] = cleaned_results\n                cleaned_results = wrapper\n\n            return cleaned_results\n\n        def from_task(self, task_id: int, include_task: bool = False):\n            self.parent.log.info('--> Attempting to decrypt results!')\n\n            # get_results also handles decryption\n            results = self.parent.get_results(task_id=task_id,\n                                              include_task=include_task)\n            cleaned_results = []\n            for result in results:\n                if result.get('result'):\n                    des_res = deserialization.load_data(result.get('result'))\n                    result['result'] = des_res\n                cleaned_results.append(result)\n\n            return cleaned_results\n\n    class Rule(ClientBase.SubClient):\n\n        @post_filtering(iterable=False)\n        def get(self, id_: int) -> dict:\n            \"\"\"View specific rule\n\n            Parameters\n            ----------\n            id_ : int\n                Id of the rule you want to view\n\n            Returns\n            -------\n            dict\n                Containing the information about this rule\n            \"\"\"\n            return self.parent.request(f'rule/{id_}')\n\n        @post_filtering()\n        def list(self, name: str = None, operation: str = None,\n                 scope: str = None, role: int = None, page: int = 1,\n                 per_page: int = 20, include_metadata: bool = True) -> list:\n            \"\"\"List of all available rules\n\n            Parameters\n            ----------\n            name: str, optional\n                Filter by rule name\n            operation: str, optional\n                Filter by operation\n            scope: str, optional\n                Filter by scope\n            role: int, optional\n                Only show rules that belong to this role id\n            page: int, optional\n                Pagination page, by default 1\n            per_page: int, optional\n                Number of items on a single page, by default 20\n            include_metadata: bool, optional\n                Whenever to include the pagination metadata. If this is\n                set to False the output is no longer wrapped in a\n                dictonairy, by default True\n\n            Returns\n            -------\n            list of dicts\n                Containing all the rules from the vantage6 server\n            \"\"\"\n            includes = ['metadata'] if include_metadata else []\n            params = {\n                'page': page, 'per_page': per_page, 'include': includes,\n                'name': name, 'operation': operation, 'scope': scope,\n                'role_id': role\n            }\n            return self.parent.request('rule', params=params)\n\n\n# TODO remove in v4+ (deprecated for AlgorithmClient but still kept for\n# backwards compatibility)\nclass ContainerClient(ClientBase):\n    \"\"\" Container interface to the local proxy server (central server).\n\n        An algorithm container should never communicate directly to the\n        central server. Therefore the algorithm container has no\n        internet connection. The algorithm can, however, talk to a local\n        proxy server which has interface to the central server. This way\n        we make sure that the algorithm container does not share stuff\n        with others, and we also can encrypt the results for a specific\n        receiver. Thus this not a interface to the central server but to\n        the local proxy server. However the interface is identical thus\n        we are happy that we can ignore this detail.\n    \"\"\"\n\n    def __init__(self, token: str, *args, **kwargs):\n        \"\"\"Container client.\n        A client which can be used by algorithms. All permissions of the\n        container are derived from the token.\n\n        Parameters\n        ----------\n        token : str\n            JWT (container) token, generated by the node\n                the algorithm container runs on\n        \"\"\"\n        super().__init__(*args, **kwargs)\n\n        # obtain the identity from the token\n        jwt_payload = jwt.decode(\n            token, options={\"verify_signature\": False})\n\n        # FIXME: 'identity' is no longer needed in version 4+. So this if\n        # statement can be removed\n        if 'sub' in jwt_payload:\n            container_identity = jwt_payload['sub']\n        elif 'identity' in jwt_payload:\n            container_identity = jwt_payload['identity']\n\n        self.image = container_identity.get(\"image\")\n        self.database = container_identity.get('database')\n        self.host_node_id = container_identity.get(\"node_id\")\n        self.collaboration_id = container_identity.get(\"collaboration_id\")\n        self.log.info(\n            f\"Container in collaboration_id={self.collaboration_id} \\n\"\n            f\"Key created by node_id {self.host_node_id} \\n\"\n            f\"Can only use image={self.image}\"\n        )\n\n        self._access_token = token\n        self.log.debug(f\"Access token={self._access_token}\")\n\n    def authenticate(self):\n        \"\"\" Containers obtain their key via their host Node.\"\"\"\n        self.log.warn(\"Containers do not authenticate?!\")\n        return\n\n    def refresh_token(self):\n        \"\"\" Containers cannot refresh their token.\n\n            TODO we might want to notify node/server about this...\n            TODO make a more usefull exception\n        \"\"\"\n        raise Exception(\"Containers cannot refresh!\")\n\n    def get_results(self, task_id: int):\n        \"\"\" Obtain results from a specific task at the server\n\n            Containers are allowed to obtain the results of their\n            children (having the same run_id at the server). The\n            permissions are checked at te central server.\n\n            :param task_id: id of the task from which you want to obtain\n                the results\n        \"\"\"\n        results = self.request(\n            f\"task/{task_id}/result\"\n        )\n\n        res = []\n        # Encryption is not done at the client level for the container.\n        # Although I am not completely sure that the format is always\n        # a pickle.\n        # for result in results:\n        #     self._decrypt_result(result)\n        #     res.append(result.get(\"result\"))\n        #\n        try:\n            res = [pickle.loads(base64s_to_bytes(result.get(\"result\")))\n                   for result in results if result.get(\"result\")]\n        except Exception as e:\n            self.log.error('Unable to unpickle result')\n            self.log.debug(e)\n\n        return res\n\n    def get_algorithm_addresses(self, task_id: int):\n        \"\"\"\n        Return IP address and port number of other algorithm containers\n        involved in a task so that VPN can be used for communication\n        \"\"\"\n        results = self.request(f\"task/{task_id}/result\")\n\n        algorithm_addresses = []\n        for result in results:\n            for port in result['ports']:\n                algorithm_addresses.append({\n                    'ip': result['node']['ip'],\n                    'port': port['port'],\n                    'label': port['label']\n                })\n        return algorithm_addresses\n\n    def get_algorithm_address_by_label(self, task_id: int, label: str) -> str:\n        \"\"\"\n        Return the IP address plus port number of a given port label\n        \"\"\"\n        algorithm_addresses = self.get_algorithm_addresses(task_id=task_id)\n        for address in algorithm_addresses:\n            if address['label'] == label:\n                return f\"{address['ip']}:{address['port']}\"\n        return None\n\n    def get_task(self, task_id: int):\n        return self.request(\n            f\"task/{task_id}\"\n        )\n\n    def create_new_task(self, input_, organization_ids=[]):\n        \"\"\" Create a new (child) task at the central server.\n\n            Containers are allowed to create child tasks (having the\n            same run_id) at the central server. The docker image must\n            be the same as the docker image of this container self.\n\n            :param input_: input to the task\n            :param organization_ids: organization ids which need to\n                execute this task\n        \"\"\"\n        self.log.debug(f\"create new task for {organization_ids}\")\n\n        return self.post_task(\n            name=\"subtask\",\n            description=f\"task from container on node_id={self.host_node_id}\",\n            collaboration_id=self.collaboration_id,\n            organization_ids=organization_ids,\n            input_=input_,\n            image=self.image,\n            database=self.database\n        )\n\n    def get_organizations_in_my_collaboration(self):\n        \"\"\" Obtain all organization in the collaboration.\n\n            The container runs in a Node which is part of a single\n            collaboration. This method retrieves all organization data\n            that are within that collaboration. This can be used to\n            target specific organizations in a collaboration.\n        \"\"\"\n        organizations = self.request(\n            f\"collaboration/{self.collaboration_id}/organization\")\n        return organizations\n\n    def post_task(self, name: str, image: str, collaboration_id: int,\n                  input_: str = '', description='',\n                  organization_ids: list = [], database='default') -> dict:\n        \"\"\" Post a new task at the central server.\n\n            ! To create a new task from the algorithm container you\n            should use the `create_new_task` function !\n\n            Creating a task from a container does need to be encrypted.\n            This is done because the container should never have access\n            to the private key of this organization. The encryption\n            takes place in the local proxy server to which the algorithm\n            communicates (indirectly to the central server). Therefore\n            we needed to overload the post_task function.\n\n            :param name: human-readable name\n            :param image: docker image name of the task\n            :param collaboration_id: id of the collaboration in which\n                the task should run\n            :param input_: input to the task\n            :param description: human-readable description\n            :param organization_ids: ids of the organizations where this\n                task should run\n        \"\"\"\n        self.log.debug(\"post task without encryption (is handled by proxy)\")\n\n        serialized_input = bytes_to_base64s(pickle.dumps(input_))\n\n        organization_json_list = []\n        for org_id in organization_ids:\n            organization_json_list.append(\n                {\n                    \"id\": org_id,\n                    \"input\": serialized_input\n                }\n            )\n\n        return self.request('task', method='post', json={\n            \"name\": name,\n            \"image\": image,\n            \"collaboration_id\": collaboration_id,\n            \"description\": description,\n            \"organizations\": organization_json_list,\n            \"database\": database\n        })\n\n\n# For backwards compatibility\nClient = UserClient\n", "\"\"\"\nA node in its simplest would retrieve a task from the central server by\nan API call, run this task and finally return the results to the central\nserver again.\n\nThe node application runs four threads:\n\n*Main thread*\n    Checks the task queue and run the next task if there is one available.\n*Listening thread*\n    Listens for incoming websocket messages. Among other functionality, it adds\n    new tasks to the task queue.\n*Speaking thread*\n    Waits for tasks to finish. When they do, return the results to the central\n    server.\n*Proxy server thread*\n    Algorithm containers are isolated from the internet for security reasons.\n    The local proxy server provides an interface to the central server for\n    *master* containers to create subtasks and retrieve their results.\n\nThe node connects to the server using a websocket connection. This connection\nis mainly used for sharing status updates. This avoids the need for polling to\nsee if there are new tasks available.\n\"\"\"\nimport sys\nimport os\nimport random\nimport time\nimport datetime\nimport logging\nimport queue\nimport json\nimport shutil\nimport requests.exceptions\n\nfrom pathlib import Path\nfrom threading import Thread\nfrom typing import Dict, List, Union, Type\nfrom socketio import Client as SocketIO\nfrom gevent.pywsgi import WSGIServer\nfrom enum import Enum\n\nfrom vantage6.common.docker.addons import (\n    ContainerKillListener, check_docker_running, running_in_docker\n)\nfrom vantage6.common.globals import VPN_CONFIG_FILE\nfrom vantage6.common.exceptions import AuthenticationException\nfrom vantage6.common.docker.network_manager import NetworkManager\nfrom vantage6.common.task_status import TaskStatus\nfrom vantage6.cli.context import NodeContext\nfrom vantage6.node.context import DockerNodeContext\nfrom vantage6.node.globals import (\n    NODE_PROXY_SERVER_HOSTNAME, SLEEP_BTWN_NODE_LOGIN_TRIES,\n    TIME_LIMIT_RETRY_CONNECT_NODE, TIME_LIMIT_INITIAL_CONNECTION_WEBSOCKET\n)\nfrom vantage6.node.server_io import NodeClient\nfrom vantage6.node import proxy_server\nfrom vantage6.node.util import logger_name, get_parent_id\nfrom vantage6.node.docker.docker_manager import DockerManager\nfrom vantage6.node.docker.vpn_manager import VPNManager\nfrom vantage6.node.socket import NodeTaskNamespace\nfrom vantage6.node.docker.ssh_tunnel import SSHTunnel\n\n\nclass VPNConnectMode(Enum):\n    FIRST_TRY = 1\n    REFRESH_KEYPAIR = 2\n    REFRESH_COMPLETE = 3\n\n\n# ------------------------------------------------------------------------------\nclass Node(object):\n    \"\"\"\n    Authenticates to the central server, setup encryption, a\n    websocket connection, retrieving task that were posted while\n    offline, preparing dataset for usage and finally setup a\n    local proxy server..\n\n    Parameters\n    ----------\n    ctx: Union[NodeContext, DockerNodeContext]\n        Application context object.\n\n    \"\"\"\n    def __init__(self, ctx: Union[NodeContext, DockerNodeContext]):\n\n        self.log = logging.getLogger(logger_name(__name__))\n        self.ctx = ctx\n\n        # Initialize the node. If it crashes, shut down the parts that started\n        # already\n        try:\n            self.initialize()\n        except Exception:\n            self.cleanup()\n            raise\n\n    def initialize(self) -> None:\n        \"\"\"Initialization of the node\"\"\"\n        # check if docker is running, otherwise exit with error\n        check_docker_running()\n\n        self.config = self.ctx.config\n        self.queue = queue.Queue()\n        self._using_encryption = None\n\n        # initialize Node connection to the server\n        self.server_io = NodeClient(\n            host=self.config.get('server_url'),\n            port=self.config.get('port'),\n            path=self.config.get('api_path')\n        )\n\n        self.log.info(f\"Connecting server: {self.server_io.base_path}\")\n\n        # Authenticate with the server, obtaining a JSON Web Token.\n        # Note that self.authenticate() blocks until it succeeds.\n        self.log.debug(\"Authenticating\")\n        self.authenticate()\n\n        # Setup encryption\n        self.setup_encryption()\n\n        # Thread for proxy server for algorithm containers, so they can\n        # communicate with the central server.\n        self.log.info(\"Setting up proxy server\")\n        t = Thread(target=self.__proxy_server_worker, daemon=True)\n        t.start()\n\n        # Create a long-lasting websocket connection.\n        self.log.debug(\"Creating websocket connection with the server\")\n        self.connect_to_socket()\n\n        # setup docker isolated network manager\n        internal_ = running_in_docker()\n        if not internal_:\n            self.log.warn(\n                \"Algorithms have internet connection! \"\n                \"This happens because you use 'vnode-local'!\"\n            )\n        isolated_network_mgr = NetworkManager(self.ctx.docker_network_name)\n        isolated_network_mgr.create_network(is_internal=internal_)\n\n        # Setup tasks dir\n        self._set_task_dir(self.ctx)\n\n        # Setup VPN connection\n        self.vpn_manager = self.setup_vpn_connection(\n            isolated_network_mgr, self.ctx)\n\n        # Create SSH tunnel according to the node configuration\n        self.ssh_tunnels = self.setup_ssh_tunnels(isolated_network_mgr)\n\n        # setup the docker manager\n        self.log.debug(\"Setting up the docker manager\")\n        self.__docker = DockerManager(\n            ctx=self.ctx,\n            isolated_network_mgr=isolated_network_mgr,\n            vpn_manager=self.vpn_manager,\n            tasks_dir=self.__tasks_dir,\n            client=self.server_io,\n        )\n\n        # Connect the node to the isolated algorithm network *only* if we're\n        # running in a docker container.\n        if self.ctx.running_in_docker:\n            isolated_network_mgr.connect(\n                container_name=self.ctx.docker_container_name,\n                aliases=[NODE_PROXY_SERVER_HOSTNAME]\n            )\n\n        # Connect any docker services specified in the configuration file to\n        # the node container\n        self.link_docker_services()\n\n        # Thread for sending results to the server when they come available.\n        self.log.debug(\"Start thread for sending messages (results)\")\n        t = Thread(target=self.__speaking_worker, daemon=True)\n        t.start()\n\n        # listen forever for incoming messages, tasks are stored in\n        # the queue.\n        self.log.debug(\"Starting thread for incoming messages (tasks)\")\n        t = Thread(target=self.__listening_worker, daemon=True)\n        t.start()\n\n        self.log.info('Init complete')\n\n    def __proxy_server_worker(self) -> None:\n        \"\"\"\n        Proxy algorithm container communcation.\n\n        A proxy for communication between algorithms and central\n        server.\n        \"\"\"\n        if self.ctx.running_in_docker:\n            # NODE_PROXY_SERVER_HOSTNAME points to the name of the proxy\n            # when running in the isolated docker network.\n            default_proxy_host = NODE_PROXY_SERVER_HOSTNAME\n        else:\n            # If we're running non-dockerized, assume that the proxy is\n            # accessible from within the docker algorithm container on\n            # host.docker.internal.\n            default_proxy_host = 'host.docker.internal'\n\n        # If PROXY_SERVER_HOST was set in the environment, it overrides our\n        # value.\n        proxy_host = os.environ.get(\"PROXY_SERVER_HOST\", default_proxy_host)\n        os.environ[\"PROXY_SERVER_HOST\"] = proxy_host\n\n        proxy_port = int(os.environ.get(\"PROXY_SERVER_PORT\", 8080))\n\n        # 'app' is defined in vantage6.node.proxy_server\n        # app.debug = True\n        proxy_server.app.config[\"SERVER_IO\"] = self.server_io\n        proxy_server.server_url = self.server_io.base_path\n\n        # this is where we try to find a port for the proxyserver\n        for try_number in range(5):\n            self.log.info(\n                f\"Starting proxyserver at '{proxy_host}:{proxy_port}'\")\n            http_server = WSGIServer(('0.0.0.0', proxy_port), proxy_server.app)\n\n            try:\n                http_server.serve_forever()\n\n            except OSError as e:\n                self.log.debug(f'Error during attempt {try_number}')\n                self.log.debug(f'{type(e)}: {e}')\n\n                if e.errno == 48:\n                    proxy_port = random.randint(2048, 16384)\n                    self.log.critical(\n                        f\"Retrying with a different port: {proxy_port}\")\n                    os.environ['PROXY_SERVER_PORT'] = str(proxy_port)\n\n                else:\n                    raise\n\n            except Exception as e:\n                self.log.error('Proxyserver could not be started or crashed!')\n                self.log.error(e)\n\n    def sync_task_queue_with_server(self) -> None:\n        \"\"\" Get all unprocessed tasks from the server for this node.\"\"\"\n        assert self.server_io.cryptor, \"Encrpytion has not been setup\"\n\n        # request open tasks from the server\n        tasks = self.server_io.get_results(state=\"open\", include_task=True)\n        self.log.debug(tasks)\n        for task in tasks:\n            self.queue.put(task)\n\n        self.log.info(f\"received {self.queue._qsize()} tasks\")\n\n    def __start_task(self, taskresult: dict) -> None:\n        \"\"\"\n        Start the docker image and notify the server that the task has been\n        started.\n\n        Parameters\n        ----------\n        taskresult : dict\n            A dictionary with information required to run the algorithm\n        \"\"\"\n        task = taskresult['task']\n        self.log.info(\"Starting task {id} - {name}\".format(**task))\n\n        # notify that we are processing this task\n        self.server_io.set_task_start_time(taskresult[\"id\"])\n\n        token = self.server_io.request_token_for_container(\n            task[\"id\"],\n            task[\"image\"]\n        )\n        token = token[\"container_token\"]\n\n        # create a temporary volume for each run_id\n        vol_name = self.ctx.docker_temporary_volume_name(task[\"run_id\"])\n        self.__docker.create_volume(vol_name)\n\n        # For some reason, if the key 'input' consists of JSON, it is\n        # automatically marshalled? This causes trouble, so we'll serialize it\n        # again.\n        # FIXME: should probably find & fix the root cause?\n        if type(taskresult['input']) == dict:\n            taskresult['input'] = json.dumps(taskresult['input'])\n\n        # Run the container. This adds the created container/task to the list\n        # __docker.active_tasks\n        task_status, vpn_ports = self.__docker.run(\n            result_id=taskresult[\"id\"],\n            task_info=task,\n            image=task[\"image\"],\n            docker_input=taskresult['input'],\n            tmp_vol_name=vol_name,\n            token=token,\n            database=task.get('database', 'default')\n        )\n\n        # save task status to the server and send socket event to update others\n        self.server_io.patch_results(\n            id=taskresult['id'], result={'status': task_status}\n        )\n        self.socketIO.emit(\n            'algorithm_status_change',\n            data={\n                'node_id': self.server_io.whoami.id_,\n                'status': task_status,\n                'result_id': taskresult['id'],\n                'task_id': task['id'],\n                'collaboration_id': self.server_io.collaboration_id,\n                'organization_id': self.server_io.whoami.organization_id,\n                'parent_id': get_parent_id(task),\n            },\n            namespace='/tasks',\n        )\n\n        if vpn_ports:\n            # Save port of VPN client container at which it redirects traffic\n            # to the algorithm container. First delete any existing port\n            # assignments in case algorithm has crashed\n            self.server_io.request(\n                'port', params={'result_id': taskresult['id']}, method=\"DELETE\"\n            )\n            for port in vpn_ports:\n                port['result_id'] = taskresult['id']\n                self.server_io.request('port', method='POST', json=port)\n\n            # Save IP address of VPN container\n            # FIXME BvB 2023-02-21: node IP is now updated when task is started\n            # but this should be done when VPN connection is established\n            node_id = self.server_io.whoami.id_\n            node_ip = self.vpn_manager.get_vpn_ip()\n            self.server_io.request(\n                f\"node/{node_id}\", json={\"ip\": node_ip}, method=\"PATCH\"\n            )\n\n    def __listening_worker(self) -> None:\n        \"\"\"\n        Listen for incoming (websocket) messages from the server.\n\n        Runs in a separate thread. Received events are handled by the\n        appropriate action handler.\n        \"\"\"\n        self.log.debug(\"Listening for incoming messages\")\n\n        # FIXME: while True in combination with a wait() call that never exits\n        #   makes joining the tread (to terminate) difficult?\n        while True:\n            # incoming messages are handled by the action_handler instance\n            # which is attached when the socket connection was made. wait()\n            # is blocks forever (if no time is specified).\n            try:\n                self.socketIO.wait()\n            except Exception as e:\n                self.log.error('Listening thread had an exception')\n                self.log.debug(e)\n\n    def __speaking_worker(self) -> None:\n        \"\"\"\n        Sending messages to central server.\n\n        Routine that is in a seperate thread sending results\n        to the server when they come available.\n        \"\"\"\n        # TODO change to a single request, might need to reconsider\n        #     the flow\n        self.log.debug(\"Waiting for results to send to the server\")\n\n        while True:\n            try:\n                results = self.__docker.get_result()\n\n                # notify socket channel of algorithm status change\n                self.socketIO.emit(\n                    'algorithm_status_change',\n                    data={\n                        'node_id': self.server_io.whoami.id_,\n                        'status': results.status,\n                        'result_id': results.result_id,\n                        'task_id': results.task_id,\n                        'collaboration_id': self.server_io.collaboration_id,\n                        'organization_id':\n                            self.server_io.whoami.organization_id,\n                        'parent_id': results.parent_id,\n                    },\n                    namespace='/tasks',\n                )\n\n                self.log.info(\n                    f\"Sending result (id={results.result_id}) to the server!\")\n\n                # FIXME: why are we retrieving the result *again*? Shouldn't we\n                # just store the task_id when retrieving the task the first\n                # time?\n                response = self.server_io.request(\n                    f\"result/{results.result_id}\"\n                )\n                task_id = response.get(\"task\").get(\"id\")\n\n                if not task_id:\n                    self.log.error(\n                        f\"task_id of result (id={results.result_id}) \"\n                        f\"could not be retrieved\"\n                    )\n                    return\n\n                response = self.server_io.request(f\"task/{task_id}\")\n\n                init_org_id = response.get(\"initiator\")\n                if not init_org_id:\n                    self.log.error(\n                        f\"Initiator organization from task (id={task_id})could\"\n                        \" not be retrieved!\"\n                    )\n\n                self.server_io.patch_results(\n                    id=results.result_id,\n                    result={\n                        'result': results.data,\n                        'log': results.logs,\n                        'status': results.status,\n                        'finished_at': datetime.datetime.now().isoformat(),\n                    },\n                    init_org_id=init_org_id,\n                )\n            except Exception:\n                self.log.exception('Speaking thread had an exception')\n\n    def __print_connection_error_logs(self):\n        \"\"\" Print error message when node cannot find the server \"\"\"\n        self.log.warning(\n            \"Could not connect to the server. Retrying in 10 seconds\")\n        if self.server_io.host == 'http://localhost' and running_in_docker():\n            self.log.warn(\n                f\"You are trying to reach the server at {self.server_io.host}.\"\n                \" As your node is running inside a Docker container, it cannot\"\n                \" reach localhost on your host system. Probably, you have to \"\n                \"change your serverl URL to http://host.docker.internal \"\n                \"(Windows/MacOS) or http://172.17.0.1 (Linux).\"\n            )\n        else:\n            self.log.debug(\"Are you sure the server can be reached at \"\n                           f\"{self.server_io.base_path}?\")\n\n    def authenticate(self) -> None:\n        \"\"\"\n        Authenticate with the server using the api-key from the configuration\n        file. If the server rejects for any reason -other than a wrong API key-\n        serveral attempts are taken to retry.\n        \"\"\"\n\n        api_key = self.config.get(\"api_key\")\n\n        success = False\n        i = 0\n        while i < TIME_LIMIT_RETRY_CONNECT_NODE / SLEEP_BTWN_NODE_LOGIN_TRIES:\n            i = i + 1\n            try:\n                self.server_io.authenticate(api_key)\n\n            except AuthenticationException as e:\n                msg = \"Authentication failed: API key is wrong!\"\n                self.log.warning(msg)\n                self.log.debug(e)\n                break\n            except requests.exceptions.ConnectionError:\n                self.__print_connection_error_logs()\n                time.sleep(SLEEP_BTWN_NODE_LOGIN_TRIES)\n            except Exception as e:\n                msg = ('Authentication failed. Retrying in '\n                       f'{SLEEP_BTWN_NODE_LOGIN_TRIES} seconds!')\n                self.log.warning(msg)\n                self.log.debug(e)\n                time.sleep(SLEEP_BTWN_NODE_LOGIN_TRIES)\n\n            else:\n                # This is only executed if try-block executed without error.\n                success = True\n                break\n\n        if success:\n            self.log.info(f\"Node name: {self.server_io.name}\")\n        else:\n            self.log.critical('Unable to authenticate. Exiting')\n            exit(1)\n\n        # start thread to keep the connection alive by refreshing the token\n        self.server_io.auto_refresh_token()\n\n    def private_key_filename(self) -> Path:\n        \"\"\"Get the path to the private key.\"\"\"\n\n        # FIXME: Code duplication: vantage6/cli/node.py uses a lot of the same\n        #   logic. Suggest moving this to ctx.get_private_key()\n        filename = self.config['encryption'][\"private_key\"]\n\n        # filename may be set to an empty string\n        if not filename:\n            filename = 'private_key.pem'\n\n        # If we're running dockerized, the location may have been overridden\n        filename = os.environ.get('PRIVATE_KEY', filename)\n\n        # If ctx.get_data_file() receives an absolute path, its returned as-is\n        fullpath = Path(self.ctx.get_data_file(filename))\n\n        return fullpath\n\n    def setup_encryption(self) -> None:\n        \"\"\" Setup encryption if the node is part of encrypted collaboration \"\"\"\n        encrypted_collaboration = self.server_io.is_encrypted_collaboration()\n        encrypted_node = self.config['encryption'][\"enabled\"]\n\n        if encrypted_collaboration != encrypted_node:\n            # You can't force it if it just ain't right, you know?\n            raise Exception(\"Expectations on encryption don't match?!\")\n\n        if encrypted_collaboration:\n            self.log.warn('Enabling encryption!')\n            private_key_file = self.private_key_filename()\n            self.server_io.setup_encryption(private_key_file)\n\n        else:\n            self.log.warn('Disabling encryption!')\n            self.server_io.setup_encryption(None)\n\n    def _set_task_dir(self, ctx) -> None:\n        \"\"\"\n        Set the task dir\n\n        Parameters\n        ----------\n        ctx: DockerNodeContext or NodeContext\n            Context object containing settings\n        \"\"\"\n        # If we're in a 'regular' context, we'll copy the dataset to our data\n        # dir and mount it in any algorithm container that's run; bind mounts\n        # on a folder will work just fine.\n        #\n        # If we're running in dockerized mode we *cannot* bind mount a folder,\n        # because the folder is in the container and not in the host. We'll\n        # have to use a docker volume instead. This means:\n        #  1. we need to know the name of the volume so we can pass it along\n        #  2. need to have this volume mounted so we can copy files to it.\n        #\n        #  Ad 1: We'll use a default name that can be overridden by an\n        #        environment variable.\n        #  Ad 2: We'll expect `ctx.data_dir` to point to the right place. This\n        #        is OK, since ctx will be a DockerNodeContext.\n        #\n        #  This also means that the volume will have to be created & mounted\n        #  *before* this node is started, so we won't do anything with it here.\n\n        # We'll create a subfolder in the data_dir. We need this subfolder so\n        # we can easily mount it in the algorithm containers; the root folder\n        # may contain the private key, which which we don't want to share.\n        # We'll only do this if we're running outside docker, otherwise we\n        # would create '/data' on the data volume.\n        if not ctx.running_in_docker:\n            self.__tasks_dir = ctx.data_dir / 'data'\n            os.makedirs(self.__tasks_dir, exist_ok=True)\n            self.__vpn_dir = ctx.data_dir / 'vpn'\n            os.makedirs(self.__vpn_dir, exist_ok=True)\n        else:\n            self.__tasks_dir = ctx.data_dir\n            self.__vpn_dir = ctx.vpn_dir\n\n    def setup_ssh_tunnels(self, isolated_network_mgr: Type[NetworkManager]) \\\n            -> List[SSHTunnel]:\n        \"\"\"\n        Create a SSH tunnels when they are defined in the configuration file.\n        For each tunnel a new container is created. The image used can be\n        specified in the configuration file as `ssh-tunnel` in the `images`\n        section, else the default image is used.\n\n        Parameters\n        ----------\n        isolated_network_mgr: NetworkManager\n            Manager for the isolated network\n        \"\"\"\n        if 'ssh-tunnels' not in self.config:\n            self.log.info(\"No SSH tunnels configured\")\n            return\n\n        custom_tunnel_image = self.config.get('images', {}).get('ssh-tunnel') \\\n            if 'images' in self.config else None\n\n        configs = self.config['ssh-tunnels']\n        self.log.info(f\"Setting up {len(configs)} SSH tunnels\")\n\n        tunnels: List[SSHTunnel] = []\n        for config in configs:\n            self.log.debug(f\"SSH tunnel config: {config}\")\n\n            # copy (rename) the ssh key to the correct name, this is done so\n            # that the file is in the volume (somehow we can not file mount\n            # within a volume)\n            if self.ctx.running_in_docker:\n                ssh_key = f\"/mnt/ssh/{config['hostname']}.pem.tmp\"\n                key_path = shutil.copy(ssh_key,\n                                       f\"/mnt/ssh/{config['hostname']}.pem\")\n                volume = self.ctx.docker_ssh_volume_name\n\n            else:\n                ssh_key = config['ssh']['identity']['key']\n\n                volume = str(Path(ssh_key).parent)\n                key_path = shutil.copy(ssh_key,\n                                       f\"{volume}/{config['hostname']}.pem\")\n\n            os.chmod(key_path, 0o600)\n\n            try:\n                new_tunnel = SSHTunnel(isolated_network_mgr, config,\n                                       self.ctx.name, volume,\n                                       custom_tunnel_image)\n            except Exception as e:\n                self.log.error(\"Error setting up SSH tunnel\")\n                self.log.debug(e, exc_info=True)\n                continue\n\n            tunnels.append(new_tunnel)\n\n        return tunnels\n\n    def setup_vpn_connection(self, isolated_network_mgr: NetworkManager,\n                             ctx: Union[DockerNodeContext, NodeContext]\n                             ) -> VPNManager:\n        \"\"\"\n        Setup container which has a VPN connection\n\n        Parameters\n        ----------\n        isolated_network_mgr: NetworkManager\n            Manager for the isolated Docker network\n        ctx: NodeContext\n            Context object for the node\n\n        Returns\n        -------\n        VPNManager\n            Manages the VPN connection\n        \"\"\"\n        ovpn_file = os.path.join(self.__vpn_dir, VPN_CONFIG_FILE)\n\n        self.log.info(\"Setting up VPN client container\")\n        vpn_volume_name = self.ctx.docker_vpn_volume_name \\\n            if ctx.running_in_docker else self.__vpn_dir\n\n        # FIXME: remove me in 4+. alpine image has been moved into the `images`\n        # key. This is to support older configuration files.\n        legacy_alpine = self.config.get('alpine')\n\n        # user can specify custom images in the configuration file\n        custom_alpine = self.config['images'].get('alpine') \\\n            if 'images' in self.config else None\n        custom_vpn_client = self.config['images'].get('vpn_client') \\\n            if 'images' in self.config else None\n        custom_network = self.config['images'].get('network_config') \\\n            if 'images' in self.config else None\n\n        vpn_manager = VPNManager(\n            isolated_network_mgr=isolated_network_mgr,\n            node_name=self.ctx.name,\n            vpn_volume_name=vpn_volume_name,\n            vpn_subnet=self.config.get('vpn_subnet'),\n            alpine_image=custom_alpine or legacy_alpine,\n            vpn_client_image=custom_vpn_client,\n            network_config_image=custom_network\n        )\n\n        if not self.config.get('vpn_subnet'):\n            self.log.warn(\"VPN subnet is not defined! VPN disabled.\")\n        elif not os.path.isfile(ovpn_file):\n            # if vpn config doesn't exist, get it and write to disk\n            self._connect_vpn(vpn_manager, VPNConnectMode.REFRESH_COMPLETE,\n                              ovpn_file)\n        else:\n            self._connect_vpn(vpn_manager, VPNConnectMode.FIRST_TRY, ovpn_file)\n\n        return vpn_manager\n\n    def _connect_vpn(self, vpn_manager: VPNManager,\n                     connect_mode: VPNConnectMode, ovpn_file: str) -> None:\n        \"\"\"\n        Connect to the VPN by starting up a VPN client container. If no VPN\n        config file exists, we only try once after first obtaining a config\n        file. If a VPN config file already exists, we first try to connect,\n        then try to refresh the keypair, and finally try to renew the entire\n        config file, until a connection is established.\n\n        Parameters\n        ----------\n        vpn_manager: VPNManager\n            Manages the VPN connection\n        connect_mode: VPNConnectMode\n            Specifies which parts of a config file to refresh before attempting\n            to connect\n        ovpn_file: str\n            Path to the VPN configuration file\n        \"\"\"\n        do_try = True\n        if connect_mode == VPNConnectMode.FIRST_TRY:\n            self.log.debug(\"Using existing config file to connect to VPN\")\n            next_mode = VPNConnectMode.REFRESH_KEYPAIR\n        elif connect_mode == VPNConnectMode.REFRESH_KEYPAIR:\n            self.log.debug(\"Refreshing VPN keypair...\")\n            do_try = self.server_io.refresh_vpn_keypair(ovpn_file=ovpn_file)\n            next_mode = VPNConnectMode.REFRESH_COMPLETE\n        elif connect_mode == VPNConnectMode.REFRESH_COMPLETE:\n            self.log.debug(\"Requesting new VPN configuration file...\")\n            do_try = self._get_vpn_config_file(ovpn_file)\n            next_mode = None  # if new config file doesn't work, give up\n\n        if do_try:\n            # try connecting to VPN\n            try:\n                vpn_manager.connect_vpn()\n            except Exception as e:\n                self.log.debug(\"Could not connect to VPN.\")\n                self.log.debug(f\"Exception: {e}\")\n                # try again in another fashion\n                if next_mode:\n                    self._connect_vpn(vpn_manager, next_mode, ovpn_file)\n\n    def _get_vpn_config_file(self, ovpn_file: str) -> bool:\n        \"\"\"\n        Obtain VPN configuration file from the server\n\n        Parameters\n        ----------\n        ovpn_file: str\n            Path to the VPN configuration file\n\n        Returns\n        -------\n        bool\n            Whether or not configuration file was successfully obtained\n        \"\"\"\n        # get the ovpn configuration from the server\n        success, ovpn_config = self.server_io.get_vpn_config()\n        if not success:\n            self.log.warn(\"Obtaining VPN configuration file not successful!\")\n            self.log.warn(\"Disabling node-to-node communication via VPN\")\n            return False\n\n        # write ovpn config to node docker volume\n        with open(ovpn_file, 'w') as f:\n            f.write(ovpn_config)\n        return True\n\n    def link_docker_services(self) -> None:\n        docker_services = self.ctx.config.get(\"docker_services\")\n        if not docker_services:\n            return\n        self.log.info(\"Linking docker services specified in the configuration\")\n        for alias, container_name in docker_services.items():\n            self.__docker.link_container_to_network(\n                container_name=container_name, config_alias=alias\n            )\n\n    def connect_to_socket(self) -> None:\n        \"\"\"\n        Create long-lasting websocket connection with the server. The\n        connection is used to receive status updates, such as new tasks.\n        \"\"\"\n        self.socketIO = SocketIO(request_timeout=60)\n\n        self.socketIO.register_namespace(NodeTaskNamespace('/tasks'))\n        NodeTaskNamespace.node_worker_ref = self\n\n        self.socketIO.connect(\n            url=f'{self.server_io.host}:{self.server_io.port}',\n            headers=self.server_io.headers,\n            wait=False\n        )\n\n        # Log the outcome\n        i = 0\n        while not self.socketIO.connected:\n            if i > TIME_LIMIT_INITIAL_CONNECTION_WEBSOCKET:\n                self.log.critical('Could not connect to the websocket '\n                                  'channels, do you have a slow connection?')\n                exit(1)\n            self.log.debug('Waiting for socket connection...')\n            time.sleep(1)\n            i += 1\n\n        self.log.info(f'Connected to host={self.server_io.host} on port='\n                      f'{self.server_io.port}')\n\n    def get_task_and_add_to_queue(self, task_id: int) -> None:\n        \"\"\"\n        Fetches (open) task with task_id from the server. The `task_id` is\n        delivered by the websocket-connection.\n\n        Parameters\n        ----------\n        task_id : int\n            Task identifier\n        \"\"\"\n        # fetch (open) result for the node with the task_id\n        tasks = self.server_io.get_results(\n            include_task=True,\n            state='open',\n            task_id=task_id\n        )\n\n        # in the current setup, only a single result for a single node\n        # in a task exists.\n        for task in tasks:\n            self.queue.put(task)\n\n    def run_forever(self) -> None:\n        \"\"\"Keep checking queue for incoming tasks (and execute them).\"\"\"\n        kill_listener = ContainerKillListener()\n        try:\n            while True:\n                # blocking untill a task comes available\n                # timeout specified, else Keyboard interupts are ignored\n                self.log.info(\"Waiting for new tasks....\")\n\n                while not kill_listener.kill_now:\n                    try:\n                        task = self.queue.get(timeout=1)\n                        # if no item is returned, the Empty exception is\n                        # triggered, thus break statement is not reached\n                        break\n\n                    except queue.Empty:\n                        pass\n\n                    except Exception as e:\n                        self.log.debug(e)\n\n                if kill_listener.kill_now:\n                    raise InterruptedError\n\n                # if task comes available, attempt to execute it\n                try:\n                    self.__start_task(task)\n                except Exception as e:\n                    self.log.exception(e)\n\n        except (KeyboardInterrupt, InterruptedError):\n            self.log.info(\"Vnode is interrupted, shutting down...\")\n            self.cleanup()\n            sys.exit()\n\n    def kill_containers(self, kill_info: Dict) -> List[Dict]:\n        \"\"\"\n        Kill containers on instruction from socket event\n\n        Parameters\n        ----------\n        kill_info: Dict\n            Dictionary received over websocket with instructions for which\n            tasks to kill\n\n        Returns\n        -------\n        List[Dict]:\n            List of dictionaries with information on killed task (keys:\n            result_id, task_id and parent_id)\n        \"\"\"\n        if kill_info['collaboration_id'] != self.server_io.collaboration_id:\n            self.log.debug(\n                \"Not killing tasks as this node is in another collaboration.\"\n            )\n            return []\n        elif 'node_id' in kill_info and \\\n                kill_info['node_id'] != self.server_io.whoami.id_:\n            self.log.debug(\n                \"Not killing tasks as instructions to kill tasks were directed\"\n                \" at another node in this collaboration.\")\n            return []\n\n        # kill specific task if specified, else kill all algorithms\n        kill_list = kill_info.get('kill_list')\n        killed_algos = self.__docker.kill_tasks(\n            org_id=self.server_io.whoami.organization_id, kill_list=kill_list\n        )\n        # update status of killed tasks\n        for killed_algo in killed_algos:\n            self.server_io.patch_results(\n                id=killed_algo.result_id, result={'status': TaskStatus.KILLED}\n            )\n        return killed_algos\n\n    def share_node_details(self) -> None:\n        \"\"\"\n        Share part of the node's configuration with the server.\n\n        This helps the other parties in a collaboration to see e.g. which\n        algorithms they are allowed to run on this node.\n        \"\"\"\n        # check if node allows to share node details, otherwise return\n        if not self.config.get('share_config', True):\n            self.log.debug(\"Not sharing node configuration in accordance with \"\n                           \"the configuration setting.\")\n            return\n\n        config_to_share = {}\n\n        encryption_config = self.config.get('encryption')\n        if encryption_config:\n            if encryption_config.get('enabled') is not None:\n                config_to_share['encryption'] = \\\n                    encryption_config.get('enabled')\n\n        allowed_algos = self.config.get('allowed_images')\n        config_to_share['allowed_images'] = allowed_algos if allowed_algos \\\n            else 'all'\n\n        self.log.debug(f\"Sharing node configuration: {config_to_share}\")\n        self.socketIO.emit(\n            'node_info_update', config_to_share, namespace='/tasks'\n        )\n\n    def cleanup(self) -> None:\n\n        if hasattr(self, 'socketIO') and self.socketIO:\n            self.socketIO.disconnect()\n        if hasattr(self, 'vpn_manager') and self.vpn_manager:\n            self.vpn_manager.exit_vpn()\n        if hasattr(self, 'ssh_tunnels') and self.ssh_tunnels:\n            for tunnel in self.ssh_tunnels:\n                tunnel.stop()\n        if hasattr(self, '_Node__docker') and self.__docker:\n            self.__docker.cleanup()\n\n        self.log.info(\"Bye!\")\n\n\n# ------------------------------------------------------------------------------\ndef run(ctx):\n    \"\"\" Start the node.\"\"\"\n    logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n    logging.getLogger(\"requests\").setLevel(logging.WARNING)\n    logging.getLogger(\"engineio.client\").setLevel(logging.WARNING)\n\n    # initialize node, connect to the server using websockets\n    node = Node(ctx)\n\n    # put the node to work, executing tasks that are in the que\n    node.run_forever()\n", "from pathlib import Path\nfrom vantage6.common.globals import APPNAME\n\n#\n#   NODE SETTINGS\n#\nDEFAULT_NODE_SYSTEM_FOLDERS = False\n\nDEFAULT_NODE_ENVIRONMENT = \"application\"\n\n\n#\n#   INSTALLATION SETTINGS\n#\nPACKAGE_FOLDER = Path(__file__).parent.parent.parent\n\nNODE_PROXY_SERVER_HOSTNAME = \"proxyserver\"\n\nDATA_FOLDER = PACKAGE_FOLDER / APPNAME / \"_data\"\n\n# with open(Path(PACKAGE_FOLDER) / APPNAME / \"node\" / \"VERSION\") as f:\n#     VERSION = f.read()\n\n\n# constants for retrying node login\nSLEEP_BTWN_NODE_LOGIN_TRIES = 10  # retry every 10s\nTIME_LIMIT_RETRY_CONNECT_NODE = 60 * 60 * 24 * 7  # i.e. 1 week\n\n# constant for waiting for the initial websocket connection\nTIME_LIMIT_INITIAL_CONNECTION_WEBSOCKET = 60\n\n#\n#    VPN CONFIGURATION RELATED CONSTANTS\n#\n# TODO move part of these constants elsewhere?! Or make context?\nVPN_CLIENT_IMAGE = 'harbor2.vantage6.ai/infrastructure/vpn-client'\nNETWORK_CONFIG_IMAGE = 'harbor2.vantage6.ai/infrastructure/vpn-configurator'\nALPINE_IMAGE = 'harbor2.vantage6.ai/infrastructure/alpine'\nMAX_CHECK_VPN_ATTEMPTS = 60   # max attempts to obtain VPN IP (1 second apart)\nFREE_PORT_RANGE = range(49152, 65535)\nDEFAULT_ALGO_VPN_PORT = '8888'  # default VPN port for algorithm container\n\n#\n#   SSH TUNNEL RELATED CONSTANTS\n#\nSSH_TUNNEL_IMAGE = \"harbor2.vantage6.ai/infrastructure/ssh-tunnel\"\n\n# start trying to refresh the JWT token 10 minutes before it expires.\nREFRESH_BEFORE_EXPIRES_SECONDS = 600\n", "\"\"\"\nThis module provides a client interface for the node to communicate with the\ncentral server.\n\"\"\"\nimport jwt\nimport datetime\nimport time\n\nfrom typing import Dict, Tuple\nfrom threading import Thread\n\nfrom vantage6.common import WhoAmI\nfrom vantage6.client import ClientBase\nfrom vantage6.node.globals import REFRESH_BEFORE_EXPIRES_SECONDS\n\n\nclass NodeClient(ClientBase):\n    \"\"\" Node interface to the central server.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # self.name = None\n        self.collaboration_id = None\n        self.whoami = None\n\n    def authenticate(self, api_key: str):\n        \"\"\" Nodes authentication at the central server.\n\n            It also identifies itself by retrieving the collaboration\n            and organization to which this node belongs. The server\n            returns a JWT-token that is used in all succeeding requests.\n\n            :param api_key: api-key used to authenticate to the central\n                server\n        \"\"\"\n        super().authenticate({\"api_key\": api_key}, path=\"token/node\")\n\n        # obtain the server authenticatable id\n        jwt_payload = jwt.decode(self.token,\n                                 options={\"verify_signature\": False})\n\n        # FIXME: 'identity' is no longer needed in version 4+. So this if\n        # statement can be removed\n        if 'sub' in jwt_payload:\n            id_ = jwt_payload['sub']\n        elif 'identity' in jwt_payload:\n            id_ = jwt_payload['identity']\n\n        # get info on how the server sees me\n        node = self.request(f\"node/{id_}\")\n\n        name = node.get(\"name\")\n        self.collaboration_id = node.get(\"collaboration\").get(\"id\")\n\n        organization_id = node.get(\"organization\").get(\"id\")\n        organization = self.request(f\"organization/{organization_id}\")\n        organization_name = organization.get(\"name\")\n\n        self.whoami = WhoAmI(\n            type_=\"node\",\n            id_=id_,\n            name=name,\n            organization_id=organization_id,\n            organization_name=organization_name\n        )\n\n    def auto_refresh_token(self) -> None:\n        \"\"\" Start a thread that refreshes token before it expires. \"\"\"\n        # set up thread to refresh token\n        t = Thread(target=self.__refresh_token_worker, daemon=True)\n        t.start()\n\n    def __refresh_token_worker(self) -> None:\n        \"\"\" Keep refreshing token to prevent it from expiring. \"\"\"\n        while True:\n            # get the time until the token expires\n            expiry_time = jwt.decode(\n                self.token, options={\"verify_signature\": False})[\"exp\"]\n            time_until_expiry = expiry_time - time.time()\n            if time_until_expiry < REFRESH_BEFORE_EXPIRES_SECONDS:\n                self.refresh_token()\n            else:\n                time.sleep(\n                    int(time_until_expiry - REFRESH_BEFORE_EXPIRES_SECONDS + 1)\n                )\n\n    def request_token_for_container(self, task_id: int, image: str):\n        \"\"\" Request a container-token at the central server.\n\n            This token is used by algorithm containers that run on this\n            node. These algorithms can then post tasks and retrieve\n            child-results (usually refered to as a master container).\n            The server performs a few checks (e.g. if the task you\n            request the key for is still open) before handing out this\n            token.\n\n            :param task_id: id from the task, which is going to use this\n                container-token (a task results in a algorithm-\n                container at the node)\n            :param image: image-name of the task\n        \"\"\"\n        self.log.debug(\n            f\"requesting container token for task_id={task_id} \"\n            f\"and image={image}\"\n        )\n        return self.request('/token/container', method=\"post\", json={\n            \"task_id\": task_id,\n            \"image\": image\n        })\n\n    def get_results(self, id=None, state=None, include_task=False,\n                    task_id=None):\n        \"\"\" Obtain the results for a specific task.\n\n            Overload the definition of the parent by entering the\n            task_id automatically.\n        \"\"\"\n        return super().get_results(\n            id=id,\n            state=state,\n            include_task=include_task,\n            task_id=task_id,\n            node_id=self.whoami.id_\n        )\n\n    def is_encrypted_collaboration(self):\n        \"\"\" Boolean whenever the encryption is enabled.\n\n            End-to-end encryption is per collaboration managed at the\n            central server. It is important to note that the local\n            configuration-file should allow explicitly for unencrpyted\n            messages. This function returns the setting from the server.\n        \"\"\"\n        response = self.request(f\"collaboration/{self.collaboration_id}\")\n        return response.get(\"encrypted\") == 1\n\n    def set_task_start_time(self, id: int):\n        \"\"\" Sets the start time of the task at the central server.\n\n            This is important as this will note that the task has been\n            started, and is waiting for restuls.\n\n            :param id: id of the task to set the start-time of\n\n            TODO the initiator_id does not make sens here...\n        \"\"\"\n        self.patch_results(id, result={\n            \"started_at\": datetime.datetime.now().isoformat()\n        })\n\n    def patch_results(self, id: int, result: Dict,\n                      init_org_id: int = None) -> None:\n        \"\"\"\n        Update the results at the central server.\n\n        Typically used when to algorithm container is finished or\n        when a status-update is posted (started, finished)\n\n        Parameters\n        ----------\n        id: int\n            ID of the result to patch\n        result: Dict\n            Dictionary of fields that are to be patched\n        init_org_id: int, optional\n            Organization id of the origin of the task. This is required\n            when the result dict includes results, because then results have\n            to be encrypted specifically for them\n        \"\"\"\n        # TODO: the key `result` is not always present, e.g. when\n        #     only the timestamps are updated\n        # FIXME: public keys should be cached\n        if \"result\" in result:\n            if not init_org_id:\n                self.log.critical(\n                    \"Organization id is not provided: cannot send results to \"\n                    \"server as they cannot be encrypted\")\n            msg = f\"Retrieving public key from organization={init_org_id}\"\n            self.log.debug(msg)\n\n            org = self.request(f\"organization/{init_org_id}\")\n            public_key = None\n            try:\n                public_key = org[\"public_key\"]\n            except KeyError:\n                self.log.critical('Public key could not be retrieved...')\n                self.log.critical('Does the initiating organization belong to '\n                                  'your organization?')\n\n            result[\"result\"] = self.cryptor.encrypt_bytes_to_str(\n                result[\"result\"],\n                public_key\n            )\n\n            self.log.debug(\"Sending results to server\")\n        else:\n            self.log.debug(\"Just patchin'\")\n\n        return self.request(f\"result/{id}\", json=result, method='patch')\n\n    def get_vpn_config(self) -> Tuple[bool, str]:\n        \"\"\"\n        Obtain VPN configuration from the server\n\n        Returns\n        -------\n        bool\n            Whether or not obtaining VPN config was successful\n        str\n            OVPN configuration file content\n        \"\"\"\n        response = self.request(\"vpn\")\n\n        ovpn_config = response.get(\"ovpn_config\")\n        if ovpn_config is None:\n            return False, ''\n\n        # replace windows line endings to linux style to prevent extra\n        # whitespace in writing the file\n        ovpn_config = ovpn_config.replace(\"\\r\\n\", \"\\n\")\n\n        return True, ovpn_config\n\n    def refresh_vpn_keypair(self, ovpn_file: str) -> bool:\n        \"\"\"\n        Refresh the client's keypair in an ovpn configuration file\n\n        Parameters\n        ----------\n        ovpn_file: str\n            The path to the current ovpn configuration on disk\n        \"\"\"\n        # Extract the contents of the VPN file\n        with open(ovpn_file, 'r') as file:\n            ovpn_config = file.read()\n\n        response = self.request(\n            \"vpn/update\",\n            method=\"POST\",\n            json={'vpn_config': ovpn_config},\n        )\n        ovpn_config = response.get(\"ovpn_config\")\n        if not ovpn_config:\n            self.log.warn(\"Refreshing VPN keypair not successful!\")\n            self.log.warn(\"Disabling node-to-node communication via VPN\")\n            return False\n\n        # write new configuration back to file\n        with open(ovpn_file, 'w') as f:\n            f.write(ovpn_config)\n        return True\n", "# -*- coding: utf-8 -*-\nfrom gevent import monkey\n\n# flake8: noqa: E402 (ignore import error)\nmonkey.patch_all()\n\nimport importlib\nimport logging\nimport os\nimport uuid\nimport json\nimport time\nimport datetime as dt\nimport traceback\n\nfrom http import HTTPStatus\nfrom werkzeug.exceptions import HTTPException\nfrom flasgger import Swagger\nfrom flask import (\n    Flask, make_response, current_app, request, send_from_directory, Request\n)\nfrom flask_cors import CORS\nfrom flask_jwt_extended import JWTManager\nfrom flask_marshmallow import Marshmallow\nfrom flask_restful import Api\nfrom flask_mail import Mail\nfrom flask_principal import Principal, Identity, identity_changed\nfrom flask_socketio import SocketIO\nfrom threading import Thread\n\nfrom vantage6.server import db\nfrom vantage6.cli.context import ServerContext\nfrom vantage6.server.model.base import DatabaseSessionManager, Database\nfrom vantage6.server.resource.common._schema import HATEOASModelSchema\nfrom vantage6.common import logger_name\nfrom vantage6.server.permission import RuleNeed, PermissionManager\nfrom vantage6.server.globals import (\n    APPNAME,\n    ACCESS_TOKEN_EXPIRES_HOURS,\n    JWT_TEST_ACCESS_TOKEN_EXPIRES,\n    RESOURCES,\n    SUPER_USER_INFO,\n    REFRESH_TOKENS_EXPIRE_HOURS,\n    DEFAULT_SUPPORT_EMAIL_ADDRESS,\n    MAX_RESPONSE_TIME_PING,\n    MIN_TOKEN_VALIDITY_SECONDS,\n    MIN_REFRESH_TOKEN_EXPIRY_DELTA,\n)\nfrom vantage6.server.resource.common.swagger_templates import swagger_template\nfrom vantage6.server._version import __version__\nfrom vantage6.server.mail_service import MailService\nfrom vantage6.server.websockets import DefaultSocketNamespace\nfrom vantage6.server.default_roles import get_default_roles, DefaultRole\n\n\nmodule_name = logger_name(__name__)\nlog = logging.getLogger(module_name)\n\n\nclass ServerApp:\n    \"\"\"Vantage6 server instance.\"\"\"\n\n    def __init__(self, ctx):\n        \"\"\"Create a vantage6-server application.\"\"\"\n\n        self.ctx = ctx\n\n        # initialize, configure Flask\n        self.app = Flask(APPNAME, root_path=os.path.dirname(__file__))\n        self.configure_flask()\n\n        # Setup SQLAlchemy and Marshmallow for marshalling/serializing\n        self.ma = Marshmallow(self.app)\n\n        # Setup the Flask-JWT-Extended extension (JWT: JSON Web Token)\n        self.jwt = JWTManager(self.app)\n        self.configure_jwt()\n\n        # Setup Principal, granular API access manegement\n        self.principal = Principal(self.app, use_sessions=False)\n\n        # Enable cross-origin resource sharing\n        self.cors = CORS(self.app)\n\n        # SWAGGER documentation\n        self.swagger = Swagger(self.app, template=swagger_template)\n\n        # Setup the Flask-Mail client\n        self.mail = MailService(self.app, Mail(self.app))\n\n        # Setup websocket channel\n        self.socketio = self.setup_socket_connection()\n\n        # setup the permission manager for the API endpoints\n        self.permissions = PermissionManager()\n\n        # Api - REST JSON-rpc\n        self.api = Api(self.app)\n        self.configure_api()\n        self.load_resources()\n\n        # make specific log settings (muting etc)\n        self.configure_logging()\n\n        # set the server version\n        self.__version__ = __version__\n\n        # set up socket ping/pong\n        log.debug(\n            \"Starting thread for socket ping/pong between server and nodes\")\n        self.socketio.start_background_task(self.__socket_pingpong_worker)\n\n        log.info(\"Initialization done\")\n\n    def setup_socket_connection(self):\n\n        msg_queue = self.ctx.config.get('rabbitmq_uri')\n        if msg_queue:\n            log.debug(f'Connecting to msg queue: {msg_queue}')\n\n        try:\n            socketio = SocketIO(\n                self.app,\n                async_mode='gevent_uwsgi',\n                message_queue=msg_queue,\n                ping_timeout=60,\n                cors_allowed_origins='*'\n            )\n        except Exception as e:\n            log.warning('Default socketio settings failed, attempt to run '\n                        'without gevent_uwsgi packages! This leads to '\n                        'performance issues and possible issues concerning '\n                        'the websocket channels!')\n            log.debug(e)\n            socketio = SocketIO(\n                self.app,\n                message_queue=msg_queue,\n                ping_timeout=60,\n                cors_allowed_origins='*'\n            )\n\n        # FIXME: temporary fix to get socket object into the namespace class\n        DefaultSocketNamespace.socketio = socketio\n        socketio.on_namespace(DefaultSocketNamespace(\"/tasks\"))\n\n        return socketio\n\n    @staticmethod\n    def configure_logging():\n        \"\"\"Set third party loggers to a warning level\"\"\"\n\n        # Prevent logging from urllib3\n        logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n        logging.getLogger(\"socketIO-client\").setLevel(logging.WARNING)\n        logging.getLogger(\"engineio.server\").setLevel(logging.WARNING)\n        logging.getLogger(\"socketio.server\").setLevel(logging.WARNING)\n        logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)\n        logging.getLogger('requests_oauthlib.oauth2_session')\\\n            .setLevel(logging.WARNING)\n\n    def configure_flask(self):\n        \"\"\"All flask config settings should go here.\"\"\"\n\n        # let us handle exceptions\n        self.app.config['PROPAGATE_EXCEPTIONS'] = True\n\n        # patch where to obtain token\n        self.app.config['JWT_AUTH_URL_RULE'] = '/api/token'\n\n        # If no secret is set in the config file, one is generated. This\n        # implies that all (even refresh) tokens will be invalidated on restart\n        self.app.config['JWT_SECRET_KEY'] = self.ctx.config.get(\n            'jwt_secret_key',\n            str(uuid.uuid1())\n        )\n\n        # Default expiration time\n        token_expiry_seconds = self._get_jwt_expiration_seconds(\n            config_key='token_expires_hours',\n            default_hours=ACCESS_TOKEN_EXPIRES_HOURS\n        )\n        self.app.config['JWT_ACCESS_TOKEN_EXPIRES'] = token_expiry_seconds\n\n        # Set refresh token expiration time\n        self.app.config['JWT_REFRESH_TOKEN_EXPIRES'] = \\\n                self._get_jwt_expiration_seconds(\n            config_key='refresh_token_expires_hours',\n            default_hours=REFRESH_TOKENS_EXPIRE_HOURS,\n            longer_than=token_expiry_seconds + MIN_REFRESH_TOKEN_EXPIRY_DELTA,\n            is_refresh=True\n        )\n\n        # Set an extra long expiration time on access tokens for testing\n        # TODO: this does not seem needed...\n        environment = self.ctx.config.get('type')\n        self.app.config['environment'] = environment\n        if environment == 'test':\n            log.warning(\"Setting 'JWT_ACCESS_TOKEN_EXPIRES' to one day!\")\n            self.app.config['JWT_ACCESS_TOKEN_EXPIRES'] = \\\n                JWT_TEST_ACCESS_TOKEN_EXPIRES\n\n        # Open Api Specification (f.k.a. swagger)\n        self.app.config['SWAGGER'] = {\n            'title': APPNAME,\n            'uiversion': \"3\",\n            'openapi': '3.0.0',\n            'version': __version__\n        }\n\n        # Mail settings\n        mail_config = self.ctx.config.get(\"smtp\", {})\n        self.app.config[\"MAIL_PORT\"] = mail_config.get(\"port\", 1025)\n        self.app.config[\"MAIL_SERVER\"] = mail_config.get(\"server\", \"localhost\")\n        self.app.config[\"MAIL_USERNAME\"] = mail_config.get(\n            \"username\",\n            DEFAULT_SUPPORT_EMAIL_ADDRESS\n        )\n        self.app.config[\"MAIL_PASSWORD\"] = mail_config.get(\"password\", \"\")\n        self.app.config[\"MAIL_USE_TLS\"] = mail_config.get(\"MAIL_USE_TLS\",\n                                                          True)\n        self.app.config[\"MAIL_USE_SSL\"] = mail_config.get(\"MAIL_USE_SSL\",\n                                                          False)\n\n        def _get_request_path(request: Request) -> str:\n            \"\"\"\n            Return request extension of request URL, e.g.\n            http://localhost:5000/api/task/1 -> api/task/1\n\n            Parameters\n            ----------\n            request: Request\n                Flask request object\n\n            Returns\n            -------\n            string:\n                The endpoint path of the request\n            \"\"\"\n            return request.url.replace(request.url_root, '')\n\n        # before request\n        @self.app.before_request\n        def do_before_request():\n            \"\"\"Before every flask request method.\"\"\"\n            # Add log message before each request\n            log.debug(f\"Received request: {request.method} \"\n                      f\"{_get_request_path(request)}\")\n\n            # This will obtain a (scoped) db session from the session factory\n            # that is linked to the flask request global `g`. In every endpoint\n            # we then can access the database by using this session. We ensure\n            # that the session is removed (and uncommited changes are rolled\n            # back) at the end of every request.\n            DatabaseSessionManager.new_session()\n\n        @self.app.after_request\n        def remove_db_session(response):\n            \"\"\"After every flask request.\n\n            This will close the database session created by the\n            `before_request`.\n            \"\"\"\n            DatabaseSessionManager.clear_session()\n            return response\n\n        @self.app.errorhandler(HTTPException)\n        def error_remove_db_session(error: HTTPException):\n            \"\"\"In case an HTTP-exception occurs during the request.\n\n            It is important to close the db session to avoid having dangling\n            sessions.\n            \"\"\"\n            if error.code == 404:\n                log.debug(\n                    f\"404 error for route '{_get_request_path(request)}'\")\n            else:\n                log.warn('HTTP Exception occured during request')\n                log.debug(traceback.format_exc())\n            DatabaseSessionManager.clear_session()\n            return error.get_response()\n\n        @self.app.errorhandler(Exception)\n        def error2_remove_db_session(error):\n            \"\"\"In case an exception occurs during the request.\n\n            It is important to close the db session to avoid having dangling\n            sessions.\n            \"\"\"\n            log.exception('Exception occured during request')\n            DatabaseSessionManager.clear_session()\n            return {'msg': f'An unexpected error occurred on the server!'}, \\\n                HTTPStatus.INTERNAL_SERVER_ERROR\n\n        @self.app.route('/robots.txt')\n        def static_from_root():\n            return send_from_directory(self.app.static_folder,\n                                       request.path[1:])\n\n\n    def _get_jwt_expiration_seconds(\n        self, config_key: str, default_hours: int,\n        longer_than: int = MIN_TOKEN_VALIDITY_SECONDS,\n        is_refresh: bool = False\n    ) -> int:\n        \"\"\"\n        Return the expiration time for JWT tokens.\n\n        This time may be specified in the config file. If it is not, the\n        default value is returned.\n\n        Parameters\n        ----------\n        config_key: str\n            The config key to look for that sets the expiration time\n        default_hours: int\n            The default expiration time in hours\n        longer_than: int\n            The minimum expiration time in hours.\n        is_refresh: bool\n            If True, the expiration time is for a refresh token. If False, it\n            is for an access token.\n\n        Returns\n        -------\n        int:\n            The JWT token expiration time in seconds\n        \"\"\"\n        hours_expire = self.ctx.config.get(config_key)\n        if hours_expire is None:\n            # No value is present in the config file, use default\n            refresh_expire = int(float(default_hours) * 3600)\n        elif isinstance(hours_expire, (int, float)) or \\\n                hours_expire.is_numeric():\n            # Numeric value is present in the config file\n            refresh_expire = int(float(hours_expire) * 3600)\n            if refresh_expire < longer_than:\n                log.warning(\n                    f\"Invalid value for '{config_key}': {hours_expire}. Tokens\"\n                    f\" must be valid for at least {longer_than} seconds. Using\"\n                    f\" default value: {REFRESH_TOKENS_EXPIRE_HOURS} hours\")\n                if is_refresh:\n                    log.warning(\"Note that refresh tokens should be valid at \"\n                                f\"least {MIN_REFRESH_TOKEN_EXPIRY_DELTA} \"\n                                \"seconds longer than access tokens.\")\n                refresh_expire = int(float(REFRESH_TOKENS_EXPIRE_HOURS) * 3600)\n        else:\n            # Non-numeric value is present in the config file. Warn and use\n            # default\n            log.warning(\"Invalid value for 'refresh_token_expires_hours':\"\n                        f\" {hours_expire}. Using default value: \"\n                        f\"{REFRESH_TOKENS_EXPIRE_HOURS} hours\")\n            refresh_expire = int(float(REFRESH_TOKENS_EXPIRE_HOURS) * 3600)\n\n        return refresh_expire\n\n\n    def configure_api(self):\n        \"\"\"\"Define global API output.\"\"\"\n\n        # helper to create HATEOAS schemas\n        HATEOASModelSchema.api = self.api\n\n        # whatever you get try to json it\n        @self.api.representation('application/json')\n        def output_json(data, code, headers=None):\n\n            if isinstance(data, db.Base):\n                data = db.jsonable(data)\n            elif isinstance(data, list) and len(data) and \\\n                    isinstance(data[0], db.Base):\n                data = db.jsonable(data)\n\n            resp = make_response(json.dumps(data), code)\n            resp.headers.extend(headers or {})\n            return resp\n\n    def configure_jwt(self):\n        \"\"\"Load user and its claims.\"\"\"\n\n        @self.jwt.additional_claims_loader\n        def additional_claims_loader(identity):\n            roles = []\n            if isinstance(identity, db.User):\n                type_ = 'user'\n                roles = [role.name for role in identity.roles]\n\n            elif isinstance(identity, db.Node):\n                type_ = 'node'\n            elif isinstance(identity, dict):\n                type_ = 'container'\n            else:\n                log.error(f\"could not create claims from {str(identity)}\")\n                return\n\n            claims = {\n                'client_type': type_,\n                'roles': roles,\n            }\n\n            return claims\n\n        @self.jwt.user_identity_loader\n        def user_identity_loader(identity):\n            \"\"\"\"JSON serializing identity to be used by create_access_token.\"\"\"\n            if isinstance(identity, db.Authenticatable):\n                return identity.id\n            if isinstance(identity, dict):\n                return identity\n\n            log.error(f\"Could not create a JSON serializable identity \\\n                        from '{str(identity)}'\")\n\n        @self.jwt.user_lookup_loader\n        def user_lookup_loader(jwt_payload, jwt_headers):\n            identity = jwt_headers['sub']\n            auth_identity = Identity(identity)\n\n            # in case of a user or node an auth id is shared as identity\n            if isinstance(identity, int):\n\n                # auth_identity = Identity(identity)\n\n                auth = db.Authenticatable.get(identity)\n\n                if isinstance(auth, db.Node):\n\n                    for rule in db.Role.get_by_name(DefaultRole.NODE).rules:\n                        auth_identity.provides.add(\n                                RuleNeed(\n                                    name=rule.name,\n                                    scope=rule.scope,\n                                    operation=rule.operation\n                                )\n                            )\n\n                if isinstance(auth, db.User):\n\n                    # add role permissions\n                    for role in auth.roles:\n                        for rule in role.rules:\n                            auth_identity.provides.add(\n                                RuleNeed(\n                                    name=rule.name,\n                                    scope=rule.scope,\n                                    operation=rule.operation\n                                )\n                            )\n\n                    # add 'extra' permissions\n                    for rule in auth.rules:\n                        auth_identity.provides.add(\n                            RuleNeed(\n                                name=rule.name,\n                                scope=rule.scope,\n                                operation=rule.operation\n                            )\n                        )\n\n                identity_changed.send(current_app._get_current_object(),\n                                      identity=auth_identity)\n\n                return auth\n            else:\n\n                for rule in db.Role.get_by_name(DefaultRole.CONTAINER).rules:\n                    auth_identity.provides.add(\n                        RuleNeed(\n                            name=rule.name,\n                            scope=rule.scope,\n                            operation=rule.operation\n                        )\n                    )\n                identity_changed.send(current_app._get_current_object(),\n                                      identity=auth_identity)\n                log.debug(identity)\n                return identity\n\n    def load_resources(self):\n        \"\"\"Import the modules containing Resources.\"\"\"\n\n        # make services available to the endpoints, this way each endpoint can\n        # make use of 'em.\n        services = {\n            \"socketio\": self.socketio,\n            \"mail\": self.mail,\n            \"api\": self.api,\n            \"permissions\": self.permissions,\n            \"config\": self.ctx.config\n        }\n\n        for res in RESOURCES:\n            module = importlib.import_module('vantage6.server.resource.' + res)\n            module.setup(self.api, self.ctx.config['api_path'], services)\n\n    # TODO consider moving this method elsewhere. This is not trivial at the\n    # moment because of the circular import issue with `db`, see\n    # https://github.com/vantage6/vantage6/issues/53\n    @staticmethod\n    def _add_default_roles():\n        for role in get_default_roles(db):\n            if not db.Role.get_by_name(role['name']):\n                log.warn(f\"Creating new default role {role['name']}...\")\n                new_role = db.Role(\n                    name=role['name'],\n                    description=role['description'],\n                    rules=role['rules']\n                )\n                new_role.save()\n\n    def start(self):\n        \"\"\"Start the server.\n        \"\"\"\n\n        # add default roles (if they don't exist yet)\n        self._add_default_roles()\n\n        # create root user if it is not in the DB yet\n        try:\n            db.User.get_by_username(SUPER_USER_INFO['username'])\n        except Exception:\n            log.warn(\"No root user found! Is this the first run?\")\n\n            log.debug(\"Creating organization for root user\")\n            org = db.Organization(name=\"root\")\n\n            # TODO use constant instead of 'Root' literal\n            root = db.Role.get_by_name(\"Root\")\n\n            log.warn(f\"Creating root user: \"\n                     f\"username={SUPER_USER_INFO['username']}, \"\n                     f\"password={SUPER_USER_INFO['password']}\")\n\n            user = db.User(username=SUPER_USER_INFO['username'], roles=[root],\n                           organization=org, email=\"root@domain.ext\",\n                           password=SUPER_USER_INFO['password'],\n                           failed_login_attempts=0,\n                           last_login_attempt=None)\n            user.save()\n        return self\n\n    def __socket_pingpong_worker(self) -> None:\n        \"\"\"\n        Send ping messages periodically to nodes over the socketIO connection\n        and set node status online/offline depending on whether they respond\n        or not.\n        \"\"\"\n        # when starting up the server, wait a few seconds to allow nodes that\n        # are already online to connect back to the server (otherwise they\n        # would be incorrectly set to offline for one period)\n        time.sleep(5)\n\n        # start periodic check if nodes are responsive\n        while True:\n            # Send ping event\n            try:\n                ping_time = dt.datetime.utcnow()\n                self.socketio.emit(\n                    'ping', namespace='/tasks', room='all_nodes',\n                    callback=self.__pong_response\n                )\n\n                # Wait a while to give nodes opportunity to pong\n                time.sleep(MAX_RESPONSE_TIME_PING)\n\n                # Check for each node that is online if they have responded.\n                # Otherwise set them to offline.\n                online_status_nodes = db.Node.get_online_nodes()\n                for node in online_status_nodes:\n                    if node.last_seen < ping_time:\n                        node.status = 'offline'\n                        node.save()\n\n                # we need to sleep here for a bit to make sure that there is a\n                # delay between setting nodes offline and pinging again - this\n                # prevents a racing condition in setting status\n                time.sleep(5)\n            except Exception:\n                log.exception('Pingpong thread had an exception')\n                time.sleep(MAX_RESPONSE_TIME_PING)\n\n    def __pong_response(self, node_id) -> None:\n        node = db.Node.get(node_id)\n        node.status = 'online'\n        node.last_seen = dt.datetime.utcnow()\n        node.save()\n\n\ndef run_server(config: str, environment: str = 'prod',\n               system_folders: bool = True):\n    ctx = ServerContext.from_external_config_file(\n        config,\n        environment,\n        system_folders\n    )\n    allow_drop_all = ctx.config[\"allow_drop_all\"]\n    Database().connect(uri=ctx.get_database_uri(),\n                       allow_drop_all=allow_drop_all)\n    return ServerApp(ctx).start()\n\n\ndef run_dev_server(server_app: ServerApp, *args, **kwargs):\n    log.warn('*'*80)\n    log.warn(' DEVELOPMENT SERVER '.center(80, '*'))\n    log.warn('*'*80)\n    kwargs.setdefault('log_output', False)\n    server_app.socketio.run(server_app.app, *args, **kwargs)\n", "import datetime\n\nfrom pathlib import Path\n\nfrom vantage6.common.globals import APPNAME\n\n#\n#   INSTALLATION SETTINGS\n#\nPACKAGE_FOLDER = Path(__file__).parent.parent.parent\n\nDATA_FOLDER = PACKAGE_FOLDER / APPNAME / \"server\" / \"_data\"\n\n#\n#   RUNTIME SETTINGS\n#\n\n# Expiretime of JWT tokens\nACCESS_TOKEN_EXPIRES_HOURS = datetime.timedelta(hours=6)\n\n# minimum validity of JWT Tokens in seconds\nMIN_TOKEN_VALIDITY_SECONDS = 1800\n\n# Expiretime of JWT token in a test environment\nJWT_TEST_ACCESS_TOKEN_EXPIRES = datetime.timedelta(days=1)\n\n# Which resources should be initialized. These names correspond to the\n# file-names in the resource directory\nRESOURCES = ['node', 'collaboration', 'organization', 'task', 'result',\n             'token', 'user', 'version', 'recover', 'role',\n             'rule', 'health', 'vpn', 'port', 'event']\n\n# Super user information. This user is only created if it is not in the\n# database yet at startup time.\nSUPER_USER_INFO = {\n    \"username\": \"root\",\n    \"password\": \"root\"\n}\n\n# Expiration time of refresh tokens\nREFRESH_TOKENS_EXPIRE_HOURS = 48\n\n# Minimum time in seconds that a refresh token must be valid *longer than* the\n# access token. This is to prevent the access token from expiring before the\n# refresh token.\nMIN_REFRESH_TOKEN_EXPIRY_DELTA = 1\n\n# default support email address\nDEFAULT_SUPPORT_EMAIL_ADDRESS = 'support@vantage6.ai'\n\n# default time that token is valid in minutes\nDEFAULT_EMAILED_TOKEN_VALIDITY_MINUTES = 60\n\n# maximum time given to nodes to respond to ping in seconds\nMAX_RESPONSE_TIME_PING = 60\n", "# -*- coding: utf-8 -*-\n\"\"\"\nResources below '/<api_base>/token'\n\"\"\"\nimport logging\nimport datetime as dt\nimport pyotp\n\nfrom typing import Union\nfrom flask import request, g, render_template\nfrom flask_jwt_extended import (\n    jwt_required,\n    create_access_token,\n    create_refresh_token,\n    get_jwt_identity\n)\nfrom flask_restful import Api\nfrom http import HTTPStatus\n\nfrom vantage6 import server\nfrom vantage6.server import db\nfrom vantage6.server.model.user import User\nfrom vantage6.server.resource import (\n    with_node,\n    ServicesResources\n)\nfrom vantage6.server.resource.common.auth_helper import (\n  user_login, create_qr_uri\n)\n\nmodule_name = __name__.split('.')[-1]\nlog = logging.getLogger(module_name)\n\n\ndef setup(api, api_base, services):\n\n    path = \"/\".join([api_base, module_name])\n    log.info('Setting up \"{}\" and subdirectories'.format(path))\n\n    api.add_resource(\n        UserToken,\n        path+'/user',\n        endpoint='user_token',\n        methods=('POST',),\n        resource_class_kwargs=services\n    )\n\n    api.add_resource(\n        NodeToken,\n        path+'/node',\n        endpoint='node_token',\n        methods=('POST',),\n        resource_class_kwargs=services\n    )\n\n    api.add_resource(\n        ContainerToken,\n        path+'/container',\n        endpoint='container_token',\n        methods=('POST',),\n        resource_class_kwargs=services\n    )\n\n    api.add_resource(\n        RefreshToken,\n        path+'/refresh',\n        endpoint='refresh_token',\n        methods=('POST',),\n        resource_class_kwargs=services\n    )\n\n\n# ------------------------------------------------------------------------------\n# Resources / API's\n# ------------------------------------------------------------------------------\nclass UserToken(ServicesResources):\n    \"\"\"resource for api/token\"\"\"\n\n    def post(self):\n        \"\"\"Login user\n        ---\n        description: >-\n          Allow user to sign in by supplying a username and password. When MFA\n          is enabled on the server, a code is also required\n\n        requestBody:\n          content:\n            application/json:\n              schema:\n                properties:\n                  username:\n                    type: string\n                    description: Username of user that is logging in\n                  password:\n                    type: string\n                    description: User password\n                  mfa_code:\n                    type: string\n                    description: Two-factor authentication code. Only required\n                      if two-factor authentication is mandatory.\n\n        responses:\n          200:\n            description: Ok, authenticated\n          400:\n            description: Username/password combination unknown, or missing from\n              request body.\n          401:\n            description: Password and/or two-factor authentication token\n              incorrect.\n\n        tags: [\"Authentication\"]\n        \"\"\"\n        log.debug(\"Authenticate user using username and password\")\n\n        if not request.is_json:\n            log.warning('Authentication failed because no JSON body was '\n                        'provided!')\n            return {\"msg\": \"Missing JSON in request\"}, HTTPStatus.BAD_REQUEST\n\n        # Check JSON body\n        username = request.json.get('username', None)\n        password = request.json.get('password', None)\n        if not username and password:\n            msg = \"Username and/or password missing in JSON body\"\n            log.error(msg)\n            return {\"msg\": msg}, HTTPStatus.BAD_REQUEST\n\n        user, code = user_login(self.config.get(\"password_policy\", {}),\n                                username, password)\n        if code != HTTPStatus.OK:  # login failed\n            log.error(f\"Failed to login for user='{username}'\")\n            return user, code\n\n        is_mfa_enabled = self.config.get('two_factor_auth', False)\n        if is_mfa_enabled:\n            if user.otp_secret is None:\n                # server requires mfa but user hasn't set it up yet. Return\n                # an URI to generate a QR code\n                log.info(f'Redirecting user {username} to setup MFA')\n                return create_qr_uri(user), HTTPStatus.OK\n            else:\n                # 2nd authentication factor: check the OTP secret of the user\n                mfa_code = request.json.get('mfa_code')\n                if not mfa_code:\n                    # note: this is not treated as error, but simply guide\n                    # user to also fill in second factor\n                    return {\"msg\": \"Please include your two-factor \"\n                            \"authentication code\"}, HTTPStatus.OK\n                elif not self.validate_2fa_token(user, mfa_code):\n                    return {\n                        \"msg\": \"Your two-factor authentication code is \"\n                               \"incorrect!\"\n                    }, HTTPStatus.UNAUTHORIZED\n\n        token = _get_token_dict(user, self.api)\n\n        log.info(f\"Succesfull login from {username}\")\n        return token, HTTPStatus.OK, {'jwt-token': token['access_token']}\n\n    def user_login(self, username: str, password: str) -> Union[dict, db.User]:\n        \"\"\"Returns user a message in case of failed login attempt.\"\"\"\n        log.info(f\"Trying to login '{username}'\")\n        failed_login_msg = \"Failed to login\"\n        if db.User.username_exists(username):\n            user = db.User.get_by_username(username)\n            pw_policy = self.config.get('password_policy', {})\n            max_failed_attempts = pw_policy.get('max_failed_attempts', 5)\n            inactivation_time = pw_policy.get('inactivation_minutes', 15)\n\n            is_blocked, min_rem = user.is_blocked(max_failed_attempts,\n                                                  inactivation_time)\n            if is_blocked:\n                self.notify_user_blocked(user, max_failed_attempts, min_rem)\n                return {\"msg\": failed_login_msg}, HTTPStatus.UNAUTHORIZED\n            elif user.check_password(password):\n                user.failed_login_attempts = 0\n                user.save()\n                return user, HTTPStatus.OK\n            else:\n                # update the number of failed login attempts\n                user.failed_login_attempts = 1 \\\n                    if (\n                        not user.failed_login_attempts or\n                        user.failed_login_attempts >= max_failed_attempts\n                    ) else user.failed_login_attempts + 1\n                user.last_login_attempt = dt.datetime.now()\n                user.save()\n\n        return {\"msg\": failed_login_msg}, HTTPStatus.UNAUTHORIZED\n\n    def notify_user_blocked(self, user: db.User, max_n_attempts: int,\n                            min_rem: int) -> None:\n        \"\"\"Sends an email to the user when his or her account is locked\"\"\"\n        if not user.email:\n            log.warning(f'User {user.username} is locked, but does not have'\n                        'an email registered. So no message has been sent.')\n\n        log.info(f'User {user.username} is locked')\n\n        template_vars = {'firstname': user.firstname,\n                         'number_of_allowed_attempts': max_n_attempts,\n                         'ip': request.access_route[-1],\n                         'time': dt.datetime.now(dt.timezone.utc),\n                         'time_remaining': min_rem}\n\n        self.mail.send_email(\n            \"Your account has been temporary suspended\",\n            sender=\"support@vantage6.ai\",\n            recipients=[user.email],\n            text_body=render_template(\"mail/blocked_account.txt\",\n                                      **template_vars),\n            html_body=render_template(\"mail/blocked_account.html\",\n                                      **template_vars)\n        )\n\n    @staticmethod\n    def validate_2fa_token(user: User, mfa_code: Union[int, str]) -> bool:\n        \"\"\"\n        Check whether the 6-digit two-factor authentication code is valid\n\n        Parameters\n        ----------\n        user: User\n            The SQLAlchemy model of the user who is authenticating\n        mfa_code:\n            A six-digit TOTP code from an authenticator app\n\n        Returns\n        -------\n        bool\n          Whether six-digit code is valid or not\n        \"\"\"\n        # the option `valid_window=1` means the code from 1 time window (30s)\n        # ago, is also valid. This prevents that users around the edge of\n        # the time window can still login successfully if server is a bit slow\n        return pyotp.TOTP(user.otp_secret).verify(str(mfa_code),\n                                                  valid_window=1)\n\n\nclass NodeToken(ServicesResources):\n\n    def post(self):\n        \"\"\"Login node\n        ---\n        description: >-\n          Allows node to sign in using a unique API key. If the login is\n          successful this returns a dictionary with access and refresh tokens\n          for the node as well as a node_url and a refresh_url.\n\n        requestBody:\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Node'\n\n        responses:\n          200:\n            description: Ok, authenticated\n          400:\n            description: No API key provided in request body.\n          401:\n            description: Invalid API key\n\n        tags: [\"Authentication\"]\n        \"\"\"\n        log.debug(\"Authenticate Node using api key\")\n\n        if not request.is_json:\n            log.warning('Authentication failed because no JSON body was '\n                        'provided!')\n            return {\"msg\": \"Missing JSON in request\"}, HTTPStatus.BAD_REQUEST\n\n        # Check JSON body\n        api_key = request.json.get('api_key', None)\n        if not api_key:\n            msg = \"api_key missing in JSON body\"\n            log.error(msg)\n            return {\"msg\": msg}, HTTPStatus.BAD_REQUEST\n\n        node = db.Node.get_by_api_key(api_key)\n\n        if not node:  # login failed\n            log.error(\"Api key is not recognized\")\n            return {\"msg\": \"Api key is not recognized!\"}, \\\n                HTTPStatus.UNAUTHORIZED\n\n        token = _get_token_dict(node, self.api)\n\n        log.info(f\"Succesfull login as node '{node.id}' ({node.name})\")\n        return token, HTTPStatus.OK, {'jwt-token': token['access_token']}\n\n\nclass ContainerToken(ServicesResources):\n\n    @with_node\n    def post(self):\n        \"\"\"Algorithm container login\n        ---\n        description: >-\n          Generate token for the algorithm container of a specific task.\\n\n\n          Not available to users; only for authenticated nodes.\n\n        requestBody:\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ContainerToken'\n\n        responses:\n          200:\n            description: Container token generated\n          400:\n            description: Task does not exist or is already completed\n          401:\n            description: Key request for invalid image or task\n\n        tags: [\"Authentication\"]\n        \"\"\"\n        log.debug(\"Creating a token for a container running on a node\")\n\n        data = request.get_json()\n\n        task_id = data.get(\"task_id\")\n        claim_image = data.get(\"image\")\n\n        db_task = db.Task.get(task_id)\n        if not db_task:\n            log.warning(f\"Node {g.node.id} attempts to generate key for task \"\n                        f\"{task_id} that does not exist\")\n            return {\"msg\": \"Master task does not exist!\"}, \\\n                HTTPStatus.BAD_REQUEST\n\n        # verify that task the token is requested for exists\n        if claim_image != db_task.image:\n            log.warning(\n                f\"Node {g.node.id} attempts to generate key for image \"\n                f\"{claim_image} that does not belong to task {task_id}.\"\n            )\n            return {\"msg\": \"Image and task do no match\"}, \\\n                HTTPStatus.UNAUTHORIZED\n\n        # check if the node is in the collaboration to which the task is\n        # enlisted\n        if g.node.collaboration_id != db_task.collaboration_id:\n            log.warning(\n                f\"Node {g.node.id} attempts to generate key for task {task_id}\"\n                f\" which is outside its collaboration \"\n                f\"({g.node.collaboration_id}/{db_task.collaboration_id}).\"\n            )\n            return {\"msg\": \"You are not within the collaboration\"}, \\\n                HTTPStatus.UNAUTHORIZED\n\n        # validate that the task not has been finished yet\n        if db_task.complete:\n            log.warning(f\"Node {g.node.id} attempts to generate a key for \"\n                        f\"completed task {task_id}\")\n            return {\"msg\": \"Task is already finished!\"}, HTTPStatus.BAD_REQUEST\n\n        # container identity consists of its node_id,\n        # task_id, collaboration_id and image_id\n        container = {\n            \"client_type\": \"container\",\n            \"node_id\": g.node.id,\n            \"organization_id\": g.node.organization_id,\n            \"collaboration_id\": g.node.collaboration_id,\n            \"task_id\": task_id,\n            \"image\": claim_image,\n            \"database\": db_task.database\n        }\n        token = create_access_token(container, expires_delta=False)\n\n        return {'container_token': token}, HTTPStatus.OK\n\n\nclass RefreshToken(ServicesResources):\n\n    @jwt_required(refresh=True)\n    def post(self):\n        \"\"\"Refresh token\n        ---\n        description: >-\n          Refresh access token if the previous one is expired.\\n\n\n          Your refresh token must be present in the request headers to use\n          this endpoint.\n\n        responses:\n          200:\n            description: Token refreshed\n\n        security:\n          - bearerAuth: []\n\n        tags: [\"Authentication\"]\n        \"\"\"\n        user_or_node_id = get_jwt_identity()\n        log.info(f'Refreshing token for user or node \"{user_or_node_id}\"')\n        user_or_node = db.Authenticatable.get(user_or_node_id)\n\n        return _get_token_dict(user_or_node, self.api), HTTPStatus.OK\n\n\ndef _get_token_dict(user_or_node: db.Authenticatable, api: Api) -> dict:\n    \"\"\"\n    Create a dictionary with the tokens and urls for the user or node.\n\n    Parameters\n    ----------\n    user_or_node : db.Authenticatable\n        The user or node to create the tokens for.\n    api : Api\n        The api to create the urls for.\n    \"\"\"\n    token_dict = {\n        'access_token': create_access_token(user_or_node),\n        'refresh_token': create_refresh_token(user_or_node),\n        'refresh_url': api.url_for(RefreshToken),\n    }\n    if isinstance(user_or_node, db.User):\n        token_dict['user_url'] = api.url_for(server.resource.user.User,\n                                             id=user_or_node.id)\n    else:\n        token_dict['node_url'] = api.url_for(server.resource.node.Node,\n                                             id=user_or_node.id)\n    return token_dict\n"], "filenames": ["docs/server/yaml/server_config.yaml", "vantage6-client/vantage6/client/__init__.py", "vantage6-node/vantage6/node/__init__.py", "vantage6-node/vantage6/node/globals.py", "vantage6-node/vantage6/node/server_io.py", "vantage6-server/vantage6/server/__init__.py", "vantage6-server/vantage6/server/globals.py", "vantage6-server/vantage6/server/resource/token.py"], "buggy_code_start_loc": [3, 368, 488, 46, 6, 39, 19, 16], "buggy_code_end_loc": [107, 368, 488, 46, 62, 285, 40, 418], "fixing_code_start_loc": [3, 369, 489, 47, 7, 39, 19, 17], "fixing_code_end_loc": [113, 370, 492, 50, 87, 356, 47, 428], "type": "CWE-613", "message": "vantage6 is a privacy preserving federated learning infrastructure for secure insight exchange. Currently, the refresh token is valid indefinitely. The refresh token should get a validity of 24-48 hours. A fix was released in version 3.8.0.", "other": {"cve": {"id": "CVE-2023-23929", "sourceIdentifier": "security-advisories@github.com", "published": "2023-03-04T00:15:15.380", "lastModified": "2023-03-10T15:06:43.170", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "vantage6 is a privacy preserving federated learning infrastructure for secure insight exchange. Currently, the refresh token is valid indefinitely. The refresh token should get a validity of 24-48 hours. A fix was released in version 3.8.0."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 8.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.8, "impactScore": 5.9}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-613"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:vantage6:vantage6:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.8.0", "matchCriteriaId": "48327D5F-C4AD-4B29-9DA8-3661E5E2C91E"}]}]}], "references": [{"url": "https://github.com/vantage6/vantage6/commit/48ebfca42359e9a6743e9598684585e2522cdce8", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/vantage6/vantage6/security/advisories/GHSA-4w59-c3gc-rrhp", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/vantage6/vantage6/commit/48ebfca42359e9a6743e9598684585e2522cdce8"}}