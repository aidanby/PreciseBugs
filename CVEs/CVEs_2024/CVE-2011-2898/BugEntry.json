{"buggy_code": ["#ifndef __LINUX_IF_PACKET_H\n#define __LINUX_IF_PACKET_H\n\n#include <linux/types.h>\n\nstruct sockaddr_pkt {\n\tunsigned short spkt_family;\n\tunsigned char spkt_device[14];\n\t__be16 spkt_protocol;\n};\n\nstruct sockaddr_ll {\n\tunsigned short\tsll_family;\n\t__be16\t\tsll_protocol;\n\tint\t\tsll_ifindex;\n\tunsigned short\tsll_hatype;\n\tunsigned char\tsll_pkttype;\n\tunsigned char\tsll_halen;\n\tunsigned char\tsll_addr[8];\n};\n\n/* Packet types */\n\n#define PACKET_HOST\t\t0\t\t/* To us\t\t*/\n#define PACKET_BROADCAST\t1\t\t/* To all\t\t*/\n#define PACKET_MULTICAST\t2\t\t/* To group\t\t*/\n#define PACKET_OTHERHOST\t3\t\t/* To someone else \t*/\n#define PACKET_OUTGOING\t\t4\t\t/* Outgoing of any type */\n/* These ones are invisible by user level */\n#define PACKET_LOOPBACK\t\t5\t\t/* MC/BRD frame looped back */\n#define PACKET_FASTROUTE\t6\t\t/* Fastrouted frame\t*/\n\n/* Packet socket options */\n\n#define PACKET_ADD_MEMBERSHIP\t\t1\n#define PACKET_DROP_MEMBERSHIP\t\t2\n#define PACKET_RECV_OUTPUT\t\t3\n/* Value 4 is still used by obsolete turbo-packet. */\n#define PACKET_RX_RING\t\t\t5\n#define PACKET_STATISTICS\t\t6\n#define PACKET_COPY_THRESH\t\t7\n#define PACKET_AUXDATA\t\t\t8\n#define PACKET_ORIGDEV\t\t\t9\n#define PACKET_VERSION\t\t\t10\n#define PACKET_HDRLEN\t\t\t11\n#define PACKET_RESERVE\t\t\t12\n#define PACKET_TX_RING\t\t\t13\n#define PACKET_LOSS\t\t\t14\n#define PACKET_VNET_HDR\t\t\t15\n#define PACKET_TX_TIMESTAMP\t\t16\n#define PACKET_TIMESTAMP\t\t17\n\nstruct tpacket_stats {\n\tunsigned int\ttp_packets;\n\tunsigned int\ttp_drops;\n};\n\nstruct tpacket_auxdata {\n\t__u32\t\ttp_status;\n\t__u32\t\ttp_len;\n\t__u32\t\ttp_snaplen;\n\t__u16\t\ttp_mac;\n\t__u16\t\ttp_net;\n\t__u16\t\ttp_vlan_tci;\n};\n\n/* Rx ring - header status */\n#define TP_STATUS_KERNEL\t0x0\n#define TP_STATUS_USER\t\t0x1\n#define TP_STATUS_COPY\t\t0x2\n#define TP_STATUS_LOSING\t0x4\n#define TP_STATUS_CSUMNOTREADY\t0x8\n#define TP_STATUS_VLAN_VALID   0x10 /* auxdata has valid tp_vlan_tci */\n\n/* Tx ring - header status */\n#define TP_STATUS_AVAILABLE\t0x0\n#define TP_STATUS_SEND_REQUEST\t0x1\n#define TP_STATUS_SENDING\t0x2\n#define TP_STATUS_WRONG_FORMAT\t0x4\n\nstruct tpacket_hdr {\n\tunsigned long\ttp_status;\n\tunsigned int\ttp_len;\n\tunsigned int\ttp_snaplen;\n\tunsigned short\ttp_mac;\n\tunsigned short\ttp_net;\n\tunsigned int\ttp_sec;\n\tunsigned int\ttp_usec;\n};\n\n#define TPACKET_ALIGNMENT\t16\n#define TPACKET_ALIGN(x)\t(((x)+TPACKET_ALIGNMENT-1)&~(TPACKET_ALIGNMENT-1))\n#define TPACKET_HDRLEN\t\t(TPACKET_ALIGN(sizeof(struct tpacket_hdr)) + sizeof(struct sockaddr_ll))\n\nstruct tpacket2_hdr {\n\t__u32\t\ttp_status;\n\t__u32\t\ttp_len;\n\t__u32\t\ttp_snaplen;\n\t__u16\t\ttp_mac;\n\t__u16\t\ttp_net;\n\t__u32\t\ttp_sec;\n\t__u32\t\ttp_nsec;\n\t__u16\t\ttp_vlan_tci;\n};\n\n#define TPACKET2_HDRLEN\t\t(TPACKET_ALIGN(sizeof(struct tpacket2_hdr)) + sizeof(struct sockaddr_ll))\n\nenum tpacket_versions {\n\tTPACKET_V1,\n\tTPACKET_V2,\n};\n\n/*\n   Frame structure:\n\n   - Start. Frame must be aligned to TPACKET_ALIGNMENT=16\n   - struct tpacket_hdr\n   - pad to TPACKET_ALIGNMENT=16\n   - struct sockaddr_ll\n   - Gap, chosen so that packet data (Start+tp_net) alignes to TPACKET_ALIGNMENT=16\n   - Start+tp_mac: [ Optional MAC header ]\n   - Start+tp_net: Packet data, aligned to TPACKET_ALIGNMENT=16.\n   - Pad to align to TPACKET_ALIGNMENT=16\n */\n\nstruct tpacket_req {\n\tunsigned int\ttp_block_size;\t/* Minimal size of contiguous block */\n\tunsigned int\ttp_block_nr;\t/* Number of blocks */\n\tunsigned int\ttp_frame_size;\t/* Size of frame */\n\tunsigned int\ttp_frame_nr;\t/* Total number of frames */\n};\n\nstruct packet_mreq {\n\tint\t\tmr_ifindex;\n\tunsigned short\tmr_type;\n\tunsigned short\tmr_alen;\n\tunsigned char\tmr_address[8];\n};\n\n#define PACKET_MR_MULTICAST\t0\n#define PACKET_MR_PROMISC\t1\n#define PACKET_MR_ALLMULTI\t2\n#define PACKET_MR_UNICAST\t3\n\n#endif\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tPACKET - implements raw packet sockets.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tverify_area() now used correctly\n *\t\tAlan Cox\t:\tnew skbuff lists, look ma no backlogs!\n *\t\tAlan Cox\t:\ttidied skbuff lists.\n *\t\tAlan Cox\t:\tNow uses generic datagram routines I\n *\t\t\t\t\tadded. Also fixed the peek/read crash\n *\t\t\t\t\tfrom all old Linux datagram code.\n *\t\tAlan Cox\t:\tUses the improved datagram code.\n *\t\tAlan Cox\t:\tAdded NULL's for socket options.\n *\t\tAlan Cox\t:\tRe-commented the code.\n *\t\tAlan Cox\t:\tUse new kernel side addressing\n *\t\tRob Janssen\t:\tCorrect MTU usage.\n *\t\tDave Platt\t:\tCounter leaks caused by incorrect\n *\t\t\t\t\tinterrupt locking and some slightly\n *\t\t\t\t\tdubious gcc output. Can you read\n *\t\t\t\t\tcompiler: it said _VOLATILE_\n *\tRichard Kooijman\t:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tNew buffers. Use sk->mac.raw.\n *\t\tAlan Cox\t:\tsendmsg/recvmsg support.\n *\t\tAlan Cox\t:\tProtocol setting support\n *\tAlexey Kuznetsov\t:\tUntied from IPv4 stack.\n *\tCyrus Durgin\t\t:\tFixed kerneld for kmod.\n *\tMichal Ostrowski        :       Module initialization cleanup.\n *         Ulises Alonso        :       Frame number limit removal and\n *                                      packet_set_ring memory leak.\n *\t\tEric Biederman\t:\tAllow for > 8 byte hardware addresses.\n *\t\t\t\t\tThe convention is that longer addresses\n *\t\t\t\t\twill simply extend the hardware address\n *\t\t\t\t\tbyte arrays at the end of sockaddr_ll\n *\t\t\t\t\tand packet_mreq.\n *\t\tJohann Baudy\t:\tAdded TX RING.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n */\n\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/capability.h>\n#include <linux/fcntl.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/if_packet.h>\n#include <linux/wireless.h>\n#include <linux/kernel.h>\n#include <linux/kmod.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <net/net_namespace.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/ioctls.h>\n#include <asm/page.h>\n#include <asm/cacheflush.h>\n#include <asm/io.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/poll.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/mutex.h>\n#include <linux/if_vlan.h>\n#include <linux/virtio_net.h>\n#include <linux/errqueue.h>\n#include <linux/net_tstamp.h>\n\n#ifdef CONFIG_INET\n#include <net/inet_common.h>\n#endif\n\n/*\n   Assumptions:\n   - if device has no dev->hard_header routine, it adds and removes ll header\n     inside itself. In this case ll header is invisible outside of device,\n     but higher levels still should reserve dev->hard_header_len.\n     Some devices are enough clever to reallocate skb, when header\n     will not fit to reserved space (tunnel), another ones are silly\n     (PPP).\n   - packet socket receives packets with pulled ll header,\n     so that SOCK_RAW should push it back.\n\nOn receive:\n-----------\n\nIncoming, dev->hard_header!=NULL\n   mac_header -> ll header\n   data       -> data\n\nOutgoing, dev->hard_header!=NULL\n   mac_header -> ll header\n   data       -> ll header\n\nIncoming, dev->hard_header==NULL\n   mac_header -> UNKNOWN position. It is very likely, that it points to ll\n\t\t header.  PPP makes it, that is wrong, because introduce\n\t\t assymetry between rx and tx paths.\n   data       -> data\n\nOutgoing, dev->hard_header==NULL\n   mac_header -> data. ll header is still not built!\n   data       -> data\n\nResume\n  If dev->hard_header==NULL we are unlikely to restore sensible ll header.\n\n\nOn transmit:\n------------\n\ndev->hard_header != NULL\n   mac_header -> ll header\n   data       -> ll header\n\ndev->hard_header == NULL (ll header is added by device, we cannot control it)\n   mac_header -> data\n   data       -> data\n\n   We should set nh.raw on output to correct posistion,\n   packet classifier depends on it.\n */\n\n/* Private packet socket structures. */\n\nstruct packet_mclist {\n\tstruct packet_mclist\t*next;\n\tint\t\t\tifindex;\n\tint\t\t\tcount;\n\tunsigned short\t\ttype;\n\tunsigned short\t\talen;\n\tunsigned char\t\taddr[MAX_ADDR_LEN];\n};\n/* identical to struct packet_mreq except it has\n * a longer address field.\n */\nstruct packet_mreq_max {\n\tint\t\tmr_ifindex;\n\tunsigned short\tmr_type;\n\tunsigned short\tmr_alen;\n\tunsigned char\tmr_address[MAX_ADDR_LEN];\n};\n\nstatic int packet_set_ring(struct sock *sk, struct tpacket_req *req,\n\t\tint closing, int tx_ring);\n\nstruct pgv {\n\tchar *buffer;\n};\n\nstruct packet_ring_buffer {\n\tstruct pgv\t\t*pg_vec;\n\tunsigned int\t\thead;\n\tunsigned int\t\tframes_per_block;\n\tunsigned int\t\tframe_size;\n\tunsigned int\t\tframe_max;\n\n\tunsigned int\t\tpg_vec_order;\n\tunsigned int\t\tpg_vec_pages;\n\tunsigned int\t\tpg_vec_len;\n\n\tatomic_t\t\tpending;\n};\n\nstruct packet_sock;\nstatic int tpacket_snd(struct packet_sock *po, struct msghdr *msg);\n\nstatic void packet_flush_mclist(struct sock *sk);\n\nstruct packet_sock {\n\t/* struct sock has to be the first member of packet_sock */\n\tstruct sock\t\tsk;\n\tstruct tpacket_stats\tstats;\n\tstruct packet_ring_buffer\trx_ring;\n\tstruct packet_ring_buffer\ttx_ring;\n\tint\t\t\tcopy_thresh;\n\tspinlock_t\t\tbind_lock;\n\tstruct mutex\t\tpg_vec_lock;\n\tunsigned int\t\trunning:1,\t/* prot_hook is attached*/\n\t\t\t\tauxdata:1,\n\t\t\t\torigdev:1,\n\t\t\t\thas_vnet_hdr:1;\n\tint\t\t\tifindex;\t/* bound device\t\t*/\n\t__be16\t\t\tnum;\n\tstruct packet_mclist\t*mclist;\n\tatomic_t\t\tmapped;\n\tenum tpacket_versions\ttp_version;\n\tunsigned int\t\ttp_hdrlen;\n\tunsigned int\t\ttp_reserve;\n\tunsigned int\t\ttp_loss:1;\n\tunsigned int\t\ttp_tstamp;\n\tstruct packet_type\tprot_hook ____cacheline_aligned_in_smp;\n};\n\nstruct packet_skb_cb {\n\tunsigned int origlen;\n\tunion {\n\t\tstruct sockaddr_pkt pkt;\n\t\tstruct sockaddr_ll ll;\n\t} sa;\n};\n\n#define PACKET_SKB_CB(__skb)\t((struct packet_skb_cb *)((__skb)->cb))\n\nstatic inline __pure struct page *pgv_to_page(void *addr)\n{\n\tif (is_vmalloc_addr(addr))\n\t\treturn vmalloc_to_page(addr);\n\treturn virt_to_page(addr);\n}\n\nstatic void __packet_set_status(struct packet_sock *po, void *frame, int status)\n{\n\tunion {\n\t\tstruct tpacket_hdr *h1;\n\t\tstruct tpacket2_hdr *h2;\n\t\tvoid *raw;\n\t} h;\n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_status = status;\n\t\tflush_dcache_page(pgv_to_page(&h.h1->tp_status));\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_status = status;\n\t\tflush_dcache_page(pgv_to_page(&h.h2->tp_status));\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"TPACKET version not supported\\n\");\n\t\tBUG();\n\t}\n\n\tsmp_wmb();\n}\n\nstatic int __packet_get_status(struct packet_sock *po, void *frame)\n{\n\tunion {\n\t\tstruct tpacket_hdr *h1;\n\t\tstruct tpacket2_hdr *h2;\n\t\tvoid *raw;\n\t} h;\n\n\tsmp_rmb();\n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\tflush_dcache_page(pgv_to_page(&h.h1->tp_status));\n\t\treturn h.h1->tp_status;\n\tcase TPACKET_V2:\n\t\tflush_dcache_page(pgv_to_page(&h.h2->tp_status));\n\t\treturn h.h2->tp_status;\n\tdefault:\n\t\tpr_err(\"TPACKET version not supported\\n\");\n\t\tBUG();\n\t\treturn 0;\n\t}\n}\n\nstatic void *packet_lookup_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tunsigned int position,\n\t\tint status)\n{\n\tunsigned int pg_vec_pos, frame_offset;\n\tunion {\n\t\tstruct tpacket_hdr *h1;\n\t\tstruct tpacket2_hdr *h2;\n\t\tvoid *raw;\n\t} h;\n\n\tpg_vec_pos = position / rb->frames_per_block;\n\tframe_offset = position % rb->frames_per_block;\n\n\th.raw = rb->pg_vec[pg_vec_pos].buffer +\n\t\t(frame_offset * rb->frame_size);\n\n\tif (status != __packet_get_status(po, h.raw))\n\t\treturn NULL;\n\n\treturn h.raw;\n}\n\nstatic inline void *packet_current_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status)\n{\n\treturn packet_lookup_frame(po, rb, rb->head, status);\n}\n\nstatic inline void *packet_previous_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status)\n{\n\tunsigned int previous = rb->head ? rb->head - 1 : rb->frame_max;\n\treturn packet_lookup_frame(po, rb, previous, status);\n}\n\nstatic inline void packet_increment_head(struct packet_ring_buffer *buff)\n{\n\tbuff->head = buff->head != buff->frame_max ? buff->head+1 : 0;\n}\n\nstatic inline struct packet_sock *pkt_sk(struct sock *sk)\n{\n\treturn (struct packet_sock *)sk;\n}\n\nstatic void packet_sock_destruct(struct sock *sk)\n{\n\tskb_queue_purge(&sk->sk_error_queue);\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive packet socket: %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tsk_refcnt_debug_dec(sk);\n}\n\n\nstatic const struct proto_ops packet_ops;\n\nstatic const struct proto_ops packet_ops_spkt;\n\nstatic int packet_rcv_spkt(struct sk_buff *skb, struct net_device *dev,\n\t\t\t   struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct sockaddr_pkt *spkt;\n\n\t/*\n\t *\tWhen we registered the protocol we saved the socket in the data\n\t *\tfield for just this event.\n\t */\n\n\tsk = pt->af_packet_priv;\n\n\t/*\n\t *\tYank back the headers [hope the device set this\n\t *\tright or kerboom...]\n\t *\n\t *\tIncoming packets have ll header pulled,\n\t *\tpush it back.\n\t *\n\t *\tFor outgoing ones skb->data == skb_mac_header(skb)\n\t *\tso that this procedure is noop.\n\t */\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto out;\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto out;\n\n\tskb = skb_share_check(skb, GFP_ATOMIC);\n\tif (skb == NULL)\n\t\tgoto oom;\n\n\t/* drop any routing info */\n\tskb_dst_drop(skb);\n\n\t/* drop conntrack reference */\n\tnf_reset(skb);\n\n\tspkt = &PACKET_SKB_CB(skb)->sa.pkt;\n\n\tskb_push(skb, skb->data - skb_mac_header(skb));\n\n\t/*\n\t *\tThe SOCK_PACKET socket receives _all_ frames.\n\t */\n\n\tspkt->spkt_family = dev->type;\n\tstrlcpy(spkt->spkt_device, dev->name, sizeof(spkt->spkt_device));\n\tspkt->spkt_protocol = skb->protocol;\n\n\t/*\n\t *\tCharge the memory to the socket. This is done specifically\n\t *\tto prevent sockets using all the memory up.\n\t */\n\n\tif (sock_queue_rcv_skb(sk, skb) == 0)\n\t\treturn 0;\n\nout:\n\tkfree_skb(skb);\noom:\n\treturn 0;\n}\n\n\n/*\n *\tOutput a raw packet to a device layer. This bypasses all the other\n *\tprotocol layers and you must therefore supply it with a complete frame\n */\n\nstatic int packet_sendmsg_spkt(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pkt *saddr = (struct sockaddr_pkt *)msg->msg_name;\n\tstruct sk_buff *skb = NULL;\n\tstruct net_device *dev;\n\t__be16 proto = 0;\n\tint err;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (saddr) {\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr))\n\t\t\treturn -EINVAL;\n\t\tif (msg->msg_namelen == sizeof(struct sockaddr_pkt))\n\t\t\tproto = saddr->spkt_protocol;\n\t} else\n\t\treturn -ENOTCONN;\t/* SOCK_PACKET must be sent giving an address */\n\n\t/*\n\t *\tFind the device first to size check it\n\t */\n\n\tsaddr->spkt_device[13] = 0;\nretry:\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(sock_net(sk), saddr->spkt_device);\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out_unlock;\n\n\terr = -ENETDOWN;\n\tif (!(dev->flags & IFF_UP))\n\t\tgoto out_unlock;\n\n\t/*\n\t * You may not queue a frame bigger than the mtu. This is the lowest level\n\t * raw protocol and you must do your own fragmentation at this level.\n\t */\n\n\terr = -EMSGSIZE;\n\tif (len > dev->mtu + dev->hard_header_len + VLAN_HLEN)\n\t\tgoto out_unlock;\n\n\tif (!skb) {\n\t\tsize_t reserved = LL_RESERVED_SPACE(dev);\n\t\tunsigned int hhlen = dev->header_ops ? dev->hard_header_len : 0;\n\n\t\trcu_read_unlock();\n\t\tskb = sock_wmalloc(sk, len + reserved, 0, GFP_KERNEL);\n\t\tif (skb == NULL)\n\t\t\treturn -ENOBUFS;\n\t\t/* FIXME: Save some space for broken drivers that write a hard\n\t\t * header at transmission time by themselves. PPP is the notable\n\t\t * one here. This should really be fixed at the driver level.\n\t\t */\n\t\tskb_reserve(skb, reserved);\n\t\tskb_reset_network_header(skb);\n\n\t\t/* Try to align data part correctly */\n\t\tif (hhlen) {\n\t\t\tskb->data -= hhlen;\n\t\t\tskb->tail -= hhlen;\n\t\t\tif (len < hhlen)\n\t\t\t\tskb_reset_network_header(skb);\n\t\t}\n\t\terr = memcpy_fromiovec(skb_put(skb, len), msg->msg_iov, len);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tgoto retry;\n\t}\n\n\tif (len > (dev->mtu + dev->hard_header_len)) {\n\t\t/* Earlier code assumed this would be a VLAN pkt,\n\t\t * double-check this now that we have the actual\n\t\t * packet in hand.\n\t\t */\n\t\tstruct ethhdr *ehdr;\n\t\tskb_reset_mac_header(skb);\n\t\tehdr = eth_hdr(skb);\n\t\tif (ehdr->h_proto != htons(ETH_P_8021Q)) {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\terr = sock_tx_timestamp(sk, &skb_shinfo(skb)->tx_flags);\n\tif (err < 0)\n\t\tgoto out_unlock;\n\n\tdev_queue_xmit(skb);\n\trcu_read_unlock();\n\treturn len;\n\nout_unlock:\n\trcu_read_unlock();\nout_free:\n\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic inline unsigned int run_filter(const struct sk_buff *skb,\n\t\t\t\t      const struct sock *sk,\n\t\t\t\t      unsigned int res)\n{\n\tstruct sk_filter *filter;\n\n\trcu_read_lock();\n\tfilter = rcu_dereference(sk->sk_filter);\n\tif (filter != NULL)\n\t\tres = SK_RUN_FILTER(filter, skb);\n\trcu_read_unlock();\n\n\treturn res;\n}\n\n/*\n * This function makes lazy skb cloning in hope that most of packets\n * are discarded by BPF.\n *\n * Note tricky part: we DO mangle shared skb! skb->data, skb->len\n * and skb->cb are mangled. It works because (and until) packets\n * falling here are owned by current CPU. Output packets are cloned\n * by dev_queue_xmit_nit(), input packets are processed by net_bh\n * sequencially, so that if we return skb to original state on exit,\n * we will not harm anyone.\n */\n\nstatic int packet_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t      struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct sockaddr_ll *sll;\n\tstruct packet_sock *po;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tskb->dev = dev;\n\n\tif (dev->header_ops) {\n\t\t/* The device has an explicit notion of ll header,\n\t\t * exported to higher levels.\n\t\t *\n\t\t * Otherwise, the device hides details of its frame\n\t\t * structure, so that corresponding packet head is\n\t\t * never delivered to user.\n\t\t */\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=\n\t    (unsigned)sk->sk_rcvbuf)\n\t\tgoto drop_n_acct;\n\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (nskb == NULL)\n\t\t\tgoto drop_n_acct;\n\n\t\tif (skb_head != skb->data) {\n\t\t\tskb->data = skb_head;\n\t\t\tskb->len = skb_len;\n\t\t}\n\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\n\tBUILD_BUG_ON(sizeof(*PACKET_SKB_CB(skb)) + MAX_ADDR_LEN - 8 >\n\t\t     sizeof(skb->cb));\n\n\tsll = &PACKET_SKB_CB(skb)->sa.ll;\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\n\tPACKET_SKB_CB(skb)->origlen = skb->len;\n\n\tif (pskb_trim(skb, snaplen))\n\t\tgoto drop_n_acct;\n\n\tskb_set_owner_r(skb, sk);\n\tskb->dev = NULL;\n\tskb_dst_drop(skb);\n\n\t/* drop conntrack reference */\n\tnf_reset(skb);\n\n\tspin_lock(&sk->sk_receive_queue.lock);\n\tpo->stats.tp_packets++;\n\tskb->dropcount = atomic_read(&sk->sk_drops);\n\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tsk->sk_data_ready(sk, skb->len);\n\treturn 0;\n\ndrop_n_acct:\n\tpo->stats.tp_drops = atomic_inc_return(&sk->sk_drops);\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tconsume_skb(skb);\n\treturn 0;\n}\n\nstatic int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion {\n\t\tstruct tpacket_hdr *h1;\n\t\tstruct tpacket2_hdr *h2;\n\t\tvoid *raw;\n\t} h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_LOSING|TP_STATUS_USER;\n\tunsigned short macoff, netoff, hdrlen;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timeval tv;\n\tstruct timespec ts;\n\tstruct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev->header_ops) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\tpo->tp_reserve;\n\t\tmacoff = netoff - maclen;\n\t}\n\n\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\tif (po->copy_thresh &&\n\t\t    atomic_read(&sk->sk_rmem_alloc) + skb->truesize <\n\t\t    (unsigned)sk->sk_rcvbuf) {\n\t\t\tif (skb_shared(skb)) {\n\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t} else {\n\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\tskb_head = skb->data;\n\t\t\t}\n\t\t\tif (copy_skb)\n\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t}\n\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\tif ((int)snaplen < 0)\n\t\t\tsnaplen = 0;\n\t}\n\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_frame(po, &po->rx_ring, TP_STATUS_KERNEL);\n\tif (!h.raw)\n\t\tgoto ring_is_full;\n\tpacket_increment_head(&po->rx_ring);\n\tpo->stats.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tif (!po->stats.tp_drops)\n\t\tstatus &= ~TP_STATUS_LOSING;\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\tif ((po->tp_tstamp & SOF_TIMESTAMPING_SYS_HARDWARE)\n\t\t\t\t&& shhwtstamps->syststamp.tv64)\n\t\t\ttv = ktime_to_timeval(shhwtstamps->syststamp);\n\t\telse if ((po->tp_tstamp & SOF_TIMESTAMPING_RAW_HARDWARE)\n\t\t\t\t&& shhwtstamps->hwtstamp.tv64)\n\t\t\ttv = ktime_to_timeval(shhwtstamps->hwtstamp);\n\t\telse if (skb->tstamp.tv64)\n\t\t\ttv = ktime_to_timeval(skb->tstamp);\n\t\telse\n\t\t\tdo_gettimeofday(&tv);\n\t\th.h1->tp_sec = tv.tv_sec;\n\t\th.h1->tp_usec = tv.tv_usec;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\tif ((po->tp_tstamp & SOF_TIMESTAMPING_SYS_HARDWARE)\n\t\t\t\t&& shhwtstamps->syststamp.tv64)\n\t\t\tts = ktime_to_timespec(shhwtstamps->syststamp);\n\t\telse if ((po->tp_tstamp & SOF_TIMESTAMPING_RAW_HARDWARE)\n\t\t\t\t&& shhwtstamps->hwtstamp.tv64)\n\t\t\tts = ktime_to_timespec(shhwtstamps->hwtstamp);\n\t\telse if (skb->tstamp.tv64)\n\t\t\tts = ktime_to_timespec(skb->tstamp);\n\t\telse\n\t\t\tgetnstimeofday(&ts);\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (vlan_tx_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = vlan_tx_tag_get(skb);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t}\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\t__packet_set_status(po, h.raw, status);\n\tsmp_mb();\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\t{\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *)PAGE_ALIGN((unsigned long)h.raw + macoff + snaplen);\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n#endif\n\n\tsk->sk_data_ready(sk, 0);\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tkfree_skb(skb);\n\treturn 0;\n\nring_is_full:\n\tpo->stats.tp_drops++;\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tsk->sk_data_ready(sk, 0);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}\n\nstatic void tpacket_destruct_skb(struct sk_buff *skb)\n{\n\tstruct packet_sock *po = pkt_sk(skb->sk);\n\tvoid *ph;\n\n\tBUG_ON(skb == NULL);\n\n\tif (likely(po->tx_ring.pg_vec)) {\n\t\tph = skb_shinfo(skb)->destructor_arg;\n\t\tBUG_ON(__packet_get_status(po, ph) != TP_STATUS_SENDING);\n\t\tBUG_ON(atomic_read(&po->tx_ring.pending) == 0);\n\t\tatomic_dec(&po->tx_ring.pending);\n\t\t__packet_set_status(po, ph, TP_STATUS_AVAILABLE);\n\t}\n\n\tsock_wfree(skb);\n}\n\nstatic int tpacket_fill_skb(struct packet_sock *po, struct sk_buff *skb,\n\t\tvoid *frame, struct net_device *dev, int size_max,\n\t\t__be16 proto, unsigned char *addr)\n{\n\tunion {\n\t\tstruct tpacket_hdr *h1;\n\t\tstruct tpacket2_hdr *h2;\n\t\tvoid *raw;\n\t} ph;\n\tint to_write, offset, len, tp_len, nr_frags, len_max;\n\tstruct socket *sock = po->sk.sk_socket;\n\tstruct page *page;\n\tvoid *data;\n\tint err;\n\n\tph.raw = frame;\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = po->sk.sk_priority;\n\tskb->mark = po->sk.sk_mark;\n\tskb_shinfo(skb)->destructor_arg = ph.raw;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V2:\n\t\ttp_len = ph.h2->tp_len;\n\t\tbreak;\n\tdefault:\n\t\ttp_len = ph.h1->tp_len;\n\t\tbreak;\n\t}\n\tif (unlikely(tp_len > size_max)) {\n\t\tpr_err(\"packet size is too long (%d > %d)\\n\", tp_len, size_max);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tskb_reserve(skb, LL_RESERVED_SPACE(dev));\n\tskb_reset_network_header(skb);\n\n\tdata = ph.raw + po->tp_hdrlen - sizeof(struct sockaddr_ll);\n\tto_write = tp_len;\n\n\tif (sock->type == SOCK_DGRAM) {\n\t\terr = dev_hard_header(skb, dev, ntohs(proto), addr,\n\t\t\t\tNULL, tp_len);\n\t\tif (unlikely(err < 0))\n\t\t\treturn -EINVAL;\n\t} else if (dev->hard_header_len) {\n\t\t/* net device doesn't like empty head */\n\t\tif (unlikely(tp_len <= dev->hard_header_len)) {\n\t\t\tpr_err(\"packet size is too short (%d < %d)\\n\",\n\t\t\t       tp_len, dev->hard_header_len);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tskb_push(skb, dev->hard_header_len);\n\t\terr = skb_store_bits(skb, 0, data,\n\t\t\t\tdev->hard_header_len);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\n\t\tdata += dev->hard_header_len;\n\t\tto_write -= dev->hard_header_len;\n\t}\n\n\terr = -EFAULT;\n\toffset = offset_in_page(data);\n\tlen_max = PAGE_SIZE - offset;\n\tlen = ((to_write > len_max) ? len_max : to_write);\n\n\tskb->data_len = to_write;\n\tskb->len += to_write;\n\tskb->truesize += to_write;\n\tatomic_add(to_write, &po->sk.sk_wmem_alloc);\n\n\twhile (likely(to_write)) {\n\t\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\t\tif (unlikely(nr_frags >= MAX_SKB_FRAGS)) {\n\t\t\tpr_err(\"Packet exceed the number of skb frags(%lu)\\n\",\n\t\t\t       MAX_SKB_FRAGS);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tpage = pgv_to_page(data);\n\t\tdata += len;\n\t\tflush_dcache_page(page);\n\t\tget_page(page);\n\t\tskb_fill_page_desc(skb, nr_frags, page, offset, len);\n\t\tto_write -= len;\n\t\toffset = 0;\n\t\tlen_max = PAGE_SIZE;\n\t\tlen = ((to_write > len_max) ? len_max : to_write);\n\t}\n\n\treturn tp_len;\n}\n\nstatic int tpacket_snd(struct packet_sock *po, struct msghdr *msg)\n{\n\tstruct sk_buff *skb;\n\tstruct net_device *dev;\n\t__be16 proto;\n\tint ifindex, err, reserve = 0;\n\tvoid *ph;\n\tstruct sockaddr_ll *saddr = (struct sockaddr_ll *)msg->msg_name;\n\tint tp_len, size_max;\n\tunsigned char *addr;\n\tint len_sum = 0;\n\tint status = 0;\n\n\tmutex_lock(&po->pg_vec_lock);\n\n\terr = -EBUSY;\n\tif (saddr == NULL) {\n\t\tifindex\t= po->ifindex;\n\t\tproto\t= po->num;\n\t\taddr\t= NULL;\n\t} else {\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll))\n\t\t\tgoto out;\n\t\tif (msg->msg_namelen < (saddr->sll_halen\n\t\t\t\t\t+ offsetof(struct sockaddr_ll,\n\t\t\t\t\t\tsll_addr)))\n\t\t\tgoto out;\n\t\tifindex\t= saddr->sll_ifindex;\n\t\tproto\t= saddr->sll_protocol;\n\t\taddr\t= saddr->sll_addr;\n\t}\n\n\tdev = dev_get_by_index(sock_net(&po->sk), ifindex);\n\terr = -ENXIO;\n\tif (unlikely(dev == NULL))\n\t\tgoto out;\n\n\treserve = dev->hard_header_len;\n\n\terr = -ENETDOWN;\n\tif (unlikely(!(dev->flags & IFF_UP)))\n\t\tgoto out_put;\n\n\tsize_max = po->tx_ring.frame_size\n\t\t- (po->tp_hdrlen - sizeof(struct sockaddr_ll));\n\n\tif (size_max > dev->mtu + reserve)\n\t\tsize_max = dev->mtu + reserve;\n\n\tdo {\n\t\tph = packet_current_frame(po, &po->tx_ring,\n\t\t\t\tTP_STATUS_SEND_REQUEST);\n\n\t\tif (unlikely(ph == NULL)) {\n\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\n\t\tstatus = TP_STATUS_SEND_REQUEST;\n\t\tskb = sock_alloc_send_skb(&po->sk,\n\t\t\t\tLL_ALLOCATED_SPACE(dev)\n\t\t\t\t+ sizeof(struct sockaddr_ll),\n\t\t\t\t0, &err);\n\n\t\tif (unlikely(skb == NULL))\n\t\t\tgoto out_status;\n\n\t\ttp_len = tpacket_fill_skb(po, skb, ph, dev, size_max, proto,\n\t\t\t\taddr);\n\n\t\tif (unlikely(tp_len < 0)) {\n\t\t\tif (po->tp_loss) {\n\t\t\t\t__packet_set_status(po, ph,\n\t\t\t\t\t\tTP_STATUS_AVAILABLE);\n\t\t\t\tpacket_increment_head(&po->tx_ring);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tstatus = TP_STATUS_WRONG_FORMAT;\n\t\t\t\terr = tp_len;\n\t\t\t\tgoto out_status;\n\t\t\t}\n\t\t}\n\n\t\tskb->destructor = tpacket_destruct_skb;\n\t\t__packet_set_status(po, ph, TP_STATUS_SENDING);\n\t\tatomic_inc(&po->tx_ring.pending);\n\n\t\tstatus = TP_STATUS_SEND_REQUEST;\n\t\terr = dev_queue_xmit(skb);\n\t\tif (unlikely(err > 0)) {\n\t\t\terr = net_xmit_errno(err);\n\t\t\tif (err && __packet_get_status(po, ph) ==\n\t\t\t\t   TP_STATUS_AVAILABLE) {\n\t\t\t\t/* skb was destructed already */\n\t\t\t\tskb = NULL;\n\t\t\t\tgoto out_status;\n\t\t\t}\n\t\t\t/*\n\t\t\t * skb was dropped but not destructed yet;\n\t\t\t * let's treat it like congestion or err < 0\n\t\t\t */\n\t\t\terr = 0;\n\t\t}\n\t\tpacket_increment_head(&po->tx_ring);\n\t\tlen_sum += tp_len;\n\t} while (likely((ph != NULL) ||\n\t\t\t((!(msg->msg_flags & MSG_DONTWAIT)) &&\n\t\t\t (atomic_read(&po->tx_ring.pending))))\n\t\t);\n\n\terr = len_sum;\n\tgoto out_put;\n\nout_status:\n\t__packet_set_status(po, ph, status);\n\tkfree_skb(skb);\nout_put:\n\tdev_put(dev);\nout:\n\tmutex_unlock(&po->pg_vec_lock);\n\treturn err;\n}\n\nstatic inline struct sk_buff *packet_alloc_skb(struct sock *sk, size_t prepad,\n\t\t\t\t\t       size_t reserve, size_t len,\n\t\t\t\t\t       size_t linear, int noblock,\n\t\t\t\t\t       int *err)\n{\n\tstruct sk_buff *skb;\n\n\t/* Under a page?  Don't bother with paged skb. */\n\tif (prepad + len < PAGE_SIZE || !linear)\n\t\tlinear = len;\n\n\tskb = sock_alloc_send_pskb(sk, prepad + linear, len - linear, noblock,\n\t\t\t\t   err);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, reserve);\n\tskb_put(skb, linear);\n\tskb->data_len = len - linear;\n\tskb->len += len - linear;\n\n\treturn skb;\n}\n\nstatic int packet_snd(struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_ll *saddr = (struct sockaddr_ll *)msg->msg_name;\n\tstruct sk_buff *skb;\n\tstruct net_device *dev;\n\t__be16 proto;\n\tunsigned char *addr;\n\tint ifindex, err, reserve = 0;\n\tstruct virtio_net_hdr vnet_hdr = { 0 };\n\tint offset = 0;\n\tint vnet_hdr_len;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned short gso_type = 0;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (saddr == NULL) {\n\t\tifindex\t= po->ifindex;\n\t\tproto\t= po->num;\n\t\taddr\t= NULL;\n\t} else {\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll))\n\t\t\tgoto out;\n\t\tif (msg->msg_namelen < (saddr->sll_halen + offsetof(struct sockaddr_ll, sll_addr)))\n\t\t\tgoto out;\n\t\tifindex\t= saddr->sll_ifindex;\n\t\tproto\t= saddr->sll_protocol;\n\t\taddr\t= saddr->sll_addr;\n\t}\n\n\n\tdev = dev_get_by_index(sock_net(sk), ifindex);\n\terr = -ENXIO;\n\tif (dev == NULL)\n\t\tgoto out_unlock;\n\tif (sock->type == SOCK_RAW)\n\t\treserve = dev->hard_header_len;\n\n\terr = -ENETDOWN;\n\tif (!(dev->flags & IFF_UP))\n\t\tgoto out_unlock;\n\n\tif (po->has_vnet_hdr) {\n\t\tvnet_hdr_len = sizeof(vnet_hdr);\n\n\t\terr = -EINVAL;\n\t\tif (len < vnet_hdr_len)\n\t\t\tgoto out_unlock;\n\n\t\tlen -= vnet_hdr_len;\n\n\t\terr = memcpy_fromiovec((void *)&vnet_hdr, msg->msg_iov,\n\t\t\t\t       vnet_hdr_len);\n\t\tif (err < 0)\n\t\t\tgoto out_unlock;\n\n\t\tif ((vnet_hdr.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\n\t\t    (vnet_hdr.csum_start + vnet_hdr.csum_offset + 2 >\n\t\t      vnet_hdr.hdr_len))\n\t\t\tvnet_hdr.hdr_len = vnet_hdr.csum_start +\n\t\t\t\t\t\t vnet_hdr.csum_offset + 2;\n\n\t\terr = -EINVAL;\n\t\tif (vnet_hdr.hdr_len > len)\n\t\t\tgoto out_unlock;\n\n\t\tif (vnet_hdr.gso_type != VIRTIO_NET_HDR_GSO_NONE) {\n\t\t\tswitch (vnet_hdr.gso_type & ~VIRTIO_NET_HDR_GSO_ECN) {\n\t\t\tcase VIRTIO_NET_HDR_GSO_TCPV4:\n\t\t\t\tgso_type = SKB_GSO_TCPV4;\n\t\t\t\tbreak;\n\t\t\tcase VIRTIO_NET_HDR_GSO_TCPV6:\n\t\t\t\tgso_type = SKB_GSO_TCPV6;\n\t\t\t\tbreak;\n\t\t\tcase VIRTIO_NET_HDR_GSO_UDP:\n\t\t\t\tgso_type = SKB_GSO_UDP;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\n\t\t\tif (vnet_hdr.gso_type & VIRTIO_NET_HDR_GSO_ECN)\n\t\t\t\tgso_type |= SKB_GSO_TCP_ECN;\n\n\t\t\tif (vnet_hdr.gso_size == 0)\n\t\t\t\tgoto out_unlock;\n\n\t\t}\n\t}\n\n\terr = -EMSGSIZE;\n\tif (!gso_type && (len > dev->mtu + reserve + VLAN_HLEN))\n\t\tgoto out_unlock;\n\n\terr = -ENOBUFS;\n\tskb = packet_alloc_skb(sk, LL_ALLOCATED_SPACE(dev),\n\t\t\t       LL_RESERVED_SPACE(dev), len, vnet_hdr.hdr_len,\n\t\t\t       msg->msg_flags & MSG_DONTWAIT, &err);\n\tif (skb == NULL)\n\t\tgoto out_unlock;\n\n\tskb_set_network_header(skb, reserve);\n\n\terr = -EINVAL;\n\tif (sock->type == SOCK_DGRAM &&\n\t    (offset = dev_hard_header(skb, dev, ntohs(proto), addr, NULL, len)) < 0)\n\t\tgoto out_free;\n\n\t/* Returns -EFAULT on error */\n\terr = skb_copy_datagram_from_iovec(skb, offset, msg->msg_iov, 0, len);\n\tif (err)\n\t\tgoto out_free;\n\terr = sock_tx_timestamp(sk, &skb_shinfo(skb)->tx_flags);\n\tif (err < 0)\n\t\tgoto out_free;\n\n\tif (!gso_type && (len > dev->mtu + reserve)) {\n\t\t/* Earlier code assumed this would be a VLAN pkt,\n\t\t * double-check this now that we have the actual\n\t\t * packet in hand.\n\t\t */\n\t\tstruct ethhdr *ehdr;\n\t\tskb_reset_mac_header(skb);\n\t\tehdr = eth_hdr(skb);\n\t\tif (ehdr->h_proto != htons(ETH_P_8021Q)) {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tif (po->has_vnet_hdr) {\n\t\tif (vnet_hdr.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) {\n\t\t\tif (!skb_partial_csum_set(skb, vnet_hdr.csum_start,\n\t\t\t\t\t\t  vnet_hdr.csum_offset)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\n\t\tskb_shinfo(skb)->gso_size = vnet_hdr.gso_size;\n\t\tskb_shinfo(skb)->gso_type = gso_type;\n\n\t\t/* Header must be checked, and gso_segs computed. */\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_DODGY;\n\t\tskb_shinfo(skb)->gso_segs = 0;\n\n\t\tlen += vnet_hdr_len;\n\t}\n\n\t/*\n\t *\tNow send it\n\t */\n\n\terr = dev_queue_xmit(skb);\n\tif (err > 0 && (err = net_xmit_errno(err)) != 0)\n\t\tgoto out_unlock;\n\n\tdev_put(dev);\n\n\treturn len;\n\nout_free:\n\tkfree_skb(skb);\nout_unlock:\n\tif (dev)\n\t\tdev_put(dev);\nout:\n\treturn err;\n}\n\nstatic int packet_sendmsg(struct kiocb *iocb, struct socket *sock,\n\t\tstruct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tif (po->tx_ring.pg_vec)\n\t\treturn tpacket_snd(po, msg);\n\telse\n\t\treturn packet_snd(sock, msg, len);\n}\n\n/*\n *\tClose a PACKET socket. This is fairly simple. We immediately go\n *\tto 'closed' state and remove our protocol entry in the device list.\n */\n\nstatic int packet_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po;\n\tstruct net *net;\n\tstruct tpacket_req req;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\tpo = pkt_sk(sk);\n\n\tspin_lock_bh(&net->packet.sklist_lock);\n\tsk_del_node_init_rcu(sk);\n\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\tspin_unlock_bh(&net->packet.sklist_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (po->running) {\n\t\t/*\n\t\t * Remove from protocol table\n\t\t */\n\t\tpo->running = 0;\n\t\tpo->num = 0;\n\t\t__dev_remove_pack(&po->prot_hook);\n\t\t__sock_put(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tpacket_flush_mclist(sk);\n\n\tmemset(&req, 0, sizeof(req));\n\n\tif (po->rx_ring.pg_vec)\n\t\tpacket_set_ring(sk, &req, 1, 0);\n\n\tif (po->tx_ring.pg_vec)\n\t\tpacket_set_ring(sk, &req, 1, 1);\n\n\tsynchronize_net();\n\t/*\n\t *\tNow the socket is dead. No more input will appear.\n\t */\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\t/* Purge queues */\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tsk_refcnt_debug_release(sk);\n\n\tsock_put(sk);\n\treturn 0;\n}\n\n/*\n *\tAttach a packet hook.\n */\n\nstatic int packet_do_bind(struct sock *sk, struct net_device *dev, __be16 protocol)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\t/*\n\t *\tDetach an existing hook if present.\n\t */\n\n\tlock_sock(sk);\n\n\tspin_lock(&po->bind_lock);\n\tif (po->running) {\n\t\t__sock_put(sk);\n\t\tpo->running = 0;\n\t\tpo->num = 0;\n\t\tspin_unlock(&po->bind_lock);\n\t\tdev_remove_pack(&po->prot_hook);\n\t\tspin_lock(&po->bind_lock);\n\t}\n\n\tpo->num = protocol;\n\tpo->prot_hook.type = protocol;\n\tpo->prot_hook.dev = dev;\n\n\tpo->ifindex = dev ? dev->ifindex : 0;\n\n\tif (protocol == 0)\n\t\tgoto out_unlock;\n\n\tif (!dev || (dev->flags & IFF_UP)) {\n\t\tdev_add_pack(&po->prot_hook);\n\t\tsock_hold(sk);\n\t\tpo->running = 1;\n\t} else {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\nout_unlock:\n\tspin_unlock(&po->bind_lock);\n\trelease_sock(sk);\n\treturn 0;\n}\n\n/*\n *\tBind a packet socket to a device\n */\n\nstatic int packet_bind_spkt(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t    int addr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tchar name[15];\n\tstruct net_device *dev;\n\tint err = -ENODEV;\n\n\t/*\n\t *\tCheck legality\n\t */\n\n\tif (addr_len != sizeof(struct sockaddr))\n\t\treturn -EINVAL;\n\tstrlcpy(name, uaddr->sa_data, sizeof(name));\n\n\tdev = dev_get_by_name(sock_net(sk), name);\n\tif (dev) {\n\t\terr = packet_do_bind(sk, dev, pkt_sk(sk)->num);\n\t\tdev_put(dev);\n\t}\n\treturn err;\n}\n\nstatic int packet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_ll *sll = (struct sockaddr_ll *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct net_device *dev = NULL;\n\tint err;\n\n\n\t/*\n\t *\tCheck legality\n\t */\n\n\tif (addr_len < sizeof(struct sockaddr_ll))\n\t\treturn -EINVAL;\n\tif (sll->sll_family != AF_PACKET)\n\t\treturn -EINVAL;\n\n\tif (sll->sll_ifindex) {\n\t\terr = -ENODEV;\n\t\tdev = dev_get_by_index(sock_net(sk), sll->sll_ifindex);\n\t\tif (dev == NULL)\n\t\t\tgoto out;\n\t}\n\terr = packet_do_bind(sk, dev, sll->sll_protocol ? : pkt_sk(sk)->num);\n\tif (dev)\n\t\tdev_put(dev);\n\nout:\n\treturn err;\n}\n\nstatic struct proto packet_proto = {\n\t.name\t  = \"PACKET\",\n\t.owner\t  = THIS_MODULE,\n\t.obj_size = sizeof(struct packet_sock),\n};\n\n/*\n *\tCreate a packet of type SOCK_PACKET.\n */\n\nstatic int packet_create(struct net *net, struct socket *sock, int protocol,\n\t\t\t int kern)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\t__be16 proto = (__force __be16)protocol; /* weird, but documented */\n\tint err;\n\n\tif (!capable(CAP_NET_RAW))\n\t\treturn -EPERM;\n\tif (sock->type != SOCK_DGRAM && sock->type != SOCK_RAW &&\n\t    sock->type != SOCK_PACKET)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\tsock->state = SS_UNCONNECTED;\n\n\terr = -ENOBUFS;\n\tsk = sk_alloc(net, PF_PACKET, GFP_KERNEL, &packet_proto);\n\tif (sk == NULL)\n\t\tgoto out;\n\n\tsock->ops = &packet_ops;\n\tif (sock->type == SOCK_PACKET)\n\t\tsock->ops = &packet_ops_spkt;\n\n\tsock_init_data(sock, sk);\n\n\tpo = pkt_sk(sk);\n\tsk->sk_family = PF_PACKET;\n\tpo->num = proto;\n\n\tsk->sk_destruct = packet_sock_destruct;\n\tsk_refcnt_debug_inc(sk);\n\n\t/*\n\t *\tAttach a protocol block\n\t */\n\n\tspin_lock_init(&po->bind_lock);\n\tmutex_init(&po->pg_vec_lock);\n\tpo->prot_hook.func = packet_rcv;\n\n\tif (sock->type == SOCK_PACKET)\n\t\tpo->prot_hook.func = packet_rcv_spkt;\n\n\tpo->prot_hook.af_packet_priv = sk;\n\n\tif (proto) {\n\t\tpo->prot_hook.type = proto;\n\t\tdev_add_pack(&po->prot_hook);\n\t\tsock_hold(sk);\n\t\tpo->running = 1;\n\t}\n\n\tspin_lock_bh(&net->packet.sklist_lock);\n\tsk_add_node_rcu(sk, &net->packet.sklist);\n\tsock_prot_inuse_add(net, &packet_proto, 1);\n\tspin_unlock_bh(&net->packet.sklist_lock);\n\n\treturn 0;\nout:\n\treturn err;\n}\n\nstatic int packet_recv_error(struct sock *sk, struct msghdr *msg, int len)\n{\n\tstruct sock_exterr_skb *serr;\n\tstruct sk_buff *skb, *skb2;\n\tint copied, err;\n\n\terr = -EAGAIN;\n\tskb = skb_dequeue(&sk->sk_error_queue);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto out_free_skb;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tserr = SKB_EXT_ERR(skb);\n\tput_cmsg(msg, SOL_PACKET, PACKET_TX_TIMESTAMP,\n\t\t sizeof(serr->ee), &serr->ee);\n\n\tmsg->msg_flags |= MSG_ERRQUEUE;\n\terr = copied;\n\n\t/* Reset and regenerate socket error */\n\tspin_lock_bh(&sk->sk_error_queue.lock);\n\tsk->sk_err = 0;\n\tif ((skb2 = skb_peek(&sk->sk_error_queue)) != NULL) {\n\t\tsk->sk_err = SKB_EXT_ERR(skb2)->ee.ee_errno;\n\t\tspin_unlock_bh(&sk->sk_error_queue.lock);\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tspin_unlock_bh(&sk->sk_error_queue.lock);\n\nout_free_skb:\n\tkfree_skb(skb);\nout:\n\treturn err;\n}\n\n/*\n *\tPull a packet from our receive queue and hand it to the user.\n *\tIf necessary we block.\n */\n\nstatic int packet_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\tstruct sockaddr_ll *sll;\n\tint vnet_hdr_len = 0;\n\n\terr = -EINVAL;\n\tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT|MSG_ERRQUEUE))\n\t\tgoto out;\n\n#if 0\n\t/* What error should we return now? EUNATTACH? */\n\tif (pkt_sk(sk)->ifindex < 0)\n\t\treturn -ENODEV;\n#endif\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = packet_recv_error(sk, msg, len);\n\t\tgoto out;\n\t}\n\n\t/*\n\t *\tCall the generic datagram receiver. This handles all sorts\n\t *\tof horrible races and re-entrancy so we can forget about it\n\t *\tin the protocol layers.\n\t *\n\t *\tNow it will return ENETDOWN, if device have just gone down,\n\t *\tbut then it will block.\n\t */\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\n\t/*\n\t *\tAn error occurred so return it. Because skb_recv_datagram()\n\t *\thandles the blocking we don't see and worry about blocking\n\t *\tretries.\n\t */\n\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tif (pkt_sk(sk)->has_vnet_hdr) {\n\t\tstruct virtio_net_hdr vnet_hdr = { 0 };\n\n\t\terr = -EINVAL;\n\t\tvnet_hdr_len = sizeof(vnet_hdr);\n\t\tif (len < vnet_hdr_len)\n\t\t\tgoto out_free;\n\n\t\tlen -= vnet_hdr_len;\n\n\t\tif (skb_is_gso(skb)) {\n\t\t\tstruct skb_shared_info *sinfo = skb_shinfo(skb);\n\n\t\t\t/* This is a hint as to how much should be linear. */\n\t\t\tvnet_hdr.hdr_len = skb_headlen(skb);\n\t\t\tvnet_hdr.gso_size = sinfo->gso_size;\n\t\t\tif (sinfo->gso_type & SKB_GSO_TCPV4)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_TCPV4;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_TCPV6)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_TCPV6;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_UDP)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_UDP;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_FCOE)\n\t\t\t\tgoto out_free;\n\t\t\telse\n\t\t\t\tBUG();\n\t\t\tif (sinfo->gso_type & SKB_GSO_TCP_ECN)\n\t\t\t\tvnet_hdr.gso_type |= VIRTIO_NET_HDR_GSO_ECN;\n\t\t} else\n\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tvnet_hdr.flags = VIRTIO_NET_HDR_F_NEEDS_CSUM;\n\t\t\tvnet_hdr.csum_start = skb_checksum_start_offset(skb);\n\t\t\tvnet_hdr.csum_offset = skb->csum_offset;\n\t\t} /* else everything is zero */\n\n\t\terr = memcpy_toiovec(msg->msg_iov, (void *)&vnet_hdr,\n\t\t\t\t     vnet_hdr_len);\n\t\tif (err < 0)\n\t\t\tgoto out_free;\n\t}\n\n\t/*\n\t *\tIf the address length field is there to be filled in, we fill\n\t *\tit in now.\n\t */\n\n\tsll = &PACKET_SKB_CB(skb)->sa.ll;\n\tif (sock->type == SOCK_PACKET)\n\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);\n\telse\n\t\tmsg->msg_namelen = sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr);\n\n\t/*\n\t *\tYou lose any data beyond the buffer you gave. If it worries a\n\t *\tuser program they can ask the device for its MTU anyway.\n\t */\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto out_free;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (msg->msg_name)\n\t\tmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,\n\t\t       msg->msg_namelen);\n\n\tif (pkt_sk(sk)->auxdata) {\n\t\tstruct tpacket_auxdata aux;\n\n\t\taux.tp_status = TP_STATUS_USER;\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\taux.tp_status |= TP_STATUS_CSUMNOTREADY;\n\t\taux.tp_len = PACKET_SKB_CB(skb)->origlen;\n\t\taux.tp_snaplen = skb->len;\n\t\taux.tp_mac = 0;\n\t\taux.tp_net = skb_network_offset(skb);\n\t\tif (vlan_tx_tag_present(skb)) {\n\t\t\taux.tp_vlan_tci = vlan_tx_tag_get(skb);\n\t\t\taux.tp_status |= TP_STATUS_VLAN_VALID;\n\t\t} else {\n\t\t\taux.tp_vlan_tci = 0;\n\t\t}\n\t\tput_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);\n\t}\n\n\t/*\n\t *\tFree or return the buffer as appropriate. Again this\n\t *\thides all the races and re-entrancy issues from us.\n\t */\n\terr = vnet_hdr_len + ((flags&MSG_TRUNC) ? skb->len : copied);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n}\n\nstatic int packet_getname_spkt(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t       int *uaddr_len, int peer)\n{\n\tstruct net_device *dev;\n\tstruct sock *sk\t= sock->sk;\n\n\tif (peer)\n\t\treturn -EOPNOTSUPP;\n\n\tuaddr->sa_family = AF_PACKET;\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(sock_net(sk), pkt_sk(sk)->ifindex);\n\tif (dev)\n\t\tstrncpy(uaddr->sa_data, dev->name, 14);\n\telse\n\t\tmemset(uaddr->sa_data, 0, 14);\n\trcu_read_unlock();\n\t*uaddr_len = sizeof(*uaddr);\n\n\treturn 0;\n}\n\nstatic int packet_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t  int *uaddr_len, int peer)\n{\n\tstruct net_device *dev;\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_ll *, sll, uaddr);\n\n\tif (peer)\n\t\treturn -EOPNOTSUPP;\n\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_ifindex = po->ifindex;\n\tsll->sll_protocol = po->num;\n\tsll->sll_pkttype = 0;\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(sock_net(sk), po->ifindex);\n\tif (dev) {\n\t\tsll->sll_hatype = dev->type;\n\t\tsll->sll_halen = dev->addr_len;\n\t\tmemcpy(sll->sll_addr, dev->dev_addr, dev->addr_len);\n\t} else {\n\t\tsll->sll_hatype = 0;\t/* Bad: we have no ARPHRD_UNSPEC */\n\t\tsll->sll_halen = 0;\n\t}\n\trcu_read_unlock();\n\t*uaddr_len = offsetof(struct sockaddr_ll, sll_addr) + sll->sll_halen;\n\n\treturn 0;\n}\n\nstatic int packet_dev_mc(struct net_device *dev, struct packet_mclist *i,\n\t\t\t int what)\n{\n\tswitch (i->type) {\n\tcase PACKET_MR_MULTICAST:\n\t\tif (i->alen != dev->addr_len)\n\t\t\treturn -EINVAL;\n\t\tif (what > 0)\n\t\t\treturn dev_mc_add(dev, i->addr);\n\t\telse\n\t\t\treturn dev_mc_del(dev, i->addr);\n\t\tbreak;\n\tcase PACKET_MR_PROMISC:\n\t\treturn dev_set_promiscuity(dev, what);\n\t\tbreak;\n\tcase PACKET_MR_ALLMULTI:\n\t\treturn dev_set_allmulti(dev, what);\n\t\tbreak;\n\tcase PACKET_MR_UNICAST:\n\t\tif (i->alen != dev->addr_len)\n\t\t\treturn -EINVAL;\n\t\tif (what > 0)\n\t\t\treturn dev_uc_add(dev, i->addr);\n\t\telse\n\t\t\treturn dev_uc_del(dev, i->addr);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic void packet_dev_mclist(struct net_device *dev, struct packet_mclist *i, int what)\n{\n\tfor ( ; i; i = i->next) {\n\t\tif (i->ifindex == dev->ifindex)\n\t\t\tpacket_dev_mc(dev, i, what);\n\t}\n}\n\nstatic int packet_mc_add(struct sock *sk, struct packet_mreq_max *mreq)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_mclist *ml, *i;\n\tstruct net_device *dev;\n\tint err;\n\n\trtnl_lock();\n\n\terr = -ENODEV;\n\tdev = __dev_get_by_index(sock_net(sk), mreq->mr_ifindex);\n\tif (!dev)\n\t\tgoto done;\n\n\terr = -EINVAL;\n\tif (mreq->mr_alen > dev->addr_len)\n\t\tgoto done;\n\n\terr = -ENOBUFS;\n\ti = kmalloc(sizeof(*i), GFP_KERNEL);\n\tif (i == NULL)\n\t\tgoto done;\n\n\terr = 0;\n\tfor (ml = po->mclist; ml; ml = ml->next) {\n\t\tif (ml->ifindex == mreq->mr_ifindex &&\n\t\t    ml->type == mreq->mr_type &&\n\t\t    ml->alen == mreq->mr_alen &&\n\t\t    memcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\n\t\t\tml->count++;\n\t\t\t/* Free the new element ... */\n\t\t\tkfree(i);\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\ti->type = mreq->mr_type;\n\ti->ifindex = mreq->mr_ifindex;\n\ti->alen = mreq->mr_alen;\n\tmemcpy(i->addr, mreq->mr_address, i->alen);\n\ti->count = 1;\n\ti->next = po->mclist;\n\tpo->mclist = i;\n\terr = packet_dev_mc(dev, i, 1);\n\tif (err) {\n\t\tpo->mclist = i->next;\n\t\tkfree(i);\n\t}\n\ndone:\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic int packet_mc_drop(struct sock *sk, struct packet_mreq_max *mreq)\n{\n\tstruct packet_mclist *ml, **mlp;\n\n\trtnl_lock();\n\n\tfor (mlp = &pkt_sk(sk)->mclist; (ml = *mlp) != NULL; mlp = &ml->next) {\n\t\tif (ml->ifindex == mreq->mr_ifindex &&\n\t\t    ml->type == mreq->mr_type &&\n\t\t    ml->alen == mreq->mr_alen &&\n\t\t    memcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\n\t\t\tif (--ml->count == 0) {\n\t\t\t\tstruct net_device *dev;\n\t\t\t\t*mlp = ml->next;\n\t\t\t\tdev = __dev_get_by_index(sock_net(sk), ml->ifindex);\n\t\t\t\tif (dev)\n\t\t\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\t\t\tkfree(ml);\n\t\t\t}\n\t\t\trtnl_unlock();\n\t\t\treturn 0;\n\t\t}\n\t}\n\trtnl_unlock();\n\treturn -EADDRNOTAVAIL;\n}\n\nstatic void packet_flush_mclist(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_mclist *ml;\n\n\tif (!po->mclist)\n\t\treturn;\n\n\trtnl_lock();\n\twhile ((ml = po->mclist) != NULL) {\n\t\tstruct net_device *dev;\n\n\t\tpo->mclist = ml->next;\n\t\tdev = __dev_get_by_index(sock_net(sk), ml->ifindex);\n\t\tif (dev != NULL)\n\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\tkfree(ml);\n\t}\n\trtnl_unlock();\n}\n\nstatic int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tstruct tpacket_req req;\n\n\t\tif (optlen < sizeof(req))\n\t\t\treturn -EINVAL;\n\t\tif (pkt_sk(sk)->has_vnet_hdr)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req, optval, sizeof(req)))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req, 0, optname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_version = val;\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}\n\nstatic int packet_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t     char __user *optval, int __user *optlen)\n{\n\tint len;\n\tint val;\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tvoid *data;\n\tstruct tpacket_stats st;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase PACKET_STATISTICS:\n\t\tif (len > sizeof(struct tpacket_stats))\n\t\t\tlen = sizeof(struct tpacket_stats);\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tst = po->stats;\n\t\tmemset(&po->stats, 0, sizeof(st));\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\tst.tp_packets += st.tp_drops;\n\n\t\tdata = &st;\n\t\tbreak;\n\tcase PACKET_AUXDATA:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tval = po->auxdata;\n\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_ORIGDEV:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tval = po->origdev;\n\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_VNET_HDR:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tval = po->has_vnet_hdr;\n\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_VERSION:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tval = po->tp_version;\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_HDRLEN:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tif (copy_from_user(&val, optval, len))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\t\tval = sizeof(struct tpacket_hdr);\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tval = sizeof(struct tpacket2_hdr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_RESERVE:\n\t\tif (len > sizeof(unsigned int))\n\t\t\tlen = sizeof(unsigned int);\n\t\tval = po->tp_reserve;\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_LOSS:\n\t\tif (len > sizeof(unsigned int))\n\t\t\tlen = sizeof(unsigned int);\n\t\tval = po->tp_loss;\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_TIMESTAMP:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tval = po->tp_tstamp;\n\t\tdata = &val;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, data, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n\nstatic int packet_notifier(struct notifier_block *this, unsigned long msg, void *data)\n{\n\tstruct sock *sk;\n\tstruct hlist_node *node;\n\tstruct net_device *dev = data;\n\tstruct net *net = dev_net(dev);\n\n\trcu_read_lock();\n\tsk_for_each_rcu(sk, node, &net->packet.sklist) {\n\t\tstruct packet_sock *po = pkt_sk(sk);\n\n\t\tswitch (msg) {\n\t\tcase NETDEV_UNREGISTER:\n\t\t\tif (po->mclist)\n\t\t\t\tpacket_dev_mclist(dev, po->mclist, -1);\n\t\t\t/* fallthrough */\n\n\t\tcase NETDEV_DOWN:\n\t\t\tif (dev->ifindex == po->ifindex) {\n\t\t\t\tspin_lock(&po->bind_lock);\n\t\t\t\tif (po->running) {\n\t\t\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\t\t\t__sock_put(sk);\n\t\t\t\t\tpo->running = 0;\n\t\t\t\t\tsk->sk_err = ENETDOWN;\n\t\t\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\t\t\tsk->sk_error_report(sk);\n\t\t\t\t}\n\t\t\t\tif (msg == NETDEV_UNREGISTER) {\n\t\t\t\t\tpo->ifindex = -1;\n\t\t\t\t\tpo->prot_hook.dev = NULL;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&po->bind_lock);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NETDEV_UP:\n\t\t\tif (dev->ifindex == po->ifindex) {\n\t\t\t\tspin_lock(&po->bind_lock);\n\t\t\t\tif (po->num && !po->running) {\n\t\t\t\t\tdev_add_pack(&po->prot_hook);\n\t\t\t\t\tsock_hold(sk);\n\t\t\t\t\tpo->running = 1;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&po->bind_lock);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn NOTIFY_DONE;\n}\n\n\nstatic int packet_ioctl(struct socket *sock, unsigned int cmd,\n\t\t\tunsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t{\n\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\tcase SIOCINQ:\n\t{\n\t\tstruct sk_buff *skb;\n\t\tint amount = 0;\n\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tamount = skb->len;\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\tcase SIOCGSTAMP:\n\t\treturn sock_get_timestamp(sk, (struct timeval __user *)arg);\n\tcase SIOCGSTAMPNS:\n\t\treturn sock_get_timestampns(sk, (struct timespec __user *)arg);\n\n#ifdef CONFIG_INET\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\tcase SIOCDARP:\n\tcase SIOCGARP:\n\tcase SIOCSARP:\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCSIFFLAGS:\n\t\treturn inet_dgram_ops.ioctl(sock, cmd, arg);\n#endif\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\treturn 0;\n}\n\nstatic unsigned int packet_poll(struct file *file, struct socket *sock,\n\t\t\t\tpoll_table *wait)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned int mask = datagram_poll(file, sock, wait);\n\n\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\tif (po->rx_ring.pg_vec) {\n\t\tif (!packet_previous_frame(po, &po->rx_ring, TP_STATUS_KERNEL))\n\t\t\tmask |= POLLIN | POLLRDNORM;\n\t}\n\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\tspin_lock_bh(&sk->sk_write_queue.lock);\n\tif (po->tx_ring.pg_vec) {\n\t\tif (packet_current_frame(po, &po->tx_ring, TP_STATUS_AVAILABLE))\n\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t}\n\tspin_unlock_bh(&sk->sk_write_queue.lock);\n\treturn mask;\n}\n\n\n/* Dirty? Well, I still did not learn better way to account\n * for user mmaps.\n */\n\nstatic void packet_mm_open(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct socket *sock = file->private_data;\n\tstruct sock *sk = sock->sk;\n\n\tif (sk)\n\t\tatomic_inc(&pkt_sk(sk)->mapped);\n}\n\nstatic void packet_mm_close(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct socket *sock = file->private_data;\n\tstruct sock *sk = sock->sk;\n\n\tif (sk)\n\t\tatomic_dec(&pkt_sk(sk)->mapped);\n}\n\nstatic const struct vm_operations_struct packet_mmap_ops = {\n\t.open\t=\tpacket_mm_open,\n\t.close\t=\tpacket_mm_close,\n};\n\nstatic void free_pg_vec(struct pgv *pg_vec, unsigned int order,\n\t\t\tunsigned int len)\n{\n\tint i;\n\n\tfor (i = 0; i < len; i++) {\n\t\tif (likely(pg_vec[i].buffer)) {\n\t\t\tif (is_vmalloc_addr(pg_vec[i].buffer))\n\t\t\t\tvfree(pg_vec[i].buffer);\n\t\t\telse\n\t\t\t\tfree_pages((unsigned long)pg_vec[i].buffer,\n\t\t\t\t\t   order);\n\t\t\tpg_vec[i].buffer = NULL;\n\t\t}\n\t}\n\tkfree(pg_vec);\n}\n\nstatic inline char *alloc_one_pg_vec_page(unsigned long order)\n{\n\tchar *buffer = NULL;\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_COMP |\n\t\t\t  __GFP_ZERO | __GFP_NOWARN | __GFP_NORETRY;\n\n\tbuffer = (char *) __get_free_pages(gfp_flags, order);\n\n\tif (buffer)\n\t\treturn buffer;\n\n\t/*\n\t * __get_free_pages failed, fall back to vmalloc\n\t */\n\tbuffer = vzalloc((1 << order) * PAGE_SIZE);\n\n\tif (buffer)\n\t\treturn buffer;\n\n\t/*\n\t * vmalloc failed, lets dig into swap here\n\t */\n\tgfp_flags &= ~__GFP_NORETRY;\n\tbuffer = (char *)__get_free_pages(gfp_flags, order);\n\tif (buffer)\n\t\treturn buffer;\n\n\t/*\n\t * complete and utter failure\n\t */\n\treturn NULL;\n}\n\nstatic struct pgv *alloc_pg_vec(struct tpacket_req *req, int order)\n{\n\tunsigned int block_nr = req->tp_block_nr;\n\tstruct pgv *pg_vec;\n\tint i;\n\n\tpg_vec = kcalloc(block_nr, sizeof(struct pgv), GFP_KERNEL);\n\tif (unlikely(!pg_vec))\n\t\tgoto out;\n\n\tfor (i = 0; i < block_nr; i++) {\n\t\tpg_vec[i].buffer = alloc_one_pg_vec_page(order);\n\t\tif (unlikely(!pg_vec[i].buffer))\n\t\t\tgoto out_free_pgvec;\n\t}\n\nout:\n\treturn pg_vec;\n\nout_free_pgvec:\n\tfree_pg_vec(pg_vec, order, block_nr);\n\tpg_vec = NULL;\n\tgoto out;\n}\n\nstatic int packet_set_ring(struct sock *sk, struct tpacket_req *req,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err;\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (atomic_read(&rb->pending))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_block_size & (PAGE_SIZE - 1)))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size/req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\t__dev_remove_pack(&po->prot_hook);\n\t\tpo->num = 0;\n\t\tpo->running = 0;\n\t\t__sock_put(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running && !po->running) {\n\t\tsock_hold(sk);\n\t\tpo->running = 1;\n\t\tpo->num = num;\n\t\tdev_add_pack(&po->prot_hook);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\trelease_sock(sk);\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\treturn err;\n}\n\nstatic int packet_mmap(struct file *file, struct socket *sock,\n\t\tstruct vm_area_struct *vma)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned long size, expected_size;\n\tstruct packet_ring_buffer *rb;\n\tunsigned long start;\n\tint err = -EINVAL;\n\tint i;\n\n\tif (vma->vm_pgoff)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&po->pg_vec_lock);\n\n\texpected_size = 0;\n\tfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\n\t\tif (rb->pg_vec) {\n\t\t\texpected_size += rb->pg_vec_len\n\t\t\t\t\t\t* rb->pg_vec_pages\n\t\t\t\t\t\t* PAGE_SIZE;\n\t\t}\n\t}\n\n\tif (expected_size == 0)\n\t\tgoto out;\n\n\tsize = vma->vm_end - vma->vm_start;\n\tif (size != expected_size)\n\t\tgoto out;\n\n\tstart = vma->vm_start;\n\tfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\n\t\tif (rb->pg_vec == NULL)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < rb->pg_vec_len; i++) {\n\t\t\tstruct page *page;\n\t\t\tvoid *kaddr = rb->pg_vec[i].buffer;\n\t\t\tint pg_num;\n\n\t\t\tfor (pg_num = 0; pg_num < rb->pg_vec_pages; pg_num++) {\n\t\t\t\tpage = pgv_to_page(kaddr);\n\t\t\t\terr = vm_insert_page(vma, start, page);\n\t\t\t\tif (unlikely(err))\n\t\t\t\t\tgoto out;\n\t\t\t\tstart += PAGE_SIZE;\n\t\t\t\tkaddr += PAGE_SIZE;\n\t\t\t}\n\t\t}\n\t}\n\n\tatomic_inc(&po->mapped);\n\tvma->vm_ops = &packet_mmap_ops;\n\terr = 0;\n\nout:\n\tmutex_unlock(&po->pg_vec_lock);\n\treturn err;\n}\n\nstatic const struct proto_ops packet_ops_spkt = {\n\t.family =\tPF_PACKET,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tpacket_release,\n\t.bind =\t\tpacket_bind_spkt,\n\t.connect =\tsock_no_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tpacket_getname_spkt,\n\t.poll =\t\tdatagram_poll,\n\t.ioctl =\tpacket_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tsock_no_setsockopt,\n\t.getsockopt =\tsock_no_getsockopt,\n\t.sendmsg =\tpacket_sendmsg_spkt,\n\t.recvmsg =\tpacket_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tsock_no_sendpage,\n};\n\nstatic const struct proto_ops packet_ops = {\n\t.family =\tPF_PACKET,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tpacket_release,\n\t.bind =\t\tpacket_bind,\n\t.connect =\tsock_no_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tpacket_getname,\n\t.poll =\t\tpacket_poll,\n\t.ioctl =\tpacket_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tpacket_setsockopt,\n\t.getsockopt =\tpacket_getsockopt,\n\t.sendmsg =\tpacket_sendmsg,\n\t.recvmsg =\tpacket_recvmsg,\n\t.mmap =\t\tpacket_mmap,\n\t.sendpage =\tsock_no_sendpage,\n};\n\nstatic const struct net_proto_family packet_family_ops = {\n\t.family =\tPF_PACKET,\n\t.create =\tpacket_create,\n\t.owner\t=\tTHIS_MODULE,\n};\n\nstatic struct notifier_block packet_netdev_notifier = {\n\t.notifier_call =\tpacket_notifier,\n};\n\n#ifdef CONFIG_PROC_FS\n\nstatic void *packet_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(RCU)\n{\n\tstruct net *net = seq_file_net(seq);\n\n\trcu_read_lock();\n\treturn seq_hlist_start_head_rcu(&net->packet.sklist, *pos);\n}\n\nstatic void *packet_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct net *net = seq_file_net(seq);\n\treturn seq_hlist_next_rcu(v, &net->packet.sklist, pos);\n}\n\nstatic void packet_seq_stop(struct seq_file *seq, void *v)\n\t__releases(RCU)\n{\n\trcu_read_unlock();\n}\n\nstatic int packet_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"sk       RefCnt Type Proto  Iface R Rmem   User   Inode\\n\");\n\telse {\n\t\tstruct sock *s = sk_entry(v);\n\t\tconst struct packet_sock *po = pkt_sk(s);\n\n\t\tseq_printf(seq,\n\t\t\t   \"%pK %-6d %-4d %04x   %-5d %1d %-6u %-6u %-6lu\\n\",\n\t\t\t   s,\n\t\t\t   atomic_read(&s->sk_refcnt),\n\t\t\t   s->sk_type,\n\t\t\t   ntohs(po->num),\n\t\t\t   po->ifindex,\n\t\t\t   po->running,\n\t\t\t   atomic_read(&s->sk_rmem_alloc),\n\t\t\t   sock_i_uid(s),\n\t\t\t   sock_i_ino(s));\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations packet_seq_ops = {\n\t.start\t= packet_seq_start,\n\t.next\t= packet_seq_next,\n\t.stop\t= packet_seq_stop,\n\t.show\t= packet_seq_show,\n};\n\nstatic int packet_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &packet_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations packet_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= packet_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\n#endif\n\nstatic int __net_init packet_net_init(struct net *net)\n{\n\tspin_lock_init(&net->packet.sklist_lock);\n\tINIT_HLIST_HEAD(&net->packet.sklist);\n\n\tif (!proc_net_fops_create(net, \"packet\", 0, &packet_seq_fops))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void __net_exit packet_net_exit(struct net *net)\n{\n\tproc_net_remove(net, \"packet\");\n}\n\nstatic struct pernet_operations packet_net_ops = {\n\t.init = packet_net_init,\n\t.exit = packet_net_exit,\n};\n\n\nstatic void __exit packet_exit(void)\n{\n\tunregister_netdevice_notifier(&packet_netdev_notifier);\n\tunregister_pernet_subsys(&packet_net_ops);\n\tsock_unregister(PF_PACKET);\n\tproto_unregister(&packet_proto);\n}\n\nstatic int __init packet_init(void)\n{\n\tint rc = proto_register(&packet_proto, 0);\n\n\tif (rc != 0)\n\t\tgoto out;\n\n\tsock_register(&packet_family_ops);\n\tregister_pernet_subsys(&packet_net_ops);\n\tregister_netdevice_notifier(&packet_netdev_notifier);\nout:\n\treturn rc;\n}\n\nmodule_init(packet_init);\nmodule_exit(packet_exit);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_NETPROTO(PF_PACKET);\n"], "fixing_code": ["#ifndef __LINUX_IF_PACKET_H\n#define __LINUX_IF_PACKET_H\n\n#include <linux/types.h>\n\nstruct sockaddr_pkt {\n\tunsigned short spkt_family;\n\tunsigned char spkt_device[14];\n\t__be16 spkt_protocol;\n};\n\nstruct sockaddr_ll {\n\tunsigned short\tsll_family;\n\t__be16\t\tsll_protocol;\n\tint\t\tsll_ifindex;\n\tunsigned short\tsll_hatype;\n\tunsigned char\tsll_pkttype;\n\tunsigned char\tsll_halen;\n\tunsigned char\tsll_addr[8];\n};\n\n/* Packet types */\n\n#define PACKET_HOST\t\t0\t\t/* To us\t\t*/\n#define PACKET_BROADCAST\t1\t\t/* To all\t\t*/\n#define PACKET_MULTICAST\t2\t\t/* To group\t\t*/\n#define PACKET_OTHERHOST\t3\t\t/* To someone else \t*/\n#define PACKET_OUTGOING\t\t4\t\t/* Outgoing of any type */\n/* These ones are invisible by user level */\n#define PACKET_LOOPBACK\t\t5\t\t/* MC/BRD frame looped back */\n#define PACKET_FASTROUTE\t6\t\t/* Fastrouted frame\t*/\n\n/* Packet socket options */\n\n#define PACKET_ADD_MEMBERSHIP\t\t1\n#define PACKET_DROP_MEMBERSHIP\t\t2\n#define PACKET_RECV_OUTPUT\t\t3\n/* Value 4 is still used by obsolete turbo-packet. */\n#define PACKET_RX_RING\t\t\t5\n#define PACKET_STATISTICS\t\t6\n#define PACKET_COPY_THRESH\t\t7\n#define PACKET_AUXDATA\t\t\t8\n#define PACKET_ORIGDEV\t\t\t9\n#define PACKET_VERSION\t\t\t10\n#define PACKET_HDRLEN\t\t\t11\n#define PACKET_RESERVE\t\t\t12\n#define PACKET_TX_RING\t\t\t13\n#define PACKET_LOSS\t\t\t14\n#define PACKET_VNET_HDR\t\t\t15\n#define PACKET_TX_TIMESTAMP\t\t16\n#define PACKET_TIMESTAMP\t\t17\n\nstruct tpacket_stats {\n\tunsigned int\ttp_packets;\n\tunsigned int\ttp_drops;\n};\n\nstruct tpacket_auxdata {\n\t__u32\t\ttp_status;\n\t__u32\t\ttp_len;\n\t__u32\t\ttp_snaplen;\n\t__u16\t\ttp_mac;\n\t__u16\t\ttp_net;\n\t__u16\t\ttp_vlan_tci;\n\t__u16\t\ttp_padding;\n};\n\n/* Rx ring - header status */\n#define TP_STATUS_KERNEL\t0x0\n#define TP_STATUS_USER\t\t0x1\n#define TP_STATUS_COPY\t\t0x2\n#define TP_STATUS_LOSING\t0x4\n#define TP_STATUS_CSUMNOTREADY\t0x8\n#define TP_STATUS_VLAN_VALID   0x10 /* auxdata has valid tp_vlan_tci */\n\n/* Tx ring - header status */\n#define TP_STATUS_AVAILABLE\t0x0\n#define TP_STATUS_SEND_REQUEST\t0x1\n#define TP_STATUS_SENDING\t0x2\n#define TP_STATUS_WRONG_FORMAT\t0x4\n\nstruct tpacket_hdr {\n\tunsigned long\ttp_status;\n\tunsigned int\ttp_len;\n\tunsigned int\ttp_snaplen;\n\tunsigned short\ttp_mac;\n\tunsigned short\ttp_net;\n\tunsigned int\ttp_sec;\n\tunsigned int\ttp_usec;\n};\n\n#define TPACKET_ALIGNMENT\t16\n#define TPACKET_ALIGN(x)\t(((x)+TPACKET_ALIGNMENT-1)&~(TPACKET_ALIGNMENT-1))\n#define TPACKET_HDRLEN\t\t(TPACKET_ALIGN(sizeof(struct tpacket_hdr)) + sizeof(struct sockaddr_ll))\n\nstruct tpacket2_hdr {\n\t__u32\t\ttp_status;\n\t__u32\t\ttp_len;\n\t__u32\t\ttp_snaplen;\n\t__u16\t\ttp_mac;\n\t__u16\t\ttp_net;\n\t__u32\t\ttp_sec;\n\t__u32\t\ttp_nsec;\n\t__u16\t\ttp_vlan_tci;\n\t__u16\t\ttp_padding;\n};\n\n#define TPACKET2_HDRLEN\t\t(TPACKET_ALIGN(sizeof(struct tpacket2_hdr)) + sizeof(struct sockaddr_ll))\n\nenum tpacket_versions {\n\tTPACKET_V1,\n\tTPACKET_V2,\n};\n\n/*\n   Frame structure:\n\n   - Start. Frame must be aligned to TPACKET_ALIGNMENT=16\n   - struct tpacket_hdr\n   - pad to TPACKET_ALIGNMENT=16\n   - struct sockaddr_ll\n   - Gap, chosen so that packet data (Start+tp_net) alignes to TPACKET_ALIGNMENT=16\n   - Start+tp_mac: [ Optional MAC header ]\n   - Start+tp_net: Packet data, aligned to TPACKET_ALIGNMENT=16.\n   - Pad to align to TPACKET_ALIGNMENT=16\n */\n\nstruct tpacket_req {\n\tunsigned int\ttp_block_size;\t/* Minimal size of contiguous block */\n\tunsigned int\ttp_block_nr;\t/* Number of blocks */\n\tunsigned int\ttp_frame_size;\t/* Size of frame */\n\tunsigned int\ttp_frame_nr;\t/* Total number of frames */\n};\n\nstruct packet_mreq {\n\tint\t\tmr_ifindex;\n\tunsigned short\tmr_type;\n\tunsigned short\tmr_alen;\n\tunsigned char\tmr_address[8];\n};\n\n#define PACKET_MR_MULTICAST\t0\n#define PACKET_MR_PROMISC\t1\n#define PACKET_MR_ALLMULTI\t2\n#define PACKET_MR_UNICAST\t3\n\n#endif\n", "/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tPACKET - implements raw packet sockets.\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tverify_area() now used correctly\n *\t\tAlan Cox\t:\tnew skbuff lists, look ma no backlogs!\n *\t\tAlan Cox\t:\ttidied skbuff lists.\n *\t\tAlan Cox\t:\tNow uses generic datagram routines I\n *\t\t\t\t\tadded. Also fixed the peek/read crash\n *\t\t\t\t\tfrom all old Linux datagram code.\n *\t\tAlan Cox\t:\tUses the improved datagram code.\n *\t\tAlan Cox\t:\tAdded NULL's for socket options.\n *\t\tAlan Cox\t:\tRe-commented the code.\n *\t\tAlan Cox\t:\tUse new kernel side addressing\n *\t\tRob Janssen\t:\tCorrect MTU usage.\n *\t\tDave Platt\t:\tCounter leaks caused by incorrect\n *\t\t\t\t\tinterrupt locking and some slightly\n *\t\t\t\t\tdubious gcc output. Can you read\n *\t\t\t\t\tcompiler: it said _VOLATILE_\n *\tRichard Kooijman\t:\tTimestamp fixes.\n *\t\tAlan Cox\t:\tNew buffers. Use sk->mac.raw.\n *\t\tAlan Cox\t:\tsendmsg/recvmsg support.\n *\t\tAlan Cox\t:\tProtocol setting support\n *\tAlexey Kuznetsov\t:\tUntied from IPv4 stack.\n *\tCyrus Durgin\t\t:\tFixed kerneld for kmod.\n *\tMichal Ostrowski        :       Module initialization cleanup.\n *         Ulises Alonso        :       Frame number limit removal and\n *                                      packet_set_ring memory leak.\n *\t\tEric Biederman\t:\tAllow for > 8 byte hardware addresses.\n *\t\t\t\t\tThe convention is that longer addresses\n *\t\t\t\t\twill simply extend the hardware address\n *\t\t\t\t\tbyte arrays at the end of sockaddr_ll\n *\t\t\t\t\tand packet_mreq.\n *\t\tJohann Baudy\t:\tAdded TX RING.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n */\n\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/capability.h>\n#include <linux/fcntl.h>\n#include <linux/socket.h>\n#include <linux/in.h>\n#include <linux/inet.h>\n#include <linux/netdevice.h>\n#include <linux/if_packet.h>\n#include <linux/wireless.h>\n#include <linux/kernel.h>\n#include <linux/kmod.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <net/net_namespace.h>\n#include <net/ip.h>\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <linux/errno.h>\n#include <linux/timer.h>\n#include <asm/system.h>\n#include <asm/uaccess.h>\n#include <asm/ioctls.h>\n#include <asm/page.h>\n#include <asm/cacheflush.h>\n#include <asm/io.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/poll.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/mutex.h>\n#include <linux/if_vlan.h>\n#include <linux/virtio_net.h>\n#include <linux/errqueue.h>\n#include <linux/net_tstamp.h>\n\n#ifdef CONFIG_INET\n#include <net/inet_common.h>\n#endif\n\n/*\n   Assumptions:\n   - if device has no dev->hard_header routine, it adds and removes ll header\n     inside itself. In this case ll header is invisible outside of device,\n     but higher levels still should reserve dev->hard_header_len.\n     Some devices are enough clever to reallocate skb, when header\n     will not fit to reserved space (tunnel), another ones are silly\n     (PPP).\n   - packet socket receives packets with pulled ll header,\n     so that SOCK_RAW should push it back.\n\nOn receive:\n-----------\n\nIncoming, dev->hard_header!=NULL\n   mac_header -> ll header\n   data       -> data\n\nOutgoing, dev->hard_header!=NULL\n   mac_header -> ll header\n   data       -> ll header\n\nIncoming, dev->hard_header==NULL\n   mac_header -> UNKNOWN position. It is very likely, that it points to ll\n\t\t header.  PPP makes it, that is wrong, because introduce\n\t\t assymetry between rx and tx paths.\n   data       -> data\n\nOutgoing, dev->hard_header==NULL\n   mac_header -> data. ll header is still not built!\n   data       -> data\n\nResume\n  If dev->hard_header==NULL we are unlikely to restore sensible ll header.\n\n\nOn transmit:\n------------\n\ndev->hard_header != NULL\n   mac_header -> ll header\n   data       -> ll header\n\ndev->hard_header == NULL (ll header is added by device, we cannot control it)\n   mac_header -> data\n   data       -> data\n\n   We should set nh.raw on output to correct posistion,\n   packet classifier depends on it.\n */\n\n/* Private packet socket structures. */\n\nstruct packet_mclist {\n\tstruct packet_mclist\t*next;\n\tint\t\t\tifindex;\n\tint\t\t\tcount;\n\tunsigned short\t\ttype;\n\tunsigned short\t\talen;\n\tunsigned char\t\taddr[MAX_ADDR_LEN];\n};\n/* identical to struct packet_mreq except it has\n * a longer address field.\n */\nstruct packet_mreq_max {\n\tint\t\tmr_ifindex;\n\tunsigned short\tmr_type;\n\tunsigned short\tmr_alen;\n\tunsigned char\tmr_address[MAX_ADDR_LEN];\n};\n\nstatic int packet_set_ring(struct sock *sk, struct tpacket_req *req,\n\t\tint closing, int tx_ring);\n\nstruct pgv {\n\tchar *buffer;\n};\n\nstruct packet_ring_buffer {\n\tstruct pgv\t\t*pg_vec;\n\tunsigned int\t\thead;\n\tunsigned int\t\tframes_per_block;\n\tunsigned int\t\tframe_size;\n\tunsigned int\t\tframe_max;\n\n\tunsigned int\t\tpg_vec_order;\n\tunsigned int\t\tpg_vec_pages;\n\tunsigned int\t\tpg_vec_len;\n\n\tatomic_t\t\tpending;\n};\n\nstruct packet_sock;\nstatic int tpacket_snd(struct packet_sock *po, struct msghdr *msg);\n\nstatic void packet_flush_mclist(struct sock *sk);\n\nstruct packet_sock {\n\t/* struct sock has to be the first member of packet_sock */\n\tstruct sock\t\tsk;\n\tstruct tpacket_stats\tstats;\n\tstruct packet_ring_buffer\trx_ring;\n\tstruct packet_ring_buffer\ttx_ring;\n\tint\t\t\tcopy_thresh;\n\tspinlock_t\t\tbind_lock;\n\tstruct mutex\t\tpg_vec_lock;\n\tunsigned int\t\trunning:1,\t/* prot_hook is attached*/\n\t\t\t\tauxdata:1,\n\t\t\t\torigdev:1,\n\t\t\t\thas_vnet_hdr:1;\n\tint\t\t\tifindex;\t/* bound device\t\t*/\n\t__be16\t\t\tnum;\n\tstruct packet_mclist\t*mclist;\n\tatomic_t\t\tmapped;\n\tenum tpacket_versions\ttp_version;\n\tunsigned int\t\ttp_hdrlen;\n\tunsigned int\t\ttp_reserve;\n\tunsigned int\t\ttp_loss:1;\n\tunsigned int\t\ttp_tstamp;\n\tstruct packet_type\tprot_hook ____cacheline_aligned_in_smp;\n};\n\nstruct packet_skb_cb {\n\tunsigned int origlen;\n\tunion {\n\t\tstruct sockaddr_pkt pkt;\n\t\tstruct sockaddr_ll ll;\n\t} sa;\n};\n\n#define PACKET_SKB_CB(__skb)\t((struct packet_skb_cb *)((__skb)->cb))\n\nstatic inline __pure struct page *pgv_to_page(void *addr)\n{\n\tif (is_vmalloc_addr(addr))\n\t\treturn vmalloc_to_page(addr);\n\treturn virt_to_page(addr);\n}\n\nstatic void __packet_set_status(struct packet_sock *po, void *frame, int status)\n{\n\tunion {\n\t\tstruct tpacket_hdr *h1;\n\t\tstruct tpacket2_hdr *h2;\n\t\tvoid *raw;\n\t} h;\n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_status = status;\n\t\tflush_dcache_page(pgv_to_page(&h.h1->tp_status));\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_status = status;\n\t\tflush_dcache_page(pgv_to_page(&h.h2->tp_status));\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"TPACKET version not supported\\n\");\n\t\tBUG();\n\t}\n\n\tsmp_wmb();\n}\n\nstatic int __packet_get_status(struct packet_sock *po, void *frame)\n{\n\tunion {\n\t\tstruct tpacket_hdr *h1;\n\t\tstruct tpacket2_hdr *h2;\n\t\tvoid *raw;\n\t} h;\n\n\tsmp_rmb();\n\n\th.raw = frame;\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\tflush_dcache_page(pgv_to_page(&h.h1->tp_status));\n\t\treturn h.h1->tp_status;\n\tcase TPACKET_V2:\n\t\tflush_dcache_page(pgv_to_page(&h.h2->tp_status));\n\t\treturn h.h2->tp_status;\n\tdefault:\n\t\tpr_err(\"TPACKET version not supported\\n\");\n\t\tBUG();\n\t\treturn 0;\n\t}\n}\n\nstatic void *packet_lookup_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tunsigned int position,\n\t\tint status)\n{\n\tunsigned int pg_vec_pos, frame_offset;\n\tunion {\n\t\tstruct tpacket_hdr *h1;\n\t\tstruct tpacket2_hdr *h2;\n\t\tvoid *raw;\n\t} h;\n\n\tpg_vec_pos = position / rb->frames_per_block;\n\tframe_offset = position % rb->frames_per_block;\n\n\th.raw = rb->pg_vec[pg_vec_pos].buffer +\n\t\t(frame_offset * rb->frame_size);\n\n\tif (status != __packet_get_status(po, h.raw))\n\t\treturn NULL;\n\n\treturn h.raw;\n}\n\nstatic inline void *packet_current_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status)\n{\n\treturn packet_lookup_frame(po, rb, rb->head, status);\n}\n\nstatic inline void *packet_previous_frame(struct packet_sock *po,\n\t\tstruct packet_ring_buffer *rb,\n\t\tint status)\n{\n\tunsigned int previous = rb->head ? rb->head - 1 : rb->frame_max;\n\treturn packet_lookup_frame(po, rb, previous, status);\n}\n\nstatic inline void packet_increment_head(struct packet_ring_buffer *buff)\n{\n\tbuff->head = buff->head != buff->frame_max ? buff->head+1 : 0;\n}\n\nstatic inline struct packet_sock *pkt_sk(struct sock *sk)\n{\n\treturn (struct packet_sock *)sk;\n}\n\nstatic void packet_sock_destruct(struct sock *sk)\n{\n\tskb_queue_purge(&sk->sk_error_queue);\n\n\tWARN_ON(atomic_read(&sk->sk_rmem_alloc));\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_err(\"Attempt to release alive packet socket: %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tsk_refcnt_debug_dec(sk);\n}\n\n\nstatic const struct proto_ops packet_ops;\n\nstatic const struct proto_ops packet_ops_spkt;\n\nstatic int packet_rcv_spkt(struct sk_buff *skb, struct net_device *dev,\n\t\t\t   struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct sockaddr_pkt *spkt;\n\n\t/*\n\t *\tWhen we registered the protocol we saved the socket in the data\n\t *\tfield for just this event.\n\t */\n\n\tsk = pt->af_packet_priv;\n\n\t/*\n\t *\tYank back the headers [hope the device set this\n\t *\tright or kerboom...]\n\t *\n\t *\tIncoming packets have ll header pulled,\n\t *\tpush it back.\n\t *\n\t *\tFor outgoing ones skb->data == skb_mac_header(skb)\n\t *\tso that this procedure is noop.\n\t */\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto out;\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto out;\n\n\tskb = skb_share_check(skb, GFP_ATOMIC);\n\tif (skb == NULL)\n\t\tgoto oom;\n\n\t/* drop any routing info */\n\tskb_dst_drop(skb);\n\n\t/* drop conntrack reference */\n\tnf_reset(skb);\n\n\tspkt = &PACKET_SKB_CB(skb)->sa.pkt;\n\n\tskb_push(skb, skb->data - skb_mac_header(skb));\n\n\t/*\n\t *\tThe SOCK_PACKET socket receives _all_ frames.\n\t */\n\n\tspkt->spkt_family = dev->type;\n\tstrlcpy(spkt->spkt_device, dev->name, sizeof(spkt->spkt_device));\n\tspkt->spkt_protocol = skb->protocol;\n\n\t/*\n\t *\tCharge the memory to the socket. This is done specifically\n\t *\tto prevent sockets using all the memory up.\n\t */\n\n\tif (sock_queue_rcv_skb(sk, skb) == 0)\n\t\treturn 0;\n\nout:\n\tkfree_skb(skb);\noom:\n\treturn 0;\n}\n\n\n/*\n *\tOutput a raw packet to a device layer. This bypasses all the other\n *\tprotocol layers and you must therefore supply it with a complete frame\n */\n\nstatic int packet_sendmsg_spkt(struct kiocb *iocb, struct socket *sock,\n\t\t\t       struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_pkt *saddr = (struct sockaddr_pkt *)msg->msg_name;\n\tstruct sk_buff *skb = NULL;\n\tstruct net_device *dev;\n\t__be16 proto = 0;\n\tint err;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (saddr) {\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr))\n\t\t\treturn -EINVAL;\n\t\tif (msg->msg_namelen == sizeof(struct sockaddr_pkt))\n\t\t\tproto = saddr->spkt_protocol;\n\t} else\n\t\treturn -ENOTCONN;\t/* SOCK_PACKET must be sent giving an address */\n\n\t/*\n\t *\tFind the device first to size check it\n\t */\n\n\tsaddr->spkt_device[13] = 0;\nretry:\n\trcu_read_lock();\n\tdev = dev_get_by_name_rcu(sock_net(sk), saddr->spkt_device);\n\terr = -ENODEV;\n\tif (dev == NULL)\n\t\tgoto out_unlock;\n\n\terr = -ENETDOWN;\n\tif (!(dev->flags & IFF_UP))\n\t\tgoto out_unlock;\n\n\t/*\n\t * You may not queue a frame bigger than the mtu. This is the lowest level\n\t * raw protocol and you must do your own fragmentation at this level.\n\t */\n\n\terr = -EMSGSIZE;\n\tif (len > dev->mtu + dev->hard_header_len + VLAN_HLEN)\n\t\tgoto out_unlock;\n\n\tif (!skb) {\n\t\tsize_t reserved = LL_RESERVED_SPACE(dev);\n\t\tunsigned int hhlen = dev->header_ops ? dev->hard_header_len : 0;\n\n\t\trcu_read_unlock();\n\t\tskb = sock_wmalloc(sk, len + reserved, 0, GFP_KERNEL);\n\t\tif (skb == NULL)\n\t\t\treturn -ENOBUFS;\n\t\t/* FIXME: Save some space for broken drivers that write a hard\n\t\t * header at transmission time by themselves. PPP is the notable\n\t\t * one here. This should really be fixed at the driver level.\n\t\t */\n\t\tskb_reserve(skb, reserved);\n\t\tskb_reset_network_header(skb);\n\n\t\t/* Try to align data part correctly */\n\t\tif (hhlen) {\n\t\t\tskb->data -= hhlen;\n\t\t\tskb->tail -= hhlen;\n\t\t\tif (len < hhlen)\n\t\t\t\tskb_reset_network_header(skb);\n\t\t}\n\t\terr = memcpy_fromiovec(skb_put(skb, len), msg->msg_iov, len);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tgoto retry;\n\t}\n\n\tif (len > (dev->mtu + dev->hard_header_len)) {\n\t\t/* Earlier code assumed this would be a VLAN pkt,\n\t\t * double-check this now that we have the actual\n\t\t * packet in hand.\n\t\t */\n\t\tstruct ethhdr *ehdr;\n\t\tskb_reset_mac_header(skb);\n\t\tehdr = eth_hdr(skb);\n\t\tif (ehdr->h_proto != htons(ETH_P_8021Q)) {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\terr = sock_tx_timestamp(sk, &skb_shinfo(skb)->tx_flags);\n\tif (err < 0)\n\t\tgoto out_unlock;\n\n\tdev_queue_xmit(skb);\n\trcu_read_unlock();\n\treturn len;\n\nout_unlock:\n\trcu_read_unlock();\nout_free:\n\tkfree_skb(skb);\n\treturn err;\n}\n\nstatic inline unsigned int run_filter(const struct sk_buff *skb,\n\t\t\t\t      const struct sock *sk,\n\t\t\t\t      unsigned int res)\n{\n\tstruct sk_filter *filter;\n\n\trcu_read_lock();\n\tfilter = rcu_dereference(sk->sk_filter);\n\tif (filter != NULL)\n\t\tres = SK_RUN_FILTER(filter, skb);\n\trcu_read_unlock();\n\n\treturn res;\n}\n\n/*\n * This function makes lazy skb cloning in hope that most of packets\n * are discarded by BPF.\n *\n * Note tricky part: we DO mangle shared skb! skb->data, skb->len\n * and skb->cb are mangled. It works because (and until) packets\n * falling here are owned by current CPU. Output packets are cloned\n * by dev_queue_xmit_nit(), input packets are processed by net_bh\n * sequencially, so that if we return skb to original state on exit,\n * we will not harm anyone.\n */\n\nstatic int packet_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t      struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct sockaddr_ll *sll;\n\tstruct packet_sock *po;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tskb->dev = dev;\n\n\tif (dev->header_ops) {\n\t\t/* The device has an explicit notion of ll header,\n\t\t * exported to higher levels.\n\t\t *\n\t\t * Otherwise, the device hides details of its frame\n\t\t * structure, so that corresponding packet head is\n\t\t * never delivered to user.\n\t\t */\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (atomic_read(&sk->sk_rmem_alloc) + skb->truesize >=\n\t    (unsigned)sk->sk_rcvbuf)\n\t\tgoto drop_n_acct;\n\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, GFP_ATOMIC);\n\t\tif (nskb == NULL)\n\t\t\tgoto drop_n_acct;\n\n\t\tif (skb_head != skb->data) {\n\t\t\tskb->data = skb_head;\n\t\t\tskb->len = skb_len;\n\t\t}\n\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\n\tBUILD_BUG_ON(sizeof(*PACKET_SKB_CB(skb)) + MAX_ADDR_LEN - 8 >\n\t\t     sizeof(skb->cb));\n\n\tsll = &PACKET_SKB_CB(skb)->sa.ll;\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\n\tPACKET_SKB_CB(skb)->origlen = skb->len;\n\n\tif (pskb_trim(skb, snaplen))\n\t\tgoto drop_n_acct;\n\n\tskb_set_owner_r(skb, sk);\n\tskb->dev = NULL;\n\tskb_dst_drop(skb);\n\n\t/* drop conntrack reference */\n\tnf_reset(skb);\n\n\tspin_lock(&sk->sk_receive_queue.lock);\n\tpo->stats.tp_packets++;\n\tskb->dropcount = atomic_read(&sk->sk_drops);\n\t__skb_queue_tail(&sk->sk_receive_queue, skb);\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tsk->sk_data_ready(sk, skb->len);\n\treturn 0;\n\ndrop_n_acct:\n\tpo->stats.tp_drops = atomic_inc_return(&sk->sk_drops);\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tconsume_skb(skb);\n\treturn 0;\n}\n\nstatic int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion {\n\t\tstruct tpacket_hdr *h1;\n\t\tstruct tpacket2_hdr *h2;\n\t\tvoid *raw;\n\t} h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_LOSING|TP_STATUS_USER;\n\tunsigned short macoff, netoff, hdrlen;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timeval tv;\n\tstruct timespec ts;\n\tstruct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev->header_ops) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\tpo->tp_reserve;\n\t\tmacoff = netoff - maclen;\n\t}\n\n\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\tif (po->copy_thresh &&\n\t\t    atomic_read(&sk->sk_rmem_alloc) + skb->truesize <\n\t\t    (unsigned)sk->sk_rcvbuf) {\n\t\t\tif (skb_shared(skb)) {\n\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t} else {\n\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\tskb_head = skb->data;\n\t\t\t}\n\t\t\tif (copy_skb)\n\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t}\n\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\tif ((int)snaplen < 0)\n\t\t\tsnaplen = 0;\n\t}\n\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_frame(po, &po->rx_ring, TP_STATUS_KERNEL);\n\tif (!h.raw)\n\t\tgoto ring_is_full;\n\tpacket_increment_head(&po->rx_ring);\n\tpo->stats.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tif (!po->stats.tp_drops)\n\t\tstatus &= ~TP_STATUS_LOSING;\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\tif ((po->tp_tstamp & SOF_TIMESTAMPING_SYS_HARDWARE)\n\t\t\t\t&& shhwtstamps->syststamp.tv64)\n\t\t\ttv = ktime_to_timeval(shhwtstamps->syststamp);\n\t\telse if ((po->tp_tstamp & SOF_TIMESTAMPING_RAW_HARDWARE)\n\t\t\t\t&& shhwtstamps->hwtstamp.tv64)\n\t\t\ttv = ktime_to_timeval(shhwtstamps->hwtstamp);\n\t\telse if (skb->tstamp.tv64)\n\t\t\ttv = ktime_to_timeval(skb->tstamp);\n\t\telse\n\t\t\tdo_gettimeofday(&tv);\n\t\th.h1->tp_sec = tv.tv_sec;\n\t\th.h1->tp_usec = tv.tv_usec;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\tif ((po->tp_tstamp & SOF_TIMESTAMPING_SYS_HARDWARE)\n\t\t\t\t&& shhwtstamps->syststamp.tv64)\n\t\t\tts = ktime_to_timespec(shhwtstamps->syststamp);\n\t\telse if ((po->tp_tstamp & SOF_TIMESTAMPING_RAW_HARDWARE)\n\t\t\t\t&& shhwtstamps->hwtstamp.tv64)\n\t\t\tts = ktime_to_timespec(shhwtstamps->hwtstamp);\n\t\telse if (skb->tstamp.tv64)\n\t\t\tts = ktime_to_timespec(skb->tstamp);\n\t\telse\n\t\t\tgetnstimeofday(&ts);\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (vlan_tx_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = vlan_tx_tag_get(skb);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t}\n\t\th.h2->tp_padding = 0;\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\t__packet_set_status(po, h.raw, status);\n\tsmp_mb();\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\t{\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *)PAGE_ALIGN((unsigned long)h.raw + macoff + snaplen);\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n#endif\n\n\tsk->sk_data_ready(sk, 0);\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tkfree_skb(skb);\n\treturn 0;\n\nring_is_full:\n\tpo->stats.tp_drops++;\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tsk->sk_data_ready(sk, 0);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}\n\nstatic void tpacket_destruct_skb(struct sk_buff *skb)\n{\n\tstruct packet_sock *po = pkt_sk(skb->sk);\n\tvoid *ph;\n\n\tBUG_ON(skb == NULL);\n\n\tif (likely(po->tx_ring.pg_vec)) {\n\t\tph = skb_shinfo(skb)->destructor_arg;\n\t\tBUG_ON(__packet_get_status(po, ph) != TP_STATUS_SENDING);\n\t\tBUG_ON(atomic_read(&po->tx_ring.pending) == 0);\n\t\tatomic_dec(&po->tx_ring.pending);\n\t\t__packet_set_status(po, ph, TP_STATUS_AVAILABLE);\n\t}\n\n\tsock_wfree(skb);\n}\n\nstatic int tpacket_fill_skb(struct packet_sock *po, struct sk_buff *skb,\n\t\tvoid *frame, struct net_device *dev, int size_max,\n\t\t__be16 proto, unsigned char *addr)\n{\n\tunion {\n\t\tstruct tpacket_hdr *h1;\n\t\tstruct tpacket2_hdr *h2;\n\t\tvoid *raw;\n\t} ph;\n\tint to_write, offset, len, tp_len, nr_frags, len_max;\n\tstruct socket *sock = po->sk.sk_socket;\n\tstruct page *page;\n\tvoid *data;\n\tint err;\n\n\tph.raw = frame;\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = po->sk.sk_priority;\n\tskb->mark = po->sk.sk_mark;\n\tskb_shinfo(skb)->destructor_arg = ph.raw;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V2:\n\t\ttp_len = ph.h2->tp_len;\n\t\tbreak;\n\tdefault:\n\t\ttp_len = ph.h1->tp_len;\n\t\tbreak;\n\t}\n\tif (unlikely(tp_len > size_max)) {\n\t\tpr_err(\"packet size is too long (%d > %d)\\n\", tp_len, size_max);\n\t\treturn -EMSGSIZE;\n\t}\n\n\tskb_reserve(skb, LL_RESERVED_SPACE(dev));\n\tskb_reset_network_header(skb);\n\n\tdata = ph.raw + po->tp_hdrlen - sizeof(struct sockaddr_ll);\n\tto_write = tp_len;\n\n\tif (sock->type == SOCK_DGRAM) {\n\t\terr = dev_hard_header(skb, dev, ntohs(proto), addr,\n\t\t\t\tNULL, tp_len);\n\t\tif (unlikely(err < 0))\n\t\t\treturn -EINVAL;\n\t} else if (dev->hard_header_len) {\n\t\t/* net device doesn't like empty head */\n\t\tif (unlikely(tp_len <= dev->hard_header_len)) {\n\t\t\tpr_err(\"packet size is too short (%d < %d)\\n\",\n\t\t\t       tp_len, dev->hard_header_len);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tskb_push(skb, dev->hard_header_len);\n\t\terr = skb_store_bits(skb, 0, data,\n\t\t\t\tdev->hard_header_len);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\n\t\tdata += dev->hard_header_len;\n\t\tto_write -= dev->hard_header_len;\n\t}\n\n\terr = -EFAULT;\n\toffset = offset_in_page(data);\n\tlen_max = PAGE_SIZE - offset;\n\tlen = ((to_write > len_max) ? len_max : to_write);\n\n\tskb->data_len = to_write;\n\tskb->len += to_write;\n\tskb->truesize += to_write;\n\tatomic_add(to_write, &po->sk.sk_wmem_alloc);\n\n\twhile (likely(to_write)) {\n\t\tnr_frags = skb_shinfo(skb)->nr_frags;\n\n\t\tif (unlikely(nr_frags >= MAX_SKB_FRAGS)) {\n\t\t\tpr_err(\"Packet exceed the number of skb frags(%lu)\\n\",\n\t\t\t       MAX_SKB_FRAGS);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tpage = pgv_to_page(data);\n\t\tdata += len;\n\t\tflush_dcache_page(page);\n\t\tget_page(page);\n\t\tskb_fill_page_desc(skb, nr_frags, page, offset, len);\n\t\tto_write -= len;\n\t\toffset = 0;\n\t\tlen_max = PAGE_SIZE;\n\t\tlen = ((to_write > len_max) ? len_max : to_write);\n\t}\n\n\treturn tp_len;\n}\n\nstatic int tpacket_snd(struct packet_sock *po, struct msghdr *msg)\n{\n\tstruct sk_buff *skb;\n\tstruct net_device *dev;\n\t__be16 proto;\n\tint ifindex, err, reserve = 0;\n\tvoid *ph;\n\tstruct sockaddr_ll *saddr = (struct sockaddr_ll *)msg->msg_name;\n\tint tp_len, size_max;\n\tunsigned char *addr;\n\tint len_sum = 0;\n\tint status = 0;\n\n\tmutex_lock(&po->pg_vec_lock);\n\n\terr = -EBUSY;\n\tif (saddr == NULL) {\n\t\tifindex\t= po->ifindex;\n\t\tproto\t= po->num;\n\t\taddr\t= NULL;\n\t} else {\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll))\n\t\t\tgoto out;\n\t\tif (msg->msg_namelen < (saddr->sll_halen\n\t\t\t\t\t+ offsetof(struct sockaddr_ll,\n\t\t\t\t\t\tsll_addr)))\n\t\t\tgoto out;\n\t\tifindex\t= saddr->sll_ifindex;\n\t\tproto\t= saddr->sll_protocol;\n\t\taddr\t= saddr->sll_addr;\n\t}\n\n\tdev = dev_get_by_index(sock_net(&po->sk), ifindex);\n\terr = -ENXIO;\n\tif (unlikely(dev == NULL))\n\t\tgoto out;\n\n\treserve = dev->hard_header_len;\n\n\terr = -ENETDOWN;\n\tif (unlikely(!(dev->flags & IFF_UP)))\n\t\tgoto out_put;\n\n\tsize_max = po->tx_ring.frame_size\n\t\t- (po->tp_hdrlen - sizeof(struct sockaddr_ll));\n\n\tif (size_max > dev->mtu + reserve)\n\t\tsize_max = dev->mtu + reserve;\n\n\tdo {\n\t\tph = packet_current_frame(po, &po->tx_ring,\n\t\t\t\tTP_STATUS_SEND_REQUEST);\n\n\t\tif (unlikely(ph == NULL)) {\n\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\n\t\tstatus = TP_STATUS_SEND_REQUEST;\n\t\tskb = sock_alloc_send_skb(&po->sk,\n\t\t\t\tLL_ALLOCATED_SPACE(dev)\n\t\t\t\t+ sizeof(struct sockaddr_ll),\n\t\t\t\t0, &err);\n\n\t\tif (unlikely(skb == NULL))\n\t\t\tgoto out_status;\n\n\t\ttp_len = tpacket_fill_skb(po, skb, ph, dev, size_max, proto,\n\t\t\t\taddr);\n\n\t\tif (unlikely(tp_len < 0)) {\n\t\t\tif (po->tp_loss) {\n\t\t\t\t__packet_set_status(po, ph,\n\t\t\t\t\t\tTP_STATUS_AVAILABLE);\n\t\t\t\tpacket_increment_head(&po->tx_ring);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tstatus = TP_STATUS_WRONG_FORMAT;\n\t\t\t\terr = tp_len;\n\t\t\t\tgoto out_status;\n\t\t\t}\n\t\t}\n\n\t\tskb->destructor = tpacket_destruct_skb;\n\t\t__packet_set_status(po, ph, TP_STATUS_SENDING);\n\t\tatomic_inc(&po->tx_ring.pending);\n\n\t\tstatus = TP_STATUS_SEND_REQUEST;\n\t\terr = dev_queue_xmit(skb);\n\t\tif (unlikely(err > 0)) {\n\t\t\terr = net_xmit_errno(err);\n\t\t\tif (err && __packet_get_status(po, ph) ==\n\t\t\t\t   TP_STATUS_AVAILABLE) {\n\t\t\t\t/* skb was destructed already */\n\t\t\t\tskb = NULL;\n\t\t\t\tgoto out_status;\n\t\t\t}\n\t\t\t/*\n\t\t\t * skb was dropped but not destructed yet;\n\t\t\t * let's treat it like congestion or err < 0\n\t\t\t */\n\t\t\terr = 0;\n\t\t}\n\t\tpacket_increment_head(&po->tx_ring);\n\t\tlen_sum += tp_len;\n\t} while (likely((ph != NULL) ||\n\t\t\t((!(msg->msg_flags & MSG_DONTWAIT)) &&\n\t\t\t (atomic_read(&po->tx_ring.pending))))\n\t\t);\n\n\terr = len_sum;\n\tgoto out_put;\n\nout_status:\n\t__packet_set_status(po, ph, status);\n\tkfree_skb(skb);\nout_put:\n\tdev_put(dev);\nout:\n\tmutex_unlock(&po->pg_vec_lock);\n\treturn err;\n}\n\nstatic inline struct sk_buff *packet_alloc_skb(struct sock *sk, size_t prepad,\n\t\t\t\t\t       size_t reserve, size_t len,\n\t\t\t\t\t       size_t linear, int noblock,\n\t\t\t\t\t       int *err)\n{\n\tstruct sk_buff *skb;\n\n\t/* Under a page?  Don't bother with paged skb. */\n\tif (prepad + len < PAGE_SIZE || !linear)\n\t\tlinear = len;\n\n\tskb = sock_alloc_send_pskb(sk, prepad + linear, len - linear, noblock,\n\t\t\t\t   err);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, reserve);\n\tskb_put(skb, linear);\n\tskb->data_len = len - linear;\n\tskb->len += len - linear;\n\n\treturn skb;\n}\n\nstatic int packet_snd(struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sockaddr_ll *saddr = (struct sockaddr_ll *)msg->msg_name;\n\tstruct sk_buff *skb;\n\tstruct net_device *dev;\n\t__be16 proto;\n\tunsigned char *addr;\n\tint ifindex, err, reserve = 0;\n\tstruct virtio_net_hdr vnet_hdr = { 0 };\n\tint offset = 0;\n\tint vnet_hdr_len;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned short gso_type = 0;\n\n\t/*\n\t *\tGet and verify the address.\n\t */\n\n\tif (saddr == NULL) {\n\t\tifindex\t= po->ifindex;\n\t\tproto\t= po->num;\n\t\taddr\t= NULL;\n\t} else {\n\t\terr = -EINVAL;\n\t\tif (msg->msg_namelen < sizeof(struct sockaddr_ll))\n\t\t\tgoto out;\n\t\tif (msg->msg_namelen < (saddr->sll_halen + offsetof(struct sockaddr_ll, sll_addr)))\n\t\t\tgoto out;\n\t\tifindex\t= saddr->sll_ifindex;\n\t\tproto\t= saddr->sll_protocol;\n\t\taddr\t= saddr->sll_addr;\n\t}\n\n\n\tdev = dev_get_by_index(sock_net(sk), ifindex);\n\terr = -ENXIO;\n\tif (dev == NULL)\n\t\tgoto out_unlock;\n\tif (sock->type == SOCK_RAW)\n\t\treserve = dev->hard_header_len;\n\n\terr = -ENETDOWN;\n\tif (!(dev->flags & IFF_UP))\n\t\tgoto out_unlock;\n\n\tif (po->has_vnet_hdr) {\n\t\tvnet_hdr_len = sizeof(vnet_hdr);\n\n\t\terr = -EINVAL;\n\t\tif (len < vnet_hdr_len)\n\t\t\tgoto out_unlock;\n\n\t\tlen -= vnet_hdr_len;\n\n\t\terr = memcpy_fromiovec((void *)&vnet_hdr, msg->msg_iov,\n\t\t\t\t       vnet_hdr_len);\n\t\tif (err < 0)\n\t\t\tgoto out_unlock;\n\n\t\tif ((vnet_hdr.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\n\t\t    (vnet_hdr.csum_start + vnet_hdr.csum_offset + 2 >\n\t\t      vnet_hdr.hdr_len))\n\t\t\tvnet_hdr.hdr_len = vnet_hdr.csum_start +\n\t\t\t\t\t\t vnet_hdr.csum_offset + 2;\n\n\t\terr = -EINVAL;\n\t\tif (vnet_hdr.hdr_len > len)\n\t\t\tgoto out_unlock;\n\n\t\tif (vnet_hdr.gso_type != VIRTIO_NET_HDR_GSO_NONE) {\n\t\t\tswitch (vnet_hdr.gso_type & ~VIRTIO_NET_HDR_GSO_ECN) {\n\t\t\tcase VIRTIO_NET_HDR_GSO_TCPV4:\n\t\t\t\tgso_type = SKB_GSO_TCPV4;\n\t\t\t\tbreak;\n\t\t\tcase VIRTIO_NET_HDR_GSO_TCPV6:\n\t\t\t\tgso_type = SKB_GSO_TCPV6;\n\t\t\t\tbreak;\n\t\t\tcase VIRTIO_NET_HDR_GSO_UDP:\n\t\t\t\tgso_type = SKB_GSO_UDP;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\n\t\t\tif (vnet_hdr.gso_type & VIRTIO_NET_HDR_GSO_ECN)\n\t\t\t\tgso_type |= SKB_GSO_TCP_ECN;\n\n\t\t\tif (vnet_hdr.gso_size == 0)\n\t\t\t\tgoto out_unlock;\n\n\t\t}\n\t}\n\n\terr = -EMSGSIZE;\n\tif (!gso_type && (len > dev->mtu + reserve + VLAN_HLEN))\n\t\tgoto out_unlock;\n\n\terr = -ENOBUFS;\n\tskb = packet_alloc_skb(sk, LL_ALLOCATED_SPACE(dev),\n\t\t\t       LL_RESERVED_SPACE(dev), len, vnet_hdr.hdr_len,\n\t\t\t       msg->msg_flags & MSG_DONTWAIT, &err);\n\tif (skb == NULL)\n\t\tgoto out_unlock;\n\n\tskb_set_network_header(skb, reserve);\n\n\terr = -EINVAL;\n\tif (sock->type == SOCK_DGRAM &&\n\t    (offset = dev_hard_header(skb, dev, ntohs(proto), addr, NULL, len)) < 0)\n\t\tgoto out_free;\n\n\t/* Returns -EFAULT on error */\n\terr = skb_copy_datagram_from_iovec(skb, offset, msg->msg_iov, 0, len);\n\tif (err)\n\t\tgoto out_free;\n\terr = sock_tx_timestamp(sk, &skb_shinfo(skb)->tx_flags);\n\tif (err < 0)\n\t\tgoto out_free;\n\n\tif (!gso_type && (len > dev->mtu + reserve)) {\n\t\t/* Earlier code assumed this would be a VLAN pkt,\n\t\t * double-check this now that we have the actual\n\t\t * packet in hand.\n\t\t */\n\t\tstruct ethhdr *ehdr;\n\t\tskb_reset_mac_header(skb);\n\t\tehdr = eth_hdr(skb);\n\t\tif (ehdr->h_proto != htons(ETH_P_8021Q)) {\n\t\t\terr = -EMSGSIZE;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tskb->protocol = proto;\n\tskb->dev = dev;\n\tskb->priority = sk->sk_priority;\n\tskb->mark = sk->sk_mark;\n\n\tif (po->has_vnet_hdr) {\n\t\tif (vnet_hdr.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) {\n\t\t\tif (!skb_partial_csum_set(skb, vnet_hdr.csum_start,\n\t\t\t\t\t\t  vnet_hdr.csum_offset)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t}\n\n\t\tskb_shinfo(skb)->gso_size = vnet_hdr.gso_size;\n\t\tskb_shinfo(skb)->gso_type = gso_type;\n\n\t\t/* Header must be checked, and gso_segs computed. */\n\t\tskb_shinfo(skb)->gso_type |= SKB_GSO_DODGY;\n\t\tskb_shinfo(skb)->gso_segs = 0;\n\n\t\tlen += vnet_hdr_len;\n\t}\n\n\t/*\n\t *\tNow send it\n\t */\n\n\terr = dev_queue_xmit(skb);\n\tif (err > 0 && (err = net_xmit_errno(err)) != 0)\n\t\tgoto out_unlock;\n\n\tdev_put(dev);\n\n\treturn len;\n\nout_free:\n\tkfree_skb(skb);\nout_unlock:\n\tif (dev)\n\t\tdev_put(dev);\nout:\n\treturn err;\n}\n\nstatic int packet_sendmsg(struct kiocb *iocb, struct socket *sock,\n\t\tstruct msghdr *msg, size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tif (po->tx_ring.pg_vec)\n\t\treturn tpacket_snd(po, msg);\n\telse\n\t\treturn packet_snd(sock, msg, len);\n}\n\n/*\n *\tClose a PACKET socket. This is fairly simple. We immediately go\n *\tto 'closed' state and remove our protocol entry in the device list.\n */\n\nstatic int packet_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po;\n\tstruct net *net;\n\tstruct tpacket_req req;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tnet = sock_net(sk);\n\tpo = pkt_sk(sk);\n\n\tspin_lock_bh(&net->packet.sklist_lock);\n\tsk_del_node_init_rcu(sk);\n\tsock_prot_inuse_add(net, sk->sk_prot, -1);\n\tspin_unlock_bh(&net->packet.sklist_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (po->running) {\n\t\t/*\n\t\t * Remove from protocol table\n\t\t */\n\t\tpo->running = 0;\n\t\tpo->num = 0;\n\t\t__dev_remove_pack(&po->prot_hook);\n\t\t__sock_put(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tpacket_flush_mclist(sk);\n\n\tmemset(&req, 0, sizeof(req));\n\n\tif (po->rx_ring.pg_vec)\n\t\tpacket_set_ring(sk, &req, 1, 0);\n\n\tif (po->tx_ring.pg_vec)\n\t\tpacket_set_ring(sk, &req, 1, 1);\n\n\tsynchronize_net();\n\t/*\n\t *\tNow the socket is dead. No more input will appear.\n\t */\n\tsock_orphan(sk);\n\tsock->sk = NULL;\n\n\t/* Purge queues */\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\tsk_refcnt_debug_release(sk);\n\n\tsock_put(sk);\n\treturn 0;\n}\n\n/*\n *\tAttach a packet hook.\n */\n\nstatic int packet_do_bind(struct sock *sk, struct net_device *dev, __be16 protocol)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\t/*\n\t *\tDetach an existing hook if present.\n\t */\n\n\tlock_sock(sk);\n\n\tspin_lock(&po->bind_lock);\n\tif (po->running) {\n\t\t__sock_put(sk);\n\t\tpo->running = 0;\n\t\tpo->num = 0;\n\t\tspin_unlock(&po->bind_lock);\n\t\tdev_remove_pack(&po->prot_hook);\n\t\tspin_lock(&po->bind_lock);\n\t}\n\n\tpo->num = protocol;\n\tpo->prot_hook.type = protocol;\n\tpo->prot_hook.dev = dev;\n\n\tpo->ifindex = dev ? dev->ifindex : 0;\n\n\tif (protocol == 0)\n\t\tgoto out_unlock;\n\n\tif (!dev || (dev->flags & IFF_UP)) {\n\t\tdev_add_pack(&po->prot_hook);\n\t\tsock_hold(sk);\n\t\tpo->running = 1;\n\t} else {\n\t\tsk->sk_err = ENETDOWN;\n\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\tsk->sk_error_report(sk);\n\t}\n\nout_unlock:\n\tspin_unlock(&po->bind_lock);\n\trelease_sock(sk);\n\treturn 0;\n}\n\n/*\n *\tBind a packet socket to a device\n */\n\nstatic int packet_bind_spkt(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t    int addr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tchar name[15];\n\tstruct net_device *dev;\n\tint err = -ENODEV;\n\n\t/*\n\t *\tCheck legality\n\t */\n\n\tif (addr_len != sizeof(struct sockaddr))\n\t\treturn -EINVAL;\n\tstrlcpy(name, uaddr->sa_data, sizeof(name));\n\n\tdev = dev_get_by_name(sock_net(sk), name);\n\tif (dev) {\n\t\terr = packet_do_bind(sk, dev, pkt_sk(sk)->num);\n\t\tdev_put(dev);\n\t}\n\treturn err;\n}\n\nstatic int packet_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sockaddr_ll *sll = (struct sockaddr_ll *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct net_device *dev = NULL;\n\tint err;\n\n\n\t/*\n\t *\tCheck legality\n\t */\n\n\tif (addr_len < sizeof(struct sockaddr_ll))\n\t\treturn -EINVAL;\n\tif (sll->sll_family != AF_PACKET)\n\t\treturn -EINVAL;\n\n\tif (sll->sll_ifindex) {\n\t\terr = -ENODEV;\n\t\tdev = dev_get_by_index(sock_net(sk), sll->sll_ifindex);\n\t\tif (dev == NULL)\n\t\t\tgoto out;\n\t}\n\terr = packet_do_bind(sk, dev, sll->sll_protocol ? : pkt_sk(sk)->num);\n\tif (dev)\n\t\tdev_put(dev);\n\nout:\n\treturn err;\n}\n\nstatic struct proto packet_proto = {\n\t.name\t  = \"PACKET\",\n\t.owner\t  = THIS_MODULE,\n\t.obj_size = sizeof(struct packet_sock),\n};\n\n/*\n *\tCreate a packet of type SOCK_PACKET.\n */\n\nstatic int packet_create(struct net *net, struct socket *sock, int protocol,\n\t\t\t int kern)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\t__be16 proto = (__force __be16)protocol; /* weird, but documented */\n\tint err;\n\n\tif (!capable(CAP_NET_RAW))\n\t\treturn -EPERM;\n\tif (sock->type != SOCK_DGRAM && sock->type != SOCK_RAW &&\n\t    sock->type != SOCK_PACKET)\n\t\treturn -ESOCKTNOSUPPORT;\n\n\tsock->state = SS_UNCONNECTED;\n\n\terr = -ENOBUFS;\n\tsk = sk_alloc(net, PF_PACKET, GFP_KERNEL, &packet_proto);\n\tif (sk == NULL)\n\t\tgoto out;\n\n\tsock->ops = &packet_ops;\n\tif (sock->type == SOCK_PACKET)\n\t\tsock->ops = &packet_ops_spkt;\n\n\tsock_init_data(sock, sk);\n\n\tpo = pkt_sk(sk);\n\tsk->sk_family = PF_PACKET;\n\tpo->num = proto;\n\n\tsk->sk_destruct = packet_sock_destruct;\n\tsk_refcnt_debug_inc(sk);\n\n\t/*\n\t *\tAttach a protocol block\n\t */\n\n\tspin_lock_init(&po->bind_lock);\n\tmutex_init(&po->pg_vec_lock);\n\tpo->prot_hook.func = packet_rcv;\n\n\tif (sock->type == SOCK_PACKET)\n\t\tpo->prot_hook.func = packet_rcv_spkt;\n\n\tpo->prot_hook.af_packet_priv = sk;\n\n\tif (proto) {\n\t\tpo->prot_hook.type = proto;\n\t\tdev_add_pack(&po->prot_hook);\n\t\tsock_hold(sk);\n\t\tpo->running = 1;\n\t}\n\n\tspin_lock_bh(&net->packet.sklist_lock);\n\tsk_add_node_rcu(sk, &net->packet.sklist);\n\tsock_prot_inuse_add(net, &packet_proto, 1);\n\tspin_unlock_bh(&net->packet.sklist_lock);\n\n\treturn 0;\nout:\n\treturn err;\n}\n\nstatic int packet_recv_error(struct sock *sk, struct msghdr *msg, int len)\n{\n\tstruct sock_exterr_skb *serr;\n\tstruct sk_buff *skb, *skb2;\n\tint copied, err;\n\n\terr = -EAGAIN;\n\tskb = skb_dequeue(&sk->sk_error_queue);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t\tcopied = len;\n\t}\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto out_free_skb;\n\n\tsock_recv_timestamp(msg, sk, skb);\n\n\tserr = SKB_EXT_ERR(skb);\n\tput_cmsg(msg, SOL_PACKET, PACKET_TX_TIMESTAMP,\n\t\t sizeof(serr->ee), &serr->ee);\n\n\tmsg->msg_flags |= MSG_ERRQUEUE;\n\terr = copied;\n\n\t/* Reset and regenerate socket error */\n\tspin_lock_bh(&sk->sk_error_queue.lock);\n\tsk->sk_err = 0;\n\tif ((skb2 = skb_peek(&sk->sk_error_queue)) != NULL) {\n\t\tsk->sk_err = SKB_EXT_ERR(skb2)->ee.ee_errno;\n\t\tspin_unlock_bh(&sk->sk_error_queue.lock);\n\t\tsk->sk_error_report(sk);\n\t} else\n\t\tspin_unlock_bh(&sk->sk_error_queue.lock);\n\nout_free_skb:\n\tkfree_skb(skb);\nout:\n\treturn err;\n}\n\n/*\n *\tPull a packet from our receive queue and hand it to the user.\n *\tIf necessary we block.\n */\n\nstatic int packet_recvmsg(struct kiocb *iocb, struct socket *sock,\n\t\t\t  struct msghdr *msg, size_t len, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sk_buff *skb;\n\tint copied, err;\n\tstruct sockaddr_ll *sll;\n\tint vnet_hdr_len = 0;\n\n\terr = -EINVAL;\n\tif (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT|MSG_ERRQUEUE))\n\t\tgoto out;\n\n#if 0\n\t/* What error should we return now? EUNATTACH? */\n\tif (pkt_sk(sk)->ifindex < 0)\n\t\treturn -ENODEV;\n#endif\n\n\tif (flags & MSG_ERRQUEUE) {\n\t\terr = packet_recv_error(sk, msg, len);\n\t\tgoto out;\n\t}\n\n\t/*\n\t *\tCall the generic datagram receiver. This handles all sorts\n\t *\tof horrible races and re-entrancy so we can forget about it\n\t *\tin the protocol layers.\n\t *\n\t *\tNow it will return ENETDOWN, if device have just gone down,\n\t *\tbut then it will block.\n\t */\n\n\tskb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);\n\n\t/*\n\t *\tAn error occurred so return it. Because skb_recv_datagram()\n\t *\thandles the blocking we don't see and worry about blocking\n\t *\tretries.\n\t */\n\n\tif (skb == NULL)\n\t\tgoto out;\n\n\tif (pkt_sk(sk)->has_vnet_hdr) {\n\t\tstruct virtio_net_hdr vnet_hdr = { 0 };\n\n\t\terr = -EINVAL;\n\t\tvnet_hdr_len = sizeof(vnet_hdr);\n\t\tif (len < vnet_hdr_len)\n\t\t\tgoto out_free;\n\n\t\tlen -= vnet_hdr_len;\n\n\t\tif (skb_is_gso(skb)) {\n\t\t\tstruct skb_shared_info *sinfo = skb_shinfo(skb);\n\n\t\t\t/* This is a hint as to how much should be linear. */\n\t\t\tvnet_hdr.hdr_len = skb_headlen(skb);\n\t\t\tvnet_hdr.gso_size = sinfo->gso_size;\n\t\t\tif (sinfo->gso_type & SKB_GSO_TCPV4)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_TCPV4;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_TCPV6)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_TCPV6;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_UDP)\n\t\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_UDP;\n\t\t\telse if (sinfo->gso_type & SKB_GSO_FCOE)\n\t\t\t\tgoto out_free;\n\t\t\telse\n\t\t\t\tBUG();\n\t\t\tif (sinfo->gso_type & SKB_GSO_TCP_ECN)\n\t\t\t\tvnet_hdr.gso_type |= VIRTIO_NET_HDR_GSO_ECN;\n\t\t} else\n\t\t\tvnet_hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\tvnet_hdr.flags = VIRTIO_NET_HDR_F_NEEDS_CSUM;\n\t\t\tvnet_hdr.csum_start = skb_checksum_start_offset(skb);\n\t\t\tvnet_hdr.csum_offset = skb->csum_offset;\n\t\t} /* else everything is zero */\n\n\t\terr = memcpy_toiovec(msg->msg_iov, (void *)&vnet_hdr,\n\t\t\t\t     vnet_hdr_len);\n\t\tif (err < 0)\n\t\t\tgoto out_free;\n\t}\n\n\t/*\n\t *\tIf the address length field is there to be filled in, we fill\n\t *\tit in now.\n\t */\n\n\tsll = &PACKET_SKB_CB(skb)->sa.ll;\n\tif (sock->type == SOCK_PACKET)\n\t\tmsg->msg_namelen = sizeof(struct sockaddr_pkt);\n\telse\n\t\tmsg->msg_namelen = sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr);\n\n\t/*\n\t *\tYou lose any data beyond the buffer you gave. If it worries a\n\t *\tuser program they can ask the device for its MTU anyway.\n\t */\n\n\tcopied = skb->len;\n\tif (copied > len) {\n\t\tcopied = len;\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\t}\n\n\terr = skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);\n\tif (err)\n\t\tgoto out_free;\n\n\tsock_recv_ts_and_drops(msg, sk, skb);\n\n\tif (msg->msg_name)\n\t\tmemcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,\n\t\t       msg->msg_namelen);\n\n\tif (pkt_sk(sk)->auxdata) {\n\t\tstruct tpacket_auxdata aux;\n\n\t\taux.tp_status = TP_STATUS_USER;\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\taux.tp_status |= TP_STATUS_CSUMNOTREADY;\n\t\taux.tp_len = PACKET_SKB_CB(skb)->origlen;\n\t\taux.tp_snaplen = skb->len;\n\t\taux.tp_mac = 0;\n\t\taux.tp_net = skb_network_offset(skb);\n\t\tif (vlan_tx_tag_present(skb)) {\n\t\t\taux.tp_vlan_tci = vlan_tx_tag_get(skb);\n\t\t\taux.tp_status |= TP_STATUS_VLAN_VALID;\n\t\t} else {\n\t\t\taux.tp_vlan_tci = 0;\n\t\t}\n\t\taux.tp_padding = 0;\n\t\tput_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);\n\t}\n\n\t/*\n\t *\tFree or return the buffer as appropriate. Again this\n\t *\thides all the races and re-entrancy issues from us.\n\t */\n\terr = vnet_hdr_len + ((flags&MSG_TRUNC) ? skb->len : copied);\n\nout_free:\n\tskb_free_datagram(sk, skb);\nout:\n\treturn err;\n}\n\nstatic int packet_getname_spkt(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t       int *uaddr_len, int peer)\n{\n\tstruct net_device *dev;\n\tstruct sock *sk\t= sock->sk;\n\n\tif (peer)\n\t\treturn -EOPNOTSUPP;\n\n\tuaddr->sa_family = AF_PACKET;\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(sock_net(sk), pkt_sk(sk)->ifindex);\n\tif (dev)\n\t\tstrncpy(uaddr->sa_data, dev->name, 14);\n\telse\n\t\tmemset(uaddr->sa_data, 0, 14);\n\trcu_read_unlock();\n\t*uaddr_len = sizeof(*uaddr);\n\n\treturn 0;\n}\n\nstatic int packet_getname(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t  int *uaddr_len, int peer)\n{\n\tstruct net_device *dev;\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_ll *, sll, uaddr);\n\n\tif (peer)\n\t\treturn -EOPNOTSUPP;\n\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_ifindex = po->ifindex;\n\tsll->sll_protocol = po->num;\n\tsll->sll_pkttype = 0;\n\trcu_read_lock();\n\tdev = dev_get_by_index_rcu(sock_net(sk), po->ifindex);\n\tif (dev) {\n\t\tsll->sll_hatype = dev->type;\n\t\tsll->sll_halen = dev->addr_len;\n\t\tmemcpy(sll->sll_addr, dev->dev_addr, dev->addr_len);\n\t} else {\n\t\tsll->sll_hatype = 0;\t/* Bad: we have no ARPHRD_UNSPEC */\n\t\tsll->sll_halen = 0;\n\t}\n\trcu_read_unlock();\n\t*uaddr_len = offsetof(struct sockaddr_ll, sll_addr) + sll->sll_halen;\n\n\treturn 0;\n}\n\nstatic int packet_dev_mc(struct net_device *dev, struct packet_mclist *i,\n\t\t\t int what)\n{\n\tswitch (i->type) {\n\tcase PACKET_MR_MULTICAST:\n\t\tif (i->alen != dev->addr_len)\n\t\t\treturn -EINVAL;\n\t\tif (what > 0)\n\t\t\treturn dev_mc_add(dev, i->addr);\n\t\telse\n\t\t\treturn dev_mc_del(dev, i->addr);\n\t\tbreak;\n\tcase PACKET_MR_PROMISC:\n\t\treturn dev_set_promiscuity(dev, what);\n\t\tbreak;\n\tcase PACKET_MR_ALLMULTI:\n\t\treturn dev_set_allmulti(dev, what);\n\t\tbreak;\n\tcase PACKET_MR_UNICAST:\n\t\tif (i->alen != dev->addr_len)\n\t\t\treturn -EINVAL;\n\t\tif (what > 0)\n\t\t\treturn dev_uc_add(dev, i->addr);\n\t\telse\n\t\t\treturn dev_uc_del(dev, i->addr);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic void packet_dev_mclist(struct net_device *dev, struct packet_mclist *i, int what)\n{\n\tfor ( ; i; i = i->next) {\n\t\tif (i->ifindex == dev->ifindex)\n\t\t\tpacket_dev_mc(dev, i, what);\n\t}\n}\n\nstatic int packet_mc_add(struct sock *sk, struct packet_mreq_max *mreq)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_mclist *ml, *i;\n\tstruct net_device *dev;\n\tint err;\n\n\trtnl_lock();\n\n\terr = -ENODEV;\n\tdev = __dev_get_by_index(sock_net(sk), mreq->mr_ifindex);\n\tif (!dev)\n\t\tgoto done;\n\n\terr = -EINVAL;\n\tif (mreq->mr_alen > dev->addr_len)\n\t\tgoto done;\n\n\terr = -ENOBUFS;\n\ti = kmalloc(sizeof(*i), GFP_KERNEL);\n\tif (i == NULL)\n\t\tgoto done;\n\n\terr = 0;\n\tfor (ml = po->mclist; ml; ml = ml->next) {\n\t\tif (ml->ifindex == mreq->mr_ifindex &&\n\t\t    ml->type == mreq->mr_type &&\n\t\t    ml->alen == mreq->mr_alen &&\n\t\t    memcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\n\t\t\tml->count++;\n\t\t\t/* Free the new element ... */\n\t\t\tkfree(i);\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\ti->type = mreq->mr_type;\n\ti->ifindex = mreq->mr_ifindex;\n\ti->alen = mreq->mr_alen;\n\tmemcpy(i->addr, mreq->mr_address, i->alen);\n\ti->count = 1;\n\ti->next = po->mclist;\n\tpo->mclist = i;\n\terr = packet_dev_mc(dev, i, 1);\n\tif (err) {\n\t\tpo->mclist = i->next;\n\t\tkfree(i);\n\t}\n\ndone:\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic int packet_mc_drop(struct sock *sk, struct packet_mreq_max *mreq)\n{\n\tstruct packet_mclist *ml, **mlp;\n\n\trtnl_lock();\n\n\tfor (mlp = &pkt_sk(sk)->mclist; (ml = *mlp) != NULL; mlp = &ml->next) {\n\t\tif (ml->ifindex == mreq->mr_ifindex &&\n\t\t    ml->type == mreq->mr_type &&\n\t\t    ml->alen == mreq->mr_alen &&\n\t\t    memcmp(ml->addr, mreq->mr_address, ml->alen) == 0) {\n\t\t\tif (--ml->count == 0) {\n\t\t\t\tstruct net_device *dev;\n\t\t\t\t*mlp = ml->next;\n\t\t\t\tdev = __dev_get_by_index(sock_net(sk), ml->ifindex);\n\t\t\t\tif (dev)\n\t\t\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\t\t\tkfree(ml);\n\t\t\t}\n\t\t\trtnl_unlock();\n\t\t\treturn 0;\n\t\t}\n\t}\n\trtnl_unlock();\n\treturn -EADDRNOTAVAIL;\n}\n\nstatic void packet_flush_mclist(struct sock *sk)\n{\n\tstruct packet_sock *po = pkt_sk(sk);\n\tstruct packet_mclist *ml;\n\n\tif (!po->mclist)\n\t\treturn;\n\n\trtnl_lock();\n\twhile ((ml = po->mclist) != NULL) {\n\t\tstruct net_device *dev;\n\n\t\tpo->mclist = ml->next;\n\t\tdev = __dev_get_by_index(sock_net(sk), ml->ifindex);\n\t\tif (dev != NULL)\n\t\t\tpacket_dev_mc(dev, ml, -1);\n\t\tkfree(ml);\n\t}\n\trtnl_unlock();\n}\n\nstatic int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tstruct tpacket_req req;\n\n\t\tif (optlen < sizeof(req))\n\t\t\treturn -EINVAL;\n\t\tif (pkt_sk(sk)->has_vnet_hdr)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req, optval, sizeof(req)))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req, 0, optname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_version = val;\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}\n\nstatic int packet_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t     char __user *optval, int __user *optlen)\n{\n\tint len;\n\tint val;\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tvoid *data;\n\tstruct tpacket_stats st;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase PACKET_STATISTICS:\n\t\tif (len > sizeof(struct tpacket_stats))\n\t\t\tlen = sizeof(struct tpacket_stats);\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tst = po->stats;\n\t\tmemset(&po->stats, 0, sizeof(st));\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\tst.tp_packets += st.tp_drops;\n\n\t\tdata = &st;\n\t\tbreak;\n\tcase PACKET_AUXDATA:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tval = po->auxdata;\n\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_ORIGDEV:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tval = po->origdev;\n\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_VNET_HDR:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tval = po->has_vnet_hdr;\n\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_VERSION:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tval = po->tp_version;\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_HDRLEN:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tif (copy_from_user(&val, optval, len))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\t\tval = sizeof(struct tpacket_hdr);\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tval = sizeof(struct tpacket2_hdr);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_RESERVE:\n\t\tif (len > sizeof(unsigned int))\n\t\t\tlen = sizeof(unsigned int);\n\t\tval = po->tp_reserve;\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_LOSS:\n\t\tif (len > sizeof(unsigned int))\n\t\t\tlen = sizeof(unsigned int);\n\t\tval = po->tp_loss;\n\t\tdata = &val;\n\t\tbreak;\n\tcase PACKET_TIMESTAMP:\n\t\tif (len > sizeof(int))\n\t\t\tlen = sizeof(int);\n\t\tval = po->tp_tstamp;\n\t\tdata = &val;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, data, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n\nstatic int packet_notifier(struct notifier_block *this, unsigned long msg, void *data)\n{\n\tstruct sock *sk;\n\tstruct hlist_node *node;\n\tstruct net_device *dev = data;\n\tstruct net *net = dev_net(dev);\n\n\trcu_read_lock();\n\tsk_for_each_rcu(sk, node, &net->packet.sklist) {\n\t\tstruct packet_sock *po = pkt_sk(sk);\n\n\t\tswitch (msg) {\n\t\tcase NETDEV_UNREGISTER:\n\t\t\tif (po->mclist)\n\t\t\t\tpacket_dev_mclist(dev, po->mclist, -1);\n\t\t\t/* fallthrough */\n\n\t\tcase NETDEV_DOWN:\n\t\t\tif (dev->ifindex == po->ifindex) {\n\t\t\t\tspin_lock(&po->bind_lock);\n\t\t\t\tif (po->running) {\n\t\t\t\t\t__dev_remove_pack(&po->prot_hook);\n\t\t\t\t\t__sock_put(sk);\n\t\t\t\t\tpo->running = 0;\n\t\t\t\t\tsk->sk_err = ENETDOWN;\n\t\t\t\t\tif (!sock_flag(sk, SOCK_DEAD))\n\t\t\t\t\t\tsk->sk_error_report(sk);\n\t\t\t\t}\n\t\t\t\tif (msg == NETDEV_UNREGISTER) {\n\t\t\t\t\tpo->ifindex = -1;\n\t\t\t\t\tpo->prot_hook.dev = NULL;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&po->bind_lock);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NETDEV_UP:\n\t\t\tif (dev->ifindex == po->ifindex) {\n\t\t\t\tspin_lock(&po->bind_lock);\n\t\t\t\tif (po->num && !po->running) {\n\t\t\t\t\tdev_add_pack(&po->prot_hook);\n\t\t\t\t\tsock_hold(sk);\n\t\t\t\t\tpo->running = 1;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&po->bind_lock);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn NOTIFY_DONE;\n}\n\n\nstatic int packet_ioctl(struct socket *sock, unsigned int cmd,\n\t\t\tunsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t{\n\t\tint amount = sk_wmem_alloc_get(sk);\n\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\tcase SIOCINQ:\n\t{\n\t\tstruct sk_buff *skb;\n\t\tint amount = 0;\n\n\t\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tamount = skb->len;\n\t\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\t\treturn put_user(amount, (int __user *)arg);\n\t}\n\tcase SIOCGSTAMP:\n\t\treturn sock_get_timestamp(sk, (struct timeval __user *)arg);\n\tcase SIOCGSTAMPNS:\n\t\treturn sock_get_timestampns(sk, (struct timespec __user *)arg);\n\n#ifdef CONFIG_INET\n\tcase SIOCADDRT:\n\tcase SIOCDELRT:\n\tcase SIOCDARP:\n\tcase SIOCGARP:\n\tcase SIOCSARP:\n\tcase SIOCGIFADDR:\n\tcase SIOCSIFADDR:\n\tcase SIOCGIFBRDADDR:\n\tcase SIOCSIFBRDADDR:\n\tcase SIOCGIFNETMASK:\n\tcase SIOCSIFNETMASK:\n\tcase SIOCGIFDSTADDR:\n\tcase SIOCSIFDSTADDR:\n\tcase SIOCSIFFLAGS:\n\t\treturn inet_dgram_ops.ioctl(sock, cmd, arg);\n#endif\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\treturn 0;\n}\n\nstatic unsigned int packet_poll(struct file *file, struct socket *sock,\n\t\t\t\tpoll_table *wait)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned int mask = datagram_poll(file, sock, wait);\n\n\tspin_lock_bh(&sk->sk_receive_queue.lock);\n\tif (po->rx_ring.pg_vec) {\n\t\tif (!packet_previous_frame(po, &po->rx_ring, TP_STATUS_KERNEL))\n\t\t\tmask |= POLLIN | POLLRDNORM;\n\t}\n\tspin_unlock_bh(&sk->sk_receive_queue.lock);\n\tspin_lock_bh(&sk->sk_write_queue.lock);\n\tif (po->tx_ring.pg_vec) {\n\t\tif (packet_current_frame(po, &po->tx_ring, TP_STATUS_AVAILABLE))\n\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t}\n\tspin_unlock_bh(&sk->sk_write_queue.lock);\n\treturn mask;\n}\n\n\n/* Dirty? Well, I still did not learn better way to account\n * for user mmaps.\n */\n\nstatic void packet_mm_open(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct socket *sock = file->private_data;\n\tstruct sock *sk = sock->sk;\n\n\tif (sk)\n\t\tatomic_inc(&pkt_sk(sk)->mapped);\n}\n\nstatic void packet_mm_close(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct socket *sock = file->private_data;\n\tstruct sock *sk = sock->sk;\n\n\tif (sk)\n\t\tatomic_dec(&pkt_sk(sk)->mapped);\n}\n\nstatic const struct vm_operations_struct packet_mmap_ops = {\n\t.open\t=\tpacket_mm_open,\n\t.close\t=\tpacket_mm_close,\n};\n\nstatic void free_pg_vec(struct pgv *pg_vec, unsigned int order,\n\t\t\tunsigned int len)\n{\n\tint i;\n\n\tfor (i = 0; i < len; i++) {\n\t\tif (likely(pg_vec[i].buffer)) {\n\t\t\tif (is_vmalloc_addr(pg_vec[i].buffer))\n\t\t\t\tvfree(pg_vec[i].buffer);\n\t\t\telse\n\t\t\t\tfree_pages((unsigned long)pg_vec[i].buffer,\n\t\t\t\t\t   order);\n\t\t\tpg_vec[i].buffer = NULL;\n\t\t}\n\t}\n\tkfree(pg_vec);\n}\n\nstatic inline char *alloc_one_pg_vec_page(unsigned long order)\n{\n\tchar *buffer = NULL;\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_COMP |\n\t\t\t  __GFP_ZERO | __GFP_NOWARN | __GFP_NORETRY;\n\n\tbuffer = (char *) __get_free_pages(gfp_flags, order);\n\n\tif (buffer)\n\t\treturn buffer;\n\n\t/*\n\t * __get_free_pages failed, fall back to vmalloc\n\t */\n\tbuffer = vzalloc((1 << order) * PAGE_SIZE);\n\n\tif (buffer)\n\t\treturn buffer;\n\n\t/*\n\t * vmalloc failed, lets dig into swap here\n\t */\n\tgfp_flags &= ~__GFP_NORETRY;\n\tbuffer = (char *)__get_free_pages(gfp_flags, order);\n\tif (buffer)\n\t\treturn buffer;\n\n\t/*\n\t * complete and utter failure\n\t */\n\treturn NULL;\n}\n\nstatic struct pgv *alloc_pg_vec(struct tpacket_req *req, int order)\n{\n\tunsigned int block_nr = req->tp_block_nr;\n\tstruct pgv *pg_vec;\n\tint i;\n\n\tpg_vec = kcalloc(block_nr, sizeof(struct pgv), GFP_KERNEL);\n\tif (unlikely(!pg_vec))\n\t\tgoto out;\n\n\tfor (i = 0; i < block_nr; i++) {\n\t\tpg_vec[i].buffer = alloc_one_pg_vec_page(order);\n\t\tif (unlikely(!pg_vec[i].buffer))\n\t\t\tgoto out_free_pgvec;\n\t}\n\nout:\n\treturn pg_vec;\n\nout_free_pgvec:\n\tfree_pg_vec(pg_vec, order, block_nr);\n\tpg_vec = NULL;\n\tgoto out;\n}\n\nstatic int packet_set_ring(struct sock *sk, struct tpacket_req *req,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err;\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (atomic_read(&rb->pending))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_block_size & (PAGE_SIZE - 1)))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size/req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\t__dev_remove_pack(&po->prot_hook);\n\t\tpo->num = 0;\n\t\tpo->running = 0;\n\t\t__sock_put(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running && !po->running) {\n\t\tsock_hold(sk);\n\t\tpo->running = 1;\n\t\tpo->num = num;\n\t\tdev_add_pack(&po->prot_hook);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\trelease_sock(sk);\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\treturn err;\n}\n\nstatic int packet_mmap(struct file *file, struct socket *sock,\n\t\tstruct vm_area_struct *vma)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tunsigned long size, expected_size;\n\tstruct packet_ring_buffer *rb;\n\tunsigned long start;\n\tint err = -EINVAL;\n\tint i;\n\n\tif (vma->vm_pgoff)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&po->pg_vec_lock);\n\n\texpected_size = 0;\n\tfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\n\t\tif (rb->pg_vec) {\n\t\t\texpected_size += rb->pg_vec_len\n\t\t\t\t\t\t* rb->pg_vec_pages\n\t\t\t\t\t\t* PAGE_SIZE;\n\t\t}\n\t}\n\n\tif (expected_size == 0)\n\t\tgoto out;\n\n\tsize = vma->vm_end - vma->vm_start;\n\tif (size != expected_size)\n\t\tgoto out;\n\n\tstart = vma->vm_start;\n\tfor (rb = &po->rx_ring; rb <= &po->tx_ring; rb++) {\n\t\tif (rb->pg_vec == NULL)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < rb->pg_vec_len; i++) {\n\t\t\tstruct page *page;\n\t\t\tvoid *kaddr = rb->pg_vec[i].buffer;\n\t\t\tint pg_num;\n\n\t\t\tfor (pg_num = 0; pg_num < rb->pg_vec_pages; pg_num++) {\n\t\t\t\tpage = pgv_to_page(kaddr);\n\t\t\t\terr = vm_insert_page(vma, start, page);\n\t\t\t\tif (unlikely(err))\n\t\t\t\t\tgoto out;\n\t\t\t\tstart += PAGE_SIZE;\n\t\t\t\tkaddr += PAGE_SIZE;\n\t\t\t}\n\t\t}\n\t}\n\n\tatomic_inc(&po->mapped);\n\tvma->vm_ops = &packet_mmap_ops;\n\terr = 0;\n\nout:\n\tmutex_unlock(&po->pg_vec_lock);\n\treturn err;\n}\n\nstatic const struct proto_ops packet_ops_spkt = {\n\t.family =\tPF_PACKET,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tpacket_release,\n\t.bind =\t\tpacket_bind_spkt,\n\t.connect =\tsock_no_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tpacket_getname_spkt,\n\t.poll =\t\tdatagram_poll,\n\t.ioctl =\tpacket_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tsock_no_setsockopt,\n\t.getsockopt =\tsock_no_getsockopt,\n\t.sendmsg =\tpacket_sendmsg_spkt,\n\t.recvmsg =\tpacket_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tsock_no_sendpage,\n};\n\nstatic const struct proto_ops packet_ops = {\n\t.family =\tPF_PACKET,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tpacket_release,\n\t.bind =\t\tpacket_bind,\n\t.connect =\tsock_no_connect,\n\t.socketpair =\tsock_no_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tpacket_getname,\n\t.poll =\t\tpacket_poll,\n\t.ioctl =\tpacket_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tsock_no_shutdown,\n\t.setsockopt =\tpacket_setsockopt,\n\t.getsockopt =\tpacket_getsockopt,\n\t.sendmsg =\tpacket_sendmsg,\n\t.recvmsg =\tpacket_recvmsg,\n\t.mmap =\t\tpacket_mmap,\n\t.sendpage =\tsock_no_sendpage,\n};\n\nstatic const struct net_proto_family packet_family_ops = {\n\t.family =\tPF_PACKET,\n\t.create =\tpacket_create,\n\t.owner\t=\tTHIS_MODULE,\n};\n\nstatic struct notifier_block packet_netdev_notifier = {\n\t.notifier_call =\tpacket_notifier,\n};\n\n#ifdef CONFIG_PROC_FS\n\nstatic void *packet_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(RCU)\n{\n\tstruct net *net = seq_file_net(seq);\n\n\trcu_read_lock();\n\treturn seq_hlist_start_head_rcu(&net->packet.sklist, *pos);\n}\n\nstatic void *packet_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct net *net = seq_file_net(seq);\n\treturn seq_hlist_next_rcu(v, &net->packet.sklist, pos);\n}\n\nstatic void packet_seq_stop(struct seq_file *seq, void *v)\n\t__releases(RCU)\n{\n\trcu_read_unlock();\n}\n\nstatic int packet_seq_show(struct seq_file *seq, void *v)\n{\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"sk       RefCnt Type Proto  Iface R Rmem   User   Inode\\n\");\n\telse {\n\t\tstruct sock *s = sk_entry(v);\n\t\tconst struct packet_sock *po = pkt_sk(s);\n\n\t\tseq_printf(seq,\n\t\t\t   \"%pK %-6d %-4d %04x   %-5d %1d %-6u %-6u %-6lu\\n\",\n\t\t\t   s,\n\t\t\t   atomic_read(&s->sk_refcnt),\n\t\t\t   s->sk_type,\n\t\t\t   ntohs(po->num),\n\t\t\t   po->ifindex,\n\t\t\t   po->running,\n\t\t\t   atomic_read(&s->sk_rmem_alloc),\n\t\t\t   sock_i_uid(s),\n\t\t\t   sock_i_ino(s));\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations packet_seq_ops = {\n\t.start\t= packet_seq_start,\n\t.next\t= packet_seq_next,\n\t.stop\t= packet_seq_stop,\n\t.show\t= packet_seq_show,\n};\n\nstatic int packet_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &packet_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations packet_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= packet_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\n#endif\n\nstatic int __net_init packet_net_init(struct net *net)\n{\n\tspin_lock_init(&net->packet.sklist_lock);\n\tINIT_HLIST_HEAD(&net->packet.sklist);\n\n\tif (!proc_net_fops_create(net, \"packet\", 0, &packet_seq_fops))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void __net_exit packet_net_exit(struct net *net)\n{\n\tproc_net_remove(net, \"packet\");\n}\n\nstatic struct pernet_operations packet_net_ops = {\n\t.init = packet_net_init,\n\t.exit = packet_net_exit,\n};\n\n\nstatic void __exit packet_exit(void)\n{\n\tunregister_netdevice_notifier(&packet_netdev_notifier);\n\tunregister_pernet_subsys(&packet_net_ops);\n\tsock_unregister(PF_PACKET);\n\tproto_unregister(&packet_proto);\n}\n\nstatic int __init packet_init(void)\n{\n\tint rc = proto_register(&packet_proto, 0);\n\n\tif (rc != 0)\n\t\tgoto out;\n\n\tsock_register(&packet_family_ops);\n\tregister_pernet_subsys(&packet_net_ops);\n\tregister_netdevice_notifier(&packet_netdev_notifier);\nout:\n\treturn rc;\n}\n\nmodule_init(packet_init);\nmodule_exit(packet_exit);\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_NETPROTO(PF_PACKET);\n"], "filenames": ["include/linux/if_packet.h", "net/packet/af_packet.c"], "buggy_code_start_loc": [64, 806], "buggy_code_end_loc": [103, 1738], "fixing_code_start_loc": [65, 807], "fixing_code_end_loc": [106, 1741], "type": "CWE-200", "message": "net/packet/af_packet.c in the Linux kernel before 2.6.39.3 does not properly restrict user-space access to certain packet data structures associated with VLAN Tag Control Information, which allows local users to obtain potentially sensitive information via a crafted application.", "other": {"cve": {"id": "CVE-2011-2898", "sourceIdentifier": "secalert@redhat.com", "published": "2012-05-24T23:55:02.057", "lastModified": "2023-02-13T04:31:14.910", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "net/packet/af_packet.c in the Linux kernel before 2.6.39.3 does not properly restrict user-space access to certain packet data structures associated with VLAN Tag Control Information, which allows local users to obtain potentially sensitive information via a crafted application."}, {"lang": "es", "value": "net/packet/af_packet.c en el kernel de Linux antes de v2.6.39.3 no restringe adecuadamente el acceso al espacio de usuario a ciertas estructuras de paquetes de datos asociados VLAN Tag Control Information, lo que permite a usuarios locales obtener informaci\u00f3n sensible a trav\u00e9s de una aplicaci\u00f3n modificada."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 1.9}, "baseSeverity": "LOW", "exploitabilityScore": 3.4, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": true}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.39.3", "matchCriteriaId": "D83B3E02-C11C-4F8E-8CAC-F2270BC8190C"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=13fcb7bd322164c67926ffe272846d4860196dc6", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2011/08/03/7", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=728023", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/13fcb7bd322164c67926ffe272846d4860196dc6", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/13fcb7bd322164c67926ffe272846d4860196dc6"}}