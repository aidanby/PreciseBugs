{"buggy_code": ["# Line too long - pylint: disable=C0301\n# Copyright (c) Greenplum Inc 2011. All Rights Reserved.\n\nfrom contextlib import closing\nimport os\nimport platform\nimport shutil\nimport sys\nimport tarfile\n\ntry:\n    from gppylib import gplog\n    from gppylib.commands import gp\n    from gppylib.commands.base import Command, REMOTE, WorkerPool, ExecutionError\n    from gppylib.commands.unix import Scp\n    from gppylib.gpversion import GpVersion\n    from gppylib.mainUtils import ExceptionNoStackTraceNeeded\n    from gppylib.operations import Operation\n    from gppylib.operations.utils import RemoteOperation, ParallelOperation\n    from gppylib.operations.unix import CheckFile, CheckDir, MakeDir, RemoveFile, RemoveRemoteTree, RemoveRemoteFile, \\\n        CheckRemoteDir, MakeRemoteDir, CheckRemoteFile, ListRemoteFilesByPattern, ListFiles, ListFilesByPattern\n    from gppylib.utils import TableLogger\n\n    import yaml\n    from yaml.scanner import ScannerError\nexcept ImportError, ex:\n    sys.exit(\n        'Operation: Cannot import modules.  Please check that you have sourced greenplum_path.sh.  Detail: ' + str(ex))\n\nlogger = gplog.get_default_logger()\n\n\ndef dereference_symlink(path):\n    \"\"\"\n    MPP-15429: rpm is funky with symlinks...\n    During an rpm -e invocation, rpm mucks with the /usr/local/greenplum-db symlink.\n    From strace output, it appears that rpm tries to rmdir any directories it may have created during\n    package installation. And, in the case of our GPHOME symlink, rpm will actually try to unlink it.\n    To avoid this scenario, we perform all rpm actions against the \"symlink dereferenced\" $GPHOME.\n    \"\"\"\n    path = os.path.normpath(path)\n    if not os.path.islink(path):\n        return path\n    link = os.path.normpath(os.readlink(path))\n    if os.path.isabs(link):\n        return link\n    return os.path.join(os.path.dirname(path), link)\n\n\nGPHOME = dereference_symlink(gp.get_gphome())\n\nGPPKG_EXTENSION = '.gppkg'\nSPECFILE_NAME = 'gppkg_spec.yml'\nSPECFILE_REQUIRED_TAGS = ['pkgname', 'version', 'architecture', 'os', 'description', 'gpdbversion']\nSPECFILE_OPTIONAL_TAGS = ['preinstall', 'postinstall', 'preuninstall', 'postuninstall', 'postupdate']\n\n# TODO: AK: Our interactions with the internal RPM database could benefit from an abstraction layer\n# that hides the underlying commands used for installation, uninstallation, queries, etc.\nRPM_DATABASE_PATH = 'share/packages/database'\nRPM_DATABASE = os.path.join(GPHOME, RPM_DATABASE_PATH)\nRPM_INSTALLATION_PATH = GPHOME\n\n# An Update for deb package\nDEB_DATABASE_DIR = GPHOME + '/share/packages/database/deb/{info,updates,triggers}'\nDEB_DATABASE = os.path.join(GPHOME + '/share/packages/database/deb')\nDEB_STATUSFILE = GPHOME + '/share/packages/database/deb/status'\n\n# TODO: AK: Our interactions with the archive could benefit from an abstraction layer\n# that hides the implementations of archival, unarchival, queries, etc.\n# That is, consider the query \"is this package already archived?\" Currently, this is implemented\n# with a CheckFile. Rather, it should be a call to Archive.contains(package), where package\n# is instanceof Gppkg.\nARCHIVE_PATH = 'share/packages/archive'\nGPPKG_ARCHIVE_PATH = os.path.join(GPHOME, ARCHIVE_PATH)\n\n# TODO: AK: Shouldn't this be \"$GPHOME/.tmp\"?\n# i.e. what if remote host has its $GPHOME elsewhere?\nTEMP_EXTRACTION_PATH = GPHOME + '/.tmp'\nDEPS_DIR = 'deps'\n\n\nclass GpdbVersionError(Exception):\n    \"\"\"\n        Exception to notify that the gpdb version\n        does not match\n    \"\"\"\n    pass\n\n\nclass AlreadyInstalledError(Exception):\n    def __init__(self, package_name):\n        Exception.__init__(self, '%s is already installed.' % package_name)\n\n\nclass NotInstalledError(Exception):\n    def __init__(self, package_name):\n        Exception.__init__(self, '%s is not installed.' % package_name)\n\n\nclass BuildPkgError(Exception):\n    \"\"\"\n        Exception to notify that there was an error during\n        the building of a gppkg\n    \"\"\"\n    pass\n\n\nclass MissingDependencyError(Exception):\n    \"\"\"\n        Exception to catch missing dependency\n    \"\"\"\n\n    def __init__(self, value):\n        Exception.__init__(self, 'Dependency %s is missing' % value)\n\n\nclass OSCompatibilityError(Exception):\n    \"\"\"\n        Exception to notify that OS does not meet the\n        requirement\n    \"\"\"\n\n    def __init__(self, requiredos, foundos):\n        Exception.__init__(self, '%s OS required. %s OS found' % (requiredos, foundos))\n\n\nclass ArchCompatibilityError(Exception):\n    \"\"\"\n        Exception to notify that architecture does not meet\n        the requirement\n    \"\"\"\n\n    def __init__(self, requiredarch, foundarch):\n        Exception.__init__(self, '%s Arch required. %s Arch found' % (requiredarch, foundarch))\n\n\nclass RequiredDependencyError(Exception):\n    \"\"\"\n        Exception to notify that the package being uninstalled\n        is a dependency for another package\n    \"\"\"\n    pass\n\n\nclass Gppkg:\n    \"\"\"\n        This class stores all the information about a gppkg\n    \"\"\"\n\n    def __init__(self, pkg, pkgname, main_rpm, version, architecture, os, gpdbversion, description, abspath, preinstall,\n                 postinstall, preuninstall, postuninstall, postupdate, dependencies, file_list):\n        \"\"\"\n            The constructor takes the following arguments\n            pkg             The complete package name e.g pgcrypto-1.0-Darwin-i386.gppkg        TODO: AK: This is an awful variable name. Change to \"package_filename\".\n            pkgname         The name of the package as specified in the spec file\n            main_rpm        The name of the main rpm. e.g PL/R, PostGIS etc\n            version         The version of the gppkg\n            architecture    The architecture for which the package is built\n            os              The operating system for which the package is built\n            gpdbversion     The Greenplum Database version for which package is built\n            description     A short description for the package\n            abspath         This is the absolute path where the package sits on the host\n            preinstall      The cluster level preinstallation hooks\n            postinstall     The cluster level postinstallation hooks\n            preuninstall    The cluster level preuninstallation hooks\n            postuninstall   The cluster level postuninstallation hooks\n            postupdate      The cluster level postupdate hooks\n            dependencies    The dependencies of the package. e.g Geos, Proj in case of PostGIS\n            file_list       The list of files present in the package\n        \"\"\"\n\n        logger.debug('Gppkg Constructor')\n\n        self.pkg = pkg\n        self.pkgname = pkgname\n        self.main_rpm = main_rpm\n        self.version = version\n        self.architecture = architecture\n        self.os = os\n        self.gpdbversion = gpdbversion\n        self.description = description\n        self.abspath = abspath\n        self.preinstall = preinstall\n        self.postinstall = postinstall\n        self.preuninstall = preuninstall\n        self.postuninstall = postuninstall\n        self.postupdate = postupdate\n        self.dependencies = dependencies\n        self.file_list = file_list\n\n    @staticmethod\n    def from_package_path(pkg_path):\n        '''\n             This method takes a package as the argument and\n             obtains all the information about the package\n             Details include name, arch, OS, version, description, dependencies,\n             list of files present in the package and returns a gppkg object\n        '''\n\n        logger.debug('from_package_path')\n\n        if not os.path.exists(pkg_path):\n            logger.error('Cannot find package %s' % pkg_path)\n            raise IOError\n\n        # We check for a directory first because\n        # is_tarfile does not accept directories as path names\n        if os.path.isdir(pkg_path):\n            logger.error('%s is a directory !' % pkg_path)\n            raise IOError\n\n        if not tarfile.is_tarfile(pkg_path) or not pkg_path.endswith(GPPKG_EXTENSION):\n            logger.error('%s is Not a valid package' % pkg_path)\n            raise IOError\n\n        if os.path.getsize(pkg_path) == 0:\n            logger.error('Package is empty')\n            raise IOError\n\n        pkg = {}\n\n        # XXX: AK: It's purely coincidence that the optional tags are lists.\n        for tag in SPECFILE_REQUIRED_TAGS:\n            pkg[tag] = ''\n        for tag in SPECFILE_OPTIONAL_TAGS:\n            pkg[tag] = []\n\n        pkg['file_list'] = []\n        pkg['dependencies'] = []\n\n        with closing(tarfile.open(pkg_path, 'r:gz')) as tarinfo:\n            # store the list of all files present in the archive\n            archive_list = tarinfo.getnames()\n            pkg[\"file_list\"] = archive_list\n            keys = []\n            yamlfile = {}\n\n            # The spec file has to be called gppkg_spec\n            # so there will only be one such file,\n            for cur_file in archive_list:\n                if cur_file.endswith(SPECFILE_NAME):\n                    specfile = tarinfo.extractfile(cur_file)\n                    yamlfile = yaml.safe_load(specfile)\n                    keys = yamlfile.keys()\n                    break\n\n        # store all the tags\n        for key in keys:\n            pkg[key.lower()] = yamlfile[key]\n\n        # update the pkgpath\n        pkg['pkg'] = os.path.split(pkg_path)[-1]\n\n        # make the version as string\n        pkg['version'] = str(pkg['version'])\n\n        # Convert the required version to a GpVersion\n        pkg['gpdbversion'] = GpVersion(str(pkg['gpdbversion']))\n\n        # update the absolute path\n        pkg['abspath'] = pkg_path\n\n        # store all the dependencies of the gppkg\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            for cur_file in archive_list:\n                if cur_file.find('deps/') != -1 and cur_file.endswith('.deb'):\n                    pkg['dependencies'].append(cur_file[cur_file.rfind('/') + 1:])\n\n            # store the main deb, remain the attr name main_rpm to keep compatibility\n            for cur_file in archive_list:\n                if cur_file.find('deps/') == -1 and cur_file.endswith('.deb'):\n                    pkg['main_rpm'] = cur_file\n\n        else:\n            for cur_file in archive_list:\n                if cur_file.find('deps/') != -1 and cur_file.endswith('.rpm'):\n                    pkg['dependencies'].append(cur_file[cur_file.rfind('/') + 1:])\n\n            # store the main rpm\n            for cur_file in archive_list:\n                if cur_file.find('deps/') == -1 and cur_file.endswith('.rpm'):\n                    pkg['main_rpm'] = cur_file\n\n        gppkg = Gppkg(**pkg)\n\n        return gppkg\n\n\nclass LocalCommand(Operation):\n    \"\"\"\n    DEPRECATED\n\n    TODO: AK: Eliminate this. Replace invocations with Command(...).run(validateAfter = True)\n    \"\"\"\n\n    def __init__(self, cmd_str, echo=False):\n        self.cmd_str = cmd_str\n        self.echo = echo\n\n    def execute(self):\n\n        logger.debug(self.cmd_str)\n        cmd = Command(name='LocalCommand', cmdStr=self.cmd_str)\n        cmd.run(validateAfter=True)\n        if self.echo:\n            echo_str = cmd.get_results().stdout.strip()\n            if echo_str:\n                logger.info(echo_str)\n        return cmd.get_results()\n\n\nclass RemoteCommand(Operation):\n    \"\"\"\n    DEPRECATED\n\n    TODO: AK: Rename as GpSsh, like GpScp below.\n    \"\"\"\n\n    def __init__(self, cmd_str, host_list):\n        self.cmd_str = cmd_str\n        self.host_list = host_list\n        self.pool = None\n\n    def execute(self):\n        logger.debug(self.cmd_str)\n\n        # Create Worker pool\n        # and add commands to it\n        self.pool = WorkerPool()\n\n        for host in self.host_list:\n            cmd = Command(name='Remote Command', cmdStr=self.cmd_str, ctxt=REMOTE, remoteHost=host)\n            self.pool.addCommand(cmd)\n        self.pool.join()\n\n        # This will raise ExecutionError exception if even a single command fails\n        self.pool.check_results()\n\n\nclass ListPackages(Operation):\n    \"\"\"\n        Lists all the packages present in\n        $GPHOME/share/packages/archive\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def execute(self):\n        # Ensure archive path exists\n        # TODO: AK: In hindsight, this should've been named MakeDirP,\n        # to reflect that it won't blow up if the path already exists.\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n\n        package_list = ListFilesByPattern(GPPKG_ARCHIVE_PATH, '*' + GPPKG_EXTENSION).run()\n\n        package_name_list = []\n\n        for pkg in package_list:\n            pkg_name = pkg.split('/')[-1]\n            if len(pkg_name.split('-')) >= 2:\n                package_name_list.append(pkg_name[:pkg_name.index('-', pkg_name.index('-') + 1)])\n            else:\n                try:\n                    package_name_list.append(pkg_name[:pkg_name.index('.')])\n                except ValueError:\n                    raise Exception('unable to parse %s as a gppkg' % pkg_name)\n\n        return package_name_list\n\n\nclass CleanupDir(Operation):\n    \"\"\"\n        Cleans up the given dir\n        Returns True if either the dir is already removed\n        or if we were able to remove the dir successfully\n        False for other errors\n    \"\"\"\n\n    def __init__(self, dir_path):\n        self.dir_path = dir_path\n\n    def execute(self):\n\n        dir_path = self.dir_path\n\n        logger.debug('Cleaning up %s' % dir_path)\n\n        # If file does not exist, nothing to remove\n        # So we return true\n        if not os.path.exists(dir_path):\n            return True\n\n        if os.path.isdir(dir_path):\n            shutil.rmtree(dir_path)\n        else:\n            return False\n\n        return True\n\n\nclass IsVersionCompatible(Operation):\n    \"\"\"\n        Returns True if the gppkg is compatible\n        with the gpdb version that has been installed\n    \"\"\"\n\n    def __init__(self, gppkg):\n        super(IsVersionCompatible, self).__init__()\n        self.gppkg = gppkg\n\n    def execute(self):\n\n        gppkg = self.gppkg\n\n        gpdb_version = self._get_gpdb_version()\n        required_gpdb_version = gppkg.gpdbversion\n\n        logger.debug('Greenplum Database Version = %s' % gpdb_version)\n        logger.debug('Required Greenplum Database version = %s' % required_gpdb_version)\n\n        if gpdb_version is None:\n            logger.error('Could not determine Greenplum Database version')\n            return False\n\n        if not required_gpdb_version.isVersionRelease(gpdb_version):\n            logger.error('%s requires Greenplum Database version %s' % (gppkg.pkgname, required_gpdb_version))\n            return False\n\n        return True\n\n    def _get_gpdb_version(self):\n        \"\"\"\n            Get the version of the current GPDB\n            Returns a string consisting of the major\n            release version\n        \"\"\"\n        logger.debug('_get_gpdb_version')\n        self.gphome = gp.get_gphome()\n        version = gp.GpVersion.local('local GP software version check', self.gphome)\n        gpdb_version = GpVersion(version.strip())\n        return gpdb_version\n\n    def _convert_to_magic_number_version(self, gpversion_obj):\n        \"\"\"\n            Converts GPDB version to the GPDB magic number\n            Returns an int consisting of the major and minor release version\n        \"\"\"\n        logger.debug('_convert_to_magic_number_version')\n\n        ver_list = gpversion_obj.version\n\n        # The generation of the magic version number (GP_VERSION_NUM) is\n        # retrieved from our configure.in file\n        magic_num = \"%d%02d%02d\" % (ver_list[0], ver_list[1],\n                                    ver_list[2] if len(ver_list) > 2 else 0)\n        return int(magic_num)\n\n\nclass ValidateInstallDebPackage(Operation):\n    \"\"\"\n    A replicate class of ValidateInstallPackage that used for test deb package for Ubuntu,\n    it will use dpkg --verify that is instead of rpm --test\n\n    NOTE: In newest GPDB, the dependencies will use system's dependencies, we still leave the gppkg\n    to install dependencies, but did not check it anymore.\n\n    \"\"\"\n\n    def __init__(self, gppkg, is_update=False):\n        self.gppkg = gppkg\n        self.is_update = is_update\n\n    def execute(self):\n        # Check the GPDB requirements\n        if not IsVersionCompatible(self.gppkg).run():\n            raise GpdbVersionError\n\n        self.prepare_deb_env()\n\n        # No need to check the architecture here as there is no i386 arch in GPDB 5 or newer version\n        deb_set = set([self.gppkg.main_rpm] + self.gppkg.dependencies)\n        deb_packages_path = ' '.join([os.path.join(TEMP_EXTRACTION_PATH, deb_path) for deb_path in deb_set])\n\n        if self.is_update:\n            deb_test_command = 'fakeroot dpkg --dry-run --force-not-root --force-depends --log=/dev/null --admindir=%s ' \\\n                               '--instdir=%s -i %s' % (DEB_DATABASE, GPHOME, deb_packages_path)\n        else:\n            deb_test_command = 'fakeroot dpkg --dry-run --force-not-root --log=/dev/null --admindir=%s ' \\\n                               '--instdir=%s -i %s' % (DEB_DATABASE, GPHOME, deb_packages_path)\n\n        cmd = Command('Validating deb installation', deb_test_command)\n        logger.info(cmd)\n\n        try:\n            cmd.run(validateAfter=True)\n        except ExecutionError, e:\n            lines = e.cmd.get_results().stderr.splitlines()\n\n            if len(lines) == 0:\n                raise\n\n            # TODO: Add dependency and already installed check\n\n        archive_package_exists = CheckFile(os.path.join(GPPKG_ARCHIVE_PATH, self.gppkg.pkg)).run()\n        package_already_installed = (not deb_set) and archive_package_exists\n        if package_already_installed:\n            raise AlreadyInstalledError(self.gppkg.pkg)\n\n        already_installed = self.check_existence()\n        if self.is_update:\n            if not already_installed:\n                raise NotInstalledError(self.gppkg.pkg)\n        else:\n            if already_installed:\n                raise AlreadyInstalledError(self.gppkg.pkg)\n\n        return deb_set\n\n    def check_existence(self):\n        command = 'dpkg --force-not-root --log=/dev/null --admindir=%s --instdir=%s -l %s' % (DEB_DATABASE, GPHOME, self.gppkg.pkgname)\n        cmd = Command('Check installation of package', command)\n        logger.info(cmd)\n\n        cmd.run(validateAfter=False)\n        return cmd.results.rc == 0\n\n    def prepare_deb_env(self):\n        prepare_env_dir = 'mkdir -p ' + DEB_DATABASE_DIR\n        prepare_env_file = 'touch ' + DEB_STATUSFILE\n        cmd_dir = Command('Prepare deb package DIY directories', prepare_env_dir)\n        cmd_file = Command('Prepare deb package Status file', prepare_env_file)\n\n        try:\n            cmd_dir.run(validateAfter=True)\n            cmd_file.run(validateAfter=True)\n        except ExecutionError, e:\n            lines = e.cmd.get_results().stderr.splitlines()\n\n            raise ExecutionError('Can not setup deb package env', lines)\n\n\nclass ValidateInstallPackage(Operation):\n    \"\"\"\n    Ensure that the given rpms can be installed safely. This is accomplished mainly\n    through use of rpm --test, which will have one of a few outcomes:\n    1) A return code of 0, indicating the installation should proceed smoothly\n    2) A non-zero return code, and stderr indicating some of the rpms are already installed.\n       We simply omit such rpms from the returned list of rpms, indicating to the caller\n       that to be successful, installation should only be attempted on the filtered list of rpms.\n    3) A non-zero return code, and stderr indicating that a failed dependency issue will arise.\n       This scenario must result in a MissingDependencyError.\n\n    Note: install and update share this code, because there is extensive commonality in regards\n    to the version, os, arch. checking, in addition to the 3 code paths enumerated just above.\n\n    Lastly, for an edge case, if we determine that all of the relevant rpms are currently installed\n    *and* the archive package already exists we declare the package is already installed.\n\n    TODO: This is depending on ExtractPackage having put the dependencies in this same directory.\n    TODO: Use regexes for more reliable string matching. CR-2865#c20112\n    \"\"\"\n\n    def __init__(self, gppkg, is_update=False):\n        self.gppkg = gppkg\n        self.is_update = is_update\n\n    def execute(self):\n        # Check the GPDB requirements\n        if not IsVersionCompatible(self.gppkg).run():\n            raise GpdbVersionError\n\n        # TODO: AK: I've changed our use of the OS tag from 'Linux' to 'rhel5' or 'suse10'.\n        # So, the two lines below will not work properly.\n        # if self.gppkg.os.lower() != platform.system().lower():\n        #    raise OSCompatibilityError(self.gppkg.os, platform.system().lower())\n\n        # architecture compatibility\n        if self.gppkg.architecture.lower() != platform.machine().lower():\n            raise ArchCompatibilityError(self.gppkg.architecture, platform.machine().lower())\n\n        rpm_set = set([self.gppkg.main_rpm] + self.gppkg.dependencies)\n        rpm_install_string = ' '.join([os.path.join(TEMP_EXTRACTION_PATH, rpm) for rpm in rpm_set])\n        if self.is_update:\n            rpm_install_command = 'rpm --test -U --force %s --dbpath %s --prefix %s' % (\n            rpm_install_string, RPM_DATABASE, RPM_INSTALLATION_PATH)\n        else:\n            rpm_install_command = 'rpm --test -i %s --dbpath %s --prefix %s' % (\n            rpm_install_string, RPM_DATABASE, RPM_INSTALLATION_PATH)\n        cmd = Command('Validating rpm installation', rpm_install_command)\n        logger.info(cmd)  # TODO: AK: This should be debug(), but RMI cannot propagate a log level.\n\n        try:\n            cmd.run(validateAfter=True)\n        except ExecutionError, e:\n            already_install = False\n            lines = e.cmd.get_results().stderr.splitlines()\n\n            # Forking between code paths 2 and 3 depends on some meaningful stderr\n            # Without such stderr, we must bubble up the ExecutionError.\n            if len(lines) == 0:\n                raise\n\n            if 'failed dependencies' in lines[0].lower():\n                # Code path 3 (see docstring)\n                # example stderr:\n                # error: Failed dependencies:\n                #    geos-3.2.2-1.x86_64.rpm is needed by postgis-1.0-1.x86_64\n\n                # TODO: AK: Dependencies should be parsed out here and used to initialize\n                # this MissingDependencyError. However, this exception does not support\n                # multiple missing dependencies. Some refactoring work is needed in both places.\n                logger.error(e.cmd.get_results().stderr)\n                raise MissingDependencyError('')\n\n            # Code path 2, possibly (see docstring)\n            # example stderr:\n            #    package geos-3.2.2-1.x86_64 is already installed\n            #    package proj-4.7.0-1.x86_64 is already installed\n            #    package postgis-1.0-1.x86_64 is already installed\n            for line in lines:\n                if 'already installed' in line.lower():\n                    # if installed version is newer than currently, we use old version name\n                    if 'newer than' in line.lower():\n                        # example: package json-c-0.12-1.x86_64 (which is newer than json-c-0.11-1.x86_64) is already installed\n                        package_name = line.split()[6].replace(')','')\n                    else:\n                        package_name = line.split()[1]\n                    rpm_name = \"%s.rpm\" % package_name\n                    rpm_set.remove(rpm_name)\n                    already_install = True\n                elif 'conflicts with file' in line.lower():\n                    # if the library file(s) is(are) the same as installed dependencies, we skip it and use the installed dependencies\n                    already_install = True\n                else:\n                    # This is unexpected, so bubble up the ExecutionError.\n                    if already_install is not True:\n                        raise\n\n        # MPP-14359 - installation and uninstallation prechecks must also consider\n        # the archive. That is, if a partial installation had added all rpms\n        # but failed to add the archive package, then for our purposes, we consider\n        # the package not yet installed and still in need of InstallPackageLocally.\n        archive_package_exists = CheckFile(os.path.join(GPPKG_ARCHIVE_PATH, self.gppkg.pkg)).run()\n        package_already_installed = (not rpm_set) and archive_package_exists\n        if package_already_installed:\n            raise AlreadyInstalledError(self.gppkg.pkg)\n\n        # Code path 1 (See docstring)\n        return rpm_set\n\n\nclass ValidateUninstallDebPackage(Operation):\n    \"\"\"\n     A replicate class of ValidateUninstallPackage that used for pre-uninstall deb package for Ubuntu.\n     We do not need to test dependencies in GPDB 6 as there is no GPDB's dependencies anymore.\n\n    \"\"\"\n\n    def __init__(self, gppkg):\n        self.gppkg = gppkg\n\n    def execute(self):\n        deb_list = [self.gppkg.main_rpm] + self.gppkg.dependencies\n\n        def strip_extension_and_arch(filename):\n            # expecting filename of form %{name}-%{version}-%{release}.%{arch}.rpm\n            rest = str.split(filename, '-')\n            return rest[-3]\n\n        # We check the installed list only\n        deb_set = set([strip_extension_and_arch(deb_package) for deb_package in deb_list])\n        archive_package_exists = CheckFile(os.path.join(GPPKG_ARCHIVE_PATH, self.gppkg.pkg)).run()\n        package_not_installed = (not deb_set) and (not archive_package_exists)\n        if package_not_installed:\n            raise NotInstalledError(self.gppkg.pkg)\n\n        return deb_set\n\n\nclass ValidateUninstallPackage(Operation):\n    \"\"\"\n    Ensure that the given rpms can be uninstalled safely. This is accomplished mainly\n    through use of rpm --test, which will have one of a few outcomes:\n    1) A return code of 0, indicating the uninstallation should proceed smoothly\n    2) A non-zero return code, and stderr indicating some of the rpms are already uninstalled.\n       We simply omit such rpms from the returned list of rpms, indicating to the caller\n       that to be successful, uninstallation should only be attempted on the filtered list of rpms.\n    3) A non-zero return code, and stderr indicating that dependencies remain.\n\n    Lastly, for an edge case, if we determine that none of the relevant rpms are currently installed\n    *and* the archive package does not exist, we declare the package is not installed.\n\n    TODO: Use regexes for more reliable string matching.\n    \"\"\"\n\n    def __init__(self, gppkg):\n        self.gppkg = gppkg\n\n    def execute(self):\n        rpm_list = [self.gppkg.main_rpm] + self.gppkg.dependencies\n\n        def strip_extension_and_arch(filename):\n            # expecting filename of form %{name}-%{version}-%{release}.%{arch}.rpm\n            rest, ext = os.path.splitext(filename)\n            rest, arch = os.path.splitext(rest)\n            return rest\n\n        rpm_set = set([strip_extension_and_arch(rpm) for rpm in rpm_list])\n        rpm_uninstall_string = ' '.join(rpm_set)\n        rpm_uninstall_command = 'rpm --test -e %s --dbpath %s' % (rpm_uninstall_string, RPM_DATABASE)\n        cmd = Command('Validating rpm uninstallation', rpm_uninstall_command)\n        logger.info(cmd)  # TODO: AK: This should be debug(), but RMI cannot propagate a log level.\n\n        try:\n            cmd.run(validateAfter=True)\n        except ExecutionError, e:\n            lines = e.cmd.get_results().stderr.splitlines()\n\n            # Forking between code paths 2 and 3 depends on some meaningful stderr\n            # Without such stderr, we must bubble up the ExecutionError.\n            if len(lines) == 0:\n                raise\n\n            if 'failed dependencies' in lines[0].lower():\n                # Code path 3 (see docstring)\n                # example stderr:\n                # error: Failed dependencies:\n                #    jre = 1.6.0_26 is needed by (installed) gphdfs-1.1-1.x86_64\n                self.resolve_shared_dependencies(rpm_set, lines[1:])\n            else:\n                # Code path 2, possibly (see docstring)\n                # example stderr:\n                #   error: package postgis-1.0-1.x86_64 is not installed\n                #   error: package proj-4.7.0-1.x86_64 is not installed\n                #   error: package geos-3.2.2-1.x86_64 is not installed\n                for line in lines:\n                    if 'not installed' in line.lower():\n                        package_name = line.split()[2]\n                        rpm_set.remove(package_name)\n                    else:\n                        # This is unexpected, so bubble up the ExecutionError.\n                        raise\n\n        # MPP-14359 - installation and uninstallation prechecks must also consider\n        # the archive. That is, if a partial uninstallation had removed all rpms\n        # but failed to remove the archive package, then for our purposes, we consider\n        # the package installed and still in need of UninstallPackageLocally.\n        archive_package_exists = CheckFile(os.path.join(GPPKG_ARCHIVE_PATH, self.gppkg.pkg)).run()\n        package_not_installed = (not rpm_set) and (not archive_package_exists)\n        if package_not_installed:\n            raise NotInstalledError(self.gppkg.pkg)\n\n        # Code path 1 (See docstring)\n        return rpm_set\n\n    def resolve_shared_dependencies(self, rpm_set, dependency_lines):\n        \"\"\"\n        This is a very naive resolution to shared dependencies. (See code path #3 in ValidateUninstallPackage.execute)\n\n        Among the rpms we attempt to remove from the system, a subset cannot be\n        removed during this particular gppkg uninstallation, because their removal would violate\n        the dependency constraints of other rpms that remain in the system; we simply leave these culprit rpm(s) behind.\n        More specifically, the preceding rpm --test -e command has given us the violated *capabilities*. For each *capability*,\n        we query the rpm database with --whatprovides to discern the culprit rpm(s).\n\n        In simpler terms, consider this example:\n            pljava depends on jre, which its gppkg contains\n            install the gppkgs for pljava\n            uninstall pljava gppkg\n            we determine that the jre rpm is responsible for *providing* \"jre = 1.6\"\n            so, we ultimately omit the jre rpm from our \"rpm -e\" and move on\n\n        TODO: AK: A more robust version of this function would ensure that the remaining\n        rpms are, in fact, bound by a remaining gppkg.  We defer this responsibility for now because gppkgs\n        should not have external dependencies. That is, no package should have requirements on rpms\n        not contained in its own gppkg distro. So, it's safe to assume that if foo is a culprit rpm, there exists\n        some gppkg bar that internally contains foo. (I realize that, with time, this will not be a scalable requirement\n        for gppkgs... hence the TODO.)\n\n        @type  rpm_set: set\n        @param rpm_set: rpms being uninstalled, among which there exists an rpm\n                        whose removal violates the dependencies of remaining rpms\n        @type  dependency_lines: list\n        @param dependency_lines: lines produced from the stderr in\n                                 code path #3 in ValidateUninstallPackage.execute\n                                 ex: [\"     jre >= 1.6.0_26 is needed by (installed) gphdfs-1.1-1.x86_64\"]\n        \"\"\"\n        for dependency_line in dependency_lines:\n            violated_capability = dependency_line.split()[0]  # e.g. \"jre\"\n            cmd = Command('Discerning culprit rpms for %s' % violated_capability,\n                          'rpm -q --whatprovides %s --dbpath %s' % (violated_capability, RPM_DATABASE))\n            cmd.run(validateAfter=True)\n            # remove the .x86_64 suffix for each rpm package to match the name in rpm_set\n            culprit_rpms = set(dep.replace('.x86_64', '') for dep in cmd.get_results().stdout.splitlines())\n            rpm_set -= culprit_rpms\n\n\nclass ExtractPackage(Operation):\n    \"\"\"\n    Extract the contents of the package into the temp folder\n\n    TODO: AK: Extraction should be implemented as a context manager.\n    \"\"\"\n\n    def __init__(self, gppkg):\n        self.gppkg = gppkg\n\n    def execute(self):\n        # clean up tmp extraction folder\n        if os.path.exists(TEMP_EXTRACTION_PATH) and not CleanupDir(TEMP_EXTRACTION_PATH).run():\n            logger.error('Could not clean temp folder')\n            raise IOError\n\n        # untar the package into tmp folder\n        with closing(tarfile.open(self.gppkg.abspath)) as tarinfo:\n            tarinfo.extractall(TEMP_EXTRACTION_PATH)\n\n        # move all the deps into same folder as the main rpm\n        path = os.path.join(TEMP_EXTRACTION_PATH, DEPS_DIR)\n        if os.path.exists(path):\n            for cur_file in os.listdir(path):\n                shutil.move(os.path.join(TEMP_EXTRACTION_PATH, DEPS_DIR, cur_file), TEMP_EXTRACTION_PATH)\n\n\nclass InstallDebPackageLocally(Operation):\n    \"\"\"\n    For deb package\n    \"\"\"\n    def __init__(self, package_path, is_update=False):\n        self.package_path = package_path\n        self.is_update = is_update\n\n    def execute(self):\n        current_package_location = self.package_path\n        package_name = os.path.basename(current_package_location)\n        logger.info('Installing %s locally' % package_name)\n        final_package_location = os.path.join(GPPKG_ARCHIVE_PATH, package_name)\n\n        gppkg = Gppkg.from_package_path(current_package_location)\n        ExtractPackage(gppkg).run()\n\n        # squash AlreadyInstalledError here: the caller doesn't ever need to\n        # know that we didn't have to do anything here\n        try:\n            deb_set = ValidateInstallDebPackage(gppkg, is_update=self.is_update).run()\n        except AlreadyInstalledError, e:\n            logger.info(e)\n            return\n\n        if deb_set:\n            deb_packages = \" \".join([os.path.join(TEMP_EXTRACTION_PATH, deb_package) for deb_package in deb_set])\n            if self.is_update:\n                deb_install_command = 'fakeroot dpkg --force-not-root --force-depends --log=/dev/null --admindir=%s ' \\\n                                      '--instdir=%s -i %s' % (DEB_DATABASE, GPHOME, deb_packages)\n            else:\n                deb_install_command = 'fakeroot dpkg --force-not-root --log=/dev/null --admindir=%s --instdir=%s -i %s' % \\\n                                      (DEB_DATABASE, GPHOME, deb_packages)\n\n            cmd = Command('Installing debs', deb_install_command)\n            logger.info(cmd)\n            cmd.run(validateAfter=True)\n\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n        shutil.copy(current_package_location, final_package_location)\n        logger.info(\"Completed local installation of %s.\" % package_name)\n\n\nclass InstallPackageLocally(Operation):\n    \"\"\"\n    Installs a package on the local host\n\n    This operation must take a slew of starting conditions and drive the state\n    of the local machine towards the ending state, in which the given package is successfully\n    installed, the rpm database is sane, and the package resides in the designated archive.\n    To that end, we indiscriminately squash AlreadyInstalledErrors arising from ValidateInstallPackage,\n    because in this context, it's not an exception, but rather an indication of our desired ending\n    conditions.\n\n    We must consider the following scenarios and more: package was deleted from archive,\n    the main comprising rpm was uninstalled, dependent rpms were removed, the rpm database was\n    corrupted, etc.\n\n    Again, much like ValidateInstallPackages, we make cheap reuse of this code for the purposes\n    of an --update as there is considerable commonality.\n    \"\"\"\n\n    def __init__(self, package_path, is_update=False):\n        self.package_path = package_path\n        self.is_update = is_update\n\n    def execute(self):\n        current_package_location = self.package_path\n        package_name = os.path.basename(current_package_location)\n        logger.info('Installing %s locally' % package_name)\n        final_package_location = os.path.join(GPPKG_ARCHIVE_PATH, package_name)\n\n        gppkg = Gppkg.from_package_path(current_package_location)\n        ExtractPackage(gppkg).run()\n\n        # squash AlreadyInstalledError here: the caller doesn't ever need to\n        # know that we didn't have to do anything here\n        try:\n            rpm_set = ValidateInstallPackage(gppkg, is_update=self.is_update).run()\n        except AlreadyInstalledError, e:\n            logger.info(e)\n            return\n\n        if rpm_set:\n            if self.is_update:\n                rpm_install_command = 'rpm -U --force %s --dbpath %s --prefix=%s'\n            else:\n                rpm_install_command = 'rpm -i --force %s --dbpath %s --prefix=%s'\n            rpm_install_command = rpm_install_command % \\\n                                  (\" \".join([os.path.join(TEMP_EXTRACTION_PATH, rpm) for rpm in rpm_set]),\n                                   RPM_DATABASE,\n                                   RPM_INSTALLATION_PATH)\n            cmd = Command('Installing rpms', rpm_install_command)\n            logger.info(cmd)\n            cmd.run(validateAfter=True)\n\n        # TODO: AK: MPP-15568\n        # TODO: AK: abstraction layer for archive interactions... to hide use of shutil.copy, RemoveFile, etc.\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n        shutil.copy(current_package_location, final_package_location)\n        logger.info(\"Completed local installation of %s.\" % package_name)\n\n\nclass UninstallDebPackageLocally(Operation):\n    \"\"\"\n     A replicate class of ValidateUninstallPackage that used for uninstall deb package for Ubuntu.\n     Same logic as UninstallPackageLocally\n\n    \"\"\"\n\n    def __init__(self, package_name):\n        self.package_name = package_name\n\n    def execute(self):\n        current_package_location = os.path.join(GPPKG_ARCHIVE_PATH, self.package_name)\n        gppkg = Gppkg.from_package_path(current_package_location)\n\n        # squash NotInstalledError here: the caller doesn't ever need to\n        # know that we didn't have to do anything here\n        try:\n            deb_set = ValidateUninstallDebPackage(gppkg).run()\n        except NotInstalledError, e:\n            logger.info(e)\n            return\n\n        if deb_set:\n            deb_uninstall_command = 'fakeroot dpkg --force-not-root --log=/dev/null --admindir=%s --instdir=%s -r %s' % (\n                DEB_DATABASE, GPHOME, \" \".join(deb_set))\n            cmd = Command('Uninstalling debs', deb_uninstall_command)\n            logger.info(cmd)\n            cmd.run(validateAfter=True)\n\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n        RemoveFile(current_package_location).run()\n        logger.info(\"Completed local uninstallation of %s.\" % self.package_name)\n\n\nclass UninstallPackageLocally(Operation):\n    \"\"\"\n    Uninstalls a package on the local host\n\n    This operation must take a slew of starting conditions and drive the state\n    of the local machine towards the ending state, in which the given package is successfully\n    uninstalled, the rpm database is sane, and the package is removed from the archive.\n    To that end, we indiscriminately squash NotInstalledErrors arising from ValidateUninstallPackage,\n    because in this context, it's not an exception, but rather an indication of our desired ending\n    conditions.\n\n    We must consider the following scenarios and more: package was deleted from archive,\n    the main comprising rpm was uninstalled, dependent rpms were removed, the rpm database was\n    corrupted, etc.\n    \"\"\"\n\n    def __init__(self, package_name):\n        self.package_name = package_name\n\n    def execute(self):\n        # TODO: AK: MPP-15737 - we're entirely dependent on the package residing in the archive\n        current_package_location = os.path.join(GPPKG_ARCHIVE_PATH, self.package_name)\n        gppkg = Gppkg.from_package_path(current_package_location)\n\n        # squash NotInstalledError here: the caller doesn't ever need to\n        # know that we didn't have to do anything here\n        try:\n            rpm_set = ValidateUninstallPackage(gppkg).run()\n        except NotInstalledError, e:\n            logger.info(e)\n            return\n\n        if rpm_set:\n            rpm_uninstall_command = 'rpm -e %s --dbpath %s' % (\" \".join(rpm_set), RPM_DATABASE)\n            cmd = Command('Uninstalling rpms', rpm_uninstall_command)\n            logger.info(cmd)\n            cmd.run(validateAfter=True)\n\n        # TODO: AK: abstraction layer for archive interactions... to hide use of shutil.copy, RemoveFile, etc.\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n        RemoveFile(current_package_location).run()\n        logger.info(\"Completed local uninstallation of %s.\" % self.package_name)\n\n\nclass SyncPackages(Operation):\n    \"\"\"\n    Synchronizes packages from master to a remote host\n\n    TODO: AK: MPP-15568\n    \"\"\"\n\n    def __init__(self, host):\n        self.host = host\n\n    def execute(self):\n        if not CheckDir(GPPKG_ARCHIVE_PATH).run():\n            MakeDir(GPPKG_ARCHIVE_PATH).run()\n        if not CheckRemoteDir(GPPKG_ARCHIVE_PATH, self.host).run():\n            MakeRemoteDir(GPPKG_ARCHIVE_PATH, self.host).run()\n\n        # set of packages on the master\n        master_package_set = set(ListFilesByPattern(GPPKG_ARCHIVE_PATH, '*' + GPPKG_EXTENSION).run())\n        # set of packages on the remote host\n        remote_package_set = set(ListRemoteFilesByPattern(GPPKG_ARCHIVE_PATH, '*' + GPPKG_EXTENSION, self.host).run())\n        # packages to be uninstalled on the remote host\n        uninstall_package_set = remote_package_set - master_package_set\n        # packages to be installed on the remote host\n        install_package_set = master_package_set - remote_package_set\n\n        if not install_package_set and not uninstall_package_set:\n            logger.info('The packages on %s are consistent.' % self.host)\n            return\n\n        if install_package_set:\n            logger.info(\n                'The following packages will be installed on %s: %s' % (self.host, ', '.join(install_package_set)))\n            for package in install_package_set:\n                logger.debug('copying %s to %s' % (package, self.host))\n                dstFile = os.path.join(GPHOME, package)\n                Scp(name='copying %s to %s' % (package, self.host),\n                    srcFile=os.path.join(GPPKG_ARCHIVE_PATH, package),\n                    dstFile=dstFile,\n                    dstHost=self.host).run(validateAfter=True)\n                if platform.linux_distribution()[0] == 'Ubuntu':\n                    RemoteOperation(InstallDebPackageLocally(dstFile), self.host).run()\n                else:\n                    RemoteOperation(InstallPackageLocally(dstFile), self.host).run()\n                RemoveRemoteFile(dstFile, self.host).run()\n\n        if uninstall_package_set:\n            logger.info(\n                'The following packages will be uninstalled on %s: %s' % (self.host, ', '.join(uninstall_package_set)))\n            for package in uninstall_package_set:\n                if platform.linux_distribution()[0] == 'Ubuntu':\n                    RemoteOperation(UninstallDebPackageLocally(package), self.host).run()\n                else:\n                    RemoteOperation(UninstallPackageLocally(package), self.host).run()\n\n\nclass InstallPackage(Operation):\n    def __init__(self, gppkg, master_host, standby_host, segment_host_list):\n        self.gppkg = gppkg\n        self.master_host = master_host\n        if master_host != standby_host:\n            self.standby_host = standby_host\n        else:\n            self.standby_host = None\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        logger.info('Installing package %s' % self.gppkg.pkg)\n\n        # TODO: AK: MPP-15736 - precheck package state on master\n        ExtractPackage(self.gppkg).run()\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            ValidateInstallDebPackage(self.gppkg).run()\n        else:\n            ValidateInstallPackage(self.gppkg).run()\n\n        # perform any pre-installation steps\n        PerformHooks(hooks=self.gppkg.preinstall,\n                     master_host=self.master_host,\n                     standby_host=self.standby_host,\n                     segment_host_list=self.segment_host_list).run()\n\n        # distribute package to segments\n        srcFile = self.gppkg.abspath\n        dstFile = os.path.join(GPHOME, self.gppkg.pkg)\n\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            # install package on segments\n            if self.segment_host_list:\n                GpScp(srcFile, dstFile, self.segment_host_list).run()\n                HostOperation(InstallDebPackageLocally(dstFile), self.segment_host_list).run()\n\n            # install package on standby\n            if self.standby_host:\n                Scp(name='copying %s to %s' % (srcFile, self.standby_host),\n                    srcFile=srcFile,\n                    dstFile=dstFile,\n                    dstHost=self.standby_host).run(validateAfter=True)\n                RemoteOperation(InstallDebPackageLocally(dstFile), self.standby_host).run()\n\n            # install package on master\n            InstallDebPackageLocally(srcFile).run()\n        else:\n            # install package on segments\n            if self.segment_host_list:\n                GpScp(srcFile, dstFile, self.segment_host_list).run()\n                HostOperation(InstallPackageLocally(dstFile), self.segment_host_list).run()\n\n            # install package on standby\n            if self.standby_host:\n                Scp(name='copying %s to %s' % (srcFile, self.standby_host),\n                    srcFile=srcFile,\n                    dstFile=dstFile,\n                    dstHost=self.standby_host).run(validateAfter=True)\n                RemoteOperation(InstallPackageLocally(dstFile), self.standby_host).run()\n\n            # install package on master\n            InstallPackageLocally(srcFile).run()\n\n        # perform any post-installation steps\n        PerformHooks(hooks=self.gppkg.postinstall,\n                     master_host=self.master_host,\n                     standby_host=self.standby_host,\n                     segment_host_list=self.segment_host_list).run()\n\n        logger.info('%s successfully installed.' % (self.gppkg.pkg))\n\n\nclass PerformHooks(Operation):\n    def __init__(self, hooks, master_host, standby_host, segment_host_list):\n        \"\"\"\n        Performs steps that have been specified in the yaml file for a particular\n        stage of gppkg execution\n\n        TODO: AK: A packager may have added commands to their hooks, with the\n        assumption that the current working directory would be that which contains\n        the spec file, rpms, and other artifacts (external scripts, perhaps.) To support\n        this, these commands should be prefixed with a \"cd\".\n\n        TODO: AK: I'm adding master_host for consistency.\n        But, why would we ever need master_host?  We're on the master host!\n        \"\"\"\n        self.hooks = hooks\n        self.master_host = master_host\n        if master_host != standby_host:\n            self.standby_host = standby_host\n        else:\n            self.standby_host = []\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        if self.hooks is None:\n            return\n        for hook in self.hooks:\n            key = hook.keys()\n            if key is None:\n                return\n            key_str = key[0]\n            if key_str.lower() == 'master':\n                if self.standby_host:\n                    RemoteCommand(hook[key_str], [self.standby_host]).run()\n                LocalCommand(hook[key_str], True).run()\n            elif key_str.lower() == 'segment':\n                RemoteCommand(hook[key_str], self.segment_host_list).run()\n            elif key_str.lower() == 'all':\n                if self.standby_host:\n                    RemoteCommand(hook[key_str], [self.standby_host]).run()\n                # Change on Master\n                LocalCommand(hook[key_str], True).run()\n                # Change on Segment hosts\n                RemoteCommand(hook[key_str], self.segment_host_list).run()\n\n\n\nclass UninstallPackage(Operation):\n    def __init__(self, gppkg, master_host, standby_host, segment_host_list):\n        self.gppkg = gppkg\n        self.master_host = master_host\n        if master_host != standby_host:\n            self.standby_host = standby_host\n        else:\n            self.standby_host = []\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        logger.info('Uninstalling package %s' % self.gppkg.pkg)\n\n        # TODO: AK: MPP-15736 - precheck package state on master\n        ExtractPackage(self.gppkg).run()\n\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            ValidateUninstallDebPackage(self.gppkg).run()\n        else:\n            ValidateUninstallPackage(self.gppkg).run()\n\n        # perform any pre-uninstallation steps\n        PerformHooks(hooks=self.gppkg.preuninstall,\n                     master_host=self.master_host,\n                     standby_host=self.standby_host,\n                     segment_host_list=self.segment_host_list).run()\n\n        # uninstall on segments\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            HostOperation(UninstallDebPackageLocally(self.gppkg.pkg), self.segment_host_list).run()\n\n            if self.standby_host:\n                RemoteOperation(UninstallDebPackageLocally(self.gppkg.pkg), self.standby_host).run()\n\n            UninstallDebPackageLocally(self.gppkg.pkg).run()\n\n        else:\n            HostOperation(UninstallPackageLocally(self.gppkg.pkg), self.segment_host_list).run()\n\n            if self.standby_host:\n                RemoteOperation(UninstallPackageLocally(self.gppkg.pkg), self.standby_host).run()\n\n            UninstallPackageLocally(self.gppkg.pkg).run()\n\n        # perform any post-installation steps\n        PerformHooks(hooks=self.gppkg.postuninstall,\n                     master_host=self.master_host,\n                     standby_host=self.standby_host,\n                     segment_host_list=self.segment_host_list).run()\n\n        logger.info('%s successfully uninstalled.' % self.gppkg.pkg)\n\n\nclass QueryPackage(Operation):\n    INFO, LIST, ALL = range(3)\n\n    def __init__(self, query_type, package_path):\n        self.query_type = query_type\n        self.package_path = package_path\n\n    def execute(self):\n        if self.query_type == QueryPackage.INFO:\n            def package_details(p):\n                yield 'Name', p.pkgname\n                yield 'Version', p.version\n                yield 'Architecture', p.architecture\n                yield 'OS', p.os\n                yield 'GPDBVersion', str(p.gpdbversion)\n                yield 'Description', p.description\n\n            def print_package_info(package):\n                tabLog = TableLogger()\n                for name, value in package_details(package):\n                    tabLog.info([name, value])\n                tabLog.outputTable()\n\n            package = Gppkg.from_package_path(self.package_path)\n            print_package_info(package)\n\n        elif self.query_type == QueryPackage.LIST:\n            package = Gppkg.from_package_path(self.package_path)\n            for file in package.file_list:\n                print file\n        elif self.query_type == QueryPackage.ALL:\n            package_name_list = ListPackages().run()\n            for package_name in package_name_list:\n                print package_name\n        else:\n            package = Gppkg.from_package_path(self.package_path)\n            try:\n                ExtractPackage(package).run()\n                ValidateInstallPackage(package).run()\n            except AlreadyInstalledError:\n                print '%s is installed.' % package.pkgname\n            else:\n                print '%s is not installed.' % package.pkgname\n\n\nclass BuildGppkg(Operation):\n    \"\"\"\n        Builds a gppkg given a directory containing\n        the spec file, rpms and any pre/post installation scripts\n    \"\"\"\n\n    def __init__(self, directory, filename):\n        self.directory = directory\n        self.filename = filename;\n\n    def execute(self):\n\n        directory = self.directory\n\n        logger.info('Building gppkg')\n\n        # Check if the directory is valid\n        if not os.path.exists(directory) or not os.path.isdir(directory):\n            logger.error('%s is an Invalid directory' % directory)\n            raise BuildPkgError\n\n        filelist = os.listdir(directory)\n\n        # Check for the spec file\n        specfile = directory + '/' + SPECFILE_NAME\n\n        if not os.path.exists(specfile):\n            logger.error(' Spec file does not exist')\n            raise BuildPkgError\n\n        # parse the spec file and get the name, version and arch\n        # this is used to name the gppkg\n        pkg_path_details = self._get_package_name_details(specfile)\n\n        if pkg_path_details is None:\n            raise BuildPkgError\n\n        # The file already exists. Rewrite the original with the new one\n        if self.filename is None:\n            pkg = pkg_path_details['pkgname'] + '-' + str(pkg_path_details['version']) + '-' + pkg_path_details[\n                'os'] + '-' + pkg_path_details['architecture'] + GPPKG_EXTENSION\n        else:\n            pkg = self.filename\n        if os.path.exists(pkg):\n            os.remove(pkg)\n\n        # Verify the spec file\n        if not self._verify_specfile(specfile, directory):\n            raise BuildPkgError\n\n        # tar and gzip the directory\n        # rename the file with .gppkg extension\n        with closing(tarfile.open(pkg, 'w:gz')) as tarinfo:\n            for cur_file in filelist:\n                tarinfo.add(name=os.path.join(directory, cur_file),\n                            arcname=cur_file)\n\n        logger.info('Completed building gppkg')\n\n    def _get_package_name_details(self, specfile):\n        \"\"\"\n            Get details about the name, version, operating system, architecture\n            of the package. The final gppkg which will be created\n            will be named as <name>-<version>-<os>-<arch>.gppkg\n        \"\"\"\n\n        logger.debug('_get_package_name_details')\n        cur_file = None\n\n        with open(specfile) as cur_file:\n            yamlfile = yaml.safe_load(cur_file)\n\n            tags = yamlfile.keys()\n\n            pkg_path_details = {}\n\n            # return all the required tags as a dict\n            for tag in tags:\n                if tag.lower() in SPECFILE_REQUIRED_TAGS:\n                    pkg_path_details[tag.lower()] = yamlfile[tag]\n\n            return pkg_path_details\n\n    def _verify_specfile(self, specfile, directory):\n        '''\n            Reads the spec file and makes sure that the tags are correct.\n        '''\n\n        logger.debug('_verify_specfile')\n        cur_file = None\n\n        try:\n            with open(specfile) as cur_file:\n                yamlfile = yaml.safe_load(cur_file)\n\n                if not self._verify_tags(yamlfile):\n                    return False\n\n                return True\n        except ScannerError, ex:\n            return False\n\n    def _verify_tags(self, yamlfile):\n        \"\"\"\n            Verify that the tags are valid.\n            Returns true if all tags are valid\n            False otherwise\n        \"\"\"\n\n        logger.debug('_verify_tags')\n        tags = yamlfile.keys()\n\n        tags = [tag.lower() for tag in tags]\n\n        # check required tags\n        for required_tag in SPECFILE_REQUIRED_TAGS:\n            if required_tag not in tags:\n                logger.error(' Required tag %s missing in Spec file' % required_tag)\n                return False\n\n        # check for invalid tags\n        for tag in tags:\n            if tag not in SPECFILE_OPTIONAL_TAGS and tag not in SPECFILE_REQUIRED_TAGS:\n                logger.error(' Invalid tag %s in Spec file' % tag)\n                return False\n\n        return True\n\n\nclass UpdatePackage(Operation):\n    \"\"\" TODO: AK: Enforce gppkg version is higher than currently installed version \"\"\"\n\n    def __init__(self, gppkg, master_host, standby_host, segment_host_list):\n        self.gppkg = gppkg\n        self.master_host = master_host\n        if master_host != standby_host:\n            self.standby_host = standby_host\n        else:\n            self.standby_host = []\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        logger.info('Updating package %s' % self.gppkg.pkg)\n\n        ExtractPackage(self.gppkg).run()\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            ValidateInstallDebPackage(self.gppkg, is_update=True).run()\n        else:\n            ValidateInstallPackage(self.gppkg, is_update=True).run()\n\n        # distribute package to segments\n        srcFile = self.gppkg.abspath\n        dstFile = os.path.join(GPHOME, self.gppkg.pkg)\n        GpScp(srcFile, dstFile, self.segment_host_list).run()\n\n        # update package on segments\n        HostOperation(UpdatePackageLocally(dstFile), self.segment_host_list).run()\n\n        # update package on standby\n        if self.standby_host:\n            Scp(name='copying %s to %s' % (srcFile, self.standby_host),\n                srcFile=srcFile,\n                dstFile=dstFile,\n                dstHost=self.standby_host).run(validateAfter=True)\n            RemoteOperation(UpdatePackageLocally(dstFile), self.standby_host).run()\n\n        # update package on master\n        UpdatePackageLocally(srcFile).run()\n\n        # perform any post-update steps\n        PerformHooks(hooks=self.gppkg.postupdate,\n                     master_host=self.master_host,\n                     standby_host=self.standby_host,\n                     segment_host_list=self.segment_host_list).run()\n\n        logger.info('%s successfully updated.' % (self.gppkg.pkg))\n\n\nclass UpdatePackageLocally(Operation):\n    \"\"\"\n    Updates a package on the local host\n\n    We make cheap reuse of InstallPackageLocally with the propagation of is_update = True, which\n    effectively changes the rpm --test command to use -U instead of -i. Beyond the invocation of\n    InstallPackageLocally, here, we also clean up the archive directory to remove other (ideally, older)\n    versions of the updated package.\n    \"\"\"\n\n    def __init__(self, package_path):\n        self.package_path = package_path\n\n    def execute(self):\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            InstallDebPackageLocally(self.package_path, is_update=True).run()\n        else:\n            InstallPackageLocally(self.package_path, is_update=True).run()\n\n        # Remove other versions of the package from archive.\n        # Note: Do not rely on filename format to discern such packages.\n        # Rather, interrogate a package only through the Gppkg class interface.\n        current_package = Gppkg.from_package_path(self.package_path)\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n        archived_package_paths = ListFiles(GPPKG_ARCHIVE_PATH).run()\n        for archived_package_path in archived_package_paths:\n            temp_package = Gppkg.from_package_path(os.path.join(GPPKG_ARCHIVE_PATH, archived_package_path))\n            if temp_package.pkgname == current_package.pkgname and temp_package.version != current_package.version:\n                RemoveFile(os.path.join(GPPKG_ARCHIVE_PATH, archived_package_path)).run()\n\n\nclass CleanGppkg(Operation):\n    \"\"\"\n        Cleans up the Gppkg from the cluster in case of partial\n        installation or removal. This might not be required if\n        we can make the install and uninstall options idempotent.\n        This operation is exactly the same as remove but we dont\n        check on each host to see if the rpm is installed or not.\n    \"\"\"\n\n    def __init__(self, standby_host, segment_host_list):\n        self.standby_host = standby_host\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        operations = [SyncPackages(host) for host in self.segment_host_list]\n\n        if self.standby_host:\n            operations.append(SyncPackages(self.standby_host))\n\n        ParallelOperation(operations).run()\n\n        err_msgs = 'SyncPackages failed:'\n        exceptions = \"\"\n        for operation in operations:\n            try:\n                operation.get_ret()\n            except Exception, e:\n                exceptions += '\\n'+str(e)\n\n        if exceptions:\n            raise ExceptionNoStackTraceNeeded(\"%s%s\" % (err_msgs, exceptions))\n\n        logger.info('Successfully cleaned the cluster')\n\n\nclass MigratePackages(Operation):\n    \"\"\"\n    Migrates packages from another $GPHOME to this one\n\n    This functionality is meant to facilitate minor version upgrade, whereby old packages\n    need to be brought over from the older $GPHOME to the newer $GPHOME.\n\n    Presumably, this could also be used to migrate packages across arbitrary choices\n    of $GPHOMEs. However, the migration will only succeed if the packages being migrated\n    are actually compatible with the target GPDB.\n    \"\"\"\n\n    def __init__(self, from_gphome, to_gphome, standby_host, segment_host_list):\n        self.from_gphome = from_gphome\n        self.to_gphome = to_gphome\n        self.standby_host = standby_host\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        if not os.path.samefile(self.to_gphome, GPHOME):\n            raise ExceptionNoStackTraceNeeded(\n                'The target GPHOME, %s, must match the current $GPHOME used to launch gppkg.' % self.to_gphome)\n        if os.path.samefile(self.to_gphome, self.from_gphome):\n            raise ExceptionNoStackTraceNeeded(\n                'The source and target GPHOMEs, %s => %s, must differ for packages to be migrated.' % (\n                self.from_gphome, self.to_gphome))\n\n        # TODO: AK: Given an invalid from_gphome, we'll end up creating a 'share/packages' subdirectory within it.\n        old_archive_path = os.path.join(self.from_gphome, ARCHIVE_PATH)\n        MakeDir(old_archive_path).run()\n        packages = ListFilesByPattern(old_archive_path, '*' + GPPKG_EXTENSION).run()\n        if not packages:\n            logger.info('There are no packages to migrate from %s.' % self.from_gphome)\n            return\n\n        logger.info('The following packages will be migrated: %s' % ', '.join(packages))\n        for package in packages:\n            package_path = os.path.join(old_archive_path, package)\n            try:\n                if platform.linux_distribution()[0] == 'Ubuntu':\n                    InstallDebPackageLocally(package_path).run()\n                else:\n                    InstallPackageLocally(package_path).run()\n            except AlreadyInstalledError:\n                logger.info(\"%s is already installed.\" % package)\n            except Exception:\n                logger.exception(\"Failed to migrate %s from %s\" % (old_archive_path, package))\n\n        CleanGppkg(self.standby_host, self.segment_host_list).run()\n\n        logger.info('The package migration has completed.')\n\n\nclass GpScp(Operation):\n    \"\"\"\n    TODO: AK: This obviously does not belong here. My preference would be that it remain here until\n    the following problem is solved.\n\n    MPP-15270 - Improve performance of file transfer across large clusters\n\n    I suggest:\n\n        We consume an extra parameter 'fanout'. We partition the host_list into a number of buckets\n        given by 'fanout'. For each bucket, we scp the artifact to the first host in the bucket, and then\n        we recursively invoke GpScp on that machine for the remaining hosts in its bucket.\n\n        GpScp := ParallelOperation([ A(i) for i in range(0, n) ])\n        A := SerialOperation(B, C)\n        B := scp source_path target_path @ host_i\n            where host_i := the first host in the ith bucket\n        C := RemoteOperation(GpScp(target_path, target_path, host_list_i))\n            where host_list_i := the remaining hosts in the ith bucket\n    \"\"\"\n\n    def __init__(self, source_path, target_path, host_list):\n        self.source_path = source_path\n        self.target_path = target_path\n        self.host_list = host_list\n        self.pool = None\n\n    def execute(self):\n        self.pool = WorkerPool()\n        for host in self.host_list:\n            self.pool.addCommand(Scp(name='copying %s to %s' % (self.source_path, host),\n                                     srcFile=self.source_path,\n                                     dstFile=self.target_path,\n                                     dstHost=host))\n        self.pool.join()\n\n\nclass HostOperation(Operation):\n    \"\"\"\n    TODO: AK: This obviously does not belong here. My preference would be to move it to gppylib.operations.utils\n    when another consumer becomes clear.\n\n    TODO: AK: For generality, the underlying operation should inherit/implement NestedHostOperation so that\n    it may be initialized with information about the host to which it's been bound. This is fortunately not necessary\n    for our purposes here, so it's deferrable.\n\n    TODO: AK: Build a SegHostOperation that wraps this and is driven by GpArray content.\n\n    TODO: AK: Implement something similar for a SegmentOperation + NestedSegmentOperation.\n\n    TODO: AK: This (as well as ParallelOperation) would benefit from an appropriate choice of return value. The likely\n    choice would be: [op.get_ret() for op in self.operations]\n    \"\"\"\n\n    def __init__(self, operation, host_list):\n        self.operation = operation\n        self.host_list = host_list\n\n    def execute(self):\n        operations = []\n        for host in self.host_list:\n            operations.append(RemoteOperation(self.operation, host))\n        ParallelOperation(operations).run()\n        for operation in operations:\n            operation.get_ret()\n"], "fixing_code": ["# Line too long - pylint: disable=C0301\n# Copyright (c) Greenplum Inc 2011. All Rights Reserved.\n\nfrom contextlib import closing\nimport os\nimport platform\nimport shutil\nimport sys\nimport tarfile\n\ntry:\n    from gppylib import gplog\n    from gppylib.commands import gp\n    from gppylib.commands.base import Command, REMOTE, WorkerPool, ExecutionError\n    from gppylib.commands.unix import Scp\n    from gppylib.gpversion import GpVersion\n    from gppylib.mainUtils import ExceptionNoStackTraceNeeded\n    from gppylib.operations import Operation\n    from gppylib.operations.utils import RemoteOperation, ParallelOperation\n    from gppylib.operations.unix import CheckFile, CheckDir, MakeDir, RemoveFile, RemoveRemoteTree, RemoveRemoteFile, \\\n        CheckRemoteDir, MakeRemoteDir, CheckRemoteFile, ListRemoteFilesByPattern, ListFiles, ListFilesByPattern\n    from gppylib.utils import TableLogger\n\n    import yaml\n    from yaml.scanner import ScannerError\nexcept ImportError, ex:\n    sys.exit(\n        'Operation: Cannot import modules.  Please check that you have sourced greenplum_path.sh.  Detail: ' + str(ex))\n\nlogger = gplog.get_default_logger()\n\n\ndef dereference_symlink(path):\n    \"\"\"\n    MPP-15429: rpm is funky with symlinks...\n    During an rpm -e invocation, rpm mucks with the /usr/local/greenplum-db symlink.\n    From strace output, it appears that rpm tries to rmdir any directories it may have created during\n    package installation. And, in the case of our GPHOME symlink, rpm will actually try to unlink it.\n    To avoid this scenario, we perform all rpm actions against the \"symlink dereferenced\" $GPHOME.\n    \"\"\"\n    path = os.path.normpath(path)\n    if not os.path.islink(path):\n        return path\n    link = os.path.normpath(os.readlink(path))\n    if os.path.isabs(link):\n        return link\n    return os.path.join(os.path.dirname(path), link)\n\n\nGPHOME = dereference_symlink(gp.get_gphome())\n\nGPPKG_EXTENSION = '.gppkg'\nSPECFILE_NAME = 'gppkg_spec.yml'\nSPECFILE_REQUIRED_TAGS = ['pkgname', 'version', 'architecture', 'os', 'description', 'gpdbversion']\nSPECFILE_OPTIONAL_TAGS = ['preinstall', 'postinstall', 'preuninstall', 'postuninstall', 'postupdate']\n\n# TODO: AK: Our interactions with the internal RPM database could benefit from an abstraction layer\n# that hides the underlying commands used for installation, uninstallation, queries, etc.\nRPM_DATABASE_PATH = 'share/packages/database'\nRPM_DATABASE = os.path.join(GPHOME, RPM_DATABASE_PATH)\nRPM_INSTALLATION_PATH = GPHOME\n\n# An Update for deb package\nDEB_DATABASE_DIR = GPHOME + '/share/packages/database/deb/{info,updates,triggers}'\nDEB_DATABASE = os.path.join(GPHOME + '/share/packages/database/deb')\nDEB_STATUSFILE = GPHOME + '/share/packages/database/deb/status'\n\n# TODO: AK: Our interactions with the archive could benefit from an abstraction layer\n# that hides the implementations of archival, unarchival, queries, etc.\n# That is, consider the query \"is this package already archived?\" Currently, this is implemented\n# with a CheckFile. Rather, it should be a call to Archive.contains(package), where package\n# is instanceof Gppkg.\nARCHIVE_PATH = 'share/packages/archive'\nGPPKG_ARCHIVE_PATH = os.path.join(GPHOME, ARCHIVE_PATH)\n\n# TODO: AK: Shouldn't this be \"$GPHOME/.tmp\"?\n# i.e. what if remote host has its $GPHOME elsewhere?\nTEMP_EXTRACTION_PATH = GPHOME + '/.tmp'\nDEPS_DIR = 'deps'\n\n\nclass GpdbVersionError(Exception):\n    \"\"\"\n        Exception to notify that the gpdb version\n        does not match\n    \"\"\"\n    pass\n\n\nclass AlreadyInstalledError(Exception):\n    def __init__(self, package_name):\n        Exception.__init__(self, '%s is already installed.' % package_name)\n\n\nclass NotInstalledError(Exception):\n    def __init__(self, package_name):\n        Exception.__init__(self, '%s is not installed.' % package_name)\n\n\nclass BuildPkgError(Exception):\n    \"\"\"\n        Exception to notify that there was an error during\n        the building of a gppkg\n    \"\"\"\n    pass\n\n\nclass MissingDependencyError(Exception):\n    \"\"\"\n        Exception to catch missing dependency\n    \"\"\"\n\n    def __init__(self, value):\n        Exception.__init__(self, 'Dependency %s is missing' % value)\n\n\nclass OSCompatibilityError(Exception):\n    \"\"\"\n        Exception to notify that OS does not meet the\n        requirement\n    \"\"\"\n\n    def __init__(self, requiredos, foundos):\n        Exception.__init__(self, '%s OS required. %s OS found' % (requiredos, foundos))\n\n\nclass ArchCompatibilityError(Exception):\n    \"\"\"\n        Exception to notify that architecture does not meet\n        the requirement\n    \"\"\"\n\n    def __init__(self, requiredarch, foundarch):\n        Exception.__init__(self, '%s Arch required. %s Arch found' % (requiredarch, foundarch))\n\n\nclass RequiredDependencyError(Exception):\n    \"\"\"\n        Exception to notify that the package being uninstalled\n        is a dependency for another package\n    \"\"\"\n    pass\n\n\nclass Gppkg:\n    \"\"\"\n        This class stores all the information about a gppkg\n    \"\"\"\n\n    def __init__(self, pkg, pkgname, main_rpm, version, architecture, os, gpdbversion, description, abspath, preinstall,\n                 postinstall, preuninstall, postuninstall, postupdate, dependencies, file_list):\n        \"\"\"\n            The constructor takes the following arguments\n            pkg             The complete package name e.g pgcrypto-1.0-Darwin-i386.gppkg        TODO: AK: This is an awful variable name. Change to \"package_filename\".\n            pkgname         The name of the package as specified in the spec file\n            main_rpm        The name of the main rpm. e.g PL/R, PostGIS etc\n            version         The version of the gppkg\n            architecture    The architecture for which the package is built\n            os              The operating system for which the package is built\n            gpdbversion     The Greenplum Database version for which package is built\n            description     A short description for the package\n            abspath         This is the absolute path where the package sits on the host\n            preinstall      The cluster level preinstallation hooks\n            postinstall     The cluster level postinstallation hooks\n            preuninstall    The cluster level preuninstallation hooks\n            postuninstall   The cluster level postuninstallation hooks\n            postupdate      The cluster level postupdate hooks\n            dependencies    The dependencies of the package. e.g Geos, Proj in case of PostGIS\n            file_list       The list of files present in the package\n        \"\"\"\n\n        logger.debug('Gppkg Constructor')\n\n        self.pkg = pkg\n        self.pkgname = pkgname\n        self.main_rpm = main_rpm\n        self.version = version\n        self.architecture = architecture\n        self.os = os\n        self.gpdbversion = gpdbversion\n        self.description = description\n        self.abspath = abspath\n        self.preinstall = preinstall\n        self.postinstall = postinstall\n        self.preuninstall = preuninstall\n        self.postuninstall = postuninstall\n        self.postupdate = postupdate\n        self.dependencies = dependencies\n        self.file_list = file_list\n\n    @staticmethod\n    def from_package_path(pkg_path):\n        '''\n             This method takes a package as the argument and\n             obtains all the information about the package\n             Details include name, arch, OS, version, description, dependencies,\n             list of files present in the package and returns a gppkg object\n        '''\n\n        logger.debug('from_package_path')\n\n        if not os.path.exists(pkg_path):\n            logger.error('Cannot find package %s' % pkg_path)\n            raise IOError\n\n        # We check for a directory first because\n        # is_tarfile does not accept directories as path names\n        if os.path.isdir(pkg_path):\n            logger.error('%s is a directory !' % pkg_path)\n            raise IOError\n\n        if not tarfile.is_tarfile(pkg_path) or not pkg_path.endswith(GPPKG_EXTENSION):\n            logger.error('%s is Not a valid package' % pkg_path)\n            raise IOError\n\n        if os.path.getsize(pkg_path) == 0:\n            logger.error('Package is empty')\n            raise IOError\n\n        pkg = {}\n\n        # XXX: AK: It's purely coincidence that the optional tags are lists.\n        for tag in SPECFILE_REQUIRED_TAGS:\n            pkg[tag] = ''\n        for tag in SPECFILE_OPTIONAL_TAGS:\n            pkg[tag] = []\n\n        pkg['file_list'] = []\n        pkg['dependencies'] = []\n\n        with closing(tarfile.open(pkg_path, 'r:gz')) as tarinfo:\n            # store the list of all files present in the archive\n            archive_list = tarinfo.getnames()\n            pkg[\"file_list\"] = archive_list\n            keys = []\n            yamlfile = {}\n\n            # The spec file has to be called gppkg_spec\n            # so there will only be one such file,\n            for cur_file in archive_list:\n                if cur_file.endswith(SPECFILE_NAME):\n                    specfile = tarinfo.extractfile(cur_file)\n                    yamlfile = yaml.safe_load(specfile)\n                    keys = yamlfile.keys()\n                    break\n\n        # store all the tags\n        for key in keys:\n            pkg[key.lower()] = yamlfile[key]\n\n        # update the pkgpath\n        pkg['pkg'] = os.path.split(pkg_path)[-1]\n\n        # make the version as string\n        pkg['version'] = str(pkg['version'])\n\n        # Convert the required version to a GpVersion\n        pkg['gpdbversion'] = GpVersion(str(pkg['gpdbversion']))\n\n        # update the absolute path\n        pkg['abspath'] = pkg_path\n\n        # store all the dependencies of the gppkg\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            for cur_file in archive_list:\n                if cur_file.find('deps/') != -1 and cur_file.endswith('.deb'):\n                    pkg['dependencies'].append(cur_file[cur_file.rfind('/') + 1:])\n\n            # store the main deb, remain the attr name main_rpm to keep compatibility\n            for cur_file in archive_list:\n                if cur_file.find('deps/') == -1 and cur_file.endswith('.deb'):\n                    pkg['main_rpm'] = cur_file\n\n        else:\n            for cur_file in archive_list:\n                if cur_file.find('deps/') != -1 and cur_file.endswith('.rpm'):\n                    pkg['dependencies'].append(cur_file[cur_file.rfind('/') + 1:])\n\n            # store the main rpm\n            for cur_file in archive_list:\n                if cur_file.find('deps/') == -1 and cur_file.endswith('.rpm'):\n                    pkg['main_rpm'] = cur_file\n\n        gppkg = Gppkg(**pkg)\n\n        return gppkg\n\n\nclass LocalCommand(Operation):\n    \"\"\"\n    DEPRECATED\n\n    TODO: AK: Eliminate this. Replace invocations with Command(...).run(validateAfter = True)\n    \"\"\"\n\n    def __init__(self, cmd_str, echo=False):\n        self.cmd_str = cmd_str\n        self.echo = echo\n\n    def execute(self):\n\n        logger.debug(self.cmd_str)\n        cmd = Command(name='LocalCommand', cmdStr=self.cmd_str)\n        cmd.run(validateAfter=True)\n        if self.echo:\n            echo_str = cmd.get_results().stdout.strip()\n            if echo_str:\n                logger.info(echo_str)\n        return cmd.get_results()\n\n\nclass RemoteCommand(Operation):\n    \"\"\"\n    DEPRECATED\n\n    TODO: AK: Rename as GpSsh, like GpScp below.\n    \"\"\"\n\n    def __init__(self, cmd_str, host_list):\n        self.cmd_str = cmd_str\n        self.host_list = host_list\n        self.pool = None\n\n    def execute(self):\n        logger.debug(self.cmd_str)\n\n        # Create Worker pool\n        # and add commands to it\n        self.pool = WorkerPool()\n\n        for host in self.host_list:\n            cmd = Command(name='Remote Command', cmdStr=self.cmd_str, ctxt=REMOTE, remoteHost=host)\n            self.pool.addCommand(cmd)\n        self.pool.join()\n\n        # This will raise ExecutionError exception if even a single command fails\n        self.pool.check_results()\n\n\nclass ListPackages(Operation):\n    \"\"\"\n        Lists all the packages present in\n        $GPHOME/share/packages/archive\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def execute(self):\n        # Ensure archive path exists\n        # TODO: AK: In hindsight, this should've been named MakeDirP,\n        # to reflect that it won't blow up if the path already exists.\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n\n        package_list = ListFilesByPattern(GPPKG_ARCHIVE_PATH, '*' + GPPKG_EXTENSION).run()\n\n        package_name_list = []\n\n        for pkg in package_list:\n            pkg_name = pkg.split('/')[-1]\n            if len(pkg_name.split('-')) >= 2:\n                package_name_list.append(pkg_name[:pkg_name.index('-', pkg_name.index('-') + 1)])\n            else:\n                try:\n                    package_name_list.append(pkg_name[:pkg_name.index('.')])\n                except ValueError:\n                    raise Exception('unable to parse %s as a gppkg' % pkg_name)\n\n        return package_name_list\n\n\nclass CleanupDir(Operation):\n    \"\"\"\n        Cleans up the given dir\n        Returns True if either the dir is already removed\n        or if we were able to remove the dir successfully\n        False for other errors\n    \"\"\"\n\n    def __init__(self, dir_path):\n        self.dir_path = dir_path\n\n    def execute(self):\n\n        dir_path = self.dir_path\n\n        logger.debug('Cleaning up %s' % dir_path)\n\n        # If file does not exist, nothing to remove\n        # So we return true\n        if not os.path.exists(dir_path):\n            return True\n\n        if os.path.isdir(dir_path):\n            shutil.rmtree(dir_path)\n        else:\n            return False\n\n        return True\n\n\nclass IsVersionCompatible(Operation):\n    \"\"\"\n        Returns True if the gppkg is compatible\n        with the gpdb version that has been installed\n    \"\"\"\n\n    def __init__(self, gppkg):\n        super(IsVersionCompatible, self).__init__()\n        self.gppkg = gppkg\n\n    def execute(self):\n\n        gppkg = self.gppkg\n\n        gpdb_version = self._get_gpdb_version()\n        required_gpdb_version = gppkg.gpdbversion\n\n        logger.debug('Greenplum Database Version = %s' % gpdb_version)\n        logger.debug('Required Greenplum Database version = %s' % required_gpdb_version)\n\n        if gpdb_version is None:\n            logger.error('Could not determine Greenplum Database version')\n            return False\n\n        if not required_gpdb_version.isVersionRelease(gpdb_version):\n            logger.error('%s requires Greenplum Database version %s' % (gppkg.pkgname, required_gpdb_version))\n            return False\n\n        return True\n\n    def _get_gpdb_version(self):\n        \"\"\"\n            Get the version of the current GPDB\n            Returns a string consisting of the major\n            release version\n        \"\"\"\n        logger.debug('_get_gpdb_version')\n        self.gphome = gp.get_gphome()\n        version = gp.GpVersion.local('local GP software version check', self.gphome)\n        gpdb_version = GpVersion(version.strip())\n        return gpdb_version\n\n    def _convert_to_magic_number_version(self, gpversion_obj):\n        \"\"\"\n            Converts GPDB version to the GPDB magic number\n            Returns an int consisting of the major and minor release version\n        \"\"\"\n        logger.debug('_convert_to_magic_number_version')\n\n        ver_list = gpversion_obj.version\n\n        # The generation of the magic version number (GP_VERSION_NUM) is\n        # retrieved from our configure.in file\n        magic_num = \"%d%02d%02d\" % (ver_list[0], ver_list[1],\n                                    ver_list[2] if len(ver_list) > 2 else 0)\n        return int(magic_num)\n\n\nclass ValidateInstallDebPackage(Operation):\n    \"\"\"\n    A replicate class of ValidateInstallPackage that used for test deb package for Ubuntu,\n    it will use dpkg --verify that is instead of rpm --test\n\n    NOTE: In newest GPDB, the dependencies will use system's dependencies, we still leave the gppkg\n    to install dependencies, but did not check it anymore.\n\n    \"\"\"\n\n    def __init__(self, gppkg, is_update=False):\n        self.gppkg = gppkg\n        self.is_update = is_update\n\n    def execute(self):\n        # Check the GPDB requirements\n        if not IsVersionCompatible(self.gppkg).run():\n            raise GpdbVersionError\n\n        self.prepare_deb_env()\n\n        # No need to check the architecture here as there is no i386 arch in GPDB 5 or newer version\n        deb_set = set([self.gppkg.main_rpm] + self.gppkg.dependencies)\n        deb_packages_path = ' '.join([os.path.join(TEMP_EXTRACTION_PATH, deb_path) for deb_path in deb_set])\n\n        if self.is_update:\n            deb_test_command = 'fakeroot dpkg --dry-run --force-not-root --force-depends --log=/dev/null --admindir=%s ' \\\n                               '--instdir=%s -i %s' % (DEB_DATABASE, GPHOME, deb_packages_path)\n        else:\n            deb_test_command = 'fakeroot dpkg --dry-run --force-not-root --log=/dev/null --admindir=%s ' \\\n                               '--instdir=%s -i %s' % (DEB_DATABASE, GPHOME, deb_packages_path)\n\n        cmd = Command('Validating deb installation', deb_test_command)\n        logger.info(cmd)\n\n        try:\n            cmd.run(validateAfter=True)\n        except ExecutionError, e:\n            lines = e.cmd.get_results().stderr.splitlines()\n\n            if len(lines) == 0:\n                raise\n\n            # TODO: Add dependency and already installed check\n\n        archive_package_exists = CheckFile(os.path.join(GPPKG_ARCHIVE_PATH, self.gppkg.pkg)).run()\n        package_already_installed = (not deb_set) and archive_package_exists\n        if package_already_installed:\n            raise AlreadyInstalledError(self.gppkg.pkg)\n\n        already_installed = self.check_existence()\n        if self.is_update:\n            if not already_installed:\n                raise NotInstalledError(self.gppkg.pkg)\n        else:\n            if already_installed:\n                raise AlreadyInstalledError(self.gppkg.pkg)\n\n        return deb_set\n\n    def check_existence(self):\n        command = 'dpkg --force-not-root --log=/dev/null --admindir=%s --instdir=%s -l %s' % (DEB_DATABASE, GPHOME, self.gppkg.pkgname)\n        cmd = Command('Check installation of package', command)\n        logger.info(cmd)\n\n        cmd.run(validateAfter=False)\n        return cmd.results.rc == 0\n\n    def prepare_deb_env(self):\n        prepare_env_dir = 'mkdir -p ' + DEB_DATABASE_DIR\n        prepare_env_file = 'touch ' + DEB_STATUSFILE\n        cmd_dir = Command('Prepare deb package DIY directories', prepare_env_dir)\n        cmd_file = Command('Prepare deb package Status file', prepare_env_file)\n\n        try:\n            cmd_dir.run(validateAfter=True)\n            cmd_file.run(validateAfter=True)\n        except ExecutionError, e:\n            lines = e.cmd.get_results().stderr.splitlines()\n\n            raise ExecutionError('Can not setup deb package env', lines)\n\n\nclass ValidateInstallPackage(Operation):\n    \"\"\"\n    Ensure that the given rpms can be installed safely. This is accomplished mainly\n    through use of rpm --test, which will have one of a few outcomes:\n    1) A return code of 0, indicating the installation should proceed smoothly\n    2) A non-zero return code, and stderr indicating some of the rpms are already installed.\n       We simply omit such rpms from the returned list of rpms, indicating to the caller\n       that to be successful, installation should only be attempted on the filtered list of rpms.\n    3) A non-zero return code, and stderr indicating that a failed dependency issue will arise.\n       This scenario must result in a MissingDependencyError.\n\n    Note: install and update share this code, because there is extensive commonality in regards\n    to the version, os, arch. checking, in addition to the 3 code paths enumerated just above.\n\n    Lastly, for an edge case, if we determine that all of the relevant rpms are currently installed\n    *and* the archive package already exists we declare the package is already installed.\n\n    TODO: This is depending on ExtractPackage having put the dependencies in this same directory.\n    TODO: Use regexes for more reliable string matching. CR-2865#c20112\n    \"\"\"\n\n    def __init__(self, gppkg, is_update=False):\n        self.gppkg = gppkg\n        self.is_update = is_update\n\n    def execute(self):\n        # Check the GPDB requirements\n        if not IsVersionCompatible(self.gppkg).run():\n            raise GpdbVersionError\n\n        # TODO: AK: I've changed our use of the OS tag from 'Linux' to 'rhel5' or 'suse10'.\n        # So, the two lines below will not work properly.\n        # if self.gppkg.os.lower() != platform.system().lower():\n        #    raise OSCompatibilityError(self.gppkg.os, platform.system().lower())\n\n        # architecture compatibility\n        if self.gppkg.architecture.lower() != platform.machine().lower():\n            raise ArchCompatibilityError(self.gppkg.architecture, platform.machine().lower())\n\n        rpm_set = set([self.gppkg.main_rpm] + self.gppkg.dependencies)\n        rpm_install_string = ' '.join([os.path.join(TEMP_EXTRACTION_PATH, rpm) for rpm in rpm_set])\n        if self.is_update:\n            rpm_install_command = 'rpm --test -U --force %s --dbpath %s --prefix %s' % (\n            rpm_install_string, RPM_DATABASE, RPM_INSTALLATION_PATH)\n        else:\n            rpm_install_command = 'rpm --test -i %s --dbpath %s --prefix %s' % (\n            rpm_install_string, RPM_DATABASE, RPM_INSTALLATION_PATH)\n        cmd = Command('Validating rpm installation', rpm_install_command)\n        logger.info(cmd)  # TODO: AK: This should be debug(), but RMI cannot propagate a log level.\n\n        try:\n            cmd.run(validateAfter=True)\n        except ExecutionError, e:\n            already_install = False\n            lines = e.cmd.get_results().stderr.splitlines()\n\n            # Forking between code paths 2 and 3 depends on some meaningful stderr\n            # Without such stderr, we must bubble up the ExecutionError.\n            if len(lines) == 0:\n                raise\n\n            if 'failed dependencies' in lines[0].lower():\n                # Code path 3 (see docstring)\n                # example stderr:\n                # error: Failed dependencies:\n                #    geos-3.2.2-1.x86_64.rpm is needed by postgis-1.0-1.x86_64\n\n                # TODO: AK: Dependencies should be parsed out here and used to initialize\n                # this MissingDependencyError. However, this exception does not support\n                # multiple missing dependencies. Some refactoring work is needed in both places.\n                logger.error(e.cmd.get_results().stderr)\n                raise MissingDependencyError('')\n\n            # Code path 2, possibly (see docstring)\n            # example stderr:\n            #    package geos-3.2.2-1.x86_64 is already installed\n            #    package proj-4.7.0-1.x86_64 is already installed\n            #    package postgis-1.0-1.x86_64 is already installed\n            for line in lines:\n                if 'already installed' in line.lower():\n                    # if installed version is newer than currently, we use old version name\n                    if 'newer than' in line.lower():\n                        # example: package json-c-0.12-1.x86_64 (which is newer than json-c-0.11-1.x86_64) is already installed\n                        package_name = line.split()[6].replace(')','')\n                    else:\n                        package_name = line.split()[1]\n                    rpm_name = \"%s.rpm\" % package_name\n                    rpm_set.remove(rpm_name)\n                    already_install = True\n                elif 'conflicts with file' in line.lower():\n                    # if the library file(s) is(are) the same as installed dependencies, we skip it and use the installed dependencies\n                    already_install = True\n                else:\n                    # This is unexpected, so bubble up the ExecutionError.\n                    if already_install is not True:\n                        raise\n\n        # MPP-14359 - installation and uninstallation prechecks must also consider\n        # the archive. That is, if a partial installation had added all rpms\n        # but failed to add the archive package, then for our purposes, we consider\n        # the package not yet installed and still in need of InstallPackageLocally.\n        archive_package_exists = CheckFile(os.path.join(GPPKG_ARCHIVE_PATH, self.gppkg.pkg)).run()\n        package_already_installed = (not rpm_set) and archive_package_exists\n        if package_already_installed:\n            raise AlreadyInstalledError(self.gppkg.pkg)\n\n        # Code path 1 (See docstring)\n        return rpm_set\n\n\nclass ValidateUninstallDebPackage(Operation):\n    \"\"\"\n     A replicate class of ValidateUninstallPackage that used for pre-uninstall deb package for Ubuntu.\n     We do not need to test dependencies in GPDB 6 as there is no GPDB's dependencies anymore.\n\n    \"\"\"\n\n    def __init__(self, gppkg):\n        self.gppkg = gppkg\n\n    def execute(self):\n        deb_list = [self.gppkg.main_rpm] + self.gppkg.dependencies\n\n        def strip_extension_and_arch(filename):\n            # expecting filename of form %{name}-%{version}-%{release}.%{arch}.rpm\n            rest = str.split(filename, '-')\n            return rest[-3]\n\n        # We check the installed list only\n        deb_set = set([strip_extension_and_arch(deb_package) for deb_package in deb_list])\n        archive_package_exists = CheckFile(os.path.join(GPPKG_ARCHIVE_PATH, self.gppkg.pkg)).run()\n        package_not_installed = (not deb_set) and (not archive_package_exists)\n        if package_not_installed:\n            raise NotInstalledError(self.gppkg.pkg)\n\n        return deb_set\n\n\nclass ValidateUninstallPackage(Operation):\n    \"\"\"\n    Ensure that the given rpms can be uninstalled safely. This is accomplished mainly\n    through use of rpm --test, which will have one of a few outcomes:\n    1) A return code of 0, indicating the uninstallation should proceed smoothly\n    2) A non-zero return code, and stderr indicating some of the rpms are already uninstalled.\n       We simply omit such rpms from the returned list of rpms, indicating to the caller\n       that to be successful, uninstallation should only be attempted on the filtered list of rpms.\n    3) A non-zero return code, and stderr indicating that dependencies remain.\n\n    Lastly, for an edge case, if we determine that none of the relevant rpms are currently installed\n    *and* the archive package does not exist, we declare the package is not installed.\n\n    TODO: Use regexes for more reliable string matching.\n    \"\"\"\n\n    def __init__(self, gppkg):\n        self.gppkg = gppkg\n\n    def execute(self):\n        rpm_list = [self.gppkg.main_rpm] + self.gppkg.dependencies\n\n        def strip_extension_and_arch(filename):\n            # expecting filename of form %{name}-%{version}-%{release}.%{arch}.rpm\n            rest, ext = os.path.splitext(filename)\n            rest, arch = os.path.splitext(rest)\n            return rest\n\n        rpm_set = set([strip_extension_and_arch(rpm) for rpm in rpm_list])\n        rpm_uninstall_string = ' '.join(rpm_set)\n        rpm_uninstall_command = 'rpm --test -e %s --dbpath %s' % (rpm_uninstall_string, RPM_DATABASE)\n        cmd = Command('Validating rpm uninstallation', rpm_uninstall_command)\n        logger.info(cmd)  # TODO: AK: This should be debug(), but RMI cannot propagate a log level.\n\n        try:\n            cmd.run(validateAfter=True)\n        except ExecutionError, e:\n            lines = e.cmd.get_results().stderr.splitlines()\n\n            # Forking between code paths 2 and 3 depends on some meaningful stderr\n            # Without such stderr, we must bubble up the ExecutionError.\n            if len(lines) == 0:\n                raise\n\n            if 'failed dependencies' in lines[0].lower():\n                # Code path 3 (see docstring)\n                # example stderr:\n                # error: Failed dependencies:\n                #    jre = 1.6.0_26 is needed by (installed) gphdfs-1.1-1.x86_64\n                self.resolve_shared_dependencies(rpm_set, lines[1:])\n            else:\n                # Code path 2, possibly (see docstring)\n                # example stderr:\n                #   error: package postgis-1.0-1.x86_64 is not installed\n                #   error: package proj-4.7.0-1.x86_64 is not installed\n                #   error: package geos-3.2.2-1.x86_64 is not installed\n                for line in lines:\n                    if 'not installed' in line.lower():\n                        package_name = line.split()[2]\n                        rpm_set.remove(package_name)\n                    else:\n                        # This is unexpected, so bubble up the ExecutionError.\n                        raise\n\n        # MPP-14359 - installation and uninstallation prechecks must also consider\n        # the archive. That is, if a partial uninstallation had removed all rpms\n        # but failed to remove the archive package, then for our purposes, we consider\n        # the package installed and still in need of UninstallPackageLocally.\n        archive_package_exists = CheckFile(os.path.join(GPPKG_ARCHIVE_PATH, self.gppkg.pkg)).run()\n        package_not_installed = (not rpm_set) and (not archive_package_exists)\n        if package_not_installed:\n            raise NotInstalledError(self.gppkg.pkg)\n\n        # Code path 1 (See docstring)\n        return rpm_set\n\n    def resolve_shared_dependencies(self, rpm_set, dependency_lines):\n        \"\"\"\n        This is a very naive resolution to shared dependencies. (See code path #3 in ValidateUninstallPackage.execute)\n\n        Among the rpms we attempt to remove from the system, a subset cannot be\n        removed during this particular gppkg uninstallation, because their removal would violate\n        the dependency constraints of other rpms that remain in the system; we simply leave these culprit rpm(s) behind.\n        More specifically, the preceding rpm --test -e command has given us the violated *capabilities*. For each *capability*,\n        we query the rpm database with --whatprovides to discern the culprit rpm(s).\n\n        In simpler terms, consider this example:\n            pljava depends on jre, which its gppkg contains\n            install the gppkgs for pljava\n            uninstall pljava gppkg\n            we determine that the jre rpm is responsible for *providing* \"jre = 1.6\"\n            so, we ultimately omit the jre rpm from our \"rpm -e\" and move on\n\n        TODO: AK: A more robust version of this function would ensure that the remaining\n        rpms are, in fact, bound by a remaining gppkg.  We defer this responsibility for now because gppkgs\n        should not have external dependencies. That is, no package should have requirements on rpms\n        not contained in its own gppkg distro. So, it's safe to assume that if foo is a culprit rpm, there exists\n        some gppkg bar that internally contains foo. (I realize that, with time, this will not be a scalable requirement\n        for gppkgs... hence the TODO.)\n\n        @type  rpm_set: set\n        @param rpm_set: rpms being uninstalled, among which there exists an rpm\n                        whose removal violates the dependencies of remaining rpms\n        @type  dependency_lines: list\n        @param dependency_lines: lines produced from the stderr in\n                                 code path #3 in ValidateUninstallPackage.execute\n                                 ex: [\"     jre >= 1.6.0_26 is needed by (installed) gphdfs-1.1-1.x86_64\"]\n        \"\"\"\n        for dependency_line in dependency_lines:\n            violated_capability = dependency_line.split()[0]  # e.g. \"jre\"\n            cmd = Command('Discerning culprit rpms for %s' % violated_capability,\n                          'rpm -q --whatprovides %s --dbpath %s' % (violated_capability, RPM_DATABASE))\n            cmd.run(validateAfter=True)\n            # remove the .x86_64 suffix for each rpm package to match the name in rpm_set\n            culprit_rpms = set(dep.replace('.x86_64', '') for dep in cmd.get_results().stdout.splitlines())\n            rpm_set -= culprit_rpms\n\n\nclass ExtractPackage(Operation):\n    \"\"\"\n    Extract the contents of the package into the temp folder\n\n    TODO: AK: Extraction should be implemented as a context manager.\n    \"\"\"\n\n    def __init__(self, gppkg):\n        self.gppkg = gppkg\n\n    def execute(self):\n        # clean up tmp extraction folder\n        if os.path.exists(TEMP_EXTRACTION_PATH) and not CleanupDir(TEMP_EXTRACTION_PATH).run():\n            logger.error('Could not clean temp folder')\n            raise IOError\n\n        # untar the package into tmp folder\n        with closing(tarfile.open(self.gppkg.abspath)) as tarinfo:\n            def is_within_directory(directory, target):\n\n                abs_directory = os.path.abspath(directory)\n                abs_target = os.path.abspath(target)\n\n                prefix = os.path.commonprefix([abs_directory, abs_target])\n\n                return prefix == abs_directory\n\n            def safe_extract(tar, path=\".\", members=None):\n\n                for member in tar.getmembers():\n                    member_path = os.path.join(path, member.name)\n                    if not is_within_directory(path, member_path):\n                        raise Exception(\"Attempted Path Traversal in Tar File\")\n\n                tar.extractall(path, members)\n\n            safe_extract(tarinfo, path=TEMP_EXTRACTION_PATH)\n\n        # move all the deps into same folder as the main rpm\n        path = os.path.join(TEMP_EXTRACTION_PATH, DEPS_DIR)\n        if os.path.exists(path):\n            for cur_file in os.listdir(path):\n                shutil.move(os.path.join(TEMP_EXTRACTION_PATH, DEPS_DIR, cur_file), TEMP_EXTRACTION_PATH)\n\n\nclass InstallDebPackageLocally(Operation):\n    \"\"\"\n    For deb package\n    \"\"\"\n    def __init__(self, package_path, is_update=False):\n        self.package_path = package_path\n        self.is_update = is_update\n\n    def execute(self):\n        current_package_location = self.package_path\n        package_name = os.path.basename(current_package_location)\n        logger.info('Installing %s locally' % package_name)\n        final_package_location = os.path.join(GPPKG_ARCHIVE_PATH, package_name)\n\n        gppkg = Gppkg.from_package_path(current_package_location)\n        ExtractPackage(gppkg).run()\n\n        # squash AlreadyInstalledError here: the caller doesn't ever need to\n        # know that we didn't have to do anything here\n        try:\n            deb_set = ValidateInstallDebPackage(gppkg, is_update=self.is_update).run()\n        except AlreadyInstalledError, e:\n            logger.info(e)\n            return\n\n        if deb_set:\n            deb_packages = \" \".join([os.path.join(TEMP_EXTRACTION_PATH, deb_package) for deb_package in deb_set])\n            if self.is_update:\n                deb_install_command = 'fakeroot dpkg --force-not-root --force-depends --log=/dev/null --admindir=%s ' \\\n                                      '--instdir=%s -i %s' % (DEB_DATABASE, GPHOME, deb_packages)\n            else:\n                deb_install_command = 'fakeroot dpkg --force-not-root --log=/dev/null --admindir=%s --instdir=%s -i %s' % \\\n                                      (DEB_DATABASE, GPHOME, deb_packages)\n\n            cmd = Command('Installing debs', deb_install_command)\n            logger.info(cmd)\n            cmd.run(validateAfter=True)\n\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n        shutil.copy(current_package_location, final_package_location)\n        logger.info(\"Completed local installation of %s.\" % package_name)\n\n\nclass InstallPackageLocally(Operation):\n    \"\"\"\n    Installs a package on the local host\n\n    This operation must take a slew of starting conditions and drive the state\n    of the local machine towards the ending state, in which the given package is successfully\n    installed, the rpm database is sane, and the package resides in the designated archive.\n    To that end, we indiscriminately squash AlreadyInstalledErrors arising from ValidateInstallPackage,\n    because in this context, it's not an exception, but rather an indication of our desired ending\n    conditions.\n\n    We must consider the following scenarios and more: package was deleted from archive,\n    the main comprising rpm was uninstalled, dependent rpms were removed, the rpm database was\n    corrupted, etc.\n\n    Again, much like ValidateInstallPackages, we make cheap reuse of this code for the purposes\n    of an --update as there is considerable commonality.\n    \"\"\"\n\n    def __init__(self, package_path, is_update=False):\n        self.package_path = package_path\n        self.is_update = is_update\n\n    def execute(self):\n        current_package_location = self.package_path\n        package_name = os.path.basename(current_package_location)\n        logger.info('Installing %s locally' % package_name)\n        final_package_location = os.path.join(GPPKG_ARCHIVE_PATH, package_name)\n\n        gppkg = Gppkg.from_package_path(current_package_location)\n        ExtractPackage(gppkg).run()\n\n        # squash AlreadyInstalledError here: the caller doesn't ever need to\n        # know that we didn't have to do anything here\n        try:\n            rpm_set = ValidateInstallPackage(gppkg, is_update=self.is_update).run()\n        except AlreadyInstalledError, e:\n            logger.info(e)\n            return\n\n        if rpm_set:\n            if self.is_update:\n                rpm_install_command = 'rpm -U --force %s --dbpath %s --prefix=%s'\n            else:\n                rpm_install_command = 'rpm -i --force %s --dbpath %s --prefix=%s'\n            rpm_install_command = rpm_install_command % \\\n                                  (\" \".join([os.path.join(TEMP_EXTRACTION_PATH, rpm) for rpm in rpm_set]),\n                                   RPM_DATABASE,\n                                   RPM_INSTALLATION_PATH)\n            cmd = Command('Installing rpms', rpm_install_command)\n            logger.info(cmd)\n            cmd.run(validateAfter=True)\n\n        # TODO: AK: MPP-15568\n        # TODO: AK: abstraction layer for archive interactions... to hide use of shutil.copy, RemoveFile, etc.\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n        shutil.copy(current_package_location, final_package_location)\n        logger.info(\"Completed local installation of %s.\" % package_name)\n\n\nclass UninstallDebPackageLocally(Operation):\n    \"\"\"\n     A replicate class of ValidateUninstallPackage that used for uninstall deb package for Ubuntu.\n     Same logic as UninstallPackageLocally\n\n    \"\"\"\n\n    def __init__(self, package_name):\n        self.package_name = package_name\n\n    def execute(self):\n        current_package_location = os.path.join(GPPKG_ARCHIVE_PATH, self.package_name)\n        gppkg = Gppkg.from_package_path(current_package_location)\n\n        # squash NotInstalledError here: the caller doesn't ever need to\n        # know that we didn't have to do anything here\n        try:\n            deb_set = ValidateUninstallDebPackage(gppkg).run()\n        except NotInstalledError, e:\n            logger.info(e)\n            return\n\n        if deb_set:\n            deb_uninstall_command = 'fakeroot dpkg --force-not-root --log=/dev/null --admindir=%s --instdir=%s -r %s' % (\n                DEB_DATABASE, GPHOME, \" \".join(deb_set))\n            cmd = Command('Uninstalling debs', deb_uninstall_command)\n            logger.info(cmd)\n            cmd.run(validateAfter=True)\n\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n        RemoveFile(current_package_location).run()\n        logger.info(\"Completed local uninstallation of %s.\" % self.package_name)\n\n\nclass UninstallPackageLocally(Operation):\n    \"\"\"\n    Uninstalls a package on the local host\n\n    This operation must take a slew of starting conditions and drive the state\n    of the local machine towards the ending state, in which the given package is successfully\n    uninstalled, the rpm database is sane, and the package is removed from the archive.\n    To that end, we indiscriminately squash NotInstalledErrors arising from ValidateUninstallPackage,\n    because in this context, it's not an exception, but rather an indication of our desired ending\n    conditions.\n\n    We must consider the following scenarios and more: package was deleted from archive,\n    the main comprising rpm was uninstalled, dependent rpms were removed, the rpm database was\n    corrupted, etc.\n    \"\"\"\n\n    def __init__(self, package_name):\n        self.package_name = package_name\n\n    def execute(self):\n        # TODO: AK: MPP-15737 - we're entirely dependent on the package residing in the archive\n        current_package_location = os.path.join(GPPKG_ARCHIVE_PATH, self.package_name)\n        gppkg = Gppkg.from_package_path(current_package_location)\n\n        # squash NotInstalledError here: the caller doesn't ever need to\n        # know that we didn't have to do anything here\n        try:\n            rpm_set = ValidateUninstallPackage(gppkg).run()\n        except NotInstalledError, e:\n            logger.info(e)\n            return\n\n        if rpm_set:\n            rpm_uninstall_command = 'rpm -e %s --dbpath %s' % (\" \".join(rpm_set), RPM_DATABASE)\n            cmd = Command('Uninstalling rpms', rpm_uninstall_command)\n            logger.info(cmd)\n            cmd.run(validateAfter=True)\n\n        # TODO: AK: abstraction layer for archive interactions... to hide use of shutil.copy, RemoveFile, etc.\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n        RemoveFile(current_package_location).run()\n        logger.info(\"Completed local uninstallation of %s.\" % self.package_name)\n\n\nclass SyncPackages(Operation):\n    \"\"\"\n    Synchronizes packages from master to a remote host\n\n    TODO: AK: MPP-15568\n    \"\"\"\n\n    def __init__(self, host):\n        self.host = host\n\n    def execute(self):\n        if not CheckDir(GPPKG_ARCHIVE_PATH).run():\n            MakeDir(GPPKG_ARCHIVE_PATH).run()\n        if not CheckRemoteDir(GPPKG_ARCHIVE_PATH, self.host).run():\n            MakeRemoteDir(GPPKG_ARCHIVE_PATH, self.host).run()\n\n        # set of packages on the master\n        master_package_set = set(ListFilesByPattern(GPPKG_ARCHIVE_PATH, '*' + GPPKG_EXTENSION).run())\n        # set of packages on the remote host\n        remote_package_set = set(ListRemoteFilesByPattern(GPPKG_ARCHIVE_PATH, '*' + GPPKG_EXTENSION, self.host).run())\n        # packages to be uninstalled on the remote host\n        uninstall_package_set = remote_package_set - master_package_set\n        # packages to be installed on the remote host\n        install_package_set = master_package_set - remote_package_set\n\n        if not install_package_set and not uninstall_package_set:\n            logger.info('The packages on %s are consistent.' % self.host)\n            return\n\n        if install_package_set:\n            logger.info(\n                'The following packages will be installed on %s: %s' % (self.host, ', '.join(install_package_set)))\n            for package in install_package_set:\n                logger.debug('copying %s to %s' % (package, self.host))\n                dstFile = os.path.join(GPHOME, package)\n                Scp(name='copying %s to %s' % (package, self.host),\n                    srcFile=os.path.join(GPPKG_ARCHIVE_PATH, package),\n                    dstFile=dstFile,\n                    dstHost=self.host).run(validateAfter=True)\n                if platform.linux_distribution()[0] == 'Ubuntu':\n                    RemoteOperation(InstallDebPackageLocally(dstFile), self.host).run()\n                else:\n                    RemoteOperation(InstallPackageLocally(dstFile), self.host).run()\n                RemoveRemoteFile(dstFile, self.host).run()\n\n        if uninstall_package_set:\n            logger.info(\n                'The following packages will be uninstalled on %s: %s' % (self.host, ', '.join(uninstall_package_set)))\n            for package in uninstall_package_set:\n                if platform.linux_distribution()[0] == 'Ubuntu':\n                    RemoteOperation(UninstallDebPackageLocally(package), self.host).run()\n                else:\n                    RemoteOperation(UninstallPackageLocally(package), self.host).run()\n\n\nclass InstallPackage(Operation):\n    def __init__(self, gppkg, master_host, standby_host, segment_host_list):\n        self.gppkg = gppkg\n        self.master_host = master_host\n        if master_host != standby_host:\n            self.standby_host = standby_host\n        else:\n            self.standby_host = None\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        logger.info('Installing package %s' % self.gppkg.pkg)\n\n        # TODO: AK: MPP-15736 - precheck package state on master\n        ExtractPackage(self.gppkg).run()\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            ValidateInstallDebPackage(self.gppkg).run()\n        else:\n            ValidateInstallPackage(self.gppkg).run()\n\n        # perform any pre-installation steps\n        PerformHooks(hooks=self.gppkg.preinstall,\n                     master_host=self.master_host,\n                     standby_host=self.standby_host,\n                     segment_host_list=self.segment_host_list).run()\n\n        # distribute package to segments\n        srcFile = self.gppkg.abspath\n        dstFile = os.path.join(GPHOME, self.gppkg.pkg)\n\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            # install package on segments\n            if self.segment_host_list:\n                GpScp(srcFile, dstFile, self.segment_host_list).run()\n                HostOperation(InstallDebPackageLocally(dstFile), self.segment_host_list).run()\n\n            # install package on standby\n            if self.standby_host:\n                Scp(name='copying %s to %s' % (srcFile, self.standby_host),\n                    srcFile=srcFile,\n                    dstFile=dstFile,\n                    dstHost=self.standby_host).run(validateAfter=True)\n                RemoteOperation(InstallDebPackageLocally(dstFile), self.standby_host).run()\n\n            # install package on master\n            InstallDebPackageLocally(srcFile).run()\n        else:\n            # install package on segments\n            if self.segment_host_list:\n                GpScp(srcFile, dstFile, self.segment_host_list).run()\n                HostOperation(InstallPackageLocally(dstFile), self.segment_host_list).run()\n\n            # install package on standby\n            if self.standby_host:\n                Scp(name='copying %s to %s' % (srcFile, self.standby_host),\n                    srcFile=srcFile,\n                    dstFile=dstFile,\n                    dstHost=self.standby_host).run(validateAfter=True)\n                RemoteOperation(InstallPackageLocally(dstFile), self.standby_host).run()\n\n            # install package on master\n            InstallPackageLocally(srcFile).run()\n\n        # perform any post-installation steps\n        PerformHooks(hooks=self.gppkg.postinstall,\n                     master_host=self.master_host,\n                     standby_host=self.standby_host,\n                     segment_host_list=self.segment_host_list).run()\n\n        logger.info('%s successfully installed.' % (self.gppkg.pkg))\n\n\nclass PerformHooks(Operation):\n    def __init__(self, hooks, master_host, standby_host, segment_host_list):\n        \"\"\"\n        Performs steps that have been specified in the yaml file for a particular\n        stage of gppkg execution\n\n        TODO: AK: A packager may have added commands to their hooks, with the\n        assumption that the current working directory would be that which contains\n        the spec file, rpms, and other artifacts (external scripts, perhaps.) To support\n        this, these commands should be prefixed with a \"cd\".\n\n        TODO: AK: I'm adding master_host for consistency.\n        But, why would we ever need master_host?  We're on the master host!\n        \"\"\"\n        self.hooks = hooks\n        self.master_host = master_host\n        if master_host != standby_host:\n            self.standby_host = standby_host\n        else:\n            self.standby_host = []\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        if self.hooks is None:\n            return\n        for hook in self.hooks:\n            key = hook.keys()\n            if key is None:\n                return\n            key_str = key[0]\n            if key_str.lower() == 'master':\n                if self.standby_host:\n                    RemoteCommand(hook[key_str], [self.standby_host]).run()\n                LocalCommand(hook[key_str], True).run()\n            elif key_str.lower() == 'segment':\n                RemoteCommand(hook[key_str], self.segment_host_list).run()\n            elif key_str.lower() == 'all':\n                if self.standby_host:\n                    RemoteCommand(hook[key_str], [self.standby_host]).run()\n                # Change on Master\n                LocalCommand(hook[key_str], True).run()\n                # Change on Segment hosts\n                RemoteCommand(hook[key_str], self.segment_host_list).run()\n\n\n\nclass UninstallPackage(Operation):\n    def __init__(self, gppkg, master_host, standby_host, segment_host_list):\n        self.gppkg = gppkg\n        self.master_host = master_host\n        if master_host != standby_host:\n            self.standby_host = standby_host\n        else:\n            self.standby_host = []\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        logger.info('Uninstalling package %s' % self.gppkg.pkg)\n\n        # TODO: AK: MPP-15736 - precheck package state on master\n        ExtractPackage(self.gppkg).run()\n\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            ValidateUninstallDebPackage(self.gppkg).run()\n        else:\n            ValidateUninstallPackage(self.gppkg).run()\n\n        # perform any pre-uninstallation steps\n        PerformHooks(hooks=self.gppkg.preuninstall,\n                     master_host=self.master_host,\n                     standby_host=self.standby_host,\n                     segment_host_list=self.segment_host_list).run()\n\n        # uninstall on segments\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            HostOperation(UninstallDebPackageLocally(self.gppkg.pkg), self.segment_host_list).run()\n\n            if self.standby_host:\n                RemoteOperation(UninstallDebPackageLocally(self.gppkg.pkg), self.standby_host).run()\n\n            UninstallDebPackageLocally(self.gppkg.pkg).run()\n\n        else:\n            HostOperation(UninstallPackageLocally(self.gppkg.pkg), self.segment_host_list).run()\n\n            if self.standby_host:\n                RemoteOperation(UninstallPackageLocally(self.gppkg.pkg), self.standby_host).run()\n\n            UninstallPackageLocally(self.gppkg.pkg).run()\n\n        # perform any post-installation steps\n        PerformHooks(hooks=self.gppkg.postuninstall,\n                     master_host=self.master_host,\n                     standby_host=self.standby_host,\n                     segment_host_list=self.segment_host_list).run()\n\n        logger.info('%s successfully uninstalled.' % self.gppkg.pkg)\n\n\nclass QueryPackage(Operation):\n    INFO, LIST, ALL = range(3)\n\n    def __init__(self, query_type, package_path):\n        self.query_type = query_type\n        self.package_path = package_path\n\n    def execute(self):\n        if self.query_type == QueryPackage.INFO:\n            def package_details(p):\n                yield 'Name', p.pkgname\n                yield 'Version', p.version\n                yield 'Architecture', p.architecture\n                yield 'OS', p.os\n                yield 'GPDBVersion', str(p.gpdbversion)\n                yield 'Description', p.description\n\n            def print_package_info(package):\n                tabLog = TableLogger()\n                for name, value in package_details(package):\n                    tabLog.info([name, value])\n                tabLog.outputTable()\n\n            package = Gppkg.from_package_path(self.package_path)\n            print_package_info(package)\n\n        elif self.query_type == QueryPackage.LIST:\n            package = Gppkg.from_package_path(self.package_path)\n            for file in package.file_list:\n                print file\n        elif self.query_type == QueryPackage.ALL:\n            package_name_list = ListPackages().run()\n            for package_name in package_name_list:\n                print package_name\n        else:\n            package = Gppkg.from_package_path(self.package_path)\n            try:\n                ExtractPackage(package).run()\n                ValidateInstallPackage(package).run()\n            except AlreadyInstalledError:\n                print '%s is installed.' % package.pkgname\n            else:\n                print '%s is not installed.' % package.pkgname\n\n\nclass BuildGppkg(Operation):\n    \"\"\"\n        Builds a gppkg given a directory containing\n        the spec file, rpms and any pre/post installation scripts\n    \"\"\"\n\n    def __init__(self, directory, filename):\n        self.directory = directory\n        self.filename = filename;\n\n    def execute(self):\n\n        directory = self.directory\n\n        logger.info('Building gppkg')\n\n        # Check if the directory is valid\n        if not os.path.exists(directory) or not os.path.isdir(directory):\n            logger.error('%s is an Invalid directory' % directory)\n            raise BuildPkgError\n\n        filelist = os.listdir(directory)\n\n        # Check for the spec file\n        specfile = directory + '/' + SPECFILE_NAME\n\n        if not os.path.exists(specfile):\n            logger.error(' Spec file does not exist')\n            raise BuildPkgError\n\n        # parse the spec file and get the name, version and arch\n        # this is used to name the gppkg\n        pkg_path_details = self._get_package_name_details(specfile)\n\n        if pkg_path_details is None:\n            raise BuildPkgError\n\n        # The file already exists. Rewrite the original with the new one\n        if self.filename is None:\n            pkg = pkg_path_details['pkgname'] + '-' + str(pkg_path_details['version']) + '-' + pkg_path_details[\n                'os'] + '-' + pkg_path_details['architecture'] + GPPKG_EXTENSION\n        else:\n            pkg = self.filename\n        if os.path.exists(pkg):\n            os.remove(pkg)\n\n        # Verify the spec file\n        if not self._verify_specfile(specfile, directory):\n            raise BuildPkgError\n\n        # tar and gzip the directory\n        # rename the file with .gppkg extension\n        with closing(tarfile.open(pkg, 'w:gz')) as tarinfo:\n            for cur_file in filelist:\n                tarinfo.add(name=os.path.join(directory, cur_file),\n                            arcname=cur_file)\n\n        logger.info('Completed building gppkg')\n\n    def _get_package_name_details(self, specfile):\n        \"\"\"\n            Get details about the name, version, operating system, architecture\n            of the package. The final gppkg which will be created\n            will be named as <name>-<version>-<os>-<arch>.gppkg\n        \"\"\"\n\n        logger.debug('_get_package_name_details')\n        cur_file = None\n\n        with open(specfile) as cur_file:\n            yamlfile = yaml.safe_load(cur_file)\n\n            tags = yamlfile.keys()\n\n            pkg_path_details = {}\n\n            # return all the required tags as a dict\n            for tag in tags:\n                if tag.lower() in SPECFILE_REQUIRED_TAGS:\n                    pkg_path_details[tag.lower()] = yamlfile[tag]\n\n            return pkg_path_details\n\n    def _verify_specfile(self, specfile, directory):\n        '''\n            Reads the spec file and makes sure that the tags are correct.\n        '''\n\n        logger.debug('_verify_specfile')\n        cur_file = None\n\n        try:\n            with open(specfile) as cur_file:\n                yamlfile = yaml.safe_load(cur_file)\n\n                if not self._verify_tags(yamlfile):\n                    return False\n\n                return True\n        except ScannerError, ex:\n            return False\n\n    def _verify_tags(self, yamlfile):\n        \"\"\"\n            Verify that the tags are valid.\n            Returns true if all tags are valid\n            False otherwise\n        \"\"\"\n\n        logger.debug('_verify_tags')\n        tags = yamlfile.keys()\n\n        tags = [tag.lower() for tag in tags]\n\n        # check required tags\n        for required_tag in SPECFILE_REQUIRED_TAGS:\n            if required_tag not in tags:\n                logger.error(' Required tag %s missing in Spec file' % required_tag)\n                return False\n\n        # check for invalid tags\n        for tag in tags:\n            if tag not in SPECFILE_OPTIONAL_TAGS and tag not in SPECFILE_REQUIRED_TAGS:\n                logger.error(' Invalid tag %s in Spec file' % tag)\n                return False\n\n        return True\n\n\nclass UpdatePackage(Operation):\n    \"\"\" TODO: AK: Enforce gppkg version is higher than currently installed version \"\"\"\n\n    def __init__(self, gppkg, master_host, standby_host, segment_host_list):\n        self.gppkg = gppkg\n        self.master_host = master_host\n        if master_host != standby_host:\n            self.standby_host = standby_host\n        else:\n            self.standby_host = []\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        logger.info('Updating package %s' % self.gppkg.pkg)\n\n        ExtractPackage(self.gppkg).run()\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            ValidateInstallDebPackage(self.gppkg, is_update=True).run()\n        else:\n            ValidateInstallPackage(self.gppkg, is_update=True).run()\n\n        # distribute package to segments\n        srcFile = self.gppkg.abspath\n        dstFile = os.path.join(GPHOME, self.gppkg.pkg)\n        GpScp(srcFile, dstFile, self.segment_host_list).run()\n\n        # update package on segments\n        HostOperation(UpdatePackageLocally(dstFile), self.segment_host_list).run()\n\n        # update package on standby\n        if self.standby_host:\n            Scp(name='copying %s to %s' % (srcFile, self.standby_host),\n                srcFile=srcFile,\n                dstFile=dstFile,\n                dstHost=self.standby_host).run(validateAfter=True)\n            RemoteOperation(UpdatePackageLocally(dstFile), self.standby_host).run()\n\n        # update package on master\n        UpdatePackageLocally(srcFile).run()\n\n        # perform any post-update steps\n        PerformHooks(hooks=self.gppkg.postupdate,\n                     master_host=self.master_host,\n                     standby_host=self.standby_host,\n                     segment_host_list=self.segment_host_list).run()\n\n        logger.info('%s successfully updated.' % (self.gppkg.pkg))\n\n\nclass UpdatePackageLocally(Operation):\n    \"\"\"\n    Updates a package on the local host\n\n    We make cheap reuse of InstallPackageLocally with the propagation of is_update = True, which\n    effectively changes the rpm --test command to use -U instead of -i. Beyond the invocation of\n    InstallPackageLocally, here, we also clean up the archive directory to remove other (ideally, older)\n    versions of the updated package.\n    \"\"\"\n\n    def __init__(self, package_path):\n        self.package_path = package_path\n\n    def execute(self):\n        if platform.linux_distribution()[0] == 'Ubuntu':\n            InstallDebPackageLocally(self.package_path, is_update=True).run()\n        else:\n            InstallPackageLocally(self.package_path, is_update=True).run()\n\n        # Remove other versions of the package from archive.\n        # Note: Do not rely on filename format to discern such packages.\n        # Rather, interrogate a package only through the Gppkg class interface.\n        current_package = Gppkg.from_package_path(self.package_path)\n        MakeDir(GPPKG_ARCHIVE_PATH).run()\n        archived_package_paths = ListFiles(GPPKG_ARCHIVE_PATH).run()\n        for archived_package_path in archived_package_paths:\n            temp_package = Gppkg.from_package_path(os.path.join(GPPKG_ARCHIVE_PATH, archived_package_path))\n            if temp_package.pkgname == current_package.pkgname and temp_package.version != current_package.version:\n                RemoveFile(os.path.join(GPPKG_ARCHIVE_PATH, archived_package_path)).run()\n\n\nclass CleanGppkg(Operation):\n    \"\"\"\n        Cleans up the Gppkg from the cluster in case of partial\n        installation or removal. This might not be required if\n        we can make the install and uninstall options idempotent.\n        This operation is exactly the same as remove but we dont\n        check on each host to see if the rpm is installed or not.\n    \"\"\"\n\n    def __init__(self, standby_host, segment_host_list):\n        self.standby_host = standby_host\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        operations = [SyncPackages(host) for host in self.segment_host_list]\n\n        if self.standby_host:\n            operations.append(SyncPackages(self.standby_host))\n\n        ParallelOperation(operations).run()\n\n        err_msgs = 'SyncPackages failed:'\n        exceptions = \"\"\n        for operation in operations:\n            try:\n                operation.get_ret()\n            except Exception, e:\n                exceptions += '\\n'+str(e)\n\n        if exceptions:\n            raise ExceptionNoStackTraceNeeded(\"%s%s\" % (err_msgs, exceptions))\n\n        logger.info('Successfully cleaned the cluster')\n\n\nclass MigratePackages(Operation):\n    \"\"\"\n    Migrates packages from another $GPHOME to this one\n\n    This functionality is meant to facilitate minor version upgrade, whereby old packages\n    need to be brought over from the older $GPHOME to the newer $GPHOME.\n\n    Presumably, this could also be used to migrate packages across arbitrary choices\n    of $GPHOMEs. However, the migration will only succeed if the packages being migrated\n    are actually compatible with the target GPDB.\n    \"\"\"\n\n    def __init__(self, from_gphome, to_gphome, standby_host, segment_host_list):\n        self.from_gphome = from_gphome\n        self.to_gphome = to_gphome\n        self.standby_host = standby_host\n        self.segment_host_list = segment_host_list\n\n    def execute(self):\n        if not os.path.samefile(self.to_gphome, GPHOME):\n            raise ExceptionNoStackTraceNeeded(\n                'The target GPHOME, %s, must match the current $GPHOME used to launch gppkg.' % self.to_gphome)\n        if os.path.samefile(self.to_gphome, self.from_gphome):\n            raise ExceptionNoStackTraceNeeded(\n                'The source and target GPHOMEs, %s => %s, must differ for packages to be migrated.' % (\n                self.from_gphome, self.to_gphome))\n\n        # TODO: AK: Given an invalid from_gphome, we'll end up creating a 'share/packages' subdirectory within it.\n        old_archive_path = os.path.join(self.from_gphome, ARCHIVE_PATH)\n        MakeDir(old_archive_path).run()\n        packages = ListFilesByPattern(old_archive_path, '*' + GPPKG_EXTENSION).run()\n        if not packages:\n            logger.info('There are no packages to migrate from %s.' % self.from_gphome)\n            return\n\n        logger.info('The following packages will be migrated: %s' % ', '.join(packages))\n        for package in packages:\n            package_path = os.path.join(old_archive_path, package)\n            try:\n                if platform.linux_distribution()[0] == 'Ubuntu':\n                    InstallDebPackageLocally(package_path).run()\n                else:\n                    InstallPackageLocally(package_path).run()\n            except AlreadyInstalledError:\n                logger.info(\"%s is already installed.\" % package)\n            except Exception:\n                logger.exception(\"Failed to migrate %s from %s\" % (old_archive_path, package))\n\n        CleanGppkg(self.standby_host, self.segment_host_list).run()\n\n        logger.info('The package migration has completed.')\n\n\nclass GpScp(Operation):\n    \"\"\"\n    TODO: AK: This obviously does not belong here. My preference would be that it remain here until\n    the following problem is solved.\n\n    MPP-15270 - Improve performance of file transfer across large clusters\n\n    I suggest:\n\n        We consume an extra parameter 'fanout'. We partition the host_list into a number of buckets\n        given by 'fanout'. For each bucket, we scp the artifact to the first host in the bucket, and then\n        we recursively invoke GpScp on that machine for the remaining hosts in its bucket.\n\n        GpScp := ParallelOperation([ A(i) for i in range(0, n) ])\n        A := SerialOperation(B, C)\n        B := scp source_path target_path @ host_i\n            where host_i := the first host in the ith bucket\n        C := RemoteOperation(GpScp(target_path, target_path, host_list_i))\n            where host_list_i := the remaining hosts in the ith bucket\n    \"\"\"\n\n    def __init__(self, source_path, target_path, host_list):\n        self.source_path = source_path\n        self.target_path = target_path\n        self.host_list = host_list\n        self.pool = None\n\n    def execute(self):\n        self.pool = WorkerPool()\n        for host in self.host_list:\n            self.pool.addCommand(Scp(name='copying %s to %s' % (self.source_path, host),\n                                     srcFile=self.source_path,\n                                     dstFile=self.target_path,\n                                     dstHost=host))\n        self.pool.join()\n\n\nclass HostOperation(Operation):\n    \"\"\"\n    TODO: AK: This obviously does not belong here. My preference would be to move it to gppylib.operations.utils\n    when another consumer becomes clear.\n\n    TODO: AK: For generality, the underlying operation should inherit/implement NestedHostOperation so that\n    it may be initialized with information about the host to which it's been bound. This is fortunately not necessary\n    for our purposes here, so it's deferrable.\n\n    TODO: AK: Build a SegHostOperation that wraps this and is driven by GpArray content.\n\n    TODO: AK: Implement something similar for a SegmentOperation + NestedSegmentOperation.\n\n    TODO: AK: This (as well as ParallelOperation) would benefit from an appropriate choice of return value. The likely\n    choice would be: [op.get_ret() for op in self.operations]\n    \"\"\"\n\n    def __init__(self, operation, host_list):\n        self.operation = operation\n        self.host_list = host_list\n\n    def execute(self):\n        operations = []\n        for host in self.host_list:\n            operations.append(RemoteOperation(self.operation, host))\n        ParallelOperation(operations).run()\n        for operation in operations:\n            operation.get_ret()\n"], "filenames": ["gpMgmt/bin/gppylib/operations/package.py"], "buggy_code_start_loc": [817], "buggy_code_end_loc": [818], "fixing_code_start_loc": [817], "fixing_code_end_loc": [836], "type": "CWE-22", "message": "Greenplum Database (GPDB) is an open source data warehouse based on PostgreSQL. In versions prior to 6.22.3 Greenplum Database used an unsafe methods to extract tar files within GPPKGs. greenplum-db is vulnerable to path traversal leading to arbitrary file writes. An attacker can use this vulnerability to overwrite data or system files potentially leading to crash or malfunction of the system. Any files which are accessible to the running process are at risk. All users are requested to upgrade to Greenplum Database version 6.23.2 or higher. There are no known workarounds for this vulnerability.", "other": {"cve": {"id": "CVE-2023-31131", "sourceIdentifier": "security-advisories@github.com", "published": "2023-05-15T22:15:12.273", "lastModified": "2023-05-25T17:06:11.477", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Greenplum Database (GPDB) is an open source data warehouse based on PostgreSQL. In versions prior to 6.22.3 Greenplum Database used an unsafe methods to extract tar files within GPPKGs. greenplum-db is vulnerable to path traversal leading to arbitrary file writes. An attacker can use this vulnerability to overwrite data or system files potentially leading to crash or malfunction of the system. Any files which are accessible to the running process are at risk. All users are requested to upgrade to Greenplum Database version 6.23.2 or higher. There are no known workarounds for this vulnerability."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.1, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.2}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.4, "baseSeverity": "HIGH"}, "exploitabilityScore": 2.2, "impactScore": 5.2}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-22"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:vmware:greenplum_database:*:*:*:*:*:*:*:*", "versionEndExcluding": "6.22.3", "matchCriteriaId": "19687964-5180-47DC-9C0C-5482B2B44A3C"}]}]}], "references": [{"url": "https://github.com/greenplum-db/gpdb/commit/1ec4affbba7c9745f64edbd80a6680ad29b09471", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/greenplum-db/gpdb/security/advisories/GHSA-hgm9-2q42-c7f3", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/greenplum-db/gpdb/commit/1ec4affbba7c9745f64edbd80a6680ad29b09471"}}