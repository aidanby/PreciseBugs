{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/sparse_tensor_dense_add_op.h\"\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_util.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n// NOTE: does not support GPU yet.\n\nnamespace {\n\ntemplate <typename Index>\nStatus ValidateInputs(const Tensor *a_indices, const Tensor *a_values,\n                      const Tensor *a_shape, const Tensor *b) {\n  if (!TensorShapeUtils::IsMatrix(a_indices->shape())) {\n    return errors::InvalidArgument(\n        \"Input a_indices should be a matrix but received shape: \",\n        a_indices->shape().DebugString());\n  }\n  if (!TensorShapeUtils::IsVector(a_values->shape()) ||\n      !TensorShapeUtils::IsVector(a_shape->shape())) {\n    return errors::InvalidArgument(\n        \"Inputs a_values and a_shape should be vectors \"\n        \"but received shapes: \",\n        a_values->shape().DebugString(), \" and \",\n        a_shape->shape().DebugString());\n  }\n  if (a_shape->NumElements() != b->dims()) {\n    return errors::InvalidArgument(\n        \"Two operands have different ranks; received: \", a_shape->NumElements(),\n        \" and \", b->dims());\n  }\n  const auto a_shape_flat = a_shape->flat<Index>();\n  for (int i = 0; i < b->dims(); ++i) {\n    if (a_shape_flat(i) != b->dim_size(i)) {\n      return errors::InvalidArgument(\n          \"Dimension \", i,\n          \" does not equal (no broadcasting is supported): sparse side \",\n          a_shape_flat(i), \" vs dense side \", b->dim_size(i));\n    }\n  }\n  return Status::OK();\n}\n\n}  // namespace\n\ntemplate <typename Device, typename T, typename Index>\nclass SparseTensorDenseAddOp : public OpKernel {\n public:\n  explicit SparseTensorDenseAddOp(OpKernelConstruction *ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext *ctx) override {\n    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b;\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_indices\", &a_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_shape\", &a_shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b));\n    OP_REQUIRES_OK(\n        ctx, ValidateInputs<Index>(a_indices_t, a_values_t, a_shape_t, b));\n\n    Tensor *out_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, b->shape(), &out_t));\n\n    const int ndims = static_cast<int>(a_indices_t->dim_size(1));\n    const auto a_indices_mat = a_indices_t->flat_inner_dims<Index>();\n    const auto a_values_flat = a_values_t->flat<T>();\n\n    switch (ndims) {\n#define NDIMS_CASE(N)                                                     \\\n  case N: {                                                               \\\n    auto out_tensor = out_t->tensor<T, N>();                              \\\n    out_tensor.device(ctx->eigen_device<Device>()) = b->tensor<T, N>();   \\\n    const Index result =                                                  \\\n        functor::ScatterNdFunctor<Device, T, Index, N,                    \\\n                                  scatter_op::UpdateOp::ADD>()(           \\\n            ctx->eigen_device<Device>(), a_indices_mat, a_values_flat,    \\\n            out_tensor);                                                  \\\n    OP_REQUIRES(                                                          \\\n        ctx, result == -1,                                                \\\n        errors::InvalidArgument(                                          \\\n            \"Sparse tensor has some invalid index on dimension \", result, \\\n            \"; dense tensor shape: \", b->shape().DebugString()));         \\\n  } break;\n\n      NDIMS_CASE(1);\n      NDIMS_CASE(2);\n      NDIMS_CASE(3);\n      NDIMS_CASE(4);\n      NDIMS_CASE(5);\n      default:\n        OP_REQUIRES(\n            ctx, false,\n            errors::InvalidArgument(\"Only tensors with ranks between 1 and 5 \"\n                                    \"are currently supported.  Tensor rank: \",\n                                    ndims));\n#undef NDIMS_CASE\n    }\n  }\n};\n\nnamespace functor {\ntemplate <typename T, typename Index, int NDIMS>\nstruct ScatterNdFunctor<CPUDevice, T, Index, NDIMS, scatter_op::UpdateOp::ADD> {\n  Index operator()(const CPUDevice &d,\n                   typename TTypes<Index>::ConstMatrix indices,\n                   typename TTypes<T>::ConstFlat updates,\n                   typename TTypes<T, NDIMS>::Tensor out) {\n    Eigen::array<Eigen::DenseIndex, NDIMS> idx;\n    const int num_nnz = static_cast<int>(indices.dimension(0));\n    for (int i = 0; i < num_nnz; ++i) {\n      for (int d = 0; d < NDIMS; ++d) {\n        idx[d] = internal::SubtleMustCopy(indices(i, d));\n        if (!FastBoundsCheck(idx[d], out.dimension(d))) {\n          return d;  // on failure: d nonnegative\n        }\n      }\n      out(idx) += updates(i);\n    }\n    return -1;  // on success\n  }\n};\n}  // namespace functor\n\n#define REGISTER_KERNELS_CPU(TypeT, TypeIndex)                        \\\n  REGISTER_KERNEL_BUILDER(Name(\"SparseTensorDenseAdd\")                \\\n                              .Device(DEVICE_CPU)                     \\\n                              .TypeConstraint<TypeT>(\"T\")             \\\n                              .TypeConstraint<TypeIndex>(\"Tindices\"), \\\n                          SparseTensorDenseAddOp<CPUDevice, TypeT, TypeIndex>)\n\n#define REGISTER_KERNELS(T)         \\\n  REGISTER_KERNELS_CPU(T, int64_t); \\\n  REGISTER_KERNELS_CPU(T, int32)\n\nTF_CALL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n#undef REGISTER_KERNELS_CPU\n}  // namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for SparseAdd.\"\"\"\n\nimport timeit\n\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import sparse_ops\nimport tensorflow.python.ops.sparse_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\ndef _sparsify(x, thresh=0.5, index_dtype=np.int64):\n  x[x < thresh] = 0\n\n  non_zero = np.where(x)\n  x_indices = np.vstack(non_zero).astype(index_dtype).T\n  x_values = x[non_zero]\n  x_shape = x.shape\n\n  return sparse_tensor.SparseTensor(\n      indices=x_indices, values=x_values, dense_shape=x_shape), len(x_values)\n\n\nclass SparseAddTest(test.TestCase):\n\n  def _randomTensor(self, size, np_dtype, sparse=True):\n    n, m = size\n    x = np.random.randn(n, m).astype(np_dtype)\n    return _sparsify(x) if sparse else x\n\n  def _SparseTensorValue_3x3(self, negate=False):\n    # [    1]\n    # [2    ]\n    # [3   4]\n    # ...or its cwise negation, if `negate`\n    ind = np.array([[0, 1], [1, 0], [2, 0], [2, 1]])\n    val = np.array([1, 2, 3, 4])\n    if negate:\n      val = -np.array([1, 2, 3, 4])\n    shape = np.array([3, 3])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(val, np.float32), np.array(shape, np.int64))\n\n  def _SparseTensor_3x3(self, negate=False):\n    return sparse_tensor.SparseTensor.from_value(\n        self._SparseTensorValue_3x3(negate))\n\n  def _SparseTensor_3x3_v2(self):\n    # [           1]\n    # [-1.9        ]\n    # [   3    -4.2]\n    ind = np.array([[0, 1], [1, 0], [2, 0], [2, 1]])\n    val = np.array([1, -1.9, 3, -4.2])\n    shape = np.array([3, 3])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.float32),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testAddSelf(self):\n    with test_util.force_cpu():\n      for sp_a in (self._SparseTensorValue_3x3(), self._SparseTensor_3x3()):\n        for sp_b in (self._SparseTensorValue_3x3(), self._SparseTensor_3x3()):\n          sp_sum = sparse_ops.sparse_add(sp_a, sp_b)\n          self.assertAllEqual((3, 3), sp_sum.get_shape())\n\n          sum_out = self.evaluate(sp_sum)\n\n          self.assertEqual(sp_sum.dense_shape.get_shape(), [2])\n          self.assertAllEqual(sum_out.indices, [[0, 1], [1, 0], [2, 0], [2, 1]])\n          self.assertAllEqual(sum_out.values, [2, 4, 6, 8])\n          self.assertAllEqual(sum_out.dense_shape, [3, 3])\n\n  def testAddSelfAndNegation(self):\n    with test_util.force_cpu():\n      sp_a = self._SparseTensor_3x3()\n      sp_b = self._SparseTensor_3x3(negate=True)\n\n      sp_sum = sparse_ops.sparse_add(sp_a, sp_b, 0.1)\n      sum_out = self.evaluate(sp_sum)\n\n      self.assertEqual(sp_sum.dense_shape.get_shape(), [2])\n      self.assertAllEqual(sum_out.indices, np.empty([0, 2]))\n      self.assertAllEqual(sum_out.values, [])\n      self.assertAllEqual(sum_out.dense_shape, [3, 3])\n\n  def testSmallValuesShouldVanish(self):\n    with test_util.force_cpu():\n      sp_a = self._SparseTensor_3x3()\n      sp_b = self._SparseTensor_3x3_v2()\n\n      # sum:\n      # [       2]\n      # [.1      ]\n      # [ 6   -.2]\n\n      # two values should vanish: |.1| < .21, and |-.2| < .21\n      sp_sum = sparse_ops.sparse_add(sp_a, sp_b, thresh=0.21)\n      sum_out = self.evaluate(sp_sum)\n\n      self.assertEqual(sp_sum.dense_shape.get_shape(), [2])\n      self.assertAllEqual(sum_out.indices, [[0, 1], [2, 0]])\n      self.assertAllEqual(sum_out.values, [2, 6])\n      self.assertAllEqual(sum_out.dense_shape, [3, 3])\n\n      # only .1 vanishes\n      sp_sum = sparse_ops.sparse_add(sp_a, sp_b, thresh=0.11)\n      sum_out = self.evaluate(sp_sum)\n\n      self.assertEqual(sp_sum.dense_shape.get_shape(), [2])\n      self.assertAllEqual(sum_out.indices, [[0, 1], [2, 0], [2, 1]])\n      self.assertAllClose(sum_out.values, [2, 6, -.2])\n      self.assertAllEqual(sum_out.dense_shape, [3, 3])\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    np.random.seed(1618)  # Make it reproducible.\n    with self.session(use_gpu=False):\n      for n in [10, 31]:\n        for m in [4, 17]:\n          sp_a, nnz_a = self._randomTensor([n, m], np.float32)\n          sp_b, nnz_b = self._randomTensor([n, m], np.float32)\n          sp_sum = sparse_ops.sparse_add(sp_a, sp_b)\n          nnz_sum = len(self.evaluate(sp_sum.values))\n\n          err = gradient_checker.compute_gradient_error(\n              [sp_a.values, sp_b.values], [(nnz_a,), (nnz_b,)], sp_sum.values,\n              (nnz_sum,))\n          self.assertLess(err, 1e-3)\n\n  def testAddSparseDense(self):\n    np.random.seed(1618)  # Make it reproducible.\n    n, m = np.random.randint(30, size=2)\n    for dtype in [np.float32, np.float64, np.int64, np.complex64]:\n      for index_dtype in [np.int32, np.int64]:\n        rand_vals_np = np.random.randn(n, m).astype(dtype)\n        dense_np = np.random.randn(n, m).astype(dtype)\n\n        with test_util.force_cpu():\n          sparse, unused_nnz = _sparsify(rand_vals_np, index_dtype=index_dtype)\n          s = self.evaluate(\n              sparse_ops.sparse_add(sparse, constant_op.constant(dense_np)))\n          self.assertAllEqual(dense_np + rand_vals_np, s)\n          self.assertTrue(s.dtype == dtype)\n\n          # check commutativity\n          s = self.evaluate(\n              sparse_ops.sparse_add(constant_op.constant(dense_np), sparse))\n          self.assertAllEqual(dense_np + rand_vals_np, s)\n          self.assertTrue(s.dtype == dtype)\n\n  @test_util.run_deprecated_v1\n  def testSparseTensorDenseAddGradients(self):\n    np.random.seed(1618)  # Make it reproducible.\n    n, m = np.random.randint(30, size=2)\n    rand_vals_np = np.random.randn(n, m).astype(np.float32)\n    dense_np = np.random.randn(n, m).astype(np.float32)\n\n    with self.session(use_gpu=False):\n      sparse, nnz = _sparsify(rand_vals_np)\n      dense = constant_op.constant(dense_np, dtype=dtypes.float32)\n      s = sparse_ops.sparse_add(sparse, dense)\n\n      err = gradient_checker.compute_gradient_error([sparse.values, dense],\n                                                    [(nnz,), (n, m)], s, (n, m))\n      self.assertLess(err, 1e-3)\n\n  @test_util.run_deprecated_v1\n  def testInvalidSparseTensor(self):\n    with test_util.force_cpu():\n      shape = [2, 2]\n      val = [0]\n      dense = constant_op.constant(np.zeros(shape, dtype=np.int32))\n\n      for bad_idx in [\n          [[-1, 0]],  # -1 is invalid.\n          [[1, 3]],  # ...so is 3.\n      ]:\n        sparse = sparse_tensor.SparseTensorValue(bad_idx, val, shape)\n        s = sparse_ops.sparse_add(sparse, dense)\n\n        with self.assertRaisesRegex(errors_impl.InvalidArgumentError,\n                                    \"invalid index\"):\n          self.evaluate(s)\n\n######################## Benchmarking code\n\n\ndef _s2d_add_vs_sparse_add(sparsity, n, m, num_iters=50):\n  np.random.seed(1618)\n\n  with session.Session(graph=ops.Graph()) as sess:\n    sp_vals = np.random.rand(n, m).astype(np.float32)\n    sp_t, unused_nnz = _sparsify(sp_vals, thresh=sparsity, index_dtype=np.int32)\n    vals = np.random.rand(n, m).astype(np.float32)\n\n    s2d = math_ops.add(\n        sparse_ops.sparse_tensor_to_dense(sp_t), constant_op.constant(vals))\n    sa = sparse_ops.sparse_add(sp_t, constant_op.constant(vals))\n\n    timeit.timeit(lambda: sess.run(s2d), number=3)\n    timeit.timeit(lambda: sess.run(sa), number=3)\n\n    s2d_total = timeit.timeit(lambda: sess.run(s2d), number=num_iters)\n    sa_total = timeit.timeit(lambda: sess.run(sa), number=num_iters)\n\n  # per-iter latency; secs to millis\n  return s2d_total * 1e3 / num_iters, sa_total * 1e3 / num_iters\n\n\nclass SparseAddBenchmark(test.Benchmark):\n\n  def benchmarkSparseAddDense(self):\n\n    print(\"SparseAddDense: add with sparse_to_dense vs. sparse_add\")\n    print(\"%nnz \\t n \\t m \\t millis(s2d) \\t millis(sparse_add) \\t speedup\")\n\n    for sparsity in [0.99, 0.5, 0.01]:\n      for n in [1, 256, 50000]:\n        for m in [100, 1000]:\n          s2d_dt, sa_dt = _s2d_add_vs_sparse_add(sparsity, n, m)\n          print(\"%.2f \\t %d \\t %d \\t %.4f \\t %.4f \\t %.2f\" % (sparsity, n, m,\n                                                              s2d_dt, sa_dt,\n                                                              s2d_dt / sa_dt))\n\n\nif __name__ == \"__main__\":\n  test.main()\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Python ops defined in sparse_ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.sparse_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.platform import test\n\n\n# TODO(zongheng): it'd be great to factor out this function and various random\n# SparseTensor gen funcs.\ndef _sparsify(x, thresh=0.5, index_dtype=np.int64):\n  x[x < thresh] = 0\n\n  non_zero = np.where(x)\n  x_indices = np.vstack(non_zero).astype(index_dtype).T\n  x_values = x[non_zero]\n  x_shape = x.shape\n\n  return sparse_tensor.SparseTensor(\n      indices=x_indices, values=x_values, dense_shape=x_shape), len(x_values)\n\n\nclass SparseToIndicatorTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_5x6(self, dtype):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x3x4(self, dtype):\n    # Includes two entries with the form [1, 1, x] : 150.\n    ind = np.array([[0, 0, 1], [0, 1, 0], [0, 1, 2], [1, 0, 3], [1, 1, 0],\n                    [1, 1, 1], [1, 1, 2], [1, 2, 2]])\n    val = np.array([1, 10, 12, 103, 150, 149, 150, 122])\n    shape = np.array([2, 3, 4])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testInt32(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int32)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = ((0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33))\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testInt64(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = [(0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testHigherRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x3x4(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 200)\n\n      expected_output = np.zeros((2, 3, 200), dtype=np.bool_)\n      expected_trues = [(0, 0, 1), (0, 1, 10), (0, 1, 12), (1, 0, 103),\n                        (1, 1, 149), (1, 1, 150), (1, 2, 122)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n\nclass SparseMergeTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices = np.array([0, 13, 10, 33, 32, 14])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return indices, values\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    indices, values = self._SparseTensorValue_3x50(indices_dtype, values_dtype)\n    return (sparse_tensor.SparseTensor.from_value(indices),\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 10], [1, 13], [1, 14], [2, 32], [2, 33]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def _AssertResultsNotSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 13], [1, 10], [2, 33], [2, 32], [1, 14]])\n    self.assertAllEqual(output.values, [-3, 4, 1, 9, 5, 1])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def testInt32AndFloat32(self):\n    vocab_size = 50\n    indices_v, values_v = self._SparseTensorValue_3x50(np.int32, np.float32)\n    with test_util.force_cpu():\n      for indices in (indices_v,\n                      sparse_tensor.SparseTensor.from_value(indices_v)):\n        for values in (values_v,\n                       sparse_tensor.SparseTensor.from_value(values_v)):\n          sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n          output = self.evaluate(sp_output)\n          self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt32AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int32, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat64NonCanonicalOrder(self):\n    vocab_size = 50\n    vocab_size_tensor = constant_op.constant(vocab_size, dtypes.int64)\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size_tensor, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testShouldSetLastDimensionInDynamicShape(self):\n    with ops.Graph().as_default():\n      shape = constant_op.constant([2, 2], dtype=dtypes.int64)\n      dynamic_shape = array_ops.placeholder_with_default(shape, shape=[2])\n      ids = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[1, 3],\n          dense_shape=dynamic_shape)\n      values = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[0.4, 0.7],\n          dense_shape=dynamic_shape)\n      merged = sparse_ops.sparse_merge(\n          sp_ids=ids, sp_values=values, vocab_size=5)\n      self.assertEqual(5, merged.get_shape()[1])\n\n\nclass SparseMergeHighDimTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices0 = np.array([0, 13, 10, 33, 32, 14])\n    indices1 = np.array([12, 4, 0, 0, 1, 30])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices0 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices0, indices_dtype), np.array(shape, np.int64))\n    indices1 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices1, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return ([sparse_tensor.SparseTensor.from_value(indices0),\n             sparse_tensor.SparseTensor.from_value(indices1)],\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(\n        output.indices,\n        [[0, 0, 12], [1, 10, 0], [1, 13, 4], [1, 14, 30], [2, 32, 1],\n         [2, 33, 0]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3] + vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64Shape(self):\n    vocab_size = [50, 30]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n\nclass SparseRetainTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(val, np.int32), np.array(shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        to_retain = np.array([1, 0, 0, 1, 1, 0], dtype=np.bool_)\n        sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n        output = self.evaluate(sp_output)\n\n        self.assertAllEqual(output.indices, [[0, 0], [1, 4], [3, 2]])\n        self.assertAllEqual(output.values, [0, 14, 32])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testRetainNone(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.zeros((6,), dtype=np.bool_)\n      sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, np.array([]).reshape((0, 2)))\n      self.assertAllEqual(output.values, [])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testMismatchedRetainShape(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.array([1, 0, 0, 1, 0], dtype=np.bool_)\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_retain(sp_input, to_retain)\n\n\nclass SparseResetShapeTest(test_util.TensorFlowTestCase):\n\n  _IND_2_5_6 = np.array(\n      [[0, 0, 0], [0, 1, 0], [0, 1, 3], [1, 1, 4], [1, 3, 2], [1, 3, 3]],\n      dtype=np.int64)\n  _VAL_2_5_6 = np.array([0, 10, 13, 14, 32, 33], dtype=np.int32)\n  _SHP_2_5_6 = np.array([2, 5, 6], dtype=np.int64)\n\n  def _SparseTensor_2x5x6(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(self._IND_2_5_6, dtypes.int64),\n        constant_op.constant(self._VAL_2_5_6, dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensor_2x5x6_Empty(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(\n            np.empty(shape=[0, 3], dtype=np.int64), dtypes.int64),\n        constant_op.constant(np.empty(shape=[0], dtype=np.int32), dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensorValue_2x5x6(self):\n    return sparse_tensor.SparseTensorValue(self._IND_2_5_6, self._VAL_2_5_6,\n                                           self._SHP_2_5_6)\n\n  def testStaticShapeInfoPreservedWhenNewShapeIsProvidedAndStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 6, 7], dtype=np.int64)\n    sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n    self.assertAllEqual([3, 6, 7], sp_output.get_shape())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testInputUnavailableInGraphConstructionOk(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensorValue_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  @test_util.run_deprecated_v1\n  def testFeedInputUnavailableInGraphConstructionOk(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = sess.run(sp_output,\n                        feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testTightBoundingBox(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [2, 4, 5])\n\n  def testTightBoundingBoxEmpty(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6_Empty()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices.shape, [0, 3])\n      self.assertAllEqual(output.values.shape, [0])\n      self.assertAllEqual(output.dense_shape, [0, 0, 0])\n\n  def testInvalidRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 7], dtype=np.int64)\n\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidRankNewShapeUnavailableInGraphConstruction(self):\n    with self.session(use_gpu=False) as sess:\n      new_shape = array_ops.placeholder(dtype=dtypes.int64)\n      sp_input = self._SparseTensor_2x5x6()\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x == y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: np.array([3, 7], dtype=np.int64)})\n\n  def testInvalidDimensionSizeStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 7, 5], dtype=np.int64)\n\n    with self.assertRaisesRegex(ValueError, \"should have dimension sizes\"):\n      sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeDynamic(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = array_ops.placeholder(dtype=dtypes.int32)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: [3, 7, 5]})\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeInputUnavailableInGraphConstruction(self):\n    sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n    with self.session(use_gpu=False) as sess:\n      new_shape = np.array([3, 7, 5], dtype=np.int64)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n\nclass SparseFillEmptyRowsTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self, dtype=np.int32):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64), np.array(val, dtype), np.array(\n            shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def _SparseTensor_String5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.string),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4]])\n    val = np.array([0, 10, 13, 14])\n    shape = np.array([2, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.int32),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testFillNumber(self):\n    with test_util.use_gpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        sp_output, empty_row_indicator = (\n            sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n        output, empty_row_indicator_out = self.evaluate(\n            [sp_output, empty_row_indicator])\n\n        self.assertAllEqual(\n            output.indices,\n            [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n        self.assertAllEqual(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n        self.assertAllEqual(empty_row_indicator_out,\n                            np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  @test_util.run_deprecated_v1\n  def testFillFloat(self):\n    with self.session():\n      values = constant_op.constant(\n          [0.0, 10.0, 13.0, 14.0, 32.0, 33.0], dtype=dtypes.float64)\n      default_value = constant_op.constant(-1.0, dtype=dtypes.float64)\n      sp_input = sparse_tensor.SparseTensorValue(\n          indices=np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]]),\n          values=values,\n          dense_shape=np.array([5, 6]))\n      sp_output, empty_row_indicator = (sparse_ops.sparse_fill_empty_rows(\n          sp_input, default_value))\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4],\n                                           [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllClose(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n      values_grad_err = gradient_checker.compute_gradient_error(\n          values, values.shape.as_list(), sp_output.values, [8], delta=1e-8)\n      self.assertGreater(values_grad_err, 0)\n      self.assertLess(values_grad_err, 1e-8)\n\n      default_value_grad_err = gradient_checker.compute_gradient_error(\n          default_value,\n          default_value.shape.as_list(),\n          sp_output.values, [8],\n          delta=1e-8)\n      self.assertGreater(default_value_grad_err, 0)\n      self.assertLess(default_value_grad_err, 1e-8)\n\n  def testFillString(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_String5x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, \"\"))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(\n          output.indices,\n          [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllEqual(output.values,\n                          [b\"a\", b\"b\", b\"c\", b\"d\", b\"\", b\"e\", b\"f\", b\"\"])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  def testNoEmptyRows(self):\n    with test_util.use_gpu():\n      sp_input = self._SparseTensor_2x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14])\n      self.assertAllEqual(output.dense_shape, [2, 6])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testNoEmptyRowsAndUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 1], [0, 3], [1, 2], [1, 3]])\n      self.assertAllEqual(output.values, [2, 4, 1, 3])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[2, 3], [2, 2], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([3, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices,\n                          [[0, 1], [0, 3], [1, 0], [2, 3], [2, 2]])\n      self.assertAllEqual(output.values, [2, 4, -1, 1, 3])\n      self.assertAllEqual(output.dense_shape, [3, 5])\n      self.assertAllEqual(empty_row_indicator_out, [False, True, False])\n\n  def testEmptyIndicesTensor(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0]])\n      self.assertAllEqual(output.values, [-1, -1])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.ones(2).astype(np.bool_))\n\n  def testEmptyOutput(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([0, 3]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, np.ones([0, 2]))\n      self.assertAllEqual(output.values, np.ones([0]))\n      self.assertAllEqual(output.dense_shape, [0, 3])\n      self.assertAllEqual(empty_row_indicator_out, [])\n\n  def testInvalidIndices(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [99, 1], [99, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  r\"indices\\(2, 0\\) is invalid\"):\n        self.evaluate(sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n\nclass SparseAddTest(test_util.TensorFlowTestCase):\n\n  def testValuesInVariable(self):\n    indices = constant_op.constant([[1]], dtype=dtypes.int64)\n    values = variables.Variable([1], trainable=False, dtype=dtypes.float32)\n    shape = constant_op.constant([1], dtype=dtypes.int64)\n\n    sp_input = sparse_tensor.SparseTensor(indices, values, shape)\n    sp_output = sparse_ops.sparse_add(sp_input, sp_input)\n\n    with test_util.force_cpu():\n      self.evaluate(variables.global_variables_initializer())\n      output = self.evaluate(sp_output)\n      self.assertAllEqual(output.values, [2])\n\n\nclass SparseReduceTest(test_util.TensorFlowTestCase):\n\n  # [[1, ?, 2]\n  #  [?, 3, ?]]\n  # where ? is implicitly-zero.\n  ind = np.array([[0, 0], [0, 2], [1, 1]]).astype(np.int64)\n  vals = np.array([1, 1, 1]).astype(np.int32)\n  dense_shape = np.array([2, 3]).astype(np.int64)\n\n  def _compare(self, sp_t, reduction_axes, ndims, keep_dims, do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_ans = densified\n    if reduction_axes is None:\n      if do_sum:\n        np_ans = np.sum(np_ans, keepdims=keep_dims)\n      else:\n        np_ans = np.max(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        if do_sum:\n          np_ans = np.sum(np_ans, axis=ra, keepdims=keep_dims)\n        else:\n          np_ans = np.max(np_ans, axis=ra, keepdims=keep_dims)\n\n    with self.cached_session():\n      if do_sum:\n        tf_dense_ans = sparse_ops.sparse_reduce_sum(sp_t, reduction_axes,\n                                                    keep_dims)\n      else:\n        tf_dense_ans = sparse_ops.sparse_reduce_max(sp_t, reduction_axes,\n                                                    keep_dims)\n      out_dense = self.evaluate(tf_dense_ans)\n\n      if do_sum:\n        tf_sparse_ans = sparse_ops.sparse_reduce_sum_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      else:\n        tf_sparse_ans = sparse_ops.sparse_reduce_max_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      # Convert to dense for comparison purposes.\n      out_sparse = sparse_ops.sparse_tensor_to_dense(tf_sparse_ans)\n\n    self.assertAllClose(np_ans, out_dense)\n    self.assertAllClose(np_ans, out_sparse)\n\n  def _compare_all(self, sp_t, reduction_axes, ndims):\n    self._compare(sp_t, reduction_axes, ndims, False, False)\n    self._compare(sp_t, reduction_axes, ndims, False, True)\n    self._compare(sp_t, reduction_axes, ndims, True, False)\n    self._compare(sp_t, reduction_axes, ndims, True, True)\n\n  # (TODO:b/133851381): Re-enable this test.\n  def disabledtestSimpleAndRandomInputs(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      self._compare_all(sp_t, None, ndims=2)\n      self._compare_all(sp_t, 0, ndims=2)\n      self._compare_all(sp_t, [1], ndims=2)\n      self._compare_all(sp_t, [0, 1], ndims=2)\n      self._compare_all(sp_t, [1, 0], ndims=2)\n      self._compare_all(sp_t, [-1], ndims=2)\n      self._compare_all(sp_t, [1, -2], ndims=2)\n\n    np.random.seed(1618)\n    test_dims = [(1618, 1, 11, 7, 1), (1,), (1, 1, 1)]\n    with test_util.force_cpu():\n      for dims in test_dims:\n        sp_t, unused_nnz = _sparsify(np.random.randn(*dims))\n        # reduce all using None\n        self._compare_all(sp_t, None, ndims=len(dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          self._compare_all(sp_t, axes, ndims=len(dims))\n\n  def testInvalidAxes(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n    with test_util.force_cpu():\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, 2))\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, 2))\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    np.random.seed(8161)\n    test_dims = [(11, 1, 5, 7, 1), (2, 2)]\n    with self.session(use_gpu=False):\n      for dims in test_dims:\n        sp_t, nnz = _sparsify(np.random.randn(*dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          reduced = sparse_ops.sparse_reduce_sum(sp_t, axes)\n\n          err = gradient_checker.compute_gradient_error(\n              sp_t.values, (nnz,), reduced,\n              self.evaluate(reduced).shape)\n          self.assertLess(err, 1e-3)\n\n        # Tests for negative axes.\n        reduced = sparse_ops.sparse_reduce_sum(sp_t, -1)\n        err = gradient_checker.compute_gradient_error(\n            sp_t.values, (nnz,), reduced,\n            self.evaluate(reduced).shape)\n        self.assertLess(err, 1e-3)\n\n  def _testSparseReduceShape(self, sp_t, reduction_axes, ndims, keep_dims,\n                             do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_op = np.sum\n    tf_op = sparse_ops.sparse_reduce_sum\n    if not do_sum:\n      np_op = np.max\n      tf_op = sparse_ops.sparse_reduce_max\n\n    np_ans = densified\n    if reduction_axes is None:\n      np_ans = np_op(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        np_ans = np_op(np_ans, axis=ra, keepdims=keep_dims)\n\n    tf_ans = tf_op(sp_t, reduction_axes, keep_dims)\n    self.assertAllEqual(np_ans.shape, tf_ans.get_shape().as_list())\n\n  # (TODO:b/133851381): Re-enable this test\n  def disabledtestSparseReduceSumOrMaxShape(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      for do_sum in [True, False]:\n        for keep_dims in [True, False]:\n          self._testSparseReduceShape(sp_t, None, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, 0, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [0, 1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, 0], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [-1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, -2], 2, keep_dims, do_sum)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max_sparse(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_sum(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMathOpsTest(test_util.TensorFlowTestCase):\n\n  def _check(self, result_tensor, result_np, input_sp_t):\n    self.assertTrue(isinstance(result_tensor, sparse_tensor.SparseTensor))\n    self.assertTrue(isinstance(input_sp_t, sparse_tensor.SparseTensor))\n    self.assertAllCloseAccordingToType(input_sp_t.indices,\n                                       result_tensor.indices)\n    self.assertAllCloseAccordingToType(input_sp_t.dense_shape,\n                                       result_tensor.dense_shape)\n\n    res_densified = sparse_ops.sparse_to_dense(\n        result_tensor.indices, result_tensor.dense_shape, result_tensor.values)\n    self.assertAllCloseAccordingToType(result_np, res_densified)\n\n  @test_util.run_deprecated_v1\n  def testCwiseShapeValidation(self):\n    # Test case for GitHub 24072.\n    with test_util.force_cpu():\n      a = array_ops.ones([3, 4, 1], dtype=dtypes.int32)\n      b = sparse_tensor.SparseTensor([[0, 0, 1, 0], [0, 0, 3, 0]], [10, 20],\n                                     [1, 1, 4, 2])\n      c = a * b\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          \"broadcasts dense to sparse only; got incompatible shapes\"):\n        self.evaluate(c)\n\n  def testCwiseDivAndMul(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with test_util.force_cpu():\n      for dtype in [np.float32, np.float64, np.int32, np.int64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, unused_nnz = _sparsify(sp_vals_np, thresh=1.5)\n          sp_t_densified = sparse_ops.sparse_tensor_to_dense(sp_t)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          self._check(sp_t / dense_t, sp_t_densified / dense_vals_np, sp_t)\n          # Check commutative.\n          self._check(sp_t * dense_t, sp_t_densified * dense_vals_np, sp_t)\n          self._check(dense_t * sp_t, sp_t_densified * dense_vals_np, sp_t)\n\n          if dtype in [np.int32, np.int64]:\n            res = sp_t / dense_t  # should invoke \"__truediv__\"\n            self.assertEqual(res.values.dtype, np.float64)\n\n  def testCwiseAdd(self):\n    with test_util.force_cpu():\n      # Identity(2) + AllOnes(2,2).  Should be equal to 2 * Identity(2).\n      indices = [[0, 0], [1, 1]]\n      vals = [1, 1]\n      shape = (2, 2)\n\n      sp_t = sparse_tensor.SparseTensor(indices, vals, shape)\n      dense_t = array_ops.ones(shape, dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n      # Variant of above, but broadcasts the dense side.\n      dense_t = array_ops.ones([1], dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with self.session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, nnz = _sparsify(sp_vals_np, thresh=1.5)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          cmul = sp_t * dense_t\n          err = gradient_checker.compute_gradient_error([sp_t.values, dense_t],\n                                                        [(nnz,), dense_shape],\n                                                        cmul.values, (nnz,))\n          self.assertLess(err, 1e-4)\n\n          cdiv = sp_t / dense_t\n          err = gradient_checker.compute_gradient_error(sp_t.values, (nnz,),\n                                                        cdiv.values, (nnz,))\n          self.assertLess(err, 1e-4)\n          err = gradient_checker.compute_gradient_error(\n              dense_t,\n              dense_shape,\n              cdiv.values, (nnz,),\n              x_init_value=dense_vals_np)\n          self.assertLess(err, 2e-4)\n\n\nclass SparseSoftmaxTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testEquivalentToDensified(self):\n    np.random.seed(1618)\n    n, m = np.random.choice(20, size=2)\n\n    for dtype in [np.float32, np.float64]:\n      sp_vals_np = np.random.rand(n, m).astype(dtype)\n\n      batched_sp_t, unused_nnz1 = _sparsify(\n          sp_vals_np.reshape((1, n, m)), thresh=0.)  # No masking.\n\n      with test_util.force_cpu():\n        densified = constant_op.constant(sp_vals_np)\n\n        sp_result = self.evaluate(\n            sparse_ops.sparse_softmax(batched_sp_t)).values.reshape((n, m))\n        dense_result = nn_ops.softmax(densified)\n\n        self.assertAllClose(dense_result, sp_result)\n\n  def testHigherRanks(self):\n    # For the first shape:\n    # First batch:\n    # [?   e.]\n    # [1.  ? ]\n    # Second batch:\n    # [e   ? ]\n    # [e   e ]\n    #\n    # The softmax results should be:\n    # [?   1.]     [1    ?]\n    # [1.  ? ] and [.5  .5]\n    # where ? means implicitly zero.\n    #\n    # The second shape: same input data, but with a higher-rank shape.\n    shapes = [[2, 2, 2], [2, 1, 2, 2]]\n    for shape in shapes:\n      values = np.asarray(\n          [0., np.e, 1., 0., np.e, 0., np.e, np.e]).reshape(shape)\n      sp_t, unused_nnz = _sparsify(values, thresh=1e-2)\n      expected_values = [1., 1., 1., .5, .5]\n\n      with test_util.force_cpu():\n        result = sparse_ops.sparse_softmax(sp_t)\n\n        self.assertAllEqual(expected_values, result.values)\n        self.assertAllEqual(sp_t.indices, result.indices)\n        self.assertAllEqual(shape, result.dense_shape)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [2, 5, 10]\n    with self.cached_session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        x_np = np.random.randn(*x_shape).astype(dtype)\n        x_tf, nnz = _sparsify(x_np)\n        y_tf = sparse_ops.sparse_softmax(x_tf)\n        err = gradient_checker.compute_gradient_error(x_tf.values, (nnz,),\n                                                      y_tf.values, (nnz,))\n        self.assertLess(err, 1e-4)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]],\n            sp_values=[2.0],\n            sp_shape=[2**32, 2**31],\n            name=None)\n\n        self.evaluate(res)\n\n  def testReshapeNegativeShape(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]], sp_values=[2.0], sp_shape=[-1, 1], name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMinimumMaximumTest(test_util.TensorFlowTestCase):\n\n  def _assertSparseTensorValueEqual(self, a, b):\n    self.assertAllEqual(a.indices, b.indices)\n    self.assertAllEqual(a.values, b.values)\n    self.assertAllEqual(a.dense_shape, b.dense_shape)\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      # 1-D, values at index 0.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_one)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_one)\n      self._assertSparseTensorValueEqual(sp_one, max_tf)\n      self._assertSparseTensorValueEqual(sp_zero, min_tf)\n\n      # Values at different indices.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_zero_2 = sparse_tensor.SparseTensor([[1]], [0], [7])\n      expected = sparse_tensor.SparseTensor([[0], [1]], [0, 0], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_zero_2)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_zero_2)\n      self._assertSparseTensorValueEqual(expected, max_tf)\n      self._assertSparseTensorValueEqual(expected, min_tf)\n\n  @test_util.run_deprecated_v1\n  def testRandom(self):\n    np.random.seed(1618)\n    shapes = [(13,), (6, 8), (1, 7, 1)]\n    for shape in shapes:\n      for dtype in [np.int32, np.int64, np.float16, np.float32, np.float64]:\n        a_np = np.random.randn(*shape).astype(dtype)\n        b_np = np.random.randn(*shape).astype(dtype)\n        sp_a, unused_a_nnz = _sparsify(a_np, thresh=-.5)\n        sp_b, unused_b_nnz = _sparsify(b_np, thresh=-.5)\n\n        with self.cached_session(use_gpu=False):\n          maximum_tf = sparse_ops.sparse_maximum(sp_a, sp_b)\n          maximum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              maximum_tf).eval()\n          minimum_tf = sparse_ops.sparse_minimum(sp_a, sp_b)\n          minimum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              minimum_tf).eval()\n\n          a_densified = sparse_ops.sparse_tensor_to_dense(sp_a).eval()\n          b_densified = sparse_ops.sparse_tensor_to_dense(sp_b).eval()\n\n        self.assertAllEqual(\n            np.maximum(a_densified, b_densified), maximum_tf_densified)\n        self.assertAllEqual(\n            np.minimum(a_densified, b_densified), minimum_tf_densified)\n\n  def testMismatchedShapes(self):\n    with test_util.force_cpu():\n      sp_zero = sparse_tensor.SparseTensor([[0, 0]], [0], [1, 1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands do not have the same ranks\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands' shapes do not match\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n\nclass SparseTransposeTest(test.TestCase):\n\n  def testTranspose(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    with test_util.force_cpu():\n      np.random.seed(1618)\n      shapes = [np.random.randint(1, 10, size=rank) for rank in range(1, 6)]\n      for shape in shapes:\n        for dtype in [np.int32, np.int64, np.float32, np.float64]:\n          dn_input = np.random.randn(*shape).astype(dtype)\n          rank = self.evaluate(array_ops.rank(dn_input))\n          perm = np.random.choice(rank, rank, False)\n          sp_input, unused_a_nnz = _sparsify(dn_input)\n          sp_trans = sparse_ops.sparse_transpose(sp_input, perm=perm)\n          dn_trans = sparse_ops.sparse_tensor_to_dense(sp_trans)\n          expected_trans = array_ops.transpose(dn_input, perm=perm)\n          self.assertAllEqual(expected_trans.shape, sp_trans.get_shape())\n          self.assertAllEqual(dn_trans, expected_trans)\n\n\nclass SparsePlaceholderTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testPlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(10, 47))\n    self.assertAllEqual([10, 47], foo.get_shape())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testPartialShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(None, 47))\n    self.assertAllEqual([None, 47], foo.get_shape().as_list())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testNoShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=None)\n    self.assertAllEqual(None, foo.get_shape())\n    self.assertAllEqual([None, None], foo.indices.get_shape().as_list())\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/sparse_tensor_dense_add_op.h\"\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/op_requires.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_util.h\"\n#include \"tensorflow/core/framework/types.h\"\n#include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n\nnamespace tensorflow {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n// NOTE: does not support GPU yet.\n\nnamespace {\n\ntemplate <typename Index>\nStatus ValidateInputs(const Tensor *a_indices, const Tensor *a_values,\n                      const Tensor *a_shape, const Tensor *b) {\n  if (!TensorShapeUtils::IsMatrix(a_indices->shape())) {\n    return errors::InvalidArgument(\n        \"Input a_indices should be a matrix but received shape: \",\n        a_indices->shape().DebugString());\n  }\n  if (!TensorShapeUtils::IsVector(a_values->shape()) ||\n      !TensorShapeUtils::IsVector(a_shape->shape())) {\n    return errors::InvalidArgument(\n        \"Inputs a_values and a_shape should be vectors \"\n        \"but received shapes: \",\n        a_values->shape().DebugString(), \" and \",\n        a_shape->shape().DebugString());\n  }\n  int64_t nnz = a_indices->dim_size(0);\n  int64_t ndims = a_indices->dim_size(1);\n  if (a_values->dim_size(0) != nnz) {\n    return errors::InvalidArgument(\"Dimensions \", nnz, \" and \",\n                                   a_values->dim_size(0),\n                                   \" are not compatible\");\n  }\n  if (a_shape->dim_size(0) != ndims) {\n    return errors::InvalidArgument(\"Dimensions \", ndims, \" and \",\n                                   a_shape->dim_size(0), \" are not compatible\");\n  }\n  if (a_shape->NumElements() != b->dims()) {\n    return errors::InvalidArgument(\n        \"Two operands have different ranks; received: \", a_shape->NumElements(),\n        \" and \", b->dims());\n  }\n  const auto a_shape_flat = a_shape->flat<Index>();\n  for (int i = 0; i < b->dims(); ++i) {\n    if (a_shape_flat(i) != b->dim_size(i)) {\n      return errors::InvalidArgument(\n          \"Dimension \", i,\n          \" does not equal (no broadcasting is supported): sparse side \",\n          a_shape_flat(i), \" vs dense side \", b->dim_size(i));\n    }\n  }\n\n  // Check for invalid indices.\n  const auto a_indices_mat = a_indices->flat_inner_dims<Index>();\n\n  for (int64_t zidx = 0; zidx < nnz; ++zidx) {\n    for (int64_t didx = 0; didx < ndims; ++didx) {\n      const Index idx = a_indices_mat(zidx, didx);\n      if (idx < 0 || idx >= a_shape_flat(didx)) {\n        return errors::InvalidArgument(\n            \"Sparse tensor has an invalid index on dimension \", didx,\n            \": \"\n            \"a_indices(\",\n            zidx, \",\", didx, \") = \", idx,\n            \", dense tensor shape: \", a_shape_flat);\n      }\n    }\n  }\n\n  return Status::OK();\n}\n\n}  // namespace\n\ntemplate <typename Device, typename T, typename Index>\nclass SparseTensorDenseAddOp : public OpKernel {\n public:\n  explicit SparseTensorDenseAddOp(OpKernelConstruction *ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext *ctx) override {\n    const Tensor *a_indices_t, *a_values_t, *a_shape_t, *b;\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_indices\", &a_indices_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_shape\", &a_shape_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b));\n    OP_REQUIRES_OK(\n        ctx, ValidateInputs<Index>(a_indices_t, a_values_t, a_shape_t, b));\n\n    Tensor *out_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, b->shape(), &out_t));\n\n    const int ndims = static_cast<int>(a_indices_t->dim_size(1));\n    const auto a_indices_mat = a_indices_t->flat_inner_dims<Index>();\n    const auto a_values_flat = a_values_t->flat<T>();\n\n    switch (ndims) {\n#define NDIMS_CASE(N)                                                     \\\n  case N: {                                                               \\\n    auto out_tensor = out_t->tensor<T, N>();                              \\\n    out_tensor.device(ctx->eigen_device<Device>()) = b->tensor<T, N>();   \\\n    const Index result =                                                  \\\n        functor::ScatterNdFunctor<Device, T, Index, N,                    \\\n                                  scatter_op::UpdateOp::ADD>()(           \\\n            ctx->eigen_device<Device>(), a_indices_mat, a_values_flat,    \\\n            out_tensor);                                                  \\\n    OP_REQUIRES(                                                          \\\n        ctx, result == -1,                                                \\\n        errors::InvalidArgument(                                          \\\n            \"Sparse tensor has some invalid index on dimension \", result, \\\n            \"; dense tensor shape: \", b->shape().DebugString()));         \\\n  } break;\n\n      NDIMS_CASE(1);\n      NDIMS_CASE(2);\n      NDIMS_CASE(3);\n      NDIMS_CASE(4);\n      NDIMS_CASE(5);\n      default:\n        OP_REQUIRES(\n            ctx, false,\n            errors::InvalidArgument(\"Only tensors with ranks between 1 and 5 \"\n                                    \"are currently supported.  Tensor rank: \",\n                                    ndims));\n#undef NDIMS_CASE\n    }\n  }\n};\n\nnamespace functor {\ntemplate <typename T, typename Index, int NDIMS>\nstruct ScatterNdFunctor<CPUDevice, T, Index, NDIMS, scatter_op::UpdateOp::ADD> {\n  Index operator()(const CPUDevice &d,\n                   typename TTypes<Index>::ConstMatrix indices,\n                   typename TTypes<T>::ConstFlat updates,\n                   typename TTypes<T, NDIMS>::Tensor out) {\n    Eigen::array<Eigen::DenseIndex, NDIMS> idx;\n    const int num_nnz = static_cast<int>(indices.dimension(0));\n    for (int i = 0; i < num_nnz; ++i) {\n      for (int d = 0; d < NDIMS; ++d) {\n        idx[d] = internal::SubtleMustCopy(indices(i, d));\n        if (!FastBoundsCheck(idx[d], out.dimension(d))) {\n          return d;  // on failure: d nonnegative\n        }\n      }\n      out(idx) += updates(i);\n    }\n    return -1;  // on success\n  }\n};\n}  // namespace functor\n\n#define REGISTER_KERNELS_CPU(TypeT, TypeIndex)                        \\\n  REGISTER_KERNEL_BUILDER(Name(\"SparseTensorDenseAdd\")                \\\n                              .Device(DEVICE_CPU)                     \\\n                              .TypeConstraint<TypeT>(\"T\")             \\\n                              .TypeConstraint<TypeIndex>(\"Tindices\"), \\\n                          SparseTensorDenseAddOp<CPUDevice, TypeT, TypeIndex>)\n\n#define REGISTER_KERNELS(T)         \\\n  REGISTER_KERNELS_CPU(T, int64_t); \\\n  REGISTER_KERNELS_CPU(T, int32)\n\nTF_CALL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n#undef REGISTER_KERNELS_CPU\n}  // namespace tensorflow\n", "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for SparseAdd.\"\"\"\n\nimport timeit\n\nimport numpy as np\n\nfrom tensorflow.python.client import session\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import sparse_ops\nimport tensorflow.python.ops.sparse_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\n\n\ndef _sparsify(x, thresh=0.5, index_dtype=np.int64):\n  x[x < thresh] = 0\n\n  non_zero = np.where(x)\n  x_indices = np.vstack(non_zero).astype(index_dtype).T\n  x_values = x[non_zero]\n  x_shape = x.shape\n\n  return sparse_tensor.SparseTensor(\n      indices=x_indices, values=x_values, dense_shape=x_shape), len(x_values)\n\n\nclass SparseAddTest(test.TestCase):\n\n  def _randomTensor(self, size, np_dtype, sparse=True):\n    n, m = size\n    x = np.random.randn(n, m).astype(np_dtype)\n    return _sparsify(x) if sparse else x\n\n  def _SparseTensorValue_3x3(self, negate=False):\n    # [    1]\n    # [2    ]\n    # [3   4]\n    # ...or its cwise negation, if `negate`\n    ind = np.array([[0, 1], [1, 0], [2, 0], [2, 1]])\n    val = np.array([1, 2, 3, 4])\n    if negate:\n      val = -np.array([1, 2, 3, 4])\n    shape = np.array([3, 3])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(val, np.float32), np.array(shape, np.int64))\n\n  def _SparseTensor_3x3(self, negate=False):\n    return sparse_tensor.SparseTensor.from_value(\n        self._SparseTensorValue_3x3(negate))\n\n  def _SparseTensor_3x3_v2(self):\n    # [           1]\n    # [-1.9        ]\n    # [   3    -4.2]\n    ind = np.array([[0, 1], [1, 0], [2, 0], [2, 1]])\n    val = np.array([1, -1.9, 3, -4.2])\n    shape = np.array([3, 3])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.float32),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testAddSelf(self):\n    with test_util.force_cpu():\n      for sp_a in (self._SparseTensorValue_3x3(), self._SparseTensor_3x3()):\n        for sp_b in (self._SparseTensorValue_3x3(), self._SparseTensor_3x3()):\n          sp_sum = sparse_ops.sparse_add(sp_a, sp_b)\n          self.assertAllEqual((3, 3), sp_sum.get_shape())\n\n          sum_out = self.evaluate(sp_sum)\n\n          self.assertEqual(sp_sum.dense_shape.get_shape(), [2])\n          self.assertAllEqual(sum_out.indices, [[0, 1], [1, 0], [2, 0], [2, 1]])\n          self.assertAllEqual(sum_out.values, [2, 4, 6, 8])\n          self.assertAllEqual(sum_out.dense_shape, [3, 3])\n\n  def testAddSelfAndNegation(self):\n    with test_util.force_cpu():\n      sp_a = self._SparseTensor_3x3()\n      sp_b = self._SparseTensor_3x3(negate=True)\n\n      sp_sum = sparse_ops.sparse_add(sp_a, sp_b, 0.1)\n      sum_out = self.evaluate(sp_sum)\n\n      self.assertEqual(sp_sum.dense_shape.get_shape(), [2])\n      self.assertAllEqual(sum_out.indices, np.empty([0, 2]))\n      self.assertAllEqual(sum_out.values, [])\n      self.assertAllEqual(sum_out.dense_shape, [3, 3])\n\n  def testSmallValuesShouldVanish(self):\n    with test_util.force_cpu():\n      sp_a = self._SparseTensor_3x3()\n      sp_b = self._SparseTensor_3x3_v2()\n\n      # sum:\n      # [       2]\n      # [.1      ]\n      # [ 6   -.2]\n\n      # two values should vanish: |.1| < .21, and |-.2| < .21\n      sp_sum = sparse_ops.sparse_add(sp_a, sp_b, thresh=0.21)\n      sum_out = self.evaluate(sp_sum)\n\n      self.assertEqual(sp_sum.dense_shape.get_shape(), [2])\n      self.assertAllEqual(sum_out.indices, [[0, 1], [2, 0]])\n      self.assertAllEqual(sum_out.values, [2, 6])\n      self.assertAllEqual(sum_out.dense_shape, [3, 3])\n\n      # only .1 vanishes\n      sp_sum = sparse_ops.sparse_add(sp_a, sp_b, thresh=0.11)\n      sum_out = self.evaluate(sp_sum)\n\n      self.assertEqual(sp_sum.dense_shape.get_shape(), [2])\n      self.assertAllEqual(sum_out.indices, [[0, 1], [2, 0], [2, 1]])\n      self.assertAllClose(sum_out.values, [2, 6, -.2])\n      self.assertAllEqual(sum_out.dense_shape, [3, 3])\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    np.random.seed(1618)  # Make it reproducible.\n    with self.session(use_gpu=False):\n      for n in [10, 31]:\n        for m in [4, 17]:\n          sp_a, nnz_a = self._randomTensor([n, m], np.float32)\n          sp_b, nnz_b = self._randomTensor([n, m], np.float32)\n          sp_sum = sparse_ops.sparse_add(sp_a, sp_b)\n          nnz_sum = len(self.evaluate(sp_sum.values))\n\n          err = gradient_checker.compute_gradient_error(\n              [sp_a.values, sp_b.values], [(nnz_a,), (nnz_b,)], sp_sum.values,\n              (nnz_sum,))\n          self.assertLess(err, 1e-3)\n\n  def testAddSparseDense(self):\n    np.random.seed(1618)  # Make it reproducible.\n    n, m = np.random.randint(30, size=2)\n    for dtype in [np.float32, np.float64, np.int64, np.complex64]:\n      for index_dtype in [np.int32, np.int64]:\n        rand_vals_np = np.random.randn(n, m).astype(dtype)\n        dense_np = np.random.randn(n, m).astype(dtype)\n\n        with test_util.force_cpu():\n          sparse, unused_nnz = _sparsify(rand_vals_np, index_dtype=index_dtype)\n          s = self.evaluate(\n              sparse_ops.sparse_add(sparse, constant_op.constant(dense_np)))\n          self.assertAllEqual(dense_np + rand_vals_np, s)\n          self.assertTrue(s.dtype == dtype)\n\n          # check commutativity\n          s = self.evaluate(\n              sparse_ops.sparse_add(constant_op.constant(dense_np), sparse))\n          self.assertAllEqual(dense_np + rand_vals_np, s)\n          self.assertTrue(s.dtype == dtype)\n\n  @test_util.run_deprecated_v1\n  def testSparseTensorDenseAddGradients(self):\n    np.random.seed(1618)  # Make it reproducible.\n    n, m = np.random.randint(30, size=2)\n    rand_vals_np = np.random.randn(n, m).astype(np.float32)\n    dense_np = np.random.randn(n, m).astype(np.float32)\n\n    with self.session(use_gpu=False):\n      sparse, nnz = _sparsify(rand_vals_np)\n      dense = constant_op.constant(dense_np, dtype=dtypes.float32)\n      s = sparse_ops.sparse_add(sparse, dense)\n\n      err = gradient_checker.compute_gradient_error([sparse.values, dense],\n                                                    [(nnz,), (n, m)], s, (n, m))\n      self.assertLess(err, 1e-3)\n\n  def testInvalidSparseTensor(self):\n    with test_util.force_cpu():\n      shape = [2, 2]\n      val = [0]\n      dense = constant_op.constant(np.zeros(shape, dtype=np.int32))\n\n      for bad_idx in [\n          [[-1, 0]],  # -1 is invalid.\n          [[1, 3]],  # ...so is 3.\n      ]:\n        sparse = sparse_tensor.SparseTensorValue(bad_idx, val, shape)\n        with self.assertRaisesRegex(\n            (ValueError, errors_impl.InvalidArgumentError), \"invalid index\"):\n          s = sparse_ops.sparse_add(sparse, dense)\n          self.evaluate(s)\n\n  def _testSparseDenseInvalidInputs(self,\n                                    a_indices,\n                                    a_values,\n                                    a_shape,\n                                    b,\n                                    expected_error=\"\"):\n    # Public API call to sparse-dense add.\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                expected_error):\n      a = sparse_tensor.SparseTensor(a_indices, a_values, a_shape)\n      self.evaluate(sparse_ops.sparse_add(a, b))\n    # Directly call generated kernel, by-passing SparseTensor validation.\n    with self.assertRaisesRegex((ValueError, errors_impl.InvalidArgumentError),\n                                expected_error):\n      self.evaluate(\n          sparse_ops.gen_sparse_ops.sparse_tensor_dense_add(\n              a_indices, a_values, a_shape, b))\n\n  def testSparseDenseInvalidInputs(self):\n    self._testSparseDenseInvalidInputs(\n        a_indices=constant_op.constant(0, shape=[17, 2], dtype=dtypes.int64),\n        a_values=constant_op.constant(0, shape=[5], dtype=dtypes.float32),\n        a_shape=constant_op.constant([3, 4], dtype=dtypes.int64),\n        b=constant_op.constant(1, shape=[3, 4], dtype=dtypes.float32),\n        expected_error=\"Dimensions 17 and 5 are not compatible\")\n    self._testSparseDenseInvalidInputs(\n        a_indices=constant_op.constant(0, shape=[17, 4], dtype=dtypes.int64),\n        a_values=constant_op.constant(0, shape=[17], dtype=dtypes.float32),\n        a_shape=constant_op.constant([3, 4], dtype=dtypes.int64),\n        b=constant_op.constant(1, shape=[3, 4], dtype=dtypes.float32),\n        expected_error=\"Dimensions 4 and 2 are not compatible\")\n    self._testSparseDenseInvalidInputs(\n        a_indices=constant_op.constant(7, shape=[17, 2], dtype=dtypes.int64),\n        a_values=constant_op.constant(0, shape=[17], dtype=dtypes.float32),\n        a_shape=constant_op.constant([3, 4], dtype=dtypes.int64),\n        b=constant_op.constant(1, shape=[3, 4], dtype=dtypes.float32),\n        expected_error=\"invalid index\")\n\n######################## Benchmarking code\n\n\ndef _s2d_add_vs_sparse_add(sparsity, n, m, num_iters=50):\n  np.random.seed(1618)\n\n  with session.Session(graph=ops.Graph()) as sess:\n    sp_vals = np.random.rand(n, m).astype(np.float32)\n    sp_t, unused_nnz = _sparsify(sp_vals, thresh=sparsity, index_dtype=np.int32)\n    vals = np.random.rand(n, m).astype(np.float32)\n\n    s2d = math_ops.add(\n        sparse_ops.sparse_tensor_to_dense(sp_t), constant_op.constant(vals))\n    sa = sparse_ops.sparse_add(sp_t, constant_op.constant(vals))\n\n    timeit.timeit(lambda: sess.run(s2d), number=3)\n    timeit.timeit(lambda: sess.run(sa), number=3)\n\n    s2d_total = timeit.timeit(lambda: sess.run(s2d), number=num_iters)\n    sa_total = timeit.timeit(lambda: sess.run(sa), number=num_iters)\n\n  # per-iter latency; secs to millis\n  return s2d_total * 1e3 / num_iters, sa_total * 1e3 / num_iters\n\n\nclass SparseAddBenchmark(test.Benchmark):\n\n  def benchmarkSparseAddDense(self):\n\n    print(\"SparseAddDense: add with sparse_to_dense vs. sparse_add\")\n    print(\"%nnz \\t n \\t m \\t millis(s2d) \\t millis(sparse_add) \\t speedup\")\n\n    for sparsity in [0.99, 0.5, 0.01]:\n      for n in [1, 256, 50000]:\n        for m in [100, 1000]:\n          s2d_dt, sa_dt = _s2d_add_vs_sparse_add(sparsity, n, m)\n          print(\"%.2f \\t %d \\t %d \\t %.4f \\t %.4f \\t %.2f\" % (sparsity, n, m,\n                                                              s2d_dt, sa_dt,\n                                                              s2d_dt / sa_dt))\n\n\nif __name__ == \"__main__\":\n  test.main()\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for Python ops defined in sparse_ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import nn_ops\nfrom tensorflow.python.ops import sparse_ops\nfrom tensorflow.python.ops import variables\nimport tensorflow.python.ops.sparse_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.platform import test\n\n\n# TODO(zongheng): it'd be great to factor out this function and various random\n# SparseTensor gen funcs.\ndef _sparsify(x, thresh=0.5, index_dtype=np.int64):\n  x[x < thresh] = 0\n\n  non_zero = np.where(x)\n  x_indices = np.vstack(non_zero).astype(index_dtype).T\n  x_values = x[non_zero]\n  x_shape = x.shape\n\n  return sparse_tensor.SparseTensor(\n      indices=x_indices, values=x_values, dense_shape=x_shape), len(x_values)\n\n\nclass SparseToIndicatorTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_5x6(self, dtype):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x3x4(self, dtype):\n    # Includes two entries with the form [1, 1, x] : 150.\n    ind = np.array([[0, 0, 1], [0, 1, 0], [0, 1, 2], [1, 0, 3], [1, 1, 0],\n                    [1, 1, 1], [1, 1, 2], [1, 2, 2]])\n    val = np.array([1, 10, 12, 103, 150, 149, 150, 122])\n    shape = np.array([2, 3, 4])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtype),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testInt32(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int32)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = ((0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33))\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testInt64(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 50)\n\n      expected_output = np.zeros((5, 50), dtype=np.bool_)\n      expected_trues = [(0, 0), (1, 10), (1, 13), (1, 14), (3, 32), (3, 33)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n  def testHigherRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x3x4(dtypes.int64)\n      output = sparse_ops.sparse_to_indicator(sp_input, 200)\n\n      expected_output = np.zeros((2, 3, 200), dtype=np.bool_)\n      expected_trues = [(0, 0, 1), (0, 1, 10), (0, 1, 12), (1, 0, 103),\n                        (1, 1, 149), (1, 1, 150), (1, 2, 122)]\n      for expected_true in expected_trues:\n        expected_output[expected_true] = True\n\n      self.assertAllEqual(output, expected_output)\n\n\nclass SparseMergeTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices = np.array([0, 13, 10, 33, 32, 14])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return indices, values\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    indices, values = self._SparseTensorValue_3x50(indices_dtype, values_dtype)\n    return (sparse_tensor.SparseTensor.from_value(indices),\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 10], [1, 13], [1, 14], [2, 32], [2, 33]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def _AssertResultsNotSorted(self, output, vocab_size):\n    self.assertAllEqual(output.indices,\n                        [[0, 0], [1, 13], [1, 10], [2, 33], [2, 32], [1, 14]])\n    self.assertAllEqual(output.values, [-3, 4, 1, 9, 5, 1])\n    self.assertAllEqual(output.dense_shape, [3, vocab_size])\n\n  def testInt32AndFloat32(self):\n    vocab_size = 50\n    indices_v, values_v = self._SparseTensorValue_3x50(np.int32, np.float32)\n    with test_util.force_cpu():\n      for indices in (indices_v,\n                      sparse_tensor.SparseTensor.from_value(indices_v)):\n        for values in (values_v,\n                       sparse_tensor.SparseTensor.from_value(values_v)):\n          sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n          output = self.evaluate(sp_output)\n          self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt32AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int32, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat32NonCanonicalOrder(self):\n    vocab_size = 50\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testInt64AndFloat64NonCanonicalOrder(self):\n    vocab_size = 50\n    vocab_size_tensor = constant_op.constant(vocab_size, dtypes.int64)\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(\n          indices, values, vocab_size_tensor, already_sorted=True)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsNotSorted(output, vocab_size)\n\n  def testShouldSetLastDimensionInDynamicShape(self):\n    with ops.Graph().as_default():\n      shape = constant_op.constant([2, 2], dtype=dtypes.int64)\n      dynamic_shape = array_ops.placeholder_with_default(shape, shape=[2])\n      ids = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[1, 3],\n          dense_shape=dynamic_shape)\n      values = sparse_tensor.SparseTensor(\n          indices=[[0, 0], [0, 1]],\n          values=[0.4, 0.7],\n          dense_shape=dynamic_shape)\n      merged = sparse_ops.sparse_merge(\n          sp_ids=ids, sp_values=values, vocab_size=5)\n      self.assertEqual(5, merged.get_shape()[1])\n\n\nclass SparseMergeHighDimTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensor_3x50(self, indices_dtype, values_dtype):\n    # NOTE: This input is intentionally not sorted to validate the\n    # already_sorted flag below.\n    ind = np.array([[0, 0], [1, 0], [1, 2], [2, 0], [2, 1], [1, 1]])\n    # NB: these are not sorted\n    indices0 = np.array([0, 13, 10, 33, 32, 14])\n    indices1 = np.array([12, 4, 0, 0, 1, 30])\n    values = np.array([-3, 4, 1, 9, 5, 1])\n    shape = np.array([3, 3])\n    indices0 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices0, indices_dtype), np.array(shape, np.int64))\n    indices1 = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(indices1, indices_dtype), np.array(shape, np.int64))\n    values = sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(values, values_dtype), np.array(shape, np.int64))\n    return ([sparse_tensor.SparseTensor.from_value(indices0),\n             sparse_tensor.SparseTensor.from_value(indices1)],\n            sparse_tensor.SparseTensor.from_value(values))\n\n  def _AssertResultsSorted(self, output, vocab_size):\n    self.assertAllEqual(\n        output.indices,\n        [[0, 0, 12], [1, 10, 0], [1, 13, 4], [1, 14, 30], [2, 32, 1],\n         [2, 33, 0]])\n    self.assertAllEqual(output.values, [-3, 1, 4, 1, 5, 9])\n    self.assertAllEqual(output.dense_shape, [3] + vocab_size)\n\n  def testInt64AndFloat32(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float32)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64(self):\n    vocab_size = [50, 31]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n  def testInt64AndFloat64Shape(self):\n    vocab_size = [50, 30]\n    with test_util.force_cpu():\n      indices, values = self._SparseTensor_3x50(np.int64, np.float64)\n      sp_output = sparse_ops.sparse_merge(indices, values, vocab_size)\n\n      output = self.evaluate(sp_output)\n      self._AssertResultsSorted(output, vocab_size)\n\n\nclass SparseRetainTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64),\n        np.array(val, np.int32), np.array(shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        to_retain = np.array([1, 0, 0, 1, 1, 0], dtype=np.bool_)\n        sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n        output = self.evaluate(sp_output)\n\n        self.assertAllEqual(output.indices, [[0, 0], [1, 4], [3, 2]])\n        self.assertAllEqual(output.values, [0, 14, 32])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testRetainNone(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.zeros((6,), dtype=np.bool_)\n      sp_output = sparse_ops.sparse_retain(sp_input, to_retain)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, np.array([]).reshape((0, 2)))\n      self.assertAllEqual(output.values, [])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n\n  def testMismatchedRetainShape(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_5x6()\n      to_retain = np.array([1, 0, 0, 1, 0], dtype=np.bool_)\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_retain(sp_input, to_retain)\n\n\nclass SparseResetShapeTest(test_util.TensorFlowTestCase):\n\n  _IND_2_5_6 = np.array(\n      [[0, 0, 0], [0, 1, 0], [0, 1, 3], [1, 1, 4], [1, 3, 2], [1, 3, 3]],\n      dtype=np.int64)\n  _VAL_2_5_6 = np.array([0, 10, 13, 14, 32, 33], dtype=np.int32)\n  _SHP_2_5_6 = np.array([2, 5, 6], dtype=np.int64)\n\n  def _SparseTensor_2x5x6(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(self._IND_2_5_6, dtypes.int64),\n        constant_op.constant(self._VAL_2_5_6, dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensor_2x5x6_Empty(self):\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(\n            np.empty(shape=[0, 3], dtype=np.int64), dtypes.int64),\n        constant_op.constant(np.empty(shape=[0], dtype=np.int32), dtypes.int32),\n        constant_op.constant(self._SHP_2_5_6, dtypes.int64))\n\n  def _SparseTensorValue_2x5x6(self):\n    return sparse_tensor.SparseTensorValue(self._IND_2_5_6, self._VAL_2_5_6,\n                                           self._SHP_2_5_6)\n\n  def testStaticShapeInfoPreservedWhenNewShapeIsProvidedAndStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 6, 7], dtype=np.int64)\n    sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n    self.assertAllEqual([3, 6, 7], sp_output.get_shape())\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testInputUnavailableInGraphConstructionOk(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensorValue_2x5x6()\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  @test_util.run_deprecated_v1\n  def testFeedInputUnavailableInGraphConstructionOk(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n      new_shape = np.array([3, 6, 7], dtype=np.int64)\n      sp_output = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      output = sess.run(sp_output,\n                        feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [3, 6, 7])\n\n  def testTightBoundingBox(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices, [[0, 0, 0], [0, 1, 0], [0, 1, 3],\n                                           [1, 1, 4], [1, 3, 2], [1, 3, 3]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14, 32, 33])\n      self.assertAllEqual(output.dense_shape, [2, 4, 5])\n\n  def testTightBoundingBoxEmpty(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6_Empty()\n      sp_output = sparse_ops.sparse_reset_shape(sp_input)\n\n      output = self.evaluate(sp_output)\n\n      self.assertAllEqual(output.indices.shape, [0, 3])\n      self.assertAllEqual(output.values.shape, [0])\n      self.assertAllEqual(output.dense_shape, [0, 0, 0])\n\n  def testInvalidRank(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = np.array([3, 7], dtype=np.int64)\n\n      with self.assertRaises(ValueError):\n        sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidRankNewShapeUnavailableInGraphConstruction(self):\n    with self.session(use_gpu=False) as sess:\n      new_shape = array_ops.placeholder(dtype=dtypes.int64)\n      sp_input = self._SparseTensor_2x5x6()\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x == y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: np.array([3, 7], dtype=np.int64)})\n\n  def testInvalidDimensionSizeStatic(self):\n    sp_input = self._SparseTensor_2x5x6()\n    new_shape = np.array([3, 7, 5], dtype=np.int64)\n\n    with self.assertRaisesRegex(ValueError, \"should have dimension sizes\"):\n      sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeDynamic(self):\n    with self.session(use_gpu=False) as sess:\n      sp_input = self._SparseTensor_2x5x6()\n      new_shape = array_ops.placeholder(dtype=dtypes.int32)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={new_shape: [3, 7, 5]})\n\n  @test_util.run_deprecated_v1\n  def testInvalidDimensionSizeInputUnavailableInGraphConstruction(self):\n    sp_input = array_ops.sparse_placeholder(dtype=dtypes.int32)\n    with self.session(use_gpu=False) as sess:\n      new_shape = np.array([3, 7, 5], dtype=np.int64)\n      out = sparse_ops.sparse_reset_shape(sp_input, new_shape)\n\n      with self.assertRaisesOpError(\"x <= y did not hold element-wise\"):\n        sess.run(out, feed_dict={sp_input: self._SparseTensorValue_2x5x6()})\n\n\nclass SparseFillEmptyRowsTest(test_util.TensorFlowTestCase):\n\n  def _SparseTensorValue_5x6(self, dtype=np.int32):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([0, 10, 13, 14, 32, 33])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensorValue(\n        np.array(ind, np.int64), np.array(val, dtype), np.array(\n            shape, np.int64))\n\n  def _SparseTensor_5x6(self):\n    return sparse_tensor.SparseTensor.from_value(self._SparseTensorValue_5x6())\n\n  def _SparseTensor_String5x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]])\n    val = np.array([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])\n    shape = np.array([5, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.string),\n        constant_op.constant(shape, dtypes.int64))\n\n  def _SparseTensor_2x6(self):\n    ind = np.array([[0, 0], [1, 0], [1, 3], [1, 4]])\n    val = np.array([0, 10, 13, 14])\n    shape = np.array([2, 6])\n    return sparse_tensor.SparseTensor(\n        constant_op.constant(ind, dtypes.int64),\n        constant_op.constant(val, dtypes.int32),\n        constant_op.constant(shape, dtypes.int64))\n\n  def testFillNumber(self):\n    with test_util.use_gpu():\n      for sp_input in (self._SparseTensorValue_5x6(), self._SparseTensor_5x6()):\n        sp_output, empty_row_indicator = (\n            sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n        output, empty_row_indicator_out = self.evaluate(\n            [sp_output, empty_row_indicator])\n\n        self.assertAllEqual(\n            output.indices,\n            [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n        self.assertAllEqual(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n        self.assertAllEqual(output.dense_shape, [5, 6])\n        self.assertAllEqual(empty_row_indicator_out,\n                            np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  @test_util.run_deprecated_v1\n  def testFillFloat(self):\n    with self.session():\n      values = constant_op.constant(\n          [0.0, 10.0, 13.0, 14.0, 32.0, 33.0], dtype=dtypes.float64)\n      default_value = constant_op.constant(-1.0, dtype=dtypes.float64)\n      sp_input = sparse_tensor.SparseTensorValue(\n          indices=np.array([[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]]),\n          values=values,\n          dense_shape=np.array([5, 6]))\n      sp_output, empty_row_indicator = (sparse_ops.sparse_fill_empty_rows(\n          sp_input, default_value))\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4],\n                                           [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllClose(output.values, [0, 10, 13, 14, -1, 32, 33, -1])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n      values_grad_err = gradient_checker.compute_gradient_error(\n          values, values.shape.as_list(), sp_output.values, [8], delta=1e-8)\n      self.assertGreater(values_grad_err, 0)\n      self.assertLess(values_grad_err, 1e-8)\n\n      default_value_grad_err = gradient_checker.compute_gradient_error(\n          default_value,\n          default_value.shape.as_list(),\n          sp_output.values, [8],\n          delta=1e-8)\n      self.assertGreater(default_value_grad_err, 0)\n      self.assertLess(default_value_grad_err, 1e-8)\n\n  def testFillString(self):\n    with test_util.force_cpu():\n      sp_input = self._SparseTensor_String5x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, \"\"))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(\n          output.indices,\n          [[0, 0], [1, 0], [1, 3], [1, 4], [2, 0], [3, 2], [3, 3], [4, 0]])\n      self.assertAllEqual(output.values,\n                          [b\"a\", b\"b\", b\"c\", b\"d\", b\"\", b\"e\", b\"f\", b\"\"])\n      self.assertAllEqual(output.dense_shape, [5, 6])\n      self.assertAllEqual(empty_row_indicator_out,\n                          np.array([0, 0, 1, 0, 1]).astype(np.bool_))\n\n  def testNoEmptyRows(self):\n    with test_util.use_gpu():\n      sp_input = self._SparseTensor_2x6()\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0], [1, 3], [1, 4]])\n      self.assertAllEqual(output.values, [0, 10, 13, 14])\n      self.assertAllEqual(output.dense_shape, [2, 6])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testNoEmptyRowsAndUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 1], [0, 3], [1, 2], [1, 3]])\n      self.assertAllEqual(output.values, [2, 4, 1, 3])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.zeros(2).astype(np.bool_))\n\n  def testUnordered(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[2, 3], [2, 2], [0, 1], [0, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([3, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices,\n                          [[0, 1], [0, 3], [1, 0], [2, 3], [2, 2]])\n      self.assertAllEqual(output.values, [2, 4, -1, 1, 3])\n      self.assertAllEqual(output.dense_shape, [3, 5])\n      self.assertAllEqual(empty_row_indicator_out, [False, True, False])\n\n  def testEmptyIndicesTensor(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([2, 5]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, [[0, 0], [1, 0]])\n      self.assertAllEqual(output.values, [-1, -1])\n      self.assertAllEqual(output.dense_shape, [2, 5])\n      self.assertAllEqual(empty_row_indicator_out, np.ones(2).astype(np.bool_))\n\n  def testEmptyOutput(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.ones([0, 2]),\n          values=np.ones([0]),\n          dense_shape=np.array([0, 3]))\n      sp_output, empty_row_indicator = (\n          sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n      output, empty_row_indicator_out = self.evaluate(\n          [sp_output, empty_row_indicator])\n\n      self.assertAllEqual(output.indices, np.ones([0, 2]))\n      self.assertAllEqual(output.values, np.ones([0]))\n      self.assertAllEqual(output.dense_shape, [0, 3])\n      self.assertAllEqual(empty_row_indicator_out, [])\n\n  def testInvalidIndices(self):\n    with test_util.use_gpu():\n      sp_input = sparse_tensor.SparseTensor(\n          indices=np.array([[1, 2], [1, 3], [99, 1], [99, 3]]),\n          values=np.array([1, 3, 2, 4]),\n          dense_shape=np.array([2, 5]))\n\n      with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                  r\"indices\\(2, 0\\) is invalid\"):\n        self.evaluate(sparse_ops.sparse_fill_empty_rows(sp_input, -1))\n\n\nclass SparseAddTest(test_util.TensorFlowTestCase):\n\n  def testValuesInVariable(self):\n    indices = constant_op.constant([[0]], dtype=dtypes.int64)\n    values = variables.Variable([1], trainable=False, dtype=dtypes.float32)\n    shape = constant_op.constant([1], dtype=dtypes.int64)\n\n    sp_input = sparse_tensor.SparseTensor(indices, values, shape)\n    sp_output = sparse_ops.sparse_add(sp_input, sp_input)\n\n    with test_util.force_cpu():\n      self.evaluate(variables.global_variables_initializer())\n      output = self.evaluate(sp_output)\n      self.assertAllEqual(output.values, [2])\n\n\nclass SparseReduceTest(test_util.TensorFlowTestCase):\n\n  # [[1, ?, 2]\n  #  [?, 3, ?]]\n  # where ? is implicitly-zero.\n  ind = np.array([[0, 0], [0, 2], [1, 1]]).astype(np.int64)\n  vals = np.array([1, 1, 1]).astype(np.int32)\n  dense_shape = np.array([2, 3]).astype(np.int64)\n\n  def _compare(self, sp_t, reduction_axes, ndims, keep_dims, do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_ans = densified\n    if reduction_axes is None:\n      if do_sum:\n        np_ans = np.sum(np_ans, keepdims=keep_dims)\n      else:\n        np_ans = np.max(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        if do_sum:\n          np_ans = np.sum(np_ans, axis=ra, keepdims=keep_dims)\n        else:\n          np_ans = np.max(np_ans, axis=ra, keepdims=keep_dims)\n\n    with self.cached_session():\n      if do_sum:\n        tf_dense_ans = sparse_ops.sparse_reduce_sum(sp_t, reduction_axes,\n                                                    keep_dims)\n      else:\n        tf_dense_ans = sparse_ops.sparse_reduce_max(sp_t, reduction_axes,\n                                                    keep_dims)\n      out_dense = self.evaluate(tf_dense_ans)\n\n      if do_sum:\n        tf_sparse_ans = sparse_ops.sparse_reduce_sum_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      else:\n        tf_sparse_ans = sparse_ops.sparse_reduce_max_sparse(sp_t,\n                                                            reduction_axes,\n                                                            keep_dims)\n      # Convert to dense for comparison purposes.\n      out_sparse = sparse_ops.sparse_tensor_to_dense(tf_sparse_ans)\n\n    self.assertAllClose(np_ans, out_dense)\n    self.assertAllClose(np_ans, out_sparse)\n\n  def _compare_all(self, sp_t, reduction_axes, ndims):\n    self._compare(sp_t, reduction_axes, ndims, False, False)\n    self._compare(sp_t, reduction_axes, ndims, False, True)\n    self._compare(sp_t, reduction_axes, ndims, True, False)\n    self._compare(sp_t, reduction_axes, ndims, True, True)\n\n  # (TODO:b/133851381): Re-enable this test.\n  def disabledtestSimpleAndRandomInputs(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      self._compare_all(sp_t, None, ndims=2)\n      self._compare_all(sp_t, 0, ndims=2)\n      self._compare_all(sp_t, [1], ndims=2)\n      self._compare_all(sp_t, [0, 1], ndims=2)\n      self._compare_all(sp_t, [1, 0], ndims=2)\n      self._compare_all(sp_t, [-1], ndims=2)\n      self._compare_all(sp_t, [1, -2], ndims=2)\n\n    np.random.seed(1618)\n    test_dims = [(1618, 1, 11, 7, 1), (1,), (1, 1, 1)]\n    with test_util.force_cpu():\n      for dims in test_dims:\n        sp_t, unused_nnz = _sparsify(np.random.randn(*dims))\n        # reduce all using None\n        self._compare_all(sp_t, None, ndims=len(dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          self._compare_all(sp_t, axes, ndims=len(dims))\n\n  def testInvalidAxes(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n    with test_util.force_cpu():\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_sum(sp_t, 2))\n      with self.assertRaisesOpError(\"Invalid reduction dimension -3\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, -3))\n      with self.assertRaisesOpError(\"Invalid reduction dimension 2\"):\n        self.evaluate(sparse_ops.sparse_reduce_max(sp_t, 2))\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    np.random.seed(8161)\n    test_dims = [(11, 1, 5, 7, 1), (2, 2)]\n    with self.session(use_gpu=False):\n      for dims in test_dims:\n        sp_t, nnz = _sparsify(np.random.randn(*dims))\n        # reduce random axes from 1D to N-D\n        for d in range(1, len(dims) + 1):\n          axes = np.random.choice(len(dims), size=d, replace=False).tolist()\n          reduced = sparse_ops.sparse_reduce_sum(sp_t, axes)\n\n          err = gradient_checker.compute_gradient_error(\n              sp_t.values, (nnz,), reduced,\n              self.evaluate(reduced).shape)\n          self.assertLess(err, 1e-3)\n\n        # Tests for negative axes.\n        reduced = sparse_ops.sparse_reduce_sum(sp_t, -1)\n        err = gradient_checker.compute_gradient_error(\n            sp_t.values, (nnz,), reduced,\n            self.evaluate(reduced).shape)\n        self.assertLess(err, 1e-3)\n\n  def _testSparseReduceShape(self, sp_t, reduction_axes, ndims, keep_dims,\n                             do_sum):\n    densified = self.evaluate(sparse_ops.sparse_tensor_to_dense(sp_t))\n\n    np_op = np.sum\n    tf_op = sparse_ops.sparse_reduce_sum\n    if not do_sum:\n      np_op = np.max\n      tf_op = sparse_ops.sparse_reduce_max\n\n    np_ans = densified\n    if reduction_axes is None:\n      np_ans = np_op(np_ans, keepdims=keep_dims)\n    else:\n      if not isinstance(reduction_axes, list):  # Single scalar.\n        reduction_axes = [reduction_axes]\n      reduction_axes = np.array(reduction_axes).astype(np.int32)\n      # Handles negative axes.\n      reduction_axes = (reduction_axes + ndims) % ndims\n      # Loop below depends on sorted.\n      reduction_axes.sort()\n      for ra in reduction_axes.ravel()[::-1]:\n        np_ans = np_op(np_ans, axis=ra, keepdims=keep_dims)\n\n    tf_ans = tf_op(sp_t, reduction_axes, keep_dims)\n    self.assertAllEqual(np_ans.shape, tf_ans.get_shape().as_list())\n\n  # (TODO:b/133851381): Re-enable this test\n  def disabledtestSparseReduceSumOrMaxShape(self):\n    sp_t = sparse_tensor.SparseTensor(self.ind, self.vals, self.dense_shape)\n\n    with test_util.force_cpu():\n      for do_sum in [True, False]:\n        for keep_dims in [True, False]:\n          self._testSparseReduceShape(sp_t, None, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, 0, 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [0, 1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, 0], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [-1], 2, keep_dims, do_sum)\n          self._testSparseReduceShape(sp_t, [1, -2], 2, keep_dims, do_sum)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_max_sparse(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_reduce_sum(\n            input_indices=[[1, 2], [3, 4]],\n            input_shape=[2**32, 2**31],\n            input_values=[1, 3],\n            reduction_axes=[0],\n            keep_dims=False,\n            name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMathOpsTest(test_util.TensorFlowTestCase):\n\n  def _check(self, result_tensor, result_np, input_sp_t):\n    self.assertTrue(isinstance(result_tensor, sparse_tensor.SparseTensor))\n    self.assertTrue(isinstance(input_sp_t, sparse_tensor.SparseTensor))\n    self.assertAllCloseAccordingToType(input_sp_t.indices,\n                                       result_tensor.indices)\n    self.assertAllCloseAccordingToType(input_sp_t.dense_shape,\n                                       result_tensor.dense_shape)\n\n    res_densified = sparse_ops.sparse_to_dense(\n        result_tensor.indices, result_tensor.dense_shape, result_tensor.values)\n    self.assertAllCloseAccordingToType(result_np, res_densified)\n\n  @test_util.run_deprecated_v1\n  def testCwiseShapeValidation(self):\n    # Test case for GitHub 24072.\n    with test_util.force_cpu():\n      a = array_ops.ones([3, 4, 1], dtype=dtypes.int32)\n      b = sparse_tensor.SparseTensor([[0, 0, 1, 0], [0, 0, 3, 0]], [10, 20],\n                                     [1, 1, 4, 2])\n      c = a * b\n      with self.assertRaisesRegex(\n          errors.InvalidArgumentError,\n          \"broadcasts dense to sparse only; got incompatible shapes\"):\n        self.evaluate(c)\n\n  def testCwiseDivAndMul(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with test_util.force_cpu():\n      for dtype in [np.float32, np.float64, np.int32, np.int64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, unused_nnz = _sparsify(sp_vals_np, thresh=1.5)\n          sp_t_densified = sparse_ops.sparse_tensor_to_dense(sp_t)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          self._check(sp_t / dense_t, sp_t_densified / dense_vals_np, sp_t)\n          # Check commutative.\n          self._check(sp_t * dense_t, sp_t_densified * dense_vals_np, sp_t)\n          self._check(dense_t * sp_t, sp_t_densified * dense_vals_np, sp_t)\n\n          if dtype in [np.int32, np.int64]:\n            res = sp_t / dense_t  # should invoke \"__truediv__\"\n            self.assertEqual(res.values.dtype, np.float64)\n\n  def testCwiseAdd(self):\n    with test_util.force_cpu():\n      # Identity(2) + AllOnes(2,2).  Should be equal to 2 * Identity(2).\n      indices = [[0, 0], [1, 1]]\n      vals = [1, 1]\n      shape = (2, 2)\n\n      sp_t = sparse_tensor.SparseTensor(indices, vals, shape)\n      dense_t = array_ops.ones(shape, dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n      # Variant of above, but broadcasts the dense side.\n      dense_t = array_ops.ones([1], dtype=dtypes.int32)\n      self._check(\n          sparse_ops.sparse_dense_cwise_add(sp_t, dense_t),\n          np.identity(2) * 2, sp_t)\n\n  @test_util.run_deprecated_v1\n  def testGradients(self):\n    np.random.seed(1618)\n    sp_shapes = [(10, 10, 10), (5, 5), (1618,), (3, 3, 7)]\n    dense_shapes = [(10, 10, 1), (5, 5), (1,), (1, 7)]\n\n    with self.session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        for sp_shape, dense_shape in zip(sp_shapes, dense_shapes):\n          sp_vals_np = np.random.rand(*sp_shape).astype(dtype) + 1\n          dense_vals_np = np.random.rand(*dense_shape).astype(dtype) + 1\n          sp_t, nnz = _sparsify(sp_vals_np, thresh=1.5)\n          dense_t = constant_op.constant(dense_vals_np)\n\n          cmul = sp_t * dense_t\n          err = gradient_checker.compute_gradient_error([sp_t.values, dense_t],\n                                                        [(nnz,), dense_shape],\n                                                        cmul.values, (nnz,))\n          self.assertLess(err, 1e-4)\n\n          cdiv = sp_t / dense_t\n          err = gradient_checker.compute_gradient_error(sp_t.values, (nnz,),\n                                                        cdiv.values, (nnz,))\n          self.assertLess(err, 1e-4)\n          err = gradient_checker.compute_gradient_error(\n              dense_t,\n              dense_shape,\n              cdiv.values, (nnz,),\n              x_init_value=dense_vals_np)\n          self.assertLess(err, 2e-4)\n\n\nclass SparseSoftmaxTest(test_util.TensorFlowTestCase):\n\n  @test_util.run_deprecated_v1\n  def testEquivalentToDensified(self):\n    np.random.seed(1618)\n    n, m = np.random.choice(20, size=2)\n\n    for dtype in [np.float32, np.float64]:\n      sp_vals_np = np.random.rand(n, m).astype(dtype)\n\n      batched_sp_t, unused_nnz1 = _sparsify(\n          sp_vals_np.reshape((1, n, m)), thresh=0.)  # No masking.\n\n      with test_util.force_cpu():\n        densified = constant_op.constant(sp_vals_np)\n\n        sp_result = self.evaluate(\n            sparse_ops.sparse_softmax(batched_sp_t)).values.reshape((n, m))\n        dense_result = nn_ops.softmax(densified)\n\n        self.assertAllClose(dense_result, sp_result)\n\n  def testHigherRanks(self):\n    # For the first shape:\n    # First batch:\n    # [?   e.]\n    # [1.  ? ]\n    # Second batch:\n    # [e   ? ]\n    # [e   e ]\n    #\n    # The softmax results should be:\n    # [?   1.]     [1    ?]\n    # [1.  ? ] and [.5  .5]\n    # where ? means implicitly zero.\n    #\n    # The second shape: same input data, but with a higher-rank shape.\n    shapes = [[2, 2, 2], [2, 1, 2, 2]]\n    for shape in shapes:\n      values = np.asarray(\n          [0., np.e, 1., 0., np.e, 0., np.e, np.e]).reshape(shape)\n      sp_t, unused_nnz = _sparsify(values, thresh=1e-2)\n      expected_values = [1., 1., 1., .5, .5]\n\n      with test_util.force_cpu():\n        result = sparse_ops.sparse_softmax(sp_t)\n\n        self.assertAllEqual(expected_values, result.values)\n        self.assertAllEqual(sp_t.indices, result.indices)\n        self.assertAllEqual(shape, result.dense_shape)\n\n  @test_util.run_deprecated_v1\n  def testGradient(self):\n    x_shape = [2, 5, 10]\n    with self.cached_session(use_gpu=False):\n      for dtype in [np.float32, np.float64]:\n        x_np = np.random.randn(*x_shape).astype(dtype)\n        x_tf, nnz = _sparsify(x_np)\n        y_tf = sparse_ops.sparse_softmax(x_tf)\n        err = gradient_checker.compute_gradient_error(x_tf.values, (nnz,),\n                                                      y_tf.values, (nnz,))\n        self.assertLess(err, 1e-4)\n\n  def testIntegerOverflow(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]],\n            sp_values=[2.0],\n            sp_shape=[2**32, 2**31],\n            name=None)\n\n        self.evaluate(res)\n\n  def testReshapeNegativeShape(self):\n    with self.cached_session(use_gpu=False):\n      with self.assertRaises(errors.InvalidArgumentError):\n        res = sparse_ops.gen_sparse_ops.sparse_softmax(\n            sp_indices=[[1, 1]], sp_values=[2.0], sp_shape=[-1, 1], name=None)\n\n        self.evaluate(res)\n\n\nclass SparseMinimumMaximumTest(test_util.TensorFlowTestCase):\n\n  def _assertSparseTensorValueEqual(self, a, b):\n    self.assertAllEqual(a.indices, b.indices)\n    self.assertAllEqual(a.values, b.values)\n    self.assertAllEqual(a.dense_shape, b.dense_shape)\n\n  def testBasic(self):\n    with test_util.force_cpu():\n      # 1-D, values at index 0.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_one)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_one)\n      self._assertSparseTensorValueEqual(sp_one, max_tf)\n      self._assertSparseTensorValueEqual(sp_zero, min_tf)\n\n      # Values at different indices.\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [7])\n      sp_zero_2 = sparse_tensor.SparseTensor([[1]], [0], [7])\n      expected = sparse_tensor.SparseTensor([[0], [1]], [0, 0], [7])\n      max_tf = sparse_ops.sparse_maximum(sp_zero, sp_zero_2)\n      min_tf = sparse_ops.sparse_minimum(sp_zero, sp_zero_2)\n      self._assertSparseTensorValueEqual(expected, max_tf)\n      self._assertSparseTensorValueEqual(expected, min_tf)\n\n  @test_util.run_deprecated_v1\n  def testRandom(self):\n    np.random.seed(1618)\n    shapes = [(13,), (6, 8), (1, 7, 1)]\n    for shape in shapes:\n      for dtype in [np.int32, np.int64, np.float16, np.float32, np.float64]:\n        a_np = np.random.randn(*shape).astype(dtype)\n        b_np = np.random.randn(*shape).astype(dtype)\n        sp_a, unused_a_nnz = _sparsify(a_np, thresh=-.5)\n        sp_b, unused_b_nnz = _sparsify(b_np, thresh=-.5)\n\n        with self.cached_session(use_gpu=False):\n          maximum_tf = sparse_ops.sparse_maximum(sp_a, sp_b)\n          maximum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              maximum_tf).eval()\n          minimum_tf = sparse_ops.sparse_minimum(sp_a, sp_b)\n          minimum_tf_densified = sparse_ops.sparse_tensor_to_dense(\n              minimum_tf).eval()\n\n          a_densified = sparse_ops.sparse_tensor_to_dense(sp_a).eval()\n          b_densified = sparse_ops.sparse_tensor_to_dense(sp_b).eval()\n\n        self.assertAllEqual(\n            np.maximum(a_densified, b_densified), maximum_tf_densified)\n        self.assertAllEqual(\n            np.minimum(a_densified, b_densified), minimum_tf_densified)\n\n  def testMismatchedShapes(self):\n    with test_util.force_cpu():\n      sp_zero = sparse_tensor.SparseTensor([[0, 0]], [0], [1, 1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands do not have the same ranks\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n      sp_zero = sparse_tensor.SparseTensor([[0]], [0], [1])\n      sp_one = sparse_tensor.SparseTensor([[0]], [1], [2])\n      with self.assertRaisesOpError(\"Operands' shapes do not match\"):\n        self.evaluate(sparse_ops.sparse_maximum(sp_zero, sp_one))\n\n\nclass SparseTransposeTest(test.TestCase):\n\n  def testTranspose(self):\n    if np.__version__ == \"1.13.0\":\n      self.skipTest(\"numpy 1.13.0 bug\")\n\n    with test_util.force_cpu():\n      np.random.seed(1618)\n      shapes = [np.random.randint(1, 10, size=rank) for rank in range(1, 6)]\n      for shape in shapes:\n        for dtype in [np.int32, np.int64, np.float32, np.float64]:\n          dn_input = np.random.randn(*shape).astype(dtype)\n          rank = self.evaluate(array_ops.rank(dn_input))\n          perm = np.random.choice(rank, rank, False)\n          sp_input, unused_a_nnz = _sparsify(dn_input)\n          sp_trans = sparse_ops.sparse_transpose(sp_input, perm=perm)\n          dn_trans = sparse_ops.sparse_tensor_to_dense(sp_trans)\n          expected_trans = array_ops.transpose(dn_input, perm=perm)\n          self.assertAllEqual(expected_trans.shape, sp_trans.get_shape())\n          self.assertAllEqual(dn_trans, expected_trans)\n\n\nclass SparsePlaceholderTest(test.TestCase):\n\n  @test_util.run_deprecated_v1\n  def testPlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(10, 47))\n    self.assertAllEqual([10, 47], foo.get_shape())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testPartialShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=(None, 47))\n    self.assertAllEqual([None, 47], foo.get_shape().as_list())\n    self.assertAllEqual([None, 2], foo.indices.get_shape().as_list())\n\n  @test_util.run_deprecated_v1\n  def testNoShapePlaceholder(self):\n    foo = array_ops.sparse_placeholder(dtypes.float32, shape=None)\n    self.assertAllEqual(None, foo.get_shape())\n    self.assertAllEqual([None, None], foo.indices.get_shape().as_list())\n\n\nif __name__ == \"__main__\":\n  googletest.main()\n"], "filenames": ["tensorflow/core/kernels/sparse_tensor_dense_add_op.cc", "tensorflow/python/kernel_tests/sparse_ops/sparse_add_op_test.py", "tensorflow/python/kernel_tests/sparse_ops/sparse_ops_test.py"], "buggy_code_start_loc": [20, 192, 668], "buggy_code_end_loc": [63, 208, 669], "fixing_code_start_loc": [21, 191, 668], "fixing_code_end_loc": [94, 245, 669], "type": "CWE-20", "message": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.raw_ops.SparseTensorDenseAdd` does not fully validate the input arguments. In this case, a reference gets bound to a `nullptr` during kernel execution. This is undefined behavior. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue.", "other": {"cve": {"id": "CVE-2022-29206", "sourceIdentifier": "security-advisories@github.com", "published": "2022-05-20T23:15:44.887", "lastModified": "2022-06-02T19:37:28.170", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. Prior to versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4, the implementation of `tf.raw_ops.SparseTensorDenseAdd` does not fully validate the input arguments. In this case, a reference gets bound to a `nullptr` during kernel execution. This is undefined behavior. Versions 2.9.0, 2.8.1, 2.7.2, and 2.6.4 contain a patch for this issue."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En versiones anteriores a 2.9.0, 2.8.1, 2.7.2 y 2.6.4, la implementaci\u00f3n de \"tf.raw_ops.SparseTensorDenseAdd\" no comprobaba completamente los argumentos de entrada. En este caso, una referencia es vinculada a un \"nullptr\" durante la ejecuci\u00f3n del kernel. Esto es un comportamiento no definido. Las versiones 2.9.0, 2.8.1, 2.7.2 y 2.6.4 contienen un parche para este problema"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}, {"lang": "en", "value": "CWE-476"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionEndExcluding": "2.6.4", "matchCriteriaId": "D9359D32-D090-44CF-AC43-2046084A28BB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.7.0", "versionEndExcluding": "2.7.2", "matchCriteriaId": "C4DFBF2D-5283-42F6-8800-D653BFA5CE82"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "A58EDA5C-66D6-46F1-962E-60AFB7C784A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "89522760-C2DF-400D-9624-626D8F160CBA"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:-:*:*:*:*:*:*", "matchCriteriaId": "E9EA1898-ACAA-4699-8BAE-54D62C1819FB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "130DE3C9-6842-456F-A259-BF8FF8457217"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.8.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "BBF2FCEF-989C-409D-9F4C-81418C65B972"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.9.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "9CFB1CFC-579D-4647-A472-6DE8BE1951DE"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.9.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "F3F3F37E-D27F-4060-830C-0AFF16150777"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/blob/f3b9bf4c3c0597563b289c0512e98d4ce81f886e/tensorflow/core/kernels/sparse_tensor_dense_add_op.cc", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/commit/11ced8467eccad9c7cb94867708be8fa5c66c730", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.6.4", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.7.2", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.8.1", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/releases/tag/v2.9.0", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-rc9w-5c64-9vqq", "source": "security-advisories@github.com", "tags": ["Exploit", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/11ced8467eccad9c7cb94867708be8fa5c66c730"}}