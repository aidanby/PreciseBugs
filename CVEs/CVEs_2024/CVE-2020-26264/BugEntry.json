{"buggy_code": ["// Copyright 2019 The go-ethereum Authors\n// This file is part of the go-ethereum library.\n//\n// The go-ethereum library is free software: you can redistribute it and/or modify\n// it under the terms of the GNU Lesser General Public License as published by\n// the Free Software Foundation, either version 3 of the License, or\n// (at your option) any later version.\n//\n// The go-ethereum library is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n// GNU Lesser General Public License for more details.\n//\n// You should have received a copy of the GNU Lesser General Public License\n// along with the go-ethereum library. If not, see <http://www.gnu.org/licenses/>.\n\npackage les\n\nimport (\n\t\"crypto/ecdsa\"\n\t\"encoding/binary\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/ethereum/go-ethereum/common\"\n\t\"github.com/ethereum/go-ethereum/common/mclock\"\n\t\"github.com/ethereum/go-ethereum/core\"\n\t\"github.com/ethereum/go-ethereum/core/rawdb\"\n\t\"github.com/ethereum/go-ethereum/core/state\"\n\t\"github.com/ethereum/go-ethereum/core/types\"\n\t\"github.com/ethereum/go-ethereum/ethdb\"\n\tlps \"github.com/ethereum/go-ethereum/les/lespay/server\"\n\t\"github.com/ethereum/go-ethereum/light\"\n\t\"github.com/ethereum/go-ethereum/log\"\n\t\"github.com/ethereum/go-ethereum/metrics\"\n\t\"github.com/ethereum/go-ethereum/p2p\"\n\t\"github.com/ethereum/go-ethereum/p2p/enode\"\n\t\"github.com/ethereum/go-ethereum/p2p/nodestate\"\n\t\"github.com/ethereum/go-ethereum/rlp\"\n\t\"github.com/ethereum/go-ethereum/trie\"\n)\n\nconst (\n\tsoftResponseLimit = 2 * 1024 * 1024 // Target maximum size of returned blocks, headers or node data.\n\testHeaderRlpSize  = 500             // Approximate size of an RLP encoded block header\n\tethVersion        = 63              // equivalent eth version for the downloader\n\n\tMaxHeaderFetch           = 192 // Amount of block headers to be fetched per retrieval request\n\tMaxBodyFetch             = 32  // Amount of block bodies to be fetched per retrieval request\n\tMaxReceiptFetch          = 128 // Amount of transaction receipts to allow fetching per request\n\tMaxCodeFetch             = 64  // Amount of contract codes to allow fetching per request\n\tMaxProofsFetch           = 64  // Amount of merkle proofs to be fetched per retrieval request\n\tMaxHelperTrieProofsFetch = 64  // Amount of helper tries to be fetched per retrieval request\n\tMaxTxSend                = 64  // Amount of transactions to be send per request\n\tMaxTxStatus              = 256 // Amount of transactions to queried per request\n)\n\nvar (\n\terrTooManyInvalidRequest = errors.New(\"too many invalid requests made\")\n\terrFullClientPool        = errors.New(\"client pool is full\")\n)\n\n// serverHandler is responsible for serving light client and process\n// all incoming light requests.\ntype serverHandler struct {\n\tblockchain *core.BlockChain\n\tchainDb    ethdb.Database\n\ttxpool     *core.TxPool\n\tserver     *LesServer\n\n\tcloseCh chan struct{}  // Channel used to exit all background routines of handler.\n\twg      sync.WaitGroup // WaitGroup used to track all background routines of handler.\n\tsynced  func() bool    // Callback function used to determine whether local node is synced.\n\n\t// Testing fields\n\taddTxsSync bool\n}\n\nfunc newServerHandler(server *LesServer, blockchain *core.BlockChain, chainDb ethdb.Database, txpool *core.TxPool, synced func() bool) *serverHandler {\n\thandler := &serverHandler{\n\t\tserver:     server,\n\t\tblockchain: blockchain,\n\t\tchainDb:    chainDb,\n\t\ttxpool:     txpool,\n\t\tcloseCh:    make(chan struct{}),\n\t\tsynced:     synced,\n\t}\n\treturn handler\n}\n\n// start starts the server handler.\nfunc (h *serverHandler) start() {\n\th.wg.Add(1)\n\tgo h.broadcastLoop()\n}\n\n// stop stops the server handler.\nfunc (h *serverHandler) stop() {\n\tclose(h.closeCh)\n\th.wg.Wait()\n}\n\n// runPeer is the p2p protocol run function for the given version.\nfunc (h *serverHandler) runPeer(version uint, p *p2p.Peer, rw p2p.MsgReadWriter) error {\n\tpeer := newClientPeer(int(version), h.server.config.NetworkId, p, newMeteredMsgWriter(rw, int(version)))\n\tdefer peer.close()\n\th.wg.Add(1)\n\tdefer h.wg.Done()\n\treturn h.handle(peer)\n}\n\nfunc (h *serverHandler) handle(p *clientPeer) error {\n\tp.Log().Debug(\"Light Ethereum peer connected\", \"name\", p.Name())\n\n\t// Execute the LES handshake\n\tvar (\n\t\thead   = h.blockchain.CurrentHeader()\n\t\thash   = head.Hash()\n\t\tnumber = head.Number.Uint64()\n\t\ttd     = h.blockchain.GetTd(hash, number)\n\t)\n\tif err := p.Handshake(td, hash, number, h.blockchain.Genesis().Hash(), h.server); err != nil {\n\t\tp.Log().Debug(\"Light Ethereum handshake failed\", \"err\", err)\n\t\treturn err\n\t}\n\t// Reject the duplicated peer, otherwise register it to peerset.\n\tvar registered bool\n\tif err := h.server.ns.Operation(func() {\n\t\tif h.server.ns.GetField(p.Node(), clientPeerField) != nil {\n\t\t\tregistered = true\n\t\t} else {\n\t\t\th.server.ns.SetFieldSub(p.Node(), clientPeerField, p)\n\t\t}\n\t}); err != nil {\n\t\treturn err\n\t}\n\tif registered {\n\t\treturn errAlreadyRegistered\n\t}\n\n\tdefer func() {\n\t\th.server.ns.SetField(p.Node(), clientPeerField, nil)\n\t\tif p.fcClient != nil { // is nil when connecting another server\n\t\t\tp.fcClient.Disconnect()\n\t\t}\n\t}()\n\tif p.server {\n\t\t// connected to another server, no messages expected, just wait for disconnection\n\t\t_, err := p.rw.ReadMsg()\n\t\treturn err\n\t}\n\t// Reject light clients if server is not synced.\n\t//\n\t// Put this checking here, so that \"non-synced\" les-server peers are still allowed\n\t// to keep the connection.\n\tif !h.synced() {\n\t\tp.Log().Debug(\"Light server not synced, rejecting peer\")\n\t\treturn p2p.DiscRequested\n\t}\n\t// Disconnect the inbound peer if it's rejected by clientPool\n\tif cap, err := h.server.clientPool.connect(p); cap != p.fcParams.MinRecharge || err != nil {\n\t\tp.Log().Debug(\"Light Ethereum peer rejected\", \"err\", errFullClientPool)\n\t\treturn errFullClientPool\n\t}\n\tp.balance, _ = h.server.ns.GetField(p.Node(), h.server.clientPool.BalanceField).(*lps.NodeBalance)\n\tif p.balance == nil {\n\t\treturn p2p.DiscRequested\n\t}\n\tactiveCount, _ := h.server.clientPool.pp.Active()\n\tclientConnectionGauge.Update(int64(activeCount))\n\n\tvar wg sync.WaitGroup // Wait group used to track all in-flight task routines.\n\n\tconnectedAt := mclock.Now()\n\tdefer func() {\n\t\twg.Wait() // Ensure all background task routines have exited.\n\t\th.server.clientPool.disconnect(p)\n\t\tp.balance = nil\n\t\tactiveCount, _ := h.server.clientPool.pp.Active()\n\t\tclientConnectionGauge.Update(int64(activeCount))\n\t\tconnectionTimer.Update(time.Duration(mclock.Now() - connectedAt))\n\t}()\n\t// Mark the peer starts to be served.\n\tatomic.StoreUint32(&p.serving, 1)\n\tdefer atomic.StoreUint32(&p.serving, 0)\n\n\t// Spawn a main loop to handle all incoming messages.\n\tfor {\n\t\tselect {\n\t\tcase err := <-p.errCh:\n\t\t\tp.Log().Debug(\"Failed to send light ethereum response\", \"err\", err)\n\t\t\treturn err\n\t\tdefault:\n\t\t}\n\t\tif err := h.handleMsg(p, &wg); err != nil {\n\t\t\tp.Log().Debug(\"Light Ethereum message handling failed\", \"err\", err)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\n// handleMsg is invoked whenever an inbound message is received from a remote\n// peer. The remote connection is torn down upon returning any error.\nfunc (h *serverHandler) handleMsg(p *clientPeer, wg *sync.WaitGroup) error {\n\t// Read the next message from the remote peer, and ensure it's fully consumed\n\tmsg, err := p.rw.ReadMsg()\n\tif err != nil {\n\t\treturn err\n\t}\n\tp.Log().Trace(\"Light Ethereum message arrived\", \"code\", msg.Code, \"bytes\", msg.Size)\n\n\t// Discard large message which exceeds the limitation.\n\tif msg.Size > ProtocolMaxMsgSize {\n\t\tclientErrorMeter.Mark(1)\n\t\treturn errResp(ErrMsgTooLarge, \"%v > %v\", msg.Size, ProtocolMaxMsgSize)\n\t}\n\tdefer msg.Discard()\n\n\tvar (\n\t\tmaxCost uint64\n\t\ttask    *servingTask\n\t)\n\tp.responseCount++\n\tresponseCount := p.responseCount\n\t// accept returns an indicator whether the request can be served.\n\t// If so, deduct the max cost from the flow control buffer.\n\taccept := func(reqID, reqCnt, maxCnt uint64) bool {\n\t\t// Short circuit if the peer is already frozen or the request is invalid.\n\t\tinSizeCost := h.server.costTracker.realCost(0, msg.Size, 0)\n\t\tif p.isFrozen() || reqCnt == 0 || reqCnt > maxCnt {\n\t\t\tp.fcClient.OneTimeCost(inSizeCost)\n\t\t\treturn false\n\t\t}\n\t\t// Prepaid max cost units before request been serving.\n\t\tmaxCost = p.fcCosts.getMaxCost(msg.Code, reqCnt)\n\t\taccepted, bufShort, priority := p.fcClient.AcceptRequest(reqID, responseCount, maxCost)\n\t\tif !accepted {\n\t\t\tp.freeze()\n\t\t\tp.Log().Error(\"Request came too early\", \"remaining\", common.PrettyDuration(time.Duration(bufShort*1000000/p.fcParams.MinRecharge)))\n\t\t\tp.fcClient.OneTimeCost(inSizeCost)\n\t\t\treturn false\n\t\t}\n\t\t// Create a multi-stage task, estimate the time it takes for the task to\n\t\t// execute, and cache it in the request service queue.\n\t\tfactor := h.server.costTracker.globalFactor()\n\t\tif factor < 0.001 {\n\t\t\tfactor = 1\n\t\t\tp.Log().Error(\"Invalid global cost factor\", \"factor\", factor)\n\t\t}\n\t\tmaxTime := uint64(float64(maxCost) / factor)\n\t\ttask = h.server.servingQueue.newTask(p, maxTime, priority)\n\t\tif task.start() {\n\t\t\treturn true\n\t\t}\n\t\tp.fcClient.RequestProcessed(reqID, responseCount, maxCost, inSizeCost)\n\t\treturn false\n\t}\n\t// sendResponse sends back the response and updates the flow control statistic.\n\tsendResponse := func(reqID, amount uint64, reply *reply, servingTime uint64) {\n\t\tp.responseLock.Lock()\n\t\tdefer p.responseLock.Unlock()\n\n\t\t// Short circuit if the client is already frozen.\n\t\tif p.isFrozen() {\n\t\t\trealCost := h.server.costTracker.realCost(servingTime, msg.Size, 0)\n\t\t\tp.fcClient.RequestProcessed(reqID, responseCount, maxCost, realCost)\n\t\t\treturn\n\t\t}\n\t\t// Positive correction buffer value with real cost.\n\t\tvar replySize uint32\n\t\tif reply != nil {\n\t\t\treplySize = reply.size()\n\t\t}\n\t\tvar realCost uint64\n\t\tif h.server.costTracker.testing {\n\t\t\trealCost = maxCost // Assign a fake cost for testing purpose\n\t\t} else {\n\t\t\trealCost = h.server.costTracker.realCost(servingTime, msg.Size, replySize)\n\t\t\tif realCost > maxCost {\n\t\t\t\trealCost = maxCost\n\t\t\t}\n\t\t}\n\t\tbv := p.fcClient.RequestProcessed(reqID, responseCount, maxCost, realCost)\n\t\tif amount != 0 {\n\t\t\t// Feed cost tracker request serving statistic.\n\t\t\th.server.costTracker.updateStats(msg.Code, amount, servingTime, realCost)\n\t\t\t// Reduce priority \"balance\" for the specific peer.\n\t\t\tp.balance.RequestServed(realCost)\n\t\t}\n\t\tif reply != nil {\n\t\t\tp.queueSend(func() {\n\t\t\t\tif err := reply.send(bv); err != nil {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase p.errCh <- err:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t}\n\tswitch msg.Code {\n\tcase GetBlockHeadersMsg:\n\t\tp.Log().Trace(\"Received block header request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInHeaderPacketsMeter.Mark(1)\n\t\t\tmiscInHeaderTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID uint64\n\t\t\tQuery getBlockHeadersData\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"%v: %v\", msg, err)\n\t\t}\n\t\tquery := req.Query\n\t\tif accept(req.ReqID, query.Amount, MaxHeaderFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\thashMode := query.Origin.Hash != (common.Hash{})\n\t\t\t\tfirst := true\n\t\t\t\tmaxNonCanonical := uint64(100)\n\n\t\t\t\t// Gather headers until the fetch or network limits is reached\n\t\t\t\tvar (\n\t\t\t\t\tbytes   common.StorageSize\n\t\t\t\t\theaders []*types.Header\n\t\t\t\t\tunknown bool\n\t\t\t\t)\n\t\t\t\tfor !unknown && len(headers) < int(query.Amount) && bytes < softResponseLimit {\n\t\t\t\t\tif !first && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// Retrieve the next header satisfying the query\n\t\t\t\t\tvar origin *types.Header\n\t\t\t\t\tif hashMode {\n\t\t\t\t\t\tif first {\n\t\t\t\t\t\t\torigin = h.blockchain.GetHeaderByHash(query.Origin.Hash)\n\t\t\t\t\t\t\tif origin != nil {\n\t\t\t\t\t\t\t\tquery.Origin.Number = origin.Number.Uint64()\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\torigin = h.blockchain.GetHeader(query.Origin.Hash, query.Origin.Number)\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\torigin = h.blockchain.GetHeaderByNumber(query.Origin.Number)\n\t\t\t\t\t}\n\t\t\t\t\tif origin == nil {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\theaders = append(headers, origin)\n\t\t\t\t\tbytes += estHeaderRlpSize\n\n\t\t\t\t\t// Advance to the next header of the query\n\t\t\t\t\tswitch {\n\t\t\t\t\tcase hashMode && query.Reverse:\n\t\t\t\t\t\t// Hash based traversal towards the genesis block\n\t\t\t\t\t\tancestor := query.Skip + 1\n\t\t\t\t\t\tif ancestor == 0 {\n\t\t\t\t\t\t\tunknown = true\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tquery.Origin.Hash, query.Origin.Number = h.blockchain.GetAncestor(query.Origin.Hash, query.Origin.Number, ancestor, &maxNonCanonical)\n\t\t\t\t\t\t\tunknown = query.Origin.Hash == common.Hash{}\n\t\t\t\t\t\t}\n\t\t\t\t\tcase hashMode && !query.Reverse:\n\t\t\t\t\t\t// Hash based traversal towards the leaf block\n\t\t\t\t\t\tvar (\n\t\t\t\t\t\t\tcurrent = origin.Number.Uint64()\n\t\t\t\t\t\t\tnext    = current + query.Skip + 1\n\t\t\t\t\t\t)\n\t\t\t\t\t\tif next <= current {\n\t\t\t\t\t\t\tinfos, _ := json.MarshalIndent(p.Peer.Info(), \"\", \"  \")\n\t\t\t\t\t\t\tp.Log().Warn(\"GetBlockHeaders skip overflow attack\", \"current\", current, \"skip\", query.Skip, \"next\", next, \"attacker\", infos)\n\t\t\t\t\t\t\tunknown = true\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tif header := h.blockchain.GetHeaderByNumber(next); header != nil {\n\t\t\t\t\t\t\t\tnextHash := header.Hash()\n\t\t\t\t\t\t\t\texpOldHash, _ := h.blockchain.GetAncestor(nextHash, next, query.Skip+1, &maxNonCanonical)\n\t\t\t\t\t\t\t\tif expOldHash == query.Origin.Hash {\n\t\t\t\t\t\t\t\t\tquery.Origin.Hash, query.Origin.Number = nextHash, next\n\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\tunknown = true\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tunknown = true\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\tcase query.Reverse:\n\t\t\t\t\t\t// Number based traversal towards the genesis block\n\t\t\t\t\t\tif query.Origin.Number >= query.Skip+1 {\n\t\t\t\t\t\t\tquery.Origin.Number -= query.Skip + 1\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tunknown = true\n\t\t\t\t\t\t}\n\n\t\t\t\t\tcase !query.Reverse:\n\t\t\t\t\t\t// Number based traversal towards the leaf block\n\t\t\t\t\t\tquery.Origin.Number += query.Skip + 1\n\t\t\t\t\t}\n\t\t\t\t\tfirst = false\n\t\t\t\t}\n\t\t\t\treply := p.replyBlockHeaders(req.ReqID, headers)\n\t\t\t\tsendResponse(req.ReqID, query.Amount, reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutHeaderPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutHeaderTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeHeaderTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetBlockBodiesMsg:\n\t\tp.Log().Trace(\"Received block bodies request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInBodyPacketsMeter.Mark(1)\n\t\t\tmiscInBodyTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID  uint64\n\t\t\tHashes []common.Hash\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\tvar (\n\t\t\tbytes  int\n\t\t\tbodies []rlp.RawValue\n\t\t)\n\t\treqCnt := len(req.Hashes)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxBodyFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tfor i, hash := range req.Hashes {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tif bytes >= softResponseLimit {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\tbody := h.blockchain.GetBodyRLP(hash)\n\t\t\t\t\tif body == nil {\n\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tbodies = append(bodies, body)\n\t\t\t\t\tbytes += len(body)\n\t\t\t\t}\n\t\t\t\treply := p.replyBlockBodiesRLP(req.ReqID, bodies)\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutBodyPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutBodyTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeBodyTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetCodeMsg:\n\t\tp.Log().Trace(\"Received code request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInCodePacketsMeter.Mark(1)\n\t\t\tmiscInCodeTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID uint64\n\t\t\tReqs  []CodeReq\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\tvar (\n\t\t\tbytes int\n\t\t\tdata  [][]byte\n\t\t)\n\t\treqCnt := len(req.Reqs)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxCodeFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tfor i, request := range req.Reqs {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// Look up the root hash belonging to the request\n\t\t\t\t\theader := h.blockchain.GetHeaderByHash(request.BHash)\n\t\t\t\t\tif header == nil {\n\t\t\t\t\t\tp.Log().Warn(\"Failed to retrieve associate header for code\", \"hash\", request.BHash)\n\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// Refuse to search stale state data in the database since looking for\n\t\t\t\t\t// a non-exist key is kind of expensive.\n\t\t\t\t\tlocal := h.blockchain.CurrentHeader().Number.Uint64()\n\t\t\t\t\tif !h.server.archiveMode && header.Number.Uint64()+core.TriesInMemory <= local {\n\t\t\t\t\t\tp.Log().Debug(\"Reject stale code request\", \"number\", header.Number.Uint64(), \"head\", local)\n\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\ttriedb := h.blockchain.StateCache().TrieDB()\n\n\t\t\t\t\taccount, err := h.getAccount(triedb, header.Root, common.BytesToHash(request.AccKey))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tp.Log().Warn(\"Failed to retrieve account for code\", \"block\", header.Number, \"hash\", header.Hash(), \"account\", common.BytesToHash(request.AccKey), \"err\", err)\n\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tcode, err := h.blockchain.StateCache().ContractCode(common.BytesToHash(request.AccKey), common.BytesToHash(account.CodeHash))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tp.Log().Warn(\"Failed to retrieve account code\", \"block\", header.Number, \"hash\", header.Hash(), \"account\", common.BytesToHash(request.AccKey), \"codehash\", common.BytesToHash(account.CodeHash), \"err\", err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// Accumulate the code and abort if enough data was retrieved\n\t\t\t\t\tdata = append(data, code)\n\t\t\t\t\tif bytes += len(code); bytes >= softResponseLimit {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treply := p.replyCode(req.ReqID, data)\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutCodePacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutCodeTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeCodeTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetReceiptsMsg:\n\t\tp.Log().Trace(\"Received receipts request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInReceiptPacketsMeter.Mark(1)\n\t\t\tmiscInReceiptTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID  uint64\n\t\t\tHashes []common.Hash\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\tvar (\n\t\t\tbytes    int\n\t\t\treceipts []rlp.RawValue\n\t\t)\n\t\treqCnt := len(req.Hashes)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxReceiptFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tfor i, hash := range req.Hashes {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tif bytes >= softResponseLimit {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\t// Retrieve the requested block's receipts, skipping if unknown to us\n\t\t\t\t\tresults := h.blockchain.GetReceiptsByHash(hash)\n\t\t\t\t\tif results == nil {\n\t\t\t\t\t\tif header := h.blockchain.GetHeaderByHash(hash); header == nil || header.ReceiptHash != types.EmptyRootHash {\n\t\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// If known, encode and queue for response packet\n\t\t\t\t\tif encoded, err := rlp.EncodeToBytes(results); err != nil {\n\t\t\t\t\t\tlog.Error(\"Failed to encode receipt\", \"err\", err)\n\t\t\t\t\t} else {\n\t\t\t\t\t\treceipts = append(receipts, encoded)\n\t\t\t\t\t\tbytes += len(encoded)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treply := p.replyReceiptsRLP(req.ReqID, receipts)\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutReceiptPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutReceiptTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeReceiptTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetProofsV2Msg:\n\t\tp.Log().Trace(\"Received les/2 proofs request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInTrieProofPacketsMeter.Mark(1)\n\t\t\tmiscInTrieProofTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID uint64\n\t\t\tReqs  []ProofReq\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\t// Gather state data until the fetch or network limits is reached\n\t\tvar (\n\t\t\tlastBHash common.Hash\n\t\t\troot      common.Hash\n\t\t)\n\t\treqCnt := len(req.Reqs)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxProofsFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tnodes := light.NewNodeSet()\n\n\t\t\t\tfor i, request := range req.Reqs {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// Look up the root hash belonging to the request\n\t\t\t\t\tvar (\n\t\t\t\t\t\theader *types.Header\n\t\t\t\t\t\ttrie   state.Trie\n\t\t\t\t\t)\n\t\t\t\t\tif request.BHash != lastBHash {\n\t\t\t\t\t\troot, lastBHash = common.Hash{}, request.BHash\n\n\t\t\t\t\t\tif header = h.blockchain.GetHeaderByHash(request.BHash); header == nil {\n\t\t\t\t\t\t\tp.Log().Warn(\"Failed to retrieve header for proof\", \"hash\", request.BHash)\n\t\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Refuse to search stale state data in the database since looking for\n\t\t\t\t\t\t// a non-exist key is kind of expensive.\n\t\t\t\t\t\tlocal := h.blockchain.CurrentHeader().Number.Uint64()\n\t\t\t\t\t\tif !h.server.archiveMode && header.Number.Uint64()+core.TriesInMemory <= local {\n\t\t\t\t\t\t\tp.Log().Debug(\"Reject stale trie request\", \"number\", header.Number.Uint64(), \"head\", local)\n\t\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\troot = header.Root\n\t\t\t\t\t}\n\t\t\t\t\t// If a header lookup failed (non existent), ignore subsequent requests for the same header\n\t\t\t\t\tif root == (common.Hash{}) {\n\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// Open the account or storage trie for the request\n\t\t\t\t\tstatedb := h.blockchain.StateCache()\n\n\t\t\t\t\tswitch len(request.AccKey) {\n\t\t\t\t\tcase 0:\n\t\t\t\t\t\t// No account key specified, open an account trie\n\t\t\t\t\t\ttrie, err = statedb.OpenTrie(root)\n\t\t\t\t\t\tif trie == nil || err != nil {\n\t\t\t\t\t\t\tp.Log().Warn(\"Failed to open storage trie for proof\", \"block\", header.Number, \"hash\", header.Hash(), \"root\", root, \"err\", err)\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\tdefault:\n\t\t\t\t\t\t// Account key specified, open a storage trie\n\t\t\t\t\t\taccount, err := h.getAccount(statedb.TrieDB(), root, common.BytesToHash(request.AccKey))\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tp.Log().Warn(\"Failed to retrieve account for proof\", \"block\", header.Number, \"hash\", header.Hash(), \"account\", common.BytesToHash(request.AccKey), \"err\", err)\n\t\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\ttrie, err = statedb.OpenStorageTrie(common.BytesToHash(request.AccKey), account.Root)\n\t\t\t\t\t\tif trie == nil || err != nil {\n\t\t\t\t\t\t\tp.Log().Warn(\"Failed to open storage trie for proof\", \"block\", header.Number, \"hash\", header.Hash(), \"account\", common.BytesToHash(request.AccKey), \"root\", account.Root, \"err\", err)\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// Prove the user's request from the account or stroage trie\n\t\t\t\t\tif err := trie.Prove(request.Key, request.FromLevel, nodes); err != nil {\n\t\t\t\t\t\tp.Log().Warn(\"Failed to prove state request\", \"block\", header.Number, \"hash\", header.Hash(), \"err\", err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif nodes.DataSize() >= softResponseLimit {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treply := p.replyProofsV2(req.ReqID, nodes.NodeList())\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutTrieProofPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutTrieProofTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeTrieProofTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetHelperTrieProofsMsg:\n\t\tp.Log().Trace(\"Received helper trie proof request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInHelperTriePacketsMeter.Mark(1)\n\t\t\tmiscInHelperTrieTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID uint64\n\t\t\tReqs  []HelperTrieReq\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\t// Gather state data until the fetch or network limits is reached\n\t\tvar (\n\t\t\tauxBytes int\n\t\t\tauxData  [][]byte\n\t\t)\n\t\treqCnt := len(req.Reqs)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxHelperTrieProofsFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tvar (\n\t\t\t\t\tlastIdx  uint64\n\t\t\t\t\tlastType uint\n\t\t\t\t\troot     common.Hash\n\t\t\t\t\tauxTrie  *trie.Trie\n\t\t\t\t)\n\t\t\t\tnodes := light.NewNodeSet()\n\t\t\t\tfor i, request := range req.Reqs {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tif auxTrie == nil || request.Type != lastType || request.TrieIdx != lastIdx {\n\t\t\t\t\t\tauxTrie, lastType, lastIdx = nil, request.Type, request.TrieIdx\n\n\t\t\t\t\t\tvar prefix string\n\t\t\t\t\t\tif root, prefix = h.getHelperTrie(request.Type, request.TrieIdx); root != (common.Hash{}) {\n\t\t\t\t\t\t\tauxTrie, _ = trie.New(root, trie.NewDatabase(rawdb.NewTable(h.chainDb, prefix)))\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif request.AuxReq == auxRoot {\n\t\t\t\t\t\tvar data []byte\n\t\t\t\t\t\tif root != (common.Hash{}) {\n\t\t\t\t\t\t\tdata = root[:]\n\t\t\t\t\t\t}\n\t\t\t\t\t\tauxData = append(auxData, data)\n\t\t\t\t\t\tauxBytes += len(data)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tif auxTrie != nil {\n\t\t\t\t\t\t\tauxTrie.Prove(request.Key, request.FromLevel, nodes)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif request.AuxReq != 0 {\n\t\t\t\t\t\t\tdata := h.getAuxiliaryHeaders(request)\n\t\t\t\t\t\t\tauxData = append(auxData, data)\n\t\t\t\t\t\t\tauxBytes += len(data)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif nodes.DataSize()+auxBytes >= softResponseLimit {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treply := p.replyHelperTrieProofs(req.ReqID, HelperTrieResps{Proofs: nodes.NodeList(), AuxData: auxData})\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutHelperTriePacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutHelperTrieTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeHelperTrieTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase SendTxV2Msg:\n\t\tp.Log().Trace(\"Received new transactions\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInTxsPacketsMeter.Mark(1)\n\t\t\tmiscInTxsTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID uint64\n\t\t\tTxs   []*types.Transaction\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\treqCnt := len(req.Txs)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxTxSend) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tstats := make([]light.TxStatus, len(req.Txs))\n\t\t\t\tfor i, tx := range req.Txs {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\thash := tx.Hash()\n\t\t\t\t\tstats[i] = h.txStatus(hash)\n\t\t\t\t\tif stats[i].Status == core.TxStatusUnknown {\n\t\t\t\t\t\taddFn := h.txpool.AddRemotes\n\t\t\t\t\t\t// Add txs synchronously for testing purpose\n\t\t\t\t\t\tif h.addTxsSync {\n\t\t\t\t\t\t\taddFn = h.txpool.AddRemotesSync\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif errs := addFn([]*types.Transaction{tx}); errs[0] != nil {\n\t\t\t\t\t\t\tstats[i].Error = errs[0].Error()\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\tstats[i] = h.txStatus(hash)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treply := p.replyTxStatus(req.ReqID, stats)\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutTxsPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutTxsTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeTxTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetTxStatusMsg:\n\t\tp.Log().Trace(\"Received transaction status query request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInTxStatusPacketsMeter.Mark(1)\n\t\t\tmiscInTxStatusTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID  uint64\n\t\t\tHashes []common.Hash\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\treqCnt := len(req.Hashes)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxTxStatus) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tstats := make([]light.TxStatus, len(req.Hashes))\n\t\t\t\tfor i, hash := range req.Hashes {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tstats[i] = h.txStatus(hash)\n\t\t\t\t}\n\t\t\t\treply := p.replyTxStatus(req.ReqID, stats)\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutTxStatusPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutTxStatusTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeTxStatusTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tdefault:\n\t\tp.Log().Trace(\"Received invalid message\", \"code\", msg.Code)\n\t\tclientErrorMeter.Mark(1)\n\t\treturn errResp(ErrInvalidMsgCode, \"%v\", msg.Code)\n\t}\n\t// If the client has made too much invalid request(e.g. request a non-existent data),\n\t// reject them to prevent SPAM attack.\n\tif p.getInvalid() > maxRequestErrors {\n\t\tclientErrorMeter.Mark(1)\n\t\treturn errTooManyInvalidRequest\n\t}\n\treturn nil\n}\n\n// getAccount retrieves an account from the state based on root.\nfunc (h *serverHandler) getAccount(triedb *trie.Database, root, hash common.Hash) (state.Account, error) {\n\ttrie, err := trie.New(root, triedb)\n\tif err != nil {\n\t\treturn state.Account{}, err\n\t}\n\tblob, err := trie.TryGet(hash[:])\n\tif err != nil {\n\t\treturn state.Account{}, err\n\t}\n\tvar account state.Account\n\tif err = rlp.DecodeBytes(blob, &account); err != nil {\n\t\treturn state.Account{}, err\n\t}\n\treturn account, nil\n}\n\n// getHelperTrie returns the post-processed trie root for the given trie ID and section index\nfunc (h *serverHandler) getHelperTrie(typ uint, index uint64) (common.Hash, string) {\n\tswitch typ {\n\tcase htCanonical:\n\t\tsectionHead := rawdb.ReadCanonicalHash(h.chainDb, (index+1)*h.server.iConfig.ChtSize-1)\n\t\treturn light.GetChtRoot(h.chainDb, index, sectionHead), light.ChtTablePrefix\n\tcase htBloomBits:\n\t\tsectionHead := rawdb.ReadCanonicalHash(h.chainDb, (index+1)*h.server.iConfig.BloomTrieSize-1)\n\t\treturn light.GetBloomTrieRoot(h.chainDb, index, sectionHead), light.BloomTrieTablePrefix\n\t}\n\treturn common.Hash{}, \"\"\n}\n\n// getAuxiliaryHeaders returns requested auxiliary headers for the CHT request.\nfunc (h *serverHandler) getAuxiliaryHeaders(req HelperTrieReq) []byte {\n\tif req.Type == htCanonical && req.AuxReq == auxHeader && len(req.Key) == 8 {\n\t\tblockNum := binary.BigEndian.Uint64(req.Key)\n\t\thash := rawdb.ReadCanonicalHash(h.chainDb, blockNum)\n\t\treturn rawdb.ReadHeaderRLP(h.chainDb, hash, blockNum)\n\t}\n\treturn nil\n}\n\n// txStatus returns the status of a specified transaction.\nfunc (h *serverHandler) txStatus(hash common.Hash) light.TxStatus {\n\tvar stat light.TxStatus\n\t// Looking the transaction in txpool first.\n\tstat.Status = h.txpool.Status([]common.Hash{hash})[0]\n\n\t// If the transaction is unknown to the pool, try looking it up locally.\n\tif stat.Status == core.TxStatusUnknown {\n\t\tlookup := h.blockchain.GetTransactionLookup(hash)\n\t\tif lookup != nil {\n\t\t\tstat.Status = core.TxStatusIncluded\n\t\t\tstat.Lookup = lookup\n\t\t}\n\t}\n\treturn stat\n}\n\n// broadcastLoop broadcasts new block information to all connected light\n// clients. According to the agreement between client and server, server should\n// only broadcast new announcement if the total difficulty is higher than the\n// last one. Besides server will add the signature if client requires.\nfunc (h *serverHandler) broadcastLoop() {\n\tdefer h.wg.Done()\n\n\theadCh := make(chan core.ChainHeadEvent, 10)\n\theadSub := h.blockchain.SubscribeChainHeadEvent(headCh)\n\tdefer headSub.Unsubscribe()\n\n\tvar (\n\t\tlastHead *types.Header\n\t\tlastTd   = common.Big0\n\t)\n\tfor {\n\t\tselect {\n\t\tcase ev := <-headCh:\n\t\t\theader := ev.Block.Header()\n\t\t\thash, number := header.Hash(), header.Number.Uint64()\n\t\t\ttd := h.blockchain.GetTd(hash, number)\n\t\t\tif td == nil || td.Cmp(lastTd) <= 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvar reorg uint64\n\t\t\tif lastHead != nil {\n\t\t\t\treorg = lastHead.Number.Uint64() - rawdb.FindCommonAncestor(h.chainDb, header, lastHead).Number.Uint64()\n\t\t\t}\n\t\t\tlastHead, lastTd = header, td\n\t\t\tlog.Debug(\"Announcing block to peers\", \"number\", number, \"hash\", hash, \"td\", td, \"reorg\", reorg)\n\t\t\th.server.broadcaster.broadcast(announceData{Hash: hash, Number: number, Td: td, ReorgDepth: reorg})\n\t\tcase <-h.closeCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// broadcaster sends new header announcements to active client peers\ntype broadcaster struct {\n\tns                           *nodestate.NodeStateMachine\n\tprivateKey                   *ecdsa.PrivateKey\n\tlastAnnounce, signedAnnounce announceData\n}\n\n// newBroadcaster creates a new broadcaster\nfunc newBroadcaster(ns *nodestate.NodeStateMachine) *broadcaster {\n\tb := &broadcaster{ns: ns}\n\tns.SubscribeState(priorityPoolSetup.ActiveFlag, func(node *enode.Node, oldState, newState nodestate.Flags) {\n\t\tif newState.Equals(priorityPoolSetup.ActiveFlag) {\n\t\t\t// send last announcement to activated peers\n\t\t\tb.sendTo(node)\n\t\t}\n\t})\n\treturn b\n}\n\n// setSignerKey sets the signer key for signed announcements. Should be called before\n// starting the protocol handler.\nfunc (b *broadcaster) setSignerKey(privateKey *ecdsa.PrivateKey) {\n\tb.privateKey = privateKey\n}\n\n// broadcast sends the given announcements to all active peers\nfunc (b *broadcaster) broadcast(announce announceData) {\n\tb.ns.Operation(func() {\n\t\t// iterate in an Operation to ensure that the active set does not change while iterating\n\t\tb.lastAnnounce = announce\n\t\tb.ns.ForEach(priorityPoolSetup.ActiveFlag, nodestate.Flags{}, func(node *enode.Node, state nodestate.Flags) {\n\t\t\tb.sendTo(node)\n\t\t})\n\t})\n}\n\n// sendTo sends the most recent announcement to the given node unless the same or higher Td\n// announcement has already been sent.\nfunc (b *broadcaster) sendTo(node *enode.Node) {\n\tif b.lastAnnounce.Td == nil {\n\t\treturn\n\t}\n\tif p, _ := b.ns.GetField(node, clientPeerField).(*clientPeer); p != nil {\n\t\tif p.headInfo.Td == nil || b.lastAnnounce.Td.Cmp(p.headInfo.Td) > 0 {\n\t\t\tannounce := b.lastAnnounce\n\t\t\tswitch p.announceType {\n\t\t\tcase announceTypeSimple:\n\t\t\t\tif !p.queueSend(func() { p.sendAnnounce(announce) }) {\n\t\t\t\t\tlog.Debug(\"Drop announcement because queue is full\", \"number\", announce.Number, \"hash\", announce.Hash)\n\t\t\t\t} else {\n\t\t\t\t\tlog.Debug(\"Sent announcement\", \"number\", announce.Number, \"hash\", announce.Hash)\n\t\t\t\t}\n\t\t\tcase announceTypeSigned:\n\t\t\t\tif b.signedAnnounce.Hash != b.lastAnnounce.Hash {\n\t\t\t\t\tb.signedAnnounce = b.lastAnnounce\n\t\t\t\t\tb.signedAnnounce.sign(b.privateKey)\n\t\t\t\t}\n\t\t\t\tannounce := b.signedAnnounce\n\t\t\t\tif !p.queueSend(func() { p.sendAnnounce(announce) }) {\n\t\t\t\t\tlog.Debug(\"Drop announcement because queue is full\", \"number\", announce.Number, \"hash\", announce.Hash)\n\t\t\t\t} else {\n\t\t\t\t\tlog.Debug(\"Sent announcement\", \"number\", announce.Number, \"hash\", announce.Hash)\n\t\t\t\t}\n\t\t\t}\n\t\t\tp.headInfo = blockInfo{b.lastAnnounce.Hash, b.lastAnnounce.Number, b.lastAnnounce.Td}\n\t\t}\n\t}\n}\n"], "fixing_code": ["// Copyright 2019 The go-ethereum Authors\n// This file is part of the go-ethereum library.\n//\n// The go-ethereum library is free software: you can redistribute it and/or modify\n// it under the terms of the GNU Lesser General Public License as published by\n// the Free Software Foundation, either version 3 of the License, or\n// (at your option) any later version.\n//\n// The go-ethereum library is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n// GNU Lesser General Public License for more details.\n//\n// You should have received a copy of the GNU Lesser General Public License\n// along with the go-ethereum library. If not, see <http://www.gnu.org/licenses/>.\n\npackage les\n\nimport (\n\t\"crypto/ecdsa\"\n\t\"encoding/binary\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/ethereum/go-ethereum/common\"\n\t\"github.com/ethereum/go-ethereum/common/mclock\"\n\t\"github.com/ethereum/go-ethereum/core\"\n\t\"github.com/ethereum/go-ethereum/core/rawdb\"\n\t\"github.com/ethereum/go-ethereum/core/state\"\n\t\"github.com/ethereum/go-ethereum/core/types\"\n\t\"github.com/ethereum/go-ethereum/ethdb\"\n\tlps \"github.com/ethereum/go-ethereum/les/lespay/server\"\n\t\"github.com/ethereum/go-ethereum/light\"\n\t\"github.com/ethereum/go-ethereum/log\"\n\t\"github.com/ethereum/go-ethereum/metrics\"\n\t\"github.com/ethereum/go-ethereum/p2p\"\n\t\"github.com/ethereum/go-ethereum/p2p/enode\"\n\t\"github.com/ethereum/go-ethereum/p2p/nodestate\"\n\t\"github.com/ethereum/go-ethereum/rlp\"\n\t\"github.com/ethereum/go-ethereum/trie\"\n)\n\nconst (\n\tsoftResponseLimit = 2 * 1024 * 1024 // Target maximum size of returned blocks, headers or node data.\n\testHeaderRlpSize  = 500             // Approximate size of an RLP encoded block header\n\tethVersion        = 63              // equivalent eth version for the downloader\n\n\tMaxHeaderFetch           = 192 // Amount of block headers to be fetched per retrieval request\n\tMaxBodyFetch             = 32  // Amount of block bodies to be fetched per retrieval request\n\tMaxReceiptFetch          = 128 // Amount of transaction receipts to allow fetching per request\n\tMaxCodeFetch             = 64  // Amount of contract codes to allow fetching per request\n\tMaxProofsFetch           = 64  // Amount of merkle proofs to be fetched per retrieval request\n\tMaxHelperTrieProofsFetch = 64  // Amount of helper tries to be fetched per retrieval request\n\tMaxTxSend                = 64  // Amount of transactions to be send per request\n\tMaxTxStatus              = 256 // Amount of transactions to queried per request\n)\n\nvar (\n\terrTooManyInvalidRequest = errors.New(\"too many invalid requests made\")\n\terrFullClientPool        = errors.New(\"client pool is full\")\n)\n\n// serverHandler is responsible for serving light client and process\n// all incoming light requests.\ntype serverHandler struct {\n\tblockchain *core.BlockChain\n\tchainDb    ethdb.Database\n\ttxpool     *core.TxPool\n\tserver     *LesServer\n\n\tcloseCh chan struct{}  // Channel used to exit all background routines of handler.\n\twg      sync.WaitGroup // WaitGroup used to track all background routines of handler.\n\tsynced  func() bool    // Callback function used to determine whether local node is synced.\n\n\t// Testing fields\n\taddTxsSync bool\n}\n\nfunc newServerHandler(server *LesServer, blockchain *core.BlockChain, chainDb ethdb.Database, txpool *core.TxPool, synced func() bool) *serverHandler {\n\thandler := &serverHandler{\n\t\tserver:     server,\n\t\tblockchain: blockchain,\n\t\tchainDb:    chainDb,\n\t\ttxpool:     txpool,\n\t\tcloseCh:    make(chan struct{}),\n\t\tsynced:     synced,\n\t}\n\treturn handler\n}\n\n// start starts the server handler.\nfunc (h *serverHandler) start() {\n\th.wg.Add(1)\n\tgo h.broadcastLoop()\n}\n\n// stop stops the server handler.\nfunc (h *serverHandler) stop() {\n\tclose(h.closeCh)\n\th.wg.Wait()\n}\n\n// runPeer is the p2p protocol run function for the given version.\nfunc (h *serverHandler) runPeer(version uint, p *p2p.Peer, rw p2p.MsgReadWriter) error {\n\tpeer := newClientPeer(int(version), h.server.config.NetworkId, p, newMeteredMsgWriter(rw, int(version)))\n\tdefer peer.close()\n\th.wg.Add(1)\n\tdefer h.wg.Done()\n\treturn h.handle(peer)\n}\n\nfunc (h *serverHandler) handle(p *clientPeer) error {\n\tp.Log().Debug(\"Light Ethereum peer connected\", \"name\", p.Name())\n\n\t// Execute the LES handshake\n\tvar (\n\t\thead   = h.blockchain.CurrentHeader()\n\t\thash   = head.Hash()\n\t\tnumber = head.Number.Uint64()\n\t\ttd     = h.blockchain.GetTd(hash, number)\n\t)\n\tif err := p.Handshake(td, hash, number, h.blockchain.Genesis().Hash(), h.server); err != nil {\n\t\tp.Log().Debug(\"Light Ethereum handshake failed\", \"err\", err)\n\t\treturn err\n\t}\n\t// Reject the duplicated peer, otherwise register it to peerset.\n\tvar registered bool\n\tif err := h.server.ns.Operation(func() {\n\t\tif h.server.ns.GetField(p.Node(), clientPeerField) != nil {\n\t\t\tregistered = true\n\t\t} else {\n\t\t\th.server.ns.SetFieldSub(p.Node(), clientPeerField, p)\n\t\t}\n\t}); err != nil {\n\t\treturn err\n\t}\n\tif registered {\n\t\treturn errAlreadyRegistered\n\t}\n\n\tdefer func() {\n\t\th.server.ns.SetField(p.Node(), clientPeerField, nil)\n\t\tif p.fcClient != nil { // is nil when connecting another server\n\t\t\tp.fcClient.Disconnect()\n\t\t}\n\t}()\n\tif p.server {\n\t\t// connected to another server, no messages expected, just wait for disconnection\n\t\t_, err := p.rw.ReadMsg()\n\t\treturn err\n\t}\n\t// Reject light clients if server is not synced.\n\t//\n\t// Put this checking here, so that \"non-synced\" les-server peers are still allowed\n\t// to keep the connection.\n\tif !h.synced() {\n\t\tp.Log().Debug(\"Light server not synced, rejecting peer\")\n\t\treturn p2p.DiscRequested\n\t}\n\t// Disconnect the inbound peer if it's rejected by clientPool\n\tif cap, err := h.server.clientPool.connect(p); cap != p.fcParams.MinRecharge || err != nil {\n\t\tp.Log().Debug(\"Light Ethereum peer rejected\", \"err\", errFullClientPool)\n\t\treturn errFullClientPool\n\t}\n\tp.balance, _ = h.server.ns.GetField(p.Node(), h.server.clientPool.BalanceField).(*lps.NodeBalance)\n\tif p.balance == nil {\n\t\treturn p2p.DiscRequested\n\t}\n\tactiveCount, _ := h.server.clientPool.pp.Active()\n\tclientConnectionGauge.Update(int64(activeCount))\n\n\tvar wg sync.WaitGroup // Wait group used to track all in-flight task routines.\n\n\tconnectedAt := mclock.Now()\n\tdefer func() {\n\t\twg.Wait() // Ensure all background task routines have exited.\n\t\th.server.clientPool.disconnect(p)\n\t\tp.balance = nil\n\t\tactiveCount, _ := h.server.clientPool.pp.Active()\n\t\tclientConnectionGauge.Update(int64(activeCount))\n\t\tconnectionTimer.Update(time.Duration(mclock.Now() - connectedAt))\n\t}()\n\t// Mark the peer starts to be served.\n\tatomic.StoreUint32(&p.serving, 1)\n\tdefer atomic.StoreUint32(&p.serving, 0)\n\n\t// Spawn a main loop to handle all incoming messages.\n\tfor {\n\t\tselect {\n\t\tcase err := <-p.errCh:\n\t\t\tp.Log().Debug(\"Failed to send light ethereum response\", \"err\", err)\n\t\t\treturn err\n\t\tdefault:\n\t\t}\n\t\tif err := h.handleMsg(p, &wg); err != nil {\n\t\t\tp.Log().Debug(\"Light Ethereum message handling failed\", \"err\", err)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\n// handleMsg is invoked whenever an inbound message is received from a remote\n// peer. The remote connection is torn down upon returning any error.\nfunc (h *serverHandler) handleMsg(p *clientPeer, wg *sync.WaitGroup) error {\n\t// Read the next message from the remote peer, and ensure it's fully consumed\n\tmsg, err := p.rw.ReadMsg()\n\tif err != nil {\n\t\treturn err\n\t}\n\tp.Log().Trace(\"Light Ethereum message arrived\", \"code\", msg.Code, \"bytes\", msg.Size)\n\n\t// Discard large message which exceeds the limitation.\n\tif msg.Size > ProtocolMaxMsgSize {\n\t\tclientErrorMeter.Mark(1)\n\t\treturn errResp(ErrMsgTooLarge, \"%v > %v\", msg.Size, ProtocolMaxMsgSize)\n\t}\n\tdefer msg.Discard()\n\n\tvar (\n\t\tmaxCost uint64\n\t\ttask    *servingTask\n\t)\n\tp.responseCount++\n\tresponseCount := p.responseCount\n\t// accept returns an indicator whether the request can be served.\n\t// If so, deduct the max cost from the flow control buffer.\n\taccept := func(reqID, reqCnt, maxCnt uint64) bool {\n\t\t// Short circuit if the peer is already frozen or the request is invalid.\n\t\tinSizeCost := h.server.costTracker.realCost(0, msg.Size, 0)\n\t\tif p.isFrozen() || reqCnt == 0 || reqCnt > maxCnt {\n\t\t\tp.fcClient.OneTimeCost(inSizeCost)\n\t\t\treturn false\n\t\t}\n\t\t// Prepaid max cost units before request been serving.\n\t\tmaxCost = p.fcCosts.getMaxCost(msg.Code, reqCnt)\n\t\taccepted, bufShort, priority := p.fcClient.AcceptRequest(reqID, responseCount, maxCost)\n\t\tif !accepted {\n\t\t\tp.freeze()\n\t\t\tp.Log().Error(\"Request came too early\", \"remaining\", common.PrettyDuration(time.Duration(bufShort*1000000/p.fcParams.MinRecharge)))\n\t\t\tp.fcClient.OneTimeCost(inSizeCost)\n\t\t\treturn false\n\t\t}\n\t\t// Create a multi-stage task, estimate the time it takes for the task to\n\t\t// execute, and cache it in the request service queue.\n\t\tfactor := h.server.costTracker.globalFactor()\n\t\tif factor < 0.001 {\n\t\t\tfactor = 1\n\t\t\tp.Log().Error(\"Invalid global cost factor\", \"factor\", factor)\n\t\t}\n\t\tmaxTime := uint64(float64(maxCost) / factor)\n\t\ttask = h.server.servingQueue.newTask(p, maxTime, priority)\n\t\tif task.start() {\n\t\t\treturn true\n\t\t}\n\t\tp.fcClient.RequestProcessed(reqID, responseCount, maxCost, inSizeCost)\n\t\treturn false\n\t}\n\t// sendResponse sends back the response and updates the flow control statistic.\n\tsendResponse := func(reqID, amount uint64, reply *reply, servingTime uint64) {\n\t\tp.responseLock.Lock()\n\t\tdefer p.responseLock.Unlock()\n\n\t\t// Short circuit if the client is already frozen.\n\t\tif p.isFrozen() {\n\t\t\trealCost := h.server.costTracker.realCost(servingTime, msg.Size, 0)\n\t\t\tp.fcClient.RequestProcessed(reqID, responseCount, maxCost, realCost)\n\t\t\treturn\n\t\t}\n\t\t// Positive correction buffer value with real cost.\n\t\tvar replySize uint32\n\t\tif reply != nil {\n\t\t\treplySize = reply.size()\n\t\t}\n\t\tvar realCost uint64\n\t\tif h.server.costTracker.testing {\n\t\t\trealCost = maxCost // Assign a fake cost for testing purpose\n\t\t} else {\n\t\t\trealCost = h.server.costTracker.realCost(servingTime, msg.Size, replySize)\n\t\t\tif realCost > maxCost {\n\t\t\t\trealCost = maxCost\n\t\t\t}\n\t\t}\n\t\tbv := p.fcClient.RequestProcessed(reqID, responseCount, maxCost, realCost)\n\t\tif amount != 0 {\n\t\t\t// Feed cost tracker request serving statistic.\n\t\t\th.server.costTracker.updateStats(msg.Code, amount, servingTime, realCost)\n\t\t\t// Reduce priority \"balance\" for the specific peer.\n\t\t\tp.balance.RequestServed(realCost)\n\t\t}\n\t\tif reply != nil {\n\t\t\tp.queueSend(func() {\n\t\t\t\tif err := reply.send(bv); err != nil {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase p.errCh <- err:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\t}\n\tswitch msg.Code {\n\tcase GetBlockHeadersMsg:\n\t\tp.Log().Trace(\"Received block header request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInHeaderPacketsMeter.Mark(1)\n\t\t\tmiscInHeaderTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID uint64\n\t\t\tQuery getBlockHeadersData\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"%v: %v\", msg, err)\n\t\t}\n\t\tquery := req.Query\n\t\tif accept(req.ReqID, query.Amount, MaxHeaderFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\thashMode := query.Origin.Hash != (common.Hash{})\n\t\t\t\tfirst := true\n\t\t\t\tmaxNonCanonical := uint64(100)\n\n\t\t\t\t// Gather headers until the fetch or network limits is reached\n\t\t\t\tvar (\n\t\t\t\t\tbytes   common.StorageSize\n\t\t\t\t\theaders []*types.Header\n\t\t\t\t\tunknown bool\n\t\t\t\t)\n\t\t\t\tfor !unknown && len(headers) < int(query.Amount) && bytes < softResponseLimit {\n\t\t\t\t\tif !first && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// Retrieve the next header satisfying the query\n\t\t\t\t\tvar origin *types.Header\n\t\t\t\t\tif hashMode {\n\t\t\t\t\t\tif first {\n\t\t\t\t\t\t\torigin = h.blockchain.GetHeaderByHash(query.Origin.Hash)\n\t\t\t\t\t\t\tif origin != nil {\n\t\t\t\t\t\t\t\tquery.Origin.Number = origin.Number.Uint64()\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\torigin = h.blockchain.GetHeader(query.Origin.Hash, query.Origin.Number)\n\t\t\t\t\t\t}\n\t\t\t\t\t} else {\n\t\t\t\t\t\torigin = h.blockchain.GetHeaderByNumber(query.Origin.Number)\n\t\t\t\t\t}\n\t\t\t\t\tif origin == nil {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\theaders = append(headers, origin)\n\t\t\t\t\tbytes += estHeaderRlpSize\n\n\t\t\t\t\t// Advance to the next header of the query\n\t\t\t\t\tswitch {\n\t\t\t\t\tcase hashMode && query.Reverse:\n\t\t\t\t\t\t// Hash based traversal towards the genesis block\n\t\t\t\t\t\tancestor := query.Skip + 1\n\t\t\t\t\t\tif ancestor == 0 {\n\t\t\t\t\t\t\tunknown = true\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tquery.Origin.Hash, query.Origin.Number = h.blockchain.GetAncestor(query.Origin.Hash, query.Origin.Number, ancestor, &maxNonCanonical)\n\t\t\t\t\t\t\tunknown = query.Origin.Hash == common.Hash{}\n\t\t\t\t\t\t}\n\t\t\t\t\tcase hashMode && !query.Reverse:\n\t\t\t\t\t\t// Hash based traversal towards the leaf block\n\t\t\t\t\t\tvar (\n\t\t\t\t\t\t\tcurrent = origin.Number.Uint64()\n\t\t\t\t\t\t\tnext    = current + query.Skip + 1\n\t\t\t\t\t\t)\n\t\t\t\t\t\tif next <= current {\n\t\t\t\t\t\t\tinfos, _ := json.MarshalIndent(p.Peer.Info(), \"\", \"  \")\n\t\t\t\t\t\t\tp.Log().Warn(\"GetBlockHeaders skip overflow attack\", \"current\", current, \"skip\", query.Skip, \"next\", next, \"attacker\", infos)\n\t\t\t\t\t\t\tunknown = true\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tif header := h.blockchain.GetHeaderByNumber(next); header != nil {\n\t\t\t\t\t\t\t\tnextHash := header.Hash()\n\t\t\t\t\t\t\t\texpOldHash, _ := h.blockchain.GetAncestor(nextHash, next, query.Skip+1, &maxNonCanonical)\n\t\t\t\t\t\t\t\tif expOldHash == query.Origin.Hash {\n\t\t\t\t\t\t\t\t\tquery.Origin.Hash, query.Origin.Number = nextHash, next\n\t\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\t\tunknown = true\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tunknown = true\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\tcase query.Reverse:\n\t\t\t\t\t\t// Number based traversal towards the genesis block\n\t\t\t\t\t\tif query.Origin.Number >= query.Skip+1 {\n\t\t\t\t\t\t\tquery.Origin.Number -= query.Skip + 1\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tunknown = true\n\t\t\t\t\t\t}\n\n\t\t\t\t\tcase !query.Reverse:\n\t\t\t\t\t\t// Number based traversal towards the leaf block\n\t\t\t\t\t\tquery.Origin.Number += query.Skip + 1\n\t\t\t\t\t}\n\t\t\t\t\tfirst = false\n\t\t\t\t}\n\t\t\t\treply := p.replyBlockHeaders(req.ReqID, headers)\n\t\t\t\tsendResponse(req.ReqID, query.Amount, reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutHeaderPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutHeaderTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeHeaderTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetBlockBodiesMsg:\n\t\tp.Log().Trace(\"Received block bodies request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInBodyPacketsMeter.Mark(1)\n\t\t\tmiscInBodyTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID  uint64\n\t\t\tHashes []common.Hash\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\tvar (\n\t\t\tbytes  int\n\t\t\tbodies []rlp.RawValue\n\t\t)\n\t\treqCnt := len(req.Hashes)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxBodyFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tfor i, hash := range req.Hashes {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tif bytes >= softResponseLimit {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\tbody := h.blockchain.GetBodyRLP(hash)\n\t\t\t\t\tif body == nil {\n\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tbodies = append(bodies, body)\n\t\t\t\t\tbytes += len(body)\n\t\t\t\t}\n\t\t\t\treply := p.replyBlockBodiesRLP(req.ReqID, bodies)\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutBodyPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutBodyTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeBodyTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetCodeMsg:\n\t\tp.Log().Trace(\"Received code request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInCodePacketsMeter.Mark(1)\n\t\t\tmiscInCodeTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID uint64\n\t\t\tReqs  []CodeReq\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\tvar (\n\t\t\tbytes int\n\t\t\tdata  [][]byte\n\t\t)\n\t\treqCnt := len(req.Reqs)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxCodeFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tfor i, request := range req.Reqs {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// Look up the root hash belonging to the request\n\t\t\t\t\theader := h.blockchain.GetHeaderByHash(request.BHash)\n\t\t\t\t\tif header == nil {\n\t\t\t\t\t\tp.Log().Warn(\"Failed to retrieve associate header for code\", \"hash\", request.BHash)\n\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// Refuse to search stale state data in the database since looking for\n\t\t\t\t\t// a non-exist key is kind of expensive.\n\t\t\t\t\tlocal := h.blockchain.CurrentHeader().Number.Uint64()\n\t\t\t\t\tif !h.server.archiveMode && header.Number.Uint64()+core.TriesInMemory <= local {\n\t\t\t\t\t\tp.Log().Debug(\"Reject stale code request\", \"number\", header.Number.Uint64(), \"head\", local)\n\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\ttriedb := h.blockchain.StateCache().TrieDB()\n\n\t\t\t\t\taccount, err := h.getAccount(triedb, header.Root, common.BytesToHash(request.AccKey))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tp.Log().Warn(\"Failed to retrieve account for code\", \"block\", header.Number, \"hash\", header.Hash(), \"account\", common.BytesToHash(request.AccKey), \"err\", err)\n\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tcode, err := h.blockchain.StateCache().ContractCode(common.BytesToHash(request.AccKey), common.BytesToHash(account.CodeHash))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tp.Log().Warn(\"Failed to retrieve account code\", \"block\", header.Number, \"hash\", header.Hash(), \"account\", common.BytesToHash(request.AccKey), \"codehash\", common.BytesToHash(account.CodeHash), \"err\", err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// Accumulate the code and abort if enough data was retrieved\n\t\t\t\t\tdata = append(data, code)\n\t\t\t\t\tif bytes += len(code); bytes >= softResponseLimit {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treply := p.replyCode(req.ReqID, data)\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutCodePacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutCodeTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeCodeTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetReceiptsMsg:\n\t\tp.Log().Trace(\"Received receipts request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInReceiptPacketsMeter.Mark(1)\n\t\t\tmiscInReceiptTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID  uint64\n\t\t\tHashes []common.Hash\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\tvar (\n\t\t\tbytes    int\n\t\t\treceipts []rlp.RawValue\n\t\t)\n\t\treqCnt := len(req.Hashes)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxReceiptFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tfor i, hash := range req.Hashes {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tif bytes >= softResponseLimit {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\t// Retrieve the requested block's receipts, skipping if unknown to us\n\t\t\t\t\tresults := h.blockchain.GetReceiptsByHash(hash)\n\t\t\t\t\tif results == nil {\n\t\t\t\t\t\tif header := h.blockchain.GetHeaderByHash(hash); header == nil || header.ReceiptHash != types.EmptyRootHash {\n\t\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// If known, encode and queue for response packet\n\t\t\t\t\tif encoded, err := rlp.EncodeToBytes(results); err != nil {\n\t\t\t\t\t\tlog.Error(\"Failed to encode receipt\", \"err\", err)\n\t\t\t\t\t} else {\n\t\t\t\t\t\treceipts = append(receipts, encoded)\n\t\t\t\t\t\tbytes += len(encoded)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treply := p.replyReceiptsRLP(req.ReqID, receipts)\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutReceiptPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutReceiptTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeReceiptTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetProofsV2Msg:\n\t\tp.Log().Trace(\"Received les/2 proofs request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInTrieProofPacketsMeter.Mark(1)\n\t\t\tmiscInTrieProofTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID uint64\n\t\t\tReqs  []ProofReq\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\t// Gather state data until the fetch or network limits is reached\n\t\tvar (\n\t\t\tlastBHash common.Hash\n\t\t\troot      common.Hash\n\t\t\theader    *types.Header\n\t\t)\n\t\treqCnt := len(req.Reqs)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxProofsFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tnodes := light.NewNodeSet()\n\n\t\t\t\tfor i, request := range req.Reqs {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\t// Look up the root hash belonging to the request\n\t\t\t\t\tif request.BHash != lastBHash {\n\t\t\t\t\t\troot, lastBHash = common.Hash{}, request.BHash\n\n\t\t\t\t\t\tif header = h.blockchain.GetHeaderByHash(request.BHash); header == nil {\n\t\t\t\t\t\t\tp.Log().Warn(\"Failed to retrieve header for proof\", \"hash\", request.BHash)\n\t\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// Refuse to search stale state data in the database since looking for\n\t\t\t\t\t\t// a non-exist key is kind of expensive.\n\t\t\t\t\t\tlocal := h.blockchain.CurrentHeader().Number.Uint64()\n\t\t\t\t\t\tif !h.server.archiveMode && header.Number.Uint64()+core.TriesInMemory <= local {\n\t\t\t\t\t\t\tp.Log().Debug(\"Reject stale trie request\", \"number\", header.Number.Uint64(), \"head\", local)\n\t\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\troot = header.Root\n\t\t\t\t\t}\n\t\t\t\t\t// If a header lookup failed (non existent), ignore subsequent requests for the same header\n\t\t\t\t\tif root == (common.Hash{}) {\n\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\t// Open the account or storage trie for the request\n\t\t\t\t\tstatedb := h.blockchain.StateCache()\n\n\t\t\t\t\tvar trie state.Trie\n\t\t\t\t\tswitch len(request.AccKey) {\n\t\t\t\t\tcase 0:\n\t\t\t\t\t\t// No account key specified, open an account trie\n\t\t\t\t\t\ttrie, err = statedb.OpenTrie(root)\n\t\t\t\t\t\tif trie == nil || err != nil {\n\t\t\t\t\t\t\tp.Log().Warn(\"Failed to open storage trie for proof\", \"block\", header.Number, \"hash\", header.Hash(), \"root\", root, \"err\", err)\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\tdefault:\n\t\t\t\t\t\t// Account key specified, open a storage trie\n\t\t\t\t\t\taccount, err := h.getAccount(statedb.TrieDB(), root, common.BytesToHash(request.AccKey))\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\tp.Log().Warn(\"Failed to retrieve account for proof\", \"block\", header.Number, \"hash\", header.Hash(), \"account\", common.BytesToHash(request.AccKey), \"err\", err)\n\t\t\t\t\t\t\tp.bumpInvalid()\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\ttrie, err = statedb.OpenStorageTrie(common.BytesToHash(request.AccKey), account.Root)\n\t\t\t\t\t\tif trie == nil || err != nil {\n\t\t\t\t\t\t\tp.Log().Warn(\"Failed to open storage trie for proof\", \"block\", header.Number, \"hash\", header.Hash(), \"account\", common.BytesToHash(request.AccKey), \"root\", account.Root, \"err\", err)\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// Prove the user's request from the account or stroage trie\n\t\t\t\t\tif err := trie.Prove(request.Key, request.FromLevel, nodes); err != nil {\n\t\t\t\t\t\tp.Log().Warn(\"Failed to prove state request\", \"block\", header.Number, \"hash\", header.Hash(), \"err\", err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif nodes.DataSize() >= softResponseLimit {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treply := p.replyProofsV2(req.ReqID, nodes.NodeList())\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutTrieProofPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutTrieProofTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeTrieProofTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetHelperTrieProofsMsg:\n\t\tp.Log().Trace(\"Received helper trie proof request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInHelperTriePacketsMeter.Mark(1)\n\t\t\tmiscInHelperTrieTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID uint64\n\t\t\tReqs  []HelperTrieReq\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\t// Gather state data until the fetch or network limits is reached\n\t\tvar (\n\t\t\tauxBytes int\n\t\t\tauxData  [][]byte\n\t\t)\n\t\treqCnt := len(req.Reqs)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxHelperTrieProofsFetch) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tvar (\n\t\t\t\t\tlastIdx  uint64\n\t\t\t\t\tlastType uint\n\t\t\t\t\troot     common.Hash\n\t\t\t\t\tauxTrie  *trie.Trie\n\t\t\t\t)\n\t\t\t\tnodes := light.NewNodeSet()\n\t\t\t\tfor i, request := range req.Reqs {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tif auxTrie == nil || request.Type != lastType || request.TrieIdx != lastIdx {\n\t\t\t\t\t\tauxTrie, lastType, lastIdx = nil, request.Type, request.TrieIdx\n\n\t\t\t\t\t\tvar prefix string\n\t\t\t\t\t\tif root, prefix = h.getHelperTrie(request.Type, request.TrieIdx); root != (common.Hash{}) {\n\t\t\t\t\t\t\tauxTrie, _ = trie.New(root, trie.NewDatabase(rawdb.NewTable(h.chainDb, prefix)))\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif request.AuxReq == auxRoot {\n\t\t\t\t\t\tvar data []byte\n\t\t\t\t\t\tif root != (common.Hash{}) {\n\t\t\t\t\t\t\tdata = root[:]\n\t\t\t\t\t\t}\n\t\t\t\t\t\tauxData = append(auxData, data)\n\t\t\t\t\t\tauxBytes += len(data)\n\t\t\t\t\t} else {\n\t\t\t\t\t\tif auxTrie != nil {\n\t\t\t\t\t\t\tauxTrie.Prove(request.Key, request.FromLevel, nodes)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif request.AuxReq != 0 {\n\t\t\t\t\t\t\tdata := h.getAuxiliaryHeaders(request)\n\t\t\t\t\t\t\tauxData = append(auxData, data)\n\t\t\t\t\t\t\tauxBytes += len(data)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tif nodes.DataSize()+auxBytes >= softResponseLimit {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treply := p.replyHelperTrieProofs(req.ReqID, HelperTrieResps{Proofs: nodes.NodeList(), AuxData: auxData})\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutHelperTriePacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutHelperTrieTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeHelperTrieTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase SendTxV2Msg:\n\t\tp.Log().Trace(\"Received new transactions\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInTxsPacketsMeter.Mark(1)\n\t\t\tmiscInTxsTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID uint64\n\t\t\tTxs   []*types.Transaction\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\treqCnt := len(req.Txs)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxTxSend) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tstats := make([]light.TxStatus, len(req.Txs))\n\t\t\t\tfor i, tx := range req.Txs {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\thash := tx.Hash()\n\t\t\t\t\tstats[i] = h.txStatus(hash)\n\t\t\t\t\tif stats[i].Status == core.TxStatusUnknown {\n\t\t\t\t\t\taddFn := h.txpool.AddRemotes\n\t\t\t\t\t\t// Add txs synchronously for testing purpose\n\t\t\t\t\t\tif h.addTxsSync {\n\t\t\t\t\t\t\taddFn = h.txpool.AddRemotesSync\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif errs := addFn([]*types.Transaction{tx}); errs[0] != nil {\n\t\t\t\t\t\t\tstats[i].Error = errs[0].Error()\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t\tstats[i] = h.txStatus(hash)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treply := p.replyTxStatus(req.ReqID, stats)\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutTxsPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutTxsTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeTxTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tcase GetTxStatusMsg:\n\t\tp.Log().Trace(\"Received transaction status query request\")\n\t\tif metrics.EnabledExpensive {\n\t\t\tmiscInTxStatusPacketsMeter.Mark(1)\n\t\t\tmiscInTxStatusTrafficMeter.Mark(int64(msg.Size))\n\t\t}\n\t\tvar req struct {\n\t\t\tReqID  uint64\n\t\t\tHashes []common.Hash\n\t\t}\n\t\tif err := msg.Decode(&req); err != nil {\n\t\t\tclientErrorMeter.Mark(1)\n\t\t\treturn errResp(ErrDecode, \"msg %v: %v\", msg, err)\n\t\t}\n\t\treqCnt := len(req.Hashes)\n\t\tif accept(req.ReqID, uint64(reqCnt), MaxTxStatus) {\n\t\t\twg.Add(1)\n\t\t\tgo func() {\n\t\t\t\tdefer wg.Done()\n\t\t\t\tstats := make([]light.TxStatus, len(req.Hashes))\n\t\t\t\tfor i, hash := range req.Hashes {\n\t\t\t\t\tif i != 0 && !task.waitOrStop() {\n\t\t\t\t\t\tsendResponse(req.ReqID, 0, nil, task.servingTime)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t\tstats[i] = h.txStatus(hash)\n\t\t\t\t}\n\t\t\t\treply := p.replyTxStatus(req.ReqID, stats)\n\t\t\t\tsendResponse(req.ReqID, uint64(reqCnt), reply, task.done())\n\t\t\t\tif metrics.EnabledExpensive {\n\t\t\t\t\tmiscOutTxStatusPacketsMeter.Mark(1)\n\t\t\t\t\tmiscOutTxStatusTrafficMeter.Mark(int64(reply.size()))\n\t\t\t\t\tmiscServingTimeTxStatusTimer.Update(time.Duration(task.servingTime))\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\tdefault:\n\t\tp.Log().Trace(\"Received invalid message\", \"code\", msg.Code)\n\t\tclientErrorMeter.Mark(1)\n\t\treturn errResp(ErrInvalidMsgCode, \"%v\", msg.Code)\n\t}\n\t// If the client has made too much invalid request(e.g. request a non-existent data),\n\t// reject them to prevent SPAM attack.\n\tif p.getInvalid() > maxRequestErrors {\n\t\tclientErrorMeter.Mark(1)\n\t\treturn errTooManyInvalidRequest\n\t}\n\treturn nil\n}\n\n// getAccount retrieves an account from the state based on root.\nfunc (h *serverHandler) getAccount(triedb *trie.Database, root, hash common.Hash) (state.Account, error) {\n\ttrie, err := trie.New(root, triedb)\n\tif err != nil {\n\t\treturn state.Account{}, err\n\t}\n\tblob, err := trie.TryGet(hash[:])\n\tif err != nil {\n\t\treturn state.Account{}, err\n\t}\n\tvar account state.Account\n\tif err = rlp.DecodeBytes(blob, &account); err != nil {\n\t\treturn state.Account{}, err\n\t}\n\treturn account, nil\n}\n\n// getHelperTrie returns the post-processed trie root for the given trie ID and section index\nfunc (h *serverHandler) getHelperTrie(typ uint, index uint64) (common.Hash, string) {\n\tswitch typ {\n\tcase htCanonical:\n\t\tsectionHead := rawdb.ReadCanonicalHash(h.chainDb, (index+1)*h.server.iConfig.ChtSize-1)\n\t\treturn light.GetChtRoot(h.chainDb, index, sectionHead), light.ChtTablePrefix\n\tcase htBloomBits:\n\t\tsectionHead := rawdb.ReadCanonicalHash(h.chainDb, (index+1)*h.server.iConfig.BloomTrieSize-1)\n\t\treturn light.GetBloomTrieRoot(h.chainDb, index, sectionHead), light.BloomTrieTablePrefix\n\t}\n\treturn common.Hash{}, \"\"\n}\n\n// getAuxiliaryHeaders returns requested auxiliary headers for the CHT request.\nfunc (h *serverHandler) getAuxiliaryHeaders(req HelperTrieReq) []byte {\n\tif req.Type == htCanonical && req.AuxReq == auxHeader && len(req.Key) == 8 {\n\t\tblockNum := binary.BigEndian.Uint64(req.Key)\n\t\thash := rawdb.ReadCanonicalHash(h.chainDb, blockNum)\n\t\treturn rawdb.ReadHeaderRLP(h.chainDb, hash, blockNum)\n\t}\n\treturn nil\n}\n\n// txStatus returns the status of a specified transaction.\nfunc (h *serverHandler) txStatus(hash common.Hash) light.TxStatus {\n\tvar stat light.TxStatus\n\t// Looking the transaction in txpool first.\n\tstat.Status = h.txpool.Status([]common.Hash{hash})[0]\n\n\t// If the transaction is unknown to the pool, try looking it up locally.\n\tif stat.Status == core.TxStatusUnknown {\n\t\tlookup := h.blockchain.GetTransactionLookup(hash)\n\t\tif lookup != nil {\n\t\t\tstat.Status = core.TxStatusIncluded\n\t\t\tstat.Lookup = lookup\n\t\t}\n\t}\n\treturn stat\n}\n\n// broadcastLoop broadcasts new block information to all connected light\n// clients. According to the agreement between client and server, server should\n// only broadcast new announcement if the total difficulty is higher than the\n// last one. Besides server will add the signature if client requires.\nfunc (h *serverHandler) broadcastLoop() {\n\tdefer h.wg.Done()\n\n\theadCh := make(chan core.ChainHeadEvent, 10)\n\theadSub := h.blockchain.SubscribeChainHeadEvent(headCh)\n\tdefer headSub.Unsubscribe()\n\n\tvar (\n\t\tlastHead *types.Header\n\t\tlastTd   = common.Big0\n\t)\n\tfor {\n\t\tselect {\n\t\tcase ev := <-headCh:\n\t\t\theader := ev.Block.Header()\n\t\t\thash, number := header.Hash(), header.Number.Uint64()\n\t\t\ttd := h.blockchain.GetTd(hash, number)\n\t\t\tif td == nil || td.Cmp(lastTd) <= 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvar reorg uint64\n\t\t\tif lastHead != nil {\n\t\t\t\treorg = lastHead.Number.Uint64() - rawdb.FindCommonAncestor(h.chainDb, header, lastHead).Number.Uint64()\n\t\t\t}\n\t\t\tlastHead, lastTd = header, td\n\t\t\tlog.Debug(\"Announcing block to peers\", \"number\", number, \"hash\", hash, \"td\", td, \"reorg\", reorg)\n\t\t\th.server.broadcaster.broadcast(announceData{Hash: hash, Number: number, Td: td, ReorgDepth: reorg})\n\t\tcase <-h.closeCh:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// broadcaster sends new header announcements to active client peers\ntype broadcaster struct {\n\tns                           *nodestate.NodeStateMachine\n\tprivateKey                   *ecdsa.PrivateKey\n\tlastAnnounce, signedAnnounce announceData\n}\n\n// newBroadcaster creates a new broadcaster\nfunc newBroadcaster(ns *nodestate.NodeStateMachine) *broadcaster {\n\tb := &broadcaster{ns: ns}\n\tns.SubscribeState(priorityPoolSetup.ActiveFlag, func(node *enode.Node, oldState, newState nodestate.Flags) {\n\t\tif newState.Equals(priorityPoolSetup.ActiveFlag) {\n\t\t\t// send last announcement to activated peers\n\t\t\tb.sendTo(node)\n\t\t}\n\t})\n\treturn b\n}\n\n// setSignerKey sets the signer key for signed announcements. Should be called before\n// starting the protocol handler.\nfunc (b *broadcaster) setSignerKey(privateKey *ecdsa.PrivateKey) {\n\tb.privateKey = privateKey\n}\n\n// broadcast sends the given announcements to all active peers\nfunc (b *broadcaster) broadcast(announce announceData) {\n\tb.ns.Operation(func() {\n\t\t// iterate in an Operation to ensure that the active set does not change while iterating\n\t\tb.lastAnnounce = announce\n\t\tb.ns.ForEach(priorityPoolSetup.ActiveFlag, nodestate.Flags{}, func(node *enode.Node, state nodestate.Flags) {\n\t\t\tb.sendTo(node)\n\t\t})\n\t})\n}\n\n// sendTo sends the most recent announcement to the given node unless the same or higher Td\n// announcement has already been sent.\nfunc (b *broadcaster) sendTo(node *enode.Node) {\n\tif b.lastAnnounce.Td == nil {\n\t\treturn\n\t}\n\tif p, _ := b.ns.GetField(node, clientPeerField).(*clientPeer); p != nil {\n\t\tif p.headInfo.Td == nil || b.lastAnnounce.Td.Cmp(p.headInfo.Td) > 0 {\n\t\t\tannounce := b.lastAnnounce\n\t\t\tswitch p.announceType {\n\t\t\tcase announceTypeSimple:\n\t\t\t\tif !p.queueSend(func() { p.sendAnnounce(announce) }) {\n\t\t\t\t\tlog.Debug(\"Drop announcement because queue is full\", \"number\", announce.Number, \"hash\", announce.Hash)\n\t\t\t\t} else {\n\t\t\t\t\tlog.Debug(\"Sent announcement\", \"number\", announce.Number, \"hash\", announce.Hash)\n\t\t\t\t}\n\t\t\tcase announceTypeSigned:\n\t\t\t\tif b.signedAnnounce.Hash != b.lastAnnounce.Hash {\n\t\t\t\t\tb.signedAnnounce = b.lastAnnounce\n\t\t\t\t\tb.signedAnnounce.sign(b.privateKey)\n\t\t\t\t}\n\t\t\t\tannounce := b.signedAnnounce\n\t\t\t\tif !p.queueSend(func() { p.sendAnnounce(announce) }) {\n\t\t\t\t\tlog.Debug(\"Drop announcement because queue is full\", \"number\", announce.Number, \"hash\", announce.Hash)\n\t\t\t\t} else {\n\t\t\t\t\tlog.Debug(\"Sent announcement\", \"number\", announce.Number, \"hash\", announce.Hash)\n\t\t\t\t}\n\t\t\t}\n\t\t\tp.headInfo = blockInfo{b.lastAnnounce.Hash, b.lastAnnounce.Number, b.lastAnnounce.Td}\n\t\t}\n\t}\n}\n"], "filenames": ["les/server_handler.go"], "buggy_code_start_loc": [612], "buggy_code_end_loc": [656], "fixing_code_start_loc": [613], "fixing_code_end_loc": [655], "type": "CWE-400", "message": "Go Ethereum, or \"Geth\", is the official Golang implementation of the Ethereum protocol. In Geth before version 1.9.25 a denial-of-service vulnerability can make a LES server crash via malicious GetProofsV2 request from a connected LES client. This vulnerability only concerns users explicitly enabling les server; disabling les prevents the exploit. The vulnerability was patched in version 1.9.25.", "other": {"cve": {"id": "CVE-2020-26264", "sourceIdentifier": "security-advisories@github.com", "published": "2020-12-11T17:15:12.793", "lastModified": "2020-12-14T18:06:07.520", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Go Ethereum, or \"Geth\", is the official Golang implementation of the Ethereum protocol. In Geth before version 1.9.25 a denial-of-service vulnerability can make a LES server crash via malicious GetProofsV2 request from a connected LES client. This vulnerability only concerns users explicitly enabling les server; disabling les prevents the exploit. The vulnerability was patched in version 1.9.25."}, {"lang": "es", "value": "Go Ethereum, o \"Geth\", es la implementaci\u00f3n oficial de Golang del protocolo Ethereum.&#xa0;En Geth versiones anteriores a 1.9.25, una vulnerabilidad de Denegaci\u00f3n de Servicio puede hacer a un servidor LES bloquearse por medio de una petici\u00f3n GetProofsV2 maliciosa de un cliente LES conectado.&#xa0;Esta vulnerabilidad solo afecta a usuarios que habilitan expl\u00edcitamente el servidor de archivos;&#xa0;deshabilitar archivos evita la explotaci\u00f3n.&#xa0;La vulnerabilidad fue parcheada en versi\u00f3n 1.9.25"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:S/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "SINGLE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 4.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-400"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-400"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:ethereum:go_ethereum:*:*:*:*:*:*:*:*", "versionEndExcluding": "1.9.25", "matchCriteriaId": "5AC7FBD4-34A5-414C-BBBA-2512124CFBF0"}]}]}], "references": [{"url": "https://github.com/ethereum/go-ethereum/commit/bddd103a9f0af27ef533f04e06ea429cf76b6d46", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/ethereum/go-ethereum/pull/21896", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/ethereum/go-ethereum/releases/tag/v1.9.25", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/ethereum/go-ethereum/security/advisories/GHSA-r33q-22hv-j29q", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/ethereum/go-ethereum/commit/bddd103a9f0af27ef533f04e06ea429cf76b6d46"}}