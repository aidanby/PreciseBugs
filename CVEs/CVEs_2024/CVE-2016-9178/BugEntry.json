{"buggy_code": ["#ifndef _ASM_X86_UACCESS_H\n#define _ASM_X86_UACCESS_H\n/*\n * User space memory access functions\n */\n#include <linux/errno.h>\n#include <linux/compiler.h>\n#include <linux/kasan-checks.h>\n#include <linux/thread_info.h>\n#include <linux/string.h>\n#include <asm/asm.h>\n#include <asm/page.h>\n#include <asm/smap.h>\n\n#define VERIFY_READ 0\n#define VERIFY_WRITE 1\n\n/*\n * The fs value determines whether argument validity checking should be\n * performed or not.  If get_fs() == USER_DS, checking is performed, with\n * get_fs() == KERNEL_DS, checking is bypassed.\n *\n * For historical reasons, these macros are grossly misnamed.\n */\n\n#define MAKE_MM_SEG(s)\t((mm_segment_t) { (s) })\n\n#define KERNEL_DS\tMAKE_MM_SEG(-1UL)\n#define USER_DS \tMAKE_MM_SEG(TASK_SIZE_MAX)\n\n#define get_ds()\t(KERNEL_DS)\n#define get_fs()\t(current->thread.addr_limit)\n#define set_fs(x)\t(current->thread.addr_limit = (x))\n\n#define segment_eq(a, b)\t((a).seg == (b).seg)\n\n#define user_addr_max() (current->thread.addr_limit.seg)\n#define __addr_ok(addr) \t\\\n\t((unsigned long __force)(addr) < user_addr_max())\n\n/*\n * Test whether a block of memory is a valid user space address.\n * Returns 0 if the range is valid, nonzero otherwise.\n */\nstatic inline bool __chk_range_not_ok(unsigned long addr, unsigned long size, unsigned long limit)\n{\n\t/*\n\t * If we have used \"sizeof()\" for the size,\n\t * we know it won't overflow the limit (but\n\t * it might overflow the 'addr', so it's\n\t * important to subtract the size from the\n\t * limit, not add it to the address).\n\t */\n\tif (__builtin_constant_p(size))\n\t\treturn unlikely(addr > limit - size);\n\n\t/* Arbitrary sizes? Be careful about overflow */\n\taddr += size;\n\tif (unlikely(addr < size))\n\t\treturn true;\n\treturn unlikely(addr > limit);\n}\n\n#define __range_not_ok(addr, size, limit)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(addr);\t\t\t\t\t\t\\\n\t__chk_range_not_ok((unsigned long __force)(addr), size, limit); \\\n})\n\n/**\n * access_ok: - Checks if a user space pointer is valid\n * @type: Type of access: %VERIFY_READ or %VERIFY_WRITE.  Note that\n *        %VERIFY_WRITE is a superset of %VERIFY_READ - if it is safe\n *        to write to a block, it is always safe to read from it.\n * @addr: User space pointer to start of block to check\n * @size: Size of block to check\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * Checks if a pointer to a block of memory in user space is valid.\n *\n * Returns true (nonzero) if the memory block may be valid, false (zero)\n * if it is definitely invalid.\n *\n * Note that, depending on architecture, this function probably just\n * checks that the pointer is in the user space range - after calling\n * this function, memory access functions may still return -EFAULT.\n */\n#define access_ok(type, addr, size) \\\n\tlikely(!__range_not_ok(addr, size, user_addr_max()))\n\n/*\n * The exception table consists of triples of addresses relative to the\n * exception table entry itself. The first address is of an instruction\n * that is allowed to fault, the second is the target at which the program\n * should continue. The third is a handler function to deal with the fault\n * caused by the instruction in the first field.\n *\n * All the routines below use bits of fixup code that are out of line\n * with the main instruction path.  This means when everything is well,\n * we don't even have to jump over them.  Further, they do not intrude\n * on our cache or tlb entries.\n */\n\nstruct exception_table_entry {\n\tint insn, fixup, handler;\n};\n\n#define ARCH_HAS_RELATIVE_EXTABLE\n\n#define swap_ex_entry_fixup(a, b, tmp, delta)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\t(a)->fixup = (b)->fixup + (delta);\t\t\\\n\t\t(b)->fixup = (tmp).fixup - (delta);\t\t\\\n\t\t(a)->handler = (b)->handler + (delta);\t\t\\\n\t\t(b)->handler = (tmp).handler - (delta);\t\t\\\n\t} while (0)\n\nextern int fixup_exception(struct pt_regs *regs, int trapnr);\nextern bool ex_has_fault_handler(unsigned long ip);\nextern void early_fixup_exception(struct pt_regs *regs, int trapnr);\n\n/*\n * These are the main single-value transfer routines.  They automatically\n * use the right size if we just have the right pointer type.\n *\n * This gets kind of ugly. We want to return _two_ values in \"get_user()\"\n * and yet we don't want to do any pointers, because that is too much\n * of a performance impact. Thus we have a few rather ugly macros here,\n * and hide all the ugliness from the user.\n *\n * The \"__xxx\" versions of the user access functions are versions that\n * do not verify the address space, that must have been done previously\n * with a separate \"access_ok()\" call (this is used when we do multiple\n * accesses to the same area of user memory).\n */\n\nextern int __get_user_1(void);\nextern int __get_user_2(void);\nextern int __get_user_4(void);\nextern int __get_user_8(void);\nextern int __get_user_bad(void);\n\n#define __uaccess_begin() stac()\n#define __uaccess_end()   clac()\n\n/*\n * This is a type: either unsigned long, if the argument fits into\n * that type, or otherwise unsigned long long.\n */\n#define __inttype(x) \\\n__typeof__(__builtin_choose_expr(sizeof(x) > sizeof(0UL), 0ULL, 0UL))\n\n/**\n * get_user: - Get a simple variable from user space.\n * @x:   Variable to store result.\n * @ptr: Source address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple variable from user space to kernel\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and the result of\n * dereferencing @ptr must be assignable to @x without a cast.\n *\n * Returns zero on success, or -EFAULT on error.\n * On error, the variable @x is set to zero.\n */\n/*\n * Careful: we have to cast the result to the type of the pointer\n * for sign reasons.\n *\n * The use of _ASM_DX as the register specifier is a bit of a\n * simplification, as gcc only cares about it as the starting point\n * and not size: for a 64-bit value it will use %ecx:%edx on 32 bits\n * (%ecx being the next register in gcc's x86 register sequence), and\n * %rdx on 64 bits.\n *\n * Clang/LLVM cares about the size of the register, but still wants\n * the base register for something that ends up being a pair.\n */\n#define get_user(x, ptr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __ret_gu;\t\t\t\t\t\t\t\\\n\tregister __inttype(*(ptr)) __val_gu asm(\"%\"_ASM_DX);\t\t\\\n\tregister void *__sp asm(_ASM_SP);\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tmight_fault();\t\t\t\t\t\t\t\\\n\tasm volatile(\"call __get_user_%P4\"\t\t\t\t\\\n\t\t     : \"=a\" (__ret_gu), \"=r\" (__val_gu), \"+r\" (__sp)\t\\\n\t\t     : \"0\" (ptr), \"i\" (sizeof(*(ptr))));\t\t\\\n\t(x) = (__force __typeof__(*(ptr))) __val_gu;\t\t\t\\\n\t__builtin_expect(__ret_gu, 0);\t\t\t\t\t\\\n})\n\n#define __put_user_x(size, x, ptr, __ret_pu)\t\t\t\\\n\tasm volatile(\"call __put_user_\" #size : \"=a\" (__ret_pu)\t\\\n\t\t     : \"0\" ((typeof(*(ptr)))(x)), \"c\" (ptr) : \"ebx\")\n\n\n\n#ifdef CONFIG_X86_32\n#define __put_user_asm_u64(x, addr, err, errret)\t\t\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmovl %%eax,0(%2)\\n\"\t\t\t\\\n\t\t     \"2:\tmovl %%edx,4(%2)\\n\"\t\t\t\\\n\t\t     \"3:\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"4:\tmovl %3,%0\\n\"\t\t\t\t\\\n\t\t     \"\tjmp 3b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE(1b, 4b)\t\t\t\t\\\n\t\t     _ASM_EXTABLE(2b, 4b)\t\t\t\t\\\n\t\t     : \"=r\" (err)\t\t\t\t\t\\\n\t\t     : \"A\" (x), \"r\" (addr), \"i\" (errret), \"0\" (err))\n\n#define __put_user_asm_ex_u64(x, addr)\t\t\t\t\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmovl %%eax,0(%1)\\n\"\t\t\t\\\n\t\t     \"2:\tmovl %%edx,4(%1)\\n\"\t\t\t\\\n\t\t     \"3:\"\t\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 2b)\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(2b, 3b)\t\t\t\t\\\n\t\t     : : \"A\" (x), \"r\" (addr))\n\n#define __put_user_x8(x, ptr, __ret_pu)\t\t\t\t\\\n\tasm volatile(\"call __put_user_8\" : \"=a\" (__ret_pu)\t\\\n\t\t     : \"A\" ((typeof(*(ptr)))(x)), \"c\" (ptr) : \"ebx\")\n#else\n#define __put_user_asm_u64(x, ptr, retval, errret) \\\n\t__put_user_asm(x, ptr, retval, \"q\", \"\", \"er\", errret)\n#define __put_user_asm_ex_u64(x, addr)\t\\\n\t__put_user_asm_ex(x, addr, \"q\", \"\", \"er\")\n#define __put_user_x8(x, ptr, __ret_pu) __put_user_x(8, x, ptr, __ret_pu)\n#endif\n\nextern void __put_user_bad(void);\n\n/*\n * Strange magic calling convention: pointer in %ecx,\n * value in %eax(:%edx), return value in %eax. clobbers %rbx\n */\nextern void __put_user_1(void);\nextern void __put_user_2(void);\nextern void __put_user_4(void);\nextern void __put_user_8(void);\n\n/**\n * put_user: - Write a simple value into user space.\n * @x:   Value to copy to user space.\n * @ptr: Destination address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple value from kernel space to user\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and @x must be assignable\n * to the result of dereferencing @ptr.\n *\n * Returns zero on success, or -EFAULT on error.\n */\n#define put_user(x, ptr)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tint __ret_pu;\t\t\t\t\t\t\\\n\t__typeof__(*(ptr)) __pu_val;\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\\\n\tmight_fault();\t\t\t\t\t\t\\\n\t__pu_val = x;\t\t\t\t\t\t\\\n\tswitch (sizeof(*(ptr))) {\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(1, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(2, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(4, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\\\n\t\t__put_user_x8(__pu_val, ptr, __ret_pu);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\\\n\t\t__put_user_x(X, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\t__builtin_expect(__ret_pu, 0);\t\t\t\t\\\n})\n\n#define __put_user_size(x, ptr, size, retval, errret)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tretval = 0;\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm(x, ptr, retval, \"b\", \"b\", \"iq\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm(x, ptr, retval, \"w\", \"w\", \"ir\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm(x, ptr, retval, \"l\", \"k\", \"ir\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_u64((__typeof__(*ptr))(x), ptr, retval,\t\\\n\t\t\t\t   errret);\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__put_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * This doesn't do __uaccess_begin/end - the exception handling\n * around it must do that.\n */\n#define __put_user_size_ex(x, ptr, size)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"b\", \"b\", \"iq\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"w\", \"w\", \"ir\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"l\", \"k\", \"ir\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex_u64((__typeof__(*ptr))(x), ptr);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__put_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#ifdef CONFIG_X86_32\n#define __get_user_asm_u64(x, ptr, retval, errret)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__typeof__(ptr) __ptr = (ptr);\t\t\t\t\t\\\n\tasm volatile(ASM_STAC \"\\n\"\t\t\t\t\t\\\n\t\t     \"1:\tmovl %2,%%eax\\n\"\t\t\t\\\n\t\t     \"2:\tmovl %3,%%edx\\n\"\t\t\t\\\n\t\t     \"3: \" ASM_CLAC \"\\n\"\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"4:\tmov %4,%0\\n\"\t\t\t\t\\\n\t\t     \"\txorl %%eax,%%eax\\n\"\t\t\t\t\\\n\t\t     \"\txorl %%edx,%%edx\\n\"\t\t\t\t\\\n\t\t     \"\tjmp 3b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE(1b, 4b)\t\t\t\t\\\n\t\t     _ASM_EXTABLE(2b, 4b)\t\t\t\t\\\n\t\t     : \"=r\" (retval), \"=A\"(x)\t\t\t\t\\\n\t\t     : \"m\" (__m(__ptr)), \"m\" __m(((u32 *)(__ptr)) + 1),\t\\\n\t\t       \"i\" (errret), \"0\" (retval));\t\t\t\\\n})\n\n#define __get_user_asm_ex_u64(x, ptr)\t\t\t(x) = __get_user_bad()\n#else\n#define __get_user_asm_u64(x, ptr, retval, errret) \\\n\t __get_user_asm(x, ptr, retval, \"q\", \"\", \"=r\", errret)\n#define __get_user_asm_ex_u64(x, ptr) \\\n\t __get_user_asm_ex(x, ptr, \"q\", \"\", \"=r\")\n#endif\n\n#define __get_user_size(x, ptr, size, retval, errret)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tretval = 0;\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"b\", \"b\", \"=q\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"w\", \"w\", \"=r\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"l\", \"k\", \"=r\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_u64(x, ptr, retval, errret);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t(x) = __get_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define __get_user_asm(x, addr, err, itype, rtype, ltype, errret)\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmov\"itype\" %2,%\"rtype\"1\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"3:\tmov %3,%0\\n\"\t\t\t\t\\\n\t\t     \"\txor\"itype\" %\"rtype\"1,%\"rtype\"1\\n\"\t\t\\\n\t\t     \"\tjmp 2b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t     : \"=r\" (err), ltype(x)\t\t\t\t\\\n\t\t     : \"m\" (__m(addr)), \"i\" (errret), \"0\" (err))\n\n/*\n * This doesn't do __uaccess_begin/end - the exception handling\n * around it must do that.\n */\n#define __get_user_size_ex(x, ptr, size)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"b\", \"b\", \"=q\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"w\", \"w\", \"=r\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"l\", \"k\", \"=r\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex_u64(x, ptr);\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t(x) = __get_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define __get_user_asm_ex(x, addr, itype, rtype, ltype)\t\t\t\\\n\tasm volatile(\"1:\tmov\"itype\" %1,%\"rtype\"0\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 2b)\t\t\t\t\\\n\t\t     : ltype(x) : \"m\" (__m(addr)))\n\n#define __put_user_nocheck(x, ptr, size)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tint __pu_err;\t\t\t\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\\\n\t__put_user_size((x), (ptr), (size), __pu_err, -EFAULT);\t\\\n\t__uaccess_end();\t\t\t\t\t\\\n\t__builtin_expect(__pu_err, 0);\t\t\t\t\\\n})\n\n#define __get_user_nocheck(x, ptr, size)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __gu_err;\t\t\t\t\t\t\t\\\n\t__inttype(*(ptr)) __gu_val;\t\t\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\t\\\n\t__get_user_size(__gu_val, (ptr), (size), __gu_err, -EFAULT);\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gu_val;\t\t\t\\\n\t__builtin_expect(__gu_err, 0);\t\t\t\t\t\\\n})\n\n/* FIXME: this hack is definitely wrong -AK */\nstruct __large_struct { unsigned long buf[100]; };\n#define __m(x) (*(struct __large_struct __user *)(x))\n\n/*\n * Tell gcc we read from memory instead of writing: this is because\n * we do not write to any memory gcc knows about, so there are no\n * aliasing issues.\n */\n#define __put_user_asm(x, addr, err, itype, rtype, ltype, errret)\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmov\"itype\" %\"rtype\"1,%2\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"3:\tmov %3,%0\\n\"\t\t\t\t\\\n\t\t     \"\tjmp 2b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t     : \"=r\"(err)\t\t\t\t\t\\\n\t\t     : ltype(x), \"m\" (__m(addr)), \"i\" (errret), \"0\" (err))\n\n#define __put_user_asm_ex(x, addr, itype, rtype, ltype)\t\t\t\\\n\tasm volatile(\"1:\tmov\"itype\" %\"rtype\"0,%1\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 2b)\t\t\t\t\\\n\t\t     : : ltype(x), \"m\" (__m(addr)))\n\n/*\n * uaccess_try and catch\n */\n#define uaccess_try\tdo {\t\t\t\t\t\t\\\n\tcurrent->thread.uaccess_err = 0;\t\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\t\\\n\tbarrier();\n\n#define uaccess_catch(err)\t\t\t\t\t\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t(err) |= (current->thread.uaccess_err ? -EFAULT : 0);\t\t\\\n} while (0)\n\n/**\n * __get_user: - Get a simple variable from user space, with less checking.\n * @x:   Variable to store result.\n * @ptr: Source address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple variable from user space to kernel\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and the result of\n * dereferencing @ptr must be assignable to @x without a cast.\n *\n * Caller must check the pointer with access_ok() before calling this\n * function.\n *\n * Returns zero on success, or -EFAULT on error.\n * On error, the variable @x is set to zero.\n */\n\n#define __get_user(x, ptr)\t\t\t\t\t\t\\\n\t__get_user_nocheck((x), (ptr), sizeof(*(ptr)))\n\n/**\n * __put_user: - Write a simple value into user space, with less checking.\n * @x:   Value to copy to user space.\n * @ptr: Destination address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple value from kernel space to user\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and @x must be assignable\n * to the result of dereferencing @ptr.\n *\n * Caller must check the pointer with access_ok() before calling this\n * function.\n *\n * Returns zero on success, or -EFAULT on error.\n */\n\n#define __put_user(x, ptr)\t\t\t\t\t\t\\\n\t__put_user_nocheck((__typeof__(*(ptr)))(x), (ptr), sizeof(*(ptr)))\n\n#define __get_user_unaligned __get_user\n#define __put_user_unaligned __put_user\n\n/*\n * {get|put}_user_try and catch\n *\n * get_user_try {\n *\tget_user_ex(...);\n * } get_user_catch(err)\n */\n#define get_user_try\t\tuaccess_try\n#define get_user_catch(err)\tuaccess_catch(err)\n\n#define get_user_ex(x, ptr)\tdo {\t\t\t\t\t\\\n\tunsigned long __gue_val;\t\t\t\t\t\\\n\t__get_user_size_ex((__gue_val), (ptr), (sizeof(*(ptr))));\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gue_val;\t\t\t\\\n} while (0)\n\n#define put_user_try\t\tuaccess_try\n#define put_user_catch(err)\tuaccess_catch(err)\n\n#define put_user_ex(x, ptr)\t\t\t\t\t\t\\\n\t__put_user_size_ex((__typeof__(*(ptr)))(x), (ptr), sizeof(*(ptr)))\n\nextern unsigned long\ncopy_from_user_nmi(void *to, const void __user *from, unsigned long n);\nextern __must_check long\nstrncpy_from_user(char *dst, const char __user *src, long count);\n\nextern __must_check long strlen_user(const char __user *str);\nextern __must_check long strnlen_user(const char __user *str, long n);\n\nunsigned long __must_check clear_user(void __user *mem, unsigned long len);\nunsigned long __must_check __clear_user(void __user *mem, unsigned long len);\n\nextern void __cmpxchg_wrong_size(void)\n\t__compiletime_error(\"Bad argument size for cmpxchg\");\n\n#define __user_atomic_cmpxchg_inatomic(uval, ptr, old, new, size)\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __ret = 0;\t\t\t\t\t\t\t\\\n\t__typeof__(ptr) __uval = (uval);\t\t\t\t\\\n\t__typeof__(*(ptr)) __old = (old);\t\t\t\t\\\n\t__typeof__(*(ptr)) __new = (new);\t\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgb %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"q\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgw %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgl %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tif (!IS_ENABLED(CONFIG_X86_64))\t\t\t\t\\\n\t\t\t__cmpxchg_wrong_size();\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgq %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__cmpxchg_wrong_size();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t*__uval = __old;\t\t\t\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define user_atomic_cmpxchg_inatomic(uval, ptr, old, new)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\taccess_ok(VERIFY_WRITE, (ptr), sizeof(*(ptr))) ?\t\t\\\n\t\t__user_atomic_cmpxchg_inatomic((uval), (ptr),\t\t\\\n\t\t\t\t(old), (new), sizeof(*(ptr))) :\t\t\\\n\t\t-EFAULT;\t\t\t\t\t\t\\\n})\n\n/*\n * movsl can be slow when source and dest are not both 8-byte aligned\n */\n#ifdef CONFIG_X86_INTEL_USERCOPY\nextern struct movsl_mask {\n\tint mask;\n} ____cacheline_aligned_in_smp movsl_mask;\n#endif\n\n#define ARCH_HAS_NOCACHE_UACCESS 1\n\n#ifdef CONFIG_X86_32\n# include <asm/uaccess_32.h>\n#else\n# include <asm/uaccess_64.h>\n#endif\n\nunsigned long __must_check _copy_from_user(void *to, const void __user *from,\n\t\t\t\t\t   unsigned n);\nunsigned long __must_check _copy_to_user(void __user *to, const void *from,\n\t\t\t\t\t unsigned n);\n\nextern void __compiletime_error(\"usercopy buffer size is too small\")\n__bad_copy_user(void);\n\nstatic inline void copy_user_overflow(int size, unsigned long count)\n{\n\tWARN(1, \"Buffer overflow detected (%d < %lu)!\\n\", size, count);\n}\n\nstatic __always_inline unsigned long __must_check\ncopy_from_user(void *to, const void __user *from, unsigned long n)\n{\n\tint sz = __compiletime_object_size(to);\n\n\tmight_fault();\n\n\tkasan_check_write(to, n);\n\n\tif (likely(sz < 0 || sz >= n)) {\n\t\tcheck_object_size(to, n, false);\n\t\tn = _copy_from_user(to, from, n);\n\t} else if (!__builtin_constant_p(n))\n\t\tcopy_user_overflow(sz, n);\n\telse\n\t\t__bad_copy_user();\n\n\treturn n;\n}\n\nstatic __always_inline unsigned long __must_check\ncopy_to_user(void __user *to, const void *from, unsigned long n)\n{\n\tint sz = __compiletime_object_size(from);\n\n\tkasan_check_read(from, n);\n\n\tmight_fault();\n\n\tif (likely(sz < 0 || sz >= n)) {\n\t\tcheck_object_size(from, n, true);\n\t\tn = _copy_to_user(to, from, n);\n\t} else if (!__builtin_constant_p(n))\n\t\tcopy_user_overflow(sz, n);\n\telse\n\t\t__bad_copy_user();\n\n\treturn n;\n}\n\n/*\n * We rely on the nested NMI work to allow atomic faults from the NMI path; the\n * nested NMI paths are careful to preserve CR2.\n *\n * Caller must use pagefault_enable/disable, or run in interrupt context,\n * and also do a uaccess_ok() check\n */\n#define __copy_from_user_nmi __copy_from_user_inatomic\n\n/*\n * The \"unsafe\" user accesses aren't really \"unsafe\", but the naming\n * is a big fat warning: you have to not only do the access_ok()\n * checking before using them, but you have to surround them with the\n * user_access_begin/end() pair.\n */\n#define user_access_begin()\t__uaccess_begin()\n#define user_access_end()\t__uaccess_end()\n\n#define unsafe_put_user(x, ptr, err_label)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\tint __pu_err;\t\t\t\t\t\t\t\t\\\n\t__put_user_size((x), (ptr), sizeof(*(ptr)), __pu_err, -EFAULT);\t\t\\\n\tif (unlikely(__pu_err)) goto err_label;\t\t\t\t\t\\\n} while (0)\n\n#define unsafe_get_user(x, ptr, err_label)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\tint __gu_err;\t\t\t\t\t\t\t\t\\\n\tunsigned long __gu_val;\t\t\t\t\t\t\t\\\n\t__get_user_size(__gu_val, (ptr), sizeof(*(ptr)), __gu_err, -EFAULT);\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gu_val;\t\t\t\t\\\n\tif (unlikely(__gu_err)) goto err_label;\t\t\t\t\t\\\n} while (0)\n\n#endif /* _ASM_X86_UACCESS_H */\n\n"], "fixing_code": ["#ifndef _ASM_X86_UACCESS_H\n#define _ASM_X86_UACCESS_H\n/*\n * User space memory access functions\n */\n#include <linux/errno.h>\n#include <linux/compiler.h>\n#include <linux/kasan-checks.h>\n#include <linux/thread_info.h>\n#include <linux/string.h>\n#include <asm/asm.h>\n#include <asm/page.h>\n#include <asm/smap.h>\n\n#define VERIFY_READ 0\n#define VERIFY_WRITE 1\n\n/*\n * The fs value determines whether argument validity checking should be\n * performed or not.  If get_fs() == USER_DS, checking is performed, with\n * get_fs() == KERNEL_DS, checking is bypassed.\n *\n * For historical reasons, these macros are grossly misnamed.\n */\n\n#define MAKE_MM_SEG(s)\t((mm_segment_t) { (s) })\n\n#define KERNEL_DS\tMAKE_MM_SEG(-1UL)\n#define USER_DS \tMAKE_MM_SEG(TASK_SIZE_MAX)\n\n#define get_ds()\t(KERNEL_DS)\n#define get_fs()\t(current->thread.addr_limit)\n#define set_fs(x)\t(current->thread.addr_limit = (x))\n\n#define segment_eq(a, b)\t((a).seg == (b).seg)\n\n#define user_addr_max() (current->thread.addr_limit.seg)\n#define __addr_ok(addr) \t\\\n\t((unsigned long __force)(addr) < user_addr_max())\n\n/*\n * Test whether a block of memory is a valid user space address.\n * Returns 0 if the range is valid, nonzero otherwise.\n */\nstatic inline bool __chk_range_not_ok(unsigned long addr, unsigned long size, unsigned long limit)\n{\n\t/*\n\t * If we have used \"sizeof()\" for the size,\n\t * we know it won't overflow the limit (but\n\t * it might overflow the 'addr', so it's\n\t * important to subtract the size from the\n\t * limit, not add it to the address).\n\t */\n\tif (__builtin_constant_p(size))\n\t\treturn unlikely(addr > limit - size);\n\n\t/* Arbitrary sizes? Be careful about overflow */\n\taddr += size;\n\tif (unlikely(addr < size))\n\t\treturn true;\n\treturn unlikely(addr > limit);\n}\n\n#define __range_not_ok(addr, size, limit)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(addr);\t\t\t\t\t\t\\\n\t__chk_range_not_ok((unsigned long __force)(addr), size, limit); \\\n})\n\n/**\n * access_ok: - Checks if a user space pointer is valid\n * @type: Type of access: %VERIFY_READ or %VERIFY_WRITE.  Note that\n *        %VERIFY_WRITE is a superset of %VERIFY_READ - if it is safe\n *        to write to a block, it is always safe to read from it.\n * @addr: User space pointer to start of block to check\n * @size: Size of block to check\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * Checks if a pointer to a block of memory in user space is valid.\n *\n * Returns true (nonzero) if the memory block may be valid, false (zero)\n * if it is definitely invalid.\n *\n * Note that, depending on architecture, this function probably just\n * checks that the pointer is in the user space range - after calling\n * this function, memory access functions may still return -EFAULT.\n */\n#define access_ok(type, addr, size) \\\n\tlikely(!__range_not_ok(addr, size, user_addr_max()))\n\n/*\n * The exception table consists of triples of addresses relative to the\n * exception table entry itself. The first address is of an instruction\n * that is allowed to fault, the second is the target at which the program\n * should continue. The third is a handler function to deal with the fault\n * caused by the instruction in the first field.\n *\n * All the routines below use bits of fixup code that are out of line\n * with the main instruction path.  This means when everything is well,\n * we don't even have to jump over them.  Further, they do not intrude\n * on our cache or tlb entries.\n */\n\nstruct exception_table_entry {\n\tint insn, fixup, handler;\n};\n\n#define ARCH_HAS_RELATIVE_EXTABLE\n\n#define swap_ex_entry_fixup(a, b, tmp, delta)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\t(a)->fixup = (b)->fixup + (delta);\t\t\\\n\t\t(b)->fixup = (tmp).fixup - (delta);\t\t\\\n\t\t(a)->handler = (b)->handler + (delta);\t\t\\\n\t\t(b)->handler = (tmp).handler - (delta);\t\t\\\n\t} while (0)\n\nextern int fixup_exception(struct pt_regs *regs, int trapnr);\nextern bool ex_has_fault_handler(unsigned long ip);\nextern void early_fixup_exception(struct pt_regs *regs, int trapnr);\n\n/*\n * These are the main single-value transfer routines.  They automatically\n * use the right size if we just have the right pointer type.\n *\n * This gets kind of ugly. We want to return _two_ values in \"get_user()\"\n * and yet we don't want to do any pointers, because that is too much\n * of a performance impact. Thus we have a few rather ugly macros here,\n * and hide all the ugliness from the user.\n *\n * The \"__xxx\" versions of the user access functions are versions that\n * do not verify the address space, that must have been done previously\n * with a separate \"access_ok()\" call (this is used when we do multiple\n * accesses to the same area of user memory).\n */\n\nextern int __get_user_1(void);\nextern int __get_user_2(void);\nextern int __get_user_4(void);\nextern int __get_user_8(void);\nextern int __get_user_bad(void);\n\n#define __uaccess_begin() stac()\n#define __uaccess_end()   clac()\n\n/*\n * This is a type: either unsigned long, if the argument fits into\n * that type, or otherwise unsigned long long.\n */\n#define __inttype(x) \\\n__typeof__(__builtin_choose_expr(sizeof(x) > sizeof(0UL), 0ULL, 0UL))\n\n/**\n * get_user: - Get a simple variable from user space.\n * @x:   Variable to store result.\n * @ptr: Source address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple variable from user space to kernel\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and the result of\n * dereferencing @ptr must be assignable to @x without a cast.\n *\n * Returns zero on success, or -EFAULT on error.\n * On error, the variable @x is set to zero.\n */\n/*\n * Careful: we have to cast the result to the type of the pointer\n * for sign reasons.\n *\n * The use of _ASM_DX as the register specifier is a bit of a\n * simplification, as gcc only cares about it as the starting point\n * and not size: for a 64-bit value it will use %ecx:%edx on 32 bits\n * (%ecx being the next register in gcc's x86 register sequence), and\n * %rdx on 64 bits.\n *\n * Clang/LLVM cares about the size of the register, but still wants\n * the base register for something that ends up being a pair.\n */\n#define get_user(x, ptr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __ret_gu;\t\t\t\t\t\t\t\\\n\tregister __inttype(*(ptr)) __val_gu asm(\"%\"_ASM_DX);\t\t\\\n\tregister void *__sp asm(_ASM_SP);\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tmight_fault();\t\t\t\t\t\t\t\\\n\tasm volatile(\"call __get_user_%P4\"\t\t\t\t\\\n\t\t     : \"=a\" (__ret_gu), \"=r\" (__val_gu), \"+r\" (__sp)\t\\\n\t\t     : \"0\" (ptr), \"i\" (sizeof(*(ptr))));\t\t\\\n\t(x) = (__force __typeof__(*(ptr))) __val_gu;\t\t\t\\\n\t__builtin_expect(__ret_gu, 0);\t\t\t\t\t\\\n})\n\n#define __put_user_x(size, x, ptr, __ret_pu)\t\t\t\\\n\tasm volatile(\"call __put_user_\" #size : \"=a\" (__ret_pu)\t\\\n\t\t     : \"0\" ((typeof(*(ptr)))(x)), \"c\" (ptr) : \"ebx\")\n\n\n\n#ifdef CONFIG_X86_32\n#define __put_user_asm_u64(x, addr, err, errret)\t\t\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmovl %%eax,0(%2)\\n\"\t\t\t\\\n\t\t     \"2:\tmovl %%edx,4(%2)\\n\"\t\t\t\\\n\t\t     \"3:\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"4:\tmovl %3,%0\\n\"\t\t\t\t\\\n\t\t     \"\tjmp 3b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE(1b, 4b)\t\t\t\t\\\n\t\t     _ASM_EXTABLE(2b, 4b)\t\t\t\t\\\n\t\t     : \"=r\" (err)\t\t\t\t\t\\\n\t\t     : \"A\" (x), \"r\" (addr), \"i\" (errret), \"0\" (err))\n\n#define __put_user_asm_ex_u64(x, addr)\t\t\t\t\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmovl %%eax,0(%1)\\n\"\t\t\t\\\n\t\t     \"2:\tmovl %%edx,4(%1)\\n\"\t\t\t\\\n\t\t     \"3:\"\t\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 2b)\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(2b, 3b)\t\t\t\t\\\n\t\t     : : \"A\" (x), \"r\" (addr))\n\n#define __put_user_x8(x, ptr, __ret_pu)\t\t\t\t\\\n\tasm volatile(\"call __put_user_8\" : \"=a\" (__ret_pu)\t\\\n\t\t     : \"A\" ((typeof(*(ptr)))(x)), \"c\" (ptr) : \"ebx\")\n#else\n#define __put_user_asm_u64(x, ptr, retval, errret) \\\n\t__put_user_asm(x, ptr, retval, \"q\", \"\", \"er\", errret)\n#define __put_user_asm_ex_u64(x, addr)\t\\\n\t__put_user_asm_ex(x, addr, \"q\", \"\", \"er\")\n#define __put_user_x8(x, ptr, __ret_pu) __put_user_x(8, x, ptr, __ret_pu)\n#endif\n\nextern void __put_user_bad(void);\n\n/*\n * Strange magic calling convention: pointer in %ecx,\n * value in %eax(:%edx), return value in %eax. clobbers %rbx\n */\nextern void __put_user_1(void);\nextern void __put_user_2(void);\nextern void __put_user_4(void);\nextern void __put_user_8(void);\n\n/**\n * put_user: - Write a simple value into user space.\n * @x:   Value to copy to user space.\n * @ptr: Destination address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple value from kernel space to user\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and @x must be assignable\n * to the result of dereferencing @ptr.\n *\n * Returns zero on success, or -EFAULT on error.\n */\n#define put_user(x, ptr)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tint __ret_pu;\t\t\t\t\t\t\\\n\t__typeof__(*(ptr)) __pu_val;\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\\\n\tmight_fault();\t\t\t\t\t\t\\\n\t__pu_val = x;\t\t\t\t\t\t\\\n\tswitch (sizeof(*(ptr))) {\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(1, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(2, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\\\n\t\t__put_user_x(4, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\\\n\t\t__put_user_x8(__pu_val, ptr, __ret_pu);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\\\n\t\t__put_user_x(X, __pu_val, ptr, __ret_pu);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\t__builtin_expect(__ret_pu, 0);\t\t\t\t\\\n})\n\n#define __put_user_size(x, ptr, size, retval, errret)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tretval = 0;\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm(x, ptr, retval, \"b\", \"b\", \"iq\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm(x, ptr, retval, \"w\", \"w\", \"ir\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm(x, ptr, retval, \"l\", \"k\", \"ir\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_u64((__typeof__(*ptr))(x), ptr, retval,\t\\\n\t\t\t\t   errret);\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__put_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n/*\n * This doesn't do __uaccess_begin/end - the exception handling\n * around it must do that.\n */\n#define __put_user_size_ex(x, ptr, size)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"b\", \"b\", \"iq\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"w\", \"w\", \"ir\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex(x, ptr, \"l\", \"k\", \"ir\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__put_user_asm_ex_u64((__typeof__(*ptr))(x), ptr);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__put_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#ifdef CONFIG_X86_32\n#define __get_user_asm_u64(x, ptr, retval, errret)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__typeof__(ptr) __ptr = (ptr);\t\t\t\t\t\\\n\tasm volatile(ASM_STAC \"\\n\"\t\t\t\t\t\\\n\t\t     \"1:\tmovl %2,%%eax\\n\"\t\t\t\\\n\t\t     \"2:\tmovl %3,%%edx\\n\"\t\t\t\\\n\t\t     \"3: \" ASM_CLAC \"\\n\"\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"4:\tmov %4,%0\\n\"\t\t\t\t\\\n\t\t     \"\txorl %%eax,%%eax\\n\"\t\t\t\t\\\n\t\t     \"\txorl %%edx,%%edx\\n\"\t\t\t\t\\\n\t\t     \"\tjmp 3b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE(1b, 4b)\t\t\t\t\\\n\t\t     _ASM_EXTABLE(2b, 4b)\t\t\t\t\\\n\t\t     : \"=r\" (retval), \"=A\"(x)\t\t\t\t\\\n\t\t     : \"m\" (__m(__ptr)), \"m\" __m(((u32 *)(__ptr)) + 1),\t\\\n\t\t       \"i\" (errret), \"0\" (retval));\t\t\t\\\n})\n\n#define __get_user_asm_ex_u64(x, ptr)\t\t\t(x) = __get_user_bad()\n#else\n#define __get_user_asm_u64(x, ptr, retval, errret) \\\n\t __get_user_asm(x, ptr, retval, \"q\", \"\", \"=r\", errret)\n#define __get_user_asm_ex_u64(x, ptr) \\\n\t __get_user_asm_ex(x, ptr, \"q\", \"\", \"=r\")\n#endif\n\n#define __get_user_size(x, ptr, size, retval, errret)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tretval = 0;\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"b\", \"b\", \"=q\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"w\", \"w\", \"=r\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm(x, ptr, retval, \"l\", \"k\", \"=r\", errret);\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_u64(x, ptr, retval, errret);\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t(x) = __get_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define __get_user_asm(x, addr, err, itype, rtype, ltype, errret)\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmov\"itype\" %2,%\"rtype\"1\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"3:\tmov %3,%0\\n\"\t\t\t\t\\\n\t\t     \"\txor\"itype\" %\"rtype\"1,%\"rtype\"1\\n\"\t\t\\\n\t\t     \"\tjmp 2b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t     : \"=r\" (err), ltype(x)\t\t\t\t\\\n\t\t     : \"m\" (__m(addr)), \"i\" (errret), \"0\" (err))\n\n/*\n * This doesn't do __uaccess_begin/end - the exception handling\n * around it must do that.\n */\n#define __get_user_size_ex(x, ptr, size)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__chk_user_ptr(ptr);\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"b\", \"b\", \"=q\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"w\", \"w\", \"=r\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex(x, ptr, \"l\", \"k\", \"=r\");\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t\t__get_user_asm_ex_u64(x, ptr);\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t(x) = __get_user_bad();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define __get_user_asm_ex(x, addr, itype, rtype, ltype)\t\t\t\\\n\tasm volatile(\"1:\tmov\"itype\" %1,%\"rtype\"0\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n                     \"3:xor\"itype\" %\"rtype\"0,%\"rtype\"0\\n\"\t\t\\\n\t\t     \"  jmp 2b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 3b)\t\t\t\t\\\n\t\t     : ltype(x) : \"m\" (__m(addr)))\n\n#define __put_user_nocheck(x, ptr, size)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tint __pu_err;\t\t\t\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\\\n\t__put_user_size((x), (ptr), (size), __pu_err, -EFAULT);\t\\\n\t__uaccess_end();\t\t\t\t\t\\\n\t__builtin_expect(__pu_err, 0);\t\t\t\t\\\n})\n\n#define __get_user_nocheck(x, ptr, size)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __gu_err;\t\t\t\t\t\t\t\\\n\t__inttype(*(ptr)) __gu_val;\t\t\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\t\\\n\t__get_user_size(__gu_val, (ptr), (size), __gu_err, -EFAULT);\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gu_val;\t\t\t\\\n\t__builtin_expect(__gu_err, 0);\t\t\t\t\t\\\n})\n\n/* FIXME: this hack is definitely wrong -AK */\nstruct __large_struct { unsigned long buf[100]; };\n#define __m(x) (*(struct __large_struct __user *)(x))\n\n/*\n * Tell gcc we read from memory instead of writing: this is because\n * we do not write to any memory gcc knows about, so there are no\n * aliasing issues.\n */\n#define __put_user_asm(x, addr, err, itype, rtype, ltype, errret)\t\\\n\tasm volatile(\"\\n\"\t\t\t\t\t\t\\\n\t\t     \"1:\tmov\"itype\" %\"rtype\"1,%2\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     \".section .fixup,\\\"ax\\\"\\n\"\t\t\t\t\\\n\t\t     \"3:\tmov %3,%0\\n\"\t\t\t\t\\\n\t\t     \"\tjmp 2b\\n\"\t\t\t\t\t\\\n\t\t     \".previous\\n\"\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t     : \"=r\"(err)\t\t\t\t\t\\\n\t\t     : ltype(x), \"m\" (__m(addr)), \"i\" (errret), \"0\" (err))\n\n#define __put_user_asm_ex(x, addr, itype, rtype, ltype)\t\t\t\\\n\tasm volatile(\"1:\tmov\"itype\" %\"rtype\"0,%1\\n\"\t\t\\\n\t\t     \"2:\\n\"\t\t\t\t\t\t\\\n\t\t     _ASM_EXTABLE_EX(1b, 2b)\t\t\t\t\\\n\t\t     : : ltype(x), \"m\" (__m(addr)))\n\n/*\n * uaccess_try and catch\n */\n#define uaccess_try\tdo {\t\t\t\t\t\t\\\n\tcurrent->thread.uaccess_err = 0;\t\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\t\\\n\tbarrier();\n\n#define uaccess_catch(err)\t\t\t\t\t\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t(err) |= (current->thread.uaccess_err ? -EFAULT : 0);\t\t\\\n} while (0)\n\n/**\n * __get_user: - Get a simple variable from user space, with less checking.\n * @x:   Variable to store result.\n * @ptr: Source address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple variable from user space to kernel\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and the result of\n * dereferencing @ptr must be assignable to @x without a cast.\n *\n * Caller must check the pointer with access_ok() before calling this\n * function.\n *\n * Returns zero on success, or -EFAULT on error.\n * On error, the variable @x is set to zero.\n */\n\n#define __get_user(x, ptr)\t\t\t\t\t\t\\\n\t__get_user_nocheck((x), (ptr), sizeof(*(ptr)))\n\n/**\n * __put_user: - Write a simple value into user space, with less checking.\n * @x:   Value to copy to user space.\n * @ptr: Destination address, in user space.\n *\n * Context: User context only. This function may sleep if pagefaults are\n *          enabled.\n *\n * This macro copies a single simple value from kernel space to user\n * space.  It supports simple types like char and int, but not larger\n * data types like structures or arrays.\n *\n * @ptr must have pointer-to-simple-variable type, and @x must be assignable\n * to the result of dereferencing @ptr.\n *\n * Caller must check the pointer with access_ok() before calling this\n * function.\n *\n * Returns zero on success, or -EFAULT on error.\n */\n\n#define __put_user(x, ptr)\t\t\t\t\t\t\\\n\t__put_user_nocheck((__typeof__(*(ptr)))(x), (ptr), sizeof(*(ptr)))\n\n#define __get_user_unaligned __get_user\n#define __put_user_unaligned __put_user\n\n/*\n * {get|put}_user_try and catch\n *\n * get_user_try {\n *\tget_user_ex(...);\n * } get_user_catch(err)\n */\n#define get_user_try\t\tuaccess_try\n#define get_user_catch(err)\tuaccess_catch(err)\n\n#define get_user_ex(x, ptr)\tdo {\t\t\t\t\t\\\n\tunsigned long __gue_val;\t\t\t\t\t\\\n\t__get_user_size_ex((__gue_val), (ptr), (sizeof(*(ptr))));\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gue_val;\t\t\t\\\n} while (0)\n\n#define put_user_try\t\tuaccess_try\n#define put_user_catch(err)\tuaccess_catch(err)\n\n#define put_user_ex(x, ptr)\t\t\t\t\t\t\\\n\t__put_user_size_ex((__typeof__(*(ptr)))(x), (ptr), sizeof(*(ptr)))\n\nextern unsigned long\ncopy_from_user_nmi(void *to, const void __user *from, unsigned long n);\nextern __must_check long\nstrncpy_from_user(char *dst, const char __user *src, long count);\n\nextern __must_check long strlen_user(const char __user *str);\nextern __must_check long strnlen_user(const char __user *str, long n);\n\nunsigned long __must_check clear_user(void __user *mem, unsigned long len);\nunsigned long __must_check __clear_user(void __user *mem, unsigned long len);\n\nextern void __cmpxchg_wrong_size(void)\n\t__compiletime_error(\"Bad argument size for cmpxchg\");\n\n#define __user_atomic_cmpxchg_inatomic(uval, ptr, old, new, size)\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tint __ret = 0;\t\t\t\t\t\t\t\\\n\t__typeof__(ptr) __uval = (uval);\t\t\t\t\\\n\t__typeof__(*(ptr)) __old = (old);\t\t\t\t\\\n\t__typeof__(*(ptr)) __new = (new);\t\t\t\t\\\n\t__uaccess_begin();\t\t\t\t\t\t\\\n\tswitch (size) {\t\t\t\t\t\t\t\\\n\tcase 1:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgb %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"q\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 2:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgw %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 4:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgl %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tcase 8:\t\t\t\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tif (!IS_ENABLED(CONFIG_X86_64))\t\t\t\t\\\n\t\t\t__cmpxchg_wrong_size();\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tasm volatile(\"\\n\"\t\t\t\t\t\\\n\t\t\t\"1:\\t\" LOCK_PREFIX \"cmpxchgq %4, %2\\n\"\t\t\\\n\t\t\t\"2:\\n\"\t\t\t\t\t\t\\\n\t\t\t\"\\t.section .fixup, \\\"ax\\\"\\n\"\t\t\t\\\n\t\t\t\"3:\\tmov     %3, %0\\n\"\t\t\t\t\\\n\t\t\t\"\\tjmp     2b\\n\"\t\t\t\t\\\n\t\t\t\"\\t.previous\\n\"\t\t\t\t\t\\\n\t\t\t_ASM_EXTABLE(1b, 3b)\t\t\t\t\\\n\t\t\t: \"+r\" (__ret), \"=a\" (__old), \"+m\" (*(ptr))\t\\\n\t\t\t: \"i\" (-EFAULT), \"r\" (__new), \"1\" (__old)\t\\\n\t\t\t: \"memory\"\t\t\t\t\t\\\n\t\t);\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tdefault:\t\t\t\t\t\t\t\\\n\t\t__cmpxchg_wrong_size();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t__uaccess_end();\t\t\t\t\t\t\\\n\t*__uval = __old;\t\t\t\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define user_atomic_cmpxchg_inatomic(uval, ptr, old, new)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\taccess_ok(VERIFY_WRITE, (ptr), sizeof(*(ptr))) ?\t\t\\\n\t\t__user_atomic_cmpxchg_inatomic((uval), (ptr),\t\t\\\n\t\t\t\t(old), (new), sizeof(*(ptr))) :\t\t\\\n\t\t-EFAULT;\t\t\t\t\t\t\\\n})\n\n/*\n * movsl can be slow when source and dest are not both 8-byte aligned\n */\n#ifdef CONFIG_X86_INTEL_USERCOPY\nextern struct movsl_mask {\n\tint mask;\n} ____cacheline_aligned_in_smp movsl_mask;\n#endif\n\n#define ARCH_HAS_NOCACHE_UACCESS 1\n\n#ifdef CONFIG_X86_32\n# include <asm/uaccess_32.h>\n#else\n# include <asm/uaccess_64.h>\n#endif\n\nunsigned long __must_check _copy_from_user(void *to, const void __user *from,\n\t\t\t\t\t   unsigned n);\nunsigned long __must_check _copy_to_user(void __user *to, const void *from,\n\t\t\t\t\t unsigned n);\n\nextern void __compiletime_error(\"usercopy buffer size is too small\")\n__bad_copy_user(void);\n\nstatic inline void copy_user_overflow(int size, unsigned long count)\n{\n\tWARN(1, \"Buffer overflow detected (%d < %lu)!\\n\", size, count);\n}\n\nstatic __always_inline unsigned long __must_check\ncopy_from_user(void *to, const void __user *from, unsigned long n)\n{\n\tint sz = __compiletime_object_size(to);\n\n\tmight_fault();\n\n\tkasan_check_write(to, n);\n\n\tif (likely(sz < 0 || sz >= n)) {\n\t\tcheck_object_size(to, n, false);\n\t\tn = _copy_from_user(to, from, n);\n\t} else if (!__builtin_constant_p(n))\n\t\tcopy_user_overflow(sz, n);\n\telse\n\t\t__bad_copy_user();\n\n\treturn n;\n}\n\nstatic __always_inline unsigned long __must_check\ncopy_to_user(void __user *to, const void *from, unsigned long n)\n{\n\tint sz = __compiletime_object_size(from);\n\n\tkasan_check_read(from, n);\n\n\tmight_fault();\n\n\tif (likely(sz < 0 || sz >= n)) {\n\t\tcheck_object_size(from, n, true);\n\t\tn = _copy_to_user(to, from, n);\n\t} else if (!__builtin_constant_p(n))\n\t\tcopy_user_overflow(sz, n);\n\telse\n\t\t__bad_copy_user();\n\n\treturn n;\n}\n\n/*\n * We rely on the nested NMI work to allow atomic faults from the NMI path; the\n * nested NMI paths are careful to preserve CR2.\n *\n * Caller must use pagefault_enable/disable, or run in interrupt context,\n * and also do a uaccess_ok() check\n */\n#define __copy_from_user_nmi __copy_from_user_inatomic\n\n/*\n * The \"unsafe\" user accesses aren't really \"unsafe\", but the naming\n * is a big fat warning: you have to not only do the access_ok()\n * checking before using them, but you have to surround them with the\n * user_access_begin/end() pair.\n */\n#define user_access_begin()\t__uaccess_begin()\n#define user_access_end()\t__uaccess_end()\n\n#define unsafe_put_user(x, ptr, err_label)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\tint __pu_err;\t\t\t\t\t\t\t\t\\\n\t__put_user_size((x), (ptr), sizeof(*(ptr)), __pu_err, -EFAULT);\t\t\\\n\tif (unlikely(__pu_err)) goto err_label;\t\t\t\t\t\\\n} while (0)\n\n#define unsafe_get_user(x, ptr, err_label)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\tint __gu_err;\t\t\t\t\t\t\t\t\\\n\tunsigned long __gu_val;\t\t\t\t\t\t\t\\\n\t__get_user_size(__gu_val, (ptr), sizeof(*(ptr)), __gu_err, -EFAULT);\t\\\n\t(x) = (__force __typeof__(*(ptr)))__gu_val;\t\t\t\t\\\n\tif (unlikely(__gu_err)) goto err_label;\t\t\t\t\t\\\n} while (0)\n\n#endif /* _ASM_X86_UACCESS_H */\n\n"], "filenames": ["arch/x86/include/asm/uaccess.h"], "buggy_code_start_loc": [436], "buggy_code_end_loc": [437], "fixing_code_start_loc": [436], "fixing_code_end_loc": [441], "type": "CWE-200", "message": "The __get_user_asm_ex macro in arch/x86/include/asm/uaccess.h in the Linux kernel before 4.7.5 does not initialize a certain integer variable, which allows local users to obtain sensitive information from kernel stack memory by triggering failure of a get_user_ex call.", "other": {"cve": {"id": "CVE-2016-9178", "sourceIdentifier": "cve@mitre.org", "published": "2016-11-28T03:59:13.080", "lastModified": "2016-11-28T22:06:13.193", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The __get_user_asm_ex macro in arch/x86/include/asm/uaccess.h in the Linux kernel before 4.7.5 does not initialize a certain integer variable, which allows local users to obtain sensitive information from kernel stack memory by triggering failure of a get_user_ex call."}, {"lang": "es", "value": "El macro __get_user_asm_ex en arch/x86/include/asm/uaccess.h en el kernel Linux en versiones anteriores a 4.7.5 no inicia ciertas variables de entero, lo que permite a usuarios locales obtener informaci\u00f3n sensible de la memoria basado en pila del kernel desencadenando un fallo de la llamada get_user_ex."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-200"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.7.4", "matchCriteriaId": "4C613BBC-9849-4519-86AE-ECB4BBC32BD7"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=1c109fabbd51863475cd12ac206bdd249aee35af", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.7.5", "source": "cve@mitre.org", "tags": ["Release Notes"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/11/04/4", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/94144", "source": "cve@mitre.org"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1391908", "source": "cve@mitre.org", "tags": ["Issue Tracking"]}, {"url": "https://github.com/torvalds/linux/commit/1c109fabbd51863475cd12ac206bdd249aee35af", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/1c109fabbd51863475cd12ac206bdd249aee35af"}}