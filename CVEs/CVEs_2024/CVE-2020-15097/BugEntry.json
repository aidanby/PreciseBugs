{"buggy_code": ["/**\n *  DAO\n *  Copyright 22.02.2015 by Michael Peter Christen, @0rb1t3r\n *\n *  This library is free software; you can redistribute it and/or\n *  modify it under the terms of the GNU Lesser General Public\n *  License as published by the Free Software Foundation; either\n *  version 2.1 of the License, or (at your option) any later version.\n *\n *  This library is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n *  Lesser General Public License for more details.\n *\n *  You should have received a copy of the GNU Lesser General Public License\n *  along with this program in the file lgpl21.txt\n *  If not, see <http://www.gnu.org/licenses/>.\n */\n\npackage org.loklak.data;\n\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.github.fge.jackson.JsonLoader;\nimport com.google.common.base.Charsets;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileNotFoundException;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.nio.file.Path;\nimport java.security.KeyPair;\nimport java.security.KeyPairGenerator;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Random;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\nimport org.elasticsearch.cluster.health.ClusterHealthStatus;\nimport org.elasticsearch.common.logging.ESLoggerFactory;\nimport org.elasticsearch.common.logging.slf4j.Slf4jESLoggerFactory;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.RangeQueryBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\nimport org.json.JSONException;\nimport org.json.JSONObject;\nimport org.json.JSONArray;\nimport org.loklak.Caretaker;\nimport org.loklak.api.search.SearchServlet;\nimport org.loklak.api.search.WordpressCrawlerService;\nimport org.loklak.geo.GeoNames;\nimport org.loklak.harvester.Post;\nimport org.loklak.harvester.TwitterScraper;\nimport org.loklak.harvester.YoutubeScraper;\nimport org.loklak.harvester.TwitterScraper.TwitterTweet;\nimport org.loklak.api.search.GithubProfileScraper;\nimport org.loklak.api.search.InstagramProfileScraper;\nimport org.loklak.api.search.QuoraProfileScraper;\nimport org.loklak.api.search.TweetScraper;\nimport org.loklak.harvester.BaseScraper;\nimport org.loklak.http.AccessTracker;\nimport org.loklak.http.ClientConnection;\nimport org.loklak.http.RemoteAccess;\nimport org.loklak.ir.AccountFactory;\nimport org.loklak.ir.BulkWriteResult;\nimport org.loklak.ir.ElasticsearchClient;\nimport org.loklak.ir.ImportProfileFactory;\nimport org.loklak.ir.MessageFactory;\nimport org.loklak.ir.QueryFactory;\nimport org.loklak.ir.UserFactory;\nimport org.loklak.objects.AbstractObjectEntry;\nimport org.loklak.objects.AccountEntry;\nimport org.loklak.objects.ImportProfileEntry;\nimport org.loklak.objects.Peers;\nimport org.loklak.objects.QueryEntry;\nimport org.loklak.objects.ResultList;\nimport org.loklak.objects.SourceType;\nimport org.loklak.objects.TwitterTimeline;\nimport org.loklak.objects.PostTimeline;\nimport org.loklak.objects.TimelineCache;\nimport org.loklak.objects.UserEntry;\nimport org.loklak.server.*;\nimport org.loklak.stream.MQTTPublisher;\nimport org.loklak.tools.DateParser;\nimport org.loklak.tools.OS;\nimport org.loklak.tools.storage.*;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * The Data Access Object for the message project.\n * This provides only static methods because the class methods shall be available for\n * all other classes.\n *\n * To debug, call elasticsearch directly i.e.:\n *\n * get statistics\n * curl localhost:9200/_stats?pretty=true\n *\n * get statistics for message index\n * curl -XGET 'http://127.0.0.1:9200/messages?pretty=true'\n *\n * get mappings in message index\n * curl -XGET \"http://localhost:9200/messages/_mapping?pretty=true\"\n *\n * get search result from message index\n * curl -XGET 'http://127.0.0.1:9200/messages/_search?q=*&pretty=true'\n */\npublic class DAO {\n\n    public final static com.fasterxml.jackson.core.JsonFactory jsonFactory = new com.fasterxml.jackson.core.JsonFactory();\n    public final static com.fasterxml.jackson.databind.ObjectMapper jsonMapper = new com.fasterxml.jackson.databind.ObjectMapper(DAO.jsonFactory);\n    public final static com.fasterxml.jackson.core.type.TypeReference<HashMap<String,Object>> jsonTypeRef = new com.fasterxml.jackson.core.type.TypeReference<HashMap<String,Object>>() {};\n\n    private static Logger logger = LoggerFactory.getLogger(DAO.class);\n    \n    public final static String MESSAGE_DUMP_FILE_PREFIX = \"messages_\";\n    public final static String ACCOUNT_DUMP_FILE_PREFIX = \"accounts_\";\n    public final static String USER_DUMP_FILE_PREFIX = \"users_\";\n    public final static String ACCESS_DUMP_FILE_PREFIX = \"access_\";\n    public final static String FOLLOWERS_DUMP_FILE_PREFIX = \"followers_\";\n    public final static String FOLLOWING_DUMP_FILE_PREFIX = \"following_\";\n    private static final String IMPORT_PROFILE_FILE_PREFIX = \"profile_\";\n\n    public static boolean writeDump;\n\n    public final static int CACHE_MAXSIZE =   10000;\n    public final static int EXIST_MAXSIZE = 4000000;\n\n    public  static File data_dir, conf_dir, bin_dir, html_dir;\n    private static File external_data, assets, dictionaries;\n    public static Settings public_settings, private_settings;\n    private static Path message_dump_dir, account_dump_dir, import_profile_dump_dir;\n    public static Path push_cache_dir;\n    public static JsonRepository message_dump;\n    private static JsonRepository account_dump;\n    private static JsonRepository import_profile_dump;\n    public  static JsonDataset user_dump, followers_dump, following_dump;\n    public  static AccessTracker access;\n    private static File schema_dir, conv_schema_dir;\n    public static ElasticsearchClient elasticsearch_client;\n    //private static Node elasticsearch_node;\n    //private static Client elasticsearch_client;\n    public static UserFactory users;\n    private static AccountFactory accounts;\n    public static MessageFactory messages;\n    public static MessageFactory messages_hour;\n    public static MessageFactory messages_day;\n    public static MessageFactory messages_week;\n    public static QueryFactory queries;\n    private static ImportProfileFactory importProfiles;\n    private static Map<String, String> config = new HashMap<>();\n    public  static GeoNames geoNames = null;\n    public static Peers peers = new Peers();\n    public static OutgoingMessageBuffer outgoingMessages = new OutgoingMessageBuffer();\n\n    // AAA Schema for server usage\n    public static JsonTray authentication;\n    public static JsonTray authorization;\n    public static JsonTray accounting;\n    public static UserRoles userRoles;\n    public static JsonTray passwordreset;\n    public static Map<String, Accounting> accounting_temporary = new HashMap<>();\n    public static JsonFile login_keys;\n    public static TimelineCache timelineCache;\n\n    public static MQTTPublisher mqttPublisher = null;\n    public static boolean streamEnabled = false;\n    public static List<String> randomTerms = new ArrayList<>();\n\n    public static enum IndexName {\n    \tmessages_hour(\"messages.json\"), messages_day(\"messages.json\"), messages_week(\"messages.json\"), messages, queries, users, accounts, import_profiles;\n        private String schemaFileName;\n    \tprivate IndexName() {\n    \t    schemaFileName = this.name() + \".json\";\n    \t}\n    \tprivate IndexName(String filename) {\n            schemaFileName = filename;\n        }\n    \tpublic String getSchemaFilename() {\n    \t    return this.schemaFileName;\n    \t}\n    }\n\n    /**\n     * initialize the DAO\n     * @param configMap\n     * @param dataPath the path to the data directory\n     */\n    public static void init(Map<String, String> configMap, Path dataPath) throws Exception{\n\n        log(\"initializing loklak DAO\");\n\n        config = configMap;\n        data_dir = dataPath.toFile();\n        conf_dir = new File(\"conf\");\n        bin_dir = new File(\"bin\");\n        html_dir = new File(\"html\");\n\n        writeDump = DAO.getConfig(\"dump.write_enabled\", true);\n\n        // initialize public and private keys\n\t\tpublic_settings = new Settings(new File(\"data/settings/public.settings.json\"));\n\t\tFile private_file = new File(\"data/settings/private.settings.json\");\n\t\tprivate_settings = new Settings(private_file);\n\t\tOS.protectPath(private_file.toPath());\n\n\t\tif(!private_settings.loadPrivateKey() || !public_settings.loadPublicKey()){\n        \tlog(\"Can't load key pair. Creating new one\");\n\n        \t// create new key pair\n        \tKeyPairGenerator keyGen;\n\t\t\ttry {\n\t\t\t\tString algorithm = \"RSA\";\n\t\t\t\tkeyGen = KeyPairGenerator.getInstance(algorithm);\n\t\t\t\tkeyGen.initialize(2048);\n\t\t\t\tKeyPair keyPair = keyGen.genKeyPair();\n\t\t\t\tprivate_settings.setPrivateKey(keyPair.getPrivate(), algorithm);\n\t\t\t\tpublic_settings.setPublicKey(keyPair.getPublic(), algorithm);\n\t\t\t} catch (NoSuchAlgorithmException e) {\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t\tlog(\"Key creation finished. Peer hash: \" + public_settings.getPeerHashAlgorithm() + \" \" + public_settings.getPeerHash());\n        }\n        else{\n        \tlog(\"Key pair loaded from file. Peer hash: \" + public_settings.getPeerHashAlgorithm() + \" \" + public_settings.getPeerHash());\n        }\n\n        File datadir = dataPath.toFile();\n        // check if elasticsearch shall be accessed as external cluster\n        String transport = configMap.get(\"elasticsearch_transport.enabled\");\n        if (transport != null && \"true\".equals(transport)) {\n            String cluster_name = configMap.get(\"elasticsearch_transport.cluster.name\");\n            String transport_addresses_string = configMap.get(\"elasticsearch_transport.addresses\");\n            if (transport_addresses_string != null && transport_addresses_string.length() > 0) {\n                String[] transport_addresses = transport_addresses_string.split(\",\");\n                elasticsearch_client = new ElasticsearchClient(transport_addresses, cluster_name);\n            }\n        } else {\n            // use all config attributes with a key starting with \"elasticsearch.\" to set elasticsearch settings\n\n        \tESLoggerFactory.setDefaultFactory(new Slf4jESLoggerFactory());\n            org.elasticsearch.common.settings.Settings.Builder settings = org.elasticsearch.common.settings.Settings.builder();\n            for (Map.Entry<String, String> entry: config.entrySet()) {\n                String key = entry.getKey();\n                if (key.startsWith(\"elasticsearch.\")) settings.put(key.substring(14), entry.getValue());\n            }\n            // patch the home path\n            settings.put(\"path.home\", datadir.getAbsolutePath());\n            settings.put(\"path.data\", datadir.getAbsolutePath());\n            settings.build();\n\n            // start elasticsearch\n            elasticsearch_client = new ElasticsearchClient(settings);\n        }\n\n        // open AAA storage\n        Path settings_dir = dataPath.resolve(\"settings\");\n        settings_dir.toFile().mkdirs();\n        Path authentication_path = settings_dir.resolve(\"authentication.json\");\n        authentication = new JsonTray(authentication_path.toFile(), 10000);\n        OS.protectPath(authentication_path);\n        Path authorization_path = settings_dir.resolve(\"authorization.json\");\n        authorization = new JsonTray(authorization_path.toFile(), 10000);\n        OS.protectPath(authorization_path);\n        Path passwordreset_path = settings_dir.resolve(\"passwordreset.json\");\n        passwordreset = new JsonTray(passwordreset_path.toFile(), 10000);\n        OS.protectPath(passwordreset_path);\n        Path accounting_path = settings_dir.resolve(\"accounting.json\");\n        accounting = new JsonTray(accounting_path.toFile(), 10000);\n        OS.protectPath(accounting_path);\n        Path login_keys_path = settings_dir.resolve(\"login-keys.json\");\n        login_keys = new JsonFile(login_keys_path.toFile());\n        OS.protectPath(login_keys_path);\n\n\n        DAO.log(\"Initializing user roles\");\n\n        Path userRoles_path = settings_dir.resolve(\"userRoles.json\");\n        userRoles = new UserRoles(new JsonFile(userRoles_path.toFile()));\n        OS.protectPath(userRoles_path);\n\n        try{\n            userRoles.loadUserRolesFromObject();\n            DAO.log(\"Loaded user roles from file\");\n        }catch (IllegalArgumentException e){\n            DAO.log(\"Load default user roles\");\n            userRoles.loadDefaultUserRoles();\n        }\n\n        // open index\n        Path index_dir = dataPath.resolve(\"index\");\n        if (index_dir.toFile().exists()) OS.protectPath(index_dir); // no other permissions to this path\n\n        // define the index factories\n        boolean noio = configMap.containsValue(\"noio\") && configMap.get(\"noio\").equals(\"true\");\n        messages = new MessageFactory(noio ? null : elasticsearch_client, IndexName.messages.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        messages_hour = new MessageFactory(noio ? null : elasticsearch_client, IndexName.messages_hour.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        messages_day = new MessageFactory(noio ? null : elasticsearch_client, IndexName.messages_day.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        messages_week = new MessageFactory(noio ? null : elasticsearch_client, IndexName.messages_week.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        users = new UserFactory(noio ? null : elasticsearch_client, IndexName.users.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        accounts = new AccountFactory(noio ? null : elasticsearch_client, IndexName.accounts.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        queries = new QueryFactory(noio ? null : elasticsearch_client, IndexName.queries.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        importProfiles = new ImportProfileFactory(noio ? null : elasticsearch_client, IndexName.import_profiles.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n\n        // create indices and set mapping (that shows how 'elastic' elasticsearch is: it's always good to define data types)\n        File mappingsDir = new File(new File(conf_dir, \"elasticsearch\"), \"mappings\");\n        int shards = Integer.parseInt(configMap.get(\"elasticsearch.index.number_of_shards\"));\n        int replicas = Integer.parseInt(configMap.get(\"elasticsearch.index.number_of_replicas\"));\n        for (IndexName index: IndexName.values()) {\n            log(\"initializing index '\" + index.name() + \"'...\");\n        \ttry {\n        \t    elasticsearch_client.createIndexIfNotExists(index.name(), shards, replicas);\n        \t} catch (Throwable e) {\n        \t\tDAO.severe(e);\n        \t}\n            try {\n                elasticsearch_client.setMapping(index.name(), new File(mappingsDir, index.getSchemaFilename()));\n            } catch (Throwable e) {\n            \tDAO.severe(e);\n            }\n        }\n        // elasticsearch will probably take some time until it is started up. We do some other stuff meanwhile..\n\n        // create and document the data dump dir\n        assets = new File(datadir, \"assets\");\n        external_data = new File(datadir, \"external\");\n        dictionaries = new File(external_data, \"dictionaries\");\n        dictionaries.mkdirs();\n\n        push_cache_dir = dataPath.resolve(\"pushcache\");\n        push_cache_dir.toFile().mkdirs();\n\n        // create message dump dir\n        String message_dump_readme =\n            \"This directory contains dump files for messages which arrived the platform.\\n\" +\n            \"There are three subdirectories for dump files:\\n\" +\n            \"- own:      for messages received with this peer. There is one file for each month.\\n\" +\n            \"- import:   hand-over directory for message dumps to be imported. Drop dumps here and they are imported.\\n\" +\n            \"- imported: dump files which had been processed from the import directory are moved here.\\n\" +\n            \"You can import dump files from other peers by dropping them into the import directory.\\n\" +\n            \"Each dump file must start with the prefix '\" + MESSAGE_DUMP_FILE_PREFIX + \"' to be recognized.\\n\";\n        message_dump_dir = dataPath.resolve(\"dump\");\n        message_dump = new JsonRepository(message_dump_dir.toFile(), MESSAGE_DUMP_FILE_PREFIX, message_dump_readme, JsonRepository.COMPRESSED_MODE, true, Runtime.getRuntime().availableProcessors());\n\n        account_dump_dir = dataPath.resolve(\"accounts\");\n        account_dump_dir.toFile().mkdirs();\n        OS.protectPath(account_dump_dir); // no other permissions to this path\n        account_dump = new JsonRepository(account_dump_dir.toFile(), ACCOUNT_DUMP_FILE_PREFIX, null, JsonRepository.REWRITABLE_MODE, false, Runtime.getRuntime().availableProcessors());\n\n        File user_dump_dir = new File(datadir, \"accounts\");\n        user_dump_dir.mkdirs();\n        log(\"initializing user dump ...\");\n        user_dump = new JsonDataset(\n                user_dump_dir,USER_DUMP_FILE_PREFIX,\n                new JsonDataset.Column[]{new JsonDataset.Column(\"id_str\", false), new JsonDataset.Column(\"screen_name\", true)},\n                \"retrieval_date\", DateParser.PATTERN_ISO8601MILLIS,\n                JsonRepository.REWRITABLE_MODE, false, Integer.MAX_VALUE);\n        log(\"initializing followers dump ...\");\n        followers_dump = new JsonDataset(\n                user_dump_dir, FOLLOWERS_DUMP_FILE_PREFIX,\n                new JsonDataset.Column[]{new JsonDataset.Column(\"screen_name\", true)},\n                \"retrieval_date\", DateParser.PATTERN_ISO8601MILLIS,\n                JsonRepository.REWRITABLE_MODE, false, Integer.MAX_VALUE);\n        log(\"initializing following dump ...\");\n        following_dump = new JsonDataset(\n                user_dump_dir, FOLLOWING_DUMP_FILE_PREFIX,\n                new JsonDataset.Column[]{new JsonDataset.Column(\"screen_name\", true)},\n                \"retrieval_date\", DateParser.PATTERN_ISO8601MILLIS,\n                JsonRepository.REWRITABLE_MODE, false, Integer.MAX_VALUE);\n\n        Path log_dump_dir = dataPath.resolve(\"log\");\n        log_dump_dir.toFile().mkdirs();\n        OS.protectPath(log_dump_dir); // no other permissions to this path\n        access = new AccessTracker(log_dump_dir.toFile(), ACCESS_DUMP_FILE_PREFIX, 60000, 3000);\n        access.start(); // start monitor\n\n        timelineCache = new TimelineCache(60000);\n\n        import_profile_dump_dir = dataPath.resolve(\"import-profiles\");\n        import_profile_dump = new JsonRepository(import_profile_dump_dir.toFile(), IMPORT_PROFILE_FILE_PREFIX, null, JsonRepository.COMPRESSED_MODE, false, Runtime.getRuntime().availableProcessors());\n\n        // load schema folder\n        conv_schema_dir = new File(\"conf/conversion\");\n        schema_dir = new File(\"conf/schema\");\n\n        // load dictionaries if they are embedded here\n        // read the file allCountries.zip from http://download.geonames.org/export/dump/allCountries.zip\n        //File allCountries = new File(dictionaries, \"allCountries.zip\");\n        File cities1000 = new File(dictionaries, \"cities1000.zip\");\n        if (!cities1000.exists()) {\n            // download this file\n            ClientConnection.download(\"http://download.geonames.org/export/dump/cities1000.zip\", cities1000);\n        }\n\n        if(cities1000.exists()){\n\t        try{\n\t        \tgeoNames = new GeoNames(cities1000, new File(conf_dir, \"iso3166.json\"), 1);\n\t        }catch(IOException e){\n\t        \tDAO.severe(e.getMessage());\n\t        \tcities1000.delete();\n\t        \tgeoNames = null;\n\t        }\n    \t}\n\n        // Connect to MQTT message broker\n        String mqttAddress = getConfig(\"stream.mqtt.address\", \"tcp://127.0.0.1:1883\");\n        streamEnabled = getConfig(\"stream.enabled\", false);\n        if (streamEnabled) {\n            mqttPublisher = new MQTTPublisher(mqttAddress);\n        }\n\n        // finally wait for healthy status of elasticsearch shards\n        ClusterHealthStatus required_status = ClusterHealthStatus.fromString(config.get(\"elasticsearch_requiredClusterHealthStatus\"));\n        boolean ok;\n        do {\n            log(\"Waiting for elasticsearch \" + required_status.name() + \" status\");\n            ok = elasticsearch_client.wait_ready(60000l, required_status);\n        } while (!ok);\n        /**\n        do {\n            log(\"Waiting for elasticsearch green status\");\n            health = elasticsearch_client.admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet();\n        } while (health.isTimedOut());\n        **/\n        log(\"elasticsearch has started up!\");\n\n        // start the classifier\n        new Thread(){\n            public void run() {\n                log(\"initializing the classifier...\");\n                try {\n                    Classifier.init(10000, 1000);\n                } catch (Throwable e) {\n                \tDAO.severe(e);\n                }\n                log(\"classifier initialized!\");\n            }\n        }.start();\n\n        log(\"initializing queries...\");\n        File harvestingPath = new File(datadir, \"queries\");\n        if (!harvestingPath.exists()) harvestingPath.mkdirs();\n        String[] list = harvestingPath.list();\n        if (list.length < 10) {\n            // use the test data instead\n            harvestingPath = new File(new File(datadir.getParentFile(), \"test\"), \"queries\");\n            list = harvestingPath.list();\n        }\n        for (String queryfile: list) {\n            if (queryfile.startsWith(\".\") || queryfile.endsWith(\"~\")) continue;\n            try {\n                BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(new File(harvestingPath, queryfile))));\n                String line;\n                List<IndexEntry<QueryEntry>> bulkEntries = new ArrayList<>();\n                while ((line = reader.readLine()) != null) {\n                    line = line.trim().toLowerCase();\n                    if (line.length() == 0) continue;\n                    if (line.charAt(0) <= '9') {\n                        // truncate statistic\n                        int p = line.indexOf(' ');\n                        if (p < 0) continue;\n                        line = line.substring(p + 1).trim();\n                    }\n                    // write line into query database\n                    if (!existQuery(line)) {\n                        randomTerms.add(line);\n                        bulkEntries.add(\n                            new IndexEntry<QueryEntry>(\n                                line,\n                                SourceType.TWITTER,\n                                new QueryEntry(line, 0, 60000, SourceType.TWITTER, false))\n                        );\n                    }\n                    if (bulkEntries.size() > 1000) {\n                        queries.writeEntries(bulkEntries);\n                        bulkEntries.clear();\n                    }\n                }\n                queries.writeEntries(bulkEntries);\n                reader.close();\n            } catch (IOException e) {\n            \tDAO.severe(e);\n            }\n        }\n        log(\"queries initialized.\");\n\n        log(\"finished DAO initialization\");\n    }\n\n    public static boolean wait_ready(long maxtimemillis) {\n        ClusterHealthStatus required_status = ClusterHealthStatus.fromString(config.get(\"elasticsearch_requiredClusterHealthStatus\"));\n        return elasticsearch_client.wait_ready(maxtimemillis, required_status);\n    }\n\n    public static String pendingClusterTasks() {\n        return elasticsearch_client.pendingClusterTasks();\n    }\n\n    public static String clusterStats() {\n        return elasticsearch_client.clusterStats();\n    }\n\n    public static Map<String, String> nodeSettings() {\n        return elasticsearch_client.nodeSettings();\n    }\n\n    public static File getAssetFile(String screen_name, String id_str, String file) {\n        String letter0 = (\"\" + screen_name.charAt(0)).toLowerCase();\n        String letter1 = (\"\" + screen_name.charAt(1)).toLowerCase();\n        File storage_path = new File(new File(new File(assets, letter0), letter1), screen_name);\n        return new File(storage_path, id_str + \"_\" + file); // all assets for one user in one file\n    }\n\n    public static Collection<File> getTweetOwnDumps(int count) {\n        return message_dump.getOwnDumps(count);\n    }\n\n    public static void importAccountDumps(int count) throws IOException {\n        Collection<File> dumps = account_dump.getImportDumps(count);\n        if (dumps == null || dumps.size() == 0) return;\n        for (File dump: dumps) {\n            JsonReader reader = account_dump.getDumpReader(dump);\n            final JsonReader dumpReader = reader;\n            Thread[] indexerThreads = new Thread[dumpReader.getConcurrency()];\n            for (int i = 0; i < dumpReader.getConcurrency(); i++) {\n                indexerThreads[i] = new Thread() {\n                    public void run() {\n                        JsonFactory accountEntry;\n                        try {\n                            while ((accountEntry = dumpReader.take()) != JsonStreamReader.POISON_JSON_MAP) {\n                                try {\n                                    JSONObject json = accountEntry.getJSON();\n                                    AccountEntry a = new AccountEntry(json);\n                                    DAO.writeAccount(a, false);\n                                } catch (IOException e) {\n                                \tDAO.severe(e);\n                                }\n                            }\n                        } catch (InterruptedException e) {\n                        \tDAO.severe(e);\n                        }\n                    }\n                };\n                indexerThreads[i].start();\n            }\n            for (int i = 0; i < dumpReader.getConcurrency(); i++) {\n                try {indexerThreads[i].join();} catch (InterruptedException e) {}\n            }\n            account_dump.shiftProcessedDump(dump.getName());\n        }\n    }\n\n    /**\n     * close all objects in this class\n     */\n    public static void close() {\n        DAO.log(\"closing DAO\");\n\n        // close the dump files\n        message_dump.close();\n        account_dump.close();\n        import_profile_dump.close();\n        user_dump.close();\n        followers_dump.close();\n        following_dump.close();\n\n        // close the tracker\n        access.close();\n\n        // close the index factories (flushes the caches)\n        messages.close();\n        messages_hour.close();\n        messages_day.close();\n        messages_week.close();\n        users.close();\n        accounts.close();\n        queries.close();\n        importProfiles.close();\n\n        // close the index\n        elasticsearch_client.close();\n\n        DAO.log(\"closed DAO\");\n    }\n\n    /**\n     * get values from\n     * @param key\n     * @param default_val\n     * @return\n     */\n    public static String getConfig(String key, String default_val) {\n        String value = config.get(key);\n        return value == null ? default_val : value;\n    }\n\n    public static String[] getConfig(String key, String[] default_val, String delim) {\n        String value = config.get(key);\n        return value == null || value.length() == 0 ? default_val : value.split(delim);\n    }\n\n    public static long getConfig(String key, long default_val) {\n        String value = config.get(key);\n        try {\n            return value == null ? default_val : Long.parseLong(value);\n        } catch (NumberFormatException e) {\n            return default_val;\n        }\n    }\n\n    public static double getConfig(String key, double default_val) {\n        String value = config.get(key);\n        try {\n            return value == null ? default_val : Double.parseDouble(value);\n        } catch (NumberFormatException e) {\n            return default_val;\n        }\n    }\n\n    public static int getConfig(String key, int default_val) {\n        String value = config.get(key);\n        try {\n            return value == null ? default_val : Integer.parseInt(value);\n        } catch (NumberFormatException e) {\n            return default_val;\n        }\n    }\n/*\n    public static void setConfig(String key, String value) {\n        config.put(key, value);\n    }\n\n    public static void setConfig(String key, long value) {\n        setConfig(key, Long.toString(value));\n    }\n\n    public static void setConfig(String key, double value) {\n        setConfig(key, Double.toString(value));\n    }\n*/\n    public static JsonNode getSchema(String key) throws IOException {\n        File schema = new File(schema_dir, key);\n        if (!schema.exists()) {\n            throw new FileNotFoundException(\"No schema file with name \" + key + \" found\");\n        }\n        return JsonLoader.fromFile(schema);\n    }\n\n    public static JSONObject getConversionSchema(String key) throws IOException {\n        File schema = new File(conv_schema_dir, key);\n        if (!schema.exists()) {\n            throw new FileNotFoundException(\"No schema file with name \" + key + \" found\");\n        }\n        return new JSONObject(com.google.common.io.Files.toString(schema, Charsets.UTF_8));\n    }\n\n    public static boolean getConfig(String key, boolean default_val) {\n        String value = config.get(key);\n        return value == null ? default_val : value.equals(\"true\") || value.equals(\"on\") || value.equals(\"1\");\n    }\n\n    public static Set<String> getConfigKeys() {\n        return config.keySet();\n    }\n\n    public static class MessageWrapper {\n        public TwitterTweet t;\n        public UserEntry u;\n        public boolean dump;\n        public MessageWrapper(TwitterTweet t, UserEntry u, boolean dump) {\n            this.t = t;\n            this.u = u;\n            this.dump = dump;\n        }\n    }\n\n    /**\n     * Store a message together with a user into the search index\n     * @param mw a message wrapper\n     * @return true if the record was stored because it did not exist, false if it was not stored because the record existed already\n     */\n    public static boolean writeMessage(MessageWrapper mw) {\n        if (mw.t == null) return false;\n        try {\n            synchronized (DAO.class) {\n                // record tweet into search index and check if this is a new entry\n                // and check if the message exists\n                boolean exists = false;\n                if (mw.t.getCreatedAt().after(DateParser.oneHourAgo())) {\n                    exists = messages_hour.writeEntry(new IndexEntry<Post>(mw.t.getPostId(), mw.t.getSourceType(), mw.t));\n                    if (exists) return false;\n                }\n                if (mw.t.getCreatedAt().after(DateParser.oneDayAgo())) {\n                    exists = messages_day.writeEntry(new IndexEntry<Post>(mw.t.getPostId(), mw.t.getSourceType(), mw.t));\n                    if (exists) return false;\n                }\n                if (mw.t.getCreatedAt().after(DateParser.oneWeekAgo())) {\n                    exists = messages_week.writeEntry(new IndexEntry<Post>(mw.t.getPostId(), mw.t.getSourceType(), mw.t));\n                    if (exists) return false;\n                }\n                exists = messages.writeEntry(new IndexEntry<Post>(mw.t.getPostId(), mw.t.getSourceType(), mw.t));\n                if (exists) return false;\n\n                // write the user into the index\n                users.writeEntryAsync(new IndexEntry<UserEntry>(mw.u.getScreenName(), mw.t.getSourceType(), mw.u));\n\n                // record tweet into text file\n                if (mw.dump && writeDump) {\n                    message_dump.write(mw.t.toJSON(mw.u, false, Integer.MAX_VALUE, \"\"), true);\n                }\n                mw.t.publishToMQTT();\n             }\n\n            // teach the classifier\n            Classifier.learnPhrase(mw.t.getText());\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        return true;\n    }\n\n    public static Set<String> writeMessageBulk(Collection<MessageWrapper> mws) {\n        List<MessageWrapper> noDump = new ArrayList<>();\n        List<MessageWrapper> dump = new ArrayList<>();\n        for (MessageWrapper mw: mws) {\n            if (mw.t == null) continue;\n            if (mw.dump) dump.add(mw); else noDump.add(mw);\n        }\n        Set<String> createdIDs = new HashSet<>();\n        createdIDs.addAll(writeMessageBulkNoDump(noDump));\n        // does also do an writeMessageBulkNoDump internally\n        createdIDs.addAll(writeMessageBulkDump(dump));\n        return createdIDs;\n    }\n\n    public static Set<String> writeMessageBulk(Set<PostTimeline> postBulk) {\n        for (PostTimeline postList: postBulk) {\n            if (postList.size() < 1) continue;\n            if(postList.dump) {\n                writeMessageBulkDump(postList);\n            }\n            writeMessageBulkNoDump(postList);\n        }\n        //TODO: return total dumped, or IDs dumped to create hash of IDs\n        return new HashSet<>();\n    }\n\n    private static Set<String> writeMessageBulkNoDump(PostTimeline postList) {\n        if (postList.size() == 0) return new HashSet<>();\n        List<Post> messageBulk = new ArrayList<>();\n\n        for (Post post: postList) {\n            if (messages.existsCache(post.getPostId())) {\n                continue;\n            }\n            synchronized (DAO.class) {\n                messageBulk.add(post);\n            }\n        }\n        BulkWriteResult result = null;\n        try {\n            Date limitDate = new Date();\n            limitDate.setTime(DateParser.oneHourAgo().getTime());\n            List<Post> macc = new ArrayList<Post>();\n            final Set<String> existed = new HashSet<>();\n            long hourLong = limitDate.getTime();\n            for(Post post : messageBulk) {\n                if(hourLong <= post.getTimestamp()) macc.add(post);\n            }\n            \n            result = messages_hour.writeEntries(macc);\n            for (Post i: macc) if (!(result.getCreated().contains(i.getPostId()))) existed.add(i.getPostId());\n\n            limitDate.setTime(DateParser.oneDayAgo().getTime());\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getPostId()))).filter(i -> i.getCreated().after(limitDate)).collect(Collectors.toList());\n            result = messages_day.writeEntries(macc);\n            for (Post i: macc) if (!(result.getCreated().contains(i.getPostId()))) existed.add(i.getPostId());\n\n            limitDate.setTime(DateParser.oneWeekAgo().getTime());\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getPostId())))\n                    .filter(i -> i.getCreated().after(limitDate)).collect(Collectors.toList());\n            result = messages_week.writeEntries(macc);\n            for (Post i: macc) if (!(result.getCreated().contains(i.getPostId()))) existed.add(i.getPostId());\n\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getPostId()))).collect(Collectors.toList());\n            result = messages.writeEntries(macc);\n            for (Post i: macc) if (!(result.getCreated().contains(i.getPostId()))) existed.add(i.getPostId());\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        if (result == null) return new HashSet<String>();\n        return result.getCreated();\n    }\n\n    /**\n     * write messages without writing them to the dump file\n     * @param mws a collection of message wrappers\n     * @return a set of message IDs which had been created with this bulk write.\n     */\n    private static Set<String> writeMessageBulkNoDump(Collection<MessageWrapper> mws) {\n        if (mws.size() == 0) return new HashSet<>();\n        List<IndexEntry<UserEntry>> userBulk = new ArrayList<>();\n        List<IndexEntry<Post>> messageBulk = new ArrayList<>();\n        for (MessageWrapper mw: mws) {\n            if (messages.existsCache(mw.t.getPostId())) continue; // we omit writing this again\n            synchronized (DAO.class) {\n                // write the user into the index\n                userBulk.add(new IndexEntry<UserEntry>(mw.u.getScreenName(), mw.t.getSourceType(), mw.u));\n\n                // record tweet into search index\n                messageBulk.add(new IndexEntry<Post>(mw.t.getPostId(), mw.t.getSourceType(), mw.t));\n             }\n\n            // teach the classifier\n            Classifier.learnPhrase(mw.t.getText());\n        }\n        BulkWriteResult result = null;\n        Set<String> created_ids = new HashSet<>();\n        try {\n            final Date limitDate = new Date();\n            List<IndexEntry<Post>> macc;\n            final Set<String> existed = new HashSet<>();\n\n            //DAO.log(\"***DEBUG messages     INIT: \" + messageBulk.size());\n\n            limitDate.setTime(DateParser.oneHourAgo().getTime());\n            macc = messageBulk.stream().filter(i -> i.getObject().getCreated().after(limitDate)).collect(Collectors.toList());\n            //DAO.log(\"***DEBUG messages for HOUR: \" + macc.size());\n            result = messages_hour.writeEntries(macc);\n            created_ids.addAll(result.getCreated());\n            //DAO.log(\"***DEBUG messages for HOUR: \" + result.getCreated().size() + \"  created\");\n            for (IndexEntry<Post> i: macc) if (!(result.getCreated().contains(i.getId()))) existed.add(i.getId());\n            //DAO.log(\"***DEBUG messages for HOUR: \" + existed.size() + \"  existed\");\n\n            limitDate.setTime(DateParser.oneDayAgo().getTime());\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getObject().getPostId()))).filter(i -> i.getObject().getCreated().after(limitDate)).collect(Collectors.toList());\n            //DAO.log(\"***DEBUG messages for  DAY : \" + macc.size());\n            result = messages_day.writeEntries(macc);\n            created_ids.addAll(result.getCreated());\n            //DAO.log(\"***DEBUG messages for  DAY: \" + result.getCreated().size() + \" created\");\n            for (IndexEntry<Post> i: macc) if (!(result.getCreated().contains(i.getId()))) existed.add(i.getId());\n            //DAO.log(\"***DEBUG messages for  DAY: \" + existed.size()  + \"  existed\");\n\n            limitDate.setTime(DateParser.oneWeekAgo().getTime());\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getObject().getPostId()))).filter(i -> i.getObject().getCreated().after(limitDate)).collect(Collectors.toList());\n            //DAO.log(\"***DEBUG messages for WEEK: \" + macc.size());\n            result = messages_week.writeEntries(macc);\n            created_ids.addAll(result.getCreated());\n            //DAO.log(\"***DEBUG messages for WEEK: \" + result.getCreated().size() + \"  created\");\n            for (IndexEntry<Post> i: macc) if (!(result.getCreated().contains(i.getId()))) existed.add(i.getId());\n            //DAO.log(\"***DEBUG messages for WEEK: \" + existed.size()  + \"  existed\");\n\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getObject().getPostId()))).collect(Collectors.toList());\n            //DAO.log(\"***DEBUG messages for  ALL : \" + macc.size());\n            result = messages.writeEntries(macc);\n            created_ids.addAll(result.getCreated());\n            //DAO.log(\"***DEBUG messages for  ALL: \" + result.getCreated().size() + \"  created\");\n            for (IndexEntry<Post> i: macc) if (!(result.getCreated().contains(i.getId()))) existed.add(i.getId());\n            //DAO.log(\"***DEBUG messages for  ALL: \" + existed.size()  + \"  existed\");\n\n            users.writeEntries(userBulk);\n\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        return created_ids;\n    }\n\n    private static Set<String> writeMessageBulkDump(Collection<MessageWrapper> mws) {\n        Set<String> created = writeMessageBulkNoDump(mws);\n\n        for (MessageWrapper mw: mws) try {\n            mw.t.publishToMQTT();\n            if (!created.contains(mw.t.getPostId())) continue;\n            synchronized (DAO.class) {\n                \n                // record tweet into text file\n                if (writeDump) {\n                    message_dump.write(mw.t.toJSON(mw.u, false, Integer.MAX_VALUE, \"\"), true);\n                }\n            }\n\n            // teach the classifier\n            if (randomPicker.nextInt(100) == 0) Classifier.learnPhrase(mw.t.getText());\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n\n        return created;\n    }\n\n    private static Set<String> writeMessageBulkDump(PostTimeline postList) {\n        Set<String> created = writeMessageBulkNoDump(postList);\n\n        for (Post post: postList) try {\n            if (!created.contains(post.getPostId())) continue;\n            synchronized (DAO.class) {\n                // record tweet into text file\n                if (writeDump) {\n                    message_dump.write(post, true);\n                }\n            }\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        return created;\n    }\n\n    /**\n     * Store an account together with a user into the search index\n     * This method is synchronized to prevent concurrent IO caused by this call.\n     * @param a an account\n     * @param dump\n     * @return true if the record was stored because it did not exist, false if it was not stored because the record existed already\n     */\n    public static boolean writeAccount(AccountEntry a, boolean dump) {\n        try {\n            // record account into text file\n            if (dump && writeDump) {\n                account_dump.write(a.toJSON(null), true);\n            }\n\n            // record account into search index\n            accounts.writeEntryAsync(new IndexEntry<AccountEntry>(a.getScreenName(), a.getSourceType(), a));\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        return true;\n    }\n\n    /**\n     * Store an import profile into the search index\n     * @param i an import profile\n     * @return true if the record was stored because it did not exist, false if it was not stored because the record existed already\n     */\n    public static boolean writeImportProfile(ImportProfileEntry i, boolean dump) {\n        try {\n            // record import profile into text file\n            if (dump && writeDump) {\n                import_profile_dump.write(i.toJSON(), true);\n            }\n            // record import profile into search index\n            importProfiles.writeEntryAsync(new IndexEntry<ImportProfileEntry>(i.getId(), i.getSourceType(), i));\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        return true;\n    }\n\n    private static long countLocalHourMessages(final long millis, boolean created_at) {\n        if (millis > DateParser.HOUR_MILLIS) return countLocalDayMessages(millis, created_at);\n        if (created_at && millis == DateParser.HOUR_MILLIS) return elasticsearch_client.count(IndexName.messages_hour.name());\n        return elasticsearch_client.count(\n                created_at ? IndexName.messages_hour.name() : IndexName.messages_day.name(),\n                created_at ? AbstractObjectEntry.CREATED_AT_FIELDNAME : AbstractObjectEntry.TIMESTAMP_FIELDNAME,\n                millis);\n    }\n\n    private static long countLocalDayMessages(final long millis, boolean created_at) {\n        if (millis > DateParser.DAY_MILLIS) return countLocalWeekMessages(millis, created_at);\n        if (created_at && millis == DateParser.DAY_MILLIS) return elasticsearch_client.count(IndexName.messages_day.name());\n        return elasticsearch_client.count(\n                created_at ? IndexName.messages_day.name() : IndexName.messages_week.name(),\n                created_at ? AbstractObjectEntry.CREATED_AT_FIELDNAME : AbstractObjectEntry.TIMESTAMP_FIELDNAME,\n                millis);\n    }\n\n    private static long countLocalWeekMessages(final long millis, boolean created_at) {\n        if (millis > DateParser.WEEK_MILLIS) return countLocalMessages(millis, created_at);\n        if (created_at && millis == DateParser.WEEK_MILLIS) return elasticsearch_client.count(IndexName.messages_week.name());\n        return elasticsearch_client.count(\n                created_at ? IndexName.messages_week.name() : IndexName.messages.name(),\n                created_at ? AbstractObjectEntry.CREATED_AT_FIELDNAME : AbstractObjectEntry.TIMESTAMP_FIELDNAME,\n                millis);\n    }\n\n    /**\n     * count the messages in the local index\n     * @param millis number of milliseconds in the past\n     * @param created_at field selector: true -> use CREATED_AT, the time when the tweet was created; false -> use TIMESTAMP, the time when the tweet was harvested\n     * @return the number of messages in that time span\n     */\n    public static long countLocalMessages(final long millis, boolean created_at) {\n        if (millis == 0) return 0;\n        if (millis > 0) {\n            if (millis <= DateParser.HOUR_MILLIS) return countLocalHourMessages(millis, created_at);\n            if (millis <= DateParser.DAY_MILLIS) return countLocalDayMessages(millis, created_at);\n            if (millis <= DateParser.WEEK_MILLIS) return countLocalWeekMessages(millis, created_at);\n        }\n        return elasticsearch_client.count(\n                IndexName.messages.name(),\n                created_at ? AbstractObjectEntry.CREATED_AT_FIELDNAME : AbstractObjectEntry.TIMESTAMP_FIELDNAME,\n                millis == Long.MAX_VALUE ? -1 : millis);\n    }\n\n    public static long countLocalMessages() {\n        return elasticsearch_client.count(IndexName.messages.name(), AbstractObjectEntry.TIMESTAMP_FIELDNAME, -1);\n    }\n\n    public static long countLocalMessages(String provider_hash) {\n        return elasticsearch_client.countLocal(IndexName.messages.name(), provider_hash);\n    }\n\n    public static long countLocalUsers() {\n        return elasticsearch_client.count(IndexName.users.name(), AbstractObjectEntry.TIMESTAMP_FIELDNAME, -1);\n    }\n\n    public static long countLocalQueries() {\n        return elasticsearch_client.count(IndexName.queries.name(), AbstractObjectEntry.TIMESTAMP_FIELDNAME, -1);\n    }\n\n    public static long countLocalAccounts() {\n        return elasticsearch_client.count(IndexName.accounts.name(), AbstractObjectEntry.TIMESTAMP_FIELDNAME, -1);\n    }\n\n    public static Post readMessage(String id) throws IOException {\n        Post m = null;\n        return messages_hour != null && ((m = messages_hour.read(id)) != null) ? m :\n               messages_day  != null && ((m = messages_day.read(id))  != null) ? m :\n               messages_week != null && ((m = messages_week.read(id)) != null) ? m :\n               messages.read(id);\n    }\n\n    public static boolean existMessage(String id) {\n        return messages_hour != null && messages_hour.exists(id) ||\n               messages_day  != null && messages_day.exists(id)  ||\n               messages_week != null && messages_week.exists(id) ||\n               messages      != null && messages.exists(id);\n    }\n\n    public static boolean existUser(String id) {\n        return users.exists(id);\n    }\n\n    public static boolean existQuery(String id) {\n        return queries.exists(id);\n    }\n\n    public static boolean deleteQuery(String id, SourceType sourceType) {\n        return queries.delete(id, sourceType);\n    }\n    \n    public static String getRandomTerm() {\n        return randomTerms.size() == 0 ? null : randomTerms.get(randomPicker.nextInt(randomTerms.size()));\n    }\n\n    public static boolean deleteImportProfile(String id, SourceType sourceType) {\n        return importProfiles.delete(id, sourceType);\n    }\n\n    public static int deleteOld(IndexName indexName, Date createDateLimit) {\n        RangeQueryBuilder rangeQuery = QueryBuilders.rangeQuery(AbstractObjectEntry.CREATED_AT_FIELDNAME).to(createDateLimit);\n        return elasticsearch_client.deleteByQuery(indexName.name(), rangeQuery);\n    }\n\n    public static class SearchLocalMessages {\n        public TwitterTimeline timeline;\n        public PostTimeline postList;\n        public Map<String, List<Map.Entry<String, AtomicLong>>> aggregations;\n        public ElasticsearchClient.Query query;\n\n        /**\n         * Search the local message cache using a elasticsearch query.\n         * @param q - the query, for aggregation this which should include a time frame in the form since:yyyy-MM-dd until:yyyy-MM-dd\n         * @param order_field - the field to order the results, i.e. Timeline.Order.CREATED_AT\n         * @param timezoneOffset - an offset in minutes that is applied on dates given in the query of the form since:date until:date\n         * @param resultCount - the number of messages in the result; can be zero if only aggregations are wanted\n         * @param aggregationLimit - the maximum count of facet entities, not search results\n         * @param aggregationFields - names of the aggregation fields. If no aggregation is wanted, pass no (zero) field(s)\n         * @param filterList - list of filters in String datatype\n         */\n        public SearchLocalMessages (\n                final String q,\n                final TwitterTimeline.Order orderField,\n                final int timezoneOffset,\n                final int resultCount,\n                final int aggregationLimit,\n                final ArrayList<String> filterList,\n                final String... aggregationFields\n        ) {\n            this.timeline = new TwitterTimeline(orderField);\n            QueryEntry.ElasticsearchQuery sq = new QueryEntry.ElasticsearchQuery(q, timezoneOffset, filterList);\n            long interval = sq.until.getTime() - sq.since.getTime();\n            IndexName resultIndex;\n            if (aggregationFields.length > 0 && q.contains(\"since:\")) {\n                if (q.contains(\"since:hour\")) {\n                    this.query =  elasticsearch_client.query((resultIndex = IndexName.messages_hour).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                } else if (q.contains(\"since:day\")) {\n                    this.query =  elasticsearch_client.query((resultIndex = IndexName.messages_day).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                } else if (q.contains(\"since:week\")) {\n                    this.query =  elasticsearch_client.query((resultIndex = IndexName.messages_week).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                } else {\n                    this.query = elasticsearch_client.query((resultIndex = IndexName.messages).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                }\n            } else {\n                // use only a time frame that is sufficient for a result\n                this.query = elasticsearch_client.query((resultIndex = IndexName.messages_hour).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                if (!q.contains(\"since:hour\") && insufficient(this.query, resultCount, aggregationLimit, aggregationFields)) {\n                    ElasticsearchClient.Query aq = elasticsearch_client.query((resultIndex = IndexName.messages_day).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                    this.query.add(aq);\n                    if (!q.contains(\"since:day\") && insufficient(this.query, resultCount, aggregationLimit, aggregationFields)) {\n                        this.query.add(aq);\n                        if (!q.contains(\"since:week\") && insufficient(this.query, resultCount, aggregationLimit, aggregationFields)) {\n                            aq = elasticsearch_client.query((resultIndex = IndexName.messages).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                            this.query.add(aq);\n                }}}\n            }\n                \n            timeline.setHits(query.getHitCount());\n            timeline.setResultIndex(resultIndex);\n\n            // evaluate search result\n            for (Map<String, Object> map: query.getResult()) {\n                TwitterTweet tweet = new TwitterTweet(new JSONObject(map));\n                try {\n                    UserEntry user = users.read(tweet.getScreenName());\n                    assert user != null;\n                    if (user != null) {\n                        timeline.add(tweet, user);\n                    }\n                } catch (IOException e) {\n                \tDAO.severe(e);\n                }\n            }\n            this.aggregations = query.getAggregations();\n        }\n\n        public SearchLocalMessages (\n                final String q,\n                final TwitterTimeline.Order orderField,\n                final int timezoneOffset,\n                final int resultCount,\n                final int aggregationLimit,\n                final String... aggregationFields\n        ) {\n            this(\n                q,\n                orderField,\n                timezoneOffset,\n                resultCount,\n                aggregationLimit,\n                new ArrayList<>(),\n                aggregationFields\n            );\n        }\n\n        public SearchLocalMessages (\n                Map<String, Map<String, String>> inputMap,\n                final PostTimeline.Order orderField,\n                final int resultCount\n        ) {\n            this.postList = new PostTimeline(orderField);\n            IndexName resultIndex;\n            QueryEntry.ElasticsearchQuery sq = new QueryEntry.ElasticsearchQuery(\n                    inputMap.get(\"get\"), inputMap.get(\"not_get\"), inputMap.get(\"also_get\"));\n\n            this.query = elasticsearch_client.query(\n                    (resultIndex = IndexName.messages_hour).name(),\n                    sq.queryBuilder,\n                    orderField.getMessageFieldName(),\n                    resultCount\n            );\n\n            if (this.query.getHitCount() < resultCount) {\n                this.query =  elasticsearch_client.query(\n                        (resultIndex = IndexName.messages_day).name(),\n                        sq.queryBuilder,\n                        orderField.getMessageFieldName(),\n                        resultCount\n                );\n            }\n\n            if (this.query.getHitCount() < resultCount) {\n                this.query =  elasticsearch_client.query(\n                        (resultIndex = IndexName.messages_week).name(),\n                        sq.queryBuilder,\n                        orderField.getMessageFieldName(),\n                        resultCount\n                );\n            }\n\n            if (this.query.getHitCount() < resultCount) {\n                this.query =  elasticsearch_client.query(\n                        (resultIndex = IndexName.messages).name(),\n                        sq.queryBuilder,\n                        orderField.getMessageFieldName(),\n                        resultCount\n                );\n            }\n\n            // Feed search results to postList\n            this.postList.setResultIndex(resultIndex);\n\n            addResults();\n        }\n\n        private void addResults() {\n            Post outputPost;\n            for (Map<String, Object> map: this.query.getResult()) {\n                outputPost = new Post(map);\n                this.postList.addPost(outputPost);\n            }\n        }\n\n        private static boolean insufficient(\n                ElasticsearchClient.Query query,\n                int resultCount,\n                int aggregationLimit,\n                String... aggregationFields\n        ) {\n            return query.getHitCount() < resultCount || (aggregationFields.length > 0\n                    && getAggregationResultLimit(query.getAggregations()) < aggregationLimit);\n        }\n\n        public JSONObject getAggregations() {\n            JSONObject json = new JSONObject(true);\n            if (aggregations == null) return json;\n            for (Map.Entry<String, List<Map.Entry<String, AtomicLong>>> aggregation: aggregations.entrySet()) {\n                JSONObject facet = new JSONObject(true);\n                for (Map.Entry<String, AtomicLong> a: aggregation.getValue()) {\n                    if (a.getValue().equals(query)) continue; // we omit obvious terms that cannot be used for faceting, like search for \"#abc\" -> most hashtag is \"#abc\"\n                    facet.put(a.getKey(), a.getValue().get());\n                }\n                json.put(aggregation.getKey(), facet);\n            }\n            return json;\n        }\n\n        private static int getAggregationResultLimit(Map<String, List<Map.Entry<String, AtomicLong>>> agg) {\n            if (agg == null) return 0;\n            int l = 0;\n            for (List<Map.Entry<String, AtomicLong>> a: agg.values()) l = Math.max(l, a.size());\n            return l;\n        }\n    }\n\n    public static LinkedHashMap<String, Long> FullDateHistogram(int timezoneOffset) {\n        return elasticsearch_client.fullDateHistogram(IndexName.messages.name(), timezoneOffset, AbstractObjectEntry.CREATED_AT_FIELDNAME);\n    }\n\n    /**\n     * Search the local user cache using a elasticsearch query.\n     * @param screen_name - the user id\n     */\n    public static UserEntry searchLocalUserByScreenName(final String screen_name) {\n        try {\n            return users.read(screen_name);\n        } catch (IOException e) {\n        \tDAO.severe(e);\n            return null;\n        }\n    }\n\n    public static UserEntry searchLocalUserByUserId(final String user_id) {\n        if (user_id == null || user_id.length() == 0) return null;\n        Map<String, Object> map = elasticsearch_client.query(IndexName.users.name(), UserEntry.field_user_id, user_id);\n        if (map == null) return null;\n        return new UserEntry(new JSONObject(map));\n    }\n\n    /**\n     * Search the local account cache using an elasticsearch query.\n     * @param screen_name - the user id\n     */\n    public static AccountEntry searchLocalAccount(final String screen_name) {\n        try {\n            return accounts.read(screen_name);\n        } catch (IOException e) {\n        \tDAO.severe(e);\n            return null;\n        }\n    }\n\n    /**\n     * Search the local message cache using a elasticsearch query.\n     * @param q - the query, can be empty for a matchall-query\n     * @param resultCount - the number of messages in the result\n     * @param sort_field - the field name to sort the result list, i.e. \"query_first\"\n     * @param sort_order - the sort order (you want to use SortOrder.DESC here)\n     */\n    public static ResultList<QueryEntry> SearchLocalQueries(final String q, final int resultCount, final String sort_field, final String default_sort_type, final SortOrder sort_order, final Date since, final Date until, final String range_field) {\n        ResultList<QueryEntry> queries = new ResultList<>();\n        ResultList<Map<String, Object>> result = elasticsearch_client.fuzzyquery(IndexName.queries.name(), \"query\", q, resultCount, sort_field, default_sort_type, sort_order, since, until, range_field);\n        queries.setHits(result.getHits());\n        for (Map<String, Object> map: result) {\n            queries.add(new QueryEntry(new JSONObject(map)));\n        }\n        return queries;\n    }\n\n    public static ImportProfileEntry SearchLocalImportProfiles(final String id) {\n        try {\n            return importProfiles.read(id);\n        } catch (IOException e) {\n        \tDAO.severe(e);\n            return null;\n        }\n    }\n\n    public static Collection<ImportProfileEntry> SearchLocalImportProfilesWithConstraints(final Map<String, String> constraints, boolean latest) throws IOException {\n        List<ImportProfileEntry> rawResults = new ArrayList<>();\n        List<Map<String, Object>> result = elasticsearch_client.queryWithConstraints(IndexName.import_profiles.name(), \"active_status\", ImportProfileEntry.EntryStatus.ACTIVE.name().toLowerCase(), constraints, latest);\n        for (Map<String, Object> map: result) {\n            rawResults.add(new ImportProfileEntry(new JSONObject(map)));\n        }\n\n        if (!latest) {\n            return rawResults;\n        }\n\n        // filter results to display only latest profiles\n        Map<String, ImportProfileEntry> latests = new HashMap<>();\n        for (ImportProfileEntry entry : rawResults) {\n            String uniqueKey;\n            if (entry.getImporter() != null) {\n                uniqueKey = entry.getSourceUrl() + entry.getImporter();\n            } else {\n                uniqueKey = entry.getSourceUrl() + entry.getClientHost();\n            }\n            if (latests.containsKey(uniqueKey)) {\n                if (entry.getLastModified().compareTo(latests.get(uniqueKey).getLastModified()) > 0) {\n                    latests.put(uniqueKey, entry);\n                }\n            } else {\n                latests.put(uniqueKey, entry);\n            }\n        }\n        return latests.values();\n    }\n\n    public static TwitterTimeline scrapeTwitter(\n            final Query post,\n            final String q,\n            final TwitterTimeline.Order order,\n            final int timezoneOffset,\n            boolean byUserQuery,\n            long timeout,\n            boolean recordQuery) {\n\n        return scrapeTwitter(post, new ArrayList<>(), q, order, timezoneOffset, byUserQuery, timeout, recordQuery);\n    }\n\n    public static TwitterTimeline scrapeTwitter(\n            final Query post,\n            final ArrayList<String> filterList,\n            final String q,\n            final TwitterTimeline.Order order,\n            final int timezoneOffset,\n            boolean byUserQuery,\n            long timeout,\n            boolean recordQuery) {\n        // retrieve messages from remote server\n\n        ArrayList<String> remote = DAO.getFrontPeers();\n        TwitterTimeline tl;\n        if (remote.size() > 0 && (peerLatency.get(remote.get(0)) == null || peerLatency.get(remote.get(0)).longValue() < 3000)) {\n            long start = System.currentTimeMillis();\n            tl = searchOnOtherPeers(remote, q, filterList, order, 100, timezoneOffset, \"all\", SearchServlet.frontpeer_hash, timeout); // all must be selected here to catch up missing tweets between intervals\n            // at this point the remote list can be empty as a side-effect of the remote search attempt\n            if (post != null && remote.size() > 0 && tl != null) post.recordEvent(\"remote_scraper_on_\" + remote.get(0), System.currentTimeMillis() - start);\n            if (tl == null || tl.size() == 0) {\n                // maybe the remote server died, we try then ourself\n                start = System.currentTimeMillis();\n\n                tl = TwitterScraper.search(q, filterList, order, true, true, 400);\n                if (post != null) post.recordEvent(\"local_scraper_after_unsuccessful_remote\", System.currentTimeMillis() - start);\n            } else {\n                tl.writeToIndex();\n            }\n        } else {\n            if (post != null && remote.size() > 0) post.recordEvent(\"omitted_scraper_latency_\" + remote.get(0), peerLatency.get(remote.get(0)));\n            long start = System.currentTimeMillis();\n\n            tl = TwitterScraper.search(q, filterList, order, true, true, 400);\n            if (post != null) post.recordEvent(\"local_scraper\", System.currentTimeMillis() - start);\n        }\n\n        // record the query\n        long start2 = System.currentTimeMillis();\n        QueryEntry qe = null;\n        try {\n            qe = queries.read(q);\n        } catch (IOException | JSONException e) {\n        \tDAO.severe(e);\n        }\n\n        if (recordQuery && Caretaker.acceptQuery4Retrieval(q)) {\n            if (qe == null) {\n                // a new query occurred\n                qe = new QueryEntry(q, timezoneOffset, tl.period(), SourceType.TWITTER, byUserQuery);\n            } else {\n                // existing queries are updated\n                qe.update(tl.period(), byUserQuery);\n            }\n            try {\n                queries.writeEntryAsync(new IndexEntry<QueryEntry>(q, qe.source_type == null ? SourceType.TWITTER : qe.source_type, qe));\n            } catch (IOException e) {\n            \tDAO.severe(e);\n            }\n        } else {\n            // accept rules may change, we want to delete the query then in the index\n            if (qe != null) queries.delete(q, qe.source_type);\n        }\n        if (post != null) post.recordEvent(\"query_recorder\", System.currentTimeMillis() - start2);\n        //log(\"SCRAPER: TIME LEFT after recording = \" + (termination - System.currentTimeMillis()));\n\n        return tl;\n    }\n\n\n    public static JSONArray scrapeLoklak(\n            Map<String, String> inputMap,\n            boolean byUserQuery,\n            boolean recordQuery,\n            JSONObject metadata) {\n        PostTimeline.Order order= getOrder(inputMap.get(\"order\"));\n        PostTimeline dataSet = new PostTimeline(order);\n        List<String> scraperList = Arrays.asList(inputMap.get(\"scraper\").trim().split(\"\\\\s*,\\\\s*\"));\n        List<BaseScraper> scraperObjList = getScraperObjects(scraperList, inputMap);\n        ExecutorService scraperRunner = Executors.newFixedThreadPool(scraperObjList.size());\n\n        try{\n            for (BaseScraper scraper : scraperObjList) {\n                scraperRunner.execute(() -> {\n                    dataSet.add(scraper.getData());\n                    \n                });\n            }\n        } finally {\n            scraperRunner.shutdown();\n            try {\n                scraperRunner.awaitTermination(3000L, TimeUnit.SECONDS);\n            } catch (InterruptedException e) { }\n        }\n        dataSet.collectMetadata(metadata);\n        return dataSet.toArray();\n    }\n\n    public static List<BaseScraper> getScraperObjects(List<String> scraperList, Map<String, String> inputMap) {\n        //TODO: use SourceType to get this job done\n        List<BaseScraper> scraperObjList = new ArrayList<BaseScraper>();\n        BaseScraper scraperObj = null;\n\n        if (scraperList.contains(\"github\") || scraperList.contains(\"all\")) {\n            scraperObj = new GithubProfileScraper(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        if (scraperList.contains(\"quora\") || scraperList.contains(\"all\")) {\n            scraperObj = new QuoraProfileScraper(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        if (scraperList.contains(\"instagram\") || scraperList.contains(\"all\")) {\n            scraperObj = new InstagramProfileScraper(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        if (scraperList.contains(\"youtube\") || scraperList.contains(\"all\")) {\n            scraperObj = new YoutubeScraper(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        if (scraperList.contains(\"wordpress\") || scraperList.contains(\"all\")) {\n            scraperObj = new WordpressCrawlerService(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        if (scraperList.contains(\"twitter\") || scraperList.contains(\"all\")) {\n            scraperObj = new TweetScraper(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        //TODO: add more scrapers\n        return scraperObjList;\n    }\n\n    public static PostTimeline.Order getOrder(String orderString) {\n        //TODO: order set according to input\n        return PostTimeline.parseOrder(\"timestamp\");\n    }\n\n    public static final Random random = new Random(System.currentTimeMillis());\n    private static final Map<String, Long> peerLatency = new HashMap<>();\n    private static ArrayList<String> getBestPeers(Collection<String> peers) {\n        ArrayList<String> best = new ArrayList<>();\n        if (peers == null || peers.size() == 0) return best;\n        // first check if any of the given peers has unknown latency\n        TreeMap<Long, String> o = new TreeMap<>();\n        for (String peer: peers) {\n            if (peerLatency.containsKey(peer)) {\n                o.put(peerLatency.get(peer) * 1000 + best.size(), peer);\n            } else {\n                best.add(peer);\n            }\n        }\n        best.addAll(o.values());\n        return best;\n    }\n    public static void healLatency(float factor) {\n        for (Map.Entry<String, Long> entry: peerLatency.entrySet()) {\n            entry.setValue((long) (factor * entry.getValue()));\n        }\n    }\n\n    private static Set<String> frontPeerCache = new HashSet<String>();\n    private static Set<String> backendPeerCache = new HashSet<String>();\n\n    public static void updateFrontPeerCache(RemoteAccess remoteAccess) {\n        if (remoteAccess.getLocalHTTPPort() >= 80) {\n            frontPeerCache.add(\"http://\" + remoteAccess.getRemoteHost() + (remoteAccess.getLocalHTTPPort() == 80 ? \"\" : \":\" + remoteAccess.getLocalHTTPPort()));\n        } else if (remoteAccess.getLocalHTTPSPort() >= 443) {\n            frontPeerCache.add(\"https://\" + remoteAccess.getRemoteHost() + (remoteAccess.getLocalHTTPSPort() == 443 ? \"\" : \":\" + remoteAccess.getLocalHTTPSPort()));\n        }\n    }\n\n    /**\n     * from all known front peers, generate a list of available peers, ordered by the peer latency\n     * @return a list of front peers. only the first one shall be used, but the other are fail-over peers\n     */\n    public static ArrayList<String> getFrontPeers() {\n        String[] remote = DAO.getConfig(\"frontpeers\", new String[0], \",\");\n        ArrayList<String> testpeers = new ArrayList<>();\n        if (remote.length > 0) {\n            for (String peer: remote) testpeers.add(peer);\n            return testpeers;\n        }\n        if (frontPeerCache.size() == 0) {\n            // add dynamically all peers that contacted myself\n            for (Map<String, RemoteAccess> hmap: RemoteAccess.history.values()) {\n                for (Map.Entry<String, RemoteAccess> peer: hmap.entrySet()) {\n                    updateFrontPeerCache(peer.getValue());\n                }\n            }\n        }\n        testpeers.addAll(frontPeerCache);\n        return getBestPeers(testpeers);\n    }\n\n    public static String[] getBackend() {\n        return DAO.getConfig(\"backend\", new String[0], \",\");\n    }\n    \n    public static List<String> getBackendPeers() {\n        List<String> testpeers = new ArrayList<>();\n        if (backendPeerCache.size() == 0) {\n            final String[] remote = DAO.getBackend();\n            for (String peer: remote) backendPeerCache.add(peer);\n        }\n        testpeers.addAll(backendPeerCache);\n        return getBestPeers(testpeers);\n    }\n\n    public static TwitterTimeline searchBackend(final String q,final ArrayList<String> filterList, final TwitterTimeline.Order order, final int count, final int timezoneOffset, final String where, final long timeout) {\n        List<String> remote = getBackendPeers();\n\n        if (remote.size() > 0 /*&& (peerLatency.get(remote.get(0)) == null || peerLatency.get(remote.get(0)) < 3000)*/) { // condition deactivated because we need always at least one peer\n            TwitterTimeline tt = searchOnOtherPeers(remote, q, filterList, order, count, timezoneOffset, where, SearchServlet.backend_hash, timeout);\n            if (tt != null) tt.writeToIndex();\n            return tt;\n        }\n        return null;\n    }\n\n    private final static Random randomPicker = new Random(System.currentTimeMillis());\n\n    public static TwitterTimeline searchOnOtherPeers(final List<String> remote, final String q, final ArrayList<String> filterList,final TwitterTimeline.Order order, final int count, final int timezoneOffset, final String source, final String provider_hash, final long timeout) {\n        // select remote peer\n        while (remote.size() > 0) {\n            int pick = randomPicker.nextInt(remote.size());\n            String peer = remote.get(pick);\n            long start = System.currentTimeMillis();\n            try {\n                TwitterTimeline tl = SearchServlet.search(new String[]{peer}, q, filterList, order, source, count, timezoneOffset, provider_hash, timeout);\n                peerLatency.put(peer, System.currentTimeMillis() - start);\n                // to show which peer was used for the retrieval, we move the picked peer to the front of the list\n                if (pick != 0) remote.add(0, remote.remove(pick));\n                tl.setScraperInfo(tl.getScraperInfo().length() > 0 ? peer + \",\" + tl.getScraperInfo() : peer);\n                return tl;\n            } catch (IOException e) {\n                DAO.log(\"searchOnOtherPeers: no IO to scraping target: \" + e.getMessage());\n                // the remote peer seems to be unresponsive, remove it (temporary) from the remote peer list\n                peerLatency.put(peer, DateParser.HOUR_MILLIS);\n                frontPeerCache.remove(peer);\n                backendPeerCache.remove(peer);\n                remote.remove(pick);\n            }\n        }\n        return null;\n    }\n\n    public final static Set<Number> newUserIds = ConcurrentHashMap.newKeySet();\n\n    public static void announceNewUserId(TwitterTimeline tl) {\n        for (TwitterTweet message: tl) {\n            UserEntry user = tl.getUser(message);\n            assert user != null;\n            if (user == null) continue;\n            Number id = user.getUser();\n            if (id != null) announceNewUserId(id);\n        }\n    }\n\n    public static void announceNewUserId(Number id) {\n        JsonFactory mapcapsule = DAO.user_dump.get(\"id_str\", id.toString());\n        JSONObject map = null;\n        try {map = mapcapsule == null ? null : mapcapsule.getJSON();} catch (IOException e) {}\n        if (map == null) newUserIds.add(id);\n    }\n\n    public static Set<Number> getNewUserIdsChunk() {\n        if (newUserIds.size() < 100) return null;\n        Set<Number> chunk = new HashSet<>();\n        Iterator<Number> i = newUserIds.iterator();\n        for (int j = 0; j < 100; j++) {\n            chunk.add(i.next());\n            i.remove();\n        }\n        return chunk;\n    }\n\n    /**\n     * For logging informational events\n     */\n    public static void log(String line) {\n        if (DAO.getConfig(\"flag.log.dao\", \"true\").equals(\"true\")) {\n            logger.info(line);\n        }\n    }\n\n    /**\n     * For events serious enough to inform and log, but not fatal.\n     */\n    public static void severe(String line) {\n        if (DAO.getConfig(\"flag.severe.dao\", \"true\").equals(\"true\")) {\n            logger.warn(line);\n        }\n    }\n\n    public static void severe(String line, Throwable e) {\n        if (DAO.getConfig(\"flag.severe.dao\", \"true\").equals(\"true\")) {\n            logger.warn(line, e);\n        }\n    }\n\n    public static void severe(Throwable e) {\n        if (DAO.getConfig(\"flag.severe.dao\", \"true\").equals(\"true\")) {\n            logger.warn(\"\", e);\n        }\n    }\n\n    /**\n     * For Debugging events (very noisy).\n     */\n    public static void debug(Throwable e) {\n        if (DAO.getConfig(\"flag.debug.dao\", \"true\").equals(\"true\")) {\n            DAO.severe(e);\n        }\n    }\n\n    /**\n     * For Stacktracing exceptions (preferred over debug).\n     */\n    public static void trace(Throwable e) {\n        if(DAO.getConfig(\"flag.trace.dao\", \"true\").equals(\"true\")) {\n            e.printStackTrace();\n        }\n    }\n}\n", "package org.loklak.tools;\n\nimport javax.annotation.Nonnull;\n\nimport org.loklak.data.DAO;\n\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.security.*;\nimport java.security.spec.InvalidKeySpecException;\nimport java.security.spec.X509EncodedKeySpec;\nimport java.util.Base64;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic final class IO {\n\t\n\tprivate static Map<Path,String> map;\n\tprivate static boolean initialized = false;\n\n\tpublic static String readFile(@Nonnull Path path) throws IOException\n\t{\n\t\tbyte[] encoded = Files.readAllBytes(path);\n\t\treturn new String(encoded);\n\t}\n\t\n\tpublic static String readFileCached(@Nonnull Path path) throws IOException\n\t{\n\t\tPath absPath = path.toAbsolutePath();\n\t\tif(!initialized) init();\n\t\tif(map.containsKey(absPath)){\n\t\t\treturn map.get(absPath);\n\t\t}\n\t\telse{\n\t\t\tString result = readFile(absPath);\n\t\t\tmap.put(absPath, result);\n\t\t\treturn result;\n\t\t}\n\t}\n\t\n\tprivate static void init(){\n\t\tmap = new HashMap<Path,String>();\n\t\tinitialized = true;\n\t}\n\n\t/**\n\t * Create hash for a key\n\t * @param pubkey\n\t * @param algorithm\n\t * @return String hash\n\t */\n\tpublic static String getKeyHash(@Nonnull PublicKey pubkey, @Nonnull String algorithm){\n\t\ttry {\n\t\t\tMessageDigest md = MessageDigest.getInstance(algorithm);\n\t\t\tmd.update(pubkey.getEncoded());\n\t\t\treturn Base64.getEncoder().encodeToString(md.digest());\n\t\t} catch (NoSuchAlgorithmException e) {\n\t\t\tDAO.severe(e);\n\t\t}\n\t\treturn null;\n\t}\n\n\t/**\n\t * Create hash for a key, use default algorithm SHA-256\n\t * @param pubkey\n\t * @return String hash\n\t */\n\tpublic static String getKeyHash(@Nonnull PublicKey pubkey){\n\t\treturn getKeyHash(pubkey, \"SHA-256\");\n\t}\n\n\t/**\n\t * Get String representation of a key\n\t * @param key\n\t * @return String representation of a key\n\t */\n\tpublic static String getKeyAsString(@Nonnull Key key){\n\t\treturn Base64.getEncoder().encodeToString(key.getEncoded());\n\t}\n\n\t/**\n\t * Create PublicKey from String representation\n\t * @param encodedKey\n\t * @param algorithm\n\t * @return PublicKey public_key\n\t */\n\tpublic synchronized static PublicKey decodePublicKey(@Nonnull String encodedKey, @Nonnull String algorithm){\n\t\ttry{\n\t\t\tX509EncodedKeySpec keySpec = new X509EncodedKeySpec(Base64.getDecoder().decode(encodedKey));\n\t\t\tPublicKey pub = KeyFactory.getInstance(algorithm).generatePublic(keySpec);\n\t\t\treturn pub;\n\t\t}\n\t\tcatch(NoSuchAlgorithmException | InvalidKeySpecException e){\n\t\t\tDAO.severe(e);\n\t\t}\n\t\treturn null;\n\t}\n}\n"], "fixing_code": ["/**\n *  DAO\n *  Copyright 22.02.2015 by Michael Peter Christen, @0rb1t3r\n *\n *  This library is free software; you can redistribute it and/or\n *  modify it under the terms of the GNU Lesser General Public\n *  License as published by the Free Software Foundation; either\n *  version 2.1 of the License, or (at your option) any later version.\n *\n *  This library is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n *  Lesser General Public License for more details.\n *\n *  You should have received a copy of the GNU Lesser General Public License\n *  along with this program in the file lgpl21.txt\n *  If not, see <http://www.gnu.org/licenses/>.\n */\n\npackage org.loklak.data;\n\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.github.fge.jackson.JsonLoader;\nimport com.google.common.base.Charsets;\n\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileNotFoundException;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicLong;\nimport java.nio.file.Path;\nimport java.security.KeyPair;\nimport java.security.KeyPairGenerator;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Date;\nimport java.util.HashMap;\nimport java.util.HashSet;\nimport java.util.Iterator;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Random;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport java.util.TreeMap;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\n\nimport org.elasticsearch.cluster.health.ClusterHealthStatus;\nimport org.elasticsearch.common.logging.ESLoggerFactory;\nimport org.elasticsearch.common.logging.slf4j.Slf4jESLoggerFactory;\nimport org.elasticsearch.index.query.QueryBuilders;\nimport org.elasticsearch.index.query.RangeQueryBuilder;\nimport org.elasticsearch.search.sort.SortOrder;\nimport org.json.JSONException;\nimport org.json.JSONObject;\nimport org.json.JSONArray;\nimport org.loklak.Caretaker;\nimport org.loklak.api.search.SearchServlet;\nimport org.loklak.api.search.WordpressCrawlerService;\nimport org.loklak.geo.GeoNames;\nimport org.loklak.harvester.Post;\nimport org.loklak.harvester.TwitterScraper;\nimport org.loklak.harvester.YoutubeScraper;\nimport org.loklak.harvester.TwitterScraper.TwitterTweet;\nimport org.loklak.api.search.GithubProfileScraper;\nimport org.loklak.api.search.InstagramProfileScraper;\nimport org.loklak.api.search.QuoraProfileScraper;\nimport org.loklak.api.search.TweetScraper;\nimport org.loklak.harvester.BaseScraper;\nimport org.loklak.http.AccessTracker;\nimport org.loklak.http.ClientConnection;\nimport org.loklak.http.RemoteAccess;\nimport org.loklak.ir.AccountFactory;\nimport org.loklak.ir.BulkWriteResult;\nimport org.loklak.ir.ElasticsearchClient;\nimport org.loklak.ir.ImportProfileFactory;\nimport org.loklak.ir.MessageFactory;\nimport org.loklak.ir.QueryFactory;\nimport org.loklak.ir.UserFactory;\nimport org.loklak.objects.AbstractObjectEntry;\nimport org.loklak.objects.AccountEntry;\nimport org.loklak.objects.ImportProfileEntry;\nimport org.loklak.objects.Peers;\nimport org.loklak.objects.QueryEntry;\nimport org.loklak.objects.ResultList;\nimport org.loklak.objects.SourceType;\nimport org.loklak.objects.TwitterTimeline;\nimport org.loklak.objects.PostTimeline;\nimport org.loklak.objects.TimelineCache;\nimport org.loklak.objects.UserEntry;\nimport org.loklak.server.*;\nimport org.loklak.stream.MQTTPublisher;\nimport org.loklak.tools.DateParser;\nimport org.loklak.tools.IO;\nimport org.loklak.tools.OS;\nimport org.loklak.tools.storage.*;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * The Data Access Object for the message project.\n * This provides only static methods because the class methods shall be available for\n * all other classes.\n *\n * To debug, call elasticsearch directly i.e.:\n *\n * get statistics\n * curl localhost:9200/_stats?pretty=true\n *\n * get statistics for message index\n * curl -XGET 'http://127.0.0.1:9200/messages?pretty=true'\n *\n * get mappings in message index\n * curl -XGET \"http://localhost:9200/messages/_mapping?pretty=true\"\n *\n * get search result from message index\n * curl -XGET 'http://127.0.0.1:9200/messages/_search?q=*&pretty=true'\n */\npublic class DAO {\n\n    public final static com.fasterxml.jackson.core.JsonFactory jsonFactory = new com.fasterxml.jackson.core.JsonFactory();\n    public final static com.fasterxml.jackson.databind.ObjectMapper jsonMapper = new com.fasterxml.jackson.databind.ObjectMapper(DAO.jsonFactory);\n    public final static com.fasterxml.jackson.core.type.TypeReference<HashMap<String,Object>> jsonTypeRef = new com.fasterxml.jackson.core.type.TypeReference<HashMap<String,Object>>() {};\n\n    private static Logger logger = LoggerFactory.getLogger(DAO.class);\n    \n    public final static String MESSAGE_DUMP_FILE_PREFIX = \"messages_\";\n    public final static String ACCOUNT_DUMP_FILE_PREFIX = \"accounts_\";\n    public final static String USER_DUMP_FILE_PREFIX = \"users_\";\n    public final static String ACCESS_DUMP_FILE_PREFIX = \"access_\";\n    public final static String FOLLOWERS_DUMP_FILE_PREFIX = \"followers_\";\n    public final static String FOLLOWING_DUMP_FILE_PREFIX = \"following_\";\n    private static final String IMPORT_PROFILE_FILE_PREFIX = \"profile_\";\n\n    public static boolean writeDump;\n\n    public final static int CACHE_MAXSIZE =   10000;\n    public final static int EXIST_MAXSIZE = 4000000;\n\n    public  static File data_dir, conf_dir, bin_dir, html_dir;\n    private static File external_data, assets, dictionaries;\n    public static Settings public_settings, private_settings;\n    private static Path message_dump_dir, account_dump_dir, import_profile_dump_dir;\n    public static Path push_cache_dir;\n    public static JsonRepository message_dump;\n    private static JsonRepository account_dump;\n    private static JsonRepository import_profile_dump;\n    public  static JsonDataset user_dump, followers_dump, following_dump;\n    public  static AccessTracker access;\n    private static File schema_dir, conv_schema_dir;\n    public static ElasticsearchClient elasticsearch_client;\n    //private static Node elasticsearch_node;\n    //private static Client elasticsearch_client;\n    public static UserFactory users;\n    private static AccountFactory accounts;\n    public static MessageFactory messages;\n    public static MessageFactory messages_hour;\n    public static MessageFactory messages_day;\n    public static MessageFactory messages_week;\n    public static QueryFactory queries;\n    private static ImportProfileFactory importProfiles;\n    private static Map<String, String> config = new HashMap<>();\n    public  static GeoNames geoNames = null;\n    public static Peers peers = new Peers();\n    public static OutgoingMessageBuffer outgoingMessages = new OutgoingMessageBuffer();\n\n    // AAA Schema for server usage\n    public static JsonTray authentication;\n    public static JsonTray authorization;\n    public static JsonTray accounting;\n    public static UserRoles userRoles;\n    public static JsonTray passwordreset;\n    public static Map<String, Accounting> accounting_temporary = new HashMap<>();\n    public static JsonFile login_keys;\n    public static TimelineCache timelineCache;\n\n    public static MQTTPublisher mqttPublisher = null;\n    public static boolean streamEnabled = false;\n    public static List<String> randomTerms = new ArrayList<>();\n\n    public static enum IndexName {\n    \tmessages_hour(\"messages.json\"), messages_day(\"messages.json\"), messages_week(\"messages.json\"), messages, queries, users, accounts, import_profiles;\n        private String schemaFileName;\n    \tprivate IndexName() {\n    \t    schemaFileName = this.name() + \".json\";\n    \t}\n    \tprivate IndexName(String filename) {\n            schemaFileName = filename;\n        }\n    \tpublic String getSchemaFilename() {\n    \t    return this.schemaFileName;\n    \t}\n    }\n\n    /**\n     * initialize the DAO\n     * @param configMap\n     * @param dataPath the path to the data directory\n     */\n    public static void init(Map<String, String> configMap, Path dataPath) throws Exception{\n\n        log(\"initializing loklak DAO\");\n\n        config = configMap;\n        data_dir = dataPath.toFile();\n        conf_dir = new File(\"conf\");\n        bin_dir = new File(\"bin\");\n        html_dir = new File(\"html\");\n\n        writeDump = DAO.getConfig(\"dump.write_enabled\", true);\n\n        // initialize public and private keys\n\t\tpublic_settings = new Settings(new File(\"data/settings/public.settings.json\"));\n\t\tFile private_file = new File(\"data/settings/private.settings.json\");\n\t\tprivate_settings = new Settings(private_file);\n\t\tOS.protectPath(private_file.toPath());\n\n\t\tif(!private_settings.loadPrivateKey() || !public_settings.loadPublicKey()){\n        \tlog(\"Can't load key pair. Creating new one\");\n\n        \t// create new key pair\n        \tKeyPairGenerator keyGen;\n\t\t\ttry {\n\t\t\t\tString algorithm = \"RSA\";\n\t\t\t\tkeyGen = KeyPairGenerator.getInstance(algorithm);\n\t\t\t\tkeyGen.initialize(2048);\n\t\t\t\tKeyPair keyPair = keyGen.genKeyPair();\n\t\t\t\tprivate_settings.setPrivateKey(keyPair.getPrivate(), algorithm);\n\t\t\t\tpublic_settings.setPublicKey(keyPair.getPublic(), algorithm);\n\t\t\t} catch (NoSuchAlgorithmException e) {\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t\tlog(\"Key creation finished. Peer hash: \" + public_settings.getPeerHashAlgorithm() + \" \" + public_settings.getPeerHash());\n        }\n        else{\n        \tlog(\"Key pair loaded from file. Peer hash: \" + public_settings.getPeerHashAlgorithm() + \" \" + public_settings.getPeerHash());\n        }\n\n        File datadir = dataPath.toFile();\n        // check if elasticsearch shall be accessed as external cluster\n        String transport = configMap.get(\"elasticsearch_transport.enabled\");\n        if (transport != null && \"true\".equals(transport)) {\n            String cluster_name = configMap.get(\"elasticsearch_transport.cluster.name\");\n            String transport_addresses_string = configMap.get(\"elasticsearch_transport.addresses\");\n            if (transport_addresses_string != null && transport_addresses_string.length() > 0) {\n                String[] transport_addresses = transport_addresses_string.split(\",\");\n                elasticsearch_client = new ElasticsearchClient(transport_addresses, cluster_name);\n            }\n        } else {\n            // use all config attributes with a key starting with \"elasticsearch.\" to set elasticsearch settings\n\n        \tESLoggerFactory.setDefaultFactory(new Slf4jESLoggerFactory());\n            org.elasticsearch.common.settings.Settings.Builder settings = org.elasticsearch.common.settings.Settings.builder();\n            for (Map.Entry<String, String> entry: config.entrySet()) {\n                String key = entry.getKey();\n                if (key.startsWith(\"elasticsearch.\")) settings.put(key.substring(14), entry.getValue());\n            }\n            // patch the home path\n            settings.put(\"path.home\", datadir.getAbsolutePath());\n            settings.put(\"path.data\", datadir.getAbsolutePath());\n            settings.build();\n\n            // start elasticsearch\n            elasticsearch_client = new ElasticsearchClient(settings);\n        }\n\n        // open AAA storage\n        Path settings_dir = dataPath.resolve(\"settings\");\n        settings_dir.toFile().mkdirs();\n        Path authentication_path = settings_dir.resolve(\"authentication.json\");\n        authentication = new JsonTray(authentication_path.toFile(), 10000);\n        OS.protectPath(authentication_path);\n        Path authorization_path = settings_dir.resolve(\"authorization.json\");\n        authorization = new JsonTray(authorization_path.toFile(), 10000);\n        OS.protectPath(authorization_path);\n        Path passwordreset_path = settings_dir.resolve(\"passwordreset.json\");\n        passwordreset = new JsonTray(passwordreset_path.toFile(), 10000);\n        OS.protectPath(passwordreset_path);\n        Path accounting_path = settings_dir.resolve(\"accounting.json\");\n        accounting = new JsonTray(accounting_path.toFile(), 10000);\n        OS.protectPath(accounting_path);\n        Path login_keys_path = settings_dir.resolve(\"login-keys.json\");\n        login_keys = new JsonFile(login_keys_path.toFile());\n        OS.protectPath(login_keys_path);\n\n\n        DAO.log(\"Initializing user roles\");\n\n        Path userRoles_path = settings_dir.resolve(\"userRoles.json\");\n        userRoles = new UserRoles(new JsonFile(userRoles_path.toFile()));\n        OS.protectPath(userRoles_path);\n\n        try{\n            userRoles.loadUserRolesFromObject();\n            DAO.log(\"Loaded user roles from file\");\n        }catch (IllegalArgumentException e){\n            DAO.log(\"Load default user roles\");\n            userRoles.loadDefaultUserRoles();\n        }\n\n        // open index\n        Path index_dir = dataPath.resolve(\"index\");\n        if (index_dir.toFile().exists()) OS.protectPath(index_dir); // no other permissions to this path\n\n        // define the index factories\n        boolean noio = configMap.containsValue(\"noio\") && configMap.get(\"noio\").equals(\"true\");\n        messages = new MessageFactory(noio ? null : elasticsearch_client, IndexName.messages.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        messages_hour = new MessageFactory(noio ? null : elasticsearch_client, IndexName.messages_hour.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        messages_day = new MessageFactory(noio ? null : elasticsearch_client, IndexName.messages_day.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        messages_week = new MessageFactory(noio ? null : elasticsearch_client, IndexName.messages_week.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        users = new UserFactory(noio ? null : elasticsearch_client, IndexName.users.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        accounts = new AccountFactory(noio ? null : elasticsearch_client, IndexName.accounts.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        queries = new QueryFactory(noio ? null : elasticsearch_client, IndexName.queries.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n        importProfiles = new ImportProfileFactory(noio ? null : elasticsearch_client, IndexName.import_profiles.name(), CACHE_MAXSIZE, EXIST_MAXSIZE);\n\n        // create indices and set mapping (that shows how 'elastic' elasticsearch is: it's always good to define data types)\n        File mappingsDir = new File(new File(conf_dir, \"elasticsearch\"), \"mappings\");\n        int shards = Integer.parseInt(configMap.get(\"elasticsearch.index.number_of_shards\"));\n        int replicas = Integer.parseInt(configMap.get(\"elasticsearch.index.number_of_replicas\"));\n        for (IndexName index: IndexName.values()) {\n            log(\"initializing index '\" + index.name() + \"'...\");\n        \ttry {\n        \t    elasticsearch_client.createIndexIfNotExists(index.name(), shards, replicas);\n        \t} catch (Throwable e) {\n        \t\tDAO.severe(e);\n        \t}\n            try {\n                elasticsearch_client.setMapping(index.name(), new File(mappingsDir, index.getSchemaFilename()));\n            } catch (Throwable e) {\n            \tDAO.severe(e);\n            }\n        }\n        // elasticsearch will probably take some time until it is started up. We do some other stuff meanwhile..\n\n        // create and document the data dump dir\n        assets = new File(datadir, \"assets\").getAbsoluteFile();\n        external_data = new File(datadir, \"external\");\n        dictionaries = new File(external_data, \"dictionaries\");\n        dictionaries.mkdirs();\n\n        push_cache_dir = dataPath.resolve(\"pushcache\");\n        push_cache_dir.toFile().mkdirs();\n\n        // create message dump dir\n        String message_dump_readme =\n            \"This directory contains dump files for messages which arrived the platform.\\n\" +\n            \"There are three subdirectories for dump files:\\n\" +\n            \"- own:      for messages received with this peer. There is one file for each month.\\n\" +\n            \"- import:   hand-over directory for message dumps to be imported. Drop dumps here and they are imported.\\n\" +\n            \"- imported: dump files which had been processed from the import directory are moved here.\\n\" +\n            \"You can import dump files from other peers by dropping them into the import directory.\\n\" +\n            \"Each dump file must start with the prefix '\" + MESSAGE_DUMP_FILE_PREFIX + \"' to be recognized.\\n\";\n        message_dump_dir = dataPath.resolve(\"dump\");\n        message_dump = new JsonRepository(message_dump_dir.toFile(), MESSAGE_DUMP_FILE_PREFIX, message_dump_readme, JsonRepository.COMPRESSED_MODE, true, Runtime.getRuntime().availableProcessors());\n\n        account_dump_dir = dataPath.resolve(\"accounts\");\n        account_dump_dir.toFile().mkdirs();\n        OS.protectPath(account_dump_dir); // no other permissions to this path\n        account_dump = new JsonRepository(account_dump_dir.toFile(), ACCOUNT_DUMP_FILE_PREFIX, null, JsonRepository.REWRITABLE_MODE, false, Runtime.getRuntime().availableProcessors());\n\n        File user_dump_dir = new File(datadir, \"accounts\");\n        user_dump_dir.mkdirs();\n        log(\"initializing user dump ...\");\n        user_dump = new JsonDataset(\n                user_dump_dir,USER_DUMP_FILE_PREFIX,\n                new JsonDataset.Column[]{new JsonDataset.Column(\"id_str\", false), new JsonDataset.Column(\"screen_name\", true)},\n                \"retrieval_date\", DateParser.PATTERN_ISO8601MILLIS,\n                JsonRepository.REWRITABLE_MODE, false, Integer.MAX_VALUE);\n        log(\"initializing followers dump ...\");\n        followers_dump = new JsonDataset(\n                user_dump_dir, FOLLOWERS_DUMP_FILE_PREFIX,\n                new JsonDataset.Column[]{new JsonDataset.Column(\"screen_name\", true)},\n                \"retrieval_date\", DateParser.PATTERN_ISO8601MILLIS,\n                JsonRepository.REWRITABLE_MODE, false, Integer.MAX_VALUE);\n        log(\"initializing following dump ...\");\n        following_dump = new JsonDataset(\n                user_dump_dir, FOLLOWING_DUMP_FILE_PREFIX,\n                new JsonDataset.Column[]{new JsonDataset.Column(\"screen_name\", true)},\n                \"retrieval_date\", DateParser.PATTERN_ISO8601MILLIS,\n                JsonRepository.REWRITABLE_MODE, false, Integer.MAX_VALUE);\n\n        Path log_dump_dir = dataPath.resolve(\"log\");\n        log_dump_dir.toFile().mkdirs();\n        OS.protectPath(log_dump_dir); // no other permissions to this path\n        access = new AccessTracker(log_dump_dir.toFile(), ACCESS_DUMP_FILE_PREFIX, 60000, 3000);\n        access.start(); // start monitor\n\n        timelineCache = new TimelineCache(60000);\n\n        import_profile_dump_dir = dataPath.resolve(\"import-profiles\");\n        import_profile_dump = new JsonRepository(import_profile_dump_dir.toFile(), IMPORT_PROFILE_FILE_PREFIX, null, JsonRepository.COMPRESSED_MODE, false, Runtime.getRuntime().availableProcessors());\n\n        // load schema folder\n        conv_schema_dir = new File(\"conf/conversion\");\n        schema_dir = new File(\"conf/schema\");\n\n        // load dictionaries if they are embedded here\n        // read the file allCountries.zip from http://download.geonames.org/export/dump/allCountries.zip\n        //File allCountries = new File(dictionaries, \"allCountries.zip\");\n        File cities1000 = new File(dictionaries, \"cities1000.zip\");\n        if (!cities1000.exists()) {\n            // download this file\n            ClientConnection.download(\"http://download.geonames.org/export/dump/cities1000.zip\", cities1000);\n        }\n\n        if(cities1000.exists()){\n\t        try{\n\t        \tgeoNames = new GeoNames(cities1000, new File(conf_dir, \"iso3166.json\"), 1);\n\t        }catch(IOException e){\n\t        \tDAO.severe(e.getMessage());\n\t        \tcities1000.delete();\n\t        \tgeoNames = null;\n\t        }\n    \t}\n\n        // Connect to MQTT message broker\n        String mqttAddress = getConfig(\"stream.mqtt.address\", \"tcp://127.0.0.1:1883\");\n        streamEnabled = getConfig(\"stream.enabled\", false);\n        if (streamEnabled) {\n            mqttPublisher = new MQTTPublisher(mqttAddress);\n        }\n\n        // finally wait for healthy status of elasticsearch shards\n        ClusterHealthStatus required_status = ClusterHealthStatus.fromString(config.get(\"elasticsearch_requiredClusterHealthStatus\"));\n        boolean ok;\n        do {\n            log(\"Waiting for elasticsearch \" + required_status.name() + \" status\");\n            ok = elasticsearch_client.wait_ready(60000l, required_status);\n        } while (!ok);\n        /**\n        do {\n            log(\"Waiting for elasticsearch green status\");\n            health = elasticsearch_client.admin().cluster().prepareHealth().setWaitForGreenStatus().execute().actionGet();\n        } while (health.isTimedOut());\n        **/\n        log(\"elasticsearch has started up!\");\n\n        // start the classifier\n        new Thread(){\n            public void run() {\n                log(\"initializing the classifier...\");\n                try {\n                    Classifier.init(10000, 1000);\n                } catch (Throwable e) {\n                \tDAO.severe(e);\n                }\n                log(\"classifier initialized!\");\n            }\n        }.start();\n\n        log(\"initializing queries...\");\n        File harvestingPath = new File(datadir, \"queries\");\n        if (!harvestingPath.exists()) harvestingPath.mkdirs();\n        String[] list = harvestingPath.list();\n        if (list.length < 10) {\n            // use the test data instead\n            harvestingPath = new File(new File(datadir.getParentFile(), \"test\"), \"queries\");\n            list = harvestingPath.list();\n        }\n        for (String queryfile: list) {\n            if (queryfile.startsWith(\".\") || queryfile.endsWith(\"~\")) continue;\n            try {\n                BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(new File(harvestingPath, queryfile))));\n                String line;\n                List<IndexEntry<QueryEntry>> bulkEntries = new ArrayList<>();\n                while ((line = reader.readLine()) != null) {\n                    line = line.trim().toLowerCase();\n                    if (line.length() == 0) continue;\n                    if (line.charAt(0) <= '9') {\n                        // truncate statistic\n                        int p = line.indexOf(' ');\n                        if (p < 0) continue;\n                        line = line.substring(p + 1).trim();\n                    }\n                    // write line into query database\n                    if (!existQuery(line)) {\n                        randomTerms.add(line);\n                        bulkEntries.add(\n                            new IndexEntry<QueryEntry>(\n                                line,\n                                SourceType.TWITTER,\n                                new QueryEntry(line, 0, 60000, SourceType.TWITTER, false))\n                        );\n                    }\n                    if (bulkEntries.size() > 1000) {\n                        queries.writeEntries(bulkEntries);\n                        bulkEntries.clear();\n                    }\n                }\n                queries.writeEntries(bulkEntries);\n                reader.close();\n            } catch (IOException e) {\n            \tDAO.severe(e);\n            }\n        }\n        log(\"queries initialized.\");\n\n        log(\"finished DAO initialization\");\n    }\n\n    public static boolean wait_ready(long maxtimemillis) {\n        ClusterHealthStatus required_status = ClusterHealthStatus.fromString(config.get(\"elasticsearch_requiredClusterHealthStatus\"));\n        return elasticsearch_client.wait_ready(maxtimemillis, required_status);\n    }\n\n    public static String pendingClusterTasks() {\n        return elasticsearch_client.pendingClusterTasks();\n    }\n\n    public static String clusterStats() {\n        return elasticsearch_client.clusterStats();\n    }\n\n    public static Map<String, String> nodeSettings() {\n        return elasticsearch_client.nodeSettings();\n    }\n\n    public static File getAssetFile(String screen_name, String id_str, String file) {\n        String letter0 = (\"\" + screen_name.charAt(0)).toLowerCase();\n        String letter1 = (\"\" + screen_name.charAt(1)).toLowerCase();\n        Path storage_path = IO.resolvePath(assets.toPath(), letter0, letter1, screen_name);\n        return IO.resolvePath(storage_path, id_str + \"_\" + file).toFile(); // all assets for one user in one file\n    }\n\n    public static Collection<File> getTweetOwnDumps(int count) {\n        return message_dump.getOwnDumps(count);\n    }\n\n    public static void importAccountDumps(int count) throws IOException {\n        Collection<File> dumps = account_dump.getImportDumps(count);\n        if (dumps == null || dumps.size() == 0) return;\n        for (File dump: dumps) {\n            JsonReader reader = account_dump.getDumpReader(dump);\n            final JsonReader dumpReader = reader;\n            Thread[] indexerThreads = new Thread[dumpReader.getConcurrency()];\n            for (int i = 0; i < dumpReader.getConcurrency(); i++) {\n                indexerThreads[i] = new Thread() {\n                    public void run() {\n                        JsonFactory accountEntry;\n                        try {\n                            while ((accountEntry = dumpReader.take()) != JsonStreamReader.POISON_JSON_MAP) {\n                                try {\n                                    JSONObject json = accountEntry.getJSON();\n                                    AccountEntry a = new AccountEntry(json);\n                                    DAO.writeAccount(a, false);\n                                } catch (IOException e) {\n                                \tDAO.severe(e);\n                                }\n                            }\n                        } catch (InterruptedException e) {\n                        \tDAO.severe(e);\n                        }\n                    }\n                };\n                indexerThreads[i].start();\n            }\n            for (int i = 0; i < dumpReader.getConcurrency(); i++) {\n                try {indexerThreads[i].join();} catch (InterruptedException e) {}\n            }\n            account_dump.shiftProcessedDump(dump.getName());\n        }\n    }\n\n    /**\n     * close all objects in this class\n     */\n    public static void close() {\n        DAO.log(\"closing DAO\");\n\n        // close the dump files\n        message_dump.close();\n        account_dump.close();\n        import_profile_dump.close();\n        user_dump.close();\n        followers_dump.close();\n        following_dump.close();\n\n        // close the tracker\n        access.close();\n\n        // close the index factories (flushes the caches)\n        messages.close();\n        messages_hour.close();\n        messages_day.close();\n        messages_week.close();\n        users.close();\n        accounts.close();\n        queries.close();\n        importProfiles.close();\n\n        // close the index\n        elasticsearch_client.close();\n\n        DAO.log(\"closed DAO\");\n    }\n\n    /**\n     * get values from\n     * @param key\n     * @param default_val\n     * @return\n     */\n    public static String getConfig(String key, String default_val) {\n        String value = config.get(key);\n        return value == null ? default_val : value;\n    }\n\n    public static String[] getConfig(String key, String[] default_val, String delim) {\n        String value = config.get(key);\n        return value == null || value.length() == 0 ? default_val : value.split(delim);\n    }\n\n    public static long getConfig(String key, long default_val) {\n        String value = config.get(key);\n        try {\n            return value == null ? default_val : Long.parseLong(value);\n        } catch (NumberFormatException e) {\n            return default_val;\n        }\n    }\n\n    public static double getConfig(String key, double default_val) {\n        String value = config.get(key);\n        try {\n            return value == null ? default_val : Double.parseDouble(value);\n        } catch (NumberFormatException e) {\n            return default_val;\n        }\n    }\n\n    public static int getConfig(String key, int default_val) {\n        String value = config.get(key);\n        try {\n            return value == null ? default_val : Integer.parseInt(value);\n        } catch (NumberFormatException e) {\n            return default_val;\n        }\n    }\n/*\n    public static void setConfig(String key, String value) {\n        config.put(key, value);\n    }\n\n    public static void setConfig(String key, long value) {\n        setConfig(key, Long.toString(value));\n    }\n\n    public static void setConfig(String key, double value) {\n        setConfig(key, Double.toString(value));\n    }\n*/\n    public static JsonNode getSchema(String key) throws IOException {\n        File schema = new File(schema_dir, key);\n        if (!schema.exists()) {\n            throw new FileNotFoundException(\"No schema file with name \" + key + \" found\");\n        }\n        return JsonLoader.fromFile(schema);\n    }\n\n    public static JSONObject getConversionSchema(String key) throws IOException {\n        File schema = new File(conv_schema_dir, key);\n        if (!schema.exists()) {\n            throw new FileNotFoundException(\"No schema file with name \" + key + \" found\");\n        }\n        return new JSONObject(com.google.common.io.Files.toString(schema, Charsets.UTF_8));\n    }\n\n    public static boolean getConfig(String key, boolean default_val) {\n        String value = config.get(key);\n        return value == null ? default_val : value.equals(\"true\") || value.equals(\"on\") || value.equals(\"1\");\n    }\n\n    public static Set<String> getConfigKeys() {\n        return config.keySet();\n    }\n\n    public static class MessageWrapper {\n        public TwitterTweet t;\n        public UserEntry u;\n        public boolean dump;\n        public MessageWrapper(TwitterTweet t, UserEntry u, boolean dump) {\n            this.t = t;\n            this.u = u;\n            this.dump = dump;\n        }\n    }\n\n    /**\n     * Store a message together with a user into the search index\n     * @param mw a message wrapper\n     * @return true if the record was stored because it did not exist, false if it was not stored because the record existed already\n     */\n    public static boolean writeMessage(MessageWrapper mw) {\n        if (mw.t == null) return false;\n        try {\n            synchronized (DAO.class) {\n                // record tweet into search index and check if this is a new entry\n                // and check if the message exists\n                boolean exists = false;\n                if (mw.t.getCreatedAt().after(DateParser.oneHourAgo())) {\n                    exists = messages_hour.writeEntry(new IndexEntry<Post>(mw.t.getPostId(), mw.t.getSourceType(), mw.t));\n                    if (exists) return false;\n                }\n                if (mw.t.getCreatedAt().after(DateParser.oneDayAgo())) {\n                    exists = messages_day.writeEntry(new IndexEntry<Post>(mw.t.getPostId(), mw.t.getSourceType(), mw.t));\n                    if (exists) return false;\n                }\n                if (mw.t.getCreatedAt().after(DateParser.oneWeekAgo())) {\n                    exists = messages_week.writeEntry(new IndexEntry<Post>(mw.t.getPostId(), mw.t.getSourceType(), mw.t));\n                    if (exists) return false;\n                }\n                exists = messages.writeEntry(new IndexEntry<Post>(mw.t.getPostId(), mw.t.getSourceType(), mw.t));\n                if (exists) return false;\n\n                // write the user into the index\n                users.writeEntryAsync(new IndexEntry<UserEntry>(mw.u.getScreenName(), mw.t.getSourceType(), mw.u));\n\n                // record tweet into text file\n                if (mw.dump && writeDump) {\n                    message_dump.write(mw.t.toJSON(mw.u, false, Integer.MAX_VALUE, \"\"), true);\n                }\n                mw.t.publishToMQTT();\n             }\n\n            // teach the classifier\n            Classifier.learnPhrase(mw.t.getText());\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        return true;\n    }\n\n    public static Set<String> writeMessageBulk(Collection<MessageWrapper> mws) {\n        List<MessageWrapper> noDump = new ArrayList<>();\n        List<MessageWrapper> dump = new ArrayList<>();\n        for (MessageWrapper mw: mws) {\n            if (mw.t == null) continue;\n            if (mw.dump) dump.add(mw); else noDump.add(mw);\n        }\n        Set<String> createdIDs = new HashSet<>();\n        createdIDs.addAll(writeMessageBulkNoDump(noDump));\n        // does also do an writeMessageBulkNoDump internally\n        createdIDs.addAll(writeMessageBulkDump(dump));\n        return createdIDs;\n    }\n\n    public static Set<String> writeMessageBulk(Set<PostTimeline> postBulk) {\n        for (PostTimeline postList: postBulk) {\n            if (postList.size() < 1) continue;\n            if(postList.dump) {\n                writeMessageBulkDump(postList);\n            }\n            writeMessageBulkNoDump(postList);\n        }\n        //TODO: return total dumped, or IDs dumped to create hash of IDs\n        return new HashSet<>();\n    }\n\n    private static Set<String> writeMessageBulkNoDump(PostTimeline postList) {\n        if (postList.size() == 0) return new HashSet<>();\n        List<Post> messageBulk = new ArrayList<>();\n\n        for (Post post: postList) {\n            if (messages.existsCache(post.getPostId())) {\n                continue;\n            }\n            synchronized (DAO.class) {\n                messageBulk.add(post);\n            }\n        }\n        BulkWriteResult result = null;\n        try {\n            Date limitDate = new Date();\n            limitDate.setTime(DateParser.oneHourAgo().getTime());\n            List<Post> macc = new ArrayList<Post>();\n            final Set<String> existed = new HashSet<>();\n            long hourLong = limitDate.getTime();\n            for(Post post : messageBulk) {\n                if(hourLong <= post.getTimestamp()) macc.add(post);\n            }\n            \n            result = messages_hour.writeEntries(macc);\n            for (Post i: macc) if (!(result.getCreated().contains(i.getPostId()))) existed.add(i.getPostId());\n\n            limitDate.setTime(DateParser.oneDayAgo().getTime());\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getPostId()))).filter(i -> i.getCreated().after(limitDate)).collect(Collectors.toList());\n            result = messages_day.writeEntries(macc);\n            for (Post i: macc) if (!(result.getCreated().contains(i.getPostId()))) existed.add(i.getPostId());\n\n            limitDate.setTime(DateParser.oneWeekAgo().getTime());\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getPostId())))\n                    .filter(i -> i.getCreated().after(limitDate)).collect(Collectors.toList());\n            result = messages_week.writeEntries(macc);\n            for (Post i: macc) if (!(result.getCreated().contains(i.getPostId()))) existed.add(i.getPostId());\n\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getPostId()))).collect(Collectors.toList());\n            result = messages.writeEntries(macc);\n            for (Post i: macc) if (!(result.getCreated().contains(i.getPostId()))) existed.add(i.getPostId());\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        if (result == null) return new HashSet<String>();\n        return result.getCreated();\n    }\n\n    /**\n     * write messages without writing them to the dump file\n     * @param mws a collection of message wrappers\n     * @return a set of message IDs which had been created with this bulk write.\n     */\n    private static Set<String> writeMessageBulkNoDump(Collection<MessageWrapper> mws) {\n        if (mws.size() == 0) return new HashSet<>();\n        List<IndexEntry<UserEntry>> userBulk = new ArrayList<>();\n        List<IndexEntry<Post>> messageBulk = new ArrayList<>();\n        for (MessageWrapper mw: mws) {\n            if (messages.existsCache(mw.t.getPostId())) continue; // we omit writing this again\n            synchronized (DAO.class) {\n                // write the user into the index\n                userBulk.add(new IndexEntry<UserEntry>(mw.u.getScreenName(), mw.t.getSourceType(), mw.u));\n\n                // record tweet into search index\n                messageBulk.add(new IndexEntry<Post>(mw.t.getPostId(), mw.t.getSourceType(), mw.t));\n             }\n\n            // teach the classifier\n            Classifier.learnPhrase(mw.t.getText());\n        }\n        BulkWriteResult result = null;\n        Set<String> created_ids = new HashSet<>();\n        try {\n            final Date limitDate = new Date();\n            List<IndexEntry<Post>> macc;\n            final Set<String> existed = new HashSet<>();\n\n            //DAO.log(\"***DEBUG messages     INIT: \" + messageBulk.size());\n\n            limitDate.setTime(DateParser.oneHourAgo().getTime());\n            macc = messageBulk.stream().filter(i -> i.getObject().getCreated().after(limitDate)).collect(Collectors.toList());\n            //DAO.log(\"***DEBUG messages for HOUR: \" + macc.size());\n            result = messages_hour.writeEntries(macc);\n            created_ids.addAll(result.getCreated());\n            //DAO.log(\"***DEBUG messages for HOUR: \" + result.getCreated().size() + \"  created\");\n            for (IndexEntry<Post> i: macc) if (!(result.getCreated().contains(i.getId()))) existed.add(i.getId());\n            //DAO.log(\"***DEBUG messages for HOUR: \" + existed.size() + \"  existed\");\n\n            limitDate.setTime(DateParser.oneDayAgo().getTime());\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getObject().getPostId()))).filter(i -> i.getObject().getCreated().after(limitDate)).collect(Collectors.toList());\n            //DAO.log(\"***DEBUG messages for  DAY : \" + macc.size());\n            result = messages_day.writeEntries(macc);\n            created_ids.addAll(result.getCreated());\n            //DAO.log(\"***DEBUG messages for  DAY: \" + result.getCreated().size() + \" created\");\n            for (IndexEntry<Post> i: macc) if (!(result.getCreated().contains(i.getId()))) existed.add(i.getId());\n            //DAO.log(\"***DEBUG messages for  DAY: \" + existed.size()  + \"  existed\");\n\n            limitDate.setTime(DateParser.oneWeekAgo().getTime());\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getObject().getPostId()))).filter(i -> i.getObject().getCreated().after(limitDate)).collect(Collectors.toList());\n            //DAO.log(\"***DEBUG messages for WEEK: \" + macc.size());\n            result = messages_week.writeEntries(macc);\n            created_ids.addAll(result.getCreated());\n            //DAO.log(\"***DEBUG messages for WEEK: \" + result.getCreated().size() + \"  created\");\n            for (IndexEntry<Post> i: macc) if (!(result.getCreated().contains(i.getId()))) existed.add(i.getId());\n            //DAO.log(\"***DEBUG messages for WEEK: \" + existed.size()  + \"  existed\");\n\n            macc = messageBulk.stream().filter(i -> !(existed.contains(i.getObject().getPostId()))).collect(Collectors.toList());\n            //DAO.log(\"***DEBUG messages for  ALL : \" + macc.size());\n            result = messages.writeEntries(macc);\n            created_ids.addAll(result.getCreated());\n            //DAO.log(\"***DEBUG messages for  ALL: \" + result.getCreated().size() + \"  created\");\n            for (IndexEntry<Post> i: macc) if (!(result.getCreated().contains(i.getId()))) existed.add(i.getId());\n            //DAO.log(\"***DEBUG messages for  ALL: \" + existed.size()  + \"  existed\");\n\n            users.writeEntries(userBulk);\n\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        return created_ids;\n    }\n\n    private static Set<String> writeMessageBulkDump(Collection<MessageWrapper> mws) {\n        Set<String> created = writeMessageBulkNoDump(mws);\n\n        for (MessageWrapper mw: mws) try {\n            mw.t.publishToMQTT();\n            if (!created.contains(mw.t.getPostId())) continue;\n            synchronized (DAO.class) {\n                \n                // record tweet into text file\n                if (writeDump) {\n                    message_dump.write(mw.t.toJSON(mw.u, false, Integer.MAX_VALUE, \"\"), true);\n                }\n            }\n\n            // teach the classifier\n            if (randomPicker.nextInt(100) == 0) Classifier.learnPhrase(mw.t.getText());\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n\n        return created;\n    }\n\n    private static Set<String> writeMessageBulkDump(PostTimeline postList) {\n        Set<String> created = writeMessageBulkNoDump(postList);\n\n        for (Post post: postList) try {\n            if (!created.contains(post.getPostId())) continue;\n            synchronized (DAO.class) {\n                // record tweet into text file\n                if (writeDump) {\n                    message_dump.write(post, true);\n                }\n            }\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        return created;\n    }\n\n    /**\n     * Store an account together with a user into the search index\n     * This method is synchronized to prevent concurrent IO caused by this call.\n     * @param a an account\n     * @param dump\n     * @return true if the record was stored because it did not exist, false if it was not stored because the record existed already\n     */\n    public static boolean writeAccount(AccountEntry a, boolean dump) {\n        try {\n            // record account into text file\n            if (dump && writeDump) {\n                account_dump.write(a.toJSON(null), true);\n            }\n\n            // record account into search index\n            accounts.writeEntryAsync(new IndexEntry<AccountEntry>(a.getScreenName(), a.getSourceType(), a));\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        return true;\n    }\n\n    /**\n     * Store an import profile into the search index\n     * @param i an import profile\n     * @return true if the record was stored because it did not exist, false if it was not stored because the record existed already\n     */\n    public static boolean writeImportProfile(ImportProfileEntry i, boolean dump) {\n        try {\n            // record import profile into text file\n            if (dump && writeDump) {\n                import_profile_dump.write(i.toJSON(), true);\n            }\n            // record import profile into search index\n            importProfiles.writeEntryAsync(new IndexEntry<ImportProfileEntry>(i.getId(), i.getSourceType(), i));\n        } catch (IOException e) {\n        \tDAO.severe(e);\n        }\n        return true;\n    }\n\n    private static long countLocalHourMessages(final long millis, boolean created_at) {\n        if (millis > DateParser.HOUR_MILLIS) return countLocalDayMessages(millis, created_at);\n        if (created_at && millis == DateParser.HOUR_MILLIS) return elasticsearch_client.count(IndexName.messages_hour.name());\n        return elasticsearch_client.count(\n                created_at ? IndexName.messages_hour.name() : IndexName.messages_day.name(),\n                created_at ? AbstractObjectEntry.CREATED_AT_FIELDNAME : AbstractObjectEntry.TIMESTAMP_FIELDNAME,\n                millis);\n    }\n\n    private static long countLocalDayMessages(final long millis, boolean created_at) {\n        if (millis > DateParser.DAY_MILLIS) return countLocalWeekMessages(millis, created_at);\n        if (created_at && millis == DateParser.DAY_MILLIS) return elasticsearch_client.count(IndexName.messages_day.name());\n        return elasticsearch_client.count(\n                created_at ? IndexName.messages_day.name() : IndexName.messages_week.name(),\n                created_at ? AbstractObjectEntry.CREATED_AT_FIELDNAME : AbstractObjectEntry.TIMESTAMP_FIELDNAME,\n                millis);\n    }\n\n    private static long countLocalWeekMessages(final long millis, boolean created_at) {\n        if (millis > DateParser.WEEK_MILLIS) return countLocalMessages(millis, created_at);\n        if (created_at && millis == DateParser.WEEK_MILLIS) return elasticsearch_client.count(IndexName.messages_week.name());\n        return elasticsearch_client.count(\n                created_at ? IndexName.messages_week.name() : IndexName.messages.name(),\n                created_at ? AbstractObjectEntry.CREATED_AT_FIELDNAME : AbstractObjectEntry.TIMESTAMP_FIELDNAME,\n                millis);\n    }\n\n    /**\n     * count the messages in the local index\n     * @param millis number of milliseconds in the past\n     * @param created_at field selector: true -> use CREATED_AT, the time when the tweet was created; false -> use TIMESTAMP, the time when the tweet was harvested\n     * @return the number of messages in that time span\n     */\n    public static long countLocalMessages(final long millis, boolean created_at) {\n        if (millis == 0) return 0;\n        if (millis > 0) {\n            if (millis <= DateParser.HOUR_MILLIS) return countLocalHourMessages(millis, created_at);\n            if (millis <= DateParser.DAY_MILLIS) return countLocalDayMessages(millis, created_at);\n            if (millis <= DateParser.WEEK_MILLIS) return countLocalWeekMessages(millis, created_at);\n        }\n        return elasticsearch_client.count(\n                IndexName.messages.name(),\n                created_at ? AbstractObjectEntry.CREATED_AT_FIELDNAME : AbstractObjectEntry.TIMESTAMP_FIELDNAME,\n                millis == Long.MAX_VALUE ? -1 : millis);\n    }\n\n    public static long countLocalMessages() {\n        return elasticsearch_client.count(IndexName.messages.name(), AbstractObjectEntry.TIMESTAMP_FIELDNAME, -1);\n    }\n\n    public static long countLocalMessages(String provider_hash) {\n        return elasticsearch_client.countLocal(IndexName.messages.name(), provider_hash);\n    }\n\n    public static long countLocalUsers() {\n        return elasticsearch_client.count(IndexName.users.name(), AbstractObjectEntry.TIMESTAMP_FIELDNAME, -1);\n    }\n\n    public static long countLocalQueries() {\n        return elasticsearch_client.count(IndexName.queries.name(), AbstractObjectEntry.TIMESTAMP_FIELDNAME, -1);\n    }\n\n    public static long countLocalAccounts() {\n        return elasticsearch_client.count(IndexName.accounts.name(), AbstractObjectEntry.TIMESTAMP_FIELDNAME, -1);\n    }\n\n    public static Post readMessage(String id) throws IOException {\n        Post m = null;\n        return messages_hour != null && ((m = messages_hour.read(id)) != null) ? m :\n               messages_day  != null && ((m = messages_day.read(id))  != null) ? m :\n               messages_week != null && ((m = messages_week.read(id)) != null) ? m :\n               messages.read(id);\n    }\n\n    public static boolean existMessage(String id) {\n        return messages_hour != null && messages_hour.exists(id) ||\n               messages_day  != null && messages_day.exists(id)  ||\n               messages_week != null && messages_week.exists(id) ||\n               messages      != null && messages.exists(id);\n    }\n\n    public static boolean existUser(String id) {\n        return users.exists(id);\n    }\n\n    public static boolean existQuery(String id) {\n        return queries.exists(id);\n    }\n\n    public static boolean deleteQuery(String id, SourceType sourceType) {\n        return queries.delete(id, sourceType);\n    }\n    \n    public static String getRandomTerm() {\n        return randomTerms.size() == 0 ? null : randomTerms.get(randomPicker.nextInt(randomTerms.size()));\n    }\n\n    public static boolean deleteImportProfile(String id, SourceType sourceType) {\n        return importProfiles.delete(id, sourceType);\n    }\n\n    public static int deleteOld(IndexName indexName, Date createDateLimit) {\n        RangeQueryBuilder rangeQuery = QueryBuilders.rangeQuery(AbstractObjectEntry.CREATED_AT_FIELDNAME).to(createDateLimit);\n        return elasticsearch_client.deleteByQuery(indexName.name(), rangeQuery);\n    }\n\n    public static class SearchLocalMessages {\n        public TwitterTimeline timeline;\n        public PostTimeline postList;\n        public Map<String, List<Map.Entry<String, AtomicLong>>> aggregations;\n        public ElasticsearchClient.Query query;\n\n        /**\n         * Search the local message cache using a elasticsearch query.\n         * @param q - the query, for aggregation this which should include a time frame in the form since:yyyy-MM-dd until:yyyy-MM-dd\n         * @param order_field - the field to order the results, i.e. Timeline.Order.CREATED_AT\n         * @param timezoneOffset - an offset in minutes that is applied on dates given in the query of the form since:date until:date\n         * @param resultCount - the number of messages in the result; can be zero if only aggregations are wanted\n         * @param aggregationLimit - the maximum count of facet entities, not search results\n         * @param aggregationFields - names of the aggregation fields. If no aggregation is wanted, pass no (zero) field(s)\n         * @param filterList - list of filters in String datatype\n         */\n        public SearchLocalMessages (\n                final String q,\n                final TwitterTimeline.Order orderField,\n                final int timezoneOffset,\n                final int resultCount,\n                final int aggregationLimit,\n                final ArrayList<String> filterList,\n                final String... aggregationFields\n        ) {\n            this.timeline = new TwitterTimeline(orderField);\n            QueryEntry.ElasticsearchQuery sq = new QueryEntry.ElasticsearchQuery(q, timezoneOffset, filterList);\n            long interval = sq.until.getTime() - sq.since.getTime();\n            IndexName resultIndex;\n            if (aggregationFields.length > 0 && q.contains(\"since:\")) {\n                if (q.contains(\"since:hour\")) {\n                    this.query =  elasticsearch_client.query((resultIndex = IndexName.messages_hour).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                } else if (q.contains(\"since:day\")) {\n                    this.query =  elasticsearch_client.query((resultIndex = IndexName.messages_day).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                } else if (q.contains(\"since:week\")) {\n                    this.query =  elasticsearch_client.query((resultIndex = IndexName.messages_week).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                } else {\n                    this.query = elasticsearch_client.query((resultIndex = IndexName.messages).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                }\n            } else {\n                // use only a time frame that is sufficient for a result\n                this.query = elasticsearch_client.query((resultIndex = IndexName.messages_hour).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                if (!q.contains(\"since:hour\") && insufficient(this.query, resultCount, aggregationLimit, aggregationFields)) {\n                    ElasticsearchClient.Query aq = elasticsearch_client.query((resultIndex = IndexName.messages_day).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                    this.query.add(aq);\n                    if (!q.contains(\"since:day\") && insufficient(this.query, resultCount, aggregationLimit, aggregationFields)) {\n                        this.query.add(aq);\n                        if (!q.contains(\"since:week\") && insufficient(this.query, resultCount, aggregationLimit, aggregationFields)) {\n                            aq = elasticsearch_client.query((resultIndex = IndexName.messages).name(), sq.queryBuilder, orderField.getMessageFieldName(), timezoneOffset, resultCount, interval, AbstractObjectEntry.CREATED_AT_FIELDNAME, aggregationLimit, aggregationFields);\n                            this.query.add(aq);\n                }}}\n            }\n                \n            timeline.setHits(query.getHitCount());\n            timeline.setResultIndex(resultIndex);\n\n            // evaluate search result\n            for (Map<String, Object> map: query.getResult()) {\n                TwitterTweet tweet = new TwitterTweet(new JSONObject(map));\n                try {\n                    UserEntry user = users.read(tweet.getScreenName());\n                    assert user != null;\n                    if (user != null) {\n                        timeline.add(tweet, user);\n                    }\n                } catch (IOException e) {\n                \tDAO.severe(e);\n                }\n            }\n            this.aggregations = query.getAggregations();\n        }\n\n        public SearchLocalMessages (\n                final String q,\n                final TwitterTimeline.Order orderField,\n                final int timezoneOffset,\n                final int resultCount,\n                final int aggregationLimit,\n                final String... aggregationFields\n        ) {\n            this(\n                q,\n                orderField,\n                timezoneOffset,\n                resultCount,\n                aggregationLimit,\n                new ArrayList<>(),\n                aggregationFields\n            );\n        }\n\n        public SearchLocalMessages (\n                Map<String, Map<String, String>> inputMap,\n                final PostTimeline.Order orderField,\n                final int resultCount\n        ) {\n            this.postList = new PostTimeline(orderField);\n            IndexName resultIndex;\n            QueryEntry.ElasticsearchQuery sq = new QueryEntry.ElasticsearchQuery(\n                    inputMap.get(\"get\"), inputMap.get(\"not_get\"), inputMap.get(\"also_get\"));\n\n            this.query = elasticsearch_client.query(\n                    (resultIndex = IndexName.messages_hour).name(),\n                    sq.queryBuilder,\n                    orderField.getMessageFieldName(),\n                    resultCount\n            );\n\n            if (this.query.getHitCount() < resultCount) {\n                this.query =  elasticsearch_client.query(\n                        (resultIndex = IndexName.messages_day).name(),\n                        sq.queryBuilder,\n                        orderField.getMessageFieldName(),\n                        resultCount\n                );\n            }\n\n            if (this.query.getHitCount() < resultCount) {\n                this.query =  elasticsearch_client.query(\n                        (resultIndex = IndexName.messages_week).name(),\n                        sq.queryBuilder,\n                        orderField.getMessageFieldName(),\n                        resultCount\n                );\n            }\n\n            if (this.query.getHitCount() < resultCount) {\n                this.query =  elasticsearch_client.query(\n                        (resultIndex = IndexName.messages).name(),\n                        sq.queryBuilder,\n                        orderField.getMessageFieldName(),\n                        resultCount\n                );\n            }\n\n            // Feed search results to postList\n            this.postList.setResultIndex(resultIndex);\n\n            addResults();\n        }\n\n        private void addResults() {\n            Post outputPost;\n            for (Map<String, Object> map: this.query.getResult()) {\n                outputPost = new Post(map);\n                this.postList.addPost(outputPost);\n            }\n        }\n\n        private static boolean insufficient(\n                ElasticsearchClient.Query query,\n                int resultCount,\n                int aggregationLimit,\n                String... aggregationFields\n        ) {\n            return query.getHitCount() < resultCount || (aggregationFields.length > 0\n                    && getAggregationResultLimit(query.getAggregations()) < aggregationLimit);\n        }\n\n        public JSONObject getAggregations() {\n            JSONObject json = new JSONObject(true);\n            if (aggregations == null) return json;\n            for (Map.Entry<String, List<Map.Entry<String, AtomicLong>>> aggregation: aggregations.entrySet()) {\n                JSONObject facet = new JSONObject(true);\n                for (Map.Entry<String, AtomicLong> a: aggregation.getValue()) {\n                    if (a.getValue().equals(query)) continue; // we omit obvious terms that cannot be used for faceting, like search for \"#abc\" -> most hashtag is \"#abc\"\n                    facet.put(a.getKey(), a.getValue().get());\n                }\n                json.put(aggregation.getKey(), facet);\n            }\n            return json;\n        }\n\n        private static int getAggregationResultLimit(Map<String, List<Map.Entry<String, AtomicLong>>> agg) {\n            if (agg == null) return 0;\n            int l = 0;\n            for (List<Map.Entry<String, AtomicLong>> a: agg.values()) l = Math.max(l, a.size());\n            return l;\n        }\n    }\n\n    public static LinkedHashMap<String, Long> FullDateHistogram(int timezoneOffset) {\n        return elasticsearch_client.fullDateHistogram(IndexName.messages.name(), timezoneOffset, AbstractObjectEntry.CREATED_AT_FIELDNAME);\n    }\n\n    /**\n     * Search the local user cache using a elasticsearch query.\n     * @param screen_name - the user id\n     */\n    public static UserEntry searchLocalUserByScreenName(final String screen_name) {\n        try {\n            return users.read(screen_name);\n        } catch (IOException e) {\n        \tDAO.severe(e);\n            return null;\n        }\n    }\n\n    public static UserEntry searchLocalUserByUserId(final String user_id) {\n        if (user_id == null || user_id.length() == 0) return null;\n        Map<String, Object> map = elasticsearch_client.query(IndexName.users.name(), UserEntry.field_user_id, user_id);\n        if (map == null) return null;\n        return new UserEntry(new JSONObject(map));\n    }\n\n    /**\n     * Search the local account cache using an elasticsearch query.\n     * @param screen_name - the user id\n     */\n    public static AccountEntry searchLocalAccount(final String screen_name) {\n        try {\n            return accounts.read(screen_name);\n        } catch (IOException e) {\n        \tDAO.severe(e);\n            return null;\n        }\n    }\n\n    /**\n     * Search the local message cache using a elasticsearch query.\n     * @param q - the query, can be empty for a matchall-query\n     * @param resultCount - the number of messages in the result\n     * @param sort_field - the field name to sort the result list, i.e. \"query_first\"\n     * @param sort_order - the sort order (you want to use SortOrder.DESC here)\n     */\n    public static ResultList<QueryEntry> SearchLocalQueries(final String q, final int resultCount, final String sort_field, final String default_sort_type, final SortOrder sort_order, final Date since, final Date until, final String range_field) {\n        ResultList<QueryEntry> queries = new ResultList<>();\n        ResultList<Map<String, Object>> result = elasticsearch_client.fuzzyquery(IndexName.queries.name(), \"query\", q, resultCount, sort_field, default_sort_type, sort_order, since, until, range_field);\n        queries.setHits(result.getHits());\n        for (Map<String, Object> map: result) {\n            queries.add(new QueryEntry(new JSONObject(map)));\n        }\n        return queries;\n    }\n\n    public static ImportProfileEntry SearchLocalImportProfiles(final String id) {\n        try {\n            return importProfiles.read(id);\n        } catch (IOException e) {\n        \tDAO.severe(e);\n            return null;\n        }\n    }\n\n    public static Collection<ImportProfileEntry> SearchLocalImportProfilesWithConstraints(final Map<String, String> constraints, boolean latest) throws IOException {\n        List<ImportProfileEntry> rawResults = new ArrayList<>();\n        List<Map<String, Object>> result = elasticsearch_client.queryWithConstraints(IndexName.import_profiles.name(), \"active_status\", ImportProfileEntry.EntryStatus.ACTIVE.name().toLowerCase(), constraints, latest);\n        for (Map<String, Object> map: result) {\n            rawResults.add(new ImportProfileEntry(new JSONObject(map)));\n        }\n\n        if (!latest) {\n            return rawResults;\n        }\n\n        // filter results to display only latest profiles\n        Map<String, ImportProfileEntry> latests = new HashMap<>();\n        for (ImportProfileEntry entry : rawResults) {\n            String uniqueKey;\n            if (entry.getImporter() != null) {\n                uniqueKey = entry.getSourceUrl() + entry.getImporter();\n            } else {\n                uniqueKey = entry.getSourceUrl() + entry.getClientHost();\n            }\n            if (latests.containsKey(uniqueKey)) {\n                if (entry.getLastModified().compareTo(latests.get(uniqueKey).getLastModified()) > 0) {\n                    latests.put(uniqueKey, entry);\n                }\n            } else {\n                latests.put(uniqueKey, entry);\n            }\n        }\n        return latests.values();\n    }\n\n    public static TwitterTimeline scrapeTwitter(\n            final Query post,\n            final String q,\n            final TwitterTimeline.Order order,\n            final int timezoneOffset,\n            boolean byUserQuery,\n            long timeout,\n            boolean recordQuery) {\n\n        return scrapeTwitter(post, new ArrayList<>(), q, order, timezoneOffset, byUserQuery, timeout, recordQuery);\n    }\n\n    public static TwitterTimeline scrapeTwitter(\n            final Query post,\n            final ArrayList<String> filterList,\n            final String q,\n            final TwitterTimeline.Order order,\n            final int timezoneOffset,\n            boolean byUserQuery,\n            long timeout,\n            boolean recordQuery) {\n        // retrieve messages from remote server\n\n        ArrayList<String> remote = DAO.getFrontPeers();\n        TwitterTimeline tl;\n        if (remote.size() > 0 && (peerLatency.get(remote.get(0)) == null || peerLatency.get(remote.get(0)).longValue() < 3000)) {\n            long start = System.currentTimeMillis();\n            tl = searchOnOtherPeers(remote, q, filterList, order, 100, timezoneOffset, \"all\", SearchServlet.frontpeer_hash, timeout); // all must be selected here to catch up missing tweets between intervals\n            // at this point the remote list can be empty as a side-effect of the remote search attempt\n            if (post != null && remote.size() > 0 && tl != null) post.recordEvent(\"remote_scraper_on_\" + remote.get(0), System.currentTimeMillis() - start);\n            if (tl == null || tl.size() == 0) {\n                // maybe the remote server died, we try then ourself\n                start = System.currentTimeMillis();\n\n                tl = TwitterScraper.search(q, filterList, order, true, true, 400);\n                if (post != null) post.recordEvent(\"local_scraper_after_unsuccessful_remote\", System.currentTimeMillis() - start);\n            } else {\n                tl.writeToIndex();\n            }\n        } else {\n            if (post != null && remote.size() > 0) post.recordEvent(\"omitted_scraper_latency_\" + remote.get(0), peerLatency.get(remote.get(0)));\n            long start = System.currentTimeMillis();\n\n            tl = TwitterScraper.search(q, filterList, order, true, true, 400);\n            if (post != null) post.recordEvent(\"local_scraper\", System.currentTimeMillis() - start);\n        }\n\n        // record the query\n        long start2 = System.currentTimeMillis();\n        QueryEntry qe = null;\n        try {\n            qe = queries.read(q);\n        } catch (IOException | JSONException e) {\n        \tDAO.severe(e);\n        }\n\n        if (recordQuery && Caretaker.acceptQuery4Retrieval(q)) {\n            if (qe == null) {\n                // a new query occurred\n                qe = new QueryEntry(q, timezoneOffset, tl.period(), SourceType.TWITTER, byUserQuery);\n            } else {\n                // existing queries are updated\n                qe.update(tl.period(), byUserQuery);\n            }\n            try {\n                queries.writeEntryAsync(new IndexEntry<QueryEntry>(q, qe.source_type == null ? SourceType.TWITTER : qe.source_type, qe));\n            } catch (IOException e) {\n            \tDAO.severe(e);\n            }\n        } else {\n            // accept rules may change, we want to delete the query then in the index\n            if (qe != null) queries.delete(q, qe.source_type);\n        }\n        if (post != null) post.recordEvent(\"query_recorder\", System.currentTimeMillis() - start2);\n        //log(\"SCRAPER: TIME LEFT after recording = \" + (termination - System.currentTimeMillis()));\n\n        return tl;\n    }\n\n\n    public static JSONArray scrapeLoklak(\n            Map<String, String> inputMap,\n            boolean byUserQuery,\n            boolean recordQuery,\n            JSONObject metadata) {\n        PostTimeline.Order order= getOrder(inputMap.get(\"order\"));\n        PostTimeline dataSet = new PostTimeline(order);\n        List<String> scraperList = Arrays.asList(inputMap.get(\"scraper\").trim().split(\"\\\\s*,\\\\s*\"));\n        List<BaseScraper> scraperObjList = getScraperObjects(scraperList, inputMap);\n        ExecutorService scraperRunner = Executors.newFixedThreadPool(scraperObjList.size());\n\n        try{\n            for (BaseScraper scraper : scraperObjList) {\n                scraperRunner.execute(() -> {\n                    dataSet.add(scraper.getData());\n                    \n                });\n            }\n        } finally {\n            scraperRunner.shutdown();\n            try {\n                scraperRunner.awaitTermination(3000L, TimeUnit.SECONDS);\n            } catch (InterruptedException e) { }\n        }\n        dataSet.collectMetadata(metadata);\n        return dataSet.toArray();\n    }\n\n    public static List<BaseScraper> getScraperObjects(List<String> scraperList, Map<String, String> inputMap) {\n        //TODO: use SourceType to get this job done\n        List<BaseScraper> scraperObjList = new ArrayList<BaseScraper>();\n        BaseScraper scraperObj = null;\n\n        if (scraperList.contains(\"github\") || scraperList.contains(\"all\")) {\n            scraperObj = new GithubProfileScraper(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        if (scraperList.contains(\"quora\") || scraperList.contains(\"all\")) {\n            scraperObj = new QuoraProfileScraper(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        if (scraperList.contains(\"instagram\") || scraperList.contains(\"all\")) {\n            scraperObj = new InstagramProfileScraper(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        if (scraperList.contains(\"youtube\") || scraperList.contains(\"all\")) {\n            scraperObj = new YoutubeScraper(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        if (scraperList.contains(\"wordpress\") || scraperList.contains(\"all\")) {\n            scraperObj = new WordpressCrawlerService(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        if (scraperList.contains(\"twitter\") || scraperList.contains(\"all\")) {\n            scraperObj = new TweetScraper(inputMap);\n            scraperObjList.add(scraperObj);\n        }\n        //TODO: add more scrapers\n        return scraperObjList;\n    }\n\n    public static PostTimeline.Order getOrder(String orderString) {\n        //TODO: order set according to input\n        return PostTimeline.parseOrder(\"timestamp\");\n    }\n\n    public static final Random random = new Random(System.currentTimeMillis());\n    private static final Map<String, Long> peerLatency = new HashMap<>();\n    private static ArrayList<String> getBestPeers(Collection<String> peers) {\n        ArrayList<String> best = new ArrayList<>();\n        if (peers == null || peers.size() == 0) return best;\n        // first check if any of the given peers has unknown latency\n        TreeMap<Long, String> o = new TreeMap<>();\n        for (String peer: peers) {\n            if (peerLatency.containsKey(peer)) {\n                o.put(peerLatency.get(peer) * 1000 + best.size(), peer);\n            } else {\n                best.add(peer);\n            }\n        }\n        best.addAll(o.values());\n        return best;\n    }\n    public static void healLatency(float factor) {\n        for (Map.Entry<String, Long> entry: peerLatency.entrySet()) {\n            entry.setValue((long) (factor * entry.getValue()));\n        }\n    }\n\n    private static Set<String> frontPeerCache = new HashSet<String>();\n    private static Set<String> backendPeerCache = new HashSet<String>();\n\n    public static void updateFrontPeerCache(RemoteAccess remoteAccess) {\n        if (remoteAccess.getLocalHTTPPort() >= 80) {\n            frontPeerCache.add(\"http://\" + remoteAccess.getRemoteHost() + (remoteAccess.getLocalHTTPPort() == 80 ? \"\" : \":\" + remoteAccess.getLocalHTTPPort()));\n        } else if (remoteAccess.getLocalHTTPSPort() >= 443) {\n            frontPeerCache.add(\"https://\" + remoteAccess.getRemoteHost() + (remoteAccess.getLocalHTTPSPort() == 443 ? \"\" : \":\" + remoteAccess.getLocalHTTPSPort()));\n        }\n    }\n\n    /**\n     * from all known front peers, generate a list of available peers, ordered by the peer latency\n     * @return a list of front peers. only the first one shall be used, but the other are fail-over peers\n     */\n    public static ArrayList<String> getFrontPeers() {\n        String[] remote = DAO.getConfig(\"frontpeers\", new String[0], \",\");\n        ArrayList<String> testpeers = new ArrayList<>();\n        if (remote.length > 0) {\n            for (String peer: remote) testpeers.add(peer);\n            return testpeers;\n        }\n        if (frontPeerCache.size() == 0) {\n            // add dynamically all peers that contacted myself\n            for (Map<String, RemoteAccess> hmap: RemoteAccess.history.values()) {\n                for (Map.Entry<String, RemoteAccess> peer: hmap.entrySet()) {\n                    updateFrontPeerCache(peer.getValue());\n                }\n            }\n        }\n        testpeers.addAll(frontPeerCache);\n        return getBestPeers(testpeers);\n    }\n\n    public static String[] getBackend() {\n        return DAO.getConfig(\"backend\", new String[0], \",\");\n    }\n    \n    public static List<String> getBackendPeers() {\n        List<String> testpeers = new ArrayList<>();\n        if (backendPeerCache.size() == 0) {\n            final String[] remote = DAO.getBackend();\n            for (String peer: remote) backendPeerCache.add(peer);\n        }\n        testpeers.addAll(backendPeerCache);\n        return getBestPeers(testpeers);\n    }\n\n    public static TwitterTimeline searchBackend(final String q,final ArrayList<String> filterList, final TwitterTimeline.Order order, final int count, final int timezoneOffset, final String where, final long timeout) {\n        List<String> remote = getBackendPeers();\n\n        if (remote.size() > 0 /*&& (peerLatency.get(remote.get(0)) == null || peerLatency.get(remote.get(0)) < 3000)*/) { // condition deactivated because we need always at least one peer\n            TwitterTimeline tt = searchOnOtherPeers(remote, q, filterList, order, count, timezoneOffset, where, SearchServlet.backend_hash, timeout);\n            if (tt != null) tt.writeToIndex();\n            return tt;\n        }\n        return null;\n    }\n\n    private final static Random randomPicker = new Random(System.currentTimeMillis());\n\n    public static TwitterTimeline searchOnOtherPeers(final List<String> remote, final String q, final ArrayList<String> filterList,final TwitterTimeline.Order order, final int count, final int timezoneOffset, final String source, final String provider_hash, final long timeout) {\n        // select remote peer\n        while (remote.size() > 0) {\n            int pick = randomPicker.nextInt(remote.size());\n            String peer = remote.get(pick);\n            long start = System.currentTimeMillis();\n            try {\n                TwitterTimeline tl = SearchServlet.search(new String[]{peer}, q, filterList, order, source, count, timezoneOffset, provider_hash, timeout);\n                peerLatency.put(peer, System.currentTimeMillis() - start);\n                // to show which peer was used for the retrieval, we move the picked peer to the front of the list\n                if (pick != 0) remote.add(0, remote.remove(pick));\n                tl.setScraperInfo(tl.getScraperInfo().length() > 0 ? peer + \",\" + tl.getScraperInfo() : peer);\n                return tl;\n            } catch (IOException e) {\n                DAO.log(\"searchOnOtherPeers: no IO to scraping target: \" + e.getMessage());\n                // the remote peer seems to be unresponsive, remove it (temporary) from the remote peer list\n                peerLatency.put(peer, DateParser.HOUR_MILLIS);\n                frontPeerCache.remove(peer);\n                backendPeerCache.remove(peer);\n                remote.remove(pick);\n            }\n        }\n        return null;\n    }\n\n    public final static Set<Number> newUserIds = ConcurrentHashMap.newKeySet();\n\n    public static void announceNewUserId(TwitterTimeline tl) {\n        for (TwitterTweet message: tl) {\n            UserEntry user = tl.getUser(message);\n            assert user != null;\n            if (user == null) continue;\n            Number id = user.getUser();\n            if (id != null) announceNewUserId(id);\n        }\n    }\n\n    public static void announceNewUserId(Number id) {\n        JsonFactory mapcapsule = DAO.user_dump.get(\"id_str\", id.toString());\n        JSONObject map = null;\n        try {map = mapcapsule == null ? null : mapcapsule.getJSON();} catch (IOException e) {}\n        if (map == null) newUserIds.add(id);\n    }\n\n    public static Set<Number> getNewUserIdsChunk() {\n        if (newUserIds.size() < 100) return null;\n        Set<Number> chunk = new HashSet<>();\n        Iterator<Number> i = newUserIds.iterator();\n        for (int j = 0; j < 100; j++) {\n            chunk.add(i.next());\n            i.remove();\n        }\n        return chunk;\n    }\n\n    /**\n     * For logging informational events\n     */\n    public static void log(String line) {\n        if (DAO.getConfig(\"flag.log.dao\", \"true\").equals(\"true\")) {\n            logger.info(line);\n        }\n    }\n\n    /**\n     * For events serious enough to inform and log, but not fatal.\n     */\n    public static void severe(String line) {\n        if (DAO.getConfig(\"flag.severe.dao\", \"true\").equals(\"true\")) {\n            logger.warn(line);\n        }\n    }\n\n    public static void severe(String line, Throwable e) {\n        if (DAO.getConfig(\"flag.severe.dao\", \"true\").equals(\"true\")) {\n            logger.warn(line, e);\n        }\n    }\n\n    public static void severe(Throwable e) {\n        if (DAO.getConfig(\"flag.severe.dao\", \"true\").equals(\"true\")) {\n            logger.warn(\"\", e);\n        }\n    }\n\n    /**\n     * For Debugging events (very noisy).\n     */\n    public static void debug(Throwable e) {\n        if (DAO.getConfig(\"flag.debug.dao\", \"true\").equals(\"true\")) {\n            DAO.severe(e);\n        }\n    }\n\n    /**\n     * For Stacktracing exceptions (preferred over debug).\n     */\n    public static void trace(Throwable e) {\n        if(DAO.getConfig(\"flag.trace.dao\", \"true\").equals(\"true\")) {\n            e.printStackTrace();\n        }\n    }\n}\n", "package org.loklak.tools;\n\nimport javax.annotation.Nonnull;\n\nimport org.loklak.data.DAO;\n\nimport java.io.IOException;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.security.*;\nimport java.security.spec.InvalidKeySpecException;\nimport java.security.spec.X509EncodedKeySpec;\nimport java.util.Base64;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic final class IO {\n\t\n\tprivate static Map<Path,String> map;\n\tprivate static boolean initialized = false;\n\n\tpublic static String readFile(@Nonnull Path path) throws IOException\n\t{\n\t\tbyte[] encoded = Files.readAllBytes(path);\n\t\treturn new String(encoded);\n\t}\n\t\n\tpublic static String readFileCached(@Nonnull Path path) throws IOException\n\t{\n\t\tPath absPath = path.toAbsolutePath();\n\t\tif(!initialized) init();\n\t\tif(map.containsKey(absPath)){\n\t\t\treturn map.get(absPath);\n\t\t}\n\t\telse{\n\t\t\tString result = readFile(absPath);\n\t\t\tmap.put(absPath, result);\n\t\t\treturn result;\n\t\t}\n\t}\n\t\n\tprivate static void init(){\n\t\tmap = new HashMap<Path,String>();\n\t\tinitialized = true;\n\t}\n\n\t/**\n\t * Create hash for a key\n\t * @param pubkey\n\t * @param algorithm\n\t * @return String hash\n\t */\n\tpublic static String getKeyHash(@Nonnull PublicKey pubkey, @Nonnull String algorithm){\n\t\ttry {\n\t\t\tMessageDigest md = MessageDigest.getInstance(algorithm);\n\t\t\tmd.update(pubkey.getEncoded());\n\t\t\treturn Base64.getEncoder().encodeToString(md.digest());\n\t\t} catch (NoSuchAlgorithmException e) {\n\t\t\tDAO.severe(e);\n\t\t}\n\t\treturn null;\n\t}\n\n\t/**\n\t * Create hash for a key, use default algorithm SHA-256\n\t * @param pubkey\n\t * @return String hash\n\t */\n\tpublic static String getKeyHash(@Nonnull PublicKey pubkey){\n\t\treturn getKeyHash(pubkey, \"SHA-256\");\n\t}\n\n\t/**\n\t * Get String representation of a key\n\t * @param key\n\t * @return String representation of a key\n\t */\n\tpublic static String getKeyAsString(@Nonnull Key key){\n\t\treturn Base64.getEncoder().encodeToString(key.getEncoded());\n\t}\n\n\t/**\n\t * Create PublicKey from String representation\n\t * @param encodedKey\n\t * @param algorithm\n\t * @return PublicKey public_key\n\t */\n\tpublic synchronized static PublicKey decodePublicKey(@Nonnull String encodedKey, @Nonnull String algorithm){\n\t\ttry{\n\t\t\tX509EncodedKeySpec keySpec = new X509EncodedKeySpec(Base64.getDecoder().decode(encodedKey));\n\t\t\tPublicKey pub = KeyFactory.getInstance(algorithm).generatePublic(keySpec);\n\t\t\treturn pub;\n\t\t}\n\t\tcatch(NoSuchAlgorithmException | InvalidKeySpecException e){\n\t\t\tDAO.severe(e);\n\t\t}\n\t\treturn null;\n\t}\n\t\n\t/**\n\t * Resolves an untrusted user-specified path against the API's base directory.\n\t * Paths that try to escape the base directory are rejected.\n\t *\n\t * @param baseDirPath  the absolute path of the base directory that all\n\t * user-specified paths should be within\n\t * @param userPath  the untrusted path provided by the API user, expected to be\n\t * relative to {@code baseDirPath}\n\t */\n\tpublic static Path resolvePath(final Path baseDirPath, final Path userPath) {\n\t\tif (!baseDirPath.isAbsolute()) {\n\t\t\tthrow new IllegalArgumentException(\"Base path must be absolute\");\n\t\t}\n\n\t\tif (userPath.isAbsolute()) {\n\t\t\tthrow new IllegalArgumentException(\"User path must be relative\");\n\t\t}\n\n\t\t// Join the two paths together, then normalize so that any \"..\" elements\n\t\t// in the userPath can remove parts of baseDirPath.\n\t\t// (e.g. \"/foo/bar/baz\" + \"../attack\" -> \"/foo/bar/attack\")\n\t\tfinal Path resolvedPath = baseDirPath.resolve(userPath).normalize();\n\n\t\t// Make sure the resulting path is still within the required directory.\n\t\t// (In the example above, \"/foo/bar/attack\" is not.)\n\t\tif (!resolvedPath.startsWith(baseDirPath)) {\n\t\t\tthrow new IllegalArgumentException(\"User path escapes the base path\");\n\t\t}\n\n\t\treturn resolvedPath;\n\t}\n\n\tpublic static Path resolvePath(final Path baseDirPath, final String userPath) {\n\t\treturn resolvePath(baseDirPath, Paths.get(userPath));\n\t}\n\n\t/**\n\t * Checks each subsequent path to be strictly within the baseDirPath so that\n\t * no path argument leads to directory traversal attack\n\t *\n\t * E.g. /models/ + req.model + '/' + req.lang + /images/ + req.image\n\t * Should be checked for ('models', req.model, req.lang, 'images', req.image)\n\t * that each subsequent element is within the previous and not breaking out by passing\n\t * req.model => ..\n\t * req.lang  => ..\n\t * req.image => ../../private/data.json\n\t *\n\t * Since just checking the last argument isn't enough\n\t *\n\t * @param baseDirPath the absolute path of the base directory that all\n\t * user-specified paths should be within\n\t * @param paths the untrusted paths provided by the API user, expected to be\n\t * relative to {@code baseDirPath}\n\t */\n\tpublic static Path resolvePath(final Path baseDirPath, final String... paths) {\n\t\tPath resolved = baseDirPath;\n\t\tfor (String path: paths) {\n\t\t\tresolved = resolvePath(resolved, path);\n\t\t}\n\n\t\treturn resolved;\n\t}\n}\n"], "filenames": ["src/org/loklak/data/DAO.java", "src/org/loklak/tools/IO.java"], "buggy_code_start_loc": [101, 9], "buggy_code_end_loc": [530, 98], "fixing_code_start_loc": [102, 10], "fixing_code_end_loc": [531, 163], "type": "CWE-22", "message": "loklak is an open-source server application which is able to collect messages from various sources, including twitter. The server contains a search index and a peer-to-peer index sharing interface. All messages are stored in an elasticsearch index. In loklak less than or equal to commit 5f48476, a path traversal vulnerability exists. Insufficient input validation in the APIs exposed by the loklak server allowed a directory traversal vulnerability. Any admin configuration and files readable by the app available on the hosted file system can be retrieved by the attacker. Furthermore, user-controlled content could be written to any admin config and files readable by the application. This has been patched in commit 50dd692. Users will need to upgrade their hosted instances of loklak to not be vulnerable to this exploit.", "other": {"cve": {"id": "CVE-2020-15097", "sourceIdentifier": "security-advisories@github.com", "published": "2021-02-02T18:15:11.420", "lastModified": "2021-02-08T14:19:02.443", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "loklak is an open-source server application which is able to collect messages from various sources, including twitter. The server contains a search index and a peer-to-peer index sharing interface. All messages are stored in an elasticsearch index. In loklak less than or equal to commit 5f48476, a path traversal vulnerability exists. Insufficient input validation in the APIs exposed by the loklak server allowed a directory traversal vulnerability. Any admin configuration and files readable by the app available on the hosted file system can be retrieved by the attacker. Furthermore, user-controlled content could be written to any admin config and files readable by the application. This has been patched in commit 50dd692. Users will need to upgrade their hosted instances of loklak to not be vulnerable to this exploit."}, {"lang": "es", "value": "loklak es una aplicaci\u00f3n de servidor de c\u00f3digo abierto que puede recopilar mensajes de varias fuentes, incluyendo Twitter.&#xa0;El servidor contiene un \u00edndice de b\u00fasqueda y una interfaz de intercambio de \u00edndices de igual a igual.&#xa0;Todos los mensajes son almacenados en un \u00edndice elasticsearch.&#xa0;En loklak menor o igual al commit 5f48476, se presenta una vulnerabilidad de salto de ruta.&#xa0;Una comprobaci\u00f3n insuficiente de la entrada en las API expuestas por el servidor de loklak permiti\u00f3 una vulnerabilidad de salto de directorio.&#xa0;Cualquier configuraci\u00f3n del administrador y los archivos legibles por la aplicaci\u00f3n disponibles en el sistema de archivos alojados pueden ser recuperados por el atacante.&#xa0;Adem\u00e1s, el contenido controlado por el usuario podr\u00eda escribirse en cualquier configuraci\u00f3n de administrador y archivos legibles por la aplicaci\u00f3n.&#xa0;Esto ha sido parcheado en el commit 50dd692.&#xa0;Los usuarios deber\u00e1n actualizar sus instancias alojadas de loklak para no ser vulnerables a esta explotaci\u00f3n"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 9.1, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.2}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 9.1, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:P/A:N", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 6.4}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-22"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:loklak_project:loklak:*:*:*:*:*:*:*:*", "versionEndIncluding": "2020-01-22", "matchCriteriaId": "80F8E47F-4BA9-44F2-9E37-1DCF5C87E690"}]}]}], "references": [{"url": "https://github.com/loklak/loklak_server/commit/50dd69230d3cd71dab0bfa7156682ffeca8ed8b9", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/loklak/loklak_server/security/advisories/GHSA-7557-4v29-rqw6", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/loklak/loklak_server/commit/50dd69230d3cd71dab0bfa7156682ffeca8ed8b9"}}