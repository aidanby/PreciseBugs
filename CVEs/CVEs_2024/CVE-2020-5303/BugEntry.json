{"buggy_code": ["package node\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/http\"\n\t_ \"net/http/pprof\" // nolint: gosec // securely exposed on separate, optional port\n\t\"os\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/cors\"\n\n\tamino \"github.com/tendermint/go-amino\"\n\tdbm \"github.com/tendermint/tm-db\"\n\n\tabci \"github.com/tendermint/tendermint/abci/types\"\n\tbcv0 \"github.com/tendermint/tendermint/blockchain/v0\"\n\tbcv1 \"github.com/tendermint/tendermint/blockchain/v1\"\n\tbcv2 \"github.com/tendermint/tendermint/blockchain/v2\"\n\tcfg \"github.com/tendermint/tendermint/config\"\n\t\"github.com/tendermint/tendermint/consensus\"\n\tcs \"github.com/tendermint/tendermint/consensus\"\n\t\"github.com/tendermint/tendermint/crypto\"\n\t\"github.com/tendermint/tendermint/evidence\"\n\t\"github.com/tendermint/tendermint/libs/log\"\n\ttmpubsub \"github.com/tendermint/tendermint/libs/pubsub\"\n\t\"github.com/tendermint/tendermint/libs/service\"\n\tmempl \"github.com/tendermint/tendermint/mempool\"\n\t\"github.com/tendermint/tendermint/p2p\"\n\t\"github.com/tendermint/tendermint/p2p/pex\"\n\t\"github.com/tendermint/tendermint/privval\"\n\t\"github.com/tendermint/tendermint/proxy\"\n\trpccore \"github.com/tendermint/tendermint/rpc/core\"\n\tctypes \"github.com/tendermint/tendermint/rpc/core/types\"\n\tgrpccore \"github.com/tendermint/tendermint/rpc/grpc\"\n\trpcserver \"github.com/tendermint/tendermint/rpc/lib/server\"\n\tsm \"github.com/tendermint/tendermint/state\"\n\t\"github.com/tendermint/tendermint/state/txindex\"\n\t\"github.com/tendermint/tendermint/state/txindex/kv\"\n\t\"github.com/tendermint/tendermint/state/txindex/null\"\n\t\"github.com/tendermint/tendermint/store\"\n\t\"github.com/tendermint/tendermint/types\"\n\ttmtime \"github.com/tendermint/tendermint/types/time\"\n\t\"github.com/tendermint/tendermint/version\"\n)\n\n//------------------------------------------------------------------------------\n\n// DBContext specifies config information for loading a new DB.\ntype DBContext struct {\n\tID     string\n\tConfig *cfg.Config\n}\n\n// DBProvider takes a DBContext and returns an instantiated DB.\ntype DBProvider func(*DBContext) (dbm.DB, error)\n\n// DefaultDBProvider returns a database using the DBBackend and DBDir\n// specified in the ctx.Config.\nfunc DefaultDBProvider(ctx *DBContext) (dbm.DB, error) {\n\tdbType := dbm.BackendType(ctx.Config.DBBackend)\n\treturn dbm.NewDB(ctx.ID, dbType, ctx.Config.DBDir()), nil\n}\n\n// GenesisDocProvider returns a GenesisDoc.\n// It allows the GenesisDoc to be pulled from sources other than the\n// filesystem, for instance from a distributed key-value store cluster.\ntype GenesisDocProvider func() (*types.GenesisDoc, error)\n\n// DefaultGenesisDocProviderFunc returns a GenesisDocProvider that loads\n// the GenesisDoc from the config.GenesisFile() on the filesystem.\nfunc DefaultGenesisDocProviderFunc(config *cfg.Config) GenesisDocProvider {\n\treturn func() (*types.GenesisDoc, error) {\n\t\treturn types.GenesisDocFromFile(config.GenesisFile())\n\t}\n}\n\n// Provider takes a config and a logger and returns a ready to go Node.\ntype Provider func(*cfg.Config, log.Logger) (*Node, error)\n\n// DefaultNewNode returns a Tendermint node with default settings for the\n// PrivValidator, ClientCreator, GenesisDoc, and DBProvider.\n// It implements NodeProvider.\nfunc DefaultNewNode(config *cfg.Config, logger log.Logger) (*Node, error) {\n\t// Generate node PrivKey\n\tnodeKey, err := p2p.LoadOrGenNodeKey(config.NodeKeyFile())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Convert old PrivValidator if it exists.\n\toldPrivVal := config.OldPrivValidatorFile()\n\tnewPrivValKey := config.PrivValidatorKeyFile()\n\tnewPrivValState := config.PrivValidatorStateFile()\n\tif _, err := os.Stat(oldPrivVal); !os.IsNotExist(err) {\n\t\toldPV, err := privval.LoadOldFilePV(oldPrivVal)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error reading OldPrivValidator from %v: %v\", oldPrivVal, err)\n\t\t}\n\t\tlogger.Info(\"Upgrading PrivValidator file\",\n\t\t\t\"old\", oldPrivVal,\n\t\t\t\"newKey\", newPrivValKey,\n\t\t\t\"newState\", newPrivValState,\n\t\t)\n\t\toldPV.Upgrade(newPrivValKey, newPrivValState)\n\t}\n\n\treturn NewNode(config,\n\t\tprivval.LoadOrGenFilePV(newPrivValKey, newPrivValState),\n\t\tnodeKey,\n\t\tproxy.DefaultClientCreator(config.ProxyApp, config.ABCI, config.DBDir()),\n\t\tDefaultGenesisDocProviderFunc(config),\n\t\tDefaultDBProvider,\n\t\tDefaultMetricsProvider(config.Instrumentation),\n\t\tlogger,\n\t)\n}\n\n// MetricsProvider returns a consensus, p2p and mempool Metrics.\ntype MetricsProvider func(chainID string) (*cs.Metrics, *p2p.Metrics, *mempl.Metrics, *sm.Metrics)\n\n// DefaultMetricsProvider returns Metrics build using Prometheus client library\n// if Prometheus is enabled. Otherwise, it returns no-op Metrics.\nfunc DefaultMetricsProvider(config *cfg.InstrumentationConfig) MetricsProvider {\n\treturn func(chainID string) (*cs.Metrics, *p2p.Metrics, *mempl.Metrics, *sm.Metrics) {\n\t\tif config.Prometheus {\n\t\t\treturn cs.PrometheusMetrics(config.Namespace, \"chain_id\", chainID),\n\t\t\t\tp2p.PrometheusMetrics(config.Namespace, \"chain_id\", chainID),\n\t\t\t\tmempl.PrometheusMetrics(config.Namespace, \"chain_id\", chainID),\n\t\t\t\tsm.PrometheusMetrics(config.Namespace, \"chain_id\", chainID)\n\t\t}\n\t\treturn cs.NopMetrics(), p2p.NopMetrics(), mempl.NopMetrics(), sm.NopMetrics()\n\t}\n}\n\n// Option sets a parameter for the node.\ntype Option func(*Node)\n\n// CustomReactors allows you to add custom reactors (name -> p2p.Reactor) to\n// the node's Switch.\n//\n// WARNING: using any name from the below list of the existing reactors will\n// result in replacing it with the custom one.\n//\n//  - MEMPOOL\n//  - BLOCKCHAIN\n//  - CONSENSUS\n//  - EVIDENCE\n//  - PEX\nfunc CustomReactors(reactors map[string]p2p.Reactor) Option {\n\treturn func(n *Node) {\n\t\tfor name, reactor := range reactors {\n\t\t\tif existingReactor := n.sw.Reactor(name); existingReactor != nil {\n\t\t\t\tn.sw.Logger.Info(\"Replacing existing reactor with a custom one\",\n\t\t\t\t\t\"name\", name, \"existing\", existingReactor, \"custom\", reactor)\n\t\t\t\tn.sw.RemoveReactor(name, existingReactor)\n\t\t\t}\n\t\t\tn.sw.AddReactor(name, reactor)\n\t\t}\n\t}\n}\n\n//------------------------------------------------------------------------------\n\n// Node is the highest level interface to a full Tendermint node.\n// It includes all configuration information and running services.\ntype Node struct {\n\tservice.BaseService\n\n\t// config\n\tconfig        *cfg.Config\n\tgenesisDoc    *types.GenesisDoc   // initial validator set\n\tprivValidator types.PrivValidator // local node's validator key\n\n\t// network\n\ttransport   *p2p.MultiplexTransport\n\tsw          *p2p.Switch  // p2p connections\n\taddrBook    pex.AddrBook // known peers\n\tnodeInfo    p2p.NodeInfo\n\tnodeKey     *p2p.NodeKey // our node privkey\n\tisListening bool\n\n\t// services\n\teventBus         *types.EventBus // pub/sub for services\n\tstateDB          dbm.DB\n\tblockStore       *store.BlockStore // store the blockchain to disk\n\tbcReactor        p2p.Reactor       // for fast-syncing\n\tmempoolReactor   *mempl.Reactor    // for gossipping transactions\n\tmempool          mempl.Mempool\n\tconsensusState   *cs.State      // latest consensus state\n\tconsensusReactor *cs.Reactor    // for participating in the consensus\n\tpexReactor       *pex.Reactor   // for exchanging peer addresses\n\tevidencePool     *evidence.Pool // tracking evidence\n\tproxyApp         proxy.AppConns // connection to the application\n\trpcListeners     []net.Listener // rpc servers\n\ttxIndexer        txindex.TxIndexer\n\tindexerService   *txindex.IndexerService\n\tprometheusSrv    *http.Server\n}\n\nfunc initDBs(config *cfg.Config, dbProvider DBProvider) (blockStore *store.BlockStore, stateDB dbm.DB, err error) {\n\tvar blockStoreDB dbm.DB\n\tblockStoreDB, err = dbProvider(&DBContext{\"blockstore\", config})\n\tif err != nil {\n\t\treturn\n\t}\n\tblockStore = store.NewBlockStore(blockStoreDB)\n\n\tstateDB, err = dbProvider(&DBContext{\"state\", config})\n\tif err != nil {\n\t\treturn\n\t}\n\n\treturn\n}\n\nfunc createAndStartProxyAppConns(clientCreator proxy.ClientCreator, logger log.Logger) (proxy.AppConns, error) {\n\tproxyApp := proxy.NewAppConns(clientCreator)\n\tproxyApp.SetLogger(logger.With(\"module\", \"proxy\"))\n\tif err := proxyApp.Start(); err != nil {\n\t\treturn nil, fmt.Errorf(\"error starting proxy app connections: %v\", err)\n\t}\n\treturn proxyApp, nil\n}\n\nfunc createAndStartEventBus(logger log.Logger) (*types.EventBus, error) {\n\teventBus := types.NewEventBus()\n\teventBus.SetLogger(logger.With(\"module\", \"events\"))\n\tif err := eventBus.Start(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn eventBus, nil\n}\n\nfunc createAndStartIndexerService(config *cfg.Config, dbProvider DBProvider,\n\teventBus *types.EventBus, logger log.Logger) (*txindex.IndexerService, txindex.TxIndexer, error) {\n\n\tvar txIndexer txindex.TxIndexer\n\tswitch config.TxIndex.Indexer {\n\tcase \"kv\":\n\t\tstore, err := dbProvider(&DBContext{\"tx_index\", config})\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tswitch {\n\t\tcase config.TxIndex.IndexKeys != \"\":\n\t\t\ttxIndexer = kv.NewTxIndex(store, kv.IndexEvents(splitAndTrimEmpty(config.TxIndex.IndexKeys, \",\", \" \")))\n\t\tcase config.TxIndex.IndexAllKeys:\n\t\t\ttxIndexer = kv.NewTxIndex(store, kv.IndexAllEvents())\n\t\tdefault:\n\t\t\ttxIndexer = kv.NewTxIndex(store)\n\t\t}\n\tdefault:\n\t\ttxIndexer = &null.TxIndex{}\n\t}\n\n\tindexerService := txindex.NewIndexerService(txIndexer, eventBus)\n\tindexerService.SetLogger(logger.With(\"module\", \"txindex\"))\n\tif err := indexerService.Start(); err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn indexerService, txIndexer, nil\n}\n\nfunc doHandshake(\n\tstateDB dbm.DB,\n\tstate sm.State,\n\tblockStore sm.BlockStore,\n\tgenDoc *types.GenesisDoc,\n\teventBus types.BlockEventPublisher,\n\tproxyApp proxy.AppConns,\n\tconsensusLogger log.Logger) error {\n\n\thandshaker := cs.NewHandshaker(stateDB, state, blockStore, genDoc)\n\thandshaker.SetLogger(consensusLogger)\n\thandshaker.SetEventBus(eventBus)\n\tif err := handshaker.Handshake(proxyApp); err != nil {\n\t\treturn fmt.Errorf(\"error during handshake: %v\", err)\n\t}\n\treturn nil\n}\n\nfunc logNodeStartupInfo(state sm.State, pubKey crypto.PubKey, logger, consensusLogger log.Logger) {\n\t// Log the version info.\n\tlogger.Info(\"Version info\",\n\t\t\"software\", version.TMCoreSemVer,\n\t\t\"block\", version.BlockProtocol,\n\t\t\"p2p\", version.P2PProtocol,\n\t)\n\n\t// If the state and software differ in block version, at least log it.\n\tif state.Version.Consensus.Block != version.BlockProtocol {\n\t\tlogger.Info(\"Software and state have different block protocols\",\n\t\t\t\"software\", version.BlockProtocol,\n\t\t\t\"state\", state.Version.Consensus.Block,\n\t\t)\n\t}\n\n\taddr := pubKey.Address()\n\t// Log whether this node is a validator or an observer\n\tif state.Validators.HasAddress(addr) {\n\t\tconsensusLogger.Info(\"This node is a validator\", \"addr\", addr, \"pubKey\", pubKey)\n\t} else {\n\t\tconsensusLogger.Info(\"This node is not a validator\", \"addr\", addr, \"pubKey\", pubKey)\n\t}\n}\n\nfunc onlyValidatorIsUs(state sm.State, pubKey crypto.PubKey) bool {\n\tif state.Validators.Size() > 1 {\n\t\treturn false\n\t}\n\taddr, _ := state.Validators.GetByIndex(0)\n\treturn bytes.Equal(pubKey.Address(), addr)\n}\n\nfunc createMempoolAndMempoolReactor(config *cfg.Config, proxyApp proxy.AppConns,\n\tstate sm.State, memplMetrics *mempl.Metrics, logger log.Logger) (*mempl.Reactor, *mempl.CListMempool) {\n\n\tmempool := mempl.NewCListMempool(\n\t\tconfig.Mempool,\n\t\tproxyApp.Mempool(),\n\t\tstate.LastBlockHeight,\n\t\tmempl.WithMetrics(memplMetrics),\n\t\tmempl.WithPreCheck(sm.TxPreCheck(state)),\n\t\tmempl.WithPostCheck(sm.TxPostCheck(state)),\n\t)\n\tmempoolLogger := logger.With(\"module\", \"mempool\")\n\tmempoolReactor := mempl.NewReactor(config.Mempool, mempool)\n\tmempoolReactor.SetLogger(mempoolLogger)\n\n\tif config.Consensus.WaitForTxs() {\n\t\tmempool.EnableTxsAvailable()\n\t}\n\treturn mempoolReactor, mempool\n}\n\nfunc createEvidenceReactor(config *cfg.Config, dbProvider DBProvider,\n\tstateDB dbm.DB, logger log.Logger) (*evidence.Reactor, *evidence.Pool, error) {\n\n\tevidenceDB, err := dbProvider(&DBContext{\"evidence\", config})\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tevidenceLogger := logger.With(\"module\", \"evidence\")\n\tevidencePool := evidence.NewPool(stateDB, evidenceDB)\n\tevidencePool.SetLogger(evidenceLogger)\n\tevidenceReactor := evidence.NewReactor(evidencePool)\n\tevidenceReactor.SetLogger(evidenceLogger)\n\treturn evidenceReactor, evidencePool, nil\n}\n\nfunc createBlockchainReactor(config *cfg.Config,\n\tstate sm.State,\n\tblockExec *sm.BlockExecutor,\n\tblockStore *store.BlockStore,\n\tfastSync bool,\n\tlogger log.Logger) (bcReactor p2p.Reactor, err error) {\n\n\tswitch config.FastSync.Version {\n\tcase \"v0\":\n\t\tbcReactor = bcv0.NewBlockchainReactor(state.Copy(), blockExec, blockStore, fastSync)\n\tcase \"v1\":\n\t\tbcReactor = bcv1.NewBlockchainReactor(state.Copy(), blockExec, blockStore, fastSync)\n\tcase \"v2\":\n\t\tbcReactor = bcv2.NewBlockchainReactor(state.Copy(), blockExec, blockStore, fastSync)\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown fastsync version %s\", config.FastSync.Version)\n\t}\n\n\tbcReactor.SetLogger(logger.With(\"module\", \"blockchain\"))\n\treturn bcReactor, nil\n}\n\nfunc createConsensusReactor(config *cfg.Config,\n\tstate sm.State,\n\tblockExec *sm.BlockExecutor,\n\tblockStore sm.BlockStore,\n\tmempool *mempl.CListMempool,\n\tevidencePool *evidence.Pool,\n\tprivValidator types.PrivValidator,\n\tcsMetrics *cs.Metrics,\n\tfastSync bool,\n\teventBus *types.EventBus,\n\tconsensusLogger log.Logger) (*consensus.Reactor, *consensus.State) {\n\n\tconsensusState := cs.NewState(\n\t\tconfig.Consensus,\n\t\tstate.Copy(),\n\t\tblockExec,\n\t\tblockStore,\n\t\tmempool,\n\t\tevidencePool,\n\t\tcs.StateMetrics(csMetrics),\n\t)\n\tconsensusState.SetLogger(consensusLogger)\n\tif privValidator != nil {\n\t\tconsensusState.SetPrivValidator(privValidator)\n\t}\n\tconsensusReactor := cs.NewReactor(consensusState, fastSync, cs.ReactorMetrics(csMetrics))\n\tconsensusReactor.SetLogger(consensusLogger)\n\t// services which will be publishing and/or subscribing for messages (events)\n\t// consensusReactor will set it on consensusState and blockExecutor\n\tconsensusReactor.SetEventBus(eventBus)\n\treturn consensusReactor, consensusState\n}\n\nfunc createTransport(\n\tconfig *cfg.Config,\n\tnodeInfo p2p.NodeInfo,\n\tnodeKey *p2p.NodeKey,\n\tproxyApp proxy.AppConns,\n) (\n\t*p2p.MultiplexTransport,\n\t[]p2p.PeerFilterFunc,\n) {\n\tvar (\n\t\tmConnConfig = p2p.MConnConfig(config.P2P)\n\t\ttransport   = p2p.NewMultiplexTransport(nodeInfo, *nodeKey, mConnConfig)\n\t\tconnFilters = []p2p.ConnFilterFunc{}\n\t\tpeerFilters = []p2p.PeerFilterFunc{}\n\t)\n\n\tif !config.P2P.AllowDuplicateIP {\n\t\tconnFilters = append(connFilters, p2p.ConnDuplicateIPFilter())\n\t}\n\n\t// Filter peers by addr or pubkey with an ABCI query.\n\t// If the query return code is OK, add peer.\n\tif config.FilterPeers {\n\t\tconnFilters = append(\n\t\t\tconnFilters,\n\t\t\t// ABCI query for address filtering.\n\t\t\tfunc(_ p2p.ConnSet, c net.Conn, _ []net.IP) error {\n\t\t\t\tres, err := proxyApp.Query().QuerySync(abci.RequestQuery{\n\t\t\t\t\tPath: fmt.Sprintf(\"/p2p/filter/addr/%s\", c.RemoteAddr().String()),\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif res.IsErr() {\n\t\t\t\t\treturn fmt.Errorf(\"error querying abci app: %v\", res)\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t},\n\t\t)\n\n\t\tpeerFilters = append(\n\t\t\tpeerFilters,\n\t\t\t// ABCI query for ID filtering.\n\t\t\tfunc(_ p2p.IPeerSet, p p2p.Peer) error {\n\t\t\t\tres, err := proxyApp.Query().QuerySync(abci.RequestQuery{\n\t\t\t\t\tPath: fmt.Sprintf(\"/p2p/filter/id/%s\", p.ID()),\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif res.IsErr() {\n\t\t\t\t\treturn fmt.Errorf(\"error querying abci app: %v\", res)\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t},\n\t\t)\n\t}\n\n\tp2p.MultiplexTransportConnFilters(connFilters...)(transport)\n\treturn transport, peerFilters\n}\n\nfunc createSwitch(config *cfg.Config,\n\ttransport p2p.Transport,\n\tp2pMetrics *p2p.Metrics,\n\tpeerFilters []p2p.PeerFilterFunc,\n\tmempoolReactor *mempl.Reactor,\n\tbcReactor p2p.Reactor,\n\tconsensusReactor *consensus.Reactor,\n\tevidenceReactor *evidence.Reactor,\n\tnodeInfo p2p.NodeInfo,\n\tnodeKey *p2p.NodeKey,\n\tp2pLogger log.Logger) *p2p.Switch {\n\n\tsw := p2p.NewSwitch(\n\t\tconfig.P2P,\n\t\ttransport,\n\t\tp2p.WithMetrics(p2pMetrics),\n\t\tp2p.SwitchPeerFilters(peerFilters...),\n\t)\n\tsw.SetLogger(p2pLogger)\n\tsw.AddReactor(\"MEMPOOL\", mempoolReactor)\n\tsw.AddReactor(\"BLOCKCHAIN\", bcReactor)\n\tsw.AddReactor(\"CONSENSUS\", consensusReactor)\n\tsw.AddReactor(\"EVIDENCE\", evidenceReactor)\n\n\tsw.SetNodeInfo(nodeInfo)\n\tsw.SetNodeKey(nodeKey)\n\n\tp2pLogger.Info(\"P2P Node ID\", \"ID\", nodeKey.ID(), \"file\", config.NodeKeyFile())\n\treturn sw\n}\n\nfunc createAddrBookAndSetOnSwitch(config *cfg.Config, sw *p2p.Switch,\n\tp2pLogger log.Logger, nodeKey *p2p.NodeKey) (pex.AddrBook, error) {\n\n\taddrBook := pex.NewAddrBook(config.P2P.AddrBookFile(), config.P2P.AddrBookStrict)\n\taddrBook.SetLogger(p2pLogger.With(\"book\", config.P2P.AddrBookFile()))\n\n\t// Add ourselves to addrbook to prevent dialing ourselves\n\tif config.P2P.ExternalAddress != \"\" {\n\t\taddr, err := p2p.NewNetAddressString(p2p.IDAddressString(nodeKey.ID(), config.P2P.ExternalAddress))\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"p2p.external_address is incorrect\")\n\t\t}\n\t\taddrBook.AddOurAddress(addr)\n\t}\n\tif config.P2P.ListenAddress != \"\" {\n\t\taddr, err := p2p.NewNetAddressString(p2p.IDAddressString(nodeKey.ID(), config.P2P.ListenAddress))\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"p2p.laddr is incorrect\")\n\t\t}\n\t\taddrBook.AddOurAddress(addr)\n\t}\n\n\tsw.SetAddrBook(addrBook)\n\n\treturn addrBook, nil\n}\n\nfunc createPEXReactorAndAddToSwitch(addrBook pex.AddrBook, config *cfg.Config,\n\tsw *p2p.Switch, logger log.Logger) *pex.Reactor {\n\n\t// TODO persistent peers ? so we can have their DNS addrs saved\n\tpexReactor := pex.NewReactor(addrBook,\n\t\t&pex.ReactorConfig{\n\t\t\tSeeds:    splitAndTrimEmpty(config.P2P.Seeds, \",\", \" \"),\n\t\t\tSeedMode: config.P2P.SeedMode,\n\t\t\t// See consensus/reactor.go: blocksToContributeToBecomeGoodPeer 10000\n\t\t\t// blocks assuming 10s blocks ~ 28 hours.\n\t\t\t// TODO (melekes): make it dynamic based on the actual block latencies\n\t\t\t// from the live network.\n\t\t\t// https://github.com/tendermint/tendermint/issues/3523\n\t\t\tSeedDisconnectWaitPeriod:     28 * time.Hour,\n\t\t\tPersistentPeersMaxDialPeriod: config.P2P.PersistentPeersMaxDialPeriod,\n\t\t})\n\tpexReactor.SetLogger(logger.With(\"module\", \"pex\"))\n\tsw.AddReactor(\"PEX\", pexReactor)\n\treturn pexReactor\n}\n\n// NewNode returns a new, ready to go, Tendermint Node.\nfunc NewNode(config *cfg.Config,\n\tprivValidator types.PrivValidator,\n\tnodeKey *p2p.NodeKey,\n\tclientCreator proxy.ClientCreator,\n\tgenesisDocProvider GenesisDocProvider,\n\tdbProvider DBProvider,\n\tmetricsProvider MetricsProvider,\n\tlogger log.Logger,\n\toptions ...Option) (*Node, error) {\n\n\tblockStore, stateDB, err := initDBs(config, dbProvider)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tstate, genDoc, err := LoadStateFromDBOrGenesisDocProvider(stateDB, genesisDocProvider)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the proxyApp and establish connections to the ABCI app (consensus, mempool, query).\n\tproxyApp, err := createAndStartProxyAppConns(clientCreator, logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// EventBus and IndexerService must be started before the handshake because\n\t// we might need to index the txs of the replayed block as this might not have happened\n\t// when the node stopped last time (i.e. the node stopped after it saved the block\n\t// but before it indexed the txs, or, endblocker panicked)\n\teventBus, err := createAndStartEventBus(logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Transaction indexing\n\tindexerService, txIndexer, err := createAndStartIndexerService(config, dbProvider, eventBus, logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the handshaker, which calls RequestInfo, sets the AppVersion on the state,\n\t// and replays any blocks as necessary to sync tendermint with the app.\n\tconsensusLogger := logger.With(\"module\", \"consensus\")\n\tif err := doHandshake(stateDB, state, blockStore, genDoc, eventBus, proxyApp, consensusLogger); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Reload the state. It will have the Version.Consensus.App set by the\n\t// Handshake, and may have other modifications as well (ie. depending on\n\t// what happened during block replay).\n\tstate = sm.LoadState(stateDB)\n\n\t// If an address is provided, listen on the socket for a connection from an\n\t// external signing process.\n\tif config.PrivValidatorListenAddr != \"\" {\n\t\t// FIXME: we should start services inside OnStart\n\t\tprivValidator, err = createAndStartPrivValidatorSocketClient(config.PrivValidatorListenAddr, logger)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"error with private validator socket client\")\n\t\t}\n\t}\n\n\tpubKey, err := privValidator.GetPubKey()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"can't get pubkey\")\n\t}\n\n\tlogNodeStartupInfo(state, pubKey, logger, consensusLogger)\n\n\t// Decide whether to fast-sync or not\n\t// We don't fast-sync when the only validator is us.\n\tfastSync := config.FastSyncMode && !onlyValidatorIsUs(state, pubKey)\n\n\tcsMetrics, p2pMetrics, memplMetrics, smMetrics := metricsProvider(genDoc.ChainID)\n\n\t// Make MempoolReactor\n\tmempoolReactor, mempool := createMempoolAndMempoolReactor(config, proxyApp, state, memplMetrics, logger)\n\n\t// Make Evidence Reactor\n\tevidenceReactor, evidencePool, err := createEvidenceReactor(config, dbProvider, stateDB, logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// make block executor for consensus and blockchain reactors to execute blocks\n\tblockExec := sm.NewBlockExecutor(\n\t\tstateDB,\n\t\tlogger.With(\"module\", \"state\"),\n\t\tproxyApp.Consensus(),\n\t\tmempool,\n\t\tevidencePool,\n\t\tsm.BlockExecutorWithMetrics(smMetrics),\n\t)\n\n\t// Make BlockchainReactor\n\tbcReactor, err := createBlockchainReactor(config, state, blockExec, blockStore, fastSync, logger)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"could not create blockchain reactor\")\n\t}\n\n\t// Make ConsensusReactor\n\tconsensusReactor, consensusState := createConsensusReactor(\n\t\tconfig, state, blockExec, blockStore, mempool, evidencePool,\n\t\tprivValidator, csMetrics, fastSync, eventBus, consensusLogger,\n\t)\n\n\tnodeInfo, err := makeNodeInfo(config, nodeKey, txIndexer, genDoc, state)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Setup Transport.\n\ttransport, peerFilters := createTransport(config, nodeInfo, nodeKey, proxyApp)\n\n\t// Setup Switch.\n\tp2pLogger := logger.With(\"module\", \"p2p\")\n\tsw := createSwitch(\n\t\tconfig, transport, p2pMetrics, peerFilters, mempoolReactor, bcReactor,\n\t\tconsensusReactor, evidenceReactor, nodeInfo, nodeKey, p2pLogger,\n\t)\n\n\terr = sw.AddPersistentPeers(splitAndTrimEmpty(config.P2P.PersistentPeers, \",\", \" \"))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"could not add peers from persistent_peers field\")\n\t}\n\n\terr = sw.AddUnconditionalPeerIDs(splitAndTrimEmpty(config.P2P.UnconditionalPeerIDs, \",\", \" \"))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"could not add peer ids from unconditional_peer_ids field\")\n\t}\n\n\taddrBook, err := createAddrBookAndSetOnSwitch(config, sw, p2pLogger, nodeKey)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"could not create addrbook\")\n\t}\n\n\t// Optionally, start the pex reactor\n\t//\n\t// TODO:\n\t//\n\t// We need to set Seeds and PersistentPeers on the switch,\n\t// since it needs to be able to use these (and their DNS names)\n\t// even if the PEX is off. We can include the DNS name in the NetAddress,\n\t// but it would still be nice to have a clear list of the current \"PersistentPeers\"\n\t// somewhere that we can return with net_info.\n\t//\n\t// If PEX is on, it should handle dialing the seeds. Otherwise the switch does it.\n\t// Note we currently use the addrBook regardless at least for AddOurAddress\n\tvar pexReactor *pex.Reactor\n\tif config.P2P.PexReactor {\n\t\tpexReactor = createPEXReactorAndAddToSwitch(addrBook, config, sw, logger)\n\t}\n\n\tif config.ProfListenAddress != \"\" {\n\t\tgo func() {\n\t\t\tlogger.Error(\"Profile server\", \"err\", http.ListenAndServe(config.ProfListenAddress, nil))\n\t\t}()\n\t}\n\n\tnode := &Node{\n\t\tconfig:        config,\n\t\tgenesisDoc:    genDoc,\n\t\tprivValidator: privValidator,\n\n\t\ttransport: transport,\n\t\tsw:        sw,\n\t\taddrBook:  addrBook,\n\t\tnodeInfo:  nodeInfo,\n\t\tnodeKey:   nodeKey,\n\n\t\tstateDB:          stateDB,\n\t\tblockStore:       blockStore,\n\t\tbcReactor:        bcReactor,\n\t\tmempoolReactor:   mempoolReactor,\n\t\tmempool:          mempool,\n\t\tconsensusState:   consensusState,\n\t\tconsensusReactor: consensusReactor,\n\t\tpexReactor:       pexReactor,\n\t\tevidencePool:     evidencePool,\n\t\tproxyApp:         proxyApp,\n\t\ttxIndexer:        txIndexer,\n\t\tindexerService:   indexerService,\n\t\teventBus:         eventBus,\n\t}\n\tnode.BaseService = *service.NewBaseService(logger, \"Node\", node)\n\n\tfor _, option := range options {\n\t\toption(node)\n\t}\n\n\treturn node, nil\n}\n\n// OnStart starts the Node. It implements service.Service.\nfunc (n *Node) OnStart() error {\n\tnow := tmtime.Now()\n\tgenTime := n.genesisDoc.GenesisTime\n\tif genTime.After(now) {\n\t\tn.Logger.Info(\"Genesis time is in the future. Sleeping until then...\", \"genTime\", genTime)\n\t\ttime.Sleep(genTime.Sub(now))\n\t}\n\n\t// Add private IDs to addrbook to block those peers being added\n\tn.addrBook.AddPrivateIDs(splitAndTrimEmpty(n.config.P2P.PrivatePeerIDs, \",\", \" \"))\n\n\t// Start the RPC server before the P2P server\n\t// so we can eg. receive txs for the first block\n\tif n.config.RPC.ListenAddress != \"\" {\n\t\tlisteners, err := n.startRPC()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tn.rpcListeners = listeners\n\t}\n\n\tif n.config.Instrumentation.Prometheus &&\n\t\tn.config.Instrumentation.PrometheusListenAddr != \"\" {\n\t\tn.prometheusSrv = n.startPrometheusServer(n.config.Instrumentation.PrometheusListenAddr)\n\t}\n\n\t// Start the transport.\n\taddr, err := p2p.NewNetAddressString(p2p.IDAddressString(n.nodeKey.ID(), n.config.P2P.ListenAddress))\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := n.transport.Listen(*addr); err != nil {\n\t\treturn err\n\t}\n\n\tn.isListening = true\n\n\tif n.config.Mempool.WalEnabled() {\n\t\tn.mempool.InitWAL() // no need to have the mempool wal during tests\n\t}\n\n\t// Start the switch (the P2P server).\n\terr = n.sw.Start()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Always connect to persistent peers\n\terr = n.sw.DialPeersAsync(splitAndTrimEmpty(n.config.P2P.PersistentPeers, \",\", \" \"))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"could not dial peers from persistent_peers field\")\n\t}\n\n\treturn nil\n}\n\n// OnStop stops the Node. It implements service.Service.\nfunc (n *Node) OnStop() {\n\tn.BaseService.OnStop()\n\n\tn.Logger.Info(\"Stopping Node\")\n\n\t// first stop the non-reactor services\n\tn.eventBus.Stop()\n\tn.indexerService.Stop()\n\n\t// now stop the reactors\n\tn.sw.Stop()\n\n\t// stop mempool WAL\n\tif n.config.Mempool.WalEnabled() {\n\t\tn.mempool.CloseWAL()\n\t}\n\n\tif err := n.transport.Close(); err != nil {\n\t\tn.Logger.Error(\"Error closing transport\", \"err\", err)\n\t}\n\n\tn.isListening = false\n\n\t// finally stop the listeners / external services\n\tfor _, l := range n.rpcListeners {\n\t\tn.Logger.Info(\"Closing rpc listener\", \"listener\", l)\n\t\tif err := l.Close(); err != nil {\n\t\t\tn.Logger.Error(\"Error closing listener\", \"listener\", l, \"err\", err)\n\t\t}\n\t}\n\n\tif pvsc, ok := n.privValidator.(service.Service); ok {\n\t\tpvsc.Stop()\n\t}\n\n\tif n.prometheusSrv != nil {\n\t\tif err := n.prometheusSrv.Shutdown(context.Background()); err != nil {\n\t\t\t// Error from closing listeners, or context timeout:\n\t\t\tn.Logger.Error(\"Prometheus HTTP server Shutdown\", \"err\", err)\n\t\t}\n\t}\n}\n\n// ConfigureRPC sets all variables in rpccore so they will serve\n// rpc calls from this node\nfunc (n *Node) ConfigureRPC() {\n\trpccore.SetStateDB(n.stateDB)\n\trpccore.SetBlockStore(n.blockStore)\n\trpccore.SetConsensusState(n.consensusState)\n\trpccore.SetMempool(n.mempool)\n\trpccore.SetEvidencePool(n.evidencePool)\n\trpccore.SetP2PPeers(n.sw)\n\trpccore.SetP2PTransport(n)\n\tpubKey, err := n.privValidator.GetPubKey()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\trpccore.SetPubKey(pubKey)\n\trpccore.SetGenesisDoc(n.genesisDoc)\n\trpccore.SetProxyAppQuery(n.proxyApp.Query())\n\trpccore.SetTxIndexer(n.txIndexer)\n\trpccore.SetConsensusReactor(n.consensusReactor)\n\trpccore.SetEventBus(n.eventBus)\n\trpccore.SetLogger(n.Logger.With(\"module\", \"rpc\"))\n\trpccore.SetConfig(*n.config.RPC)\n}\n\nfunc (n *Node) startRPC() ([]net.Listener, error) {\n\tn.ConfigureRPC()\n\tlistenAddrs := splitAndTrimEmpty(n.config.RPC.ListenAddress, \",\", \" \")\n\tcoreCodec := amino.NewCodec()\n\tctypes.RegisterAmino(coreCodec)\n\n\tif n.config.RPC.Unsafe {\n\t\trpccore.AddUnsafeRoutes()\n\t}\n\n\tconfig := rpcserver.DefaultConfig()\n\tconfig.MaxBodyBytes = n.config.RPC.MaxBodyBytes\n\tconfig.MaxHeaderBytes = n.config.RPC.MaxHeaderBytes\n\tconfig.MaxOpenConnections = n.config.RPC.MaxOpenConnections\n\t// If necessary adjust global WriteTimeout to ensure it's greater than\n\t// TimeoutBroadcastTxCommit.\n\t// See https://github.com/tendermint/tendermint/issues/3435\n\tif config.WriteTimeout <= n.config.RPC.TimeoutBroadcastTxCommit {\n\t\tconfig.WriteTimeout = n.config.RPC.TimeoutBroadcastTxCommit + 1*time.Second\n\t}\n\n\t// we may expose the rpc over both a unix and tcp socket\n\tlisteners := make([]net.Listener, len(listenAddrs))\n\tfor i, listenAddr := range listenAddrs {\n\t\tmux := http.NewServeMux()\n\t\trpcLogger := n.Logger.With(\"module\", \"rpc-server\")\n\t\twmLogger := rpcLogger.With(\"protocol\", \"websocket\")\n\t\twm := rpcserver.NewWebsocketManager(rpccore.Routes, coreCodec,\n\t\t\trpcserver.OnDisconnect(func(remoteAddr string) {\n\t\t\t\terr := n.eventBus.UnsubscribeAll(context.Background(), remoteAddr)\n\t\t\t\tif err != nil && err != tmpubsub.ErrSubscriptionNotFound {\n\t\t\t\t\twmLogger.Error(\"Failed to unsubscribe addr from events\", \"addr\", remoteAddr, \"err\", err)\n\t\t\t\t}\n\t\t\t}),\n\t\t\trpcserver.ReadLimit(config.MaxBodyBytes),\n\t\t)\n\t\twm.SetLogger(wmLogger)\n\t\tmux.HandleFunc(\"/websocket\", wm.WebsocketHandler)\n\t\trpcserver.RegisterRPCFuncs(mux, rpccore.Routes, coreCodec, rpcLogger)\n\t\tlistener, err := rpcserver.Listen(\n\t\t\tlistenAddr,\n\t\t\tconfig,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tvar rootHandler http.Handler = mux\n\t\tif n.config.RPC.IsCorsEnabled() {\n\t\t\tcorsMiddleware := cors.New(cors.Options{\n\t\t\t\tAllowedOrigins: n.config.RPC.CORSAllowedOrigins,\n\t\t\t\tAllowedMethods: n.config.RPC.CORSAllowedMethods,\n\t\t\t\tAllowedHeaders: n.config.RPC.CORSAllowedHeaders,\n\t\t\t})\n\t\t\trootHandler = corsMiddleware.Handler(mux)\n\t\t}\n\t\tif n.config.RPC.IsTLSEnabled() {\n\t\t\tgo rpcserver.StartHTTPAndTLSServer(\n\t\t\t\tlistener,\n\t\t\t\trootHandler,\n\t\t\t\tn.config.RPC.CertFile(),\n\t\t\t\tn.config.RPC.KeyFile(),\n\t\t\t\trpcLogger,\n\t\t\t\tconfig,\n\t\t\t)\n\t\t} else {\n\t\t\tgo rpcserver.StartHTTPServer(\n\t\t\t\tlistener,\n\t\t\t\trootHandler,\n\t\t\t\trpcLogger,\n\t\t\t\tconfig,\n\t\t\t)\n\t\t}\n\n\t\tlisteners[i] = listener\n\t}\n\n\t// we expose a simplified api over grpc for convenience to app devs\n\tgrpcListenAddr := n.config.RPC.GRPCListenAddress\n\tif grpcListenAddr != \"\" {\n\t\tconfig := rpcserver.DefaultConfig()\n\t\tconfig.MaxBodyBytes = n.config.RPC.MaxBodyBytes\n\t\tconfig.MaxHeaderBytes = n.config.RPC.MaxHeaderBytes\n\t\t// NOTE: GRPCMaxOpenConnections is used, not MaxOpenConnections\n\t\tconfig.MaxOpenConnections = n.config.RPC.GRPCMaxOpenConnections\n\t\t// If necessary adjust global WriteTimeout to ensure it's greater than\n\t\t// TimeoutBroadcastTxCommit.\n\t\t// See https://github.com/tendermint/tendermint/issues/3435\n\t\tif config.WriteTimeout <= n.config.RPC.TimeoutBroadcastTxCommit {\n\t\t\tconfig.WriteTimeout = n.config.RPC.TimeoutBroadcastTxCommit + 1*time.Second\n\t\t}\n\t\tlistener, err := rpcserver.Listen(grpcListenAddr, config)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tgo grpccore.StartGRPCServer(listener)\n\t\tlisteners = append(listeners, listener)\n\t}\n\n\treturn listeners, nil\n}\n\n// startPrometheusServer starts a Prometheus HTTP server, listening for metrics\n// collectors on addr.\nfunc (n *Node) startPrometheusServer(addr string) *http.Server {\n\tsrv := &http.Server{\n\t\tAddr: addr,\n\t\tHandler: promhttp.InstrumentMetricHandler(\n\t\t\tprometheus.DefaultRegisterer, promhttp.HandlerFor(\n\t\t\t\tprometheus.DefaultGatherer,\n\t\t\t\tpromhttp.HandlerOpts{MaxRequestsInFlight: n.config.Instrumentation.MaxOpenConnections},\n\t\t\t),\n\t\t),\n\t}\n\tgo func() {\n\t\tif err := srv.ListenAndServe(); err != http.ErrServerClosed {\n\t\t\t// Error starting or closing listener:\n\t\t\tn.Logger.Error(\"Prometheus HTTP server ListenAndServe\", \"err\", err)\n\t\t}\n\t}()\n\treturn srv\n}\n\n// Switch returns the Node's Switch.\nfunc (n *Node) Switch() *p2p.Switch {\n\treturn n.sw\n}\n\n// BlockStore returns the Node's BlockStore.\nfunc (n *Node) BlockStore() *store.BlockStore {\n\treturn n.blockStore\n}\n\n// ConsensusState returns the Node's ConsensusState.\nfunc (n *Node) ConsensusState() *cs.State {\n\treturn n.consensusState\n}\n\n// ConsensusReactor returns the Node's ConsensusReactor.\nfunc (n *Node) ConsensusReactor() *cs.Reactor {\n\treturn n.consensusReactor\n}\n\n// MempoolReactor returns the Node's mempool reactor.\nfunc (n *Node) MempoolReactor() *mempl.Reactor {\n\treturn n.mempoolReactor\n}\n\n// Mempool returns the Node's mempool.\nfunc (n *Node) Mempool() mempl.Mempool {\n\treturn n.mempool\n}\n\n// PEXReactor returns the Node's PEXReactor. It returns nil if PEX is disabled.\nfunc (n *Node) PEXReactor() *pex.Reactor {\n\treturn n.pexReactor\n}\n\n// EvidencePool returns the Node's EvidencePool.\nfunc (n *Node) EvidencePool() *evidence.Pool {\n\treturn n.evidencePool\n}\n\n// EventBus returns the Node's EventBus.\nfunc (n *Node) EventBus() *types.EventBus {\n\treturn n.eventBus\n}\n\n// PrivValidator returns the Node's PrivValidator.\n// XXX: for convenience only!\nfunc (n *Node) PrivValidator() types.PrivValidator {\n\treturn n.privValidator\n}\n\n// GenesisDoc returns the Node's GenesisDoc.\nfunc (n *Node) GenesisDoc() *types.GenesisDoc {\n\treturn n.genesisDoc\n}\n\n// ProxyApp returns the Node's AppConns, representing its connections to the ABCI application.\nfunc (n *Node) ProxyApp() proxy.AppConns {\n\treturn n.proxyApp\n}\n\n// Config returns the Node's config.\nfunc (n *Node) Config() *cfg.Config {\n\treturn n.config\n}\n\n//------------------------------------------------------------------------------\n\nfunc (n *Node) Listeners() []string {\n\treturn []string{\n\t\tfmt.Sprintf(\"Listener(@%v)\", n.config.P2P.ExternalAddress),\n\t}\n}\n\nfunc (n *Node) IsListening() bool {\n\treturn n.isListening\n}\n\n// NodeInfo returns the Node's Info from the Switch.\nfunc (n *Node) NodeInfo() p2p.NodeInfo {\n\treturn n.nodeInfo\n}\n\nfunc makeNodeInfo(\n\tconfig *cfg.Config,\n\tnodeKey *p2p.NodeKey,\n\ttxIndexer txindex.TxIndexer,\n\tgenDoc *types.GenesisDoc,\n\tstate sm.State,\n) (p2p.NodeInfo, error) {\n\ttxIndexerStatus := \"on\"\n\tif _, ok := txIndexer.(*null.TxIndex); ok {\n\t\ttxIndexerStatus = \"off\"\n\t}\n\n\tvar bcChannel byte\n\tswitch config.FastSync.Version {\n\tcase \"v0\":\n\t\tbcChannel = bcv0.BlockchainChannel\n\tcase \"v1\":\n\t\tbcChannel = bcv1.BlockchainChannel\n\tcase \"v2\":\n\t\tbcChannel = bcv2.BlockchainChannel\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown fastsync version %s\", config.FastSync.Version)\n\t}\n\n\tnodeInfo := p2p.DefaultNodeInfo{\n\t\tProtocolVersion: p2p.NewProtocolVersion(\n\t\t\tversion.P2PProtocol, // global\n\t\t\tstate.Version.Consensus.Block,\n\t\t\tstate.Version.Consensus.App,\n\t\t),\n\t\tDefaultNodeID: nodeKey.ID(),\n\t\tNetwork:       genDoc.ChainID,\n\t\tVersion:       version.TMCoreSemVer,\n\t\tChannels: []byte{\n\t\t\tbcChannel,\n\t\t\tcs.StateChannel, cs.DataChannel, cs.VoteChannel, cs.VoteSetBitsChannel,\n\t\t\tmempl.MempoolChannel,\n\t\t\tevidence.EvidenceChannel,\n\t\t},\n\t\tMoniker: config.Moniker,\n\t\tOther: p2p.DefaultNodeInfoOther{\n\t\t\tTxIndex:    txIndexerStatus,\n\t\t\tRPCAddress: config.RPC.ListenAddress,\n\t\t},\n\t}\n\n\tif config.P2P.PexReactor {\n\t\tnodeInfo.Channels = append(nodeInfo.Channels, pex.PexChannel)\n\t}\n\n\tlAddr := config.P2P.ExternalAddress\n\n\tif lAddr == \"\" {\n\t\tlAddr = config.P2P.ListenAddress\n\t}\n\n\tnodeInfo.ListenAddr = lAddr\n\n\terr := nodeInfo.Validate()\n\treturn nodeInfo, err\n}\n\n//------------------------------------------------------------------------------\n\nvar (\n\tgenesisDocKey = []byte(\"genesisDoc\")\n)\n\n// LoadStateFromDBOrGenesisDocProvider attempts to load the state from the\n// database, or creates one using the given genesisDocProvider and persists the\n// result to the database. On success this also returns the genesis doc loaded\n// through the given provider.\nfunc LoadStateFromDBOrGenesisDocProvider(\n\tstateDB dbm.DB,\n\tgenesisDocProvider GenesisDocProvider,\n) (sm.State, *types.GenesisDoc, error) {\n\t// Get genesis doc\n\tgenDoc, err := loadGenesisDoc(stateDB)\n\tif err != nil {\n\t\tgenDoc, err = genesisDocProvider()\n\t\tif err != nil {\n\t\t\treturn sm.State{}, nil, err\n\t\t}\n\t\t// save genesis doc to prevent a certain class of user errors (e.g. when it\n\t\t// was changed, accidentally or not). Also good for audit trail.\n\t\tsaveGenesisDoc(stateDB, genDoc)\n\t}\n\tstate, err := sm.LoadStateFromDBOrGenesisDoc(stateDB, genDoc)\n\tif err != nil {\n\t\treturn sm.State{}, nil, err\n\t}\n\treturn state, genDoc, nil\n}\n\n// panics if failed to unmarshal bytes\nfunc loadGenesisDoc(db dbm.DB) (*types.GenesisDoc, error) {\n\tb, err := db.Get(genesisDocKey)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tif len(b) == 0 {\n\t\treturn nil, errors.New(\"genesis doc not found\")\n\t}\n\tvar genDoc *types.GenesisDoc\n\terr = cdc.UnmarshalJSON(b, &genDoc)\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"Failed to load genesis doc due to unmarshaling error: %v (bytes: %X)\", err, b))\n\t}\n\treturn genDoc, nil\n}\n\n// panics if failed to marshal the given genesis document\nfunc saveGenesisDoc(db dbm.DB, genDoc *types.GenesisDoc) {\n\tb, err := cdc.MarshalJSON(genDoc)\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"Failed to save genesis doc due to marshaling error: %v\", err))\n\t}\n\tdb.SetSync(genesisDocKey, b)\n}\n\nfunc createAndStartPrivValidatorSocketClient(\n\tlistenAddr string,\n\tlogger log.Logger,\n) (types.PrivValidator, error) {\n\tpve, err := privval.NewSignerListener(listenAddr, logger)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to start private validator\")\n\t}\n\n\tpvsc, err := privval.NewSignerClient(pve)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to start private validator\")\n\t}\n\n\treturn pvsc, nil\n}\n\n// splitAndTrimEmpty slices s into all subslices separated by sep and returns a\n// slice of the string s with all leading and trailing Unicode code points\n// contained in cutset removed. If sep is empty, SplitAndTrim splits after each\n// UTF-8 sequence. First part is equivalent to strings.SplitN with a count of\n// -1.  also filter out empty strings, only return non-empty strings.\nfunc splitAndTrimEmpty(s, sep, cutset string) []string {\n\tif s == \"\" {\n\t\treturn []string{}\n\t}\n\n\tspl := strings.Split(s, sep)\n\tnonEmptyStrings := make([]string, 0, len(spl))\n\tfor i := 0; i < len(spl); i++ {\n\t\telement := strings.Trim(spl[i], cutset)\n\t\tif element != \"\" {\n\t\t\tnonEmptyStrings = append(nonEmptyStrings, element)\n\t\t}\n\t}\n\treturn nonEmptyStrings\n}\n", "package p2p\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\n\t\"github.com/tendermint/tendermint/crypto\"\n\t\"github.com/tendermint/tendermint/p2p/conn\"\n)\n\nconst (\n\tdefaultDialTimeout      = time.Second\n\tdefaultFilterTimeout    = 5 * time.Second\n\tdefaultHandshakeTimeout = 3 * time.Second\n)\n\n// IPResolver is a behaviour subset of net.Resolver.\ntype IPResolver interface {\n\tLookupIPAddr(context.Context, string) ([]net.IPAddr, error)\n}\n\n// accept is the container to carry the upgraded connection and NodeInfo from an\n// asynchronously running routine to the Accept method.\ntype accept struct {\n\tnetAddr  *NetAddress\n\tconn     net.Conn\n\tnodeInfo NodeInfo\n\terr      error\n}\n\n// peerConfig is used to bundle data we need to fully setup a Peer with an\n// MConn, provided by the caller of Accept and Dial (currently the Switch). This\n// a temporary measure until reactor setup is less dynamic and we introduce the\n// concept of PeerBehaviour to communicate about significant Peer lifecycle\n// events.\n// TODO(xla): Refactor out with more static Reactor setup and PeerBehaviour.\ntype peerConfig struct {\n\tchDescs     []*conn.ChannelDescriptor\n\tonPeerError func(Peer, interface{})\n\toutbound    bool\n\t// isPersistent allows you to set a function, which, given socket address\n\t// (for outbound peers) OR self-reported address (for inbound peers), tells\n\t// if the peer is persistent or not.\n\tisPersistent func(*NetAddress) bool\n\treactorsByCh map[byte]Reactor\n\tmetrics      *Metrics\n}\n\n// Transport emits and connects to Peers. The implementation of Peer is left to\n// the transport. Each transport is also responsible to filter establishing\n// peers specific to its domain.\ntype Transport interface {\n\t// Listening address.\n\tNetAddress() NetAddress\n\n\t// Accept returns a newly connected Peer.\n\tAccept(peerConfig) (Peer, error)\n\n\t// Dial connects to the Peer for the address.\n\tDial(NetAddress, peerConfig) (Peer, error)\n\n\t// Cleanup any resources associated with Peer.\n\tCleanup(Peer)\n}\n\n// transportLifecycle bundles the methods for callers to control start and stop\n// behaviour.\ntype transportLifecycle interface {\n\tClose() error\n\tListen(NetAddress) error\n}\n\n// ConnFilterFunc to be implemented by filter hooks after a new connection has\n// been established. The set of exisiting connections is passed along together\n// with all resolved IPs for the new connection.\ntype ConnFilterFunc func(ConnSet, net.Conn, []net.IP) error\n\n// ConnDuplicateIPFilter resolves and keeps all ips for an incoming connection\n// and refuses new ones if they come from a known ip.\nfunc ConnDuplicateIPFilter() ConnFilterFunc {\n\treturn func(cs ConnSet, c net.Conn, ips []net.IP) error {\n\t\tfor _, ip := range ips {\n\t\t\tif cs.HasIP(ip) {\n\t\t\t\treturn ErrRejected{\n\t\t\t\t\tconn:        c,\n\t\t\t\t\terr:         fmt.Errorf(\"ip<%v> already connected\", ip),\n\t\t\t\t\tisDuplicate: true,\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}\n}\n\n// MultiplexTransportOption sets an optional parameter on the\n// MultiplexTransport.\ntype MultiplexTransportOption func(*MultiplexTransport)\n\n// MultiplexTransportConnFilters sets the filters for rejection new connections.\nfunc MultiplexTransportConnFilters(\n\tfilters ...ConnFilterFunc,\n) MultiplexTransportOption {\n\treturn func(mt *MultiplexTransport) { mt.connFilters = filters }\n}\n\n// MultiplexTransportFilterTimeout sets the timeout waited for filter calls to\n// return.\nfunc MultiplexTransportFilterTimeout(\n\ttimeout time.Duration,\n) MultiplexTransportOption {\n\treturn func(mt *MultiplexTransport) { mt.filterTimeout = timeout }\n}\n\n// MultiplexTransportResolver sets the Resolver used for ip lokkups, defaults to\n// net.DefaultResolver.\nfunc MultiplexTransportResolver(resolver IPResolver) MultiplexTransportOption {\n\treturn func(mt *MultiplexTransport) { mt.resolver = resolver }\n}\n\n// MultiplexTransport accepts and dials tcp connections and upgrades them to\n// multiplexed peers.\ntype MultiplexTransport struct {\n\tnetAddr  NetAddress\n\tlistener net.Listener\n\n\tacceptc chan accept\n\tclosec  chan struct{}\n\n\t// Lookup table for duplicate ip and id checks.\n\tconns       ConnSet\n\tconnFilters []ConnFilterFunc\n\n\tdialTimeout      time.Duration\n\tfilterTimeout    time.Duration\n\thandshakeTimeout time.Duration\n\tnodeInfo         NodeInfo\n\tnodeKey          NodeKey\n\tresolver         IPResolver\n\n\t// TODO(xla): This config is still needed as we parameterise peerConn and\n\t// peer currently. All relevant configuration should be refactored into options\n\t// with sane defaults.\n\tmConfig conn.MConnConfig\n}\n\n// Test multiplexTransport for interface completeness.\nvar _ Transport = (*MultiplexTransport)(nil)\nvar _ transportLifecycle = (*MultiplexTransport)(nil)\n\n// NewMultiplexTransport returns a tcp connected multiplexed peer.\nfunc NewMultiplexTransport(\n\tnodeInfo NodeInfo,\n\tnodeKey NodeKey,\n\tmConfig conn.MConnConfig,\n) *MultiplexTransport {\n\treturn &MultiplexTransport{\n\t\tacceptc:          make(chan accept),\n\t\tclosec:           make(chan struct{}),\n\t\tdialTimeout:      defaultDialTimeout,\n\t\tfilterTimeout:    defaultFilterTimeout,\n\t\thandshakeTimeout: defaultHandshakeTimeout,\n\t\tmConfig:          mConfig,\n\t\tnodeInfo:         nodeInfo,\n\t\tnodeKey:          nodeKey,\n\t\tconns:            NewConnSet(),\n\t\tresolver:         net.DefaultResolver,\n\t}\n}\n\n// NetAddress implements Transport.\nfunc (mt *MultiplexTransport) NetAddress() NetAddress {\n\treturn mt.netAddr\n}\n\n// Accept implements Transport.\nfunc (mt *MultiplexTransport) Accept(cfg peerConfig) (Peer, error) {\n\tselect {\n\t// This case should never have any side-effectful/blocking operations to\n\t// ensure that quality peers are ready to be used.\n\tcase a := <-mt.acceptc:\n\t\tif a.err != nil {\n\t\t\treturn nil, a.err\n\t\t}\n\n\t\tcfg.outbound = false\n\n\t\treturn mt.wrapPeer(a.conn, a.nodeInfo, cfg, a.netAddr), nil\n\tcase <-mt.closec:\n\t\treturn nil, ErrTransportClosed{}\n\t}\n}\n\n// Dial implements Transport.\nfunc (mt *MultiplexTransport) Dial(\n\taddr NetAddress,\n\tcfg peerConfig,\n) (Peer, error) {\n\tc, err := addr.DialTimeout(mt.dialTimeout)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// TODO(xla): Evaluate if we should apply filters if we explicitly dial.\n\tif err := mt.filterConn(c); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsecretConn, nodeInfo, err := mt.upgrade(c, &addr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcfg.outbound = true\n\n\tp := mt.wrapPeer(secretConn, nodeInfo, cfg, &addr)\n\n\treturn p, nil\n}\n\n// Close implements transportLifecycle.\nfunc (mt *MultiplexTransport) Close() error {\n\tclose(mt.closec)\n\n\tif mt.listener != nil {\n\t\treturn mt.listener.Close()\n\t}\n\n\treturn nil\n}\n\n// Listen implements transportLifecycle.\nfunc (mt *MultiplexTransport) Listen(addr NetAddress) error {\n\tln, err := net.Listen(\"tcp\", addr.DialString())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tmt.netAddr = addr\n\tmt.listener = ln\n\n\tgo mt.acceptPeers()\n\n\treturn nil\n}\n\nfunc (mt *MultiplexTransport) acceptPeers() {\n\tfor {\n\t\tc, err := mt.listener.Accept()\n\t\tif err != nil {\n\t\t\t// If Close() has been called, silently exit.\n\t\t\tselect {\n\t\t\tcase _, ok := <-mt.closec:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\t// Transport is not closed\n\t\t\t}\n\n\t\t\tmt.acceptc <- accept{err: err}\n\t\t\treturn\n\t\t}\n\n\t\t// Connection upgrade and filtering should be asynchronous to avoid\n\t\t// Head-of-line blocking[0].\n\t\t// Reference:  https://github.com/tendermint/tendermint/issues/2047\n\t\t//\n\t\t// [0] https://en.wikipedia.org/wiki/Head-of-line_blocking\n\t\tgo func(c net.Conn) {\n\t\t\tdefer func() {\n\t\t\t\tif r := recover(); r != nil {\n\t\t\t\t\terr := ErrRejected{\n\t\t\t\t\t\tconn:          c,\n\t\t\t\t\t\terr:           errors.Errorf(\"recovered from panic: %v\", r),\n\t\t\t\t\t\tisAuthFailure: true,\n\t\t\t\t\t}\n\t\t\t\t\tselect {\n\t\t\t\t\tcase mt.acceptc <- accept{err: err}:\n\t\t\t\t\tcase <-mt.closec:\n\t\t\t\t\t\t// Give up if the transport was closed.\n\t\t\t\t\t\t_ = c.Close()\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\tvar (\n\t\t\t\tnodeInfo   NodeInfo\n\t\t\t\tsecretConn *conn.SecretConnection\n\t\t\t\tnetAddr    *NetAddress\n\t\t\t)\n\n\t\t\terr := mt.filterConn(c)\n\t\t\tif err == nil {\n\t\t\t\tsecretConn, nodeInfo, err = mt.upgrade(c, nil)\n\t\t\t\tif err == nil {\n\t\t\t\t\taddr := c.RemoteAddr()\n\t\t\t\t\tid := PubKeyToID(secretConn.RemotePubKey())\n\t\t\t\t\tnetAddr = NewNetAddress(id, addr)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase mt.acceptc <- accept{netAddr, secretConn, nodeInfo, err}:\n\t\t\t\t// Make the upgraded peer available.\n\t\t\tcase <-mt.closec:\n\t\t\t\t// Give up if the transport was closed.\n\t\t\t\t_ = c.Close()\n\t\t\t\treturn\n\t\t\t}\n\t\t}(c)\n\t}\n}\n\n// Cleanup removes the given address from the connections set and\n// closes the connection.\nfunc (mt *MultiplexTransport) Cleanup(p Peer) {\n\tmt.conns.RemoveAddr(p.RemoteAddr())\n\t_ = p.CloseConn()\n}\n\nfunc (mt *MultiplexTransport) cleanup(c net.Conn) error {\n\tmt.conns.Remove(c)\n\n\treturn c.Close()\n}\n\nfunc (mt *MultiplexTransport) filterConn(c net.Conn) (err error) {\n\tdefer func() {\n\t\tif err != nil {\n\t\t\t_ = c.Close()\n\t\t}\n\t}()\n\n\t// Reject if connection is already present.\n\tif mt.conns.Has(c) {\n\t\treturn ErrRejected{conn: c, isDuplicate: true}\n\t}\n\n\t// Resolve ips for incoming conn.\n\tips, err := resolveIPs(mt.resolver, c)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terrc := make(chan error, len(mt.connFilters))\n\n\tfor _, f := range mt.connFilters {\n\t\tgo func(f ConnFilterFunc, c net.Conn, ips []net.IP, errc chan<- error) {\n\t\t\terrc <- f(mt.conns, c, ips)\n\t\t}(f, c, ips, errc)\n\t}\n\n\tfor i := 0; i < cap(errc); i++ {\n\t\tselect {\n\t\tcase err := <-errc:\n\t\t\tif err != nil {\n\t\t\t\treturn ErrRejected{conn: c, err: err, isFiltered: true}\n\t\t\t}\n\t\tcase <-time.After(mt.filterTimeout):\n\t\t\treturn ErrFilterTimeout{}\n\t\t}\n\n\t}\n\n\tmt.conns.Set(c, ips)\n\n\treturn nil\n}\n\nfunc (mt *MultiplexTransport) upgrade(\n\tc net.Conn,\n\tdialedAddr *NetAddress,\n) (secretConn *conn.SecretConnection, nodeInfo NodeInfo, err error) {\n\tdefer func() {\n\t\tif err != nil {\n\t\t\t_ = mt.cleanup(c)\n\t\t}\n\t}()\n\n\tsecretConn, err = upgradeSecretConn(c, mt.handshakeTimeout, mt.nodeKey.PrivKey)\n\tif err != nil {\n\t\treturn nil, nil, ErrRejected{\n\t\t\tconn:          c,\n\t\t\terr:           fmt.Errorf(\"secret conn failed: %v\", err),\n\t\t\tisAuthFailure: true,\n\t\t}\n\t}\n\n\t// For outgoing conns, ensure connection key matches dialed key.\n\tconnID := PubKeyToID(secretConn.RemotePubKey())\n\tif dialedAddr != nil {\n\t\tif dialedID := dialedAddr.ID; connID != dialedID {\n\t\t\treturn nil, nil, ErrRejected{\n\t\t\t\tconn: c,\n\t\t\t\tid:   connID,\n\t\t\t\terr: fmt.Errorf(\n\t\t\t\t\t\"conn.ID (%v) dialed ID (%v) mismatch\",\n\t\t\t\t\tconnID,\n\t\t\t\t\tdialedID,\n\t\t\t\t),\n\t\t\t\tisAuthFailure: true,\n\t\t\t}\n\t\t}\n\t}\n\n\tnodeInfo, err = handshake(secretConn, mt.handshakeTimeout, mt.nodeInfo)\n\tif err != nil {\n\t\treturn nil, nil, ErrRejected{\n\t\t\tconn:          c,\n\t\t\terr:           fmt.Errorf(\"handshake failed: %v\", err),\n\t\t\tisAuthFailure: true,\n\t\t}\n\t}\n\n\tif err := nodeInfo.Validate(); err != nil {\n\t\treturn nil, nil, ErrRejected{\n\t\t\tconn:              c,\n\t\t\terr:               err,\n\t\t\tisNodeInfoInvalid: true,\n\t\t}\n\t}\n\n\t// Ensure connection key matches self reported key.\n\tif connID != nodeInfo.ID() {\n\t\treturn nil, nil, ErrRejected{\n\t\t\tconn: c,\n\t\t\tid:   connID,\n\t\t\terr: fmt.Errorf(\n\t\t\t\t\"conn.ID (%v) NodeInfo.ID (%v) mismatch\",\n\t\t\t\tconnID,\n\t\t\t\tnodeInfo.ID(),\n\t\t\t),\n\t\t\tisAuthFailure: true,\n\t\t}\n\t}\n\n\t// Reject self.\n\tif mt.nodeInfo.ID() == nodeInfo.ID() {\n\t\treturn nil, nil, ErrRejected{\n\t\t\taddr:   *NewNetAddress(nodeInfo.ID(), c.RemoteAddr()),\n\t\t\tconn:   c,\n\t\t\tid:     nodeInfo.ID(),\n\t\t\tisSelf: true,\n\t\t}\n\t}\n\n\tif err := mt.nodeInfo.CompatibleWith(nodeInfo); err != nil {\n\t\treturn nil, nil, ErrRejected{\n\t\t\tconn:           c,\n\t\t\terr:            err,\n\t\t\tid:             nodeInfo.ID(),\n\t\t\tisIncompatible: true,\n\t\t}\n\t}\n\n\treturn secretConn, nodeInfo, nil\n}\n\nfunc (mt *MultiplexTransport) wrapPeer(\n\tc net.Conn,\n\tni NodeInfo,\n\tcfg peerConfig,\n\tsocketAddr *NetAddress,\n) Peer {\n\n\tpersistent := false\n\tif cfg.isPersistent != nil {\n\t\tif cfg.outbound {\n\t\t\tpersistent = cfg.isPersistent(socketAddr)\n\t\t} else {\n\t\t\tselfReportedAddr, err := ni.NetAddress()\n\t\t\tif err == nil {\n\t\t\t\tpersistent = cfg.isPersistent(selfReportedAddr)\n\t\t\t}\n\t\t}\n\t}\n\n\tpeerConn := newPeerConn(\n\t\tcfg.outbound,\n\t\tpersistent,\n\t\tc,\n\t\tsocketAddr,\n\t)\n\n\tp := newPeer(\n\t\tpeerConn,\n\t\tmt.mConfig,\n\t\tni,\n\t\tcfg.reactorsByCh,\n\t\tcfg.chDescs,\n\t\tcfg.onPeerError,\n\t\tPeerMetrics(cfg.metrics),\n\t)\n\n\treturn p\n}\n\nfunc handshake(\n\tc net.Conn,\n\ttimeout time.Duration,\n\tnodeInfo NodeInfo,\n) (NodeInfo, error) {\n\tif err := c.SetDeadline(time.Now().Add(timeout)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\terrc = make(chan error, 2)\n\n\t\tpeerNodeInfo DefaultNodeInfo\n\t\tourNodeInfo  = nodeInfo.(DefaultNodeInfo)\n\t)\n\n\tgo func(errc chan<- error, c net.Conn) {\n\t\t_, err := cdc.MarshalBinaryLengthPrefixedWriter(c, ourNodeInfo)\n\t\terrc <- err\n\t}(errc, c)\n\tgo func(errc chan<- error, c net.Conn) {\n\t\t_, err := cdc.UnmarshalBinaryLengthPrefixedReader(\n\t\t\tc,\n\t\t\t&peerNodeInfo,\n\t\t\tint64(MaxNodeInfoSize()),\n\t\t)\n\t\terrc <- err\n\t}(errc, c)\n\n\tfor i := 0; i < cap(errc); i++ {\n\t\terr := <-errc\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn peerNodeInfo, c.SetDeadline(time.Time{})\n}\n\nfunc upgradeSecretConn(\n\tc net.Conn,\n\ttimeout time.Duration,\n\tprivKey crypto.PrivKey,\n) (*conn.SecretConnection, error) {\n\tif err := c.SetDeadline(time.Now().Add(timeout)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsc, err := conn.MakeSecretConnection(c, privKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn sc, sc.SetDeadline(time.Time{})\n}\n\nfunc resolveIPs(resolver IPResolver, c net.Conn) ([]net.IP, error) {\n\thost, _, err := net.SplitHostPort(c.RemoteAddr().String())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\taddrs, err := resolver.LookupIPAddr(context.Background(), host)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tips := []net.IP{}\n\n\tfor _, addr := range addrs {\n\t\tips = append(ips, addr.IP)\n\t}\n\n\treturn ips, nil\n}\n", "package p2p\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"net\"\n\t\"reflect\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/tendermint/tendermint/crypto/ed25519\"\n\t\"github.com/tendermint/tendermint/p2p/conn\"\n)\n\nvar defaultNodeName = \"host_peer\"\n\nfunc emptyNodeInfo() NodeInfo {\n\treturn DefaultNodeInfo{}\n}\n\n// newMultiplexTransport returns a tcp connected multiplexed peer\n// using the default MConnConfig. It's a convenience function used\n// for testing.\nfunc newMultiplexTransport(\n\tnodeInfo NodeInfo,\n\tnodeKey NodeKey,\n) *MultiplexTransport {\n\treturn NewMultiplexTransport(\n\t\tnodeInfo, nodeKey, conn.DefaultMConnConfig(),\n\t)\n}\n\nfunc TestTransportMultiplexConnFilter(t *testing.T) {\n\tmt := newMultiplexTransport(\n\t\temptyNodeInfo(),\n\t\tNodeKey{\n\t\t\tPrivKey: ed25519.GenPrivKey(),\n\t\t},\n\t)\n\tid := mt.nodeKey.ID()\n\n\tMultiplexTransportConnFilters(\n\t\tfunc(_ ConnSet, _ net.Conn, _ []net.IP) error { return nil },\n\t\tfunc(_ ConnSet, _ net.Conn, _ []net.IP) error { return nil },\n\t\tfunc(_ ConnSet, _ net.Conn, _ []net.IP) error {\n\t\t\treturn fmt.Errorf(\"rejected\")\n\t\t},\n\t)(mt)\n\n\taddr, err := NewNetAddressString(IDAddressString(id, \"127.0.0.1:0\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := mt.Listen(*addr); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\taddr := NewNetAddress(id, mt.listener.Addr())\n\n\t\t_, err := addr.Dial()\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tt.Errorf(\"connection failed: %v\", err)\n\t}\n\n\t_, err = mt.Accept(peerConfig{})\n\tif err, ok := err.(ErrRejected); ok {\n\t\tif !err.IsFiltered() {\n\t\t\tt.Errorf(\"expected peer to be filtered\")\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected ErrRejected\")\n\t}\n}\n\nfunc TestTransportMultiplexConnFilterTimeout(t *testing.T) {\n\tmt := newMultiplexTransport(\n\t\temptyNodeInfo(),\n\t\tNodeKey{\n\t\t\tPrivKey: ed25519.GenPrivKey(),\n\t\t},\n\t)\n\tid := mt.nodeKey.ID()\n\n\tMultiplexTransportFilterTimeout(5 * time.Millisecond)(mt)\n\tMultiplexTransportConnFilters(\n\t\tfunc(_ ConnSet, _ net.Conn, _ []net.IP) error {\n\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\treturn nil\n\t\t},\n\t)(mt)\n\n\taddr, err := NewNetAddressString(IDAddressString(id, \"127.0.0.1:0\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := mt.Listen(*addr); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\taddr := NewNetAddress(id, mt.listener.Addr())\n\n\t\t_, err := addr.Dial()\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tt.Errorf(\"connection failed: %v\", err)\n\t}\n\n\t_, err = mt.Accept(peerConfig{})\n\tif _, ok := err.(ErrFilterTimeout); !ok {\n\t\tt.Errorf(\"expected ErrFilterTimeout\")\n\t}\n}\n\nfunc TestTransportMultiplexAcceptMultiple(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\tladdr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\tvar (\n\t\tseed     = rand.New(rand.NewSource(time.Now().UnixNano()))\n\t\tnDialers = seed.Intn(64) + 64\n\t\terrc     = make(chan error, nDialers)\n\t)\n\n\t// Setup dialers.\n\tfor i := 0; i < nDialers; i++ {\n\t\tgo testDialer(*laddr, errc)\n\t}\n\n\t// Catch connection errors.\n\tfor i := 0; i < nDialers; i++ {\n\t\tif err := <-errc; err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tps := []Peer{}\n\n\t// Accept all peers.\n\tfor i := 0; i < cap(errc); i++ {\n\t\tp, err := mt.Accept(peerConfig{})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif err := p.Start(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tps = append(ps, p)\n\t}\n\n\tif have, want := len(ps), cap(errc); have != want {\n\t\tt.Errorf(\"have %v, want %v\", have, want)\n\t}\n\n\t// Stop all peers.\n\tfor _, p := range ps {\n\t\tif err := p.Stop(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tif err := mt.Close(); err != nil {\n\t\tt.Errorf(\"close errored: %v\", err)\n\t}\n}\n\nfunc testDialer(dialAddr NetAddress, errc chan error) {\n\tvar (\n\t\tpv     = ed25519.GenPrivKey()\n\t\tdialer = newMultiplexTransport(\n\t\t\ttestNodeInfo(PubKeyToID(pv.PubKey()), defaultNodeName),\n\t\t\tNodeKey{\n\t\t\t\tPrivKey: pv,\n\t\t\t},\n\t\t)\n\t)\n\n\t_, err := dialer.Dial(dialAddr, peerConfig{})\n\tif err != nil {\n\t\terrc <- err\n\t\treturn\n\t}\n\n\t// Signal that the connection was established.\n\terrc <- nil\n}\n\nfunc TestTransportMultiplexAcceptNonBlocking(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\tvar (\n\t\tfastNodePV   = ed25519.GenPrivKey()\n\t\tfastNodeInfo = testNodeInfo(PubKeyToID(fastNodePV.PubKey()), \"fastnode\")\n\t\terrc         = make(chan error)\n\t\tfastc        = make(chan struct{})\n\t\tslowc        = make(chan struct{})\n\t)\n\n\t// Simulate slow Peer.\n\tgo func() {\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\tc, err := addr.Dial()\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(slowc)\n\n\t\tselect {\n\t\tcase <-fastc:\n\t\t\t// Fast peer connected.\n\t\tcase <-time.After(50 * time.Millisecond):\n\t\t\t// We error if the fast peer didn't succeed.\n\t\t\terrc <- fmt.Errorf(\"fast peer timed out\")\n\t\t}\n\n\t\tsc, err := upgradeSecretConn(c, 20*time.Millisecond, ed25519.GenPrivKey())\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\t_, err = handshake(sc, 20*time.Millisecond,\n\t\t\ttestNodeInfo(\n\t\t\t\tPubKeyToID(ed25519.GenPrivKey().PubKey()),\n\t\t\t\t\"slow_peer\",\n\t\t\t))\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\t}()\n\n\t// Simulate fast Peer.\n\tgo func() {\n\t\t<-slowc\n\n\t\tvar (\n\t\t\tdialer = newMultiplexTransport(\n\t\t\t\tfastNodeInfo,\n\t\t\t\tNodeKey{\n\t\t\t\t\tPrivKey: fastNodePV,\n\t\t\t\t},\n\t\t\t)\n\t\t)\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\t_, err := dialer.Dial(*addr, peerConfig{})\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t\tclose(fastc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tt.Errorf(\"connection failed: %v\", err)\n\t}\n\n\tp, err := mt.Accept(peerConfig{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif have, want := p.NodeInfo(), fastNodeInfo; !reflect.DeepEqual(have, want) {\n\t\tt.Errorf(\"have %v, want %v\", have, want)\n\t}\n}\n\nfunc TestTransportMultiplexValidateNodeInfo(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\tvar (\n\t\t\tpv     = ed25519.GenPrivKey()\n\t\t\tdialer = newMultiplexTransport(\n\t\t\t\ttestNodeInfo(PubKeyToID(pv.PubKey()), \"\"), // Should not be empty\n\t\t\t\tNodeKey{\n\t\t\t\t\tPrivKey: pv,\n\t\t\t\t},\n\t\t\t)\n\t\t)\n\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\t_, err := dialer.Dial(*addr, peerConfig{})\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tt.Errorf(\"connection failed: %v\", err)\n\t}\n\n\t_, err := mt.Accept(peerConfig{})\n\tif err, ok := err.(ErrRejected); ok {\n\t\tif !err.IsNodeInfoInvalid() {\n\t\t\tt.Errorf(\"expected NodeInfo to be invalid\")\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected ErrRejected\")\n\t}\n}\n\nfunc TestTransportMultiplexRejectMissmatchID(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\tdialer := newMultiplexTransport(\n\t\t\ttestNodeInfo(\n\t\t\t\tPubKeyToID(ed25519.GenPrivKey().PubKey()), \"dialer\",\n\t\t\t),\n\t\t\tNodeKey{\n\t\t\t\tPrivKey: ed25519.GenPrivKey(),\n\t\t\t},\n\t\t)\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\t_, err := dialer.Dial(*addr, peerConfig{})\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tt.Errorf(\"connection failed: %v\", err)\n\t}\n\n\t_, err := mt.Accept(peerConfig{})\n\tif err, ok := err.(ErrRejected); ok {\n\t\tif !err.IsAuthFailure() {\n\t\t\tt.Errorf(\"expected auth failure\")\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected ErrRejected\")\n\t}\n}\n\nfunc TestTransportMultiplexDialRejectWrongID(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\tvar (\n\t\tpv     = ed25519.GenPrivKey()\n\t\tdialer = newMultiplexTransport(\n\t\t\ttestNodeInfo(PubKeyToID(pv.PubKey()), \"\"), // Should not be empty\n\t\t\tNodeKey{\n\t\t\t\tPrivKey: pv,\n\t\t\t},\n\t\t)\n\t)\n\n\twrongID := PubKeyToID(ed25519.GenPrivKey().PubKey())\n\taddr := NewNetAddress(wrongID, mt.listener.Addr())\n\n\t_, err := dialer.Dial(*addr, peerConfig{})\n\tif err != nil {\n\t\tt.Logf(\"connection failed: %v\", err)\n\t\tif err, ok := err.(ErrRejected); ok {\n\t\t\tif !err.IsAuthFailure() {\n\t\t\t\tt.Errorf(\"expected auth failure\")\n\t\t\t}\n\t\t} else {\n\t\t\tt.Errorf(\"expected ErrRejected\")\n\t\t}\n\t}\n}\n\nfunc TestTransportMultiplexRejectIncompatible(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\tvar (\n\t\t\tpv     = ed25519.GenPrivKey()\n\t\t\tdialer = newMultiplexTransport(\n\t\t\t\ttestNodeInfoWithNetwork(PubKeyToID(pv.PubKey()), \"dialer\", \"incompatible-network\"),\n\t\t\t\tNodeKey{\n\t\t\t\t\tPrivKey: pv,\n\t\t\t\t},\n\t\t\t)\n\t\t)\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\t_, err := dialer.Dial(*addr, peerConfig{})\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\t_, err := mt.Accept(peerConfig{})\n\tif err, ok := err.(ErrRejected); ok {\n\t\tif !err.IsIncompatible() {\n\t\t\tt.Errorf(\"expected to reject incompatible\")\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected ErrRejected\")\n\t}\n}\n\nfunc TestTransportMultiplexRejectSelf(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\t_, err := mt.Dial(*addr, peerConfig{})\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tif err, ok := err.(ErrRejected); ok {\n\t\t\tif !err.IsSelf() {\n\t\t\t\tt.Errorf(\"expected to reject self, got: %v\", err)\n\t\t\t}\n\t\t} else {\n\t\t\tt.Errorf(\"expected ErrRejected\")\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected connection failure\")\n\t}\n\n\t_, err := mt.Accept(peerConfig{})\n\tif err, ok := err.(ErrRejected); ok {\n\t\tif !err.IsSelf() {\n\t\t\tt.Errorf(\"expected to reject self, got: %v\", err)\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected ErrRejected\")\n\t}\n}\n\nfunc TestTransportConnDuplicateIPFilter(t *testing.T) {\n\tfilter := ConnDuplicateIPFilter()\n\n\tif err := filter(nil, &testTransportConn{}, nil); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar (\n\t\tc  = &testTransportConn{}\n\t\tcs = NewConnSet()\n\t)\n\n\tcs.Set(c, []net.IP{\n\t\t{10, 0, 10, 1},\n\t\t{10, 0, 10, 2},\n\t\t{10, 0, 10, 3},\n\t})\n\n\tif err := filter(cs, c, []net.IP{\n\t\t{10, 0, 10, 2},\n\t}); err == nil {\n\t\tt.Errorf(\"expected Peer to be rejected as duplicate\")\n\t}\n}\n\nfunc TestTransportHandshake(t *testing.T) {\n\tln, err := net.Listen(\"tcp\", \"127.0.0.1:0\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar (\n\t\tpeerPV       = ed25519.GenPrivKey()\n\t\tpeerNodeInfo = testNodeInfo(PubKeyToID(peerPV.PubKey()), defaultNodeName)\n\t)\n\n\tgo func() {\n\t\tc, err := net.Dial(ln.Addr().Network(), ln.Addr().String())\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\n\t\tgo func(c net.Conn) {\n\t\t\t_, err := cdc.MarshalBinaryLengthPrefixedWriter(c, peerNodeInfo.(DefaultNodeInfo))\n\t\t\tif err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t}\n\t\t}(c)\n\t\tgo func(c net.Conn) {\n\t\t\tvar ni DefaultNodeInfo\n\n\t\t\t_, err := cdc.UnmarshalBinaryLengthPrefixedReader(\n\t\t\t\tc,\n\t\t\t\t&ni,\n\t\t\t\tint64(MaxNodeInfoSize()),\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t}\n\t\t}(c)\n\t}()\n\n\tc, err := ln.Accept()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tni, err := handshake(c, 20*time.Millisecond, emptyNodeInfo())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif have, want := ni, peerNodeInfo; !reflect.DeepEqual(have, want) {\n\t\tt.Errorf(\"have %v, want %v\", have, want)\n\t}\n}\n\n// create listener\nfunc testSetupMultiplexTransport(t *testing.T) *MultiplexTransport {\n\tvar (\n\t\tpv = ed25519.GenPrivKey()\n\t\tid = PubKeyToID(pv.PubKey())\n\t\tmt = newMultiplexTransport(\n\t\t\ttestNodeInfo(\n\t\t\t\tid, \"transport\",\n\t\t\t),\n\t\t\tNodeKey{\n\t\t\t\tPrivKey: pv,\n\t\t\t},\n\t\t)\n\t)\n\n\taddr, err := NewNetAddressString(IDAddressString(id, \"127.0.0.1:0\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := mt.Listen(*addr); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\treturn mt\n}\n\ntype testTransportAddr struct{}\n\nfunc (a *testTransportAddr) Network() string { return \"tcp\" }\nfunc (a *testTransportAddr) String() string  { return \"test.local:1234\" }\n\ntype testTransportConn struct{}\n\nfunc (c *testTransportConn) Close() error {\n\treturn fmt.Errorf(\"close() not implemented\")\n}\n\nfunc (c *testTransportConn) LocalAddr() net.Addr {\n\treturn &testTransportAddr{}\n}\n\nfunc (c *testTransportConn) RemoteAddr() net.Addr {\n\treturn &testTransportAddr{}\n}\n\nfunc (c *testTransportConn) Read(_ []byte) (int, error) {\n\treturn -1, fmt.Errorf(\"read() not implemented\")\n}\n\nfunc (c *testTransportConn) SetDeadline(_ time.Time) error {\n\treturn fmt.Errorf(\"setDeadline() not implemented\")\n}\n\nfunc (c *testTransportConn) SetReadDeadline(_ time.Time) error {\n\treturn fmt.Errorf(\"setReadDeadline() not implemented\")\n}\n\nfunc (c *testTransportConn) SetWriteDeadline(_ time.Time) error {\n\treturn fmt.Errorf(\"setWriteDeadline() not implemented\")\n}\n\nfunc (c *testTransportConn) Write(_ []byte) (int, error) {\n\treturn -1, fmt.Errorf(\"write() not implemented\")\n}\n"], "fixing_code": ["package node\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"net/http\"\n\t_ \"net/http/pprof\" // nolint: gosec // securely exposed on separate, optional port\n\t\"os\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/rs/cors\"\n\n\tamino \"github.com/tendermint/go-amino\"\n\tdbm \"github.com/tendermint/tm-db\"\n\n\tabci \"github.com/tendermint/tendermint/abci/types\"\n\tbcv0 \"github.com/tendermint/tendermint/blockchain/v0\"\n\tbcv1 \"github.com/tendermint/tendermint/blockchain/v1\"\n\tbcv2 \"github.com/tendermint/tendermint/blockchain/v2\"\n\tcfg \"github.com/tendermint/tendermint/config\"\n\t\"github.com/tendermint/tendermint/consensus\"\n\tcs \"github.com/tendermint/tendermint/consensus\"\n\t\"github.com/tendermint/tendermint/crypto\"\n\t\"github.com/tendermint/tendermint/evidence\"\n\t\"github.com/tendermint/tendermint/libs/log\"\n\ttmpubsub \"github.com/tendermint/tendermint/libs/pubsub\"\n\t\"github.com/tendermint/tendermint/libs/service\"\n\tmempl \"github.com/tendermint/tendermint/mempool\"\n\t\"github.com/tendermint/tendermint/p2p\"\n\t\"github.com/tendermint/tendermint/p2p/pex\"\n\t\"github.com/tendermint/tendermint/privval\"\n\t\"github.com/tendermint/tendermint/proxy\"\n\trpccore \"github.com/tendermint/tendermint/rpc/core\"\n\tctypes \"github.com/tendermint/tendermint/rpc/core/types\"\n\tgrpccore \"github.com/tendermint/tendermint/rpc/grpc\"\n\trpcserver \"github.com/tendermint/tendermint/rpc/lib/server\"\n\tsm \"github.com/tendermint/tendermint/state\"\n\t\"github.com/tendermint/tendermint/state/txindex\"\n\t\"github.com/tendermint/tendermint/state/txindex/kv\"\n\t\"github.com/tendermint/tendermint/state/txindex/null\"\n\t\"github.com/tendermint/tendermint/store\"\n\t\"github.com/tendermint/tendermint/types\"\n\ttmtime \"github.com/tendermint/tendermint/types/time\"\n\t\"github.com/tendermint/tendermint/version\"\n)\n\n//------------------------------------------------------------------------------\n\n// DBContext specifies config information for loading a new DB.\ntype DBContext struct {\n\tID     string\n\tConfig *cfg.Config\n}\n\n// DBProvider takes a DBContext and returns an instantiated DB.\ntype DBProvider func(*DBContext) (dbm.DB, error)\n\n// DefaultDBProvider returns a database using the DBBackend and DBDir\n// specified in the ctx.Config.\nfunc DefaultDBProvider(ctx *DBContext) (dbm.DB, error) {\n\tdbType := dbm.BackendType(ctx.Config.DBBackend)\n\treturn dbm.NewDB(ctx.ID, dbType, ctx.Config.DBDir()), nil\n}\n\n// GenesisDocProvider returns a GenesisDoc.\n// It allows the GenesisDoc to be pulled from sources other than the\n// filesystem, for instance from a distributed key-value store cluster.\ntype GenesisDocProvider func() (*types.GenesisDoc, error)\n\n// DefaultGenesisDocProviderFunc returns a GenesisDocProvider that loads\n// the GenesisDoc from the config.GenesisFile() on the filesystem.\nfunc DefaultGenesisDocProviderFunc(config *cfg.Config) GenesisDocProvider {\n\treturn func() (*types.GenesisDoc, error) {\n\t\treturn types.GenesisDocFromFile(config.GenesisFile())\n\t}\n}\n\n// Provider takes a config and a logger and returns a ready to go Node.\ntype Provider func(*cfg.Config, log.Logger) (*Node, error)\n\n// DefaultNewNode returns a Tendermint node with default settings for the\n// PrivValidator, ClientCreator, GenesisDoc, and DBProvider.\n// It implements NodeProvider.\nfunc DefaultNewNode(config *cfg.Config, logger log.Logger) (*Node, error) {\n\t// Generate node PrivKey\n\tnodeKey, err := p2p.LoadOrGenNodeKey(config.NodeKeyFile())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Convert old PrivValidator if it exists.\n\toldPrivVal := config.OldPrivValidatorFile()\n\tnewPrivValKey := config.PrivValidatorKeyFile()\n\tnewPrivValState := config.PrivValidatorStateFile()\n\tif _, err := os.Stat(oldPrivVal); !os.IsNotExist(err) {\n\t\toldPV, err := privval.LoadOldFilePV(oldPrivVal)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error reading OldPrivValidator from %v: %v\", oldPrivVal, err)\n\t\t}\n\t\tlogger.Info(\"Upgrading PrivValidator file\",\n\t\t\t\"old\", oldPrivVal,\n\t\t\t\"newKey\", newPrivValKey,\n\t\t\t\"newState\", newPrivValState,\n\t\t)\n\t\toldPV.Upgrade(newPrivValKey, newPrivValState)\n\t}\n\n\treturn NewNode(config,\n\t\tprivval.LoadOrGenFilePV(newPrivValKey, newPrivValState),\n\t\tnodeKey,\n\t\tproxy.DefaultClientCreator(config.ProxyApp, config.ABCI, config.DBDir()),\n\t\tDefaultGenesisDocProviderFunc(config),\n\t\tDefaultDBProvider,\n\t\tDefaultMetricsProvider(config.Instrumentation),\n\t\tlogger,\n\t)\n}\n\n// MetricsProvider returns a consensus, p2p and mempool Metrics.\ntype MetricsProvider func(chainID string) (*cs.Metrics, *p2p.Metrics, *mempl.Metrics, *sm.Metrics)\n\n// DefaultMetricsProvider returns Metrics build using Prometheus client library\n// if Prometheus is enabled. Otherwise, it returns no-op Metrics.\nfunc DefaultMetricsProvider(config *cfg.InstrumentationConfig) MetricsProvider {\n\treturn func(chainID string) (*cs.Metrics, *p2p.Metrics, *mempl.Metrics, *sm.Metrics) {\n\t\tif config.Prometheus {\n\t\t\treturn cs.PrometheusMetrics(config.Namespace, \"chain_id\", chainID),\n\t\t\t\tp2p.PrometheusMetrics(config.Namespace, \"chain_id\", chainID),\n\t\t\t\tmempl.PrometheusMetrics(config.Namespace, \"chain_id\", chainID),\n\t\t\t\tsm.PrometheusMetrics(config.Namespace, \"chain_id\", chainID)\n\t\t}\n\t\treturn cs.NopMetrics(), p2p.NopMetrics(), mempl.NopMetrics(), sm.NopMetrics()\n\t}\n}\n\n// Option sets a parameter for the node.\ntype Option func(*Node)\n\n// CustomReactors allows you to add custom reactors (name -> p2p.Reactor) to\n// the node's Switch.\n//\n// WARNING: using any name from the below list of the existing reactors will\n// result in replacing it with the custom one.\n//\n//  - MEMPOOL\n//  - BLOCKCHAIN\n//  - CONSENSUS\n//  - EVIDENCE\n//  - PEX\nfunc CustomReactors(reactors map[string]p2p.Reactor) Option {\n\treturn func(n *Node) {\n\t\tfor name, reactor := range reactors {\n\t\t\tif existingReactor := n.sw.Reactor(name); existingReactor != nil {\n\t\t\t\tn.sw.Logger.Info(\"Replacing existing reactor with a custom one\",\n\t\t\t\t\t\"name\", name, \"existing\", existingReactor, \"custom\", reactor)\n\t\t\t\tn.sw.RemoveReactor(name, existingReactor)\n\t\t\t}\n\t\t\tn.sw.AddReactor(name, reactor)\n\t\t}\n\t}\n}\n\n//------------------------------------------------------------------------------\n\n// Node is the highest level interface to a full Tendermint node.\n// It includes all configuration information and running services.\ntype Node struct {\n\tservice.BaseService\n\n\t// config\n\tconfig        *cfg.Config\n\tgenesisDoc    *types.GenesisDoc   // initial validator set\n\tprivValidator types.PrivValidator // local node's validator key\n\n\t// network\n\ttransport   *p2p.MultiplexTransport\n\tsw          *p2p.Switch  // p2p connections\n\taddrBook    pex.AddrBook // known peers\n\tnodeInfo    p2p.NodeInfo\n\tnodeKey     *p2p.NodeKey // our node privkey\n\tisListening bool\n\n\t// services\n\teventBus         *types.EventBus // pub/sub for services\n\tstateDB          dbm.DB\n\tblockStore       *store.BlockStore // store the blockchain to disk\n\tbcReactor        p2p.Reactor       // for fast-syncing\n\tmempoolReactor   *mempl.Reactor    // for gossipping transactions\n\tmempool          mempl.Mempool\n\tconsensusState   *cs.State      // latest consensus state\n\tconsensusReactor *cs.Reactor    // for participating in the consensus\n\tpexReactor       *pex.Reactor   // for exchanging peer addresses\n\tevidencePool     *evidence.Pool // tracking evidence\n\tproxyApp         proxy.AppConns // connection to the application\n\trpcListeners     []net.Listener // rpc servers\n\ttxIndexer        txindex.TxIndexer\n\tindexerService   *txindex.IndexerService\n\tprometheusSrv    *http.Server\n}\n\nfunc initDBs(config *cfg.Config, dbProvider DBProvider) (blockStore *store.BlockStore, stateDB dbm.DB, err error) {\n\tvar blockStoreDB dbm.DB\n\tblockStoreDB, err = dbProvider(&DBContext{\"blockstore\", config})\n\tif err != nil {\n\t\treturn\n\t}\n\tblockStore = store.NewBlockStore(blockStoreDB)\n\n\tstateDB, err = dbProvider(&DBContext{\"state\", config})\n\tif err != nil {\n\t\treturn\n\t}\n\n\treturn\n}\n\nfunc createAndStartProxyAppConns(clientCreator proxy.ClientCreator, logger log.Logger) (proxy.AppConns, error) {\n\tproxyApp := proxy.NewAppConns(clientCreator)\n\tproxyApp.SetLogger(logger.With(\"module\", \"proxy\"))\n\tif err := proxyApp.Start(); err != nil {\n\t\treturn nil, fmt.Errorf(\"error starting proxy app connections: %v\", err)\n\t}\n\treturn proxyApp, nil\n}\n\nfunc createAndStartEventBus(logger log.Logger) (*types.EventBus, error) {\n\teventBus := types.NewEventBus()\n\teventBus.SetLogger(logger.With(\"module\", \"events\"))\n\tif err := eventBus.Start(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn eventBus, nil\n}\n\nfunc createAndStartIndexerService(config *cfg.Config, dbProvider DBProvider,\n\teventBus *types.EventBus, logger log.Logger) (*txindex.IndexerService, txindex.TxIndexer, error) {\n\n\tvar txIndexer txindex.TxIndexer\n\tswitch config.TxIndex.Indexer {\n\tcase \"kv\":\n\t\tstore, err := dbProvider(&DBContext{\"tx_index\", config})\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t\tswitch {\n\t\tcase config.TxIndex.IndexKeys != \"\":\n\t\t\ttxIndexer = kv.NewTxIndex(store, kv.IndexEvents(splitAndTrimEmpty(config.TxIndex.IndexKeys, \",\", \" \")))\n\t\tcase config.TxIndex.IndexAllKeys:\n\t\t\ttxIndexer = kv.NewTxIndex(store, kv.IndexAllEvents())\n\t\tdefault:\n\t\t\ttxIndexer = kv.NewTxIndex(store)\n\t\t}\n\tdefault:\n\t\ttxIndexer = &null.TxIndex{}\n\t}\n\n\tindexerService := txindex.NewIndexerService(txIndexer, eventBus)\n\tindexerService.SetLogger(logger.With(\"module\", \"txindex\"))\n\tif err := indexerService.Start(); err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn indexerService, txIndexer, nil\n}\n\nfunc doHandshake(\n\tstateDB dbm.DB,\n\tstate sm.State,\n\tblockStore sm.BlockStore,\n\tgenDoc *types.GenesisDoc,\n\teventBus types.BlockEventPublisher,\n\tproxyApp proxy.AppConns,\n\tconsensusLogger log.Logger) error {\n\n\thandshaker := cs.NewHandshaker(stateDB, state, blockStore, genDoc)\n\thandshaker.SetLogger(consensusLogger)\n\thandshaker.SetEventBus(eventBus)\n\tif err := handshaker.Handshake(proxyApp); err != nil {\n\t\treturn fmt.Errorf(\"error during handshake: %v\", err)\n\t}\n\treturn nil\n}\n\nfunc logNodeStartupInfo(state sm.State, pubKey crypto.PubKey, logger, consensusLogger log.Logger) {\n\t// Log the version info.\n\tlogger.Info(\"Version info\",\n\t\t\"software\", version.TMCoreSemVer,\n\t\t\"block\", version.BlockProtocol,\n\t\t\"p2p\", version.P2PProtocol,\n\t)\n\n\t// If the state and software differ in block version, at least log it.\n\tif state.Version.Consensus.Block != version.BlockProtocol {\n\t\tlogger.Info(\"Software and state have different block protocols\",\n\t\t\t\"software\", version.BlockProtocol,\n\t\t\t\"state\", state.Version.Consensus.Block,\n\t\t)\n\t}\n\n\taddr := pubKey.Address()\n\t// Log whether this node is a validator or an observer\n\tif state.Validators.HasAddress(addr) {\n\t\tconsensusLogger.Info(\"This node is a validator\", \"addr\", addr, \"pubKey\", pubKey)\n\t} else {\n\t\tconsensusLogger.Info(\"This node is not a validator\", \"addr\", addr, \"pubKey\", pubKey)\n\t}\n}\n\nfunc onlyValidatorIsUs(state sm.State, pubKey crypto.PubKey) bool {\n\tif state.Validators.Size() > 1 {\n\t\treturn false\n\t}\n\taddr, _ := state.Validators.GetByIndex(0)\n\treturn bytes.Equal(pubKey.Address(), addr)\n}\n\nfunc createMempoolAndMempoolReactor(config *cfg.Config, proxyApp proxy.AppConns,\n\tstate sm.State, memplMetrics *mempl.Metrics, logger log.Logger) (*mempl.Reactor, *mempl.CListMempool) {\n\n\tmempool := mempl.NewCListMempool(\n\t\tconfig.Mempool,\n\t\tproxyApp.Mempool(),\n\t\tstate.LastBlockHeight,\n\t\tmempl.WithMetrics(memplMetrics),\n\t\tmempl.WithPreCheck(sm.TxPreCheck(state)),\n\t\tmempl.WithPostCheck(sm.TxPostCheck(state)),\n\t)\n\tmempoolLogger := logger.With(\"module\", \"mempool\")\n\tmempoolReactor := mempl.NewReactor(config.Mempool, mempool)\n\tmempoolReactor.SetLogger(mempoolLogger)\n\n\tif config.Consensus.WaitForTxs() {\n\t\tmempool.EnableTxsAvailable()\n\t}\n\treturn mempoolReactor, mempool\n}\n\nfunc createEvidenceReactor(config *cfg.Config, dbProvider DBProvider,\n\tstateDB dbm.DB, logger log.Logger) (*evidence.Reactor, *evidence.Pool, error) {\n\n\tevidenceDB, err := dbProvider(&DBContext{\"evidence\", config})\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tevidenceLogger := logger.With(\"module\", \"evidence\")\n\tevidencePool := evidence.NewPool(stateDB, evidenceDB)\n\tevidencePool.SetLogger(evidenceLogger)\n\tevidenceReactor := evidence.NewReactor(evidencePool)\n\tevidenceReactor.SetLogger(evidenceLogger)\n\treturn evidenceReactor, evidencePool, nil\n}\n\nfunc createBlockchainReactor(config *cfg.Config,\n\tstate sm.State,\n\tblockExec *sm.BlockExecutor,\n\tblockStore *store.BlockStore,\n\tfastSync bool,\n\tlogger log.Logger) (bcReactor p2p.Reactor, err error) {\n\n\tswitch config.FastSync.Version {\n\tcase \"v0\":\n\t\tbcReactor = bcv0.NewBlockchainReactor(state.Copy(), blockExec, blockStore, fastSync)\n\tcase \"v1\":\n\t\tbcReactor = bcv1.NewBlockchainReactor(state.Copy(), blockExec, blockStore, fastSync)\n\tcase \"v2\":\n\t\tbcReactor = bcv2.NewBlockchainReactor(state.Copy(), blockExec, blockStore, fastSync)\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown fastsync version %s\", config.FastSync.Version)\n\t}\n\n\tbcReactor.SetLogger(logger.With(\"module\", \"blockchain\"))\n\treturn bcReactor, nil\n}\n\nfunc createConsensusReactor(config *cfg.Config,\n\tstate sm.State,\n\tblockExec *sm.BlockExecutor,\n\tblockStore sm.BlockStore,\n\tmempool *mempl.CListMempool,\n\tevidencePool *evidence.Pool,\n\tprivValidator types.PrivValidator,\n\tcsMetrics *cs.Metrics,\n\tfastSync bool,\n\teventBus *types.EventBus,\n\tconsensusLogger log.Logger) (*consensus.Reactor, *consensus.State) {\n\n\tconsensusState := cs.NewState(\n\t\tconfig.Consensus,\n\t\tstate.Copy(),\n\t\tblockExec,\n\t\tblockStore,\n\t\tmempool,\n\t\tevidencePool,\n\t\tcs.StateMetrics(csMetrics),\n\t)\n\tconsensusState.SetLogger(consensusLogger)\n\tif privValidator != nil {\n\t\tconsensusState.SetPrivValidator(privValidator)\n\t}\n\tconsensusReactor := cs.NewReactor(consensusState, fastSync, cs.ReactorMetrics(csMetrics))\n\tconsensusReactor.SetLogger(consensusLogger)\n\t// services which will be publishing and/or subscribing for messages (events)\n\t// consensusReactor will set it on consensusState and blockExecutor\n\tconsensusReactor.SetEventBus(eventBus)\n\treturn consensusReactor, consensusState\n}\n\nfunc createTransport(\n\tconfig *cfg.Config,\n\tnodeInfo p2p.NodeInfo,\n\tnodeKey *p2p.NodeKey,\n\tproxyApp proxy.AppConns,\n) (\n\t*p2p.MultiplexTransport,\n\t[]p2p.PeerFilterFunc,\n) {\n\tvar (\n\t\tmConnConfig = p2p.MConnConfig(config.P2P)\n\t\ttransport   = p2p.NewMultiplexTransport(nodeInfo, *nodeKey, mConnConfig)\n\t\tconnFilters = []p2p.ConnFilterFunc{}\n\t\tpeerFilters = []p2p.PeerFilterFunc{}\n\t)\n\n\tif !config.P2P.AllowDuplicateIP {\n\t\tconnFilters = append(connFilters, p2p.ConnDuplicateIPFilter())\n\t}\n\n\t// Filter peers by addr or pubkey with an ABCI query.\n\t// If the query return code is OK, add peer.\n\tif config.FilterPeers {\n\t\tconnFilters = append(\n\t\t\tconnFilters,\n\t\t\t// ABCI query for address filtering.\n\t\t\tfunc(_ p2p.ConnSet, c net.Conn, _ []net.IP) error {\n\t\t\t\tres, err := proxyApp.Query().QuerySync(abci.RequestQuery{\n\t\t\t\t\tPath: fmt.Sprintf(\"/p2p/filter/addr/%s\", c.RemoteAddr().String()),\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif res.IsErr() {\n\t\t\t\t\treturn fmt.Errorf(\"error querying abci app: %v\", res)\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t},\n\t\t)\n\n\t\tpeerFilters = append(\n\t\t\tpeerFilters,\n\t\t\t// ABCI query for ID filtering.\n\t\t\tfunc(_ p2p.IPeerSet, p p2p.Peer) error {\n\t\t\t\tres, err := proxyApp.Query().QuerySync(abci.RequestQuery{\n\t\t\t\t\tPath: fmt.Sprintf(\"/p2p/filter/id/%s\", p.ID()),\n\t\t\t\t})\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif res.IsErr() {\n\t\t\t\t\treturn fmt.Errorf(\"error querying abci app: %v\", res)\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t},\n\t\t)\n\t}\n\n\tp2p.MultiplexTransportConnFilters(connFilters...)(transport)\n\n\t// Limit the number of incoming connections.\n\tmax := config.P2P.MaxNumInboundPeers + len(splitAndTrimEmpty(config.P2P.UnconditionalPeerIDs, \",\", \" \"))\n\tp2p.MultiplexTransportMaxIncomingConnections(max)(transport)\n\n\treturn transport, peerFilters\n}\n\nfunc createSwitch(config *cfg.Config,\n\ttransport p2p.Transport,\n\tp2pMetrics *p2p.Metrics,\n\tpeerFilters []p2p.PeerFilterFunc,\n\tmempoolReactor *mempl.Reactor,\n\tbcReactor p2p.Reactor,\n\tconsensusReactor *consensus.Reactor,\n\tevidenceReactor *evidence.Reactor,\n\tnodeInfo p2p.NodeInfo,\n\tnodeKey *p2p.NodeKey,\n\tp2pLogger log.Logger) *p2p.Switch {\n\n\tsw := p2p.NewSwitch(\n\t\tconfig.P2P,\n\t\ttransport,\n\t\tp2p.WithMetrics(p2pMetrics),\n\t\tp2p.SwitchPeerFilters(peerFilters...),\n\t)\n\tsw.SetLogger(p2pLogger)\n\tsw.AddReactor(\"MEMPOOL\", mempoolReactor)\n\tsw.AddReactor(\"BLOCKCHAIN\", bcReactor)\n\tsw.AddReactor(\"CONSENSUS\", consensusReactor)\n\tsw.AddReactor(\"EVIDENCE\", evidenceReactor)\n\n\tsw.SetNodeInfo(nodeInfo)\n\tsw.SetNodeKey(nodeKey)\n\n\tp2pLogger.Info(\"P2P Node ID\", \"ID\", nodeKey.ID(), \"file\", config.NodeKeyFile())\n\treturn sw\n}\n\nfunc createAddrBookAndSetOnSwitch(config *cfg.Config, sw *p2p.Switch,\n\tp2pLogger log.Logger, nodeKey *p2p.NodeKey) (pex.AddrBook, error) {\n\n\taddrBook := pex.NewAddrBook(config.P2P.AddrBookFile(), config.P2P.AddrBookStrict)\n\taddrBook.SetLogger(p2pLogger.With(\"book\", config.P2P.AddrBookFile()))\n\n\t// Add ourselves to addrbook to prevent dialing ourselves\n\tif config.P2P.ExternalAddress != \"\" {\n\t\taddr, err := p2p.NewNetAddressString(p2p.IDAddressString(nodeKey.ID(), config.P2P.ExternalAddress))\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"p2p.external_address is incorrect\")\n\t\t}\n\t\taddrBook.AddOurAddress(addr)\n\t}\n\tif config.P2P.ListenAddress != \"\" {\n\t\taddr, err := p2p.NewNetAddressString(p2p.IDAddressString(nodeKey.ID(), config.P2P.ListenAddress))\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"p2p.laddr is incorrect\")\n\t\t}\n\t\taddrBook.AddOurAddress(addr)\n\t}\n\n\tsw.SetAddrBook(addrBook)\n\n\treturn addrBook, nil\n}\n\nfunc createPEXReactorAndAddToSwitch(addrBook pex.AddrBook, config *cfg.Config,\n\tsw *p2p.Switch, logger log.Logger) *pex.Reactor {\n\n\t// TODO persistent peers ? so we can have their DNS addrs saved\n\tpexReactor := pex.NewReactor(addrBook,\n\t\t&pex.ReactorConfig{\n\t\t\tSeeds:    splitAndTrimEmpty(config.P2P.Seeds, \",\", \" \"),\n\t\t\tSeedMode: config.P2P.SeedMode,\n\t\t\t// See consensus/reactor.go: blocksToContributeToBecomeGoodPeer 10000\n\t\t\t// blocks assuming 10s blocks ~ 28 hours.\n\t\t\t// TODO (melekes): make it dynamic based on the actual block latencies\n\t\t\t// from the live network.\n\t\t\t// https://github.com/tendermint/tendermint/issues/3523\n\t\t\tSeedDisconnectWaitPeriod:     28 * time.Hour,\n\t\t\tPersistentPeersMaxDialPeriod: config.P2P.PersistentPeersMaxDialPeriod,\n\t\t})\n\tpexReactor.SetLogger(logger.With(\"module\", \"pex\"))\n\tsw.AddReactor(\"PEX\", pexReactor)\n\treturn pexReactor\n}\n\n// NewNode returns a new, ready to go, Tendermint Node.\nfunc NewNode(config *cfg.Config,\n\tprivValidator types.PrivValidator,\n\tnodeKey *p2p.NodeKey,\n\tclientCreator proxy.ClientCreator,\n\tgenesisDocProvider GenesisDocProvider,\n\tdbProvider DBProvider,\n\tmetricsProvider MetricsProvider,\n\tlogger log.Logger,\n\toptions ...Option) (*Node, error) {\n\n\tblockStore, stateDB, err := initDBs(config, dbProvider)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tstate, genDoc, err := LoadStateFromDBOrGenesisDocProvider(stateDB, genesisDocProvider)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the proxyApp and establish connections to the ABCI app (consensus, mempool, query).\n\tproxyApp, err := createAndStartProxyAppConns(clientCreator, logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// EventBus and IndexerService must be started before the handshake because\n\t// we might need to index the txs of the replayed block as this might not have happened\n\t// when the node stopped last time (i.e. the node stopped after it saved the block\n\t// but before it indexed the txs, or, endblocker panicked)\n\teventBus, err := createAndStartEventBus(logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Transaction indexing\n\tindexerService, txIndexer, err := createAndStartIndexerService(config, dbProvider, eventBus, logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the handshaker, which calls RequestInfo, sets the AppVersion on the state,\n\t// and replays any blocks as necessary to sync tendermint with the app.\n\tconsensusLogger := logger.With(\"module\", \"consensus\")\n\tif err := doHandshake(stateDB, state, blockStore, genDoc, eventBus, proxyApp, consensusLogger); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Reload the state. It will have the Version.Consensus.App set by the\n\t// Handshake, and may have other modifications as well (ie. depending on\n\t// what happened during block replay).\n\tstate = sm.LoadState(stateDB)\n\n\t// If an address is provided, listen on the socket for a connection from an\n\t// external signing process.\n\tif config.PrivValidatorListenAddr != \"\" {\n\t\t// FIXME: we should start services inside OnStart\n\t\tprivValidator, err = createAndStartPrivValidatorSocketClient(config.PrivValidatorListenAddr, logger)\n\t\tif err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"error with private validator socket client\")\n\t\t}\n\t}\n\n\tpubKey, err := privValidator.GetPubKey()\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"can't get pubkey\")\n\t}\n\n\tlogNodeStartupInfo(state, pubKey, logger, consensusLogger)\n\n\t// Decide whether to fast-sync or not\n\t// We don't fast-sync when the only validator is us.\n\tfastSync := config.FastSyncMode && !onlyValidatorIsUs(state, pubKey)\n\n\tcsMetrics, p2pMetrics, memplMetrics, smMetrics := metricsProvider(genDoc.ChainID)\n\n\t// Make MempoolReactor\n\tmempoolReactor, mempool := createMempoolAndMempoolReactor(config, proxyApp, state, memplMetrics, logger)\n\n\t// Make Evidence Reactor\n\tevidenceReactor, evidencePool, err := createEvidenceReactor(config, dbProvider, stateDB, logger)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// make block executor for consensus and blockchain reactors to execute blocks\n\tblockExec := sm.NewBlockExecutor(\n\t\tstateDB,\n\t\tlogger.With(\"module\", \"state\"),\n\t\tproxyApp.Consensus(),\n\t\tmempool,\n\t\tevidencePool,\n\t\tsm.BlockExecutorWithMetrics(smMetrics),\n\t)\n\n\t// Make BlockchainReactor\n\tbcReactor, err := createBlockchainReactor(config, state, blockExec, blockStore, fastSync, logger)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"could not create blockchain reactor\")\n\t}\n\n\t// Make ConsensusReactor\n\tconsensusReactor, consensusState := createConsensusReactor(\n\t\tconfig, state, blockExec, blockStore, mempool, evidencePool,\n\t\tprivValidator, csMetrics, fastSync, eventBus, consensusLogger,\n\t)\n\n\tnodeInfo, err := makeNodeInfo(config, nodeKey, txIndexer, genDoc, state)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Setup Transport.\n\ttransport, peerFilters := createTransport(config, nodeInfo, nodeKey, proxyApp)\n\n\t// Setup Switch.\n\tp2pLogger := logger.With(\"module\", \"p2p\")\n\tsw := createSwitch(\n\t\tconfig, transport, p2pMetrics, peerFilters, mempoolReactor, bcReactor,\n\t\tconsensusReactor, evidenceReactor, nodeInfo, nodeKey, p2pLogger,\n\t)\n\n\terr = sw.AddPersistentPeers(splitAndTrimEmpty(config.P2P.PersistentPeers, \",\", \" \"))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"could not add peers from persistent_peers field\")\n\t}\n\n\terr = sw.AddUnconditionalPeerIDs(splitAndTrimEmpty(config.P2P.UnconditionalPeerIDs, \",\", \" \"))\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"could not add peer ids from unconditional_peer_ids field\")\n\t}\n\n\taddrBook, err := createAddrBookAndSetOnSwitch(config, sw, p2pLogger, nodeKey)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"could not create addrbook\")\n\t}\n\n\t// Optionally, start the pex reactor\n\t//\n\t// TODO:\n\t//\n\t// We need to set Seeds and PersistentPeers on the switch,\n\t// since it needs to be able to use these (and their DNS names)\n\t// even if the PEX is off. We can include the DNS name in the NetAddress,\n\t// but it would still be nice to have a clear list of the current \"PersistentPeers\"\n\t// somewhere that we can return with net_info.\n\t//\n\t// If PEX is on, it should handle dialing the seeds. Otherwise the switch does it.\n\t// Note we currently use the addrBook regardless at least for AddOurAddress\n\tvar pexReactor *pex.Reactor\n\tif config.P2P.PexReactor {\n\t\tpexReactor = createPEXReactorAndAddToSwitch(addrBook, config, sw, logger)\n\t}\n\n\tif config.ProfListenAddress != \"\" {\n\t\tgo func() {\n\t\t\tlogger.Error(\"Profile server\", \"err\", http.ListenAndServe(config.ProfListenAddress, nil))\n\t\t}()\n\t}\n\n\tnode := &Node{\n\t\tconfig:        config,\n\t\tgenesisDoc:    genDoc,\n\t\tprivValidator: privValidator,\n\n\t\ttransport: transport,\n\t\tsw:        sw,\n\t\taddrBook:  addrBook,\n\t\tnodeInfo:  nodeInfo,\n\t\tnodeKey:   nodeKey,\n\n\t\tstateDB:          stateDB,\n\t\tblockStore:       blockStore,\n\t\tbcReactor:        bcReactor,\n\t\tmempoolReactor:   mempoolReactor,\n\t\tmempool:          mempool,\n\t\tconsensusState:   consensusState,\n\t\tconsensusReactor: consensusReactor,\n\t\tpexReactor:       pexReactor,\n\t\tevidencePool:     evidencePool,\n\t\tproxyApp:         proxyApp,\n\t\ttxIndexer:        txIndexer,\n\t\tindexerService:   indexerService,\n\t\teventBus:         eventBus,\n\t}\n\tnode.BaseService = *service.NewBaseService(logger, \"Node\", node)\n\n\tfor _, option := range options {\n\t\toption(node)\n\t}\n\n\treturn node, nil\n}\n\n// OnStart starts the Node. It implements service.Service.\nfunc (n *Node) OnStart() error {\n\tnow := tmtime.Now()\n\tgenTime := n.genesisDoc.GenesisTime\n\tif genTime.After(now) {\n\t\tn.Logger.Info(\"Genesis time is in the future. Sleeping until then...\", \"genTime\", genTime)\n\t\ttime.Sleep(genTime.Sub(now))\n\t}\n\n\t// Add private IDs to addrbook to block those peers being added\n\tn.addrBook.AddPrivateIDs(splitAndTrimEmpty(n.config.P2P.PrivatePeerIDs, \",\", \" \"))\n\n\t// Start the RPC server before the P2P server\n\t// so we can eg. receive txs for the first block\n\tif n.config.RPC.ListenAddress != \"\" {\n\t\tlisteners, err := n.startRPC()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tn.rpcListeners = listeners\n\t}\n\n\tif n.config.Instrumentation.Prometheus &&\n\t\tn.config.Instrumentation.PrometheusListenAddr != \"\" {\n\t\tn.prometheusSrv = n.startPrometheusServer(n.config.Instrumentation.PrometheusListenAddr)\n\t}\n\n\t// Start the transport.\n\taddr, err := p2p.NewNetAddressString(p2p.IDAddressString(n.nodeKey.ID(), n.config.P2P.ListenAddress))\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := n.transport.Listen(*addr); err != nil {\n\t\treturn err\n\t}\n\n\tn.isListening = true\n\n\tif n.config.Mempool.WalEnabled() {\n\t\tn.mempool.InitWAL() // no need to have the mempool wal during tests\n\t}\n\n\t// Start the switch (the P2P server).\n\terr = n.sw.Start()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Always connect to persistent peers\n\terr = n.sw.DialPeersAsync(splitAndTrimEmpty(n.config.P2P.PersistentPeers, \",\", \" \"))\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"could not dial peers from persistent_peers field\")\n\t}\n\n\treturn nil\n}\n\n// OnStop stops the Node. It implements service.Service.\nfunc (n *Node) OnStop() {\n\tn.BaseService.OnStop()\n\n\tn.Logger.Info(\"Stopping Node\")\n\n\t// first stop the non-reactor services\n\tn.eventBus.Stop()\n\tn.indexerService.Stop()\n\n\t// now stop the reactors\n\tn.sw.Stop()\n\n\t// stop mempool WAL\n\tif n.config.Mempool.WalEnabled() {\n\t\tn.mempool.CloseWAL()\n\t}\n\n\tif err := n.transport.Close(); err != nil {\n\t\tn.Logger.Error(\"Error closing transport\", \"err\", err)\n\t}\n\n\tn.isListening = false\n\n\t// finally stop the listeners / external services\n\tfor _, l := range n.rpcListeners {\n\t\tn.Logger.Info(\"Closing rpc listener\", \"listener\", l)\n\t\tif err := l.Close(); err != nil {\n\t\t\tn.Logger.Error(\"Error closing listener\", \"listener\", l, \"err\", err)\n\t\t}\n\t}\n\n\tif pvsc, ok := n.privValidator.(service.Service); ok {\n\t\tpvsc.Stop()\n\t}\n\n\tif n.prometheusSrv != nil {\n\t\tif err := n.prometheusSrv.Shutdown(context.Background()); err != nil {\n\t\t\t// Error from closing listeners, or context timeout:\n\t\t\tn.Logger.Error(\"Prometheus HTTP server Shutdown\", \"err\", err)\n\t\t}\n\t}\n}\n\n// ConfigureRPC sets all variables in rpccore so they will serve\n// rpc calls from this node\nfunc (n *Node) ConfigureRPC() {\n\trpccore.SetStateDB(n.stateDB)\n\trpccore.SetBlockStore(n.blockStore)\n\trpccore.SetConsensusState(n.consensusState)\n\trpccore.SetMempool(n.mempool)\n\trpccore.SetEvidencePool(n.evidencePool)\n\trpccore.SetP2PPeers(n.sw)\n\trpccore.SetP2PTransport(n)\n\tpubKey, err := n.privValidator.GetPubKey()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\trpccore.SetPubKey(pubKey)\n\trpccore.SetGenesisDoc(n.genesisDoc)\n\trpccore.SetProxyAppQuery(n.proxyApp.Query())\n\trpccore.SetTxIndexer(n.txIndexer)\n\trpccore.SetConsensusReactor(n.consensusReactor)\n\trpccore.SetEventBus(n.eventBus)\n\trpccore.SetLogger(n.Logger.With(\"module\", \"rpc\"))\n\trpccore.SetConfig(*n.config.RPC)\n}\n\nfunc (n *Node) startRPC() ([]net.Listener, error) {\n\tn.ConfigureRPC()\n\tlistenAddrs := splitAndTrimEmpty(n.config.RPC.ListenAddress, \",\", \" \")\n\tcoreCodec := amino.NewCodec()\n\tctypes.RegisterAmino(coreCodec)\n\n\tif n.config.RPC.Unsafe {\n\t\trpccore.AddUnsafeRoutes()\n\t}\n\n\tconfig := rpcserver.DefaultConfig()\n\tconfig.MaxBodyBytes = n.config.RPC.MaxBodyBytes\n\tconfig.MaxHeaderBytes = n.config.RPC.MaxHeaderBytes\n\tconfig.MaxOpenConnections = n.config.RPC.MaxOpenConnections\n\t// If necessary adjust global WriteTimeout to ensure it's greater than\n\t// TimeoutBroadcastTxCommit.\n\t// See https://github.com/tendermint/tendermint/issues/3435\n\tif config.WriteTimeout <= n.config.RPC.TimeoutBroadcastTxCommit {\n\t\tconfig.WriteTimeout = n.config.RPC.TimeoutBroadcastTxCommit + 1*time.Second\n\t}\n\n\t// we may expose the rpc over both a unix and tcp socket\n\tlisteners := make([]net.Listener, len(listenAddrs))\n\tfor i, listenAddr := range listenAddrs {\n\t\tmux := http.NewServeMux()\n\t\trpcLogger := n.Logger.With(\"module\", \"rpc-server\")\n\t\twmLogger := rpcLogger.With(\"protocol\", \"websocket\")\n\t\twm := rpcserver.NewWebsocketManager(rpccore.Routes, coreCodec,\n\t\t\trpcserver.OnDisconnect(func(remoteAddr string) {\n\t\t\t\terr := n.eventBus.UnsubscribeAll(context.Background(), remoteAddr)\n\t\t\t\tif err != nil && err != tmpubsub.ErrSubscriptionNotFound {\n\t\t\t\t\twmLogger.Error(\"Failed to unsubscribe addr from events\", \"addr\", remoteAddr, \"err\", err)\n\t\t\t\t}\n\t\t\t}),\n\t\t\trpcserver.ReadLimit(config.MaxBodyBytes),\n\t\t)\n\t\twm.SetLogger(wmLogger)\n\t\tmux.HandleFunc(\"/websocket\", wm.WebsocketHandler)\n\t\trpcserver.RegisterRPCFuncs(mux, rpccore.Routes, coreCodec, rpcLogger)\n\t\tlistener, err := rpcserver.Listen(\n\t\t\tlistenAddr,\n\t\t\tconfig,\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tvar rootHandler http.Handler = mux\n\t\tif n.config.RPC.IsCorsEnabled() {\n\t\t\tcorsMiddleware := cors.New(cors.Options{\n\t\t\t\tAllowedOrigins: n.config.RPC.CORSAllowedOrigins,\n\t\t\t\tAllowedMethods: n.config.RPC.CORSAllowedMethods,\n\t\t\t\tAllowedHeaders: n.config.RPC.CORSAllowedHeaders,\n\t\t\t})\n\t\t\trootHandler = corsMiddleware.Handler(mux)\n\t\t}\n\t\tif n.config.RPC.IsTLSEnabled() {\n\t\t\tgo rpcserver.StartHTTPAndTLSServer(\n\t\t\t\tlistener,\n\t\t\t\trootHandler,\n\t\t\t\tn.config.RPC.CertFile(),\n\t\t\t\tn.config.RPC.KeyFile(),\n\t\t\t\trpcLogger,\n\t\t\t\tconfig,\n\t\t\t)\n\t\t} else {\n\t\t\tgo rpcserver.StartHTTPServer(\n\t\t\t\tlistener,\n\t\t\t\trootHandler,\n\t\t\t\trpcLogger,\n\t\t\t\tconfig,\n\t\t\t)\n\t\t}\n\n\t\tlisteners[i] = listener\n\t}\n\n\t// we expose a simplified api over grpc for convenience to app devs\n\tgrpcListenAddr := n.config.RPC.GRPCListenAddress\n\tif grpcListenAddr != \"\" {\n\t\tconfig := rpcserver.DefaultConfig()\n\t\tconfig.MaxBodyBytes = n.config.RPC.MaxBodyBytes\n\t\tconfig.MaxHeaderBytes = n.config.RPC.MaxHeaderBytes\n\t\t// NOTE: GRPCMaxOpenConnections is used, not MaxOpenConnections\n\t\tconfig.MaxOpenConnections = n.config.RPC.GRPCMaxOpenConnections\n\t\t// If necessary adjust global WriteTimeout to ensure it's greater than\n\t\t// TimeoutBroadcastTxCommit.\n\t\t// See https://github.com/tendermint/tendermint/issues/3435\n\t\tif config.WriteTimeout <= n.config.RPC.TimeoutBroadcastTxCommit {\n\t\t\tconfig.WriteTimeout = n.config.RPC.TimeoutBroadcastTxCommit + 1*time.Second\n\t\t}\n\t\tlistener, err := rpcserver.Listen(grpcListenAddr, config)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tgo grpccore.StartGRPCServer(listener)\n\t\tlisteners = append(listeners, listener)\n\t}\n\n\treturn listeners, nil\n}\n\n// startPrometheusServer starts a Prometheus HTTP server, listening for metrics\n// collectors on addr.\nfunc (n *Node) startPrometheusServer(addr string) *http.Server {\n\tsrv := &http.Server{\n\t\tAddr: addr,\n\t\tHandler: promhttp.InstrumentMetricHandler(\n\t\t\tprometheus.DefaultRegisterer, promhttp.HandlerFor(\n\t\t\t\tprometheus.DefaultGatherer,\n\t\t\t\tpromhttp.HandlerOpts{MaxRequestsInFlight: n.config.Instrumentation.MaxOpenConnections},\n\t\t\t),\n\t\t),\n\t}\n\tgo func() {\n\t\tif err := srv.ListenAndServe(); err != http.ErrServerClosed {\n\t\t\t// Error starting or closing listener:\n\t\t\tn.Logger.Error(\"Prometheus HTTP server ListenAndServe\", \"err\", err)\n\t\t}\n\t}()\n\treturn srv\n}\n\n// Switch returns the Node's Switch.\nfunc (n *Node) Switch() *p2p.Switch {\n\treturn n.sw\n}\n\n// BlockStore returns the Node's BlockStore.\nfunc (n *Node) BlockStore() *store.BlockStore {\n\treturn n.blockStore\n}\n\n// ConsensusState returns the Node's ConsensusState.\nfunc (n *Node) ConsensusState() *cs.State {\n\treturn n.consensusState\n}\n\n// ConsensusReactor returns the Node's ConsensusReactor.\nfunc (n *Node) ConsensusReactor() *cs.Reactor {\n\treturn n.consensusReactor\n}\n\n// MempoolReactor returns the Node's mempool reactor.\nfunc (n *Node) MempoolReactor() *mempl.Reactor {\n\treturn n.mempoolReactor\n}\n\n// Mempool returns the Node's mempool.\nfunc (n *Node) Mempool() mempl.Mempool {\n\treturn n.mempool\n}\n\n// PEXReactor returns the Node's PEXReactor. It returns nil if PEX is disabled.\nfunc (n *Node) PEXReactor() *pex.Reactor {\n\treturn n.pexReactor\n}\n\n// EvidencePool returns the Node's EvidencePool.\nfunc (n *Node) EvidencePool() *evidence.Pool {\n\treturn n.evidencePool\n}\n\n// EventBus returns the Node's EventBus.\nfunc (n *Node) EventBus() *types.EventBus {\n\treturn n.eventBus\n}\n\n// PrivValidator returns the Node's PrivValidator.\n// XXX: for convenience only!\nfunc (n *Node) PrivValidator() types.PrivValidator {\n\treturn n.privValidator\n}\n\n// GenesisDoc returns the Node's GenesisDoc.\nfunc (n *Node) GenesisDoc() *types.GenesisDoc {\n\treturn n.genesisDoc\n}\n\n// ProxyApp returns the Node's AppConns, representing its connections to the ABCI application.\nfunc (n *Node) ProxyApp() proxy.AppConns {\n\treturn n.proxyApp\n}\n\n// Config returns the Node's config.\nfunc (n *Node) Config() *cfg.Config {\n\treturn n.config\n}\n\n//------------------------------------------------------------------------------\n\nfunc (n *Node) Listeners() []string {\n\treturn []string{\n\t\tfmt.Sprintf(\"Listener(@%v)\", n.config.P2P.ExternalAddress),\n\t}\n}\n\nfunc (n *Node) IsListening() bool {\n\treturn n.isListening\n}\n\n// NodeInfo returns the Node's Info from the Switch.\nfunc (n *Node) NodeInfo() p2p.NodeInfo {\n\treturn n.nodeInfo\n}\n\nfunc makeNodeInfo(\n\tconfig *cfg.Config,\n\tnodeKey *p2p.NodeKey,\n\ttxIndexer txindex.TxIndexer,\n\tgenDoc *types.GenesisDoc,\n\tstate sm.State,\n) (p2p.NodeInfo, error) {\n\ttxIndexerStatus := \"on\"\n\tif _, ok := txIndexer.(*null.TxIndex); ok {\n\t\ttxIndexerStatus = \"off\"\n\t}\n\n\tvar bcChannel byte\n\tswitch config.FastSync.Version {\n\tcase \"v0\":\n\t\tbcChannel = bcv0.BlockchainChannel\n\tcase \"v1\":\n\t\tbcChannel = bcv1.BlockchainChannel\n\tcase \"v2\":\n\t\tbcChannel = bcv2.BlockchainChannel\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown fastsync version %s\", config.FastSync.Version)\n\t}\n\n\tnodeInfo := p2p.DefaultNodeInfo{\n\t\tProtocolVersion: p2p.NewProtocolVersion(\n\t\t\tversion.P2PProtocol, // global\n\t\t\tstate.Version.Consensus.Block,\n\t\t\tstate.Version.Consensus.App,\n\t\t),\n\t\tDefaultNodeID: nodeKey.ID(),\n\t\tNetwork:       genDoc.ChainID,\n\t\tVersion:       version.TMCoreSemVer,\n\t\tChannels: []byte{\n\t\t\tbcChannel,\n\t\t\tcs.StateChannel, cs.DataChannel, cs.VoteChannel, cs.VoteSetBitsChannel,\n\t\t\tmempl.MempoolChannel,\n\t\t\tevidence.EvidenceChannel,\n\t\t},\n\t\tMoniker: config.Moniker,\n\t\tOther: p2p.DefaultNodeInfoOther{\n\t\t\tTxIndex:    txIndexerStatus,\n\t\t\tRPCAddress: config.RPC.ListenAddress,\n\t\t},\n\t}\n\n\tif config.P2P.PexReactor {\n\t\tnodeInfo.Channels = append(nodeInfo.Channels, pex.PexChannel)\n\t}\n\n\tlAddr := config.P2P.ExternalAddress\n\n\tif lAddr == \"\" {\n\t\tlAddr = config.P2P.ListenAddress\n\t}\n\n\tnodeInfo.ListenAddr = lAddr\n\n\terr := nodeInfo.Validate()\n\treturn nodeInfo, err\n}\n\n//------------------------------------------------------------------------------\n\nvar (\n\tgenesisDocKey = []byte(\"genesisDoc\")\n)\n\n// LoadStateFromDBOrGenesisDocProvider attempts to load the state from the\n// database, or creates one using the given genesisDocProvider and persists the\n// result to the database. On success this also returns the genesis doc loaded\n// through the given provider.\nfunc LoadStateFromDBOrGenesisDocProvider(\n\tstateDB dbm.DB,\n\tgenesisDocProvider GenesisDocProvider,\n) (sm.State, *types.GenesisDoc, error) {\n\t// Get genesis doc\n\tgenDoc, err := loadGenesisDoc(stateDB)\n\tif err != nil {\n\t\tgenDoc, err = genesisDocProvider()\n\t\tif err != nil {\n\t\t\treturn sm.State{}, nil, err\n\t\t}\n\t\t// save genesis doc to prevent a certain class of user errors (e.g. when it\n\t\t// was changed, accidentally or not). Also good for audit trail.\n\t\tsaveGenesisDoc(stateDB, genDoc)\n\t}\n\tstate, err := sm.LoadStateFromDBOrGenesisDoc(stateDB, genDoc)\n\tif err != nil {\n\t\treturn sm.State{}, nil, err\n\t}\n\treturn state, genDoc, nil\n}\n\n// panics if failed to unmarshal bytes\nfunc loadGenesisDoc(db dbm.DB) (*types.GenesisDoc, error) {\n\tb, err := db.Get(genesisDocKey)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tif len(b) == 0 {\n\t\treturn nil, errors.New(\"genesis doc not found\")\n\t}\n\tvar genDoc *types.GenesisDoc\n\terr = cdc.UnmarshalJSON(b, &genDoc)\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"Failed to load genesis doc due to unmarshaling error: %v (bytes: %X)\", err, b))\n\t}\n\treturn genDoc, nil\n}\n\n// panics if failed to marshal the given genesis document\nfunc saveGenesisDoc(db dbm.DB, genDoc *types.GenesisDoc) {\n\tb, err := cdc.MarshalJSON(genDoc)\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"Failed to save genesis doc due to marshaling error: %v\", err))\n\t}\n\tdb.SetSync(genesisDocKey, b)\n}\n\nfunc createAndStartPrivValidatorSocketClient(\n\tlistenAddr string,\n\tlogger log.Logger,\n) (types.PrivValidator, error) {\n\tpve, err := privval.NewSignerListener(listenAddr, logger)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to start private validator\")\n\t}\n\n\tpvsc, err := privval.NewSignerClient(pve)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to start private validator\")\n\t}\n\n\treturn pvsc, nil\n}\n\n// splitAndTrimEmpty slices s into all subslices separated by sep and returns a\n// slice of the string s with all leading and trailing Unicode code points\n// contained in cutset removed. If sep is empty, SplitAndTrim splits after each\n// UTF-8 sequence. First part is equivalent to strings.SplitN with a count of\n// -1.  also filter out empty strings, only return non-empty strings.\nfunc splitAndTrimEmpty(s, sep, cutset string) []string {\n\tif s == \"\" {\n\t\treturn []string{}\n\t}\n\n\tspl := strings.Split(s, sep)\n\tnonEmptyStrings := make([]string, 0, len(spl))\n\tfor i := 0; i < len(spl); i++ {\n\t\telement := strings.Trim(spl[i], cutset)\n\t\tif element != \"\" {\n\t\t\tnonEmptyStrings = append(nonEmptyStrings, element)\n\t\t}\n\t}\n\treturn nonEmptyStrings\n}\n", "package p2p\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/net/netutil\"\n\n\t\"github.com/tendermint/tendermint/crypto\"\n\t\"github.com/tendermint/tendermint/p2p/conn\"\n)\n\nconst (\n\tdefaultDialTimeout      = time.Second\n\tdefaultFilterTimeout    = 5 * time.Second\n\tdefaultHandshakeTimeout = 3 * time.Second\n)\n\n// IPResolver is a behaviour subset of net.Resolver.\ntype IPResolver interface {\n\tLookupIPAddr(context.Context, string) ([]net.IPAddr, error)\n}\n\n// accept is the container to carry the upgraded connection and NodeInfo from an\n// asynchronously running routine to the Accept method.\ntype accept struct {\n\tnetAddr  *NetAddress\n\tconn     net.Conn\n\tnodeInfo NodeInfo\n\terr      error\n}\n\n// peerConfig is used to bundle data we need to fully setup a Peer with an\n// MConn, provided by the caller of Accept and Dial (currently the Switch). This\n// a temporary measure until reactor setup is less dynamic and we introduce the\n// concept of PeerBehaviour to communicate about significant Peer lifecycle\n// events.\n// TODO(xla): Refactor out with more static Reactor setup and PeerBehaviour.\ntype peerConfig struct {\n\tchDescs     []*conn.ChannelDescriptor\n\tonPeerError func(Peer, interface{})\n\toutbound    bool\n\t// isPersistent allows you to set a function, which, given socket address\n\t// (for outbound peers) OR self-reported address (for inbound peers), tells\n\t// if the peer is persistent or not.\n\tisPersistent func(*NetAddress) bool\n\treactorsByCh map[byte]Reactor\n\tmetrics      *Metrics\n}\n\n// Transport emits and connects to Peers. The implementation of Peer is left to\n// the transport. Each transport is also responsible to filter establishing\n// peers specific to its domain.\ntype Transport interface {\n\t// Listening address.\n\tNetAddress() NetAddress\n\n\t// Accept returns a newly connected Peer.\n\tAccept(peerConfig) (Peer, error)\n\n\t// Dial connects to the Peer for the address.\n\tDial(NetAddress, peerConfig) (Peer, error)\n\n\t// Cleanup any resources associated with Peer.\n\tCleanup(Peer)\n}\n\n// transportLifecycle bundles the methods for callers to control start and stop\n// behaviour.\ntype transportLifecycle interface {\n\tClose() error\n\tListen(NetAddress) error\n}\n\n// ConnFilterFunc to be implemented by filter hooks after a new connection has\n// been established. The set of exisiting connections is passed along together\n// with all resolved IPs for the new connection.\ntype ConnFilterFunc func(ConnSet, net.Conn, []net.IP) error\n\n// ConnDuplicateIPFilter resolves and keeps all ips for an incoming connection\n// and refuses new ones if they come from a known ip.\nfunc ConnDuplicateIPFilter() ConnFilterFunc {\n\treturn func(cs ConnSet, c net.Conn, ips []net.IP) error {\n\t\tfor _, ip := range ips {\n\t\t\tif cs.HasIP(ip) {\n\t\t\t\treturn ErrRejected{\n\t\t\t\t\tconn:        c,\n\t\t\t\t\terr:         fmt.Errorf(\"ip<%v> already connected\", ip),\n\t\t\t\t\tisDuplicate: true,\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t}\n}\n\n// MultiplexTransportOption sets an optional parameter on the\n// MultiplexTransport.\ntype MultiplexTransportOption func(*MultiplexTransport)\n\n// MultiplexTransportConnFilters sets the filters for rejection new connections.\nfunc MultiplexTransportConnFilters(\n\tfilters ...ConnFilterFunc,\n) MultiplexTransportOption {\n\treturn func(mt *MultiplexTransport) { mt.connFilters = filters }\n}\n\n// MultiplexTransportFilterTimeout sets the timeout waited for filter calls to\n// return.\nfunc MultiplexTransportFilterTimeout(\n\ttimeout time.Duration,\n) MultiplexTransportOption {\n\treturn func(mt *MultiplexTransport) { mt.filterTimeout = timeout }\n}\n\n// MultiplexTransportResolver sets the Resolver used for ip lokkups, defaults to\n// net.DefaultResolver.\nfunc MultiplexTransportResolver(resolver IPResolver) MultiplexTransportOption {\n\treturn func(mt *MultiplexTransport) { mt.resolver = resolver }\n}\n\n// MultiplexTransportMaxIncomingConnections sets the maximum number of\n// simultaneous connections (incoming). Default: 0 (unlimited)\nfunc MultiplexTransportMaxIncomingConnections(n int) MultiplexTransportOption {\n\treturn func(mt *MultiplexTransport) { mt.maxIncomingConnections = n }\n}\n\n// MultiplexTransport accepts and dials tcp connections and upgrades them to\n// multiplexed peers.\ntype MultiplexTransport struct {\n\tnetAddr                NetAddress\n\tlistener               net.Listener\n\tmaxIncomingConnections int // see MaxIncomingConnections\n\n\tacceptc chan accept\n\tclosec  chan struct{}\n\n\t// Lookup table for duplicate ip and id checks.\n\tconns       ConnSet\n\tconnFilters []ConnFilterFunc\n\n\tdialTimeout      time.Duration\n\tfilterTimeout    time.Duration\n\thandshakeTimeout time.Duration\n\tnodeInfo         NodeInfo\n\tnodeKey          NodeKey\n\tresolver         IPResolver\n\n\t// TODO(xla): This config is still needed as we parameterise peerConn and\n\t// peer currently. All relevant configuration should be refactored into options\n\t// with sane defaults.\n\tmConfig conn.MConnConfig\n}\n\n// Test multiplexTransport for interface completeness.\nvar _ Transport = (*MultiplexTransport)(nil)\nvar _ transportLifecycle = (*MultiplexTransport)(nil)\n\n// NewMultiplexTransport returns a tcp connected multiplexed peer.\nfunc NewMultiplexTransport(\n\tnodeInfo NodeInfo,\n\tnodeKey NodeKey,\n\tmConfig conn.MConnConfig,\n) *MultiplexTransport {\n\treturn &MultiplexTransport{\n\t\tacceptc:          make(chan accept),\n\t\tclosec:           make(chan struct{}),\n\t\tdialTimeout:      defaultDialTimeout,\n\t\tfilterTimeout:    defaultFilterTimeout,\n\t\thandshakeTimeout: defaultHandshakeTimeout,\n\t\tmConfig:          mConfig,\n\t\tnodeInfo:         nodeInfo,\n\t\tnodeKey:          nodeKey,\n\t\tconns:            NewConnSet(),\n\t\tresolver:         net.DefaultResolver,\n\t}\n}\n\n// NetAddress implements Transport.\nfunc (mt *MultiplexTransport) NetAddress() NetAddress {\n\treturn mt.netAddr\n}\n\n// Accept implements Transport.\nfunc (mt *MultiplexTransport) Accept(cfg peerConfig) (Peer, error) {\n\tselect {\n\t// This case should never have any side-effectful/blocking operations to\n\t// ensure that quality peers are ready to be used.\n\tcase a := <-mt.acceptc:\n\t\tif a.err != nil {\n\t\t\treturn nil, a.err\n\t\t}\n\n\t\tcfg.outbound = false\n\n\t\treturn mt.wrapPeer(a.conn, a.nodeInfo, cfg, a.netAddr), nil\n\tcase <-mt.closec:\n\t\treturn nil, ErrTransportClosed{}\n\t}\n}\n\n// Dial implements Transport.\nfunc (mt *MultiplexTransport) Dial(\n\taddr NetAddress,\n\tcfg peerConfig,\n) (Peer, error) {\n\tc, err := addr.DialTimeout(mt.dialTimeout)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// TODO(xla): Evaluate if we should apply filters if we explicitly dial.\n\tif err := mt.filterConn(c); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsecretConn, nodeInfo, err := mt.upgrade(c, &addr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tcfg.outbound = true\n\n\tp := mt.wrapPeer(secretConn, nodeInfo, cfg, &addr)\n\n\treturn p, nil\n}\n\n// Close implements transportLifecycle.\nfunc (mt *MultiplexTransport) Close() error {\n\tclose(mt.closec)\n\n\tif mt.listener != nil {\n\t\treturn mt.listener.Close()\n\t}\n\n\treturn nil\n}\n\n// Listen implements transportLifecycle.\nfunc (mt *MultiplexTransport) Listen(addr NetAddress) error {\n\tln, err := net.Listen(\"tcp\", addr.DialString())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif mt.maxIncomingConnections > 0 {\n\t\tln = netutil.LimitListener(ln, mt.maxIncomingConnections)\n\t}\n\n\tmt.netAddr = addr\n\tmt.listener = ln\n\n\tgo mt.acceptPeers()\n\n\treturn nil\n}\n\nfunc (mt *MultiplexTransport) acceptPeers() {\n\tfor {\n\t\tc, err := mt.listener.Accept()\n\t\tif err != nil {\n\t\t\t// If Close() has been called, silently exit.\n\t\t\tselect {\n\t\t\tcase _, ok := <-mt.closec:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\t// Transport is not closed\n\t\t\t}\n\n\t\t\tmt.acceptc <- accept{err: err}\n\t\t\treturn\n\t\t}\n\n\t\t// Connection upgrade and filtering should be asynchronous to avoid\n\t\t// Head-of-line blocking[0].\n\t\t// Reference:  https://github.com/tendermint/tendermint/issues/2047\n\t\t//\n\t\t// [0] https://en.wikipedia.org/wiki/Head-of-line_blocking\n\t\tgo func(c net.Conn) {\n\t\t\tdefer func() {\n\t\t\t\tif r := recover(); r != nil {\n\t\t\t\t\terr := ErrRejected{\n\t\t\t\t\t\tconn:          c,\n\t\t\t\t\t\terr:           errors.Errorf(\"recovered from panic: %v\", r),\n\t\t\t\t\t\tisAuthFailure: true,\n\t\t\t\t\t}\n\t\t\t\t\tselect {\n\t\t\t\t\tcase mt.acceptc <- accept{err: err}:\n\t\t\t\t\tcase <-mt.closec:\n\t\t\t\t\t\t// Give up if the transport was closed.\n\t\t\t\t\t\t_ = c.Close()\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\tvar (\n\t\t\t\tnodeInfo   NodeInfo\n\t\t\t\tsecretConn *conn.SecretConnection\n\t\t\t\tnetAddr    *NetAddress\n\t\t\t)\n\n\t\t\terr := mt.filterConn(c)\n\t\t\tif err == nil {\n\t\t\t\tsecretConn, nodeInfo, err = mt.upgrade(c, nil)\n\t\t\t\tif err == nil {\n\t\t\t\t\taddr := c.RemoteAddr()\n\t\t\t\t\tid := PubKeyToID(secretConn.RemotePubKey())\n\t\t\t\t\tnetAddr = NewNetAddress(id, addr)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase mt.acceptc <- accept{netAddr, secretConn, nodeInfo, err}:\n\t\t\t\t// Make the upgraded peer available.\n\t\t\tcase <-mt.closec:\n\t\t\t\t// Give up if the transport was closed.\n\t\t\t\t_ = c.Close()\n\t\t\t\treturn\n\t\t\t}\n\t\t}(c)\n\t}\n}\n\n// Cleanup removes the given address from the connections set and\n// closes the connection.\nfunc (mt *MultiplexTransport) Cleanup(p Peer) {\n\tmt.conns.RemoveAddr(p.RemoteAddr())\n\t_ = p.CloseConn()\n}\n\nfunc (mt *MultiplexTransport) cleanup(c net.Conn) error {\n\tmt.conns.Remove(c)\n\n\treturn c.Close()\n}\n\nfunc (mt *MultiplexTransport) filterConn(c net.Conn) (err error) {\n\tdefer func() {\n\t\tif err != nil {\n\t\t\t_ = c.Close()\n\t\t}\n\t}()\n\n\t// Reject if connection is already present.\n\tif mt.conns.Has(c) {\n\t\treturn ErrRejected{conn: c, isDuplicate: true}\n\t}\n\n\t// Resolve ips for incoming conn.\n\tips, err := resolveIPs(mt.resolver, c)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\terrc := make(chan error, len(mt.connFilters))\n\n\tfor _, f := range mt.connFilters {\n\t\tgo func(f ConnFilterFunc, c net.Conn, ips []net.IP, errc chan<- error) {\n\t\t\terrc <- f(mt.conns, c, ips)\n\t\t}(f, c, ips, errc)\n\t}\n\n\tfor i := 0; i < cap(errc); i++ {\n\t\tselect {\n\t\tcase err := <-errc:\n\t\t\tif err != nil {\n\t\t\t\treturn ErrRejected{conn: c, err: err, isFiltered: true}\n\t\t\t}\n\t\tcase <-time.After(mt.filterTimeout):\n\t\t\treturn ErrFilterTimeout{}\n\t\t}\n\n\t}\n\n\tmt.conns.Set(c, ips)\n\n\treturn nil\n}\n\nfunc (mt *MultiplexTransport) upgrade(\n\tc net.Conn,\n\tdialedAddr *NetAddress,\n) (secretConn *conn.SecretConnection, nodeInfo NodeInfo, err error) {\n\tdefer func() {\n\t\tif err != nil {\n\t\t\t_ = mt.cleanup(c)\n\t\t}\n\t}()\n\n\tsecretConn, err = upgradeSecretConn(c, mt.handshakeTimeout, mt.nodeKey.PrivKey)\n\tif err != nil {\n\t\treturn nil, nil, ErrRejected{\n\t\t\tconn:          c,\n\t\t\terr:           fmt.Errorf(\"secret conn failed: %v\", err),\n\t\t\tisAuthFailure: true,\n\t\t}\n\t}\n\n\t// For outgoing conns, ensure connection key matches dialed key.\n\tconnID := PubKeyToID(secretConn.RemotePubKey())\n\tif dialedAddr != nil {\n\t\tif dialedID := dialedAddr.ID; connID != dialedID {\n\t\t\treturn nil, nil, ErrRejected{\n\t\t\t\tconn: c,\n\t\t\t\tid:   connID,\n\t\t\t\terr: fmt.Errorf(\n\t\t\t\t\t\"conn.ID (%v) dialed ID (%v) mismatch\",\n\t\t\t\t\tconnID,\n\t\t\t\t\tdialedID,\n\t\t\t\t),\n\t\t\t\tisAuthFailure: true,\n\t\t\t}\n\t\t}\n\t}\n\n\tnodeInfo, err = handshake(secretConn, mt.handshakeTimeout, mt.nodeInfo)\n\tif err != nil {\n\t\treturn nil, nil, ErrRejected{\n\t\t\tconn:          c,\n\t\t\terr:           fmt.Errorf(\"handshake failed: %v\", err),\n\t\t\tisAuthFailure: true,\n\t\t}\n\t}\n\n\tif err := nodeInfo.Validate(); err != nil {\n\t\treturn nil, nil, ErrRejected{\n\t\t\tconn:              c,\n\t\t\terr:               err,\n\t\t\tisNodeInfoInvalid: true,\n\t\t}\n\t}\n\n\t// Ensure connection key matches self reported key.\n\tif connID != nodeInfo.ID() {\n\t\treturn nil, nil, ErrRejected{\n\t\t\tconn: c,\n\t\t\tid:   connID,\n\t\t\terr: fmt.Errorf(\n\t\t\t\t\"conn.ID (%v) NodeInfo.ID (%v) mismatch\",\n\t\t\t\tconnID,\n\t\t\t\tnodeInfo.ID(),\n\t\t\t),\n\t\t\tisAuthFailure: true,\n\t\t}\n\t}\n\n\t// Reject self.\n\tif mt.nodeInfo.ID() == nodeInfo.ID() {\n\t\treturn nil, nil, ErrRejected{\n\t\t\taddr:   *NewNetAddress(nodeInfo.ID(), c.RemoteAddr()),\n\t\t\tconn:   c,\n\t\t\tid:     nodeInfo.ID(),\n\t\t\tisSelf: true,\n\t\t}\n\t}\n\n\tif err := mt.nodeInfo.CompatibleWith(nodeInfo); err != nil {\n\t\treturn nil, nil, ErrRejected{\n\t\t\tconn:           c,\n\t\t\terr:            err,\n\t\t\tid:             nodeInfo.ID(),\n\t\t\tisIncompatible: true,\n\t\t}\n\t}\n\n\treturn secretConn, nodeInfo, nil\n}\n\nfunc (mt *MultiplexTransport) wrapPeer(\n\tc net.Conn,\n\tni NodeInfo,\n\tcfg peerConfig,\n\tsocketAddr *NetAddress,\n) Peer {\n\n\tpersistent := false\n\tif cfg.isPersistent != nil {\n\t\tif cfg.outbound {\n\t\t\tpersistent = cfg.isPersistent(socketAddr)\n\t\t} else {\n\t\t\tselfReportedAddr, err := ni.NetAddress()\n\t\t\tif err == nil {\n\t\t\t\tpersistent = cfg.isPersistent(selfReportedAddr)\n\t\t\t}\n\t\t}\n\t}\n\n\tpeerConn := newPeerConn(\n\t\tcfg.outbound,\n\t\tpersistent,\n\t\tc,\n\t\tsocketAddr,\n\t)\n\n\tp := newPeer(\n\t\tpeerConn,\n\t\tmt.mConfig,\n\t\tni,\n\t\tcfg.reactorsByCh,\n\t\tcfg.chDescs,\n\t\tcfg.onPeerError,\n\t\tPeerMetrics(cfg.metrics),\n\t)\n\n\treturn p\n}\n\nfunc handshake(\n\tc net.Conn,\n\ttimeout time.Duration,\n\tnodeInfo NodeInfo,\n) (NodeInfo, error) {\n\tif err := c.SetDeadline(time.Now().Add(timeout)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar (\n\t\terrc = make(chan error, 2)\n\n\t\tpeerNodeInfo DefaultNodeInfo\n\t\tourNodeInfo  = nodeInfo.(DefaultNodeInfo)\n\t)\n\n\tgo func(errc chan<- error, c net.Conn) {\n\t\t_, err := cdc.MarshalBinaryLengthPrefixedWriter(c, ourNodeInfo)\n\t\terrc <- err\n\t}(errc, c)\n\tgo func(errc chan<- error, c net.Conn) {\n\t\t_, err := cdc.UnmarshalBinaryLengthPrefixedReader(\n\t\t\tc,\n\t\t\t&peerNodeInfo,\n\t\t\tint64(MaxNodeInfoSize()),\n\t\t)\n\t\terrc <- err\n\t}(errc, c)\n\n\tfor i := 0; i < cap(errc); i++ {\n\t\terr := <-errc\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn peerNodeInfo, c.SetDeadline(time.Time{})\n}\n\nfunc upgradeSecretConn(\n\tc net.Conn,\n\ttimeout time.Duration,\n\tprivKey crypto.PrivKey,\n) (*conn.SecretConnection, error) {\n\tif err := c.SetDeadline(time.Now().Add(timeout)); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsc, err := conn.MakeSecretConnection(c, privKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn sc, sc.SetDeadline(time.Time{})\n}\n\nfunc resolveIPs(resolver IPResolver, c net.Conn) ([]net.IP, error) {\n\thost, _, err := net.SplitHostPort(c.RemoteAddr().String())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\taddrs, err := resolver.LookupIPAddr(context.Background(), host)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tips := []net.IP{}\n\n\tfor _, addr := range addrs {\n\t\tips = append(ips, addr.IP)\n\t}\n\n\treturn ips, nil\n}\n", "package p2p\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/tendermint/tendermint/crypto/ed25519\"\n\t\"github.com/tendermint/tendermint/p2p/conn\"\n)\n\nvar defaultNodeName = \"host_peer\"\n\nfunc emptyNodeInfo() NodeInfo {\n\treturn DefaultNodeInfo{}\n}\n\n// newMultiplexTransport returns a tcp connected multiplexed peer\n// using the default MConnConfig. It's a convenience function used\n// for testing.\nfunc newMultiplexTransport(\n\tnodeInfo NodeInfo,\n\tnodeKey NodeKey,\n) *MultiplexTransport {\n\treturn NewMultiplexTransport(\n\t\tnodeInfo, nodeKey, conn.DefaultMConnConfig(),\n\t)\n}\n\nfunc TestTransportMultiplexConnFilter(t *testing.T) {\n\tmt := newMultiplexTransport(\n\t\temptyNodeInfo(),\n\t\tNodeKey{\n\t\t\tPrivKey: ed25519.GenPrivKey(),\n\t\t},\n\t)\n\tid := mt.nodeKey.ID()\n\n\tMultiplexTransportConnFilters(\n\t\tfunc(_ ConnSet, _ net.Conn, _ []net.IP) error { return nil },\n\t\tfunc(_ ConnSet, _ net.Conn, _ []net.IP) error { return nil },\n\t\tfunc(_ ConnSet, _ net.Conn, _ []net.IP) error {\n\t\t\treturn fmt.Errorf(\"rejected\")\n\t\t},\n\t)(mt)\n\n\taddr, err := NewNetAddressString(IDAddressString(id, \"127.0.0.1:0\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := mt.Listen(*addr); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\taddr := NewNetAddress(id, mt.listener.Addr())\n\n\t\t_, err := addr.Dial()\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tt.Errorf(\"connection failed: %v\", err)\n\t}\n\n\t_, err = mt.Accept(peerConfig{})\n\tif err, ok := err.(ErrRejected); ok {\n\t\tif !err.IsFiltered() {\n\t\t\tt.Errorf(\"expected peer to be filtered\")\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected ErrRejected\")\n\t}\n}\n\nfunc TestTransportMultiplexConnFilterTimeout(t *testing.T) {\n\tmt := newMultiplexTransport(\n\t\temptyNodeInfo(),\n\t\tNodeKey{\n\t\t\tPrivKey: ed25519.GenPrivKey(),\n\t\t},\n\t)\n\tid := mt.nodeKey.ID()\n\n\tMultiplexTransportFilterTimeout(5 * time.Millisecond)(mt)\n\tMultiplexTransportConnFilters(\n\t\tfunc(_ ConnSet, _ net.Conn, _ []net.IP) error {\n\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\treturn nil\n\t\t},\n\t)(mt)\n\n\taddr, err := NewNetAddressString(IDAddressString(id, \"127.0.0.1:0\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := mt.Listen(*addr); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\taddr := NewNetAddress(id, mt.listener.Addr())\n\n\t\t_, err := addr.Dial()\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tt.Errorf(\"connection failed: %v\", err)\n\t}\n\n\t_, err = mt.Accept(peerConfig{})\n\tif _, ok := err.(ErrFilterTimeout); !ok {\n\t\tt.Errorf(\"expected ErrFilterTimeout\")\n\t}\n}\n\nfunc TestTransportMultiplexMaxIncomingConnections(t *testing.T) {\n\tmt := newMultiplexTransport(\n\t\temptyNodeInfo(),\n\t\tNodeKey{\n\t\t\tPrivKey: ed25519.GenPrivKey(),\n\t\t},\n\t)\n\tid := mt.nodeKey.ID()\n\n\tMultiplexTransportMaxIncomingConnections(0)(mt)\n\n\taddr, err := NewNetAddressString(IDAddressString(id, \"127.0.0.1:0\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := mt.Listen(*addr); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\taddr := NewNetAddress(id, mt.listener.Addr())\n\n\t\t_, err := addr.Dial()\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tt.Errorf(\"connection failed: %v\", err)\n\t}\n\n\t_, err = mt.Accept(peerConfig{})\n\tif err == nil || !strings.Contains(err.Error(), \"connection reset by peer\") {\n\t\tt.Errorf(\"expected connection reset by peer error, got %v\", err)\n\t}\n}\n\nfunc TestTransportMultiplexAcceptMultiple(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\tladdr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\tvar (\n\t\tseed     = rand.New(rand.NewSource(time.Now().UnixNano()))\n\t\tnDialers = seed.Intn(64) + 64\n\t\terrc     = make(chan error, nDialers)\n\t)\n\n\t// Setup dialers.\n\tfor i := 0; i < nDialers; i++ {\n\t\tgo testDialer(*laddr, errc)\n\t}\n\n\t// Catch connection errors.\n\tfor i := 0; i < nDialers; i++ {\n\t\tif err := <-errc; err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tps := []Peer{}\n\n\t// Accept all peers.\n\tfor i := 0; i < cap(errc); i++ {\n\t\tp, err := mt.Accept(peerConfig{})\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif err := p.Start(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tps = append(ps, p)\n\t}\n\n\tif have, want := len(ps), cap(errc); have != want {\n\t\tt.Errorf(\"have %v, want %v\", have, want)\n\t}\n\n\t// Stop all peers.\n\tfor _, p := range ps {\n\t\tif err := p.Stop(); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tif err := mt.Close(); err != nil {\n\t\tt.Errorf(\"close errored: %v\", err)\n\t}\n}\n\nfunc testDialer(dialAddr NetAddress, errc chan error) {\n\tvar (\n\t\tpv     = ed25519.GenPrivKey()\n\t\tdialer = newMultiplexTransport(\n\t\t\ttestNodeInfo(PubKeyToID(pv.PubKey()), defaultNodeName),\n\t\t\tNodeKey{\n\t\t\t\tPrivKey: pv,\n\t\t\t},\n\t\t)\n\t)\n\n\t_, err := dialer.Dial(dialAddr, peerConfig{})\n\tif err != nil {\n\t\terrc <- err\n\t\treturn\n\t}\n\n\t// Signal that the connection was established.\n\terrc <- nil\n}\n\nfunc TestTransportMultiplexAcceptNonBlocking(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\tvar (\n\t\tfastNodePV   = ed25519.GenPrivKey()\n\t\tfastNodeInfo = testNodeInfo(PubKeyToID(fastNodePV.PubKey()), \"fastnode\")\n\t\terrc         = make(chan error)\n\t\tfastc        = make(chan struct{})\n\t\tslowc        = make(chan struct{})\n\t)\n\n\t// Simulate slow Peer.\n\tgo func() {\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\tc, err := addr.Dial()\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(slowc)\n\n\t\tselect {\n\t\tcase <-fastc:\n\t\t\t// Fast peer connected.\n\t\tcase <-time.After(50 * time.Millisecond):\n\t\t\t// We error if the fast peer didn't succeed.\n\t\t\terrc <- fmt.Errorf(\"fast peer timed out\")\n\t\t}\n\n\t\tsc, err := upgradeSecretConn(c, 20*time.Millisecond, ed25519.GenPrivKey())\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\t_, err = handshake(sc, 20*time.Millisecond,\n\t\t\ttestNodeInfo(\n\t\t\t\tPubKeyToID(ed25519.GenPrivKey().PubKey()),\n\t\t\t\t\"slow_peer\",\n\t\t\t))\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\t}()\n\n\t// Simulate fast Peer.\n\tgo func() {\n\t\t<-slowc\n\n\t\tvar (\n\t\t\tdialer = newMultiplexTransport(\n\t\t\t\tfastNodeInfo,\n\t\t\t\tNodeKey{\n\t\t\t\t\tPrivKey: fastNodePV,\n\t\t\t\t},\n\t\t\t)\n\t\t)\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\t_, err := dialer.Dial(*addr, peerConfig{})\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t\tclose(fastc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tt.Errorf(\"connection failed: %v\", err)\n\t}\n\n\tp, err := mt.Accept(peerConfig{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif have, want := p.NodeInfo(), fastNodeInfo; !reflect.DeepEqual(have, want) {\n\t\tt.Errorf(\"have %v, want %v\", have, want)\n\t}\n}\n\nfunc TestTransportMultiplexValidateNodeInfo(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\tvar (\n\t\t\tpv     = ed25519.GenPrivKey()\n\t\t\tdialer = newMultiplexTransport(\n\t\t\t\ttestNodeInfo(PubKeyToID(pv.PubKey()), \"\"), // Should not be empty\n\t\t\t\tNodeKey{\n\t\t\t\t\tPrivKey: pv,\n\t\t\t\t},\n\t\t\t)\n\t\t)\n\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\t_, err := dialer.Dial(*addr, peerConfig{})\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tt.Errorf(\"connection failed: %v\", err)\n\t}\n\n\t_, err := mt.Accept(peerConfig{})\n\tif err, ok := err.(ErrRejected); ok {\n\t\tif !err.IsNodeInfoInvalid() {\n\t\t\tt.Errorf(\"expected NodeInfo to be invalid\")\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected ErrRejected\")\n\t}\n}\n\nfunc TestTransportMultiplexRejectMissmatchID(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\tdialer := newMultiplexTransport(\n\t\t\ttestNodeInfo(\n\t\t\t\tPubKeyToID(ed25519.GenPrivKey().PubKey()), \"dialer\",\n\t\t\t),\n\t\t\tNodeKey{\n\t\t\t\tPrivKey: ed25519.GenPrivKey(),\n\t\t\t},\n\t\t)\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\t_, err := dialer.Dial(*addr, peerConfig{})\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tt.Errorf(\"connection failed: %v\", err)\n\t}\n\n\t_, err := mt.Accept(peerConfig{})\n\tif err, ok := err.(ErrRejected); ok {\n\t\tif !err.IsAuthFailure() {\n\t\t\tt.Errorf(\"expected auth failure\")\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected ErrRejected\")\n\t}\n}\n\nfunc TestTransportMultiplexDialRejectWrongID(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\tvar (\n\t\tpv     = ed25519.GenPrivKey()\n\t\tdialer = newMultiplexTransport(\n\t\t\ttestNodeInfo(PubKeyToID(pv.PubKey()), \"\"), // Should not be empty\n\t\t\tNodeKey{\n\t\t\t\tPrivKey: pv,\n\t\t\t},\n\t\t)\n\t)\n\n\twrongID := PubKeyToID(ed25519.GenPrivKey().PubKey())\n\taddr := NewNetAddress(wrongID, mt.listener.Addr())\n\n\t_, err := dialer.Dial(*addr, peerConfig{})\n\tif err != nil {\n\t\tt.Logf(\"connection failed: %v\", err)\n\t\tif err, ok := err.(ErrRejected); ok {\n\t\t\tif !err.IsAuthFailure() {\n\t\t\t\tt.Errorf(\"expected auth failure\")\n\t\t\t}\n\t\t} else {\n\t\t\tt.Errorf(\"expected ErrRejected\")\n\t\t}\n\t}\n}\n\nfunc TestTransportMultiplexRejectIncompatible(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\tvar (\n\t\t\tpv     = ed25519.GenPrivKey()\n\t\t\tdialer = newMultiplexTransport(\n\t\t\t\ttestNodeInfoWithNetwork(PubKeyToID(pv.PubKey()), \"dialer\", \"incompatible-network\"),\n\t\t\t\tNodeKey{\n\t\t\t\t\tPrivKey: pv,\n\t\t\t\t},\n\t\t\t)\n\t\t)\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\t_, err := dialer.Dial(*addr, peerConfig{})\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\t_, err := mt.Accept(peerConfig{})\n\tif err, ok := err.(ErrRejected); ok {\n\t\tif !err.IsIncompatible() {\n\t\t\tt.Errorf(\"expected to reject incompatible\")\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected ErrRejected\")\n\t}\n}\n\nfunc TestTransportMultiplexRejectSelf(t *testing.T) {\n\tmt := testSetupMultiplexTransport(t)\n\n\terrc := make(chan error)\n\n\tgo func() {\n\t\taddr := NewNetAddress(mt.nodeKey.ID(), mt.listener.Addr())\n\n\t\t_, err := mt.Dial(*addr, peerConfig{})\n\t\tif err != nil {\n\t\t\terrc <- err\n\t\t\treturn\n\t\t}\n\n\t\tclose(errc)\n\t}()\n\n\tif err := <-errc; err != nil {\n\t\tif err, ok := err.(ErrRejected); ok {\n\t\t\tif !err.IsSelf() {\n\t\t\t\tt.Errorf(\"expected to reject self, got: %v\", err)\n\t\t\t}\n\t\t} else {\n\t\t\tt.Errorf(\"expected ErrRejected\")\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected connection failure\")\n\t}\n\n\t_, err := mt.Accept(peerConfig{})\n\tif err, ok := err.(ErrRejected); ok {\n\t\tif !err.IsSelf() {\n\t\t\tt.Errorf(\"expected to reject self, got: %v\", err)\n\t\t}\n\t} else {\n\t\tt.Errorf(\"expected ErrRejected\")\n\t}\n}\n\nfunc TestTransportConnDuplicateIPFilter(t *testing.T) {\n\tfilter := ConnDuplicateIPFilter()\n\n\tif err := filter(nil, &testTransportConn{}, nil); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar (\n\t\tc  = &testTransportConn{}\n\t\tcs = NewConnSet()\n\t)\n\n\tcs.Set(c, []net.IP{\n\t\t{10, 0, 10, 1},\n\t\t{10, 0, 10, 2},\n\t\t{10, 0, 10, 3},\n\t})\n\n\tif err := filter(cs, c, []net.IP{\n\t\t{10, 0, 10, 2},\n\t}); err == nil {\n\t\tt.Errorf(\"expected Peer to be rejected as duplicate\")\n\t}\n}\n\nfunc TestTransportHandshake(t *testing.T) {\n\tln, err := net.Listen(\"tcp\", \"127.0.0.1:0\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar (\n\t\tpeerPV       = ed25519.GenPrivKey()\n\t\tpeerNodeInfo = testNodeInfo(PubKeyToID(peerPV.PubKey()), defaultNodeName)\n\t)\n\n\tgo func() {\n\t\tc, err := net.Dial(ln.Addr().Network(), ln.Addr().String())\n\t\tif err != nil {\n\t\t\tt.Error(err)\n\t\t\treturn\n\t\t}\n\n\t\tgo func(c net.Conn) {\n\t\t\t_, err := cdc.MarshalBinaryLengthPrefixedWriter(c, peerNodeInfo.(DefaultNodeInfo))\n\t\t\tif err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t}\n\t\t}(c)\n\t\tgo func(c net.Conn) {\n\t\t\tvar ni DefaultNodeInfo\n\n\t\t\t_, err := cdc.UnmarshalBinaryLengthPrefixedReader(\n\t\t\t\tc,\n\t\t\t\t&ni,\n\t\t\t\tint64(MaxNodeInfoSize()),\n\t\t\t)\n\t\t\tif err != nil {\n\t\t\t\tt.Error(err)\n\t\t\t}\n\t\t}(c)\n\t}()\n\n\tc, err := ln.Accept()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tni, err := handshake(c, 20*time.Millisecond, emptyNodeInfo())\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif have, want := ni, peerNodeInfo; !reflect.DeepEqual(have, want) {\n\t\tt.Errorf(\"have %v, want %v\", have, want)\n\t}\n}\n\n// create listener\nfunc testSetupMultiplexTransport(t *testing.T) *MultiplexTransport {\n\tvar (\n\t\tpv = ed25519.GenPrivKey()\n\t\tid = PubKeyToID(pv.PubKey())\n\t\tmt = newMultiplexTransport(\n\t\t\ttestNodeInfo(\n\t\t\t\tid, \"transport\",\n\t\t\t),\n\t\t\tNodeKey{\n\t\t\t\tPrivKey: pv,\n\t\t\t},\n\t\t)\n\t)\n\n\taddr, err := NewNetAddressString(IDAddressString(id, \"127.0.0.1:0\"))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif err := mt.Listen(*addr); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\treturn mt\n}\n\ntype testTransportAddr struct{}\n\nfunc (a *testTransportAddr) Network() string { return \"tcp\" }\nfunc (a *testTransportAddr) String() string  { return \"test.local:1234\" }\n\ntype testTransportConn struct{}\n\nfunc (c *testTransportConn) Close() error {\n\treturn fmt.Errorf(\"close() not implemented\")\n}\n\nfunc (c *testTransportConn) LocalAddr() net.Addr {\n\treturn &testTransportAddr{}\n}\n\nfunc (c *testTransportConn) RemoteAddr() net.Addr {\n\treturn &testTransportAddr{}\n}\n\nfunc (c *testTransportConn) Read(_ []byte) (int, error) {\n\treturn -1, fmt.Errorf(\"read() not implemented\")\n}\n\nfunc (c *testTransportConn) SetDeadline(_ time.Time) error {\n\treturn fmt.Errorf(\"setDeadline() not implemented\")\n}\n\nfunc (c *testTransportConn) SetReadDeadline(_ time.Time) error {\n\treturn fmt.Errorf(\"setReadDeadline() not implemented\")\n}\n\nfunc (c *testTransportConn) SetWriteDeadline(_ time.Time) error {\n\treturn fmt.Errorf(\"setWriteDeadline() not implemented\")\n}\n\nfunc (c *testTransportConn) Write(_ []byte) (int, error) {\n\treturn -1, fmt.Errorf(\"write() not implemented\")\n}\n"], "filenames": ["node/node.go", "p2p/transport.go", "p2p/transport_test.go"], "buggy_code_start_loc": [473, 9, 7], "buggy_code_end_loc": [473, 240, 133], "fixing_code_start_loc": [474, 10, 8], "fixing_code_end_loc": [479, 253, 179], "type": "CWE-787", "message": "Tendermint before versions 0.33.3, 0.32.10, and 0.31.12 has a denial-of-service vulnerability. Tendermint does not limit the number of P2P connection requests. For each p2p connection, it allocates XXX bytes. Even though this memory is garbage collected once the connection is terminated (due to duplicate IP or reaching a maximum number of inbound peers), temporary memory spikes can lead to OOM (Out-Of-Memory) exceptions. Additionally, Tendermint does not reclaim activeID of a peer after it's removed in Mempool reactor. This does not happen all the time. It only happens when a connection fails (for any reason) before the Peer is created and added to all reactors. RemovePeer is therefore called before AddPeer, which leads to always growing memory (activeIDs map). The activeIDs map has a maximum size of 65535 and the node will panic if this map reaches the maximum. An attacker can create a lot of connection attempts (exploiting above denial of service), which ultimately will lead to the node panicking. These issues are patched in Tendermint 0.33.3 and 0.32.10.", "other": {"cve": {"id": "CVE-2020-5303", "sourceIdentifier": "security-advisories@github.com", "published": "2020-04-10T19:15:13.290", "lastModified": "2020-06-30T18:15:12.553", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Tendermint before versions 0.33.3, 0.32.10, and 0.31.12 has a denial-of-service vulnerability. Tendermint does not limit the number of P2P connection requests. For each p2p connection, it allocates XXX bytes. Even though this memory is garbage collected once the connection is terminated (due to duplicate IP or reaching a maximum number of inbound peers), temporary memory spikes can lead to OOM (Out-Of-Memory) exceptions. Additionally, Tendermint does not reclaim activeID of a peer after it's removed in Mempool reactor. This does not happen all the time. It only happens when a connection fails (for any reason) before the Peer is created and added to all reactors. RemovePeer is therefore called before AddPeer, which leads to always growing memory (activeIDs map). The activeIDs map has a maximum size of 65535 and the node will panic if this map reaches the maximum. An attacker can create a lot of connection attempts (exploiting above denial of service), which ultimately will lead to the node panicking. These issues are patched in Tendermint 0.33.3 and 0.32.10."}, {"lang": "es", "value": "Tendermint versiones anteriores a 0.33.3, 0.32.10 y 0.31.12, presenta una vulnerabilidad de denegaci\u00f3n de servicio. Tendermint no limita el n\u00famero de peticiones de conexi\u00f3n P2P. Para cada conexi\u00f3n p2p, asigna XXX bytes. Aun cuando esta memoria es de tipo garbage collected una vez que se termina la conexi\u00f3n (debido a IP duplicada o que alcanza un n\u00famero m\u00e1ximo de peers entrantes), los picos de memoria temporales pueden conllevar a excepciones OOM (Fuera de la Memoria). Adicionalmente, Tendermint no recupera el \"activeID\" de un peer despu\u00e9s de que es eliminado en el reactor Mempool. Esto no sucede todo el tiempo. Solo se presenta cuando se produce un fallo de conexi\u00f3n (por cualquier motivo) antes de que el Peer sea creado y agregado a todos los reactores. RemovePeer, por lo tanto, es llamado antes \"AddPeer\", lo que conlleva a una memoria creciente siempre (mapa \"activeIDs\"). El mapa activeIDs presenta un tama\u00f1o m\u00e1ximo de 65535 y el nodo entrar\u00e1 en p\u00e1nico si este mapa alcanza el m\u00e1ximo. Un atacante puede crear muchos intentos de conexi\u00f3n (explotar por encima de la denegaci\u00f3n de servicio), lo que finalmente conllevar\u00e1 al p\u00e1nico del nodo. Estos problemas est\u00e1n parcheados en Tendermint versiones 0.33.3 y 0.32.10"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:L", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 3.7, "baseSeverity": "LOW"}, "exploitabilityScore": 2.2, "impactScore": 1.4}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:H/PR:N/UI:R/S:U/C:N/I:N/A:L", "attackVector": "NETWORK", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 3.1, "baseSeverity": "LOW"}, "exploitabilityScore": 1.6, "impactScore": 1.4}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:M/Au:N/C:N/I:N/A:P", "accessVector": "NETWORK", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 4.3}, "baseSeverity": "MEDIUM", "exploitabilityScore": 8.6, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-787"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-789"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:tendermint:tendermint:*:*:*:*:*:*:*:*", "versionEndExcluding": "0.31.12", "matchCriteriaId": "9D919600-9671-435B-B13B-F3CCF335F789"}, {"vulnerable": true, "criteria": "cpe:2.3:a:tendermint:tendermint:*:*:*:*:*:*:*:*", "versionStartIncluding": "0.32.0", "versionEndExcluding": "0.32.10", "matchCriteriaId": "FE745543-7E8B-412B-B0ED-2496B163B7B4"}, {"vulnerable": true, "criteria": "cpe:2.3:a:tendermint:tendermint:*:*:*:*:*:*:*:*", "versionStartIncluding": "0.33.0", "versionEndExcluding": "0.33.3", "matchCriteriaId": "326E6655-BC1A-4C1C-B08C-5F5FA1B2ACB9"}]}]}], "references": [{"url": "https://github.com/tendermint/tendermint/commit/e2d6859afd7dba4cf97c7f7d412e7d8fc908d1cd", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tendermint/tendermint/security/advisories/GHSA-v24h-pjjv-mcp6", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://hackerone.com/reports/820317", "source": "security-advisories@github.com", "tags": ["Permissions Required", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tendermint/tendermint/commit/e2d6859afd7dba4cf97c7f7d412e7d8fc908d1cd"}}