{"buggy_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/bfloat16.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/inplace_ops_functor.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n\nnamespace tensorflow {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nnamespace functor {\n\ntemplate <typename Device, typename T>\nStatus DoParallelConcatUpdate(const Device& d, const Tensor& value, int32_t loc,\n                              Tensor* output) {\n  auto Tvalue = value.shaped<T, 2>({1, value.NumElements()});\n  auto Toutput = output->flat_outer_dims<T>();\n  auto nrows = Toutput.dimension(0);\n  auto r = (loc % nrows + nrows) % nrows;  // Guard index range.\n  Toutput.template chip<0>(r).device(d) = Tvalue.template chip<0>(0);\n  return Status::OK();\n}\n\ntemplate <>\nStatus DoParallelConcat(const CPUDevice& d, const Tensor& value, int32_t loc,\n                        Tensor* output) {\n  CHECK_EQ(value.dtype(), output->dtype());\n  switch (value.dtype()) {\n#define CASE(type)                  \\\n  case DataTypeToEnum<type>::value: \\\n    return DoParallelConcatUpdate<CPUDevice, type>(d, value, loc, output);\n    TF_CALL_POD_TYPES(CASE);\n    TF_CALL_tstring(CASE);\n    TF_CALL_variant(CASE);\n#undef CASE\n    default:\n      return errors::InvalidArgument(\"Unsupported data type: \",\n                                     DataTypeString(value.dtype()));\n  }\n}\n\n}  // end namespace functor\n\nnamespace {\n\ntemplate <typename Device>\nclass ParallelConcatUpdate : public OpKernel {\n public:\n  explicit ParallelConcatUpdate(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"loc\", &loc_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    auto value = ctx->input(0);\n    auto update = ctx->input(1);\n\n    OP_REQUIRES(\n        ctx, value.dims() == update.dims(),\n        errors::InvalidArgument(\"value and update shape doesn't match: \",\n                                value.shape().DebugString(), \" vs. \",\n                                update.shape().DebugString()));\n    for (int i = 1; i < value.dims(); ++i) {\n      OP_REQUIRES(\n          ctx, value.dim_size(i) == update.dim_size(i),\n          errors::InvalidArgument(\"value and update shape doesn't match \",\n                                  value.shape().DebugString(), \" vs. \",\n                                  update.shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, 1 == update.dim_size(0),\n                errors::InvalidArgument(\"update shape doesn't match: \",\n                                        update.shape().DebugString()));\n\n    Tensor output = value;  // This creates an alias intentionally.\n    const auto& d = ctx->eigen_device<Device>();\n    OP_REQUIRES_OK(\n        ctx, ::tensorflow::functor::DoParallelConcat(d, update, loc_, &output));\n    ctx->set_output(0, output);\n  }\n\n private:\n  int32 loc_;\n};\n\ntemplate <typename Device, typename T>\nclass ParallelConcatStart : public OpKernel {\n public:\n  explicit ParallelConcatStart(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"shape\", &shape_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    Tensor* out = nullptr;\n    // We do not know whether the output will be used on GPU. Setting it to be\n    // gpu-compatible for now.\n    AllocatorAttributes attr;\n    attr.set_gpu_compatible(true);\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, shape_, &out, attr));\n  }\n\n private:\n  TensorShape shape_;\n};\n\nclass FailureKernel : public OpKernel {\n public:\n  explicit FailureKernel(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx,\n                   errors::Internal(\"Found instance of parallel_stack which \"\n                                    \"could not be properly replaced.\"));\n  }\n\n  void Compute(OpKernelContext*) override {}\n};\n\n#define REGISTER(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"_ParallelConcatUpdate\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          ParallelConcatUpdate<CPUDevice>);\nTF_CALL_POD_STRING_TYPES(REGISTER)\n#undef REGISTER\n\n#define REGISTER_EMPTY(type)                                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"_ParallelConcatStart\")        \\\n                              .Device(DEVICE_CPU)             \\\n                              .TypeConstraint<type>(\"dtype\"), \\\n                          ParallelConcatStart<CPUDevice, type>)\n\nTF_CALL_POD_STRING_TYPES(REGISTER_EMPTY)\n#undef REGISTER_EMPTY\n\n#define REGISTER_PARALLEL_CONCAT(type)                                     \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"ParallelConcat\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      FailureKernel);\nTF_CALL_POD_STRING_TYPES(REGISTER_PARALLEL_CONCAT);\n#undef REGISTER_PARALLEL_CONCAT\n\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntypedef Eigen::GpuDevice GPUDevice;\n\n#define REGISTER_PARALLEL_CONCAT_START(type)                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"_ParallelConcatStart\")        \\\n                              .Device(DEVICE_GPU)             \\\n                              .TypeConstraint<type>(\"dtype\"), \\\n                          ParallelConcatStart<GPUDevice, type>);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_PARALLEL_CONCAT_START)\n#undef REGISTER_PARALLEL_CONCAT_START\n\n#define REGISTER_PARALLEL_CONCAT(type)                                     \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"ParallelConcat\").Device(DEVICE_GPU).TypeConstraint<type>(\"T\"), \\\n      FailureKernel);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_PARALLEL_CONCAT);\n#undef REGISTER_PARALLEL_CONCAT\n\n#define REGISTER(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"_ParallelConcatUpdate\")   \\\n                              .Device(DEVICE_GPU)         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          ParallelConcatUpdate<GPUDevice>);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER)\n#undef REGISTER\n\n// Register versions that operate on int32 data on the CPU even though the op\n// has been placed on the GPU\n\nREGISTER_KERNEL_BUILDER(Name(\"_ParallelConcatUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"value\")\n                            .HostMemory(\"update\")\n                            .HostMemory(\"output\")\n                            .TypeConstraint<int32>(\"T\"),\n                        ParallelConcatUpdate<CPUDevice>);\n#endif\n\nclass InplaceOpBase : public OpKernel {\n public:\n  explicit InplaceOpBase(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    auto x = ctx->input(0);\n    auto i = ctx->input(1);\n    auto v = ctx->input(2);\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(i.shape()),\n                errors::InvalidArgument(\"i must be a vector. \",\n                                        i.shape().DebugString()));\n    OP_REQUIRES(ctx, x.dims() == v.dims(),\n                errors::InvalidArgument(\n                    \"x and v shape doesn't match (ranks differ): \",\n                    x.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n    for (int i = 1; i < x.dims(); ++i) {\n      OP_REQUIRES(\n          ctx, x.dim_size(i) == v.dim_size(i),\n          errors::InvalidArgument(\"x and v shape doesn't match at index \", i,\n                                  \" : \", x.shape().DebugString(), \" vs. \",\n                                  v.shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, i.dim_size(0) == v.dim_size(0),\n                errors::InvalidArgument(\n                    \"i and x shape doesn't match at index 0: \",\n                    i.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n\n    Tensor y = x;  // This creates an alias intentionally.\n    // Skip processing if tensors are empty.\n    if (x.NumElements() > 0 && v.NumElements() > 0) {\n      OP_REQUIRES_OK(ctx, DoCompute(ctx, i, v, &y));\n    }\n    ctx->set_output(0, y);\n  }\n\n protected:\n  virtual Status DoCompute(OpKernelContext* ctx, const Tensor& i,\n                           const Tensor& v, Tensor* y) = 0;\n};\n\n}  // end namespace\n\nnamespace functor {\n\ntemplate <typename T>\nvoid DoInplaceOp(const CPUDevice& d, InplaceOpType op, const Tensor& i,\n                 const Tensor& v, Tensor* y) {\n  auto Ti = i.flat<int32>();\n  auto Tv = v.flat_outer_dims<T>();\n  auto Ty = y->flat_outer_dims<T>();\n  auto nrows = Ty.dimension(0);\n  for (int64_t j = 0; j < Ti.size(); ++j) {\n    auto r = (Ti(j) % nrows + nrows) % nrows;  // Guard index range.\n    switch (op) {\n      case I_UPDATE:\n        Ty.template chip<0>(r).device(d) = Tv.template chip<0>(j);\n        break;\n      case I_ADD:\n        Ty.template chip<0>(r).device(d) += Tv.template chip<0>(j);\n        break;\n      case I_SUB:\n        Ty.template chip<0>(r).device(d) -= Tv.template chip<0>(j);\n        break;\n    }\n  }\n}\n\n// String type only supports inplace update.\nvoid DoInplaceStringUpdateOp(const CPUDevice& d, const Tensor& i,\n                             const Tensor& v, Tensor* y) {\n  auto Ti = i.flat<int32>();\n  auto Tv = v.flat_outer_dims<tstring>();\n  auto Ty = y->flat_outer_dims<tstring>();\n  auto nrows = Ty.dimension(0);\n  for (int64_t j = 0; j < Ti.size(); ++j) {\n    auto r = (Ti(j) % nrows + nrows) % nrows;  // Guard index range.\n    Ty.template chip<0>(r).device(d) = Tv.template chip<0>(j);\n  }\n}\n\ntemplate <>\nStatus DoInplace(const CPUDevice& device, InplaceOpType op, const Tensor& i,\n                 const Tensor& v, Tensor* y) {\n  CHECK_EQ(v.dtype(), y->dtype());\n  if (op == I_UPDATE) {\n    if (v.dtype() == DT_STRING) {\n      DoInplaceStringUpdateOp(device, i, v, y);\n      return Status::OK();\n    } else if (v.dtype() == DT_BOOL) {\n      DoInplaceOp<bool>(device, op, i, v, y);\n      return Status::OK();\n    }\n  }\n  switch (v.dtype()) {\n#define CASE(type)                          \\\n  case DataTypeToEnum<type>::value:         \\\n    DoInplaceOp<type>(device, op, i, v, y); \\\n    break;\n    TF_CALL_NUMBER_TYPES(CASE);\n#undef CASE\n    default:\n      return errors::InvalidArgument(\"Unsupported data type: \",\n                                     DataTypeString(v.dtype()));\n  }\n  return Status::OK();\n}\n\n}  // end namespace functor\n\nnamespace {\ntemplate <typename Device, functor::InplaceOpType op>\nclass InplaceOp : public InplaceOpBase {\n public:\n  explicit InplaceOp(OpKernelConstruction* ctx) : InplaceOpBase(ctx) {}\n\n protected:\n  Status DoCompute(OpKernelContext* ctx, const Tensor& i, const Tensor& v,\n                   Tensor* y) override {\n    const auto& d = ctx->eigen_device<Device>();\n    return ::tensorflow::functor::DoInplace(d, op, i, v, y);\n  }\n};\n\nclass CopyOpBase : public OpKernel {\n public:\n  explicit CopyOpBase(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    auto x = ctx->input(0);\n    Tensor* y;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, x.shape(), &y));\n    OP_REQUIRES_OK(ctx, DoCompute(ctx, x, y));\n  }\n\n protected:\n  virtual Status DoCompute(OpKernelContext* ctx, const Tensor& x,\n                           Tensor* y) = 0;\n};\n\ntemplate <typename Device>\nclass CopyOp : public CopyOpBase {\n public:\n  explicit CopyOp(OpKernelConstruction* ctx) : CopyOpBase(ctx) {}\n\n protected:\n  Status DoCompute(OpKernelContext* ctx, const Tensor& x, Tensor* y) override {\n    const auto& d = ctx->eigen_device<Device>();\n    return ::tensorflow::functor::DoCopy(d, x, y);\n  }\n};\n\n}  // end namespace\n\nnamespace functor {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <>\nStatus DoCopy(const CPUDevice& device, const Tensor& x, Tensor* y) {\n  CHECK_EQ(x.dtype(), y->dtype());\n  switch (x.dtype()) {\n#define CASE(type)                                   \\\n  case DataTypeToEnum<type>::value:                  \\\n    y->flat<type>().device(device) = x.flat<type>(); \\\n    break;\n\n    TF_CALL_NUMBER_TYPES(CASE);\n    TF_CALL_bool(CASE);\n    TF_CALL_tstring(CASE);\n#undef CASE\n    default:\n      return errors::InvalidArgument(\"Unsupported data type: \",\n                                     DataTypeString(x.dtype()));\n  }\n  return Status::OK();\n}\n\n}  // end namespace functor\n\nnamespace {\ntemplate <typename Device, typename T>\nclass EmptyOp : public OpKernel {\n public:\n  explicit EmptyOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"init\", &init_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& shape = ctx->input(0);\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(shape.shape()),\n        errors::InvalidArgument(\"shape must be a vector of int32, got shape \",\n                                shape.shape().DebugString()));\n    auto dims = shape.flat<int32>();\n    TensorShape out_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            reinterpret_cast<const int32*>(dims.data()),\n                            dims.size(), &out_shape));\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));\n\n    if (init_) {\n      functor::SetZeroFunctor<Device, T>()(ctx->eigen_device<Device>(),\n                                           out->flat<T>());\n    }\n  }\n\n private:\n  bool init_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"InplaceUpdate\").Device(DEVICE_CPU),\n                        InplaceOp<CPUDevice, functor::I_UPDATE>);\nREGISTER_KERNEL_BUILDER(Name(\"InplaceAdd\").Device(DEVICE_CPU),\n                        InplaceOp<CPUDevice, functor::I_ADD>);\nREGISTER_KERNEL_BUILDER(Name(\"InplaceSub\").Device(DEVICE_CPU),\n                        InplaceOp<CPUDevice, functor::I_SUB>);\nREGISTER_KERNEL_BUILDER(Name(\"DeepCopy\").Device(DEVICE_CPU), CopyOp<CPUDevice>);\n\n#define REGISTER_EMPTY(type, dev)                             \\\n  REGISTER_KERNEL_BUILDER(Name(\"Empty\")                       \\\n                              .Device(DEVICE_##dev)           \\\n                              .HostMemory(\"shape\")            \\\n                              .TypeConstraint<type>(\"dtype\"), \\\n                          EmptyOp<dev##Device, type>)\n\nREGISTER_EMPTY(float, CPU)\nREGISTER_EMPTY(bfloat16, CPU)\nREGISTER_EMPTY(double, CPU)\nREGISTER_EMPTY(Eigen::half, CPU)\nREGISTER_EMPTY(tstring, CPU)\nREGISTER_EMPTY(int32, CPU)\nREGISTER_EMPTY(int64_t, CPU)\nREGISTER_EMPTY(bool, CPU)\nREGISTER_EMPTY(uint8, CPU)\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntypedef Eigen::GpuDevice GPUDevice;\n\n#define REGISTER(TYPE)                                                    \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"InplaceUpdate\").Device(DEVICE_GPU).TypeConstraint<TYPE>(\"T\"), \\\n      InplaceOp<GPUDevice, functor::I_UPDATE>);                           \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"InplaceAdd\").Device(DEVICE_GPU).TypeConstraint<TYPE>(\"T\"),    \\\n      InplaceOp<GPUDevice, functor::I_ADD>);                              \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"InplaceSub\").Device(DEVICE_GPU).TypeConstraint<TYPE>(\"T\"),    \\\n      InplaceOp<GPUDevice, functor::I_SUB>);                              \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"DeepCopy\").Device(DEVICE_GPU).TypeConstraint<TYPE>(\"T\"),      \\\n      CopyOp<GPUDevice>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"InplaceUpdate\").Device(DEVICE_GPU).TypeConstraint<bool>(\"T\"),\n    InplaceOp<GPUDevice, functor::I_UPDATE>);\nREGISTER(float);\nREGISTER(double);\nREGISTER(Eigen::half);\nREGISTER(int64_t);\n\nREGISTER_KERNEL_BUILDER(Name(\"InplaceUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"x\")\n                            .HostMemory(\"i\")\n                            .HostMemory(\"v\")\n                            .HostMemory(\"y\")\n                            .TypeConstraint<int32>(\"T\"),\n                        InplaceOp<CPUDevice, functor::I_UPDATE>);\nREGISTER_KERNEL_BUILDER(Name(\"InplaceAdd\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"x\")\n                            .HostMemory(\"i\")\n                            .HostMemory(\"v\")\n                            .HostMemory(\"y\")\n                            .TypeConstraint<int32>(\"T\"),\n                        InplaceOp<CPUDevice, functor::I_ADD>);\nREGISTER_KERNEL_BUILDER(Name(\"InplaceSub\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"x\")\n                            .HostMemory(\"i\")\n                            .HostMemory(\"v\")\n                            .HostMemory(\"y\")\n                            .TypeConstraint<int32>(\"T\"),\n                        InplaceOp<CPUDevice, functor::I_SUB>);\n\nREGISTER_KERNEL_BUILDER(Name(\"DeepCopy\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"x\")\n                            .HostMemory(\"y\")\n                            .TypeConstraint<int32>(\"T\"),\n                        CopyOp<CPUDevice>);\nREGISTER_EMPTY(float, GPU);\nREGISTER_EMPTY(double, GPU);\nREGISTER_EMPTY(Eigen::half, GPU);\nREGISTER_EMPTY(int64_t, GPU);\nREGISTER_EMPTY(int32, GPU);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // end namespace\n}  // end namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for Stack and ParallelStack Ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.platform import test\n\n\ndef np_split_squeeze(array, axis):\n  axis_len = array.shape[axis]\n  return [\n      np.squeeze(\n          arr, axis=(axis,)) for arr in np.split(\n              array, axis_len, axis=axis)\n  ]\n\n\nclass StackOpTest(test.TestCase):\n\n  def randn(self, shape, dtype):\n    data = np.random.randn(*shape)\n    if dtype == np.bool_:\n      return data < 0  # Naive casting yields True with P(1)!\n    else:\n      return data.astype(dtype)\n\n  def testSimple(self):\n    np.random.seed(7)\n    for shape in (2,), (3,), (2, 3), (3, 2), (8, 2, 10):\n      rank = len(shape)\n      for axis in range(-rank, rank):\n        for dtype in [np.bool_, np.float32, np.int32, np.int64]:\n          data = self.randn(shape, dtype)\n          xs = np_split_squeeze(data, axis)\n          # Stack back into a single tensorflow tensor\n          with self.subTest(shape=shape, axis=axis, dtype=dtype):\n            c = array_ops.stack(xs, axis=axis)\n            self.assertAllEqual(c, data)\n\n  def testSimpleParallelCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      np.random.seed(7)\n      with test_util.device(use_gpu=False):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (100, 24, 24, 3):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            xs = list(map(constant_op.constant, data))\n            c = array_ops.parallel_stack(xs)\n            self.assertAllEqual(c, data)\n\n  def testSimpleParallelGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      with test_util.device(use_gpu=True):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (100, 24, 24, 3):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            xs = list(map(constant_op.constant, data))\n            c = array_ops.parallel_stack(xs)\n            self.assertAllEqual(c, data)\n\n  def testConst(self):\n    np.random.seed(7)\n    with test_util.use_gpu():\n      # Verify that shape induction works with shapes produced via const stack\n      a = constant_op.constant([1, 2, 3, 4, 5, 6])\n      b = array_ops.reshape(a, array_ops.stack([2, 3]))\n      self.assertAllEqual(b.get_shape(), [2, 3])\n\n      # Check on a variety of shapes and types\n      for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (8, 2, 10):\n        for dtype in [np.bool_, np.float32, np.int16, np.int32, np.int64]:\n          with self.subTest(shape=shape, dtype=dtype):\n            data = self.randn(shape, dtype)\n            # Stack back into a single tensorflow tensor directly using np array\n            c = array_ops.stack(data)\n            if not context.executing_eagerly():\n              # This is implemented via a Const:\n              self.assertEqual(c.op.type, \"Const\")\n            self.assertAllEqual(c, data)\n\n            # Python lists also work for 1-D case:\n            if len(shape) == 1:\n              data_list = list(data)\n              cl = array_ops.stack(data_list)\n              if not context.executing_eagerly():\n                self.assertEqual(cl.op.type, \"Const\")\n              self.assertAllEqual(cl, data)\n\n  def testConstParallelCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      np.random.seed(7)\n      with test_util.device(use_gpu=False):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (8, 2, 10):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            if len(shape) == 1:\n              data_list = list(data)\n              cl = array_ops.parallel_stack(data_list)\n              self.assertAllEqual(cl, data)\n\n            data = self.randn(shape, np.float32)\n            c = array_ops.parallel_stack(data)\n            self.assertAllEqual(c, data)\n\n  def testConstParallelGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      np.random.seed(7)\n      with test_util.device(use_gpu=True):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            if len(shape) == 1:\n              data_list = list(data)\n              cl = array_ops.parallel_stack(data_list)\n              self.assertAllEqual(cl, data)\n\n            data = self.randn(shape, np.float32)\n            c = array_ops.parallel_stack(data)\n            self.assertAllEqual(c, data)\n\n  def testGradientsAxis0(self):\n    np.random.seed(7)\n    for shape in (2,), (3,), (2, 3), (3, 2), (8, 2, 10):\n      data = np.random.randn(*shape)\n      with self.subTest(shape=shape):\n        with self.cached_session():\n\n          def func(*xs):\n            return array_ops.stack(xs)\n          # TODO(irving): Remove list() once we handle maps correctly\n          xs = list(map(constant_op.constant, data))\n          theoretical, numerical = gradient_checker_v2.compute_gradient(\n              func, xs)\n          self.assertAllClose(theoretical, numerical)\n\n  def testGradientsAxis1(self):\n    np.random.seed(7)\n    for shape in (2, 3), (3, 2), (8, 2, 10):\n      data = np.random.randn(*shape)\n      out_shape = list(shape[1:])\n      out_shape.insert(1, shape[0])\n      with self.subTest(shape=shape):\n        with self.cached_session():\n\n          def func(*inp):\n            return array_ops.stack(inp, axis=1)\n          # TODO(irving): Remove list() once we handle maps correctly\n          xs = list(map(constant_op.constant, data))\n          theoretical, numerical = gradient_checker_v2.compute_gradient(\n              func, xs)\n          self.assertAllClose(theoretical, numerical)\n\n  def testZeroSizeCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      # Verify that stack doesn't crash for zero size inputs\n      with test_util.device(use_gpu=False):\n        for shape in (0,), (3, 0), (0, 3):\n          with self.subTest(shape=shape):\n            x = np.zeros((2,) + shape).astype(np.int32)\n            p = self.evaluate(array_ops.stack(list(x)))\n            self.assertAllEqual(p, x)\n\n            p = self.evaluate(array_ops.parallel_stack(list(x)))\n            self.assertAllEqual(p, x)\n\n  def testZeroSizeGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      # Verify that stack doesn't crash for zero size inputs\n      with test_util.device(use_gpu=True):\n        for shape in (0,), (3, 0), (0, 3):\n          with self.subTest(shape=shape):\n            x = np.zeros((2,) + shape).astype(np.int32)\n            p = self.evaluate(array_ops.stack(list(x)))\n            self.assertAllEqual(p, x)\n\n            p = self.evaluate(array_ops.parallel_stack(list(x)))\n            self.assertAllEqual(p, x)\n\n  def testAxis0DefaultCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      with test_util.device(use_gpu=False):\n        t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n        stacked = self.evaluate(array_ops.stack(t))\n        parallel_stacked = self.evaluate(array_ops.parallel_stack(t))\n\n      expected = np.array([[1, 2, 3], [4, 5, 6]])\n      self.assertAllEqual(stacked, expected)\n      self.assertAllEqual(parallel_stacked, expected)\n\n  def testAxis0DefaultGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      with test_util.device(use_gpu=True):\n        t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n        stacked = self.evaluate(array_ops.stack(t))\n        parallel_stacked = self.evaluate(array_ops.parallel_stack(t))\n\n      expected = np.array([[1, 2, 3], [4, 5, 6]])\n      self.assertAllEqual(stacked, expected)\n      self.assertAllEqual(parallel_stacked, expected)\n\n  def testAgainstNumpy(self):\n    # For 1 to 5 dimensions.\n    for shape in (3,), (2, 2, 3), (4, 1, 2, 2), (8, 2, 10):\n      rank = len(shape)\n      expected = self.randn(shape, np.float32)\n      for dtype in [np.bool_, np.float32, np.int32, np.int64]:\n        # For all the possible axis to split it, including negative indices.\n        for axis in range(-rank, rank):\n          test_arrays = np_split_squeeze(expected, axis)\n\n          with self.cached_session():\n            with self.subTest(shape=shape, dtype=dtype, axis=axis):\n              actual_pack = array_ops.stack(test_arrays, axis=axis)\n              self.assertEqual(expected.shape, actual_pack.get_shape())\n              actual_pack = self.evaluate(actual_pack)\n\n              actual_stack = array_ops.stack(test_arrays, axis=axis)\n              self.assertEqual(expected.shape, actual_stack.get_shape())\n              actual_stack = self.evaluate(actual_stack)\n\n              self.assertNDArrayNear(expected, actual_stack, 1e-6)\n\n  def testDimOutOfRange(self):\n    t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n    with self.assertRaisesRegex(ValueError,\n                                r\"Argument `axis` = 2 not in range \\[-2, 2\\)\"):\n      array_ops.stack(t, axis=2)\n\n  def testDimOutOfNegativeRange(self):\n    t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n    with self.assertRaisesRegex(ValueError,\n                                r\"Argument `axis` = -3 not in range \\[-2, 2\\)\"):\n      array_ops.stack(t, axis=-3)\n\n  def testComplex(self):\n    np.random.seed(7)\n    with self.session():\n      for shape in (2,), (3,), (2, 3), (3, 2), (8, 2, 10):\n        for dtype in [np.complex64, np.complex128]:\n          with self.subTest(shape=shape, dtype=dtype):\n            data = self.randn(shape, dtype)\n            xs = list(map(constant_op.constant, data))\n            c = array_ops.stack(xs)\n            self.assertAllEqual(self.evaluate(c), data)\n\n\nclass AutomaticStackingTest(test.TestCase):\n\n  def testSimple(self):\n    self.assertAllEqual([1, 0, 2],\n                        ops.convert_to_tensor([1, constant_op.constant(0), 2]))\n    self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],\n                        ops.convert_to_tensor([[0, 0, 0],\n                                               [0,\n                                                constant_op.constant(1), 0],\n                                               [0, 0, 0]]))\n    self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],\n                        ops.convert_to_tensor([[0, 0, 0],\n                                               constant_op.constant([0, 1, 0]),\n                                               [0, 0, 0]]))\n    self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],\n                        ops.convert_to_tensor([\n                            constant_op.constant([0, 0, 0]),\n                            constant_op.constant([0, 1, 0]),\n                            constant_op.constant([0, 0, 0])\n                        ]))\n\n  def testWithNDArray(self):\n    with self.session():\n      result = ops.convert_to_tensor([[[0., 0.],\n                                       constant_op.constant([1., 1.])],\n                                      np.array(\n                                          [[2., 2.], [3., 3.]],\n                                          dtype=np.float32)])\n      self.assertAllEqual([[[0., 0.], [1., 1.]], [[2., 2.], [3., 3.]]],\n                          self.evaluate(result))\n\n  def testDtype(self):\n    t_0 = ops.convert_to_tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])\n    self.assertEqual(dtypes.float32, t_0.dtype)\n\n    t_1 = ops.convert_to_tensor([[0., 0., 0.], constant_op.constant(\n        [0., 0., 0.], dtype=dtypes.float64), [0., 0., 0.]])\n    self.assertEqual(dtypes.float64, t_1.dtype)\n\n    t_2 = ops.convert_to_tensor(\n        [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=dtypes.float64)\n    self.assertEqual(dtypes.float64, t_2.dtype)\n\n    t_3 = ops.convert_to_tensor(\n        [[0., 0., 0.],\n         constant_op.constant([0., 0., 0.], dtype=dtypes.float64), [0., 0., 0.]\n        ],\n        dtype=dtypes.float32)\n    self.assertEqual(dtypes.float32, t_3.dtype)\n\n    t_4 = ops.convert_to_tensor(\n        [constant_op.constant([0., 0., 0.], dtype=dtypes.float64)],\n        dtype=dtypes.float32)\n    self.assertEqual(dtypes.float32, t_4.dtype)\n\n    with self.assertRaises(TypeError):\n      ops.convert_to_tensor([\n          constant_op.constant(\n              [0., 0., 0.], dtype=dtypes.float32), constant_op.constant(\n                  [0., 0., 0.], dtype=dtypes.float64), [0., 0., 0.]\n      ])\n\n  def testDtypeConversionWhenTensorDtypeMismatch(self):\n    t_0 = ops.convert_to_tensor([0., 0., 0.])\n    self.assertEqual(dtypes.float32, t_0.dtype)\n\n    t_1 = ops.convert_to_tensor([0, 0, 0])\n    self.assertEqual(dtypes.int32, t_1.dtype)\n\n    t_2 = ops.convert_to_tensor([t_0, t_0, t_1], dtype=dtypes.float64)\n    self.assertEqual(dtypes.float64, t_2.dtype)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "fixing_code": ["/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/framework/bfloat16.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/kernels/fill_functor.h\"\n#include \"tensorflow/core/kernels/inplace_ops_functor.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n\nnamespace tensorflow {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\nnamespace functor {\n\ntemplate <typename Device, typename T>\nStatus DoParallelConcatUpdate(const Device& d, const Tensor& value, int32_t loc,\n                              Tensor* output) {\n  auto Tvalue = value.shaped<T, 2>({1, value.NumElements()});\n  auto Toutput = output->flat_outer_dims<T>();\n  auto nrows = Toutput.dimension(0);\n  auto r = (loc % nrows + nrows) % nrows;  // Guard index range.\n  Toutput.template chip<0>(r).device(d) = Tvalue.template chip<0>(0);\n  return Status::OK();\n}\n\ntemplate <>\nStatus DoParallelConcat(const CPUDevice& d, const Tensor& value, int32_t loc,\n                        Tensor* output) {\n  CHECK_EQ(value.dtype(), output->dtype());\n  switch (value.dtype()) {\n#define CASE(type)                  \\\n  case DataTypeToEnum<type>::value: \\\n    return DoParallelConcatUpdate<CPUDevice, type>(d, value, loc, output);\n    TF_CALL_POD_TYPES(CASE);\n    TF_CALL_tstring(CASE);\n    TF_CALL_variant(CASE);\n#undef CASE\n    default:\n      return errors::InvalidArgument(\"Unsupported data type: \",\n                                     DataTypeString(value.dtype()));\n  }\n}\n\n}  // end namespace functor\n\nnamespace {\n\ntemplate <typename Device>\nclass ParallelConcatUpdate : public OpKernel {\n public:\n  explicit ParallelConcatUpdate(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"loc\", &loc_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    auto value = ctx->input(0);\n    // Value should be at least rank 1. Also the 0th dimension should be\n    // at least loc_.\n    OP_REQUIRES(ctx, value.dims() >= 1,\n                errors::InvalidArgument(\"value should be at least rank 1.\"));\n    OP_REQUIRES(\n        ctx, value.dim_size(0) > loc_,\n        errors::InvalidArgument(\"0th dimension of value = \", value.dim_size(0),\n                                \" is less than loc_=\", loc_));\n\n    auto update = ctx->input(1);\n\n    OP_REQUIRES(\n        ctx, value.dims() == update.dims(),\n        errors::InvalidArgument(\"value and update shape doesn't match: \",\n                                value.shape().DebugString(), \" vs. \",\n                                update.shape().DebugString()));\n    for (int i = 1; i < value.dims(); ++i) {\n      OP_REQUIRES(\n          ctx, value.dim_size(i) == update.dim_size(i),\n          errors::InvalidArgument(\"value and update shape doesn't match \",\n                                  value.shape().DebugString(), \" vs. \",\n                                  update.shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, 1 == update.dim_size(0),\n                errors::InvalidArgument(\"update shape doesn't match: \",\n                                        update.shape().DebugString()));\n\n    Tensor output = value;  // This creates an alias intentionally.\n    const auto& d = ctx->eigen_device<Device>();\n    OP_REQUIRES_OK(\n        ctx, ::tensorflow::functor::DoParallelConcat(d, update, loc_, &output));\n    ctx->set_output(0, output);\n  }\n\n private:\n  int32 loc_;\n};\n\ntemplate <typename Device, typename T>\nclass ParallelConcatStart : public OpKernel {\n public:\n  explicit ParallelConcatStart(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"shape\", &shape_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    Tensor* out = nullptr;\n    // We do not know whether the output will be used on GPU. Setting it to be\n    // gpu-compatible for now.\n    AllocatorAttributes attr;\n    attr.set_gpu_compatible(true);\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, shape_, &out, attr));\n  }\n\n private:\n  TensorShape shape_;\n};\n\nclass FailureKernel : public OpKernel {\n public:\n  explicit FailureKernel(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx,\n                   errors::Internal(\"Found instance of parallel_stack which \"\n                                    \"could not be properly replaced.\"));\n  }\n\n  void Compute(OpKernelContext*) override {}\n};\n\n#define REGISTER(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"_ParallelConcatUpdate\")   \\\n                              .Device(DEVICE_CPU)         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          ParallelConcatUpdate<CPUDevice>);\nTF_CALL_POD_STRING_TYPES(REGISTER)\n#undef REGISTER\n\n#define REGISTER_EMPTY(type)                                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"_ParallelConcatStart\")        \\\n                              .Device(DEVICE_CPU)             \\\n                              .TypeConstraint<type>(\"dtype\"), \\\n                          ParallelConcatStart<CPUDevice, type>)\n\nTF_CALL_POD_STRING_TYPES(REGISTER_EMPTY)\n#undef REGISTER_EMPTY\n\n#define REGISTER_PARALLEL_CONCAT(type)                                     \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"ParallelConcat\").Device(DEVICE_CPU).TypeConstraint<type>(\"T\"), \\\n      FailureKernel);\nTF_CALL_POD_STRING_TYPES(REGISTER_PARALLEL_CONCAT);\n#undef REGISTER_PARALLEL_CONCAT\n\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntypedef Eigen::GpuDevice GPUDevice;\n\n#define REGISTER_PARALLEL_CONCAT_START(type)                  \\\n  REGISTER_KERNEL_BUILDER(Name(\"_ParallelConcatStart\")        \\\n                              .Device(DEVICE_GPU)             \\\n                              .TypeConstraint<type>(\"dtype\"), \\\n                          ParallelConcatStart<GPUDevice, type>);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_PARALLEL_CONCAT_START)\n#undef REGISTER_PARALLEL_CONCAT_START\n\n#define REGISTER_PARALLEL_CONCAT(type)                                     \\\n  REGISTER_KERNEL_BUILDER(                                                 \\\n      Name(\"ParallelConcat\").Device(DEVICE_GPU).TypeConstraint<type>(\"T\"), \\\n      FailureKernel);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER_PARALLEL_CONCAT);\n#undef REGISTER_PARALLEL_CONCAT\n\n#define REGISTER(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"_ParallelConcatUpdate\")   \\\n                              .Device(DEVICE_GPU)         \\\n                              .TypeConstraint<type>(\"T\"), \\\n                          ParallelConcatUpdate<GPUDevice>);\nTF_CALL_GPU_NUMBER_TYPES(REGISTER)\n#undef REGISTER\n\n// Register versions that operate on int32 data on the CPU even though the op\n// has been placed on the GPU\n\nREGISTER_KERNEL_BUILDER(Name(\"_ParallelConcatUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"value\")\n                            .HostMemory(\"update\")\n                            .HostMemory(\"output\")\n                            .TypeConstraint<int32>(\"T\"),\n                        ParallelConcatUpdate<CPUDevice>);\n#endif\n\nclass InplaceOpBase : public OpKernel {\n public:\n  explicit InplaceOpBase(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    auto x = ctx->input(0);\n    auto i = ctx->input(1);\n    auto v = ctx->input(2);\n\n    OP_REQUIRES(ctx, TensorShapeUtils::IsVector(i.shape()),\n                errors::InvalidArgument(\"i must be a vector. \",\n                                        i.shape().DebugString()));\n    OP_REQUIRES(ctx, x.dims() == v.dims(),\n                errors::InvalidArgument(\n                    \"x and v shape doesn't match (ranks differ): \",\n                    x.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n    for (int i = 1; i < x.dims(); ++i) {\n      OP_REQUIRES(\n          ctx, x.dim_size(i) == v.dim_size(i),\n          errors::InvalidArgument(\"x and v shape doesn't match at index \", i,\n                                  \" : \", x.shape().DebugString(), \" vs. \",\n                                  v.shape().DebugString()));\n    }\n    OP_REQUIRES(ctx, i.dim_size(0) == v.dim_size(0),\n                errors::InvalidArgument(\n                    \"i and x shape doesn't match at index 0: \",\n                    i.shape().DebugString(), \" vs. \", v.shape().DebugString()));\n\n    Tensor y = x;  // This creates an alias intentionally.\n    // Skip processing if tensors are empty.\n    if (x.NumElements() > 0 && v.NumElements() > 0) {\n      OP_REQUIRES_OK(ctx, DoCompute(ctx, i, v, &y));\n    }\n    ctx->set_output(0, y);\n  }\n\n protected:\n  virtual Status DoCompute(OpKernelContext* ctx, const Tensor& i,\n                           const Tensor& v, Tensor* y) = 0;\n};\n\n}  // end namespace\n\nnamespace functor {\n\ntemplate <typename T>\nvoid DoInplaceOp(const CPUDevice& d, InplaceOpType op, const Tensor& i,\n                 const Tensor& v, Tensor* y) {\n  auto Ti = i.flat<int32>();\n  auto Tv = v.flat_outer_dims<T>();\n  auto Ty = y->flat_outer_dims<T>();\n  auto nrows = Ty.dimension(0);\n  for (int64_t j = 0; j < Ti.size(); ++j) {\n    auto r = (Ti(j) % nrows + nrows) % nrows;  // Guard index range.\n    switch (op) {\n      case I_UPDATE:\n        Ty.template chip<0>(r).device(d) = Tv.template chip<0>(j);\n        break;\n      case I_ADD:\n        Ty.template chip<0>(r).device(d) += Tv.template chip<0>(j);\n        break;\n      case I_SUB:\n        Ty.template chip<0>(r).device(d) -= Tv.template chip<0>(j);\n        break;\n    }\n  }\n}\n\n// String type only supports inplace update.\nvoid DoInplaceStringUpdateOp(const CPUDevice& d, const Tensor& i,\n                             const Tensor& v, Tensor* y) {\n  auto Ti = i.flat<int32>();\n  auto Tv = v.flat_outer_dims<tstring>();\n  auto Ty = y->flat_outer_dims<tstring>();\n  auto nrows = Ty.dimension(0);\n  for (int64_t j = 0; j < Ti.size(); ++j) {\n    auto r = (Ti(j) % nrows + nrows) % nrows;  // Guard index range.\n    Ty.template chip<0>(r).device(d) = Tv.template chip<0>(j);\n  }\n}\n\ntemplate <>\nStatus DoInplace(const CPUDevice& device, InplaceOpType op, const Tensor& i,\n                 const Tensor& v, Tensor* y) {\n  CHECK_EQ(v.dtype(), y->dtype());\n  if (op == I_UPDATE) {\n    if (v.dtype() == DT_STRING) {\n      DoInplaceStringUpdateOp(device, i, v, y);\n      return Status::OK();\n    } else if (v.dtype() == DT_BOOL) {\n      DoInplaceOp<bool>(device, op, i, v, y);\n      return Status::OK();\n    }\n  }\n  switch (v.dtype()) {\n#define CASE(type)                          \\\n  case DataTypeToEnum<type>::value:         \\\n    DoInplaceOp<type>(device, op, i, v, y); \\\n    break;\n    TF_CALL_NUMBER_TYPES(CASE);\n#undef CASE\n    default:\n      return errors::InvalidArgument(\"Unsupported data type: \",\n                                     DataTypeString(v.dtype()));\n  }\n  return Status::OK();\n}\n\n}  // end namespace functor\n\nnamespace {\ntemplate <typename Device, functor::InplaceOpType op>\nclass InplaceOp : public InplaceOpBase {\n public:\n  explicit InplaceOp(OpKernelConstruction* ctx) : InplaceOpBase(ctx) {}\n\n protected:\n  Status DoCompute(OpKernelContext* ctx, const Tensor& i, const Tensor& v,\n                   Tensor* y) override {\n    const auto& d = ctx->eigen_device<Device>();\n    return ::tensorflow::functor::DoInplace(d, op, i, v, y);\n  }\n};\n\nclass CopyOpBase : public OpKernel {\n public:\n  explicit CopyOpBase(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    auto x = ctx->input(0);\n    Tensor* y;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, x.shape(), &y));\n    OP_REQUIRES_OK(ctx, DoCompute(ctx, x, y));\n  }\n\n protected:\n  virtual Status DoCompute(OpKernelContext* ctx, const Tensor& x,\n                           Tensor* y) = 0;\n};\n\ntemplate <typename Device>\nclass CopyOp : public CopyOpBase {\n public:\n  explicit CopyOp(OpKernelConstruction* ctx) : CopyOpBase(ctx) {}\n\n protected:\n  Status DoCompute(OpKernelContext* ctx, const Tensor& x, Tensor* y) override {\n    const auto& d = ctx->eigen_device<Device>();\n    return ::tensorflow::functor::DoCopy(d, x, y);\n  }\n};\n\n}  // end namespace\n\nnamespace functor {\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\n\ntemplate <>\nStatus DoCopy(const CPUDevice& device, const Tensor& x, Tensor* y) {\n  CHECK_EQ(x.dtype(), y->dtype());\n  switch (x.dtype()) {\n#define CASE(type)                                   \\\n  case DataTypeToEnum<type>::value:                  \\\n    y->flat<type>().device(device) = x.flat<type>(); \\\n    break;\n\n    TF_CALL_NUMBER_TYPES(CASE);\n    TF_CALL_bool(CASE);\n    TF_CALL_tstring(CASE);\n#undef CASE\n    default:\n      return errors::InvalidArgument(\"Unsupported data type: \",\n                                     DataTypeString(x.dtype()));\n  }\n  return Status::OK();\n}\n\n}  // end namespace functor\n\nnamespace {\ntemplate <typename Device, typename T>\nclass EmptyOp : public OpKernel {\n public:\n  explicit EmptyOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"init\", &init_));\n  }\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& shape = ctx->input(0);\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(shape.shape()),\n        errors::InvalidArgument(\"shape must be a vector of int32, got shape \",\n                                shape.shape().DebugString()));\n    auto dims = shape.flat<int32>();\n    TensorShape out_shape;\n    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(\n                            reinterpret_cast<const int32*>(dims.data()),\n                            dims.size(), &out_shape));\n    Tensor* out = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));\n\n    if (init_) {\n      functor::SetZeroFunctor<Device, T>()(ctx->eigen_device<Device>(),\n                                           out->flat<T>());\n    }\n  }\n\n private:\n  bool init_;\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"InplaceUpdate\").Device(DEVICE_CPU),\n                        InplaceOp<CPUDevice, functor::I_UPDATE>);\nREGISTER_KERNEL_BUILDER(Name(\"InplaceAdd\").Device(DEVICE_CPU),\n                        InplaceOp<CPUDevice, functor::I_ADD>);\nREGISTER_KERNEL_BUILDER(Name(\"InplaceSub\").Device(DEVICE_CPU),\n                        InplaceOp<CPUDevice, functor::I_SUB>);\nREGISTER_KERNEL_BUILDER(Name(\"DeepCopy\").Device(DEVICE_CPU), CopyOp<CPUDevice>);\n\n#define REGISTER_EMPTY(type, dev)                             \\\n  REGISTER_KERNEL_BUILDER(Name(\"Empty\")                       \\\n                              .Device(DEVICE_##dev)           \\\n                              .HostMemory(\"shape\")            \\\n                              .TypeConstraint<type>(\"dtype\"), \\\n                          EmptyOp<dev##Device, type>)\n\nREGISTER_EMPTY(float, CPU)\nREGISTER_EMPTY(bfloat16, CPU)\nREGISTER_EMPTY(double, CPU)\nREGISTER_EMPTY(Eigen::half, CPU)\nREGISTER_EMPTY(tstring, CPU)\nREGISTER_EMPTY(int32, CPU)\nREGISTER_EMPTY(int64_t, CPU)\nREGISTER_EMPTY(bool, CPU)\nREGISTER_EMPTY(uint8, CPU)\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\ntypedef Eigen::GpuDevice GPUDevice;\n\n#define REGISTER(TYPE)                                                    \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"InplaceUpdate\").Device(DEVICE_GPU).TypeConstraint<TYPE>(\"T\"), \\\n      InplaceOp<GPUDevice, functor::I_UPDATE>);                           \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"InplaceAdd\").Device(DEVICE_GPU).TypeConstraint<TYPE>(\"T\"),    \\\n      InplaceOp<GPUDevice, functor::I_ADD>);                              \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"InplaceSub\").Device(DEVICE_GPU).TypeConstraint<TYPE>(\"T\"),    \\\n      InplaceOp<GPUDevice, functor::I_SUB>);                              \\\n  REGISTER_KERNEL_BUILDER(                                                \\\n      Name(\"DeepCopy\").Device(DEVICE_GPU).TypeConstraint<TYPE>(\"T\"),      \\\n      CopyOp<GPUDevice>);\n\nREGISTER_KERNEL_BUILDER(\n    Name(\"InplaceUpdate\").Device(DEVICE_GPU).TypeConstraint<bool>(\"T\"),\n    InplaceOp<GPUDevice, functor::I_UPDATE>);\nREGISTER(float);\nREGISTER(double);\nREGISTER(Eigen::half);\nREGISTER(int64_t);\n\nREGISTER_KERNEL_BUILDER(Name(\"InplaceUpdate\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"x\")\n                            .HostMemory(\"i\")\n                            .HostMemory(\"v\")\n                            .HostMemory(\"y\")\n                            .TypeConstraint<int32>(\"T\"),\n                        InplaceOp<CPUDevice, functor::I_UPDATE>);\nREGISTER_KERNEL_BUILDER(Name(\"InplaceAdd\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"x\")\n                            .HostMemory(\"i\")\n                            .HostMemory(\"v\")\n                            .HostMemory(\"y\")\n                            .TypeConstraint<int32>(\"T\"),\n                        InplaceOp<CPUDevice, functor::I_ADD>);\nREGISTER_KERNEL_BUILDER(Name(\"InplaceSub\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"x\")\n                            .HostMemory(\"i\")\n                            .HostMemory(\"v\")\n                            .HostMemory(\"y\")\n                            .TypeConstraint<int32>(\"T\"),\n                        InplaceOp<CPUDevice, functor::I_SUB>);\n\nREGISTER_KERNEL_BUILDER(Name(\"DeepCopy\")\n                            .Device(DEVICE_GPU)\n                            .HostMemory(\"x\")\n                            .HostMemory(\"y\")\n                            .TypeConstraint<int32>(\"T\"),\n                        CopyOp<CPUDevice>);\nREGISTER_EMPTY(float, GPU);\nREGISTER_EMPTY(double, GPU);\nREGISTER_EMPTY(Eigen::half, GPU);\nREGISTER_EMPTY(int64_t, GPU);\nREGISTER_EMPTY(int32, GPU);\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n}  // end namespace\n}  // end namespace tensorflow\n", "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Functional tests for Stack and ParallelStack Ops.\"\"\"\n\nimport numpy as np\n\nfrom tensorflow.python import tf2\nfrom tensorflow.python.eager import context\nfrom tensorflow.python.eager import def_function\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import gen_array_ops\nfrom tensorflow.python.ops import gradient_checker_v2\nfrom tensorflow.python.platform import test\n\n\ndef np_split_squeeze(array, axis):\n  axis_len = array.shape[axis]\n  return [\n      np.squeeze(\n          arr, axis=(axis,)) for arr in np.split(\n              array, axis_len, axis=axis)\n  ]\n\n\nclass StackOpTest(test.TestCase):\n\n  def randn(self, shape, dtype):\n    data = np.random.randn(*shape)\n    if dtype == np.bool_:\n      return data < 0  # Naive casting yields True with P(1)!\n    else:\n      return data.astype(dtype)\n\n  def testSimple(self):\n    np.random.seed(7)\n    for shape in (2,), (3,), (2, 3), (3, 2), (8, 2, 10):\n      rank = len(shape)\n      for axis in range(-rank, rank):\n        for dtype in [np.bool_, np.float32, np.int32, np.int64]:\n          data = self.randn(shape, dtype)\n          xs = np_split_squeeze(data, axis)\n          # Stack back into a single tensorflow tensor\n          with self.subTest(shape=shape, axis=axis, dtype=dtype):\n            c = array_ops.stack(xs, axis=axis)\n            self.assertAllEqual(c, data)\n\n  def testSimpleParallelCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      np.random.seed(7)\n      with test_util.device(use_gpu=False):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (100, 24, 24, 3):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            xs = list(map(constant_op.constant, data))\n            c = array_ops.parallel_stack(xs)\n            self.assertAllEqual(c, data)\n\n  def testParallelConcatShapeZero(self):\n    if not tf2.enabled():\n      self.skipTest(\"only fails in TF2\")\n\n    @def_function.function\n    def f():\n      y = gen_array_ops.parallel_concat(values=[[\"tf\"]], shape=0)\n      return y\n\n    with self.assertRaisesRegex(errors.InvalidArgumentError,\n                                r\"0th dimension of value .* is less than\"):\n      f()\n\n  def testSimpleParallelGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      with test_util.device(use_gpu=True):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (100, 24, 24, 3):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            xs = list(map(constant_op.constant, data))\n            c = array_ops.parallel_stack(xs)\n            self.assertAllEqual(c, data)\n\n  def testConst(self):\n    np.random.seed(7)\n    with test_util.use_gpu():\n      # Verify that shape induction works with shapes produced via const stack\n      a = constant_op.constant([1, 2, 3, 4, 5, 6])\n      b = array_ops.reshape(a, array_ops.stack([2, 3]))\n      self.assertAllEqual(b.get_shape(), [2, 3])\n\n      # Check on a variety of shapes and types\n      for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (8, 2, 10):\n        for dtype in [np.bool_, np.float32, np.int16, np.int32, np.int64]:\n          with self.subTest(shape=shape, dtype=dtype):\n            data = self.randn(shape, dtype)\n            # Stack back into a single tensorflow tensor directly using np array\n            c = array_ops.stack(data)\n            if not context.executing_eagerly():\n              # This is implemented via a Const:\n              self.assertEqual(c.op.type, \"Const\")\n            self.assertAllEqual(c, data)\n\n            # Python lists also work for 1-D case:\n            if len(shape) == 1:\n              data_list = list(data)\n              cl = array_ops.stack(data_list)\n              if not context.executing_eagerly():\n                self.assertEqual(cl.op.type, \"Const\")\n              self.assertAllEqual(cl, data)\n\n  def testConstParallelCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      np.random.seed(7)\n      with test_util.device(use_gpu=False):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2), (8, 2, 10):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            if len(shape) == 1:\n              data_list = list(data)\n              cl = array_ops.parallel_stack(data_list)\n              self.assertAllEqual(cl, data)\n\n            data = self.randn(shape, np.float32)\n            c = array_ops.parallel_stack(data)\n            self.assertAllEqual(c, data)\n\n  def testConstParallelGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      np.random.seed(7)\n      with test_util.device(use_gpu=True):\n        for shape in (2,), (3,), (2, 3), (3, 2), (4, 3, 2):\n          with self.subTest(shape=shape):\n            data = self.randn(shape, np.float32)\n            if len(shape) == 1:\n              data_list = list(data)\n              cl = array_ops.parallel_stack(data_list)\n              self.assertAllEqual(cl, data)\n\n            data = self.randn(shape, np.float32)\n            c = array_ops.parallel_stack(data)\n            self.assertAllEqual(c, data)\n\n  def testGradientsAxis0(self):\n    np.random.seed(7)\n    for shape in (2,), (3,), (2, 3), (3, 2), (8, 2, 10):\n      data = np.random.randn(*shape)\n      with self.subTest(shape=shape):\n        with self.cached_session():\n\n          def func(*xs):\n            return array_ops.stack(xs)\n          # TODO(irving): Remove list() once we handle maps correctly\n          xs = list(map(constant_op.constant, data))\n          theoretical, numerical = gradient_checker_v2.compute_gradient(\n              func, xs)\n          self.assertAllClose(theoretical, numerical)\n\n  def testGradientsAxis1(self):\n    np.random.seed(7)\n    for shape in (2, 3), (3, 2), (8, 2, 10):\n      data = np.random.randn(*shape)\n      out_shape = list(shape[1:])\n      out_shape.insert(1, shape[0])\n      with self.subTest(shape=shape):\n        with self.cached_session():\n\n          def func(*inp):\n            return array_ops.stack(inp, axis=1)\n          # TODO(irving): Remove list() once we handle maps correctly\n          xs = list(map(constant_op.constant, data))\n          theoretical, numerical = gradient_checker_v2.compute_gradient(\n              func, xs)\n          self.assertAllClose(theoretical, numerical)\n\n  def testZeroSizeCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      # Verify that stack doesn't crash for zero size inputs\n      with test_util.device(use_gpu=False):\n        for shape in (0,), (3, 0), (0, 3):\n          with self.subTest(shape=shape):\n            x = np.zeros((2,) + shape).astype(np.int32)\n            p = self.evaluate(array_ops.stack(list(x)))\n            self.assertAllEqual(p, x)\n\n            p = self.evaluate(array_ops.parallel_stack(list(x)))\n            self.assertAllEqual(p, x)\n\n  def testZeroSizeGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      # Verify that stack doesn't crash for zero size inputs\n      with test_util.device(use_gpu=True):\n        for shape in (0,), (3, 0), (0, 3):\n          with self.subTest(shape=shape):\n            x = np.zeros((2,) + shape).astype(np.int32)\n            p = self.evaluate(array_ops.stack(list(x)))\n            self.assertAllEqual(p, x)\n\n            p = self.evaluate(array_ops.parallel_stack(list(x)))\n            self.assertAllEqual(p, x)\n\n  def testAxis0DefaultCPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      with test_util.device(use_gpu=False):\n        t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n        stacked = self.evaluate(array_ops.stack(t))\n        parallel_stacked = self.evaluate(array_ops.parallel_stack(t))\n\n      expected = np.array([[1, 2, 3], [4, 5, 6]])\n      self.assertAllEqual(stacked, expected)\n      self.assertAllEqual(parallel_stacked, expected)\n\n  def testAxis0DefaultGPU(self):\n    # tf.parallel_stack is only supported in graph mode.\n    with ops.Graph().as_default():\n      with test_util.device(use_gpu=True):\n        t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n        stacked = self.evaluate(array_ops.stack(t))\n        parallel_stacked = self.evaluate(array_ops.parallel_stack(t))\n\n      expected = np.array([[1, 2, 3], [4, 5, 6]])\n      self.assertAllEqual(stacked, expected)\n      self.assertAllEqual(parallel_stacked, expected)\n\n  def testAgainstNumpy(self):\n    # For 1 to 5 dimensions.\n    for shape in (3,), (2, 2, 3), (4, 1, 2, 2), (8, 2, 10):\n      rank = len(shape)\n      expected = self.randn(shape, np.float32)\n      for dtype in [np.bool_, np.float32, np.int32, np.int64]:\n        # For all the possible axis to split it, including negative indices.\n        for axis in range(-rank, rank):\n          test_arrays = np_split_squeeze(expected, axis)\n\n          with self.cached_session():\n            with self.subTest(shape=shape, dtype=dtype, axis=axis):\n              actual_pack = array_ops.stack(test_arrays, axis=axis)\n              self.assertEqual(expected.shape, actual_pack.get_shape())\n              actual_pack = self.evaluate(actual_pack)\n\n              actual_stack = array_ops.stack(test_arrays, axis=axis)\n              self.assertEqual(expected.shape, actual_stack.get_shape())\n              actual_stack = self.evaluate(actual_stack)\n\n              self.assertNDArrayNear(expected, actual_stack, 1e-6)\n\n  def testDimOutOfRange(self):\n    t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n    with self.assertRaisesRegex(ValueError,\n                                r\"Argument `axis` = 2 not in range \\[-2, 2\\)\"):\n      array_ops.stack(t, axis=2)\n\n  def testDimOutOfNegativeRange(self):\n    t = [constant_op.constant([1, 2, 3]), constant_op.constant([4, 5, 6])]\n    with self.assertRaisesRegex(ValueError,\n                                r\"Argument `axis` = -3 not in range \\[-2, 2\\)\"):\n      array_ops.stack(t, axis=-3)\n\n  def testComplex(self):\n    np.random.seed(7)\n    with self.session():\n      for shape in (2,), (3,), (2, 3), (3, 2), (8, 2, 10):\n        for dtype in [np.complex64, np.complex128]:\n          with self.subTest(shape=shape, dtype=dtype):\n            data = self.randn(shape, dtype)\n            xs = list(map(constant_op.constant, data))\n            c = array_ops.stack(xs)\n            self.assertAllEqual(self.evaluate(c), data)\n\n\nclass AutomaticStackingTest(test.TestCase):\n\n  def testSimple(self):\n    self.assertAllEqual([1, 0, 2],\n                        ops.convert_to_tensor([1, constant_op.constant(0), 2]))\n    self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],\n                        ops.convert_to_tensor([[0, 0, 0],\n                                               [0,\n                                                constant_op.constant(1), 0],\n                                               [0, 0, 0]]))\n    self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],\n                        ops.convert_to_tensor([[0, 0, 0],\n                                               constant_op.constant([0, 1, 0]),\n                                               [0, 0, 0]]))\n    self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],\n                        ops.convert_to_tensor([\n                            constant_op.constant([0, 0, 0]),\n                            constant_op.constant([0, 1, 0]),\n                            constant_op.constant([0, 0, 0])\n                        ]))\n\n  def testWithNDArray(self):\n    with self.session():\n      result = ops.convert_to_tensor([[[0., 0.],\n                                       constant_op.constant([1., 1.])],\n                                      np.array(\n                                          [[2., 2.], [3., 3.]],\n                                          dtype=np.float32)])\n      self.assertAllEqual([[[0., 0.], [1., 1.]], [[2., 2.], [3., 3.]]],\n                          self.evaluate(result))\n\n  def testDtype(self):\n    t_0 = ops.convert_to_tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])\n    self.assertEqual(dtypes.float32, t_0.dtype)\n\n    t_1 = ops.convert_to_tensor([[0., 0., 0.], constant_op.constant(\n        [0., 0., 0.], dtype=dtypes.float64), [0., 0., 0.]])\n    self.assertEqual(dtypes.float64, t_1.dtype)\n\n    t_2 = ops.convert_to_tensor(\n        [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=dtypes.float64)\n    self.assertEqual(dtypes.float64, t_2.dtype)\n\n    t_3 = ops.convert_to_tensor(\n        [[0., 0., 0.],\n         constant_op.constant([0., 0., 0.], dtype=dtypes.float64), [0., 0., 0.]\n        ],\n        dtype=dtypes.float32)\n    self.assertEqual(dtypes.float32, t_3.dtype)\n\n    t_4 = ops.convert_to_tensor(\n        [constant_op.constant([0., 0., 0.], dtype=dtypes.float64)],\n        dtype=dtypes.float32)\n    self.assertEqual(dtypes.float32, t_4.dtype)\n\n    with self.assertRaises(TypeError):\n      ops.convert_to_tensor([\n          constant_op.constant(\n              [0., 0., 0.], dtype=dtypes.float32), constant_op.constant(\n                  [0., 0., 0.], dtype=dtypes.float64), [0., 0., 0.]\n      ])\n\n  def testDtypeConversionWhenTensorDtypeMismatch(self):\n    t_0 = ops.convert_to_tensor([0., 0., 0.])\n    self.assertEqual(dtypes.float32, t_0.dtype)\n\n    t_1 = ops.convert_to_tensor([0, 0, 0])\n    self.assertEqual(dtypes.int32, t_1.dtype)\n\n    t_2 = ops.convert_to_tensor([t_0, t_0, t_1], dtype=dtypes.float64)\n    self.assertEqual(dtypes.float64, t_2.dtype)\n\n\nif __name__ == \"__main__\":\n  test.main()\n"], "filenames": ["tensorflow/core/kernels/inplace_ops.cc", "tensorflow/python/kernel_tests/array_ops/stack_op_test.py"], "buggy_code_start_loc": [73, 18], "buggy_code_end_loc": [73, 70], "fixing_code_start_loc": [74, 19], "fixing_code_end_loc": [83, 88], "type": "CWE-369", "message": "TensorFlow is an open source platform for machine learning. In affected versions the implementation of `ParallelConcat` misses some input validation and can produce a division by 0. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-41207", "sourceIdentifier": "security-advisories@github.com", "published": "2021-11-05T22:15:08.470", "lastModified": "2021-11-09T18:32:49.717", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an open source platform for machine learning. In affected versions the implementation of `ParallelConcat` misses some input validation and can produce a division by 0. The fix will be included in TensorFlow 2.7.0. We will also cherrypick this commit on TensorFlow 2.6.1, TensorFlow 2.5.2, and TensorFlow 2.4.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto para el aprendizaje autom\u00e1tico. En las versiones afectadas, la implementaci\u00f3n de \"ParallelConcat\" falla en la comprobaci\u00f3n de la entrada y puede producir una divisi\u00f3n por 0. La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.7.0. Tambi\u00e9n vamos a incluir este commit en TensorFlow versi\u00f3n 2.6.1, TensorFlow versi\u00f3n 2.5.2, y TensorFlow versi\u00f3n 2.4.4, ya que estos tambi\u00e9n est\u00e1n afectados y todav\u00eda en el rango admitido"}], "metrics": {"cvssMetricV31": [{"source": "security-advisories@github.com", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-369"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.4", "matchCriteriaId": "0E596567-6F67-4880-8EC4-CB262BF02E0D"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.5.0", "versionEndExcluding": "2.5.2", "matchCriteriaId": "035CDF63-1548-4FB4-B8A9-B8D328FAF910"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.6.0", "versionEndExcluding": "2.6.1", "matchCriteriaId": "5D68D8D1-DB27-4395-9D3D-2BED901B852C"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "A58EDA5C-66D6-46F1-962E-60AFB7C784A7"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.7.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "89522760-C2DF-400D-9624-626D8F160CBA"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/f2c3931113eaafe9ef558faaddd48e00a6606235", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-7v94-64hj-m82h", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/f2c3931113eaafe9ef558faaddd48e00a6606235"}}