{"buggy_code": ["#ifndef __LINUX_NET_AFUNIX_H\n#define __LINUX_NET_AFUNIX_H\n\n#include <linux/socket.h>\n#include <linux/un.h>\n#include <linux/mutex.h>\n#include <net/sock.h>\n\nvoid unix_inflight(struct file *fp);\nvoid unix_notinflight(struct file *fp);\nvoid unix_gc(void);\nvoid wait_for_unix_gc(void);\nstruct sock *unix_get_socket(struct file *filp);\nstruct sock *unix_peer_get(struct sock *);\n\n#define UNIX_HASH_SIZE\t256\n#define UNIX_HASH_BITS\t8\n\nextern unsigned int unix_tot_inflight;\nextern spinlock_t unix_table_lock;\nextern struct hlist_head unix_socket_table[2 * UNIX_HASH_SIZE];\n\nstruct unix_address {\n\tatomic_t\trefcnt;\n\tint\t\tlen;\n\tunsigned int\thash;\n\tstruct sockaddr_un name[0];\n};\n\nstruct unix_skb_parms {\n\tstruct pid\t\t*pid;\t\t/* Skb credentials\t*/\n\tkuid_t\t\t\tuid;\n\tkgid_t\t\t\tgid;\n\tstruct scm_fp_list\t*fp;\t\t/* Passed files\t\t*/\n#ifdef CONFIG_SECURITY_NETWORK\n\tu32\t\t\tsecid;\t\t/* Security ID\t\t*/\n#endif\n\tu32\t\t\tconsumed;\n};\n\n#define UNIXCB(skb) \t(*(struct unix_skb_parms *)&((skb)->cb))\n\n#define unix_state_lock(s)\tspin_lock(&unix_sk(s)->lock)\n#define unix_state_unlock(s)\tspin_unlock(&unix_sk(s)->lock)\n#define unix_state_lock_nested(s) \\\n\t\t\t\tspin_lock_nested(&unix_sk(s)->lock, \\\n\t\t\t\tSINGLE_DEPTH_NESTING)\n\n/* The AF_UNIX socket */\nstruct unix_sock {\n\t/* WARNING: sk has to be the first member */\n\tstruct sock\t\tsk;\n\tstruct unix_address     *addr;\n\tstruct path\t\tpath;\n\tstruct mutex\t\treadlock;\n\tstruct sock\t\t*peer;\n\tstruct list_head\tlink;\n\tatomic_long_t\t\tinflight;\n\tspinlock_t\t\tlock;\n\tunsigned char\t\trecursion_level;\n\tunsigned long\t\tgc_flags;\n#define UNIX_GC_CANDIDATE\t0\n#define UNIX_GC_MAYBE_CYCLE\t1\n\tstruct socket_wq\tpeer_wq;\n\twait_queue_t\t\tpeer_wake;\n};\n\nstatic inline struct unix_sock *unix_sk(const struct sock *sk)\n{\n\treturn (struct unix_sock *)sk;\n}\n\n#define peer_wait peer_wq.wait\n\nlong unix_inq_len(struct sock *sk);\nlong unix_outq_len(struct sock *sk);\n\n#ifdef CONFIG_SYSCTL\nint unix_sysctl_register(struct net *net);\nvoid unix_sysctl_unregister(struct net *net);\n#else\nstatic inline int unix_sysctl_register(struct net *net) { return 0; }\nstatic inline void unix_sysctl_unregister(struct net *net) {}\n#endif\n#endif\n", "#ifndef __LINUX_NET_SCM_H\n#define __LINUX_NET_SCM_H\n\n#include <linux/limits.h>\n#include <linux/net.h>\n#include <linux/security.h>\n#include <linux/pid.h>\n#include <linux/nsproxy.h>\n\n/* Well, we should have at least one descriptor open\n * to accept passed FDs 8)\n */\n#define SCM_MAX_FD\t253\n\nstruct scm_creds {\n\tu32\tpid;\n\tkuid_t\tuid;\n\tkgid_t\tgid;\n};\n\nstruct scm_fp_list {\n\tshort\t\t\tcount;\n\tshort\t\t\tmax;\n\tstruct file\t\t*fp[SCM_MAX_FD];\n};\n\nstruct scm_cookie {\n\tstruct pid\t\t*pid;\t\t/* Skb credentials */\n\tstruct scm_fp_list\t*fp;\t\t/* Passed files\t\t*/\n\tstruct scm_creds\tcreds;\t\t/* Skb credentials\t*/\n#ifdef CONFIG_SECURITY_NETWORK\n\tu32\t\t\tsecid;\t\t/* Passed security ID \t*/\n#endif\n};\n\nvoid scm_detach_fds(struct msghdr *msg, struct scm_cookie *scm);\nvoid scm_detach_fds_compat(struct msghdr *msg, struct scm_cookie *scm);\nint __scm_send(struct socket *sock, struct msghdr *msg, struct scm_cookie *scm);\nvoid __scm_destroy(struct scm_cookie *scm);\nstruct scm_fp_list *scm_fp_dup(struct scm_fp_list *fpl);\n\n#ifdef CONFIG_SECURITY_NETWORK\nstatic __inline__ void unix_get_peersec_dgram(struct socket *sock, struct scm_cookie *scm)\n{\n\tsecurity_socket_getpeersec_dgram(sock, NULL, &scm->secid);\n}\n#else\nstatic __inline__ void unix_get_peersec_dgram(struct socket *sock, struct scm_cookie *scm)\n{ }\n#endif /* CONFIG_SECURITY_NETWORK */\n\nstatic __inline__ void scm_set_cred(struct scm_cookie *scm,\n\t\t\t\t    struct pid *pid, kuid_t uid, kgid_t gid)\n{\n\tscm->pid  = get_pid(pid);\n\tscm->creds.pid = pid_vnr(pid);\n\tscm->creds.uid = uid;\n\tscm->creds.gid = gid;\n}\n\nstatic __inline__ void scm_destroy_cred(struct scm_cookie *scm)\n{\n\tput_pid(scm->pid);\n\tscm->pid  = NULL;\n}\n\nstatic __inline__ void scm_destroy(struct scm_cookie *scm)\n{\n\tscm_destroy_cred(scm);\n\tif (scm->fp)\n\t\t__scm_destroy(scm);\n}\n\nstatic __inline__ int scm_send(struct socket *sock, struct msghdr *msg,\n\t\t\t       struct scm_cookie *scm, bool forcecreds)\n{\n\tmemset(scm, 0, sizeof(*scm));\n\tscm->creds.uid = INVALID_UID;\n\tscm->creds.gid = INVALID_GID;\n\tif (forcecreds)\n\t\tscm_set_cred(scm, task_tgid(current), current_uid(), current_gid());\n\tunix_get_peersec_dgram(sock, scm);\n\tif (msg->msg_controllen <= 0)\n\t\treturn 0;\n\treturn __scm_send(sock, msg, scm);\n}\n\n#ifdef CONFIG_SECURITY_NETWORK\nstatic inline void scm_passec(struct socket *sock, struct msghdr *msg, struct scm_cookie *scm)\n{\n\tchar *secdata;\n\tu32 seclen;\n\tint err;\n\n\tif (test_bit(SOCK_PASSSEC, &sock->flags)) {\n\t\terr = security_secid_to_secctx(scm->secid, &secdata, &seclen);\n\n\t\tif (!err) {\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_SECURITY, seclen, secdata);\n\t\t\tsecurity_release_secctx(secdata, seclen);\n\t\t}\n\t}\n}\n#else\nstatic inline void scm_passec(struct socket *sock, struct msghdr *msg, struct scm_cookie *scm)\n{ }\n#endif /* CONFIG_SECURITY_NETWORK */\n\nstatic __inline__ void scm_recv(struct socket *sock, struct msghdr *msg,\n\t\t\t\tstruct scm_cookie *scm, int flags)\n{\n\tif (!msg->msg_control) {\n\t\tif (test_bit(SOCK_PASSCRED, &sock->flags) || scm->fp)\n\t\t\tmsg->msg_flags |= MSG_CTRUNC;\n\t\tscm_destroy(scm);\n\t\treturn;\n\t}\n\n\tif (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\tstruct user_namespace *current_ns = current_user_ns();\n\t\tstruct ucred ucreds = {\n\t\t\t.pid = scm->creds.pid,\n\t\t\t.uid = from_kuid_munged(current_ns, scm->creds.uid),\n\t\t\t.gid = from_kgid_munged(current_ns, scm->creds.gid),\n\t\t};\n\t\tput_cmsg(msg, SOL_SOCKET, SCM_CREDENTIALS, sizeof(ucreds), &ucreds);\n\t}\n\n\tscm_destroy_cred(scm);\n\n\tscm_passec(sock, msg, scm);\n\n\tif (!scm->fp)\n\t\treturn;\n\t\n\tscm_detach_fds(msg, scm);\n}\n\n\n#endif /* __LINUX_NET_SCM_H */\n\n", "/* scm.c - Socket level control messages processing.\n *\n * Author:\tAlexey Kuznetsov, <kuznet@ms2.inr.ac.ru>\n *              Alignment and value checking mods by Craig Metz\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/module.h>\n#include <linux/signal.h>\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/kernel.h>\n#include <linux/stat.h>\n#include <linux/socket.h>\n#include <linux/file.h>\n#include <linux/fcntl.h>\n#include <linux/net.h>\n#include <linux/interrupt.h>\n#include <linux/netdevice.h>\n#include <linux/security.h>\n#include <linux/pid_namespace.h>\n#include <linux/pid.h>\n#include <linux/nsproxy.h>\n#include <linux/slab.h>\n\n#include <asm/uaccess.h>\n\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <net/compat.h>\n#include <net/scm.h>\n#include <net/cls_cgroup.h>\n\n\n/*\n *\tOnly allow a user to send credentials, that they could set with\n *\tsetu(g)id.\n */\n\nstatic __inline__ int scm_check_creds(struct ucred *creds)\n{\n\tconst struct cred *cred = current_cred();\n\tkuid_t uid = make_kuid(cred->user_ns, creds->uid);\n\tkgid_t gid = make_kgid(cred->user_ns, creds->gid);\n\n\tif (!uid_valid(uid) || !gid_valid(gid))\n\t\treturn -EINVAL;\n\n\tif ((creds->pid == task_tgid_vnr(current) ||\n\t     ns_capable(task_active_pid_ns(current)->user_ns, CAP_SYS_ADMIN)) &&\n\t    ((uid_eq(uid, cred->uid)   || uid_eq(uid, cred->euid) ||\n\t      uid_eq(uid, cred->suid)) || ns_capable(cred->user_ns, CAP_SETUID)) &&\n\t    ((gid_eq(gid, cred->gid)   || gid_eq(gid, cred->egid) ||\n\t      gid_eq(gid, cred->sgid)) || ns_capable(cred->user_ns, CAP_SETGID))) {\n\t       return 0;\n\t}\n\treturn -EPERM;\n}\n\nstatic int scm_fp_copy(struct cmsghdr *cmsg, struct scm_fp_list **fplp)\n{\n\tint *fdp = (int*)CMSG_DATA(cmsg);\n\tstruct scm_fp_list *fpl = *fplp;\n\tstruct file **fpp;\n\tint i, num;\n\n\tnum = (cmsg->cmsg_len - CMSG_ALIGN(sizeof(struct cmsghdr)))/sizeof(int);\n\n\tif (num <= 0)\n\t\treturn 0;\n\n\tif (num > SCM_MAX_FD)\n\t\treturn -EINVAL;\n\n\tif (!fpl)\n\t{\n\t\tfpl = kmalloc(sizeof(struct scm_fp_list), GFP_KERNEL);\n\t\tif (!fpl)\n\t\t\treturn -ENOMEM;\n\t\t*fplp = fpl;\n\t\tfpl->count = 0;\n\t\tfpl->max = SCM_MAX_FD;\n\t}\n\tfpp = &fpl->fp[fpl->count];\n\n\tif (fpl->count + num > fpl->max)\n\t\treturn -EINVAL;\n\n\t/*\n\t *\tVerify the descriptors and increment the usage count.\n\t */\n\n\tfor (i=0; i< num; i++)\n\t{\n\t\tint fd = fdp[i];\n\t\tstruct file *file;\n\n\t\tif (fd < 0 || !(file = fget_raw(fd)))\n\t\t\treturn -EBADF;\n\t\t*fpp++ = file;\n\t\tfpl->count++;\n\t}\n\treturn num;\n}\n\nvoid __scm_destroy(struct scm_cookie *scm)\n{\n\tstruct scm_fp_list *fpl = scm->fp;\n\tint i;\n\n\tif (fpl) {\n\t\tscm->fp = NULL;\n\t\tfor (i=fpl->count-1; i>=0; i--)\n\t\t\tfput(fpl->fp[i]);\n\t\tkfree(fpl);\n\t}\n}\nEXPORT_SYMBOL(__scm_destroy);\n\nint __scm_send(struct socket *sock, struct msghdr *msg, struct scm_cookie *p)\n{\n\tstruct cmsghdr *cmsg;\n\tint err;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\terr = -EINVAL;\n\n\t\t/* Verify that cmsg_len is at least sizeof(struct cmsghdr) */\n\t\t/* The first check was omitted in <= 2.2.5. The reasoning was\n\t\t   that parser checks cmsg_len in any case, so that\n\t\t   additional check would be work duplication.\n\t\t   But if cmsg_level is not SOL_SOCKET, we do not check\n\t\t   for too short ancillary data object at all! Oops.\n\t\t   OK, let's add it...\n\t\t */\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\tgoto error;\n\n\t\tif (cmsg->cmsg_level != SOL_SOCKET)\n\t\t\tcontinue;\n\n\t\tswitch (cmsg->cmsg_type)\n\t\t{\n\t\tcase SCM_RIGHTS:\n\t\t\tif (!sock->ops || sock->ops->family != PF_UNIX)\n\t\t\t\tgoto error;\n\t\t\terr=scm_fp_copy(cmsg, &p->fp);\n\t\t\tif (err<0)\n\t\t\t\tgoto error;\n\t\t\tbreak;\n\t\tcase SCM_CREDENTIALS:\n\t\t{\n\t\t\tstruct ucred creds;\n\t\t\tkuid_t uid;\n\t\t\tkgid_t gid;\n\t\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(struct ucred)))\n\t\t\t\tgoto error;\n\t\t\tmemcpy(&creds, CMSG_DATA(cmsg), sizeof(struct ucred));\n\t\t\terr = scm_check_creds(&creds);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\n\t\t\tp->creds.pid = creds.pid;\n\t\t\tif (!p->pid || pid_vnr(p->pid) != creds.pid) {\n\t\t\t\tstruct pid *pid;\n\t\t\t\terr = -ESRCH;\n\t\t\t\tpid = find_get_pid(creds.pid);\n\t\t\t\tif (!pid)\n\t\t\t\t\tgoto error;\n\t\t\t\tput_pid(p->pid);\n\t\t\t\tp->pid = pid;\n\t\t\t}\n\n\t\t\terr = -EINVAL;\n\t\t\tuid = make_kuid(current_user_ns(), creds.uid);\n\t\t\tgid = make_kgid(current_user_ns(), creds.gid);\n\t\t\tif (!uid_valid(uid) || !gid_valid(gid))\n\t\t\t\tgoto error;\n\n\t\t\tp->creds.uid = uid;\n\t\t\tp->creds.gid = gid;\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tif (p->fp && !p->fp->count)\n\t{\n\t\tkfree(p->fp);\n\t\tp->fp = NULL;\n\t}\n\treturn 0;\n\nerror:\n\tscm_destroy(p);\n\treturn err;\n}\nEXPORT_SYMBOL(__scm_send);\n\nint put_cmsg(struct msghdr * msg, int level, int type, int len, void *data)\n{\n\tstruct cmsghdr __user *cm\n\t\t= (__force struct cmsghdr __user *)msg->msg_control;\n\tstruct cmsghdr cmhdr;\n\tint cmlen = CMSG_LEN(len);\n\tint err;\n\n\tif (MSG_CMSG_COMPAT & msg->msg_flags)\n\t\treturn put_cmsg_compat(msg, level, type, len, data);\n\n\tif (cm==NULL || msg->msg_controllen < sizeof(*cm)) {\n\t\tmsg->msg_flags |= MSG_CTRUNC;\n\t\treturn 0; /* XXX: return error? check spec. */\n\t}\n\tif (msg->msg_controllen < cmlen) {\n\t\tmsg->msg_flags |= MSG_CTRUNC;\n\t\tcmlen = msg->msg_controllen;\n\t}\n\tcmhdr.cmsg_level = level;\n\tcmhdr.cmsg_type = type;\n\tcmhdr.cmsg_len = cmlen;\n\n\terr = -EFAULT;\n\tif (copy_to_user(cm, &cmhdr, sizeof cmhdr))\n\t\tgoto out;\n\tif (copy_to_user(CMSG_DATA(cm), data, cmlen - sizeof(struct cmsghdr)))\n\t\tgoto out;\n\tcmlen = CMSG_SPACE(len);\n\tif (msg->msg_controllen < cmlen)\n\t\tcmlen = msg->msg_controllen;\n\tmsg->msg_control += cmlen;\n\tmsg->msg_controllen -= cmlen;\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(put_cmsg);\n\nvoid scm_detach_fds(struct msghdr *msg, struct scm_cookie *scm)\n{\n\tstruct cmsghdr __user *cm\n\t\t= (__force struct cmsghdr __user*)msg->msg_control;\n\n\tint fdmax = 0;\n\tint fdnum = scm->fp->count;\n\tstruct file **fp = scm->fp->fp;\n\tint __user *cmfptr;\n\tint err = 0, i;\n\n\tif (MSG_CMSG_COMPAT & msg->msg_flags) {\n\t\tscm_detach_fds_compat(msg, scm);\n\t\treturn;\n\t}\n\n\tif (msg->msg_controllen > sizeof(struct cmsghdr))\n\t\tfdmax = ((msg->msg_controllen - sizeof(struct cmsghdr))\n\t\t\t / sizeof(int));\n\n\tif (fdnum < fdmax)\n\t\tfdmax = fdnum;\n\n\tfor (i=0, cmfptr=(__force int __user *)CMSG_DATA(cm); i<fdmax;\n\t     i++, cmfptr++)\n\t{\n\t\tstruct socket *sock;\n\t\tint new_fd;\n\t\terr = security_file_receive(fp[i]);\n\t\tif (err)\n\t\t\tbreak;\n\t\terr = get_unused_fd_flags(MSG_CMSG_CLOEXEC & msg->msg_flags\n\t\t\t\t\t  ? O_CLOEXEC : 0);\n\t\tif (err < 0)\n\t\t\tbreak;\n\t\tnew_fd = err;\n\t\terr = put_user(new_fd, cmfptr);\n\t\tif (err) {\n\t\t\tput_unused_fd(new_fd);\n\t\t\tbreak;\n\t\t}\n\t\t/* Bump the usage count and install the file. */\n\t\tsock = sock_from_file(fp[i], &err);\n\t\tif (sock) {\n\t\t\tsock_update_netprioidx(&sock->sk->sk_cgrp_data);\n\t\t\tsock_update_classid(&sock->sk->sk_cgrp_data);\n\t\t}\n\t\tfd_install(new_fd, get_file(fp[i]));\n\t}\n\n\tif (i > 0)\n\t{\n\t\tint cmlen = CMSG_LEN(i*sizeof(int));\n\t\terr = put_user(SOL_SOCKET, &cm->cmsg_level);\n\t\tif (!err)\n\t\t\terr = put_user(SCM_RIGHTS, &cm->cmsg_type);\n\t\tif (!err)\n\t\t\terr = put_user(cmlen, &cm->cmsg_len);\n\t\tif (!err) {\n\t\t\tcmlen = CMSG_SPACE(i*sizeof(int));\n\t\t\tif (msg->msg_controllen < cmlen)\n\t\t\t\tcmlen = msg->msg_controllen;\n\t\t\tmsg->msg_control += cmlen;\n\t\t\tmsg->msg_controllen -= cmlen;\n\t\t}\n\t}\n\tif (i < fdnum || (fdnum && fdmax <= 0))\n\t\tmsg->msg_flags |= MSG_CTRUNC;\n\n\t/*\n\t * All of the files that fit in the message have had their\n\t * usage counts incremented, so we just free the list.\n\t */\n\t__scm_destroy(scm);\n}\nEXPORT_SYMBOL(scm_detach_fds);\n\nstruct scm_fp_list *scm_fp_dup(struct scm_fp_list *fpl)\n{\n\tstruct scm_fp_list *new_fpl;\n\tint i;\n\n\tif (!fpl)\n\t\treturn NULL;\n\n\tnew_fpl = kmemdup(fpl, offsetof(struct scm_fp_list, fp[fpl->count]),\n\t\t\t  GFP_KERNEL);\n\tif (new_fpl) {\n\t\tfor (i = 0; i < fpl->count; i++)\n\t\t\tget_file(fpl->fp[i]);\n\t\tnew_fpl->max = new_fpl->count;\n\t}\n\treturn new_fpl;\n}\nEXPORT_SYMBOL(scm_fp_dup);\n", "/*\n * NET4:\tImplementation of BSD Unix domain sockets.\n *\n * Authors:\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n * Fixes:\n *\t\tLinus Torvalds\t:\tAssorted bug cures.\n *\t\tNiibe Yutaka\t:\tasync I/O support.\n *\t\tCarsten Paeth\t:\tPF_UNIX check, address fixes.\n *\t\tAlan Cox\t:\tLimit size of allocated blocks.\n *\t\tAlan Cox\t:\tFixed the stupid socketpair bug.\n *\t\tAlan Cox\t:\tBSD compatibility fine tuning.\n *\t\tAlan Cox\t:\tFixed a bug in connect when interrupted.\n *\t\tAlan Cox\t:\tSorted out a proper draft version of\n *\t\t\t\t\tfile descriptor passing hacked up from\n *\t\t\t\t\tMike Shaver's work.\n *\t\tMarty Leisner\t:\tFixes to fd passing\n *\t\tNick Nevin\t:\trecvmsg bugfix.\n *\t\tAlan Cox\t:\tStarted proper garbage collector\n *\t\tHeiko EiBfeldt\t:\tMissing verify_area check\n *\t\tAlan Cox\t:\tStarted POSIXisms\n *\t\tAndreas Schwab\t:\tReplace inode by dentry for proper\n *\t\t\t\t\treference counting\n *\t\tKirk Petersen\t:\tMade this a module\n *\t    Christoph Rohland\t:\tElegant non-blocking accept/connect algorithm.\n *\t\t\t\t\tLots of bug fixes.\n *\t     Alexey Kuznetosv\t:\tRepaired (I hope) bugs introduces\n *\t\t\t\t\tby above two patches.\n *\t     Andrea Arcangeli\t:\tIf possible we block in connect(2)\n *\t\t\t\t\tif the max backlog of the listen socket\n *\t\t\t\t\tis been reached. This won't break\n *\t\t\t\t\told apps and it will avoid huge amount\n *\t\t\t\t\tof socks hashed (this for unix_gc()\n *\t\t\t\t\tperformances reasons).\n *\t\t\t\t\tSecurity fix that limits the max\n *\t\t\t\t\tnumber of socks to 2*max_files and\n *\t\t\t\t\tthe number of skb queueable in the\n *\t\t\t\t\tdgram receiver.\n *\t\tArtur Skawina   :\tHash function optimizations\n *\t     Alexey Kuznetsov   :\tFull scale SMP. Lot of bugs are introduced 8)\n *\t      Malcolm Beattie   :\tSet peercred for socketpair\n *\t     Michal Ostrowski   :       Module initialization cleanup.\n *\t     Arnaldo C. Melo\t:\tRemove MOD_{INC,DEC}_USE_COUNT,\n *\t     \t\t\t\tthe core infrastructure is doing that\n *\t     \t\t\t\tfor all net proto families now (2.5.69+)\n *\n *\n * Known differences from reference BSD that was tested:\n *\n *\t[TO FIX]\n *\tECONNREFUSED is not returned from one end of a connected() socket to the\n *\t\tother the moment one end closes.\n *\tfstat() doesn't return st_dev=0, and give the blksize as high water mark\n *\t\tand a fake inode identifier (nor the BSD first socket fstat twice bug).\n *\t[NOT TO FIX]\n *\taccept() returns a path name even if the connecting socket has closed\n *\t\tin the meantime (BSD loses the path and gives up).\n *\taccept() returns 0 length path for an unbound connector. BSD returns 16\n *\t\tand a null first byte in the path (but not for gethost/peername - BSD bug ??)\n *\tsocketpair(...SOCK_RAW..) doesn't panic the kernel.\n *\tBSD af_unix apparently has connect forgetting to block properly.\n *\t\t(need to check this with the POSIX spec in detail)\n *\n * Differences from 2.0.0-11-... (ANK)\n *\tBug fixes and improvements.\n *\t\t- client shutdown killed server socket.\n *\t\t- removed all useless cli/sti pairs.\n *\n *\tSemantic changes/extensions.\n *\t\t- generic control message passing.\n *\t\t- SCM_CREDENTIALS control message.\n *\t\t- \"Abstract\" (not FS based) socket bindings.\n *\t\t  Abstract names are sequences of bytes (not zero terminated)\n *\t\t  started by 0, so that this name space does not intersect\n *\t\t  with BSD names.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/stat.h>\n#include <linux/dcache.h>\n#include <linux/namei.h>\n#include <linux/socket.h>\n#include <linux/un.h>\n#include <linux/fcntl.h>\n#include <linux/termios.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in.h>\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/tcp_states.h>\n#include <net/af_unix.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <net/scm.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/mount.h>\n#include <net/checksum.h>\n#include <linux/security.h>\n#include <linux/freezer.h>\n\nstruct hlist_head unix_socket_table[2 * UNIX_HASH_SIZE];\nEXPORT_SYMBOL_GPL(unix_socket_table);\nDEFINE_SPINLOCK(unix_table_lock);\nEXPORT_SYMBOL_GPL(unix_table_lock);\nstatic atomic_long_t unix_nr_socks;\n\n\nstatic struct hlist_head *unix_sockets_unbound(void *addr)\n{\n\tunsigned long hash = (unsigned long)addr;\n\n\thash ^= hash >> 16;\n\thash ^= hash >> 8;\n\thash %= UNIX_HASH_SIZE;\n\treturn &unix_socket_table[UNIX_HASH_SIZE + hash];\n}\n\n#define UNIX_ABSTRACT(sk)\t(unix_sk(sk)->addr->hash < UNIX_HASH_SIZE)\n\n#ifdef CONFIG_SECURITY_NETWORK\nstatic void unix_get_secdata(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\tUNIXCB(skb).secid = scm->secid;\n}\n\nstatic inline void unix_set_secdata(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\tscm->secid = UNIXCB(skb).secid;\n}\n\nstatic inline bool unix_secdata_eq(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\treturn (scm->secid == UNIXCB(skb).secid);\n}\n#else\nstatic inline void unix_get_secdata(struct scm_cookie *scm, struct sk_buff *skb)\n{ }\n\nstatic inline void unix_set_secdata(struct scm_cookie *scm, struct sk_buff *skb)\n{ }\n\nstatic inline bool unix_secdata_eq(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\treturn true;\n}\n#endif /* CONFIG_SECURITY_NETWORK */\n\n/*\n *  SMP locking strategy:\n *    hash table is protected with spinlock unix_table_lock\n *    each socket state is protected by separate spin lock.\n */\n\nstatic inline unsigned int unix_hash_fold(__wsum n)\n{\n\tunsigned int hash = (__force unsigned int)csum_fold(n);\n\n\thash ^= hash>>8;\n\treturn hash&(UNIX_HASH_SIZE-1);\n}\n\n#define unix_peer(sk) (unix_sk(sk)->peer)\n\nstatic inline int unix_our_peer(struct sock *sk, struct sock *osk)\n{\n\treturn unix_peer(osk) == sk;\n}\n\nstatic inline int unix_may_send(struct sock *sk, struct sock *osk)\n{\n\treturn unix_peer(osk) == NULL || unix_our_peer(sk, osk);\n}\n\nstatic inline int unix_recvq_full(struct sock const *sk)\n{\n\treturn skb_queue_len(&sk->sk_receive_queue) > sk->sk_max_ack_backlog;\n}\n\nstruct sock *unix_peer_get(struct sock *s)\n{\n\tstruct sock *peer;\n\n\tunix_state_lock(s);\n\tpeer = unix_peer(s);\n\tif (peer)\n\t\tsock_hold(peer);\n\tunix_state_unlock(s);\n\treturn peer;\n}\nEXPORT_SYMBOL_GPL(unix_peer_get);\n\nstatic inline void unix_release_addr(struct unix_address *addr)\n{\n\tif (atomic_dec_and_test(&addr->refcnt))\n\t\tkfree(addr);\n}\n\n/*\n *\tCheck unix socket name:\n *\t\t- should be not zero length.\n *\t        - if started by not zero, should be NULL terminated (FS object)\n *\t\t- if started by zero, it is abstract name.\n */\n\nstatic int unix_mkname(struct sockaddr_un *sunaddr, int len, unsigned int *hashp)\n{\n\tif (len <= sizeof(short) || len > sizeof(*sunaddr))\n\t\treturn -EINVAL;\n\tif (!sunaddr || sunaddr->sun_family != AF_UNIX)\n\t\treturn -EINVAL;\n\tif (sunaddr->sun_path[0]) {\n\t\t/*\n\t\t * This may look like an off by one error but it is a bit more\n\t\t * subtle. 108 is the longest valid AF_UNIX path for a binding.\n\t\t * sun_path[108] doesn't as such exist.  However in kernel space\n\t\t * we are guaranteed that it is a valid memory location in our\n\t\t * kernel address buffer.\n\t\t */\n\t\t((char *)sunaddr)[len] = 0;\n\t\tlen = strlen(sunaddr->sun_path)+1+sizeof(short);\n\t\treturn len;\n\t}\n\n\t*hashp = unix_hash_fold(csum_partial(sunaddr, len, 0));\n\treturn len;\n}\n\nstatic void __unix_remove_socket(struct sock *sk)\n{\n\tsk_del_node_init(sk);\n}\n\nstatic void __unix_insert_socket(struct hlist_head *list, struct sock *sk)\n{\n\tWARN_ON(!sk_unhashed(sk));\n\tsk_add_node(sk, list);\n}\n\nstatic inline void unix_remove_socket(struct sock *sk)\n{\n\tspin_lock(&unix_table_lock);\n\t__unix_remove_socket(sk);\n\tspin_unlock(&unix_table_lock);\n}\n\nstatic inline void unix_insert_socket(struct hlist_head *list, struct sock *sk)\n{\n\tspin_lock(&unix_table_lock);\n\t__unix_insert_socket(list, sk);\n\tspin_unlock(&unix_table_lock);\n}\n\nstatic struct sock *__unix_find_socket_byname(struct net *net,\n\t\t\t\t\t      struct sockaddr_un *sunname,\n\t\t\t\t\t      int len, int type, unsigned int hash)\n{\n\tstruct sock *s;\n\n\tsk_for_each(s, &unix_socket_table[hash ^ type]) {\n\t\tstruct unix_sock *u = unix_sk(s);\n\n\t\tif (!net_eq(sock_net(s), net))\n\t\t\tcontinue;\n\n\t\tif (u->addr->len == len &&\n\t\t    !memcmp(u->addr->name, sunname, len))\n\t\t\tgoto found;\n\t}\n\ts = NULL;\nfound:\n\treturn s;\n}\n\nstatic inline struct sock *unix_find_socket_byname(struct net *net,\n\t\t\t\t\t\t   struct sockaddr_un *sunname,\n\t\t\t\t\t\t   int len, int type,\n\t\t\t\t\t\t   unsigned int hash)\n{\n\tstruct sock *s;\n\n\tspin_lock(&unix_table_lock);\n\ts = __unix_find_socket_byname(net, sunname, len, type, hash);\n\tif (s)\n\t\tsock_hold(s);\n\tspin_unlock(&unix_table_lock);\n\treturn s;\n}\n\nstatic struct sock *unix_find_socket_byinode(struct inode *i)\n{\n\tstruct sock *s;\n\n\tspin_lock(&unix_table_lock);\n\tsk_for_each(s,\n\t\t    &unix_socket_table[i->i_ino & (UNIX_HASH_SIZE - 1)]) {\n\t\tstruct dentry *dentry = unix_sk(s)->path.dentry;\n\n\t\tif (dentry && d_backing_inode(dentry) == i) {\n\t\t\tsock_hold(s);\n\t\t\tgoto found;\n\t\t}\n\t}\n\ts = NULL;\nfound:\n\tspin_unlock(&unix_table_lock);\n\treturn s;\n}\n\n/* Support code for asymmetrically connected dgram sockets\n *\n * If a datagram socket is connected to a socket not itself connected\n * to the first socket (eg, /dev/log), clients may only enqueue more\n * messages if the present receive queue of the server socket is not\n * \"too large\". This means there's a second writeability condition\n * poll and sendmsg need to test. The dgram recv code will do a wake\n * up on the peer_wait wait queue of a socket upon reception of a\n * datagram which needs to be propagated to sleeping would-be writers\n * since these might not have sent anything so far. This can't be\n * accomplished via poll_wait because the lifetime of the server\n * socket might be less than that of its clients if these break their\n * association with it or if the server socket is closed while clients\n * are still connected to it and there's no way to inform \"a polling\n * implementation\" that it should let go of a certain wait queue\n *\n * In order to propagate a wake up, a wait_queue_t of the client\n * socket is enqueued on the peer_wait queue of the server socket\n * whose wake function does a wake_up on the ordinary client socket\n * wait queue. This connection is established whenever a write (or\n * poll for write) hit the flow control condition and broken when the\n * association to the server socket is dissolved or after a wake up\n * was relayed.\n */\n\nstatic int unix_dgram_peer_wake_relay(wait_queue_t *q, unsigned mode, int flags,\n\t\t\t\t      void *key)\n{\n\tstruct unix_sock *u;\n\twait_queue_head_t *u_sleep;\n\n\tu = container_of(q, struct unix_sock, peer_wake);\n\n\t__remove_wait_queue(&unix_sk(u->peer_wake.private)->peer_wait,\n\t\t\t    q);\n\tu->peer_wake.private = NULL;\n\n\t/* relaying can only happen while the wq still exists */\n\tu_sleep = sk_sleep(&u->sk);\n\tif (u_sleep)\n\t\twake_up_interruptible_poll(u_sleep, key);\n\n\treturn 0;\n}\n\nstatic int unix_dgram_peer_wake_connect(struct sock *sk, struct sock *other)\n{\n\tstruct unix_sock *u, *u_other;\n\tint rc;\n\n\tu = unix_sk(sk);\n\tu_other = unix_sk(other);\n\trc = 0;\n\tspin_lock(&u_other->peer_wait.lock);\n\n\tif (!u->peer_wake.private) {\n\t\tu->peer_wake.private = other;\n\t\t__add_wait_queue(&u_other->peer_wait, &u->peer_wake);\n\n\t\trc = 1;\n\t}\n\n\tspin_unlock(&u_other->peer_wait.lock);\n\treturn rc;\n}\n\nstatic void unix_dgram_peer_wake_disconnect(struct sock *sk,\n\t\t\t\t\t    struct sock *other)\n{\n\tstruct unix_sock *u, *u_other;\n\n\tu = unix_sk(sk);\n\tu_other = unix_sk(other);\n\tspin_lock(&u_other->peer_wait.lock);\n\n\tif (u->peer_wake.private == other) {\n\t\t__remove_wait_queue(&u_other->peer_wait, &u->peer_wake);\n\t\tu->peer_wake.private = NULL;\n\t}\n\n\tspin_unlock(&u_other->peer_wait.lock);\n}\n\nstatic void unix_dgram_peer_wake_disconnect_wakeup(struct sock *sk,\n\t\t\t\t\t\t   struct sock *other)\n{\n\tunix_dgram_peer_wake_disconnect(sk, other);\n\twake_up_interruptible_poll(sk_sleep(sk),\n\t\t\t\t   POLLOUT |\n\t\t\t\t   POLLWRNORM |\n\t\t\t\t   POLLWRBAND);\n}\n\n/* preconditions:\n *\t- unix_peer(sk) == other\n *\t- association is stable\n */\nstatic int unix_dgram_peer_wake_me(struct sock *sk, struct sock *other)\n{\n\tint connected;\n\n\tconnected = unix_dgram_peer_wake_connect(sk, other);\n\n\tif (unix_recvq_full(other))\n\t\treturn 1;\n\n\tif (connected)\n\t\tunix_dgram_peer_wake_disconnect(sk, other);\n\n\treturn 0;\n}\n\nstatic int unix_writable(const struct sock *sk)\n{\n\treturn sk->sk_state != TCP_LISTEN &&\n\t       (atomic_read(&sk->sk_wmem_alloc) << 2) <= sk->sk_sndbuf;\n}\n\nstatic void unix_write_space(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\tif (unix_writable(sk)) {\n\t\twq = rcu_dereference(sk->sk_wq);\n\t\tif (skwq_has_sleeper(wq))\n\t\t\twake_up_interruptible_sync_poll(&wq->wait,\n\t\t\t\tPOLLOUT | POLLWRNORM | POLLWRBAND);\n\t\tsk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT);\n\t}\n\trcu_read_unlock();\n}\n\n/* When dgram socket disconnects (or changes its peer), we clear its receive\n * queue of packets arrived from previous peer. First, it allows to do\n * flow control based only on wmem_alloc; second, sk connected to peer\n * may receive messages only from that peer. */\nstatic void unix_dgram_disconnected(struct sock *sk, struct sock *other)\n{\n\tif (!skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tskb_queue_purge(&sk->sk_receive_queue);\n\t\twake_up_interruptible_all(&unix_sk(sk)->peer_wait);\n\n\t\t/* If one link of bidirectional dgram pipe is disconnected,\n\t\t * we signal error. Messages are lost. Do not make this,\n\t\t * when peer was not connected to us.\n\t\t */\n\t\tif (!sock_flag(other, SOCK_DEAD) && unix_peer(other) == sk) {\n\t\t\tother->sk_err = ECONNRESET;\n\t\t\tother->sk_error_report(other);\n\t\t}\n\t}\n}\n\nstatic void unix_sock_destructor(struct sock *sk)\n{\n\tstruct unix_sock *u = unix_sk(sk);\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(!sk_unhashed(sk));\n\tWARN_ON(sk->sk_socket);\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_info(\"Attempt to release alive unix socket: %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tif (u->addr)\n\t\tunix_release_addr(u->addr);\n\n\tatomic_long_dec(&unix_nr_socks);\n\tlocal_bh_disable();\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\tlocal_bh_enable();\n#ifdef UNIX_REFCNT_DEBUG\n\tpr_debug(\"UNIX %p is destroyed, %ld are still alive.\\n\", sk,\n\t\tatomic_long_read(&unix_nr_socks));\n#endif\n}\n\nstatic void unix_release_sock(struct sock *sk, int embrion)\n{\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct path path;\n\tstruct sock *skpair;\n\tstruct sk_buff *skb;\n\tint state;\n\n\tunix_remove_socket(sk);\n\n\t/* Clear state */\n\tunix_state_lock(sk);\n\tsock_orphan(sk);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\tpath\t     = u->path;\n\tu->path.dentry = NULL;\n\tu->path.mnt = NULL;\n\tstate = sk->sk_state;\n\tsk->sk_state = TCP_CLOSE;\n\tunix_state_unlock(sk);\n\n\twake_up_interruptible_all(&u->peer_wait);\n\n\tskpair = unix_peer(sk);\n\n\tif (skpair != NULL) {\n\t\tif (sk->sk_type == SOCK_STREAM || sk->sk_type == SOCK_SEQPACKET) {\n\t\t\tunix_state_lock(skpair);\n\t\t\t/* No more writes */\n\t\t\tskpair->sk_shutdown = SHUTDOWN_MASK;\n\t\t\tif (!skb_queue_empty(&sk->sk_receive_queue) || embrion)\n\t\t\t\tskpair->sk_err = ECONNRESET;\n\t\t\tunix_state_unlock(skpair);\n\t\t\tskpair->sk_state_change(skpair);\n\t\t\tsk_wake_async(skpair, SOCK_WAKE_WAITD, POLL_HUP);\n\t\t}\n\n\t\tunix_dgram_peer_wake_disconnect(sk, skpair);\n\t\tsock_put(skpair); /* It may now die */\n\t\tunix_peer(sk) = NULL;\n\t}\n\n\t/* Try to flush out this socket. Throw out buffers at least */\n\n\twhile ((skb = skb_dequeue(&sk->sk_receive_queue)) != NULL) {\n\t\tif (state == TCP_LISTEN)\n\t\t\tunix_release_sock(skb->sk, 1);\n\t\t/* passed fds are erased in the kfree_skb hook\t      */\n\t\tUNIXCB(skb).consumed = skb->len;\n\t\tkfree_skb(skb);\n\t}\n\n\tif (path.dentry)\n\t\tpath_put(&path);\n\n\tsock_put(sk);\n\n\t/* ---- Socket is dead now and most probably destroyed ---- */\n\n\t/*\n\t * Fixme: BSD difference: In BSD all sockets connected to us get\n\t *\t  ECONNRESET and we die on the spot. In Linux we behave\n\t *\t  like files and pipes do and wait for the last\n\t *\t  dereference.\n\t *\n\t * Can't we simply set sock->err?\n\t *\n\t *\t  What the above comment does talk about? --ANK(980817)\n\t */\n\n\tif (unix_tot_inflight)\n\t\tunix_gc();\t\t/* Garbage collect fds */\n}\n\nstatic void init_peercred(struct sock *sk)\n{\n\tput_pid(sk->sk_peer_pid);\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tsk->sk_peer_pid  = get_pid(task_tgid(current));\n\tsk->sk_peer_cred = get_current_cred();\n}\n\nstatic void copy_peercred(struct sock *sk, struct sock *peersk)\n{\n\tput_pid(sk->sk_peer_pid);\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tsk->sk_peer_pid  = get_pid(peersk->sk_peer_pid);\n\tsk->sk_peer_cred = get_cred(peersk->sk_peer_cred);\n}\n\nstatic int unix_listen(struct socket *sock, int backlog)\n{\n\tint err;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct pid *old_pid = NULL;\n\n\terr = -EOPNOTSUPP;\n\tif (sock->type != SOCK_STREAM && sock->type != SOCK_SEQPACKET)\n\t\tgoto out;\t/* Only stream/seqpacket sockets accept */\n\terr = -EINVAL;\n\tif (!u->addr)\n\t\tgoto out;\t/* No listens on an unbound socket */\n\tunix_state_lock(sk);\n\tif (sk->sk_state != TCP_CLOSE && sk->sk_state != TCP_LISTEN)\n\t\tgoto out_unlock;\n\tif (backlog > sk->sk_max_ack_backlog)\n\t\twake_up_interruptible_all(&u->peer_wait);\n\tsk->sk_max_ack_backlog\t= backlog;\n\tsk->sk_state\t\t= TCP_LISTEN;\n\t/* set credentials so connect can copy them */\n\tinit_peercred(sk);\n\terr = 0;\n\nout_unlock:\n\tunix_state_unlock(sk);\n\tput_pid(old_pid);\nout:\n\treturn err;\n}\n\nstatic int unix_release(struct socket *);\nstatic int unix_bind(struct socket *, struct sockaddr *, int);\nstatic int unix_stream_connect(struct socket *, struct sockaddr *,\n\t\t\t       int addr_len, int flags);\nstatic int unix_socketpair(struct socket *, struct socket *);\nstatic int unix_accept(struct socket *, struct socket *, int);\nstatic int unix_getname(struct socket *, struct sockaddr *, int *, int);\nstatic unsigned int unix_poll(struct file *, struct socket *, poll_table *);\nstatic unsigned int unix_dgram_poll(struct file *, struct socket *,\n\t\t\t\t    poll_table *);\nstatic int unix_ioctl(struct socket *, unsigned int, unsigned long);\nstatic int unix_shutdown(struct socket *, int);\nstatic int unix_stream_sendmsg(struct socket *, struct msghdr *, size_t);\nstatic int unix_stream_recvmsg(struct socket *, struct msghdr *, size_t, int);\nstatic ssize_t unix_stream_sendpage(struct socket *, struct page *, int offset,\n\t\t\t\t    size_t size, int flags);\nstatic ssize_t unix_stream_splice_read(struct socket *,  loff_t *ppos,\n\t\t\t\t       struct pipe_inode_info *, size_t size,\n\t\t\t\t       unsigned int flags);\nstatic int unix_dgram_sendmsg(struct socket *, struct msghdr *, size_t);\nstatic int unix_dgram_recvmsg(struct socket *, struct msghdr *, size_t, int);\nstatic int unix_dgram_connect(struct socket *, struct sockaddr *,\n\t\t\t      int, int);\nstatic int unix_seqpacket_sendmsg(struct socket *, struct msghdr *, size_t);\nstatic int unix_seqpacket_recvmsg(struct socket *, struct msghdr *, size_t,\n\t\t\t\t  int);\n\nstatic int unix_set_peek_off(struct sock *sk, int val)\n{\n\tstruct unix_sock *u = unix_sk(sk);\n\n\tif (mutex_lock_interruptible(&u->readlock))\n\t\treturn -EINTR;\n\n\tsk->sk_peek_off = val;\n\tmutex_unlock(&u->readlock);\n\n\treturn 0;\n}\n\n\nstatic const struct proto_ops unix_stream_ops = {\n\t.family =\tPF_UNIX,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tunix_release,\n\t.bind =\t\tunix_bind,\n\t.connect =\tunix_stream_connect,\n\t.socketpair =\tunix_socketpair,\n\t.accept =\tunix_accept,\n\t.getname =\tunix_getname,\n\t.poll =\t\tunix_poll,\n\t.ioctl =\tunix_ioctl,\n\t.listen =\tunix_listen,\n\t.shutdown =\tunix_shutdown,\n\t.setsockopt =\tsock_no_setsockopt,\n\t.getsockopt =\tsock_no_getsockopt,\n\t.sendmsg =\tunix_stream_sendmsg,\n\t.recvmsg =\tunix_stream_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tunix_stream_sendpage,\n\t.splice_read =\tunix_stream_splice_read,\n\t.set_peek_off =\tunix_set_peek_off,\n};\n\nstatic const struct proto_ops unix_dgram_ops = {\n\t.family =\tPF_UNIX,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tunix_release,\n\t.bind =\t\tunix_bind,\n\t.connect =\tunix_dgram_connect,\n\t.socketpair =\tunix_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tunix_getname,\n\t.poll =\t\tunix_dgram_poll,\n\t.ioctl =\tunix_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tunix_shutdown,\n\t.setsockopt =\tsock_no_setsockopt,\n\t.getsockopt =\tsock_no_getsockopt,\n\t.sendmsg =\tunix_dgram_sendmsg,\n\t.recvmsg =\tunix_dgram_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tsock_no_sendpage,\n\t.set_peek_off =\tunix_set_peek_off,\n};\n\nstatic const struct proto_ops unix_seqpacket_ops = {\n\t.family =\tPF_UNIX,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tunix_release,\n\t.bind =\t\tunix_bind,\n\t.connect =\tunix_stream_connect,\n\t.socketpair =\tunix_socketpair,\n\t.accept =\tunix_accept,\n\t.getname =\tunix_getname,\n\t.poll =\t\tunix_dgram_poll,\n\t.ioctl =\tunix_ioctl,\n\t.listen =\tunix_listen,\n\t.shutdown =\tunix_shutdown,\n\t.setsockopt =\tsock_no_setsockopt,\n\t.getsockopt =\tsock_no_getsockopt,\n\t.sendmsg =\tunix_seqpacket_sendmsg,\n\t.recvmsg =\tunix_seqpacket_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tsock_no_sendpage,\n\t.set_peek_off =\tunix_set_peek_off,\n};\n\nstatic struct proto unix_proto = {\n\t.name\t\t\t= \"UNIX\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.obj_size\t\t= sizeof(struct unix_sock),\n};\n\n/*\n * AF_UNIX sockets do not interact with hardware, hence they\n * dont trigger interrupts - so it's safe for them to have\n * bh-unsafe locking for their sk_receive_queue.lock. Split off\n * this special lock-class by reinitializing the spinlock key:\n */\nstatic struct lock_class_key af_unix_sk_receive_queue_lock_key;\n\nstatic struct sock *unix_create1(struct net *net, struct socket *sock, int kern)\n{\n\tstruct sock *sk = NULL;\n\tstruct unix_sock *u;\n\n\tatomic_long_inc(&unix_nr_socks);\n\tif (atomic_long_read(&unix_nr_socks) > 2 * get_max_files())\n\t\tgoto out;\n\n\tsk = sk_alloc(net, PF_UNIX, GFP_KERNEL, &unix_proto, kern);\n\tif (!sk)\n\t\tgoto out;\n\n\tsock_init_data(sock, sk);\n\tlockdep_set_class(&sk->sk_receive_queue.lock,\n\t\t\t\t&af_unix_sk_receive_queue_lock_key);\n\n\tsk->sk_write_space\t= unix_write_space;\n\tsk->sk_max_ack_backlog\t= net->unx.sysctl_max_dgram_qlen;\n\tsk->sk_destruct\t\t= unix_sock_destructor;\n\tu\t  = unix_sk(sk);\n\tu->path.dentry = NULL;\n\tu->path.mnt = NULL;\n\tspin_lock_init(&u->lock);\n\tatomic_long_set(&u->inflight, 0);\n\tINIT_LIST_HEAD(&u->link);\n\tmutex_init(&u->readlock); /* single task reading lock */\n\tinit_waitqueue_head(&u->peer_wait);\n\tinit_waitqueue_func_entry(&u->peer_wake, unix_dgram_peer_wake_relay);\n\tunix_insert_socket(unix_sockets_unbound(sk), sk);\nout:\n\tif (sk == NULL)\n\t\tatomic_long_dec(&unix_nr_socks);\n\telse {\n\t\tlocal_bh_disable();\n\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n\t\tlocal_bh_enable();\n\t}\n\treturn sk;\n}\n\nstatic int unix_create(struct net *net, struct socket *sock, int protocol,\n\t\t       int kern)\n{\n\tif (protocol && protocol != PF_UNIX)\n\t\treturn -EPROTONOSUPPORT;\n\n\tsock->state = SS_UNCONNECTED;\n\n\tswitch (sock->type) {\n\tcase SOCK_STREAM:\n\t\tsock->ops = &unix_stream_ops;\n\t\tbreak;\n\t\t/*\n\t\t *\tBelieve it or not BSD has AF_UNIX, SOCK_RAW though\n\t\t *\tnothing uses it.\n\t\t */\n\tcase SOCK_RAW:\n\t\tsock->type = SOCK_DGRAM;\n\tcase SOCK_DGRAM:\n\t\tsock->ops = &unix_dgram_ops;\n\t\tbreak;\n\tcase SOCK_SEQPACKET:\n\t\tsock->ops = &unix_seqpacket_ops;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\treturn unix_create1(net, sock, kern) ? 0 : -ENOMEM;\n}\n\nstatic int unix_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tunix_release_sock(sk, 0);\n\tsock->sk = NULL;\n\n\treturn 0;\n}\n\nstatic int unix_autobind(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct unix_sock *u = unix_sk(sk);\n\tstatic u32 ordernum = 1;\n\tstruct unix_address *addr;\n\tint err;\n\tunsigned int retries = 0;\n\n\terr = mutex_lock_interruptible(&u->readlock);\n\tif (err)\n\t\treturn err;\n\n\terr = 0;\n\tif (u->addr)\n\t\tgoto out;\n\n\terr = -ENOMEM;\n\taddr = kzalloc(sizeof(*addr) + sizeof(short) + 16, GFP_KERNEL);\n\tif (!addr)\n\t\tgoto out;\n\n\taddr->name->sun_family = AF_UNIX;\n\tatomic_set(&addr->refcnt, 1);\n\nretry:\n\taddr->len = sprintf(addr->name->sun_path+1, \"%05x\", ordernum) + 1 + sizeof(short);\n\taddr->hash = unix_hash_fold(csum_partial(addr->name, addr->len, 0));\n\n\tspin_lock(&unix_table_lock);\n\tordernum = (ordernum+1)&0xFFFFF;\n\n\tif (__unix_find_socket_byname(net, addr->name, addr->len, sock->type,\n\t\t\t\t      addr->hash)) {\n\t\tspin_unlock(&unix_table_lock);\n\t\t/*\n\t\t * __unix_find_socket_byname() may take long time if many names\n\t\t * are already in use.\n\t\t */\n\t\tcond_resched();\n\t\t/* Give up if all names seems to be in use. */\n\t\tif (retries++ == 0xFFFFF) {\n\t\t\terr = -ENOSPC;\n\t\t\tkfree(addr);\n\t\t\tgoto out;\n\t\t}\n\t\tgoto retry;\n\t}\n\taddr->hash ^= sk->sk_type;\n\n\t__unix_remove_socket(sk);\n\tu->addr = addr;\n\t__unix_insert_socket(&unix_socket_table[addr->hash], sk);\n\tspin_unlock(&unix_table_lock);\n\terr = 0;\n\nout:\tmutex_unlock(&u->readlock);\n\treturn err;\n}\n\nstatic struct sock *unix_find_other(struct net *net,\n\t\t\t\t    struct sockaddr_un *sunname, int len,\n\t\t\t\t    int type, unsigned int hash, int *error)\n{\n\tstruct sock *u;\n\tstruct path path;\n\tint err = 0;\n\n\tif (sunname->sun_path[0]) {\n\t\tstruct inode *inode;\n\t\terr = kern_path(sunname->sun_path, LOOKUP_FOLLOW, &path);\n\t\tif (err)\n\t\t\tgoto fail;\n\t\tinode = d_backing_inode(path.dentry);\n\t\terr = inode_permission(inode, MAY_WRITE);\n\t\tif (err)\n\t\t\tgoto put_fail;\n\n\t\terr = -ECONNREFUSED;\n\t\tif (!S_ISSOCK(inode->i_mode))\n\t\t\tgoto put_fail;\n\t\tu = unix_find_socket_byinode(inode);\n\t\tif (!u)\n\t\t\tgoto put_fail;\n\n\t\tif (u->sk_type == type)\n\t\t\ttouch_atime(&path);\n\n\t\tpath_put(&path);\n\n\t\terr = -EPROTOTYPE;\n\t\tif (u->sk_type != type) {\n\t\t\tsock_put(u);\n\t\t\tgoto fail;\n\t\t}\n\t} else {\n\t\terr = -ECONNREFUSED;\n\t\tu = unix_find_socket_byname(net, sunname, len, type, hash);\n\t\tif (u) {\n\t\t\tstruct dentry *dentry;\n\t\t\tdentry = unix_sk(u)->path.dentry;\n\t\t\tif (dentry)\n\t\t\t\ttouch_atime(&unix_sk(u)->path);\n\t\t} else\n\t\t\tgoto fail;\n\t}\n\treturn u;\n\nput_fail:\n\tpath_put(&path);\nfail:\n\t*error = err;\n\treturn NULL;\n}\n\nstatic int unix_mknod(struct dentry *dentry, struct path *path, umode_t mode,\n\t\t      struct path *res)\n{\n\tint err;\n\n\terr = security_path_mknod(path, dentry, mode, 0);\n\tif (!err) {\n\t\terr = vfs_mknod(d_inode(path->dentry), dentry, mode, 0);\n\t\tif (!err) {\n\t\t\tres->mnt = mntget(path->mnt);\n\t\t\tres->dentry = dget(dentry);\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic int unix_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sockaddr_un *sunaddr = (struct sockaddr_un *)uaddr;\n\tchar *sun_path = sunaddr->sun_path;\n\tint err, name_err;\n\tunsigned int hash;\n\tstruct unix_address *addr;\n\tstruct hlist_head *list;\n\tstruct path path;\n\tstruct dentry *dentry;\n\n\terr = -EINVAL;\n\tif (sunaddr->sun_family != AF_UNIX)\n\t\tgoto out;\n\n\tif (addr_len == sizeof(short)) {\n\t\terr = unix_autobind(sock);\n\t\tgoto out;\n\t}\n\n\terr = unix_mkname(sunaddr, addr_len, &hash);\n\tif (err < 0)\n\t\tgoto out;\n\taddr_len = err;\n\n\tname_err = 0;\n\tdentry = NULL;\n\tif (sun_path[0]) {\n\t\t/* Get the parent directory, calculate the hash for last\n\t\t * component.\n\t\t */\n\t\tdentry = kern_path_create(AT_FDCWD, sun_path, &path, 0);\n\n\t\tif (IS_ERR(dentry)) {\n\t\t\t/* delay report until after 'already bound' check */\n\t\t\tname_err = PTR_ERR(dentry);\n\t\t\tdentry = NULL;\n\t\t}\n\t}\n\n\terr = mutex_lock_interruptible(&u->readlock);\n\tif (err)\n\t\tgoto out_path;\n\n\terr = -EINVAL;\n\tif (u->addr)\n\t\tgoto out_up;\n\n\tif (name_err) {\n\t\terr = name_err == -EEXIST ? -EADDRINUSE : name_err;\n\t\tgoto out_up;\n\t}\n\n\terr = -ENOMEM;\n\taddr = kmalloc(sizeof(*addr)+addr_len, GFP_KERNEL);\n\tif (!addr)\n\t\tgoto out_up;\n\n\tmemcpy(addr->name, sunaddr, addr_len);\n\taddr->len = addr_len;\n\taddr->hash = hash ^ sk->sk_type;\n\tatomic_set(&addr->refcnt, 1);\n\n\tif (dentry) {\n\t\tstruct path u_path;\n\t\tumode_t mode = S_IFSOCK |\n\t\t       (SOCK_INODE(sock)->i_mode & ~current_umask());\n\t\terr = unix_mknod(dentry, &path, mode, &u_path);\n\t\tif (err) {\n\t\t\tif (err == -EEXIST)\n\t\t\t\terr = -EADDRINUSE;\n\t\t\tunix_release_addr(addr);\n\t\t\tgoto out_up;\n\t\t}\n\t\taddr->hash = UNIX_HASH_SIZE;\n\t\thash = d_backing_inode(dentry)->i_ino & (UNIX_HASH_SIZE - 1);\n\t\tspin_lock(&unix_table_lock);\n\t\tu->path = u_path;\n\t\tlist = &unix_socket_table[hash];\n\t} else {\n\t\tspin_lock(&unix_table_lock);\n\t\terr = -EADDRINUSE;\n\t\tif (__unix_find_socket_byname(net, sunaddr, addr_len,\n\t\t\t\t\t      sk->sk_type, hash)) {\n\t\t\tunix_release_addr(addr);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tlist = &unix_socket_table[addr->hash];\n\t}\n\n\terr = 0;\n\t__unix_remove_socket(sk);\n\tu->addr = addr;\n\t__unix_insert_socket(list, sk);\n\nout_unlock:\n\tspin_unlock(&unix_table_lock);\nout_up:\n\tmutex_unlock(&u->readlock);\nout_path:\n\tif (dentry)\n\t\tdone_path_create(&path, dentry);\n\nout:\n\treturn err;\n}\n\nstatic void unix_state_double_lock(struct sock *sk1, struct sock *sk2)\n{\n\tif (unlikely(sk1 == sk2) || !sk2) {\n\t\tunix_state_lock(sk1);\n\t\treturn;\n\t}\n\tif (sk1 < sk2) {\n\t\tunix_state_lock(sk1);\n\t\tunix_state_lock_nested(sk2);\n\t} else {\n\t\tunix_state_lock(sk2);\n\t\tunix_state_lock_nested(sk1);\n\t}\n}\n\nstatic void unix_state_double_unlock(struct sock *sk1, struct sock *sk2)\n{\n\tif (unlikely(sk1 == sk2) || !sk2) {\n\t\tunix_state_unlock(sk1);\n\t\treturn;\n\t}\n\tunix_state_unlock(sk1);\n\tunix_state_unlock(sk2);\n}\n\nstatic int unix_dgram_connect(struct socket *sock, struct sockaddr *addr,\n\t\t\t      int alen, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct sockaddr_un *sunaddr = (struct sockaddr_un *)addr;\n\tstruct sock *other;\n\tunsigned int hash;\n\tint err;\n\n\tif (addr->sa_family != AF_UNSPEC) {\n\t\terr = unix_mkname(sunaddr, alen, &hash);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\talen = err;\n\n\t\tif (test_bit(SOCK_PASSCRED, &sock->flags) &&\n\t\t    !unix_sk(sk)->addr && (err = unix_autobind(sock)) != 0)\n\t\t\tgoto out;\n\nrestart:\n\t\tother = unix_find_other(net, sunaddr, alen, sock->type, hash, &err);\n\t\tif (!other)\n\t\t\tgoto out;\n\n\t\tunix_state_double_lock(sk, other);\n\n\t\t/* Apparently VFS overslept socket death. Retry. */\n\t\tif (sock_flag(other, SOCK_DEAD)) {\n\t\t\tunix_state_double_unlock(sk, other);\n\t\t\tsock_put(other);\n\t\t\tgoto restart;\n\t\t}\n\n\t\terr = -EPERM;\n\t\tif (!unix_may_send(sk, other))\n\t\t\tgoto out_unlock;\n\n\t\terr = security_unix_may_send(sk->sk_socket, other->sk_socket);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\n\t} else {\n\t\t/*\n\t\t *\t1003.1g breaking connected state with AF_UNSPEC\n\t\t */\n\t\tother = NULL;\n\t\tunix_state_double_lock(sk, other);\n\t}\n\n\t/*\n\t * If it was connected, reconnect.\n\t */\n\tif (unix_peer(sk)) {\n\t\tstruct sock *old_peer = unix_peer(sk);\n\t\tunix_peer(sk) = other;\n\t\tunix_dgram_peer_wake_disconnect_wakeup(sk, old_peer);\n\n\t\tunix_state_double_unlock(sk, other);\n\n\t\tif (other != old_peer)\n\t\t\tunix_dgram_disconnected(sk, old_peer);\n\t\tsock_put(old_peer);\n\t} else {\n\t\tunix_peer(sk) = other;\n\t\tunix_state_double_unlock(sk, other);\n\t}\n\treturn 0;\n\nout_unlock:\n\tunix_state_double_unlock(sk, other);\n\tsock_put(other);\nout:\n\treturn err;\n}\n\nstatic long unix_wait_for_peer(struct sock *other, long timeo)\n{\n\tstruct unix_sock *u = unix_sk(other);\n\tint sched;\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait_exclusive(&u->peer_wait, &wait, TASK_INTERRUPTIBLE);\n\n\tsched = !sock_flag(other, SOCK_DEAD) &&\n\t\t!(other->sk_shutdown & RCV_SHUTDOWN) &&\n\t\tunix_recvq_full(other);\n\n\tunix_state_unlock(other);\n\n\tif (sched)\n\t\ttimeo = schedule_timeout(timeo);\n\n\tfinish_wait(&u->peer_wait, &wait);\n\treturn timeo;\n}\n\nstatic int unix_stream_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t       int addr_len, int flags)\n{\n\tstruct sockaddr_un *sunaddr = (struct sockaddr_un *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct unix_sock *u = unix_sk(sk), *newu, *otheru;\n\tstruct sock *newsk = NULL;\n\tstruct sock *other = NULL;\n\tstruct sk_buff *skb = NULL;\n\tunsigned int hash;\n\tint st;\n\tint err;\n\tlong timeo;\n\n\terr = unix_mkname(sunaddr, addr_len, &hash);\n\tif (err < 0)\n\t\tgoto out;\n\taddr_len = err;\n\n\tif (test_bit(SOCK_PASSCRED, &sock->flags) && !u->addr &&\n\t    (err = unix_autobind(sock)) != 0)\n\t\tgoto out;\n\n\ttimeo = sock_sndtimeo(sk, flags & O_NONBLOCK);\n\n\t/* First of all allocate resources.\n\t   If we will make it after state is locked,\n\t   we will have to recheck all again in any case.\n\t */\n\n\terr = -ENOMEM;\n\n\t/* create new sock for complete connection */\n\tnewsk = unix_create1(sock_net(sk), NULL, 0);\n\tif (newsk == NULL)\n\t\tgoto out;\n\n\t/* Allocate skb for sending to listening sock */\n\tskb = sock_wmalloc(newsk, 1, 0, GFP_KERNEL);\n\tif (skb == NULL)\n\t\tgoto out;\n\nrestart:\n\t/*  Find listening sock. */\n\tother = unix_find_other(net, sunaddr, addr_len, sk->sk_type, hash, &err);\n\tif (!other)\n\t\tgoto out;\n\n\t/* Latch state of peer */\n\tunix_state_lock(other);\n\n\t/* Apparently VFS overslept socket death. Retry. */\n\tif (sock_flag(other, SOCK_DEAD)) {\n\t\tunix_state_unlock(other);\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\terr = -ECONNREFUSED;\n\tif (other->sk_state != TCP_LISTEN)\n\t\tgoto out_unlock;\n\tif (other->sk_shutdown & RCV_SHUTDOWN)\n\t\tgoto out_unlock;\n\n\tif (unix_recvq_full(other)) {\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto out_unlock;\n\n\t\ttimeo = unix_wait_for_peer(other, timeo);\n\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\t/* Latch our state.\n\n\t   It is tricky place. We need to grab our state lock and cannot\n\t   drop lock on peer. It is dangerous because deadlock is\n\t   possible. Connect to self case and simultaneous\n\t   attempt to connect are eliminated by checking socket\n\t   state. other is TCP_LISTEN, if sk is TCP_LISTEN we\n\t   check this before attempt to grab lock.\n\n\t   Well, and we have to recheck the state after socket locked.\n\t */\n\tst = sk->sk_state;\n\n\tswitch (st) {\n\tcase TCP_CLOSE:\n\t\t/* This is ok... continue with connect */\n\t\tbreak;\n\tcase TCP_ESTABLISHED:\n\t\t/* Socket is already connected */\n\t\terr = -EISCONN;\n\t\tgoto out_unlock;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tunix_state_lock_nested(sk);\n\n\tif (sk->sk_state != st) {\n\t\tunix_state_unlock(sk);\n\t\tunix_state_unlock(other);\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\terr = security_unix_stream_connect(sk, other, newsk);\n\tif (err) {\n\t\tunix_state_unlock(sk);\n\t\tgoto out_unlock;\n\t}\n\n\t/* The way is open! Fastly set all the necessary fields... */\n\n\tsock_hold(sk);\n\tunix_peer(newsk)\t= sk;\n\tnewsk->sk_state\t\t= TCP_ESTABLISHED;\n\tnewsk->sk_type\t\t= sk->sk_type;\n\tinit_peercred(newsk);\n\tnewu = unix_sk(newsk);\n\tRCU_INIT_POINTER(newsk->sk_wq, &newu->peer_wq);\n\totheru = unix_sk(other);\n\n\t/* copy address information from listening to new sock*/\n\tif (otheru->addr) {\n\t\tatomic_inc(&otheru->addr->refcnt);\n\t\tnewu->addr = otheru->addr;\n\t}\n\tif (otheru->path.dentry) {\n\t\tpath_get(&otheru->path);\n\t\tnewu->path = otheru->path;\n\t}\n\n\t/* Set credentials */\n\tcopy_peercred(sk, other);\n\n\tsock->state\t= SS_CONNECTED;\n\tsk->sk_state\t= TCP_ESTABLISHED;\n\tsock_hold(newsk);\n\n\tsmp_mb__after_atomic();\t/* sock_hold() does an atomic_inc() */\n\tunix_peer(sk)\t= newsk;\n\n\tunix_state_unlock(sk);\n\n\t/* take ten and and send info to listening sock */\n\tspin_lock(&other->sk_receive_queue.lock);\n\t__skb_queue_tail(&other->sk_receive_queue, skb);\n\tspin_unlock(&other->sk_receive_queue.lock);\n\tunix_state_unlock(other);\n\tother->sk_data_ready(other);\n\tsock_put(other);\n\treturn 0;\n\nout_unlock:\n\tif (other)\n\t\tunix_state_unlock(other);\n\nout:\n\tkfree_skb(skb);\n\tif (newsk)\n\t\tunix_release_sock(newsk, 0);\n\tif (other)\n\t\tsock_put(other);\n\treturn err;\n}\n\nstatic int unix_socketpair(struct socket *socka, struct socket *sockb)\n{\n\tstruct sock *ska = socka->sk, *skb = sockb->sk;\n\n\t/* Join our sockets back to back */\n\tsock_hold(ska);\n\tsock_hold(skb);\n\tunix_peer(ska) = skb;\n\tunix_peer(skb) = ska;\n\tinit_peercred(ska);\n\tinit_peercred(skb);\n\n\tif (ska->sk_type != SOCK_DGRAM) {\n\t\tska->sk_state = TCP_ESTABLISHED;\n\t\tskb->sk_state = TCP_ESTABLISHED;\n\t\tsocka->state  = SS_CONNECTED;\n\t\tsockb->state  = SS_CONNECTED;\n\t}\n\treturn 0;\n}\n\nstatic void unix_sock_inherit_flags(const struct socket *old,\n\t\t\t\t    struct socket *new)\n{\n\tif (test_bit(SOCK_PASSCRED, &old->flags))\n\t\tset_bit(SOCK_PASSCRED, &new->flags);\n\tif (test_bit(SOCK_PASSSEC, &old->flags))\n\t\tset_bit(SOCK_PASSSEC, &new->flags);\n}\n\nstatic int unix_accept(struct socket *sock, struct socket *newsock, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sock *tsk;\n\tstruct sk_buff *skb;\n\tint err;\n\n\terr = -EOPNOTSUPP;\n\tif (sock->type != SOCK_STREAM && sock->type != SOCK_SEQPACKET)\n\t\tgoto out;\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_LISTEN)\n\t\tgoto out;\n\n\t/* If socket state is TCP_LISTEN it cannot change (for now...),\n\t * so that no locks are necessary.\n\t */\n\n\tskb = skb_recv_datagram(sk, 0, flags&O_NONBLOCK, &err);\n\tif (!skb) {\n\t\t/* This means receive shutdown. */\n\t\tif (err == 0)\n\t\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttsk = skb->sk;\n\tskb_free_datagram(sk, skb);\n\twake_up_interruptible(&unix_sk(sk)->peer_wait);\n\n\t/* attach accepted sock to socket */\n\tunix_state_lock(tsk);\n\tnewsock->state = SS_CONNECTED;\n\tunix_sock_inherit_flags(sock, newsock);\n\tsock_graft(tsk, newsock);\n\tunix_state_unlock(tsk);\n\treturn 0;\n\nout:\n\treturn err;\n}\n\n\nstatic int unix_getname(struct socket *sock, struct sockaddr *uaddr, int *uaddr_len, int peer)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u;\n\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr, uaddr);\n\tint err = 0;\n\n\tif (peer) {\n\t\tsk = unix_peer_get(sk);\n\n\t\terr = -ENOTCONN;\n\t\tif (!sk)\n\t\t\tgoto out;\n\t\terr = 0;\n\t} else {\n\t\tsock_hold(sk);\n\t}\n\n\tu = unix_sk(sk);\n\tunix_state_lock(sk);\n\tif (!u->addr) {\n\t\tsunaddr->sun_family = AF_UNIX;\n\t\tsunaddr->sun_path[0] = 0;\n\t\t*uaddr_len = sizeof(short);\n\t} else {\n\t\tstruct unix_address *addr = u->addr;\n\n\t\t*uaddr_len = addr->len;\n\t\tmemcpy(sunaddr, addr->name, *uaddr_len);\n\t}\n\tunix_state_unlock(sk);\n\tsock_put(sk);\nout:\n\treturn err;\n}\n\nstatic void unix_detach_fds(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\tint i;\n\n\tscm->fp = UNIXCB(skb).fp;\n\tUNIXCB(skb).fp = NULL;\n\n\tfor (i = scm->fp->count-1; i >= 0; i--)\n\t\tunix_notinflight(scm->fp->fp[i]);\n}\n\nstatic void unix_destruct_scm(struct sk_buff *skb)\n{\n\tstruct scm_cookie scm;\n\tmemset(&scm, 0, sizeof(scm));\n\tscm.pid  = UNIXCB(skb).pid;\n\tif (UNIXCB(skb).fp)\n\t\tunix_detach_fds(&scm, skb);\n\n\t/* Alas, it calls VFS */\n\t/* So fscking what? fput() had been SMP-safe since the last Summer */\n\tscm_destroy(&scm);\n\tsock_wfree(skb);\n}\n\n/*\n * The \"user->unix_inflight\" variable is protected by the garbage\n * collection lock, and we just read it locklessly here. If you go\n * over the limit, there might be a tiny race in actually noticing\n * it across threads. Tough.\n */\nstatic inline bool too_many_unix_fds(struct task_struct *p)\n{\n\tstruct user_struct *user = current_user();\n\n\tif (unlikely(user->unix_inflight > task_rlimit(p, RLIMIT_NOFILE)))\n\t\treturn !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN);\n\treturn false;\n}\n\n#define MAX_RECURSION_LEVEL 4\n\nstatic int unix_attach_fds(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\tint i;\n\tunsigned char max_level = 0;\n\tint unix_sock_count = 0;\n\n\tif (too_many_unix_fds(current))\n\t\treturn -ETOOMANYREFS;\n\n\tfor (i = scm->fp->count - 1; i >= 0; i--) {\n\t\tstruct sock *sk = unix_get_socket(scm->fp->fp[i]);\n\n\t\tif (sk) {\n\t\t\tunix_sock_count++;\n\t\t\tmax_level = max(max_level,\n\t\t\t\t\tunix_sk(sk)->recursion_level);\n\t\t}\n\t}\n\tif (unlikely(max_level > MAX_RECURSION_LEVEL))\n\t\treturn -ETOOMANYREFS;\n\n\t/*\n\t * Need to duplicate file references for the sake of garbage\n\t * collection.  Otherwise a socket in the fps might become a\n\t * candidate for GC while the skb is not yet queued.\n\t */\n\tUNIXCB(skb).fp = scm_fp_dup(scm->fp);\n\tif (!UNIXCB(skb).fp)\n\t\treturn -ENOMEM;\n\n\tfor (i = scm->fp->count - 1; i >= 0; i--)\n\t\tunix_inflight(scm->fp->fp[i]);\n\treturn max_level;\n}\n\nstatic int unix_scm_to_skb(struct scm_cookie *scm, struct sk_buff *skb, bool send_fds)\n{\n\tint err = 0;\n\n\tUNIXCB(skb).pid  = get_pid(scm->pid);\n\tUNIXCB(skb).uid = scm->creds.uid;\n\tUNIXCB(skb).gid = scm->creds.gid;\n\tUNIXCB(skb).fp = NULL;\n\tunix_get_secdata(scm, skb);\n\tif (scm->fp && send_fds)\n\t\terr = unix_attach_fds(scm, skb);\n\n\tskb->destructor = unix_destruct_scm;\n\treturn err;\n}\n\nstatic bool unix_passcred_enabled(const struct socket *sock,\n\t\t\t\t  const struct sock *other)\n{\n\treturn test_bit(SOCK_PASSCRED, &sock->flags) ||\n\t       !other->sk_socket ||\n\t       test_bit(SOCK_PASSCRED, &other->sk_socket->flags);\n}\n\n/*\n * Some apps rely on write() giving SCM_CREDENTIALS\n * We include credentials if source or destination socket\n * asserted SOCK_PASSCRED.\n */\nstatic void maybe_add_creds(struct sk_buff *skb, const struct socket *sock,\n\t\t\t    const struct sock *other)\n{\n\tif (UNIXCB(skb).pid)\n\t\treturn;\n\tif (unix_passcred_enabled(sock, other)) {\n\t\tUNIXCB(skb).pid  = get_pid(task_tgid(current));\n\t\tcurrent_uid_gid(&UNIXCB(skb).uid, &UNIXCB(skb).gid);\n\t}\n}\n\nstatic int maybe_init_creds(struct scm_cookie *scm,\n\t\t\t    struct socket *socket,\n\t\t\t    const struct sock *other)\n{\n\tint err;\n\tstruct msghdr msg = { .msg_controllen = 0 };\n\n\terr = scm_send(socket, &msg, scm, false);\n\tif (err)\n\t\treturn err;\n\n\tif (unix_passcred_enabled(socket, other)) {\n\t\tscm->pid = get_pid(task_tgid(current));\n\t\tcurrent_uid_gid(&scm->creds.uid, &scm->creds.gid);\n\t}\n\treturn err;\n}\n\nstatic bool unix_skb_scm_eq(struct sk_buff *skb,\n\t\t\t    struct scm_cookie *scm)\n{\n\tconst struct unix_skb_parms *u = &UNIXCB(skb);\n\n\treturn u->pid == scm->pid &&\n\t       uid_eq(u->uid, scm->creds.uid) &&\n\t       gid_eq(u->gid, scm->creds.gid) &&\n\t       unix_secdata_eq(scm, skb);\n}\n\n/*\n *\tSend AF_UNIX data.\n */\n\nstatic int unix_dgram_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t      size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct unix_sock *u = unix_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr, msg->msg_name);\n\tstruct sock *other = NULL;\n\tint namelen = 0; /* fake GCC */\n\tint err;\n\tunsigned int hash;\n\tstruct sk_buff *skb;\n\tlong timeo;\n\tstruct scm_cookie scm;\n\tint max_level;\n\tint data_len = 0;\n\tint sk_locked;\n\n\twait_for_unix_gc();\n\terr = scm_send(sock, msg, &scm, false);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags&MSG_OOB)\n\t\tgoto out;\n\n\tif (msg->msg_namelen) {\n\t\terr = unix_mkname(sunaddr, msg->msg_namelen, &hash);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\tnamelen = err;\n\t} else {\n\t\tsunaddr = NULL;\n\t\terr = -ENOTCONN;\n\t\tother = unix_peer_get(sk);\n\t\tif (!other)\n\t\t\tgoto out;\n\t}\n\n\tif (test_bit(SOCK_PASSCRED, &sock->flags) && !u->addr\n\t    && (err = unix_autobind(sock)) != 0)\n\t\tgoto out;\n\n\terr = -EMSGSIZE;\n\tif (len > sk->sk_sndbuf - 32)\n\t\tgoto out;\n\n\tif (len > SKB_MAX_ALLOC) {\n\t\tdata_len = min_t(size_t,\n\t\t\t\t len - SKB_MAX_ALLOC,\n\t\t\t\t MAX_SKB_FRAGS * PAGE_SIZE);\n\t\tdata_len = PAGE_ALIGN(data_len);\n\n\t\tBUILD_BUG_ON(SKB_MAX_ALLOC < PAGE_SIZE);\n\t}\n\n\tskb = sock_alloc_send_pskb(sk, len - data_len, data_len,\n\t\t\t\t   msg->msg_flags & MSG_DONTWAIT, &err,\n\t\t\t\t   PAGE_ALLOC_COSTLY_ORDER);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\terr = unix_scm_to_skb(&scm, skb, true);\n\tif (err < 0)\n\t\tgoto out_free;\n\tmax_level = err + 1;\n\n\tskb_put(skb, len - data_len);\n\tskb->data_len = data_len;\n\tskb->len = len;\n\terr = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, len);\n\tif (err)\n\t\tgoto out_free;\n\n\ttimeo = sock_sndtimeo(sk, msg->msg_flags & MSG_DONTWAIT);\n\nrestart:\n\tif (!other) {\n\t\terr = -ECONNRESET;\n\t\tif (sunaddr == NULL)\n\t\t\tgoto out_free;\n\n\t\tother = unix_find_other(net, sunaddr, namelen, sk->sk_type,\n\t\t\t\t\thash, &err);\n\t\tif (other == NULL)\n\t\t\tgoto out_free;\n\t}\n\n\tif (sk_filter(other, skb) < 0) {\n\t\t/* Toss the packet but do not return any error to the sender */\n\t\terr = len;\n\t\tgoto out_free;\n\t}\n\n\tsk_locked = 0;\n\tunix_state_lock(other);\nrestart_locked:\n\terr = -EPERM;\n\tif (!unix_may_send(sk, other))\n\t\tgoto out_unlock;\n\n\tif (unlikely(sock_flag(other, SOCK_DEAD))) {\n\t\t/*\n\t\t *\tCheck with 1003.1g - what should\n\t\t *\tdatagram error\n\t\t */\n\t\tunix_state_unlock(other);\n\t\tsock_put(other);\n\n\t\tif (!sk_locked)\n\t\t\tunix_state_lock(sk);\n\n\t\terr = 0;\n\t\tif (unix_peer(sk) == other) {\n\t\t\tunix_peer(sk) = NULL;\n\t\t\tunix_dgram_peer_wake_disconnect_wakeup(sk, other);\n\n\t\t\tunix_state_unlock(sk);\n\n\t\t\tunix_dgram_disconnected(sk, other);\n\t\t\tsock_put(other);\n\t\t\terr = -ECONNREFUSED;\n\t\t} else {\n\t\t\tunix_state_unlock(sk);\n\t\t}\n\n\t\tother = NULL;\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tgoto restart;\n\t}\n\n\terr = -EPIPE;\n\tif (other->sk_shutdown & RCV_SHUTDOWN)\n\t\tgoto out_unlock;\n\n\tif (sk->sk_type != SOCK_SEQPACKET) {\n\t\terr = security_unix_may_send(sk->sk_socket, other->sk_socket);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tif (unlikely(unix_peer(other) != sk && unix_recvq_full(other))) {\n\t\tif (timeo) {\n\t\t\ttimeo = unix_wait_for_peer(other, timeo);\n\n\t\t\terr = sock_intr_errno(timeo);\n\t\t\tif (signal_pending(current))\n\t\t\t\tgoto out_free;\n\n\t\t\tgoto restart;\n\t\t}\n\n\t\tif (!sk_locked) {\n\t\t\tunix_state_unlock(other);\n\t\t\tunix_state_double_lock(sk, other);\n\t\t}\n\n\t\tif (unix_peer(sk) != other ||\n\t\t    unix_dgram_peer_wake_me(sk, other)) {\n\t\t\terr = -EAGAIN;\n\t\t\tsk_locked = 1;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (!sk_locked) {\n\t\t\tsk_locked = 1;\n\t\t\tgoto restart_locked;\n\t\t}\n\t}\n\n\tif (unlikely(sk_locked))\n\t\tunix_state_unlock(sk);\n\n\tif (sock_flag(other, SOCK_RCVTSTAMP))\n\t\t__net_timestamp(skb);\n\tmaybe_add_creds(skb, sock, other);\n\tskb_queue_tail(&other->sk_receive_queue, skb);\n\tif (max_level > unix_sk(other)->recursion_level)\n\t\tunix_sk(other)->recursion_level = max_level;\n\tunix_state_unlock(other);\n\tother->sk_data_ready(other);\n\tsock_put(other);\n\tscm_destroy(&scm);\n\treturn len;\n\nout_unlock:\n\tif (sk_locked)\n\t\tunix_state_unlock(sk);\n\tunix_state_unlock(other);\nout_free:\n\tkfree_skb(skb);\nout:\n\tif (other)\n\t\tsock_put(other);\n\tscm_destroy(&scm);\n\treturn err;\n}\n\n/* We use paged skbs for stream sockets, and limit occupancy to 32768\n * bytes, and a minimun of a full page.\n */\n#define UNIX_SKB_FRAGS_SZ (PAGE_SIZE << get_order(32768))\n\nstatic int unix_stream_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t       size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sock *other = NULL;\n\tint err, size;\n\tstruct sk_buff *skb;\n\tint sent = 0;\n\tstruct scm_cookie scm;\n\tbool fds_sent = false;\n\tint max_level;\n\tint data_len;\n\n\twait_for_unix_gc();\n\terr = scm_send(sock, msg, &scm, false);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags&MSG_OOB)\n\t\tgoto out_err;\n\n\tif (msg->msg_namelen) {\n\t\terr = sk->sk_state == TCP_ESTABLISHED ? -EISCONN : -EOPNOTSUPP;\n\t\tgoto out_err;\n\t} else {\n\t\terr = -ENOTCONN;\n\t\tother = unix_peer(sk);\n\t\tif (!other)\n\t\t\tgoto out_err;\n\t}\n\n\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\tgoto pipe_err;\n\n\twhile (sent < len) {\n\t\tsize = len - sent;\n\n\t\t/* Keep two messages in the pipe so it schedules better */\n\t\tsize = min_t(int, size, (sk->sk_sndbuf >> 1) - 64);\n\n\t\t/* allow fallback to order-0 allocations */\n\t\tsize = min_t(int, size, SKB_MAX_HEAD(0) + UNIX_SKB_FRAGS_SZ);\n\n\t\tdata_len = max_t(int, 0, size - SKB_MAX_HEAD(0));\n\n\t\tdata_len = min_t(size_t, size, PAGE_ALIGN(data_len));\n\n\t\tskb = sock_alloc_send_pskb(sk, size - data_len, data_len,\n\t\t\t\t\t   msg->msg_flags & MSG_DONTWAIT, &err,\n\t\t\t\t\t   get_order(UNIX_SKB_FRAGS_SZ));\n\t\tif (!skb)\n\t\t\tgoto out_err;\n\n\t\t/* Only send the fds in the first buffer */\n\t\terr = unix_scm_to_skb(&scm, skb, !fds_sent);\n\t\tif (err < 0) {\n\t\t\tkfree_skb(skb);\n\t\t\tgoto out_err;\n\t\t}\n\t\tmax_level = err + 1;\n\t\tfds_sent = true;\n\n\t\tskb_put(skb, size - data_len);\n\t\tskb->data_len = data_len;\n\t\tskb->len = size;\n\t\terr = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, size);\n\t\tif (err) {\n\t\t\tkfree_skb(skb);\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tunix_state_lock(other);\n\n\t\tif (sock_flag(other, SOCK_DEAD) ||\n\t\t    (other->sk_shutdown & RCV_SHUTDOWN))\n\t\t\tgoto pipe_err_free;\n\n\t\tmaybe_add_creds(skb, sock, other);\n\t\tskb_queue_tail(&other->sk_receive_queue, skb);\n\t\tif (max_level > unix_sk(other)->recursion_level)\n\t\t\tunix_sk(other)->recursion_level = max_level;\n\t\tunix_state_unlock(other);\n\t\tother->sk_data_ready(other);\n\t\tsent += size;\n\t}\n\n\tscm_destroy(&scm);\n\n\treturn sent;\n\npipe_err_free:\n\tunix_state_unlock(other);\n\tkfree_skb(skb);\npipe_err:\n\tif (sent == 0 && !(msg->msg_flags&MSG_NOSIGNAL))\n\t\tsend_sig(SIGPIPE, current, 0);\n\terr = -EPIPE;\nout_err:\n\tscm_destroy(&scm);\n\treturn sent ? : err;\n}\n\nstatic ssize_t unix_stream_sendpage(struct socket *socket, struct page *page,\n\t\t\t\t    int offset, size_t size, int flags)\n{\n\tint err;\n\tbool send_sigpipe = false;\n\tbool init_scm = true;\n\tstruct scm_cookie scm;\n\tstruct sock *other, *sk = socket->sk;\n\tstruct sk_buff *skb, *newskb = NULL, *tail = NULL;\n\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tother = unix_peer(sk);\n\tif (!other || sk->sk_state != TCP_ESTABLISHED)\n\t\treturn -ENOTCONN;\n\n\tif (false) {\nalloc_skb:\n\t\tunix_state_unlock(other);\n\t\tmutex_unlock(&unix_sk(other)->readlock);\n\t\tnewskb = sock_alloc_send_pskb(sk, 0, 0, flags & MSG_DONTWAIT,\n\t\t\t\t\t      &err, 0);\n\t\tif (!newskb)\n\t\t\tgoto err;\n\t}\n\n\t/* we must acquire readlock as we modify already present\n\t * skbs in the sk_receive_queue and mess with skb->len\n\t */\n\terr = mutex_lock_interruptible(&unix_sk(other)->readlock);\n\tif (err) {\n\t\terr = flags & MSG_DONTWAIT ? -EAGAIN : -ERESTARTSYS;\n\t\tgoto err;\n\t}\n\n\tif (sk->sk_shutdown & SEND_SHUTDOWN) {\n\t\terr = -EPIPE;\n\t\tsend_sigpipe = true;\n\t\tgoto err_unlock;\n\t}\n\n\tunix_state_lock(other);\n\n\tif (sock_flag(other, SOCK_DEAD) ||\n\t    other->sk_shutdown & RCV_SHUTDOWN) {\n\t\terr = -EPIPE;\n\t\tsend_sigpipe = true;\n\t\tgoto err_state_unlock;\n\t}\n\n\tif (init_scm) {\n\t\terr = maybe_init_creds(&scm, socket, other);\n\t\tif (err)\n\t\t\tgoto err_state_unlock;\n\t\tinit_scm = false;\n\t}\n\n\tskb = skb_peek_tail(&other->sk_receive_queue);\n\tif (tail && tail == skb) {\n\t\tskb = newskb;\n\t} else if (!skb || !unix_skb_scm_eq(skb, &scm)) {\n\t\tif (newskb) {\n\t\t\tskb = newskb;\n\t\t} else {\n\t\t\ttail = skb;\n\t\t\tgoto alloc_skb;\n\t\t}\n\t} else if (newskb) {\n\t\t/* this is fast path, we don't necessarily need to\n\t\t * call to kfree_skb even though with newskb == NULL\n\t\t * this - does no harm\n\t\t */\n\t\tconsume_skb(newskb);\n\t\tnewskb = NULL;\n\t}\n\n\tif (skb_append_pagefrags(skb, page, offset, size)) {\n\t\ttail = skb;\n\t\tgoto alloc_skb;\n\t}\n\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += size;\n\tatomic_add(size, &sk->sk_wmem_alloc);\n\n\tif (newskb) {\n\t\terr = unix_scm_to_skb(&scm, skb, false);\n\t\tif (err)\n\t\t\tgoto err_state_unlock;\n\t\tspin_lock(&other->sk_receive_queue.lock);\n\t\t__skb_queue_tail(&other->sk_receive_queue, newskb);\n\t\tspin_unlock(&other->sk_receive_queue.lock);\n\t}\n\n\tunix_state_unlock(other);\n\tmutex_unlock(&unix_sk(other)->readlock);\n\n\tother->sk_data_ready(other);\n\tscm_destroy(&scm);\n\treturn size;\n\nerr_state_unlock:\n\tunix_state_unlock(other);\nerr_unlock:\n\tmutex_unlock(&unix_sk(other)->readlock);\nerr:\n\tkfree_skb(newskb);\n\tif (send_sigpipe && !(flags & MSG_NOSIGNAL))\n\t\tsend_sig(SIGPIPE, current, 0);\n\tif (!init_scm)\n\t\tscm_destroy(&scm);\n\treturn err;\n}\n\nstatic int unix_seqpacket_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t\t  size_t len)\n{\n\tint err;\n\tstruct sock *sk = sock->sk;\n\n\terr = sock_error(sk);\n\tif (err)\n\t\treturn err;\n\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\treturn -ENOTCONN;\n\n\tif (msg->msg_namelen)\n\t\tmsg->msg_namelen = 0;\n\n\treturn unix_dgram_sendmsg(sock, msg, len);\n}\n\nstatic int unix_seqpacket_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t\t  size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\treturn -ENOTCONN;\n\n\treturn unix_dgram_recvmsg(sock, msg, size, flags);\n}\n\nstatic void unix_copy_addr(struct msghdr *msg, struct sock *sk)\n{\n\tstruct unix_sock *u = unix_sk(sk);\n\n\tif (u->addr) {\n\t\tmsg->msg_namelen = u->addr->len;\n\t\tmemcpy(msg->msg_name, u->addr->name, u->addr->len);\n\t}\n}\n\nstatic int unix_dgram_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t      size_t size, int flags)\n{\n\tstruct scm_cookie scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sk_buff *skb, *last;\n\tlong timeo;\n\tint err;\n\tint peeked, skip;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\tdo {\n\t\tmutex_lock(&u->readlock);\n\n\t\tskip = sk_peek_offset(sk, flags);\n\t\tskb = __skb_try_recv_datagram(sk, flags, &peeked, &skip, &err,\n\t\t\t\t\t      &last);\n\t\tif (skb)\n\t\t\tbreak;\n\n\t\tmutex_unlock(&u->readlock);\n\n\t\tif (err != -EAGAIN)\n\t\t\tbreak;\n\t} while (timeo &&\n\t\t !__skb_wait_for_more_packets(sk, &err, &timeo, last));\n\n\tif (!skb) { /* implies readlock unlocked */\n\t\tunix_state_lock(sk);\n\t\t/* Signal EOF on disconnected non-blocking SEQPACKET socket. */\n\t\tif (sk->sk_type == SOCK_SEQPACKET && err == -EAGAIN &&\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\t\terr = 0;\n\t\tunix_state_unlock(sk);\n\t\tgoto out;\n\t}\n\n\tif (wq_has_sleeper(&u->peer_wait))\n\t\twake_up_interruptible_sync_poll(&u->peer_wait,\n\t\t\t\t\t\tPOLLOUT | POLLWRNORM |\n\t\t\t\t\t\tPOLLWRBAND);\n\n\tif (msg->msg_name)\n\t\tunix_copy_addr(msg, skb->sk);\n\n\tif (size > skb->len - skip)\n\t\tsize = skb->len - skip;\n\telse if (size < skb->len - skip)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\terr = skb_copy_datagram_msg(skb, skip, msg, size);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock_flag(sk, SOCK_RCVTSTAMP))\n\t\t__sock_recv_timestamp(msg, sk, skb);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\tunix_set_secdata(&scm, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tif (UNIXCB(skb).fp)\n\t\t\tunix_detach_fds(&scm, skb);\n\n\t\tsk_peek_offset_bwd(sk, skb->len);\n\t} else {\n\t\t/* It is questionable: on PEEK we could:\n\t\t   - do not return fds - good, but too simple 8)\n\t\t   - return fds, and do not return them on read (old strategy,\n\t\t     apparently wrong)\n\t\t   - clone fds (I chose it for now, it is the most universal\n\t\t     solution)\n\n\t\t   POSIX 1003.1g does not actually define this clearly\n\t\t   at all. POSIX 1003.1g doesn't define a lot of things\n\t\t   clearly however!\n\n\t\t*/\n\n\t\tsk_peek_offset_fwd(sk, size);\n\n\t\tif (UNIXCB(skb).fp)\n\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n\t}\n\terr = (flags & MSG_TRUNC) ? skb->len - skip : size;\n\n\tscm_recv(sock, msg, &scm, flags);\n\nout_free:\n\tskb_free_datagram(sk, skb);\n\tmutex_unlock(&u->readlock);\nout:\n\treturn err;\n}\n\n/*\n *\tSleep until more data has arrived. But check for races..\n */\nstatic long unix_stream_data_wait(struct sock *sk, long timeo,\n\t\t\t\t  struct sk_buff *last, unsigned int last_len)\n{\n\tstruct sk_buff *tail;\n\tDEFINE_WAIT(wait);\n\n\tunix_state_lock(sk);\n\n\tfor (;;) {\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\t\ttail = skb_peek_tail(&sk->sk_receive_queue);\n\t\tif (tail != last ||\n\t\t    (tail && tail->len != last_len) ||\n\t\t    sk->sk_err ||\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t    signal_pending(current) ||\n\t\t    !timeo)\n\t\t\tbreak;\n\n\t\tsk_set_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\t\tunix_state_unlock(sk);\n\t\ttimeo = freezable_schedule_timeout(timeo);\n\t\tunix_state_lock(sk);\n\n\t\tif (sock_flag(sk, SOCK_DEAD))\n\t\t\tbreak;\n\n\t\tsk_clear_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\t}\n\n\tfinish_wait(sk_sleep(sk), &wait);\n\tunix_state_unlock(sk);\n\treturn timeo;\n}\n\nstatic unsigned int unix_skb_len(const struct sk_buff *skb)\n{\n\treturn skb->len - UNIXCB(skb).consumed;\n}\n\nstruct unix_stream_read_state {\n\tint (*recv_actor)(struct sk_buff *, int, int,\n\t\t\t  struct unix_stream_read_state *);\n\tstruct socket *socket;\n\tstruct msghdr *msg;\n\tstruct pipe_inode_info *pipe;\n\tsize_t size;\n\tint flags;\n\tunsigned int splice_flags;\n};\n\nstatic int unix_stream_read_generic(struct unix_stream_read_state *state)\n{\n\tstruct scm_cookie scm;\n\tstruct socket *sock = state->socket;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tint copied = 0;\n\tint flags = state->flags;\n\tint noblock = flags & MSG_DONTWAIT;\n\tbool check_creds = false;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\tsize_t size = state->size;\n\tunsigned int last_len;\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\tgoto out;\n\n\terr = -EOPNOTSUPP;\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, noblock);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\tmutex_lock(&u->readlock);\n\n\tif (flags & MSG_PEEK)\n\t\tskip = sk_peek_offset(sk, flags);\n\telse\n\t\tskip = 0;\n\n\tdo {\n\t\tint chunk;\n\t\tbool drop_skb;\n\t\tstruct sk_buff *skb, *last;\n\n\t\tunix_state_lock(sk);\n\t\tif (sock_flag(sk, SOCK_DEAD)) {\n\t\t\terr = -ECONNRESET;\n\t\t\tgoto unlock;\n\t\t}\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\n\t\tlast_len = last ? last->len : 0;\nagain:\n\t\tif (skb == NULL) {\n\t\t\tunix_sk(sk)->recursion_level = 0;\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\t\t\tmutex_unlock(&u->readlock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last,\n\t\t\t\t\t\t      last_len);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tscm_destroy(&scm);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmutex_lock(&u->readlock);\n\t\t\tcontinue;\nunlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif (!unix_skb_scm_eq(skb, &scm))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tunix_set_secdata(&scm, skb);\n\t\t\tcheck_creds = true;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (state->msg && state->msg->msg_name) {\n\t\t\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr,\n\t\t\t\t\t state->msg->msg_name);\n\t\t\tunix_copy_addr(state->msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tskb_get(skb);\n\t\tchunk = state->recv_actor(skb, skip, chunk, state);\n\t\tdrop_skb = !unix_skb_len(skb);\n\t\t/* skb is only safe to use if !drop_skb */\n\t\tconsume_skb(skb);\n\t\tif (chunk < 0) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\tif (drop_skb) {\n\t\t\t/* the skb was touched by a concurrent reader;\n\t\t\t * we should not expect anything from this skb\n\t\t\t * anymore and assume it invalid - we can be\n\t\t\t * sure it was dropped from the socket queue\n\t\t\t *\n\t\t\t * let's report a short read\n\t\t\t */\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tunix_detach_fds(&scm, skb);\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (scm.fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tbreak;\n\n\t\t\tskip = 0;\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tunix_state_lock(sk);\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (skb)\n\t\t\t\tgoto again;\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->readlock);\n\tif (state->msg)\n\t\tscm_recv(sock, state->msg, &scm, flags);\n\telse\n\t\tscm_destroy(&scm);\nout:\n\treturn copied ? : err;\n}\n\nstatic int unix_stream_read_actor(struct sk_buff *skb,\n\t\t\t\t  int skip, int chunk,\n\t\t\t\t  struct unix_stream_read_state *state)\n{\n\tint ret;\n\n\tret = skb_copy_datagram_msg(skb, UNIXCB(skb).consumed + skip,\n\t\t\t\t    state->msg, chunk);\n\treturn ret ?: chunk;\n}\n\nstatic int unix_stream_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t       size_t size, int flags)\n{\n\tstruct unix_stream_read_state state = {\n\t\t.recv_actor = unix_stream_read_actor,\n\t\t.socket = sock,\n\t\t.msg = msg,\n\t\t.size = size,\n\t\t.flags = flags\n\t};\n\n\treturn unix_stream_read_generic(&state);\n}\n\nstatic ssize_t skb_unix_socket_splice(struct sock *sk,\n\t\t\t\t      struct pipe_inode_info *pipe,\n\t\t\t\t      struct splice_pipe_desc *spd)\n{\n\tint ret;\n\tstruct unix_sock *u = unix_sk(sk);\n\n\tmutex_unlock(&u->readlock);\n\tret = splice_to_pipe(pipe, spd);\n\tmutex_lock(&u->readlock);\n\n\treturn ret;\n}\n\nstatic int unix_stream_splice_actor(struct sk_buff *skb,\n\t\t\t\t    int skip, int chunk,\n\t\t\t\t    struct unix_stream_read_state *state)\n{\n\treturn skb_splice_bits(skb, state->socket->sk,\n\t\t\t       UNIXCB(skb).consumed + skip,\n\t\t\t       state->pipe, chunk, state->splice_flags,\n\t\t\t       skb_unix_socket_splice);\n}\n\nstatic ssize_t unix_stream_splice_read(struct socket *sock,  loff_t *ppos,\n\t\t\t\t       struct pipe_inode_info *pipe,\n\t\t\t\t       size_t size, unsigned int flags)\n{\n\tstruct unix_stream_read_state state = {\n\t\t.recv_actor = unix_stream_splice_actor,\n\t\t.socket = sock,\n\t\t.pipe = pipe,\n\t\t.size = size,\n\t\t.splice_flags = flags,\n\t};\n\n\tif (unlikely(*ppos))\n\t\treturn -ESPIPE;\n\n\tif (sock->file->f_flags & O_NONBLOCK ||\n\t    flags & SPLICE_F_NONBLOCK)\n\t\tstate.flags = MSG_DONTWAIT;\n\n\treturn unix_stream_read_generic(&state);\n}\n\nstatic int unix_shutdown(struct socket *sock, int mode)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sock *other;\n\n\tif (mode < SHUT_RD || mode > SHUT_RDWR)\n\t\treturn -EINVAL;\n\t/* This maps:\n\t * SHUT_RD   (0) -> RCV_SHUTDOWN  (1)\n\t * SHUT_WR   (1) -> SEND_SHUTDOWN (2)\n\t * SHUT_RDWR (2) -> SHUTDOWN_MASK (3)\n\t */\n\t++mode;\n\n\tunix_state_lock(sk);\n\tsk->sk_shutdown |= mode;\n\tother = unix_peer(sk);\n\tif (other)\n\t\tsock_hold(other);\n\tunix_state_unlock(sk);\n\tsk->sk_state_change(sk);\n\n\tif (other &&\n\t\t(sk->sk_type == SOCK_STREAM || sk->sk_type == SOCK_SEQPACKET)) {\n\n\t\tint peer_mode = 0;\n\n\t\tif (mode&RCV_SHUTDOWN)\n\t\t\tpeer_mode |= SEND_SHUTDOWN;\n\t\tif (mode&SEND_SHUTDOWN)\n\t\t\tpeer_mode |= RCV_SHUTDOWN;\n\t\tunix_state_lock(other);\n\t\tother->sk_shutdown |= peer_mode;\n\t\tunix_state_unlock(other);\n\t\tother->sk_state_change(other);\n\t\tif (peer_mode == SHUTDOWN_MASK)\n\t\t\tsk_wake_async(other, SOCK_WAKE_WAITD, POLL_HUP);\n\t\telse if (peer_mode & RCV_SHUTDOWN)\n\t\t\tsk_wake_async(other, SOCK_WAKE_WAITD, POLL_IN);\n\t}\n\tif (other)\n\t\tsock_put(other);\n\n\treturn 0;\n}\n\nlong unix_inq_len(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tlong amount = 0;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn -EINVAL;\n\n\tspin_lock(&sk->sk_receive_queue.lock);\n\tif (sk->sk_type == SOCK_STREAM ||\n\t    sk->sk_type == SOCK_SEQPACKET) {\n\t\tskb_queue_walk(&sk->sk_receive_queue, skb)\n\t\t\tamount += unix_skb_len(skb);\n\t} else {\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tamount = skb->len;\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\treturn amount;\n}\nEXPORT_SYMBOL_GPL(unix_inq_len);\n\nlong unix_outq_len(struct sock *sk)\n{\n\treturn sk_wmem_alloc_get(sk);\n}\nEXPORT_SYMBOL_GPL(unix_outq_len);\n\nstatic int unix_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tlong amount = 0;\n\tint err;\n\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t\tamount = unix_outq_len(sk);\n\t\terr = put_user(amount, (int __user *)arg);\n\t\tbreak;\n\tcase SIOCINQ:\n\t\tamount = unix_inq_len(sk);\n\t\tif (amount < 0)\n\t\t\terr = amount;\n\t\telse\n\t\t\terr = put_user(amount, (int __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOIOCTLCMD;\n\t\tbreak;\n\t}\n\treturn err;\n}\n\nstatic unsigned int unix_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\tstruct sock *sk = sock->sk;\n\tunsigned int mask;\n\n\tsock_poll_wait(file, sk_sleep(sk), wait);\n\tmask = 0;\n\n\t/* exceptional events? */\n\tif (sk->sk_err)\n\t\tmask |= POLLERR;\n\tif (sk->sk_shutdown == SHUTDOWN_MASK)\n\t\tmask |= POLLHUP;\n\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\tmask |= POLLRDHUP | POLLIN | POLLRDNORM;\n\n\t/* readable? */\n\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\tmask |= POLLIN | POLLRDNORM;\n\n\t/* Connection-based need to check for termination and startup */\n\tif ((sk->sk_type == SOCK_STREAM || sk->sk_type == SOCK_SEQPACKET) &&\n\t    sk->sk_state == TCP_CLOSE)\n\t\tmask |= POLLHUP;\n\n\t/*\n\t * we set writable also when the other side has shut down the\n\t * connection. This prevents stuck sockets.\n\t */\n\tif (unix_writable(sk))\n\t\tmask |= POLLOUT | POLLWRNORM | POLLWRBAND;\n\n\treturn mask;\n}\n\nstatic unsigned int unix_dgram_poll(struct file *file, struct socket *sock,\n\t\t\t\t    poll_table *wait)\n{\n\tstruct sock *sk = sock->sk, *other;\n\tunsigned int mask, writable;\n\n\tsock_poll_wait(file, sk_sleep(sk), wait);\n\tmask = 0;\n\n\t/* exceptional events? */\n\tif (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue))\n\t\tmask |= POLLERR |\n\t\t\t(sock_flag(sk, SOCK_SELECT_ERR_QUEUE) ? POLLPRI : 0);\n\n\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\tmask |= POLLRDHUP | POLLIN | POLLRDNORM;\n\tif (sk->sk_shutdown == SHUTDOWN_MASK)\n\t\tmask |= POLLHUP;\n\n\t/* readable? */\n\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\tmask |= POLLIN | POLLRDNORM;\n\n\t/* Connection-based need to check for termination and startup */\n\tif (sk->sk_type == SOCK_SEQPACKET) {\n\t\tif (sk->sk_state == TCP_CLOSE)\n\t\t\tmask |= POLLHUP;\n\t\t/* connection hasn't started yet? */\n\t\tif (sk->sk_state == TCP_SYN_SENT)\n\t\t\treturn mask;\n\t}\n\n\t/* No write status requested, avoid expensive OUT tests. */\n\tif (!(poll_requested_events(wait) & (POLLWRBAND|POLLWRNORM|POLLOUT)))\n\t\treturn mask;\n\n\twritable = unix_writable(sk);\n\tif (writable) {\n\t\tunix_state_lock(sk);\n\n\t\tother = unix_peer(sk);\n\t\tif (other && unix_peer(other) != sk &&\n\t\t    unix_recvq_full(other) &&\n\t\t    unix_dgram_peer_wake_me(sk, other))\n\t\t\twritable = 0;\n\n\t\tunix_state_unlock(sk);\n\t}\n\n\tif (writable)\n\t\tmask |= POLLOUT | POLLWRNORM | POLLWRBAND;\n\telse\n\t\tsk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\n\treturn mask;\n}\n\n#ifdef CONFIG_PROC_FS\n\n#define BUCKET_SPACE (BITS_PER_LONG - (UNIX_HASH_BITS + 1) - 1)\n\n#define get_bucket(x) ((x) >> BUCKET_SPACE)\n#define get_offset(x) ((x) & ((1L << BUCKET_SPACE) - 1))\n#define set_bucket_offset(b, o) ((b) << BUCKET_SPACE | (o))\n\nstatic struct sock *unix_from_bucket(struct seq_file *seq, loff_t *pos)\n{\n\tunsigned long offset = get_offset(*pos);\n\tunsigned long bucket = get_bucket(*pos);\n\tstruct sock *sk;\n\tunsigned long count = 0;\n\n\tfor (sk = sk_head(&unix_socket_table[bucket]); sk; sk = sk_next(sk)) {\n\t\tif (sock_net(sk) != seq_file_net(seq))\n\t\t\tcontinue;\n\t\tif (++count == offset)\n\t\t\tbreak;\n\t}\n\n\treturn sk;\n}\n\nstatic struct sock *unix_next_socket(struct seq_file *seq,\n\t\t\t\t     struct sock *sk,\n\t\t\t\t     loff_t *pos)\n{\n\tunsigned long bucket;\n\n\twhile (sk > (struct sock *)SEQ_START_TOKEN) {\n\t\tsk = sk_next(sk);\n\t\tif (!sk)\n\t\t\tgoto next_bucket;\n\t\tif (sock_net(sk) == seq_file_net(seq))\n\t\t\treturn sk;\n\t}\n\n\tdo {\n\t\tsk = unix_from_bucket(seq, pos);\n\t\tif (sk)\n\t\t\treturn sk;\n\nnext_bucket:\n\t\tbucket = get_bucket(*pos) + 1;\n\t\t*pos = set_bucket_offset(bucket, 1);\n\t} while (bucket < ARRAY_SIZE(unix_socket_table));\n\n\treturn NULL;\n}\n\nstatic void *unix_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(unix_table_lock)\n{\n\tspin_lock(&unix_table_lock);\n\n\tif (!*pos)\n\t\treturn SEQ_START_TOKEN;\n\n\tif (get_bucket(*pos) >= ARRAY_SIZE(unix_socket_table))\n\t\treturn NULL;\n\n\treturn unix_next_socket(seq, NULL, pos);\n}\n\nstatic void *unix_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\t++*pos;\n\treturn unix_next_socket(seq, v, pos);\n}\n\nstatic void unix_seq_stop(struct seq_file *seq, void *v)\n\t__releases(unix_table_lock)\n{\n\tspin_unlock(&unix_table_lock);\n}\n\nstatic int unix_seq_show(struct seq_file *seq, void *v)\n{\n\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"Num       RefCount Protocol Flags    Type St \"\n\t\t\t \"Inode Path\\n\");\n\telse {\n\t\tstruct sock *s = v;\n\t\tstruct unix_sock *u = unix_sk(s);\n\t\tunix_state_lock(s);\n\n\t\tseq_printf(seq, \"%pK: %08X %08X %08X %04X %02X %5lu\",\n\t\t\ts,\n\t\t\tatomic_read(&s->sk_refcnt),\n\t\t\t0,\n\t\t\ts->sk_state == TCP_LISTEN ? __SO_ACCEPTCON : 0,\n\t\t\ts->sk_type,\n\t\t\ts->sk_socket ?\n\t\t\t(s->sk_state == TCP_ESTABLISHED ? SS_CONNECTED : SS_UNCONNECTED) :\n\t\t\t(s->sk_state == TCP_ESTABLISHED ? SS_CONNECTING : SS_DISCONNECTING),\n\t\t\tsock_i_ino(s));\n\n\t\tif (u->addr) {\n\t\t\tint i, len;\n\t\t\tseq_putc(seq, ' ');\n\n\t\t\ti = 0;\n\t\t\tlen = u->addr->len - sizeof(short);\n\t\t\tif (!UNIX_ABSTRACT(s))\n\t\t\t\tlen--;\n\t\t\telse {\n\t\t\t\tseq_putc(seq, '@');\n\t\t\t\ti++;\n\t\t\t}\n\t\t\tfor ( ; i < len; i++)\n\t\t\t\tseq_putc(seq, u->addr->name->sun_path[i]);\n\t\t}\n\t\tunix_state_unlock(s);\n\t\tseq_putc(seq, '\\n');\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations unix_seq_ops = {\n\t.start  = unix_seq_start,\n\t.next   = unix_seq_next,\n\t.stop   = unix_seq_stop,\n\t.show   = unix_seq_show,\n};\n\nstatic int unix_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &unix_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations unix_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= unix_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\n#endif\n\nstatic const struct net_proto_family unix_family_ops = {\n\t.family = PF_UNIX,\n\t.create = unix_create,\n\t.owner\t= THIS_MODULE,\n};\n\n\nstatic int __net_init unix_net_init(struct net *net)\n{\n\tint error = -ENOMEM;\n\n\tnet->unx.sysctl_max_dgram_qlen = 10;\n\tif (unix_sysctl_register(net))\n\t\tgoto out;\n\n#ifdef CONFIG_PROC_FS\n\tif (!proc_create(\"unix\", 0, net->proc_net, &unix_seq_fops)) {\n\t\tunix_sysctl_unregister(net);\n\t\tgoto out;\n\t}\n#endif\n\terror = 0;\nout:\n\treturn error;\n}\n\nstatic void __net_exit unix_net_exit(struct net *net)\n{\n\tunix_sysctl_unregister(net);\n\tremove_proc_entry(\"unix\", net->proc_net);\n}\n\nstatic struct pernet_operations unix_net_ops = {\n\t.init = unix_net_init,\n\t.exit = unix_net_exit,\n};\n\nstatic int __init af_unix_init(void)\n{\n\tint rc = -1;\n\n\tBUILD_BUG_ON(sizeof(struct unix_skb_parms) > FIELD_SIZEOF(struct sk_buff, cb));\n\n\trc = proto_register(&unix_proto, 1);\n\tif (rc != 0) {\n\t\tpr_crit(\"%s: Cannot create unix_sock SLAB cache!\\n\", __func__);\n\t\tgoto out;\n\t}\n\n\tsock_register(&unix_family_ops);\n\tregister_pernet_subsys(&unix_net_ops);\nout:\n\treturn rc;\n}\n\nstatic void __exit af_unix_exit(void)\n{\n\tsock_unregister(PF_UNIX);\n\tproto_unregister(&unix_proto);\n\tunregister_pernet_subsys(&unix_net_ops);\n}\n\n/* Earlier than device_initcall() so that other drivers invoking\n   request_module() don't end up in a loop when modprobe tries\n   to use a UNIX socket. But later than subsys_initcall() because\n   we depend on stuff initialised there */\nfs_initcall(af_unix_init);\nmodule_exit(af_unix_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_NETPROTO(PF_UNIX);\n", "/*\n * NET3:\tGarbage Collector For AF_UNIX sockets\n *\n * Garbage Collector:\n *\tCopyright (C) Barak A. Pearlmutter.\n *\tReleased under the GPL version 2 or later.\n *\n * Chopped about by Alan Cox 22/3/96 to make it fit the AF_UNIX socket problem.\n * If it doesn't work blame me, it worked when Barak sent it.\n *\n * Assumptions:\n *\n *  - object w/ a bit\n *  - free list\n *\n * Current optimizations:\n *\n *  - explicit stack instead of recursion\n *  - tail recurse on first born instead of immediate push/pop\n *  - we gather the stuff that should not be killed into tree\n *    and stack is just a path from root to the current pointer.\n *\n *  Future optimizations:\n *\n *  - don't just push entire root set; process in place\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n *  Fixes:\n *\tAlan Cox\t07 Sept\t1997\tVmalloc internal stack as needed.\n *\t\t\t\t\tCope with changing max_files.\n *\tAl Viro\t\t11 Oct 1998\n *\t\tGraph may have cycles. That is, we can send the descriptor\n *\t\tof foo to bar and vice versa. Current code chokes on that.\n *\t\tFix: move SCM_RIGHTS ones into the separate list and then\n *\t\tskb_free() them all instead of doing explicit fput's.\n *\t\tAnother problem: since fput() may block somebody may\n *\t\tcreate a new unix_socket when we are in the middle of sweep\n *\t\tphase. Fix: revert the logic wrt MARKED. Mark everything\n *\t\tupon the beginning and unmark non-junk ones.\n *\n *\t\t[12 Oct 1998] AAARGH! New code purges all SCM_RIGHTS\n *\t\tsent to connect()'ed but still not accept()'ed sockets.\n *\t\tFixed. Old code had slightly different problem here:\n *\t\textra fput() in situation when we passed the descriptor via\n *\t\tsuch socket and closed it (descriptor). That would happen on\n *\t\teach unix_gc() until the accept(). Since the struct file in\n *\t\tquestion would go to the free list and might be reused...\n *\t\tThat might be the reason of random oopses on filp_close()\n *\t\tin unrelated processes.\n *\n *\tAV\t\t28 Feb 1999\n *\t\tKill the explicit allocation of stack. Now we keep the tree\n *\t\twith root in dummy + pointer (gc_current) to one of the nodes.\n *\t\tStack is represented as path from gc_current to dummy. Unmark\n *\t\tnow means \"add to tree\". Push == \"make it a son of gc_current\".\n *\t\tPop == \"move gc_current to parent\". We keep only pointers to\n *\t\tparents (->gc_tree).\n *\tAV\t\t1 Mar 1999\n *\t\tDamn. Added missing check for ->dead in listen queues scanning.\n *\n *\tMiklos Szeredi 25 Jun 2007\n *\t\tReimplement with a cycle collecting algorithm. This should\n *\t\tsolve several problems with the previous code, like being racy\n *\t\twrt receive and holding up unrelated socket operations.\n */\n\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/socket.h>\n#include <linux/un.h>\n#include <linux/net.h>\n#include <linux/fs.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/file.h>\n#include <linux/proc_fs.h>\n#include <linux/mutex.h>\n#include <linux/wait.h>\n\n#include <net/sock.h>\n#include <net/af_unix.h>\n#include <net/scm.h>\n#include <net/tcp_states.h>\n\n/* Internal data structures and random procedures: */\n\nstatic LIST_HEAD(gc_inflight_list);\nstatic LIST_HEAD(gc_candidates);\nstatic DEFINE_SPINLOCK(unix_gc_lock);\nstatic DECLARE_WAIT_QUEUE_HEAD(unix_gc_wait);\n\nunsigned int unix_tot_inflight;\n\nstruct sock *unix_get_socket(struct file *filp)\n{\n\tstruct sock *u_sock = NULL;\n\tstruct inode *inode = file_inode(filp);\n\n\t/* Socket ? */\n\tif (S_ISSOCK(inode->i_mode) && !(filp->f_mode & FMODE_PATH)) {\n\t\tstruct socket *sock = SOCKET_I(inode);\n\t\tstruct sock *s = sock->sk;\n\n\t\t/* PF_UNIX ? */\n\t\tif (s && sock->ops && sock->ops->family == PF_UNIX)\n\t\t\tu_sock = s;\n\t}\n\treturn u_sock;\n}\n\n/* Keep the number of times in flight count for the file\n * descriptor if it is for an AF_UNIX socket.\n */\n\nvoid unix_inflight(struct file *fp)\n{\n\tstruct sock *s = unix_get_socket(fp);\n\n\tspin_lock(&unix_gc_lock);\n\n\tif (s) {\n\t\tstruct unix_sock *u = unix_sk(s);\n\n\t\tif (atomic_long_inc_return(&u->inflight) == 1) {\n\t\t\tBUG_ON(!list_empty(&u->link));\n\t\t\tlist_add_tail(&u->link, &gc_inflight_list);\n\t\t} else {\n\t\t\tBUG_ON(list_empty(&u->link));\n\t\t}\n\t\tunix_tot_inflight++;\n\t}\n\tfp->f_cred->user->unix_inflight++;\n\tspin_unlock(&unix_gc_lock);\n}\n\nvoid unix_notinflight(struct file *fp)\n{\n\tstruct sock *s = unix_get_socket(fp);\n\n\tspin_lock(&unix_gc_lock);\n\n\tif (s) {\n\t\tstruct unix_sock *u = unix_sk(s);\n\n\t\tBUG_ON(list_empty(&u->link));\n\n\t\tif (atomic_long_dec_and_test(&u->inflight))\n\t\t\tlist_del_init(&u->link);\n\t\tunix_tot_inflight--;\n\t}\n\tfp->f_cred->user->unix_inflight--;\n\tspin_unlock(&unix_gc_lock);\n}\n\nstatic void scan_inflight(struct sock *x, void (*func)(struct unix_sock *),\n\t\t\t  struct sk_buff_head *hitlist)\n{\n\tstruct sk_buff *skb;\n\tstruct sk_buff *next;\n\n\tspin_lock(&x->sk_receive_queue.lock);\n\tskb_queue_walk_safe(&x->sk_receive_queue, skb, next) {\n\t\t/* Do we have file descriptors ? */\n\t\tif (UNIXCB(skb).fp) {\n\t\t\tbool hit = false;\n\t\t\t/* Process the descriptors of this socket */\n\t\t\tint nfd = UNIXCB(skb).fp->count;\n\t\t\tstruct file **fp = UNIXCB(skb).fp->fp;\n\n\t\t\twhile (nfd--) {\n\t\t\t\t/* Get the socket the fd matches if it indeed does so */\n\t\t\t\tstruct sock *sk = unix_get_socket(*fp++);\n\n\t\t\t\tif (sk) {\n\t\t\t\t\tstruct unix_sock *u = unix_sk(sk);\n\n\t\t\t\t\t/* Ignore non-candidates, they could\n\t\t\t\t\t * have been added to the queues after\n\t\t\t\t\t * starting the garbage collection\n\t\t\t\t\t */\n\t\t\t\t\tif (test_bit(UNIX_GC_CANDIDATE, &u->gc_flags)) {\n\t\t\t\t\t\thit = true;\n\n\t\t\t\t\t\tfunc(u);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (hit && hitlist != NULL) {\n\t\t\t\t__skb_unlink(skb, &x->sk_receive_queue);\n\t\t\t\t__skb_queue_tail(hitlist, skb);\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock(&x->sk_receive_queue.lock);\n}\n\nstatic void scan_children(struct sock *x, void (*func)(struct unix_sock *),\n\t\t\t  struct sk_buff_head *hitlist)\n{\n\tif (x->sk_state != TCP_LISTEN) {\n\t\tscan_inflight(x, func, hitlist);\n\t} else {\n\t\tstruct sk_buff *skb;\n\t\tstruct sk_buff *next;\n\t\tstruct unix_sock *u;\n\t\tLIST_HEAD(embryos);\n\n\t\t/* For a listening socket collect the queued embryos\n\t\t * and perform a scan on them as well.\n\t\t */\n\t\tspin_lock(&x->sk_receive_queue.lock);\n\t\tskb_queue_walk_safe(&x->sk_receive_queue, skb, next) {\n\t\t\tu = unix_sk(skb->sk);\n\n\t\t\t/* An embryo cannot be in-flight, so it's safe\n\t\t\t * to use the list link.\n\t\t\t */\n\t\t\tBUG_ON(!list_empty(&u->link));\n\t\t\tlist_add_tail(&u->link, &embryos);\n\t\t}\n\t\tspin_unlock(&x->sk_receive_queue.lock);\n\n\t\twhile (!list_empty(&embryos)) {\n\t\t\tu = list_entry(embryos.next, struct unix_sock, link);\n\t\t\tscan_inflight(&u->sk, func, hitlist);\n\t\t\tlist_del_init(&u->link);\n\t\t}\n\t}\n}\n\nstatic void dec_inflight(struct unix_sock *usk)\n{\n\tatomic_long_dec(&usk->inflight);\n}\n\nstatic void inc_inflight(struct unix_sock *usk)\n{\n\tatomic_long_inc(&usk->inflight);\n}\n\nstatic void inc_inflight_move_tail(struct unix_sock *u)\n{\n\tatomic_long_inc(&u->inflight);\n\t/* If this still might be part of a cycle, move it to the end\n\t * of the list, so that it's checked even if it was already\n\t * passed over\n\t */\n\tif (test_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags))\n\t\tlist_move_tail(&u->link, &gc_candidates);\n}\n\nstatic bool gc_in_progress;\n#define UNIX_INFLIGHT_TRIGGER_GC 16000\n\nvoid wait_for_unix_gc(void)\n{\n\t/* If number of inflight sockets is insane,\n\t * force a garbage collect right now.\n\t */\n\tif (unix_tot_inflight > UNIX_INFLIGHT_TRIGGER_GC && !gc_in_progress)\n\t\tunix_gc();\n\twait_event(unix_gc_wait, gc_in_progress == false);\n}\n\n/* The external entry point: unix_gc() */\nvoid unix_gc(void)\n{\n\tstruct unix_sock *u;\n\tstruct unix_sock *next;\n\tstruct sk_buff_head hitlist;\n\tstruct list_head cursor;\n\tLIST_HEAD(not_cycle_list);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* Avoid a recursive GC. */\n\tif (gc_in_progress)\n\t\tgoto out;\n\n\tgc_in_progress = true;\n\t/* First, select candidates for garbage collection.  Only\n\t * in-flight sockets are considered, and from those only ones\n\t * which don't have any external reference.\n\t *\n\t * Holding unix_gc_lock will protect these candidates from\n\t * being detached, and hence from gaining an external\n\t * reference.  Since there are no possible receivers, all\n\t * buffers currently on the candidates' queues stay there\n\t * during the garbage collection.\n\t *\n\t * We also know that no new candidate can be added onto the\n\t * receive queues.  Other, non candidate sockets _can_ be\n\t * added to queue, so we must make sure only to touch\n\t * candidates.\n\t */\n\tlist_for_each_entry_safe(u, next, &gc_inflight_list, link) {\n\t\tlong total_refs;\n\t\tlong inflight_refs;\n\n\t\ttotal_refs = file_count(u->sk.sk_socket->file);\n\t\tinflight_refs = atomic_long_read(&u->inflight);\n\n\t\tBUG_ON(inflight_refs < 1);\n\t\tBUG_ON(total_refs < inflight_refs);\n\t\tif (total_refs == inflight_refs) {\n\t\t\tlist_move_tail(&u->link, &gc_candidates);\n\t\t\t__set_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\t\t__set_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t}\n\t}\n\n\t/* Now remove all internal in-flight reference to children of\n\t * the candidates.\n\t */\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, dec_inflight, NULL);\n\n\t/* Restore the references for children of all candidates,\n\t * which have remaining references.  Do this recursively, so\n\t * only those remain, which form cyclic references.\n\t *\n\t * Use a \"cursor\" link, to make the list traversal safe, even\n\t * though elements might be moved about.\n\t */\n\tlist_add(&cursor, &gc_candidates);\n\twhile (cursor.next != &gc_candidates) {\n\t\tu = list_entry(cursor.next, struct unix_sock, link);\n\n\t\t/* Move cursor to after the current position. */\n\t\tlist_move(&cursor, &u->link);\n\n\t\tif (atomic_long_read(&u->inflight) > 0) {\n\t\t\tlist_move_tail(&u->link, &not_cycle_list);\n\t\t\t__clear_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t\tscan_children(&u->sk, inc_inflight_move_tail, NULL);\n\t\t}\n\t}\n\tlist_del(&cursor);\n\n\t/* not_cycle_list contains those sockets which do not make up a\n\t * cycle.  Restore these to the inflight list.\n\t */\n\twhile (!list_empty(&not_cycle_list)) {\n\t\tu = list_entry(not_cycle_list.next, struct unix_sock, link);\n\t\t__clear_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\tlist_move_tail(&u->link, &gc_inflight_list);\n\t}\n\n\t/* Now gc_candidates contains only garbage.  Restore original\n\t * inflight counters for these as well, and remove the skbuffs\n\t * which are creating the cycle(s).\n\t */\n\tskb_queue_head_init(&hitlist);\n\tlist_for_each_entry(u, &gc_candidates, link)\n\tscan_children(&u->sk, inc_inflight, &hitlist);\n\n\tspin_unlock(&unix_gc_lock);\n\n\t/* Here we are. Hitlist is filled. Die. */\n\t__skb_queue_purge(&hitlist);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* All candidates should have been detached by now. */\n\tBUG_ON(!list_empty(&gc_candidates));\n\tgc_in_progress = false;\n\twake_up(&unix_gc_wait);\n\n out:\n\tspin_unlock(&unix_gc_lock);\n}\n"], "fixing_code": ["#ifndef __LINUX_NET_AFUNIX_H\n#define __LINUX_NET_AFUNIX_H\n\n#include <linux/socket.h>\n#include <linux/un.h>\n#include <linux/mutex.h>\n#include <net/sock.h>\n\nvoid unix_inflight(struct user_struct *user, struct file *fp);\nvoid unix_notinflight(struct user_struct *user, struct file *fp);\nvoid unix_gc(void);\nvoid wait_for_unix_gc(void);\nstruct sock *unix_get_socket(struct file *filp);\nstruct sock *unix_peer_get(struct sock *);\n\n#define UNIX_HASH_SIZE\t256\n#define UNIX_HASH_BITS\t8\n\nextern unsigned int unix_tot_inflight;\nextern spinlock_t unix_table_lock;\nextern struct hlist_head unix_socket_table[2 * UNIX_HASH_SIZE];\n\nstruct unix_address {\n\tatomic_t\trefcnt;\n\tint\t\tlen;\n\tunsigned int\thash;\n\tstruct sockaddr_un name[0];\n};\n\nstruct unix_skb_parms {\n\tstruct pid\t\t*pid;\t\t/* Skb credentials\t*/\n\tkuid_t\t\t\tuid;\n\tkgid_t\t\t\tgid;\n\tstruct scm_fp_list\t*fp;\t\t/* Passed files\t\t*/\n#ifdef CONFIG_SECURITY_NETWORK\n\tu32\t\t\tsecid;\t\t/* Security ID\t\t*/\n#endif\n\tu32\t\t\tconsumed;\n};\n\n#define UNIXCB(skb) \t(*(struct unix_skb_parms *)&((skb)->cb))\n\n#define unix_state_lock(s)\tspin_lock(&unix_sk(s)->lock)\n#define unix_state_unlock(s)\tspin_unlock(&unix_sk(s)->lock)\n#define unix_state_lock_nested(s) \\\n\t\t\t\tspin_lock_nested(&unix_sk(s)->lock, \\\n\t\t\t\tSINGLE_DEPTH_NESTING)\n\n/* The AF_UNIX socket */\nstruct unix_sock {\n\t/* WARNING: sk has to be the first member */\n\tstruct sock\t\tsk;\n\tstruct unix_address     *addr;\n\tstruct path\t\tpath;\n\tstruct mutex\t\treadlock;\n\tstruct sock\t\t*peer;\n\tstruct list_head\tlink;\n\tatomic_long_t\t\tinflight;\n\tspinlock_t\t\tlock;\n\tunsigned char\t\trecursion_level;\n\tunsigned long\t\tgc_flags;\n#define UNIX_GC_CANDIDATE\t0\n#define UNIX_GC_MAYBE_CYCLE\t1\n\tstruct socket_wq\tpeer_wq;\n\twait_queue_t\t\tpeer_wake;\n};\n\nstatic inline struct unix_sock *unix_sk(const struct sock *sk)\n{\n\treturn (struct unix_sock *)sk;\n}\n\n#define peer_wait peer_wq.wait\n\nlong unix_inq_len(struct sock *sk);\nlong unix_outq_len(struct sock *sk);\n\n#ifdef CONFIG_SYSCTL\nint unix_sysctl_register(struct net *net);\nvoid unix_sysctl_unregister(struct net *net);\n#else\nstatic inline int unix_sysctl_register(struct net *net) { return 0; }\nstatic inline void unix_sysctl_unregister(struct net *net) {}\n#endif\n#endif\n", "#ifndef __LINUX_NET_SCM_H\n#define __LINUX_NET_SCM_H\n\n#include <linux/limits.h>\n#include <linux/net.h>\n#include <linux/security.h>\n#include <linux/pid.h>\n#include <linux/nsproxy.h>\n\n/* Well, we should have at least one descriptor open\n * to accept passed FDs 8)\n */\n#define SCM_MAX_FD\t253\n\nstruct scm_creds {\n\tu32\tpid;\n\tkuid_t\tuid;\n\tkgid_t\tgid;\n};\n\nstruct scm_fp_list {\n\tshort\t\t\tcount;\n\tshort\t\t\tmax;\n\tstruct user_struct\t*user;\n\tstruct file\t\t*fp[SCM_MAX_FD];\n};\n\nstruct scm_cookie {\n\tstruct pid\t\t*pid;\t\t/* Skb credentials */\n\tstruct scm_fp_list\t*fp;\t\t/* Passed files\t\t*/\n\tstruct scm_creds\tcreds;\t\t/* Skb credentials\t*/\n#ifdef CONFIG_SECURITY_NETWORK\n\tu32\t\t\tsecid;\t\t/* Passed security ID \t*/\n#endif\n};\n\nvoid scm_detach_fds(struct msghdr *msg, struct scm_cookie *scm);\nvoid scm_detach_fds_compat(struct msghdr *msg, struct scm_cookie *scm);\nint __scm_send(struct socket *sock, struct msghdr *msg, struct scm_cookie *scm);\nvoid __scm_destroy(struct scm_cookie *scm);\nstruct scm_fp_list *scm_fp_dup(struct scm_fp_list *fpl);\n\n#ifdef CONFIG_SECURITY_NETWORK\nstatic __inline__ void unix_get_peersec_dgram(struct socket *sock, struct scm_cookie *scm)\n{\n\tsecurity_socket_getpeersec_dgram(sock, NULL, &scm->secid);\n}\n#else\nstatic __inline__ void unix_get_peersec_dgram(struct socket *sock, struct scm_cookie *scm)\n{ }\n#endif /* CONFIG_SECURITY_NETWORK */\n\nstatic __inline__ void scm_set_cred(struct scm_cookie *scm,\n\t\t\t\t    struct pid *pid, kuid_t uid, kgid_t gid)\n{\n\tscm->pid  = get_pid(pid);\n\tscm->creds.pid = pid_vnr(pid);\n\tscm->creds.uid = uid;\n\tscm->creds.gid = gid;\n}\n\nstatic __inline__ void scm_destroy_cred(struct scm_cookie *scm)\n{\n\tput_pid(scm->pid);\n\tscm->pid  = NULL;\n}\n\nstatic __inline__ void scm_destroy(struct scm_cookie *scm)\n{\n\tscm_destroy_cred(scm);\n\tif (scm->fp)\n\t\t__scm_destroy(scm);\n}\n\nstatic __inline__ int scm_send(struct socket *sock, struct msghdr *msg,\n\t\t\t       struct scm_cookie *scm, bool forcecreds)\n{\n\tmemset(scm, 0, sizeof(*scm));\n\tscm->creds.uid = INVALID_UID;\n\tscm->creds.gid = INVALID_GID;\n\tif (forcecreds)\n\t\tscm_set_cred(scm, task_tgid(current), current_uid(), current_gid());\n\tunix_get_peersec_dgram(sock, scm);\n\tif (msg->msg_controllen <= 0)\n\t\treturn 0;\n\treturn __scm_send(sock, msg, scm);\n}\n\n#ifdef CONFIG_SECURITY_NETWORK\nstatic inline void scm_passec(struct socket *sock, struct msghdr *msg, struct scm_cookie *scm)\n{\n\tchar *secdata;\n\tu32 seclen;\n\tint err;\n\n\tif (test_bit(SOCK_PASSSEC, &sock->flags)) {\n\t\terr = security_secid_to_secctx(scm->secid, &secdata, &seclen);\n\n\t\tif (!err) {\n\t\t\tput_cmsg(msg, SOL_SOCKET, SCM_SECURITY, seclen, secdata);\n\t\t\tsecurity_release_secctx(secdata, seclen);\n\t\t}\n\t}\n}\n#else\nstatic inline void scm_passec(struct socket *sock, struct msghdr *msg, struct scm_cookie *scm)\n{ }\n#endif /* CONFIG_SECURITY_NETWORK */\n\nstatic __inline__ void scm_recv(struct socket *sock, struct msghdr *msg,\n\t\t\t\tstruct scm_cookie *scm, int flags)\n{\n\tif (!msg->msg_control) {\n\t\tif (test_bit(SOCK_PASSCRED, &sock->flags) || scm->fp)\n\t\t\tmsg->msg_flags |= MSG_CTRUNC;\n\t\tscm_destroy(scm);\n\t\treturn;\n\t}\n\n\tif (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\tstruct user_namespace *current_ns = current_user_ns();\n\t\tstruct ucred ucreds = {\n\t\t\t.pid = scm->creds.pid,\n\t\t\t.uid = from_kuid_munged(current_ns, scm->creds.uid),\n\t\t\t.gid = from_kgid_munged(current_ns, scm->creds.gid),\n\t\t};\n\t\tput_cmsg(msg, SOL_SOCKET, SCM_CREDENTIALS, sizeof(ucreds), &ucreds);\n\t}\n\n\tscm_destroy_cred(scm);\n\n\tscm_passec(sock, msg, scm);\n\n\tif (!scm->fp)\n\t\treturn;\n\t\n\tscm_detach_fds(msg, scm);\n}\n\n\n#endif /* __LINUX_NET_SCM_H */\n\n", "/* scm.c - Socket level control messages processing.\n *\n * Author:\tAlexey Kuznetsov, <kuznet@ms2.inr.ac.ru>\n *              Alignment and value checking mods by Craig Metz\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n */\n\n#include <linux/module.h>\n#include <linux/signal.h>\n#include <linux/capability.h>\n#include <linux/errno.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/kernel.h>\n#include <linux/stat.h>\n#include <linux/socket.h>\n#include <linux/file.h>\n#include <linux/fcntl.h>\n#include <linux/net.h>\n#include <linux/interrupt.h>\n#include <linux/netdevice.h>\n#include <linux/security.h>\n#include <linux/pid_namespace.h>\n#include <linux/pid.h>\n#include <linux/nsproxy.h>\n#include <linux/slab.h>\n\n#include <asm/uaccess.h>\n\n#include <net/protocol.h>\n#include <linux/skbuff.h>\n#include <net/sock.h>\n#include <net/compat.h>\n#include <net/scm.h>\n#include <net/cls_cgroup.h>\n\n\n/*\n *\tOnly allow a user to send credentials, that they could set with\n *\tsetu(g)id.\n */\n\nstatic __inline__ int scm_check_creds(struct ucred *creds)\n{\n\tconst struct cred *cred = current_cred();\n\tkuid_t uid = make_kuid(cred->user_ns, creds->uid);\n\tkgid_t gid = make_kgid(cred->user_ns, creds->gid);\n\n\tif (!uid_valid(uid) || !gid_valid(gid))\n\t\treturn -EINVAL;\n\n\tif ((creds->pid == task_tgid_vnr(current) ||\n\t     ns_capable(task_active_pid_ns(current)->user_ns, CAP_SYS_ADMIN)) &&\n\t    ((uid_eq(uid, cred->uid)   || uid_eq(uid, cred->euid) ||\n\t      uid_eq(uid, cred->suid)) || ns_capable(cred->user_ns, CAP_SETUID)) &&\n\t    ((gid_eq(gid, cred->gid)   || gid_eq(gid, cred->egid) ||\n\t      gid_eq(gid, cred->sgid)) || ns_capable(cred->user_ns, CAP_SETGID))) {\n\t       return 0;\n\t}\n\treturn -EPERM;\n}\n\nstatic int scm_fp_copy(struct cmsghdr *cmsg, struct scm_fp_list **fplp)\n{\n\tint *fdp = (int*)CMSG_DATA(cmsg);\n\tstruct scm_fp_list *fpl = *fplp;\n\tstruct file **fpp;\n\tint i, num;\n\n\tnum = (cmsg->cmsg_len - CMSG_ALIGN(sizeof(struct cmsghdr)))/sizeof(int);\n\n\tif (num <= 0)\n\t\treturn 0;\n\n\tif (num > SCM_MAX_FD)\n\t\treturn -EINVAL;\n\n\tif (!fpl)\n\t{\n\t\tfpl = kmalloc(sizeof(struct scm_fp_list), GFP_KERNEL);\n\t\tif (!fpl)\n\t\t\treturn -ENOMEM;\n\t\t*fplp = fpl;\n\t\tfpl->count = 0;\n\t\tfpl->max = SCM_MAX_FD;\n\t\tfpl->user = NULL;\n\t}\n\tfpp = &fpl->fp[fpl->count];\n\n\tif (fpl->count + num > fpl->max)\n\t\treturn -EINVAL;\n\n\t/*\n\t *\tVerify the descriptors and increment the usage count.\n\t */\n\n\tfor (i=0; i< num; i++)\n\t{\n\t\tint fd = fdp[i];\n\t\tstruct file *file;\n\n\t\tif (fd < 0 || !(file = fget_raw(fd)))\n\t\t\treturn -EBADF;\n\t\t*fpp++ = file;\n\t\tfpl->count++;\n\t}\n\n\tif (!fpl->user)\n\t\tfpl->user = get_uid(current_user());\n\n\treturn num;\n}\n\nvoid __scm_destroy(struct scm_cookie *scm)\n{\n\tstruct scm_fp_list *fpl = scm->fp;\n\tint i;\n\n\tif (fpl) {\n\t\tscm->fp = NULL;\n\t\tfor (i=fpl->count-1; i>=0; i--)\n\t\t\tfput(fpl->fp[i]);\n\t\tfree_uid(fpl->user);\n\t\tkfree(fpl);\n\t}\n}\nEXPORT_SYMBOL(__scm_destroy);\n\nint __scm_send(struct socket *sock, struct msghdr *msg, struct scm_cookie *p)\n{\n\tstruct cmsghdr *cmsg;\n\tint err;\n\n\tfor_each_cmsghdr(cmsg, msg) {\n\t\terr = -EINVAL;\n\n\t\t/* Verify that cmsg_len is at least sizeof(struct cmsghdr) */\n\t\t/* The first check was omitted in <= 2.2.5. The reasoning was\n\t\t   that parser checks cmsg_len in any case, so that\n\t\t   additional check would be work duplication.\n\t\t   But if cmsg_level is not SOL_SOCKET, we do not check\n\t\t   for too short ancillary data object at all! Oops.\n\t\t   OK, let's add it...\n\t\t */\n\t\tif (!CMSG_OK(msg, cmsg))\n\t\t\tgoto error;\n\n\t\tif (cmsg->cmsg_level != SOL_SOCKET)\n\t\t\tcontinue;\n\n\t\tswitch (cmsg->cmsg_type)\n\t\t{\n\t\tcase SCM_RIGHTS:\n\t\t\tif (!sock->ops || sock->ops->family != PF_UNIX)\n\t\t\t\tgoto error;\n\t\t\terr=scm_fp_copy(cmsg, &p->fp);\n\t\t\tif (err<0)\n\t\t\t\tgoto error;\n\t\t\tbreak;\n\t\tcase SCM_CREDENTIALS:\n\t\t{\n\t\t\tstruct ucred creds;\n\t\t\tkuid_t uid;\n\t\t\tkgid_t gid;\n\t\t\tif (cmsg->cmsg_len != CMSG_LEN(sizeof(struct ucred)))\n\t\t\t\tgoto error;\n\t\t\tmemcpy(&creds, CMSG_DATA(cmsg), sizeof(struct ucred));\n\t\t\terr = scm_check_creds(&creds);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\n\t\t\tp->creds.pid = creds.pid;\n\t\t\tif (!p->pid || pid_vnr(p->pid) != creds.pid) {\n\t\t\t\tstruct pid *pid;\n\t\t\t\terr = -ESRCH;\n\t\t\t\tpid = find_get_pid(creds.pid);\n\t\t\t\tif (!pid)\n\t\t\t\t\tgoto error;\n\t\t\t\tput_pid(p->pid);\n\t\t\t\tp->pid = pid;\n\t\t\t}\n\n\t\t\terr = -EINVAL;\n\t\t\tuid = make_kuid(current_user_ns(), creds.uid);\n\t\t\tgid = make_kgid(current_user_ns(), creds.gid);\n\t\t\tif (!uid_valid(uid) || !gid_valid(gid))\n\t\t\t\tgoto error;\n\n\t\t\tp->creds.uid = uid;\n\t\t\tp->creds.gid = gid;\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tif (p->fp && !p->fp->count)\n\t{\n\t\tkfree(p->fp);\n\t\tp->fp = NULL;\n\t}\n\treturn 0;\n\nerror:\n\tscm_destroy(p);\n\treturn err;\n}\nEXPORT_SYMBOL(__scm_send);\n\nint put_cmsg(struct msghdr * msg, int level, int type, int len, void *data)\n{\n\tstruct cmsghdr __user *cm\n\t\t= (__force struct cmsghdr __user *)msg->msg_control;\n\tstruct cmsghdr cmhdr;\n\tint cmlen = CMSG_LEN(len);\n\tint err;\n\n\tif (MSG_CMSG_COMPAT & msg->msg_flags)\n\t\treturn put_cmsg_compat(msg, level, type, len, data);\n\n\tif (cm==NULL || msg->msg_controllen < sizeof(*cm)) {\n\t\tmsg->msg_flags |= MSG_CTRUNC;\n\t\treturn 0; /* XXX: return error? check spec. */\n\t}\n\tif (msg->msg_controllen < cmlen) {\n\t\tmsg->msg_flags |= MSG_CTRUNC;\n\t\tcmlen = msg->msg_controllen;\n\t}\n\tcmhdr.cmsg_level = level;\n\tcmhdr.cmsg_type = type;\n\tcmhdr.cmsg_len = cmlen;\n\n\terr = -EFAULT;\n\tif (copy_to_user(cm, &cmhdr, sizeof cmhdr))\n\t\tgoto out;\n\tif (copy_to_user(CMSG_DATA(cm), data, cmlen - sizeof(struct cmsghdr)))\n\t\tgoto out;\n\tcmlen = CMSG_SPACE(len);\n\tif (msg->msg_controllen < cmlen)\n\t\tcmlen = msg->msg_controllen;\n\tmsg->msg_control += cmlen;\n\tmsg->msg_controllen -= cmlen;\n\terr = 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(put_cmsg);\n\nvoid scm_detach_fds(struct msghdr *msg, struct scm_cookie *scm)\n{\n\tstruct cmsghdr __user *cm\n\t\t= (__force struct cmsghdr __user*)msg->msg_control;\n\n\tint fdmax = 0;\n\tint fdnum = scm->fp->count;\n\tstruct file **fp = scm->fp->fp;\n\tint __user *cmfptr;\n\tint err = 0, i;\n\n\tif (MSG_CMSG_COMPAT & msg->msg_flags) {\n\t\tscm_detach_fds_compat(msg, scm);\n\t\treturn;\n\t}\n\n\tif (msg->msg_controllen > sizeof(struct cmsghdr))\n\t\tfdmax = ((msg->msg_controllen - sizeof(struct cmsghdr))\n\t\t\t / sizeof(int));\n\n\tif (fdnum < fdmax)\n\t\tfdmax = fdnum;\n\n\tfor (i=0, cmfptr=(__force int __user *)CMSG_DATA(cm); i<fdmax;\n\t     i++, cmfptr++)\n\t{\n\t\tstruct socket *sock;\n\t\tint new_fd;\n\t\terr = security_file_receive(fp[i]);\n\t\tif (err)\n\t\t\tbreak;\n\t\terr = get_unused_fd_flags(MSG_CMSG_CLOEXEC & msg->msg_flags\n\t\t\t\t\t  ? O_CLOEXEC : 0);\n\t\tif (err < 0)\n\t\t\tbreak;\n\t\tnew_fd = err;\n\t\terr = put_user(new_fd, cmfptr);\n\t\tif (err) {\n\t\t\tput_unused_fd(new_fd);\n\t\t\tbreak;\n\t\t}\n\t\t/* Bump the usage count and install the file. */\n\t\tsock = sock_from_file(fp[i], &err);\n\t\tif (sock) {\n\t\t\tsock_update_netprioidx(&sock->sk->sk_cgrp_data);\n\t\t\tsock_update_classid(&sock->sk->sk_cgrp_data);\n\t\t}\n\t\tfd_install(new_fd, get_file(fp[i]));\n\t}\n\n\tif (i > 0)\n\t{\n\t\tint cmlen = CMSG_LEN(i*sizeof(int));\n\t\terr = put_user(SOL_SOCKET, &cm->cmsg_level);\n\t\tif (!err)\n\t\t\terr = put_user(SCM_RIGHTS, &cm->cmsg_type);\n\t\tif (!err)\n\t\t\terr = put_user(cmlen, &cm->cmsg_len);\n\t\tif (!err) {\n\t\t\tcmlen = CMSG_SPACE(i*sizeof(int));\n\t\t\tif (msg->msg_controllen < cmlen)\n\t\t\t\tcmlen = msg->msg_controllen;\n\t\t\tmsg->msg_control += cmlen;\n\t\t\tmsg->msg_controllen -= cmlen;\n\t\t}\n\t}\n\tif (i < fdnum || (fdnum && fdmax <= 0))\n\t\tmsg->msg_flags |= MSG_CTRUNC;\n\n\t/*\n\t * All of the files that fit in the message have had their\n\t * usage counts incremented, so we just free the list.\n\t */\n\t__scm_destroy(scm);\n}\nEXPORT_SYMBOL(scm_detach_fds);\n\nstruct scm_fp_list *scm_fp_dup(struct scm_fp_list *fpl)\n{\n\tstruct scm_fp_list *new_fpl;\n\tint i;\n\n\tif (!fpl)\n\t\treturn NULL;\n\n\tnew_fpl = kmemdup(fpl, offsetof(struct scm_fp_list, fp[fpl->count]),\n\t\t\t  GFP_KERNEL);\n\tif (new_fpl) {\n\t\tfor (i = 0; i < fpl->count; i++)\n\t\t\tget_file(fpl->fp[i]);\n\t\tnew_fpl->max = new_fpl->count;\n\t\tnew_fpl->user = get_uid(fpl->user);\n\t}\n\treturn new_fpl;\n}\nEXPORT_SYMBOL(scm_fp_dup);\n", "/*\n * NET4:\tImplementation of BSD Unix domain sockets.\n *\n * Authors:\tAlan Cox, <alan@lxorguk.ukuu.org.uk>\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or (at your option) any later version.\n *\n * Fixes:\n *\t\tLinus Torvalds\t:\tAssorted bug cures.\n *\t\tNiibe Yutaka\t:\tasync I/O support.\n *\t\tCarsten Paeth\t:\tPF_UNIX check, address fixes.\n *\t\tAlan Cox\t:\tLimit size of allocated blocks.\n *\t\tAlan Cox\t:\tFixed the stupid socketpair bug.\n *\t\tAlan Cox\t:\tBSD compatibility fine tuning.\n *\t\tAlan Cox\t:\tFixed a bug in connect when interrupted.\n *\t\tAlan Cox\t:\tSorted out a proper draft version of\n *\t\t\t\t\tfile descriptor passing hacked up from\n *\t\t\t\t\tMike Shaver's work.\n *\t\tMarty Leisner\t:\tFixes to fd passing\n *\t\tNick Nevin\t:\trecvmsg bugfix.\n *\t\tAlan Cox\t:\tStarted proper garbage collector\n *\t\tHeiko EiBfeldt\t:\tMissing verify_area check\n *\t\tAlan Cox\t:\tStarted POSIXisms\n *\t\tAndreas Schwab\t:\tReplace inode by dentry for proper\n *\t\t\t\t\treference counting\n *\t\tKirk Petersen\t:\tMade this a module\n *\t    Christoph Rohland\t:\tElegant non-blocking accept/connect algorithm.\n *\t\t\t\t\tLots of bug fixes.\n *\t     Alexey Kuznetosv\t:\tRepaired (I hope) bugs introduces\n *\t\t\t\t\tby above two patches.\n *\t     Andrea Arcangeli\t:\tIf possible we block in connect(2)\n *\t\t\t\t\tif the max backlog of the listen socket\n *\t\t\t\t\tis been reached. This won't break\n *\t\t\t\t\told apps and it will avoid huge amount\n *\t\t\t\t\tof socks hashed (this for unix_gc()\n *\t\t\t\t\tperformances reasons).\n *\t\t\t\t\tSecurity fix that limits the max\n *\t\t\t\t\tnumber of socks to 2*max_files and\n *\t\t\t\t\tthe number of skb queueable in the\n *\t\t\t\t\tdgram receiver.\n *\t\tArtur Skawina   :\tHash function optimizations\n *\t     Alexey Kuznetsov   :\tFull scale SMP. Lot of bugs are introduced 8)\n *\t      Malcolm Beattie   :\tSet peercred for socketpair\n *\t     Michal Ostrowski   :       Module initialization cleanup.\n *\t     Arnaldo C. Melo\t:\tRemove MOD_{INC,DEC}_USE_COUNT,\n *\t     \t\t\t\tthe core infrastructure is doing that\n *\t     \t\t\t\tfor all net proto families now (2.5.69+)\n *\n *\n * Known differences from reference BSD that was tested:\n *\n *\t[TO FIX]\n *\tECONNREFUSED is not returned from one end of a connected() socket to the\n *\t\tother the moment one end closes.\n *\tfstat() doesn't return st_dev=0, and give the blksize as high water mark\n *\t\tand a fake inode identifier (nor the BSD first socket fstat twice bug).\n *\t[NOT TO FIX]\n *\taccept() returns a path name even if the connecting socket has closed\n *\t\tin the meantime (BSD loses the path and gives up).\n *\taccept() returns 0 length path for an unbound connector. BSD returns 16\n *\t\tand a null first byte in the path (but not for gethost/peername - BSD bug ??)\n *\tsocketpair(...SOCK_RAW..) doesn't panic the kernel.\n *\tBSD af_unix apparently has connect forgetting to block properly.\n *\t\t(need to check this with the POSIX spec in detail)\n *\n * Differences from 2.0.0-11-... (ANK)\n *\tBug fixes and improvements.\n *\t\t- client shutdown killed server socket.\n *\t\t- removed all useless cli/sti pairs.\n *\n *\tSemantic changes/extensions.\n *\t\t- generic control message passing.\n *\t\t- SCM_CREDENTIALS control message.\n *\t\t- \"Abstract\" (not FS based) socket bindings.\n *\t\t  Abstract names are sequences of bytes (not zero terminated)\n *\t\t  started by 0, so that this name space does not intersect\n *\t\t  with BSD names.\n */\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/stat.h>\n#include <linux/dcache.h>\n#include <linux/namei.h>\n#include <linux/socket.h>\n#include <linux/un.h>\n#include <linux/fcntl.h>\n#include <linux/termios.h>\n#include <linux/sockios.h>\n#include <linux/net.h>\n#include <linux/in.h>\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <asm/uaccess.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <net/net_namespace.h>\n#include <net/sock.h>\n#include <net/tcp_states.h>\n#include <net/af_unix.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <net/scm.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/rtnetlink.h>\n#include <linux/mount.h>\n#include <net/checksum.h>\n#include <linux/security.h>\n#include <linux/freezer.h>\n\nstruct hlist_head unix_socket_table[2 * UNIX_HASH_SIZE];\nEXPORT_SYMBOL_GPL(unix_socket_table);\nDEFINE_SPINLOCK(unix_table_lock);\nEXPORT_SYMBOL_GPL(unix_table_lock);\nstatic atomic_long_t unix_nr_socks;\n\n\nstatic struct hlist_head *unix_sockets_unbound(void *addr)\n{\n\tunsigned long hash = (unsigned long)addr;\n\n\thash ^= hash >> 16;\n\thash ^= hash >> 8;\n\thash %= UNIX_HASH_SIZE;\n\treturn &unix_socket_table[UNIX_HASH_SIZE + hash];\n}\n\n#define UNIX_ABSTRACT(sk)\t(unix_sk(sk)->addr->hash < UNIX_HASH_SIZE)\n\n#ifdef CONFIG_SECURITY_NETWORK\nstatic void unix_get_secdata(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\tUNIXCB(skb).secid = scm->secid;\n}\n\nstatic inline void unix_set_secdata(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\tscm->secid = UNIXCB(skb).secid;\n}\n\nstatic inline bool unix_secdata_eq(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\treturn (scm->secid == UNIXCB(skb).secid);\n}\n#else\nstatic inline void unix_get_secdata(struct scm_cookie *scm, struct sk_buff *skb)\n{ }\n\nstatic inline void unix_set_secdata(struct scm_cookie *scm, struct sk_buff *skb)\n{ }\n\nstatic inline bool unix_secdata_eq(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\treturn true;\n}\n#endif /* CONFIG_SECURITY_NETWORK */\n\n/*\n *  SMP locking strategy:\n *    hash table is protected with spinlock unix_table_lock\n *    each socket state is protected by separate spin lock.\n */\n\nstatic inline unsigned int unix_hash_fold(__wsum n)\n{\n\tunsigned int hash = (__force unsigned int)csum_fold(n);\n\n\thash ^= hash>>8;\n\treturn hash&(UNIX_HASH_SIZE-1);\n}\n\n#define unix_peer(sk) (unix_sk(sk)->peer)\n\nstatic inline int unix_our_peer(struct sock *sk, struct sock *osk)\n{\n\treturn unix_peer(osk) == sk;\n}\n\nstatic inline int unix_may_send(struct sock *sk, struct sock *osk)\n{\n\treturn unix_peer(osk) == NULL || unix_our_peer(sk, osk);\n}\n\nstatic inline int unix_recvq_full(struct sock const *sk)\n{\n\treturn skb_queue_len(&sk->sk_receive_queue) > sk->sk_max_ack_backlog;\n}\n\nstruct sock *unix_peer_get(struct sock *s)\n{\n\tstruct sock *peer;\n\n\tunix_state_lock(s);\n\tpeer = unix_peer(s);\n\tif (peer)\n\t\tsock_hold(peer);\n\tunix_state_unlock(s);\n\treturn peer;\n}\nEXPORT_SYMBOL_GPL(unix_peer_get);\n\nstatic inline void unix_release_addr(struct unix_address *addr)\n{\n\tif (atomic_dec_and_test(&addr->refcnt))\n\t\tkfree(addr);\n}\n\n/*\n *\tCheck unix socket name:\n *\t\t- should be not zero length.\n *\t        - if started by not zero, should be NULL terminated (FS object)\n *\t\t- if started by zero, it is abstract name.\n */\n\nstatic int unix_mkname(struct sockaddr_un *sunaddr, int len, unsigned int *hashp)\n{\n\tif (len <= sizeof(short) || len > sizeof(*sunaddr))\n\t\treturn -EINVAL;\n\tif (!sunaddr || sunaddr->sun_family != AF_UNIX)\n\t\treturn -EINVAL;\n\tif (sunaddr->sun_path[0]) {\n\t\t/*\n\t\t * This may look like an off by one error but it is a bit more\n\t\t * subtle. 108 is the longest valid AF_UNIX path for a binding.\n\t\t * sun_path[108] doesn't as such exist.  However in kernel space\n\t\t * we are guaranteed that it is a valid memory location in our\n\t\t * kernel address buffer.\n\t\t */\n\t\t((char *)sunaddr)[len] = 0;\n\t\tlen = strlen(sunaddr->sun_path)+1+sizeof(short);\n\t\treturn len;\n\t}\n\n\t*hashp = unix_hash_fold(csum_partial(sunaddr, len, 0));\n\treturn len;\n}\n\nstatic void __unix_remove_socket(struct sock *sk)\n{\n\tsk_del_node_init(sk);\n}\n\nstatic void __unix_insert_socket(struct hlist_head *list, struct sock *sk)\n{\n\tWARN_ON(!sk_unhashed(sk));\n\tsk_add_node(sk, list);\n}\n\nstatic inline void unix_remove_socket(struct sock *sk)\n{\n\tspin_lock(&unix_table_lock);\n\t__unix_remove_socket(sk);\n\tspin_unlock(&unix_table_lock);\n}\n\nstatic inline void unix_insert_socket(struct hlist_head *list, struct sock *sk)\n{\n\tspin_lock(&unix_table_lock);\n\t__unix_insert_socket(list, sk);\n\tspin_unlock(&unix_table_lock);\n}\n\nstatic struct sock *__unix_find_socket_byname(struct net *net,\n\t\t\t\t\t      struct sockaddr_un *sunname,\n\t\t\t\t\t      int len, int type, unsigned int hash)\n{\n\tstruct sock *s;\n\n\tsk_for_each(s, &unix_socket_table[hash ^ type]) {\n\t\tstruct unix_sock *u = unix_sk(s);\n\n\t\tif (!net_eq(sock_net(s), net))\n\t\t\tcontinue;\n\n\t\tif (u->addr->len == len &&\n\t\t    !memcmp(u->addr->name, sunname, len))\n\t\t\tgoto found;\n\t}\n\ts = NULL;\nfound:\n\treturn s;\n}\n\nstatic inline struct sock *unix_find_socket_byname(struct net *net,\n\t\t\t\t\t\t   struct sockaddr_un *sunname,\n\t\t\t\t\t\t   int len, int type,\n\t\t\t\t\t\t   unsigned int hash)\n{\n\tstruct sock *s;\n\n\tspin_lock(&unix_table_lock);\n\ts = __unix_find_socket_byname(net, sunname, len, type, hash);\n\tif (s)\n\t\tsock_hold(s);\n\tspin_unlock(&unix_table_lock);\n\treturn s;\n}\n\nstatic struct sock *unix_find_socket_byinode(struct inode *i)\n{\n\tstruct sock *s;\n\n\tspin_lock(&unix_table_lock);\n\tsk_for_each(s,\n\t\t    &unix_socket_table[i->i_ino & (UNIX_HASH_SIZE - 1)]) {\n\t\tstruct dentry *dentry = unix_sk(s)->path.dentry;\n\n\t\tif (dentry && d_backing_inode(dentry) == i) {\n\t\t\tsock_hold(s);\n\t\t\tgoto found;\n\t\t}\n\t}\n\ts = NULL;\nfound:\n\tspin_unlock(&unix_table_lock);\n\treturn s;\n}\n\n/* Support code for asymmetrically connected dgram sockets\n *\n * If a datagram socket is connected to a socket not itself connected\n * to the first socket (eg, /dev/log), clients may only enqueue more\n * messages if the present receive queue of the server socket is not\n * \"too large\". This means there's a second writeability condition\n * poll and sendmsg need to test. The dgram recv code will do a wake\n * up on the peer_wait wait queue of a socket upon reception of a\n * datagram which needs to be propagated to sleeping would-be writers\n * since these might not have sent anything so far. This can't be\n * accomplished via poll_wait because the lifetime of the server\n * socket might be less than that of its clients if these break their\n * association with it or if the server socket is closed while clients\n * are still connected to it and there's no way to inform \"a polling\n * implementation\" that it should let go of a certain wait queue\n *\n * In order to propagate a wake up, a wait_queue_t of the client\n * socket is enqueued on the peer_wait queue of the server socket\n * whose wake function does a wake_up on the ordinary client socket\n * wait queue. This connection is established whenever a write (or\n * poll for write) hit the flow control condition and broken when the\n * association to the server socket is dissolved or after a wake up\n * was relayed.\n */\n\nstatic int unix_dgram_peer_wake_relay(wait_queue_t *q, unsigned mode, int flags,\n\t\t\t\t      void *key)\n{\n\tstruct unix_sock *u;\n\twait_queue_head_t *u_sleep;\n\n\tu = container_of(q, struct unix_sock, peer_wake);\n\n\t__remove_wait_queue(&unix_sk(u->peer_wake.private)->peer_wait,\n\t\t\t    q);\n\tu->peer_wake.private = NULL;\n\n\t/* relaying can only happen while the wq still exists */\n\tu_sleep = sk_sleep(&u->sk);\n\tif (u_sleep)\n\t\twake_up_interruptible_poll(u_sleep, key);\n\n\treturn 0;\n}\n\nstatic int unix_dgram_peer_wake_connect(struct sock *sk, struct sock *other)\n{\n\tstruct unix_sock *u, *u_other;\n\tint rc;\n\n\tu = unix_sk(sk);\n\tu_other = unix_sk(other);\n\trc = 0;\n\tspin_lock(&u_other->peer_wait.lock);\n\n\tif (!u->peer_wake.private) {\n\t\tu->peer_wake.private = other;\n\t\t__add_wait_queue(&u_other->peer_wait, &u->peer_wake);\n\n\t\trc = 1;\n\t}\n\n\tspin_unlock(&u_other->peer_wait.lock);\n\treturn rc;\n}\n\nstatic void unix_dgram_peer_wake_disconnect(struct sock *sk,\n\t\t\t\t\t    struct sock *other)\n{\n\tstruct unix_sock *u, *u_other;\n\n\tu = unix_sk(sk);\n\tu_other = unix_sk(other);\n\tspin_lock(&u_other->peer_wait.lock);\n\n\tif (u->peer_wake.private == other) {\n\t\t__remove_wait_queue(&u_other->peer_wait, &u->peer_wake);\n\t\tu->peer_wake.private = NULL;\n\t}\n\n\tspin_unlock(&u_other->peer_wait.lock);\n}\n\nstatic void unix_dgram_peer_wake_disconnect_wakeup(struct sock *sk,\n\t\t\t\t\t\t   struct sock *other)\n{\n\tunix_dgram_peer_wake_disconnect(sk, other);\n\twake_up_interruptible_poll(sk_sleep(sk),\n\t\t\t\t   POLLOUT |\n\t\t\t\t   POLLWRNORM |\n\t\t\t\t   POLLWRBAND);\n}\n\n/* preconditions:\n *\t- unix_peer(sk) == other\n *\t- association is stable\n */\nstatic int unix_dgram_peer_wake_me(struct sock *sk, struct sock *other)\n{\n\tint connected;\n\n\tconnected = unix_dgram_peer_wake_connect(sk, other);\n\n\tif (unix_recvq_full(other))\n\t\treturn 1;\n\n\tif (connected)\n\t\tunix_dgram_peer_wake_disconnect(sk, other);\n\n\treturn 0;\n}\n\nstatic int unix_writable(const struct sock *sk)\n{\n\treturn sk->sk_state != TCP_LISTEN &&\n\t       (atomic_read(&sk->sk_wmem_alloc) << 2) <= sk->sk_sndbuf;\n}\n\nstatic void unix_write_space(struct sock *sk)\n{\n\tstruct socket_wq *wq;\n\n\trcu_read_lock();\n\tif (unix_writable(sk)) {\n\t\twq = rcu_dereference(sk->sk_wq);\n\t\tif (skwq_has_sleeper(wq))\n\t\t\twake_up_interruptible_sync_poll(&wq->wait,\n\t\t\t\tPOLLOUT | POLLWRNORM | POLLWRBAND);\n\t\tsk_wake_async(sk, SOCK_WAKE_SPACE, POLL_OUT);\n\t}\n\trcu_read_unlock();\n}\n\n/* When dgram socket disconnects (or changes its peer), we clear its receive\n * queue of packets arrived from previous peer. First, it allows to do\n * flow control based only on wmem_alloc; second, sk connected to peer\n * may receive messages only from that peer. */\nstatic void unix_dgram_disconnected(struct sock *sk, struct sock *other)\n{\n\tif (!skb_queue_empty(&sk->sk_receive_queue)) {\n\t\tskb_queue_purge(&sk->sk_receive_queue);\n\t\twake_up_interruptible_all(&unix_sk(sk)->peer_wait);\n\n\t\t/* If one link of bidirectional dgram pipe is disconnected,\n\t\t * we signal error. Messages are lost. Do not make this,\n\t\t * when peer was not connected to us.\n\t\t */\n\t\tif (!sock_flag(other, SOCK_DEAD) && unix_peer(other) == sk) {\n\t\t\tother->sk_err = ECONNRESET;\n\t\t\tother->sk_error_report(other);\n\t\t}\n\t}\n}\n\nstatic void unix_sock_destructor(struct sock *sk)\n{\n\tstruct unix_sock *u = unix_sk(sk);\n\n\tskb_queue_purge(&sk->sk_receive_queue);\n\n\tWARN_ON(atomic_read(&sk->sk_wmem_alloc));\n\tWARN_ON(!sk_unhashed(sk));\n\tWARN_ON(sk->sk_socket);\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tpr_info(\"Attempt to release alive unix socket: %p\\n\", sk);\n\t\treturn;\n\t}\n\n\tif (u->addr)\n\t\tunix_release_addr(u->addr);\n\n\tatomic_long_dec(&unix_nr_socks);\n\tlocal_bh_disable();\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\n\tlocal_bh_enable();\n#ifdef UNIX_REFCNT_DEBUG\n\tpr_debug(\"UNIX %p is destroyed, %ld are still alive.\\n\", sk,\n\t\tatomic_long_read(&unix_nr_socks));\n#endif\n}\n\nstatic void unix_release_sock(struct sock *sk, int embrion)\n{\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct path path;\n\tstruct sock *skpair;\n\tstruct sk_buff *skb;\n\tint state;\n\n\tunix_remove_socket(sk);\n\n\t/* Clear state */\n\tunix_state_lock(sk);\n\tsock_orphan(sk);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\tpath\t     = u->path;\n\tu->path.dentry = NULL;\n\tu->path.mnt = NULL;\n\tstate = sk->sk_state;\n\tsk->sk_state = TCP_CLOSE;\n\tunix_state_unlock(sk);\n\n\twake_up_interruptible_all(&u->peer_wait);\n\n\tskpair = unix_peer(sk);\n\n\tif (skpair != NULL) {\n\t\tif (sk->sk_type == SOCK_STREAM || sk->sk_type == SOCK_SEQPACKET) {\n\t\t\tunix_state_lock(skpair);\n\t\t\t/* No more writes */\n\t\t\tskpair->sk_shutdown = SHUTDOWN_MASK;\n\t\t\tif (!skb_queue_empty(&sk->sk_receive_queue) || embrion)\n\t\t\t\tskpair->sk_err = ECONNRESET;\n\t\t\tunix_state_unlock(skpair);\n\t\t\tskpair->sk_state_change(skpair);\n\t\t\tsk_wake_async(skpair, SOCK_WAKE_WAITD, POLL_HUP);\n\t\t}\n\n\t\tunix_dgram_peer_wake_disconnect(sk, skpair);\n\t\tsock_put(skpair); /* It may now die */\n\t\tunix_peer(sk) = NULL;\n\t}\n\n\t/* Try to flush out this socket. Throw out buffers at least */\n\n\twhile ((skb = skb_dequeue(&sk->sk_receive_queue)) != NULL) {\n\t\tif (state == TCP_LISTEN)\n\t\t\tunix_release_sock(skb->sk, 1);\n\t\t/* passed fds are erased in the kfree_skb hook\t      */\n\t\tUNIXCB(skb).consumed = skb->len;\n\t\tkfree_skb(skb);\n\t}\n\n\tif (path.dentry)\n\t\tpath_put(&path);\n\n\tsock_put(sk);\n\n\t/* ---- Socket is dead now and most probably destroyed ---- */\n\n\t/*\n\t * Fixme: BSD difference: In BSD all sockets connected to us get\n\t *\t  ECONNRESET and we die on the spot. In Linux we behave\n\t *\t  like files and pipes do and wait for the last\n\t *\t  dereference.\n\t *\n\t * Can't we simply set sock->err?\n\t *\n\t *\t  What the above comment does talk about? --ANK(980817)\n\t */\n\n\tif (unix_tot_inflight)\n\t\tunix_gc();\t\t/* Garbage collect fds */\n}\n\nstatic void init_peercred(struct sock *sk)\n{\n\tput_pid(sk->sk_peer_pid);\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tsk->sk_peer_pid  = get_pid(task_tgid(current));\n\tsk->sk_peer_cred = get_current_cred();\n}\n\nstatic void copy_peercred(struct sock *sk, struct sock *peersk)\n{\n\tput_pid(sk->sk_peer_pid);\n\tif (sk->sk_peer_cred)\n\t\tput_cred(sk->sk_peer_cred);\n\tsk->sk_peer_pid  = get_pid(peersk->sk_peer_pid);\n\tsk->sk_peer_cred = get_cred(peersk->sk_peer_cred);\n}\n\nstatic int unix_listen(struct socket *sock, int backlog)\n{\n\tint err;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct pid *old_pid = NULL;\n\n\terr = -EOPNOTSUPP;\n\tif (sock->type != SOCK_STREAM && sock->type != SOCK_SEQPACKET)\n\t\tgoto out;\t/* Only stream/seqpacket sockets accept */\n\terr = -EINVAL;\n\tif (!u->addr)\n\t\tgoto out;\t/* No listens on an unbound socket */\n\tunix_state_lock(sk);\n\tif (sk->sk_state != TCP_CLOSE && sk->sk_state != TCP_LISTEN)\n\t\tgoto out_unlock;\n\tif (backlog > sk->sk_max_ack_backlog)\n\t\twake_up_interruptible_all(&u->peer_wait);\n\tsk->sk_max_ack_backlog\t= backlog;\n\tsk->sk_state\t\t= TCP_LISTEN;\n\t/* set credentials so connect can copy them */\n\tinit_peercred(sk);\n\terr = 0;\n\nout_unlock:\n\tunix_state_unlock(sk);\n\tput_pid(old_pid);\nout:\n\treturn err;\n}\n\nstatic int unix_release(struct socket *);\nstatic int unix_bind(struct socket *, struct sockaddr *, int);\nstatic int unix_stream_connect(struct socket *, struct sockaddr *,\n\t\t\t       int addr_len, int flags);\nstatic int unix_socketpair(struct socket *, struct socket *);\nstatic int unix_accept(struct socket *, struct socket *, int);\nstatic int unix_getname(struct socket *, struct sockaddr *, int *, int);\nstatic unsigned int unix_poll(struct file *, struct socket *, poll_table *);\nstatic unsigned int unix_dgram_poll(struct file *, struct socket *,\n\t\t\t\t    poll_table *);\nstatic int unix_ioctl(struct socket *, unsigned int, unsigned long);\nstatic int unix_shutdown(struct socket *, int);\nstatic int unix_stream_sendmsg(struct socket *, struct msghdr *, size_t);\nstatic int unix_stream_recvmsg(struct socket *, struct msghdr *, size_t, int);\nstatic ssize_t unix_stream_sendpage(struct socket *, struct page *, int offset,\n\t\t\t\t    size_t size, int flags);\nstatic ssize_t unix_stream_splice_read(struct socket *,  loff_t *ppos,\n\t\t\t\t       struct pipe_inode_info *, size_t size,\n\t\t\t\t       unsigned int flags);\nstatic int unix_dgram_sendmsg(struct socket *, struct msghdr *, size_t);\nstatic int unix_dgram_recvmsg(struct socket *, struct msghdr *, size_t, int);\nstatic int unix_dgram_connect(struct socket *, struct sockaddr *,\n\t\t\t      int, int);\nstatic int unix_seqpacket_sendmsg(struct socket *, struct msghdr *, size_t);\nstatic int unix_seqpacket_recvmsg(struct socket *, struct msghdr *, size_t,\n\t\t\t\t  int);\n\nstatic int unix_set_peek_off(struct sock *sk, int val)\n{\n\tstruct unix_sock *u = unix_sk(sk);\n\n\tif (mutex_lock_interruptible(&u->readlock))\n\t\treturn -EINTR;\n\n\tsk->sk_peek_off = val;\n\tmutex_unlock(&u->readlock);\n\n\treturn 0;\n}\n\n\nstatic const struct proto_ops unix_stream_ops = {\n\t.family =\tPF_UNIX,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tunix_release,\n\t.bind =\t\tunix_bind,\n\t.connect =\tunix_stream_connect,\n\t.socketpair =\tunix_socketpair,\n\t.accept =\tunix_accept,\n\t.getname =\tunix_getname,\n\t.poll =\t\tunix_poll,\n\t.ioctl =\tunix_ioctl,\n\t.listen =\tunix_listen,\n\t.shutdown =\tunix_shutdown,\n\t.setsockopt =\tsock_no_setsockopt,\n\t.getsockopt =\tsock_no_getsockopt,\n\t.sendmsg =\tunix_stream_sendmsg,\n\t.recvmsg =\tunix_stream_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tunix_stream_sendpage,\n\t.splice_read =\tunix_stream_splice_read,\n\t.set_peek_off =\tunix_set_peek_off,\n};\n\nstatic const struct proto_ops unix_dgram_ops = {\n\t.family =\tPF_UNIX,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tunix_release,\n\t.bind =\t\tunix_bind,\n\t.connect =\tunix_dgram_connect,\n\t.socketpair =\tunix_socketpair,\n\t.accept =\tsock_no_accept,\n\t.getname =\tunix_getname,\n\t.poll =\t\tunix_dgram_poll,\n\t.ioctl =\tunix_ioctl,\n\t.listen =\tsock_no_listen,\n\t.shutdown =\tunix_shutdown,\n\t.setsockopt =\tsock_no_setsockopt,\n\t.getsockopt =\tsock_no_getsockopt,\n\t.sendmsg =\tunix_dgram_sendmsg,\n\t.recvmsg =\tunix_dgram_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tsock_no_sendpage,\n\t.set_peek_off =\tunix_set_peek_off,\n};\n\nstatic const struct proto_ops unix_seqpacket_ops = {\n\t.family =\tPF_UNIX,\n\t.owner =\tTHIS_MODULE,\n\t.release =\tunix_release,\n\t.bind =\t\tunix_bind,\n\t.connect =\tunix_stream_connect,\n\t.socketpair =\tunix_socketpair,\n\t.accept =\tunix_accept,\n\t.getname =\tunix_getname,\n\t.poll =\t\tunix_dgram_poll,\n\t.ioctl =\tunix_ioctl,\n\t.listen =\tunix_listen,\n\t.shutdown =\tunix_shutdown,\n\t.setsockopt =\tsock_no_setsockopt,\n\t.getsockopt =\tsock_no_getsockopt,\n\t.sendmsg =\tunix_seqpacket_sendmsg,\n\t.recvmsg =\tunix_seqpacket_recvmsg,\n\t.mmap =\t\tsock_no_mmap,\n\t.sendpage =\tsock_no_sendpage,\n\t.set_peek_off =\tunix_set_peek_off,\n};\n\nstatic struct proto unix_proto = {\n\t.name\t\t\t= \"UNIX\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.obj_size\t\t= sizeof(struct unix_sock),\n};\n\n/*\n * AF_UNIX sockets do not interact with hardware, hence they\n * dont trigger interrupts - so it's safe for them to have\n * bh-unsafe locking for their sk_receive_queue.lock. Split off\n * this special lock-class by reinitializing the spinlock key:\n */\nstatic struct lock_class_key af_unix_sk_receive_queue_lock_key;\n\nstatic struct sock *unix_create1(struct net *net, struct socket *sock, int kern)\n{\n\tstruct sock *sk = NULL;\n\tstruct unix_sock *u;\n\n\tatomic_long_inc(&unix_nr_socks);\n\tif (atomic_long_read(&unix_nr_socks) > 2 * get_max_files())\n\t\tgoto out;\n\n\tsk = sk_alloc(net, PF_UNIX, GFP_KERNEL, &unix_proto, kern);\n\tif (!sk)\n\t\tgoto out;\n\n\tsock_init_data(sock, sk);\n\tlockdep_set_class(&sk->sk_receive_queue.lock,\n\t\t\t\t&af_unix_sk_receive_queue_lock_key);\n\n\tsk->sk_write_space\t= unix_write_space;\n\tsk->sk_max_ack_backlog\t= net->unx.sysctl_max_dgram_qlen;\n\tsk->sk_destruct\t\t= unix_sock_destructor;\n\tu\t  = unix_sk(sk);\n\tu->path.dentry = NULL;\n\tu->path.mnt = NULL;\n\tspin_lock_init(&u->lock);\n\tatomic_long_set(&u->inflight, 0);\n\tINIT_LIST_HEAD(&u->link);\n\tmutex_init(&u->readlock); /* single task reading lock */\n\tinit_waitqueue_head(&u->peer_wait);\n\tinit_waitqueue_func_entry(&u->peer_wake, unix_dgram_peer_wake_relay);\n\tunix_insert_socket(unix_sockets_unbound(sk), sk);\nout:\n\tif (sk == NULL)\n\t\tatomic_long_dec(&unix_nr_socks);\n\telse {\n\t\tlocal_bh_disable();\n\t\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\n\t\tlocal_bh_enable();\n\t}\n\treturn sk;\n}\n\nstatic int unix_create(struct net *net, struct socket *sock, int protocol,\n\t\t       int kern)\n{\n\tif (protocol && protocol != PF_UNIX)\n\t\treturn -EPROTONOSUPPORT;\n\n\tsock->state = SS_UNCONNECTED;\n\n\tswitch (sock->type) {\n\tcase SOCK_STREAM:\n\t\tsock->ops = &unix_stream_ops;\n\t\tbreak;\n\t\t/*\n\t\t *\tBelieve it or not BSD has AF_UNIX, SOCK_RAW though\n\t\t *\tnothing uses it.\n\t\t */\n\tcase SOCK_RAW:\n\t\tsock->type = SOCK_DGRAM;\n\tcase SOCK_DGRAM:\n\t\tsock->ops = &unix_dgram_ops;\n\t\tbreak;\n\tcase SOCK_SEQPACKET:\n\t\tsock->ops = &unix_seqpacket_ops;\n\t\tbreak;\n\tdefault:\n\t\treturn -ESOCKTNOSUPPORT;\n\t}\n\n\treturn unix_create1(net, sock, kern) ? 0 : -ENOMEM;\n}\n\nstatic int unix_release(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (!sk)\n\t\treturn 0;\n\n\tunix_release_sock(sk, 0);\n\tsock->sk = NULL;\n\n\treturn 0;\n}\n\nstatic int unix_autobind(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct unix_sock *u = unix_sk(sk);\n\tstatic u32 ordernum = 1;\n\tstruct unix_address *addr;\n\tint err;\n\tunsigned int retries = 0;\n\n\terr = mutex_lock_interruptible(&u->readlock);\n\tif (err)\n\t\treturn err;\n\n\terr = 0;\n\tif (u->addr)\n\t\tgoto out;\n\n\terr = -ENOMEM;\n\taddr = kzalloc(sizeof(*addr) + sizeof(short) + 16, GFP_KERNEL);\n\tif (!addr)\n\t\tgoto out;\n\n\taddr->name->sun_family = AF_UNIX;\n\tatomic_set(&addr->refcnt, 1);\n\nretry:\n\taddr->len = sprintf(addr->name->sun_path+1, \"%05x\", ordernum) + 1 + sizeof(short);\n\taddr->hash = unix_hash_fold(csum_partial(addr->name, addr->len, 0));\n\n\tspin_lock(&unix_table_lock);\n\tordernum = (ordernum+1)&0xFFFFF;\n\n\tif (__unix_find_socket_byname(net, addr->name, addr->len, sock->type,\n\t\t\t\t      addr->hash)) {\n\t\tspin_unlock(&unix_table_lock);\n\t\t/*\n\t\t * __unix_find_socket_byname() may take long time if many names\n\t\t * are already in use.\n\t\t */\n\t\tcond_resched();\n\t\t/* Give up if all names seems to be in use. */\n\t\tif (retries++ == 0xFFFFF) {\n\t\t\terr = -ENOSPC;\n\t\t\tkfree(addr);\n\t\t\tgoto out;\n\t\t}\n\t\tgoto retry;\n\t}\n\taddr->hash ^= sk->sk_type;\n\n\t__unix_remove_socket(sk);\n\tu->addr = addr;\n\t__unix_insert_socket(&unix_socket_table[addr->hash], sk);\n\tspin_unlock(&unix_table_lock);\n\terr = 0;\n\nout:\tmutex_unlock(&u->readlock);\n\treturn err;\n}\n\nstatic struct sock *unix_find_other(struct net *net,\n\t\t\t\t    struct sockaddr_un *sunname, int len,\n\t\t\t\t    int type, unsigned int hash, int *error)\n{\n\tstruct sock *u;\n\tstruct path path;\n\tint err = 0;\n\n\tif (sunname->sun_path[0]) {\n\t\tstruct inode *inode;\n\t\terr = kern_path(sunname->sun_path, LOOKUP_FOLLOW, &path);\n\t\tif (err)\n\t\t\tgoto fail;\n\t\tinode = d_backing_inode(path.dentry);\n\t\terr = inode_permission(inode, MAY_WRITE);\n\t\tif (err)\n\t\t\tgoto put_fail;\n\n\t\terr = -ECONNREFUSED;\n\t\tif (!S_ISSOCK(inode->i_mode))\n\t\t\tgoto put_fail;\n\t\tu = unix_find_socket_byinode(inode);\n\t\tif (!u)\n\t\t\tgoto put_fail;\n\n\t\tif (u->sk_type == type)\n\t\t\ttouch_atime(&path);\n\n\t\tpath_put(&path);\n\n\t\terr = -EPROTOTYPE;\n\t\tif (u->sk_type != type) {\n\t\t\tsock_put(u);\n\t\t\tgoto fail;\n\t\t}\n\t} else {\n\t\terr = -ECONNREFUSED;\n\t\tu = unix_find_socket_byname(net, sunname, len, type, hash);\n\t\tif (u) {\n\t\t\tstruct dentry *dentry;\n\t\t\tdentry = unix_sk(u)->path.dentry;\n\t\t\tif (dentry)\n\t\t\t\ttouch_atime(&unix_sk(u)->path);\n\t\t} else\n\t\t\tgoto fail;\n\t}\n\treturn u;\n\nput_fail:\n\tpath_put(&path);\nfail:\n\t*error = err;\n\treturn NULL;\n}\n\nstatic int unix_mknod(struct dentry *dentry, struct path *path, umode_t mode,\n\t\t      struct path *res)\n{\n\tint err;\n\n\terr = security_path_mknod(path, dentry, mode, 0);\n\tif (!err) {\n\t\terr = vfs_mknod(d_inode(path->dentry), dentry, mode, 0);\n\t\tif (!err) {\n\t\t\tres->mnt = mntget(path->mnt);\n\t\t\tres->dentry = dget(dentry);\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic int unix_bind(struct socket *sock, struct sockaddr *uaddr, int addr_len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sockaddr_un *sunaddr = (struct sockaddr_un *)uaddr;\n\tchar *sun_path = sunaddr->sun_path;\n\tint err, name_err;\n\tunsigned int hash;\n\tstruct unix_address *addr;\n\tstruct hlist_head *list;\n\tstruct path path;\n\tstruct dentry *dentry;\n\n\terr = -EINVAL;\n\tif (sunaddr->sun_family != AF_UNIX)\n\t\tgoto out;\n\n\tif (addr_len == sizeof(short)) {\n\t\terr = unix_autobind(sock);\n\t\tgoto out;\n\t}\n\n\terr = unix_mkname(sunaddr, addr_len, &hash);\n\tif (err < 0)\n\t\tgoto out;\n\taddr_len = err;\n\n\tname_err = 0;\n\tdentry = NULL;\n\tif (sun_path[0]) {\n\t\t/* Get the parent directory, calculate the hash for last\n\t\t * component.\n\t\t */\n\t\tdentry = kern_path_create(AT_FDCWD, sun_path, &path, 0);\n\n\t\tif (IS_ERR(dentry)) {\n\t\t\t/* delay report until after 'already bound' check */\n\t\t\tname_err = PTR_ERR(dentry);\n\t\t\tdentry = NULL;\n\t\t}\n\t}\n\n\terr = mutex_lock_interruptible(&u->readlock);\n\tif (err)\n\t\tgoto out_path;\n\n\terr = -EINVAL;\n\tif (u->addr)\n\t\tgoto out_up;\n\n\tif (name_err) {\n\t\terr = name_err == -EEXIST ? -EADDRINUSE : name_err;\n\t\tgoto out_up;\n\t}\n\n\terr = -ENOMEM;\n\taddr = kmalloc(sizeof(*addr)+addr_len, GFP_KERNEL);\n\tif (!addr)\n\t\tgoto out_up;\n\n\tmemcpy(addr->name, sunaddr, addr_len);\n\taddr->len = addr_len;\n\taddr->hash = hash ^ sk->sk_type;\n\tatomic_set(&addr->refcnt, 1);\n\n\tif (dentry) {\n\t\tstruct path u_path;\n\t\tumode_t mode = S_IFSOCK |\n\t\t       (SOCK_INODE(sock)->i_mode & ~current_umask());\n\t\terr = unix_mknod(dentry, &path, mode, &u_path);\n\t\tif (err) {\n\t\t\tif (err == -EEXIST)\n\t\t\t\terr = -EADDRINUSE;\n\t\t\tunix_release_addr(addr);\n\t\t\tgoto out_up;\n\t\t}\n\t\taddr->hash = UNIX_HASH_SIZE;\n\t\thash = d_backing_inode(dentry)->i_ino & (UNIX_HASH_SIZE - 1);\n\t\tspin_lock(&unix_table_lock);\n\t\tu->path = u_path;\n\t\tlist = &unix_socket_table[hash];\n\t} else {\n\t\tspin_lock(&unix_table_lock);\n\t\terr = -EADDRINUSE;\n\t\tif (__unix_find_socket_byname(net, sunaddr, addr_len,\n\t\t\t\t\t      sk->sk_type, hash)) {\n\t\t\tunix_release_addr(addr);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tlist = &unix_socket_table[addr->hash];\n\t}\n\n\terr = 0;\n\t__unix_remove_socket(sk);\n\tu->addr = addr;\n\t__unix_insert_socket(list, sk);\n\nout_unlock:\n\tspin_unlock(&unix_table_lock);\nout_up:\n\tmutex_unlock(&u->readlock);\nout_path:\n\tif (dentry)\n\t\tdone_path_create(&path, dentry);\n\nout:\n\treturn err;\n}\n\nstatic void unix_state_double_lock(struct sock *sk1, struct sock *sk2)\n{\n\tif (unlikely(sk1 == sk2) || !sk2) {\n\t\tunix_state_lock(sk1);\n\t\treturn;\n\t}\n\tif (sk1 < sk2) {\n\t\tunix_state_lock(sk1);\n\t\tunix_state_lock_nested(sk2);\n\t} else {\n\t\tunix_state_lock(sk2);\n\t\tunix_state_lock_nested(sk1);\n\t}\n}\n\nstatic void unix_state_double_unlock(struct sock *sk1, struct sock *sk2)\n{\n\tif (unlikely(sk1 == sk2) || !sk2) {\n\t\tunix_state_unlock(sk1);\n\t\treturn;\n\t}\n\tunix_state_unlock(sk1);\n\tunix_state_unlock(sk2);\n}\n\nstatic int unix_dgram_connect(struct socket *sock, struct sockaddr *addr,\n\t\t\t      int alen, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct sockaddr_un *sunaddr = (struct sockaddr_un *)addr;\n\tstruct sock *other;\n\tunsigned int hash;\n\tint err;\n\n\tif (addr->sa_family != AF_UNSPEC) {\n\t\terr = unix_mkname(sunaddr, alen, &hash);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\talen = err;\n\n\t\tif (test_bit(SOCK_PASSCRED, &sock->flags) &&\n\t\t    !unix_sk(sk)->addr && (err = unix_autobind(sock)) != 0)\n\t\t\tgoto out;\n\nrestart:\n\t\tother = unix_find_other(net, sunaddr, alen, sock->type, hash, &err);\n\t\tif (!other)\n\t\t\tgoto out;\n\n\t\tunix_state_double_lock(sk, other);\n\n\t\t/* Apparently VFS overslept socket death. Retry. */\n\t\tif (sock_flag(other, SOCK_DEAD)) {\n\t\t\tunix_state_double_unlock(sk, other);\n\t\t\tsock_put(other);\n\t\t\tgoto restart;\n\t\t}\n\n\t\terr = -EPERM;\n\t\tif (!unix_may_send(sk, other))\n\t\t\tgoto out_unlock;\n\n\t\terr = security_unix_may_send(sk->sk_socket, other->sk_socket);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\n\t} else {\n\t\t/*\n\t\t *\t1003.1g breaking connected state with AF_UNSPEC\n\t\t */\n\t\tother = NULL;\n\t\tunix_state_double_lock(sk, other);\n\t}\n\n\t/*\n\t * If it was connected, reconnect.\n\t */\n\tif (unix_peer(sk)) {\n\t\tstruct sock *old_peer = unix_peer(sk);\n\t\tunix_peer(sk) = other;\n\t\tunix_dgram_peer_wake_disconnect_wakeup(sk, old_peer);\n\n\t\tunix_state_double_unlock(sk, other);\n\n\t\tif (other != old_peer)\n\t\t\tunix_dgram_disconnected(sk, old_peer);\n\t\tsock_put(old_peer);\n\t} else {\n\t\tunix_peer(sk) = other;\n\t\tunix_state_double_unlock(sk, other);\n\t}\n\treturn 0;\n\nout_unlock:\n\tunix_state_double_unlock(sk, other);\n\tsock_put(other);\nout:\n\treturn err;\n}\n\nstatic long unix_wait_for_peer(struct sock *other, long timeo)\n{\n\tstruct unix_sock *u = unix_sk(other);\n\tint sched;\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait_exclusive(&u->peer_wait, &wait, TASK_INTERRUPTIBLE);\n\n\tsched = !sock_flag(other, SOCK_DEAD) &&\n\t\t!(other->sk_shutdown & RCV_SHUTDOWN) &&\n\t\tunix_recvq_full(other);\n\n\tunix_state_unlock(other);\n\n\tif (sched)\n\t\ttimeo = schedule_timeout(timeo);\n\n\tfinish_wait(&u->peer_wait, &wait);\n\treturn timeo;\n}\n\nstatic int unix_stream_connect(struct socket *sock, struct sockaddr *uaddr,\n\t\t\t       int addr_len, int flags)\n{\n\tstruct sockaddr_un *sunaddr = (struct sockaddr_un *)uaddr;\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct unix_sock *u = unix_sk(sk), *newu, *otheru;\n\tstruct sock *newsk = NULL;\n\tstruct sock *other = NULL;\n\tstruct sk_buff *skb = NULL;\n\tunsigned int hash;\n\tint st;\n\tint err;\n\tlong timeo;\n\n\terr = unix_mkname(sunaddr, addr_len, &hash);\n\tif (err < 0)\n\t\tgoto out;\n\taddr_len = err;\n\n\tif (test_bit(SOCK_PASSCRED, &sock->flags) && !u->addr &&\n\t    (err = unix_autobind(sock)) != 0)\n\t\tgoto out;\n\n\ttimeo = sock_sndtimeo(sk, flags & O_NONBLOCK);\n\n\t/* First of all allocate resources.\n\t   If we will make it after state is locked,\n\t   we will have to recheck all again in any case.\n\t */\n\n\terr = -ENOMEM;\n\n\t/* create new sock for complete connection */\n\tnewsk = unix_create1(sock_net(sk), NULL, 0);\n\tif (newsk == NULL)\n\t\tgoto out;\n\n\t/* Allocate skb for sending to listening sock */\n\tskb = sock_wmalloc(newsk, 1, 0, GFP_KERNEL);\n\tif (skb == NULL)\n\t\tgoto out;\n\nrestart:\n\t/*  Find listening sock. */\n\tother = unix_find_other(net, sunaddr, addr_len, sk->sk_type, hash, &err);\n\tif (!other)\n\t\tgoto out;\n\n\t/* Latch state of peer */\n\tunix_state_lock(other);\n\n\t/* Apparently VFS overslept socket death. Retry. */\n\tif (sock_flag(other, SOCK_DEAD)) {\n\t\tunix_state_unlock(other);\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\terr = -ECONNREFUSED;\n\tif (other->sk_state != TCP_LISTEN)\n\t\tgoto out_unlock;\n\tif (other->sk_shutdown & RCV_SHUTDOWN)\n\t\tgoto out_unlock;\n\n\tif (unix_recvq_full(other)) {\n\t\terr = -EAGAIN;\n\t\tif (!timeo)\n\t\t\tgoto out_unlock;\n\n\t\ttimeo = unix_wait_for_peer(other, timeo);\n\n\t\terr = sock_intr_errno(timeo);\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\t/* Latch our state.\n\n\t   It is tricky place. We need to grab our state lock and cannot\n\t   drop lock on peer. It is dangerous because deadlock is\n\t   possible. Connect to self case and simultaneous\n\t   attempt to connect are eliminated by checking socket\n\t   state. other is TCP_LISTEN, if sk is TCP_LISTEN we\n\t   check this before attempt to grab lock.\n\n\t   Well, and we have to recheck the state after socket locked.\n\t */\n\tst = sk->sk_state;\n\n\tswitch (st) {\n\tcase TCP_CLOSE:\n\t\t/* This is ok... continue with connect */\n\t\tbreak;\n\tcase TCP_ESTABLISHED:\n\t\t/* Socket is already connected */\n\t\terr = -EISCONN;\n\t\tgoto out_unlock;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tunix_state_lock_nested(sk);\n\n\tif (sk->sk_state != st) {\n\t\tunix_state_unlock(sk);\n\t\tunix_state_unlock(other);\n\t\tsock_put(other);\n\t\tgoto restart;\n\t}\n\n\terr = security_unix_stream_connect(sk, other, newsk);\n\tif (err) {\n\t\tunix_state_unlock(sk);\n\t\tgoto out_unlock;\n\t}\n\n\t/* The way is open! Fastly set all the necessary fields... */\n\n\tsock_hold(sk);\n\tunix_peer(newsk)\t= sk;\n\tnewsk->sk_state\t\t= TCP_ESTABLISHED;\n\tnewsk->sk_type\t\t= sk->sk_type;\n\tinit_peercred(newsk);\n\tnewu = unix_sk(newsk);\n\tRCU_INIT_POINTER(newsk->sk_wq, &newu->peer_wq);\n\totheru = unix_sk(other);\n\n\t/* copy address information from listening to new sock*/\n\tif (otheru->addr) {\n\t\tatomic_inc(&otheru->addr->refcnt);\n\t\tnewu->addr = otheru->addr;\n\t}\n\tif (otheru->path.dentry) {\n\t\tpath_get(&otheru->path);\n\t\tnewu->path = otheru->path;\n\t}\n\n\t/* Set credentials */\n\tcopy_peercred(sk, other);\n\n\tsock->state\t= SS_CONNECTED;\n\tsk->sk_state\t= TCP_ESTABLISHED;\n\tsock_hold(newsk);\n\n\tsmp_mb__after_atomic();\t/* sock_hold() does an atomic_inc() */\n\tunix_peer(sk)\t= newsk;\n\n\tunix_state_unlock(sk);\n\n\t/* take ten and and send info to listening sock */\n\tspin_lock(&other->sk_receive_queue.lock);\n\t__skb_queue_tail(&other->sk_receive_queue, skb);\n\tspin_unlock(&other->sk_receive_queue.lock);\n\tunix_state_unlock(other);\n\tother->sk_data_ready(other);\n\tsock_put(other);\n\treturn 0;\n\nout_unlock:\n\tif (other)\n\t\tunix_state_unlock(other);\n\nout:\n\tkfree_skb(skb);\n\tif (newsk)\n\t\tunix_release_sock(newsk, 0);\n\tif (other)\n\t\tsock_put(other);\n\treturn err;\n}\n\nstatic int unix_socketpair(struct socket *socka, struct socket *sockb)\n{\n\tstruct sock *ska = socka->sk, *skb = sockb->sk;\n\n\t/* Join our sockets back to back */\n\tsock_hold(ska);\n\tsock_hold(skb);\n\tunix_peer(ska) = skb;\n\tunix_peer(skb) = ska;\n\tinit_peercred(ska);\n\tinit_peercred(skb);\n\n\tif (ska->sk_type != SOCK_DGRAM) {\n\t\tska->sk_state = TCP_ESTABLISHED;\n\t\tskb->sk_state = TCP_ESTABLISHED;\n\t\tsocka->state  = SS_CONNECTED;\n\t\tsockb->state  = SS_CONNECTED;\n\t}\n\treturn 0;\n}\n\nstatic void unix_sock_inherit_flags(const struct socket *old,\n\t\t\t\t    struct socket *new)\n{\n\tif (test_bit(SOCK_PASSCRED, &old->flags))\n\t\tset_bit(SOCK_PASSCRED, &new->flags);\n\tif (test_bit(SOCK_PASSSEC, &old->flags))\n\t\tset_bit(SOCK_PASSSEC, &new->flags);\n}\n\nstatic int unix_accept(struct socket *sock, struct socket *newsock, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sock *tsk;\n\tstruct sk_buff *skb;\n\tint err;\n\n\terr = -EOPNOTSUPP;\n\tif (sock->type != SOCK_STREAM && sock->type != SOCK_SEQPACKET)\n\t\tgoto out;\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_LISTEN)\n\t\tgoto out;\n\n\t/* If socket state is TCP_LISTEN it cannot change (for now...),\n\t * so that no locks are necessary.\n\t */\n\n\tskb = skb_recv_datagram(sk, 0, flags&O_NONBLOCK, &err);\n\tif (!skb) {\n\t\t/* This means receive shutdown. */\n\t\tif (err == 0)\n\t\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttsk = skb->sk;\n\tskb_free_datagram(sk, skb);\n\twake_up_interruptible(&unix_sk(sk)->peer_wait);\n\n\t/* attach accepted sock to socket */\n\tunix_state_lock(tsk);\n\tnewsock->state = SS_CONNECTED;\n\tunix_sock_inherit_flags(sock, newsock);\n\tsock_graft(tsk, newsock);\n\tunix_state_unlock(tsk);\n\treturn 0;\n\nout:\n\treturn err;\n}\n\n\nstatic int unix_getname(struct socket *sock, struct sockaddr *uaddr, int *uaddr_len, int peer)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u;\n\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr, uaddr);\n\tint err = 0;\n\n\tif (peer) {\n\t\tsk = unix_peer_get(sk);\n\n\t\terr = -ENOTCONN;\n\t\tif (!sk)\n\t\t\tgoto out;\n\t\terr = 0;\n\t} else {\n\t\tsock_hold(sk);\n\t}\n\n\tu = unix_sk(sk);\n\tunix_state_lock(sk);\n\tif (!u->addr) {\n\t\tsunaddr->sun_family = AF_UNIX;\n\t\tsunaddr->sun_path[0] = 0;\n\t\t*uaddr_len = sizeof(short);\n\t} else {\n\t\tstruct unix_address *addr = u->addr;\n\n\t\t*uaddr_len = addr->len;\n\t\tmemcpy(sunaddr, addr->name, *uaddr_len);\n\t}\n\tunix_state_unlock(sk);\n\tsock_put(sk);\nout:\n\treturn err;\n}\n\nstatic void unix_detach_fds(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\tint i;\n\n\tscm->fp = UNIXCB(skb).fp;\n\tUNIXCB(skb).fp = NULL;\n\n\tfor (i = scm->fp->count-1; i >= 0; i--)\n\t\tunix_notinflight(scm->fp->user, scm->fp->fp[i]);\n}\n\nstatic void unix_destruct_scm(struct sk_buff *skb)\n{\n\tstruct scm_cookie scm;\n\tmemset(&scm, 0, sizeof(scm));\n\tscm.pid  = UNIXCB(skb).pid;\n\tif (UNIXCB(skb).fp)\n\t\tunix_detach_fds(&scm, skb);\n\n\t/* Alas, it calls VFS */\n\t/* So fscking what? fput() had been SMP-safe since the last Summer */\n\tscm_destroy(&scm);\n\tsock_wfree(skb);\n}\n\n/*\n * The \"user->unix_inflight\" variable is protected by the garbage\n * collection lock, and we just read it locklessly here. If you go\n * over the limit, there might be a tiny race in actually noticing\n * it across threads. Tough.\n */\nstatic inline bool too_many_unix_fds(struct task_struct *p)\n{\n\tstruct user_struct *user = current_user();\n\n\tif (unlikely(user->unix_inflight > task_rlimit(p, RLIMIT_NOFILE)))\n\t\treturn !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN);\n\treturn false;\n}\n\n#define MAX_RECURSION_LEVEL 4\n\nstatic int unix_attach_fds(struct scm_cookie *scm, struct sk_buff *skb)\n{\n\tint i;\n\tunsigned char max_level = 0;\n\tint unix_sock_count = 0;\n\n\tif (too_many_unix_fds(current))\n\t\treturn -ETOOMANYREFS;\n\n\tfor (i = scm->fp->count - 1; i >= 0; i--) {\n\t\tstruct sock *sk = unix_get_socket(scm->fp->fp[i]);\n\n\t\tif (sk) {\n\t\t\tunix_sock_count++;\n\t\t\tmax_level = max(max_level,\n\t\t\t\t\tunix_sk(sk)->recursion_level);\n\t\t}\n\t}\n\tif (unlikely(max_level > MAX_RECURSION_LEVEL))\n\t\treturn -ETOOMANYREFS;\n\n\t/*\n\t * Need to duplicate file references for the sake of garbage\n\t * collection.  Otherwise a socket in the fps might become a\n\t * candidate for GC while the skb is not yet queued.\n\t */\n\tUNIXCB(skb).fp = scm_fp_dup(scm->fp);\n\tif (!UNIXCB(skb).fp)\n\t\treturn -ENOMEM;\n\n\tfor (i = scm->fp->count - 1; i >= 0; i--)\n\t\tunix_inflight(scm->fp->user, scm->fp->fp[i]);\n\treturn max_level;\n}\n\nstatic int unix_scm_to_skb(struct scm_cookie *scm, struct sk_buff *skb, bool send_fds)\n{\n\tint err = 0;\n\n\tUNIXCB(skb).pid  = get_pid(scm->pid);\n\tUNIXCB(skb).uid = scm->creds.uid;\n\tUNIXCB(skb).gid = scm->creds.gid;\n\tUNIXCB(skb).fp = NULL;\n\tunix_get_secdata(scm, skb);\n\tif (scm->fp && send_fds)\n\t\terr = unix_attach_fds(scm, skb);\n\n\tskb->destructor = unix_destruct_scm;\n\treturn err;\n}\n\nstatic bool unix_passcred_enabled(const struct socket *sock,\n\t\t\t\t  const struct sock *other)\n{\n\treturn test_bit(SOCK_PASSCRED, &sock->flags) ||\n\t       !other->sk_socket ||\n\t       test_bit(SOCK_PASSCRED, &other->sk_socket->flags);\n}\n\n/*\n * Some apps rely on write() giving SCM_CREDENTIALS\n * We include credentials if source or destination socket\n * asserted SOCK_PASSCRED.\n */\nstatic void maybe_add_creds(struct sk_buff *skb, const struct socket *sock,\n\t\t\t    const struct sock *other)\n{\n\tif (UNIXCB(skb).pid)\n\t\treturn;\n\tif (unix_passcred_enabled(sock, other)) {\n\t\tUNIXCB(skb).pid  = get_pid(task_tgid(current));\n\t\tcurrent_uid_gid(&UNIXCB(skb).uid, &UNIXCB(skb).gid);\n\t}\n}\n\nstatic int maybe_init_creds(struct scm_cookie *scm,\n\t\t\t    struct socket *socket,\n\t\t\t    const struct sock *other)\n{\n\tint err;\n\tstruct msghdr msg = { .msg_controllen = 0 };\n\n\terr = scm_send(socket, &msg, scm, false);\n\tif (err)\n\t\treturn err;\n\n\tif (unix_passcred_enabled(socket, other)) {\n\t\tscm->pid = get_pid(task_tgid(current));\n\t\tcurrent_uid_gid(&scm->creds.uid, &scm->creds.gid);\n\t}\n\treturn err;\n}\n\nstatic bool unix_skb_scm_eq(struct sk_buff *skb,\n\t\t\t    struct scm_cookie *scm)\n{\n\tconst struct unix_skb_parms *u = &UNIXCB(skb);\n\n\treturn u->pid == scm->pid &&\n\t       uid_eq(u->uid, scm->creds.uid) &&\n\t       gid_eq(u->gid, scm->creds.gid) &&\n\t       unix_secdata_eq(scm, skb);\n}\n\n/*\n *\tSend AF_UNIX data.\n */\n\nstatic int unix_dgram_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t      size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct net *net = sock_net(sk);\n\tstruct unix_sock *u = unix_sk(sk);\n\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr, msg->msg_name);\n\tstruct sock *other = NULL;\n\tint namelen = 0; /* fake GCC */\n\tint err;\n\tunsigned int hash;\n\tstruct sk_buff *skb;\n\tlong timeo;\n\tstruct scm_cookie scm;\n\tint max_level;\n\tint data_len = 0;\n\tint sk_locked;\n\n\twait_for_unix_gc();\n\terr = scm_send(sock, msg, &scm, false);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags&MSG_OOB)\n\t\tgoto out;\n\n\tif (msg->msg_namelen) {\n\t\terr = unix_mkname(sunaddr, msg->msg_namelen, &hash);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\tnamelen = err;\n\t} else {\n\t\tsunaddr = NULL;\n\t\terr = -ENOTCONN;\n\t\tother = unix_peer_get(sk);\n\t\tif (!other)\n\t\t\tgoto out;\n\t}\n\n\tif (test_bit(SOCK_PASSCRED, &sock->flags) && !u->addr\n\t    && (err = unix_autobind(sock)) != 0)\n\t\tgoto out;\n\n\terr = -EMSGSIZE;\n\tif (len > sk->sk_sndbuf - 32)\n\t\tgoto out;\n\n\tif (len > SKB_MAX_ALLOC) {\n\t\tdata_len = min_t(size_t,\n\t\t\t\t len - SKB_MAX_ALLOC,\n\t\t\t\t MAX_SKB_FRAGS * PAGE_SIZE);\n\t\tdata_len = PAGE_ALIGN(data_len);\n\n\t\tBUILD_BUG_ON(SKB_MAX_ALLOC < PAGE_SIZE);\n\t}\n\n\tskb = sock_alloc_send_pskb(sk, len - data_len, data_len,\n\t\t\t\t   msg->msg_flags & MSG_DONTWAIT, &err,\n\t\t\t\t   PAGE_ALLOC_COSTLY_ORDER);\n\tif (skb == NULL)\n\t\tgoto out;\n\n\terr = unix_scm_to_skb(&scm, skb, true);\n\tif (err < 0)\n\t\tgoto out_free;\n\tmax_level = err + 1;\n\n\tskb_put(skb, len - data_len);\n\tskb->data_len = data_len;\n\tskb->len = len;\n\terr = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, len);\n\tif (err)\n\t\tgoto out_free;\n\n\ttimeo = sock_sndtimeo(sk, msg->msg_flags & MSG_DONTWAIT);\n\nrestart:\n\tif (!other) {\n\t\terr = -ECONNRESET;\n\t\tif (sunaddr == NULL)\n\t\t\tgoto out_free;\n\n\t\tother = unix_find_other(net, sunaddr, namelen, sk->sk_type,\n\t\t\t\t\thash, &err);\n\t\tif (other == NULL)\n\t\t\tgoto out_free;\n\t}\n\n\tif (sk_filter(other, skb) < 0) {\n\t\t/* Toss the packet but do not return any error to the sender */\n\t\terr = len;\n\t\tgoto out_free;\n\t}\n\n\tsk_locked = 0;\n\tunix_state_lock(other);\nrestart_locked:\n\terr = -EPERM;\n\tif (!unix_may_send(sk, other))\n\t\tgoto out_unlock;\n\n\tif (unlikely(sock_flag(other, SOCK_DEAD))) {\n\t\t/*\n\t\t *\tCheck with 1003.1g - what should\n\t\t *\tdatagram error\n\t\t */\n\t\tunix_state_unlock(other);\n\t\tsock_put(other);\n\n\t\tif (!sk_locked)\n\t\t\tunix_state_lock(sk);\n\n\t\terr = 0;\n\t\tif (unix_peer(sk) == other) {\n\t\t\tunix_peer(sk) = NULL;\n\t\t\tunix_dgram_peer_wake_disconnect_wakeup(sk, other);\n\n\t\t\tunix_state_unlock(sk);\n\n\t\t\tunix_dgram_disconnected(sk, other);\n\t\t\tsock_put(other);\n\t\t\terr = -ECONNREFUSED;\n\t\t} else {\n\t\t\tunix_state_unlock(sk);\n\t\t}\n\n\t\tother = NULL;\n\t\tif (err)\n\t\t\tgoto out_free;\n\t\tgoto restart;\n\t}\n\n\terr = -EPIPE;\n\tif (other->sk_shutdown & RCV_SHUTDOWN)\n\t\tgoto out_unlock;\n\n\tif (sk->sk_type != SOCK_SEQPACKET) {\n\t\terr = security_unix_may_send(sk->sk_socket, other->sk_socket);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t}\n\n\tif (unlikely(unix_peer(other) != sk && unix_recvq_full(other))) {\n\t\tif (timeo) {\n\t\t\ttimeo = unix_wait_for_peer(other, timeo);\n\n\t\t\terr = sock_intr_errno(timeo);\n\t\t\tif (signal_pending(current))\n\t\t\t\tgoto out_free;\n\n\t\t\tgoto restart;\n\t\t}\n\n\t\tif (!sk_locked) {\n\t\t\tunix_state_unlock(other);\n\t\t\tunix_state_double_lock(sk, other);\n\t\t}\n\n\t\tif (unix_peer(sk) != other ||\n\t\t    unix_dgram_peer_wake_me(sk, other)) {\n\t\t\terr = -EAGAIN;\n\t\t\tsk_locked = 1;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (!sk_locked) {\n\t\t\tsk_locked = 1;\n\t\t\tgoto restart_locked;\n\t\t}\n\t}\n\n\tif (unlikely(sk_locked))\n\t\tunix_state_unlock(sk);\n\n\tif (sock_flag(other, SOCK_RCVTSTAMP))\n\t\t__net_timestamp(skb);\n\tmaybe_add_creds(skb, sock, other);\n\tskb_queue_tail(&other->sk_receive_queue, skb);\n\tif (max_level > unix_sk(other)->recursion_level)\n\t\tunix_sk(other)->recursion_level = max_level;\n\tunix_state_unlock(other);\n\tother->sk_data_ready(other);\n\tsock_put(other);\n\tscm_destroy(&scm);\n\treturn len;\n\nout_unlock:\n\tif (sk_locked)\n\t\tunix_state_unlock(sk);\n\tunix_state_unlock(other);\nout_free:\n\tkfree_skb(skb);\nout:\n\tif (other)\n\t\tsock_put(other);\n\tscm_destroy(&scm);\n\treturn err;\n}\n\n/* We use paged skbs for stream sockets, and limit occupancy to 32768\n * bytes, and a minimun of a full page.\n */\n#define UNIX_SKB_FRAGS_SZ (PAGE_SIZE << get_order(32768))\n\nstatic int unix_stream_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t       size_t len)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sock *other = NULL;\n\tint err, size;\n\tstruct sk_buff *skb;\n\tint sent = 0;\n\tstruct scm_cookie scm;\n\tbool fds_sent = false;\n\tint max_level;\n\tint data_len;\n\n\twait_for_unix_gc();\n\terr = scm_send(sock, msg, &scm, false);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = -EOPNOTSUPP;\n\tif (msg->msg_flags&MSG_OOB)\n\t\tgoto out_err;\n\n\tif (msg->msg_namelen) {\n\t\terr = sk->sk_state == TCP_ESTABLISHED ? -EISCONN : -EOPNOTSUPP;\n\t\tgoto out_err;\n\t} else {\n\t\terr = -ENOTCONN;\n\t\tother = unix_peer(sk);\n\t\tif (!other)\n\t\t\tgoto out_err;\n\t}\n\n\tif (sk->sk_shutdown & SEND_SHUTDOWN)\n\t\tgoto pipe_err;\n\n\twhile (sent < len) {\n\t\tsize = len - sent;\n\n\t\t/* Keep two messages in the pipe so it schedules better */\n\t\tsize = min_t(int, size, (sk->sk_sndbuf >> 1) - 64);\n\n\t\t/* allow fallback to order-0 allocations */\n\t\tsize = min_t(int, size, SKB_MAX_HEAD(0) + UNIX_SKB_FRAGS_SZ);\n\n\t\tdata_len = max_t(int, 0, size - SKB_MAX_HEAD(0));\n\n\t\tdata_len = min_t(size_t, size, PAGE_ALIGN(data_len));\n\n\t\tskb = sock_alloc_send_pskb(sk, size - data_len, data_len,\n\t\t\t\t\t   msg->msg_flags & MSG_DONTWAIT, &err,\n\t\t\t\t\t   get_order(UNIX_SKB_FRAGS_SZ));\n\t\tif (!skb)\n\t\t\tgoto out_err;\n\n\t\t/* Only send the fds in the first buffer */\n\t\terr = unix_scm_to_skb(&scm, skb, !fds_sent);\n\t\tif (err < 0) {\n\t\t\tkfree_skb(skb);\n\t\t\tgoto out_err;\n\t\t}\n\t\tmax_level = err + 1;\n\t\tfds_sent = true;\n\n\t\tskb_put(skb, size - data_len);\n\t\tskb->data_len = data_len;\n\t\tskb->len = size;\n\t\terr = skb_copy_datagram_from_iter(skb, 0, &msg->msg_iter, size);\n\t\tif (err) {\n\t\t\tkfree_skb(skb);\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tunix_state_lock(other);\n\n\t\tif (sock_flag(other, SOCK_DEAD) ||\n\t\t    (other->sk_shutdown & RCV_SHUTDOWN))\n\t\t\tgoto pipe_err_free;\n\n\t\tmaybe_add_creds(skb, sock, other);\n\t\tskb_queue_tail(&other->sk_receive_queue, skb);\n\t\tif (max_level > unix_sk(other)->recursion_level)\n\t\t\tunix_sk(other)->recursion_level = max_level;\n\t\tunix_state_unlock(other);\n\t\tother->sk_data_ready(other);\n\t\tsent += size;\n\t}\n\n\tscm_destroy(&scm);\n\n\treturn sent;\n\npipe_err_free:\n\tunix_state_unlock(other);\n\tkfree_skb(skb);\npipe_err:\n\tif (sent == 0 && !(msg->msg_flags&MSG_NOSIGNAL))\n\t\tsend_sig(SIGPIPE, current, 0);\n\terr = -EPIPE;\nout_err:\n\tscm_destroy(&scm);\n\treturn sent ? : err;\n}\n\nstatic ssize_t unix_stream_sendpage(struct socket *socket, struct page *page,\n\t\t\t\t    int offset, size_t size, int flags)\n{\n\tint err;\n\tbool send_sigpipe = false;\n\tbool init_scm = true;\n\tstruct scm_cookie scm;\n\tstruct sock *other, *sk = socket->sk;\n\tstruct sk_buff *skb, *newskb = NULL, *tail = NULL;\n\n\tif (flags & MSG_OOB)\n\t\treturn -EOPNOTSUPP;\n\n\tother = unix_peer(sk);\n\tif (!other || sk->sk_state != TCP_ESTABLISHED)\n\t\treturn -ENOTCONN;\n\n\tif (false) {\nalloc_skb:\n\t\tunix_state_unlock(other);\n\t\tmutex_unlock(&unix_sk(other)->readlock);\n\t\tnewskb = sock_alloc_send_pskb(sk, 0, 0, flags & MSG_DONTWAIT,\n\t\t\t\t\t      &err, 0);\n\t\tif (!newskb)\n\t\t\tgoto err;\n\t}\n\n\t/* we must acquire readlock as we modify already present\n\t * skbs in the sk_receive_queue and mess with skb->len\n\t */\n\terr = mutex_lock_interruptible(&unix_sk(other)->readlock);\n\tif (err) {\n\t\terr = flags & MSG_DONTWAIT ? -EAGAIN : -ERESTARTSYS;\n\t\tgoto err;\n\t}\n\n\tif (sk->sk_shutdown & SEND_SHUTDOWN) {\n\t\terr = -EPIPE;\n\t\tsend_sigpipe = true;\n\t\tgoto err_unlock;\n\t}\n\n\tunix_state_lock(other);\n\n\tif (sock_flag(other, SOCK_DEAD) ||\n\t    other->sk_shutdown & RCV_SHUTDOWN) {\n\t\terr = -EPIPE;\n\t\tsend_sigpipe = true;\n\t\tgoto err_state_unlock;\n\t}\n\n\tif (init_scm) {\n\t\terr = maybe_init_creds(&scm, socket, other);\n\t\tif (err)\n\t\t\tgoto err_state_unlock;\n\t\tinit_scm = false;\n\t}\n\n\tskb = skb_peek_tail(&other->sk_receive_queue);\n\tif (tail && tail == skb) {\n\t\tskb = newskb;\n\t} else if (!skb || !unix_skb_scm_eq(skb, &scm)) {\n\t\tif (newskb) {\n\t\t\tskb = newskb;\n\t\t} else {\n\t\t\ttail = skb;\n\t\t\tgoto alloc_skb;\n\t\t}\n\t} else if (newskb) {\n\t\t/* this is fast path, we don't necessarily need to\n\t\t * call to kfree_skb even though with newskb == NULL\n\t\t * this - does no harm\n\t\t */\n\t\tconsume_skb(newskb);\n\t\tnewskb = NULL;\n\t}\n\n\tif (skb_append_pagefrags(skb, page, offset, size)) {\n\t\ttail = skb;\n\t\tgoto alloc_skb;\n\t}\n\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += size;\n\tatomic_add(size, &sk->sk_wmem_alloc);\n\n\tif (newskb) {\n\t\terr = unix_scm_to_skb(&scm, skb, false);\n\t\tif (err)\n\t\t\tgoto err_state_unlock;\n\t\tspin_lock(&other->sk_receive_queue.lock);\n\t\t__skb_queue_tail(&other->sk_receive_queue, newskb);\n\t\tspin_unlock(&other->sk_receive_queue.lock);\n\t}\n\n\tunix_state_unlock(other);\n\tmutex_unlock(&unix_sk(other)->readlock);\n\n\tother->sk_data_ready(other);\n\tscm_destroy(&scm);\n\treturn size;\n\nerr_state_unlock:\n\tunix_state_unlock(other);\nerr_unlock:\n\tmutex_unlock(&unix_sk(other)->readlock);\nerr:\n\tkfree_skb(newskb);\n\tif (send_sigpipe && !(flags & MSG_NOSIGNAL))\n\t\tsend_sig(SIGPIPE, current, 0);\n\tif (!init_scm)\n\t\tscm_destroy(&scm);\n\treturn err;\n}\n\nstatic int unix_seqpacket_sendmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t\t  size_t len)\n{\n\tint err;\n\tstruct sock *sk = sock->sk;\n\n\terr = sock_error(sk);\n\tif (err)\n\t\treturn err;\n\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\treturn -ENOTCONN;\n\n\tif (msg->msg_namelen)\n\t\tmsg->msg_namelen = 0;\n\n\treturn unix_dgram_sendmsg(sock, msg, len);\n}\n\nstatic int unix_seqpacket_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t\t  size_t size, int flags)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\treturn -ENOTCONN;\n\n\treturn unix_dgram_recvmsg(sock, msg, size, flags);\n}\n\nstatic void unix_copy_addr(struct msghdr *msg, struct sock *sk)\n{\n\tstruct unix_sock *u = unix_sk(sk);\n\n\tif (u->addr) {\n\t\tmsg->msg_namelen = u->addr->len;\n\t\tmemcpy(msg->msg_name, u->addr->name, u->addr->len);\n\t}\n}\n\nstatic int unix_dgram_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t      size_t size, int flags)\n{\n\tstruct scm_cookie scm;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tstruct sk_buff *skb, *last;\n\tlong timeo;\n\tint err;\n\tint peeked, skip;\n\n\terr = -EOPNOTSUPP;\n\tif (flags&MSG_OOB)\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);\n\n\tdo {\n\t\tmutex_lock(&u->readlock);\n\n\t\tskip = sk_peek_offset(sk, flags);\n\t\tskb = __skb_try_recv_datagram(sk, flags, &peeked, &skip, &err,\n\t\t\t\t\t      &last);\n\t\tif (skb)\n\t\t\tbreak;\n\n\t\tmutex_unlock(&u->readlock);\n\n\t\tif (err != -EAGAIN)\n\t\t\tbreak;\n\t} while (timeo &&\n\t\t !__skb_wait_for_more_packets(sk, &err, &timeo, last));\n\n\tif (!skb) { /* implies readlock unlocked */\n\t\tunix_state_lock(sk);\n\t\t/* Signal EOF on disconnected non-blocking SEQPACKET socket. */\n\t\tif (sk->sk_type == SOCK_SEQPACKET && err == -EAGAIN &&\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\t\terr = 0;\n\t\tunix_state_unlock(sk);\n\t\tgoto out;\n\t}\n\n\tif (wq_has_sleeper(&u->peer_wait))\n\t\twake_up_interruptible_sync_poll(&u->peer_wait,\n\t\t\t\t\t\tPOLLOUT | POLLWRNORM |\n\t\t\t\t\t\tPOLLWRBAND);\n\n\tif (msg->msg_name)\n\t\tunix_copy_addr(msg, skb->sk);\n\n\tif (size > skb->len - skip)\n\t\tsize = skb->len - skip;\n\telse if (size < skb->len - skip)\n\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\terr = skb_copy_datagram_msg(skb, skip, msg, size);\n\tif (err)\n\t\tgoto out_free;\n\n\tif (sock_flag(sk, SOCK_RCVTSTAMP))\n\t\t__sock_recv_timestamp(msg, sk, skb);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\tunix_set_secdata(&scm, skb);\n\n\tif (!(flags & MSG_PEEK)) {\n\t\tif (UNIXCB(skb).fp)\n\t\t\tunix_detach_fds(&scm, skb);\n\n\t\tsk_peek_offset_bwd(sk, skb->len);\n\t} else {\n\t\t/* It is questionable: on PEEK we could:\n\t\t   - do not return fds - good, but too simple 8)\n\t\t   - return fds, and do not return them on read (old strategy,\n\t\t     apparently wrong)\n\t\t   - clone fds (I chose it for now, it is the most universal\n\t\t     solution)\n\n\t\t   POSIX 1003.1g does not actually define this clearly\n\t\t   at all. POSIX 1003.1g doesn't define a lot of things\n\t\t   clearly however!\n\n\t\t*/\n\n\t\tsk_peek_offset_fwd(sk, size);\n\n\t\tif (UNIXCB(skb).fp)\n\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n\t}\n\terr = (flags & MSG_TRUNC) ? skb->len - skip : size;\n\n\tscm_recv(sock, msg, &scm, flags);\n\nout_free:\n\tskb_free_datagram(sk, skb);\n\tmutex_unlock(&u->readlock);\nout:\n\treturn err;\n}\n\n/*\n *\tSleep until more data has arrived. But check for races..\n */\nstatic long unix_stream_data_wait(struct sock *sk, long timeo,\n\t\t\t\t  struct sk_buff *last, unsigned int last_len)\n{\n\tstruct sk_buff *tail;\n\tDEFINE_WAIT(wait);\n\n\tunix_state_lock(sk);\n\n\tfor (;;) {\n\t\tprepare_to_wait(sk_sleep(sk), &wait, TASK_INTERRUPTIBLE);\n\n\t\ttail = skb_peek_tail(&sk->sk_receive_queue);\n\t\tif (tail != last ||\n\t\t    (tail && tail->len != last_len) ||\n\t\t    sk->sk_err ||\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t    signal_pending(current) ||\n\t\t    !timeo)\n\t\t\tbreak;\n\n\t\tsk_set_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\t\tunix_state_unlock(sk);\n\t\ttimeo = freezable_schedule_timeout(timeo);\n\t\tunix_state_lock(sk);\n\n\t\tif (sock_flag(sk, SOCK_DEAD))\n\t\t\tbreak;\n\n\t\tsk_clear_bit(SOCKWQ_ASYNC_WAITDATA, sk);\n\t}\n\n\tfinish_wait(sk_sleep(sk), &wait);\n\tunix_state_unlock(sk);\n\treturn timeo;\n}\n\nstatic unsigned int unix_skb_len(const struct sk_buff *skb)\n{\n\treturn skb->len - UNIXCB(skb).consumed;\n}\n\nstruct unix_stream_read_state {\n\tint (*recv_actor)(struct sk_buff *, int, int,\n\t\t\t  struct unix_stream_read_state *);\n\tstruct socket *socket;\n\tstruct msghdr *msg;\n\tstruct pipe_inode_info *pipe;\n\tsize_t size;\n\tint flags;\n\tunsigned int splice_flags;\n};\n\nstatic int unix_stream_read_generic(struct unix_stream_read_state *state)\n{\n\tstruct scm_cookie scm;\n\tstruct socket *sock = state->socket;\n\tstruct sock *sk = sock->sk;\n\tstruct unix_sock *u = unix_sk(sk);\n\tint copied = 0;\n\tint flags = state->flags;\n\tint noblock = flags & MSG_DONTWAIT;\n\tbool check_creds = false;\n\tint target;\n\tint err = 0;\n\tlong timeo;\n\tint skip;\n\tsize_t size = state->size;\n\tunsigned int last_len;\n\n\terr = -EINVAL;\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\tgoto out;\n\n\terr = -EOPNOTSUPP;\n\tif (flags & MSG_OOB)\n\t\tgoto out;\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, size);\n\ttimeo = sock_rcvtimeo(sk, noblock);\n\n\tmemset(&scm, 0, sizeof(scm));\n\n\t/* Lock the socket to prevent queue disordering\n\t * while sleeps in memcpy_tomsg\n\t */\n\tmutex_lock(&u->readlock);\n\n\tif (flags & MSG_PEEK)\n\t\tskip = sk_peek_offset(sk, flags);\n\telse\n\t\tskip = 0;\n\n\tdo {\n\t\tint chunk;\n\t\tbool drop_skb;\n\t\tstruct sk_buff *skb, *last;\n\n\t\tunix_state_lock(sk);\n\t\tif (sock_flag(sk, SOCK_DEAD)) {\n\t\t\terr = -ECONNRESET;\n\t\t\tgoto unlock;\n\t\t}\n\t\tlast = skb = skb_peek(&sk->sk_receive_queue);\n\t\tlast_len = last ? last->len : 0;\nagain:\n\t\tif (skb == NULL) {\n\t\t\tunix_sk(sk)->recursion_level = 0;\n\t\t\tif (copied >= target)\n\t\t\t\tgoto unlock;\n\n\t\t\t/*\n\t\t\t *\tPOSIX 1003.1g mandates this order.\n\t\t\t */\n\n\t\t\terr = sock_error(sk);\n\t\t\tif (err)\n\t\t\t\tgoto unlock;\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tgoto unlock;\n\n\t\t\tunix_state_unlock(sk);\n\t\t\terr = -EAGAIN;\n\t\t\tif (!timeo)\n\t\t\t\tbreak;\n\t\t\tmutex_unlock(&u->readlock);\n\n\t\t\ttimeo = unix_stream_data_wait(sk, timeo, last,\n\t\t\t\t\t\t      last_len);\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\terr = sock_intr_errno(timeo);\n\t\t\t\tscm_destroy(&scm);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmutex_lock(&u->readlock);\n\t\t\tcontinue;\nunlock:\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\n\t\twhile (skip >= unix_skb_len(skb)) {\n\t\t\tskip -= unix_skb_len(skb);\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (!skb)\n\t\t\t\tgoto again;\n\t\t}\n\n\t\tunix_state_unlock(sk);\n\n\t\tif (check_creds) {\n\t\t\t/* Never glue messages from different writers */\n\t\t\tif (!unix_skb_scm_eq(skb, &scm))\n\t\t\t\tbreak;\n\t\t} else if (test_bit(SOCK_PASSCRED, &sock->flags)) {\n\t\t\t/* Copy credentials */\n\t\t\tscm_set_cred(&scm, UNIXCB(skb).pid, UNIXCB(skb).uid, UNIXCB(skb).gid);\n\t\t\tunix_set_secdata(&scm, skb);\n\t\t\tcheck_creds = true;\n\t\t}\n\n\t\t/* Copy address just once */\n\t\tif (state->msg && state->msg->msg_name) {\n\t\t\tDECLARE_SOCKADDR(struct sockaddr_un *, sunaddr,\n\t\t\t\t\t state->msg->msg_name);\n\t\t\tunix_copy_addr(state->msg, skb->sk);\n\t\t\tsunaddr = NULL;\n\t\t}\n\n\t\tchunk = min_t(unsigned int, unix_skb_len(skb) - skip, size);\n\t\tskb_get(skb);\n\t\tchunk = state->recv_actor(skb, skip, chunk, state);\n\t\tdrop_skb = !unix_skb_len(skb);\n\t\t/* skb is only safe to use if !drop_skb */\n\t\tconsume_skb(skb);\n\t\tif (chunk < 0) {\n\t\t\tif (copied == 0)\n\t\t\t\tcopied = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tcopied += chunk;\n\t\tsize -= chunk;\n\n\t\tif (drop_skb) {\n\t\t\t/* the skb was touched by a concurrent reader;\n\t\t\t * we should not expect anything from this skb\n\t\t\t * anymore and assume it invalid - we can be\n\t\t\t * sure it was dropped from the socket queue\n\t\t\t *\n\t\t\t * let's report a short read\n\t\t\t */\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Mark read part of skb as used */\n\t\tif (!(flags & MSG_PEEK)) {\n\t\t\tUNIXCB(skb).consumed += chunk;\n\n\t\t\tsk_peek_offset_bwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tunix_detach_fds(&scm, skb);\n\n\t\t\tif (unix_skb_len(skb))\n\t\t\t\tbreak;\n\n\t\t\tskb_unlink(skb, &sk->sk_receive_queue);\n\t\t\tconsume_skb(skb);\n\n\t\t\tif (scm.fp)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t/* It is questionable, see note in unix_dgram_recvmsg.\n\t\t\t */\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tscm.fp = scm_fp_dup(UNIXCB(skb).fp);\n\n\t\t\tsk_peek_offset_fwd(sk, chunk);\n\n\t\t\tif (UNIXCB(skb).fp)\n\t\t\t\tbreak;\n\n\t\t\tskip = 0;\n\t\t\tlast = skb;\n\t\t\tlast_len = skb->len;\n\t\t\tunix_state_lock(sk);\n\t\t\tskb = skb_peek_next(skb, &sk->sk_receive_queue);\n\t\t\tif (skb)\n\t\t\t\tgoto again;\n\t\t\tunix_state_unlock(sk);\n\t\t\tbreak;\n\t\t}\n\t} while (size);\n\n\tmutex_unlock(&u->readlock);\n\tif (state->msg)\n\t\tscm_recv(sock, state->msg, &scm, flags);\n\telse\n\t\tscm_destroy(&scm);\nout:\n\treturn copied ? : err;\n}\n\nstatic int unix_stream_read_actor(struct sk_buff *skb,\n\t\t\t\t  int skip, int chunk,\n\t\t\t\t  struct unix_stream_read_state *state)\n{\n\tint ret;\n\n\tret = skb_copy_datagram_msg(skb, UNIXCB(skb).consumed + skip,\n\t\t\t\t    state->msg, chunk);\n\treturn ret ?: chunk;\n}\n\nstatic int unix_stream_recvmsg(struct socket *sock, struct msghdr *msg,\n\t\t\t       size_t size, int flags)\n{\n\tstruct unix_stream_read_state state = {\n\t\t.recv_actor = unix_stream_read_actor,\n\t\t.socket = sock,\n\t\t.msg = msg,\n\t\t.size = size,\n\t\t.flags = flags\n\t};\n\n\treturn unix_stream_read_generic(&state);\n}\n\nstatic ssize_t skb_unix_socket_splice(struct sock *sk,\n\t\t\t\t      struct pipe_inode_info *pipe,\n\t\t\t\t      struct splice_pipe_desc *spd)\n{\n\tint ret;\n\tstruct unix_sock *u = unix_sk(sk);\n\n\tmutex_unlock(&u->readlock);\n\tret = splice_to_pipe(pipe, spd);\n\tmutex_lock(&u->readlock);\n\n\treturn ret;\n}\n\nstatic int unix_stream_splice_actor(struct sk_buff *skb,\n\t\t\t\t    int skip, int chunk,\n\t\t\t\t    struct unix_stream_read_state *state)\n{\n\treturn skb_splice_bits(skb, state->socket->sk,\n\t\t\t       UNIXCB(skb).consumed + skip,\n\t\t\t       state->pipe, chunk, state->splice_flags,\n\t\t\t       skb_unix_socket_splice);\n}\n\nstatic ssize_t unix_stream_splice_read(struct socket *sock,  loff_t *ppos,\n\t\t\t\t       struct pipe_inode_info *pipe,\n\t\t\t\t       size_t size, unsigned int flags)\n{\n\tstruct unix_stream_read_state state = {\n\t\t.recv_actor = unix_stream_splice_actor,\n\t\t.socket = sock,\n\t\t.pipe = pipe,\n\t\t.size = size,\n\t\t.splice_flags = flags,\n\t};\n\n\tif (unlikely(*ppos))\n\t\treturn -ESPIPE;\n\n\tif (sock->file->f_flags & O_NONBLOCK ||\n\t    flags & SPLICE_F_NONBLOCK)\n\t\tstate.flags = MSG_DONTWAIT;\n\n\treturn unix_stream_read_generic(&state);\n}\n\nstatic int unix_shutdown(struct socket *sock, int mode)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct sock *other;\n\n\tif (mode < SHUT_RD || mode > SHUT_RDWR)\n\t\treturn -EINVAL;\n\t/* This maps:\n\t * SHUT_RD   (0) -> RCV_SHUTDOWN  (1)\n\t * SHUT_WR   (1) -> SEND_SHUTDOWN (2)\n\t * SHUT_RDWR (2) -> SHUTDOWN_MASK (3)\n\t */\n\t++mode;\n\n\tunix_state_lock(sk);\n\tsk->sk_shutdown |= mode;\n\tother = unix_peer(sk);\n\tif (other)\n\t\tsock_hold(other);\n\tunix_state_unlock(sk);\n\tsk->sk_state_change(sk);\n\n\tif (other &&\n\t\t(sk->sk_type == SOCK_STREAM || sk->sk_type == SOCK_SEQPACKET)) {\n\n\t\tint peer_mode = 0;\n\n\t\tif (mode&RCV_SHUTDOWN)\n\t\t\tpeer_mode |= SEND_SHUTDOWN;\n\t\tif (mode&SEND_SHUTDOWN)\n\t\t\tpeer_mode |= RCV_SHUTDOWN;\n\t\tunix_state_lock(other);\n\t\tother->sk_shutdown |= peer_mode;\n\t\tunix_state_unlock(other);\n\t\tother->sk_state_change(other);\n\t\tif (peer_mode == SHUTDOWN_MASK)\n\t\t\tsk_wake_async(other, SOCK_WAKE_WAITD, POLL_HUP);\n\t\telse if (peer_mode & RCV_SHUTDOWN)\n\t\t\tsk_wake_async(other, SOCK_WAKE_WAITD, POLL_IN);\n\t}\n\tif (other)\n\t\tsock_put(other);\n\n\treturn 0;\n}\n\nlong unix_inq_len(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tlong amount = 0;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn -EINVAL;\n\n\tspin_lock(&sk->sk_receive_queue.lock);\n\tif (sk->sk_type == SOCK_STREAM ||\n\t    sk->sk_type == SOCK_SEQPACKET) {\n\t\tskb_queue_walk(&sk->sk_receive_queue, skb)\n\t\t\tamount += unix_skb_len(skb);\n\t} else {\n\t\tskb = skb_peek(&sk->sk_receive_queue);\n\t\tif (skb)\n\t\t\tamount = skb->len;\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\treturn amount;\n}\nEXPORT_SYMBOL_GPL(unix_inq_len);\n\nlong unix_outq_len(struct sock *sk)\n{\n\treturn sk_wmem_alloc_get(sk);\n}\nEXPORT_SYMBOL_GPL(unix_outq_len);\n\nstatic int unix_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)\n{\n\tstruct sock *sk = sock->sk;\n\tlong amount = 0;\n\tint err;\n\n\tswitch (cmd) {\n\tcase SIOCOUTQ:\n\t\tamount = unix_outq_len(sk);\n\t\terr = put_user(amount, (int __user *)arg);\n\t\tbreak;\n\tcase SIOCINQ:\n\t\tamount = unix_inq_len(sk);\n\t\tif (amount < 0)\n\t\t\terr = amount;\n\t\telse\n\t\t\terr = put_user(amount, (int __user *)arg);\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOIOCTLCMD;\n\t\tbreak;\n\t}\n\treturn err;\n}\n\nstatic unsigned int unix_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\tstruct sock *sk = sock->sk;\n\tunsigned int mask;\n\n\tsock_poll_wait(file, sk_sleep(sk), wait);\n\tmask = 0;\n\n\t/* exceptional events? */\n\tif (sk->sk_err)\n\t\tmask |= POLLERR;\n\tif (sk->sk_shutdown == SHUTDOWN_MASK)\n\t\tmask |= POLLHUP;\n\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\tmask |= POLLRDHUP | POLLIN | POLLRDNORM;\n\n\t/* readable? */\n\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\tmask |= POLLIN | POLLRDNORM;\n\n\t/* Connection-based need to check for termination and startup */\n\tif ((sk->sk_type == SOCK_STREAM || sk->sk_type == SOCK_SEQPACKET) &&\n\t    sk->sk_state == TCP_CLOSE)\n\t\tmask |= POLLHUP;\n\n\t/*\n\t * we set writable also when the other side has shut down the\n\t * connection. This prevents stuck sockets.\n\t */\n\tif (unix_writable(sk))\n\t\tmask |= POLLOUT | POLLWRNORM | POLLWRBAND;\n\n\treturn mask;\n}\n\nstatic unsigned int unix_dgram_poll(struct file *file, struct socket *sock,\n\t\t\t\t    poll_table *wait)\n{\n\tstruct sock *sk = sock->sk, *other;\n\tunsigned int mask, writable;\n\n\tsock_poll_wait(file, sk_sleep(sk), wait);\n\tmask = 0;\n\n\t/* exceptional events? */\n\tif (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue))\n\t\tmask |= POLLERR |\n\t\t\t(sock_flag(sk, SOCK_SELECT_ERR_QUEUE) ? POLLPRI : 0);\n\n\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\tmask |= POLLRDHUP | POLLIN | POLLRDNORM;\n\tif (sk->sk_shutdown == SHUTDOWN_MASK)\n\t\tmask |= POLLHUP;\n\n\t/* readable? */\n\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\tmask |= POLLIN | POLLRDNORM;\n\n\t/* Connection-based need to check for termination and startup */\n\tif (sk->sk_type == SOCK_SEQPACKET) {\n\t\tif (sk->sk_state == TCP_CLOSE)\n\t\t\tmask |= POLLHUP;\n\t\t/* connection hasn't started yet? */\n\t\tif (sk->sk_state == TCP_SYN_SENT)\n\t\t\treturn mask;\n\t}\n\n\t/* No write status requested, avoid expensive OUT tests. */\n\tif (!(poll_requested_events(wait) & (POLLWRBAND|POLLWRNORM|POLLOUT)))\n\t\treturn mask;\n\n\twritable = unix_writable(sk);\n\tif (writable) {\n\t\tunix_state_lock(sk);\n\n\t\tother = unix_peer(sk);\n\t\tif (other && unix_peer(other) != sk &&\n\t\t    unix_recvq_full(other) &&\n\t\t    unix_dgram_peer_wake_me(sk, other))\n\t\t\twritable = 0;\n\n\t\tunix_state_unlock(sk);\n\t}\n\n\tif (writable)\n\t\tmask |= POLLOUT | POLLWRNORM | POLLWRBAND;\n\telse\n\t\tsk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\n\treturn mask;\n}\n\n#ifdef CONFIG_PROC_FS\n\n#define BUCKET_SPACE (BITS_PER_LONG - (UNIX_HASH_BITS + 1) - 1)\n\n#define get_bucket(x) ((x) >> BUCKET_SPACE)\n#define get_offset(x) ((x) & ((1L << BUCKET_SPACE) - 1))\n#define set_bucket_offset(b, o) ((b) << BUCKET_SPACE | (o))\n\nstatic struct sock *unix_from_bucket(struct seq_file *seq, loff_t *pos)\n{\n\tunsigned long offset = get_offset(*pos);\n\tunsigned long bucket = get_bucket(*pos);\n\tstruct sock *sk;\n\tunsigned long count = 0;\n\n\tfor (sk = sk_head(&unix_socket_table[bucket]); sk; sk = sk_next(sk)) {\n\t\tif (sock_net(sk) != seq_file_net(seq))\n\t\t\tcontinue;\n\t\tif (++count == offset)\n\t\t\tbreak;\n\t}\n\n\treturn sk;\n}\n\nstatic struct sock *unix_next_socket(struct seq_file *seq,\n\t\t\t\t     struct sock *sk,\n\t\t\t\t     loff_t *pos)\n{\n\tunsigned long bucket;\n\n\twhile (sk > (struct sock *)SEQ_START_TOKEN) {\n\t\tsk = sk_next(sk);\n\t\tif (!sk)\n\t\t\tgoto next_bucket;\n\t\tif (sock_net(sk) == seq_file_net(seq))\n\t\t\treturn sk;\n\t}\n\n\tdo {\n\t\tsk = unix_from_bucket(seq, pos);\n\t\tif (sk)\n\t\t\treturn sk;\n\nnext_bucket:\n\t\tbucket = get_bucket(*pos) + 1;\n\t\t*pos = set_bucket_offset(bucket, 1);\n\t} while (bucket < ARRAY_SIZE(unix_socket_table));\n\n\treturn NULL;\n}\n\nstatic void *unix_seq_start(struct seq_file *seq, loff_t *pos)\n\t__acquires(unix_table_lock)\n{\n\tspin_lock(&unix_table_lock);\n\n\tif (!*pos)\n\t\treturn SEQ_START_TOKEN;\n\n\tif (get_bucket(*pos) >= ARRAY_SIZE(unix_socket_table))\n\t\treturn NULL;\n\n\treturn unix_next_socket(seq, NULL, pos);\n}\n\nstatic void *unix_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\t++*pos;\n\treturn unix_next_socket(seq, v, pos);\n}\n\nstatic void unix_seq_stop(struct seq_file *seq, void *v)\n\t__releases(unix_table_lock)\n{\n\tspin_unlock(&unix_table_lock);\n}\n\nstatic int unix_seq_show(struct seq_file *seq, void *v)\n{\n\n\tif (v == SEQ_START_TOKEN)\n\t\tseq_puts(seq, \"Num       RefCount Protocol Flags    Type St \"\n\t\t\t \"Inode Path\\n\");\n\telse {\n\t\tstruct sock *s = v;\n\t\tstruct unix_sock *u = unix_sk(s);\n\t\tunix_state_lock(s);\n\n\t\tseq_printf(seq, \"%pK: %08X %08X %08X %04X %02X %5lu\",\n\t\t\ts,\n\t\t\tatomic_read(&s->sk_refcnt),\n\t\t\t0,\n\t\t\ts->sk_state == TCP_LISTEN ? __SO_ACCEPTCON : 0,\n\t\t\ts->sk_type,\n\t\t\ts->sk_socket ?\n\t\t\t(s->sk_state == TCP_ESTABLISHED ? SS_CONNECTED : SS_UNCONNECTED) :\n\t\t\t(s->sk_state == TCP_ESTABLISHED ? SS_CONNECTING : SS_DISCONNECTING),\n\t\t\tsock_i_ino(s));\n\n\t\tif (u->addr) {\n\t\t\tint i, len;\n\t\t\tseq_putc(seq, ' ');\n\n\t\t\ti = 0;\n\t\t\tlen = u->addr->len - sizeof(short);\n\t\t\tif (!UNIX_ABSTRACT(s))\n\t\t\t\tlen--;\n\t\t\telse {\n\t\t\t\tseq_putc(seq, '@');\n\t\t\t\ti++;\n\t\t\t}\n\t\t\tfor ( ; i < len; i++)\n\t\t\t\tseq_putc(seq, u->addr->name->sun_path[i]);\n\t\t}\n\t\tunix_state_unlock(s);\n\t\tseq_putc(seq, '\\n');\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations unix_seq_ops = {\n\t.start  = unix_seq_start,\n\t.next   = unix_seq_next,\n\t.stop   = unix_seq_stop,\n\t.show   = unix_seq_show,\n};\n\nstatic int unix_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open_net(inode, file, &unix_seq_ops,\n\t\t\t    sizeof(struct seq_net_private));\n}\n\nstatic const struct file_operations unix_seq_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= unix_seq_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release_net,\n};\n\n#endif\n\nstatic const struct net_proto_family unix_family_ops = {\n\t.family = PF_UNIX,\n\t.create = unix_create,\n\t.owner\t= THIS_MODULE,\n};\n\n\nstatic int __net_init unix_net_init(struct net *net)\n{\n\tint error = -ENOMEM;\n\n\tnet->unx.sysctl_max_dgram_qlen = 10;\n\tif (unix_sysctl_register(net))\n\t\tgoto out;\n\n#ifdef CONFIG_PROC_FS\n\tif (!proc_create(\"unix\", 0, net->proc_net, &unix_seq_fops)) {\n\t\tunix_sysctl_unregister(net);\n\t\tgoto out;\n\t}\n#endif\n\terror = 0;\nout:\n\treturn error;\n}\n\nstatic void __net_exit unix_net_exit(struct net *net)\n{\n\tunix_sysctl_unregister(net);\n\tremove_proc_entry(\"unix\", net->proc_net);\n}\n\nstatic struct pernet_operations unix_net_ops = {\n\t.init = unix_net_init,\n\t.exit = unix_net_exit,\n};\n\nstatic int __init af_unix_init(void)\n{\n\tint rc = -1;\n\n\tBUILD_BUG_ON(sizeof(struct unix_skb_parms) > FIELD_SIZEOF(struct sk_buff, cb));\n\n\trc = proto_register(&unix_proto, 1);\n\tif (rc != 0) {\n\t\tpr_crit(\"%s: Cannot create unix_sock SLAB cache!\\n\", __func__);\n\t\tgoto out;\n\t}\n\n\tsock_register(&unix_family_ops);\n\tregister_pernet_subsys(&unix_net_ops);\nout:\n\treturn rc;\n}\n\nstatic void __exit af_unix_exit(void)\n{\n\tsock_unregister(PF_UNIX);\n\tproto_unregister(&unix_proto);\n\tunregister_pernet_subsys(&unix_net_ops);\n}\n\n/* Earlier than device_initcall() so that other drivers invoking\n   request_module() don't end up in a loop when modprobe tries\n   to use a UNIX socket. But later than subsys_initcall() because\n   we depend on stuff initialised there */\nfs_initcall(af_unix_init);\nmodule_exit(af_unix_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_NETPROTO(PF_UNIX);\n", "/*\n * NET3:\tGarbage Collector For AF_UNIX sockets\n *\n * Garbage Collector:\n *\tCopyright (C) Barak A. Pearlmutter.\n *\tReleased under the GPL version 2 or later.\n *\n * Chopped about by Alan Cox 22/3/96 to make it fit the AF_UNIX socket problem.\n * If it doesn't work blame me, it worked when Barak sent it.\n *\n * Assumptions:\n *\n *  - object w/ a bit\n *  - free list\n *\n * Current optimizations:\n *\n *  - explicit stack instead of recursion\n *  - tail recurse on first born instead of immediate push/pop\n *  - we gather the stuff that should not be killed into tree\n *    and stack is just a path from root to the current pointer.\n *\n *  Future optimizations:\n *\n *  - don't just push entire root set; process in place\n *\n *\tThis program is free software; you can redistribute it and/or\n *\tmodify it under the terms of the GNU General Public License\n *\tas published by the Free Software Foundation; either version\n *\t2 of the License, or (at your option) any later version.\n *\n *  Fixes:\n *\tAlan Cox\t07 Sept\t1997\tVmalloc internal stack as needed.\n *\t\t\t\t\tCope with changing max_files.\n *\tAl Viro\t\t11 Oct 1998\n *\t\tGraph may have cycles. That is, we can send the descriptor\n *\t\tof foo to bar and vice versa. Current code chokes on that.\n *\t\tFix: move SCM_RIGHTS ones into the separate list and then\n *\t\tskb_free() them all instead of doing explicit fput's.\n *\t\tAnother problem: since fput() may block somebody may\n *\t\tcreate a new unix_socket when we are in the middle of sweep\n *\t\tphase. Fix: revert the logic wrt MARKED. Mark everything\n *\t\tupon the beginning and unmark non-junk ones.\n *\n *\t\t[12 Oct 1998] AAARGH! New code purges all SCM_RIGHTS\n *\t\tsent to connect()'ed but still not accept()'ed sockets.\n *\t\tFixed. Old code had slightly different problem here:\n *\t\textra fput() in situation when we passed the descriptor via\n *\t\tsuch socket and closed it (descriptor). That would happen on\n *\t\teach unix_gc() until the accept(). Since the struct file in\n *\t\tquestion would go to the free list and might be reused...\n *\t\tThat might be the reason of random oopses on filp_close()\n *\t\tin unrelated processes.\n *\n *\tAV\t\t28 Feb 1999\n *\t\tKill the explicit allocation of stack. Now we keep the tree\n *\t\twith root in dummy + pointer (gc_current) to one of the nodes.\n *\t\tStack is represented as path from gc_current to dummy. Unmark\n *\t\tnow means \"add to tree\". Push == \"make it a son of gc_current\".\n *\t\tPop == \"move gc_current to parent\". We keep only pointers to\n *\t\tparents (->gc_tree).\n *\tAV\t\t1 Mar 1999\n *\t\tDamn. Added missing check for ->dead in listen queues scanning.\n *\n *\tMiklos Szeredi 25 Jun 2007\n *\t\tReimplement with a cycle collecting algorithm. This should\n *\t\tsolve several problems with the previous code, like being racy\n *\t\twrt receive and holding up unrelated socket operations.\n */\n\n#include <linux/kernel.h>\n#include <linux/string.h>\n#include <linux/socket.h>\n#include <linux/un.h>\n#include <linux/net.h>\n#include <linux/fs.h>\n#include <linux/skbuff.h>\n#include <linux/netdevice.h>\n#include <linux/file.h>\n#include <linux/proc_fs.h>\n#include <linux/mutex.h>\n#include <linux/wait.h>\n\n#include <net/sock.h>\n#include <net/af_unix.h>\n#include <net/scm.h>\n#include <net/tcp_states.h>\n\n/* Internal data structures and random procedures: */\n\nstatic LIST_HEAD(gc_inflight_list);\nstatic LIST_HEAD(gc_candidates);\nstatic DEFINE_SPINLOCK(unix_gc_lock);\nstatic DECLARE_WAIT_QUEUE_HEAD(unix_gc_wait);\n\nunsigned int unix_tot_inflight;\n\nstruct sock *unix_get_socket(struct file *filp)\n{\n\tstruct sock *u_sock = NULL;\n\tstruct inode *inode = file_inode(filp);\n\n\t/* Socket ? */\n\tif (S_ISSOCK(inode->i_mode) && !(filp->f_mode & FMODE_PATH)) {\n\t\tstruct socket *sock = SOCKET_I(inode);\n\t\tstruct sock *s = sock->sk;\n\n\t\t/* PF_UNIX ? */\n\t\tif (s && sock->ops && sock->ops->family == PF_UNIX)\n\t\t\tu_sock = s;\n\t}\n\treturn u_sock;\n}\n\n/* Keep the number of times in flight count for the file\n * descriptor if it is for an AF_UNIX socket.\n */\n\nvoid unix_inflight(struct user_struct *user, struct file *fp)\n{\n\tstruct sock *s = unix_get_socket(fp);\n\n\tspin_lock(&unix_gc_lock);\n\n\tif (s) {\n\t\tstruct unix_sock *u = unix_sk(s);\n\n\t\tif (atomic_long_inc_return(&u->inflight) == 1) {\n\t\t\tBUG_ON(!list_empty(&u->link));\n\t\t\tlist_add_tail(&u->link, &gc_inflight_list);\n\t\t} else {\n\t\t\tBUG_ON(list_empty(&u->link));\n\t\t}\n\t\tunix_tot_inflight++;\n\t}\n\tuser->unix_inflight++;\n\tspin_unlock(&unix_gc_lock);\n}\n\nvoid unix_notinflight(struct user_struct *user, struct file *fp)\n{\n\tstruct sock *s = unix_get_socket(fp);\n\n\tspin_lock(&unix_gc_lock);\n\n\tif (s) {\n\t\tstruct unix_sock *u = unix_sk(s);\n\n\t\tBUG_ON(list_empty(&u->link));\n\n\t\tif (atomic_long_dec_and_test(&u->inflight))\n\t\t\tlist_del_init(&u->link);\n\t\tunix_tot_inflight--;\n\t}\n\tuser->unix_inflight--;\n\tspin_unlock(&unix_gc_lock);\n}\n\nstatic void scan_inflight(struct sock *x, void (*func)(struct unix_sock *),\n\t\t\t  struct sk_buff_head *hitlist)\n{\n\tstruct sk_buff *skb;\n\tstruct sk_buff *next;\n\n\tspin_lock(&x->sk_receive_queue.lock);\n\tskb_queue_walk_safe(&x->sk_receive_queue, skb, next) {\n\t\t/* Do we have file descriptors ? */\n\t\tif (UNIXCB(skb).fp) {\n\t\t\tbool hit = false;\n\t\t\t/* Process the descriptors of this socket */\n\t\t\tint nfd = UNIXCB(skb).fp->count;\n\t\t\tstruct file **fp = UNIXCB(skb).fp->fp;\n\n\t\t\twhile (nfd--) {\n\t\t\t\t/* Get the socket the fd matches if it indeed does so */\n\t\t\t\tstruct sock *sk = unix_get_socket(*fp++);\n\n\t\t\t\tif (sk) {\n\t\t\t\t\tstruct unix_sock *u = unix_sk(sk);\n\n\t\t\t\t\t/* Ignore non-candidates, they could\n\t\t\t\t\t * have been added to the queues after\n\t\t\t\t\t * starting the garbage collection\n\t\t\t\t\t */\n\t\t\t\t\tif (test_bit(UNIX_GC_CANDIDATE, &u->gc_flags)) {\n\t\t\t\t\t\thit = true;\n\n\t\t\t\t\t\tfunc(u);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (hit && hitlist != NULL) {\n\t\t\t\t__skb_unlink(skb, &x->sk_receive_queue);\n\t\t\t\t__skb_queue_tail(hitlist, skb);\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock(&x->sk_receive_queue.lock);\n}\n\nstatic void scan_children(struct sock *x, void (*func)(struct unix_sock *),\n\t\t\t  struct sk_buff_head *hitlist)\n{\n\tif (x->sk_state != TCP_LISTEN) {\n\t\tscan_inflight(x, func, hitlist);\n\t} else {\n\t\tstruct sk_buff *skb;\n\t\tstruct sk_buff *next;\n\t\tstruct unix_sock *u;\n\t\tLIST_HEAD(embryos);\n\n\t\t/* For a listening socket collect the queued embryos\n\t\t * and perform a scan on them as well.\n\t\t */\n\t\tspin_lock(&x->sk_receive_queue.lock);\n\t\tskb_queue_walk_safe(&x->sk_receive_queue, skb, next) {\n\t\t\tu = unix_sk(skb->sk);\n\n\t\t\t/* An embryo cannot be in-flight, so it's safe\n\t\t\t * to use the list link.\n\t\t\t */\n\t\t\tBUG_ON(!list_empty(&u->link));\n\t\t\tlist_add_tail(&u->link, &embryos);\n\t\t}\n\t\tspin_unlock(&x->sk_receive_queue.lock);\n\n\t\twhile (!list_empty(&embryos)) {\n\t\t\tu = list_entry(embryos.next, struct unix_sock, link);\n\t\t\tscan_inflight(&u->sk, func, hitlist);\n\t\t\tlist_del_init(&u->link);\n\t\t}\n\t}\n}\n\nstatic void dec_inflight(struct unix_sock *usk)\n{\n\tatomic_long_dec(&usk->inflight);\n}\n\nstatic void inc_inflight(struct unix_sock *usk)\n{\n\tatomic_long_inc(&usk->inflight);\n}\n\nstatic void inc_inflight_move_tail(struct unix_sock *u)\n{\n\tatomic_long_inc(&u->inflight);\n\t/* If this still might be part of a cycle, move it to the end\n\t * of the list, so that it's checked even if it was already\n\t * passed over\n\t */\n\tif (test_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags))\n\t\tlist_move_tail(&u->link, &gc_candidates);\n}\n\nstatic bool gc_in_progress;\n#define UNIX_INFLIGHT_TRIGGER_GC 16000\n\nvoid wait_for_unix_gc(void)\n{\n\t/* If number of inflight sockets is insane,\n\t * force a garbage collect right now.\n\t */\n\tif (unix_tot_inflight > UNIX_INFLIGHT_TRIGGER_GC && !gc_in_progress)\n\t\tunix_gc();\n\twait_event(unix_gc_wait, gc_in_progress == false);\n}\n\n/* The external entry point: unix_gc() */\nvoid unix_gc(void)\n{\n\tstruct unix_sock *u;\n\tstruct unix_sock *next;\n\tstruct sk_buff_head hitlist;\n\tstruct list_head cursor;\n\tLIST_HEAD(not_cycle_list);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* Avoid a recursive GC. */\n\tif (gc_in_progress)\n\t\tgoto out;\n\n\tgc_in_progress = true;\n\t/* First, select candidates for garbage collection.  Only\n\t * in-flight sockets are considered, and from those only ones\n\t * which don't have any external reference.\n\t *\n\t * Holding unix_gc_lock will protect these candidates from\n\t * being detached, and hence from gaining an external\n\t * reference.  Since there are no possible receivers, all\n\t * buffers currently on the candidates' queues stay there\n\t * during the garbage collection.\n\t *\n\t * We also know that no new candidate can be added onto the\n\t * receive queues.  Other, non candidate sockets _can_ be\n\t * added to queue, so we must make sure only to touch\n\t * candidates.\n\t */\n\tlist_for_each_entry_safe(u, next, &gc_inflight_list, link) {\n\t\tlong total_refs;\n\t\tlong inflight_refs;\n\n\t\ttotal_refs = file_count(u->sk.sk_socket->file);\n\t\tinflight_refs = atomic_long_read(&u->inflight);\n\n\t\tBUG_ON(inflight_refs < 1);\n\t\tBUG_ON(total_refs < inflight_refs);\n\t\tif (total_refs == inflight_refs) {\n\t\t\tlist_move_tail(&u->link, &gc_candidates);\n\t\t\t__set_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\t\t__set_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t}\n\t}\n\n\t/* Now remove all internal in-flight reference to children of\n\t * the candidates.\n\t */\n\tlist_for_each_entry(u, &gc_candidates, link)\n\t\tscan_children(&u->sk, dec_inflight, NULL);\n\n\t/* Restore the references for children of all candidates,\n\t * which have remaining references.  Do this recursively, so\n\t * only those remain, which form cyclic references.\n\t *\n\t * Use a \"cursor\" link, to make the list traversal safe, even\n\t * though elements might be moved about.\n\t */\n\tlist_add(&cursor, &gc_candidates);\n\twhile (cursor.next != &gc_candidates) {\n\t\tu = list_entry(cursor.next, struct unix_sock, link);\n\n\t\t/* Move cursor to after the current position. */\n\t\tlist_move(&cursor, &u->link);\n\n\t\tif (atomic_long_read(&u->inflight) > 0) {\n\t\t\tlist_move_tail(&u->link, &not_cycle_list);\n\t\t\t__clear_bit(UNIX_GC_MAYBE_CYCLE, &u->gc_flags);\n\t\t\tscan_children(&u->sk, inc_inflight_move_tail, NULL);\n\t\t}\n\t}\n\tlist_del(&cursor);\n\n\t/* not_cycle_list contains those sockets which do not make up a\n\t * cycle.  Restore these to the inflight list.\n\t */\n\twhile (!list_empty(&not_cycle_list)) {\n\t\tu = list_entry(not_cycle_list.next, struct unix_sock, link);\n\t\t__clear_bit(UNIX_GC_CANDIDATE, &u->gc_flags);\n\t\tlist_move_tail(&u->link, &gc_inflight_list);\n\t}\n\n\t/* Now gc_candidates contains only garbage.  Restore original\n\t * inflight counters for these as well, and remove the skbuffs\n\t * which are creating the cycle(s).\n\t */\n\tskb_queue_head_init(&hitlist);\n\tlist_for_each_entry(u, &gc_candidates, link)\n\tscan_children(&u->sk, inc_inflight, &hitlist);\n\n\tspin_unlock(&unix_gc_lock);\n\n\t/* Here we are. Hitlist is filled. Die. */\n\t__skb_queue_purge(&hitlist);\n\n\tspin_lock(&unix_gc_lock);\n\n\t/* All candidates should have been detached by now. */\n\tBUG_ON(!list_empty(&gc_candidates));\n\tgc_in_progress = false;\n\twake_up(&unix_gc_wait);\n\n out:\n\tspin_unlock(&unix_gc_lock);\n}\n"], "filenames": ["include/net/af_unix.h", "include/net/scm.h", "net/core/scm.c", "net/unix/af_unix.c", "net/unix/garbage.c"], "buggy_code_start_loc": [9, 23, 89, 1499, 119], "buggy_code_end_loc": [11, 23, 338, 1565, 156], "fixing_code_start_loc": [9, 24, 90, 1499, 119], "fixing_code_end_loc": [11, 25, 346, 1565, 156], "type": "CWE-399", "message": "The Linux kernel before 4.5 allows local users to bypass file-descriptor limits and cause a denial of service (memory consumption) by leveraging incorrect tracking of descriptor ownership and sending each descriptor over a UNIX socket before closing it. NOTE: this vulnerability exists because of an incorrect fix for CVE-2013-4312.", "other": {"cve": {"id": "CVE-2016-2550", "sourceIdentifier": "security@debian.org", "published": "2016-04-27T17:59:19.977", "lastModified": "2018-01-18T18:18:05.917", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Linux kernel before 4.5 allows local users to bypass file-descriptor limits and cause a denial of service (memory consumption) by leveraging incorrect tracking of descriptor ownership and sending each descriptor over a UNIX socket before closing it. NOTE: this vulnerability exists because of an incorrect fix for CVE-2013-4312."}, {"lang": "es", "value": "El kernel de Linux en versiones anteriores a 4.5 permite a usuarios locales eludir los l\u00edmites del archivo descriptor y causar una denegaci\u00f3n de servicio (consumo de memoria) mediante el aprovechamiento del incorrecto seguimiento de la propiedad del descriptor y enviando cada descriptor a trav\u00e9s de un socket UNIX antes de cerrarlo. NOTA: esta vulnerabilidad existe debido a una soluci\u00f3n incorrecta para CVE-2013-4312."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-399"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.4.8", "matchCriteriaId": "3E43C27F-72D6-4615-8337-67245A069FFD"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=415e3d3e90ce9e18727e8843ae343eda5a58fad6", "source": "security@debian.org", "tags": ["Vendor Advisory"]}, {"url": "http://www.debian.org/security/2016/dsa-3503", "source": "security@debian.org"}, {"url": "http://www.openwall.com/lists/oss-security/2016/02/23/2", "source": "security@debian.org"}, {"url": "http://www.oracle.com/technetwork/security-advisory/cpujan2018-3236628.html", "source": "security@debian.org"}, {"url": "http://www.ubuntu.com/usn/USN-2946-1", "source": "security@debian.org"}, {"url": "http://www.ubuntu.com/usn/USN-2946-2", "source": "security@debian.org"}, {"url": "http://www.ubuntu.com/usn/USN-2947-1", "source": "security@debian.org"}, {"url": "http://www.ubuntu.com/usn/USN-2947-2", "source": "security@debian.org"}, {"url": "http://www.ubuntu.com/usn/USN-2947-3", "source": "security@debian.org"}, {"url": "http://www.ubuntu.com/usn/USN-2948-1", "source": "security@debian.org"}, {"url": "http://www.ubuntu.com/usn/USN-2948-2", "source": "security@debian.org"}, {"url": "http://www.ubuntu.com/usn/USN-2949-1", "source": "security@debian.org"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1311517", "source": "security@debian.org"}, {"url": "https://github.com/torvalds/linux/commit/415e3d3e90ce9e18727e8843ae343eda5a58fad6", "source": "security@debian.org", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/415e3d3e90ce9e18727e8843ae343eda5a58fad6"}}