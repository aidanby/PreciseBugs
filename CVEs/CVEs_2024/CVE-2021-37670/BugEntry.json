{"buggy_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/searchsorted_op.h\"\n\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\ntemplate <typename T, typename OutType>\nstruct UpperBoundFunctor<CPUDevice, T, OutType> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<T, 1>::ConstTensor& sorted_inputs,\n                        const typename TTypes<T, 1>::ConstTensor& values,\n                        int batch_size, int num_inputs, int num_values,\n                        typename TTypes<OutType, 1>::Tensor* output) {\n    // TODO(rmlarsen): add multithreading or interleaving.\n    for (int b = 0; b < batch_size; ++b) {\n      const T* sorted_inputs_ptr = sorted_inputs.data() + b * num_inputs;\n      OutType* output_ptr = output->data() + b * num_values;\n      for (int i = 0; i < num_values; ++i) {\n        output_ptr[i] =\n            std::upper_bound(sorted_inputs_ptr, sorted_inputs_ptr + num_inputs,\n                             values(i + b * num_values)) -\n            sorted_inputs_ptr;\n      }\n    }\n\n    return Status::OK();\n  }\n};\n\ntemplate <typename T, typename OutType>\nstruct LowerBoundFunctor<CPUDevice, T, OutType> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<T, 1>::ConstTensor& sorted_inputs,\n                        const typename TTypes<T, 1>::ConstTensor& values,\n                        int batch_size, int num_inputs, int num_values,\n                        typename TTypes<OutType, 1>::Tensor* output) {\n    // TODO(rmlarsen): add multithreading or interleaving.\n    for (int b = 0; b < batch_size; ++b) {\n      const T* sorted_inputs_ptr = sorted_inputs.data() + b * num_inputs;\n      OutType* output_ptr = output->data() + b * num_values;\n      for (int i = 0; i < num_values; ++i) {\n        output_ptr[i] =\n            std::lower_bound(sorted_inputs_ptr, sorted_inputs_ptr + num_inputs,\n                             values(i + b * num_values)) -\n            sorted_inputs_ptr;\n      }\n    }\n\n    return Status::OK();\n  }\n};\n}  // namespace functor\n\ntemplate <typename Device, typename T, typename OutType>\nclass UpperBoundOp : public OpKernel {\n public:\n  explicit UpperBoundOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::UpperBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};\n\ntemplate <typename Device, typename T, typename OutType>\nclass LowerBoundOp : public OpKernel {\n public:\n  explicit LowerBoundOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::LowerBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                      \\\n                              .Device(DEVICE_CPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          UpperBoundOp<CPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                      \\\n                              .Device(DEVICE_CPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int64>(\"out_type\"), \\\n                          UpperBoundOp<CPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                      \\\n                              .Device(DEVICE_GPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          UpperBoundOp<GPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                      \\\n                              .Device(DEVICE_GPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int64>(\"out_type\"), \\\n                          UpperBoundOp<GPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                      \\\n                              .Device(DEVICE_CPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          LowerBoundOp<CPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                      \\\n                              .Device(DEVICE_CPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int64>(\"out_type\"), \\\n                          LowerBoundOp<CPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                      \\\n                              .Device(DEVICE_GPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          LowerBoundOp<GPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                      \\\n                              .Device(DEVICE_GPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int64>(\"out_type\"), \\\n                          LowerBoundOp<GPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n}  // namespace tensorflow\n"], "fixing_code": ["/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#define EIGEN_USE_THREADS\n\n#include \"tensorflow/core/kernels/searchsorted_op.h\"\n\n#include \"tensorflow/core/framework/bounds_check.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_shape.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\nnamespace functor {\ntemplate <typename T, typename OutType>\nstruct UpperBoundFunctor<CPUDevice, T, OutType> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<T, 1>::ConstTensor& sorted_inputs,\n                        const typename TTypes<T, 1>::ConstTensor& values,\n                        int batch_size, int num_inputs, int num_values,\n                        typename TTypes<OutType, 1>::Tensor* output) {\n    // TODO(rmlarsen): add multithreading or interleaving.\n    for (int b = 0; b < batch_size; ++b) {\n      const T* sorted_inputs_ptr = sorted_inputs.data() + b * num_inputs;\n      OutType* output_ptr = output->data() + b * num_values;\n      for (int i = 0; i < num_values; ++i) {\n        output_ptr[i] =\n            std::upper_bound(sorted_inputs_ptr, sorted_inputs_ptr + num_inputs,\n                             values(i + b * num_values)) -\n            sorted_inputs_ptr;\n      }\n    }\n\n    return Status::OK();\n  }\n};\n\ntemplate <typename T, typename OutType>\nstruct LowerBoundFunctor<CPUDevice, T, OutType> {\n  static Status Compute(OpKernelContext* context,\n                        const typename TTypes<T, 1>::ConstTensor& sorted_inputs,\n                        const typename TTypes<T, 1>::ConstTensor& values,\n                        int batch_size, int num_inputs, int num_values,\n                        typename TTypes<OutType, 1>::Tensor* output) {\n    // TODO(rmlarsen): add multithreading or interleaving.\n    for (int b = 0; b < batch_size; ++b) {\n      const T* sorted_inputs_ptr = sorted_inputs.data() + b * num_inputs;\n      OutType* output_ptr = output->data() + b * num_values;\n      for (int i = 0; i < num_values; ++i) {\n        output_ptr[i] =\n            std::lower_bound(sorted_inputs_ptr, sorted_inputs_ptr + num_inputs,\n                             values(i + b * num_values)) -\n            sorted_inputs_ptr;\n      }\n    }\n\n    return Status::OK();\n  }\n};\n}  // namespace functor\n\ntemplate <typename Device, typename T, typename OutType>\nclass UpperBoundOp : public OpKernel {\n public:\n  explicit UpperBoundOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // inputs must be at least a matrix\n    OP_REQUIRES(\n        ctx, sorted_inputs_t.shape().dims() >= 2,\n        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::UpperBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};\n\ntemplate <typename Device, typename T, typename OutType>\nclass LowerBoundOp : public OpKernel {\n public:\n  explicit LowerBoundOp(OpKernelConstruction* ctx) : OpKernel(ctx) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    const Tensor& sorted_inputs_t = ctx->input(0);\n    const Tensor& values_t = ctx->input(1);\n\n    // inputs must be at least a matrix\n    OP_REQUIRES(\n        ctx, sorted_inputs_t.shape().dims() >= 2,\n        errors::InvalidArgument(\"sorted input argument must be a matrix\"));\n    // must have same batch dim_size for both\n    OP_REQUIRES(ctx, sorted_inputs_t.dim_size(0) == values_t.dim_size(0),\n                Status(error::INVALID_ARGUMENT,\n                       \"Leading dim_size of both tensors must match.\"));\n\n    // this is required because we do indexing in int32 on the GPU\n    OP_REQUIRES(ctx, values_t.NumElements() < std::numeric_limits<int>::max(),\n                Status(error::INVALID_ARGUMENT,\n                       \"values tensor size must less than INT_MAX\"));\n\n    Tensor* output_t;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, values_t.shape(), &output_t));\n\n    if (output_t->dtype() == DT_INT32) {\n      OP_REQUIRES(ctx,\n                  FastBoundsCheck(sorted_inputs_t.dim_size(1),\n                                  std::numeric_limits<int>::max()),\n                  errors::InvalidArgument(\"trailing dim_size must less than \"\n                                          \"INT_MAX for int32 output type, was \",\n                                          sorted_inputs_t.dim_size(1)));\n    }\n\n    auto output = output_t->template flat<OutType>();\n    const auto sorted_inputs = sorted_inputs_t.template flat<T>();\n    const auto values = values_t.template flat<T>();\n    OP_REQUIRES_OK(\n        ctx, functor::LowerBoundFunctor<Device, T, OutType>::Compute(\n                 ctx, sorted_inputs, values, sorted_inputs_t.dim_size(0),\n                 sorted_inputs_t.dim_size(1), values_t.dim_size(1), &output));\n  }\n};\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                      \\\n                              .Device(DEVICE_CPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          UpperBoundOp<CPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                      \\\n                              .Device(DEVICE_CPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int64>(\"out_type\"), \\\n                          UpperBoundOp<CPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                      \\\n                              .Device(DEVICE_GPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          UpperBoundOp<GPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"UpperBound\")                      \\\n                              .Device(DEVICE_GPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int64>(\"out_type\"), \\\n                          UpperBoundOp<GPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                      \\\n                              .Device(DEVICE_CPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          LowerBoundOp<CPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                      \\\n                              .Device(DEVICE_CPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int64>(\"out_type\"), \\\n                          LowerBoundOp<CPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                      \\\n                              .Device(DEVICE_GPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int32>(\"out_type\"), \\\n                          LowerBoundOp<GPUDevice, type, int32>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#define REGISTER_KERNELS(type)                                    \\\n  REGISTER_KERNEL_BUILDER(Name(\"LowerBound\")                      \\\n                              .Device(DEVICE_GPU)                 \\\n                              .TypeConstraint<type>(\"T\")          \\\n                              .TypeConstraint<int64>(\"out_type\"), \\\n                          LowerBoundOp<GPUDevice, type, int64>);\n\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNELS);\n#undef REGISTER_KERNELS\n\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n}  // namespace tensorflow\n"], "filenames": ["tensorflow/core/kernels/searchsorted_op.cc"], "buggy_code_start_loc": [88], "buggy_code_end_loc": [129], "fixing_code_start_loc": [89], "fixing_code_end_loc": [138], "type": "CWE-125", "message": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions an attacker can read from outside of bounds of heap allocated data by sending specially crafted illegal arguments to `tf.raw_ops.UpperBound`. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/searchsorted_op.cc#L85-L104) does not validate the rank of `sorted_input` argument. A similar issue occurs in `tf.raw_ops.LowerBound`. We have patched the issue in GitHub commit 42459e4273c2e47a3232cc16c4f4fff3b3a35c38. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range.", "other": {"cve": {"id": "CVE-2021-37670", "sourceIdentifier": "security-advisories@github.com", "published": "2021-08-12T23:15:07.693", "lastModified": "2021-08-19T14:28:10.210", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "TensorFlow is an end-to-end open source platform for machine learning. In affected versions an attacker can read from outside of bounds of heap allocated data by sending specially crafted illegal arguments to `tf.raw_ops.UpperBound`. The [implementation](https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/searchsorted_op.cc#L85-L104) does not validate the rank of `sorted_input` argument. A similar issue occurs in `tf.raw_ops.LowerBound`. We have patched the issue in GitHub commit 42459e4273c2e47a3232cc16c4f4fff3b3a35c38. The fix will be included in TensorFlow 2.6.0. We will also cherrypick this commit on TensorFlow 2.5.1, TensorFlow 2.4.3, and TensorFlow 2.3.4, as these are also affected and still in supported range."}, {"lang": "es", "value": "TensorFlow es una plataforma de c\u00f3digo abierto de extremo a extremo para el aprendizaje autom\u00e1tico.&#xa0;En las versiones afectadas, un atacante puede leer desde fuera de l\u00edmites de los datos asignados a la pila mediante el env\u00edo de argumentos ilegales especialmente dise\u00f1ados a \"tf.raw_ops.UpperBound\".&#xa0;La [implementaci\u00f3n] (https://github.com/tensorflow/tensorflow/blob/460e000de3a83278fb00b61a16d161b1964f15f4/tensorflow/core/kernels/searchsorted_op.cc#L85-L104) no comprueba el rango del argumento \"sorted_input\".&#xa0;Un problema similar ocurre en \"tf.raw_ops.LowerBound\".&#xa0;Hemos solucionado el problema en GitHub commit 42459e4273c2e47a3232cc16c4f4fff3b3a35c38.&#xa0;La correcci\u00f3n ser\u00e1 incluida en TensorFlow versi\u00f3n 2.6.0.&#xa0;Tambi\u00e9n seleccionaremos este commit en TensorFlow versi\u00f3n 2.5.1, TensorFlow versi\u00f3n 2.4.3 y TensorFlow versi\u00f3n 2.3.4, ya que estos tambi\u00e9n est\u00e1n afectados y a\u00fan se encuentran en el rango admitido."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.3.0", "versionEndExcluding": "2.3.4", "matchCriteriaId": "0F83C081-51CC-415F-A8C0-0A44C75E2CD6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:*:*:*:*:*:*:*:*", "versionStartIncluding": "2.4.0", "versionEndExcluding": "2.4.3", "matchCriteriaId": "BD3F2BF8-EBA9-42BF-8F9B-D918B880B15A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.5.0:*:*:*:*:*:*:*", "matchCriteriaId": "D03E99A7-4E3D-427D-A156-C0713E9FB02A"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc0:*:*:*:*:*:*", "matchCriteriaId": "70FA6E48-6C57-40CA-809F-4E3D07CBF348"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc1:*:*:*:*:*:*", "matchCriteriaId": "42187561-E491-434D-828C-F36701446634"}, {"vulnerable": true, "criteria": "cpe:2.3:a:google:tensorflow:2.6.0:rc2:*:*:*:*:*:*", "matchCriteriaId": "C66B61C8-450A-4C5E-9174-F970D6DEE778"}]}]}], "references": [{"url": "https://github.com/tensorflow/tensorflow/commit/42459e4273c2e47a3232cc16c4f4fff3b3a35c38", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/tensorflow/tensorflow/security/advisories/GHSA-9697-98pf-4rw7", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/tensorflow/tensorflow/commit/42459e4273c2e47a3232cc16c4f4fff3b3a35c38"}}