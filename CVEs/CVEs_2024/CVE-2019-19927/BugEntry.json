{"buggy_code": ["/*\n * Copyright (c) Red Hat Inc.\n\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sub license,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the\n * next paragraph) shall be included in all copies or substantial portions\n * of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL\n * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie <airlied@redhat.com>\n *          Jerome Glisse <jglisse@redhat.com>\n *          Pauli Nieminen <suokkos@gmail.com>\n */\n\n/* simple list based uncached page pool\n * - Pool collects resently freed pages for reuse\n * - Use page->lru to keep a free list\n * - doesn't track currently in use pages\n */\n\n#define pr_fmt(fmt) \"[TTM] \" fmt\n\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/highmem.h>\n#include <linux/mm_types.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/seq_file.h> /* for seq_printf */\n#include <linux/slab.h>\n#include <linux/dma-mapping.h>\n\n#include <linux/atomic.h>\n\n#include <drm/ttm/ttm_bo_driver.h>\n#include <drm/ttm/ttm_page_alloc.h>\n#include <drm/ttm/ttm_set_memory.h>\n\n#define NUM_PAGES_TO_ALLOC\t\t(PAGE_SIZE/sizeof(struct page *))\n#define SMALL_ALLOCATION\t\t16\n#define FREE_ALL_PAGES\t\t\t(~0U)\n/* times are in msecs */\n#define PAGE_FREE_INTERVAL\t\t1000\n\n/**\n * struct ttm_page_pool - Pool to reuse recently allocated uc/wc pages.\n *\n * @lock: Protects the shared pool from concurrnet access. Must be used with\n * irqsave/irqrestore variants because pool allocator maybe called from\n * delayed work.\n * @fill_lock: Prevent concurrent calls to fill.\n * @list: Pool of free uc/wc pages for fast reuse.\n * @gfp_flags: Flags to pass for alloc_page.\n * @npages: Number of pages in pool.\n */\nstruct ttm_page_pool {\n\tspinlock_t\t\tlock;\n\tbool\t\t\tfill_lock;\n\tstruct list_head\tlist;\n\tgfp_t\t\t\tgfp_flags;\n\tunsigned\t\tnpages;\n\tchar\t\t\t*name;\n\tunsigned long\t\tnfrees;\n\tunsigned long\t\tnrefills;\n\tunsigned int\t\torder;\n};\n\n/**\n * Limits for the pool. They are handled without locks because only place where\n * they may change is in sysfs store. They won't have immediate effect anyway\n * so forcing serialization to access them is pointless.\n */\n\nstruct ttm_pool_opts {\n\tunsigned\talloc_size;\n\tunsigned\tmax_size;\n\tunsigned\tsmall;\n};\n\n#define NUM_POOLS 6\n\n/**\n * struct ttm_pool_manager - Holds memory pools for fst allocation\n *\n * Manager is read only object for pool code so it doesn't need locking.\n *\n * @free_interval: minimum number of jiffies between freeing pages from pool.\n * @page_alloc_inited: reference counting for pool allocation.\n * @work: Work that is used to shrink the pool. Work is only run when there is\n * some pages to free.\n * @small_allocation: Limit in number of pages what is small allocation.\n *\n * @pools: All pool objects in use.\n **/\nstruct ttm_pool_manager {\n\tstruct kobject\t\tkobj;\n\tstruct shrinker\t\tmm_shrink;\n\tstruct ttm_pool_opts\toptions;\n\n\tunion {\n\t\tstruct ttm_page_pool\tpools[NUM_POOLS];\n\t\tstruct {\n\t\t\tstruct ttm_page_pool\twc_pool;\n\t\t\tstruct ttm_page_pool\tuc_pool;\n\t\t\tstruct ttm_page_pool\twc_pool_dma32;\n\t\t\tstruct ttm_page_pool\tuc_pool_dma32;\n\t\t\tstruct ttm_page_pool\twc_pool_huge;\n\t\t\tstruct ttm_page_pool\tuc_pool_huge;\n\t\t} ;\n\t};\n};\n\nstatic struct attribute ttm_page_pool_max = {\n\t.name = \"pool_max_size\",\n\t.mode = S_IRUGO | S_IWUSR\n};\nstatic struct attribute ttm_page_pool_small = {\n\t.name = \"pool_small_allocation\",\n\t.mode = S_IRUGO | S_IWUSR\n};\nstatic struct attribute ttm_page_pool_alloc_size = {\n\t.name = \"pool_allocation_size\",\n\t.mode = S_IRUGO | S_IWUSR\n};\n\nstatic struct attribute *ttm_pool_attrs[] = {\n\t&ttm_page_pool_max,\n\t&ttm_page_pool_small,\n\t&ttm_page_pool_alloc_size,\n\tNULL\n};\n\nstatic void ttm_pool_kobj_release(struct kobject *kobj)\n{\n\tstruct ttm_pool_manager *m =\n\t\tcontainer_of(kobj, struct ttm_pool_manager, kobj);\n\tkfree(m);\n}\n\nstatic ssize_t ttm_pool_store(struct kobject *kobj,\n\t\tstruct attribute *attr, const char *buffer, size_t size)\n{\n\tstruct ttm_pool_manager *m =\n\t\tcontainer_of(kobj, struct ttm_pool_manager, kobj);\n\tint chars;\n\tunsigned val;\n\tchars = sscanf(buffer, \"%u\", &val);\n\tif (chars == 0)\n\t\treturn size;\n\n\t/* Convert kb to number of pages */\n\tval = val / (PAGE_SIZE >> 10);\n\n\tif (attr == &ttm_page_pool_max)\n\t\tm->options.max_size = val;\n\telse if (attr == &ttm_page_pool_small)\n\t\tm->options.small = val;\n\telse if (attr == &ttm_page_pool_alloc_size) {\n\t\tif (val > NUM_PAGES_TO_ALLOC*8) {\n\t\t\tpr_err(\"Setting allocation size to %lu is not allowed. Recommended size is %lu\\n\",\n\t\t\t       NUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 7),\n\t\t\t       NUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 10));\n\t\t\treturn size;\n\t\t} else if (val > NUM_PAGES_TO_ALLOC) {\n\t\t\tpr_warn(\"Setting allocation size to larger than %lu is not recommended\\n\",\n\t\t\t\tNUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 10));\n\t\t}\n\t\tm->options.alloc_size = val;\n\t}\n\n\treturn size;\n}\n\nstatic ssize_t ttm_pool_show(struct kobject *kobj,\n\t\tstruct attribute *attr, char *buffer)\n{\n\tstruct ttm_pool_manager *m =\n\t\tcontainer_of(kobj, struct ttm_pool_manager, kobj);\n\tunsigned val = 0;\n\n\tif (attr == &ttm_page_pool_max)\n\t\tval = m->options.max_size;\n\telse if (attr == &ttm_page_pool_small)\n\t\tval = m->options.small;\n\telse if (attr == &ttm_page_pool_alloc_size)\n\t\tval = m->options.alloc_size;\n\n\tval = val * (PAGE_SIZE >> 10);\n\n\treturn snprintf(buffer, PAGE_SIZE, \"%u\\n\", val);\n}\n\nstatic const struct sysfs_ops ttm_pool_sysfs_ops = {\n\t.show = &ttm_pool_show,\n\t.store = &ttm_pool_store,\n};\n\nstatic struct kobj_type ttm_pool_kobj_type = {\n\t.release = &ttm_pool_kobj_release,\n\t.sysfs_ops = &ttm_pool_sysfs_ops,\n\t.default_attrs = ttm_pool_attrs,\n};\n\nstatic struct ttm_pool_manager *_manager;\n\n/**\n * Select the right pool or requested caching state and ttm flags. */\nstatic struct ttm_page_pool *ttm_get_pool(int flags, bool huge,\n\t\t\t\t\t  enum ttm_caching_state cstate)\n{\n\tint pool_index;\n\n\tif (cstate == tt_cached)\n\t\treturn NULL;\n\n\tif (cstate == tt_wc)\n\t\tpool_index = 0x0;\n\telse\n\t\tpool_index = 0x1;\n\n\tif (flags & TTM_PAGE_FLAG_DMA32) {\n\t\tif (huge)\n\t\t\treturn NULL;\n\t\tpool_index |= 0x2;\n\n\t} else if (huge) {\n\t\tpool_index |= 0x4;\n\t}\n\n\treturn &_manager->pools[pool_index];\n}\n\n/* set memory back to wb and free the pages. */\nstatic void ttm_pages_put(struct page *pages[], unsigned npages,\n\t\tunsigned int order)\n{\n\tunsigned int i, pages_nr = (1 << order);\n\n\tif (order == 0) {\n\t\tif (ttm_set_pages_array_wb(pages, npages))\n\t\t\tpr_err(\"Failed to set %d pages to wb!\\n\", npages);\n\t}\n\n\tfor (i = 0; i < npages; ++i) {\n\t\tif (order > 0) {\n\t\t\tif (ttm_set_pages_wb(pages[i], pages_nr))\n\t\t\t\tpr_err(\"Failed to set %d pages to wb!\\n\", pages_nr);\n\t\t}\n\t\t__free_pages(pages[i], order);\n\t}\n}\n\nstatic void ttm_pool_update_free_locked(struct ttm_page_pool *pool,\n\t\tunsigned freed_pages)\n{\n\tpool->npages -= freed_pages;\n\tpool->nfrees += freed_pages;\n}\n\n/**\n * Free pages from pool.\n *\n * To prevent hogging the ttm_swap process we only free NUM_PAGES_TO_ALLOC\n * number of pages in one go.\n *\n * @pool: to free the pages from\n * @free_all: If set to true will free all pages in pool\n * @use_static: Safe to use static buffer\n **/\nstatic int ttm_page_pool_free(struct ttm_page_pool *pool, unsigned nr_free,\n\t\t\t      bool use_static)\n{\n\tstatic struct page *static_buf[NUM_PAGES_TO_ALLOC];\n\tunsigned long irq_flags;\n\tstruct page *p;\n\tstruct page **pages_to_free;\n\tunsigned freed_pages = 0,\n\t\t npages_to_free = nr_free;\n\n\tif (NUM_PAGES_TO_ALLOC < nr_free)\n\t\tnpages_to_free = NUM_PAGES_TO_ALLOC;\n\n\tif (use_static)\n\t\tpages_to_free = static_buf;\n\telse\n\t\tpages_to_free = kmalloc_array(npages_to_free,\n\t\t\t\t\t      sizeof(struct page *),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!pages_to_free) {\n\t\tpr_debug(\"Failed to allocate memory for pool free operation\\n\");\n\t\treturn 0;\n\t}\n\nrestart:\n\tspin_lock_irqsave(&pool->lock, irq_flags);\n\n\tlist_for_each_entry_reverse(p, &pool->list, lru) {\n\t\tif (freed_pages >= npages_to_free)\n\t\t\tbreak;\n\n\t\tpages_to_free[freed_pages++] = p;\n\t\t/* We can only remove NUM_PAGES_TO_ALLOC at a time. */\n\t\tif (freed_pages >= NUM_PAGES_TO_ALLOC) {\n\t\t\t/* remove range of pages from the pool */\n\t\t\t__list_del(p->lru.prev, &pool->list);\n\n\t\t\tttm_pool_update_free_locked(pool, freed_pages);\n\t\t\t/**\n\t\t\t * Because changing page caching is costly\n\t\t\t * we unlock the pool to prevent stalling.\n\t\t\t */\n\t\t\tspin_unlock_irqrestore(&pool->lock, irq_flags);\n\n\t\t\tttm_pages_put(pages_to_free, freed_pages, pool->order);\n\t\t\tif (likely(nr_free != FREE_ALL_PAGES))\n\t\t\t\tnr_free -= freed_pages;\n\n\t\t\tif (NUM_PAGES_TO_ALLOC >= nr_free)\n\t\t\t\tnpages_to_free = nr_free;\n\t\t\telse\n\t\t\t\tnpages_to_free = NUM_PAGES_TO_ALLOC;\n\n\t\t\tfreed_pages = 0;\n\n\t\t\t/* free all so restart the processing */\n\t\t\tif (nr_free)\n\t\t\t\tgoto restart;\n\n\t\t\t/* Not allowed to fall through or break because\n\t\t\t * following context is inside spinlock while we are\n\t\t\t * outside here.\n\t\t\t */\n\t\t\tgoto out;\n\n\t\t}\n\t}\n\n\t/* remove range of pages from the pool */\n\tif (freed_pages) {\n\t\t__list_del(&p->lru, &pool->list);\n\n\t\tttm_pool_update_free_locked(pool, freed_pages);\n\t\tnr_free -= freed_pages;\n\t}\n\n\tspin_unlock_irqrestore(&pool->lock, irq_flags);\n\n\tif (freed_pages)\n\t\tttm_pages_put(pages_to_free, freed_pages, pool->order);\nout:\n\tif (pages_to_free != static_buf)\n\t\tkfree(pages_to_free);\n\treturn nr_free;\n}\n\n/**\n * Callback for mm to request pool to reduce number of page held.\n *\n * XXX: (dchinner) Deadlock warning!\n *\n * This code is crying out for a shrinker per pool....\n */\nstatic unsigned long\nttm_pool_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tstatic DEFINE_MUTEX(lock);\n\tstatic unsigned start_pool;\n\tunsigned i;\n\tunsigned pool_offset;\n\tstruct ttm_page_pool *pool;\n\tint shrink_pages = sc->nr_to_scan;\n\tunsigned long freed = 0;\n\tunsigned int nr_free_pool;\n\n\tif (!mutex_trylock(&lock))\n\t\treturn SHRINK_STOP;\n\tpool_offset = ++start_pool % NUM_POOLS;\n\t/* select start pool in round robin fashion */\n\tfor (i = 0; i < NUM_POOLS; ++i) {\n\t\tunsigned nr_free = shrink_pages;\n\t\tunsigned page_nr;\n\n\t\tif (shrink_pages == 0)\n\t\t\tbreak;\n\n\t\tpool = &_manager->pools[(i + pool_offset)%NUM_POOLS];\n\t\tpage_nr = (1 << pool->order);\n\t\t/* OK to use static buffer since global mutex is held. */\n\t\tnr_free_pool = roundup(nr_free, page_nr) >> pool->order;\n\t\tshrink_pages = ttm_page_pool_free(pool, nr_free_pool, true);\n\t\tfreed += (nr_free_pool - shrink_pages) << pool->order;\n\t\tif (freed >= sc->nr_to_scan)\n\t\t\tbreak;\n\t\tshrink_pages <<= pool->order;\n\t}\n\tmutex_unlock(&lock);\n\treturn freed;\n}\n\n\nstatic unsigned long\nttm_pool_shrink_count(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tunsigned i;\n\tunsigned long count = 0;\n\tstruct ttm_page_pool *pool;\n\n\tfor (i = 0; i < NUM_POOLS; ++i) {\n\t\tpool = &_manager->pools[i];\n\t\tcount += (pool->npages << pool->order);\n\t}\n\n\treturn count;\n}\n\nstatic int ttm_pool_mm_shrink_init(struct ttm_pool_manager *manager)\n{\n\tmanager->mm_shrink.count_objects = ttm_pool_shrink_count;\n\tmanager->mm_shrink.scan_objects = ttm_pool_shrink_scan;\n\tmanager->mm_shrink.seeks = 1;\n\treturn register_shrinker(&manager->mm_shrink);\n}\n\nstatic void ttm_pool_mm_shrink_fini(struct ttm_pool_manager *manager)\n{\n\tunregister_shrinker(&manager->mm_shrink);\n}\n\nstatic int ttm_set_pages_caching(struct page **pages,\n\t\tenum ttm_caching_state cstate, unsigned cpages)\n{\n\tint r = 0;\n\t/* Set page caching */\n\tswitch (cstate) {\n\tcase tt_uncached:\n\t\tr = ttm_set_pages_array_uc(pages, cpages);\n\t\tif (r)\n\t\t\tpr_err(\"Failed to set %d pages to uc!\\n\", cpages);\n\t\tbreak;\n\tcase tt_wc:\n\t\tr = ttm_set_pages_array_wc(pages, cpages);\n\t\tif (r)\n\t\t\tpr_err(\"Failed to set %d pages to wc!\\n\", cpages);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn r;\n}\n\n/**\n * Free pages the pages that failed to change the caching state. If there is\n * any pages that have changed their caching state already put them to the\n * pool.\n */\nstatic void ttm_handle_caching_state_failure(struct list_head *pages,\n\t\tint ttm_flags, enum ttm_caching_state cstate,\n\t\tstruct page **failed_pages, unsigned cpages)\n{\n\tunsigned i;\n\t/* Failed pages have to be freed */\n\tfor (i = 0; i < cpages; ++i) {\n\t\tlist_del(&failed_pages[i]->lru);\n\t\t__free_page(failed_pages[i]);\n\t}\n}\n\n/**\n * Allocate new pages with correct caching.\n *\n * This function is reentrant if caller updates count depending on number of\n * pages returned in pages array.\n */\nstatic int ttm_alloc_new_pages(struct list_head *pages, gfp_t gfp_flags,\n\t\t\t       int ttm_flags, enum ttm_caching_state cstate,\n\t\t\t       unsigned count, unsigned order)\n{\n\tstruct page **caching_array;\n\tstruct page *p;\n\tint r = 0;\n\tunsigned i, j, cpages;\n\tunsigned npages = 1 << order;\n\tunsigned max_cpages = min(count << order, (unsigned)NUM_PAGES_TO_ALLOC);\n\n\t/* allocate array for page caching change */\n\tcaching_array = kmalloc_array(max_cpages, sizeof(struct page *),\n\t\t\t\t      GFP_KERNEL);\n\n\tif (!caching_array) {\n\t\tpr_debug(\"Unable to allocate table for new pages\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0, cpages = 0; i < count; ++i) {\n\t\tp = alloc_pages(gfp_flags, order);\n\n\t\tif (!p) {\n\t\t\tpr_debug(\"Unable to get page %u\\n\", i);\n\n\t\t\t/* store already allocated pages in the pool after\n\t\t\t * setting the caching state */\n\t\t\tif (cpages) {\n\t\t\t\tr = ttm_set_pages_caching(caching_array,\n\t\t\t\t\t\t\t  cstate, cpages);\n\t\t\t\tif (r)\n\t\t\t\t\tttm_handle_caching_state_failure(pages,\n\t\t\t\t\t\tttm_flags, cstate,\n\t\t\t\t\t\tcaching_array, cpages);\n\t\t\t}\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tlist_add(&p->lru, pages);\n\n#ifdef CONFIG_HIGHMEM\n\t\t/* gfp flags of highmem page should never be dma32 so we\n\t\t * we should be fine in such case\n\t\t */\n\t\tif (PageHighMem(p))\n\t\t\tcontinue;\n\n#endif\n\t\tfor (j = 0; j < npages; ++j) {\n\t\t\tcaching_array[cpages++] = p++;\n\t\t\tif (cpages == max_cpages) {\n\n\t\t\t\tr = ttm_set_pages_caching(caching_array,\n\t\t\t\t\t\tcstate, cpages);\n\t\t\t\tif (r) {\n\t\t\t\t\tttm_handle_caching_state_failure(pages,\n\t\t\t\t\t\tttm_flags, cstate,\n\t\t\t\t\t\tcaching_array, cpages);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tcpages = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (cpages) {\n\t\tr = ttm_set_pages_caching(caching_array, cstate, cpages);\n\t\tif (r)\n\t\t\tttm_handle_caching_state_failure(pages,\n\t\t\t\t\tttm_flags, cstate,\n\t\t\t\t\tcaching_array, cpages);\n\t}\nout:\n\tkfree(caching_array);\n\n\treturn r;\n}\n\n/**\n * Fill the given pool if there aren't enough pages and the requested number of\n * pages is small.\n */\nstatic void ttm_page_pool_fill_locked(struct ttm_page_pool *pool, int ttm_flags,\n\t\t\t\t      enum ttm_caching_state cstate,\n\t\t\t\t      unsigned count, unsigned long *irq_flags)\n{\n\tstruct page *p;\n\tint r;\n\tunsigned cpages = 0;\n\t/**\n\t * Only allow one pool fill operation at a time.\n\t * If pool doesn't have enough pages for the allocation new pages are\n\t * allocated from outside of pool.\n\t */\n\tif (pool->fill_lock)\n\t\treturn;\n\n\tpool->fill_lock = true;\n\n\t/* If allocation request is small and there are not enough\n\t * pages in a pool we fill the pool up first. */\n\tif (count < _manager->options.small\n\t\t&& count > pool->npages) {\n\t\tstruct list_head new_pages;\n\t\tunsigned alloc_size = _manager->options.alloc_size;\n\n\t\t/**\n\t\t * Can't change page caching if in irqsave context. We have to\n\t\t * drop the pool->lock.\n\t\t */\n\t\tspin_unlock_irqrestore(&pool->lock, *irq_flags);\n\n\t\tINIT_LIST_HEAD(&new_pages);\n\t\tr = ttm_alloc_new_pages(&new_pages, pool->gfp_flags, ttm_flags,\n\t\t\t\t\tcstate, alloc_size, 0);\n\t\tspin_lock_irqsave(&pool->lock, *irq_flags);\n\n\t\tif (!r) {\n\t\t\tlist_splice(&new_pages, &pool->list);\n\t\t\t++pool->nrefills;\n\t\t\tpool->npages += alloc_size;\n\t\t} else {\n\t\t\tpr_debug(\"Failed to fill pool (%p)\\n\", pool);\n\t\t\t/* If we have any pages left put them to the pool. */\n\t\t\tlist_for_each_entry(p, &new_pages, lru) {\n\t\t\t\t++cpages;\n\t\t\t}\n\t\t\tlist_splice(&new_pages, &pool->list);\n\t\t\tpool->npages += cpages;\n\t\t}\n\n\t}\n\tpool->fill_lock = false;\n}\n\n/**\n * Allocate pages from the pool and put them on the return list.\n *\n * @return zero for success or negative error code.\n */\nstatic int ttm_page_pool_get_pages(struct ttm_page_pool *pool,\n\t\t\t\t   struct list_head *pages,\n\t\t\t\t   int ttm_flags,\n\t\t\t\t   enum ttm_caching_state cstate,\n\t\t\t\t   unsigned count, unsigned order)\n{\n\tunsigned long irq_flags;\n\tstruct list_head *p;\n\tunsigned i;\n\tint r = 0;\n\n\tspin_lock_irqsave(&pool->lock, irq_flags);\n\tif (!order)\n\t\tttm_page_pool_fill_locked(pool, ttm_flags, cstate, count,\n\t\t\t\t\t  &irq_flags);\n\n\tif (count >= pool->npages) {\n\t\t/* take all pages from the pool */\n\t\tlist_splice_init(&pool->list, pages);\n\t\tcount -= pool->npages;\n\t\tpool->npages = 0;\n\t\tgoto out;\n\t}\n\t/* find the last pages to include for requested number of pages. Split\n\t * pool to begin and halve it to reduce search space. */\n\tif (count <= pool->npages/2) {\n\t\ti = 0;\n\t\tlist_for_each(p, &pool->list) {\n\t\t\tif (++i == count)\n\t\t\t\tbreak;\n\t\t}\n\t} else {\n\t\ti = pool->npages + 1;\n\t\tlist_for_each_prev(p, &pool->list) {\n\t\t\tif (--i == count)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\t/* Cut 'count' number of pages from the pool */\n\tlist_cut_position(pages, &pool->list, p);\n\tpool->npages -= count;\n\tcount = 0;\nout:\n\tspin_unlock_irqrestore(&pool->lock, irq_flags);\n\n\t/* clear the pages coming from the pool if requested */\n\tif (ttm_flags & TTM_PAGE_FLAG_ZERO_ALLOC) {\n\t\tstruct page *page;\n\n\t\tlist_for_each_entry(page, pages, lru) {\n\t\t\tif (PageHighMem(page))\n\t\t\t\tclear_highpage(page);\n\t\t\telse\n\t\t\t\tclear_page(page_address(page));\n\t\t}\n\t}\n\n\t/* If pool didn't have enough pages allocate new one. */\n\tif (count) {\n\t\tgfp_t gfp_flags = pool->gfp_flags;\n\n\t\t/* set zero flag for page allocation if required */\n\t\tif (ttm_flags & TTM_PAGE_FLAG_ZERO_ALLOC)\n\t\t\tgfp_flags |= __GFP_ZERO;\n\n\t\tif (ttm_flags & TTM_PAGE_FLAG_NO_RETRY)\n\t\t\tgfp_flags |= __GFP_RETRY_MAYFAIL;\n\n\t\t/* ttm_alloc_new_pages doesn't reference pool so we can run\n\t\t * multiple requests in parallel.\n\t\t **/\n\t\tr = ttm_alloc_new_pages(pages, gfp_flags, ttm_flags, cstate,\n\t\t\t\t\tcount, order);\n\t}\n\n\treturn r;\n}\n\n/* Put all pages in pages list to correct pool to wait for reuse */\nstatic void ttm_put_pages(struct page **pages, unsigned npages, int flags,\n\t\t\t  enum ttm_caching_state cstate)\n{\n\tstruct ttm_page_pool *pool = ttm_get_pool(flags, false, cstate);\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tstruct ttm_page_pool *huge = ttm_get_pool(flags, true, cstate);\n#endif\n\tunsigned long irq_flags;\n\tunsigned i;\n\n\tif (pool == NULL) {\n\t\t/* No pool for this memory type so free the pages */\n\t\ti = 0;\n\t\twhile (i < npages) {\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tstruct page *p = pages[i];\n#endif\n\t\t\tunsigned order = 0, j;\n\n\t\t\tif (!pages[i]) {\n\t\t\t\t++i;\n\t\t\t\tcontinue;\n\t\t\t}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tif (!(flags & TTM_PAGE_FLAG_DMA32) &&\n\t\t\t    (npages - i) >= HPAGE_PMD_NR) {\n\t\t\t\tfor (j = 1; j < HPAGE_PMD_NR; ++j)\n\t\t\t\t\tif (p++ != pages[i + j])\n\t\t\t\t\t    break;\n\n\t\t\t\tif (j == HPAGE_PMD_NR)\n\t\t\t\t\torder = HPAGE_PMD_ORDER;\n\t\t\t}\n#endif\n\n\t\t\tif (page_count(pages[i]) != 1)\n\t\t\t\tpr_err(\"Erroneous page count. Leaking pages.\\n\");\n\t\t\t__free_pages(pages[i], order);\n\n\t\t\tj = 1 << order;\n\t\t\twhile (j) {\n\t\t\t\tpages[i++] = NULL;\n\t\t\t\t--j;\n\t\t\t}\n\t\t}\n\t\treturn;\n\t}\n\n\ti = 0;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tif (huge) {\n\t\tunsigned max_size, n2free;\n\n\t\tspin_lock_irqsave(&huge->lock, irq_flags);\n\t\twhile ((npages - i) >= HPAGE_PMD_NR) {\n\t\t\tstruct page *p = pages[i];\n\t\t\tunsigned j;\n\n\t\t\tif (!p)\n\t\t\t\tbreak;\n\n\t\t\tfor (j = 1; j < HPAGE_PMD_NR; ++j)\n\t\t\t\tif (p++ != pages[i + j])\n\t\t\t\t    break;\n\n\t\t\tif (j != HPAGE_PMD_NR)\n\t\t\t\tbreak;\n\n\t\t\tlist_add_tail(&pages[i]->lru, &huge->list);\n\n\t\t\tfor (j = 0; j < HPAGE_PMD_NR; ++j)\n\t\t\t\tpages[i++] = NULL;\n\t\t\thuge->npages++;\n\t\t}\n\n\t\t/* Check that we don't go over the pool limit */\n\t\tmax_size = _manager->options.max_size;\n\t\tmax_size /= HPAGE_PMD_NR;\n\t\tif (huge->npages > max_size)\n\t\t\tn2free = huge->npages - max_size;\n\t\telse\n\t\t\tn2free = 0;\n\t\tspin_unlock_irqrestore(&huge->lock, irq_flags);\n\t\tif (n2free)\n\t\t\tttm_page_pool_free(huge, n2free, false);\n\t}\n#endif\n\n\tspin_lock_irqsave(&pool->lock, irq_flags);\n\twhile (i < npages) {\n\t\tif (pages[i]) {\n\t\t\tif (page_count(pages[i]) != 1)\n\t\t\t\tpr_err(\"Erroneous page count. Leaking pages.\\n\");\n\t\t\tlist_add_tail(&pages[i]->lru, &pool->list);\n\t\t\tpages[i] = NULL;\n\t\t\tpool->npages++;\n\t\t}\n\t\t++i;\n\t}\n\t/* Check that we don't go over the pool limit */\n\tnpages = 0;\n\tif (pool->npages > _manager->options.max_size) {\n\t\tnpages = pool->npages - _manager->options.max_size;\n\t\t/* free at least NUM_PAGES_TO_ALLOC number of pages\n\t\t * to reduce calls to set_memory_wb */\n\t\tif (npages < NUM_PAGES_TO_ALLOC)\n\t\t\tnpages = NUM_PAGES_TO_ALLOC;\n\t}\n\tspin_unlock_irqrestore(&pool->lock, irq_flags);\n\tif (npages)\n\t\tttm_page_pool_free(pool, npages, false);\n}\n\n/*\n * On success pages list will hold count number of correctly\n * cached pages.\n */\nstatic int ttm_get_pages(struct page **pages, unsigned npages, int flags,\n\t\t\t enum ttm_caching_state cstate)\n{\n\tstruct ttm_page_pool *pool = ttm_get_pool(flags, false, cstate);\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tstruct ttm_page_pool *huge = ttm_get_pool(flags, true, cstate);\n#endif\n\tstruct list_head plist;\n\tstruct page *p = NULL;\n\tunsigned count, first;\n\tint r;\n\n\t/* No pool for cached pages */\n\tif (pool == NULL) {\n\t\tgfp_t gfp_flags = GFP_USER;\n\t\tunsigned i;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\tunsigned j;\n#endif\n\n\t\t/* set zero flag for page allocation if required */\n\t\tif (flags & TTM_PAGE_FLAG_ZERO_ALLOC)\n\t\t\tgfp_flags |= __GFP_ZERO;\n\n\t\tif (flags & TTM_PAGE_FLAG_NO_RETRY)\n\t\t\tgfp_flags |= __GFP_RETRY_MAYFAIL;\n\n\t\tif (flags & TTM_PAGE_FLAG_DMA32)\n\t\t\tgfp_flags |= GFP_DMA32;\n\t\telse\n\t\t\tgfp_flags |= GFP_HIGHUSER;\n\n\t\ti = 0;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\tif (!(gfp_flags & GFP_DMA32)) {\n\t\t\twhile (npages >= HPAGE_PMD_NR) {\n\t\t\t\tgfp_t huge_flags = gfp_flags;\n\n\t\t\t\thuge_flags |= GFP_TRANSHUGE_LIGHT | __GFP_NORETRY |\n\t\t\t\t\t__GFP_KSWAPD_RECLAIM;\n\t\t\t\thuge_flags &= ~__GFP_MOVABLE;\n\t\t\t\thuge_flags &= ~__GFP_COMP;\n\t\t\t\tp = alloc_pages(huge_flags, HPAGE_PMD_ORDER);\n\t\t\t\tif (!p)\n\t\t\t\t\tbreak;\n\n\t\t\t\tfor (j = 0; j < HPAGE_PMD_NR; ++j)\n\t\t\t\t\tpages[i++] = p++;\n\n\t\t\t\tnpages -= HPAGE_PMD_NR;\n\t\t\t}\n\t\t}\n#endif\n\n\t\tfirst = i;\n\t\twhile (npages) {\n\t\t\tp = alloc_page(gfp_flags);\n\t\t\tif (!p) {\n\t\t\t\tpr_debug(\"Unable to allocate page\\n\");\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\t/* Swap the pages if we detect consecutive order */\n\t\t\tif (i > first && pages[i - 1] == p - 1)\n\t\t\t\tswap(p, pages[i - 1]);\n\n\t\t\tpages[i++] = p;\n\t\t\t--npages;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tcount = 0;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tif (huge && npages >= HPAGE_PMD_NR) {\n\t\tINIT_LIST_HEAD(&plist);\n\t\tttm_page_pool_get_pages(huge, &plist, flags, cstate,\n\t\t\t\t\tnpages / HPAGE_PMD_NR,\n\t\t\t\t\tHPAGE_PMD_ORDER);\n\n\t\tlist_for_each_entry(p, &plist, lru) {\n\t\t\tunsigned j;\n\n\t\t\tfor (j = 0; j < HPAGE_PMD_NR; ++j)\n\t\t\t\tpages[count++] = &p[j];\n\t\t}\n\t}\n#endif\n\n\tINIT_LIST_HEAD(&plist);\n\tr = ttm_page_pool_get_pages(pool, &plist, flags, cstate,\n\t\t\t\t    npages - count, 0);\n\n\tfirst = count;\n\tlist_for_each_entry(p, &plist, lru) {\n\t\tstruct page *tmp = p;\n\n\t\t/* Swap the pages if we detect consecutive order */\n\t\tif (count > first && pages[count - 1] == tmp - 1)\n\t\t\tswap(tmp, pages[count - 1]);\n\t\tpages[count++] = tmp;\n\t}\n\n\tif (r) {\n\t\t/* If there is any pages in the list put them back to\n\t\t * the pool.\n\t\t */\n\t\tpr_debug(\"Failed to allocate extra pages for large request\\n\");\n\t\tttm_put_pages(pages, count, flags, cstate);\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic void ttm_page_pool_init_locked(struct ttm_page_pool *pool, gfp_t flags,\n\t\tchar *name, unsigned int order)\n{\n\tspin_lock_init(&pool->lock);\n\tpool->fill_lock = false;\n\tINIT_LIST_HEAD(&pool->list);\n\tpool->npages = pool->nfrees = 0;\n\tpool->gfp_flags = flags;\n\tpool->name = name;\n\tpool->order = order;\n}\n\nint ttm_page_alloc_init(struct ttm_mem_global *glob, unsigned max_pages)\n{\n\tint ret;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tunsigned order = HPAGE_PMD_ORDER;\n#else\n\tunsigned order = 0;\n#endif\n\n\tWARN_ON(_manager);\n\n\tpr_info(\"Initializing pool allocator\\n\");\n\n\t_manager = kzalloc(sizeof(*_manager), GFP_KERNEL);\n\tif (!_manager)\n\t\treturn -ENOMEM;\n\n\tttm_page_pool_init_locked(&_manager->wc_pool, GFP_HIGHUSER, \"wc\", 0);\n\n\tttm_page_pool_init_locked(&_manager->uc_pool, GFP_HIGHUSER, \"uc\", 0);\n\n\tttm_page_pool_init_locked(&_manager->wc_pool_dma32,\n\t\t\t\t  GFP_USER | GFP_DMA32, \"wc dma\", 0);\n\n\tttm_page_pool_init_locked(&_manager->uc_pool_dma32,\n\t\t\t\t  GFP_USER | GFP_DMA32, \"uc dma\", 0);\n\n\tttm_page_pool_init_locked(&_manager->wc_pool_huge,\n\t\t\t\t  (GFP_TRANSHUGE_LIGHT | __GFP_NORETRY |\n\t\t\t\t   __GFP_KSWAPD_RECLAIM) &\n\t\t\t\t  ~(__GFP_MOVABLE | __GFP_COMP),\n\t\t\t\t  \"wc huge\", order);\n\n\tttm_page_pool_init_locked(&_manager->uc_pool_huge,\n\t\t\t\t  (GFP_TRANSHUGE_LIGHT | __GFP_NORETRY |\n\t\t\t\t   __GFP_KSWAPD_RECLAIM) &\n\t\t\t\t  ~(__GFP_MOVABLE | __GFP_COMP)\n\t\t\t\t  , \"uc huge\", order);\n\n\t_manager->options.max_size = max_pages;\n\t_manager->options.small = SMALL_ALLOCATION;\n\t_manager->options.alloc_size = NUM_PAGES_TO_ALLOC;\n\n\tret = kobject_init_and_add(&_manager->kobj, &ttm_pool_kobj_type,\n\t\t\t\t   &glob->kobj, \"pool\");\n\tif (unlikely(ret != 0))\n\t\tgoto error;\n\n\tret = ttm_pool_mm_shrink_init(_manager);\n\tif (unlikely(ret != 0))\n\t\tgoto error;\n\treturn 0;\n\nerror:\n\tkobject_put(&_manager->kobj);\n\t_manager = NULL;\n\treturn ret;\n}\n\nvoid ttm_page_alloc_fini(void)\n{\n\tint i;\n\n\tpr_info(\"Finalizing pool allocator\\n\");\n\tttm_pool_mm_shrink_fini(_manager);\n\n\t/* OK to use static buffer since global mutex is no longer used. */\n\tfor (i = 0; i < NUM_POOLS; ++i)\n\t\tttm_page_pool_free(&_manager->pools[i], FREE_ALL_PAGES, true);\n\n\tkobject_put(&_manager->kobj);\n\t_manager = NULL;\n}\n\nstatic void\nttm_pool_unpopulate_helper(struct ttm_tt *ttm, unsigned mem_count_update)\n{\n\tstruct ttm_mem_global *mem_glob = ttm->bdev->glob->mem_glob;\n\tunsigned i;\n\n\tif (mem_count_update == 0)\n\t\tgoto put_pages;\n\n\tfor (i = 0; i < mem_count_update; ++i) {\n\t\tif (!ttm->pages[i])\n\t\t\tcontinue;\n\n\t\tttm_mem_global_free_page(mem_glob, ttm->pages[i], PAGE_SIZE);\n\t}\n\nput_pages:\n\tttm_put_pages(ttm->pages, ttm->num_pages, ttm->page_flags,\n\t\t      ttm->caching_state);\n\tttm->state = tt_unpopulated;\n}\n\nint ttm_pool_populate(struct ttm_tt *ttm, struct ttm_operation_ctx *ctx)\n{\n\tstruct ttm_mem_global *mem_glob = ttm->bdev->glob->mem_glob;\n\tunsigned i;\n\tint ret;\n\n\tif (ttm->state != tt_unpopulated)\n\t\treturn 0;\n\n\tif (ttm_check_under_lowerlimit(mem_glob, ttm->num_pages, ctx))\n\t\treturn -ENOMEM;\n\n\tret = ttm_get_pages(ttm->pages, ttm->num_pages, ttm->page_flags,\n\t\t\t    ttm->caching_state);\n\tif (unlikely(ret != 0)) {\n\t\tttm_pool_unpopulate_helper(ttm, 0);\n\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < ttm->num_pages; ++i) {\n\t\tret = ttm_mem_global_alloc_page(mem_glob, ttm->pages[i],\n\t\t\t\t\t\tPAGE_SIZE, ctx);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tttm_pool_unpopulate_helper(ttm, i);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tif (unlikely(ttm->page_flags & TTM_PAGE_FLAG_SWAPPED)) {\n\t\tret = ttm_tt_swapin(ttm);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tttm_pool_unpopulate(ttm);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tttm->state = tt_unbound;\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_pool_populate);\n\nvoid ttm_pool_unpopulate(struct ttm_tt *ttm)\n{\n\tttm_pool_unpopulate_helper(ttm, ttm->num_pages);\n}\nEXPORT_SYMBOL(ttm_pool_unpopulate);\n\nint ttm_populate_and_map_pages(struct device *dev, struct ttm_dma_tt *tt,\n\t\t\t\t\tstruct ttm_operation_ctx *ctx)\n{\n\tunsigned i, j;\n\tint r;\n\n\tr = ttm_pool_populate(&tt->ttm, ctx);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < tt->ttm.num_pages; ++i) {\n\t\tstruct page *p = tt->ttm.pages[i];\n\t\tsize_t num_pages = 1;\n\n\t\tfor (j = i + 1; j < tt->ttm.num_pages; ++j) {\n\t\t\tif (++p != tt->ttm.pages[j])\n\t\t\t\tbreak;\n\n\t\t\t++num_pages;\n\t\t}\n\n\t\ttt->dma_address[i] = dma_map_page(dev, tt->ttm.pages[i],\n\t\t\t\t\t\t  0, num_pages * PAGE_SIZE,\n\t\t\t\t\t\t  DMA_BIDIRECTIONAL);\n\t\tif (dma_mapping_error(dev, tt->dma_address[i])) {\n\t\t\twhile (i--) {\n\t\t\t\tdma_unmap_page(dev, tt->dma_address[i],\n\t\t\t\t\t       PAGE_SIZE, DMA_BIDIRECTIONAL);\n\t\t\t\ttt->dma_address[i] = 0;\n\t\t\t}\n\t\t\tttm_pool_unpopulate(&tt->ttm);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tfor (j = 1; j < num_pages; ++j) {\n\t\t\ttt->dma_address[i + 1] = tt->dma_address[i] + PAGE_SIZE;\n\t\t\t++i;\n\t\t}\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_populate_and_map_pages);\n\nvoid ttm_unmap_and_unpopulate_pages(struct device *dev, struct ttm_dma_tt *tt)\n{\n\tunsigned i, j;\n\n\tfor (i = 0; i < tt->ttm.num_pages;) {\n\t\tstruct page *p = tt->ttm.pages[i];\n\t\tsize_t num_pages = 1;\n\n\t\tif (!tt->dma_address[i] || !tt->ttm.pages[i]) {\n\t\t\t++i;\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor (j = i + 1; j < tt->ttm.num_pages; ++j) {\n\t\t\tif (++p != tt->ttm.pages[j])\n\t\t\t\tbreak;\n\n\t\t\t++num_pages;\n\t\t}\n\n\t\tdma_unmap_page(dev, tt->dma_address[i], num_pages * PAGE_SIZE,\n\t\t\t       DMA_BIDIRECTIONAL);\n\n\t\ti += num_pages;\n\t}\n\tttm_pool_unpopulate(&tt->ttm);\n}\nEXPORT_SYMBOL(ttm_unmap_and_unpopulate_pages);\n\nint ttm_page_alloc_debugfs(struct seq_file *m, void *data)\n{\n\tstruct ttm_page_pool *p;\n\tunsigned i;\n\tchar *h[] = {\"pool\", \"refills\", \"pages freed\", \"size\"};\n\tif (!_manager) {\n\t\tseq_printf(m, \"No pool allocator running.\\n\");\n\t\treturn 0;\n\t}\n\tseq_printf(m, \"%7s %12s %13s %8s\\n\",\n\t\t\th[0], h[1], h[2], h[3]);\n\tfor (i = 0; i < NUM_POOLS; ++i) {\n\t\tp = &_manager->pools[i];\n\n\t\tseq_printf(m, \"%7s %12ld %13ld %8d\\n\",\n\t\t\t\tp->name, p->nrefills,\n\t\t\t\tp->nfrees, p->npages);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_page_alloc_debugfs);\n"], "fixing_code": ["/*\n * Copyright (c) Red Hat Inc.\n\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sub license,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice (including the\n * next paragraph) shall be included in all copies or substantial portions\n * of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL\n * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n *\n * Authors: Dave Airlie <airlied@redhat.com>\n *          Jerome Glisse <jglisse@redhat.com>\n *          Pauli Nieminen <suokkos@gmail.com>\n */\n\n/* simple list based uncached page pool\n * - Pool collects resently freed pages for reuse\n * - Use page->lru to keep a free list\n * - doesn't track currently in use pages\n */\n\n#define pr_fmt(fmt) \"[TTM] \" fmt\n\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/highmem.h>\n#include <linux/mm_types.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/seq_file.h> /* for seq_printf */\n#include <linux/slab.h>\n#include <linux/dma-mapping.h>\n\n#include <linux/atomic.h>\n\n#include <drm/ttm/ttm_bo_driver.h>\n#include <drm/ttm/ttm_page_alloc.h>\n#include <drm/ttm/ttm_set_memory.h>\n\n#define NUM_PAGES_TO_ALLOC\t\t(PAGE_SIZE/sizeof(struct page *))\n#define SMALL_ALLOCATION\t\t16\n#define FREE_ALL_PAGES\t\t\t(~0U)\n/* times are in msecs */\n#define PAGE_FREE_INTERVAL\t\t1000\n\n/**\n * struct ttm_page_pool - Pool to reuse recently allocated uc/wc pages.\n *\n * @lock: Protects the shared pool from concurrnet access. Must be used with\n * irqsave/irqrestore variants because pool allocator maybe called from\n * delayed work.\n * @fill_lock: Prevent concurrent calls to fill.\n * @list: Pool of free uc/wc pages for fast reuse.\n * @gfp_flags: Flags to pass for alloc_page.\n * @npages: Number of pages in pool.\n */\nstruct ttm_page_pool {\n\tspinlock_t\t\tlock;\n\tbool\t\t\tfill_lock;\n\tstruct list_head\tlist;\n\tgfp_t\t\t\tgfp_flags;\n\tunsigned\t\tnpages;\n\tchar\t\t\t*name;\n\tunsigned long\t\tnfrees;\n\tunsigned long\t\tnrefills;\n\tunsigned int\t\torder;\n};\n\n/**\n * Limits for the pool. They are handled without locks because only place where\n * they may change is in sysfs store. They won't have immediate effect anyway\n * so forcing serialization to access them is pointless.\n */\n\nstruct ttm_pool_opts {\n\tunsigned\talloc_size;\n\tunsigned\tmax_size;\n\tunsigned\tsmall;\n};\n\n#define NUM_POOLS 6\n\n/**\n * struct ttm_pool_manager - Holds memory pools for fst allocation\n *\n * Manager is read only object for pool code so it doesn't need locking.\n *\n * @free_interval: minimum number of jiffies between freeing pages from pool.\n * @page_alloc_inited: reference counting for pool allocation.\n * @work: Work that is used to shrink the pool. Work is only run when there is\n * some pages to free.\n * @small_allocation: Limit in number of pages what is small allocation.\n *\n * @pools: All pool objects in use.\n **/\nstruct ttm_pool_manager {\n\tstruct kobject\t\tkobj;\n\tstruct shrinker\t\tmm_shrink;\n\tstruct ttm_pool_opts\toptions;\n\n\tunion {\n\t\tstruct ttm_page_pool\tpools[NUM_POOLS];\n\t\tstruct {\n\t\t\tstruct ttm_page_pool\twc_pool;\n\t\t\tstruct ttm_page_pool\tuc_pool;\n\t\t\tstruct ttm_page_pool\twc_pool_dma32;\n\t\t\tstruct ttm_page_pool\tuc_pool_dma32;\n\t\t\tstruct ttm_page_pool\twc_pool_huge;\n\t\t\tstruct ttm_page_pool\tuc_pool_huge;\n\t\t} ;\n\t};\n};\n\nstatic struct attribute ttm_page_pool_max = {\n\t.name = \"pool_max_size\",\n\t.mode = S_IRUGO | S_IWUSR\n};\nstatic struct attribute ttm_page_pool_small = {\n\t.name = \"pool_small_allocation\",\n\t.mode = S_IRUGO | S_IWUSR\n};\nstatic struct attribute ttm_page_pool_alloc_size = {\n\t.name = \"pool_allocation_size\",\n\t.mode = S_IRUGO | S_IWUSR\n};\n\nstatic struct attribute *ttm_pool_attrs[] = {\n\t&ttm_page_pool_max,\n\t&ttm_page_pool_small,\n\t&ttm_page_pool_alloc_size,\n\tNULL\n};\n\nstatic void ttm_pool_kobj_release(struct kobject *kobj)\n{\n\tstruct ttm_pool_manager *m =\n\t\tcontainer_of(kobj, struct ttm_pool_manager, kobj);\n\tkfree(m);\n}\n\nstatic ssize_t ttm_pool_store(struct kobject *kobj,\n\t\tstruct attribute *attr, const char *buffer, size_t size)\n{\n\tstruct ttm_pool_manager *m =\n\t\tcontainer_of(kobj, struct ttm_pool_manager, kobj);\n\tint chars;\n\tunsigned val;\n\tchars = sscanf(buffer, \"%u\", &val);\n\tif (chars == 0)\n\t\treturn size;\n\n\t/* Convert kb to number of pages */\n\tval = val / (PAGE_SIZE >> 10);\n\n\tif (attr == &ttm_page_pool_max)\n\t\tm->options.max_size = val;\n\telse if (attr == &ttm_page_pool_small)\n\t\tm->options.small = val;\n\telse if (attr == &ttm_page_pool_alloc_size) {\n\t\tif (val > NUM_PAGES_TO_ALLOC*8) {\n\t\t\tpr_err(\"Setting allocation size to %lu is not allowed. Recommended size is %lu\\n\",\n\t\t\t       NUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 7),\n\t\t\t       NUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 10));\n\t\t\treturn size;\n\t\t} else if (val > NUM_PAGES_TO_ALLOC) {\n\t\t\tpr_warn(\"Setting allocation size to larger than %lu is not recommended\\n\",\n\t\t\t\tNUM_PAGES_TO_ALLOC*(PAGE_SIZE >> 10));\n\t\t}\n\t\tm->options.alloc_size = val;\n\t}\n\n\treturn size;\n}\n\nstatic ssize_t ttm_pool_show(struct kobject *kobj,\n\t\tstruct attribute *attr, char *buffer)\n{\n\tstruct ttm_pool_manager *m =\n\t\tcontainer_of(kobj, struct ttm_pool_manager, kobj);\n\tunsigned val = 0;\n\n\tif (attr == &ttm_page_pool_max)\n\t\tval = m->options.max_size;\n\telse if (attr == &ttm_page_pool_small)\n\t\tval = m->options.small;\n\telse if (attr == &ttm_page_pool_alloc_size)\n\t\tval = m->options.alloc_size;\n\n\tval = val * (PAGE_SIZE >> 10);\n\n\treturn snprintf(buffer, PAGE_SIZE, \"%u\\n\", val);\n}\n\nstatic const struct sysfs_ops ttm_pool_sysfs_ops = {\n\t.show = &ttm_pool_show,\n\t.store = &ttm_pool_store,\n};\n\nstatic struct kobj_type ttm_pool_kobj_type = {\n\t.release = &ttm_pool_kobj_release,\n\t.sysfs_ops = &ttm_pool_sysfs_ops,\n\t.default_attrs = ttm_pool_attrs,\n};\n\nstatic struct ttm_pool_manager *_manager;\n\n/**\n * Select the right pool or requested caching state and ttm flags. */\nstatic struct ttm_page_pool *ttm_get_pool(int flags, bool huge,\n\t\t\t\t\t  enum ttm_caching_state cstate)\n{\n\tint pool_index;\n\n\tif (cstate == tt_cached)\n\t\treturn NULL;\n\n\tif (cstate == tt_wc)\n\t\tpool_index = 0x0;\n\telse\n\t\tpool_index = 0x1;\n\n\tif (flags & TTM_PAGE_FLAG_DMA32) {\n\t\tif (huge)\n\t\t\treturn NULL;\n\t\tpool_index |= 0x2;\n\n\t} else if (huge) {\n\t\tpool_index |= 0x4;\n\t}\n\n\treturn &_manager->pools[pool_index];\n}\n\n/* set memory back to wb and free the pages. */\nstatic void ttm_pages_put(struct page *pages[], unsigned npages,\n\t\tunsigned int order)\n{\n\tunsigned int i, pages_nr = (1 << order);\n\n\tif (order == 0) {\n\t\tif (ttm_set_pages_array_wb(pages, npages))\n\t\t\tpr_err(\"Failed to set %d pages to wb!\\n\", npages);\n\t}\n\n\tfor (i = 0; i < npages; ++i) {\n\t\tif (order > 0) {\n\t\t\tif (ttm_set_pages_wb(pages[i], pages_nr))\n\t\t\t\tpr_err(\"Failed to set %d pages to wb!\\n\", pages_nr);\n\t\t}\n\t\t__free_pages(pages[i], order);\n\t}\n}\n\nstatic void ttm_pool_update_free_locked(struct ttm_page_pool *pool,\n\t\tunsigned freed_pages)\n{\n\tpool->npages -= freed_pages;\n\tpool->nfrees += freed_pages;\n}\n\n/**\n * Free pages from pool.\n *\n * To prevent hogging the ttm_swap process we only free NUM_PAGES_TO_ALLOC\n * number of pages in one go.\n *\n * @pool: to free the pages from\n * @free_all: If set to true will free all pages in pool\n * @use_static: Safe to use static buffer\n **/\nstatic int ttm_page_pool_free(struct ttm_page_pool *pool, unsigned nr_free,\n\t\t\t      bool use_static)\n{\n\tstatic struct page *static_buf[NUM_PAGES_TO_ALLOC];\n\tunsigned long irq_flags;\n\tstruct page *p;\n\tstruct page **pages_to_free;\n\tunsigned freed_pages = 0,\n\t\t npages_to_free = nr_free;\n\n\tif (NUM_PAGES_TO_ALLOC < nr_free)\n\t\tnpages_to_free = NUM_PAGES_TO_ALLOC;\n\n\tif (use_static)\n\t\tpages_to_free = static_buf;\n\telse\n\t\tpages_to_free = kmalloc_array(npages_to_free,\n\t\t\t\t\t      sizeof(struct page *),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!pages_to_free) {\n\t\tpr_debug(\"Failed to allocate memory for pool free operation\\n\");\n\t\treturn 0;\n\t}\n\nrestart:\n\tspin_lock_irqsave(&pool->lock, irq_flags);\n\n\tlist_for_each_entry_reverse(p, &pool->list, lru) {\n\t\tif (freed_pages >= npages_to_free)\n\t\t\tbreak;\n\n\t\tpages_to_free[freed_pages++] = p;\n\t\t/* We can only remove NUM_PAGES_TO_ALLOC at a time. */\n\t\tif (freed_pages >= NUM_PAGES_TO_ALLOC) {\n\t\t\t/* remove range of pages from the pool */\n\t\t\t__list_del(p->lru.prev, &pool->list);\n\n\t\t\tttm_pool_update_free_locked(pool, freed_pages);\n\t\t\t/**\n\t\t\t * Because changing page caching is costly\n\t\t\t * we unlock the pool to prevent stalling.\n\t\t\t */\n\t\t\tspin_unlock_irqrestore(&pool->lock, irq_flags);\n\n\t\t\tttm_pages_put(pages_to_free, freed_pages, pool->order);\n\t\t\tif (likely(nr_free != FREE_ALL_PAGES))\n\t\t\t\tnr_free -= freed_pages;\n\n\t\t\tif (NUM_PAGES_TO_ALLOC >= nr_free)\n\t\t\t\tnpages_to_free = nr_free;\n\t\t\telse\n\t\t\t\tnpages_to_free = NUM_PAGES_TO_ALLOC;\n\n\t\t\tfreed_pages = 0;\n\n\t\t\t/* free all so restart the processing */\n\t\t\tif (nr_free)\n\t\t\t\tgoto restart;\n\n\t\t\t/* Not allowed to fall through or break because\n\t\t\t * following context is inside spinlock while we are\n\t\t\t * outside here.\n\t\t\t */\n\t\t\tgoto out;\n\n\t\t}\n\t}\n\n\t/* remove range of pages from the pool */\n\tif (freed_pages) {\n\t\t__list_del(&p->lru, &pool->list);\n\n\t\tttm_pool_update_free_locked(pool, freed_pages);\n\t\tnr_free -= freed_pages;\n\t}\n\n\tspin_unlock_irqrestore(&pool->lock, irq_flags);\n\n\tif (freed_pages)\n\t\tttm_pages_put(pages_to_free, freed_pages, pool->order);\nout:\n\tif (pages_to_free != static_buf)\n\t\tkfree(pages_to_free);\n\treturn nr_free;\n}\n\n/**\n * Callback for mm to request pool to reduce number of page held.\n *\n * XXX: (dchinner) Deadlock warning!\n *\n * This code is crying out for a shrinker per pool....\n */\nstatic unsigned long\nttm_pool_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tstatic DEFINE_MUTEX(lock);\n\tstatic unsigned start_pool;\n\tunsigned i;\n\tunsigned pool_offset;\n\tstruct ttm_page_pool *pool;\n\tint shrink_pages = sc->nr_to_scan;\n\tunsigned long freed = 0;\n\tunsigned int nr_free_pool;\n\n\tif (!mutex_trylock(&lock))\n\t\treturn SHRINK_STOP;\n\tpool_offset = ++start_pool % NUM_POOLS;\n\t/* select start pool in round robin fashion */\n\tfor (i = 0; i < NUM_POOLS; ++i) {\n\t\tunsigned nr_free = shrink_pages;\n\t\tunsigned page_nr;\n\n\t\tif (shrink_pages == 0)\n\t\t\tbreak;\n\n\t\tpool = &_manager->pools[(i + pool_offset)%NUM_POOLS];\n\t\tpage_nr = (1 << pool->order);\n\t\t/* OK to use static buffer since global mutex is held. */\n\t\tnr_free_pool = roundup(nr_free, page_nr) >> pool->order;\n\t\tshrink_pages = ttm_page_pool_free(pool, nr_free_pool, true);\n\t\tfreed += (nr_free_pool - shrink_pages) << pool->order;\n\t\tif (freed >= sc->nr_to_scan)\n\t\t\tbreak;\n\t\tshrink_pages <<= pool->order;\n\t}\n\tmutex_unlock(&lock);\n\treturn freed;\n}\n\n\nstatic unsigned long\nttm_pool_shrink_count(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tunsigned i;\n\tunsigned long count = 0;\n\tstruct ttm_page_pool *pool;\n\n\tfor (i = 0; i < NUM_POOLS; ++i) {\n\t\tpool = &_manager->pools[i];\n\t\tcount += (pool->npages << pool->order);\n\t}\n\n\treturn count;\n}\n\nstatic int ttm_pool_mm_shrink_init(struct ttm_pool_manager *manager)\n{\n\tmanager->mm_shrink.count_objects = ttm_pool_shrink_count;\n\tmanager->mm_shrink.scan_objects = ttm_pool_shrink_scan;\n\tmanager->mm_shrink.seeks = 1;\n\treturn register_shrinker(&manager->mm_shrink);\n}\n\nstatic void ttm_pool_mm_shrink_fini(struct ttm_pool_manager *manager)\n{\n\tunregister_shrinker(&manager->mm_shrink);\n}\n\nstatic int ttm_set_pages_caching(struct page **pages,\n\t\tenum ttm_caching_state cstate, unsigned cpages)\n{\n\tint r = 0;\n\t/* Set page caching */\n\tswitch (cstate) {\n\tcase tt_uncached:\n\t\tr = ttm_set_pages_array_uc(pages, cpages);\n\t\tif (r)\n\t\t\tpr_err(\"Failed to set %d pages to uc!\\n\", cpages);\n\t\tbreak;\n\tcase tt_wc:\n\t\tr = ttm_set_pages_array_wc(pages, cpages);\n\t\tif (r)\n\t\t\tpr_err(\"Failed to set %d pages to wc!\\n\", cpages);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn r;\n}\n\n/**\n * Free pages the pages that failed to change the caching state. If there is\n * any pages that have changed their caching state already put them to the\n * pool.\n */\nstatic void ttm_handle_caching_state_failure(struct list_head *pages,\n\t\tint ttm_flags, enum ttm_caching_state cstate,\n\t\tstruct page **failed_pages, unsigned cpages)\n{\n\tunsigned i;\n\t/* Failed pages have to be freed */\n\tfor (i = 0; i < cpages; ++i) {\n\t\tlist_del(&failed_pages[i]->lru);\n\t\t__free_page(failed_pages[i]);\n\t}\n}\n\n/**\n * Allocate new pages with correct caching.\n *\n * This function is reentrant if caller updates count depending on number of\n * pages returned in pages array.\n */\nstatic int ttm_alloc_new_pages(struct list_head *pages, gfp_t gfp_flags,\n\t\t\t       int ttm_flags, enum ttm_caching_state cstate,\n\t\t\t       unsigned count, unsigned order)\n{\n\tstruct page **caching_array;\n\tstruct page *p;\n\tint r = 0;\n\tunsigned i, j, cpages;\n\tunsigned npages = 1 << order;\n\tunsigned max_cpages = min(count << order, (unsigned)NUM_PAGES_TO_ALLOC);\n\n\t/* allocate array for page caching change */\n\tcaching_array = kmalloc_array(max_cpages, sizeof(struct page *),\n\t\t\t\t      GFP_KERNEL);\n\n\tif (!caching_array) {\n\t\tpr_debug(\"Unable to allocate table for new pages\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0, cpages = 0; i < count; ++i) {\n\t\tp = alloc_pages(gfp_flags, order);\n\n\t\tif (!p) {\n\t\t\tpr_debug(\"Unable to get page %u\\n\", i);\n\n\t\t\t/* store already allocated pages in the pool after\n\t\t\t * setting the caching state */\n\t\t\tif (cpages) {\n\t\t\t\tr = ttm_set_pages_caching(caching_array,\n\t\t\t\t\t\t\t  cstate, cpages);\n\t\t\t\tif (r)\n\t\t\t\t\tttm_handle_caching_state_failure(pages,\n\t\t\t\t\t\tttm_flags, cstate,\n\t\t\t\t\t\tcaching_array, cpages);\n\t\t\t}\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tlist_add(&p->lru, pages);\n\n#ifdef CONFIG_HIGHMEM\n\t\t/* gfp flags of highmem page should never be dma32 so we\n\t\t * we should be fine in such case\n\t\t */\n\t\tif (PageHighMem(p))\n\t\t\tcontinue;\n\n#endif\n\t\tfor (j = 0; j < npages; ++j) {\n\t\t\tcaching_array[cpages++] = p++;\n\t\t\tif (cpages == max_cpages) {\n\n\t\t\t\tr = ttm_set_pages_caching(caching_array,\n\t\t\t\t\t\tcstate, cpages);\n\t\t\t\tif (r) {\n\t\t\t\t\tttm_handle_caching_state_failure(pages,\n\t\t\t\t\t\tttm_flags, cstate,\n\t\t\t\t\t\tcaching_array, cpages);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tcpages = 0;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (cpages) {\n\t\tr = ttm_set_pages_caching(caching_array, cstate, cpages);\n\t\tif (r)\n\t\t\tttm_handle_caching_state_failure(pages,\n\t\t\t\t\tttm_flags, cstate,\n\t\t\t\t\tcaching_array, cpages);\n\t}\nout:\n\tkfree(caching_array);\n\n\treturn r;\n}\n\n/**\n * Fill the given pool if there aren't enough pages and the requested number of\n * pages is small.\n */\nstatic void ttm_page_pool_fill_locked(struct ttm_page_pool *pool, int ttm_flags,\n\t\t\t\t      enum ttm_caching_state cstate,\n\t\t\t\t      unsigned count, unsigned long *irq_flags)\n{\n\tstruct page *p;\n\tint r;\n\tunsigned cpages = 0;\n\t/**\n\t * Only allow one pool fill operation at a time.\n\t * If pool doesn't have enough pages for the allocation new pages are\n\t * allocated from outside of pool.\n\t */\n\tif (pool->fill_lock)\n\t\treturn;\n\n\tpool->fill_lock = true;\n\n\t/* If allocation request is small and there are not enough\n\t * pages in a pool we fill the pool up first. */\n\tif (count < _manager->options.small\n\t\t&& count > pool->npages) {\n\t\tstruct list_head new_pages;\n\t\tunsigned alloc_size = _manager->options.alloc_size;\n\n\t\t/**\n\t\t * Can't change page caching if in irqsave context. We have to\n\t\t * drop the pool->lock.\n\t\t */\n\t\tspin_unlock_irqrestore(&pool->lock, *irq_flags);\n\n\t\tINIT_LIST_HEAD(&new_pages);\n\t\tr = ttm_alloc_new_pages(&new_pages, pool->gfp_flags, ttm_flags,\n\t\t\t\t\tcstate, alloc_size, 0);\n\t\tspin_lock_irqsave(&pool->lock, *irq_flags);\n\n\t\tif (!r) {\n\t\t\tlist_splice(&new_pages, &pool->list);\n\t\t\t++pool->nrefills;\n\t\t\tpool->npages += alloc_size;\n\t\t} else {\n\t\t\tpr_debug(\"Failed to fill pool (%p)\\n\", pool);\n\t\t\t/* If we have any pages left put them to the pool. */\n\t\t\tlist_for_each_entry(p, &new_pages, lru) {\n\t\t\t\t++cpages;\n\t\t\t}\n\t\t\tlist_splice(&new_pages, &pool->list);\n\t\t\tpool->npages += cpages;\n\t\t}\n\n\t}\n\tpool->fill_lock = false;\n}\n\n/**\n * Allocate pages from the pool and put them on the return list.\n *\n * @return zero for success or negative error code.\n */\nstatic int ttm_page_pool_get_pages(struct ttm_page_pool *pool,\n\t\t\t\t   struct list_head *pages,\n\t\t\t\t   int ttm_flags,\n\t\t\t\t   enum ttm_caching_state cstate,\n\t\t\t\t   unsigned count, unsigned order)\n{\n\tunsigned long irq_flags;\n\tstruct list_head *p;\n\tunsigned i;\n\tint r = 0;\n\n\tspin_lock_irqsave(&pool->lock, irq_flags);\n\tif (!order)\n\t\tttm_page_pool_fill_locked(pool, ttm_flags, cstate, count,\n\t\t\t\t\t  &irq_flags);\n\n\tif (count >= pool->npages) {\n\t\t/* take all pages from the pool */\n\t\tlist_splice_init(&pool->list, pages);\n\t\tcount -= pool->npages;\n\t\tpool->npages = 0;\n\t\tgoto out;\n\t}\n\t/* find the last pages to include for requested number of pages. Split\n\t * pool to begin and halve it to reduce search space. */\n\tif (count <= pool->npages/2) {\n\t\ti = 0;\n\t\tlist_for_each(p, &pool->list) {\n\t\t\tif (++i == count)\n\t\t\t\tbreak;\n\t\t}\n\t} else {\n\t\ti = pool->npages + 1;\n\t\tlist_for_each_prev(p, &pool->list) {\n\t\t\tif (--i == count)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\t/* Cut 'count' number of pages from the pool */\n\tlist_cut_position(pages, &pool->list, p);\n\tpool->npages -= count;\n\tcount = 0;\nout:\n\tspin_unlock_irqrestore(&pool->lock, irq_flags);\n\n\t/* clear the pages coming from the pool if requested */\n\tif (ttm_flags & TTM_PAGE_FLAG_ZERO_ALLOC) {\n\t\tstruct page *page;\n\n\t\tlist_for_each_entry(page, pages, lru) {\n\t\t\tif (PageHighMem(page))\n\t\t\t\tclear_highpage(page);\n\t\t\telse\n\t\t\t\tclear_page(page_address(page));\n\t\t}\n\t}\n\n\t/* If pool didn't have enough pages allocate new one. */\n\tif (count) {\n\t\tgfp_t gfp_flags = pool->gfp_flags;\n\n\t\t/* set zero flag for page allocation if required */\n\t\tif (ttm_flags & TTM_PAGE_FLAG_ZERO_ALLOC)\n\t\t\tgfp_flags |= __GFP_ZERO;\n\n\t\tif (ttm_flags & TTM_PAGE_FLAG_NO_RETRY)\n\t\t\tgfp_flags |= __GFP_RETRY_MAYFAIL;\n\n\t\t/* ttm_alloc_new_pages doesn't reference pool so we can run\n\t\t * multiple requests in parallel.\n\t\t **/\n\t\tr = ttm_alloc_new_pages(pages, gfp_flags, ttm_flags, cstate,\n\t\t\t\t\tcount, order);\n\t}\n\n\treturn r;\n}\n\n/* Put all pages in pages list to correct pool to wait for reuse */\nstatic void ttm_put_pages(struct page **pages, unsigned npages, int flags,\n\t\t\t  enum ttm_caching_state cstate)\n{\n\tstruct ttm_page_pool *pool = ttm_get_pool(flags, false, cstate);\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tstruct ttm_page_pool *huge = ttm_get_pool(flags, true, cstate);\n#endif\n\tunsigned long irq_flags;\n\tunsigned i;\n\n\tif (pool == NULL) {\n\t\t/* No pool for this memory type so free the pages */\n\t\ti = 0;\n\t\twhile (i < npages) {\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tstruct page *p = pages[i];\n#endif\n\t\t\tunsigned order = 0, j;\n\n\t\t\tif (!pages[i]) {\n\t\t\t\t++i;\n\t\t\t\tcontinue;\n\t\t\t}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tif (!(flags & TTM_PAGE_FLAG_DMA32) &&\n\t\t\t    (npages - i) >= HPAGE_PMD_NR) {\n\t\t\t\tfor (j = 1; j < HPAGE_PMD_NR; ++j)\n\t\t\t\t\tif (++p != pages[i + j])\n\t\t\t\t\t    break;\n\n\t\t\t\tif (j == HPAGE_PMD_NR)\n\t\t\t\t\torder = HPAGE_PMD_ORDER;\n\t\t\t}\n#endif\n\n\t\t\tif (page_count(pages[i]) != 1)\n\t\t\t\tpr_err(\"Erroneous page count. Leaking pages.\\n\");\n\t\t\t__free_pages(pages[i], order);\n\n\t\t\tj = 1 << order;\n\t\t\twhile (j) {\n\t\t\t\tpages[i++] = NULL;\n\t\t\t\t--j;\n\t\t\t}\n\t\t}\n\t\treturn;\n\t}\n\n\ti = 0;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tif (huge) {\n\t\tunsigned max_size, n2free;\n\n\t\tspin_lock_irqsave(&huge->lock, irq_flags);\n\t\twhile ((npages - i) >= HPAGE_PMD_NR) {\n\t\t\tstruct page *p = pages[i];\n\t\t\tunsigned j;\n\n\t\t\tif (!p)\n\t\t\t\tbreak;\n\n\t\t\tfor (j = 1; j < HPAGE_PMD_NR; ++j)\n\t\t\t\tif (++p != pages[i + j])\n\t\t\t\t    break;\n\n\t\t\tif (j != HPAGE_PMD_NR)\n\t\t\t\tbreak;\n\n\t\t\tlist_add_tail(&pages[i]->lru, &huge->list);\n\n\t\t\tfor (j = 0; j < HPAGE_PMD_NR; ++j)\n\t\t\t\tpages[i++] = NULL;\n\t\t\thuge->npages++;\n\t\t}\n\n\t\t/* Check that we don't go over the pool limit */\n\t\tmax_size = _manager->options.max_size;\n\t\tmax_size /= HPAGE_PMD_NR;\n\t\tif (huge->npages > max_size)\n\t\t\tn2free = huge->npages - max_size;\n\t\telse\n\t\t\tn2free = 0;\n\t\tspin_unlock_irqrestore(&huge->lock, irq_flags);\n\t\tif (n2free)\n\t\t\tttm_page_pool_free(huge, n2free, false);\n\t}\n#endif\n\n\tspin_lock_irqsave(&pool->lock, irq_flags);\n\twhile (i < npages) {\n\t\tif (pages[i]) {\n\t\t\tif (page_count(pages[i]) != 1)\n\t\t\t\tpr_err(\"Erroneous page count. Leaking pages.\\n\");\n\t\t\tlist_add_tail(&pages[i]->lru, &pool->list);\n\t\t\tpages[i] = NULL;\n\t\t\tpool->npages++;\n\t\t}\n\t\t++i;\n\t}\n\t/* Check that we don't go over the pool limit */\n\tnpages = 0;\n\tif (pool->npages > _manager->options.max_size) {\n\t\tnpages = pool->npages - _manager->options.max_size;\n\t\t/* free at least NUM_PAGES_TO_ALLOC number of pages\n\t\t * to reduce calls to set_memory_wb */\n\t\tif (npages < NUM_PAGES_TO_ALLOC)\n\t\t\tnpages = NUM_PAGES_TO_ALLOC;\n\t}\n\tspin_unlock_irqrestore(&pool->lock, irq_flags);\n\tif (npages)\n\t\tttm_page_pool_free(pool, npages, false);\n}\n\n/*\n * On success pages list will hold count number of correctly\n * cached pages.\n */\nstatic int ttm_get_pages(struct page **pages, unsigned npages, int flags,\n\t\t\t enum ttm_caching_state cstate)\n{\n\tstruct ttm_page_pool *pool = ttm_get_pool(flags, false, cstate);\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tstruct ttm_page_pool *huge = ttm_get_pool(flags, true, cstate);\n#endif\n\tstruct list_head plist;\n\tstruct page *p = NULL;\n\tunsigned count, first;\n\tint r;\n\n\t/* No pool for cached pages */\n\tif (pool == NULL) {\n\t\tgfp_t gfp_flags = GFP_USER;\n\t\tunsigned i;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\tunsigned j;\n#endif\n\n\t\t/* set zero flag for page allocation if required */\n\t\tif (flags & TTM_PAGE_FLAG_ZERO_ALLOC)\n\t\t\tgfp_flags |= __GFP_ZERO;\n\n\t\tif (flags & TTM_PAGE_FLAG_NO_RETRY)\n\t\t\tgfp_flags |= __GFP_RETRY_MAYFAIL;\n\n\t\tif (flags & TTM_PAGE_FLAG_DMA32)\n\t\t\tgfp_flags |= GFP_DMA32;\n\t\telse\n\t\t\tgfp_flags |= GFP_HIGHUSER;\n\n\t\ti = 0;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\tif (!(gfp_flags & GFP_DMA32)) {\n\t\t\twhile (npages >= HPAGE_PMD_NR) {\n\t\t\t\tgfp_t huge_flags = gfp_flags;\n\n\t\t\t\thuge_flags |= GFP_TRANSHUGE_LIGHT | __GFP_NORETRY |\n\t\t\t\t\t__GFP_KSWAPD_RECLAIM;\n\t\t\t\thuge_flags &= ~__GFP_MOVABLE;\n\t\t\t\thuge_flags &= ~__GFP_COMP;\n\t\t\t\tp = alloc_pages(huge_flags, HPAGE_PMD_ORDER);\n\t\t\t\tif (!p)\n\t\t\t\t\tbreak;\n\n\t\t\t\tfor (j = 0; j < HPAGE_PMD_NR; ++j)\n\t\t\t\t\tpages[i++] = p++;\n\n\t\t\t\tnpages -= HPAGE_PMD_NR;\n\t\t\t}\n\t\t}\n#endif\n\n\t\tfirst = i;\n\t\twhile (npages) {\n\t\t\tp = alloc_page(gfp_flags);\n\t\t\tif (!p) {\n\t\t\t\tpr_debug(\"Unable to allocate page\\n\");\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\t/* Swap the pages if we detect consecutive order */\n\t\t\tif (i > first && pages[i - 1] == p - 1)\n\t\t\t\tswap(p, pages[i - 1]);\n\n\t\t\tpages[i++] = p;\n\t\t\t--npages;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tcount = 0;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tif (huge && npages >= HPAGE_PMD_NR) {\n\t\tINIT_LIST_HEAD(&plist);\n\t\tttm_page_pool_get_pages(huge, &plist, flags, cstate,\n\t\t\t\t\tnpages / HPAGE_PMD_NR,\n\t\t\t\t\tHPAGE_PMD_ORDER);\n\n\t\tlist_for_each_entry(p, &plist, lru) {\n\t\t\tunsigned j;\n\n\t\t\tfor (j = 0; j < HPAGE_PMD_NR; ++j)\n\t\t\t\tpages[count++] = &p[j];\n\t\t}\n\t}\n#endif\n\n\tINIT_LIST_HEAD(&plist);\n\tr = ttm_page_pool_get_pages(pool, &plist, flags, cstate,\n\t\t\t\t    npages - count, 0);\n\n\tfirst = count;\n\tlist_for_each_entry(p, &plist, lru) {\n\t\tstruct page *tmp = p;\n\n\t\t/* Swap the pages if we detect consecutive order */\n\t\tif (count > first && pages[count - 1] == tmp - 1)\n\t\t\tswap(tmp, pages[count - 1]);\n\t\tpages[count++] = tmp;\n\t}\n\n\tif (r) {\n\t\t/* If there is any pages in the list put them back to\n\t\t * the pool.\n\t\t */\n\t\tpr_debug(\"Failed to allocate extra pages for large request\\n\");\n\t\tttm_put_pages(pages, count, flags, cstate);\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic void ttm_page_pool_init_locked(struct ttm_page_pool *pool, gfp_t flags,\n\t\tchar *name, unsigned int order)\n{\n\tspin_lock_init(&pool->lock);\n\tpool->fill_lock = false;\n\tINIT_LIST_HEAD(&pool->list);\n\tpool->npages = pool->nfrees = 0;\n\tpool->gfp_flags = flags;\n\tpool->name = name;\n\tpool->order = order;\n}\n\nint ttm_page_alloc_init(struct ttm_mem_global *glob, unsigned max_pages)\n{\n\tint ret;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tunsigned order = HPAGE_PMD_ORDER;\n#else\n\tunsigned order = 0;\n#endif\n\n\tWARN_ON(_manager);\n\n\tpr_info(\"Initializing pool allocator\\n\");\n\n\t_manager = kzalloc(sizeof(*_manager), GFP_KERNEL);\n\tif (!_manager)\n\t\treturn -ENOMEM;\n\n\tttm_page_pool_init_locked(&_manager->wc_pool, GFP_HIGHUSER, \"wc\", 0);\n\n\tttm_page_pool_init_locked(&_manager->uc_pool, GFP_HIGHUSER, \"uc\", 0);\n\n\tttm_page_pool_init_locked(&_manager->wc_pool_dma32,\n\t\t\t\t  GFP_USER | GFP_DMA32, \"wc dma\", 0);\n\n\tttm_page_pool_init_locked(&_manager->uc_pool_dma32,\n\t\t\t\t  GFP_USER | GFP_DMA32, \"uc dma\", 0);\n\n\tttm_page_pool_init_locked(&_manager->wc_pool_huge,\n\t\t\t\t  (GFP_TRANSHUGE_LIGHT | __GFP_NORETRY |\n\t\t\t\t   __GFP_KSWAPD_RECLAIM) &\n\t\t\t\t  ~(__GFP_MOVABLE | __GFP_COMP),\n\t\t\t\t  \"wc huge\", order);\n\n\tttm_page_pool_init_locked(&_manager->uc_pool_huge,\n\t\t\t\t  (GFP_TRANSHUGE_LIGHT | __GFP_NORETRY |\n\t\t\t\t   __GFP_KSWAPD_RECLAIM) &\n\t\t\t\t  ~(__GFP_MOVABLE | __GFP_COMP)\n\t\t\t\t  , \"uc huge\", order);\n\n\t_manager->options.max_size = max_pages;\n\t_manager->options.small = SMALL_ALLOCATION;\n\t_manager->options.alloc_size = NUM_PAGES_TO_ALLOC;\n\n\tret = kobject_init_and_add(&_manager->kobj, &ttm_pool_kobj_type,\n\t\t\t\t   &glob->kobj, \"pool\");\n\tif (unlikely(ret != 0))\n\t\tgoto error;\n\n\tret = ttm_pool_mm_shrink_init(_manager);\n\tif (unlikely(ret != 0))\n\t\tgoto error;\n\treturn 0;\n\nerror:\n\tkobject_put(&_manager->kobj);\n\t_manager = NULL;\n\treturn ret;\n}\n\nvoid ttm_page_alloc_fini(void)\n{\n\tint i;\n\n\tpr_info(\"Finalizing pool allocator\\n\");\n\tttm_pool_mm_shrink_fini(_manager);\n\n\t/* OK to use static buffer since global mutex is no longer used. */\n\tfor (i = 0; i < NUM_POOLS; ++i)\n\t\tttm_page_pool_free(&_manager->pools[i], FREE_ALL_PAGES, true);\n\n\tkobject_put(&_manager->kobj);\n\t_manager = NULL;\n}\n\nstatic void\nttm_pool_unpopulate_helper(struct ttm_tt *ttm, unsigned mem_count_update)\n{\n\tstruct ttm_mem_global *mem_glob = ttm->bdev->glob->mem_glob;\n\tunsigned i;\n\n\tif (mem_count_update == 0)\n\t\tgoto put_pages;\n\n\tfor (i = 0; i < mem_count_update; ++i) {\n\t\tif (!ttm->pages[i])\n\t\t\tcontinue;\n\n\t\tttm_mem_global_free_page(mem_glob, ttm->pages[i], PAGE_SIZE);\n\t}\n\nput_pages:\n\tttm_put_pages(ttm->pages, ttm->num_pages, ttm->page_flags,\n\t\t      ttm->caching_state);\n\tttm->state = tt_unpopulated;\n}\n\nint ttm_pool_populate(struct ttm_tt *ttm, struct ttm_operation_ctx *ctx)\n{\n\tstruct ttm_mem_global *mem_glob = ttm->bdev->glob->mem_glob;\n\tunsigned i;\n\tint ret;\n\n\tif (ttm->state != tt_unpopulated)\n\t\treturn 0;\n\n\tif (ttm_check_under_lowerlimit(mem_glob, ttm->num_pages, ctx))\n\t\treturn -ENOMEM;\n\n\tret = ttm_get_pages(ttm->pages, ttm->num_pages, ttm->page_flags,\n\t\t\t    ttm->caching_state);\n\tif (unlikely(ret != 0)) {\n\t\tttm_pool_unpopulate_helper(ttm, 0);\n\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < ttm->num_pages; ++i) {\n\t\tret = ttm_mem_global_alloc_page(mem_glob, ttm->pages[i],\n\t\t\t\t\t\tPAGE_SIZE, ctx);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tttm_pool_unpopulate_helper(ttm, i);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tif (unlikely(ttm->page_flags & TTM_PAGE_FLAG_SWAPPED)) {\n\t\tret = ttm_tt_swapin(ttm);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tttm_pool_unpopulate(ttm);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tttm->state = tt_unbound;\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_pool_populate);\n\nvoid ttm_pool_unpopulate(struct ttm_tt *ttm)\n{\n\tttm_pool_unpopulate_helper(ttm, ttm->num_pages);\n}\nEXPORT_SYMBOL(ttm_pool_unpopulate);\n\nint ttm_populate_and_map_pages(struct device *dev, struct ttm_dma_tt *tt,\n\t\t\t\t\tstruct ttm_operation_ctx *ctx)\n{\n\tunsigned i, j;\n\tint r;\n\n\tr = ttm_pool_populate(&tt->ttm, ctx);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < tt->ttm.num_pages; ++i) {\n\t\tstruct page *p = tt->ttm.pages[i];\n\t\tsize_t num_pages = 1;\n\n\t\tfor (j = i + 1; j < tt->ttm.num_pages; ++j) {\n\t\t\tif (++p != tt->ttm.pages[j])\n\t\t\t\tbreak;\n\n\t\t\t++num_pages;\n\t\t}\n\n\t\ttt->dma_address[i] = dma_map_page(dev, tt->ttm.pages[i],\n\t\t\t\t\t\t  0, num_pages * PAGE_SIZE,\n\t\t\t\t\t\t  DMA_BIDIRECTIONAL);\n\t\tif (dma_mapping_error(dev, tt->dma_address[i])) {\n\t\t\twhile (i--) {\n\t\t\t\tdma_unmap_page(dev, tt->dma_address[i],\n\t\t\t\t\t       PAGE_SIZE, DMA_BIDIRECTIONAL);\n\t\t\t\ttt->dma_address[i] = 0;\n\t\t\t}\n\t\t\tttm_pool_unpopulate(&tt->ttm);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tfor (j = 1; j < num_pages; ++j) {\n\t\t\ttt->dma_address[i + 1] = tt->dma_address[i] + PAGE_SIZE;\n\t\t\t++i;\n\t\t}\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_populate_and_map_pages);\n\nvoid ttm_unmap_and_unpopulate_pages(struct device *dev, struct ttm_dma_tt *tt)\n{\n\tunsigned i, j;\n\n\tfor (i = 0; i < tt->ttm.num_pages;) {\n\t\tstruct page *p = tt->ttm.pages[i];\n\t\tsize_t num_pages = 1;\n\n\t\tif (!tt->dma_address[i] || !tt->ttm.pages[i]) {\n\t\t\t++i;\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor (j = i + 1; j < tt->ttm.num_pages; ++j) {\n\t\t\tif (++p != tt->ttm.pages[j])\n\t\t\t\tbreak;\n\n\t\t\t++num_pages;\n\t\t}\n\n\t\tdma_unmap_page(dev, tt->dma_address[i], num_pages * PAGE_SIZE,\n\t\t\t       DMA_BIDIRECTIONAL);\n\n\t\ti += num_pages;\n\t}\n\tttm_pool_unpopulate(&tt->ttm);\n}\nEXPORT_SYMBOL(ttm_unmap_and_unpopulate_pages);\n\nint ttm_page_alloc_debugfs(struct seq_file *m, void *data)\n{\n\tstruct ttm_page_pool *p;\n\tunsigned i;\n\tchar *h[] = {\"pool\", \"refills\", \"pages freed\", \"size\"};\n\tif (!_manager) {\n\t\tseq_printf(m, \"No pool allocator running.\\n\");\n\t\treturn 0;\n\t}\n\tseq_printf(m, \"%7s %12s %13s %8s\\n\",\n\t\t\th[0], h[1], h[2], h[3]);\n\tfor (i = 0; i < NUM_POOLS; ++i) {\n\t\tp = &_manager->pools[i];\n\n\t\tseq_printf(m, \"%7s %12ld %13ld %8d\\n\",\n\t\t\t\tp->name, p->nrefills,\n\t\t\t\tp->nfrees, p->npages);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_page_alloc_debugfs);\n"], "filenames": ["drivers/gpu/drm/ttm/ttm_page_alloc.c"], "buggy_code_start_loc": [736], "buggy_code_end_loc": [772], "fixing_code_start_loc": [736], "fixing_code_end_loc": [772], "type": "CWE-125", "message": "In the Linux kernel 5.0.0-rc7 (as distributed in ubuntu/linux.git on kernel.ubuntu.com), mounting a crafted f2fs filesystem image and performing some operations can lead to slab-out-of-bounds read access in ttm_put_pages in drivers/gpu/drm/ttm/ttm_page_alloc.c. This is related to the vmwgfx or ttm module.", "other": {"cve": {"id": "CVE-2019-19927", "sourceIdentifier": "cve@mitre.org", "published": "2019-12-31T02:15:10.647", "lastModified": "2020-05-14T14:48:24.137", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In the Linux kernel 5.0.0-rc7 (as distributed in ubuntu/linux.git on kernel.ubuntu.com), mounting a crafted f2fs filesystem image and performing some operations can lead to slab-out-of-bounds read access in ttm_put_pages in drivers/gpu/drm/ttm/ttm_page_alloc.c. This is related to the vmwgfx or ttm module."}, {"lang": "es", "value": "En el kernel de Linux versi\u00f3n 5.0.0-rc7 (distribuido en ubuntu/linux.git en kernel.ubuntu.com), montar una imagen de sistema de archivos f2fs especialmente  dise\u00f1ada y llevar a cabo algunas operaciones puede conllevar a un acceso de lectura fuera de l\u00edmites en la funci\u00f3n ttm_put_pages en el archivo drivers/gpu/drm/ttm/ttm_page_alloc.c. Esto est\u00e1 relacionado con el m\u00f3dulo vmwgfx o ttm."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:U/C:H/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 6.0, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 0.8, "impactScore": 5.2}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "PARTIAL", "baseScore": 3.6}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 4.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:5.0:rc7:*:*:*:*:*:*", "matchCriteriaId": "77F342FB-3D7B-4EAE-BF8B-57B7B860BAFD"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:opensuse:leap:15.1:*:*:*:*:*:*:*", "matchCriteriaId": "B620311B-34A3-48A6-82DF-6F078D7A4493"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2020-03/msg00021.html", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/bobfuzzer/CVE/tree/master/CVE-2019-19927", "source": "cve@mitre.org", "tags": ["Exploit", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/453393369dc9806d2455151e329c599684762428", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/a66477b0efe511d98dde3e4aaeb189790e6f0a39", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/ac1e516d5a4c56bf0cb4a3dfc0672f689131cfd4", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20200204-0002/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/453393369dc9806d2455151e329c599684762428"}}