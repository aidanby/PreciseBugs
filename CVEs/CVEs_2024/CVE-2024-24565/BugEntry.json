{"buggy_code": [".. _version_5.5.4:\n\n==========================\nVersion 5.5.4 - Unreleased\n==========================\n\n\n.. comment 1. Remove the \" - Unreleased\" from the header above and adjust the ==\n.. comment 2. Remove the NOTE below and replace with: \"Released on 20XX-XX-XX.\"\n.. comment    (without a NOTE entry, simply starting from col 1 of the line)\n\n.. NOTE::\n    In development. 5.5.4 isn't released yet. These are the release notes for\n    the upcoming release.\n\n.. NOTE::\n    If you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher\n    before you upgrade to 5.5.4.\n\n    We recommend that you upgrade to the latest 5.4 release before moving to\n    5.5.4.\n\n    A rolling upgrade from 5.4.x to 5.5.4 is supported.\n\n    Before upgrading, you should `back up your data`_.\n\n.. WARNING::\n\n    Tables that were created before CrateDB 4.x will not function with 5.x\n    and must be recreated before moving to 5.x.x.\n\n    You can recreate tables using ``COPY TO`` and ``COPY FROM`` or by\n    `inserting the data into a new table`_.\n\n.. _back up your data: https://crate.io/docs/crate/reference/en/latest/admin/snapshots.html\n\n.. _inserting the data into a new table: https://crate.io/docs/crate/reference/en/latest/admin/system-information.html#tables-need-to-be-recreated\n\n.. rubric:: Table of contents\n\n.. contents::\n   :local:\n\nSee the :ref:`version_5.5.0` release notes for a full list of changes in the\n5.5 series.\n\nFixes\n=====\n\n- Fixed an issue that caused ``SELECT`` statements with ``WHERE``\n  clause having an equality condition on a primary key to return ``NULL`` when\n  selecting an object sub-column of ``ARRAY(OBJECT)`` type.\n\n- Fixed an issue that caused queries to return invalid results when the\n  ``WHERE`` clause involving ``primary key`` columns have the following\n  form::\n\n    SELECT * FROM t WHERE NOT(pk_col != 1 AND pk_col IS NULL);\n\n  An equivalent query that returned valid results::\n\n    SELECT * FROM t WHERE pk_col = 1 OR pk_col IS NOT NULL;\n\n- Fixed an issue that caused failure of a statement, mixing correlated subquery\n  and sub-select. An example::\n\n    CREATE TABLE tbl(x INT);\n    INSERT INTO tbl(x) VALUES (1);\n    SELECT (\n       SELECT x FROM tbl\n          WHERE t.x = tbl.x\n        AND\n          tbl.x IN (SELECT generate_series from generate_series(1, 1))\n    ) FROM tbl t;\n", ".. _version_5.6.1:\n\n==========================\nVersion 5.6.1 - Unreleased\n==========================\n\n\n.. comment 1. Remove the \" - Unreleased\" from the header above and adjust the ==\n.. comment 2. Remove the NOTE below and replace with: \"Released on 20XX-XX-XX.\"\n.. comment    (without a NOTE entry, simply starting from col 1 of the line)\n\n.. NOTE::\n    In development. 5.6.1 isn't released yet. These are the release notes for\n    the upcoming release.\n\n.. NOTE::\n\n    If you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher\n    before you upgrade to 5.6.1.\n\n    We recommend that you upgrade to the latest 5.5 release before moving to\n    5.6.1.\n\n    A rolling upgrade from 5.5.x to 5.6.1 is supported.\n    Before upgrading, you should `back up your data`_.\n\n.. WARNING::\n\n    Tables that were created before CrateDB 4.x will not function with 5.x\n    and must be recreated before moving to 5.x.x.\n\n    You can recreate tables using ``COPY TO`` and ``COPY FROM`` or by\n    `inserting the data into a new table`_.\n\n.. _back up your data: https://crate.io/docs/crate/reference/en/latest/admin/snapshots.html\n.. _inserting the data into a new table: https://crate.io/docs/crate/reference/en/latest/admin/system-information.html#tables-need-to-be-recreated\n\n.. rubric:: Table of contents\n\n.. contents::\n   :local:\n\nSee the :ref:`version_5.6.0` release notes for a full list of changes in the\n5.6 series.\n\n\nFixes\n=====\n\n- Added a workaround for a change in JDK 21.0.2 which caused many operations to\n  get stuck.\n\n- Fixed an issue that led to errors when\n  :ref:`privileges <administration-privileges>` are defined for users, when\n  performing a rolling upgrade of a cluster from a version before\n  :ref:`version_5.6.0` to :ref:`version_5.6.0`.\n\n- Fixed an issue that caused ``SELECT`` statements with ``WHERE``\n  clause having an equality condition on a primary key to return ``NULL`` when\n  selecting an object sub-column of ``ARRAY(OBJECT)`` type.\n\n- Fixed an issue that caused queries to return invalid results when the\n  ``WHERE`` clause involving ``primary key`` columns have the following\n  form::\n\n    SELECT * FROM t WHERE NOT(pk_col != 1 AND pk_col IS NULL);\n\n  An equivalent query that returned valid results::\n\n    SELECT * FROM t WHERE pk_col = 1 OR pk_col IS NOT NULL;\n\n- Fixed an issue that caused failure of a statement, mixing correlated subquery\n  and sub-select. An example::\n\n    CREATE TABLE tbl(x INT);\n    INSERT INTO tbl(x) VALUES (1);\n    SELECT (\n       SELECT x FROM tbl\n          WHERE t.x = tbl.x\n        AND\n          tbl.x IN (SELECT generate_series from generate_series(1, 1))\n    ) FROM tbl t;\n", ".. highlight:: psql\n\n.. _sql-copy-from:\n\n=============\n``COPY FROM``\n=============\n\nYou can use the ``COPY FROM`` :ref:`statement <gloss-statement>` to copy data\nfrom a file into a table.\n\n.. SEEALSO::\n\n    :ref:`Data manipulation: Import and export <dml-import-export>`\n\n    :ref:`SQL syntax: COPY TO <sql-copy-to>`\n\n.. rubric:: Table of contents\n\n.. contents::\n   :local:\n   :depth: 2\n\n.. _sql-copy-from-synopsis:\n\nSynopsis\n========\n\n::\n\n    COPY table_identifier\n      [ ( column_ident [, ...] ) ]\n      [ PARTITION (partition_column = value [ , ... ]) ]\n      FROM uri [ WITH ( option = value [, ...] ) ] [ RETURN SUMMARY ]\n\n\n.. _sql-copy-from-desc:\n\nDescription\n===========\n\nA ``COPY FROM`` copies data from a URI to the specified table.\n\nThe nodes in the cluster will attempt to read the files available at the URI\nand import the data.\n\nHere's an example:\n\n::\n\n    cr> COPY quotes FROM 'file:///tmp/import_data/quotes.json';\n    COPY OK, 3 rows affected (... sec)\n\n.. NOTE::\n\n    The ``COPY`` statements use :ref:`Overload Protection <overload_protection>` to ensure other\n    queries can still perform. Please change these settings during large inserts if needed.\n\n.. _sql-copy-from-formats:\n\nFile formats\n------------\n\nCrateDB accepts both JSON and CSV inputs. The format is inferred from the file\nextension (``.json`` or ``.csv`` respectively) if possible. The :ref:`format\n<sql-copy-from-format>` can also be set as an option. If a format is not\nspecified and the format cannot be inferred, the file will be processed as\nJSON.\n\nJSON files must contain a single JSON object per line and all files must be\nUTF-8 encoded. Also, any empty lines are skipped.\n\nExample JSON data::\n\n    {\"id\": 1, \"quote\": \"Don't panic\"}\n    {\"id\": 2, \"quote\": \"Ford, you're turning into a penguin. Stop it.\"}\n\nA CSV file may or may not contain a header. See :ref:`CSV header option\n<sql-copy-from-header>` for further details.\n\nExample CSV data::\n\n    id,quote\n    1,\"Don't panic\"\n    2,\"Ford, you're turning into a penguin. Stop it.\"\n\nExample CSV data with no header::\n\n    1,\"Don't panic\"\n    2,\"Ford, you're turning into a penguin. Stop it.\"\n\nSee also: :ref:`dml-importing-data`.\n\n\n.. _sql-copy-from-type-checks:\n\nData type checks\n----------------\n\nCrateDB checks if the columns' data types match the types from the import file.\nIt casts the types and will always import the data as in the source file.\nFurthermore CrateDB will check for all :ref:`column_constraints`.\n\nFor example a `WKT`_ string cannot be imported into a column of ``geo_shape``\nor ``geo_point`` type, since there is no implicit cast to the `GeoJSON`_ format.\n\n.. NOTE::\n\n   In case the ``COPY FROM`` statement fails, the log output on the node will\n   provide an error message. Any data that has been imported until then has\n   been written to the table and should be deleted before restarting the\n   import.\n\n\n.. _sql-copy-from-params:\n\nParameters\n==========\n\n.. _sql-copy-from-table_ident:\n\n``table_ident``\n  The name (optionally schema-qualified) of an existing table where the data\n  should be put.\n\n.. _sql-copy-from-column_ident:\n\n``column_ident``\n  Used in an optional columns declaration, each ``column_ident`` is the name of a column in the ``table_ident`` table.\n\n  This currently only has an effect if using the CSV file format. See the ``header`` section for how it behaves.\n\n.. _sql-copy-from-uri:\n\n``uri``\n  An expression or array of expressions. Each :ref:`expression\n  <gloss-expression>` must :ref:`evaluate <gloss-evaluation>` to a string\n  literal that is a `well-formed URI`_.\n\n  URIs must use one of the supported :ref:`URI schemes\n  <sql-copy-from-schemes>`. CrateDB supports :ref:`globbing\n  <sql-copy-from-globbing>` for the :ref:`file <sql-copy-from-file>` and\n  :ref:`s3 <sql-copy-from-s3>` URI schemes.\n\n  .. NOTE::\n\n      If the URI scheme is missing, CrateDB assumes the value is a pathname and\n      will prepend the :ref:`file <sql-copy-from-file>` URI scheme (i.e.,\n      ``file://``). So, for example, CrateDB will convert ``/tmp/file.json`` to\n      ``file:///tmp/file.json``.\n\n\n.. _sql-copy-from-globbing:\n\nURI globbing\n------------\n\nWith :ref:`file <sql-copy-from-file>` and :ref:`s3 <sql-copy-from-s3>` URI\nschemes, you can use pathname `globbing`_ (i.e., ``*`` wildcards) with the\n``COPY FROM`` statement to construct URIs that can match multiple directories\nand files.\n\nSuppose you used ``file:///tmp/import_data/*/*.json`` as the URI. This URI\nwould match all JSON files located in subdirectories of the\n``/tmp/import_data`` directory.\n\nSo, for example, these files would match:\n\n- ``/tmp/import_data/foo/1.json``\n- ``/tmp/import_data/bar/2.json``\n- ``/tmp/import_data/1/boz.json``\n\n.. CAUTION::\n\n    A file named ``/tmp/import_data/foo/.json`` would also match the\n    ``file:///tmp/import_data/*/*.json`` URI. The ``*`` wildcard matches any\n    number of characters, including none.\n\nHowever, these files would not match:\n\n- ``/tmp/import_data/1.json`` (two few subdirectories)\n- ``/tmp/import_data/foo/bar/2.json`` (too many subdirectories)\n- ``/tmp/import_data/1/boz.js`` (file extension mismatch)\n\n\n.. _sql-copy-from-schemes:\n\nURI schemes\n-----------\n\nCrateDB supports the following URI schemes:\n\n.. contents::\n   :local:\n   :depth: 1\n\n\n.. _sql-copy-from-file:\n\n``file``\n''''''''\n\nYou can use the ``file://`` scheme to specify an absolute path to one or more\nfiles accessible via the local filesystem of one or more CrateDB nodes.\n\nFor example:\n\n.. code-block:: text\n\n    file:///path/to/dir\n\nThe files must be accessible on at least one node and the system user running\nthe ``crate`` process must have read access to every file specified.\n\nBy default, every node will attempt to import every file. If the file is\naccessible on multiple nodes, you can set the `shared`_ option to true in order\nto avoid importing duplicates.\n\nUse :ref:`sql-copy-from-return-summary` to get information about what actions\nwere performed on each node.\n\n.. TIP::\n\n    If you are running CrateDB inside a container, the file must be inside the\n    container. If you are using *Docker*, you may have to configure a `Docker\n    volume`_ to accomplish this.\n\n.. TIP::\n\n    If you are using *Microsoft Windows*, you must include the drive letter in\n    the file URI.\n\n    For example:\n\n    .. code-block:: text\n\n        file://C:\\/tmp/import_data/quotes.json\n\n    Consult the `Windows documentation`_ for more information.\n\n\n.. _sql-copy-from-s3:\n\n``s3``\n''''''\n\nYou can use the ``s3://`` scheme to access buckets on the `Amazon Simple\nStorage Service`_ (Amazon S3).\n\nFor example:\n\n.. code-block:: text\n\n    s3://[<accesskey>:<secretkey>@][<host>:<port>/]<bucketname>/<path>\n\nS3 compatible storage providers can be specified by the optional pair of host\nand port, which defaults to Amazon S3 if not provided.\n\nHere is a more concrete example:\n\n.. code-block:: text\n\n    COPY t FROM 's3://accessKey:secretKey@s3.amazonaws.com:443/myBucket/key/a.json' with (protocol = 'https')\n\nIf no credentials are set the s3 client will operate in anonymous mode.\nSee `AWS Java Documentation`_.\n\nUsing the ``s3://`` scheme automatically sets the `shared`_ to true.\n\n.. TIP::\n\n   A ``secretkey`` provided by Amazon Web Services can contain characters such\n   as '/', '+' or '='. These characters must be `URL encoded`_. For a detailed\n   explanation read the official `AWS documentation`_.\n\n   To escape a secret key, you can use a snippet like this:\n\n   .. code-block:: console\n\n      sh$ python -c \"from getpass import getpass; from urllib.parse import quote_plus; print(quote_plus(getpass('secret_key: ')))\"\n\n   This will prompt for the secret key and print the encoded variant.\n\n   Additionally, versions prior to 0.51.x use HTTP for connections to S3. Since\n   0.51.x these connections are using the HTTPS protocol. Please make sure you\n   update your firewall rules to allow outgoing connections on port ``443``.\n\n\n.. _sql-copy-from-other-schemes:\n\nOther schemes\n'''''''''''''\n\nIn addition to the schemes above, CrateDB supports all protocols supported by\nthe `URL`_ implementation of its JVM (typically ``http``, ``https``, ``ftp``,\nand ``jar``). Please refer to the documentation of the JVM vendor for an\naccurate list of supported protocols.\n\n.. NOTE::\n\n    These schemes *do not* support wildcard expansion.\n\n\n.. _sql-copy-from-clauses:\n\nClauses\n=======\n\nThe ``COPY FROM`` :ref:`statement <gloss-statement>` supports the following\nclauses:\n\n.. contents::\n   :local:\n   :depth: 1\n\n\n.. _sql-copy-from-partition:\n\n``PARTITION``\n-------------\n\n.. EDITORIAL NOTE\n   ##############\n\n   Multiple files (in this directory) use the same standard text for\n   documenting the ``PARTITION`` clause. (Minor verb changes are made to\n   accomodate the specifics of the parent statement.)\n\n   For consistency, if you make changes here, please be sure to make a\n   corresponding change to the other files.\n\nIf the table is :ref:`partitioned <partitioned-tables>`, the optional\n``PARTITION`` clause can be used to import data into one partition exclusively.\n\n::\n\n    [ PARTITION ( partition_column = value [ , ... ] ) ]\n\n:partition_column:\n  One of the column names used for table partitioning\n\n:value:\n  The respective column value.\n\nAll :ref:`partition columns <gloss-partition-column>` (specified by the\n:ref:`sql-create-table-partitioned-by` clause) must be listed inside the\nparentheses along with their respective values using the ``partition_column =\nvalue`` syntax (separated by commas).\n\nBecause each partition corresponds to a unique set of :ref:`partition column\n<gloss-partition-column>` row values, this clause uniquely identifies a single\npartition for import.\n\n.. TIP::\n\n    The :ref:`ref-show-create-table` statement will show you the complete list\n    of partition columns specified by the\n    :ref:`sql-create-table-partitioned-by` clause.\n\n.. CAUTION::\n\n    Partitioned tables do not store the row values for the partition columns,\n    hence every row will be imported into the specified partition regardless of\n    partition column values.\n\n\n.. _sql-copy-from-with:\n\n``WITH``\n--------\n\nYou can use the optional ``WITH`` clause to specify option values.\n\n::\n\n    [ WITH ( option = value [, ...] ) ]\n\nThe ``WITH`` clause supports the following options:\n\n.. contents::\n   :local:\n   :depth: 1\n\n\n.. _sql-copy-from-bulk_size:\n\n``bulk_size``\n'''''''''''''\n\nCrateDB will process the lines it reads from the ``path`` in bulks. This option\nspecifies the size of one batch. The provided value must be greater than 0, the\ndefault value is 10000.\n\n\n.. _sql-copy-from-fail_fast:\n\n``fail_fast``\n'''''''''''''\n\nA boolean value indicating if the ``COPY FROM`` operation should abort early\nafter an error. This is best effort and due to the distributed execution, it\nmay continue processing some records before it aborts.\nDefaults to ``false``.\n\n.. _sql-copy-from-wait_for_completion:\n\n``wait_for_completion``\n'''''''''''''''''''''''\n\nA boolean value indicating if the ``COPY FROM`` should wait for\nthe copy operation to complete. If set to ``false`` the request\nreturns at once and the copy operation runs in the background.\nDefaults to ``true``.\n\n.. _sql-copy-from-shared:\n\n``shared``\n''''''''''\n\nThis option should be set to true if the URIs location is accessible by more\nthan one CrateDB node to prevent them from importing the same file.\n\nThe default value depends on the scheme of each URI.\n\nIf an array of URIs is passed to ``COPY FROM`` this option will overwrite the\ndefault for *all* URIs.\n\n\n.. _sql-copy-from-node_filters:\n\n``node_filters``\n''''''''''''''''\n\nA filter :ref:`expression <gloss-expression>` to select the nodes to run the\n*read* operation.\n\nIt's an object in the form of::\n\n    {\n        name = '<node_name_regex>',\n        id = '<node_id_regex>'\n    }\n\nOnly one of the keys is required.\n\nThe ``name`` :ref:`regular expression <gloss-regular-expression>` is applied on\nthe ``name`` of all execution nodes, whereas the ``id`` regex is applied on the\n``node id``.\n\nIf both keys are set, *both* regular expressions have to match for a node to be\nincluded.\n\nIf the `shared`_ option is false, a strict node filter might exclude nodes with\naccess to the data leading to a partial import.\n\nTo verify which nodes match the filter, run the statement with\n:ref:`EXPLAIN <ref-explain>`.\n\n\n.. _sql-copy-from-num_readers:\n\n``num_readers``\n'''''''''''''''\n\nThe number of nodes that will read the resources specified in the URI. Defaults\nto the number of nodes available in the cluster. If the option is set to a\nnumber greater than the number of available nodes it will still use each node\nonly once to do the import. However, the value must be an integer greater than\n0.\n\nIf `shared`_ is set to false this option has to be used with caution. It might\nexclude the wrong nodes, causing COPY FROM to read no files or only a subset of\nthe files.\n\n\n.. _sql-copy-from-compression:\n\n``compression``\n'''''''''''''''\n\nThe default value is ``null``, set to ``gzip`` to read gzipped files.\n\n\n.. _sql-copy-from-protocol:\n\n``protocol``\n'''''''''''''''\n\nUsed for :ref:`s3 <sql-copy-from-s3>` scheme only. It is set to HTTPS by\ndefault.\n\n\n.. _sql-copy-from-overwrite_duplicates:\n\n``overwrite_duplicates``\n''''''''''''''''''''''''\n\nDefault: false\n\n``COPY FROM`` by default won't overwrite rows if a document with the same\nprimary key already exists. Set to true to overwrite duplicate rows.\n\n\n.. _sql-copy-from-empty_string_as_null:\n\n``empty_string_as_null``\n''''''''''''''''''''''''\n\nIf set to ``true`` the ``empty_string_as_null`` option enables conversion of\nempty strings into ``NULL``. The default value is ``false`` meaning that no\naction will be taken on empty strings during the COPY FROM execution.\n\nThe option is only supported when using the ``CSV`` format, otherwise, it will\nbe ignored.\n\n\n.. _sql-copy-from-delimiter:\n\n``delimiter``\n'''''''''''''\n\nSpecifies a single one-byte character that separates columns within each line\nof the file. The default delimiter is ``,``.\n\nThe option is only supported when using the ``CSV`` format, otherwise, it will\nbe ignored.\n\n\n.. _sql-copy-from-format:\n\n``format``\n''''''''''\n\nThis option specifies the format of the input file. Available formats are\n``csv`` or ``json``. If a format is not specified and the format cannot be\nguessed from the file extension, the file will be processed as JSON.\n\n\n.. _sql-copy-from-header:\n\n``header``\n''''''''''\n\nUsed to indicate if the first line of a CSV file contains a header with the\ncolumn names. Defaults to ``true``.\n\nIf set to ``false``, the CSV must not contain column names in the first line\nand instead the columns declared in the statement are used. If no columns are\ndeclared in the statement, it will default to all columns present in the table\nin their ``CREATE TABLE`` declaration order.\n\nIf set to ``true`` the first line in the CSV file must contain the column\nnames. You can use the optional column declaration in addition to import only a\nsubset of the data.\n\nIf the statement contains no column declarations, all fields in the CSV are\nread and if it contains fields where there is no matching column in the table,\nthe behavior depends on the ``column_policy`` table setting. If ``dynamic`` it\nimplicitly adds new columns, if ``strict`` the operation will fail.\n\nAn example of using input file with no header\n\n::\n\n    cr> COPY quotes FROM 'file:///tmp/import_data/quotes.csv' with (format='csv', header=false);\n    COPY OK, 3 rows affected (... sec)\n\n\n.. _sql-copy-from-skip:\n\n``skip``\n''''''''\n\nDefault: ``0``\n\nSetting this option to ``n`` skips the first ``n`` rows while copying.\n\n.. NOTE::\n\n    CrateDB by default expects a header in CSV files. If you're using the SKIP\n    option to skip the header, you have to set ``header = false`` as well. See\n    :ref:`header <sql-copy-from-header>`.\n\n\n.. _sql-copy-from-return-summary:\n\n``RETURN SUMMARY``\n------------------\n\nBy using the optional ``RETURN SUMMARY`` clause, a per-node result set will be\nreturned containing information about possible failures and successfully\ninserted records.\n\n::\n\n    [ RETURN SUMMARY ]\n\n+---------------------------------------+------------------------------------------------+---------------+\n| Column Name                           | Description                                    |  Return Type  |\n+=======================================+================================================+===============+\n| ``node``                              | Information about the node that has processed  | ``OBJECT``    |\n|                                       | the URI resource.                              |               |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``node['id']``                        | The id of the node.                            | ``TEXT``      |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``node['name']``                      | The name of the node.                          | ``TEXT``      |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``uri``                               | The URI the node has processed.                | ``TEXT``      |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``error_count``                       | The total number of records which failed.      | ``BIGINT``    |\n|                                       | A NULL value indicates a general URI reading   |               |\n|                                       | error, the error will be listed inside the     |               |\n|                                       | ``errors`` column.                             |               |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``success_count``                     | The total number of records which were         | ``BIGINT``    |\n|                                       | inserted.                                      |               |\n|                                       | A NULL value indicates a general URI reading   |               |\n|                                       | error, the error will be listed inside the     |               |\n|                                       | ``errors`` column.                             |               |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``errors``                            | Contains detailed information about all        | ``OBJECT``    |\n|                                       | errors. Limited to at most 25 error messages.  |               |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``errors[ERROR_MSG]``                 | Contains information about a type of an error. | ``OBJECT``    |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``errors[ERROR_MSG]['count']``        | The number records failed with this error.     | ``BIGINT``    |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``errors[ERROR_MSG]['line_numbers']`` | The line numbers of the source URI where the   | ``ARRAY``     |\n|                                       | error occurred, limited to the first 50        |               |\n|                                       | errors, to avoid buffer pressure on clients.   |               |\n+---------------------------------------+------------------------------------------------+---------------+\n\n\n.. _Amazon Simple Storage Service: https://aws.amazon.com/s3/\n.. _AWS documentation: https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html\n.. _AWS Java Documentation: https://docs.aws.amazon.com/AmazonS3/latest/dev/AuthUsingAcctOrUserCredJava.html\n.. _Docker volume: https://docs.docker.com/storage/volumes/\n.. _GeoJSON: https://geojson.org/\n.. _globbing: https://en.wikipedia.org/wiki/Glob_(programming)\n.. _percent-encoding: https://en.wikipedia.org/wiki/Percent-encoding\n.. _URI Scheme: https://en.wikipedia.org/wiki/URI_scheme\n.. _URL encoded: https://en.wikipedia.org/wiki/Percent-encoding\n.. _URL: https://docs.oracle.com/javase/8/docs/api/java/net/URL.html\n.. _well-formed URI: https://www.ietf.org/rfc/rfc2396.txt\n.. _Windows documentation: https://docs.microsoft.com/en-us/dotnet/standard/io/file-path-formats\n.. _WKT: https://en.wikipedia.org/wiki/Well-known_text\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.copy.s3;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.mockito.ArgumentMatchers.any;\nimport static org.mockito.ArgumentMatchers.anyString;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.when;\n\nimport java.net.SocketTimeoutException;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.TimeUnit;\n\nimport org.elasticsearch.common.settings.Settings;\nimport org.elasticsearch.test.ESTestCase;\nimport org.elasticsearch.threadpool.TestThreadPool;\nimport org.elasticsearch.threadpool.ThreadPool;\nimport org.junit.AfterClass;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\nimport org.mockito.Mockito;\nimport org.mockito.invocation.InvocationOnMock;\nimport org.mockito.stubbing.Answer;\n\nimport com.amazonaws.services.s3.AmazonS3;\nimport com.amazonaws.services.s3.AmazonS3Client;\nimport com.amazonaws.services.s3.model.ObjectListing;\nimport com.amazonaws.services.s3.model.S3Object;\nimport com.amazonaws.services.s3.model.S3ObjectInputStream;\nimport com.amazonaws.services.s3.model.S3ObjectSummary;\n\nimport io.crate.copy.s3.common.S3ClientHelper;\nimport io.crate.data.BatchIterator;\nimport io.crate.execution.engine.collect.files.FileReadingIterator;\nimport io.crate.execution.engine.collect.files.FileReadingIterator.LineCursor;\n\npublic class S3FileReadingCollectorTest extends ESTestCase {\n    private static ThreadPool THREAD_POOL;\n\n    @BeforeClass\n    public static void setUpClass() throws Exception {\n        THREAD_POOL = new TestThreadPool(Thread.currentThread().getName());\n    }\n\n\n    @AfterClass\n    public static void tearDownClass() {\n        ThreadPool.terminate(THREAD_POOL, 30, TimeUnit.SECONDS);\n    }\n\n    @Test\n    public void testCollectFromS3Uri() throws Throwable {\n        // this test just verifies the s3 schema detection and bucketName / prefix extraction from the uri.\n        // real s3 interaction is mocked completely.\n        S3ObjectInputStream inputStream = mock(S3ObjectInputStream.class);\n        when(inputStream.read(any(byte[].class), Mockito.anyInt(), Mockito.anyInt())).thenReturn(-1);\n\n        FileReadingIterator it = createBatchIterator(inputStream, \"s3://fakebucket/foo\");\n        assertThat(it.moveNext()).isFalse();\n    }\n\n    @Test\n    public void testCollectWithOneSocketTimeout() throws Throwable {\n        S3ObjectInputStream inputStream = mock(S3ObjectInputStream.class);\n\n        when(inputStream.read(any(byte[].class), Mockito.anyInt(), Mockito.anyInt()))\n            .thenAnswer(new WriteBufferAnswer(new byte[]{102, 111, 111, 10}))  // first line: foo\n            .thenThrow(new SocketTimeoutException())  // exception causes retry\n            .thenAnswer(new WriteBufferAnswer(new byte[]{102, 111, 111, 10}))  // first line again, because of retry\n            .thenAnswer(new WriteBufferAnswer(new byte[]{98, 97, 114, 10}))  // second line: bar\n            .thenReturn(-1);\n\n        FileReadingIterator it = createBatchIterator(inputStream, \"s3://fakebucket/foo\");\n        BatchIterator<LineCursor> immutableLines = it.map(LineCursor::copy);\n        List<LineCursor> lines = immutableLines.toList().get(5, TimeUnit.SECONDS);\n        assertThat(lines).satisfiesExactly(\n            line1 -> assertThat(line1.line()).isEqualTo(\"foo\"),\n            line1 -> assertThat(line1.line()).isEqualTo(\"bar\")\n        );\n    }\n\n\n    private FileReadingIterator createBatchIterator(S3ObjectInputStream inputStream, String ... fileUris) {\n        String compression = null;\n        return new FileReadingIterator(\n            Arrays.asList(fileUris),\n            compression,\n            Map.of(\n                S3FileInputFactory.NAME,\n                (uri, withClauseOptions) -> new S3FileInput(new S3ClientHelper() {\n                    @Override\n                    protected AmazonS3 initClient(String accessKey, String secretKey, String endpoint, String protocol) {\n                        AmazonS3 client = mock(AmazonS3Client.class);\n                        ObjectListing objectListing = mock(ObjectListing.class);\n                        S3ObjectSummary summary = mock(S3ObjectSummary.class);\n                        S3Object s3Object = mock(S3Object.class);\n                        when(client.listObjects(anyString(), anyString())).thenReturn(objectListing);\n                        when(objectListing.getObjectSummaries()).thenReturn(Collections.singletonList(summary));\n                        when(summary.getKey()).thenReturn(\"foo\");\n                        when(client.getObject(\"fakebucket\", \"foo\")).thenReturn(s3Object);\n                        when(s3Object.getObjectContent()).thenReturn(inputStream);\n                        when(client.listNextBatchOfObjects(any(ObjectListing.class))).thenReturn(objectListing);\n                        when(objectListing.isTruncated()).thenReturn(false);\n                        return client;\n                    }\n                }, uri, \"https\")),\n            false,\n            1,\n            0,\n            Settings.EMPTY,\n            THREAD_POOL.scheduler());\n    }\n\n    private record WriteBufferAnswer(byte[] bytes) implements Answer<Integer> {\n\n        @Override\n        public Integer answer(InvocationOnMock invocation) {\n            byte[] buffer = (byte[]) invocation.getArguments()[0];\n            System.arraycopy(bytes, 0, buffer, 0, bytes.length);\n            return bytes.length;\n        }\n    }\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.exceptions;\n\npublic class UnauthorizedException extends RuntimeException implements UnscopedException {\n\n    public UnauthorizedException(String message) {\n        super(message);\n    }\n\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect.files;\n\nimport static io.crate.common.exceptions.Exceptions.rethrowUnchecked;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.net.SocketException;\nimport java.net.SocketTimeoutException;\nimport java.net.URI;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Paths;\nimport java.util.Collection;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Predicate;\nimport java.util.zip.GZIPInputStream;\n\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\nimport org.elasticsearch.action.bulk.BackoffPolicy;\nimport org.elasticsearch.common.settings.Settings;\nimport org.jetbrains.annotations.NotNull;\nimport org.jetbrains.annotations.Nullable;\n\nimport io.crate.common.annotations.VisibleForTesting;\nimport io.crate.common.exceptions.Exceptions;\nimport io.crate.common.unit.TimeValue;\nimport io.crate.data.BatchIterator;\n\n/**\n * BatchIterator to read lines from one or more {@link URI}s.\n *\n * <p>\n * URIs are opened using a {@link FileInputFactory}.\n * A map from Scheme -> FileInputFactory is parameterized in the constructor to support\n * arbitrary sources.\n * </p>\n *\n * <p>\n * The iterator automatically retries reading on\n * @{link {@link SocketException} or {@link SocketTimeoutException}\n * </p>\n *\n * <p>\n * The file content is exposed via a shared {@link LineCursor}\n * It's properties are mutated after each {@link #moveNext()} call.\n * Use {@link LineCursor#copy()} if you need an instance that's not shared.\n *\n * {@link #currentElement()} can be used in a \"off-position\", before the first {@link #moveNext()} call\n * to gain early access to the cursor.\n * </p>\n */\npublic class FileReadingIterator implements BatchIterator<FileReadingIterator.LineCursor> {\n\n    private static final Logger LOGGER = LogManager.getLogger(FileReadingIterator.class);\n    @VisibleForTesting\n    static final int MAX_SOCKET_TIMEOUT_RETRIES = 5;\n\n    private static final Predicate<URI> MATCH_ALL_PREDICATE = (URI input) -> true;\n\n    private final Map<String, FileInputFactory> fileInputFactories;\n    private final Boolean shared;\n    private final int numReaders;\n    private final int readerNumber;\n    private final boolean compressed;\n    private final List<FileInput> fileInputs;\n\n    private volatile Throwable killed;\n\n    private Iterator<FileInput> fileInputsIterator = null;\n    private FileInput currentInput = null;\n    private Iterator<URI> currentInputUriIterator = null;\n    private BufferedReader currentReader = null;\n\n    @VisibleForTesting\n    long watermark;\n\n    private final LineCursor cursor;\n    private final ScheduledExecutorService scheduler;\n    private final Iterator<TimeValue> backOffPolicy;\n\n    public static class LineCursor {\n        private URI uri;\n        private long lineNumber;\n        private String line;\n        private IOException failure;\n\n        public LineCursor() {\n        }\n\n        public LineCursor(URI uri, long lineNumber, @Nullable String line, @Nullable IOException failure) {\n            this.uri = uri;\n            this.lineNumber = lineNumber;\n            this.line = line;\n            this.failure = failure;\n        }\n\n        public URI uri() {\n            return uri;\n        }\n\n        public long lineNumber() {\n            return lineNumber;\n        }\n\n        @Nullable\n        public String line() {\n            return line;\n        }\n\n        @Nullable\n        public IOException failure() {\n            return failure;\n        }\n\n        @VisibleForTesting\n        public LineCursor copy() {\n            return new LineCursor(uri, lineNumber, line, failure);\n        }\n\n        @Override\n        public String toString() {\n            return \"LineCursor{\" + uri + \":\" + lineNumber + \":line=\" + line + \", failure=\" + failure + \"}\";\n        }\n\n        @Override\n        public int hashCode() {\n            return Objects.hash(uri, lineNumber, line, failure);\n        }\n\n        @Override\n        public boolean equals(Object obj) {\n            if (this == obj) {\n                return true;\n            }\n            if (obj == null) {\n                return false;\n            }\n            if (getClass() != obj.getClass()) {\n                return false;\n            }\n            LineCursor other = (LineCursor) obj;\n            return Objects.equals(uri, other.uri)\n                && lineNumber == other.lineNumber\n                && Objects.equals(line, other.line)\n                && Objects.equals(failure, other.failure);\n        }\n    }\n\n    public FileReadingIterator(Collection<String> fileUris,\n                               String compression,\n                               Map<String, FileInputFactory> fileInputFactories,\n                               Boolean shared,\n                               int numReaders,\n                               int readerNumber,\n                               Settings withClauseOptions,\n                               ScheduledExecutorService scheduler) {\n        this.compressed = compression != null && compression.equalsIgnoreCase(\"gzip\");\n        this.fileInputFactories = fileInputFactories;\n        this.cursor = new LineCursor();\n        this.shared = shared;\n        this.numReaders = numReaders;\n        this.readerNumber = readerNumber;\n        this.scheduler = scheduler;\n        this.backOffPolicy = BackoffPolicy.exponentialBackoff(TimeValue.ZERO, MAX_SOCKET_TIMEOUT_RETRIES).iterator();\n\n        this.fileInputs = fileUris.stream()\n            .map(uri -> toFileInput(uri, withClauseOptions))\n            .filter(Objects::nonNull)\n            .toList();\n        fileInputsIterator = fileInputs.iterator();\n    }\n\n    @Override\n    public LineCursor currentElement() {\n        return cursor;\n    }\n\n    @Override\n    public void kill(@NotNull Throwable throwable) {\n        killed = throwable;\n    }\n\n    @Override\n    public void moveToStart() {\n        raiseIfKilled();\n        reset();\n        watermark = 0;\n        fileInputsIterator = fileInputs.iterator();\n    }\n\n    @Override\n    public boolean moveNext() {\n        raiseIfKilled();\n        try {\n            if (currentReader != null) {\n                String line;\n                try {\n                    line = getLine(currentReader);\n                } catch (SocketException | SocketTimeoutException e) {\n                    if (backOffPolicy.hasNext()) {\n                        return false;\n                    }\n                    throw e;\n                }\n                if (line == null) {\n                    closeReader();\n                    return moveNext();\n                }\n                cursor.line = line;\n                cursor.failure = null;\n                return true;\n            } else if (currentInputUriIterator != null && currentInputUriIterator.hasNext()) {\n                advanceToNextUri(currentInput);\n                return moveNext();\n            } else if (fileInputsIterator != null && fileInputsIterator.hasNext()) {\n                advanceToNextFileInput();\n                return moveNext();\n            } else {\n                reset();\n                return false;\n            }\n        } catch (IOException e) {\n            cursor.failure = e;\n            closeReader();\n            // If IOError happens on file opening, let consumers collect the error\n            // This is mostly for RETURN SUMMARY of COPY FROM\n            if (cursor.lineNumber == 0) {\n                return true;\n            }\n            return moveNext();\n        }\n    }\n\n    private void advanceToNextUri(FileInput fileInput) throws IOException {\n        watermark = 0;\n        createReader(fileInput, currentInputUriIterator.next());\n    }\n\n    private void advanceToNextFileInput() throws IOException {\n        currentInput = fileInputsIterator.next();\n        List<URI> uris = currentInput.expandUri().stream().filter(this::shouldBeReadByCurrentNode).toList();\n        if (uris.size() > 0) {\n            currentInputUriIterator = uris.iterator();\n            advanceToNextUri(currentInput);\n        } else if (currentInput.isGlobbed()) {\n            URI uri = currentInput.uri();\n            cursor.uri = uri;\n            throw new IOException(\"Cannot find any URI matching: \" + uri.toString());\n        }\n    }\n\n    private boolean shouldBeReadByCurrentNode(URI uri) {\n        boolean sharedStorage = Objects.requireNonNullElse(shared, currentInput.sharedStorageDefault());\n        if (sharedStorage) {\n            return moduloPredicateImpl(uri, this.readerNumber, this.numReaders);\n        } else {\n            return MATCH_ALL_PREDICATE.test(uri);\n        }\n    }\n\n    private void createReader(FileInput fileInput, URI uri) throws IOException {\n        cursor.uri = uri;\n        cursor.lineNumber = 0;\n        InputStream stream = fileInput.getStream(uri);\n        currentReader = createBufferedReader(stream);\n    }\n\n    private void closeReader() {\n        if (currentReader != null) {\n            try {\n                currentReader.close();\n            } catch (IOException e) {\n                LOGGER.error(\"Unable to close reader for \" + cursor.uri, e);\n            }\n            currentReader = null;\n        }\n    }\n\n    private String getLine(BufferedReader reader) throws IOException {\n        String line = null;\n        try {\n            while ((line = reader.readLine()) != null) {\n                cursor.lineNumber++;\n                if (cursor.lineNumber < watermark) {\n                    continue;\n                } else {\n                    watermark = 0;\n                }\n                if (line.length() == 0) {\n                    continue;\n                }\n                break;\n            }\n        } catch (SocketException | SocketTimeoutException e) {\n            if (backOffPolicy.hasNext()) {\n                watermark = watermark == 0 ? cursor.lineNumber + 1 : watermark;\n                closeReader();\n                createReader(currentInput, cursor.uri);\n            } else {\n                URI uri = currentInput.uri();\n                LOGGER.error(\"Timeout during COPY FROM '\" + uri.toString() +\n                             \"' after \" + MAX_SOCKET_TIMEOUT_RETRIES +\n                             \" retries\", e);\n            }\n            throw e;\n        } catch (Exception e) {\n            URI uri = currentInput.uri();\n            // it's nice to know which exact file/uri threw an error\n            // when COPY FROM returns less rows than expected\n            LOGGER.error(\"Error during COPY FROM '\" + uri.toString() + \"'\", e);\n            rethrowUnchecked(e);\n        }\n        return line;\n    }\n\n    @Override\n    public void close() {\n        closeReader();\n        reset();\n        killed = BatchIterator.CLOSED;\n    }\n\n    private void reset() {\n        fileInputsIterator = null;\n        currentInputUriIterator = null;\n        currentInput = null;\n        cursor.failure = null;\n    }\n\n    @Override\n    public CompletableFuture<?> loadNextBatch() throws IOException {\n        if (backOffPolicy.hasNext()) {\n            CompletableFuture<Void> cf = new CompletableFuture<>();\n            scheduler.schedule(\n                (Runnable) () -> cf.complete(null), // cast to Runnable for enabling mockito tests\n                backOffPolicy.next().millis(),\n                TimeUnit.MILLISECONDS);\n            return cf;\n        } else {\n            throw new IllegalStateException(\"All batches already loaded\");\n        }\n    }\n\n    @Override\n    public boolean allLoaded() {\n        return !backOffPolicy.hasNext();\n    }\n\n    @Override\n    public boolean hasLazyResultSet() {\n        return true;\n    }\n\n    @VisibleForTesting\n    public static URI toURI(String fileUri) {\n        if (fileUri.startsWith(\"/\")) {\n            // using Paths.get().toUri instead of new URI(...) as it also encodes umlauts and other special characters\n            return Paths.get(fileUri).toUri();\n        } else {\n            URI uri = URI.create(fileUri);\n            if (uri.getScheme() == null) {\n                throw new IllegalArgumentException(\"relative fileURIs are not allowed\");\n            }\n            if (uri.getScheme().equals(\"file\") && !uri.getSchemeSpecificPart().startsWith(\"///\")) {\n                throw new IllegalArgumentException(\"Invalid fileURI\");\n            }\n            return uri;\n        }\n    }\n\n    @Nullable\n    private FileInput toFileInput(String fileUri, Settings withClauseOptions) {\n        URI uri = toURI(fileUri);\n        FileInputFactory fileInputFactory = fileInputFactories.get(uri.getScheme());\n        if (fileInputFactory != null) {\n            try {\n                return fileInputFactory.create(uri, withClauseOptions);\n            } catch (IOException e) {\n                return null;\n            }\n        }\n        return new URLFileInput(uri);\n    }\n\n    @VisibleForTesting\n    BufferedReader createBufferedReader(InputStream inputStream) throws IOException {\n        BufferedReader reader;\n        if (compressed) {\n            reader = new BufferedReader(new InputStreamReader(new GZIPInputStream(inputStream),\n                StandardCharsets.UTF_8));\n        } else {\n            reader = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8));\n        }\n        return reader;\n    }\n\n    @VisibleForTesting\n    public static boolean moduloPredicateImpl(URI input, int readerNumber, int numReaders) {\n        int hash = input.hashCode();\n        if (hash == Integer.MIN_VALUE) {\n            hash = 0; // Math.abs(Integer.MIN_VALUE) == Integer.MIN_VALUE\n        }\n        return Math.abs(hash) % numReaders == readerNumber;\n    }\n\n    private void raiseIfKilled() {\n        if (killed != null) {\n            Exceptions.rethrowUnchecked(killed);\n        }\n    }\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect.sources;\n\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.CompletableFuture;\n\nimport org.elasticsearch.cluster.service.ClusterService;\nimport org.elasticsearch.common.inject.Inject;\nimport org.elasticsearch.common.inject.Singleton;\nimport org.elasticsearch.threadpool.ThreadPool;\n\nimport io.crate.analyze.AnalyzedCopyFrom;\nimport io.crate.analyze.CopyFromParserProperties;\nimport io.crate.analyze.SymbolEvaluator;\nimport io.crate.common.annotations.VisibleForTesting;\nimport io.crate.data.BatchIterator;\nimport io.crate.data.Row;\nimport io.crate.data.SkippingBatchIterator;\nimport io.crate.execution.dsl.phases.CollectPhase;\nimport io.crate.execution.dsl.phases.FileUriCollectPhase;\nimport io.crate.execution.engine.collect.CollectTask;\nimport io.crate.execution.engine.collect.files.FileInputFactory;\nimport io.crate.execution.engine.collect.files.FileReadingIterator;\nimport io.crate.execution.engine.collect.files.LineCollectorExpression;\nimport io.crate.execution.engine.collect.files.LineProcessor;\nimport io.crate.expression.InputFactory;\nimport io.crate.expression.reference.file.FileLineReferenceResolver;\nimport io.crate.expression.symbol.Symbol;\nimport io.crate.metadata.NodeContext;\nimport io.crate.metadata.TransactionContext;\nimport io.crate.planner.operators.SubQueryResults;\nimport io.crate.types.DataTypes;\n\n@Singleton\npublic class FileCollectSource implements CollectSource {\n\n    private final ClusterService clusterService;\n    private final Map<String, FileInputFactory> fileInputFactoryMap;\n    private final InputFactory inputFactory;\n    private final NodeContext nodeCtx;\n    private final ThreadPool threadPool;\n\n    @Inject\n    public FileCollectSource(NodeContext nodeCtx,\n                             ClusterService clusterService,\n                             Map<String, FileInputFactory> fileInputFactoryMap,\n                             ThreadPool threadPool) {\n        this.fileInputFactoryMap = fileInputFactoryMap;\n        this.nodeCtx = nodeCtx;\n        this.inputFactory = new InputFactory(nodeCtx);\n        this.clusterService = clusterService;\n        this.threadPool = threadPool;\n    }\n\n    @Override\n    public CompletableFuture<BatchIterator<Row>> getIterator(TransactionContext txnCtx,\n                                                             CollectPhase collectPhase,\n                                                             CollectTask collectTask,\n                                                             boolean supportMoveToStart) {\n        FileUriCollectPhase fileUriCollectPhase = (FileUriCollectPhase) collectPhase;\n        InputFactory.Context<LineCollectorExpression<?>> ctx =\n            inputFactory.ctxForRefs(txnCtx, FileLineReferenceResolver::getImplementation);\n        ctx.add(collectPhase.toCollect());\n\n        List<String> fileUris = targetUriToStringList(txnCtx, nodeCtx, fileUriCollectPhase.targetUri());\n        FileReadingIterator fileReadingIterator = new FileReadingIterator(\n            fileUris,\n            fileUriCollectPhase.compression(),\n            fileInputFactoryMap,\n            fileUriCollectPhase.sharedStorage(),\n            fileUriCollectPhase.nodeIds().size(),\n            getReaderNumber(fileUriCollectPhase.nodeIds(), clusterService.state().nodes().getLocalNodeId()),\n            fileUriCollectPhase.withClauseOptions(),\n            threadPool.scheduler()\n        );\n        CopyFromParserProperties parserProperties = fileUriCollectPhase.parserProperties();\n        LineProcessor lineProcessor = new LineProcessor(\n            parserProperties.skipNumLines() > 0\n                ? new SkippingBatchIterator<>(fileReadingIterator, (int) parserProperties.skipNumLines())\n                : fileReadingIterator,\n            ctx.topLevelInputs(),\n            ctx.expressions(),\n            fileUriCollectPhase.inputFormat(),\n            parserProperties,\n            fileUriCollectPhase.targetColumns()\n        );\n        return CompletableFuture.completedFuture(lineProcessor);\n    }\n\n    @VisibleForTesting\n    public static int getReaderNumber(Collection<String> nodeIds, String localNodeId) {\n        String[] readers = nodeIds.toArray(new String[0]);\n        Arrays.sort(readers);\n        return Arrays.binarySearch(readers, localNodeId);\n    }\n\n    private static List<String> targetUriToStringList(TransactionContext txnCtx,\n                                                      NodeContext nodeCtx,\n                                                      Symbol targetUri) {\n        Object value = SymbolEvaluator.evaluate(txnCtx, nodeCtx, targetUri, Row.EMPTY, SubQueryResults.EMPTY);\n        if (targetUri.valueType().id() == DataTypes.STRING.id()) {\n            String uri = (String) value;\n            return Collections.singletonList(uri);\n        } else if (DataTypes.STRING_ARRAY.equals(targetUri.valueType())) {\n            return DataTypes.STRING_ARRAY.implicitCast(value);\n        }\n\n        // this case actually never happens because the check is already done in the analyzer\n        throw AnalyzedCopyFrom.raiseInvalidType(targetUri.valueType());\n    }\n}\n", "/*\n * Licensed to Elasticsearch under one or more contributor\n * license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright\n * ownership. Elasticsearch licenses this file to you under\n * the Apache License, Version 2.0 (the \"License\"); you may\n * not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\npackage org.elasticsearch;\n\nimport static io.crate.server.xcontent.XContentParserUtils.ensureExpectedToken;\nimport static java.util.Collections.emptyMap;\nimport static java.util.Collections.unmodifiableMap;\nimport static org.elasticsearch.cluster.metadata.IndexMetadata.INDEX_UUID_NA_VALUE;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.stream.Collectors;\n\nimport org.elasticsearch.action.admin.indices.alias.AliasesNotFoundException;\nimport org.elasticsearch.action.support.replication.ReplicationOperation;\nimport org.elasticsearch.cluster.action.shard.ShardStateAction;\nimport org.elasticsearch.common.io.stream.StreamInput;\nimport org.elasticsearch.common.io.stream.StreamOutput;\nimport org.elasticsearch.common.io.stream.Writeable;\nimport org.elasticsearch.common.logging.LoggerMessageFormat;\nimport org.elasticsearch.common.xcontent.ParseField;\nimport org.elasticsearch.common.xcontent.ToXContentFragment;\nimport org.elasticsearch.common.xcontent.XContentBuilder;\nimport org.elasticsearch.common.xcontent.XContentParser;\nimport org.elasticsearch.index.Index;\nimport org.elasticsearch.index.shard.ShardId;\nimport org.elasticsearch.rest.RestStatus;\nimport org.elasticsearch.transport.TcpTransport;\n\nimport io.crate.common.CheckedFunction;\nimport io.crate.common.exceptions.Exceptions;\nimport io.crate.exceptions.ArrayViaDocValuesUnsupportedException;\nimport io.crate.exceptions.SQLExceptions;\n\n/**\n * A base class for all elasticsearch exceptions.\n */\npublic class ElasticsearchException extends RuntimeException implements ToXContentFragment, Writeable {\n\n    private static final Version UNKNOWN_VERSION_ADDED = Version.fromId(0);\n\n    /**\n     * Passed in the {@link Params} of {@link #generateThrowableXContent(XContentBuilder, Params, Throwable)}\n     * to control if the {@code caused_by} element should render. Unlike most parameters to {@code toXContent} methods this parameter is\n     * internal only and not available as a URL parameter.\n     */\n    private static final String REST_EXCEPTION_SKIP_CAUSE = \"rest.exception.cause.skip\";\n    /**\n     * Passed in the {@link Params} of {@link #generateThrowableXContent(XContentBuilder, Params, Throwable)}\n     * to control if the {@code stack_trace} element should render. Unlike most parameters to {@code toXContent} methods this parameter is\n     * internal only and not available as a URL parameter. Use the {@code error_trace} parameter instead.\n     */\n    public static final String REST_EXCEPTION_SKIP_STACK_TRACE = \"rest.exception.stacktrace.skip\";\n    public static final boolean REST_EXCEPTION_SKIP_STACK_TRACE_DEFAULT = true;\n    private static final boolean REST_EXCEPTION_SKIP_CAUSE_DEFAULT = false;\n    private static final String INDEX_METADATA_KEY = \"es.index\";\n    private static final String INDEX_METADATA_KEY_UUID = \"es.index_uuid\";\n    private static final String SHARD_METADATA_KEY = \"es.shard\";\n    private static final String RESOURCE_METADATA_TYPE_KEY = \"es.resource.type\";\n    private static final String RESOURCE_METADATA_ID_KEY = \"es.resource.id\";\n\n    private static final String TYPE = \"type\";\n    private static final String REASON = \"reason\";\n    private static final String CAUSED_BY = \"caused_by\";\n    private static final ParseField SUPPRESSED = new ParseField(\"suppressed\");\n    private static final String STACK_TRACE = \"stack_trace\";\n    private static final String HEADER = \"header\";\n    private static final String ROOT_CAUSE = \"root_cause\";\n\n    private static final Map<Integer, CheckedFunction<StreamInput, ? extends ElasticsearchException, IOException>> ID_TO_SUPPLIER;\n    private static final Map<Class<? extends ElasticsearchException>, ElasticsearchExceptionHandle> CLASS_TO_ELASTICSEARCH_EXCEPTION_HANDLE;\n    private final Map<String, List<String>> metadata = new HashMap<>();\n    private final Map<String, List<String>> headers = new HashMap<>();\n\n    /**\n     * Construct a <code>ElasticsearchException</code> with the specified cause exception.\n     */\n    public ElasticsearchException(Throwable cause) {\n        super(cause);\n    }\n\n    /**\n     * Construct a <code>ElasticsearchException</code> with the specified detail message.\n     *\n     * The message can be parameterized using <code>{}</code> as placeholders for the given\n     * arguments\n     *\n     * @param msg  the detail message\n     * @param args the arguments for the message\n     */\n    public ElasticsearchException(String msg, Object... args) {\n        super(LoggerMessageFormat.format(msg, args));\n    }\n\n    /**\n     * Construct a <code>ElasticsearchException</code> with the specified detail message\n     * and nested exception.\n     *\n     * The message can be parameterized using <code>{}</code> as placeholders for the given\n     * arguments\n     *\n     * @param msg   the detail message\n     * @param cause the nested exception\n     * @param args  the arguments for the message\n     */\n    public ElasticsearchException(String msg, Throwable cause, Object... args) {\n        super(LoggerMessageFormat.format(msg, args), cause);\n    }\n\n    public ElasticsearchException(StreamInput in) throws IOException {\n        super(in.readOptionalString(), in.readException());\n        readStackTrace(this, in);\n        headers.putAll(in.readMapOfLists(StreamInput::readString, StreamInput::readString));\n        metadata.putAll(in.readMapOfLists(StreamInput::readString, StreamInput::readString));\n    }\n\n    /**\n     * Adds a new piece of metadata with the given key.\n     * If the provided key is already present, the corresponding metadata will be replaced\n     */\n    public void addMetadata(String key, String... values) {\n        addMetadata(key, Arrays.asList(values));\n    }\n\n    /**\n     * Adds a new piece of metadata with the given key.\n     * If the provided key is already present, the corresponding metadata will be replaced\n     */\n    public void addMetadata(String key, List<String> values) {\n        //we need to enforce this otherwise bw comp doesn't work properly, as \"es.\" was the previous criteria to split headers in two sets\n        if (key.startsWith(\"es.\") == false) {\n            throw new IllegalArgumentException(\"exception metadata must start with [es.], found [\" + key + \"] instead\");\n        }\n        this.metadata.put(key, values);\n    }\n\n    /**\n     * Returns a set of all metadata keys on this exception\n     */\n    public Set<String> getMetadataKeys() {\n        return metadata.keySet();\n    }\n\n    /**\n     * Returns the list of metadata values for the given key or {@code null} if no metadata for the\n     * given key exists.\n     */\n    public List<String> getMetadata(String key) {\n        return metadata.get(key);\n    }\n\n    protected Map<String, List<String>> getMetadata() {\n        return metadata;\n    }\n\n    /**\n     * Adds a new header with the given key.\n     * This method will replace existing header if a header with the same key already exists\n     */\n    public void addHeader(String key, List<String> value) {\n        //we need to enforce this otherwise bw comp doesn't work properly, as \"es.\" was the previous criteria to split headers in two sets\n        if (key.startsWith(\"es.\")) {\n            throw new IllegalArgumentException(\"exception headers must not start with [es.], found [\" + key + \"] instead\");\n        }\n        this.headers.put(key, value);\n    }\n\n    /**\n     * Returns a set of all header keys on this exception\n     */\n    public Set<String> getHeaderKeys() {\n        return headers.keySet();\n    }\n\n    /**\n     * Returns the list of header values for the given key or {@code null} if no header for the\n     * given key exists.\n     */\n    public List<String> getHeader(String key) {\n        return headers.get(key);\n    }\n\n    protected Map<String, List<String>> getHeaders() {\n        return headers;\n    }\n\n    /**\n     * Returns the rest status code associated with this exception.\n     */\n    public RestStatus status() {\n        Throwable cause = unwrapCause();\n        if (cause == this) {\n            return RestStatus.INTERNAL_SERVER_ERROR;\n        } else {\n            return SQLExceptions.status(cause);\n        }\n    }\n\n    /**\n     * Unwraps the actual cause from the exception for cases when the exception is a\n     * {@link ElasticsearchWrapperException}.\n     *\n     * @see ExceptionsHelper#unwrapCause(Throwable)\n     */\n    public Throwable unwrapCause() {\n        return SQLExceptions.unwrap(this);\n    }\n\n    /**\n     * Return the detail message, including the message from the nested exception\n     * if there is one.\n     */\n    public String getDetailedMessage() {\n        if (getCause() != null) {\n            StringBuilder sb = new StringBuilder();\n            sb.append(toString()).append(\"; \");\n            if (getCause() instanceof ElasticsearchException) {\n                sb.append(((ElasticsearchException) getCause()).getDetailedMessage());\n            } else {\n                sb.append(getCause());\n            }\n            return sb.toString();\n        } else {\n            return super.toString();\n        }\n    }\n\n    /**\n     * Retrieve the innermost cause of this exception, if none, returns the current exception.\n     */\n    public Throwable getRootCause() {\n        Throwable rootCause = this;\n        Throwable cause = getCause();\n        while (cause != null && cause != rootCause) {\n            rootCause = cause;\n            cause = cause.getCause();\n        }\n        return rootCause;\n    }\n\n    @Override\n    public void writeTo(StreamOutput out) throws IOException {\n        out.writeOptionalString(this.getMessage());\n        out.writeException(this.getCause());\n        writeStackTraces(this, out, StreamOutput::writeException);\n        out.writeMapOfLists(headers, StreamOutput::writeString, StreamOutput::writeString);\n        out.writeMapOfLists(metadata, StreamOutput::writeString, StreamOutput::writeString);\n    }\n\n    public static ElasticsearchException readException(StreamInput input, int id) throws IOException {\n        CheckedFunction<StreamInput, ? extends ElasticsearchException, IOException> elasticsearchException = ID_TO_SUPPLIER.get(id);\n        if (elasticsearchException == null) {\n            throw new IllegalStateException(\"unknown exception for id: \" + id);\n        }\n        return elasticsearchException.apply(input);\n    }\n\n    /**\n     * Returns <code>true</code> iff the given class is a registered for an exception to be read.\n     */\n    public static boolean isRegistered(Class<? extends Throwable> exception, Version version) {\n        ElasticsearchExceptionHandle elasticsearchExceptionHandle = CLASS_TO_ELASTICSEARCH_EXCEPTION_HANDLE.get(exception);\n        if (elasticsearchExceptionHandle != null) {\n            return version.onOrAfter(elasticsearchExceptionHandle.versionAdded);\n        }\n        return false;\n    }\n\n    /**\n     * Returns the serialization id the given exception.\n     */\n    public static int getId(Class<? extends ElasticsearchException> exception) {\n        return CLASS_TO_ELASTICSEARCH_EXCEPTION_HANDLE.get(exception).id;\n    }\n\n    @Override\n    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n        Throwable ex = SQLExceptions.unwrap(this);\n        if (ex != this) {\n            generateThrowableXContent(builder, params, this);\n        } else {\n            innerToXContent(builder, params, this, getExceptionName(), getMessage(), headers, metadata, getCause());\n        }\n        return builder;\n    }\n\n    protected static void innerToXContent(XContentBuilder builder, Params params,\n                                          Throwable throwable, String type, String message, Map<String, List<String>> headers,\n                                          Map<String, List<String>> metadata, Throwable cause) throws IOException {\n        builder.field(TYPE, type);\n        builder.field(REASON, message);\n\n        for (Map.Entry<String, List<String>> entry : metadata.entrySet()) {\n            headerToXContent(builder, entry.getKey().substring(\"es.\".length()), entry.getValue());\n        }\n\n        if (throwable instanceof ElasticsearchException) {\n            ElasticsearchException exception = (ElasticsearchException) throwable;\n            exception.metadataToXContent(builder, params);\n        }\n\n        if (params.paramAsBoolean(REST_EXCEPTION_SKIP_CAUSE, REST_EXCEPTION_SKIP_CAUSE_DEFAULT) == false) {\n            if (cause != null) {\n                builder.field(CAUSED_BY);\n                builder.startObject();\n                generateThrowableXContent(builder, params, cause);\n                builder.endObject();\n            }\n        }\n\n        if (headers.isEmpty() == false) {\n            builder.startObject(HEADER);\n            for (Map.Entry<String, List<String>> entry : headers.entrySet()) {\n                headerToXContent(builder, entry.getKey(), entry.getValue());\n            }\n            builder.endObject();\n        }\n\n        if (params.paramAsBoolean(REST_EXCEPTION_SKIP_STACK_TRACE, REST_EXCEPTION_SKIP_STACK_TRACE_DEFAULT) == false) {\n            builder.field(STACK_TRACE, Exceptions.stackTrace(throwable));\n        }\n\n        Throwable[] allSuppressed = throwable.getSuppressed();\n        if (allSuppressed.length > 0) {\n            builder.startArray(SUPPRESSED.getPreferredName());\n            for (Throwable suppressed : allSuppressed) {\n                builder.startObject();\n                generateThrowableXContent(builder, params, suppressed);\n                builder.endObject();\n            }\n            builder.endArray();\n        }\n    }\n\n    private static void headerToXContent(XContentBuilder builder, String key, List<String> values) throws IOException {\n        if (values != null && values.isEmpty() == false) {\n            if (values.size() == 1) {\n                builder.field(key, values.get(0));\n            } else {\n                builder.startArray(key);\n                for (String value : values) {\n                    builder.value(value);\n                }\n                builder.endArray();\n            }\n        }\n    }\n\n    /**\n     * Renders additional per exception information into the XContent\n     */\n    protected void metadataToXContent(XContentBuilder builder, Params params) throws IOException {\n    }\n\n    /**\n     * Generate a {@link ElasticsearchException} from a {@link XContentParser}. This does not\n     * return the original exception type (ie NodeClosedException for example) but just wraps\n     * the type, the reason and the cause of the exception. It also recursively parses the\n     * tree structure of the cause, returning it as a tree structure of {@link ElasticsearchException}\n     * instances.\n     */\n    public static ElasticsearchException fromXContent(XContentParser parser) throws IOException {\n        XContentParser.Token token = parser.nextToken();\n        ensureExpectedToken(XContentParser.Token.FIELD_NAME, token, parser);\n        return innerFromXContent(parser, false);\n    }\n\n    public static ElasticsearchException innerFromXContent(XContentParser parser, boolean parseRootCauses) throws IOException {\n        XContentParser.Token token = parser.currentToken();\n        ensureExpectedToken(XContentParser.Token.FIELD_NAME, token, parser);\n\n        String type = null, reason = null, stack = null;\n        ElasticsearchException cause = null;\n        Map<String, List<String>> metadata = new HashMap<>();\n        Map<String, List<String>> headers = new HashMap<>();\n        List<ElasticsearchException> rootCauses = new ArrayList<>();\n        List<ElasticsearchException> suppressed = new ArrayList<>();\n\n        for (; token == XContentParser.Token.FIELD_NAME; token = parser.nextToken()) {\n            String currentFieldName = parser.currentName();\n            token = parser.nextToken();\n\n            if (token.isValue()) {\n                if (TYPE.equals(currentFieldName)) {\n                    type = parser.text();\n                } else if (REASON.equals(currentFieldName)) {\n                    reason = parser.text();\n                } else if (STACK_TRACE.equals(currentFieldName)) {\n                    stack = parser.text();\n                } else if (token == XContentParser.Token.VALUE_STRING) {\n                    metadata.put(currentFieldName, Collections.singletonList(parser.text()));\n                }\n            } else if (token == XContentParser.Token.START_OBJECT) {\n                if (CAUSED_BY.equals(currentFieldName)) {\n                    cause = fromXContent(parser);\n                } else if (HEADER.equals(currentFieldName)) {\n                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n                        if (token == XContentParser.Token.FIELD_NAME) {\n                            currentFieldName = parser.currentName();\n                        } else {\n                            List<String> values = headers.getOrDefault(currentFieldName, new ArrayList<>());\n                            if (token == XContentParser.Token.VALUE_STRING) {\n                                values.add(parser.text());\n                            } else if (token == XContentParser.Token.START_ARRAY) {\n                                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {\n                                    if (token == XContentParser.Token.VALUE_STRING) {\n                                        values.add(parser.text());\n                                    } else {\n                                        parser.skipChildren();\n                                    }\n                                }\n                            } else if (token == XContentParser.Token.START_OBJECT) {\n                                parser.skipChildren();\n                            }\n                            headers.put(currentFieldName, values);\n                        }\n                    }\n                } else {\n                    // Any additional metadata object added by the metadataToXContent method is ignored\n                    // and skipped, so that the parser does not fail on unknown fields. The parser only\n                    // support metadata key-pairs and metadata arrays of values.\n                    parser.skipChildren();\n                }\n            } else if (token == XContentParser.Token.START_ARRAY) {\n                if (parseRootCauses && ROOT_CAUSE.equals(currentFieldName)) {\n                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {\n                        rootCauses.add(fromXContent(parser));\n                    }\n                } else if (SUPPRESSED.match(currentFieldName, parser.getDeprecationHandler())) {\n                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {\n                        suppressed.add(fromXContent(parser));\n                    }\n                } else {\n                    // Parse the array and add each item to the corresponding list of metadata.\n                    // Arrays of objects are not supported yet and just ignored and skipped.\n                    List<String> values = new ArrayList<>();\n                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {\n                        if (token == XContentParser.Token.VALUE_STRING) {\n                            values.add(parser.text());\n                        } else {\n                            parser.skipChildren();\n                        }\n                    }\n                    if (values.size() > 0) {\n                        if (metadata.containsKey(currentFieldName)) {\n                            values.addAll(metadata.get(currentFieldName));\n                        }\n                        metadata.put(currentFieldName, values);\n                    }\n                }\n            }\n        }\n\n        ElasticsearchException e = new ElasticsearchException(buildMessage(type, reason, stack), cause);\n        for (Map.Entry<String, List<String>> entry : metadata.entrySet()) {\n            //subclasses can print out additional metadata through the metadataToXContent method. Simple key-value pairs will be\n            //parsed back and become part of this metadata set, while objects and arrays are not supported when parsing back.\n            //Those key-value pairs become part of the metadata set and inherit the \"es.\" prefix as that is currently required\n            //by addMetadata. The prefix will get stripped out when printing metadata out so it will be effectively invisible.\n            //TODO move subclasses that print out simple metadata to using addMetadata directly and support also numbers and booleans.\n            //TODO rename metadataToXContent and have only SearchPhaseExecutionException use it, which prints out complex objects\n            e.addMetadata(\"es.\" + entry.getKey(), entry.getValue());\n        }\n        for (Map.Entry<String, List<String>> header : headers.entrySet()) {\n            e.addHeader(header.getKey(), header.getValue());\n        }\n\n        // Adds root causes as suppressed exception. This way they are not lost\n        // after parsing and can be retrieved using getSuppressed() method.\n        for (ElasticsearchException rootCause : rootCauses) {\n            e.addSuppressed(rootCause);\n        }\n        for (ElasticsearchException s : suppressed) {\n            e.addSuppressed(s);\n        }\n        return e;\n    }\n\n    /**\n     * Static toXContent helper method that renders {@link org.elasticsearch.ElasticsearchException} or {@link Throwable} instances\n     * as XContent, delegating the rendering to {@link #toXContent(XContentBuilder, Params)}\n     * or {@link #innerToXContent(XContentBuilder, Params, Throwable, String, String, Map, Map, Throwable)}.\n     *\n     * This method is usually used when the {@link Throwable} is rendered as a part of another XContent object, and its result can\n     * be parsed back using the {@link #fromXContent(XContentParser)} method.\n     */\n    public static void generateThrowableXContent(XContentBuilder builder, Params params, Throwable t) throws IOException {\n        t = SQLExceptions.unwrap(t);\n\n        if (t instanceof ElasticsearchException) {\n            ((ElasticsearchException) t).toXContent(builder, params);\n        } else {\n            innerToXContent(builder, params, t, getExceptionName(t), t.getMessage(), emptyMap(), emptyMap(), t.getCause());\n        }\n    }\n\n    protected String getExceptionName() {\n        return getExceptionName(this);\n    }\n\n    /**\n     * Returns a underscore case name for the given exception. This method strips {@code Elasticsearch} prefixes from exception names.\n     */\n    public static String getExceptionName(Throwable ex) {\n        String simpleName = ex.getClass().getSimpleName();\n        if (simpleName.startsWith(\"Elasticsearch\")) {\n            simpleName = simpleName.substring(\"Elasticsearch\".length());\n        }\n        // TODO: do we really need to make the exception name in underscore casing?\n        return toUnderscoreCase(simpleName);\n    }\n\n    static String buildMessage(String type, String reason, String stack) {\n        StringBuilder message = new StringBuilder(\"Elasticsearch exception [\");\n        message.append(TYPE).append('=').append(type).append(\", \");\n        message.append(REASON).append('=').append(reason);\n        if (stack != null) {\n            message.append(\", \").append(STACK_TRACE).append('=').append(stack);\n        }\n        message.append(']');\n        return message.toString();\n    }\n\n    @Override\n    public String toString() {\n        StringBuilder builder = new StringBuilder();\n        if (metadata.containsKey(INDEX_METADATA_KEY)) {\n            builder.append(getIndex());\n            if (metadata.containsKey(SHARD_METADATA_KEY)) {\n                builder.append('[').append(getShardId()).append(']');\n            }\n            builder.append(' ');\n        }\n        return builder.append(super.toString().trim()).toString();\n    }\n\n    /**\n     * Deserializes stacktrace elements as well as suppressed exceptions from the given output stream and\n     * adds it to the given exception.\n     */\n    public static <T extends Throwable> T readStackTrace(T throwable, StreamInput in) throws IOException {\n        throwable.setStackTrace(in.readArray(i -> {\n            final String declaringClasss = i.readString();\n            final String fileName = i.readOptionalString();\n            final String methodName = i.readString();\n            final int lineNumber = i.readVInt();\n            return new StackTraceElement(declaringClasss, methodName, fileName, lineNumber);\n        }, StackTraceElement[]::new));\n\n        int numSuppressed = in.readVInt();\n        for (int i = 0; i < numSuppressed; i++) {\n            throwable.addSuppressed(in.readException());\n        }\n        return throwable;\n    }\n\n    /**\n     * Serializes the given exceptions stacktrace elements as well as it's suppressed exceptions to the given output stream.\n     */\n    public static <T extends Throwable> T writeStackTraces(T throwable, StreamOutput out,\n                                                           Writer<Throwable> exceptionWriter) throws IOException {\n        out.writeArray((o, v) -> {\n            o.writeString(v.getClassName());\n            o.writeOptionalString(v.getFileName());\n            o.writeString(v.getMethodName());\n            o.writeVInt(v.getLineNumber());\n        }, throwable.getStackTrace());\n        out.writeArray(exceptionWriter, throwable.getSuppressed());\n        return throwable;\n    }\n\n    /**\n     * This is the list of Exceptions Elasticsearch can throw over the wire or save into a corruption marker. Each value in the enum is a\n     * single exception tying the Class to an id for use of the encode side and the id back to a constructor for use on the decode side. As\n     * such its ok if the exceptions to change names so long as their constructor can still read the exception. Each exception is listed\n     * in id order below. If you want to remove an exception leave a tombstone comment and mark the id as null in\n     * ExceptionSerializationTests.testIds.ids.\n     */\n    private enum ElasticsearchExceptionHandle {\n        INDEX_SHARD_SNAPSHOT_FAILED_EXCEPTION(org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException.class,\n                org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException::new, 0, UNKNOWN_VERSION_ADDED),\n        // 1 was DfsPhaseExecutionException\n        EXECUTION_CANCELLED_EXCEPTION(org.elasticsearch.common.util.CancellableThreads.ExecutionCancelledException.class,\n                org.elasticsearch.common.util.CancellableThreads.ExecutionCancelledException::new, 2, UNKNOWN_VERSION_ADDED),\n        MASTER_NOT_DISCOVERED_EXCEPTION(org.elasticsearch.discovery.MasterNotDiscoveredException.class,\n                org.elasticsearch.discovery.MasterNotDiscoveredException::new, 3, UNKNOWN_VERSION_ADDED),\n        ELASTICSEARCH_SECURITY_EXCEPTION(org.elasticsearch.ElasticsearchSecurityException.class,\n                org.elasticsearch.ElasticsearchSecurityException::new, 4, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_RESTORE_EXCEPTION(org.elasticsearch.index.snapshots.IndexShardRestoreException.class,\n                org.elasticsearch.index.snapshots.IndexShardRestoreException::new, 5, UNKNOWN_VERSION_ADDED),\n        INDEX_CLOSED_EXCEPTION(org.elasticsearch.indices.IndexClosedException.class,\n                org.elasticsearch.indices.IndexClosedException::new, 6, UNKNOWN_VERSION_ADDED),\n        BIND_HTTP_EXCEPTION(org.elasticsearch.http.BindHttpException.class,\n                org.elasticsearch.http.BindHttpException::new, 7, UNKNOWN_VERSION_ADDED),\n        // 8 was ReduceSearchPhaseException\n        NODE_CLOSED_EXCEPTION(org.elasticsearch.node.NodeClosedException.class,\n                org.elasticsearch.node.NodeClosedException::new, 9, UNKNOWN_VERSION_ADDED),\n        SNAPSHOT_FAILED_ENGINE_EXCEPTION(org.elasticsearch.index.engine.SnapshotFailedEngineException.class,\n                org.elasticsearch.index.engine.SnapshotFailedEngineException::new, 10, UNKNOWN_VERSION_ADDED),\n        SHARD_NOT_FOUND_EXCEPTION(org.elasticsearch.index.shard.ShardNotFoundException.class,\n                org.elasticsearch.index.shard.ShardNotFoundException::new, 11, UNKNOWN_VERSION_ADDED),\n        CONNECT_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ConnectTransportException.class,\n                org.elasticsearch.transport.ConnectTransportException::new, 12, UNKNOWN_VERSION_ADDED),\n        NOT_SERIALIZABLE_TRANSPORT_EXCEPTION(org.elasticsearch.transport.NotSerializableTransportException.class,\n                org.elasticsearch.transport.NotSerializableTransportException::new, 13, UNKNOWN_VERSION_ADDED),\n        RESPONSE_HANDLER_FAILURE_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ResponseHandlerFailureTransportException.class,\n                org.elasticsearch.transport.ResponseHandlerFailureTransportException::new, 14, UNKNOWN_VERSION_ADDED),\n        INDEX_CREATION_EXCEPTION(org.elasticsearch.indices.IndexCreationException.class,\n                org.elasticsearch.indices.IndexCreationException::new, 15, UNKNOWN_VERSION_ADDED),\n        INDEX_NOT_FOUND_EXCEPTION(org.elasticsearch.index.IndexNotFoundException.class,\n                org.elasticsearch.index.IndexNotFoundException::new, 16, UNKNOWN_VERSION_ADDED),\n        ILLEGAL_SHARD_ROUTING_STATE_EXCEPTION(org.elasticsearch.cluster.routing.IllegalShardRoutingStateException.class,\n                org.elasticsearch.cluster.routing.IllegalShardRoutingStateException::new, 17, UNKNOWN_VERSION_ADDED),\n        BROADCAST_SHARD_OPERATION_FAILED_EXCEPTION(org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException.class,\n                org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException::new, 18, UNKNOWN_VERSION_ADDED),\n        RESOURCE_NOT_FOUND_EXCEPTION(org.elasticsearch.ResourceNotFoundException.class,\n                org.elasticsearch.ResourceNotFoundException::new, 19, UNKNOWN_VERSION_ADDED),\n        ACTION_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ActionTransportException.class,\n                org.elasticsearch.transport.ActionTransportException::new, 20, UNKNOWN_VERSION_ADDED),\n        ELASTICSEARCH_GENERATION_EXCEPTION(org.elasticsearch.ElasticsearchGenerationException.class,\n                org.elasticsearch.ElasticsearchGenerationException::new, 21, UNKNOWN_VERSION_ADDED),\n        //      22 was CreateFailedEngineException\n        INDEX_SHARD_STARTED_EXCEPTION(org.elasticsearch.index.shard.IndexShardStartedException.class,\n                org.elasticsearch.index.shard.IndexShardStartedException::new, 23, UNKNOWN_VERSION_ADDED),\n        // 24 was SearchContextMissingException\n        // 25 was GeneralScriptException\n        // 26 was BatchOperationException\n        // 27 was SnapshotCreationException\n        // 28 was DeleteFailedEngineException, deprecated in 6.0, removed in 7.0\n        DOCUMENT_MISSING_EXCEPTION(org.elasticsearch.index.engine.DocumentMissingException.class,\n                org.elasticsearch.index.engine.DocumentMissingException::new, 29, UNKNOWN_VERSION_ADDED),\n        SNAPSHOT_EXCEPTION(org.elasticsearch.snapshots.SnapshotException.class,\n                org.elasticsearch.snapshots.SnapshotException::new, 30, UNKNOWN_VERSION_ADDED),\n        INVALID_ALIAS_NAME_EXCEPTION(org.elasticsearch.indices.InvalidAliasNameException.class,\n                org.elasticsearch.indices.InvalidAliasNameException::new, 31, UNKNOWN_VERSION_ADDED),\n        INVALID_INDEX_NAME_EXCEPTION(org.elasticsearch.indices.InvalidIndexNameException.class,\n                org.elasticsearch.indices.InvalidIndexNameException::new, 32, UNKNOWN_VERSION_ADDED),\n        INDEX_PRIMARY_SHARD_NOT_ALLOCATED_EXCEPTION(org.elasticsearch.indices.IndexPrimaryShardNotAllocatedException.class,\n                org.elasticsearch.indices.IndexPrimaryShardNotAllocatedException::new, 33, UNKNOWN_VERSION_ADDED),\n        TRANSPORT_EXCEPTION(org.elasticsearch.transport.TransportException.class,\n                org.elasticsearch.transport.TransportException::new, 34, UNKNOWN_VERSION_ADDED),\n        ELASTICSEARCH_PARSE_EXCEPTION(org.elasticsearch.ElasticsearchParseException.class,\n                org.elasticsearch.ElasticsearchParseException::new, 35, UNKNOWN_VERSION_ADDED),\n        // 36 was SearchException\n        MAPPER_EXCEPTION(org.elasticsearch.index.mapper.MapperException.class,\n                org.elasticsearch.index.mapper.MapperException::new, 37, UNKNOWN_VERSION_ADDED),\n        INVALID_TYPE_NAME_EXCEPTION(org.elasticsearch.indices.InvalidTypeNameException.class,\n                org.elasticsearch.indices.InvalidTypeNameException::new, 38, UNKNOWN_VERSION_ADDED),\n        SNAPSHOT_RESTORE_EXCEPTION(org.elasticsearch.snapshots.SnapshotRestoreException.class,\n                org.elasticsearch.snapshots.SnapshotRestoreException::new, 39, UNKNOWN_VERSION_ADDED),\n        PARSING_EXCEPTION(org.elasticsearch.common.ParsingException.class, org.elasticsearch.common.ParsingException::new, 40,\n            UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_CLOSED_EXCEPTION(org.elasticsearch.index.shard.IndexShardClosedException.class,\n                org.elasticsearch.index.shard.IndexShardClosedException::new, 41, UNKNOWN_VERSION_ADDED),\n        RECOVER_FILES_RECOVERY_EXCEPTION(org.elasticsearch.indices.recovery.RecoverFilesRecoveryException.class,\n                org.elasticsearch.indices.recovery.RecoverFilesRecoveryException::new, 42, UNKNOWN_VERSION_ADDED),\n        TRUNCATED_TRANSLOG_EXCEPTION(org.elasticsearch.index.translog.TruncatedTranslogException.class,\n                org.elasticsearch.index.translog.TruncatedTranslogException::new, 43, UNKNOWN_VERSION_ADDED),\n        RECOVERY_FAILED_EXCEPTION(org.elasticsearch.indices.recovery.RecoveryFailedException.class,\n                org.elasticsearch.indices.recovery.RecoveryFailedException::new, 44, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_RELOCATED_EXCEPTION(org.elasticsearch.index.shard.IndexShardRelocatedException.class,\n                org.elasticsearch.index.shard.IndexShardRelocatedException::new, 45, UNKNOWN_VERSION_ADDED),\n        NODE_SHOULD_NOT_CONNECT_EXCEPTION(org.elasticsearch.transport.NodeShouldNotConnectException.class,\n                org.elasticsearch.transport.NodeShouldNotConnectException::new, 46, UNKNOWN_VERSION_ADDED),\n        // 47 used to be for IndexTemplateAlreadyExistsException which was deprecated in 5.1 removed in 6.0\n        TRANSLOG_CORRUPTED_EXCEPTION(org.elasticsearch.index.translog.TranslogCorruptedException.class,\n                org.elasticsearch.index.translog.TranslogCorruptedException::new, 48, UNKNOWN_VERSION_ADDED),\n        CLUSTER_BLOCK_EXCEPTION(org.elasticsearch.cluster.block.ClusterBlockException.class,\n                org.elasticsearch.cluster.block.ClusterBlockException::new, 49, UNKNOWN_VERSION_ADDED),\n        // 50 was FetchPhaseExecutionException\n        // 51 used to be for IndexShardAlreadyExistsException which was deprecated in 5.1 removed in 6.0\n        VERSION_CONFLICT_ENGINE_EXCEPTION(org.elasticsearch.index.engine.VersionConflictEngineException.class,\n                org.elasticsearch.index.engine.VersionConflictEngineException::new, 52, UNKNOWN_VERSION_ADDED),\n        ENGINE_EXCEPTION(org.elasticsearch.index.engine.EngineException.class, org.elasticsearch.index.engine.EngineException::new, 53,\n            UNKNOWN_VERSION_ADDED),\n        // 54 was DocumentAlreadyExistsException, which is superseded by VersionConflictEngineException\n        NO_SUCH_NODE_EXCEPTION(org.elasticsearch.action.NoSuchNodeException.class, org.elasticsearch.action.NoSuchNodeException::new, 55,\n            UNKNOWN_VERSION_ADDED),\n        SETTINGS_EXCEPTION(org.elasticsearch.common.settings.SettingsException.class,\n                org.elasticsearch.common.settings.SettingsException::new, 56, UNKNOWN_VERSION_ADDED),\n        INDEX_TEMPLATE_MISSING_EXCEPTION(org.elasticsearch.indices.IndexTemplateMissingException.class,\n                org.elasticsearch.indices.IndexTemplateMissingException::new, 57, UNKNOWN_VERSION_ADDED),\n        SEND_REQUEST_TRANSPORT_EXCEPTION(org.elasticsearch.transport.SendRequestTransportException.class,\n                org.elasticsearch.transport.SendRequestTransportException::new, 58, UNKNOWN_VERSION_ADDED),\n        // 59 used to be EsRejectedExecutionException\n        // 60 used to be for EarlyTerminationException\n        // 61 used to be for RoutingValidationException\n        NOT_SERIALIZABLE_EXCEPTION_WRAPPER(org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.class,\n                org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper::new, 62, UNKNOWN_VERSION_ADDED),\n        // 63 was AliasFilterParsingException\n        // 64 was DeleteByQueryFailedEngineException, which was removed in 5.0\n        GATEWAY_EXCEPTION(org.elasticsearch.gateway.GatewayException.class, org.elasticsearch.gateway.GatewayException::new, 65,\n            UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_NOT_RECOVERING_EXCEPTION(org.elasticsearch.index.shard.IndexShardNotRecoveringException.class,\n                org.elasticsearch.index.shard.IndexShardNotRecoveringException::new, 66, UNKNOWN_VERSION_ADDED),\n        HTTP_EXCEPTION(org.elasticsearch.http.HttpException.class, org.elasticsearch.http.HttpException::new, 67, UNKNOWN_VERSION_ADDED),\n        ELASTICSEARCH_EXCEPTION(org.elasticsearch.ElasticsearchException.class,\n                org.elasticsearch.ElasticsearchException::new, 68, UNKNOWN_VERSION_ADDED),\n        SNAPSHOT_MISSING_EXCEPTION(org.elasticsearch.snapshots.SnapshotMissingException.class,\n                org.elasticsearch.snapshots.SnapshotMissingException::new, 69, UNKNOWN_VERSION_ADDED),\n        PRIMARY_MISSING_ACTION_EXCEPTION(org.elasticsearch.action.PrimaryMissingActionException.class,\n                org.elasticsearch.action.PrimaryMissingActionException::new, 70, UNKNOWN_VERSION_ADDED),\n        FAILED_NODE_EXCEPTION(org.elasticsearch.action.FailedNodeException.class, org.elasticsearch.action.FailedNodeException::new, 71,\n            UNKNOWN_VERSION_ADDED),\n        // 72 was SearchParseException\n        CONCURRENT_SNAPSHOT_EXECUTION_EXCEPTION(org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException.class,\n                org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException::new, 73, UNKNOWN_VERSION_ADDED),\n        BLOB_STORE_EXCEPTION(org.elasticsearch.common.blobstore.BlobStoreException.class,\n                org.elasticsearch.common.blobstore.BlobStoreException::new, 74, UNKNOWN_VERSION_ADDED),\n        INCOMPATIBLE_CLUSTER_STATE_VERSION_EXCEPTION(org.elasticsearch.cluster.IncompatibleClusterStateVersionException.class,\n                org.elasticsearch.cluster.IncompatibleClusterStateVersionException::new, 75, UNKNOWN_VERSION_ADDED),\n        RECOVERY_ENGINE_EXCEPTION(org.elasticsearch.index.engine.RecoveryEngineException.class,\n                org.elasticsearch.index.engine.RecoveryEngineException::new, 76, UNKNOWN_VERSION_ADDED),\n        UNCATEGORIZED_EXECUTION_EXCEPTION(org.elasticsearch.common.util.concurrent.UncategorizedExecutionException.class,\n                org.elasticsearch.common.util.concurrent.UncategorizedExecutionException::new, 77, UNKNOWN_VERSION_ADDED),\n        // 78 was TimestampParsingException\n        // 79 was RoutingMissingException\n        // 80 was IndexFailedEngineException, deprecated in 6.0, removed in 7.0\n        INDEX_SHARD_RESTORE_FAILED_EXCEPTION(org.elasticsearch.index.snapshots.IndexShardRestoreFailedException.class,\n                org.elasticsearch.index.snapshots.IndexShardRestoreFailedException::new, 81, UNKNOWN_VERSION_ADDED),\n        REPOSITORY_EXCEPTION(org.elasticsearch.repositories.RepositoryException.class,\n                org.elasticsearch.repositories.RepositoryException::new, 82, UNKNOWN_VERSION_ADDED),\n        RECEIVE_TIMEOUT_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ReceiveTimeoutTransportException.class,\n                org.elasticsearch.transport.ReceiveTimeoutTransportException::new, 83, UNKNOWN_VERSION_ADDED),\n        NODE_DISCONNECTED_EXCEPTION(org.elasticsearch.transport.NodeDisconnectedException.class,\n                org.elasticsearch.transport.NodeDisconnectedException::new, 84, UNKNOWN_VERSION_ADDED),\n        // 85 used to be for AlreadyExpiredException\n        // 86 used to be for AggregationExecutionException\n        // 87 used to be for MergeMappingException\n        INVALID_INDEX_TEMPLATE_EXCEPTION(org.elasticsearch.indices.InvalidIndexTemplateException.class,\n                org.elasticsearch.indices.InvalidIndexTemplateException::new, 88, UNKNOWN_VERSION_ADDED),\n        REFRESH_FAILED_ENGINE_EXCEPTION(org.elasticsearch.index.engine.RefreshFailedEngineException.class,\n                org.elasticsearch.index.engine.RefreshFailedEngineException::new, 90, UNKNOWN_VERSION_ADDED),\n        // 91 used to be for AggregationInitializationException\n        DELAY_RECOVERY_EXCEPTION(org.elasticsearch.indices.recovery.DelayRecoveryException.class,\n                org.elasticsearch.indices.recovery.DelayRecoveryException::new, 92, UNKNOWN_VERSION_ADDED),\n        // 93 used to be for IndexWarmerMissingException\n        NO_NODE_AVAILABLE_EXCEPTION(org.elasticsearch.client.transport.NoNodeAvailableException.class,\n                org.elasticsearch.client.transport.NoNodeAvailableException::new, 94, UNKNOWN_VERSION_ADDED),\n        INVALID_SNAPSHOT_NAME_EXCEPTION(org.elasticsearch.snapshots.InvalidSnapshotNameException.class,\n                org.elasticsearch.snapshots.InvalidSnapshotNameException::new, 96, UNKNOWN_VERSION_ADDED),\n        ILLEGAL_INDEX_SHARD_STATE_EXCEPTION(org.elasticsearch.index.shard.IllegalIndexShardStateException.class,\n                org.elasticsearch.index.shard.IllegalIndexShardStateException::new, 97, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_SNAPSHOT_EXCEPTION(org.elasticsearch.index.snapshots.IndexShardSnapshotException.class,\n                org.elasticsearch.index.snapshots.IndexShardSnapshotException::new, 98, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_NOT_STARTED_EXCEPTION(org.elasticsearch.index.shard.IndexShardNotStartedException.class,\n                org.elasticsearch.index.shard.IndexShardNotStartedException::new, 99, UNKNOWN_VERSION_ADDED),\n        // 100 used to bew for SearchPhaseExecutionException\n        ACTION_NOT_FOUND_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ActionNotFoundTransportException.class,\n                org.elasticsearch.transport.ActionNotFoundTransportException::new, 101, UNKNOWN_VERSION_ADDED),\n        TRANSPORT_SERIALIZATION_EXCEPTION(org.elasticsearch.transport.TransportSerializationException.class,\n                org.elasticsearch.transport.TransportSerializationException::new, 102, UNKNOWN_VERSION_ADDED),\n        REMOTE_TRANSPORT_EXCEPTION(org.elasticsearch.transport.RemoteTransportException.class,\n                org.elasticsearch.transport.RemoteTransportException::new, 103, UNKNOWN_VERSION_ADDED),\n        ENGINE_CREATION_FAILURE_EXCEPTION(org.elasticsearch.index.engine.EngineCreationFailureException.class,\n                org.elasticsearch.index.engine.EngineCreationFailureException::new, 104, UNKNOWN_VERSION_ADDED),\n        ROUTING_EXCEPTION(org.elasticsearch.cluster.routing.RoutingException.class,\n                org.elasticsearch.cluster.routing.RoutingException::new, 105, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_RECOVERY_EXCEPTION(org.elasticsearch.index.shard.IndexShardRecoveryException.class,\n                org.elasticsearch.index.shard.IndexShardRecoveryException::new, 106, UNKNOWN_VERSION_ADDED),\n        REPOSITORY_MISSING_EXCEPTION(org.elasticsearch.repositories.RepositoryMissingException.class,\n                org.elasticsearch.repositories.RepositoryMissingException::new, 107, UNKNOWN_VERSION_ADDED),\n        DOCUMENT_SOURCE_MISSING_EXCEPTION(org.elasticsearch.index.engine.DocumentSourceMissingException.class,\n                org.elasticsearch.index.engine.DocumentSourceMissingException::new, 109, UNKNOWN_VERSION_ADDED),\n        // 110 used to be FlushNotAllowedEngineException\n        NO_CLASS_SETTINGS_EXCEPTION(org.elasticsearch.common.settings.NoClassSettingsException.class,\n                org.elasticsearch.common.settings.NoClassSettingsException::new, 111, UNKNOWN_VERSION_ADDED),\n        BIND_TRANSPORT_EXCEPTION(org.elasticsearch.transport.BindTransportException.class,\n                org.elasticsearch.transport.BindTransportException::new, 112, UNKNOWN_VERSION_ADDED),\n        ALIASES_NOT_FOUND_EXCEPTION(AliasesNotFoundException.class,\n                AliasesNotFoundException::new, 113, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_RECOVERING_EXCEPTION(org.elasticsearch.index.shard.IndexShardRecoveringException.class,\n                org.elasticsearch.index.shard.IndexShardRecoveringException::new, 114, UNKNOWN_VERSION_ADDED),\n        TRANSLOG_EXCEPTION(org.elasticsearch.index.translog.TranslogException.class,\n                org.elasticsearch.index.translog.TranslogException::new, 115, UNKNOWN_VERSION_ADDED),\n        PROCESS_CLUSTER_EVENT_TIMEOUT_EXCEPTION(org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException.class,\n                org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException::new, 116, UNKNOWN_VERSION_ADDED),\n        RETRY_ON_PRIMARY_EXCEPTION(ReplicationOperation.RetryOnPrimaryException.class,\n                ReplicationOperation.RetryOnPrimaryException::new, 117, UNKNOWN_VERSION_ADDED),\n        ELASTICSEARCH_TIMEOUT_EXCEPTION(org.elasticsearch.ElasticsearchTimeoutException.class,\n                org.elasticsearch.ElasticsearchTimeoutException::new, 118, UNKNOWN_VERSION_ADDED),\n        // 119 was QueryPhaseExecutionException\n        REPOSITORY_VERIFICATION_EXCEPTION(org.elasticsearch.repositories.RepositoryVerificationException.class,\n                org.elasticsearch.repositories.RepositoryVerificationException::new, 120, UNKNOWN_VERSION_ADDED),\n        // 121 was InvalidAggregationPathException\n        // 123 used to be IndexAlreadyExistsException and was renamed\n        RESOURCE_ALREADY_EXISTS_EXCEPTION(ResourceAlreadyExistsException.class,\n            ResourceAlreadyExistsException::new, 123, UNKNOWN_VERSION_ADDED),\n        // 124 used to be Script.ScriptParseException\n        HTTP_REQUEST_ON_TRANSPORT_EXCEPTION(TcpTransport.HttpRequestOnTransportException.class,\n                TcpTransport.HttpRequestOnTransportException::new, 125, UNKNOWN_VERSION_ADDED),\n        MAPPER_PARSING_EXCEPTION(org.elasticsearch.index.mapper.MapperParsingException.class,\n                org.elasticsearch.index.mapper.MapperParsingException::new, 126, UNKNOWN_VERSION_ADDED),\n        // 127 was SearchContextException\n        // 128 was SearchSourceBuilderException\n        // 129 was EngineClosedException\n        NO_SHARD_AVAILABLE_ACTION_EXCEPTION(org.elasticsearch.action.NoShardAvailableActionException.class,\n                org.elasticsearch.action.NoShardAvailableActionException::new, 130, UNKNOWN_VERSION_ADDED),\n        UNAVAILABLE_SHARDS_EXCEPTION(org.elasticsearch.action.UnavailableShardsException.class,\n                org.elasticsearch.action.UnavailableShardsException::new, 131, UNKNOWN_VERSION_ADDED),\n        FLUSH_FAILED_ENGINE_EXCEPTION(org.elasticsearch.index.engine.FlushFailedEngineException.class,\n                org.elasticsearch.index.engine.FlushFailedEngineException::new, 132, UNKNOWN_VERSION_ADDED),\n        CIRCUIT_BREAKING_EXCEPTION(org.elasticsearch.common.breaker.CircuitBreakingException.class,\n                org.elasticsearch.common.breaker.CircuitBreakingException::new, 133, UNKNOWN_VERSION_ADDED),\n        NODE_NOT_CONNECTED_EXCEPTION(org.elasticsearch.transport.NodeNotConnectedException.class,\n                org.elasticsearch.transport.NodeNotConnectedException::new, 134, UNKNOWN_VERSION_ADDED),\n        STRICT_DYNAMIC_MAPPING_EXCEPTION(org.elasticsearch.index.mapper.StrictDynamicMappingException.class,\n                org.elasticsearch.index.mapper.StrictDynamicMappingException::new, 135, UNKNOWN_VERSION_ADDED),\n        RETRY_ON_REPLICA_EXCEPTION(org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnReplicaException.class,\n                org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnReplicaException::new, 136,\n            UNKNOWN_VERSION_ADDED),\n        TYPE_MISSING_EXCEPTION(org.elasticsearch.indices.TypeMissingException.class,\n                org.elasticsearch.indices.TypeMissingException::new, 137, UNKNOWN_VERSION_ADDED),\n        FAILED_TO_COMMIT_CLUSTER_STATE_EXCEPTION(org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException.class,\n                org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException::new, 140, UNKNOWN_VERSION_ADDED),\n        QUERY_SHARD_EXCEPTION(org.elasticsearch.index.query.QueryShardException.class,\n                org.elasticsearch.index.query.QueryShardException::new, 141, UNKNOWN_VERSION_ADDED),\n        NO_LONGER_PRIMARY_SHARD_EXCEPTION(ShardStateAction.NoLongerPrimaryShardException.class,\n                ShardStateAction.NoLongerPrimaryShardException::new, 142, UNKNOWN_VERSION_ADDED),\n        // 143 was ScriptException\n        NOT_MASTER_EXCEPTION(org.elasticsearch.cluster.NotMasterException.class, org.elasticsearch.cluster.NotMasterException::new, 144,\n            UNKNOWN_VERSION_ADDED),\n        STATUS_EXCEPTION(org.elasticsearch.ElasticsearchStatusException.class, org.elasticsearch.ElasticsearchStatusException::new, 145,\n            UNKNOWN_VERSION_ADDED),\n        TASK_CANCELLED_EXCEPTION(org.elasticsearch.tasks.TaskCancelledException.class,\n            org.elasticsearch.tasks.TaskCancelledException::new, 146, UNKNOWN_VERSION_ADDED),\n        SHARD_LOCK_OBTAIN_FAILED_EXCEPTION(org.elasticsearch.env.ShardLockObtainFailedException.class,\n                                           org.elasticsearch.env.ShardLockObtainFailedException::new, 147, UNKNOWN_VERSION_ADDED),\n        // 148 was UnknownNamedObjectException\n        // 149 was TooManyBucketsException\n        COORDINATION_STATE_REJECTED_EXCEPTION(org.elasticsearch.cluster.coordination.CoordinationStateRejectedException.class,\n            org.elasticsearch.cluster.coordination.CoordinationStateRejectedException::new, 150, Version.V_4_0_0),\n\n        RETENTION_LEASE_ALREADY_EXISTS_EXCEPTION(\n                org.elasticsearch.index.seqno.RetentionLeaseAlreadyExistsException.class,\n                org.elasticsearch.index.seqno.RetentionLeaseAlreadyExistsException::new,\n                153,\n                Version.V_4_3_0),\n        RETENTION_LEASE_NOT_FOUND_EXCEPTION(\n                org.elasticsearch.index.seqno.RetentionLeaseNotFoundException.class,\n                org.elasticsearch.index.seqno.RetentionLeaseNotFoundException::new,\n                154,\n                Version.V_4_3_0),\n        SHARD_NOT_IN_PRIMARY_MODE_EXCEPTION(\n                org.elasticsearch.index.shard.ShardNotInPrimaryModeException.class,\n                org.elasticsearch.index.shard.ShardNotInPrimaryModeException::new,\n                155,\n                Version.V_4_3_0),\n        JOB_KILLED_EXCEPTION(\n            io.crate.exceptions.JobKilledException.class,\n            io.crate.exceptions.JobKilledException::new,\n                156,\n            Version.V_4_3_0),\n        TASK_MISSING_EXCEPTION(\n            io.crate.exceptions.TaskMissing.class,\n            io.crate.exceptions.TaskMissing::new,\n                157,\n            Version.V_4_3_0),\n        RETENTION_LEASE_INVALID_RETAINING_SEQUENCE_NUMBER_EXCEPTION(\n            org.elasticsearch.index.seqno.RetentionLeaseInvalidRetainingSeqNoException.class,\n            org.elasticsearch.index.seqno.RetentionLeaseInvalidRetainingSeqNoException::new,\n            158,\n            Version.V_4_3_0),\n        RELATION_ALREADY_EXISTS(\n            io.crate.exceptions.RelationAlreadyExists.class,\n            io.crate.exceptions.RelationAlreadyExists::new,\n            159,\n            Version.V_4_7_0),\n        RELATION_UNKNOWN(\n            io.crate.exceptions.RelationUnknown.class,\n            io.crate.exceptions.RelationUnknown::new,\n            160,\n            Version.V_4_7_0),\n        SCHEMA_UNKNOWN(\n            io.crate.exceptions.SchemaUnknownException.class,\n            io.crate.exceptions.SchemaUnknownException::new,\n            161,\n            Version.V_4_7_0),\n        GROUP_BY_ON_ARRAY_UNSUPPORTED_EXCEPTION(\n            ArrayViaDocValuesUnsupportedException.class,\n            ArrayViaDocValuesUnsupportedException::new,\n            162,\n            Version.V_4_7_0),\n        INVALID_ARGUMENT_EXCEPTION(\n            io.crate.exceptions.InvalidArgumentException.class,\n            io.crate.exceptions.InvalidArgumentException::new,\n            163,\n            Version.V_4_7_0),\n        INVALID_COLUMN_NAME_EXCEPTION(\n            io.crate.exceptions.InvalidColumnNameException.class,\n            io.crate.exceptions.InvalidColumnNameException::new,\n            164,\n            Version.V_4_7_0),\n        UNSUPPORTED_FEATURE_EXCEPTION(\n            io.crate.exceptions.UnsupportedFeatureException.class,\n            io.crate.exceptions.UnsupportedFeatureException::new,\n            165,\n            Version.V_4_7_0),\n        USER_DEFINED_FUNCTION_ALREADY_EXISTS_EXCEPTION(\n            io.crate.exceptions.UserDefinedFunctionAlreadyExistsException.class,\n            io.crate.exceptions.UserDefinedFunctionAlreadyExistsException::new,\n            166,\n            Version.V_4_7_0),\n        USER_DEFINED_FUNCTION_UNKNOWN_EXCEPTION(\n            io.crate.exceptions.UserDefinedFunctionUnknownException.class,\n            io.crate.exceptions.UserDefinedFunctionUnknownException::new,\n            167,\n            Version.V_4_7_0),\n        VERSIONING_VALIDATION_EXCEPTION(\n            io.crate.exceptions.VersioningValidationException.class,\n            io.crate.exceptions.VersioningValidationException::new,\n            168,\n            Version.V_4_7_0),\n        PUBLICATION_ALREADY_EXISTS_EXCEPTION(\n            io.crate.replication.logical.exceptions.PublicationAlreadyExistsException.class,\n            io.crate.replication.logical.exceptions.PublicationAlreadyExistsException::new,\n            169,\n            Version.V_4_7_0),\n        PUBLICATION_UNKNOWN_EXCEPTION(\n            io.crate.replication.logical.exceptions.PublicationUnknownException.class,\n            io.crate.replication.logical.exceptions.PublicationUnknownException::new,\n            170,\n            Version.V_4_7_0),\n        SUBSCRIPTION_ALREADY_EXISTS_EXCEPTION(\n            io.crate.replication.logical.exceptions.SubscriptionAlreadyExistsException.class,\n            io.crate.replication.logical.exceptions.SubscriptionAlreadyExistsException::new,\n            171,\n            Version.V_4_7_0),\n        SUBSCRIPTION_UNKNOWN_EXCEPTION(\n            io.crate.replication.logical.exceptions.SubscriptionUnknownException.class,\n            io.crate.replication.logical.exceptions.SubscriptionUnknownException::new,\n            172,\n            Version.V_4_7_0),\n        MISSING_SHARD_OPERATIONS_EXCEPTION(\n            io.crate.replication.logical.exceptions.MissingShardOperationsException.class,\n            io.crate.replication.logical.exceptions.MissingShardOperationsException::new,\n            173,\n            Version.V_4_7_0),\n        PEER_RECOVERY_NOT_FOUND_EXCEPTION(\n            org.elasticsearch.indices.recovery.PeerRecoveryNotFound.class,\n            org.elasticsearch.indices.recovery.PeerRecoveryNotFound::new,\n            174,\n            Version.V_5_1_0),\n        NODE_HEALTH_CHECK_FAILURE_EXCEPTION(\n            org.elasticsearch.cluster.coordination.NodeHealthCheckFailureException.class,\n            org.elasticsearch.cluster.coordination.NodeHealthCheckFailureException::new,\n            175,\n            Version.V_5_2_0),\n        OPERATION_ON_INACCESSIBLE_RELATION_EXCEPTION(\n            io.crate.exceptions.OperationOnInaccessibleRelationException.class,\n            io.crate.exceptions.OperationOnInaccessibleRelationException::new,\n            176,\n            Version.V_5_6_0);\n\n        final Class<? extends ElasticsearchException> exceptionClass;\n        final CheckedFunction<StreamInput, ? extends ElasticsearchException, IOException> constructor;\n        final int id;\n        final Version versionAdded;\n\n        <E extends ElasticsearchException> ElasticsearchExceptionHandle(Class<E> exceptionClass,\n                                                                        CheckedFunction<StreamInput, E, IOException> constructor, int id,\n                                                                        Version versionAdded) {\n            // We need the exceptionClass because you can't dig it out of the constructor reliably.\n            this.exceptionClass = exceptionClass;\n            this.constructor = constructor;\n            this.versionAdded = versionAdded;\n            this.id = id;\n        }\n    }\n\n    static {\n        ID_TO_SUPPLIER = unmodifiableMap(Arrays\n                .stream(ElasticsearchExceptionHandle.values()).collect(Collectors.toMap(e -> e.id, e -> e.constructor)));\n        CLASS_TO_ELASTICSEARCH_EXCEPTION_HANDLE = unmodifiableMap(Arrays\n                .stream(ElasticsearchExceptionHandle.values()).collect(Collectors.toMap(e -> e.exceptionClass, e -> e)));\n    }\n\n    public Index getIndex() {\n        List<String> index = getMetadata(INDEX_METADATA_KEY);\n        if (index != null && index.isEmpty() == false) {\n            List<String> index_uuid = getMetadata(INDEX_METADATA_KEY_UUID);\n            return new Index(index.get(0), index_uuid.get(0));\n        }\n\n        return null;\n    }\n\n    public ShardId getShardId() {\n        List<String> shard = getMetadata(SHARD_METADATA_KEY);\n        if (shard != null && shard.isEmpty() == false) {\n            return new ShardId(getIndex(), Integer.parseInt(shard.get(0)));\n        }\n        return null;\n    }\n\n    public void setIndex(Index index) {\n        if (index != null) {\n            addMetadata(INDEX_METADATA_KEY, index.getName());\n            addMetadata(INDEX_METADATA_KEY_UUID, index.getUUID());\n        }\n    }\n\n    public void setIndex(String index) {\n        if (index != null) {\n            setIndex(new Index(index, INDEX_UUID_NA_VALUE));\n        }\n    }\n\n    public void setShard(ShardId shardId) {\n        if (shardId != null) {\n            setIndex(shardId.getIndex());\n            addMetadata(SHARD_METADATA_KEY, Integer.toString(shardId.id()));\n        }\n    }\n\n    public void setResources(String type, String... id) {\n        assert type != null;\n        addMetadata(RESOURCE_METADATA_ID_KEY, id);\n        addMetadata(RESOURCE_METADATA_TYPE_KEY, type);\n    }\n\n    // lower cases and adds underscores to transitions in a name\n    private static String toUnderscoreCase(String value) {\n        StringBuilder sb = new StringBuilder();\n        boolean changed = false;\n        for (int i = 0; i < value.length(); i++) {\n            char c = value.charAt(i);\n            if (Character.isUpperCase(c)) {\n                if (!changed) {\n                    // copy it over here\n                    for (int j = 0; j < i; j++) {\n                        sb.append(value.charAt(j));\n                    }\n                    changed = true;\n                    if (i == 0) {\n                        sb.append(Character.toLowerCase(c));\n                    } else {\n                        sb.append('_');\n                        sb.append(Character.toLowerCase(c));\n                    }\n                } else {\n                    sb.append('_');\n                    sb.append(Character.toLowerCase(c));\n                }\n            } else {\n                if (changed) {\n                    sb.append(c);\n                }\n            }\n        }\n        if (!changed) {\n            return value;\n        }\n        return sb.toString();\n    }\n\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect;\n\nimport static io.crate.testing.TestingHelpers.createNodeContext;\nimport static io.crate.testing.TestingHelpers.createReference;\nimport static io.crate.testing.TestingHelpers.isRow;\nimport static org.hamcrest.Matchers.contains;\nimport static org.junit.Assert.assertThat;\nimport static org.mockito.Mockito.mock;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.OutputStreamWriter;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Paths;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\n\nimport org.elasticsearch.common.settings.Settings;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.TemporaryFolder;\n\nimport io.crate.analyze.CopyFromParserProperties;\nimport io.crate.data.BatchIterator;\nimport io.crate.data.CollectionBucket;\nimport io.crate.data.Row;\nimport io.crate.data.testing.TestingRowConsumer;\nimport io.crate.execution.dsl.phases.FileUriCollectPhase;\nimport io.crate.execution.engine.collect.sources.FileCollectSource;\nimport io.crate.expression.symbol.Literal;\nimport io.crate.metadata.ColumnIdent;\nimport io.crate.metadata.CoordinatorTxnCtx;\nimport io.crate.test.integration.CrateDummyClusterServiceUnitTest;\nimport io.crate.types.DataTypes;\n\n\npublic class MapSideDataCollectOperationTest extends CrateDummyClusterServiceUnitTest {\n\n    @Rule\n    public TemporaryFolder temporaryFolder = new TemporaryFolder();\n\n    @Test\n    public void testFileUriCollect() throws Exception {\n        FileCollectSource fileCollectSource = new FileCollectSource(\n            createNodeContext(),\n            clusterService,\n            Collections.emptyMap(),\n            THREAD_POOL\n            );\n\n        File tmpFile = temporaryFolder.newFile(\"fileUriCollectOperation.json\");\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(tmpFile), StandardCharsets.UTF_8)) {\n            writer.write(\"{\\\"name\\\": \\\"Arthur\\\", \\\"id\\\": 4, \\\"details\\\": {\\\"age\\\": 38}}\\n\");\n            writer.write(\"{\\\"id\\\": 5, \\\"name\\\": \\\"Trillian\\\", \\\"details\\\": {\\\"age\\\": 33}}\\n\");\n        }\n\n        FileUriCollectPhase collectNode = new FileUriCollectPhase(\n            UUID.randomUUID(),\n            0,\n            \"test\",\n            Collections.singletonList(\"noop_id\"),\n            Literal.of(Paths.get(tmpFile.toURI()).toUri().toString()),\n            List.of(\"a\", \"b\"),\n            Arrays.asList(\n                createReference(\"name\", DataTypes.STRING),\n                createReference(new ColumnIdent(\"details\", \"age\"), DataTypes.INTEGER)\n            ),\n            Collections.emptyList(),\n            null,\n            false,\n            CopyFromParserProperties.DEFAULT,\n            FileUriCollectPhase.InputFormat.JSON,\n            Settings.EMPTY\n        );\n        TestingRowConsumer consumer = new TestingRowConsumer();\n        CollectTask collectTask = mock(CollectTask.class);\n        BatchIterator<Row> iterator = fileCollectSource.getIterator(\n            CoordinatorTxnCtx.systemTransactionContext(), collectNode, collectTask, false).get(5, TimeUnit.SECONDS);\n        consumer.accept(iterator, null);\n        assertThat(new CollectionBucket(consumer.getResult()), contains(\n            isRow(\"Arthur\", 38),\n            isRow(\"Trillian\", 33)\n        ));\n    }\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect.files;\n\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\n\nimport java.io.BufferedWriter;\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.OutputStreamWriter;\nimport java.net.URI;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.TimeUnit;\nimport java.util.zip.GZIPOutputStream;\n\nimport org.elasticsearch.common.settings.Settings;\nimport org.elasticsearch.test.ESTestCase;\nimport org.elasticsearch.threadpool.TestThreadPool;\nimport org.elasticsearch.threadpool.ThreadPool;\nimport org.junit.AfterClass;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport io.crate.execution.engine.collect.files.FileReadingIterator.LineCursor;\n\npublic class FileReadingCollectorTest extends ESTestCase {\n    private static ThreadPool THREAD_POOL;\n    private static File tmpFile;\n    private static File tmpFileGz;\n    private static File tmpFileEmptyLine;\n\n    private static String line1 = \"{\\\"name\\\": \\\"Arthur\\\", \\\"id\\\": 4, \\\"details\\\": {\\\"age\\\": 38}}\";\n    private static String line2 = \"{\\\"id\\\": 5, \\\"name\\\": \\\"Trillian\\\", \\\"details\\\": {\\\"age\\\": 33}}\";\n\n    private static LineCursor[] expectedResult(File file) throws Exception {\n        return new LineCursor[] {\n            new LineCursor(fileToURI(file), 1, line1, null),\n            new LineCursor(fileToURI(file), 2, line2, null)\n        };\n    }\n\n    @BeforeClass\n    public static void setUpClass() throws Exception {\n        Path copy_from = Files.createTempDirectory(\"copy_from\");\n        Path copy_from_gz = Files.createTempDirectory(\"copy_from_gz\");\n        Path copy_from_empty = Files.createTempDirectory(\"copy_from_empty\");\n        tmpFileGz = File.createTempFile(\"fileReadingCollector\", \".json.gz\", copy_from_gz.toFile());\n        tmpFile = File.createTempFile(\"fileReadingCollector\", \".json\", copy_from.toFile());\n        tmpFileEmptyLine = File.createTempFile(\"emptyLine\", \".json\", copy_from_empty.toFile());\n        try (BufferedWriter writer =\n                 new BufferedWriter(new OutputStreamWriter(new GZIPOutputStream(new FileOutputStream(tmpFileGz)),\n                     StandardCharsets.UTF_8))) {\n            writer.write(line1);\n            writer.write(\"\\n\");\n            writer.write(line2);\n            writer.write(\"\\n\");\n        }\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(tmpFile), StandardCharsets.UTF_8)) {\n            writer.write(line1);\n            writer.write(\"\\n\");\n            writer.write(line2);\n            writer.write(\"\\n\");\n        }\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(tmpFileEmptyLine), StandardCharsets.UTF_8)) {\n            writer.write(line1);\n            writer.write(\"\\n\");\n            writer.write(\"\\n\");\n            writer.write(line2);\n            writer.write(\"\\n\");\n        }\n        THREAD_POOL = new TestThreadPool(Thread.currentThread().getName());\n    }\n\n\n    @AfterClass\n    public static void tearDownClass() throws Exception {\n        assertThat(tmpFile.delete()).isTrue();\n        assertThat(tmpFileGz.delete()).isTrue();\n        assertThat(tmpFileEmptyLine.delete()).isTrue();\n        ThreadPool.terminate(THREAD_POOL, 30, TimeUnit.SECONDS);\n    }\n\n    @Test\n    public void testUmlautsAndWhitespacesWithExplicitURIThrowsAre() throws Throwable {\n        assertThatThrownBy(() -> collect(\"file:///this will f\u00e4il.json\"))\n            .isExactlyInstanceOf(IllegalArgumentException.class)\n            .hasMessage(\"Illegal character in path at index 12: file:///this will f\u00e4il.json\");\n    }\n\n    @Test\n    public void testNoErrorIfNoSuchFile() throws Throwable {\n        assertThat(collect(\"file:///some/path/that/shouldnt/exist/foo.json\")).satisfiesExactly(\n            line1 -> assertThat(line1.failure()).hasMessageContaining(\"No such file or directory\")\n        );\n        assertThat(collect(\"file:///some/path/that/shouldnt/exist/*\")).isEmpty();\n    }\n\n    @Test\n    public void testRelativeImport() throws Throwable {\n        assertThatThrownBy(() -> collect(\"xy\"))\n            .isExactlyInstanceOf(IllegalArgumentException.class)\n            .hasMessage(\"relative fileURIs are not allowed\");\n    }\n\n    @Test\n    public void testCollectFromUriWithGlob() throws Throwable {\n        List<LineCursor> result = collect(fileToURI(tmpFile.getParentFile()) + \"file*.json\");\n        assertThat(result).containsExactly(expectedResult(tmpFile));\n    }\n\n    @Test\n    public void testCollectFromDirectory() throws Throwable {\n        List<LineCursor> result = collect(fileToURI(tmpFile.getParentFile()) + \"*\");\n        assertThat(result).containsExactly(expectedResult(tmpFile));\n    }\n\n    @Test\n    public void test_collect_exact_uri() throws Throwable {\n        List<LineCursor> result = collect(fileToURI(tmpFile).toString());\n        assertThat(result).containsExactly(expectedResult(tmpFile));\n    }\n\n    @Test\n    public void testDoCollectRawFromCompressed() throws Throwable {\n        List<LineCursor> result = collect(Collections.singletonList(fileToURI(tmpFileGz).toString()), \"gzip\");\n        assertThat(result).containsExactly(expectedResult(tmpFileGz));\n    }\n\n    @Test\n    public void testCollectWithEmptyLine() throws Throwable {\n        List<LineCursor> result = collect(fileToURI(tmpFileEmptyLine).toString());\n        assertThat(result).containsExactly(\n            new LineCursor(fileToURI(tmpFileEmptyLine), 1, \"{\\\"name\\\": \\\"Arthur\\\", \\\"id\\\": 4, \\\"details\\\": {\\\"age\\\": 38}}\", null),\n            new LineCursor(fileToURI(tmpFileEmptyLine), 3, \"{\\\"id\\\": 5, \\\"name\\\": \\\"Trillian\\\", \\\"details\\\": {\\\"age\\\": 33}}\", null)\n        );\n    }\n\n    @Test\n    public void unsupportedURITest() throws Throwable {\n        FileReadingIterator it = it(\"invalid://crate.io/docs/en/latest/sql/reference/copy_from.html\");\n        LineCursor currentElement = it.currentElement();\n        assertThat(it.moveNext()).isTrue();\n        assertThat(currentElement.lineNumber()).isEqualTo(0);\n        assertThat(currentElement.line()).isNull();\n        assertThat(currentElement.failure()).hasMessage(\"unknown protocol: invalid\");\n    }\n\n    @Test\n    public void testMultipleUriSupport() throws Throwable {\n        List<String> fileUris = new ArrayList<>();\n        fileUris.add(Paths.get(tmpFile.toURI()).toUri().toString());\n        fileUris.add(Paths.get(tmpFileEmptyLine.toURI()).toUri().toString());\n        List<LineCursor> results = collect(fileUris, null);\n        assertThat(results).containsExactly(\n            new LineCursor(tmpFile.toURI(), 1, \"{\\\"name\\\": \\\"Arthur\\\", \\\"id\\\": 4, \\\"details\\\": {\\\"age\\\": 38}}\", null),\n            new LineCursor(tmpFile.toURI(), 2, \"{\\\"id\\\": 5, \\\"name\\\": \\\"Trillian\\\", \\\"details\\\": {\\\"age\\\": 33}}\", null),\n            new LineCursor(tmpFileEmptyLine.toURI(), 1, \"{\\\"name\\\": \\\"Arthur\\\", \\\"id\\\": 4, \\\"details\\\": {\\\"age\\\": 38}}\", null),\n            new LineCursor(tmpFileEmptyLine.toURI(), 3, \"{\\\"id\\\": 5, \\\"name\\\": \\\"Trillian\\\", \\\"details\\\": {\\\"age\\\": 33}}\", null)\n        );\n    }\n\n    private static List<LineCursor> collect(String ... fileUris) throws Exception {\n        return collect(Arrays.asList(fileUris), null);\n    }\n\n    private static FileReadingIterator it(String ... fileUris) {\n        return it(Arrays.asList(fileUris), null);\n    }\n\n    private static FileReadingIterator it(Collection<String> fileUris, String compression) {\n        return new FileReadingIterator(\n            fileUris,\n            compression,\n            Map.of(LocalFsFileInputFactory.NAME, new LocalFsFileInputFactory()),\n            false,\n            1,\n            0,\n            Settings.EMPTY,\n            THREAD_POOL.scheduler());\n    }\n\n    private static List<LineCursor> collect(Collection<String> fileUris, String compression) throws Exception {\n        return it(fileUris, compression)\n            .map(LineCursor::copy)\n            .toList()\n            .get(5, TimeUnit.SECONDS);\n    }\n\n    private static URI fileToURI(File file) throws IOException {\n        return file.toPath().toRealPath().toUri();\n    }\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect.files;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.mockito.ArgumentMatchers.any;\nimport static org.mockito.ArgumentMatchers.eq;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.times;\nimport static org.mockito.Mockito.verify;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.net.SocketTimeoutException;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Supplier;\n\nimport org.elasticsearch.common.settings.Settings;\nimport org.elasticsearch.test.ESTestCase;\nimport org.elasticsearch.threadpool.TestThreadPool;\nimport org.elasticsearch.threadpool.ThreadPool;\nimport org.junit.AfterClass;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\nimport org.mockito.ArgumentCaptor;\n\nimport io.crate.data.BatchIterator;\nimport io.crate.data.testing.BatchIteratorTester;\nimport io.crate.execution.engine.collect.files.FileReadingIterator.LineCursor;\n\npublic class FileReadingIteratorTest extends ESTestCase {\n\n    private static ThreadPool THREAD_POOL;\n\n\n    @BeforeClass\n    public static void setupThreadPool() {\n        THREAD_POOL = new TestThreadPool(Thread.currentThread().getName());\n    }\n\n    @AfterClass\n    public static void shutdownThreadPool() {\n        ThreadPool.terminate(THREAD_POOL, 30, TimeUnit.SECONDS);\n    }\n\n    /**\n     * Tests a regression resulting in an infinitive loop as the reader wasn't closed on IO errors\n     */\n    @Test\n    public void test_iterator_closes_current_reader_on_io_error() throws Exception {\n        Path tempFile1 = createTempFile(\"tempfile1\", \".csv\");\n        List<String> lines1 = List.of(\n            \"name,id,age\",\n            \"Arthur,4,38\",\n            \"Douglas,6,42\"  // <--- reader will fail on this line, so it is not part of the expected results\n        );\n        Files.write(tempFile1, lines1);\n        Path tempFile2 = createTempFile(\"tempfile2\", \".csv\");\n        List<String> lines2 = List.of(\"name,id,age\", \"Trillian,5,33\");\n        Files.write(tempFile2, lines2);\n        List<String> fileUris = List.of(tempFile1.toUri().toString(), tempFile2.toUri().toString());\n\n        Supplier<BatchIterator<LineCursor>> batchIteratorSupplier =\n            () -> new FileReadingIterator(\n                fileUris,\n                null,\n                Map.of(LocalFsFileInputFactory.NAME, new LocalFsFileInputFactory()),\n                false,\n                1,\n                0,\n                Settings.EMPTY,\n                THREAD_POOL.scheduler()\n            ) {\n\n                @Override\n                BufferedReader createBufferedReader(InputStream inputStream) throws IOException {\n                    return new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8)) {\n\n                        private int currentLineNumber = 0;\n\n                        @Override\n                        public String readLine() throws IOException {\n                            var line = super.readLine();\n                            currentLineNumber++;\n                            if (line != null && currentLineNumber > 2) {      // fail on 3rd line, succeed on header and first row\n                                throw new IOException(\"dummy\");\n                            }\n                            return line;\n                        }\n                    };\n                }\n            };\n\n        List<String> expectedResult = Arrays.asList(\n            \"name,id,age\",\n            \"Arthur,4,38\",\n            \"name,id,age\",\n            \"Trillian,5,33\"\n        );\n        var tester = new BatchIteratorTester<>(() -> batchIteratorSupplier.get().map(LineCursor::line));\n        tester.verifyResultAndEdgeCaseBehaviour(expectedResult);\n    }\n\n    /**\n     * Validates a bug which was resulting in duplicate reads of the same line when consecutive retries happen.\n     * https://github.com/crate/crate/pull/13261\n     */\n    @Test\n    public void test_consecutive_retries_will_not_result_in_duplicate_reads() throws Exception {\n        Path tempFile = createTempFile(\"tempfile1\", \".csv\");\n        List<String> lines = List.of(\"id\", \"1\", \"2\", \"3\", \"4\", \"5\");\n        Files.write(tempFile, lines);\n        List<String> fileUris = List.of(tempFile.toUri().toString());\n\n        Supplier<BatchIterator<LineCursor>> batchIteratorSupplier =\n            () -> new FileReadingIterator(\n                fileUris,\n                null,\n                Map.of(LocalFsFileInputFactory.NAME, new LocalFsFileInputFactory()),\n                false,\n                1,\n                0,\n                Settings.EMPTY,\n                THREAD_POOL.scheduler()\n            ) {\n                int retry = 0;\n\n                @Override\n                BufferedReader createBufferedReader(InputStream inputStream) throws IOException {\n                    return new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8)) {\n\n                        private int currentLineNumber = 0;\n\n                        @Override\n                        public String readLine() throws IOException {\n                            var line = super.readLine();\n                            // current implementation does not handle SocketTimeoutException thrown when parsing header so skip it here as well.\n                            if (currentLineNumber++ > 0 && retry++ < MAX_SOCKET_TIMEOUT_RETRIES) {\n                                throw new SocketTimeoutException(\"dummy\");\n                            }\n                            return line;\n                        }\n                    };\n                }\n            };\n\n        var tester = new BatchIteratorTester<>(() -> batchIteratorSupplier.get().map(LineCursor::line));\n        tester.verifyResultAndEdgeCaseBehaviour(lines);\n    }\n\n    @Test\n    public void test_loadNextBatch_implements_retry_with_backoff() throws IOException {\n        ScheduledExecutorService scheduler = mock(ScheduledExecutorService.class);\n        var fi = new FileReadingIterator(\n            List.of(),\n            null,\n            Map.of(),\n            false,\n            1,\n            0,\n            Settings.EMPTY,\n            scheduler\n        );\n        ArgumentCaptor<Long> delays = ArgumentCaptor.forClass(Long.class);\n\n        for (int i = 0; i < FileReadingIterator.MAX_SOCKET_TIMEOUT_RETRIES; i++) {\n            fi.loadNextBatch().complete(null);\n        }\n\n        verify(scheduler, times(FileReadingIterator.MAX_SOCKET_TIMEOUT_RETRIES))\n            .schedule(any(Runnable.class), delays.capture(), eq(TimeUnit.MILLISECONDS));\n        final List<Long> actualDelays = delays.getAllValues();\n        assertThat(actualDelays).isEqualTo(Arrays.asList(0L, 10L, 30L, 100L, 230L));\n\n        // retry fails if MAX_SOCKET_TIMEOUT_RETRIES is exceeded\n        assertThatThrownBy(fi::loadNextBatch)\n            .isExactlyInstanceOf(IllegalStateException.class)\n            .hasMessage(\"All batches already loaded\");\n    }\n\n    @Test\n    public void test_retry_from_one_uri_does_not_affect_reading_next_uri() throws Exception {\n        Path tempFile = createTempFile(\"tempfile1\", \".csv\");\n        Files.write(tempFile, List.of(\"1\", \"2\", \"3\"));\n        Path tempFile2 = createTempFile(\"tempfile2\", \".csv\");\n        Files.write(tempFile2, List.of(\"4\", \"5\", \"6\"));\n        List<String> fileUris = List.of(tempFile.toUri().toString(), tempFile2.toUri().toString());\n\n        var fi = new FileReadingIterator(\n            fileUris,\n            null,\n            Map.of(LocalFsFileInputFactory.NAME, new LocalFsFileInputFactory()),\n            false,\n            1,\n            0,\n            Settings.EMPTY,\n            THREAD_POOL.scheduler()\n        ) {\n            private boolean isThrownOnce = false;\n            final int lineToThrow = 2;\n\n            @Override\n            BufferedReader createBufferedReader(InputStream inputStream) throws IOException {\n                return new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8)) {\n\n                    private int currentLineNumber = 0;\n                    @Override\n                    public String readLine() throws IOException {\n                        var line = super.readLine();\n                        if (!isThrownOnce && currentLineNumber++ == lineToThrow) {\n                            isThrownOnce = true;\n                            throw new SocketTimeoutException(\"dummy\");\n                        }\n                        return line;\n                    }\n                };\n            }\n        };\n\n        assertThat(fi.moveNext()).isEqualTo(true);\n        assertThat(fi.currentElement().line()).isEqualTo(\"1\");\n        assertThat(fi.moveNext()).isEqualTo(true);\n        assertThat(fi.currentElement().line()).isEqualTo(\"2\");\n        assertThat(fi.moveNext()).isEqualTo(false);\n        assertThat(fi.allLoaded()).isEqualTo(false);\n        assertThat(fi.loadNextBatch()).succeedsWithin(5, TimeUnit.SECONDS)\n            .satisfies(x -> {\n                assertThat(fi.currentElement().line()).isEqualTo(\"2\");\n                assertThat(fi.watermark).isEqualTo(3);\n                assertThat(fi.moveNext()).isEqualTo(true);\n                // the watermark helped 'fi' to recover the state right before the exception then cleared\n                assertThat(fi.watermark).isEqualTo(0);\n                assertThat(fi.currentElement().line()).isEqualTo(\"3\");\n\n                // verify the exception did not prevent reading the next URI\n                assertThat(fi.moveNext()).isEqualTo(true);\n                assertThat(fi.currentElement().line()).isEqualTo(\"4\");\n            });\n    }\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect.sources;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.mockito.Mockito.mock;\n\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.UUID;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.TimeUnit;\n\nimport org.elasticsearch.common.settings.Settings;\nimport org.junit.Test;\n\nimport io.crate.analyze.CopyFromParserProperties;\nimport io.crate.data.BatchIterator;\nimport io.crate.data.Row;\nimport io.crate.execution.dsl.phases.FileUriCollectPhase;\nimport io.crate.execution.dsl.phases.FileUriCollectPhase.InputFormat;\nimport io.crate.execution.dsl.projection.Projection;\nimport io.crate.execution.engine.collect.CollectTask;\nimport io.crate.expression.symbol.Literal;\nimport io.crate.expression.symbol.Symbol;\nimport io.crate.metadata.CoordinatorTxnCtx;\nimport io.crate.metadata.Functions;\nimport io.crate.metadata.NodeContext;\nimport io.crate.test.integration.CrateDummyClusterServiceUnitTest;\nimport io.crate.testing.TestingHelpers;\nimport io.crate.types.DataTypes;\nimport io.crate.role.Role;\nimport io.crate.role.Roles;\n\npublic class FileCollectSourceTest extends CrateDummyClusterServiceUnitTest {\n\n\n    @Test\n    public void test_file_collect_source_returns_iterator_that_can_skip_lines() throws Exception {\n        List<String> targetColumns = List.of();\n        List<Projection> projections = List.of();\n        List<Symbol> toCollect = List.of(\n            TestingHelpers.createReference(\"_raw\", DataTypes.STRING)\n        );\n        Path tmpFile = createTempFile(\"tempfile1\", \".csv\");\n        Files.write(tmpFile, List.of(\n            \"garbage1\",\n            \"garbage2\",\n            \"x,y\",\n            \"1,2\",\n            \"10,20\"\n        ));\n        FileUriCollectPhase fileUriCollectPhase = new FileUriCollectPhase(\n            UUID.randomUUID(),\n            1,\n            \"copy from\",\n            List.of(),\n            Literal.of(tmpFile.toUri().toString()),\n            targetColumns,\n            toCollect,\n            projections,\n            null,\n            false,\n            new CopyFromParserProperties(true, true, ',', 2),\n            InputFormat.CSV,\n            Settings.EMPTY\n        );\n\n        Roles roles = () -> List.of(Role.CRATE_USER);\n        FileCollectSource fileCollectSource = new FileCollectSource(\n            new NodeContext(new Functions(Map.of()), roles),\n            clusterService,\n            Map.of(),\n            THREAD_POOL\n        );\n\n        CompletableFuture<BatchIterator<Row>> iterator = fileCollectSource.getIterator(\n            CoordinatorTxnCtx.systemTransactionContext(),\n            fileUriCollectPhase,\n            mock(CollectTask.class),\n            false\n        );\n        assertThat(iterator).succeedsWithin(5, TimeUnit.SECONDS);\n        CompletableFuture<List<Object>> resultFuture = iterator.join()\n            .map(row -> row.get(0))\n            .toList();\n\n        assertThat(resultFuture).succeedsWithin(5, TimeUnit.SECONDS);\n        assertThat(resultFuture.join()).containsExactly(\n            \"{\\\"x\\\":\\\"1\\\",\\\"y\\\":\\\"2\\\"}\",\n            \"{\\\"x\\\":\\\"10\\\",\\\"y\\\":\\\"20\\\"}\"\n        );\n    }\n}\n\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.integrationtests;\n\nimport static com.carrotsearch.randomizedtesting.RandomizedTest.newTempDir;\nimport static io.crate.protocols.postgres.PGErrorStatus.INTERNAL_ERROR;\nimport static io.crate.testing.Asserts.assertThat;\nimport static io.crate.testing.TestingHelpers.printedTable;\nimport static io.netty.handler.codec.http.HttpResponseStatus.BAD_REQUEST;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.OutputStreamWriter;\nimport java.net.URISyntaxException;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.DirectoryStream;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.UUID;\n\nimport org.elasticsearch.test.IntegTestCase;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.TemporaryFolder;\n\nimport com.carrotsearch.randomizedtesting.LifecycleScope;\n\nimport io.crate.testing.Asserts;\nimport io.crate.testing.SQLResponse;\nimport io.crate.testing.UseJdbc;\nimport io.crate.testing.UseNewCluster;\n\n@IntegTestCase.ClusterScope(numDataNodes = 2)\npublic class CopyIntegrationTest extends SQLHttpIntegrationTest {\n\n    private final String copyFilePath =\n        Paths.get(getClass().getResource(\"/essetup/data/copy\").toURI()).toUri().toString();\n    private final String copyFilePathShared =\n        Paths.get(getClass().getResource(\"/essetup/data/copy/shared\").toURI()).toUri().toString();\n    private final String nestedArrayCopyFilePath =\n        Paths.get(getClass().getResource(\"/essetup/data/nested_array\").toURI()).toUri().toString();\n\n    private Setup setup = new Setup(sqlExecutor);\n\n    @Rule\n    public TemporaryFolder folder = new TemporaryFolder();\n\n    public CopyIntegrationTest() throws URISyntaxException {\n    }\n\n    @Test\n    public void testCopyFromUnknownDirectory() {\n        execute(\"create table t (a int)\");\n        ensureYellow();\n        execute(\"copy t from 'file:///tmp/unknown_dir/*'\");\n    }\n\n    @Test\n    public void testCopyFromFileWithJsonExtension() {\n        execute(\"create table quotes (id int primary key, \" +\n                \"quote string index using fulltext) with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes from ?\", new Object[] {copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(3L);\n        assertThat(response.rows()[0]).hasSize(2);\n\n        execute(\"select quote from quotes where id = 1\");\n        assertThat(response.rows()[0][0]).isEqualTo(\"Don't pa\\u00f1ic.\");\n    }\n\n    @Test\n    public void testCopyFromFileWithCSVOption() {\n        execute(\"create table quotes (id int primary key, \" +\n            \"quote string index using fulltext) with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes from ? with (format='csv')\", new Object[]{copyFilePath + \"test_copy_from_csv.ext\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(3L);\n        assertThat(response.rows()[0]).hasSize(2);\n\n        execute(\"select quote from quotes where id = 1\");\n        assertThat(response.rows()[0][0]).isEqualTo(\"Don't pa\\u00f1ic.\");\n    }\n\n    @Test\n    public void testCopyFromFileWithCSVOptionWithDynamicColumnCreation() {\n        execute(\"create table quotes (id int primary key, \" +\n                \"quote string index using fulltext) with (number_of_replicas = 0, column_policy = 'dynamic')\");\n\n        execute(\"copy quotes from ? with (format='csv')\", new Object[]{copyFilePath + \"test_copy_from_csv_extra_column.ext\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(3L);\n        assertThat(response.rows()[0]).hasSize(3);\n\n        execute(\"select quote, comment from quotes where id = 1\");\n        assertThat(response.rows()[0][0]).isEqualTo(\"Don't pa\\u00f1ic.\");\n        assertThat(response.rows()[0][1]).isEqualTo(\"good one\");\n    }\n\n    @Test\n    public void testCopyFromFileWithCSVOptionWithTargetColumns() {\n        execute(\"create table quotes (id int primary key, \" +\n                \"quote string index using fulltext, comment text) with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes(id, quote, comment) from ? with (format='csv')\", new Object[]{copyFilePath + \"test_copy_from_csv_extra_column.ext\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(3L);\n        assertThat(response.rows()[0]).hasSize(3);\n\n        execute(\"select quote, comment from quotes where id = 1\");\n        assertThat(response.rows()[0][0]).isEqualTo(\"Don't pa\\u00f1ic.\");\n        assertThat(response.rows()[0][1]).isEqualTo(\"good one\");\n    }\n\n    @Test\n    public void testCopyFromFileWithCSVOptionWithNoHeader() {\n        execute(\"create table quotes (id int primary key, \" +\n                \"quote string index using fulltext) with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes from ? with (format='csv', header=false)\", new Object[]{copyFilePath + \"test_copy_from_csv_no_header.ext\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(3L);\n        assertThat(response.rows()[0]).hasSize(2);\n\n        execute(\"select quote from quotes where id = 1\");\n        assertThat(response.rows()[0][0]).isEqualTo(\"Don't pa\\u00f1ic.\");\n    }\n\n    @Test\n    public void testCopyFromFileWithUmlautsWhitespacesAndGlobs() throws Exception {\n        execute(\"create table t (id int primary key, name string) clustered into 1 shards with (number_of_replicas = 0)\");\n        File tmpFolder = folder.newFolder(\"\u00e4wes\u00f6me f\u00f6lder\");\n        File file = new File(tmpFolder, \"s\u00fcp\u00e4r.json\");\n\n        List<String> lines = Collections.singletonList(\"{\\\"id\\\": 1, \\\"name\\\": \\\"Arthur\\\"}\");\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ?\", new Object[]{Paths.get(tmpFolder.toURI()).toUri().toString() + \"s*.json\"});\n        assertThat(response).hasRowCount(1L);\n    }\n\n    @Test\n    public void testCopyFromWithOverwriteDuplicates() throws Exception {\n        execute(\"create table t (id int primary key) with (number_of_replicas = 0)\");\n\n        execute(\"insert into t (id) values (?)\", new Object[][]{\n            new Object[]{1},\n            new Object[]{2},\n            new Object[]{3},\n            new Object[]{4}\n        });\n        execute(\"refresh table t\");\n\n        File tmpExport = folder.newFolder(\"tmpExport\");\n        String uriTemplate = Paths.get(tmpExport.toURI()).toUri().toString();\n        execute(\"copy t to directory ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(4L);\n        execute(\"copy t from ?\", new Object[]{uriTemplate + \"*\"});\n        assertThat(response).hasRowCount(0L);\n        execute(\"copy t from ? with (overwrite_duplicates = true, shared=true)\",\n            new Object[]{uriTemplate + \"*\"});\n        assertThat(response).hasRowCount(4L);\n        execute(\"refresh table t\");\n        execute(\"select count(*) from t\");\n        assertThat(((Long) response.rows()[0][0])).isEqualTo(4L);\n    }\n\n    @Test\n    public void testCopyFromFileWithoutPK() throws Exception {\n        execute(\"create table quotes (id int, \" +\n                \"quote string index using fulltext) with (number_of_replicas=0)\");\n\n        execute(\"copy quotes from ?\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(6L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(6L);\n        assertThat(response.rows()[0]).hasSize(2);\n    }\n\n    @Test\n    public void testCopyFromFilePattern() {\n        execute(\"create table quotes (id int primary key, \" +\n                \"quote string index using fulltext) with (number_of_replicas=0)\");\n\n        execute(\"copy quotes from ?\", new Object[]{copyFilePathShared + \"*.json\"});\n        assertThat(response).hasRowCount(6L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(6L);\n    }\n\n    @Test\n    public void testCopyFromFileWithEmptyLine() throws Exception {\n        execute(\"create table foo (id integer primary key) clustered into 1 shards with (number_of_replicas=0)\");\n        File newFile = folder.newFile();\n\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(newFile), StandardCharsets.UTF_8)) {\n            writer.write(\"{\\\"id\\\":1}\\n\");\n            writer.write(\"\\n\");\n            writer.write(\"{\\\"id\\\":2}\\n\");\n        }\n        execute(\"copy foo from ?\", new Object[]{Paths.get(newFile.toURI()).toUri().toString()});\n        assertThat(response).hasRowCount(2L);\n        refresh();\n\n        execute(\"select * from foo order by id\");\n        assertThat(response.rows()[0][0]).isEqualTo(1);\n        assertThat(response.rows()[1][0]).isEqualTo(2);\n    }\n\n    /**\n     * Disable JDBC/PSQL as object values are streamed via JSON on the PSQL wire protocol which is not type safe.\n     */\n    @UseJdbc(0)\n    @Test\n    public void testCopyFromFileWithInvalidColumns() throws Exception {\n        execute(\"create table foo (id integer primary key) clustered into 1 shards \" +\n                \"with (number_of_replicas=0, column_policy='dynamic')\");\n        File newFile = folder.newFile();\n\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(newFile), StandardCharsets.UTF_8)) {\n            writer.write(\"{\\\"id\\\":1, \\\"_invalid\\\":1}\\n\");\n            writer.write(\"{\\\"id\\\":2, \\\"invalid['index']\\\":2}\\n\");\n            writer.write(\"{\\\"id\\\":3, \\\"invalid['_invalid']\\\":3}\\n\");\n            writer.write(\"{\\\"id\\\":4, \\\"valid\\\": {\\\"_valid\\\": 4}}\\n\");\n        }\n\n        execute(\"copy foo from ?\", new Object[]{Paths.get(newFile.toURI()).toUri().toString()});\n        assertThat(response).hasRowCount(1L);\n        refresh();\n\n        execute(\"select * from foo order by id\");\n\n        // Check columns.\n        assertThat(response.cols()).hasSize(2);\n        assertThat(response.cols()[1]).isEqualTo(\"valid\");\n\n        // Check data of column.\n        assertThat(response.rows()[0][0]).isEqualTo(4);\n        HashMap<?, ?> data = (HashMap<?, ?>)response.rows()[0][1];\n        // The inner value will result in an Long type as we rely on ES mappers here and the dynamic ES parsing\n        // will define integers as longs (no concrete type was specified so use long to be safe)\n        assertThat(data.get(\"_valid\")).isEqualTo(4L);\n    }\n\n    @Test\n    public void testCopyFromInvalidJson() throws Exception {\n        execute(\"create table foo (id integer primary key) clustered into 1 shards with (number_of_replicas=0)\");\n        File newFile = folder.newFile();\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(newFile), StandardCharsets.UTF_8)) {\n            writer.write(\"{|}\");\n        }\n        execute(\"copy foo from ?\", new Object[]{Paths.get(newFile.toURI()).toUri().toString()});\n        assertThat(response).hasRowCount(0L);\n    }\n\n    @Test\n    public void testCopyFromFileIntoSinglePartition() throws Exception {\n        execute(\"CREATE TABLE quotes (id INTEGER, quote STRING) PARTITIONED BY (id)\");\n        ensureGreen();\n        execute(\"COPY quotes PARTITION (id = 1) FROM ? WITH (shared = true)\", new Object[]{\n            copyFilePath + \"test_copy_from.json\"});\n        refresh();\n\n        execute(\"SELECT * FROM quotes\");\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyFromFileWithCompression() throws Exception {\n        execute(\"create table quotes (id int, \" +\n                \"quote string)\");\n        execute(\"copy quotes from ? with (compression='gzip')\", new Object[]{copyFilePath + \"test_copy_from.gz\"});\n        execute(\"refresh table quotes\");\n        execute(\"select * from quotes order by quote\");\n        assertThat(response).hasRows(\n            \"1| Don't pa\u00f1ic.\",\n            \"1| Don't pa\u00f1ic.\",\n            \"3| Time is an illusion. Lunchtime doubly so.\",\n            \"3| Time is an illusion. Lunchtime doubly so.\",\n            \"2| Would it save you a lot of time if I just gave up and went mad now?\",\n            \"2| Would it save you a lot of time if I just gave up and went mad now?\"\n        );\n    }\n\n    @Test\n    public void testCopyFromWithGeneratedColumn() throws Exception {\n        execute(\"create table quotes (\" +\n                \" id int,\" +\n                \" quote string,\" +\n                \" gen_quote as concat(quote, ' This is awesome!')\" +\n                \")\");\n\n        execute(\"copy quotes from ? with (shared=true)\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        refresh();\n\n        execute(\"select gen_quote from quotes limit 1\");\n        assertThat((String) response.rows()[0][0]).endsWith(\"This is awesome!\");\n    }\n\n    @Test\n    public void testCopyFromWithInvalidGivenGeneratedColumn() throws Exception {\n        execute(\"create table quotes (\" +\n                \" id int,\" +\n                \" quote as cast(id as string)\" +\n                \")\");\n        execute(\"copy quotes from ? with (shared=true)\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(0L);\n    }\n\n    @Test\n    public void testCopyFromToPartitionedTableWithGeneratedColumn() throws Exception {\n        execute(\"create table quotes (\" +\n                \" id int,\" +\n                \" quote string,\" +\n                \" gen_quote as concat(quote, ' Partitioned by awesomeness!')\" +\n                \") partitioned by (gen_quote)\");\n\n        execute(\"copy quotes from ? with (shared=true)\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        refresh();\n\n        execute(\"select gen_quote from quotes limit 1\");\n        assertThat((String) response.rows()[0][0]).endsWith(\"Partitioned by awesomeness!\");\n    }\n\n    @Test\n    public void testCopyFromToPartitionedTableWithNullValue() {\n        execute(\"CREATE TABLE times (\" +\n                \"   time timestamp with time zone\" +\n                \") partitioned by (time)\");\n\n        execute(\"copy times from ? with (shared=true)\", new Object[]{copyFilePath + \"test_copy_from_null_value.json\"});\n        refresh();\n\n        execute(\"select time from times\");\n        assertThat(response).hasRowCount(1L);\n        assertThat(response.rows()[0][0]).isNull();\n    }\n\n    @Test\n    public void testCopyFromIntoPartitionWithInvalidGivenGeneratedColumnAsPartitionKey() throws Exception {\n        // test that rows are imported into defined partition even that the partition value does not match the\n        // generated column expression value\n        execute(\"create table quotes (\" +\n                \" id int,\" +\n                \" quote string,\" +\n                \" id_str as cast(id+1 as string)\" +\n                \") partitioned by (id_str)\");\n\n        execute(\"copy quotes partition (id_str = 1) from ? with (shared=true)\", new Object[]{\n            copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes where id_str = 1\");\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyToFile() throws Exception {\n        execute(\"create table singleshard (name string) clustered into 1 shards with (number_of_replicas = 0)\");\n\n        Asserts.assertSQLError(() -> execute(\"copy singleshard to '/tmp/file.json'\"))\n            .hasPGError(INTERNAL_ERROR)\n            .hasHTTPError(BAD_REQUEST, 4004)\n            .hasMessageContaining(\"Using COPY TO without specifying a DIRECTORY is not supported\");\n    }\n\n    @Test\n    public void testCopyToDirectory() throws Exception {\n        this.setup.groupBySetup();\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy characters to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(7L);\n        String[] list = folder.getRoot().list();\n        assertThat(list).hasSizeGreaterThanOrEqualTo(1);\n        for (String file : list) {\n            assertThat(file).startsWith(\"characters_\");\n        }\n\n        List<String> lines = new ArrayList<>(7);\n        DirectoryStream<Path> stream = Files.newDirectoryStream(Paths.get(folder.getRoot().toURI()), \"*.json\");\n        for (Path path : stream) {\n            lines.addAll(Files.readAllLines(path, StandardCharsets.UTF_8));\n        }\n        assertThat(lines).hasSize(7);\n        for (String line : lines) {\n            assertThat(line).startsWith(\"{\");\n            assertThat(line).endsWith(\"}\");\n        }\n    }\n\n    @Test\n    public void testCopyToWithCompression() throws Exception {\n        execute(\"create table singleshard (name string) clustered into 1 shards with (number_of_replicas = 0)\");\n        execute(\"insert into singleshard (name) values ('foo')\");\n        execute(\"refresh table singleshard\");\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy singleshard to DIRECTORY ? with (compression='gzip')\", new Object[]{uriTemplate});\n\n        assertThat(response).hasRowCount(1L);\n\n        String[] list = folder.getRoot().list();\n        assertThat(list).hasSize(1);\n        String file = list[0];\n        assertThat(file)\n            .startsWith(\"singleshard_\")\n            .endsWith(\".json.gz\");\n\n        long size = Files.size(Paths.get(folder.getRoot().toURI().resolve(file)));\n        assertThat(size).isEqualTo(35L);\n    }\n\n    @Test\n    public void testCopyColumnsToDirectory() throws Exception {\n        this.setup.groupBySetup();\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy characters (name, details['job']) to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response.cols()).isEmpty();\n        assertThat(response).hasRowCount(7L);\n        List<String> lines = new ArrayList<>(7);\n        DirectoryStream<Path> stream = Files.newDirectoryStream(Paths.get(folder.getRoot().toURI()), \"*.json\");\n        for (Path entry : stream) {\n            lines.addAll(Files.readAllLines(entry, StandardCharsets.UTF_8));\n        }\n        Path path = Paths.get(folder.getRoot().toURI().resolve(\"characters_0_.json\"));\n        assertThat(path).exists();\n        assertThat(lines).hasSize(7);\n        assertThat(lines)\n            .anySatisfy(x -> assertThat(x).contains(\"Sandwitch Maker\"))\n            .anySatisfy(x -> assertThat(x).contains(\"Arthur Dent\"));\n        assertThat(lines).allSatisfy(\n            x -> assertThat(x.trim()).startsWith(\"[\").endsWith(\"]\"));\n    }\n\n    @Test\n    public void testCopyToFileColumnsJsonObjectOutput() throws Exception {\n        execute(\"create table singleshard (name string, test object as (foo string)) clustered into 1 shards with (number_of_replicas = 0)\");\n        execute(\"insert into singleshard (name, test) values ('foobar', {foo='bar'})\");\n        execute(\"refresh table singleshard\");\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy singleshard (name, test['foo']) to DIRECTORY ? with (format='json_object')\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(1L);\n\n        String[] list = folder.getRoot().list();\n        assertThat(list).hasSize(1);\n        List<String> lines = Files.readAllLines(\n            Paths.get(folder.getRoot().toURI().resolve(list[0])), StandardCharsets.UTF_8);\n\n        assertThat(lines).hasSize(1);\n        for (String line : lines) {\n            assertThat(line).startsWith(\"{\");\n            assertThat(line).endsWith(\"}\");\n        }\n    }\n\n    @Test\n    public void testCopyToWithWhere() throws Exception {\n        this.setup.groupBySetup();\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy characters where gender = 'female' to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(2L);\n    }\n\n    @Test\n    public void testCopyToWithWhereOnPrimaryKey() throws Exception {\n        execute(\"create table t1 (id int primary key) with (number_of_replicas = 0)\");\n        execute(\"insert into t1 (id) values (1)\");\n        execute(\"refresh table t1\");\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy t1 where id = 1 to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(1L);\n    }\n\n    @Test\n    public void testCopyToWithWhereNoMatch() throws Exception {\n        this.setup.groupBySetup();\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy characters where gender = 'foo' to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(0L);\n    }\n\n    @Test\n    public void testCopyFromNestedArrayRow() throws Exception {\n        // assert that rows with nested arrays aren't imported\n        execute(\"create table users (id int, \" +\n            \"name string) with (number_of_replicas=0, column_policy = 'dynamic')\");\n        execute(\"copy users from ? with (shared=true)\", new Object[]{\n            nestedArrayCopyFilePath + \"nested_array_copy_from.json\"});\n        assertThat(response).hasRowCount(1L); // only 1 document got inserted\n        refresh();\n\n        execute(\"select * from users\");\n        assertThat(response).hasRowCount(1L);\n\n        assertThat(printedTable(response.rows())).isEqualTo(\"2| Trillian\\n\");\n    }\n\n    @Test\n    public void testCopyToWithGeneratedColumn() {\n        execute(\"CREATE TABLE foo (\" +\n                \"   day TIMESTAMP WITH TIME ZONE GENERATED ALWAYS AS date_trunc('day', timestamp),\" +\n                \"   timestamp TIMESTAMP WITH TIME ZONE\" +\n                \") PARTITIONED BY (day)\");\n        execute(\"insert into foo (timestamp) values (1454454000377)\");\n        refresh();\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy foo to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(1L);\n    }\n\n    @Test\n    public void testCopyFromWithRoutingInPK() throws Exception {\n        execute(\"create table t (i int primary key, c string primary key, a int)\" +\n            \" clustered by (c) with (number_of_replicas=0)\");\n        ensureGreen();\n        execute(\"insert into t (i, c) values (1, 'clusteredbyvalue'), (2, 'clusteredbyvalue')\");\n        refresh();\n\n        String uri = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy t to directory ?\", new Object[]{uri});\n        assertThat(response).hasRowCount(2L);\n\n        execute(\"delete from t\");\n        refresh();\n\n        execute(\"copy t from ? with (shared=true)\", new Object[]{uri + \"t_*\"});\n        refresh();\n\n        // only one shard should have all imported rows, since we have the same routing for both rows\n        response = execute(\"select count(*) from sys.shards where num_docs>0 and table_name='t'\");\n        assertThat(response.rows()[0][0]).isEqualTo(1L);\n    }\n\n    @Test\n    public void testCopyFromTwoHttpUrls() throws Exception {\n        execute(\"create blob table blobs with (number_of_replicas = 0)\");\n        execute(\"create table names (id int primary key, name string) with (number_of_replicas = 0)\");\n\n        String r1 = \"{\\\"id\\\": 1, \\\"name\\\":\\\"Marvin\\\"}\";\n        String r2 = \"{\\\"id\\\": 2, \\\"name\\\":\\\"Slartibartfast\\\"}\";\n        List<String> urls = List.of(upload(\"blobs\", r1), upload(\"blobs\", r2));\n\n        execute(\"copy names from ?\", new Object[]{urls});\n        assertThat(response).hasRowCount(2L);\n        execute(\"refresh table names\");\n        execute(\"select name from names order by id\");\n        assertThat(printedTable(response.rows())).isEqualTo(\"Marvin\\nSlartibartfast\\n\");\n    }\n\n    @Test\n    public void testCopyFromTwoUriMixedSchemaAndWildcardUse() throws Exception {\n        execute(\"create blob table blobs with (number_of_replicas = 0)\");\n        execute(\"create table names (id int primary key, name string) with (number_of_replicas = 0)\");\n\n        Path tmpDir = newTempDir(LifecycleScope.TEST);\n        File file = new File(tmpDir.toFile(), \"names.json\");\n        String r1 = \"{\\\"id\\\": 1, \\\"name\\\": \\\"Arthur\\\"}\";\n        String r2 = \"{\\\"id\\\": 2, \\\"name\\\":\\\"Slartibartfast\\\"}\";\n\n        Files.write(file.toPath(), Collections.singletonList(r1), StandardCharsets.UTF_8);\n        List<String> urls = List.of(tmpDir.toUri().toString() + \"*.json\", upload(\"blobs\", r2));\n\n        execute(\"copy names from ?\", new Object[]{urls});\n        assertThat(response).hasRowCount(2L);\n        execute(\"refresh table names\");\n        execute(\"select name from names order by id\");\n        assertThat(printedTable(response.rows())).isEqualTo(\"Arthur\\nSlartibartfast\\n\");\n    }\n\n    @Test\n    public void testCopyFromIntoTableWithClusterBy() throws Exception {\n        execute(\"create table quotes (id int, quote string) \" +\n            \"clustered by (id)\" +\n            \"with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes from ? with (shared = true)\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select quote from quotes where id = 2\");\n        assertThat((String) response.rows()[0][0]).contains(\"lot of time\");\n    }\n\n    @Test\n    public void testCopyFromIntoTableWithPkAndClusterBy() throws Exception {\n        execute(\"create table quotes (id int primary key, quote string) \" +\n            \"clustered by (id)\" +\n            \"with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes from ?\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select quote from quotes where id = 3\");\n        assertThat((String) response.rows()[0][0]).contains(\"Time is an illusion.\");\n    }\n\n    private Path setUpTableAndSymlink(String tableName) throws IOException {\n        execute(String.format(Locale.ENGLISH,\n            \"create table %s (a int) with (number_of_replicas = 0)\",\n            tableName));\n\n        String r1 = \"{\\\"a\\\": 1}\";\n        String r2 = \"{\\\"a\\\": 2}\";\n        String r3 = \"{\\\"a\\\": 3}\";\n        return tmpFileWithLines(List.of(r1, r2, r3));\n    }\n\n    public static Path tmpFileWithLines(Iterable<String> lines) throws IOException {\n        Path tmpDir = newTempDir(LifecycleScope.TEST);\n        Path target = Files.createDirectories(tmpDir.resolve(\"target\"));\n        tmpFileWithLines(lines, \"data.json\", target);\n        return Files.createSymbolicLink(tmpDir.resolve(\"link\"), target);\n    }\n\n    public static void tmpFileWithLines(Iterable<String> lines, String filename, Path target) throws IOException {\n        File file = new File(target.toFile(), filename);\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n    }\n\n    @Test\n    public void testCopyFromSymlinkFolderWithWildcard() throws Exception {\n        Path link = setUpTableAndSymlink(\"t\");\n        execute(\"copy t from ? with (shared=true)\", new Object[]{\n            link.toUri().toString() + \"*\"\n        });\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyFromSymlinkFolderWithPrefixedWildcard() throws Exception {\n        Path link = setUpTableAndSymlink(\"t\");\n        execute(\"copy t from ? with (shared=true)\", new Object[]{\n            link.toUri().toString() + \"d*\"\n        });\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyFromSymlinkFolderWithSuffixedWildcard() throws Exception {\n        Path link = setUpTableAndSymlink(\"t\");\n        execute(\"copy t from ? with (shared=true)\", new Object[]{\n            link.toUri().toString() + \"*.json\"\n        });\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyFromFileInSymlinkFolder() throws Exception {\n        Path link = setUpTableAndSymlink(\"t\");\n        execute(\"copy t from ? with (shared=true)\", new Object[]{\n            link.toUri().toString() + \"data.json\"\n        });\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyWithGeneratedPartitionColumnThatIsPartOfPrimaryKey() throws Exception {\n        execute(\n            \"create table t1 (\" +\n            \"   guid string,\" +\n            \"   ts timestamp with time zone,\" +\n            \"   g_ts_month timestamp with time zone generated always as date_trunc('month', ts),\" +\n            \"   primary key (guid, g_ts_month)\" +\n            \") partitioned by (g_ts_month)\");\n\n        Path path = tmpFileWithLines(Arrays.asList(\n            \"{\\\"guid\\\": \\\"a\\\", \\\"ts\\\": 1496275200000}\",\n            \"{\\\"guid\\\": \\\"b\\\", \\\"ts\\\": 1496275300000}\"\n        ));\n        execute(\"copy t1 from ? with (shared=true)\", new Object[] { path.toUri().toString() + \"*.json\"});\n        assertThat(response).hasRowCount(2L);\n\n        execute(\"copy t1 partition (g_ts_month = 1496275200000) from ? with (shared=true, overwrite_duplicates=true)\",\n            new Object[] { path.toUri().toString() + \"*.json\"});\n        assertThat(response).hasRowCount(2L);\n    }\n\n    @Test\n    public void testCopyFromReturnSummaryWithFailedRows() throws Exception {\n        execute(\"create table t1 (id int primary key, ts timestamp with time zone)\");\n\n        Path tmpDir = newTempDir(LifecycleScope.TEST);\n        Path target = Files.createDirectories(tmpDir.resolve(\"target\"));\n\n        tmpFileWithLines(Arrays.asList(\n            \"{\\\"id\\\": 1, \\\"ts\\\": 1496275200000}\",\n            \"{\\\"id\\\": 2, \\\"ts\\\": 1496275300000}\"\n        ), \"data1.json\", target);\n        tmpFileWithLines(Arrays.asList(\n            \"{\\\"id\\\": 2, \\\"ts\\\": 1496275200000}\",       // <-- duplicate key\n            \"{\\\"id\\\": 3, \\\"ts\\\": 1496275300000}\"\n        ), \"data2.json\", target);\n        tmpFileWithLines(Arrays.asList(\n            \"{\\\"id\\\": 4, \\\"ts\\\": 1496275200000}\",\n            \"{\\\"id\\\": 5, \\\"ts\\\": \\\"May\\\"}\",              // <-- invalid timestamp\n            \"{\\\"id\\\": 7, \\\"ts\\\": \\\"Juli\\\"}\"              // <-- invalid timestamp\n        ), \"data3.json\", target);\n        tmpFileWithLines(Arrays.asList(\n            \"foo\",                                      // <-- invalid json\n            \"{\\\"id\\\": 6, \\\"ts\\\": 1496275200000}\"\n        ), \"data4.json\", target);\n\n        execute(\"copy t1 from ? with (shared=true) return summary\", new Object[]{target.toUri().toString() + \"*\"});\n        String result = printedTable(response.rows());\n\n        // one of the first files should be processed without any error\n        assertThat(result).contains(\"| 2| 0| {}\");\n        // one of the first files will have a duplicate key error\n        assertThat(result).contains(\"| 1| 1| {A document with the same primary key exists already={count=1, line_numbers=[\");\n        // file `data3.json` has a invalid timestamp error\n        assertThat(result).contains(\"data3.json| 1| 2| {Cannot cast value \");\n        assertThat(result).contains(\"Cannot cast value `Juli` to type `timestamp with time zone`={count=1, line_numbers=[3]}\");\n        assertThat(result).contains(\"Cannot cast value `May` to type `timestamp with time zone`={count=1, line_numbers=[2]}\");\n        // file `data4.json` has an invalid json item entry\n        assertThat(result).contains(\"data4.json| 1| 1| {JSON parser error: \");\n    }\n\n    @Test\n    @SuppressWarnings(\"unchecked\")\n    public void testCopyFromReturnSummaryWithFailedURI() throws Exception {\n        execute(\"create table t1 (id int primary key, ts timestamp with time zone)\");\n\n        Path tmpDir = newTempDir(LifecycleScope.TEST);\n        String tmpDirStr = tmpDir.toUri().toString();\n\n        String filename = \"nonexistingfile.json\";\n        execute(\"copy t1 from ? return summary\", new Object[]{tmpDirStr + filename});\n        assertThat(response).hasRowCount((long) cluster().numDataNodes());\n\n        boolean isRunningOnWindows = System.getProperty(\"os.name\").startsWith(\"Windows\");\n        String expected = isRunningOnWindows\n            ? \"(The system cannot find the file specified)\"\n            : \"(No such file or directory)\";\n        for (Object[] row : response.rows()) {\n            assertThat((String) row[1]).endsWith(filename);\n            assertThat(row[2]).isNull();\n            assertThat(row[3]).isNull();\n            assertThat(((Map<String, Object>) row[4]).keySet())\n                .anySatisfy(key -> assertThat(key).contains(expected));\n        }\n\n        // with shared=true, only 1 data node must try to process the uri\n        execute(\"copy t1 from ? with (shared=true) return summary\", new Object[]{tmpDirStr + filename});\n        assertThat(response).hasRowCount(1L);\n\n        for (Object[] row : response.rows()) {\n            assertThat((String) row[1]).endsWith(filename);\n            assertThat(row[2]).isNull();\n            assertThat(row[3]).isNull();\n            assertThat(((Map<String, Object>) row[4]).keySet())\n                .anySatisfy(key -> assertThat(key).contains(expected));\n        }\n\n        // with shared=true and wildcards all nodes will try to match a file\n        filename = \"*.json\";\n        execute(\"copy t1 from ? with (shared=true) return summary\", new Object[] {tmpDirStr + filename});\n        assertThat(response).hasRowCount((long) cluster().numDataNodes());\n\n        for (Object[] row : response.rows()) {\n            assertThat((String) row[1]).endsWith(\"*.json\");\n            assertThat(row[2]).isNull();\n            assertThat(row[3]).isNull();\n            assertThat(((Map<String, Object>) row[4]).keySet())\n                .anySatisfy(key -> assertThat(key).contains(\"Cannot find any URI matching:\"));\n        }\n    }\n\n    @Test\n    public void test_copy_from_csv_file_with_empty_string_as_null_property() throws Exception {\n        execute(\n            \"CREATE TABLE t (id int primary key, name text) \" +\n            \"CLUSTERED INTO 1 SHARDS \");\n        File file = folder.newFile(UUID.randomUUID().toString());\n\n        List<String> lines = List.of(\n            \"id,name\",\n            \"1, \\\"foo\\\"\",\n            \"2,\\\"\\\"\",\n            \"3,\"\n        );\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\n            \"COPY t FROM ? WITH (format='csv', empty_string_as_null=true)\",\n            new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"SELECT * FROM t ORDER BY id\");\n        assertThat(response).hasRows(\n            \"1| foo\",\n            \"2| NULL\",\n            \"3| NULL\"\n        );\n    }\n\n    @Test\n    public void test_can_import_data_requiring_cast_from_csv_into_partitioned_table() throws Exception {\n        execute(\n            \"\"\"\n                    create table tbl (\n                        ts timestamp with time zone not null,\n                        ts_month timestamp with time zone generated always as date_trunc('month', ts)\n                    ) partitioned by (ts_month)\n                \"\"\");\n        List<String> lines = List.of(\n            \"ts\",\n            \"1626188198073\"\n        );\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"COPY tbl FROM ? WITH (format = 'csv', shared = true)\",\n                new Object[] {Paths.get(file.toURI()).toUri().toString()}\n        );\n        execute(\"refresh table tbl\");\n        execute(\"SELECT * FROM tbl\");\n        assertThat(response).hasRows(\n            \"1626188198073| 1625097600000\"\n        );\n    }\n\n    @UseNewCluster\n    @Test\n    public void test_copy_excludes_partitioned_values_from_source() throws Exception {\n        execute(\"create table tbl (x int, p int) partitioned by (p)\");\n\n        {\n            List<String> lines = List.of(\n                \"\"\"\n                {\"x\": 10, \"p\": 1}\n                \"\"\"\n            );\n            File file = folder.newFile(UUID.randomUUID().toString());\n            Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n            execute(\n                \"copy tbl from ? with (wait_for_completion = true, shared = true)\",\n                new Object[] { file.toPath().toUri().toString() }\n            );\n            execute(\"refresh table tbl\");\n            execute(\"SELECT _raw, * FROM tbl\");\n            assertThat(response).hasRows(\n                \"{\\\"1\\\":10}| 10| 1\"\n            );\n        }\n\n        {\n            execute(\"create table tbl2 (x int, o object as (p int)) partitioned by (o['p'])\");\n            List<String> lines = List.of(\n                \"\"\"\n                {\"x\": 10, \"o\": {\"p\": 1}}\n                \"\"\"\n            );\n            File file = folder.newFile(UUID.randomUUID().toString());\n            Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n            execute(\n                \"copy tbl2 from ? with (wait_for_completion = true, shared = true)\",\n                new Object[] { file.toPath().toUri().toString() }\n            );\n            execute(\"refresh table tbl2\");\n            execute(\"SELECT _raw, * FROM tbl2\");\n            assertThat(response).hasRows(\n                \"{\\\"3\\\":10,\\\"4\\\":{}}| 10| {p=1}\"\n            );\n        }\n    }\n\n    @Test\n    public void test_copy_from_unknown_column_to_strict_object() throws Exception {\n        // test for strict_table ver. can be found at FromRawInsertSourceTest, whereas strict_object is tested by DocumentMapper\n        execute(\"create table t (o object(strict) as (a int))\");\n\n        List<String> lines = List.of(\n            \"{\\\"o\\\": {\\\"a\\\":123, \\\"b\\\":456}}\"\n        );\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? return summary\", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(printedTable(response.rows())).contains(\n            \"Cannot add column `b` to strict object `o`\");\n    }\n\n    @Test\n    public void test_copy_from_unknown_column_to_dynamic_object() throws Exception {\n        execute(\"create table t (o object(dynamic) as (a int))\");\n\n        List<String> lines = List.of(\n            \"{\\\"o\\\": {\\\"a\\\":123, \\\"b\\\":456}}\"\n        );\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true)\", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        execute(\"refresh table t\");\n        execute(\"select o['a'] + o['b'] from t\");\n        assertThat(printedTable(response.rows())).isEqualTo(\"579\\n\");\n    }\n\n    @Test\n    public void testCopyFromWithValidationSetToTrueDoesTypeValidation() throws Exception {\n\n        // copying an empty string to a boolean column\n\n        execute(\"create table t (a boolean)\");\n\n        List<String> lines = List.of(\"{\\\"a\\\": \\\"\\\"}\");\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true, validation = true) return summary\",\n                new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(printedTable(response.rows())).contains(\"Cannot cast value `` to type `boolean`\");\n        execute(\"refresh table t\");\n        execute(\"select count(*) from t\");\n        assertThat(response.rows()[0][0]).isEqualTo(0L);\n    }\n\n    @Test\n    public void testCopyFromWithValidationSetToFalseStillValidatesIfGeneratedColumnsInvolved() throws Exception {\n        execute(\"create table t (a boolean, b int generated always as 1)\");\n\n        List<String> lines = List.of(\"{\\\"a\\\": \\\"\\\"}\");\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true, validation = false) return summary\",\n                new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(printedTable(response.rows())).contains(\"Cannot cast value `` to type `boolean`\");\n        execute(\"refresh table t\");\n        execute(\"select count(*) from t\");\n        assertThat(response.rows()[0][0]).isEqualTo(0L);\n    }\n\n    @Test\n    public void testCopyFromWithValidationSetToFalseAndInsertingToPartitionedByColumn() throws Exception {\n        // copying an empty string to a boolean column\n\n        execute(\"create table t (a boolean, b boolean) partitioned by (b)\");\n\n        List<String> lines = List.of(\"{\\\"b\\\": \\\"\\\"}\");\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true, validation = false) return summary\",\n                new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(printedTable(response.rows())).contains(\n            // The validation should be skipped but since it is partitioned by column, that is used to create shards,\n            // the values cannot stay raw. Notice that the error message is different from,\n            // Cannot cast value `` to type `boolean`\n            \"Can't convert \\\"\\\" to boolean={count=1, line_numbers=[1]}\"\n        );\n        execute(\"refresh table t\");\n        execute(\"select count(*) from t\");\n        assertThat(response.rows()[0][0]).isEqualTo(0L);\n    }\n\n    @Test\n    public void test_validation_set_to_false_has_no_effect_and_results_in_validation_errors() throws Exception {\n        // copying an empty string to a boolean column\n\n        execute(\"create table t (a boolean, b boolean) partitioned by (b)\");\n\n        List<String> lines = List.of(\"{\\\"a\\\": \\\"\\\"}\"); // a\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true, validation = false) return summary\",\n                new Object[]{Paths.get(file.toURI()).toUri().toString()});\n\n        assertThat(response.rows()[0][4].toString()).contains(\"Cannot cast value `` to type `boolean`\");\n    }\n\n    @Test\n    public void testCopyFromWithValidationSetToFalseStillValidatesIfDefaultExpressionsInvolved() throws Exception {\n        execute(\"create table t (a boolean, b int default 1)\");\n\n        List<String> lines = List.of(\"{\\\"a\\\": \\\"\\\"}\");\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true, validation = false) return summary\",\n                new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(printedTable(response.rows())).contains(\"Cannot cast value `` to type `boolean`\");\n        execute(\"refresh table t\");\n        execute(\"select count(*) from t\");\n        assertThat(response.rows()[0][0]).isEqualTo(0L);\n    }\n\n    @Test\n    public void test_copy_from_fail_gracefully_in_case_of_invalid_data() throws Exception {\n        execute(\"create table t (obj object(dynamic) as (x int))\");\n\n        // Checking out all variants from https://github.com/crate/crate/issues/12201#issuecomment-1072472337\n\n        // Intentionally \"sandwiching\" valid line between 2 invalids as there used to be test failure depending on the valid/invalid order.\n        List<String> lines = List.of(\n            \"obj\\n\",\n            \"1,2\\n\",\n            \"\\\"{\\\"\\\"x\\\"\\\":1}\\\"\\n\",   // \"{\"\"x\"\":1}\" - works\n            \"3,4\\n\",\n            \"\\\"{\\\"x\\\":1}\\\"\\n\",       // \"{\"x\":1}\"\n            \"\\\"'{\\\"\\\"x\\\"\\\":1}'\\\"\\n\", // \"'{\"\"x\"\":1}'\"\n            \"\\\"{\\\"x\\\" = 1}\\\"\\n\",     // \"{\"x\" = 1}\"\n            \"\\\"{\\\"\\\"x\\\"\\\" = 1}\\\"\\n\", // \"{\"\"x\"\" = 1}\"\n            \"{\\\"\\\"x\\\"\\\":1}\\n\",       // {\"\"x\"\":1}\n            \"{\\\"x\\\":1}\\n\",           // {\"x\":1} - works\n            \"{\\\"x\\\" = 1}\\n\",         // {\"x\" = 1}\n            \"{x = 1}\\n\"              // {x = 1}\n        );\n\n        File file = folder.newFile(UUID.randomUUID() + \".csv\");\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n        execute(\"copy t from ? with (shared = true) return summary\", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(response.rows()[0][2]).isEqualTo(2L);\n        assertThat(response.rows()[0][3]).isEqualTo(9L);\n    }\n\n    @Test\n    public void test_copy_preserves_implied_top_level_column_order() throws IOException {\n        execute(\n            \"\"\"\n                create table t (\n                    p int\n                ) partitioned by (p) with (column_policy = 'dynamic');\n                \"\"\"\n        );\n        var lines = List.of(\n            \"\"\"\n            {\"b\":1, \"a\":1, \"d\":1, \"c\":1}\n            \"\"\"\n        );\n        var file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n        execute(\"copy t from ? \", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        execute(\"refresh table t\");\n        execute(\"select * from t\");\n        assertThat(response.cols())\n            // follow the same order as provided by '{\"b\":1, \"a\":1, \"d\":1, \"c\":1}'\n            .isEqualTo(new String[] {\"p\", \"b\", \"a\", \"d\", \"c\"});\n    }\n\n    @Test\n    public void test_copy_preserves_the_implied_sub_column_order() throws IOException {\n        execute(\n            \"\"\"\n                create table doc.t (\n                    p int,\n                    o object\n                ) partitioned by (p) with (column_policy = 'dynamic');\n                \"\"\"\n        );\n        var lines = List.of(\n            \"\"\"\n            {\"o\":{\"c\":1, \"a\":{\"d\":1, \"b\":1, \"c\":1, \"a\":1}, \"b\":1}}\n            \"\"\"\n        );\n        var file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n        execute(\"copy doc.t from ? \", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        execute(\"refresh table doc.t\");\n        execute(\"show create table doc.t\");\n        assertThat(printedTable(response.rows()))\n            // follow the same order as provided by '{\"o\":{\"c\":1, \"a\":{\"d\":1, \"b\":1, \"c\":1, \"a\":1}, \"b\":1}}'\n            .contains(\n                \"CREATE TABLE IF NOT EXISTS \\\"doc\\\".\\\"t\\\" (\\n\" +\n                \"   \\\"p\\\" INTEGER,\\n\" +\n                \"   \\\"o\\\" OBJECT(DYNAMIC) AS (\\n\" +\n                \"      \\\"c\\\" BIGINT,\\n\" +\n                \"      \\\"a\\\" OBJECT(DYNAMIC) AS (\\n\" +\n                \"         \\\"d\\\" BIGINT,\\n\" +\n                \"         \\\"b\\\" BIGINT,\\n\" +\n                \"         \\\"c\\\" BIGINT,\\n\" +\n                \"         \\\"a\\\" BIGINT\\n\" +\n                \"      ),\\n\" +\n                \"      \\\"b\\\" BIGINT\\n\" +\n                \"   )\\n\" +\n                \")\"\n            );\n    }\n\n    @Test\n    public void test_generated_non_deterministic_value_is_consistent_on_primary_and_replica() throws Exception {\n        execute(\"\"\"\n            create table tbl (x int, created generated always as round((random() + 1) * 100))\n            clustered into 1 shards\n            with (number_of_replicas = 1)\n            \"\"\"\n        );\n        Path path = tmpFileWithLines(Arrays.asList(\n            \"{\\\"x\\\": 1}\"\n        ));\n        execute(\"copy tbl from ? with (shared=true)\", new Object[] { path.toUri().toString() + \"*.json\"});\n\n        execute(\"refresh table tbl\");\n        execute(\"select x, created from tbl\");\n\n        // (int) response.rows()[0][0] used to be null because replica\n        // used regular Indexer instead of RawIndexer\n        // with values [\"{\\\"x\\\": 1}\"][some_long_timestamp], ie tried to write String value into int column.\n\n        int x = (int) response.rows()[0][0];\n        long created = (long) response.rows()[0][1];\n\n        // some iterations to ensure it hits both primary and replica\n        for (int i = 0; i < 30; i++) {\n            execute(\"select x, created from tbl\").rows();\n            assertThat(response.rows()[0][0]).isEqualTo(x);\n            assertThat(response.rows()[0][1]).isEqualTo(created);\n        }\n    }\n\n    @Test\n    public void test_copy_from_can_import_json_with_same_columns_in_different_order() throws Exception {\n        execute(\"\"\"\n            create table t (\n                 id long,\n                 first_column long,\n                 second_column string,\n                 third_column long,\n                 primary key (id)\n            )\n            \"\"\"\n        );\n\n        var lines = List.of(\n            \"\"\"\n                {\"id\":1,\"first_column\":38392,\"second_column\":\"apple safari\",\"third_column\":151155}\n                {\"id\":2,\"second_column\":\"apple safari\",\"third_column\":23073,\"first_column\":31123}\n            \"\"\"\n        );\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true)\", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        execute(\"refresh table t\");\n        execute(\"select * from t order by id\");\n        assertThat(response).hasRows(\n            \"1| 38392| apple safari| 151155\",\n            \"2| 31123| apple safari| 23073\"\n        );\n    }\n}\n"], "fixing_code": [".. _version_5.5.4:\n\n==========================\nVersion 5.5.4 - Unreleased\n==========================\n\n\n.. comment 1. Remove the \" - Unreleased\" from the header above and adjust the ==\n.. comment 2. Remove the NOTE below and replace with: \"Released on 20XX-XX-XX.\"\n.. comment    (without a NOTE entry, simply starting from col 1 of the line)\n\n.. NOTE::\n    In development. 5.5.4 isn't released yet. These are the release notes for\n    the upcoming release.\n\n.. NOTE::\n    If you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher\n    before you upgrade to 5.5.4.\n\n    We recommend that you upgrade to the latest 5.4 release before moving to\n    5.5.4.\n\n    A rolling upgrade from 5.4.x to 5.5.4 is supported.\n\n    Before upgrading, you should `back up your data`_.\n\n.. WARNING::\n\n    Tables that were created before CrateDB 4.x will not function with 5.x\n    and must be recreated before moving to 5.x.x.\n\n    You can recreate tables using ``COPY TO`` and ``COPY FROM`` or by\n    `inserting the data into a new table`_.\n\n.. _back up your data: https://crate.io/docs/crate/reference/en/latest/admin/snapshots.html\n\n.. _inserting the data into a new table: https://crate.io/docs/crate/reference/en/latest/admin/system-information.html#tables-need-to-be-recreated\n\n.. rubric:: Table of contents\n\n.. contents::\n   :local:\n\nSee the :ref:`version_5.5.0` release notes for a full list of changes in the\n5.5 series.\n\nSecurity Fixes\n==============\n\n- Fixed a security issue where any CrateDB user could read/import the content of\n  any file on the host system, the CrateDB process user has read access to, by\n  using the ``COPY FROM`` command with a file URI. This access is now restricted\n  to the ``crate`` superuser only.\n\nFixes\n=====\n\n- Fixed an issue that caused ``SELECT`` statements with ``WHERE``\n  clause having an equality condition on a primary key to return ``NULL`` when\n  selecting an object sub-column of ``ARRAY(OBJECT)`` type.\n\n- Fixed an issue that caused queries to return invalid results when the\n  ``WHERE`` clause involving ``primary key`` columns have the following\n  form::\n\n    SELECT * FROM t WHERE NOT(pk_col != 1 AND pk_col IS NULL);\n\n  An equivalent query that returned valid results::\n\n    SELECT * FROM t WHERE pk_col = 1 OR pk_col IS NOT NULL;\n\n- Fixed an issue that caused failure of a statement, mixing correlated subquery\n  and sub-select. An example::\n\n    CREATE TABLE tbl(x INT);\n    INSERT INTO tbl(x) VALUES (1);\n    SELECT (\n       SELECT x FROM tbl\n          WHERE t.x = tbl.x\n        AND\n          tbl.x IN (SELECT generate_series from generate_series(1, 1))\n    ) FROM tbl t;\n", ".. _version_5.6.1:\n\n==========================\nVersion 5.6.1 - Unreleased\n==========================\n\n\n.. comment 1. Remove the \" - Unreleased\" from the header above and adjust the ==\n.. comment 2. Remove the NOTE below and replace with: \"Released on 20XX-XX-XX.\"\n.. comment    (without a NOTE entry, simply starting from col 1 of the line)\n\n.. NOTE::\n    In development. 5.6.1 isn't released yet. These are the release notes for\n    the upcoming release.\n\n.. NOTE::\n\n    If you are upgrading a cluster, you must be running CrateDB 4.0.2 or higher\n    before you upgrade to 5.6.1.\n\n    We recommend that you upgrade to the latest 5.5 release before moving to\n    5.6.1.\n\n    A rolling upgrade from 5.5.x to 5.6.1 is supported.\n    Before upgrading, you should `back up your data`_.\n\n.. WARNING::\n\n    Tables that were created before CrateDB 4.x will not function with 5.x\n    and must be recreated before moving to 5.x.x.\n\n    You can recreate tables using ``COPY TO`` and ``COPY FROM`` or by\n    `inserting the data into a new table`_.\n\n.. _back up your data: https://crate.io/docs/crate/reference/en/latest/admin/snapshots.html\n.. _inserting the data into a new table: https://crate.io/docs/crate/reference/en/latest/admin/system-information.html#tables-need-to-be-recreated\n\n.. rubric:: Table of contents\n\n.. contents::\n   :local:\n\nSee the :ref:`version_5.6.0` release notes for a full list of changes in the\n5.6 series.\n\nSecurity Fixes\n==============\n\n- Fixed a security issue where any CrateDB user could read/import the content of\n  any file on the host system, the CrateDB process user has read access to, by\n  using the ``COPY FROM`` command with a file URI. This access is now restricted\n  to the ``crate`` superuser only.\n\nFixes\n=====\n\n- Added a workaround for a change in JDK 21.0.2 which caused many operations to\n  get stuck.\n\n- Fixed an issue that led to errors when\n  :ref:`privileges <administration-privileges>` are defined for users, when\n  performing a rolling upgrade of a cluster from a version before\n  :ref:`version_5.6.0` to :ref:`version_5.6.0`.\n\n- Fixed an issue that caused ``SELECT`` statements with ``WHERE``\n  clause having an equality condition on a primary key to return ``NULL`` when\n  selecting an object sub-column of ``ARRAY(OBJECT)`` type.\n\n- Fixed an issue that caused queries to return invalid results when the\n  ``WHERE`` clause involving ``primary key`` columns have the following\n  form::\n\n    SELECT * FROM t WHERE NOT(pk_col != 1 AND pk_col IS NULL);\n\n  An equivalent query that returned valid results::\n\n    SELECT * FROM t WHERE pk_col = 1 OR pk_col IS NOT NULL;\n\n- Fixed an issue that caused failure of a statement, mixing correlated subquery\n  and sub-select. An example::\n\n    CREATE TABLE tbl(x INT);\n    INSERT INTO tbl(x) VALUES (1);\n    SELECT (\n       SELECT x FROM tbl\n          WHERE t.x = tbl.x\n        AND\n          tbl.x IN (SELECT generate_series from generate_series(1, 1))\n    ) FROM tbl t;\n", ".. highlight:: psql\n\n.. _sql-copy-from:\n\n=============\n``COPY FROM``\n=============\n\nYou can use the ``COPY FROM`` :ref:`statement <gloss-statement>` to copy data\nfrom a file into a table.\n\n.. SEEALSO::\n\n    :ref:`Data manipulation: Import and export <dml-import-export>`\n\n    :ref:`SQL syntax: COPY TO <sql-copy-to>`\n\n.. rubric:: Table of contents\n\n.. contents::\n   :local:\n   :depth: 2\n\n.. _sql-copy-from-synopsis:\n\nSynopsis\n========\n\n::\n\n    COPY table_identifier\n      [ ( column_ident [, ...] ) ]\n      [ PARTITION (partition_column = value [ , ... ]) ]\n      FROM uri [ WITH ( option = value [, ...] ) ] [ RETURN SUMMARY ]\n\n\n.. _sql-copy-from-desc:\n\nDescription\n===========\n\nA ``COPY FROM`` copies data from a URI to the specified table.\n\nThe nodes in the cluster will attempt to read the files available at the URI\nand import the data.\n\nHere's an example:\n\n::\n\n    cr> COPY quotes FROM 'file:///tmp/import_data/quotes.json';\n    COPY OK, 3 rows affected (... sec)\n\n.. NOTE::\n\n    The ``COPY`` statements use :ref:`Overload Protection <overload_protection>` to ensure other\n    queries can still perform. Please change these settings during large inserts if needed.\n\n.. _sql-copy-from-formats:\n\nFile formats\n------------\n\nCrateDB accepts both JSON and CSV inputs. The format is inferred from the file\nextension (``.json`` or ``.csv`` respectively) if possible. The :ref:`format\n<sql-copy-from-format>` can also be set as an option. If a format is not\nspecified and the format cannot be inferred, the file will be processed as\nJSON.\n\nJSON files must contain a single JSON object per line and all files must be\nUTF-8 encoded. Also, any empty lines are skipped.\n\nExample JSON data::\n\n    {\"id\": 1, \"quote\": \"Don't panic\"}\n    {\"id\": 2, \"quote\": \"Ford, you're turning into a penguin. Stop it.\"}\n\nA CSV file may or may not contain a header. See :ref:`CSV header option\n<sql-copy-from-header>` for further details.\n\nExample CSV data::\n\n    id,quote\n    1,\"Don't panic\"\n    2,\"Ford, you're turning into a penguin. Stop it.\"\n\nExample CSV data with no header::\n\n    1,\"Don't panic\"\n    2,\"Ford, you're turning into a penguin. Stop it.\"\n\nSee also: :ref:`dml-importing-data`.\n\n\n.. _sql-copy-from-type-checks:\n\nData type checks\n----------------\n\nCrateDB checks if the columns' data types match the types from the import file.\nIt casts the types and will always import the data as in the source file.\nFurthermore CrateDB will check for all :ref:`column_constraints`.\n\nFor example a `WKT`_ string cannot be imported into a column of ``geo_shape``\nor ``geo_point`` type, since there is no implicit cast to the `GeoJSON`_ format.\n\n.. NOTE::\n\n   In case the ``COPY FROM`` statement fails, the log output on the node will\n   provide an error message. Any data that has been imported until then has\n   been written to the table and should be deleted before restarting the\n   import.\n\n\n.. _sql-copy-from-params:\n\nParameters\n==========\n\n.. _sql-copy-from-table_ident:\n\n``table_ident``\n  The name (optionally schema-qualified) of an existing table where the data\n  should be put.\n\n.. _sql-copy-from-column_ident:\n\n``column_ident``\n  Used in an optional columns declaration, each ``column_ident`` is the name of a column in the ``table_ident`` table.\n\n  This currently only has an effect if using the CSV file format. See the ``header`` section for how it behaves.\n\n.. _sql-copy-from-uri:\n\n``uri``\n  An expression or array of expressions. Each :ref:`expression\n  <gloss-expression>` must :ref:`evaluate <gloss-evaluation>` to a string\n  literal that is a `well-formed URI`_.\n\n  URIs must use one of the supported :ref:`URI schemes\n  <sql-copy-from-schemes>`. CrateDB supports :ref:`globbing\n  <sql-copy-from-globbing>` for the :ref:`file <sql-copy-from-file>` and\n  :ref:`s3 <sql-copy-from-s3>` URI schemes.\n\n  .. NOTE::\n\n      If the URI scheme is missing, CrateDB assumes the value is a pathname and\n      will prepend the :ref:`file <sql-copy-from-file>` URI scheme (i.e.,\n      ``file://``). So, for example, CrateDB will convert ``/tmp/file.json`` to\n      ``file:///tmp/file.json``.\n\n\n.. _sql-copy-from-globbing:\n\nURI globbing\n------------\n\nWith :ref:`file <sql-copy-from-file>` and :ref:`s3 <sql-copy-from-s3>` URI\nschemes, you can use pathname `globbing`_ (i.e., ``*`` wildcards) with the\n``COPY FROM`` statement to construct URIs that can match multiple directories\nand files.\n\nSuppose you used ``file:///tmp/import_data/*/*.json`` as the URI. This URI\nwould match all JSON files located in subdirectories of the\n``/tmp/import_data`` directory.\n\nSo, for example, these files would match:\n\n- ``/tmp/import_data/foo/1.json``\n- ``/tmp/import_data/bar/2.json``\n- ``/tmp/import_data/1/boz.json``\n\n.. CAUTION::\n\n    A file named ``/tmp/import_data/foo/.json`` would also match the\n    ``file:///tmp/import_data/*/*.json`` URI. The ``*`` wildcard matches any\n    number of characters, including none.\n\nHowever, these files would not match:\n\n- ``/tmp/import_data/1.json`` (two few subdirectories)\n- ``/tmp/import_data/foo/bar/2.json`` (too many subdirectories)\n- ``/tmp/import_data/1/boz.js`` (file extension mismatch)\n\n\n.. _sql-copy-from-schemes:\n\nURI schemes\n-----------\n\nCrateDB supports the following URI schemes:\n\n.. contents::\n   :local:\n   :depth: 1\n\n\n.. _sql-copy-from-file:\n\n``file``\n''''''''\n\nYou can use the ``file://`` scheme to specify an absolute path to one or more\nfiles accessible via the local filesystem of one or more CrateDB nodes.\n\nFor example:\n\n.. code-block:: text\n\n    file:///path/to/dir\n\nThe files must be accessible on at least one node and the system user running\nthe ``crate`` process must have read access to every file specified.\nAdditionally, only the ``crate`` superuser is allowed to use the ``file://``\nscheme.\n\nBy default, every node will attempt to import every file. If the file is\naccessible on multiple nodes, you can set the `shared`_ option to true in order\nto avoid importing duplicates.\n\nUse :ref:`sql-copy-from-return-summary` to get information about what actions\nwere performed on each node.\n\n.. TIP::\n\n    If you are running CrateDB inside a container, the file must be inside the\n    container. If you are using *Docker*, you may have to configure a `Docker\n    volume`_ to accomplish this.\n\n.. TIP::\n\n    If you are using *Microsoft Windows*, you must include the drive letter in\n    the file URI.\n\n    For example:\n\n    .. code-block:: text\n\n        file://C:\\/tmp/import_data/quotes.json\n\n    Consult the `Windows documentation`_ for more information.\n\n\n.. _sql-copy-from-s3:\n\n``s3``\n''''''\n\nYou can use the ``s3://`` scheme to access buckets on the `Amazon Simple\nStorage Service`_ (Amazon S3).\n\nFor example:\n\n.. code-block:: text\n\n    s3://[<accesskey>:<secretkey>@][<host>:<port>/]<bucketname>/<path>\n\nS3 compatible storage providers can be specified by the optional pair of host\nand port, which defaults to Amazon S3 if not provided.\n\nHere is a more concrete example:\n\n.. code-block:: text\n\n    COPY t FROM 's3://accessKey:secretKey@s3.amazonaws.com:443/myBucket/key/a.json' with (protocol = 'https')\n\nIf no credentials are set the s3 client will operate in anonymous mode.\nSee `AWS Java Documentation`_.\n\nUsing the ``s3://`` scheme automatically sets the `shared`_ to true.\n\n.. TIP::\n\n   A ``secretkey`` provided by Amazon Web Services can contain characters such\n   as '/', '+' or '='. These characters must be `URL encoded`_. For a detailed\n   explanation read the official `AWS documentation`_.\n\n   To escape a secret key, you can use a snippet like this:\n\n   .. code-block:: console\n\n      sh$ python -c \"from getpass import getpass; from urllib.parse import quote_plus; print(quote_plus(getpass('secret_key: ')))\"\n\n   This will prompt for the secret key and print the encoded variant.\n\n   Additionally, versions prior to 0.51.x use HTTP for connections to S3. Since\n   0.51.x these connections are using the HTTPS protocol. Please make sure you\n   update your firewall rules to allow outgoing connections on port ``443``.\n\n\n.. _sql-copy-from-other-schemes:\n\nOther schemes\n'''''''''''''\n\nIn addition to the schemes above, CrateDB supports all protocols supported by\nthe `URL`_ implementation of its JVM (typically ``http``, ``https``, ``ftp``,\nand ``jar``). Please refer to the documentation of the JVM vendor for an\naccurate list of supported protocols.\n\n.. NOTE::\n\n    These schemes *do not* support wildcard expansion.\n\n\n.. _sql-copy-from-clauses:\n\nClauses\n=======\n\nThe ``COPY FROM`` :ref:`statement <gloss-statement>` supports the following\nclauses:\n\n.. contents::\n   :local:\n   :depth: 1\n\n\n.. _sql-copy-from-partition:\n\n``PARTITION``\n-------------\n\n.. EDITORIAL NOTE\n   ##############\n\n   Multiple files (in this directory) use the same standard text for\n   documenting the ``PARTITION`` clause. (Minor verb changes are made to\n   accomodate the specifics of the parent statement.)\n\n   For consistency, if you make changes here, please be sure to make a\n   corresponding change to the other files.\n\nIf the table is :ref:`partitioned <partitioned-tables>`, the optional\n``PARTITION`` clause can be used to import data into one partition exclusively.\n\n::\n\n    [ PARTITION ( partition_column = value [ , ... ] ) ]\n\n:partition_column:\n  One of the column names used for table partitioning\n\n:value:\n  The respective column value.\n\nAll :ref:`partition columns <gloss-partition-column>` (specified by the\n:ref:`sql-create-table-partitioned-by` clause) must be listed inside the\nparentheses along with their respective values using the ``partition_column =\nvalue`` syntax (separated by commas).\n\nBecause each partition corresponds to a unique set of :ref:`partition column\n<gloss-partition-column>` row values, this clause uniquely identifies a single\npartition for import.\n\n.. TIP::\n\n    The :ref:`ref-show-create-table` statement will show you the complete list\n    of partition columns specified by the\n    :ref:`sql-create-table-partitioned-by` clause.\n\n.. CAUTION::\n\n    Partitioned tables do not store the row values for the partition columns,\n    hence every row will be imported into the specified partition regardless of\n    partition column values.\n\n\n.. _sql-copy-from-with:\n\n``WITH``\n--------\n\nYou can use the optional ``WITH`` clause to specify option values.\n\n::\n\n    [ WITH ( option = value [, ...] ) ]\n\nThe ``WITH`` clause supports the following options:\n\n.. contents::\n   :local:\n   :depth: 1\n\n\n.. _sql-copy-from-bulk_size:\n\n``bulk_size``\n'''''''''''''\n\nCrateDB will process the lines it reads from the ``path`` in bulks. This option\nspecifies the size of one batch. The provided value must be greater than 0, the\ndefault value is 10000.\n\n\n.. _sql-copy-from-fail_fast:\n\n``fail_fast``\n'''''''''''''\n\nA boolean value indicating if the ``COPY FROM`` operation should abort early\nafter an error. This is best effort and due to the distributed execution, it\nmay continue processing some records before it aborts.\nDefaults to ``false``.\n\n.. _sql-copy-from-wait_for_completion:\n\n``wait_for_completion``\n'''''''''''''''''''''''\n\nA boolean value indicating if the ``COPY FROM`` should wait for\nthe copy operation to complete. If set to ``false`` the request\nreturns at once and the copy operation runs in the background.\nDefaults to ``true``.\n\n.. _sql-copy-from-shared:\n\n``shared``\n''''''''''\n\nThis option should be set to true if the URIs location is accessible by more\nthan one CrateDB node to prevent them from importing the same file.\n\nThe default value depends on the scheme of each URI.\n\nIf an array of URIs is passed to ``COPY FROM`` this option will overwrite the\ndefault for *all* URIs.\n\n\n.. _sql-copy-from-node_filters:\n\n``node_filters``\n''''''''''''''''\n\nA filter :ref:`expression <gloss-expression>` to select the nodes to run the\n*read* operation.\n\nIt's an object in the form of::\n\n    {\n        name = '<node_name_regex>',\n        id = '<node_id_regex>'\n    }\n\nOnly one of the keys is required.\n\nThe ``name`` :ref:`regular expression <gloss-regular-expression>` is applied on\nthe ``name`` of all execution nodes, whereas the ``id`` regex is applied on the\n``node id``.\n\nIf both keys are set, *both* regular expressions have to match for a node to be\nincluded.\n\nIf the `shared`_ option is false, a strict node filter might exclude nodes with\naccess to the data leading to a partial import.\n\nTo verify which nodes match the filter, run the statement with\n:ref:`EXPLAIN <ref-explain>`.\n\n\n.. _sql-copy-from-num_readers:\n\n``num_readers``\n'''''''''''''''\n\nThe number of nodes that will read the resources specified in the URI. Defaults\nto the number of nodes available in the cluster. If the option is set to a\nnumber greater than the number of available nodes it will still use each node\nonly once to do the import. However, the value must be an integer greater than\n0.\n\nIf `shared`_ is set to false this option has to be used with caution. It might\nexclude the wrong nodes, causing COPY FROM to read no files or only a subset of\nthe files.\n\n\n.. _sql-copy-from-compression:\n\n``compression``\n'''''''''''''''\n\nThe default value is ``null``, set to ``gzip`` to read gzipped files.\n\n\n.. _sql-copy-from-protocol:\n\n``protocol``\n'''''''''''''''\n\nUsed for :ref:`s3 <sql-copy-from-s3>` scheme only. It is set to HTTPS by\ndefault.\n\n\n.. _sql-copy-from-overwrite_duplicates:\n\n``overwrite_duplicates``\n''''''''''''''''''''''''\n\nDefault: false\n\n``COPY FROM`` by default won't overwrite rows if a document with the same\nprimary key already exists. Set to true to overwrite duplicate rows.\n\n\n.. _sql-copy-from-empty_string_as_null:\n\n``empty_string_as_null``\n''''''''''''''''''''''''\n\nIf set to ``true`` the ``empty_string_as_null`` option enables conversion of\nempty strings into ``NULL``. The default value is ``false`` meaning that no\naction will be taken on empty strings during the COPY FROM execution.\n\nThe option is only supported when using the ``CSV`` format, otherwise, it will\nbe ignored.\n\n\n.. _sql-copy-from-delimiter:\n\n``delimiter``\n'''''''''''''\n\nSpecifies a single one-byte character that separates columns within each line\nof the file. The default delimiter is ``,``.\n\nThe option is only supported when using the ``CSV`` format, otherwise, it will\nbe ignored.\n\n\n.. _sql-copy-from-format:\n\n``format``\n''''''''''\n\nThis option specifies the format of the input file. Available formats are\n``csv`` or ``json``. If a format is not specified and the format cannot be\nguessed from the file extension, the file will be processed as JSON.\n\n\n.. _sql-copy-from-header:\n\n``header``\n''''''''''\n\nUsed to indicate if the first line of a CSV file contains a header with the\ncolumn names. Defaults to ``true``.\n\nIf set to ``false``, the CSV must not contain column names in the first line\nand instead the columns declared in the statement are used. If no columns are\ndeclared in the statement, it will default to all columns present in the table\nin their ``CREATE TABLE`` declaration order.\n\nIf set to ``true`` the first line in the CSV file must contain the column\nnames. You can use the optional column declaration in addition to import only a\nsubset of the data.\n\nIf the statement contains no column declarations, all fields in the CSV are\nread and if it contains fields where there is no matching column in the table,\nthe behavior depends on the ``column_policy`` table setting. If ``dynamic`` it\nimplicitly adds new columns, if ``strict`` the operation will fail.\n\nAn example of using input file with no header\n\n::\n\n    cr> COPY quotes FROM 'file:///tmp/import_data/quotes.csv' with (format='csv', header=false);\n    COPY OK, 3 rows affected (... sec)\n\n\n.. _sql-copy-from-skip:\n\n``skip``\n''''''''\n\nDefault: ``0``\n\nSetting this option to ``n`` skips the first ``n`` rows while copying.\n\n.. NOTE::\n\n    CrateDB by default expects a header in CSV files. If you're using the SKIP\n    option to skip the header, you have to set ``header = false`` as well. See\n    :ref:`header <sql-copy-from-header>`.\n\n\n.. _sql-copy-from-return-summary:\n\n``RETURN SUMMARY``\n------------------\n\nBy using the optional ``RETURN SUMMARY`` clause, a per-node result set will be\nreturned containing information about possible failures and successfully\ninserted records.\n\n::\n\n    [ RETURN SUMMARY ]\n\n+---------------------------------------+------------------------------------------------+---------------+\n| Column Name                           | Description                                    |  Return Type  |\n+=======================================+================================================+===============+\n| ``node``                              | Information about the node that has processed  | ``OBJECT``    |\n|                                       | the URI resource.                              |               |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``node['id']``                        | The id of the node.                            | ``TEXT``      |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``node['name']``                      | The name of the node.                          | ``TEXT``      |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``uri``                               | The URI the node has processed.                | ``TEXT``      |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``error_count``                       | The total number of records which failed.      | ``BIGINT``    |\n|                                       | A NULL value indicates a general URI reading   |               |\n|                                       | error, the error will be listed inside the     |               |\n|                                       | ``errors`` column.                             |               |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``success_count``                     | The total number of records which were         | ``BIGINT``    |\n|                                       | inserted.                                      |               |\n|                                       | A NULL value indicates a general URI reading   |               |\n|                                       | error, the error will be listed inside the     |               |\n|                                       | ``errors`` column.                             |               |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``errors``                            | Contains detailed information about all        | ``OBJECT``    |\n|                                       | errors. Limited to at most 25 error messages.  |               |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``errors[ERROR_MSG]``                 | Contains information about a type of an error. | ``OBJECT``    |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``errors[ERROR_MSG]['count']``        | The number records failed with this error.     | ``BIGINT``    |\n+---------------------------------------+------------------------------------------------+---------------+\n| ``errors[ERROR_MSG]['line_numbers']`` | The line numbers of the source URI where the   | ``ARRAY``     |\n|                                       | error occurred, limited to the first 50        |               |\n|                                       | errors, to avoid buffer pressure on clients.   |               |\n+---------------------------------------+------------------------------------------------+---------------+\n\n\n.. _Amazon Simple Storage Service: https://aws.amazon.com/s3/\n.. _AWS documentation: https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html\n.. _AWS Java Documentation: https://docs.aws.amazon.com/AmazonS3/latest/dev/AuthUsingAcctOrUserCredJava.html\n.. _Docker volume: https://docs.docker.com/storage/volumes/\n.. _GeoJSON: https://geojson.org/\n.. _globbing: https://en.wikipedia.org/wiki/Glob_(programming)\n.. _percent-encoding: https://en.wikipedia.org/wiki/Percent-encoding\n.. _URI Scheme: https://en.wikipedia.org/wiki/URI_scheme\n.. _URL encoded: https://en.wikipedia.org/wiki/Percent-encoding\n.. _URL: https://docs.oracle.com/javase/8/docs/api/java/net/URL.html\n.. _well-formed URI: https://www.ietf.org/rfc/rfc2396.txt\n.. _Windows documentation: https://docs.microsoft.com/en-us/dotnet/standard/io/file-path-formats\n.. _WKT: https://en.wikipedia.org/wiki/Well-known_text\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.copy.s3;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.mockito.ArgumentMatchers.any;\nimport static org.mockito.ArgumentMatchers.anyString;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.when;\n\nimport java.net.SocketTimeoutException;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.TimeUnit;\n\nimport org.elasticsearch.common.settings.Settings;\nimport org.elasticsearch.test.ESTestCase;\nimport org.elasticsearch.threadpool.TestThreadPool;\nimport org.elasticsearch.threadpool.ThreadPool;\nimport org.junit.AfterClass;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\nimport org.mockito.Mockito;\nimport org.mockito.invocation.InvocationOnMock;\nimport org.mockito.stubbing.Answer;\n\nimport com.amazonaws.services.s3.AmazonS3;\nimport com.amazonaws.services.s3.AmazonS3Client;\nimport com.amazonaws.services.s3.model.ObjectListing;\nimport com.amazonaws.services.s3.model.S3Object;\nimport com.amazonaws.services.s3.model.S3ObjectInputStream;\nimport com.amazonaws.services.s3.model.S3ObjectSummary;\n\nimport io.crate.copy.s3.common.S3ClientHelper;\nimport io.crate.data.BatchIterator;\nimport io.crate.execution.engine.collect.files.FileReadingIterator;\nimport io.crate.execution.engine.collect.files.FileReadingIterator.LineCursor;\n\npublic class S3FileReadingCollectorTest extends ESTestCase {\n    private static ThreadPool THREAD_POOL;\n\n    @BeforeClass\n    public static void setUpClass() throws Exception {\n        THREAD_POOL = new TestThreadPool(Thread.currentThread().getName());\n    }\n\n\n    @AfterClass\n    public static void tearDownClass() {\n        ThreadPool.terminate(THREAD_POOL, 30, TimeUnit.SECONDS);\n    }\n\n    @Test\n    public void testCollectFromS3Uri() throws Throwable {\n        // this test just verifies the s3 schema detection and bucketName / prefix extraction from the uri.\n        // real s3 interaction is mocked completely.\n        S3ObjectInputStream inputStream = mock(S3ObjectInputStream.class);\n        when(inputStream.read(any(byte[].class), Mockito.anyInt(), Mockito.anyInt())).thenReturn(-1);\n\n        FileReadingIterator it = createBatchIterator(inputStream, \"s3://fakebucket/foo\");\n        assertThat(it.moveNext()).isFalse();\n    }\n\n    @Test\n    public void testCollectWithOneSocketTimeout() throws Throwable {\n        S3ObjectInputStream inputStream = mock(S3ObjectInputStream.class);\n\n        when(inputStream.read(any(byte[].class), Mockito.anyInt(), Mockito.anyInt()))\n            .thenAnswer(new WriteBufferAnswer(new byte[]{102, 111, 111, 10}))  // first line: foo\n            .thenThrow(new SocketTimeoutException())  // exception causes retry\n            .thenAnswer(new WriteBufferAnswer(new byte[]{102, 111, 111, 10}))  // first line again, because of retry\n            .thenAnswer(new WriteBufferAnswer(new byte[]{98, 97, 114, 10}))  // second line: bar\n            .thenReturn(-1);\n\n        FileReadingIterator it = createBatchIterator(inputStream, \"s3://fakebucket/foo\");\n        BatchIterator<LineCursor> immutableLines = it.map(LineCursor::copy);\n        List<LineCursor> lines = immutableLines.toList().get(5, TimeUnit.SECONDS);\n        assertThat(lines).satisfiesExactly(\n            line1 -> assertThat(line1.line()).isEqualTo(\"foo\"),\n            line1 -> assertThat(line1.line()).isEqualTo(\"bar\")\n        );\n    }\n\n\n    private FileReadingIterator createBatchIterator(S3ObjectInputStream inputStream, String ... fileUris) {\n        String compression = null;\n        return new FileReadingIterator(\n            Arrays.stream(fileUris).map(FileReadingIterator::toURI).toList(),\n            compression,\n            Map.of(\n                S3FileInputFactory.NAME,\n                (uri, withClauseOptions) -> new S3FileInput(new S3ClientHelper() {\n                    @Override\n                    protected AmazonS3 initClient(String accessKey, String secretKey, String endpoint, String protocol) {\n                        AmazonS3 client = mock(AmazonS3Client.class);\n                        ObjectListing objectListing = mock(ObjectListing.class);\n                        S3ObjectSummary summary = mock(S3ObjectSummary.class);\n                        S3Object s3Object = mock(S3Object.class);\n                        when(client.listObjects(anyString(), anyString())).thenReturn(objectListing);\n                        when(objectListing.getObjectSummaries()).thenReturn(Collections.singletonList(summary));\n                        when(summary.getKey()).thenReturn(\"foo\");\n                        when(client.getObject(\"fakebucket\", \"foo\")).thenReturn(s3Object);\n                        when(s3Object.getObjectContent()).thenReturn(inputStream);\n                        when(client.listNextBatchOfObjects(any(ObjectListing.class))).thenReturn(objectListing);\n                        when(objectListing.isTruncated()).thenReturn(false);\n                        return client;\n                    }\n                }, uri, \"https\")),\n            false,\n            1,\n            0,\n            Settings.EMPTY,\n            THREAD_POOL.scheduler());\n    }\n\n    private record WriteBufferAnswer(byte[] bytes) implements Answer<Integer> {\n\n        @Override\n        public Integer answer(InvocationOnMock invocation) {\n            byte[] buffer = (byte[]) invocation.getArguments()[0];\n            System.arraycopy(bytes, 0, buffer, 0, bytes.length);\n            return bytes.length;\n        }\n    }\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.exceptions;\n\nimport java.io.IOException;\n\nimport org.elasticsearch.ElasticsearchException;\nimport org.elasticsearch.common.io.stream.StreamInput;\n\npublic class UnauthorizedException extends ElasticsearchException implements UnscopedException {\n\n    public UnauthorizedException(String message) {\n        super(message);\n    }\n\n    public UnauthorizedException(StreamInput in) throws IOException {\n        super(in);\n    }\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect.files;\n\nimport static io.crate.common.exceptions.Exceptions.rethrowUnchecked;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.net.SocketException;\nimport java.net.SocketTimeoutException;\nimport java.net.URI;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Paths;\nimport java.util.Collection;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Predicate;\nimport java.util.zip.GZIPInputStream;\n\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\nimport org.elasticsearch.action.bulk.BackoffPolicy;\nimport org.elasticsearch.common.settings.Settings;\nimport org.jetbrains.annotations.NotNull;\nimport org.jetbrains.annotations.Nullable;\n\nimport io.crate.common.annotations.VisibleForTesting;\nimport io.crate.common.exceptions.Exceptions;\nimport io.crate.common.unit.TimeValue;\nimport io.crate.data.BatchIterator;\n\n/**\n * BatchIterator to read lines from one or more {@link URI}s.\n *\n * <p>\n * URIs are opened using a {@link FileInputFactory}.\n * A map from Scheme -> FileInputFactory is parameterized in the constructor to support\n * arbitrary sources.\n * </p>\n *\n * <p>\n * The iterator automatically retries reading on\n * @{link {@link SocketException} or {@link SocketTimeoutException}\n * </p>\n *\n * <p>\n * The file content is exposed via a shared {@link LineCursor}\n * It's properties are mutated after each {@link #moveNext()} call.\n * Use {@link LineCursor#copy()} if you need an instance that's not shared.\n *\n * {@link #currentElement()} can be used in a \"off-position\", before the first {@link #moveNext()} call\n * to gain early access to the cursor.\n * </p>\n */\npublic class FileReadingIterator implements BatchIterator<FileReadingIterator.LineCursor> {\n\n    private static final Logger LOGGER = LogManager.getLogger(FileReadingIterator.class);\n    @VisibleForTesting\n    static final int MAX_SOCKET_TIMEOUT_RETRIES = 5;\n\n    private static final Predicate<URI> MATCH_ALL_PREDICATE = (URI input) -> true;\n\n    private final Map<String, FileInputFactory> fileInputFactories;\n    private final Boolean shared;\n    private final int numReaders;\n    private final int readerNumber;\n    private final boolean compressed;\n    private final List<FileInput> fileInputs;\n\n    private volatile Throwable killed;\n\n    private Iterator<FileInput> fileInputsIterator = null;\n    private FileInput currentInput = null;\n    private Iterator<URI> currentInputUriIterator = null;\n    private BufferedReader currentReader = null;\n\n    @VisibleForTesting\n    long watermark;\n\n    private final LineCursor cursor;\n    private final ScheduledExecutorService scheduler;\n    private final Iterator<TimeValue> backOffPolicy;\n\n    public static class LineCursor {\n        private URI uri;\n        private long lineNumber;\n        private String line;\n        private IOException failure;\n\n        public LineCursor() {\n        }\n\n        public LineCursor(URI uri, long lineNumber, @Nullable String line, @Nullable IOException failure) {\n            this.uri = uri;\n            this.lineNumber = lineNumber;\n            this.line = line;\n            this.failure = failure;\n        }\n\n        public URI uri() {\n            return uri;\n        }\n\n        public long lineNumber() {\n            return lineNumber;\n        }\n\n        @Nullable\n        public String line() {\n            return line;\n        }\n\n        @Nullable\n        public IOException failure() {\n            return failure;\n        }\n\n        @VisibleForTesting\n        public LineCursor copy() {\n            return new LineCursor(uri, lineNumber, line, failure);\n        }\n\n        @Override\n        public String toString() {\n            return \"LineCursor{\" + uri + \":\" + lineNumber + \":line=\" + line + \", failure=\" + failure + \"}\";\n        }\n\n        @Override\n        public int hashCode() {\n            return Objects.hash(uri, lineNumber, line, failure);\n        }\n\n        @Override\n        public boolean equals(Object obj) {\n            if (this == obj) {\n                return true;\n            }\n            if (obj == null) {\n                return false;\n            }\n            if (getClass() != obj.getClass()) {\n                return false;\n            }\n            LineCursor other = (LineCursor) obj;\n            return Objects.equals(uri, other.uri)\n                && lineNumber == other.lineNumber\n                && Objects.equals(line, other.line)\n                && Objects.equals(failure, other.failure);\n        }\n    }\n\n    public FileReadingIterator(Collection<URI> fileUris,\n                               String compression,\n                               Map<String, FileInputFactory> fileInputFactories,\n                               Boolean shared,\n                               int numReaders,\n                               int readerNumber,\n                               Settings withClauseOptions,\n                               ScheduledExecutorService scheduler) {\n        this.compressed = compression != null && compression.equalsIgnoreCase(\"gzip\");\n        this.fileInputFactories = fileInputFactories;\n        this.cursor = new LineCursor();\n        this.shared = shared;\n        this.numReaders = numReaders;\n        this.readerNumber = readerNumber;\n        this.scheduler = scheduler;\n        this.backOffPolicy = BackoffPolicy.exponentialBackoff(TimeValue.ZERO, MAX_SOCKET_TIMEOUT_RETRIES).iterator();\n\n        this.fileInputs = fileUris.stream()\n            .map(uri -> toFileInput(uri, withClauseOptions))\n            .filter(Objects::nonNull)\n            .toList();\n        fileInputsIterator = fileInputs.iterator();\n    }\n\n    @Override\n    public LineCursor currentElement() {\n        return cursor;\n    }\n\n    @Override\n    public void kill(@NotNull Throwable throwable) {\n        killed = throwable;\n    }\n\n    @Override\n    public void moveToStart() {\n        raiseIfKilled();\n        reset();\n        watermark = 0;\n        fileInputsIterator = fileInputs.iterator();\n    }\n\n    @Override\n    public boolean moveNext() {\n        raiseIfKilled();\n        try {\n            if (currentReader != null) {\n                String line;\n                try {\n                    line = getLine(currentReader);\n                } catch (SocketException | SocketTimeoutException e) {\n                    if (backOffPolicy.hasNext()) {\n                        return false;\n                    }\n                    throw e;\n                }\n                if (line == null) {\n                    closeReader();\n                    return moveNext();\n                }\n                cursor.line = line;\n                cursor.failure = null;\n                return true;\n            } else if (currentInputUriIterator != null && currentInputUriIterator.hasNext()) {\n                advanceToNextUri(currentInput);\n                return moveNext();\n            } else if (fileInputsIterator != null && fileInputsIterator.hasNext()) {\n                advanceToNextFileInput();\n                return moveNext();\n            } else {\n                reset();\n                return false;\n            }\n        } catch (IOException e) {\n            cursor.failure = e;\n            closeReader();\n            // If IOError happens on file opening, let consumers collect the error\n            // This is mostly for RETURN SUMMARY of COPY FROM\n            if (cursor.lineNumber == 0) {\n                return true;\n            }\n            return moveNext();\n        }\n    }\n\n    private void advanceToNextUri(FileInput fileInput) throws IOException {\n        watermark = 0;\n        createReader(fileInput, currentInputUriIterator.next());\n    }\n\n    private void advanceToNextFileInput() throws IOException {\n        currentInput = fileInputsIterator.next();\n        List<URI> uris = currentInput.expandUri().stream().filter(this::shouldBeReadByCurrentNode).toList();\n        if (uris.size() > 0) {\n            currentInputUriIterator = uris.iterator();\n            advanceToNextUri(currentInput);\n        } else if (currentInput.isGlobbed()) {\n            URI uri = currentInput.uri();\n            cursor.uri = uri;\n            throw new IOException(\"Cannot find any URI matching: \" + uri.toString());\n        }\n    }\n\n    private boolean shouldBeReadByCurrentNode(URI uri) {\n        boolean sharedStorage = Objects.requireNonNullElse(shared, currentInput.sharedStorageDefault());\n        if (sharedStorage) {\n            return moduloPredicateImpl(uri, this.readerNumber, this.numReaders);\n        } else {\n            return MATCH_ALL_PREDICATE.test(uri);\n        }\n    }\n\n    private void createReader(FileInput fileInput, URI uri) throws IOException {\n        cursor.uri = uri;\n        cursor.lineNumber = 0;\n        InputStream stream = fileInput.getStream(uri);\n        currentReader = createBufferedReader(stream);\n    }\n\n    private void closeReader() {\n        if (currentReader != null) {\n            try {\n                currentReader.close();\n            } catch (IOException e) {\n                LOGGER.error(\"Unable to close reader for \" + cursor.uri, e);\n            }\n            currentReader = null;\n        }\n    }\n\n    private String getLine(BufferedReader reader) throws IOException {\n        String line = null;\n        try {\n            while ((line = reader.readLine()) != null) {\n                cursor.lineNumber++;\n                if (cursor.lineNumber < watermark) {\n                    continue;\n                } else {\n                    watermark = 0;\n                }\n                if (line.length() == 0) {\n                    continue;\n                }\n                break;\n            }\n        } catch (SocketException | SocketTimeoutException e) {\n            if (backOffPolicy.hasNext()) {\n                watermark = watermark == 0 ? cursor.lineNumber + 1 : watermark;\n                closeReader();\n                createReader(currentInput, cursor.uri);\n            } else {\n                URI uri = currentInput.uri();\n                LOGGER.error(\"Timeout during COPY FROM '\" + uri.toString() +\n                             \"' after \" + MAX_SOCKET_TIMEOUT_RETRIES +\n                             \" retries\", e);\n            }\n            throw e;\n        } catch (Exception e) {\n            URI uri = currentInput.uri();\n            // it's nice to know which exact file/uri threw an error\n            // when COPY FROM returns less rows than expected\n            LOGGER.error(\"Error during COPY FROM '\" + uri.toString() + \"'\", e);\n            rethrowUnchecked(e);\n        }\n        return line;\n    }\n\n    @Override\n    public void close() {\n        closeReader();\n        reset();\n        killed = BatchIterator.CLOSED;\n    }\n\n    private void reset() {\n        fileInputsIterator = null;\n        currentInputUriIterator = null;\n        currentInput = null;\n        cursor.failure = null;\n    }\n\n    @Override\n    public CompletableFuture<?> loadNextBatch() throws IOException {\n        if (backOffPolicy.hasNext()) {\n            CompletableFuture<Void> cf = new CompletableFuture<>();\n            scheduler.schedule(\n                (Runnable) () -> cf.complete(null), // cast to Runnable for enabling mockito tests\n                backOffPolicy.next().millis(),\n                TimeUnit.MILLISECONDS);\n            return cf;\n        } else {\n            throw new IllegalStateException(\"All batches already loaded\");\n        }\n    }\n\n    @Override\n    public boolean allLoaded() {\n        return !backOffPolicy.hasNext();\n    }\n\n    @Override\n    public boolean hasLazyResultSet() {\n        return true;\n    }\n\n    @VisibleForTesting\n    public static URI toURI(String fileUri) {\n        if (fileUri.startsWith(\"/\")) {\n            // using Paths.get().toUri instead of new URI(...) as it also encodes umlauts and other special characters\n            return Paths.get(fileUri).toUri();\n        } else {\n            URI uri = URI.create(fileUri);\n            if (uri.getScheme() == null) {\n                throw new IllegalArgumentException(\"relative fileURIs are not allowed\");\n            }\n            if (uri.getScheme().equals(\"file\") && !uri.getSchemeSpecificPart().startsWith(\"///\")) {\n                throw new IllegalArgumentException(\"Invalid fileURI\");\n            }\n            return uri;\n        }\n    }\n\n    @Nullable\n    private FileInput toFileInput(URI uri, Settings withClauseOptions) {\n        FileInputFactory fileInputFactory = fileInputFactories.get(uri.getScheme());\n        if (fileInputFactory != null) {\n            try {\n                return fileInputFactory.create(uri, withClauseOptions);\n            } catch (IOException e) {\n                return null;\n            }\n        }\n        return new URLFileInput(uri);\n    }\n\n    @VisibleForTesting\n    BufferedReader createBufferedReader(InputStream inputStream) throws IOException {\n        BufferedReader reader;\n        if (compressed) {\n            reader = new BufferedReader(new InputStreamReader(new GZIPInputStream(inputStream),\n                StandardCharsets.UTF_8));\n        } else {\n            reader = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8));\n        }\n        return reader;\n    }\n\n    @VisibleForTesting\n    public static boolean moduloPredicateImpl(URI input, int readerNumber, int numReaders) {\n        int hash = input.hashCode();\n        if (hash == Integer.MIN_VALUE) {\n            hash = 0; // Math.abs(Integer.MIN_VALUE) == Integer.MIN_VALUE\n        }\n        return Math.abs(hash) % numReaders == readerNumber;\n    }\n\n    private void raiseIfKilled() {\n        if (killed != null) {\n            Exceptions.rethrowUnchecked(killed);\n        }\n    }\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect.sources;\n\nimport static java.util.Objects.requireNonNull;\n\nimport java.net.URI;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.CompletableFuture;\n\nimport org.elasticsearch.cluster.service.ClusterService;\nimport org.elasticsearch.common.inject.Inject;\nimport org.elasticsearch.common.inject.Singleton;\nimport org.elasticsearch.threadpool.ThreadPool;\n\nimport io.crate.analyze.AnalyzedCopyFrom;\nimport io.crate.analyze.CopyFromParserProperties;\nimport io.crate.analyze.SymbolEvaluator;\nimport io.crate.common.annotations.VisibleForTesting;\nimport io.crate.data.BatchIterator;\nimport io.crate.data.Row;\nimport io.crate.data.SkippingBatchIterator;\nimport io.crate.exceptions.UnauthorizedException;\nimport io.crate.execution.dsl.phases.CollectPhase;\nimport io.crate.execution.dsl.phases.FileUriCollectPhase;\nimport io.crate.execution.engine.collect.CollectTask;\nimport io.crate.execution.engine.collect.files.FileInputFactory;\nimport io.crate.execution.engine.collect.files.FileReadingIterator;\nimport io.crate.execution.engine.collect.files.LineCollectorExpression;\nimport io.crate.execution.engine.collect.files.LineProcessor;\nimport io.crate.expression.InputFactory;\nimport io.crate.expression.reference.file.FileLineReferenceResolver;\nimport io.crate.expression.symbol.Symbol;\nimport io.crate.metadata.NodeContext;\nimport io.crate.metadata.TransactionContext;\nimport io.crate.planner.operators.SubQueryResults;\nimport io.crate.role.Role;\nimport io.crate.role.Roles;\nimport io.crate.types.DataTypes;\n\n@Singleton\npublic class FileCollectSource implements CollectSource {\n\n    private final ClusterService clusterService;\n    private final Map<String, FileInputFactory> fileInputFactoryMap;\n    private final InputFactory inputFactory;\n    private final NodeContext nodeCtx;\n    private final ThreadPool threadPool;\n    private final Roles roles;\n\n    @Inject\n    public FileCollectSource(NodeContext nodeCtx,\n                             ClusterService clusterService,\n                             Map<String, FileInputFactory> fileInputFactoryMap,\n                             ThreadPool threadPool,\n                             Roles roles) {\n        this.fileInputFactoryMap = fileInputFactoryMap;\n        this.nodeCtx = nodeCtx;\n        this.inputFactory = new InputFactory(nodeCtx);\n        this.clusterService = clusterService;\n        this.threadPool = threadPool;\n        this.roles = roles;\n    }\n\n    @Override\n    public CompletableFuture<BatchIterator<Row>> getIterator(TransactionContext txnCtx,\n                                                             CollectPhase collectPhase,\n                                                             CollectTask collectTask,\n                                                             boolean supportMoveToStart) {\n        FileUriCollectPhase fileUriCollectPhase = (FileUriCollectPhase) collectPhase;\n        InputFactory.Context<LineCollectorExpression<?>> ctx =\n            inputFactory.ctxForRefs(txnCtx, FileLineReferenceResolver::getImplementation);\n        ctx.add(collectPhase.toCollect());\n\n        Role user = requireNonNull(roles.findUser(txnCtx.sessionSettings().userName()), \"User who invoked a statement must exist\");\n        List<URI> fileUris = targetUriToStringList(txnCtx, nodeCtx, fileUriCollectPhase.targetUri()).stream()\n            .map(s -> {\n                var uri = FileReadingIterator.toURI(s);\n                if (uri.getScheme().equals(\"file\") && user.isSuperUser() == false) {\n                    throw new UnauthorizedException(\"Only a superuser can read from the local file system\");\n                }\n                return uri;\n            })\n            .toList();\n        FileReadingIterator fileReadingIterator = new FileReadingIterator(\n            fileUris,\n            fileUriCollectPhase.compression(),\n            fileInputFactoryMap,\n            fileUriCollectPhase.sharedStorage(),\n            fileUriCollectPhase.nodeIds().size(),\n            getReaderNumber(fileUriCollectPhase.nodeIds(), clusterService.state().nodes().getLocalNodeId()),\n            fileUriCollectPhase.withClauseOptions(),\n            threadPool.scheduler()\n        );\n        CopyFromParserProperties parserProperties = fileUriCollectPhase.parserProperties();\n        LineProcessor lineProcessor = new LineProcessor(\n            parserProperties.skipNumLines() > 0\n                ? new SkippingBatchIterator<>(fileReadingIterator, (int) parserProperties.skipNumLines())\n                : fileReadingIterator,\n            ctx.topLevelInputs(),\n            ctx.expressions(),\n            fileUriCollectPhase.inputFormat(),\n            parserProperties,\n            fileUriCollectPhase.targetColumns()\n        );\n        return CompletableFuture.completedFuture(lineProcessor);\n    }\n\n    @VisibleForTesting\n    public static int getReaderNumber(Collection<String> nodeIds, String localNodeId) {\n        String[] readers = nodeIds.toArray(new String[0]);\n        Arrays.sort(readers);\n        return Arrays.binarySearch(readers, localNodeId);\n    }\n\n    private static List<String> targetUriToStringList(TransactionContext txnCtx,\n                                                      NodeContext nodeCtx,\n                                                      Symbol targetUri) {\n        Object value = SymbolEvaluator.evaluate(txnCtx, nodeCtx, targetUri, Row.EMPTY, SubQueryResults.EMPTY);\n        if (targetUri.valueType().id() == DataTypes.STRING.id()) {\n            String uri = (String) value;\n            return Collections.singletonList(uri);\n        } else if (DataTypes.STRING_ARRAY.equals(targetUri.valueType())) {\n            return DataTypes.STRING_ARRAY.implicitCast(value);\n        }\n\n        // this case actually never happens because the check is already done in the analyzer\n        throw AnalyzedCopyFrom.raiseInvalidType(targetUri.valueType());\n    }\n}\n", "/*\n * Licensed to Elasticsearch under one or more contributor\n * license agreements. See the NOTICE file distributed with\n * this work for additional information regarding copyright\n * ownership. Elasticsearch licenses this file to you under\n * the Apache License, Version 2.0 (the \"License\"); you may\n * not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\npackage org.elasticsearch;\n\nimport static io.crate.server.xcontent.XContentParserUtils.ensureExpectedToken;\nimport static java.util.Collections.emptyMap;\nimport static java.util.Collections.unmodifiableMap;\nimport static org.elasticsearch.cluster.metadata.IndexMetadata.INDEX_UUID_NA_VALUE;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.stream.Collectors;\n\nimport org.elasticsearch.action.admin.indices.alias.AliasesNotFoundException;\nimport org.elasticsearch.action.support.replication.ReplicationOperation;\nimport org.elasticsearch.cluster.action.shard.ShardStateAction;\nimport org.elasticsearch.common.io.stream.StreamInput;\nimport org.elasticsearch.common.io.stream.StreamOutput;\nimport org.elasticsearch.common.io.stream.Writeable;\nimport org.elasticsearch.common.logging.LoggerMessageFormat;\nimport org.elasticsearch.common.xcontent.ParseField;\nimport org.elasticsearch.common.xcontent.ToXContentFragment;\nimport org.elasticsearch.common.xcontent.XContentBuilder;\nimport org.elasticsearch.common.xcontent.XContentParser;\nimport org.elasticsearch.index.Index;\nimport org.elasticsearch.index.shard.ShardId;\nimport org.elasticsearch.rest.RestStatus;\nimport org.elasticsearch.transport.TcpTransport;\n\nimport io.crate.common.CheckedFunction;\nimport io.crate.common.exceptions.Exceptions;\nimport io.crate.exceptions.ArrayViaDocValuesUnsupportedException;\nimport io.crate.exceptions.SQLExceptions;\n\n/**\n * A base class for all elasticsearch exceptions.\n */\npublic class ElasticsearchException extends RuntimeException implements ToXContentFragment, Writeable {\n\n    private static final Version UNKNOWN_VERSION_ADDED = Version.fromId(0);\n\n    /**\n     * Passed in the {@link Params} of {@link #generateThrowableXContent(XContentBuilder, Params, Throwable)}\n     * to control if the {@code caused_by} element should render. Unlike most parameters to {@code toXContent} methods this parameter is\n     * internal only and not available as a URL parameter.\n     */\n    private static final String REST_EXCEPTION_SKIP_CAUSE = \"rest.exception.cause.skip\";\n    /**\n     * Passed in the {@link Params} of {@link #generateThrowableXContent(XContentBuilder, Params, Throwable)}\n     * to control if the {@code stack_trace} element should render. Unlike most parameters to {@code toXContent} methods this parameter is\n     * internal only and not available as a URL parameter. Use the {@code error_trace} parameter instead.\n     */\n    public static final String REST_EXCEPTION_SKIP_STACK_TRACE = \"rest.exception.stacktrace.skip\";\n    public static final boolean REST_EXCEPTION_SKIP_STACK_TRACE_DEFAULT = true;\n    private static final boolean REST_EXCEPTION_SKIP_CAUSE_DEFAULT = false;\n    private static final String INDEX_METADATA_KEY = \"es.index\";\n    private static final String INDEX_METADATA_KEY_UUID = \"es.index_uuid\";\n    private static final String SHARD_METADATA_KEY = \"es.shard\";\n    private static final String RESOURCE_METADATA_TYPE_KEY = \"es.resource.type\";\n    private static final String RESOURCE_METADATA_ID_KEY = \"es.resource.id\";\n\n    private static final String TYPE = \"type\";\n    private static final String REASON = \"reason\";\n    private static final String CAUSED_BY = \"caused_by\";\n    private static final ParseField SUPPRESSED = new ParseField(\"suppressed\");\n    private static final String STACK_TRACE = \"stack_trace\";\n    private static final String HEADER = \"header\";\n    private static final String ROOT_CAUSE = \"root_cause\";\n\n    private static final Map<Integer, CheckedFunction<StreamInput, ? extends ElasticsearchException, IOException>> ID_TO_SUPPLIER;\n    private static final Map<Class<? extends ElasticsearchException>, ElasticsearchExceptionHandle> CLASS_TO_ELASTICSEARCH_EXCEPTION_HANDLE;\n    private final Map<String, List<String>> metadata = new HashMap<>();\n    private final Map<String, List<String>> headers = new HashMap<>();\n\n    /**\n     * Construct a <code>ElasticsearchException</code> with the specified cause exception.\n     */\n    public ElasticsearchException(Throwable cause) {\n        super(cause);\n    }\n\n    /**\n     * Construct a <code>ElasticsearchException</code> with the specified detail message.\n     *\n     * The message can be parameterized using <code>{}</code> as placeholders for the given\n     * arguments\n     *\n     * @param msg  the detail message\n     * @param args the arguments for the message\n     */\n    public ElasticsearchException(String msg, Object... args) {\n        super(LoggerMessageFormat.format(msg, args));\n    }\n\n    /**\n     * Construct a <code>ElasticsearchException</code> with the specified detail message\n     * and nested exception.\n     *\n     * The message can be parameterized using <code>{}</code> as placeholders for the given\n     * arguments\n     *\n     * @param msg   the detail message\n     * @param cause the nested exception\n     * @param args  the arguments for the message\n     */\n    public ElasticsearchException(String msg, Throwable cause, Object... args) {\n        super(LoggerMessageFormat.format(msg, args), cause);\n    }\n\n    public ElasticsearchException(StreamInput in) throws IOException {\n        super(in.readOptionalString(), in.readException());\n        readStackTrace(this, in);\n        headers.putAll(in.readMapOfLists(StreamInput::readString, StreamInput::readString));\n        metadata.putAll(in.readMapOfLists(StreamInput::readString, StreamInput::readString));\n    }\n\n    /**\n     * Adds a new piece of metadata with the given key.\n     * If the provided key is already present, the corresponding metadata will be replaced\n     */\n    public void addMetadata(String key, String... values) {\n        addMetadata(key, Arrays.asList(values));\n    }\n\n    /**\n     * Adds a new piece of metadata with the given key.\n     * If the provided key is already present, the corresponding metadata will be replaced\n     */\n    public void addMetadata(String key, List<String> values) {\n        //we need to enforce this otherwise bw comp doesn't work properly, as \"es.\" was the previous criteria to split headers in two sets\n        if (key.startsWith(\"es.\") == false) {\n            throw new IllegalArgumentException(\"exception metadata must start with [es.], found [\" + key + \"] instead\");\n        }\n        this.metadata.put(key, values);\n    }\n\n    /**\n     * Returns a set of all metadata keys on this exception\n     */\n    public Set<String> getMetadataKeys() {\n        return metadata.keySet();\n    }\n\n    /**\n     * Returns the list of metadata values for the given key or {@code null} if no metadata for the\n     * given key exists.\n     */\n    public List<String> getMetadata(String key) {\n        return metadata.get(key);\n    }\n\n    protected Map<String, List<String>> getMetadata() {\n        return metadata;\n    }\n\n    /**\n     * Adds a new header with the given key.\n     * This method will replace existing header if a header with the same key already exists\n     */\n    public void addHeader(String key, List<String> value) {\n        //we need to enforce this otherwise bw comp doesn't work properly, as \"es.\" was the previous criteria to split headers in two sets\n        if (key.startsWith(\"es.\")) {\n            throw new IllegalArgumentException(\"exception headers must not start with [es.], found [\" + key + \"] instead\");\n        }\n        this.headers.put(key, value);\n    }\n\n    /**\n     * Returns a set of all header keys on this exception\n     */\n    public Set<String> getHeaderKeys() {\n        return headers.keySet();\n    }\n\n    /**\n     * Returns the list of header values for the given key or {@code null} if no header for the\n     * given key exists.\n     */\n    public List<String> getHeader(String key) {\n        return headers.get(key);\n    }\n\n    protected Map<String, List<String>> getHeaders() {\n        return headers;\n    }\n\n    /**\n     * Returns the rest status code associated with this exception.\n     */\n    public RestStatus status() {\n        Throwable cause = unwrapCause();\n        if (cause == this) {\n            return RestStatus.INTERNAL_SERVER_ERROR;\n        } else {\n            return SQLExceptions.status(cause);\n        }\n    }\n\n    /**\n     * Unwraps the actual cause from the exception for cases when the exception is a\n     * {@link ElasticsearchWrapperException}.\n     *\n     * @see ExceptionsHelper#unwrapCause(Throwable)\n     */\n    public Throwable unwrapCause() {\n        return SQLExceptions.unwrap(this);\n    }\n\n    /**\n     * Return the detail message, including the message from the nested exception\n     * if there is one.\n     */\n    public String getDetailedMessage() {\n        if (getCause() != null) {\n            StringBuilder sb = new StringBuilder();\n            sb.append(toString()).append(\"; \");\n            if (getCause() instanceof ElasticsearchException) {\n                sb.append(((ElasticsearchException) getCause()).getDetailedMessage());\n            } else {\n                sb.append(getCause());\n            }\n            return sb.toString();\n        } else {\n            return super.toString();\n        }\n    }\n\n    /**\n     * Retrieve the innermost cause of this exception, if none, returns the current exception.\n     */\n    public Throwable getRootCause() {\n        Throwable rootCause = this;\n        Throwable cause = getCause();\n        while (cause != null && cause != rootCause) {\n            rootCause = cause;\n            cause = cause.getCause();\n        }\n        return rootCause;\n    }\n\n    @Override\n    public void writeTo(StreamOutput out) throws IOException {\n        out.writeOptionalString(this.getMessage());\n        out.writeException(this.getCause());\n        writeStackTraces(this, out, StreamOutput::writeException);\n        out.writeMapOfLists(headers, StreamOutput::writeString, StreamOutput::writeString);\n        out.writeMapOfLists(metadata, StreamOutput::writeString, StreamOutput::writeString);\n    }\n\n    public static ElasticsearchException readException(StreamInput input, int id) throws IOException {\n        CheckedFunction<StreamInput, ? extends ElasticsearchException, IOException> elasticsearchException = ID_TO_SUPPLIER.get(id);\n        if (elasticsearchException == null) {\n            throw new IllegalStateException(\"unknown exception for id: \" + id);\n        }\n        return elasticsearchException.apply(input);\n    }\n\n    /**\n     * Returns <code>true</code> iff the given class is a registered for an exception to be read.\n     */\n    public static boolean isRegistered(Class<? extends Throwable> exception, Version version) {\n        ElasticsearchExceptionHandle elasticsearchExceptionHandle = CLASS_TO_ELASTICSEARCH_EXCEPTION_HANDLE.get(exception);\n        if (elasticsearchExceptionHandle != null) {\n            return version.onOrAfter(elasticsearchExceptionHandle.versionAdded);\n        }\n        return false;\n    }\n\n    /**\n     * Returns the serialization id the given exception.\n     */\n    public static int getId(Class<? extends ElasticsearchException> exception) {\n        return CLASS_TO_ELASTICSEARCH_EXCEPTION_HANDLE.get(exception).id;\n    }\n\n    @Override\n    public XContentBuilder toXContent(XContentBuilder builder, Params params) throws IOException {\n        Throwable ex = SQLExceptions.unwrap(this);\n        if (ex != this) {\n            generateThrowableXContent(builder, params, this);\n        } else {\n            innerToXContent(builder, params, this, getExceptionName(), getMessage(), headers, metadata, getCause());\n        }\n        return builder;\n    }\n\n    protected static void innerToXContent(XContentBuilder builder, Params params,\n                                          Throwable throwable, String type, String message, Map<String, List<String>> headers,\n                                          Map<String, List<String>> metadata, Throwable cause) throws IOException {\n        builder.field(TYPE, type);\n        builder.field(REASON, message);\n\n        for (Map.Entry<String, List<String>> entry : metadata.entrySet()) {\n            headerToXContent(builder, entry.getKey().substring(\"es.\".length()), entry.getValue());\n        }\n\n        if (throwable instanceof ElasticsearchException) {\n            ElasticsearchException exception = (ElasticsearchException) throwable;\n            exception.metadataToXContent(builder, params);\n        }\n\n        if (params.paramAsBoolean(REST_EXCEPTION_SKIP_CAUSE, REST_EXCEPTION_SKIP_CAUSE_DEFAULT) == false) {\n            if (cause != null) {\n                builder.field(CAUSED_BY);\n                builder.startObject();\n                generateThrowableXContent(builder, params, cause);\n                builder.endObject();\n            }\n        }\n\n        if (headers.isEmpty() == false) {\n            builder.startObject(HEADER);\n            for (Map.Entry<String, List<String>> entry : headers.entrySet()) {\n                headerToXContent(builder, entry.getKey(), entry.getValue());\n            }\n            builder.endObject();\n        }\n\n        if (params.paramAsBoolean(REST_EXCEPTION_SKIP_STACK_TRACE, REST_EXCEPTION_SKIP_STACK_TRACE_DEFAULT) == false) {\n            builder.field(STACK_TRACE, Exceptions.stackTrace(throwable));\n        }\n\n        Throwable[] allSuppressed = throwable.getSuppressed();\n        if (allSuppressed.length > 0) {\n            builder.startArray(SUPPRESSED.getPreferredName());\n            for (Throwable suppressed : allSuppressed) {\n                builder.startObject();\n                generateThrowableXContent(builder, params, suppressed);\n                builder.endObject();\n            }\n            builder.endArray();\n        }\n    }\n\n    private static void headerToXContent(XContentBuilder builder, String key, List<String> values) throws IOException {\n        if (values != null && values.isEmpty() == false) {\n            if (values.size() == 1) {\n                builder.field(key, values.get(0));\n            } else {\n                builder.startArray(key);\n                for (String value : values) {\n                    builder.value(value);\n                }\n                builder.endArray();\n            }\n        }\n    }\n\n    /**\n     * Renders additional per exception information into the XContent\n     */\n    protected void metadataToXContent(XContentBuilder builder, Params params) throws IOException {\n    }\n\n    /**\n     * Generate a {@link ElasticsearchException} from a {@link XContentParser}. This does not\n     * return the original exception type (ie NodeClosedException for example) but just wraps\n     * the type, the reason and the cause of the exception. It also recursively parses the\n     * tree structure of the cause, returning it as a tree structure of {@link ElasticsearchException}\n     * instances.\n     */\n    public static ElasticsearchException fromXContent(XContentParser parser) throws IOException {\n        XContentParser.Token token = parser.nextToken();\n        ensureExpectedToken(XContentParser.Token.FIELD_NAME, token, parser);\n        return innerFromXContent(parser, false);\n    }\n\n    public static ElasticsearchException innerFromXContent(XContentParser parser, boolean parseRootCauses) throws IOException {\n        XContentParser.Token token = parser.currentToken();\n        ensureExpectedToken(XContentParser.Token.FIELD_NAME, token, parser);\n\n        String type = null, reason = null, stack = null;\n        ElasticsearchException cause = null;\n        Map<String, List<String>> metadata = new HashMap<>();\n        Map<String, List<String>> headers = new HashMap<>();\n        List<ElasticsearchException> rootCauses = new ArrayList<>();\n        List<ElasticsearchException> suppressed = new ArrayList<>();\n\n        for (; token == XContentParser.Token.FIELD_NAME; token = parser.nextToken()) {\n            String currentFieldName = parser.currentName();\n            token = parser.nextToken();\n\n            if (token.isValue()) {\n                if (TYPE.equals(currentFieldName)) {\n                    type = parser.text();\n                } else if (REASON.equals(currentFieldName)) {\n                    reason = parser.text();\n                } else if (STACK_TRACE.equals(currentFieldName)) {\n                    stack = parser.text();\n                } else if (token == XContentParser.Token.VALUE_STRING) {\n                    metadata.put(currentFieldName, Collections.singletonList(parser.text()));\n                }\n            } else if (token == XContentParser.Token.START_OBJECT) {\n                if (CAUSED_BY.equals(currentFieldName)) {\n                    cause = fromXContent(parser);\n                } else if (HEADER.equals(currentFieldName)) {\n                    while ((token = parser.nextToken()) != XContentParser.Token.END_OBJECT) {\n                        if (token == XContentParser.Token.FIELD_NAME) {\n                            currentFieldName = parser.currentName();\n                        } else {\n                            List<String> values = headers.getOrDefault(currentFieldName, new ArrayList<>());\n                            if (token == XContentParser.Token.VALUE_STRING) {\n                                values.add(parser.text());\n                            } else if (token == XContentParser.Token.START_ARRAY) {\n                                while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {\n                                    if (token == XContentParser.Token.VALUE_STRING) {\n                                        values.add(parser.text());\n                                    } else {\n                                        parser.skipChildren();\n                                    }\n                                }\n                            } else if (token == XContentParser.Token.START_OBJECT) {\n                                parser.skipChildren();\n                            }\n                            headers.put(currentFieldName, values);\n                        }\n                    }\n                } else {\n                    // Any additional metadata object added by the metadataToXContent method is ignored\n                    // and skipped, so that the parser does not fail on unknown fields. The parser only\n                    // support metadata key-pairs and metadata arrays of values.\n                    parser.skipChildren();\n                }\n            } else if (token == XContentParser.Token.START_ARRAY) {\n                if (parseRootCauses && ROOT_CAUSE.equals(currentFieldName)) {\n                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {\n                        rootCauses.add(fromXContent(parser));\n                    }\n                } else if (SUPPRESSED.match(currentFieldName, parser.getDeprecationHandler())) {\n                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {\n                        suppressed.add(fromXContent(parser));\n                    }\n                } else {\n                    // Parse the array and add each item to the corresponding list of metadata.\n                    // Arrays of objects are not supported yet and just ignored and skipped.\n                    List<String> values = new ArrayList<>();\n                    while ((token = parser.nextToken()) != XContentParser.Token.END_ARRAY) {\n                        if (token == XContentParser.Token.VALUE_STRING) {\n                            values.add(parser.text());\n                        } else {\n                            parser.skipChildren();\n                        }\n                    }\n                    if (values.size() > 0) {\n                        if (metadata.containsKey(currentFieldName)) {\n                            values.addAll(metadata.get(currentFieldName));\n                        }\n                        metadata.put(currentFieldName, values);\n                    }\n                }\n            }\n        }\n\n        ElasticsearchException e = new ElasticsearchException(buildMessage(type, reason, stack), cause);\n        for (Map.Entry<String, List<String>> entry : metadata.entrySet()) {\n            //subclasses can print out additional metadata through the metadataToXContent method. Simple key-value pairs will be\n            //parsed back and become part of this metadata set, while objects and arrays are not supported when parsing back.\n            //Those key-value pairs become part of the metadata set and inherit the \"es.\" prefix as that is currently required\n            //by addMetadata. The prefix will get stripped out when printing metadata out so it will be effectively invisible.\n            //TODO move subclasses that print out simple metadata to using addMetadata directly and support also numbers and booleans.\n            //TODO rename metadataToXContent and have only SearchPhaseExecutionException use it, which prints out complex objects\n            e.addMetadata(\"es.\" + entry.getKey(), entry.getValue());\n        }\n        for (Map.Entry<String, List<String>> header : headers.entrySet()) {\n            e.addHeader(header.getKey(), header.getValue());\n        }\n\n        // Adds root causes as suppressed exception. This way they are not lost\n        // after parsing and can be retrieved using getSuppressed() method.\n        for (ElasticsearchException rootCause : rootCauses) {\n            e.addSuppressed(rootCause);\n        }\n        for (ElasticsearchException s : suppressed) {\n            e.addSuppressed(s);\n        }\n        return e;\n    }\n\n    /**\n     * Static toXContent helper method that renders {@link org.elasticsearch.ElasticsearchException} or {@link Throwable} instances\n     * as XContent, delegating the rendering to {@link #toXContent(XContentBuilder, Params)}\n     * or {@link #innerToXContent(XContentBuilder, Params, Throwable, String, String, Map, Map, Throwable)}.\n     *\n     * This method is usually used when the {@link Throwable} is rendered as a part of another XContent object, and its result can\n     * be parsed back using the {@link #fromXContent(XContentParser)} method.\n     */\n    public static void generateThrowableXContent(XContentBuilder builder, Params params, Throwable t) throws IOException {\n        t = SQLExceptions.unwrap(t);\n\n        if (t instanceof ElasticsearchException) {\n            ((ElasticsearchException) t).toXContent(builder, params);\n        } else {\n            innerToXContent(builder, params, t, getExceptionName(t), t.getMessage(), emptyMap(), emptyMap(), t.getCause());\n        }\n    }\n\n    protected String getExceptionName() {\n        return getExceptionName(this);\n    }\n\n    /**\n     * Returns a underscore case name for the given exception. This method strips {@code Elasticsearch} prefixes from exception names.\n     */\n    public static String getExceptionName(Throwable ex) {\n        String simpleName = ex.getClass().getSimpleName();\n        if (simpleName.startsWith(\"Elasticsearch\")) {\n            simpleName = simpleName.substring(\"Elasticsearch\".length());\n        }\n        // TODO: do we really need to make the exception name in underscore casing?\n        return toUnderscoreCase(simpleName);\n    }\n\n    static String buildMessage(String type, String reason, String stack) {\n        StringBuilder message = new StringBuilder(\"Elasticsearch exception [\");\n        message.append(TYPE).append('=').append(type).append(\", \");\n        message.append(REASON).append('=').append(reason);\n        if (stack != null) {\n            message.append(\", \").append(STACK_TRACE).append('=').append(stack);\n        }\n        message.append(']');\n        return message.toString();\n    }\n\n    @Override\n    public String toString() {\n        StringBuilder builder = new StringBuilder();\n        if (metadata.containsKey(INDEX_METADATA_KEY)) {\n            builder.append(getIndex());\n            if (metadata.containsKey(SHARD_METADATA_KEY)) {\n                builder.append('[').append(getShardId()).append(']');\n            }\n            builder.append(' ');\n        }\n        return builder.append(super.toString().trim()).toString();\n    }\n\n    /**\n     * Deserializes stacktrace elements as well as suppressed exceptions from the given output stream and\n     * adds it to the given exception.\n     */\n    public static <T extends Throwable> T readStackTrace(T throwable, StreamInput in) throws IOException {\n        throwable.setStackTrace(in.readArray(i -> {\n            final String declaringClasss = i.readString();\n            final String fileName = i.readOptionalString();\n            final String methodName = i.readString();\n            final int lineNumber = i.readVInt();\n            return new StackTraceElement(declaringClasss, methodName, fileName, lineNumber);\n        }, StackTraceElement[]::new));\n\n        int numSuppressed = in.readVInt();\n        for (int i = 0; i < numSuppressed; i++) {\n            throwable.addSuppressed(in.readException());\n        }\n        return throwable;\n    }\n\n    /**\n     * Serializes the given exceptions stacktrace elements as well as it's suppressed exceptions to the given output stream.\n     */\n    public static <T extends Throwable> T writeStackTraces(T throwable, StreamOutput out,\n                                                           Writer<Throwable> exceptionWriter) throws IOException {\n        out.writeArray((o, v) -> {\n            o.writeString(v.getClassName());\n            o.writeOptionalString(v.getFileName());\n            o.writeString(v.getMethodName());\n            o.writeVInt(v.getLineNumber());\n        }, throwable.getStackTrace());\n        out.writeArray(exceptionWriter, throwable.getSuppressed());\n        return throwable;\n    }\n\n    /**\n     * This is the list of Exceptions Elasticsearch can throw over the wire or save into a corruption marker. Each value in the enum is a\n     * single exception tying the Class to an id for use of the encode side and the id back to a constructor for use on the decode side. As\n     * such its ok if the exceptions to change names so long as their constructor can still read the exception. Each exception is listed\n     * in id order below. If you want to remove an exception leave a tombstone comment and mark the id as null in\n     * ExceptionSerializationTests.testIds.ids.\n     */\n    private enum ElasticsearchExceptionHandle {\n        INDEX_SHARD_SNAPSHOT_FAILED_EXCEPTION(org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException.class,\n                org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException::new, 0, UNKNOWN_VERSION_ADDED),\n        // 1 was DfsPhaseExecutionException\n        EXECUTION_CANCELLED_EXCEPTION(org.elasticsearch.common.util.CancellableThreads.ExecutionCancelledException.class,\n                org.elasticsearch.common.util.CancellableThreads.ExecutionCancelledException::new, 2, UNKNOWN_VERSION_ADDED),\n        MASTER_NOT_DISCOVERED_EXCEPTION(org.elasticsearch.discovery.MasterNotDiscoveredException.class,\n                org.elasticsearch.discovery.MasterNotDiscoveredException::new, 3, UNKNOWN_VERSION_ADDED),\n        ELASTICSEARCH_SECURITY_EXCEPTION(org.elasticsearch.ElasticsearchSecurityException.class,\n                org.elasticsearch.ElasticsearchSecurityException::new, 4, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_RESTORE_EXCEPTION(org.elasticsearch.index.snapshots.IndexShardRestoreException.class,\n                org.elasticsearch.index.snapshots.IndexShardRestoreException::new, 5, UNKNOWN_VERSION_ADDED),\n        INDEX_CLOSED_EXCEPTION(org.elasticsearch.indices.IndexClosedException.class,\n                org.elasticsearch.indices.IndexClosedException::new, 6, UNKNOWN_VERSION_ADDED),\n        BIND_HTTP_EXCEPTION(org.elasticsearch.http.BindHttpException.class,\n                org.elasticsearch.http.BindHttpException::new, 7, UNKNOWN_VERSION_ADDED),\n        // 8 was ReduceSearchPhaseException\n        NODE_CLOSED_EXCEPTION(org.elasticsearch.node.NodeClosedException.class,\n                org.elasticsearch.node.NodeClosedException::new, 9, UNKNOWN_VERSION_ADDED),\n        SNAPSHOT_FAILED_ENGINE_EXCEPTION(org.elasticsearch.index.engine.SnapshotFailedEngineException.class,\n                org.elasticsearch.index.engine.SnapshotFailedEngineException::new, 10, UNKNOWN_VERSION_ADDED),\n        SHARD_NOT_FOUND_EXCEPTION(org.elasticsearch.index.shard.ShardNotFoundException.class,\n                org.elasticsearch.index.shard.ShardNotFoundException::new, 11, UNKNOWN_VERSION_ADDED),\n        CONNECT_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ConnectTransportException.class,\n                org.elasticsearch.transport.ConnectTransportException::new, 12, UNKNOWN_VERSION_ADDED),\n        NOT_SERIALIZABLE_TRANSPORT_EXCEPTION(org.elasticsearch.transport.NotSerializableTransportException.class,\n                org.elasticsearch.transport.NotSerializableTransportException::new, 13, UNKNOWN_VERSION_ADDED),\n        RESPONSE_HANDLER_FAILURE_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ResponseHandlerFailureTransportException.class,\n                org.elasticsearch.transport.ResponseHandlerFailureTransportException::new, 14, UNKNOWN_VERSION_ADDED),\n        INDEX_CREATION_EXCEPTION(org.elasticsearch.indices.IndexCreationException.class,\n                org.elasticsearch.indices.IndexCreationException::new, 15, UNKNOWN_VERSION_ADDED),\n        INDEX_NOT_FOUND_EXCEPTION(org.elasticsearch.index.IndexNotFoundException.class,\n                org.elasticsearch.index.IndexNotFoundException::new, 16, UNKNOWN_VERSION_ADDED),\n        ILLEGAL_SHARD_ROUTING_STATE_EXCEPTION(org.elasticsearch.cluster.routing.IllegalShardRoutingStateException.class,\n                org.elasticsearch.cluster.routing.IllegalShardRoutingStateException::new, 17, UNKNOWN_VERSION_ADDED),\n        BROADCAST_SHARD_OPERATION_FAILED_EXCEPTION(org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException.class,\n                org.elasticsearch.action.support.broadcast.BroadcastShardOperationFailedException::new, 18, UNKNOWN_VERSION_ADDED),\n        RESOURCE_NOT_FOUND_EXCEPTION(org.elasticsearch.ResourceNotFoundException.class,\n                org.elasticsearch.ResourceNotFoundException::new, 19, UNKNOWN_VERSION_ADDED),\n        ACTION_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ActionTransportException.class,\n                org.elasticsearch.transport.ActionTransportException::new, 20, UNKNOWN_VERSION_ADDED),\n        ELASTICSEARCH_GENERATION_EXCEPTION(org.elasticsearch.ElasticsearchGenerationException.class,\n                org.elasticsearch.ElasticsearchGenerationException::new, 21, UNKNOWN_VERSION_ADDED),\n        //      22 was CreateFailedEngineException\n        INDEX_SHARD_STARTED_EXCEPTION(org.elasticsearch.index.shard.IndexShardStartedException.class,\n                org.elasticsearch.index.shard.IndexShardStartedException::new, 23, UNKNOWN_VERSION_ADDED),\n        // 24 was SearchContextMissingException\n        // 25 was GeneralScriptException\n        // 26 was BatchOperationException\n        // 27 was SnapshotCreationException\n        // 28 was DeleteFailedEngineException, deprecated in 6.0, removed in 7.0\n        DOCUMENT_MISSING_EXCEPTION(org.elasticsearch.index.engine.DocumentMissingException.class,\n                org.elasticsearch.index.engine.DocumentMissingException::new, 29, UNKNOWN_VERSION_ADDED),\n        SNAPSHOT_EXCEPTION(org.elasticsearch.snapshots.SnapshotException.class,\n                org.elasticsearch.snapshots.SnapshotException::new, 30, UNKNOWN_VERSION_ADDED),\n        INVALID_ALIAS_NAME_EXCEPTION(org.elasticsearch.indices.InvalidAliasNameException.class,\n                org.elasticsearch.indices.InvalidAliasNameException::new, 31, UNKNOWN_VERSION_ADDED),\n        INVALID_INDEX_NAME_EXCEPTION(org.elasticsearch.indices.InvalidIndexNameException.class,\n                org.elasticsearch.indices.InvalidIndexNameException::new, 32, UNKNOWN_VERSION_ADDED),\n        INDEX_PRIMARY_SHARD_NOT_ALLOCATED_EXCEPTION(org.elasticsearch.indices.IndexPrimaryShardNotAllocatedException.class,\n                org.elasticsearch.indices.IndexPrimaryShardNotAllocatedException::new, 33, UNKNOWN_VERSION_ADDED),\n        TRANSPORT_EXCEPTION(org.elasticsearch.transport.TransportException.class,\n                org.elasticsearch.transport.TransportException::new, 34, UNKNOWN_VERSION_ADDED),\n        ELASTICSEARCH_PARSE_EXCEPTION(org.elasticsearch.ElasticsearchParseException.class,\n                org.elasticsearch.ElasticsearchParseException::new, 35, UNKNOWN_VERSION_ADDED),\n        // 36 was SearchException\n        MAPPER_EXCEPTION(org.elasticsearch.index.mapper.MapperException.class,\n                org.elasticsearch.index.mapper.MapperException::new, 37, UNKNOWN_VERSION_ADDED),\n        INVALID_TYPE_NAME_EXCEPTION(org.elasticsearch.indices.InvalidTypeNameException.class,\n                org.elasticsearch.indices.InvalidTypeNameException::new, 38, UNKNOWN_VERSION_ADDED),\n        SNAPSHOT_RESTORE_EXCEPTION(org.elasticsearch.snapshots.SnapshotRestoreException.class,\n                org.elasticsearch.snapshots.SnapshotRestoreException::new, 39, UNKNOWN_VERSION_ADDED),\n        PARSING_EXCEPTION(org.elasticsearch.common.ParsingException.class, org.elasticsearch.common.ParsingException::new, 40,\n            UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_CLOSED_EXCEPTION(org.elasticsearch.index.shard.IndexShardClosedException.class,\n                org.elasticsearch.index.shard.IndexShardClosedException::new, 41, UNKNOWN_VERSION_ADDED),\n        RECOVER_FILES_RECOVERY_EXCEPTION(org.elasticsearch.indices.recovery.RecoverFilesRecoveryException.class,\n                org.elasticsearch.indices.recovery.RecoverFilesRecoveryException::new, 42, UNKNOWN_VERSION_ADDED),\n        TRUNCATED_TRANSLOG_EXCEPTION(org.elasticsearch.index.translog.TruncatedTranslogException.class,\n                org.elasticsearch.index.translog.TruncatedTranslogException::new, 43, UNKNOWN_VERSION_ADDED),\n        RECOVERY_FAILED_EXCEPTION(org.elasticsearch.indices.recovery.RecoveryFailedException.class,\n                org.elasticsearch.indices.recovery.RecoveryFailedException::new, 44, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_RELOCATED_EXCEPTION(org.elasticsearch.index.shard.IndexShardRelocatedException.class,\n                org.elasticsearch.index.shard.IndexShardRelocatedException::new, 45, UNKNOWN_VERSION_ADDED),\n        NODE_SHOULD_NOT_CONNECT_EXCEPTION(org.elasticsearch.transport.NodeShouldNotConnectException.class,\n                org.elasticsearch.transport.NodeShouldNotConnectException::new, 46, UNKNOWN_VERSION_ADDED),\n        // 47 used to be for IndexTemplateAlreadyExistsException which was deprecated in 5.1 removed in 6.0\n        TRANSLOG_CORRUPTED_EXCEPTION(org.elasticsearch.index.translog.TranslogCorruptedException.class,\n                org.elasticsearch.index.translog.TranslogCorruptedException::new, 48, UNKNOWN_VERSION_ADDED),\n        CLUSTER_BLOCK_EXCEPTION(org.elasticsearch.cluster.block.ClusterBlockException.class,\n                org.elasticsearch.cluster.block.ClusterBlockException::new, 49, UNKNOWN_VERSION_ADDED),\n        // 50 was FetchPhaseExecutionException\n        // 51 used to be for IndexShardAlreadyExistsException which was deprecated in 5.1 removed in 6.0\n        VERSION_CONFLICT_ENGINE_EXCEPTION(org.elasticsearch.index.engine.VersionConflictEngineException.class,\n                org.elasticsearch.index.engine.VersionConflictEngineException::new, 52, UNKNOWN_VERSION_ADDED),\n        ENGINE_EXCEPTION(org.elasticsearch.index.engine.EngineException.class, org.elasticsearch.index.engine.EngineException::new, 53,\n            UNKNOWN_VERSION_ADDED),\n        // 54 was DocumentAlreadyExistsException, which is superseded by VersionConflictEngineException\n        NO_SUCH_NODE_EXCEPTION(org.elasticsearch.action.NoSuchNodeException.class, org.elasticsearch.action.NoSuchNodeException::new, 55,\n            UNKNOWN_VERSION_ADDED),\n        SETTINGS_EXCEPTION(org.elasticsearch.common.settings.SettingsException.class,\n                org.elasticsearch.common.settings.SettingsException::new, 56, UNKNOWN_VERSION_ADDED),\n        INDEX_TEMPLATE_MISSING_EXCEPTION(org.elasticsearch.indices.IndexTemplateMissingException.class,\n                org.elasticsearch.indices.IndexTemplateMissingException::new, 57, UNKNOWN_VERSION_ADDED),\n        SEND_REQUEST_TRANSPORT_EXCEPTION(org.elasticsearch.transport.SendRequestTransportException.class,\n                org.elasticsearch.transport.SendRequestTransportException::new, 58, UNKNOWN_VERSION_ADDED),\n        // 59 used to be EsRejectedExecutionException\n        // 60 used to be for EarlyTerminationException\n        // 61 used to be for RoutingValidationException\n        NOT_SERIALIZABLE_EXCEPTION_WRAPPER(org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper.class,\n                org.elasticsearch.common.io.stream.NotSerializableExceptionWrapper::new, 62, UNKNOWN_VERSION_ADDED),\n        // 63 was AliasFilterParsingException\n        // 64 was DeleteByQueryFailedEngineException, which was removed in 5.0\n        GATEWAY_EXCEPTION(org.elasticsearch.gateway.GatewayException.class, org.elasticsearch.gateway.GatewayException::new, 65,\n            UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_NOT_RECOVERING_EXCEPTION(org.elasticsearch.index.shard.IndexShardNotRecoveringException.class,\n                org.elasticsearch.index.shard.IndexShardNotRecoveringException::new, 66, UNKNOWN_VERSION_ADDED),\n        HTTP_EXCEPTION(org.elasticsearch.http.HttpException.class, org.elasticsearch.http.HttpException::new, 67, UNKNOWN_VERSION_ADDED),\n        ELASTICSEARCH_EXCEPTION(org.elasticsearch.ElasticsearchException.class,\n                org.elasticsearch.ElasticsearchException::new, 68, UNKNOWN_VERSION_ADDED),\n        SNAPSHOT_MISSING_EXCEPTION(org.elasticsearch.snapshots.SnapshotMissingException.class,\n                org.elasticsearch.snapshots.SnapshotMissingException::new, 69, UNKNOWN_VERSION_ADDED),\n        PRIMARY_MISSING_ACTION_EXCEPTION(org.elasticsearch.action.PrimaryMissingActionException.class,\n                org.elasticsearch.action.PrimaryMissingActionException::new, 70, UNKNOWN_VERSION_ADDED),\n        FAILED_NODE_EXCEPTION(org.elasticsearch.action.FailedNodeException.class, org.elasticsearch.action.FailedNodeException::new, 71,\n            UNKNOWN_VERSION_ADDED),\n        // 72 was SearchParseException\n        CONCURRENT_SNAPSHOT_EXECUTION_EXCEPTION(org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException.class,\n                org.elasticsearch.snapshots.ConcurrentSnapshotExecutionException::new, 73, UNKNOWN_VERSION_ADDED),\n        BLOB_STORE_EXCEPTION(org.elasticsearch.common.blobstore.BlobStoreException.class,\n                org.elasticsearch.common.blobstore.BlobStoreException::new, 74, UNKNOWN_VERSION_ADDED),\n        INCOMPATIBLE_CLUSTER_STATE_VERSION_EXCEPTION(org.elasticsearch.cluster.IncompatibleClusterStateVersionException.class,\n                org.elasticsearch.cluster.IncompatibleClusterStateVersionException::new, 75, UNKNOWN_VERSION_ADDED),\n        RECOVERY_ENGINE_EXCEPTION(org.elasticsearch.index.engine.RecoveryEngineException.class,\n                org.elasticsearch.index.engine.RecoveryEngineException::new, 76, UNKNOWN_VERSION_ADDED),\n        UNCATEGORIZED_EXECUTION_EXCEPTION(org.elasticsearch.common.util.concurrent.UncategorizedExecutionException.class,\n                org.elasticsearch.common.util.concurrent.UncategorizedExecutionException::new, 77, UNKNOWN_VERSION_ADDED),\n        // 78 was TimestampParsingException\n        // 79 was RoutingMissingException\n        // 80 was IndexFailedEngineException, deprecated in 6.0, removed in 7.0\n        INDEX_SHARD_RESTORE_FAILED_EXCEPTION(org.elasticsearch.index.snapshots.IndexShardRestoreFailedException.class,\n                org.elasticsearch.index.snapshots.IndexShardRestoreFailedException::new, 81, UNKNOWN_VERSION_ADDED),\n        REPOSITORY_EXCEPTION(org.elasticsearch.repositories.RepositoryException.class,\n                org.elasticsearch.repositories.RepositoryException::new, 82, UNKNOWN_VERSION_ADDED),\n        RECEIVE_TIMEOUT_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ReceiveTimeoutTransportException.class,\n                org.elasticsearch.transport.ReceiveTimeoutTransportException::new, 83, UNKNOWN_VERSION_ADDED),\n        NODE_DISCONNECTED_EXCEPTION(org.elasticsearch.transport.NodeDisconnectedException.class,\n                org.elasticsearch.transport.NodeDisconnectedException::new, 84, UNKNOWN_VERSION_ADDED),\n        // 85 used to be for AlreadyExpiredException\n        // 86 used to be for AggregationExecutionException\n        // 87 used to be for MergeMappingException\n        INVALID_INDEX_TEMPLATE_EXCEPTION(org.elasticsearch.indices.InvalidIndexTemplateException.class,\n                org.elasticsearch.indices.InvalidIndexTemplateException::new, 88, UNKNOWN_VERSION_ADDED),\n        REFRESH_FAILED_ENGINE_EXCEPTION(org.elasticsearch.index.engine.RefreshFailedEngineException.class,\n                org.elasticsearch.index.engine.RefreshFailedEngineException::new, 90, UNKNOWN_VERSION_ADDED),\n        // 91 used to be for AggregationInitializationException\n        DELAY_RECOVERY_EXCEPTION(org.elasticsearch.indices.recovery.DelayRecoveryException.class,\n                org.elasticsearch.indices.recovery.DelayRecoveryException::new, 92, UNKNOWN_VERSION_ADDED),\n        // 93 used to be for IndexWarmerMissingException\n        NO_NODE_AVAILABLE_EXCEPTION(org.elasticsearch.client.transport.NoNodeAvailableException.class,\n                org.elasticsearch.client.transport.NoNodeAvailableException::new, 94, UNKNOWN_VERSION_ADDED),\n        INVALID_SNAPSHOT_NAME_EXCEPTION(org.elasticsearch.snapshots.InvalidSnapshotNameException.class,\n                org.elasticsearch.snapshots.InvalidSnapshotNameException::new, 96, UNKNOWN_VERSION_ADDED),\n        ILLEGAL_INDEX_SHARD_STATE_EXCEPTION(org.elasticsearch.index.shard.IllegalIndexShardStateException.class,\n                org.elasticsearch.index.shard.IllegalIndexShardStateException::new, 97, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_SNAPSHOT_EXCEPTION(org.elasticsearch.index.snapshots.IndexShardSnapshotException.class,\n                org.elasticsearch.index.snapshots.IndexShardSnapshotException::new, 98, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_NOT_STARTED_EXCEPTION(org.elasticsearch.index.shard.IndexShardNotStartedException.class,\n                org.elasticsearch.index.shard.IndexShardNotStartedException::new, 99, UNKNOWN_VERSION_ADDED),\n        // 100 used to bew for SearchPhaseExecutionException\n        ACTION_NOT_FOUND_TRANSPORT_EXCEPTION(org.elasticsearch.transport.ActionNotFoundTransportException.class,\n                org.elasticsearch.transport.ActionNotFoundTransportException::new, 101, UNKNOWN_VERSION_ADDED),\n        TRANSPORT_SERIALIZATION_EXCEPTION(org.elasticsearch.transport.TransportSerializationException.class,\n                org.elasticsearch.transport.TransportSerializationException::new, 102, UNKNOWN_VERSION_ADDED),\n        REMOTE_TRANSPORT_EXCEPTION(org.elasticsearch.transport.RemoteTransportException.class,\n                org.elasticsearch.transport.RemoteTransportException::new, 103, UNKNOWN_VERSION_ADDED),\n        ENGINE_CREATION_FAILURE_EXCEPTION(org.elasticsearch.index.engine.EngineCreationFailureException.class,\n                org.elasticsearch.index.engine.EngineCreationFailureException::new, 104, UNKNOWN_VERSION_ADDED),\n        ROUTING_EXCEPTION(org.elasticsearch.cluster.routing.RoutingException.class,\n                org.elasticsearch.cluster.routing.RoutingException::new, 105, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_RECOVERY_EXCEPTION(org.elasticsearch.index.shard.IndexShardRecoveryException.class,\n                org.elasticsearch.index.shard.IndexShardRecoveryException::new, 106, UNKNOWN_VERSION_ADDED),\n        REPOSITORY_MISSING_EXCEPTION(org.elasticsearch.repositories.RepositoryMissingException.class,\n                org.elasticsearch.repositories.RepositoryMissingException::new, 107, UNKNOWN_VERSION_ADDED),\n        DOCUMENT_SOURCE_MISSING_EXCEPTION(org.elasticsearch.index.engine.DocumentSourceMissingException.class,\n                org.elasticsearch.index.engine.DocumentSourceMissingException::new, 109, UNKNOWN_VERSION_ADDED),\n        // 110 used to be FlushNotAllowedEngineException\n        NO_CLASS_SETTINGS_EXCEPTION(org.elasticsearch.common.settings.NoClassSettingsException.class,\n                org.elasticsearch.common.settings.NoClassSettingsException::new, 111, UNKNOWN_VERSION_ADDED),\n        BIND_TRANSPORT_EXCEPTION(org.elasticsearch.transport.BindTransportException.class,\n                org.elasticsearch.transport.BindTransportException::new, 112, UNKNOWN_VERSION_ADDED),\n        ALIASES_NOT_FOUND_EXCEPTION(AliasesNotFoundException.class,\n                AliasesNotFoundException::new, 113, UNKNOWN_VERSION_ADDED),\n        INDEX_SHARD_RECOVERING_EXCEPTION(org.elasticsearch.index.shard.IndexShardRecoveringException.class,\n                org.elasticsearch.index.shard.IndexShardRecoveringException::new, 114, UNKNOWN_VERSION_ADDED),\n        TRANSLOG_EXCEPTION(org.elasticsearch.index.translog.TranslogException.class,\n                org.elasticsearch.index.translog.TranslogException::new, 115, UNKNOWN_VERSION_ADDED),\n        PROCESS_CLUSTER_EVENT_TIMEOUT_EXCEPTION(org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException.class,\n                org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException::new, 116, UNKNOWN_VERSION_ADDED),\n        RETRY_ON_PRIMARY_EXCEPTION(ReplicationOperation.RetryOnPrimaryException.class,\n                ReplicationOperation.RetryOnPrimaryException::new, 117, UNKNOWN_VERSION_ADDED),\n        ELASTICSEARCH_TIMEOUT_EXCEPTION(org.elasticsearch.ElasticsearchTimeoutException.class,\n                org.elasticsearch.ElasticsearchTimeoutException::new, 118, UNKNOWN_VERSION_ADDED),\n        // 119 was QueryPhaseExecutionException\n        REPOSITORY_VERIFICATION_EXCEPTION(org.elasticsearch.repositories.RepositoryVerificationException.class,\n                org.elasticsearch.repositories.RepositoryVerificationException::new, 120, UNKNOWN_VERSION_ADDED),\n        // 121 was InvalidAggregationPathException\n        // 123 used to be IndexAlreadyExistsException and was renamed\n        RESOURCE_ALREADY_EXISTS_EXCEPTION(ResourceAlreadyExistsException.class,\n            ResourceAlreadyExistsException::new, 123, UNKNOWN_VERSION_ADDED),\n        // 124 used to be Script.ScriptParseException\n        HTTP_REQUEST_ON_TRANSPORT_EXCEPTION(TcpTransport.HttpRequestOnTransportException.class,\n                TcpTransport.HttpRequestOnTransportException::new, 125, UNKNOWN_VERSION_ADDED),\n        MAPPER_PARSING_EXCEPTION(org.elasticsearch.index.mapper.MapperParsingException.class,\n                org.elasticsearch.index.mapper.MapperParsingException::new, 126, UNKNOWN_VERSION_ADDED),\n        // 127 was SearchContextException\n        // 128 was SearchSourceBuilderException\n        // 129 was EngineClosedException\n        NO_SHARD_AVAILABLE_ACTION_EXCEPTION(org.elasticsearch.action.NoShardAvailableActionException.class,\n                org.elasticsearch.action.NoShardAvailableActionException::new, 130, UNKNOWN_VERSION_ADDED),\n        UNAVAILABLE_SHARDS_EXCEPTION(org.elasticsearch.action.UnavailableShardsException.class,\n                org.elasticsearch.action.UnavailableShardsException::new, 131, UNKNOWN_VERSION_ADDED),\n        FLUSH_FAILED_ENGINE_EXCEPTION(org.elasticsearch.index.engine.FlushFailedEngineException.class,\n                org.elasticsearch.index.engine.FlushFailedEngineException::new, 132, UNKNOWN_VERSION_ADDED),\n        CIRCUIT_BREAKING_EXCEPTION(org.elasticsearch.common.breaker.CircuitBreakingException.class,\n                org.elasticsearch.common.breaker.CircuitBreakingException::new, 133, UNKNOWN_VERSION_ADDED),\n        NODE_NOT_CONNECTED_EXCEPTION(org.elasticsearch.transport.NodeNotConnectedException.class,\n                org.elasticsearch.transport.NodeNotConnectedException::new, 134, UNKNOWN_VERSION_ADDED),\n        STRICT_DYNAMIC_MAPPING_EXCEPTION(org.elasticsearch.index.mapper.StrictDynamicMappingException.class,\n                org.elasticsearch.index.mapper.StrictDynamicMappingException::new, 135, UNKNOWN_VERSION_ADDED),\n        RETRY_ON_REPLICA_EXCEPTION(org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnReplicaException.class,\n                org.elasticsearch.action.support.replication.TransportReplicationAction.RetryOnReplicaException::new, 136,\n            UNKNOWN_VERSION_ADDED),\n        TYPE_MISSING_EXCEPTION(org.elasticsearch.indices.TypeMissingException.class,\n                org.elasticsearch.indices.TypeMissingException::new, 137, UNKNOWN_VERSION_ADDED),\n        FAILED_TO_COMMIT_CLUSTER_STATE_EXCEPTION(org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException.class,\n                org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException::new, 140, UNKNOWN_VERSION_ADDED),\n        QUERY_SHARD_EXCEPTION(org.elasticsearch.index.query.QueryShardException.class,\n                org.elasticsearch.index.query.QueryShardException::new, 141, UNKNOWN_VERSION_ADDED),\n        NO_LONGER_PRIMARY_SHARD_EXCEPTION(ShardStateAction.NoLongerPrimaryShardException.class,\n                ShardStateAction.NoLongerPrimaryShardException::new, 142, UNKNOWN_VERSION_ADDED),\n        // 143 was ScriptException\n        NOT_MASTER_EXCEPTION(org.elasticsearch.cluster.NotMasterException.class, org.elasticsearch.cluster.NotMasterException::new, 144,\n            UNKNOWN_VERSION_ADDED),\n        STATUS_EXCEPTION(org.elasticsearch.ElasticsearchStatusException.class, org.elasticsearch.ElasticsearchStatusException::new, 145,\n            UNKNOWN_VERSION_ADDED),\n        TASK_CANCELLED_EXCEPTION(org.elasticsearch.tasks.TaskCancelledException.class,\n            org.elasticsearch.tasks.TaskCancelledException::new, 146, UNKNOWN_VERSION_ADDED),\n        SHARD_LOCK_OBTAIN_FAILED_EXCEPTION(org.elasticsearch.env.ShardLockObtainFailedException.class,\n                                           org.elasticsearch.env.ShardLockObtainFailedException::new, 147, UNKNOWN_VERSION_ADDED),\n        // 148 was UnknownNamedObjectException\n        // 149 was TooManyBucketsException\n        COORDINATION_STATE_REJECTED_EXCEPTION(org.elasticsearch.cluster.coordination.CoordinationStateRejectedException.class,\n            org.elasticsearch.cluster.coordination.CoordinationStateRejectedException::new, 150, Version.V_4_0_0),\n\n        RETENTION_LEASE_ALREADY_EXISTS_EXCEPTION(\n                org.elasticsearch.index.seqno.RetentionLeaseAlreadyExistsException.class,\n                org.elasticsearch.index.seqno.RetentionLeaseAlreadyExistsException::new,\n                153,\n                Version.V_4_3_0),\n        RETENTION_LEASE_NOT_FOUND_EXCEPTION(\n                org.elasticsearch.index.seqno.RetentionLeaseNotFoundException.class,\n                org.elasticsearch.index.seqno.RetentionLeaseNotFoundException::new,\n                154,\n                Version.V_4_3_0),\n        SHARD_NOT_IN_PRIMARY_MODE_EXCEPTION(\n                org.elasticsearch.index.shard.ShardNotInPrimaryModeException.class,\n                org.elasticsearch.index.shard.ShardNotInPrimaryModeException::new,\n                155,\n                Version.V_4_3_0),\n        JOB_KILLED_EXCEPTION(\n            io.crate.exceptions.JobKilledException.class,\n            io.crate.exceptions.JobKilledException::new,\n                156,\n            Version.V_4_3_0),\n        TASK_MISSING_EXCEPTION(\n            io.crate.exceptions.TaskMissing.class,\n            io.crate.exceptions.TaskMissing::new,\n                157,\n            Version.V_4_3_0),\n        RETENTION_LEASE_INVALID_RETAINING_SEQUENCE_NUMBER_EXCEPTION(\n            org.elasticsearch.index.seqno.RetentionLeaseInvalidRetainingSeqNoException.class,\n            org.elasticsearch.index.seqno.RetentionLeaseInvalidRetainingSeqNoException::new,\n            158,\n            Version.V_4_3_0),\n        RELATION_ALREADY_EXISTS(\n            io.crate.exceptions.RelationAlreadyExists.class,\n            io.crate.exceptions.RelationAlreadyExists::new,\n            159,\n            Version.V_4_7_0),\n        RELATION_UNKNOWN(\n            io.crate.exceptions.RelationUnknown.class,\n            io.crate.exceptions.RelationUnknown::new,\n            160,\n            Version.V_4_7_0),\n        SCHEMA_UNKNOWN(\n            io.crate.exceptions.SchemaUnknownException.class,\n            io.crate.exceptions.SchemaUnknownException::new,\n            161,\n            Version.V_4_7_0),\n        GROUP_BY_ON_ARRAY_UNSUPPORTED_EXCEPTION(\n            ArrayViaDocValuesUnsupportedException.class,\n            ArrayViaDocValuesUnsupportedException::new,\n            162,\n            Version.V_4_7_0),\n        INVALID_ARGUMENT_EXCEPTION(\n            io.crate.exceptions.InvalidArgumentException.class,\n            io.crate.exceptions.InvalidArgumentException::new,\n            163,\n            Version.V_4_7_0),\n        INVALID_COLUMN_NAME_EXCEPTION(\n            io.crate.exceptions.InvalidColumnNameException.class,\n            io.crate.exceptions.InvalidColumnNameException::new,\n            164,\n            Version.V_4_7_0),\n        UNSUPPORTED_FEATURE_EXCEPTION(\n            io.crate.exceptions.UnsupportedFeatureException.class,\n            io.crate.exceptions.UnsupportedFeatureException::new,\n            165,\n            Version.V_4_7_0),\n        USER_DEFINED_FUNCTION_ALREADY_EXISTS_EXCEPTION(\n            io.crate.exceptions.UserDefinedFunctionAlreadyExistsException.class,\n            io.crate.exceptions.UserDefinedFunctionAlreadyExistsException::new,\n            166,\n            Version.V_4_7_0),\n        USER_DEFINED_FUNCTION_UNKNOWN_EXCEPTION(\n            io.crate.exceptions.UserDefinedFunctionUnknownException.class,\n            io.crate.exceptions.UserDefinedFunctionUnknownException::new,\n            167,\n            Version.V_4_7_0),\n        VERSIONING_VALIDATION_EXCEPTION(\n            io.crate.exceptions.VersioningValidationException.class,\n            io.crate.exceptions.VersioningValidationException::new,\n            168,\n            Version.V_4_7_0),\n        PUBLICATION_ALREADY_EXISTS_EXCEPTION(\n            io.crate.replication.logical.exceptions.PublicationAlreadyExistsException.class,\n            io.crate.replication.logical.exceptions.PublicationAlreadyExistsException::new,\n            169,\n            Version.V_4_7_0),\n        PUBLICATION_UNKNOWN_EXCEPTION(\n            io.crate.replication.logical.exceptions.PublicationUnknownException.class,\n            io.crate.replication.logical.exceptions.PublicationUnknownException::new,\n            170,\n            Version.V_4_7_0),\n        SUBSCRIPTION_ALREADY_EXISTS_EXCEPTION(\n            io.crate.replication.logical.exceptions.SubscriptionAlreadyExistsException.class,\n            io.crate.replication.logical.exceptions.SubscriptionAlreadyExistsException::new,\n            171,\n            Version.V_4_7_0),\n        SUBSCRIPTION_UNKNOWN_EXCEPTION(\n            io.crate.replication.logical.exceptions.SubscriptionUnknownException.class,\n            io.crate.replication.logical.exceptions.SubscriptionUnknownException::new,\n            172,\n            Version.V_4_7_0),\n        MISSING_SHARD_OPERATIONS_EXCEPTION(\n            io.crate.replication.logical.exceptions.MissingShardOperationsException.class,\n            io.crate.replication.logical.exceptions.MissingShardOperationsException::new,\n            173,\n            Version.V_4_7_0),\n        PEER_RECOVERY_NOT_FOUND_EXCEPTION(\n            org.elasticsearch.indices.recovery.PeerRecoveryNotFound.class,\n            org.elasticsearch.indices.recovery.PeerRecoveryNotFound::new,\n            174,\n            Version.V_5_1_0),\n        NODE_HEALTH_CHECK_FAILURE_EXCEPTION(\n            org.elasticsearch.cluster.coordination.NodeHealthCheckFailureException.class,\n            org.elasticsearch.cluster.coordination.NodeHealthCheckFailureException::new,\n            175,\n            Version.V_5_2_0),\n        OPERATION_ON_INACCESSIBLE_RELATION_EXCEPTION(\n            io.crate.exceptions.OperationOnInaccessibleRelationException.class,\n            io.crate.exceptions.OperationOnInaccessibleRelationException::new,\n            176,\n            Version.V_5_6_0),\n        UNAUTHORIZED_EXCEPTION(\n            io.crate.exceptions.UnauthorizedException.class,\n            io.crate.exceptions.UnauthorizedException::new,\n            177,\n            Version.V_5_7_0);\n\n        final Class<? extends ElasticsearchException> exceptionClass;\n        final CheckedFunction<StreamInput, ? extends ElasticsearchException, IOException> constructor;\n        final int id;\n        final Version versionAdded;\n\n        <E extends ElasticsearchException> ElasticsearchExceptionHandle(Class<E> exceptionClass,\n                                                                        CheckedFunction<StreamInput, E, IOException> constructor, int id,\n                                                                        Version versionAdded) {\n            // We need the exceptionClass because you can't dig it out of the constructor reliably.\n            this.exceptionClass = exceptionClass;\n            this.constructor = constructor;\n            this.versionAdded = versionAdded;\n            this.id = id;\n        }\n    }\n\n    static {\n        ID_TO_SUPPLIER = unmodifiableMap(Arrays\n                .stream(ElasticsearchExceptionHandle.values()).collect(Collectors.toMap(e -> e.id, e -> e.constructor)));\n        CLASS_TO_ELASTICSEARCH_EXCEPTION_HANDLE = unmodifiableMap(Arrays\n                .stream(ElasticsearchExceptionHandle.values()).collect(Collectors.toMap(e -> e.exceptionClass, e -> e)));\n    }\n\n    public Index getIndex() {\n        List<String> index = getMetadata(INDEX_METADATA_KEY);\n        if (index != null && index.isEmpty() == false) {\n            List<String> index_uuid = getMetadata(INDEX_METADATA_KEY_UUID);\n            return new Index(index.get(0), index_uuid.get(0));\n        }\n\n        return null;\n    }\n\n    public ShardId getShardId() {\n        List<String> shard = getMetadata(SHARD_METADATA_KEY);\n        if (shard != null && shard.isEmpty() == false) {\n            return new ShardId(getIndex(), Integer.parseInt(shard.get(0)));\n        }\n        return null;\n    }\n\n    public void setIndex(Index index) {\n        if (index != null) {\n            addMetadata(INDEX_METADATA_KEY, index.getName());\n            addMetadata(INDEX_METADATA_KEY_UUID, index.getUUID());\n        }\n    }\n\n    public void setIndex(String index) {\n        if (index != null) {\n            setIndex(new Index(index, INDEX_UUID_NA_VALUE));\n        }\n    }\n\n    public void setShard(ShardId shardId) {\n        if (shardId != null) {\n            setIndex(shardId.getIndex());\n            addMetadata(SHARD_METADATA_KEY, Integer.toString(shardId.id()));\n        }\n    }\n\n    public void setResources(String type, String... id) {\n        assert type != null;\n        addMetadata(RESOURCE_METADATA_ID_KEY, id);\n        addMetadata(RESOURCE_METADATA_TYPE_KEY, type);\n    }\n\n    // lower cases and adds underscores to transitions in a name\n    private static String toUnderscoreCase(String value) {\n        StringBuilder sb = new StringBuilder();\n        boolean changed = false;\n        for (int i = 0; i < value.length(); i++) {\n            char c = value.charAt(i);\n            if (Character.isUpperCase(c)) {\n                if (!changed) {\n                    // copy it over here\n                    for (int j = 0; j < i; j++) {\n                        sb.append(value.charAt(j));\n                    }\n                    changed = true;\n                    if (i == 0) {\n                        sb.append(Character.toLowerCase(c));\n                    } else {\n                        sb.append('_');\n                        sb.append(Character.toLowerCase(c));\n                    }\n                } else {\n                    sb.append('_');\n                    sb.append(Character.toLowerCase(c));\n                }\n            } else {\n                if (changed) {\n                    sb.append(c);\n                }\n            }\n        }\n        if (!changed) {\n            return value;\n        }\n        return sb.toString();\n    }\n\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect;\n\nimport static io.crate.testing.TestingHelpers.createNodeContext;\nimport static io.crate.testing.TestingHelpers.createReference;\nimport static io.crate.testing.TestingHelpers.isRow;\nimport static org.hamcrest.Matchers.contains;\nimport static org.junit.Assert.assertThat;\nimport static org.mockito.Mockito.mock;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.OutputStreamWriter;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Paths;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.UUID;\nimport java.util.concurrent.TimeUnit;\n\nimport org.elasticsearch.common.settings.Settings;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.TemporaryFolder;\n\nimport io.crate.analyze.CopyFromParserProperties;\nimport io.crate.data.BatchIterator;\nimport io.crate.data.CollectionBucket;\nimport io.crate.data.Row;\nimport io.crate.data.testing.TestingRowConsumer;\nimport io.crate.execution.dsl.phases.FileUriCollectPhase;\nimport io.crate.execution.engine.collect.sources.FileCollectSource;\nimport io.crate.expression.symbol.Literal;\nimport io.crate.metadata.ColumnIdent;\nimport io.crate.metadata.CoordinatorTxnCtx;\nimport io.crate.role.Role;\nimport io.crate.test.integration.CrateDummyClusterServiceUnitTest;\nimport io.crate.types.DataTypes;\n\n\npublic class MapSideDataCollectOperationTest extends CrateDummyClusterServiceUnitTest {\n\n    @Rule\n    public TemporaryFolder temporaryFolder = new TemporaryFolder();\n\n    @Test\n    public void testFileUriCollect() throws Exception {\n        FileCollectSource fileCollectSource = new FileCollectSource(\n            createNodeContext(),\n            clusterService,\n            Collections.emptyMap(),\n            THREAD_POOL,\n            () -> List.of(Role.CRATE_USER)\n            );\n\n        File tmpFile = temporaryFolder.newFile(\"fileUriCollectOperation.json\");\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(tmpFile), StandardCharsets.UTF_8)) {\n            writer.write(\"{\\\"name\\\": \\\"Arthur\\\", \\\"id\\\": 4, \\\"details\\\": {\\\"age\\\": 38}}\\n\");\n            writer.write(\"{\\\"id\\\": 5, \\\"name\\\": \\\"Trillian\\\", \\\"details\\\": {\\\"age\\\": 33}}\\n\");\n        }\n\n        FileUriCollectPhase collectNode = new FileUriCollectPhase(\n            UUID.randomUUID(),\n            0,\n            \"test\",\n            Collections.singletonList(\"noop_id\"),\n            Literal.of(Paths.get(tmpFile.toURI()).toUri().toString()),\n            List.of(\"a\", \"b\"),\n            Arrays.asList(\n                createReference(\"name\", DataTypes.STRING),\n                createReference(new ColumnIdent(\"details\", \"age\"), DataTypes.INTEGER)\n            ),\n            Collections.emptyList(),\n            null,\n            false,\n            CopyFromParserProperties.DEFAULT,\n            FileUriCollectPhase.InputFormat.JSON,\n            Settings.EMPTY\n        );\n        TestingRowConsumer consumer = new TestingRowConsumer();\n        CollectTask collectTask = mock(CollectTask.class);\n        BatchIterator<Row> iterator = fileCollectSource.getIterator(\n            CoordinatorTxnCtx.systemTransactionContext(), collectNode, collectTask, false).get(5, TimeUnit.SECONDS);\n        consumer.accept(iterator, null);\n        assertThat(new CollectionBucket(consumer.getResult()), contains(\n            isRow(\"Arthur\", 38),\n            isRow(\"Trillian\", 33)\n        ));\n    }\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect.files;\n\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\n\nimport java.io.BufferedWriter;\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.OutputStreamWriter;\nimport java.net.URI;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.TimeUnit;\nimport java.util.zip.GZIPOutputStream;\n\nimport org.elasticsearch.common.settings.Settings;\nimport org.elasticsearch.test.ESTestCase;\nimport org.elasticsearch.threadpool.TestThreadPool;\nimport org.elasticsearch.threadpool.ThreadPool;\nimport org.junit.AfterClass;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\n\nimport io.crate.execution.engine.collect.files.FileReadingIterator.LineCursor;\n\npublic class FileReadingCollectorTest extends ESTestCase {\n    private static ThreadPool THREAD_POOL;\n    private static File tmpFile;\n    private static File tmpFileGz;\n    private static File tmpFileEmptyLine;\n\n    private static String line1 = \"{\\\"name\\\": \\\"Arthur\\\", \\\"id\\\": 4, \\\"details\\\": {\\\"age\\\": 38}}\";\n    private static String line2 = \"{\\\"id\\\": 5, \\\"name\\\": \\\"Trillian\\\", \\\"details\\\": {\\\"age\\\": 33}}\";\n\n    private static LineCursor[] expectedResult(File file) throws Exception {\n        return new LineCursor[] {\n            new LineCursor(fileToURI(file), 1, line1, null),\n            new LineCursor(fileToURI(file), 2, line2, null)\n        };\n    }\n\n    @BeforeClass\n    public static void setUpClass() throws Exception {\n        Path copy_from = Files.createTempDirectory(\"copy_from\");\n        Path copy_from_gz = Files.createTempDirectory(\"copy_from_gz\");\n        Path copy_from_empty = Files.createTempDirectory(\"copy_from_empty\");\n        tmpFileGz = File.createTempFile(\"fileReadingCollector\", \".json.gz\", copy_from_gz.toFile());\n        tmpFile = File.createTempFile(\"fileReadingCollector\", \".json\", copy_from.toFile());\n        tmpFileEmptyLine = File.createTempFile(\"emptyLine\", \".json\", copy_from_empty.toFile());\n        try (BufferedWriter writer =\n                 new BufferedWriter(new OutputStreamWriter(new GZIPOutputStream(new FileOutputStream(tmpFileGz)),\n                     StandardCharsets.UTF_8))) {\n            writer.write(line1);\n            writer.write(\"\\n\");\n            writer.write(line2);\n            writer.write(\"\\n\");\n        }\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(tmpFile), StandardCharsets.UTF_8)) {\n            writer.write(line1);\n            writer.write(\"\\n\");\n            writer.write(line2);\n            writer.write(\"\\n\");\n        }\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(tmpFileEmptyLine), StandardCharsets.UTF_8)) {\n            writer.write(line1);\n            writer.write(\"\\n\");\n            writer.write(\"\\n\");\n            writer.write(line2);\n            writer.write(\"\\n\");\n        }\n        THREAD_POOL = new TestThreadPool(Thread.currentThread().getName());\n    }\n\n\n    @AfterClass\n    public static void tearDownClass() throws Exception {\n        assertThat(tmpFile.delete()).isTrue();\n        assertThat(tmpFileGz.delete()).isTrue();\n        assertThat(tmpFileEmptyLine.delete()).isTrue();\n        ThreadPool.terminate(THREAD_POOL, 30, TimeUnit.SECONDS);\n    }\n\n    @Test\n    public void testUmlautsAndWhitespacesWithExplicitURIThrowsAre() throws Throwable {\n        assertThatThrownBy(() -> collect(\"file:///this will f\u00e4il.json\"))\n            .isExactlyInstanceOf(IllegalArgumentException.class)\n            .hasMessage(\"Illegal character in path at index 12: file:///this will f\u00e4il.json\");\n    }\n\n    @Test\n    public void testNoErrorIfNoSuchFile() throws Throwable {\n        assertThat(collect(\"file:///some/path/that/shouldnt/exist/foo.json\")).satisfiesExactly(\n            line1 -> assertThat(line1.failure()).hasMessageContaining(\"No such file or directory\")\n        );\n        assertThat(collect(\"file:///some/path/that/shouldnt/exist/*\")).isEmpty();\n    }\n\n    @Test\n    public void testRelativeImport() throws Throwable {\n        assertThatThrownBy(() -> collect(\"xy\"))\n            .isExactlyInstanceOf(IllegalArgumentException.class)\n            .hasMessage(\"relative fileURIs are not allowed\");\n    }\n\n    @Test\n    public void testCollectFromUriWithGlob() throws Throwable {\n        List<LineCursor> result = collect(fileToURI(tmpFile.getParentFile()) + \"file*.json\");\n        assertThat(result).containsExactly(expectedResult(tmpFile));\n    }\n\n    @Test\n    public void testCollectFromDirectory() throws Throwable {\n        List<LineCursor> result = collect(fileToURI(tmpFile.getParentFile()) + \"*\");\n        assertThat(result).containsExactly(expectedResult(tmpFile));\n    }\n\n    @Test\n    public void test_collect_exact_uri() throws Throwable {\n        List<LineCursor> result = collect(fileToURI(tmpFile).toString());\n        assertThat(result).containsExactly(expectedResult(tmpFile));\n    }\n\n    @Test\n    public void testDoCollectRawFromCompressed() throws Throwable {\n        List<LineCursor> result = collect(Collections.singletonList(fileToURI(tmpFileGz).toString()), \"gzip\");\n        assertThat(result).containsExactly(expectedResult(tmpFileGz));\n    }\n\n    @Test\n    public void testCollectWithEmptyLine() throws Throwable {\n        List<LineCursor> result = collect(fileToURI(tmpFileEmptyLine).toString());\n        assertThat(result).containsExactly(\n            new LineCursor(fileToURI(tmpFileEmptyLine), 1, \"{\\\"name\\\": \\\"Arthur\\\", \\\"id\\\": 4, \\\"details\\\": {\\\"age\\\": 38}}\", null),\n            new LineCursor(fileToURI(tmpFileEmptyLine), 3, \"{\\\"id\\\": 5, \\\"name\\\": \\\"Trillian\\\", \\\"details\\\": {\\\"age\\\": 33}}\", null)\n        );\n    }\n\n    @Test\n    public void unsupportedURITest() throws Throwable {\n        FileReadingIterator it = it(\"invalid://crate.io/docs/en/latest/sql/reference/copy_from.html\");\n        LineCursor currentElement = it.currentElement();\n        assertThat(it.moveNext()).isTrue();\n        assertThat(currentElement.lineNumber()).isEqualTo(0);\n        assertThat(currentElement.line()).isNull();\n        assertThat(currentElement.failure()).hasMessage(\"unknown protocol: invalid\");\n    }\n\n    @Test\n    public void testMultipleUriSupport() throws Throwable {\n        List<String> fileUris = new ArrayList<>();\n        fileUris.add(Paths.get(tmpFile.toURI()).toUri().toString());\n        fileUris.add(Paths.get(tmpFileEmptyLine.toURI()).toUri().toString());\n        List<LineCursor> results = collect(fileUris, null);\n        assertThat(results).containsExactly(\n            new LineCursor(tmpFile.toURI(), 1, \"{\\\"name\\\": \\\"Arthur\\\", \\\"id\\\": 4, \\\"details\\\": {\\\"age\\\": 38}}\", null),\n            new LineCursor(tmpFile.toURI(), 2, \"{\\\"id\\\": 5, \\\"name\\\": \\\"Trillian\\\", \\\"details\\\": {\\\"age\\\": 33}}\", null),\n            new LineCursor(tmpFileEmptyLine.toURI(), 1, \"{\\\"name\\\": \\\"Arthur\\\", \\\"id\\\": 4, \\\"details\\\": {\\\"age\\\": 38}}\", null),\n            new LineCursor(tmpFileEmptyLine.toURI(), 3, \"{\\\"id\\\": 5, \\\"name\\\": \\\"Trillian\\\", \\\"details\\\": {\\\"age\\\": 33}}\", null)\n        );\n    }\n\n    private static List<LineCursor> collect(String ... fileUris) throws Exception {\n        return collect(Arrays.asList(fileUris), null);\n    }\n\n    private static FileReadingIterator it(String ... fileUris) {\n        return it(Arrays.asList(fileUris), null);\n    }\n\n    private static FileReadingIterator it(Collection<String> fileUris, String compression) {\n        return new FileReadingIterator(\n            fileUris.stream().map(FileReadingIterator::toURI).toList(),\n            compression,\n            Map.of(LocalFsFileInputFactory.NAME, new LocalFsFileInputFactory()),\n            false,\n            1,\n            0,\n            Settings.EMPTY,\n            THREAD_POOL.scheduler());\n    }\n\n    private static List<LineCursor> collect(Collection<String> fileUris, String compression) throws Exception {\n        return it(fileUris, compression)\n            .map(LineCursor::copy)\n            .toList()\n            .get(5, TimeUnit.SECONDS);\n    }\n\n    private static URI fileToURI(File file) throws IOException {\n        return file.toPath().toRealPath().toUri();\n    }\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect.files;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.mockito.ArgumentMatchers.any;\nimport static org.mockito.ArgumentMatchers.eq;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.times;\nimport static org.mockito.Mockito.verify;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.net.SocketTimeoutException;\nimport java.net.URI;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.Supplier;\nimport java.util.stream.Stream;\n\nimport org.elasticsearch.common.settings.Settings;\nimport org.elasticsearch.test.ESTestCase;\nimport org.elasticsearch.threadpool.TestThreadPool;\nimport org.elasticsearch.threadpool.ThreadPool;\nimport org.junit.AfterClass;\nimport org.junit.BeforeClass;\nimport org.junit.Test;\nimport org.mockito.ArgumentCaptor;\n\nimport io.crate.data.BatchIterator;\nimport io.crate.data.testing.BatchIteratorTester;\nimport io.crate.execution.engine.collect.files.FileReadingIterator.LineCursor;\n\npublic class FileReadingIteratorTest extends ESTestCase {\n\n    private static ThreadPool THREAD_POOL;\n\n\n    @BeforeClass\n    public static void setupThreadPool() {\n        THREAD_POOL = new TestThreadPool(Thread.currentThread().getName());\n    }\n\n    @AfterClass\n    public static void shutdownThreadPool() {\n        ThreadPool.terminate(THREAD_POOL, 30, TimeUnit.SECONDS);\n    }\n\n    /**\n     * Tests a regression resulting in an infinitive loop as the reader wasn't closed on IO errors\n     */\n    @Test\n    public void test_iterator_closes_current_reader_on_io_error() throws Exception {\n        Path tempFile1 = createTempFile(\"tempfile1\", \".csv\");\n        List<String> lines1 = List.of(\n            \"name,id,age\",\n            \"Arthur,4,38\",\n            \"Douglas,6,42\"  // <--- reader will fail on this line, so it is not part of the expected results\n        );\n        Files.write(tempFile1, lines1);\n        Path tempFile2 = createTempFile(\"tempfile2\", \".csv\");\n        List<String> lines2 = List.of(\"name,id,age\", \"Trillian,5,33\");\n        Files.write(tempFile2, lines2);\n        List<URI> fileUris = Stream.of(tempFile1.toUri().toString(), tempFile2.toUri().toString())\n            .map(FileReadingIterator::toURI)\n            .toList();\n\n        Supplier<BatchIterator<LineCursor>> batchIteratorSupplier =\n            () -> new FileReadingIterator(\n                fileUris,\n                null,\n                Map.of(LocalFsFileInputFactory.NAME, new LocalFsFileInputFactory()),\n                false,\n                1,\n                0,\n                Settings.EMPTY,\n                THREAD_POOL.scheduler()\n            ) {\n\n                @Override\n                BufferedReader createBufferedReader(InputStream inputStream) throws IOException {\n                    return new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8)) {\n\n                        private int currentLineNumber = 0;\n\n                        @Override\n                        public String readLine() throws IOException {\n                            var line = super.readLine();\n                            currentLineNumber++;\n                            if (line != null && currentLineNumber > 2) {      // fail on 3rd line, succeed on header and first row\n                                throw new IOException(\"dummy\");\n                            }\n                            return line;\n                        }\n                    };\n                }\n            };\n\n        List<String> expectedResult = Arrays.asList(\n            \"name,id,age\",\n            \"Arthur,4,38\",\n            \"name,id,age\",\n            \"Trillian,5,33\"\n        );\n        var tester = new BatchIteratorTester<>(() -> batchIteratorSupplier.get().map(LineCursor::line));\n        tester.verifyResultAndEdgeCaseBehaviour(expectedResult);\n    }\n\n    /**\n     * Validates a bug which was resulting in duplicate reads of the same line when consecutive retries happen.\n     * https://github.com/crate/crate/pull/13261\n     */\n    @Test\n    public void test_consecutive_retries_will_not_result_in_duplicate_reads() throws Exception {\n        Path tempFile = createTempFile(\"tempfile1\", \".csv\");\n        List<String> lines = List.of(\"id\", \"1\", \"2\", \"3\", \"4\", \"5\");\n        Files.write(tempFile, lines);\n        List<URI> fileUris = Stream.of(tempFile.toUri().toString())\n            .map(FileReadingIterator::toURI).toList();\n\n        Supplier<BatchIterator<LineCursor>> batchIteratorSupplier =\n            () -> new FileReadingIterator(\n                fileUris,\n                null,\n                Map.of(LocalFsFileInputFactory.NAME, new LocalFsFileInputFactory()),\n                false,\n                1,\n                0,\n                Settings.EMPTY,\n                THREAD_POOL.scheduler()\n            ) {\n                int retry = 0;\n\n                @Override\n                BufferedReader createBufferedReader(InputStream inputStream) throws IOException {\n                    return new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8)) {\n\n                        private int currentLineNumber = 0;\n\n                        @Override\n                        public String readLine() throws IOException {\n                            var line = super.readLine();\n                            // current implementation does not handle SocketTimeoutException thrown when parsing header so skip it here as well.\n                            if (currentLineNumber++ > 0 && retry++ < MAX_SOCKET_TIMEOUT_RETRIES) {\n                                throw new SocketTimeoutException(\"dummy\");\n                            }\n                            return line;\n                        }\n                    };\n                }\n            };\n\n        var tester = new BatchIteratorTester<>(() -> batchIteratorSupplier.get().map(LineCursor::line));\n        tester.verifyResultAndEdgeCaseBehaviour(lines);\n    }\n\n    @Test\n    public void test_loadNextBatch_implements_retry_with_backoff() throws IOException {\n        ScheduledExecutorService scheduler = mock(ScheduledExecutorService.class);\n        var fi = new FileReadingIterator(\n            List.of(),\n            null,\n            Map.of(),\n            false,\n            1,\n            0,\n            Settings.EMPTY,\n            scheduler\n        );\n        ArgumentCaptor<Long> delays = ArgumentCaptor.forClass(Long.class);\n\n        for (int i = 0; i < FileReadingIterator.MAX_SOCKET_TIMEOUT_RETRIES; i++) {\n            fi.loadNextBatch().complete(null);\n        }\n\n        verify(scheduler, times(FileReadingIterator.MAX_SOCKET_TIMEOUT_RETRIES))\n            .schedule(any(Runnable.class), delays.capture(), eq(TimeUnit.MILLISECONDS));\n        final List<Long> actualDelays = delays.getAllValues();\n        assertThat(actualDelays).isEqualTo(Arrays.asList(0L, 10L, 30L, 100L, 230L));\n\n        // retry fails if MAX_SOCKET_TIMEOUT_RETRIES is exceeded\n        assertThatThrownBy(fi::loadNextBatch)\n            .isExactlyInstanceOf(IllegalStateException.class)\n            .hasMessage(\"All batches already loaded\");\n    }\n\n    @Test\n    public void test_retry_from_one_uri_does_not_affect_reading_next_uri() throws Exception {\n        Path tempFile = createTempFile(\"tempfile1\", \".csv\");\n        Files.write(tempFile, List.of(\"1\", \"2\", \"3\"));\n        Path tempFile2 = createTempFile(\"tempfile2\", \".csv\");\n        Files.write(tempFile2, List.of(\"4\", \"5\", \"6\"));\n        List<URI> fileUris = Stream.of(tempFile.toUri().toString(), tempFile2.toUri().toString())\n            .map(FileReadingIterator::toURI).toList();\n\n        var fi = new FileReadingIterator(\n            fileUris,\n            null,\n            Map.of(LocalFsFileInputFactory.NAME, new LocalFsFileInputFactory()),\n            false,\n            1,\n            0,\n            Settings.EMPTY,\n            THREAD_POOL.scheduler()\n        ) {\n            private boolean isThrownOnce = false;\n            final int lineToThrow = 2;\n\n            @Override\n            BufferedReader createBufferedReader(InputStream inputStream) throws IOException {\n                return new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8)) {\n\n                    private int currentLineNumber = 0;\n                    @Override\n                    public String readLine() throws IOException {\n                        var line = super.readLine();\n                        if (!isThrownOnce && currentLineNumber++ == lineToThrow) {\n                            isThrownOnce = true;\n                            throw new SocketTimeoutException(\"dummy\");\n                        }\n                        return line;\n                    }\n                };\n            }\n        };\n\n        assertThat(fi.moveNext()).isEqualTo(true);\n        assertThat(fi.currentElement().line()).isEqualTo(\"1\");\n        assertThat(fi.moveNext()).isEqualTo(true);\n        assertThat(fi.currentElement().line()).isEqualTo(\"2\");\n        assertThat(fi.moveNext()).isEqualTo(false);\n        assertThat(fi.allLoaded()).isEqualTo(false);\n        assertThat(fi.loadNextBatch()).succeedsWithin(5, TimeUnit.SECONDS)\n            .satisfies(x -> {\n                assertThat(fi.currentElement().line()).isEqualTo(\"2\");\n                assertThat(fi.watermark).isEqualTo(3);\n                assertThat(fi.moveNext()).isEqualTo(true);\n                // the watermark helped 'fi' to recover the state right before the exception then cleared\n                assertThat(fi.watermark).isEqualTo(0);\n                assertThat(fi.currentElement().line()).isEqualTo(\"3\");\n\n                // verify the exception did not prevent reading the next URI\n                assertThat(fi.moveNext()).isEqualTo(true);\n                assertThat(fi.currentElement().line()).isEqualTo(\"4\");\n            });\n    }\n}\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.execution.engine.collect.sources;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.mockito.Mockito.mock;\n\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.UUID;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.TimeUnit;\n\nimport org.elasticsearch.common.settings.Settings;\nimport org.junit.Test;\n\nimport io.crate.analyze.CopyFromParserProperties;\nimport io.crate.data.BatchIterator;\nimport io.crate.data.Row;\nimport io.crate.execution.dsl.phases.FileUriCollectPhase;\nimport io.crate.execution.dsl.phases.FileUriCollectPhase.InputFormat;\nimport io.crate.execution.dsl.projection.Projection;\nimport io.crate.execution.engine.collect.CollectTask;\nimport io.crate.expression.symbol.Literal;\nimport io.crate.expression.symbol.Symbol;\nimport io.crate.metadata.CoordinatorTxnCtx;\nimport io.crate.metadata.Functions;\nimport io.crate.metadata.NodeContext;\nimport io.crate.test.integration.CrateDummyClusterServiceUnitTest;\nimport io.crate.testing.TestingHelpers;\nimport io.crate.types.DataTypes;\nimport io.crate.role.Role;\nimport io.crate.role.Roles;\n\npublic class FileCollectSourceTest extends CrateDummyClusterServiceUnitTest {\n\n\n    @Test\n    public void test_file_collect_source_returns_iterator_that_can_skip_lines() throws Exception {\n        List<String> targetColumns = List.of();\n        List<Projection> projections = List.of();\n        List<Symbol> toCollect = List.of(\n            TestingHelpers.createReference(\"_raw\", DataTypes.STRING)\n        );\n        Path tmpFile = createTempFile(\"tempfile1\", \".csv\");\n        Files.write(tmpFile, List.of(\n            \"garbage1\",\n            \"garbage2\",\n            \"x,y\",\n            \"1,2\",\n            \"10,20\"\n        ));\n        FileUriCollectPhase fileUriCollectPhase = new FileUriCollectPhase(\n            UUID.randomUUID(),\n            1,\n            \"copy from\",\n            List.of(),\n            Literal.of(tmpFile.toUri().toString()),\n            targetColumns,\n            toCollect,\n            projections,\n            null,\n            false,\n            new CopyFromParserProperties(true, true, ',', 2),\n            InputFormat.CSV,\n            Settings.EMPTY\n        );\n\n        Roles roles = () -> List.of(Role.CRATE_USER);\n        FileCollectSource fileCollectSource = new FileCollectSource(\n            new NodeContext(new Functions(Map.of()), roles),\n            clusterService,\n            Map.of(),\n            THREAD_POOL,\n            () -> List.of(Role.CRATE_USER)\n        );\n\n        CompletableFuture<BatchIterator<Row>> iterator = fileCollectSource.getIterator(\n            CoordinatorTxnCtx.systemTransactionContext(),\n            fileUriCollectPhase,\n            mock(CollectTask.class),\n            false\n        );\n        assertThat(iterator).succeedsWithin(5, TimeUnit.SECONDS);\n        CompletableFuture<List<Object>> resultFuture = iterator.join()\n            .map(row -> row.get(0))\n            .toList();\n\n        assertThat(resultFuture).succeedsWithin(5, TimeUnit.SECONDS);\n        assertThat(resultFuture.join()).containsExactly(\n            \"{\\\"x\\\":\\\"1\\\",\\\"y\\\":\\\"2\\\"}\",\n            \"{\\\"x\\\":\\\"10\\\",\\\"y\\\":\\\"20\\\"}\"\n        );\n    }\n}\n\n", "/*\n * Licensed to Crate.io GmbH (\"Crate\") under one or more contributor\n * license agreements.  See the NOTICE file distributed with this work for\n * additional information regarding copyright ownership.  Crate licenses\n * this file to you under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.  You may\n * obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n * License for the specific language governing permissions and limitations\n * under the License.\n *\n * However, if you have executed another commercial license agreement\n * with Crate these terms will supersede the license and you may use the\n * software solely pursuant to the terms of the relevant commercial agreement.\n */\n\npackage io.crate.integrationtests;\n\nimport static com.carrotsearch.randomizedtesting.RandomizedTest.newTempDir;\nimport static io.crate.protocols.postgres.PGErrorStatus.INTERNAL_ERROR;\nimport static io.crate.testing.Asserts.assertThat;\nimport static io.crate.testing.TestingHelpers.printedTable;\nimport static io.netty.handler.codec.http.HttpResponseStatus.BAD_REQUEST;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.OutputStreamWriter;\nimport java.net.URISyntaxException;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.DirectoryStream;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;\nimport java.util.UUID;\n\nimport org.elasticsearch.test.IntegTestCase;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.TemporaryFolder;\n\nimport com.carrotsearch.randomizedtesting.LifecycleScope;\n\nimport io.crate.action.sql.Sessions;\nimport io.crate.exceptions.UnauthorizedException;\nimport io.crate.role.Role;\nimport io.crate.role.Roles;\nimport io.crate.testing.Asserts;\nimport io.crate.testing.SQLResponse;\nimport io.crate.testing.UseJdbc;\nimport io.crate.testing.UseNewCluster;\nimport io.crate.testing.UseRandomizedSchema;\n\n@IntegTestCase.ClusterScope(numDataNodes = 2)\npublic class CopyIntegrationTest extends SQLHttpIntegrationTest {\n\n    private final String copyFilePath =\n        Paths.get(getClass().getResource(\"/essetup/data/copy\").toURI()).toUri().toString();\n    private final String copyFilePathShared =\n        Paths.get(getClass().getResource(\"/essetup/data/copy/shared\").toURI()).toUri().toString();\n    private final String nestedArrayCopyFilePath =\n        Paths.get(getClass().getResource(\"/essetup/data/nested_array\").toURI()).toUri().toString();\n\n    private Setup setup = new Setup(sqlExecutor);\n\n    @Rule\n    public TemporaryFolder folder = new TemporaryFolder();\n\n    public CopyIntegrationTest() throws URISyntaxException {\n    }\n\n    @Test\n    public void testCopyFromUnknownDirectory() {\n        execute(\"create table t (a int)\");\n        ensureYellow();\n        execute(\"copy t from 'file:///tmp/unknown_dir/*'\");\n    }\n\n    @Test\n    public void testCopyFromFileWithJsonExtension() {\n        execute(\"create table quotes (id int primary key, \" +\n                \"quote string index using fulltext) with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes from ?\", new Object[] {copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(3L);\n        assertThat(response.rows()[0]).hasSize(2);\n\n        execute(\"select quote from quotes where id = 1\");\n        assertThat(response.rows()[0][0]).isEqualTo(\"Don't pa\\u00f1ic.\");\n    }\n\n    @Test\n    public void testCopyFromFileWithCSVOption() {\n        execute(\"create table quotes (id int primary key, \" +\n            \"quote string index using fulltext) with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes from ? with (format='csv')\", new Object[]{copyFilePath + \"test_copy_from_csv.ext\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(3L);\n        assertThat(response.rows()[0]).hasSize(2);\n\n        execute(\"select quote from quotes where id = 1\");\n        assertThat(response.rows()[0][0]).isEqualTo(\"Don't pa\\u00f1ic.\");\n    }\n\n    @Test\n    public void testCopyFromFileWithCSVOptionWithDynamicColumnCreation() {\n        execute(\"create table quotes (id int primary key, \" +\n                \"quote string index using fulltext) with (number_of_replicas = 0, column_policy = 'dynamic')\");\n\n        execute(\"copy quotes from ? with (format='csv')\", new Object[]{copyFilePath + \"test_copy_from_csv_extra_column.ext\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(3L);\n        assertThat(response.rows()[0]).hasSize(3);\n\n        execute(\"select quote, comment from quotes where id = 1\");\n        assertThat(response.rows()[0][0]).isEqualTo(\"Don't pa\\u00f1ic.\");\n        assertThat(response.rows()[0][1]).isEqualTo(\"good one\");\n    }\n\n    @Test\n    public void testCopyFromFileWithCSVOptionWithTargetColumns() {\n        execute(\"create table quotes (id int primary key, \" +\n                \"quote string index using fulltext, comment text) with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes(id, quote, comment) from ? with (format='csv')\", new Object[]{copyFilePath + \"test_copy_from_csv_extra_column.ext\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(3L);\n        assertThat(response.rows()[0]).hasSize(3);\n\n        execute(\"select quote, comment from quotes where id = 1\");\n        assertThat(response.rows()[0][0]).isEqualTo(\"Don't pa\\u00f1ic.\");\n        assertThat(response.rows()[0][1]).isEqualTo(\"good one\");\n    }\n\n    @Test\n    public void testCopyFromFileWithCSVOptionWithNoHeader() {\n        execute(\"create table quotes (id int primary key, \" +\n                \"quote string index using fulltext) with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes from ? with (format='csv', header=false)\", new Object[]{copyFilePath + \"test_copy_from_csv_no_header.ext\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(3L);\n        assertThat(response.rows()[0]).hasSize(2);\n\n        execute(\"select quote from quotes where id = 1\");\n        assertThat(response.rows()[0][0]).isEqualTo(\"Don't pa\\u00f1ic.\");\n    }\n\n    @Test\n    public void testCopyFromFileWithUmlautsWhitespacesAndGlobs() throws Exception {\n        execute(\"create table t (id int primary key, name string) clustered into 1 shards with (number_of_replicas = 0)\");\n        File tmpFolder = folder.newFolder(\"\u00e4wes\u00f6me f\u00f6lder\");\n        File file = new File(tmpFolder, \"s\u00fcp\u00e4r.json\");\n\n        List<String> lines = Collections.singletonList(\"{\\\"id\\\": 1, \\\"name\\\": \\\"Arthur\\\"}\");\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ?\", new Object[]{Paths.get(tmpFolder.toURI()).toUri().toString() + \"s*.json\"});\n        assertThat(response).hasRowCount(1L);\n    }\n\n    @Test\n    public void testCopyFromWithOverwriteDuplicates() throws Exception {\n        execute(\"create table t (id int primary key) with (number_of_replicas = 0)\");\n\n        execute(\"insert into t (id) values (?)\", new Object[][]{\n            new Object[]{1},\n            new Object[]{2},\n            new Object[]{3},\n            new Object[]{4}\n        });\n        execute(\"refresh table t\");\n\n        File tmpExport = folder.newFolder(\"tmpExport\");\n        String uriTemplate = Paths.get(tmpExport.toURI()).toUri().toString();\n        execute(\"copy t to directory ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(4L);\n        execute(\"copy t from ?\", new Object[]{uriTemplate + \"*\"});\n        assertThat(response).hasRowCount(0L);\n        execute(\"copy t from ? with (overwrite_duplicates = true, shared=true)\",\n            new Object[]{uriTemplate + \"*\"});\n        assertThat(response).hasRowCount(4L);\n        execute(\"refresh table t\");\n        execute(\"select count(*) from t\");\n        assertThat(((Long) response.rows()[0][0])).isEqualTo(4L);\n    }\n\n    @Test\n    public void testCopyFromFileWithoutPK() throws Exception {\n        execute(\"create table quotes (id int, \" +\n                \"quote string index using fulltext) with (number_of_replicas=0)\");\n\n        execute(\"copy quotes from ?\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(6L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(6L);\n        assertThat(response.rows()[0]).hasSize(2);\n    }\n\n    @Test\n    public void testCopyFromFilePattern() {\n        execute(\"create table quotes (id int primary key, \" +\n                \"quote string index using fulltext) with (number_of_replicas=0)\");\n\n        execute(\"copy quotes from ?\", new Object[]{copyFilePathShared + \"*.json\"});\n        assertThat(response).hasRowCount(6L);\n        refresh();\n\n        execute(\"select * from quotes\");\n        assertThat(response).hasRowCount(6L);\n    }\n\n    @Test\n    public void testCopyFromFileWithEmptyLine() throws Exception {\n        execute(\"create table foo (id integer primary key) clustered into 1 shards with (number_of_replicas=0)\");\n        File newFile = folder.newFile();\n\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(newFile), StandardCharsets.UTF_8)) {\n            writer.write(\"{\\\"id\\\":1}\\n\");\n            writer.write(\"\\n\");\n            writer.write(\"{\\\"id\\\":2}\\n\");\n        }\n        execute(\"copy foo from ?\", new Object[]{Paths.get(newFile.toURI()).toUri().toString()});\n        assertThat(response).hasRowCount(2L);\n        refresh();\n\n        execute(\"select * from foo order by id\");\n        assertThat(response.rows()[0][0]).isEqualTo(1);\n        assertThat(response.rows()[1][0]).isEqualTo(2);\n    }\n\n    /**\n     * Disable JDBC/PSQL as object values are streamed via JSON on the PSQL wire protocol which is not type safe.\n     */\n    @UseJdbc(0)\n    @Test\n    public void testCopyFromFileWithInvalidColumns() throws Exception {\n        execute(\"create table foo (id integer primary key) clustered into 1 shards \" +\n                \"with (number_of_replicas=0, column_policy='dynamic')\");\n        File newFile = folder.newFile();\n\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(newFile), StandardCharsets.UTF_8)) {\n            writer.write(\"{\\\"id\\\":1, \\\"_invalid\\\":1}\\n\");\n            writer.write(\"{\\\"id\\\":2, \\\"invalid['index']\\\":2}\\n\");\n            writer.write(\"{\\\"id\\\":3, \\\"invalid['_invalid']\\\":3}\\n\");\n            writer.write(\"{\\\"id\\\":4, \\\"valid\\\": {\\\"_valid\\\": 4}}\\n\");\n        }\n\n        execute(\"copy foo from ?\", new Object[]{Paths.get(newFile.toURI()).toUri().toString()});\n        assertThat(response).hasRowCount(1L);\n        refresh();\n\n        execute(\"select * from foo order by id\");\n\n        // Check columns.\n        assertThat(response.cols()).hasSize(2);\n        assertThat(response.cols()[1]).isEqualTo(\"valid\");\n\n        // Check data of column.\n        assertThat(response.rows()[0][0]).isEqualTo(4);\n        HashMap<?, ?> data = (HashMap<?, ?>)response.rows()[0][1];\n        // The inner value will result in an Long type as we rely on ES mappers here and the dynamic ES parsing\n        // will define integers as longs (no concrete type was specified so use long to be safe)\n        assertThat(data.get(\"_valid\")).isEqualTo(4L);\n    }\n\n    @Test\n    public void testCopyFromInvalidJson() throws Exception {\n        execute(\"create table foo (id integer primary key) clustered into 1 shards with (number_of_replicas=0)\");\n        File newFile = folder.newFile();\n        try (OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(newFile), StandardCharsets.UTF_8)) {\n            writer.write(\"{|}\");\n        }\n        execute(\"copy foo from ?\", new Object[]{Paths.get(newFile.toURI()).toUri().toString()});\n        assertThat(response).hasRowCount(0L);\n    }\n\n    @Test\n    public void testCopyFromFileIntoSinglePartition() throws Exception {\n        execute(\"CREATE TABLE quotes (id INTEGER, quote STRING) PARTITIONED BY (id)\");\n        ensureGreen();\n        execute(\"COPY quotes PARTITION (id = 1) FROM ? WITH (shared = true)\", new Object[]{\n            copyFilePath + \"test_copy_from.json\"});\n        refresh();\n\n        execute(\"SELECT * FROM quotes\");\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyFromFileWithCompression() throws Exception {\n        execute(\"create table quotes (id int, \" +\n                \"quote string)\");\n        execute(\"copy quotes from ? with (compression='gzip')\", new Object[]{copyFilePath + \"test_copy_from.gz\"});\n        execute(\"refresh table quotes\");\n        execute(\"select * from quotes order by quote\");\n        assertThat(response).hasRows(\n            \"1| Don't pa\u00f1ic.\",\n            \"1| Don't pa\u00f1ic.\",\n            \"3| Time is an illusion. Lunchtime doubly so.\",\n            \"3| Time is an illusion. Lunchtime doubly so.\",\n            \"2| Would it save you a lot of time if I just gave up and went mad now?\",\n            \"2| Would it save you a lot of time if I just gave up and went mad now?\"\n        );\n    }\n\n    @Test\n    public void testCopyFromWithGeneratedColumn() throws Exception {\n        execute(\"create table quotes (\" +\n                \" id int,\" +\n                \" quote string,\" +\n                \" gen_quote as concat(quote, ' This is awesome!')\" +\n                \")\");\n\n        execute(\"copy quotes from ? with (shared=true)\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        refresh();\n\n        execute(\"select gen_quote from quotes limit 1\");\n        assertThat((String) response.rows()[0][0]).endsWith(\"This is awesome!\");\n    }\n\n    @Test\n    public void testCopyFromWithInvalidGivenGeneratedColumn() throws Exception {\n        execute(\"create table quotes (\" +\n                \" id int,\" +\n                \" quote as cast(id as string)\" +\n                \")\");\n        execute(\"copy quotes from ? with (shared=true)\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(0L);\n    }\n\n    @Test\n    public void testCopyFromToPartitionedTableWithGeneratedColumn() throws Exception {\n        execute(\"create table quotes (\" +\n                \" id int,\" +\n                \" quote string,\" +\n                \" gen_quote as concat(quote, ' Partitioned by awesomeness!')\" +\n                \") partitioned by (gen_quote)\");\n\n        execute(\"copy quotes from ? with (shared=true)\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        refresh();\n\n        execute(\"select gen_quote from quotes limit 1\");\n        assertThat((String) response.rows()[0][0]).endsWith(\"Partitioned by awesomeness!\");\n    }\n\n    @Test\n    public void testCopyFromToPartitionedTableWithNullValue() {\n        execute(\"CREATE TABLE times (\" +\n                \"   time timestamp with time zone\" +\n                \") partitioned by (time)\");\n\n        execute(\"copy times from ? with (shared=true)\", new Object[]{copyFilePath + \"test_copy_from_null_value.json\"});\n        refresh();\n\n        execute(\"select time from times\");\n        assertThat(response).hasRowCount(1L);\n        assertThat(response.rows()[0][0]).isNull();\n    }\n\n    @Test\n    public void testCopyFromIntoPartitionWithInvalidGivenGeneratedColumnAsPartitionKey() throws Exception {\n        // test that rows are imported into defined partition even that the partition value does not match the\n        // generated column expression value\n        execute(\"create table quotes (\" +\n                \" id int,\" +\n                \" quote string,\" +\n                \" id_str as cast(id+1 as string)\" +\n                \") partitioned by (id_str)\");\n\n        execute(\"copy quotes partition (id_str = 1) from ? with (shared=true)\", new Object[]{\n            copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select * from quotes where id_str = 1\");\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyToFile() throws Exception {\n        execute(\"create table singleshard (name string) clustered into 1 shards with (number_of_replicas = 0)\");\n\n        Asserts.assertSQLError(() -> execute(\"copy singleshard to '/tmp/file.json'\"))\n            .hasPGError(INTERNAL_ERROR)\n            .hasHTTPError(BAD_REQUEST, 4004)\n            .hasMessageContaining(\"Using COPY TO without specifying a DIRECTORY is not supported\");\n    }\n\n    @Test\n    public void testCopyToDirectory() throws Exception {\n        this.setup.groupBySetup();\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy characters to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(7L);\n        String[] list = folder.getRoot().list();\n        assertThat(list).hasSizeGreaterThanOrEqualTo(1);\n        for (String file : list) {\n            assertThat(file).startsWith(\"characters_\");\n        }\n\n        List<String> lines = new ArrayList<>(7);\n        DirectoryStream<Path> stream = Files.newDirectoryStream(Paths.get(folder.getRoot().toURI()), \"*.json\");\n        for (Path path : stream) {\n            lines.addAll(Files.readAllLines(path, StandardCharsets.UTF_8));\n        }\n        assertThat(lines).hasSize(7);\n        for (String line : lines) {\n            assertThat(line).startsWith(\"{\");\n            assertThat(line).endsWith(\"}\");\n        }\n    }\n\n    @Test\n    public void testCopyToWithCompression() throws Exception {\n        execute(\"create table singleshard (name string) clustered into 1 shards with (number_of_replicas = 0)\");\n        execute(\"insert into singleshard (name) values ('foo')\");\n        execute(\"refresh table singleshard\");\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy singleshard to DIRECTORY ? with (compression='gzip')\", new Object[]{uriTemplate});\n\n        assertThat(response).hasRowCount(1L);\n\n        String[] list = folder.getRoot().list();\n        assertThat(list).hasSize(1);\n        String file = list[0];\n        assertThat(file)\n            .startsWith(\"singleshard_\")\n            .endsWith(\".json.gz\");\n\n        long size = Files.size(Paths.get(folder.getRoot().toURI().resolve(file)));\n        assertThat(size).isEqualTo(35L);\n    }\n\n    @Test\n    public void testCopyColumnsToDirectory() throws Exception {\n        this.setup.groupBySetup();\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy characters (name, details['job']) to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response.cols()).isEmpty();\n        assertThat(response).hasRowCount(7L);\n        List<String> lines = new ArrayList<>(7);\n        DirectoryStream<Path> stream = Files.newDirectoryStream(Paths.get(folder.getRoot().toURI()), \"*.json\");\n        for (Path entry : stream) {\n            lines.addAll(Files.readAllLines(entry, StandardCharsets.UTF_8));\n        }\n        Path path = Paths.get(folder.getRoot().toURI().resolve(\"characters_0_.json\"));\n        assertThat(path).exists();\n        assertThat(lines).hasSize(7);\n        assertThat(lines)\n            .anySatisfy(x -> assertThat(x).contains(\"Sandwitch Maker\"))\n            .anySatisfy(x -> assertThat(x).contains(\"Arthur Dent\"));\n        assertThat(lines).allSatisfy(\n            x -> assertThat(x.trim()).startsWith(\"[\").endsWith(\"]\"));\n    }\n\n    @Test\n    public void testCopyToFileColumnsJsonObjectOutput() throws Exception {\n        execute(\"create table singleshard (name string, test object as (foo string)) clustered into 1 shards with (number_of_replicas = 0)\");\n        execute(\"insert into singleshard (name, test) values ('foobar', {foo='bar'})\");\n        execute(\"refresh table singleshard\");\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy singleshard (name, test['foo']) to DIRECTORY ? with (format='json_object')\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(1L);\n\n        String[] list = folder.getRoot().list();\n        assertThat(list).hasSize(1);\n        List<String> lines = Files.readAllLines(\n            Paths.get(folder.getRoot().toURI().resolve(list[0])), StandardCharsets.UTF_8);\n\n        assertThat(lines).hasSize(1);\n        for (String line : lines) {\n            assertThat(line).startsWith(\"{\");\n            assertThat(line).endsWith(\"}\");\n        }\n    }\n\n    @Test\n    public void testCopyToWithWhere() throws Exception {\n        this.setup.groupBySetup();\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy characters where gender = 'female' to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(2L);\n    }\n\n    @Test\n    public void testCopyToWithWhereOnPrimaryKey() throws Exception {\n        execute(\"create table t1 (id int primary key) with (number_of_replicas = 0)\");\n        execute(\"insert into t1 (id) values (1)\");\n        execute(\"refresh table t1\");\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy t1 where id = 1 to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(1L);\n    }\n\n    @Test\n    public void testCopyToWithWhereNoMatch() throws Exception {\n        this.setup.groupBySetup();\n\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy characters where gender = 'foo' to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(0L);\n    }\n\n    @Test\n    public void testCopyFromNestedArrayRow() throws Exception {\n        // assert that rows with nested arrays aren't imported\n        execute(\"create table users (id int, \" +\n            \"name string) with (number_of_replicas=0, column_policy = 'dynamic')\");\n        execute(\"copy users from ? with (shared=true)\", new Object[]{\n            nestedArrayCopyFilePath + \"nested_array_copy_from.json\"});\n        assertThat(response).hasRowCount(1L); // only 1 document got inserted\n        refresh();\n\n        execute(\"select * from users\");\n        assertThat(response).hasRowCount(1L);\n\n        assertThat(printedTable(response.rows())).isEqualTo(\"2| Trillian\\n\");\n    }\n\n    @Test\n    public void testCopyToWithGeneratedColumn() {\n        execute(\"CREATE TABLE foo (\" +\n                \"   day TIMESTAMP WITH TIME ZONE GENERATED ALWAYS AS date_trunc('day', timestamp),\" +\n                \"   timestamp TIMESTAMP WITH TIME ZONE\" +\n                \") PARTITIONED BY (day)\");\n        execute(\"insert into foo (timestamp) values (1454454000377)\");\n        refresh();\n        String uriTemplate = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy foo to DIRECTORY ?\", new Object[]{uriTemplate});\n        assertThat(response).hasRowCount(1L);\n    }\n\n    @Test\n    public void testCopyFromWithRoutingInPK() throws Exception {\n        execute(\"create table t (i int primary key, c string primary key, a int)\" +\n            \" clustered by (c) with (number_of_replicas=0)\");\n        ensureGreen();\n        execute(\"insert into t (i, c) values (1, 'clusteredbyvalue'), (2, 'clusteredbyvalue')\");\n        refresh();\n\n        String uri = Paths.get(folder.getRoot().toURI()).toUri().toString();\n        SQLResponse response = execute(\"copy t to directory ?\", new Object[]{uri});\n        assertThat(response).hasRowCount(2L);\n\n        execute(\"delete from t\");\n        refresh();\n\n        execute(\"copy t from ? with (shared=true)\", new Object[]{uri + \"t_*\"});\n        refresh();\n\n        // only one shard should have all imported rows, since we have the same routing for both rows\n        response = execute(\"select count(*) from sys.shards where num_docs>0 and table_name='t'\");\n        assertThat(response.rows()[0][0]).isEqualTo(1L);\n    }\n\n    @Test\n    public void testCopyFromTwoHttpUrls() throws Exception {\n        execute(\"create blob table blobs with (number_of_replicas = 0)\");\n        execute(\"create table names (id int primary key, name string) with (number_of_replicas = 0)\");\n\n        String r1 = \"{\\\"id\\\": 1, \\\"name\\\":\\\"Marvin\\\"}\";\n        String r2 = \"{\\\"id\\\": 2, \\\"name\\\":\\\"Slartibartfast\\\"}\";\n        List<String> urls = List.of(upload(\"blobs\", r1), upload(\"blobs\", r2));\n\n        execute(\"copy names from ?\", new Object[]{urls});\n        assertThat(response).hasRowCount(2L);\n        execute(\"refresh table names\");\n        execute(\"select name from names order by id\");\n        assertThat(printedTable(response.rows())).isEqualTo(\"Marvin\\nSlartibartfast\\n\");\n    }\n\n    @Test\n    public void testCopyFromTwoUriMixedSchemaAndWildcardUse() throws Exception {\n        execute(\"create blob table blobs with (number_of_replicas = 0)\");\n        execute(\"create table names (id int primary key, name string) with (number_of_replicas = 0)\");\n\n        Path tmpDir = newTempDir(LifecycleScope.TEST);\n        File file = new File(tmpDir.toFile(), \"names.json\");\n        String r1 = \"{\\\"id\\\": 1, \\\"name\\\": \\\"Arthur\\\"}\";\n        String r2 = \"{\\\"id\\\": 2, \\\"name\\\":\\\"Slartibartfast\\\"}\";\n\n        Files.write(file.toPath(), Collections.singletonList(r1), StandardCharsets.UTF_8);\n        List<String> urls = List.of(tmpDir.toUri().toString() + \"*.json\", upload(\"blobs\", r2));\n\n        execute(\"copy names from ?\", new Object[]{urls});\n        assertThat(response).hasRowCount(2L);\n        execute(\"refresh table names\");\n        execute(\"select name from names order by id\");\n        assertThat(printedTable(response.rows())).isEqualTo(\"Arthur\\nSlartibartfast\\n\");\n    }\n\n    @Test\n    public void testCopyFromIntoTableWithClusterBy() throws Exception {\n        execute(\"create table quotes (id int, quote string) \" +\n            \"clustered by (id)\" +\n            \"with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes from ? with (shared = true)\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select quote from quotes where id = 2\");\n        assertThat((String) response.rows()[0][0]).contains(\"lot of time\");\n    }\n\n    @Test\n    public void testCopyFromIntoTableWithPkAndClusterBy() throws Exception {\n        execute(\"create table quotes (id int primary key, quote string) \" +\n            \"clustered by (id)\" +\n            \"with (number_of_replicas = 0)\");\n\n        execute(\"copy quotes from ?\", new Object[]{copyFilePath + \"test_copy_from.json\"});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"select quote from quotes where id = 3\");\n        assertThat((String) response.rows()[0][0]).contains(\"Time is an illusion.\");\n    }\n\n    private Path setUpTableAndSymlink(String tableName) throws IOException {\n        execute(String.format(Locale.ENGLISH,\n            \"create table %s (a int) with (number_of_replicas = 0)\",\n            tableName));\n\n        String r1 = \"{\\\"a\\\": 1}\";\n        String r2 = \"{\\\"a\\\": 2}\";\n        String r3 = \"{\\\"a\\\": 3}\";\n        return tmpFileWithLines(List.of(r1, r2, r3));\n    }\n\n    public static Path tmpFileWithLines(Iterable<String> lines) throws IOException {\n        Path tmpDir = newTempDir(LifecycleScope.TEST);\n        Path target = Files.createDirectories(tmpDir.resolve(\"target\"));\n        tmpFileWithLines(lines, \"data.json\", target);\n        return Files.createSymbolicLink(tmpDir.resolve(\"link\"), target);\n    }\n\n    public static void tmpFileWithLines(Iterable<String> lines, String filename, Path target) throws IOException {\n        File file = new File(target.toFile(), filename);\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n    }\n\n    @Test\n    public void testCopyFromSymlinkFolderWithWildcard() throws Exception {\n        Path link = setUpTableAndSymlink(\"t\");\n        execute(\"copy t from ? with (shared=true)\", new Object[]{\n            link.toUri().toString() + \"*\"\n        });\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyFromSymlinkFolderWithPrefixedWildcard() throws Exception {\n        Path link = setUpTableAndSymlink(\"t\");\n        execute(\"copy t from ? with (shared=true)\", new Object[]{\n            link.toUri().toString() + \"d*\"\n        });\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyFromSymlinkFolderWithSuffixedWildcard() throws Exception {\n        Path link = setUpTableAndSymlink(\"t\");\n        execute(\"copy t from ? with (shared=true)\", new Object[]{\n            link.toUri().toString() + \"*.json\"\n        });\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyFromFileInSymlinkFolder() throws Exception {\n        Path link = setUpTableAndSymlink(\"t\");\n        execute(\"copy t from ? with (shared=true)\", new Object[]{\n            link.toUri().toString() + \"data.json\"\n        });\n        assertThat(response).hasRowCount(3L);\n    }\n\n    @Test\n    public void testCopyWithGeneratedPartitionColumnThatIsPartOfPrimaryKey() throws Exception {\n        execute(\n            \"create table t1 (\" +\n            \"   guid string,\" +\n            \"   ts timestamp with time zone,\" +\n            \"   g_ts_month timestamp with time zone generated always as date_trunc('month', ts),\" +\n            \"   primary key (guid, g_ts_month)\" +\n            \") partitioned by (g_ts_month)\");\n\n        Path path = tmpFileWithLines(Arrays.asList(\n            \"{\\\"guid\\\": \\\"a\\\", \\\"ts\\\": 1496275200000}\",\n            \"{\\\"guid\\\": \\\"b\\\", \\\"ts\\\": 1496275300000}\"\n        ));\n        execute(\"copy t1 from ? with (shared=true)\", new Object[] { path.toUri().toString() + \"*.json\"});\n        assertThat(response).hasRowCount(2L);\n\n        execute(\"copy t1 partition (g_ts_month = 1496275200000) from ? with (shared=true, overwrite_duplicates=true)\",\n            new Object[] { path.toUri().toString() + \"*.json\"});\n        assertThat(response).hasRowCount(2L);\n    }\n\n    @Test\n    public void testCopyFromReturnSummaryWithFailedRows() throws Exception {\n        execute(\"create table t1 (id int primary key, ts timestamp with time zone)\");\n\n        Path tmpDir = newTempDir(LifecycleScope.TEST);\n        Path target = Files.createDirectories(tmpDir.resolve(\"target\"));\n\n        tmpFileWithLines(Arrays.asList(\n            \"{\\\"id\\\": 1, \\\"ts\\\": 1496275200000}\",\n            \"{\\\"id\\\": 2, \\\"ts\\\": 1496275300000}\"\n        ), \"data1.json\", target);\n        tmpFileWithLines(Arrays.asList(\n            \"{\\\"id\\\": 2, \\\"ts\\\": 1496275200000}\",       // <-- duplicate key\n            \"{\\\"id\\\": 3, \\\"ts\\\": 1496275300000}\"\n        ), \"data2.json\", target);\n        tmpFileWithLines(Arrays.asList(\n            \"{\\\"id\\\": 4, \\\"ts\\\": 1496275200000}\",\n            \"{\\\"id\\\": 5, \\\"ts\\\": \\\"May\\\"}\",              // <-- invalid timestamp\n            \"{\\\"id\\\": 7, \\\"ts\\\": \\\"Juli\\\"}\"              // <-- invalid timestamp\n        ), \"data3.json\", target);\n        tmpFileWithLines(Arrays.asList(\n            \"foo\",                                      // <-- invalid json\n            \"{\\\"id\\\": 6, \\\"ts\\\": 1496275200000}\"\n        ), \"data4.json\", target);\n\n        execute(\"copy t1 from ? with (shared=true) return summary\", new Object[]{target.toUri().toString() + \"*\"});\n        String result = printedTable(response.rows());\n\n        // one of the first files should be processed without any error\n        assertThat(result).contains(\"| 2| 0| {}\");\n        // one of the first files will have a duplicate key error\n        assertThat(result).contains(\"| 1| 1| {A document with the same primary key exists already={count=1, line_numbers=[\");\n        // file `data3.json` has a invalid timestamp error\n        assertThat(result).contains(\"data3.json| 1| 2| {Cannot cast value \");\n        assertThat(result).contains(\"Cannot cast value `Juli` to type `timestamp with time zone`={count=1, line_numbers=[3]}\");\n        assertThat(result).contains(\"Cannot cast value `May` to type `timestamp with time zone`={count=1, line_numbers=[2]}\");\n        // file `data4.json` has an invalid json item entry\n        assertThat(result).contains(\"data4.json| 1| 1| {JSON parser error: \");\n    }\n\n    @Test\n    @SuppressWarnings(\"unchecked\")\n    public void testCopyFromReturnSummaryWithFailedURI() throws Exception {\n        execute(\"create table t1 (id int primary key, ts timestamp with time zone)\");\n\n        Path tmpDir = newTempDir(LifecycleScope.TEST);\n        String tmpDirStr = tmpDir.toUri().toString();\n\n        String filename = \"nonexistingfile.json\";\n        execute(\"copy t1 from ? return summary\", new Object[]{tmpDirStr + filename});\n        assertThat(response).hasRowCount((long) cluster().numDataNodes());\n\n        boolean isRunningOnWindows = System.getProperty(\"os.name\").startsWith(\"Windows\");\n        String expected = isRunningOnWindows\n            ? \"(The system cannot find the file specified)\"\n            : \"(No such file or directory)\";\n        for (Object[] row : response.rows()) {\n            assertThat((String) row[1]).endsWith(filename);\n            assertThat(row[2]).isNull();\n            assertThat(row[3]).isNull();\n            assertThat(((Map<String, Object>) row[4]).keySet())\n                .anySatisfy(key -> assertThat(key).contains(expected));\n        }\n\n        // with shared=true, only 1 data node must try to process the uri\n        execute(\"copy t1 from ? with (shared=true) return summary\", new Object[]{tmpDirStr + filename});\n        assertThat(response).hasRowCount(1L);\n\n        for (Object[] row : response.rows()) {\n            assertThat((String) row[1]).endsWith(filename);\n            assertThat(row[2]).isNull();\n            assertThat(row[3]).isNull();\n            assertThat(((Map<String, Object>) row[4]).keySet())\n                .anySatisfy(key -> assertThat(key).contains(expected));\n        }\n\n        // with shared=true and wildcards all nodes will try to match a file\n        filename = \"*.json\";\n        execute(\"copy t1 from ? with (shared=true) return summary\", new Object[] {tmpDirStr + filename});\n        assertThat(response).hasRowCount((long) cluster().numDataNodes());\n\n        for (Object[] row : response.rows()) {\n            assertThat((String) row[1]).endsWith(\"*.json\");\n            assertThat(row[2]).isNull();\n            assertThat(row[3]).isNull();\n            assertThat(((Map<String, Object>) row[4]).keySet())\n                .anySatisfy(key -> assertThat(key).contains(\"Cannot find any URI matching:\"));\n        }\n    }\n\n    @Test\n    public void test_copy_from_csv_file_with_empty_string_as_null_property() throws Exception {\n        execute(\n            \"CREATE TABLE t (id int primary key, name text) \" +\n            \"CLUSTERED INTO 1 SHARDS \");\n        File file = folder.newFile(UUID.randomUUID().toString());\n\n        List<String> lines = List.of(\n            \"id,name\",\n            \"1, \\\"foo\\\"\",\n            \"2,\\\"\\\"\",\n            \"3,\"\n        );\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\n            \"COPY t FROM ? WITH (format='csv', empty_string_as_null=true)\",\n            new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(response).hasRowCount(3L);\n        refresh();\n\n        execute(\"SELECT * FROM t ORDER BY id\");\n        assertThat(response).hasRows(\n            \"1| foo\",\n            \"2| NULL\",\n            \"3| NULL\"\n        );\n    }\n\n    @Test\n    public void test_can_import_data_requiring_cast_from_csv_into_partitioned_table() throws Exception {\n        execute(\n            \"\"\"\n                    create table tbl (\n                        ts timestamp with time zone not null,\n                        ts_month timestamp with time zone generated always as date_trunc('month', ts)\n                    ) partitioned by (ts_month)\n                \"\"\");\n        List<String> lines = List.of(\n            \"ts\",\n            \"1626188198073\"\n        );\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"COPY tbl FROM ? WITH (format = 'csv', shared = true)\",\n                new Object[] {Paths.get(file.toURI()).toUri().toString()}\n        );\n        execute(\"refresh table tbl\");\n        execute(\"SELECT * FROM tbl\");\n        assertThat(response).hasRows(\n            \"1626188198073| 1625097600000\"\n        );\n    }\n\n    @UseNewCluster\n    @Test\n    public void test_copy_excludes_partitioned_values_from_source() throws Exception {\n        execute(\"create table tbl (x int, p int) partitioned by (p)\");\n\n        {\n            List<String> lines = List.of(\n                \"\"\"\n                {\"x\": 10, \"p\": 1}\n                \"\"\"\n            );\n            File file = folder.newFile(UUID.randomUUID().toString());\n            Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n            execute(\n                \"copy tbl from ? with (wait_for_completion = true, shared = true)\",\n                new Object[] { file.toPath().toUri().toString() }\n            );\n            execute(\"refresh table tbl\");\n            execute(\"SELECT _raw, * FROM tbl\");\n            assertThat(response).hasRows(\n                \"{\\\"1\\\":10}| 10| 1\"\n            );\n        }\n\n        {\n            execute(\"create table tbl2 (x int, o object as (p int)) partitioned by (o['p'])\");\n            List<String> lines = List.of(\n                \"\"\"\n                {\"x\": 10, \"o\": {\"p\": 1}}\n                \"\"\"\n            );\n            File file = folder.newFile(UUID.randomUUID().toString());\n            Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n            execute(\n                \"copy tbl2 from ? with (wait_for_completion = true, shared = true)\",\n                new Object[] { file.toPath().toUri().toString() }\n            );\n            execute(\"refresh table tbl2\");\n            execute(\"SELECT _raw, * FROM tbl2\");\n            assertThat(response).hasRows(\n                \"{\\\"3\\\":10,\\\"4\\\":{}}| 10| {p=1}\"\n            );\n        }\n    }\n\n    @Test\n    public void test_copy_from_unknown_column_to_strict_object() throws Exception {\n        // test for strict_table ver. can be found at FromRawInsertSourceTest, whereas strict_object is tested by DocumentMapper\n        execute(\"create table t (o object(strict) as (a int))\");\n\n        List<String> lines = List.of(\n            \"{\\\"o\\\": {\\\"a\\\":123, \\\"b\\\":456}}\"\n        );\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? return summary\", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(printedTable(response.rows())).contains(\n            \"Cannot add column `b` to strict object `o`\");\n    }\n\n    @Test\n    public void test_copy_from_unknown_column_to_dynamic_object() throws Exception {\n        execute(\"create table t (o object(dynamic) as (a int))\");\n\n        List<String> lines = List.of(\n            \"{\\\"o\\\": {\\\"a\\\":123, \\\"b\\\":456}}\"\n        );\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true)\", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        execute(\"refresh table t\");\n        execute(\"select o['a'] + o['b'] from t\");\n        assertThat(printedTable(response.rows())).isEqualTo(\"579\\n\");\n    }\n\n    @Test\n    public void testCopyFromWithValidationSetToTrueDoesTypeValidation() throws Exception {\n\n        // copying an empty string to a boolean column\n\n        execute(\"create table t (a boolean)\");\n\n        List<String> lines = List.of(\"{\\\"a\\\": \\\"\\\"}\");\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true, validation = true) return summary\",\n                new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(printedTable(response.rows())).contains(\"Cannot cast value `` to type `boolean`\");\n        execute(\"refresh table t\");\n        execute(\"select count(*) from t\");\n        assertThat(response.rows()[0][0]).isEqualTo(0L);\n    }\n\n    @Test\n    public void testCopyFromWithValidationSetToFalseStillValidatesIfGeneratedColumnsInvolved() throws Exception {\n        execute(\"create table t (a boolean, b int generated always as 1)\");\n\n        List<String> lines = List.of(\"{\\\"a\\\": \\\"\\\"}\");\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true, validation = false) return summary\",\n                new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(printedTable(response.rows())).contains(\"Cannot cast value `` to type `boolean`\");\n        execute(\"refresh table t\");\n        execute(\"select count(*) from t\");\n        assertThat(response.rows()[0][0]).isEqualTo(0L);\n    }\n\n    @Test\n    public void testCopyFromWithValidationSetToFalseAndInsertingToPartitionedByColumn() throws Exception {\n        // copying an empty string to a boolean column\n\n        execute(\"create table t (a boolean, b boolean) partitioned by (b)\");\n\n        List<String> lines = List.of(\"{\\\"b\\\": \\\"\\\"}\");\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true, validation = false) return summary\",\n                new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(printedTable(response.rows())).contains(\n            // The validation should be skipped but since it is partitioned by column, that is used to create shards,\n            // the values cannot stay raw. Notice that the error message is different from,\n            // Cannot cast value `` to type `boolean`\n            \"Can't convert \\\"\\\" to boolean={count=1, line_numbers=[1]}\"\n        );\n        execute(\"refresh table t\");\n        execute(\"select count(*) from t\");\n        assertThat(response.rows()[0][0]).isEqualTo(0L);\n    }\n\n    @Test\n    public void test_validation_set_to_false_has_no_effect_and_results_in_validation_errors() throws Exception {\n        // copying an empty string to a boolean column\n\n        execute(\"create table t (a boolean, b boolean) partitioned by (b)\");\n\n        List<String> lines = List.of(\"{\\\"a\\\": \\\"\\\"}\"); // a\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true, validation = false) return summary\",\n                new Object[]{Paths.get(file.toURI()).toUri().toString()});\n\n        assertThat(response.rows()[0][4].toString()).contains(\"Cannot cast value `` to type `boolean`\");\n    }\n\n    @Test\n    public void testCopyFromWithValidationSetToFalseStillValidatesIfDefaultExpressionsInvolved() throws Exception {\n        execute(\"create table t (a boolean, b int default 1)\");\n\n        List<String> lines = List.of(\"{\\\"a\\\": \\\"\\\"}\");\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true, validation = false) return summary\",\n                new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(printedTable(response.rows())).contains(\"Cannot cast value `` to type `boolean`\");\n        execute(\"refresh table t\");\n        execute(\"select count(*) from t\");\n        assertThat(response.rows()[0][0]).isEqualTo(0L);\n    }\n\n    @Test\n    public void test_copy_from_fail_gracefully_in_case_of_invalid_data() throws Exception {\n        execute(\"create table t (obj object(dynamic) as (x int))\");\n\n        // Checking out all variants from https://github.com/crate/crate/issues/12201#issuecomment-1072472337\n\n        // Intentionally \"sandwiching\" valid line between 2 invalids as there used to be test failure depending on the valid/invalid order.\n        List<String> lines = List.of(\n            \"obj\\n\",\n            \"1,2\\n\",\n            \"\\\"{\\\"\\\"x\\\"\\\":1}\\\"\\n\",   // \"{\"\"x\"\":1}\" - works\n            \"3,4\\n\",\n            \"\\\"{\\\"x\\\":1}\\\"\\n\",       // \"{\"x\":1}\"\n            \"\\\"'{\\\"\\\"x\\\"\\\":1}'\\\"\\n\", // \"'{\"\"x\"\":1}'\"\n            \"\\\"{\\\"x\\\" = 1}\\\"\\n\",     // \"{\"x\" = 1}\"\n            \"\\\"{\\\"\\\"x\\\"\\\" = 1}\\\"\\n\", // \"{\"\"x\"\" = 1}\"\n            \"{\\\"\\\"x\\\"\\\":1}\\n\",       // {\"\"x\"\":1}\n            \"{\\\"x\\\":1}\\n\",           // {\"x\":1} - works\n            \"{\\\"x\\\" = 1}\\n\",         // {\"x\" = 1}\n            \"{x = 1}\\n\"              // {x = 1}\n        );\n\n        File file = folder.newFile(UUID.randomUUID() + \".csv\");\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n        execute(\"copy t from ? with (shared = true) return summary\", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        assertThat(response.rows()[0][2]).isEqualTo(2L);\n        assertThat(response.rows()[0][3]).isEqualTo(9L);\n    }\n\n    @Test\n    public void test_copy_preserves_implied_top_level_column_order() throws IOException {\n        execute(\n            \"\"\"\n                create table t (\n                    p int\n                ) partitioned by (p) with (column_policy = 'dynamic');\n                \"\"\"\n        );\n        var lines = List.of(\n            \"\"\"\n            {\"b\":1, \"a\":1, \"d\":1, \"c\":1}\n            \"\"\"\n        );\n        var file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n        execute(\"copy t from ? \", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        execute(\"refresh table t\");\n        execute(\"select * from t\");\n        assertThat(response.cols())\n            // follow the same order as provided by '{\"b\":1, \"a\":1, \"d\":1, \"c\":1}'\n            .isEqualTo(new String[] {\"p\", \"b\", \"a\", \"d\", \"c\"});\n    }\n\n    @Test\n    public void test_copy_preserves_the_implied_sub_column_order() throws IOException {\n        execute(\n            \"\"\"\n                create table doc.t (\n                    p int,\n                    o object\n                ) partitioned by (p) with (column_policy = 'dynamic');\n                \"\"\"\n        );\n        var lines = List.of(\n            \"\"\"\n            {\"o\":{\"c\":1, \"a\":{\"d\":1, \"b\":1, \"c\":1, \"a\":1}, \"b\":1}}\n            \"\"\"\n        );\n        var file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n        execute(\"copy doc.t from ? \", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        execute(\"refresh table doc.t\");\n        execute(\"show create table doc.t\");\n        assertThat(printedTable(response.rows()))\n            // follow the same order as provided by '{\"o\":{\"c\":1, \"a\":{\"d\":1, \"b\":1, \"c\":1, \"a\":1}, \"b\":1}}'\n            .contains(\n                \"CREATE TABLE IF NOT EXISTS \\\"doc\\\".\\\"t\\\" (\\n\" +\n                \"   \\\"p\\\" INTEGER,\\n\" +\n                \"   \\\"o\\\" OBJECT(DYNAMIC) AS (\\n\" +\n                \"      \\\"c\\\" BIGINT,\\n\" +\n                \"      \\\"a\\\" OBJECT(DYNAMIC) AS (\\n\" +\n                \"         \\\"d\\\" BIGINT,\\n\" +\n                \"         \\\"b\\\" BIGINT,\\n\" +\n                \"         \\\"c\\\" BIGINT,\\n\" +\n                \"         \\\"a\\\" BIGINT\\n\" +\n                \"      ),\\n\" +\n                \"      \\\"b\\\" BIGINT\\n\" +\n                \"   )\\n\" +\n                \")\"\n            );\n    }\n\n    @Test\n    public void test_generated_non_deterministic_value_is_consistent_on_primary_and_replica() throws Exception {\n        execute(\"\"\"\n            create table tbl (x int, created generated always as round((random() + 1) * 100))\n            clustered into 1 shards\n            with (number_of_replicas = 1)\n            \"\"\"\n        );\n        Path path = tmpFileWithLines(Arrays.asList(\n            \"{\\\"x\\\": 1}\"\n        ));\n        execute(\"copy tbl from ? with (shared=true)\", new Object[] { path.toUri().toString() + \"*.json\"});\n\n        execute(\"refresh table tbl\");\n        execute(\"select x, created from tbl\");\n\n        // (int) response.rows()[0][0] used to be null because replica\n        // used regular Indexer instead of RawIndexer\n        // with values [\"{\\\"x\\\": 1}\"][some_long_timestamp], ie tried to write String value into int column.\n\n        int x = (int) response.rows()[0][0];\n        long created = (long) response.rows()[0][1];\n\n        // some iterations to ensure it hits both primary and replica\n        for (int i = 0; i < 30; i++) {\n            execute(\"select x, created from tbl\").rows();\n            assertThat(response.rows()[0][0]).isEqualTo(x);\n            assertThat(response.rows()[0][1]).isEqualTo(created);\n        }\n    }\n\n    @Test\n    public void test_copy_from_can_import_json_with_same_columns_in_different_order() throws Exception {\n        execute(\"\"\"\n            create table t (\n                 id long,\n                 first_column long,\n                 second_column string,\n                 third_column long,\n                 primary key (id)\n            )\n            \"\"\"\n        );\n\n        var lines = List.of(\n            \"\"\"\n                {\"id\":1,\"first_column\":38392,\"second_column\":\"apple safari\",\"third_column\":151155}\n                {\"id\":2,\"second_column\":\"apple safari\",\"third_column\":23073,\"first_column\":31123}\n            \"\"\"\n        );\n        File file = folder.newFile(UUID.randomUUID().toString());\n        Files.write(file.toPath(), lines, StandardCharsets.UTF_8);\n\n        execute(\"copy t from ? with (shared = true)\", new Object[]{Paths.get(file.toURI()).toUri().toString()});\n        execute(\"refresh table t\");\n        execute(\"select * from t order by id\");\n        assertThat(response).hasRows(\n            \"1| 38392| apple safari| 151155\",\n            \"2| 31123| apple safari| 23073\"\n        );\n    }\n\n    @UseRandomizedSchema(random = false)\n    @Test\n    public void test_copy_from_local_file_is_only_allowed_for_superusers() {\n        execute(\"CREATE TABLE quotes (id INT PRIMARY KEY, \" +\n            \"quote STRING INDEX USING FULLTEXT) WITH (number_of_replicas = 0)\");\n        execute(\"CREATE USER test_user\");\n        execute(\"GRANT ALL TO test_user\");\n\n        var roles = cluster().getInstance(Roles.class);\n        Role user = roles.findUser(\"test_user\");\n        Sessions sqlOperations = cluster().getInstance(Sessions.class);\n        try (var session = sqlOperations.newSession(null, user)) {\n            assertThatThrownBy(() -> execute(\"COPY quotes FROM ?\", new Object[]{copyFilePath + \"test_copy_from.json\"}, session))\n                .isExactlyInstanceOf(UnauthorizedException.class)\n                .hasMessage(\"Only a superuser can read from the local file system\");\n        }\n    }\n}\n"], "filenames": ["docs/appendices/release-notes/5.5.4.rst", "docs/appendices/release-notes/5.6.1.rst", "docs/sql/statements/copy-from.rst", "plugins/cr8-copy-s3/src/test/java/io/crate/copy/s3/S3FileReadingCollectorTest.java", "server/src/main/java/io/crate/exceptions/UnauthorizedException.java", "server/src/main/java/io/crate/execution/engine/collect/files/FileReadingIterator.java", "server/src/main/java/io/crate/execution/engine/collect/sources/FileCollectSource.java", "server/src/main/java/org/elasticsearch/ElasticsearchException.java", "server/src/test/java/io/crate/execution/engine/collect/MapSideDataCollectOperationTest.java", "server/src/test/java/io/crate/execution/engine/collect/files/FileReadingCollectorTest.java", "server/src/test/java/io/crate/execution/engine/collect/files/FileReadingIteratorTest.java", "server/src/test/java/io/crate/execution/engine/collect/sources/FileCollectSourceTest.java", "server/src/test/java/io/crate/integrationtests/CopyIntegrationTest.java"], "buggy_code_start_loc": [46, 45, 213, 109, 24, 178, 23, 976, 56, 203, 36, 95, 28], "buggy_code_end_loc": [46, 45, 213, 110, 29, 403, 90, 977, 73, 204, 217, 96, 1198], "fixing_code_start_loc": [47, 46, 214, 109, 24, 178, 24, 976, 57, 203, 37, 95, 29], "fixing_code_end_loc": [55, 53, 216, 110, 38, 402, 108, 982, 75, 204, 223, 97, 1223], "type": "CWE-22", "message": "CrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of data in real-time. There is a COPY FROM function in the CrateDB database that is used to import file data into database tables. This function has a flaw, and authenticated attackers can use the COPY FROM function to import arbitrary file content into database tables, resulting in information leakage. This vulnerability is patched in 5.3.9, 5.4.8, 5.5.4, and 5.6.1.", "other": {"cve": {"id": "CVE-2024-24565", "sourceIdentifier": "security-advisories@github.com", "published": "2024-01-30T17:15:12.110", "lastModified": "2024-02-05T20:55:23.270", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "CrateDB is a distributed SQL database that makes it simple to store and analyze massive amounts of data in real-time. There is a COPY FROM function in the CrateDB database that is used to import file data into database tables. This function has a flaw, and authenticated attackers can use the COPY FROM function to import arbitrary file content into database tables, resulting in information leakage. This vulnerability is patched in 5.3.9, 5.4.8, 5.5.4, and 5.6.1."}, {"lang": "es", "value": "CrateDB es una base de datos SQL distribuida que simplifica el almacenamiento y an\u00e1lisis de cantidades masivas de datos en tiempo real. Hay una funci\u00f3n COPIAR DESDE en la base de datos CrateDB que se utiliza para importar datos de archivos a tablas de bases de datos. Esta funci\u00f3n tiene un defecto y los atacantes autenticados pueden utilizar la funci\u00f3n COPIAR DE para importar contenido de archivos arbitrario en tablas de bases de datos, lo que provoca una fuga de informaci\u00f3n. Esta vulnerabilidad est\u00e1 parcheada en 5.3.9, 5.4.8, 5.5.4 y 5.6.1."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.8, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:R/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.1, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-22"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:cratedb:cratedb:*:*:*:*:*:*:*:*", "versionEndExcluding": "5.3.9", "matchCriteriaId": "FAF44B85-F769-493B-A674-7A8456A874F8"}, {"vulnerable": true, "criteria": "cpe:2.3:a:cratedb:cratedb:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.4.0", "versionEndExcluding": "5.4.8", "matchCriteriaId": "64B0DBA5-7EA3-4398-ACF8-D852EFA5C744"}, {"vulnerable": true, "criteria": "cpe:2.3:a:cratedb:cratedb:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.5.0", "versionEndExcluding": "5.5.4", "matchCriteriaId": "A4ABD3EE-2E41-4CF6-BC26-F947BB6499E2"}, {"vulnerable": true, "criteria": "cpe:2.3:a:cratedb:cratedb:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.6.0", "versionEndExcluding": "5.6.1", "matchCriteriaId": "80901C4B-6B82-40BF-BC22-DA206B95003C"}]}]}], "references": [{"url": "https://github.com/crate/crate/commit/4e857d675683095945dd524d6ba03e692c70ecd6", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/crate/crate/security/advisories/GHSA-475g-vj6c-xf96", "source": "security-advisories@github.com", "tags": ["Exploit", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/crate/crate/commit/4e857d675683095945dd524d6ba03e692c70ecd6"}}