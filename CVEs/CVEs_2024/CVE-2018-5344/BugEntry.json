{"buggy_code": ["/*\n *  linux/drivers/block/loop.c\n *\n *  Written by Theodore Ts'o, 3/29/93\n *\n * Copyright 1993 by Theodore Ts'o.  Redistribution of this file is\n * permitted under the GNU General Public License.\n *\n * DES encryption plus some minor changes by Werner Almesberger, 30-MAY-1993\n * more DES encryption plus IDEA encryption by Nicholas J. Leon, June 20, 1996\n *\n * Modularized and updated for 1.1.16 kernel - Mitch Dsouza 28th May 1994\n * Adapted for 1.3.59 kernel - Andries Brouwer, 1 Feb 1996\n *\n * Fixed do_loop_request() re-entrancy - Vincent.Renardias@waw.com Mar 20, 1997\n *\n * Added devfs support - Richard Gooch <rgooch@atnf.csiro.au> 16-Jan-1998\n *\n * Handle sparse backing files correctly - Kenn Humborg, Jun 28, 1998\n *\n * Loadable modules and other fixes by AK, 1998\n *\n * Make real block number available to downstream transfer functions, enables\n * CBC (and relatives) mode encryption requiring unique IVs per data block.\n * Reed H. Petty, rhp@draper.net\n *\n * Maximum number of loop devices now dynamic via max_loop module parameter.\n * Russell Kroll <rkroll@exploits.org> 19990701\n *\n * Maximum number of loop devices when compiled-in now selectable by passing\n * max_loop=<1-255> to the kernel on boot.\n * Erik I. Bols\u00f8, <eriki@himolde.no>, Oct 31, 1999\n *\n * Completely rewrite request handling to be make_request_fn style and\n * non blocking, pushing work to a helper thread. Lots of fixes from\n * Al Viro too.\n * Jens Axboe <axboe@suse.de>, Nov 2000\n *\n * Support up to 256 loop devices\n * Heinz Mauelshagen <mge@sistina.com>, Feb 2002\n *\n * Support for falling back on the write file operation when the address space\n * operations write_begin is not available on the backing filesystem.\n * Anton Altaparmakov, 16 Feb 2005\n *\n * Still To Fix:\n * - Advisory locking is ignored here.\n * - Should use an own CAP_* category instead of CAP_SYS_ADMIN\n *\n */\n\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/sched.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/stat.h>\n#include <linux/errno.h>\n#include <linux/major.h>\n#include <linux/wait.h>\n#include <linux/blkdev.h>\n#include <linux/blkpg.h>\n#include <linux/init.h>\n#include <linux/swap.h>\n#include <linux/slab.h>\n#include <linux/compat.h>\n#include <linux/suspend.h>\n#include <linux/freezer.h>\n#include <linux/mutex.h>\n#include <linux/writeback.h>\n#include <linux/completion.h>\n#include <linux/highmem.h>\n#include <linux/kthread.h>\n#include <linux/splice.h>\n#include <linux/sysfs.h>\n#include <linux/miscdevice.h>\n#include <linux/falloc.h>\n#include <linux/uio.h>\n#include \"loop.h\"\n\n#include <linux/uaccess.h>\n\nstatic DEFINE_IDR(loop_index_idr);\nstatic DEFINE_MUTEX(loop_index_mutex);\n\nstatic int max_part;\nstatic int part_shift;\n\nstatic int transfer_xor(struct loop_device *lo, int cmd,\n\t\t\tstruct page *raw_page, unsigned raw_off,\n\t\t\tstruct page *loop_page, unsigned loop_off,\n\t\t\tint size, sector_t real_block)\n{\n\tchar *raw_buf = kmap_atomic(raw_page) + raw_off;\n\tchar *loop_buf = kmap_atomic(loop_page) + loop_off;\n\tchar *in, *out, *key;\n\tint i, keysize;\n\n\tif (cmd == READ) {\n\t\tin = raw_buf;\n\t\tout = loop_buf;\n\t} else {\n\t\tin = loop_buf;\n\t\tout = raw_buf;\n\t}\n\n\tkey = lo->lo_encrypt_key;\n\tkeysize = lo->lo_encrypt_key_size;\n\tfor (i = 0; i < size; i++)\n\t\t*out++ = *in++ ^ key[(i & 511) % keysize];\n\n\tkunmap_atomic(loop_buf);\n\tkunmap_atomic(raw_buf);\n\tcond_resched();\n\treturn 0;\n}\n\nstatic int xor_init(struct loop_device *lo, const struct loop_info64 *info)\n{\n\tif (unlikely(info->lo_encrypt_key_size <= 0))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic struct loop_func_table none_funcs = {\n\t.number = LO_CRYPT_NONE,\n}; \n\nstatic struct loop_func_table xor_funcs = {\n\t.number = LO_CRYPT_XOR,\n\t.transfer = transfer_xor,\n\t.init = xor_init\n}; \n\n/* xfer_funcs[0] is special - its release function is never called */\nstatic struct loop_func_table *xfer_funcs[MAX_LO_CRYPT] = {\n\t&none_funcs,\n\t&xor_funcs\n};\n\nstatic loff_t get_size(loff_t offset, loff_t sizelimit, struct file *file)\n{\n\tloff_t loopsize;\n\n\t/* Compute loopsize in bytes */\n\tloopsize = i_size_read(file->f_mapping->host);\n\tif (offset > 0)\n\t\tloopsize -= offset;\n\t/* offset is beyond i_size, weird but possible */\n\tif (loopsize < 0)\n\t\treturn 0;\n\n\tif (sizelimit > 0 && sizelimit < loopsize)\n\t\tloopsize = sizelimit;\n\t/*\n\t * Unfortunately, if we want to do I/O on the device,\n\t * the number of 512-byte sectors has to fit into a sector_t.\n\t */\n\treturn loopsize >> 9;\n}\n\nstatic loff_t get_loop_size(struct loop_device *lo, struct file *file)\n{\n\treturn get_size(lo->lo_offset, lo->lo_sizelimit, file);\n}\n\nstatic void __loop_update_dio(struct loop_device *lo, bool dio)\n{\n\tstruct file *file = lo->lo_backing_file;\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tunsigned short sb_bsize = 0;\n\tunsigned dio_align = 0;\n\tbool use_dio;\n\n\tif (inode->i_sb->s_bdev) {\n\t\tsb_bsize = bdev_logical_block_size(inode->i_sb->s_bdev);\n\t\tdio_align = sb_bsize - 1;\n\t}\n\n\t/*\n\t * We support direct I/O only if lo_offset is aligned with the\n\t * logical I/O size of backing device, and the logical block\n\t * size of loop is bigger than the backing device's and the loop\n\t * needn't transform transfer.\n\t *\n\t * TODO: the above condition may be loosed in the future, and\n\t * direct I/O may be switched runtime at that time because most\n\t * of requests in sane applications should be PAGE_SIZE aligned\n\t */\n\tif (dio) {\n\t\tif (queue_logical_block_size(lo->lo_queue) >= sb_bsize &&\n\t\t\t\t!(lo->lo_offset & dio_align) &&\n\t\t\t\tmapping->a_ops->direct_IO &&\n\t\t\t\t!lo->transfer)\n\t\t\tuse_dio = true;\n\t\telse\n\t\t\tuse_dio = false;\n\t} else {\n\t\tuse_dio = false;\n\t}\n\n\tif (lo->use_dio == use_dio)\n\t\treturn;\n\n\t/* flush dirty pages before changing direct IO */\n\tvfs_fsync(file, 0);\n\n\t/*\n\t * The flag of LO_FLAGS_DIRECT_IO is handled similarly with\n\t * LO_FLAGS_READ_ONLY, both are set from kernel, and losetup\n\t * will get updated by ioctl(LOOP_GET_STATUS)\n\t */\n\tblk_mq_freeze_queue(lo->lo_queue);\n\tlo->use_dio = use_dio;\n\tif (use_dio) {\n\t\tqueue_flag_clear_unlocked(QUEUE_FLAG_NOMERGES, lo->lo_queue);\n\t\tlo->lo_flags |= LO_FLAGS_DIRECT_IO;\n\t} else {\n\t\tqueue_flag_set_unlocked(QUEUE_FLAG_NOMERGES, lo->lo_queue);\n\t\tlo->lo_flags &= ~LO_FLAGS_DIRECT_IO;\n\t}\n\tblk_mq_unfreeze_queue(lo->lo_queue);\n}\n\nstatic int\nfigure_loop_size(struct loop_device *lo, loff_t offset, loff_t sizelimit)\n{\n\tloff_t size = get_size(offset, sizelimit, lo->lo_backing_file);\n\tsector_t x = (sector_t)size;\n\tstruct block_device *bdev = lo->lo_device;\n\n\tif (unlikely((loff_t)x != size))\n\t\treturn -EFBIG;\n\tif (lo->lo_offset != offset)\n\t\tlo->lo_offset = offset;\n\tif (lo->lo_sizelimit != sizelimit)\n\t\tlo->lo_sizelimit = sizelimit;\n\tset_capacity(lo->lo_disk, x);\n\tbd_set_size(bdev, (loff_t)get_capacity(bdev->bd_disk) << 9);\n\t/* let user-space know about the new size */\n\tkobject_uevent(&disk_to_dev(bdev->bd_disk)->kobj, KOBJ_CHANGE);\n\treturn 0;\n}\n\nstatic inline int\nlo_do_transfer(struct loop_device *lo, int cmd,\n\t       struct page *rpage, unsigned roffs,\n\t       struct page *lpage, unsigned loffs,\n\t       int size, sector_t rblock)\n{\n\tint ret;\n\n\tret = lo->transfer(lo, cmd, rpage, roffs, lpage, loffs, size, rblock);\n\tif (likely(!ret))\n\t\treturn 0;\n\n\tprintk_ratelimited(KERN_ERR\n\t\t\"loop: Transfer error at byte offset %llu, length %i.\\n\",\n\t\t(unsigned long long)rblock << 9, size);\n\treturn ret;\n}\n\nstatic int lo_write_bvec(struct file *file, struct bio_vec *bvec, loff_t *ppos)\n{\n\tstruct iov_iter i;\n\tssize_t bw;\n\n\tiov_iter_bvec(&i, ITER_BVEC, bvec, 1, bvec->bv_len);\n\n\tfile_start_write(file);\n\tbw = vfs_iter_write(file, &i, ppos, 0);\n\tfile_end_write(file);\n\n\tif (likely(bw ==  bvec->bv_len))\n\t\treturn 0;\n\n\tprintk_ratelimited(KERN_ERR\n\t\t\"loop: Write error at byte offset %llu, length %i.\\n\",\n\t\t(unsigned long long)*ppos, bvec->bv_len);\n\tif (bw >= 0)\n\t\tbw = -EIO;\n\treturn bw;\n}\n\nstatic int lo_write_simple(struct loop_device *lo, struct request *rq,\n\t\tloff_t pos)\n{\n\tstruct bio_vec bvec;\n\tstruct req_iterator iter;\n\tint ret = 0;\n\n\trq_for_each_segment(bvec, rq, iter) {\n\t\tret = lo_write_bvec(lo->lo_backing_file, &bvec, &pos);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\n\treturn ret;\n}\n\n/*\n * This is the slow, transforming version that needs to double buffer the\n * data as it cannot do the transformations in place without having direct\n * access to the destination pages of the backing file.\n */\nstatic int lo_write_transfer(struct loop_device *lo, struct request *rq,\n\t\tloff_t pos)\n{\n\tstruct bio_vec bvec, b;\n\tstruct req_iterator iter;\n\tstruct page *page;\n\tint ret = 0;\n\n\tpage = alloc_page(GFP_NOIO);\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\n\trq_for_each_segment(bvec, rq, iter) {\n\t\tret = lo_do_transfer(lo, WRITE, page, 0, bvec.bv_page,\n\t\t\tbvec.bv_offset, bvec.bv_len, pos >> 9);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\n\t\tb.bv_page = page;\n\t\tb.bv_offset = 0;\n\t\tb.bv_len = bvec.bv_len;\n\t\tret = lo_write_bvec(lo->lo_backing_file, &b, &pos);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t}\n\n\t__free_page(page);\n\treturn ret;\n}\n\nstatic int lo_read_simple(struct loop_device *lo, struct request *rq,\n\t\tloff_t pos)\n{\n\tstruct bio_vec bvec;\n\tstruct req_iterator iter;\n\tstruct iov_iter i;\n\tssize_t len;\n\n\trq_for_each_segment(bvec, rq, iter) {\n\t\tiov_iter_bvec(&i, ITER_BVEC, &bvec, 1, bvec.bv_len);\n\t\tlen = vfs_iter_read(lo->lo_backing_file, &i, &pos, 0);\n\t\tif (len < 0)\n\t\t\treturn len;\n\n\t\tflush_dcache_page(bvec.bv_page);\n\n\t\tif (len != bvec.bv_len) {\n\t\t\tstruct bio *bio;\n\n\t\t\t__rq_for_each_bio(bio, rq)\n\t\t\t\tzero_fill_bio(bio);\n\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n}\n\nstatic int lo_read_transfer(struct loop_device *lo, struct request *rq,\n\t\tloff_t pos)\n{\n\tstruct bio_vec bvec, b;\n\tstruct req_iterator iter;\n\tstruct iov_iter i;\n\tstruct page *page;\n\tssize_t len;\n\tint ret = 0;\n\n\tpage = alloc_page(GFP_NOIO);\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\n\trq_for_each_segment(bvec, rq, iter) {\n\t\tloff_t offset = pos;\n\n\t\tb.bv_page = page;\n\t\tb.bv_offset = 0;\n\t\tb.bv_len = bvec.bv_len;\n\n\t\tiov_iter_bvec(&i, ITER_BVEC, &b, 1, b.bv_len);\n\t\tlen = vfs_iter_read(lo->lo_backing_file, &i, &pos, 0);\n\t\tif (len < 0) {\n\t\t\tret = len;\n\t\t\tgoto out_free_page;\n\t\t}\n\n\t\tret = lo_do_transfer(lo, READ, page, 0, bvec.bv_page,\n\t\t\tbvec.bv_offset, len, offset >> 9);\n\t\tif (ret)\n\t\t\tgoto out_free_page;\n\n\t\tflush_dcache_page(bvec.bv_page);\n\n\t\tif (len != bvec.bv_len) {\n\t\t\tstruct bio *bio;\n\n\t\t\t__rq_for_each_bio(bio, rq)\n\t\t\t\tzero_fill_bio(bio);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tret = 0;\nout_free_page:\n\t__free_page(page);\n\treturn ret;\n}\n\nstatic int lo_discard(struct loop_device *lo, struct request *rq, loff_t pos)\n{\n\t/*\n\t * We use punch hole to reclaim the free space used by the\n\t * image a.k.a. discard. However we do not support discard if\n\t * encryption is enabled, because it may give an attacker\n\t * useful information.\n\t */\n\tstruct file *file = lo->lo_backing_file;\n\tint mode = FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE;\n\tint ret;\n\n\tif ((!file->f_op->fallocate) || lo->lo_encrypt_key_size) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tret = file->f_op->fallocate(file, mode, pos, blk_rq_bytes(rq));\n\tif (unlikely(ret && ret != -EINVAL && ret != -EOPNOTSUPP))\n\t\tret = -EIO;\n out:\n\treturn ret;\n}\n\nstatic int lo_req_flush(struct loop_device *lo, struct request *rq)\n{\n\tstruct file *file = lo->lo_backing_file;\n\tint ret = vfs_fsync(file, 0);\n\tif (unlikely(ret && ret != -EINVAL))\n\t\tret = -EIO;\n\n\treturn ret;\n}\n\nstatic void lo_complete_rq(struct request *rq)\n{\n\tstruct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);\n\n\tif (unlikely(req_op(cmd->rq) == REQ_OP_READ && cmd->use_aio &&\n\t\t     cmd->ret >= 0 && cmd->ret < blk_rq_bytes(cmd->rq))) {\n\t\tstruct bio *bio = cmd->rq->bio;\n\n\t\tbio_advance(bio, cmd->ret);\n\t\tzero_fill_bio(bio);\n\t}\n\n\tblk_mq_end_request(rq, cmd->ret < 0 ? BLK_STS_IOERR : BLK_STS_OK);\n}\n\nstatic void lo_rw_aio_do_completion(struct loop_cmd *cmd)\n{\n\tif (!atomic_dec_and_test(&cmd->ref))\n\t\treturn;\n\tkfree(cmd->bvec);\n\tcmd->bvec = NULL;\n\tblk_mq_complete_request(cmd->rq);\n}\n\nstatic void lo_rw_aio_complete(struct kiocb *iocb, long ret, long ret2)\n{\n\tstruct loop_cmd *cmd = container_of(iocb, struct loop_cmd, iocb);\n\n\tif (cmd->css)\n\t\tcss_put(cmd->css);\n\tcmd->ret = ret;\n\tlo_rw_aio_do_completion(cmd);\n}\n\nstatic int lo_rw_aio(struct loop_device *lo, struct loop_cmd *cmd,\n\t\t     loff_t pos, bool rw)\n{\n\tstruct iov_iter iter;\n\tstruct bio_vec *bvec;\n\tstruct request *rq = cmd->rq;\n\tstruct bio *bio = rq->bio;\n\tstruct file *file = lo->lo_backing_file;\n\tunsigned int offset;\n\tint segments = 0;\n\tint ret;\n\n\tif (rq->bio != rq->biotail) {\n\t\tstruct req_iterator iter;\n\t\tstruct bio_vec tmp;\n\n\t\t__rq_for_each_bio(bio, rq)\n\t\t\tsegments += bio_segments(bio);\n\t\tbvec = kmalloc(sizeof(struct bio_vec) * segments, GFP_NOIO);\n\t\tif (!bvec)\n\t\t\treturn -EIO;\n\t\tcmd->bvec = bvec;\n\n\t\t/*\n\t\t * The bios of the request may be started from the middle of\n\t\t * the 'bvec' because of bio splitting, so we can't directly\n\t\t * copy bio->bi_iov_vec to new bvec. The rq_for_each_segment\n\t\t * API will take care of all details for us.\n\t\t */\n\t\trq_for_each_segment(tmp, rq, iter) {\n\t\t\t*bvec = tmp;\n\t\t\tbvec++;\n\t\t}\n\t\tbvec = cmd->bvec;\n\t\toffset = 0;\n\t} else {\n\t\t/*\n\t\t * Same here, this bio may be started from the middle of the\n\t\t * 'bvec' because of bio splitting, so offset from the bvec\n\t\t * must be passed to iov iterator\n\t\t */\n\t\toffset = bio->bi_iter.bi_bvec_done;\n\t\tbvec = __bvec_iter_bvec(bio->bi_io_vec, bio->bi_iter);\n\t\tsegments = bio_segments(bio);\n\t}\n\tatomic_set(&cmd->ref, 2);\n\n\tiov_iter_bvec(&iter, ITER_BVEC | rw, bvec,\n\t\t      segments, blk_rq_bytes(rq));\n\titer.iov_offset = offset;\n\n\tcmd->iocb.ki_pos = pos;\n\tcmd->iocb.ki_filp = file;\n\tcmd->iocb.ki_complete = lo_rw_aio_complete;\n\tcmd->iocb.ki_flags = IOCB_DIRECT;\n\tif (cmd->css)\n\t\tkthread_associate_blkcg(cmd->css);\n\n\tif (rw == WRITE)\n\t\tret = call_write_iter(file, &cmd->iocb, &iter);\n\telse\n\t\tret = call_read_iter(file, &cmd->iocb, &iter);\n\n\tlo_rw_aio_do_completion(cmd);\n\tkthread_associate_blkcg(NULL);\n\n\tif (ret != -EIOCBQUEUED)\n\t\tcmd->iocb.ki_complete(&cmd->iocb, ret, 0);\n\treturn 0;\n}\n\nstatic int do_req_filebacked(struct loop_device *lo, struct request *rq)\n{\n\tstruct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);\n\tloff_t pos = ((loff_t) blk_rq_pos(rq) << 9) + lo->lo_offset;\n\n\t/*\n\t * lo_write_simple and lo_read_simple should have been covered\n\t * by io submit style function like lo_rw_aio(), one blocker\n\t * is that lo_read_simple() need to call flush_dcache_page after\n\t * the page is written from kernel, and it isn't easy to handle\n\t * this in io submit style function which submits all segments\n\t * of the req at one time. And direct read IO doesn't need to\n\t * run flush_dcache_page().\n\t */\n\tswitch (req_op(rq)) {\n\tcase REQ_OP_FLUSH:\n\t\treturn lo_req_flush(lo, rq);\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\treturn lo_discard(lo, rq, pos);\n\tcase REQ_OP_WRITE:\n\t\tif (lo->transfer)\n\t\t\treturn lo_write_transfer(lo, rq, pos);\n\t\telse if (cmd->use_aio)\n\t\t\treturn lo_rw_aio(lo, cmd, pos, WRITE);\n\t\telse\n\t\t\treturn lo_write_simple(lo, rq, pos);\n\tcase REQ_OP_READ:\n\t\tif (lo->transfer)\n\t\t\treturn lo_read_transfer(lo, rq, pos);\n\t\telse if (cmd->use_aio)\n\t\t\treturn lo_rw_aio(lo, cmd, pos, READ);\n\t\telse\n\t\t\treturn lo_read_simple(lo, rq, pos);\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn -EIO;\n\t\tbreak;\n\t}\n}\n\nstatic inline void loop_update_dio(struct loop_device *lo)\n{\n\t__loop_update_dio(lo, io_is_direct(lo->lo_backing_file) |\n\t\t\tlo->use_dio);\n}\n\nstatic void loop_reread_partitions(struct loop_device *lo,\n\t\t\t\t   struct block_device *bdev)\n{\n\tint rc;\n\n\t/*\n\t * bd_mutex has been held already in release path, so don't\n\t * acquire it if this function is called in such case.\n\t *\n\t * If the reread partition isn't from release path, lo_refcnt\n\t * must be at least one and it can only become zero when the\n\t * current holder is released.\n\t */\n\tif (!atomic_read(&lo->lo_refcnt))\n\t\trc = __blkdev_reread_part(bdev);\n\telse\n\t\trc = blkdev_reread_part(bdev);\n\tif (rc)\n\t\tpr_warn(\"%s: partition scan of loop%d (%s) failed (rc=%d)\\n\",\n\t\t\t__func__, lo->lo_number, lo->lo_file_name, rc);\n}\n\n/*\n * loop_change_fd switched the backing store of a loopback device to\n * a new file. This is useful for operating system installers to free up\n * the original file and in High Availability environments to switch to\n * an alternative location for the content in case of server meltdown.\n * This can only work if the loop device is used read-only, and if the\n * new backing store is the same size and type as the old backing store.\n */\nstatic int loop_change_fd(struct loop_device *lo, struct block_device *bdev,\n\t\t\t  unsigned int arg)\n{\n\tstruct file\t*file, *old_file;\n\tstruct inode\t*inode;\n\tint\t\terror;\n\n\terror = -ENXIO;\n\tif (lo->lo_state != Lo_bound)\n\t\tgoto out;\n\n\t/* the loop device has to be read-only */\n\terror = -EINVAL;\n\tif (!(lo->lo_flags & LO_FLAGS_READ_ONLY))\n\t\tgoto out;\n\n\terror = -EBADF;\n\tfile = fget(arg);\n\tif (!file)\n\t\tgoto out;\n\n\tinode = file->f_mapping->host;\n\told_file = lo->lo_backing_file;\n\n\terror = -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode) && !S_ISBLK(inode->i_mode))\n\t\tgoto out_putf;\n\n\t/* size of the new backing store needs to be the same */\n\tif (get_loop_size(lo, file) != get_loop_size(lo, old_file))\n\t\tgoto out_putf;\n\n\t/* and ... switch */\n\tblk_mq_freeze_queue(lo->lo_queue);\n\tmapping_set_gfp_mask(old_file->f_mapping, lo->old_gfp_mask);\n\tlo->lo_backing_file = file;\n\tlo->old_gfp_mask = mapping_gfp_mask(file->f_mapping);\n\tmapping_set_gfp_mask(file->f_mapping,\n\t\t\t     lo->old_gfp_mask & ~(__GFP_IO|__GFP_FS));\n\tloop_update_dio(lo);\n\tblk_mq_unfreeze_queue(lo->lo_queue);\n\n\tfput(old_file);\n\tif (lo->lo_flags & LO_FLAGS_PARTSCAN)\n\t\tloop_reread_partitions(lo, bdev);\n\treturn 0;\n\n out_putf:\n\tfput(file);\n out:\n\treturn error;\n}\n\nstatic inline int is_loop_device(struct file *file)\n{\n\tstruct inode *i = file->f_mapping->host;\n\n\treturn i && S_ISBLK(i->i_mode) && MAJOR(i->i_rdev) == LOOP_MAJOR;\n}\n\n/* loop sysfs attributes */\n\nstatic ssize_t loop_attr_show(struct device *dev, char *page,\n\t\t\t      ssize_t (*callback)(struct loop_device *, char *))\n{\n\tstruct gendisk *disk = dev_to_disk(dev);\n\tstruct loop_device *lo = disk->private_data;\n\n\treturn callback(lo, page);\n}\n\n#define LOOP_ATTR_RO(_name)\t\t\t\t\t\t\\\nstatic ssize_t loop_attr_##_name##_show(struct loop_device *, char *);\t\\\nstatic ssize_t loop_attr_do_show_##_name(struct device *d,\t\t\\\n\t\t\t\tstruct device_attribute *attr, char *b)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn loop_attr_show(d, b, loop_attr_##_name##_show);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic struct device_attribute loop_attr_##_name =\t\t\t\\\n\t__ATTR(_name, S_IRUGO, loop_attr_do_show_##_name, NULL);\n\nstatic ssize_t loop_attr_backing_file_show(struct loop_device *lo, char *buf)\n{\n\tssize_t ret;\n\tchar *p = NULL;\n\n\tspin_lock_irq(&lo->lo_lock);\n\tif (lo->lo_backing_file)\n\t\tp = file_path(lo->lo_backing_file, buf, PAGE_SIZE - 1);\n\tspin_unlock_irq(&lo->lo_lock);\n\n\tif (IS_ERR_OR_NULL(p))\n\t\tret = PTR_ERR(p);\n\telse {\n\t\tret = strlen(p);\n\t\tmemmove(buf, p, ret);\n\t\tbuf[ret++] = '\\n';\n\t\tbuf[ret] = 0;\n\t}\n\n\treturn ret;\n}\n\nstatic ssize_t loop_attr_offset_show(struct loop_device *lo, char *buf)\n{\n\treturn sprintf(buf, \"%llu\\n\", (unsigned long long)lo->lo_offset);\n}\n\nstatic ssize_t loop_attr_sizelimit_show(struct loop_device *lo, char *buf)\n{\n\treturn sprintf(buf, \"%llu\\n\", (unsigned long long)lo->lo_sizelimit);\n}\n\nstatic ssize_t loop_attr_autoclear_show(struct loop_device *lo, char *buf)\n{\n\tint autoclear = (lo->lo_flags & LO_FLAGS_AUTOCLEAR);\n\n\treturn sprintf(buf, \"%s\\n\", autoclear ? \"1\" : \"0\");\n}\n\nstatic ssize_t loop_attr_partscan_show(struct loop_device *lo, char *buf)\n{\n\tint partscan = (lo->lo_flags & LO_FLAGS_PARTSCAN);\n\n\treturn sprintf(buf, \"%s\\n\", partscan ? \"1\" : \"0\");\n}\n\nstatic ssize_t loop_attr_dio_show(struct loop_device *lo, char *buf)\n{\n\tint dio = (lo->lo_flags & LO_FLAGS_DIRECT_IO);\n\n\treturn sprintf(buf, \"%s\\n\", dio ? \"1\" : \"0\");\n}\n\nLOOP_ATTR_RO(backing_file);\nLOOP_ATTR_RO(offset);\nLOOP_ATTR_RO(sizelimit);\nLOOP_ATTR_RO(autoclear);\nLOOP_ATTR_RO(partscan);\nLOOP_ATTR_RO(dio);\n\nstatic struct attribute *loop_attrs[] = {\n\t&loop_attr_backing_file.attr,\n\t&loop_attr_offset.attr,\n\t&loop_attr_sizelimit.attr,\n\t&loop_attr_autoclear.attr,\n\t&loop_attr_partscan.attr,\n\t&loop_attr_dio.attr,\n\tNULL,\n};\n\nstatic struct attribute_group loop_attribute_group = {\n\t.name = \"loop\",\n\t.attrs= loop_attrs,\n};\n\nstatic int loop_sysfs_init(struct loop_device *lo)\n{\n\treturn sysfs_create_group(&disk_to_dev(lo->lo_disk)->kobj,\n\t\t\t\t  &loop_attribute_group);\n}\n\nstatic void loop_sysfs_exit(struct loop_device *lo)\n{\n\tsysfs_remove_group(&disk_to_dev(lo->lo_disk)->kobj,\n\t\t\t   &loop_attribute_group);\n}\n\nstatic void loop_config_discard(struct loop_device *lo)\n{\n\tstruct file *file = lo->lo_backing_file;\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct request_queue *q = lo->lo_queue;\n\n\t/*\n\t * We use punch hole to reclaim the free space used by the\n\t * image a.k.a. discard. However we do not support discard if\n\t * encryption is enabled, because it may give an attacker\n\t * useful information.\n\t */\n\tif ((!file->f_op->fallocate) ||\n\t    lo->lo_encrypt_key_size) {\n\t\tq->limits.discard_granularity = 0;\n\t\tq->limits.discard_alignment = 0;\n\t\tblk_queue_max_discard_sectors(q, 0);\n\t\tblk_queue_max_write_zeroes_sectors(q, 0);\n\t\tqueue_flag_clear_unlocked(QUEUE_FLAG_DISCARD, q);\n\t\treturn;\n\t}\n\n\tq->limits.discard_granularity = inode->i_sb->s_blocksize;\n\tq->limits.discard_alignment = 0;\n\n\tblk_queue_max_discard_sectors(q, UINT_MAX >> 9);\n\tblk_queue_max_write_zeroes_sectors(q, UINT_MAX >> 9);\n\tqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);\n}\n\nstatic void loop_unprepare_queue(struct loop_device *lo)\n{\n\tkthread_flush_worker(&lo->worker);\n\tkthread_stop(lo->worker_task);\n}\n\nstatic int loop_kthread_worker_fn(void *worker_ptr)\n{\n\tcurrent->flags |= PF_LESS_THROTTLE;\n\treturn kthread_worker_fn(worker_ptr);\n}\n\nstatic int loop_prepare_queue(struct loop_device *lo)\n{\n\tkthread_init_worker(&lo->worker);\n\tlo->worker_task = kthread_run(loop_kthread_worker_fn,\n\t\t\t&lo->worker, \"loop%d\", lo->lo_number);\n\tif (IS_ERR(lo->worker_task))\n\t\treturn -ENOMEM;\n\tset_user_nice(lo->worker_task, MIN_NICE);\n\treturn 0;\n}\n\nstatic int loop_set_fd(struct loop_device *lo, fmode_t mode,\n\t\t       struct block_device *bdev, unsigned int arg)\n{\n\tstruct file\t*file, *f;\n\tstruct inode\t*inode;\n\tstruct address_space *mapping;\n\tint\t\tlo_flags = 0;\n\tint\t\terror;\n\tloff_t\t\tsize;\n\n\t/* This is safe, since we have a reference from open(). */\n\t__module_get(THIS_MODULE);\n\n\terror = -EBADF;\n\tfile = fget(arg);\n\tif (!file)\n\t\tgoto out;\n\n\terror = -EBUSY;\n\tif (lo->lo_state != Lo_unbound)\n\t\tgoto out_putf;\n\n\t/* Avoid recursion */\n\tf = file;\n\twhile (is_loop_device(f)) {\n\t\tstruct loop_device *l;\n\n\t\tif (f->f_mapping->host->i_bdev == bdev)\n\t\t\tgoto out_putf;\n\n\t\tl = f->f_mapping->host->i_bdev->bd_disk->private_data;\n\t\tif (l->lo_state == Lo_unbound) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_putf;\n\t\t}\n\t\tf = l->lo_backing_file;\n\t}\n\n\tmapping = file->f_mapping;\n\tinode = mapping->host;\n\n\terror = -EINVAL;\n\tif (!S_ISREG(inode->i_mode) && !S_ISBLK(inode->i_mode))\n\t\tgoto out_putf;\n\n\tif (!(file->f_mode & FMODE_WRITE) || !(mode & FMODE_WRITE) ||\n\t    !file->f_op->write_iter)\n\t\tlo_flags |= LO_FLAGS_READ_ONLY;\n\n\terror = -EFBIG;\n\tsize = get_loop_size(lo, file);\n\tif ((loff_t)(sector_t)size != size)\n\t\tgoto out_putf;\n\terror = loop_prepare_queue(lo);\n\tif (error)\n\t\tgoto out_putf;\n\n\terror = 0;\n\n\tset_device_ro(bdev, (lo_flags & LO_FLAGS_READ_ONLY) != 0);\n\n\tlo->use_dio = false;\n\tlo->lo_device = bdev;\n\tlo->lo_flags = lo_flags;\n\tlo->lo_backing_file = file;\n\tlo->transfer = NULL;\n\tlo->ioctl = NULL;\n\tlo->lo_sizelimit = 0;\n\tlo->old_gfp_mask = mapping_gfp_mask(mapping);\n\tmapping_set_gfp_mask(mapping, lo->old_gfp_mask & ~(__GFP_IO|__GFP_FS));\n\n\tif (!(lo_flags & LO_FLAGS_READ_ONLY) && file->f_op->fsync)\n\t\tblk_queue_write_cache(lo->lo_queue, true, false);\n\n\tloop_update_dio(lo);\n\tset_capacity(lo->lo_disk, size);\n\tbd_set_size(bdev, size << 9);\n\tloop_sysfs_init(lo);\n\t/* let user-space know about the new size */\n\tkobject_uevent(&disk_to_dev(bdev->bd_disk)->kobj, KOBJ_CHANGE);\n\n\tset_blocksize(bdev, S_ISBLK(inode->i_mode) ?\n\t\t      block_size(inode->i_bdev) : PAGE_SIZE);\n\n\tlo->lo_state = Lo_bound;\n\tif (part_shift)\n\t\tlo->lo_flags |= LO_FLAGS_PARTSCAN;\n\tif (lo->lo_flags & LO_FLAGS_PARTSCAN)\n\t\tloop_reread_partitions(lo, bdev);\n\n\t/* Grab the block_device to prevent its destruction after we\n\t * put /dev/loopXX inode. Later in loop_clr_fd() we bdput(bdev).\n\t */\n\tbdgrab(bdev);\n\treturn 0;\n\n out_putf:\n\tfput(file);\n out:\n\t/* This is safe: open() is still holding a reference. */\n\tmodule_put(THIS_MODULE);\n\treturn error;\n}\n\nstatic int\nloop_release_xfer(struct loop_device *lo)\n{\n\tint err = 0;\n\tstruct loop_func_table *xfer = lo->lo_encryption;\n\n\tif (xfer) {\n\t\tif (xfer->release)\n\t\t\terr = xfer->release(lo);\n\t\tlo->transfer = NULL;\n\t\tlo->lo_encryption = NULL;\n\t\tmodule_put(xfer->owner);\n\t}\n\treturn err;\n}\n\nstatic int\nloop_init_xfer(struct loop_device *lo, struct loop_func_table *xfer,\n\t       const struct loop_info64 *i)\n{\n\tint err = 0;\n\n\tif (xfer) {\n\t\tstruct module *owner = xfer->owner;\n\n\t\tif (!try_module_get(owner))\n\t\t\treturn -EINVAL;\n\t\tif (xfer->init)\n\t\t\terr = xfer->init(lo, i);\n\t\tif (err)\n\t\t\tmodule_put(owner);\n\t\telse\n\t\t\tlo->lo_encryption = xfer;\n\t}\n\treturn err;\n}\n\nstatic int loop_clr_fd(struct loop_device *lo)\n{\n\tstruct file *filp = lo->lo_backing_file;\n\tgfp_t gfp = lo->old_gfp_mask;\n\tstruct block_device *bdev = lo->lo_device;\n\n\tif (lo->lo_state != Lo_bound)\n\t\treturn -ENXIO;\n\n\t/*\n\t * If we've explicitly asked to tear down the loop device,\n\t * and it has an elevated reference count, set it for auto-teardown when\n\t * the last reference goes away. This stops $!~#$@ udev from\n\t * preventing teardown because it decided that it needs to run blkid on\n\t * the loopback device whenever they appear. xfstests is notorious for\n\t * failing tests because blkid via udev races with a losetup\n\t * <dev>/do something like mkfs/losetup -d <dev> causing the losetup -d\n\t * command to fail with EBUSY.\n\t */\n\tif (atomic_read(&lo->lo_refcnt) > 1) {\n\t\tlo->lo_flags |= LO_FLAGS_AUTOCLEAR;\n\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\treturn 0;\n\t}\n\n\tif (filp == NULL)\n\t\treturn -EINVAL;\n\n\t/* freeze request queue during the transition */\n\tblk_mq_freeze_queue(lo->lo_queue);\n\n\tspin_lock_irq(&lo->lo_lock);\n\tlo->lo_state = Lo_rundown;\n\tlo->lo_backing_file = NULL;\n\tspin_unlock_irq(&lo->lo_lock);\n\n\tloop_release_xfer(lo);\n\tlo->transfer = NULL;\n\tlo->ioctl = NULL;\n\tlo->lo_device = NULL;\n\tlo->lo_encryption = NULL;\n\tlo->lo_offset = 0;\n\tlo->lo_sizelimit = 0;\n\tlo->lo_encrypt_key_size = 0;\n\tmemset(lo->lo_encrypt_key, 0, LO_KEY_SIZE);\n\tmemset(lo->lo_crypt_name, 0, LO_NAME_SIZE);\n\tmemset(lo->lo_file_name, 0, LO_NAME_SIZE);\n\tblk_queue_logical_block_size(lo->lo_queue, 512);\n\tblk_queue_physical_block_size(lo->lo_queue, 512);\n\tblk_queue_io_min(lo->lo_queue, 512);\n\tif (bdev) {\n\t\tbdput(bdev);\n\t\tinvalidate_bdev(bdev);\n\t}\n\tset_capacity(lo->lo_disk, 0);\n\tloop_sysfs_exit(lo);\n\tif (bdev) {\n\t\tbd_set_size(bdev, 0);\n\t\t/* let user-space know about this change */\n\t\tkobject_uevent(&disk_to_dev(bdev->bd_disk)->kobj, KOBJ_CHANGE);\n\t}\n\tmapping_set_gfp_mask(filp->f_mapping, gfp);\n\tlo->lo_state = Lo_unbound;\n\t/* This is safe: open() is still holding a reference. */\n\tmodule_put(THIS_MODULE);\n\tblk_mq_unfreeze_queue(lo->lo_queue);\n\n\tif (lo->lo_flags & LO_FLAGS_PARTSCAN && bdev)\n\t\tloop_reread_partitions(lo, bdev);\n\tlo->lo_flags = 0;\n\tif (!part_shift)\n\t\tlo->lo_disk->flags |= GENHD_FL_NO_PART_SCAN;\n\tloop_unprepare_queue(lo);\n\tmutex_unlock(&lo->lo_ctl_mutex);\n\t/*\n\t * Need not hold lo_ctl_mutex to fput backing file.\n\t * Calling fput holding lo_ctl_mutex triggers a circular\n\t * lock dependency possibility warning as fput can take\n\t * bd_mutex which is usually taken before lo_ctl_mutex.\n\t */\n\tfput(filp);\n\treturn 0;\n}\n\nstatic int\nloop_set_status(struct loop_device *lo, const struct loop_info64 *info)\n{\n\tint err;\n\tstruct loop_func_table *xfer;\n\tkuid_t uid = current_uid();\n\n\tif (lo->lo_encrypt_key_size &&\n\t    !uid_eq(lo->lo_key_owner, uid) &&\n\t    !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\tif (lo->lo_state != Lo_bound)\n\t\treturn -ENXIO;\n\tif ((unsigned int) info->lo_encrypt_key_size > LO_KEY_SIZE)\n\t\treturn -EINVAL;\n\n\t/* I/O need to be drained during transfer transition */\n\tblk_mq_freeze_queue(lo->lo_queue);\n\n\terr = loop_release_xfer(lo);\n\tif (err)\n\t\tgoto exit;\n\n\tif (info->lo_encrypt_type) {\n\t\tunsigned int type = info->lo_encrypt_type;\n\n\t\tif (type >= MAX_LO_CRYPT)\n\t\t\treturn -EINVAL;\n\t\txfer = xfer_funcs[type];\n\t\tif (xfer == NULL)\n\t\t\treturn -EINVAL;\n\t} else\n\t\txfer = NULL;\n\n\terr = loop_init_xfer(lo, xfer, info);\n\tif (err)\n\t\tgoto exit;\n\n\tif (lo->lo_offset != info->lo_offset ||\n\t    lo->lo_sizelimit != info->lo_sizelimit) {\n\t\tif (figure_loop_size(lo, info->lo_offset, info->lo_sizelimit)) {\n\t\t\terr = -EFBIG;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\n\tloop_config_discard(lo);\n\n\tmemcpy(lo->lo_file_name, info->lo_file_name, LO_NAME_SIZE);\n\tmemcpy(lo->lo_crypt_name, info->lo_crypt_name, LO_NAME_SIZE);\n\tlo->lo_file_name[LO_NAME_SIZE-1] = 0;\n\tlo->lo_crypt_name[LO_NAME_SIZE-1] = 0;\n\n\tif (!xfer)\n\t\txfer = &none_funcs;\n\tlo->transfer = xfer->transfer;\n\tlo->ioctl = xfer->ioctl;\n\n\tif ((lo->lo_flags & LO_FLAGS_AUTOCLEAR) !=\n\t     (info->lo_flags & LO_FLAGS_AUTOCLEAR))\n\t\tlo->lo_flags ^= LO_FLAGS_AUTOCLEAR;\n\n\tlo->lo_encrypt_key_size = info->lo_encrypt_key_size;\n\tlo->lo_init[0] = info->lo_init[0];\n\tlo->lo_init[1] = info->lo_init[1];\n\tif (info->lo_encrypt_key_size) {\n\t\tmemcpy(lo->lo_encrypt_key, info->lo_encrypt_key,\n\t\t       info->lo_encrypt_key_size);\n\t\tlo->lo_key_owner = uid;\n\t}\n\n\t/* update dio if lo_offset or transfer is changed */\n\t__loop_update_dio(lo, lo->use_dio);\n\n exit:\n\tblk_mq_unfreeze_queue(lo->lo_queue);\n\n\tif (!err && (info->lo_flags & LO_FLAGS_PARTSCAN) &&\n\t     !(lo->lo_flags & LO_FLAGS_PARTSCAN)) {\n\t\tlo->lo_flags |= LO_FLAGS_PARTSCAN;\n\t\tlo->lo_disk->flags &= ~GENHD_FL_NO_PART_SCAN;\n\t\tloop_reread_partitions(lo, lo->lo_device);\n\t}\n\n\treturn err;\n}\n\nstatic int\nloop_get_status(struct loop_device *lo, struct loop_info64 *info)\n{\n\tstruct file *file = lo->lo_backing_file;\n\tstruct kstat stat;\n\tint error;\n\n\tif (lo->lo_state != Lo_bound)\n\t\treturn -ENXIO;\n\terror = vfs_getattr(&file->f_path, &stat,\n\t\t\t    STATX_INO, AT_STATX_SYNC_AS_STAT);\n\tif (error)\n\t\treturn error;\n\tmemset(info, 0, sizeof(*info));\n\tinfo->lo_number = lo->lo_number;\n\tinfo->lo_device = huge_encode_dev(stat.dev);\n\tinfo->lo_inode = stat.ino;\n\tinfo->lo_rdevice = huge_encode_dev(lo->lo_device ? stat.rdev : stat.dev);\n\tinfo->lo_offset = lo->lo_offset;\n\tinfo->lo_sizelimit = lo->lo_sizelimit;\n\tinfo->lo_flags = lo->lo_flags;\n\tmemcpy(info->lo_file_name, lo->lo_file_name, LO_NAME_SIZE);\n\tmemcpy(info->lo_crypt_name, lo->lo_crypt_name, LO_NAME_SIZE);\n\tinfo->lo_encrypt_type =\n\t\tlo->lo_encryption ? lo->lo_encryption->number : 0;\n\tif (lo->lo_encrypt_key_size && capable(CAP_SYS_ADMIN)) {\n\t\tinfo->lo_encrypt_key_size = lo->lo_encrypt_key_size;\n\t\tmemcpy(info->lo_encrypt_key, lo->lo_encrypt_key,\n\t\t       lo->lo_encrypt_key_size);\n\t}\n\treturn 0;\n}\n\nstatic void\nloop_info64_from_old(const struct loop_info *info, struct loop_info64 *info64)\n{\n\tmemset(info64, 0, sizeof(*info64));\n\tinfo64->lo_number = info->lo_number;\n\tinfo64->lo_device = info->lo_device;\n\tinfo64->lo_inode = info->lo_inode;\n\tinfo64->lo_rdevice = info->lo_rdevice;\n\tinfo64->lo_offset = info->lo_offset;\n\tinfo64->lo_sizelimit = 0;\n\tinfo64->lo_encrypt_type = info->lo_encrypt_type;\n\tinfo64->lo_encrypt_key_size = info->lo_encrypt_key_size;\n\tinfo64->lo_flags = info->lo_flags;\n\tinfo64->lo_init[0] = info->lo_init[0];\n\tinfo64->lo_init[1] = info->lo_init[1];\n\tif (info->lo_encrypt_type == LO_CRYPT_CRYPTOAPI)\n\t\tmemcpy(info64->lo_crypt_name, info->lo_name, LO_NAME_SIZE);\n\telse\n\t\tmemcpy(info64->lo_file_name, info->lo_name, LO_NAME_SIZE);\n\tmemcpy(info64->lo_encrypt_key, info->lo_encrypt_key, LO_KEY_SIZE);\n}\n\nstatic int\nloop_info64_to_old(const struct loop_info64 *info64, struct loop_info *info)\n{\n\tmemset(info, 0, sizeof(*info));\n\tinfo->lo_number = info64->lo_number;\n\tinfo->lo_device = info64->lo_device;\n\tinfo->lo_inode = info64->lo_inode;\n\tinfo->lo_rdevice = info64->lo_rdevice;\n\tinfo->lo_offset = info64->lo_offset;\n\tinfo->lo_encrypt_type = info64->lo_encrypt_type;\n\tinfo->lo_encrypt_key_size = info64->lo_encrypt_key_size;\n\tinfo->lo_flags = info64->lo_flags;\n\tinfo->lo_init[0] = info64->lo_init[0];\n\tinfo->lo_init[1] = info64->lo_init[1];\n\tif (info->lo_encrypt_type == LO_CRYPT_CRYPTOAPI)\n\t\tmemcpy(info->lo_name, info64->lo_crypt_name, LO_NAME_SIZE);\n\telse\n\t\tmemcpy(info->lo_name, info64->lo_file_name, LO_NAME_SIZE);\n\tmemcpy(info->lo_encrypt_key, info64->lo_encrypt_key, LO_KEY_SIZE);\n\n\t/* error in case values were truncated */\n\tif (info->lo_device != info64->lo_device ||\n\t    info->lo_rdevice != info64->lo_rdevice ||\n\t    info->lo_inode != info64->lo_inode ||\n\t    info->lo_offset != info64->lo_offset)\n\t\treturn -EOVERFLOW;\n\n\treturn 0;\n}\n\nstatic int\nloop_set_status_old(struct loop_device *lo, const struct loop_info __user *arg)\n{\n\tstruct loop_info info;\n\tstruct loop_info64 info64;\n\n\tif (copy_from_user(&info, arg, sizeof (struct loop_info)))\n\t\treturn -EFAULT;\n\tloop_info64_from_old(&info, &info64);\n\treturn loop_set_status(lo, &info64);\n}\n\nstatic int\nloop_set_status64(struct loop_device *lo, const struct loop_info64 __user *arg)\n{\n\tstruct loop_info64 info64;\n\n\tif (copy_from_user(&info64, arg, sizeof (struct loop_info64)))\n\t\treturn -EFAULT;\n\treturn loop_set_status(lo, &info64);\n}\n\nstatic int\nloop_get_status_old(struct loop_device *lo, struct loop_info __user *arg) {\n\tstruct loop_info info;\n\tstruct loop_info64 info64;\n\tint err = 0;\n\n\tif (!arg)\n\t\terr = -EINVAL;\n\tif (!err)\n\t\terr = loop_get_status(lo, &info64);\n\tif (!err)\n\t\terr = loop_info64_to_old(&info64, &info);\n\tif (!err && copy_to_user(arg, &info, sizeof(info)))\n\t\terr = -EFAULT;\n\n\treturn err;\n}\n\nstatic int\nloop_get_status64(struct loop_device *lo, struct loop_info64 __user *arg) {\n\tstruct loop_info64 info64;\n\tint err = 0;\n\n\tif (!arg)\n\t\terr = -EINVAL;\n\tif (!err)\n\t\terr = loop_get_status(lo, &info64);\n\tif (!err && copy_to_user(arg, &info64, sizeof(info64)))\n\t\terr = -EFAULT;\n\n\treturn err;\n}\n\nstatic int loop_set_capacity(struct loop_device *lo)\n{\n\tif (unlikely(lo->lo_state != Lo_bound))\n\t\treturn -ENXIO;\n\n\treturn figure_loop_size(lo, lo->lo_offset, lo->lo_sizelimit);\n}\n\nstatic int loop_set_dio(struct loop_device *lo, unsigned long arg)\n{\n\tint error = -ENXIO;\n\tif (lo->lo_state != Lo_bound)\n\t\tgoto out;\n\n\t__loop_update_dio(lo, !!arg);\n\tif (lo->use_dio == !!arg)\n\t\treturn 0;\n\terror = -EINVAL;\n out:\n\treturn error;\n}\n\nstatic int loop_set_block_size(struct loop_device *lo, unsigned long arg)\n{\n\tif (lo->lo_state != Lo_bound)\n\t\treturn -ENXIO;\n\n\tif (arg < 512 || arg > PAGE_SIZE || !is_power_of_2(arg))\n\t\treturn -EINVAL;\n\n\tblk_mq_freeze_queue(lo->lo_queue);\n\n\tblk_queue_logical_block_size(lo->lo_queue, arg);\n\tblk_queue_physical_block_size(lo->lo_queue, arg);\n\tblk_queue_io_min(lo->lo_queue, arg);\n\tloop_update_dio(lo);\n\n\tblk_mq_unfreeze_queue(lo->lo_queue);\n\n\treturn 0;\n}\n\nstatic int lo_ioctl(struct block_device *bdev, fmode_t mode,\n\tunsigned int cmd, unsigned long arg)\n{\n\tstruct loop_device *lo = bdev->bd_disk->private_data;\n\tint err;\n\n\tmutex_lock_nested(&lo->lo_ctl_mutex, 1);\n\tswitch (cmd) {\n\tcase LOOP_SET_FD:\n\t\terr = loop_set_fd(lo, mode, bdev, arg);\n\t\tbreak;\n\tcase LOOP_CHANGE_FD:\n\t\terr = loop_change_fd(lo, bdev, arg);\n\t\tbreak;\n\tcase LOOP_CLR_FD:\n\t\t/* loop_clr_fd would have unlocked lo_ctl_mutex on success */\n\t\terr = loop_clr_fd(lo);\n\t\tif (!err)\n\t\t\tgoto out_unlocked;\n\t\tbreak;\n\tcase LOOP_SET_STATUS:\n\t\terr = -EPERM;\n\t\tif ((mode & FMODE_WRITE) || capable(CAP_SYS_ADMIN))\n\t\t\terr = loop_set_status_old(lo,\n\t\t\t\t\t(struct loop_info __user *)arg);\n\t\tbreak;\n\tcase LOOP_GET_STATUS:\n\t\terr = loop_get_status_old(lo, (struct loop_info __user *) arg);\n\t\tbreak;\n\tcase LOOP_SET_STATUS64:\n\t\terr = -EPERM;\n\t\tif ((mode & FMODE_WRITE) || capable(CAP_SYS_ADMIN))\n\t\t\terr = loop_set_status64(lo,\n\t\t\t\t\t(struct loop_info64 __user *) arg);\n\t\tbreak;\n\tcase LOOP_GET_STATUS64:\n\t\terr = loop_get_status64(lo, (struct loop_info64 __user *) arg);\n\t\tbreak;\n\tcase LOOP_SET_CAPACITY:\n\t\terr = -EPERM;\n\t\tif ((mode & FMODE_WRITE) || capable(CAP_SYS_ADMIN))\n\t\t\terr = loop_set_capacity(lo);\n\t\tbreak;\n\tcase LOOP_SET_DIRECT_IO:\n\t\terr = -EPERM;\n\t\tif ((mode & FMODE_WRITE) || capable(CAP_SYS_ADMIN))\n\t\t\terr = loop_set_dio(lo, arg);\n\t\tbreak;\n\tcase LOOP_SET_BLOCK_SIZE:\n\t\terr = -EPERM;\n\t\tif ((mode & FMODE_WRITE) || capable(CAP_SYS_ADMIN))\n\t\t\terr = loop_set_block_size(lo, arg);\n\t\tbreak;\n\tdefault:\n\t\terr = lo->ioctl ? lo->ioctl(lo, cmd, arg) : -EINVAL;\n\t}\n\tmutex_unlock(&lo->lo_ctl_mutex);\n\nout_unlocked:\n\treturn err;\n}\n\n#ifdef CONFIG_COMPAT\nstruct compat_loop_info {\n\tcompat_int_t\tlo_number;      /* ioctl r/o */\n\tcompat_dev_t\tlo_device;      /* ioctl r/o */\n\tcompat_ulong_t\tlo_inode;       /* ioctl r/o */\n\tcompat_dev_t\tlo_rdevice;     /* ioctl r/o */\n\tcompat_int_t\tlo_offset;\n\tcompat_int_t\tlo_encrypt_type;\n\tcompat_int_t\tlo_encrypt_key_size;    /* ioctl w/o */\n\tcompat_int_t\tlo_flags;       /* ioctl r/o */\n\tchar\t\tlo_name[LO_NAME_SIZE];\n\tunsigned char\tlo_encrypt_key[LO_KEY_SIZE]; /* ioctl w/o */\n\tcompat_ulong_t\tlo_init[2];\n\tchar\t\treserved[4];\n};\n\n/*\n * Transfer 32-bit compatibility structure in userspace to 64-bit loop info\n * - noinlined to reduce stack space usage in main part of driver\n */\nstatic noinline int\nloop_info64_from_compat(const struct compat_loop_info __user *arg,\n\t\t\tstruct loop_info64 *info64)\n{\n\tstruct compat_loop_info info;\n\n\tif (copy_from_user(&info, arg, sizeof(info)))\n\t\treturn -EFAULT;\n\n\tmemset(info64, 0, sizeof(*info64));\n\tinfo64->lo_number = info.lo_number;\n\tinfo64->lo_device = info.lo_device;\n\tinfo64->lo_inode = info.lo_inode;\n\tinfo64->lo_rdevice = info.lo_rdevice;\n\tinfo64->lo_offset = info.lo_offset;\n\tinfo64->lo_sizelimit = 0;\n\tinfo64->lo_encrypt_type = info.lo_encrypt_type;\n\tinfo64->lo_encrypt_key_size = info.lo_encrypt_key_size;\n\tinfo64->lo_flags = info.lo_flags;\n\tinfo64->lo_init[0] = info.lo_init[0];\n\tinfo64->lo_init[1] = info.lo_init[1];\n\tif (info.lo_encrypt_type == LO_CRYPT_CRYPTOAPI)\n\t\tmemcpy(info64->lo_crypt_name, info.lo_name, LO_NAME_SIZE);\n\telse\n\t\tmemcpy(info64->lo_file_name, info.lo_name, LO_NAME_SIZE);\n\tmemcpy(info64->lo_encrypt_key, info.lo_encrypt_key, LO_KEY_SIZE);\n\treturn 0;\n}\n\n/*\n * Transfer 64-bit loop info to 32-bit compatibility structure in userspace\n * - noinlined to reduce stack space usage in main part of driver\n */\nstatic noinline int\nloop_info64_to_compat(const struct loop_info64 *info64,\n\t\t      struct compat_loop_info __user *arg)\n{\n\tstruct compat_loop_info info;\n\n\tmemset(&info, 0, sizeof(info));\n\tinfo.lo_number = info64->lo_number;\n\tinfo.lo_device = info64->lo_device;\n\tinfo.lo_inode = info64->lo_inode;\n\tinfo.lo_rdevice = info64->lo_rdevice;\n\tinfo.lo_offset = info64->lo_offset;\n\tinfo.lo_encrypt_type = info64->lo_encrypt_type;\n\tinfo.lo_encrypt_key_size = info64->lo_encrypt_key_size;\n\tinfo.lo_flags = info64->lo_flags;\n\tinfo.lo_init[0] = info64->lo_init[0];\n\tinfo.lo_init[1] = info64->lo_init[1];\n\tif (info.lo_encrypt_type == LO_CRYPT_CRYPTOAPI)\n\t\tmemcpy(info.lo_name, info64->lo_crypt_name, LO_NAME_SIZE);\n\telse\n\t\tmemcpy(info.lo_name, info64->lo_file_name, LO_NAME_SIZE);\n\tmemcpy(info.lo_encrypt_key, info64->lo_encrypt_key, LO_KEY_SIZE);\n\n\t/* error in case values were truncated */\n\tif (info.lo_device != info64->lo_device ||\n\t    info.lo_rdevice != info64->lo_rdevice ||\n\t    info.lo_inode != info64->lo_inode ||\n\t    info.lo_offset != info64->lo_offset ||\n\t    info.lo_init[0] != info64->lo_init[0] ||\n\t    info.lo_init[1] != info64->lo_init[1])\n\t\treturn -EOVERFLOW;\n\n\tif (copy_to_user(arg, &info, sizeof(info)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int\nloop_set_status_compat(struct loop_device *lo,\n\t\t       const struct compat_loop_info __user *arg)\n{\n\tstruct loop_info64 info64;\n\tint ret;\n\n\tret = loop_info64_from_compat(arg, &info64);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn loop_set_status(lo, &info64);\n}\n\nstatic int\nloop_get_status_compat(struct loop_device *lo,\n\t\t       struct compat_loop_info __user *arg)\n{\n\tstruct loop_info64 info64;\n\tint err = 0;\n\n\tif (!arg)\n\t\terr = -EINVAL;\n\tif (!err)\n\t\terr = loop_get_status(lo, &info64);\n\tif (!err)\n\t\terr = loop_info64_to_compat(&info64, arg);\n\treturn err;\n}\n\nstatic int lo_compat_ioctl(struct block_device *bdev, fmode_t mode,\n\t\t\t   unsigned int cmd, unsigned long arg)\n{\n\tstruct loop_device *lo = bdev->bd_disk->private_data;\n\tint err;\n\n\tswitch(cmd) {\n\tcase LOOP_SET_STATUS:\n\t\tmutex_lock(&lo->lo_ctl_mutex);\n\t\terr = loop_set_status_compat(\n\t\t\tlo, (const struct compat_loop_info __user *) arg);\n\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\tbreak;\n\tcase LOOP_GET_STATUS:\n\t\tmutex_lock(&lo->lo_ctl_mutex);\n\t\terr = loop_get_status_compat(\n\t\t\tlo, (struct compat_loop_info __user *) arg);\n\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\tbreak;\n\tcase LOOP_SET_CAPACITY:\n\tcase LOOP_CLR_FD:\n\tcase LOOP_GET_STATUS64:\n\tcase LOOP_SET_STATUS64:\n\t\targ = (unsigned long) compat_ptr(arg);\n\tcase LOOP_SET_FD:\n\tcase LOOP_CHANGE_FD:\n\t\terr = lo_ioctl(bdev, mode, cmd, arg);\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOIOCTLCMD;\n\t\tbreak;\n\t}\n\treturn err;\n}\n#endif\n\nstatic int lo_open(struct block_device *bdev, fmode_t mode)\n{\n\tstruct loop_device *lo;\n\tint err = 0;\n\n\tmutex_lock(&loop_index_mutex);\n\tlo = bdev->bd_disk->private_data;\n\tif (!lo) {\n\t\terr = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tatomic_inc(&lo->lo_refcnt);\nout:\n\tmutex_unlock(&loop_index_mutex);\n\treturn err;\n}\n\nstatic void lo_release(struct gendisk *disk, fmode_t mode)\n{\n\tstruct loop_device *lo = disk->private_data;\n\tint err;\n\n\tif (atomic_dec_return(&lo->lo_refcnt))\n\t\treturn;\n\n\tmutex_lock(&lo->lo_ctl_mutex);\n\tif (lo->lo_flags & LO_FLAGS_AUTOCLEAR) {\n\t\t/*\n\t\t * In autoclear mode, stop the loop thread\n\t\t * and remove configuration after last close.\n\t\t */\n\t\terr = loop_clr_fd(lo);\n\t\tif (!err)\n\t\t\treturn;\n\t} else if (lo->lo_state == Lo_bound) {\n\t\t/*\n\t\t * Otherwise keep thread (if running) and config,\n\t\t * but flush possible ongoing bios in thread.\n\t\t */\n\t\tblk_mq_freeze_queue(lo->lo_queue);\n\t\tblk_mq_unfreeze_queue(lo->lo_queue);\n\t}\n\n\tmutex_unlock(&lo->lo_ctl_mutex);\n}\n\nstatic const struct block_device_operations lo_fops = {\n\t.owner =\tTHIS_MODULE,\n\t.open =\t\tlo_open,\n\t.release =\tlo_release,\n\t.ioctl =\tlo_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl =\tlo_compat_ioctl,\n#endif\n};\n\n/*\n * And now the modules code and kernel interface.\n */\nstatic int max_loop;\nmodule_param(max_loop, int, S_IRUGO);\nMODULE_PARM_DESC(max_loop, \"Maximum number of loop devices\");\nmodule_param(max_part, int, S_IRUGO);\nMODULE_PARM_DESC(max_part, \"Maximum number of partitions per loop device\");\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_BLOCKDEV_MAJOR(LOOP_MAJOR);\n\nint loop_register_transfer(struct loop_func_table *funcs)\n{\n\tunsigned int n = funcs->number;\n\n\tif (n >= MAX_LO_CRYPT || xfer_funcs[n])\n\t\treturn -EINVAL;\n\txfer_funcs[n] = funcs;\n\treturn 0;\n}\n\nstatic int unregister_transfer_cb(int id, void *ptr, void *data)\n{\n\tstruct loop_device *lo = ptr;\n\tstruct loop_func_table *xfer = data;\n\n\tmutex_lock(&lo->lo_ctl_mutex);\n\tif (lo->lo_encryption == xfer)\n\t\tloop_release_xfer(lo);\n\tmutex_unlock(&lo->lo_ctl_mutex);\n\treturn 0;\n}\n\nint loop_unregister_transfer(int number)\n{\n\tunsigned int n = number;\n\tstruct loop_func_table *xfer;\n\n\tif (n == 0 || n >= MAX_LO_CRYPT || (xfer = xfer_funcs[n]) == NULL)\n\t\treturn -EINVAL;\n\n\txfer_funcs[n] = NULL;\n\tidr_for_each(&loop_index_idr, &unregister_transfer_cb, xfer);\n\treturn 0;\n}\n\nEXPORT_SYMBOL(loop_register_transfer);\nEXPORT_SYMBOL(loop_unregister_transfer);\n\nstatic blk_status_t loop_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\tconst struct blk_mq_queue_data *bd)\n{\n\tstruct loop_cmd *cmd = blk_mq_rq_to_pdu(bd->rq);\n\tstruct loop_device *lo = cmd->rq->q->queuedata;\n\n\tblk_mq_start_request(bd->rq);\n\n\tif (lo->lo_state != Lo_bound)\n\t\treturn BLK_STS_IOERR;\n\n\tswitch (req_op(cmd->rq)) {\n\tcase REQ_OP_FLUSH:\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tcmd->use_aio = false;\n\t\tbreak;\n\tdefault:\n\t\tcmd->use_aio = lo->use_dio;\n\t\tbreak;\n\t}\n\n\t/* always use the first bio's css */\n#ifdef CONFIG_BLK_CGROUP\n\tif (cmd->use_aio && cmd->rq->bio && cmd->rq->bio->bi_css) {\n\t\tcmd->css = cmd->rq->bio->bi_css;\n\t\tcss_get(cmd->css);\n\t} else\n#endif\n\t\tcmd->css = NULL;\n\tkthread_queue_work(&lo->worker, &cmd->work);\n\n\treturn BLK_STS_OK;\n}\n\nstatic void loop_handle_cmd(struct loop_cmd *cmd)\n{\n\tconst bool write = op_is_write(req_op(cmd->rq));\n\tstruct loop_device *lo = cmd->rq->q->queuedata;\n\tint ret = 0;\n\n\tif (write && (lo->lo_flags & LO_FLAGS_READ_ONLY)) {\n\t\tret = -EIO;\n\t\tgoto failed;\n\t}\n\n\tret = do_req_filebacked(lo, cmd->rq);\n failed:\n\t/* complete non-aio request */\n\tif (!cmd->use_aio || ret) {\n\t\tcmd->ret = ret ? -EIO : 0;\n\t\tblk_mq_complete_request(cmd->rq);\n\t}\n}\n\nstatic void loop_queue_work(struct kthread_work *work)\n{\n\tstruct loop_cmd *cmd =\n\t\tcontainer_of(work, struct loop_cmd, work);\n\n\tloop_handle_cmd(cmd);\n}\n\nstatic int loop_init_request(struct blk_mq_tag_set *set, struct request *rq,\n\t\tunsigned int hctx_idx, unsigned int numa_node)\n{\n\tstruct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);\n\n\tcmd->rq = rq;\n\tkthread_init_work(&cmd->work, loop_queue_work);\n\n\treturn 0;\n}\n\nstatic const struct blk_mq_ops loop_mq_ops = {\n\t.queue_rq       = loop_queue_rq,\n\t.init_request\t= loop_init_request,\n\t.complete\t= lo_complete_rq,\n};\n\nstatic int loop_add(struct loop_device **l, int i)\n{\n\tstruct loop_device *lo;\n\tstruct gendisk *disk;\n\tint err;\n\n\terr = -ENOMEM;\n\tlo = kzalloc(sizeof(*lo), GFP_KERNEL);\n\tif (!lo)\n\t\tgoto out;\n\n\tlo->lo_state = Lo_unbound;\n\n\t/* allocate id, if @id >= 0, we're requesting that specific id */\n\tif (i >= 0) {\n\t\terr = idr_alloc(&loop_index_idr, lo, i, i + 1, GFP_KERNEL);\n\t\tif (err == -ENOSPC)\n\t\t\terr = -EEXIST;\n\t} else {\n\t\terr = idr_alloc(&loop_index_idr, lo, 0, 0, GFP_KERNEL);\n\t}\n\tif (err < 0)\n\t\tgoto out_free_dev;\n\ti = err;\n\n\terr = -ENOMEM;\n\tlo->tag_set.ops = &loop_mq_ops;\n\tlo->tag_set.nr_hw_queues = 1;\n\tlo->tag_set.queue_depth = 128;\n\tlo->tag_set.numa_node = NUMA_NO_NODE;\n\tlo->tag_set.cmd_size = sizeof(struct loop_cmd);\n\tlo->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;\n\tlo->tag_set.driver_data = lo;\n\n\terr = blk_mq_alloc_tag_set(&lo->tag_set);\n\tif (err)\n\t\tgoto out_free_idr;\n\n\tlo->lo_queue = blk_mq_init_queue(&lo->tag_set);\n\tif (IS_ERR_OR_NULL(lo->lo_queue)) {\n\t\terr = PTR_ERR(lo->lo_queue);\n\t\tgoto out_cleanup_tags;\n\t}\n\tlo->lo_queue->queuedata = lo;\n\n\tblk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);\n\n\t/*\n\t * By default, we do buffer IO, so it doesn't make sense to enable\n\t * merge because the I/O submitted to backing file is handled page by\n\t * page. For directio mode, merge does help to dispatch bigger request\n\t * to underlayer disk. We will enable merge once directio is enabled.\n\t */\n\tqueue_flag_set_unlocked(QUEUE_FLAG_NOMERGES, lo->lo_queue);\n\n\terr = -ENOMEM;\n\tdisk = lo->lo_disk = alloc_disk(1 << part_shift);\n\tif (!disk)\n\t\tgoto out_free_queue;\n\n\t/*\n\t * Disable partition scanning by default. The in-kernel partition\n\t * scanning can be requested individually per-device during its\n\t * setup. Userspace can always add and remove partitions from all\n\t * devices. The needed partition minors are allocated from the\n\t * extended minor space, the main loop device numbers will continue\n\t * to match the loop minors, regardless of the number of partitions\n\t * used.\n\t *\n\t * If max_part is given, partition scanning is globally enabled for\n\t * all loop devices. The minors for the main loop devices will be\n\t * multiples of max_part.\n\t *\n\t * Note: Global-for-all-devices, set-only-at-init, read-only module\n\t * parameteters like 'max_loop' and 'max_part' make things needlessly\n\t * complicated, are too static, inflexible and may surprise\n\t * userspace tools. Parameters like this in general should be avoided.\n\t */\n\tif (!part_shift)\n\t\tdisk->flags |= GENHD_FL_NO_PART_SCAN;\n\tdisk->flags |= GENHD_FL_EXT_DEVT;\n\tmutex_init(&lo->lo_ctl_mutex);\n\tatomic_set(&lo->lo_refcnt, 0);\n\tlo->lo_number\t\t= i;\n\tspin_lock_init(&lo->lo_lock);\n\tdisk->major\t\t= LOOP_MAJOR;\n\tdisk->first_minor\t= i << part_shift;\n\tdisk->fops\t\t= &lo_fops;\n\tdisk->private_data\t= lo;\n\tdisk->queue\t\t= lo->lo_queue;\n\tsprintf(disk->disk_name, \"loop%d\", i);\n\tadd_disk(disk);\n\t*l = lo;\n\treturn lo->lo_number;\n\nout_free_queue:\n\tblk_cleanup_queue(lo->lo_queue);\nout_cleanup_tags:\n\tblk_mq_free_tag_set(&lo->tag_set);\nout_free_idr:\n\tidr_remove(&loop_index_idr, i);\nout_free_dev:\n\tkfree(lo);\nout:\n\treturn err;\n}\n\nstatic void loop_remove(struct loop_device *lo)\n{\n\tblk_cleanup_queue(lo->lo_queue);\n\tdel_gendisk(lo->lo_disk);\n\tblk_mq_free_tag_set(&lo->tag_set);\n\tput_disk(lo->lo_disk);\n\tkfree(lo);\n}\n\nstatic int find_free_cb(int id, void *ptr, void *data)\n{\n\tstruct loop_device *lo = ptr;\n\tstruct loop_device **l = data;\n\n\tif (lo->lo_state == Lo_unbound) {\n\t\t*l = lo;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int loop_lookup(struct loop_device **l, int i)\n{\n\tstruct loop_device *lo;\n\tint ret = -ENODEV;\n\n\tif (i < 0) {\n\t\tint err;\n\n\t\terr = idr_for_each(&loop_index_idr, &find_free_cb, &lo);\n\t\tif (err == 1) {\n\t\t\t*l = lo;\n\t\t\tret = lo->lo_number;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* lookup and return a specific i */\n\tlo = idr_find(&loop_index_idr, i);\n\tif (lo) {\n\t\t*l = lo;\n\t\tret = lo->lo_number;\n\t}\nout:\n\treturn ret;\n}\n\nstatic struct kobject *loop_probe(dev_t dev, int *part, void *data)\n{\n\tstruct loop_device *lo;\n\tstruct kobject *kobj;\n\tint err;\n\n\tmutex_lock(&loop_index_mutex);\n\terr = loop_lookup(&lo, MINOR(dev) >> part_shift);\n\tif (err < 0)\n\t\terr = loop_add(&lo, MINOR(dev) >> part_shift);\n\tif (err < 0)\n\t\tkobj = NULL;\n\telse\n\t\tkobj = get_disk(lo->lo_disk);\n\tmutex_unlock(&loop_index_mutex);\n\n\t*part = 0;\n\treturn kobj;\n}\n\nstatic long loop_control_ioctl(struct file *file, unsigned int cmd,\n\t\t\t       unsigned long parm)\n{\n\tstruct loop_device *lo;\n\tint ret = -ENOSYS;\n\n\tmutex_lock(&loop_index_mutex);\n\tswitch (cmd) {\n\tcase LOOP_CTL_ADD:\n\t\tret = loop_lookup(&lo, parm);\n\t\tif (ret >= 0) {\n\t\t\tret = -EEXIST;\n\t\t\tbreak;\n\t\t}\n\t\tret = loop_add(&lo, parm);\n\t\tbreak;\n\tcase LOOP_CTL_REMOVE:\n\t\tret = loop_lookup(&lo, parm);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tmutex_lock(&lo->lo_ctl_mutex);\n\t\tif (lo->lo_state != Lo_unbound) {\n\t\t\tret = -EBUSY;\n\t\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\t\tbreak;\n\t\t}\n\t\tif (atomic_read(&lo->lo_refcnt) > 0) {\n\t\t\tret = -EBUSY;\n\t\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\t\tbreak;\n\t\t}\n\t\tlo->lo_disk->private_data = NULL;\n\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\tidr_remove(&loop_index_idr, lo->lo_number);\n\t\tloop_remove(lo);\n\t\tbreak;\n\tcase LOOP_CTL_GET_FREE:\n\t\tret = loop_lookup(&lo, -1);\n\t\tif (ret >= 0)\n\t\t\tbreak;\n\t\tret = loop_add(&lo, -1);\n\t}\n\tmutex_unlock(&loop_index_mutex);\n\n\treturn ret;\n}\n\nstatic const struct file_operations loop_ctl_fops = {\n\t.open\t\t= nonseekable_open,\n\t.unlocked_ioctl\t= loop_control_ioctl,\n\t.compat_ioctl\t= loop_control_ioctl,\n\t.owner\t\t= THIS_MODULE,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic struct miscdevice loop_misc = {\n\t.minor\t\t= LOOP_CTRL_MINOR,\n\t.name\t\t= \"loop-control\",\n\t.fops\t\t= &loop_ctl_fops,\n};\n\nMODULE_ALIAS_MISCDEV(LOOP_CTRL_MINOR);\nMODULE_ALIAS(\"devname:loop-control\");\n\nstatic int __init loop_init(void)\n{\n\tint i, nr;\n\tunsigned long range;\n\tstruct loop_device *lo;\n\tint err;\n\n\tpart_shift = 0;\n\tif (max_part > 0) {\n\t\tpart_shift = fls(max_part);\n\n\t\t/*\n\t\t * Adjust max_part according to part_shift as it is exported\n\t\t * to user space so that user can decide correct minor number\n\t\t * if [s]he want to create more devices.\n\t\t *\n\t\t * Note that -1 is required because partition 0 is reserved\n\t\t * for the whole disk.\n\t\t */\n\t\tmax_part = (1UL << part_shift) - 1;\n\t}\n\n\tif ((1UL << part_shift) > DISK_MAX_PARTS) {\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (max_loop > 1UL << (MINORBITS - part_shift)) {\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\t/*\n\t * If max_loop is specified, create that many devices upfront.\n\t * This also becomes a hard limit. If max_loop is not specified,\n\t * create CONFIG_BLK_DEV_LOOP_MIN_COUNT loop devices at module\n\t * init time. Loop devices can be requested on-demand with the\n\t * /dev/loop-control interface, or be instantiated by accessing\n\t * a 'dead' device node.\n\t */\n\tif (max_loop) {\n\t\tnr = max_loop;\n\t\trange = max_loop << part_shift;\n\t} else {\n\t\tnr = CONFIG_BLK_DEV_LOOP_MIN_COUNT;\n\t\trange = 1UL << MINORBITS;\n\t}\n\n\terr = misc_register(&loop_misc);\n\tif (err < 0)\n\t\tgoto err_out;\n\n\n\tif (register_blkdev(LOOP_MAJOR, \"loop\")) {\n\t\terr = -EIO;\n\t\tgoto misc_out;\n\t}\n\n\tblk_register_region(MKDEV(LOOP_MAJOR, 0), range,\n\t\t\t\t  THIS_MODULE, loop_probe, NULL, NULL);\n\n\t/* pre-create number of devices given by config or max_loop */\n\tmutex_lock(&loop_index_mutex);\n\tfor (i = 0; i < nr; i++)\n\t\tloop_add(&lo, i);\n\tmutex_unlock(&loop_index_mutex);\n\n\tprintk(KERN_INFO \"loop: module loaded\\n\");\n\treturn 0;\n\nmisc_out:\n\tmisc_deregister(&loop_misc);\nerr_out:\n\treturn err;\n}\n\nstatic int loop_exit_cb(int id, void *ptr, void *data)\n{\n\tstruct loop_device *lo = ptr;\n\n\tloop_remove(lo);\n\treturn 0;\n}\n\nstatic void __exit loop_exit(void)\n{\n\tunsigned long range;\n\n\trange = max_loop ? max_loop << part_shift : 1UL << MINORBITS;\n\n\tidr_for_each(&loop_index_idr, &loop_exit_cb, NULL);\n\tidr_destroy(&loop_index_idr);\n\n\tblk_unregister_region(MKDEV(LOOP_MAJOR, 0), range);\n\tunregister_blkdev(LOOP_MAJOR, \"loop\");\n\n\tmisc_deregister(&loop_misc);\n}\n\nmodule_init(loop_init);\nmodule_exit(loop_exit);\n\n#ifndef MODULE\nstatic int __init max_loop_setup(char *str)\n{\n\tmax_loop = simple_strtol(str, NULL, 0);\n\treturn 1;\n}\n\n__setup(\"max_loop=\", max_loop_setup);\n#endif\n"], "fixing_code": ["/*\n *  linux/drivers/block/loop.c\n *\n *  Written by Theodore Ts'o, 3/29/93\n *\n * Copyright 1993 by Theodore Ts'o.  Redistribution of this file is\n * permitted under the GNU General Public License.\n *\n * DES encryption plus some minor changes by Werner Almesberger, 30-MAY-1993\n * more DES encryption plus IDEA encryption by Nicholas J. Leon, June 20, 1996\n *\n * Modularized and updated for 1.1.16 kernel - Mitch Dsouza 28th May 1994\n * Adapted for 1.3.59 kernel - Andries Brouwer, 1 Feb 1996\n *\n * Fixed do_loop_request() re-entrancy - Vincent.Renardias@waw.com Mar 20, 1997\n *\n * Added devfs support - Richard Gooch <rgooch@atnf.csiro.au> 16-Jan-1998\n *\n * Handle sparse backing files correctly - Kenn Humborg, Jun 28, 1998\n *\n * Loadable modules and other fixes by AK, 1998\n *\n * Make real block number available to downstream transfer functions, enables\n * CBC (and relatives) mode encryption requiring unique IVs per data block.\n * Reed H. Petty, rhp@draper.net\n *\n * Maximum number of loop devices now dynamic via max_loop module parameter.\n * Russell Kroll <rkroll@exploits.org> 19990701\n *\n * Maximum number of loop devices when compiled-in now selectable by passing\n * max_loop=<1-255> to the kernel on boot.\n * Erik I. Bols\u00f8, <eriki@himolde.no>, Oct 31, 1999\n *\n * Completely rewrite request handling to be make_request_fn style and\n * non blocking, pushing work to a helper thread. Lots of fixes from\n * Al Viro too.\n * Jens Axboe <axboe@suse.de>, Nov 2000\n *\n * Support up to 256 loop devices\n * Heinz Mauelshagen <mge@sistina.com>, Feb 2002\n *\n * Support for falling back on the write file operation when the address space\n * operations write_begin is not available on the backing filesystem.\n * Anton Altaparmakov, 16 Feb 2005\n *\n * Still To Fix:\n * - Advisory locking is ignored here.\n * - Should use an own CAP_* category instead of CAP_SYS_ADMIN\n *\n */\n\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/sched.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/stat.h>\n#include <linux/errno.h>\n#include <linux/major.h>\n#include <linux/wait.h>\n#include <linux/blkdev.h>\n#include <linux/blkpg.h>\n#include <linux/init.h>\n#include <linux/swap.h>\n#include <linux/slab.h>\n#include <linux/compat.h>\n#include <linux/suspend.h>\n#include <linux/freezer.h>\n#include <linux/mutex.h>\n#include <linux/writeback.h>\n#include <linux/completion.h>\n#include <linux/highmem.h>\n#include <linux/kthread.h>\n#include <linux/splice.h>\n#include <linux/sysfs.h>\n#include <linux/miscdevice.h>\n#include <linux/falloc.h>\n#include <linux/uio.h>\n#include \"loop.h\"\n\n#include <linux/uaccess.h>\n\nstatic DEFINE_IDR(loop_index_idr);\nstatic DEFINE_MUTEX(loop_index_mutex);\n\nstatic int max_part;\nstatic int part_shift;\n\nstatic int transfer_xor(struct loop_device *lo, int cmd,\n\t\t\tstruct page *raw_page, unsigned raw_off,\n\t\t\tstruct page *loop_page, unsigned loop_off,\n\t\t\tint size, sector_t real_block)\n{\n\tchar *raw_buf = kmap_atomic(raw_page) + raw_off;\n\tchar *loop_buf = kmap_atomic(loop_page) + loop_off;\n\tchar *in, *out, *key;\n\tint i, keysize;\n\n\tif (cmd == READ) {\n\t\tin = raw_buf;\n\t\tout = loop_buf;\n\t} else {\n\t\tin = loop_buf;\n\t\tout = raw_buf;\n\t}\n\n\tkey = lo->lo_encrypt_key;\n\tkeysize = lo->lo_encrypt_key_size;\n\tfor (i = 0; i < size; i++)\n\t\t*out++ = *in++ ^ key[(i & 511) % keysize];\n\n\tkunmap_atomic(loop_buf);\n\tkunmap_atomic(raw_buf);\n\tcond_resched();\n\treturn 0;\n}\n\nstatic int xor_init(struct loop_device *lo, const struct loop_info64 *info)\n{\n\tif (unlikely(info->lo_encrypt_key_size <= 0))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic struct loop_func_table none_funcs = {\n\t.number = LO_CRYPT_NONE,\n}; \n\nstatic struct loop_func_table xor_funcs = {\n\t.number = LO_CRYPT_XOR,\n\t.transfer = transfer_xor,\n\t.init = xor_init\n}; \n\n/* xfer_funcs[0] is special - its release function is never called */\nstatic struct loop_func_table *xfer_funcs[MAX_LO_CRYPT] = {\n\t&none_funcs,\n\t&xor_funcs\n};\n\nstatic loff_t get_size(loff_t offset, loff_t sizelimit, struct file *file)\n{\n\tloff_t loopsize;\n\n\t/* Compute loopsize in bytes */\n\tloopsize = i_size_read(file->f_mapping->host);\n\tif (offset > 0)\n\t\tloopsize -= offset;\n\t/* offset is beyond i_size, weird but possible */\n\tif (loopsize < 0)\n\t\treturn 0;\n\n\tif (sizelimit > 0 && sizelimit < loopsize)\n\t\tloopsize = sizelimit;\n\t/*\n\t * Unfortunately, if we want to do I/O on the device,\n\t * the number of 512-byte sectors has to fit into a sector_t.\n\t */\n\treturn loopsize >> 9;\n}\n\nstatic loff_t get_loop_size(struct loop_device *lo, struct file *file)\n{\n\treturn get_size(lo->lo_offset, lo->lo_sizelimit, file);\n}\n\nstatic void __loop_update_dio(struct loop_device *lo, bool dio)\n{\n\tstruct file *file = lo->lo_backing_file;\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tunsigned short sb_bsize = 0;\n\tunsigned dio_align = 0;\n\tbool use_dio;\n\n\tif (inode->i_sb->s_bdev) {\n\t\tsb_bsize = bdev_logical_block_size(inode->i_sb->s_bdev);\n\t\tdio_align = sb_bsize - 1;\n\t}\n\n\t/*\n\t * We support direct I/O only if lo_offset is aligned with the\n\t * logical I/O size of backing device, and the logical block\n\t * size of loop is bigger than the backing device's and the loop\n\t * needn't transform transfer.\n\t *\n\t * TODO: the above condition may be loosed in the future, and\n\t * direct I/O may be switched runtime at that time because most\n\t * of requests in sane applications should be PAGE_SIZE aligned\n\t */\n\tif (dio) {\n\t\tif (queue_logical_block_size(lo->lo_queue) >= sb_bsize &&\n\t\t\t\t!(lo->lo_offset & dio_align) &&\n\t\t\t\tmapping->a_ops->direct_IO &&\n\t\t\t\t!lo->transfer)\n\t\t\tuse_dio = true;\n\t\telse\n\t\t\tuse_dio = false;\n\t} else {\n\t\tuse_dio = false;\n\t}\n\n\tif (lo->use_dio == use_dio)\n\t\treturn;\n\n\t/* flush dirty pages before changing direct IO */\n\tvfs_fsync(file, 0);\n\n\t/*\n\t * The flag of LO_FLAGS_DIRECT_IO is handled similarly with\n\t * LO_FLAGS_READ_ONLY, both are set from kernel, and losetup\n\t * will get updated by ioctl(LOOP_GET_STATUS)\n\t */\n\tblk_mq_freeze_queue(lo->lo_queue);\n\tlo->use_dio = use_dio;\n\tif (use_dio) {\n\t\tqueue_flag_clear_unlocked(QUEUE_FLAG_NOMERGES, lo->lo_queue);\n\t\tlo->lo_flags |= LO_FLAGS_DIRECT_IO;\n\t} else {\n\t\tqueue_flag_set_unlocked(QUEUE_FLAG_NOMERGES, lo->lo_queue);\n\t\tlo->lo_flags &= ~LO_FLAGS_DIRECT_IO;\n\t}\n\tblk_mq_unfreeze_queue(lo->lo_queue);\n}\n\nstatic int\nfigure_loop_size(struct loop_device *lo, loff_t offset, loff_t sizelimit)\n{\n\tloff_t size = get_size(offset, sizelimit, lo->lo_backing_file);\n\tsector_t x = (sector_t)size;\n\tstruct block_device *bdev = lo->lo_device;\n\n\tif (unlikely((loff_t)x != size))\n\t\treturn -EFBIG;\n\tif (lo->lo_offset != offset)\n\t\tlo->lo_offset = offset;\n\tif (lo->lo_sizelimit != sizelimit)\n\t\tlo->lo_sizelimit = sizelimit;\n\tset_capacity(lo->lo_disk, x);\n\tbd_set_size(bdev, (loff_t)get_capacity(bdev->bd_disk) << 9);\n\t/* let user-space know about the new size */\n\tkobject_uevent(&disk_to_dev(bdev->bd_disk)->kobj, KOBJ_CHANGE);\n\treturn 0;\n}\n\nstatic inline int\nlo_do_transfer(struct loop_device *lo, int cmd,\n\t       struct page *rpage, unsigned roffs,\n\t       struct page *lpage, unsigned loffs,\n\t       int size, sector_t rblock)\n{\n\tint ret;\n\n\tret = lo->transfer(lo, cmd, rpage, roffs, lpage, loffs, size, rblock);\n\tif (likely(!ret))\n\t\treturn 0;\n\n\tprintk_ratelimited(KERN_ERR\n\t\t\"loop: Transfer error at byte offset %llu, length %i.\\n\",\n\t\t(unsigned long long)rblock << 9, size);\n\treturn ret;\n}\n\nstatic int lo_write_bvec(struct file *file, struct bio_vec *bvec, loff_t *ppos)\n{\n\tstruct iov_iter i;\n\tssize_t bw;\n\n\tiov_iter_bvec(&i, ITER_BVEC, bvec, 1, bvec->bv_len);\n\n\tfile_start_write(file);\n\tbw = vfs_iter_write(file, &i, ppos, 0);\n\tfile_end_write(file);\n\n\tif (likely(bw ==  bvec->bv_len))\n\t\treturn 0;\n\n\tprintk_ratelimited(KERN_ERR\n\t\t\"loop: Write error at byte offset %llu, length %i.\\n\",\n\t\t(unsigned long long)*ppos, bvec->bv_len);\n\tif (bw >= 0)\n\t\tbw = -EIO;\n\treturn bw;\n}\n\nstatic int lo_write_simple(struct loop_device *lo, struct request *rq,\n\t\tloff_t pos)\n{\n\tstruct bio_vec bvec;\n\tstruct req_iterator iter;\n\tint ret = 0;\n\n\trq_for_each_segment(bvec, rq, iter) {\n\t\tret = lo_write_bvec(lo->lo_backing_file, &bvec, &pos);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\n\treturn ret;\n}\n\n/*\n * This is the slow, transforming version that needs to double buffer the\n * data as it cannot do the transformations in place without having direct\n * access to the destination pages of the backing file.\n */\nstatic int lo_write_transfer(struct loop_device *lo, struct request *rq,\n\t\tloff_t pos)\n{\n\tstruct bio_vec bvec, b;\n\tstruct req_iterator iter;\n\tstruct page *page;\n\tint ret = 0;\n\n\tpage = alloc_page(GFP_NOIO);\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\n\trq_for_each_segment(bvec, rq, iter) {\n\t\tret = lo_do_transfer(lo, WRITE, page, 0, bvec.bv_page,\n\t\t\tbvec.bv_offset, bvec.bv_len, pos >> 9);\n\t\tif (unlikely(ret))\n\t\t\tbreak;\n\n\t\tb.bv_page = page;\n\t\tb.bv_offset = 0;\n\t\tb.bv_len = bvec.bv_len;\n\t\tret = lo_write_bvec(lo->lo_backing_file, &b, &pos);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t}\n\n\t__free_page(page);\n\treturn ret;\n}\n\nstatic int lo_read_simple(struct loop_device *lo, struct request *rq,\n\t\tloff_t pos)\n{\n\tstruct bio_vec bvec;\n\tstruct req_iterator iter;\n\tstruct iov_iter i;\n\tssize_t len;\n\n\trq_for_each_segment(bvec, rq, iter) {\n\t\tiov_iter_bvec(&i, ITER_BVEC, &bvec, 1, bvec.bv_len);\n\t\tlen = vfs_iter_read(lo->lo_backing_file, &i, &pos, 0);\n\t\tif (len < 0)\n\t\t\treturn len;\n\n\t\tflush_dcache_page(bvec.bv_page);\n\n\t\tif (len != bvec.bv_len) {\n\t\t\tstruct bio *bio;\n\n\t\t\t__rq_for_each_bio(bio, rq)\n\t\t\t\tzero_fill_bio(bio);\n\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n}\n\nstatic int lo_read_transfer(struct loop_device *lo, struct request *rq,\n\t\tloff_t pos)\n{\n\tstruct bio_vec bvec, b;\n\tstruct req_iterator iter;\n\tstruct iov_iter i;\n\tstruct page *page;\n\tssize_t len;\n\tint ret = 0;\n\n\tpage = alloc_page(GFP_NOIO);\n\tif (unlikely(!page))\n\t\treturn -ENOMEM;\n\n\trq_for_each_segment(bvec, rq, iter) {\n\t\tloff_t offset = pos;\n\n\t\tb.bv_page = page;\n\t\tb.bv_offset = 0;\n\t\tb.bv_len = bvec.bv_len;\n\n\t\tiov_iter_bvec(&i, ITER_BVEC, &b, 1, b.bv_len);\n\t\tlen = vfs_iter_read(lo->lo_backing_file, &i, &pos, 0);\n\t\tif (len < 0) {\n\t\t\tret = len;\n\t\t\tgoto out_free_page;\n\t\t}\n\n\t\tret = lo_do_transfer(lo, READ, page, 0, bvec.bv_page,\n\t\t\tbvec.bv_offset, len, offset >> 9);\n\t\tif (ret)\n\t\t\tgoto out_free_page;\n\n\t\tflush_dcache_page(bvec.bv_page);\n\n\t\tif (len != bvec.bv_len) {\n\t\t\tstruct bio *bio;\n\n\t\t\t__rq_for_each_bio(bio, rq)\n\t\t\t\tzero_fill_bio(bio);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tret = 0;\nout_free_page:\n\t__free_page(page);\n\treturn ret;\n}\n\nstatic int lo_discard(struct loop_device *lo, struct request *rq, loff_t pos)\n{\n\t/*\n\t * We use punch hole to reclaim the free space used by the\n\t * image a.k.a. discard. However we do not support discard if\n\t * encryption is enabled, because it may give an attacker\n\t * useful information.\n\t */\n\tstruct file *file = lo->lo_backing_file;\n\tint mode = FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE;\n\tint ret;\n\n\tif ((!file->f_op->fallocate) || lo->lo_encrypt_key_size) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tret = file->f_op->fallocate(file, mode, pos, blk_rq_bytes(rq));\n\tif (unlikely(ret && ret != -EINVAL && ret != -EOPNOTSUPP))\n\t\tret = -EIO;\n out:\n\treturn ret;\n}\n\nstatic int lo_req_flush(struct loop_device *lo, struct request *rq)\n{\n\tstruct file *file = lo->lo_backing_file;\n\tint ret = vfs_fsync(file, 0);\n\tif (unlikely(ret && ret != -EINVAL))\n\t\tret = -EIO;\n\n\treturn ret;\n}\n\nstatic void lo_complete_rq(struct request *rq)\n{\n\tstruct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);\n\n\tif (unlikely(req_op(cmd->rq) == REQ_OP_READ && cmd->use_aio &&\n\t\t     cmd->ret >= 0 && cmd->ret < blk_rq_bytes(cmd->rq))) {\n\t\tstruct bio *bio = cmd->rq->bio;\n\n\t\tbio_advance(bio, cmd->ret);\n\t\tzero_fill_bio(bio);\n\t}\n\n\tblk_mq_end_request(rq, cmd->ret < 0 ? BLK_STS_IOERR : BLK_STS_OK);\n}\n\nstatic void lo_rw_aio_do_completion(struct loop_cmd *cmd)\n{\n\tif (!atomic_dec_and_test(&cmd->ref))\n\t\treturn;\n\tkfree(cmd->bvec);\n\tcmd->bvec = NULL;\n\tblk_mq_complete_request(cmd->rq);\n}\n\nstatic void lo_rw_aio_complete(struct kiocb *iocb, long ret, long ret2)\n{\n\tstruct loop_cmd *cmd = container_of(iocb, struct loop_cmd, iocb);\n\n\tif (cmd->css)\n\t\tcss_put(cmd->css);\n\tcmd->ret = ret;\n\tlo_rw_aio_do_completion(cmd);\n}\n\nstatic int lo_rw_aio(struct loop_device *lo, struct loop_cmd *cmd,\n\t\t     loff_t pos, bool rw)\n{\n\tstruct iov_iter iter;\n\tstruct bio_vec *bvec;\n\tstruct request *rq = cmd->rq;\n\tstruct bio *bio = rq->bio;\n\tstruct file *file = lo->lo_backing_file;\n\tunsigned int offset;\n\tint segments = 0;\n\tint ret;\n\n\tif (rq->bio != rq->biotail) {\n\t\tstruct req_iterator iter;\n\t\tstruct bio_vec tmp;\n\n\t\t__rq_for_each_bio(bio, rq)\n\t\t\tsegments += bio_segments(bio);\n\t\tbvec = kmalloc(sizeof(struct bio_vec) * segments, GFP_NOIO);\n\t\tif (!bvec)\n\t\t\treturn -EIO;\n\t\tcmd->bvec = bvec;\n\n\t\t/*\n\t\t * The bios of the request may be started from the middle of\n\t\t * the 'bvec' because of bio splitting, so we can't directly\n\t\t * copy bio->bi_iov_vec to new bvec. The rq_for_each_segment\n\t\t * API will take care of all details for us.\n\t\t */\n\t\trq_for_each_segment(tmp, rq, iter) {\n\t\t\t*bvec = tmp;\n\t\t\tbvec++;\n\t\t}\n\t\tbvec = cmd->bvec;\n\t\toffset = 0;\n\t} else {\n\t\t/*\n\t\t * Same here, this bio may be started from the middle of the\n\t\t * 'bvec' because of bio splitting, so offset from the bvec\n\t\t * must be passed to iov iterator\n\t\t */\n\t\toffset = bio->bi_iter.bi_bvec_done;\n\t\tbvec = __bvec_iter_bvec(bio->bi_io_vec, bio->bi_iter);\n\t\tsegments = bio_segments(bio);\n\t}\n\tatomic_set(&cmd->ref, 2);\n\n\tiov_iter_bvec(&iter, ITER_BVEC | rw, bvec,\n\t\t      segments, blk_rq_bytes(rq));\n\titer.iov_offset = offset;\n\n\tcmd->iocb.ki_pos = pos;\n\tcmd->iocb.ki_filp = file;\n\tcmd->iocb.ki_complete = lo_rw_aio_complete;\n\tcmd->iocb.ki_flags = IOCB_DIRECT;\n\tif (cmd->css)\n\t\tkthread_associate_blkcg(cmd->css);\n\n\tif (rw == WRITE)\n\t\tret = call_write_iter(file, &cmd->iocb, &iter);\n\telse\n\t\tret = call_read_iter(file, &cmd->iocb, &iter);\n\n\tlo_rw_aio_do_completion(cmd);\n\tkthread_associate_blkcg(NULL);\n\n\tif (ret != -EIOCBQUEUED)\n\t\tcmd->iocb.ki_complete(&cmd->iocb, ret, 0);\n\treturn 0;\n}\n\nstatic int do_req_filebacked(struct loop_device *lo, struct request *rq)\n{\n\tstruct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);\n\tloff_t pos = ((loff_t) blk_rq_pos(rq) << 9) + lo->lo_offset;\n\n\t/*\n\t * lo_write_simple and lo_read_simple should have been covered\n\t * by io submit style function like lo_rw_aio(), one blocker\n\t * is that lo_read_simple() need to call flush_dcache_page after\n\t * the page is written from kernel, and it isn't easy to handle\n\t * this in io submit style function which submits all segments\n\t * of the req at one time. And direct read IO doesn't need to\n\t * run flush_dcache_page().\n\t */\n\tswitch (req_op(rq)) {\n\tcase REQ_OP_FLUSH:\n\t\treturn lo_req_flush(lo, rq);\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\treturn lo_discard(lo, rq, pos);\n\tcase REQ_OP_WRITE:\n\t\tif (lo->transfer)\n\t\t\treturn lo_write_transfer(lo, rq, pos);\n\t\telse if (cmd->use_aio)\n\t\t\treturn lo_rw_aio(lo, cmd, pos, WRITE);\n\t\telse\n\t\t\treturn lo_write_simple(lo, rq, pos);\n\tcase REQ_OP_READ:\n\t\tif (lo->transfer)\n\t\t\treturn lo_read_transfer(lo, rq, pos);\n\t\telse if (cmd->use_aio)\n\t\t\treturn lo_rw_aio(lo, cmd, pos, READ);\n\t\telse\n\t\t\treturn lo_read_simple(lo, rq, pos);\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn -EIO;\n\t\tbreak;\n\t}\n}\n\nstatic inline void loop_update_dio(struct loop_device *lo)\n{\n\t__loop_update_dio(lo, io_is_direct(lo->lo_backing_file) |\n\t\t\tlo->use_dio);\n}\n\nstatic void loop_reread_partitions(struct loop_device *lo,\n\t\t\t\t   struct block_device *bdev)\n{\n\tint rc;\n\n\t/*\n\t * bd_mutex has been held already in release path, so don't\n\t * acquire it if this function is called in such case.\n\t *\n\t * If the reread partition isn't from release path, lo_refcnt\n\t * must be at least one and it can only become zero when the\n\t * current holder is released.\n\t */\n\tif (!atomic_read(&lo->lo_refcnt))\n\t\trc = __blkdev_reread_part(bdev);\n\telse\n\t\trc = blkdev_reread_part(bdev);\n\tif (rc)\n\t\tpr_warn(\"%s: partition scan of loop%d (%s) failed (rc=%d)\\n\",\n\t\t\t__func__, lo->lo_number, lo->lo_file_name, rc);\n}\n\n/*\n * loop_change_fd switched the backing store of a loopback device to\n * a new file. This is useful for operating system installers to free up\n * the original file and in High Availability environments to switch to\n * an alternative location for the content in case of server meltdown.\n * This can only work if the loop device is used read-only, and if the\n * new backing store is the same size and type as the old backing store.\n */\nstatic int loop_change_fd(struct loop_device *lo, struct block_device *bdev,\n\t\t\t  unsigned int arg)\n{\n\tstruct file\t*file, *old_file;\n\tstruct inode\t*inode;\n\tint\t\terror;\n\n\terror = -ENXIO;\n\tif (lo->lo_state != Lo_bound)\n\t\tgoto out;\n\n\t/* the loop device has to be read-only */\n\terror = -EINVAL;\n\tif (!(lo->lo_flags & LO_FLAGS_READ_ONLY))\n\t\tgoto out;\n\n\terror = -EBADF;\n\tfile = fget(arg);\n\tif (!file)\n\t\tgoto out;\n\n\tinode = file->f_mapping->host;\n\told_file = lo->lo_backing_file;\n\n\terror = -EINVAL;\n\n\tif (!S_ISREG(inode->i_mode) && !S_ISBLK(inode->i_mode))\n\t\tgoto out_putf;\n\n\t/* size of the new backing store needs to be the same */\n\tif (get_loop_size(lo, file) != get_loop_size(lo, old_file))\n\t\tgoto out_putf;\n\n\t/* and ... switch */\n\tblk_mq_freeze_queue(lo->lo_queue);\n\tmapping_set_gfp_mask(old_file->f_mapping, lo->old_gfp_mask);\n\tlo->lo_backing_file = file;\n\tlo->old_gfp_mask = mapping_gfp_mask(file->f_mapping);\n\tmapping_set_gfp_mask(file->f_mapping,\n\t\t\t     lo->old_gfp_mask & ~(__GFP_IO|__GFP_FS));\n\tloop_update_dio(lo);\n\tblk_mq_unfreeze_queue(lo->lo_queue);\n\n\tfput(old_file);\n\tif (lo->lo_flags & LO_FLAGS_PARTSCAN)\n\t\tloop_reread_partitions(lo, bdev);\n\treturn 0;\n\n out_putf:\n\tfput(file);\n out:\n\treturn error;\n}\n\nstatic inline int is_loop_device(struct file *file)\n{\n\tstruct inode *i = file->f_mapping->host;\n\n\treturn i && S_ISBLK(i->i_mode) && MAJOR(i->i_rdev) == LOOP_MAJOR;\n}\n\n/* loop sysfs attributes */\n\nstatic ssize_t loop_attr_show(struct device *dev, char *page,\n\t\t\t      ssize_t (*callback)(struct loop_device *, char *))\n{\n\tstruct gendisk *disk = dev_to_disk(dev);\n\tstruct loop_device *lo = disk->private_data;\n\n\treturn callback(lo, page);\n}\n\n#define LOOP_ATTR_RO(_name)\t\t\t\t\t\t\\\nstatic ssize_t loop_attr_##_name##_show(struct loop_device *, char *);\t\\\nstatic ssize_t loop_attr_do_show_##_name(struct device *d,\t\t\\\n\t\t\t\tstruct device_attribute *attr, char *b)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn loop_attr_show(d, b, loop_attr_##_name##_show);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic struct device_attribute loop_attr_##_name =\t\t\t\\\n\t__ATTR(_name, S_IRUGO, loop_attr_do_show_##_name, NULL);\n\nstatic ssize_t loop_attr_backing_file_show(struct loop_device *lo, char *buf)\n{\n\tssize_t ret;\n\tchar *p = NULL;\n\n\tspin_lock_irq(&lo->lo_lock);\n\tif (lo->lo_backing_file)\n\t\tp = file_path(lo->lo_backing_file, buf, PAGE_SIZE - 1);\n\tspin_unlock_irq(&lo->lo_lock);\n\n\tif (IS_ERR_OR_NULL(p))\n\t\tret = PTR_ERR(p);\n\telse {\n\t\tret = strlen(p);\n\t\tmemmove(buf, p, ret);\n\t\tbuf[ret++] = '\\n';\n\t\tbuf[ret] = 0;\n\t}\n\n\treturn ret;\n}\n\nstatic ssize_t loop_attr_offset_show(struct loop_device *lo, char *buf)\n{\n\treturn sprintf(buf, \"%llu\\n\", (unsigned long long)lo->lo_offset);\n}\n\nstatic ssize_t loop_attr_sizelimit_show(struct loop_device *lo, char *buf)\n{\n\treturn sprintf(buf, \"%llu\\n\", (unsigned long long)lo->lo_sizelimit);\n}\n\nstatic ssize_t loop_attr_autoclear_show(struct loop_device *lo, char *buf)\n{\n\tint autoclear = (lo->lo_flags & LO_FLAGS_AUTOCLEAR);\n\n\treturn sprintf(buf, \"%s\\n\", autoclear ? \"1\" : \"0\");\n}\n\nstatic ssize_t loop_attr_partscan_show(struct loop_device *lo, char *buf)\n{\n\tint partscan = (lo->lo_flags & LO_FLAGS_PARTSCAN);\n\n\treturn sprintf(buf, \"%s\\n\", partscan ? \"1\" : \"0\");\n}\n\nstatic ssize_t loop_attr_dio_show(struct loop_device *lo, char *buf)\n{\n\tint dio = (lo->lo_flags & LO_FLAGS_DIRECT_IO);\n\n\treturn sprintf(buf, \"%s\\n\", dio ? \"1\" : \"0\");\n}\n\nLOOP_ATTR_RO(backing_file);\nLOOP_ATTR_RO(offset);\nLOOP_ATTR_RO(sizelimit);\nLOOP_ATTR_RO(autoclear);\nLOOP_ATTR_RO(partscan);\nLOOP_ATTR_RO(dio);\n\nstatic struct attribute *loop_attrs[] = {\n\t&loop_attr_backing_file.attr,\n\t&loop_attr_offset.attr,\n\t&loop_attr_sizelimit.attr,\n\t&loop_attr_autoclear.attr,\n\t&loop_attr_partscan.attr,\n\t&loop_attr_dio.attr,\n\tNULL,\n};\n\nstatic struct attribute_group loop_attribute_group = {\n\t.name = \"loop\",\n\t.attrs= loop_attrs,\n};\n\nstatic int loop_sysfs_init(struct loop_device *lo)\n{\n\treturn sysfs_create_group(&disk_to_dev(lo->lo_disk)->kobj,\n\t\t\t\t  &loop_attribute_group);\n}\n\nstatic void loop_sysfs_exit(struct loop_device *lo)\n{\n\tsysfs_remove_group(&disk_to_dev(lo->lo_disk)->kobj,\n\t\t\t   &loop_attribute_group);\n}\n\nstatic void loop_config_discard(struct loop_device *lo)\n{\n\tstruct file *file = lo->lo_backing_file;\n\tstruct inode *inode = file->f_mapping->host;\n\tstruct request_queue *q = lo->lo_queue;\n\n\t/*\n\t * We use punch hole to reclaim the free space used by the\n\t * image a.k.a. discard. However we do not support discard if\n\t * encryption is enabled, because it may give an attacker\n\t * useful information.\n\t */\n\tif ((!file->f_op->fallocate) ||\n\t    lo->lo_encrypt_key_size) {\n\t\tq->limits.discard_granularity = 0;\n\t\tq->limits.discard_alignment = 0;\n\t\tblk_queue_max_discard_sectors(q, 0);\n\t\tblk_queue_max_write_zeroes_sectors(q, 0);\n\t\tqueue_flag_clear_unlocked(QUEUE_FLAG_DISCARD, q);\n\t\treturn;\n\t}\n\n\tq->limits.discard_granularity = inode->i_sb->s_blocksize;\n\tq->limits.discard_alignment = 0;\n\n\tblk_queue_max_discard_sectors(q, UINT_MAX >> 9);\n\tblk_queue_max_write_zeroes_sectors(q, UINT_MAX >> 9);\n\tqueue_flag_set_unlocked(QUEUE_FLAG_DISCARD, q);\n}\n\nstatic void loop_unprepare_queue(struct loop_device *lo)\n{\n\tkthread_flush_worker(&lo->worker);\n\tkthread_stop(lo->worker_task);\n}\n\nstatic int loop_kthread_worker_fn(void *worker_ptr)\n{\n\tcurrent->flags |= PF_LESS_THROTTLE;\n\treturn kthread_worker_fn(worker_ptr);\n}\n\nstatic int loop_prepare_queue(struct loop_device *lo)\n{\n\tkthread_init_worker(&lo->worker);\n\tlo->worker_task = kthread_run(loop_kthread_worker_fn,\n\t\t\t&lo->worker, \"loop%d\", lo->lo_number);\n\tif (IS_ERR(lo->worker_task))\n\t\treturn -ENOMEM;\n\tset_user_nice(lo->worker_task, MIN_NICE);\n\treturn 0;\n}\n\nstatic int loop_set_fd(struct loop_device *lo, fmode_t mode,\n\t\t       struct block_device *bdev, unsigned int arg)\n{\n\tstruct file\t*file, *f;\n\tstruct inode\t*inode;\n\tstruct address_space *mapping;\n\tint\t\tlo_flags = 0;\n\tint\t\terror;\n\tloff_t\t\tsize;\n\n\t/* This is safe, since we have a reference from open(). */\n\t__module_get(THIS_MODULE);\n\n\terror = -EBADF;\n\tfile = fget(arg);\n\tif (!file)\n\t\tgoto out;\n\n\terror = -EBUSY;\n\tif (lo->lo_state != Lo_unbound)\n\t\tgoto out_putf;\n\n\t/* Avoid recursion */\n\tf = file;\n\twhile (is_loop_device(f)) {\n\t\tstruct loop_device *l;\n\n\t\tif (f->f_mapping->host->i_bdev == bdev)\n\t\t\tgoto out_putf;\n\n\t\tl = f->f_mapping->host->i_bdev->bd_disk->private_data;\n\t\tif (l->lo_state == Lo_unbound) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_putf;\n\t\t}\n\t\tf = l->lo_backing_file;\n\t}\n\n\tmapping = file->f_mapping;\n\tinode = mapping->host;\n\n\terror = -EINVAL;\n\tif (!S_ISREG(inode->i_mode) && !S_ISBLK(inode->i_mode))\n\t\tgoto out_putf;\n\n\tif (!(file->f_mode & FMODE_WRITE) || !(mode & FMODE_WRITE) ||\n\t    !file->f_op->write_iter)\n\t\tlo_flags |= LO_FLAGS_READ_ONLY;\n\n\terror = -EFBIG;\n\tsize = get_loop_size(lo, file);\n\tif ((loff_t)(sector_t)size != size)\n\t\tgoto out_putf;\n\terror = loop_prepare_queue(lo);\n\tif (error)\n\t\tgoto out_putf;\n\n\terror = 0;\n\n\tset_device_ro(bdev, (lo_flags & LO_FLAGS_READ_ONLY) != 0);\n\n\tlo->use_dio = false;\n\tlo->lo_device = bdev;\n\tlo->lo_flags = lo_flags;\n\tlo->lo_backing_file = file;\n\tlo->transfer = NULL;\n\tlo->ioctl = NULL;\n\tlo->lo_sizelimit = 0;\n\tlo->old_gfp_mask = mapping_gfp_mask(mapping);\n\tmapping_set_gfp_mask(mapping, lo->old_gfp_mask & ~(__GFP_IO|__GFP_FS));\n\n\tif (!(lo_flags & LO_FLAGS_READ_ONLY) && file->f_op->fsync)\n\t\tblk_queue_write_cache(lo->lo_queue, true, false);\n\n\tloop_update_dio(lo);\n\tset_capacity(lo->lo_disk, size);\n\tbd_set_size(bdev, size << 9);\n\tloop_sysfs_init(lo);\n\t/* let user-space know about the new size */\n\tkobject_uevent(&disk_to_dev(bdev->bd_disk)->kobj, KOBJ_CHANGE);\n\n\tset_blocksize(bdev, S_ISBLK(inode->i_mode) ?\n\t\t      block_size(inode->i_bdev) : PAGE_SIZE);\n\n\tlo->lo_state = Lo_bound;\n\tif (part_shift)\n\t\tlo->lo_flags |= LO_FLAGS_PARTSCAN;\n\tif (lo->lo_flags & LO_FLAGS_PARTSCAN)\n\t\tloop_reread_partitions(lo, bdev);\n\n\t/* Grab the block_device to prevent its destruction after we\n\t * put /dev/loopXX inode. Later in loop_clr_fd() we bdput(bdev).\n\t */\n\tbdgrab(bdev);\n\treturn 0;\n\n out_putf:\n\tfput(file);\n out:\n\t/* This is safe: open() is still holding a reference. */\n\tmodule_put(THIS_MODULE);\n\treturn error;\n}\n\nstatic int\nloop_release_xfer(struct loop_device *lo)\n{\n\tint err = 0;\n\tstruct loop_func_table *xfer = lo->lo_encryption;\n\n\tif (xfer) {\n\t\tif (xfer->release)\n\t\t\terr = xfer->release(lo);\n\t\tlo->transfer = NULL;\n\t\tlo->lo_encryption = NULL;\n\t\tmodule_put(xfer->owner);\n\t}\n\treturn err;\n}\n\nstatic int\nloop_init_xfer(struct loop_device *lo, struct loop_func_table *xfer,\n\t       const struct loop_info64 *i)\n{\n\tint err = 0;\n\n\tif (xfer) {\n\t\tstruct module *owner = xfer->owner;\n\n\t\tif (!try_module_get(owner))\n\t\t\treturn -EINVAL;\n\t\tif (xfer->init)\n\t\t\terr = xfer->init(lo, i);\n\t\tif (err)\n\t\t\tmodule_put(owner);\n\t\telse\n\t\t\tlo->lo_encryption = xfer;\n\t}\n\treturn err;\n}\n\nstatic int loop_clr_fd(struct loop_device *lo)\n{\n\tstruct file *filp = lo->lo_backing_file;\n\tgfp_t gfp = lo->old_gfp_mask;\n\tstruct block_device *bdev = lo->lo_device;\n\n\tif (lo->lo_state != Lo_bound)\n\t\treturn -ENXIO;\n\n\t/*\n\t * If we've explicitly asked to tear down the loop device,\n\t * and it has an elevated reference count, set it for auto-teardown when\n\t * the last reference goes away. This stops $!~#$@ udev from\n\t * preventing teardown because it decided that it needs to run blkid on\n\t * the loopback device whenever they appear. xfstests is notorious for\n\t * failing tests because blkid via udev races with a losetup\n\t * <dev>/do something like mkfs/losetup -d <dev> causing the losetup -d\n\t * command to fail with EBUSY.\n\t */\n\tif (atomic_read(&lo->lo_refcnt) > 1) {\n\t\tlo->lo_flags |= LO_FLAGS_AUTOCLEAR;\n\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\treturn 0;\n\t}\n\n\tif (filp == NULL)\n\t\treturn -EINVAL;\n\n\t/* freeze request queue during the transition */\n\tblk_mq_freeze_queue(lo->lo_queue);\n\n\tspin_lock_irq(&lo->lo_lock);\n\tlo->lo_state = Lo_rundown;\n\tlo->lo_backing_file = NULL;\n\tspin_unlock_irq(&lo->lo_lock);\n\n\tloop_release_xfer(lo);\n\tlo->transfer = NULL;\n\tlo->ioctl = NULL;\n\tlo->lo_device = NULL;\n\tlo->lo_encryption = NULL;\n\tlo->lo_offset = 0;\n\tlo->lo_sizelimit = 0;\n\tlo->lo_encrypt_key_size = 0;\n\tmemset(lo->lo_encrypt_key, 0, LO_KEY_SIZE);\n\tmemset(lo->lo_crypt_name, 0, LO_NAME_SIZE);\n\tmemset(lo->lo_file_name, 0, LO_NAME_SIZE);\n\tblk_queue_logical_block_size(lo->lo_queue, 512);\n\tblk_queue_physical_block_size(lo->lo_queue, 512);\n\tblk_queue_io_min(lo->lo_queue, 512);\n\tif (bdev) {\n\t\tbdput(bdev);\n\t\tinvalidate_bdev(bdev);\n\t}\n\tset_capacity(lo->lo_disk, 0);\n\tloop_sysfs_exit(lo);\n\tif (bdev) {\n\t\tbd_set_size(bdev, 0);\n\t\t/* let user-space know about this change */\n\t\tkobject_uevent(&disk_to_dev(bdev->bd_disk)->kobj, KOBJ_CHANGE);\n\t}\n\tmapping_set_gfp_mask(filp->f_mapping, gfp);\n\tlo->lo_state = Lo_unbound;\n\t/* This is safe: open() is still holding a reference. */\n\tmodule_put(THIS_MODULE);\n\tblk_mq_unfreeze_queue(lo->lo_queue);\n\n\tif (lo->lo_flags & LO_FLAGS_PARTSCAN && bdev)\n\t\tloop_reread_partitions(lo, bdev);\n\tlo->lo_flags = 0;\n\tif (!part_shift)\n\t\tlo->lo_disk->flags |= GENHD_FL_NO_PART_SCAN;\n\tloop_unprepare_queue(lo);\n\tmutex_unlock(&lo->lo_ctl_mutex);\n\t/*\n\t * Need not hold lo_ctl_mutex to fput backing file.\n\t * Calling fput holding lo_ctl_mutex triggers a circular\n\t * lock dependency possibility warning as fput can take\n\t * bd_mutex which is usually taken before lo_ctl_mutex.\n\t */\n\tfput(filp);\n\treturn 0;\n}\n\nstatic int\nloop_set_status(struct loop_device *lo, const struct loop_info64 *info)\n{\n\tint err;\n\tstruct loop_func_table *xfer;\n\tkuid_t uid = current_uid();\n\n\tif (lo->lo_encrypt_key_size &&\n\t    !uid_eq(lo->lo_key_owner, uid) &&\n\t    !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\tif (lo->lo_state != Lo_bound)\n\t\treturn -ENXIO;\n\tif ((unsigned int) info->lo_encrypt_key_size > LO_KEY_SIZE)\n\t\treturn -EINVAL;\n\n\t/* I/O need to be drained during transfer transition */\n\tblk_mq_freeze_queue(lo->lo_queue);\n\n\terr = loop_release_xfer(lo);\n\tif (err)\n\t\tgoto exit;\n\n\tif (info->lo_encrypt_type) {\n\t\tunsigned int type = info->lo_encrypt_type;\n\n\t\tif (type >= MAX_LO_CRYPT)\n\t\t\treturn -EINVAL;\n\t\txfer = xfer_funcs[type];\n\t\tif (xfer == NULL)\n\t\t\treturn -EINVAL;\n\t} else\n\t\txfer = NULL;\n\n\terr = loop_init_xfer(lo, xfer, info);\n\tif (err)\n\t\tgoto exit;\n\n\tif (lo->lo_offset != info->lo_offset ||\n\t    lo->lo_sizelimit != info->lo_sizelimit) {\n\t\tif (figure_loop_size(lo, info->lo_offset, info->lo_sizelimit)) {\n\t\t\terr = -EFBIG;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\n\tloop_config_discard(lo);\n\n\tmemcpy(lo->lo_file_name, info->lo_file_name, LO_NAME_SIZE);\n\tmemcpy(lo->lo_crypt_name, info->lo_crypt_name, LO_NAME_SIZE);\n\tlo->lo_file_name[LO_NAME_SIZE-1] = 0;\n\tlo->lo_crypt_name[LO_NAME_SIZE-1] = 0;\n\n\tif (!xfer)\n\t\txfer = &none_funcs;\n\tlo->transfer = xfer->transfer;\n\tlo->ioctl = xfer->ioctl;\n\n\tif ((lo->lo_flags & LO_FLAGS_AUTOCLEAR) !=\n\t     (info->lo_flags & LO_FLAGS_AUTOCLEAR))\n\t\tlo->lo_flags ^= LO_FLAGS_AUTOCLEAR;\n\n\tlo->lo_encrypt_key_size = info->lo_encrypt_key_size;\n\tlo->lo_init[0] = info->lo_init[0];\n\tlo->lo_init[1] = info->lo_init[1];\n\tif (info->lo_encrypt_key_size) {\n\t\tmemcpy(lo->lo_encrypt_key, info->lo_encrypt_key,\n\t\t       info->lo_encrypt_key_size);\n\t\tlo->lo_key_owner = uid;\n\t}\n\n\t/* update dio if lo_offset or transfer is changed */\n\t__loop_update_dio(lo, lo->use_dio);\n\n exit:\n\tblk_mq_unfreeze_queue(lo->lo_queue);\n\n\tif (!err && (info->lo_flags & LO_FLAGS_PARTSCAN) &&\n\t     !(lo->lo_flags & LO_FLAGS_PARTSCAN)) {\n\t\tlo->lo_flags |= LO_FLAGS_PARTSCAN;\n\t\tlo->lo_disk->flags &= ~GENHD_FL_NO_PART_SCAN;\n\t\tloop_reread_partitions(lo, lo->lo_device);\n\t}\n\n\treturn err;\n}\n\nstatic int\nloop_get_status(struct loop_device *lo, struct loop_info64 *info)\n{\n\tstruct file *file = lo->lo_backing_file;\n\tstruct kstat stat;\n\tint error;\n\n\tif (lo->lo_state != Lo_bound)\n\t\treturn -ENXIO;\n\terror = vfs_getattr(&file->f_path, &stat,\n\t\t\t    STATX_INO, AT_STATX_SYNC_AS_STAT);\n\tif (error)\n\t\treturn error;\n\tmemset(info, 0, sizeof(*info));\n\tinfo->lo_number = lo->lo_number;\n\tinfo->lo_device = huge_encode_dev(stat.dev);\n\tinfo->lo_inode = stat.ino;\n\tinfo->lo_rdevice = huge_encode_dev(lo->lo_device ? stat.rdev : stat.dev);\n\tinfo->lo_offset = lo->lo_offset;\n\tinfo->lo_sizelimit = lo->lo_sizelimit;\n\tinfo->lo_flags = lo->lo_flags;\n\tmemcpy(info->lo_file_name, lo->lo_file_name, LO_NAME_SIZE);\n\tmemcpy(info->lo_crypt_name, lo->lo_crypt_name, LO_NAME_SIZE);\n\tinfo->lo_encrypt_type =\n\t\tlo->lo_encryption ? lo->lo_encryption->number : 0;\n\tif (lo->lo_encrypt_key_size && capable(CAP_SYS_ADMIN)) {\n\t\tinfo->lo_encrypt_key_size = lo->lo_encrypt_key_size;\n\t\tmemcpy(info->lo_encrypt_key, lo->lo_encrypt_key,\n\t\t       lo->lo_encrypt_key_size);\n\t}\n\treturn 0;\n}\n\nstatic void\nloop_info64_from_old(const struct loop_info *info, struct loop_info64 *info64)\n{\n\tmemset(info64, 0, sizeof(*info64));\n\tinfo64->lo_number = info->lo_number;\n\tinfo64->lo_device = info->lo_device;\n\tinfo64->lo_inode = info->lo_inode;\n\tinfo64->lo_rdevice = info->lo_rdevice;\n\tinfo64->lo_offset = info->lo_offset;\n\tinfo64->lo_sizelimit = 0;\n\tinfo64->lo_encrypt_type = info->lo_encrypt_type;\n\tinfo64->lo_encrypt_key_size = info->lo_encrypt_key_size;\n\tinfo64->lo_flags = info->lo_flags;\n\tinfo64->lo_init[0] = info->lo_init[0];\n\tinfo64->lo_init[1] = info->lo_init[1];\n\tif (info->lo_encrypt_type == LO_CRYPT_CRYPTOAPI)\n\t\tmemcpy(info64->lo_crypt_name, info->lo_name, LO_NAME_SIZE);\n\telse\n\t\tmemcpy(info64->lo_file_name, info->lo_name, LO_NAME_SIZE);\n\tmemcpy(info64->lo_encrypt_key, info->lo_encrypt_key, LO_KEY_SIZE);\n}\n\nstatic int\nloop_info64_to_old(const struct loop_info64 *info64, struct loop_info *info)\n{\n\tmemset(info, 0, sizeof(*info));\n\tinfo->lo_number = info64->lo_number;\n\tinfo->lo_device = info64->lo_device;\n\tinfo->lo_inode = info64->lo_inode;\n\tinfo->lo_rdevice = info64->lo_rdevice;\n\tinfo->lo_offset = info64->lo_offset;\n\tinfo->lo_encrypt_type = info64->lo_encrypt_type;\n\tinfo->lo_encrypt_key_size = info64->lo_encrypt_key_size;\n\tinfo->lo_flags = info64->lo_flags;\n\tinfo->lo_init[0] = info64->lo_init[0];\n\tinfo->lo_init[1] = info64->lo_init[1];\n\tif (info->lo_encrypt_type == LO_CRYPT_CRYPTOAPI)\n\t\tmemcpy(info->lo_name, info64->lo_crypt_name, LO_NAME_SIZE);\n\telse\n\t\tmemcpy(info->lo_name, info64->lo_file_name, LO_NAME_SIZE);\n\tmemcpy(info->lo_encrypt_key, info64->lo_encrypt_key, LO_KEY_SIZE);\n\n\t/* error in case values were truncated */\n\tif (info->lo_device != info64->lo_device ||\n\t    info->lo_rdevice != info64->lo_rdevice ||\n\t    info->lo_inode != info64->lo_inode ||\n\t    info->lo_offset != info64->lo_offset)\n\t\treturn -EOVERFLOW;\n\n\treturn 0;\n}\n\nstatic int\nloop_set_status_old(struct loop_device *lo, const struct loop_info __user *arg)\n{\n\tstruct loop_info info;\n\tstruct loop_info64 info64;\n\n\tif (copy_from_user(&info, arg, sizeof (struct loop_info)))\n\t\treturn -EFAULT;\n\tloop_info64_from_old(&info, &info64);\n\treturn loop_set_status(lo, &info64);\n}\n\nstatic int\nloop_set_status64(struct loop_device *lo, const struct loop_info64 __user *arg)\n{\n\tstruct loop_info64 info64;\n\n\tif (copy_from_user(&info64, arg, sizeof (struct loop_info64)))\n\t\treturn -EFAULT;\n\treturn loop_set_status(lo, &info64);\n}\n\nstatic int\nloop_get_status_old(struct loop_device *lo, struct loop_info __user *arg) {\n\tstruct loop_info info;\n\tstruct loop_info64 info64;\n\tint err = 0;\n\n\tif (!arg)\n\t\terr = -EINVAL;\n\tif (!err)\n\t\terr = loop_get_status(lo, &info64);\n\tif (!err)\n\t\terr = loop_info64_to_old(&info64, &info);\n\tif (!err && copy_to_user(arg, &info, sizeof(info)))\n\t\terr = -EFAULT;\n\n\treturn err;\n}\n\nstatic int\nloop_get_status64(struct loop_device *lo, struct loop_info64 __user *arg) {\n\tstruct loop_info64 info64;\n\tint err = 0;\n\n\tif (!arg)\n\t\terr = -EINVAL;\n\tif (!err)\n\t\terr = loop_get_status(lo, &info64);\n\tif (!err && copy_to_user(arg, &info64, sizeof(info64)))\n\t\terr = -EFAULT;\n\n\treturn err;\n}\n\nstatic int loop_set_capacity(struct loop_device *lo)\n{\n\tif (unlikely(lo->lo_state != Lo_bound))\n\t\treturn -ENXIO;\n\n\treturn figure_loop_size(lo, lo->lo_offset, lo->lo_sizelimit);\n}\n\nstatic int loop_set_dio(struct loop_device *lo, unsigned long arg)\n{\n\tint error = -ENXIO;\n\tif (lo->lo_state != Lo_bound)\n\t\tgoto out;\n\n\t__loop_update_dio(lo, !!arg);\n\tif (lo->use_dio == !!arg)\n\t\treturn 0;\n\terror = -EINVAL;\n out:\n\treturn error;\n}\n\nstatic int loop_set_block_size(struct loop_device *lo, unsigned long arg)\n{\n\tif (lo->lo_state != Lo_bound)\n\t\treturn -ENXIO;\n\n\tif (arg < 512 || arg > PAGE_SIZE || !is_power_of_2(arg))\n\t\treturn -EINVAL;\n\n\tblk_mq_freeze_queue(lo->lo_queue);\n\n\tblk_queue_logical_block_size(lo->lo_queue, arg);\n\tblk_queue_physical_block_size(lo->lo_queue, arg);\n\tblk_queue_io_min(lo->lo_queue, arg);\n\tloop_update_dio(lo);\n\n\tblk_mq_unfreeze_queue(lo->lo_queue);\n\n\treturn 0;\n}\n\nstatic int lo_ioctl(struct block_device *bdev, fmode_t mode,\n\tunsigned int cmd, unsigned long arg)\n{\n\tstruct loop_device *lo = bdev->bd_disk->private_data;\n\tint err;\n\n\tmutex_lock_nested(&lo->lo_ctl_mutex, 1);\n\tswitch (cmd) {\n\tcase LOOP_SET_FD:\n\t\terr = loop_set_fd(lo, mode, bdev, arg);\n\t\tbreak;\n\tcase LOOP_CHANGE_FD:\n\t\terr = loop_change_fd(lo, bdev, arg);\n\t\tbreak;\n\tcase LOOP_CLR_FD:\n\t\t/* loop_clr_fd would have unlocked lo_ctl_mutex on success */\n\t\terr = loop_clr_fd(lo);\n\t\tif (!err)\n\t\t\tgoto out_unlocked;\n\t\tbreak;\n\tcase LOOP_SET_STATUS:\n\t\terr = -EPERM;\n\t\tif ((mode & FMODE_WRITE) || capable(CAP_SYS_ADMIN))\n\t\t\terr = loop_set_status_old(lo,\n\t\t\t\t\t(struct loop_info __user *)arg);\n\t\tbreak;\n\tcase LOOP_GET_STATUS:\n\t\terr = loop_get_status_old(lo, (struct loop_info __user *) arg);\n\t\tbreak;\n\tcase LOOP_SET_STATUS64:\n\t\terr = -EPERM;\n\t\tif ((mode & FMODE_WRITE) || capable(CAP_SYS_ADMIN))\n\t\t\terr = loop_set_status64(lo,\n\t\t\t\t\t(struct loop_info64 __user *) arg);\n\t\tbreak;\n\tcase LOOP_GET_STATUS64:\n\t\terr = loop_get_status64(lo, (struct loop_info64 __user *) arg);\n\t\tbreak;\n\tcase LOOP_SET_CAPACITY:\n\t\terr = -EPERM;\n\t\tif ((mode & FMODE_WRITE) || capable(CAP_SYS_ADMIN))\n\t\t\terr = loop_set_capacity(lo);\n\t\tbreak;\n\tcase LOOP_SET_DIRECT_IO:\n\t\terr = -EPERM;\n\t\tif ((mode & FMODE_WRITE) || capable(CAP_SYS_ADMIN))\n\t\t\terr = loop_set_dio(lo, arg);\n\t\tbreak;\n\tcase LOOP_SET_BLOCK_SIZE:\n\t\terr = -EPERM;\n\t\tif ((mode & FMODE_WRITE) || capable(CAP_SYS_ADMIN))\n\t\t\terr = loop_set_block_size(lo, arg);\n\t\tbreak;\n\tdefault:\n\t\terr = lo->ioctl ? lo->ioctl(lo, cmd, arg) : -EINVAL;\n\t}\n\tmutex_unlock(&lo->lo_ctl_mutex);\n\nout_unlocked:\n\treturn err;\n}\n\n#ifdef CONFIG_COMPAT\nstruct compat_loop_info {\n\tcompat_int_t\tlo_number;      /* ioctl r/o */\n\tcompat_dev_t\tlo_device;      /* ioctl r/o */\n\tcompat_ulong_t\tlo_inode;       /* ioctl r/o */\n\tcompat_dev_t\tlo_rdevice;     /* ioctl r/o */\n\tcompat_int_t\tlo_offset;\n\tcompat_int_t\tlo_encrypt_type;\n\tcompat_int_t\tlo_encrypt_key_size;    /* ioctl w/o */\n\tcompat_int_t\tlo_flags;       /* ioctl r/o */\n\tchar\t\tlo_name[LO_NAME_SIZE];\n\tunsigned char\tlo_encrypt_key[LO_KEY_SIZE]; /* ioctl w/o */\n\tcompat_ulong_t\tlo_init[2];\n\tchar\t\treserved[4];\n};\n\n/*\n * Transfer 32-bit compatibility structure in userspace to 64-bit loop info\n * - noinlined to reduce stack space usage in main part of driver\n */\nstatic noinline int\nloop_info64_from_compat(const struct compat_loop_info __user *arg,\n\t\t\tstruct loop_info64 *info64)\n{\n\tstruct compat_loop_info info;\n\n\tif (copy_from_user(&info, arg, sizeof(info)))\n\t\treturn -EFAULT;\n\n\tmemset(info64, 0, sizeof(*info64));\n\tinfo64->lo_number = info.lo_number;\n\tinfo64->lo_device = info.lo_device;\n\tinfo64->lo_inode = info.lo_inode;\n\tinfo64->lo_rdevice = info.lo_rdevice;\n\tinfo64->lo_offset = info.lo_offset;\n\tinfo64->lo_sizelimit = 0;\n\tinfo64->lo_encrypt_type = info.lo_encrypt_type;\n\tinfo64->lo_encrypt_key_size = info.lo_encrypt_key_size;\n\tinfo64->lo_flags = info.lo_flags;\n\tinfo64->lo_init[0] = info.lo_init[0];\n\tinfo64->lo_init[1] = info.lo_init[1];\n\tif (info.lo_encrypt_type == LO_CRYPT_CRYPTOAPI)\n\t\tmemcpy(info64->lo_crypt_name, info.lo_name, LO_NAME_SIZE);\n\telse\n\t\tmemcpy(info64->lo_file_name, info.lo_name, LO_NAME_SIZE);\n\tmemcpy(info64->lo_encrypt_key, info.lo_encrypt_key, LO_KEY_SIZE);\n\treturn 0;\n}\n\n/*\n * Transfer 64-bit loop info to 32-bit compatibility structure in userspace\n * - noinlined to reduce stack space usage in main part of driver\n */\nstatic noinline int\nloop_info64_to_compat(const struct loop_info64 *info64,\n\t\t      struct compat_loop_info __user *arg)\n{\n\tstruct compat_loop_info info;\n\n\tmemset(&info, 0, sizeof(info));\n\tinfo.lo_number = info64->lo_number;\n\tinfo.lo_device = info64->lo_device;\n\tinfo.lo_inode = info64->lo_inode;\n\tinfo.lo_rdevice = info64->lo_rdevice;\n\tinfo.lo_offset = info64->lo_offset;\n\tinfo.lo_encrypt_type = info64->lo_encrypt_type;\n\tinfo.lo_encrypt_key_size = info64->lo_encrypt_key_size;\n\tinfo.lo_flags = info64->lo_flags;\n\tinfo.lo_init[0] = info64->lo_init[0];\n\tinfo.lo_init[1] = info64->lo_init[1];\n\tif (info.lo_encrypt_type == LO_CRYPT_CRYPTOAPI)\n\t\tmemcpy(info.lo_name, info64->lo_crypt_name, LO_NAME_SIZE);\n\telse\n\t\tmemcpy(info.lo_name, info64->lo_file_name, LO_NAME_SIZE);\n\tmemcpy(info.lo_encrypt_key, info64->lo_encrypt_key, LO_KEY_SIZE);\n\n\t/* error in case values were truncated */\n\tif (info.lo_device != info64->lo_device ||\n\t    info.lo_rdevice != info64->lo_rdevice ||\n\t    info.lo_inode != info64->lo_inode ||\n\t    info.lo_offset != info64->lo_offset ||\n\t    info.lo_init[0] != info64->lo_init[0] ||\n\t    info.lo_init[1] != info64->lo_init[1])\n\t\treturn -EOVERFLOW;\n\n\tif (copy_to_user(arg, &info, sizeof(info)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int\nloop_set_status_compat(struct loop_device *lo,\n\t\t       const struct compat_loop_info __user *arg)\n{\n\tstruct loop_info64 info64;\n\tint ret;\n\n\tret = loop_info64_from_compat(arg, &info64);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn loop_set_status(lo, &info64);\n}\n\nstatic int\nloop_get_status_compat(struct loop_device *lo,\n\t\t       struct compat_loop_info __user *arg)\n{\n\tstruct loop_info64 info64;\n\tint err = 0;\n\n\tif (!arg)\n\t\terr = -EINVAL;\n\tif (!err)\n\t\terr = loop_get_status(lo, &info64);\n\tif (!err)\n\t\terr = loop_info64_to_compat(&info64, arg);\n\treturn err;\n}\n\nstatic int lo_compat_ioctl(struct block_device *bdev, fmode_t mode,\n\t\t\t   unsigned int cmd, unsigned long arg)\n{\n\tstruct loop_device *lo = bdev->bd_disk->private_data;\n\tint err;\n\n\tswitch(cmd) {\n\tcase LOOP_SET_STATUS:\n\t\tmutex_lock(&lo->lo_ctl_mutex);\n\t\terr = loop_set_status_compat(\n\t\t\tlo, (const struct compat_loop_info __user *) arg);\n\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\tbreak;\n\tcase LOOP_GET_STATUS:\n\t\tmutex_lock(&lo->lo_ctl_mutex);\n\t\terr = loop_get_status_compat(\n\t\t\tlo, (struct compat_loop_info __user *) arg);\n\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\tbreak;\n\tcase LOOP_SET_CAPACITY:\n\tcase LOOP_CLR_FD:\n\tcase LOOP_GET_STATUS64:\n\tcase LOOP_SET_STATUS64:\n\t\targ = (unsigned long) compat_ptr(arg);\n\tcase LOOP_SET_FD:\n\tcase LOOP_CHANGE_FD:\n\t\terr = lo_ioctl(bdev, mode, cmd, arg);\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOIOCTLCMD;\n\t\tbreak;\n\t}\n\treturn err;\n}\n#endif\n\nstatic int lo_open(struct block_device *bdev, fmode_t mode)\n{\n\tstruct loop_device *lo;\n\tint err = 0;\n\n\tmutex_lock(&loop_index_mutex);\n\tlo = bdev->bd_disk->private_data;\n\tif (!lo) {\n\t\terr = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tatomic_inc(&lo->lo_refcnt);\nout:\n\tmutex_unlock(&loop_index_mutex);\n\treturn err;\n}\n\nstatic void __lo_release(struct loop_device *lo)\n{\n\tint err;\n\n\tif (atomic_dec_return(&lo->lo_refcnt))\n\t\treturn;\n\n\tmutex_lock(&lo->lo_ctl_mutex);\n\tif (lo->lo_flags & LO_FLAGS_AUTOCLEAR) {\n\t\t/*\n\t\t * In autoclear mode, stop the loop thread\n\t\t * and remove configuration after last close.\n\t\t */\n\t\terr = loop_clr_fd(lo);\n\t\tif (!err)\n\t\t\treturn;\n\t} else if (lo->lo_state == Lo_bound) {\n\t\t/*\n\t\t * Otherwise keep thread (if running) and config,\n\t\t * but flush possible ongoing bios in thread.\n\t\t */\n\t\tblk_mq_freeze_queue(lo->lo_queue);\n\t\tblk_mq_unfreeze_queue(lo->lo_queue);\n\t}\n\n\tmutex_unlock(&lo->lo_ctl_mutex);\n}\n\nstatic void lo_release(struct gendisk *disk, fmode_t mode)\n{\n\tmutex_lock(&loop_index_mutex);\n\t__lo_release(disk->private_data);\n\tmutex_unlock(&loop_index_mutex);\n}\n\nstatic const struct block_device_operations lo_fops = {\n\t.owner =\tTHIS_MODULE,\n\t.open =\t\tlo_open,\n\t.release =\tlo_release,\n\t.ioctl =\tlo_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl =\tlo_compat_ioctl,\n#endif\n};\n\n/*\n * And now the modules code and kernel interface.\n */\nstatic int max_loop;\nmodule_param(max_loop, int, S_IRUGO);\nMODULE_PARM_DESC(max_loop, \"Maximum number of loop devices\");\nmodule_param(max_part, int, S_IRUGO);\nMODULE_PARM_DESC(max_part, \"Maximum number of partitions per loop device\");\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_BLOCKDEV_MAJOR(LOOP_MAJOR);\n\nint loop_register_transfer(struct loop_func_table *funcs)\n{\n\tunsigned int n = funcs->number;\n\n\tif (n >= MAX_LO_CRYPT || xfer_funcs[n])\n\t\treturn -EINVAL;\n\txfer_funcs[n] = funcs;\n\treturn 0;\n}\n\nstatic int unregister_transfer_cb(int id, void *ptr, void *data)\n{\n\tstruct loop_device *lo = ptr;\n\tstruct loop_func_table *xfer = data;\n\n\tmutex_lock(&lo->lo_ctl_mutex);\n\tif (lo->lo_encryption == xfer)\n\t\tloop_release_xfer(lo);\n\tmutex_unlock(&lo->lo_ctl_mutex);\n\treturn 0;\n}\n\nint loop_unregister_transfer(int number)\n{\n\tunsigned int n = number;\n\tstruct loop_func_table *xfer;\n\n\tif (n == 0 || n >= MAX_LO_CRYPT || (xfer = xfer_funcs[n]) == NULL)\n\t\treturn -EINVAL;\n\n\txfer_funcs[n] = NULL;\n\tidr_for_each(&loop_index_idr, &unregister_transfer_cb, xfer);\n\treturn 0;\n}\n\nEXPORT_SYMBOL(loop_register_transfer);\nEXPORT_SYMBOL(loop_unregister_transfer);\n\nstatic blk_status_t loop_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\tconst struct blk_mq_queue_data *bd)\n{\n\tstruct loop_cmd *cmd = blk_mq_rq_to_pdu(bd->rq);\n\tstruct loop_device *lo = cmd->rq->q->queuedata;\n\n\tblk_mq_start_request(bd->rq);\n\n\tif (lo->lo_state != Lo_bound)\n\t\treturn BLK_STS_IOERR;\n\n\tswitch (req_op(cmd->rq)) {\n\tcase REQ_OP_FLUSH:\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tcmd->use_aio = false;\n\t\tbreak;\n\tdefault:\n\t\tcmd->use_aio = lo->use_dio;\n\t\tbreak;\n\t}\n\n\t/* always use the first bio's css */\n#ifdef CONFIG_BLK_CGROUP\n\tif (cmd->use_aio && cmd->rq->bio && cmd->rq->bio->bi_css) {\n\t\tcmd->css = cmd->rq->bio->bi_css;\n\t\tcss_get(cmd->css);\n\t} else\n#endif\n\t\tcmd->css = NULL;\n\tkthread_queue_work(&lo->worker, &cmd->work);\n\n\treturn BLK_STS_OK;\n}\n\nstatic void loop_handle_cmd(struct loop_cmd *cmd)\n{\n\tconst bool write = op_is_write(req_op(cmd->rq));\n\tstruct loop_device *lo = cmd->rq->q->queuedata;\n\tint ret = 0;\n\n\tif (write && (lo->lo_flags & LO_FLAGS_READ_ONLY)) {\n\t\tret = -EIO;\n\t\tgoto failed;\n\t}\n\n\tret = do_req_filebacked(lo, cmd->rq);\n failed:\n\t/* complete non-aio request */\n\tif (!cmd->use_aio || ret) {\n\t\tcmd->ret = ret ? -EIO : 0;\n\t\tblk_mq_complete_request(cmd->rq);\n\t}\n}\n\nstatic void loop_queue_work(struct kthread_work *work)\n{\n\tstruct loop_cmd *cmd =\n\t\tcontainer_of(work, struct loop_cmd, work);\n\n\tloop_handle_cmd(cmd);\n}\n\nstatic int loop_init_request(struct blk_mq_tag_set *set, struct request *rq,\n\t\tunsigned int hctx_idx, unsigned int numa_node)\n{\n\tstruct loop_cmd *cmd = blk_mq_rq_to_pdu(rq);\n\n\tcmd->rq = rq;\n\tkthread_init_work(&cmd->work, loop_queue_work);\n\n\treturn 0;\n}\n\nstatic const struct blk_mq_ops loop_mq_ops = {\n\t.queue_rq       = loop_queue_rq,\n\t.init_request\t= loop_init_request,\n\t.complete\t= lo_complete_rq,\n};\n\nstatic int loop_add(struct loop_device **l, int i)\n{\n\tstruct loop_device *lo;\n\tstruct gendisk *disk;\n\tint err;\n\n\terr = -ENOMEM;\n\tlo = kzalloc(sizeof(*lo), GFP_KERNEL);\n\tif (!lo)\n\t\tgoto out;\n\n\tlo->lo_state = Lo_unbound;\n\n\t/* allocate id, if @id >= 0, we're requesting that specific id */\n\tif (i >= 0) {\n\t\terr = idr_alloc(&loop_index_idr, lo, i, i + 1, GFP_KERNEL);\n\t\tif (err == -ENOSPC)\n\t\t\terr = -EEXIST;\n\t} else {\n\t\terr = idr_alloc(&loop_index_idr, lo, 0, 0, GFP_KERNEL);\n\t}\n\tif (err < 0)\n\t\tgoto out_free_dev;\n\ti = err;\n\n\terr = -ENOMEM;\n\tlo->tag_set.ops = &loop_mq_ops;\n\tlo->tag_set.nr_hw_queues = 1;\n\tlo->tag_set.queue_depth = 128;\n\tlo->tag_set.numa_node = NUMA_NO_NODE;\n\tlo->tag_set.cmd_size = sizeof(struct loop_cmd);\n\tlo->tag_set.flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_SG_MERGE;\n\tlo->tag_set.driver_data = lo;\n\n\terr = blk_mq_alloc_tag_set(&lo->tag_set);\n\tif (err)\n\t\tgoto out_free_idr;\n\n\tlo->lo_queue = blk_mq_init_queue(&lo->tag_set);\n\tif (IS_ERR_OR_NULL(lo->lo_queue)) {\n\t\terr = PTR_ERR(lo->lo_queue);\n\t\tgoto out_cleanup_tags;\n\t}\n\tlo->lo_queue->queuedata = lo;\n\n\tblk_queue_max_hw_sectors(lo->lo_queue, BLK_DEF_MAX_SECTORS);\n\n\t/*\n\t * By default, we do buffer IO, so it doesn't make sense to enable\n\t * merge because the I/O submitted to backing file is handled page by\n\t * page. For directio mode, merge does help to dispatch bigger request\n\t * to underlayer disk. We will enable merge once directio is enabled.\n\t */\n\tqueue_flag_set_unlocked(QUEUE_FLAG_NOMERGES, lo->lo_queue);\n\n\terr = -ENOMEM;\n\tdisk = lo->lo_disk = alloc_disk(1 << part_shift);\n\tif (!disk)\n\t\tgoto out_free_queue;\n\n\t/*\n\t * Disable partition scanning by default. The in-kernel partition\n\t * scanning can be requested individually per-device during its\n\t * setup. Userspace can always add and remove partitions from all\n\t * devices. The needed partition minors are allocated from the\n\t * extended minor space, the main loop device numbers will continue\n\t * to match the loop minors, regardless of the number of partitions\n\t * used.\n\t *\n\t * If max_part is given, partition scanning is globally enabled for\n\t * all loop devices. The minors for the main loop devices will be\n\t * multiples of max_part.\n\t *\n\t * Note: Global-for-all-devices, set-only-at-init, read-only module\n\t * parameteters like 'max_loop' and 'max_part' make things needlessly\n\t * complicated, are too static, inflexible and may surprise\n\t * userspace tools. Parameters like this in general should be avoided.\n\t */\n\tif (!part_shift)\n\t\tdisk->flags |= GENHD_FL_NO_PART_SCAN;\n\tdisk->flags |= GENHD_FL_EXT_DEVT;\n\tmutex_init(&lo->lo_ctl_mutex);\n\tatomic_set(&lo->lo_refcnt, 0);\n\tlo->lo_number\t\t= i;\n\tspin_lock_init(&lo->lo_lock);\n\tdisk->major\t\t= LOOP_MAJOR;\n\tdisk->first_minor\t= i << part_shift;\n\tdisk->fops\t\t= &lo_fops;\n\tdisk->private_data\t= lo;\n\tdisk->queue\t\t= lo->lo_queue;\n\tsprintf(disk->disk_name, \"loop%d\", i);\n\tadd_disk(disk);\n\t*l = lo;\n\treturn lo->lo_number;\n\nout_free_queue:\n\tblk_cleanup_queue(lo->lo_queue);\nout_cleanup_tags:\n\tblk_mq_free_tag_set(&lo->tag_set);\nout_free_idr:\n\tidr_remove(&loop_index_idr, i);\nout_free_dev:\n\tkfree(lo);\nout:\n\treturn err;\n}\n\nstatic void loop_remove(struct loop_device *lo)\n{\n\tblk_cleanup_queue(lo->lo_queue);\n\tdel_gendisk(lo->lo_disk);\n\tblk_mq_free_tag_set(&lo->tag_set);\n\tput_disk(lo->lo_disk);\n\tkfree(lo);\n}\n\nstatic int find_free_cb(int id, void *ptr, void *data)\n{\n\tstruct loop_device *lo = ptr;\n\tstruct loop_device **l = data;\n\n\tif (lo->lo_state == Lo_unbound) {\n\t\t*l = lo;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int loop_lookup(struct loop_device **l, int i)\n{\n\tstruct loop_device *lo;\n\tint ret = -ENODEV;\n\n\tif (i < 0) {\n\t\tint err;\n\n\t\terr = idr_for_each(&loop_index_idr, &find_free_cb, &lo);\n\t\tif (err == 1) {\n\t\t\t*l = lo;\n\t\t\tret = lo->lo_number;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t/* lookup and return a specific i */\n\tlo = idr_find(&loop_index_idr, i);\n\tif (lo) {\n\t\t*l = lo;\n\t\tret = lo->lo_number;\n\t}\nout:\n\treturn ret;\n}\n\nstatic struct kobject *loop_probe(dev_t dev, int *part, void *data)\n{\n\tstruct loop_device *lo;\n\tstruct kobject *kobj;\n\tint err;\n\n\tmutex_lock(&loop_index_mutex);\n\terr = loop_lookup(&lo, MINOR(dev) >> part_shift);\n\tif (err < 0)\n\t\terr = loop_add(&lo, MINOR(dev) >> part_shift);\n\tif (err < 0)\n\t\tkobj = NULL;\n\telse\n\t\tkobj = get_disk(lo->lo_disk);\n\tmutex_unlock(&loop_index_mutex);\n\n\t*part = 0;\n\treturn kobj;\n}\n\nstatic long loop_control_ioctl(struct file *file, unsigned int cmd,\n\t\t\t       unsigned long parm)\n{\n\tstruct loop_device *lo;\n\tint ret = -ENOSYS;\n\n\tmutex_lock(&loop_index_mutex);\n\tswitch (cmd) {\n\tcase LOOP_CTL_ADD:\n\t\tret = loop_lookup(&lo, parm);\n\t\tif (ret >= 0) {\n\t\t\tret = -EEXIST;\n\t\t\tbreak;\n\t\t}\n\t\tret = loop_add(&lo, parm);\n\t\tbreak;\n\tcase LOOP_CTL_REMOVE:\n\t\tret = loop_lookup(&lo, parm);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tmutex_lock(&lo->lo_ctl_mutex);\n\t\tif (lo->lo_state != Lo_unbound) {\n\t\t\tret = -EBUSY;\n\t\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\t\tbreak;\n\t\t}\n\t\tif (atomic_read(&lo->lo_refcnt) > 0) {\n\t\t\tret = -EBUSY;\n\t\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\t\tbreak;\n\t\t}\n\t\tlo->lo_disk->private_data = NULL;\n\t\tmutex_unlock(&lo->lo_ctl_mutex);\n\t\tidr_remove(&loop_index_idr, lo->lo_number);\n\t\tloop_remove(lo);\n\t\tbreak;\n\tcase LOOP_CTL_GET_FREE:\n\t\tret = loop_lookup(&lo, -1);\n\t\tif (ret >= 0)\n\t\t\tbreak;\n\t\tret = loop_add(&lo, -1);\n\t}\n\tmutex_unlock(&loop_index_mutex);\n\n\treturn ret;\n}\n\nstatic const struct file_operations loop_ctl_fops = {\n\t.open\t\t= nonseekable_open,\n\t.unlocked_ioctl\t= loop_control_ioctl,\n\t.compat_ioctl\t= loop_control_ioctl,\n\t.owner\t\t= THIS_MODULE,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic struct miscdevice loop_misc = {\n\t.minor\t\t= LOOP_CTRL_MINOR,\n\t.name\t\t= \"loop-control\",\n\t.fops\t\t= &loop_ctl_fops,\n};\n\nMODULE_ALIAS_MISCDEV(LOOP_CTRL_MINOR);\nMODULE_ALIAS(\"devname:loop-control\");\n\nstatic int __init loop_init(void)\n{\n\tint i, nr;\n\tunsigned long range;\n\tstruct loop_device *lo;\n\tint err;\n\n\tpart_shift = 0;\n\tif (max_part > 0) {\n\t\tpart_shift = fls(max_part);\n\n\t\t/*\n\t\t * Adjust max_part according to part_shift as it is exported\n\t\t * to user space so that user can decide correct minor number\n\t\t * if [s]he want to create more devices.\n\t\t *\n\t\t * Note that -1 is required because partition 0 is reserved\n\t\t * for the whole disk.\n\t\t */\n\t\tmax_part = (1UL << part_shift) - 1;\n\t}\n\n\tif ((1UL << part_shift) > DISK_MAX_PARTS) {\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tif (max_loop > 1UL << (MINORBITS - part_shift)) {\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\t/*\n\t * If max_loop is specified, create that many devices upfront.\n\t * This also becomes a hard limit. If max_loop is not specified,\n\t * create CONFIG_BLK_DEV_LOOP_MIN_COUNT loop devices at module\n\t * init time. Loop devices can be requested on-demand with the\n\t * /dev/loop-control interface, or be instantiated by accessing\n\t * a 'dead' device node.\n\t */\n\tif (max_loop) {\n\t\tnr = max_loop;\n\t\trange = max_loop << part_shift;\n\t} else {\n\t\tnr = CONFIG_BLK_DEV_LOOP_MIN_COUNT;\n\t\trange = 1UL << MINORBITS;\n\t}\n\n\terr = misc_register(&loop_misc);\n\tif (err < 0)\n\t\tgoto err_out;\n\n\n\tif (register_blkdev(LOOP_MAJOR, \"loop\")) {\n\t\terr = -EIO;\n\t\tgoto misc_out;\n\t}\n\n\tblk_register_region(MKDEV(LOOP_MAJOR, 0), range,\n\t\t\t\t  THIS_MODULE, loop_probe, NULL, NULL);\n\n\t/* pre-create number of devices given by config or max_loop */\n\tmutex_lock(&loop_index_mutex);\n\tfor (i = 0; i < nr; i++)\n\t\tloop_add(&lo, i);\n\tmutex_unlock(&loop_index_mutex);\n\n\tprintk(KERN_INFO \"loop: module loaded\\n\");\n\treturn 0;\n\nmisc_out:\n\tmisc_deregister(&loop_misc);\nerr_out:\n\treturn err;\n}\n\nstatic int loop_exit_cb(int id, void *ptr, void *data)\n{\n\tstruct loop_device *lo = ptr;\n\n\tloop_remove(lo);\n\treturn 0;\n}\n\nstatic void __exit loop_exit(void)\n{\n\tunsigned long range;\n\n\trange = max_loop ? max_loop << part_shift : 1UL << MINORBITS;\n\n\tidr_for_each(&loop_index_idr, &loop_exit_cb, NULL);\n\tidr_destroy(&loop_index_idr);\n\n\tblk_unregister_region(MKDEV(LOOP_MAJOR, 0), range);\n\tunregister_blkdev(LOOP_MAJOR, \"loop\");\n\n\tmisc_deregister(&loop_misc);\n}\n\nmodule_init(loop_init);\nmodule_exit(loop_exit);\n\n#ifndef MODULE\nstatic int __init max_loop_setup(char *str)\n{\n\tmax_loop = simple_strtol(str, NULL, 0);\n\treturn 1;\n}\n\n__setup(\"max_loop=\", max_loop_setup);\n#endif\n"], "filenames": ["drivers/block/loop.c"], "buggy_code_start_loc": [1584], "buggy_code_end_loc": [1610], "fixing_code_start_loc": [1584], "fixing_code_end_loc": [1617], "type": "CWE-362", "message": "In the Linux kernel through 4.14.13, drivers/block/loop.c mishandles lo_release serialization, which allows attackers to cause a denial of service (__lock_acquire use-after-free) or possibly have unspecified other impact.", "other": {"cve": {"id": "CVE-2018-5344", "sourceIdentifier": "cve@mitre.org", "published": "2018-01-12T09:29:00.540", "lastModified": "2020-08-24T17:37:01.140", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In the Linux kernel through 4.14.13, drivers/block/loop.c mishandles lo_release serialization, which allows attackers to cause a denial of service (__lock_acquire use-after-free) or possibly have unspecified other impact."}, {"lang": "es", "value": "En el kernel de Linux hasta la versi\u00f3n 4.14.13, drivers/block/loop.c gestiona de manera incorrecta la serializaci\u00f3n de lo_release, lo que permite que atacantes provoquen una denegaci\u00f3n de servicio (uso de memoria previamente liberada de __lock_acquire) o, posiblemente, otro impacto sin especificar."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.8, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.8, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 4.6}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.4, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-362"}, {"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.14.13", "matchCriteriaId": "664475BB-B9FD-4E3F-AFCF-7E11350BE23E"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:esm:*:*:*", "matchCriteriaId": "8D305F7A-D159-4716-AB26-5E38BB5CD991"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:17.10:*:*:*:*:*:*:*", "matchCriteriaId": "9070C9D8-A14A-467F-8253-33B966C16886"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:redhat:virtualization:4.0:*:*:*:*:*:*:*", "matchCriteriaId": "6BBD7A51-0590-4DDF-8249-5AFA8D645CB6"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_desktop:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "33C068A4-3780-4EAB-A937-6082DF847564"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_server:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "51EF4996-72F4-4FA4-814F-F5991E7A8318"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_workstation:7.0:*:*:*:*:*:*:*", "matchCriteriaId": "825ECE2D-E232-46E0-A047-074B34DB1E97"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=ae6650163c66a7eff1acd6eb8b0f752dcfa8eba5", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/102503", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:2948", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3083", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:3096", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/ae6650163c66a7eff1acd6eb8b0f752dcfa8eba5", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3583-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3583-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3617-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3617-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3617-3/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3619-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3619-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3632-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/ae6650163c66a7eff1acd6eb8b0f752dcfa8eba5"}}