{"buggy_code": ["/*\n *  linux/mm/oom_kill.c\n * \n *  Copyright (C)  1998,2000  Rik van Riel\n *\tThanks go out to Claus Fischer for some serious inspiration and\n *\tfor goading me into coding this file...\n *  Copyright (C)  2010  Google, Inc.\n *\tRewritten by David Rientjes\n *\n *  The routines in this file are used to kill a process when\n *  we're seriously out of memory. This gets called from __alloc_pages()\n *  in mm/page_alloc.c when we really run out of memory.\n *\n *  Since we won't call these routines often (on a well-configured\n *  machine) this file will double as a 'coding guide' and a signpost\n *  for newbie kernel hackers. It features several pointers to major\n *  kernel subsystems and hints as to where to find out what things do.\n */\n\n#include <linux/oom.h>\n#include <linux/mm.h>\n#include <linux/err.h>\n#include <linux/gfp.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/task.h>\n#include <linux/swap.h>\n#include <linux/timex.h>\n#include <linux/jiffies.h>\n#include <linux/cpuset.h>\n#include <linux/export.h>\n#include <linux/notifier.h>\n#include <linux/memcontrol.h>\n#include <linux/mempolicy.h>\n#include <linux/security.h>\n#include <linux/ptrace.h>\n#include <linux/freezer.h>\n#include <linux/ftrace.h>\n#include <linux/ratelimit.h>\n#include <linux/kthread.h>\n#include <linux/init.h>\n#include <linux/mmu_notifier.h>\n\n#include <asm/tlb.h>\n#include \"internal.h\"\n#include \"slab.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/oom.h>\n\nint sysctl_panic_on_oom;\nint sysctl_oom_kill_allocating_task;\nint sysctl_oom_dump_tasks = 1;\n\nDEFINE_MUTEX(oom_lock);\n\n#ifdef CONFIG_NUMA\n/**\n * has_intersects_mems_allowed() - check task eligiblity for kill\n * @start: task struct of which task to consider\n * @mask: nodemask passed to page allocator for mempolicy ooms\n *\n * Task eligibility is determined by whether or not a candidate task, @tsk,\n * shares the same mempolicy nodes as current if it is bound by such a policy\n * and whether or not it has the same set of allowed cpuset nodes.\n */\nstatic bool has_intersects_mems_allowed(struct task_struct *start,\n\t\t\t\t\tconst nodemask_t *mask)\n{\n\tstruct task_struct *tsk;\n\tbool ret = false;\n\n\trcu_read_lock();\n\tfor_each_thread(start, tsk) {\n\t\tif (mask) {\n\t\t\t/*\n\t\t\t * If this is a mempolicy constrained oom, tsk's\n\t\t\t * cpuset is irrelevant.  Only return true if its\n\t\t\t * mempolicy intersects current, otherwise it may be\n\t\t\t * needlessly killed.\n\t\t\t */\n\t\t\tret = mempolicy_nodemask_intersects(tsk, mask);\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is not a mempolicy constrained oom, so only\n\t\t\t * check the mems of tsk's cpuset.\n\t\t\t */\n\t\t\tret = cpuset_mems_allowed_intersects(current, tsk);\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n#else\nstatic bool has_intersects_mems_allowed(struct task_struct *tsk,\n\t\t\t\t\tconst nodemask_t *mask)\n{\n\treturn true;\n}\n#endif /* CONFIG_NUMA */\n\n/*\n * The process p may have detached its own ->mm while exiting or through\n * use_mm(), but one or more of its subthreads may still have a valid\n * pointer.  Return p, or any of its subthreads with a valid ->mm, with\n * task_lock() held.\n */\nstruct task_struct *find_lock_task_mm(struct task_struct *p)\n{\n\tstruct task_struct *t;\n\n\trcu_read_lock();\n\n\tfor_each_thread(p, t) {\n\t\ttask_lock(t);\n\t\tif (likely(t->mm))\n\t\t\tgoto found;\n\t\ttask_unlock(t);\n\t}\n\tt = NULL;\nfound:\n\trcu_read_unlock();\n\n\treturn t;\n}\n\n/*\n * order == -1 means the oom kill is required by sysrq, otherwise only\n * for display purposes.\n */\nstatic inline bool is_sysrq_oom(struct oom_control *oc)\n{\n\treturn oc->order == -1;\n}\n\nstatic inline bool is_memcg_oom(struct oom_control *oc)\n{\n\treturn oc->memcg != NULL;\n}\n\n/* return true if the task is not adequate as candidate victim task. */\nstatic bool oom_unkillable_task(struct task_struct *p,\n\t\tstruct mem_cgroup *memcg, const nodemask_t *nodemask)\n{\n\tif (is_global_init(p))\n\t\treturn true;\n\tif (p->flags & PF_KTHREAD)\n\t\treturn true;\n\n\t/* When mem_cgroup_out_of_memory() and p is not member of the group */\n\tif (memcg && !task_in_mem_cgroup(p, memcg))\n\t\treturn true;\n\n\t/* p may not have freeable memory in nodemask */\n\tif (!has_intersects_mems_allowed(p, nodemask))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Print out unreclaimble slabs info when unreclaimable slabs amount is greater\n * than all user memory (LRU pages)\n */\nstatic bool is_dump_unreclaim_slabs(void)\n{\n\tunsigned long nr_lru;\n\n\tnr_lru = global_node_page_state(NR_ACTIVE_ANON) +\n\t\t global_node_page_state(NR_INACTIVE_ANON) +\n\t\t global_node_page_state(NR_ACTIVE_FILE) +\n\t\t global_node_page_state(NR_INACTIVE_FILE) +\n\t\t global_node_page_state(NR_ISOLATED_ANON) +\n\t\t global_node_page_state(NR_ISOLATED_FILE) +\n\t\t global_node_page_state(NR_UNEVICTABLE);\n\n\treturn (global_node_page_state(NR_SLAB_UNRECLAIMABLE) > nr_lru);\n}\n\n/**\n * oom_badness - heuristic function to determine which candidate task to kill\n * @p: task struct of which task we should calculate\n * @totalpages: total present RAM allowed for page allocation\n *\n * The heuristic for determining which task to kill is made to be as simple and\n * predictable as possible.  The goal is to return the highest value for the\n * task consuming the most memory to avoid subsequent oom failures.\n */\nunsigned long oom_badness(struct task_struct *p, struct mem_cgroup *memcg,\n\t\t\t  const nodemask_t *nodemask, unsigned long totalpages)\n{\n\tlong points;\n\tlong adj;\n\n\tif (oom_unkillable_task(p, memcg, nodemask))\n\t\treturn 0;\n\n\tp = find_lock_task_mm(p);\n\tif (!p)\n\t\treturn 0;\n\n\t/*\n\t * Do not even consider tasks which are explicitly marked oom\n\t * unkillable or have been already oom reaped or the are in\n\t * the middle of vfork\n\t */\n\tadj = (long)p->signal->oom_score_adj;\n\tif (adj == OOM_SCORE_ADJ_MIN ||\n\t\t\ttest_bit(MMF_OOM_SKIP, &p->mm->flags) ||\n\t\t\tin_vfork(p)) {\n\t\ttask_unlock(p);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * The baseline for the badness score is the proportion of RAM that each\n\t * task's rss, pagetable and swap space use.\n\t */\n\tpoints = get_mm_rss(p->mm) + get_mm_counter(p->mm, MM_SWAPENTS) +\n\t\tmm_pgtables_bytes(p->mm) / PAGE_SIZE;\n\ttask_unlock(p);\n\n\t/*\n\t * Root processes get 3% bonus, just like the __vm_enough_memory()\n\t * implementation used by LSMs.\n\t */\n\tif (has_capability_noaudit(p, CAP_SYS_ADMIN))\n\t\tpoints -= (points * 3) / 100;\n\n\t/* Normalize to oom_score_adj units */\n\tadj *= totalpages / 1000;\n\tpoints += adj;\n\n\t/*\n\t * Never return 0 for an eligible task regardless of the root bonus and\n\t * oom_score_adj (oom_score_adj can't be OOM_SCORE_ADJ_MIN here).\n\t */\n\treturn points > 0 ? points : 1;\n}\n\nenum oom_constraint {\n\tCONSTRAINT_NONE,\n\tCONSTRAINT_CPUSET,\n\tCONSTRAINT_MEMORY_POLICY,\n\tCONSTRAINT_MEMCG,\n};\n\n/*\n * Determine the type of allocation constraint.\n */\nstatic enum oom_constraint constrained_alloc(struct oom_control *oc)\n{\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tenum zone_type high_zoneidx = gfp_zone(oc->gfp_mask);\n\tbool cpuset_limited = false;\n\tint nid;\n\n\tif (is_memcg_oom(oc)) {\n\t\toc->totalpages = mem_cgroup_get_limit(oc->memcg) ?: 1;\n\t\treturn CONSTRAINT_MEMCG;\n\t}\n\n\t/* Default to all available memory */\n\toc->totalpages = totalram_pages + total_swap_pages;\n\n\tif (!IS_ENABLED(CONFIG_NUMA))\n\t\treturn CONSTRAINT_NONE;\n\n\tif (!oc->zonelist)\n\t\treturn CONSTRAINT_NONE;\n\t/*\n\t * Reach here only when __GFP_NOFAIL is used. So, we should avoid\n\t * to kill current.We have to random task kill in this case.\n\t * Hopefully, CONSTRAINT_THISNODE...but no way to handle it, now.\n\t */\n\tif (oc->gfp_mask & __GFP_THISNODE)\n\t\treturn CONSTRAINT_NONE;\n\n\t/*\n\t * This is not a __GFP_THISNODE allocation, so a truncated nodemask in\n\t * the page allocator means a mempolicy is in effect.  Cpuset policy\n\t * is enforced in get_page_from_freelist().\n\t */\n\tif (oc->nodemask &&\n\t    !nodes_subset(node_states[N_MEMORY], *oc->nodemask)) {\n\t\toc->totalpages = total_swap_pages;\n\t\tfor_each_node_mask(nid, *oc->nodemask)\n\t\t\toc->totalpages += node_spanned_pages(nid);\n\t\treturn CONSTRAINT_MEMORY_POLICY;\n\t}\n\n\t/* Check this allocation failure is caused by cpuset's wall function */\n\tfor_each_zone_zonelist_nodemask(zone, z, oc->zonelist,\n\t\t\thigh_zoneidx, oc->nodemask)\n\t\tif (!cpuset_zone_allowed(zone, oc->gfp_mask))\n\t\t\tcpuset_limited = true;\n\n\tif (cpuset_limited) {\n\t\toc->totalpages = total_swap_pages;\n\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n\t\t\toc->totalpages += node_spanned_pages(nid);\n\t\treturn CONSTRAINT_CPUSET;\n\t}\n\treturn CONSTRAINT_NONE;\n}\n\nstatic int oom_evaluate_task(struct task_struct *task, void *arg)\n{\n\tstruct oom_control *oc = arg;\n\tunsigned long points;\n\n\tif (oom_unkillable_task(task, NULL, oc->nodemask))\n\t\tgoto next;\n\n\t/*\n\t * This task already has access to memory reserves and is being killed.\n\t * Don't allow any other task to have access to the reserves unless\n\t * the task has MMF_OOM_SKIP because chances that it would release\n\t * any memory is quite low.\n\t */\n\tif (!is_sysrq_oom(oc) && tsk_is_oom_victim(task)) {\n\t\tif (test_bit(MMF_OOM_SKIP, &task->signal->oom_mm->flags))\n\t\t\tgoto next;\n\t\tgoto abort;\n\t}\n\n\t/*\n\t * If task is allocating a lot of memory and has been marked to be\n\t * killed first if it triggers an oom, then select it.\n\t */\n\tif (oom_task_origin(task)) {\n\t\tpoints = ULONG_MAX;\n\t\tgoto select;\n\t}\n\n\tpoints = oom_badness(task, NULL, oc->nodemask, oc->totalpages);\n\tif (!points || points < oc->chosen_points)\n\t\tgoto next;\n\n\t/* Prefer thread group leaders for display purposes */\n\tif (points == oc->chosen_points && thread_group_leader(oc->chosen))\n\t\tgoto next;\nselect:\n\tif (oc->chosen)\n\t\tput_task_struct(oc->chosen);\n\tget_task_struct(task);\n\toc->chosen = task;\n\toc->chosen_points = points;\nnext:\n\treturn 0;\nabort:\n\tif (oc->chosen)\n\t\tput_task_struct(oc->chosen);\n\toc->chosen = (void *)-1UL;\n\treturn 1;\n}\n\n/*\n * Simple selection loop. We choose the process with the highest number of\n * 'points'. In case scan was aborted, oc->chosen is set to -1.\n */\nstatic void select_bad_process(struct oom_control *oc)\n{\n\tif (is_memcg_oom(oc))\n\t\tmem_cgroup_scan_tasks(oc->memcg, oom_evaluate_task, oc);\n\telse {\n\t\tstruct task_struct *p;\n\n\t\trcu_read_lock();\n\t\tfor_each_process(p)\n\t\t\tif (oom_evaluate_task(p, oc))\n\t\t\t\tbreak;\n\t\trcu_read_unlock();\n\t}\n\n\toc->chosen_points = oc->chosen_points * 1000 / oc->totalpages;\n}\n\n/**\n * dump_tasks - dump current memory state of all system tasks\n * @memcg: current's memory controller, if constrained\n * @nodemask: nodemask passed to page allocator for mempolicy ooms\n *\n * Dumps the current memory state of all eligible tasks.  Tasks not in the same\n * memcg, not in the same cpuset, or bound to a disjoint set of mempolicy nodes\n * are not shown.\n * State information includes task's pid, uid, tgid, vm size, rss,\n * pgtables_bytes, swapents, oom_score_adj value, and name.\n */\nstatic void dump_tasks(struct mem_cgroup *memcg, const nodemask_t *nodemask)\n{\n\tstruct task_struct *p;\n\tstruct task_struct *task;\n\n\tpr_info(\"[ pid ]   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name\\n\");\n\trcu_read_lock();\n\tfor_each_process(p) {\n\t\tif (oom_unkillable_task(p, memcg, nodemask))\n\t\t\tcontinue;\n\n\t\ttask = find_lock_task_mm(p);\n\t\tif (!task) {\n\t\t\t/*\n\t\t\t * This is a kthread or all of p's threads have already\n\t\t\t * detached their mm's.  There's no need to report\n\t\t\t * them; they can't be oom killed anyway.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tpr_info(\"[%5d] %5d %5d %8lu %8lu %8ld %8lu         %5hd %s\\n\",\n\t\t\ttask->pid, from_kuid(&init_user_ns, task_uid(task)),\n\t\t\ttask->tgid, task->mm->total_vm, get_mm_rss(task->mm),\n\t\t\tmm_pgtables_bytes(task->mm),\n\t\t\tget_mm_counter(task->mm, MM_SWAPENTS),\n\t\t\ttask->signal->oom_score_adj, task->comm);\n\t\ttask_unlock(task);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void dump_header(struct oom_control *oc, struct task_struct *p)\n{\n\tpr_warn(\"%s invoked oom-killer: gfp_mask=%#x(%pGg), nodemask=%*pbl, order=%d, oom_score_adj=%hd\\n\",\n\t\tcurrent->comm, oc->gfp_mask, &oc->gfp_mask,\n\t\tnodemask_pr_args(oc->nodemask), oc->order,\n\t\t\tcurrent->signal->oom_score_adj);\n\tif (!IS_ENABLED(CONFIG_COMPACTION) && oc->order)\n\t\tpr_warn(\"COMPACTION is disabled!!!\\n\");\n\n\tcpuset_print_current_mems_allowed();\n\tdump_stack();\n\tif (is_memcg_oom(oc))\n\t\tmem_cgroup_print_oom_info(oc->memcg, p);\n\telse {\n\t\tshow_mem(SHOW_MEM_FILTER_NODES, oc->nodemask);\n\t\tif (is_dump_unreclaim_slabs())\n\t\t\tdump_unreclaimable_slab();\n\t}\n\tif (sysctl_oom_dump_tasks)\n\t\tdump_tasks(oc->memcg, oc->nodemask);\n}\n\n/*\n * Number of OOM victims in flight\n */\nstatic atomic_t oom_victims = ATOMIC_INIT(0);\nstatic DECLARE_WAIT_QUEUE_HEAD(oom_victims_wait);\n\nstatic bool oom_killer_disabled __read_mostly;\n\n#define K(x) ((x) << (PAGE_SHIFT-10))\n\n/*\n * task->mm can be NULL if the task is the exited group leader.  So to\n * determine whether the task is using a particular mm, we examine all the\n * task's threads: if one of those is using this mm then this task was also\n * using it.\n */\nbool process_shares_mm(struct task_struct *p, struct mm_struct *mm)\n{\n\tstruct task_struct *t;\n\n\tfor_each_thread(p, t) {\n\t\tstruct mm_struct *t_mm = READ_ONCE(t->mm);\n\t\tif (t_mm)\n\t\t\treturn t_mm == mm;\n\t}\n\treturn false;\n}\n\n\n#ifdef CONFIG_MMU\n/*\n * OOM Reaper kernel thread which tries to reap the memory used by the OOM\n * victim (if that is possible) to help the OOM killer to move on.\n */\nstatic struct task_struct *oom_reaper_th;\nstatic DECLARE_WAIT_QUEUE_HEAD(oom_reaper_wait);\nstatic struct task_struct *oom_reaper_list;\nstatic DEFINE_SPINLOCK(oom_reaper_lock);\n\nstatic bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tbool ret = true;\n\n\t/*\n\t * We have to make sure to not race with the victim exit path\n\t * and cause premature new oom victim selection:\n\t * __oom_reap_task_mm\t\texit_mm\n\t *   mmget_not_zero\n\t *\t\t\t\t  mmput\n\t *\t\t\t\t    atomic_dec_and_test\n\t *\t\t\t\t  exit_oom_victim\n\t *\t\t\t\t[...]\n\t *\t\t\t\tout_of_memory\n\t *\t\t\t\t  select_bad_process\n\t *\t\t\t\t    # no TIF_MEMDIE task selects new victim\n\t *  unmap_page_range # frees some memory\n\t */\n\tmutex_lock(&oom_lock);\n\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tret = false;\n\t\ttrace_skip_task_reaping(tsk->pid);\n\t\tgoto unlock_oom;\n\t}\n\n\t/*\n\t * If the mm has notifiers then we would need to invalidate them around\n\t * unmap_page_range and that is risky because notifiers can sleep and\n\t * what they do is basically undeterministic.  So let's have a short\n\t * sleep to give the oom victim some more time.\n\t * TODO: we really want to get rid of this ugly hack and make sure that\n\t * notifiers cannot block for unbounded amount of time and add\n\t * mmu_notifier_invalidate_range_{start,end} around unmap_page_range\n\t */\n\tif (mm_has_notifiers(mm)) {\n\t\tup_read(&mm->mmap_sem);\n\t\tschedule_timeout_idle(HZ);\n\t\tgoto unlock_oom;\n\t}\n\n\t/*\n\t * MMF_OOM_SKIP is set by exit_mmap when the OOM reaper can't\n\t * work on the mm anymore. The check for MMF_OOM_SKIP must run\n\t * under mmap_sem for reading because it serializes against the\n\t * down_write();up_write() cycle in exit_mmap().\n\t */\n\tif (test_bit(MMF_OOM_SKIP, &mm->flags)) {\n\t\tup_read(&mm->mmap_sem);\n\t\ttrace_skip_task_reaping(tsk->pid);\n\t\tgoto unlock_oom;\n\t}\n\n\ttrace_start_task_reaping(tsk->pid);\n\n\t/*\n\t * Tell all users of get_user/copy_from_user etc... that the content\n\t * is no longer stable. No barriers really needed because unmapping\n\t * should imply barriers already and the reader would hit a page fault\n\t * if it stumbled over a reaped memory.\n\t */\n\tset_bit(MMF_UNSTABLE, &mm->flags);\n\n\ttlb_gather_mmu(&tlb, mm, 0, -1);\n\tfor (vma = mm->mmap ; vma; vma = vma->vm_next) {\n\t\tif (!can_madv_dontneed_vma(vma))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Only anonymous pages have a good chance to be dropped\n\t\t * without additional steps which we cannot afford as we\n\t\t * are OOM already.\n\t\t *\n\t\t * We do not even care about fs backed pages because all\n\t\t * which are reclaimable have already been reclaimed and\n\t\t * we do not want to block exit_mmap by keeping mm ref\n\t\t * count elevated without a good reason.\n\t\t */\n\t\tif (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED))\n\t\t\tunmap_page_range(&tlb, vma, vma->vm_start, vma->vm_end,\n\t\t\t\t\t NULL);\n\t}\n\ttlb_finish_mmu(&tlb, 0, -1);\n\tpr_info(\"oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\\n\",\n\t\t\ttask_pid_nr(tsk), tsk->comm,\n\t\t\tK(get_mm_counter(mm, MM_ANONPAGES)),\n\t\t\tK(get_mm_counter(mm, MM_FILEPAGES)),\n\t\t\tK(get_mm_counter(mm, MM_SHMEMPAGES)));\n\tup_read(&mm->mmap_sem);\n\n\ttrace_finish_task_reaping(tsk->pid);\nunlock_oom:\n\tmutex_unlock(&oom_lock);\n\treturn ret;\n}\n\n#define MAX_OOM_REAP_RETRIES 10\nstatic void oom_reap_task(struct task_struct *tsk)\n{\n\tint attempts = 0;\n\tstruct mm_struct *mm = tsk->signal->oom_mm;\n\n\t/* Retry the down_read_trylock(mmap_sem) a few times */\n\twhile (attempts++ < MAX_OOM_REAP_RETRIES && !__oom_reap_task_mm(tsk, mm))\n\t\tschedule_timeout_idle(HZ/10);\n\n\tif (attempts <= MAX_OOM_REAP_RETRIES)\n\t\tgoto done;\n\n\n\tpr_info(\"oom_reaper: unable to reap pid:%d (%s)\\n\",\n\t\ttask_pid_nr(tsk), tsk->comm);\n\tdebug_show_all_locks();\n\ndone:\n\ttsk->oom_reaper_list = NULL;\n\n\t/*\n\t * Hide this mm from OOM killer because it has been either reaped or\n\t * somebody can't call up_write(mmap_sem).\n\t */\n\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\n\t/* Drop a reference taken by wake_oom_reaper */\n\tput_task_struct(tsk);\n}\n\nstatic int oom_reaper(void *unused)\n{\n\twhile (true) {\n\t\tstruct task_struct *tsk = NULL;\n\n\t\twait_event_freezable(oom_reaper_wait, oom_reaper_list != NULL);\n\t\tspin_lock(&oom_reaper_lock);\n\t\tif (oom_reaper_list != NULL) {\n\t\t\ttsk = oom_reaper_list;\n\t\t\toom_reaper_list = tsk->oom_reaper_list;\n\t\t}\n\t\tspin_unlock(&oom_reaper_lock);\n\n\t\tif (tsk)\n\t\t\toom_reap_task(tsk);\n\t}\n\n\treturn 0;\n}\n\nstatic void wake_oom_reaper(struct task_struct *tsk)\n{\n\t/* tsk is already queued? */\n\tif (tsk == oom_reaper_list || tsk->oom_reaper_list)\n\t\treturn;\n\n\tget_task_struct(tsk);\n\n\tspin_lock(&oom_reaper_lock);\n\ttsk->oom_reaper_list = oom_reaper_list;\n\toom_reaper_list = tsk;\n\tspin_unlock(&oom_reaper_lock);\n\ttrace_wake_reaper(tsk->pid);\n\twake_up(&oom_reaper_wait);\n}\n\nstatic int __init oom_init(void)\n{\n\toom_reaper_th = kthread_run(oom_reaper, NULL, \"oom_reaper\");\n\treturn 0;\n}\nsubsys_initcall(oom_init)\n#else\nstatic inline void wake_oom_reaper(struct task_struct *tsk)\n{\n}\n#endif /* CONFIG_MMU */\n\n/**\n * mark_oom_victim - mark the given task as OOM victim\n * @tsk: task to mark\n *\n * Has to be called with oom_lock held and never after\n * oom has been disabled already.\n *\n * tsk->mm has to be non NULL and caller has to guarantee it is stable (either\n * under task_lock or operate on the current).\n */\nstatic void mark_oom_victim(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm = tsk->mm;\n\n\tWARN_ON(oom_killer_disabled);\n\t/* OOM killer might race with memcg OOM */\n\tif (test_and_set_tsk_thread_flag(tsk, TIF_MEMDIE))\n\t\treturn;\n\n\t/* oom_mm is bound to the signal struct life time. */\n\tif (!cmpxchg(&tsk->signal->oom_mm, NULL, mm))\n\t\tmmgrab(tsk->signal->oom_mm);\n\n\t/*\n\t * Make sure that the task is woken up from uninterruptible sleep\n\t * if it is frozen because OOM killer wouldn't be able to free\n\t * any memory and livelock. freezing_slow_path will tell the freezer\n\t * that TIF_MEMDIE tasks should be ignored.\n\t */\n\t__thaw_task(tsk);\n\tatomic_inc(&oom_victims);\n\ttrace_mark_victim(tsk->pid);\n}\n\n/**\n * exit_oom_victim - note the exit of an OOM victim\n */\nvoid exit_oom_victim(void)\n{\n\tclear_thread_flag(TIF_MEMDIE);\n\n\tif (!atomic_dec_return(&oom_victims))\n\t\twake_up_all(&oom_victims_wait);\n}\n\n/**\n * oom_killer_enable - enable OOM killer\n */\nvoid oom_killer_enable(void)\n{\n\toom_killer_disabled = false;\n\tpr_info(\"OOM killer enabled.\\n\");\n}\n\n/**\n * oom_killer_disable - disable OOM killer\n * @timeout: maximum timeout to wait for oom victims in jiffies\n *\n * Forces all page allocations to fail rather than trigger OOM killer.\n * Will block and wait until all OOM victims are killed or the given\n * timeout expires.\n *\n * The function cannot be called when there are runnable user tasks because\n * the userspace would see unexpected allocation failures as a result. Any\n * new usage of this function should be consulted with MM people.\n *\n * Returns true if successful and false if the OOM killer cannot be\n * disabled.\n */\nbool oom_killer_disable(signed long timeout)\n{\n\tsigned long ret;\n\n\t/*\n\t * Make sure to not race with an ongoing OOM killer. Check that the\n\t * current is not killed (possibly due to sharing the victim's memory).\n\t */\n\tif (mutex_lock_killable(&oom_lock))\n\t\treturn false;\n\toom_killer_disabled = true;\n\tmutex_unlock(&oom_lock);\n\n\tret = wait_event_interruptible_timeout(oom_victims_wait,\n\t\t\t!atomic_read(&oom_victims), timeout);\n\tif (ret <= 0) {\n\t\toom_killer_enable();\n\t\treturn false;\n\t}\n\tpr_info(\"OOM killer disabled.\\n\");\n\n\treturn true;\n}\n\nstatic inline bool __task_will_free_mem(struct task_struct *task)\n{\n\tstruct signal_struct *sig = task->signal;\n\n\t/*\n\t * A coredumping process may sleep for an extended period in exit_mm(),\n\t * so the oom killer cannot assume that the process will promptly exit\n\t * and release memory.\n\t */\n\tif (sig->flags & SIGNAL_GROUP_COREDUMP)\n\t\treturn false;\n\n\tif (sig->flags & SIGNAL_GROUP_EXIT)\n\t\treturn true;\n\n\tif (thread_group_empty(task) && (task->flags & PF_EXITING))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Checks whether the given task is dying or exiting and likely to\n * release its address space. This means that all threads and processes\n * sharing the same mm have to be killed or exiting.\n * Caller has to make sure that task->mm is stable (hold task_lock or\n * it operates on the current).\n */\nstatic bool task_will_free_mem(struct task_struct *task)\n{\n\tstruct mm_struct *mm = task->mm;\n\tstruct task_struct *p;\n\tbool ret = true;\n\n\t/*\n\t * Skip tasks without mm because it might have passed its exit_mm and\n\t * exit_oom_victim. oom_reaper could have rescued that but do not rely\n\t * on that for now. We can consider find_lock_task_mm in future.\n\t */\n\tif (!mm)\n\t\treturn false;\n\n\tif (!__task_will_free_mem(task))\n\t\treturn false;\n\n\t/*\n\t * This task has already been drained by the oom reaper so there are\n\t * only small chances it will free some more\n\t */\n\tif (test_bit(MMF_OOM_SKIP, &mm->flags))\n\t\treturn false;\n\n\tif (atomic_read(&mm->mm_users) <= 1)\n\t\treturn true;\n\n\t/*\n\t * Make sure that all tasks which share the mm with the given tasks\n\t * are dying as well to make sure that a) nobody pins its mm and\n\t * b) the task is also reapable by the oom reaper.\n\t */\n\trcu_read_lock();\n\tfor_each_process(p) {\n\t\tif (!process_shares_mm(p, mm))\n\t\t\tcontinue;\n\t\tif (same_thread_group(task, p))\n\t\t\tcontinue;\n\t\tret = __task_will_free_mem(p);\n\t\tif (!ret)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic void oom_kill_process(struct oom_control *oc, const char *message)\n{\n\tstruct task_struct *p = oc->chosen;\n\tunsigned int points = oc->chosen_points;\n\tstruct task_struct *victim = p;\n\tstruct task_struct *child;\n\tstruct task_struct *t;\n\tstruct mm_struct *mm;\n\tunsigned int victim_points = 0;\n\tstatic DEFINE_RATELIMIT_STATE(oom_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\tbool can_oom_reap = true;\n\n\t/*\n\t * If the task is already exiting, don't alarm the sysadmin or kill\n\t * its children or threads, just give it access to memory reserves\n\t * so it can die quickly\n\t */\n\ttask_lock(p);\n\tif (task_will_free_mem(p)) {\n\t\tmark_oom_victim(p);\n\t\twake_oom_reaper(p);\n\t\ttask_unlock(p);\n\t\tput_task_struct(p);\n\t\treturn;\n\t}\n\ttask_unlock(p);\n\n\tif (__ratelimit(&oom_rs))\n\t\tdump_header(oc, p);\n\n\tpr_err(\"%s: Kill process %d (%s) score %u or sacrifice child\\n\",\n\t\tmessage, task_pid_nr(p), p->comm, points);\n\n\t/*\n\t * If any of p's children has a different mm and is eligible for kill,\n\t * the one with the highest oom_badness() score is sacrificed for its\n\t * parent.  This attempts to lose the minimal amount of work done while\n\t * still freeing memory.\n\t */\n\tread_lock(&tasklist_lock);\n\tfor_each_thread(p, t) {\n\t\tlist_for_each_entry(child, &t->children, sibling) {\n\t\t\tunsigned int child_points;\n\n\t\t\tif (process_shares_mm(child, p->mm))\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * oom_badness() returns 0 if the thread is unkillable\n\t\t\t */\n\t\t\tchild_points = oom_badness(child,\n\t\t\t\toc->memcg, oc->nodemask, oc->totalpages);\n\t\t\tif (child_points > victim_points) {\n\t\t\t\tput_task_struct(victim);\n\t\t\t\tvictim = child;\n\t\t\t\tvictim_points = child_points;\n\t\t\t\tget_task_struct(victim);\n\t\t\t}\n\t\t}\n\t}\n\tread_unlock(&tasklist_lock);\n\n\tp = find_lock_task_mm(victim);\n\tif (!p) {\n\t\tput_task_struct(victim);\n\t\treturn;\n\t} else if (victim != p) {\n\t\tget_task_struct(p);\n\t\tput_task_struct(victim);\n\t\tvictim = p;\n\t}\n\n\t/* Get a reference to safely compare mm after task_unlock(victim) */\n\tmm = victim->mm;\n\tmmgrab(mm);\n\n\t/* Raise event before sending signal: task reaper must see this */\n\tcount_vm_event(OOM_KILL);\n\tcount_memcg_event_mm(mm, OOM_KILL);\n\n\t/*\n\t * We should send SIGKILL before granting access to memory reserves\n\t * in order to prevent the OOM victim from depleting the memory\n\t * reserves from the user space under its control.\n\t */\n\tdo_send_sig_info(SIGKILL, SEND_SIG_FORCED, victim, true);\n\tmark_oom_victim(victim);\n\tpr_err(\"Killed process %d (%s) total-vm:%lukB, anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\\n\",\n\t\ttask_pid_nr(victim), victim->comm, K(victim->mm->total_vm),\n\t\tK(get_mm_counter(victim->mm, MM_ANONPAGES)),\n\t\tK(get_mm_counter(victim->mm, MM_FILEPAGES)),\n\t\tK(get_mm_counter(victim->mm, MM_SHMEMPAGES)));\n\ttask_unlock(victim);\n\n\t/*\n\t * Kill all user processes sharing victim->mm in other thread groups, if\n\t * any.  They don't get access to memory reserves, though, to avoid\n\t * depletion of all memory.  This prevents mm->mmap_sem livelock when an\n\t * oom killed thread cannot exit because it requires the semaphore and\n\t * its contended by another thread trying to allocate memory itself.\n\t * That thread will now get access to memory reserves since it has a\n\t * pending fatal signal.\n\t */\n\trcu_read_lock();\n\tfor_each_process(p) {\n\t\tif (!process_shares_mm(p, mm))\n\t\t\tcontinue;\n\t\tif (same_thread_group(p, victim))\n\t\t\tcontinue;\n\t\tif (is_global_init(p)) {\n\t\t\tcan_oom_reap = false;\n\t\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\t\t\tpr_info(\"oom killer %d (%s) has mm pinned by %d (%s)\\n\",\n\t\t\t\t\ttask_pid_nr(victim), victim->comm,\n\t\t\t\t\ttask_pid_nr(p), p->comm);\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * No use_mm() user needs to read from the userspace so we are\n\t\t * ok to reap it.\n\t\t */\n\t\tif (unlikely(p->flags & PF_KTHREAD))\n\t\t\tcontinue;\n\t\tdo_send_sig_info(SIGKILL, SEND_SIG_FORCED, p, true);\n\t}\n\trcu_read_unlock();\n\n\tif (can_oom_reap)\n\t\twake_oom_reaper(victim);\n\n\tmmdrop(mm);\n\tput_task_struct(victim);\n}\n#undef K\n\n/*\n * Determines whether the kernel must panic because of the panic_on_oom sysctl.\n */\nstatic void check_panic_on_oom(struct oom_control *oc,\n\t\t\t       enum oom_constraint constraint)\n{\n\tif (likely(!sysctl_panic_on_oom))\n\t\treturn;\n\tif (sysctl_panic_on_oom != 2) {\n\t\t/*\n\t\t * panic_on_oom == 1 only affects CONSTRAINT_NONE, the kernel\n\t\t * does not panic for cpuset, mempolicy, or memcg allocation\n\t\t * failures.\n\t\t */\n\t\tif (constraint != CONSTRAINT_NONE)\n\t\t\treturn;\n\t}\n\t/* Do not panic for oom kills triggered by sysrq */\n\tif (is_sysrq_oom(oc))\n\t\treturn;\n\tdump_header(oc, NULL);\n\tpanic(\"Out of memory: %s panic_on_oom is enabled\\n\",\n\t\tsysctl_panic_on_oom == 2 ? \"compulsory\" : \"system-wide\");\n}\n\nstatic BLOCKING_NOTIFIER_HEAD(oom_notify_list);\n\nint register_oom_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&oom_notify_list, nb);\n}\nEXPORT_SYMBOL_GPL(register_oom_notifier);\n\nint unregister_oom_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&oom_notify_list, nb);\n}\nEXPORT_SYMBOL_GPL(unregister_oom_notifier);\n\n/**\n * out_of_memory - kill the \"best\" process when we run out of memory\n * @oc: pointer to struct oom_control\n *\n * If we run out of memory, we have the choice between either\n * killing a random task (bad), letting the system crash (worse)\n * OR try to be smart about which process to kill. Note that we\n * don't have to be perfect here, we just have to be good.\n */\nbool out_of_memory(struct oom_control *oc)\n{\n\tunsigned long freed = 0;\n\tenum oom_constraint constraint = CONSTRAINT_NONE;\n\n\tif (oom_killer_disabled)\n\t\treturn false;\n\n\tif (!is_memcg_oom(oc)) {\n\t\tblocking_notifier_call_chain(&oom_notify_list, 0, &freed);\n\t\tif (freed > 0)\n\t\t\t/* Got some memory back in the last second. */\n\t\t\treturn true;\n\t}\n\n\t/*\n\t * If current has a pending SIGKILL or is exiting, then automatically\n\t * select it.  The goal is to allow it to allocate so that it may\n\t * quickly exit and free its memory.\n\t */\n\tif (task_will_free_mem(current)) {\n\t\tmark_oom_victim(current);\n\t\twake_oom_reaper(current);\n\t\treturn true;\n\t}\n\n\t/*\n\t * The OOM killer does not compensate for IO-less reclaim.\n\t * pagefault_out_of_memory lost its gfp context so we have to\n\t * make sure exclude 0 mask - all other users should have at least\n\t * ___GFP_DIRECT_RECLAIM to get here.\n\t */\n\tif (oc->gfp_mask && !(oc->gfp_mask & __GFP_FS))\n\t\treturn true;\n\n\t/*\n\t * Check if there were limitations on the allocation (only relevant for\n\t * NUMA and memcg) that may require different handling.\n\t */\n\tconstraint = constrained_alloc(oc);\n\tif (constraint != CONSTRAINT_MEMORY_POLICY)\n\t\toc->nodemask = NULL;\n\tcheck_panic_on_oom(oc, constraint);\n\n\tif (!is_memcg_oom(oc) && sysctl_oom_kill_allocating_task &&\n\t    current->mm && !oom_unkillable_task(current, NULL, oc->nodemask) &&\n\t    current->signal->oom_score_adj != OOM_SCORE_ADJ_MIN) {\n\t\tget_task_struct(current);\n\t\toc->chosen = current;\n\t\toom_kill_process(oc, \"Out of memory (oom_kill_allocating_task)\");\n\t\treturn true;\n\t}\n\n\tselect_bad_process(oc);\n\t/* Found nothing?!?! Either we hang forever, or we panic. */\n\tif (!oc->chosen && !is_sysrq_oom(oc) && !is_memcg_oom(oc)) {\n\t\tdump_header(oc, NULL);\n\t\tpanic(\"Out of memory and no killable processes...\\n\");\n\t}\n\tif (oc->chosen && oc->chosen != (void *)-1UL) {\n\t\toom_kill_process(oc, !is_memcg_oom(oc) ? \"Out of memory\" :\n\t\t\t\t \"Memory cgroup out of memory\");\n\t\t/*\n\t\t * Give the killed process a good chance to exit before trying\n\t\t * to allocate memory again.\n\t\t */\n\t\tschedule_timeout_killable(1);\n\t}\n\treturn !!oc->chosen;\n}\n\n/*\n * The pagefault handler calls here because it is out of memory, so kill a\n * memory-hogging task. If oom_lock is held by somebody else, a parallel oom\n * killing is already in progress so do nothing.\n */\nvoid pagefault_out_of_memory(void)\n{\n\tstruct oom_control oc = {\n\t\t.zonelist = NULL,\n\t\t.nodemask = NULL,\n\t\t.memcg = NULL,\n\t\t.gfp_mask = 0,\n\t\t.order = 0,\n\t};\n\n\tif (mem_cgroup_oom_synchronize(true))\n\t\treturn;\n\n\tif (!mutex_trylock(&oom_lock))\n\t\treturn;\n\tout_of_memory(&oc);\n\tmutex_unlock(&oom_lock);\n}\n"], "fixing_code": ["/*\n *  linux/mm/oom_kill.c\n * \n *  Copyright (C)  1998,2000  Rik van Riel\n *\tThanks go out to Claus Fischer for some serious inspiration and\n *\tfor goading me into coding this file...\n *  Copyright (C)  2010  Google, Inc.\n *\tRewritten by David Rientjes\n *\n *  The routines in this file are used to kill a process when\n *  we're seriously out of memory. This gets called from __alloc_pages()\n *  in mm/page_alloc.c when we really run out of memory.\n *\n *  Since we won't call these routines often (on a well-configured\n *  machine) this file will double as a 'coding guide' and a signpost\n *  for newbie kernel hackers. It features several pointers to major\n *  kernel subsystems and hints as to where to find out what things do.\n */\n\n#include <linux/oom.h>\n#include <linux/mm.h>\n#include <linux/err.h>\n#include <linux/gfp.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/coredump.h>\n#include <linux/sched/task.h>\n#include <linux/swap.h>\n#include <linux/timex.h>\n#include <linux/jiffies.h>\n#include <linux/cpuset.h>\n#include <linux/export.h>\n#include <linux/notifier.h>\n#include <linux/memcontrol.h>\n#include <linux/mempolicy.h>\n#include <linux/security.h>\n#include <linux/ptrace.h>\n#include <linux/freezer.h>\n#include <linux/ftrace.h>\n#include <linux/ratelimit.h>\n#include <linux/kthread.h>\n#include <linux/init.h>\n#include <linux/mmu_notifier.h>\n\n#include <asm/tlb.h>\n#include \"internal.h\"\n#include \"slab.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/oom.h>\n\nint sysctl_panic_on_oom;\nint sysctl_oom_kill_allocating_task;\nint sysctl_oom_dump_tasks = 1;\n\nDEFINE_MUTEX(oom_lock);\n\n#ifdef CONFIG_NUMA\n/**\n * has_intersects_mems_allowed() - check task eligiblity for kill\n * @start: task struct of which task to consider\n * @mask: nodemask passed to page allocator for mempolicy ooms\n *\n * Task eligibility is determined by whether or not a candidate task, @tsk,\n * shares the same mempolicy nodes as current if it is bound by such a policy\n * and whether or not it has the same set of allowed cpuset nodes.\n */\nstatic bool has_intersects_mems_allowed(struct task_struct *start,\n\t\t\t\t\tconst nodemask_t *mask)\n{\n\tstruct task_struct *tsk;\n\tbool ret = false;\n\n\trcu_read_lock();\n\tfor_each_thread(start, tsk) {\n\t\tif (mask) {\n\t\t\t/*\n\t\t\t * If this is a mempolicy constrained oom, tsk's\n\t\t\t * cpuset is irrelevant.  Only return true if its\n\t\t\t * mempolicy intersects current, otherwise it may be\n\t\t\t * needlessly killed.\n\t\t\t */\n\t\t\tret = mempolicy_nodemask_intersects(tsk, mask);\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is not a mempolicy constrained oom, so only\n\t\t\t * check the mems of tsk's cpuset.\n\t\t\t */\n\t\t\tret = cpuset_mems_allowed_intersects(current, tsk);\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n#else\nstatic bool has_intersects_mems_allowed(struct task_struct *tsk,\n\t\t\t\t\tconst nodemask_t *mask)\n{\n\treturn true;\n}\n#endif /* CONFIG_NUMA */\n\n/*\n * The process p may have detached its own ->mm while exiting or through\n * use_mm(), but one or more of its subthreads may still have a valid\n * pointer.  Return p, or any of its subthreads with a valid ->mm, with\n * task_lock() held.\n */\nstruct task_struct *find_lock_task_mm(struct task_struct *p)\n{\n\tstruct task_struct *t;\n\n\trcu_read_lock();\n\n\tfor_each_thread(p, t) {\n\t\ttask_lock(t);\n\t\tif (likely(t->mm))\n\t\t\tgoto found;\n\t\ttask_unlock(t);\n\t}\n\tt = NULL;\nfound:\n\trcu_read_unlock();\n\n\treturn t;\n}\n\n/*\n * order == -1 means the oom kill is required by sysrq, otherwise only\n * for display purposes.\n */\nstatic inline bool is_sysrq_oom(struct oom_control *oc)\n{\n\treturn oc->order == -1;\n}\n\nstatic inline bool is_memcg_oom(struct oom_control *oc)\n{\n\treturn oc->memcg != NULL;\n}\n\n/* return true if the task is not adequate as candidate victim task. */\nstatic bool oom_unkillable_task(struct task_struct *p,\n\t\tstruct mem_cgroup *memcg, const nodemask_t *nodemask)\n{\n\tif (is_global_init(p))\n\t\treturn true;\n\tif (p->flags & PF_KTHREAD)\n\t\treturn true;\n\n\t/* When mem_cgroup_out_of_memory() and p is not member of the group */\n\tif (memcg && !task_in_mem_cgroup(p, memcg))\n\t\treturn true;\n\n\t/* p may not have freeable memory in nodemask */\n\tif (!has_intersects_mems_allowed(p, nodemask))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Print out unreclaimble slabs info when unreclaimable slabs amount is greater\n * than all user memory (LRU pages)\n */\nstatic bool is_dump_unreclaim_slabs(void)\n{\n\tunsigned long nr_lru;\n\n\tnr_lru = global_node_page_state(NR_ACTIVE_ANON) +\n\t\t global_node_page_state(NR_INACTIVE_ANON) +\n\t\t global_node_page_state(NR_ACTIVE_FILE) +\n\t\t global_node_page_state(NR_INACTIVE_FILE) +\n\t\t global_node_page_state(NR_ISOLATED_ANON) +\n\t\t global_node_page_state(NR_ISOLATED_FILE) +\n\t\t global_node_page_state(NR_UNEVICTABLE);\n\n\treturn (global_node_page_state(NR_SLAB_UNRECLAIMABLE) > nr_lru);\n}\n\n/**\n * oom_badness - heuristic function to determine which candidate task to kill\n * @p: task struct of which task we should calculate\n * @totalpages: total present RAM allowed for page allocation\n *\n * The heuristic for determining which task to kill is made to be as simple and\n * predictable as possible.  The goal is to return the highest value for the\n * task consuming the most memory to avoid subsequent oom failures.\n */\nunsigned long oom_badness(struct task_struct *p, struct mem_cgroup *memcg,\n\t\t\t  const nodemask_t *nodemask, unsigned long totalpages)\n{\n\tlong points;\n\tlong adj;\n\n\tif (oom_unkillable_task(p, memcg, nodemask))\n\t\treturn 0;\n\n\tp = find_lock_task_mm(p);\n\tif (!p)\n\t\treturn 0;\n\n\t/*\n\t * Do not even consider tasks which are explicitly marked oom\n\t * unkillable or have been already oom reaped or the are in\n\t * the middle of vfork\n\t */\n\tadj = (long)p->signal->oom_score_adj;\n\tif (adj == OOM_SCORE_ADJ_MIN ||\n\t\t\ttest_bit(MMF_OOM_SKIP, &p->mm->flags) ||\n\t\t\tin_vfork(p)) {\n\t\ttask_unlock(p);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * The baseline for the badness score is the proportion of RAM that each\n\t * task's rss, pagetable and swap space use.\n\t */\n\tpoints = get_mm_rss(p->mm) + get_mm_counter(p->mm, MM_SWAPENTS) +\n\t\tmm_pgtables_bytes(p->mm) / PAGE_SIZE;\n\ttask_unlock(p);\n\n\t/*\n\t * Root processes get 3% bonus, just like the __vm_enough_memory()\n\t * implementation used by LSMs.\n\t */\n\tif (has_capability_noaudit(p, CAP_SYS_ADMIN))\n\t\tpoints -= (points * 3) / 100;\n\n\t/* Normalize to oom_score_adj units */\n\tadj *= totalpages / 1000;\n\tpoints += adj;\n\n\t/*\n\t * Never return 0 for an eligible task regardless of the root bonus and\n\t * oom_score_adj (oom_score_adj can't be OOM_SCORE_ADJ_MIN here).\n\t */\n\treturn points > 0 ? points : 1;\n}\n\nenum oom_constraint {\n\tCONSTRAINT_NONE,\n\tCONSTRAINT_CPUSET,\n\tCONSTRAINT_MEMORY_POLICY,\n\tCONSTRAINT_MEMCG,\n};\n\n/*\n * Determine the type of allocation constraint.\n */\nstatic enum oom_constraint constrained_alloc(struct oom_control *oc)\n{\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tenum zone_type high_zoneidx = gfp_zone(oc->gfp_mask);\n\tbool cpuset_limited = false;\n\tint nid;\n\n\tif (is_memcg_oom(oc)) {\n\t\toc->totalpages = mem_cgroup_get_limit(oc->memcg) ?: 1;\n\t\treturn CONSTRAINT_MEMCG;\n\t}\n\n\t/* Default to all available memory */\n\toc->totalpages = totalram_pages + total_swap_pages;\n\n\tif (!IS_ENABLED(CONFIG_NUMA))\n\t\treturn CONSTRAINT_NONE;\n\n\tif (!oc->zonelist)\n\t\treturn CONSTRAINT_NONE;\n\t/*\n\t * Reach here only when __GFP_NOFAIL is used. So, we should avoid\n\t * to kill current.We have to random task kill in this case.\n\t * Hopefully, CONSTRAINT_THISNODE...but no way to handle it, now.\n\t */\n\tif (oc->gfp_mask & __GFP_THISNODE)\n\t\treturn CONSTRAINT_NONE;\n\n\t/*\n\t * This is not a __GFP_THISNODE allocation, so a truncated nodemask in\n\t * the page allocator means a mempolicy is in effect.  Cpuset policy\n\t * is enforced in get_page_from_freelist().\n\t */\n\tif (oc->nodemask &&\n\t    !nodes_subset(node_states[N_MEMORY], *oc->nodemask)) {\n\t\toc->totalpages = total_swap_pages;\n\t\tfor_each_node_mask(nid, *oc->nodemask)\n\t\t\toc->totalpages += node_spanned_pages(nid);\n\t\treturn CONSTRAINT_MEMORY_POLICY;\n\t}\n\n\t/* Check this allocation failure is caused by cpuset's wall function */\n\tfor_each_zone_zonelist_nodemask(zone, z, oc->zonelist,\n\t\t\thigh_zoneidx, oc->nodemask)\n\t\tif (!cpuset_zone_allowed(zone, oc->gfp_mask))\n\t\t\tcpuset_limited = true;\n\n\tif (cpuset_limited) {\n\t\toc->totalpages = total_swap_pages;\n\t\tfor_each_node_mask(nid, cpuset_current_mems_allowed)\n\t\t\toc->totalpages += node_spanned_pages(nid);\n\t\treturn CONSTRAINT_CPUSET;\n\t}\n\treturn CONSTRAINT_NONE;\n}\n\nstatic int oom_evaluate_task(struct task_struct *task, void *arg)\n{\n\tstruct oom_control *oc = arg;\n\tunsigned long points;\n\n\tif (oom_unkillable_task(task, NULL, oc->nodemask))\n\t\tgoto next;\n\n\t/*\n\t * This task already has access to memory reserves and is being killed.\n\t * Don't allow any other task to have access to the reserves unless\n\t * the task has MMF_OOM_SKIP because chances that it would release\n\t * any memory is quite low.\n\t */\n\tif (!is_sysrq_oom(oc) && tsk_is_oom_victim(task)) {\n\t\tif (test_bit(MMF_OOM_SKIP, &task->signal->oom_mm->flags))\n\t\t\tgoto next;\n\t\tgoto abort;\n\t}\n\n\t/*\n\t * If task is allocating a lot of memory and has been marked to be\n\t * killed first if it triggers an oom, then select it.\n\t */\n\tif (oom_task_origin(task)) {\n\t\tpoints = ULONG_MAX;\n\t\tgoto select;\n\t}\n\n\tpoints = oom_badness(task, NULL, oc->nodemask, oc->totalpages);\n\tif (!points || points < oc->chosen_points)\n\t\tgoto next;\n\n\t/* Prefer thread group leaders for display purposes */\n\tif (points == oc->chosen_points && thread_group_leader(oc->chosen))\n\t\tgoto next;\nselect:\n\tif (oc->chosen)\n\t\tput_task_struct(oc->chosen);\n\tget_task_struct(task);\n\toc->chosen = task;\n\toc->chosen_points = points;\nnext:\n\treturn 0;\nabort:\n\tif (oc->chosen)\n\t\tput_task_struct(oc->chosen);\n\toc->chosen = (void *)-1UL;\n\treturn 1;\n}\n\n/*\n * Simple selection loop. We choose the process with the highest number of\n * 'points'. In case scan was aborted, oc->chosen is set to -1.\n */\nstatic void select_bad_process(struct oom_control *oc)\n{\n\tif (is_memcg_oom(oc))\n\t\tmem_cgroup_scan_tasks(oc->memcg, oom_evaluate_task, oc);\n\telse {\n\t\tstruct task_struct *p;\n\n\t\trcu_read_lock();\n\t\tfor_each_process(p)\n\t\t\tif (oom_evaluate_task(p, oc))\n\t\t\t\tbreak;\n\t\trcu_read_unlock();\n\t}\n\n\toc->chosen_points = oc->chosen_points * 1000 / oc->totalpages;\n}\n\n/**\n * dump_tasks - dump current memory state of all system tasks\n * @memcg: current's memory controller, if constrained\n * @nodemask: nodemask passed to page allocator for mempolicy ooms\n *\n * Dumps the current memory state of all eligible tasks.  Tasks not in the same\n * memcg, not in the same cpuset, or bound to a disjoint set of mempolicy nodes\n * are not shown.\n * State information includes task's pid, uid, tgid, vm size, rss,\n * pgtables_bytes, swapents, oom_score_adj value, and name.\n */\nstatic void dump_tasks(struct mem_cgroup *memcg, const nodemask_t *nodemask)\n{\n\tstruct task_struct *p;\n\tstruct task_struct *task;\n\n\tpr_info(\"[ pid ]   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name\\n\");\n\trcu_read_lock();\n\tfor_each_process(p) {\n\t\tif (oom_unkillable_task(p, memcg, nodemask))\n\t\t\tcontinue;\n\n\t\ttask = find_lock_task_mm(p);\n\t\tif (!task) {\n\t\t\t/*\n\t\t\t * This is a kthread or all of p's threads have already\n\t\t\t * detached their mm's.  There's no need to report\n\t\t\t * them; they can't be oom killed anyway.\n\t\t\t */\n\t\t\tcontinue;\n\t\t}\n\n\t\tpr_info(\"[%5d] %5d %5d %8lu %8lu %8ld %8lu         %5hd %s\\n\",\n\t\t\ttask->pid, from_kuid(&init_user_ns, task_uid(task)),\n\t\t\ttask->tgid, task->mm->total_vm, get_mm_rss(task->mm),\n\t\t\tmm_pgtables_bytes(task->mm),\n\t\t\tget_mm_counter(task->mm, MM_SWAPENTS),\n\t\t\ttask->signal->oom_score_adj, task->comm);\n\t\ttask_unlock(task);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void dump_header(struct oom_control *oc, struct task_struct *p)\n{\n\tpr_warn(\"%s invoked oom-killer: gfp_mask=%#x(%pGg), nodemask=%*pbl, order=%d, oom_score_adj=%hd\\n\",\n\t\tcurrent->comm, oc->gfp_mask, &oc->gfp_mask,\n\t\tnodemask_pr_args(oc->nodemask), oc->order,\n\t\t\tcurrent->signal->oom_score_adj);\n\tif (!IS_ENABLED(CONFIG_COMPACTION) && oc->order)\n\t\tpr_warn(\"COMPACTION is disabled!!!\\n\");\n\n\tcpuset_print_current_mems_allowed();\n\tdump_stack();\n\tif (is_memcg_oom(oc))\n\t\tmem_cgroup_print_oom_info(oc->memcg, p);\n\telse {\n\t\tshow_mem(SHOW_MEM_FILTER_NODES, oc->nodemask);\n\t\tif (is_dump_unreclaim_slabs())\n\t\t\tdump_unreclaimable_slab();\n\t}\n\tif (sysctl_oom_dump_tasks)\n\t\tdump_tasks(oc->memcg, oc->nodemask);\n}\n\n/*\n * Number of OOM victims in flight\n */\nstatic atomic_t oom_victims = ATOMIC_INIT(0);\nstatic DECLARE_WAIT_QUEUE_HEAD(oom_victims_wait);\n\nstatic bool oom_killer_disabled __read_mostly;\n\n#define K(x) ((x) << (PAGE_SHIFT-10))\n\n/*\n * task->mm can be NULL if the task is the exited group leader.  So to\n * determine whether the task is using a particular mm, we examine all the\n * task's threads: if one of those is using this mm then this task was also\n * using it.\n */\nbool process_shares_mm(struct task_struct *p, struct mm_struct *mm)\n{\n\tstruct task_struct *t;\n\n\tfor_each_thread(p, t) {\n\t\tstruct mm_struct *t_mm = READ_ONCE(t->mm);\n\t\tif (t_mm)\n\t\t\treturn t_mm == mm;\n\t}\n\treturn false;\n}\n\n\n#ifdef CONFIG_MMU\n/*\n * OOM Reaper kernel thread which tries to reap the memory used by the OOM\n * victim (if that is possible) to help the OOM killer to move on.\n */\nstatic struct task_struct *oom_reaper_th;\nstatic DECLARE_WAIT_QUEUE_HEAD(oom_reaper_wait);\nstatic struct task_struct *oom_reaper_list;\nstatic DEFINE_SPINLOCK(oom_reaper_lock);\n\nstatic bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)\n{\n\tstruct mmu_gather tlb;\n\tstruct vm_area_struct *vma;\n\tbool ret = true;\n\n\t/*\n\t * We have to make sure to not race with the victim exit path\n\t * and cause premature new oom victim selection:\n\t * __oom_reap_task_mm\t\texit_mm\n\t *   mmget_not_zero\n\t *\t\t\t\t  mmput\n\t *\t\t\t\t    atomic_dec_and_test\n\t *\t\t\t\t  exit_oom_victim\n\t *\t\t\t\t[...]\n\t *\t\t\t\tout_of_memory\n\t *\t\t\t\t  select_bad_process\n\t *\t\t\t\t    # no TIF_MEMDIE task selects new victim\n\t *  unmap_page_range # frees some memory\n\t */\n\tmutex_lock(&oom_lock);\n\n\tif (!down_read_trylock(&mm->mmap_sem)) {\n\t\tret = false;\n\t\ttrace_skip_task_reaping(tsk->pid);\n\t\tgoto unlock_oom;\n\t}\n\n\t/*\n\t * If the mm has notifiers then we would need to invalidate them around\n\t * unmap_page_range and that is risky because notifiers can sleep and\n\t * what they do is basically undeterministic.  So let's have a short\n\t * sleep to give the oom victim some more time.\n\t * TODO: we really want to get rid of this ugly hack and make sure that\n\t * notifiers cannot block for unbounded amount of time and add\n\t * mmu_notifier_invalidate_range_{start,end} around unmap_page_range\n\t */\n\tif (mm_has_notifiers(mm)) {\n\t\tup_read(&mm->mmap_sem);\n\t\tschedule_timeout_idle(HZ);\n\t\tgoto unlock_oom;\n\t}\n\n\t/*\n\t * MMF_OOM_SKIP is set by exit_mmap when the OOM reaper can't\n\t * work on the mm anymore. The check for MMF_OOM_SKIP must run\n\t * under mmap_sem for reading because it serializes against the\n\t * down_write();up_write() cycle in exit_mmap().\n\t */\n\tif (test_bit(MMF_OOM_SKIP, &mm->flags)) {\n\t\tup_read(&mm->mmap_sem);\n\t\ttrace_skip_task_reaping(tsk->pid);\n\t\tgoto unlock_oom;\n\t}\n\n\ttrace_start_task_reaping(tsk->pid);\n\n\t/*\n\t * Tell all users of get_user/copy_from_user etc... that the content\n\t * is no longer stable. No barriers really needed because unmapping\n\t * should imply barriers already and the reader would hit a page fault\n\t * if it stumbled over a reaped memory.\n\t */\n\tset_bit(MMF_UNSTABLE, &mm->flags);\n\n\tfor (vma = mm->mmap ; vma; vma = vma->vm_next) {\n\t\tif (!can_madv_dontneed_vma(vma))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Only anonymous pages have a good chance to be dropped\n\t\t * without additional steps which we cannot afford as we\n\t\t * are OOM already.\n\t\t *\n\t\t * We do not even care about fs backed pages because all\n\t\t * which are reclaimable have already been reclaimed and\n\t\t * we do not want to block exit_mmap by keeping mm ref\n\t\t * count elevated without a good reason.\n\t\t */\n\t\tif (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED)) {\n\t\t\ttlb_gather_mmu(&tlb, mm, vma->vm_start, vma->vm_end);\n\t\t\tunmap_page_range(&tlb, vma, vma->vm_start, vma->vm_end,\n\t\t\t\t\t NULL);\n\t\t\ttlb_finish_mmu(&tlb, vma->vm_start, vma->vm_end);\n\t\t}\n\t}\n\tpr_info(\"oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\\n\",\n\t\t\ttask_pid_nr(tsk), tsk->comm,\n\t\t\tK(get_mm_counter(mm, MM_ANONPAGES)),\n\t\t\tK(get_mm_counter(mm, MM_FILEPAGES)),\n\t\t\tK(get_mm_counter(mm, MM_SHMEMPAGES)));\n\tup_read(&mm->mmap_sem);\n\n\ttrace_finish_task_reaping(tsk->pid);\nunlock_oom:\n\tmutex_unlock(&oom_lock);\n\treturn ret;\n}\n\n#define MAX_OOM_REAP_RETRIES 10\nstatic void oom_reap_task(struct task_struct *tsk)\n{\n\tint attempts = 0;\n\tstruct mm_struct *mm = tsk->signal->oom_mm;\n\n\t/* Retry the down_read_trylock(mmap_sem) a few times */\n\twhile (attempts++ < MAX_OOM_REAP_RETRIES && !__oom_reap_task_mm(tsk, mm))\n\t\tschedule_timeout_idle(HZ/10);\n\n\tif (attempts <= MAX_OOM_REAP_RETRIES)\n\t\tgoto done;\n\n\n\tpr_info(\"oom_reaper: unable to reap pid:%d (%s)\\n\",\n\t\ttask_pid_nr(tsk), tsk->comm);\n\tdebug_show_all_locks();\n\ndone:\n\ttsk->oom_reaper_list = NULL;\n\n\t/*\n\t * Hide this mm from OOM killer because it has been either reaped or\n\t * somebody can't call up_write(mmap_sem).\n\t */\n\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\n\t/* Drop a reference taken by wake_oom_reaper */\n\tput_task_struct(tsk);\n}\n\nstatic int oom_reaper(void *unused)\n{\n\twhile (true) {\n\t\tstruct task_struct *tsk = NULL;\n\n\t\twait_event_freezable(oom_reaper_wait, oom_reaper_list != NULL);\n\t\tspin_lock(&oom_reaper_lock);\n\t\tif (oom_reaper_list != NULL) {\n\t\t\ttsk = oom_reaper_list;\n\t\t\toom_reaper_list = tsk->oom_reaper_list;\n\t\t}\n\t\tspin_unlock(&oom_reaper_lock);\n\n\t\tif (tsk)\n\t\t\toom_reap_task(tsk);\n\t}\n\n\treturn 0;\n}\n\nstatic void wake_oom_reaper(struct task_struct *tsk)\n{\n\t/* tsk is already queued? */\n\tif (tsk == oom_reaper_list || tsk->oom_reaper_list)\n\t\treturn;\n\n\tget_task_struct(tsk);\n\n\tspin_lock(&oom_reaper_lock);\n\ttsk->oom_reaper_list = oom_reaper_list;\n\toom_reaper_list = tsk;\n\tspin_unlock(&oom_reaper_lock);\n\ttrace_wake_reaper(tsk->pid);\n\twake_up(&oom_reaper_wait);\n}\n\nstatic int __init oom_init(void)\n{\n\toom_reaper_th = kthread_run(oom_reaper, NULL, \"oom_reaper\");\n\treturn 0;\n}\nsubsys_initcall(oom_init)\n#else\nstatic inline void wake_oom_reaper(struct task_struct *tsk)\n{\n}\n#endif /* CONFIG_MMU */\n\n/**\n * mark_oom_victim - mark the given task as OOM victim\n * @tsk: task to mark\n *\n * Has to be called with oom_lock held and never after\n * oom has been disabled already.\n *\n * tsk->mm has to be non NULL and caller has to guarantee it is stable (either\n * under task_lock or operate on the current).\n */\nstatic void mark_oom_victim(struct task_struct *tsk)\n{\n\tstruct mm_struct *mm = tsk->mm;\n\n\tWARN_ON(oom_killer_disabled);\n\t/* OOM killer might race with memcg OOM */\n\tif (test_and_set_tsk_thread_flag(tsk, TIF_MEMDIE))\n\t\treturn;\n\n\t/* oom_mm is bound to the signal struct life time. */\n\tif (!cmpxchg(&tsk->signal->oom_mm, NULL, mm))\n\t\tmmgrab(tsk->signal->oom_mm);\n\n\t/*\n\t * Make sure that the task is woken up from uninterruptible sleep\n\t * if it is frozen because OOM killer wouldn't be able to free\n\t * any memory and livelock. freezing_slow_path will tell the freezer\n\t * that TIF_MEMDIE tasks should be ignored.\n\t */\n\t__thaw_task(tsk);\n\tatomic_inc(&oom_victims);\n\ttrace_mark_victim(tsk->pid);\n}\n\n/**\n * exit_oom_victim - note the exit of an OOM victim\n */\nvoid exit_oom_victim(void)\n{\n\tclear_thread_flag(TIF_MEMDIE);\n\n\tif (!atomic_dec_return(&oom_victims))\n\t\twake_up_all(&oom_victims_wait);\n}\n\n/**\n * oom_killer_enable - enable OOM killer\n */\nvoid oom_killer_enable(void)\n{\n\toom_killer_disabled = false;\n\tpr_info(\"OOM killer enabled.\\n\");\n}\n\n/**\n * oom_killer_disable - disable OOM killer\n * @timeout: maximum timeout to wait for oom victims in jiffies\n *\n * Forces all page allocations to fail rather than trigger OOM killer.\n * Will block and wait until all OOM victims are killed or the given\n * timeout expires.\n *\n * The function cannot be called when there are runnable user tasks because\n * the userspace would see unexpected allocation failures as a result. Any\n * new usage of this function should be consulted with MM people.\n *\n * Returns true if successful and false if the OOM killer cannot be\n * disabled.\n */\nbool oom_killer_disable(signed long timeout)\n{\n\tsigned long ret;\n\n\t/*\n\t * Make sure to not race with an ongoing OOM killer. Check that the\n\t * current is not killed (possibly due to sharing the victim's memory).\n\t */\n\tif (mutex_lock_killable(&oom_lock))\n\t\treturn false;\n\toom_killer_disabled = true;\n\tmutex_unlock(&oom_lock);\n\n\tret = wait_event_interruptible_timeout(oom_victims_wait,\n\t\t\t!atomic_read(&oom_victims), timeout);\n\tif (ret <= 0) {\n\t\toom_killer_enable();\n\t\treturn false;\n\t}\n\tpr_info(\"OOM killer disabled.\\n\");\n\n\treturn true;\n}\n\nstatic inline bool __task_will_free_mem(struct task_struct *task)\n{\n\tstruct signal_struct *sig = task->signal;\n\n\t/*\n\t * A coredumping process may sleep for an extended period in exit_mm(),\n\t * so the oom killer cannot assume that the process will promptly exit\n\t * and release memory.\n\t */\n\tif (sig->flags & SIGNAL_GROUP_COREDUMP)\n\t\treturn false;\n\n\tif (sig->flags & SIGNAL_GROUP_EXIT)\n\t\treturn true;\n\n\tif (thread_group_empty(task) && (task->flags & PF_EXITING))\n\t\treturn true;\n\n\treturn false;\n}\n\n/*\n * Checks whether the given task is dying or exiting and likely to\n * release its address space. This means that all threads and processes\n * sharing the same mm have to be killed or exiting.\n * Caller has to make sure that task->mm is stable (hold task_lock or\n * it operates on the current).\n */\nstatic bool task_will_free_mem(struct task_struct *task)\n{\n\tstruct mm_struct *mm = task->mm;\n\tstruct task_struct *p;\n\tbool ret = true;\n\n\t/*\n\t * Skip tasks without mm because it might have passed its exit_mm and\n\t * exit_oom_victim. oom_reaper could have rescued that but do not rely\n\t * on that for now. We can consider find_lock_task_mm in future.\n\t */\n\tif (!mm)\n\t\treturn false;\n\n\tif (!__task_will_free_mem(task))\n\t\treturn false;\n\n\t/*\n\t * This task has already been drained by the oom reaper so there are\n\t * only small chances it will free some more\n\t */\n\tif (test_bit(MMF_OOM_SKIP, &mm->flags))\n\t\treturn false;\n\n\tif (atomic_read(&mm->mm_users) <= 1)\n\t\treturn true;\n\n\t/*\n\t * Make sure that all tasks which share the mm with the given tasks\n\t * are dying as well to make sure that a) nobody pins its mm and\n\t * b) the task is also reapable by the oom reaper.\n\t */\n\trcu_read_lock();\n\tfor_each_process(p) {\n\t\tif (!process_shares_mm(p, mm))\n\t\t\tcontinue;\n\t\tif (same_thread_group(task, p))\n\t\t\tcontinue;\n\t\tret = __task_will_free_mem(p);\n\t\tif (!ret)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic void oom_kill_process(struct oom_control *oc, const char *message)\n{\n\tstruct task_struct *p = oc->chosen;\n\tunsigned int points = oc->chosen_points;\n\tstruct task_struct *victim = p;\n\tstruct task_struct *child;\n\tstruct task_struct *t;\n\tstruct mm_struct *mm;\n\tunsigned int victim_points = 0;\n\tstatic DEFINE_RATELIMIT_STATE(oom_rs, DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\tbool can_oom_reap = true;\n\n\t/*\n\t * If the task is already exiting, don't alarm the sysadmin or kill\n\t * its children or threads, just give it access to memory reserves\n\t * so it can die quickly\n\t */\n\ttask_lock(p);\n\tif (task_will_free_mem(p)) {\n\t\tmark_oom_victim(p);\n\t\twake_oom_reaper(p);\n\t\ttask_unlock(p);\n\t\tput_task_struct(p);\n\t\treturn;\n\t}\n\ttask_unlock(p);\n\n\tif (__ratelimit(&oom_rs))\n\t\tdump_header(oc, p);\n\n\tpr_err(\"%s: Kill process %d (%s) score %u or sacrifice child\\n\",\n\t\tmessage, task_pid_nr(p), p->comm, points);\n\n\t/*\n\t * If any of p's children has a different mm and is eligible for kill,\n\t * the one with the highest oom_badness() score is sacrificed for its\n\t * parent.  This attempts to lose the minimal amount of work done while\n\t * still freeing memory.\n\t */\n\tread_lock(&tasklist_lock);\n\tfor_each_thread(p, t) {\n\t\tlist_for_each_entry(child, &t->children, sibling) {\n\t\t\tunsigned int child_points;\n\n\t\t\tif (process_shares_mm(child, p->mm))\n\t\t\t\tcontinue;\n\t\t\t/*\n\t\t\t * oom_badness() returns 0 if the thread is unkillable\n\t\t\t */\n\t\t\tchild_points = oom_badness(child,\n\t\t\t\toc->memcg, oc->nodemask, oc->totalpages);\n\t\t\tif (child_points > victim_points) {\n\t\t\t\tput_task_struct(victim);\n\t\t\t\tvictim = child;\n\t\t\t\tvictim_points = child_points;\n\t\t\t\tget_task_struct(victim);\n\t\t\t}\n\t\t}\n\t}\n\tread_unlock(&tasklist_lock);\n\n\tp = find_lock_task_mm(victim);\n\tif (!p) {\n\t\tput_task_struct(victim);\n\t\treturn;\n\t} else if (victim != p) {\n\t\tget_task_struct(p);\n\t\tput_task_struct(victim);\n\t\tvictim = p;\n\t}\n\n\t/* Get a reference to safely compare mm after task_unlock(victim) */\n\tmm = victim->mm;\n\tmmgrab(mm);\n\n\t/* Raise event before sending signal: task reaper must see this */\n\tcount_vm_event(OOM_KILL);\n\tcount_memcg_event_mm(mm, OOM_KILL);\n\n\t/*\n\t * We should send SIGKILL before granting access to memory reserves\n\t * in order to prevent the OOM victim from depleting the memory\n\t * reserves from the user space under its control.\n\t */\n\tdo_send_sig_info(SIGKILL, SEND_SIG_FORCED, victim, true);\n\tmark_oom_victim(victim);\n\tpr_err(\"Killed process %d (%s) total-vm:%lukB, anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\\n\",\n\t\ttask_pid_nr(victim), victim->comm, K(victim->mm->total_vm),\n\t\tK(get_mm_counter(victim->mm, MM_ANONPAGES)),\n\t\tK(get_mm_counter(victim->mm, MM_FILEPAGES)),\n\t\tK(get_mm_counter(victim->mm, MM_SHMEMPAGES)));\n\ttask_unlock(victim);\n\n\t/*\n\t * Kill all user processes sharing victim->mm in other thread groups, if\n\t * any.  They don't get access to memory reserves, though, to avoid\n\t * depletion of all memory.  This prevents mm->mmap_sem livelock when an\n\t * oom killed thread cannot exit because it requires the semaphore and\n\t * its contended by another thread trying to allocate memory itself.\n\t * That thread will now get access to memory reserves since it has a\n\t * pending fatal signal.\n\t */\n\trcu_read_lock();\n\tfor_each_process(p) {\n\t\tif (!process_shares_mm(p, mm))\n\t\t\tcontinue;\n\t\tif (same_thread_group(p, victim))\n\t\t\tcontinue;\n\t\tif (is_global_init(p)) {\n\t\t\tcan_oom_reap = false;\n\t\t\tset_bit(MMF_OOM_SKIP, &mm->flags);\n\t\t\tpr_info(\"oom killer %d (%s) has mm pinned by %d (%s)\\n\",\n\t\t\t\t\ttask_pid_nr(victim), victim->comm,\n\t\t\t\t\ttask_pid_nr(p), p->comm);\n\t\t\tcontinue;\n\t\t}\n\t\t/*\n\t\t * No use_mm() user needs to read from the userspace so we are\n\t\t * ok to reap it.\n\t\t */\n\t\tif (unlikely(p->flags & PF_KTHREAD))\n\t\t\tcontinue;\n\t\tdo_send_sig_info(SIGKILL, SEND_SIG_FORCED, p, true);\n\t}\n\trcu_read_unlock();\n\n\tif (can_oom_reap)\n\t\twake_oom_reaper(victim);\n\n\tmmdrop(mm);\n\tput_task_struct(victim);\n}\n#undef K\n\n/*\n * Determines whether the kernel must panic because of the panic_on_oom sysctl.\n */\nstatic void check_panic_on_oom(struct oom_control *oc,\n\t\t\t       enum oom_constraint constraint)\n{\n\tif (likely(!sysctl_panic_on_oom))\n\t\treturn;\n\tif (sysctl_panic_on_oom != 2) {\n\t\t/*\n\t\t * panic_on_oom == 1 only affects CONSTRAINT_NONE, the kernel\n\t\t * does not panic for cpuset, mempolicy, or memcg allocation\n\t\t * failures.\n\t\t */\n\t\tif (constraint != CONSTRAINT_NONE)\n\t\t\treturn;\n\t}\n\t/* Do not panic for oom kills triggered by sysrq */\n\tif (is_sysrq_oom(oc))\n\t\treturn;\n\tdump_header(oc, NULL);\n\tpanic(\"Out of memory: %s panic_on_oom is enabled\\n\",\n\t\tsysctl_panic_on_oom == 2 ? \"compulsory\" : \"system-wide\");\n}\n\nstatic BLOCKING_NOTIFIER_HEAD(oom_notify_list);\n\nint register_oom_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_register(&oom_notify_list, nb);\n}\nEXPORT_SYMBOL_GPL(register_oom_notifier);\n\nint unregister_oom_notifier(struct notifier_block *nb)\n{\n\treturn blocking_notifier_chain_unregister(&oom_notify_list, nb);\n}\nEXPORT_SYMBOL_GPL(unregister_oom_notifier);\n\n/**\n * out_of_memory - kill the \"best\" process when we run out of memory\n * @oc: pointer to struct oom_control\n *\n * If we run out of memory, we have the choice between either\n * killing a random task (bad), letting the system crash (worse)\n * OR try to be smart about which process to kill. Note that we\n * don't have to be perfect here, we just have to be good.\n */\nbool out_of_memory(struct oom_control *oc)\n{\n\tunsigned long freed = 0;\n\tenum oom_constraint constraint = CONSTRAINT_NONE;\n\n\tif (oom_killer_disabled)\n\t\treturn false;\n\n\tif (!is_memcg_oom(oc)) {\n\t\tblocking_notifier_call_chain(&oom_notify_list, 0, &freed);\n\t\tif (freed > 0)\n\t\t\t/* Got some memory back in the last second. */\n\t\t\treturn true;\n\t}\n\n\t/*\n\t * If current has a pending SIGKILL or is exiting, then automatically\n\t * select it.  The goal is to allow it to allocate so that it may\n\t * quickly exit and free its memory.\n\t */\n\tif (task_will_free_mem(current)) {\n\t\tmark_oom_victim(current);\n\t\twake_oom_reaper(current);\n\t\treturn true;\n\t}\n\n\t/*\n\t * The OOM killer does not compensate for IO-less reclaim.\n\t * pagefault_out_of_memory lost its gfp context so we have to\n\t * make sure exclude 0 mask - all other users should have at least\n\t * ___GFP_DIRECT_RECLAIM to get here.\n\t */\n\tif (oc->gfp_mask && !(oc->gfp_mask & __GFP_FS))\n\t\treturn true;\n\n\t/*\n\t * Check if there were limitations on the allocation (only relevant for\n\t * NUMA and memcg) that may require different handling.\n\t */\n\tconstraint = constrained_alloc(oc);\n\tif (constraint != CONSTRAINT_MEMORY_POLICY)\n\t\toc->nodemask = NULL;\n\tcheck_panic_on_oom(oc, constraint);\n\n\tif (!is_memcg_oom(oc) && sysctl_oom_kill_allocating_task &&\n\t    current->mm && !oom_unkillable_task(current, NULL, oc->nodemask) &&\n\t    current->signal->oom_score_adj != OOM_SCORE_ADJ_MIN) {\n\t\tget_task_struct(current);\n\t\toc->chosen = current;\n\t\toom_kill_process(oc, \"Out of memory (oom_kill_allocating_task)\");\n\t\treturn true;\n\t}\n\n\tselect_bad_process(oc);\n\t/* Found nothing?!?! Either we hang forever, or we panic. */\n\tif (!oc->chosen && !is_sysrq_oom(oc) && !is_memcg_oom(oc)) {\n\t\tdump_header(oc, NULL);\n\t\tpanic(\"Out of memory and no killable processes...\\n\");\n\t}\n\tif (oc->chosen && oc->chosen != (void *)-1UL) {\n\t\toom_kill_process(oc, !is_memcg_oom(oc) ? \"Out of memory\" :\n\t\t\t\t \"Memory cgroup out of memory\");\n\t\t/*\n\t\t * Give the killed process a good chance to exit before trying\n\t\t * to allocate memory again.\n\t\t */\n\t\tschedule_timeout_killable(1);\n\t}\n\treturn !!oc->chosen;\n}\n\n/*\n * The pagefault handler calls here because it is out of memory, so kill a\n * memory-hogging task. If oom_lock is held by somebody else, a parallel oom\n * killing is already in progress so do nothing.\n */\nvoid pagefault_out_of_memory(void)\n{\n\tstruct oom_control oc = {\n\t\t.zonelist = NULL,\n\t\t.nodemask = NULL,\n\t\t.memcg = NULL,\n\t\t.gfp_mask = 0,\n\t\t.order = 0,\n\t};\n\n\tif (mem_cgroup_oom_synchronize(true))\n\t\treturn;\n\n\tif (!mutex_trylock(&oom_lock))\n\t\treturn;\n\tout_of_memory(&oc);\n\tmutex_unlock(&oom_lock);\n}\n"], "filenames": ["mm/oom_kill.c"], "buggy_code_start_loc": [553], "buggy_code_end_loc": [573], "fixing_code_start_loc": [552], "fixing_code_end_loc": [574], "type": "CWE-416", "message": "The __oom_reap_task_mm function in mm/oom_kill.c in the Linux kernel before 4.14.4 mishandles gather operations, which allows attackers to cause a denial of service (TLB entry leak or use-after-free) or possibly have unspecified other impact by triggering a copy_to_user call within a certain time window.", "other": {"cve": {"id": "CVE-2017-18202", "sourceIdentifier": "cve@mitre.org", "published": "2018-02-27T06:29:00.257", "lastModified": "2018-09-26T10:29:11.820", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The __oom_reap_task_mm function in mm/oom_kill.c in the Linux kernel before 4.14.4 mishandles gather operations, which allows attackers to cause a denial of service (TLB entry leak or use-after-free) or possibly have unspecified other impact by triggering a copy_to_user call within a certain time window."}, {"lang": "es", "value": "La funci\u00f3n __oom_reap_task_mm en mm/oom_kill.c en el kernel de Linux, en versiones anteriores a la 4.14.4, gestiona de manera incorrecta las operaciones de recopilaci\u00f3n. Esto permite que los atacantes provoquen una denegaci\u00f3n de servicio (filtrado de entrada TLB o uso de memoria previamente liberada) u otro tipo de impacto sin especificar desencadenando una llamada copy_to_user en un periodo de tiempo determinado."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.0, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.0, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.14.4", "matchCriteriaId": "273770A3-0E6A-491C-ABE6-E3B747F4C5BD"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=687cb0884a714ff484d038e9190edc874edcf146", "source": "cve@mitre.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://www.securityfocus.com/bid/103161", "source": "cve@mitre.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2018:2772", "source": "cve@mitre.org"}, {"url": "https://github.com/torvalds/linux/commit/687cb0884a714ff484d038e9190edc874edcf146", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://www.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.14.4", "source": "cve@mitre.org", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/687cb0884a714ff484d038e9190edc874edcf146"}}