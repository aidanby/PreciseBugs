{"buggy_code": ["/* Copyright (C) 2018-2020 Open Information Security Foundation\n *\n * You can copy, redistribute or modify this Program under the terms of\n * the GNU General Public License version 2 as published by the Free\n * Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * version 2 along with this program; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n * 02110-1301, USA.\n */\n\n/**\n * \\file\n *\n *  \\author Victor Julien <victor@inliniac.net>\n *\n *  Implements the dataset keyword\n */\n\n#include \"suricata-common.h\"\n#include \"decode.h\"\n#include \"detect.h\"\n#include \"threads.h\"\n#include \"datasets.h\"\n#include \"detect-dataset.h\"\n\n#include \"detect-parse.h\"\n#include \"detect-engine.h\"\n#include \"detect-engine-mpm.h\"\n#include \"detect-engine-state.h\"\n\n#include \"util-debug.h\"\n#include \"util-print.h\"\n#include \"util-misc.h\"\n\nint DetectDatasetMatch (ThreadVars *, DetectEngineThreadCtx *, Packet *,\n        const Signature *, const SigMatchCtx *);\nstatic int DetectDatasetSetup (DetectEngineCtx *, Signature *, const char *);\nvoid DetectDatasetFree (DetectEngineCtx *, void *);\n\nvoid DetectDatasetRegister (void)\n{\n    sigmatch_table[DETECT_DATASET].name = \"dataset\";\n    sigmatch_table[DETECT_DATASET].desc = \"match sticky buffer against datasets (experimental)\";\n    sigmatch_table[DETECT_DATASET].url = \"/rules/dataset-keywords.html#dataset\";\n    sigmatch_table[DETECT_DATASET].Setup = DetectDatasetSetup;\n    sigmatch_table[DETECT_DATASET].Free  = DetectDatasetFree;\n}\n\n/*\n    1 match\n    0 no match\n    -1 can't match\n */\nint DetectDatasetBufferMatch(DetectEngineThreadCtx *det_ctx,\n    const DetectDatasetData *sd,\n    const uint8_t *data, const uint32_t data_len)\n{\n    if (data == NULL || data_len == 0)\n        return 0;\n\n    switch (sd->cmd) {\n        case DETECT_DATASET_CMD_ISSET: {\n            //PrintRawDataFp(stdout, data, data_len);\n            int r = DatasetLookup(sd->set, data, data_len);\n            SCLogDebug(\"r %d\", r);\n            if (r == 1)\n                return 1;\n            break;\n        }\n        case DETECT_DATASET_CMD_ISNOTSET: {\n            //PrintRawDataFp(stdout, data, data_len);\n            int r = DatasetLookup(sd->set, data, data_len);\n            SCLogDebug(\"r %d\", r);\n            if (r < 1)\n                return 1;\n            break;\n        }\n        case DETECT_DATASET_CMD_SET: {\n            //PrintRawDataFp(stdout, data, data_len);\n            int r = DatasetAdd(sd->set, data, data_len);\n            if (r == 1)\n                return 1;\n            break;\n        }\n        default:\n            abort();\n    }\n    return 0;\n}\n\nstatic int DetectDatasetParse(const char *str, char *cmd, int cmd_len, char *name, int name_len,\n        enum DatasetTypes *type, char *load, size_t load_size, char *save, size_t save_size,\n        uint64_t *memcap, uint32_t *hashsize)\n{\n    bool cmd_set = false;\n    bool name_set = false;\n    bool load_set = false;\n    bool save_set = false;\n    bool state_set = false;\n\n    char copy[strlen(str)+1];\n    strlcpy(copy, str, sizeof(copy));\n    char *xsaveptr = NULL;\n    char *key = strtok_r(copy, \",\", &xsaveptr);\n    while (key != NULL) {\n        while (*key != '\\0' && isblank(*key)) {\n            key++;\n        }\n        char *val = strchr(key, ' ');\n        if (val != NULL) {\n            *val++ = '\\0';\n            while (*val != '\\0' && isblank(*val)) {\n                val++;\n                SCLogDebug(\"cmd %s val %s\", key, val);\n            }\n        } else {\n            SCLogDebug(\"cmd %s\", key);\n        }\n\n        if (strlen(key) == 0) {\n            goto next;\n        }\n\n        if (!cmd_set) {\n            if (val && strlen(val) != 0) {\n                return -1;\n            }\n            strlcpy(cmd, key, cmd_len);\n            cmd_set = true;\n        } else if (!name_set) {\n            if (val && strlen(val) != 0) {\n                return -1;\n            }\n            strlcpy(name, key, name_len);\n            name_set = true;\n        } else {\n            if (val == NULL) {\n                return -1;\n            }\n\n            if (strcmp(key, \"type\") == 0) {\n                SCLogDebug(\"type %s\", val);\n\n                if (strcmp(val, \"md5\") == 0) {\n                    *type = DATASET_TYPE_MD5;\n                } else if (strcmp(val, \"sha256\") == 0) {\n                    *type = DATASET_TYPE_SHA256;\n                } else if (strcmp(val, \"string\") == 0) {\n                    *type = DATASET_TYPE_STRING;\n                } else {\n                    SCLogError(SC_ERR_INVALID_SIGNATURE, \"bad type %s\", val);\n                    return -1;\n                }\n\n            } else if (strcmp(key, \"save\") == 0) {\n                if (save_set) {\n                    SCLogWarning(SC_ERR_INVALID_SIGNATURE,\n                        \"'save' can only appear once\");\n                    return -1;\n                }\n                SCLogDebug(\"save %s\", val);\n                strlcpy(save, val, save_size);\n                save_set = true;\n            } else if (strcmp(key, \"load\") == 0) {\n                if (load_set) {\n                    SCLogWarning(SC_ERR_INVALID_SIGNATURE,\n                        \"'load' can only appear once\");\n                    return -1;\n                }\n                SCLogDebug(\"load %s\", val);\n                strlcpy(load, val, load_size);\n                load_set = true;\n            } else if (strcmp(key, \"state\") == 0) {\n                if (state_set) {\n                    SCLogWarning(SC_ERR_INVALID_SIGNATURE,\n                        \"'state' can only appear once\");\n                    return -1;\n                }\n                SCLogDebug(\"state %s\", val);\n                strlcpy(load, val, load_size);\n                strlcpy(save, val, save_size);\n                state_set = true;\n            }\n            if (strcmp(key, \"memcap\") == 0) {\n                if (ParseSizeStringU64(val, memcap) < 0) {\n                    SCLogWarning(SC_ERR_INVALID_VALUE,\n                            \"invalid value for memcap: %s,\"\n                            \" resetting to default\",\n                            val);\n                    *memcap = 0;\n                }\n            }\n            if (strcmp(key, \"hashsize\") == 0) {\n                if (ParseSizeStringU32(val, hashsize) < 0) {\n                    SCLogWarning(SC_ERR_INVALID_VALUE,\n                            \"invalid value for hashsize: %s,\"\n                            \" resetting to default\",\n                            val);\n                    *hashsize = 0;\n                }\n            }\n        }\n\n        SCLogDebug(\"key: %s, value: %s\", key, val);\n\n    next:\n        key = strtok_r(NULL, \",\", &xsaveptr);\n    }\n\n    if ((load_set || save_set) && state_set) {\n        SCLogWarning(SC_ERR_INVALID_SIGNATURE,\n                \"'state' can not be mixed with 'load' and 'save'\");\n        return -1;\n    }\n\n    /* Trim trailing whitespace. */\n    while (strlen(name) > 0 && isblank(name[strlen(name) - 1])) {\n        name[strlen(name) - 1] = '\\0';\n    }\n\n    /* Validate name, spaces are not allowed. */\n    for (size_t i = 0; i < strlen(name); i++) {\n        if (isblank(name[i])) {\n            SCLogError(SC_ERR_INVALID_SIGNATURE,\n                    \"spaces not allowed in dataset names\");\n            return 0;\n        }\n    }\n\n    return 1;\n}\n\n/** \\brief wrapper around dirname that does leave input untouched */\nstatic void GetDirName(const char *in, char *out, size_t outs)\n{\n    if (strlen(in) == 0) {\n        return;\n    }\n\n    size_t size = strlen(in) + 1;\n    char tmp[size];\n    strlcpy(tmp, in, size);\n\n    char *dir = dirname(tmp);\n    BUG_ON(dir == NULL);\n    strlcpy(out, dir, outs);\n    return;\n}\n\nstatic int SetupLoadPath(const DetectEngineCtx *de_ctx,\n        char *load, size_t load_size)\n{\n    SCLogDebug(\"load %s\", load);\n\n    if (PathIsAbsolute(load)) {\n        return 0;\n    }\n\n    bool done = false;\n#ifdef HAVE_LIBGEN_H\n    BUG_ON(de_ctx->rule_file == NULL);\n\n    char dir[PATH_MAX] = \"\";\n    GetDirName(de_ctx->rule_file, dir, sizeof(dir));\n\n    SCLogDebug(\"rule_file %s dir %s\", de_ctx->rule_file, dir);\n    char path[PATH_MAX];\n    if (snprintf(path, sizeof(path), \"%s/%s\", dir, load) >= (int)sizeof(path)) // TODO windows path\n        return -1;\n\n    if (SCPathExists(path)) {\n        done = true;\n        strlcpy(load, path, load_size);\n        SCLogDebug(\"using path '%s' (HAVE_LIBGEN_H)\", load);\n    }\n#endif\n    if (!done) {\n        char *loadp = DetectLoadCompleteSigPath(de_ctx, load);\n        if (loadp == NULL) {\n            return -1;\n        }\n        SCLogDebug(\"loadp %s\", loadp);\n\n        if (SCPathExists(loadp)) {\n            strlcpy(load, loadp, load_size);\n            SCLogDebug(\"using path '%s' (non-HAVE_LIBGEN_H)\", load);\n        }\n        SCFree(loadp);\n    }\n    return 0;\n}\n\nstatic int SetupSavePath(const DetectEngineCtx *de_ctx,\n        char *save, size_t save_size)\n{\n    SCLogDebug(\"save %s\", save);\n\n    int allow_absolute = 0;\n    (void)ConfGetBool(\"datasets.rules.allow-absolute-filenames\", &allow_absolute);\n    if (allow_absolute) {\n        SCLogNotice(\"Allowing absolute filename for dataset rule: %s\", save);\n    } else {\n        if (PathIsAbsolute(save)) {\n            SCLogError(SC_ERR_INVALID_ARGUMENT, \"Absolute paths not allowed: %s\", save);\n            return -1;\n        }\n\n        if (SCPathContainsTraversal(save)) {\n            SCLogError(SC_ERR_INVALID_ARGUMENT, \"Directory traversals not allowed: %s\", save);\n            return -1;\n        }\n    }\n\n    // data dir\n    const char *dir = ConfigGetDataDirectory();\n    BUG_ON(dir == NULL); // should not be able to fail\n    char path[PATH_MAX];\n    if (snprintf(path, sizeof(path), \"%s/%s\", dir, save) >= (int)sizeof(path)) // TODO windows path\n        return -1;\n\n    /* TODO check if location exists and is writable */\n\n    strlcpy(save, path, save_size);\n\n    return 0;\n}\n\nint DetectDatasetSetup (DetectEngineCtx *de_ctx, Signature *s, const char *rawstr)\n{\n    DetectDatasetData *cd = NULL;\n    SigMatch *sm = NULL;\n    uint8_t cmd = 0;\n    uint64_t memcap = 0;\n    uint32_t hashsize = 0;\n    char cmd_str[16] = \"\", name[DATASET_NAME_MAX_LEN + 1] = \"\";\n    enum DatasetTypes type = DATASET_TYPE_NOTSET;\n    char load[PATH_MAX] = \"\";\n    char save[PATH_MAX] = \"\";\n\n    if (DetectBufferGetActiveList(de_ctx, s) == -1) {\n        SCLogError(SC_ERR_INVALID_SIGNATURE,\n                \"datasets are only supported for sticky buffers\");\n        SCReturnInt(-1);\n    }\n\n    int list = s->init_data->list;\n    if (list == DETECT_SM_LIST_NOTSET) {\n        SCLogError(SC_ERR_INVALID_SIGNATURE,\n                \"datasets are only supported for sticky buffers\");\n        SCReturnInt(-1);\n    }\n\n    if (!DetectDatasetParse(rawstr, cmd_str, sizeof(cmd_str), name, sizeof(name), &type, load,\n                sizeof(load), save, sizeof(save), &memcap, &hashsize)) {\n        return -1;\n    }\n\n    if (strcmp(cmd_str,\"isset\") == 0) {\n        cmd = DETECT_DATASET_CMD_ISSET;\n    } else if (strcmp(cmd_str,\"isnotset\") == 0) {\n        cmd = DETECT_DATASET_CMD_ISNOTSET;\n    } else if (strcmp(cmd_str,\"set\") == 0) {\n        cmd = DETECT_DATASET_CMD_SET;\n    } else if (strcmp(cmd_str,\"unset\") == 0) {\n        cmd = DETECT_DATASET_CMD_UNSET;\n    } else {\n        SCLogError(SC_ERR_UNKNOWN_VALUE,\n                \"dataset action \\\"%s\\\" is not supported.\", cmd_str);\n        return -1;\n    }\n\n    /* if just 'load' is set, we load data from the same dir as the\n     * rule file. If load+save is used, we use data dir */\n    if (strlen(save) == 0 && strlen(load) != 0) {\n        if (SetupLoadPath(de_ctx, load, sizeof(load)) != 0)\n            return -1;\n    /* if just 'save' is set, we use either full path or the\n     * data-dir */\n    } else if (strlen(save) != 0 && strlen(load) == 0) {\n        if (SetupSavePath(de_ctx, save, sizeof(save)) != 0)\n            return -1;\n    /* use 'save' logic for 'state', but put the resulting\n     * path into 'load' as well. */\n    } else if (strlen(save) != 0 && strlen(load) != 0 &&\n            strcmp(save, load) == 0) {\n        if (SetupSavePath(de_ctx, save, sizeof(save)) != 0)\n            return -1;\n        strlcpy(load, save, sizeof(load));\n    }\n\n    SCLogDebug(\"name '%s' load '%s' save '%s'\", name, load, save);\n    Dataset *set = DatasetGet(name, type, save, load, memcap, hashsize);\n    if (set == NULL) {\n        SCLogError(SC_ERR_INVALID_SIGNATURE,\n                \"failed to set up dataset '%s'.\", name);\n        return -1;\n    }\n    if (set->hash && SC_ATOMIC_GET(set->hash->memcap_reached)) {\n        SCLogError(SC_ERR_THASH_INIT, \"dataset too large for set memcap\");\n        return -1;\n    }\n\n    cd = SCCalloc(1, sizeof(DetectDatasetData));\n    if (unlikely(cd == NULL))\n        goto error;\n\n    cd->set = set;\n    cd->cmd = cmd;\n\n    SCLogDebug(\"cmd %s, name %s\",\n        cmd_str, strlen(name) ? name : \"(none)\");\n\n    /* Okay so far so good, lets get this into a SigMatch\n     * and put it in the Signature. */\n    sm = SigMatchAlloc();\n    if (sm == NULL)\n        goto error;\n\n    sm->type = DETECT_DATASET;\n    sm->ctx = (SigMatchCtx *)cd;\n    SigMatchAppendSMToList(s, sm, list);\n    return 0;\n\nerror:\n    if (cd != NULL)\n        SCFree(cd);\n    if (sm != NULL)\n        SCFree(sm);\n    return -1;\n}\n\nvoid DetectDatasetFree (DetectEngineCtx *de_ctx, void *ptr)\n{\n    DetectDatasetData *fd = (DetectDatasetData *)ptr;\n    if (fd == NULL)\n        return;\n\n    SCFree(fd);\n}\n", "%YAML 1.1\n---\n\n# Suricata configuration file. In addition to the comments describing all\n# options in this file, full documentation can be found at:\n# https://suricata.readthedocs.io/en/latest/configuration/suricata-yaml.html\n\n# This configuration file generated by Suricata @PACKAGE_VERSION@.\nsuricata-version: \"@MAJOR_MINOR@\"\n\n##\n## Step 1: Inform Suricata about your network\n##\n\nvars:\n  # more specific is better for alert accuracy and performance\n  address-groups:\n    HOME_NET: \"[192.168.0.0/16,10.0.0.0/8,172.16.0.0/12]\"\n    #HOME_NET: \"[192.168.0.0/16]\"\n    #HOME_NET: \"[10.0.0.0/8]\"\n    #HOME_NET: \"[172.16.0.0/12]\"\n    #HOME_NET: \"any\"\n\n    EXTERNAL_NET: \"!$HOME_NET\"\n    #EXTERNAL_NET: \"any\"\n\n    HTTP_SERVERS: \"$HOME_NET\"\n    SMTP_SERVERS: \"$HOME_NET\"\n    SQL_SERVERS: \"$HOME_NET\"\n    DNS_SERVERS: \"$HOME_NET\"\n    TELNET_SERVERS: \"$HOME_NET\"\n    AIM_SERVERS: \"$EXTERNAL_NET\"\n    DC_SERVERS: \"$HOME_NET\"\n    DNP3_SERVER: \"$HOME_NET\"\n    DNP3_CLIENT: \"$HOME_NET\"\n    MODBUS_CLIENT: \"$HOME_NET\"\n    MODBUS_SERVER: \"$HOME_NET\"\n    ENIP_CLIENT: \"$HOME_NET\"\n    ENIP_SERVER: \"$HOME_NET\"\n\n  port-groups:\n    HTTP_PORTS: \"80\"\n    SHELLCODE_PORTS: \"!80\"\n    ORACLE_PORTS: 1521\n    SSH_PORTS: 22\n    DNP3_PORTS: 20000\n    MODBUS_PORTS: 502\n    FILE_DATA_PORTS: \"[$HTTP_PORTS,110,143]\"\n    FTP_PORTS: 21\n    GENEVE_PORTS: 6081\n    VXLAN_PORTS: 4789\n    TEREDO_PORTS: 3544\n\n##\n## Step 2: Select outputs to enable\n##\n\n# The default logging directory.  Any log or output file will be\n# placed here if it's not specified with a full path name. This can be\n# overridden with the -l command line parameter.\ndefault-log-dir: @e_logdir@\n\n# Global stats configuration\nstats:\n  enabled: yes\n  # The interval field (in seconds) controls the interval at\n  # which stats are updated in the log.\n  interval: 8\n  # Add decode events to stats.\n  #decoder-events: true\n  # Decoder event prefix in stats. Has been 'decoder' before, but that leads\n  # to missing events in the eve.stats records. See issue #2225.\n  #decoder-events-prefix: \"decoder.event\"\n  # Add stream events as stats.\n  #stream-events: false\n\n# Configure the type of alert (and other) logging you would like.\noutputs:\n  # a line based alerts log similar to Snort's fast.log\n  - fast:\n      enabled: yes\n      filename: fast.log\n      append: yes\n      #filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'\n\n  # Extensible Event Format (nicknamed EVE) event log in JSON format\n  - eve-log:\n      enabled: @e_enable_evelog@\n      filetype: regular #regular|syslog|unix_dgram|unix_stream|redis\n      filename: eve.json\n      # Enable for multi-threaded eve.json output; output files are amended with\n      # with an identifier, e.g., eve.9.json\n      #threaded: false\n      #prefix: \"@cee: \" # prefix to prepend to each log entry\n      # the following are valid when type: syslog above\n      #identity: \"suricata\"\n      #facility: local5\n      #level: Info ## possible levels: Emergency, Alert, Critical,\n                   ## Error, Warning, Notice, Info, Debug\n      #ethernet: no  # log ethernet header in events when available\n      #redis:\n      #  server: 127.0.0.1\n      #  port: 6379\n      #  async: true ## if redis replies are read asynchronously\n      #  mode: list ## possible values: list|lpush (default), rpush, channel|publish\n      #             ## lpush and rpush are using a Redis list. \"list\" is an alias for lpush\n      #             ## publish is using a Redis channel. \"channel\" is an alias for publish\n      #  key: suricata ## key or channel to use (default to suricata)\n      # Redis pipelining set up. This will enable to only do a query every\n      # 'batch-size' events. This should lower the latency induced by network\n      # connection at the cost of some memory. There is no flushing implemented\n      # so this setting should be reserved to high traffic Suricata deployments.\n      #  pipelining:\n      #    enabled: yes ## set enable to yes to enable query pipelining\n      #    batch-size: 10 ## number of entries to keep in buffer\n\n      # Include top level metadata. Default yes.\n      #metadata: no\n\n      # include the name of the input pcap file in pcap file processing mode\n      pcap-file: false\n\n      # Community Flow ID\n      # Adds a 'community_id' field to EVE records. These are meant to give\n      # records a predictable flow ID that can be used to match records to\n      # output of other tools such as Zeek (Bro).\n      #\n      # Takes a 'seed' that needs to be same across sensors and tools\n      # to make the id less predictable.\n\n      # enable/disable the community id feature.\n      community-id: false\n      # Seed value for the ID output. Valid values are 0-65535.\n      community-id-seed: 0\n\n      # HTTP X-Forwarded-For support by adding an extra field or overwriting\n      # the source or destination IP address (depending on flow direction)\n      # with the one reported in the X-Forwarded-For HTTP header. This is\n      # helpful when reviewing alerts for traffic that is being reverse\n      # or forward proxied.\n      xff:\n        enabled: no\n        # Two operation modes are available: \"extra-data\" and \"overwrite\".\n        mode: extra-data\n        # Two proxy deployments are supported: \"reverse\" and \"forward\". In\n        # a \"reverse\" deployment the IP address used is the last one, in a\n        # \"forward\" deployment the first IP address is used.\n        deployment: reverse\n        # Header name where the actual IP address will be reported. If more\n        # than one IP address is present, the last IP address will be the\n        # one taken into consideration.\n        header: X-Forwarded-For\n\n      types:\n        - alert:\n            # payload: yes             # enable dumping payload in Base64\n            # payload-buffer-size: 4kb # max size of payload buffer to output in eve-log\n            # payload-printable: yes   # enable dumping payload in printable (lossy) format\n            # packet: yes              # enable dumping of packet (without stream segments)\n            # metadata: no             # enable inclusion of app layer metadata with alert. Default yes\n            # http-body: yes           # Requires metadata; enable dumping of HTTP body in Base64\n            # http-body-printable: yes # Requires metadata; enable dumping of HTTP body in printable format\n\n            # Enable the logging of tagged packets for rules using the\n            # \"tag\" keyword.\n            tagged-packets: yes\n        - anomaly:\n            # Anomaly log records describe unexpected conditions such\n            # as truncated packets, packets with invalid IP/UDP/TCP\n            # length values, and other events that render the packet\n            # invalid for further processing or describe unexpected\n            # behavior on an established stream. Networks which\n            # experience high occurrences of anomalies may experience\n            # packet processing degradation.\n            #\n            # Anomalies are reported for the following:\n            # 1. Decode: Values and conditions that are detected while\n            # decoding individual packets. This includes invalid or\n            # unexpected values for low-level protocol lengths as well\n            # as stream related events (TCP 3-way handshake issues,\n            # unexpected sequence number, etc).\n            # 2. Stream: This includes stream related events (TCP\n            # 3-way handshake issues, unexpected sequence number,\n            # etc).\n            # 3. Application layer: These denote application layer\n            # specific conditions that are unexpected, invalid or are\n            # unexpected given the application monitoring state.\n            #\n            # By default, anomaly logging is enabled. When anomaly\n            # logging is enabled, applayer anomaly reporting is\n            # also enabled.\n            enabled: yes\n            #\n            # Choose one or more types of anomaly logging and whether to enable\n            # logging of the packet header for packet anomalies.\n            types:\n              # decode: no\n              # stream: no\n              # applayer: yes\n            #packethdr: no\n        - http:\n            extended: yes     # enable this for extended logging information\n            # custom allows additional HTTP fields to be included in eve-log.\n            # the example below adds three additional fields when uncommented\n            #custom: [Accept-Encoding, Accept-Language, Authorization]\n            # set this value to one and only one from {both, request, response}\n            # to dump all HTTP headers for every HTTP request and/or response\n            # dump-all-headers: none\n        - dns:\n            # This configuration uses the new DNS logging format,\n            # the old configuration is still available:\n            # https://suricata.readthedocs.io/en/latest/output/eve/eve-json-output.html#dns-v1-format\n\n            # As of Suricata 5.0, version 2 of the eve dns output\n            # format is the default.\n            #version: 2\n\n            # Enable/disable this logger. Default: enabled.\n            #enabled: yes\n\n            # Control logging of requests and responses:\n            # - requests: enable logging of DNS queries\n            # - responses: enable logging of DNS answers\n            # By default both requests and responses are logged.\n            #requests: no\n            #responses: no\n\n            # Format of answer logging:\n            # - detailed: array item per answer\n            # - grouped: answers aggregated by type\n            # Default: all\n            #formats: [detailed, grouped]\n\n            # DNS record types to log, based on the query type.\n            # Default: all.\n            #types: [a, aaaa, cname, mx, ns, ptr, txt]\n        - tls:\n            extended: yes     # enable this for extended logging information\n            # output TLS transaction where the session is resumed using a\n            # session id\n            #session-resumption: no\n            # custom controls which TLS fields that are included in eve-log\n            #custom: [subject, issuer, session_resumed, serial, fingerprint, sni, version, not_before, not_after, certificate, chain, ja3, ja3s]\n        - files:\n            force-magic: no   # force logging magic on all logged files\n            # force logging of checksums, available hash functions are md5,\n            # sha1 and sha256\n            #force-hash: [md5]\n        #- drop:\n        #    alerts: yes      # log alerts that caused drops\n        #    flows: all       # start or all: 'start' logs only a single drop\n        #                     # per flow direction. All logs each dropped pkt.\n        - smtp:\n            #extended: yes # enable this for extended logging information\n            # this includes: bcc, message-id, subject, x_mailer, user-agent\n            # custom fields logging from the list:\n            #  reply-to, bcc, message-id, subject, x-mailer, user-agent, received,\n            #  x-originating-ip, in-reply-to, references, importance, priority,\n            #  sensitivity, organization, content-md5, date\n            #custom: [received, x-mailer, x-originating-ip, relays, reply-to, bcc]\n            # output md5 of fields: body, subject\n            # for the body you need to set app-layer.protocols.smtp.mime.body-md5\n            # to yes\n            #md5: [body, subject]\n\n        #- dnp3\n        - ftp\n        - rdp\n        - nfs\n        - smb\n        - tftp\n        - ikev2\n        - dcerpc\n        - krb5\n        - snmp\n        - rfb\n        - sip\n        - dhcp:\n            enabled: yes\n            # When extended mode is on, all DHCP messages are logged\n            # with full detail. When extended mode is off (the\n            # default), just enough information to map a MAC address\n            # to an IP address is logged.\n            extended: no\n        - ssh\n        - mqtt:\n            # passwords: yes           # enable output of passwords\n        # HTTP2 logging. HTTP2 support is currently experimental and\n        # disabled by default. To enable, uncomment the following line\n        # and be sure to enable http2 in the app-layer section.\n        #- http2\n        - stats:\n            totals: yes       # stats for all threads merged together\n            threads: no       # per thread stats\n            deltas: no        # include delta values\n        # bi-directional flows\n        - flow\n        # uni-directional flows\n        #- netflow\n\n        # Metadata event type. Triggered whenever a pktvar is saved\n        # and will include the pktvars, flowvars, flowbits and\n        # flowints.\n        #- metadata\n\n  # a line based log of HTTP requests (no alerts)\n  - http-log:\n      enabled: no\n      filename: http.log\n      append: yes\n      #extended: yes     # enable this for extended logging information\n      #custom: yes       # enable the custom logging format (defined by customformat)\n      #customformat: \"%{%D-%H:%M:%S}t.%z %{X-Forwarded-For}i %H %m %h %u %s %B %a:%p -> %A:%P\"\n      #filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'\n\n  # a line based log of TLS handshake parameters (no alerts)\n  - tls-log:\n      enabled: no  # Log TLS connections.\n      filename: tls.log # File to store TLS logs.\n      append: yes\n      #extended: yes     # Log extended information like fingerprint\n      #custom: yes       # enabled the custom logging format (defined by customformat)\n      #customformat: \"%{%D-%H:%M:%S}t.%z %a:%p -> %A:%P %v %n %d %D\"\n      #filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'\n      # output TLS transaction where the session is resumed using a\n      # session id\n      #session-resumption: no\n\n  # output module to store certificates chain to disk\n  - tls-store:\n      enabled: no\n      #certs-log-dir: certs # directory to store the certificates files\n\n  # Packet log... log packets in pcap format. 3 modes of operation: \"normal\"\n  # \"multi\" and \"sguil\".\n  #\n  # In normal mode a pcap file \"filename\" is created in the default-log-dir,\n  # or as specified by \"dir\".\n  # In multi mode, a file is created per thread. This will perform much\n  # better, but will create multiple files where 'normal' would create one.\n  # In multi mode the filename takes a few special variables:\n  # - %n -- thread number\n  # - %i -- thread id\n  # - %t -- timestamp (secs or secs.usecs based on 'ts-format'\n  # E.g. filename: pcap.%n.%t\n  #\n  # Note that it's possible to use directories, but the directories are not\n  # created by Suricata. E.g. filename: pcaps/%n/log.%s will log into the\n  # per thread directory.\n  #\n  # Also note that the limit and max-files settings are enforced per thread.\n  # So the size limit when using 8 threads with 1000mb files and 2000 files\n  # is: 8*1000*2000 ~ 16TiB.\n  #\n  # In Sguil mode \"dir\" indicates the base directory. In this base dir the\n  # pcaps are created in the directory structure Sguil expects:\n  #\n  # $sguil-base-dir/YYYY-MM-DD/$filename.<timestamp>\n  #\n  # By default all packets are logged except:\n  # - TCP streams beyond stream.reassembly.depth\n  # - encrypted streams after the key exchange\n  #\n  - pcap-log:\n      enabled: no\n      filename: log.pcap\n\n      # File size limit.  Can be specified in kb, mb, gb.  Just a number\n      # is parsed as bytes.\n      limit: 1000mb\n\n      # If set to a value, ring buffer mode is enabled. Will keep maximum of\n      # \"max-files\" of size \"limit\"\n      max-files: 2000\n\n      # Compression algorithm for pcap files. Possible values: none, lz4.\n      # Enabling compression is incompatible with the sguil mode. Note also\n      # that on Windows, enabling compression will *increase* disk I/O.\n      compression: none\n\n      # Further options for lz4 compression. The compression level can be set\n      # to a value between 0 and 16, where higher values result in higher\n      # compression.\n      #lz4-checksum: no\n      #lz4-level: 0\n\n      mode: normal # normal, multi or sguil.\n\n      # Directory to place pcap files. If not provided the default log\n      # directory will be used. Required for \"sguil\" mode.\n      #dir: /nsm_data/\n\n      #ts-format: usec # sec or usec second format (default) is filename.sec usec is filename.sec.usec\n      use-stream-depth: no #If set to \"yes\" packets seen after reaching stream inspection depth are ignored. \"no\" logs all packets\n      honor-pass-rules: no # If set to \"yes\", flows in which a pass rule matched will stop being logged.\n\n  # a full alert log containing much information for signature writers\n  # or for investigating suspected false positives.\n  - alert-debug:\n      enabled: no\n      filename: alert-debug.log\n      append: yes\n      #filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'\n\n  # alert output to prelude (https://www.prelude-siem.org/) only\n  # available if Suricata has been compiled with --enable-prelude\n  - alert-prelude:\n      enabled: no\n      profile: suricata\n      log-packet-content: no\n      log-packet-header: yes\n\n  # Stats.log contains data from various counters of the Suricata engine.\n  - stats:\n      enabled: yes\n      filename: stats.log\n      append: yes       # append to file (yes) or overwrite it (no)\n      totals: yes       # stats for all threads merged together\n      threads: no       # per thread stats\n      #null-values: yes  # print counters that have value 0. Default: no\n\n  # a line based alerts log similar to fast.log into syslog\n  - syslog:\n      enabled: no\n      # reported identity to syslog. If omitted the program name (usually\n      # suricata) will be used.\n      #identity: \"suricata\"\n      facility: local5\n      #level: Info ## possible levels: Emergency, Alert, Critical,\n                   ## Error, Warning, Notice, Info, Debug\n\n  # Output module for storing files on disk. Files are stored in\n  # directory names consisting of the first 2 characters of the\n  # SHA256 of the file. Each file is given its SHA256 as a filename.\n  #\n  # When a duplicate file is found, the timestamps on the existing file\n  # are updated.\n  #\n  # Unlike the older filestore, metadata is not written by default\n  # as each file should already have a \"fileinfo\" record in the\n  # eve-log. If write-fileinfo is set to yes, then each file will have\n  # one more associated .json files that consist of the fileinfo\n  # record. A fileinfo file will be written for each occurrence of the\n  # file seen using a filename suffix to ensure uniqueness.\n  #\n  # To prune the filestore directory see the \"suricatactl filestore\n  # prune\" command which can delete files over a certain age.\n  - file-store:\n      version: 2\n      enabled: no\n\n      # Set the directory for the filestore. Relative pathnames\n      # are contained within the \"default-log-dir\".\n      #dir: filestore\n\n      # Write out a fileinfo record for each occurrence of a file.\n      # Disabled by default as each occurrence is already logged\n      # as a fileinfo record to the main eve-log.\n      #write-fileinfo: yes\n\n      # Force storing of all files. Default: no.\n      #force-filestore: yes\n\n      # Override the global stream-depth for sessions in which we want\n      # to perform file extraction. Set to 0 for unlimited; otherwise,\n      # must be greater than the global stream-depth value to be used.\n      #stream-depth: 0\n\n      # Uncomment the following variable to define how many files can\n      # remain open for filestore by Suricata. Default value is 0 which\n      # means files get closed after each write to the file.\n      #max-open-files: 1000\n\n      # Force logging of checksums: available hash functions are md5,\n      # sha1 and sha256. Note that SHA256 is automatically forced by\n      # the use of this output module as it uses the SHA256 as the\n      # file naming scheme.\n      #force-hash: [sha1, md5]\n      # NOTE: X-Forwarded configuration is ignored if write-fileinfo is disabled\n      # HTTP X-Forwarded-For support by adding an extra field or overwriting\n      # the source or destination IP address (depending on flow direction)\n      # with the one reported in the X-Forwarded-For HTTP header. This is\n      # helpful when reviewing alerts for traffic that is being reverse\n      # or forward proxied.\n      xff:\n        enabled: no\n        # Two operation modes are available, \"extra-data\" and \"overwrite\".\n        mode: extra-data\n        # Two proxy deployments are supported, \"reverse\" and \"forward\". In\n        # a \"reverse\" deployment the IP address used is the last one, in a\n        # \"forward\" deployment the first IP address is used.\n        deployment: reverse\n        # Header name where the actual IP address will be reported. If more\n        # than one IP address is present, the last IP address will be the\n        # one taken into consideration.\n        header: X-Forwarded-For\n\n  # Log TCP data after stream normalization\n  # Two types: file or dir:\n  #     - file logs into a single logfile.\n  #     - dir creates 2 files per TCP session and stores the raw TCP\n  #            data into them.\n  # Use 'both' to enable both file and dir modes.\n  #\n  # Note: limited by \"stream.reassembly.depth\"\n  - tcp-data:\n      enabled: no\n      type: file\n      filename: tcp-data.log\n\n  # Log HTTP body data after normalization, de-chunking and unzipping.\n  # Two types: file or dir.\n  #     - file logs into a single logfile.\n  #     - dir creates 2 files per HTTP session and stores the\n  #           normalized data into them.\n  # Use 'both' to enable both file and dir modes.\n  #\n  # Note: limited by the body limit settings\n  - http-body-data:\n      enabled: no\n      type: file\n      filename: http-data.log\n\n  # Lua Output Support - execute lua script to generate alert and event\n  # output.\n  # Documented at:\n  # https://suricata.readthedocs.io/en/latest/output/lua-output.html\n  - lua:\n      enabled: no\n      #scripts-dir: /etc/suricata/lua-output/\n      scripts:\n      #   - script1.lua\n\n# Logging configuration.  This is not about logging IDS alerts/events, but\n# output about what Suricata is doing, like startup messages, errors, etc.\nlogging:\n  # The default log level: can be overridden in an output section.\n  # Note that debug level logging will only be emitted if Suricata was\n  # compiled with the --enable-debug configure option.\n  #\n  # This value is overridden by the SC_LOG_LEVEL env var.\n  default-log-level: notice\n\n  # The default output format.  Optional parameter, should default to\n  # something reasonable if not provided.  Can be overridden in an\n  # output section.  You can leave this out to get the default.\n  #\n  # This value is overridden by the SC_LOG_FORMAT env var.\n  #default-log-format: \"[%i] %t - (%f:%l) <%d> (%n) -- \"\n\n  # A regex to filter output.  Can be overridden in an output section.\n  # Defaults to empty (no filter).\n  #\n  # This value is overridden by the SC_LOG_OP_FILTER env var.\n  default-output-filter:\n\n  # Requires libunwind to be available when Suricata is configured and built.\n  # If a signal unexpectedly terminates Suricata, displays a brief diagnostic\n  # message with the offending stacktrace if enabled.\n  #stacktrace-on-signal: on\n\n  # Define your logging outputs.  If none are defined, or they are all\n  # disabled you will get the default: console output.\n  outputs:\n  - console:\n      enabled: yes\n      # type: json\n  - file:\n      enabled: yes\n      level: info\n      filename: suricata.log\n      # type: json\n  - syslog:\n      enabled: no\n      facility: local5\n      format: \"[%i] <%d> -- \"\n      # type: json\n\n\n##\n## Step 3: Configure common capture settings\n##\n## See \"Advanced Capture Options\" below for more options, including Netmap\n## and PF_RING.\n##\n\n# Linux high speed capture support\naf-packet:\n  - interface: eth0\n    # Number of receive threads. \"auto\" uses the number of cores\n    #threads: auto\n    # Default clusterid. AF_PACKET will load balance packets based on flow.\n    cluster-id: 99\n    # Default AF_PACKET cluster type. AF_PACKET can load balance per flow or per hash.\n    # This is only supported for Linux kernel > 3.1\n    # possible value are:\n    #  * cluster_flow: all packets of a given flow are sent to the same socket\n    #  * cluster_cpu: all packets treated in kernel by a CPU are sent to the same socket\n    #  * cluster_qm: all packets linked by network card to a RSS queue are sent to the same\n    #  socket. Requires at least Linux 3.14.\n    #  * cluster_ebpf: eBPF file load balancing. See doc/userguide/capture-hardware/ebpf-xdp.rst for\n    #  more info.\n    # Recommended modes are cluster_flow on most boxes and cluster_cpu or cluster_qm on system\n    # with capture card using RSS (requires cpu affinity tuning and system IRQ tuning)\n    cluster-type: cluster_flow\n    # In some fragmentation cases, the hash can not be computed. If \"defrag\" is set\n    # to yes, the kernel will do the needed defragmentation before sending the packets.\n    defrag: yes\n    # To use the ring feature of AF_PACKET, set 'use-mmap' to yes\n    #use-mmap: yes\n    # Lock memory map to avoid it being swapped. Be careful that over\n    # subscribing could lock your system\n    #mmap-locked: yes\n    # Use tpacket_v3 capture mode, only active if use-mmap is true\n    # Don't use it in IPS or TAP mode as it causes severe latency\n    #tpacket-v3: yes\n    # Ring size will be computed with respect to \"max-pending-packets\" and number\n    # of threads. You can set manually the ring size in number of packets by setting\n    # the following value. If you are using flow \"cluster-type\" and have really network\n    # intensive single-flow you may want to set the \"ring-size\" independently of the number\n    # of threads:\n    #ring-size: 2048\n    # Block size is used by tpacket_v3 only. It should set to a value high enough to contain\n    # a decent number of packets. Size is in bytes so please consider your MTU. It should be\n    # a power of 2 and it must be multiple of page size (usually 4096).\n    #block-size: 32768\n    # tpacket_v3 block timeout: an open block is passed to userspace if it is not\n    # filled after block-timeout milliseconds.\n    #block-timeout: 10\n    # On busy systems, set it to yes to help recover from a packet drop\n    # phase. This will result in some packets (at max a ring flush) not being inspected.\n    #use-emergency-flush: yes\n    # recv buffer size, increased value could improve performance\n    # buffer-size: 32768\n    # Set to yes to disable promiscuous mode\n    # disable-promisc: no\n    # Choose checksum verification mode for the interface. At the moment\n    # of the capture, some packets may have an invalid checksum due to\n    # the checksum computation being offloaded to the network card.\n    # Possible values are:\n    #  - kernel: use indication sent by kernel for each packet (default)\n    #  - yes: checksum validation is forced\n    #  - no: checksum validation is disabled\n    #  - auto: Suricata uses a statistical approach to detect when\n    #  checksum off-loading is used.\n    # Warning: 'capture.checksum-validation' must be set to yes to have any validation\n    #checksum-checks: kernel\n    # BPF filter to apply to this interface. The pcap filter syntax applies here.\n    #bpf-filter: port 80 or udp\n    # You can use the following variables to activate AF_PACKET tap or IPS mode.\n    # If copy-mode is set to ips or tap, the traffic coming to the current\n    # interface will be copied to the copy-iface interface. If 'tap' is set, the\n    # copy is complete. If 'ips' is set, the packet matching a 'drop' action\n    # will not be copied.\n    #copy-mode: ips\n    #copy-iface: eth1\n    #  For eBPF and XDP setup including bypass, filter and load balancing, please\n    #  see doc/userguide/capture-hardware/ebpf-xdp.rst for more info.\n\n  # Put default values here. These will be used for an interface that is not\n  # in the list above.\n  - interface: default\n    #threads: auto\n    #use-mmap: no\n    #tpacket-v3: yes\n\n# Cross platform libpcap capture support\npcap:\n  - interface: eth0\n    # On Linux, pcap will try to use mmap'ed capture and will use \"buffer-size\"\n    # as total memory used by the ring. So set this to something bigger\n    # than 1% of your bandwidth.\n    #buffer-size: 16777216\n    #bpf-filter: \"tcp and port 25\"\n    # Choose checksum verification mode for the interface. At the moment\n    # of the capture, some packets may have an invalid checksum due to\n    # the checksum computation being offloaded to the network card.\n    # Possible values are:\n    #  - yes: checksum validation is forced\n    #  - no: checksum validation is disabled\n    #  - auto: Suricata uses a statistical approach to detect when\n    #  checksum off-loading is used. (default)\n    # Warning: 'capture.checksum-validation' must be set to yes to have any validation\n    #checksum-checks: auto\n    # With some accelerator cards using a modified libpcap (like Myricom), you\n    # may want to have the same number of capture threads as the number of capture\n    # rings. In this case, set up the threads variable to N to start N threads\n    # listening on the same interface.\n    #threads: 16\n    # set to no to disable promiscuous mode:\n    #promisc: no\n    # set snaplen, if not set it defaults to MTU if MTU can be known\n    # via ioctl call and to full capture if not.\n    #snaplen: 1518\n  # Put default values here\n  - interface: default\n    #checksum-checks: auto\n\n# Settings for reading pcap files\npcap-file:\n  # Possible values are:\n  #  - yes: checksum validation is forced\n  #  - no: checksum validation is disabled\n  #  - auto: Suricata uses a statistical approach to detect when\n  #  checksum off-loading is used. (default)\n  # Warning: 'checksum-validation' must be set to yes to have checksum tested\n  checksum-checks: auto\n\n# See \"Advanced Capture Options\" below for more options, including Netmap\n# and PF_RING.\n\n\n##\n## Step 4: App Layer Protocol configuration\n##\n\n# Configure the app-layer parsers.\n#\n# The error-policy setting applies to all app-layer parsers. Values can be\n# \"drop-flow\", \"pass-flow\", \"bypass\", \"drop-packet\", \"pass-packet\", \"reject\" or\n# \"ignore\" (the default).\n#\n# The protocol's section details each protocol.\n#\n# The option \"enabled\" takes 3 values - \"yes\", \"no\", \"detection-only\".\n# \"yes\" enables both detection and the parser, \"no\" disables both, and\n# \"detection-only\" enables protocol detection only (parser disabled).\napp-layer:\n  # error-policy: ignore\n  protocols:\n    rfb:\n      enabled: yes\n      detection-ports:\n        dp: 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909\n    # MQTT, disabled by default.\n    mqtt:\n      enabled: yes\n      # max-msg-length: 1mb\n      # subscribe-topic-match-limit: 100\n      # unsubscribe-topic-match-limit: 100\n      # Maximum number of live MQTT transactions per flow\n      # max-tx: 4096\n    krb5:\n      enabled: yes\n    snmp:\n      enabled: yes\n    ikev2:\n      enabled: yes\n    tls:\n      enabled: yes\n      detection-ports:\n        dp: 443\n\n      # Generate JA3 fingerprint from client hello. If not specified it\n      # will be disabled by default, but enabled if rules require it.\n      #ja3-fingerprints: auto\n\n      # What to do when the encrypted communications start:\n      # - default: keep tracking TLS session, check for protocol anomalies,\n      #            inspect tls_* keywords. Disables inspection of unmodified\n      #            'content' signatures.\n      # - bypass:  stop processing this flow as much as possible. No further\n      #            TLS parsing and inspection. Offload flow bypass to kernel\n      #            or hardware if possible.\n      # - full:    keep tracking and inspection as normal. Unmodified content\n      #            keyword signatures are inspected as well.\n      #\n      # For best performance, select 'bypass'.\n      #\n      #encryption-handling: default\n\n    dcerpc:\n      enabled: yes\n    ftp:\n      enabled: yes\n      # memcap: 64mb\n    rdp:\n      enabled: yes\n    ssh:\n      enabled: yes\n      #hassh: yes\n    # HTTP2: Experimental HTTP 2 support. Disabled by default.\n    http2:\n      enabled: no\n      # use http keywords on HTTP2 traffic\n      http1-rules: no\n    smtp:\n      enabled: yes\n      raw-extraction: no\n      # Configure SMTP-MIME Decoder\n      mime:\n        # Decode MIME messages from SMTP transactions\n        # (may be resource intensive)\n        # This field supersedes all others because it turns the entire\n        # process on or off\n        decode-mime: yes\n\n        # Decode MIME entity bodies (ie. Base64, quoted-printable, etc.)\n        decode-base64: yes\n        decode-quoted-printable: yes\n\n        # Maximum bytes per header data value stored in the data structure\n        # (default is 2000)\n        header-value-depth: 2000\n\n        # Extract URLs and save in state data structure\n        extract-urls: yes\n        # Set to yes to compute the md5 of the mail body. You will then\n        # be able to journalize it.\n        body-md5: no\n      # Configure inspected-tracker for file_data keyword\n      inspected-tracker:\n        content-limit: 100000\n        content-inspect-min-size: 32768\n        content-inspect-window: 4096\n    imap:\n      enabled: detection-only\n    smb:\n      enabled: yes\n      detection-ports:\n        dp: 139, 445\n\n      # Stream reassembly size for SMB streams. By default track it completely.\n      #stream-depth: 0\n\n    nfs:\n      enabled: yes\n    tftp:\n      enabled: yes\n    dns:\n      tcp:\n        enabled: yes\n        detection-ports:\n          dp: 53\n      udp:\n        enabled: yes\n        detection-ports:\n          dp: 53\n    http:\n      enabled: yes\n      # memcap:                   Maximum memory capacity for HTTP\n      #                           Default is unlimited, values can be 64mb, e.g.\n\n      # default-config:           Used when no server-config matches\n      #   personality:            List of personalities used by default\n      #   request-body-limit:     Limit reassembly of request body for inspection\n      #                           by http_client_body & pcre /P option.\n      #   response-body-limit:    Limit reassembly of response body for inspection\n      #                           by file_data, http_server_body & pcre /Q option.\n      #\n      #   For advanced options, see the user guide\n\n\n      # server-config:            List of server configurations to use if address matches\n      #   address:                List of IP addresses or networks for this block\n      #   personality:            List of personalities used by this block\n      #\n      #                           Then, all the fields from default-config can be overloaded\n      #\n      # Currently Available Personalities:\n      #   Minimal, Generic, IDS (default), IIS_4_0, IIS_5_0, IIS_5_1, IIS_6_0,\n      #   IIS_7_0, IIS_7_5, Apache_2\n      libhtp:\n         default-config:\n           personality: IDS\n\n           # Can be specified in kb, mb, gb.  Just a number indicates\n           # it's in bytes.\n           request-body-limit: 100kb\n           response-body-limit: 100kb\n\n           # inspection limits\n           request-body-minimal-inspect-size: 32kb\n           request-body-inspect-window: 4kb\n           response-body-minimal-inspect-size: 40kb\n           response-body-inspect-window: 16kb\n\n           # response body decompression (0 disables)\n           response-body-decompress-layer-limit: 2\n\n           # auto will use http-body-inline mode in IPS mode, yes or no set it statically\n           http-body-inline: auto\n\n           # Decompress SWF files.\n           # Two types: 'deflate', 'lzma', 'both' will decompress deflate and lzma\n           # compress-depth:\n           # Specifies the maximum amount of data to decompress,\n           # set 0 for unlimited.\n           # decompress-depth:\n           # Specifies the maximum amount of decompressed data to obtain,\n           # set 0 for unlimited.\n           swf-decompression:\n             enabled: yes\n             type: both\n             compress-depth: 100kb\n             decompress-depth: 100kb\n\n           # Use a random value for inspection sizes around the specified value.\n           # This lowers the risk of some evasion techniques but could lead\n           # to detection change between runs. It is set to 'yes' by default.\n           #randomize-inspection-sizes: yes\n           # If \"randomize-inspection-sizes\" is active, the value of various\n           # inspection size will be chosen from the [1 - range%, 1 + range%]\n           # range\n           # Default value of \"randomize-inspection-range\" is 10.\n           #randomize-inspection-range: 10\n\n           # decoding\n           double-decode-path: no\n           double-decode-query: no\n\n           # Can enable LZMA decompression\n           #lzma-enabled: false\n           # Memory limit usage for LZMA decompression dictionary\n           # Data is decompressed until dictionary reaches this size\n           #lzma-memlimit: 1mb\n           # Maximum decompressed size with a compression ratio\n           # above 2048 (only LZMA can reach this ratio, deflate cannot)\n           #compression-bomb-limit: 1mb\n           # Maximum time spent decompressing a single transaction in usec\n           #decompression-time-limit: 100000\n\n         server-config:\n\n           #- apache:\n           #    address: [192.168.1.0/24, 127.0.0.0/8, \"::1\"]\n           #    personality: Apache_2\n           #    # Can be specified in kb, mb, gb.  Just a number indicates\n           #    # it's in bytes.\n           #    request-body-limit: 4096\n           #    response-body-limit: 4096\n           #    double-decode-path: no\n           #    double-decode-query: no\n\n           #- iis7:\n           #    address:\n           #      - 192.168.0.0/24\n           #      - 192.168.10.0/24\n           #    personality: IIS_7_0\n           #    # Can be specified in kb, mb, gb.  Just a number indicates\n           #    # it's in bytes.\n           #    request-body-limit: 4096\n           #    response-body-limit: 4096\n           #    double-decode-path: no\n           #    double-decode-query: no\n\n    # Note: Modbus probe parser is minimalist due to the limited usage in the field.\n    # Only Modbus message length (greater than Modbus header length)\n    # and protocol ID (equal to 0) are checked in probing parser\n    # It is important to enable detection port and define Modbus port\n    # to avoid false positives\n    modbus:\n      # How many unanswered Modbus requests are considered a flood.\n      # If the limit is reached, the app-layer-event:modbus.flooded; will match.\n      #request-flood: 500\n\n      enabled: no\n      detection-ports:\n        dp: 502\n      # According to MODBUS Messaging on TCP/IP Implementation Guide V1.0b, it\n      # is recommended to keep the TCP connection opened with a remote device\n      # and not to open and close it for each MODBUS/TCP transaction. In that\n      # case, it is important to set the depth of the stream reassembling as\n      # unlimited (stream.reassembly.depth: 0)\n\n      # Stream reassembly size for modbus. By default track it completely.\n      stream-depth: 0\n\n    # DNP3\n    dnp3:\n      enabled: no\n      detection-ports:\n        dp: 20000\n\n    # SCADA EtherNet/IP and CIP protocol support\n    enip:\n      enabled: no\n      detection-ports:\n        dp: 44818\n        sp: 44818\n\n    ntp:\n      enabled: yes\n\n    dhcp:\n      enabled: yes\n\n    sip:\n      enabled: yes\n\n# Limit for the maximum number of asn1 frames to decode (default 256)\nasn1-max-frames: 256\n\n# Datasets default settings\n# datasets:\n#   # Default fallback memcap and hashsize values for datasets in case these\n#   # were not explicitly defined.\n#   defaults:\n#     memcap: 100mb\n#     hashsize: 2048\n#\n#  rules:\n#    # Set to true to allow absolute filenames and filenames that use\n#    # \"..\" components to reference parent directories in rules that specify\n#    # their filenames.\n#    #allow-absolute-filenames: false\n\n##############################################################################\n##\n## Advanced settings below\n##\n##############################################################################\n\n##\n## Run Options\n##\n\n# Run Suricata with a specific user-id and group-id:\n#run-as:\n#  user: suri\n#  group: suri\n\nsecurity:\n  lua:\n    # Allow Lua rules. Disabled by default.\n    #allow-rules: false\n\n# Some logging modules will use that name in event as identifier. The default\n# value is the hostname\n#sensor-name: suricata\n\n# Default location of the pid file. The pid file is only used in\n# daemon mode (start Suricata with -D). If not running in daemon mode\n# the --pidfile command line option must be used to create a pid file.\n#pid-file: @e_rundir@suricata.pid\n\n# Daemon working directory\n# Suricata will change directory to this one if provided\n# Default: \"/\"\n#daemon-directory: \"/\"\n\n# Umask.\n# Suricata will use this umask if it is provided. By default it will use the\n# umask passed on by the shell.\n#umask: 022\n\n# Suricata core dump configuration. Limits the size of the core dump file to\n# approximately max-dump. The actual core dump size will be a multiple of the\n# page size. Core dumps that would be larger than max-dump are truncated. On\n# Linux, the actual core dump size may be a few pages larger than max-dump.\n# Setting max-dump to 0 disables core dumping.\n# Setting max-dump to 'unlimited' will give the full core dump file.\n# On 32-bit Linux, a max-dump value >= ULONG_MAX may cause the core dump size\n# to be 'unlimited'.\n\ncoredump:\n  max-dump: unlimited\n\n# If the Suricata box is a router for the sniffed networks, set it to 'router'. If\n# it is a pure sniffing setup, set it to 'sniffer-only'.\n# If set to auto, the variable is internally switched to 'router' in IPS mode\n# and 'sniffer-only' in IDS mode.\n# This feature is currently only used by the reject* keywords.\nhost-mode: auto\n\n# Number of packets preallocated per thread. The default is 1024. A higher number \n# will make sure each CPU will be more easily kept busy, but may negatively \n# impact caching.\n#max-pending-packets: 1024\n\n# Runmode the engine should use. Please check --list-runmodes to get the available\n# runmodes for each packet acquisition method. Default depends on selected capture\n# method. 'workers' generally gives best performance.\n#runmode: autofp\n\n# Specifies the kind of flow load balancer used by the flow pinned autofp mode.\n#\n# Supported schedulers are:\n#\n# hash     - Flow assigned to threads using the 5-7 tuple hash.\n# ippair   - Flow assigned to threads using addresses only.\n#\n#autofp-scheduler: hash\n\n# Preallocated size for each packet. Default is 1514 which is the classical\n# size for pcap on Ethernet. You should adjust this value to the highest\n# packet size (MTU + hardware header) on your system.\n#default-packet-size: 1514\n\n# Unix command socket that can be used to pass commands to Suricata.\n# An external tool can then connect to get information from Suricata\n# or trigger some modifications of the engine. Set enabled to yes\n# to activate the feature. In auto mode, the feature will only be\n# activated in live capture mode. You can use the filename variable to set\n# the file name of the socket.\nunix-command:\n  enabled: auto\n  #filename: custom.socket\n\n# Magic file. The extension .mgc is added to the value here.\n#magic-file: /usr/share/file/magic\n@e_magic_file_comment@magic-file: @e_magic_file@\n\n# GeoIP2 database file. Specify path and filename of GeoIP2 database\n# if using rules with \"geoip\" rule option.\n#geoip-database: /usr/local/share/GeoLite2/GeoLite2-Country.mmdb\n\nlegacy:\n  uricontent: enabled\n\n##\n## Detection settings\n##\n\n# Set the order of alerts based on actions\n# The default order is pass, drop, reject, alert\n# action-order:\n#   - pass\n#   - drop\n#   - reject\n#   - alert\n\n# Define maximum number of possible alerts that can be triggered for the same\n# packet. Default is 15\n#packet-alert-max: 15\n\n# IP Reputation\n#reputation-categories-file: @e_sysconfdir@iprep/categories.txt\n#default-reputation-path: @e_sysconfdir@iprep\n#reputation-files:\n# - reputation.list\n\n# When run with the option --engine-analysis, the engine will read each of\n# the parameters below, and print reports for each of the enabled sections\n# and exit.  The reports are printed to a file in the default log dir\n# given by the parameter \"default-log-dir\", with engine reporting\n# subsection below printing reports in its own report file.\nengine-analysis:\n  # enables printing reports for fast-pattern for every rule.\n  rules-fast-pattern: yes\n  # enables printing reports for each rule\n  rules: yes\n\n#recursion and match limits for PCRE where supported\npcre:\n  match-limit: 3500\n  match-limit-recursion: 1500\n\n##\n## Advanced Traffic Tracking and Reconstruction Settings\n##\n\n# Host specific policies for defragmentation and TCP stream\n# reassembly. The host OS lookup is done using a radix tree, just\n# like a routing table so the most specific entry matches.\nhost-os-policy:\n  # Make the default policy windows.\n  windows: [0.0.0.0/0]\n  bsd: []\n  bsd-right: []\n  old-linux: []\n  linux: []\n  old-solaris: []\n  solaris: []\n  hpux10: []\n  hpux11: []\n  irix: []\n  macos: []\n  vista: []\n  windows2k3: []\n\n# Defrag settings:\n\n# The memcap-policy value can be \"drop-packet\", \"pass-packet\", \"reject\" or\n# \"ignore\" (which is the default).\ndefrag:\n  memcap: 32mb\n  # memcap-policy: ignore\n  hash-size: 65536\n  trackers: 65535 # number of defragmented flows to follow\n  max-frags: 65535 # number of fragments to keep (higher than trackers)\n  prealloc: yes\n  timeout: 60\n\n# Enable defrag per host settings\n#  host-config:\n#\n#    - dmz:\n#        timeout: 30\n#        address: [192.168.1.0/24, 127.0.0.0/8, 1.1.1.0/24, 2.2.2.0/24, \"1.1.1.1\", \"2.2.2.2\", \"::1\"]\n#\n#    - lan:\n#        timeout: 45\n#        address:\n#          - 192.168.0.0/24\n#          - 192.168.10.0/24\n#          - 172.16.14.0/24\n\n# Flow settings:\n# By default, the reserved memory (memcap) for flows is 32MB. This is the limit\n# for flow allocation inside the engine. You can change this value to allow\n# more memory usage for flows.\n# The hash-size determines the size of the hash used to identify flows inside\n# the engine, and by default the value is 65536.\n# At startup, the engine can preallocate a number of flows, to get better\n# performance. The number of flows preallocated is 10000 by default.\n# emergency-recovery is the percentage of flows that the engine needs to\n# prune before clearing the emergency state. The emergency state is activated\n# when the memcap limit is reached, allowing new flows to be created, but\n# pruning them with the emergency timeouts (they are defined below).\n# If the memcap is reached, the engine will try to prune flows\n# with the default timeouts. If it doesn't find a flow to prune, it will set\n# the emergency bit and it will try again with more aggressive timeouts.\n# If that doesn't work, then it will try to kill the oldest flows using\n# last time seen flows.\n# The memcap can be specified in kb, mb, gb.  Just a number indicates it's\n# in bytes.\n# The memcap-policy can be \"drop-packet\", \"pass-packet\", \"reject\" or \"ignore\"\n# (which is the default).\n\nflow:\n  memcap: 128mb\n  #memcap-policy: ignore\n  hash-size: 65536\n  prealloc: 10000\n  emergency-recovery: 30\n  #managers: 1 # default to one flow manager\n  #recyclers: 1 # default to one flow recycler thread\n\n# This option controls the use of VLAN ids in the flow (and defrag)\n# hashing. Normally this should be enabled, but in some (broken)\n# setups where both sides of a flow are not tagged with the same VLAN\n# tag, we can ignore the VLAN id's in the flow hashing.\nvlan:\n  use-for-tracking: true\n\n# Specific timeouts for flows. Here you can specify the timeouts that the\n# active flows will wait to transit from the current state to another, on each\n# protocol. The value of \"new\" determines the seconds to wait after a handshake or\n# stream startup before the engine frees the data of that flow it doesn't\n# change the state to established (usually if we don't receive more packets\n# of that flow). The value of \"established\" is the amount of\n# seconds that the engine will wait to free the flow if that time elapses\n# without receiving new packets or closing the connection. \"closed\" is the\n# amount of time to wait after a flow is closed (usually zero). \"bypassed\"\n# timeout controls locally bypassed flows. For these flows we don't do any other\n# tracking. If no packets have been seen after this timeout, the flow is discarded.\n#\n# There's an emergency mode that will become active under attack circumstances,\n# making the engine to check flow status faster. This configuration variables\n# use the prefix \"emergency-\" and work similar as the normal ones.\n# Some timeouts doesn't apply to all the protocols, like \"closed\", for udp and\n# icmp.\n\nflow-timeouts:\n\n  default:\n    new: 30\n    established: 300\n    closed: 0\n    bypassed: 100\n    emergency-new: 10\n    emergency-established: 100\n    emergency-closed: 0\n    emergency-bypassed: 50\n  tcp:\n    new: 60\n    established: 600\n    closed: 60\n    bypassed: 100\n    emergency-new: 5\n    emergency-established: 100\n    emergency-closed: 10\n    emergency-bypassed: 50\n  udp:\n    new: 30\n    established: 300\n    bypassed: 100\n    emergency-new: 10\n    emergency-established: 100\n    emergency-bypassed: 50\n  icmp:\n    new: 30\n    established: 300\n    bypassed: 100\n    emergency-new: 10\n    emergency-established: 100\n    emergency-bypassed: 50\n\n# Stream engine settings. Here the TCP stream tracking and reassembly\n# engine is configured.\n#\n# stream:\n#   memcap: 64mb                # Can be specified in kb, mb, gb.  Just a\n#                               # number indicates it's in bytes.\n#   memcap-policy: ignore       # Can be \"drop-flow\", \"pass-flow\", \"bypass\",\n#                               # \"drop-packet\", \"pass-packet\", \"reject\" or\n#                               # \"ignore\" default is \"ignore\"\n#   checksum-validation: yes    # To validate the checksum of received\n#                               # packet. If csum validation is specified as\n#                               # \"yes\", then packets with invalid csum values will not\n#                               # be processed by the engine stream/app layer.\n#                               # Warning: locally generated traffic can be\n#                               # generated without checksum due to hardware offload\n#                               # of checksum. You can control the handling of checksum\n#                               # on a per-interface basis via the 'checksum-checks'\n#                               # option\n#   prealloc-sessions: 2k       # 2k sessions prealloc'd per stream thread\n#   midstream: false            # don't allow midstream session pickups\n#   midstream-policy: ignore    # Can be \"drop-flow\", \"pass-flow\", \"bypass\",\n#                               # \"drop-packet\", \"pass-packet\", \"reject\" or\n#                               # \"ignore\" default is \"ignore\"\n#   async-oneside: false        # don't enable async stream handling\n#   inline: no                  # stream inline mode\n#   drop-invalid: yes           # in inline mode, drop packets that are invalid with regards to streaming engine\n#   max-synack-queued: 5        # Max different SYN/ACKs to queue\n#   bypass: no                  # Bypass packets when stream.reassembly.depth is reached.\n#                               # Warning: first side to reach this triggers\n#                               # the bypass.\n#   liberal-timestamps: false   # Treat all timestamps as if the Linux policy applies. This\n#                               # means it's slightly more permissive. Enabled by default.\n#\n#   reassembly:\n#     memcap: 256mb             # Can be specified in kb, mb, gb.  Just a number\n#                               # indicates it's in bytes.\n#     memcap-policy: ignore     # Can be \"drop-flow\", \"pass-flow\", \"bypass\",\n#                               # \"drop-packet\", \"pass-packet\", \"reject\" or\n#                               # \"ignore\" default is \"ignore\"\n#     depth: 1mb                # Can be specified in kb, mb, gb.  Just a number\n#                               # indicates it's in bytes.\n#     toserver-chunk-size: 2560 # inspect raw stream in chunks of at least\n#                               # this size.  Can be specified in kb, mb,\n#                               # gb.  Just a number indicates it's in bytes.\n#     toclient-chunk-size: 2560 # inspect raw stream in chunks of at least\n#                               # this size.  Can be specified in kb, mb,\n#                               # gb.  Just a number indicates it's in bytes.\n#     randomize-chunk-size: yes # Take a random value for chunk size around the specified value.\n#                               # This lowers the risk of some evasion techniques but could lead\n#                               # to detection change between runs. It is set to 'yes' by default.\n#     randomize-chunk-range: 10 # If randomize-chunk-size is active, the value of chunk-size is\n#                               # a random value between (1 - randomize-chunk-range/100)*toserver-chunk-size\n#                               # and (1 + randomize-chunk-range/100)*toserver-chunk-size and the same\n#                               # calculation for toclient-chunk-size.\n#                               # Default value of randomize-chunk-range is 10.\n#\n#     raw: yes                  # 'Raw' reassembly enabled or disabled.\n#                               # raw is for content inspection by detection\n#                               # engine.\n#\n#     segment-prealloc: 2048    # number of segments preallocated per thread\n#\n#     check-overlap-different-data: true|false\n#                               # check if a segment contains different data\n#                               # than what we've already seen for that\n#                               # position in the stream.\n#                               # This is enabled automatically if inline mode\n#                               # is used or when stream-event:reassembly_overlap_different_data;\n#                               # is used in a rule.\n#\nstream:\n  memcap: 64mb\n  #memcap-policy: ignore\n  checksum-validation: yes      # reject incorrect csums\n  #midstream: false\n  #midstream-policy: ignore\n  inline: auto                  # auto will use inline mode in IPS mode, yes or no set it statically\n  reassembly:\n    memcap: 256mb\n    #memcap-policy: ignore\n    depth: 1mb                  # reassemble 1mb into a stream\n    toserver-chunk-size: 2560\n    toclient-chunk-size: 2560\n    randomize-chunk-size: yes\n    #randomize-chunk-range: 10\n    #raw: yes\n    #segment-prealloc: 2048\n    #check-overlap-different-data: true\n\n# Host table:\n#\n# Host table is used by the tagging and per host thresholding subsystems.\n#\nhost:\n  hash-size: 4096\n  prealloc: 1000\n  memcap: 32mb\n\n# IP Pair table:\n#\n# Used by xbits 'ippair' tracking.\n#\n#ippair:\n#  hash-size: 4096\n#  prealloc: 1000\n#  memcap: 32mb\n\n# Decoder settings\n\ndecoder:\n  # Teredo decoder is known to not be completely accurate\n  # as it will sometimes detect non-teredo as teredo.\n  teredo:\n    enabled: true\n    # ports to look for Teredo. Max 4 ports. If no ports are given, or\n    # the value is set to 'any', Teredo detection runs on _all_ UDP packets.\n    ports: $TEREDO_PORTS # syntax: '[3544, 1234]' or '3533' or 'any'.\n\n  # VXLAN decoder is assigned to up to 4 UDP ports. By default only the\n  # IANA assigned port 4789 is enabled.\n  vxlan:\n    enabled: true\n    ports: $VXLAN_PORTS # syntax: '[8472, 4789]' or '4789'.\n\n  # VNTag decode support\n  vntag:\n    enabled: false\n\n  # Geneve decoder is assigned to up to 4 UDP ports. By default only the\n  # IANA assigned port 6081 is enabled.\n  geneve:\n    enabled: true\n    ports: $GENEVE_PORTS # syntax: '[6081, 1234]' or '6081'.\n\n  # maximum number of decoder layers for a packet\n  # max-layers: 16\n\n##\n## Performance tuning and profiling\n##\n\n# The detection engine builds internal groups of signatures. The engine\n# allows us to specify the profile to use for them, to manage memory in an\n# efficient way keeping good performance. For the profile keyword you\n# can use the words \"low\", \"medium\", \"high\" or \"custom\". If you use custom,\n# make sure to define the values in the \"custom-values\" section.\n# Usually you would prefer medium/high/low.\n#\n# \"sgh mpm-context\", indicates how the staging should allot mpm contexts for\n# the signature groups.  \"single\" indicates the use of a single context for\n# all the signature group heads.  \"full\" indicates a mpm-context for each\n# group head.  \"auto\" lets the engine decide the distribution of contexts\n# based on the information the engine gathers on the patterns from each\n# group head.\n#\n# The option inspection-recursion-limit is used to limit the recursive calls\n# in the content inspection code.  For certain payload-sig combinations, we\n# might end up taking too much time in the content inspection code.\n# If the argument specified is 0, the engine uses an internally defined\n# default limit.  When a value is not specified, there are no limits on the recursion.\ndetect:\n  profile: medium\n  custom-values:\n    toclient-groups: 3\n    toserver-groups: 25\n  sgh-mpm-context: auto\n  inspection-recursion-limit: 3000\n  # If set to yes, the loading of signatures will be made after the capture\n  # is started. This will limit the downtime in IPS mode.\n  #delayed-detect: yes\n\n  prefilter:\n    # default prefiltering setting. \"mpm\" only creates MPM/fast_pattern\n    # engines. \"auto\" also sets up prefilter engines for other keywords.\n    # Use --list-keywords=all to see which keywords support prefiltering.\n    default: mpm\n\n  # the grouping values above control how many groups are created per\n  # direction. Port whitelisting forces that port to get its own group.\n  # Very common ports will benefit, as well as ports with many expensive\n  # rules.\n  grouping:\n    #tcp-whitelist: 53, 80, 139, 443, 445, 1433, 3306, 3389, 6666, 6667, 8080\n    #udp-whitelist: 53, 135, 5060\n\n  profiling:\n    # Log the rules that made it past the prefilter stage, per packet\n    # default is off. The threshold setting determines how many rules\n    # must have made it past pre-filter for that rule to trigger the\n    # logging.\n    #inspect-logging-threshold: 200\n    grouping:\n      dump-to-disk: false\n      include-rules: false      # very verbose\n      include-mpm-stats: false\n\n# Select the multi pattern algorithm you want to run for scan/search the\n# in the engine.\n#\n# The supported algorithms are:\n# \"ac\"      - Aho-Corasick, default implementation\n# \"ac-bs\"   - Aho-Corasick, reduced memory implementation\n# \"ac-ks\"   - Aho-Corasick, \"Ken Steele\" variant\n# \"hs\"      - Hyperscan, available when built with Hyperscan support\n#\n# The default mpm-algo value of \"auto\" will use \"hs\" if Hyperscan is\n# available, \"ac\" otherwise.\n#\n# The mpm you choose also decides the distribution of mpm contexts for\n# signature groups, specified by the conf - \"detect.sgh-mpm-context\".\n# Selecting \"ac\" as the mpm would require \"detect.sgh-mpm-context\"\n# to be set to \"single\", because of ac's memory requirements, unless the\n# ruleset is small enough to fit in memory, in which case one can\n# use \"full\" with \"ac\".  The rest of the mpms can be run in \"full\" mode.\n\nmpm-algo: auto\n\n# Select the matching algorithm you want to use for single-pattern searches.\n#\n# Supported algorithms are \"bm\" (Boyer-Moore) and \"hs\" (Hyperscan, only\n# available if Suricata has been built with Hyperscan support).\n#\n# The default of \"auto\" will use \"hs\" if available, otherwise \"bm\".\n\nspm-algo: auto\n\n# Suricata is multi-threaded. Here the threading can be influenced.\nthreading:\n  set-cpu-affinity: no\n  # Tune cpu affinity of threads. Each family of threads can be bound\n  # to specific CPUs.\n  #\n  # These 2 apply to the all runmodes:\n  # management-cpu-set is used for flow timeout handling, counters\n  # worker-cpu-set is used for 'worker' threads\n  #\n  # Additionally, for autofp these apply:\n  # receive-cpu-set is used for capture threads\n  # verdict-cpu-set is used for IPS verdict threads\n  #\n  cpu-affinity:\n    - management-cpu-set:\n        cpu: [ 0 ]  # include only these CPUs in affinity settings\n    - receive-cpu-set:\n        cpu: [ 0 ]  # include only these CPUs in affinity settings\n    - worker-cpu-set:\n        cpu: [ \"all\" ]\n        mode: \"exclusive\"\n        # Use explicitly 3 threads and don't compute number by using\n        # detect-thread-ratio variable:\n        # threads: 3\n        prio:\n          low: [ 0 ]\n          medium: [ \"1-2\" ]\n          high: [ 3 ]\n          default: \"medium\"\n    #- verdict-cpu-set:\n    #    cpu: [ 0 ]\n    #    prio:\n    #      default: \"high\"\n  #\n  # By default Suricata creates one \"detect\" thread per available CPU/CPU core.\n  # This setting allows controlling this behaviour. A ratio setting of 2 will\n  # create 2 detect threads for each CPU/CPU core. So for a dual core CPU this\n  # will result in 4 detect threads. If values below 1 are used, less threads\n  # are created. So on a dual core CPU a setting of 0.5 results in 1 detect\n  # thread being created. Regardless of the setting at a minimum 1 detect\n  # thread will always be created.\n  #\n  detect-thread-ratio: 1.0\n  #\n  # By default, the per-thread stack size is left to its default setting. If\n  # the default thread stack size is too small, use the following configuration\n  # setting to change the size. Note that if any thread's stack size cannot be\n  # set to this value, a fatal error occurs.\n  #\n  # Generally, the per-thread stack-size should not exceed 8MB.\n  #stack-size: 8mb\n\n# Luajit has a strange memory requirement, its 'states' need to be in the\n# first 2G of the process' memory.\n#\n# 'luajit.states' is used to control how many states are preallocated.\n# State use: per detect script: 1 per detect thread. Per output script: 1 per\n# script.\nluajit:\n  states: 128\n\n# Profiling settings. Only effective if Suricata has been built with\n# the --enable-profiling configure flag.\n#\nprofiling:\n  # Run profiling for every X-th packet. The default is 1, which means we\n  # profile every packet. If set to 1000, one packet is profiled for every\n  # 1000 received.\n  #sample-rate: 1000\n\n  # rule profiling\n  rules:\n\n    # Profiling can be disabled here, but it will still have a\n    # performance impact if compiled in.\n    enabled: yes\n    filename: rule_perf.log\n    append: yes\n\n    # Sort options: ticks, avgticks, checks, matches, maxticks\n    # If commented out all the sort options will be used.\n    #sort: avgticks\n\n    # Limit the number of sids for which stats are shown at exit (per sort).\n    limit: 10\n\n    # output to json\n    json: @e_enable_evelog@\n\n  # per keyword profiling\n  keywords:\n    enabled: yes\n    filename: keyword_perf.log\n    append: yes\n\n  prefilter:\n    enabled: yes\n    filename: prefilter_perf.log\n    append: yes\n\n  # per rulegroup profiling\n  rulegroups:\n    enabled: yes\n    filename: rule_group_perf.log\n    append: yes\n\n  # packet profiling\n  packets:\n\n    # Profiling can be disabled here, but it will still have a\n    # performance impact if compiled in.\n    enabled: yes\n    filename: packet_stats.log\n    append: yes\n\n    # per packet csv output\n    csv:\n\n      # Output can be disabled here, but it will still have a\n      # performance impact if compiled in.\n      enabled: no\n      filename: packet_stats.csv\n\n  # profiling of locking. Only available when Suricata was built with\n  # --enable-profiling-locks.\n  locks:\n    enabled: no\n    filename: lock_stats.log\n    append: yes\n\n  pcap-log:\n    enabled: no\n    filename: pcaplog_stats.log\n    append: yes\n\n##\n## Netfilter integration\n##\n\n# When running in NFQ inline mode, it is possible to use a simulated\n# non-terminal NFQUEUE verdict.\n# This permits sending all needed packet to Suricata via this rule:\n#        iptables -I FORWARD -m mark ! --mark $MARK/$MASK -j NFQUEUE\n# And below, you can have your standard filtering ruleset. To activate\n# this mode, you need to set mode to 'repeat'\n# If you want a packet to be sent to another queue after an ACCEPT decision\n# set the mode to 'route' and set next-queue value.\n# On Linux >= 3.1, you can set batchcount to a value > 1 to improve performance\n# by processing several packets before sending a verdict (worker runmode only).\n# On Linux >= 3.6, you can set the fail-open option to yes to have the kernel\n# accept the packet if Suricata is not able to keep pace.\n# bypass mark and mask can be used to implement NFQ bypass. If bypass mark is\n# set then the NFQ bypass is activated. Suricata will set the bypass mark/mask\n# on packet of a flow that need to be bypassed. The Nefilter ruleset has to\n# directly accept all packets of a flow once a packet has been marked.\nnfq:\n#  mode: accept\n#  repeat-mark: 1\n#  repeat-mask: 1\n#  bypass-mark: 1\n#  bypass-mask: 1\n#  route-queue: 2\n#  batchcount: 20\n#  fail-open: yes\n\n#nflog support\nnflog:\n    # netlink multicast group\n    # (the same as the iptables --nflog-group param)\n    # Group 0 is used by the kernel, so you can't use it\n  - group: 2\n    # netlink buffer size\n    buffer-size: 18432\n    # put default value here\n  - group: default\n    # set number of packets to queue inside kernel\n    qthreshold: 1\n    # set the delay before flushing packet in the kernel's queue\n    qtimeout: 100\n    # netlink max buffer size\n    max-size: 20000\n\n##\n## Advanced Capture Options\n##\n\n# General settings affecting packet capture\ncapture:\n  # disable NIC offloading. It's restored when Suricata exits.\n  # Enabled by default.\n  #disable-offloading: false\n  #\n  # disable checksum validation. Same as setting '-k none' on the\n  # commandline.\n  #checksum-validation: none\n\n# Netmap support\n#\n# Netmap operates with NIC directly in driver, so you need FreeBSD 11+ which has\n# built-in Netmap support or compile and install the Netmap module and appropriate\n# NIC driver for your Linux system.\n# To reach maximum throughput disable all receive-, segmentation-,\n# checksum- offloading on your NIC (using ethtool or similar).\n# Disabling TX checksum offloading is *required* for connecting OS endpoint\n# with NIC endpoint.\n# You can find more information at https://github.com/luigirizzo/netmap\n#\nnetmap:\n   # To specify OS endpoint add plus sign at the end (e.g. \"eth0+\")\n - interface: eth2\n   # Number of capture threads. \"auto\" uses number of RSS queues on interface.\n   # Warning: unless the RSS hashing is symmetrical, this will lead to\n   # accuracy issues.\n   #threads: auto\n   # You can use the following variables to activate netmap tap or IPS mode.\n   # If copy-mode is set to ips or tap, the traffic coming to the current\n   # interface will be copied to the copy-iface interface. If 'tap' is set, the\n   # copy is complete. If 'ips' is set, the packet matching a 'drop' action\n   # will not be copied.\n   # To specify the OS as the copy-iface (so the OS can route packets, or forward\n   # to a service running on the same machine) add a plus sign at the end\n   # (e.g. \"copy-iface: eth0+\"). Don't forget to set up a symmetrical eth0+ -> eth0\n   # for return packets. Hardware checksumming must be *off* on the interface if\n   # using an OS endpoint (e.g. 'ifconfig eth0 -rxcsum -txcsum -rxcsum6 -txcsum6' for FreeBSD\n   # or 'ethtool -K eth0 tx off rx off' for Linux).\n   #copy-mode: tap\n   #copy-iface: eth3\n   # Set to yes to disable promiscuous mode\n   # disable-promisc: no\n   # Choose checksum verification mode for the interface. At the moment\n   # of the capture, some packets may have an invalid checksum due to\n   # the checksum computation being offloaded to the network card.\n   # Possible values are:\n   #  - yes: checksum validation is forced\n   #  - no: checksum validation is disabled\n   #  - auto: Suricata uses a statistical approach to detect when\n   #  checksum off-loading is used.\n   # Warning: 'checksum-validation' must be set to yes to have any validation\n   #checksum-checks: auto\n   # BPF filter to apply to this interface. The pcap filter syntax apply here.\n   #bpf-filter: port 80 or udp\n #- interface: eth3\n   #threads: auto\n   #copy-mode: tap\n   #copy-iface: eth2\n   # Put default values here\n - interface: default\n\n# PF_RING configuration: for use with native PF_RING support\n# for more info see http://www.ntop.org/products/pf_ring/\npfring:\n  - interface: eth0\n    # Number of receive threads. If set to 'auto' Suricata will first try\n    # to use CPU (core) count and otherwise RSS queue count.\n    threads: auto\n\n    # Default clusterid.  PF_RING will load balance packets based on flow.\n    # All threads/processes that will participate need to have the same\n    # clusterid.\n    cluster-id: 99\n\n    # Default PF_RING cluster type. PF_RING can load balance per flow.\n    # Possible values are:\n    # - cluster_flow:               6-tuple: <src ip, src_port, dst ip, dst port, proto, vlan>\n    # - cluster_inner_flow:         6-tuple: <src ip, src port, dst ip, dst port, proto, vlan>\n    # - cluster_inner_flow_2_tuple: 2-tuple: <src ip,           dst ip                       >\n    # - cluster_inner_flow_4_tuple: 4-tuple: <src ip, src port, dst ip, dst port             >\n    # - cluster_inner_flow_5_tuple: 5-tuple: <src ip, src port, dst ip, dst port, proto      >\n    # - cluster_round_robin (NOT RECOMMENDED)\n    cluster-type: cluster_flow\n\n    # bpf filter for this interface\n    #bpf-filter: tcp\n\n    # If bypass is set then the PF_RING hw bypass is activated, when supported\n    # by the network interface. Suricata will instruct the interface to bypass\n    # all future packets for a flow that need to be bypassed.\n    #bypass: yes\n\n    # Choose checksum verification mode for the interface. At the moment\n    # of the capture, some packets may have an invalid checksum due to\n    # the checksum computation being offloaded to the network card.\n    # Possible values are:\n    #  - rxonly: only compute checksum for packets received by network card.\n    #  - yes: checksum validation is forced\n    #  - no: checksum validation is disabled\n    #  - auto: Suricata uses a statistical approach to detect when\n    #  checksum off-loading is used. (default)\n    # Warning: 'checksum-validation' must be set to yes to have any validation\n    #checksum-checks: auto\n  # Second interface\n  #- interface: eth1\n  #  threads: 3\n  #  cluster-id: 93\n  #  cluster-type: cluster_flow\n  # Put default values here\n  - interface: default\n    #threads: 2\n\n# For FreeBSD ipfw(8) divert(4) support.\n# Please make sure you have ipfw_load=\"YES\" and ipdivert_load=\"YES\"\n# in /etc/loader.conf or kldload'ing the appropriate kernel modules.\n# Additionally, you need to have an ipfw rule for the engine to see\n# the packets from ipfw.  For Example:\n#\n#   ipfw add 100 divert 8000 ip from any to any\n#\n# N.B. This example uses \"8000\" -- this number must mach the values\n# you passed on the command line, i.e., -d 8000\n#\nipfw:\n\n  # Reinject packets at the specified ipfw rule number.  This config\n  # option is the ipfw rule number AT WHICH rule processing continues\n  # in the ipfw processing system after the engine has finished\n  # inspecting the packet for acceptance.  If no rule number is specified,\n  # accepted packets are reinjected at the divert rule which they entered\n  # and IPFW rule processing continues.  No check is done to verify\n  # this will rule makes sense so care must be taken to avoid loops in ipfw.\n  #\n  ## The following example tells the engine to reinject packets\n  # back into the ipfw firewall AT rule number 5500:\n  #\n  # ipfw-reinjection-rule-number: 5500\n\n\nnapatech:\n    # When use_all_streams is set to \"yes\" the initialization code will query\n    # the Napatech service for all configured streams and listen on all of them.\n    # When set to \"no\" the streams config array will be used.\n    #\n    # This option necessitates running the appropriate NTPL commands to create\n    # the desired streams prior to running Suricata.\n    #use-all-streams: no\n\n    # The streams to listen on when auto-config is disabled or when and threading\n    # cpu-affinity is disabled.  This can be either:\n    #   an individual stream (e.g. streams: [0])\n    # or\n    #   a range of streams (e.g. streams: [\"0-3\"])\n    #\n    streams: [\"0-3\"]\n\n    # Stream stats can be enabled to provide fine grain packet and byte counters\n    # for each thread/stream that is configured.\n    #\n    enable-stream-stats: no\n\n    # When auto-config is enabled the streams will be created and assigned\n    # automatically to the NUMA node where the thread resides.  If cpu-affinity\n    # is enabled in the threading section.  Then the streams will be created\n    # according to the number of worker threads specified in the worker-cpu-set.\n    # Otherwise, the streams array is used to define the streams.\n    #\n    # This option is intended primarily to support legacy configurations.\n    #\n    # This option cannot be used simultaneously with either \"use-all-streams\"\n    # or \"hardware-bypass\".\n    #\n    auto-config: yes\n\n    # Enable hardware level flow bypass.\n    #\n    hardware-bypass: yes\n\n    # Enable inline operation.  When enabled traffic arriving on a given port is\n    # automatically forwarded out its peer port after analysis by Suricata.\n    #\n    inline: no\n\n    # Ports indicates which Napatech ports are to be used in auto-config mode.\n    # these are the port IDs of the ports that will be merged prior to the\n    # traffic being distributed to the streams.\n    #\n    # When hardware-bypass is enabled the ports must be configured as a segment.\n    # specify the port(s) on which upstream and downstream traffic will arrive.\n    # This information is necessary for the hardware to properly process flows.\n    #\n    # When using a tap configuration one of the ports will receive inbound traffic\n    # for the network and the other will receive outbound traffic. The two ports on a\n    # given segment must reside on the same network adapter.\n    #\n    # When using a SPAN-port configuration the upstream and downstream traffic\n    # arrives on a single port. This is configured by setting the two sides of the\n    # segment to reference the same port.  (e.g. 0-0 to configure a SPAN port on\n    # port 0).\n    #\n    # port segments are specified in the form:\n    #    ports: [0-1,2-3,4-5,6-6,7-7]\n    #\n    # For legacy systems when hardware-bypass is disabled this can be specified in any\n    # of the following ways:\n    #\n    #   a list of individual ports (e.g. ports: [0,1,2,3])\n    #\n    #   a range of ports (e.g. ports: [0-3])\n    #\n    #   \"all\" to indicate that all ports are to be merged together\n    #   (e.g. ports: [all])\n    #\n    # This parameter has no effect if auto-config is disabled.\n    #\n    ports: [0-1,2-3]\n\n    # When auto-config is enabled the hashmode specifies the algorithm for\n    # determining to which stream a given packet is to be delivered.\n    # This can be any valid Napatech NTPL hashmode command.\n    #\n    # The most common hashmode commands are:  hash2tuple, hash2tuplesorted,\n    # hash5tuple, hash5tuplesorted and roundrobin.\n    #\n    # See Napatech NTPL documentation other hashmodes and details on their use.\n    #\n    # This parameter has no effect if auto-config is disabled.\n    #\n    hashmode: hash5tuplesorted\n\n##\n## Configure Suricata to load Suricata-Update managed rules.\n##\n\ndefault-rule-path: @e_defaultruledir@\n\nrule-files:\n  - suricata.rules\n\n##\n## Auxiliary configuration files.\n##\n\nclassification-file: @e_sysconfdir@classification.config\nreference-config-file: @e_sysconfdir@reference.config\n# threshold-file: @e_sysconfdir@threshold.config\n\n##\n## Include other configs\n##\n\n# Includes:  Files included here will be handled as if they were in-lined\n# in this configuration file. Files with relative pathnames will be\n# searched for in the same directory as this configuration file. You may\n# use absolute pathnames too.\n# You can specify more than 2 configuration files, if needed.\n#include: include1.yaml\n#include: include2.yaml\n"], "fixing_code": ["/* Copyright (C) 2018-2020 Open Information Security Foundation\n *\n * You can copy, redistribute or modify this Program under the terms of\n * the GNU General Public License version 2 as published by the Free\n * Software Foundation.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * version 2 along with this program; if not, write to the Free Software\n * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n * 02110-1301, USA.\n */\n\n/**\n * \\file\n *\n *  \\author Victor Julien <victor@inliniac.net>\n *\n *  Implements the dataset keyword\n */\n\n#include \"suricata-common.h\"\n#include \"decode.h\"\n#include \"detect.h\"\n#include \"threads.h\"\n#include \"datasets.h\"\n#include \"detect-dataset.h\"\n\n#include \"detect-parse.h\"\n#include \"detect-engine.h\"\n#include \"detect-engine-mpm.h\"\n#include \"detect-engine-state.h\"\n\n#include \"util-debug.h\"\n#include \"util-print.h\"\n#include \"util-misc.h\"\n\nint DetectDatasetMatch (ThreadVars *, DetectEngineThreadCtx *, Packet *,\n        const Signature *, const SigMatchCtx *);\nstatic int DetectDatasetSetup (DetectEngineCtx *, Signature *, const char *);\nvoid DetectDatasetFree (DetectEngineCtx *, void *);\n\nvoid DetectDatasetRegister (void)\n{\n    sigmatch_table[DETECT_DATASET].name = \"dataset\";\n    sigmatch_table[DETECT_DATASET].desc = \"match sticky buffer against datasets (experimental)\";\n    sigmatch_table[DETECT_DATASET].url = \"/rules/dataset-keywords.html#dataset\";\n    sigmatch_table[DETECT_DATASET].Setup = DetectDatasetSetup;\n    sigmatch_table[DETECT_DATASET].Free  = DetectDatasetFree;\n}\n\n/*\n    1 match\n    0 no match\n    -1 can't match\n */\nint DetectDatasetBufferMatch(DetectEngineThreadCtx *det_ctx,\n    const DetectDatasetData *sd,\n    const uint8_t *data, const uint32_t data_len)\n{\n    if (data == NULL || data_len == 0)\n        return 0;\n\n    switch (sd->cmd) {\n        case DETECT_DATASET_CMD_ISSET: {\n            //PrintRawDataFp(stdout, data, data_len);\n            int r = DatasetLookup(sd->set, data, data_len);\n            SCLogDebug(\"r %d\", r);\n            if (r == 1)\n                return 1;\n            break;\n        }\n        case DETECT_DATASET_CMD_ISNOTSET: {\n            //PrintRawDataFp(stdout, data, data_len);\n            int r = DatasetLookup(sd->set, data, data_len);\n            SCLogDebug(\"r %d\", r);\n            if (r < 1)\n                return 1;\n            break;\n        }\n        case DETECT_DATASET_CMD_SET: {\n            //PrintRawDataFp(stdout, data, data_len);\n            int r = DatasetAdd(sd->set, data, data_len);\n            if (r == 1)\n                return 1;\n            break;\n        }\n        default:\n            abort();\n    }\n    return 0;\n}\n\nstatic int DetectDatasetParse(const char *str, char *cmd, int cmd_len, char *name, int name_len,\n        enum DatasetTypes *type, char *load, size_t load_size, char *save, size_t save_size,\n        uint64_t *memcap, uint32_t *hashsize)\n{\n    bool cmd_set = false;\n    bool name_set = false;\n    bool load_set = false;\n    bool save_set = false;\n    bool state_set = false;\n\n    char copy[strlen(str)+1];\n    strlcpy(copy, str, sizeof(copy));\n    char *xsaveptr = NULL;\n    char *key = strtok_r(copy, \",\", &xsaveptr);\n    while (key != NULL) {\n        while (*key != '\\0' && isblank(*key)) {\n            key++;\n        }\n        char *val = strchr(key, ' ');\n        if (val != NULL) {\n            *val++ = '\\0';\n            while (*val != '\\0' && isblank(*val)) {\n                val++;\n                SCLogDebug(\"cmd %s val %s\", key, val);\n            }\n        } else {\n            SCLogDebug(\"cmd %s\", key);\n        }\n\n        if (strlen(key) == 0) {\n            goto next;\n        }\n\n        if (!cmd_set) {\n            if (val && strlen(val) != 0) {\n                return -1;\n            }\n            strlcpy(cmd, key, cmd_len);\n            cmd_set = true;\n        } else if (!name_set) {\n            if (val && strlen(val) != 0) {\n                return -1;\n            }\n            strlcpy(name, key, name_len);\n            name_set = true;\n        } else {\n            if (val == NULL) {\n                return -1;\n            }\n\n            if (strcmp(key, \"type\") == 0) {\n                SCLogDebug(\"type %s\", val);\n\n                if (strcmp(val, \"md5\") == 0) {\n                    *type = DATASET_TYPE_MD5;\n                } else if (strcmp(val, \"sha256\") == 0) {\n                    *type = DATASET_TYPE_SHA256;\n                } else if (strcmp(val, \"string\") == 0) {\n                    *type = DATASET_TYPE_STRING;\n                } else {\n                    SCLogError(SC_ERR_INVALID_SIGNATURE, \"bad type %s\", val);\n                    return -1;\n                }\n\n            } else if (strcmp(key, \"save\") == 0) {\n                if (save_set) {\n                    SCLogWarning(SC_ERR_INVALID_SIGNATURE,\n                        \"'save' can only appear once\");\n                    return -1;\n                }\n                SCLogDebug(\"save %s\", val);\n                strlcpy(save, val, save_size);\n                save_set = true;\n            } else if (strcmp(key, \"load\") == 0) {\n                if (load_set) {\n                    SCLogWarning(SC_ERR_INVALID_SIGNATURE,\n                        \"'load' can only appear once\");\n                    return -1;\n                }\n                SCLogDebug(\"load %s\", val);\n                strlcpy(load, val, load_size);\n                load_set = true;\n            } else if (strcmp(key, \"state\") == 0) {\n                if (state_set) {\n                    SCLogWarning(SC_ERR_INVALID_SIGNATURE,\n                        \"'state' can only appear once\");\n                    return -1;\n                }\n                SCLogDebug(\"state %s\", val);\n                strlcpy(load, val, load_size);\n                strlcpy(save, val, save_size);\n                state_set = true;\n            }\n            if (strcmp(key, \"memcap\") == 0) {\n                if (ParseSizeStringU64(val, memcap) < 0) {\n                    SCLogWarning(SC_ERR_INVALID_VALUE,\n                            \"invalid value for memcap: %s,\"\n                            \" resetting to default\",\n                            val);\n                    *memcap = 0;\n                }\n            }\n            if (strcmp(key, \"hashsize\") == 0) {\n                if (ParseSizeStringU32(val, hashsize) < 0) {\n                    SCLogWarning(SC_ERR_INVALID_VALUE,\n                            \"invalid value for hashsize: %s,\"\n                            \" resetting to default\",\n                            val);\n                    *hashsize = 0;\n                }\n            }\n        }\n\n        SCLogDebug(\"key: %s, value: %s\", key, val);\n\n    next:\n        key = strtok_r(NULL, \",\", &xsaveptr);\n    }\n\n    if ((load_set || save_set) && state_set) {\n        SCLogWarning(SC_ERR_INVALID_SIGNATURE,\n                \"'state' can not be mixed with 'load' and 'save'\");\n        return -1;\n    }\n\n    /* Trim trailing whitespace. */\n    while (strlen(name) > 0 && isblank(name[strlen(name) - 1])) {\n        name[strlen(name) - 1] = '\\0';\n    }\n\n    /* Validate name, spaces are not allowed. */\n    for (size_t i = 0; i < strlen(name); i++) {\n        if (isblank(name[i])) {\n            SCLogError(SC_ERR_INVALID_SIGNATURE,\n                    \"spaces not allowed in dataset names\");\n            return 0;\n        }\n    }\n\n    return 1;\n}\n\n/** \\brief wrapper around dirname that does leave input untouched */\nstatic void GetDirName(const char *in, char *out, size_t outs)\n{\n    if (strlen(in) == 0) {\n        return;\n    }\n\n    size_t size = strlen(in) + 1;\n    char tmp[size];\n    strlcpy(tmp, in, size);\n\n    char *dir = dirname(tmp);\n    BUG_ON(dir == NULL);\n    strlcpy(out, dir, outs);\n    return;\n}\n\nstatic int SetupLoadPath(const DetectEngineCtx *de_ctx,\n        char *load, size_t load_size)\n{\n    SCLogDebug(\"load %s\", load);\n\n    if (PathIsAbsolute(load)) {\n        return 0;\n    }\n\n    bool done = false;\n#ifdef HAVE_LIBGEN_H\n    BUG_ON(de_ctx->rule_file == NULL);\n\n    char dir[PATH_MAX] = \"\";\n    GetDirName(de_ctx->rule_file, dir, sizeof(dir));\n\n    SCLogDebug(\"rule_file %s dir %s\", de_ctx->rule_file, dir);\n    char path[PATH_MAX];\n    if (snprintf(path, sizeof(path), \"%s/%s\", dir, load) >= (int)sizeof(path)) // TODO windows path\n        return -1;\n\n    if (SCPathExists(path)) {\n        done = true;\n        strlcpy(load, path, load_size);\n        SCLogDebug(\"using path '%s' (HAVE_LIBGEN_H)\", load);\n    }\n#endif\n    if (!done) {\n        char *loadp = DetectLoadCompleteSigPath(de_ctx, load);\n        if (loadp == NULL) {\n            return -1;\n        }\n        SCLogDebug(\"loadp %s\", loadp);\n\n        if (SCPathExists(loadp)) {\n            strlcpy(load, loadp, load_size);\n            SCLogDebug(\"using path '%s' (non-HAVE_LIBGEN_H)\", load);\n        }\n        SCFree(loadp);\n    }\n    return 0;\n}\n\nstatic int SetupSavePath(const DetectEngineCtx *de_ctx,\n        char *save, size_t save_size)\n{\n    SCLogDebug(\"save %s\", save);\n\n    int allow_save = 1;\n    if (ConfGetBool(\"datasets.rules.allow-write\", &allow_save)) {\n        if (!allow_save) {\n            SCLogError(SC_ERR_INVALID_SIGNATURE,\n                    \"Rules containing save/state datasets have been disabled\");\n            return -1;\n        }\n    }\n\n    int allow_absolute = 0;\n    (void)ConfGetBool(\"datasets.rules.allow-absolute-filenames\", &allow_absolute);\n    if (allow_absolute) {\n        SCLogNotice(\"Allowing absolute filename for dataset rule: %s\", save);\n    } else {\n        if (PathIsAbsolute(save)) {\n            SCLogError(SC_ERR_INVALID_ARGUMENT, \"Absolute paths not allowed: %s\", save);\n            return -1;\n        }\n\n        if (SCPathContainsTraversal(save)) {\n            SCLogError(SC_ERR_INVALID_ARGUMENT, \"Directory traversals not allowed: %s\", save);\n            return -1;\n        }\n    }\n\n    // data dir\n    const char *dir = ConfigGetDataDirectory();\n    BUG_ON(dir == NULL); // should not be able to fail\n    char path[PATH_MAX];\n    if (snprintf(path, sizeof(path), \"%s/%s\", dir, save) >= (int)sizeof(path)) // TODO windows path\n        return -1;\n\n    /* TODO check if location exists and is writable */\n\n    strlcpy(save, path, save_size);\n\n    return 0;\n}\n\nint DetectDatasetSetup (DetectEngineCtx *de_ctx, Signature *s, const char *rawstr)\n{\n    DetectDatasetData *cd = NULL;\n    SigMatch *sm = NULL;\n    uint8_t cmd = 0;\n    uint64_t memcap = 0;\n    uint32_t hashsize = 0;\n    char cmd_str[16] = \"\", name[DATASET_NAME_MAX_LEN + 1] = \"\";\n    enum DatasetTypes type = DATASET_TYPE_NOTSET;\n    char load[PATH_MAX] = \"\";\n    char save[PATH_MAX] = \"\";\n\n    if (DetectBufferGetActiveList(de_ctx, s) == -1) {\n        SCLogError(SC_ERR_INVALID_SIGNATURE,\n                \"datasets are only supported for sticky buffers\");\n        SCReturnInt(-1);\n    }\n\n    int list = s->init_data->list;\n    if (list == DETECT_SM_LIST_NOTSET) {\n        SCLogError(SC_ERR_INVALID_SIGNATURE,\n                \"datasets are only supported for sticky buffers\");\n        SCReturnInt(-1);\n    }\n\n    if (!DetectDatasetParse(rawstr, cmd_str, sizeof(cmd_str), name, sizeof(name), &type, load,\n                sizeof(load), save, sizeof(save), &memcap, &hashsize)) {\n        return -1;\n    }\n\n    if (strcmp(cmd_str,\"isset\") == 0) {\n        cmd = DETECT_DATASET_CMD_ISSET;\n    } else if (strcmp(cmd_str,\"isnotset\") == 0) {\n        cmd = DETECT_DATASET_CMD_ISNOTSET;\n    } else if (strcmp(cmd_str,\"set\") == 0) {\n        cmd = DETECT_DATASET_CMD_SET;\n    } else if (strcmp(cmd_str,\"unset\") == 0) {\n        cmd = DETECT_DATASET_CMD_UNSET;\n    } else {\n        SCLogError(SC_ERR_UNKNOWN_VALUE,\n                \"dataset action \\\"%s\\\" is not supported.\", cmd_str);\n        return -1;\n    }\n\n    /* if just 'load' is set, we load data from the same dir as the\n     * rule file. If load+save is used, we use data dir */\n    if (strlen(save) == 0 && strlen(load) != 0) {\n        if (SetupLoadPath(de_ctx, load, sizeof(load)) != 0)\n            return -1;\n    /* if just 'save' is set, we use either full path or the\n     * data-dir */\n    } else if (strlen(save) != 0 && strlen(load) == 0) {\n        if (SetupSavePath(de_ctx, save, sizeof(save)) != 0)\n            return -1;\n    /* use 'save' logic for 'state', but put the resulting\n     * path into 'load' as well. */\n    } else if (strlen(save) != 0 && strlen(load) != 0 &&\n            strcmp(save, load) == 0) {\n        if (SetupSavePath(de_ctx, save, sizeof(save)) != 0)\n            return -1;\n        strlcpy(load, save, sizeof(load));\n    }\n\n    SCLogDebug(\"name '%s' load '%s' save '%s'\", name, load, save);\n    Dataset *set = DatasetGet(name, type, save, load, memcap, hashsize);\n    if (set == NULL) {\n        SCLogError(SC_ERR_INVALID_SIGNATURE,\n                \"failed to set up dataset '%s'.\", name);\n        return -1;\n    }\n    if (set->hash && SC_ATOMIC_GET(set->hash->memcap_reached)) {\n        SCLogError(SC_ERR_THASH_INIT, \"dataset too large for set memcap\");\n        return -1;\n    }\n\n    cd = SCCalloc(1, sizeof(DetectDatasetData));\n    if (unlikely(cd == NULL))\n        goto error;\n\n    cd->set = set;\n    cd->cmd = cmd;\n\n    SCLogDebug(\"cmd %s, name %s\",\n        cmd_str, strlen(name) ? name : \"(none)\");\n\n    /* Okay so far so good, lets get this into a SigMatch\n     * and put it in the Signature. */\n    sm = SigMatchAlloc();\n    if (sm == NULL)\n        goto error;\n\n    sm->type = DETECT_DATASET;\n    sm->ctx = (SigMatchCtx *)cd;\n    SigMatchAppendSMToList(s, sm, list);\n    return 0;\n\nerror:\n    if (cd != NULL)\n        SCFree(cd);\n    if (sm != NULL)\n        SCFree(sm);\n    return -1;\n}\n\nvoid DetectDatasetFree (DetectEngineCtx *de_ctx, void *ptr)\n{\n    DetectDatasetData *fd = (DetectDatasetData *)ptr;\n    if (fd == NULL)\n        return;\n\n    SCFree(fd);\n}\n", "%YAML 1.1\n---\n\n# Suricata configuration file. In addition to the comments describing all\n# options in this file, full documentation can be found at:\n# https://suricata.readthedocs.io/en/latest/configuration/suricata-yaml.html\n\n# This configuration file generated by Suricata @PACKAGE_VERSION@.\nsuricata-version: \"@MAJOR_MINOR@\"\n\n##\n## Step 1: Inform Suricata about your network\n##\n\nvars:\n  # more specific is better for alert accuracy and performance\n  address-groups:\n    HOME_NET: \"[192.168.0.0/16,10.0.0.0/8,172.16.0.0/12]\"\n    #HOME_NET: \"[192.168.0.0/16]\"\n    #HOME_NET: \"[10.0.0.0/8]\"\n    #HOME_NET: \"[172.16.0.0/12]\"\n    #HOME_NET: \"any\"\n\n    EXTERNAL_NET: \"!$HOME_NET\"\n    #EXTERNAL_NET: \"any\"\n\n    HTTP_SERVERS: \"$HOME_NET\"\n    SMTP_SERVERS: \"$HOME_NET\"\n    SQL_SERVERS: \"$HOME_NET\"\n    DNS_SERVERS: \"$HOME_NET\"\n    TELNET_SERVERS: \"$HOME_NET\"\n    AIM_SERVERS: \"$EXTERNAL_NET\"\n    DC_SERVERS: \"$HOME_NET\"\n    DNP3_SERVER: \"$HOME_NET\"\n    DNP3_CLIENT: \"$HOME_NET\"\n    MODBUS_CLIENT: \"$HOME_NET\"\n    MODBUS_SERVER: \"$HOME_NET\"\n    ENIP_CLIENT: \"$HOME_NET\"\n    ENIP_SERVER: \"$HOME_NET\"\n\n  port-groups:\n    HTTP_PORTS: \"80\"\n    SHELLCODE_PORTS: \"!80\"\n    ORACLE_PORTS: 1521\n    SSH_PORTS: 22\n    DNP3_PORTS: 20000\n    MODBUS_PORTS: 502\n    FILE_DATA_PORTS: \"[$HTTP_PORTS,110,143]\"\n    FTP_PORTS: 21\n    GENEVE_PORTS: 6081\n    VXLAN_PORTS: 4789\n    TEREDO_PORTS: 3544\n\n##\n## Step 2: Select outputs to enable\n##\n\n# The default logging directory.  Any log or output file will be\n# placed here if it's not specified with a full path name. This can be\n# overridden with the -l command line parameter.\ndefault-log-dir: @e_logdir@\n\n# Global stats configuration\nstats:\n  enabled: yes\n  # The interval field (in seconds) controls the interval at\n  # which stats are updated in the log.\n  interval: 8\n  # Add decode events to stats.\n  #decoder-events: true\n  # Decoder event prefix in stats. Has been 'decoder' before, but that leads\n  # to missing events in the eve.stats records. See issue #2225.\n  #decoder-events-prefix: \"decoder.event\"\n  # Add stream events as stats.\n  #stream-events: false\n\n# Configure the type of alert (and other) logging you would like.\noutputs:\n  # a line based alerts log similar to Snort's fast.log\n  - fast:\n      enabled: yes\n      filename: fast.log\n      append: yes\n      #filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'\n\n  # Extensible Event Format (nicknamed EVE) event log in JSON format\n  - eve-log:\n      enabled: @e_enable_evelog@\n      filetype: regular #regular|syslog|unix_dgram|unix_stream|redis\n      filename: eve.json\n      # Enable for multi-threaded eve.json output; output files are amended with\n      # with an identifier, e.g., eve.9.json\n      #threaded: false\n      #prefix: \"@cee: \" # prefix to prepend to each log entry\n      # the following are valid when type: syslog above\n      #identity: \"suricata\"\n      #facility: local5\n      #level: Info ## possible levels: Emergency, Alert, Critical,\n                   ## Error, Warning, Notice, Info, Debug\n      #ethernet: no  # log ethernet header in events when available\n      #redis:\n      #  server: 127.0.0.1\n      #  port: 6379\n      #  async: true ## if redis replies are read asynchronously\n      #  mode: list ## possible values: list|lpush (default), rpush, channel|publish\n      #             ## lpush and rpush are using a Redis list. \"list\" is an alias for lpush\n      #             ## publish is using a Redis channel. \"channel\" is an alias for publish\n      #  key: suricata ## key or channel to use (default to suricata)\n      # Redis pipelining set up. This will enable to only do a query every\n      # 'batch-size' events. This should lower the latency induced by network\n      # connection at the cost of some memory. There is no flushing implemented\n      # so this setting should be reserved to high traffic Suricata deployments.\n      #  pipelining:\n      #    enabled: yes ## set enable to yes to enable query pipelining\n      #    batch-size: 10 ## number of entries to keep in buffer\n\n      # Include top level metadata. Default yes.\n      #metadata: no\n\n      # include the name of the input pcap file in pcap file processing mode\n      pcap-file: false\n\n      # Community Flow ID\n      # Adds a 'community_id' field to EVE records. These are meant to give\n      # records a predictable flow ID that can be used to match records to\n      # output of other tools such as Zeek (Bro).\n      #\n      # Takes a 'seed' that needs to be same across sensors and tools\n      # to make the id less predictable.\n\n      # enable/disable the community id feature.\n      community-id: false\n      # Seed value for the ID output. Valid values are 0-65535.\n      community-id-seed: 0\n\n      # HTTP X-Forwarded-For support by adding an extra field or overwriting\n      # the source or destination IP address (depending on flow direction)\n      # with the one reported in the X-Forwarded-For HTTP header. This is\n      # helpful when reviewing alerts for traffic that is being reverse\n      # or forward proxied.\n      xff:\n        enabled: no\n        # Two operation modes are available: \"extra-data\" and \"overwrite\".\n        mode: extra-data\n        # Two proxy deployments are supported: \"reverse\" and \"forward\". In\n        # a \"reverse\" deployment the IP address used is the last one, in a\n        # \"forward\" deployment the first IP address is used.\n        deployment: reverse\n        # Header name where the actual IP address will be reported. If more\n        # than one IP address is present, the last IP address will be the\n        # one taken into consideration.\n        header: X-Forwarded-For\n\n      types:\n        - alert:\n            # payload: yes             # enable dumping payload in Base64\n            # payload-buffer-size: 4kb # max size of payload buffer to output in eve-log\n            # payload-printable: yes   # enable dumping payload in printable (lossy) format\n            # packet: yes              # enable dumping of packet (without stream segments)\n            # metadata: no             # enable inclusion of app layer metadata with alert. Default yes\n            # http-body: yes           # Requires metadata; enable dumping of HTTP body in Base64\n            # http-body-printable: yes # Requires metadata; enable dumping of HTTP body in printable format\n\n            # Enable the logging of tagged packets for rules using the\n            # \"tag\" keyword.\n            tagged-packets: yes\n        - anomaly:\n            # Anomaly log records describe unexpected conditions such\n            # as truncated packets, packets with invalid IP/UDP/TCP\n            # length values, and other events that render the packet\n            # invalid for further processing or describe unexpected\n            # behavior on an established stream. Networks which\n            # experience high occurrences of anomalies may experience\n            # packet processing degradation.\n            #\n            # Anomalies are reported for the following:\n            # 1. Decode: Values and conditions that are detected while\n            # decoding individual packets. This includes invalid or\n            # unexpected values for low-level protocol lengths as well\n            # as stream related events (TCP 3-way handshake issues,\n            # unexpected sequence number, etc).\n            # 2. Stream: This includes stream related events (TCP\n            # 3-way handshake issues, unexpected sequence number,\n            # etc).\n            # 3. Application layer: These denote application layer\n            # specific conditions that are unexpected, invalid or are\n            # unexpected given the application monitoring state.\n            #\n            # By default, anomaly logging is enabled. When anomaly\n            # logging is enabled, applayer anomaly reporting is\n            # also enabled.\n            enabled: yes\n            #\n            # Choose one or more types of anomaly logging and whether to enable\n            # logging of the packet header for packet anomalies.\n            types:\n              # decode: no\n              # stream: no\n              # applayer: yes\n            #packethdr: no\n        - http:\n            extended: yes     # enable this for extended logging information\n            # custom allows additional HTTP fields to be included in eve-log.\n            # the example below adds three additional fields when uncommented\n            #custom: [Accept-Encoding, Accept-Language, Authorization]\n            # set this value to one and only one from {both, request, response}\n            # to dump all HTTP headers for every HTTP request and/or response\n            # dump-all-headers: none\n        - dns:\n            # This configuration uses the new DNS logging format,\n            # the old configuration is still available:\n            # https://suricata.readthedocs.io/en/latest/output/eve/eve-json-output.html#dns-v1-format\n\n            # As of Suricata 5.0, version 2 of the eve dns output\n            # format is the default.\n            #version: 2\n\n            # Enable/disable this logger. Default: enabled.\n            #enabled: yes\n\n            # Control logging of requests and responses:\n            # - requests: enable logging of DNS queries\n            # - responses: enable logging of DNS answers\n            # By default both requests and responses are logged.\n            #requests: no\n            #responses: no\n\n            # Format of answer logging:\n            # - detailed: array item per answer\n            # - grouped: answers aggregated by type\n            # Default: all\n            #formats: [detailed, grouped]\n\n            # DNS record types to log, based on the query type.\n            # Default: all.\n            #types: [a, aaaa, cname, mx, ns, ptr, txt]\n        - tls:\n            extended: yes     # enable this for extended logging information\n            # output TLS transaction where the session is resumed using a\n            # session id\n            #session-resumption: no\n            # custom controls which TLS fields that are included in eve-log\n            #custom: [subject, issuer, session_resumed, serial, fingerprint, sni, version, not_before, not_after, certificate, chain, ja3, ja3s]\n        - files:\n            force-magic: no   # force logging magic on all logged files\n            # force logging of checksums, available hash functions are md5,\n            # sha1 and sha256\n            #force-hash: [md5]\n        #- drop:\n        #    alerts: yes      # log alerts that caused drops\n        #    flows: all       # start or all: 'start' logs only a single drop\n        #                     # per flow direction. All logs each dropped pkt.\n        - smtp:\n            #extended: yes # enable this for extended logging information\n            # this includes: bcc, message-id, subject, x_mailer, user-agent\n            # custom fields logging from the list:\n            #  reply-to, bcc, message-id, subject, x-mailer, user-agent, received,\n            #  x-originating-ip, in-reply-to, references, importance, priority,\n            #  sensitivity, organization, content-md5, date\n            #custom: [received, x-mailer, x-originating-ip, relays, reply-to, bcc]\n            # output md5 of fields: body, subject\n            # for the body you need to set app-layer.protocols.smtp.mime.body-md5\n            # to yes\n            #md5: [body, subject]\n\n        #- dnp3\n        - ftp\n        - rdp\n        - nfs\n        - smb\n        - tftp\n        - ikev2\n        - dcerpc\n        - krb5\n        - snmp\n        - rfb\n        - sip\n        - dhcp:\n            enabled: yes\n            # When extended mode is on, all DHCP messages are logged\n            # with full detail. When extended mode is off (the\n            # default), just enough information to map a MAC address\n            # to an IP address is logged.\n            extended: no\n        - ssh\n        - mqtt:\n            # passwords: yes           # enable output of passwords\n        # HTTP2 logging. HTTP2 support is currently experimental and\n        # disabled by default. To enable, uncomment the following line\n        # and be sure to enable http2 in the app-layer section.\n        #- http2\n        - stats:\n            totals: yes       # stats for all threads merged together\n            threads: no       # per thread stats\n            deltas: no        # include delta values\n        # bi-directional flows\n        - flow\n        # uni-directional flows\n        #- netflow\n\n        # Metadata event type. Triggered whenever a pktvar is saved\n        # and will include the pktvars, flowvars, flowbits and\n        # flowints.\n        #- metadata\n\n  # a line based log of HTTP requests (no alerts)\n  - http-log:\n      enabled: no\n      filename: http.log\n      append: yes\n      #extended: yes     # enable this for extended logging information\n      #custom: yes       # enable the custom logging format (defined by customformat)\n      #customformat: \"%{%D-%H:%M:%S}t.%z %{X-Forwarded-For}i %H %m %h %u %s %B %a:%p -> %A:%P\"\n      #filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'\n\n  # a line based log of TLS handshake parameters (no alerts)\n  - tls-log:\n      enabled: no  # Log TLS connections.\n      filename: tls.log # File to store TLS logs.\n      append: yes\n      #extended: yes     # Log extended information like fingerprint\n      #custom: yes       # enabled the custom logging format (defined by customformat)\n      #customformat: \"%{%D-%H:%M:%S}t.%z %a:%p -> %A:%P %v %n %d %D\"\n      #filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'\n      # output TLS transaction where the session is resumed using a\n      # session id\n      #session-resumption: no\n\n  # output module to store certificates chain to disk\n  - tls-store:\n      enabled: no\n      #certs-log-dir: certs # directory to store the certificates files\n\n  # Packet log... log packets in pcap format. 3 modes of operation: \"normal\"\n  # \"multi\" and \"sguil\".\n  #\n  # In normal mode a pcap file \"filename\" is created in the default-log-dir,\n  # or as specified by \"dir\".\n  # In multi mode, a file is created per thread. This will perform much\n  # better, but will create multiple files where 'normal' would create one.\n  # In multi mode the filename takes a few special variables:\n  # - %n -- thread number\n  # - %i -- thread id\n  # - %t -- timestamp (secs or secs.usecs based on 'ts-format'\n  # E.g. filename: pcap.%n.%t\n  #\n  # Note that it's possible to use directories, but the directories are not\n  # created by Suricata. E.g. filename: pcaps/%n/log.%s will log into the\n  # per thread directory.\n  #\n  # Also note that the limit and max-files settings are enforced per thread.\n  # So the size limit when using 8 threads with 1000mb files and 2000 files\n  # is: 8*1000*2000 ~ 16TiB.\n  #\n  # In Sguil mode \"dir\" indicates the base directory. In this base dir the\n  # pcaps are created in the directory structure Sguil expects:\n  #\n  # $sguil-base-dir/YYYY-MM-DD/$filename.<timestamp>\n  #\n  # By default all packets are logged except:\n  # - TCP streams beyond stream.reassembly.depth\n  # - encrypted streams after the key exchange\n  #\n  - pcap-log:\n      enabled: no\n      filename: log.pcap\n\n      # File size limit.  Can be specified in kb, mb, gb.  Just a number\n      # is parsed as bytes.\n      limit: 1000mb\n\n      # If set to a value, ring buffer mode is enabled. Will keep maximum of\n      # \"max-files\" of size \"limit\"\n      max-files: 2000\n\n      # Compression algorithm for pcap files. Possible values: none, lz4.\n      # Enabling compression is incompatible with the sguil mode. Note also\n      # that on Windows, enabling compression will *increase* disk I/O.\n      compression: none\n\n      # Further options for lz4 compression. The compression level can be set\n      # to a value between 0 and 16, where higher values result in higher\n      # compression.\n      #lz4-checksum: no\n      #lz4-level: 0\n\n      mode: normal # normal, multi or sguil.\n\n      # Directory to place pcap files. If not provided the default log\n      # directory will be used. Required for \"sguil\" mode.\n      #dir: /nsm_data/\n\n      #ts-format: usec # sec or usec second format (default) is filename.sec usec is filename.sec.usec\n      use-stream-depth: no #If set to \"yes\" packets seen after reaching stream inspection depth are ignored. \"no\" logs all packets\n      honor-pass-rules: no # If set to \"yes\", flows in which a pass rule matched will stop being logged.\n\n  # a full alert log containing much information for signature writers\n  # or for investigating suspected false positives.\n  - alert-debug:\n      enabled: no\n      filename: alert-debug.log\n      append: yes\n      #filetype: regular # 'regular', 'unix_stream' or 'unix_dgram'\n\n  # alert output to prelude (https://www.prelude-siem.org/) only\n  # available if Suricata has been compiled with --enable-prelude\n  - alert-prelude:\n      enabled: no\n      profile: suricata\n      log-packet-content: no\n      log-packet-header: yes\n\n  # Stats.log contains data from various counters of the Suricata engine.\n  - stats:\n      enabled: yes\n      filename: stats.log\n      append: yes       # append to file (yes) or overwrite it (no)\n      totals: yes       # stats for all threads merged together\n      threads: no       # per thread stats\n      #null-values: yes  # print counters that have value 0. Default: no\n\n  # a line based alerts log similar to fast.log into syslog\n  - syslog:\n      enabled: no\n      # reported identity to syslog. If omitted the program name (usually\n      # suricata) will be used.\n      #identity: \"suricata\"\n      facility: local5\n      #level: Info ## possible levels: Emergency, Alert, Critical,\n                   ## Error, Warning, Notice, Info, Debug\n\n  # Output module for storing files on disk. Files are stored in\n  # directory names consisting of the first 2 characters of the\n  # SHA256 of the file. Each file is given its SHA256 as a filename.\n  #\n  # When a duplicate file is found, the timestamps on the existing file\n  # are updated.\n  #\n  # Unlike the older filestore, metadata is not written by default\n  # as each file should already have a \"fileinfo\" record in the\n  # eve-log. If write-fileinfo is set to yes, then each file will have\n  # one more associated .json files that consist of the fileinfo\n  # record. A fileinfo file will be written for each occurrence of the\n  # file seen using a filename suffix to ensure uniqueness.\n  #\n  # To prune the filestore directory see the \"suricatactl filestore\n  # prune\" command which can delete files over a certain age.\n  - file-store:\n      version: 2\n      enabled: no\n\n      # Set the directory for the filestore. Relative pathnames\n      # are contained within the \"default-log-dir\".\n      #dir: filestore\n\n      # Write out a fileinfo record for each occurrence of a file.\n      # Disabled by default as each occurrence is already logged\n      # as a fileinfo record to the main eve-log.\n      #write-fileinfo: yes\n\n      # Force storing of all files. Default: no.\n      #force-filestore: yes\n\n      # Override the global stream-depth for sessions in which we want\n      # to perform file extraction. Set to 0 for unlimited; otherwise,\n      # must be greater than the global stream-depth value to be used.\n      #stream-depth: 0\n\n      # Uncomment the following variable to define how many files can\n      # remain open for filestore by Suricata. Default value is 0 which\n      # means files get closed after each write to the file.\n      #max-open-files: 1000\n\n      # Force logging of checksums: available hash functions are md5,\n      # sha1 and sha256. Note that SHA256 is automatically forced by\n      # the use of this output module as it uses the SHA256 as the\n      # file naming scheme.\n      #force-hash: [sha1, md5]\n      # NOTE: X-Forwarded configuration is ignored if write-fileinfo is disabled\n      # HTTP X-Forwarded-For support by adding an extra field or overwriting\n      # the source or destination IP address (depending on flow direction)\n      # with the one reported in the X-Forwarded-For HTTP header. This is\n      # helpful when reviewing alerts for traffic that is being reverse\n      # or forward proxied.\n      xff:\n        enabled: no\n        # Two operation modes are available, \"extra-data\" and \"overwrite\".\n        mode: extra-data\n        # Two proxy deployments are supported, \"reverse\" and \"forward\". In\n        # a \"reverse\" deployment the IP address used is the last one, in a\n        # \"forward\" deployment the first IP address is used.\n        deployment: reverse\n        # Header name where the actual IP address will be reported. If more\n        # than one IP address is present, the last IP address will be the\n        # one taken into consideration.\n        header: X-Forwarded-For\n\n  # Log TCP data after stream normalization\n  # Two types: file or dir:\n  #     - file logs into a single logfile.\n  #     - dir creates 2 files per TCP session and stores the raw TCP\n  #            data into them.\n  # Use 'both' to enable both file and dir modes.\n  #\n  # Note: limited by \"stream.reassembly.depth\"\n  - tcp-data:\n      enabled: no\n      type: file\n      filename: tcp-data.log\n\n  # Log HTTP body data after normalization, de-chunking and unzipping.\n  # Two types: file or dir.\n  #     - file logs into a single logfile.\n  #     - dir creates 2 files per HTTP session and stores the\n  #           normalized data into them.\n  # Use 'both' to enable both file and dir modes.\n  #\n  # Note: limited by the body limit settings\n  - http-body-data:\n      enabled: no\n      type: file\n      filename: http-data.log\n\n  # Lua Output Support - execute lua script to generate alert and event\n  # output.\n  # Documented at:\n  # https://suricata.readthedocs.io/en/latest/output/lua-output.html\n  - lua:\n      enabled: no\n      #scripts-dir: /etc/suricata/lua-output/\n      scripts:\n      #   - script1.lua\n\n# Logging configuration.  This is not about logging IDS alerts/events, but\n# output about what Suricata is doing, like startup messages, errors, etc.\nlogging:\n  # The default log level: can be overridden in an output section.\n  # Note that debug level logging will only be emitted if Suricata was\n  # compiled with the --enable-debug configure option.\n  #\n  # This value is overridden by the SC_LOG_LEVEL env var.\n  default-log-level: notice\n\n  # The default output format.  Optional parameter, should default to\n  # something reasonable if not provided.  Can be overridden in an\n  # output section.  You can leave this out to get the default.\n  #\n  # This value is overridden by the SC_LOG_FORMAT env var.\n  #default-log-format: \"[%i] %t - (%f:%l) <%d> (%n) -- \"\n\n  # A regex to filter output.  Can be overridden in an output section.\n  # Defaults to empty (no filter).\n  #\n  # This value is overridden by the SC_LOG_OP_FILTER env var.\n  default-output-filter:\n\n  # Requires libunwind to be available when Suricata is configured and built.\n  # If a signal unexpectedly terminates Suricata, displays a brief diagnostic\n  # message with the offending stacktrace if enabled.\n  #stacktrace-on-signal: on\n\n  # Define your logging outputs.  If none are defined, or they are all\n  # disabled you will get the default: console output.\n  outputs:\n  - console:\n      enabled: yes\n      # type: json\n  - file:\n      enabled: yes\n      level: info\n      filename: suricata.log\n      # type: json\n  - syslog:\n      enabled: no\n      facility: local5\n      format: \"[%i] <%d> -- \"\n      # type: json\n\n\n##\n## Step 3: Configure common capture settings\n##\n## See \"Advanced Capture Options\" below for more options, including Netmap\n## and PF_RING.\n##\n\n# Linux high speed capture support\naf-packet:\n  - interface: eth0\n    # Number of receive threads. \"auto\" uses the number of cores\n    #threads: auto\n    # Default clusterid. AF_PACKET will load balance packets based on flow.\n    cluster-id: 99\n    # Default AF_PACKET cluster type. AF_PACKET can load balance per flow or per hash.\n    # This is only supported for Linux kernel > 3.1\n    # possible value are:\n    #  * cluster_flow: all packets of a given flow are sent to the same socket\n    #  * cluster_cpu: all packets treated in kernel by a CPU are sent to the same socket\n    #  * cluster_qm: all packets linked by network card to a RSS queue are sent to the same\n    #  socket. Requires at least Linux 3.14.\n    #  * cluster_ebpf: eBPF file load balancing. See doc/userguide/capture-hardware/ebpf-xdp.rst for\n    #  more info.\n    # Recommended modes are cluster_flow on most boxes and cluster_cpu or cluster_qm on system\n    # with capture card using RSS (requires cpu affinity tuning and system IRQ tuning)\n    cluster-type: cluster_flow\n    # In some fragmentation cases, the hash can not be computed. If \"defrag\" is set\n    # to yes, the kernel will do the needed defragmentation before sending the packets.\n    defrag: yes\n    # To use the ring feature of AF_PACKET, set 'use-mmap' to yes\n    #use-mmap: yes\n    # Lock memory map to avoid it being swapped. Be careful that over\n    # subscribing could lock your system\n    #mmap-locked: yes\n    # Use tpacket_v3 capture mode, only active if use-mmap is true\n    # Don't use it in IPS or TAP mode as it causes severe latency\n    #tpacket-v3: yes\n    # Ring size will be computed with respect to \"max-pending-packets\" and number\n    # of threads. You can set manually the ring size in number of packets by setting\n    # the following value. If you are using flow \"cluster-type\" and have really network\n    # intensive single-flow you may want to set the \"ring-size\" independently of the number\n    # of threads:\n    #ring-size: 2048\n    # Block size is used by tpacket_v3 only. It should set to a value high enough to contain\n    # a decent number of packets. Size is in bytes so please consider your MTU. It should be\n    # a power of 2 and it must be multiple of page size (usually 4096).\n    #block-size: 32768\n    # tpacket_v3 block timeout: an open block is passed to userspace if it is not\n    # filled after block-timeout milliseconds.\n    #block-timeout: 10\n    # On busy systems, set it to yes to help recover from a packet drop\n    # phase. This will result in some packets (at max a ring flush) not being inspected.\n    #use-emergency-flush: yes\n    # recv buffer size, increased value could improve performance\n    # buffer-size: 32768\n    # Set to yes to disable promiscuous mode\n    # disable-promisc: no\n    # Choose checksum verification mode for the interface. At the moment\n    # of the capture, some packets may have an invalid checksum due to\n    # the checksum computation being offloaded to the network card.\n    # Possible values are:\n    #  - kernel: use indication sent by kernel for each packet (default)\n    #  - yes: checksum validation is forced\n    #  - no: checksum validation is disabled\n    #  - auto: Suricata uses a statistical approach to detect when\n    #  checksum off-loading is used.\n    # Warning: 'capture.checksum-validation' must be set to yes to have any validation\n    #checksum-checks: kernel\n    # BPF filter to apply to this interface. The pcap filter syntax applies here.\n    #bpf-filter: port 80 or udp\n    # You can use the following variables to activate AF_PACKET tap or IPS mode.\n    # If copy-mode is set to ips or tap, the traffic coming to the current\n    # interface will be copied to the copy-iface interface. If 'tap' is set, the\n    # copy is complete. If 'ips' is set, the packet matching a 'drop' action\n    # will not be copied.\n    #copy-mode: ips\n    #copy-iface: eth1\n    #  For eBPF and XDP setup including bypass, filter and load balancing, please\n    #  see doc/userguide/capture-hardware/ebpf-xdp.rst for more info.\n\n  # Put default values here. These will be used for an interface that is not\n  # in the list above.\n  - interface: default\n    #threads: auto\n    #use-mmap: no\n    #tpacket-v3: yes\n\n# Cross platform libpcap capture support\npcap:\n  - interface: eth0\n    # On Linux, pcap will try to use mmap'ed capture and will use \"buffer-size\"\n    # as total memory used by the ring. So set this to something bigger\n    # than 1% of your bandwidth.\n    #buffer-size: 16777216\n    #bpf-filter: \"tcp and port 25\"\n    # Choose checksum verification mode for the interface. At the moment\n    # of the capture, some packets may have an invalid checksum due to\n    # the checksum computation being offloaded to the network card.\n    # Possible values are:\n    #  - yes: checksum validation is forced\n    #  - no: checksum validation is disabled\n    #  - auto: Suricata uses a statistical approach to detect when\n    #  checksum off-loading is used. (default)\n    # Warning: 'capture.checksum-validation' must be set to yes to have any validation\n    #checksum-checks: auto\n    # With some accelerator cards using a modified libpcap (like Myricom), you\n    # may want to have the same number of capture threads as the number of capture\n    # rings. In this case, set up the threads variable to N to start N threads\n    # listening on the same interface.\n    #threads: 16\n    # set to no to disable promiscuous mode:\n    #promisc: no\n    # set snaplen, if not set it defaults to MTU if MTU can be known\n    # via ioctl call and to full capture if not.\n    #snaplen: 1518\n  # Put default values here\n  - interface: default\n    #checksum-checks: auto\n\n# Settings for reading pcap files\npcap-file:\n  # Possible values are:\n  #  - yes: checksum validation is forced\n  #  - no: checksum validation is disabled\n  #  - auto: Suricata uses a statistical approach to detect when\n  #  checksum off-loading is used. (default)\n  # Warning: 'checksum-validation' must be set to yes to have checksum tested\n  checksum-checks: auto\n\n# See \"Advanced Capture Options\" below for more options, including Netmap\n# and PF_RING.\n\n\n##\n## Step 4: App Layer Protocol configuration\n##\n\n# Configure the app-layer parsers.\n#\n# The error-policy setting applies to all app-layer parsers. Values can be\n# \"drop-flow\", \"pass-flow\", \"bypass\", \"drop-packet\", \"pass-packet\", \"reject\" or\n# \"ignore\" (the default).\n#\n# The protocol's section details each protocol.\n#\n# The option \"enabled\" takes 3 values - \"yes\", \"no\", \"detection-only\".\n# \"yes\" enables both detection and the parser, \"no\" disables both, and\n# \"detection-only\" enables protocol detection only (parser disabled).\napp-layer:\n  # error-policy: ignore\n  protocols:\n    rfb:\n      enabled: yes\n      detection-ports:\n        dp: 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909\n    # MQTT, disabled by default.\n    mqtt:\n      enabled: yes\n      # max-msg-length: 1mb\n      # subscribe-topic-match-limit: 100\n      # unsubscribe-topic-match-limit: 100\n      # Maximum number of live MQTT transactions per flow\n      # max-tx: 4096\n    krb5:\n      enabled: yes\n    snmp:\n      enabled: yes\n    ikev2:\n      enabled: yes\n    tls:\n      enabled: yes\n      detection-ports:\n        dp: 443\n\n      # Generate JA3 fingerprint from client hello. If not specified it\n      # will be disabled by default, but enabled if rules require it.\n      #ja3-fingerprints: auto\n\n      # What to do when the encrypted communications start:\n      # - default: keep tracking TLS session, check for protocol anomalies,\n      #            inspect tls_* keywords. Disables inspection of unmodified\n      #            'content' signatures.\n      # - bypass:  stop processing this flow as much as possible. No further\n      #            TLS parsing and inspection. Offload flow bypass to kernel\n      #            or hardware if possible.\n      # - full:    keep tracking and inspection as normal. Unmodified content\n      #            keyword signatures are inspected as well.\n      #\n      # For best performance, select 'bypass'.\n      #\n      #encryption-handling: default\n\n    dcerpc:\n      enabled: yes\n    ftp:\n      enabled: yes\n      # memcap: 64mb\n    rdp:\n      enabled: yes\n    ssh:\n      enabled: yes\n      #hassh: yes\n    # HTTP2: Experimental HTTP 2 support. Disabled by default.\n    http2:\n      enabled: no\n      # use http keywords on HTTP2 traffic\n      http1-rules: no\n    smtp:\n      enabled: yes\n      raw-extraction: no\n      # Configure SMTP-MIME Decoder\n      mime:\n        # Decode MIME messages from SMTP transactions\n        # (may be resource intensive)\n        # This field supersedes all others because it turns the entire\n        # process on or off\n        decode-mime: yes\n\n        # Decode MIME entity bodies (ie. Base64, quoted-printable, etc.)\n        decode-base64: yes\n        decode-quoted-printable: yes\n\n        # Maximum bytes per header data value stored in the data structure\n        # (default is 2000)\n        header-value-depth: 2000\n\n        # Extract URLs and save in state data structure\n        extract-urls: yes\n        # Set to yes to compute the md5 of the mail body. You will then\n        # be able to journalize it.\n        body-md5: no\n      # Configure inspected-tracker for file_data keyword\n      inspected-tracker:\n        content-limit: 100000\n        content-inspect-min-size: 32768\n        content-inspect-window: 4096\n    imap:\n      enabled: detection-only\n    smb:\n      enabled: yes\n      detection-ports:\n        dp: 139, 445\n\n      # Stream reassembly size for SMB streams. By default track it completely.\n      #stream-depth: 0\n\n    nfs:\n      enabled: yes\n    tftp:\n      enabled: yes\n    dns:\n      tcp:\n        enabled: yes\n        detection-ports:\n          dp: 53\n      udp:\n        enabled: yes\n        detection-ports:\n          dp: 53\n    http:\n      enabled: yes\n      # memcap:                   Maximum memory capacity for HTTP\n      #                           Default is unlimited, values can be 64mb, e.g.\n\n      # default-config:           Used when no server-config matches\n      #   personality:            List of personalities used by default\n      #   request-body-limit:     Limit reassembly of request body for inspection\n      #                           by http_client_body & pcre /P option.\n      #   response-body-limit:    Limit reassembly of response body for inspection\n      #                           by file_data, http_server_body & pcre /Q option.\n      #\n      #   For advanced options, see the user guide\n\n\n      # server-config:            List of server configurations to use if address matches\n      #   address:                List of IP addresses or networks for this block\n      #   personality:            List of personalities used by this block\n      #\n      #                           Then, all the fields from default-config can be overloaded\n      #\n      # Currently Available Personalities:\n      #   Minimal, Generic, IDS (default), IIS_4_0, IIS_5_0, IIS_5_1, IIS_6_0,\n      #   IIS_7_0, IIS_7_5, Apache_2\n      libhtp:\n         default-config:\n           personality: IDS\n\n           # Can be specified in kb, mb, gb.  Just a number indicates\n           # it's in bytes.\n           request-body-limit: 100kb\n           response-body-limit: 100kb\n\n           # inspection limits\n           request-body-minimal-inspect-size: 32kb\n           request-body-inspect-window: 4kb\n           response-body-minimal-inspect-size: 40kb\n           response-body-inspect-window: 16kb\n\n           # response body decompression (0 disables)\n           response-body-decompress-layer-limit: 2\n\n           # auto will use http-body-inline mode in IPS mode, yes or no set it statically\n           http-body-inline: auto\n\n           # Decompress SWF files.\n           # Two types: 'deflate', 'lzma', 'both' will decompress deflate and lzma\n           # compress-depth:\n           # Specifies the maximum amount of data to decompress,\n           # set 0 for unlimited.\n           # decompress-depth:\n           # Specifies the maximum amount of decompressed data to obtain,\n           # set 0 for unlimited.\n           swf-decompression:\n             enabled: yes\n             type: both\n             compress-depth: 100kb\n             decompress-depth: 100kb\n\n           # Use a random value for inspection sizes around the specified value.\n           # This lowers the risk of some evasion techniques but could lead\n           # to detection change between runs. It is set to 'yes' by default.\n           #randomize-inspection-sizes: yes\n           # If \"randomize-inspection-sizes\" is active, the value of various\n           # inspection size will be chosen from the [1 - range%, 1 + range%]\n           # range\n           # Default value of \"randomize-inspection-range\" is 10.\n           #randomize-inspection-range: 10\n\n           # decoding\n           double-decode-path: no\n           double-decode-query: no\n\n           # Can enable LZMA decompression\n           #lzma-enabled: false\n           # Memory limit usage for LZMA decompression dictionary\n           # Data is decompressed until dictionary reaches this size\n           #lzma-memlimit: 1mb\n           # Maximum decompressed size with a compression ratio\n           # above 2048 (only LZMA can reach this ratio, deflate cannot)\n           #compression-bomb-limit: 1mb\n           # Maximum time spent decompressing a single transaction in usec\n           #decompression-time-limit: 100000\n\n         server-config:\n\n           #- apache:\n           #    address: [192.168.1.0/24, 127.0.0.0/8, \"::1\"]\n           #    personality: Apache_2\n           #    # Can be specified in kb, mb, gb.  Just a number indicates\n           #    # it's in bytes.\n           #    request-body-limit: 4096\n           #    response-body-limit: 4096\n           #    double-decode-path: no\n           #    double-decode-query: no\n\n           #- iis7:\n           #    address:\n           #      - 192.168.0.0/24\n           #      - 192.168.10.0/24\n           #    personality: IIS_7_0\n           #    # Can be specified in kb, mb, gb.  Just a number indicates\n           #    # it's in bytes.\n           #    request-body-limit: 4096\n           #    response-body-limit: 4096\n           #    double-decode-path: no\n           #    double-decode-query: no\n\n    # Note: Modbus probe parser is minimalist due to the limited usage in the field.\n    # Only Modbus message length (greater than Modbus header length)\n    # and protocol ID (equal to 0) are checked in probing parser\n    # It is important to enable detection port and define Modbus port\n    # to avoid false positives\n    modbus:\n      # How many unanswered Modbus requests are considered a flood.\n      # If the limit is reached, the app-layer-event:modbus.flooded; will match.\n      #request-flood: 500\n\n      enabled: no\n      detection-ports:\n        dp: 502\n      # According to MODBUS Messaging on TCP/IP Implementation Guide V1.0b, it\n      # is recommended to keep the TCP connection opened with a remote device\n      # and not to open and close it for each MODBUS/TCP transaction. In that\n      # case, it is important to set the depth of the stream reassembling as\n      # unlimited (stream.reassembly.depth: 0)\n\n      # Stream reassembly size for modbus. By default track it completely.\n      stream-depth: 0\n\n    # DNP3\n    dnp3:\n      enabled: no\n      detection-ports:\n        dp: 20000\n\n    # SCADA EtherNet/IP and CIP protocol support\n    enip:\n      enabled: no\n      detection-ports:\n        dp: 44818\n        sp: 44818\n\n    ntp:\n      enabled: yes\n\n    dhcp:\n      enabled: yes\n\n    sip:\n      enabled: yes\n\n# Limit for the maximum number of asn1 frames to decode (default 256)\nasn1-max-frames: 256\n\n# Datasets default settings\n# datasets:\n#   # Default fallback memcap and hashsize values for datasets in case these\n#   # were not explicitly defined.\n#   defaults:\n#     memcap: 100mb\n#     hashsize: 2048\n#\n#  rules:\n#    # Set to true to allow absolute filenames and filenames that use\n#    # \"..\" components to reference parent directories in rules that specify\n#    # their filenames.\n#    #allow-absolute-filenames: false\n#\n#    # Allow datasets in rules write access for \"save\" and\n#    # \"state\". This is enabled by default, however write access is\n#    # limited to the data directory.\n#    #allow-write: true\n\n##############################################################################\n##\n## Advanced settings below\n##\n##############################################################################\n\n##\n## Run Options\n##\n\n# Run Suricata with a specific user-id and group-id:\n#run-as:\n#  user: suri\n#  group: suri\n\nsecurity:\n  lua:\n    # Allow Lua rules. Disabled by default.\n    #allow-rules: false\n\n# Some logging modules will use that name in event as identifier. The default\n# value is the hostname\n#sensor-name: suricata\n\n# Default location of the pid file. The pid file is only used in\n# daemon mode (start Suricata with -D). If not running in daemon mode\n# the --pidfile command line option must be used to create a pid file.\n#pid-file: @e_rundir@suricata.pid\n\n# Daemon working directory\n# Suricata will change directory to this one if provided\n# Default: \"/\"\n#daemon-directory: \"/\"\n\n# Umask.\n# Suricata will use this umask if it is provided. By default it will use the\n# umask passed on by the shell.\n#umask: 022\n\n# Suricata core dump configuration. Limits the size of the core dump file to\n# approximately max-dump. The actual core dump size will be a multiple of the\n# page size. Core dumps that would be larger than max-dump are truncated. On\n# Linux, the actual core dump size may be a few pages larger than max-dump.\n# Setting max-dump to 0 disables core dumping.\n# Setting max-dump to 'unlimited' will give the full core dump file.\n# On 32-bit Linux, a max-dump value >= ULONG_MAX may cause the core dump size\n# to be 'unlimited'.\n\ncoredump:\n  max-dump: unlimited\n\n# If the Suricata box is a router for the sniffed networks, set it to 'router'. If\n# it is a pure sniffing setup, set it to 'sniffer-only'.\n# If set to auto, the variable is internally switched to 'router' in IPS mode\n# and 'sniffer-only' in IDS mode.\n# This feature is currently only used by the reject* keywords.\nhost-mode: auto\n\n# Number of packets preallocated per thread. The default is 1024. A higher number \n# will make sure each CPU will be more easily kept busy, but may negatively \n# impact caching.\n#max-pending-packets: 1024\n\n# Runmode the engine should use. Please check --list-runmodes to get the available\n# runmodes for each packet acquisition method. Default depends on selected capture\n# method. 'workers' generally gives best performance.\n#runmode: autofp\n\n# Specifies the kind of flow load balancer used by the flow pinned autofp mode.\n#\n# Supported schedulers are:\n#\n# hash     - Flow assigned to threads using the 5-7 tuple hash.\n# ippair   - Flow assigned to threads using addresses only.\n#\n#autofp-scheduler: hash\n\n# Preallocated size for each packet. Default is 1514 which is the classical\n# size for pcap on Ethernet. You should adjust this value to the highest\n# packet size (MTU + hardware header) on your system.\n#default-packet-size: 1514\n\n# Unix command socket that can be used to pass commands to Suricata.\n# An external tool can then connect to get information from Suricata\n# or trigger some modifications of the engine. Set enabled to yes\n# to activate the feature. In auto mode, the feature will only be\n# activated in live capture mode. You can use the filename variable to set\n# the file name of the socket.\nunix-command:\n  enabled: auto\n  #filename: custom.socket\n\n# Magic file. The extension .mgc is added to the value here.\n#magic-file: /usr/share/file/magic\n@e_magic_file_comment@magic-file: @e_magic_file@\n\n# GeoIP2 database file. Specify path and filename of GeoIP2 database\n# if using rules with \"geoip\" rule option.\n#geoip-database: /usr/local/share/GeoLite2/GeoLite2-Country.mmdb\n\nlegacy:\n  uricontent: enabled\n\n##\n## Detection settings\n##\n\n# Set the order of alerts based on actions\n# The default order is pass, drop, reject, alert\n# action-order:\n#   - pass\n#   - drop\n#   - reject\n#   - alert\n\n# Define maximum number of possible alerts that can be triggered for the same\n# packet. Default is 15\n#packet-alert-max: 15\n\n# IP Reputation\n#reputation-categories-file: @e_sysconfdir@iprep/categories.txt\n#default-reputation-path: @e_sysconfdir@iprep\n#reputation-files:\n# - reputation.list\n\n# When run with the option --engine-analysis, the engine will read each of\n# the parameters below, and print reports for each of the enabled sections\n# and exit.  The reports are printed to a file in the default log dir\n# given by the parameter \"default-log-dir\", with engine reporting\n# subsection below printing reports in its own report file.\nengine-analysis:\n  # enables printing reports for fast-pattern for every rule.\n  rules-fast-pattern: yes\n  # enables printing reports for each rule\n  rules: yes\n\n#recursion and match limits for PCRE where supported\npcre:\n  match-limit: 3500\n  match-limit-recursion: 1500\n\n##\n## Advanced Traffic Tracking and Reconstruction Settings\n##\n\n# Host specific policies for defragmentation and TCP stream\n# reassembly. The host OS lookup is done using a radix tree, just\n# like a routing table so the most specific entry matches.\nhost-os-policy:\n  # Make the default policy windows.\n  windows: [0.0.0.0/0]\n  bsd: []\n  bsd-right: []\n  old-linux: []\n  linux: []\n  old-solaris: []\n  solaris: []\n  hpux10: []\n  hpux11: []\n  irix: []\n  macos: []\n  vista: []\n  windows2k3: []\n\n# Defrag settings:\n\n# The memcap-policy value can be \"drop-packet\", \"pass-packet\", \"reject\" or\n# \"ignore\" (which is the default).\ndefrag:\n  memcap: 32mb\n  # memcap-policy: ignore\n  hash-size: 65536\n  trackers: 65535 # number of defragmented flows to follow\n  max-frags: 65535 # number of fragments to keep (higher than trackers)\n  prealloc: yes\n  timeout: 60\n\n# Enable defrag per host settings\n#  host-config:\n#\n#    - dmz:\n#        timeout: 30\n#        address: [192.168.1.0/24, 127.0.0.0/8, 1.1.1.0/24, 2.2.2.0/24, \"1.1.1.1\", \"2.2.2.2\", \"::1\"]\n#\n#    - lan:\n#        timeout: 45\n#        address:\n#          - 192.168.0.0/24\n#          - 192.168.10.0/24\n#          - 172.16.14.0/24\n\n# Flow settings:\n# By default, the reserved memory (memcap) for flows is 32MB. This is the limit\n# for flow allocation inside the engine. You can change this value to allow\n# more memory usage for flows.\n# The hash-size determines the size of the hash used to identify flows inside\n# the engine, and by default the value is 65536.\n# At startup, the engine can preallocate a number of flows, to get better\n# performance. The number of flows preallocated is 10000 by default.\n# emergency-recovery is the percentage of flows that the engine needs to\n# prune before clearing the emergency state. The emergency state is activated\n# when the memcap limit is reached, allowing new flows to be created, but\n# pruning them with the emergency timeouts (they are defined below).\n# If the memcap is reached, the engine will try to prune flows\n# with the default timeouts. If it doesn't find a flow to prune, it will set\n# the emergency bit and it will try again with more aggressive timeouts.\n# If that doesn't work, then it will try to kill the oldest flows using\n# last time seen flows.\n# The memcap can be specified in kb, mb, gb.  Just a number indicates it's\n# in bytes.\n# The memcap-policy can be \"drop-packet\", \"pass-packet\", \"reject\" or \"ignore\"\n# (which is the default).\n\nflow:\n  memcap: 128mb\n  #memcap-policy: ignore\n  hash-size: 65536\n  prealloc: 10000\n  emergency-recovery: 30\n  #managers: 1 # default to one flow manager\n  #recyclers: 1 # default to one flow recycler thread\n\n# This option controls the use of VLAN ids in the flow (and defrag)\n# hashing. Normally this should be enabled, but in some (broken)\n# setups where both sides of a flow are not tagged with the same VLAN\n# tag, we can ignore the VLAN id's in the flow hashing.\nvlan:\n  use-for-tracking: true\n\n# Specific timeouts for flows. Here you can specify the timeouts that the\n# active flows will wait to transit from the current state to another, on each\n# protocol. The value of \"new\" determines the seconds to wait after a handshake or\n# stream startup before the engine frees the data of that flow it doesn't\n# change the state to established (usually if we don't receive more packets\n# of that flow). The value of \"established\" is the amount of\n# seconds that the engine will wait to free the flow if that time elapses\n# without receiving new packets or closing the connection. \"closed\" is the\n# amount of time to wait after a flow is closed (usually zero). \"bypassed\"\n# timeout controls locally bypassed flows. For these flows we don't do any other\n# tracking. If no packets have been seen after this timeout, the flow is discarded.\n#\n# There's an emergency mode that will become active under attack circumstances,\n# making the engine to check flow status faster. This configuration variables\n# use the prefix \"emergency-\" and work similar as the normal ones.\n# Some timeouts doesn't apply to all the protocols, like \"closed\", for udp and\n# icmp.\n\nflow-timeouts:\n\n  default:\n    new: 30\n    established: 300\n    closed: 0\n    bypassed: 100\n    emergency-new: 10\n    emergency-established: 100\n    emergency-closed: 0\n    emergency-bypassed: 50\n  tcp:\n    new: 60\n    established: 600\n    closed: 60\n    bypassed: 100\n    emergency-new: 5\n    emergency-established: 100\n    emergency-closed: 10\n    emergency-bypassed: 50\n  udp:\n    new: 30\n    established: 300\n    bypassed: 100\n    emergency-new: 10\n    emergency-established: 100\n    emergency-bypassed: 50\n  icmp:\n    new: 30\n    established: 300\n    bypassed: 100\n    emergency-new: 10\n    emergency-established: 100\n    emergency-bypassed: 50\n\n# Stream engine settings. Here the TCP stream tracking and reassembly\n# engine is configured.\n#\n# stream:\n#   memcap: 64mb                # Can be specified in kb, mb, gb.  Just a\n#                               # number indicates it's in bytes.\n#   memcap-policy: ignore       # Can be \"drop-flow\", \"pass-flow\", \"bypass\",\n#                               # \"drop-packet\", \"pass-packet\", \"reject\" or\n#                               # \"ignore\" default is \"ignore\"\n#   checksum-validation: yes    # To validate the checksum of received\n#                               # packet. If csum validation is specified as\n#                               # \"yes\", then packets with invalid csum values will not\n#                               # be processed by the engine stream/app layer.\n#                               # Warning: locally generated traffic can be\n#                               # generated without checksum due to hardware offload\n#                               # of checksum. You can control the handling of checksum\n#                               # on a per-interface basis via the 'checksum-checks'\n#                               # option\n#   prealloc-sessions: 2k       # 2k sessions prealloc'd per stream thread\n#   midstream: false            # don't allow midstream session pickups\n#   midstream-policy: ignore    # Can be \"drop-flow\", \"pass-flow\", \"bypass\",\n#                               # \"drop-packet\", \"pass-packet\", \"reject\" or\n#                               # \"ignore\" default is \"ignore\"\n#   async-oneside: false        # don't enable async stream handling\n#   inline: no                  # stream inline mode\n#   drop-invalid: yes           # in inline mode, drop packets that are invalid with regards to streaming engine\n#   max-synack-queued: 5        # Max different SYN/ACKs to queue\n#   bypass: no                  # Bypass packets when stream.reassembly.depth is reached.\n#                               # Warning: first side to reach this triggers\n#                               # the bypass.\n#   liberal-timestamps: false   # Treat all timestamps as if the Linux policy applies. This\n#                               # means it's slightly more permissive. Enabled by default.\n#\n#   reassembly:\n#     memcap: 256mb             # Can be specified in kb, mb, gb.  Just a number\n#                               # indicates it's in bytes.\n#     memcap-policy: ignore     # Can be \"drop-flow\", \"pass-flow\", \"bypass\",\n#                               # \"drop-packet\", \"pass-packet\", \"reject\" or\n#                               # \"ignore\" default is \"ignore\"\n#     depth: 1mb                # Can be specified in kb, mb, gb.  Just a number\n#                               # indicates it's in bytes.\n#     toserver-chunk-size: 2560 # inspect raw stream in chunks of at least\n#                               # this size.  Can be specified in kb, mb,\n#                               # gb.  Just a number indicates it's in bytes.\n#     toclient-chunk-size: 2560 # inspect raw stream in chunks of at least\n#                               # this size.  Can be specified in kb, mb,\n#                               # gb.  Just a number indicates it's in bytes.\n#     randomize-chunk-size: yes # Take a random value for chunk size around the specified value.\n#                               # This lowers the risk of some evasion techniques but could lead\n#                               # to detection change between runs. It is set to 'yes' by default.\n#     randomize-chunk-range: 10 # If randomize-chunk-size is active, the value of chunk-size is\n#                               # a random value between (1 - randomize-chunk-range/100)*toserver-chunk-size\n#                               # and (1 + randomize-chunk-range/100)*toserver-chunk-size and the same\n#                               # calculation for toclient-chunk-size.\n#                               # Default value of randomize-chunk-range is 10.\n#\n#     raw: yes                  # 'Raw' reassembly enabled or disabled.\n#                               # raw is for content inspection by detection\n#                               # engine.\n#\n#     segment-prealloc: 2048    # number of segments preallocated per thread\n#\n#     check-overlap-different-data: true|false\n#                               # check if a segment contains different data\n#                               # than what we've already seen for that\n#                               # position in the stream.\n#                               # This is enabled automatically if inline mode\n#                               # is used or when stream-event:reassembly_overlap_different_data;\n#                               # is used in a rule.\n#\nstream:\n  memcap: 64mb\n  #memcap-policy: ignore\n  checksum-validation: yes      # reject incorrect csums\n  #midstream: false\n  #midstream-policy: ignore\n  inline: auto                  # auto will use inline mode in IPS mode, yes or no set it statically\n  reassembly:\n    memcap: 256mb\n    #memcap-policy: ignore\n    depth: 1mb                  # reassemble 1mb into a stream\n    toserver-chunk-size: 2560\n    toclient-chunk-size: 2560\n    randomize-chunk-size: yes\n    #randomize-chunk-range: 10\n    #raw: yes\n    #segment-prealloc: 2048\n    #check-overlap-different-data: true\n\n# Host table:\n#\n# Host table is used by the tagging and per host thresholding subsystems.\n#\nhost:\n  hash-size: 4096\n  prealloc: 1000\n  memcap: 32mb\n\n# IP Pair table:\n#\n# Used by xbits 'ippair' tracking.\n#\n#ippair:\n#  hash-size: 4096\n#  prealloc: 1000\n#  memcap: 32mb\n\n# Decoder settings\n\ndecoder:\n  # Teredo decoder is known to not be completely accurate\n  # as it will sometimes detect non-teredo as teredo.\n  teredo:\n    enabled: true\n    # ports to look for Teredo. Max 4 ports. If no ports are given, or\n    # the value is set to 'any', Teredo detection runs on _all_ UDP packets.\n    ports: $TEREDO_PORTS # syntax: '[3544, 1234]' or '3533' or 'any'.\n\n  # VXLAN decoder is assigned to up to 4 UDP ports. By default only the\n  # IANA assigned port 4789 is enabled.\n  vxlan:\n    enabled: true\n    ports: $VXLAN_PORTS # syntax: '[8472, 4789]' or '4789'.\n\n  # VNTag decode support\n  vntag:\n    enabled: false\n\n  # Geneve decoder is assigned to up to 4 UDP ports. By default only the\n  # IANA assigned port 6081 is enabled.\n  geneve:\n    enabled: true\n    ports: $GENEVE_PORTS # syntax: '[6081, 1234]' or '6081'.\n\n  # maximum number of decoder layers for a packet\n  # max-layers: 16\n\n##\n## Performance tuning and profiling\n##\n\n# The detection engine builds internal groups of signatures. The engine\n# allows us to specify the profile to use for them, to manage memory in an\n# efficient way keeping good performance. For the profile keyword you\n# can use the words \"low\", \"medium\", \"high\" or \"custom\". If you use custom,\n# make sure to define the values in the \"custom-values\" section.\n# Usually you would prefer medium/high/low.\n#\n# \"sgh mpm-context\", indicates how the staging should allot mpm contexts for\n# the signature groups.  \"single\" indicates the use of a single context for\n# all the signature group heads.  \"full\" indicates a mpm-context for each\n# group head.  \"auto\" lets the engine decide the distribution of contexts\n# based on the information the engine gathers on the patterns from each\n# group head.\n#\n# The option inspection-recursion-limit is used to limit the recursive calls\n# in the content inspection code.  For certain payload-sig combinations, we\n# might end up taking too much time in the content inspection code.\n# If the argument specified is 0, the engine uses an internally defined\n# default limit.  When a value is not specified, there are no limits on the recursion.\ndetect:\n  profile: medium\n  custom-values:\n    toclient-groups: 3\n    toserver-groups: 25\n  sgh-mpm-context: auto\n  inspection-recursion-limit: 3000\n  # If set to yes, the loading of signatures will be made after the capture\n  # is started. This will limit the downtime in IPS mode.\n  #delayed-detect: yes\n\n  prefilter:\n    # default prefiltering setting. \"mpm\" only creates MPM/fast_pattern\n    # engines. \"auto\" also sets up prefilter engines for other keywords.\n    # Use --list-keywords=all to see which keywords support prefiltering.\n    default: mpm\n\n  # the grouping values above control how many groups are created per\n  # direction. Port whitelisting forces that port to get its own group.\n  # Very common ports will benefit, as well as ports with many expensive\n  # rules.\n  grouping:\n    #tcp-whitelist: 53, 80, 139, 443, 445, 1433, 3306, 3389, 6666, 6667, 8080\n    #udp-whitelist: 53, 135, 5060\n\n  profiling:\n    # Log the rules that made it past the prefilter stage, per packet\n    # default is off. The threshold setting determines how many rules\n    # must have made it past pre-filter for that rule to trigger the\n    # logging.\n    #inspect-logging-threshold: 200\n    grouping:\n      dump-to-disk: false\n      include-rules: false      # very verbose\n      include-mpm-stats: false\n\n# Select the multi pattern algorithm you want to run for scan/search the\n# in the engine.\n#\n# The supported algorithms are:\n# \"ac\"      - Aho-Corasick, default implementation\n# \"ac-bs\"   - Aho-Corasick, reduced memory implementation\n# \"ac-ks\"   - Aho-Corasick, \"Ken Steele\" variant\n# \"hs\"      - Hyperscan, available when built with Hyperscan support\n#\n# The default mpm-algo value of \"auto\" will use \"hs\" if Hyperscan is\n# available, \"ac\" otherwise.\n#\n# The mpm you choose also decides the distribution of mpm contexts for\n# signature groups, specified by the conf - \"detect.sgh-mpm-context\".\n# Selecting \"ac\" as the mpm would require \"detect.sgh-mpm-context\"\n# to be set to \"single\", because of ac's memory requirements, unless the\n# ruleset is small enough to fit in memory, in which case one can\n# use \"full\" with \"ac\".  The rest of the mpms can be run in \"full\" mode.\n\nmpm-algo: auto\n\n# Select the matching algorithm you want to use for single-pattern searches.\n#\n# Supported algorithms are \"bm\" (Boyer-Moore) and \"hs\" (Hyperscan, only\n# available if Suricata has been built with Hyperscan support).\n#\n# The default of \"auto\" will use \"hs\" if available, otherwise \"bm\".\n\nspm-algo: auto\n\n# Suricata is multi-threaded. Here the threading can be influenced.\nthreading:\n  set-cpu-affinity: no\n  # Tune cpu affinity of threads. Each family of threads can be bound\n  # to specific CPUs.\n  #\n  # These 2 apply to the all runmodes:\n  # management-cpu-set is used for flow timeout handling, counters\n  # worker-cpu-set is used for 'worker' threads\n  #\n  # Additionally, for autofp these apply:\n  # receive-cpu-set is used for capture threads\n  # verdict-cpu-set is used for IPS verdict threads\n  #\n  cpu-affinity:\n    - management-cpu-set:\n        cpu: [ 0 ]  # include only these CPUs in affinity settings\n    - receive-cpu-set:\n        cpu: [ 0 ]  # include only these CPUs in affinity settings\n    - worker-cpu-set:\n        cpu: [ \"all\" ]\n        mode: \"exclusive\"\n        # Use explicitly 3 threads and don't compute number by using\n        # detect-thread-ratio variable:\n        # threads: 3\n        prio:\n          low: [ 0 ]\n          medium: [ \"1-2\" ]\n          high: [ 3 ]\n          default: \"medium\"\n    #- verdict-cpu-set:\n    #    cpu: [ 0 ]\n    #    prio:\n    #      default: \"high\"\n  #\n  # By default Suricata creates one \"detect\" thread per available CPU/CPU core.\n  # This setting allows controlling this behaviour. A ratio setting of 2 will\n  # create 2 detect threads for each CPU/CPU core. So for a dual core CPU this\n  # will result in 4 detect threads. If values below 1 are used, less threads\n  # are created. So on a dual core CPU a setting of 0.5 results in 1 detect\n  # thread being created. Regardless of the setting at a minimum 1 detect\n  # thread will always be created.\n  #\n  detect-thread-ratio: 1.0\n  #\n  # By default, the per-thread stack size is left to its default setting. If\n  # the default thread stack size is too small, use the following configuration\n  # setting to change the size. Note that if any thread's stack size cannot be\n  # set to this value, a fatal error occurs.\n  #\n  # Generally, the per-thread stack-size should not exceed 8MB.\n  #stack-size: 8mb\n\n# Luajit has a strange memory requirement, its 'states' need to be in the\n# first 2G of the process' memory.\n#\n# 'luajit.states' is used to control how many states are preallocated.\n# State use: per detect script: 1 per detect thread. Per output script: 1 per\n# script.\nluajit:\n  states: 128\n\n# Profiling settings. Only effective if Suricata has been built with\n# the --enable-profiling configure flag.\n#\nprofiling:\n  # Run profiling for every X-th packet. The default is 1, which means we\n  # profile every packet. If set to 1000, one packet is profiled for every\n  # 1000 received.\n  #sample-rate: 1000\n\n  # rule profiling\n  rules:\n\n    # Profiling can be disabled here, but it will still have a\n    # performance impact if compiled in.\n    enabled: yes\n    filename: rule_perf.log\n    append: yes\n\n    # Sort options: ticks, avgticks, checks, matches, maxticks\n    # If commented out all the sort options will be used.\n    #sort: avgticks\n\n    # Limit the number of sids for which stats are shown at exit (per sort).\n    limit: 10\n\n    # output to json\n    json: @e_enable_evelog@\n\n  # per keyword profiling\n  keywords:\n    enabled: yes\n    filename: keyword_perf.log\n    append: yes\n\n  prefilter:\n    enabled: yes\n    filename: prefilter_perf.log\n    append: yes\n\n  # per rulegroup profiling\n  rulegroups:\n    enabled: yes\n    filename: rule_group_perf.log\n    append: yes\n\n  # packet profiling\n  packets:\n\n    # Profiling can be disabled here, but it will still have a\n    # performance impact if compiled in.\n    enabled: yes\n    filename: packet_stats.log\n    append: yes\n\n    # per packet csv output\n    csv:\n\n      # Output can be disabled here, but it will still have a\n      # performance impact if compiled in.\n      enabled: no\n      filename: packet_stats.csv\n\n  # profiling of locking. Only available when Suricata was built with\n  # --enable-profiling-locks.\n  locks:\n    enabled: no\n    filename: lock_stats.log\n    append: yes\n\n  pcap-log:\n    enabled: no\n    filename: pcaplog_stats.log\n    append: yes\n\n##\n## Netfilter integration\n##\n\n# When running in NFQ inline mode, it is possible to use a simulated\n# non-terminal NFQUEUE verdict.\n# This permits sending all needed packet to Suricata via this rule:\n#        iptables -I FORWARD -m mark ! --mark $MARK/$MASK -j NFQUEUE\n# And below, you can have your standard filtering ruleset. To activate\n# this mode, you need to set mode to 'repeat'\n# If you want a packet to be sent to another queue after an ACCEPT decision\n# set the mode to 'route' and set next-queue value.\n# On Linux >= 3.1, you can set batchcount to a value > 1 to improve performance\n# by processing several packets before sending a verdict (worker runmode only).\n# On Linux >= 3.6, you can set the fail-open option to yes to have the kernel\n# accept the packet if Suricata is not able to keep pace.\n# bypass mark and mask can be used to implement NFQ bypass. If bypass mark is\n# set then the NFQ bypass is activated. Suricata will set the bypass mark/mask\n# on packet of a flow that need to be bypassed. The Nefilter ruleset has to\n# directly accept all packets of a flow once a packet has been marked.\nnfq:\n#  mode: accept\n#  repeat-mark: 1\n#  repeat-mask: 1\n#  bypass-mark: 1\n#  bypass-mask: 1\n#  route-queue: 2\n#  batchcount: 20\n#  fail-open: yes\n\n#nflog support\nnflog:\n    # netlink multicast group\n    # (the same as the iptables --nflog-group param)\n    # Group 0 is used by the kernel, so you can't use it\n  - group: 2\n    # netlink buffer size\n    buffer-size: 18432\n    # put default value here\n  - group: default\n    # set number of packets to queue inside kernel\n    qthreshold: 1\n    # set the delay before flushing packet in the kernel's queue\n    qtimeout: 100\n    # netlink max buffer size\n    max-size: 20000\n\n##\n## Advanced Capture Options\n##\n\n# General settings affecting packet capture\ncapture:\n  # disable NIC offloading. It's restored when Suricata exits.\n  # Enabled by default.\n  #disable-offloading: false\n  #\n  # disable checksum validation. Same as setting '-k none' on the\n  # commandline.\n  #checksum-validation: none\n\n# Netmap support\n#\n# Netmap operates with NIC directly in driver, so you need FreeBSD 11+ which has\n# built-in Netmap support or compile and install the Netmap module and appropriate\n# NIC driver for your Linux system.\n# To reach maximum throughput disable all receive-, segmentation-,\n# checksum- offloading on your NIC (using ethtool or similar).\n# Disabling TX checksum offloading is *required* for connecting OS endpoint\n# with NIC endpoint.\n# You can find more information at https://github.com/luigirizzo/netmap\n#\nnetmap:\n   # To specify OS endpoint add plus sign at the end (e.g. \"eth0+\")\n - interface: eth2\n   # Number of capture threads. \"auto\" uses number of RSS queues on interface.\n   # Warning: unless the RSS hashing is symmetrical, this will lead to\n   # accuracy issues.\n   #threads: auto\n   # You can use the following variables to activate netmap tap or IPS mode.\n   # If copy-mode is set to ips or tap, the traffic coming to the current\n   # interface will be copied to the copy-iface interface. If 'tap' is set, the\n   # copy is complete. If 'ips' is set, the packet matching a 'drop' action\n   # will not be copied.\n   # To specify the OS as the copy-iface (so the OS can route packets, or forward\n   # to a service running on the same machine) add a plus sign at the end\n   # (e.g. \"copy-iface: eth0+\"). Don't forget to set up a symmetrical eth0+ -> eth0\n   # for return packets. Hardware checksumming must be *off* on the interface if\n   # using an OS endpoint (e.g. 'ifconfig eth0 -rxcsum -txcsum -rxcsum6 -txcsum6' for FreeBSD\n   # or 'ethtool -K eth0 tx off rx off' for Linux).\n   #copy-mode: tap\n   #copy-iface: eth3\n   # Set to yes to disable promiscuous mode\n   # disable-promisc: no\n   # Choose checksum verification mode for the interface. At the moment\n   # of the capture, some packets may have an invalid checksum due to\n   # the checksum computation being offloaded to the network card.\n   # Possible values are:\n   #  - yes: checksum validation is forced\n   #  - no: checksum validation is disabled\n   #  - auto: Suricata uses a statistical approach to detect when\n   #  checksum off-loading is used.\n   # Warning: 'checksum-validation' must be set to yes to have any validation\n   #checksum-checks: auto\n   # BPF filter to apply to this interface. The pcap filter syntax apply here.\n   #bpf-filter: port 80 or udp\n #- interface: eth3\n   #threads: auto\n   #copy-mode: tap\n   #copy-iface: eth2\n   # Put default values here\n - interface: default\n\n# PF_RING configuration: for use with native PF_RING support\n# for more info see http://www.ntop.org/products/pf_ring/\npfring:\n  - interface: eth0\n    # Number of receive threads. If set to 'auto' Suricata will first try\n    # to use CPU (core) count and otherwise RSS queue count.\n    threads: auto\n\n    # Default clusterid.  PF_RING will load balance packets based on flow.\n    # All threads/processes that will participate need to have the same\n    # clusterid.\n    cluster-id: 99\n\n    # Default PF_RING cluster type. PF_RING can load balance per flow.\n    # Possible values are:\n    # - cluster_flow:               6-tuple: <src ip, src_port, dst ip, dst port, proto, vlan>\n    # - cluster_inner_flow:         6-tuple: <src ip, src port, dst ip, dst port, proto, vlan>\n    # - cluster_inner_flow_2_tuple: 2-tuple: <src ip,           dst ip                       >\n    # - cluster_inner_flow_4_tuple: 4-tuple: <src ip, src port, dst ip, dst port             >\n    # - cluster_inner_flow_5_tuple: 5-tuple: <src ip, src port, dst ip, dst port, proto      >\n    # - cluster_round_robin (NOT RECOMMENDED)\n    cluster-type: cluster_flow\n\n    # bpf filter for this interface\n    #bpf-filter: tcp\n\n    # If bypass is set then the PF_RING hw bypass is activated, when supported\n    # by the network interface. Suricata will instruct the interface to bypass\n    # all future packets for a flow that need to be bypassed.\n    #bypass: yes\n\n    # Choose checksum verification mode for the interface. At the moment\n    # of the capture, some packets may have an invalid checksum due to\n    # the checksum computation being offloaded to the network card.\n    # Possible values are:\n    #  - rxonly: only compute checksum for packets received by network card.\n    #  - yes: checksum validation is forced\n    #  - no: checksum validation is disabled\n    #  - auto: Suricata uses a statistical approach to detect when\n    #  checksum off-loading is used. (default)\n    # Warning: 'checksum-validation' must be set to yes to have any validation\n    #checksum-checks: auto\n  # Second interface\n  #- interface: eth1\n  #  threads: 3\n  #  cluster-id: 93\n  #  cluster-type: cluster_flow\n  # Put default values here\n  - interface: default\n    #threads: 2\n\n# For FreeBSD ipfw(8) divert(4) support.\n# Please make sure you have ipfw_load=\"YES\" and ipdivert_load=\"YES\"\n# in /etc/loader.conf or kldload'ing the appropriate kernel modules.\n# Additionally, you need to have an ipfw rule for the engine to see\n# the packets from ipfw.  For Example:\n#\n#   ipfw add 100 divert 8000 ip from any to any\n#\n# N.B. This example uses \"8000\" -- this number must mach the values\n# you passed on the command line, i.e., -d 8000\n#\nipfw:\n\n  # Reinject packets at the specified ipfw rule number.  This config\n  # option is the ipfw rule number AT WHICH rule processing continues\n  # in the ipfw processing system after the engine has finished\n  # inspecting the packet for acceptance.  If no rule number is specified,\n  # accepted packets are reinjected at the divert rule which they entered\n  # and IPFW rule processing continues.  No check is done to verify\n  # this will rule makes sense so care must be taken to avoid loops in ipfw.\n  #\n  ## The following example tells the engine to reinject packets\n  # back into the ipfw firewall AT rule number 5500:\n  #\n  # ipfw-reinjection-rule-number: 5500\n\n\nnapatech:\n    # When use_all_streams is set to \"yes\" the initialization code will query\n    # the Napatech service for all configured streams and listen on all of them.\n    # When set to \"no\" the streams config array will be used.\n    #\n    # This option necessitates running the appropriate NTPL commands to create\n    # the desired streams prior to running Suricata.\n    #use-all-streams: no\n\n    # The streams to listen on when auto-config is disabled or when and threading\n    # cpu-affinity is disabled.  This can be either:\n    #   an individual stream (e.g. streams: [0])\n    # or\n    #   a range of streams (e.g. streams: [\"0-3\"])\n    #\n    streams: [\"0-3\"]\n\n    # Stream stats can be enabled to provide fine grain packet and byte counters\n    # for each thread/stream that is configured.\n    #\n    enable-stream-stats: no\n\n    # When auto-config is enabled the streams will be created and assigned\n    # automatically to the NUMA node where the thread resides.  If cpu-affinity\n    # is enabled in the threading section.  Then the streams will be created\n    # according to the number of worker threads specified in the worker-cpu-set.\n    # Otherwise, the streams array is used to define the streams.\n    #\n    # This option is intended primarily to support legacy configurations.\n    #\n    # This option cannot be used simultaneously with either \"use-all-streams\"\n    # or \"hardware-bypass\".\n    #\n    auto-config: yes\n\n    # Enable hardware level flow bypass.\n    #\n    hardware-bypass: yes\n\n    # Enable inline operation.  When enabled traffic arriving on a given port is\n    # automatically forwarded out its peer port after analysis by Suricata.\n    #\n    inline: no\n\n    # Ports indicates which Napatech ports are to be used in auto-config mode.\n    # these are the port IDs of the ports that will be merged prior to the\n    # traffic being distributed to the streams.\n    #\n    # When hardware-bypass is enabled the ports must be configured as a segment.\n    # specify the port(s) on which upstream and downstream traffic will arrive.\n    # This information is necessary for the hardware to properly process flows.\n    #\n    # When using a tap configuration one of the ports will receive inbound traffic\n    # for the network and the other will receive outbound traffic. The two ports on a\n    # given segment must reside on the same network adapter.\n    #\n    # When using a SPAN-port configuration the upstream and downstream traffic\n    # arrives on a single port. This is configured by setting the two sides of the\n    # segment to reference the same port.  (e.g. 0-0 to configure a SPAN port on\n    # port 0).\n    #\n    # port segments are specified in the form:\n    #    ports: [0-1,2-3,4-5,6-6,7-7]\n    #\n    # For legacy systems when hardware-bypass is disabled this can be specified in any\n    # of the following ways:\n    #\n    #   a list of individual ports (e.g. ports: [0,1,2,3])\n    #\n    #   a range of ports (e.g. ports: [0-3])\n    #\n    #   \"all\" to indicate that all ports are to be merged together\n    #   (e.g. ports: [all])\n    #\n    # This parameter has no effect if auto-config is disabled.\n    #\n    ports: [0-1,2-3]\n\n    # When auto-config is enabled the hashmode specifies the algorithm for\n    # determining to which stream a given packet is to be delivered.\n    # This can be any valid Napatech NTPL hashmode command.\n    #\n    # The most common hashmode commands are:  hash2tuple, hash2tuplesorted,\n    # hash5tuple, hash5tuplesorted and roundrobin.\n    #\n    # See Napatech NTPL documentation other hashmodes and details on their use.\n    #\n    # This parameter has no effect if auto-config is disabled.\n    #\n    hashmode: hash5tuplesorted\n\n##\n## Configure Suricata to load Suricata-Update managed rules.\n##\n\ndefault-rule-path: @e_defaultruledir@\n\nrule-files:\n  - suricata.rules\n\n##\n## Auxiliary configuration files.\n##\n\nclassification-file: @e_sysconfdir@classification.config\nreference-config-file: @e_sysconfdir@reference.config\n# threshold-file: @e_sysconfdir@threshold.config\n\n##\n## Include other configs\n##\n\n# Includes:  Files included here will be handled as if they were in-lined\n# in this configuration file. Files with relative pathnames will be\n# searched for in the same directory as this configuration file. You may\n# use absolute pathnames too.\n# You can specify more than 2 configuration files, if needed.\n#include: include1.yaml\n#include: include2.yaml\n"], "filenames": ["src/detect-dataset.c", "suricata.yaml.in"], "buggy_code_start_loc": [304, 1006], "buggy_code_end_loc": [304, 1006], "fixing_code_start_loc": [305, 1007], "fixing_code_end_loc": [314, 1012], "type": "CWE-22", "message": "In Suricata before 6.0.13 (when there is an adversary who controls an external source of rules), a dataset filename, that comes from a rule, may trigger absolute or relative directory traversal, and lead to write access to a local filesystem. This is addressed in 6.0.13 by requiring allow-absolute-filenames and allow-write (in the datasets rules configuration section) if an installation requires traversal/writing in this situation.", "other": {"cve": {"id": "CVE-2023-35852", "sourceIdentifier": "cve@mitre.org", "published": "2023-06-19T04:15:11.217", "lastModified": "2023-06-28T18:44:55.743", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "In Suricata before 6.0.13 (when there is an adversary who controls an external source of rules), a dataset filename, that comes from a rule, may trigger absolute or relative directory traversal, and lead to write access to a local filesystem. This is addressed in 6.0.13 by requiring allow-absolute-filenames and allow-write (in the datasets rules configuration section) if an installation requires traversal/writing in this situation."}, {"lang": "es", "value": "En Suricata antes de la versi\u00f3n 6.0.13 (cuando hay un adversario que controla una fuente externa de reglas), un nombre de archivo de conjunto de datos, que proviene de una regla, puede desencadenar el salto de directorios absolutos o relativos, y conducir al acceso de escritura a un sistema de archivos local. Esto se soluciona en 6.0.13 requiriendo \"allow-absolute-filenames\" y \"allow-write\" (en la secci\u00f3n de configuraci\u00f3n de reglas de conjuntos de datos) si una instalaci\u00f3n requiere saltar/escribir en esta situaci\u00f3n. "}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-22"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:oisf:suricata:*:*:*:*:*:*:*:*", "versionEndExcluding": "6.0.13", "matchCriteriaId": "1FDDE2F7-D633-4FBC-8EE1-6145A82AC02F"}]}]}], "references": [{"url": "https://github.com/OISF/suricata/commit/735f5aa9ca3b28cfacc7a443f93a44387fbacf17", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://github.com/OISF/suricata/commit/aee1523b4591430ebed1ded0bb95508e6717a335", "source": "cve@mitre.org", "tags": ["Patch"]}, {"url": "https://github.com/OISF/suricata/compare/suricata-6.0.12...suricata-6.0.13", "source": "cve@mitre.org", "tags": ["Vendor Advisory"]}, {"url": "https://www.stamus-networks.com/stamus-labs", "source": "cve@mitre.org", "tags": ["Not Applicable"]}]}, "github_commit_url": "https://github.com/OISF/suricata/commit/735f5aa9ca3b28cfacc7a443f93a44387fbacf17"}}