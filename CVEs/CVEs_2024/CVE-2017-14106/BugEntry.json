{"buggy_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tImplementation of the Transmission Control Protocol(TCP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tCharles Hedrick, <hedrick@klinzhai.rutgers.edu>\n *\t\tLinus Torvalds, <torvalds@cs.helsinki.fi>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tMatthew Dillon, <dillon@apollo.west.oic.com>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tNumerous verify_area() calls\n *\t\tAlan Cox\t:\tSet the ACK bit on a reset\n *\t\tAlan Cox\t:\tStopped it crashing if it closed while\n *\t\t\t\t\tsk->inuse=1 and was trying to connect\n *\t\t\t\t\t(tcp_err()).\n *\t\tAlan Cox\t:\tAll icmp error handling was broken\n *\t\t\t\t\tpointers passed where wrong and the\n *\t\t\t\t\tsocket was looked up backwards. Nobody\n *\t\t\t\t\ttested any icmp error code obviously.\n *\t\tAlan Cox\t:\ttcp_err() now handled properly. It\n *\t\t\t\t\twakes people on errors. poll\n *\t\t\t\t\tbehaves and the icmp error race\n *\t\t\t\t\thas gone by moving it into sock.c\n *\t\tAlan Cox\t:\ttcp_send_reset() fixed to work for\n *\t\t\t\t\teverything not just packets for\n *\t\t\t\t\tunknown sockets.\n *\t\tAlan Cox\t:\ttcp option processing.\n *\t\tAlan Cox\t:\tReset tweaked (still not 100%) [Had\n *\t\t\t\t\tsyn rule wrong]\n *\t\tHerp Rosmanith  :\tMore reset fixes\n *\t\tAlan Cox\t:\tNo longer acks invalid rst frames.\n *\t\t\t\t\tAcking any kind of RST is right out.\n *\t\tAlan Cox\t:\tSets an ignore me flag on an rst\n *\t\t\t\t\treceive otherwise odd bits of prattle\n *\t\t\t\t\tescape still\n *\t\tAlan Cox\t:\tFixed another acking RST frame bug.\n *\t\t\t\t\tShould stop LAN workplace lockups.\n *\t\tAlan Cox\t: \tSome tidyups using the new skb list\n *\t\t\t\t\tfacilities\n *\t\tAlan Cox\t:\tsk->keepopen now seems to work\n *\t\tAlan Cox\t:\tPulls options out correctly on accepts\n *\t\tAlan Cox\t:\tFixed assorted sk->rqueue->next errors\n *\t\tAlan Cox\t:\tPSH doesn't end a TCP read. Switched a\n *\t\t\t\t\tbit to skb ops.\n *\t\tAlan Cox\t:\tTidied tcp_data to avoid a potential\n *\t\t\t\t\tnasty.\n *\t\tAlan Cox\t:\tAdded some better commenting, as the\n *\t\t\t\t\ttcp is hard to follow\n *\t\tAlan Cox\t:\tRemoved incorrect check for 20 * psh\n *\tMichael O'Reilly\t:\tack < copied bug fix.\n *\tJohannes Stille\t\t:\tMisc tcp fixes (not all in yet).\n *\t\tAlan Cox\t:\tFIN with no memory -> CRASH\n *\t\tAlan Cox\t:\tAdded socket option proto entries.\n *\t\t\t\t\tAlso added awareness of them to accept.\n *\t\tAlan Cox\t:\tAdded TCP options (SOL_TCP)\n *\t\tAlan Cox\t:\tSwitched wakeup calls to callbacks,\n *\t\t\t\t\tso the kernel can layer network\n *\t\t\t\t\tsockets.\n *\t\tAlan Cox\t:\tUse ip_tos/ip_ttl settings.\n *\t\tAlan Cox\t:\tHandle FIN (more) properly (we hope).\n *\t\tAlan Cox\t:\tRST frames sent on unsynchronised\n *\t\t\t\t\tstate ack error.\n *\t\tAlan Cox\t:\tPut in missing check for SYN bit.\n *\t\tAlan Cox\t:\tAdded tcp_select_window() aka NET2E\n *\t\t\t\t\twindow non shrink trick.\n *\t\tAlan Cox\t:\tAdded a couple of small NET2E timer\n *\t\t\t\t\tfixes\n *\t\tCharles Hedrick :\tTCP fixes\n *\t\tToomas Tamm\t:\tTCP window fixes\n *\t\tAlan Cox\t:\tSmall URG fix to rlogin ^C ack fight\n *\t\tCharles Hedrick\t:\tRewrote most of it to actually work\n *\t\tLinus\t\t:\tRewrote tcp_read() and URG handling\n *\t\t\t\t\tcompletely\n *\t\tGerhard Koerting:\tFixed some missing timer handling\n *\t\tMatthew Dillon  :\tReworked TCP machine states as per RFC\n *\t\tGerhard Koerting:\tPC/TCP workarounds\n *\t\tAdam Caldwell\t:\tAssorted timer/timing errors\n *\t\tMatthew Dillon\t:\tFixed another RST bug\n *\t\tAlan Cox\t:\tMove to kernel side addressing changes.\n *\t\tAlan Cox\t:\tBeginning work on TCP fastpathing\n *\t\t\t\t\t(not yet usable)\n *\t\tArnt Gulbrandsen:\tTurbocharged tcp_check() routine.\n *\t\tAlan Cox\t:\tTCP fast path debugging\n *\t\tAlan Cox\t:\tWindow clamping\n *\t\tMichael Riepe\t:\tBug in tcp_check()\n *\t\tMatt Dillon\t:\tMore TCP improvements and RST bug fixes\n *\t\tMatt Dillon\t:\tYet more small nasties remove from the\n *\t\t\t\t\tTCP code (Be very nice to this man if\n *\t\t\t\t\ttcp finally works 100%) 8)\n *\t\tAlan Cox\t:\tBSD accept semantics.\n *\t\tAlan Cox\t:\tReset on closedown bug.\n *\tPeter De Schrijver\t:\tENOTCONN check missing in tcp_sendto().\n *\t\tMichael Pall\t:\tHandle poll() after URG properly in\n *\t\t\t\t\tall cases.\n *\t\tMichael Pall\t:\tUndo the last fix in tcp_read_urg()\n *\t\t\t\t\t(multi URG PUSH broke rlogin).\n *\t\tMichael Pall\t:\tFix the multi URG PUSH problem in\n *\t\t\t\t\ttcp_readable(), poll() after URG\n *\t\t\t\t\tworks now.\n *\t\tMichael Pall\t:\trecv(...,MSG_OOB) never blocks in the\n *\t\t\t\t\tBSD api.\n *\t\tAlan Cox\t:\tChanged the semantics of sk->socket to\n *\t\t\t\t\tfix a race and a signal problem with\n *\t\t\t\t\taccept() and async I/O.\n *\t\tAlan Cox\t:\tRelaxed the rules on tcp_sendto().\n *\t\tYury Shevchuk\t:\tReally fixed accept() blocking problem.\n *\t\tCraig I. Hagan  :\tAllow for BSD compatible TIME_WAIT for\n *\t\t\t\t\tclients/servers which listen in on\n *\t\t\t\t\tfixed ports.\n *\t\tAlan Cox\t:\tCleaned the above up and shrank it to\n *\t\t\t\t\ta sensible code size.\n *\t\tAlan Cox\t:\tSelf connect lockup fix.\n *\t\tAlan Cox\t:\tNo connect to multicast.\n *\t\tRoss Biro\t:\tClose unaccepted children on master\n *\t\t\t\t\tsocket close.\n *\t\tAlan Cox\t:\tReset tracing code.\n *\t\tAlan Cox\t:\tSpurious resets on shutdown.\n *\t\tAlan Cox\t:\tGiant 15 minute/60 second timer error\n *\t\tAlan Cox\t:\tSmall whoops in polling before an\n *\t\t\t\t\taccept.\n *\t\tAlan Cox\t:\tKept the state trace facility since\n *\t\t\t\t\tit's handy for debugging.\n *\t\tAlan Cox\t:\tMore reset handler fixes.\n *\t\tAlan Cox\t:\tStarted rewriting the code based on\n *\t\t\t\t\tthe RFC's for other useful protocol\n *\t\t\t\t\treferences see: Comer, KA9Q NOS, and\n *\t\t\t\t\tfor a reference on the difference\n *\t\t\t\t\tbetween specifications and how BSD\n *\t\t\t\t\tworks see the 4.4lite source.\n *\t\tA.N.Kuznetsov\t:\tDon't time wait on completion of tidy\n *\t\t\t\t\tclose.\n *\t\tLinus Torvalds\t:\tFin/Shutdown & copied_seq changes.\n *\t\tLinus Torvalds\t:\tFixed BSD port reuse to work first syn\n *\t\tAlan Cox\t:\tReimplemented timers as per the RFC\n *\t\t\t\t\tand using multiple timers for sanity.\n *\t\tAlan Cox\t:\tSmall bug fixes, and a lot of new\n *\t\t\t\t\tcomments.\n *\t\tAlan Cox\t:\tFixed dual reader crash by locking\n *\t\t\t\t\tthe buffers (much like datagram.c)\n *\t\tAlan Cox\t:\tFixed stuck sockets in probe. A probe\n *\t\t\t\t\tnow gets fed up of retrying without\n *\t\t\t\t\t(even a no space) answer.\n *\t\tAlan Cox\t:\tExtracted closing code better\n *\t\tAlan Cox\t:\tFixed the closing state machine to\n *\t\t\t\t\tresemble the RFC.\n *\t\tAlan Cox\t:\tMore 'per spec' fixes.\n *\t\tJorge Cwik\t:\tEven faster checksumming.\n *\t\tAlan Cox\t:\ttcp_data() doesn't ack illegal PSH\n *\t\t\t\t\tonly frames. At least one pc tcp stack\n *\t\t\t\t\tgenerates them.\n *\t\tAlan Cox\t:\tCache last socket.\n *\t\tAlan Cox\t:\tPer route irtt.\n *\t\tMatt Day\t:\tpoll()->select() match BSD precisely on error\n *\t\tAlan Cox\t:\tNew buffers\n *\t\tMarc Tamsky\t:\tVarious sk->prot->retransmits and\n *\t\t\t\t\tsk->retransmits misupdating fixed.\n *\t\t\t\t\tFixed tcp_write_timeout: stuck close,\n *\t\t\t\t\tand TCP syn retries gets used now.\n *\t\tMark Yarvis\t:\tIn tcp_read_wakeup(), don't send an\n *\t\t\t\t\tack if state is TCP_CLOSED.\n *\t\tAlan Cox\t:\tLook up device on a retransmit - routes may\n *\t\t\t\t\tchange. Doesn't yet cope with MSS shrink right\n *\t\t\t\t\tbut it's a start!\n *\t\tMarc Tamsky\t:\tClosing in closing fixes.\n *\t\tMike Shaver\t:\tRFC1122 verifications.\n *\t\tAlan Cox\t:\trcv_saddr errors.\n *\t\tAlan Cox\t:\tBlock double connect().\n *\t\tAlan Cox\t:\tSmall hooks for enSKIP.\n *\t\tAlexey Kuznetsov:\tPath MTU discovery.\n *\t\tAlan Cox\t:\tSupport soft errors.\n *\t\tAlan Cox\t:\tFix MTU discovery pathological case\n *\t\t\t\t\twhen the remote claims no mtu!\n *\t\tMarc Tamsky\t:\tTCP_CLOSE fix.\n *\t\tColin (G3TNE)\t:\tSend a reset on syn ack replies in\n *\t\t\t\t\twindow but wrong (fixes NT lpd problems)\n *\t\tPedro Roque\t:\tBetter TCP window handling, delayed ack.\n *\t\tJoerg Reuter\t:\tNo modification of locked buffers in\n *\t\t\t\t\ttcp_do_retransmit()\n *\t\tEric Schenk\t:\tChanged receiver side silly window\n *\t\t\t\t\tavoidance algorithm to BSD style\n *\t\t\t\t\talgorithm. This doubles throughput\n *\t\t\t\t\tagainst machines running Solaris,\n *\t\t\t\t\tand seems to result in general\n *\t\t\t\t\timprovement.\n *\tStefan Magdalinski\t:\tadjusted tcp_readable() to fix FIONREAD\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\tMike McLagan\t\t:\tRouting by source\n *\t\tKeith Owens\t:\tDo proper merging with partial SKB's in\n *\t\t\t\t\ttcp_do_sendmsg to avoid burstiness.\n *\t\tEric Schenk\t:\tFix fast close down bug with\n *\t\t\t\t\tshutdown() followed by close().\n *\t\tAndi Kleen \t:\tMake poll agree with SIGIO\n *\tSalvatore Sanfilippo\t:\tSupport SO_LINGER with linger == 1 and\n *\t\t\t\t\tlingertime == 0 (RFC 793 ABORT Call)\n *\tHirokazu Takahashi\t:\tUse copy_from_user() instead of\n *\t\t\t\t\tcsum_and_copy_from_user() if possible.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or(at your option) any later version.\n *\n * Description of States:\n *\n *\tTCP_SYN_SENT\t\tsent a connection request, waiting for ack\n *\n *\tTCP_SYN_RECV\t\treceived a connection request, sent ack,\n *\t\t\t\twaiting for final ack in three-way handshake.\n *\n *\tTCP_ESTABLISHED\t\tconnection established\n *\n *\tTCP_FIN_WAIT1\t\tour side has shutdown, waiting to complete\n *\t\t\t\ttransmission of remaining buffered data\n *\n *\tTCP_FIN_WAIT2\t\tall buffered data sent, waiting for remote\n *\t\t\t\tto shutdown\n *\n *\tTCP_CLOSING\t\tboth sides have shutdown but we still have\n *\t\t\t\tdata we have to finish sending\n *\n *\tTCP_TIME_WAIT\t\ttimeout to catch resent junk before entering\n *\t\t\t\tclosed, can only be entered from FIN_WAIT2\n *\t\t\t\tor CLOSING.  Required because the other end\n *\t\t\t\tmay not have gotten our last ACK causing it\n *\t\t\t\tto retransmit the data packet (which we ignore)\n *\n *\tTCP_CLOSE_WAIT\t\tremote side has shutdown and is waiting for\n *\t\t\t\tus to finish writing our data and to shutdown\n *\t\t\t\t(we have to close() to move on to LAST_ACK)\n *\n *\tTCP_LAST_ACK\t\tout side has shutdown after remote has\n *\t\t\t\tshutdown.  There may still be data in our\n *\t\t\t\tbuffer that we have to finish sending\n *\n *\tTCP_CLOSE\t\tsocket is finished\n */\n\n#define pr_fmt(fmt) \"TCP: \" fmt\n\n#include <crypto/hash.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/poll.h>\n#include <linux/inet_diag.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/skbuff.h>\n#include <linux/scatterlist.h>\n#include <linux/splice.h>\n#include <linux/net.h>\n#include <linux/socket.h>\n#include <linux/random.h>\n#include <linux/bootmem.h>\n#include <linux/highmem.h>\n#include <linux/swap.h>\n#include <linux/cache.h>\n#include <linux/err.h>\n#include <linux/time.h>\n#include <linux/slab.h>\n\n#include <net/icmp.h>\n#include <net/inet_common.h>\n#include <net/tcp.h>\n#include <net/xfrm.h>\n#include <net/ip.h>\n#include <net/sock.h>\n\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n#include <net/busy_poll.h>\n\nint sysctl_tcp_min_tso_segs __read_mostly = 2;\n\nint sysctl_tcp_autocorking __read_mostly = 1;\n\nstruct percpu_counter tcp_orphan_count;\nEXPORT_SYMBOL_GPL(tcp_orphan_count);\n\nlong sysctl_tcp_mem[3] __read_mostly;\nint sysctl_tcp_wmem[3] __read_mostly;\nint sysctl_tcp_rmem[3] __read_mostly;\n\nEXPORT_SYMBOL(sysctl_tcp_mem);\nEXPORT_SYMBOL(sysctl_tcp_rmem);\nEXPORT_SYMBOL(sysctl_tcp_wmem);\n\natomic_long_t tcp_memory_allocated;\t/* Current allocated memory. */\nEXPORT_SYMBOL(tcp_memory_allocated);\n\n/*\n * Current number of TCP sockets.\n */\nstruct percpu_counter tcp_sockets_allocated;\nEXPORT_SYMBOL(tcp_sockets_allocated);\n\n/*\n * TCP splice context\n */\nstruct tcp_splice_state {\n\tstruct pipe_inode_info *pipe;\n\tsize_t len;\n\tunsigned int flags;\n};\n\n/*\n * Pressure flag: try to collapse.\n * Technical note: it is used by multiple contexts non atomically.\n * All the __sk_mem_schedule() is of this nature: accounting\n * is strict, actions are advisory and have some latency.\n */\nint tcp_memory_pressure __read_mostly;\nEXPORT_SYMBOL(tcp_memory_pressure);\n\nvoid tcp_enter_memory_pressure(struct sock *sk)\n{\n\tif (!tcp_memory_pressure) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMEMORYPRESSURES);\n\t\ttcp_memory_pressure = 1;\n\t}\n}\nEXPORT_SYMBOL(tcp_enter_memory_pressure);\n\n/* Convert seconds to retransmits based on initial and max timeout */\nstatic u8 secs_to_retrans(int seconds, int timeout, int rto_max)\n{\n\tu8 res = 0;\n\n\tif (seconds > 0) {\n\t\tint period = timeout;\n\n\t\tres = 1;\n\t\twhile (seconds > period && res < 255) {\n\t\t\tres++;\n\t\t\ttimeout <<= 1;\n\t\t\tif (timeout > rto_max)\n\t\t\t\ttimeout = rto_max;\n\t\t\tperiod += timeout;\n\t\t}\n\t}\n\treturn res;\n}\n\n/* Convert retransmits to seconds based on initial and max timeout */\nstatic int retrans_to_secs(u8 retrans, int timeout, int rto_max)\n{\n\tint period = 0;\n\n\tif (retrans > 0) {\n\t\tperiod = timeout;\n\t\twhile (--retrans) {\n\t\t\ttimeout <<= 1;\n\t\t\tif (timeout > rto_max)\n\t\t\t\ttimeout = rto_max;\n\t\t\tperiod += timeout;\n\t\t}\n\t}\n\treturn period;\n}\n\n/* Address-family independent initialization for a tcp_sock.\n *\n * NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nvoid tcp_init_sock(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->out_of_order_queue = RB_ROOT;\n\ttcp_init_xmit_timers(sk);\n\ttcp_prequeue_init(tp);\n\tINIT_LIST_HEAD(&tp->tsq_node);\n\n\ticsk->icsk_rto = TCP_TIMEOUT_INIT;\n\ttp->mdev_us = jiffies_to_usecs(TCP_TIMEOUT_INIT);\n\tminmax_reset(&tp->rtt_min, tcp_time_stamp, ~0U);\n\n\t/* So many TCP implementations out there (incorrectly) count the\n\t * initial SYN frame in their delayed-ACK and congestion control\n\t * algorithms that we must have the following bandaid to talk\n\t * efficiently to them.  -DaveM\n\t */\n\ttp->snd_cwnd = TCP_INIT_CWND;\n\n\t/* There's a bubble in the pipe until at least the first ACK. */\n\ttp->app_limited = ~0U;\n\n\t/* See draft-stevens-tcpca-spec-01 for discussion of the\n\t * initialization of these values.\n\t */\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttp->snd_cwnd_clamp = ~0;\n\ttp->mss_cache = TCP_MSS_DEFAULT;\n\n\ttp->reordering = sock_net(sk)->ipv4.sysctl_tcp_reordering;\n\ttcp_assign_congestion_control(sk);\n\n\ttp->tsoffset = 0;\n\n\tsk->sk_state = TCP_CLOSE;\n\n\tsk->sk_write_space = sk_stream_write_space;\n\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\n\n\ticsk->icsk_sync_mss = tcp_sync_mss;\n\n\tsk->sk_sndbuf = sysctl_tcp_wmem[1];\n\tsk->sk_rcvbuf = sysctl_tcp_rmem[1];\n\n\tsk_sockets_allocated_inc(sk);\n}\nEXPORT_SYMBOL(tcp_init_sock);\n\nstatic void tcp_tx_timestamp(struct sock *sk, u16 tsflags, struct sk_buff *skb)\n{\n\tif (tsflags && skb) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\t\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\t\tsock_tx_timestamp(sk, tsflags, &shinfo->tx_flags);\n\t\tif (tsflags & SOF_TIMESTAMPING_TX_ACK)\n\t\t\ttcb->txstamp_ack = 1;\n\t\tif (tsflags & SOF_TIMESTAMPING_TX_RECORD_MASK)\n\t\t\tshinfo->tskey = TCP_SKB_CB(skb)->seq + skb->len - 1;\n\t}\n}\n\n/*\n *\tWait for a TCP event.\n *\n *\tNote that we don't need to lock the socket, as the upper poll layers\n *\ttake care of normal races (between the test and the event) and we don't\n *\tgo look at any of the socket buffers directly.\n */\nunsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\tunsigned int mask;\n\tstruct sock *sk = sock->sk;\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tint state;\n\n\tsock_rps_record_flow(sk);\n\n\tsock_poll_wait(file, sk_sleep(sk), wait);\n\n\tstate = sk_state_load(sk);\n\tif (state == TCP_LISTEN)\n\t\treturn inet_csk_listen_poll(sk);\n\n\t/* Socket is not locked. We are protected from async events\n\t * by poll logic and correct handling of state changes\n\t * made by other threads is impossible in any case.\n\t */\n\n\tmask = 0;\n\n\t/*\n\t * POLLHUP is certainly not done right. But poll() doesn't\n\t * have a notion of HUP in just one direction, and for a\n\t * socket the read side is more interesting.\n\t *\n\t * Some poll() documentation says that POLLHUP is incompatible\n\t * with the POLLOUT/POLLWR flags, so somebody should check this\n\t * all. But careful, it tends to be safer to return too many\n\t * bits than too few, and you can easily break real applications\n\t * if you don't tell them that something has hung up!\n\t *\n\t * Check-me.\n\t *\n\t * Check number 1. POLLHUP is _UNMASKABLE_ event (see UNIX98 and\n\t * our fs/select.c). It means that after we received EOF,\n\t * poll always returns immediately, making impossible poll() on write()\n\t * in state CLOSE_WAIT. One solution is evident --- to set POLLHUP\n\t * if and only if shutdown has been made in both directions.\n\t * Actually, it is interesting to look how Solaris and DUX\n\t * solve this dilemma. I would prefer, if POLLHUP were maskable,\n\t * then we could set it on SND_SHUTDOWN. BTW examples given\n\t * in Stevens' books assume exactly this behaviour, it explains\n\t * why POLLHUP is incompatible with POLLOUT.\t--ANK\n\t *\n\t * NOTE. Check for TCP_CLOSE is added. The goal is to prevent\n\t * blocking on fresh not-connected or disconnected socket. --ANK\n\t */\n\tif (sk->sk_shutdown == SHUTDOWN_MASK || state == TCP_CLOSE)\n\t\tmask |= POLLHUP;\n\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\tmask |= POLLIN | POLLRDNORM | POLLRDHUP;\n\n\t/* Connected or passive Fast Open socket? */\n\tif (state != TCP_SYN_SENT &&\n\t    (state != TCP_SYN_RECV || tp->fastopen_rsk)) {\n\t\tint target = sock_rcvlowat(sk, 0, INT_MAX);\n\n\t\tif (tp->urg_seq == tp->copied_seq &&\n\t\t    !sock_flag(sk, SOCK_URGINLINE) &&\n\t\t    tp->urg_data)\n\t\t\ttarget++;\n\n\t\tif (tp->rcv_nxt - tp->copied_seq >= target)\n\t\t\tmask |= POLLIN | POLLRDNORM;\n\n\t\tif (!(sk->sk_shutdown & SEND_SHUTDOWN)) {\n\t\t\tif (sk_stream_is_writeable(sk)) {\n\t\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t\t\t} else {  /* send SIGIO later */\n\t\t\t\tsk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\t\t\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\n\t\t\t\t/* Race breaker. If space is freed after\n\t\t\t\t * wspace test but before the flags are set,\n\t\t\t\t * IO signal will be lost. Memory barrier\n\t\t\t\t * pairs with the input side.\n\t\t\t\t */\n\t\t\t\tsmp_mb__after_atomic();\n\t\t\t\tif (sk_stream_is_writeable(sk))\n\t\t\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t\t\t}\n\t\t} else\n\t\t\tmask |= POLLOUT | POLLWRNORM;\n\n\t\tif (tp->urg_data & TCP_URG_VALID)\n\t\t\tmask |= POLLPRI;\n\t} else if (state == TCP_SYN_SENT && inet_sk(sk)->defer_connect) {\n\t\t/* Active TCP fastopen socket with defer_connect\n\t\t * Return POLLOUT so application can call write()\n\t\t * in order for kernel to generate SYN+data\n\t\t */\n\t\tmask |= POLLOUT | POLLWRNORM;\n\t}\n\t/* This barrier is coupled with smp_wmb() in tcp_reset() */\n\tsmp_rmb();\n\tif (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue))\n\t\tmask |= POLLERR;\n\n\treturn mask;\n}\nEXPORT_SYMBOL(tcp_poll);\n\nint tcp_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint answ;\n\tbool slow;\n\n\tswitch (cmd) {\n\tcase SIOCINQ:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tslow = lock_sock_fast(sk);\n\t\tansw = tcp_inq(sk);\n\t\tunlock_sock_fast(sk, slow);\n\t\tbreak;\n\tcase SIOCATMARK:\n\t\tansw = tp->urg_data && tp->urg_seq == tp->copied_seq;\n\t\tbreak;\n\tcase SIOCOUTQ:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))\n\t\t\tansw = 0;\n\t\telse\n\t\t\tansw = tp->write_seq - tp->snd_una;\n\t\tbreak;\n\tcase SIOCOUTQNSD:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))\n\t\t\tansw = 0;\n\t\telse\n\t\t\tansw = tp->write_seq - tp->snd_nxt;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn put_user(answ, (int __user *)arg);\n}\nEXPORT_SYMBOL(tcp_ioctl);\n\nstatic inline void tcp_mark_push(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\ttp->pushed_seq = tp->write_seq;\n}\n\nstatic inline bool forced_push(const struct tcp_sock *tp)\n{\n\treturn after(tp->write_seq, tp->pushed_seq + (tp->max_window >> 1));\n}\n\nstatic void skb_entail(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\tskb->csum    = 0;\n\ttcb->seq     = tcb->end_seq = tp->write_seq;\n\ttcb->tcp_flags = TCPHDR_ACK;\n\ttcb->sacked  = 0;\n\t__skb_header_release(skb);\n\ttcp_add_write_queue_tail(sk, skb);\n\tsk->sk_wmem_queued += skb->truesize;\n\tsk_mem_charge(sk, skb->truesize);\n\tif (tp->nonagle & TCP_NAGLE_PUSH)\n\t\ttp->nonagle &= ~TCP_NAGLE_PUSH;\n\n\ttcp_slow_start_after_idle_check(sk);\n}\n\nstatic inline void tcp_mark_urg(struct tcp_sock *tp, int flags)\n{\n\tif (flags & MSG_OOB)\n\t\ttp->snd_up = tp->write_seq;\n}\n\n/* If a not yet filled skb is pushed, do not send it if\n * we have data packets in Qdisc or NIC queues :\n * Because TX completion will happen shortly, it gives a chance\n * to coalesce future sendmsg() payload into this skb, without\n * need for a timer, and with no latency trade off.\n * As packets containing data payload have a bigger truesize\n * than pure acks (dataless) packets, the last checks prevent\n * autocorking if we only have an ACK in Qdisc/NIC queues,\n * or if TX completion was delayed after we processed ACK packet.\n */\nstatic bool tcp_should_autocork(struct sock *sk, struct sk_buff *skb,\n\t\t\t\tint size_goal)\n{\n\treturn skb->len < size_goal &&\n\t       sysctl_tcp_autocorking &&\n\t       skb != tcp_write_queue_head(sk) &&\n\t       atomic_read(&sk->sk_wmem_alloc) > skb->truesize;\n}\n\nstatic void tcp_push(struct sock *sk, int flags, int mss_now,\n\t\t     int nonagle, int size_goal)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tif (!tcp_send_head(sk))\n\t\treturn;\n\n\tskb = tcp_write_queue_tail(sk);\n\tif (!(flags & MSG_MORE) || forced_push(tp))\n\t\ttcp_mark_push(tp, skb);\n\n\ttcp_mark_urg(tp, flags);\n\n\tif (tcp_should_autocork(sk, skb, size_goal)) {\n\n\t\t/* avoid atomic op if TSQ_THROTTLED bit is already set */\n\t\tif (!test_bit(TSQ_THROTTLED, &sk->sk_tsq_flags)) {\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPAUTOCORKING);\n\t\t\tset_bit(TSQ_THROTTLED, &sk->sk_tsq_flags);\n\t\t}\n\t\t/* It is possible TX completion already happened\n\t\t * before we set TSQ_THROTTLED.\n\t\t */\n\t\tif (atomic_read(&sk->sk_wmem_alloc) > skb->truesize)\n\t\t\treturn;\n\t}\n\n\tif (flags & MSG_MORE)\n\t\tnonagle = TCP_NAGLE_CORK;\n\n\t__tcp_push_pending_frames(sk, mss_now, nonagle);\n}\n\nstatic int tcp_splice_data_recv(read_descriptor_t *rd_desc, struct sk_buff *skb,\n\t\t\t\tunsigned int offset, size_t len)\n{\n\tstruct tcp_splice_state *tss = rd_desc->arg.data;\n\tint ret;\n\n\tret = skb_splice_bits(skb, skb->sk, offset, tss->pipe,\n\t\t\t      min(rd_desc->count, len), tss->flags);\n\tif (ret > 0)\n\t\trd_desc->count -= ret;\n\treturn ret;\n}\n\nstatic int __tcp_splice_read(struct sock *sk, struct tcp_splice_state *tss)\n{\n\t/* Store TCP splice context information in read_descriptor_t. */\n\tread_descriptor_t rd_desc = {\n\t\t.arg.data = tss,\n\t\t.count\t  = tss->len,\n\t};\n\n\treturn tcp_read_sock(sk, &rd_desc, tcp_splice_data_recv);\n}\n\n/**\n *  tcp_splice_read - splice data from TCP socket to a pipe\n * @sock:\tsocket to splice from\n * @ppos:\tposition (not valid)\n * @pipe:\tpipe to splice to\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    Will read pages from given socket and fill them into a pipe.\n *\n **/\nssize_t tcp_splice_read(struct socket *sock, loff_t *ppos,\n\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\tunsigned int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tcp_splice_state tss = {\n\t\t.pipe = pipe,\n\t\t.len = len,\n\t\t.flags = flags,\n\t};\n\tlong timeo;\n\tssize_t spliced;\n\tint ret;\n\n\tsock_rps_record_flow(sk);\n\t/*\n\t * We can't seek on a socket input\n\t */\n\tif (unlikely(*ppos))\n\t\treturn -ESPIPE;\n\n\tret = spliced = 0;\n\n\tlock_sock(sk);\n\n\ttimeo = sock_rcvtimeo(sk, sock->file->f_flags & O_NONBLOCK);\n\twhile (tss.len) {\n\t\tret = __tcp_splice_read(sk, &tss);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\telse if (!ret) {\n\t\t\tif (spliced)\n\t\t\t\tbreak;\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_err) {\n\t\t\t\tret = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\t/*\n\t\t\t\t * This occurs when user tries to read\n\t\t\t\t * from never connected socket.\n\t\t\t\t */\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE))\n\t\t\t\t\tret = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* if __tcp_splice_read() got nothing while we have\n\t\t\t * an skb in receive queue, we do not want to loop.\n\t\t\t * This might happen with URG data.\n\t\t\t */\n\t\t\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\t\t\tbreak;\n\t\t\tsk_wait_data(sk, &timeo, NULL);\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tret = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\ttss.len -= ret;\n\t\tspliced += ret;\n\n\t\tif (!timeo)\n\t\t\tbreak;\n\t\trelease_sock(sk);\n\t\tlock_sock(sk);\n\n\t\tif (sk->sk_err || sk->sk_state == TCP_CLOSE ||\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t    signal_pending(current))\n\t\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\n\tif (spliced)\n\t\treturn spliced;\n\n\treturn ret;\n}\nEXPORT_SYMBOL(tcp_splice_read);\n\nstruct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,\n\t\t\t\t    bool force_schedule)\n{\n\tstruct sk_buff *skb;\n\n\t/* The TCP header must be at least 32-bit aligned.  */\n\tsize = ALIGN(size, 4);\n\n\tif (unlikely(tcp_under_memory_pressure(sk)))\n\t\tsk_mem_reclaim_partial(sk);\n\n\tskb = alloc_skb_fclone(size + sk->sk_prot->max_header, gfp);\n\tif (likely(skb)) {\n\t\tbool mem_scheduled;\n\n\t\tif (force_schedule) {\n\t\t\tmem_scheduled = true;\n\t\t\tsk_forced_mem_schedule(sk, skb->truesize);\n\t\t} else {\n\t\t\tmem_scheduled = sk_wmem_schedule(sk, skb->truesize);\n\t\t}\n\t\tif (likely(mem_scheduled)) {\n\t\t\tskb_reserve(skb, sk->sk_prot->max_header);\n\t\t\t/*\n\t\t\t * Make sure that we have exactly size bytes\n\t\t\t * available to the caller, no more, no less.\n\t\t\t */\n\t\t\tskb->reserved_tailroom = skb->end - skb->tail - size;\n\t\t\treturn skb;\n\t\t}\n\t\t__kfree_skb(skb);\n\t} else {\n\t\tsk->sk_prot->enter_memory_pressure(sk);\n\t\tsk_stream_moderate_sndbuf(sk);\n\t}\n\treturn NULL;\n}\n\nstatic unsigned int tcp_xmit_size_goal(struct sock *sk, u32 mss_now,\n\t\t\t\t       int large_allowed)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 new_size_goal, size_goal;\n\n\tif (!large_allowed || !sk_can_gso(sk))\n\t\treturn mss_now;\n\n\t/* Note : tcp_tso_autosize() will eventually split this later */\n\tnew_size_goal = sk->sk_gso_max_size - 1 - MAX_TCP_HEADER;\n\tnew_size_goal = tcp_bound_to_half_wnd(tp, new_size_goal);\n\n\t/* We try hard to avoid divides here */\n\tsize_goal = tp->gso_segs * mss_now;\n\tif (unlikely(new_size_goal < size_goal ||\n\t\t     new_size_goal >= size_goal + mss_now)) {\n\t\ttp->gso_segs = min_t(u16, new_size_goal / mss_now,\n\t\t\t\t     sk->sk_gso_max_segs);\n\t\tsize_goal = tp->gso_segs * mss_now;\n\t}\n\n\treturn max(size_goal, mss_now);\n}\n\nstatic int tcp_send_mss(struct sock *sk, int *size_goal, int flags)\n{\n\tint mss_now;\n\n\tmss_now = tcp_current_mss(sk);\n\t*size_goal = tcp_xmit_size_goal(sk, mss_now, !(flags & MSG_OOB));\n\n\treturn mss_now;\n}\n\nstatic ssize_t do_tcp_sendpages(struct sock *sk, struct page *page, int offset,\n\t\t\t\tsize_t size, int flags)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint mss_now, size_goal;\n\tint err;\n\tssize_t copied;\n\tlong timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);\n\n\t/* Wait for a connection to finish. One exception is TCP Fast Open\n\t * (passive side) where data is allowed to be sent before a connection\n\t * is fully established.\n\t */\n\tif (((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) &&\n\t    !tcp_passive_fastopen(sk)) {\n\t\terr = sk_stream_wait_connect(sk, &timeo);\n\t\tif (err != 0)\n\t\t\tgoto out_err;\n\t}\n\n\tsk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\n\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\tcopied = 0;\n\n\terr = -EPIPE;\n\tif (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))\n\t\tgoto out_err;\n\n\twhile (size > 0) {\n\t\tstruct sk_buff *skb = tcp_write_queue_tail(sk);\n\t\tint copy, i;\n\t\tbool can_coalesce;\n\n\t\tif (!tcp_send_head(sk) || (copy = size_goal - skb->len) <= 0 ||\n\t\t    !tcp_skb_can_collapse_to(skb)) {\nnew_segment:\n\t\t\tif (!sk_stream_memory_free(sk))\n\t\t\t\tgoto wait_for_sndbuf;\n\n\t\t\tskb = sk_stream_alloc_skb(sk, 0, sk->sk_allocation,\n\t\t\t\t\t\t  skb_queue_empty(&sk->sk_write_queue));\n\t\t\tif (!skb)\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\tskb_entail(sk, skb);\n\t\t\tcopy = size_goal;\n\t\t}\n\n\t\tif (copy > size)\n\t\t\tcopy = size;\n\n\t\ti = skb_shinfo(skb)->nr_frags;\n\t\tcan_coalesce = skb_can_coalesce(skb, i, page, offset);\n\t\tif (!can_coalesce && i >= sysctl_max_skb_frags) {\n\t\t\ttcp_mark_push(tp, skb);\n\t\t\tgoto new_segment;\n\t\t}\n\t\tif (!sk_wmem_schedule(sk, copy))\n\t\t\tgoto wait_for_memory;\n\n\t\tif (can_coalesce) {\n\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t} else {\n\t\t\tget_page(page);\n\t\t\tskb_fill_page_desc(skb, i, page, offset, copy);\n\t\t}\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_SHARED_FRAG;\n\n\t\tskb->len += copy;\n\t\tskb->data_len += copy;\n\t\tskb->truesize += copy;\n\t\tsk->sk_wmem_queued += copy;\n\t\tsk_mem_charge(sk, copy);\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\ttp->write_seq += copy;\n\t\tTCP_SKB_CB(skb)->end_seq += copy;\n\t\ttcp_skb_pcount_set(skb, 0);\n\n\t\tif (!copied)\n\t\t\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_PSH;\n\n\t\tcopied += copy;\n\t\toffset += copy;\n\t\tsize -= copy;\n\t\tif (!size)\n\t\t\tgoto out;\n\n\t\tif (skb->len < size_goal || (flags & MSG_OOB))\n\t\t\tcontinue;\n\n\t\tif (forced_push(tp)) {\n\t\t\ttcp_mark_push(tp, skb);\n\t\t\t__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);\n\t\t} else if (skb == tcp_send_head(sk))\n\t\t\ttcp_push_one(sk, mss_now);\n\t\tcontinue;\n\nwait_for_sndbuf:\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\nwait_for_memory:\n\t\ttcp_push(sk, flags & ~MSG_MORE, mss_now,\n\t\t\t TCP_NAGLE_PUSH, size_goal);\n\n\t\terr = sk_stream_wait_memory(sk, &timeo);\n\t\tif (err != 0)\n\t\t\tgoto do_error;\n\n\t\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\t}\n\nout:\n\tif (copied) {\n\t\ttcp_tx_timestamp(sk, sk->sk_tsflags, tcp_write_queue_tail(sk));\n\t\tif (!(flags & MSG_SENDPAGE_NOTLAST))\n\t\t\ttcp_push(sk, flags, mss_now, tp->nonagle, size_goal);\n\t}\n\treturn copied;\n\ndo_error:\n\tif (copied)\n\t\tgoto out;\nout_err:\n\t/* make sure we wake any epoll edge trigger waiter */\n\tif (unlikely(skb_queue_len(&sk->sk_write_queue) == 0 &&\n\t\t     err == -EAGAIN)) {\n\t\tsk->sk_write_space(sk);\n\t\ttcp_chrono_stop(sk, TCP_CHRONO_SNDBUF_LIMITED);\n\t}\n\treturn sk_stream_error(sk, flags, err);\n}\n\nint tcp_sendpage(struct sock *sk, struct page *page, int offset,\n\t\t size_t size, int flags)\n{\n\tssize_t res;\n\n\tif (!(sk->sk_route_caps & NETIF_F_SG) ||\n\t    !sk_check_csum_caps(sk))\n\t\treturn sock_no_sendpage(sk->sk_socket, page, offset, size,\n\t\t\t\t\tflags);\n\n\tlock_sock(sk);\n\n\ttcp_rate_check_app_limited(sk);  /* is sending application-limited? */\n\n\tres = do_tcp_sendpages(sk, page, offset, size, flags);\n\trelease_sock(sk);\n\treturn res;\n}\nEXPORT_SYMBOL(tcp_sendpage);\n\n/* Do not bother using a page frag for very small frames.\n * But use this heuristic only for the first skb in write queue.\n *\n * Having no payload in skb->head allows better SACK shifting\n * in tcp_shift_skb_data(), reducing sack/rack overhead, because\n * write queue has less skbs.\n * Each skb can hold up to MAX_SKB_FRAGS * 32Kbytes, or ~0.5 MB.\n * This also speeds up tso_fragment(), since it wont fallback\n * to tcp_fragment().\n */\nstatic int linear_payload_sz(bool first_skb)\n{\n\tif (first_skb)\n\t\treturn SKB_WITH_OVERHEAD(2048 - MAX_TCP_HEADER);\n\treturn 0;\n}\n\nstatic int select_size(const struct sock *sk, bool sg, bool first_skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tint tmp = tp->mss_cache;\n\n\tif (sg) {\n\t\tif (sk_can_gso(sk)) {\n\t\t\ttmp = linear_payload_sz(first_skb);\n\t\t} else {\n\t\t\tint pgbreak = SKB_MAX_HEAD(MAX_TCP_HEADER);\n\n\t\t\tif (tmp >= pgbreak &&\n\t\t\t    tmp <= pgbreak + (MAX_SKB_FRAGS - 1) * PAGE_SIZE)\n\t\t\t\ttmp = pgbreak;\n\t\t}\n\t}\n\n\treturn tmp;\n}\n\nvoid tcp_free_fastopen_req(struct tcp_sock *tp)\n{\n\tif (tp->fastopen_req) {\n\t\tkfree(tp->fastopen_req);\n\t\ttp->fastopen_req = NULL;\n\t}\n}\n\nstatic int tcp_sendmsg_fastopen(struct sock *sk, struct msghdr *msg,\n\t\t\t\tint *copied, size_t size)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err, flags;\n\n\tif (!(sysctl_tcp_fastopen & TFO_CLIENT_ENABLE))\n\t\treturn -EOPNOTSUPP;\n\tif (tp->fastopen_req)\n\t\treturn -EALREADY; /* Another Fast Open is in progress */\n\n\ttp->fastopen_req = kzalloc(sizeof(struct tcp_fastopen_request),\n\t\t\t\t   sk->sk_allocation);\n\tif (unlikely(!tp->fastopen_req))\n\t\treturn -ENOBUFS;\n\ttp->fastopen_req->data = msg;\n\ttp->fastopen_req->size = size;\n\n\tif (inet->defer_connect) {\n\t\terr = tcp_connect(sk);\n\t\t/* Same failure procedure as in tcp_v4/6_connect */\n\t\tif (err) {\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\tinet->inet_dport = 0;\n\t\t\tsk->sk_route_caps = 0;\n\t\t}\n\t}\n\tflags = (msg->msg_flags & MSG_DONTWAIT) ? O_NONBLOCK : 0;\n\terr = __inet_stream_connect(sk->sk_socket, msg->msg_name,\n\t\t\t\t    msg->msg_namelen, flags, 1);\n\t/* fastopen_req could already be freed in __inet_stream_connect\n\t * if the connection times out or gets rst\n\t */\n\tif (tp->fastopen_req) {\n\t\t*copied = tp->fastopen_req->copied;\n\t\ttcp_free_fastopen_req(tp);\n\t\tinet->defer_connect = 0;\n\t}\n\treturn err;\n}\n\nint tcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct sockcm_cookie sockc;\n\tint flags, err, copied = 0;\n\tint mss_now = 0, size_goal, copied_syn = 0;\n\tbool process_backlog = false;\n\tbool sg;\n\tlong timeo;\n\n\tlock_sock(sk);\n\n\tflags = msg->msg_flags;\n\tif (unlikely(flags & MSG_FASTOPEN || inet_sk(sk)->defer_connect)) {\n\t\terr = tcp_sendmsg_fastopen(sk, msg, &copied_syn, size);\n\t\tif (err == -EINPROGRESS && copied_syn > 0)\n\t\t\tgoto out;\n\t\telse if (err)\n\t\t\tgoto out_err;\n\t}\n\n\ttimeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);\n\n\ttcp_rate_check_app_limited(sk);  /* is sending application-limited? */\n\n\t/* Wait for a connection to finish. One exception is TCP Fast Open\n\t * (passive side) where data is allowed to be sent before a connection\n\t * is fully established.\n\t */\n\tif (((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) &&\n\t    !tcp_passive_fastopen(sk)) {\n\t\terr = sk_stream_wait_connect(sk, &timeo);\n\t\tif (err != 0)\n\t\t\tgoto do_error;\n\t}\n\n\tif (unlikely(tp->repair)) {\n\t\tif (tp->repair_queue == TCP_RECV_QUEUE) {\n\t\t\tcopied = tcp_send_rcvq(sk, msg, size);\n\t\t\tgoto out_nopush;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (tp->repair_queue == TCP_NO_QUEUE)\n\t\t\tgoto out_err;\n\n\t\t/* 'common' sending to sendq */\n\t}\n\n\tsockc.tsflags = sk->sk_tsflags;\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(sk, msg, &sockc);\n\t\tif (unlikely(err)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_err;\n\t\t}\n\t}\n\n\t/* This should be in poll */\n\tsk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\n\t/* Ok commence sending. */\n\tcopied = 0;\n\nrestart:\n\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\n\terr = -EPIPE;\n\tif (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))\n\t\tgoto do_error;\n\n\tsg = !!(sk->sk_route_caps & NETIF_F_SG);\n\n\twhile (msg_data_left(msg)) {\n\t\tint copy = 0;\n\t\tint max = size_goal;\n\n\t\tskb = tcp_write_queue_tail(sk);\n\t\tif (tcp_send_head(sk)) {\n\t\t\tif (skb->ip_summed == CHECKSUM_NONE)\n\t\t\t\tmax = mss_now;\n\t\t\tcopy = max - skb->len;\n\t\t}\n\n\t\tif (copy <= 0 || !tcp_skb_can_collapse_to(skb)) {\n\t\t\tbool first_skb;\n\nnew_segment:\n\t\t\t/* Allocate new segment. If the interface is SG,\n\t\t\t * allocate skb fitting to single page.\n\t\t\t */\n\t\t\tif (!sk_stream_memory_free(sk))\n\t\t\t\tgoto wait_for_sndbuf;\n\n\t\t\tif (process_backlog && sk_flush_backlog(sk)) {\n\t\t\t\tprocess_backlog = false;\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t\tfirst_skb = skb_queue_empty(&sk->sk_write_queue);\n\t\t\tskb = sk_stream_alloc_skb(sk,\n\t\t\t\t\t\t  select_size(sk, sg, first_skb),\n\t\t\t\t\t\t  sk->sk_allocation,\n\t\t\t\t\t\t  first_skb);\n\t\t\tif (!skb)\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\tprocess_backlog = true;\n\t\t\t/*\n\t\t\t * Check whether we can use HW checksum.\n\t\t\t */\n\t\t\tif (sk_check_csum_caps(sk))\n\t\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\t\t\tskb_entail(sk, skb);\n\t\t\tcopy = size_goal;\n\t\t\tmax = size_goal;\n\n\t\t\t/* All packets are restored as if they have\n\t\t\t * already been sent. skb_mstamp isn't set to\n\t\t\t * avoid wrong rtt estimation.\n\t\t\t */\n\t\t\tif (tp->repair)\n\t\t\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_REPAIRED;\n\t\t}\n\n\t\t/* Try to append data to the end of skb. */\n\t\tif (copy > msg_data_left(msg))\n\t\t\tcopy = msg_data_left(msg);\n\n\t\t/* Where to copy to? */\n\t\tif (skb_availroom(skb) > 0) {\n\t\t\t/* We have some space in skb head. Superb! */\n\t\t\tcopy = min_t(int, copy, skb_availroom(skb));\n\t\t\terr = skb_add_data_nocache(sk, skb, &msg->msg_iter, copy);\n\t\t\tif (err)\n\t\t\t\tgoto do_fault;\n\t\t} else {\n\t\t\tbool merge = true;\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\t\t\tstruct page_frag *pfrag = sk_page_frag(sk);\n\n\t\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\tif (!skb_can_coalesce(skb, i, pfrag->page,\n\t\t\t\t\t      pfrag->offset)) {\n\t\t\t\tif (i >= sysctl_max_skb_frags || !sg) {\n\t\t\t\t\ttcp_mark_push(tp, skb);\n\t\t\t\t\tgoto new_segment;\n\t\t\t\t}\n\t\t\t\tmerge = false;\n\t\t\t}\n\n\t\t\tcopy = min_t(int, copy, pfrag->size - pfrag->offset);\n\n\t\t\tif (!sk_wmem_schedule(sk, copy))\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\terr = skb_copy_to_page_nocache(sk, &msg->msg_iter, skb,\n\t\t\t\t\t\t       pfrag->page,\n\t\t\t\t\t\t       pfrag->offset,\n\t\t\t\t\t\t       copy);\n\t\t\tif (err)\n\t\t\t\tgoto do_error;\n\n\t\t\t/* Update the skb. */\n\t\t\tif (merge) {\n\t\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t\t} else {\n\t\t\t\tskb_fill_page_desc(skb, i, pfrag->page,\n\t\t\t\t\t\t   pfrag->offset, copy);\n\t\t\t\tpage_ref_inc(pfrag->page);\n\t\t\t}\n\t\t\tpfrag->offset += copy;\n\t\t}\n\n\t\tif (!copied)\n\t\t\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_PSH;\n\n\t\ttp->write_seq += copy;\n\t\tTCP_SKB_CB(skb)->end_seq += copy;\n\t\ttcp_skb_pcount_set(skb, 0);\n\n\t\tcopied += copy;\n\t\tif (!msg_data_left(msg)) {\n\t\t\tif (unlikely(flags & MSG_EOR))\n\t\t\t\tTCP_SKB_CB(skb)->eor = 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (skb->len < max || (flags & MSG_OOB) || unlikely(tp->repair))\n\t\t\tcontinue;\n\n\t\tif (forced_push(tp)) {\n\t\t\ttcp_mark_push(tp, skb);\n\t\t\t__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);\n\t\t} else if (skb == tcp_send_head(sk))\n\t\t\ttcp_push_one(sk, mss_now);\n\t\tcontinue;\n\nwait_for_sndbuf:\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\nwait_for_memory:\n\t\tif (copied)\n\t\t\ttcp_push(sk, flags & ~MSG_MORE, mss_now,\n\t\t\t\t TCP_NAGLE_PUSH, size_goal);\n\n\t\terr = sk_stream_wait_memory(sk, &timeo);\n\t\tif (err != 0)\n\t\t\tgoto do_error;\n\n\t\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\t}\n\nout:\n\tif (copied) {\n\t\ttcp_tx_timestamp(sk, sockc.tsflags, tcp_write_queue_tail(sk));\n\t\ttcp_push(sk, flags, mss_now, tp->nonagle, size_goal);\n\t}\nout_nopush:\n\trelease_sock(sk);\n\treturn copied + copied_syn;\n\ndo_fault:\n\tif (!skb->len) {\n\t\ttcp_unlink_write_queue(skb, sk);\n\t\t/* It is the one place in all of TCP, except connection\n\t\t * reset, where we can be unlinking the send_head.\n\t\t */\n\t\ttcp_check_send_head(sk, skb);\n\t\tsk_wmem_free_skb(sk, skb);\n\t}\n\ndo_error:\n\tif (copied + copied_syn)\n\t\tgoto out;\nout_err:\n\terr = sk_stream_error(sk, flags, err);\n\t/* make sure we wake any epoll edge trigger waiter */\n\tif (unlikely(skb_queue_len(&sk->sk_write_queue) == 0 &&\n\t\t     err == -EAGAIN)) {\n\t\tsk->sk_write_space(sk);\n\t\ttcp_chrono_stop(sk, TCP_CHRONO_SNDBUF_LIMITED);\n\t}\n\trelease_sock(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(tcp_sendmsg);\n\n/*\n *\tHandle reading urgent data. BSD has very simple semantics for\n *\tthis, no blocking and very strange errors 8)\n */\n\nstatic int tcp_recv_urg(struct sock *sk, struct msghdr *msg, int len, int flags)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* No URG data to read. */\n\tif (sock_flag(sk, SOCK_URGINLINE) || !tp->urg_data ||\n\t    tp->urg_data == TCP_URG_READ)\n\t\treturn -EINVAL;\t/* Yes this is right ! */\n\n\tif (sk->sk_state == TCP_CLOSE && !sock_flag(sk, SOCK_DONE))\n\t\treturn -ENOTCONN;\n\n\tif (tp->urg_data & TCP_URG_VALID) {\n\t\tint err = 0;\n\t\tchar c = tp->urg_data;\n\n\t\tif (!(flags & MSG_PEEK))\n\t\t\ttp->urg_data = TCP_URG_READ;\n\n\t\t/* Read urgent data. */\n\t\tmsg->msg_flags |= MSG_OOB;\n\n\t\tif (len > 0) {\n\t\t\tif (!(flags & MSG_TRUNC))\n\t\t\t\terr = memcpy_to_msg(msg, &c, 1);\n\t\t\tlen = 1;\n\t\t} else\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\t\treturn err ? -EFAULT : len;\n\t}\n\n\tif (sk->sk_state == TCP_CLOSE || (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\treturn 0;\n\n\t/* Fixed the recv(..., MSG_OOB) behaviour.  BSD docs and\n\t * the available implementations agree in this case:\n\t * this call should never block, independent of the\n\t * blocking state of the socket.\n\t * Mike <pall@rz.uni-karlsruhe.de>\n\t */\n\treturn -EAGAIN;\n}\n\nstatic int tcp_peek_sndq(struct sock *sk, struct msghdr *msg, int len)\n{\n\tstruct sk_buff *skb;\n\tint copied = 0, err = 0;\n\n\t/* XXX -- need to support SO_PEEK_OFF */\n\n\tskb_queue_walk(&sk->sk_write_queue, skb) {\n\t\terr = skb_copy_datagram_msg(skb, 0, msg, skb->len);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tcopied += skb->len;\n\t}\n\n\treturn err ?: copied;\n}\n\n/* Clean up the receive buffer for full frames taken by the user,\n * then send an ACK if necessary.  COPIED is the number of bytes\n * tcp_recvmsg has given to the user so far, it speeds up the\n * calculation of whether or not we must ACK for the sake of\n * a window update.\n */\nstatic void tcp_cleanup_rbuf(struct sock *sk, int copied)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool time_to_ack = false;\n\n\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n\n\tWARN(skb && !before(tp->copied_seq, TCP_SKB_CB(skb)->end_seq),\n\t     \"cleanup rbuf bug: copied %X seq %X rcvnxt %X\\n\",\n\t     tp->copied_seq, TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt);\n\n\tif (inet_csk_ack_scheduled(sk)) {\n\t\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\t\t   /* Delayed ACKs frequently hit locked sockets during bulk\n\t\t    * receive. */\n\t\tif (icsk->icsk_ack.blocked ||\n\t\t    /* Once-per-two-segments ACK was not sent by tcp_input.c */\n\t\t    tp->rcv_nxt - tp->rcv_wup > icsk->icsk_ack.rcv_mss ||\n\t\t    /*\n\t\t     * If this read emptied read buffer, we send ACK, if\n\t\t     * connection is not bidirectional, user drained\n\t\t     * receive buffer and there was a small segment\n\t\t     * in queue.\n\t\t     */\n\t\t    (copied > 0 &&\n\t\t     ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED2) ||\n\t\t      ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED) &&\n\t\t       !icsk->icsk_ack.pingpong)) &&\n\t\t      !atomic_read(&sk->sk_rmem_alloc)))\n\t\t\ttime_to_ack = true;\n\t}\n\n\t/* We send an ACK if we can now advertise a non-zero window\n\t * which has been raised \"significantly\".\n\t *\n\t * Even if window raised up to infinity, do not send window open ACK\n\t * in states, where we will not receive more. It is useless.\n\t */\n\tif (copied > 0 && !time_to_ack && !(sk->sk_shutdown & RCV_SHUTDOWN)) {\n\t\t__u32 rcv_window_now = tcp_receive_window(tp);\n\n\t\t/* Optimize, __tcp_select_window() is not cheap. */\n\t\tif (2*rcv_window_now <= tp->window_clamp) {\n\t\t\t__u32 new_window = __tcp_select_window(sk);\n\n\t\t\t/* Send ACK now, if this read freed lots of space\n\t\t\t * in our buffer. Certainly, new_window is new window.\n\t\t\t * We can advertise it now, if it is not less than current one.\n\t\t\t * \"Lots\" means \"at least twice\" here.\n\t\t\t */\n\t\t\tif (new_window && new_window >= 2 * rcv_window_now)\n\t\t\t\ttime_to_ack = true;\n\t\t}\n\t}\n\tif (time_to_ack)\n\t\ttcp_send_ack(sk);\n}\n\nstatic void tcp_prequeue_process(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPREQUEUED);\n\n\twhile ((skb = __skb_dequeue(&tp->ucopy.prequeue)) != NULL)\n\t\tsk_backlog_rcv(sk, skb);\n\n\t/* Clear memory counter. */\n\ttp->ucopy.memory = 0;\n}\n\nstatic struct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off)\n{\n\tstruct sk_buff *skb;\n\tu32 offset;\n\n\twhile ((skb = skb_peek(&sk->sk_receive_queue)) != NULL) {\n\t\toffset = seq - TCP_SKB_CB(skb)->seq;\n\t\tif (unlikely(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\t\tpr_err_once(\"%s: found a SYN, please report !\\n\", __func__);\n\t\t\toffset--;\n\t\t}\n\t\tif (offset < skb->len || (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)) {\n\t\t\t*off = offset;\n\t\t\treturn skb;\n\t\t}\n\t\t/* This looks weird, but this can happen if TCP collapsing\n\t\t * splitted a fat GRO packet, while we released socket lock\n\t\t * in skb_splice_bits()\n\t\t */\n\t\tsk_eat_skb(sk, skb);\n\t}\n\treturn NULL;\n}\n\n/*\n * This routine provides an alternative to tcp_recvmsg() for routines\n * that would like to handle copying from skbuffs directly in 'sendfile'\n * fashion.\n * Note:\n *\t- It is assumed that the socket was locked by the caller.\n *\t- The routine does not block.\n *\t- At present, there is no support for reading OOB data\n *\t  or for 'peeking' the socket using this routine\n *\t  (although both would be easy to implement).\n */\nint tcp_read_sock(struct sock *sk, read_descriptor_t *desc,\n\t\t  sk_read_actor_t recv_actor)\n{\n\tstruct sk_buff *skb;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 seq = tp->copied_seq;\n\tu32 offset;\n\tint copied = 0;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn -ENOTCONN;\n\twhile ((skb = tcp_recv_skb(sk, seq, &offset)) != NULL) {\n\t\tif (offset < skb->len) {\n\t\t\tint used;\n\t\t\tsize_t len;\n\n\t\t\tlen = skb->len - offset;\n\t\t\t/* Stop reading if we hit a patch of urgent data */\n\t\t\tif (tp->urg_data) {\n\t\t\t\tu32 urg_offset = tp->urg_seq - seq;\n\t\t\t\tif (urg_offset < len)\n\t\t\t\t\tlen = urg_offset;\n\t\t\t\tif (!len)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tused = recv_actor(desc, skb, offset, len);\n\t\t\tif (used <= 0) {\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = used;\n\t\t\t\tbreak;\n\t\t\t} else if (used <= len) {\n\t\t\t\tseq += used;\n\t\t\t\tcopied += used;\n\t\t\t\toffset += used;\n\t\t\t}\n\t\t\t/* If recv_actor drops the lock (e.g. TCP splice\n\t\t\t * receive) the skb pointer might be invalid when\n\t\t\t * getting here: tcp_collapse might have deleted it\n\t\t\t * while aggregating skbs from the socket queue.\n\t\t\t */\n\t\t\tskb = tcp_recv_skb(sk, seq - 1, &offset);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t\t/* TCP coalescing might have appended data to the skb.\n\t\t\t * Try to splice more frags\n\t\t\t */\n\t\t\tif (offset + 1 != skb->len)\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN) {\n\t\t\tsk_eat_skb(sk, skb);\n\t\t\t++seq;\n\t\t\tbreak;\n\t\t}\n\t\tsk_eat_skb(sk, skb);\n\t\tif (!desc->count)\n\t\t\tbreak;\n\t\ttp->copied_seq = seq;\n\t}\n\ttp->copied_seq = seq;\n\n\ttcp_rcv_space_adjust(sk);\n\n\t/* Clean up data we have read: This will do ACK frames. */\n\tif (copied > 0) {\n\t\ttcp_recv_skb(sk, seq, &offset);\n\t\ttcp_cleanup_rbuf(sk, copied);\n\t}\n\treturn copied;\n}\nEXPORT_SYMBOL(tcp_read_sock);\n\nint tcp_peek_len(struct socket *sock)\n{\n\treturn tcp_inq(sock->sk);\n}\nEXPORT_SYMBOL(tcp_peek_len);\n\n/*\n *\tThis routine copies from a sock struct into the user buffer.\n *\n *\tTechnical note: in 2.3 we work on _locked_ socket, so that\n *\ttricks with *seq access order and skb->users are not required.\n *\tProbably, code can be easily improved even more.\n */\n\nint tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,\n\t\tint flags, int *addr_len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint copied = 0;\n\tu32 peek_seq;\n\tu32 *seq;\n\tunsigned long used;\n\tint err;\n\tint target;\t\t/* Read at least this many bytes */\n\tlong timeo;\n\tstruct task_struct *user_recv = NULL;\n\tstruct sk_buff *skb, *last;\n\tu32 urg_hole = 0;\n\n\tif (unlikely(flags & MSG_ERRQUEUE))\n\t\treturn inet_recv_error(sk, msg, len, addr_len);\n\n\tif (sk_can_busy_loop(sk) && skb_queue_empty(&sk->sk_receive_queue) &&\n\t    (sk->sk_state == TCP_ESTABLISHED))\n\t\tsk_busy_loop(sk, nonblock);\n\n\tlock_sock(sk);\n\n\terr = -ENOTCONN;\n\tif (sk->sk_state == TCP_LISTEN)\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\t/* Urgent data needs to be handled specially. */\n\tif (flags & MSG_OOB)\n\t\tgoto recv_urg;\n\n\tif (unlikely(tp->repair)) {\n\t\terr = -EPERM;\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tgoto out;\n\n\t\tif (tp->repair_queue == TCP_SEND_QUEUE)\n\t\t\tgoto recv_sndq;\n\n\t\terr = -EINVAL;\n\t\tif (tp->repair_queue == TCP_NO_QUEUE)\n\t\t\tgoto out;\n\n\t\t/* 'common' recv queue MSG_PEEK-ing */\n\t}\n\n\tseq = &tp->copied_seq;\n\tif (flags & MSG_PEEK) {\n\t\tpeek_seq = tp->copied_seq;\n\t\tseq = &peek_seq;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\n\tdo {\n\t\tu32 offset;\n\n\t\t/* Are we at urgent data? Stop if we have read anything or have SIGURG pending. */\n\t\tif (tp->urg_data && tp->urg_seq == *seq) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* Next get a buffer. */\n\n\t\tlast = skb_peek_tail(&sk->sk_receive_queue);\n\t\tskb_queue_walk(&sk->sk_receive_queue, skb) {\n\t\t\tlast = skb;\n\t\t\t/* Now that we have two receive queues this\n\t\t\t * shouldn't happen.\n\t\t\t */\n\t\t\tif (WARN(before(*seq, TCP_SKB_CB(skb)->seq),\n\t\t\t\t \"recvmsg bug: copied %X seq %X rcvnxt %X fl %X\\n\",\n\t\t\t\t *seq, TCP_SKB_CB(skb)->seq, tp->rcv_nxt,\n\t\t\t\t flags))\n\t\t\t\tbreak;\n\n\t\t\toffset = *seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (unlikely(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\t\t\tpr_err_once(\"%s: found a SYN, please report !\\n\", __func__);\n\t\t\t\toffset--;\n\t\t\t}\n\t\t\tif (offset < skb->len)\n\t\t\t\tgoto found_ok_skb;\n\t\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\t\tgoto found_fin_ok;\n\t\t\tWARN(!(flags & MSG_PEEK),\n\t\t\t     \"recvmsg bug 2: copied %X seq %X rcvnxt %X fl %X\\n\",\n\t\t\t     *seq, TCP_SKB_CB(skb)->seq, tp->rcv_nxt, flags);\n\t\t}\n\n\t\t/* Well, if we have backlog, try to process it now yet. */\n\n\t\tif (copied >= target && !sk->sk_backlog.tail)\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err ||\n\t\t\t    sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    !timeo ||\n\t\t\t    signal_pending(current))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE)) {\n\t\t\t\t\t/* This occurs when user tries to read\n\t\t\t\t\t * from never connected socket.\n\t\t\t\t\t */\n\t\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\ttcp_cleanup_rbuf(sk, copied);\n\n\t\tif (!sysctl_tcp_low_latency && tp->ucopy.task == user_recv) {\n\t\t\t/* Install new reader */\n\t\t\tif (!user_recv && !(flags & (MSG_TRUNC | MSG_PEEK))) {\n\t\t\t\tuser_recv = current;\n\t\t\t\ttp->ucopy.task = user_recv;\n\t\t\t\ttp->ucopy.msg = msg;\n\t\t\t}\n\n\t\t\ttp->ucopy.len = len;\n\n\t\t\tWARN_ON(tp->copied_seq != tp->rcv_nxt &&\n\t\t\t\t!(flags & (MSG_PEEK | MSG_TRUNC)));\n\n\t\t\t/* Ugly... If prequeue is not empty, we have to\n\t\t\t * process it before releasing socket, otherwise\n\t\t\t * order will be broken at second iteration.\n\t\t\t * More elegant solution is required!!!\n\t\t\t *\n\t\t\t * Look: we have the following (pseudo)queues:\n\t\t\t *\n\t\t\t * 1. packets in flight\n\t\t\t * 2. backlog\n\t\t\t * 3. prequeue\n\t\t\t * 4. receive_queue\n\t\t\t *\n\t\t\t * Each queue can be processed only if the next ones\n\t\t\t * are empty. At this point we have empty receive_queue.\n\t\t\t * But prequeue _can_ be not empty after 2nd iteration,\n\t\t\t * when we jumped to start of loop because backlog\n\t\t\t * processing added something to receive_queue.\n\t\t\t * We cannot release_sock(), because backlog contains\n\t\t\t * packets arrived _after_ prequeued ones.\n\t\t\t *\n\t\t\t * Shortly, algorithm is clear --- to process all\n\t\t\t * the queues in order. We could make it more directly,\n\t\t\t * requeueing packets from backlog to prequeue, if\n\t\t\t * is not empty. It is more elegant, but eats cycles,\n\t\t\t * unfortunately.\n\t\t\t */\n\t\t\tif (!skb_queue_empty(&tp->ucopy.prequeue))\n\t\t\t\tgoto do_prequeue;\n\n\t\t\t/* __ Set realtime policy in scheduler __ */\n\t\t}\n\n\t\tif (copied >= target) {\n\t\t\t/* Do not sleep, just process backlog. */\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t} else {\n\t\t\tsk_wait_data(sk, &timeo, last);\n\t\t}\n\n\t\tif (user_recv) {\n\t\t\tint chunk;\n\n\t\t\t/* __ Restore normal policy in scheduler __ */\n\n\t\t\tchunk = len - tp->ucopy.len;\n\t\t\tif (chunk != 0) {\n\t\t\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMBACKLOG, chunk);\n\t\t\t\tlen -= chunk;\n\t\t\t\tcopied += chunk;\n\t\t\t}\n\n\t\t\tif (tp->rcv_nxt == tp->copied_seq &&\n\t\t\t    !skb_queue_empty(&tp->ucopy.prequeue)) {\ndo_prequeue:\n\t\t\t\ttcp_prequeue_process(sk);\n\n\t\t\t\tchunk = len - tp->ucopy.len;\n\t\t\t\tif (chunk != 0) {\n\t\t\t\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);\n\t\t\t\t\tlen -= chunk;\n\t\t\t\t\tcopied += chunk;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ((flags & MSG_PEEK) &&\n\t\t    (peek_seq - copied - urg_hole != tp->copied_seq)) {\n\t\t\tnet_dbg_ratelimited(\"TCP(%s:%d): Application bug, race in MSG_PEEK\\n\",\n\t\t\t\t\t    current->comm,\n\t\t\t\t\t    task_pid_nr(current));\n\t\t\tpeek_seq = tp->copied_seq;\n\t\t}\n\t\tcontinue;\n\n\tfound_ok_skb:\n\t\t/* Ok so how much can we use? */\n\t\tused = skb->len - offset;\n\t\tif (len < used)\n\t\t\tused = len;\n\n\t\t/* Do we have urgent data here? */\n\t\tif (tp->urg_data) {\n\t\t\tu32 urg_offset = tp->urg_seq - *seq;\n\t\t\tif (urg_offset < used) {\n\t\t\t\tif (!urg_offset) {\n\t\t\t\t\tif (!sock_flag(sk, SOCK_URGINLINE)) {\n\t\t\t\t\t\t++*seq;\n\t\t\t\t\t\turg_hole++;\n\t\t\t\t\t\toffset++;\n\t\t\t\t\t\tused--;\n\t\t\t\t\t\tif (!used)\n\t\t\t\t\t\t\tgoto skip_copy;\n\t\t\t\t\t}\n\t\t\t\t} else\n\t\t\t\t\tused = urg_offset;\n\t\t\t}\n\t\t}\n\n\t\tif (!(flags & MSG_TRUNC)) {\n\t\t\terr = skb_copy_datagram_msg(skb, offset, msg, used);\n\t\t\tif (err) {\n\t\t\t\t/* Exception. Bailout! */\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t*seq += used;\n\t\tcopied += used;\n\t\tlen -= used;\n\n\t\ttcp_rcv_space_adjust(sk);\n\nskip_copy:\n\t\tif (tp->urg_data && after(tp->copied_seq, tp->urg_seq)) {\n\t\t\ttp->urg_data = 0;\n\t\t\ttcp_fast_path_check(sk);\n\t\t}\n\t\tif (used + offset < skb->len)\n\t\t\tcontinue;\n\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\tgoto found_fin_ok;\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tsk_eat_skb(sk, skb);\n\t\tcontinue;\n\n\tfound_fin_ok:\n\t\t/* Process the FIN. */\n\t\t++*seq;\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tsk_eat_skb(sk, skb);\n\t\tbreak;\n\t} while (len > 0);\n\n\tif (user_recv) {\n\t\tif (!skb_queue_empty(&tp->ucopy.prequeue)) {\n\t\t\tint chunk;\n\n\t\t\ttp->ucopy.len = copied > 0 ? len : 0;\n\n\t\t\ttcp_prequeue_process(sk);\n\n\t\t\tif (copied > 0 && (chunk = len - tp->ucopy.len) != 0) {\n\t\t\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);\n\t\t\t\tlen -= chunk;\n\t\t\t\tcopied += chunk;\n\t\t\t}\n\t\t}\n\n\t\ttp->ucopy.task = NULL;\n\t\ttp->ucopy.len = 0;\n\t}\n\n\t/* According to UNIX98, msg_name/msg_namelen are ignored\n\t * on connected socket. I was just happy when found this 8) --ANK\n\t */\n\n\t/* Clean up data we have read: This will do ACK frames. */\n\ttcp_cleanup_rbuf(sk, copied);\n\n\trelease_sock(sk);\n\treturn copied;\n\nout:\n\trelease_sock(sk);\n\treturn err;\n\nrecv_urg:\n\terr = tcp_recv_urg(sk, msg, len, flags);\n\tgoto out;\n\nrecv_sndq:\n\terr = tcp_peek_sndq(sk, msg, len);\n\tgoto out;\n}\nEXPORT_SYMBOL(tcp_recvmsg);\n\nvoid tcp_set_state(struct sock *sk, int state)\n{\n\tint oldstate = sk->sk_state;\n\n\tswitch (state) {\n\tcase TCP_ESTABLISHED:\n\t\tif (oldstate != TCP_ESTABLISHED)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CURRESTAB);\n\t\tbreak;\n\n\tcase TCP_CLOSE:\n\t\tif (oldstate == TCP_CLOSE_WAIT || oldstate == TCP_ESTABLISHED)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ESTABRESETS);\n\n\t\tsk->sk_prot->unhash(sk);\n\t\tif (inet_csk(sk)->icsk_bind_hash &&\n\t\t    !(sk->sk_userlocks & SOCK_BINDPORT_LOCK))\n\t\t\tinet_put_port(sk);\n\t\t/* fall through */\n\tdefault:\n\t\tif (oldstate == TCP_ESTABLISHED)\n\t\t\tTCP_DEC_STATS(sock_net(sk), TCP_MIB_CURRESTAB);\n\t}\n\n\t/* Change state AFTER socket is unhashed to avoid closed\n\t * socket sitting in hash tables.\n\t */\n\tsk_state_store(sk, state);\n\n#ifdef STATE_TRACE\n\tSOCK_DEBUG(sk, \"TCP sk=%p, State %s -> %s\\n\", sk, statename[oldstate], statename[state]);\n#endif\n}\nEXPORT_SYMBOL_GPL(tcp_set_state);\n\n/*\n *\tState processing on a close. This implements the state shift for\n *\tsending our FIN frame. Note that we only send a FIN for some\n *\tstates. A shutdown() may have already sent the FIN, or we may be\n *\tclosed.\n */\n\nstatic const unsigned char new_state[16] = {\n  /* current state:        new state:      action:\t*/\n  [0 /* (Invalid) */]\t= TCP_CLOSE,\n  [TCP_ESTABLISHED]\t= TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n  [TCP_SYN_SENT]\t= TCP_CLOSE,\n  [TCP_SYN_RECV]\t= TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n  [TCP_FIN_WAIT1]\t= TCP_FIN_WAIT1,\n  [TCP_FIN_WAIT2]\t= TCP_FIN_WAIT2,\n  [TCP_TIME_WAIT]\t= TCP_CLOSE,\n  [TCP_CLOSE]\t\t= TCP_CLOSE,\n  [TCP_CLOSE_WAIT]\t= TCP_LAST_ACK  | TCP_ACTION_FIN,\n  [TCP_LAST_ACK]\t= TCP_LAST_ACK,\n  [TCP_LISTEN]\t\t= TCP_CLOSE,\n  [TCP_CLOSING]\t\t= TCP_CLOSING,\n  [TCP_NEW_SYN_RECV]\t= TCP_CLOSE,\t/* should not happen ! */\n};\n\nstatic int tcp_close_state(struct sock *sk)\n{\n\tint next = (int)new_state[sk->sk_state];\n\tint ns = next & TCP_STATE_MASK;\n\n\ttcp_set_state(sk, ns);\n\n\treturn next & TCP_ACTION_FIN;\n}\n\n/*\n *\tShutdown the sending side of a connection. Much like close except\n *\tthat we don't receive shut down or sock_set_flag(sk, SOCK_DEAD).\n */\n\nvoid tcp_shutdown(struct sock *sk, int how)\n{\n\t/*\tWe need to grab some memory, and put together a FIN,\n\t *\tand then put it into the queue to be sent.\n\t *\t\tTim MacKenzie(tym@dibbler.cs.monash.edu.au) 4 Dec '92.\n\t */\n\tif (!(how & SEND_SHUTDOWN))\n\t\treturn;\n\n\t/* If we've already sent a FIN, or it's a closed state, skip this. */\n\tif ((1 << sk->sk_state) &\n\t    (TCPF_ESTABLISHED | TCPF_SYN_SENT |\n\t     TCPF_SYN_RECV | TCPF_CLOSE_WAIT)) {\n\t\t/* Clear out any half completed packets.  FIN if needed. */\n\t\tif (tcp_close_state(sk))\n\t\t\ttcp_send_fin(sk);\n\t}\n}\nEXPORT_SYMBOL(tcp_shutdown);\n\nbool tcp_check_oom(struct sock *sk, int shift)\n{\n\tbool too_many_orphans, out_of_socket_memory;\n\n\ttoo_many_orphans = tcp_too_many_orphans(sk, shift);\n\tout_of_socket_memory = tcp_out_of_memory(sk);\n\n\tif (too_many_orphans)\n\t\tnet_info_ratelimited(\"too many orphaned sockets\\n\");\n\tif (out_of_socket_memory)\n\t\tnet_info_ratelimited(\"out of memory -- consider tuning tcp_mem\\n\");\n\treturn too_many_orphans || out_of_socket_memory;\n}\n\nvoid tcp_close(struct sock *sk, long timeout)\n{\n\tstruct sk_buff *skb;\n\tint data_was_unread = 0;\n\tint state;\n\n\tlock_sock(sk);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\n\t\t/* Special case. */\n\t\tinet_csk_listen_stop(sk);\n\n\t\tgoto adjudge_to_death;\n\t}\n\n\t/*  We need to flush the recv. buffs.  We do this only on the\n\t *  descriptor close, not protocol-sourced closes, because the\n\t *  reader process may not have drained the data yet!\n\t */\n\twhile ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {\n\t\tu32 len = TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq;\n\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\tlen--;\n\t\tdata_was_unread += len;\n\t\t__kfree_skb(skb);\n\t}\n\n\tsk_mem_reclaim(sk);\n\n\t/* If socket has been already reset (e.g. in tcp_reset()) - kill it. */\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto adjudge_to_death;\n\n\t/* As outlined in RFC 2525, section 2.17, we send a RST here because\n\t * data was lost. To witness the awful effects of the old behavior of\n\t * always doing a FIN, run an older 2.1.x kernel or 2.0.x, start a bulk\n\t * GET in an FTP client, suspend the process, wait for the client to\n\t * advertise a zero window, then kill -9 the FTP client, wheee...\n\t * Note: timeout is always zero in such a case.\n\t */\n\tif (unlikely(tcp_sk(sk)->repair)) {\n\t\tsk->sk_prot->disconnect(sk, 0);\n\t} else if (data_was_unread) {\n\t\t/* Unread data was tossed, zap the connection. */\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\ttcp_send_active_reset(sk, sk->sk_allocation);\n\t} else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime) {\n\t\t/* Check zero linger _after_ checking for unread data. */\n\t\tsk->sk_prot->disconnect(sk, 0);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t} else if (tcp_close_state(sk)) {\n\t\t/* We FIN if the application ate all the data before\n\t\t * zapping the connection.\n\t\t */\n\n\t\t/* RED-PEN. Formally speaking, we have broken TCP state\n\t\t * machine. State transitions:\n\t\t *\n\t\t * TCP_ESTABLISHED -> TCP_FIN_WAIT1\n\t\t * TCP_SYN_RECV\t-> TCP_FIN_WAIT1 (forget it, it's impossible)\n\t\t * TCP_CLOSE_WAIT -> TCP_LAST_ACK\n\t\t *\n\t\t * are legal only when FIN has been sent (i.e. in window),\n\t\t * rather than queued out of window. Purists blame.\n\t\t *\n\t\t * F.e. \"RFC state\" is ESTABLISHED,\n\t\t * if Linux state is FIN-WAIT-1, but FIN is still not sent.\n\t\t *\n\t\t * The visible declinations are that sometimes\n\t\t * we enter time-wait state, when it is not required really\n\t\t * (harmless), do not send active resets, when they are\n\t\t * required by specs (TCP_ESTABLISHED, TCP_CLOSE_WAIT, when\n\t\t * they look as CLOSING or LAST_ACK for Linux)\n\t\t * Probably, I missed some more holelets.\n\t\t * \t\t\t\t\t\t--ANK\n\t\t * XXX (TFO) - To start off we don't support SYN+ACK+FIN\n\t\t * in a single packet! (May consider it later but will\n\t\t * probably need API support or TCP_CORK SYN-ACK until\n\t\t * data is written and socket is closed.)\n\t\t */\n\t\ttcp_send_fin(sk);\n\t}\n\n\tsk_stream_wait_close(sk, timeout);\n\nadjudge_to_death:\n\tstate = sk->sk_state;\n\tsock_hold(sk);\n\tsock_orphan(sk);\n\n\t/* It is the last release_sock in its life. It will remove backlog. */\n\trelease_sock(sk);\n\n\n\t/* Now socket is owned by kernel and we acquire BH lock\n\t   to finish close. No need to check for user refs.\n\t */\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\tWARN_ON(sock_owned_by_user(sk));\n\n\tpercpu_counter_inc(sk->sk_prot->orphan_count);\n\n\t/* Have we already been destroyed by a softirq or backlog? */\n\tif (state != TCP_CLOSE && sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\t/*\tThis is a (useful) BSD violating of the RFC. There is a\n\t *\tproblem with TCP as specified in that the other end could\n\t *\tkeep a socket open forever with no application left this end.\n\t *\tWe use a 1 minute timeout (about the same as BSD) then kill\n\t *\tour end. If they send after that then tough - BUT: long enough\n\t *\tthat we won't make the old 4*rto = almost no time - whoops\n\t *\treset mistake.\n\t *\n\t *\tNope, it was not mistake. It is really desired behaviour\n\t *\tf.e. on http servers, when such sockets are useless, but\n\t *\tconsume significant resources. Let's do it with special\n\t *\tlinger2\toption.\t\t\t\t\t--ANK\n\t */\n\n\tif (sk->sk_state == TCP_FIN_WAIT2) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tif (tp->linger2 < 0) {\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\t\t__NET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPABORTONLINGER);\n\t\t} else {\n\t\t\tconst int tmo = tcp_fin_time(sk);\n\n\t\t\tif (tmo > TCP_TIMEWAIT_LEN) {\n\t\t\t\tinet_csk_reset_keepalive_timer(sk,\n\t\t\t\t\t\ttmo - TCP_TIMEWAIT_LEN);\n\t\t\t} else {\n\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\tsk_mem_reclaim(sk);\n\t\tif (tcp_check_oom(sk, 0)) {\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\t\t__NET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPABORTONMEMORY);\n\t\t}\n\t}\n\n\tif (sk->sk_state == TCP_CLOSE) {\n\t\tstruct request_sock *req = tcp_sk(sk)->fastopen_rsk;\n\t\t/* We could get here with a non-NULL req if the socket is\n\t\t * aborted (e.g., closed with unread data) before 3WHS\n\t\t * finishes.\n\t\t */\n\t\tif (req)\n\t\t\treqsk_fastopen_remove(sk, req, false);\n\t\tinet_csk_destroy_sock(sk);\n\t}\n\t/* Otherwise, socket is reprieved until protocol close. */\n\nout:\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(tcp_close);\n\n/* These states need RST on ABORT according to RFC793 */\n\nstatic inline bool tcp_need_reset(int state)\n{\n\treturn (1 << state) &\n\t       (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT | TCPF_FIN_WAIT1 |\n\t\tTCPF_FIN_WAIT2 | TCPF_SYN_RECV);\n}\n\nint tcp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint err = 0;\n\tint old_state = sk->sk_state;\n\n\tif (old_state != TCP_CLOSE)\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\n\t/* ABORT function of RFC793 */\n\tif (old_state == TCP_LISTEN) {\n\t\tinet_csk_listen_stop(sk);\n\t} else if (unlikely(tp->repair)) {\n\t\tsk->sk_err = ECONNABORTED;\n\t} else if (tcp_need_reset(old_state) ||\n\t\t   (tp->snd_nxt != tp->write_seq &&\n\t\t    (1 << old_state) & (TCPF_CLOSING | TCPF_LAST_ACK))) {\n\t\t/* The last check adjusts for discrepancy of Linux wrt. RFC\n\t\t * states\n\t\t */\n\t\ttcp_send_active_reset(sk, gfp_any());\n\t\tsk->sk_err = ECONNRESET;\n\t} else if (old_state == TCP_SYN_SENT)\n\t\tsk->sk_err = ECONNRESET;\n\n\ttcp_clear_xmit_timers(sk);\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\ttcp_write_queue_purge(sk);\n\ttcp_fastopen_active_disable_ofo_check(sk);\n\tskb_rbtree_purge(&tp->out_of_order_queue);\n\n\tinet->inet_dport = 0;\n\n\tif (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK))\n\t\tinet_reset_saddr(sk);\n\n\tsk->sk_shutdown = 0;\n\tsock_reset_flag(sk, SOCK_DONE);\n\ttp->srtt_us = 0;\n\ttp->write_seq += tp->max_window + 2;\n\tif (tp->write_seq == 0)\n\t\ttp->write_seq = 1;\n\ticsk->icsk_backoff = 0;\n\ttp->snd_cwnd = 2;\n\ticsk->icsk_probes_out = 0;\n\ttp->packets_out = 0;\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttp->snd_cwnd_cnt = 0;\n\ttp->window_clamp = 0;\n\ttcp_set_ca_state(sk, TCP_CA_Open);\n\ttcp_clear_retrans(tp);\n\tinet_csk_delack_init(sk);\n\ttcp_init_send_head(sk);\n\tmemset(&tp->rx_opt, 0, sizeof(tp->rx_opt));\n\t__sk_dst_reset(sk);\n\ttcp_saved_syn_free(tp);\n\n\t/* Clean up fastopen related fields */\n\ttcp_free_fastopen_req(tp);\n\tinet->defer_connect = 0;\n\n\tWARN_ON(inet->inet_num && !icsk->icsk_bind_hash);\n\n\tsk->sk_error_report(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(tcp_disconnect);\n\nstatic inline bool tcp_can_repair_sock(const struct sock *sk)\n{\n\treturn ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN) &&\n\t\t(sk->sk_state != TCP_LISTEN);\n}\n\nstatic int tcp_repair_set_window(struct tcp_sock *tp, char __user *optbuf, int len)\n{\n\tstruct tcp_repair_window opt;\n\n\tif (!tp->repair)\n\t\treturn -EPERM;\n\n\tif (len != sizeof(opt))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&opt, optbuf, sizeof(opt)))\n\t\treturn -EFAULT;\n\n\tif (opt.max_window < opt.snd_wnd)\n\t\treturn -EINVAL;\n\n\tif (after(opt.snd_wl1, tp->rcv_nxt + opt.rcv_wnd))\n\t\treturn -EINVAL;\n\n\tif (after(opt.rcv_wup, tp->rcv_nxt))\n\t\treturn -EINVAL;\n\n\ttp->snd_wl1\t= opt.snd_wl1;\n\ttp->snd_wnd\t= opt.snd_wnd;\n\ttp->max_window\t= opt.max_window;\n\n\ttp->rcv_wnd\t= opt.rcv_wnd;\n\ttp->rcv_wup\t= opt.rcv_wup;\n\n\treturn 0;\n}\n\nstatic int tcp_repair_options_est(struct tcp_sock *tp,\n\t\tstruct tcp_repair_opt __user *optbuf, unsigned int len)\n{\n\tstruct tcp_repair_opt opt;\n\n\twhile (len >= sizeof(opt)) {\n\t\tif (copy_from_user(&opt, optbuf, sizeof(opt)))\n\t\t\treturn -EFAULT;\n\n\t\toptbuf++;\n\t\tlen -= sizeof(opt);\n\n\t\tswitch (opt.opt_code) {\n\t\tcase TCPOPT_MSS:\n\t\t\ttp->rx_opt.mss_clamp = opt.opt_val;\n\t\t\tbreak;\n\t\tcase TCPOPT_WINDOW:\n\t\t\t{\n\t\t\t\tu16 snd_wscale = opt.opt_val & 0xFFFF;\n\t\t\t\tu16 rcv_wscale = opt.opt_val >> 16;\n\n\t\t\t\tif (snd_wscale > TCP_MAX_WSCALE || rcv_wscale > TCP_MAX_WSCALE)\n\t\t\t\t\treturn -EFBIG;\n\n\t\t\t\ttp->rx_opt.snd_wscale = snd_wscale;\n\t\t\t\ttp->rx_opt.rcv_wscale = rcv_wscale;\n\t\t\t\ttp->rx_opt.wscale_ok = 1;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase TCPOPT_SACK_PERM:\n\t\t\tif (opt.opt_val != 0)\n\t\t\t\treturn -EINVAL;\n\n\t\t\ttp->rx_opt.sack_ok |= TCP_SACK_SEEN;\n\t\t\tif (sysctl_tcp_fack)\n\t\t\t\ttcp_enable_fack(tp);\n\t\t\tbreak;\n\t\tcase TCPOPT_TIMESTAMP:\n\t\t\tif (opt.opt_val != 0)\n\t\t\t\treturn -EINVAL;\n\n\t\t\ttp->rx_opt.tstamp_ok = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n *\tSocket option code for TCP.\n */\nstatic int do_tcp_setsockopt(struct sock *sk, int level,\n\t\tint optname, char __user *optval, unsigned int optlen)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val;\n\tint err = 0;\n\n\t/* These are data/string values, all the others are ints */\n\tswitch (optname) {\n\tcase TCP_CONGESTION: {\n\t\tchar name[TCP_CA_NAME_MAX];\n\n\t\tif (optlen < 1)\n\t\t\treturn -EINVAL;\n\n\t\tval = strncpy_from_user(name, optval,\n\t\t\t\t\tmin_t(long, TCP_CA_NAME_MAX-1, optlen));\n\t\tif (val < 0)\n\t\t\treturn -EFAULT;\n\t\tname[val] = 0;\n\n\t\tlock_sock(sk);\n\t\terr = tcp_set_congestion_control(sk, name);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tdefault:\n\t\t/* fallthru */\n\t\tbreak;\n\t}\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase TCP_MAXSEG:\n\t\t/* Values greater than interface MTU won't take effect. However\n\t\t * at the point when this call is done we typically don't yet\n\t\t * know which interface is going to be used */\n\t\tif (val && (val < TCP_MIN_MSS || val > MAX_TCP_WINDOW)) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\ttp->rx_opt.user_mss = val;\n\t\tbreak;\n\n\tcase TCP_NODELAY:\n\t\tif (val) {\n\t\t\t/* TCP_NODELAY is weaker than TCP_CORK, so that\n\t\t\t * this option on corked socket is remembered, but\n\t\t\t * it is not activated until cork is cleared.\n\t\t\t *\n\t\t\t * However, when TCP_NODELAY is set we make\n\t\t\t * an explicit push, which overrides even TCP_CORK\n\t\t\t * for currently queued segments.\n\t\t\t */\n\t\t\ttp->nonagle |= TCP_NAGLE_OFF|TCP_NAGLE_PUSH;\n\t\t\ttcp_push_pending_frames(sk);\n\t\t} else {\n\t\t\ttp->nonagle &= ~TCP_NAGLE_OFF;\n\t\t}\n\t\tbreak;\n\n\tcase TCP_THIN_LINEAR_TIMEOUTS:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->thin_lto = val;\n\t\tbreak;\n\n\tcase TCP_THIN_DUPACK:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\tbreak;\n\n\tcase TCP_REPAIR:\n\t\tif (!tcp_can_repair_sock(sk))\n\t\t\terr = -EPERM;\n\t\telse if (val == 1) {\n\t\t\ttp->repair = 1;\n\t\t\tsk->sk_reuse = SK_FORCE_REUSE;\n\t\t\ttp->repair_queue = TCP_NO_QUEUE;\n\t\t} else if (val == 0) {\n\t\t\ttp->repair = 0;\n\t\t\tsk->sk_reuse = SK_NO_REUSE;\n\t\t\ttcp_send_window_probe(sk);\n\t\t} else\n\t\t\terr = -EINVAL;\n\n\t\tbreak;\n\n\tcase TCP_REPAIR_QUEUE:\n\t\tif (!tp->repair)\n\t\t\terr = -EPERM;\n\t\telse if (val < TCP_QUEUES_NR)\n\t\t\ttp->repair_queue = val;\n\t\telse\n\t\t\terr = -EINVAL;\n\t\tbreak;\n\n\tcase TCP_QUEUE_SEQ:\n\t\tif (sk->sk_state != TCP_CLOSE)\n\t\t\terr = -EPERM;\n\t\telse if (tp->repair_queue == TCP_SEND_QUEUE)\n\t\t\ttp->write_seq = val;\n\t\telse if (tp->repair_queue == TCP_RECV_QUEUE)\n\t\t\ttp->rcv_nxt = val;\n\t\telse\n\t\t\terr = -EINVAL;\n\t\tbreak;\n\n\tcase TCP_REPAIR_OPTIONS:\n\t\tif (!tp->repair)\n\t\t\terr = -EINVAL;\n\t\telse if (sk->sk_state == TCP_ESTABLISHED)\n\t\t\terr = tcp_repair_options_est(tp,\n\t\t\t\t\t(struct tcp_repair_opt __user *)optval,\n\t\t\t\t\toptlen);\n\t\telse\n\t\t\terr = -EPERM;\n\t\tbreak;\n\n\tcase TCP_CORK:\n\t\t/* When set indicates to always queue non-full frames.\n\t\t * Later the user clears this option and we transmit\n\t\t * any pending partial frames in the queue.  This is\n\t\t * meant to be used alongside sendfile() to get properly\n\t\t * filled frames when the user (for example) must write\n\t\t * out headers with a write() call first and then use\n\t\t * sendfile to send out the data parts.\n\t\t *\n\t\t * TCP_CORK can be set together with TCP_NODELAY and it is\n\t\t * stronger than TCP_NODELAY.\n\t\t */\n\t\tif (val) {\n\t\t\ttp->nonagle |= TCP_NAGLE_CORK;\n\t\t} else {\n\t\t\ttp->nonagle &= ~TCP_NAGLE_CORK;\n\t\t\tif (tp->nonagle&TCP_NAGLE_OFF)\n\t\t\t\ttp->nonagle |= TCP_NAGLE_PUSH;\n\t\t\ttcp_push_pending_frames(sk);\n\t\t}\n\t\tbreak;\n\n\tcase TCP_KEEPIDLE:\n\t\tif (val < 1 || val > MAX_TCP_KEEPIDLE)\n\t\t\terr = -EINVAL;\n\t\telse {\n\t\t\ttp->keepalive_time = val * HZ;\n\t\t\tif (sock_flag(sk, SOCK_KEEPOPEN) &&\n\t\t\t    !((1 << sk->sk_state) &\n\t\t\t      (TCPF_CLOSE | TCPF_LISTEN))) {\n\t\t\t\tu32 elapsed = keepalive_time_elapsed(tp);\n\t\t\t\tif (tp->keepalive_time > elapsed)\n\t\t\t\t\telapsed = tp->keepalive_time - elapsed;\n\t\t\t\telse\n\t\t\t\t\telapsed = 0;\n\t\t\t\tinet_csk_reset_keepalive_timer(sk, elapsed);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase TCP_KEEPINTVL:\n\t\tif (val < 1 || val > MAX_TCP_KEEPINTVL)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->keepalive_intvl = val * HZ;\n\t\tbreak;\n\tcase TCP_KEEPCNT:\n\t\tif (val < 1 || val > MAX_TCP_KEEPCNT)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->keepalive_probes = val;\n\t\tbreak;\n\tcase TCP_SYNCNT:\n\t\tif (val < 1 || val > MAX_TCP_SYNCNT)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ticsk->icsk_syn_retries = val;\n\t\tbreak;\n\n\tcase TCP_SAVE_SYN:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->save_syn = val;\n\t\tbreak;\n\n\tcase TCP_LINGER2:\n\t\tif (val < 0)\n\t\t\ttp->linger2 = -1;\n\t\telse if (val > net->ipv4.sysctl_tcp_fin_timeout / HZ)\n\t\t\ttp->linger2 = 0;\n\t\telse\n\t\t\ttp->linger2 = val * HZ;\n\t\tbreak;\n\n\tcase TCP_DEFER_ACCEPT:\n\t\t/* Translate value in seconds to number of retransmits */\n\t\ticsk->icsk_accept_queue.rskq_defer_accept =\n\t\t\tsecs_to_retrans(val, TCP_TIMEOUT_INIT / HZ,\n\t\t\t\t\tTCP_RTO_MAX / HZ);\n\t\tbreak;\n\n\tcase TCP_WINDOW_CLAMP:\n\t\tif (!val) {\n\t\t\tif (sk->sk_state != TCP_CLOSE) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttp->window_clamp = 0;\n\t\t} else\n\t\t\ttp->window_clamp = val < SOCK_MIN_RCVBUF / 2 ?\n\t\t\t\t\t\tSOCK_MIN_RCVBUF / 2 : val;\n\t\tbreak;\n\n\tcase TCP_QUICKACK:\n\t\tif (!val) {\n\t\t\ticsk->icsk_ack.pingpong = 1;\n\t\t} else {\n\t\t\ticsk->icsk_ack.pingpong = 0;\n\t\t\tif ((1 << sk->sk_state) &\n\t\t\t    (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT) &&\n\t\t\t    inet_csk_ack_scheduled(sk)) {\n\t\t\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED;\n\t\t\t\ttcp_cleanup_rbuf(sk, 1);\n\t\t\t\tif (!(val & 1))\n\t\t\t\t\ticsk->icsk_ack.pingpong = 1;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tcase TCP_MD5SIG:\n\t\t/* Read the IP->Key mappings from userspace */\n\t\terr = tp->af_specific->md5_parse(sk, optval, optlen);\n\t\tbreak;\n#endif\n\tcase TCP_USER_TIMEOUT:\n\t\t/* Cap the max time in ms TCP will retry or probe the window\n\t\t * before giving up and aborting (ETIMEDOUT) a connection.\n\t\t */\n\t\tif (val < 0)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ticsk->icsk_user_timeout = msecs_to_jiffies(val);\n\t\tbreak;\n\n\tcase TCP_FASTOPEN:\n\t\tif (val >= 0 && ((1 << sk->sk_state) & (TCPF_CLOSE |\n\t\t    TCPF_LISTEN))) {\n\t\t\ttcp_fastopen_init_key_once(true);\n\n\t\t\tfastopen_queue_tune(sk, val);\n\t\t} else {\n\t\t\terr = -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase TCP_FASTOPEN_CONNECT:\n\t\tif (val > 1 || val < 0) {\n\t\t\terr = -EINVAL;\n\t\t} else if (sysctl_tcp_fastopen & TFO_CLIENT_ENABLE) {\n\t\t\tif (sk->sk_state == TCP_CLOSE)\n\t\t\t\ttp->fastopen_connect = val;\n\t\t\telse\n\t\t\t\terr = -EINVAL;\n\t\t} else {\n\t\t\terr = -EOPNOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase TCP_TIMESTAMP:\n\t\tif (!tp->repair)\n\t\t\terr = -EPERM;\n\t\telse\n\t\t\ttp->tsoffset = val - tcp_time_stamp;\n\t\tbreak;\n\tcase TCP_REPAIR_WINDOW:\n\t\terr = tcp_repair_set_window(tp, optval, optlen);\n\t\tbreak;\n\tcase TCP_NOTSENT_LOWAT:\n\t\ttp->notsent_lowat = val;\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\treturn err;\n}\n\nint tcp_setsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(tcp_setsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_tcp_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (level != SOL_TCP)\n\t\treturn inet_csk_compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t  optval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_tcp_setsockopt);\n#endif\n\nstatic void tcp_get_info_chrono_stats(const struct tcp_sock *tp,\n\t\t\t\t      struct tcp_info *info)\n{\n\tu64 stats[__TCP_CHRONO_MAX], total = 0;\n\tenum tcp_chrono i;\n\n\tfor (i = TCP_CHRONO_BUSY; i < __TCP_CHRONO_MAX; ++i) {\n\t\tstats[i] = tp->chrono_stat[i - 1];\n\t\tif (i == tp->chrono_type)\n\t\t\tstats[i] += tcp_time_stamp - tp->chrono_start;\n\t\tstats[i] *= USEC_PER_SEC / HZ;\n\t\ttotal += stats[i];\n\t}\n\n\tinfo->tcpi_busy_time = total;\n\tinfo->tcpi_rwnd_limited = stats[TCP_CHRONO_RWND_LIMITED];\n\tinfo->tcpi_sndbuf_limited = stats[TCP_CHRONO_SNDBUF_LIMITED];\n}\n\n/* Return information about state of tcp endpoint in API format. */\nvoid tcp_get_info(struct sock *sk, struct tcp_info *info)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk); /* iff sk_type == SOCK_STREAM */\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tu32 now, intv;\n\tu64 rate64;\n\tbool slow;\n\tu32 rate;\n\n\tmemset(info, 0, sizeof(*info));\n\tif (sk->sk_type != SOCK_STREAM)\n\t\treturn;\n\n\tinfo->tcpi_state = sk_state_load(sk);\n\n\t/* Report meaningful fields for all TCP states, including listeners */\n\trate = READ_ONCE(sk->sk_pacing_rate);\n\trate64 = rate != ~0U ? rate : ~0ULL;\n\tinfo->tcpi_pacing_rate = rate64;\n\n\trate = READ_ONCE(sk->sk_max_pacing_rate);\n\trate64 = rate != ~0U ? rate : ~0ULL;\n\tinfo->tcpi_max_pacing_rate = rate64;\n\n\tinfo->tcpi_reordering = tp->reordering;\n\tinfo->tcpi_snd_cwnd = tp->snd_cwnd;\n\n\tif (info->tcpi_state == TCP_LISTEN) {\n\t\t/* listeners aliased fields :\n\t\t * tcpi_unacked -> Number of children ready for accept()\n\t\t * tcpi_sacked  -> max backlog\n\t\t */\n\t\tinfo->tcpi_unacked = sk->sk_ack_backlog;\n\t\tinfo->tcpi_sacked = sk->sk_max_ack_backlog;\n\t\treturn;\n\t}\n\n\tslow = lock_sock_fast(sk);\n\n\tinfo->tcpi_ca_state = icsk->icsk_ca_state;\n\tinfo->tcpi_retransmits = icsk->icsk_retransmits;\n\tinfo->tcpi_probes = icsk->icsk_probes_out;\n\tinfo->tcpi_backoff = icsk->icsk_backoff;\n\n\tif (tp->rx_opt.tstamp_ok)\n\t\tinfo->tcpi_options |= TCPI_OPT_TIMESTAMPS;\n\tif (tcp_is_sack(tp))\n\t\tinfo->tcpi_options |= TCPI_OPT_SACK;\n\tif (tp->rx_opt.wscale_ok) {\n\t\tinfo->tcpi_options |= TCPI_OPT_WSCALE;\n\t\tinfo->tcpi_snd_wscale = tp->rx_opt.snd_wscale;\n\t\tinfo->tcpi_rcv_wscale = tp->rx_opt.rcv_wscale;\n\t}\n\n\tif (tp->ecn_flags & TCP_ECN_OK)\n\t\tinfo->tcpi_options |= TCPI_OPT_ECN;\n\tif (tp->ecn_flags & TCP_ECN_SEEN)\n\t\tinfo->tcpi_options |= TCPI_OPT_ECN_SEEN;\n\tif (tp->syn_data_acked)\n\t\tinfo->tcpi_options |= TCPI_OPT_SYN_DATA;\n\n\tinfo->tcpi_rto = jiffies_to_usecs(icsk->icsk_rto);\n\tinfo->tcpi_ato = jiffies_to_usecs(icsk->icsk_ack.ato);\n\tinfo->tcpi_snd_mss = tp->mss_cache;\n\tinfo->tcpi_rcv_mss = icsk->icsk_ack.rcv_mss;\n\n\tinfo->tcpi_unacked = tp->packets_out;\n\tinfo->tcpi_sacked = tp->sacked_out;\n\n\tinfo->tcpi_lost = tp->lost_out;\n\tinfo->tcpi_retrans = tp->retrans_out;\n\tinfo->tcpi_fackets = tp->fackets_out;\n\n\tnow = tcp_time_stamp;\n\tinfo->tcpi_last_data_sent = jiffies_to_msecs(now - tp->lsndtime);\n\tinfo->tcpi_last_data_recv = jiffies_to_msecs(now - icsk->icsk_ack.lrcvtime);\n\tinfo->tcpi_last_ack_recv = jiffies_to_msecs(now - tp->rcv_tstamp);\n\n\tinfo->tcpi_pmtu = icsk->icsk_pmtu_cookie;\n\tinfo->tcpi_rcv_ssthresh = tp->rcv_ssthresh;\n\tinfo->tcpi_rtt = tp->srtt_us >> 3;\n\tinfo->tcpi_rttvar = tp->mdev_us >> 2;\n\tinfo->tcpi_snd_ssthresh = tp->snd_ssthresh;\n\tinfo->tcpi_advmss = tp->advmss;\n\n\tinfo->tcpi_rcv_rtt = tp->rcv_rtt_est.rtt_us >> 3;\n\tinfo->tcpi_rcv_space = tp->rcvq_space.space;\n\n\tinfo->tcpi_total_retrans = tp->total_retrans;\n\n\tinfo->tcpi_bytes_acked = tp->bytes_acked;\n\tinfo->tcpi_bytes_received = tp->bytes_received;\n\tinfo->tcpi_notsent_bytes = max_t(int, 0, tp->write_seq - tp->snd_nxt);\n\ttcp_get_info_chrono_stats(tp, info);\n\n\tinfo->tcpi_segs_out = tp->segs_out;\n\tinfo->tcpi_segs_in = tp->segs_in;\n\n\tinfo->tcpi_min_rtt = tcp_min_rtt(tp);\n\tinfo->tcpi_data_segs_in = tp->data_segs_in;\n\tinfo->tcpi_data_segs_out = tp->data_segs_out;\n\n\tinfo->tcpi_delivery_rate_app_limited = tp->rate_app_limited ? 1 : 0;\n\trate = READ_ONCE(tp->rate_delivered);\n\tintv = READ_ONCE(tp->rate_interval_us);\n\tif (rate && intv) {\n\t\trate64 = (u64)rate * tp->mss_cache * USEC_PER_SEC;\n\t\tdo_div(rate64, intv);\n\t\tinfo->tcpi_delivery_rate = rate64;\n\t}\n\tunlock_sock_fast(sk, slow);\n}\nEXPORT_SYMBOL_GPL(tcp_get_info);\n\nstruct sk_buff *tcp_get_timestamping_opt_stats(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *stats;\n\tstruct tcp_info info;\n\n\tstats = alloc_skb(5 * nla_total_size_64bit(sizeof(u64)), GFP_ATOMIC);\n\tif (!stats)\n\t\treturn NULL;\n\n\ttcp_get_info_chrono_stats(tp, &info);\n\tnla_put_u64_64bit(stats, TCP_NLA_BUSY,\n\t\t\t  info.tcpi_busy_time, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_RWND_LIMITED,\n\t\t\t  info.tcpi_rwnd_limited, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_SNDBUF_LIMITED,\n\t\t\t  info.tcpi_sndbuf_limited, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_DATA_SEGS_OUT,\n\t\t\t  tp->data_segs_out, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_TOTAL_RETRANS,\n\t\t\t  tp->total_retrans, TCP_NLA_PAD);\n\treturn stats;\n}\n\nstatic int do_tcp_getsockopt(struct sock *sk, int level,\n\t\tint optname, char __user *optval, int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlen = min_t(unsigned int, len, sizeof(int));\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase TCP_MAXSEG:\n\t\tval = tp->mss_cache;\n\t\tif (!val && ((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN)))\n\t\t\tval = tp->rx_opt.user_mss;\n\t\tif (tp->repair)\n\t\t\tval = tp->rx_opt.mss_clamp;\n\t\tbreak;\n\tcase TCP_NODELAY:\n\t\tval = !!(tp->nonagle&TCP_NAGLE_OFF);\n\t\tbreak;\n\tcase TCP_CORK:\n\t\tval = !!(tp->nonagle&TCP_NAGLE_CORK);\n\t\tbreak;\n\tcase TCP_KEEPIDLE:\n\t\tval = keepalive_time_when(tp) / HZ;\n\t\tbreak;\n\tcase TCP_KEEPINTVL:\n\t\tval = keepalive_intvl_when(tp) / HZ;\n\t\tbreak;\n\tcase TCP_KEEPCNT:\n\t\tval = keepalive_probes(tp);\n\t\tbreak;\n\tcase TCP_SYNCNT:\n\t\tval = icsk->icsk_syn_retries ? : net->ipv4.sysctl_tcp_syn_retries;\n\t\tbreak;\n\tcase TCP_LINGER2:\n\t\tval = tp->linger2;\n\t\tif (val >= 0)\n\t\t\tval = (val ? : net->ipv4.sysctl_tcp_fin_timeout) / HZ;\n\t\tbreak;\n\tcase TCP_DEFER_ACCEPT:\n\t\tval = retrans_to_secs(icsk->icsk_accept_queue.rskq_defer_accept,\n\t\t\t\t      TCP_TIMEOUT_INIT / HZ, TCP_RTO_MAX / HZ);\n\t\tbreak;\n\tcase TCP_WINDOW_CLAMP:\n\t\tval = tp->window_clamp;\n\t\tbreak;\n\tcase TCP_INFO: {\n\t\tstruct tcp_info info;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\ttcp_get_info(sk, &info);\n\n\t\tlen = min_t(unsigned int, len, sizeof(info));\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &info, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_CC_INFO: {\n\t\tconst struct tcp_congestion_ops *ca_ops;\n\t\tunion tcp_cc_info info;\n\t\tsize_t sz = 0;\n\t\tint attr;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tca_ops = icsk->icsk_ca_ops;\n\t\tif (ca_ops && ca_ops->get_info)\n\t\t\tsz = ca_ops->get_info(sk, ~0U, &attr, &info);\n\n\t\tlen = min_t(unsigned int, len, sz);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &info, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_QUICKACK:\n\t\tval = !icsk->icsk_ack.pingpong;\n\t\tbreak;\n\n\tcase TCP_CONGESTION:\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tlen = min_t(unsigned int, len, TCP_CA_NAME_MAX);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, icsk->icsk_ca_ops->name, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\n\tcase TCP_THIN_LINEAR_TIMEOUTS:\n\t\tval = tp->thin_lto;\n\t\tbreak;\n\n\tcase TCP_THIN_DUPACK:\n\t\tval = 0;\n\t\tbreak;\n\n\tcase TCP_REPAIR:\n\t\tval = tp->repair;\n\t\tbreak;\n\n\tcase TCP_REPAIR_QUEUE:\n\t\tif (tp->repair)\n\t\t\tval = tp->repair_queue;\n\t\telse\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\n\tcase TCP_REPAIR_WINDOW: {\n\t\tstruct tcp_repair_window opt;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tif (len != sizeof(opt))\n\t\t\treturn -EINVAL;\n\n\t\tif (!tp->repair)\n\t\t\treturn -EPERM;\n\n\t\topt.snd_wl1\t= tp->snd_wl1;\n\t\topt.snd_wnd\t= tp->snd_wnd;\n\t\topt.max_window\t= tp->max_window;\n\t\topt.rcv_wnd\t= tp->rcv_wnd;\n\t\topt.rcv_wup\t= tp->rcv_wup;\n\n\t\tif (copy_to_user(optval, &opt, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_QUEUE_SEQ:\n\t\tif (tp->repair_queue == TCP_SEND_QUEUE)\n\t\t\tval = tp->write_seq;\n\t\telse if (tp->repair_queue == TCP_RECV_QUEUE)\n\t\t\tval = tp->rcv_nxt;\n\t\telse\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\n\tcase TCP_USER_TIMEOUT:\n\t\tval = jiffies_to_msecs(icsk->icsk_user_timeout);\n\t\tbreak;\n\n\tcase TCP_FASTOPEN:\n\t\tval = icsk->icsk_accept_queue.fastopenq.max_qlen;\n\t\tbreak;\n\n\tcase TCP_FASTOPEN_CONNECT:\n\t\tval = tp->fastopen_connect;\n\t\tbreak;\n\n\tcase TCP_TIMESTAMP:\n\t\tval = tcp_time_stamp + tp->tsoffset;\n\t\tbreak;\n\tcase TCP_NOTSENT_LOWAT:\n\t\tval = tp->notsent_lowat;\n\t\tbreak;\n\tcase TCP_SAVE_SYN:\n\t\tval = tp->save_syn;\n\t\tbreak;\n\tcase TCP_SAVED_SYN: {\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tlock_sock(sk);\n\t\tif (tp->saved_syn) {\n\t\t\tif (len < tp->saved_syn[0]) {\n\t\t\t\tif (put_user(tp->saved_syn[0], optlen)) {\n\t\t\t\t\trelease_sock(sk);\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tlen = tp->saved_syn[0];\n\t\t\tif (put_user(len, optlen)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tif (copy_to_user(optval, tp->saved_syn + 1, len)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\ttcp_saved_syn_free(tp);\n\t\t\trelease_sock(sk);\n\t\t} else {\n\t\t\trelease_sock(sk);\n\t\t\tlen = 0;\n\t\t\tif (put_user(len, optlen))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nint tcp_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(tcp_getsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_tcp_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tif (level != SOL_TCP)\n\t\treturn inet_csk_compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t  optval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_tcp_getsockopt);\n#endif\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic DEFINE_PER_CPU(struct tcp_md5sig_pool, tcp_md5sig_pool);\nstatic DEFINE_MUTEX(tcp_md5sig_mutex);\nstatic bool tcp_md5sig_pool_populated = false;\n\nstatic void __tcp_alloc_md5sig_pool(void)\n{\n\tstruct crypto_ahash *hash;\n\tint cpu;\n\n\thash = crypto_alloc_ahash(\"md5\", 0, CRYPTO_ALG_ASYNC);\n\tif (IS_ERR(hash))\n\t\treturn;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tvoid *scratch = per_cpu(tcp_md5sig_pool, cpu).scratch;\n\t\tstruct ahash_request *req;\n\n\t\tif (!scratch) {\n\t\t\tscratch = kmalloc_node(sizeof(union tcp_md5sum_block) +\n\t\t\t\t\t       sizeof(struct tcphdr),\n\t\t\t\t\t       GFP_KERNEL,\n\t\t\t\t\t       cpu_to_node(cpu));\n\t\t\tif (!scratch)\n\t\t\t\treturn;\n\t\t\tper_cpu(tcp_md5sig_pool, cpu).scratch = scratch;\n\t\t}\n\t\tif (per_cpu(tcp_md5sig_pool, cpu).md5_req)\n\t\t\tcontinue;\n\n\t\treq = ahash_request_alloc(hash, GFP_KERNEL);\n\t\tif (!req)\n\t\t\treturn;\n\n\t\tahash_request_set_callback(req, 0, NULL, NULL);\n\n\t\tper_cpu(tcp_md5sig_pool, cpu).md5_req = req;\n\t}\n\t/* before setting tcp_md5sig_pool_populated, we must commit all writes\n\t * to memory. See smp_rmb() in tcp_get_md5sig_pool()\n\t */\n\tsmp_wmb();\n\ttcp_md5sig_pool_populated = true;\n}\n\nbool tcp_alloc_md5sig_pool(void)\n{\n\tif (unlikely(!tcp_md5sig_pool_populated)) {\n\t\tmutex_lock(&tcp_md5sig_mutex);\n\n\t\tif (!tcp_md5sig_pool_populated)\n\t\t\t__tcp_alloc_md5sig_pool();\n\n\t\tmutex_unlock(&tcp_md5sig_mutex);\n\t}\n\treturn tcp_md5sig_pool_populated;\n}\nEXPORT_SYMBOL(tcp_alloc_md5sig_pool);\n\n\n/**\n *\ttcp_get_md5sig_pool - get md5sig_pool for this user\n *\n *\tWe use percpu structure, so if we succeed, we exit with preemption\n *\tand BH disabled, to make sure another thread or softirq handling\n *\twont try to get same context.\n */\nstruct tcp_md5sig_pool *tcp_get_md5sig_pool(void)\n{\n\tlocal_bh_disable();\n\n\tif (tcp_md5sig_pool_populated) {\n\t\t/* coupled with smp_wmb() in __tcp_alloc_md5sig_pool() */\n\t\tsmp_rmb();\n\t\treturn this_cpu_ptr(&tcp_md5sig_pool);\n\t}\n\tlocal_bh_enable();\n\treturn NULL;\n}\nEXPORT_SYMBOL(tcp_get_md5sig_pool);\n\nint tcp_md5_hash_skb_data(struct tcp_md5sig_pool *hp,\n\t\t\t  const struct sk_buff *skb, unsigned int header_len)\n{\n\tstruct scatterlist sg;\n\tconst struct tcphdr *tp = tcp_hdr(skb);\n\tstruct ahash_request *req = hp->md5_req;\n\tunsigned int i;\n\tconst unsigned int head_data_len = skb_headlen(skb) > header_len ?\n\t\t\t\t\t   skb_headlen(skb) - header_len : 0;\n\tconst struct skb_shared_info *shi = skb_shinfo(skb);\n\tstruct sk_buff *frag_iter;\n\n\tsg_init_table(&sg, 1);\n\n\tsg_set_buf(&sg, ((u8 *) tp) + header_len, head_data_len);\n\tahash_request_set_crypt(req, &sg, NULL, head_data_len);\n\tif (crypto_ahash_update(req))\n\t\treturn 1;\n\n\tfor (i = 0; i < shi->nr_frags; ++i) {\n\t\tconst struct skb_frag_struct *f = &shi->frags[i];\n\t\tunsigned int offset = f->page_offset;\n\t\tstruct page *page = skb_frag_page(f) + (offset >> PAGE_SHIFT);\n\n\t\tsg_set_page(&sg, page, skb_frag_size(f),\n\t\t\t    offset_in_page(offset));\n\t\tahash_request_set_crypt(req, &sg, NULL, skb_frag_size(f));\n\t\tif (crypto_ahash_update(req))\n\t\t\treturn 1;\n\t}\n\n\tskb_walk_frags(skb, frag_iter)\n\t\tif (tcp_md5_hash_skb_data(hp, frag_iter, 0))\n\t\t\treturn 1;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_md5_hash_skb_data);\n\nint tcp_md5_hash_key(struct tcp_md5sig_pool *hp, const struct tcp_md5sig_key *key)\n{\n\tstruct scatterlist sg;\n\n\tsg_init_one(&sg, key->key, key->keylen);\n\tahash_request_set_crypt(hp->md5_req, &sg, NULL, key->keylen);\n\treturn crypto_ahash_update(hp->md5_req);\n}\nEXPORT_SYMBOL(tcp_md5_hash_key);\n\n#endif\n\nvoid tcp_done(struct sock *sk)\n{\n\tstruct request_sock *req = tcp_sk(sk)->fastopen_rsk;\n\n\tif (sk->sk_state == TCP_SYN_SENT || sk->sk_state == TCP_SYN_RECV)\n\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ATTEMPTFAILS);\n\n\ttcp_set_state(sk, TCP_CLOSE);\n\ttcp_clear_xmit_timers(sk);\n\tif (req)\n\t\treqsk_fastopen_remove(sk, req, false);\n\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_state_change(sk);\n\telse\n\t\tinet_csk_destroy_sock(sk);\n}\nEXPORT_SYMBOL_GPL(tcp_done);\n\nint tcp_abort(struct sock *sk, int err)\n{\n\tif (!sk_fullsock(sk)) {\n\t\tif (sk->sk_state == TCP_NEW_SYN_RECV) {\n\t\t\tstruct request_sock *req = inet_reqsk(sk);\n\n\t\t\tlocal_bh_disable();\n\t\t\tinet_csk_reqsk_queue_drop_and_put(req->rsk_listener,\n\t\t\t\t\t\t\t  req);\n\t\t\tlocal_bh_enable();\n\t\t\treturn 0;\n\t\t}\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/* Don't race with userspace socket closes such as tcp_close. */\n\tlock_sock(sk);\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\tinet_csk_listen_stop(sk);\n\t}\n\n\t/* Don't race with BH socket closes such as inet_csk_listen_stop. */\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tsk->sk_err = err;\n\t\t/* This barrier is coupled with smp_rmb() in tcp_poll() */\n\t\tsmp_wmb();\n\t\tsk->sk_error_report(sk);\n\t\tif (tcp_need_reset(sk->sk_state))\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\ttcp_done(sk);\n\t}\n\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\trelease_sock(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(tcp_abort);\n\nextern struct tcp_congestion_ops tcp_reno;\n\nstatic __initdata unsigned long thash_entries;\nstatic int __init set_thash_entries(char *str)\n{\n\tssize_t ret;\n\n\tif (!str)\n\t\treturn 0;\n\n\tret = kstrtoul(str, 0, &thash_entries);\n\tif (ret)\n\t\treturn 0;\n\n\treturn 1;\n}\n__setup(\"thash_entries=\", set_thash_entries);\n\nstatic void __init tcp_init_mem(void)\n{\n\tunsigned long limit = nr_free_buffer_pages() / 16;\n\n\tlimit = max(limit, 128UL);\n\tsysctl_tcp_mem[0] = limit / 4 * 3;\t\t/* 4.68 % */\n\tsysctl_tcp_mem[1] = limit;\t\t\t/* 6.25 % */\n\tsysctl_tcp_mem[2] = sysctl_tcp_mem[0] * 2;\t/* 9.37 % */\n}\n\nvoid __init tcp_init(void)\n{\n\tint max_rshare, max_wshare, cnt;\n\tunsigned long limit;\n\tunsigned int i;\n\n\tBUILD_BUG_ON(sizeof(struct tcp_skb_cb) >\n\t\t     FIELD_SIZEOF(struct sk_buff, cb));\n\n\tpercpu_counter_init(&tcp_sockets_allocated, 0, GFP_KERNEL);\n\tpercpu_counter_init(&tcp_orphan_count, 0, GFP_KERNEL);\n\tinet_hashinfo_init(&tcp_hashinfo);\n\ttcp_hashinfo.bind_bucket_cachep =\n\t\tkmem_cache_create(\"tcp_bind_bucket\",\n\t\t\t\t  sizeof(struct inet_bind_bucket), 0,\n\t\t\t\t  SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);\n\n\t/* Size and allocate the main established and bind bucket\n\t * hash tables.\n\t *\n\t * The methodology is similar to that of the buffer cache.\n\t */\n\ttcp_hashinfo.ehash =\n\t\talloc_large_system_hash(\"TCP established\",\n\t\t\t\t\tsizeof(struct inet_ehash_bucket),\n\t\t\t\t\tthash_entries,\n\t\t\t\t\t17, /* one slot per 128 KB of memory */\n\t\t\t\t\t0,\n\t\t\t\t\tNULL,\n\t\t\t\t\t&tcp_hashinfo.ehash_mask,\n\t\t\t\t\t0,\n\t\t\t\t\tthash_entries ? 0 : 512 * 1024);\n\tfor (i = 0; i <= tcp_hashinfo.ehash_mask; i++)\n\t\tINIT_HLIST_NULLS_HEAD(&tcp_hashinfo.ehash[i].chain, i);\n\n\tif (inet_ehash_locks_alloc(&tcp_hashinfo))\n\t\tpanic(\"TCP: failed to alloc ehash_locks\");\n\ttcp_hashinfo.bhash =\n\t\talloc_large_system_hash(\"TCP bind\",\n\t\t\t\t\tsizeof(struct inet_bind_hashbucket),\n\t\t\t\t\ttcp_hashinfo.ehash_mask + 1,\n\t\t\t\t\t17, /* one slot per 128 KB of memory */\n\t\t\t\t\t0,\n\t\t\t\t\t&tcp_hashinfo.bhash_size,\n\t\t\t\t\tNULL,\n\t\t\t\t\t0,\n\t\t\t\t\t64 * 1024);\n\ttcp_hashinfo.bhash_size = 1U << tcp_hashinfo.bhash_size;\n\tfor (i = 0; i < tcp_hashinfo.bhash_size; i++) {\n\t\tspin_lock_init(&tcp_hashinfo.bhash[i].lock);\n\t\tINIT_HLIST_HEAD(&tcp_hashinfo.bhash[i].chain);\n\t}\n\n\n\tcnt = tcp_hashinfo.ehash_mask + 1;\n\tsysctl_tcp_max_orphans = cnt / 2;\n\n\ttcp_init_mem();\n\t/* Set per-socket limits to no more than 1/128 the pressure threshold */\n\tlimit = nr_free_buffer_pages() << (PAGE_SHIFT - 7);\n\tmax_wshare = min(4UL*1024*1024, limit);\n\tmax_rshare = min(6UL*1024*1024, limit);\n\n\tsysctl_tcp_wmem[0] = SK_MEM_QUANTUM;\n\tsysctl_tcp_wmem[1] = 16*1024;\n\tsysctl_tcp_wmem[2] = max(64*1024, max_wshare);\n\n\tsysctl_tcp_rmem[0] = SK_MEM_QUANTUM;\n\tsysctl_tcp_rmem[1] = 87380;\n\tsysctl_tcp_rmem[2] = max(87380, max_rshare);\n\n\tpr_info(\"Hash tables configured (established %u bind %u)\\n\",\n\t\ttcp_hashinfo.ehash_mask + 1, tcp_hashinfo.bhash_size);\n\n\ttcp_v4_init();\n\ttcp_metrics_init();\n\tBUG_ON(tcp_register_congestion_control(&tcp_reno) != 0);\n\ttcp_tasklet_init();\n}\n"], "fixing_code": ["/*\n * INET\t\tAn implementation of the TCP/IP protocol suite for the LINUX\n *\t\toperating system.  INET is implemented using the  BSD Socket\n *\t\tinterface as the means of communication with the user level.\n *\n *\t\tImplementation of the Transmission Control Protocol(TCP).\n *\n * Authors:\tRoss Biro\n *\t\tFred N. van Kempen, <waltje@uWalt.NL.Mugnet.ORG>\n *\t\tMark Evans, <evansmp@uhura.aston.ac.uk>\n *\t\tCorey Minyard <wf-rch!minyard@relay.EU.net>\n *\t\tFlorian La Roche, <flla@stud.uni-sb.de>\n *\t\tCharles Hedrick, <hedrick@klinzhai.rutgers.edu>\n *\t\tLinus Torvalds, <torvalds@cs.helsinki.fi>\n *\t\tAlan Cox, <gw4pts@gw4pts.ampr.org>\n *\t\tMatthew Dillon, <dillon@apollo.west.oic.com>\n *\t\tArnt Gulbrandsen, <agulbra@nvg.unit.no>\n *\t\tJorge Cwik, <jorge@laser.satlink.net>\n *\n * Fixes:\n *\t\tAlan Cox\t:\tNumerous verify_area() calls\n *\t\tAlan Cox\t:\tSet the ACK bit on a reset\n *\t\tAlan Cox\t:\tStopped it crashing if it closed while\n *\t\t\t\t\tsk->inuse=1 and was trying to connect\n *\t\t\t\t\t(tcp_err()).\n *\t\tAlan Cox\t:\tAll icmp error handling was broken\n *\t\t\t\t\tpointers passed where wrong and the\n *\t\t\t\t\tsocket was looked up backwards. Nobody\n *\t\t\t\t\ttested any icmp error code obviously.\n *\t\tAlan Cox\t:\ttcp_err() now handled properly. It\n *\t\t\t\t\twakes people on errors. poll\n *\t\t\t\t\tbehaves and the icmp error race\n *\t\t\t\t\thas gone by moving it into sock.c\n *\t\tAlan Cox\t:\ttcp_send_reset() fixed to work for\n *\t\t\t\t\teverything not just packets for\n *\t\t\t\t\tunknown sockets.\n *\t\tAlan Cox\t:\ttcp option processing.\n *\t\tAlan Cox\t:\tReset tweaked (still not 100%) [Had\n *\t\t\t\t\tsyn rule wrong]\n *\t\tHerp Rosmanith  :\tMore reset fixes\n *\t\tAlan Cox\t:\tNo longer acks invalid rst frames.\n *\t\t\t\t\tAcking any kind of RST is right out.\n *\t\tAlan Cox\t:\tSets an ignore me flag on an rst\n *\t\t\t\t\treceive otherwise odd bits of prattle\n *\t\t\t\t\tescape still\n *\t\tAlan Cox\t:\tFixed another acking RST frame bug.\n *\t\t\t\t\tShould stop LAN workplace lockups.\n *\t\tAlan Cox\t: \tSome tidyups using the new skb list\n *\t\t\t\t\tfacilities\n *\t\tAlan Cox\t:\tsk->keepopen now seems to work\n *\t\tAlan Cox\t:\tPulls options out correctly on accepts\n *\t\tAlan Cox\t:\tFixed assorted sk->rqueue->next errors\n *\t\tAlan Cox\t:\tPSH doesn't end a TCP read. Switched a\n *\t\t\t\t\tbit to skb ops.\n *\t\tAlan Cox\t:\tTidied tcp_data to avoid a potential\n *\t\t\t\t\tnasty.\n *\t\tAlan Cox\t:\tAdded some better commenting, as the\n *\t\t\t\t\ttcp is hard to follow\n *\t\tAlan Cox\t:\tRemoved incorrect check for 20 * psh\n *\tMichael O'Reilly\t:\tack < copied bug fix.\n *\tJohannes Stille\t\t:\tMisc tcp fixes (not all in yet).\n *\t\tAlan Cox\t:\tFIN with no memory -> CRASH\n *\t\tAlan Cox\t:\tAdded socket option proto entries.\n *\t\t\t\t\tAlso added awareness of them to accept.\n *\t\tAlan Cox\t:\tAdded TCP options (SOL_TCP)\n *\t\tAlan Cox\t:\tSwitched wakeup calls to callbacks,\n *\t\t\t\t\tso the kernel can layer network\n *\t\t\t\t\tsockets.\n *\t\tAlan Cox\t:\tUse ip_tos/ip_ttl settings.\n *\t\tAlan Cox\t:\tHandle FIN (more) properly (we hope).\n *\t\tAlan Cox\t:\tRST frames sent on unsynchronised\n *\t\t\t\t\tstate ack error.\n *\t\tAlan Cox\t:\tPut in missing check for SYN bit.\n *\t\tAlan Cox\t:\tAdded tcp_select_window() aka NET2E\n *\t\t\t\t\twindow non shrink trick.\n *\t\tAlan Cox\t:\tAdded a couple of small NET2E timer\n *\t\t\t\t\tfixes\n *\t\tCharles Hedrick :\tTCP fixes\n *\t\tToomas Tamm\t:\tTCP window fixes\n *\t\tAlan Cox\t:\tSmall URG fix to rlogin ^C ack fight\n *\t\tCharles Hedrick\t:\tRewrote most of it to actually work\n *\t\tLinus\t\t:\tRewrote tcp_read() and URG handling\n *\t\t\t\t\tcompletely\n *\t\tGerhard Koerting:\tFixed some missing timer handling\n *\t\tMatthew Dillon  :\tReworked TCP machine states as per RFC\n *\t\tGerhard Koerting:\tPC/TCP workarounds\n *\t\tAdam Caldwell\t:\tAssorted timer/timing errors\n *\t\tMatthew Dillon\t:\tFixed another RST bug\n *\t\tAlan Cox\t:\tMove to kernel side addressing changes.\n *\t\tAlan Cox\t:\tBeginning work on TCP fastpathing\n *\t\t\t\t\t(not yet usable)\n *\t\tArnt Gulbrandsen:\tTurbocharged tcp_check() routine.\n *\t\tAlan Cox\t:\tTCP fast path debugging\n *\t\tAlan Cox\t:\tWindow clamping\n *\t\tMichael Riepe\t:\tBug in tcp_check()\n *\t\tMatt Dillon\t:\tMore TCP improvements and RST bug fixes\n *\t\tMatt Dillon\t:\tYet more small nasties remove from the\n *\t\t\t\t\tTCP code (Be very nice to this man if\n *\t\t\t\t\ttcp finally works 100%) 8)\n *\t\tAlan Cox\t:\tBSD accept semantics.\n *\t\tAlan Cox\t:\tReset on closedown bug.\n *\tPeter De Schrijver\t:\tENOTCONN check missing in tcp_sendto().\n *\t\tMichael Pall\t:\tHandle poll() after URG properly in\n *\t\t\t\t\tall cases.\n *\t\tMichael Pall\t:\tUndo the last fix in tcp_read_urg()\n *\t\t\t\t\t(multi URG PUSH broke rlogin).\n *\t\tMichael Pall\t:\tFix the multi URG PUSH problem in\n *\t\t\t\t\ttcp_readable(), poll() after URG\n *\t\t\t\t\tworks now.\n *\t\tMichael Pall\t:\trecv(...,MSG_OOB) never blocks in the\n *\t\t\t\t\tBSD api.\n *\t\tAlan Cox\t:\tChanged the semantics of sk->socket to\n *\t\t\t\t\tfix a race and a signal problem with\n *\t\t\t\t\taccept() and async I/O.\n *\t\tAlan Cox\t:\tRelaxed the rules on tcp_sendto().\n *\t\tYury Shevchuk\t:\tReally fixed accept() blocking problem.\n *\t\tCraig I. Hagan  :\tAllow for BSD compatible TIME_WAIT for\n *\t\t\t\t\tclients/servers which listen in on\n *\t\t\t\t\tfixed ports.\n *\t\tAlan Cox\t:\tCleaned the above up and shrank it to\n *\t\t\t\t\ta sensible code size.\n *\t\tAlan Cox\t:\tSelf connect lockup fix.\n *\t\tAlan Cox\t:\tNo connect to multicast.\n *\t\tRoss Biro\t:\tClose unaccepted children on master\n *\t\t\t\t\tsocket close.\n *\t\tAlan Cox\t:\tReset tracing code.\n *\t\tAlan Cox\t:\tSpurious resets on shutdown.\n *\t\tAlan Cox\t:\tGiant 15 minute/60 second timer error\n *\t\tAlan Cox\t:\tSmall whoops in polling before an\n *\t\t\t\t\taccept.\n *\t\tAlan Cox\t:\tKept the state trace facility since\n *\t\t\t\t\tit's handy for debugging.\n *\t\tAlan Cox\t:\tMore reset handler fixes.\n *\t\tAlan Cox\t:\tStarted rewriting the code based on\n *\t\t\t\t\tthe RFC's for other useful protocol\n *\t\t\t\t\treferences see: Comer, KA9Q NOS, and\n *\t\t\t\t\tfor a reference on the difference\n *\t\t\t\t\tbetween specifications and how BSD\n *\t\t\t\t\tworks see the 4.4lite source.\n *\t\tA.N.Kuznetsov\t:\tDon't time wait on completion of tidy\n *\t\t\t\t\tclose.\n *\t\tLinus Torvalds\t:\tFin/Shutdown & copied_seq changes.\n *\t\tLinus Torvalds\t:\tFixed BSD port reuse to work first syn\n *\t\tAlan Cox\t:\tReimplemented timers as per the RFC\n *\t\t\t\t\tand using multiple timers for sanity.\n *\t\tAlan Cox\t:\tSmall bug fixes, and a lot of new\n *\t\t\t\t\tcomments.\n *\t\tAlan Cox\t:\tFixed dual reader crash by locking\n *\t\t\t\t\tthe buffers (much like datagram.c)\n *\t\tAlan Cox\t:\tFixed stuck sockets in probe. A probe\n *\t\t\t\t\tnow gets fed up of retrying without\n *\t\t\t\t\t(even a no space) answer.\n *\t\tAlan Cox\t:\tExtracted closing code better\n *\t\tAlan Cox\t:\tFixed the closing state machine to\n *\t\t\t\t\tresemble the RFC.\n *\t\tAlan Cox\t:\tMore 'per spec' fixes.\n *\t\tJorge Cwik\t:\tEven faster checksumming.\n *\t\tAlan Cox\t:\ttcp_data() doesn't ack illegal PSH\n *\t\t\t\t\tonly frames. At least one pc tcp stack\n *\t\t\t\t\tgenerates them.\n *\t\tAlan Cox\t:\tCache last socket.\n *\t\tAlan Cox\t:\tPer route irtt.\n *\t\tMatt Day\t:\tpoll()->select() match BSD precisely on error\n *\t\tAlan Cox\t:\tNew buffers\n *\t\tMarc Tamsky\t:\tVarious sk->prot->retransmits and\n *\t\t\t\t\tsk->retransmits misupdating fixed.\n *\t\t\t\t\tFixed tcp_write_timeout: stuck close,\n *\t\t\t\t\tand TCP syn retries gets used now.\n *\t\tMark Yarvis\t:\tIn tcp_read_wakeup(), don't send an\n *\t\t\t\t\tack if state is TCP_CLOSED.\n *\t\tAlan Cox\t:\tLook up device on a retransmit - routes may\n *\t\t\t\t\tchange. Doesn't yet cope with MSS shrink right\n *\t\t\t\t\tbut it's a start!\n *\t\tMarc Tamsky\t:\tClosing in closing fixes.\n *\t\tMike Shaver\t:\tRFC1122 verifications.\n *\t\tAlan Cox\t:\trcv_saddr errors.\n *\t\tAlan Cox\t:\tBlock double connect().\n *\t\tAlan Cox\t:\tSmall hooks for enSKIP.\n *\t\tAlexey Kuznetsov:\tPath MTU discovery.\n *\t\tAlan Cox\t:\tSupport soft errors.\n *\t\tAlan Cox\t:\tFix MTU discovery pathological case\n *\t\t\t\t\twhen the remote claims no mtu!\n *\t\tMarc Tamsky\t:\tTCP_CLOSE fix.\n *\t\tColin (G3TNE)\t:\tSend a reset on syn ack replies in\n *\t\t\t\t\twindow but wrong (fixes NT lpd problems)\n *\t\tPedro Roque\t:\tBetter TCP window handling, delayed ack.\n *\t\tJoerg Reuter\t:\tNo modification of locked buffers in\n *\t\t\t\t\ttcp_do_retransmit()\n *\t\tEric Schenk\t:\tChanged receiver side silly window\n *\t\t\t\t\tavoidance algorithm to BSD style\n *\t\t\t\t\talgorithm. This doubles throughput\n *\t\t\t\t\tagainst machines running Solaris,\n *\t\t\t\t\tand seems to result in general\n *\t\t\t\t\timprovement.\n *\tStefan Magdalinski\t:\tadjusted tcp_readable() to fix FIONREAD\n *\tWilly Konynenberg\t:\tTransparent proxying support.\n *\tMike McLagan\t\t:\tRouting by source\n *\t\tKeith Owens\t:\tDo proper merging with partial SKB's in\n *\t\t\t\t\ttcp_do_sendmsg to avoid burstiness.\n *\t\tEric Schenk\t:\tFix fast close down bug with\n *\t\t\t\t\tshutdown() followed by close().\n *\t\tAndi Kleen \t:\tMake poll agree with SIGIO\n *\tSalvatore Sanfilippo\t:\tSupport SO_LINGER with linger == 1 and\n *\t\t\t\t\tlingertime == 0 (RFC 793 ABORT Call)\n *\tHirokazu Takahashi\t:\tUse copy_from_user() instead of\n *\t\t\t\t\tcsum_and_copy_from_user() if possible.\n *\n *\t\tThis program is free software; you can redistribute it and/or\n *\t\tmodify it under the terms of the GNU General Public License\n *\t\tas published by the Free Software Foundation; either version\n *\t\t2 of the License, or(at your option) any later version.\n *\n * Description of States:\n *\n *\tTCP_SYN_SENT\t\tsent a connection request, waiting for ack\n *\n *\tTCP_SYN_RECV\t\treceived a connection request, sent ack,\n *\t\t\t\twaiting for final ack in three-way handshake.\n *\n *\tTCP_ESTABLISHED\t\tconnection established\n *\n *\tTCP_FIN_WAIT1\t\tour side has shutdown, waiting to complete\n *\t\t\t\ttransmission of remaining buffered data\n *\n *\tTCP_FIN_WAIT2\t\tall buffered data sent, waiting for remote\n *\t\t\t\tto shutdown\n *\n *\tTCP_CLOSING\t\tboth sides have shutdown but we still have\n *\t\t\t\tdata we have to finish sending\n *\n *\tTCP_TIME_WAIT\t\ttimeout to catch resent junk before entering\n *\t\t\t\tclosed, can only be entered from FIN_WAIT2\n *\t\t\t\tor CLOSING.  Required because the other end\n *\t\t\t\tmay not have gotten our last ACK causing it\n *\t\t\t\tto retransmit the data packet (which we ignore)\n *\n *\tTCP_CLOSE_WAIT\t\tremote side has shutdown and is waiting for\n *\t\t\t\tus to finish writing our data and to shutdown\n *\t\t\t\t(we have to close() to move on to LAST_ACK)\n *\n *\tTCP_LAST_ACK\t\tout side has shutdown after remote has\n *\t\t\t\tshutdown.  There may still be data in our\n *\t\t\t\tbuffer that we have to finish sending\n *\n *\tTCP_CLOSE\t\tsocket is finished\n */\n\n#define pr_fmt(fmt) \"TCP: \" fmt\n\n#include <crypto/hash.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/types.h>\n#include <linux/fcntl.h>\n#include <linux/poll.h>\n#include <linux/inet_diag.h>\n#include <linux/init.h>\n#include <linux/fs.h>\n#include <linux/skbuff.h>\n#include <linux/scatterlist.h>\n#include <linux/splice.h>\n#include <linux/net.h>\n#include <linux/socket.h>\n#include <linux/random.h>\n#include <linux/bootmem.h>\n#include <linux/highmem.h>\n#include <linux/swap.h>\n#include <linux/cache.h>\n#include <linux/err.h>\n#include <linux/time.h>\n#include <linux/slab.h>\n\n#include <net/icmp.h>\n#include <net/inet_common.h>\n#include <net/tcp.h>\n#include <net/xfrm.h>\n#include <net/ip.h>\n#include <net/sock.h>\n\n#include <linux/uaccess.h>\n#include <asm/ioctls.h>\n#include <net/busy_poll.h>\n\nint sysctl_tcp_min_tso_segs __read_mostly = 2;\n\nint sysctl_tcp_autocorking __read_mostly = 1;\n\nstruct percpu_counter tcp_orphan_count;\nEXPORT_SYMBOL_GPL(tcp_orphan_count);\n\nlong sysctl_tcp_mem[3] __read_mostly;\nint sysctl_tcp_wmem[3] __read_mostly;\nint sysctl_tcp_rmem[3] __read_mostly;\n\nEXPORT_SYMBOL(sysctl_tcp_mem);\nEXPORT_SYMBOL(sysctl_tcp_rmem);\nEXPORT_SYMBOL(sysctl_tcp_wmem);\n\natomic_long_t tcp_memory_allocated;\t/* Current allocated memory. */\nEXPORT_SYMBOL(tcp_memory_allocated);\n\n/*\n * Current number of TCP sockets.\n */\nstruct percpu_counter tcp_sockets_allocated;\nEXPORT_SYMBOL(tcp_sockets_allocated);\n\n/*\n * TCP splice context\n */\nstruct tcp_splice_state {\n\tstruct pipe_inode_info *pipe;\n\tsize_t len;\n\tunsigned int flags;\n};\n\n/*\n * Pressure flag: try to collapse.\n * Technical note: it is used by multiple contexts non atomically.\n * All the __sk_mem_schedule() is of this nature: accounting\n * is strict, actions are advisory and have some latency.\n */\nint tcp_memory_pressure __read_mostly;\nEXPORT_SYMBOL(tcp_memory_pressure);\n\nvoid tcp_enter_memory_pressure(struct sock *sk)\n{\n\tif (!tcp_memory_pressure) {\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPMEMORYPRESSURES);\n\t\ttcp_memory_pressure = 1;\n\t}\n}\nEXPORT_SYMBOL(tcp_enter_memory_pressure);\n\n/* Convert seconds to retransmits based on initial and max timeout */\nstatic u8 secs_to_retrans(int seconds, int timeout, int rto_max)\n{\n\tu8 res = 0;\n\n\tif (seconds > 0) {\n\t\tint period = timeout;\n\n\t\tres = 1;\n\t\twhile (seconds > period && res < 255) {\n\t\t\tres++;\n\t\t\ttimeout <<= 1;\n\t\t\tif (timeout > rto_max)\n\t\t\t\ttimeout = rto_max;\n\t\t\tperiod += timeout;\n\t\t}\n\t}\n\treturn res;\n}\n\n/* Convert retransmits to seconds based on initial and max timeout */\nstatic int retrans_to_secs(u8 retrans, int timeout, int rto_max)\n{\n\tint period = 0;\n\n\tif (retrans > 0) {\n\t\tperiod = timeout;\n\t\twhile (--retrans) {\n\t\t\ttimeout <<= 1;\n\t\t\tif (timeout > rto_max)\n\t\t\t\ttimeout = rto_max;\n\t\t\tperiod += timeout;\n\t\t}\n\t}\n\treturn period;\n}\n\n/* Address-family independent initialization for a tcp_sock.\n *\n * NOTE: A lot of things set to zero explicitly by call to\n *       sk_alloc() so need not be done here.\n */\nvoid tcp_init_sock(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->out_of_order_queue = RB_ROOT;\n\ttcp_init_xmit_timers(sk);\n\ttcp_prequeue_init(tp);\n\tINIT_LIST_HEAD(&tp->tsq_node);\n\n\ticsk->icsk_rto = TCP_TIMEOUT_INIT;\n\ttp->mdev_us = jiffies_to_usecs(TCP_TIMEOUT_INIT);\n\tminmax_reset(&tp->rtt_min, tcp_time_stamp, ~0U);\n\n\t/* So many TCP implementations out there (incorrectly) count the\n\t * initial SYN frame in their delayed-ACK and congestion control\n\t * algorithms that we must have the following bandaid to talk\n\t * efficiently to them.  -DaveM\n\t */\n\ttp->snd_cwnd = TCP_INIT_CWND;\n\n\t/* There's a bubble in the pipe until at least the first ACK. */\n\ttp->app_limited = ~0U;\n\n\t/* See draft-stevens-tcpca-spec-01 for discussion of the\n\t * initialization of these values.\n\t */\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttp->snd_cwnd_clamp = ~0;\n\ttp->mss_cache = TCP_MSS_DEFAULT;\n\n\ttp->reordering = sock_net(sk)->ipv4.sysctl_tcp_reordering;\n\ttcp_assign_congestion_control(sk);\n\n\ttp->tsoffset = 0;\n\n\tsk->sk_state = TCP_CLOSE;\n\n\tsk->sk_write_space = sk_stream_write_space;\n\tsock_set_flag(sk, SOCK_USE_WRITE_QUEUE);\n\n\ticsk->icsk_sync_mss = tcp_sync_mss;\n\n\tsk->sk_sndbuf = sysctl_tcp_wmem[1];\n\tsk->sk_rcvbuf = sysctl_tcp_rmem[1];\n\n\tsk_sockets_allocated_inc(sk);\n}\nEXPORT_SYMBOL(tcp_init_sock);\n\nstatic void tcp_tx_timestamp(struct sock *sk, u16 tsflags, struct sk_buff *skb)\n{\n\tif (tsflags && skb) {\n\t\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\t\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\t\tsock_tx_timestamp(sk, tsflags, &shinfo->tx_flags);\n\t\tif (tsflags & SOF_TIMESTAMPING_TX_ACK)\n\t\t\ttcb->txstamp_ack = 1;\n\t\tif (tsflags & SOF_TIMESTAMPING_TX_RECORD_MASK)\n\t\t\tshinfo->tskey = TCP_SKB_CB(skb)->seq + skb->len - 1;\n\t}\n}\n\n/*\n *\tWait for a TCP event.\n *\n *\tNote that we don't need to lock the socket, as the upper poll layers\n *\ttake care of normal races (between the test and the event) and we don't\n *\tgo look at any of the socket buffers directly.\n */\nunsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)\n{\n\tunsigned int mask;\n\tstruct sock *sk = sock->sk;\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tint state;\n\n\tsock_rps_record_flow(sk);\n\n\tsock_poll_wait(file, sk_sleep(sk), wait);\n\n\tstate = sk_state_load(sk);\n\tif (state == TCP_LISTEN)\n\t\treturn inet_csk_listen_poll(sk);\n\n\t/* Socket is not locked. We are protected from async events\n\t * by poll logic and correct handling of state changes\n\t * made by other threads is impossible in any case.\n\t */\n\n\tmask = 0;\n\n\t/*\n\t * POLLHUP is certainly not done right. But poll() doesn't\n\t * have a notion of HUP in just one direction, and for a\n\t * socket the read side is more interesting.\n\t *\n\t * Some poll() documentation says that POLLHUP is incompatible\n\t * with the POLLOUT/POLLWR flags, so somebody should check this\n\t * all. But careful, it tends to be safer to return too many\n\t * bits than too few, and you can easily break real applications\n\t * if you don't tell them that something has hung up!\n\t *\n\t * Check-me.\n\t *\n\t * Check number 1. POLLHUP is _UNMASKABLE_ event (see UNIX98 and\n\t * our fs/select.c). It means that after we received EOF,\n\t * poll always returns immediately, making impossible poll() on write()\n\t * in state CLOSE_WAIT. One solution is evident --- to set POLLHUP\n\t * if and only if shutdown has been made in both directions.\n\t * Actually, it is interesting to look how Solaris and DUX\n\t * solve this dilemma. I would prefer, if POLLHUP were maskable,\n\t * then we could set it on SND_SHUTDOWN. BTW examples given\n\t * in Stevens' books assume exactly this behaviour, it explains\n\t * why POLLHUP is incompatible with POLLOUT.\t--ANK\n\t *\n\t * NOTE. Check for TCP_CLOSE is added. The goal is to prevent\n\t * blocking on fresh not-connected or disconnected socket. --ANK\n\t */\n\tif (sk->sk_shutdown == SHUTDOWN_MASK || state == TCP_CLOSE)\n\t\tmask |= POLLHUP;\n\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\tmask |= POLLIN | POLLRDNORM | POLLRDHUP;\n\n\t/* Connected or passive Fast Open socket? */\n\tif (state != TCP_SYN_SENT &&\n\t    (state != TCP_SYN_RECV || tp->fastopen_rsk)) {\n\t\tint target = sock_rcvlowat(sk, 0, INT_MAX);\n\n\t\tif (tp->urg_seq == tp->copied_seq &&\n\t\t    !sock_flag(sk, SOCK_URGINLINE) &&\n\t\t    tp->urg_data)\n\t\t\ttarget++;\n\n\t\tif (tp->rcv_nxt - tp->copied_seq >= target)\n\t\t\tmask |= POLLIN | POLLRDNORM;\n\n\t\tif (!(sk->sk_shutdown & SEND_SHUTDOWN)) {\n\t\t\tif (sk_stream_is_writeable(sk)) {\n\t\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t\t\t} else {  /* send SIGIO later */\n\t\t\t\tsk_set_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\t\t\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\n\t\t\t\t/* Race breaker. If space is freed after\n\t\t\t\t * wspace test but before the flags are set,\n\t\t\t\t * IO signal will be lost. Memory barrier\n\t\t\t\t * pairs with the input side.\n\t\t\t\t */\n\t\t\t\tsmp_mb__after_atomic();\n\t\t\t\tif (sk_stream_is_writeable(sk))\n\t\t\t\t\tmask |= POLLOUT | POLLWRNORM;\n\t\t\t}\n\t\t} else\n\t\t\tmask |= POLLOUT | POLLWRNORM;\n\n\t\tif (tp->urg_data & TCP_URG_VALID)\n\t\t\tmask |= POLLPRI;\n\t} else if (state == TCP_SYN_SENT && inet_sk(sk)->defer_connect) {\n\t\t/* Active TCP fastopen socket with defer_connect\n\t\t * Return POLLOUT so application can call write()\n\t\t * in order for kernel to generate SYN+data\n\t\t */\n\t\tmask |= POLLOUT | POLLWRNORM;\n\t}\n\t/* This barrier is coupled with smp_wmb() in tcp_reset() */\n\tsmp_rmb();\n\tif (sk->sk_err || !skb_queue_empty(&sk->sk_error_queue))\n\t\tmask |= POLLERR;\n\n\treturn mask;\n}\nEXPORT_SYMBOL(tcp_poll);\n\nint tcp_ioctl(struct sock *sk, int cmd, unsigned long arg)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint answ;\n\tbool slow;\n\n\tswitch (cmd) {\n\tcase SIOCINQ:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tslow = lock_sock_fast(sk);\n\t\tansw = tcp_inq(sk);\n\t\tunlock_sock_fast(sk, slow);\n\t\tbreak;\n\tcase SIOCATMARK:\n\t\tansw = tp->urg_data && tp->urg_seq == tp->copied_seq;\n\t\tbreak;\n\tcase SIOCOUTQ:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))\n\t\t\tansw = 0;\n\t\telse\n\t\t\tansw = tp->write_seq - tp->snd_una;\n\t\tbreak;\n\tcase SIOCOUTQNSD:\n\t\tif (sk->sk_state == TCP_LISTEN)\n\t\t\treturn -EINVAL;\n\n\t\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV))\n\t\t\tansw = 0;\n\t\telse\n\t\t\tansw = tp->write_seq - tp->snd_nxt;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn put_user(answ, (int __user *)arg);\n}\nEXPORT_SYMBOL(tcp_ioctl);\n\nstatic inline void tcp_mark_push(struct tcp_sock *tp, struct sk_buff *skb)\n{\n\tTCP_SKB_CB(skb)->tcp_flags |= TCPHDR_PSH;\n\ttp->pushed_seq = tp->write_seq;\n}\n\nstatic inline bool forced_push(const struct tcp_sock *tp)\n{\n\treturn after(tp->write_seq, tp->pushed_seq + (tp->max_window >> 1));\n}\n\nstatic void skb_entail(struct sock *sk, struct sk_buff *skb)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct tcp_skb_cb *tcb = TCP_SKB_CB(skb);\n\n\tskb->csum    = 0;\n\ttcb->seq     = tcb->end_seq = tp->write_seq;\n\ttcb->tcp_flags = TCPHDR_ACK;\n\ttcb->sacked  = 0;\n\t__skb_header_release(skb);\n\ttcp_add_write_queue_tail(sk, skb);\n\tsk->sk_wmem_queued += skb->truesize;\n\tsk_mem_charge(sk, skb->truesize);\n\tif (tp->nonagle & TCP_NAGLE_PUSH)\n\t\ttp->nonagle &= ~TCP_NAGLE_PUSH;\n\n\ttcp_slow_start_after_idle_check(sk);\n}\n\nstatic inline void tcp_mark_urg(struct tcp_sock *tp, int flags)\n{\n\tif (flags & MSG_OOB)\n\t\ttp->snd_up = tp->write_seq;\n}\n\n/* If a not yet filled skb is pushed, do not send it if\n * we have data packets in Qdisc or NIC queues :\n * Because TX completion will happen shortly, it gives a chance\n * to coalesce future sendmsg() payload into this skb, without\n * need for a timer, and with no latency trade off.\n * As packets containing data payload have a bigger truesize\n * than pure acks (dataless) packets, the last checks prevent\n * autocorking if we only have an ACK in Qdisc/NIC queues,\n * or if TX completion was delayed after we processed ACK packet.\n */\nstatic bool tcp_should_autocork(struct sock *sk, struct sk_buff *skb,\n\t\t\t\tint size_goal)\n{\n\treturn skb->len < size_goal &&\n\t       sysctl_tcp_autocorking &&\n\t       skb != tcp_write_queue_head(sk) &&\n\t       atomic_read(&sk->sk_wmem_alloc) > skb->truesize;\n}\n\nstatic void tcp_push(struct sock *sk, int flags, int mss_now,\n\t\t     int nonagle, int size_goal)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\n\tif (!tcp_send_head(sk))\n\t\treturn;\n\n\tskb = tcp_write_queue_tail(sk);\n\tif (!(flags & MSG_MORE) || forced_push(tp))\n\t\ttcp_mark_push(tp, skb);\n\n\ttcp_mark_urg(tp, flags);\n\n\tif (tcp_should_autocork(sk, skb, size_goal)) {\n\n\t\t/* avoid atomic op if TSQ_THROTTLED bit is already set */\n\t\tif (!test_bit(TSQ_THROTTLED, &sk->sk_tsq_flags)) {\n\t\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPAUTOCORKING);\n\t\t\tset_bit(TSQ_THROTTLED, &sk->sk_tsq_flags);\n\t\t}\n\t\t/* It is possible TX completion already happened\n\t\t * before we set TSQ_THROTTLED.\n\t\t */\n\t\tif (atomic_read(&sk->sk_wmem_alloc) > skb->truesize)\n\t\t\treturn;\n\t}\n\n\tif (flags & MSG_MORE)\n\t\tnonagle = TCP_NAGLE_CORK;\n\n\t__tcp_push_pending_frames(sk, mss_now, nonagle);\n}\n\nstatic int tcp_splice_data_recv(read_descriptor_t *rd_desc, struct sk_buff *skb,\n\t\t\t\tunsigned int offset, size_t len)\n{\n\tstruct tcp_splice_state *tss = rd_desc->arg.data;\n\tint ret;\n\n\tret = skb_splice_bits(skb, skb->sk, offset, tss->pipe,\n\t\t\t      min(rd_desc->count, len), tss->flags);\n\tif (ret > 0)\n\t\trd_desc->count -= ret;\n\treturn ret;\n}\n\nstatic int __tcp_splice_read(struct sock *sk, struct tcp_splice_state *tss)\n{\n\t/* Store TCP splice context information in read_descriptor_t. */\n\tread_descriptor_t rd_desc = {\n\t\t.arg.data = tss,\n\t\t.count\t  = tss->len,\n\t};\n\n\treturn tcp_read_sock(sk, &rd_desc, tcp_splice_data_recv);\n}\n\n/**\n *  tcp_splice_read - splice data from TCP socket to a pipe\n * @sock:\tsocket to splice from\n * @ppos:\tposition (not valid)\n * @pipe:\tpipe to splice to\n * @len:\tnumber of bytes to splice\n * @flags:\tsplice modifier flags\n *\n * Description:\n *    Will read pages from given socket and fill them into a pipe.\n *\n **/\nssize_t tcp_splice_read(struct socket *sock, loff_t *ppos,\n\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\tunsigned int flags)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct tcp_splice_state tss = {\n\t\t.pipe = pipe,\n\t\t.len = len,\n\t\t.flags = flags,\n\t};\n\tlong timeo;\n\tssize_t spliced;\n\tint ret;\n\n\tsock_rps_record_flow(sk);\n\t/*\n\t * We can't seek on a socket input\n\t */\n\tif (unlikely(*ppos))\n\t\treturn -ESPIPE;\n\n\tret = spliced = 0;\n\n\tlock_sock(sk);\n\n\ttimeo = sock_rcvtimeo(sk, sock->file->f_flags & O_NONBLOCK);\n\twhile (tss.len) {\n\t\tret = __tcp_splice_read(sk, &tss);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\telse if (!ret) {\n\t\t\tif (spliced)\n\t\t\t\tbreak;\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_err) {\n\t\t\t\tret = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\t/*\n\t\t\t\t * This occurs when user tries to read\n\t\t\t\t * from never connected socket.\n\t\t\t\t */\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE))\n\t\t\t\t\tret = -ENOTCONN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!timeo) {\n\t\t\t\tret = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t/* if __tcp_splice_read() got nothing while we have\n\t\t\t * an skb in receive queue, we do not want to loop.\n\t\t\t * This might happen with URG data.\n\t\t\t */\n\t\t\tif (!skb_queue_empty(&sk->sk_receive_queue))\n\t\t\t\tbreak;\n\t\t\tsk_wait_data(sk, &timeo, NULL);\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tret = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\ttss.len -= ret;\n\t\tspliced += ret;\n\n\t\tif (!timeo)\n\t\t\tbreak;\n\t\trelease_sock(sk);\n\t\tlock_sock(sk);\n\n\t\tif (sk->sk_err || sk->sk_state == TCP_CLOSE ||\n\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t    signal_pending(current))\n\t\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\n\tif (spliced)\n\t\treturn spliced;\n\n\treturn ret;\n}\nEXPORT_SYMBOL(tcp_splice_read);\n\nstruct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,\n\t\t\t\t    bool force_schedule)\n{\n\tstruct sk_buff *skb;\n\n\t/* The TCP header must be at least 32-bit aligned.  */\n\tsize = ALIGN(size, 4);\n\n\tif (unlikely(tcp_under_memory_pressure(sk)))\n\t\tsk_mem_reclaim_partial(sk);\n\n\tskb = alloc_skb_fclone(size + sk->sk_prot->max_header, gfp);\n\tif (likely(skb)) {\n\t\tbool mem_scheduled;\n\n\t\tif (force_schedule) {\n\t\t\tmem_scheduled = true;\n\t\t\tsk_forced_mem_schedule(sk, skb->truesize);\n\t\t} else {\n\t\t\tmem_scheduled = sk_wmem_schedule(sk, skb->truesize);\n\t\t}\n\t\tif (likely(mem_scheduled)) {\n\t\t\tskb_reserve(skb, sk->sk_prot->max_header);\n\t\t\t/*\n\t\t\t * Make sure that we have exactly size bytes\n\t\t\t * available to the caller, no more, no less.\n\t\t\t */\n\t\t\tskb->reserved_tailroom = skb->end - skb->tail - size;\n\t\t\treturn skb;\n\t\t}\n\t\t__kfree_skb(skb);\n\t} else {\n\t\tsk->sk_prot->enter_memory_pressure(sk);\n\t\tsk_stream_moderate_sndbuf(sk);\n\t}\n\treturn NULL;\n}\n\nstatic unsigned int tcp_xmit_size_goal(struct sock *sk, u32 mss_now,\n\t\t\t\t       int large_allowed)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 new_size_goal, size_goal;\n\n\tif (!large_allowed || !sk_can_gso(sk))\n\t\treturn mss_now;\n\n\t/* Note : tcp_tso_autosize() will eventually split this later */\n\tnew_size_goal = sk->sk_gso_max_size - 1 - MAX_TCP_HEADER;\n\tnew_size_goal = tcp_bound_to_half_wnd(tp, new_size_goal);\n\n\t/* We try hard to avoid divides here */\n\tsize_goal = tp->gso_segs * mss_now;\n\tif (unlikely(new_size_goal < size_goal ||\n\t\t     new_size_goal >= size_goal + mss_now)) {\n\t\ttp->gso_segs = min_t(u16, new_size_goal / mss_now,\n\t\t\t\t     sk->sk_gso_max_segs);\n\t\tsize_goal = tp->gso_segs * mss_now;\n\t}\n\n\treturn max(size_goal, mss_now);\n}\n\nstatic int tcp_send_mss(struct sock *sk, int *size_goal, int flags)\n{\n\tint mss_now;\n\n\tmss_now = tcp_current_mss(sk);\n\t*size_goal = tcp_xmit_size_goal(sk, mss_now, !(flags & MSG_OOB));\n\n\treturn mss_now;\n}\n\nstatic ssize_t do_tcp_sendpages(struct sock *sk, struct page *page, int offset,\n\t\t\t\tsize_t size, int flags)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint mss_now, size_goal;\n\tint err;\n\tssize_t copied;\n\tlong timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);\n\n\t/* Wait for a connection to finish. One exception is TCP Fast Open\n\t * (passive side) where data is allowed to be sent before a connection\n\t * is fully established.\n\t */\n\tif (((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) &&\n\t    !tcp_passive_fastopen(sk)) {\n\t\terr = sk_stream_wait_connect(sk, &timeo);\n\t\tif (err != 0)\n\t\t\tgoto out_err;\n\t}\n\n\tsk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\n\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\tcopied = 0;\n\n\terr = -EPIPE;\n\tif (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))\n\t\tgoto out_err;\n\n\twhile (size > 0) {\n\t\tstruct sk_buff *skb = tcp_write_queue_tail(sk);\n\t\tint copy, i;\n\t\tbool can_coalesce;\n\n\t\tif (!tcp_send_head(sk) || (copy = size_goal - skb->len) <= 0 ||\n\t\t    !tcp_skb_can_collapse_to(skb)) {\nnew_segment:\n\t\t\tif (!sk_stream_memory_free(sk))\n\t\t\t\tgoto wait_for_sndbuf;\n\n\t\t\tskb = sk_stream_alloc_skb(sk, 0, sk->sk_allocation,\n\t\t\t\t\t\t  skb_queue_empty(&sk->sk_write_queue));\n\t\t\tif (!skb)\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\tskb_entail(sk, skb);\n\t\t\tcopy = size_goal;\n\t\t}\n\n\t\tif (copy > size)\n\t\t\tcopy = size;\n\n\t\ti = skb_shinfo(skb)->nr_frags;\n\t\tcan_coalesce = skb_can_coalesce(skb, i, page, offset);\n\t\tif (!can_coalesce && i >= sysctl_max_skb_frags) {\n\t\t\ttcp_mark_push(tp, skb);\n\t\t\tgoto new_segment;\n\t\t}\n\t\tif (!sk_wmem_schedule(sk, copy))\n\t\t\tgoto wait_for_memory;\n\n\t\tif (can_coalesce) {\n\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t} else {\n\t\t\tget_page(page);\n\t\t\tskb_fill_page_desc(skb, i, page, offset, copy);\n\t\t}\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_SHARED_FRAG;\n\n\t\tskb->len += copy;\n\t\tskb->data_len += copy;\n\t\tskb->truesize += copy;\n\t\tsk->sk_wmem_queued += copy;\n\t\tsk_mem_charge(sk, copy);\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\ttp->write_seq += copy;\n\t\tTCP_SKB_CB(skb)->end_seq += copy;\n\t\ttcp_skb_pcount_set(skb, 0);\n\n\t\tif (!copied)\n\t\t\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_PSH;\n\n\t\tcopied += copy;\n\t\toffset += copy;\n\t\tsize -= copy;\n\t\tif (!size)\n\t\t\tgoto out;\n\n\t\tif (skb->len < size_goal || (flags & MSG_OOB))\n\t\t\tcontinue;\n\n\t\tif (forced_push(tp)) {\n\t\t\ttcp_mark_push(tp, skb);\n\t\t\t__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);\n\t\t} else if (skb == tcp_send_head(sk))\n\t\t\ttcp_push_one(sk, mss_now);\n\t\tcontinue;\n\nwait_for_sndbuf:\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\nwait_for_memory:\n\t\ttcp_push(sk, flags & ~MSG_MORE, mss_now,\n\t\t\t TCP_NAGLE_PUSH, size_goal);\n\n\t\terr = sk_stream_wait_memory(sk, &timeo);\n\t\tif (err != 0)\n\t\t\tgoto do_error;\n\n\t\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\t}\n\nout:\n\tif (copied) {\n\t\ttcp_tx_timestamp(sk, sk->sk_tsflags, tcp_write_queue_tail(sk));\n\t\tif (!(flags & MSG_SENDPAGE_NOTLAST))\n\t\t\ttcp_push(sk, flags, mss_now, tp->nonagle, size_goal);\n\t}\n\treturn copied;\n\ndo_error:\n\tif (copied)\n\t\tgoto out;\nout_err:\n\t/* make sure we wake any epoll edge trigger waiter */\n\tif (unlikely(skb_queue_len(&sk->sk_write_queue) == 0 &&\n\t\t     err == -EAGAIN)) {\n\t\tsk->sk_write_space(sk);\n\t\ttcp_chrono_stop(sk, TCP_CHRONO_SNDBUF_LIMITED);\n\t}\n\treturn sk_stream_error(sk, flags, err);\n}\n\nint tcp_sendpage(struct sock *sk, struct page *page, int offset,\n\t\t size_t size, int flags)\n{\n\tssize_t res;\n\n\tif (!(sk->sk_route_caps & NETIF_F_SG) ||\n\t    !sk_check_csum_caps(sk))\n\t\treturn sock_no_sendpage(sk->sk_socket, page, offset, size,\n\t\t\t\t\tflags);\n\n\tlock_sock(sk);\n\n\ttcp_rate_check_app_limited(sk);  /* is sending application-limited? */\n\n\tres = do_tcp_sendpages(sk, page, offset, size, flags);\n\trelease_sock(sk);\n\treturn res;\n}\nEXPORT_SYMBOL(tcp_sendpage);\n\n/* Do not bother using a page frag for very small frames.\n * But use this heuristic only for the first skb in write queue.\n *\n * Having no payload in skb->head allows better SACK shifting\n * in tcp_shift_skb_data(), reducing sack/rack overhead, because\n * write queue has less skbs.\n * Each skb can hold up to MAX_SKB_FRAGS * 32Kbytes, or ~0.5 MB.\n * This also speeds up tso_fragment(), since it wont fallback\n * to tcp_fragment().\n */\nstatic int linear_payload_sz(bool first_skb)\n{\n\tif (first_skb)\n\t\treturn SKB_WITH_OVERHEAD(2048 - MAX_TCP_HEADER);\n\treturn 0;\n}\n\nstatic int select_size(const struct sock *sk, bool sg, bool first_skb)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tint tmp = tp->mss_cache;\n\n\tif (sg) {\n\t\tif (sk_can_gso(sk)) {\n\t\t\ttmp = linear_payload_sz(first_skb);\n\t\t} else {\n\t\t\tint pgbreak = SKB_MAX_HEAD(MAX_TCP_HEADER);\n\n\t\t\tif (tmp >= pgbreak &&\n\t\t\t    tmp <= pgbreak + (MAX_SKB_FRAGS - 1) * PAGE_SIZE)\n\t\t\t\ttmp = pgbreak;\n\t\t}\n\t}\n\n\treturn tmp;\n}\n\nvoid tcp_free_fastopen_req(struct tcp_sock *tp)\n{\n\tif (tp->fastopen_req) {\n\t\tkfree(tp->fastopen_req);\n\t\ttp->fastopen_req = NULL;\n\t}\n}\n\nstatic int tcp_sendmsg_fastopen(struct sock *sk, struct msghdr *msg,\n\t\t\t\tint *copied, size_t size)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_sock *inet = inet_sk(sk);\n\tint err, flags;\n\n\tif (!(sysctl_tcp_fastopen & TFO_CLIENT_ENABLE))\n\t\treturn -EOPNOTSUPP;\n\tif (tp->fastopen_req)\n\t\treturn -EALREADY; /* Another Fast Open is in progress */\n\n\ttp->fastopen_req = kzalloc(sizeof(struct tcp_fastopen_request),\n\t\t\t\t   sk->sk_allocation);\n\tif (unlikely(!tp->fastopen_req))\n\t\treturn -ENOBUFS;\n\ttp->fastopen_req->data = msg;\n\ttp->fastopen_req->size = size;\n\n\tif (inet->defer_connect) {\n\t\terr = tcp_connect(sk);\n\t\t/* Same failure procedure as in tcp_v4/6_connect */\n\t\tif (err) {\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\tinet->inet_dport = 0;\n\t\t\tsk->sk_route_caps = 0;\n\t\t}\n\t}\n\tflags = (msg->msg_flags & MSG_DONTWAIT) ? O_NONBLOCK : 0;\n\terr = __inet_stream_connect(sk->sk_socket, msg->msg_name,\n\t\t\t\t    msg->msg_namelen, flags, 1);\n\t/* fastopen_req could already be freed in __inet_stream_connect\n\t * if the connection times out or gets rst\n\t */\n\tif (tp->fastopen_req) {\n\t\t*copied = tp->fastopen_req->copied;\n\t\ttcp_free_fastopen_req(tp);\n\t\tinet->defer_connect = 0;\n\t}\n\treturn err;\n}\n\nint tcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *skb;\n\tstruct sockcm_cookie sockc;\n\tint flags, err, copied = 0;\n\tint mss_now = 0, size_goal, copied_syn = 0;\n\tbool process_backlog = false;\n\tbool sg;\n\tlong timeo;\n\n\tlock_sock(sk);\n\n\tflags = msg->msg_flags;\n\tif (unlikely(flags & MSG_FASTOPEN || inet_sk(sk)->defer_connect)) {\n\t\terr = tcp_sendmsg_fastopen(sk, msg, &copied_syn, size);\n\t\tif (err == -EINPROGRESS && copied_syn > 0)\n\t\t\tgoto out;\n\t\telse if (err)\n\t\t\tgoto out_err;\n\t}\n\n\ttimeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);\n\n\ttcp_rate_check_app_limited(sk);  /* is sending application-limited? */\n\n\t/* Wait for a connection to finish. One exception is TCP Fast Open\n\t * (passive side) where data is allowed to be sent before a connection\n\t * is fully established.\n\t */\n\tif (((1 << sk->sk_state) & ~(TCPF_ESTABLISHED | TCPF_CLOSE_WAIT)) &&\n\t    !tcp_passive_fastopen(sk)) {\n\t\terr = sk_stream_wait_connect(sk, &timeo);\n\t\tif (err != 0)\n\t\t\tgoto do_error;\n\t}\n\n\tif (unlikely(tp->repair)) {\n\t\tif (tp->repair_queue == TCP_RECV_QUEUE) {\n\t\t\tcopied = tcp_send_rcvq(sk, msg, size);\n\t\t\tgoto out_nopush;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (tp->repair_queue == TCP_NO_QUEUE)\n\t\t\tgoto out_err;\n\n\t\t/* 'common' sending to sendq */\n\t}\n\n\tsockc.tsflags = sk->sk_tsflags;\n\tif (msg->msg_controllen) {\n\t\terr = sock_cmsg_send(sk, msg, &sockc);\n\t\tif (unlikely(err)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_err;\n\t\t}\n\t}\n\n\t/* This should be in poll */\n\tsk_clear_bit(SOCKWQ_ASYNC_NOSPACE, sk);\n\n\t/* Ok commence sending. */\n\tcopied = 0;\n\nrestart:\n\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\n\terr = -EPIPE;\n\tif (sk->sk_err || (sk->sk_shutdown & SEND_SHUTDOWN))\n\t\tgoto do_error;\n\n\tsg = !!(sk->sk_route_caps & NETIF_F_SG);\n\n\twhile (msg_data_left(msg)) {\n\t\tint copy = 0;\n\t\tint max = size_goal;\n\n\t\tskb = tcp_write_queue_tail(sk);\n\t\tif (tcp_send_head(sk)) {\n\t\t\tif (skb->ip_summed == CHECKSUM_NONE)\n\t\t\t\tmax = mss_now;\n\t\t\tcopy = max - skb->len;\n\t\t}\n\n\t\tif (copy <= 0 || !tcp_skb_can_collapse_to(skb)) {\n\t\t\tbool first_skb;\n\nnew_segment:\n\t\t\t/* Allocate new segment. If the interface is SG,\n\t\t\t * allocate skb fitting to single page.\n\t\t\t */\n\t\t\tif (!sk_stream_memory_free(sk))\n\t\t\t\tgoto wait_for_sndbuf;\n\n\t\t\tif (process_backlog && sk_flush_backlog(sk)) {\n\t\t\t\tprocess_backlog = false;\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t\tfirst_skb = skb_queue_empty(&sk->sk_write_queue);\n\t\t\tskb = sk_stream_alloc_skb(sk,\n\t\t\t\t\t\t  select_size(sk, sg, first_skb),\n\t\t\t\t\t\t  sk->sk_allocation,\n\t\t\t\t\t\t  first_skb);\n\t\t\tif (!skb)\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\tprocess_backlog = true;\n\t\t\t/*\n\t\t\t * Check whether we can use HW checksum.\n\t\t\t */\n\t\t\tif (sk_check_csum_caps(sk))\n\t\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\n\t\t\tskb_entail(sk, skb);\n\t\t\tcopy = size_goal;\n\t\t\tmax = size_goal;\n\n\t\t\t/* All packets are restored as if they have\n\t\t\t * already been sent. skb_mstamp isn't set to\n\t\t\t * avoid wrong rtt estimation.\n\t\t\t */\n\t\t\tif (tp->repair)\n\t\t\t\tTCP_SKB_CB(skb)->sacked |= TCPCB_REPAIRED;\n\t\t}\n\n\t\t/* Try to append data to the end of skb. */\n\t\tif (copy > msg_data_left(msg))\n\t\t\tcopy = msg_data_left(msg);\n\n\t\t/* Where to copy to? */\n\t\tif (skb_availroom(skb) > 0) {\n\t\t\t/* We have some space in skb head. Superb! */\n\t\t\tcopy = min_t(int, copy, skb_availroom(skb));\n\t\t\terr = skb_add_data_nocache(sk, skb, &msg->msg_iter, copy);\n\t\t\tif (err)\n\t\t\t\tgoto do_fault;\n\t\t} else {\n\t\t\tbool merge = true;\n\t\t\tint i = skb_shinfo(skb)->nr_frags;\n\t\t\tstruct page_frag *pfrag = sk_page_frag(sk);\n\n\t\t\tif (!sk_page_frag_refill(sk, pfrag))\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\tif (!skb_can_coalesce(skb, i, pfrag->page,\n\t\t\t\t\t      pfrag->offset)) {\n\t\t\t\tif (i >= sysctl_max_skb_frags || !sg) {\n\t\t\t\t\ttcp_mark_push(tp, skb);\n\t\t\t\t\tgoto new_segment;\n\t\t\t\t}\n\t\t\t\tmerge = false;\n\t\t\t}\n\n\t\t\tcopy = min_t(int, copy, pfrag->size - pfrag->offset);\n\n\t\t\tif (!sk_wmem_schedule(sk, copy))\n\t\t\t\tgoto wait_for_memory;\n\n\t\t\terr = skb_copy_to_page_nocache(sk, &msg->msg_iter, skb,\n\t\t\t\t\t\t       pfrag->page,\n\t\t\t\t\t\t       pfrag->offset,\n\t\t\t\t\t\t       copy);\n\t\t\tif (err)\n\t\t\t\tgoto do_error;\n\n\t\t\t/* Update the skb. */\n\t\t\tif (merge) {\n\t\t\t\tskb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);\n\t\t\t} else {\n\t\t\t\tskb_fill_page_desc(skb, i, pfrag->page,\n\t\t\t\t\t\t   pfrag->offset, copy);\n\t\t\t\tpage_ref_inc(pfrag->page);\n\t\t\t}\n\t\t\tpfrag->offset += copy;\n\t\t}\n\n\t\tif (!copied)\n\t\t\tTCP_SKB_CB(skb)->tcp_flags &= ~TCPHDR_PSH;\n\n\t\ttp->write_seq += copy;\n\t\tTCP_SKB_CB(skb)->end_seq += copy;\n\t\ttcp_skb_pcount_set(skb, 0);\n\n\t\tcopied += copy;\n\t\tif (!msg_data_left(msg)) {\n\t\t\tif (unlikely(flags & MSG_EOR))\n\t\t\t\tTCP_SKB_CB(skb)->eor = 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (skb->len < max || (flags & MSG_OOB) || unlikely(tp->repair))\n\t\t\tcontinue;\n\n\t\tif (forced_push(tp)) {\n\t\t\ttcp_mark_push(tp, skb);\n\t\t\t__tcp_push_pending_frames(sk, mss_now, TCP_NAGLE_PUSH);\n\t\t} else if (skb == tcp_send_head(sk))\n\t\t\ttcp_push_one(sk, mss_now);\n\t\tcontinue;\n\nwait_for_sndbuf:\n\t\tset_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\nwait_for_memory:\n\t\tif (copied)\n\t\t\ttcp_push(sk, flags & ~MSG_MORE, mss_now,\n\t\t\t\t TCP_NAGLE_PUSH, size_goal);\n\n\t\terr = sk_stream_wait_memory(sk, &timeo);\n\t\tif (err != 0)\n\t\t\tgoto do_error;\n\n\t\tmss_now = tcp_send_mss(sk, &size_goal, flags);\n\t}\n\nout:\n\tif (copied) {\n\t\ttcp_tx_timestamp(sk, sockc.tsflags, tcp_write_queue_tail(sk));\n\t\ttcp_push(sk, flags, mss_now, tp->nonagle, size_goal);\n\t}\nout_nopush:\n\trelease_sock(sk);\n\treturn copied + copied_syn;\n\ndo_fault:\n\tif (!skb->len) {\n\t\ttcp_unlink_write_queue(skb, sk);\n\t\t/* It is the one place in all of TCP, except connection\n\t\t * reset, where we can be unlinking the send_head.\n\t\t */\n\t\ttcp_check_send_head(sk, skb);\n\t\tsk_wmem_free_skb(sk, skb);\n\t}\n\ndo_error:\n\tif (copied + copied_syn)\n\t\tgoto out;\nout_err:\n\terr = sk_stream_error(sk, flags, err);\n\t/* make sure we wake any epoll edge trigger waiter */\n\tif (unlikely(skb_queue_len(&sk->sk_write_queue) == 0 &&\n\t\t     err == -EAGAIN)) {\n\t\tsk->sk_write_space(sk);\n\t\ttcp_chrono_stop(sk, TCP_CHRONO_SNDBUF_LIMITED);\n\t}\n\trelease_sock(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(tcp_sendmsg);\n\n/*\n *\tHandle reading urgent data. BSD has very simple semantics for\n *\tthis, no blocking and very strange errors 8)\n */\n\nstatic int tcp_recv_urg(struct sock *sk, struct msghdr *msg, int len, int flags)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t/* No URG data to read. */\n\tif (sock_flag(sk, SOCK_URGINLINE) || !tp->urg_data ||\n\t    tp->urg_data == TCP_URG_READ)\n\t\treturn -EINVAL;\t/* Yes this is right ! */\n\n\tif (sk->sk_state == TCP_CLOSE && !sock_flag(sk, SOCK_DONE))\n\t\treturn -ENOTCONN;\n\n\tif (tp->urg_data & TCP_URG_VALID) {\n\t\tint err = 0;\n\t\tchar c = tp->urg_data;\n\n\t\tif (!(flags & MSG_PEEK))\n\t\t\ttp->urg_data = TCP_URG_READ;\n\n\t\t/* Read urgent data. */\n\t\tmsg->msg_flags |= MSG_OOB;\n\n\t\tif (len > 0) {\n\t\t\tif (!(flags & MSG_TRUNC))\n\t\t\t\terr = memcpy_to_msg(msg, &c, 1);\n\t\t\tlen = 1;\n\t\t} else\n\t\t\tmsg->msg_flags |= MSG_TRUNC;\n\n\t\treturn err ? -EFAULT : len;\n\t}\n\n\tif (sk->sk_state == TCP_CLOSE || (sk->sk_shutdown & RCV_SHUTDOWN))\n\t\treturn 0;\n\n\t/* Fixed the recv(..., MSG_OOB) behaviour.  BSD docs and\n\t * the available implementations agree in this case:\n\t * this call should never block, independent of the\n\t * blocking state of the socket.\n\t * Mike <pall@rz.uni-karlsruhe.de>\n\t */\n\treturn -EAGAIN;\n}\n\nstatic int tcp_peek_sndq(struct sock *sk, struct msghdr *msg, int len)\n{\n\tstruct sk_buff *skb;\n\tint copied = 0, err = 0;\n\n\t/* XXX -- need to support SO_PEEK_OFF */\n\n\tskb_queue_walk(&sk->sk_write_queue, skb) {\n\t\terr = skb_copy_datagram_msg(skb, 0, msg, skb->len);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tcopied += skb->len;\n\t}\n\n\treturn err ?: copied;\n}\n\n/* Clean up the receive buffer for full frames taken by the user,\n * then send an ACK if necessary.  COPIED is the number of bytes\n * tcp_recvmsg has given to the user so far, it speeds up the\n * calculation of whether or not we must ACK for the sake of\n * a window update.\n */\nstatic void tcp_cleanup_rbuf(struct sock *sk, int copied)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tbool time_to_ack = false;\n\n\tstruct sk_buff *skb = skb_peek(&sk->sk_receive_queue);\n\n\tWARN(skb && !before(tp->copied_seq, TCP_SKB_CB(skb)->end_seq),\n\t     \"cleanup rbuf bug: copied %X seq %X rcvnxt %X\\n\",\n\t     tp->copied_seq, TCP_SKB_CB(skb)->end_seq, tp->rcv_nxt);\n\n\tif (inet_csk_ack_scheduled(sk)) {\n\t\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\t\t   /* Delayed ACKs frequently hit locked sockets during bulk\n\t\t    * receive. */\n\t\tif (icsk->icsk_ack.blocked ||\n\t\t    /* Once-per-two-segments ACK was not sent by tcp_input.c */\n\t\t    tp->rcv_nxt - tp->rcv_wup > icsk->icsk_ack.rcv_mss ||\n\t\t    /*\n\t\t     * If this read emptied read buffer, we send ACK, if\n\t\t     * connection is not bidirectional, user drained\n\t\t     * receive buffer and there was a small segment\n\t\t     * in queue.\n\t\t     */\n\t\t    (copied > 0 &&\n\t\t     ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED2) ||\n\t\t      ((icsk->icsk_ack.pending & ICSK_ACK_PUSHED) &&\n\t\t       !icsk->icsk_ack.pingpong)) &&\n\t\t      !atomic_read(&sk->sk_rmem_alloc)))\n\t\t\ttime_to_ack = true;\n\t}\n\n\t/* We send an ACK if we can now advertise a non-zero window\n\t * which has been raised \"significantly\".\n\t *\n\t * Even if window raised up to infinity, do not send window open ACK\n\t * in states, where we will not receive more. It is useless.\n\t */\n\tif (copied > 0 && !time_to_ack && !(sk->sk_shutdown & RCV_SHUTDOWN)) {\n\t\t__u32 rcv_window_now = tcp_receive_window(tp);\n\n\t\t/* Optimize, __tcp_select_window() is not cheap. */\n\t\tif (2*rcv_window_now <= tp->window_clamp) {\n\t\t\t__u32 new_window = __tcp_select_window(sk);\n\n\t\t\t/* Send ACK now, if this read freed lots of space\n\t\t\t * in our buffer. Certainly, new_window is new window.\n\t\t\t * We can advertise it now, if it is not less than current one.\n\t\t\t * \"Lots\" means \"at least twice\" here.\n\t\t\t */\n\t\t\tif (new_window && new_window >= 2 * rcv_window_now)\n\t\t\t\ttime_to_ack = true;\n\t\t}\n\t}\n\tif (time_to_ack)\n\t\ttcp_send_ack(sk);\n}\n\nstatic void tcp_prequeue_process(struct sock *sk)\n{\n\tstruct sk_buff *skb;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPREQUEUED);\n\n\twhile ((skb = __skb_dequeue(&tp->ucopy.prequeue)) != NULL)\n\t\tsk_backlog_rcv(sk, skb);\n\n\t/* Clear memory counter. */\n\ttp->ucopy.memory = 0;\n}\n\nstatic struct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off)\n{\n\tstruct sk_buff *skb;\n\tu32 offset;\n\n\twhile ((skb = skb_peek(&sk->sk_receive_queue)) != NULL) {\n\t\toffset = seq - TCP_SKB_CB(skb)->seq;\n\t\tif (unlikely(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\t\tpr_err_once(\"%s: found a SYN, please report !\\n\", __func__);\n\t\t\toffset--;\n\t\t}\n\t\tif (offset < skb->len || (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)) {\n\t\t\t*off = offset;\n\t\t\treturn skb;\n\t\t}\n\t\t/* This looks weird, but this can happen if TCP collapsing\n\t\t * splitted a fat GRO packet, while we released socket lock\n\t\t * in skb_splice_bits()\n\t\t */\n\t\tsk_eat_skb(sk, skb);\n\t}\n\treturn NULL;\n}\n\n/*\n * This routine provides an alternative to tcp_recvmsg() for routines\n * that would like to handle copying from skbuffs directly in 'sendfile'\n * fashion.\n * Note:\n *\t- It is assumed that the socket was locked by the caller.\n *\t- The routine does not block.\n *\t- At present, there is no support for reading OOB data\n *\t  or for 'peeking' the socket using this routine\n *\t  (although both would be easy to implement).\n */\nint tcp_read_sock(struct sock *sk, read_descriptor_t *desc,\n\t\t  sk_read_actor_t recv_actor)\n{\n\tstruct sk_buff *skb;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tu32 seq = tp->copied_seq;\n\tu32 offset;\n\tint copied = 0;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\treturn -ENOTCONN;\n\twhile ((skb = tcp_recv_skb(sk, seq, &offset)) != NULL) {\n\t\tif (offset < skb->len) {\n\t\t\tint used;\n\t\t\tsize_t len;\n\n\t\t\tlen = skb->len - offset;\n\t\t\t/* Stop reading if we hit a patch of urgent data */\n\t\t\tif (tp->urg_data) {\n\t\t\t\tu32 urg_offset = tp->urg_seq - seq;\n\t\t\t\tif (urg_offset < len)\n\t\t\t\t\tlen = urg_offset;\n\t\t\t\tif (!len)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tused = recv_actor(desc, skb, offset, len);\n\t\t\tif (used <= 0) {\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = used;\n\t\t\t\tbreak;\n\t\t\t} else if (used <= len) {\n\t\t\t\tseq += used;\n\t\t\t\tcopied += used;\n\t\t\t\toffset += used;\n\t\t\t}\n\t\t\t/* If recv_actor drops the lock (e.g. TCP splice\n\t\t\t * receive) the skb pointer might be invalid when\n\t\t\t * getting here: tcp_collapse might have deleted it\n\t\t\t * while aggregating skbs from the socket queue.\n\t\t\t */\n\t\t\tskb = tcp_recv_skb(sk, seq - 1, &offset);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\t\t\t/* TCP coalescing might have appended data to the skb.\n\t\t\t * Try to splice more frags\n\t\t\t */\n\t\t\tif (offset + 1 != skb->len)\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN) {\n\t\t\tsk_eat_skb(sk, skb);\n\t\t\t++seq;\n\t\t\tbreak;\n\t\t}\n\t\tsk_eat_skb(sk, skb);\n\t\tif (!desc->count)\n\t\t\tbreak;\n\t\ttp->copied_seq = seq;\n\t}\n\ttp->copied_seq = seq;\n\n\ttcp_rcv_space_adjust(sk);\n\n\t/* Clean up data we have read: This will do ACK frames. */\n\tif (copied > 0) {\n\t\ttcp_recv_skb(sk, seq, &offset);\n\t\ttcp_cleanup_rbuf(sk, copied);\n\t}\n\treturn copied;\n}\nEXPORT_SYMBOL(tcp_read_sock);\n\nint tcp_peek_len(struct socket *sock)\n{\n\treturn tcp_inq(sock->sk);\n}\nEXPORT_SYMBOL(tcp_peek_len);\n\n/*\n *\tThis routine copies from a sock struct into the user buffer.\n *\n *\tTechnical note: in 2.3 we work on _locked_ socket, so that\n *\ttricks with *seq access order and skb->users are not required.\n *\tProbably, code can be easily improved even more.\n */\n\nint tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,\n\t\tint flags, int *addr_len)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint copied = 0;\n\tu32 peek_seq;\n\tu32 *seq;\n\tunsigned long used;\n\tint err;\n\tint target;\t\t/* Read at least this many bytes */\n\tlong timeo;\n\tstruct task_struct *user_recv = NULL;\n\tstruct sk_buff *skb, *last;\n\tu32 urg_hole = 0;\n\n\tif (unlikely(flags & MSG_ERRQUEUE))\n\t\treturn inet_recv_error(sk, msg, len, addr_len);\n\n\tif (sk_can_busy_loop(sk) && skb_queue_empty(&sk->sk_receive_queue) &&\n\t    (sk->sk_state == TCP_ESTABLISHED))\n\t\tsk_busy_loop(sk, nonblock);\n\n\tlock_sock(sk);\n\n\terr = -ENOTCONN;\n\tif (sk->sk_state == TCP_LISTEN)\n\t\tgoto out;\n\n\ttimeo = sock_rcvtimeo(sk, nonblock);\n\n\t/* Urgent data needs to be handled specially. */\n\tif (flags & MSG_OOB)\n\t\tgoto recv_urg;\n\n\tif (unlikely(tp->repair)) {\n\t\terr = -EPERM;\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tgoto out;\n\n\t\tif (tp->repair_queue == TCP_SEND_QUEUE)\n\t\t\tgoto recv_sndq;\n\n\t\terr = -EINVAL;\n\t\tif (tp->repair_queue == TCP_NO_QUEUE)\n\t\t\tgoto out;\n\n\t\t/* 'common' recv queue MSG_PEEK-ing */\n\t}\n\n\tseq = &tp->copied_seq;\n\tif (flags & MSG_PEEK) {\n\t\tpeek_seq = tp->copied_seq;\n\t\tseq = &peek_seq;\n\t}\n\n\ttarget = sock_rcvlowat(sk, flags & MSG_WAITALL, len);\n\n\tdo {\n\t\tu32 offset;\n\n\t\t/* Are we at urgent data? Stop if we have read anything or have SIGURG pending. */\n\t\tif (tp->urg_data && tp->urg_seq == *seq) {\n\t\t\tif (copied)\n\t\t\t\tbreak;\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = timeo ? sock_intr_errno(timeo) : -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t/* Next get a buffer. */\n\n\t\tlast = skb_peek_tail(&sk->sk_receive_queue);\n\t\tskb_queue_walk(&sk->sk_receive_queue, skb) {\n\t\t\tlast = skb;\n\t\t\t/* Now that we have two receive queues this\n\t\t\t * shouldn't happen.\n\t\t\t */\n\t\t\tif (WARN(before(*seq, TCP_SKB_CB(skb)->seq),\n\t\t\t\t \"recvmsg bug: copied %X seq %X rcvnxt %X fl %X\\n\",\n\t\t\t\t *seq, TCP_SKB_CB(skb)->seq, tp->rcv_nxt,\n\t\t\t\t flags))\n\t\t\t\tbreak;\n\n\t\t\toffset = *seq - TCP_SKB_CB(skb)->seq;\n\t\t\tif (unlikely(TCP_SKB_CB(skb)->tcp_flags & TCPHDR_SYN)) {\n\t\t\t\tpr_err_once(\"%s: found a SYN, please report !\\n\", __func__);\n\t\t\t\toffset--;\n\t\t\t}\n\t\t\tif (offset < skb->len)\n\t\t\t\tgoto found_ok_skb;\n\t\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\t\tgoto found_fin_ok;\n\t\t\tWARN(!(flags & MSG_PEEK),\n\t\t\t     \"recvmsg bug 2: copied %X seq %X rcvnxt %X fl %X\\n\",\n\t\t\t     *seq, TCP_SKB_CB(skb)->seq, tp->rcv_nxt, flags);\n\t\t}\n\n\t\t/* Well, if we have backlog, try to process it now yet. */\n\n\t\tif (copied >= target && !sk->sk_backlog.tail)\n\t\t\tbreak;\n\n\t\tif (copied) {\n\t\t\tif (sk->sk_err ||\n\t\t\t    sk->sk_state == TCP_CLOSE ||\n\t\t\t    (sk->sk_shutdown & RCV_SHUTDOWN) ||\n\t\t\t    !timeo ||\n\t\t\t    signal_pending(current))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (sock_flag(sk, SOCK_DONE))\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_err) {\n\t\t\t\tcopied = sock_error(sk);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (sk->sk_shutdown & RCV_SHUTDOWN)\n\t\t\t\tbreak;\n\n\t\t\tif (sk->sk_state == TCP_CLOSE) {\n\t\t\t\tif (!sock_flag(sk, SOCK_DONE)) {\n\t\t\t\t\t/* This occurs when user tries to read\n\t\t\t\t\t * from never connected socket.\n\t\t\t\t\t */\n\t\t\t\t\tcopied = -ENOTCONN;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!timeo) {\n\t\t\t\tcopied = -EAGAIN;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (signal_pending(current)) {\n\t\t\t\tcopied = sock_intr_errno(timeo);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\ttcp_cleanup_rbuf(sk, copied);\n\n\t\tif (!sysctl_tcp_low_latency && tp->ucopy.task == user_recv) {\n\t\t\t/* Install new reader */\n\t\t\tif (!user_recv && !(flags & (MSG_TRUNC | MSG_PEEK))) {\n\t\t\t\tuser_recv = current;\n\t\t\t\ttp->ucopy.task = user_recv;\n\t\t\t\ttp->ucopy.msg = msg;\n\t\t\t}\n\n\t\t\ttp->ucopy.len = len;\n\n\t\t\tWARN_ON(tp->copied_seq != tp->rcv_nxt &&\n\t\t\t\t!(flags & (MSG_PEEK | MSG_TRUNC)));\n\n\t\t\t/* Ugly... If prequeue is not empty, we have to\n\t\t\t * process it before releasing socket, otherwise\n\t\t\t * order will be broken at second iteration.\n\t\t\t * More elegant solution is required!!!\n\t\t\t *\n\t\t\t * Look: we have the following (pseudo)queues:\n\t\t\t *\n\t\t\t * 1. packets in flight\n\t\t\t * 2. backlog\n\t\t\t * 3. prequeue\n\t\t\t * 4. receive_queue\n\t\t\t *\n\t\t\t * Each queue can be processed only if the next ones\n\t\t\t * are empty. At this point we have empty receive_queue.\n\t\t\t * But prequeue _can_ be not empty after 2nd iteration,\n\t\t\t * when we jumped to start of loop because backlog\n\t\t\t * processing added something to receive_queue.\n\t\t\t * We cannot release_sock(), because backlog contains\n\t\t\t * packets arrived _after_ prequeued ones.\n\t\t\t *\n\t\t\t * Shortly, algorithm is clear --- to process all\n\t\t\t * the queues in order. We could make it more directly,\n\t\t\t * requeueing packets from backlog to prequeue, if\n\t\t\t * is not empty. It is more elegant, but eats cycles,\n\t\t\t * unfortunately.\n\t\t\t */\n\t\t\tif (!skb_queue_empty(&tp->ucopy.prequeue))\n\t\t\t\tgoto do_prequeue;\n\n\t\t\t/* __ Set realtime policy in scheduler __ */\n\t\t}\n\n\t\tif (copied >= target) {\n\t\t\t/* Do not sleep, just process backlog. */\n\t\t\trelease_sock(sk);\n\t\t\tlock_sock(sk);\n\t\t} else {\n\t\t\tsk_wait_data(sk, &timeo, last);\n\t\t}\n\n\t\tif (user_recv) {\n\t\t\tint chunk;\n\n\t\t\t/* __ Restore normal policy in scheduler __ */\n\n\t\t\tchunk = len - tp->ucopy.len;\n\t\t\tif (chunk != 0) {\n\t\t\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMBACKLOG, chunk);\n\t\t\t\tlen -= chunk;\n\t\t\t\tcopied += chunk;\n\t\t\t}\n\n\t\t\tif (tp->rcv_nxt == tp->copied_seq &&\n\t\t\t    !skb_queue_empty(&tp->ucopy.prequeue)) {\ndo_prequeue:\n\t\t\t\ttcp_prequeue_process(sk);\n\n\t\t\t\tchunk = len - tp->ucopy.len;\n\t\t\t\tif (chunk != 0) {\n\t\t\t\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);\n\t\t\t\t\tlen -= chunk;\n\t\t\t\t\tcopied += chunk;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ((flags & MSG_PEEK) &&\n\t\t    (peek_seq - copied - urg_hole != tp->copied_seq)) {\n\t\t\tnet_dbg_ratelimited(\"TCP(%s:%d): Application bug, race in MSG_PEEK\\n\",\n\t\t\t\t\t    current->comm,\n\t\t\t\t\t    task_pid_nr(current));\n\t\t\tpeek_seq = tp->copied_seq;\n\t\t}\n\t\tcontinue;\n\n\tfound_ok_skb:\n\t\t/* Ok so how much can we use? */\n\t\tused = skb->len - offset;\n\t\tif (len < used)\n\t\t\tused = len;\n\n\t\t/* Do we have urgent data here? */\n\t\tif (tp->urg_data) {\n\t\t\tu32 urg_offset = tp->urg_seq - *seq;\n\t\t\tif (urg_offset < used) {\n\t\t\t\tif (!urg_offset) {\n\t\t\t\t\tif (!sock_flag(sk, SOCK_URGINLINE)) {\n\t\t\t\t\t\t++*seq;\n\t\t\t\t\t\turg_hole++;\n\t\t\t\t\t\toffset++;\n\t\t\t\t\t\tused--;\n\t\t\t\t\t\tif (!used)\n\t\t\t\t\t\t\tgoto skip_copy;\n\t\t\t\t\t}\n\t\t\t\t} else\n\t\t\t\t\tused = urg_offset;\n\t\t\t}\n\t\t}\n\n\t\tif (!(flags & MSG_TRUNC)) {\n\t\t\terr = skb_copy_datagram_msg(skb, offset, msg, used);\n\t\t\tif (err) {\n\t\t\t\t/* Exception. Bailout! */\n\t\t\t\tif (!copied)\n\t\t\t\t\tcopied = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t*seq += used;\n\t\tcopied += used;\n\t\tlen -= used;\n\n\t\ttcp_rcv_space_adjust(sk);\n\nskip_copy:\n\t\tif (tp->urg_data && after(tp->copied_seq, tp->urg_seq)) {\n\t\t\ttp->urg_data = 0;\n\t\t\ttcp_fast_path_check(sk);\n\t\t}\n\t\tif (used + offset < skb->len)\n\t\t\tcontinue;\n\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\tgoto found_fin_ok;\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tsk_eat_skb(sk, skb);\n\t\tcontinue;\n\n\tfound_fin_ok:\n\t\t/* Process the FIN. */\n\t\t++*seq;\n\t\tif (!(flags & MSG_PEEK))\n\t\t\tsk_eat_skb(sk, skb);\n\t\tbreak;\n\t} while (len > 0);\n\n\tif (user_recv) {\n\t\tif (!skb_queue_empty(&tp->ucopy.prequeue)) {\n\t\t\tint chunk;\n\n\t\t\ttp->ucopy.len = copied > 0 ? len : 0;\n\n\t\t\ttcp_prequeue_process(sk);\n\n\t\t\tif (copied > 0 && (chunk = len - tp->ucopy.len) != 0) {\n\t\t\t\tNET_ADD_STATS(sock_net(sk), LINUX_MIB_TCPDIRECTCOPYFROMPREQUEUE, chunk);\n\t\t\t\tlen -= chunk;\n\t\t\t\tcopied += chunk;\n\t\t\t}\n\t\t}\n\n\t\ttp->ucopy.task = NULL;\n\t\ttp->ucopy.len = 0;\n\t}\n\n\t/* According to UNIX98, msg_name/msg_namelen are ignored\n\t * on connected socket. I was just happy when found this 8) --ANK\n\t */\n\n\t/* Clean up data we have read: This will do ACK frames. */\n\ttcp_cleanup_rbuf(sk, copied);\n\n\trelease_sock(sk);\n\treturn copied;\n\nout:\n\trelease_sock(sk);\n\treturn err;\n\nrecv_urg:\n\terr = tcp_recv_urg(sk, msg, len, flags);\n\tgoto out;\n\nrecv_sndq:\n\terr = tcp_peek_sndq(sk, msg, len);\n\tgoto out;\n}\nEXPORT_SYMBOL(tcp_recvmsg);\n\nvoid tcp_set_state(struct sock *sk, int state)\n{\n\tint oldstate = sk->sk_state;\n\n\tswitch (state) {\n\tcase TCP_ESTABLISHED:\n\t\tif (oldstate != TCP_ESTABLISHED)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_CURRESTAB);\n\t\tbreak;\n\n\tcase TCP_CLOSE:\n\t\tif (oldstate == TCP_CLOSE_WAIT || oldstate == TCP_ESTABLISHED)\n\t\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ESTABRESETS);\n\n\t\tsk->sk_prot->unhash(sk);\n\t\tif (inet_csk(sk)->icsk_bind_hash &&\n\t\t    !(sk->sk_userlocks & SOCK_BINDPORT_LOCK))\n\t\t\tinet_put_port(sk);\n\t\t/* fall through */\n\tdefault:\n\t\tif (oldstate == TCP_ESTABLISHED)\n\t\t\tTCP_DEC_STATS(sock_net(sk), TCP_MIB_CURRESTAB);\n\t}\n\n\t/* Change state AFTER socket is unhashed to avoid closed\n\t * socket sitting in hash tables.\n\t */\n\tsk_state_store(sk, state);\n\n#ifdef STATE_TRACE\n\tSOCK_DEBUG(sk, \"TCP sk=%p, State %s -> %s\\n\", sk, statename[oldstate], statename[state]);\n#endif\n}\nEXPORT_SYMBOL_GPL(tcp_set_state);\n\n/*\n *\tState processing on a close. This implements the state shift for\n *\tsending our FIN frame. Note that we only send a FIN for some\n *\tstates. A shutdown() may have already sent the FIN, or we may be\n *\tclosed.\n */\n\nstatic const unsigned char new_state[16] = {\n  /* current state:        new state:      action:\t*/\n  [0 /* (Invalid) */]\t= TCP_CLOSE,\n  [TCP_ESTABLISHED]\t= TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n  [TCP_SYN_SENT]\t= TCP_CLOSE,\n  [TCP_SYN_RECV]\t= TCP_FIN_WAIT1 | TCP_ACTION_FIN,\n  [TCP_FIN_WAIT1]\t= TCP_FIN_WAIT1,\n  [TCP_FIN_WAIT2]\t= TCP_FIN_WAIT2,\n  [TCP_TIME_WAIT]\t= TCP_CLOSE,\n  [TCP_CLOSE]\t\t= TCP_CLOSE,\n  [TCP_CLOSE_WAIT]\t= TCP_LAST_ACK  | TCP_ACTION_FIN,\n  [TCP_LAST_ACK]\t= TCP_LAST_ACK,\n  [TCP_LISTEN]\t\t= TCP_CLOSE,\n  [TCP_CLOSING]\t\t= TCP_CLOSING,\n  [TCP_NEW_SYN_RECV]\t= TCP_CLOSE,\t/* should not happen ! */\n};\n\nstatic int tcp_close_state(struct sock *sk)\n{\n\tint next = (int)new_state[sk->sk_state];\n\tint ns = next & TCP_STATE_MASK;\n\n\ttcp_set_state(sk, ns);\n\n\treturn next & TCP_ACTION_FIN;\n}\n\n/*\n *\tShutdown the sending side of a connection. Much like close except\n *\tthat we don't receive shut down or sock_set_flag(sk, SOCK_DEAD).\n */\n\nvoid tcp_shutdown(struct sock *sk, int how)\n{\n\t/*\tWe need to grab some memory, and put together a FIN,\n\t *\tand then put it into the queue to be sent.\n\t *\t\tTim MacKenzie(tym@dibbler.cs.monash.edu.au) 4 Dec '92.\n\t */\n\tif (!(how & SEND_SHUTDOWN))\n\t\treturn;\n\n\t/* If we've already sent a FIN, or it's a closed state, skip this. */\n\tif ((1 << sk->sk_state) &\n\t    (TCPF_ESTABLISHED | TCPF_SYN_SENT |\n\t     TCPF_SYN_RECV | TCPF_CLOSE_WAIT)) {\n\t\t/* Clear out any half completed packets.  FIN if needed. */\n\t\tif (tcp_close_state(sk))\n\t\t\ttcp_send_fin(sk);\n\t}\n}\nEXPORT_SYMBOL(tcp_shutdown);\n\nbool tcp_check_oom(struct sock *sk, int shift)\n{\n\tbool too_many_orphans, out_of_socket_memory;\n\n\ttoo_many_orphans = tcp_too_many_orphans(sk, shift);\n\tout_of_socket_memory = tcp_out_of_memory(sk);\n\n\tif (too_many_orphans)\n\t\tnet_info_ratelimited(\"too many orphaned sockets\\n\");\n\tif (out_of_socket_memory)\n\t\tnet_info_ratelimited(\"out of memory -- consider tuning tcp_mem\\n\");\n\treturn too_many_orphans || out_of_socket_memory;\n}\n\nvoid tcp_close(struct sock *sk, long timeout)\n{\n\tstruct sk_buff *skb;\n\tint data_was_unread = 0;\n\tint state;\n\n\tlock_sock(sk);\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\n\t\t/* Special case. */\n\t\tinet_csk_listen_stop(sk);\n\n\t\tgoto adjudge_to_death;\n\t}\n\n\t/*  We need to flush the recv. buffs.  We do this only on the\n\t *  descriptor close, not protocol-sourced closes, because the\n\t *  reader process may not have drained the data yet!\n\t */\n\twhile ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {\n\t\tu32 len = TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq;\n\n\t\tif (TCP_SKB_CB(skb)->tcp_flags & TCPHDR_FIN)\n\t\t\tlen--;\n\t\tdata_was_unread += len;\n\t\t__kfree_skb(skb);\n\t}\n\n\tsk_mem_reclaim(sk);\n\n\t/* If socket has been already reset (e.g. in tcp_reset()) - kill it. */\n\tif (sk->sk_state == TCP_CLOSE)\n\t\tgoto adjudge_to_death;\n\n\t/* As outlined in RFC 2525, section 2.17, we send a RST here because\n\t * data was lost. To witness the awful effects of the old behavior of\n\t * always doing a FIN, run an older 2.1.x kernel or 2.0.x, start a bulk\n\t * GET in an FTP client, suspend the process, wait for the client to\n\t * advertise a zero window, then kill -9 the FTP client, wheee...\n\t * Note: timeout is always zero in such a case.\n\t */\n\tif (unlikely(tcp_sk(sk)->repair)) {\n\t\tsk->sk_prot->disconnect(sk, 0);\n\t} else if (data_was_unread) {\n\t\t/* Unread data was tossed, zap the connection. */\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONCLOSE);\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\ttcp_send_active_reset(sk, sk->sk_allocation);\n\t} else if (sock_flag(sk, SOCK_LINGER) && !sk->sk_lingertime) {\n\t\t/* Check zero linger _after_ checking for unread data. */\n\t\tsk->sk_prot->disconnect(sk, 0);\n\t\tNET_INC_STATS(sock_net(sk), LINUX_MIB_TCPABORTONDATA);\n\t} else if (tcp_close_state(sk)) {\n\t\t/* We FIN if the application ate all the data before\n\t\t * zapping the connection.\n\t\t */\n\n\t\t/* RED-PEN. Formally speaking, we have broken TCP state\n\t\t * machine. State transitions:\n\t\t *\n\t\t * TCP_ESTABLISHED -> TCP_FIN_WAIT1\n\t\t * TCP_SYN_RECV\t-> TCP_FIN_WAIT1 (forget it, it's impossible)\n\t\t * TCP_CLOSE_WAIT -> TCP_LAST_ACK\n\t\t *\n\t\t * are legal only when FIN has been sent (i.e. in window),\n\t\t * rather than queued out of window. Purists blame.\n\t\t *\n\t\t * F.e. \"RFC state\" is ESTABLISHED,\n\t\t * if Linux state is FIN-WAIT-1, but FIN is still not sent.\n\t\t *\n\t\t * The visible declinations are that sometimes\n\t\t * we enter time-wait state, when it is not required really\n\t\t * (harmless), do not send active resets, when they are\n\t\t * required by specs (TCP_ESTABLISHED, TCP_CLOSE_WAIT, when\n\t\t * they look as CLOSING or LAST_ACK for Linux)\n\t\t * Probably, I missed some more holelets.\n\t\t * \t\t\t\t\t\t--ANK\n\t\t * XXX (TFO) - To start off we don't support SYN+ACK+FIN\n\t\t * in a single packet! (May consider it later but will\n\t\t * probably need API support or TCP_CORK SYN-ACK until\n\t\t * data is written and socket is closed.)\n\t\t */\n\t\ttcp_send_fin(sk);\n\t}\n\n\tsk_stream_wait_close(sk, timeout);\n\nadjudge_to_death:\n\tstate = sk->sk_state;\n\tsock_hold(sk);\n\tsock_orphan(sk);\n\n\t/* It is the last release_sock in its life. It will remove backlog. */\n\trelease_sock(sk);\n\n\n\t/* Now socket is owned by kernel and we acquire BH lock\n\t   to finish close. No need to check for user refs.\n\t */\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\tWARN_ON(sock_owned_by_user(sk));\n\n\tpercpu_counter_inc(sk->sk_prot->orphan_count);\n\n\t/* Have we already been destroyed by a softirq or backlog? */\n\tif (state != TCP_CLOSE && sk->sk_state == TCP_CLOSE)\n\t\tgoto out;\n\n\t/*\tThis is a (useful) BSD violating of the RFC. There is a\n\t *\tproblem with TCP as specified in that the other end could\n\t *\tkeep a socket open forever with no application left this end.\n\t *\tWe use a 1 minute timeout (about the same as BSD) then kill\n\t *\tour end. If they send after that then tough - BUT: long enough\n\t *\tthat we won't make the old 4*rto = almost no time - whoops\n\t *\treset mistake.\n\t *\n\t *\tNope, it was not mistake. It is really desired behaviour\n\t *\tf.e. on http servers, when such sockets are useless, but\n\t *\tconsume significant resources. Let's do it with special\n\t *\tlinger2\toption.\t\t\t\t\t--ANK\n\t */\n\n\tif (sk->sk_state == TCP_FIN_WAIT2) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\t\tif (tp->linger2 < 0) {\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\t\t__NET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPABORTONLINGER);\n\t\t} else {\n\t\t\tconst int tmo = tcp_fin_time(sk);\n\n\t\t\tif (tmo > TCP_TIMEWAIT_LEN) {\n\t\t\t\tinet_csk_reset_keepalive_timer(sk,\n\t\t\t\t\t\ttmo - TCP_TIMEWAIT_LEN);\n\t\t\t} else {\n\t\t\t\ttcp_time_wait(sk, TCP_FIN_WAIT2, tmo);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tif (sk->sk_state != TCP_CLOSE) {\n\t\tsk_mem_reclaim(sk);\n\t\tif (tcp_check_oom(sk, 0)) {\n\t\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\t\t__NET_INC_STATS(sock_net(sk),\n\t\t\t\t\tLINUX_MIB_TCPABORTONMEMORY);\n\t\t}\n\t}\n\n\tif (sk->sk_state == TCP_CLOSE) {\n\t\tstruct request_sock *req = tcp_sk(sk)->fastopen_rsk;\n\t\t/* We could get here with a non-NULL req if the socket is\n\t\t * aborted (e.g., closed with unread data) before 3WHS\n\t\t * finishes.\n\t\t */\n\t\tif (req)\n\t\t\treqsk_fastopen_remove(sk, req, false);\n\t\tinet_csk_destroy_sock(sk);\n\t}\n\t/* Otherwise, socket is reprieved until protocol close. */\n\nout:\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\tsock_put(sk);\n}\nEXPORT_SYMBOL(tcp_close);\n\n/* These states need RST on ABORT according to RFC793 */\n\nstatic inline bool tcp_need_reset(int state)\n{\n\treturn (1 << state) &\n\t       (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT | TCPF_FIN_WAIT1 |\n\t\tTCPF_FIN_WAIT2 | TCPF_SYN_RECV);\n}\n\nint tcp_disconnect(struct sock *sk, int flags)\n{\n\tstruct inet_sock *inet = inet_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint err = 0;\n\tint old_state = sk->sk_state;\n\n\tif (old_state != TCP_CLOSE)\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\n\t/* ABORT function of RFC793 */\n\tif (old_state == TCP_LISTEN) {\n\t\tinet_csk_listen_stop(sk);\n\t} else if (unlikely(tp->repair)) {\n\t\tsk->sk_err = ECONNABORTED;\n\t} else if (tcp_need_reset(old_state) ||\n\t\t   (tp->snd_nxt != tp->write_seq &&\n\t\t    (1 << old_state) & (TCPF_CLOSING | TCPF_LAST_ACK))) {\n\t\t/* The last check adjusts for discrepancy of Linux wrt. RFC\n\t\t * states\n\t\t */\n\t\ttcp_send_active_reset(sk, gfp_any());\n\t\tsk->sk_err = ECONNRESET;\n\t} else if (old_state == TCP_SYN_SENT)\n\t\tsk->sk_err = ECONNRESET;\n\n\ttcp_clear_xmit_timers(sk);\n\t__skb_queue_purge(&sk->sk_receive_queue);\n\ttcp_write_queue_purge(sk);\n\ttcp_fastopen_active_disable_ofo_check(sk);\n\tskb_rbtree_purge(&tp->out_of_order_queue);\n\n\tinet->inet_dport = 0;\n\n\tif (!(sk->sk_userlocks & SOCK_BINDADDR_LOCK))\n\t\tinet_reset_saddr(sk);\n\n\tsk->sk_shutdown = 0;\n\tsock_reset_flag(sk, SOCK_DONE);\n\ttp->srtt_us = 0;\n\ttp->write_seq += tp->max_window + 2;\n\tif (tp->write_seq == 0)\n\t\ttp->write_seq = 1;\n\ticsk->icsk_backoff = 0;\n\ttp->snd_cwnd = 2;\n\ticsk->icsk_probes_out = 0;\n\ttp->packets_out = 0;\n\ttp->snd_ssthresh = TCP_INFINITE_SSTHRESH;\n\ttp->snd_cwnd_cnt = 0;\n\ttp->window_clamp = 0;\n\ttcp_set_ca_state(sk, TCP_CA_Open);\n\ttcp_clear_retrans(tp);\n\tinet_csk_delack_init(sk);\n\t/* Initialize rcv_mss to TCP_MIN_MSS to avoid division by 0\n\t * issue in __tcp_select_window()\n\t */\n\ticsk->icsk_ack.rcv_mss = TCP_MIN_MSS;\n\ttcp_init_send_head(sk);\n\tmemset(&tp->rx_opt, 0, sizeof(tp->rx_opt));\n\t__sk_dst_reset(sk);\n\ttcp_saved_syn_free(tp);\n\n\t/* Clean up fastopen related fields */\n\ttcp_free_fastopen_req(tp);\n\tinet->defer_connect = 0;\n\n\tWARN_ON(inet->inet_num && !icsk->icsk_bind_hash);\n\n\tsk->sk_error_report(sk);\n\treturn err;\n}\nEXPORT_SYMBOL(tcp_disconnect);\n\nstatic inline bool tcp_can_repair_sock(const struct sock *sk)\n{\n\treturn ns_capable(sock_net(sk)->user_ns, CAP_NET_ADMIN) &&\n\t\t(sk->sk_state != TCP_LISTEN);\n}\n\nstatic int tcp_repair_set_window(struct tcp_sock *tp, char __user *optbuf, int len)\n{\n\tstruct tcp_repair_window opt;\n\n\tif (!tp->repair)\n\t\treturn -EPERM;\n\n\tif (len != sizeof(opt))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&opt, optbuf, sizeof(opt)))\n\t\treturn -EFAULT;\n\n\tif (opt.max_window < opt.snd_wnd)\n\t\treturn -EINVAL;\n\n\tif (after(opt.snd_wl1, tp->rcv_nxt + opt.rcv_wnd))\n\t\treturn -EINVAL;\n\n\tif (after(opt.rcv_wup, tp->rcv_nxt))\n\t\treturn -EINVAL;\n\n\ttp->snd_wl1\t= opt.snd_wl1;\n\ttp->snd_wnd\t= opt.snd_wnd;\n\ttp->max_window\t= opt.max_window;\n\n\ttp->rcv_wnd\t= opt.rcv_wnd;\n\ttp->rcv_wup\t= opt.rcv_wup;\n\n\treturn 0;\n}\n\nstatic int tcp_repair_options_est(struct tcp_sock *tp,\n\t\tstruct tcp_repair_opt __user *optbuf, unsigned int len)\n{\n\tstruct tcp_repair_opt opt;\n\n\twhile (len >= sizeof(opt)) {\n\t\tif (copy_from_user(&opt, optbuf, sizeof(opt)))\n\t\t\treturn -EFAULT;\n\n\t\toptbuf++;\n\t\tlen -= sizeof(opt);\n\n\t\tswitch (opt.opt_code) {\n\t\tcase TCPOPT_MSS:\n\t\t\ttp->rx_opt.mss_clamp = opt.opt_val;\n\t\t\tbreak;\n\t\tcase TCPOPT_WINDOW:\n\t\t\t{\n\t\t\t\tu16 snd_wscale = opt.opt_val & 0xFFFF;\n\t\t\t\tu16 rcv_wscale = opt.opt_val >> 16;\n\n\t\t\t\tif (snd_wscale > TCP_MAX_WSCALE || rcv_wscale > TCP_MAX_WSCALE)\n\t\t\t\t\treturn -EFBIG;\n\n\t\t\t\ttp->rx_opt.snd_wscale = snd_wscale;\n\t\t\t\ttp->rx_opt.rcv_wscale = rcv_wscale;\n\t\t\t\ttp->rx_opt.wscale_ok = 1;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase TCPOPT_SACK_PERM:\n\t\t\tif (opt.opt_val != 0)\n\t\t\t\treturn -EINVAL;\n\n\t\t\ttp->rx_opt.sack_ok |= TCP_SACK_SEEN;\n\t\t\tif (sysctl_tcp_fack)\n\t\t\t\ttcp_enable_fack(tp);\n\t\t\tbreak;\n\t\tcase TCPOPT_TIMESTAMP:\n\t\t\tif (opt.opt_val != 0)\n\t\t\t\treturn -EINVAL;\n\n\t\t\ttp->rx_opt.tstamp_ok = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n/*\n *\tSocket option code for TCP.\n */\nstatic int do_tcp_setsockopt(struct sock *sk, int level,\n\t\tint optname, char __user *optval, unsigned int optlen)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val;\n\tint err = 0;\n\n\t/* These are data/string values, all the others are ints */\n\tswitch (optname) {\n\tcase TCP_CONGESTION: {\n\t\tchar name[TCP_CA_NAME_MAX];\n\n\t\tif (optlen < 1)\n\t\t\treturn -EINVAL;\n\n\t\tval = strncpy_from_user(name, optval,\n\t\t\t\t\tmin_t(long, TCP_CA_NAME_MAX-1, optlen));\n\t\tif (val < 0)\n\t\t\treturn -EFAULT;\n\t\tname[val] = 0;\n\n\t\tlock_sock(sk);\n\t\terr = tcp_set_congestion_control(sk, name);\n\t\trelease_sock(sk);\n\t\treturn err;\n\t}\n\tdefault:\n\t\t/* fallthru */\n\t\tbreak;\n\t}\n\n\tif (optlen < sizeof(int))\n\t\treturn -EINVAL;\n\n\tif (get_user(val, (int __user *)optval))\n\t\treturn -EFAULT;\n\n\tlock_sock(sk);\n\n\tswitch (optname) {\n\tcase TCP_MAXSEG:\n\t\t/* Values greater than interface MTU won't take effect. However\n\t\t * at the point when this call is done we typically don't yet\n\t\t * know which interface is going to be used */\n\t\tif (val && (val < TCP_MIN_MSS || val > MAX_TCP_WINDOW)) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\ttp->rx_opt.user_mss = val;\n\t\tbreak;\n\n\tcase TCP_NODELAY:\n\t\tif (val) {\n\t\t\t/* TCP_NODELAY is weaker than TCP_CORK, so that\n\t\t\t * this option on corked socket is remembered, but\n\t\t\t * it is not activated until cork is cleared.\n\t\t\t *\n\t\t\t * However, when TCP_NODELAY is set we make\n\t\t\t * an explicit push, which overrides even TCP_CORK\n\t\t\t * for currently queued segments.\n\t\t\t */\n\t\t\ttp->nonagle |= TCP_NAGLE_OFF|TCP_NAGLE_PUSH;\n\t\t\ttcp_push_pending_frames(sk);\n\t\t} else {\n\t\t\ttp->nonagle &= ~TCP_NAGLE_OFF;\n\t\t}\n\t\tbreak;\n\n\tcase TCP_THIN_LINEAR_TIMEOUTS:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->thin_lto = val;\n\t\tbreak;\n\n\tcase TCP_THIN_DUPACK:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\tbreak;\n\n\tcase TCP_REPAIR:\n\t\tif (!tcp_can_repair_sock(sk))\n\t\t\terr = -EPERM;\n\t\telse if (val == 1) {\n\t\t\ttp->repair = 1;\n\t\t\tsk->sk_reuse = SK_FORCE_REUSE;\n\t\t\ttp->repair_queue = TCP_NO_QUEUE;\n\t\t} else if (val == 0) {\n\t\t\ttp->repair = 0;\n\t\t\tsk->sk_reuse = SK_NO_REUSE;\n\t\t\ttcp_send_window_probe(sk);\n\t\t} else\n\t\t\terr = -EINVAL;\n\n\t\tbreak;\n\n\tcase TCP_REPAIR_QUEUE:\n\t\tif (!tp->repair)\n\t\t\terr = -EPERM;\n\t\telse if (val < TCP_QUEUES_NR)\n\t\t\ttp->repair_queue = val;\n\t\telse\n\t\t\terr = -EINVAL;\n\t\tbreak;\n\n\tcase TCP_QUEUE_SEQ:\n\t\tif (sk->sk_state != TCP_CLOSE)\n\t\t\terr = -EPERM;\n\t\telse if (tp->repair_queue == TCP_SEND_QUEUE)\n\t\t\ttp->write_seq = val;\n\t\telse if (tp->repair_queue == TCP_RECV_QUEUE)\n\t\t\ttp->rcv_nxt = val;\n\t\telse\n\t\t\terr = -EINVAL;\n\t\tbreak;\n\n\tcase TCP_REPAIR_OPTIONS:\n\t\tif (!tp->repair)\n\t\t\terr = -EINVAL;\n\t\telse if (sk->sk_state == TCP_ESTABLISHED)\n\t\t\terr = tcp_repair_options_est(tp,\n\t\t\t\t\t(struct tcp_repair_opt __user *)optval,\n\t\t\t\t\toptlen);\n\t\telse\n\t\t\terr = -EPERM;\n\t\tbreak;\n\n\tcase TCP_CORK:\n\t\t/* When set indicates to always queue non-full frames.\n\t\t * Later the user clears this option and we transmit\n\t\t * any pending partial frames in the queue.  This is\n\t\t * meant to be used alongside sendfile() to get properly\n\t\t * filled frames when the user (for example) must write\n\t\t * out headers with a write() call first and then use\n\t\t * sendfile to send out the data parts.\n\t\t *\n\t\t * TCP_CORK can be set together with TCP_NODELAY and it is\n\t\t * stronger than TCP_NODELAY.\n\t\t */\n\t\tif (val) {\n\t\t\ttp->nonagle |= TCP_NAGLE_CORK;\n\t\t} else {\n\t\t\ttp->nonagle &= ~TCP_NAGLE_CORK;\n\t\t\tif (tp->nonagle&TCP_NAGLE_OFF)\n\t\t\t\ttp->nonagle |= TCP_NAGLE_PUSH;\n\t\t\ttcp_push_pending_frames(sk);\n\t\t}\n\t\tbreak;\n\n\tcase TCP_KEEPIDLE:\n\t\tif (val < 1 || val > MAX_TCP_KEEPIDLE)\n\t\t\terr = -EINVAL;\n\t\telse {\n\t\t\ttp->keepalive_time = val * HZ;\n\t\t\tif (sock_flag(sk, SOCK_KEEPOPEN) &&\n\t\t\t    !((1 << sk->sk_state) &\n\t\t\t      (TCPF_CLOSE | TCPF_LISTEN))) {\n\t\t\t\tu32 elapsed = keepalive_time_elapsed(tp);\n\t\t\t\tif (tp->keepalive_time > elapsed)\n\t\t\t\t\telapsed = tp->keepalive_time - elapsed;\n\t\t\t\telse\n\t\t\t\t\telapsed = 0;\n\t\t\t\tinet_csk_reset_keepalive_timer(sk, elapsed);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase TCP_KEEPINTVL:\n\t\tif (val < 1 || val > MAX_TCP_KEEPINTVL)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->keepalive_intvl = val * HZ;\n\t\tbreak;\n\tcase TCP_KEEPCNT:\n\t\tif (val < 1 || val > MAX_TCP_KEEPCNT)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->keepalive_probes = val;\n\t\tbreak;\n\tcase TCP_SYNCNT:\n\t\tif (val < 1 || val > MAX_TCP_SYNCNT)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ticsk->icsk_syn_retries = val;\n\t\tbreak;\n\n\tcase TCP_SAVE_SYN:\n\t\tif (val < 0 || val > 1)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ttp->save_syn = val;\n\t\tbreak;\n\n\tcase TCP_LINGER2:\n\t\tif (val < 0)\n\t\t\ttp->linger2 = -1;\n\t\telse if (val > net->ipv4.sysctl_tcp_fin_timeout / HZ)\n\t\t\ttp->linger2 = 0;\n\t\telse\n\t\t\ttp->linger2 = val * HZ;\n\t\tbreak;\n\n\tcase TCP_DEFER_ACCEPT:\n\t\t/* Translate value in seconds to number of retransmits */\n\t\ticsk->icsk_accept_queue.rskq_defer_accept =\n\t\t\tsecs_to_retrans(val, TCP_TIMEOUT_INIT / HZ,\n\t\t\t\t\tTCP_RTO_MAX / HZ);\n\t\tbreak;\n\n\tcase TCP_WINDOW_CLAMP:\n\t\tif (!val) {\n\t\t\tif (sk->sk_state != TCP_CLOSE) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttp->window_clamp = 0;\n\t\t} else\n\t\t\ttp->window_clamp = val < SOCK_MIN_RCVBUF / 2 ?\n\t\t\t\t\t\tSOCK_MIN_RCVBUF / 2 : val;\n\t\tbreak;\n\n\tcase TCP_QUICKACK:\n\t\tif (!val) {\n\t\t\ticsk->icsk_ack.pingpong = 1;\n\t\t} else {\n\t\t\ticsk->icsk_ack.pingpong = 0;\n\t\t\tif ((1 << sk->sk_state) &\n\t\t\t    (TCPF_ESTABLISHED | TCPF_CLOSE_WAIT) &&\n\t\t\t    inet_csk_ack_scheduled(sk)) {\n\t\t\t\ticsk->icsk_ack.pending |= ICSK_ACK_PUSHED;\n\t\t\t\ttcp_cleanup_rbuf(sk, 1);\n\t\t\t\tif (!(val & 1))\n\t\t\t\t\ticsk->icsk_ack.pingpong = 1;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n#ifdef CONFIG_TCP_MD5SIG\n\tcase TCP_MD5SIG:\n\t\t/* Read the IP->Key mappings from userspace */\n\t\terr = tp->af_specific->md5_parse(sk, optval, optlen);\n\t\tbreak;\n#endif\n\tcase TCP_USER_TIMEOUT:\n\t\t/* Cap the max time in ms TCP will retry or probe the window\n\t\t * before giving up and aborting (ETIMEDOUT) a connection.\n\t\t */\n\t\tif (val < 0)\n\t\t\terr = -EINVAL;\n\t\telse\n\t\t\ticsk->icsk_user_timeout = msecs_to_jiffies(val);\n\t\tbreak;\n\n\tcase TCP_FASTOPEN:\n\t\tif (val >= 0 && ((1 << sk->sk_state) & (TCPF_CLOSE |\n\t\t    TCPF_LISTEN))) {\n\t\t\ttcp_fastopen_init_key_once(true);\n\n\t\t\tfastopen_queue_tune(sk, val);\n\t\t} else {\n\t\t\terr = -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase TCP_FASTOPEN_CONNECT:\n\t\tif (val > 1 || val < 0) {\n\t\t\terr = -EINVAL;\n\t\t} else if (sysctl_tcp_fastopen & TFO_CLIENT_ENABLE) {\n\t\t\tif (sk->sk_state == TCP_CLOSE)\n\t\t\t\ttp->fastopen_connect = val;\n\t\t\telse\n\t\t\t\terr = -EINVAL;\n\t\t} else {\n\t\t\terr = -EOPNOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase TCP_TIMESTAMP:\n\t\tif (!tp->repair)\n\t\t\terr = -EPERM;\n\t\telse\n\t\t\ttp->tsoffset = val - tcp_time_stamp;\n\t\tbreak;\n\tcase TCP_REPAIR_WINDOW:\n\t\terr = tcp_repair_set_window(tp, optval, optlen);\n\t\tbreak;\n\tcase TCP_NOTSENT_LOWAT:\n\t\ttp->notsent_lowat = val;\n\t\tsk->sk_write_space(sk);\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOPROTOOPT;\n\t\tbreak;\n\t}\n\n\trelease_sock(sk);\n\treturn err;\n}\n\nint tcp_setsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   unsigned int optlen)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->setsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(tcp_setsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_tcp_setsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, unsigned int optlen)\n{\n\tif (level != SOL_TCP)\n\t\treturn inet_csk_compat_setsockopt(sk, level, optname,\n\t\t\t\t\t\t  optval, optlen);\n\treturn do_tcp_setsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_tcp_setsockopt);\n#endif\n\nstatic void tcp_get_info_chrono_stats(const struct tcp_sock *tp,\n\t\t\t\t      struct tcp_info *info)\n{\n\tu64 stats[__TCP_CHRONO_MAX], total = 0;\n\tenum tcp_chrono i;\n\n\tfor (i = TCP_CHRONO_BUSY; i < __TCP_CHRONO_MAX; ++i) {\n\t\tstats[i] = tp->chrono_stat[i - 1];\n\t\tif (i == tp->chrono_type)\n\t\t\tstats[i] += tcp_time_stamp - tp->chrono_start;\n\t\tstats[i] *= USEC_PER_SEC / HZ;\n\t\ttotal += stats[i];\n\t}\n\n\tinfo->tcpi_busy_time = total;\n\tinfo->tcpi_rwnd_limited = stats[TCP_CHRONO_RWND_LIMITED];\n\tinfo->tcpi_sndbuf_limited = stats[TCP_CHRONO_SNDBUF_LIMITED];\n}\n\n/* Return information about state of tcp endpoint in API format. */\nvoid tcp_get_info(struct sock *sk, struct tcp_info *info)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk); /* iff sk_type == SOCK_STREAM */\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\tu32 now, intv;\n\tu64 rate64;\n\tbool slow;\n\tu32 rate;\n\n\tmemset(info, 0, sizeof(*info));\n\tif (sk->sk_type != SOCK_STREAM)\n\t\treturn;\n\n\tinfo->tcpi_state = sk_state_load(sk);\n\n\t/* Report meaningful fields for all TCP states, including listeners */\n\trate = READ_ONCE(sk->sk_pacing_rate);\n\trate64 = rate != ~0U ? rate : ~0ULL;\n\tinfo->tcpi_pacing_rate = rate64;\n\n\trate = READ_ONCE(sk->sk_max_pacing_rate);\n\trate64 = rate != ~0U ? rate : ~0ULL;\n\tinfo->tcpi_max_pacing_rate = rate64;\n\n\tinfo->tcpi_reordering = tp->reordering;\n\tinfo->tcpi_snd_cwnd = tp->snd_cwnd;\n\n\tif (info->tcpi_state == TCP_LISTEN) {\n\t\t/* listeners aliased fields :\n\t\t * tcpi_unacked -> Number of children ready for accept()\n\t\t * tcpi_sacked  -> max backlog\n\t\t */\n\t\tinfo->tcpi_unacked = sk->sk_ack_backlog;\n\t\tinfo->tcpi_sacked = sk->sk_max_ack_backlog;\n\t\treturn;\n\t}\n\n\tslow = lock_sock_fast(sk);\n\n\tinfo->tcpi_ca_state = icsk->icsk_ca_state;\n\tinfo->tcpi_retransmits = icsk->icsk_retransmits;\n\tinfo->tcpi_probes = icsk->icsk_probes_out;\n\tinfo->tcpi_backoff = icsk->icsk_backoff;\n\n\tif (tp->rx_opt.tstamp_ok)\n\t\tinfo->tcpi_options |= TCPI_OPT_TIMESTAMPS;\n\tif (tcp_is_sack(tp))\n\t\tinfo->tcpi_options |= TCPI_OPT_SACK;\n\tif (tp->rx_opt.wscale_ok) {\n\t\tinfo->tcpi_options |= TCPI_OPT_WSCALE;\n\t\tinfo->tcpi_snd_wscale = tp->rx_opt.snd_wscale;\n\t\tinfo->tcpi_rcv_wscale = tp->rx_opt.rcv_wscale;\n\t}\n\n\tif (tp->ecn_flags & TCP_ECN_OK)\n\t\tinfo->tcpi_options |= TCPI_OPT_ECN;\n\tif (tp->ecn_flags & TCP_ECN_SEEN)\n\t\tinfo->tcpi_options |= TCPI_OPT_ECN_SEEN;\n\tif (tp->syn_data_acked)\n\t\tinfo->tcpi_options |= TCPI_OPT_SYN_DATA;\n\n\tinfo->tcpi_rto = jiffies_to_usecs(icsk->icsk_rto);\n\tinfo->tcpi_ato = jiffies_to_usecs(icsk->icsk_ack.ato);\n\tinfo->tcpi_snd_mss = tp->mss_cache;\n\tinfo->tcpi_rcv_mss = icsk->icsk_ack.rcv_mss;\n\n\tinfo->tcpi_unacked = tp->packets_out;\n\tinfo->tcpi_sacked = tp->sacked_out;\n\n\tinfo->tcpi_lost = tp->lost_out;\n\tinfo->tcpi_retrans = tp->retrans_out;\n\tinfo->tcpi_fackets = tp->fackets_out;\n\n\tnow = tcp_time_stamp;\n\tinfo->tcpi_last_data_sent = jiffies_to_msecs(now - tp->lsndtime);\n\tinfo->tcpi_last_data_recv = jiffies_to_msecs(now - icsk->icsk_ack.lrcvtime);\n\tinfo->tcpi_last_ack_recv = jiffies_to_msecs(now - tp->rcv_tstamp);\n\n\tinfo->tcpi_pmtu = icsk->icsk_pmtu_cookie;\n\tinfo->tcpi_rcv_ssthresh = tp->rcv_ssthresh;\n\tinfo->tcpi_rtt = tp->srtt_us >> 3;\n\tinfo->tcpi_rttvar = tp->mdev_us >> 2;\n\tinfo->tcpi_snd_ssthresh = tp->snd_ssthresh;\n\tinfo->tcpi_advmss = tp->advmss;\n\n\tinfo->tcpi_rcv_rtt = tp->rcv_rtt_est.rtt_us >> 3;\n\tinfo->tcpi_rcv_space = tp->rcvq_space.space;\n\n\tinfo->tcpi_total_retrans = tp->total_retrans;\n\n\tinfo->tcpi_bytes_acked = tp->bytes_acked;\n\tinfo->tcpi_bytes_received = tp->bytes_received;\n\tinfo->tcpi_notsent_bytes = max_t(int, 0, tp->write_seq - tp->snd_nxt);\n\ttcp_get_info_chrono_stats(tp, info);\n\n\tinfo->tcpi_segs_out = tp->segs_out;\n\tinfo->tcpi_segs_in = tp->segs_in;\n\n\tinfo->tcpi_min_rtt = tcp_min_rtt(tp);\n\tinfo->tcpi_data_segs_in = tp->data_segs_in;\n\tinfo->tcpi_data_segs_out = tp->data_segs_out;\n\n\tinfo->tcpi_delivery_rate_app_limited = tp->rate_app_limited ? 1 : 0;\n\trate = READ_ONCE(tp->rate_delivered);\n\tintv = READ_ONCE(tp->rate_interval_us);\n\tif (rate && intv) {\n\t\trate64 = (u64)rate * tp->mss_cache * USEC_PER_SEC;\n\t\tdo_div(rate64, intv);\n\t\tinfo->tcpi_delivery_rate = rate64;\n\t}\n\tunlock_sock_fast(sk, slow);\n}\nEXPORT_SYMBOL_GPL(tcp_get_info);\n\nstruct sk_buff *tcp_get_timestamping_opt_stats(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tstruct sk_buff *stats;\n\tstruct tcp_info info;\n\n\tstats = alloc_skb(5 * nla_total_size_64bit(sizeof(u64)), GFP_ATOMIC);\n\tif (!stats)\n\t\treturn NULL;\n\n\ttcp_get_info_chrono_stats(tp, &info);\n\tnla_put_u64_64bit(stats, TCP_NLA_BUSY,\n\t\t\t  info.tcpi_busy_time, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_RWND_LIMITED,\n\t\t\t  info.tcpi_rwnd_limited, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_SNDBUF_LIMITED,\n\t\t\t  info.tcpi_sndbuf_limited, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_DATA_SEGS_OUT,\n\t\t\t  tp->data_segs_out, TCP_NLA_PAD);\n\tnla_put_u64_64bit(stats, TCP_NLA_TOTAL_RETRANS,\n\t\t\t  tp->total_retrans, TCP_NLA_PAD);\n\treturn stats;\n}\n\nstatic int do_tcp_getsockopt(struct sock *sk, int level,\n\t\tint optname, char __user *optval, int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tstruct net *net = sock_net(sk);\n\tint val, len;\n\n\tif (get_user(len, optlen))\n\t\treturn -EFAULT;\n\n\tlen = min_t(unsigned int, len, sizeof(int));\n\n\tif (len < 0)\n\t\treturn -EINVAL;\n\n\tswitch (optname) {\n\tcase TCP_MAXSEG:\n\t\tval = tp->mss_cache;\n\t\tif (!val && ((1 << sk->sk_state) & (TCPF_CLOSE | TCPF_LISTEN)))\n\t\t\tval = tp->rx_opt.user_mss;\n\t\tif (tp->repair)\n\t\t\tval = tp->rx_opt.mss_clamp;\n\t\tbreak;\n\tcase TCP_NODELAY:\n\t\tval = !!(tp->nonagle&TCP_NAGLE_OFF);\n\t\tbreak;\n\tcase TCP_CORK:\n\t\tval = !!(tp->nonagle&TCP_NAGLE_CORK);\n\t\tbreak;\n\tcase TCP_KEEPIDLE:\n\t\tval = keepalive_time_when(tp) / HZ;\n\t\tbreak;\n\tcase TCP_KEEPINTVL:\n\t\tval = keepalive_intvl_when(tp) / HZ;\n\t\tbreak;\n\tcase TCP_KEEPCNT:\n\t\tval = keepalive_probes(tp);\n\t\tbreak;\n\tcase TCP_SYNCNT:\n\t\tval = icsk->icsk_syn_retries ? : net->ipv4.sysctl_tcp_syn_retries;\n\t\tbreak;\n\tcase TCP_LINGER2:\n\t\tval = tp->linger2;\n\t\tif (val >= 0)\n\t\t\tval = (val ? : net->ipv4.sysctl_tcp_fin_timeout) / HZ;\n\t\tbreak;\n\tcase TCP_DEFER_ACCEPT:\n\t\tval = retrans_to_secs(icsk->icsk_accept_queue.rskq_defer_accept,\n\t\t\t\t      TCP_TIMEOUT_INIT / HZ, TCP_RTO_MAX / HZ);\n\t\tbreak;\n\tcase TCP_WINDOW_CLAMP:\n\t\tval = tp->window_clamp;\n\t\tbreak;\n\tcase TCP_INFO: {\n\t\tstruct tcp_info info;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\ttcp_get_info(sk, &info);\n\n\t\tlen = min_t(unsigned int, len, sizeof(info));\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &info, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_CC_INFO: {\n\t\tconst struct tcp_congestion_ops *ca_ops;\n\t\tunion tcp_cc_info info;\n\t\tsize_t sz = 0;\n\t\tint attr;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tca_ops = icsk->icsk_ca_ops;\n\t\tif (ca_ops && ca_ops->get_info)\n\t\t\tsz = ca_ops->get_info(sk, ~0U, &attr, &info);\n\n\t\tlen = min_t(unsigned int, len, sz);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, &info, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_QUICKACK:\n\t\tval = !icsk->icsk_ack.pingpong;\n\t\tbreak;\n\n\tcase TCP_CONGESTION:\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tlen = min_t(unsigned int, len, TCP_CA_NAME_MAX);\n\t\tif (put_user(len, optlen))\n\t\t\treturn -EFAULT;\n\t\tif (copy_to_user(optval, icsk->icsk_ca_ops->name, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\n\tcase TCP_THIN_LINEAR_TIMEOUTS:\n\t\tval = tp->thin_lto;\n\t\tbreak;\n\n\tcase TCP_THIN_DUPACK:\n\t\tval = 0;\n\t\tbreak;\n\n\tcase TCP_REPAIR:\n\t\tval = tp->repair;\n\t\tbreak;\n\n\tcase TCP_REPAIR_QUEUE:\n\t\tif (tp->repair)\n\t\t\tval = tp->repair_queue;\n\t\telse\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\n\tcase TCP_REPAIR_WINDOW: {\n\t\tstruct tcp_repair_window opt;\n\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tif (len != sizeof(opt))\n\t\t\treturn -EINVAL;\n\n\t\tif (!tp->repair)\n\t\t\treturn -EPERM;\n\n\t\topt.snd_wl1\t= tp->snd_wl1;\n\t\topt.snd_wnd\t= tp->snd_wnd;\n\t\topt.max_window\t= tp->max_window;\n\t\topt.rcv_wnd\t= tp->rcv_wnd;\n\t\topt.rcv_wup\t= tp->rcv_wup;\n\n\t\tif (copy_to_user(optval, &opt, len))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\t}\n\tcase TCP_QUEUE_SEQ:\n\t\tif (tp->repair_queue == TCP_SEND_QUEUE)\n\t\t\tval = tp->write_seq;\n\t\telse if (tp->repair_queue == TCP_RECV_QUEUE)\n\t\t\tval = tp->rcv_nxt;\n\t\telse\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\n\tcase TCP_USER_TIMEOUT:\n\t\tval = jiffies_to_msecs(icsk->icsk_user_timeout);\n\t\tbreak;\n\n\tcase TCP_FASTOPEN:\n\t\tval = icsk->icsk_accept_queue.fastopenq.max_qlen;\n\t\tbreak;\n\n\tcase TCP_FASTOPEN_CONNECT:\n\t\tval = tp->fastopen_connect;\n\t\tbreak;\n\n\tcase TCP_TIMESTAMP:\n\t\tval = tcp_time_stamp + tp->tsoffset;\n\t\tbreak;\n\tcase TCP_NOTSENT_LOWAT:\n\t\tval = tp->notsent_lowat;\n\t\tbreak;\n\tcase TCP_SAVE_SYN:\n\t\tval = tp->save_syn;\n\t\tbreak;\n\tcase TCP_SAVED_SYN: {\n\t\tif (get_user(len, optlen))\n\t\t\treturn -EFAULT;\n\n\t\tlock_sock(sk);\n\t\tif (tp->saved_syn) {\n\t\t\tif (len < tp->saved_syn[0]) {\n\t\t\t\tif (put_user(tp->saved_syn[0], optlen)) {\n\t\t\t\t\trelease_sock(sk);\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tlen = tp->saved_syn[0];\n\t\t\tif (put_user(len, optlen)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tif (copy_to_user(optval, tp->saved_syn + 1, len)) {\n\t\t\t\trelease_sock(sk);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\ttcp_saved_syn_free(tp);\n\t\t\trelease_sock(sk);\n\t\t} else {\n\t\t\trelease_sock(sk);\n\t\t\tlen = 0;\n\t\t\tif (put_user(len, optlen))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\tif (put_user(len, optlen))\n\t\treturn -EFAULT;\n\tif (copy_to_user(optval, &val, len))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nint tcp_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t   int __user *optlen)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (level != SOL_TCP)\n\t\treturn icsk->icsk_af_ops->getsockopt(sk, level, optname,\n\t\t\t\t\t\t     optval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(tcp_getsockopt);\n\n#ifdef CONFIG_COMPAT\nint compat_tcp_getsockopt(struct sock *sk, int level, int optname,\n\t\t\t  char __user *optval, int __user *optlen)\n{\n\tif (level != SOL_TCP)\n\t\treturn inet_csk_compat_getsockopt(sk, level, optname,\n\t\t\t\t\t\t  optval, optlen);\n\treturn do_tcp_getsockopt(sk, level, optname, optval, optlen);\n}\nEXPORT_SYMBOL(compat_tcp_getsockopt);\n#endif\n\n#ifdef CONFIG_TCP_MD5SIG\nstatic DEFINE_PER_CPU(struct tcp_md5sig_pool, tcp_md5sig_pool);\nstatic DEFINE_MUTEX(tcp_md5sig_mutex);\nstatic bool tcp_md5sig_pool_populated = false;\n\nstatic void __tcp_alloc_md5sig_pool(void)\n{\n\tstruct crypto_ahash *hash;\n\tint cpu;\n\n\thash = crypto_alloc_ahash(\"md5\", 0, CRYPTO_ALG_ASYNC);\n\tif (IS_ERR(hash))\n\t\treturn;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tvoid *scratch = per_cpu(tcp_md5sig_pool, cpu).scratch;\n\t\tstruct ahash_request *req;\n\n\t\tif (!scratch) {\n\t\t\tscratch = kmalloc_node(sizeof(union tcp_md5sum_block) +\n\t\t\t\t\t       sizeof(struct tcphdr),\n\t\t\t\t\t       GFP_KERNEL,\n\t\t\t\t\t       cpu_to_node(cpu));\n\t\t\tif (!scratch)\n\t\t\t\treturn;\n\t\t\tper_cpu(tcp_md5sig_pool, cpu).scratch = scratch;\n\t\t}\n\t\tif (per_cpu(tcp_md5sig_pool, cpu).md5_req)\n\t\t\tcontinue;\n\n\t\treq = ahash_request_alloc(hash, GFP_KERNEL);\n\t\tif (!req)\n\t\t\treturn;\n\n\t\tahash_request_set_callback(req, 0, NULL, NULL);\n\n\t\tper_cpu(tcp_md5sig_pool, cpu).md5_req = req;\n\t}\n\t/* before setting tcp_md5sig_pool_populated, we must commit all writes\n\t * to memory. See smp_rmb() in tcp_get_md5sig_pool()\n\t */\n\tsmp_wmb();\n\ttcp_md5sig_pool_populated = true;\n}\n\nbool tcp_alloc_md5sig_pool(void)\n{\n\tif (unlikely(!tcp_md5sig_pool_populated)) {\n\t\tmutex_lock(&tcp_md5sig_mutex);\n\n\t\tif (!tcp_md5sig_pool_populated)\n\t\t\t__tcp_alloc_md5sig_pool();\n\n\t\tmutex_unlock(&tcp_md5sig_mutex);\n\t}\n\treturn tcp_md5sig_pool_populated;\n}\nEXPORT_SYMBOL(tcp_alloc_md5sig_pool);\n\n\n/**\n *\ttcp_get_md5sig_pool - get md5sig_pool for this user\n *\n *\tWe use percpu structure, so if we succeed, we exit with preemption\n *\tand BH disabled, to make sure another thread or softirq handling\n *\twont try to get same context.\n */\nstruct tcp_md5sig_pool *tcp_get_md5sig_pool(void)\n{\n\tlocal_bh_disable();\n\n\tif (tcp_md5sig_pool_populated) {\n\t\t/* coupled with smp_wmb() in __tcp_alloc_md5sig_pool() */\n\t\tsmp_rmb();\n\t\treturn this_cpu_ptr(&tcp_md5sig_pool);\n\t}\n\tlocal_bh_enable();\n\treturn NULL;\n}\nEXPORT_SYMBOL(tcp_get_md5sig_pool);\n\nint tcp_md5_hash_skb_data(struct tcp_md5sig_pool *hp,\n\t\t\t  const struct sk_buff *skb, unsigned int header_len)\n{\n\tstruct scatterlist sg;\n\tconst struct tcphdr *tp = tcp_hdr(skb);\n\tstruct ahash_request *req = hp->md5_req;\n\tunsigned int i;\n\tconst unsigned int head_data_len = skb_headlen(skb) > header_len ?\n\t\t\t\t\t   skb_headlen(skb) - header_len : 0;\n\tconst struct skb_shared_info *shi = skb_shinfo(skb);\n\tstruct sk_buff *frag_iter;\n\n\tsg_init_table(&sg, 1);\n\n\tsg_set_buf(&sg, ((u8 *) tp) + header_len, head_data_len);\n\tahash_request_set_crypt(req, &sg, NULL, head_data_len);\n\tif (crypto_ahash_update(req))\n\t\treturn 1;\n\n\tfor (i = 0; i < shi->nr_frags; ++i) {\n\t\tconst struct skb_frag_struct *f = &shi->frags[i];\n\t\tunsigned int offset = f->page_offset;\n\t\tstruct page *page = skb_frag_page(f) + (offset >> PAGE_SHIFT);\n\n\t\tsg_set_page(&sg, page, skb_frag_size(f),\n\t\t\t    offset_in_page(offset));\n\t\tahash_request_set_crypt(req, &sg, NULL, skb_frag_size(f));\n\t\tif (crypto_ahash_update(req))\n\t\t\treturn 1;\n\t}\n\n\tskb_walk_frags(skb, frag_iter)\n\t\tif (tcp_md5_hash_skb_data(hp, frag_iter, 0))\n\t\t\treturn 1;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(tcp_md5_hash_skb_data);\n\nint tcp_md5_hash_key(struct tcp_md5sig_pool *hp, const struct tcp_md5sig_key *key)\n{\n\tstruct scatterlist sg;\n\n\tsg_init_one(&sg, key->key, key->keylen);\n\tahash_request_set_crypt(hp->md5_req, &sg, NULL, key->keylen);\n\treturn crypto_ahash_update(hp->md5_req);\n}\nEXPORT_SYMBOL(tcp_md5_hash_key);\n\n#endif\n\nvoid tcp_done(struct sock *sk)\n{\n\tstruct request_sock *req = tcp_sk(sk)->fastopen_rsk;\n\n\tif (sk->sk_state == TCP_SYN_SENT || sk->sk_state == TCP_SYN_RECV)\n\t\tTCP_INC_STATS(sock_net(sk), TCP_MIB_ATTEMPTFAILS);\n\n\ttcp_set_state(sk, TCP_CLOSE);\n\ttcp_clear_xmit_timers(sk);\n\tif (req)\n\t\treqsk_fastopen_remove(sk, req, false);\n\n\tsk->sk_shutdown = SHUTDOWN_MASK;\n\n\tif (!sock_flag(sk, SOCK_DEAD))\n\t\tsk->sk_state_change(sk);\n\telse\n\t\tinet_csk_destroy_sock(sk);\n}\nEXPORT_SYMBOL_GPL(tcp_done);\n\nint tcp_abort(struct sock *sk, int err)\n{\n\tif (!sk_fullsock(sk)) {\n\t\tif (sk->sk_state == TCP_NEW_SYN_RECV) {\n\t\t\tstruct request_sock *req = inet_reqsk(sk);\n\n\t\t\tlocal_bh_disable();\n\t\t\tinet_csk_reqsk_queue_drop_and_put(req->rsk_listener,\n\t\t\t\t\t\t\t  req);\n\t\t\tlocal_bh_enable();\n\t\t\treturn 0;\n\t\t}\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/* Don't race with userspace socket closes such as tcp_close. */\n\tlock_sock(sk);\n\n\tif (sk->sk_state == TCP_LISTEN) {\n\t\ttcp_set_state(sk, TCP_CLOSE);\n\t\tinet_csk_listen_stop(sk);\n\t}\n\n\t/* Don't race with BH socket closes such as inet_csk_listen_stop. */\n\tlocal_bh_disable();\n\tbh_lock_sock(sk);\n\n\tif (!sock_flag(sk, SOCK_DEAD)) {\n\t\tsk->sk_err = err;\n\t\t/* This barrier is coupled with smp_rmb() in tcp_poll() */\n\t\tsmp_wmb();\n\t\tsk->sk_error_report(sk);\n\t\tif (tcp_need_reset(sk->sk_state))\n\t\t\ttcp_send_active_reset(sk, GFP_ATOMIC);\n\t\ttcp_done(sk);\n\t}\n\n\tbh_unlock_sock(sk);\n\tlocal_bh_enable();\n\trelease_sock(sk);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(tcp_abort);\n\nextern struct tcp_congestion_ops tcp_reno;\n\nstatic __initdata unsigned long thash_entries;\nstatic int __init set_thash_entries(char *str)\n{\n\tssize_t ret;\n\n\tif (!str)\n\t\treturn 0;\n\n\tret = kstrtoul(str, 0, &thash_entries);\n\tif (ret)\n\t\treturn 0;\n\n\treturn 1;\n}\n__setup(\"thash_entries=\", set_thash_entries);\n\nstatic void __init tcp_init_mem(void)\n{\n\tunsigned long limit = nr_free_buffer_pages() / 16;\n\n\tlimit = max(limit, 128UL);\n\tsysctl_tcp_mem[0] = limit / 4 * 3;\t\t/* 4.68 % */\n\tsysctl_tcp_mem[1] = limit;\t\t\t/* 6.25 % */\n\tsysctl_tcp_mem[2] = sysctl_tcp_mem[0] * 2;\t/* 9.37 % */\n}\n\nvoid __init tcp_init(void)\n{\n\tint max_rshare, max_wshare, cnt;\n\tunsigned long limit;\n\tunsigned int i;\n\n\tBUILD_BUG_ON(sizeof(struct tcp_skb_cb) >\n\t\t     FIELD_SIZEOF(struct sk_buff, cb));\n\n\tpercpu_counter_init(&tcp_sockets_allocated, 0, GFP_KERNEL);\n\tpercpu_counter_init(&tcp_orphan_count, 0, GFP_KERNEL);\n\tinet_hashinfo_init(&tcp_hashinfo);\n\ttcp_hashinfo.bind_bucket_cachep =\n\t\tkmem_cache_create(\"tcp_bind_bucket\",\n\t\t\t\t  sizeof(struct inet_bind_bucket), 0,\n\t\t\t\t  SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);\n\n\t/* Size and allocate the main established and bind bucket\n\t * hash tables.\n\t *\n\t * The methodology is similar to that of the buffer cache.\n\t */\n\ttcp_hashinfo.ehash =\n\t\talloc_large_system_hash(\"TCP established\",\n\t\t\t\t\tsizeof(struct inet_ehash_bucket),\n\t\t\t\t\tthash_entries,\n\t\t\t\t\t17, /* one slot per 128 KB of memory */\n\t\t\t\t\t0,\n\t\t\t\t\tNULL,\n\t\t\t\t\t&tcp_hashinfo.ehash_mask,\n\t\t\t\t\t0,\n\t\t\t\t\tthash_entries ? 0 : 512 * 1024);\n\tfor (i = 0; i <= tcp_hashinfo.ehash_mask; i++)\n\t\tINIT_HLIST_NULLS_HEAD(&tcp_hashinfo.ehash[i].chain, i);\n\n\tif (inet_ehash_locks_alloc(&tcp_hashinfo))\n\t\tpanic(\"TCP: failed to alloc ehash_locks\");\n\ttcp_hashinfo.bhash =\n\t\talloc_large_system_hash(\"TCP bind\",\n\t\t\t\t\tsizeof(struct inet_bind_hashbucket),\n\t\t\t\t\ttcp_hashinfo.ehash_mask + 1,\n\t\t\t\t\t17, /* one slot per 128 KB of memory */\n\t\t\t\t\t0,\n\t\t\t\t\t&tcp_hashinfo.bhash_size,\n\t\t\t\t\tNULL,\n\t\t\t\t\t0,\n\t\t\t\t\t64 * 1024);\n\ttcp_hashinfo.bhash_size = 1U << tcp_hashinfo.bhash_size;\n\tfor (i = 0; i < tcp_hashinfo.bhash_size; i++) {\n\t\tspin_lock_init(&tcp_hashinfo.bhash[i].lock);\n\t\tINIT_HLIST_HEAD(&tcp_hashinfo.bhash[i].chain);\n\t}\n\n\n\tcnt = tcp_hashinfo.ehash_mask + 1;\n\tsysctl_tcp_max_orphans = cnt / 2;\n\n\ttcp_init_mem();\n\t/* Set per-socket limits to no more than 1/128 the pressure threshold */\n\tlimit = nr_free_buffer_pages() << (PAGE_SHIFT - 7);\n\tmax_wshare = min(4UL*1024*1024, limit);\n\tmax_rshare = min(6UL*1024*1024, limit);\n\n\tsysctl_tcp_wmem[0] = SK_MEM_QUANTUM;\n\tsysctl_tcp_wmem[1] = 16*1024;\n\tsysctl_tcp_wmem[2] = max(64*1024, max_wshare);\n\n\tsysctl_tcp_rmem[0] = SK_MEM_QUANTUM;\n\tsysctl_tcp_rmem[1] = 87380;\n\tsysctl_tcp_rmem[2] = max(87380, max_rshare);\n\n\tpr_info(\"Hash tables configured (established %u bind %u)\\n\",\n\t\ttcp_hashinfo.ehash_mask + 1, tcp_hashinfo.bhash_size);\n\n\ttcp_v4_init();\n\ttcp_metrics_init();\n\tBUG_ON(tcp_register_congestion_control(&tcp_reno) != 0);\n\ttcp_tasklet_init();\n}\n"], "filenames": ["net/ipv4/tcp.c"], "buggy_code_start_loc": [2322], "buggy_code_end_loc": [2322], "fixing_code_start_loc": [2323], "fixing_code_end_loc": [2327], "type": "CWE-369", "message": "The tcp_disconnect function in net/ipv4/tcp.c in the Linux kernel before 4.12 allows local users to cause a denial of service (__tcp_select_window divide-by-zero error and system crash) by triggering a disconnect within a certain tcp_recvmsg code path.", "other": {"cve": {"id": "CVE-2017-14106", "sourceIdentifier": "cve@mitre.org", "published": "2017-09-01T16:29:00.377", "lastModified": "2018-07-13T01:29:00.667", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The tcp_disconnect function in net/ipv4/tcp.c in the Linux kernel before 4.12 allows local users to cause a denial of service (__tcp_select_window divide-by-zero error and system crash) by triggering a disconnect within a certain tcp_recvmsg code path."}, {"lang": "es", "value": "La funci\u00f3n tcp_disconnect en net/ipv4/tcp.c en el kernel de Linux en versiones anteriores a la 4.12 permite que usuarios locales provoquen una denegaci\u00f3n de servicio allows local users to cause a denial of service (error __tcp_select_window de divisi\u00f3n por cero y bloqueo del sistema) desencadenando una desconexi\u00f3n en una ruta de c\u00f3digo tcp_recvmsg determinada."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-369"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.11.12", "matchCriteriaId": "13332751-6BF4-4D8A-A5D2-62A8AF6C1F92"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=499350a5a6e7512d9ed369ed63a4244b6536f4f8", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2018-01/msg00007.html", "source": "cve@mitre.org"}, {"url": "http://www.debian.org/security/2017/dsa-3981", "source": "cve@mitre.org"}, {"url": "http://www.securityfocus.com/bid/100878", "source": "cve@mitre.org"}, {"url": "http://www.securitytracker.com/id/1039549", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2017:2918", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2017:2930", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2017:2931", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2017:3200", "source": "cve@mitre.org"}, {"url": "https://access.redhat.com/errata/RHSA-2018:2172", "source": "cve@mitre.org"}, {"url": "https://github.com/torvalds/linux/commit/499350a5a6e7512d9ed369ed63a4244b6536f4f8", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://www.mail-archive.com/netdev@vger.kernel.org/msg186255.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/499350a5a6e7512d9ed369ed63a4244b6536f4f8"}}