{"buggy_code": ["#undef DEBUG\n\n/*\n * ARM performance counter support.\n *\n * Copyright (C) 2009 picoChip Designs, Ltd., Jamie Iles\n * Copyright (C) 2010 ARM Ltd., Will Deacon <will.deacon@arm.com>\n *\n * This code is based on the sparc64 perf event code, which is in turn based\n * on the x86 code. Callchain code is based on the ARM OProfile backtrace\n * code.\n */\n#define pr_fmt(fmt) \"hw perfevents: \" fmt\n\n#include <linux/kernel.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/uaccess.h>\n\n#include <asm/irq_regs.h>\n#include <asm/pmu.h>\n#include <asm/stacktrace.h>\n\nstatic int\narmpmu_map_cache_event(const unsigned (*cache_map)\n\t\t\t\t      [PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t      [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t      [PERF_COUNT_HW_CACHE_RESULT_MAX],\n\t\t       u64 config)\n{\n\tunsigned int cache_type, cache_op, cache_result, ret;\n\n\tcache_type = (config >>  0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn -EINVAL;\n\n\tcache_op = (config >>  8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn -EINVAL;\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tret = (int)(*cache_map)[cache_type][cache_op][cache_result];\n\n\tif (ret == CACHE_OP_UNSUPPORTED)\n\t\treturn -ENOENT;\n\n\treturn ret;\n}\n\nstatic int\narmpmu_map_hw_event(const unsigned (*event_map)[PERF_COUNT_HW_MAX], u64 config)\n{\n\tint mapping = (*event_map)[config];\n\treturn mapping == HW_OP_UNSUPPORTED ? -ENOENT : mapping;\n}\n\nstatic int\narmpmu_map_raw_event(u32 raw_event_mask, u64 config)\n{\n\treturn (int)(config & raw_event_mask);\n}\n\nint\narmpmu_map_event(struct perf_event *event,\n\t\t const unsigned (*event_map)[PERF_COUNT_HW_MAX],\n\t\t const unsigned (*cache_map)\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX],\n\t\t u32 raw_event_mask)\n{\n\tu64 config = event->attr.config;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\treturn armpmu_map_hw_event(event_map, config);\n\tcase PERF_TYPE_HW_CACHE:\n\t\treturn armpmu_map_cache_event(cache_map, config);\n\tcase PERF_TYPE_RAW:\n\t\treturn armpmu_map_raw_event(raw_event_mask, config);\n\t}\n\n\treturn -ENOENT;\n}\n\nint armpmu_event_set_period(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint ret = 0;\n\n\t/* The period may have been changed by PERF_EVENT_IOC_PERIOD */\n\tif (unlikely(period != hwc->last_period))\n\t\tleft = period - (hwc->last_period - left);\n\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (left > (s64)armpmu->max_period)\n\t\tleft = armpmu->max_period;\n\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\tarmpmu->write_counter(event, (u64)(-left) & 0xffffffff);\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\nu64 armpmu_event_update(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 delta, prev_raw_count, new_raw_count;\n\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tnew_raw_count = armpmu->read_counter(event);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t     new_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\tdelta = (new_raw_count - prev_raw_count) & armpmu->max_period;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\nstatic void\narmpmu_read(struct perf_event *event)\n{\n\tarmpmu_event_update(event);\n}\n\nstatic void\narmpmu_stop(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/*\n\t * ARM pmu always has to update the counter, so ignore\n\t * PERF_EF_UPDATE, see comments in armpmu_start().\n\t */\n\tif (!(hwc->state & PERF_HES_STOPPED)) {\n\t\tarmpmu->disable(event);\n\t\tarmpmu_event_update(event);\n\t\thwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\t}\n}\n\nstatic void armpmu_start(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/*\n\t * ARM pmu always has to reprogram the period, so ignore\n\t * PERF_EF_RELOAD, see the comment below.\n\t */\n\tif (flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\n\thwc->state = 0;\n\t/*\n\t * Set the period again. Some counters can't be stopped, so when we\n\t * were stopped we simply disabled the IRQ source and the counter\n\t * may have been left counting. If we don't do this step then we may\n\t * get an interrupt too soon or *way* too late if the overflow has\n\t * happened since disabling.\n\t */\n\tarmpmu_event_set_period(event);\n\tarmpmu->enable(event);\n}\n\nstatic void\narmpmu_del(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tarmpmu_stop(event, PERF_EF_UPDATE);\n\thw_events->events[idx] = NULL;\n\tclear_bit(idx, hw_events->used_mask);\n\n\tperf_event_update_userpage(event);\n}\n\nstatic int\narmpmu_add(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\tint err = 0;\n\n\tperf_pmu_disable(event->pmu);\n\n\t/* If we don't have a space for the counter then finish early. */\n\tidx = armpmu->get_event_idx(hw_events, event);\n\tif (idx < 0) {\n\t\terr = idx;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If there is an event in the counter we are going to use then make\n\t * sure it is disabled.\n\t */\n\tevent->hw.idx = idx;\n\tarmpmu->disable(event);\n\thw_events->events[idx] = event;\n\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\tif (flags & PERF_EF_START)\n\t\tarmpmu_start(event, PERF_EF_RELOAD);\n\n\t/* Propagate our changes to the userspace mapping. */\n\tperf_event_update_userpage(event);\n\nout:\n\tperf_pmu_enable(event->pmu);\n\treturn err;\n}\n\nstatic int\nvalidate_event(struct pmu_hw_events *hw_events,\n\t       struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu *leader_pmu = event->group_leader->pmu;\n\n\tif (event->pmu != leader_pmu || event->state < PERF_EVENT_STATE_OFF)\n\t\treturn 1;\n\n\tif (event->state == PERF_EVENT_STATE_OFF && !event->attr.enable_on_exec)\n\t\treturn 1;\n\n\treturn armpmu->get_event_idx(hw_events, event) >= 0;\n}\n\nstatic int\nvalidate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct pmu_hw_events fake_pmu;\n\tDECLARE_BITMAP(fake_used_mask, ARMPMU_MAX_HWEVENTS);\n\n\t/*\n\t * Initialise the fake PMU. We only need to populate the\n\t * used_mask for the purposes of validation.\n\t */\n\tmemset(fake_used_mask, 0, sizeof(fake_used_mask));\n\tfake_pmu.used_mask = fake_used_mask;\n\n\tif (!validate_event(&fake_pmu, leader))\n\t\treturn -EINVAL;\n\n\tlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\n\t\tif (!validate_event(&fake_pmu, sibling))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!validate_event(&fake_pmu, event))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic irqreturn_t armpmu_dispatch_irq(int irq, void *dev)\n{\n\tstruct arm_pmu *armpmu = (struct arm_pmu *) dev;\n\tstruct platform_device *plat_device = armpmu->plat_device;\n\tstruct arm_pmu_platdata *plat = dev_get_platdata(&plat_device->dev);\n\n\tif (plat && plat->handle_irq)\n\t\treturn plat->handle_irq(irq, dev, armpmu->handle_irq);\n\telse\n\t\treturn armpmu->handle_irq(irq, dev);\n}\n\nstatic void\narmpmu_release_hardware(struct arm_pmu *armpmu)\n{\n\tarmpmu->free_irq(armpmu);\n\tpm_runtime_put_sync(&armpmu->plat_device->dev);\n}\n\nstatic int\narmpmu_reserve_hardware(struct arm_pmu *armpmu)\n{\n\tint err;\n\tstruct platform_device *pmu_device = armpmu->plat_device;\n\n\tif (!pmu_device)\n\t\treturn -ENODEV;\n\n\tpm_runtime_get_sync(&pmu_device->dev);\n\terr = armpmu->request_irq(armpmu, armpmu_dispatch_irq);\n\tif (err) {\n\t\tarmpmu_release_hardware(armpmu);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void\nhw_perf_event_destroy(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tatomic_t *active_events\t = &armpmu->active_events;\n\tstruct mutex *pmu_reserve_mutex = &armpmu->reserve_mutex;\n\n\tif (atomic_dec_and_mutex_lock(active_events, pmu_reserve_mutex)) {\n\t\tarmpmu_release_hardware(armpmu);\n\t\tmutex_unlock(pmu_reserve_mutex);\n\t}\n}\n\nstatic int\nevent_requires_mode_exclusion(struct perf_event_attr *attr)\n{\n\treturn attr->exclude_idle || attr->exclude_user ||\n\t       attr->exclude_kernel || attr->exclude_hv;\n}\n\nstatic int\n__hw_perf_event_init(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint mapping;\n\n\tmapping = armpmu->map_event(event);\n\n\tif (mapping < 0) {\n\t\tpr_debug(\"event %x:%llx not supported\\n\", event->attr.type,\n\t\t\t event->attr.config);\n\t\treturn mapping;\n\t}\n\n\t/*\n\t * We don't assign an index until we actually place the event onto\n\t * hardware. Use -1 to signify that we haven't decided where to put it\n\t * yet. For SMP systems, each core has it's own PMU so we can't do any\n\t * clever allocation or constraints checking at this point.\n\t */\n\thwc->idx\t\t= -1;\n\thwc->config_base\t= 0;\n\thwc->config\t\t= 0;\n\thwc->event_base\t\t= 0;\n\n\t/*\n\t * Check whether we need to exclude the counter from certain modes.\n\t */\n\tif ((!armpmu->set_event_filter ||\n\t     armpmu->set_event_filter(hwc, &event->attr)) &&\n\t     event_requires_mode_exclusion(&event->attr)) {\n\t\tpr_debug(\"ARM performance counters do not support \"\n\t\t\t \"mode exclusion\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/*\n\t * Store the event encoding into the config_base field.\n\t */\n\thwc->config_base\t    |= (unsigned long)mapping;\n\n\tif (!hwc->sample_period) {\n\t\t/*\n\t\t * For non-sampling runs, limit the sample_period to half\n\t\t * of the counter width. That way, the new counter value\n\t\t * is far less likely to overtake the previous one unless\n\t\t * you have some serious IRQ latency issues.\n\t\t */\n\t\thwc->sample_period  = armpmu->max_period >> 1;\n\t\thwc->last_period    = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t}\n\n\tif (event->group_leader != event) {\n\t\tif (validate_group(event) != 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int armpmu_event_init(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tint err = 0;\n\tatomic_t *active_events = &armpmu->active_events;\n\n\t/* does not support taken branch sampling */\n\tif (has_branch_stack(event))\n\t\treturn -EOPNOTSUPP;\n\n\tif (armpmu->map_event(event) == -ENOENT)\n\t\treturn -ENOENT;\n\n\tevent->destroy = hw_perf_event_destroy;\n\n\tif (!atomic_inc_not_zero(active_events)) {\n\t\tmutex_lock(&armpmu->reserve_mutex);\n\t\tif (atomic_read(active_events) == 0)\n\t\t\terr = armpmu_reserve_hardware(armpmu);\n\n\t\tif (!err)\n\t\t\tatomic_inc(active_events);\n\t\tmutex_unlock(&armpmu->reserve_mutex);\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\terr = __hw_perf_event_init(event);\n\tif (err)\n\t\thw_perf_event_destroy(event);\n\n\treturn err;\n}\n\nstatic void armpmu_enable(struct pmu *pmu)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tint enabled = bitmap_weight(hw_events->used_mask, armpmu->num_events);\n\n\tif (enabled)\n\t\tarmpmu->start(armpmu);\n}\n\nstatic void armpmu_disable(struct pmu *pmu)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(pmu);\n\tarmpmu->stop(armpmu);\n}\n\n#ifdef CONFIG_PM_RUNTIME\nstatic int armpmu_runtime_resume(struct device *dev)\n{\n\tstruct arm_pmu_platdata *plat = dev_get_platdata(dev);\n\n\tif (plat && plat->runtime_resume)\n\t\treturn plat->runtime_resume(dev);\n\n\treturn 0;\n}\n\nstatic int armpmu_runtime_suspend(struct device *dev)\n{\n\tstruct arm_pmu_platdata *plat = dev_get_platdata(dev);\n\n\tif (plat && plat->runtime_suspend)\n\t\treturn plat->runtime_suspend(dev);\n\n\treturn 0;\n}\n#endif\n\nconst struct dev_pm_ops armpmu_dev_pm_ops = {\n\tSET_RUNTIME_PM_OPS(armpmu_runtime_suspend, armpmu_runtime_resume, NULL)\n};\n\nstatic void armpmu_init(struct arm_pmu *armpmu)\n{\n\tatomic_set(&armpmu->active_events, 0);\n\tmutex_init(&armpmu->reserve_mutex);\n\n\tarmpmu->pmu = (struct pmu) {\n\t\t.pmu_enable\t= armpmu_enable,\n\t\t.pmu_disable\t= armpmu_disable,\n\t\t.event_init\t= armpmu_event_init,\n\t\t.add\t\t= armpmu_add,\n\t\t.del\t\t= armpmu_del,\n\t\t.start\t\t= armpmu_start,\n\t\t.stop\t\t= armpmu_stop,\n\t\t.read\t\t= armpmu_read,\n\t};\n}\n\nint armpmu_register(struct arm_pmu *armpmu, int type)\n{\n\tarmpmu_init(armpmu);\n\tpm_runtime_enable(&armpmu->plat_device->dev);\n\tpr_info(\"enabled with %s PMU driver, %d counters available\\n\",\n\t\t\tarmpmu->name, armpmu->num_events);\n\treturn perf_pmu_register(&armpmu->pmu, armpmu->name, type);\n}\n\n/*\n * Callchain handling code.\n */\n\n/*\n * The registers we're interested in are at the end of the variable\n * length saved register structure. The fp points at the end of this\n * structure so the address of this struct is:\n * (struct frame_tail *)(xxx->fp)-1\n *\n * This code has been adapted from the ARM OProfile support.\n */\nstruct frame_tail {\n\tstruct frame_tail __user *fp;\n\tunsigned long sp;\n\tunsigned long lr;\n} __attribute__((packed));\n\n/*\n * Get the return address for a single stackframe and return a pointer to the\n * next frame tail.\n */\nstatic struct frame_tail __user *\nuser_backtrace(struct frame_tail __user *tail,\n\t       struct perf_callchain_entry *entry)\n{\n\tstruct frame_tail buftail;\n\n\t/* Also check accessibility of one struct frame_tail beyond */\n\tif (!access_ok(VERIFY_READ, tail, sizeof(buftail)))\n\t\treturn NULL;\n\tif (__copy_from_user_inatomic(&buftail, tail, sizeof(buftail)))\n\t\treturn NULL;\n\n\tperf_callchain_store(entry, buftail.lr);\n\n\t/*\n\t * Frame pointers should strictly progress back up the stack\n\t * (towards higher addresses).\n\t */\n\tif (tail + 1 >= buftail.fp)\n\t\treturn NULL;\n\n\treturn buftail.fp - 1;\n}\n\nvoid\nperf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs)\n{\n\tstruct frame_tail __user *tail;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tperf_callchain_store(entry, regs->ARM_pc);\n\ttail = (struct frame_tail __user *)regs->ARM_fp - 1;\n\n\twhile ((entry->nr < PERF_MAX_STACK_DEPTH) &&\n\t       tail && !((unsigned long)tail & 0x3))\n\t\ttail = user_backtrace(tail, entry);\n}\n\n/*\n * Gets called by walk_stackframe() for every stackframe. This will be called\n * whist unwinding the stackframe and is like a subroutine return so we use\n * the PC.\n */\nstatic int\ncallchain_trace(struct stackframe *fr,\n\t\tvoid *data)\n{\n\tstruct perf_callchain_entry *entry = data;\n\tperf_callchain_store(entry, fr->pc);\n\treturn 0;\n}\n\nvoid\nperf_callchain_kernel(struct perf_callchain_entry *entry, struct pt_regs *regs)\n{\n\tstruct stackframe fr;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tfr.fp = regs->ARM_fp;\n\tfr.sp = regs->ARM_sp;\n\tfr.lr = regs->ARM_lr;\n\tfr.pc = regs->ARM_pc;\n\twalk_stackframe(&fr, callchain_trace, entry);\n}\n\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest())\n\t\treturn perf_guest_cbs->get_guest_ip();\n\n\treturn instruction_pointer(regs);\n}\n\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tint misc = 0;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\tif (perf_guest_cbs->is_user_mode())\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_KERNEL;\n\t} else {\n\t\tif (user_mode(regs))\n\t\t\tmisc |= PERF_RECORD_MISC_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_KERNEL;\n\t}\n\n\treturn misc;\n}\n"], "fixing_code": ["#undef DEBUG\n\n/*\n * ARM performance counter support.\n *\n * Copyright (C) 2009 picoChip Designs, Ltd., Jamie Iles\n * Copyright (C) 2010 ARM Ltd., Will Deacon <will.deacon@arm.com>\n *\n * This code is based on the sparc64 perf event code, which is in turn based\n * on the x86 code. Callchain code is based on the ARM OProfile backtrace\n * code.\n */\n#define pr_fmt(fmt) \"hw perfevents: \" fmt\n\n#include <linux/kernel.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/uaccess.h>\n\n#include <asm/irq_regs.h>\n#include <asm/pmu.h>\n#include <asm/stacktrace.h>\n\nstatic int\narmpmu_map_cache_event(const unsigned (*cache_map)\n\t\t\t\t      [PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t      [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t      [PERF_COUNT_HW_CACHE_RESULT_MAX],\n\t\t       u64 config)\n{\n\tunsigned int cache_type, cache_op, cache_result, ret;\n\n\tcache_type = (config >>  0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn -EINVAL;\n\n\tcache_op = (config >>  8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn -EINVAL;\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tret = (int)(*cache_map)[cache_type][cache_op][cache_result];\n\n\tif (ret == CACHE_OP_UNSUPPORTED)\n\t\treturn -ENOENT;\n\n\treturn ret;\n}\n\nstatic int\narmpmu_map_hw_event(const unsigned (*event_map)[PERF_COUNT_HW_MAX], u64 config)\n{\n\tint mapping = (*event_map)[config];\n\treturn mapping == HW_OP_UNSUPPORTED ? -ENOENT : mapping;\n}\n\nstatic int\narmpmu_map_raw_event(u32 raw_event_mask, u64 config)\n{\n\treturn (int)(config & raw_event_mask);\n}\n\nint\narmpmu_map_event(struct perf_event *event,\n\t\t const unsigned (*event_map)[PERF_COUNT_HW_MAX],\n\t\t const unsigned (*cache_map)\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX],\n\t\t u32 raw_event_mask)\n{\n\tu64 config = event->attr.config;\n\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\treturn armpmu_map_hw_event(event_map, config);\n\tcase PERF_TYPE_HW_CACHE:\n\t\treturn armpmu_map_cache_event(cache_map, config);\n\tcase PERF_TYPE_RAW:\n\t\treturn armpmu_map_raw_event(raw_event_mask, config);\n\t}\n\n\treturn -ENOENT;\n}\n\nint armpmu_event_set_period(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint ret = 0;\n\n\t/* The period may have been changed by PERF_EVENT_IOC_PERIOD */\n\tif (unlikely(period != hwc->last_period))\n\t\tleft = period - (hwc->last_period - left);\n\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (left > (s64)armpmu->max_period)\n\t\tleft = armpmu->max_period;\n\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\tarmpmu->write_counter(event, (u64)(-left) & 0xffffffff);\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\nu64 armpmu_event_update(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 delta, prev_raw_count, new_raw_count;\n\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tnew_raw_count = armpmu->read_counter(event);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t     new_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\tdelta = (new_raw_count - prev_raw_count) & armpmu->max_period;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\nstatic void\narmpmu_read(struct perf_event *event)\n{\n\tarmpmu_event_update(event);\n}\n\nstatic void\narmpmu_stop(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/*\n\t * ARM pmu always has to update the counter, so ignore\n\t * PERF_EF_UPDATE, see comments in armpmu_start().\n\t */\n\tif (!(hwc->state & PERF_HES_STOPPED)) {\n\t\tarmpmu->disable(event);\n\t\tarmpmu_event_update(event);\n\t\thwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\t}\n}\n\nstatic void armpmu_start(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t/*\n\t * ARM pmu always has to reprogram the period, so ignore\n\t * PERF_EF_RELOAD, see the comment below.\n\t */\n\tif (flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\n\thwc->state = 0;\n\t/*\n\t * Set the period again. Some counters can't be stopped, so when we\n\t * were stopped we simply disabled the IRQ source and the counter\n\t * may have been left counting. If we don't do this step then we may\n\t * get an interrupt too soon or *way* too late if the overflow has\n\t * happened since disabling.\n\t */\n\tarmpmu_event_set_period(event);\n\tarmpmu->enable(event);\n}\n\nstatic void\narmpmu_del(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tarmpmu_stop(event, PERF_EF_UPDATE);\n\thw_events->events[idx] = NULL;\n\tclear_bit(idx, hw_events->used_mask);\n\n\tperf_event_update_userpage(event);\n}\n\nstatic int\narmpmu_add(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\tint err = 0;\n\n\tperf_pmu_disable(event->pmu);\n\n\t/* If we don't have a space for the counter then finish early. */\n\tidx = armpmu->get_event_idx(hw_events, event);\n\tif (idx < 0) {\n\t\terr = idx;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If there is an event in the counter we are going to use then make\n\t * sure it is disabled.\n\t */\n\tevent->hw.idx = idx;\n\tarmpmu->disable(event);\n\thw_events->events[idx] = event;\n\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\tif (flags & PERF_EF_START)\n\t\tarmpmu_start(event, PERF_EF_RELOAD);\n\n\t/* Propagate our changes to the userspace mapping. */\n\tperf_event_update_userpage(event);\n\nout:\n\tperf_pmu_enable(event->pmu);\n\treturn err;\n}\n\nstatic int\nvalidate_event(struct pmu_hw_events *hw_events,\n\t       struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu *leader_pmu = event->group_leader->pmu;\n\n\tif (is_software_event(event))\n\t\treturn 1;\n\n\tif (event->pmu != leader_pmu || event->state < PERF_EVENT_STATE_OFF)\n\t\treturn 1;\n\n\tif (event->state == PERF_EVENT_STATE_OFF && !event->attr.enable_on_exec)\n\t\treturn 1;\n\n\treturn armpmu->get_event_idx(hw_events, event) >= 0;\n}\n\nstatic int\nvalidate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct pmu_hw_events fake_pmu;\n\tDECLARE_BITMAP(fake_used_mask, ARMPMU_MAX_HWEVENTS);\n\n\t/*\n\t * Initialise the fake PMU. We only need to populate the\n\t * used_mask for the purposes of validation.\n\t */\n\tmemset(fake_used_mask, 0, sizeof(fake_used_mask));\n\tfake_pmu.used_mask = fake_used_mask;\n\n\tif (!validate_event(&fake_pmu, leader))\n\t\treturn -EINVAL;\n\n\tlist_for_each_entry(sibling, &leader->sibling_list, group_entry) {\n\t\tif (!validate_event(&fake_pmu, sibling))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!validate_event(&fake_pmu, event))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic irqreturn_t armpmu_dispatch_irq(int irq, void *dev)\n{\n\tstruct arm_pmu *armpmu = (struct arm_pmu *) dev;\n\tstruct platform_device *plat_device = armpmu->plat_device;\n\tstruct arm_pmu_platdata *plat = dev_get_platdata(&plat_device->dev);\n\n\tif (plat && plat->handle_irq)\n\t\treturn plat->handle_irq(irq, dev, armpmu->handle_irq);\n\telse\n\t\treturn armpmu->handle_irq(irq, dev);\n}\n\nstatic void\narmpmu_release_hardware(struct arm_pmu *armpmu)\n{\n\tarmpmu->free_irq(armpmu);\n\tpm_runtime_put_sync(&armpmu->plat_device->dev);\n}\n\nstatic int\narmpmu_reserve_hardware(struct arm_pmu *armpmu)\n{\n\tint err;\n\tstruct platform_device *pmu_device = armpmu->plat_device;\n\n\tif (!pmu_device)\n\t\treturn -ENODEV;\n\n\tpm_runtime_get_sync(&pmu_device->dev);\n\terr = armpmu->request_irq(armpmu, armpmu_dispatch_irq);\n\tif (err) {\n\t\tarmpmu_release_hardware(armpmu);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void\nhw_perf_event_destroy(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tatomic_t *active_events\t = &armpmu->active_events;\n\tstruct mutex *pmu_reserve_mutex = &armpmu->reserve_mutex;\n\n\tif (atomic_dec_and_mutex_lock(active_events, pmu_reserve_mutex)) {\n\t\tarmpmu_release_hardware(armpmu);\n\t\tmutex_unlock(pmu_reserve_mutex);\n\t}\n}\n\nstatic int\nevent_requires_mode_exclusion(struct perf_event_attr *attr)\n{\n\treturn attr->exclude_idle || attr->exclude_user ||\n\t       attr->exclude_kernel || attr->exclude_hv;\n}\n\nstatic int\n__hw_perf_event_init(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint mapping;\n\n\tmapping = armpmu->map_event(event);\n\n\tif (mapping < 0) {\n\t\tpr_debug(\"event %x:%llx not supported\\n\", event->attr.type,\n\t\t\t event->attr.config);\n\t\treturn mapping;\n\t}\n\n\t/*\n\t * We don't assign an index until we actually place the event onto\n\t * hardware. Use -1 to signify that we haven't decided where to put it\n\t * yet. For SMP systems, each core has it's own PMU so we can't do any\n\t * clever allocation or constraints checking at this point.\n\t */\n\thwc->idx\t\t= -1;\n\thwc->config_base\t= 0;\n\thwc->config\t\t= 0;\n\thwc->event_base\t\t= 0;\n\n\t/*\n\t * Check whether we need to exclude the counter from certain modes.\n\t */\n\tif ((!armpmu->set_event_filter ||\n\t     armpmu->set_event_filter(hwc, &event->attr)) &&\n\t     event_requires_mode_exclusion(&event->attr)) {\n\t\tpr_debug(\"ARM performance counters do not support \"\n\t\t\t \"mode exclusion\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/*\n\t * Store the event encoding into the config_base field.\n\t */\n\thwc->config_base\t    |= (unsigned long)mapping;\n\n\tif (!hwc->sample_period) {\n\t\t/*\n\t\t * For non-sampling runs, limit the sample_period to half\n\t\t * of the counter width. That way, the new counter value\n\t\t * is far less likely to overtake the previous one unless\n\t\t * you have some serious IRQ latency issues.\n\t\t */\n\t\thwc->sample_period  = armpmu->max_period >> 1;\n\t\thwc->last_period    = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t}\n\n\tif (event->group_leader != event) {\n\t\tif (validate_group(event) != 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int armpmu_event_init(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tint err = 0;\n\tatomic_t *active_events = &armpmu->active_events;\n\n\t/* does not support taken branch sampling */\n\tif (has_branch_stack(event))\n\t\treturn -EOPNOTSUPP;\n\n\tif (armpmu->map_event(event) == -ENOENT)\n\t\treturn -ENOENT;\n\n\tevent->destroy = hw_perf_event_destroy;\n\n\tif (!atomic_inc_not_zero(active_events)) {\n\t\tmutex_lock(&armpmu->reserve_mutex);\n\t\tif (atomic_read(active_events) == 0)\n\t\t\terr = armpmu_reserve_hardware(armpmu);\n\n\t\tif (!err)\n\t\t\tatomic_inc(active_events);\n\t\tmutex_unlock(&armpmu->reserve_mutex);\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\terr = __hw_perf_event_init(event);\n\tif (err)\n\t\thw_perf_event_destroy(event);\n\n\treturn err;\n}\n\nstatic void armpmu_enable(struct pmu *pmu)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(pmu);\n\tstruct pmu_hw_events *hw_events = armpmu->get_hw_events();\n\tint enabled = bitmap_weight(hw_events->used_mask, armpmu->num_events);\n\n\tif (enabled)\n\t\tarmpmu->start(armpmu);\n}\n\nstatic void armpmu_disable(struct pmu *pmu)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(pmu);\n\tarmpmu->stop(armpmu);\n}\n\n#ifdef CONFIG_PM_RUNTIME\nstatic int armpmu_runtime_resume(struct device *dev)\n{\n\tstruct arm_pmu_platdata *plat = dev_get_platdata(dev);\n\n\tif (plat && plat->runtime_resume)\n\t\treturn plat->runtime_resume(dev);\n\n\treturn 0;\n}\n\nstatic int armpmu_runtime_suspend(struct device *dev)\n{\n\tstruct arm_pmu_platdata *plat = dev_get_platdata(dev);\n\n\tif (plat && plat->runtime_suspend)\n\t\treturn plat->runtime_suspend(dev);\n\n\treturn 0;\n}\n#endif\n\nconst struct dev_pm_ops armpmu_dev_pm_ops = {\n\tSET_RUNTIME_PM_OPS(armpmu_runtime_suspend, armpmu_runtime_resume, NULL)\n};\n\nstatic void armpmu_init(struct arm_pmu *armpmu)\n{\n\tatomic_set(&armpmu->active_events, 0);\n\tmutex_init(&armpmu->reserve_mutex);\n\n\tarmpmu->pmu = (struct pmu) {\n\t\t.pmu_enable\t= armpmu_enable,\n\t\t.pmu_disable\t= armpmu_disable,\n\t\t.event_init\t= armpmu_event_init,\n\t\t.add\t\t= armpmu_add,\n\t\t.del\t\t= armpmu_del,\n\t\t.start\t\t= armpmu_start,\n\t\t.stop\t\t= armpmu_stop,\n\t\t.read\t\t= armpmu_read,\n\t};\n}\n\nint armpmu_register(struct arm_pmu *armpmu, int type)\n{\n\tarmpmu_init(armpmu);\n\tpm_runtime_enable(&armpmu->plat_device->dev);\n\tpr_info(\"enabled with %s PMU driver, %d counters available\\n\",\n\t\t\tarmpmu->name, armpmu->num_events);\n\treturn perf_pmu_register(&armpmu->pmu, armpmu->name, type);\n}\n\n/*\n * Callchain handling code.\n */\n\n/*\n * The registers we're interested in are at the end of the variable\n * length saved register structure. The fp points at the end of this\n * structure so the address of this struct is:\n * (struct frame_tail *)(xxx->fp)-1\n *\n * This code has been adapted from the ARM OProfile support.\n */\nstruct frame_tail {\n\tstruct frame_tail __user *fp;\n\tunsigned long sp;\n\tunsigned long lr;\n} __attribute__((packed));\n\n/*\n * Get the return address for a single stackframe and return a pointer to the\n * next frame tail.\n */\nstatic struct frame_tail __user *\nuser_backtrace(struct frame_tail __user *tail,\n\t       struct perf_callchain_entry *entry)\n{\n\tstruct frame_tail buftail;\n\n\t/* Also check accessibility of one struct frame_tail beyond */\n\tif (!access_ok(VERIFY_READ, tail, sizeof(buftail)))\n\t\treturn NULL;\n\tif (__copy_from_user_inatomic(&buftail, tail, sizeof(buftail)))\n\t\treturn NULL;\n\n\tperf_callchain_store(entry, buftail.lr);\n\n\t/*\n\t * Frame pointers should strictly progress back up the stack\n\t * (towards higher addresses).\n\t */\n\tif (tail + 1 >= buftail.fp)\n\t\treturn NULL;\n\n\treturn buftail.fp - 1;\n}\n\nvoid\nperf_callchain_user(struct perf_callchain_entry *entry, struct pt_regs *regs)\n{\n\tstruct frame_tail __user *tail;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tperf_callchain_store(entry, regs->ARM_pc);\n\ttail = (struct frame_tail __user *)regs->ARM_fp - 1;\n\n\twhile ((entry->nr < PERF_MAX_STACK_DEPTH) &&\n\t       tail && !((unsigned long)tail & 0x3))\n\t\ttail = user_backtrace(tail, entry);\n}\n\n/*\n * Gets called by walk_stackframe() for every stackframe. This will be called\n * whist unwinding the stackframe and is like a subroutine return so we use\n * the PC.\n */\nstatic int\ncallchain_trace(struct stackframe *fr,\n\t\tvoid *data)\n{\n\tstruct perf_callchain_entry *entry = data;\n\tperf_callchain_store(entry, fr->pc);\n\treturn 0;\n}\n\nvoid\nperf_callchain_kernel(struct perf_callchain_entry *entry, struct pt_regs *regs)\n{\n\tstruct stackframe fr;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\t/* We don't support guest os callchain now */\n\t\treturn;\n\t}\n\n\tfr.fp = regs->ARM_fp;\n\tfr.sp = regs->ARM_sp;\n\tfr.lr = regs->ARM_lr;\n\tfr.pc = regs->ARM_pc;\n\twalk_stackframe(&fr, callchain_trace, entry);\n}\n\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest())\n\t\treturn perf_guest_cbs->get_guest_ip();\n\n\treturn instruction_pointer(regs);\n}\n\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tint misc = 0;\n\n\tif (perf_guest_cbs && perf_guest_cbs->is_in_guest()) {\n\t\tif (perf_guest_cbs->is_user_mode())\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_KERNEL;\n\t} else {\n\t\tif (user_mode(regs))\n\t\t\tmisc |= PERF_RECORD_MISC_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_KERNEL;\n\t}\n\n\treturn misc;\n}\n"], "filenames": ["arch/arm/kernel/perf_event.c"], "buggy_code_start_loc": [254], "buggy_code_end_loc": [254], "fixing_code_start_loc": [255], "fixing_code_end_loc": [258], "type": "CWE-20", "message": "The validate_event function in arch/arm/kernel/perf_event.c in the Linux kernel before 3.10.8 on the ARM platform allows local users to gain privileges or cause a denial of service (NULL pointer dereference and system crash) by adding a hardware event to an event group led by a software event.", "other": {"cve": {"id": "CVE-2013-4254", "sourceIdentifier": "secalert@redhat.com", "published": "2013-08-25T03:27:32.987", "lastModified": "2023-02-13T04:45:10.773", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The validate_event function in arch/arm/kernel/perf_event.c in the Linux kernel before 3.10.8 on the ARM platform allows local users to gain privileges or cause a denial of service (NULL pointer dereference and system crash) by adding a hardware event to an event group led by a software event."}, {"lang": "es", "value": "La funci\u00f3n validate_event en arch/arm/kernel/perf_event.c en Linux kernel anterior a v3.10.8 en plataformas ARM permite a usuarios locales conseguir privilegios o causar una denegaci\u00f3n de servicio (referencia a un puntero NULL y ca\u00edda del sistema) a\u00f1adiendo un evento de hardware para un grupo de eventos encabezada por un evento de software."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:arm64:*", "versionEndIncluding": "3.10.7", "matchCriteriaId": "FEDB8FE7-327F-45FB-8C4B-9D17B22CC444"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.0:*:*:*:*:*:arm64:*", "matchCriteriaId": "C8409226-20A1-4549-9E11-6D0C3C38DCCE"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.1:*:*:*:*:*:arm64:*", "matchCriteriaId": "77482843-364E-471F-A909-F373376FAEF5"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.2:*:*:*:*:*:arm64:*", "matchCriteriaId": "7E0221EF-13B8-42A2-8CEB-B95BDA2A2F5F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.3:*:*:*:*:*:arm64:*", "matchCriteriaId": "4F8A1100-F68D-4352-AB8D-B40AD97AE0EC"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.4:*:*:*:*:*:arm64:*", "matchCriteriaId": "991EF15A-B6DD-4D7F-87B5-144ED86642DF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.5:*:*:*:*:*:arm64:*", "matchCriteriaId": "DDA334EC-14D7-4220-ABBE-6D091A4EE374"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.10.6:*:*:*:*:*:arm64:*", "matchCriteriaId": "A76A48F9-591E-4884-B758-A8E438ECC9C1"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=c95eb3184ea1a3a2551df57190c81da695e2144b", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.10.8", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2013/08/16/6", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1968-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1969-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1970-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1971-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1972-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1973-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1974-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1975-1", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=998878", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/c95eb3184ea1a3a2551df57190c81da695e2144b", "source": "secalert@redhat.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/c95eb3184ea1a3a2551df57190c81da695e2144b"}}