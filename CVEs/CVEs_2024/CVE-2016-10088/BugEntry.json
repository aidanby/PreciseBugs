{"buggy_code": ["/*\n * bsg.c - block layer implementation of the sg v4 interface\n *\n * Copyright (C) 2004 Jens Axboe <axboe@suse.de> SUSE Labs\n * Copyright (C) 2004 Peter M. Jones <pjones@redhat.com>\n *\n *  This file is subject to the terms and conditions of the GNU General Public\n *  License version 2.  See the file \"COPYING\" in the main directory of this\n *  archive for more details.\n *\n */\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/file.h>\n#include <linux/blkdev.h>\n#include <linux/poll.h>\n#include <linux/cdev.h>\n#include <linux/jiffies.h>\n#include <linux/percpu.h>\n#include <linux/uio.h>\n#include <linux/idr.h>\n#include <linux/bsg.h>\n#include <linux/slab.h>\n\n#include <scsi/scsi.h>\n#include <scsi/scsi_ioctl.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_driver.h>\n#include <scsi/sg.h>\n\n#define BSG_DESCRIPTION\t\"Block layer SCSI generic (bsg) driver\"\n#define BSG_VERSION\t\"0.4\"\n\nstruct bsg_device {\n\tstruct request_queue *queue;\n\tspinlock_t lock;\n\tstruct list_head busy_list;\n\tstruct list_head done_list;\n\tstruct hlist_node dev_list;\n\tatomic_t ref_count;\n\tint queued_cmds;\n\tint done_cmds;\n\twait_queue_head_t wq_done;\n\twait_queue_head_t wq_free;\n\tchar name[20];\n\tint max_queue;\n\tunsigned long flags;\n};\n\nenum {\n\tBSG_F_BLOCK\t\t= 1,\n};\n\n#define BSG_DEFAULT_CMDS\t64\n#define BSG_MAX_DEVS\t\t32768\n\n#undef BSG_DEBUG\n\n#ifdef BSG_DEBUG\n#define dprintk(fmt, args...) printk(KERN_ERR \"%s: \" fmt, __func__, ##args)\n#else\n#define dprintk(fmt, args...)\n#endif\n\nstatic DEFINE_MUTEX(bsg_mutex);\nstatic DEFINE_IDR(bsg_minor_idr);\n\n#define BSG_LIST_ARRAY_SIZE\t8\nstatic struct hlist_head bsg_device_list[BSG_LIST_ARRAY_SIZE];\n\nstatic struct class *bsg_class;\nstatic int bsg_major;\n\nstatic struct kmem_cache *bsg_cmd_cachep;\n\n/*\n * our internal command type\n */\nstruct bsg_command {\n\tstruct bsg_device *bd;\n\tstruct list_head list;\n\tstruct request *rq;\n\tstruct bio *bio;\n\tstruct bio *bidi_bio;\n\tint err;\n\tstruct sg_io_v4 hdr;\n\tchar sense[SCSI_SENSE_BUFFERSIZE];\n};\n\nstatic void bsg_free_command(struct bsg_command *bc)\n{\n\tstruct bsg_device *bd = bc->bd;\n\tunsigned long flags;\n\n\tkmem_cache_free(bsg_cmd_cachep, bc);\n\n\tspin_lock_irqsave(&bd->lock, flags);\n\tbd->queued_cmds--;\n\tspin_unlock_irqrestore(&bd->lock, flags);\n\n\twake_up(&bd->wq_free);\n}\n\nstatic struct bsg_command *bsg_alloc_command(struct bsg_device *bd)\n{\n\tstruct bsg_command *bc = ERR_PTR(-EINVAL);\n\n\tspin_lock_irq(&bd->lock);\n\n\tif (bd->queued_cmds >= bd->max_queue)\n\t\tgoto out;\n\n\tbd->queued_cmds++;\n\tspin_unlock_irq(&bd->lock);\n\n\tbc = kmem_cache_zalloc(bsg_cmd_cachep, GFP_KERNEL);\n\tif (unlikely(!bc)) {\n\t\tspin_lock_irq(&bd->lock);\n\t\tbd->queued_cmds--;\n\t\tbc = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tbc->bd = bd;\n\tINIT_LIST_HEAD(&bc->list);\n\tdprintk(\"%s: returning free cmd %p\\n\", bd->name, bc);\n\treturn bc;\nout:\n\tspin_unlock_irq(&bd->lock);\n\treturn bc;\n}\n\nstatic inline struct hlist_head *bsg_dev_idx_hash(int index)\n{\n\treturn &bsg_device_list[index & (BSG_LIST_ARRAY_SIZE - 1)];\n}\n\nstatic int blk_fill_sgv4_hdr_rq(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct sg_io_v4 *hdr, struct bsg_device *bd,\n\t\t\t\tfmode_t has_write_perm)\n{\n\tif (hdr->request_len > BLK_MAX_CDB) {\n\t\trq->cmd = kzalloc(hdr->request_len, GFP_KERNEL);\n\t\tif (!rq->cmd)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (copy_from_user(rq->cmd, (void __user *)(unsigned long)hdr->request,\n\t\t\t   hdr->request_len))\n\t\treturn -EFAULT;\n\n\tif (hdr->subprotocol == BSG_SUB_PROTOCOL_SCSI_CMD) {\n\t\tif (blk_verify_command(rq->cmd, has_write_perm))\n\t\t\treturn -EPERM;\n\t} else if (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\n\t/*\n\t * fill in request structure\n\t */\n\trq->cmd_len = hdr->request_len;\n\n\trq->timeout = msecs_to_jiffies(hdr->timeout);\n\tif (!rq->timeout)\n\t\trq->timeout = q->sg_timeout;\n\tif (!rq->timeout)\n\t\trq->timeout = BLK_DEFAULT_SG_TIMEOUT;\n\tif (rq->timeout < BLK_MIN_SG_TIMEOUT)\n\t\trq->timeout = BLK_MIN_SG_TIMEOUT;\n\n\treturn 0;\n}\n\n/*\n * Check if sg_io_v4 from user is allowed and valid\n */\nstatic int\nbsg_validate_sgv4_hdr(struct sg_io_v4 *hdr, int *rw)\n{\n\tint ret = 0;\n\n\tif (hdr->guard != 'Q')\n\t\treturn -EINVAL;\n\n\tswitch (hdr->protocol) {\n\tcase BSG_PROTOCOL_SCSI:\n\t\tswitch (hdr->subprotocol) {\n\t\tcase BSG_SUB_PROTOCOL_SCSI_CMD:\n\t\tcase BSG_SUB_PROTOCOL_SCSI_TRANSPORT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\t*rw = hdr->dout_xfer_len ? WRITE : READ;\n\treturn ret;\n}\n\n/*\n * map sg_io_v4 to a request.\n */\nstatic struct request *\nbsg_map_hdr(struct bsg_device *bd, struct sg_io_v4 *hdr, fmode_t has_write_perm,\n\t    u8 *sense)\n{\n\tstruct request_queue *q = bd->queue;\n\tstruct request *rq, *next_rq = NULL;\n\tint ret, rw;\n\tunsigned int dxfer_len;\n\tvoid __user *dxferp = NULL;\n\tstruct bsg_class_device *bcd = &q->bsg_dev;\n\n\t/* if the LLD has been removed then the bsg_unregister_queue will\n\t * eventually be called and the class_dev was freed, so we can no\n\t * longer use this request_queue. Return no such address.\n\t */\n\tif (!bcd->class_dev)\n\t\treturn ERR_PTR(-ENXIO);\n\n\tdprintk(\"map hdr %llx/%u %llx/%u\\n\", (unsigned long long) hdr->dout_xferp,\n\t\thdr->dout_xfer_len, (unsigned long long) hdr->din_xferp,\n\t\thdr->din_xfer_len);\n\n\tret = bsg_validate_sgv4_hdr(hdr, &rw);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\t/*\n\t * map scatter-gather elements separately and string them to request\n\t */\n\trq = blk_get_request(q, rw, GFP_KERNEL);\n\tif (IS_ERR(rq))\n\t\treturn rq;\n\tblk_rq_set_block_pc(rq);\n\n\tret = blk_fill_sgv4_hdr_rq(q, rq, hdr, bd, has_write_perm);\n\tif (ret)\n\t\tgoto out;\n\n\tif (rw == WRITE && hdr->din_xfer_len) {\n\t\tif (!test_bit(QUEUE_FLAG_BIDI, &q->queue_flags)) {\n\t\t\tret = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnext_rq = blk_get_request(q, READ, GFP_KERNEL);\n\t\tif (IS_ERR(next_rq)) {\n\t\t\tret = PTR_ERR(next_rq);\n\t\t\tnext_rq = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\trq->next_rq = next_rq;\n\t\tnext_rq->cmd_type = rq->cmd_type;\n\n\t\tdxferp = (void __user *)(unsigned long)hdr->din_xferp;\n\t\tret =  blk_rq_map_user(q, next_rq, NULL, dxferp,\n\t\t\t\t       hdr->din_xfer_len, GFP_KERNEL);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (hdr->dout_xfer_len) {\n\t\tdxfer_len = hdr->dout_xfer_len;\n\t\tdxferp = (void __user *)(unsigned long)hdr->dout_xferp;\n\t} else if (hdr->din_xfer_len) {\n\t\tdxfer_len = hdr->din_xfer_len;\n\t\tdxferp = (void __user *)(unsigned long)hdr->din_xferp;\n\t} else\n\t\tdxfer_len = 0;\n\n\tif (dxfer_len) {\n\t\tret = blk_rq_map_user(q, rq, NULL, dxferp, dxfer_len,\n\t\t\t\t      GFP_KERNEL);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\trq->sense = sense;\n\trq->sense_len = 0;\n\n\treturn rq;\nout:\n\tif (rq->cmd != rq->__cmd)\n\t\tkfree(rq->cmd);\n\tblk_put_request(rq);\n\tif (next_rq) {\n\t\tblk_rq_unmap_user(next_rq->bio);\n\t\tblk_put_request(next_rq);\n\t}\n\treturn ERR_PTR(ret);\n}\n\n/*\n * async completion call-back from the block layer, when scsi/ide/whatever\n * calls end_that_request_last() on a request\n */\nstatic void bsg_rq_end_io(struct request *rq, int uptodate)\n{\n\tstruct bsg_command *bc = rq->end_io_data;\n\tstruct bsg_device *bd = bc->bd;\n\tunsigned long flags;\n\n\tdprintk(\"%s: finished rq %p bc %p, bio %p stat %d\\n\",\n\t\tbd->name, rq, bc, bc->bio, uptodate);\n\n\tbc->hdr.duration = jiffies_to_msecs(jiffies - bc->hdr.duration);\n\n\tspin_lock_irqsave(&bd->lock, flags);\n\tlist_move_tail(&bc->list, &bd->done_list);\n\tbd->done_cmds++;\n\tspin_unlock_irqrestore(&bd->lock, flags);\n\n\twake_up(&bd->wq_done);\n}\n\n/*\n * do final setup of a 'bc' and submit the matching 'rq' to the block\n * layer for io\n */\nstatic void bsg_add_command(struct bsg_device *bd, struct request_queue *q,\n\t\t\t    struct bsg_command *bc, struct request *rq)\n{\n\tint at_head = (0 == (bc->hdr.flags & BSG_FLAG_Q_AT_TAIL));\n\n\t/*\n\t * add bc command to busy queue and submit rq for io\n\t */\n\tbc->rq = rq;\n\tbc->bio = rq->bio;\n\tif (rq->next_rq)\n\t\tbc->bidi_bio = rq->next_rq->bio;\n\tbc->hdr.duration = jiffies;\n\tspin_lock_irq(&bd->lock);\n\tlist_add_tail(&bc->list, &bd->busy_list);\n\tspin_unlock_irq(&bd->lock);\n\n\tdprintk(\"%s: queueing rq %p, bc %p\\n\", bd->name, rq, bc);\n\n\trq->end_io_data = bc;\n\tblk_execute_rq_nowait(q, NULL, rq, at_head, bsg_rq_end_io);\n}\n\nstatic struct bsg_command *bsg_next_done_cmd(struct bsg_device *bd)\n{\n\tstruct bsg_command *bc = NULL;\n\n\tspin_lock_irq(&bd->lock);\n\tif (bd->done_cmds) {\n\t\tbc = list_first_entry(&bd->done_list, struct bsg_command, list);\n\t\tlist_del(&bc->list);\n\t\tbd->done_cmds--;\n\t}\n\tspin_unlock_irq(&bd->lock);\n\n\treturn bc;\n}\n\n/*\n * Get a finished command from the done list\n */\nstatic struct bsg_command *bsg_get_done_cmd(struct bsg_device *bd)\n{\n\tstruct bsg_command *bc;\n\tint ret;\n\n\tdo {\n\t\tbc = bsg_next_done_cmd(bd);\n\t\tif (bc)\n\t\t\tbreak;\n\n\t\tif (!test_bit(BSG_F_BLOCK, &bd->flags)) {\n\t\t\tbc = ERR_PTR(-EAGAIN);\n\t\t\tbreak;\n\t\t}\n\n\t\tret = wait_event_interruptible(bd->wq_done, bd->done_cmds);\n\t\tif (ret) {\n\t\t\tbc = ERR_PTR(-ERESTARTSYS);\n\t\t\tbreak;\n\t\t}\n\t} while (1);\n\n\tdprintk(\"%s: returning done %p\\n\", bd->name, bc);\n\n\treturn bc;\n}\n\nstatic int blk_complete_sgv4_hdr_rq(struct request *rq, struct sg_io_v4 *hdr,\n\t\t\t\t    struct bio *bio, struct bio *bidi_bio)\n{\n\tint ret = 0;\n\n\tdprintk(\"rq %p bio %p 0x%x\\n\", rq, bio, rq->errors);\n\t/*\n\t * fill in all the output members\n\t */\n\thdr->device_status = rq->errors & 0xff;\n\thdr->transport_status = host_byte(rq->errors);\n\thdr->driver_status = driver_byte(rq->errors);\n\thdr->info = 0;\n\tif (hdr->device_status || hdr->transport_status || hdr->driver_status)\n\t\thdr->info |= SG_INFO_CHECK;\n\thdr->response_len = 0;\n\n\tif (rq->sense_len && hdr->response) {\n\t\tint len = min_t(unsigned int, hdr->max_response_len,\n\t\t\t\t\trq->sense_len);\n\n\t\tret = copy_to_user((void __user *)(unsigned long)hdr->response,\n\t\t\t\t   rq->sense, len);\n\t\tif (!ret)\n\t\t\thdr->response_len = len;\n\t\telse\n\t\t\tret = -EFAULT;\n\t}\n\n\tif (rq->next_rq) {\n\t\thdr->dout_resid = rq->resid_len;\n\t\thdr->din_resid = rq->next_rq->resid_len;\n\t\tblk_rq_unmap_user(bidi_bio);\n\t\tblk_put_request(rq->next_rq);\n\t} else if (rq_data_dir(rq) == READ)\n\t\thdr->din_resid = rq->resid_len;\n\telse\n\t\thdr->dout_resid = rq->resid_len;\n\n\t/*\n\t * If the request generated a negative error number, return it\n\t * (providing we aren't already returning an error); if it's\n\t * just a protocol response (i.e. non negative), that gets\n\t * processed above.\n\t */\n\tif (!ret && rq->errors < 0)\n\t\tret = rq->errors;\n\n\tblk_rq_unmap_user(bio);\n\tif (rq->cmd != rq->__cmd)\n\t\tkfree(rq->cmd);\n\tblk_put_request(rq);\n\n\treturn ret;\n}\n\nstatic bool bsg_complete(struct bsg_device *bd)\n{\n\tbool ret = false;\n\tbool spin;\n\n\tdo {\n\t\tspin_lock_irq(&bd->lock);\n\n\t\tBUG_ON(bd->done_cmds > bd->queued_cmds);\n\n\t\t/*\n\t\t * All commands consumed.\n\t\t */\n\t\tif (bd->done_cmds == bd->queued_cmds)\n\t\t\tret = true;\n\n\t\tspin = !test_bit(BSG_F_BLOCK, &bd->flags);\n\n\t\tspin_unlock_irq(&bd->lock);\n\t} while (!ret && spin);\n\n\treturn ret;\n}\n\nstatic int bsg_complete_all_commands(struct bsg_device *bd)\n{\n\tstruct bsg_command *bc;\n\tint ret, tret;\n\n\tdprintk(\"%s: entered\\n\", bd->name);\n\n\t/*\n\t * wait for all commands to complete\n\t */\n\tio_wait_event(bd->wq_done, bsg_complete(bd));\n\n\t/*\n\t * discard done commands\n\t */\n\tret = 0;\n\tdo {\n\t\tspin_lock_irq(&bd->lock);\n\t\tif (!bd->queued_cmds) {\n\t\t\tspin_unlock_irq(&bd->lock);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock_irq(&bd->lock);\n\n\t\tbc = bsg_get_done_cmd(bd);\n\t\tif (IS_ERR(bc))\n\t\t\tbreak;\n\n\t\ttret = blk_complete_sgv4_hdr_rq(bc->rq, &bc->hdr, bc->bio,\n\t\t\t\t\t\tbc->bidi_bio);\n\t\tif (!ret)\n\t\t\tret = tret;\n\n\t\tbsg_free_command(bc);\n\t} while (1);\n\n\treturn ret;\n}\n\nstatic int\n__bsg_read(char __user *buf, size_t count, struct bsg_device *bd,\n\t   const struct iovec *iov, ssize_t *bytes_read)\n{\n\tstruct bsg_command *bc;\n\tint nr_commands, ret;\n\n\tif (count % sizeof(struct sg_io_v4))\n\t\treturn -EINVAL;\n\n\tret = 0;\n\tnr_commands = count / sizeof(struct sg_io_v4);\n\twhile (nr_commands) {\n\t\tbc = bsg_get_done_cmd(bd);\n\t\tif (IS_ERR(bc)) {\n\t\t\tret = PTR_ERR(bc);\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * this is the only case where we need to copy data back\n\t\t * after completing the request. so do that here,\n\t\t * bsg_complete_work() cannot do that for us\n\t\t */\n\t\tret = blk_complete_sgv4_hdr_rq(bc->rq, &bc->hdr, bc->bio,\n\t\t\t\t\t       bc->bidi_bio);\n\n\t\tif (copy_to_user(buf, &bc->hdr, sizeof(bc->hdr)))\n\t\t\tret = -EFAULT;\n\n\t\tbsg_free_command(bc);\n\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tbuf += sizeof(struct sg_io_v4);\n\t\t*bytes_read += sizeof(struct sg_io_v4);\n\t\tnr_commands--;\n\t}\n\n\treturn ret;\n}\n\nstatic inline void bsg_set_block(struct bsg_device *bd, struct file *file)\n{\n\tif (file->f_flags & O_NONBLOCK)\n\t\tclear_bit(BSG_F_BLOCK, &bd->flags);\n\telse\n\t\tset_bit(BSG_F_BLOCK, &bd->flags);\n}\n\n/*\n * Check if the error is a \"real\" error that we should return.\n */\nstatic inline int err_block_err(int ret)\n{\n\tif (ret && ret != -ENOSPC && ret != -ENODATA && ret != -EAGAIN)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic ssize_t\nbsg_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct bsg_device *bd = file->private_data;\n\tint ret;\n\tssize_t bytes_read;\n\n\tdprintk(\"%s: read %Zd bytes\\n\", bd->name, count);\n\n\tbsg_set_block(bd, file);\n\n\tbytes_read = 0;\n\tret = __bsg_read(buf, count, bd, NULL, &bytes_read);\n\t*ppos = bytes_read;\n\n\tif (!bytes_read || err_block_err(ret))\n\t\tbytes_read = ret;\n\n\treturn bytes_read;\n}\n\nstatic int __bsg_write(struct bsg_device *bd, const char __user *buf,\n\t\t       size_t count, ssize_t *bytes_written,\n\t\t       fmode_t has_write_perm)\n{\n\tstruct bsg_command *bc;\n\tstruct request *rq;\n\tint ret, nr_commands;\n\n\tif (count % sizeof(struct sg_io_v4))\n\t\treturn -EINVAL;\n\n\tnr_commands = count / sizeof(struct sg_io_v4);\n\trq = NULL;\n\tbc = NULL;\n\tret = 0;\n\twhile (nr_commands) {\n\t\tstruct request_queue *q = bd->queue;\n\n\t\tbc = bsg_alloc_command(bd);\n\t\tif (IS_ERR(bc)) {\n\t\t\tret = PTR_ERR(bc);\n\t\t\tbc = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_from_user(&bc->hdr, buf, sizeof(bc->hdr))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * get a request, fill in the blanks, and add to request queue\n\t\t */\n\t\trq = bsg_map_hdr(bd, &bc->hdr, has_write_perm, bc->sense);\n\t\tif (IS_ERR(rq)) {\n\t\t\tret = PTR_ERR(rq);\n\t\t\trq = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tbsg_add_command(bd, q, bc, rq);\n\t\tbc = NULL;\n\t\trq = NULL;\n\t\tnr_commands--;\n\t\tbuf += sizeof(struct sg_io_v4);\n\t\t*bytes_written += sizeof(struct sg_io_v4);\n\t}\n\n\tif (bc)\n\t\tbsg_free_command(bc);\n\n\treturn ret;\n}\n\nstatic ssize_t\nbsg_write(struct file *file, const char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct bsg_device *bd = file->private_data;\n\tssize_t bytes_written;\n\tint ret;\n\n\tdprintk(\"%s: write %Zd bytes\\n\", bd->name, count);\n\n\tbsg_set_block(bd, file);\n\n\tbytes_written = 0;\n\tret = __bsg_write(bd, buf, count, &bytes_written,\n\t\t\t  file->f_mode & FMODE_WRITE);\n\n\t*ppos = bytes_written;\n\n\t/*\n\t * return bytes written on non-fatal errors\n\t */\n\tif (!bytes_written || err_block_err(ret))\n\t\tbytes_written = ret;\n\n\tdprintk(\"%s: returning %Zd\\n\", bd->name, bytes_written);\n\treturn bytes_written;\n}\n\nstatic struct bsg_device *bsg_alloc_device(void)\n{\n\tstruct bsg_device *bd;\n\n\tbd = kzalloc(sizeof(struct bsg_device), GFP_KERNEL);\n\tif (unlikely(!bd))\n\t\treturn NULL;\n\n\tspin_lock_init(&bd->lock);\n\n\tbd->max_queue = BSG_DEFAULT_CMDS;\n\n\tINIT_LIST_HEAD(&bd->busy_list);\n\tINIT_LIST_HEAD(&bd->done_list);\n\tINIT_HLIST_NODE(&bd->dev_list);\n\n\tinit_waitqueue_head(&bd->wq_free);\n\tinit_waitqueue_head(&bd->wq_done);\n\treturn bd;\n}\n\nstatic void bsg_kref_release_function(struct kref *kref)\n{\n\tstruct bsg_class_device *bcd =\n\t\tcontainer_of(kref, struct bsg_class_device, ref);\n\tstruct device *parent = bcd->parent;\n\n\tif (bcd->release)\n\t\tbcd->release(bcd->parent);\n\n\tput_device(parent);\n}\n\nstatic int bsg_put_device(struct bsg_device *bd)\n{\n\tint ret = 0, do_free;\n\tstruct request_queue *q = bd->queue;\n\n\tmutex_lock(&bsg_mutex);\n\n\tdo_free = atomic_dec_and_test(&bd->ref_count);\n\tif (!do_free) {\n\t\tmutex_unlock(&bsg_mutex);\n\t\tgoto out;\n\t}\n\n\thlist_del(&bd->dev_list);\n\tmutex_unlock(&bsg_mutex);\n\n\tdprintk(\"%s: tearing down\\n\", bd->name);\n\n\t/*\n\t * close can always block\n\t */\n\tset_bit(BSG_F_BLOCK, &bd->flags);\n\n\t/*\n\t * correct error detection baddies here again. it's the responsibility\n\t * of the app to properly reap commands before close() if it wants\n\t * fool-proof error detection\n\t */\n\tret = bsg_complete_all_commands(bd);\n\n\tkfree(bd);\nout:\n\tkref_put(&q->bsg_dev.ref, bsg_kref_release_function);\n\tif (do_free)\n\t\tblk_put_queue(q);\n\treturn ret;\n}\n\nstatic struct bsg_device *bsg_add_device(struct inode *inode,\n\t\t\t\t\t struct request_queue *rq,\n\t\t\t\t\t struct file *file)\n{\n\tstruct bsg_device *bd;\n#ifdef BSG_DEBUG\n\tunsigned char buf[32];\n#endif\n\tif (!blk_get_queue(rq))\n\t\treturn ERR_PTR(-ENXIO);\n\n\tbd = bsg_alloc_device();\n\tif (!bd) {\n\t\tblk_put_queue(rq);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tbd->queue = rq;\n\n\tbsg_set_block(bd, file);\n\n\tatomic_set(&bd->ref_count, 1);\n\tmutex_lock(&bsg_mutex);\n\thlist_add_head(&bd->dev_list, bsg_dev_idx_hash(iminor(inode)));\n\n\tstrncpy(bd->name, dev_name(rq->bsg_dev.class_dev), sizeof(bd->name) - 1);\n\tdprintk(\"bound to <%s>, max queue %d\\n\",\n\t\tformat_dev_t(buf, inode->i_rdev), bd->max_queue);\n\n\tmutex_unlock(&bsg_mutex);\n\treturn bd;\n}\n\nstatic struct bsg_device *__bsg_get_device(int minor, struct request_queue *q)\n{\n\tstruct bsg_device *bd;\n\n\tmutex_lock(&bsg_mutex);\n\n\thlist_for_each_entry(bd, bsg_dev_idx_hash(minor), dev_list) {\n\t\tif (bd->queue == q) {\n\t\t\tatomic_inc(&bd->ref_count);\n\t\t\tgoto found;\n\t\t}\n\t}\n\tbd = NULL;\nfound:\n\tmutex_unlock(&bsg_mutex);\n\treturn bd;\n}\n\nstatic struct bsg_device *bsg_get_device(struct inode *inode, struct file *file)\n{\n\tstruct bsg_device *bd;\n\tstruct bsg_class_device *bcd;\n\n\t/*\n\t * find the class device\n\t */\n\tmutex_lock(&bsg_mutex);\n\tbcd = idr_find(&bsg_minor_idr, iminor(inode));\n\tif (bcd)\n\t\tkref_get(&bcd->ref);\n\tmutex_unlock(&bsg_mutex);\n\n\tif (!bcd)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tbd = __bsg_get_device(iminor(inode), bcd->queue);\n\tif (bd)\n\t\treturn bd;\n\n\tbd = bsg_add_device(inode, bcd->queue, file);\n\tif (IS_ERR(bd))\n\t\tkref_put(&bcd->ref, bsg_kref_release_function);\n\n\treturn bd;\n}\n\nstatic int bsg_open(struct inode *inode, struct file *file)\n{\n\tstruct bsg_device *bd;\n\n\tbd = bsg_get_device(inode, file);\n\n\tif (IS_ERR(bd))\n\t\treturn PTR_ERR(bd);\n\n\tfile->private_data = bd;\n\treturn 0;\n}\n\nstatic int bsg_release(struct inode *inode, struct file *file)\n{\n\tstruct bsg_device *bd = file->private_data;\n\n\tfile->private_data = NULL;\n\treturn bsg_put_device(bd);\n}\n\nstatic unsigned int bsg_poll(struct file *file, poll_table *wait)\n{\n\tstruct bsg_device *bd = file->private_data;\n\tunsigned int mask = 0;\n\n\tpoll_wait(file, &bd->wq_done, wait);\n\tpoll_wait(file, &bd->wq_free, wait);\n\n\tspin_lock_irq(&bd->lock);\n\tif (!list_empty(&bd->done_list))\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (bd->queued_cmds < bd->max_queue)\n\t\tmask |= POLLOUT;\n\tspin_unlock_irq(&bd->lock);\n\n\treturn mask;\n}\n\nstatic long bsg_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct bsg_device *bd = file->private_data;\n\tint __user *uarg = (int __user *) arg;\n\tint ret;\n\n\tswitch (cmd) {\n\t\t/*\n\t\t * our own ioctls\n\t\t */\n\tcase SG_GET_COMMAND_Q:\n\t\treturn put_user(bd->max_queue, uarg);\n\tcase SG_SET_COMMAND_Q: {\n\t\tint queue;\n\n\t\tif (get_user(queue, uarg))\n\t\t\treturn -EFAULT;\n\t\tif (queue < 1)\n\t\t\treturn -EINVAL;\n\n\t\tspin_lock_irq(&bd->lock);\n\t\tbd->max_queue = queue;\n\t\tspin_unlock_irq(&bd->lock);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * SCSI/sg ioctls\n\t */\n\tcase SG_GET_VERSION_NUM:\n\tcase SCSI_IOCTL_GET_IDLUN:\n\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\tcase SG_SET_TIMEOUT:\n\tcase SG_GET_TIMEOUT:\n\tcase SG_GET_RESERVED_SIZE:\n\tcase SG_SET_RESERVED_SIZE:\n\tcase SG_EMULATED_HOST:\n\tcase SCSI_IOCTL_SEND_COMMAND: {\n\t\tvoid __user *uarg = (void __user *) arg;\n\t\treturn scsi_cmd_ioctl(bd->queue, NULL, file->f_mode, cmd, uarg);\n\t}\n\tcase SG_IO: {\n\t\tstruct request *rq;\n\t\tstruct bio *bio, *bidi_bio = NULL;\n\t\tstruct sg_io_v4 hdr;\n\t\tint at_head;\n\t\tu8 sense[SCSI_SENSE_BUFFERSIZE];\n\n\t\tif (copy_from_user(&hdr, uarg, sizeof(hdr)))\n\t\t\treturn -EFAULT;\n\n\t\trq = bsg_map_hdr(bd, &hdr, file->f_mode & FMODE_WRITE, sense);\n\t\tif (IS_ERR(rq))\n\t\t\treturn PTR_ERR(rq);\n\n\t\tbio = rq->bio;\n\t\tif (rq->next_rq)\n\t\t\tbidi_bio = rq->next_rq->bio;\n\n\t\tat_head = (0 == (hdr.flags & BSG_FLAG_Q_AT_TAIL));\n\t\tblk_execute_rq(bd->queue, NULL, rq, at_head);\n\t\tret = blk_complete_sgv4_hdr_rq(rq, &hdr, bio, bidi_bio);\n\n\t\tif (copy_to_user(uarg, &hdr, sizeof(hdr)))\n\t\t\treturn -EFAULT;\n\n\t\treturn ret;\n\t}\n\t/*\n\t * block device ioctls\n\t */\n\tdefault:\n#if 0\n\t\treturn ioctl_by_bdev(bd->bdev, cmd, arg);\n#else\n\t\treturn -ENOTTY;\n#endif\n\t}\n}\n\nstatic const struct file_operations bsg_fops = {\n\t.read\t\t=\tbsg_read,\n\t.write\t\t=\tbsg_write,\n\t.poll\t\t=\tbsg_poll,\n\t.open\t\t=\tbsg_open,\n\t.release\t=\tbsg_release,\n\t.unlocked_ioctl\t=\tbsg_ioctl,\n\t.owner\t\t=\tTHIS_MODULE,\n\t.llseek\t\t=\tdefault_llseek,\n};\n\nvoid bsg_unregister_queue(struct request_queue *q)\n{\n\tstruct bsg_class_device *bcd = &q->bsg_dev;\n\n\tif (!bcd->class_dev)\n\t\treturn;\n\n\tmutex_lock(&bsg_mutex);\n\tidr_remove(&bsg_minor_idr, bcd->minor);\n\tif (q->kobj.sd)\n\t\tsysfs_remove_link(&q->kobj, \"bsg\");\n\tdevice_unregister(bcd->class_dev);\n\tbcd->class_dev = NULL;\n\tkref_put(&bcd->ref, bsg_kref_release_function);\n\tmutex_unlock(&bsg_mutex);\n}\nEXPORT_SYMBOL_GPL(bsg_unregister_queue);\n\nint bsg_register_queue(struct request_queue *q, struct device *parent,\n\t\t       const char *name, void (*release)(struct device *))\n{\n\tstruct bsg_class_device *bcd;\n\tdev_t dev;\n\tint ret;\n\tstruct device *class_dev = NULL;\n\tconst char *devname;\n\n\tif (name)\n\t\tdevname = name;\n\telse\n\t\tdevname = dev_name(parent);\n\n\t/*\n\t * we need a proper transport to send commands, not a stacked device\n\t */\n\tif (!queue_is_rq_based(q))\n\t\treturn 0;\n\n\tbcd = &q->bsg_dev;\n\tmemset(bcd, 0, sizeof(*bcd));\n\n\tmutex_lock(&bsg_mutex);\n\n\tret = idr_alloc(&bsg_minor_idr, bcd, 0, BSG_MAX_DEVS, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tif (ret == -ENOSPC) {\n\t\t\tprintk(KERN_ERR \"bsg: too many bsg devices\\n\");\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tgoto unlock;\n\t}\n\n\tbcd->minor = ret;\n\tbcd->queue = q;\n\tbcd->parent = get_device(parent);\n\tbcd->release = release;\n\tkref_init(&bcd->ref);\n\tdev = MKDEV(bsg_major, bcd->minor);\n\tclass_dev = device_create(bsg_class, parent, dev, NULL, \"%s\", devname);\n\tif (IS_ERR(class_dev)) {\n\t\tret = PTR_ERR(class_dev);\n\t\tgoto put_dev;\n\t}\n\tbcd->class_dev = class_dev;\n\n\tif (q->kobj.sd) {\n\t\tret = sysfs_create_link(&q->kobj, &bcd->class_dev->kobj, \"bsg\");\n\t\tif (ret)\n\t\t\tgoto unregister_class_dev;\n\t}\n\n\tmutex_unlock(&bsg_mutex);\n\treturn 0;\n\nunregister_class_dev:\n\tdevice_unregister(class_dev);\nput_dev:\n\tput_device(parent);\n\tidr_remove(&bsg_minor_idr, bcd->minor);\nunlock:\n\tmutex_unlock(&bsg_mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(bsg_register_queue);\n\nstatic struct cdev bsg_cdev;\n\nstatic char *bsg_devnode(struct device *dev, umode_t *mode)\n{\n\treturn kasprintf(GFP_KERNEL, \"bsg/%s\", dev_name(dev));\n}\n\nstatic int __init bsg_init(void)\n{\n\tint ret, i;\n\tdev_t devid;\n\n\tbsg_cmd_cachep = kmem_cache_create(\"bsg_cmd\",\n\t\t\t\tsizeof(struct bsg_command), 0, 0, NULL);\n\tif (!bsg_cmd_cachep) {\n\t\tprintk(KERN_ERR \"bsg: failed creating slab cache\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < BSG_LIST_ARRAY_SIZE; i++)\n\t\tINIT_HLIST_HEAD(&bsg_device_list[i]);\n\n\tbsg_class = class_create(THIS_MODULE, \"bsg\");\n\tif (IS_ERR(bsg_class)) {\n\t\tret = PTR_ERR(bsg_class);\n\t\tgoto destroy_kmemcache;\n\t}\n\tbsg_class->devnode = bsg_devnode;\n\n\tret = alloc_chrdev_region(&devid, 0, BSG_MAX_DEVS, \"bsg\");\n\tif (ret)\n\t\tgoto destroy_bsg_class;\n\n\tbsg_major = MAJOR(devid);\n\n\tcdev_init(&bsg_cdev, &bsg_fops);\n\tret = cdev_add(&bsg_cdev, MKDEV(bsg_major, 0), BSG_MAX_DEVS);\n\tif (ret)\n\t\tgoto unregister_chrdev;\n\n\tprintk(KERN_INFO BSG_DESCRIPTION \" version \" BSG_VERSION\n\t       \" loaded (major %d)\\n\", bsg_major);\n\treturn 0;\nunregister_chrdev:\n\tunregister_chrdev_region(MKDEV(bsg_major, 0), BSG_MAX_DEVS);\ndestroy_bsg_class:\n\tclass_destroy(bsg_class);\ndestroy_kmemcache:\n\tkmem_cache_destroy(bsg_cmd_cachep);\n\treturn ret;\n}\n\nMODULE_AUTHOR(\"Jens Axboe\");\nMODULE_DESCRIPTION(BSG_DESCRIPTION);\nMODULE_LICENSE(\"GPL\");\n\ndevice_initcall(bsg_init);\n", "/*\n *  History:\n *  Started: Aug 9 by Lawrence Foard (entropy@world.std.com),\n *           to allow user process control of SCSI devices.\n *  Development Sponsored by Killy Corp. NY NY\n *\n * Original driver (sg.c):\n *        Copyright (C) 1992 Lawrence Foard\n * Version 2 and 3 extensions to driver:\n *        Copyright (C) 1998 - 2014 Douglas Gilbert\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2, or (at your option)\n * any later version.\n *\n */\n\nstatic int sg_version_num = 30536;\t/* 2 digits for each component */\n#define SG_VERSION_STR \"3.5.36\"\n\n/*\n *  D. P. Gilbert (dgilbert@interlog.com), notes:\n *      - scsi logging is available via SCSI_LOG_TIMEOUT macros. First\n *        the kernel/module needs to be built with CONFIG_SCSI_LOGGING\n *        (otherwise the macros compile to empty statements).\n *\n */\n#include <linux/module.h>\n\n#include <linux/fs.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/errno.h>\n#include <linux/mtio.h>\n#include <linux/ioctl.h>\n#include <linux/slab.h>\n#include <linux/fcntl.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/moduleparam.h>\n#include <linux/cdev.h>\n#include <linux/idr.h>\n#include <linux/seq_file.h>\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n#include <linux/blktrace_api.h>\n#include <linux/mutex.h>\n#include <linux/atomic.h>\n#include <linux/ratelimit.h>\n#include <linux/uio.h>\n\n#include \"scsi.h\"\n#include <scsi/scsi_dbg.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_driver.h>\n#include <scsi/scsi_ioctl.h>\n#include <scsi/sg.h>\n\n#include \"scsi_logging.h\"\n\n#ifdef CONFIG_SCSI_PROC_FS\n#include <linux/proc_fs.h>\nstatic char *sg_version_date = \"20140603\";\n\nstatic int sg_proc_init(void);\nstatic void sg_proc_cleanup(void);\n#endif\n\n#define SG_ALLOW_DIO_DEF 0\n\n#define SG_MAX_DEVS 32768\n\n/* SG_MAX_CDB_SIZE should be 260 (spc4r37 section 3.1.30) however the type\n * of sg_io_hdr::cmd_len can only represent 255. All SCSI commands greater\n * than 16 bytes are \"variable length\" whose length is a multiple of 4\n */\n#define SG_MAX_CDB_SIZE 252\n\n#define SG_DEFAULT_TIMEOUT mult_frac(SG_DEFAULT_TIMEOUT_USER, HZ, USER_HZ)\n\nint sg_big_buff = SG_DEF_RESERVED_SIZE;\n/* N.B. This variable is readable and writeable via\n   /proc/scsi/sg/def_reserved_size . Each time sg_open() is called a buffer\n   of this size (or less if there is not enough memory) will be reserved\n   for use by this file descriptor. [Deprecated usage: this variable is also\n   readable via /proc/sys/kernel/sg-big-buff if the sg driver is built into\n   the kernel (i.e. it is not a module).] */\nstatic int def_reserved_size = -1;\t/* picks up init parameter */\nstatic int sg_allow_dio = SG_ALLOW_DIO_DEF;\n\nstatic int scatter_elem_sz = SG_SCATTER_SZ;\nstatic int scatter_elem_sz_prev = SG_SCATTER_SZ;\n\n#define SG_SECTOR_SZ 512\n\nstatic int sg_add_device(struct device *, struct class_interface *);\nstatic void sg_remove_device(struct device *, struct class_interface *);\n\nstatic DEFINE_IDR(sg_index_idr);\nstatic DEFINE_RWLOCK(sg_index_lock);\t/* Also used to lock\n\t\t\t\t\t\t\t   file descriptor list for device */\n\nstatic struct class_interface sg_interface = {\n\t.add_dev        = sg_add_device,\n\t.remove_dev     = sg_remove_device,\n};\n\ntypedef struct sg_scatter_hold { /* holding area for scsi scatter gather info */\n\tunsigned short k_use_sg; /* Count of kernel scatter-gather pieces */\n\tunsigned sglist_len; /* size of malloc'd scatter-gather list ++ */\n\tunsigned bufflen;\t/* Size of (aggregate) data buffer */\n\tstruct page **pages;\n\tint page_order;\n\tchar dio_in_use;\t/* 0->indirect IO (or mmap), 1->dio */\n\tunsigned char cmd_opcode; /* first byte of command */\n} Sg_scatter_hold;\n\nstruct sg_device;\t\t/* forward declarations */\nstruct sg_fd;\n\ntypedef struct sg_request {\t/* SG_MAX_QUEUE requests outstanding per file */\n\tstruct sg_request *nextrp;\t/* NULL -> tail request (slist) */\n\tstruct sg_fd *parentfp;\t/* NULL -> not in use */\n\tSg_scatter_hold data;\t/* hold buffer, perhaps scatter list */\n\tsg_io_hdr_t header;\t/* scsi command+info, see <scsi/sg.h> */\n\tunsigned char sense_b[SCSI_SENSE_BUFFERSIZE];\n\tchar res_used;\t\t/* 1 -> using reserve buffer, 0 -> not ... */\n\tchar orphan;\t\t/* 1 -> drop on sight, 0 -> normal */\n\tchar sg_io_owned;\t/* 1 -> packet belongs to SG_IO */\n\t/* done protected by rq_list_lock */\n\tchar done;\t\t/* 0->before bh, 1->before read, 2->read */\n\tstruct request *rq;\n\tstruct bio *bio;\n\tstruct execute_work ew;\n} Sg_request;\n\ntypedef struct sg_fd {\t\t/* holds the state of a file descriptor */\n\tstruct list_head sfd_siblings;  /* protected by device's sfd_lock */\n\tstruct sg_device *parentdp;\t/* owning device */\n\twait_queue_head_t read_wait;\t/* queue read until command done */\n\trwlock_t rq_list_lock;\t/* protect access to list in req_arr */\n\tint timeout;\t\t/* defaults to SG_DEFAULT_TIMEOUT      */\n\tint timeout_user;\t/* defaults to SG_DEFAULT_TIMEOUT_USER */\n\tSg_scatter_hold reserve;\t/* buffer held for this file descriptor */\n\tunsigned save_scat_len;\t/* original length of trunc. scat. element */\n\tSg_request *headrp;\t/* head of request slist, NULL->empty */\n\tstruct fasync_struct *async_qp;\t/* used by asynchronous notification */\n\tSg_request req_arr[SG_MAX_QUEUE];\t/* used as singly-linked list */\n\tchar low_dma;\t\t/* as in parent but possibly overridden to 1 */\n\tchar force_packid;\t/* 1 -> pack_id input to read(), 0 -> ignored */\n\tchar cmd_q;\t\t/* 1 -> allow command queuing, 0 -> don't */\n\tunsigned char next_cmd_len; /* 0: automatic, >0: use on next write() */\n\tchar keep_orphan;\t/* 0 -> drop orphan (def), 1 -> keep for read() */\n\tchar mmap_called;\t/* 0 -> mmap() never called on this fd */\n\tstruct kref f_ref;\n\tstruct execute_work ew;\n} Sg_fd;\n\ntypedef struct sg_device { /* holds the state of each scsi generic device */\n\tstruct scsi_device *device;\n\twait_queue_head_t open_wait;    /* queue open() when O_EXCL present */\n\tstruct mutex open_rel_lock;     /* held when in open() or release() */\n\tint sg_tablesize;\t/* adapter's max scatter-gather table size */\n\tu32 index;\t\t/* device index number */\n\tstruct list_head sfds;\n\trwlock_t sfd_lock;      /* protect access to sfd list */\n\tatomic_t detaching;     /* 0->device usable, 1->device detaching */\n\tbool exclude;\t\t/* 1->open(O_EXCL) succeeded and is active */\n\tint open_cnt;\t\t/* count of opens (perhaps < num(sfds) ) */\n\tchar sgdebug;\t\t/* 0->off, 1->sense, 9->dump dev, 10-> all devs */\n\tstruct gendisk *disk;\n\tstruct cdev * cdev;\t/* char_dev [sysfs: /sys/cdev/major/sg<n>] */\n\tstruct kref d_ref;\n} Sg_device;\n\n/* tasklet or soft irq callback */\nstatic void sg_rq_end_io(struct request *rq, int uptodate);\nstatic int sg_start_req(Sg_request *srp, unsigned char *cmd);\nstatic int sg_finish_rem_req(Sg_request * srp);\nstatic int sg_build_indirect(Sg_scatter_hold * schp, Sg_fd * sfp, int buff_size);\nstatic ssize_t sg_new_read(Sg_fd * sfp, char __user *buf, size_t count,\n\t\t\t   Sg_request * srp);\nstatic ssize_t sg_new_write(Sg_fd *sfp, struct file *file,\n\t\t\tconst char __user *buf, size_t count, int blocking,\n\t\t\tint read_only, int sg_io_owned, Sg_request **o_srp);\nstatic int sg_common_write(Sg_fd * sfp, Sg_request * srp,\n\t\t\t   unsigned char *cmnd, int timeout, int blocking);\nstatic int sg_read_oxfer(Sg_request * srp, char __user *outp, int num_read_xfer);\nstatic void sg_remove_scat(Sg_fd * sfp, Sg_scatter_hold * schp);\nstatic void sg_build_reserve(Sg_fd * sfp, int req_size);\nstatic void sg_link_reserve(Sg_fd * sfp, Sg_request * srp, int size);\nstatic void sg_unlink_reserve(Sg_fd * sfp, Sg_request * srp);\nstatic Sg_fd *sg_add_sfp(Sg_device * sdp);\nstatic void sg_remove_sfp(struct kref *);\nstatic Sg_request *sg_get_rq_mark(Sg_fd * sfp, int pack_id);\nstatic Sg_request *sg_add_request(Sg_fd * sfp);\nstatic int sg_remove_request(Sg_fd * sfp, Sg_request * srp);\nstatic int sg_res_in_use(Sg_fd * sfp);\nstatic Sg_device *sg_get_dev(int dev);\nstatic void sg_device_destroy(struct kref *kref);\n\n#define SZ_SG_HEADER sizeof(struct sg_header)\n#define SZ_SG_IO_HDR sizeof(sg_io_hdr_t)\n#define SZ_SG_IOVEC sizeof(sg_iovec_t)\n#define SZ_SG_REQ_INFO sizeof(sg_req_info_t)\n\n#define sg_printk(prefix, sdp, fmt, a...) \\\n\tsdev_prefix_printk(prefix, (sdp)->device,\t\t\\\n\t\t\t   (sdp)->disk->disk_name, fmt, ##a)\n\nstatic int sg_allow_access(struct file *filp, unsigned char *cmd)\n{\n\tstruct sg_fd *sfp = filp->private_data;\n\n\tif (sfp->parentdp->device->type == TYPE_SCANNER)\n\t\treturn 0;\n\n\treturn blk_verify_command(cmd, filp->f_mode & FMODE_WRITE);\n}\n\nstatic int\nopen_wait(Sg_device *sdp, int flags)\n{\n\tint retval = 0;\n\n\tif (flags & O_EXCL) {\n\t\twhile (sdp->open_cnt > 0) {\n\t\t\tmutex_unlock(&sdp->open_rel_lock);\n\t\t\tretval = wait_event_interruptible(sdp->open_wait,\n\t\t\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t\t\t !sdp->open_cnt));\n\t\t\tmutex_lock(&sdp->open_rel_lock);\n\n\t\t\tif (retval) /* -ERESTARTSYS */\n\t\t\t\treturn retval;\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t}\n\t} else {\n\t\twhile (sdp->exclude) {\n\t\t\tmutex_unlock(&sdp->open_rel_lock);\n\t\t\tretval = wait_event_interruptible(sdp->open_wait,\n\t\t\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t\t\t !sdp->exclude));\n\t\t\tmutex_lock(&sdp->open_rel_lock);\n\n\t\t\tif (retval) /* -ERESTARTSYS */\n\t\t\t\treturn retval;\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t}\n\t}\n\n\treturn retval;\n}\n\n/* Returns 0 on success, else a negated errno value */\nstatic int\nsg_open(struct inode *inode, struct file *filp)\n{\n\tint dev = iminor(inode);\n\tint flags = filp->f_flags;\n\tstruct request_queue *q;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tint retval;\n\n\tnonseekable_open(inode, filp);\n\tif ((flags & O_EXCL) && (O_RDONLY == (flags & O_ACCMODE)))\n\t\treturn -EPERM; /* Can't lock it with read only access */\n\tsdp = sg_get_dev(dev);\n\tif (IS_ERR(sdp))\n\t\treturn PTR_ERR(sdp);\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_open: flags=0x%x\\n\", flags));\n\n\t/* This driver's module count bumped by fops_get in <linux/fs.h> */\n\t/* Prevent the device driver from vanishing while we sleep */\n\tretval = scsi_device_get(sdp->device);\n\tif (retval)\n\t\tgoto sg_put;\n\n\tretval = scsi_autopm_get_device(sdp->device);\n\tif (retval)\n\t\tgoto sdp_put;\n\n\t/* scsi_block_when_processing_errors() may block so bypass\n\t * check if O_NONBLOCK. Permits SCSI commands to be issued\n\t * during error recovery. Tread carefully. */\n\tif (!((flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device))) {\n\t\tretval = -ENXIO;\n\t\t/* we are in error recovery for this device */\n\t\tgoto error_out;\n\t}\n\n\tmutex_lock(&sdp->open_rel_lock);\n\tif (flags & O_NONBLOCK) {\n\t\tif (flags & O_EXCL) {\n\t\t\tif (sdp->open_cnt > 0) {\n\t\t\t\tretval = -EBUSY;\n\t\t\t\tgoto error_mutex_locked;\n\t\t\t}\n\t\t} else {\n\t\t\tif (sdp->exclude) {\n\t\t\t\tretval = -EBUSY;\n\t\t\t\tgoto error_mutex_locked;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tretval = open_wait(sdp, flags);\n\t\tif (retval) /* -ERESTARTSYS or -ENODEV */\n\t\t\tgoto error_mutex_locked;\n\t}\n\n\t/* N.B. at this point we are holding the open_rel_lock */\n\tif (flags & O_EXCL)\n\t\tsdp->exclude = true;\n\n\tif (sdp->open_cnt < 1) {  /* no existing opens */\n\t\tsdp->sgdebug = 0;\n\t\tq = sdp->device->request_queue;\n\t\tsdp->sg_tablesize = queue_max_segments(q);\n\t}\n\tsfp = sg_add_sfp(sdp);\n\tif (IS_ERR(sfp)) {\n\t\tretval = PTR_ERR(sfp);\n\t\tgoto out_undo;\n\t}\n\n\tfilp->private_data = sfp;\n\tsdp->open_cnt++;\n\tmutex_unlock(&sdp->open_rel_lock);\n\n\tretval = 0;\nsg_put:\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n\treturn retval;\n\nout_undo:\n\tif (flags & O_EXCL) {\n\t\tsdp->exclude = false;   /* undo if error */\n\t\twake_up_interruptible(&sdp->open_wait);\n\t}\nerror_mutex_locked:\n\tmutex_unlock(&sdp->open_rel_lock);\nerror_out:\n\tscsi_autopm_put_device(sdp->device);\nsdp_put:\n\tscsi_device_put(sdp->device);\n\tgoto sg_put;\n}\n\n/* Release resources associated with a successful sg_open()\n * Returns 0 on success, else a negated errno value */\nstatic int\nsg_release(struct inode *inode, struct file *filp)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp, \"sg_release\\n\"));\n\n\tmutex_lock(&sdp->open_rel_lock);\n\tscsi_autopm_put_device(sdp->device);\n\tkref_put(&sfp->f_ref, sg_remove_sfp);\n\tsdp->open_cnt--;\n\n\t/* possibly many open()s waiting on exlude clearing, start many;\n\t * only open(O_EXCL)s wait on 0==open_cnt so only start one */\n\tif (sdp->exclude) {\n\t\tsdp->exclude = false;\n\t\twake_up_interruptible_all(&sdp->open_wait);\n\t} else if (0 == sdp->open_cnt) {\n\t\twake_up_interruptible(&sdp->open_wait);\n\t}\n\tmutex_unlock(&sdp->open_rel_lock);\n\treturn 0;\n}\n\nstatic ssize_t\nsg_read(struct file *filp, char __user *buf, size_t count, loff_t * ppos)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tint req_pack_id = -1;\n\tsg_io_hdr_t *hp;\n\tstruct sg_header *old_hdr = NULL;\n\tint retval = 0;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_read: count=%d\\n\", (int) count));\n\n\tif (!access_ok(VERIFY_WRITE, buf, count))\n\t\treturn -EFAULT;\n\tif (sfp->force_packid && (count >= SZ_SG_HEADER)) {\n\t\told_hdr = kmalloc(SZ_SG_HEADER, GFP_KERNEL);\n\t\tif (!old_hdr)\n\t\t\treturn -ENOMEM;\n\t\tif (__copy_from_user(old_hdr, buf, SZ_SG_HEADER)) {\n\t\t\tretval = -EFAULT;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (old_hdr->reply_len < 0) {\n\t\t\tif (count >= SZ_SG_IO_HDR) {\n\t\t\t\tsg_io_hdr_t *new_hdr;\n\t\t\t\tnew_hdr = kmalloc(SZ_SG_IO_HDR, GFP_KERNEL);\n\t\t\t\tif (!new_hdr) {\n\t\t\t\t\tretval = -ENOMEM;\n\t\t\t\t\tgoto free_old_hdr;\n\t\t\t\t}\n\t\t\t\tretval =__copy_from_user\n\t\t\t\t    (new_hdr, buf, SZ_SG_IO_HDR);\n\t\t\t\treq_pack_id = new_hdr->pack_id;\n\t\t\t\tkfree(new_hdr);\n\t\t\t\tif (retval) {\n\t\t\t\t\tretval = -EFAULT;\n\t\t\t\t\tgoto free_old_hdr;\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\treq_pack_id = old_hdr->pack_id;\n\t}\n\tsrp = sg_get_rq_mark(sfp, req_pack_id);\n\tif (!srp) {\t\t/* now wait on packet to arrive */\n\t\tif (atomic_read(&sdp->detaching)) {\n\t\t\tretval = -ENODEV;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\tretval = -EAGAIN;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tretval = wait_event_interruptible(sfp->read_wait,\n\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t(srp = sg_get_rq_mark(sfp, req_pack_id))));\n\t\tif (atomic_read(&sdp->detaching)) {\n\t\t\tretval = -ENODEV;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (retval) {\n\t\t\t/* -ERESTARTSYS as signal hit process */\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t}\n\tif (srp->header.interface_id != '\\0') {\n\t\tretval = sg_new_read(sfp, buf, count, srp);\n\t\tgoto free_old_hdr;\n\t}\n\n\thp = &srp->header;\n\tif (old_hdr == NULL) {\n\t\told_hdr = kmalloc(SZ_SG_HEADER, GFP_KERNEL);\n\t\tif (! old_hdr) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t}\n\tmemset(old_hdr, 0, SZ_SG_HEADER);\n\told_hdr->reply_len = (int) hp->timeout;\n\told_hdr->pack_len = old_hdr->reply_len; /* old, strange behaviour */\n\told_hdr->pack_id = hp->pack_id;\n\told_hdr->twelve_byte =\n\t    ((srp->data.cmd_opcode >= 0xc0) && (12 == hp->cmd_len)) ? 1 : 0;\n\told_hdr->target_status = hp->masked_status;\n\told_hdr->host_status = hp->host_status;\n\told_hdr->driver_status = hp->driver_status;\n\tif ((CHECK_CONDITION & hp->masked_status) ||\n\t    (DRIVER_SENSE & hp->driver_status))\n\t\tmemcpy(old_hdr->sense_buffer, srp->sense_b,\n\t\t       sizeof (old_hdr->sense_buffer));\n\tswitch (hp->host_status) {\n\t/* This setup of 'result' is for backward compatibility and is best\n\t   ignored by the user who should use target, host + driver status */\n\tcase DID_OK:\n\tcase DID_PASSTHROUGH:\n\tcase DID_SOFT_ERROR:\n\t\told_hdr->result = 0;\n\t\tbreak;\n\tcase DID_NO_CONNECT:\n\tcase DID_BUS_BUSY:\n\tcase DID_TIME_OUT:\n\t\told_hdr->result = EBUSY;\n\t\tbreak;\n\tcase DID_BAD_TARGET:\n\tcase DID_ABORT:\n\tcase DID_PARITY:\n\tcase DID_RESET:\n\tcase DID_BAD_INTR:\n\t\told_hdr->result = EIO;\n\t\tbreak;\n\tcase DID_ERROR:\n\t\told_hdr->result = (srp->sense_b[0] == 0 && \n\t\t\t\t  hp->masked_status == GOOD) ? 0 : EIO;\n\t\tbreak;\n\tdefault:\n\t\told_hdr->result = EIO;\n\t\tbreak;\n\t}\n\n\t/* Now copy the result back to the user buffer.  */\n\tif (count >= SZ_SG_HEADER) {\n\t\tif (__copy_to_user(buf, old_hdr, SZ_SG_HEADER)) {\n\t\t\tretval = -EFAULT;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tbuf += SZ_SG_HEADER;\n\t\tif (count > old_hdr->reply_len)\n\t\t\tcount = old_hdr->reply_len;\n\t\tif (count > SZ_SG_HEADER) {\n\t\t\tif (sg_read_oxfer(srp, buf, count - SZ_SG_HEADER)) {\n\t\t\t\tretval = -EFAULT;\n\t\t\t\tgoto free_old_hdr;\n\t\t\t}\n\t\t}\n\t} else\n\t\tcount = (old_hdr->result == 0) ? 0 : -EIO;\n\tsg_finish_rem_req(srp);\n\tretval = count;\nfree_old_hdr:\n\tkfree(old_hdr);\n\treturn retval;\n}\n\nstatic ssize_t\nsg_new_read(Sg_fd * sfp, char __user *buf, size_t count, Sg_request * srp)\n{\n\tsg_io_hdr_t *hp = &srp->header;\n\tint err = 0, err2;\n\tint len;\n\n\tif (count < SZ_SG_IO_HDR) {\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\thp->sb_len_wr = 0;\n\tif ((hp->mx_sb_len > 0) && hp->sbp) {\n\t\tif ((CHECK_CONDITION & hp->masked_status) ||\n\t\t    (DRIVER_SENSE & hp->driver_status)) {\n\t\t\tint sb_len = SCSI_SENSE_BUFFERSIZE;\n\t\t\tsb_len = (hp->mx_sb_len > sb_len) ? sb_len : hp->mx_sb_len;\n\t\t\tlen = 8 + (int) srp->sense_b[7];\t/* Additional sense length field */\n\t\t\tlen = (len > sb_len) ? sb_len : len;\n\t\t\tif (copy_to_user(hp->sbp, srp->sense_b, len)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t\thp->sb_len_wr = len;\n\t\t}\n\t}\n\tif (hp->masked_status || hp->host_status || hp->driver_status)\n\t\thp->info |= SG_INFO_CHECK;\n\tif (copy_to_user(buf, hp, SZ_SG_IO_HDR)) {\n\t\terr = -EFAULT;\n\t\tgoto err_out;\n\t}\nerr_out:\n\terr2 = sg_finish_rem_req(srp);\n\treturn err ? : err2 ? : count;\n}\n\nstatic ssize_t\nsg_write(struct file *filp, const char __user *buf, size_t count, loff_t * ppos)\n{\n\tint mxsize, cmd_size, k;\n\tint input_size, blocking;\n\tunsigned char opcode;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tstruct sg_header old_hdr;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_write: count=%d\\n\", (int) count));\n\tif (atomic_read(&sdp->detaching))\n\t\treturn -ENODEV;\n\tif (!((filp->f_flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device)))\n\t\treturn -ENXIO;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\tif (count < SZ_SG_HEADER)\n\t\treturn -EIO;\n\tif (__copy_from_user(&old_hdr, buf, SZ_SG_HEADER))\n\t\treturn -EFAULT;\n\tblocking = !(filp->f_flags & O_NONBLOCK);\n\tif (old_hdr.reply_len < 0)\n\t\treturn sg_new_write(sfp, filp, buf, count,\n\t\t\t\t    blocking, 0, 0, NULL);\n\tif (count < (SZ_SG_HEADER + 6))\n\t\treturn -EIO;\t/* The minimum scsi command length is 6 bytes. */\n\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\t      \"sg_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tbuf += SZ_SG_HEADER;\n\t__get_user(opcode, buf);\n\tif (sfp->next_cmd_len > 0) {\n\t\tcmd_size = sfp->next_cmd_len;\n\t\tsfp->next_cmd_len = 0;\t/* reset so only this write() effected */\n\t} else {\n\t\tcmd_size = COMMAND_SIZE(opcode);\t/* based on SCSI command group */\n\t\tif ((opcode >= 0xc0) && old_hdr.twelve_byte)\n\t\t\tcmd_size = 12;\n\t}\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\"sg_write:   scsi opcode=0x%02x, cmd_size=%d\\n\", (int) opcode, cmd_size));\n/* Determine buffer size.  */\n\tinput_size = count - cmd_size;\n\tmxsize = (input_size > old_hdr.reply_len) ? input_size : old_hdr.reply_len;\n\tmxsize -= SZ_SG_HEADER;\n\tinput_size -= SZ_SG_HEADER;\n\tif (input_size < 0) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EIO;\t/* User did not pass enough bytes for this command. */\n\t}\n\thp = &srp->header;\n\thp->interface_id = '\\0';\t/* indicator of old interface tunnelled */\n\thp->cmd_len = (unsigned char) cmd_size;\n\thp->iovec_count = 0;\n\thp->mx_sb_len = 0;\n\tif (input_size > 0)\n\t\thp->dxfer_direction = (old_hdr.reply_len > SZ_SG_HEADER) ?\n\t\t    SG_DXFER_TO_FROM_DEV : SG_DXFER_TO_DEV;\n\telse\n\t\thp->dxfer_direction = (mxsize > 0) ? SG_DXFER_FROM_DEV : SG_DXFER_NONE;\n\thp->dxfer_len = mxsize;\n\tif ((hp->dxfer_direction == SG_DXFER_TO_DEV) ||\n\t    (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV))\n\t\thp->dxferp = (char __user *)buf + cmd_size;\n\telse\n\t\thp->dxferp = NULL;\n\thp->sbp = NULL;\n\thp->timeout = old_hdr.reply_len;\t/* structure abuse ... */\n\thp->flags = input_size;\t/* structure abuse ... */\n\thp->pack_id = old_hdr.pack_id;\n\thp->usr_ptr = NULL;\n\tif (__copy_from_user(cmnd, buf, cmd_size))\n\t\treturn -EFAULT;\n\t/*\n\t * SG_DXFER_TO_FROM_DEV is functionally equivalent to SG_DXFER_FROM_DEV,\n\t * but is is possible that the app intended SG_DXFER_TO_DEV, because there\n\t * is a non-zero input_size, so emit a warning.\n\t */\n\tif (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV) {\n\t\tstatic char cmd[TASK_COMM_LEN];\n\t\tif (strcmp(current->comm, cmd)) {\n\t\t\tprintk_ratelimited(KERN_WARNING\n\t\t\t\t\t   \"sg_write: data in/out %d/%d bytes \"\n\t\t\t\t\t   \"for SCSI command 0x%x-- guessing \"\n\t\t\t\t\t   \"data in;\\n   program %s not setting \"\n\t\t\t\t\t   \"count and/or reply_len properly\\n\",\n\t\t\t\t\t   old_hdr.reply_len - (int)SZ_SG_HEADER,\n\t\t\t\t\t   input_size, (unsigned int) cmnd[0],\n\t\t\t\t\t   current->comm);\n\t\t\tstrcpy(cmd, current->comm);\n\t\t}\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, sfp->timeout, blocking);\n\treturn (k < 0) ? k : count;\n}\n\nstatic ssize_t\nsg_new_write(Sg_fd *sfp, struct file *file, const char __user *buf,\n\t\t size_t count, int blocking, int read_only, int sg_io_owned,\n\t\t Sg_request **o_srp)\n{\n\tint k;\n\tSg_request *srp;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\tint timeout;\n\tunsigned long ul_timeout;\n\n\tif (count < SZ_SG_IO_HDR)\n\t\treturn -EINVAL;\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT; /* protects following copy_from_user()s + get_user()s */\n\n\tsfp->cmd_q = 1;\t/* when sg_io_hdr seen, set command queuing on */\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t\t      \"sg_new_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tsrp->sg_io_owned = sg_io_owned;\n\thp = &srp->header;\n\tif (__copy_from_user(hp, buf, SZ_SG_IO_HDR)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\n\t}\n\tif (hp->interface_id != 'S') {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -ENOSYS;\n\t}\n\tif (hp->flags & SG_FLAG_MMAP_IO) {\n\t\tif (hp->dxfer_len > sfp->reserve.bufflen) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -ENOMEM;\t/* MMAP_IO size must fit in reserve buffer */\n\t\t}\n\t\tif (hp->flags & SG_FLAG_DIRECT_IO) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -EINVAL;\t/* either MMAP_IO or DIRECT_IO (not both) */\n\t\t}\n\t\tif (sg_res_in_use(sfp)) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -EBUSY;\t/* reserve buffer already being used */\n\t\t}\n\t}\n\tul_timeout = msecs_to_jiffies(srp->header.timeout);\n\ttimeout = (ul_timeout < INT_MAX) ? ul_timeout : INT_MAX;\n\tif ((!hp->cmdp) || (hp->cmd_len < 6) || (hp->cmd_len > sizeof (cmnd))) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (!access_ok(VERIFY_READ, hp->cmdp, hp->cmd_len)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\t}\n\tif (__copy_from_user(cmnd, hp->cmdp, hp->cmd_len)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\n\t}\n\tif (read_only && sg_allow_access(file, cmnd)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EPERM;\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, timeout, blocking);\n\tif (k < 0)\n\t\treturn k;\n\tif (o_srp)\n\t\t*o_srp = srp;\n\treturn count;\n}\n\nstatic int\nsg_common_write(Sg_fd * sfp, Sg_request * srp,\n\t\tunsigned char *cmnd, int timeout, int blocking)\n{\n\tint k, at_head;\n\tSg_device *sdp = sfp->parentdp;\n\tsg_io_hdr_t *hp = &srp->header;\n\n\tsrp->data.cmd_opcode = cmnd[0];\t/* hold opcode of command */\n\thp->status = 0;\n\thp->masked_status = 0;\n\thp->msg_status = 0;\n\thp->info = 0;\n\thp->host_status = 0;\n\thp->driver_status = 0;\n\thp->resid = 0;\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\"sg_common_write:  scsi opcode=0x%02x, cmd_size=%d\\n\",\n\t\t\t(int) cmnd[0], (int) hp->cmd_len));\n\n\tk = sg_start_req(srp, cmnd);\n\tif (k) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\"sg_common_write: start_req err=%d\\n\", k));\n\t\tsg_finish_rem_req(srp);\n\t\treturn k;\t/* probably out of space --> ENOMEM */\n\t}\n\tif (atomic_read(&sdp->detaching)) {\n\t\tif (srp->bio) {\n\t\t\tif (srp->rq->cmd != srp->rq->__cmd)\n\t\t\t\tkfree(srp->rq->cmd);\n\n\t\t\tblk_end_request_all(srp->rq, -EIO);\n\t\t\tsrp->rq = NULL;\n\t\t}\n\n\t\tsg_finish_rem_req(srp);\n\t\treturn -ENODEV;\n\t}\n\n\thp->duration = jiffies_to_msecs(jiffies);\n\tif (hp->interface_id != '\\0' &&\t/* v3 (or later) interface */\n\t    (SG_FLAG_Q_AT_TAIL & hp->flags))\n\t\tat_head = 0;\n\telse\n\t\tat_head = 1;\n\n\tsrp->rq->timeout = timeout;\n\tkref_get(&sfp->f_ref); /* sg_rq_end_io() does kref_put(). */\n\tblk_execute_rq_nowait(sdp->device->request_queue, sdp->disk,\n\t\t\t      srp->rq, at_head, sg_rq_end_io);\n\treturn 0;\n}\n\nstatic int srp_done(Sg_fd *sfp, Sg_request *srp)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tread_lock_irqsave(&sfp->rq_list_lock, flags);\n\tret = srp->done;\n\tread_unlock_irqrestore(&sfp->rq_list_lock, flags);\n\treturn ret;\n}\n\nstatic int max_sectors_bytes(struct request_queue *q)\n{\n\tunsigned int max_sectors = queue_max_sectors(q);\n\n\tmax_sectors = min_t(unsigned int, max_sectors, INT_MAX >> 9);\n\n\treturn max_sectors << 9;\n}\n\nstatic long\nsg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)\n{\n\tvoid __user *p = (void __user *)arg;\n\tint __user *ip = p;\n\tint result, val, read_only;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tunsigned long iflags;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t   \"sg_ioctl: cmd=0x%x\\n\", (int) cmd_in));\n\tread_only = (O_RDWR != (filp->f_flags & O_ACCMODE));\n\n\tswitch (cmd_in) {\n\tcase SG_IO:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (!scsi_block_when_processing_errors(sdp->device))\n\t\t\treturn -ENXIO;\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_IO_HDR))\n\t\t\treturn -EFAULT;\n\t\tresult = sg_new_write(sfp, filp, p, SZ_SG_IO_HDR,\n\t\t\t\t 1, read_only, 1, &srp);\n\t\tif (result < 0)\n\t\t\treturn result;\n\t\tresult = wait_event_interruptible(sfp->read_wait,\n\t\t\t(srp_done(sfp, srp) || atomic_read(&sdp->detaching)));\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\twrite_lock_irq(&sfp->rq_list_lock);\n\t\tif (srp->done) {\n\t\t\tsrp->done = 2;\n\t\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\t\tresult = sg_new_read(sfp, p, SZ_SG_IO_HDR, srp);\n\t\t\treturn (result < 0) ? result : 0;\n\t\t}\n\t\tsrp->orphan = 1;\n\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\treturn result;\t/* -ERESTARTSYS because signal hit process */\n\tcase SG_SET_TIMEOUT:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val < 0)\n\t\t\treturn -EIO;\n\t\tif (val >= mult_frac((s64)INT_MAX, USER_HZ, HZ))\n\t\t\tval = min_t(s64, mult_frac((s64)INT_MAX, USER_HZ, HZ),\n\t\t\t\t    INT_MAX);\n\t\tsfp->timeout_user = val;\n\t\tsfp->timeout = mult_frac(val, HZ, USER_HZ);\n\n\t\treturn 0;\n\tcase SG_GET_TIMEOUT:\t/* N.B. User receives timeout as return value */\n\t\t\t\t/* strange ..., for backward compatibility */\n\t\treturn sfp->timeout_user;\n\tcase SG_SET_FORCE_LOW_DMA:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val) {\n\t\t\tsfp->low_dma = 1;\n\t\t\tif ((0 == sfp->low_dma) && (0 == sg_res_in_use(sfp))) {\n\t\t\t\tval = (int) sfp->reserve.bufflen;\n\t\t\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t\t\t\tsg_build_reserve(sfp, val);\n\t\t\t}\n\t\t} else {\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t\tsfp->low_dma = sdp->device->host->unchecked_isa_dma;\n\t\t}\n\t\treturn 0;\n\tcase SG_GET_LOW_DMA:\n\t\treturn put_user((int) sfp->low_dma, ip);\n\tcase SG_GET_SCSI_ID:\n\t\tif (!access_ok(VERIFY_WRITE, p, sizeof (sg_scsi_id_t)))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_scsi_id_t __user *sg_idp = p;\n\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t\t__put_user((int) sdp->device->host->host_no,\n\t\t\t\t   &sg_idp->host_no);\n\t\t\t__put_user((int) sdp->device->channel,\n\t\t\t\t   &sg_idp->channel);\n\t\t\t__put_user((int) sdp->device->id, &sg_idp->scsi_id);\n\t\t\t__put_user((int) sdp->device->lun, &sg_idp->lun);\n\t\t\t__put_user((int) sdp->device->type, &sg_idp->scsi_type);\n\t\t\t__put_user((short) sdp->device->host->cmd_per_lun,\n\t\t\t\t   &sg_idp->h_cmd_per_lun);\n\t\t\t__put_user((short) sdp->device->queue_depth,\n\t\t\t\t   &sg_idp->d_queue_depth);\n\t\t\t__put_user(0, &sg_idp->unused[0]);\n\t\t\t__put_user(0, &sg_idp->unused[1]);\n\t\t\treturn 0;\n\t\t}\n\tcase SG_SET_FORCE_PACK_ID:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->force_packid = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_PACK_ID:\n\t\tif (!access_ok(VERIFY_WRITE, ip, sizeof (int)))\n\t\t\treturn -EFAULT;\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tfor (srp = sfp->headrp; srp; srp = srp->nextrp) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned)) {\n\t\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock,\n\t\t\t\t\t\t       iflags);\n\t\t\t\t__put_user(srp->header.pack_id, ip);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t__put_user(-1, ip);\n\t\treturn 0;\n\tcase SG_GET_NUM_WAITING:\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tfor (val = 0, srp = sfp->headrp; srp; srp = srp->nextrp) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned))\n\t\t\t\t++val;\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_SG_TABLESIZE:\n\t\treturn put_user(sdp->sg_tablesize, ip);\n\tcase SG_SET_RESERVED_SIZE:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n                if (val < 0)\n                        return -EINVAL;\n\t\tval = min_t(int, val,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\tif (val != sfp->reserve.bufflen) {\n\t\t\tif (sg_res_in_use(sfp) || sfp->mmap_called)\n\t\t\t\treturn -EBUSY;\n\t\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t\t\tsg_build_reserve(sfp, val);\n\t\t}\n\t\treturn 0;\n\tcase SG_GET_RESERVED_SIZE:\n\t\tval = min_t(int, sfp->reserve.bufflen,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\treturn put_user(val, ip);\n\tcase SG_SET_COMMAND_Q:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->cmd_q = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_COMMAND_Q:\n\t\treturn put_user((int) sfp->cmd_q, ip);\n\tcase SG_SET_KEEP_ORPHAN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->keep_orphan = val;\n\t\treturn 0;\n\tcase SG_GET_KEEP_ORPHAN:\n\t\treturn put_user((int) sfp->keep_orphan, ip);\n\tcase SG_NEXT_CMD_LEN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->next_cmd_len = (val > 0) ? val : 0;\n\t\treturn 0;\n\tcase SG_GET_VERSION_NUM:\n\t\treturn put_user(sg_version_num, ip);\n\tcase SG_GET_ACCESS_COUNT:\n\t\t/* faked - we don't have a real access count anymore */\n\t\tval = (sdp->device ? 1 : 0);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_REQUEST_TABLE:\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_REQ_INFO * SG_MAX_QUEUE))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_req_info_t *rinfo;\n\t\t\tunsigned int ms;\n\n\t\t\trinfo = kmalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!rinfo)\n\t\t\t\treturn -ENOMEM;\n\t\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\t\tfor (srp = sfp->headrp, val = 0; val < SG_MAX_QUEUE;\n\t\t\t     ++val, srp = srp ? srp->nextrp : srp) {\n\t\t\t\tmemset(&rinfo[val], 0, SZ_SG_REQ_INFO);\n\t\t\t\tif (srp) {\n\t\t\t\t\trinfo[val].req_state = srp->done + 1;\n\t\t\t\t\trinfo[val].problem =\n\t\t\t\t\t    srp->header.masked_status & \n\t\t\t\t\t    srp->header.host_status & \n\t\t\t\t\t    srp->header.driver_status;\n\t\t\t\t\tif (srp->done)\n\t\t\t\t\t\trinfo[val].duration =\n\t\t\t\t\t\t\tsrp->header.duration;\n\t\t\t\t\telse {\n\t\t\t\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\t\t\t\trinfo[val].duration =\n\t\t\t\t\t\t    (ms > srp->header.duration) ?\n\t\t\t\t\t\t    (ms - srp->header.duration) : 0;\n\t\t\t\t\t}\n\t\t\t\t\trinfo[val].orphan = srp->orphan;\n\t\t\t\t\trinfo[val].sg_io_owned =\n\t\t\t\t\t\t\tsrp->sg_io_owned;\n\t\t\t\t\trinfo[val].pack_id =\n\t\t\t\t\t\t\tsrp->header.pack_id;\n\t\t\t\t\trinfo[val].usr_ptr =\n\t\t\t\t\t\t\tsrp->header.usr_ptr;\n\t\t\t\t}\n\t\t\t}\n\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t\tresult = __copy_to_user(p, rinfo, \n\t\t\t\t\t\tSZ_SG_REQ_INFO * SG_MAX_QUEUE);\n\t\t\tresult = result ? -EFAULT : 0;\n\t\t\tkfree(rinfo);\n\t\t\treturn result;\n\t\t}\n\tcase SG_EMULATED_HOST:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\treturn put_user(sdp->device->host->hostt->emulated, ip);\n\tcase SCSI_IOCTL_SEND_COMMAND:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (read_only) {\n\t\t\tunsigned char opcode = WRITE_6;\n\t\t\tScsi_Ioctl_Command __user *siocp = p;\n\n\t\t\tif (copy_from_user(&opcode, siocp->data, 1))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (sg_allow_access(filp, &opcode))\n\t\t\t\treturn -EPERM;\n\t\t}\n\t\treturn sg_scsi_ioctl(sdp->device->request_queue, NULL, filp->f_mode, p);\n\tcase SG_SET_DEBUG:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsdp->sgdebug = (char) val;\n\t\treturn 0;\n\tcase BLKSECTGET:\n\t\treturn put_user(max_sectors_bytes(sdp->device->request_queue),\n\t\t\t\tip);\n\tcase BLKTRACESETUP:\n\t\treturn blk_trace_setup(sdp->device->request_queue,\n\t\t\t\t       sdp->disk->disk_name,\n\t\t\t\t       MKDEV(SCSI_GENERIC_MAJOR, sdp->index),\n\t\t\t\t       NULL,\n\t\t\t\t       (char *)arg);\n\tcase BLKTRACESTART:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 1);\n\tcase BLKTRACESTOP:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 0);\n\tcase BLKTRACETEARDOWN:\n\t\treturn blk_trace_remove(sdp->device->request_queue);\n\tcase SCSI_IOCTL_GET_IDLUN:\n\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\tcase SCSI_IOCTL_PROBE_HOST:\n\tcase SG_GET_TRANSFORM:\n\tcase SG_SCSI_RESET:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tbreak;\n\tdefault:\n\t\tif (read_only)\n\t\t\treturn -EPERM;\t/* don't know so take safe approach */\n\t\tbreak;\n\t}\n\n\tresult = scsi_ioctl_block_when_processing_errors(sdp->device,\n\t\t\tcmd_in, filp->f_flags & O_NDELAY);\n\tif (result)\n\t\treturn result;\n\treturn scsi_ioctl(sdp->device, cmd_in, p);\n}\n\n#ifdef CONFIG_COMPAT\nstatic long sg_compat_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tstruct scsi_device *sdev;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\n\tsdev = sdp->device;\n\tif (sdev->host->hostt->compat_ioctl) { \n\t\tint ret;\n\n\t\tret = sdev->host->hostt->compat_ioctl(sdev, cmd_in, (void __user *)arg);\n\n\t\treturn ret;\n\t}\n\t\n\treturn -ENOIOCTLCMD;\n}\n#endif\n\nstatic unsigned int\nsg_poll(struct file *filp, poll_table * wait)\n{\n\tunsigned int res = 0;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tint count = 0;\n\tunsigned long iflags;\n\n\tsfp = filp->private_data;\n\tif (!sfp)\n\t\treturn POLLERR;\n\tsdp = sfp->parentdp;\n\tif (!sdp)\n\t\treturn POLLERR;\n\tpoll_wait(filp, &sfp->read_wait, wait);\n\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tfor (srp = sfp->headrp; srp; srp = srp->nextrp) {\n\t\t/* if any read waiting, flag it */\n\t\tif ((0 == res) && (1 == srp->done) && (!srp->sg_io_owned))\n\t\t\tres = POLLIN | POLLRDNORM;\n\t\t++count;\n\t}\n\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\n\tif (atomic_read(&sdp->detaching))\n\t\tres |= POLLHUP;\n\telse if (!sfp->cmd_q) {\n\t\tif (0 == count)\n\t\t\tres |= POLLOUT | POLLWRNORM;\n\t} else if (count < SG_MAX_QUEUE)\n\t\tres |= POLLOUT | POLLWRNORM;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_poll: res=0x%x\\n\", (int) res));\n\treturn res;\n}\n\nstatic int\nsg_fasync(int fd, struct file *filp, int mode)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_fasync: mode=%d\\n\", mode));\n\n\treturn fasync_helper(fd, filp, mode, &sfp->async_qp);\n}\n\nstatic int\nsg_vma_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tSg_fd *sfp;\n\tunsigned long offset, len, sa;\n\tSg_scatter_hold *rsv_schp;\n\tint k, length;\n\n\tif ((NULL == vma) || (!(sfp = (Sg_fd *) vma->vm_private_data)))\n\t\treturn VM_FAULT_SIGBUS;\n\trsv_schp = &sfp->reserve;\n\toffset = vmf->pgoff << PAGE_SHIFT;\n\tif (offset >= rsv_schp->bufflen)\n\t\treturn VM_FAULT_SIGBUS;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_vma_fault: offset=%lu, scatg=%d\\n\",\n\t\t\t\t      offset, rsv_schp->k_use_sg));\n\tsa = vma->vm_start;\n\tlength = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg && sa < vma->vm_end; k++) {\n\t\tlen = vma->vm_end - sa;\n\t\tlen = (len < length) ? len : length;\n\t\tif (offset < len) {\n\t\t\tstruct page *page = nth_page(rsv_schp->pages[k],\n\t\t\t\t\t\t     offset >> PAGE_SHIFT);\n\t\t\tget_page(page);\t/* increment page count */\n\t\t\tvmf->page = page;\n\t\t\treturn 0; /* success */\n\t\t}\n\t\tsa += len;\n\t\toffset -= len;\n\t}\n\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic const struct vm_operations_struct sg_mmap_vm_ops = {\n\t.fault = sg_vma_fault,\n};\n\nstatic int\nsg_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tSg_fd *sfp;\n\tunsigned long req_sz, len, sa;\n\tSg_scatter_hold *rsv_schp;\n\tint k, length;\n\n\tif ((!filp) || (!vma) || (!(sfp = (Sg_fd *) filp->private_data)))\n\t\treturn -ENXIO;\n\treq_sz = vma->vm_end - vma->vm_start;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_mmap starting, vm_start=%p, len=%d\\n\",\n\t\t\t\t      (void *) vma->vm_start, (int) req_sz));\n\tif (vma->vm_pgoff)\n\t\treturn -EINVAL;\t/* want no offset */\n\trsv_schp = &sfp->reserve;\n\tif (req_sz > rsv_schp->bufflen)\n\t\treturn -ENOMEM;\t/* cannot map more than reserved buffer */\n\n\tsa = vma->vm_start;\n\tlength = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg && sa < vma->vm_end; k++) {\n\t\tlen = vma->vm_end - sa;\n\t\tlen = (len < length) ? len : length;\n\t\tsa += len;\n\t}\n\n\tsfp->mmap_called = 1;\n\tvma->vm_flags |= VM_IO | VM_DONTEXPAND | VM_DONTDUMP;\n\tvma->vm_private_data = sfp;\n\tvma->vm_ops = &sg_mmap_vm_ops;\n\treturn 0;\n}\n\nstatic void\nsg_rq_end_io_usercontext(struct work_struct *work)\n{\n\tstruct sg_request *srp = container_of(work, struct sg_request, ew.work);\n\tstruct sg_fd *sfp = srp->parentfp;\n\n\tsg_finish_rem_req(srp);\n\tkref_put(&sfp->f_ref, sg_remove_sfp);\n}\n\n/*\n * This function is a \"bottom half\" handler that is called by the mid\n * level when a command is completed (or has failed).\n */\nstatic void\nsg_rq_end_io(struct request *rq, int uptodate)\n{\n\tstruct sg_request *srp = rq->end_io_data;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tunsigned long iflags;\n\tunsigned int ms;\n\tchar *sense;\n\tint result, resid, done = 1;\n\n\tif (WARN_ON(srp->done != 0))\n\t\treturn;\n\n\tsfp = srp->parentfp;\n\tif (WARN_ON(sfp == NULL))\n\t\treturn;\n\n\tsdp = sfp->parentdp;\n\tif (unlikely(atomic_read(&sdp->detaching)))\n\t\tpr_info(\"%s: device detaching\\n\", __func__);\n\n\tsense = rq->sense;\n\tresult = rq->errors;\n\tresid = rq->resid_len;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_cmd_done: pack_id=%d, res=0x%x\\n\",\n\t\t\t\t      srp->header.pack_id, result));\n\tsrp->header.resid = resid;\n\tms = jiffies_to_msecs(jiffies);\n\tsrp->header.duration = (ms > srp->header.duration) ?\n\t\t\t\t(ms - srp->header.duration) : 0;\n\tif (0 != result) {\n\t\tstruct scsi_sense_hdr sshdr;\n\n\t\tsrp->header.status = 0xff & result;\n\t\tsrp->header.masked_status = status_byte(result);\n\t\tsrp->header.msg_status = msg_byte(result);\n\t\tsrp->header.host_status = host_byte(result);\n\t\tsrp->header.driver_status = driver_byte(result);\n\t\tif ((sdp->sgdebug > 0) &&\n\t\t    ((CHECK_CONDITION == srp->header.masked_status) ||\n\t\t     (COMMAND_TERMINATED == srp->header.masked_status)))\n\t\t\t__scsi_print_sense(sdp->device, __func__, sense,\n\t\t\t\t\t   SCSI_SENSE_BUFFERSIZE);\n\n\t\t/* Following if statement is a patch supplied by Eric Youngdale */\n\t\tif (driver_byte(result) != 0\n\t\t    && scsi_normalize_sense(sense, SCSI_SENSE_BUFFERSIZE, &sshdr)\n\t\t    && !scsi_sense_is_deferred(&sshdr)\n\t\t    && sshdr.sense_key == UNIT_ATTENTION\n\t\t    && sdp->device->removable) {\n\t\t\t/* Detected possible disc change. Set the bit - this */\n\t\t\t/* may be used if there are filesystems using this device */\n\t\t\tsdp->device->changed = 1;\n\t\t}\n\t}\n\t/* Rely on write phase to clean out srp status values, so no \"else\" */\n\n\t/*\n\t * Free the request as soon as it is complete so that its resources\n\t * can be reused without waiting for userspace to read() the\n\t * result.  But keep the associated bio (if any) around until\n\t * blk_rq_unmap_user() can be called from user context.\n\t */\n\tsrp->rq = NULL;\n\tif (rq->cmd != rq->__cmd)\n\t\tkfree(rq->cmd);\n\t__blk_put_request(rq->q, rq);\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tif (unlikely(srp->orphan)) {\n\t\tif (sfp->keep_orphan)\n\t\t\tsrp->sg_io_owned = 0;\n\t\telse\n\t\t\tdone = 0;\n\t}\n\tsrp->done = done;\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\n\tif (likely(done)) {\n\t\t/* Now wake up any sg_read() that is waiting for this\n\t\t * packet.\n\t\t */\n\t\twake_up_interruptible(&sfp->read_wait);\n\t\tkill_fasync(&sfp->async_qp, SIGPOLL, POLL_IN);\n\t\tkref_put(&sfp->f_ref, sg_remove_sfp);\n\t} else {\n\t\tINIT_WORK(&srp->ew.work, sg_rq_end_io_usercontext);\n\t\tschedule_work(&srp->ew.work);\n\t}\n}\n\nstatic const struct file_operations sg_fops = {\n\t.owner = THIS_MODULE,\n\t.read = sg_read,\n\t.write = sg_write,\n\t.poll = sg_poll,\n\t.unlocked_ioctl = sg_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = sg_compat_ioctl,\n#endif\n\t.open = sg_open,\n\t.mmap = sg_mmap,\n\t.release = sg_release,\n\t.fasync = sg_fasync,\n\t.llseek = no_llseek,\n};\n\nstatic struct class *sg_sysfs_class;\n\nstatic int sg_sysfs_valid = 0;\n\nstatic Sg_device *\nsg_alloc(struct gendisk *disk, struct scsi_device *scsidp)\n{\n\tstruct request_queue *q = scsidp->request_queue;\n\tSg_device *sdp;\n\tunsigned long iflags;\n\tint error;\n\tu32 k;\n\n\tsdp = kzalloc(sizeof(Sg_device), GFP_KERNEL);\n\tif (!sdp) {\n\t\tsdev_printk(KERN_WARNING, scsidp, \"%s: kmalloc Sg_device \"\n\t\t\t    \"failure\\n\", __func__);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tidr_preload(GFP_KERNEL);\n\twrite_lock_irqsave(&sg_index_lock, iflags);\n\n\terror = idr_alloc(&sg_index_idr, sdp, 0, SG_MAX_DEVS, GFP_NOWAIT);\n\tif (error < 0) {\n\t\tif (error == -ENOSPC) {\n\t\t\tsdev_printk(KERN_WARNING, scsidp,\n\t\t\t\t    \"Unable to attach sg device type=%d, minor number exceeds %d\\n\",\n\t\t\t\t    scsidp->type, SG_MAX_DEVS - 1);\n\t\t\terror = -ENODEV;\n\t\t} else {\n\t\t\tsdev_printk(KERN_WARNING, scsidp, \"%s: idr \"\n\t\t\t\t    \"allocation Sg_device failure: %d\\n\",\n\t\t\t\t    __func__, error);\n\t\t}\n\t\tgoto out_unlock;\n\t}\n\tk = error;\n\n\tSCSI_LOG_TIMEOUT(3, sdev_printk(KERN_INFO, scsidp,\n\t\t\t\t\t\"sg_alloc: dev=%d \\n\", k));\n\tsprintf(disk->disk_name, \"sg%d\", k);\n\tdisk->first_minor = k;\n\tsdp->disk = disk;\n\tsdp->device = scsidp;\n\tmutex_init(&sdp->open_rel_lock);\n\tINIT_LIST_HEAD(&sdp->sfds);\n\tinit_waitqueue_head(&sdp->open_wait);\n\tatomic_set(&sdp->detaching, 0);\n\trwlock_init(&sdp->sfd_lock);\n\tsdp->sg_tablesize = queue_max_segments(q);\n\tsdp->index = k;\n\tkref_init(&sdp->d_ref);\n\terror = 0;\n\nout_unlock:\n\twrite_unlock_irqrestore(&sg_index_lock, iflags);\n\tidr_preload_end();\n\n\tif (error) {\n\t\tkfree(sdp);\n\t\treturn ERR_PTR(error);\n\t}\n\treturn sdp;\n}\n\nstatic int\nsg_add_device(struct device *cl_dev, struct class_interface *cl_intf)\n{\n\tstruct scsi_device *scsidp = to_scsi_device(cl_dev->parent);\n\tstruct gendisk *disk;\n\tSg_device *sdp = NULL;\n\tstruct cdev * cdev = NULL;\n\tint error;\n\tunsigned long iflags;\n\n\tdisk = alloc_disk(1);\n\tif (!disk) {\n\t\tpr_warn(\"%s: alloc_disk failed\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\tdisk->major = SCSI_GENERIC_MAJOR;\n\n\terror = -ENOMEM;\n\tcdev = cdev_alloc();\n\tif (!cdev) {\n\t\tpr_warn(\"%s: cdev_alloc failed\\n\", __func__);\n\t\tgoto out;\n\t}\n\tcdev->owner = THIS_MODULE;\n\tcdev->ops = &sg_fops;\n\n\tsdp = sg_alloc(disk, scsidp);\n\tif (IS_ERR(sdp)) {\n\t\tpr_warn(\"%s: sg_alloc failed\\n\", __func__);\n\t\terror = PTR_ERR(sdp);\n\t\tgoto out;\n\t}\n\n\terror = cdev_add(cdev, MKDEV(SCSI_GENERIC_MAJOR, sdp->index), 1);\n\tif (error)\n\t\tgoto cdev_add_err;\n\n\tsdp->cdev = cdev;\n\tif (sg_sysfs_valid) {\n\t\tstruct device *sg_class_member;\n\n\t\tsg_class_member = device_create(sg_sysfs_class, cl_dev->parent,\n\t\t\t\t\t\tMKDEV(SCSI_GENERIC_MAJOR,\n\t\t\t\t\t\t      sdp->index),\n\t\t\t\t\t\tsdp, \"%s\", disk->disk_name);\n\t\tif (IS_ERR(sg_class_member)) {\n\t\t\tpr_err(\"%s: device_create failed\\n\", __func__);\n\t\t\terror = PTR_ERR(sg_class_member);\n\t\t\tgoto cdev_add_err;\n\t\t}\n\t\terror = sysfs_create_link(&scsidp->sdev_gendev.kobj,\n\t\t\t\t\t  &sg_class_member->kobj, \"generic\");\n\t\tif (error)\n\t\t\tpr_err(\"%s: unable to make symlink 'generic' back \"\n\t\t\t       \"to sg%d\\n\", __func__, sdp->index);\n\t} else\n\t\tpr_warn(\"%s: sg_sys Invalid\\n\", __func__);\n\n\tsdev_printk(KERN_NOTICE, scsidp, \"Attached scsi generic sg%d \"\n\t\t    \"type %d\\n\", sdp->index, scsidp->type);\n\n\tdev_set_drvdata(cl_dev, sdp);\n\n\treturn 0;\n\ncdev_add_err:\n\twrite_lock_irqsave(&sg_index_lock, iflags);\n\tidr_remove(&sg_index_idr, sdp->index);\n\twrite_unlock_irqrestore(&sg_index_lock, iflags);\n\tkfree(sdp);\n\nout:\n\tput_disk(disk);\n\tif (cdev)\n\t\tcdev_del(cdev);\n\treturn error;\n}\n\nstatic void\nsg_device_destroy(struct kref *kref)\n{\n\tstruct sg_device *sdp = container_of(kref, struct sg_device, d_ref);\n\tunsigned long flags;\n\n\t/* CAUTION!  Note that the device can still be found via idr_find()\n\t * even though the refcount is 0.  Therefore, do idr_remove() BEFORE\n\t * any other cleanup.\n\t */\n\n\twrite_lock_irqsave(&sg_index_lock, flags);\n\tidr_remove(&sg_index_idr, sdp->index);\n\twrite_unlock_irqrestore(&sg_index_lock, flags);\n\n\tSCSI_LOG_TIMEOUT(3,\n\t\tsg_printk(KERN_INFO, sdp, \"sg_device_destroy\\n\"));\n\n\tput_disk(sdp->disk);\n\tkfree(sdp);\n}\n\nstatic void\nsg_remove_device(struct device *cl_dev, struct class_interface *cl_intf)\n{\n\tstruct scsi_device *scsidp = to_scsi_device(cl_dev->parent);\n\tSg_device *sdp = dev_get_drvdata(cl_dev);\n\tunsigned long iflags;\n\tSg_fd *sfp;\n\tint val;\n\n\tif (!sdp)\n\t\treturn;\n\t/* want sdp->detaching non-zero as soon as possible */\n\tval = atomic_inc_return(&sdp->detaching);\n\tif (val > 1)\n\t\treturn; /* only want to do following once per device */\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"%s\\n\", __func__));\n\n\tread_lock_irqsave(&sdp->sfd_lock, iflags);\n\tlist_for_each_entry(sfp, &sdp->sfds, sfd_siblings) {\n\t\twake_up_interruptible_all(&sfp->read_wait);\n\t\tkill_fasync(&sfp->async_qp, SIGPOLL, POLL_HUP);\n\t}\n\twake_up_interruptible_all(&sdp->open_wait);\n\tread_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\n\tsysfs_remove_link(&scsidp->sdev_gendev.kobj, \"generic\");\n\tdevice_destroy(sg_sysfs_class, MKDEV(SCSI_GENERIC_MAJOR, sdp->index));\n\tcdev_del(sdp->cdev);\n\tsdp->cdev = NULL;\n\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n}\n\nmodule_param_named(scatter_elem_sz, scatter_elem_sz, int, S_IRUGO | S_IWUSR);\nmodule_param_named(def_reserved_size, def_reserved_size, int,\n\t\t   S_IRUGO | S_IWUSR);\nmodule_param_named(allow_dio, sg_allow_dio, int, S_IRUGO | S_IWUSR);\n\nMODULE_AUTHOR(\"Douglas Gilbert\");\nMODULE_DESCRIPTION(\"SCSI generic (sg) driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(SG_VERSION_STR);\nMODULE_ALIAS_CHARDEV_MAJOR(SCSI_GENERIC_MAJOR);\n\nMODULE_PARM_DESC(scatter_elem_sz, \"scatter gather element \"\n                \"size (default: max(SG_SCATTER_SZ, PAGE_SIZE))\");\nMODULE_PARM_DESC(def_reserved_size, \"size of buffer reserved for each fd\");\nMODULE_PARM_DESC(allow_dio, \"allow direct I/O (default: 0 (disallow))\");\n\nstatic int __init\ninit_sg(void)\n{\n\tint rc;\n\n\tif (scatter_elem_sz < PAGE_SIZE) {\n\t\tscatter_elem_sz = PAGE_SIZE;\n\t\tscatter_elem_sz_prev = scatter_elem_sz;\n\t}\n\tif (def_reserved_size >= 0)\n\t\tsg_big_buff = def_reserved_size;\n\telse\n\t\tdef_reserved_size = sg_big_buff;\n\n\trc = register_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0), \n\t\t\t\t    SG_MAX_DEVS, \"sg\");\n\tif (rc)\n\t\treturn rc;\n        sg_sysfs_class = class_create(THIS_MODULE, \"scsi_generic\");\n        if ( IS_ERR(sg_sysfs_class) ) {\n\t\trc = PTR_ERR(sg_sysfs_class);\n\t\tgoto err_out;\n        }\n\tsg_sysfs_valid = 1;\n\trc = scsi_register_interface(&sg_interface);\n\tif (0 == rc) {\n#ifdef CONFIG_SCSI_PROC_FS\n\t\tsg_proc_init();\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\t\treturn 0;\n\t}\n\tclass_destroy(sg_sysfs_class);\nerr_out:\n\tunregister_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0), SG_MAX_DEVS);\n\treturn rc;\n}\n\nstatic void __exit\nexit_sg(void)\n{\n#ifdef CONFIG_SCSI_PROC_FS\n\tsg_proc_cleanup();\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\tscsi_unregister_interface(&sg_interface);\n\tclass_destroy(sg_sysfs_class);\n\tsg_sysfs_valid = 0;\n\tunregister_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0),\n\t\t\t\t SG_MAX_DEVS);\n\tidr_destroy(&sg_index_idr);\n}\n\nstatic int\nsg_start_req(Sg_request *srp, unsigned char *cmd)\n{\n\tint res;\n\tstruct request *rq;\n\tSg_fd *sfp = srp->parentfp;\n\tsg_io_hdr_t *hp = &srp->header;\n\tint dxfer_len = (int) hp->dxfer_len;\n\tint dxfer_dir = hp->dxfer_direction;\n\tunsigned int iov_count = hp->iovec_count;\n\tSg_scatter_hold *req_schp = &srp->data;\n\tSg_scatter_hold *rsv_schp = &sfp->reserve;\n\tstruct request_queue *q = sfp->parentdp->device->request_queue;\n\tstruct rq_map_data *md, map_data;\n\tint rw = hp->dxfer_direction == SG_DXFER_TO_DEV ? WRITE : READ;\n\tunsigned char *long_cmdp = NULL;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_start_req: dxfer_len=%d\\n\",\n\t\t\t\t      dxfer_len));\n\n\tif (hp->cmd_len > BLK_MAX_CDB) {\n\t\tlong_cmdp = kzalloc(hp->cmd_len, GFP_KERNEL);\n\t\tif (!long_cmdp)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * NOTE\n\t *\n\t * With scsi-mq enabled, there are a fixed number of preallocated\n\t * requests equal in number to shost->can_queue.  If all of the\n\t * preallocated requests are already in use, then using GFP_ATOMIC with\n\t * blk_get_request() will return -EWOULDBLOCK, whereas using GFP_KERNEL\n\t * will cause blk_get_request() to sleep until an active command\n\t * completes, freeing up a request.  Neither option is ideal, but\n\t * GFP_KERNEL is the better choice to prevent userspace from getting an\n\t * unexpected EWOULDBLOCK.\n\t *\n\t * With scsi-mq disabled, blk_get_request() with GFP_KERNEL usually\n\t * does not sleep except under memory pressure.\n\t */\n\trq = blk_get_request(q, rw, GFP_KERNEL);\n\tif (IS_ERR(rq)) {\n\t\tkfree(long_cmdp);\n\t\treturn PTR_ERR(rq);\n\t}\n\n\tblk_rq_set_block_pc(rq);\n\n\tif (hp->cmd_len > BLK_MAX_CDB)\n\t\trq->cmd = long_cmdp;\n\tmemcpy(rq->cmd, cmd, hp->cmd_len);\n\trq->cmd_len = hp->cmd_len;\n\n\tsrp->rq = rq;\n\trq->end_io_data = srp;\n\trq->sense = srp->sense_b;\n\trq->retries = SG_DEFAULT_RETRIES;\n\n\tif ((dxfer_len <= 0) || (dxfer_dir == SG_DXFER_NONE))\n\t\treturn 0;\n\n\tif (sg_allow_dio && hp->flags & SG_FLAG_DIRECT_IO &&\n\t    dxfer_dir != SG_DXFER_UNKNOWN && !iov_count &&\n\t    !sfp->parentdp->device->host->unchecked_isa_dma &&\n\t    blk_rq_aligned(q, (unsigned long)hp->dxferp, dxfer_len))\n\t\tmd = NULL;\n\telse\n\t\tmd = &map_data;\n\n\tif (md) {\n\t\tif (!sg_res_in_use(sfp) && dxfer_len <= rsv_schp->bufflen)\n\t\t\tsg_link_reserve(sfp, srp, dxfer_len);\n\t\telse {\n\t\t\tres = sg_build_indirect(req_schp, sfp, dxfer_len);\n\t\t\tif (res)\n\t\t\t\treturn res;\n\t\t}\n\n\t\tmd->pages = req_schp->pages;\n\t\tmd->page_order = req_schp->page_order;\n\t\tmd->nr_entries = req_schp->k_use_sg;\n\t\tmd->offset = 0;\n\t\tmd->null_mapped = hp->dxferp ? 0 : 1;\n\t\tif (dxfer_dir == SG_DXFER_TO_FROM_DEV)\n\t\t\tmd->from_user = 1;\n\t\telse\n\t\t\tmd->from_user = 0;\n\t}\n\n\tif (iov_count) {\n\t\tstruct iovec *iov = NULL;\n\t\tstruct iov_iter i;\n\n\t\tres = import_iovec(rw, hp->dxferp, iov_count, 0, &iov, &i);\n\t\tif (res < 0)\n\t\t\treturn res;\n\n\t\tiov_iter_truncate(&i, hp->dxfer_len);\n\n\t\tres = blk_rq_map_user_iov(q, rq, md, &i, GFP_ATOMIC);\n\t\tkfree(iov);\n\t} else\n\t\tres = blk_rq_map_user(q, rq, md, hp->dxferp,\n\t\t\t\t      hp->dxfer_len, GFP_ATOMIC);\n\n\tif (!res) {\n\t\tsrp->bio = rq->bio;\n\n\t\tif (!md) {\n\t\t\treq_schp->dio_in_use = 1;\n\t\t\thp->info |= SG_INFO_DIRECT_IO;\n\t\t}\n\t}\n\treturn res;\n}\n\nstatic int\nsg_finish_rem_req(Sg_request *srp)\n{\n\tint ret = 0;\n\n\tSg_fd *sfp = srp->parentfp;\n\tSg_scatter_hold *req_schp = &srp->data;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_finish_rem_req: res_used=%d\\n\",\n\t\t\t\t      (int) srp->res_used));\n\tif (srp->bio)\n\t\tret = blk_rq_unmap_user(srp->bio);\n\n\tif (srp->rq) {\n\t\tif (srp->rq->cmd != srp->rq->__cmd)\n\t\t\tkfree(srp->rq->cmd);\n\t\tblk_put_request(srp->rq);\n\t}\n\n\tif (srp->res_used)\n\t\tsg_unlink_reserve(sfp, srp);\n\telse\n\t\tsg_remove_scat(sfp, req_schp);\n\n\tsg_remove_request(sfp, srp);\n\n\treturn ret;\n}\n\nstatic int\nsg_build_sgat(Sg_scatter_hold * schp, const Sg_fd * sfp, int tablesize)\n{\n\tint sg_bufflen = tablesize * sizeof(struct page *);\n\tgfp_t gfp_flags = GFP_ATOMIC | __GFP_NOWARN;\n\n\tschp->pages = kzalloc(sg_bufflen, gfp_flags);\n\tif (!schp->pages)\n\t\treturn -ENOMEM;\n\tschp->sglist_len = sg_bufflen;\n\treturn tablesize;\t/* number of scat_gath elements allocated */\n}\n\nstatic int\nsg_build_indirect(Sg_scatter_hold * schp, Sg_fd * sfp, int buff_size)\n{\n\tint ret_sz = 0, i, k, rem_sz, num, mx_sc_elems;\n\tint sg_tablesize = sfp->parentdp->sg_tablesize;\n\tint blk_size = buff_size, order;\n\tgfp_t gfp_mask = GFP_ATOMIC | __GFP_COMP | __GFP_NOWARN;\n\n\tif (blk_size < 0)\n\t\treturn -EFAULT;\n\tif (0 == blk_size)\n\t\t++blk_size;\t/* don't know why */\n\t/* round request up to next highest SG_SECTOR_SZ byte boundary */\n\tblk_size = ALIGN(blk_size, SG_SECTOR_SZ);\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\"sg_build_indirect: buff_size=%d, blk_size=%d\\n\",\n\t\tbuff_size, blk_size));\n\n\t/* N.B. ret_sz carried into this block ... */\n\tmx_sc_elems = sg_build_sgat(schp, sfp, sg_tablesize);\n\tif (mx_sc_elems < 0)\n\t\treturn mx_sc_elems;\t/* most likely -ENOMEM */\n\n\tnum = scatter_elem_sz;\n\tif (unlikely(num != scatter_elem_sz_prev)) {\n\t\tif (num < PAGE_SIZE) {\n\t\t\tscatter_elem_sz = PAGE_SIZE;\n\t\t\tscatter_elem_sz_prev = PAGE_SIZE;\n\t\t} else\n\t\t\tscatter_elem_sz_prev = num;\n\t}\n\n\tif (sfp->low_dma)\n\t\tgfp_mask |= GFP_DMA;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\tgfp_mask |= __GFP_ZERO;\n\n\torder = get_order(num);\nretry:\n\tret_sz = 1 << (PAGE_SHIFT + order);\n\n\tfor (k = 0, rem_sz = blk_size; rem_sz > 0 && k < mx_sc_elems;\n\t     k++, rem_sz -= ret_sz) {\n\n\t\tnum = (rem_sz > scatter_elem_sz_prev) ?\n\t\t\tscatter_elem_sz_prev : rem_sz;\n\n\t\tschp->pages[k] = alloc_pages(gfp_mask, order);\n\t\tif (!schp->pages[k])\n\t\t\tgoto out;\n\n\t\tif (num == scatter_elem_sz_prev) {\n\t\t\tif (unlikely(ret_sz > scatter_elem_sz_prev)) {\n\t\t\t\tscatter_elem_sz = ret_sz;\n\t\t\t\tscatter_elem_sz_prev = ret_sz;\n\t\t\t}\n\t\t}\n\n\t\tSCSI_LOG_TIMEOUT(5, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t \"sg_build_indirect: k=%d, num=%d, ret_sz=%d\\n\",\n\t\t\t\t k, num, ret_sz));\n\t}\t\t/* end of for loop */\n\n\tschp->page_order = order;\n\tschp->k_use_sg = k;\n\tSCSI_LOG_TIMEOUT(5, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_build_indirect: k_use_sg=%d, rem_sz=%d\\n\",\n\t\t\t k, rem_sz));\n\n\tschp->bufflen = blk_size;\n\tif (rem_sz > 0)\t/* must have failed */\n\t\treturn -ENOMEM;\n\treturn 0;\nout:\n\tfor (i = 0; i < k; i++)\n\t\t__free_pages(schp->pages[i], order);\n\n\tif (--order >= 0)\n\t\tgoto retry;\n\n\treturn -ENOMEM;\n}\n\nstatic void\nsg_remove_scat(Sg_fd * sfp, Sg_scatter_hold * schp)\n{\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_remove_scat: k_use_sg=%d\\n\", schp->k_use_sg));\n\tif (schp->pages && schp->sglist_len > 0) {\n\t\tif (!schp->dio_in_use) {\n\t\t\tint k;\n\n\t\t\tfor (k = 0; k < schp->k_use_sg && schp->pages[k]; k++) {\n\t\t\t\tSCSI_LOG_TIMEOUT(5,\n\t\t\t\t\tsg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t\t\"sg_remove_scat: k=%d, pg=0x%p\\n\",\n\t\t\t\t\tk, schp->pages[k]));\n\t\t\t\t__free_pages(schp->pages[k], schp->page_order);\n\t\t\t}\n\n\t\t\tkfree(schp->pages);\n\t\t}\n\t}\n\tmemset(schp, 0, sizeof (*schp));\n}\n\nstatic int\nsg_read_oxfer(Sg_request * srp, char __user *outp, int num_read_xfer)\n{\n\tSg_scatter_hold *schp = &srp->data;\n\tint k, num;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, srp->parentfp->parentdp,\n\t\t\t \"sg_read_oxfer: num_read_xfer=%d\\n\",\n\t\t\t num_read_xfer));\n\tif ((!outp) || (num_read_xfer <= 0))\n\t\treturn 0;\n\n\tnum = 1 << (PAGE_SHIFT + schp->page_order);\n\tfor (k = 0; k < schp->k_use_sg && schp->pages[k]; k++) {\n\t\tif (num > num_read_xfer) {\n\t\t\tif (__copy_to_user(outp, page_address(schp->pages[k]),\n\t\t\t\t\t   num_read_xfer))\n\t\t\t\treturn -EFAULT;\n\t\t\tbreak;\n\t\t} else {\n\t\t\tif (__copy_to_user(outp, page_address(schp->pages[k]),\n\t\t\t\t\t   num))\n\t\t\t\treturn -EFAULT;\n\t\t\tnum_read_xfer -= num;\n\t\t\tif (num_read_xfer <= 0)\n\t\t\t\tbreak;\n\t\t\toutp += num;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void\nsg_build_reserve(Sg_fd * sfp, int req_size)\n{\n\tSg_scatter_hold *schp = &sfp->reserve;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_build_reserve: req_size=%d\\n\", req_size));\n\tdo {\n\t\tif (req_size < PAGE_SIZE)\n\t\t\treq_size = PAGE_SIZE;\n\t\tif (0 == sg_build_indirect(schp, sfp, req_size))\n\t\t\treturn;\n\t\telse\n\t\t\tsg_remove_scat(sfp, schp);\n\t\treq_size >>= 1;\t/* divide by 2 */\n\t} while (req_size > (PAGE_SIZE / 2));\n}\n\nstatic void\nsg_link_reserve(Sg_fd * sfp, Sg_request * srp, int size)\n{\n\tSg_scatter_hold *req_schp = &srp->data;\n\tSg_scatter_hold *rsv_schp = &sfp->reserve;\n\tint k, num, rem;\n\n\tsrp->res_used = 1;\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_link_reserve: size=%d\\n\", size));\n\trem = size;\n\n\tnum = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg; k++) {\n\t\tif (rem <= num) {\n\t\t\treq_schp->k_use_sg = k + 1;\n\t\t\treq_schp->sglist_len = rsv_schp->sglist_len;\n\t\t\treq_schp->pages = rsv_schp->pages;\n\n\t\t\treq_schp->bufflen = size;\n\t\t\treq_schp->page_order = rsv_schp->page_order;\n\t\t\tbreak;\n\t\t} else\n\t\t\trem -= num;\n\t}\n\n\tif (k >= rsv_schp->k_use_sg)\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t \"sg_link_reserve: BAD size\\n\"));\n}\n\nstatic void\nsg_unlink_reserve(Sg_fd * sfp, Sg_request * srp)\n{\n\tSg_scatter_hold *req_schp = &srp->data;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, srp->parentfp->parentdp,\n\t\t\t\t      \"sg_unlink_reserve: req->k_use_sg=%d\\n\",\n\t\t\t\t      (int) req_schp->k_use_sg));\n\treq_schp->k_use_sg = 0;\n\treq_schp->bufflen = 0;\n\treq_schp->pages = NULL;\n\treq_schp->page_order = 0;\n\treq_schp->sglist_len = 0;\n\tsfp->save_scat_len = 0;\n\tsrp->res_used = 0;\n}\n\nstatic Sg_request *\nsg_get_rq_mark(Sg_fd * sfp, int pack_id)\n{\n\tSg_request *resp;\n\tunsigned long iflags;\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tfor (resp = sfp->headrp; resp; resp = resp->nextrp) {\n\t\t/* look for requests that are ready + not SG_IO owned */\n\t\tif ((1 == resp->done) && (!resp->sg_io_owned) &&\n\t\t    ((-1 == pack_id) || (resp->header.pack_id == pack_id))) {\n\t\t\tresp->done = 2;\t/* guard against other readers */\n\t\t\tbreak;\n\t\t}\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn resp;\n}\n\n/* always adds to end of list */\nstatic Sg_request *\nsg_add_request(Sg_fd * sfp)\n{\n\tint k;\n\tunsigned long iflags;\n\tSg_request *resp;\n\tSg_request *rp = sfp->req_arr;\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tresp = sfp->headrp;\n\tif (!resp) {\n\t\tmemset(rp, 0, sizeof (Sg_request));\n\t\trp->parentfp = sfp;\n\t\tresp = rp;\n\t\tsfp->headrp = resp;\n\t} else {\n\t\tif (0 == sfp->cmd_q)\n\t\t\tresp = NULL;\t/* command queuing disallowed */\n\t\telse {\n\t\t\tfor (k = 0; k < SG_MAX_QUEUE; ++k, ++rp) {\n\t\t\t\tif (!rp->parentfp)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (k < SG_MAX_QUEUE) {\n\t\t\t\tmemset(rp, 0, sizeof (Sg_request));\n\t\t\t\trp->parentfp = sfp;\n\t\t\t\twhile (resp->nextrp)\n\t\t\t\t\tresp = resp->nextrp;\n\t\t\t\tresp->nextrp = rp;\n\t\t\t\tresp = rp;\n\t\t\t} else\n\t\t\t\tresp = NULL;\n\t\t}\n\t}\n\tif (resp) {\n\t\tresp->nextrp = NULL;\n\t\tresp->header.duration = jiffies_to_msecs(jiffies);\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn resp;\n}\n\n/* Return of 1 for found; 0 for not found */\nstatic int\nsg_remove_request(Sg_fd * sfp, Sg_request * srp)\n{\n\tSg_request *prev_rp;\n\tSg_request *rp;\n\tunsigned long iflags;\n\tint res = 0;\n\n\tif ((!sfp) || (!srp) || (!sfp->headrp))\n\t\treturn res;\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tprev_rp = sfp->headrp;\n\tif (srp == prev_rp) {\n\t\tsfp->headrp = prev_rp->nextrp;\n\t\tprev_rp->parentfp = NULL;\n\t\tres = 1;\n\t} else {\n\t\twhile ((rp = prev_rp->nextrp)) {\n\t\t\tif (srp == rp) {\n\t\t\t\tprev_rp->nextrp = rp->nextrp;\n\t\t\t\trp->parentfp = NULL;\n\t\t\t\tres = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tprev_rp = rp;\n\t\t}\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn res;\n}\n\nstatic Sg_fd *\nsg_add_sfp(Sg_device * sdp)\n{\n\tSg_fd *sfp;\n\tunsigned long iflags;\n\tint bufflen;\n\n\tsfp = kzalloc(sizeof(*sfp), GFP_ATOMIC | __GFP_NOWARN);\n\tif (!sfp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinit_waitqueue_head(&sfp->read_wait);\n\trwlock_init(&sfp->rq_list_lock);\n\n\tkref_init(&sfp->f_ref);\n\tsfp->timeout = SG_DEFAULT_TIMEOUT;\n\tsfp->timeout_user = SG_DEFAULT_TIMEOUT_USER;\n\tsfp->force_packid = SG_DEF_FORCE_PACK_ID;\n\tsfp->low_dma = (SG_DEF_FORCE_LOW_DMA == 0) ?\n\t    sdp->device->host->unchecked_isa_dma : 1;\n\tsfp->cmd_q = SG_DEF_COMMAND_Q;\n\tsfp->keep_orphan = SG_DEF_KEEP_ORPHAN;\n\tsfp->parentdp = sdp;\n\twrite_lock_irqsave(&sdp->sfd_lock, iflags);\n\tif (atomic_read(&sdp->detaching)) {\n\t\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\tlist_add_tail(&sfp->sfd_siblings, &sdp->sfds);\n\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_add_sfp: sfp=0x%p\\n\", sfp));\n\tif (unlikely(sg_big_buff != def_reserved_size))\n\t\tsg_big_buff = def_reserved_size;\n\n\tbufflen = min_t(int, sg_big_buff,\n\t\t\tmax_sectors_bytes(sdp->device->request_queue));\n\tsg_build_reserve(sfp, bufflen);\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_add_sfp: bufflen=%d, k_use_sg=%d\\n\",\n\t\t\t\t      sfp->reserve.bufflen,\n\t\t\t\t      sfp->reserve.k_use_sg));\n\n\tkref_get(&sdp->d_ref);\n\t__module_get(THIS_MODULE);\n\treturn sfp;\n}\n\nstatic void\nsg_remove_sfp_usercontext(struct work_struct *work)\n{\n\tstruct sg_fd *sfp = container_of(work, struct sg_fd, ew.work);\n\tstruct sg_device *sdp = sfp->parentdp;\n\n\t/* Cleanup any responses which were never read(). */\n\twhile (sfp->headrp)\n\t\tsg_finish_rem_req(sfp->headrp);\n\n\tif (sfp->reserve.bufflen > 0) {\n\t\tSCSI_LOG_TIMEOUT(6, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\"sg_remove_sfp:    bufflen=%d, k_use_sg=%d\\n\",\n\t\t\t\t(int) sfp->reserve.bufflen,\n\t\t\t\t(int) sfp->reserve.k_use_sg));\n\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t}\n\n\tSCSI_LOG_TIMEOUT(6, sg_printk(KERN_INFO, sdp,\n\t\t\t\"sg_remove_sfp: sfp=0x%p\\n\", sfp));\n\tkfree(sfp);\n\n\tscsi_device_put(sdp->device);\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n\tmodule_put(THIS_MODULE);\n}\n\nstatic void\nsg_remove_sfp(struct kref *kref)\n{\n\tstruct sg_fd *sfp = container_of(kref, struct sg_fd, f_ref);\n\tstruct sg_device *sdp = sfp->parentdp;\n\tunsigned long iflags;\n\n\twrite_lock_irqsave(&sdp->sfd_lock, iflags);\n\tlist_del(&sfp->sfd_siblings);\n\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\n\tINIT_WORK(&sfp->ew.work, sg_remove_sfp_usercontext);\n\tschedule_work(&sfp->ew.work);\n}\n\nstatic int\nsg_res_in_use(Sg_fd * sfp)\n{\n\tconst Sg_request *srp;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tfor (srp = sfp->headrp; srp; srp = srp->nextrp)\n\t\tif (srp->res_used)\n\t\t\tbreak;\n\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn srp ? 1 : 0;\n}\n\n#ifdef CONFIG_SCSI_PROC_FS\nstatic int\nsg_idr_max_id(int id, void *p, void *data)\n{\n\tint *k = data;\n\n\tif (*k < id)\n\t\t*k = id;\n\n\treturn 0;\n}\n\nstatic int\nsg_last_dev(void)\n{\n\tint k = -1;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tidr_for_each(&sg_index_idr, sg_idr_max_id, &k);\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn k + 1;\t\t/* origin 1 */\n}\n#endif\n\n/* must be called with sg_index_lock held */\nstatic Sg_device *sg_lookup_dev(int dev)\n{\n\treturn idr_find(&sg_index_idr, dev);\n}\n\nstatic Sg_device *\nsg_get_dev(int dev)\n{\n\tstruct sg_device *sdp;\n\tunsigned long flags;\n\n\tread_lock_irqsave(&sg_index_lock, flags);\n\tsdp = sg_lookup_dev(dev);\n\tif (!sdp)\n\t\tsdp = ERR_PTR(-ENXIO);\n\telse if (atomic_read(&sdp->detaching)) {\n\t\t/* If sdp->detaching, then the refcount may already be 0, in\n\t\t * which case it would be a bug to do kref_get().\n\t\t */\n\t\tsdp = ERR_PTR(-ENODEV);\n\t} else\n\t\tkref_get(&sdp->d_ref);\n\tread_unlock_irqrestore(&sg_index_lock, flags);\n\n\treturn sdp;\n}\n\n#ifdef CONFIG_SCSI_PROC_FS\n\nstatic struct proc_dir_entry *sg_proc_sgp = NULL;\n\nstatic char sg_proc_sg_dirname[] = \"scsi/sg\";\n\nstatic int sg_proc_seq_show_int(struct seq_file *s, void *v);\n\nstatic int sg_proc_single_open_adio(struct inode *inode, struct file *file);\nstatic ssize_t sg_proc_write_adio(struct file *filp, const char __user *buffer,\n\t\t\t          size_t count, loff_t *off);\nstatic const struct file_operations adio_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_adio,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.write = sg_proc_write_adio,\n\t.release = single_release,\n};\n\nstatic int sg_proc_single_open_dressz(struct inode *inode, struct file *file);\nstatic ssize_t sg_proc_write_dressz(struct file *filp, \n\t\tconst char __user *buffer, size_t count, loff_t *off);\nstatic const struct file_operations dressz_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_dressz,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.write = sg_proc_write_dressz,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_version(struct seq_file *s, void *v);\nstatic int sg_proc_single_open_version(struct inode *inode, struct file *file);\nstatic const struct file_operations version_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_version,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_devhdr(struct seq_file *s, void *v);\nstatic int sg_proc_single_open_devhdr(struct inode *inode, struct file *file);\nstatic const struct file_operations devhdr_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_devhdr,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_dev(struct seq_file *s, void *v);\nstatic int sg_proc_open_dev(struct inode *inode, struct file *file);\nstatic void * dev_seq_start(struct seq_file *s, loff_t *pos);\nstatic void * dev_seq_next(struct seq_file *s, void *v, loff_t *pos);\nstatic void dev_seq_stop(struct seq_file *s, void *v);\nstatic const struct file_operations dev_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_dev,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations dev_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_dev,\n};\n\nstatic int sg_proc_seq_show_devstrs(struct seq_file *s, void *v);\nstatic int sg_proc_open_devstrs(struct inode *inode, struct file *file);\nstatic const struct file_operations devstrs_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_devstrs,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations devstrs_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_devstrs,\n};\n\nstatic int sg_proc_seq_show_debug(struct seq_file *s, void *v);\nstatic int sg_proc_open_debug(struct inode *inode, struct file *file);\nstatic const struct file_operations debug_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_debug,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations debug_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_debug,\n};\n\n\nstruct sg_proc_leaf {\n\tconst char * name;\n\tconst struct file_operations * fops;\n};\n\nstatic const struct sg_proc_leaf sg_proc_leaf_arr[] = {\n\t{\"allow_dio\", &adio_fops},\n\t{\"debug\", &debug_fops},\n\t{\"def_reserved_size\", &dressz_fops},\n\t{\"device_hdr\", &devhdr_fops},\n\t{\"devices\", &dev_fops},\n\t{\"device_strs\", &devstrs_fops},\n\t{\"version\", &version_fops}\n};\n\nstatic int\nsg_proc_init(void)\n{\n\tint num_leaves = ARRAY_SIZE(sg_proc_leaf_arr);\n\tint k;\n\n\tsg_proc_sgp = proc_mkdir(sg_proc_sg_dirname, NULL);\n\tif (!sg_proc_sgp)\n\t\treturn 1;\n\tfor (k = 0; k < num_leaves; ++k) {\n\t\tconst struct sg_proc_leaf *leaf = &sg_proc_leaf_arr[k];\n\t\tumode_t mask = leaf->fops->write ? S_IRUGO | S_IWUSR : S_IRUGO;\n\t\tproc_create(leaf->name, mask, sg_proc_sgp, leaf->fops);\n\t}\n\treturn 0;\n}\n\nstatic void\nsg_proc_cleanup(void)\n{\n\tint k;\n\tint num_leaves = ARRAY_SIZE(sg_proc_leaf_arr);\n\n\tif (!sg_proc_sgp)\n\t\treturn;\n\tfor (k = 0; k < num_leaves; ++k)\n\t\tremove_proc_entry(sg_proc_leaf_arr[k].name, sg_proc_sgp);\n\tremove_proc_entry(sg_proc_sg_dirname, NULL);\n}\n\n\nstatic int sg_proc_seq_show_int(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\n\", *((int *)s->private));\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_adio(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_int, &sg_allow_dio);\n}\n\nstatic ssize_t \nsg_proc_write_adio(struct file *filp, const char __user *buffer,\n\t\t   size_t count, loff_t *off)\n{\n\tint err;\n\tunsigned long num;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\treturn -EACCES;\n\terr = kstrtoul_from_user(buffer, count, 0, &num);\n\tif (err)\n\t\treturn err;\n\tsg_allow_dio = num ? 1 : 0;\n\treturn count;\n}\n\nstatic int sg_proc_single_open_dressz(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_int, &sg_big_buff);\n}\n\nstatic ssize_t \nsg_proc_write_dressz(struct file *filp, const char __user *buffer,\n\t\t     size_t count, loff_t *off)\n{\n\tint err;\n\tunsigned long k = ULONG_MAX;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\treturn -EACCES;\n\n\terr = kstrtoul_from_user(buffer, count, 0, &k);\n\tif (err)\n\t\treturn err;\n\tif (k <= 1048576) {\t/* limit \"big buff\" to 1 MB */\n\t\tsg_big_buff = k;\n\t\treturn count;\n\t}\n\treturn -ERANGE;\n}\n\nstatic int sg_proc_seq_show_version(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\t%s [%s]\\n\", sg_version_num, SG_VERSION_STR,\n\t\t   sg_version_date);\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_version(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_version, NULL);\n}\n\nstatic int sg_proc_seq_show_devhdr(struct seq_file *s, void *v)\n{\n\tseq_puts(s, \"host\\tchan\\tid\\tlun\\ttype\\topens\\tqdepth\\tbusy\\tonline\\n\");\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_devhdr(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_devhdr, NULL);\n}\n\nstruct sg_proc_deviter {\n\tloff_t\tindex;\n\tsize_t\tmax;\n};\n\nstatic void * dev_seq_start(struct seq_file *s, loff_t *pos)\n{\n\tstruct sg_proc_deviter * it = kmalloc(sizeof(*it), GFP_KERNEL);\n\n\ts->private = it;\n\tif (! it)\n\t\treturn NULL;\n\n\tit->index = *pos;\n\tit->max = sg_last_dev();\n\tif (it->index >= it->max)\n\t\treturn NULL;\n\treturn it;\n}\n\nstatic void * dev_seq_next(struct seq_file *s, void *v, loff_t *pos)\n{\n\tstruct sg_proc_deviter * it = s->private;\n\n\t*pos = ++it->index;\n\treturn (it->index < it->max) ? it : NULL;\n}\n\nstatic void dev_seq_stop(struct seq_file *s, void *v)\n{\n\tkfree(s->private);\n}\n\nstatic int sg_proc_open_dev(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &dev_seq_ops);\n}\n\nstatic int sg_proc_seq_show_dev(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tstruct scsi_device *scsidp;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tif ((NULL == sdp) || (NULL == sdp->device) ||\n\t    (atomic_read(&sdp->detaching)))\n\t\tseq_puts(s, \"-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\n\");\n\telse {\n\t\tscsidp = sdp->device;\n\t\tseq_printf(s, \"%d\\t%d\\t%d\\t%llu\\t%d\\t%d\\t%d\\t%d\\t%d\\n\",\n\t\t\t      scsidp->host->host_no, scsidp->channel,\n\t\t\t      scsidp->id, scsidp->lun, (int) scsidp->type,\n\t\t\t      1,\n\t\t\t      (int) scsidp->queue_depth,\n\t\t\t      (int) atomic_read(&scsidp->device_busy),\n\t\t\t      (int) scsi_device_online(scsidp));\n\t}\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\nstatic int sg_proc_open_devstrs(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &devstrs_seq_ops);\n}\n\nstatic int sg_proc_seq_show_devstrs(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tstruct scsi_device *scsidp;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tscsidp = sdp ? sdp->device : NULL;\n\tif (sdp && scsidp && (!atomic_read(&sdp->detaching)))\n\t\tseq_printf(s, \"%8.8s\\t%16.16s\\t%4.4s\\n\",\n\t\t\t   scsidp->vendor, scsidp->model, scsidp->rev);\n\telse\n\t\tseq_puts(s, \"<no active device>\\n\");\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\n/* must be called while holding sg_index_lock */\nstatic void sg_proc_debug_helper(struct seq_file *s, Sg_device * sdp)\n{\n\tint k, m, new_interface, blen, usg;\n\tSg_request *srp;\n\tSg_fd *fp;\n\tconst sg_io_hdr_t *hp;\n\tconst char * cp;\n\tunsigned int ms;\n\n\tk = 0;\n\tlist_for_each_entry(fp, &sdp->sfds, sfd_siblings) {\n\t\tk++;\n\t\tread_lock(&fp->rq_list_lock); /* irqs already disabled */\n\t\tseq_printf(s, \"   FD(%d): timeout=%dms bufflen=%d \"\n\t\t\t   \"(res)sgat=%d low_dma=%d\\n\", k,\n\t\t\t   jiffies_to_msecs(fp->timeout),\n\t\t\t   fp->reserve.bufflen,\n\t\t\t   (int) fp->reserve.k_use_sg,\n\t\t\t   (int) fp->low_dma);\n\t\tseq_printf(s, \"   cmd_q=%d f_packid=%d k_orphan=%d closed=0\\n\",\n\t\t\t   (int) fp->cmd_q, (int) fp->force_packid,\n\t\t\t   (int) fp->keep_orphan);\n\t\tfor (m = 0, srp = fp->headrp;\n\t\t\t\tsrp != NULL;\n\t\t\t\t++m, srp = srp->nextrp) {\n\t\t\thp = &srp->header;\n\t\t\tnew_interface = (hp->interface_id == '\\0') ? 0 : 1;\n\t\t\tif (srp->res_used) {\n\t\t\t\tif (new_interface && \n\t\t\t\t    (SG_FLAG_MMAP_IO & hp->flags))\n\t\t\t\t\tcp = \"     mmap>> \";\n\t\t\t\telse\n\t\t\t\t\tcp = \"     rb>> \";\n\t\t\t} else {\n\t\t\t\tif (SG_INFO_DIRECT_IO_MASK & hp->info)\n\t\t\t\t\tcp = \"     dio>> \";\n\t\t\t\telse\n\t\t\t\t\tcp = \"     \";\n\t\t\t}\n\t\t\tseq_puts(s, cp);\n\t\t\tblen = srp->data.bufflen;\n\t\t\tusg = srp->data.k_use_sg;\n\t\t\tseq_puts(s, srp->done ?\n\t\t\t\t ((1 == srp->done) ?  \"rcv:\" : \"fin:\")\n\t\t\t\t  : \"act:\");\n\t\t\tseq_printf(s, \" id=%d blen=%d\",\n\t\t\t\t   srp->header.pack_id, blen);\n\t\t\tif (srp->done)\n\t\t\t\tseq_printf(s, \" dur=%d\", hp->duration);\n\t\t\telse {\n\t\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\t\tseq_printf(s, \" t_o/elap=%d/%d\",\n\t\t\t\t\t(new_interface ? hp->timeout :\n\t\t\t\t\t\t  jiffies_to_msecs(fp->timeout)),\n\t\t\t\t\t(ms > hp->duration ? ms - hp->duration : 0));\n\t\t\t}\n\t\t\tseq_printf(s, \"ms sgat=%d op=0x%02x\\n\", usg,\n\t\t\t\t   (int) srp->data.cmd_opcode);\n\t\t}\n\t\tif (0 == m)\n\t\t\tseq_puts(s, \"     No requests active\\n\");\n\t\tread_unlock(&fp->rq_list_lock);\n\t}\n}\n\nstatic int sg_proc_open_debug(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &debug_seq_ops);\n}\n\nstatic int sg_proc_seq_show_debug(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tunsigned long iflags;\n\n\tif (it && (0 == it->index))\n\t\tseq_printf(s, \"max_active_device=%d  def_reserved_size=%d\\n\",\n\t\t\t   (int)it->max, sg_big_buff);\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tif (NULL == sdp)\n\t\tgoto skip;\n\tread_lock(&sdp->sfd_lock);\n\tif (!list_empty(&sdp->sfds)) {\n\t\tseq_printf(s, \" >>> device=%s \", sdp->disk->disk_name);\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\tseq_puts(s, \"detaching pending close \");\n\t\telse if (sdp->device) {\n\t\t\tstruct scsi_device *scsidp = sdp->device;\n\n\t\t\tseq_printf(s, \"%d:%d:%d:%llu   em=%d\",\n\t\t\t\t   scsidp->host->host_no,\n\t\t\t\t   scsidp->channel, scsidp->id,\n\t\t\t\t   scsidp->lun,\n\t\t\t\t   scsidp->host->hostt->emulated);\n\t\t}\n\t\tseq_printf(s, \" sg_tablesize=%d excl=%d open_cnt=%d\\n\",\n\t\t\t   sdp->sg_tablesize, sdp->exclude, sdp->open_cnt);\n\t\tsg_proc_debug_helper(s, sdp);\n\t}\n\tread_unlock(&sdp->sfd_lock);\nskip:\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\nmodule_init(init_sg);\nmodule_exit(exit_sg);\n"], "fixing_code": ["/*\n * bsg.c - block layer implementation of the sg v4 interface\n *\n * Copyright (C) 2004 Jens Axboe <axboe@suse.de> SUSE Labs\n * Copyright (C) 2004 Peter M. Jones <pjones@redhat.com>\n *\n *  This file is subject to the terms and conditions of the GNU General Public\n *  License version 2.  See the file \"COPYING\" in the main directory of this\n *  archive for more details.\n *\n */\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/file.h>\n#include <linux/blkdev.h>\n#include <linux/poll.h>\n#include <linux/cdev.h>\n#include <linux/jiffies.h>\n#include <linux/percpu.h>\n#include <linux/uio.h>\n#include <linux/idr.h>\n#include <linux/bsg.h>\n#include <linux/slab.h>\n\n#include <scsi/scsi.h>\n#include <scsi/scsi_ioctl.h>\n#include <scsi/scsi_cmnd.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_driver.h>\n#include <scsi/sg.h>\n\n#define BSG_DESCRIPTION\t\"Block layer SCSI generic (bsg) driver\"\n#define BSG_VERSION\t\"0.4\"\n\nstruct bsg_device {\n\tstruct request_queue *queue;\n\tspinlock_t lock;\n\tstruct list_head busy_list;\n\tstruct list_head done_list;\n\tstruct hlist_node dev_list;\n\tatomic_t ref_count;\n\tint queued_cmds;\n\tint done_cmds;\n\twait_queue_head_t wq_done;\n\twait_queue_head_t wq_free;\n\tchar name[20];\n\tint max_queue;\n\tunsigned long flags;\n};\n\nenum {\n\tBSG_F_BLOCK\t\t= 1,\n};\n\n#define BSG_DEFAULT_CMDS\t64\n#define BSG_MAX_DEVS\t\t32768\n\n#undef BSG_DEBUG\n\n#ifdef BSG_DEBUG\n#define dprintk(fmt, args...) printk(KERN_ERR \"%s: \" fmt, __func__, ##args)\n#else\n#define dprintk(fmt, args...)\n#endif\n\nstatic DEFINE_MUTEX(bsg_mutex);\nstatic DEFINE_IDR(bsg_minor_idr);\n\n#define BSG_LIST_ARRAY_SIZE\t8\nstatic struct hlist_head bsg_device_list[BSG_LIST_ARRAY_SIZE];\n\nstatic struct class *bsg_class;\nstatic int bsg_major;\n\nstatic struct kmem_cache *bsg_cmd_cachep;\n\n/*\n * our internal command type\n */\nstruct bsg_command {\n\tstruct bsg_device *bd;\n\tstruct list_head list;\n\tstruct request *rq;\n\tstruct bio *bio;\n\tstruct bio *bidi_bio;\n\tint err;\n\tstruct sg_io_v4 hdr;\n\tchar sense[SCSI_SENSE_BUFFERSIZE];\n};\n\nstatic void bsg_free_command(struct bsg_command *bc)\n{\n\tstruct bsg_device *bd = bc->bd;\n\tunsigned long flags;\n\n\tkmem_cache_free(bsg_cmd_cachep, bc);\n\n\tspin_lock_irqsave(&bd->lock, flags);\n\tbd->queued_cmds--;\n\tspin_unlock_irqrestore(&bd->lock, flags);\n\n\twake_up(&bd->wq_free);\n}\n\nstatic struct bsg_command *bsg_alloc_command(struct bsg_device *bd)\n{\n\tstruct bsg_command *bc = ERR_PTR(-EINVAL);\n\n\tspin_lock_irq(&bd->lock);\n\n\tif (bd->queued_cmds >= bd->max_queue)\n\t\tgoto out;\n\n\tbd->queued_cmds++;\n\tspin_unlock_irq(&bd->lock);\n\n\tbc = kmem_cache_zalloc(bsg_cmd_cachep, GFP_KERNEL);\n\tif (unlikely(!bc)) {\n\t\tspin_lock_irq(&bd->lock);\n\t\tbd->queued_cmds--;\n\t\tbc = ERR_PTR(-ENOMEM);\n\t\tgoto out;\n\t}\n\n\tbc->bd = bd;\n\tINIT_LIST_HEAD(&bc->list);\n\tdprintk(\"%s: returning free cmd %p\\n\", bd->name, bc);\n\treturn bc;\nout:\n\tspin_unlock_irq(&bd->lock);\n\treturn bc;\n}\n\nstatic inline struct hlist_head *bsg_dev_idx_hash(int index)\n{\n\treturn &bsg_device_list[index & (BSG_LIST_ARRAY_SIZE - 1)];\n}\n\nstatic int blk_fill_sgv4_hdr_rq(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct sg_io_v4 *hdr, struct bsg_device *bd,\n\t\t\t\tfmode_t has_write_perm)\n{\n\tif (hdr->request_len > BLK_MAX_CDB) {\n\t\trq->cmd = kzalloc(hdr->request_len, GFP_KERNEL);\n\t\tif (!rq->cmd)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (copy_from_user(rq->cmd, (void __user *)(unsigned long)hdr->request,\n\t\t\t   hdr->request_len))\n\t\treturn -EFAULT;\n\n\tif (hdr->subprotocol == BSG_SUB_PROTOCOL_SCSI_CMD) {\n\t\tif (blk_verify_command(rq->cmd, has_write_perm))\n\t\t\treturn -EPERM;\n\t} else if (!capable(CAP_SYS_RAWIO))\n\t\treturn -EPERM;\n\n\t/*\n\t * fill in request structure\n\t */\n\trq->cmd_len = hdr->request_len;\n\n\trq->timeout = msecs_to_jiffies(hdr->timeout);\n\tif (!rq->timeout)\n\t\trq->timeout = q->sg_timeout;\n\tif (!rq->timeout)\n\t\trq->timeout = BLK_DEFAULT_SG_TIMEOUT;\n\tif (rq->timeout < BLK_MIN_SG_TIMEOUT)\n\t\trq->timeout = BLK_MIN_SG_TIMEOUT;\n\n\treturn 0;\n}\n\n/*\n * Check if sg_io_v4 from user is allowed and valid\n */\nstatic int\nbsg_validate_sgv4_hdr(struct sg_io_v4 *hdr, int *rw)\n{\n\tint ret = 0;\n\n\tif (hdr->guard != 'Q')\n\t\treturn -EINVAL;\n\n\tswitch (hdr->protocol) {\n\tcase BSG_PROTOCOL_SCSI:\n\t\tswitch (hdr->subprotocol) {\n\t\tcase BSG_SUB_PROTOCOL_SCSI_CMD:\n\t\tcase BSG_SUB_PROTOCOL_SCSI_TRANSPORT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\t*rw = hdr->dout_xfer_len ? WRITE : READ;\n\treturn ret;\n}\n\n/*\n * map sg_io_v4 to a request.\n */\nstatic struct request *\nbsg_map_hdr(struct bsg_device *bd, struct sg_io_v4 *hdr, fmode_t has_write_perm,\n\t    u8 *sense)\n{\n\tstruct request_queue *q = bd->queue;\n\tstruct request *rq, *next_rq = NULL;\n\tint ret, rw;\n\tunsigned int dxfer_len;\n\tvoid __user *dxferp = NULL;\n\tstruct bsg_class_device *bcd = &q->bsg_dev;\n\n\t/* if the LLD has been removed then the bsg_unregister_queue will\n\t * eventually be called and the class_dev was freed, so we can no\n\t * longer use this request_queue. Return no such address.\n\t */\n\tif (!bcd->class_dev)\n\t\treturn ERR_PTR(-ENXIO);\n\n\tdprintk(\"map hdr %llx/%u %llx/%u\\n\", (unsigned long long) hdr->dout_xferp,\n\t\thdr->dout_xfer_len, (unsigned long long) hdr->din_xferp,\n\t\thdr->din_xfer_len);\n\n\tret = bsg_validate_sgv4_hdr(hdr, &rw);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\t/*\n\t * map scatter-gather elements separately and string them to request\n\t */\n\trq = blk_get_request(q, rw, GFP_KERNEL);\n\tif (IS_ERR(rq))\n\t\treturn rq;\n\tblk_rq_set_block_pc(rq);\n\n\tret = blk_fill_sgv4_hdr_rq(q, rq, hdr, bd, has_write_perm);\n\tif (ret)\n\t\tgoto out;\n\n\tif (rw == WRITE && hdr->din_xfer_len) {\n\t\tif (!test_bit(QUEUE_FLAG_BIDI, &q->queue_flags)) {\n\t\t\tret = -EOPNOTSUPP;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnext_rq = blk_get_request(q, READ, GFP_KERNEL);\n\t\tif (IS_ERR(next_rq)) {\n\t\t\tret = PTR_ERR(next_rq);\n\t\t\tnext_rq = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\trq->next_rq = next_rq;\n\t\tnext_rq->cmd_type = rq->cmd_type;\n\n\t\tdxferp = (void __user *)(unsigned long)hdr->din_xferp;\n\t\tret =  blk_rq_map_user(q, next_rq, NULL, dxferp,\n\t\t\t\t       hdr->din_xfer_len, GFP_KERNEL);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tif (hdr->dout_xfer_len) {\n\t\tdxfer_len = hdr->dout_xfer_len;\n\t\tdxferp = (void __user *)(unsigned long)hdr->dout_xferp;\n\t} else if (hdr->din_xfer_len) {\n\t\tdxfer_len = hdr->din_xfer_len;\n\t\tdxferp = (void __user *)(unsigned long)hdr->din_xferp;\n\t} else\n\t\tdxfer_len = 0;\n\n\tif (dxfer_len) {\n\t\tret = blk_rq_map_user(q, rq, NULL, dxferp, dxfer_len,\n\t\t\t\t      GFP_KERNEL);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\trq->sense = sense;\n\trq->sense_len = 0;\n\n\treturn rq;\nout:\n\tif (rq->cmd != rq->__cmd)\n\t\tkfree(rq->cmd);\n\tblk_put_request(rq);\n\tif (next_rq) {\n\t\tblk_rq_unmap_user(next_rq->bio);\n\t\tblk_put_request(next_rq);\n\t}\n\treturn ERR_PTR(ret);\n}\n\n/*\n * async completion call-back from the block layer, when scsi/ide/whatever\n * calls end_that_request_last() on a request\n */\nstatic void bsg_rq_end_io(struct request *rq, int uptodate)\n{\n\tstruct bsg_command *bc = rq->end_io_data;\n\tstruct bsg_device *bd = bc->bd;\n\tunsigned long flags;\n\n\tdprintk(\"%s: finished rq %p bc %p, bio %p stat %d\\n\",\n\t\tbd->name, rq, bc, bc->bio, uptodate);\n\n\tbc->hdr.duration = jiffies_to_msecs(jiffies - bc->hdr.duration);\n\n\tspin_lock_irqsave(&bd->lock, flags);\n\tlist_move_tail(&bc->list, &bd->done_list);\n\tbd->done_cmds++;\n\tspin_unlock_irqrestore(&bd->lock, flags);\n\n\twake_up(&bd->wq_done);\n}\n\n/*\n * do final setup of a 'bc' and submit the matching 'rq' to the block\n * layer for io\n */\nstatic void bsg_add_command(struct bsg_device *bd, struct request_queue *q,\n\t\t\t    struct bsg_command *bc, struct request *rq)\n{\n\tint at_head = (0 == (bc->hdr.flags & BSG_FLAG_Q_AT_TAIL));\n\n\t/*\n\t * add bc command to busy queue and submit rq for io\n\t */\n\tbc->rq = rq;\n\tbc->bio = rq->bio;\n\tif (rq->next_rq)\n\t\tbc->bidi_bio = rq->next_rq->bio;\n\tbc->hdr.duration = jiffies;\n\tspin_lock_irq(&bd->lock);\n\tlist_add_tail(&bc->list, &bd->busy_list);\n\tspin_unlock_irq(&bd->lock);\n\n\tdprintk(\"%s: queueing rq %p, bc %p\\n\", bd->name, rq, bc);\n\n\trq->end_io_data = bc;\n\tblk_execute_rq_nowait(q, NULL, rq, at_head, bsg_rq_end_io);\n}\n\nstatic struct bsg_command *bsg_next_done_cmd(struct bsg_device *bd)\n{\n\tstruct bsg_command *bc = NULL;\n\n\tspin_lock_irq(&bd->lock);\n\tif (bd->done_cmds) {\n\t\tbc = list_first_entry(&bd->done_list, struct bsg_command, list);\n\t\tlist_del(&bc->list);\n\t\tbd->done_cmds--;\n\t}\n\tspin_unlock_irq(&bd->lock);\n\n\treturn bc;\n}\n\n/*\n * Get a finished command from the done list\n */\nstatic struct bsg_command *bsg_get_done_cmd(struct bsg_device *bd)\n{\n\tstruct bsg_command *bc;\n\tint ret;\n\n\tdo {\n\t\tbc = bsg_next_done_cmd(bd);\n\t\tif (bc)\n\t\t\tbreak;\n\n\t\tif (!test_bit(BSG_F_BLOCK, &bd->flags)) {\n\t\t\tbc = ERR_PTR(-EAGAIN);\n\t\t\tbreak;\n\t\t}\n\n\t\tret = wait_event_interruptible(bd->wq_done, bd->done_cmds);\n\t\tif (ret) {\n\t\t\tbc = ERR_PTR(-ERESTARTSYS);\n\t\t\tbreak;\n\t\t}\n\t} while (1);\n\n\tdprintk(\"%s: returning done %p\\n\", bd->name, bc);\n\n\treturn bc;\n}\n\nstatic int blk_complete_sgv4_hdr_rq(struct request *rq, struct sg_io_v4 *hdr,\n\t\t\t\t    struct bio *bio, struct bio *bidi_bio)\n{\n\tint ret = 0;\n\n\tdprintk(\"rq %p bio %p 0x%x\\n\", rq, bio, rq->errors);\n\t/*\n\t * fill in all the output members\n\t */\n\thdr->device_status = rq->errors & 0xff;\n\thdr->transport_status = host_byte(rq->errors);\n\thdr->driver_status = driver_byte(rq->errors);\n\thdr->info = 0;\n\tif (hdr->device_status || hdr->transport_status || hdr->driver_status)\n\t\thdr->info |= SG_INFO_CHECK;\n\thdr->response_len = 0;\n\n\tif (rq->sense_len && hdr->response) {\n\t\tint len = min_t(unsigned int, hdr->max_response_len,\n\t\t\t\t\trq->sense_len);\n\n\t\tret = copy_to_user((void __user *)(unsigned long)hdr->response,\n\t\t\t\t   rq->sense, len);\n\t\tif (!ret)\n\t\t\thdr->response_len = len;\n\t\telse\n\t\t\tret = -EFAULT;\n\t}\n\n\tif (rq->next_rq) {\n\t\thdr->dout_resid = rq->resid_len;\n\t\thdr->din_resid = rq->next_rq->resid_len;\n\t\tblk_rq_unmap_user(bidi_bio);\n\t\tblk_put_request(rq->next_rq);\n\t} else if (rq_data_dir(rq) == READ)\n\t\thdr->din_resid = rq->resid_len;\n\telse\n\t\thdr->dout_resid = rq->resid_len;\n\n\t/*\n\t * If the request generated a negative error number, return it\n\t * (providing we aren't already returning an error); if it's\n\t * just a protocol response (i.e. non negative), that gets\n\t * processed above.\n\t */\n\tif (!ret && rq->errors < 0)\n\t\tret = rq->errors;\n\n\tblk_rq_unmap_user(bio);\n\tif (rq->cmd != rq->__cmd)\n\t\tkfree(rq->cmd);\n\tblk_put_request(rq);\n\n\treturn ret;\n}\n\nstatic bool bsg_complete(struct bsg_device *bd)\n{\n\tbool ret = false;\n\tbool spin;\n\n\tdo {\n\t\tspin_lock_irq(&bd->lock);\n\n\t\tBUG_ON(bd->done_cmds > bd->queued_cmds);\n\n\t\t/*\n\t\t * All commands consumed.\n\t\t */\n\t\tif (bd->done_cmds == bd->queued_cmds)\n\t\t\tret = true;\n\n\t\tspin = !test_bit(BSG_F_BLOCK, &bd->flags);\n\n\t\tspin_unlock_irq(&bd->lock);\n\t} while (!ret && spin);\n\n\treturn ret;\n}\n\nstatic int bsg_complete_all_commands(struct bsg_device *bd)\n{\n\tstruct bsg_command *bc;\n\tint ret, tret;\n\n\tdprintk(\"%s: entered\\n\", bd->name);\n\n\t/*\n\t * wait for all commands to complete\n\t */\n\tio_wait_event(bd->wq_done, bsg_complete(bd));\n\n\t/*\n\t * discard done commands\n\t */\n\tret = 0;\n\tdo {\n\t\tspin_lock_irq(&bd->lock);\n\t\tif (!bd->queued_cmds) {\n\t\t\tspin_unlock_irq(&bd->lock);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock_irq(&bd->lock);\n\n\t\tbc = bsg_get_done_cmd(bd);\n\t\tif (IS_ERR(bc))\n\t\t\tbreak;\n\n\t\ttret = blk_complete_sgv4_hdr_rq(bc->rq, &bc->hdr, bc->bio,\n\t\t\t\t\t\tbc->bidi_bio);\n\t\tif (!ret)\n\t\t\tret = tret;\n\n\t\tbsg_free_command(bc);\n\t} while (1);\n\n\treturn ret;\n}\n\nstatic int\n__bsg_read(char __user *buf, size_t count, struct bsg_device *bd,\n\t   const struct iovec *iov, ssize_t *bytes_read)\n{\n\tstruct bsg_command *bc;\n\tint nr_commands, ret;\n\n\tif (count % sizeof(struct sg_io_v4))\n\t\treturn -EINVAL;\n\n\tret = 0;\n\tnr_commands = count / sizeof(struct sg_io_v4);\n\twhile (nr_commands) {\n\t\tbc = bsg_get_done_cmd(bd);\n\t\tif (IS_ERR(bc)) {\n\t\t\tret = PTR_ERR(bc);\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * this is the only case where we need to copy data back\n\t\t * after completing the request. so do that here,\n\t\t * bsg_complete_work() cannot do that for us\n\t\t */\n\t\tret = blk_complete_sgv4_hdr_rq(bc->rq, &bc->hdr, bc->bio,\n\t\t\t\t\t       bc->bidi_bio);\n\n\t\tif (copy_to_user(buf, &bc->hdr, sizeof(bc->hdr)))\n\t\t\tret = -EFAULT;\n\n\t\tbsg_free_command(bc);\n\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tbuf += sizeof(struct sg_io_v4);\n\t\t*bytes_read += sizeof(struct sg_io_v4);\n\t\tnr_commands--;\n\t}\n\n\treturn ret;\n}\n\nstatic inline void bsg_set_block(struct bsg_device *bd, struct file *file)\n{\n\tif (file->f_flags & O_NONBLOCK)\n\t\tclear_bit(BSG_F_BLOCK, &bd->flags);\n\telse\n\t\tset_bit(BSG_F_BLOCK, &bd->flags);\n}\n\n/*\n * Check if the error is a \"real\" error that we should return.\n */\nstatic inline int err_block_err(int ret)\n{\n\tif (ret && ret != -ENOSPC && ret != -ENODATA && ret != -EAGAIN)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic ssize_t\nbsg_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct bsg_device *bd = file->private_data;\n\tint ret;\n\tssize_t bytes_read;\n\n\tdprintk(\"%s: read %Zd bytes\\n\", bd->name, count);\n\n\tbsg_set_block(bd, file);\n\n\tbytes_read = 0;\n\tret = __bsg_read(buf, count, bd, NULL, &bytes_read);\n\t*ppos = bytes_read;\n\n\tif (!bytes_read || err_block_err(ret))\n\t\tbytes_read = ret;\n\n\treturn bytes_read;\n}\n\nstatic int __bsg_write(struct bsg_device *bd, const char __user *buf,\n\t\t       size_t count, ssize_t *bytes_written,\n\t\t       fmode_t has_write_perm)\n{\n\tstruct bsg_command *bc;\n\tstruct request *rq;\n\tint ret, nr_commands;\n\n\tif (count % sizeof(struct sg_io_v4))\n\t\treturn -EINVAL;\n\n\tnr_commands = count / sizeof(struct sg_io_v4);\n\trq = NULL;\n\tbc = NULL;\n\tret = 0;\n\twhile (nr_commands) {\n\t\tstruct request_queue *q = bd->queue;\n\n\t\tbc = bsg_alloc_command(bd);\n\t\tif (IS_ERR(bc)) {\n\t\t\tret = PTR_ERR(bc);\n\t\t\tbc = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_from_user(&bc->hdr, buf, sizeof(bc->hdr))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * get a request, fill in the blanks, and add to request queue\n\t\t */\n\t\trq = bsg_map_hdr(bd, &bc->hdr, has_write_perm, bc->sense);\n\t\tif (IS_ERR(rq)) {\n\t\t\tret = PTR_ERR(rq);\n\t\t\trq = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tbsg_add_command(bd, q, bc, rq);\n\t\tbc = NULL;\n\t\trq = NULL;\n\t\tnr_commands--;\n\t\tbuf += sizeof(struct sg_io_v4);\n\t\t*bytes_written += sizeof(struct sg_io_v4);\n\t}\n\n\tif (bc)\n\t\tbsg_free_command(bc);\n\n\treturn ret;\n}\n\nstatic ssize_t\nbsg_write(struct file *file, const char __user *buf, size_t count, loff_t *ppos)\n{\n\tstruct bsg_device *bd = file->private_data;\n\tssize_t bytes_written;\n\tint ret;\n\n\tdprintk(\"%s: write %Zd bytes\\n\", bd->name, count);\n\n\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))\n\t\treturn -EINVAL;\n\n\tbsg_set_block(bd, file);\n\n\tbytes_written = 0;\n\tret = __bsg_write(bd, buf, count, &bytes_written,\n\t\t\t  file->f_mode & FMODE_WRITE);\n\n\t*ppos = bytes_written;\n\n\t/*\n\t * return bytes written on non-fatal errors\n\t */\n\tif (!bytes_written || err_block_err(ret))\n\t\tbytes_written = ret;\n\n\tdprintk(\"%s: returning %Zd\\n\", bd->name, bytes_written);\n\treturn bytes_written;\n}\n\nstatic struct bsg_device *bsg_alloc_device(void)\n{\n\tstruct bsg_device *bd;\n\n\tbd = kzalloc(sizeof(struct bsg_device), GFP_KERNEL);\n\tif (unlikely(!bd))\n\t\treturn NULL;\n\n\tspin_lock_init(&bd->lock);\n\n\tbd->max_queue = BSG_DEFAULT_CMDS;\n\n\tINIT_LIST_HEAD(&bd->busy_list);\n\tINIT_LIST_HEAD(&bd->done_list);\n\tINIT_HLIST_NODE(&bd->dev_list);\n\n\tinit_waitqueue_head(&bd->wq_free);\n\tinit_waitqueue_head(&bd->wq_done);\n\treturn bd;\n}\n\nstatic void bsg_kref_release_function(struct kref *kref)\n{\n\tstruct bsg_class_device *bcd =\n\t\tcontainer_of(kref, struct bsg_class_device, ref);\n\tstruct device *parent = bcd->parent;\n\n\tif (bcd->release)\n\t\tbcd->release(bcd->parent);\n\n\tput_device(parent);\n}\n\nstatic int bsg_put_device(struct bsg_device *bd)\n{\n\tint ret = 0, do_free;\n\tstruct request_queue *q = bd->queue;\n\n\tmutex_lock(&bsg_mutex);\n\n\tdo_free = atomic_dec_and_test(&bd->ref_count);\n\tif (!do_free) {\n\t\tmutex_unlock(&bsg_mutex);\n\t\tgoto out;\n\t}\n\n\thlist_del(&bd->dev_list);\n\tmutex_unlock(&bsg_mutex);\n\n\tdprintk(\"%s: tearing down\\n\", bd->name);\n\n\t/*\n\t * close can always block\n\t */\n\tset_bit(BSG_F_BLOCK, &bd->flags);\n\n\t/*\n\t * correct error detection baddies here again. it's the responsibility\n\t * of the app to properly reap commands before close() if it wants\n\t * fool-proof error detection\n\t */\n\tret = bsg_complete_all_commands(bd);\n\n\tkfree(bd);\nout:\n\tkref_put(&q->bsg_dev.ref, bsg_kref_release_function);\n\tif (do_free)\n\t\tblk_put_queue(q);\n\treturn ret;\n}\n\nstatic struct bsg_device *bsg_add_device(struct inode *inode,\n\t\t\t\t\t struct request_queue *rq,\n\t\t\t\t\t struct file *file)\n{\n\tstruct bsg_device *bd;\n#ifdef BSG_DEBUG\n\tunsigned char buf[32];\n#endif\n\tif (!blk_get_queue(rq))\n\t\treturn ERR_PTR(-ENXIO);\n\n\tbd = bsg_alloc_device();\n\tif (!bd) {\n\t\tblk_put_queue(rq);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tbd->queue = rq;\n\n\tbsg_set_block(bd, file);\n\n\tatomic_set(&bd->ref_count, 1);\n\tmutex_lock(&bsg_mutex);\n\thlist_add_head(&bd->dev_list, bsg_dev_idx_hash(iminor(inode)));\n\n\tstrncpy(bd->name, dev_name(rq->bsg_dev.class_dev), sizeof(bd->name) - 1);\n\tdprintk(\"bound to <%s>, max queue %d\\n\",\n\t\tformat_dev_t(buf, inode->i_rdev), bd->max_queue);\n\n\tmutex_unlock(&bsg_mutex);\n\treturn bd;\n}\n\nstatic struct bsg_device *__bsg_get_device(int minor, struct request_queue *q)\n{\n\tstruct bsg_device *bd;\n\n\tmutex_lock(&bsg_mutex);\n\n\thlist_for_each_entry(bd, bsg_dev_idx_hash(minor), dev_list) {\n\t\tif (bd->queue == q) {\n\t\t\tatomic_inc(&bd->ref_count);\n\t\t\tgoto found;\n\t\t}\n\t}\n\tbd = NULL;\nfound:\n\tmutex_unlock(&bsg_mutex);\n\treturn bd;\n}\n\nstatic struct bsg_device *bsg_get_device(struct inode *inode, struct file *file)\n{\n\tstruct bsg_device *bd;\n\tstruct bsg_class_device *bcd;\n\n\t/*\n\t * find the class device\n\t */\n\tmutex_lock(&bsg_mutex);\n\tbcd = idr_find(&bsg_minor_idr, iminor(inode));\n\tif (bcd)\n\t\tkref_get(&bcd->ref);\n\tmutex_unlock(&bsg_mutex);\n\n\tif (!bcd)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tbd = __bsg_get_device(iminor(inode), bcd->queue);\n\tif (bd)\n\t\treturn bd;\n\n\tbd = bsg_add_device(inode, bcd->queue, file);\n\tif (IS_ERR(bd))\n\t\tkref_put(&bcd->ref, bsg_kref_release_function);\n\n\treturn bd;\n}\n\nstatic int bsg_open(struct inode *inode, struct file *file)\n{\n\tstruct bsg_device *bd;\n\n\tbd = bsg_get_device(inode, file);\n\n\tif (IS_ERR(bd))\n\t\treturn PTR_ERR(bd);\n\n\tfile->private_data = bd;\n\treturn 0;\n}\n\nstatic int bsg_release(struct inode *inode, struct file *file)\n{\n\tstruct bsg_device *bd = file->private_data;\n\n\tfile->private_data = NULL;\n\treturn bsg_put_device(bd);\n}\n\nstatic unsigned int bsg_poll(struct file *file, poll_table *wait)\n{\n\tstruct bsg_device *bd = file->private_data;\n\tunsigned int mask = 0;\n\n\tpoll_wait(file, &bd->wq_done, wait);\n\tpoll_wait(file, &bd->wq_free, wait);\n\n\tspin_lock_irq(&bd->lock);\n\tif (!list_empty(&bd->done_list))\n\t\tmask |= POLLIN | POLLRDNORM;\n\tif (bd->queued_cmds < bd->max_queue)\n\t\tmask |= POLLOUT;\n\tspin_unlock_irq(&bd->lock);\n\n\treturn mask;\n}\n\nstatic long bsg_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct bsg_device *bd = file->private_data;\n\tint __user *uarg = (int __user *) arg;\n\tint ret;\n\n\tswitch (cmd) {\n\t\t/*\n\t\t * our own ioctls\n\t\t */\n\tcase SG_GET_COMMAND_Q:\n\t\treturn put_user(bd->max_queue, uarg);\n\tcase SG_SET_COMMAND_Q: {\n\t\tint queue;\n\n\t\tif (get_user(queue, uarg))\n\t\t\treturn -EFAULT;\n\t\tif (queue < 1)\n\t\t\treturn -EINVAL;\n\n\t\tspin_lock_irq(&bd->lock);\n\t\tbd->max_queue = queue;\n\t\tspin_unlock_irq(&bd->lock);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * SCSI/sg ioctls\n\t */\n\tcase SG_GET_VERSION_NUM:\n\tcase SCSI_IOCTL_GET_IDLUN:\n\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\tcase SG_SET_TIMEOUT:\n\tcase SG_GET_TIMEOUT:\n\tcase SG_GET_RESERVED_SIZE:\n\tcase SG_SET_RESERVED_SIZE:\n\tcase SG_EMULATED_HOST:\n\tcase SCSI_IOCTL_SEND_COMMAND: {\n\t\tvoid __user *uarg = (void __user *) arg;\n\t\treturn scsi_cmd_ioctl(bd->queue, NULL, file->f_mode, cmd, uarg);\n\t}\n\tcase SG_IO: {\n\t\tstruct request *rq;\n\t\tstruct bio *bio, *bidi_bio = NULL;\n\t\tstruct sg_io_v4 hdr;\n\t\tint at_head;\n\t\tu8 sense[SCSI_SENSE_BUFFERSIZE];\n\n\t\tif (copy_from_user(&hdr, uarg, sizeof(hdr)))\n\t\t\treturn -EFAULT;\n\n\t\trq = bsg_map_hdr(bd, &hdr, file->f_mode & FMODE_WRITE, sense);\n\t\tif (IS_ERR(rq))\n\t\t\treturn PTR_ERR(rq);\n\n\t\tbio = rq->bio;\n\t\tif (rq->next_rq)\n\t\t\tbidi_bio = rq->next_rq->bio;\n\n\t\tat_head = (0 == (hdr.flags & BSG_FLAG_Q_AT_TAIL));\n\t\tblk_execute_rq(bd->queue, NULL, rq, at_head);\n\t\tret = blk_complete_sgv4_hdr_rq(rq, &hdr, bio, bidi_bio);\n\n\t\tif (copy_to_user(uarg, &hdr, sizeof(hdr)))\n\t\t\treturn -EFAULT;\n\n\t\treturn ret;\n\t}\n\t/*\n\t * block device ioctls\n\t */\n\tdefault:\n#if 0\n\t\treturn ioctl_by_bdev(bd->bdev, cmd, arg);\n#else\n\t\treturn -ENOTTY;\n#endif\n\t}\n}\n\nstatic const struct file_operations bsg_fops = {\n\t.read\t\t=\tbsg_read,\n\t.write\t\t=\tbsg_write,\n\t.poll\t\t=\tbsg_poll,\n\t.open\t\t=\tbsg_open,\n\t.release\t=\tbsg_release,\n\t.unlocked_ioctl\t=\tbsg_ioctl,\n\t.owner\t\t=\tTHIS_MODULE,\n\t.llseek\t\t=\tdefault_llseek,\n};\n\nvoid bsg_unregister_queue(struct request_queue *q)\n{\n\tstruct bsg_class_device *bcd = &q->bsg_dev;\n\n\tif (!bcd->class_dev)\n\t\treturn;\n\n\tmutex_lock(&bsg_mutex);\n\tidr_remove(&bsg_minor_idr, bcd->minor);\n\tif (q->kobj.sd)\n\t\tsysfs_remove_link(&q->kobj, \"bsg\");\n\tdevice_unregister(bcd->class_dev);\n\tbcd->class_dev = NULL;\n\tkref_put(&bcd->ref, bsg_kref_release_function);\n\tmutex_unlock(&bsg_mutex);\n}\nEXPORT_SYMBOL_GPL(bsg_unregister_queue);\n\nint bsg_register_queue(struct request_queue *q, struct device *parent,\n\t\t       const char *name, void (*release)(struct device *))\n{\n\tstruct bsg_class_device *bcd;\n\tdev_t dev;\n\tint ret;\n\tstruct device *class_dev = NULL;\n\tconst char *devname;\n\n\tif (name)\n\t\tdevname = name;\n\telse\n\t\tdevname = dev_name(parent);\n\n\t/*\n\t * we need a proper transport to send commands, not a stacked device\n\t */\n\tif (!queue_is_rq_based(q))\n\t\treturn 0;\n\n\tbcd = &q->bsg_dev;\n\tmemset(bcd, 0, sizeof(*bcd));\n\n\tmutex_lock(&bsg_mutex);\n\n\tret = idr_alloc(&bsg_minor_idr, bcd, 0, BSG_MAX_DEVS, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tif (ret == -ENOSPC) {\n\t\t\tprintk(KERN_ERR \"bsg: too many bsg devices\\n\");\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tgoto unlock;\n\t}\n\n\tbcd->minor = ret;\n\tbcd->queue = q;\n\tbcd->parent = get_device(parent);\n\tbcd->release = release;\n\tkref_init(&bcd->ref);\n\tdev = MKDEV(bsg_major, bcd->minor);\n\tclass_dev = device_create(bsg_class, parent, dev, NULL, \"%s\", devname);\n\tif (IS_ERR(class_dev)) {\n\t\tret = PTR_ERR(class_dev);\n\t\tgoto put_dev;\n\t}\n\tbcd->class_dev = class_dev;\n\n\tif (q->kobj.sd) {\n\t\tret = sysfs_create_link(&q->kobj, &bcd->class_dev->kobj, \"bsg\");\n\t\tif (ret)\n\t\t\tgoto unregister_class_dev;\n\t}\n\n\tmutex_unlock(&bsg_mutex);\n\treturn 0;\n\nunregister_class_dev:\n\tdevice_unregister(class_dev);\nput_dev:\n\tput_device(parent);\n\tidr_remove(&bsg_minor_idr, bcd->minor);\nunlock:\n\tmutex_unlock(&bsg_mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(bsg_register_queue);\n\nstatic struct cdev bsg_cdev;\n\nstatic char *bsg_devnode(struct device *dev, umode_t *mode)\n{\n\treturn kasprintf(GFP_KERNEL, \"bsg/%s\", dev_name(dev));\n}\n\nstatic int __init bsg_init(void)\n{\n\tint ret, i;\n\tdev_t devid;\n\n\tbsg_cmd_cachep = kmem_cache_create(\"bsg_cmd\",\n\t\t\t\tsizeof(struct bsg_command), 0, 0, NULL);\n\tif (!bsg_cmd_cachep) {\n\t\tprintk(KERN_ERR \"bsg: failed creating slab cache\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < BSG_LIST_ARRAY_SIZE; i++)\n\t\tINIT_HLIST_HEAD(&bsg_device_list[i]);\n\n\tbsg_class = class_create(THIS_MODULE, \"bsg\");\n\tif (IS_ERR(bsg_class)) {\n\t\tret = PTR_ERR(bsg_class);\n\t\tgoto destroy_kmemcache;\n\t}\n\tbsg_class->devnode = bsg_devnode;\n\n\tret = alloc_chrdev_region(&devid, 0, BSG_MAX_DEVS, \"bsg\");\n\tif (ret)\n\t\tgoto destroy_bsg_class;\n\n\tbsg_major = MAJOR(devid);\n\n\tcdev_init(&bsg_cdev, &bsg_fops);\n\tret = cdev_add(&bsg_cdev, MKDEV(bsg_major, 0), BSG_MAX_DEVS);\n\tif (ret)\n\t\tgoto unregister_chrdev;\n\n\tprintk(KERN_INFO BSG_DESCRIPTION \" version \" BSG_VERSION\n\t       \" loaded (major %d)\\n\", bsg_major);\n\treturn 0;\nunregister_chrdev:\n\tunregister_chrdev_region(MKDEV(bsg_major, 0), BSG_MAX_DEVS);\ndestroy_bsg_class:\n\tclass_destroy(bsg_class);\ndestroy_kmemcache:\n\tkmem_cache_destroy(bsg_cmd_cachep);\n\treturn ret;\n}\n\nMODULE_AUTHOR(\"Jens Axboe\");\nMODULE_DESCRIPTION(BSG_DESCRIPTION);\nMODULE_LICENSE(\"GPL\");\n\ndevice_initcall(bsg_init);\n", "/*\n *  History:\n *  Started: Aug 9 by Lawrence Foard (entropy@world.std.com),\n *           to allow user process control of SCSI devices.\n *  Development Sponsored by Killy Corp. NY NY\n *\n * Original driver (sg.c):\n *        Copyright (C) 1992 Lawrence Foard\n * Version 2 and 3 extensions to driver:\n *        Copyright (C) 1998 - 2014 Douglas Gilbert\n *\n * This program is free software; you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation; either version 2, or (at your option)\n * any later version.\n *\n */\n\nstatic int sg_version_num = 30536;\t/* 2 digits for each component */\n#define SG_VERSION_STR \"3.5.36\"\n\n/*\n *  D. P. Gilbert (dgilbert@interlog.com), notes:\n *      - scsi logging is available via SCSI_LOG_TIMEOUT macros. First\n *        the kernel/module needs to be built with CONFIG_SCSI_LOGGING\n *        (otherwise the macros compile to empty statements).\n *\n */\n#include <linux/module.h>\n\n#include <linux/fs.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/errno.h>\n#include <linux/mtio.h>\n#include <linux/ioctl.h>\n#include <linux/slab.h>\n#include <linux/fcntl.h>\n#include <linux/init.h>\n#include <linux/poll.h>\n#include <linux/moduleparam.h>\n#include <linux/cdev.h>\n#include <linux/idr.h>\n#include <linux/seq_file.h>\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n#include <linux/blktrace_api.h>\n#include <linux/mutex.h>\n#include <linux/atomic.h>\n#include <linux/ratelimit.h>\n#include <linux/uio.h>\n\n#include \"scsi.h\"\n#include <scsi/scsi_dbg.h>\n#include <scsi/scsi_host.h>\n#include <scsi/scsi_driver.h>\n#include <scsi/scsi_ioctl.h>\n#include <scsi/sg.h>\n\n#include \"scsi_logging.h\"\n\n#ifdef CONFIG_SCSI_PROC_FS\n#include <linux/proc_fs.h>\nstatic char *sg_version_date = \"20140603\";\n\nstatic int sg_proc_init(void);\nstatic void sg_proc_cleanup(void);\n#endif\n\n#define SG_ALLOW_DIO_DEF 0\n\n#define SG_MAX_DEVS 32768\n\n/* SG_MAX_CDB_SIZE should be 260 (spc4r37 section 3.1.30) however the type\n * of sg_io_hdr::cmd_len can only represent 255. All SCSI commands greater\n * than 16 bytes are \"variable length\" whose length is a multiple of 4\n */\n#define SG_MAX_CDB_SIZE 252\n\n#define SG_DEFAULT_TIMEOUT mult_frac(SG_DEFAULT_TIMEOUT_USER, HZ, USER_HZ)\n\nint sg_big_buff = SG_DEF_RESERVED_SIZE;\n/* N.B. This variable is readable and writeable via\n   /proc/scsi/sg/def_reserved_size . Each time sg_open() is called a buffer\n   of this size (or less if there is not enough memory) will be reserved\n   for use by this file descriptor. [Deprecated usage: this variable is also\n   readable via /proc/sys/kernel/sg-big-buff if the sg driver is built into\n   the kernel (i.e. it is not a module).] */\nstatic int def_reserved_size = -1;\t/* picks up init parameter */\nstatic int sg_allow_dio = SG_ALLOW_DIO_DEF;\n\nstatic int scatter_elem_sz = SG_SCATTER_SZ;\nstatic int scatter_elem_sz_prev = SG_SCATTER_SZ;\n\n#define SG_SECTOR_SZ 512\n\nstatic int sg_add_device(struct device *, struct class_interface *);\nstatic void sg_remove_device(struct device *, struct class_interface *);\n\nstatic DEFINE_IDR(sg_index_idr);\nstatic DEFINE_RWLOCK(sg_index_lock);\t/* Also used to lock\n\t\t\t\t\t\t\t   file descriptor list for device */\n\nstatic struct class_interface sg_interface = {\n\t.add_dev        = sg_add_device,\n\t.remove_dev     = sg_remove_device,\n};\n\ntypedef struct sg_scatter_hold { /* holding area for scsi scatter gather info */\n\tunsigned short k_use_sg; /* Count of kernel scatter-gather pieces */\n\tunsigned sglist_len; /* size of malloc'd scatter-gather list ++ */\n\tunsigned bufflen;\t/* Size of (aggregate) data buffer */\n\tstruct page **pages;\n\tint page_order;\n\tchar dio_in_use;\t/* 0->indirect IO (or mmap), 1->dio */\n\tunsigned char cmd_opcode; /* first byte of command */\n} Sg_scatter_hold;\n\nstruct sg_device;\t\t/* forward declarations */\nstruct sg_fd;\n\ntypedef struct sg_request {\t/* SG_MAX_QUEUE requests outstanding per file */\n\tstruct sg_request *nextrp;\t/* NULL -> tail request (slist) */\n\tstruct sg_fd *parentfp;\t/* NULL -> not in use */\n\tSg_scatter_hold data;\t/* hold buffer, perhaps scatter list */\n\tsg_io_hdr_t header;\t/* scsi command+info, see <scsi/sg.h> */\n\tunsigned char sense_b[SCSI_SENSE_BUFFERSIZE];\n\tchar res_used;\t\t/* 1 -> using reserve buffer, 0 -> not ... */\n\tchar orphan;\t\t/* 1 -> drop on sight, 0 -> normal */\n\tchar sg_io_owned;\t/* 1 -> packet belongs to SG_IO */\n\t/* done protected by rq_list_lock */\n\tchar done;\t\t/* 0->before bh, 1->before read, 2->read */\n\tstruct request *rq;\n\tstruct bio *bio;\n\tstruct execute_work ew;\n} Sg_request;\n\ntypedef struct sg_fd {\t\t/* holds the state of a file descriptor */\n\tstruct list_head sfd_siblings;  /* protected by device's sfd_lock */\n\tstruct sg_device *parentdp;\t/* owning device */\n\twait_queue_head_t read_wait;\t/* queue read until command done */\n\trwlock_t rq_list_lock;\t/* protect access to list in req_arr */\n\tint timeout;\t\t/* defaults to SG_DEFAULT_TIMEOUT      */\n\tint timeout_user;\t/* defaults to SG_DEFAULT_TIMEOUT_USER */\n\tSg_scatter_hold reserve;\t/* buffer held for this file descriptor */\n\tunsigned save_scat_len;\t/* original length of trunc. scat. element */\n\tSg_request *headrp;\t/* head of request slist, NULL->empty */\n\tstruct fasync_struct *async_qp;\t/* used by asynchronous notification */\n\tSg_request req_arr[SG_MAX_QUEUE];\t/* used as singly-linked list */\n\tchar low_dma;\t\t/* as in parent but possibly overridden to 1 */\n\tchar force_packid;\t/* 1 -> pack_id input to read(), 0 -> ignored */\n\tchar cmd_q;\t\t/* 1 -> allow command queuing, 0 -> don't */\n\tunsigned char next_cmd_len; /* 0: automatic, >0: use on next write() */\n\tchar keep_orphan;\t/* 0 -> drop orphan (def), 1 -> keep for read() */\n\tchar mmap_called;\t/* 0 -> mmap() never called on this fd */\n\tstruct kref f_ref;\n\tstruct execute_work ew;\n} Sg_fd;\n\ntypedef struct sg_device { /* holds the state of each scsi generic device */\n\tstruct scsi_device *device;\n\twait_queue_head_t open_wait;    /* queue open() when O_EXCL present */\n\tstruct mutex open_rel_lock;     /* held when in open() or release() */\n\tint sg_tablesize;\t/* adapter's max scatter-gather table size */\n\tu32 index;\t\t/* device index number */\n\tstruct list_head sfds;\n\trwlock_t sfd_lock;      /* protect access to sfd list */\n\tatomic_t detaching;     /* 0->device usable, 1->device detaching */\n\tbool exclude;\t\t/* 1->open(O_EXCL) succeeded and is active */\n\tint open_cnt;\t\t/* count of opens (perhaps < num(sfds) ) */\n\tchar sgdebug;\t\t/* 0->off, 1->sense, 9->dump dev, 10-> all devs */\n\tstruct gendisk *disk;\n\tstruct cdev * cdev;\t/* char_dev [sysfs: /sys/cdev/major/sg<n>] */\n\tstruct kref d_ref;\n} Sg_device;\n\n/* tasklet or soft irq callback */\nstatic void sg_rq_end_io(struct request *rq, int uptodate);\nstatic int sg_start_req(Sg_request *srp, unsigned char *cmd);\nstatic int sg_finish_rem_req(Sg_request * srp);\nstatic int sg_build_indirect(Sg_scatter_hold * schp, Sg_fd * sfp, int buff_size);\nstatic ssize_t sg_new_read(Sg_fd * sfp, char __user *buf, size_t count,\n\t\t\t   Sg_request * srp);\nstatic ssize_t sg_new_write(Sg_fd *sfp, struct file *file,\n\t\t\tconst char __user *buf, size_t count, int blocking,\n\t\t\tint read_only, int sg_io_owned, Sg_request **o_srp);\nstatic int sg_common_write(Sg_fd * sfp, Sg_request * srp,\n\t\t\t   unsigned char *cmnd, int timeout, int blocking);\nstatic int sg_read_oxfer(Sg_request * srp, char __user *outp, int num_read_xfer);\nstatic void sg_remove_scat(Sg_fd * sfp, Sg_scatter_hold * schp);\nstatic void sg_build_reserve(Sg_fd * sfp, int req_size);\nstatic void sg_link_reserve(Sg_fd * sfp, Sg_request * srp, int size);\nstatic void sg_unlink_reserve(Sg_fd * sfp, Sg_request * srp);\nstatic Sg_fd *sg_add_sfp(Sg_device * sdp);\nstatic void sg_remove_sfp(struct kref *);\nstatic Sg_request *sg_get_rq_mark(Sg_fd * sfp, int pack_id);\nstatic Sg_request *sg_add_request(Sg_fd * sfp);\nstatic int sg_remove_request(Sg_fd * sfp, Sg_request * srp);\nstatic int sg_res_in_use(Sg_fd * sfp);\nstatic Sg_device *sg_get_dev(int dev);\nstatic void sg_device_destroy(struct kref *kref);\n\n#define SZ_SG_HEADER sizeof(struct sg_header)\n#define SZ_SG_IO_HDR sizeof(sg_io_hdr_t)\n#define SZ_SG_IOVEC sizeof(sg_iovec_t)\n#define SZ_SG_REQ_INFO sizeof(sg_req_info_t)\n\n#define sg_printk(prefix, sdp, fmt, a...) \\\n\tsdev_prefix_printk(prefix, (sdp)->device,\t\t\\\n\t\t\t   (sdp)->disk->disk_name, fmt, ##a)\n\nstatic int sg_allow_access(struct file *filp, unsigned char *cmd)\n{\n\tstruct sg_fd *sfp = filp->private_data;\n\n\tif (sfp->parentdp->device->type == TYPE_SCANNER)\n\t\treturn 0;\n\n\treturn blk_verify_command(cmd, filp->f_mode & FMODE_WRITE);\n}\n\nstatic int\nopen_wait(Sg_device *sdp, int flags)\n{\n\tint retval = 0;\n\n\tif (flags & O_EXCL) {\n\t\twhile (sdp->open_cnt > 0) {\n\t\t\tmutex_unlock(&sdp->open_rel_lock);\n\t\t\tretval = wait_event_interruptible(sdp->open_wait,\n\t\t\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t\t\t !sdp->open_cnt));\n\t\t\tmutex_lock(&sdp->open_rel_lock);\n\n\t\t\tif (retval) /* -ERESTARTSYS */\n\t\t\t\treturn retval;\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t}\n\t} else {\n\t\twhile (sdp->exclude) {\n\t\t\tmutex_unlock(&sdp->open_rel_lock);\n\t\t\tretval = wait_event_interruptible(sdp->open_wait,\n\t\t\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t\t\t !sdp->exclude));\n\t\t\tmutex_lock(&sdp->open_rel_lock);\n\n\t\t\tif (retval) /* -ERESTARTSYS */\n\t\t\t\treturn retval;\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t}\n\t}\n\n\treturn retval;\n}\n\n/* Returns 0 on success, else a negated errno value */\nstatic int\nsg_open(struct inode *inode, struct file *filp)\n{\n\tint dev = iminor(inode);\n\tint flags = filp->f_flags;\n\tstruct request_queue *q;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tint retval;\n\n\tnonseekable_open(inode, filp);\n\tif ((flags & O_EXCL) && (O_RDONLY == (flags & O_ACCMODE)))\n\t\treturn -EPERM; /* Can't lock it with read only access */\n\tsdp = sg_get_dev(dev);\n\tif (IS_ERR(sdp))\n\t\treturn PTR_ERR(sdp);\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_open: flags=0x%x\\n\", flags));\n\n\t/* This driver's module count bumped by fops_get in <linux/fs.h> */\n\t/* Prevent the device driver from vanishing while we sleep */\n\tretval = scsi_device_get(sdp->device);\n\tif (retval)\n\t\tgoto sg_put;\n\n\tretval = scsi_autopm_get_device(sdp->device);\n\tif (retval)\n\t\tgoto sdp_put;\n\n\t/* scsi_block_when_processing_errors() may block so bypass\n\t * check if O_NONBLOCK. Permits SCSI commands to be issued\n\t * during error recovery. Tread carefully. */\n\tif (!((flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device))) {\n\t\tretval = -ENXIO;\n\t\t/* we are in error recovery for this device */\n\t\tgoto error_out;\n\t}\n\n\tmutex_lock(&sdp->open_rel_lock);\n\tif (flags & O_NONBLOCK) {\n\t\tif (flags & O_EXCL) {\n\t\t\tif (sdp->open_cnt > 0) {\n\t\t\t\tretval = -EBUSY;\n\t\t\t\tgoto error_mutex_locked;\n\t\t\t}\n\t\t} else {\n\t\t\tif (sdp->exclude) {\n\t\t\t\tretval = -EBUSY;\n\t\t\t\tgoto error_mutex_locked;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tretval = open_wait(sdp, flags);\n\t\tif (retval) /* -ERESTARTSYS or -ENODEV */\n\t\t\tgoto error_mutex_locked;\n\t}\n\n\t/* N.B. at this point we are holding the open_rel_lock */\n\tif (flags & O_EXCL)\n\t\tsdp->exclude = true;\n\n\tif (sdp->open_cnt < 1) {  /* no existing opens */\n\t\tsdp->sgdebug = 0;\n\t\tq = sdp->device->request_queue;\n\t\tsdp->sg_tablesize = queue_max_segments(q);\n\t}\n\tsfp = sg_add_sfp(sdp);\n\tif (IS_ERR(sfp)) {\n\t\tretval = PTR_ERR(sfp);\n\t\tgoto out_undo;\n\t}\n\n\tfilp->private_data = sfp;\n\tsdp->open_cnt++;\n\tmutex_unlock(&sdp->open_rel_lock);\n\n\tretval = 0;\nsg_put:\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n\treturn retval;\n\nout_undo:\n\tif (flags & O_EXCL) {\n\t\tsdp->exclude = false;   /* undo if error */\n\t\twake_up_interruptible(&sdp->open_wait);\n\t}\nerror_mutex_locked:\n\tmutex_unlock(&sdp->open_rel_lock);\nerror_out:\n\tscsi_autopm_put_device(sdp->device);\nsdp_put:\n\tscsi_device_put(sdp->device);\n\tgoto sg_put;\n}\n\n/* Release resources associated with a successful sg_open()\n * Returns 0 on success, else a negated errno value */\nstatic int\nsg_release(struct inode *inode, struct file *filp)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp, \"sg_release\\n\"));\n\n\tmutex_lock(&sdp->open_rel_lock);\n\tscsi_autopm_put_device(sdp->device);\n\tkref_put(&sfp->f_ref, sg_remove_sfp);\n\tsdp->open_cnt--;\n\n\t/* possibly many open()s waiting on exlude clearing, start many;\n\t * only open(O_EXCL)s wait on 0==open_cnt so only start one */\n\tif (sdp->exclude) {\n\t\tsdp->exclude = false;\n\t\twake_up_interruptible_all(&sdp->open_wait);\n\t} else if (0 == sdp->open_cnt) {\n\t\twake_up_interruptible(&sdp->open_wait);\n\t}\n\tmutex_unlock(&sdp->open_rel_lock);\n\treturn 0;\n}\n\nstatic ssize_t\nsg_read(struct file *filp, char __user *buf, size_t count, loff_t * ppos)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tint req_pack_id = -1;\n\tsg_io_hdr_t *hp;\n\tstruct sg_header *old_hdr = NULL;\n\tint retval = 0;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_read: count=%d\\n\", (int) count));\n\n\tif (!access_ok(VERIFY_WRITE, buf, count))\n\t\treturn -EFAULT;\n\tif (sfp->force_packid && (count >= SZ_SG_HEADER)) {\n\t\told_hdr = kmalloc(SZ_SG_HEADER, GFP_KERNEL);\n\t\tif (!old_hdr)\n\t\t\treturn -ENOMEM;\n\t\tif (__copy_from_user(old_hdr, buf, SZ_SG_HEADER)) {\n\t\t\tretval = -EFAULT;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (old_hdr->reply_len < 0) {\n\t\t\tif (count >= SZ_SG_IO_HDR) {\n\t\t\t\tsg_io_hdr_t *new_hdr;\n\t\t\t\tnew_hdr = kmalloc(SZ_SG_IO_HDR, GFP_KERNEL);\n\t\t\t\tif (!new_hdr) {\n\t\t\t\t\tretval = -ENOMEM;\n\t\t\t\t\tgoto free_old_hdr;\n\t\t\t\t}\n\t\t\t\tretval =__copy_from_user\n\t\t\t\t    (new_hdr, buf, SZ_SG_IO_HDR);\n\t\t\t\treq_pack_id = new_hdr->pack_id;\n\t\t\t\tkfree(new_hdr);\n\t\t\t\tif (retval) {\n\t\t\t\t\tretval = -EFAULT;\n\t\t\t\t\tgoto free_old_hdr;\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\treq_pack_id = old_hdr->pack_id;\n\t}\n\tsrp = sg_get_rq_mark(sfp, req_pack_id);\n\tif (!srp) {\t\t/* now wait on packet to arrive */\n\t\tif (atomic_read(&sdp->detaching)) {\n\t\t\tretval = -ENODEV;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (filp->f_flags & O_NONBLOCK) {\n\t\t\tretval = -EAGAIN;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tretval = wait_event_interruptible(sfp->read_wait,\n\t\t\t(atomic_read(&sdp->detaching) ||\n\t\t\t(srp = sg_get_rq_mark(sfp, req_pack_id))));\n\t\tif (atomic_read(&sdp->detaching)) {\n\t\t\tretval = -ENODEV;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tif (retval) {\n\t\t\t/* -ERESTARTSYS as signal hit process */\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t}\n\tif (srp->header.interface_id != '\\0') {\n\t\tretval = sg_new_read(sfp, buf, count, srp);\n\t\tgoto free_old_hdr;\n\t}\n\n\thp = &srp->header;\n\tif (old_hdr == NULL) {\n\t\told_hdr = kmalloc(SZ_SG_HEADER, GFP_KERNEL);\n\t\tif (! old_hdr) {\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t}\n\tmemset(old_hdr, 0, SZ_SG_HEADER);\n\told_hdr->reply_len = (int) hp->timeout;\n\told_hdr->pack_len = old_hdr->reply_len; /* old, strange behaviour */\n\told_hdr->pack_id = hp->pack_id;\n\told_hdr->twelve_byte =\n\t    ((srp->data.cmd_opcode >= 0xc0) && (12 == hp->cmd_len)) ? 1 : 0;\n\told_hdr->target_status = hp->masked_status;\n\told_hdr->host_status = hp->host_status;\n\told_hdr->driver_status = hp->driver_status;\n\tif ((CHECK_CONDITION & hp->masked_status) ||\n\t    (DRIVER_SENSE & hp->driver_status))\n\t\tmemcpy(old_hdr->sense_buffer, srp->sense_b,\n\t\t       sizeof (old_hdr->sense_buffer));\n\tswitch (hp->host_status) {\n\t/* This setup of 'result' is for backward compatibility and is best\n\t   ignored by the user who should use target, host + driver status */\n\tcase DID_OK:\n\tcase DID_PASSTHROUGH:\n\tcase DID_SOFT_ERROR:\n\t\told_hdr->result = 0;\n\t\tbreak;\n\tcase DID_NO_CONNECT:\n\tcase DID_BUS_BUSY:\n\tcase DID_TIME_OUT:\n\t\told_hdr->result = EBUSY;\n\t\tbreak;\n\tcase DID_BAD_TARGET:\n\tcase DID_ABORT:\n\tcase DID_PARITY:\n\tcase DID_RESET:\n\tcase DID_BAD_INTR:\n\t\told_hdr->result = EIO;\n\t\tbreak;\n\tcase DID_ERROR:\n\t\told_hdr->result = (srp->sense_b[0] == 0 && \n\t\t\t\t  hp->masked_status == GOOD) ? 0 : EIO;\n\t\tbreak;\n\tdefault:\n\t\told_hdr->result = EIO;\n\t\tbreak;\n\t}\n\n\t/* Now copy the result back to the user buffer.  */\n\tif (count >= SZ_SG_HEADER) {\n\t\tif (__copy_to_user(buf, old_hdr, SZ_SG_HEADER)) {\n\t\t\tretval = -EFAULT;\n\t\t\tgoto free_old_hdr;\n\t\t}\n\t\tbuf += SZ_SG_HEADER;\n\t\tif (count > old_hdr->reply_len)\n\t\t\tcount = old_hdr->reply_len;\n\t\tif (count > SZ_SG_HEADER) {\n\t\t\tif (sg_read_oxfer(srp, buf, count - SZ_SG_HEADER)) {\n\t\t\t\tretval = -EFAULT;\n\t\t\t\tgoto free_old_hdr;\n\t\t\t}\n\t\t}\n\t} else\n\t\tcount = (old_hdr->result == 0) ? 0 : -EIO;\n\tsg_finish_rem_req(srp);\n\tretval = count;\nfree_old_hdr:\n\tkfree(old_hdr);\n\treturn retval;\n}\n\nstatic ssize_t\nsg_new_read(Sg_fd * sfp, char __user *buf, size_t count, Sg_request * srp)\n{\n\tsg_io_hdr_t *hp = &srp->header;\n\tint err = 0, err2;\n\tint len;\n\n\tif (count < SZ_SG_IO_HDR) {\n\t\terr = -EINVAL;\n\t\tgoto err_out;\n\t}\n\thp->sb_len_wr = 0;\n\tif ((hp->mx_sb_len > 0) && hp->sbp) {\n\t\tif ((CHECK_CONDITION & hp->masked_status) ||\n\t\t    (DRIVER_SENSE & hp->driver_status)) {\n\t\t\tint sb_len = SCSI_SENSE_BUFFERSIZE;\n\t\t\tsb_len = (hp->mx_sb_len > sb_len) ? sb_len : hp->mx_sb_len;\n\t\t\tlen = 8 + (int) srp->sense_b[7];\t/* Additional sense length field */\n\t\t\tlen = (len > sb_len) ? sb_len : len;\n\t\t\tif (copy_to_user(hp->sbp, srp->sense_b, len)) {\n\t\t\t\terr = -EFAULT;\n\t\t\t\tgoto err_out;\n\t\t\t}\n\t\t\thp->sb_len_wr = len;\n\t\t}\n\t}\n\tif (hp->masked_status || hp->host_status || hp->driver_status)\n\t\thp->info |= SG_INFO_CHECK;\n\tif (copy_to_user(buf, hp, SZ_SG_IO_HDR)) {\n\t\terr = -EFAULT;\n\t\tgoto err_out;\n\t}\nerr_out:\n\terr2 = sg_finish_rem_req(srp);\n\treturn err ? : err2 ? : count;\n}\n\nstatic ssize_t\nsg_write(struct file *filp, const char __user *buf, size_t count, loff_t * ppos)\n{\n\tint mxsize, cmd_size, k;\n\tint input_size, blocking;\n\tunsigned char opcode;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tstruct sg_header old_hdr;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\n\tif (unlikely(segment_eq(get_fs(), KERNEL_DS)))\n\t\treturn -EINVAL;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_write: count=%d\\n\", (int) count));\n\tif (atomic_read(&sdp->detaching))\n\t\treturn -ENODEV;\n\tif (!((filp->f_flags & O_NONBLOCK) ||\n\t      scsi_block_when_processing_errors(sdp->device)))\n\t\treturn -ENXIO;\n\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\tif (count < SZ_SG_HEADER)\n\t\treturn -EIO;\n\tif (__copy_from_user(&old_hdr, buf, SZ_SG_HEADER))\n\t\treturn -EFAULT;\n\tblocking = !(filp->f_flags & O_NONBLOCK);\n\tif (old_hdr.reply_len < 0)\n\t\treturn sg_new_write(sfp, filp, buf, count,\n\t\t\t\t    blocking, 0, 0, NULL);\n\tif (count < (SZ_SG_HEADER + 6))\n\t\treturn -EIO;\t/* The minimum scsi command length is 6 bytes. */\n\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\t      \"sg_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tbuf += SZ_SG_HEADER;\n\t__get_user(opcode, buf);\n\tif (sfp->next_cmd_len > 0) {\n\t\tcmd_size = sfp->next_cmd_len;\n\t\tsfp->next_cmd_len = 0;\t/* reset so only this write() effected */\n\t} else {\n\t\tcmd_size = COMMAND_SIZE(opcode);\t/* based on SCSI command group */\n\t\tif ((opcode >= 0xc0) && old_hdr.twelve_byte)\n\t\t\tcmd_size = 12;\n\t}\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\"sg_write:   scsi opcode=0x%02x, cmd_size=%d\\n\", (int) opcode, cmd_size));\n/* Determine buffer size.  */\n\tinput_size = count - cmd_size;\n\tmxsize = (input_size > old_hdr.reply_len) ? input_size : old_hdr.reply_len;\n\tmxsize -= SZ_SG_HEADER;\n\tinput_size -= SZ_SG_HEADER;\n\tif (input_size < 0) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EIO;\t/* User did not pass enough bytes for this command. */\n\t}\n\thp = &srp->header;\n\thp->interface_id = '\\0';\t/* indicator of old interface tunnelled */\n\thp->cmd_len = (unsigned char) cmd_size;\n\thp->iovec_count = 0;\n\thp->mx_sb_len = 0;\n\tif (input_size > 0)\n\t\thp->dxfer_direction = (old_hdr.reply_len > SZ_SG_HEADER) ?\n\t\t    SG_DXFER_TO_FROM_DEV : SG_DXFER_TO_DEV;\n\telse\n\t\thp->dxfer_direction = (mxsize > 0) ? SG_DXFER_FROM_DEV : SG_DXFER_NONE;\n\thp->dxfer_len = mxsize;\n\tif ((hp->dxfer_direction == SG_DXFER_TO_DEV) ||\n\t    (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV))\n\t\thp->dxferp = (char __user *)buf + cmd_size;\n\telse\n\t\thp->dxferp = NULL;\n\thp->sbp = NULL;\n\thp->timeout = old_hdr.reply_len;\t/* structure abuse ... */\n\thp->flags = input_size;\t/* structure abuse ... */\n\thp->pack_id = old_hdr.pack_id;\n\thp->usr_ptr = NULL;\n\tif (__copy_from_user(cmnd, buf, cmd_size))\n\t\treturn -EFAULT;\n\t/*\n\t * SG_DXFER_TO_FROM_DEV is functionally equivalent to SG_DXFER_FROM_DEV,\n\t * but is is possible that the app intended SG_DXFER_TO_DEV, because there\n\t * is a non-zero input_size, so emit a warning.\n\t */\n\tif (hp->dxfer_direction == SG_DXFER_TO_FROM_DEV) {\n\t\tstatic char cmd[TASK_COMM_LEN];\n\t\tif (strcmp(current->comm, cmd)) {\n\t\t\tprintk_ratelimited(KERN_WARNING\n\t\t\t\t\t   \"sg_write: data in/out %d/%d bytes \"\n\t\t\t\t\t   \"for SCSI command 0x%x-- guessing \"\n\t\t\t\t\t   \"data in;\\n   program %s not setting \"\n\t\t\t\t\t   \"count and/or reply_len properly\\n\",\n\t\t\t\t\t   old_hdr.reply_len - (int)SZ_SG_HEADER,\n\t\t\t\t\t   input_size, (unsigned int) cmnd[0],\n\t\t\t\t\t   current->comm);\n\t\t\tstrcpy(cmd, current->comm);\n\t\t}\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, sfp->timeout, blocking);\n\treturn (k < 0) ? k : count;\n}\n\nstatic ssize_t\nsg_new_write(Sg_fd *sfp, struct file *file, const char __user *buf,\n\t\t size_t count, int blocking, int read_only, int sg_io_owned,\n\t\t Sg_request **o_srp)\n{\n\tint k;\n\tSg_request *srp;\n\tsg_io_hdr_t *hp;\n\tunsigned char cmnd[SG_MAX_CDB_SIZE];\n\tint timeout;\n\tunsigned long ul_timeout;\n\n\tif (count < SZ_SG_IO_HDR)\n\t\treturn -EINVAL;\n\tif (!access_ok(VERIFY_READ, buf, count))\n\t\treturn -EFAULT; /* protects following copy_from_user()s + get_user()s */\n\n\tsfp->cmd_q = 1;\t/* when sg_io_hdr seen, set command queuing on */\n\tif (!(srp = sg_add_request(sfp))) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t\t      \"sg_new_write: queue full\\n\"));\n\t\treturn -EDOM;\n\t}\n\tsrp->sg_io_owned = sg_io_owned;\n\thp = &srp->header;\n\tif (__copy_from_user(hp, buf, SZ_SG_IO_HDR)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\n\t}\n\tif (hp->interface_id != 'S') {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -ENOSYS;\n\t}\n\tif (hp->flags & SG_FLAG_MMAP_IO) {\n\t\tif (hp->dxfer_len > sfp->reserve.bufflen) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -ENOMEM;\t/* MMAP_IO size must fit in reserve buffer */\n\t\t}\n\t\tif (hp->flags & SG_FLAG_DIRECT_IO) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -EINVAL;\t/* either MMAP_IO or DIRECT_IO (not both) */\n\t\t}\n\t\tif (sg_res_in_use(sfp)) {\n\t\t\tsg_remove_request(sfp, srp);\n\t\t\treturn -EBUSY;\t/* reserve buffer already being used */\n\t\t}\n\t}\n\tul_timeout = msecs_to_jiffies(srp->header.timeout);\n\ttimeout = (ul_timeout < INT_MAX) ? ul_timeout : INT_MAX;\n\tif ((!hp->cmdp) || (hp->cmd_len < 6) || (hp->cmd_len > sizeof (cmnd))) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EMSGSIZE;\n\t}\n\tif (!access_ok(VERIFY_READ, hp->cmdp, hp->cmd_len)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\t/* protects following copy_from_user()s + get_user()s */\n\t}\n\tif (__copy_from_user(cmnd, hp->cmdp, hp->cmd_len)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EFAULT;\n\t}\n\tif (read_only && sg_allow_access(file, cmnd)) {\n\t\tsg_remove_request(sfp, srp);\n\t\treturn -EPERM;\n\t}\n\tk = sg_common_write(sfp, srp, cmnd, timeout, blocking);\n\tif (k < 0)\n\t\treturn k;\n\tif (o_srp)\n\t\t*o_srp = srp;\n\treturn count;\n}\n\nstatic int\nsg_common_write(Sg_fd * sfp, Sg_request * srp,\n\t\tunsigned char *cmnd, int timeout, int blocking)\n{\n\tint k, at_head;\n\tSg_device *sdp = sfp->parentdp;\n\tsg_io_hdr_t *hp = &srp->header;\n\n\tsrp->data.cmd_opcode = cmnd[0];\t/* hold opcode of command */\n\thp->status = 0;\n\thp->masked_status = 0;\n\thp->msg_status = 0;\n\thp->info = 0;\n\thp->host_status = 0;\n\thp->driver_status = 0;\n\thp->resid = 0;\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\"sg_common_write:  scsi opcode=0x%02x, cmd_size=%d\\n\",\n\t\t\t(int) cmnd[0], (int) hp->cmd_len));\n\n\tk = sg_start_req(srp, cmnd);\n\tif (k) {\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\"sg_common_write: start_req err=%d\\n\", k));\n\t\tsg_finish_rem_req(srp);\n\t\treturn k;\t/* probably out of space --> ENOMEM */\n\t}\n\tif (atomic_read(&sdp->detaching)) {\n\t\tif (srp->bio) {\n\t\t\tif (srp->rq->cmd != srp->rq->__cmd)\n\t\t\t\tkfree(srp->rq->cmd);\n\n\t\t\tblk_end_request_all(srp->rq, -EIO);\n\t\t\tsrp->rq = NULL;\n\t\t}\n\n\t\tsg_finish_rem_req(srp);\n\t\treturn -ENODEV;\n\t}\n\n\thp->duration = jiffies_to_msecs(jiffies);\n\tif (hp->interface_id != '\\0' &&\t/* v3 (or later) interface */\n\t    (SG_FLAG_Q_AT_TAIL & hp->flags))\n\t\tat_head = 0;\n\telse\n\t\tat_head = 1;\n\n\tsrp->rq->timeout = timeout;\n\tkref_get(&sfp->f_ref); /* sg_rq_end_io() does kref_put(). */\n\tblk_execute_rq_nowait(sdp->device->request_queue, sdp->disk,\n\t\t\t      srp->rq, at_head, sg_rq_end_io);\n\treturn 0;\n}\n\nstatic int srp_done(Sg_fd *sfp, Sg_request *srp)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tread_lock_irqsave(&sfp->rq_list_lock, flags);\n\tret = srp->done;\n\tread_unlock_irqrestore(&sfp->rq_list_lock, flags);\n\treturn ret;\n}\n\nstatic int max_sectors_bytes(struct request_queue *q)\n{\n\tunsigned int max_sectors = queue_max_sectors(q);\n\n\tmax_sectors = min_t(unsigned int, max_sectors, INT_MAX >> 9);\n\n\treturn max_sectors << 9;\n}\n\nstatic long\nsg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)\n{\n\tvoid __user *p = (void __user *)arg;\n\tint __user *ip = p;\n\tint result, val, read_only;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tunsigned long iflags;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t   \"sg_ioctl: cmd=0x%x\\n\", (int) cmd_in));\n\tread_only = (O_RDWR != (filp->f_flags & O_ACCMODE));\n\n\tswitch (cmd_in) {\n\tcase SG_IO:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (!scsi_block_when_processing_errors(sdp->device))\n\t\t\treturn -ENXIO;\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_IO_HDR))\n\t\t\treturn -EFAULT;\n\t\tresult = sg_new_write(sfp, filp, p, SZ_SG_IO_HDR,\n\t\t\t\t 1, read_only, 1, &srp);\n\t\tif (result < 0)\n\t\t\treturn result;\n\t\tresult = wait_event_interruptible(sfp->read_wait,\n\t\t\t(srp_done(sfp, srp) || atomic_read(&sdp->detaching)));\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\twrite_lock_irq(&sfp->rq_list_lock);\n\t\tif (srp->done) {\n\t\t\tsrp->done = 2;\n\t\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\t\tresult = sg_new_read(sfp, p, SZ_SG_IO_HDR, srp);\n\t\t\treturn (result < 0) ? result : 0;\n\t\t}\n\t\tsrp->orphan = 1;\n\t\twrite_unlock_irq(&sfp->rq_list_lock);\n\t\treturn result;\t/* -ERESTARTSYS because signal hit process */\n\tcase SG_SET_TIMEOUT:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val < 0)\n\t\t\treturn -EIO;\n\t\tif (val >= mult_frac((s64)INT_MAX, USER_HZ, HZ))\n\t\t\tval = min_t(s64, mult_frac((s64)INT_MAX, USER_HZ, HZ),\n\t\t\t\t    INT_MAX);\n\t\tsfp->timeout_user = val;\n\t\tsfp->timeout = mult_frac(val, HZ, USER_HZ);\n\n\t\treturn 0;\n\tcase SG_GET_TIMEOUT:\t/* N.B. User receives timeout as return value */\n\t\t\t\t/* strange ..., for backward compatibility */\n\t\treturn sfp->timeout_user;\n\tcase SG_SET_FORCE_LOW_DMA:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tif (val) {\n\t\t\tsfp->low_dma = 1;\n\t\t\tif ((0 == sfp->low_dma) && (0 == sg_res_in_use(sfp))) {\n\t\t\t\tval = (int) sfp->reserve.bufflen;\n\t\t\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t\t\t\tsg_build_reserve(sfp, val);\n\t\t\t}\n\t\t} else {\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t\tsfp->low_dma = sdp->device->host->unchecked_isa_dma;\n\t\t}\n\t\treturn 0;\n\tcase SG_GET_LOW_DMA:\n\t\treturn put_user((int) sfp->low_dma, ip);\n\tcase SG_GET_SCSI_ID:\n\t\tif (!access_ok(VERIFY_WRITE, p, sizeof (sg_scsi_id_t)))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_scsi_id_t __user *sg_idp = p;\n\n\t\t\tif (atomic_read(&sdp->detaching))\n\t\t\t\treturn -ENODEV;\n\t\t\t__put_user((int) sdp->device->host->host_no,\n\t\t\t\t   &sg_idp->host_no);\n\t\t\t__put_user((int) sdp->device->channel,\n\t\t\t\t   &sg_idp->channel);\n\t\t\t__put_user((int) sdp->device->id, &sg_idp->scsi_id);\n\t\t\t__put_user((int) sdp->device->lun, &sg_idp->lun);\n\t\t\t__put_user((int) sdp->device->type, &sg_idp->scsi_type);\n\t\t\t__put_user((short) sdp->device->host->cmd_per_lun,\n\t\t\t\t   &sg_idp->h_cmd_per_lun);\n\t\t\t__put_user((short) sdp->device->queue_depth,\n\t\t\t\t   &sg_idp->d_queue_depth);\n\t\t\t__put_user(0, &sg_idp->unused[0]);\n\t\t\t__put_user(0, &sg_idp->unused[1]);\n\t\t\treturn 0;\n\t\t}\n\tcase SG_SET_FORCE_PACK_ID:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->force_packid = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_PACK_ID:\n\t\tif (!access_ok(VERIFY_WRITE, ip, sizeof (int)))\n\t\t\treturn -EFAULT;\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tfor (srp = sfp->headrp; srp; srp = srp->nextrp) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned)) {\n\t\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock,\n\t\t\t\t\t\t       iflags);\n\t\t\t\t__put_user(srp->header.pack_id, ip);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t__put_user(-1, ip);\n\t\treturn 0;\n\tcase SG_GET_NUM_WAITING:\n\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\tfor (val = 0, srp = sfp->headrp; srp; srp = srp->nextrp) {\n\t\t\tif ((1 == srp->done) && (!srp->sg_io_owned))\n\t\t\t\t++val;\n\t\t}\n\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_SG_TABLESIZE:\n\t\treturn put_user(sdp->sg_tablesize, ip);\n\tcase SG_SET_RESERVED_SIZE:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n                if (val < 0)\n                        return -EINVAL;\n\t\tval = min_t(int, val,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\tif (val != sfp->reserve.bufflen) {\n\t\t\tif (sg_res_in_use(sfp) || sfp->mmap_called)\n\t\t\t\treturn -EBUSY;\n\t\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t\t\tsg_build_reserve(sfp, val);\n\t\t}\n\t\treturn 0;\n\tcase SG_GET_RESERVED_SIZE:\n\t\tval = min_t(int, sfp->reserve.bufflen,\n\t\t\t    max_sectors_bytes(sdp->device->request_queue));\n\t\treturn put_user(val, ip);\n\tcase SG_SET_COMMAND_Q:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->cmd_q = val ? 1 : 0;\n\t\treturn 0;\n\tcase SG_GET_COMMAND_Q:\n\t\treturn put_user((int) sfp->cmd_q, ip);\n\tcase SG_SET_KEEP_ORPHAN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->keep_orphan = val;\n\t\treturn 0;\n\tcase SG_GET_KEEP_ORPHAN:\n\t\treturn put_user((int) sfp->keep_orphan, ip);\n\tcase SG_NEXT_CMD_LEN:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsfp->next_cmd_len = (val > 0) ? val : 0;\n\t\treturn 0;\n\tcase SG_GET_VERSION_NUM:\n\t\treturn put_user(sg_version_num, ip);\n\tcase SG_GET_ACCESS_COUNT:\n\t\t/* faked - we don't have a real access count anymore */\n\t\tval = (sdp->device ? 1 : 0);\n\t\treturn put_user(val, ip);\n\tcase SG_GET_REQUEST_TABLE:\n\t\tif (!access_ok(VERIFY_WRITE, p, SZ_SG_REQ_INFO * SG_MAX_QUEUE))\n\t\t\treturn -EFAULT;\n\t\telse {\n\t\t\tsg_req_info_t *rinfo;\n\t\t\tunsigned int ms;\n\n\t\t\trinfo = kmalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!rinfo)\n\t\t\t\treturn -ENOMEM;\n\t\t\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\t\t\tfor (srp = sfp->headrp, val = 0; val < SG_MAX_QUEUE;\n\t\t\t     ++val, srp = srp ? srp->nextrp : srp) {\n\t\t\t\tmemset(&rinfo[val], 0, SZ_SG_REQ_INFO);\n\t\t\t\tif (srp) {\n\t\t\t\t\trinfo[val].req_state = srp->done + 1;\n\t\t\t\t\trinfo[val].problem =\n\t\t\t\t\t    srp->header.masked_status & \n\t\t\t\t\t    srp->header.host_status & \n\t\t\t\t\t    srp->header.driver_status;\n\t\t\t\t\tif (srp->done)\n\t\t\t\t\t\trinfo[val].duration =\n\t\t\t\t\t\t\tsrp->header.duration;\n\t\t\t\t\telse {\n\t\t\t\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\t\t\t\trinfo[val].duration =\n\t\t\t\t\t\t    (ms > srp->header.duration) ?\n\t\t\t\t\t\t    (ms - srp->header.duration) : 0;\n\t\t\t\t\t}\n\t\t\t\t\trinfo[val].orphan = srp->orphan;\n\t\t\t\t\trinfo[val].sg_io_owned =\n\t\t\t\t\t\t\tsrp->sg_io_owned;\n\t\t\t\t\trinfo[val].pack_id =\n\t\t\t\t\t\t\tsrp->header.pack_id;\n\t\t\t\t\trinfo[val].usr_ptr =\n\t\t\t\t\t\t\tsrp->header.usr_ptr;\n\t\t\t\t}\n\t\t\t}\n\t\t\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\t\t\tresult = __copy_to_user(p, rinfo, \n\t\t\t\t\t\tSZ_SG_REQ_INFO * SG_MAX_QUEUE);\n\t\t\tresult = result ? -EFAULT : 0;\n\t\t\tkfree(rinfo);\n\t\t\treturn result;\n\t\t}\n\tcase SG_EMULATED_HOST:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\treturn put_user(sdp->device->host->hostt->emulated, ip);\n\tcase SCSI_IOCTL_SEND_COMMAND:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tif (read_only) {\n\t\t\tunsigned char opcode = WRITE_6;\n\t\t\tScsi_Ioctl_Command __user *siocp = p;\n\n\t\t\tif (copy_from_user(&opcode, siocp->data, 1))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (sg_allow_access(filp, &opcode))\n\t\t\t\treturn -EPERM;\n\t\t}\n\t\treturn sg_scsi_ioctl(sdp->device->request_queue, NULL, filp->f_mode, p);\n\tcase SG_SET_DEBUG:\n\t\tresult = get_user(val, ip);\n\t\tif (result)\n\t\t\treturn result;\n\t\tsdp->sgdebug = (char) val;\n\t\treturn 0;\n\tcase BLKSECTGET:\n\t\treturn put_user(max_sectors_bytes(sdp->device->request_queue),\n\t\t\t\tip);\n\tcase BLKTRACESETUP:\n\t\treturn blk_trace_setup(sdp->device->request_queue,\n\t\t\t\t       sdp->disk->disk_name,\n\t\t\t\t       MKDEV(SCSI_GENERIC_MAJOR, sdp->index),\n\t\t\t\t       NULL,\n\t\t\t\t       (char *)arg);\n\tcase BLKTRACESTART:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 1);\n\tcase BLKTRACESTOP:\n\t\treturn blk_trace_startstop(sdp->device->request_queue, 0);\n\tcase BLKTRACETEARDOWN:\n\t\treturn blk_trace_remove(sdp->device->request_queue);\n\tcase SCSI_IOCTL_GET_IDLUN:\n\tcase SCSI_IOCTL_GET_BUS_NUMBER:\n\tcase SCSI_IOCTL_PROBE_HOST:\n\tcase SG_GET_TRANSFORM:\n\tcase SG_SCSI_RESET:\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\treturn -ENODEV;\n\t\tbreak;\n\tdefault:\n\t\tif (read_only)\n\t\t\treturn -EPERM;\t/* don't know so take safe approach */\n\t\tbreak;\n\t}\n\n\tresult = scsi_ioctl_block_when_processing_errors(sdp->device,\n\t\t\tcmd_in, filp->f_flags & O_NDELAY);\n\tif (result)\n\t\treturn result;\n\treturn scsi_ioctl(sdp->device, cmd_in, p);\n}\n\n#ifdef CONFIG_COMPAT\nstatic long sg_compat_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tstruct scsi_device *sdev;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\n\tsdev = sdp->device;\n\tif (sdev->host->hostt->compat_ioctl) { \n\t\tint ret;\n\n\t\tret = sdev->host->hostt->compat_ioctl(sdev, cmd_in, (void __user *)arg);\n\n\t\treturn ret;\n\t}\n\t\n\treturn -ENOIOCTLCMD;\n}\n#endif\n\nstatic unsigned int\nsg_poll(struct file *filp, poll_table * wait)\n{\n\tunsigned int res = 0;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tSg_request *srp;\n\tint count = 0;\n\tunsigned long iflags;\n\n\tsfp = filp->private_data;\n\tif (!sfp)\n\t\treturn POLLERR;\n\tsdp = sfp->parentdp;\n\tif (!sdp)\n\t\treturn POLLERR;\n\tpoll_wait(filp, &sfp->read_wait, wait);\n\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tfor (srp = sfp->headrp; srp; srp = srp->nextrp) {\n\t\t/* if any read waiting, flag it */\n\t\tif ((0 == res) && (1 == srp->done) && (!srp->sg_io_owned))\n\t\t\tres = POLLIN | POLLRDNORM;\n\t\t++count;\n\t}\n\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\n\tif (atomic_read(&sdp->detaching))\n\t\tres |= POLLHUP;\n\telse if (!sfp->cmd_q) {\n\t\tif (0 == count)\n\t\t\tres |= POLLOUT | POLLWRNORM;\n\t} else if (count < SG_MAX_QUEUE)\n\t\tres |= POLLOUT | POLLWRNORM;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_poll: res=0x%x\\n\", (int) res));\n\treturn res;\n}\n\nstatic int\nsg_fasync(int fd, struct file *filp, int mode)\n{\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\n\tif ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))\n\t\treturn -ENXIO;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_fasync: mode=%d\\n\", mode));\n\n\treturn fasync_helper(fd, filp, mode, &sfp->async_qp);\n}\n\nstatic int\nsg_vma_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tSg_fd *sfp;\n\tunsigned long offset, len, sa;\n\tSg_scatter_hold *rsv_schp;\n\tint k, length;\n\n\tif ((NULL == vma) || (!(sfp = (Sg_fd *) vma->vm_private_data)))\n\t\treturn VM_FAULT_SIGBUS;\n\trsv_schp = &sfp->reserve;\n\toffset = vmf->pgoff << PAGE_SHIFT;\n\tif (offset >= rsv_schp->bufflen)\n\t\treturn VM_FAULT_SIGBUS;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_vma_fault: offset=%lu, scatg=%d\\n\",\n\t\t\t\t      offset, rsv_schp->k_use_sg));\n\tsa = vma->vm_start;\n\tlength = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg && sa < vma->vm_end; k++) {\n\t\tlen = vma->vm_end - sa;\n\t\tlen = (len < length) ? len : length;\n\t\tif (offset < len) {\n\t\t\tstruct page *page = nth_page(rsv_schp->pages[k],\n\t\t\t\t\t\t     offset >> PAGE_SHIFT);\n\t\t\tget_page(page);\t/* increment page count */\n\t\t\tvmf->page = page;\n\t\t\treturn 0; /* success */\n\t\t}\n\t\tsa += len;\n\t\toffset -= len;\n\t}\n\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic const struct vm_operations_struct sg_mmap_vm_ops = {\n\t.fault = sg_vma_fault,\n};\n\nstatic int\nsg_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tSg_fd *sfp;\n\tunsigned long req_sz, len, sa;\n\tSg_scatter_hold *rsv_schp;\n\tint k, length;\n\n\tif ((!filp) || (!vma) || (!(sfp = (Sg_fd *) filp->private_data)))\n\t\treturn -ENXIO;\n\treq_sz = vma->vm_end - vma->vm_start;\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_mmap starting, vm_start=%p, len=%d\\n\",\n\t\t\t\t      (void *) vma->vm_start, (int) req_sz));\n\tif (vma->vm_pgoff)\n\t\treturn -EINVAL;\t/* want no offset */\n\trsv_schp = &sfp->reserve;\n\tif (req_sz > rsv_schp->bufflen)\n\t\treturn -ENOMEM;\t/* cannot map more than reserved buffer */\n\n\tsa = vma->vm_start;\n\tlength = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg && sa < vma->vm_end; k++) {\n\t\tlen = vma->vm_end - sa;\n\t\tlen = (len < length) ? len : length;\n\t\tsa += len;\n\t}\n\n\tsfp->mmap_called = 1;\n\tvma->vm_flags |= VM_IO | VM_DONTEXPAND | VM_DONTDUMP;\n\tvma->vm_private_data = sfp;\n\tvma->vm_ops = &sg_mmap_vm_ops;\n\treturn 0;\n}\n\nstatic void\nsg_rq_end_io_usercontext(struct work_struct *work)\n{\n\tstruct sg_request *srp = container_of(work, struct sg_request, ew.work);\n\tstruct sg_fd *sfp = srp->parentfp;\n\n\tsg_finish_rem_req(srp);\n\tkref_put(&sfp->f_ref, sg_remove_sfp);\n}\n\n/*\n * This function is a \"bottom half\" handler that is called by the mid\n * level when a command is completed (or has failed).\n */\nstatic void\nsg_rq_end_io(struct request *rq, int uptodate)\n{\n\tstruct sg_request *srp = rq->end_io_data;\n\tSg_device *sdp;\n\tSg_fd *sfp;\n\tunsigned long iflags;\n\tunsigned int ms;\n\tchar *sense;\n\tint result, resid, done = 1;\n\n\tif (WARN_ON(srp->done != 0))\n\t\treturn;\n\n\tsfp = srp->parentfp;\n\tif (WARN_ON(sfp == NULL))\n\t\treturn;\n\n\tsdp = sfp->parentdp;\n\tif (unlikely(atomic_read(&sdp->detaching)))\n\t\tpr_info(\"%s: device detaching\\n\", __func__);\n\n\tsense = rq->sense;\n\tresult = rq->errors;\n\tresid = rq->resid_len;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_cmd_done: pack_id=%d, res=0x%x\\n\",\n\t\t\t\t      srp->header.pack_id, result));\n\tsrp->header.resid = resid;\n\tms = jiffies_to_msecs(jiffies);\n\tsrp->header.duration = (ms > srp->header.duration) ?\n\t\t\t\t(ms - srp->header.duration) : 0;\n\tif (0 != result) {\n\t\tstruct scsi_sense_hdr sshdr;\n\n\t\tsrp->header.status = 0xff & result;\n\t\tsrp->header.masked_status = status_byte(result);\n\t\tsrp->header.msg_status = msg_byte(result);\n\t\tsrp->header.host_status = host_byte(result);\n\t\tsrp->header.driver_status = driver_byte(result);\n\t\tif ((sdp->sgdebug > 0) &&\n\t\t    ((CHECK_CONDITION == srp->header.masked_status) ||\n\t\t     (COMMAND_TERMINATED == srp->header.masked_status)))\n\t\t\t__scsi_print_sense(sdp->device, __func__, sense,\n\t\t\t\t\t   SCSI_SENSE_BUFFERSIZE);\n\n\t\t/* Following if statement is a patch supplied by Eric Youngdale */\n\t\tif (driver_byte(result) != 0\n\t\t    && scsi_normalize_sense(sense, SCSI_SENSE_BUFFERSIZE, &sshdr)\n\t\t    && !scsi_sense_is_deferred(&sshdr)\n\t\t    && sshdr.sense_key == UNIT_ATTENTION\n\t\t    && sdp->device->removable) {\n\t\t\t/* Detected possible disc change. Set the bit - this */\n\t\t\t/* may be used if there are filesystems using this device */\n\t\t\tsdp->device->changed = 1;\n\t\t}\n\t}\n\t/* Rely on write phase to clean out srp status values, so no \"else\" */\n\n\t/*\n\t * Free the request as soon as it is complete so that its resources\n\t * can be reused without waiting for userspace to read() the\n\t * result.  But keep the associated bio (if any) around until\n\t * blk_rq_unmap_user() can be called from user context.\n\t */\n\tsrp->rq = NULL;\n\tif (rq->cmd != rq->__cmd)\n\t\tkfree(rq->cmd);\n\t__blk_put_request(rq->q, rq);\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tif (unlikely(srp->orphan)) {\n\t\tif (sfp->keep_orphan)\n\t\t\tsrp->sg_io_owned = 0;\n\t\telse\n\t\t\tdone = 0;\n\t}\n\tsrp->done = done;\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\n\tif (likely(done)) {\n\t\t/* Now wake up any sg_read() that is waiting for this\n\t\t * packet.\n\t\t */\n\t\twake_up_interruptible(&sfp->read_wait);\n\t\tkill_fasync(&sfp->async_qp, SIGPOLL, POLL_IN);\n\t\tkref_put(&sfp->f_ref, sg_remove_sfp);\n\t} else {\n\t\tINIT_WORK(&srp->ew.work, sg_rq_end_io_usercontext);\n\t\tschedule_work(&srp->ew.work);\n\t}\n}\n\nstatic const struct file_operations sg_fops = {\n\t.owner = THIS_MODULE,\n\t.read = sg_read,\n\t.write = sg_write,\n\t.poll = sg_poll,\n\t.unlocked_ioctl = sg_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl = sg_compat_ioctl,\n#endif\n\t.open = sg_open,\n\t.mmap = sg_mmap,\n\t.release = sg_release,\n\t.fasync = sg_fasync,\n\t.llseek = no_llseek,\n};\n\nstatic struct class *sg_sysfs_class;\n\nstatic int sg_sysfs_valid = 0;\n\nstatic Sg_device *\nsg_alloc(struct gendisk *disk, struct scsi_device *scsidp)\n{\n\tstruct request_queue *q = scsidp->request_queue;\n\tSg_device *sdp;\n\tunsigned long iflags;\n\tint error;\n\tu32 k;\n\n\tsdp = kzalloc(sizeof(Sg_device), GFP_KERNEL);\n\tif (!sdp) {\n\t\tsdev_printk(KERN_WARNING, scsidp, \"%s: kmalloc Sg_device \"\n\t\t\t    \"failure\\n\", __func__);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tidr_preload(GFP_KERNEL);\n\twrite_lock_irqsave(&sg_index_lock, iflags);\n\n\terror = idr_alloc(&sg_index_idr, sdp, 0, SG_MAX_DEVS, GFP_NOWAIT);\n\tif (error < 0) {\n\t\tif (error == -ENOSPC) {\n\t\t\tsdev_printk(KERN_WARNING, scsidp,\n\t\t\t\t    \"Unable to attach sg device type=%d, minor number exceeds %d\\n\",\n\t\t\t\t    scsidp->type, SG_MAX_DEVS - 1);\n\t\t\terror = -ENODEV;\n\t\t} else {\n\t\t\tsdev_printk(KERN_WARNING, scsidp, \"%s: idr \"\n\t\t\t\t    \"allocation Sg_device failure: %d\\n\",\n\t\t\t\t    __func__, error);\n\t\t}\n\t\tgoto out_unlock;\n\t}\n\tk = error;\n\n\tSCSI_LOG_TIMEOUT(3, sdev_printk(KERN_INFO, scsidp,\n\t\t\t\t\t\"sg_alloc: dev=%d \\n\", k));\n\tsprintf(disk->disk_name, \"sg%d\", k);\n\tdisk->first_minor = k;\n\tsdp->disk = disk;\n\tsdp->device = scsidp;\n\tmutex_init(&sdp->open_rel_lock);\n\tINIT_LIST_HEAD(&sdp->sfds);\n\tinit_waitqueue_head(&sdp->open_wait);\n\tatomic_set(&sdp->detaching, 0);\n\trwlock_init(&sdp->sfd_lock);\n\tsdp->sg_tablesize = queue_max_segments(q);\n\tsdp->index = k;\n\tkref_init(&sdp->d_ref);\n\terror = 0;\n\nout_unlock:\n\twrite_unlock_irqrestore(&sg_index_lock, iflags);\n\tidr_preload_end();\n\n\tif (error) {\n\t\tkfree(sdp);\n\t\treturn ERR_PTR(error);\n\t}\n\treturn sdp;\n}\n\nstatic int\nsg_add_device(struct device *cl_dev, struct class_interface *cl_intf)\n{\n\tstruct scsi_device *scsidp = to_scsi_device(cl_dev->parent);\n\tstruct gendisk *disk;\n\tSg_device *sdp = NULL;\n\tstruct cdev * cdev = NULL;\n\tint error;\n\tunsigned long iflags;\n\n\tdisk = alloc_disk(1);\n\tif (!disk) {\n\t\tpr_warn(\"%s: alloc_disk failed\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\tdisk->major = SCSI_GENERIC_MAJOR;\n\n\terror = -ENOMEM;\n\tcdev = cdev_alloc();\n\tif (!cdev) {\n\t\tpr_warn(\"%s: cdev_alloc failed\\n\", __func__);\n\t\tgoto out;\n\t}\n\tcdev->owner = THIS_MODULE;\n\tcdev->ops = &sg_fops;\n\n\tsdp = sg_alloc(disk, scsidp);\n\tif (IS_ERR(sdp)) {\n\t\tpr_warn(\"%s: sg_alloc failed\\n\", __func__);\n\t\terror = PTR_ERR(sdp);\n\t\tgoto out;\n\t}\n\n\terror = cdev_add(cdev, MKDEV(SCSI_GENERIC_MAJOR, sdp->index), 1);\n\tif (error)\n\t\tgoto cdev_add_err;\n\n\tsdp->cdev = cdev;\n\tif (sg_sysfs_valid) {\n\t\tstruct device *sg_class_member;\n\n\t\tsg_class_member = device_create(sg_sysfs_class, cl_dev->parent,\n\t\t\t\t\t\tMKDEV(SCSI_GENERIC_MAJOR,\n\t\t\t\t\t\t      sdp->index),\n\t\t\t\t\t\tsdp, \"%s\", disk->disk_name);\n\t\tif (IS_ERR(sg_class_member)) {\n\t\t\tpr_err(\"%s: device_create failed\\n\", __func__);\n\t\t\terror = PTR_ERR(sg_class_member);\n\t\t\tgoto cdev_add_err;\n\t\t}\n\t\terror = sysfs_create_link(&scsidp->sdev_gendev.kobj,\n\t\t\t\t\t  &sg_class_member->kobj, \"generic\");\n\t\tif (error)\n\t\t\tpr_err(\"%s: unable to make symlink 'generic' back \"\n\t\t\t       \"to sg%d\\n\", __func__, sdp->index);\n\t} else\n\t\tpr_warn(\"%s: sg_sys Invalid\\n\", __func__);\n\n\tsdev_printk(KERN_NOTICE, scsidp, \"Attached scsi generic sg%d \"\n\t\t    \"type %d\\n\", sdp->index, scsidp->type);\n\n\tdev_set_drvdata(cl_dev, sdp);\n\n\treturn 0;\n\ncdev_add_err:\n\twrite_lock_irqsave(&sg_index_lock, iflags);\n\tidr_remove(&sg_index_idr, sdp->index);\n\twrite_unlock_irqrestore(&sg_index_lock, iflags);\n\tkfree(sdp);\n\nout:\n\tput_disk(disk);\n\tif (cdev)\n\t\tcdev_del(cdev);\n\treturn error;\n}\n\nstatic void\nsg_device_destroy(struct kref *kref)\n{\n\tstruct sg_device *sdp = container_of(kref, struct sg_device, d_ref);\n\tunsigned long flags;\n\n\t/* CAUTION!  Note that the device can still be found via idr_find()\n\t * even though the refcount is 0.  Therefore, do idr_remove() BEFORE\n\t * any other cleanup.\n\t */\n\n\twrite_lock_irqsave(&sg_index_lock, flags);\n\tidr_remove(&sg_index_idr, sdp->index);\n\twrite_unlock_irqrestore(&sg_index_lock, flags);\n\n\tSCSI_LOG_TIMEOUT(3,\n\t\tsg_printk(KERN_INFO, sdp, \"sg_device_destroy\\n\"));\n\n\tput_disk(sdp->disk);\n\tkfree(sdp);\n}\n\nstatic void\nsg_remove_device(struct device *cl_dev, struct class_interface *cl_intf)\n{\n\tstruct scsi_device *scsidp = to_scsi_device(cl_dev->parent);\n\tSg_device *sdp = dev_get_drvdata(cl_dev);\n\tunsigned long iflags;\n\tSg_fd *sfp;\n\tint val;\n\n\tif (!sdp)\n\t\treturn;\n\t/* want sdp->detaching non-zero as soon as possible */\n\tval = atomic_inc_return(&sdp->detaching);\n\tif (val > 1)\n\t\treturn; /* only want to do following once per device */\n\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"%s\\n\", __func__));\n\n\tread_lock_irqsave(&sdp->sfd_lock, iflags);\n\tlist_for_each_entry(sfp, &sdp->sfds, sfd_siblings) {\n\t\twake_up_interruptible_all(&sfp->read_wait);\n\t\tkill_fasync(&sfp->async_qp, SIGPOLL, POLL_HUP);\n\t}\n\twake_up_interruptible_all(&sdp->open_wait);\n\tread_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\n\tsysfs_remove_link(&scsidp->sdev_gendev.kobj, \"generic\");\n\tdevice_destroy(sg_sysfs_class, MKDEV(SCSI_GENERIC_MAJOR, sdp->index));\n\tcdev_del(sdp->cdev);\n\tsdp->cdev = NULL;\n\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n}\n\nmodule_param_named(scatter_elem_sz, scatter_elem_sz, int, S_IRUGO | S_IWUSR);\nmodule_param_named(def_reserved_size, def_reserved_size, int,\n\t\t   S_IRUGO | S_IWUSR);\nmodule_param_named(allow_dio, sg_allow_dio, int, S_IRUGO | S_IWUSR);\n\nMODULE_AUTHOR(\"Douglas Gilbert\");\nMODULE_DESCRIPTION(\"SCSI generic (sg) driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(SG_VERSION_STR);\nMODULE_ALIAS_CHARDEV_MAJOR(SCSI_GENERIC_MAJOR);\n\nMODULE_PARM_DESC(scatter_elem_sz, \"scatter gather element \"\n                \"size (default: max(SG_SCATTER_SZ, PAGE_SIZE))\");\nMODULE_PARM_DESC(def_reserved_size, \"size of buffer reserved for each fd\");\nMODULE_PARM_DESC(allow_dio, \"allow direct I/O (default: 0 (disallow))\");\n\nstatic int __init\ninit_sg(void)\n{\n\tint rc;\n\n\tif (scatter_elem_sz < PAGE_SIZE) {\n\t\tscatter_elem_sz = PAGE_SIZE;\n\t\tscatter_elem_sz_prev = scatter_elem_sz;\n\t}\n\tif (def_reserved_size >= 0)\n\t\tsg_big_buff = def_reserved_size;\n\telse\n\t\tdef_reserved_size = sg_big_buff;\n\n\trc = register_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0), \n\t\t\t\t    SG_MAX_DEVS, \"sg\");\n\tif (rc)\n\t\treturn rc;\n        sg_sysfs_class = class_create(THIS_MODULE, \"scsi_generic\");\n        if ( IS_ERR(sg_sysfs_class) ) {\n\t\trc = PTR_ERR(sg_sysfs_class);\n\t\tgoto err_out;\n        }\n\tsg_sysfs_valid = 1;\n\trc = scsi_register_interface(&sg_interface);\n\tif (0 == rc) {\n#ifdef CONFIG_SCSI_PROC_FS\n\t\tsg_proc_init();\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\t\treturn 0;\n\t}\n\tclass_destroy(sg_sysfs_class);\nerr_out:\n\tunregister_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0), SG_MAX_DEVS);\n\treturn rc;\n}\n\nstatic void __exit\nexit_sg(void)\n{\n#ifdef CONFIG_SCSI_PROC_FS\n\tsg_proc_cleanup();\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\tscsi_unregister_interface(&sg_interface);\n\tclass_destroy(sg_sysfs_class);\n\tsg_sysfs_valid = 0;\n\tunregister_chrdev_region(MKDEV(SCSI_GENERIC_MAJOR, 0),\n\t\t\t\t SG_MAX_DEVS);\n\tidr_destroy(&sg_index_idr);\n}\n\nstatic int\nsg_start_req(Sg_request *srp, unsigned char *cmd)\n{\n\tint res;\n\tstruct request *rq;\n\tSg_fd *sfp = srp->parentfp;\n\tsg_io_hdr_t *hp = &srp->header;\n\tint dxfer_len = (int) hp->dxfer_len;\n\tint dxfer_dir = hp->dxfer_direction;\n\tunsigned int iov_count = hp->iovec_count;\n\tSg_scatter_hold *req_schp = &srp->data;\n\tSg_scatter_hold *rsv_schp = &sfp->reserve;\n\tstruct request_queue *q = sfp->parentdp->device->request_queue;\n\tstruct rq_map_data *md, map_data;\n\tint rw = hp->dxfer_direction == SG_DXFER_TO_DEV ? WRITE : READ;\n\tunsigned char *long_cmdp = NULL;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_start_req: dxfer_len=%d\\n\",\n\t\t\t\t      dxfer_len));\n\n\tif (hp->cmd_len > BLK_MAX_CDB) {\n\t\tlong_cmdp = kzalloc(hp->cmd_len, GFP_KERNEL);\n\t\tif (!long_cmdp)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t/*\n\t * NOTE\n\t *\n\t * With scsi-mq enabled, there are a fixed number of preallocated\n\t * requests equal in number to shost->can_queue.  If all of the\n\t * preallocated requests are already in use, then using GFP_ATOMIC with\n\t * blk_get_request() will return -EWOULDBLOCK, whereas using GFP_KERNEL\n\t * will cause blk_get_request() to sleep until an active command\n\t * completes, freeing up a request.  Neither option is ideal, but\n\t * GFP_KERNEL is the better choice to prevent userspace from getting an\n\t * unexpected EWOULDBLOCK.\n\t *\n\t * With scsi-mq disabled, blk_get_request() with GFP_KERNEL usually\n\t * does not sleep except under memory pressure.\n\t */\n\trq = blk_get_request(q, rw, GFP_KERNEL);\n\tif (IS_ERR(rq)) {\n\t\tkfree(long_cmdp);\n\t\treturn PTR_ERR(rq);\n\t}\n\n\tblk_rq_set_block_pc(rq);\n\n\tif (hp->cmd_len > BLK_MAX_CDB)\n\t\trq->cmd = long_cmdp;\n\tmemcpy(rq->cmd, cmd, hp->cmd_len);\n\trq->cmd_len = hp->cmd_len;\n\n\tsrp->rq = rq;\n\trq->end_io_data = srp;\n\trq->sense = srp->sense_b;\n\trq->retries = SG_DEFAULT_RETRIES;\n\n\tif ((dxfer_len <= 0) || (dxfer_dir == SG_DXFER_NONE))\n\t\treturn 0;\n\n\tif (sg_allow_dio && hp->flags & SG_FLAG_DIRECT_IO &&\n\t    dxfer_dir != SG_DXFER_UNKNOWN && !iov_count &&\n\t    !sfp->parentdp->device->host->unchecked_isa_dma &&\n\t    blk_rq_aligned(q, (unsigned long)hp->dxferp, dxfer_len))\n\t\tmd = NULL;\n\telse\n\t\tmd = &map_data;\n\n\tif (md) {\n\t\tif (!sg_res_in_use(sfp) && dxfer_len <= rsv_schp->bufflen)\n\t\t\tsg_link_reserve(sfp, srp, dxfer_len);\n\t\telse {\n\t\t\tres = sg_build_indirect(req_schp, sfp, dxfer_len);\n\t\t\tif (res)\n\t\t\t\treturn res;\n\t\t}\n\n\t\tmd->pages = req_schp->pages;\n\t\tmd->page_order = req_schp->page_order;\n\t\tmd->nr_entries = req_schp->k_use_sg;\n\t\tmd->offset = 0;\n\t\tmd->null_mapped = hp->dxferp ? 0 : 1;\n\t\tif (dxfer_dir == SG_DXFER_TO_FROM_DEV)\n\t\t\tmd->from_user = 1;\n\t\telse\n\t\t\tmd->from_user = 0;\n\t}\n\n\tif (iov_count) {\n\t\tstruct iovec *iov = NULL;\n\t\tstruct iov_iter i;\n\n\t\tres = import_iovec(rw, hp->dxferp, iov_count, 0, &iov, &i);\n\t\tif (res < 0)\n\t\t\treturn res;\n\n\t\tiov_iter_truncate(&i, hp->dxfer_len);\n\n\t\tres = blk_rq_map_user_iov(q, rq, md, &i, GFP_ATOMIC);\n\t\tkfree(iov);\n\t} else\n\t\tres = blk_rq_map_user(q, rq, md, hp->dxferp,\n\t\t\t\t      hp->dxfer_len, GFP_ATOMIC);\n\n\tif (!res) {\n\t\tsrp->bio = rq->bio;\n\n\t\tif (!md) {\n\t\t\treq_schp->dio_in_use = 1;\n\t\t\thp->info |= SG_INFO_DIRECT_IO;\n\t\t}\n\t}\n\treturn res;\n}\n\nstatic int\nsg_finish_rem_req(Sg_request *srp)\n{\n\tint ret = 0;\n\n\tSg_fd *sfp = srp->parentfp;\n\tSg_scatter_hold *req_schp = &srp->data;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t      \"sg_finish_rem_req: res_used=%d\\n\",\n\t\t\t\t      (int) srp->res_used));\n\tif (srp->bio)\n\t\tret = blk_rq_unmap_user(srp->bio);\n\n\tif (srp->rq) {\n\t\tif (srp->rq->cmd != srp->rq->__cmd)\n\t\t\tkfree(srp->rq->cmd);\n\t\tblk_put_request(srp->rq);\n\t}\n\n\tif (srp->res_used)\n\t\tsg_unlink_reserve(sfp, srp);\n\telse\n\t\tsg_remove_scat(sfp, req_schp);\n\n\tsg_remove_request(sfp, srp);\n\n\treturn ret;\n}\n\nstatic int\nsg_build_sgat(Sg_scatter_hold * schp, const Sg_fd * sfp, int tablesize)\n{\n\tint sg_bufflen = tablesize * sizeof(struct page *);\n\tgfp_t gfp_flags = GFP_ATOMIC | __GFP_NOWARN;\n\n\tschp->pages = kzalloc(sg_bufflen, gfp_flags);\n\tif (!schp->pages)\n\t\treturn -ENOMEM;\n\tschp->sglist_len = sg_bufflen;\n\treturn tablesize;\t/* number of scat_gath elements allocated */\n}\n\nstatic int\nsg_build_indirect(Sg_scatter_hold * schp, Sg_fd * sfp, int buff_size)\n{\n\tint ret_sz = 0, i, k, rem_sz, num, mx_sc_elems;\n\tint sg_tablesize = sfp->parentdp->sg_tablesize;\n\tint blk_size = buff_size, order;\n\tgfp_t gfp_mask = GFP_ATOMIC | __GFP_COMP | __GFP_NOWARN;\n\n\tif (blk_size < 0)\n\t\treturn -EFAULT;\n\tif (0 == blk_size)\n\t\t++blk_size;\t/* don't know why */\n\t/* round request up to next highest SG_SECTOR_SZ byte boundary */\n\tblk_size = ALIGN(blk_size, SG_SECTOR_SZ);\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\"sg_build_indirect: buff_size=%d, blk_size=%d\\n\",\n\t\tbuff_size, blk_size));\n\n\t/* N.B. ret_sz carried into this block ... */\n\tmx_sc_elems = sg_build_sgat(schp, sfp, sg_tablesize);\n\tif (mx_sc_elems < 0)\n\t\treturn mx_sc_elems;\t/* most likely -ENOMEM */\n\n\tnum = scatter_elem_sz;\n\tif (unlikely(num != scatter_elem_sz_prev)) {\n\t\tif (num < PAGE_SIZE) {\n\t\t\tscatter_elem_sz = PAGE_SIZE;\n\t\t\tscatter_elem_sz_prev = PAGE_SIZE;\n\t\t} else\n\t\t\tscatter_elem_sz_prev = num;\n\t}\n\n\tif (sfp->low_dma)\n\t\tgfp_mask |= GFP_DMA;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\tgfp_mask |= __GFP_ZERO;\n\n\torder = get_order(num);\nretry:\n\tret_sz = 1 << (PAGE_SHIFT + order);\n\n\tfor (k = 0, rem_sz = blk_size; rem_sz > 0 && k < mx_sc_elems;\n\t     k++, rem_sz -= ret_sz) {\n\n\t\tnum = (rem_sz > scatter_elem_sz_prev) ?\n\t\t\tscatter_elem_sz_prev : rem_sz;\n\n\t\tschp->pages[k] = alloc_pages(gfp_mask, order);\n\t\tif (!schp->pages[k])\n\t\t\tgoto out;\n\n\t\tif (num == scatter_elem_sz_prev) {\n\t\t\tif (unlikely(ret_sz > scatter_elem_sz_prev)) {\n\t\t\t\tscatter_elem_sz = ret_sz;\n\t\t\t\tscatter_elem_sz_prev = ret_sz;\n\t\t\t}\n\t\t}\n\n\t\tSCSI_LOG_TIMEOUT(5, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t \"sg_build_indirect: k=%d, num=%d, ret_sz=%d\\n\",\n\t\t\t\t k, num, ret_sz));\n\t}\t\t/* end of for loop */\n\n\tschp->page_order = order;\n\tschp->k_use_sg = k;\n\tSCSI_LOG_TIMEOUT(5, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_build_indirect: k_use_sg=%d, rem_sz=%d\\n\",\n\t\t\t k, rem_sz));\n\n\tschp->bufflen = blk_size;\n\tif (rem_sz > 0)\t/* must have failed */\n\t\treturn -ENOMEM;\n\treturn 0;\nout:\n\tfor (i = 0; i < k; i++)\n\t\t__free_pages(schp->pages[i], order);\n\n\tif (--order >= 0)\n\t\tgoto retry;\n\n\treturn -ENOMEM;\n}\n\nstatic void\nsg_remove_scat(Sg_fd * sfp, Sg_scatter_hold * schp)\n{\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_remove_scat: k_use_sg=%d\\n\", schp->k_use_sg));\n\tif (schp->pages && schp->sglist_len > 0) {\n\t\tif (!schp->dio_in_use) {\n\t\t\tint k;\n\n\t\t\tfor (k = 0; k < schp->k_use_sg && schp->pages[k]; k++) {\n\t\t\t\tSCSI_LOG_TIMEOUT(5,\n\t\t\t\t\tsg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t\t\"sg_remove_scat: k=%d, pg=0x%p\\n\",\n\t\t\t\t\tk, schp->pages[k]));\n\t\t\t\t__free_pages(schp->pages[k], schp->page_order);\n\t\t\t}\n\n\t\t\tkfree(schp->pages);\n\t\t}\n\t}\n\tmemset(schp, 0, sizeof (*schp));\n}\n\nstatic int\nsg_read_oxfer(Sg_request * srp, char __user *outp, int num_read_xfer)\n{\n\tSg_scatter_hold *schp = &srp->data;\n\tint k, num;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, srp->parentfp->parentdp,\n\t\t\t \"sg_read_oxfer: num_read_xfer=%d\\n\",\n\t\t\t num_read_xfer));\n\tif ((!outp) || (num_read_xfer <= 0))\n\t\treturn 0;\n\n\tnum = 1 << (PAGE_SHIFT + schp->page_order);\n\tfor (k = 0; k < schp->k_use_sg && schp->pages[k]; k++) {\n\t\tif (num > num_read_xfer) {\n\t\t\tif (__copy_to_user(outp, page_address(schp->pages[k]),\n\t\t\t\t\t   num_read_xfer))\n\t\t\t\treturn -EFAULT;\n\t\t\tbreak;\n\t\t} else {\n\t\t\tif (__copy_to_user(outp, page_address(schp->pages[k]),\n\t\t\t\t\t   num))\n\t\t\t\treturn -EFAULT;\n\t\t\tnum_read_xfer -= num;\n\t\t\tif (num_read_xfer <= 0)\n\t\t\t\tbreak;\n\t\t\toutp += num;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void\nsg_build_reserve(Sg_fd * sfp, int req_size)\n{\n\tSg_scatter_hold *schp = &sfp->reserve;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_build_reserve: req_size=%d\\n\", req_size));\n\tdo {\n\t\tif (req_size < PAGE_SIZE)\n\t\t\treq_size = PAGE_SIZE;\n\t\tif (0 == sg_build_indirect(schp, sfp, req_size))\n\t\t\treturn;\n\t\telse\n\t\t\tsg_remove_scat(sfp, schp);\n\t\treq_size >>= 1;\t/* divide by 2 */\n\t} while (req_size > (PAGE_SIZE / 2));\n}\n\nstatic void\nsg_link_reserve(Sg_fd * sfp, Sg_request * srp, int size)\n{\n\tSg_scatter_hold *req_schp = &srp->data;\n\tSg_scatter_hold *rsv_schp = &sfp->reserve;\n\tint k, num, rem;\n\n\tsrp->res_used = 1;\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t \"sg_link_reserve: size=%d\\n\", size));\n\trem = size;\n\n\tnum = 1 << (PAGE_SHIFT + rsv_schp->page_order);\n\tfor (k = 0; k < rsv_schp->k_use_sg; k++) {\n\t\tif (rem <= num) {\n\t\t\treq_schp->k_use_sg = k + 1;\n\t\t\treq_schp->sglist_len = rsv_schp->sglist_len;\n\t\t\treq_schp->pages = rsv_schp->pages;\n\n\t\t\treq_schp->bufflen = size;\n\t\t\treq_schp->page_order = rsv_schp->page_order;\n\t\t\tbreak;\n\t\t} else\n\t\t\trem -= num;\n\t}\n\n\tif (k >= rsv_schp->k_use_sg)\n\t\tSCSI_LOG_TIMEOUT(1, sg_printk(KERN_INFO, sfp->parentdp,\n\t\t\t\t \"sg_link_reserve: BAD size\\n\"));\n}\n\nstatic void\nsg_unlink_reserve(Sg_fd * sfp, Sg_request * srp)\n{\n\tSg_scatter_hold *req_schp = &srp->data;\n\n\tSCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, srp->parentfp->parentdp,\n\t\t\t\t      \"sg_unlink_reserve: req->k_use_sg=%d\\n\",\n\t\t\t\t      (int) req_schp->k_use_sg));\n\treq_schp->k_use_sg = 0;\n\treq_schp->bufflen = 0;\n\treq_schp->pages = NULL;\n\treq_schp->page_order = 0;\n\treq_schp->sglist_len = 0;\n\tsfp->save_scat_len = 0;\n\tsrp->res_used = 0;\n}\n\nstatic Sg_request *\nsg_get_rq_mark(Sg_fd * sfp, int pack_id)\n{\n\tSg_request *resp;\n\tunsigned long iflags;\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tfor (resp = sfp->headrp; resp; resp = resp->nextrp) {\n\t\t/* look for requests that are ready + not SG_IO owned */\n\t\tif ((1 == resp->done) && (!resp->sg_io_owned) &&\n\t\t    ((-1 == pack_id) || (resp->header.pack_id == pack_id))) {\n\t\t\tresp->done = 2;\t/* guard against other readers */\n\t\t\tbreak;\n\t\t}\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn resp;\n}\n\n/* always adds to end of list */\nstatic Sg_request *\nsg_add_request(Sg_fd * sfp)\n{\n\tint k;\n\tunsigned long iflags;\n\tSg_request *resp;\n\tSg_request *rp = sfp->req_arr;\n\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tresp = sfp->headrp;\n\tif (!resp) {\n\t\tmemset(rp, 0, sizeof (Sg_request));\n\t\trp->parentfp = sfp;\n\t\tresp = rp;\n\t\tsfp->headrp = resp;\n\t} else {\n\t\tif (0 == sfp->cmd_q)\n\t\t\tresp = NULL;\t/* command queuing disallowed */\n\t\telse {\n\t\t\tfor (k = 0; k < SG_MAX_QUEUE; ++k, ++rp) {\n\t\t\t\tif (!rp->parentfp)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (k < SG_MAX_QUEUE) {\n\t\t\t\tmemset(rp, 0, sizeof (Sg_request));\n\t\t\t\trp->parentfp = sfp;\n\t\t\t\twhile (resp->nextrp)\n\t\t\t\t\tresp = resp->nextrp;\n\t\t\t\tresp->nextrp = rp;\n\t\t\t\tresp = rp;\n\t\t\t} else\n\t\t\t\tresp = NULL;\n\t\t}\n\t}\n\tif (resp) {\n\t\tresp->nextrp = NULL;\n\t\tresp->header.duration = jiffies_to_msecs(jiffies);\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn resp;\n}\n\n/* Return of 1 for found; 0 for not found */\nstatic int\nsg_remove_request(Sg_fd * sfp, Sg_request * srp)\n{\n\tSg_request *prev_rp;\n\tSg_request *rp;\n\tunsigned long iflags;\n\tint res = 0;\n\n\tif ((!sfp) || (!srp) || (!sfp->headrp))\n\t\treturn res;\n\twrite_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tprev_rp = sfp->headrp;\n\tif (srp == prev_rp) {\n\t\tsfp->headrp = prev_rp->nextrp;\n\t\tprev_rp->parentfp = NULL;\n\t\tres = 1;\n\t} else {\n\t\twhile ((rp = prev_rp->nextrp)) {\n\t\t\tif (srp == rp) {\n\t\t\t\tprev_rp->nextrp = rp->nextrp;\n\t\t\t\trp->parentfp = NULL;\n\t\t\t\tres = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tprev_rp = rp;\n\t\t}\n\t}\n\twrite_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn res;\n}\n\nstatic Sg_fd *\nsg_add_sfp(Sg_device * sdp)\n{\n\tSg_fd *sfp;\n\tunsigned long iflags;\n\tint bufflen;\n\n\tsfp = kzalloc(sizeof(*sfp), GFP_ATOMIC | __GFP_NOWARN);\n\tif (!sfp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinit_waitqueue_head(&sfp->read_wait);\n\trwlock_init(&sfp->rq_list_lock);\n\n\tkref_init(&sfp->f_ref);\n\tsfp->timeout = SG_DEFAULT_TIMEOUT;\n\tsfp->timeout_user = SG_DEFAULT_TIMEOUT_USER;\n\tsfp->force_packid = SG_DEF_FORCE_PACK_ID;\n\tsfp->low_dma = (SG_DEF_FORCE_LOW_DMA == 0) ?\n\t    sdp->device->host->unchecked_isa_dma : 1;\n\tsfp->cmd_q = SG_DEF_COMMAND_Q;\n\tsfp->keep_orphan = SG_DEF_KEEP_ORPHAN;\n\tsfp->parentdp = sdp;\n\twrite_lock_irqsave(&sdp->sfd_lock, iflags);\n\tif (atomic_read(&sdp->detaching)) {\n\t\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\tlist_add_tail(&sfp->sfd_siblings, &sdp->sfds);\n\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_add_sfp: sfp=0x%p\\n\", sfp));\n\tif (unlikely(sg_big_buff != def_reserved_size))\n\t\tsg_big_buff = def_reserved_size;\n\n\tbufflen = min_t(int, sg_big_buff,\n\t\t\tmax_sectors_bytes(sdp->device->request_queue));\n\tsg_build_reserve(sfp, bufflen);\n\tSCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,\n\t\t\t\t      \"sg_add_sfp: bufflen=%d, k_use_sg=%d\\n\",\n\t\t\t\t      sfp->reserve.bufflen,\n\t\t\t\t      sfp->reserve.k_use_sg));\n\n\tkref_get(&sdp->d_ref);\n\t__module_get(THIS_MODULE);\n\treturn sfp;\n}\n\nstatic void\nsg_remove_sfp_usercontext(struct work_struct *work)\n{\n\tstruct sg_fd *sfp = container_of(work, struct sg_fd, ew.work);\n\tstruct sg_device *sdp = sfp->parentdp;\n\n\t/* Cleanup any responses which were never read(). */\n\twhile (sfp->headrp)\n\t\tsg_finish_rem_req(sfp->headrp);\n\n\tif (sfp->reserve.bufflen > 0) {\n\t\tSCSI_LOG_TIMEOUT(6, sg_printk(KERN_INFO, sdp,\n\t\t\t\t\"sg_remove_sfp:    bufflen=%d, k_use_sg=%d\\n\",\n\t\t\t\t(int) sfp->reserve.bufflen,\n\t\t\t\t(int) sfp->reserve.k_use_sg));\n\t\tsg_remove_scat(sfp, &sfp->reserve);\n\t}\n\n\tSCSI_LOG_TIMEOUT(6, sg_printk(KERN_INFO, sdp,\n\t\t\t\"sg_remove_sfp: sfp=0x%p\\n\", sfp));\n\tkfree(sfp);\n\n\tscsi_device_put(sdp->device);\n\tkref_put(&sdp->d_ref, sg_device_destroy);\n\tmodule_put(THIS_MODULE);\n}\n\nstatic void\nsg_remove_sfp(struct kref *kref)\n{\n\tstruct sg_fd *sfp = container_of(kref, struct sg_fd, f_ref);\n\tstruct sg_device *sdp = sfp->parentdp;\n\tunsigned long iflags;\n\n\twrite_lock_irqsave(&sdp->sfd_lock, iflags);\n\tlist_del(&sfp->sfd_siblings);\n\twrite_unlock_irqrestore(&sdp->sfd_lock, iflags);\n\n\tINIT_WORK(&sfp->ew.work, sg_remove_sfp_usercontext);\n\tschedule_work(&sfp->ew.work);\n}\n\nstatic int\nsg_res_in_use(Sg_fd * sfp)\n{\n\tconst Sg_request *srp;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sfp->rq_list_lock, iflags);\n\tfor (srp = sfp->headrp; srp; srp = srp->nextrp)\n\t\tif (srp->res_used)\n\t\t\tbreak;\n\tread_unlock_irqrestore(&sfp->rq_list_lock, iflags);\n\treturn srp ? 1 : 0;\n}\n\n#ifdef CONFIG_SCSI_PROC_FS\nstatic int\nsg_idr_max_id(int id, void *p, void *data)\n{\n\tint *k = data;\n\n\tif (*k < id)\n\t\t*k = id;\n\n\treturn 0;\n}\n\nstatic int\nsg_last_dev(void)\n{\n\tint k = -1;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tidr_for_each(&sg_index_idr, sg_idr_max_id, &k);\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn k + 1;\t\t/* origin 1 */\n}\n#endif\n\n/* must be called with sg_index_lock held */\nstatic Sg_device *sg_lookup_dev(int dev)\n{\n\treturn idr_find(&sg_index_idr, dev);\n}\n\nstatic Sg_device *\nsg_get_dev(int dev)\n{\n\tstruct sg_device *sdp;\n\tunsigned long flags;\n\n\tread_lock_irqsave(&sg_index_lock, flags);\n\tsdp = sg_lookup_dev(dev);\n\tif (!sdp)\n\t\tsdp = ERR_PTR(-ENXIO);\n\telse if (atomic_read(&sdp->detaching)) {\n\t\t/* If sdp->detaching, then the refcount may already be 0, in\n\t\t * which case it would be a bug to do kref_get().\n\t\t */\n\t\tsdp = ERR_PTR(-ENODEV);\n\t} else\n\t\tkref_get(&sdp->d_ref);\n\tread_unlock_irqrestore(&sg_index_lock, flags);\n\n\treturn sdp;\n}\n\n#ifdef CONFIG_SCSI_PROC_FS\n\nstatic struct proc_dir_entry *sg_proc_sgp = NULL;\n\nstatic char sg_proc_sg_dirname[] = \"scsi/sg\";\n\nstatic int sg_proc_seq_show_int(struct seq_file *s, void *v);\n\nstatic int sg_proc_single_open_adio(struct inode *inode, struct file *file);\nstatic ssize_t sg_proc_write_adio(struct file *filp, const char __user *buffer,\n\t\t\t          size_t count, loff_t *off);\nstatic const struct file_operations adio_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_adio,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.write = sg_proc_write_adio,\n\t.release = single_release,\n};\n\nstatic int sg_proc_single_open_dressz(struct inode *inode, struct file *file);\nstatic ssize_t sg_proc_write_dressz(struct file *filp, \n\t\tconst char __user *buffer, size_t count, loff_t *off);\nstatic const struct file_operations dressz_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_dressz,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.write = sg_proc_write_dressz,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_version(struct seq_file *s, void *v);\nstatic int sg_proc_single_open_version(struct inode *inode, struct file *file);\nstatic const struct file_operations version_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_version,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_devhdr(struct seq_file *s, void *v);\nstatic int sg_proc_single_open_devhdr(struct inode *inode, struct file *file);\nstatic const struct file_operations devhdr_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_single_open_devhdr,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = single_release,\n};\n\nstatic int sg_proc_seq_show_dev(struct seq_file *s, void *v);\nstatic int sg_proc_open_dev(struct inode *inode, struct file *file);\nstatic void * dev_seq_start(struct seq_file *s, loff_t *pos);\nstatic void * dev_seq_next(struct seq_file *s, void *v, loff_t *pos);\nstatic void dev_seq_stop(struct seq_file *s, void *v);\nstatic const struct file_operations dev_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_dev,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations dev_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_dev,\n};\n\nstatic int sg_proc_seq_show_devstrs(struct seq_file *s, void *v);\nstatic int sg_proc_open_devstrs(struct inode *inode, struct file *file);\nstatic const struct file_operations devstrs_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_devstrs,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations devstrs_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_devstrs,\n};\n\nstatic int sg_proc_seq_show_debug(struct seq_file *s, void *v);\nstatic int sg_proc_open_debug(struct inode *inode, struct file *file);\nstatic const struct file_operations debug_fops = {\n\t.owner = THIS_MODULE,\n\t.open = sg_proc_open_debug,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\nstatic const struct seq_operations debug_seq_ops = {\n\t.start = dev_seq_start,\n\t.next  = dev_seq_next,\n\t.stop  = dev_seq_stop,\n\t.show  = sg_proc_seq_show_debug,\n};\n\n\nstruct sg_proc_leaf {\n\tconst char * name;\n\tconst struct file_operations * fops;\n};\n\nstatic const struct sg_proc_leaf sg_proc_leaf_arr[] = {\n\t{\"allow_dio\", &adio_fops},\n\t{\"debug\", &debug_fops},\n\t{\"def_reserved_size\", &dressz_fops},\n\t{\"device_hdr\", &devhdr_fops},\n\t{\"devices\", &dev_fops},\n\t{\"device_strs\", &devstrs_fops},\n\t{\"version\", &version_fops}\n};\n\nstatic int\nsg_proc_init(void)\n{\n\tint num_leaves = ARRAY_SIZE(sg_proc_leaf_arr);\n\tint k;\n\n\tsg_proc_sgp = proc_mkdir(sg_proc_sg_dirname, NULL);\n\tif (!sg_proc_sgp)\n\t\treturn 1;\n\tfor (k = 0; k < num_leaves; ++k) {\n\t\tconst struct sg_proc_leaf *leaf = &sg_proc_leaf_arr[k];\n\t\tumode_t mask = leaf->fops->write ? S_IRUGO | S_IWUSR : S_IRUGO;\n\t\tproc_create(leaf->name, mask, sg_proc_sgp, leaf->fops);\n\t}\n\treturn 0;\n}\n\nstatic void\nsg_proc_cleanup(void)\n{\n\tint k;\n\tint num_leaves = ARRAY_SIZE(sg_proc_leaf_arr);\n\n\tif (!sg_proc_sgp)\n\t\treturn;\n\tfor (k = 0; k < num_leaves; ++k)\n\t\tremove_proc_entry(sg_proc_leaf_arr[k].name, sg_proc_sgp);\n\tremove_proc_entry(sg_proc_sg_dirname, NULL);\n}\n\n\nstatic int sg_proc_seq_show_int(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\n\", *((int *)s->private));\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_adio(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_int, &sg_allow_dio);\n}\n\nstatic ssize_t \nsg_proc_write_adio(struct file *filp, const char __user *buffer,\n\t\t   size_t count, loff_t *off)\n{\n\tint err;\n\tunsigned long num;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\treturn -EACCES;\n\terr = kstrtoul_from_user(buffer, count, 0, &num);\n\tif (err)\n\t\treturn err;\n\tsg_allow_dio = num ? 1 : 0;\n\treturn count;\n}\n\nstatic int sg_proc_single_open_dressz(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_int, &sg_big_buff);\n}\n\nstatic ssize_t \nsg_proc_write_dressz(struct file *filp, const char __user *buffer,\n\t\t     size_t count, loff_t *off)\n{\n\tint err;\n\tunsigned long k = ULONG_MAX;\n\n\tif (!capable(CAP_SYS_ADMIN) || !capable(CAP_SYS_RAWIO))\n\t\treturn -EACCES;\n\n\terr = kstrtoul_from_user(buffer, count, 0, &k);\n\tif (err)\n\t\treturn err;\n\tif (k <= 1048576) {\t/* limit \"big buff\" to 1 MB */\n\t\tsg_big_buff = k;\n\t\treturn count;\n\t}\n\treturn -ERANGE;\n}\n\nstatic int sg_proc_seq_show_version(struct seq_file *s, void *v)\n{\n\tseq_printf(s, \"%d\\t%s [%s]\\n\", sg_version_num, SG_VERSION_STR,\n\t\t   sg_version_date);\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_version(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_version, NULL);\n}\n\nstatic int sg_proc_seq_show_devhdr(struct seq_file *s, void *v)\n{\n\tseq_puts(s, \"host\\tchan\\tid\\tlun\\ttype\\topens\\tqdepth\\tbusy\\tonline\\n\");\n\treturn 0;\n}\n\nstatic int sg_proc_single_open_devhdr(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, sg_proc_seq_show_devhdr, NULL);\n}\n\nstruct sg_proc_deviter {\n\tloff_t\tindex;\n\tsize_t\tmax;\n};\n\nstatic void * dev_seq_start(struct seq_file *s, loff_t *pos)\n{\n\tstruct sg_proc_deviter * it = kmalloc(sizeof(*it), GFP_KERNEL);\n\n\ts->private = it;\n\tif (! it)\n\t\treturn NULL;\n\n\tit->index = *pos;\n\tit->max = sg_last_dev();\n\tif (it->index >= it->max)\n\t\treturn NULL;\n\treturn it;\n}\n\nstatic void * dev_seq_next(struct seq_file *s, void *v, loff_t *pos)\n{\n\tstruct sg_proc_deviter * it = s->private;\n\n\t*pos = ++it->index;\n\treturn (it->index < it->max) ? it : NULL;\n}\n\nstatic void dev_seq_stop(struct seq_file *s, void *v)\n{\n\tkfree(s->private);\n}\n\nstatic int sg_proc_open_dev(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &dev_seq_ops);\n}\n\nstatic int sg_proc_seq_show_dev(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tstruct scsi_device *scsidp;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tif ((NULL == sdp) || (NULL == sdp->device) ||\n\t    (atomic_read(&sdp->detaching)))\n\t\tseq_puts(s, \"-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\t-1\\n\");\n\telse {\n\t\tscsidp = sdp->device;\n\t\tseq_printf(s, \"%d\\t%d\\t%d\\t%llu\\t%d\\t%d\\t%d\\t%d\\t%d\\n\",\n\t\t\t      scsidp->host->host_no, scsidp->channel,\n\t\t\t      scsidp->id, scsidp->lun, (int) scsidp->type,\n\t\t\t      1,\n\t\t\t      (int) scsidp->queue_depth,\n\t\t\t      (int) atomic_read(&scsidp->device_busy),\n\t\t\t      (int) scsi_device_online(scsidp));\n\t}\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\nstatic int sg_proc_open_devstrs(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &devstrs_seq_ops);\n}\n\nstatic int sg_proc_seq_show_devstrs(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tstruct scsi_device *scsidp;\n\tunsigned long iflags;\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tscsidp = sdp ? sdp->device : NULL;\n\tif (sdp && scsidp && (!atomic_read(&sdp->detaching)))\n\t\tseq_printf(s, \"%8.8s\\t%16.16s\\t%4.4s\\n\",\n\t\t\t   scsidp->vendor, scsidp->model, scsidp->rev);\n\telse\n\t\tseq_puts(s, \"<no active device>\\n\");\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\n/* must be called while holding sg_index_lock */\nstatic void sg_proc_debug_helper(struct seq_file *s, Sg_device * sdp)\n{\n\tint k, m, new_interface, blen, usg;\n\tSg_request *srp;\n\tSg_fd *fp;\n\tconst sg_io_hdr_t *hp;\n\tconst char * cp;\n\tunsigned int ms;\n\n\tk = 0;\n\tlist_for_each_entry(fp, &sdp->sfds, sfd_siblings) {\n\t\tk++;\n\t\tread_lock(&fp->rq_list_lock); /* irqs already disabled */\n\t\tseq_printf(s, \"   FD(%d): timeout=%dms bufflen=%d \"\n\t\t\t   \"(res)sgat=%d low_dma=%d\\n\", k,\n\t\t\t   jiffies_to_msecs(fp->timeout),\n\t\t\t   fp->reserve.bufflen,\n\t\t\t   (int) fp->reserve.k_use_sg,\n\t\t\t   (int) fp->low_dma);\n\t\tseq_printf(s, \"   cmd_q=%d f_packid=%d k_orphan=%d closed=0\\n\",\n\t\t\t   (int) fp->cmd_q, (int) fp->force_packid,\n\t\t\t   (int) fp->keep_orphan);\n\t\tfor (m = 0, srp = fp->headrp;\n\t\t\t\tsrp != NULL;\n\t\t\t\t++m, srp = srp->nextrp) {\n\t\t\thp = &srp->header;\n\t\t\tnew_interface = (hp->interface_id == '\\0') ? 0 : 1;\n\t\t\tif (srp->res_used) {\n\t\t\t\tif (new_interface && \n\t\t\t\t    (SG_FLAG_MMAP_IO & hp->flags))\n\t\t\t\t\tcp = \"     mmap>> \";\n\t\t\t\telse\n\t\t\t\t\tcp = \"     rb>> \";\n\t\t\t} else {\n\t\t\t\tif (SG_INFO_DIRECT_IO_MASK & hp->info)\n\t\t\t\t\tcp = \"     dio>> \";\n\t\t\t\telse\n\t\t\t\t\tcp = \"     \";\n\t\t\t}\n\t\t\tseq_puts(s, cp);\n\t\t\tblen = srp->data.bufflen;\n\t\t\tusg = srp->data.k_use_sg;\n\t\t\tseq_puts(s, srp->done ?\n\t\t\t\t ((1 == srp->done) ?  \"rcv:\" : \"fin:\")\n\t\t\t\t  : \"act:\");\n\t\t\tseq_printf(s, \" id=%d blen=%d\",\n\t\t\t\t   srp->header.pack_id, blen);\n\t\t\tif (srp->done)\n\t\t\t\tseq_printf(s, \" dur=%d\", hp->duration);\n\t\t\telse {\n\t\t\t\tms = jiffies_to_msecs(jiffies);\n\t\t\t\tseq_printf(s, \" t_o/elap=%d/%d\",\n\t\t\t\t\t(new_interface ? hp->timeout :\n\t\t\t\t\t\t  jiffies_to_msecs(fp->timeout)),\n\t\t\t\t\t(ms > hp->duration ? ms - hp->duration : 0));\n\t\t\t}\n\t\t\tseq_printf(s, \"ms sgat=%d op=0x%02x\\n\", usg,\n\t\t\t\t   (int) srp->data.cmd_opcode);\n\t\t}\n\t\tif (0 == m)\n\t\t\tseq_puts(s, \"     No requests active\\n\");\n\t\tread_unlock(&fp->rq_list_lock);\n\t}\n}\n\nstatic int sg_proc_open_debug(struct inode *inode, struct file *file)\n{\n        return seq_open(file, &debug_seq_ops);\n}\n\nstatic int sg_proc_seq_show_debug(struct seq_file *s, void *v)\n{\n\tstruct sg_proc_deviter * it = (struct sg_proc_deviter *) v;\n\tSg_device *sdp;\n\tunsigned long iflags;\n\n\tif (it && (0 == it->index))\n\t\tseq_printf(s, \"max_active_device=%d  def_reserved_size=%d\\n\",\n\t\t\t   (int)it->max, sg_big_buff);\n\n\tread_lock_irqsave(&sg_index_lock, iflags);\n\tsdp = it ? sg_lookup_dev(it->index) : NULL;\n\tif (NULL == sdp)\n\t\tgoto skip;\n\tread_lock(&sdp->sfd_lock);\n\tif (!list_empty(&sdp->sfds)) {\n\t\tseq_printf(s, \" >>> device=%s \", sdp->disk->disk_name);\n\t\tif (atomic_read(&sdp->detaching))\n\t\t\tseq_puts(s, \"detaching pending close \");\n\t\telse if (sdp->device) {\n\t\t\tstruct scsi_device *scsidp = sdp->device;\n\n\t\t\tseq_printf(s, \"%d:%d:%d:%llu   em=%d\",\n\t\t\t\t   scsidp->host->host_no,\n\t\t\t\t   scsidp->channel, scsidp->id,\n\t\t\t\t   scsidp->lun,\n\t\t\t\t   scsidp->host->hostt->emulated);\n\t\t}\n\t\tseq_printf(s, \" sg_tablesize=%d excl=%d open_cnt=%d\\n\",\n\t\t\t   sdp->sg_tablesize, sdp->exclude, sdp->open_cnt);\n\t\tsg_proc_debug_helper(s, sdp);\n\t}\n\tread_unlock(&sdp->sfd_lock);\nskip:\n\tread_unlock_irqrestore(&sg_index_lock, iflags);\n\treturn 0;\n}\n\n#endif\t\t\t\t/* CONFIG_SCSI_PROC_FS */\n\nmodule_init(init_sg);\nmodule_exit(exit_sg);\n"], "filenames": ["block/bsg.c", "drivers/scsi/sg.c"], "buggy_code_start_loc": [657, 582], "buggy_code_end_loc": [657, 582], "fixing_code_start_loc": [658, 583], "fixing_code_end_loc": [661, 586], "type": "CWE-416", "message": "The sg implementation in the Linux kernel through 4.9 does not properly restrict write operations in situations where the KERNEL_DS option is set, which allows local users to read or write to arbitrary kernel memory locations or cause a denial of service (use-after-free) by leveraging access to a /dev/sg device, related to block/bsg.c and drivers/scsi/sg.c.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2016-9576.", "other": {"cve": {"id": "CVE-2016-10088", "sourceIdentifier": "security@debian.org", "published": "2016-12-30T18:59:00.130", "lastModified": "2023-06-07T12:44:09.807", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The sg implementation in the Linux kernel through 4.9 does not properly restrict write operations in situations where the KERNEL_DS option is set, which allows local users to read or write to arbitrary kernel memory locations or cause a denial of service (use-after-free) by leveraging access to a /dev/sg device, related to block/bsg.c and drivers/scsi/sg.c.  NOTE: this vulnerability exists because of an incomplete fix for CVE-2016-9576."}, {"lang": "es", "value": "La implementaci\u00f3n sg en el kernel Linux hasta la versi\u00f3n 4.9 no restringe correctamente operaciones de escritura en situaciones donde la opci\u00f3n KERNEL_DS est\u00e1 activa, lo que permite a usuarios locales leer o escribir a ubicacioes arbitrarias de memoria de kernel o provocar una denegaci\u00f3n de servicio (uso despues de liberaci\u00f3n) aprovechando el acceso al dispositivo /dev/sg, relacionado con block/bsg.c y drivers/scsi/sg.c. NOTA: esta vulnerabilidad existe debido a una reparaci\u00f3n incompleta de CVE-2016-9576."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:L/UI:N/S:U/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.0, "baseSeverity": "HIGH"}, "exploitabilityScore": 1.0, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "3.10.107", "matchCriteriaId": "C7CC435A-771D-4B94-92E2-D1E1F6658911"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.11", "versionEndExcluding": "3.12.70", "matchCriteriaId": "62D40056-DC08-4609-8FAB-B6D924994367"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.13", "versionEndExcluding": "3.16.40", "matchCriteriaId": "1331ABAB-8C2B-4379-BA77-B655A5B9A83F"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.17", "versionEndExcluding": "3.18.47", "matchCriteriaId": "B1A82714-1C53-498D-94AA-DE9F6B577522"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "3.19", "versionEndExcluding": "4.1.38", "matchCriteriaId": "755C626E-7669-4E6E-BC91-2656E4740E66"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.2", "versionEndExcluding": "4.4.41", "matchCriteriaId": "416DE4AD-4E79-4CC6-9B9D-15BA301E0811"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.5", "versionEndExcluding": "4.8.17", "matchCriteriaId": "852FD2CB-474A-4B94-8B29-1307B3402946"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.9", "versionEndExcluding": "4.9.2", "matchCriteriaId": "F0671122-FCD7-4CEF-B818-5680B6E594DA"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=128394eff343fc6d2f32172f03e24829539c5835", "source": "security@debian.org", "tags": ["Patch", "Vendor Advisory"]}, {"url": "http://rhn.redhat.com/errata/RHSA-2017-0817.html", "source": "security@debian.org", "tags": ["Third Party Advisory"]}, {"url": "http://www.openwall.com/lists/oss-security/2016/12/30/1", "source": "security@debian.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/95169", "source": "security@debian.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "http://www.securitytracker.com/id/1037538", "source": "security@debian.org", "tags": ["Third Party Advisory", "VDB Entry"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:1842", "source": "security@debian.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2077", "source": "security@debian.org", "tags": ["Third Party Advisory"]}, {"url": "https://access.redhat.com/errata/RHSA-2017:2669", "source": "security@debian.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/128394eff343fc6d2f32172f03e24829539c5835", "source": "security@debian.org", "tags": ["Patch", "Vendor Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/128394eff343fc6d2f32172f03e24829539c5835"}}