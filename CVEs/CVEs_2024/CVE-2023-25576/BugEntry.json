{"buggy_code": ["# @fastify/multipart\n\n![CI](https://github.com/fastify/fastify-multipart/workflows/CI/badge.svg)\n[![NPM version](https://img.shields.io/npm/v/@fastify/multipart.svg?style=flat)](https://www.npmjs.com/package/@fastify/multipart)\n[![js-standard-style](https://img.shields.io/badge/code%20style-standard-brightgreen.svg?style=flat)](https://standardjs.com/)\n\nFastify plugin to parse the multipart content-type. Supports:\n\n- Async / Await\n- Async iterator support to handle multiple parts\n- Stream & Disk mode\n- Accumulate whole file in memory\n- Mode to attach all fields to the request body\n- Tested across Linux/Mac/Windows\n\nUnder the hood it uses [`@fastify/busboy`](https://github.com/fastify/busboy).\n\n## Install\n```sh\nnpm i @fastify/multipart\n```\n\n## Usage\n\nIf you are looking for the documentation for the legacy callback-api please see [here](./callback.md).\n\n```js\nconst fastify = require('fastify')()\nconst fs = require('fs')\nconst util = require('util')\nconst { pipeline } = require('stream')\nconst pump = util.promisify(pipeline)\n\nfastify.register(require('@fastify/multipart'))\n\nfastify.post('/', async function (req, reply) {\n  // process a single file\n  // also, consider that if you allow to upload multiple files\n  // you must consume all files otherwise the promise will never fulfill\n  const data = await req.file()\n\n  data.file // stream\n  data.fields // other parsed parts\n  data.fieldname\n  data.filename\n  data.encoding\n  data.mimetype\n\n  // to accumulate the file in memory! Be careful!\n  //\n  // await data.toBuffer() // Buffer\n  //\n  // or\n\n  await pump(data.file, fs.createWriteStream(data.filename))\n\n  // be careful of permission issues on disk and not overwrite\n  // sensitive files that could cause security risks\n  \n  // also, consider that if the file stream is not consumed, the promise will never fulfill\n\n  reply.send()\n})\n\nfastify.listen({ port: 3000 }, err => {\n  if (err) throw err\n  console.log(`server listening on ${fastify.server.address().port}`)\n})\n```\n\n**Note** about `data.fields`: `busboy` consumes the multipart in serial order (stream). Therefore, the order of form fields is *VERY IMPORTANT* to how `@fastify/multipart` can display the fields to you.\nWe would recommend you place the value fields first before any of the file fields.\nIt will ensure your fields are accessible before it starts consuming any files.\nIf you cannot control the order of the placed fields, be sure to read `data.fields` *AFTER* consuming the stream, or it will only contain the fields parsed at that moment.\n\nYou can also pass optional arguments to `@fastify/busboy` when registering with Fastify. This is useful for setting limits on the content that can be uploaded. A full list of available options can be found in the [`@fastify/busboy` documentation](https://github.com/fastify/busboy#busboy-methods).\n\n```js\nfastify.register(require('@fastify/multipart'), {\n  limits: {\n    fieldNameSize: 100, // Max field name size in bytes\n    fieldSize: 100,     // Max field value size in bytes\n    fields: 10,         // Max number of non-file fields\n    fileSize: 1000000,  // For multipart forms, the max file size in bytes\n    files: 1,           // Max number of file fields\n    headerPairs: 2000   // Max number of header key=>value pairs\n  }\n});\n```\n\n**Note**: if the file stream that is provided by `data.file` is not consumed, like in the example below with the usage of pump, the promise will not be fulfilled at the end of the multipart processing.\nThis behavior is inherited from [`@fastify/busboy`](https://github.com/fastify/busboy).\n\n**Note**: if you set a `fileSize` limit and you want to know if the file limit was reached you can:\n- listen to `data.file.on('limit')`\n- or check at the end of the stream the property `data.file.truncated`\n- or call `data.file.toBuffer()` and wait for the error to be thrown\n\n```js\nconst data = await req.file()\nawait pump(data.file, fs.createWriteStream(data.filename))\nif (data.file.truncated) {\n  // you may need to delete the part of the file that has been saved on disk\n  // before the `limits.fileSize` has been reached\n  reply.send(new fastify.multipartErrors.FilesLimitError());    \n}\n\n// OR\nconst data = await req.file()\ntry {\n  const buffer = await data.toBuffer()\n} catch (err) {\n  // fileSize limit reached!\n}\n\n``` \n\nAdditionally, you can pass per-request options to the  `req.file`, `req.files`, `req.saveRequestFiles` or `req.parts` function.\n\n```js\nfastify.post('/', async function (req, reply) {\n  const options = { limits: { fileSize: 1000 } };\n  const data = await req.file(options)\n  await pump(data.file, fs.createWriteStream(data.filename))\n  reply.send()\n})\n```\n\n## Handle multiple file streams\n\n```js\nfastify.post('/', async function (req, reply) {\n  const parts = req.files()\n  for await (const part of parts) {\n    await pump(part.file, fs.createWriteStream(part.filename))\n  }\n  reply.send()\n})\n```\n\n## Handle multiple file streams and fields\n\n```js\nfastify.post('/upload/raw/any', async function (req, reply) {\n  const parts = req.parts()\n  for await (const part of parts) {\n    if (part.file) {\n      await pump(part.file, fs.createWriteStream(part.filename))\n    } else {\n      console.log(part)\n    }\n  }\n  reply.send()\n})\n```\n\n## Accumulate whole file in memory\n\n```js\nfastify.post('/upload/raw/any', async function (req, reply) {\n  const data = await req.file()\n  const buffer = await data.toBuffer()\n  // upload to S3\n  reply.send()\n})\n```\n\n\n## Upload files to disk and work with temporary file paths\n\nThis will store all files in the operating system default directory for temporary files. As soon as the response ends all files are removed.\n\n```js\nfastify.post('/upload/files', async function (req, reply) {\n  // stores files to tmp dir and return files\n  const files = await req.saveRequestFiles()\n  files[0].filepath\n  files[0].fieldname\n  files[0].filename\n  files[0].encoding\n  files[0].mimetype\n  files[0].fields // other parsed parts\n\n  reply.send()\n})\n```\n\n## Handle file size limitation\n\nIf you set a `fileSize` limit, it is able to throw a `RequestFileTooLargeError` error when limit reached.\n\n```js\nfastify.post('/upload/files', async function (req, reply) {\n  try {\n    const file = await req.file({ limits: { fileSize: 17000 } })\n    //const files = req.files({ limits: { fileSize: 17000 } })\n    //const parts = req.parts({ limits: { fileSize: 17000 } })\n    //const files = await req.saveRequestFiles({ limits: { fileSize: 17000 } })\n    reply.send()\n  } catch (error) {\n    // error instanceof fastify.multipartErrors.RequestFileTooLargeError\n  }\n})\n```\n\nIf you want to fallback to the handling before `4.0.0`, you can disable the throwing behavior by passing `throwFileSizeLimit`.\nNote: It will not affect the behavior of `saveRequestFiles()`\n\n```js\n// globally disable\nfastify.register(fastifyMultipart, { throwFileSizeLimit: false })\n\nfastify.post('/upload/file', async function (req, reply) {\n  const file = await req.file({ throwFileSizeLimit: false, limits: { fileSize: 17000 } })\n  //const files = req.files({ throwFileSizeLimit: false, limits: { fileSize: 17000 } })\n  //const parts = req.parts({ throwFileSizeLimit: false, limits: { fileSize: 17000 } })\n  //const files = await req.saveRequestFiles({ throwFileSizeLimit: false, limits: { fileSize: 17000 } })\n  reply.send()\n})\n```\n\n## Parse all fields and assign them to the body\n\nThis allows you to parse all fields automatically and assign them to the `request.body`. By default files are accumulated in memory (Be careful!) to buffer objects. Uncaught errors are [handled](https://github.com/fastify/fastify/blob/main/docs/Reference/Hooks.md#manage-errors-from-a-hook) by Fastify.\n\n```js\nfastify.register(require('@fastify/multipart'), { attachFieldsToBody: true })\n\nfastify.post('/upload/files', async function (req, reply) {\n  const uploadValue = await req.body.upload.toBuffer() // access files\n  const fooValue = req.body.foo.value                  // other fields\n  const body = Object.fromEntries(\n    Object.keys(req.body).map((key) => [key, req.body[key].value])\n  ) // Request body in key-value pairs, like req.body in Express (Node 12+)\n})\n```\n\nRequest body key-value pairs can be assigned directly using `attachFieldsToBody: 'keyValues'`. Field values will be attached directly to the body object. By default, all files are converted to a string using `buffer.toString()` used as the value attached to the body.\n\n```js\nfastify.register(require('@fastify/multipart'), { attachFieldsToBody: 'keyValues' })\n\nfastify.post('/upload/files', async function (req, reply) {\n  const uploadValue = req.body.upload // access file as string\n  const fooValue = req.body.foo       // other fields\n})\n```\n\nYou can also define an `onFile` handler to avoid accumulating all files in memory.\n\n```js\nasync function onFile(part) {\n  await pump(part.file, fs.createWriteStream(part.filename))\n}\n\nfastify.register(require('@fastify/multipart'), { attachFieldsToBody: true, onFile })\n\nfastify.post('/upload/files', async function (req, reply) {\n  const fooValue = req.body.foo.value // other fields\n})\n```\n\nThe `onFile` handler can also be used with `attachFieldsToBody: 'keyValues'` in order to specify how file buffer values are decoded.\n\n```js\nasync function onFile(part) {\n  const buff = await part.toBuffer()\n  const decoded = Buffer.from(buff.toString(), 'base64').toString()\n  part.value = decoded // set `part.value` to specify the request body value\n}\n\nfastify.register(require('@fastify/multipart'), { attachFieldsToBody: 'keyValues', onFile })\n\nfastify.post('/upload/files', async function (req, reply) {\n  const uploadValue = req.body.upload // access file as base64 string\n  const fooValue = req.body.foo       // other fields\n})\n```\n\n**Note**: if you assign all fields to the body and don't define an `onFile` handler, you won't be able to read the files through streams, as they are already read and their contents are accumulated in memory.\nYou can only use the `toBuffer` method to read the content.\nIf you try to read from a stream and pipe to a new file, you will obtain an empty new file.\n\n## JSON Schema body validation\n\nIf you enable `attachFieldsToBody: 'keyValues'` then the response body and JSON Schema validation will behave similarly to `application/json` and [`application/x-www-form-urlencoded`](https://github.com/fastify/fastify-formbody) content types. Files will be decoded using `Buffer.toString()` and attached as a body value.\n\n```js\nfastify.register(require('@fastify/multipart'), { attachFieldsToBody: 'keyValues' })\n\nfastify.post('/upload/files', {\n  schema: {\n    body: {\n      type: 'object',\n      required: ['myFile'],\n      properties: {\n        // file that gets decoded to string\n        myFile: {\n          type: 'string',\n          // validate that file contents match a UUID\n          pattern: '^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'\n        },\n        hello: {\n          type: 'string',\n          enum: ['world']\n        }\n      }\n    }\n  }\n}, function (req, reply) {\n  console.log({ body: req.body })\n  reply.send('done')\n})\n```\n\nIf you enable `attachFieldsToBody: true` and set `sharedSchemaId` a shared JSON Schema is added, which can be used to validate parsed multipart fields.\n\n```js\nconst opts = {\n  attachFieldsToBody: true,\n  sharedSchemaId: '#mySharedSchema'\n}\nfastify.register(require('@fastify/multipart'), opts)\n\nfastify.post('/upload/files', {\n  schema: {\n    body: {\n      type: 'object',\n      required: ['myField'],\n      properties: {\n        // field that uses the shared schema\n        myField: { $ref: '#mySharedSchema'},\n        // or another field that uses the shared schema\n        myFiles: { type: 'array', items: fastify.getSchema('mySharedSchema') },\n        // or a field that doesn't use the shared schema\n        hello: {\n          properties: {\n            value: { \n              type: 'string',\n              enum: ['male']\n            }\n          }\n        }\n      }\n    }\n  }\n}, function (req, reply) {\n  console.log({ body: req.body })\n  reply.send('done')\n})\n```\n\nIf provided, the `sharedSchemaId` parameter must be a string ID and a shared schema will be added to your fastify instance so you will be able to apply the validation to your service (like in the example mentioned above).\n\nThe shared schema, that is added, will look like this:\n```js\n{\n  type: 'object',\n  properties: {\n    encoding: { type: 'string' },\n    filename: { type: 'string' },\n    limit: { type: 'boolean' },\n    mimetype: { type: 'string' }\n  }\n}\n```\n\n### JSON Schema non-file field\nWhen sending fields with the body (`attachFieldsToBody` set to true), the field might look like this in the `request.body`:\n```json\n{\n  \"hello\": \"world\"\n}\n```\nThe mentioned field will be converted, by this plugin, to a more complex field. The converted field will look something like this:\n```js\n{ \n  hello: {\n    fieldname: \"hello\",\n    value: \"world\",\n    fieldnameTruncated: false,\n    valueTruncated: false,\n    fields: body\n  }\n}\n```\n\nIt is important to know that this conversion happens BEFORE the field is validated, so keep that in mind when writing the JSON schema for validation for fields that don't use the shared schema. The schema for validation for the field mentioned above should look like this:\n```js\nhello: {\n  properties: {\n    value: { \n      type: 'string'\n    }\n  }\n}\n```\n\n#### JSON non-file fields\n\nIf a non file field sent has `Content-Type` header\u00a0starting with `application/json`, it will be parsed using `JSON.parse`. \n\nThe schema to validate JSON fields should look like this:\n\n```js\nhello: {\n  properties: {\n    value: { \n      type: 'object',\n      properties: {\n        /* ... */\n      }\n    }\n  }\n}\n```\n\nIf you also use the shared JSON schema as shown above, this is a full example which validates the entire field:\n\n```js\nconst opts = {\n  attachFieldsToBody: true,\n  sharedSchemaId: '#mySharedSchema'\n}\nfastify.register(require('@fastify/multipart'), opts)\n\nfastify.post('/upload/files', {\n  schema: {\n    body: {\n      type: 'object',\n      required: ['field'],\n      properties: {\n        field: {\n          allOf: [\n            { $ref: '#mySharedSchema' }, \n            { \n              properties: { \n                value: { \n                  type: 'object'\n                  properties: {\n                    child: {\n                      type: 'string'\n                    }\n                  }\n                }\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}, function (req, reply) {\n  console.log({ body: req.body })\n  reply.send('done')\n})\n```\n\n## Access all errors\n\nWe export all custom errors via a server decorator `fastify.multipartErrors`. This is useful if you want to react to specific errors. They are derived from [@fastify/error](https://github.com/fastify/fastify-error) and include the correct `statusCode` property.\n\n```js\nfastify.post('/upload/files', async function (req, reply) {\n  const { FilesLimitError } = fastify.multipartErrors\n})\n```\n\n## Acknowledgements\n\nThis project is kindly sponsored by:\n- [nearForm](https://nearform.com)\n- [LetzDoIt](https://www.letzdoitapp.com/)\n\n## License\n\nLicensed under [MIT](./LICENSE).\n", "'use strict'\n\nconst Busboy = require('@fastify/busboy')\nconst os = require('os')\nconst fp = require('fastify-plugin')\nconst eos = require('end-of-stream')\nconst { createWriteStream } = require('fs')\nconst { unlink } = require('fs').promises\nconst path = require('path')\nconst hexoid = require('hexoid')\nconst util = require('util')\nconst createError = require('@fastify/error')\nconst sendToWormhole = require('stream-wormhole')\nconst deepmergeAll = require('@fastify/deepmerge')({ all: true })\nconst { PassThrough, pipeline, Readable } = require('stream')\nconst pump = util.promisify(pipeline)\nconst secureJSON = require('secure-json-parse')\n\nconst kMultipart = Symbol('multipart')\nconst kMultipartHandler = Symbol('multipartHandler')\nconst getDescriptor = Object.getOwnPropertyDescriptor\n\nconst PartsLimitError = createError('FST_PARTS_LIMIT', 'reach parts limit', 413)\nconst FilesLimitError = createError('FST_FILES_LIMIT', 'reach files limit', 413)\nconst FieldsLimitError = createError('FST_FIELDS_LIMIT', 'reach fields limit', 413)\nconst RequestFileTooLargeError = createError('FST_REQ_FILE_TOO_LARGE', 'request file too large, please check multipart config', 413)\nconst PrototypeViolationError = createError('FST_PROTO_VIOLATION', 'prototype property is not allowed as field name', 400)\nconst InvalidMultipartContentTypeError = createError('FST_INVALID_MULTIPART_CONTENT_TYPE', 'the request is not multipart', 406)\nconst InvalidJSONFieldError = createError('FST_INVALID_JSON_FIELD_ERROR', 'a request field is not a valid JSON as declared by its Content-Type', 406)\nconst FileBufferNotFoundError = createError('FST_FILE_BUFFER_NOT_FOUND', 'the file buffer was not found', 500)\n\nfunction setMultipart (req, payload, done) {\n  // nothing to do, it will be done by the Request.multipart object\n  req.raw[kMultipart] = true\n  done()\n}\n\nfunction attachToBody (options, req, reply, next) {\n  if (req.raw[kMultipart] !== true) {\n    next()\n    return\n  }\n\n  const consumerStream = options.onFile || defaultConsumer\n  const body = {}\n  const mp = req.multipart((field, file, filename, encoding, mimetype) => {\n    body[field] = body[field] || []\n    body[field].push({\n      data: [],\n      filename,\n      encoding,\n      mimetype,\n      limit: false\n    })\n\n    const result = consumerStream(field, file, filename, encoding, mimetype, body)\n    if (result && typeof result.then === 'function') {\n      result.catch((err) => {\n        // continue with the workflow\n        err.statusCode = 500\n        file.destroy(err)\n      })\n    }\n  }, function (err) {\n    if (!err) {\n      req.body = body\n    }\n    next(err)\n  }, options)\n\n  mp.on('field', (key, value) => {\n    if (key === '__proto__' || key === 'constructor') {\n      mp.destroy(new Error(`${key} is not allowed as field name`))\n      return\n    }\n    if (body[key] === undefined) {\n      body[key] = value\n    } else if (Array.isArray(body[key])) {\n      body[key].push(value)\n    } else {\n      body[key] = [body[key], value]\n    }\n  })\n}\n\nfunction defaultConsumer (field, file, filename, encoding, mimetype, body) {\n  const fileData = []\n  const lastFile = body[field][body[field].length - 1]\n  file.on('data', data => { if (!lastFile.limit) { fileData.push(data) } })\n  file.on('limit', () => { lastFile.limit = true })\n  file.on('end', () => {\n    if (!lastFile.limit) {\n      lastFile.data = Buffer.concat(fileData)\n    } else {\n      lastFile.data = undefined\n    }\n  })\n}\n\nfunction busboy (options) {\n  try {\n    return new Busboy(options)\n  } catch (error) {\n    const errorEmitter = new PassThrough()\n    process.nextTick(function () {\n      errorEmitter.emit('error', error)\n    })\n    return errorEmitter\n  }\n}\n\nfunction fastifyMultipart (fastify, options, done) {\n  const attachFieldsToBody = options.attachFieldsToBody\n  if (options.addToBody === true) {\n    if (typeof options.sharedSchemaId === 'string') {\n      fastify.addSchema({\n        $id: options.sharedSchemaId,\n        type: 'object',\n        properties: {\n          encoding: { type: 'string' },\n          filename: { type: 'string' },\n          limit: { type: 'boolean' },\n          mimetype: { type: 'string' }\n        }\n      })\n    }\n\n    fastify.addHook('preValidation', function (req, reply, next) {\n      attachToBody(options, req, reply, next)\n    })\n  }\n\n  if (options.attachFieldsToBody === true || options.attachFieldsToBody === 'keyValues') {\n    if (typeof options.sharedSchemaId === 'string') {\n      fastify.addSchema({\n        $id: options.sharedSchemaId,\n        type: 'object',\n        properties: {\n          fieldname: { type: 'string' },\n          encoding: { type: 'string' },\n          filename: { type: 'string' },\n          mimetype: { type: 'string' }\n        }\n      })\n    }\n    fastify.addHook('preValidation', async function (req, reply) {\n      if (!req.isMultipart()) {\n        return\n      }\n      for await (const part of req.parts()) {\n        req.body = part.fields\n        if (part.file) {\n          if (options.onFile) {\n            await options.onFile(part)\n          } else {\n            await part.toBuffer()\n          }\n        }\n      }\n      if (options.attachFieldsToBody === 'keyValues') {\n        const body = {}\n        for (const key of Object.keys(req.body)) {\n          const field = req.body[key]\n          if (field.value !== undefined) {\n            body[key] = field.value\n          } else if (Array.isArray(field)) {\n            body[key] = field.map(item => {\n              if (item._buf !== undefined) {\n                return item._buf.toString()\n              }\n              return item.value\n            })\n          } else if (field._buf !== undefined) {\n            body[key] = field._buf.toString()\n          }\n        }\n        req.body = body\n      }\n    })\n  }\n\n  const defaultThrowFileSizeLimit = typeof options.throwFileSizeLimit === 'boolean'\n    ? options.throwFileSizeLimit\n    : true\n\n  fastify.decorate('multipartErrors', {\n    PartsLimitError,\n    FilesLimitError,\n    FieldsLimitError,\n    PrototypeViolationError,\n    InvalidMultipartContentTypeError,\n    RequestFileTooLargeError,\n    FileBufferNotFoundError\n  })\n\n  fastify.addContentTypeParser('multipart/form-data', setMultipart)\n  fastify.decorateRequest(kMultipartHandler, handleMultipart)\n\n  fastify.decorateRequest('parts', getMultipartIterator)\n\n  fastify.decorateRequest('isMultipart', isMultipart)\n  fastify.decorateRequest('tmpUploads', null)\n\n  // legacy\n  fastify.decorateRequest('multipart', handleLegacyMultipartApi)\n\n  // Stream mode\n  fastify.decorateRequest('file', getMultipartFile)\n  fastify.decorateRequest('files', getMultipartFiles)\n\n  // Disk mode\n  fastify.decorateRequest('saveRequestFiles', saveRequestFiles)\n  fastify.decorateRequest('cleanRequestFiles', cleanRequestFiles)\n\n  fastify.addHook('onResponse', async (request, reply) => {\n    await request.cleanRequestFiles()\n  })\n\n  const toID = hexoid()\n\n  function isMultipart () {\n    return this.raw[kMultipart] || false\n  }\n\n  // handler definition is in multipart-readstream\n  // handler(field, file, filename, encoding, mimetype)\n  // opts is a per-request override for the options object\n  function handleLegacyMultipartApi (handler, done, opts) {\n    if (typeof handler !== 'function') {\n      throw new Error('handler must be a function')\n    }\n\n    if (typeof done !== 'function') {\n      throw new Error('the callback must be a function')\n    }\n\n    if (!this.isMultipart()) {\n      done(new Error('the request is not multipart'))\n      return\n    }\n\n    const log = this.log\n\n    log.warn('the multipart callback-based api is deprecated in favour of the new promise api')\n    log.debug('starting multipart parsing')\n\n    const req = this.raw\n\n    const busboyOptions = deepmergeAll({ headers: req.headers }, options || {}, opts || {})\n    const stream = busboy(busboyOptions)\n    let completed = false\n    let files = 0\n\n    req.on('error', function (err) {\n      stream.destroy()\n      if (!completed) {\n        completed = true\n        done(err)\n      }\n    })\n\n    stream.on('finish', function () {\n      log.debug('finished receiving stream, total %d files', files)\n      if (!completed) {\n        completed = true\n        setImmediate(done)\n      }\n    })\n\n    stream.on('file', wrap)\n\n    req.pipe(stream)\n      .on('error', function (error) {\n        req.emit('error', error)\n      })\n\n    function wrap (field, file, filename, encoding, mimetype) {\n      log.debug({ field, filename, encoding, mimetype }, 'parsing part')\n      files++\n      eos(file, waitForFiles)\n      if (field === '__proto__' || field === 'constructor') {\n        file.destroy(new Error(`${field} is not allowed as field name`))\n        return\n      }\n      handler(field, file, filename, encoding, mimetype)\n    }\n\n    function waitForFiles (err) {\n      if (err) {\n        completed = true\n        done(err)\n      }\n    }\n\n    return stream\n  }\n\n  function handleMultipart (opts = {}) {\n    if (!this.isMultipart()) {\n      throw new InvalidMultipartContentTypeError()\n    }\n\n    this.log.debug('starting multipart parsing')\n\n    let values = []\n    let pendingHandler = null\n\n    // only one file / field can be processed at a time\n    // \"null\" will close the consumer side\n    const ch = (val) => {\n      if (pendingHandler) {\n        pendingHandler(val)\n        pendingHandler = null\n      } else {\n        values.push(val)\n      }\n    }\n\n    const handle = (handler) => {\n      if (values.length > 0) {\n        const value = values[0]\n        values = values.slice(1)\n        handler(value)\n      } else {\n        pendingHandler = handler\n      }\n    }\n\n    const parts = () => {\n      return new Promise((resolve, reject) => {\n        handle((val) => {\n          if (val instanceof Error) return reject(val)\n          resolve(val)\n        })\n      })\n    }\n\n    const body = {}\n    let lastError = null\n    let currentFile = null\n    const request = this.raw\n    const busboyOptions = deepmergeAll(\n      { headers: request.headers },\n      options,\n      opts\n    )\n\n    this.log.trace({ busboyOptions }, 'Providing options to busboy')\n    const bb = busboy(busboyOptions)\n\n    request.on('close', cleanup)\n    request.on('error', cleanup)\n\n    bb\n      .on('field', onField)\n      .on('file', onFile)\n      .on('close', cleanup)\n      .on('error', onEnd)\n      .on('end', onEnd)\n      .on('finish', onEnd)\n\n    bb.on('partsLimit', function () {\n      onError(new PartsLimitError())\n    })\n\n    bb.on('filesLimit', function () {\n      onError(new FilesLimitError())\n    })\n\n    bb.on('fieldsLimit', function () {\n      onError(new FieldsLimitError())\n    })\n\n    request.pipe(bb)\n\n    function onField (name, fieldValue, fieldnameTruncated, valueTruncated, encoding, contentType) {\n      // don't overwrite prototypes\n      if (getDescriptor(Object.prototype, name)) {\n        onError(new PrototypeViolationError())\n        return\n      }\n\n      // If it is a JSON field, parse it\n      if (contentType.startsWith('application/json')) {\n        // If the value was truncated, it can never be a valid JSON. Don't even try to parse\n        if (valueTruncated) {\n          onError(new InvalidJSONFieldError())\n          return\n        }\n\n        try {\n          fieldValue = secureJSON.parse(fieldValue)\n          contentType = 'application/json'\n        } catch (e) {\n          onError(new InvalidJSONFieldError())\n          return\n        }\n      }\n\n      const value = {\n        fieldname: name,\n        mimetype: contentType,\n        encoding,\n        value: fieldValue,\n        fieldnameTruncated,\n        valueTruncated,\n        fields: body\n      }\n\n      if (body[name] === undefined) {\n        body[name] = value\n      } else if (Array.isArray(body[name])) {\n        body[name].push(value)\n      } else {\n        body[name] = [body[name], value]\n      }\n\n      ch(value)\n    }\n\n    function onFile (name, file, filename, encoding, mimetype) {\n      // don't overwrite prototypes\n      if (getDescriptor(Object.prototype, name)) {\n        // ensure that stream is consumed, any error is suppressed\n        sendToWormhole(file)\n        onError(new PrototypeViolationError())\n        return\n      }\n\n      const throwFileSizeLimit = typeof options.throwFileSizeLimit === 'boolean'\n        ? options.throwFileSizeLimit\n        : defaultThrowFileSizeLimit\n\n      const value = {\n        fieldname: name,\n        filename,\n        encoding,\n        mimetype,\n        file,\n        fields: body,\n        _buf: null,\n        async toBuffer () {\n          if (this._buf) {\n            return this._buf\n          }\n          const fileChunks = []\n          let err\n          for await (const chunk of this.file) {\n            fileChunks.push(chunk)\n\n            if (throwFileSizeLimit && this.file.truncated) {\n              err = new RequestFileTooLargeError()\n              err.part = this\n\n              onError(err)\n              fileChunks.length = 0\n            }\n          }\n          if (err) {\n            // throwing in the async iterator will\n            // cause the file.destroy() to be called\n            // The stream has already been managed by\n            // busboy instead\n            throw err\n          }\n          this._buf = Buffer.concat(fileChunks)\n          return this._buf\n        }\n      }\n\n      if (throwFileSizeLimit) {\n        file.on('limit', function () {\n          const err = new RequestFileTooLargeError()\n          err.part = value\n          onError(err)\n        })\n      }\n\n      if (body[name] === undefined) {\n        body[name] = value\n      } else if (Array.isArray(body[name])) {\n        body[name].push(value)\n      } else {\n        body[name] = [body[name], value]\n      }\n      currentFile = file\n      ch(value)\n    }\n\n    function onError (err) {\n      lastError = err\n      currentFile = null\n    }\n\n    function onEnd (err) {\n      cleanup()\n\n      ch(err || lastError)\n    }\n\n    function cleanup (err) {\n      request.unpipe(bb)\n      // in node 10 it seems that error handler is not called but request.aborted is set\n      if ((err || request.aborted) && currentFile) {\n        currentFile.destroy()\n      }\n    }\n\n    return parts\n  }\n\n  async function saveRequestFiles (options) {\n    let files\n    if (attachFieldsToBody === true) {\n      files = filesFromFields.call(this, this.body)\n    } else {\n      files = await this.files(options)\n    }\n    const requestFiles = []\n    const tmpdir = (options && options.tmpdir) || os.tmpdir()\n    this.tmpUploads = []\n    for await (const file of files) {\n      const filepath = path.join(tmpdir, toID() + path.extname(file.filename))\n      const target = createWriteStream(filepath)\n      try {\n        await pump(file.file, target)\n        requestFiles.push({ ...file, filepath })\n        this.tmpUploads.push(filepath)\n      } catch (err) {\n        this.log.error({ err }, 'save request file')\n        throw err\n      }\n    }\n\n    return requestFiles\n  }\n\n  function * filesFromFields (container) {\n    try {\n      for (const field of Object.values(container)) {\n        if (Array.isArray(field)) {\n          for (const subField of filesFromFields.call(this, field)) {\n            yield subField\n          }\n        }\n        if (!field.file) {\n          continue\n        }\n        if (!field._buf) {\n          throw new FileBufferNotFoundError()\n        }\n        field.file = Readable.from(field._buf)\n        yield field\n      }\n    } catch (err) {\n      this.log.error({ err }, 'save request file failed')\n      throw err\n    }\n  }\n\n  async function cleanRequestFiles () {\n    if (!this.tmpUploads) {\n      return\n    }\n    for (const filepath of this.tmpUploads) {\n      try {\n        await unlink(filepath)\n      } catch (error) {\n        this.log.error(error, 'could not delete file')\n      }\n    }\n  }\n\n  async function getMultipartFile (options) {\n    const parts = this[kMultipartHandler](options)\n    let part\n    while ((part = await parts()) != null) {\n      if (part.file) {\n        return part\n      }\n    }\n  }\n\n  async function * getMultipartFiles (options) {\n    const parts = this[kMultipartHandler](options)\n\n    let part\n    while ((part = await parts()) != null) {\n      if (part.file) {\n        yield part\n      }\n    }\n  }\n\n  async function * getMultipartIterator (options) {\n    const parts = this[kMultipartHandler](options)\n\n    let part\n    while ((part = await parts()) != null) {\n      yield part\n    }\n  }\n\n  done()\n}\n\n/**\n * These export configurations enable JS and TS developers\n * to consumer fastify in whatever way best suits their needs.\n */\nmodule.exports = fp(fastifyMultipart, {\n  fastify: '4.x',\n  name: '@fastify/multipart'\n})\nmodule.exports.default = fastifyMultipart\nmodule.exports.fastifyMultipart = fastifyMultipart\n", "'use strict'\n\nconst test = require('tap').test\nconst FormData = require('form-data')\nconst Fastify = require('fastify')\nconst multipart = require('..')\nconst http = require('http')\nconst stream = require('readable-stream')\nconst Readable = stream.Readable\nconst pump = stream.pipeline\nconst crypto = require('crypto')\nconst sendToWormhole = require('stream-wormhole')\n\n// skipping on Github Actions because it takes too long\ntest('should upload a big file in constant memory', { skip: process.env.CI }, function (t) {\n  t.plan(9)\n\n  const fastify = Fastify()\n  const hashInput = crypto.createHash('sha256')\n\n  t.teardown(fastify.close.bind(fastify))\n\n  fastify.register(multipart)\n\n  fastify.post('/', async function (req, reply) {\n    t.ok(req.isMultipart())\n\n    for await (const part of req.parts()) {\n      if (part.file) {\n        t.equal(part.fieldname, 'upload')\n        t.equal(part.filename, 'random-data')\n        t.equal(part.encoding, '7bit')\n        t.equal(part.mimetype, 'binary/octet-stream')\n\n        await sendToWormhole(part.file)\n      }\n    }\n\n    const memory = process.memoryUsage()\n    t.ok(memory.rss < 400 * 1024 * 1024) // 200MB\n    t.ok(memory.heapTotal < 400 * 1024 * 1024) // 200MB\n\n    reply.send()\n  })\n\n  fastify.listen({ port: 0 }, function () {\n    const knownLength = 1024 * 1024 * 1024\n    let total = knownLength\n    const form = new FormData({ maxDataSize: total })\n    const rs = new Readable({\n      read (n) {\n        if (n > total) {\n          n = total\n        }\n\n        const buf = Buffer.alloc(n).fill('x')\n        hashInput.update(buf)\n        this.push(buf)\n\n        total -= n\n\n        if (total === 0) {\n          t.pass('finished generating')\n          hashInput.end()\n          this.push(null)\n        }\n      }\n    })\n    form.append('upload', rs, {\n      filename: 'random-data',\n      contentType: 'binary/octet-stream',\n      knownLength\n    })\n\n    const opts = {\n      protocol: 'http:',\n      hostname: 'localhost',\n      port: fastify.server.address().port,\n      path: '/',\n      headers: form.getHeaders(),\n      method: 'POST'\n    }\n\n    const req = http.request(opts, () => { fastify.close(noop) })\n\n    pump(form, req, function (err) {\n      t.error(err, 'client pump: no err')\n    })\n  })\n})\n\nfunction noop () { }\n", "'use strict'\n\nconst test = require('tap').test\nconst FormData = require('form-data')\nconst Fastify = require('fastify')\nconst multipart = require('./../..')\nconst http = require('http')\nconst stream = require('readable-stream')\nconst Readable = stream.Readable\nconst Writable = stream.Writable\nconst pump = stream.pipeline\nconst eos = stream.finished\nconst crypto = require('crypto')\n\n// skipping on Github Actions because it takes too long\ntest('should upload a big file in constant memory', { skip: process.env.CI }, function (t) {\n  t.plan(12)\n\n  const fastify = Fastify()\n  const hashInput = crypto.createHash('sha256')\n  let sent = false\n\n  t.teardown(fastify.close.bind(fastify))\n\n  fastify.register(multipart)\n\n  fastify.post('/', function (req, reply) {\n    t.ok(req.isMultipart())\n\n    req.multipart(handler, function (err) {\n      t.error(err)\n    })\n\n    function handler (field, file, filename, encoding, mimetype) {\n      t.equal(filename, 'random-data')\n      t.equal(field, 'upload')\n      t.equal(encoding, '7bit')\n      t.equal(mimetype, 'binary/octet-stream')\n      const hashOutput = crypto.createHash('sha256')\n\n      pump(file, hashOutput, new Writable({\n        objectMode: true,\n        write (chunk, enc, cb) {\n          if (!sent) {\n            eos(hashInput, () => {\n              this._write(chunk, enc, cb)\n            })\n            return\n          }\n\n          t.equal(hashInput.digest('hex'), chunk.toString('hex'))\n          cb()\n        }\n      }), function (err) {\n        t.error(err)\n\n        const memory = process.memoryUsage()\n        t.ok(memory.rss < 400 * 1024 * 1024) // 200MB\n        t.ok(memory.heapTotal < 400 * 1024 * 1024) // 200MB\n        reply.send()\n      })\n    }\n  })\n\n  fastify.listen({ port: 0 }, function () {\n    const knownLength = 1024 * 1024 * 1024\n    let total = knownLength\n    const form = new FormData({ maxDataSize: total })\n    const rs = new Readable({\n      read (n) {\n        if (n > total) {\n          n = total\n        }\n\n        const buf = Buffer.alloc(n).fill('x')\n        hashInput.update(buf)\n        this.push(buf)\n\n        total -= n\n\n        if (total === 0) {\n          t.pass('finished generating')\n          sent = true\n          hashInput.end()\n          this.push(null)\n        }\n      }\n    })\n    form.append('upload', rs, {\n      filename: 'random-data',\n      contentType: 'binary/octet-stream',\n      knownLength\n    })\n\n    const opts = {\n      protocol: 'http:',\n      hostname: 'localhost',\n      port: fastify.server.address().port,\n      path: '/',\n      headers: form.getHeaders(),\n      method: 'POST'\n    }\n\n    const req = http.request(opts, () => { fastify.close(noop) })\n\n    pump(form, req, function (err) {\n      t.error(err, 'client pump: no err')\n    })\n  })\n})\n\nfunction noop () { }\n", "'use strict'\n\nconst test = require('tap').test\nconst FormData = require('form-data')\nconst Fastify = require('fastify')\nconst multipart = require('..')\nconst http = require('http')\nconst path = require('path')\nconst fs = require('fs')\n\nconst filePath = path.join(__dirname, '../README.md')\n\ntest('should not allow __proto__ as file name', function (t) {\n  t.plan(4)\n\n  const fastify = Fastify()\n  t.teardown(fastify.close.bind(fastify))\n\n  fastify.register(multipart)\n\n  fastify.post('/', async function (req, reply) {\n    t.ok(req.isMultipart())\n\n    try {\n      await req.file()\n      reply.code(200).send()\n    } catch (error) {\n      t.ok(error instanceof fastify.multipartErrors.PrototypeViolationError)\n      reply.code(500).send()\n    }\n  })\n\n  fastify.listen({ port: 0 }, async function () {\n    // request\n    const form = new FormData()\n    const opts = {\n      protocol: 'http:',\n      hostname: 'localhost',\n      port: fastify.server.address().port,\n      path: '/',\n      headers: form.getHeaders(),\n      method: 'POST'\n    }\n\n    const req = http.request(opts, (res) => {\n      t.equal(res.statusCode, 500)\n      res.resume()\n      res.on('end', () => {\n        t.pass('res ended successfully')\n      })\n    })\n    const rs = fs.createReadStream(filePath)\n    form.append('__proto__', rs)\n\n    form.pipe(req)\n  })\n})\n\ntest('should not allow __proto__ as field name', function (t) {\n  t.plan(4)\n\n  const fastify = Fastify()\n  t.teardown(fastify.close.bind(fastify))\n\n  fastify.register(multipart)\n\n  fastify.post('/', async function (req, reply) {\n    t.ok(req.isMultipart())\n\n    try {\n      await req.file()\n      reply.code(200).send()\n    } catch (error) {\n      t.ok(error instanceof fastify.multipartErrors.PrototypeViolationError)\n      reply.code(500).send()\n    }\n  })\n\n  fastify.listen({ port: 0 }, async function () {\n    // request\n    const form = new FormData()\n    const opts = {\n      protocol: 'http:',\n      hostname: 'localhost',\n      port: fastify.server.address().port,\n      path: '/',\n      headers: form.getHeaders(),\n      method: 'POST'\n    }\n\n    const req = http.request(opts, (res) => {\n      t.equal(res.statusCode, 500)\n      res.resume()\n      res.on('end', () => {\n        t.pass('res ended successfully')\n      })\n    })\n    form.append('__proto__', 'world')\n\n    form.pipe(req)\n  })\n})\n"], "fixing_code": ["# @fastify/multipart\n\n![CI](https://github.com/fastify/fastify-multipart/workflows/CI/badge.svg)\n[![NPM version](https://img.shields.io/npm/v/@fastify/multipart.svg?style=flat)](https://www.npmjs.com/package/@fastify/multipart)\n[![js-standard-style](https://img.shields.io/badge/code%20style-standard-brightgreen.svg?style=flat)](https://standardjs.com/)\n\nFastify plugin to parse the multipart content-type. Supports:\n\n- Async / Await\n- Async iterator support to handle multiple parts\n- Stream & Disk mode\n- Accumulate whole file in memory\n- Mode to attach all fields to the request body\n- Tested across Linux/Mac/Windows\n\nUnder the hood it uses [`@fastify/busboy`](https://github.com/fastify/busboy).\n\n## Install\n```sh\nnpm i @fastify/multipart\n```\n\n## Usage\n\nIf you are looking for the documentation for the legacy callback-api please see [here](./callback.md).\n\n```js\nconst fastify = require('fastify')()\nconst fs = require('fs')\nconst util = require('util')\nconst { pipeline } = require('stream')\nconst pump = util.promisify(pipeline)\n\nfastify.register(require('@fastify/multipart'))\n\nfastify.post('/', async function (req, reply) {\n  // process a single file\n  // also, consider that if you allow to upload multiple files\n  // you must consume all files otherwise the promise will never fulfill\n  const data = await req.file()\n\n  data.file // stream\n  data.fields // other parsed parts\n  data.fieldname\n  data.filename\n  data.encoding\n  data.mimetype\n\n  // to accumulate the file in memory! Be careful!\n  //\n  // await data.toBuffer() // Buffer\n  //\n  // or\n\n  await pump(data.file, fs.createWriteStream(data.filename))\n\n  // be careful of permission issues on disk and not overwrite\n  // sensitive files that could cause security risks\n  \n  // also, consider that if the file stream is not consumed, the promise will never fulfill\n\n  reply.send()\n})\n\nfastify.listen({ port: 3000 }, err => {\n  if (err) throw err\n  console.log(`server listening on ${fastify.server.address().port}`)\n})\n```\n\n**Note** about `data.fields`: `busboy` consumes the multipart in serial order (stream). Therefore, the order of form fields is *VERY IMPORTANT* to how `@fastify/multipart` can display the fields to you.\nWe would recommend you place the value fields first before any of the file fields.\nIt will ensure your fields are accessible before it starts consuming any files.\nIf you cannot control the order of the placed fields, be sure to read `data.fields` *AFTER* consuming the stream, or it will only contain the fields parsed at that moment.\n\nYou can also pass optional arguments to `@fastify/busboy` when registering with Fastify. This is useful for setting limits on the content that can be uploaded. A full list of available options can be found in the [`@fastify/busboy` documentation](https://github.com/fastify/busboy#busboy-methods).\n\n```js\nfastify.register(require('@fastify/multipart'), {\n  limits: {\n    fieldNameSize: 100, // Max field name size in bytes\n    fieldSize: 100,     // Max field value size in bytes\n    fields: 10,         // Max number of non-file fields\n    fileSize: 1000000,  // For multipart forms, the max file size in bytes\n    files: 1,           // Max number of file fields\n    headerPairs: 2000   // Max number of header key=>value pairs\n    parts: 1000         // For multipart forms, the max number of parts (fields + files)\n  }\n});\n```\n\nFor security reasons, `@fastify/multipart` sets the limit for `parts` and `fileSize` being _1000_ and _1048576_ respectively.\n\n**Note**: if the file stream that is provided by `data.file` is not consumed, like in the example below with the usage of pump, the promise will not be fulfilled at the end of the multipart processing.\nThis behavior is inherited from [`@fastify/busboy`](https://github.com/fastify/busboy).\n\n**Note**: if you set a `fileSize` limit and you want to know if the file limit was reached you can:\n- listen to `data.file.on('limit')`\n- or check at the end of the stream the property `data.file.truncated`\n- or call `data.file.toBuffer()` and wait for the error to be thrown\n\n```js\nconst data = await req.file()\nawait pump(data.file, fs.createWriteStream(data.filename))\nif (data.file.truncated) {\n  // you may need to delete the part of the file that has been saved on disk\n  // before the `limits.fileSize` has been reached\n  reply.send(new fastify.multipartErrors.FilesLimitError());    \n}\n\n// OR\nconst data = await req.file()\ntry {\n  const buffer = await data.toBuffer()\n} catch (err) {\n  // fileSize limit reached!\n}\n\n``` \n\nAdditionally, you can pass per-request options to the  `req.file`, `req.files`, `req.saveRequestFiles` or `req.parts` function.\n\n```js\nfastify.post('/', async function (req, reply) {\n  const options = { limits: { fileSize: 1000 } };\n  const data = await req.file(options)\n  await pump(data.file, fs.createWriteStream(data.filename))\n  reply.send()\n})\n```\n\n## Handle multiple file streams\n\n```js\nfastify.post('/', async function (req, reply) {\n  const parts = req.files()\n  for await (const part of parts) {\n    await pump(part.file, fs.createWriteStream(part.filename))\n  }\n  reply.send()\n})\n```\n\n## Handle multiple file streams and fields\n\n```js\nfastify.post('/upload/raw/any', async function (req, reply) {\n  const parts = req.parts()\n  for await (const part of parts) {\n    if (part.file) {\n      await pump(part.file, fs.createWriteStream(part.filename))\n    } else {\n      console.log(part)\n    }\n  }\n  reply.send()\n})\n```\n\n## Accumulate whole file in memory\n\n```js\nfastify.post('/upload/raw/any', async function (req, reply) {\n  const data = await req.file()\n  const buffer = await data.toBuffer()\n  // upload to S3\n  reply.send()\n})\n```\n\n\n## Upload files to disk and work with temporary file paths\n\nThis will store all files in the operating system default directory for temporary files. As soon as the response ends all files are removed.\n\n```js\nfastify.post('/upload/files', async function (req, reply) {\n  // stores files to tmp dir and return files\n  const files = await req.saveRequestFiles()\n  files[0].filepath\n  files[0].fieldname\n  files[0].filename\n  files[0].encoding\n  files[0].mimetype\n  files[0].fields // other parsed parts\n\n  reply.send()\n})\n```\n\n## Handle file size limitation\n\nIf you set a `fileSize` limit, it is able to throw a `RequestFileTooLargeError` error when limit reached.\n\n```js\nfastify.post('/upload/files', async function (req, reply) {\n  try {\n    const file = await req.file({ limits: { fileSize: 17000 } })\n    //const files = req.files({ limits: { fileSize: 17000 } })\n    //const parts = req.parts({ limits: { fileSize: 17000 } })\n    //const files = await req.saveRequestFiles({ limits: { fileSize: 17000 } })\n    reply.send()\n  } catch (error) {\n    // error instanceof fastify.multipartErrors.RequestFileTooLargeError\n  }\n})\n```\n\nIf you want to fallback to the handling before `4.0.0`, you can disable the throwing behavior by passing `throwFileSizeLimit`.\nNote: It will not affect the behavior of `saveRequestFiles()`\n\n```js\n// globally disable\nfastify.register(fastifyMultipart, { throwFileSizeLimit: false })\n\nfastify.post('/upload/file', async function (req, reply) {\n  const file = await req.file({ throwFileSizeLimit: false, limits: { fileSize: 17000 } })\n  //const files = req.files({ throwFileSizeLimit: false, limits: { fileSize: 17000 } })\n  //const parts = req.parts({ throwFileSizeLimit: false, limits: { fileSize: 17000 } })\n  //const files = await req.saveRequestFiles({ throwFileSizeLimit: false, limits: { fileSize: 17000 } })\n  reply.send()\n})\n```\n\n## Parse all fields and assign them to the body\n\nThis allows you to parse all fields automatically and assign them to the `request.body`. By default files are accumulated in memory (Be careful!) to buffer objects. Uncaught errors are [handled](https://github.com/fastify/fastify/blob/main/docs/Reference/Hooks.md#manage-errors-from-a-hook) by Fastify.\n\n```js\nfastify.register(require('@fastify/multipart'), { attachFieldsToBody: true })\n\nfastify.post('/upload/files', async function (req, reply) {\n  const uploadValue = await req.body.upload.toBuffer() // access files\n  const fooValue = req.body.foo.value                  // other fields\n  const body = Object.fromEntries(\n    Object.keys(req.body).map((key) => [key, req.body[key].value])\n  ) // Request body in key-value pairs, like req.body in Express (Node 12+)\n})\n```\n\nRequest body key-value pairs can be assigned directly using `attachFieldsToBody: 'keyValues'`. Field values will be attached directly to the body object. By default, all files are converted to a string using `buffer.toString()` used as the value attached to the body.\n\n```js\nfastify.register(require('@fastify/multipart'), { attachFieldsToBody: 'keyValues' })\n\nfastify.post('/upload/files', async function (req, reply) {\n  const uploadValue = req.body.upload // access file as string\n  const fooValue = req.body.foo       // other fields\n})\n```\n\nYou can also define an `onFile` handler to avoid accumulating all files in memory.\n\n```js\nasync function onFile(part) {\n  await pump(part.file, fs.createWriteStream(part.filename))\n}\n\nfastify.register(require('@fastify/multipart'), { attachFieldsToBody: true, onFile })\n\nfastify.post('/upload/files', async function (req, reply) {\n  const fooValue = req.body.foo.value // other fields\n})\n```\n\nThe `onFile` handler can also be used with `attachFieldsToBody: 'keyValues'` in order to specify how file buffer values are decoded.\n\n```js\nasync function onFile(part) {\n  const buff = await part.toBuffer()\n  const decoded = Buffer.from(buff.toString(), 'base64').toString()\n  part.value = decoded // set `part.value` to specify the request body value\n}\n\nfastify.register(require('@fastify/multipart'), { attachFieldsToBody: 'keyValues', onFile })\n\nfastify.post('/upload/files', async function (req, reply) {\n  const uploadValue = req.body.upload // access file as base64 string\n  const fooValue = req.body.foo       // other fields\n})\n```\n\n**Note**: if you assign all fields to the body and don't define an `onFile` handler, you won't be able to read the files through streams, as they are already read and their contents are accumulated in memory.\nYou can only use the `toBuffer` method to read the content.\nIf you try to read from a stream and pipe to a new file, you will obtain an empty new file.\n\n## JSON Schema body validation\n\nIf you enable `attachFieldsToBody: 'keyValues'` then the response body and JSON Schema validation will behave similarly to `application/json` and [`application/x-www-form-urlencoded`](https://github.com/fastify/fastify-formbody) content types. Files will be decoded using `Buffer.toString()` and attached as a body value.\n\n```js\nfastify.register(require('@fastify/multipart'), { attachFieldsToBody: 'keyValues' })\n\nfastify.post('/upload/files', {\n  schema: {\n    body: {\n      type: 'object',\n      required: ['myFile'],\n      properties: {\n        // file that gets decoded to string\n        myFile: {\n          type: 'string',\n          // validate that file contents match a UUID\n          pattern: '^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'\n        },\n        hello: {\n          type: 'string',\n          enum: ['world']\n        }\n      }\n    }\n  }\n}, function (req, reply) {\n  console.log({ body: req.body })\n  reply.send('done')\n})\n```\n\nIf you enable `attachFieldsToBody: true` and set `sharedSchemaId` a shared JSON Schema is added, which can be used to validate parsed multipart fields.\n\n```js\nconst opts = {\n  attachFieldsToBody: true,\n  sharedSchemaId: '#mySharedSchema'\n}\nfastify.register(require('@fastify/multipart'), opts)\n\nfastify.post('/upload/files', {\n  schema: {\n    body: {\n      type: 'object',\n      required: ['myField'],\n      properties: {\n        // field that uses the shared schema\n        myField: { $ref: '#mySharedSchema'},\n        // or another field that uses the shared schema\n        myFiles: { type: 'array', items: fastify.getSchema('mySharedSchema') },\n        // or a field that doesn't use the shared schema\n        hello: {\n          properties: {\n            value: { \n              type: 'string',\n              enum: ['male']\n            }\n          }\n        }\n      }\n    }\n  }\n}, function (req, reply) {\n  console.log({ body: req.body })\n  reply.send('done')\n})\n```\n\nIf provided, the `sharedSchemaId` parameter must be a string ID and a shared schema will be added to your fastify instance so you will be able to apply the validation to your service (like in the example mentioned above).\n\nThe shared schema, that is added, will look like this:\n```js\n{\n  type: 'object',\n  properties: {\n    encoding: { type: 'string' },\n    filename: { type: 'string' },\n    limit: { type: 'boolean' },\n    mimetype: { type: 'string' }\n  }\n}\n```\n\n### JSON Schema non-file field\nWhen sending fields with the body (`attachFieldsToBody` set to true), the field might look like this in the `request.body`:\n```json\n{\n  \"hello\": \"world\"\n}\n```\nThe mentioned field will be converted, by this plugin, to a more complex field. The converted field will look something like this:\n```js\n{ \n  hello: {\n    fieldname: \"hello\",\n    value: \"world\",\n    fieldnameTruncated: false,\n    valueTruncated: false,\n    fields: body\n  }\n}\n```\n\nIt is important to know that this conversion happens BEFORE the field is validated, so keep that in mind when writing the JSON schema for validation for fields that don't use the shared schema. The schema for validation for the field mentioned above should look like this:\n```js\nhello: {\n  properties: {\n    value: { \n      type: 'string'\n    }\n  }\n}\n```\n\n#### JSON non-file fields\n\nIf a non file field sent has `Content-Type` header\u00a0starting with `application/json`, it will be parsed using `JSON.parse`. \n\nThe schema to validate JSON fields should look like this:\n\n```js\nhello: {\n  properties: {\n    value: { \n      type: 'object',\n      properties: {\n        /* ... */\n      }\n    }\n  }\n}\n```\n\nIf you also use the shared JSON schema as shown above, this is a full example which validates the entire field:\n\n```js\nconst opts = {\n  attachFieldsToBody: true,\n  sharedSchemaId: '#mySharedSchema'\n}\nfastify.register(require('@fastify/multipart'), opts)\n\nfastify.post('/upload/files', {\n  schema: {\n    body: {\n      type: 'object',\n      required: ['field'],\n      properties: {\n        field: {\n          allOf: [\n            { $ref: '#mySharedSchema' }, \n            { \n              properties: { \n                value: { \n                  type: 'object'\n                  properties: {\n                    child: {\n                      type: 'string'\n                    }\n                  }\n                }\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}, function (req, reply) {\n  console.log({ body: req.body })\n  reply.send('done')\n})\n```\n\n## Access all errors\n\nWe export all custom errors via a server decorator `fastify.multipartErrors`. This is useful if you want to react to specific errors. They are derived from [@fastify/error](https://github.com/fastify/fastify-error) and include the correct `statusCode` property.\n\n```js\nfastify.post('/upload/files', async function (req, reply) {\n  const { FilesLimitError } = fastify.multipartErrors\n})\n```\n\n## Acknowledgements\n\nThis project is kindly sponsored by:\n- [nearForm](https://nearform.com)\n- [LetzDoIt](https://www.letzdoitapp.com/)\n\n## License\n\nLicensed under [MIT](./LICENSE).\n", "'use strict'\n\nconst Busboy = require('@fastify/busboy')\nconst os = require('os')\nconst fp = require('fastify-plugin')\nconst eos = require('end-of-stream')\nconst { createWriteStream } = require('fs')\nconst { unlink } = require('fs').promises\nconst path = require('path')\nconst hexoid = require('hexoid')\nconst util = require('util')\nconst createError = require('@fastify/error')\nconst sendToWormhole = require('stream-wormhole')\nconst deepmergeAll = require('@fastify/deepmerge')({ all: true })\nconst { PassThrough, pipeline, Readable } = require('stream')\nconst pump = util.promisify(pipeline)\nconst secureJSON = require('secure-json-parse')\n\nconst kMultipart = Symbol('multipart')\nconst kMultipartHandler = Symbol('multipartHandler')\nconst getDescriptor = Object.getOwnPropertyDescriptor\n\nconst PartsLimitError = createError('FST_PARTS_LIMIT', 'reach parts limit', 413)\nconst FilesLimitError = createError('FST_FILES_LIMIT', 'reach files limit', 413)\nconst FieldsLimitError = createError('FST_FIELDS_LIMIT', 'reach fields limit', 413)\nconst RequestFileTooLargeError = createError('FST_REQ_FILE_TOO_LARGE', 'request file too large, please check multipart config', 413)\nconst PrototypeViolationError = createError('FST_PROTO_VIOLATION', 'prototype property is not allowed as field name', 400)\nconst InvalidMultipartContentTypeError = createError('FST_INVALID_MULTIPART_CONTENT_TYPE', 'the request is not multipart', 406)\nconst InvalidJSONFieldError = createError('FST_INVALID_JSON_FIELD_ERROR', 'a request field is not a valid JSON as declared by its Content-Type', 406)\nconst FileBufferNotFoundError = createError('FST_FILE_BUFFER_NOT_FOUND', 'the file buffer was not found', 500)\n\nfunction setMultipart (req, payload, done) {\n  // nothing to do, it will be done by the Request.multipart object\n  req.raw[kMultipart] = true\n  done()\n}\n\nfunction attachToBody (options, req, reply, next) {\n  if (req.raw[kMultipart] !== true) {\n    next()\n    return\n  }\n\n  const consumerStream = options.onFile || defaultConsumer\n  const body = {}\n  const mp = req.multipart((field, file, filename, encoding, mimetype) => {\n    body[field] = body[field] || []\n    body[field].push({\n      data: [],\n      filename,\n      encoding,\n      mimetype,\n      limit: false\n    })\n\n    const result = consumerStream(field, file, filename, encoding, mimetype, body)\n    if (result && typeof result.then === 'function') {\n      result.catch((err) => {\n        // continue with the workflow\n        err.statusCode = 500\n        file.destroy(err)\n      })\n    }\n  }, function (err) {\n    if (!err) {\n      req.body = body\n    }\n    next(err)\n  }, options)\n\n  mp.on('field', (key, value) => {\n    if (key === '__proto__' || key === 'constructor') {\n      mp.destroy(new Error(`${key} is not allowed as field name`))\n      return\n    }\n    if (body[key] === undefined) {\n      body[key] = value\n    } else if (Array.isArray(body[key])) {\n      body[key].push(value)\n    } else {\n      body[key] = [body[key], value]\n    }\n  })\n}\n\nfunction defaultConsumer (field, file, filename, encoding, mimetype, body) {\n  const fileData = []\n  const lastFile = body[field][body[field].length - 1]\n  file.on('data', data => { if (!lastFile.limit) { fileData.push(data) } })\n  file.on('limit', () => { lastFile.limit = true })\n  file.on('end', () => {\n    if (!lastFile.limit) {\n      lastFile.data = Buffer.concat(fileData)\n    } else {\n      lastFile.data = undefined\n    }\n  })\n}\n\nfunction busboy (options) {\n  try {\n    return new Busboy(options)\n  } catch (error) {\n    const errorEmitter = new PassThrough()\n    process.nextTick(function () {\n      errorEmitter.emit('error', error)\n    })\n    return errorEmitter\n  }\n}\n\nfunction fastifyMultipart (fastify, options, done) {\n  options.limits = {\n    ...options.limits,\n    parts: options.limits?.parts || 1000,\n    fileSize: options.limits?.fileSize || fastify.initialConfig.bodyLimit\n  }\n\n  const attachFieldsToBody = options.attachFieldsToBody\n  if (options.addToBody === true) {\n    if (typeof options.sharedSchemaId === 'string') {\n      fastify.addSchema({\n        $id: options.sharedSchemaId,\n        type: 'object',\n        properties: {\n          encoding: { type: 'string' },\n          filename: { type: 'string' },\n          limit: { type: 'boolean' },\n          mimetype: { type: 'string' }\n        }\n      })\n    }\n\n    fastify.addHook('preValidation', function (req, reply, next) {\n      attachToBody(options, req, reply, next)\n    })\n  }\n\n  if (options.attachFieldsToBody === true || options.attachFieldsToBody === 'keyValues') {\n    if (typeof options.sharedSchemaId === 'string') {\n      fastify.addSchema({\n        $id: options.sharedSchemaId,\n        type: 'object',\n        properties: {\n          fieldname: { type: 'string' },\n          encoding: { type: 'string' },\n          filename: { type: 'string' },\n          mimetype: { type: 'string' }\n        }\n      })\n    }\n    fastify.addHook('preValidation', async function (req, reply) {\n      if (!req.isMultipart()) {\n        return\n      }\n      for await (const part of req.parts()) {\n        req.body = part.fields\n        if (part.file) {\n          if (options.onFile) {\n            await options.onFile(part)\n          } else {\n            await part.toBuffer()\n          }\n        }\n      }\n      if (options.attachFieldsToBody === 'keyValues') {\n        const body = {}\n        for (const key of Object.keys(req.body)) {\n          const field = req.body[key]\n          if (field.value !== undefined) {\n            body[key] = field.value\n          } else if (Array.isArray(field)) {\n            body[key] = field.map(item => {\n              if (item._buf !== undefined) {\n                return item._buf.toString()\n              }\n              return item.value\n            })\n          } else if (field._buf !== undefined) {\n            body[key] = field._buf.toString()\n          }\n        }\n        req.body = body\n      }\n    })\n  }\n\n  const defaultThrowFileSizeLimit = typeof options.throwFileSizeLimit === 'boolean'\n    ? options.throwFileSizeLimit\n    : true\n\n  fastify.decorate('multipartErrors', {\n    PartsLimitError,\n    FilesLimitError,\n    FieldsLimitError,\n    PrototypeViolationError,\n    InvalidMultipartContentTypeError,\n    RequestFileTooLargeError,\n    FileBufferNotFoundError\n  })\n\n  fastify.addContentTypeParser('multipart/form-data', setMultipart)\n  fastify.decorateRequest(kMultipartHandler, handleMultipart)\n\n  fastify.decorateRequest('parts', getMultipartIterator)\n\n  fastify.decorateRequest('isMultipart', isMultipart)\n  fastify.decorateRequest('tmpUploads', null)\n\n  // legacy\n  fastify.decorateRequest('multipart', handleLegacyMultipartApi)\n\n  // Stream mode\n  fastify.decorateRequest('file', getMultipartFile)\n  fastify.decorateRequest('files', getMultipartFiles)\n\n  // Disk mode\n  fastify.decorateRequest('saveRequestFiles', saveRequestFiles)\n  fastify.decorateRequest('cleanRequestFiles', cleanRequestFiles)\n\n  fastify.addHook('onResponse', async (request, reply) => {\n    await request.cleanRequestFiles()\n  })\n\n  const toID = hexoid()\n\n  function isMultipart () {\n    return this.raw[kMultipart] || false\n  }\n\n  // handler definition is in multipart-readstream\n  // handler(field, file, filename, encoding, mimetype)\n  // opts is a per-request override for the options object\n  function handleLegacyMultipartApi (handler, done, opts) {\n    if (typeof handler !== 'function') {\n      throw new Error('handler must be a function')\n    }\n\n    if (typeof done !== 'function') {\n      throw new Error('the callback must be a function')\n    }\n\n    if (!this.isMultipart()) {\n      done(new Error('the request is not multipart'))\n      return\n    }\n\n    const log = this.log\n\n    log.warn('the multipart callback-based api is deprecated in favour of the new promise api')\n    log.debug('starting multipart parsing')\n\n    const req = this.raw\n\n    const busboyOptions = deepmergeAll({ headers: req.headers }, options || {}, opts || {})\n    const stream = busboy(busboyOptions)\n    let completed = false\n    let files = 0\n\n    req.on('error', function (err) {\n      stream.destroy()\n      if (!completed) {\n        completed = true\n        done(err)\n      }\n    })\n\n    stream.on('finish', function () {\n      log.debug('finished receiving stream, total %d files', files)\n      if (!completed) {\n        completed = true\n        setImmediate(done)\n      }\n    })\n\n    stream.on('file', wrap)\n\n    req.pipe(stream)\n      .on('error', function (error) {\n        req.emit('error', error)\n      })\n\n    function wrap (field, file, filename, encoding, mimetype) {\n      log.debug({ field, filename, encoding, mimetype }, 'parsing part')\n      files++\n      eos(file, waitForFiles)\n      if (field === '__proto__' || field === 'constructor') {\n        file.destroy(new Error(`${field} is not allowed as field name`))\n        return\n      }\n      handler(field, file, filename, encoding, mimetype)\n    }\n\n    function waitForFiles (err) {\n      if (err) {\n        completed = true\n        done(err)\n      }\n    }\n\n    return stream\n  }\n\n  function handleMultipart (opts = {}) {\n    if (!this.isMultipart()) {\n      throw new InvalidMultipartContentTypeError()\n    }\n\n    this.log.debug('starting multipart parsing')\n\n    let values = []\n    let pendingHandler = null\n\n    // only one file / field can be processed at a time\n    // \"null\" will close the consumer side\n    const ch = (val) => {\n      if (pendingHandler) {\n        pendingHandler(val)\n        pendingHandler = null\n      } else {\n        values.push(val)\n      }\n    }\n\n    const handle = (handler) => {\n      if (values.length > 0) {\n        const value = values[0]\n        values = values.slice(1)\n        handler(value)\n      } else {\n        pendingHandler = handler\n      }\n    }\n\n    const parts = () => {\n      return new Promise((resolve, reject) => {\n        handle((val) => {\n          if (val instanceof Error) return reject(val)\n          resolve(val)\n        })\n      })\n    }\n\n    const body = {}\n    let lastError = null\n    let currentFile = null\n    const request = this.raw\n    const busboyOptions = deepmergeAll(\n      { headers: request.headers },\n      options,\n      opts\n    )\n\n    this.log.trace({ busboyOptions }, 'Providing options to busboy')\n    const bb = busboy(busboyOptions)\n\n    request.on('close', cleanup)\n    request.on('error', cleanup)\n\n    bb\n      .on('field', onField)\n      .on('file', onFile)\n      .on('close', cleanup)\n      .on('error', onEnd)\n      .on('end', onEnd)\n      .on('finish', onEnd)\n\n    bb.on('partsLimit', function () {\n      const err = new PartsLimitError()\n      onError(err)\n      process.nextTick(() => onEnd(err))\n    })\n\n    bb.on('filesLimit', function () {\n      const err = new FilesLimitError()\n      onError(err)\n      process.nextTick(() => onEnd(err))\n    })\n\n    bb.on('fieldsLimit', function () {\n      const err = new FieldsLimitError()\n      onError(err)\n      process.nextTick(() => onEnd(err))\n    })\n\n    request.pipe(bb)\n\n    function onField (name, fieldValue, fieldnameTruncated, valueTruncated, encoding, contentType) {\n      // don't overwrite prototypes\n      if (getDescriptor(Object.prototype, name)) {\n        onError(new PrototypeViolationError())\n        return\n      }\n\n      // If it is a JSON field, parse it\n      if (contentType.startsWith('application/json')) {\n        // If the value was truncated, it can never be a valid JSON. Don't even try to parse\n        if (valueTruncated) {\n          onError(new InvalidJSONFieldError())\n          return\n        }\n\n        try {\n          fieldValue = secureJSON.parse(fieldValue)\n          contentType = 'application/json'\n        } catch (e) {\n          onError(new InvalidJSONFieldError())\n          return\n        }\n      }\n\n      const value = {\n        fieldname: name,\n        mimetype: contentType,\n        encoding,\n        value: fieldValue,\n        fieldnameTruncated,\n        valueTruncated,\n        fields: body\n      }\n\n      if (body[name] === undefined) {\n        body[name] = value\n      } else if (Array.isArray(body[name])) {\n        body[name].push(value)\n      } else {\n        body[name] = [body[name], value]\n      }\n\n      ch(value)\n    }\n\n    function onFile (name, file, filename, encoding, mimetype) {\n      // don't overwrite prototypes\n      if (getDescriptor(Object.prototype, name)) {\n        // ensure that stream is consumed, any error is suppressed\n        sendToWormhole(file)\n        onError(new PrototypeViolationError())\n        return\n      }\n\n      const throwFileSizeLimit = typeof options.throwFileSizeLimit === 'boolean'\n        ? options.throwFileSizeLimit\n        : defaultThrowFileSizeLimit\n\n      const value = {\n        fieldname: name,\n        filename,\n        encoding,\n        mimetype,\n        file,\n        fields: body,\n        _buf: null,\n        async toBuffer () {\n          if (this._buf) {\n            return this._buf\n          }\n          const fileChunks = []\n          let err\n          for await (const chunk of this.file) {\n            fileChunks.push(chunk)\n\n            if (throwFileSizeLimit && this.file.truncated) {\n              err = new RequestFileTooLargeError()\n              err.part = this\n\n              onError(err)\n              fileChunks.length = 0\n            }\n          }\n          if (err) {\n            // throwing in the async iterator will\n            // cause the file.destroy() to be called\n            // The stream has already been managed by\n            // busboy instead\n            throw err\n          }\n          this._buf = Buffer.concat(fileChunks)\n          return this._buf\n        }\n      }\n\n      if (throwFileSizeLimit) {\n        file.on('limit', function () {\n          const err = new RequestFileTooLargeError()\n          err.part = value\n          onError(err)\n        })\n      }\n\n      if (body[name] === undefined) {\n        body[name] = value\n      } else if (Array.isArray(body[name])) {\n        body[name].push(value)\n      } else {\n        body[name] = [body[name], value]\n      }\n      currentFile = file\n      ch(value)\n    }\n\n    function onError (err) {\n      lastError = err\n      currentFile = null\n    }\n\n    function onEnd (err) {\n      cleanup()\n\n      ch(err || lastError)\n    }\n\n    function cleanup (err) {\n      request.unpipe(bb)\n      // in node 10 it seems that error handler is not called but request.aborted is set\n      if ((err || request.aborted) && currentFile) {\n        currentFile.destroy()\n      }\n    }\n\n    return parts\n  }\n\n  async function saveRequestFiles (options) {\n    let files\n    if (attachFieldsToBody === true) {\n      files = filesFromFields.call(this, this.body)\n    } else {\n      files = await this.files(options)\n    }\n    const requestFiles = []\n    const tmpdir = (options && options.tmpdir) || os.tmpdir()\n    this.tmpUploads = []\n    for await (const file of files) {\n      const filepath = path.join(tmpdir, toID() + path.extname(file.filename))\n      const target = createWriteStream(filepath)\n      try {\n        await pump(file.file, target)\n        requestFiles.push({ ...file, filepath })\n        this.tmpUploads.push(filepath)\n      } catch (err) {\n        this.log.error({ err }, 'save request file')\n        throw err\n      }\n    }\n\n    return requestFiles\n  }\n\n  function * filesFromFields (container) {\n    try {\n      for (const field of Object.values(container)) {\n        if (Array.isArray(field)) {\n          for (const subField of filesFromFields.call(this, field)) {\n            yield subField\n          }\n        }\n        if (!field.file) {\n          continue\n        }\n        if (!field._buf) {\n          throw new FileBufferNotFoundError()\n        }\n        field.file = Readable.from(field._buf)\n        yield field\n      }\n    } catch (err) {\n      this.log.error({ err }, 'save request file failed')\n      throw err\n    }\n  }\n\n  async function cleanRequestFiles () {\n    if (!this.tmpUploads) {\n      return\n    }\n    for (const filepath of this.tmpUploads) {\n      try {\n        await unlink(filepath)\n      } catch (error) {\n        this.log.error(error, 'could not delete file')\n      }\n    }\n  }\n\n  async function getMultipartFile (options) {\n    const parts = this[kMultipartHandler](options)\n    let part\n    while ((part = await parts()) != null) {\n      if (part.file) {\n        return part\n      }\n    }\n  }\n\n  async function * getMultipartFiles (options) {\n    const parts = this[kMultipartHandler](options)\n\n    let part\n    while ((part = await parts()) != null) {\n      if (part.file) {\n        yield part\n      }\n    }\n  }\n\n  async function * getMultipartIterator (options) {\n    const parts = this[kMultipartHandler](options)\n\n    let part\n    while ((part = await parts()) != null) {\n      yield part\n    }\n  }\n\n  done()\n}\n\n/**\n * These export configurations enable JS and TS developers\n * to consumer fastify in whatever way best suits their needs.\n */\nmodule.exports = fp(fastifyMultipart, {\n  fastify: '4.x',\n  name: '@fastify/multipart'\n})\nmodule.exports.default = fastifyMultipart\nmodule.exports.fastifyMultipart = fastifyMultipart\n", "'use strict'\n\nconst test = require('tap').test\nconst FormData = require('form-data')\nconst Fastify = require('fastify')\nconst multipart = require('..')\nconst http = require('http')\nconst stream = require('readable-stream')\nconst Readable = stream.Readable\nconst pump = stream.pipeline\nconst crypto = require('crypto')\nconst sendToWormhole = require('stream-wormhole')\n\n// skipping on Github Actions because it takes too long\ntest('should upload a big file in constant memory', { skip: process.env.CI }, function (t) {\n  t.plan(9)\n\n  const fastify = Fastify()\n  const hashInput = crypto.createHash('sha256')\n\n  t.teardown(fastify.close.bind(fastify))\n\n  fastify.register(multipart, {\n    limits: {\n      fileSize: Infinity,\n      parts: Infinity\n    }\n  })\n\n  fastify.post('/', async function (req, reply) {\n    t.ok(req.isMultipart())\n\n    for await (const part of req.parts()) {\n      if (part.file) {\n        t.equal(part.fieldname, 'upload')\n        t.equal(part.filename, 'random-data')\n        t.equal(part.encoding, '7bit')\n        t.equal(part.mimetype, 'binary/octet-stream')\n\n        await sendToWormhole(part.file)\n      }\n    }\n\n    const memory = process.memoryUsage()\n    t.ok(memory.rss < 400 * 1024 * 1024) // 200MB\n    t.ok(memory.heapTotal < 400 * 1024 * 1024) // 200MB\n\n    reply.send()\n  })\n\n  fastify.listen({ port: 0 }, function () {\n    const knownLength = 1024 * 1024 * 1024\n    let total = knownLength\n    const form = new FormData({ maxDataSize: total })\n    const rs = new Readable({\n      read (n) {\n        if (n > total) {\n          n = total\n        }\n\n        const buf = Buffer.alloc(n).fill('x')\n        hashInput.update(buf)\n        this.push(buf)\n\n        total -= n\n\n        if (total === 0) {\n          t.pass('finished generating')\n          hashInput.end()\n          this.push(null)\n        }\n      }\n    })\n    form.append('upload', rs, {\n      filename: 'random-data',\n      contentType: 'binary/octet-stream',\n      knownLength\n    })\n\n    const opts = {\n      protocol: 'http:',\n      hostname: 'localhost',\n      port: fastify.server.address().port,\n      path: '/',\n      headers: form.getHeaders(),\n      method: 'POST'\n    }\n\n    const req = http.request(opts, () => { fastify.close(noop) })\n\n    pump(form, req, function (err) {\n      t.error(err, 'client pump: no err')\n    })\n  })\n})\n\nfunction noop () { }\n", "'use strict'\n\nconst test = require('tap').test\nconst FormData = require('form-data')\nconst Fastify = require('fastify')\nconst multipart = require('./../..')\nconst http = require('http')\nconst stream = require('readable-stream')\nconst Readable = stream.Readable\nconst Writable = stream.Writable\nconst pump = stream.pipeline\nconst eos = stream.finished\nconst crypto = require('crypto')\n\n// skipping on Github Actions because it takes too long\ntest('should upload a big file in constant memory', { skip: process.env.CI }, function (t) {\n  t.plan(12)\n\n  const fastify = Fastify()\n  const hashInput = crypto.createHash('sha256')\n  let sent = false\n\n  t.teardown(fastify.close.bind(fastify))\n\n  fastify.register(multipart, {\n    limits: {\n      fileSize: Infinity,\n      parts: Infinity\n    }\n  })\n\n  fastify.post('/', function (req, reply) {\n    t.ok(req.isMultipart())\n\n    req.multipart(handler, function (err) {\n      t.error(err)\n    })\n\n    function handler (field, file, filename, encoding, mimetype) {\n      t.equal(filename, 'random-data')\n      t.equal(field, 'upload')\n      t.equal(encoding, '7bit')\n      t.equal(mimetype, 'binary/octet-stream')\n      const hashOutput = crypto.createHash('sha256')\n\n      pump(file, hashOutput, new Writable({\n        objectMode: true,\n        write (chunk, enc, cb) {\n          if (!sent) {\n            eos(hashInput, () => {\n              this._write(chunk, enc, cb)\n            })\n            return\n          }\n\n          t.equal(hashInput.digest('hex'), chunk.toString('hex'))\n          cb()\n        }\n      }), function (err) {\n        t.error(err)\n\n        const memory = process.memoryUsage()\n        t.ok(memory.rss < 400 * 1024 * 1024) // 200MB\n        t.ok(memory.heapTotal < 400 * 1024 * 1024) // 200MB\n        reply.send()\n      })\n    }\n  })\n\n  fastify.listen({ port: 0 }, function () {\n    const knownLength = 1024 * 1024 * 1024\n    let total = knownLength\n    const form = new FormData({ maxDataSize: total })\n    const rs = new Readable({\n      read (n) {\n        if (n > total) {\n          n = total\n        }\n\n        const buf = Buffer.alloc(n).fill('x')\n        hashInput.update(buf)\n        this.push(buf)\n\n        total -= n\n\n        if (total === 0) {\n          t.pass('finished generating')\n          sent = true\n          hashInput.end()\n          this.push(null)\n        }\n      }\n    })\n    form.append('upload', rs, {\n      filename: 'random-data',\n      contentType: 'binary/octet-stream',\n      knownLength\n    })\n\n    const opts = {\n      protocol: 'http:',\n      hostname: 'localhost',\n      port: fastify.server.address().port,\n      path: '/',\n      headers: form.getHeaders(),\n      method: 'POST'\n    }\n\n    const req = http.request(opts, () => { fastify.close(noop) })\n\n    pump(form, req, function (err) {\n      t.error(err, 'client pump: no err')\n    })\n  })\n})\n\nfunction noop () { }\n", "'use strict'\n\nconst test = require('tap').test\nconst FormData = require('form-data')\nconst Fastify = require('fastify')\nconst multipart = require('..')\nconst http = require('http')\nconst path = require('path')\nconst fs = require('fs')\nconst crypto = require('crypto')\nconst EventEmitter = require('events')\nconst { once } = EventEmitter\n\nconst filePath = path.join(__dirname, '../README.md')\n\ntest('should not allow __proto__ as file name', function (t) {\n  t.plan(4)\n\n  const fastify = Fastify()\n  t.teardown(fastify.close.bind(fastify))\n\n  fastify.register(multipart)\n\n  fastify.post('/', async function (req, reply) {\n    t.ok(req.isMultipart())\n\n    try {\n      await req.file()\n      reply.code(200).send()\n    } catch (error) {\n      t.ok(error instanceof fastify.multipartErrors.PrototypeViolationError)\n      reply.code(500).send()\n    }\n  })\n\n  fastify.listen({ port: 0 }, async function () {\n    // request\n    const form = new FormData()\n    const opts = {\n      protocol: 'http:',\n      hostname: 'localhost',\n      port: fastify.server.address().port,\n      path: '/',\n      headers: form.getHeaders(),\n      method: 'POST'\n    }\n\n    const req = http.request(opts, (res) => {\n      t.equal(res.statusCode, 500)\n      res.resume()\n      res.on('end', () => {\n        t.pass('res ended successfully')\n      })\n    })\n    const rs = fs.createReadStream(filePath)\n    form.append('__proto__', rs)\n\n    form.pipe(req)\n  })\n})\n\ntest('should not allow __proto__ as field name', function (t) {\n  t.plan(4)\n\n  const fastify = Fastify()\n  t.teardown(fastify.close.bind(fastify))\n\n  fastify.register(multipart)\n\n  fastify.post('/', async function (req, reply) {\n    t.ok(req.isMultipart())\n\n    try {\n      await req.file()\n      reply.code(200).send()\n    } catch (error) {\n      t.ok(error instanceof fastify.multipartErrors.PrototypeViolationError)\n      reply.code(500).send()\n    }\n  })\n\n  fastify.listen({ port: 0 }, async function () {\n    // request\n    const form = new FormData()\n    const opts = {\n      protocol: 'http:',\n      hostname: 'localhost',\n      port: fastify.server.address().port,\n      path: '/',\n      headers: form.getHeaders(),\n      method: 'POST'\n    }\n\n    const req = http.request(opts, (res) => {\n      t.equal(res.statusCode, 500)\n      res.resume()\n      res.on('end', () => {\n        t.pass('res ended successfully')\n      })\n    })\n    form.append('__proto__', 'world')\n\n    form.pipe(req)\n  })\n})\n\ntest('should use default for fileSize', async function (t) {\n  t.plan(4)\n\n  const fastify = Fastify()\n  t.teardown(fastify.close.bind(fastify))\n\n  fastify.register(multipart, {\n    throwFileSizeLimit: true\n  })\n\n  fastify.post('/', async function (req, reply) {\n    t.ok(req.isMultipart())\n\n    const part = await req.file()\n    t.pass('the file is not consumed yet')\n\n    try {\n      await part.toBuffer()\n      t.fail('it should throw')\n    } catch (error) {\n      t.ok(error)\n      reply.send(error)\n    }\n  })\n\n  await fastify.listen({ port: 0 })\n\n  // request\n  const form = new FormData()\n  const opts = {\n    hostname: '127.0.0.1',\n    port: fastify.server.address().port,\n    path: '/',\n    headers: form.getHeaders(),\n    method: 'POST'\n  }\n\n  const randomFileBuffer = Buffer.alloc(15_000_000)\n  crypto.randomFillSync(randomFileBuffer)\n\n  const req = http.request(opts)\n  form.append('upload', randomFileBuffer)\n\n  form.pipe(req)\n\n  try {\n    const [res] = await once(req, 'response')\n    t.equal(res.statusCode, 413)\n    res.resume()\n    await once(res, 'end')\n  } catch (error) {\n    t.error(error, 'request')\n  }\n})\n\ntest('should use default for parts - 1000', function (t) {\n  const fastify = Fastify()\n  t.teardown(fastify.close.bind(fastify))\n\n  fastify.register(multipart)\n\n  fastify.post('/', async function (req, reply) {\n    try {\n      // eslint-disable-next-lint no-empty\n      for await (const _ of req.parts()) { console.assert(_) }\n      t.fail('should throw on 1001')\n      reply.code(200).send()\n    } catch (error) {\n      t.ok(error instanceof fastify.multipartErrors.PartsLimitError)\n      reply.code(500).send()\n    }\n  })\n\n  fastify.listen({ port: 0 }, async function () {\n    // request\n    const form = new FormData()\n    const opts = {\n      protocol: 'http:',\n      hostname: 'localhost',\n      port: fastify.server.address().port,\n      path: '/',\n      headers: form.getHeaders(),\n      method: 'POST'\n    }\n\n    const req = http.request(opts, (res) => {\n      t.equal(res.statusCode, 500)\n      res.resume()\n      res.on('end', () => {\n        t.pass('res ended successfully')\n        t.end()\n      })\n    })\n    for (let i = 0; i < 1000; ++i) {\n      form.append('hello' + i, 'world')\n    }\n    // Exceeds the default limit (1000)\n    form.append('hello', 'world')\n\n    form.pipe(req)\n  })\n})\n"], "filenames": ["README.md", "index.js", "test/big.test.js", "test/legacy/big.test.js", "test/multipart-security.test.js"], "buggy_code_start_loc": [86, 112, 23, 25, 9], "buggy_code_end_loc": [89, 372, 24, 26, 102], "fixing_code_start_loc": [87, 113, 23, 25, 10], "fixing_code_end_loc": [93, 384, 29, 31, 209], "type": "CWE-770", "message": "@fastify/multipart is a Fastify plugin to parse the multipart content-type. Prior to versions 7.4.1 and 6.0.1, @fastify/multipart may experience denial of service due to a number of situations in which an unlimited number of parts are accepted. This includes the multipart body parser accepting an unlimited number of file parts, the multipart body parser accepting an unlimited number of field parts, and the multipart body parser accepting an unlimited number of empty parts as field parts. This is fixed in v7.4.1 (for Fastify v4.x) and v6.0.1 (for Fastify v3.x). There are no known workarounds.", "other": {"cve": {"id": "CVE-2023-25576", "sourceIdentifier": "security-advisories@github.com", "published": "2023-02-14T16:15:11.277", "lastModified": "2023-02-22T17:12:10.547", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "@fastify/multipart is a Fastify plugin to parse the multipart content-type. Prior to versions 7.4.1 and 6.0.1, @fastify/multipart may experience denial of service due to a number of situations in which an unlimited number of parts are accepted. This includes the multipart body parser accepting an unlimited number of file parts, the multipart body parser accepting an unlimited number of field parts, and the multipart body parser accepting an unlimited number of empty parts as field parts. This is fixed in v7.4.1 (for Fastify v4.x) and v6.0.1 (for Fastify v3.x). There are no known workarounds."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-770"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-770"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:fastify:fastify-multipart:*:*:*:*:*:fastify:*:*", "versionEndExcluding": "6.0.1", "matchCriteriaId": "E689CBF7-01C4-4ED8-96C5-AC2FB4604BBB"}, {"vulnerable": true, "criteria": "cpe:2.3:a:fastify:fastify-multipart:*:*:*:*:*:fastify:*:*", "versionStartIncluding": "7.0.0", "versionEndExcluding": "7.4.1", "matchCriteriaId": "8701FBAF-4608-4963-89A2-6B54AAD7DD30"}]}]}], "references": [{"url": "https://github.com/fastify/fastify-multipart/commit/85be81bedf5b29cfd9fe3efc30fb5a17173c1297", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/fastify/fastify-multipart/releases/tag/v6.0.1", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/fastify/fastify-multipart/releases/tag/v7.4.1", "source": "security-advisories@github.com", "tags": ["Release Notes", "Third Party Advisory"]}, {"url": "https://github.com/fastify/fastify-multipart/security/advisories/GHSA-hpp2-2cr5-pf6g", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://hackerone.com/reports/1816195", "source": "security-advisories@github.com", "tags": ["Permissions Required", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/fastify/fastify-multipart/commit/85be81bedf5b29cfd9fe3efc30fb5a17173c1297"}}