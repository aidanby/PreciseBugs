{"buggy_code": ["/*\n * fs/inotify_user.c - inotify support for userspace\n *\n * Authors:\n *\tJohn McCutchan\t<ttb@tentacle.dhs.org>\n *\tRobert Love\t<rml@novell.com>\n *\n * Copyright (C) 2005 John McCutchan\n * Copyright 2006 Hewlett-Packard Development Company, L.P.\n *\n * Copyright (C) 2009 Eric Paris <Red Hat Inc>\n * inotify was largely rewriten to make use of the fsnotify infrastructure\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the\n * Free Software Foundation; either version 2, or (at your option) any\n * later version.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n */\n\n#include <linux/file.h>\n#include <linux/fs.h> /* struct inode */\n#include <linux/fsnotify_backend.h>\n#include <linux/idr.h>\n#include <linux/init.h> /* module_init */\n#include <linux/inotify.h>\n#include <linux/kernel.h> /* roundup() */\n#include <linux/namei.h> /* LOOKUP_FOLLOW */\n#include <linux/sched.h> /* struct user */\n#include <linux/slab.h> /* struct kmem_cache */\n#include <linux/syscalls.h>\n#include <linux/types.h>\n#include <linux/anon_inodes.h>\n#include <linux/uaccess.h>\n#include <linux/poll.h>\n#include <linux/wait.h>\n\n#include \"inotify.h\"\n\n#include <asm/ioctls.h>\n\n/* these are configurable via /proc/sys/fs/inotify/ */\nstatic int inotify_max_user_instances __read_mostly;\nstatic int inotify_max_queued_events __read_mostly;\nstatic int inotify_max_user_watches __read_mostly;\n\nstatic struct kmem_cache *inotify_inode_mark_cachep __read_mostly;\nstruct kmem_cache *event_priv_cachep __read_mostly;\n\n#ifdef CONFIG_SYSCTL\n\n#include <linux/sysctl.h>\n\nstatic int zero;\n\nctl_table inotify_table[] = {\n\t{\n\t\t.procname\t= \"max_user_instances\",\n\t\t.data\t\t= &inotify_max_user_instances,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"max_user_watches\",\n\t\t.data\t\t= &inotify_max_user_watches,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"max_queued_events\",\n\t\t.data\t\t= &inotify_max_queued_events,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero\n\t},\n\t{ }\n};\n#endif /* CONFIG_SYSCTL */\n\nstatic inline __u32 inotify_arg_to_mask(u32 arg)\n{\n\t__u32 mask;\n\n\t/*\n\t * everything should accept their own ignored, cares about children,\n\t * and should receive events when the inode is unmounted\n\t */\n\tmask = (FS_IN_IGNORED | FS_EVENT_ON_CHILD | FS_UNMOUNT);\n\n\t/* mask off the flags used to open the fd */\n\tmask |= (arg & (IN_ALL_EVENTS | IN_ONESHOT | IN_EXCL_UNLINK));\n\n\treturn mask;\n}\n\nstatic inline u32 inotify_mask_to_arg(__u32 mask)\n{\n\treturn mask & (IN_ALL_EVENTS | IN_ISDIR | IN_UNMOUNT | IN_IGNORED |\n\t\t       IN_Q_OVERFLOW);\n}\n\n/* intofiy userspace file descriptor functions */\nstatic unsigned int inotify_poll(struct file *file, poll_table *wait)\n{\n\tstruct fsnotify_group *group = file->private_data;\n\tint ret = 0;\n\n\tpoll_wait(file, &group->notification_waitq, wait);\n\tmutex_lock(&group->notification_mutex);\n\tif (!fsnotify_notify_queue_is_empty(group))\n\t\tret = POLLIN | POLLRDNORM;\n\tmutex_unlock(&group->notification_mutex);\n\n\treturn ret;\n}\n\n/*\n * Get an inotify_kernel_event if one exists and is small\n * enough to fit in \"count\". Return an error pointer if\n * not large enough.\n *\n * Called with the group->notification_mutex held.\n */\nstatic struct fsnotify_event *get_one_event(struct fsnotify_group *group,\n\t\t\t\t\t    size_t count)\n{\n\tsize_t event_size = sizeof(struct inotify_event);\n\tstruct fsnotify_event *event;\n\n\tif (fsnotify_notify_queue_is_empty(group))\n\t\treturn NULL;\n\n\tevent = fsnotify_peek_notify_event(group);\n\n\tpr_debug(\"%s: group=%p event=%p\\n\", __func__, group, event);\n\n\tif (event->name_len)\n\t\tevent_size += roundup(event->name_len + 1, event_size);\n\n\tif (event_size > count)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* held the notification_mutex the whole time, so this is the\n\t * same event we peeked above */\n\tfsnotify_remove_notify_event(group);\n\n\treturn event;\n}\n\n/*\n * Copy an event to user space, returning how much we copied.\n *\n * We already checked that the event size is smaller than the\n * buffer we had in \"get_one_event()\" above.\n */\nstatic ssize_t copy_event_to_user(struct fsnotify_group *group,\n\t\t\t\t  struct fsnotify_event *event,\n\t\t\t\t  char __user *buf)\n{\n\tstruct inotify_event inotify_event;\n\tstruct fsnotify_event_private_data *fsn_priv;\n\tstruct inotify_event_private_data *priv;\n\tsize_t event_size = sizeof(struct inotify_event);\n\tsize_t name_len = 0;\n\n\tpr_debug(\"%s: group=%p event=%p\\n\", __func__, group, event);\n\n\t/* we get the inotify watch descriptor from the event private data */\n\tspin_lock(&event->lock);\n\tfsn_priv = fsnotify_remove_priv_from_event(group, event);\n\tspin_unlock(&event->lock);\n\n\tif (!fsn_priv)\n\t\tinotify_event.wd = -1;\n\telse {\n\t\tpriv = container_of(fsn_priv, struct inotify_event_private_data,\n\t\t\t\t    fsnotify_event_priv_data);\n\t\tinotify_event.wd = priv->wd;\n\t\tinotify_free_event_priv(fsn_priv);\n\t}\n\n\t/*\n\t * round up event->name_len so it is a multiple of event_size\n\t * plus an extra byte for the terminating '\\0'.\n\t */\n\tif (event->name_len)\n\t\tname_len = roundup(event->name_len + 1, event_size);\n\tinotify_event.len = name_len;\n\n\tinotify_event.mask = inotify_mask_to_arg(event->mask);\n\tinotify_event.cookie = event->sync_cookie;\n\n\t/* send the main event */\n\tif (copy_to_user(buf, &inotify_event, event_size))\n\t\treturn -EFAULT;\n\n\tbuf += event_size;\n\n\t/*\n\t * fsnotify only stores the pathname, so here we have to send the pathname\n\t * and then pad that pathname out to a multiple of sizeof(inotify_event)\n\t * with zeros.  I get my zeros from the nul_inotify_event.\n\t */\n\tif (name_len) {\n\t\tunsigned int len_to_zero = name_len - event->name_len;\n\t\t/* copy the path name */\n\t\tif (copy_to_user(buf, event->file_name, event->name_len))\n\t\t\treturn -EFAULT;\n\t\tbuf += event->name_len;\n\n\t\t/* fill userspace with 0's */\n\t\tif (clear_user(buf, len_to_zero))\n\t\t\treturn -EFAULT;\n\t\tbuf += len_to_zero;\n\t\tevent_size += name_len;\n\t}\n\n\treturn event_size;\n}\n\nstatic ssize_t inotify_read(struct file *file, char __user *buf,\n\t\t\t    size_t count, loff_t *pos)\n{\n\tstruct fsnotify_group *group;\n\tstruct fsnotify_event *kevent;\n\tchar __user *start;\n\tint ret;\n\tDEFINE_WAIT(wait);\n\n\tstart = buf;\n\tgroup = file->private_data;\n\n\twhile (1) {\n\t\tprepare_to_wait(&group->notification_waitq, &wait, TASK_INTERRUPTIBLE);\n\n\t\tmutex_lock(&group->notification_mutex);\n\t\tkevent = get_one_event(group, count);\n\t\tmutex_unlock(&group->notification_mutex);\n\n\t\tpr_debug(\"%s: group=%p kevent=%p\\n\", __func__, group, kevent);\n\n\t\tif (kevent) {\n\t\t\tret = PTR_ERR(kevent);\n\t\t\tif (IS_ERR(kevent))\n\t\t\t\tbreak;\n\t\t\tret = copy_event_to_user(group, kevent, buf);\n\t\t\tfsnotify_put_event(kevent);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t\tbuf += ret;\n\t\t\tcount -= ret;\n\t\t\tcontinue;\n\t\t}\n\n\t\tret = -EAGAIN;\n\t\tif (file->f_flags & O_NONBLOCK)\n\t\t\tbreak;\n\t\tret = -EINTR;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tif (start != buf)\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\n\n\tfinish_wait(&group->notification_waitq, &wait);\n\tif (start != buf && ret != -EFAULT)\n\t\tret = buf - start;\n\treturn ret;\n}\n\nstatic int inotify_fasync(int fd, struct file *file, int on)\n{\n\tstruct fsnotify_group *group = file->private_data;\n\n\treturn fasync_helper(fd, file, on, &group->inotify_data.fa) >= 0 ? 0 : -EIO;\n}\n\nstatic int inotify_release(struct inode *ignored, struct file *file)\n{\n\tstruct fsnotify_group *group = file->private_data;\n\tstruct user_struct *user = group->inotify_data.user;\n\n\tpr_debug(\"%s: group=%p\\n\", __func__, group);\n\n\tfsnotify_clear_marks_by_group(group);\n\n\t/* free this group, matching get was inotify_init->fsnotify_obtain_group */\n\tfsnotify_put_group(group);\n\n\tatomic_dec(&user->inotify_devs);\n\n\treturn 0;\n}\n\nstatic long inotify_ioctl(struct file *file, unsigned int cmd,\n\t\t\t  unsigned long arg)\n{\n\tstruct fsnotify_group *group;\n\tstruct fsnotify_event_holder *holder;\n\tstruct fsnotify_event *event;\n\tvoid __user *p;\n\tint ret = -ENOTTY;\n\tsize_t send_len = 0;\n\n\tgroup = file->private_data;\n\tp = (void __user *) arg;\n\n\tpr_debug(\"%s: group=%p cmd=%u\\n\", __func__, group, cmd);\n\n\tswitch (cmd) {\n\tcase FIONREAD:\n\t\tmutex_lock(&group->notification_mutex);\n\t\tlist_for_each_entry(holder, &group->notification_list, event_list) {\n\t\t\tevent = holder->event;\n\t\t\tsend_len += sizeof(struct inotify_event);\n\t\t\tif (event->name_len)\n\t\t\t\tsend_len += roundup(event->name_len + 1,\n\t\t\t\t\t\tsizeof(struct inotify_event));\n\t\t}\n\t\tmutex_unlock(&group->notification_mutex);\n\t\tret = put_user(send_len, (int __user *) p);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct file_operations inotify_fops = {\n\t.poll\t\t= inotify_poll,\n\t.read\t\t= inotify_read,\n\t.fasync\t\t= inotify_fasync,\n\t.release\t= inotify_release,\n\t.unlocked_ioctl\t= inotify_ioctl,\n\t.compat_ioctl\t= inotify_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\n\n/*\n * find_inode - resolve a user-given path to a specific inode\n */\nstatic int inotify_find_inode(const char __user *dirname, struct path *path, unsigned flags)\n{\n\tint error;\n\n\terror = user_path_at(AT_FDCWD, dirname, flags, path);\n\tif (error)\n\t\treturn error;\n\t/* you can only watch an inode if you have read permissions on it */\n\terror = inode_permission(path->dentry->d_inode, MAY_READ);\n\tif (error)\n\t\tpath_put(path);\n\treturn error;\n}\n\nstatic int inotify_add_to_idr(struct idr *idr, spinlock_t *idr_lock,\n\t\t\t      int *last_wd,\n\t\t\t      struct inotify_inode_mark *i_mark)\n{\n\tint ret;\n\n\tdo {\n\t\tif (unlikely(!idr_pre_get(idr, GFP_KERNEL)))\n\t\t\treturn -ENOMEM;\n\n\t\tspin_lock(idr_lock);\n\t\tret = idr_get_new_above(idr, i_mark, *last_wd + 1,\n\t\t\t\t\t&i_mark->wd);\n\t\t/* we added the mark to the idr, take a reference */\n\t\tif (!ret) {\n\t\t\t*last_wd = i_mark->wd;\n\t\t\tfsnotify_get_mark(&i_mark->fsn_mark);\n\t\t}\n\t\tspin_unlock(idr_lock);\n\t} while (ret == -EAGAIN);\n\n\treturn ret;\n}\n\nstatic struct inotify_inode_mark *inotify_idr_find_locked(struct fsnotify_group *group,\n\t\t\t\t\t\t\t\tint wd)\n{\n\tstruct idr *idr = &group->inotify_data.idr;\n\tspinlock_t *idr_lock = &group->inotify_data.idr_lock;\n\tstruct inotify_inode_mark *i_mark;\n\n\tassert_spin_locked(idr_lock);\n\n\ti_mark = idr_find(idr, wd);\n\tif (i_mark) {\n\t\tstruct fsnotify_mark *fsn_mark = &i_mark->fsn_mark;\n\n\t\tfsnotify_get_mark(fsn_mark);\n\t\t/* One ref for being in the idr, one ref we just took */\n\t\tBUG_ON(atomic_read(&fsn_mark->refcnt) < 2);\n\t}\n\n\treturn i_mark;\n}\n\nstatic struct inotify_inode_mark *inotify_idr_find(struct fsnotify_group *group,\n\t\t\t\t\t\t\t int wd)\n{\n\tstruct inotify_inode_mark *i_mark;\n\tspinlock_t *idr_lock = &group->inotify_data.idr_lock;\n\n\tspin_lock(idr_lock);\n\ti_mark = inotify_idr_find_locked(group, wd);\n\tspin_unlock(idr_lock);\n\n\treturn i_mark;\n}\n\nstatic void do_inotify_remove_from_idr(struct fsnotify_group *group,\n\t\t\t\t       struct inotify_inode_mark *i_mark)\n{\n\tstruct idr *idr = &group->inotify_data.idr;\n\tspinlock_t *idr_lock = &group->inotify_data.idr_lock;\n\tint wd = i_mark->wd;\n\n\tassert_spin_locked(idr_lock);\n\n\tidr_remove(idr, wd);\n\n\t/* removed from the idr, drop that ref */\n\tfsnotify_put_mark(&i_mark->fsn_mark);\n}\n\n/*\n * Remove the mark from the idr (if present) and drop the reference\n * on the mark because it was in the idr.\n */\nstatic void inotify_remove_from_idr(struct fsnotify_group *group,\n\t\t\t\t    struct inotify_inode_mark *i_mark)\n{\n\tspinlock_t *idr_lock = &group->inotify_data.idr_lock;\n\tstruct inotify_inode_mark *found_i_mark = NULL;\n\tint wd;\n\n\tspin_lock(idr_lock);\n\twd = i_mark->wd;\n\n\t/*\n\t * does this i_mark think it is in the idr?  we shouldn't get called\n\t * if it wasn't....\n\t */\n\tif (wd == -1) {\n\t\tWARN_ONCE(1, \"%s: i_mark=%p i_mark->wd=%d i_mark->group=%p\"\n\t\t\t\" i_mark->inode=%p\\n\", __func__, i_mark, i_mark->wd,\n\t\t\ti_mark->fsn_mark.group, i_mark->fsn_mark.i.inode);\n\t\tgoto out;\n\t}\n\n\t/* Lets look in the idr to see if we find it */\n\tfound_i_mark = inotify_idr_find_locked(group, wd);\n\tif (unlikely(!found_i_mark)) {\n\t\tWARN_ONCE(1, \"%s: i_mark=%p i_mark->wd=%d i_mark->group=%p\"\n\t\t\t\" i_mark->inode=%p\\n\", __func__, i_mark, i_mark->wd,\n\t\t\ti_mark->fsn_mark.group, i_mark->fsn_mark.i.inode);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * We found an mark in the idr at the right wd, but it's\n\t * not the mark we were told to remove.  eparis seriously\n\t * fucked up somewhere.\n\t */\n\tif (unlikely(found_i_mark != i_mark)) {\n\t\tWARN_ONCE(1, \"%s: i_mark=%p i_mark->wd=%d i_mark->group=%p \"\n\t\t\t\"mark->inode=%p found_i_mark=%p found_i_mark->wd=%d \"\n\t\t\t\"found_i_mark->group=%p found_i_mark->inode=%p\\n\",\n\t\t\t__func__, i_mark, i_mark->wd, i_mark->fsn_mark.group,\n\t\t\ti_mark->fsn_mark.i.inode, found_i_mark, found_i_mark->wd,\n\t\t\tfound_i_mark->fsn_mark.group,\n\t\t\tfound_i_mark->fsn_mark.i.inode);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * One ref for being in the idr\n\t * one ref held by the caller trying to kill us\n\t * one ref grabbed by inotify_idr_find\n\t */\n\tif (unlikely(atomic_read(&i_mark->fsn_mark.refcnt) < 3)) {\n\t\tprintk(KERN_ERR \"%s: i_mark=%p i_mark->wd=%d i_mark->group=%p\"\n\t\t\t\" i_mark->inode=%p\\n\", __func__, i_mark, i_mark->wd,\n\t\t\ti_mark->fsn_mark.group, i_mark->fsn_mark.i.inode);\n\t\t/* we can't really recover with bad ref cnting.. */\n\t\tBUG();\n\t}\n\n\tdo_inotify_remove_from_idr(group, i_mark);\nout:\n\t/* match the ref taken by inotify_idr_find_locked() */\n\tif (found_i_mark)\n\t\tfsnotify_put_mark(&found_i_mark->fsn_mark);\n\ti_mark->wd = -1;\n\tspin_unlock(idr_lock);\n}\n\n/*\n * Send IN_IGNORED for this wd, remove this wd from the idr.\n */\nvoid inotify_ignored_and_remove_idr(struct fsnotify_mark *fsn_mark,\n\t\t\t\t    struct fsnotify_group *group)\n{\n\tstruct inotify_inode_mark *i_mark;\n\tstruct fsnotify_event *ignored_event, *notify_event;\n\tstruct inotify_event_private_data *event_priv;\n\tstruct fsnotify_event_private_data *fsn_event_priv;\n\tint ret;\n\n\tignored_event = fsnotify_create_event(NULL, FS_IN_IGNORED, NULL,\n\t\t\t\t\t      FSNOTIFY_EVENT_NONE, NULL, 0,\n\t\t\t\t\t      GFP_NOFS);\n\tif (!ignored_event)\n\t\treturn;\n\n\ti_mark = container_of(fsn_mark, struct inotify_inode_mark, fsn_mark);\n\n\tevent_priv = kmem_cache_alloc(event_priv_cachep, GFP_NOFS);\n\tif (unlikely(!event_priv))\n\t\tgoto skip_send_ignore;\n\n\tfsn_event_priv = &event_priv->fsnotify_event_priv_data;\n\n\tfsn_event_priv->group = group;\n\tevent_priv->wd = i_mark->wd;\n\n\tnotify_event = fsnotify_add_notify_event(group, ignored_event, fsn_event_priv, NULL);\n\tif (notify_event) {\n\t\tif (IS_ERR(notify_event))\n\t\t\tret = PTR_ERR(notify_event);\n\t\telse\n\t\t\tfsnotify_put_event(notify_event);\n\t\tinotify_free_event_priv(fsn_event_priv);\n\t}\n\nskip_send_ignore:\n\n\t/* matches the reference taken when the event was created */\n\tfsnotify_put_event(ignored_event);\n\n\t/* remove this mark from the idr */\n\tinotify_remove_from_idr(group, i_mark);\n\n\tatomic_dec(&group->inotify_data.user->inotify_watches);\n}\n\n/* ding dong the mark is dead */\nstatic void inotify_free_mark(struct fsnotify_mark *fsn_mark)\n{\n\tstruct inotify_inode_mark *i_mark;\n\n\ti_mark = container_of(fsn_mark, struct inotify_inode_mark, fsn_mark);\n\n\tkmem_cache_free(inotify_inode_mark_cachep, i_mark);\n}\n\nstatic int inotify_update_existing_watch(struct fsnotify_group *group,\n\t\t\t\t\t struct inode *inode,\n\t\t\t\t\t u32 arg)\n{\n\tstruct fsnotify_mark *fsn_mark;\n\tstruct inotify_inode_mark *i_mark;\n\t__u32 old_mask, new_mask;\n\t__u32 mask;\n\tint add = (arg & IN_MASK_ADD);\n\tint ret;\n\n\t/* don't allow invalid bits: we don't want flags set */\n\tmask = inotify_arg_to_mask(arg);\n\tif (unlikely(!(mask & IN_ALL_EVENTS)))\n\t\treturn -EINVAL;\n\n\tfsn_mark = fsnotify_find_inode_mark(group, inode);\n\tif (!fsn_mark)\n\t\treturn -ENOENT;\n\n\ti_mark = container_of(fsn_mark, struct inotify_inode_mark, fsn_mark);\n\n\tspin_lock(&fsn_mark->lock);\n\n\told_mask = fsn_mark->mask;\n\tif (add)\n\t\tfsnotify_set_mark_mask_locked(fsn_mark, (fsn_mark->mask | mask));\n\telse\n\t\tfsnotify_set_mark_mask_locked(fsn_mark, mask);\n\tnew_mask = fsn_mark->mask;\n\n\tspin_unlock(&fsn_mark->lock);\n\n\tif (old_mask != new_mask) {\n\t\t/* more bits in old than in new? */\n\t\tint dropped = (old_mask & ~new_mask);\n\t\t/* more bits in this fsn_mark than the inode's mask? */\n\t\tint do_inode = (new_mask & ~inode->i_fsnotify_mask);\n\n\t\t/* update the inode with this new fsn_mark */\n\t\tif (dropped || do_inode)\n\t\t\tfsnotify_recalc_inode_mask(inode);\n\n\t}\n\n\t/* return the wd */\n\tret = i_mark->wd;\n\n\t/* match the get from fsnotify_find_mark() */\n\tfsnotify_put_mark(fsn_mark);\n\n\treturn ret;\n}\n\nstatic int inotify_new_watch(struct fsnotify_group *group,\n\t\t\t     struct inode *inode,\n\t\t\t     u32 arg)\n{\n\tstruct inotify_inode_mark *tmp_i_mark;\n\t__u32 mask;\n\tint ret;\n\tstruct idr *idr = &group->inotify_data.idr;\n\tspinlock_t *idr_lock = &group->inotify_data.idr_lock;\n\n\t/* don't allow invalid bits: we don't want flags set */\n\tmask = inotify_arg_to_mask(arg);\n\tif (unlikely(!(mask & IN_ALL_EVENTS)))\n\t\treturn -EINVAL;\n\n\ttmp_i_mark = kmem_cache_alloc(inotify_inode_mark_cachep, GFP_KERNEL);\n\tif (unlikely(!tmp_i_mark))\n\t\treturn -ENOMEM;\n\n\tfsnotify_init_mark(&tmp_i_mark->fsn_mark, inotify_free_mark);\n\ttmp_i_mark->fsn_mark.mask = mask;\n\ttmp_i_mark->wd = -1;\n\n\tret = -ENOSPC;\n\tif (atomic_read(&group->inotify_data.user->inotify_watches) >= inotify_max_user_watches)\n\t\tgoto out_err;\n\n\tret = inotify_add_to_idr(idr, idr_lock, &group->inotify_data.last_wd,\n\t\t\t\t tmp_i_mark);\n\tif (ret)\n\t\tgoto out_err;\n\n\t/* we are on the idr, now get on the inode */\n\tret = fsnotify_add_mark(&tmp_i_mark->fsn_mark, group, inode, NULL, 0);\n\tif (ret) {\n\t\t/* we failed to get on the inode, get off the idr */\n\t\tinotify_remove_from_idr(group, tmp_i_mark);\n\t\tgoto out_err;\n\t}\n\n\t/* increment the number of watches the user has */\n\tatomic_inc(&group->inotify_data.user->inotify_watches);\n\n\t/* return the watch descriptor for this new mark */\n\tret = tmp_i_mark->wd;\n\nout_err:\n\t/* match the ref from fsnotify_init_mark() */\n\tfsnotify_put_mark(&tmp_i_mark->fsn_mark);\n\n\treturn ret;\n}\n\nstatic int inotify_update_watch(struct fsnotify_group *group, struct inode *inode, u32 arg)\n{\n\tint ret = 0;\n\nretry:\n\t/* try to update and existing watch with the new arg */\n\tret = inotify_update_existing_watch(group, inode, arg);\n\t/* no mark present, try to add a new one */\n\tif (ret == -ENOENT)\n\t\tret = inotify_new_watch(group, inode, arg);\n\t/*\n\t * inotify_new_watch could race with another thread which did an\n\t * inotify_new_watch between the update_existing and the add watch\n\t * here, go back and try to update an existing mark again.\n\t */\n\tif (ret == -EEXIST)\n\t\tgoto retry;\n\n\treturn ret;\n}\n\nstatic struct fsnotify_group *inotify_new_group(struct user_struct *user, unsigned int max_events)\n{\n\tstruct fsnotify_group *group;\n\n\tgroup = fsnotify_alloc_group(&inotify_fsnotify_ops);\n\tif (IS_ERR(group))\n\t\treturn group;\n\n\tgroup->max_events = max_events;\n\n\tspin_lock_init(&group->inotify_data.idr_lock);\n\tidr_init(&group->inotify_data.idr);\n\tgroup->inotify_data.last_wd = 0;\n\tgroup->inotify_data.user = user;\n\tgroup->inotify_data.fa = NULL;\n\n\treturn group;\n}\n\n\n/* inotify syscalls */\nSYSCALL_DEFINE1(inotify_init1, int, flags)\n{\n\tstruct fsnotify_group *group;\n\tstruct user_struct *user;\n\tint ret;\n\n\t/* Check the IN_* constants for consistency.  */\n\tBUILD_BUG_ON(IN_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON(IN_NONBLOCK != O_NONBLOCK);\n\n\tif (flags & ~(IN_CLOEXEC | IN_NONBLOCK))\n\t\treturn -EINVAL;\n\n\tuser = get_current_user();\n\tif (unlikely(atomic_read(&user->inotify_devs) >=\n\t\t\tinotify_max_user_instances)) {\n\t\tret = -EMFILE;\n\t\tgoto out_free_uid;\n\t}\n\n\t/* fsnotify_obtain_group took a reference to group, we put this when we kill the file in the end */\n\tgroup = inotify_new_group(user, inotify_max_queued_events);\n\tif (IS_ERR(group)) {\n\t\tret = PTR_ERR(group);\n\t\tgoto out_free_uid;\n\t}\n\n\tatomic_inc(&user->inotify_devs);\n\n\tret = anon_inode_getfd(\"inotify\", &inotify_fops, group,\n\t\t\t\t  O_RDONLY | flags);\n\tif (ret >= 0)\n\t\treturn ret;\n\n\tatomic_dec(&user->inotify_devs);\nout_free_uid:\n\tfree_uid(user);\n\treturn ret;\n}\n\nSYSCALL_DEFINE0(inotify_init)\n{\n\treturn sys_inotify_init1(0);\n}\n\nSYSCALL_DEFINE3(inotify_add_watch, int, fd, const char __user *, pathname,\n\t\tu32, mask)\n{\n\tstruct fsnotify_group *group;\n\tstruct inode *inode;\n\tstruct path path;\n\tstruct file *filp;\n\tint ret, fput_needed;\n\tunsigned flags = 0;\n\n\tfilp = fget_light(fd, &fput_needed);\n\tif (unlikely(!filp))\n\t\treturn -EBADF;\n\n\t/* verify that this is indeed an inotify instance */\n\tif (unlikely(filp->f_op != &inotify_fops)) {\n\t\tret = -EINVAL;\n\t\tgoto fput_and_out;\n\t}\n\n\tif (!(mask & IN_DONT_FOLLOW))\n\t\tflags |= LOOKUP_FOLLOW;\n\tif (mask & IN_ONLYDIR)\n\t\tflags |= LOOKUP_DIRECTORY;\n\n\tret = inotify_find_inode(pathname, &path, flags);\n\tif (ret)\n\t\tgoto fput_and_out;\n\n\t/* inode held in place by reference to path; group by fget on fd */\n\tinode = path.dentry->d_inode;\n\tgroup = filp->private_data;\n\n\t/* create/update an inode mark */\n\tret = inotify_update_watch(group, inode, mask);\n\tpath_put(&path);\nfput_and_out:\n\tfput_light(filp, fput_needed);\n\treturn ret;\n}\n\nSYSCALL_DEFINE2(inotify_rm_watch, int, fd, __s32, wd)\n{\n\tstruct fsnotify_group *group;\n\tstruct inotify_inode_mark *i_mark;\n\tstruct file *filp;\n\tint ret = 0, fput_needed;\n\n\tfilp = fget_light(fd, &fput_needed);\n\tif (unlikely(!filp))\n\t\treturn -EBADF;\n\n\t/* verify that this is indeed an inotify instance */\n\tret = -EINVAL;\n\tif (unlikely(filp->f_op != &inotify_fops))\n\t\tgoto out;\n\n\tgroup = filp->private_data;\n\n\tret = -EINVAL;\n\ti_mark = inotify_idr_find(group, wd);\n\tif (unlikely(!i_mark))\n\t\tgoto out;\n\n\tret = 0;\n\n\tfsnotify_destroy_mark(&i_mark->fsn_mark);\n\n\t/* match ref taken by inotify_idr_find */\n\tfsnotify_put_mark(&i_mark->fsn_mark);\n\nout:\n\tfput_light(filp, fput_needed);\n\treturn ret;\n}\n\n/*\n * inotify_user_setup - Our initialization function.  Note that we cannnot return\n * error because we have compiled-in VFS hooks.  So an (unlikely) failure here\n * must result in panic().\n */\nstatic int __init inotify_user_setup(void)\n{\n\tBUILD_BUG_ON(IN_ACCESS != FS_ACCESS);\n\tBUILD_BUG_ON(IN_MODIFY != FS_MODIFY);\n\tBUILD_BUG_ON(IN_ATTRIB != FS_ATTRIB);\n\tBUILD_BUG_ON(IN_CLOSE_WRITE != FS_CLOSE_WRITE);\n\tBUILD_BUG_ON(IN_CLOSE_NOWRITE != FS_CLOSE_NOWRITE);\n\tBUILD_BUG_ON(IN_OPEN != FS_OPEN);\n\tBUILD_BUG_ON(IN_MOVED_FROM != FS_MOVED_FROM);\n\tBUILD_BUG_ON(IN_MOVED_TO != FS_MOVED_TO);\n\tBUILD_BUG_ON(IN_CREATE != FS_CREATE);\n\tBUILD_BUG_ON(IN_DELETE != FS_DELETE);\n\tBUILD_BUG_ON(IN_DELETE_SELF != FS_DELETE_SELF);\n\tBUILD_BUG_ON(IN_MOVE_SELF != FS_MOVE_SELF);\n\tBUILD_BUG_ON(IN_UNMOUNT != FS_UNMOUNT);\n\tBUILD_BUG_ON(IN_Q_OVERFLOW != FS_Q_OVERFLOW);\n\tBUILD_BUG_ON(IN_IGNORED != FS_IN_IGNORED);\n\tBUILD_BUG_ON(IN_EXCL_UNLINK != FS_EXCL_UNLINK);\n\tBUILD_BUG_ON(IN_ISDIR != FS_ISDIR);\n\tBUILD_BUG_ON(IN_ONESHOT != FS_IN_ONESHOT);\n\n\tBUG_ON(hweight32(ALL_INOTIFY_BITS) != 21);\n\n\tinotify_inode_mark_cachep = KMEM_CACHE(inotify_inode_mark, SLAB_PANIC);\n\tevent_priv_cachep = KMEM_CACHE(inotify_event_private_data, SLAB_PANIC);\n\n\tinotify_max_queued_events = 16384;\n\tinotify_max_user_instances = 128;\n\tinotify_max_user_watches = 8192;\n\n\treturn 0;\n}\nmodule_init(inotify_user_setup);\n"], "fixing_code": ["/*\n * fs/inotify_user.c - inotify support for userspace\n *\n * Authors:\n *\tJohn McCutchan\t<ttb@tentacle.dhs.org>\n *\tRobert Love\t<rml@novell.com>\n *\n * Copyright (C) 2005 John McCutchan\n * Copyright 2006 Hewlett-Packard Development Company, L.P.\n *\n * Copyright (C) 2009 Eric Paris <Red Hat Inc>\n * inotify was largely rewriten to make use of the fsnotify infrastructure\n *\n * This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License as published by the\n * Free Software Foundation; either version 2, or (at your option) any\n * later version.\n *\n * This program is distributed in the hope that it will be useful, but\n * WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n * General Public License for more details.\n */\n\n#include <linux/file.h>\n#include <linux/fs.h> /* struct inode */\n#include <linux/fsnotify_backend.h>\n#include <linux/idr.h>\n#include <linux/init.h> /* module_init */\n#include <linux/inotify.h>\n#include <linux/kernel.h> /* roundup() */\n#include <linux/namei.h> /* LOOKUP_FOLLOW */\n#include <linux/sched.h> /* struct user */\n#include <linux/slab.h> /* struct kmem_cache */\n#include <linux/syscalls.h>\n#include <linux/types.h>\n#include <linux/anon_inodes.h>\n#include <linux/uaccess.h>\n#include <linux/poll.h>\n#include <linux/wait.h>\n\n#include \"inotify.h\"\n\n#include <asm/ioctls.h>\n\n/* these are configurable via /proc/sys/fs/inotify/ */\nstatic int inotify_max_user_instances __read_mostly;\nstatic int inotify_max_queued_events __read_mostly;\nstatic int inotify_max_user_watches __read_mostly;\n\nstatic struct kmem_cache *inotify_inode_mark_cachep __read_mostly;\nstruct kmem_cache *event_priv_cachep __read_mostly;\n\n#ifdef CONFIG_SYSCTL\n\n#include <linux/sysctl.h>\n\nstatic int zero;\n\nctl_table inotify_table[] = {\n\t{\n\t\t.procname\t= \"max_user_instances\",\n\t\t.data\t\t= &inotify_max_user_instances,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"max_user_watches\",\n\t\t.data\t\t= &inotify_max_user_watches,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero,\n\t},\n\t{\n\t\t.procname\t= \"max_queued_events\",\n\t\t.data\t\t= &inotify_max_queued_events,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &zero\n\t},\n\t{ }\n};\n#endif /* CONFIG_SYSCTL */\n\nstatic inline __u32 inotify_arg_to_mask(u32 arg)\n{\n\t__u32 mask;\n\n\t/*\n\t * everything should accept their own ignored, cares about children,\n\t * and should receive events when the inode is unmounted\n\t */\n\tmask = (FS_IN_IGNORED | FS_EVENT_ON_CHILD | FS_UNMOUNT);\n\n\t/* mask off the flags used to open the fd */\n\tmask |= (arg & (IN_ALL_EVENTS | IN_ONESHOT | IN_EXCL_UNLINK));\n\n\treturn mask;\n}\n\nstatic inline u32 inotify_mask_to_arg(__u32 mask)\n{\n\treturn mask & (IN_ALL_EVENTS | IN_ISDIR | IN_UNMOUNT | IN_IGNORED |\n\t\t       IN_Q_OVERFLOW);\n}\n\n/* intofiy userspace file descriptor functions */\nstatic unsigned int inotify_poll(struct file *file, poll_table *wait)\n{\n\tstruct fsnotify_group *group = file->private_data;\n\tint ret = 0;\n\n\tpoll_wait(file, &group->notification_waitq, wait);\n\tmutex_lock(&group->notification_mutex);\n\tif (!fsnotify_notify_queue_is_empty(group))\n\t\tret = POLLIN | POLLRDNORM;\n\tmutex_unlock(&group->notification_mutex);\n\n\treturn ret;\n}\n\n/*\n * Get an inotify_kernel_event if one exists and is small\n * enough to fit in \"count\". Return an error pointer if\n * not large enough.\n *\n * Called with the group->notification_mutex held.\n */\nstatic struct fsnotify_event *get_one_event(struct fsnotify_group *group,\n\t\t\t\t\t    size_t count)\n{\n\tsize_t event_size = sizeof(struct inotify_event);\n\tstruct fsnotify_event *event;\n\n\tif (fsnotify_notify_queue_is_empty(group))\n\t\treturn NULL;\n\n\tevent = fsnotify_peek_notify_event(group);\n\n\tpr_debug(\"%s: group=%p event=%p\\n\", __func__, group, event);\n\n\tif (event->name_len)\n\t\tevent_size += roundup(event->name_len + 1, event_size);\n\n\tif (event_size > count)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/* held the notification_mutex the whole time, so this is the\n\t * same event we peeked above */\n\tfsnotify_remove_notify_event(group);\n\n\treturn event;\n}\n\n/*\n * Copy an event to user space, returning how much we copied.\n *\n * We already checked that the event size is smaller than the\n * buffer we had in \"get_one_event()\" above.\n */\nstatic ssize_t copy_event_to_user(struct fsnotify_group *group,\n\t\t\t\t  struct fsnotify_event *event,\n\t\t\t\t  char __user *buf)\n{\n\tstruct inotify_event inotify_event;\n\tstruct fsnotify_event_private_data *fsn_priv;\n\tstruct inotify_event_private_data *priv;\n\tsize_t event_size = sizeof(struct inotify_event);\n\tsize_t name_len = 0;\n\n\tpr_debug(\"%s: group=%p event=%p\\n\", __func__, group, event);\n\n\t/* we get the inotify watch descriptor from the event private data */\n\tspin_lock(&event->lock);\n\tfsn_priv = fsnotify_remove_priv_from_event(group, event);\n\tspin_unlock(&event->lock);\n\n\tif (!fsn_priv)\n\t\tinotify_event.wd = -1;\n\telse {\n\t\tpriv = container_of(fsn_priv, struct inotify_event_private_data,\n\t\t\t\t    fsnotify_event_priv_data);\n\t\tinotify_event.wd = priv->wd;\n\t\tinotify_free_event_priv(fsn_priv);\n\t}\n\n\t/*\n\t * round up event->name_len so it is a multiple of event_size\n\t * plus an extra byte for the terminating '\\0'.\n\t */\n\tif (event->name_len)\n\t\tname_len = roundup(event->name_len + 1, event_size);\n\tinotify_event.len = name_len;\n\n\tinotify_event.mask = inotify_mask_to_arg(event->mask);\n\tinotify_event.cookie = event->sync_cookie;\n\n\t/* send the main event */\n\tif (copy_to_user(buf, &inotify_event, event_size))\n\t\treturn -EFAULT;\n\n\tbuf += event_size;\n\n\t/*\n\t * fsnotify only stores the pathname, so here we have to send the pathname\n\t * and then pad that pathname out to a multiple of sizeof(inotify_event)\n\t * with zeros.  I get my zeros from the nul_inotify_event.\n\t */\n\tif (name_len) {\n\t\tunsigned int len_to_zero = name_len - event->name_len;\n\t\t/* copy the path name */\n\t\tif (copy_to_user(buf, event->file_name, event->name_len))\n\t\t\treturn -EFAULT;\n\t\tbuf += event->name_len;\n\n\t\t/* fill userspace with 0's */\n\t\tif (clear_user(buf, len_to_zero))\n\t\t\treturn -EFAULT;\n\t\tbuf += len_to_zero;\n\t\tevent_size += name_len;\n\t}\n\n\treturn event_size;\n}\n\nstatic ssize_t inotify_read(struct file *file, char __user *buf,\n\t\t\t    size_t count, loff_t *pos)\n{\n\tstruct fsnotify_group *group;\n\tstruct fsnotify_event *kevent;\n\tchar __user *start;\n\tint ret;\n\tDEFINE_WAIT(wait);\n\n\tstart = buf;\n\tgroup = file->private_data;\n\n\twhile (1) {\n\t\tprepare_to_wait(&group->notification_waitq, &wait, TASK_INTERRUPTIBLE);\n\n\t\tmutex_lock(&group->notification_mutex);\n\t\tkevent = get_one_event(group, count);\n\t\tmutex_unlock(&group->notification_mutex);\n\n\t\tpr_debug(\"%s: group=%p kevent=%p\\n\", __func__, group, kevent);\n\n\t\tif (kevent) {\n\t\t\tret = PTR_ERR(kevent);\n\t\t\tif (IS_ERR(kevent))\n\t\t\t\tbreak;\n\t\t\tret = copy_event_to_user(group, kevent, buf);\n\t\t\tfsnotify_put_event(kevent);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t\tbuf += ret;\n\t\t\tcount -= ret;\n\t\t\tcontinue;\n\t\t}\n\n\t\tret = -EAGAIN;\n\t\tif (file->f_flags & O_NONBLOCK)\n\t\t\tbreak;\n\t\tret = -EINTR;\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\n\t\tif (start != buf)\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\n\n\tfinish_wait(&group->notification_waitq, &wait);\n\tif (start != buf && ret != -EFAULT)\n\t\tret = buf - start;\n\treturn ret;\n}\n\nstatic int inotify_fasync(int fd, struct file *file, int on)\n{\n\tstruct fsnotify_group *group = file->private_data;\n\n\treturn fasync_helper(fd, file, on, &group->inotify_data.fa) >= 0 ? 0 : -EIO;\n}\n\nstatic int inotify_release(struct inode *ignored, struct file *file)\n{\n\tstruct fsnotify_group *group = file->private_data;\n\tstruct user_struct *user = group->inotify_data.user;\n\n\tpr_debug(\"%s: group=%p\\n\", __func__, group);\n\n\tfsnotify_clear_marks_by_group(group);\n\n\t/* free this group, matching get was inotify_init->fsnotify_obtain_group */\n\tfsnotify_put_group(group);\n\n\tatomic_dec(&user->inotify_devs);\n\n\treturn 0;\n}\n\nstatic long inotify_ioctl(struct file *file, unsigned int cmd,\n\t\t\t  unsigned long arg)\n{\n\tstruct fsnotify_group *group;\n\tstruct fsnotify_event_holder *holder;\n\tstruct fsnotify_event *event;\n\tvoid __user *p;\n\tint ret = -ENOTTY;\n\tsize_t send_len = 0;\n\n\tgroup = file->private_data;\n\tp = (void __user *) arg;\n\n\tpr_debug(\"%s: group=%p cmd=%u\\n\", __func__, group, cmd);\n\n\tswitch (cmd) {\n\tcase FIONREAD:\n\t\tmutex_lock(&group->notification_mutex);\n\t\tlist_for_each_entry(holder, &group->notification_list, event_list) {\n\t\t\tevent = holder->event;\n\t\t\tsend_len += sizeof(struct inotify_event);\n\t\t\tif (event->name_len)\n\t\t\t\tsend_len += roundup(event->name_len + 1,\n\t\t\t\t\t\tsizeof(struct inotify_event));\n\t\t}\n\t\tmutex_unlock(&group->notification_mutex);\n\t\tret = put_user(send_len, (int __user *) p);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct file_operations inotify_fops = {\n\t.poll\t\t= inotify_poll,\n\t.read\t\t= inotify_read,\n\t.fasync\t\t= inotify_fasync,\n\t.release\t= inotify_release,\n\t.unlocked_ioctl\t= inotify_ioctl,\n\t.compat_ioctl\t= inotify_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\n\n/*\n * find_inode - resolve a user-given path to a specific inode\n */\nstatic int inotify_find_inode(const char __user *dirname, struct path *path, unsigned flags)\n{\n\tint error;\n\n\terror = user_path_at(AT_FDCWD, dirname, flags, path);\n\tif (error)\n\t\treturn error;\n\t/* you can only watch an inode if you have read permissions on it */\n\terror = inode_permission(path->dentry->d_inode, MAY_READ);\n\tif (error)\n\t\tpath_put(path);\n\treturn error;\n}\n\nstatic int inotify_add_to_idr(struct idr *idr, spinlock_t *idr_lock,\n\t\t\t      int *last_wd,\n\t\t\t      struct inotify_inode_mark *i_mark)\n{\n\tint ret;\n\n\tdo {\n\t\tif (unlikely(!idr_pre_get(idr, GFP_KERNEL)))\n\t\t\treturn -ENOMEM;\n\n\t\tspin_lock(idr_lock);\n\t\tret = idr_get_new_above(idr, i_mark, *last_wd + 1,\n\t\t\t\t\t&i_mark->wd);\n\t\t/* we added the mark to the idr, take a reference */\n\t\tif (!ret) {\n\t\t\t*last_wd = i_mark->wd;\n\t\t\tfsnotify_get_mark(&i_mark->fsn_mark);\n\t\t}\n\t\tspin_unlock(idr_lock);\n\t} while (ret == -EAGAIN);\n\n\treturn ret;\n}\n\nstatic struct inotify_inode_mark *inotify_idr_find_locked(struct fsnotify_group *group,\n\t\t\t\t\t\t\t\tint wd)\n{\n\tstruct idr *idr = &group->inotify_data.idr;\n\tspinlock_t *idr_lock = &group->inotify_data.idr_lock;\n\tstruct inotify_inode_mark *i_mark;\n\n\tassert_spin_locked(idr_lock);\n\n\ti_mark = idr_find(idr, wd);\n\tif (i_mark) {\n\t\tstruct fsnotify_mark *fsn_mark = &i_mark->fsn_mark;\n\n\t\tfsnotify_get_mark(fsn_mark);\n\t\t/* One ref for being in the idr, one ref we just took */\n\t\tBUG_ON(atomic_read(&fsn_mark->refcnt) < 2);\n\t}\n\n\treturn i_mark;\n}\n\nstatic struct inotify_inode_mark *inotify_idr_find(struct fsnotify_group *group,\n\t\t\t\t\t\t\t int wd)\n{\n\tstruct inotify_inode_mark *i_mark;\n\tspinlock_t *idr_lock = &group->inotify_data.idr_lock;\n\n\tspin_lock(idr_lock);\n\ti_mark = inotify_idr_find_locked(group, wd);\n\tspin_unlock(idr_lock);\n\n\treturn i_mark;\n}\n\nstatic void do_inotify_remove_from_idr(struct fsnotify_group *group,\n\t\t\t\t       struct inotify_inode_mark *i_mark)\n{\n\tstruct idr *idr = &group->inotify_data.idr;\n\tspinlock_t *idr_lock = &group->inotify_data.idr_lock;\n\tint wd = i_mark->wd;\n\n\tassert_spin_locked(idr_lock);\n\n\tidr_remove(idr, wd);\n\n\t/* removed from the idr, drop that ref */\n\tfsnotify_put_mark(&i_mark->fsn_mark);\n}\n\n/*\n * Remove the mark from the idr (if present) and drop the reference\n * on the mark because it was in the idr.\n */\nstatic void inotify_remove_from_idr(struct fsnotify_group *group,\n\t\t\t\t    struct inotify_inode_mark *i_mark)\n{\n\tspinlock_t *idr_lock = &group->inotify_data.idr_lock;\n\tstruct inotify_inode_mark *found_i_mark = NULL;\n\tint wd;\n\n\tspin_lock(idr_lock);\n\twd = i_mark->wd;\n\n\t/*\n\t * does this i_mark think it is in the idr?  we shouldn't get called\n\t * if it wasn't....\n\t */\n\tif (wd == -1) {\n\t\tWARN_ONCE(1, \"%s: i_mark=%p i_mark->wd=%d i_mark->group=%p\"\n\t\t\t\" i_mark->inode=%p\\n\", __func__, i_mark, i_mark->wd,\n\t\t\ti_mark->fsn_mark.group, i_mark->fsn_mark.i.inode);\n\t\tgoto out;\n\t}\n\n\t/* Lets look in the idr to see if we find it */\n\tfound_i_mark = inotify_idr_find_locked(group, wd);\n\tif (unlikely(!found_i_mark)) {\n\t\tWARN_ONCE(1, \"%s: i_mark=%p i_mark->wd=%d i_mark->group=%p\"\n\t\t\t\" i_mark->inode=%p\\n\", __func__, i_mark, i_mark->wd,\n\t\t\ti_mark->fsn_mark.group, i_mark->fsn_mark.i.inode);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * We found an mark in the idr at the right wd, but it's\n\t * not the mark we were told to remove.  eparis seriously\n\t * fucked up somewhere.\n\t */\n\tif (unlikely(found_i_mark != i_mark)) {\n\t\tWARN_ONCE(1, \"%s: i_mark=%p i_mark->wd=%d i_mark->group=%p \"\n\t\t\t\"mark->inode=%p found_i_mark=%p found_i_mark->wd=%d \"\n\t\t\t\"found_i_mark->group=%p found_i_mark->inode=%p\\n\",\n\t\t\t__func__, i_mark, i_mark->wd, i_mark->fsn_mark.group,\n\t\t\ti_mark->fsn_mark.i.inode, found_i_mark, found_i_mark->wd,\n\t\t\tfound_i_mark->fsn_mark.group,\n\t\t\tfound_i_mark->fsn_mark.i.inode);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * One ref for being in the idr\n\t * one ref held by the caller trying to kill us\n\t * one ref grabbed by inotify_idr_find\n\t */\n\tif (unlikely(atomic_read(&i_mark->fsn_mark.refcnt) < 3)) {\n\t\tprintk(KERN_ERR \"%s: i_mark=%p i_mark->wd=%d i_mark->group=%p\"\n\t\t\t\" i_mark->inode=%p\\n\", __func__, i_mark, i_mark->wd,\n\t\t\ti_mark->fsn_mark.group, i_mark->fsn_mark.i.inode);\n\t\t/* we can't really recover with bad ref cnting.. */\n\t\tBUG();\n\t}\n\n\tdo_inotify_remove_from_idr(group, i_mark);\nout:\n\t/* match the ref taken by inotify_idr_find_locked() */\n\tif (found_i_mark)\n\t\tfsnotify_put_mark(&found_i_mark->fsn_mark);\n\ti_mark->wd = -1;\n\tspin_unlock(idr_lock);\n}\n\n/*\n * Send IN_IGNORED for this wd, remove this wd from the idr.\n */\nvoid inotify_ignored_and_remove_idr(struct fsnotify_mark *fsn_mark,\n\t\t\t\t    struct fsnotify_group *group)\n{\n\tstruct inotify_inode_mark *i_mark;\n\tstruct fsnotify_event *ignored_event, *notify_event;\n\tstruct inotify_event_private_data *event_priv;\n\tstruct fsnotify_event_private_data *fsn_event_priv;\n\tint ret;\n\n\tignored_event = fsnotify_create_event(NULL, FS_IN_IGNORED, NULL,\n\t\t\t\t\t      FSNOTIFY_EVENT_NONE, NULL, 0,\n\t\t\t\t\t      GFP_NOFS);\n\tif (!ignored_event)\n\t\treturn;\n\n\ti_mark = container_of(fsn_mark, struct inotify_inode_mark, fsn_mark);\n\n\tevent_priv = kmem_cache_alloc(event_priv_cachep, GFP_NOFS);\n\tif (unlikely(!event_priv))\n\t\tgoto skip_send_ignore;\n\n\tfsn_event_priv = &event_priv->fsnotify_event_priv_data;\n\n\tfsn_event_priv->group = group;\n\tevent_priv->wd = i_mark->wd;\n\n\tnotify_event = fsnotify_add_notify_event(group, ignored_event, fsn_event_priv, NULL);\n\tif (notify_event) {\n\t\tif (IS_ERR(notify_event))\n\t\t\tret = PTR_ERR(notify_event);\n\t\telse\n\t\t\tfsnotify_put_event(notify_event);\n\t\tinotify_free_event_priv(fsn_event_priv);\n\t}\n\nskip_send_ignore:\n\n\t/* matches the reference taken when the event was created */\n\tfsnotify_put_event(ignored_event);\n\n\t/* remove this mark from the idr */\n\tinotify_remove_from_idr(group, i_mark);\n\n\tatomic_dec(&group->inotify_data.user->inotify_watches);\n}\n\n/* ding dong the mark is dead */\nstatic void inotify_free_mark(struct fsnotify_mark *fsn_mark)\n{\n\tstruct inotify_inode_mark *i_mark;\n\n\ti_mark = container_of(fsn_mark, struct inotify_inode_mark, fsn_mark);\n\n\tkmem_cache_free(inotify_inode_mark_cachep, i_mark);\n}\n\nstatic int inotify_update_existing_watch(struct fsnotify_group *group,\n\t\t\t\t\t struct inode *inode,\n\t\t\t\t\t u32 arg)\n{\n\tstruct fsnotify_mark *fsn_mark;\n\tstruct inotify_inode_mark *i_mark;\n\t__u32 old_mask, new_mask;\n\t__u32 mask;\n\tint add = (arg & IN_MASK_ADD);\n\tint ret;\n\n\t/* don't allow invalid bits: we don't want flags set */\n\tmask = inotify_arg_to_mask(arg);\n\tif (unlikely(!(mask & IN_ALL_EVENTS)))\n\t\treturn -EINVAL;\n\n\tfsn_mark = fsnotify_find_inode_mark(group, inode);\n\tif (!fsn_mark)\n\t\treturn -ENOENT;\n\n\ti_mark = container_of(fsn_mark, struct inotify_inode_mark, fsn_mark);\n\n\tspin_lock(&fsn_mark->lock);\n\n\told_mask = fsn_mark->mask;\n\tif (add)\n\t\tfsnotify_set_mark_mask_locked(fsn_mark, (fsn_mark->mask | mask));\n\telse\n\t\tfsnotify_set_mark_mask_locked(fsn_mark, mask);\n\tnew_mask = fsn_mark->mask;\n\n\tspin_unlock(&fsn_mark->lock);\n\n\tif (old_mask != new_mask) {\n\t\t/* more bits in old than in new? */\n\t\tint dropped = (old_mask & ~new_mask);\n\t\t/* more bits in this fsn_mark than the inode's mask? */\n\t\tint do_inode = (new_mask & ~inode->i_fsnotify_mask);\n\n\t\t/* update the inode with this new fsn_mark */\n\t\tif (dropped || do_inode)\n\t\t\tfsnotify_recalc_inode_mask(inode);\n\n\t}\n\n\t/* return the wd */\n\tret = i_mark->wd;\n\n\t/* match the get from fsnotify_find_mark() */\n\tfsnotify_put_mark(fsn_mark);\n\n\treturn ret;\n}\n\nstatic int inotify_new_watch(struct fsnotify_group *group,\n\t\t\t     struct inode *inode,\n\t\t\t     u32 arg)\n{\n\tstruct inotify_inode_mark *tmp_i_mark;\n\t__u32 mask;\n\tint ret;\n\tstruct idr *idr = &group->inotify_data.idr;\n\tspinlock_t *idr_lock = &group->inotify_data.idr_lock;\n\n\t/* don't allow invalid bits: we don't want flags set */\n\tmask = inotify_arg_to_mask(arg);\n\tif (unlikely(!(mask & IN_ALL_EVENTS)))\n\t\treturn -EINVAL;\n\n\ttmp_i_mark = kmem_cache_alloc(inotify_inode_mark_cachep, GFP_KERNEL);\n\tif (unlikely(!tmp_i_mark))\n\t\treturn -ENOMEM;\n\n\tfsnotify_init_mark(&tmp_i_mark->fsn_mark, inotify_free_mark);\n\ttmp_i_mark->fsn_mark.mask = mask;\n\ttmp_i_mark->wd = -1;\n\n\tret = -ENOSPC;\n\tif (atomic_read(&group->inotify_data.user->inotify_watches) >= inotify_max_user_watches)\n\t\tgoto out_err;\n\n\tret = inotify_add_to_idr(idr, idr_lock, &group->inotify_data.last_wd,\n\t\t\t\t tmp_i_mark);\n\tif (ret)\n\t\tgoto out_err;\n\n\t/* we are on the idr, now get on the inode */\n\tret = fsnotify_add_mark(&tmp_i_mark->fsn_mark, group, inode, NULL, 0);\n\tif (ret) {\n\t\t/* we failed to get on the inode, get off the idr */\n\t\tinotify_remove_from_idr(group, tmp_i_mark);\n\t\tgoto out_err;\n\t}\n\n\t/* increment the number of watches the user has */\n\tatomic_inc(&group->inotify_data.user->inotify_watches);\n\n\t/* return the watch descriptor for this new mark */\n\tret = tmp_i_mark->wd;\n\nout_err:\n\t/* match the ref from fsnotify_init_mark() */\n\tfsnotify_put_mark(&tmp_i_mark->fsn_mark);\n\n\treturn ret;\n}\n\nstatic int inotify_update_watch(struct fsnotify_group *group, struct inode *inode, u32 arg)\n{\n\tint ret = 0;\n\nretry:\n\t/* try to update and existing watch with the new arg */\n\tret = inotify_update_existing_watch(group, inode, arg);\n\t/* no mark present, try to add a new one */\n\tif (ret == -ENOENT)\n\t\tret = inotify_new_watch(group, inode, arg);\n\t/*\n\t * inotify_new_watch could race with another thread which did an\n\t * inotify_new_watch between the update_existing and the add watch\n\t * here, go back and try to update an existing mark again.\n\t */\n\tif (ret == -EEXIST)\n\t\tgoto retry;\n\n\treturn ret;\n}\n\nstatic struct fsnotify_group *inotify_new_group(struct user_struct *user, unsigned int max_events)\n{\n\tstruct fsnotify_group *group;\n\n\tgroup = fsnotify_alloc_group(&inotify_fsnotify_ops);\n\tif (IS_ERR(group))\n\t\treturn group;\n\n\tgroup->max_events = max_events;\n\n\tspin_lock_init(&group->inotify_data.idr_lock);\n\tidr_init(&group->inotify_data.idr);\n\tgroup->inotify_data.last_wd = 0;\n\tgroup->inotify_data.user = user;\n\tgroup->inotify_data.fa = NULL;\n\n\treturn group;\n}\n\n\n/* inotify syscalls */\nSYSCALL_DEFINE1(inotify_init1, int, flags)\n{\n\tstruct fsnotify_group *group;\n\tstruct user_struct *user;\n\tint ret;\n\n\t/* Check the IN_* constants for consistency.  */\n\tBUILD_BUG_ON(IN_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON(IN_NONBLOCK != O_NONBLOCK);\n\n\tif (flags & ~(IN_CLOEXEC | IN_NONBLOCK))\n\t\treturn -EINVAL;\n\n\tuser = get_current_user();\n\tif (unlikely(atomic_read(&user->inotify_devs) >=\n\t\t\tinotify_max_user_instances)) {\n\t\tret = -EMFILE;\n\t\tgoto out_free_uid;\n\t}\n\n\t/* fsnotify_obtain_group took a reference to group, we put this when we kill the file in the end */\n\tgroup = inotify_new_group(user, inotify_max_queued_events);\n\tif (IS_ERR(group)) {\n\t\tret = PTR_ERR(group);\n\t\tgoto out_free_uid;\n\t}\n\n\tatomic_inc(&user->inotify_devs);\n\n\tret = anon_inode_getfd(\"inotify\", &inotify_fops, group,\n\t\t\t\t  O_RDONLY | flags);\n\tif (ret >= 0)\n\t\treturn ret;\n\n\tfsnotify_put_group(group);\n\tatomic_dec(&user->inotify_devs);\nout_free_uid:\n\tfree_uid(user);\n\treturn ret;\n}\n\nSYSCALL_DEFINE0(inotify_init)\n{\n\treturn sys_inotify_init1(0);\n}\n\nSYSCALL_DEFINE3(inotify_add_watch, int, fd, const char __user *, pathname,\n\t\tu32, mask)\n{\n\tstruct fsnotify_group *group;\n\tstruct inode *inode;\n\tstruct path path;\n\tstruct file *filp;\n\tint ret, fput_needed;\n\tunsigned flags = 0;\n\n\tfilp = fget_light(fd, &fput_needed);\n\tif (unlikely(!filp))\n\t\treturn -EBADF;\n\n\t/* verify that this is indeed an inotify instance */\n\tif (unlikely(filp->f_op != &inotify_fops)) {\n\t\tret = -EINVAL;\n\t\tgoto fput_and_out;\n\t}\n\n\tif (!(mask & IN_DONT_FOLLOW))\n\t\tflags |= LOOKUP_FOLLOW;\n\tif (mask & IN_ONLYDIR)\n\t\tflags |= LOOKUP_DIRECTORY;\n\n\tret = inotify_find_inode(pathname, &path, flags);\n\tif (ret)\n\t\tgoto fput_and_out;\n\n\t/* inode held in place by reference to path; group by fget on fd */\n\tinode = path.dentry->d_inode;\n\tgroup = filp->private_data;\n\n\t/* create/update an inode mark */\n\tret = inotify_update_watch(group, inode, mask);\n\tpath_put(&path);\nfput_and_out:\n\tfput_light(filp, fput_needed);\n\treturn ret;\n}\n\nSYSCALL_DEFINE2(inotify_rm_watch, int, fd, __s32, wd)\n{\n\tstruct fsnotify_group *group;\n\tstruct inotify_inode_mark *i_mark;\n\tstruct file *filp;\n\tint ret = 0, fput_needed;\n\n\tfilp = fget_light(fd, &fput_needed);\n\tif (unlikely(!filp))\n\t\treturn -EBADF;\n\n\t/* verify that this is indeed an inotify instance */\n\tret = -EINVAL;\n\tif (unlikely(filp->f_op != &inotify_fops))\n\t\tgoto out;\n\n\tgroup = filp->private_data;\n\n\tret = -EINVAL;\n\ti_mark = inotify_idr_find(group, wd);\n\tif (unlikely(!i_mark))\n\t\tgoto out;\n\n\tret = 0;\n\n\tfsnotify_destroy_mark(&i_mark->fsn_mark);\n\n\t/* match ref taken by inotify_idr_find */\n\tfsnotify_put_mark(&i_mark->fsn_mark);\n\nout:\n\tfput_light(filp, fput_needed);\n\treturn ret;\n}\n\n/*\n * inotify_user_setup - Our initialization function.  Note that we cannnot return\n * error because we have compiled-in VFS hooks.  So an (unlikely) failure here\n * must result in panic().\n */\nstatic int __init inotify_user_setup(void)\n{\n\tBUILD_BUG_ON(IN_ACCESS != FS_ACCESS);\n\tBUILD_BUG_ON(IN_MODIFY != FS_MODIFY);\n\tBUILD_BUG_ON(IN_ATTRIB != FS_ATTRIB);\n\tBUILD_BUG_ON(IN_CLOSE_WRITE != FS_CLOSE_WRITE);\n\tBUILD_BUG_ON(IN_CLOSE_NOWRITE != FS_CLOSE_NOWRITE);\n\tBUILD_BUG_ON(IN_OPEN != FS_OPEN);\n\tBUILD_BUG_ON(IN_MOVED_FROM != FS_MOVED_FROM);\n\tBUILD_BUG_ON(IN_MOVED_TO != FS_MOVED_TO);\n\tBUILD_BUG_ON(IN_CREATE != FS_CREATE);\n\tBUILD_BUG_ON(IN_DELETE != FS_DELETE);\n\tBUILD_BUG_ON(IN_DELETE_SELF != FS_DELETE_SELF);\n\tBUILD_BUG_ON(IN_MOVE_SELF != FS_MOVE_SELF);\n\tBUILD_BUG_ON(IN_UNMOUNT != FS_UNMOUNT);\n\tBUILD_BUG_ON(IN_Q_OVERFLOW != FS_Q_OVERFLOW);\n\tBUILD_BUG_ON(IN_IGNORED != FS_IN_IGNORED);\n\tBUILD_BUG_ON(IN_EXCL_UNLINK != FS_EXCL_UNLINK);\n\tBUILD_BUG_ON(IN_ISDIR != FS_ISDIR);\n\tBUILD_BUG_ON(IN_ONESHOT != FS_IN_ONESHOT);\n\n\tBUG_ON(hweight32(ALL_INOTIFY_BITS) != 21);\n\n\tinotify_inode_mark_cachep = KMEM_CACHE(inotify_inode_mark, SLAB_PANIC);\n\tevent_priv_cachep = KMEM_CACHE(inotify_event_private_data, SLAB_PANIC);\n\n\tinotify_max_queued_events = 16384;\n\tinotify_max_user_instances = 128;\n\tinotify_max_user_watches = 8192;\n\n\treturn 0;\n}\nmodule_init(inotify_user_setup);\n"], "filenames": ["fs/notify/inotify/inotify_user.c"], "buggy_code_start_loc": [754], "buggy_code_end_loc": [754], "fixing_code_start_loc": [755], "fixing_code_end_loc": [756], "type": "CWE-399", "message": "Memory leak in the inotify_init1 function in fs/notify/inotify/inotify_user.c in the Linux kernel before 2.6.37 allows local users to cause a denial of service (memory consumption) via vectors involving failed attempts to create files.", "other": {"cve": {"id": "CVE-2010-4250", "sourceIdentifier": "secalert@redhat.com", "published": "2012-06-21T23:55:01.647", "lastModified": "2023-02-13T04:28:25.357", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Memory leak in the inotify_init1 function in fs/notify/inotify/inotify_user.c in the Linux kernel before 2.6.37 allows local users to cause a denial of service (memory consumption) via vectors involving failed attempts to create files."}, {"lang": "es", "value": "Una vulnerabilidad de p\u00e9rdida de memoria en la funci\u00f3n inotify_init1 en fs/notify/inotify/inotify_user.c en versiones del kernel de Linux anteriores a v2.6.37 permite a usuarios locales provocar una denegaci\u00f3n de servicio (por excesivo consumo de memoria) a trav\u00e9s de vectores relacionados con intentos fallidos de creaci\u00f3n de archivos."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-399"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "2.6.36.4", "matchCriteriaId": "C03086A2-8EEE-40E3-9A7F-A5303FBF0472"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.36.1:*:*:*:*:*:*:*", "matchCriteriaId": "907A3F7F-B11D-4CF1-A1B2-A28BBEBF03C3"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.36.2:*:*:*:*:*:*:*", "matchCriteriaId": "EE4657B8-B691-4833-8546-220AD2BA8A7D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:2.6.36.3:*:*:*:*:*:*:*", "matchCriteriaId": "A2455F37-66D8-4BE1-8739-1A20A2E5375D"}]}]}], "references": [{"url": "http://ftp.osuosl.org/pub/linux/kernel/v2.6/ChangeLog-2.6.37", "source": "secalert@redhat.com"}, {"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=a2ae4cc9a16e211c8a128ba10d22a85431f093ab", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2010/11/24/11", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=656830", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/a2ae4cc9a16e211c8a128ba10d22a85431f093ab", "source": "secalert@redhat.com", "tags": ["Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/a2ae4cc9a16e211c8a128ba10d22a85431f093ab"}}