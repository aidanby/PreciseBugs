{"buggy_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n *  Copyright (C) 1994  Linus Torvalds\n *\n *  Cyrix stuff, June 1998 by:\n *\t- Rafael R. Reilova (moved everything from head.S),\n *        <rreilova@ececs.uc.edu>\n *\t- Channing Corn (tests & fixes),\n *\t- Andrew D. Balsa (code cleanup).\n */\n#include <linux/init.h>\n#include <linux/utsname.h>\n#include <linux/cpu.h>\n#include <linux/module.h>\n#include <linux/nospec.h>\n#include <linux/prctl.h>\n\n#include <asm/spec-ctrl.h>\n#include <asm/cmdline.h>\n#include <asm/bugs.h>\n#include <asm/processor.h>\n#include <asm/processor-flags.h>\n#include <asm/fpu/internal.h>\n#include <asm/msr.h>\n#include <asm/paravirt.h>\n#include <asm/alternative.h>\n#include <asm/pgtable.h>\n#include <asm/set_memory.h>\n#include <asm/intel-family.h>\n#include <asm/hypervisor.h>\n\nstatic void __init spectre_v2_select_mitigation(void);\nstatic void __init ssb_select_mitigation(void);\n\n/*\n * Our boot-time value of the SPEC_CTRL MSR. We read it once so that any\n * writes to SPEC_CTRL contain whatever reserved bits have been set.\n */\nu64 __ro_after_init x86_spec_ctrl_base;\nEXPORT_SYMBOL_GPL(x86_spec_ctrl_base);\n\n/*\n * The vendor and possibly platform specific bits which can be modified in\n * x86_spec_ctrl_base.\n */\nstatic u64 __ro_after_init x86_spec_ctrl_mask = SPEC_CTRL_IBRS;\n\n/*\n * AMD specific MSR info for Speculative Store Bypass control.\n * x86_amd_ls_cfg_ssbd_mask is initialized in identify_boot_cpu().\n */\nu64 __ro_after_init x86_amd_ls_cfg_base;\nu64 __ro_after_init x86_amd_ls_cfg_ssbd_mask;\n\nvoid __init check_bugs(void)\n{\n\tidentify_boot_cpu();\n\n\tif (!IS_ENABLED(CONFIG_SMP)) {\n\t\tpr_info(\"CPU: \");\n\t\tprint_cpu_info(&boot_cpu_data);\n\t}\n\n\t/*\n\t * Read the SPEC_CTRL MSR to account for reserved bits which may\n\t * have unknown values. AMD64_LS_CFG MSR is cached in the early AMD\n\t * init code as it is not enumerated and depends on the family.\n\t */\n\tif (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))\n\t\trdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);\n\n\t/* Allow STIBP in MSR_SPEC_CTRL if supported */\n\tif (boot_cpu_has(X86_FEATURE_STIBP))\n\t\tx86_spec_ctrl_mask |= SPEC_CTRL_STIBP;\n\n\t/* Select the proper spectre mitigation before patching alternatives */\n\tspectre_v2_select_mitigation();\n\n\t/*\n\t * Select proper mitigation for any exposure to the Speculative Store\n\t * Bypass vulnerability.\n\t */\n\tssb_select_mitigation();\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Check whether we are able to run this kernel safely on SMP.\n\t *\n\t * - i386 is no longer supported.\n\t * - In order to run on anything without a TSC, we need to be\n\t *   compiled for a i486.\n\t */\n\tif (boot_cpu_data.x86 < 4)\n\t\tpanic(\"Kernel requires i486+ for 'invlpg' and other features\");\n\n\tinit_utsname()->machine[1] =\n\t\t'0' + (boot_cpu_data.x86 > 6 ? 6 : boot_cpu_data.x86);\n\talternative_instructions();\n\n\tfpu__init_check_bugs();\n#else /* CONFIG_X86_64 */\n\talternative_instructions();\n\n\t/*\n\t * Make sure the first 2MB area is not mapped by huge pages\n\t * There are typically fixed size MTRRs in there and overlapping\n\t * MTRRs into large pages causes slow downs.\n\t *\n\t * Right now we don't do that with gbpages because there seems\n\t * very little benefit for that case.\n\t */\n\tif (!direct_gbpages)\n\t\tset_memory_4k((unsigned long)__va(0), 1);\n#endif\n}\n\n/* The kernel command line selection */\nenum spectre_v2_mitigation_cmd {\n\tSPECTRE_V2_CMD_NONE,\n\tSPECTRE_V2_CMD_AUTO,\n\tSPECTRE_V2_CMD_FORCE,\n\tSPECTRE_V2_CMD_RETPOLINE,\n\tSPECTRE_V2_CMD_RETPOLINE_GENERIC,\n\tSPECTRE_V2_CMD_RETPOLINE_AMD,\n};\n\nstatic const char *spectre_v2_strings[] = {\n\t[SPECTRE_V2_NONE]\t\t\t= \"Vulnerable\",\n\t[SPECTRE_V2_RETPOLINE_MINIMAL]\t\t= \"Vulnerable: Minimal generic ASM retpoline\",\n\t[SPECTRE_V2_RETPOLINE_MINIMAL_AMD]\t= \"Vulnerable: Minimal AMD ASM retpoline\",\n\t[SPECTRE_V2_RETPOLINE_GENERIC]\t\t= \"Mitigation: Full generic retpoline\",\n\t[SPECTRE_V2_RETPOLINE_AMD]\t\t= \"Mitigation: Full AMD retpoline\",\n};\n\n#undef pr_fmt\n#define pr_fmt(fmt)     \"Spectre V2 : \" fmt\n\nstatic enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init =\n\tSPECTRE_V2_NONE;\n\nvoid\nx86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bool setguest)\n{\n\tu64 msrval, guestval, hostval = x86_spec_ctrl_base;\n\tstruct thread_info *ti = current_thread_info();\n\n\t/* Is MSR_SPEC_CTRL implemented ? */\n\tif (static_cpu_has(X86_FEATURE_MSR_SPEC_CTRL)) {\n\t\t/*\n\t\t * Restrict guest_spec_ctrl to supported values. Clear the\n\t\t * modifiable bits in the host base value and or the\n\t\t * modifiable bits from the guest value.\n\t\t */\n\t\tguestval = hostval & ~x86_spec_ctrl_mask;\n\t\tguestval |= guest_spec_ctrl & x86_spec_ctrl_mask;\n\n\t\t/* SSBD controlled in MSR_SPEC_CTRL */\n\t\tif (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) ||\n\t\t    static_cpu_has(X86_FEATURE_AMD_SSBD))\n\t\t\thostval |= ssbd_tif_to_spec_ctrl(ti->flags);\n\n\t\tif (hostval != guestval) {\n\t\t\tmsrval = setguest ? guestval : hostval;\n\t\t\twrmsrl(MSR_IA32_SPEC_CTRL, msrval);\n\t\t}\n\t}\n\n\t/*\n\t * If SSBD is not handled in MSR_SPEC_CTRL on AMD, update\n\t * MSR_AMD64_L2_CFG or MSR_VIRT_SPEC_CTRL if supported.\n\t */\n\tif (!static_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&\n\t    !static_cpu_has(X86_FEATURE_VIRT_SSBD))\n\t\treturn;\n\n\t/*\n\t * If the host has SSBD mitigation enabled, force it in the host's\n\t * virtual MSR value. If its not permanently enabled, evaluate\n\t * current's TIF_SSBD thread flag.\n\t */\n\tif (static_cpu_has(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE))\n\t\thostval = SPEC_CTRL_SSBD;\n\telse\n\t\thostval = ssbd_tif_to_spec_ctrl(ti->flags);\n\n\t/* Sanitize the guest value */\n\tguestval = guest_virt_spec_ctrl & SPEC_CTRL_SSBD;\n\n\tif (hostval != guestval) {\n\t\tunsigned long tif;\n\n\t\ttif = setguest ? ssbd_spec_ctrl_to_tif(guestval) :\n\t\t\t\t ssbd_spec_ctrl_to_tif(hostval);\n\n\t\tspeculative_store_bypass_update(tif);\n\t}\n}\nEXPORT_SYMBOL_GPL(x86_virt_spec_ctrl);\n\nstatic void x86_amd_ssb_disable(void)\n{\n\tu64 msrval = x86_amd_ls_cfg_base | x86_amd_ls_cfg_ssbd_mask;\n\n\tif (boot_cpu_has(X86_FEATURE_VIRT_SSBD))\n\t\twrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, SPEC_CTRL_SSBD);\n\telse if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD))\n\t\twrmsrl(MSR_AMD64_LS_CFG, msrval);\n}\n\n#ifdef RETPOLINE\nstatic bool spectre_v2_bad_module;\n\nbool retpoline_module_ok(bool has_retpoline)\n{\n\tif (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)\n\t\treturn true;\n\n\tpr_err(\"System may be vulnerable to spectre v2\\n\");\n\tspectre_v2_bad_module = true;\n\treturn false;\n}\n\nstatic inline const char *spectre_v2_module_string(void)\n{\n\treturn spectre_v2_bad_module ? \" - vulnerable module loaded\" : \"\";\n}\n#else\nstatic inline const char *spectre_v2_module_string(void) { return \"\"; }\n#endif\n\nstatic void __init spec2_print_if_insecure(const char *reason)\n{\n\tif (boot_cpu_has_bug(X86_BUG_SPECTRE_V2))\n\t\tpr_info(\"%s selected on command line.\\n\", reason);\n}\n\nstatic void __init spec2_print_if_secure(const char *reason)\n{\n\tif (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))\n\t\tpr_info(\"%s selected on command line.\\n\", reason);\n}\n\nstatic inline bool retp_compiler(void)\n{\n\treturn __is_defined(RETPOLINE);\n}\n\nstatic inline bool match_option(const char *arg, int arglen, const char *opt)\n{\n\tint len = strlen(opt);\n\n\treturn len == arglen && !strncmp(arg, opt, len);\n}\n\nstatic const struct {\n\tconst char *option;\n\tenum spectre_v2_mitigation_cmd cmd;\n\tbool secure;\n} mitigation_options[] = {\n\t{ \"off\",               SPECTRE_V2_CMD_NONE,              false },\n\t{ \"on\",                SPECTRE_V2_CMD_FORCE,             true },\n\t{ \"retpoline\",         SPECTRE_V2_CMD_RETPOLINE,         false },\n\t{ \"retpoline,amd\",     SPECTRE_V2_CMD_RETPOLINE_AMD,     false },\n\t{ \"retpoline,generic\", SPECTRE_V2_CMD_RETPOLINE_GENERIC, false },\n\t{ \"auto\",              SPECTRE_V2_CMD_AUTO,              false },\n};\n\nstatic enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)\n{\n\tchar arg[20];\n\tint ret, i;\n\tenum spectre_v2_mitigation_cmd cmd = SPECTRE_V2_CMD_AUTO;\n\n\tif (cmdline_find_option_bool(boot_command_line, \"nospectre_v2\"))\n\t\treturn SPECTRE_V2_CMD_NONE;\n\telse {\n\t\tret = cmdline_find_option(boot_command_line, \"spectre_v2\", arg, sizeof(arg));\n\t\tif (ret < 0)\n\t\t\treturn SPECTRE_V2_CMD_AUTO;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(mitigation_options); i++) {\n\t\t\tif (!match_option(arg, ret, mitigation_options[i].option))\n\t\t\t\tcontinue;\n\t\t\tcmd = mitigation_options[i].cmd;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (i >= ARRAY_SIZE(mitigation_options)) {\n\t\t\tpr_err(\"unknown option (%s). Switching to AUTO select\\n\", arg);\n\t\t\treturn SPECTRE_V2_CMD_AUTO;\n\t\t}\n\t}\n\n\tif ((cmd == SPECTRE_V2_CMD_RETPOLINE ||\n\t     cmd == SPECTRE_V2_CMD_RETPOLINE_AMD ||\n\t     cmd == SPECTRE_V2_CMD_RETPOLINE_GENERIC) &&\n\t    !IS_ENABLED(CONFIG_RETPOLINE)) {\n\t\tpr_err(\"%s selected but not compiled in. Switching to AUTO select\\n\", mitigation_options[i].option);\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tif (cmd == SPECTRE_V2_CMD_RETPOLINE_AMD &&\n\t    boot_cpu_data.x86_vendor != X86_VENDOR_AMD) {\n\t\tpr_err(\"retpoline,amd selected but CPU is not AMD. Switching to AUTO select\\n\");\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tif (mitigation_options[i].secure)\n\t\tspec2_print_if_secure(mitigation_options[i].option);\n\telse\n\t\tspec2_print_if_insecure(mitigation_options[i].option);\n\n\treturn cmd;\n}\n\n/* Check for Skylake-like CPUs (for RSB handling) */\nstatic bool __init is_skylake_era(void)\n{\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&\n\t    boot_cpu_data.x86 == 6) {\n\t\tswitch (boot_cpu_data.x86_model) {\n\t\tcase INTEL_FAM6_SKYLAKE_MOBILE:\n\t\tcase INTEL_FAM6_SKYLAKE_DESKTOP:\n\t\tcase INTEL_FAM6_SKYLAKE_X:\n\t\tcase INTEL_FAM6_KABYLAKE_MOBILE:\n\t\tcase INTEL_FAM6_KABYLAKE_DESKTOP:\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic void __init spectre_v2_select_mitigation(void)\n{\n\tenum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();\n\tenum spectre_v2_mitigation mode = SPECTRE_V2_NONE;\n\n\t/*\n\t * If the CPU is not affected and the command line mode is NONE or AUTO\n\t * then nothing to do.\n\t */\n\tif (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2) &&\n\t    (cmd == SPECTRE_V2_CMD_NONE || cmd == SPECTRE_V2_CMD_AUTO))\n\t\treturn;\n\n\tswitch (cmd) {\n\tcase SPECTRE_V2_CMD_NONE:\n\t\treturn;\n\n\tcase SPECTRE_V2_CMD_FORCE:\n\tcase SPECTRE_V2_CMD_AUTO:\n\t\tif (IS_ENABLED(CONFIG_RETPOLINE))\n\t\t\tgoto retpoline_auto;\n\t\tbreak;\n\tcase SPECTRE_V2_CMD_RETPOLINE_AMD:\n\t\tif (IS_ENABLED(CONFIG_RETPOLINE))\n\t\t\tgoto retpoline_amd;\n\t\tbreak;\n\tcase SPECTRE_V2_CMD_RETPOLINE_GENERIC:\n\t\tif (IS_ENABLED(CONFIG_RETPOLINE))\n\t\t\tgoto retpoline_generic;\n\t\tbreak;\n\tcase SPECTRE_V2_CMD_RETPOLINE:\n\t\tif (IS_ENABLED(CONFIG_RETPOLINE))\n\t\t\tgoto retpoline_auto;\n\t\tbreak;\n\t}\n\tpr_err(\"Spectre mitigation: kernel not compiled with retpoline; no mitigation available!\");\n\treturn;\n\nretpoline_auto:\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {\n\tretpoline_amd:\n\t\tif (!boot_cpu_has(X86_FEATURE_LFENCE_RDTSC)) {\n\t\t\tpr_err(\"Spectre mitigation: LFENCE not serializing, switching to generic retpoline\\n\");\n\t\t\tgoto retpoline_generic;\n\t\t}\n\t\tmode = retp_compiler() ? SPECTRE_V2_RETPOLINE_AMD :\n\t\t\t\t\t SPECTRE_V2_RETPOLINE_MINIMAL_AMD;\n\t\tsetup_force_cpu_cap(X86_FEATURE_RETPOLINE_AMD);\n\t\tsetup_force_cpu_cap(X86_FEATURE_RETPOLINE);\n\t} else {\n\tretpoline_generic:\n\t\tmode = retp_compiler() ? SPECTRE_V2_RETPOLINE_GENERIC :\n\t\t\t\t\t SPECTRE_V2_RETPOLINE_MINIMAL;\n\t\tsetup_force_cpu_cap(X86_FEATURE_RETPOLINE);\n\t}\n\n\tspectre_v2_enabled = mode;\n\tpr_info(\"%s\\n\", spectre_v2_strings[mode]);\n\n\t/*\n\t * If neither SMEP nor PTI are available, there is a risk of\n\t * hitting userspace addresses in the RSB after a context switch\n\t * from a shallow call stack to a deeper one. To prevent this fill\n\t * the entire RSB, even when using IBRS.\n\t *\n\t * Skylake era CPUs have a separate issue with *underflow* of the\n\t * RSB, when they will predict 'ret' targets from the generic BTB.\n\t * The proper mitigation for this is IBRS. If IBRS is not supported\n\t * or deactivated in favour of retpolines the RSB fill on context\n\t * switch is required.\n\t */\n\tif ((!boot_cpu_has(X86_FEATURE_PTI) &&\n\t     !boot_cpu_has(X86_FEATURE_SMEP)) || is_skylake_era()) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);\n\t\tpr_info(\"Spectre v2 mitigation: Filling RSB on context switch\\n\");\n\t}\n\n\t/* Initialize Indirect Branch Prediction Barrier if supported */\n\tif (boot_cpu_has(X86_FEATURE_IBPB)) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_USE_IBPB);\n\t\tpr_info(\"Spectre v2 mitigation: Enabling Indirect Branch Prediction Barrier\\n\");\n\t}\n\n\t/*\n\t * Retpoline means the kernel is safe because it has no indirect\n\t * branches. But firmware isn't, so use IBRS to protect that.\n\t */\n\tif (boot_cpu_has(X86_FEATURE_IBRS)) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_USE_IBRS_FW);\n\t\tpr_info(\"Enabling Restricted Speculation for firmware calls\\n\");\n\t}\n}\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"Speculative Store Bypass: \" fmt\n\nstatic enum ssb_mitigation ssb_mode __ro_after_init = SPEC_STORE_BYPASS_NONE;\n\n/* The kernel command line selection */\nenum ssb_mitigation_cmd {\n\tSPEC_STORE_BYPASS_CMD_NONE,\n\tSPEC_STORE_BYPASS_CMD_AUTO,\n\tSPEC_STORE_BYPASS_CMD_ON,\n\tSPEC_STORE_BYPASS_CMD_PRCTL,\n\tSPEC_STORE_BYPASS_CMD_SECCOMP,\n};\n\nstatic const char *ssb_strings[] = {\n\t[SPEC_STORE_BYPASS_NONE]\t= \"Vulnerable\",\n\t[SPEC_STORE_BYPASS_DISABLE]\t= \"Mitigation: Speculative Store Bypass disabled\",\n\t[SPEC_STORE_BYPASS_PRCTL]\t= \"Mitigation: Speculative Store Bypass disabled via prctl\",\n\t[SPEC_STORE_BYPASS_SECCOMP]\t= \"Mitigation: Speculative Store Bypass disabled via prctl and seccomp\",\n};\n\nstatic const struct {\n\tconst char *option;\n\tenum ssb_mitigation_cmd cmd;\n} ssb_mitigation_options[] = {\n\t{ \"auto\",\tSPEC_STORE_BYPASS_CMD_AUTO },    /* Platform decides */\n\t{ \"on\",\t\tSPEC_STORE_BYPASS_CMD_ON },      /* Disable Speculative Store Bypass */\n\t{ \"off\",\tSPEC_STORE_BYPASS_CMD_NONE },    /* Don't touch Speculative Store Bypass */\n\t{ \"prctl\",\tSPEC_STORE_BYPASS_CMD_PRCTL },   /* Disable Speculative Store Bypass via prctl */\n\t{ \"seccomp\",\tSPEC_STORE_BYPASS_CMD_SECCOMP }, /* Disable Speculative Store Bypass via prctl and seccomp */\n};\n\nstatic enum ssb_mitigation_cmd __init ssb_parse_cmdline(void)\n{\n\tenum ssb_mitigation_cmd cmd = SPEC_STORE_BYPASS_CMD_AUTO;\n\tchar arg[20];\n\tint ret, i;\n\n\tif (cmdline_find_option_bool(boot_command_line, \"nospec_store_bypass_disable\")) {\n\t\treturn SPEC_STORE_BYPASS_CMD_NONE;\n\t} else {\n\t\tret = cmdline_find_option(boot_command_line, \"spec_store_bypass_disable\",\n\t\t\t\t\t  arg, sizeof(arg));\n\t\tif (ret < 0)\n\t\t\treturn SPEC_STORE_BYPASS_CMD_AUTO;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(ssb_mitigation_options); i++) {\n\t\t\tif (!match_option(arg, ret, ssb_mitigation_options[i].option))\n\t\t\t\tcontinue;\n\n\t\t\tcmd = ssb_mitigation_options[i].cmd;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (i >= ARRAY_SIZE(ssb_mitigation_options)) {\n\t\t\tpr_err(\"unknown option (%s). Switching to AUTO select\\n\", arg);\n\t\t\treturn SPEC_STORE_BYPASS_CMD_AUTO;\n\t\t}\n\t}\n\n\treturn cmd;\n}\n\nstatic enum ssb_mitigation __init __ssb_select_mitigation(void)\n{\n\tenum ssb_mitigation mode = SPEC_STORE_BYPASS_NONE;\n\tenum ssb_mitigation_cmd cmd;\n\n\tif (!boot_cpu_has(X86_FEATURE_SSBD))\n\t\treturn mode;\n\n\tcmd = ssb_parse_cmdline();\n\tif (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS) &&\n\t    (cmd == SPEC_STORE_BYPASS_CMD_NONE ||\n\t     cmd == SPEC_STORE_BYPASS_CMD_AUTO))\n\t\treturn mode;\n\n\tswitch (cmd) {\n\tcase SPEC_STORE_BYPASS_CMD_AUTO:\n\tcase SPEC_STORE_BYPASS_CMD_SECCOMP:\n\t\t/*\n\t\t * Choose prctl+seccomp as the default mode if seccomp is\n\t\t * enabled.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_SECCOMP))\n\t\t\tmode = SPEC_STORE_BYPASS_SECCOMP;\n\t\telse\n\t\t\tmode = SPEC_STORE_BYPASS_PRCTL;\n\t\tbreak;\n\tcase SPEC_STORE_BYPASS_CMD_ON:\n\t\tmode = SPEC_STORE_BYPASS_DISABLE;\n\t\tbreak;\n\tcase SPEC_STORE_BYPASS_CMD_PRCTL:\n\t\tmode = SPEC_STORE_BYPASS_PRCTL;\n\t\tbreak;\n\tcase SPEC_STORE_BYPASS_CMD_NONE:\n\t\tbreak;\n\t}\n\n\t/*\n\t * We have three CPU feature flags that are in play here:\n\t *  - X86_BUG_SPEC_STORE_BYPASS - CPU is susceptible.\n\t *  - X86_FEATURE_SSBD - CPU is able to turn off speculative store bypass\n\t *  - X86_FEATURE_SPEC_STORE_BYPASS_DISABLE - engage the mitigation\n\t */\n\tif (mode == SPEC_STORE_BYPASS_DISABLE) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE);\n\t\t/*\n\t\t * Intel uses the SPEC CTRL MSR Bit(2) for this, while AMD may\n\t\t * use a completely different MSR and bit dependent on family.\n\t\t */\n\t\tif (!static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) &&\n\t\t    !static_cpu_has(X86_FEATURE_AMD_SSBD)) {\n\t\t\tx86_amd_ssb_disable();\n\t\t} else {\n\t\t\tx86_spec_ctrl_base |= SPEC_CTRL_SSBD;\n\t\t\tx86_spec_ctrl_mask |= SPEC_CTRL_SSBD;\n\t\t\twrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);\n\t\t}\n\t}\n\n\treturn mode;\n}\n\nstatic void ssb_select_mitigation(void)\n{\n\tssb_mode = __ssb_select_mitigation();\n\n\tif (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\tpr_info(\"%s\\n\", ssb_strings[ssb_mode]);\n}\n\n#undef pr_fmt\n#define pr_fmt(fmt)     \"Speculation prctl: \" fmt\n\nstatic int ssb_prctl_set(struct task_struct *task, unsigned long ctrl)\n{\n\tbool update;\n\n\tif (ssb_mode != SPEC_STORE_BYPASS_PRCTL &&\n\t    ssb_mode != SPEC_STORE_BYPASS_SECCOMP)\n\t\treturn -ENXIO;\n\n\tswitch (ctrl) {\n\tcase PR_SPEC_ENABLE:\n\t\t/* If speculation is force disabled, enable is not allowed */\n\t\tif (task_spec_ssb_force_disable(task))\n\t\t\treturn -EPERM;\n\t\ttask_clear_spec_ssb_disable(task);\n\t\tupdate = test_and_clear_tsk_thread_flag(task, TIF_SSBD);\n\t\tbreak;\n\tcase PR_SPEC_DISABLE:\n\t\ttask_set_spec_ssb_disable(task);\n\t\tupdate = !test_and_set_tsk_thread_flag(task, TIF_SSBD);\n\t\tbreak;\n\tcase PR_SPEC_FORCE_DISABLE:\n\t\ttask_set_spec_ssb_disable(task);\n\t\ttask_set_spec_ssb_force_disable(task);\n\t\tupdate = !test_and_set_tsk_thread_flag(task, TIF_SSBD);\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\t/*\n\t * If being set on non-current task, delay setting the CPU\n\t * mitigation until it is next scheduled.\n\t */\n\tif (task == current && update)\n\t\tspeculative_store_bypass_update_current();\n\n\treturn 0;\n}\n\nint arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,\n\t\t\t     unsigned long ctrl)\n{\n\tswitch (which) {\n\tcase PR_SPEC_STORE_BYPASS:\n\t\treturn ssb_prctl_set(task, ctrl);\n\tdefault:\n\t\treturn -ENODEV;\n\t}\n}\n\n#ifdef CONFIG_SECCOMP\nvoid arch_seccomp_spec_mitigate(struct task_struct *task)\n{\n\tif (ssb_mode == SPEC_STORE_BYPASS_SECCOMP)\n\t\tssb_prctl_set(task, PR_SPEC_FORCE_DISABLE);\n}\n#endif\n\nstatic int ssb_prctl_get(struct task_struct *task)\n{\n\tswitch (ssb_mode) {\n\tcase SPEC_STORE_BYPASS_DISABLE:\n\t\treturn PR_SPEC_DISABLE;\n\tcase SPEC_STORE_BYPASS_SECCOMP:\n\tcase SPEC_STORE_BYPASS_PRCTL:\n\t\tif (task_spec_ssb_force_disable(task))\n\t\t\treturn PR_SPEC_PRCTL | PR_SPEC_FORCE_DISABLE;\n\t\tif (task_spec_ssb_disable(task))\n\t\t\treturn PR_SPEC_PRCTL | PR_SPEC_DISABLE;\n\t\treturn PR_SPEC_PRCTL | PR_SPEC_ENABLE;\n\tdefault:\n\t\tif (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\t\treturn PR_SPEC_ENABLE;\n\t\treturn PR_SPEC_NOT_AFFECTED;\n\t}\n}\n\nint arch_prctl_spec_ctrl_get(struct task_struct *task, unsigned long which)\n{\n\tswitch (which) {\n\tcase PR_SPEC_STORE_BYPASS:\n\t\treturn ssb_prctl_get(task);\n\tdefault:\n\t\treturn -ENODEV;\n\t}\n}\n\nvoid x86_spec_ctrl_setup_ap(void)\n{\n\tif (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))\n\t\twrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);\n\n\tif (ssb_mode == SPEC_STORE_BYPASS_DISABLE)\n\t\tx86_amd_ssb_disable();\n}\n\n#ifdef CONFIG_SYSFS\n\nstatic ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,\n\t\t\t       char *buf, unsigned int bug)\n{\n\tif (!boot_cpu_has_bug(bug))\n\t\treturn sprintf(buf, \"Not affected\\n\");\n\n\tswitch (bug) {\n\tcase X86_BUG_CPU_MELTDOWN:\n\t\tif (boot_cpu_has(X86_FEATURE_PTI))\n\t\t\treturn sprintf(buf, \"Mitigation: PTI\\n\");\n\n\t\tif (hypervisor_is_type(X86_HYPER_XEN_PV))\n\t\t\treturn sprintf(buf, \"Unknown (XEN PV detected, hypervisor mitigation required)\\n\");\n\n\t\tbreak;\n\n\tcase X86_BUG_SPECTRE_V1:\n\t\treturn sprintf(buf, \"Mitigation: __user pointer sanitization\\n\");\n\n\tcase X86_BUG_SPECTRE_V2:\n\t\treturn sprintf(buf, \"%s%s%s%s\\n\", spectre_v2_strings[spectre_v2_enabled],\n\t\t\t       boot_cpu_has(X86_FEATURE_USE_IBPB) ? \", IBPB\" : \"\",\n\t\t\t       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? \", IBRS_FW\" : \"\",\n\t\t\t       spectre_v2_module_string());\n\n\tcase X86_BUG_SPEC_STORE_BYPASS:\n\t\treturn sprintf(buf, \"%s\\n\", ssb_strings[ssb_mode]);\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn sprintf(buf, \"Vulnerable\\n\");\n}\n\nssize_t cpu_show_meltdown(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_CPU_MELTDOWN);\n}\n\nssize_t cpu_show_spectre_v1(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V1);\n}\n\nssize_t cpu_show_spectre_v2(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V2);\n}\n\nssize_t cpu_show_spec_store_bypass(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_SPEC_STORE_BYPASS);\n}\n#endif\n"], "fixing_code": ["// SPDX-License-Identifier: GPL-2.0\n/*\n *  Copyright (C) 1994  Linus Torvalds\n *\n *  Cyrix stuff, June 1998 by:\n *\t- Rafael R. Reilova (moved everything from head.S),\n *        <rreilova@ececs.uc.edu>\n *\t- Channing Corn (tests & fixes),\n *\t- Andrew D. Balsa (code cleanup).\n */\n#include <linux/init.h>\n#include <linux/utsname.h>\n#include <linux/cpu.h>\n#include <linux/module.h>\n#include <linux/nospec.h>\n#include <linux/prctl.h>\n\n#include <asm/spec-ctrl.h>\n#include <asm/cmdline.h>\n#include <asm/bugs.h>\n#include <asm/processor.h>\n#include <asm/processor-flags.h>\n#include <asm/fpu/internal.h>\n#include <asm/msr.h>\n#include <asm/paravirt.h>\n#include <asm/alternative.h>\n#include <asm/pgtable.h>\n#include <asm/set_memory.h>\n#include <asm/intel-family.h>\n#include <asm/hypervisor.h>\n\nstatic void __init spectre_v2_select_mitigation(void);\nstatic void __init ssb_select_mitigation(void);\n\n/*\n * Our boot-time value of the SPEC_CTRL MSR. We read it once so that any\n * writes to SPEC_CTRL contain whatever reserved bits have been set.\n */\nu64 __ro_after_init x86_spec_ctrl_base;\nEXPORT_SYMBOL_GPL(x86_spec_ctrl_base);\n\n/*\n * The vendor and possibly platform specific bits which can be modified in\n * x86_spec_ctrl_base.\n */\nstatic u64 __ro_after_init x86_spec_ctrl_mask = SPEC_CTRL_IBRS;\n\n/*\n * AMD specific MSR info for Speculative Store Bypass control.\n * x86_amd_ls_cfg_ssbd_mask is initialized in identify_boot_cpu().\n */\nu64 __ro_after_init x86_amd_ls_cfg_base;\nu64 __ro_after_init x86_amd_ls_cfg_ssbd_mask;\n\nvoid __init check_bugs(void)\n{\n\tidentify_boot_cpu();\n\n\tif (!IS_ENABLED(CONFIG_SMP)) {\n\t\tpr_info(\"CPU: \");\n\t\tprint_cpu_info(&boot_cpu_data);\n\t}\n\n\t/*\n\t * Read the SPEC_CTRL MSR to account for reserved bits which may\n\t * have unknown values. AMD64_LS_CFG MSR is cached in the early AMD\n\t * init code as it is not enumerated and depends on the family.\n\t */\n\tif (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))\n\t\trdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);\n\n\t/* Allow STIBP in MSR_SPEC_CTRL if supported */\n\tif (boot_cpu_has(X86_FEATURE_STIBP))\n\t\tx86_spec_ctrl_mask |= SPEC_CTRL_STIBP;\n\n\t/* Select the proper spectre mitigation before patching alternatives */\n\tspectre_v2_select_mitigation();\n\n\t/*\n\t * Select proper mitigation for any exposure to the Speculative Store\n\t * Bypass vulnerability.\n\t */\n\tssb_select_mitigation();\n\n#ifdef CONFIG_X86_32\n\t/*\n\t * Check whether we are able to run this kernel safely on SMP.\n\t *\n\t * - i386 is no longer supported.\n\t * - In order to run on anything without a TSC, we need to be\n\t *   compiled for a i486.\n\t */\n\tif (boot_cpu_data.x86 < 4)\n\t\tpanic(\"Kernel requires i486+ for 'invlpg' and other features\");\n\n\tinit_utsname()->machine[1] =\n\t\t'0' + (boot_cpu_data.x86 > 6 ? 6 : boot_cpu_data.x86);\n\talternative_instructions();\n\n\tfpu__init_check_bugs();\n#else /* CONFIG_X86_64 */\n\talternative_instructions();\n\n\t/*\n\t * Make sure the first 2MB area is not mapped by huge pages\n\t * There are typically fixed size MTRRs in there and overlapping\n\t * MTRRs into large pages causes slow downs.\n\t *\n\t * Right now we don't do that with gbpages because there seems\n\t * very little benefit for that case.\n\t */\n\tif (!direct_gbpages)\n\t\tset_memory_4k((unsigned long)__va(0), 1);\n#endif\n}\n\n/* The kernel command line selection */\nenum spectre_v2_mitigation_cmd {\n\tSPECTRE_V2_CMD_NONE,\n\tSPECTRE_V2_CMD_AUTO,\n\tSPECTRE_V2_CMD_FORCE,\n\tSPECTRE_V2_CMD_RETPOLINE,\n\tSPECTRE_V2_CMD_RETPOLINE_GENERIC,\n\tSPECTRE_V2_CMD_RETPOLINE_AMD,\n};\n\nstatic const char *spectre_v2_strings[] = {\n\t[SPECTRE_V2_NONE]\t\t\t= \"Vulnerable\",\n\t[SPECTRE_V2_RETPOLINE_MINIMAL]\t\t= \"Vulnerable: Minimal generic ASM retpoline\",\n\t[SPECTRE_V2_RETPOLINE_MINIMAL_AMD]\t= \"Vulnerable: Minimal AMD ASM retpoline\",\n\t[SPECTRE_V2_RETPOLINE_GENERIC]\t\t= \"Mitigation: Full generic retpoline\",\n\t[SPECTRE_V2_RETPOLINE_AMD]\t\t= \"Mitigation: Full AMD retpoline\",\n};\n\n#undef pr_fmt\n#define pr_fmt(fmt)     \"Spectre V2 : \" fmt\n\nstatic enum spectre_v2_mitigation spectre_v2_enabled __ro_after_init =\n\tSPECTRE_V2_NONE;\n\nvoid\nx86_virt_spec_ctrl(u64 guest_spec_ctrl, u64 guest_virt_spec_ctrl, bool setguest)\n{\n\tu64 msrval, guestval, hostval = x86_spec_ctrl_base;\n\tstruct thread_info *ti = current_thread_info();\n\n\t/* Is MSR_SPEC_CTRL implemented ? */\n\tif (static_cpu_has(X86_FEATURE_MSR_SPEC_CTRL)) {\n\t\t/*\n\t\t * Restrict guest_spec_ctrl to supported values. Clear the\n\t\t * modifiable bits in the host base value and or the\n\t\t * modifiable bits from the guest value.\n\t\t */\n\t\tguestval = hostval & ~x86_spec_ctrl_mask;\n\t\tguestval |= guest_spec_ctrl & x86_spec_ctrl_mask;\n\n\t\t/* SSBD controlled in MSR_SPEC_CTRL */\n\t\tif (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) ||\n\t\t    static_cpu_has(X86_FEATURE_AMD_SSBD))\n\t\t\thostval |= ssbd_tif_to_spec_ctrl(ti->flags);\n\n\t\tif (hostval != guestval) {\n\t\t\tmsrval = setguest ? guestval : hostval;\n\t\t\twrmsrl(MSR_IA32_SPEC_CTRL, msrval);\n\t\t}\n\t}\n\n\t/*\n\t * If SSBD is not handled in MSR_SPEC_CTRL on AMD, update\n\t * MSR_AMD64_L2_CFG or MSR_VIRT_SPEC_CTRL if supported.\n\t */\n\tif (!static_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&\n\t    !static_cpu_has(X86_FEATURE_VIRT_SSBD))\n\t\treturn;\n\n\t/*\n\t * If the host has SSBD mitigation enabled, force it in the host's\n\t * virtual MSR value. If its not permanently enabled, evaluate\n\t * current's TIF_SSBD thread flag.\n\t */\n\tif (static_cpu_has(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE))\n\t\thostval = SPEC_CTRL_SSBD;\n\telse\n\t\thostval = ssbd_tif_to_spec_ctrl(ti->flags);\n\n\t/* Sanitize the guest value */\n\tguestval = guest_virt_spec_ctrl & SPEC_CTRL_SSBD;\n\n\tif (hostval != guestval) {\n\t\tunsigned long tif;\n\n\t\ttif = setguest ? ssbd_spec_ctrl_to_tif(guestval) :\n\t\t\t\t ssbd_spec_ctrl_to_tif(hostval);\n\n\t\tspeculative_store_bypass_update(tif);\n\t}\n}\nEXPORT_SYMBOL_GPL(x86_virt_spec_ctrl);\n\nstatic void x86_amd_ssb_disable(void)\n{\n\tu64 msrval = x86_amd_ls_cfg_base | x86_amd_ls_cfg_ssbd_mask;\n\n\tif (boot_cpu_has(X86_FEATURE_VIRT_SSBD))\n\t\twrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, SPEC_CTRL_SSBD);\n\telse if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD))\n\t\twrmsrl(MSR_AMD64_LS_CFG, msrval);\n}\n\n#ifdef RETPOLINE\nstatic bool spectre_v2_bad_module;\n\nbool retpoline_module_ok(bool has_retpoline)\n{\n\tif (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)\n\t\treturn true;\n\n\tpr_err(\"System may be vulnerable to spectre v2\\n\");\n\tspectre_v2_bad_module = true;\n\treturn false;\n}\n\nstatic inline const char *spectre_v2_module_string(void)\n{\n\treturn spectre_v2_bad_module ? \" - vulnerable module loaded\" : \"\";\n}\n#else\nstatic inline const char *spectre_v2_module_string(void) { return \"\"; }\n#endif\n\nstatic void __init spec2_print_if_insecure(const char *reason)\n{\n\tif (boot_cpu_has_bug(X86_BUG_SPECTRE_V2))\n\t\tpr_info(\"%s selected on command line.\\n\", reason);\n}\n\nstatic void __init spec2_print_if_secure(const char *reason)\n{\n\tif (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))\n\t\tpr_info(\"%s selected on command line.\\n\", reason);\n}\n\nstatic inline bool retp_compiler(void)\n{\n\treturn __is_defined(RETPOLINE);\n}\n\nstatic inline bool match_option(const char *arg, int arglen, const char *opt)\n{\n\tint len = strlen(opt);\n\n\treturn len == arglen && !strncmp(arg, opt, len);\n}\n\nstatic const struct {\n\tconst char *option;\n\tenum spectre_v2_mitigation_cmd cmd;\n\tbool secure;\n} mitigation_options[] = {\n\t{ \"off\",               SPECTRE_V2_CMD_NONE,              false },\n\t{ \"on\",                SPECTRE_V2_CMD_FORCE,             true },\n\t{ \"retpoline\",         SPECTRE_V2_CMD_RETPOLINE,         false },\n\t{ \"retpoline,amd\",     SPECTRE_V2_CMD_RETPOLINE_AMD,     false },\n\t{ \"retpoline,generic\", SPECTRE_V2_CMD_RETPOLINE_GENERIC, false },\n\t{ \"auto\",              SPECTRE_V2_CMD_AUTO,              false },\n};\n\nstatic enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)\n{\n\tchar arg[20];\n\tint ret, i;\n\tenum spectre_v2_mitigation_cmd cmd = SPECTRE_V2_CMD_AUTO;\n\n\tif (cmdline_find_option_bool(boot_command_line, \"nospectre_v2\"))\n\t\treturn SPECTRE_V2_CMD_NONE;\n\telse {\n\t\tret = cmdline_find_option(boot_command_line, \"spectre_v2\", arg, sizeof(arg));\n\t\tif (ret < 0)\n\t\t\treturn SPECTRE_V2_CMD_AUTO;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(mitigation_options); i++) {\n\t\t\tif (!match_option(arg, ret, mitigation_options[i].option))\n\t\t\t\tcontinue;\n\t\t\tcmd = mitigation_options[i].cmd;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (i >= ARRAY_SIZE(mitigation_options)) {\n\t\t\tpr_err(\"unknown option (%s). Switching to AUTO select\\n\", arg);\n\t\t\treturn SPECTRE_V2_CMD_AUTO;\n\t\t}\n\t}\n\n\tif ((cmd == SPECTRE_V2_CMD_RETPOLINE ||\n\t     cmd == SPECTRE_V2_CMD_RETPOLINE_AMD ||\n\t     cmd == SPECTRE_V2_CMD_RETPOLINE_GENERIC) &&\n\t    !IS_ENABLED(CONFIG_RETPOLINE)) {\n\t\tpr_err(\"%s selected but not compiled in. Switching to AUTO select\\n\", mitigation_options[i].option);\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tif (cmd == SPECTRE_V2_CMD_RETPOLINE_AMD &&\n\t    boot_cpu_data.x86_vendor != X86_VENDOR_AMD) {\n\t\tpr_err(\"retpoline,amd selected but CPU is not AMD. Switching to AUTO select\\n\");\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tif (mitigation_options[i].secure)\n\t\tspec2_print_if_secure(mitigation_options[i].option);\n\telse\n\t\tspec2_print_if_insecure(mitigation_options[i].option);\n\n\treturn cmd;\n}\n\nstatic void __init spectre_v2_select_mitigation(void)\n{\n\tenum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();\n\tenum spectre_v2_mitigation mode = SPECTRE_V2_NONE;\n\n\t/*\n\t * If the CPU is not affected and the command line mode is NONE or AUTO\n\t * then nothing to do.\n\t */\n\tif (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2) &&\n\t    (cmd == SPECTRE_V2_CMD_NONE || cmd == SPECTRE_V2_CMD_AUTO))\n\t\treturn;\n\n\tswitch (cmd) {\n\tcase SPECTRE_V2_CMD_NONE:\n\t\treturn;\n\n\tcase SPECTRE_V2_CMD_FORCE:\n\tcase SPECTRE_V2_CMD_AUTO:\n\t\tif (IS_ENABLED(CONFIG_RETPOLINE))\n\t\t\tgoto retpoline_auto;\n\t\tbreak;\n\tcase SPECTRE_V2_CMD_RETPOLINE_AMD:\n\t\tif (IS_ENABLED(CONFIG_RETPOLINE))\n\t\t\tgoto retpoline_amd;\n\t\tbreak;\n\tcase SPECTRE_V2_CMD_RETPOLINE_GENERIC:\n\t\tif (IS_ENABLED(CONFIG_RETPOLINE))\n\t\t\tgoto retpoline_generic;\n\t\tbreak;\n\tcase SPECTRE_V2_CMD_RETPOLINE:\n\t\tif (IS_ENABLED(CONFIG_RETPOLINE))\n\t\t\tgoto retpoline_auto;\n\t\tbreak;\n\t}\n\tpr_err(\"Spectre mitigation: kernel not compiled with retpoline; no mitigation available!\");\n\treturn;\n\nretpoline_auto:\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {\n\tretpoline_amd:\n\t\tif (!boot_cpu_has(X86_FEATURE_LFENCE_RDTSC)) {\n\t\t\tpr_err(\"Spectre mitigation: LFENCE not serializing, switching to generic retpoline\\n\");\n\t\t\tgoto retpoline_generic;\n\t\t}\n\t\tmode = retp_compiler() ? SPECTRE_V2_RETPOLINE_AMD :\n\t\t\t\t\t SPECTRE_V2_RETPOLINE_MINIMAL_AMD;\n\t\tsetup_force_cpu_cap(X86_FEATURE_RETPOLINE_AMD);\n\t\tsetup_force_cpu_cap(X86_FEATURE_RETPOLINE);\n\t} else {\n\tretpoline_generic:\n\t\tmode = retp_compiler() ? SPECTRE_V2_RETPOLINE_GENERIC :\n\t\t\t\t\t SPECTRE_V2_RETPOLINE_MINIMAL;\n\t\tsetup_force_cpu_cap(X86_FEATURE_RETPOLINE);\n\t}\n\n\tspectre_v2_enabled = mode;\n\tpr_info(\"%s\\n\", spectre_v2_strings[mode]);\n\n\t/*\n\t * If spectre v2 protection has been enabled, unconditionally fill\n\t * RSB during a context switch; this protects against two independent\n\t * issues:\n\t *\n\t *\t- RSB underflow (and switch to BTB) on Skylake+\n\t *\t- SpectreRSB variant of spectre v2 on X86_BUG_SPECTRE_V2 CPUs\n\t */\n\tsetup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);\n\tpr_info(\"Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\\n\");\n\n\t/* Initialize Indirect Branch Prediction Barrier if supported */\n\tif (boot_cpu_has(X86_FEATURE_IBPB)) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_USE_IBPB);\n\t\tpr_info(\"Spectre v2 mitigation: Enabling Indirect Branch Prediction Barrier\\n\");\n\t}\n\n\t/*\n\t * Retpoline means the kernel is safe because it has no indirect\n\t * branches. But firmware isn't, so use IBRS to protect that.\n\t */\n\tif (boot_cpu_has(X86_FEATURE_IBRS)) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_USE_IBRS_FW);\n\t\tpr_info(\"Enabling Restricted Speculation for firmware calls\\n\");\n\t}\n}\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"Speculative Store Bypass: \" fmt\n\nstatic enum ssb_mitigation ssb_mode __ro_after_init = SPEC_STORE_BYPASS_NONE;\n\n/* The kernel command line selection */\nenum ssb_mitigation_cmd {\n\tSPEC_STORE_BYPASS_CMD_NONE,\n\tSPEC_STORE_BYPASS_CMD_AUTO,\n\tSPEC_STORE_BYPASS_CMD_ON,\n\tSPEC_STORE_BYPASS_CMD_PRCTL,\n\tSPEC_STORE_BYPASS_CMD_SECCOMP,\n};\n\nstatic const char *ssb_strings[] = {\n\t[SPEC_STORE_BYPASS_NONE]\t= \"Vulnerable\",\n\t[SPEC_STORE_BYPASS_DISABLE]\t= \"Mitigation: Speculative Store Bypass disabled\",\n\t[SPEC_STORE_BYPASS_PRCTL]\t= \"Mitigation: Speculative Store Bypass disabled via prctl\",\n\t[SPEC_STORE_BYPASS_SECCOMP]\t= \"Mitigation: Speculative Store Bypass disabled via prctl and seccomp\",\n};\n\nstatic const struct {\n\tconst char *option;\n\tenum ssb_mitigation_cmd cmd;\n} ssb_mitigation_options[] = {\n\t{ \"auto\",\tSPEC_STORE_BYPASS_CMD_AUTO },    /* Platform decides */\n\t{ \"on\",\t\tSPEC_STORE_BYPASS_CMD_ON },      /* Disable Speculative Store Bypass */\n\t{ \"off\",\tSPEC_STORE_BYPASS_CMD_NONE },    /* Don't touch Speculative Store Bypass */\n\t{ \"prctl\",\tSPEC_STORE_BYPASS_CMD_PRCTL },   /* Disable Speculative Store Bypass via prctl */\n\t{ \"seccomp\",\tSPEC_STORE_BYPASS_CMD_SECCOMP }, /* Disable Speculative Store Bypass via prctl and seccomp */\n};\n\nstatic enum ssb_mitigation_cmd __init ssb_parse_cmdline(void)\n{\n\tenum ssb_mitigation_cmd cmd = SPEC_STORE_BYPASS_CMD_AUTO;\n\tchar arg[20];\n\tint ret, i;\n\n\tif (cmdline_find_option_bool(boot_command_line, \"nospec_store_bypass_disable\")) {\n\t\treturn SPEC_STORE_BYPASS_CMD_NONE;\n\t} else {\n\t\tret = cmdline_find_option(boot_command_line, \"spec_store_bypass_disable\",\n\t\t\t\t\t  arg, sizeof(arg));\n\t\tif (ret < 0)\n\t\t\treturn SPEC_STORE_BYPASS_CMD_AUTO;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(ssb_mitigation_options); i++) {\n\t\t\tif (!match_option(arg, ret, ssb_mitigation_options[i].option))\n\t\t\t\tcontinue;\n\n\t\t\tcmd = ssb_mitigation_options[i].cmd;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (i >= ARRAY_SIZE(ssb_mitigation_options)) {\n\t\t\tpr_err(\"unknown option (%s). Switching to AUTO select\\n\", arg);\n\t\t\treturn SPEC_STORE_BYPASS_CMD_AUTO;\n\t\t}\n\t}\n\n\treturn cmd;\n}\n\nstatic enum ssb_mitigation __init __ssb_select_mitigation(void)\n{\n\tenum ssb_mitigation mode = SPEC_STORE_BYPASS_NONE;\n\tenum ssb_mitigation_cmd cmd;\n\n\tif (!boot_cpu_has(X86_FEATURE_SSBD))\n\t\treturn mode;\n\n\tcmd = ssb_parse_cmdline();\n\tif (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS) &&\n\t    (cmd == SPEC_STORE_BYPASS_CMD_NONE ||\n\t     cmd == SPEC_STORE_BYPASS_CMD_AUTO))\n\t\treturn mode;\n\n\tswitch (cmd) {\n\tcase SPEC_STORE_BYPASS_CMD_AUTO:\n\tcase SPEC_STORE_BYPASS_CMD_SECCOMP:\n\t\t/*\n\t\t * Choose prctl+seccomp as the default mode if seccomp is\n\t\t * enabled.\n\t\t */\n\t\tif (IS_ENABLED(CONFIG_SECCOMP))\n\t\t\tmode = SPEC_STORE_BYPASS_SECCOMP;\n\t\telse\n\t\t\tmode = SPEC_STORE_BYPASS_PRCTL;\n\t\tbreak;\n\tcase SPEC_STORE_BYPASS_CMD_ON:\n\t\tmode = SPEC_STORE_BYPASS_DISABLE;\n\t\tbreak;\n\tcase SPEC_STORE_BYPASS_CMD_PRCTL:\n\t\tmode = SPEC_STORE_BYPASS_PRCTL;\n\t\tbreak;\n\tcase SPEC_STORE_BYPASS_CMD_NONE:\n\t\tbreak;\n\t}\n\n\t/*\n\t * We have three CPU feature flags that are in play here:\n\t *  - X86_BUG_SPEC_STORE_BYPASS - CPU is susceptible.\n\t *  - X86_FEATURE_SSBD - CPU is able to turn off speculative store bypass\n\t *  - X86_FEATURE_SPEC_STORE_BYPASS_DISABLE - engage the mitigation\n\t */\n\tif (mode == SPEC_STORE_BYPASS_DISABLE) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE);\n\t\t/*\n\t\t * Intel uses the SPEC CTRL MSR Bit(2) for this, while AMD may\n\t\t * use a completely different MSR and bit dependent on family.\n\t\t */\n\t\tif (!static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) &&\n\t\t    !static_cpu_has(X86_FEATURE_AMD_SSBD)) {\n\t\t\tx86_amd_ssb_disable();\n\t\t} else {\n\t\t\tx86_spec_ctrl_base |= SPEC_CTRL_SSBD;\n\t\t\tx86_spec_ctrl_mask |= SPEC_CTRL_SSBD;\n\t\t\twrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);\n\t\t}\n\t}\n\n\treturn mode;\n}\n\nstatic void ssb_select_mitigation(void)\n{\n\tssb_mode = __ssb_select_mitigation();\n\n\tif (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\tpr_info(\"%s\\n\", ssb_strings[ssb_mode]);\n}\n\n#undef pr_fmt\n#define pr_fmt(fmt)     \"Speculation prctl: \" fmt\n\nstatic int ssb_prctl_set(struct task_struct *task, unsigned long ctrl)\n{\n\tbool update;\n\n\tif (ssb_mode != SPEC_STORE_BYPASS_PRCTL &&\n\t    ssb_mode != SPEC_STORE_BYPASS_SECCOMP)\n\t\treturn -ENXIO;\n\n\tswitch (ctrl) {\n\tcase PR_SPEC_ENABLE:\n\t\t/* If speculation is force disabled, enable is not allowed */\n\t\tif (task_spec_ssb_force_disable(task))\n\t\t\treturn -EPERM;\n\t\ttask_clear_spec_ssb_disable(task);\n\t\tupdate = test_and_clear_tsk_thread_flag(task, TIF_SSBD);\n\t\tbreak;\n\tcase PR_SPEC_DISABLE:\n\t\ttask_set_spec_ssb_disable(task);\n\t\tupdate = !test_and_set_tsk_thread_flag(task, TIF_SSBD);\n\t\tbreak;\n\tcase PR_SPEC_FORCE_DISABLE:\n\t\ttask_set_spec_ssb_disable(task);\n\t\ttask_set_spec_ssb_force_disable(task);\n\t\tupdate = !test_and_set_tsk_thread_flag(task, TIF_SSBD);\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\n\t/*\n\t * If being set on non-current task, delay setting the CPU\n\t * mitigation until it is next scheduled.\n\t */\n\tif (task == current && update)\n\t\tspeculative_store_bypass_update_current();\n\n\treturn 0;\n}\n\nint arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,\n\t\t\t     unsigned long ctrl)\n{\n\tswitch (which) {\n\tcase PR_SPEC_STORE_BYPASS:\n\t\treturn ssb_prctl_set(task, ctrl);\n\tdefault:\n\t\treturn -ENODEV;\n\t}\n}\n\n#ifdef CONFIG_SECCOMP\nvoid arch_seccomp_spec_mitigate(struct task_struct *task)\n{\n\tif (ssb_mode == SPEC_STORE_BYPASS_SECCOMP)\n\t\tssb_prctl_set(task, PR_SPEC_FORCE_DISABLE);\n}\n#endif\n\nstatic int ssb_prctl_get(struct task_struct *task)\n{\n\tswitch (ssb_mode) {\n\tcase SPEC_STORE_BYPASS_DISABLE:\n\t\treturn PR_SPEC_DISABLE;\n\tcase SPEC_STORE_BYPASS_SECCOMP:\n\tcase SPEC_STORE_BYPASS_PRCTL:\n\t\tif (task_spec_ssb_force_disable(task))\n\t\t\treturn PR_SPEC_PRCTL | PR_SPEC_FORCE_DISABLE;\n\t\tif (task_spec_ssb_disable(task))\n\t\t\treturn PR_SPEC_PRCTL | PR_SPEC_DISABLE;\n\t\treturn PR_SPEC_PRCTL | PR_SPEC_ENABLE;\n\tdefault:\n\t\tif (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\t\treturn PR_SPEC_ENABLE;\n\t\treturn PR_SPEC_NOT_AFFECTED;\n\t}\n}\n\nint arch_prctl_spec_ctrl_get(struct task_struct *task, unsigned long which)\n{\n\tswitch (which) {\n\tcase PR_SPEC_STORE_BYPASS:\n\t\treturn ssb_prctl_get(task);\n\tdefault:\n\t\treturn -ENODEV;\n\t}\n}\n\nvoid x86_spec_ctrl_setup_ap(void)\n{\n\tif (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))\n\t\twrmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);\n\n\tif (ssb_mode == SPEC_STORE_BYPASS_DISABLE)\n\t\tx86_amd_ssb_disable();\n}\n\n#ifdef CONFIG_SYSFS\n\nstatic ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,\n\t\t\t       char *buf, unsigned int bug)\n{\n\tif (!boot_cpu_has_bug(bug))\n\t\treturn sprintf(buf, \"Not affected\\n\");\n\n\tswitch (bug) {\n\tcase X86_BUG_CPU_MELTDOWN:\n\t\tif (boot_cpu_has(X86_FEATURE_PTI))\n\t\t\treturn sprintf(buf, \"Mitigation: PTI\\n\");\n\n\t\tif (hypervisor_is_type(X86_HYPER_XEN_PV))\n\t\t\treturn sprintf(buf, \"Unknown (XEN PV detected, hypervisor mitigation required)\\n\");\n\n\t\tbreak;\n\n\tcase X86_BUG_SPECTRE_V1:\n\t\treturn sprintf(buf, \"Mitigation: __user pointer sanitization\\n\");\n\n\tcase X86_BUG_SPECTRE_V2:\n\t\treturn sprintf(buf, \"%s%s%s%s\\n\", spectre_v2_strings[spectre_v2_enabled],\n\t\t\t       boot_cpu_has(X86_FEATURE_USE_IBPB) ? \", IBPB\" : \"\",\n\t\t\t       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? \", IBRS_FW\" : \"\",\n\t\t\t       spectre_v2_module_string());\n\n\tcase X86_BUG_SPEC_STORE_BYPASS:\n\t\treturn sprintf(buf, \"%s\\n\", ssb_strings[ssb_mode]);\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn sprintf(buf, \"Vulnerable\\n\");\n}\n\nssize_t cpu_show_meltdown(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_CPU_MELTDOWN);\n}\n\nssize_t cpu_show_spectre_v1(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V1);\n}\n\nssize_t cpu_show_spectre_v2(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V2);\n}\n\nssize_t cpu_show_spec_store_bypass(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_SPEC_STORE_BYPASS);\n}\n#endif\n"], "filenames": ["arch/x86/kernel/cpu/bugs.c"], "buggy_code_start_loc": [316], "buggy_code_end_loc": [409], "fixing_code_start_loc": [315], "fixing_code_end_loc": [385], "type": "NVD-CWE-noinfo", "message": "The spectre_v2_select_mitigation function in arch/x86/kernel/cpu/bugs.c in the Linux kernel before 4.18.1 does not always fill RSB upon a context switch, which makes it easier for attackers to conduct userspace-userspace spectreRSB attacks.", "other": {"cve": {"id": "CVE-2018-15572", "sourceIdentifier": "cve@mitre.org", "published": "2018-08-20T02:29:00.263", "lastModified": "2019-10-03T00:03:26.223", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The spectre_v2_select_mitigation function in arch/x86/kernel/cpu/bugs.c in the Linux kernel before 4.18.1 does not always fill RSB upon a context switch, which makes it easier for attackers to conduct userspace-userspace spectreRSB attacks."}, {"lang": "es", "value": "La funci\u00f3n spectre_v2_select_mitigation en arch/x86/kernel/cpu/bugs.c en el kernel de Linux en versiones anteriores a la 4.18.1 no siempre completa RSB en un cambio de contexto, lo que hace que sea m\u00e1s f\u00e1cil para los atacantes realizar ataques spectreRSB espacio de usuario-espacio de usuario."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 6.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 2.0, "impactScore": 4.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 2.1}, "baseSeverity": "LOW", "exploitabilityScore": 3.9, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "NVD-CWE-noinfo"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:8.0:*:*:*:*:*:*:*", "matchCriteriaId": "C11E6FB0-C8C0-4527-9AA0-CB9B316F8F43"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:12.04:*:*:*:esm:*:*:*", "matchCriteriaId": "8D305F7A-D159-4716-AB26-5E38BB5CD991"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:14.04:*:*:*:lts:*:*:*", "matchCriteriaId": "B5A6F2F3-4894-4392-8296-3B8DD2679084"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:16.04:*:*:*:lts:*:*:*", "matchCriteriaId": "F7016A2A-8365-4F1A-89A2-7A19F2BCAE5B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndExcluding": "4.18.1", "matchCriteriaId": "C4E114F9-94BB-4D0A-A648-C9D431110D2F"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=fdf82a7856b32d905c39afc85e34364491e46346", "source": "cve@mitre.org", "tags": ["Mitigation", "Patch", "Vendor Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v4.x/ChangeLog-4.18.1", "source": "cve@mitre.org", "tags": ["Mitigation", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/fdf82a7856b32d905c39afc85e34364491e46346", "source": "cve@mitre.org", "tags": ["Mitigation", "Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2018/10/msg00003.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3775-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3775-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3776-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3776-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3777-1/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3777-2/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://usn.ubuntu.com/3777-3/", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://www.debian.org/security/2018/dsa-4308", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/fdf82a7856b32d905c39afc85e34364491e46346"}}