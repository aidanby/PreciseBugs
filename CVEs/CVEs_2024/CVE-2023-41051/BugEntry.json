{"buggy_code": ["# Changelog\n## [Unreleased]\n\n### Added\n\n### Changed\n\n### Fixed\n\n### Deprecated\n\n## [v0.12.1]\n\n### Fixed\n- [[#241]](https://github.com/rust-vmm/vm-memory/pull/245) mmap_xen: Don't drop\n  the FileOffset while in use #245\n\n## [Unreleased]\n\n### Deprecated\n\n- [[#244]](https://github.com/rust-vmm/vm-memory/pull/241) Deprecate volatile\n  memory's `as_ptr()` interfaces. The new interfaces to be used instead are:\n  `ptr_guard()` and `ptr_guard_mut()`.\n\n## [v0.12.0]\n\n### Added\n- [[#241]](https://github.com/rust-vmm/vm-memory/pull/241) Add Xen memory\n  mapping support: Foreign and Grant. Add new API for accessing pointers to\n  volatile slices, as `as_ptr()` can't be used with Xen's Grant mapping.\n- [[#237]](https://github.com/rust-vmm/vm-memory/pull/237) Implement `ByteValued` for `i/u128`.\n\n## [v0.11.0]\n\n### Added\n- [[#216]](https://github.com/rust-vmm/vm-memory/pull/216) Add `GuestRegionMmap::from_region`.\n\n### Fixed\n- [[#217]](https://github.com/rust-vmm/vm-memory/pull/217) Fix vm-memory internally\n  taking rust-style slices to guest memory in ways that could potentially cause \n  undefined behavior. Removes/deprecates various `as_slice`/`as_slice_mut` methods\n  whose usage violated rust's aliasing rules, as well as an unsound \n  `impl<'a> VolatileMemory for &'a mut [u8]`.\n\n## [v0.10.0]\n\n### Changed\n- [[#208]](https://github.com/rust-vmm/vm-memory/issues/208) Updated\n  vmm-sys-util dependency to v0.11.0\n- [[#203]](https://github.com/rust-vmm/vm-memory/pull/203) Switched to Rust\n  edition 2021.\n\n## [v0.9.0]\n\n### Fixed\n\n- [[#195]](https://github.com/rust-vmm/vm-memory/issues/195):\n  `mmap::check_file_offset` is doing the correct size validation for block and\n  char devices as well.\n\n### Changed\n\n- [[#198]](https://github.com/rust-vmm/vm-memory/pull/198): atomic: enable 64\n  bit atomics on ppc64le and s390x.\n- [[#200]](https://github.com/rust-vmm/vm-memory/pull/200): docs: enable all\n  features in `docs.rs`.\n- [[#199]](https://github.com/rust-vmm/vm-memory/issues/199): Update the way\n  the dependencies are pulled such that we don't end up with incompatible\n  versions.\n\n## [v0.8.0]\n\n### Fixed\n\n- [[#190]](https://github.com/rust-vmm/vm-memory/pull/190):\n  `VolatileSlice::read/write` when input slice is empty.\n\n## [v0.7.0]\n\n### Changed\n\n- [[#176]](https://github.com/rust-vmm/vm-memory/pull/176): Relax the trait\n  bounds of `Bytes` auto impl for `T: GuestMemory`\n- [[#178]](https://github.com/rust-vmm/vm-memory/pull/178):\n  `MmapRegion::build_raw` no longer requires that the length of the region is a\n  multiple of the page size.\n\n## [v0.6.0]\n\n### Added\n\n  - [[#160]](https://github.com/rust-vmm/vm-memory/pull/160): Add `ArcRef` and `AtomicBitmapArc` bitmap\n    backend implementations.\n  - [[#149]](https://github.com/rust-vmm/vm-memory/issues/149): Implement builder for MmapRegion.\n  - [[#140]](https://github.com/rust-vmm/vm-memory/issues/140): Add dirty bitmap tracking abstractions. \n\n### Deprecated \n\n  - [[#133]](https://github.com/rust-vmm/vm-memory/issues/8): Deprecate `GuestMemory::with_regions()`,\n   `GuestMemory::with_regions_mut()`, `GuestMemory::map_and_fold()`.\n\n## [v0.5.0]\n\n### Added\n\n- [[#8]](https://github.com/rust-vmm/vm-memory/issues/8): Add GuestMemory method to return an Iterator\n- [[#120]](https://github.com/rust-vmm/vm-memory/pull/120): Add is_hugetlbfs() to GuestMemoryRegion\n- [[#126]](https://github.com/rust-vmm/vm-memory/pull/126): Add VolatileSlice::split_at()\n- [[#128]](https://github.com/rust-vmm/vm-memory/pull/128): Add VolatileSlice::subslice()\n\n## [v0.4.0]\n\n### Fixed\n\n- [[#100]](https://github.com/rust-vmm/vm-memory/issues/100): Performance\n  degradation after fixing [#95](https://github.com/rust-vmm/vm-memory/pull/95).\n- [[#122]](https://github.com/rust-vmm/vm-memory/pull/122): atomic,\n  Cargo.toml: Update for arc-swap 1.0.0.\n\n## [v0.3.0]\n\n### Added\n\n- [[#109]](https://github.com/rust-vmm/vm-memory/pull/109): Added `build_raw` to\n  `MmapRegion` which can be used to operate on externally created mappings.\n- [[#101]](https://github.com/rust-vmm/vm-memory/pull/101): Added `check_range` for\n  GuestMemory which could be used to validate a range of guest memory.\n- [[#115]](https://github.com/rust-vmm/vm-memory/pull/115): Add methods for atomic\n  access to `Bytes`.\n\n### Fixed\n\n- [[#93]](https://github.com/rust-vmm/vm-memory/issues/93): DoS issue when using\n  virtio with rust-vmm/vm-memory.\n- [[#106]](https://github.com/rust-vmm/vm-memory/issues/106): Asserts trigger\n  on zero-length access.  \n\n### Removed\n\n- `integer-atomics` is no longer a distinct feature of the crate.\n\n## [v0.2.0]\n\n### Added\n\n- [[#76]](https://github.com/rust-vmm/vm-memory/issues/76): Added `get_slice` and\n  `as_volatile_slice` to `GuestMemoryRegion`.\n- [[#82]](https://github.com/rust-vmm/vm-memory/issues/82): Added `Clone` bound\n  for `GuestAddressSpace::T`, the return value of `GuestAddressSpace::memory()`.\n- [[#88]](https://github.com/rust-vmm/vm-memory/issues/88): Added `as_bytes` for\n  `ByteValued` which can be used for reading into POD structures from\n  raw bytes.\n\n## [v0.1.0]\n\n### Added\n\n- Added traits for working with VM memory.\n- Added a mmap based implemention for the Guest Memory.\n", "[package]\nname = \"vm-memory\"\nversion = \"0.12.1\"\ndescription = \"Safe abstractions for accessing the VM physical memory\"\nkeywords = [\"memory\"]\ncategories = [\"memory-management\"]\nauthors = [\"Liu Jiang <gerry@linux.alibaba.com>\"]\nrepository = \"https://github.com/rust-vmm/vm-memory\"\nreadme = \"README.md\"\nlicense = \"Apache-2.0 OR BSD-3-Clause\"\nedition = \"2021\"\nautobenches = false\n\n[features]\ndefault = []\nbackend-bitmap = []\nbackend-mmap = []\nbackend-atomic = [\"arc-swap\"]\nxen = [\"backend-mmap\", \"bitflags\", \"vmm-sys-util\"]\n\n[dependencies]\nlibc = \"0.2.39\"\narc-swap = { version = \"1.0.0\", optional = true }\nbitflags = { version = \"1.0\", optional = true }\nthiserror = \"1.0.40\"\nvmm-sys-util = { version = \"0.11.0\", optional = true }\n\n[target.'cfg(windows)'.dependencies.winapi]\nversion = \"0.3\"\nfeatures = [\"errhandlingapi\", \"sysinfoapi\"]\n\n[dev-dependencies]\ncriterion = \"0.3.0\"\nmatches = \"0.1.0\"\nvmm-sys-util = \"0.11.0\"\n\n[[bench]]\nname = \"main\"\nharness = false\n\n[profile.bench]\nlto = true\ncodegen-units = 1\n\n[package.metadata.docs.rs]\nall-features = true\n", "// Portions Copyright 2019 Red Hat, Inc.\n//\n// Copyright 2017 The Chromium OS Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the THIRT-PARTY file.\n//\n// SPDX-License-Identifier: Apache-2.0 OR BSD-3-Clause\n\n//! Types for volatile access to memory.\n//!\n//! Two of the core rules for safe rust is no data races and no aliased mutable references.\n//! `VolatileRef` and `VolatileSlice`, along with types that produce those which implement\n//! `VolatileMemory`, allow us to sidestep that rule by wrapping pointers that absolutely have to be\n//! accessed volatile. Some systems really do need to operate on shared memory and can't have the\n//! compiler reordering or eliding access because it has no visibility into what other systems are\n//! doing with that hunk of memory.\n//!\n//! For the purposes of maintaining safety, volatile memory has some rules of its own:\n//! 1. No references or slices to volatile memory (`&` or `&mut`).\n//! 2. Access should always been done with a volatile read or write.\n//! The First rule is because having references of any kind to memory considered volatile would\n//! violate pointer aliasing. The second is because unvolatile accesses are inherently undefined if\n//! done concurrently without synchronization. With volatile access we know that the compiler has\n//! not reordered or elided the access.\n\nuse std::cmp::min;\nuse std::io::{self, Read, Write};\nuse std::marker::PhantomData;\nuse std::mem::{align_of, size_of};\nuse std::ptr::copy;\nuse std::ptr::{read_volatile, write_volatile};\nuse std::result;\nuse std::sync::atomic::Ordering;\nuse std::usize;\n\nuse crate::atomic_integer::AtomicInteger;\nuse crate::bitmap::{Bitmap, BitmapSlice, BS};\nuse crate::{AtomicAccess, ByteValued, Bytes};\n\n#[cfg(all(feature = \"backend-mmap\", feature = \"xen\", unix))]\nuse crate::mmap_xen::{MmapXen as MmapInfo, MmapXenSlice};\n\n#[cfg(not(feature = \"xen\"))]\ntype MmapInfo = std::marker::PhantomData<()>;\n\nuse copy_slice_impl::{copy_from_volatile_slice, copy_to_volatile_slice};\n\n/// `VolatileMemory` related errors.\n#[allow(missing_docs)]\n#[derive(Debug, thiserror::Error)]\npub enum Error {\n    /// `addr` is out of bounds of the volatile memory slice.\n    #[error(\"address 0x{addr:x} is out of bounds\")]\n    OutOfBounds { addr: usize },\n    /// Taking a slice at `base` with `offset` would overflow `usize`.\n    #[error(\"address 0x{base:x} offset by 0x{offset:x} would overflow\")]\n    Overflow { base: usize, offset: usize },\n    /// Taking a slice whose size overflows `usize`.\n    #[error(\"{nelements:?} elements of size {size:?} would overflow a usize\")]\n    TooBig { nelements: usize, size: usize },\n    /// Trying to obtain a misaligned reference.\n    #[error(\"address 0x{addr:x} is not aligned to {alignment:?}\")]\n    Misaligned { addr: usize, alignment: usize },\n    /// Writing to memory failed\n    #[error(\"{0}\")]\n    IOError(io::Error),\n    /// Incomplete read or write\n    #[error(\"only used {completed} bytes in {expected} long buffer\")]\n    PartialBuffer { expected: usize, completed: usize },\n}\n\n/// Result of volatile memory operations.\npub type Result<T> = result::Result<T, Error>;\n\n/// Convenience function for computing `base + offset`.\n///\n/// # Errors\n///\n/// Returns [`Err(Error::Overflow)`](enum.Error.html#variant.Overflow) in case `base + offset`\n/// exceeds `usize::MAX`.\n///\n/// # Examples\n///\n/// ```\n/// # use vm_memory::volatile_memory::compute_offset;\n/// #\n/// assert_eq!(108, compute_offset(100, 8).unwrap());\n/// assert!(compute_offset(std::usize::MAX, 6).is_err());\n/// ```\npub fn compute_offset(base: usize, offset: usize) -> Result<usize> {\n    match base.checked_add(offset) {\n        None => Err(Error::Overflow { base, offset }),\n        Some(m) => Ok(m),\n    }\n}\n\n/// Types that support raw volatile access to their data.\npub trait VolatileMemory {\n    /// Type used for dirty memory tracking.\n    type B: Bitmap;\n\n    /// Gets the size of this slice.\n    fn len(&self) -> usize;\n\n    /// Check whether the region is empty.\n    fn is_empty(&self) -> bool {\n        self.len() == 0\n    }\n\n    /// Returns a [`VolatileSlice`](struct.VolatileSlice.html) of `count` bytes starting at\n    /// `offset`.\n    fn get_slice(&self, offset: usize, count: usize) -> Result<VolatileSlice<BS<Self::B>>>;\n\n    /// Gets a slice of memory for the entire region that supports volatile access.\n    fn as_volatile_slice(&self) -> VolatileSlice<BS<Self::B>> {\n        self.get_slice(0, self.len()).unwrap()\n    }\n\n    /// Gets a `VolatileRef` at `offset`.\n    fn get_ref<T: ByteValued>(&self, offset: usize) -> Result<VolatileRef<T, BS<Self::B>>> {\n        let slice = self.get_slice(offset, size_of::<T>())?;\n        // SAFETY: This is safe because the pointer is range-checked by get_slice, and\n        // the lifetime is the same as self.\n        unsafe {\n            Ok(VolatileRef::with_bitmap(\n                slice.addr,\n                slice.bitmap,\n                slice.mmap,\n            ))\n        }\n    }\n\n    /// Returns a [`VolatileArrayRef`](struct.VolatileArrayRef.html) of `n` elements starting at\n    /// `offset`.\n    fn get_array_ref<T: ByteValued>(\n        &self,\n        offset: usize,\n        n: usize,\n    ) -> Result<VolatileArrayRef<T, BS<Self::B>>> {\n        // Use isize to avoid problems with ptr::offset and ptr::add down the line.\n        let nbytes = isize::try_from(n)\n            .ok()\n            .and_then(|n| n.checked_mul(size_of::<T>() as isize))\n            .ok_or(Error::TooBig {\n                nelements: n,\n                size: size_of::<T>(),\n            })?;\n        let slice = self.get_slice(offset, nbytes as usize)?;\n        // SAFETY: This is safe because the pointer is range-checked by get_slice, and\n        // the lifetime is the same as self.\n        unsafe {\n            Ok(VolatileArrayRef::with_bitmap(\n                slice.addr,\n                n,\n                slice.bitmap,\n                slice.mmap,\n            ))\n        }\n    }\n\n    /// Returns a reference to an instance of `T` at `offset`.\n    ///\n    /// # Safety\n    /// To use this safely, the caller must guarantee that there are no other\n    /// users of the given chunk of memory for the lifetime of the result.\n    ///\n    /// # Errors\n    ///\n    /// If the resulting pointer is not aligned, this method will return an\n    /// [`Error`](enum.Error.html).\n    unsafe fn aligned_as_ref<T: ByteValued>(&self, offset: usize) -> Result<&T> {\n        let slice = self.get_slice(offset, size_of::<T>())?;\n        slice.check_alignment(align_of::<T>())?;\n        Ok(&*(slice.addr as *const T))\n    }\n\n    /// Returns a mutable reference to an instance of `T` at `offset`. Mutable accesses performed\n    /// using the resulting reference are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that there are no other\n    /// users of the given chunk of memory for the lifetime of the result.\n    ///\n    /// # Errors\n    ///\n    /// If the resulting pointer is not aligned, this method will return an\n    /// [`Error`](enum.Error.html).\n    unsafe fn aligned_as_mut<T: ByteValued>(&self, offset: usize) -> Result<&mut T> {\n        let slice = self.get_slice(offset, size_of::<T>())?;\n        slice.check_alignment(align_of::<T>())?;\n\n        Ok(&mut *(slice.addr as *mut T))\n    }\n\n    /// Returns a reference to an instance of `T` at `offset`. Mutable accesses performed\n    /// using the resulting reference are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    ///\n    /// # Errors\n    ///\n    /// If the resulting pointer is not aligned, this method will return an\n    /// [`Error`](enum.Error.html).\n    fn get_atomic_ref<T: AtomicInteger>(&self, offset: usize) -> Result<&T> {\n        let slice = self.get_slice(offset, size_of::<T>())?;\n        slice.check_alignment(align_of::<T>())?;\n\n        // SAFETY: This is safe because the pointer is range-checked by get_slice, and\n        // the lifetime is the same as self.\n        unsafe { Ok(&*(slice.addr as *const T)) }\n    }\n\n    /// Returns the sum of `base` and `offset` if the resulting address is valid.\n    fn compute_end_offset(&self, base: usize, offset: usize) -> Result<usize> {\n        let mem_end = compute_offset(base, offset)?;\n        if mem_end > self.len() {\n            return Err(Error::OutOfBounds { addr: mem_end });\n        }\n        Ok(mem_end)\n    }\n}\n\nimpl<'a> From<&'a mut [u8]> for VolatileSlice<'a, ()> {\n    fn from(value: &'a mut [u8]) -> Self {\n        // SAFETY: Since we construct the VolatileSlice from a rust slice, we know that\n        // the memory at addr `value as *mut u8` is valid for reads and writes (because mutable\n        // reference) of len `value.len()`. Since the `VolatileSlice` inherits the lifetime `'a`,\n        // it is not possible to access/mutate `value` while the VolatileSlice is alive.\n        //\n        // Note that it is possible for multiple aliasing sub slices of this `VolatileSlice`s to\n        // be created through `VolatileSlice::subslice`. This is OK, as pointers are allowed to\n        // alias, and it is impossible to get rust-style references from a `VolatileSlice`.\n        unsafe { VolatileSlice::new(value.as_mut_ptr(), value.len()) }\n    }\n}\n\n#[repr(C, packed)]\nstruct Packed<T>(T);\n\n/// A guard to perform mapping and protect unmapping of the memory.\npub struct PtrGuard {\n    addr: *mut u8,\n    len: usize,\n\n    // This isn't used anymore, but it protects the slice from getting unmapped while in use.\n    // Once this goes out of scope, the memory is unmapped automatically.\n    #[cfg(all(feature = \"xen\", unix))]\n    _slice: MmapXenSlice,\n}\n\n#[allow(clippy::len_without_is_empty)]\nimpl PtrGuard {\n    #[allow(unused_variables)]\n    fn new(mmap: Option<&MmapInfo>, addr: *mut u8, prot: i32, len: usize) -> Self {\n        #[cfg(all(feature = \"xen\", unix))]\n        let (addr, _slice) = {\n            let slice = MmapInfo::mmap(mmap, addr, prot, len);\n            (slice.addr(), slice)\n        };\n\n        Self {\n            addr,\n            len,\n\n            #[cfg(all(feature = \"xen\", unix))]\n            _slice,\n        }\n    }\n\n    fn read(mmap: Option<&MmapInfo>, addr: *mut u8, len: usize) -> Self {\n        Self::new(mmap, addr, libc::PROT_READ, len)\n    }\n\n    /// Returns a non-mutable pointer to the beginning of the slice.\n    pub fn as_ptr(&self) -> *const u8 {\n        self.addr\n    }\n\n    /// Gets the length of the mapped region.\n    pub fn len(&self) -> usize {\n        self.len\n    }\n}\n\n/// A mutable guard to perform mapping and protect unmapping of the memory.\npub struct PtrGuardMut(PtrGuard);\n\n#[allow(clippy::len_without_is_empty)]\nimpl PtrGuardMut {\n    fn write(mmap: Option<&MmapInfo>, addr: *mut u8, len: usize) -> Self {\n        Self(PtrGuard::new(mmap, addr, libc::PROT_WRITE, len))\n    }\n\n    /// Returns a mutable pointer to the beginning of the slice. Mutable accesses performed\n    /// using the resulting pointer are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    pub fn as_ptr(&self) -> *mut u8 {\n        self.0.addr\n    }\n\n    /// Gets the length of the mapped region.\n    pub fn len(&self) -> usize {\n        self.0.len\n    }\n}\n\n/// A slice of raw memory that supports volatile access.\n#[derive(Clone, Copy, Debug)]\npub struct VolatileSlice<'a, B = ()> {\n    addr: *mut u8,\n    size: usize,\n    bitmap: B,\n    mmap: Option<&'a MmapInfo>,\n}\n\nimpl<'a> VolatileSlice<'a, ()> {\n    /// Creates a slice of raw memory that must support volatile access.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is `size` bytes long\n    /// and is available for the duration of the lifetime of the new `VolatileSlice`. The caller\n    /// must also guarantee that all other users of the given chunk of memory are using volatile\n    /// accesses.\n    pub unsafe fn new(addr: *mut u8, size: usize) -> VolatileSlice<'a> {\n        Self::with_bitmap(addr, size, (), None)\n    }\n}\n\nimpl<'a, B: BitmapSlice> VolatileSlice<'a, B> {\n    /// Creates a slice of raw memory that must support volatile access, and uses the provided\n    /// `bitmap` object for dirty page tracking.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is `size` bytes long\n    /// and is available for the duration of the lifetime of the new `VolatileSlice`. The caller\n    /// must also guarantee that all other users of the given chunk of memory are using volatile\n    /// accesses.\n    pub unsafe fn with_bitmap(\n        addr: *mut u8,\n        size: usize,\n        bitmap: B,\n        mmap: Option<&'a MmapInfo>,\n    ) -> VolatileSlice<'a, B> {\n        VolatileSlice {\n            addr,\n            size,\n            bitmap,\n            mmap,\n        }\n    }\n\n    /// Returns a pointer to the beginning of the slice. Mutable accesses performed\n    /// using the resulting pointer are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    #[deprecated(\n        since = \"0.12.1\",\n        note = \"Use `.ptr_guard()` or `.ptr_guard_mut()` instead\"\n    )]\n    #[cfg(not(all(feature = \"xen\", unix)))]\n    pub fn as_ptr(&self) -> *mut u8 {\n        self.addr\n    }\n\n    /// Returns a guard for the pointer to the underlying memory.\n    pub fn ptr_guard(&self) -> PtrGuard {\n        PtrGuard::read(self.mmap, self.addr, self.len())\n    }\n\n    /// Returns a mutable guard for the pointer to the underlying memory.\n    pub fn ptr_guard_mut(&self) -> PtrGuardMut {\n        PtrGuardMut::write(self.mmap, self.addr, self.len())\n    }\n\n    /// Gets the size of this slice.\n    pub fn len(&self) -> usize {\n        self.size\n    }\n\n    /// Checks if the slice is empty.\n    pub fn is_empty(&self) -> bool {\n        self.size == 0\n    }\n\n    /// Borrows the inner `BitmapSlice`.\n    pub fn bitmap(&self) -> &B {\n        &self.bitmap\n    }\n\n    /// Divides one slice into two at an index.\n    ///\n    /// # Example\n    ///\n    /// ```\n    /// # use vm_memory::{VolatileMemory, VolatileSlice};\n    /// #\n    /// # // Create a buffer\n    /// # let mut mem = [0u8; 32];\n    /// #\n    /// # // Get a `VolatileSlice` from the buffer\n    /// let vslice = VolatileSlice::from(&mut mem[..]);\n    ///\n    /// let (start, end) = vslice.split_at(8).expect(\"Could not split VolatileSlice\");\n    /// assert_eq!(8, start.len());\n    /// assert_eq!(24, end.len());\n    /// ```\n    pub fn split_at(&self, mid: usize) -> Result<(Self, Self)> {\n        let end = self.offset(mid)?;\n        let start =\n            // SAFETY: safe because self.offset() already checked the bounds\n            unsafe { VolatileSlice::with_bitmap(self.addr, mid, self.bitmap.clone(), self.mmap) };\n\n        Ok((start, end))\n    }\n\n    /// Returns a subslice of this [`VolatileSlice`](struct.VolatileSlice.html) starting at\n    /// `offset` with `count` length.\n    ///\n    /// The returned subslice is a copy of this slice with the address increased by `offset` bytes\n    /// and the size set to `count` bytes.\n    pub fn subslice(&self, offset: usize, count: usize) -> Result<Self> {\n        let _ = self.compute_end_offset(offset, count)?;\n\n        // SAFETY: This is safe because the pointer is range-checked by compute_end_offset, and\n        // the lifetime is the same as the original slice.\n        unsafe {\n            Ok(VolatileSlice::with_bitmap(\n                self.addr.add(offset),\n                count,\n                self.bitmap.slice_at(offset),\n                self.mmap,\n            ))\n        }\n    }\n\n    /// Returns a subslice of this [`VolatileSlice`](struct.VolatileSlice.html) starting at\n    /// `offset`.\n    ///\n    /// The returned subslice is a copy of this slice with the address increased by `count` bytes\n    /// and the size reduced by `count` bytes.\n    pub fn offset(&self, count: usize) -> Result<VolatileSlice<'a, B>> {\n        let new_addr = (self.addr as usize)\n            .checked_add(count)\n            .ok_or(Error::Overflow {\n                base: self.addr as usize,\n                offset: count,\n            })?;\n        let new_size = self\n            .size\n            .checked_sub(count)\n            .ok_or(Error::OutOfBounds { addr: new_addr })?;\n        // SAFETY: Safe because the memory has the same lifetime and points to a subset of the\n        // memory of the original slice.\n        unsafe {\n            Ok(VolatileSlice::with_bitmap(\n                self.addr.add(count),\n                new_size,\n                self.bitmap.slice_at(count),\n                self.mmap,\n            ))\n        }\n    }\n\n    /// Copies as many elements of type `T` as possible from this slice to `buf`.\n    ///\n    /// Copies `self.len()` or `buf.len()` times the size of `T` bytes, whichever is smaller,\n    /// to `buf`. The copy happens from smallest to largest address in `T` sized chunks\n    /// using volatile reads.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::{VolatileMemory, VolatileSlice};\n    /// #\n    /// let mut mem = [0u8; 32];\n    /// let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let mut buf = [5u8; 16];\n    /// let res = vslice.copy_to(&mut buf[..]);\n    ///\n    /// assert_eq!(16, res);\n    /// for &v in &buf[..] {\n    ///     assert_eq!(v, 0);\n    /// }\n    /// ```\n    pub fn copy_to<T>(&self, buf: &mut [T]) -> usize\n    where\n        T: ByteValued,\n    {\n        // A fast path for u8/i8\n        if size_of::<T>() == 1 {\n            let total = buf.len().min(self.len());\n\n            // SAFETY:\n            // - dst is valid for writes of at least `total`, since total <= buf.len()\n            // - src is valid for reads of at least `total` as total <= self.len()\n            // - The regions are non-overlapping as `src` points to guest memory and `buf` is\n            //   a slice and thus has to live outside of guest memory (there can be more slices to\n            //   guest memory without violating rust's aliasing rules)\n            // - size is always a multiple of alignment, so treating *mut T as *mut u8 is fine\n            unsafe { copy_from_volatile_slice(buf.as_mut_ptr() as *mut u8, self, total) }\n        } else {\n            let count = self.size / size_of::<T>();\n            let source = self.get_array_ref::<T>(0, count).unwrap();\n            source.copy_to(buf)\n        }\n    }\n\n    /// Copies as many bytes as possible from this slice to the provided `slice`.\n    ///\n    /// The copies happen in an undefined order.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::{VolatileMemory, VolatileSlice};\n    /// #\n    /// # // Create a buffer\n    /// # let mut mem = [0u8; 32];\n    /// #\n    /// # // Get a `VolatileSlice` from the buffer\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// #\n    /// vslice.copy_to_volatile_slice(\n    ///     vslice\n    ///         .get_slice(16, 16)\n    ///         .expect(\"Could not get VolatileSlice\"),\n    /// );\n    /// ```\n    pub fn copy_to_volatile_slice<S: BitmapSlice>(&self, slice: VolatileSlice<S>) {\n        // SAFETY: Safe because the pointers are range-checked when the slices\n        // are created, and they never escape the VolatileSlices.\n        // FIXME: ... however, is it really okay to mix non-volatile\n        // operations such as copy with read_volatile and write_volatile?\n        unsafe {\n            let count = min(self.size, slice.size);\n            copy(self.addr, slice.addr, count);\n            slice.bitmap.mark_dirty(0, count);\n        }\n    }\n\n    /// Copies as many elements of type `T` as possible from `buf` to this slice.\n    ///\n    /// The copy happens from smallest to largest address in `T` sized chunks using volatile writes.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::{VolatileMemory, VolatileSlice};\n    /// #\n    /// let mut mem = [0u8; 32];\n    /// let vslice = VolatileSlice::from(&mut mem[..]);\n    ///\n    /// let buf = [5u8; 64];\n    /// vslice.copy_from(&buf[..]);\n    ///\n    /// for i in 0..4 {\n    ///     let val = vslice\n    ///         .get_ref::<u32>(i * 4)\n    ///         .expect(\"Could not get value\")\n    ///         .load();\n    ///     assert_eq!(val, 0x05050505);\n    /// }\n    /// ```\n    pub fn copy_from<T>(&self, buf: &[T])\n    where\n        T: ByteValued,\n    {\n        // A fast path for u8/i8\n        if size_of::<T>() == 1 {\n            let total = buf.len().min(self.len());\n            // SAFETY:\n            // - dst is valid for writes of at least `total`, since total <= self.len()\n            // - src is valid for reads of at least `total` as total <= buf.len()\n            // - The regions are non-overlapping as `dst` points to guest memory and `buf` is\n            //   a slice and thus has to live outside of guest memory (there can be more slices to\n            //   guest memory without violating rust's aliasing rules)\n            // - size is always a multiple of alignment, so treating *mut T as *mut u8 is fine\n            unsafe { copy_to_volatile_slice(self, buf.as_ptr() as *const u8, total) };\n        } else {\n            let count = self.size / size_of::<T>();\n            // It's ok to use unwrap here because `count` was computed based on the current\n            // length of `self`.\n            let dest = self.get_array_ref::<T>(0, count).unwrap();\n\n            // No need to explicitly call `mark_dirty` after this call because\n            // `VolatileArrayRef::copy_from` already takes care of that.\n            dest.copy_from(buf);\n        };\n    }\n\n    /// Checks if the current slice is aligned at `alignment` bytes.\n    fn check_alignment(&self, alignment: usize) -> Result<()> {\n        // Check that the desired alignment is a power of two.\n        debug_assert!((alignment & (alignment - 1)) == 0);\n        if ((self.addr as usize) & (alignment - 1)) != 0 {\n            return Err(Error::Misaligned {\n                addr: self.addr as usize,\n                alignment,\n            });\n        }\n        Ok(())\n    }\n}\n\nimpl<B: BitmapSlice> Bytes<usize> for VolatileSlice<'_, B> {\n    type E = Error;\n\n    /// # Examples\n    /// * Write a slice of size 5 at offset 1020 of a 1024-byte `VolatileSlice`.\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// #\n    /// let mut mem = [0u8; 1024];\n    /// let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let res = vslice.write(&[1, 2, 3, 4, 5], 1020);\n    ///\n    /// assert!(res.is_ok());\n    /// assert_eq!(res.unwrap(), 4);\n    /// ```\n    fn write(&self, buf: &[u8], addr: usize) -> Result<usize> {\n        if buf.is_empty() {\n            return Ok(0);\n        }\n\n        if addr >= self.size {\n            return Err(Error::OutOfBounds { addr });\n        }\n\n        let total = buf.len().min(self.len() - addr);\n        let dst = self.subslice(addr, total)?;\n\n        // SAFETY:\n        // We check above that `addr` is a valid offset within this volatile slice, and by\n        // the invariants of `VolatileSlice::new`, this volatile slice points to contiguous\n        // memory of length self.len(). Furthermore, both src and dst of the call to\n        // copy_to_volatile_slice are valid for reads and writes respectively of length `total`\n        // since total is the minimum of lengths of the memory areas pointed to. The areas do not\n        // overlap, since `dst` is inside guest memory, and buf is a slice (no slices to guest\n        // memory are possible without violating rust's aliasing rules).\n        Ok(unsafe { copy_to_volatile_slice(&dst, buf.as_ptr(), total) })\n    }\n\n    /// # Examples\n    /// * Read a slice of size 16 at offset 1010 of a 1024-byte `VolatileSlice`.\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// #\n    /// let mut mem = [0u8; 1024];\n    /// let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let buf = &mut [0u8; 16];\n    /// let res = vslice.read(buf, 1010);\n    ///\n    /// assert!(res.is_ok());\n    /// assert_eq!(res.unwrap(), 14);\n    /// ```\n    fn read(&self, buf: &mut [u8], addr: usize) -> Result<usize> {\n        if buf.is_empty() {\n            return Ok(0);\n        }\n\n        if addr >= self.size {\n            return Err(Error::OutOfBounds { addr });\n        }\n\n        let total = buf.len().min(self.len() - addr);\n        let src = self.subslice(addr, total)?;\n\n        // SAFETY:\n        // We check above that `addr` is a valid offset within this volatile slice, and by\n        // the invariants of `VolatileSlice::new`, this volatile slice points to contiguous\n        // memory of length self.len(). Furthermore, both src and dst of the call to\n        // copy_from_volatile_slice are valid for reads and writes respectively of length `total`\n        // since total is the minimum of lengths of the memory areas pointed to. The areas do not\n        // overlap, since `dst` is inside guest memory, and buf is a slice (no slices to guest\n        // memory are possible without violating rust's aliasing rules).\n        unsafe { Ok(copy_from_volatile_slice(buf.as_mut_ptr(), &src, total)) }\n    }\n\n    /// # Examples\n    /// * Write a slice at offset 256.\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// #\n    /// # // Create a buffer\n    /// # let mut mem = [0u8; 1024];\n    /// #\n    /// # // Get a `VolatileSlice` from the buffer\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// #\n    /// let res = vslice.write_slice(&[1, 2, 3, 4, 5], 256);\n    ///\n    /// assert!(res.is_ok());\n    /// assert_eq!(res.unwrap(), ());\n    /// ```\n    fn write_slice(&self, buf: &[u8], addr: usize) -> Result<()> {\n        // `mark_dirty` called within `self.write`.\n        let len = self.write(buf, addr)?;\n        if len != buf.len() {\n            return Err(Error::PartialBuffer {\n                expected: buf.len(),\n                completed: len,\n            });\n        }\n        Ok(())\n    }\n\n    /// # Examples\n    /// * Read a slice of size 16 at offset 256.\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// #\n    /// # // Create a buffer\n    /// # let mut mem = [0u8; 1024];\n    /// #\n    /// # // Get a `VolatileSlice` from the buffer\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// #\n    /// let buf = &mut [0u8; 16];\n    /// let res = vslice.read_slice(buf, 256);\n    ///\n    /// assert!(res.is_ok());\n    /// ```\n    fn read_slice(&self, buf: &mut [u8], addr: usize) -> Result<()> {\n        let len = self.read(buf, addr)?;\n        if len != buf.len() {\n            return Err(Error::PartialBuffer {\n                expected: buf.len(),\n                completed: len,\n            });\n        }\n        Ok(())\n    }\n\n    /// # Examples\n    ///\n    /// * Read bytes from /dev/urandom\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// # use std::fs::File;\n    /// # use std::path::Path;\n    /// #\n    /// # if cfg!(unix) {\n    /// # let mut mem = [0u8; 1024];\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let mut file = File::open(Path::new(\"/dev/urandom\")).expect(\"Could not open /dev/urandom\");\n    ///\n    /// vslice\n    ///     .read_from(32, &mut file, 128)\n    ///     .expect(\"Could not read bytes from file into VolatileSlice\");\n    ///\n    /// let rand_val: u32 = vslice\n    ///     .read_obj(40)\n    ///     .expect(\"Could not read value from VolatileSlice\");\n    /// # }\n    /// ```\n    fn read_from<F>(&self, addr: usize, src: &mut F, count: usize) -> Result<usize>\n    where\n        F: Read,\n    {\n        let _ = self.compute_end_offset(addr, count)?;\n\n        let mut dst = vec![0; count];\n\n        let bytes_read = loop {\n            match src.read(&mut dst) {\n                Ok(n) => break n,\n                Err(ref e) if e.kind() == std::io::ErrorKind::Interrupted => continue,\n                Err(e) => return Err(Error::IOError(e)),\n            }\n        };\n\n        // There is no guarantee that the read implementation is well-behaved, see the docs for\n        // Read::read.\n        assert!(bytes_read <= count);\n\n        let slice = self.subslice(addr, bytes_read)?;\n\n        // SAFETY: We have checked via compute_end_offset that accessing the specified\n        // region of guest memory is valid. We asserted that the value returned by `read` is between\n        // 0 and count (the length of the buffer passed to it), and that the\n        // regions don't overlap because we allocated the Vec outside of guest memory.\n        Ok(unsafe { copy_to_volatile_slice(&slice, dst.as_ptr(), bytes_read) })\n    }\n\n    /// # Examples\n    ///\n    /// * Read bytes from /dev/urandom\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// # use std::fs::File;\n    /// # use std::path::Path;\n    /// #\n    /// # if cfg!(unix) {\n    /// # let mut mem = [0u8; 1024];\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let mut file = File::open(Path::new(\"/dev/urandom\")).expect(\"Could not open /dev/urandom\");\n    ///\n    /// vslice\n    ///     .read_exact_from(32, &mut file, 128)\n    ///     .expect(\"Could not read bytes from file into VolatileSlice\");\n    ///\n    /// let rand_val: u32 = vslice\n    ///     .read_obj(40)\n    ///     .expect(\"Could not read value from VolatileSlice\");\n    /// # }\n    /// ```\n    fn read_exact_from<F>(&self, addr: usize, src: &mut F, count: usize) -> Result<()>\n    where\n        F: Read,\n    {\n        let _ = self.compute_end_offset(addr, count)?;\n\n        let mut dst = vec![0; count];\n\n        // Read into buffer that can be copied into guest memory\n        src.read_exact(&mut dst).map_err(Error::IOError)?;\n\n        let slice = self.subslice(addr, count)?;\n\n        // SAFETY: We have checked via compute_end_offset that accessing the specified\n        // region of guest memory is valid. We know that `dst` has len `count`, and that the\n        // regions don't overlap because we allocated the Vec outside of guest memory\n        unsafe { copy_to_volatile_slice(&slice, dst.as_ptr(), count) };\n        Ok(())\n    }\n\n    /// # Examples\n    ///\n    /// * Write 128 bytes to /dev/null\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// # use std::fs::OpenOptions;\n    /// # use std::path::Path;\n    /// #\n    /// # if cfg!(unix) {\n    /// # let mut mem = [0u8; 1024];\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let mut file = OpenOptions::new()\n    ///     .write(true)\n    ///     .open(\"/dev/null\")\n    ///     .expect(\"Could not open /dev/null\");\n    ///\n    /// vslice\n    ///     .write_to(32, &mut file, 128)\n    ///     .expect(\"Could not write value from VolatileSlice to /dev/null\");\n    /// # }\n    /// ```\n    fn write_to<F>(&self, addr: usize, dst: &mut F, count: usize) -> Result<usize>\n    where\n        F: Write,\n    {\n        let _ = self.compute_end_offset(addr, count)?;\n        let mut src = Vec::with_capacity(count);\n\n        let slice = self.subslice(addr, count)?;\n\n        // SAFETY: We checked the addr and count so accessing the slice is safe.\n        // It is safe to read from volatile memory. The Vec has capacity for exactly `count`\n        // many bytes, and the memory regions pointed to definitely do not overlap, as we\n        // allocated src outside of guest memory.\n        // The call to set_len is safe because the bytes between 0 and count have been initialized\n        // via copying from guest memory, and the Vec's capacity is `count`\n        unsafe {\n            copy_from_volatile_slice(src.as_mut_ptr(), &slice, count);\n            src.set_len(count);\n        }\n\n        loop {\n            match dst.write(&src) {\n                Ok(n) => break Ok(n),\n                Err(ref e) if e.kind() == std::io::ErrorKind::Interrupted => continue,\n                Err(e) => break Err(Error::IOError(e)),\n            }\n        }\n    }\n\n    /// # Examples\n    ///\n    /// * Write 128 bytes to /dev/null\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// # use std::fs::OpenOptions;\n    /// # use std::path::Path;\n    /// #\n    /// # if cfg!(unix) {\n    /// # let mut mem = [0u8; 1024];\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let mut file = OpenOptions::new()\n    ///     .write(true)\n    ///     .open(\"/dev/null\")\n    ///     .expect(\"Could not open /dev/null\");\n    ///\n    /// vslice\n    ///     .write_all_to(32, &mut file, 128)\n    ///     .expect(\"Could not write value from VolatileSlice to /dev/null\");\n    /// # }\n    /// ```\n    fn write_all_to<F>(&self, addr: usize, dst: &mut F, count: usize) -> Result<()>\n    where\n        F: Write,\n    {\n        let _ = self.compute_end_offset(addr, count)?;\n        let mut src = Vec::with_capacity(count);\n\n        let slice = self.subslice(addr, count)?;\n\n        // SAFETY: We checked the addr and count so accessing the slice is safe.\n        // It is safe to read from volatile memory. The Vec has capacity for exactly `count`\n        // many bytes, and the memory regions pointed to definitely do not overlap, as we\n        // allocated src outside of guest memory.\n        // The call to set_len is safe because the bytes between 0 and count have been initialized\n        // via copying from guest memory, and the Vec's capacity is `count`\n        unsafe {\n            copy_from_volatile_slice(src.as_mut_ptr(), &slice, count);\n            src.set_len(count);\n        }\n\n        dst.write_all(&src).map_err(Error::IOError)?;\n\n        Ok(())\n    }\n\n    fn store<T: AtomicAccess>(&self, val: T, addr: usize, order: Ordering) -> Result<()> {\n        self.get_atomic_ref::<T::A>(addr).map(|r| {\n            r.store(val.into(), order);\n            self.bitmap.mark_dirty(addr, size_of::<T>())\n        })\n    }\n\n    fn load<T: AtomicAccess>(&self, addr: usize, order: Ordering) -> Result<T> {\n        self.get_atomic_ref::<T::A>(addr)\n            .map(|r| r.load(order).into())\n    }\n}\n\nimpl<B: BitmapSlice> VolatileMemory for VolatileSlice<'_, B> {\n    type B = B;\n\n    fn len(&self) -> usize {\n        self.size\n    }\n\n    fn get_slice(&self, offset: usize, count: usize) -> Result<VolatileSlice<B>> {\n        let _ = self.compute_end_offset(offset, count)?;\n        Ok(\n            // SAFETY: This is safe because the pointer is range-checked by compute_end_offset, and\n            // the lifetime is the same as self.\n            unsafe {\n                VolatileSlice::with_bitmap(\n                    self.addr.add(offset),\n                    count,\n                    self.bitmap.slice_at(offset),\n                    self.mmap,\n                )\n            },\n        )\n    }\n}\n\n/// A memory location that supports volatile access to an instance of `T`.\n///\n/// # Examples\n///\n/// ```\n/// # use vm_memory::VolatileRef;\n/// #\n/// let mut v = 5u32;\n/// let v_ref = unsafe { VolatileRef::new(&mut v as *mut u32 as *mut u8) };\n///\n/// assert_eq!(v, 5);\n/// assert_eq!(v_ref.load(), 5);\n/// v_ref.store(500);\n/// assert_eq!(v, 500);\n/// ```\n#[derive(Clone, Copy, Debug)]\npub struct VolatileRef<'a, T, B = ()> {\n    addr: *mut Packed<T>,\n    bitmap: B,\n    mmap: Option<&'a MmapInfo>,\n}\n\nimpl<'a, T> VolatileRef<'a, T, ()>\nwhere\n    T: ByteValued,\n{\n    /// Creates a [`VolatileRef`](struct.VolatileRef.html) to an instance of `T`.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is big enough for a\n    /// `T` and is available for the duration of the lifetime of the new `VolatileRef`. The caller\n    /// must also guarantee that all other users of the given chunk of memory are using volatile\n    /// accesses.\n    pub unsafe fn new(addr: *mut u8) -> Self {\n        Self::with_bitmap(addr, (), None)\n    }\n}\n\n#[allow(clippy::len_without_is_empty)]\nimpl<'a, T, B> VolatileRef<'a, T, B>\nwhere\n    T: ByteValued,\n    B: BitmapSlice,\n{\n    /// Creates a [`VolatileRef`](struct.VolatileRef.html) to an instance of `T`, using the\n    /// provided `bitmap` object for dirty page tracking.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is big enough for a\n    /// `T` and is available for the duration of the lifetime of the new `VolatileRef`. The caller\n    /// must also guarantee that all other users of the given chunk of memory are using volatile\n    /// accesses.\n    pub unsafe fn with_bitmap(addr: *mut u8, bitmap: B, mmap: Option<&'a MmapInfo>) -> Self {\n        VolatileRef {\n            addr: addr as *mut Packed<T>,\n            bitmap,\n            mmap,\n        }\n    }\n\n    /// Returns a pointer to the underlying memory. Mutable accesses performed\n    /// using the resulting pointer are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    #[deprecated(\n        since = \"0.12.1\",\n        note = \"Use `.ptr_guard()` or `.ptr_guard_mut()` instead\"\n    )]\n    #[cfg(not(all(feature = \"xen\", unix)))]\n    pub fn as_ptr(&self) -> *mut u8 {\n        self.addr as *mut u8\n    }\n\n    /// Returns a guard for the pointer to the underlying memory.\n    pub fn ptr_guard(&self) -> PtrGuard {\n        PtrGuard::read(self.mmap, self.addr as *mut u8, self.len())\n    }\n\n    /// Returns a mutable guard for the pointer to the underlying memory.\n    pub fn ptr_guard_mut(&self) -> PtrGuardMut {\n        PtrGuardMut::write(self.mmap, self.addr as *mut u8, self.len())\n    }\n\n    /// Gets the size of the referenced type `T`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use std::mem::size_of;\n    /// # use vm_memory::VolatileRef;\n    /// #\n    /// let v_ref = unsafe { VolatileRef::<u32>::new(0 as *mut _) };\n    /// assert_eq!(v_ref.len(), size_of::<u32>() as usize);\n    /// ```\n    pub fn len(&self) -> usize {\n        size_of::<T>()\n    }\n\n    /// Borrows the inner `BitmapSlice`.\n    pub fn bitmap(&self) -> &B {\n        &self.bitmap\n    }\n\n    /// Does a volatile write of the value `v` to the address of this ref.\n    #[inline(always)]\n    pub fn store(&self, v: T) {\n        let guard = self.ptr_guard_mut();\n\n        // SAFETY: Safe because we checked the address and size when creating this VolatileRef.\n        unsafe { write_volatile(guard.as_ptr() as *mut Packed<T>, Packed::<T>(v)) };\n        self.bitmap.mark_dirty(0, self.len())\n    }\n\n    /// Does a volatile read of the value at the address of this ref.\n    #[inline(always)]\n    pub fn load(&self) -> T {\n        let guard = self.ptr_guard();\n\n        // SAFETY: Safe because we checked the address and size when creating this VolatileRef.\n        // For the purposes of demonstrating why read_volatile is necessary, try replacing the code\n        // in this function with the commented code below and running `cargo test --release`.\n        // unsafe { *(self.addr as *const T) }\n        unsafe { read_volatile(guard.as_ptr() as *const Packed<T>).0 }\n    }\n\n    /// Converts this to a [`VolatileSlice`](struct.VolatileSlice.html) with the same size and\n    /// address.\n    pub fn to_slice(&self) -> VolatileSlice<'a, B> {\n        // SAFETY: Safe because we checked the address and size when creating this VolatileRef.\n        unsafe {\n            VolatileSlice::with_bitmap(\n                self.addr as *mut u8,\n                size_of::<T>(),\n                self.bitmap.clone(),\n                self.mmap,\n            )\n        }\n    }\n}\n\n/// A memory location that supports volatile access to an array of elements of type `T`.\n///\n/// # Examples\n///\n/// ```\n/// # use vm_memory::VolatileArrayRef;\n/// #\n/// let mut v = [5u32; 1];\n/// let v_ref = unsafe { VolatileArrayRef::new(&mut v[0] as *mut u32 as *mut u8, v.len()) };\n///\n/// assert_eq!(v[0], 5);\n/// assert_eq!(v_ref.load(0), 5);\n/// v_ref.store(0, 500);\n/// assert_eq!(v[0], 500);\n/// ```\n#[derive(Clone, Copy, Debug)]\npub struct VolatileArrayRef<'a, T, B = ()> {\n    addr: *mut u8,\n    nelem: usize,\n    bitmap: B,\n    phantom: PhantomData<&'a T>,\n    mmap: Option<&'a MmapInfo>,\n}\n\nimpl<'a, T> VolatileArrayRef<'a, T>\nwhere\n    T: ByteValued,\n{\n    /// Creates a [`VolatileArrayRef`](struct.VolatileArrayRef.html) to an array of elements of\n    /// type `T`.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is big enough for\n    /// `nelem` values of type `T` and is available for the duration of the lifetime of the new\n    /// `VolatileRef`. The caller must also guarantee that all other users of the given chunk of\n    /// memory are using volatile accesses.\n    pub unsafe fn new(addr: *mut u8, nelem: usize) -> Self {\n        Self::with_bitmap(addr, nelem, (), None)\n    }\n}\n\nimpl<'a, T, B> VolatileArrayRef<'a, T, B>\nwhere\n    T: ByteValued,\n    B: BitmapSlice,\n{\n    /// Creates a [`VolatileArrayRef`](struct.VolatileArrayRef.html) to an array of elements of\n    /// type `T`, using the provided `bitmap` object for dirty page tracking.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is big enough for\n    /// `nelem` values of type `T` and is available for the duration of the lifetime of the new\n    /// `VolatileRef`. The caller must also guarantee that all other users of the given chunk of\n    /// memory are using volatile accesses.\n    pub unsafe fn with_bitmap(\n        addr: *mut u8,\n        nelem: usize,\n        bitmap: B,\n        mmap: Option<&'a MmapInfo>,\n    ) -> Self {\n        VolatileArrayRef {\n            addr,\n            nelem,\n            bitmap,\n            phantom: PhantomData,\n            mmap,\n        }\n    }\n\n    /// Returns `true` if this array is empty.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// let v_array = unsafe { VolatileArrayRef::<u32>::new(0 as *mut _, 0) };\n    /// assert!(v_array.is_empty());\n    /// ```\n    pub fn is_empty(&self) -> bool {\n        self.nelem == 0\n    }\n\n    /// Returns the number of elements in the array.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// # let v_array = unsafe { VolatileArrayRef::<u32>::new(0 as *mut _, 1) };\n    /// assert_eq!(v_array.len(), 1);\n    /// ```\n    pub fn len(&self) -> usize {\n        self.nelem\n    }\n\n    /// Returns the size of `T`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use std::mem::size_of;\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// let v_ref = unsafe { VolatileArrayRef::<u32>::new(0 as *mut _, 0) };\n    /// assert_eq!(v_ref.element_size(), size_of::<u32>() as usize);\n    /// ```\n    pub fn element_size(&self) -> usize {\n        size_of::<T>()\n    }\n\n    /// Returns a pointer to the underlying memory. Mutable accesses performed\n    /// using the resulting pointer are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    #[deprecated(\n        since = \"0.12.1\",\n        note = \"Use `.ptr_guard()` or `.ptr_guard_mut()` instead\"\n    )]\n    #[cfg(not(all(feature = \"xen\", unix)))]\n    pub fn as_ptr(&self) -> *mut u8 {\n        self.addr\n    }\n\n    /// Returns a guard for the pointer to the underlying memory.\n    pub fn ptr_guard(&self) -> PtrGuard {\n        PtrGuard::read(self.mmap, self.addr, self.len())\n    }\n\n    /// Returns a mutable guard for the pointer to the underlying memory.\n    pub fn ptr_guard_mut(&self) -> PtrGuardMut {\n        PtrGuardMut::write(self.mmap, self.addr, self.len())\n    }\n\n    /// Borrows the inner `BitmapSlice`.\n    pub fn bitmap(&self) -> &B {\n        &self.bitmap\n    }\n\n    /// Converts this to a `VolatileSlice` with the same size and address.\n    pub fn to_slice(&self) -> VolatileSlice<'a, B> {\n        // SAFETY: Safe as long as the caller validated addr when creating this object.\n        unsafe {\n            VolatileSlice::with_bitmap(\n                self.addr,\n                self.nelem * self.element_size(),\n                self.bitmap.clone(),\n                self.mmap,\n            )\n        }\n    }\n\n    /// Does a volatile read of the element at `index`.\n    ///\n    /// # Panics\n    ///\n    /// Panics if `index` is less than the number of elements of the array to which `&self` points.\n    pub fn ref_at(&self, index: usize) -> VolatileRef<'a, T, B> {\n        assert!(index < self.nelem);\n        // SAFETY: Safe because the memory has the same lifetime and points to a subset of the\n        // memory of the VolatileArrayRef.\n        unsafe {\n            // byteofs must fit in an isize as it was checked in get_array_ref.\n            let byteofs = (self.element_size() * index) as isize;\n            let ptr = self.addr.offset(byteofs);\n            VolatileRef::with_bitmap(ptr, self.bitmap.slice_at(byteofs as usize), self.mmap)\n        }\n    }\n\n    /// Does a volatile read of the element at `index`.\n    pub fn load(&self, index: usize) -> T {\n        self.ref_at(index).load()\n    }\n\n    /// Does a volatile write of the element at `index`.\n    pub fn store(&self, index: usize, value: T) {\n        // The `VolatileRef::store` call below implements the required dirty bitmap tracking logic,\n        // so no need to do that in this method as well.\n        self.ref_at(index).store(value)\n    }\n\n    /// Copies as many elements of type `T` as possible from this array to `buf`.\n    ///\n    /// Copies `self.len()` or `buf.len()` times the size of `T` bytes, whichever is smaller,\n    /// to `buf`. The copy happens from smallest to largest address in `T` sized chunks\n    /// using volatile reads.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// let mut v = [0u8; 32];\n    /// let v_ref = unsafe { VolatileArrayRef::new(v.as_mut_ptr(), v.len()) };\n    ///\n    /// let mut buf = [5u8; 16];\n    /// v_ref.copy_to(&mut buf[..]);\n    /// for &v in &buf[..] {\n    ///     assert_eq!(v, 0);\n    /// }\n    /// ```\n    pub fn copy_to(&self, buf: &mut [T]) -> usize {\n        // A fast path for u8/i8\n        if size_of::<T>() == 1 {\n            let source = self.to_slice();\n            let total = buf.len().min(source.len());\n\n            // SAFETY:\n            // - dst is valid for writes of at least `total`, since total <= buf.len()\n            // - src is valid for reads of at least `total` as total <= source.len()\n            // - The regions are non-overlapping as `src` points to guest memory and `buf` is\n            //   a slice and thus has to live outside of guest memory (there can be more slices to\n            //   guest memory without violating rust's aliasing rules)\n            // - size is always a multiple of alignment, so treating *mut T as *mut u8 is fine\n            return unsafe {\n                copy_from_volatile_slice(buf.as_mut_ptr() as *mut u8, &source, total)\n            };\n        }\n\n        let guard = self.ptr_guard();\n        let mut ptr = guard.as_ptr() as *const Packed<T>;\n        let start = ptr;\n\n        for v in buf.iter_mut().take(self.len()) {\n            // SAFETY: read_volatile is safe because the pointers are range-checked when\n            // the slices are created, and they never escape the VolatileSlices.\n            // ptr::add is safe because get_array_ref() validated that\n            // size_of::<T>() * self.len() fits in an isize.\n            unsafe {\n                *v = read_volatile(ptr).0;\n                ptr = ptr.add(1);\n            }\n        }\n\n        // SAFETY: It is guaranteed that start and ptr point to the regions of the same slice.\n        unsafe { ptr.offset_from(start) as usize }\n    }\n\n    /// Copies as many bytes as possible from this slice to the provided `slice`.\n    ///\n    /// The copies happen in an undefined order.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// let mut v = [0u8; 32];\n    /// let v_ref = unsafe { VolatileArrayRef::<u8>::new(v.as_mut_ptr(), v.len()) };\n    /// let mut buf = [5u8; 16];\n    /// let v_ref2 = unsafe { VolatileArrayRef::<u8>::new(buf.as_mut_ptr(), buf.len()) };\n    ///\n    /// v_ref.copy_to_volatile_slice(v_ref2.to_slice());\n    /// for &v in &buf[..] {\n    ///     assert_eq!(v, 0);\n    /// }\n    /// ```\n    pub fn copy_to_volatile_slice<S: BitmapSlice>(&self, slice: VolatileSlice<S>) {\n        // SAFETY: Safe because the pointers are range-checked when the slices\n        // are created, and they never escape the VolatileSlices.\n        // FIXME: ... however, is it really okay to mix non-volatile\n        // operations such as copy with read_volatile and write_volatile?\n        unsafe {\n            let count = min(self.len() * self.element_size(), slice.size);\n            copy(self.addr, slice.addr, count);\n            slice.bitmap.mark_dirty(0, count);\n        }\n    }\n\n    /// Copies as many elements of type `T` as possible from `buf` to this slice.\n    ///\n    /// Copies `self.len()` or `buf.len()` times the size of `T` bytes, whichever is smaller,\n    /// to this slice's memory. The copy happens from smallest to largest address in\n    /// `T` sized chunks using volatile writes.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// let mut v = [0u8; 32];\n    /// let v_ref = unsafe { VolatileArrayRef::<u8>::new(v.as_mut_ptr(), v.len()) };\n    ///\n    /// let buf = [5u8; 64];\n    /// v_ref.copy_from(&buf[..]);\n    /// for &val in &v[..] {\n    ///     assert_eq!(5u8, val);\n    /// }\n    /// ```\n    pub fn copy_from(&self, buf: &[T]) {\n        // A fast path for u8/i8\n        if size_of::<T>() == 1 {\n            let destination = self.to_slice();\n            let total = buf.len().min(destination.len());\n\n            // absurd formatting brought to you by clippy\n            // SAFETY:\n            // - dst is valid for writes of at least `total`, since total <= destination.len()\n            // - src is valid for reads of at least `total` as total <= buf.len()\n            // - The regions are non-overlapping as `dst` points to guest memory and `buf` is\n            //   a slice and thus has to live outside of guest memory (there can be more slices to\n            //   guest memory without violating rust's aliasing rules)\n            // - size is always a multiple of alignment, so treating *const T as *const u8 is fine\n            unsafe { copy_to_volatile_slice(&destination, buf.as_ptr() as *const u8, total) };\n        } else {\n            let guard = self.ptr_guard_mut();\n            let start = guard.as_ptr();\n            let mut ptr = start as *mut Packed<T>;\n\n            for &v in buf.iter().take(self.len()) {\n                // SAFETY: write_volatile is safe because the pointers are range-checked when\n                // the slices are created, and they never escape the VolatileSlices.\n                // ptr::add is safe because get_array_ref() validated that\n                // size_of::<T>() * self.len() fits in an isize.\n                unsafe {\n                    write_volatile(ptr, Packed::<T>(v));\n                    ptr = ptr.add(1);\n                }\n            }\n\n            self.bitmap.mark_dirty(0, ptr as usize - start as usize);\n        }\n    }\n}\n\nimpl<'a, B: BitmapSlice> From<VolatileSlice<'a, B>> for VolatileArrayRef<'a, u8, B> {\n    fn from(slice: VolatileSlice<'a, B>) -> Self {\n        // SAFETY: Safe because the result has the same lifetime and points to the same\n        // memory as the incoming VolatileSlice.\n        unsafe { VolatileArrayRef::with_bitmap(slice.addr, slice.len(), slice.bitmap, slice.mmap) }\n    }\n}\n\n// Return the largest value that `addr` is aligned to. Forcing this function to return 1 will\n// cause test_non_atomic_access to fail.\nfn alignment(addr: usize) -> usize {\n    // Rust is silly and does not let me write addr & -addr.\n    addr & (!addr + 1)\n}\n\nmod copy_slice_impl {\n    use super::*;\n\n    // SAFETY: Has the same safety requirements as `read_volatile` + `write_volatile`, namely:\n    // - `src_addr` and `dst_addr` must be valid for reads/writes.\n    // - `src_addr` and `dst_addr` must be properly aligned with respect to `align`.\n    // - `src_addr` must point to a properly initialized value, which is true here because\n    //   we're only using integer primitives.\n    unsafe fn copy_single(align: usize, src_addr: *const u8, dst_addr: *mut u8) {\n        match align {\n            8 => write_volatile(dst_addr as *mut u64, read_volatile(src_addr as *const u64)),\n            4 => write_volatile(dst_addr as *mut u32, read_volatile(src_addr as *const u32)),\n            2 => write_volatile(dst_addr as *mut u16, read_volatile(src_addr as *const u16)),\n            1 => write_volatile(dst_addr, read_volatile(src_addr)),\n            _ => unreachable!(),\n        }\n    }\n\n    /// Copies `total` bytes from `src` to `dst` using a loop of volatile reads and writes\n    ///\n    /// SAFETY: `src` and `dst` must be point to a contiguously allocated memory region of at least\n    /// length `total`. The regions must not overlap\n    unsafe fn copy_slice_volatile(mut dst: *mut u8, mut src: *const u8, total: usize) -> usize {\n        let mut left = total;\n\n        let align = min(alignment(src as usize), alignment(dst as usize));\n\n        let mut copy_aligned_slice = |min_align| {\n            if align < min_align {\n                return;\n            }\n\n            while left >= min_align {\n                // SAFETY: Safe because we check alignment beforehand, the memory areas are valid\n                // for reads/writes, and the source always contains a valid value.\n                unsafe { copy_single(min_align, src, dst) };\n\n                left -= min_align;\n\n                if left == 0 {\n                    break;\n                }\n\n                // SAFETY: We only explain the invariants for `src`, the argument for `dst` is\n                // analogous.\n                // - `src` and `src + min_align` are within (or one byte past) the same allocated object\n                //   This is given by the invariant on this function ensuring that [src, src + total)\n                //   are part of the same allocated object, and the condition on the while loop\n                //   ensures that we do not go outside this object\n                // - The computed offset in bytes cannot overflow isize, because `min_align` is at\n                //   most 8 when the closure is called (see below)\n                // - The sum `src as usize + min_align` can only wrap around if src as usize + min_align - 1 == usize::MAX,\n                //   however in this case, left == 0, and we'll have exited the loop above.\n                unsafe {\n                    src = src.add(min_align);\n                    dst = dst.add(min_align);\n                }\n            }\n        };\n\n        if size_of::<usize>() > 4 {\n            copy_aligned_slice(8);\n        }\n        copy_aligned_slice(4);\n        copy_aligned_slice(2);\n        copy_aligned_slice(1);\n\n        total\n    }\n\n    /// Copies `total` bytes from `src` to `dst`\n    ///\n    /// SAFETY: `src` and `dst` must be point to a contiguously allocated memory region of at least\n    /// length `total`. The regions must not overlap\n    unsafe fn copy_slice(dst: *mut u8, src: *const u8, total: usize) -> usize {\n        if total <= size_of::<usize>() {\n            // SAFETY: Invariants of copy_slice_volatile are the same as invariants of copy_slice\n            unsafe {\n                copy_slice_volatile(dst, src, total);\n            };\n        } else {\n            // SAFETY:\n            // - Both src and dst are allocated for reads/writes of length `total` by function\n            //   invariant\n            // - src and dst are properly aligned, as any alignment is valid for u8\n            // - The regions are not overlapping by function invariant\n            unsafe {\n                std::ptr::copy_nonoverlapping(src, dst, total);\n            }\n        }\n\n        total\n    }\n\n    /// Copies `total` bytes from `slice` to `dst`\n    ///\n    /// SAFETY: `slice` and `dst` must be point to a contiguously allocated memory region of at\n    /// least length `total`. The regions must not overlap.\n    pub(super) unsafe fn copy_from_volatile_slice<B: BitmapSlice>(\n        dst: *mut u8,\n        slice: &VolatileSlice<'_, B>,\n        total: usize,\n    ) -> usize {\n        let guard = slice.ptr_guard();\n\n        // SAFETY: guaranteed by function invariants.\n        copy_slice(dst, guard.as_ptr(), total)\n    }\n\n    /// Copies `total` bytes from 'src' to `slice`\n    ///\n    /// SAFETY: `slice` and `src` must be point to a contiguously allocated memory region of at\n    /// least length `total`. The regions must not overlap.\n    pub(super) unsafe fn copy_to_volatile_slice<B: BitmapSlice>(\n        slice: &VolatileSlice<'_, B>,\n        src: *const u8,\n        total: usize,\n    ) -> usize {\n        let guard = slice.ptr_guard_mut();\n\n        // SAFETY: guaranteed by function invariants.\n        let count = copy_slice(guard.as_ptr(), src, total);\n        slice.bitmap.mark_dirty(0, count);\n        count\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    #![allow(clippy::undocumented_unsafe_blocks)]\n\n    use super::*;\n    use std::alloc::Layout;\n\n    use std::fs::File;\n    use std::io::Cursor;\n    use std::mem::size_of_val;\n    use std::path::Path;\n    use std::sync::atomic::{AtomicUsize, Ordering};\n    use std::sync::{Arc, Barrier};\n    use std::thread::spawn;\n\n    use matches::assert_matches;\n    use vmm_sys_util::tempfile::TempFile;\n\n    use crate::bitmap::tests::{\n        check_range, range_is_clean, range_is_dirty, test_bytes, test_volatile_memory,\n    };\n    use crate::bitmap::{AtomicBitmap, RefSlice};\n\n    #[test]\n    fn test_display_error() {\n        assert_eq!(\n            format!(\"{}\", Error::OutOfBounds { addr: 0x10 }),\n            \"address 0x10 is out of bounds\"\n        );\n\n        assert_eq!(\n            format!(\n                \"{}\",\n                Error::Overflow {\n                    base: 0x0,\n                    offset: 0x10\n                }\n            ),\n            \"address 0x0 offset by 0x10 would overflow\"\n        );\n\n        assert_eq!(\n            format!(\n                \"{}\",\n                Error::TooBig {\n                    nelements: 100_000,\n                    size: 1_000_000_000\n                }\n            ),\n            \"100000 elements of size 1000000000 would overflow a usize\"\n        );\n\n        assert_eq!(\n            format!(\n                \"{}\",\n                Error::Misaligned {\n                    addr: 0x4,\n                    alignment: 8\n                }\n            ),\n            \"address 0x4 is not aligned to 8\"\n        );\n\n        assert_eq!(\n            format!(\n                \"{}\",\n                Error::PartialBuffer {\n                    expected: 100,\n                    completed: 90\n                }\n            ),\n            \"only used 90 bytes in 100 long buffer\"\n        );\n    }\n\n    #[test]\n    fn misaligned_ref() {\n        let mut a = [0u8; 3];\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        unsafe {\n            assert!(\n                a_ref.aligned_as_ref::<u16>(0).is_err() ^ a_ref.aligned_as_ref::<u16>(1).is_err()\n            );\n            assert!(\n                a_ref.aligned_as_mut::<u16>(0).is_err() ^ a_ref.aligned_as_mut::<u16>(1).is_err()\n            );\n        }\n    }\n\n    #[test]\n    fn atomic_store() {\n        let mut a = [0usize; 1];\n        {\n            let a_ref = unsafe {\n                VolatileSlice::new(&mut a[0] as *mut usize as *mut u8, size_of::<usize>())\n            };\n            let atomic = a_ref.get_atomic_ref::<AtomicUsize>(0).unwrap();\n            atomic.store(2usize, Ordering::Relaxed)\n        }\n        assert_eq!(a[0], 2);\n    }\n\n    #[test]\n    fn atomic_load() {\n        let mut a = [5usize; 1];\n        {\n            let a_ref = unsafe {\n                VolatileSlice::new(&mut a[0] as *mut usize as *mut u8,\n                                   size_of::<usize>())\n            };\n            let atomic = {\n                let atomic = a_ref.get_atomic_ref::<AtomicUsize>(0).unwrap();\n                assert_eq!(atomic.load(Ordering::Relaxed), 5usize);\n                atomic\n            };\n            // To make sure we can take the atomic out of the scope we made it in:\n            atomic.load(Ordering::Relaxed);\n            // but not too far:\n            // atomicu8\n        } //.load(std::sync::atomic::Ordering::Relaxed)\n        ;\n    }\n\n    #[test]\n    fn misaligned_atomic() {\n        let mut a = [5usize, 5usize];\n        let a_ref =\n            unsafe { VolatileSlice::new(&mut a[0] as *mut usize as *mut u8, size_of::<usize>()) };\n        assert!(a_ref.get_atomic_ref::<AtomicUsize>(0).is_ok());\n        assert!(a_ref.get_atomic_ref::<AtomicUsize>(1).is_err());\n    }\n\n    #[test]\n    fn ref_store() {\n        let mut a = [0u8; 1];\n        {\n            let a_ref = VolatileSlice::from(&mut a[..]);\n            let v_ref = a_ref.get_ref(0).unwrap();\n            v_ref.store(2u8);\n        }\n        assert_eq!(a[0], 2);\n    }\n\n    #[test]\n    fn ref_load() {\n        let mut a = [5u8; 1];\n        {\n            let a_ref = VolatileSlice::from(&mut a[..]);\n            let c = {\n                let v_ref = a_ref.get_ref::<u8>(0).unwrap();\n                assert_eq!(v_ref.load(), 5u8);\n                v_ref\n            };\n            // To make sure we can take a v_ref out of the scope we made it in:\n            c.load();\n            // but not too far:\n            // c\n        } //.load()\n        ;\n    }\n\n    #[test]\n    fn ref_to_slice() {\n        let mut a = [1u8; 5];\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        let v_ref = a_ref.get_ref(1).unwrap();\n        v_ref.store(0x1234_5678u32);\n        let ref_slice = v_ref.to_slice();\n        assert_eq!(v_ref.addr as usize, ref_slice.addr as usize);\n        assert_eq!(v_ref.len(), ref_slice.len());\n        assert!(!ref_slice.is_empty());\n    }\n\n    #[test]\n    fn observe_mutate() {\n        struct RawMemory(*mut u8);\n\n        // SAFETY: we use property synchronization below\n        unsafe impl Send for RawMemory {}\n        unsafe impl Sync for RawMemory {}\n\n        let mem = Arc::new(RawMemory(unsafe {\n            std::alloc::alloc(Layout::from_size_align(1, 1).unwrap())\n        }));\n\n        let outside_slice = unsafe { VolatileSlice::new(Arc::clone(&mem).0, 1) };\n        let inside_arc = Arc::clone(&mem);\n\n        let v_ref = outside_slice.get_ref::<u8>(0).unwrap();\n        let barrier = Arc::new(Barrier::new(2));\n        let barrier1 = barrier.clone();\n\n        v_ref.store(99);\n        spawn(move || {\n            barrier1.wait();\n            let inside_slice = unsafe { VolatileSlice::new(inside_arc.0, 1) };\n            let clone_v_ref = inside_slice.get_ref::<u8>(0).unwrap();\n            clone_v_ref.store(0);\n            barrier1.wait();\n        });\n\n        assert_eq!(v_ref.load(), 99);\n        barrier.wait();\n        barrier.wait();\n        assert_eq!(v_ref.load(), 0);\n\n        unsafe { std::alloc::dealloc(mem.0, Layout::from_size_align(1, 1).unwrap()) }\n    }\n\n    #[test]\n    fn mem_is_empty() {\n        let mut backing = vec![0u8; 100];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        assert!(!a.is_empty());\n\n        let mut backing = vec![];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        assert!(a.is_empty());\n    }\n\n    #[test]\n    fn slice_len() {\n        let mut backing = vec![0u8; 100];\n        let mem = VolatileSlice::from(backing.as_mut_slice());\n        let slice = mem.get_slice(0, 27).unwrap();\n        assert_eq!(slice.len(), 27);\n        assert!(!slice.is_empty());\n\n        let slice = mem.get_slice(34, 27).unwrap();\n        assert_eq!(slice.len(), 27);\n        assert!(!slice.is_empty());\n\n        let slice = slice.get_slice(20, 5).unwrap();\n        assert_eq!(slice.len(), 5);\n        assert!(!slice.is_empty());\n\n        let slice = mem.get_slice(34, 0).unwrap();\n        assert!(slice.is_empty());\n    }\n\n    #[test]\n    fn slice_subslice() {\n        let mut backing = vec![0u8; 100];\n        let mem = VolatileSlice::from(backing.as_mut_slice());\n        let slice = mem.get_slice(0, 100).unwrap();\n        assert!(slice.write(&[1; 80], 10).is_ok());\n\n        assert!(slice.subslice(0, 0).is_ok());\n        assert!(slice.subslice(0, 101).is_err());\n\n        assert!(slice.subslice(99, 0).is_ok());\n        assert!(slice.subslice(99, 1).is_ok());\n        assert!(slice.subslice(99, 2).is_err());\n\n        assert!(slice.subslice(100, 0).is_ok());\n        assert!(slice.subslice(100, 1).is_err());\n\n        assert!(slice.subslice(101, 0).is_err());\n        assert!(slice.subslice(101, 1).is_err());\n\n        assert!(slice.subslice(std::usize::MAX, 2).is_err());\n        assert!(slice.subslice(2, std::usize::MAX).is_err());\n\n        let maybe_offset_slice = slice.subslice(10, 80);\n        assert!(maybe_offset_slice.is_ok());\n        let offset_slice = maybe_offset_slice.unwrap();\n        assert_eq!(offset_slice.len(), 80);\n\n        let mut buf = [0; 80];\n        assert!(offset_slice.read(&mut buf, 0).is_ok());\n        assert_eq!(&buf[0..80], &[1; 80][0..80]);\n    }\n\n    #[test]\n    fn slice_offset() {\n        let mut backing = vec![0u8; 100];\n        let mem = VolatileSlice::from(backing.as_mut_slice());\n        let slice = mem.get_slice(0, 100).unwrap();\n        assert!(slice.write(&[1; 80], 10).is_ok());\n\n        assert!(slice.offset(101).is_err());\n\n        let maybe_offset_slice = slice.offset(10);\n        assert!(maybe_offset_slice.is_ok());\n        let offset_slice = maybe_offset_slice.unwrap();\n        assert_eq!(offset_slice.len(), 90);\n        let mut buf = [0; 90];\n        assert!(offset_slice.read(&mut buf, 0).is_ok());\n        assert_eq!(&buf[0..80], &[1; 80][0..80]);\n        assert_eq!(&buf[80..90], &[0; 10][0..10]);\n    }\n\n    #[test]\n    fn slice_copy_to_u8() {\n        let mut a = [2u8, 4, 6, 8, 10];\n        let mut b = [0u8; 4];\n        let mut c = [0u8; 6];\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        let v_ref = a_ref.get_slice(0, a_ref.len()).unwrap();\n        v_ref.copy_to(&mut b[..]);\n        v_ref.copy_to(&mut c[..]);\n        assert_eq!(b[0..4], a[0..4]);\n        assert_eq!(c[0..5], a[0..5]);\n    }\n\n    #[test]\n    fn slice_copy_to_u16() {\n        let mut a = [0x01u16, 0x2, 0x03, 0x4, 0x5];\n        let mut b = [0u16; 4];\n        let mut c = [0u16; 6];\n        let a_ref = &mut a[..];\n        let v_ref = unsafe { VolatileSlice::new(a_ref.as_mut_ptr() as *mut u8, 9) };\n\n        v_ref.copy_to(&mut b[..]);\n        v_ref.copy_to(&mut c[..]);\n        assert_eq!(b[0..4], a_ref[0..4]);\n        assert_eq!(c[0..4], a_ref[0..4]);\n        assert_eq!(c[4], 0);\n    }\n\n    #[test]\n    fn slice_copy_from_u8() {\n        let a = [2u8, 4, 6, 8, 10];\n        let mut b = [0u8; 4];\n        let mut c = [0u8; 6];\n        let b_ref = VolatileSlice::from(&mut b[..]);\n        let v_ref = b_ref.get_slice(0, b_ref.len()).unwrap();\n        v_ref.copy_from(&a[..]);\n        assert_eq!(b[0..4], a[0..4]);\n\n        let c_ref = VolatileSlice::from(&mut c[..]);\n        let v_ref = c_ref.get_slice(0, c_ref.len()).unwrap();\n        v_ref.copy_from(&a[..]);\n        assert_eq!(c[0..5], a[0..5]);\n    }\n\n    #[test]\n    fn slice_copy_from_u16() {\n        let a = [2u16, 4, 6, 8, 10];\n        let mut b = [0u16; 4];\n        let mut c = [0u16; 6];\n        let b_ref = &mut b[..];\n        let v_ref = unsafe { VolatileSlice::new(b_ref.as_mut_ptr() as *mut u8, 8) };\n        v_ref.copy_from(&a[..]);\n        assert_eq!(b_ref[0..4], a[0..4]);\n\n        let c_ref = &mut c[..];\n        let v_ref = unsafe { VolatileSlice::new(c_ref.as_mut_ptr() as *mut u8, 9) };\n        v_ref.copy_from(&a[..]);\n        assert_eq!(c_ref[0..4], a[0..4]);\n        assert_eq!(c_ref[4], 0);\n    }\n\n    #[test]\n    fn slice_copy_to_volatile_slice() {\n        let mut a = [2u8, 4, 6, 8, 10];\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        let a_slice = a_ref.get_slice(0, a_ref.len()).unwrap();\n\n        let mut b = [0u8; 4];\n        let b_ref = VolatileSlice::from(&mut b[..]);\n        let b_slice = b_ref.get_slice(0, b_ref.len()).unwrap();\n\n        a_slice.copy_to_volatile_slice(b_slice);\n        assert_eq!(b, [2, 4, 6, 8]);\n    }\n\n    #[test]\n    fn slice_overflow_error() {\n        use std::usize::MAX;\n        let mut backing = vec![0u8];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let res = a.get_slice(MAX, 1).unwrap_err();\n        assert_matches!(\n            res,\n            Error::Overflow {\n                base: MAX,\n                offset: 1,\n            }\n        );\n    }\n\n    #[test]\n    fn slice_oob_error() {\n        let mut backing = vec![0u8; 100];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        a.get_slice(50, 50).unwrap();\n        let res = a.get_slice(55, 50).unwrap_err();\n        assert_matches!(res, Error::OutOfBounds { addr: 105 });\n    }\n\n    #[test]\n    fn ref_overflow_error() {\n        use std::usize::MAX;\n        let mut backing = vec![0u8];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let res = a.get_ref::<u8>(MAX).unwrap_err();\n        assert_matches!(\n            res,\n            Error::Overflow {\n                base: MAX,\n                offset: 1,\n            }\n        );\n    }\n\n    #[test]\n    fn ref_oob_error() {\n        let mut backing = vec![0u8; 100];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        a.get_ref::<u8>(99).unwrap();\n        let res = a.get_ref::<u16>(99).unwrap_err();\n        assert_matches!(res, Error::OutOfBounds { addr: 101 });\n    }\n\n    #[test]\n    fn ref_oob_too_large() {\n        let mut backing = vec![0u8; 3];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let res = a.get_ref::<u32>(0).unwrap_err();\n        assert_matches!(res, Error::OutOfBounds { addr: 4 });\n    }\n\n    #[test]\n    fn slice_store() {\n        let mut backing = vec![0u8; 5];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        let r = a.get_ref(2).unwrap();\n        r.store(9u16);\n        assert_eq!(s.read_obj::<u16>(2).unwrap(), 9);\n    }\n\n    #[test]\n    fn test_write_past_end() {\n        let mut backing = vec![0u8; 5];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        let res = s.write(&[1, 2, 3, 4, 5, 6], 0);\n        assert!(res.is_ok());\n        assert_eq!(res.unwrap(), 5);\n    }\n\n    #[test]\n    fn slice_read_and_write() {\n        let mut backing = vec![0u8; 5];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        let sample_buf = [1, 2, 3];\n        assert!(s.write(&sample_buf, 5).is_err());\n        assert!(s.write(&sample_buf, 2).is_ok());\n        let mut buf = [0u8; 3];\n        assert!(s.read(&mut buf, 5).is_err());\n        assert!(s.read_slice(&mut buf, 2).is_ok());\n        assert_eq!(buf, sample_buf);\n\n        // Writing an empty buffer at the end of the volatile slice works.\n        assert_eq!(s.write(&[], 100).unwrap(), 0);\n        let buf: &mut [u8] = &mut [];\n        assert_eq!(s.read(buf, 4).unwrap(), 0);\n\n        // Check that reading and writing an empty buffer does not yield an error.\n        let mut backing = Vec::new();\n        let empty_mem = VolatileSlice::from(backing.as_mut_slice());\n        let empty = empty_mem.as_volatile_slice();\n        assert_eq!(empty.write(&[], 1).unwrap(), 0);\n        assert_eq!(empty.read(buf, 1).unwrap(), 0);\n    }\n\n    #[test]\n    fn obj_read_and_write() {\n        let mut backing = vec![0u8; 5];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        assert!(s.write_obj(55u16, 4).is_err());\n        assert!(s.write_obj(55u16, core::usize::MAX).is_err());\n        assert!(s.write_obj(55u16, 2).is_ok());\n        assert_eq!(s.read_obj::<u16>(2).unwrap(), 55u16);\n        assert!(s.read_obj::<u16>(4).is_err());\n        assert!(s.read_obj::<u16>(core::usize::MAX).is_err());\n    }\n\n    #[test]\n    fn mem_read_and_write() {\n        let mut backing = vec![0u8; 5];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        assert!(s.write_obj(!0u32, 1).is_ok());\n        let mut file = if cfg!(unix) {\n            File::open(Path::new(\"/dev/zero\")).unwrap()\n        } else {\n            File::open(Path::new(\"c:\\\\Windows\\\\system32\\\\ntoskrnl.exe\")).unwrap()\n        };\n        assert!(s.read_exact_from(2, &mut file, size_of::<u32>()).is_err());\n        assert!(s\n            .read_exact_from(core::usize::MAX, &mut file, size_of::<u32>())\n            .is_err());\n\n        assert!(s.read_exact_from(1, &mut file, size_of::<u32>()).is_ok());\n\n        let mut f = TempFile::new().unwrap().into_file();\n        assert!(s.read_exact_from(1, &mut f, size_of::<u32>()).is_err());\n        format!(\"{:?}\", s.read_exact_from(1, &mut f, size_of::<u32>()));\n\n        let value = s.read_obj::<u32>(1).unwrap();\n        if cfg!(unix) {\n            assert_eq!(value, 0);\n        } else {\n            assert_eq!(value, 0x0090_5a4d);\n        }\n\n        let mut sink = Vec::new();\n        assert!(s.write_all_to(1, &mut sink, size_of::<u32>()).is_ok());\n        assert!(s.write_all_to(2, &mut sink, size_of::<u32>()).is_err());\n        assert!(s\n            .write_all_to(core::usize::MAX, &mut sink, size_of::<u32>())\n            .is_err());\n        format!(\"{:?}\", s.write_all_to(2, &mut sink, size_of::<u32>()));\n        if cfg!(unix) {\n            assert_eq!(sink, vec![0; size_of::<u32>()]);\n        } else {\n            assert_eq!(sink, vec![0x4d, 0x5a, 0x90, 0x00]);\n        };\n    }\n\n    #[test]\n    fn unaligned_read_and_write() {\n        let mut backing = vec![0u8; 7];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        let sample_buf: [u8; 7] = [1, 2, 0xAA, 0xAA, 0xAA, 0xAA, 4];\n        assert!(s.write_slice(&sample_buf, 0).is_ok());\n        let r = a.get_ref::<u32>(2).unwrap();\n        assert_eq!(r.load(), 0xAAAA_AAAA);\n\n        r.store(0x5555_5555);\n        let sample_buf: [u8; 7] = [1, 2, 0x55, 0x55, 0x55, 0x55, 4];\n        let mut buf: [u8; 7] = Default::default();\n        assert!(s.read_slice(&mut buf, 0).is_ok());\n        assert_eq!(buf, sample_buf);\n    }\n\n    #[test]\n    fn test_read_from_exceeds_size() {\n        #[derive(Debug, Default, Copy, Clone)]\n        struct BytesToRead {\n            _val1: u128, // 16 bytes\n            _val2: u128, // 16 bytes\n        }\n        unsafe impl ByteValued for BytesToRead {}\n        let cursor_size = 20;\n        let mut image = Cursor::new(vec![1u8; cursor_size]);\n\n        // Trying to read more bytes than we have available in the cursor should\n        // make the read_from function return maximum cursor size (i.e. 20).\n        let mut bytes_to_read = BytesToRead::default();\n        let size_of_bytes = size_of_val(&bytes_to_read);\n        assert_eq!(\n            bytes_to_read\n                .as_bytes()\n                .read_from(0, &mut image, size_of_bytes)\n                .unwrap(),\n            cursor_size\n        );\n    }\n\n    #[test]\n    fn ref_array_from_slice() {\n        let mut a = [2, 4, 6, 8, 10];\n        let a_vec = a.to_vec();\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        let a_slice = a_ref.get_slice(0, a_ref.len()).unwrap();\n        let a_array_ref: VolatileArrayRef<u8, ()> = a_slice.into();\n        for (i, entry) in a_vec.iter().enumerate() {\n            assert_eq!(&a_array_ref.load(i), entry);\n        }\n    }\n\n    #[test]\n    fn ref_array_store() {\n        let mut a = [0u8; 5];\n        {\n            let a_ref = VolatileSlice::from(&mut a[..]);\n            let v_ref = a_ref.get_array_ref(1, 4).unwrap();\n            v_ref.store(1, 2u8);\n            v_ref.store(2, 4u8);\n            v_ref.store(3, 6u8);\n        }\n        let expected = [2u8, 4u8, 6u8];\n        assert_eq!(a[2..=4], expected);\n    }\n\n    #[test]\n    fn ref_array_load() {\n        let mut a = [0, 0, 2, 3, 10];\n        {\n            let a_ref = VolatileSlice::from(&mut a[..]);\n            let c = {\n                let v_ref = a_ref.get_array_ref::<u8>(1, 4).unwrap();\n                assert_eq!(v_ref.load(1), 2u8);\n                assert_eq!(v_ref.load(2), 3u8);\n                assert_eq!(v_ref.load(3), 10u8);\n                v_ref\n            };\n            // To make sure we can take a v_ref out of the scope we made it in:\n            c.load(0);\n            // but not too far:\n            // c\n        } //.load()\n        ;\n    }\n\n    #[test]\n    fn ref_array_overflow() {\n        let mut a = [0, 0, 2, 3, 10];\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        let res = a_ref.get_array_ref::<u32>(4, usize::MAX).unwrap_err();\n        assert_matches!(\n            res,\n            Error::TooBig {\n                nelements: usize::MAX,\n                size: 4,\n            }\n        );\n    }\n\n    #[test]\n    fn alignment() {\n        let a = [0u8; 64];\n        let a = &a[a.as_ptr().align_offset(32)] as *const u8 as usize;\n        assert!(super::alignment(a) >= 32);\n        assert_eq!(super::alignment(a + 9), 1);\n        assert_eq!(super::alignment(a + 30), 2);\n        assert_eq!(super::alignment(a + 12), 4);\n        assert_eq!(super::alignment(a + 8), 8);\n    }\n\n    #[test]\n    fn test_atomic_accesses() {\n        let len = 0x1000;\n        let buf = unsafe { std::alloc::alloc_zeroed(Layout::from_size_align(len, 8).unwrap()) };\n        let a = unsafe { VolatileSlice::new(buf, len) };\n\n        crate::bytes::tests::check_atomic_accesses(a, 0, 0x1000);\n        unsafe {\n            std::alloc::dealloc(buf, Layout::from_size_align(len, 8).unwrap());\n        }\n    }\n\n    #[test]\n    fn split_at() {\n        let mut mem = [0u8; 32];\n        let mem_ref = VolatileSlice::from(&mut mem[..]);\n        let vslice = mem_ref.get_slice(0, 32).unwrap();\n        let (start, end) = vslice.split_at(8).unwrap();\n        assert_eq!(start.len(), 8);\n        assert_eq!(end.len(), 24);\n        let (start, end) = vslice.split_at(0).unwrap();\n        assert_eq!(start.len(), 0);\n        assert_eq!(end.len(), 32);\n        let (start, end) = vslice.split_at(31).unwrap();\n        assert_eq!(start.len(), 31);\n        assert_eq!(end.len(), 1);\n        let (start, end) = vslice.split_at(32).unwrap();\n        assert_eq!(start.len(), 32);\n        assert_eq!(end.len(), 0);\n        let err = vslice.split_at(33).unwrap_err();\n        assert_matches!(err, Error::OutOfBounds { addr: _ })\n    }\n\n    #[test]\n    fn test_volatile_slice_dirty_tracking() {\n        let val = 123u64;\n        let dirty_offset = 0x1000;\n        let dirty_len = size_of_val(&val);\n        let page_size = 0x1000;\n\n        let len = 0x10000;\n        let buf = unsafe { std::alloc::alloc_zeroed(Layout::from_size_align(len, 8).unwrap()) };\n\n        // Invoke the `Bytes` test helper function.\n        {\n            let bitmap = AtomicBitmap::new(len, page_size);\n            let slice = unsafe { VolatileSlice::with_bitmap(buf, len, bitmap.slice_at(0), None) };\n\n            test_bytes(\n                &slice,\n                |s: &VolatileSlice<RefSlice<AtomicBitmap>>,\n                 start: usize,\n                 len: usize,\n                 clean: bool| { check_range(s.bitmap(), start, len, clean) },\n                |offset| offset,\n                0x1000,\n            );\n        }\n\n        // Invoke the `VolatileMemory` test helper function.\n        {\n            let bitmap = AtomicBitmap::new(len, page_size);\n            let slice = unsafe { VolatileSlice::with_bitmap(buf, len, bitmap.slice_at(0), None) };\n            test_volatile_memory(&slice);\n        }\n\n        let bitmap = AtomicBitmap::new(len, page_size);\n        let slice = unsafe { VolatileSlice::with_bitmap(buf, len, bitmap.slice_at(0), None) };\n\n        let bitmap2 = AtomicBitmap::new(len, page_size);\n        let slice2 = unsafe { VolatileSlice::with_bitmap(buf, len, bitmap2.slice_at(0), None) };\n\n        let bitmap3 = AtomicBitmap::new(len, page_size);\n        let slice3 = unsafe { VolatileSlice::with_bitmap(buf, len, bitmap3.slice_at(0), None) };\n\n        assert!(range_is_clean(slice.bitmap(), 0, slice.len()));\n        assert!(range_is_clean(slice2.bitmap(), 0, slice2.len()));\n\n        slice.write_obj(val, dirty_offset).unwrap();\n        assert!(range_is_dirty(slice.bitmap(), dirty_offset, dirty_len));\n\n        slice.copy_to_volatile_slice(slice2);\n        assert!(range_is_dirty(slice2.bitmap(), 0, slice2.len()));\n\n        {\n            let (s1, s2) = slice.split_at(dirty_offset).unwrap();\n            assert!(range_is_clean(s1.bitmap(), 0, s1.len()));\n            assert!(range_is_dirty(s2.bitmap(), 0, dirty_len));\n        }\n\n        {\n            let s = slice.subslice(dirty_offset, dirty_len).unwrap();\n            assert!(range_is_dirty(s.bitmap(), 0, s.len()));\n        }\n\n        {\n            let s = slice.offset(dirty_offset).unwrap();\n            assert!(range_is_dirty(s.bitmap(), 0, dirty_len));\n        }\n\n        // Test `copy_from` for size_of::<T> == 1.\n        {\n            let buf = vec![1u8; dirty_offset];\n\n            assert!(range_is_clean(slice.bitmap(), 0, dirty_offset));\n            slice.copy_from(&buf);\n            assert!(range_is_dirty(slice.bitmap(), 0, dirty_offset));\n        }\n\n        // Test `copy_from` for size_of::<T> > 1.\n        {\n            let val = 1u32;\n            let buf = vec![val; dirty_offset / size_of_val(&val)];\n\n            assert!(range_is_clean(slice3.bitmap(), 0, dirty_offset));\n            slice3.copy_from(&buf);\n            assert!(range_is_dirty(slice3.bitmap(), 0, dirty_offset));\n        }\n\n        unsafe {\n            std::alloc::dealloc(buf, Layout::from_size_align(len, 8).unwrap());\n        }\n    }\n\n    #[test]\n    fn test_volatile_ref_dirty_tracking() {\n        let val = 123u64;\n        let mut buf = vec![val];\n        let page_size = 0x1000;\n\n        let bitmap = AtomicBitmap::new(size_of_val(&val), page_size);\n        let vref = unsafe {\n            VolatileRef::with_bitmap(buf.as_mut_ptr() as *mut u8, bitmap.slice_at(0), None)\n        };\n\n        assert!(range_is_clean(vref.bitmap(), 0, vref.len()));\n        vref.store(val);\n        assert!(range_is_dirty(vref.bitmap(), 0, vref.len()));\n    }\n\n    fn test_volatile_array_ref_copy_from_tracking<T>(buf: &mut [T], index: usize, page_size: usize)\n    where\n        T: ByteValued + From<u8>,\n    {\n        let bitmap = AtomicBitmap::new(size_of_val(buf), page_size);\n        let arr = unsafe {\n            VolatileArrayRef::with_bitmap(\n                buf.as_mut_ptr() as *mut u8,\n                index + 1,\n                bitmap.slice_at(0),\n                None,\n            )\n        };\n\n        let val = T::from(123);\n        let copy_buf = vec![val; index + 1];\n\n        assert!(range_is_clean(arr.bitmap(), 0, arr.len() * size_of::<T>()));\n        arr.copy_from(copy_buf.as_slice());\n        assert!(range_is_dirty(arr.bitmap(), 0, size_of_val(buf)));\n    }\n\n    #[test]\n    fn test_volatile_array_ref_dirty_tracking() {\n        let val = 123u64;\n        let dirty_len = size_of_val(&val);\n        let index = 0x1000;\n        let dirty_offset = dirty_len * index;\n        let page_size = 0x1000;\n\n        let mut buf = vec![0u64; index + 1];\n        let mut byte_buf = vec![0u8; index + 1];\n\n        // Test `ref_at`.\n        {\n            let bitmap = AtomicBitmap::new(buf.len() * size_of_val(&val), page_size);\n            let arr = unsafe {\n                VolatileArrayRef::with_bitmap(\n                    buf.as_mut_ptr() as *mut u8,\n                    index + 1,\n                    bitmap.slice_at(0),\n                    None,\n                )\n            };\n\n            assert!(range_is_clean(arr.bitmap(), 0, arr.len() * dirty_len));\n            arr.ref_at(index).store(val);\n            assert!(range_is_dirty(arr.bitmap(), dirty_offset, dirty_len));\n        }\n\n        // Test `store`.\n        {\n            let bitmap = AtomicBitmap::new(buf.len() * size_of_val(&val), page_size);\n            let arr = unsafe {\n                VolatileArrayRef::with_bitmap(\n                    buf.as_mut_ptr() as *mut u8,\n                    index + 1,\n                    bitmap.slice_at(0),\n                    None,\n                )\n            };\n\n            let slice = arr.to_slice();\n            assert!(range_is_clean(slice.bitmap(), 0, slice.len()));\n            arr.store(index, val);\n            assert!(range_is_dirty(slice.bitmap(), dirty_offset, dirty_len));\n        }\n\n        // Test `copy_from` when size_of::<T>() == 1.\n        test_volatile_array_ref_copy_from_tracking(&mut byte_buf, index, page_size);\n        // Test `copy_from` when size_of::<T>() > 1.\n        test_volatile_array_ref_copy_from_tracking(&mut buf, index, page_size);\n    }\n}\n"], "fixing_code": ["# Changelog\n\n## [v0.12.2]\n\n### Fixed\n- [[#251]](https://github.com/rust-vmm/vm-memory/pull/251): Inserted checks\n  that verify that the value returned by `VolatileMemory::get_slice` is of\n  the correct length.\n\n### Deprecated\n- [[#244]](https://github.com/rust-vmm/vm-memory/pull/241) Deprecate volatile\n  memory's `as_ptr()` interfaces. The new interfaces to be used instead are:\n  `ptr_guard()` and `ptr_guard_mut()`.\n\n## [v0.12.1]\n\n### Fixed\n- [[#241]](https://github.com/rust-vmm/vm-memory/pull/245) mmap_xen: Don't drop\n  the FileOffset while in use #245\n\n## [v0.12.0]\n\n### Added\n- [[#241]](https://github.com/rust-vmm/vm-memory/pull/241) Add Xen memory\n  mapping support: Foreign and Grant. Add new API for accessing pointers to\n  volatile slices, as `as_ptr()` can't be used with Xen's Grant mapping.\n- [[#237]](https://github.com/rust-vmm/vm-memory/pull/237) Implement `ByteValued` for `i/u128`.\n\n## [v0.11.0]\n\n### Added\n- [[#216]](https://github.com/rust-vmm/vm-memory/pull/216) Add `GuestRegionMmap::from_region`.\n\n### Fixed\n- [[#217]](https://github.com/rust-vmm/vm-memory/pull/217) Fix vm-memory internally\n  taking rust-style slices to guest memory in ways that could potentially cause \n  undefined behavior. Removes/deprecates various `as_slice`/`as_slice_mut` methods\n  whose usage violated rust's aliasing rules, as well as an unsound \n  `impl<'a> VolatileMemory for &'a mut [u8]`.\n\n## [v0.10.0]\n\n### Changed\n- [[#208]](https://github.com/rust-vmm/vm-memory/issues/208) Updated\n  vmm-sys-util dependency to v0.11.0\n- [[#203]](https://github.com/rust-vmm/vm-memory/pull/203) Switched to Rust\n  edition 2021.\n\n## [v0.9.0]\n\n### Fixed\n\n- [[#195]](https://github.com/rust-vmm/vm-memory/issues/195):\n  `mmap::check_file_offset` is doing the correct size validation for block and\n  char devices as well.\n\n### Changed\n\n- [[#198]](https://github.com/rust-vmm/vm-memory/pull/198): atomic: enable 64\n  bit atomics on ppc64le and s390x.\n- [[#200]](https://github.com/rust-vmm/vm-memory/pull/200): docs: enable all\n  features in `docs.rs`.\n- [[#199]](https://github.com/rust-vmm/vm-memory/issues/199): Update the way\n  the dependencies are pulled such that we don't end up with incompatible\n  versions.\n\n## [v0.8.0]\n\n### Fixed\n\n- [[#190]](https://github.com/rust-vmm/vm-memory/pull/190):\n  `VolatileSlice::read/write` when input slice is empty.\n\n## [v0.7.0]\n\n### Changed\n\n- [[#176]](https://github.com/rust-vmm/vm-memory/pull/176): Relax the trait\n  bounds of `Bytes` auto impl for `T: GuestMemory`\n- [[#178]](https://github.com/rust-vmm/vm-memory/pull/178):\n  `MmapRegion::build_raw` no longer requires that the length of the region is a\n  multiple of the page size.\n\n## [v0.6.0]\n\n### Added\n\n  - [[#160]](https://github.com/rust-vmm/vm-memory/pull/160): Add `ArcRef` and `AtomicBitmapArc` bitmap\n    backend implementations.\n  - [[#149]](https://github.com/rust-vmm/vm-memory/issues/149): Implement builder for MmapRegion.\n  - [[#140]](https://github.com/rust-vmm/vm-memory/issues/140): Add dirty bitmap tracking abstractions. \n\n### Deprecated \n\n  - [[#133]](https://github.com/rust-vmm/vm-memory/issues/8): Deprecate `GuestMemory::with_regions()`,\n   `GuestMemory::with_regions_mut()`, `GuestMemory::map_and_fold()`.\n\n## [v0.5.0]\n\n### Added\n\n- [[#8]](https://github.com/rust-vmm/vm-memory/issues/8): Add GuestMemory method to return an Iterator\n- [[#120]](https://github.com/rust-vmm/vm-memory/pull/120): Add is_hugetlbfs() to GuestMemoryRegion\n- [[#126]](https://github.com/rust-vmm/vm-memory/pull/126): Add VolatileSlice::split_at()\n- [[#128]](https://github.com/rust-vmm/vm-memory/pull/128): Add VolatileSlice::subslice()\n\n## [v0.4.0]\n\n### Fixed\n\n- [[#100]](https://github.com/rust-vmm/vm-memory/issues/100): Performance\n  degradation after fixing [#95](https://github.com/rust-vmm/vm-memory/pull/95).\n- [[#122]](https://github.com/rust-vmm/vm-memory/pull/122): atomic,\n  Cargo.toml: Update for arc-swap 1.0.0.\n\n## [v0.3.0]\n\n### Added\n\n- [[#109]](https://github.com/rust-vmm/vm-memory/pull/109): Added `build_raw` to\n  `MmapRegion` which can be used to operate on externally created mappings.\n- [[#101]](https://github.com/rust-vmm/vm-memory/pull/101): Added `check_range` for\n  GuestMemory which could be used to validate a range of guest memory.\n- [[#115]](https://github.com/rust-vmm/vm-memory/pull/115): Add methods for atomic\n  access to `Bytes`.\n\n### Fixed\n\n- [[#93]](https://github.com/rust-vmm/vm-memory/issues/93): DoS issue when using\n  virtio with rust-vmm/vm-memory.\n- [[#106]](https://github.com/rust-vmm/vm-memory/issues/106): Asserts trigger\n  on zero-length access.  \n\n### Removed\n\n- `integer-atomics` is no longer a distinct feature of the crate.\n\n## [v0.2.0]\n\n### Added\n\n- [[#76]](https://github.com/rust-vmm/vm-memory/issues/76): Added `get_slice` and\n  `as_volatile_slice` to `GuestMemoryRegion`.\n- [[#82]](https://github.com/rust-vmm/vm-memory/issues/82): Added `Clone` bound\n  for `GuestAddressSpace::T`, the return value of `GuestAddressSpace::memory()`.\n- [[#88]](https://github.com/rust-vmm/vm-memory/issues/88): Added `as_bytes` for\n  `ByteValued` which can be used for reading into POD structures from\n  raw bytes.\n\n## [v0.1.0]\n\n### Added\n\n- Added traits for working with VM memory.\n- Added a mmap based implemention for the Guest Memory.\n", "[package]\nname = \"vm-memory\"\nversion = \"0.12.2\"\ndescription = \"Safe abstractions for accessing the VM physical memory\"\nkeywords = [\"memory\"]\ncategories = [\"memory-management\"]\nauthors = [\"Liu Jiang <gerry@linux.alibaba.com>\"]\nrepository = \"https://github.com/rust-vmm/vm-memory\"\nreadme = \"README.md\"\nlicense = \"Apache-2.0 OR BSD-3-Clause\"\nedition = \"2021\"\nautobenches = false\n\n[features]\ndefault = []\nbackend-bitmap = []\nbackend-mmap = []\nbackend-atomic = [\"arc-swap\"]\nxen = [\"backend-mmap\", \"bitflags\", \"vmm-sys-util\"]\n\n[dependencies]\nlibc = \"0.2.39\"\narc-swap = { version = \"1.0.0\", optional = true }\nbitflags = { version = \"1.0\", optional = true }\nthiserror = \"1.0.40\"\nvmm-sys-util = { version = \"0.11.0\", optional = true }\n\n[target.'cfg(windows)'.dependencies.winapi]\nversion = \"0.3\"\nfeatures = [\"errhandlingapi\", \"sysinfoapi\"]\n\n[dev-dependencies]\ncriterion = \"0.3.0\"\nmatches = \"0.1.0\"\nvmm-sys-util = \"0.11.0\"\n\n[[bench]]\nname = \"main\"\nharness = false\n\n[profile.bench]\nlto = true\ncodegen-units = 1\n\n[package.metadata.docs.rs]\nall-features = true\n", "// Portions Copyright 2019 Red Hat, Inc.\n//\n// Copyright 2017 The Chromium OS Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style license that can be\n// found in the THIRT-PARTY file.\n//\n// SPDX-License-Identifier: Apache-2.0 OR BSD-3-Clause\n\n//! Types for volatile access to memory.\n//!\n//! Two of the core rules for safe rust is no data races and no aliased mutable references.\n//! `VolatileRef` and `VolatileSlice`, along with types that produce those which implement\n//! `VolatileMemory`, allow us to sidestep that rule by wrapping pointers that absolutely have to be\n//! accessed volatile. Some systems really do need to operate on shared memory and can't have the\n//! compiler reordering or eliding access because it has no visibility into what other systems are\n//! doing with that hunk of memory.\n//!\n//! For the purposes of maintaining safety, volatile memory has some rules of its own:\n//! 1. No references or slices to volatile memory (`&` or `&mut`).\n//! 2. Access should always been done with a volatile read or write.\n//! The First rule is because having references of any kind to memory considered volatile would\n//! violate pointer aliasing. The second is because unvolatile accesses are inherently undefined if\n//! done concurrently without synchronization. With volatile access we know that the compiler has\n//! not reordered or elided the access.\n\nuse std::cmp::min;\nuse std::io::{self, Read, Write};\nuse std::marker::PhantomData;\nuse std::mem::{align_of, size_of};\nuse std::ptr::copy;\nuse std::ptr::{read_volatile, write_volatile};\nuse std::result;\nuse std::sync::atomic::Ordering;\nuse std::usize;\n\nuse crate::atomic_integer::AtomicInteger;\nuse crate::bitmap::{Bitmap, BitmapSlice, BS};\nuse crate::{AtomicAccess, ByteValued, Bytes};\n\n#[cfg(all(feature = \"backend-mmap\", feature = \"xen\", unix))]\nuse crate::mmap_xen::{MmapXen as MmapInfo, MmapXenSlice};\n\n#[cfg(not(feature = \"xen\"))]\ntype MmapInfo = std::marker::PhantomData<()>;\n\nuse copy_slice_impl::{copy_from_volatile_slice, copy_to_volatile_slice};\n\n/// `VolatileMemory` related errors.\n#[allow(missing_docs)]\n#[derive(Debug, thiserror::Error)]\npub enum Error {\n    /// `addr` is out of bounds of the volatile memory slice.\n    #[error(\"address 0x{addr:x} is out of bounds\")]\n    OutOfBounds { addr: usize },\n    /// Taking a slice at `base` with `offset` would overflow `usize`.\n    #[error(\"address 0x{base:x} offset by 0x{offset:x} would overflow\")]\n    Overflow { base: usize, offset: usize },\n    /// Taking a slice whose size overflows `usize`.\n    #[error(\"{nelements:?} elements of size {size:?} would overflow a usize\")]\n    TooBig { nelements: usize, size: usize },\n    /// Trying to obtain a misaligned reference.\n    #[error(\"address 0x{addr:x} is not aligned to {alignment:?}\")]\n    Misaligned { addr: usize, alignment: usize },\n    /// Writing to memory failed\n    #[error(\"{0}\")]\n    IOError(io::Error),\n    /// Incomplete read or write\n    #[error(\"only used {completed} bytes in {expected} long buffer\")]\n    PartialBuffer { expected: usize, completed: usize },\n}\n\n/// Result of volatile memory operations.\npub type Result<T> = result::Result<T, Error>;\n\n/// Convenience function for computing `base + offset`.\n///\n/// # Errors\n///\n/// Returns [`Err(Error::Overflow)`](enum.Error.html#variant.Overflow) in case `base + offset`\n/// exceeds `usize::MAX`.\n///\n/// # Examples\n///\n/// ```\n/// # use vm_memory::volatile_memory::compute_offset;\n/// #\n/// assert_eq!(108, compute_offset(100, 8).unwrap());\n/// assert!(compute_offset(std::usize::MAX, 6).is_err());\n/// ```\npub fn compute_offset(base: usize, offset: usize) -> Result<usize> {\n    match base.checked_add(offset) {\n        None => Err(Error::Overflow { base, offset }),\n        Some(m) => Ok(m),\n    }\n}\n\n/// Types that support raw volatile access to their data.\npub trait VolatileMemory {\n    /// Type used for dirty memory tracking.\n    type B: Bitmap;\n\n    /// Gets the size of this slice.\n    fn len(&self) -> usize;\n\n    /// Check whether the region is empty.\n    fn is_empty(&self) -> bool {\n        self.len() == 0\n    }\n\n    /// Returns a [`VolatileSlice`](struct.VolatileSlice.html) of `count` bytes starting at\n    /// `offset`.\n    ///\n    /// Note that the property `get_slice(offset, count).len() == count` MUST NOT be\n    /// relied on for the correctness of unsafe code. This is a safe function inside of a\n    /// safe trait, and implementors are under no obligation to follow its documentation.\n    fn get_slice(&self, offset: usize, count: usize) -> Result<VolatileSlice<BS<Self::B>>>;\n\n    /// Gets a slice of memory for the entire region that supports volatile access.\n    fn as_volatile_slice(&self) -> VolatileSlice<BS<Self::B>> {\n        self.get_slice(0, self.len()).unwrap()\n    }\n\n    /// Gets a `VolatileRef` at `offset`.\n    fn get_ref<T: ByteValued>(&self, offset: usize) -> Result<VolatileRef<T, BS<Self::B>>> {\n        let slice = self.get_slice(offset, size_of::<T>())?;\n\n        assert_eq!(\n            slice.len(),\n            size_of::<T>(),\n            \"VolatileMemory::get_slice(offset, count) returned slice of length != count.\"\n        );\n\n        // SAFETY: This is safe because the invariants of the constructors of VolatileSlice ensure that\n        // slice.addr is valid memory of size slice.len(). The assert above ensures that\n        // the length of the slice is exactly enough to hold one `T`. Lastly, the lifetime of the\n        // returned VolatileRef match that of the VolatileSlice returned by get_slice and thus the\n        // lifetime one `self`.\n        unsafe {\n            Ok(VolatileRef::with_bitmap(\n                slice.addr,\n                slice.bitmap,\n                slice.mmap,\n            ))\n        }\n    }\n\n    /// Returns a [`VolatileArrayRef`](struct.VolatileArrayRef.html) of `n` elements starting at\n    /// `offset`.\n    fn get_array_ref<T: ByteValued>(\n        &self,\n        offset: usize,\n        n: usize,\n    ) -> Result<VolatileArrayRef<T, BS<Self::B>>> {\n        // Use isize to avoid problems with ptr::offset and ptr::add down the line.\n        let nbytes = isize::try_from(n)\n            .ok()\n            .and_then(|n| n.checked_mul(size_of::<T>() as isize))\n            .ok_or(Error::TooBig {\n                nelements: n,\n                size: size_of::<T>(),\n            })?;\n        let slice = self.get_slice(offset, nbytes as usize)?;\n\n        assert_eq!(\n            slice.len(),\n            nbytes as usize,\n            \"VolatileMemory::get_slice(offset, count) returned slice of length != count.\"\n        );\n\n        // SAFETY: This is safe because the invariants of the constructors of VolatileSlice ensure that\n        // slice.addr is valid memory of size slice.len(). The assert above ensures that\n        // the length of the slice is exactly enough to hold `n` instances of `T`. Lastly, the lifetime of the\n        // returned VolatileArrayRef match that of the VolatileSlice returned by get_slice and thus the\n        // lifetime one `self`.\n        unsafe {\n            Ok(VolatileArrayRef::with_bitmap(\n                slice.addr,\n                n,\n                slice.bitmap,\n                slice.mmap,\n            ))\n        }\n    }\n\n    /// Returns a reference to an instance of `T` at `offset`.\n    ///\n    /// # Safety\n    /// To use this safely, the caller must guarantee that there are no other\n    /// users of the given chunk of memory for the lifetime of the result.\n    ///\n    /// # Errors\n    ///\n    /// If the resulting pointer is not aligned, this method will return an\n    /// [`Error`](enum.Error.html).\n    unsafe fn aligned_as_ref<T: ByteValued>(&self, offset: usize) -> Result<&T> {\n        let slice = self.get_slice(offset, size_of::<T>())?;\n        slice.check_alignment(align_of::<T>())?;\n\n        assert_eq!(\n            slice.len(),\n            size_of::<T>(),\n            \"VolatileMemory::get_slice(offset, count) returned slice of length != count.\"\n        );\n\n        // SAFETY: This is safe because the invariants of the constructors of VolatileSlice ensure that\n        // slice.addr is valid memory of size slice.len(). The assert above ensures that\n        // the length of the slice is exactly enough to hold one `T`.\n        // Dereferencing the pointer is safe because we check the alignment above, and the invariants\n        // of this function ensure that no aliasing pointers exist. Lastly, the lifetime of the\n        // returned VolatileArrayRef match that of the VolatileSlice returned by get_slice and thus the\n        // lifetime one `self`.\n        unsafe { Ok(&*(slice.addr as *const T)) }\n    }\n\n    /// Returns a mutable reference to an instance of `T` at `offset`. Mutable accesses performed\n    /// using the resulting reference are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that there are no other\n    /// users of the given chunk of memory for the lifetime of the result.\n    ///\n    /// # Errors\n    ///\n    /// If the resulting pointer is not aligned, this method will return an\n    /// [`Error`](enum.Error.html).\n    unsafe fn aligned_as_mut<T: ByteValued>(&self, offset: usize) -> Result<&mut T> {\n        let slice = self.get_slice(offset, size_of::<T>())?;\n        slice.check_alignment(align_of::<T>())?;\n\n        assert_eq!(\n            slice.len(),\n            size_of::<T>(),\n            \"VolatileMemory::get_slice(offset, count) returned slice of length != count.\"\n        );\n\n        // SAFETY: This is safe because the invariants of the constructors of VolatileSlice ensure that\n        // slice.addr is valid memory of size slice.len(). The assert above ensures that\n        // the length of the slice is exactly enough to hold one `T`.\n        // Dereferencing the pointer is safe because we check the alignment above, and the invariants\n        // of this function ensure that no aliasing pointers exist. Lastly, the lifetime of the\n        // returned VolatileArrayRef match that of the VolatileSlice returned by get_slice and thus the\n        // lifetime one `self`.\n\n        unsafe { Ok(&mut *(slice.addr as *mut T)) }\n    }\n\n    /// Returns a reference to an instance of `T` at `offset`. Mutable accesses performed\n    /// using the resulting reference are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    ///\n    /// # Errors\n    ///\n    /// If the resulting pointer is not aligned, this method will return an\n    /// [`Error`](enum.Error.html).\n    fn get_atomic_ref<T: AtomicInteger>(&self, offset: usize) -> Result<&T> {\n        let slice = self.get_slice(offset, size_of::<T>())?;\n        slice.check_alignment(align_of::<T>())?;\n\n        assert_eq!(\n            slice.len(),\n            size_of::<T>(),\n            \"VolatileMemory::get_slice(offset, count) returned slice of length != count.\"\n        );\n\n        // SAFETY: This is safe because the invariants of the constructors of VolatileSlice ensure that\n        // slice.addr is valid memory of size slice.len(). The assert above ensures that\n        // the length of the slice is exactly enough to hold one `T`.\n        // Dereferencing the pointer is safe because we check the alignment above. Lastly, the lifetime of the\n        // returned VolatileArrayRef match that of the VolatileSlice returned by get_slice and thus the\n        // lifetime one `self`.\n        unsafe { Ok(&*(slice.addr as *const T)) }\n    }\n\n    /// Returns the sum of `base` and `offset` if the resulting address is valid.\n    fn compute_end_offset(&self, base: usize, offset: usize) -> Result<usize> {\n        let mem_end = compute_offset(base, offset)?;\n        if mem_end > self.len() {\n            return Err(Error::OutOfBounds { addr: mem_end });\n        }\n        Ok(mem_end)\n    }\n}\n\nimpl<'a> From<&'a mut [u8]> for VolatileSlice<'a, ()> {\n    fn from(value: &'a mut [u8]) -> Self {\n        // SAFETY: Since we construct the VolatileSlice from a rust slice, we know that\n        // the memory at addr `value as *mut u8` is valid for reads and writes (because mutable\n        // reference) of len `value.len()`. Since the `VolatileSlice` inherits the lifetime `'a`,\n        // it is not possible to access/mutate `value` while the VolatileSlice is alive.\n        //\n        // Note that it is possible for multiple aliasing sub slices of this `VolatileSlice`s to\n        // be created through `VolatileSlice::subslice`. This is OK, as pointers are allowed to\n        // alias, and it is impossible to get rust-style references from a `VolatileSlice`.\n        unsafe { VolatileSlice::new(value.as_mut_ptr(), value.len()) }\n    }\n}\n\n#[repr(C, packed)]\nstruct Packed<T>(T);\n\n/// A guard to perform mapping and protect unmapping of the memory.\npub struct PtrGuard {\n    addr: *mut u8,\n    len: usize,\n\n    // This isn't used anymore, but it protects the slice from getting unmapped while in use.\n    // Once this goes out of scope, the memory is unmapped automatically.\n    #[cfg(all(feature = \"xen\", unix))]\n    _slice: MmapXenSlice,\n}\n\n#[allow(clippy::len_without_is_empty)]\nimpl PtrGuard {\n    #[allow(unused_variables)]\n    fn new(mmap: Option<&MmapInfo>, addr: *mut u8, prot: i32, len: usize) -> Self {\n        #[cfg(all(feature = \"xen\", unix))]\n        let (addr, _slice) = {\n            let slice = MmapInfo::mmap(mmap, addr, prot, len);\n            (slice.addr(), slice)\n        };\n\n        Self {\n            addr,\n            len,\n\n            #[cfg(all(feature = \"xen\", unix))]\n            _slice,\n        }\n    }\n\n    fn read(mmap: Option<&MmapInfo>, addr: *mut u8, len: usize) -> Self {\n        Self::new(mmap, addr, libc::PROT_READ, len)\n    }\n\n    /// Returns a non-mutable pointer to the beginning of the slice.\n    pub fn as_ptr(&self) -> *const u8 {\n        self.addr\n    }\n\n    /// Gets the length of the mapped region.\n    pub fn len(&self) -> usize {\n        self.len\n    }\n}\n\n/// A mutable guard to perform mapping and protect unmapping of the memory.\npub struct PtrGuardMut(PtrGuard);\n\n#[allow(clippy::len_without_is_empty)]\nimpl PtrGuardMut {\n    fn write(mmap: Option<&MmapInfo>, addr: *mut u8, len: usize) -> Self {\n        Self(PtrGuard::new(mmap, addr, libc::PROT_WRITE, len))\n    }\n\n    /// Returns a mutable pointer to the beginning of the slice. Mutable accesses performed\n    /// using the resulting pointer are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    pub fn as_ptr(&self) -> *mut u8 {\n        self.0.addr\n    }\n\n    /// Gets the length of the mapped region.\n    pub fn len(&self) -> usize {\n        self.0.len\n    }\n}\n\n/// A slice of raw memory that supports volatile access.\n#[derive(Clone, Copy, Debug)]\npub struct VolatileSlice<'a, B = ()> {\n    addr: *mut u8,\n    size: usize,\n    bitmap: B,\n    mmap: Option<&'a MmapInfo>,\n}\n\nimpl<'a> VolatileSlice<'a, ()> {\n    /// Creates a slice of raw memory that must support volatile access.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is `size` bytes long\n    /// and is available for the duration of the lifetime of the new `VolatileSlice`. The caller\n    /// must also guarantee that all other users of the given chunk of memory are using volatile\n    /// accesses.\n    pub unsafe fn new(addr: *mut u8, size: usize) -> VolatileSlice<'a> {\n        Self::with_bitmap(addr, size, (), None)\n    }\n}\n\nimpl<'a, B: BitmapSlice> VolatileSlice<'a, B> {\n    /// Creates a slice of raw memory that must support volatile access, and uses the provided\n    /// `bitmap` object for dirty page tracking.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is `size` bytes long\n    /// and is available for the duration of the lifetime of the new `VolatileSlice`. The caller\n    /// must also guarantee that all other users of the given chunk of memory are using volatile\n    /// accesses.\n    pub unsafe fn with_bitmap(\n        addr: *mut u8,\n        size: usize,\n        bitmap: B,\n        mmap: Option<&'a MmapInfo>,\n    ) -> VolatileSlice<'a, B> {\n        VolatileSlice {\n            addr,\n            size,\n            bitmap,\n            mmap,\n        }\n    }\n\n    /// Returns a pointer to the beginning of the slice. Mutable accesses performed\n    /// using the resulting pointer are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    #[deprecated(\n        since = \"0.12.1\",\n        note = \"Use `.ptr_guard()` or `.ptr_guard_mut()` instead\"\n    )]\n    #[cfg(not(all(feature = \"xen\", unix)))]\n    pub fn as_ptr(&self) -> *mut u8 {\n        self.addr\n    }\n\n    /// Returns a guard for the pointer to the underlying memory.\n    pub fn ptr_guard(&self) -> PtrGuard {\n        PtrGuard::read(self.mmap, self.addr, self.len())\n    }\n\n    /// Returns a mutable guard for the pointer to the underlying memory.\n    pub fn ptr_guard_mut(&self) -> PtrGuardMut {\n        PtrGuardMut::write(self.mmap, self.addr, self.len())\n    }\n\n    /// Gets the size of this slice.\n    pub fn len(&self) -> usize {\n        self.size\n    }\n\n    /// Checks if the slice is empty.\n    pub fn is_empty(&self) -> bool {\n        self.size == 0\n    }\n\n    /// Borrows the inner `BitmapSlice`.\n    pub fn bitmap(&self) -> &B {\n        &self.bitmap\n    }\n\n    /// Divides one slice into two at an index.\n    ///\n    /// # Example\n    ///\n    /// ```\n    /// # use vm_memory::{VolatileMemory, VolatileSlice};\n    /// #\n    /// # // Create a buffer\n    /// # let mut mem = [0u8; 32];\n    /// #\n    /// # // Get a `VolatileSlice` from the buffer\n    /// let vslice = VolatileSlice::from(&mut mem[..]);\n    ///\n    /// let (start, end) = vslice.split_at(8).expect(\"Could not split VolatileSlice\");\n    /// assert_eq!(8, start.len());\n    /// assert_eq!(24, end.len());\n    /// ```\n    pub fn split_at(&self, mid: usize) -> Result<(Self, Self)> {\n        let end = self.offset(mid)?;\n        let start =\n            // SAFETY: safe because self.offset() already checked the bounds\n            unsafe { VolatileSlice::with_bitmap(self.addr, mid, self.bitmap.clone(), self.mmap) };\n\n        Ok((start, end))\n    }\n\n    /// Returns a subslice of this [`VolatileSlice`](struct.VolatileSlice.html) starting at\n    /// `offset` with `count` length.\n    ///\n    /// The returned subslice is a copy of this slice with the address increased by `offset` bytes\n    /// and the size set to `count` bytes.\n    pub fn subslice(&self, offset: usize, count: usize) -> Result<Self> {\n        let _ = self.compute_end_offset(offset, count)?;\n\n        // SAFETY: This is safe because the pointer is range-checked by compute_end_offset, and\n        // the lifetime is the same as the original slice.\n        unsafe {\n            Ok(VolatileSlice::with_bitmap(\n                self.addr.add(offset),\n                count,\n                self.bitmap.slice_at(offset),\n                self.mmap,\n            ))\n        }\n    }\n\n    /// Returns a subslice of this [`VolatileSlice`](struct.VolatileSlice.html) starting at\n    /// `offset`.\n    ///\n    /// The returned subslice is a copy of this slice with the address increased by `count` bytes\n    /// and the size reduced by `count` bytes.\n    pub fn offset(&self, count: usize) -> Result<VolatileSlice<'a, B>> {\n        let new_addr = (self.addr as usize)\n            .checked_add(count)\n            .ok_or(Error::Overflow {\n                base: self.addr as usize,\n                offset: count,\n            })?;\n        let new_size = self\n            .size\n            .checked_sub(count)\n            .ok_or(Error::OutOfBounds { addr: new_addr })?;\n        // SAFETY: Safe because the memory has the same lifetime and points to a subset of the\n        // memory of the original slice.\n        unsafe {\n            Ok(VolatileSlice::with_bitmap(\n                self.addr.add(count),\n                new_size,\n                self.bitmap.slice_at(count),\n                self.mmap,\n            ))\n        }\n    }\n\n    /// Copies as many elements of type `T` as possible from this slice to `buf`.\n    ///\n    /// Copies `self.len()` or `buf.len()` times the size of `T` bytes, whichever is smaller,\n    /// to `buf`. The copy happens from smallest to largest address in `T` sized chunks\n    /// using volatile reads.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::{VolatileMemory, VolatileSlice};\n    /// #\n    /// let mut mem = [0u8; 32];\n    /// let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let mut buf = [5u8; 16];\n    /// let res = vslice.copy_to(&mut buf[..]);\n    ///\n    /// assert_eq!(16, res);\n    /// for &v in &buf[..] {\n    ///     assert_eq!(v, 0);\n    /// }\n    /// ```\n    pub fn copy_to<T>(&self, buf: &mut [T]) -> usize\n    where\n        T: ByteValued,\n    {\n        // A fast path for u8/i8\n        if size_of::<T>() == 1 {\n            let total = buf.len().min(self.len());\n\n            // SAFETY:\n            // - dst is valid for writes of at least `total`, since total <= buf.len()\n            // - src is valid for reads of at least `total` as total <= self.len()\n            // - The regions are non-overlapping as `src` points to guest memory and `buf` is\n            //   a slice and thus has to live outside of guest memory (there can be more slices to\n            //   guest memory without violating rust's aliasing rules)\n            // - size is always a multiple of alignment, so treating *mut T as *mut u8 is fine\n            unsafe { copy_from_volatile_slice(buf.as_mut_ptr() as *mut u8, self, total) }\n        } else {\n            let count = self.size / size_of::<T>();\n            let source = self.get_array_ref::<T>(0, count).unwrap();\n            source.copy_to(buf)\n        }\n    }\n\n    /// Copies as many bytes as possible from this slice to the provided `slice`.\n    ///\n    /// The copies happen in an undefined order.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::{VolatileMemory, VolatileSlice};\n    /// #\n    /// # // Create a buffer\n    /// # let mut mem = [0u8; 32];\n    /// #\n    /// # // Get a `VolatileSlice` from the buffer\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// #\n    /// vslice.copy_to_volatile_slice(\n    ///     vslice\n    ///         .get_slice(16, 16)\n    ///         .expect(\"Could not get VolatileSlice\"),\n    /// );\n    /// ```\n    pub fn copy_to_volatile_slice<S: BitmapSlice>(&self, slice: VolatileSlice<S>) {\n        // SAFETY: Safe because the pointers are range-checked when the slices\n        // are created, and they never escape the VolatileSlices.\n        // FIXME: ... however, is it really okay to mix non-volatile\n        // operations such as copy with read_volatile and write_volatile?\n        unsafe {\n            let count = min(self.size, slice.size);\n            copy(self.addr, slice.addr, count);\n            slice.bitmap.mark_dirty(0, count);\n        }\n    }\n\n    /// Copies as many elements of type `T` as possible from `buf` to this slice.\n    ///\n    /// The copy happens from smallest to largest address in `T` sized chunks using volatile writes.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::{VolatileMemory, VolatileSlice};\n    /// #\n    /// let mut mem = [0u8; 32];\n    /// let vslice = VolatileSlice::from(&mut mem[..]);\n    ///\n    /// let buf = [5u8; 64];\n    /// vslice.copy_from(&buf[..]);\n    ///\n    /// for i in 0..4 {\n    ///     let val = vslice\n    ///         .get_ref::<u32>(i * 4)\n    ///         .expect(\"Could not get value\")\n    ///         .load();\n    ///     assert_eq!(val, 0x05050505);\n    /// }\n    /// ```\n    pub fn copy_from<T>(&self, buf: &[T])\n    where\n        T: ByteValued,\n    {\n        // A fast path for u8/i8\n        if size_of::<T>() == 1 {\n            let total = buf.len().min(self.len());\n            // SAFETY:\n            // - dst is valid for writes of at least `total`, since total <= self.len()\n            // - src is valid for reads of at least `total` as total <= buf.len()\n            // - The regions are non-overlapping as `dst` points to guest memory and `buf` is\n            //   a slice and thus has to live outside of guest memory (there can be more slices to\n            //   guest memory without violating rust's aliasing rules)\n            // - size is always a multiple of alignment, so treating *mut T as *mut u8 is fine\n            unsafe { copy_to_volatile_slice(self, buf.as_ptr() as *const u8, total) };\n        } else {\n            let count = self.size / size_of::<T>();\n            // It's ok to use unwrap here because `count` was computed based on the current\n            // length of `self`.\n            let dest = self.get_array_ref::<T>(0, count).unwrap();\n\n            // No need to explicitly call `mark_dirty` after this call because\n            // `VolatileArrayRef::copy_from` already takes care of that.\n            dest.copy_from(buf);\n        };\n    }\n\n    /// Checks if the current slice is aligned at `alignment` bytes.\n    fn check_alignment(&self, alignment: usize) -> Result<()> {\n        // Check that the desired alignment is a power of two.\n        debug_assert!((alignment & (alignment - 1)) == 0);\n        if ((self.addr as usize) & (alignment - 1)) != 0 {\n            return Err(Error::Misaligned {\n                addr: self.addr as usize,\n                alignment,\n            });\n        }\n        Ok(())\n    }\n}\n\nimpl<B: BitmapSlice> Bytes<usize> for VolatileSlice<'_, B> {\n    type E = Error;\n\n    /// # Examples\n    /// * Write a slice of size 5 at offset 1020 of a 1024-byte `VolatileSlice`.\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// #\n    /// let mut mem = [0u8; 1024];\n    /// let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let res = vslice.write(&[1, 2, 3, 4, 5], 1020);\n    ///\n    /// assert!(res.is_ok());\n    /// assert_eq!(res.unwrap(), 4);\n    /// ```\n    fn write(&self, buf: &[u8], addr: usize) -> Result<usize> {\n        if buf.is_empty() {\n            return Ok(0);\n        }\n\n        if addr >= self.size {\n            return Err(Error::OutOfBounds { addr });\n        }\n\n        let total = buf.len().min(self.len() - addr);\n        let dst = self.subslice(addr, total)?;\n\n        // SAFETY:\n        // We check above that `addr` is a valid offset within this volatile slice, and by\n        // the invariants of `VolatileSlice::new`, this volatile slice points to contiguous\n        // memory of length self.len(). Furthermore, both src and dst of the call to\n        // copy_to_volatile_slice are valid for reads and writes respectively of length `total`\n        // since total is the minimum of lengths of the memory areas pointed to. The areas do not\n        // overlap, since `dst` is inside guest memory, and buf is a slice (no slices to guest\n        // memory are possible without violating rust's aliasing rules).\n        Ok(unsafe { copy_to_volatile_slice(&dst, buf.as_ptr(), total) })\n    }\n\n    /// # Examples\n    /// * Read a slice of size 16 at offset 1010 of a 1024-byte `VolatileSlice`.\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// #\n    /// let mut mem = [0u8; 1024];\n    /// let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let buf = &mut [0u8; 16];\n    /// let res = vslice.read(buf, 1010);\n    ///\n    /// assert!(res.is_ok());\n    /// assert_eq!(res.unwrap(), 14);\n    /// ```\n    fn read(&self, buf: &mut [u8], addr: usize) -> Result<usize> {\n        if buf.is_empty() {\n            return Ok(0);\n        }\n\n        if addr >= self.size {\n            return Err(Error::OutOfBounds { addr });\n        }\n\n        let total = buf.len().min(self.len() - addr);\n        let src = self.subslice(addr, total)?;\n\n        // SAFETY:\n        // We check above that `addr` is a valid offset within this volatile slice, and by\n        // the invariants of `VolatileSlice::new`, this volatile slice points to contiguous\n        // memory of length self.len(). Furthermore, both src and dst of the call to\n        // copy_from_volatile_slice are valid for reads and writes respectively of length `total`\n        // since total is the minimum of lengths of the memory areas pointed to. The areas do not\n        // overlap, since `dst` is inside guest memory, and buf is a slice (no slices to guest\n        // memory are possible without violating rust's aliasing rules).\n        unsafe { Ok(copy_from_volatile_slice(buf.as_mut_ptr(), &src, total)) }\n    }\n\n    /// # Examples\n    /// * Write a slice at offset 256.\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// #\n    /// # // Create a buffer\n    /// # let mut mem = [0u8; 1024];\n    /// #\n    /// # // Get a `VolatileSlice` from the buffer\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// #\n    /// let res = vslice.write_slice(&[1, 2, 3, 4, 5], 256);\n    ///\n    /// assert!(res.is_ok());\n    /// assert_eq!(res.unwrap(), ());\n    /// ```\n    fn write_slice(&self, buf: &[u8], addr: usize) -> Result<()> {\n        // `mark_dirty` called within `self.write`.\n        let len = self.write(buf, addr)?;\n        if len != buf.len() {\n            return Err(Error::PartialBuffer {\n                expected: buf.len(),\n                completed: len,\n            });\n        }\n        Ok(())\n    }\n\n    /// # Examples\n    /// * Read a slice of size 16 at offset 256.\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// #\n    /// # // Create a buffer\n    /// # let mut mem = [0u8; 1024];\n    /// #\n    /// # // Get a `VolatileSlice` from the buffer\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// #\n    /// let buf = &mut [0u8; 16];\n    /// let res = vslice.read_slice(buf, 256);\n    ///\n    /// assert!(res.is_ok());\n    /// ```\n    fn read_slice(&self, buf: &mut [u8], addr: usize) -> Result<()> {\n        let len = self.read(buf, addr)?;\n        if len != buf.len() {\n            return Err(Error::PartialBuffer {\n                expected: buf.len(),\n                completed: len,\n            });\n        }\n        Ok(())\n    }\n\n    /// # Examples\n    ///\n    /// * Read bytes from /dev/urandom\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// # use std::fs::File;\n    /// # use std::path::Path;\n    /// #\n    /// # if cfg!(unix) {\n    /// # let mut mem = [0u8; 1024];\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let mut file = File::open(Path::new(\"/dev/urandom\")).expect(\"Could not open /dev/urandom\");\n    ///\n    /// vslice\n    ///     .read_from(32, &mut file, 128)\n    ///     .expect(\"Could not read bytes from file into VolatileSlice\");\n    ///\n    /// let rand_val: u32 = vslice\n    ///     .read_obj(40)\n    ///     .expect(\"Could not read value from VolatileSlice\");\n    /// # }\n    /// ```\n    fn read_from<F>(&self, addr: usize, src: &mut F, count: usize) -> Result<usize>\n    where\n        F: Read,\n    {\n        let _ = self.compute_end_offset(addr, count)?;\n\n        let mut dst = vec![0; count];\n\n        let bytes_read = loop {\n            match src.read(&mut dst) {\n                Ok(n) => break n,\n                Err(ref e) if e.kind() == std::io::ErrorKind::Interrupted => continue,\n                Err(e) => return Err(Error::IOError(e)),\n            }\n        };\n\n        // There is no guarantee that the read implementation is well-behaved, see the docs for\n        // Read::read.\n        assert!(bytes_read <= count);\n\n        let slice = self.subslice(addr, bytes_read)?;\n\n        // SAFETY: We have checked via compute_end_offset that accessing the specified\n        // region of guest memory is valid. We asserted that the value returned by `read` is between\n        // 0 and count (the length of the buffer passed to it), and that the\n        // regions don't overlap because we allocated the Vec outside of guest memory.\n        Ok(unsafe { copy_to_volatile_slice(&slice, dst.as_ptr(), bytes_read) })\n    }\n\n    /// # Examples\n    ///\n    /// * Read bytes from /dev/urandom\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// # use std::fs::File;\n    /// # use std::path::Path;\n    /// #\n    /// # if cfg!(unix) {\n    /// # let mut mem = [0u8; 1024];\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let mut file = File::open(Path::new(\"/dev/urandom\")).expect(\"Could not open /dev/urandom\");\n    ///\n    /// vslice\n    ///     .read_exact_from(32, &mut file, 128)\n    ///     .expect(\"Could not read bytes from file into VolatileSlice\");\n    ///\n    /// let rand_val: u32 = vslice\n    ///     .read_obj(40)\n    ///     .expect(\"Could not read value from VolatileSlice\");\n    /// # }\n    /// ```\n    fn read_exact_from<F>(&self, addr: usize, src: &mut F, count: usize) -> Result<()>\n    where\n        F: Read,\n    {\n        let _ = self.compute_end_offset(addr, count)?;\n\n        let mut dst = vec![0; count];\n\n        // Read into buffer that can be copied into guest memory\n        src.read_exact(&mut dst).map_err(Error::IOError)?;\n\n        let slice = self.subslice(addr, count)?;\n\n        // SAFETY: We have checked via compute_end_offset that accessing the specified\n        // region of guest memory is valid. We know that `dst` has len `count`, and that the\n        // regions don't overlap because we allocated the Vec outside of guest memory\n        unsafe { copy_to_volatile_slice(&slice, dst.as_ptr(), count) };\n        Ok(())\n    }\n\n    /// # Examples\n    ///\n    /// * Write 128 bytes to /dev/null\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// # use std::fs::OpenOptions;\n    /// # use std::path::Path;\n    /// #\n    /// # if cfg!(unix) {\n    /// # let mut mem = [0u8; 1024];\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let mut file = OpenOptions::new()\n    ///     .write(true)\n    ///     .open(\"/dev/null\")\n    ///     .expect(\"Could not open /dev/null\");\n    ///\n    /// vslice\n    ///     .write_to(32, &mut file, 128)\n    ///     .expect(\"Could not write value from VolatileSlice to /dev/null\");\n    /// # }\n    /// ```\n    fn write_to<F>(&self, addr: usize, dst: &mut F, count: usize) -> Result<usize>\n    where\n        F: Write,\n    {\n        let _ = self.compute_end_offset(addr, count)?;\n        let mut src = Vec::with_capacity(count);\n\n        let slice = self.subslice(addr, count)?;\n\n        // SAFETY: We checked the addr and count so accessing the slice is safe.\n        // It is safe to read from volatile memory. The Vec has capacity for exactly `count`\n        // many bytes, and the memory regions pointed to definitely do not overlap, as we\n        // allocated src outside of guest memory.\n        // The call to set_len is safe because the bytes between 0 and count have been initialized\n        // via copying from guest memory, and the Vec's capacity is `count`\n        unsafe {\n            copy_from_volatile_slice(src.as_mut_ptr(), &slice, count);\n            src.set_len(count);\n        }\n\n        loop {\n            match dst.write(&src) {\n                Ok(n) => break Ok(n),\n                Err(ref e) if e.kind() == std::io::ErrorKind::Interrupted => continue,\n                Err(e) => break Err(Error::IOError(e)),\n            }\n        }\n    }\n\n    /// # Examples\n    ///\n    /// * Write 128 bytes to /dev/null\n    ///\n    /// ```\n    /// # use vm_memory::{Bytes, VolatileMemory, VolatileSlice};\n    /// # use std::fs::OpenOptions;\n    /// # use std::path::Path;\n    /// #\n    /// # if cfg!(unix) {\n    /// # let mut mem = [0u8; 1024];\n    /// # let vslice = VolatileSlice::from(&mut mem[..]);\n    /// let mut file = OpenOptions::new()\n    ///     .write(true)\n    ///     .open(\"/dev/null\")\n    ///     .expect(\"Could not open /dev/null\");\n    ///\n    /// vslice\n    ///     .write_all_to(32, &mut file, 128)\n    ///     .expect(\"Could not write value from VolatileSlice to /dev/null\");\n    /// # }\n    /// ```\n    fn write_all_to<F>(&self, addr: usize, dst: &mut F, count: usize) -> Result<()>\n    where\n        F: Write,\n    {\n        let _ = self.compute_end_offset(addr, count)?;\n        let mut src = Vec::with_capacity(count);\n\n        let slice = self.subslice(addr, count)?;\n\n        // SAFETY: We checked the addr and count so accessing the slice is safe.\n        // It is safe to read from volatile memory. The Vec has capacity for exactly `count`\n        // many bytes, and the memory regions pointed to definitely do not overlap, as we\n        // allocated src outside of guest memory.\n        // The call to set_len is safe because the bytes between 0 and count have been initialized\n        // via copying from guest memory, and the Vec's capacity is `count`\n        unsafe {\n            copy_from_volatile_slice(src.as_mut_ptr(), &slice, count);\n            src.set_len(count);\n        }\n\n        dst.write_all(&src).map_err(Error::IOError)?;\n\n        Ok(())\n    }\n\n    fn store<T: AtomicAccess>(&self, val: T, addr: usize, order: Ordering) -> Result<()> {\n        self.get_atomic_ref::<T::A>(addr).map(|r| {\n            r.store(val.into(), order);\n            self.bitmap.mark_dirty(addr, size_of::<T>())\n        })\n    }\n\n    fn load<T: AtomicAccess>(&self, addr: usize, order: Ordering) -> Result<T> {\n        self.get_atomic_ref::<T::A>(addr)\n            .map(|r| r.load(order).into())\n    }\n}\n\nimpl<B: BitmapSlice> VolatileMemory for VolatileSlice<'_, B> {\n    type B = B;\n\n    fn len(&self) -> usize {\n        self.size\n    }\n\n    fn get_slice(&self, offset: usize, count: usize) -> Result<VolatileSlice<B>> {\n        let _ = self.compute_end_offset(offset, count)?;\n        Ok(\n            // SAFETY: This is safe because the pointer is range-checked by compute_end_offset, and\n            // the lifetime is the same as self.\n            unsafe {\n                VolatileSlice::with_bitmap(\n                    self.addr.add(offset),\n                    count,\n                    self.bitmap.slice_at(offset),\n                    self.mmap,\n                )\n            },\n        )\n    }\n}\n\n/// A memory location that supports volatile access to an instance of `T`.\n///\n/// # Examples\n///\n/// ```\n/// # use vm_memory::VolatileRef;\n/// #\n/// let mut v = 5u32;\n/// let v_ref = unsafe { VolatileRef::new(&mut v as *mut u32 as *mut u8) };\n///\n/// assert_eq!(v, 5);\n/// assert_eq!(v_ref.load(), 5);\n/// v_ref.store(500);\n/// assert_eq!(v, 500);\n/// ```\n#[derive(Clone, Copy, Debug)]\npub struct VolatileRef<'a, T, B = ()> {\n    addr: *mut Packed<T>,\n    bitmap: B,\n    mmap: Option<&'a MmapInfo>,\n}\n\nimpl<'a, T> VolatileRef<'a, T, ()>\nwhere\n    T: ByteValued,\n{\n    /// Creates a [`VolatileRef`](struct.VolatileRef.html) to an instance of `T`.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is big enough for a\n    /// `T` and is available for the duration of the lifetime of the new `VolatileRef`. The caller\n    /// must also guarantee that all other users of the given chunk of memory are using volatile\n    /// accesses.\n    pub unsafe fn new(addr: *mut u8) -> Self {\n        Self::with_bitmap(addr, (), None)\n    }\n}\n\n#[allow(clippy::len_without_is_empty)]\nimpl<'a, T, B> VolatileRef<'a, T, B>\nwhere\n    T: ByteValued,\n    B: BitmapSlice,\n{\n    /// Creates a [`VolatileRef`](struct.VolatileRef.html) to an instance of `T`, using the\n    /// provided `bitmap` object for dirty page tracking.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is big enough for a\n    /// `T` and is available for the duration of the lifetime of the new `VolatileRef`. The caller\n    /// must also guarantee that all other users of the given chunk of memory are using volatile\n    /// accesses.\n    pub unsafe fn with_bitmap(addr: *mut u8, bitmap: B, mmap: Option<&'a MmapInfo>) -> Self {\n        VolatileRef {\n            addr: addr as *mut Packed<T>,\n            bitmap,\n            mmap,\n        }\n    }\n\n    /// Returns a pointer to the underlying memory. Mutable accesses performed\n    /// using the resulting pointer are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    #[deprecated(\n        since = \"0.12.1\",\n        note = \"Use `.ptr_guard()` or `.ptr_guard_mut()` instead\"\n    )]\n    #[cfg(not(all(feature = \"xen\", unix)))]\n    pub fn as_ptr(&self) -> *mut u8 {\n        self.addr as *mut u8\n    }\n\n    /// Returns a guard for the pointer to the underlying memory.\n    pub fn ptr_guard(&self) -> PtrGuard {\n        PtrGuard::read(self.mmap, self.addr as *mut u8, self.len())\n    }\n\n    /// Returns a mutable guard for the pointer to the underlying memory.\n    pub fn ptr_guard_mut(&self) -> PtrGuardMut {\n        PtrGuardMut::write(self.mmap, self.addr as *mut u8, self.len())\n    }\n\n    /// Gets the size of the referenced type `T`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use std::mem::size_of;\n    /// # use vm_memory::VolatileRef;\n    /// #\n    /// let v_ref = unsafe { VolatileRef::<u32>::new(0 as *mut _) };\n    /// assert_eq!(v_ref.len(), size_of::<u32>() as usize);\n    /// ```\n    pub fn len(&self) -> usize {\n        size_of::<T>()\n    }\n\n    /// Borrows the inner `BitmapSlice`.\n    pub fn bitmap(&self) -> &B {\n        &self.bitmap\n    }\n\n    /// Does a volatile write of the value `v` to the address of this ref.\n    #[inline(always)]\n    pub fn store(&self, v: T) {\n        let guard = self.ptr_guard_mut();\n\n        // SAFETY: Safe because we checked the address and size when creating this VolatileRef.\n        unsafe { write_volatile(guard.as_ptr() as *mut Packed<T>, Packed::<T>(v)) };\n        self.bitmap.mark_dirty(0, self.len())\n    }\n\n    /// Does a volatile read of the value at the address of this ref.\n    #[inline(always)]\n    pub fn load(&self) -> T {\n        let guard = self.ptr_guard();\n\n        // SAFETY: Safe because we checked the address and size when creating this VolatileRef.\n        // For the purposes of demonstrating why read_volatile is necessary, try replacing the code\n        // in this function with the commented code below and running `cargo test --release`.\n        // unsafe { *(self.addr as *const T) }\n        unsafe { read_volatile(guard.as_ptr() as *const Packed<T>).0 }\n    }\n\n    /// Converts this to a [`VolatileSlice`](struct.VolatileSlice.html) with the same size and\n    /// address.\n    pub fn to_slice(&self) -> VolatileSlice<'a, B> {\n        // SAFETY: Safe because we checked the address and size when creating this VolatileRef.\n        unsafe {\n            VolatileSlice::with_bitmap(\n                self.addr as *mut u8,\n                size_of::<T>(),\n                self.bitmap.clone(),\n                self.mmap,\n            )\n        }\n    }\n}\n\n/// A memory location that supports volatile access to an array of elements of type `T`.\n///\n/// # Examples\n///\n/// ```\n/// # use vm_memory::VolatileArrayRef;\n/// #\n/// let mut v = [5u32; 1];\n/// let v_ref = unsafe { VolatileArrayRef::new(&mut v[0] as *mut u32 as *mut u8, v.len()) };\n///\n/// assert_eq!(v[0], 5);\n/// assert_eq!(v_ref.load(0), 5);\n/// v_ref.store(0, 500);\n/// assert_eq!(v[0], 500);\n/// ```\n#[derive(Clone, Copy, Debug)]\npub struct VolatileArrayRef<'a, T, B = ()> {\n    addr: *mut u8,\n    nelem: usize,\n    bitmap: B,\n    phantom: PhantomData<&'a T>,\n    mmap: Option<&'a MmapInfo>,\n}\n\nimpl<'a, T> VolatileArrayRef<'a, T>\nwhere\n    T: ByteValued,\n{\n    /// Creates a [`VolatileArrayRef`](struct.VolatileArrayRef.html) to an array of elements of\n    /// type `T`.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is big enough for\n    /// `nelem` values of type `T` and is available for the duration of the lifetime of the new\n    /// `VolatileRef`. The caller must also guarantee that all other users of the given chunk of\n    /// memory are using volatile accesses.\n    pub unsafe fn new(addr: *mut u8, nelem: usize) -> Self {\n        Self::with_bitmap(addr, nelem, (), None)\n    }\n}\n\nimpl<'a, T, B> VolatileArrayRef<'a, T, B>\nwhere\n    T: ByteValued,\n    B: BitmapSlice,\n{\n    /// Creates a [`VolatileArrayRef`](struct.VolatileArrayRef.html) to an array of elements of\n    /// type `T`, using the provided `bitmap` object for dirty page tracking.\n    ///\n    /// # Safety\n    ///\n    /// To use this safely, the caller must guarantee that the memory at `addr` is big enough for\n    /// `nelem` values of type `T` and is available for the duration of the lifetime of the new\n    /// `VolatileRef`. The caller must also guarantee that all other users of the given chunk of\n    /// memory are using volatile accesses.\n    pub unsafe fn with_bitmap(\n        addr: *mut u8,\n        nelem: usize,\n        bitmap: B,\n        mmap: Option<&'a MmapInfo>,\n    ) -> Self {\n        VolatileArrayRef {\n            addr,\n            nelem,\n            bitmap,\n            phantom: PhantomData,\n            mmap,\n        }\n    }\n\n    /// Returns `true` if this array is empty.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// let v_array = unsafe { VolatileArrayRef::<u32>::new(0 as *mut _, 0) };\n    /// assert!(v_array.is_empty());\n    /// ```\n    pub fn is_empty(&self) -> bool {\n        self.nelem == 0\n    }\n\n    /// Returns the number of elements in the array.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// # let v_array = unsafe { VolatileArrayRef::<u32>::new(0 as *mut _, 1) };\n    /// assert_eq!(v_array.len(), 1);\n    /// ```\n    pub fn len(&self) -> usize {\n        self.nelem\n    }\n\n    /// Returns the size of `T`.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use std::mem::size_of;\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// let v_ref = unsafe { VolatileArrayRef::<u32>::new(0 as *mut _, 0) };\n    /// assert_eq!(v_ref.element_size(), size_of::<u32>() as usize);\n    /// ```\n    pub fn element_size(&self) -> usize {\n        size_of::<T>()\n    }\n\n    /// Returns a pointer to the underlying memory. Mutable accesses performed\n    /// using the resulting pointer are not automatically accounted for by the dirty bitmap\n    /// tracking functionality.\n    #[deprecated(\n        since = \"0.12.1\",\n        note = \"Use `.ptr_guard()` or `.ptr_guard_mut()` instead\"\n    )]\n    #[cfg(not(all(feature = \"xen\", unix)))]\n    pub fn as_ptr(&self) -> *mut u8 {\n        self.addr\n    }\n\n    /// Returns a guard for the pointer to the underlying memory.\n    pub fn ptr_guard(&self) -> PtrGuard {\n        PtrGuard::read(self.mmap, self.addr, self.len())\n    }\n\n    /// Returns a mutable guard for the pointer to the underlying memory.\n    pub fn ptr_guard_mut(&self) -> PtrGuardMut {\n        PtrGuardMut::write(self.mmap, self.addr, self.len())\n    }\n\n    /// Borrows the inner `BitmapSlice`.\n    pub fn bitmap(&self) -> &B {\n        &self.bitmap\n    }\n\n    /// Converts this to a `VolatileSlice` with the same size and address.\n    pub fn to_slice(&self) -> VolatileSlice<'a, B> {\n        // SAFETY: Safe as long as the caller validated addr when creating this object.\n        unsafe {\n            VolatileSlice::with_bitmap(\n                self.addr,\n                self.nelem * self.element_size(),\n                self.bitmap.clone(),\n                self.mmap,\n            )\n        }\n    }\n\n    /// Does a volatile read of the element at `index`.\n    ///\n    /// # Panics\n    ///\n    /// Panics if `index` is less than the number of elements of the array to which `&self` points.\n    pub fn ref_at(&self, index: usize) -> VolatileRef<'a, T, B> {\n        assert!(index < self.nelem);\n        // SAFETY: Safe because the memory has the same lifetime and points to a subset of the\n        // memory of the VolatileArrayRef.\n        unsafe {\n            // byteofs must fit in an isize as it was checked in get_array_ref.\n            let byteofs = (self.element_size() * index) as isize;\n            let ptr = self.addr.offset(byteofs);\n            VolatileRef::with_bitmap(ptr, self.bitmap.slice_at(byteofs as usize), self.mmap)\n        }\n    }\n\n    /// Does a volatile read of the element at `index`.\n    pub fn load(&self, index: usize) -> T {\n        self.ref_at(index).load()\n    }\n\n    /// Does a volatile write of the element at `index`.\n    pub fn store(&self, index: usize, value: T) {\n        // The `VolatileRef::store` call below implements the required dirty bitmap tracking logic,\n        // so no need to do that in this method as well.\n        self.ref_at(index).store(value)\n    }\n\n    /// Copies as many elements of type `T` as possible from this array to `buf`.\n    ///\n    /// Copies `self.len()` or `buf.len()` times the size of `T` bytes, whichever is smaller,\n    /// to `buf`. The copy happens from smallest to largest address in `T` sized chunks\n    /// using volatile reads.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// let mut v = [0u8; 32];\n    /// let v_ref = unsafe { VolatileArrayRef::new(v.as_mut_ptr(), v.len()) };\n    ///\n    /// let mut buf = [5u8; 16];\n    /// v_ref.copy_to(&mut buf[..]);\n    /// for &v in &buf[..] {\n    ///     assert_eq!(v, 0);\n    /// }\n    /// ```\n    pub fn copy_to(&self, buf: &mut [T]) -> usize {\n        // A fast path for u8/i8\n        if size_of::<T>() == 1 {\n            let source = self.to_slice();\n            let total = buf.len().min(source.len());\n\n            // SAFETY:\n            // - dst is valid for writes of at least `total`, since total <= buf.len()\n            // - src is valid for reads of at least `total` as total <= source.len()\n            // - The regions are non-overlapping as `src` points to guest memory and `buf` is\n            //   a slice and thus has to live outside of guest memory (there can be more slices to\n            //   guest memory without violating rust's aliasing rules)\n            // - size is always a multiple of alignment, so treating *mut T as *mut u8 is fine\n            return unsafe {\n                copy_from_volatile_slice(buf.as_mut_ptr() as *mut u8, &source, total)\n            };\n        }\n\n        let guard = self.ptr_guard();\n        let mut ptr = guard.as_ptr() as *const Packed<T>;\n        let start = ptr;\n\n        for v in buf.iter_mut().take(self.len()) {\n            // SAFETY: read_volatile is safe because the pointers are range-checked when\n            // the slices are created, and they never escape the VolatileSlices.\n            // ptr::add is safe because get_array_ref() validated that\n            // size_of::<T>() * self.len() fits in an isize.\n            unsafe {\n                *v = read_volatile(ptr).0;\n                ptr = ptr.add(1);\n            }\n        }\n\n        // SAFETY: It is guaranteed that start and ptr point to the regions of the same slice.\n        unsafe { ptr.offset_from(start) as usize }\n    }\n\n    /// Copies as many bytes as possible from this slice to the provided `slice`.\n    ///\n    /// The copies happen in an undefined order.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// let mut v = [0u8; 32];\n    /// let v_ref = unsafe { VolatileArrayRef::<u8>::new(v.as_mut_ptr(), v.len()) };\n    /// let mut buf = [5u8; 16];\n    /// let v_ref2 = unsafe { VolatileArrayRef::<u8>::new(buf.as_mut_ptr(), buf.len()) };\n    ///\n    /// v_ref.copy_to_volatile_slice(v_ref2.to_slice());\n    /// for &v in &buf[..] {\n    ///     assert_eq!(v, 0);\n    /// }\n    /// ```\n    pub fn copy_to_volatile_slice<S: BitmapSlice>(&self, slice: VolatileSlice<S>) {\n        // SAFETY: Safe because the pointers are range-checked when the slices\n        // are created, and they never escape the VolatileSlices.\n        // FIXME: ... however, is it really okay to mix non-volatile\n        // operations such as copy with read_volatile and write_volatile?\n        unsafe {\n            let count = min(self.len() * self.element_size(), slice.size);\n            copy(self.addr, slice.addr, count);\n            slice.bitmap.mark_dirty(0, count);\n        }\n    }\n\n    /// Copies as many elements of type `T` as possible from `buf` to this slice.\n    ///\n    /// Copies `self.len()` or `buf.len()` times the size of `T` bytes, whichever is smaller,\n    /// to this slice's memory. The copy happens from smallest to largest address in\n    /// `T` sized chunks using volatile writes.\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// # use vm_memory::VolatileArrayRef;\n    /// #\n    /// let mut v = [0u8; 32];\n    /// let v_ref = unsafe { VolatileArrayRef::<u8>::new(v.as_mut_ptr(), v.len()) };\n    ///\n    /// let buf = [5u8; 64];\n    /// v_ref.copy_from(&buf[..]);\n    /// for &val in &v[..] {\n    ///     assert_eq!(5u8, val);\n    /// }\n    /// ```\n    pub fn copy_from(&self, buf: &[T]) {\n        // A fast path for u8/i8\n        if size_of::<T>() == 1 {\n            let destination = self.to_slice();\n            let total = buf.len().min(destination.len());\n\n            // absurd formatting brought to you by clippy\n            // SAFETY:\n            // - dst is valid for writes of at least `total`, since total <= destination.len()\n            // - src is valid for reads of at least `total` as total <= buf.len()\n            // - The regions are non-overlapping as `dst` points to guest memory and `buf` is\n            //   a slice and thus has to live outside of guest memory (there can be more slices to\n            //   guest memory without violating rust's aliasing rules)\n            // - size is always a multiple of alignment, so treating *const T as *const u8 is fine\n            unsafe { copy_to_volatile_slice(&destination, buf.as_ptr() as *const u8, total) };\n        } else {\n            let guard = self.ptr_guard_mut();\n            let start = guard.as_ptr();\n            let mut ptr = start as *mut Packed<T>;\n\n            for &v in buf.iter().take(self.len()) {\n                // SAFETY: write_volatile is safe because the pointers are range-checked when\n                // the slices are created, and they never escape the VolatileSlices.\n                // ptr::add is safe because get_array_ref() validated that\n                // size_of::<T>() * self.len() fits in an isize.\n                unsafe {\n                    write_volatile(ptr, Packed::<T>(v));\n                    ptr = ptr.add(1);\n                }\n            }\n\n            self.bitmap.mark_dirty(0, ptr as usize - start as usize);\n        }\n    }\n}\n\nimpl<'a, B: BitmapSlice> From<VolatileSlice<'a, B>> for VolatileArrayRef<'a, u8, B> {\n    fn from(slice: VolatileSlice<'a, B>) -> Self {\n        // SAFETY: Safe because the result has the same lifetime and points to the same\n        // memory as the incoming VolatileSlice.\n        unsafe { VolatileArrayRef::with_bitmap(slice.addr, slice.len(), slice.bitmap, slice.mmap) }\n    }\n}\n\n// Return the largest value that `addr` is aligned to. Forcing this function to return 1 will\n// cause test_non_atomic_access to fail.\nfn alignment(addr: usize) -> usize {\n    // Rust is silly and does not let me write addr & -addr.\n    addr & (!addr + 1)\n}\n\nmod copy_slice_impl {\n    use super::*;\n\n    // SAFETY: Has the same safety requirements as `read_volatile` + `write_volatile`, namely:\n    // - `src_addr` and `dst_addr` must be valid for reads/writes.\n    // - `src_addr` and `dst_addr` must be properly aligned with respect to `align`.\n    // - `src_addr` must point to a properly initialized value, which is true here because\n    //   we're only using integer primitives.\n    unsafe fn copy_single(align: usize, src_addr: *const u8, dst_addr: *mut u8) {\n        match align {\n            8 => write_volatile(dst_addr as *mut u64, read_volatile(src_addr as *const u64)),\n            4 => write_volatile(dst_addr as *mut u32, read_volatile(src_addr as *const u32)),\n            2 => write_volatile(dst_addr as *mut u16, read_volatile(src_addr as *const u16)),\n            1 => write_volatile(dst_addr, read_volatile(src_addr)),\n            _ => unreachable!(),\n        }\n    }\n\n    /// Copies `total` bytes from `src` to `dst` using a loop of volatile reads and writes\n    ///\n    /// SAFETY: `src` and `dst` must be point to a contiguously allocated memory region of at least\n    /// length `total`. The regions must not overlap\n    unsafe fn copy_slice_volatile(mut dst: *mut u8, mut src: *const u8, total: usize) -> usize {\n        let mut left = total;\n\n        let align = min(alignment(src as usize), alignment(dst as usize));\n\n        let mut copy_aligned_slice = |min_align| {\n            if align < min_align {\n                return;\n            }\n\n            while left >= min_align {\n                // SAFETY: Safe because we check alignment beforehand, the memory areas are valid\n                // for reads/writes, and the source always contains a valid value.\n                unsafe { copy_single(min_align, src, dst) };\n\n                left -= min_align;\n\n                if left == 0 {\n                    break;\n                }\n\n                // SAFETY: We only explain the invariants for `src`, the argument for `dst` is\n                // analogous.\n                // - `src` and `src + min_align` are within (or one byte past) the same allocated object\n                //   This is given by the invariant on this function ensuring that [src, src + total)\n                //   are part of the same allocated object, and the condition on the while loop\n                //   ensures that we do not go outside this object\n                // - The computed offset in bytes cannot overflow isize, because `min_align` is at\n                //   most 8 when the closure is called (see below)\n                // - The sum `src as usize + min_align` can only wrap around if src as usize + min_align - 1 == usize::MAX,\n                //   however in this case, left == 0, and we'll have exited the loop above.\n                unsafe {\n                    src = src.add(min_align);\n                    dst = dst.add(min_align);\n                }\n            }\n        };\n\n        if size_of::<usize>() > 4 {\n            copy_aligned_slice(8);\n        }\n        copy_aligned_slice(4);\n        copy_aligned_slice(2);\n        copy_aligned_slice(1);\n\n        total\n    }\n\n    /// Copies `total` bytes from `src` to `dst`\n    ///\n    /// SAFETY: `src` and `dst` must be point to a contiguously allocated memory region of at least\n    /// length `total`. The regions must not overlap\n    unsafe fn copy_slice(dst: *mut u8, src: *const u8, total: usize) -> usize {\n        if total <= size_of::<usize>() {\n            // SAFETY: Invariants of copy_slice_volatile are the same as invariants of copy_slice\n            unsafe {\n                copy_slice_volatile(dst, src, total);\n            };\n        } else {\n            // SAFETY:\n            // - Both src and dst are allocated for reads/writes of length `total` by function\n            //   invariant\n            // - src and dst are properly aligned, as any alignment is valid for u8\n            // - The regions are not overlapping by function invariant\n            unsafe {\n                std::ptr::copy_nonoverlapping(src, dst, total);\n            }\n        }\n\n        total\n    }\n\n    /// Copies `total` bytes from `slice` to `dst`\n    ///\n    /// SAFETY: `slice` and `dst` must be point to a contiguously allocated memory region of at\n    /// least length `total`. The regions must not overlap.\n    pub(super) unsafe fn copy_from_volatile_slice<B: BitmapSlice>(\n        dst: *mut u8,\n        slice: &VolatileSlice<'_, B>,\n        total: usize,\n    ) -> usize {\n        let guard = slice.ptr_guard();\n\n        // SAFETY: guaranteed by function invariants.\n        copy_slice(dst, guard.as_ptr(), total)\n    }\n\n    /// Copies `total` bytes from 'src' to `slice`\n    ///\n    /// SAFETY: `slice` and `src` must be point to a contiguously allocated memory region of at\n    /// least length `total`. The regions must not overlap.\n    pub(super) unsafe fn copy_to_volatile_slice<B: BitmapSlice>(\n        slice: &VolatileSlice<'_, B>,\n        src: *const u8,\n        total: usize,\n    ) -> usize {\n        let guard = slice.ptr_guard_mut();\n\n        // SAFETY: guaranteed by function invariants.\n        let count = copy_slice(guard.as_ptr(), src, total);\n        slice.bitmap.mark_dirty(0, count);\n        count\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    #![allow(clippy::undocumented_unsafe_blocks)]\n\n    use super::*;\n    use std::alloc::Layout;\n\n    use std::fs::File;\n    use std::io::Cursor;\n    use std::mem::size_of_val;\n    use std::path::Path;\n    use std::sync::atomic::{AtomicUsize, Ordering};\n    use std::sync::{Arc, Barrier};\n    use std::thread::spawn;\n\n    use matches::assert_matches;\n    use vmm_sys_util::tempfile::TempFile;\n\n    use crate::bitmap::tests::{\n        check_range, range_is_clean, range_is_dirty, test_bytes, test_volatile_memory,\n    };\n    use crate::bitmap::{AtomicBitmap, RefSlice};\n\n    #[test]\n    fn test_display_error() {\n        assert_eq!(\n            format!(\"{}\", Error::OutOfBounds { addr: 0x10 }),\n            \"address 0x10 is out of bounds\"\n        );\n\n        assert_eq!(\n            format!(\n                \"{}\",\n                Error::Overflow {\n                    base: 0x0,\n                    offset: 0x10\n                }\n            ),\n            \"address 0x0 offset by 0x10 would overflow\"\n        );\n\n        assert_eq!(\n            format!(\n                \"{}\",\n                Error::TooBig {\n                    nelements: 100_000,\n                    size: 1_000_000_000\n                }\n            ),\n            \"100000 elements of size 1000000000 would overflow a usize\"\n        );\n\n        assert_eq!(\n            format!(\n                \"{}\",\n                Error::Misaligned {\n                    addr: 0x4,\n                    alignment: 8\n                }\n            ),\n            \"address 0x4 is not aligned to 8\"\n        );\n\n        assert_eq!(\n            format!(\n                \"{}\",\n                Error::PartialBuffer {\n                    expected: 100,\n                    completed: 90\n                }\n            ),\n            \"only used 90 bytes in 100 long buffer\"\n        );\n    }\n\n    #[test]\n    fn misaligned_ref() {\n        let mut a = [0u8; 3];\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        unsafe {\n            assert!(\n                a_ref.aligned_as_ref::<u16>(0).is_err() ^ a_ref.aligned_as_ref::<u16>(1).is_err()\n            );\n            assert!(\n                a_ref.aligned_as_mut::<u16>(0).is_err() ^ a_ref.aligned_as_mut::<u16>(1).is_err()\n            );\n        }\n    }\n\n    #[test]\n    fn atomic_store() {\n        let mut a = [0usize; 1];\n        {\n            let a_ref = unsafe {\n                VolatileSlice::new(&mut a[0] as *mut usize as *mut u8, size_of::<usize>())\n            };\n            let atomic = a_ref.get_atomic_ref::<AtomicUsize>(0).unwrap();\n            atomic.store(2usize, Ordering::Relaxed)\n        }\n        assert_eq!(a[0], 2);\n    }\n\n    #[test]\n    fn atomic_load() {\n        let mut a = [5usize; 1];\n        {\n            let a_ref = unsafe {\n                VolatileSlice::new(&mut a[0] as *mut usize as *mut u8,\n                                   size_of::<usize>())\n            };\n            let atomic = {\n                let atomic = a_ref.get_atomic_ref::<AtomicUsize>(0).unwrap();\n                assert_eq!(atomic.load(Ordering::Relaxed), 5usize);\n                atomic\n            };\n            // To make sure we can take the atomic out of the scope we made it in:\n            atomic.load(Ordering::Relaxed);\n            // but not too far:\n            // atomicu8\n        } //.load(std::sync::atomic::Ordering::Relaxed)\n        ;\n    }\n\n    #[test]\n    fn misaligned_atomic() {\n        let mut a = [5usize, 5usize];\n        let a_ref =\n            unsafe { VolatileSlice::new(&mut a[0] as *mut usize as *mut u8, size_of::<usize>()) };\n        assert!(a_ref.get_atomic_ref::<AtomicUsize>(0).is_ok());\n        assert!(a_ref.get_atomic_ref::<AtomicUsize>(1).is_err());\n    }\n\n    #[test]\n    fn ref_store() {\n        let mut a = [0u8; 1];\n        {\n            let a_ref = VolatileSlice::from(&mut a[..]);\n            let v_ref = a_ref.get_ref(0).unwrap();\n            v_ref.store(2u8);\n        }\n        assert_eq!(a[0], 2);\n    }\n\n    #[test]\n    fn ref_load() {\n        let mut a = [5u8; 1];\n        {\n            let a_ref = VolatileSlice::from(&mut a[..]);\n            let c = {\n                let v_ref = a_ref.get_ref::<u8>(0).unwrap();\n                assert_eq!(v_ref.load(), 5u8);\n                v_ref\n            };\n            // To make sure we can take a v_ref out of the scope we made it in:\n            c.load();\n            // but not too far:\n            // c\n        } //.load()\n        ;\n    }\n\n    #[test]\n    fn ref_to_slice() {\n        let mut a = [1u8; 5];\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        let v_ref = a_ref.get_ref(1).unwrap();\n        v_ref.store(0x1234_5678u32);\n        let ref_slice = v_ref.to_slice();\n        assert_eq!(v_ref.addr as usize, ref_slice.addr as usize);\n        assert_eq!(v_ref.len(), ref_slice.len());\n        assert!(!ref_slice.is_empty());\n    }\n\n    #[test]\n    fn observe_mutate() {\n        struct RawMemory(*mut u8);\n\n        // SAFETY: we use property synchronization below\n        unsafe impl Send for RawMemory {}\n        unsafe impl Sync for RawMemory {}\n\n        let mem = Arc::new(RawMemory(unsafe {\n            std::alloc::alloc(Layout::from_size_align(1, 1).unwrap())\n        }));\n\n        let outside_slice = unsafe { VolatileSlice::new(Arc::clone(&mem).0, 1) };\n        let inside_arc = Arc::clone(&mem);\n\n        let v_ref = outside_slice.get_ref::<u8>(0).unwrap();\n        let barrier = Arc::new(Barrier::new(2));\n        let barrier1 = barrier.clone();\n\n        v_ref.store(99);\n        spawn(move || {\n            barrier1.wait();\n            let inside_slice = unsafe { VolatileSlice::new(inside_arc.0, 1) };\n            let clone_v_ref = inside_slice.get_ref::<u8>(0).unwrap();\n            clone_v_ref.store(0);\n            barrier1.wait();\n        });\n\n        assert_eq!(v_ref.load(), 99);\n        barrier.wait();\n        barrier.wait();\n        assert_eq!(v_ref.load(), 0);\n\n        unsafe { std::alloc::dealloc(mem.0, Layout::from_size_align(1, 1).unwrap()) }\n    }\n\n    #[test]\n    fn mem_is_empty() {\n        let mut backing = vec![0u8; 100];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        assert!(!a.is_empty());\n\n        let mut backing = vec![];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        assert!(a.is_empty());\n    }\n\n    #[test]\n    fn slice_len() {\n        let mut backing = vec![0u8; 100];\n        let mem = VolatileSlice::from(backing.as_mut_slice());\n        let slice = mem.get_slice(0, 27).unwrap();\n        assert_eq!(slice.len(), 27);\n        assert!(!slice.is_empty());\n\n        let slice = mem.get_slice(34, 27).unwrap();\n        assert_eq!(slice.len(), 27);\n        assert!(!slice.is_empty());\n\n        let slice = slice.get_slice(20, 5).unwrap();\n        assert_eq!(slice.len(), 5);\n        assert!(!slice.is_empty());\n\n        let slice = mem.get_slice(34, 0).unwrap();\n        assert!(slice.is_empty());\n    }\n\n    #[test]\n    fn slice_subslice() {\n        let mut backing = vec![0u8; 100];\n        let mem = VolatileSlice::from(backing.as_mut_slice());\n        let slice = mem.get_slice(0, 100).unwrap();\n        assert!(slice.write(&[1; 80], 10).is_ok());\n\n        assert!(slice.subslice(0, 0).is_ok());\n        assert!(slice.subslice(0, 101).is_err());\n\n        assert!(slice.subslice(99, 0).is_ok());\n        assert!(slice.subslice(99, 1).is_ok());\n        assert!(slice.subslice(99, 2).is_err());\n\n        assert!(slice.subslice(100, 0).is_ok());\n        assert!(slice.subslice(100, 1).is_err());\n\n        assert!(slice.subslice(101, 0).is_err());\n        assert!(slice.subslice(101, 1).is_err());\n\n        assert!(slice.subslice(std::usize::MAX, 2).is_err());\n        assert!(slice.subslice(2, std::usize::MAX).is_err());\n\n        let maybe_offset_slice = slice.subslice(10, 80);\n        assert!(maybe_offset_slice.is_ok());\n        let offset_slice = maybe_offset_slice.unwrap();\n        assert_eq!(offset_slice.len(), 80);\n\n        let mut buf = [0; 80];\n        assert!(offset_slice.read(&mut buf, 0).is_ok());\n        assert_eq!(&buf[0..80], &[1; 80][0..80]);\n    }\n\n    #[test]\n    fn slice_offset() {\n        let mut backing = vec![0u8; 100];\n        let mem = VolatileSlice::from(backing.as_mut_slice());\n        let slice = mem.get_slice(0, 100).unwrap();\n        assert!(slice.write(&[1; 80], 10).is_ok());\n\n        assert!(slice.offset(101).is_err());\n\n        let maybe_offset_slice = slice.offset(10);\n        assert!(maybe_offset_slice.is_ok());\n        let offset_slice = maybe_offset_slice.unwrap();\n        assert_eq!(offset_slice.len(), 90);\n        let mut buf = [0; 90];\n        assert!(offset_slice.read(&mut buf, 0).is_ok());\n        assert_eq!(&buf[0..80], &[1; 80][0..80]);\n        assert_eq!(&buf[80..90], &[0; 10][0..10]);\n    }\n\n    #[test]\n    fn slice_copy_to_u8() {\n        let mut a = [2u8, 4, 6, 8, 10];\n        let mut b = [0u8; 4];\n        let mut c = [0u8; 6];\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        let v_ref = a_ref.get_slice(0, a_ref.len()).unwrap();\n        v_ref.copy_to(&mut b[..]);\n        v_ref.copy_to(&mut c[..]);\n        assert_eq!(b[0..4], a[0..4]);\n        assert_eq!(c[0..5], a[0..5]);\n    }\n\n    #[test]\n    fn slice_copy_to_u16() {\n        let mut a = [0x01u16, 0x2, 0x03, 0x4, 0x5];\n        let mut b = [0u16; 4];\n        let mut c = [0u16; 6];\n        let a_ref = &mut a[..];\n        let v_ref = unsafe { VolatileSlice::new(a_ref.as_mut_ptr() as *mut u8, 9) };\n\n        v_ref.copy_to(&mut b[..]);\n        v_ref.copy_to(&mut c[..]);\n        assert_eq!(b[0..4], a_ref[0..4]);\n        assert_eq!(c[0..4], a_ref[0..4]);\n        assert_eq!(c[4], 0);\n    }\n\n    #[test]\n    fn slice_copy_from_u8() {\n        let a = [2u8, 4, 6, 8, 10];\n        let mut b = [0u8; 4];\n        let mut c = [0u8; 6];\n        let b_ref = VolatileSlice::from(&mut b[..]);\n        let v_ref = b_ref.get_slice(0, b_ref.len()).unwrap();\n        v_ref.copy_from(&a[..]);\n        assert_eq!(b[0..4], a[0..4]);\n\n        let c_ref = VolatileSlice::from(&mut c[..]);\n        let v_ref = c_ref.get_slice(0, c_ref.len()).unwrap();\n        v_ref.copy_from(&a[..]);\n        assert_eq!(c[0..5], a[0..5]);\n    }\n\n    #[test]\n    fn slice_copy_from_u16() {\n        let a = [2u16, 4, 6, 8, 10];\n        let mut b = [0u16; 4];\n        let mut c = [0u16; 6];\n        let b_ref = &mut b[..];\n        let v_ref = unsafe { VolatileSlice::new(b_ref.as_mut_ptr() as *mut u8, 8) };\n        v_ref.copy_from(&a[..]);\n        assert_eq!(b_ref[0..4], a[0..4]);\n\n        let c_ref = &mut c[..];\n        let v_ref = unsafe { VolatileSlice::new(c_ref.as_mut_ptr() as *mut u8, 9) };\n        v_ref.copy_from(&a[..]);\n        assert_eq!(c_ref[0..4], a[0..4]);\n        assert_eq!(c_ref[4], 0);\n    }\n\n    #[test]\n    fn slice_copy_to_volatile_slice() {\n        let mut a = [2u8, 4, 6, 8, 10];\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        let a_slice = a_ref.get_slice(0, a_ref.len()).unwrap();\n\n        let mut b = [0u8; 4];\n        let b_ref = VolatileSlice::from(&mut b[..]);\n        let b_slice = b_ref.get_slice(0, b_ref.len()).unwrap();\n\n        a_slice.copy_to_volatile_slice(b_slice);\n        assert_eq!(b, [2, 4, 6, 8]);\n    }\n\n    #[test]\n    fn slice_overflow_error() {\n        use std::usize::MAX;\n        let mut backing = vec![0u8];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let res = a.get_slice(MAX, 1).unwrap_err();\n        assert_matches!(\n            res,\n            Error::Overflow {\n                base: MAX,\n                offset: 1,\n            }\n        );\n    }\n\n    #[test]\n    fn slice_oob_error() {\n        let mut backing = vec![0u8; 100];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        a.get_slice(50, 50).unwrap();\n        let res = a.get_slice(55, 50).unwrap_err();\n        assert_matches!(res, Error::OutOfBounds { addr: 105 });\n    }\n\n    #[test]\n    fn ref_overflow_error() {\n        use std::usize::MAX;\n        let mut backing = vec![0u8];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let res = a.get_ref::<u8>(MAX).unwrap_err();\n        assert_matches!(\n            res,\n            Error::Overflow {\n                base: MAX,\n                offset: 1,\n            }\n        );\n    }\n\n    #[test]\n    fn ref_oob_error() {\n        let mut backing = vec![0u8; 100];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        a.get_ref::<u8>(99).unwrap();\n        let res = a.get_ref::<u16>(99).unwrap_err();\n        assert_matches!(res, Error::OutOfBounds { addr: 101 });\n    }\n\n    #[test]\n    fn ref_oob_too_large() {\n        let mut backing = vec![0u8; 3];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let res = a.get_ref::<u32>(0).unwrap_err();\n        assert_matches!(res, Error::OutOfBounds { addr: 4 });\n    }\n\n    #[test]\n    fn slice_store() {\n        let mut backing = vec![0u8; 5];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        let r = a.get_ref(2).unwrap();\n        r.store(9u16);\n        assert_eq!(s.read_obj::<u16>(2).unwrap(), 9);\n    }\n\n    #[test]\n    fn test_write_past_end() {\n        let mut backing = vec![0u8; 5];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        let res = s.write(&[1, 2, 3, 4, 5, 6], 0);\n        assert!(res.is_ok());\n        assert_eq!(res.unwrap(), 5);\n    }\n\n    #[test]\n    fn slice_read_and_write() {\n        let mut backing = vec![0u8; 5];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        let sample_buf = [1, 2, 3];\n        assert!(s.write(&sample_buf, 5).is_err());\n        assert!(s.write(&sample_buf, 2).is_ok());\n        let mut buf = [0u8; 3];\n        assert!(s.read(&mut buf, 5).is_err());\n        assert!(s.read_slice(&mut buf, 2).is_ok());\n        assert_eq!(buf, sample_buf);\n\n        // Writing an empty buffer at the end of the volatile slice works.\n        assert_eq!(s.write(&[], 100).unwrap(), 0);\n        let buf: &mut [u8] = &mut [];\n        assert_eq!(s.read(buf, 4).unwrap(), 0);\n\n        // Check that reading and writing an empty buffer does not yield an error.\n        let mut backing = Vec::new();\n        let empty_mem = VolatileSlice::from(backing.as_mut_slice());\n        let empty = empty_mem.as_volatile_slice();\n        assert_eq!(empty.write(&[], 1).unwrap(), 0);\n        assert_eq!(empty.read(buf, 1).unwrap(), 0);\n    }\n\n    #[test]\n    fn obj_read_and_write() {\n        let mut backing = vec![0u8; 5];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        assert!(s.write_obj(55u16, 4).is_err());\n        assert!(s.write_obj(55u16, core::usize::MAX).is_err());\n        assert!(s.write_obj(55u16, 2).is_ok());\n        assert_eq!(s.read_obj::<u16>(2).unwrap(), 55u16);\n        assert!(s.read_obj::<u16>(4).is_err());\n        assert!(s.read_obj::<u16>(core::usize::MAX).is_err());\n    }\n\n    #[test]\n    fn mem_read_and_write() {\n        let mut backing = vec![0u8; 5];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        assert!(s.write_obj(!0u32, 1).is_ok());\n        let mut file = if cfg!(unix) {\n            File::open(Path::new(\"/dev/zero\")).unwrap()\n        } else {\n            File::open(Path::new(\"c:\\\\Windows\\\\system32\\\\ntoskrnl.exe\")).unwrap()\n        };\n        assert!(s.read_exact_from(2, &mut file, size_of::<u32>()).is_err());\n        assert!(s\n            .read_exact_from(core::usize::MAX, &mut file, size_of::<u32>())\n            .is_err());\n\n        assert!(s.read_exact_from(1, &mut file, size_of::<u32>()).is_ok());\n\n        let mut f = TempFile::new().unwrap().into_file();\n        assert!(s.read_exact_from(1, &mut f, size_of::<u32>()).is_err());\n        format!(\"{:?}\", s.read_exact_from(1, &mut f, size_of::<u32>()));\n\n        let value = s.read_obj::<u32>(1).unwrap();\n        if cfg!(unix) {\n            assert_eq!(value, 0);\n        } else {\n            assert_eq!(value, 0x0090_5a4d);\n        }\n\n        let mut sink = Vec::new();\n        assert!(s.write_all_to(1, &mut sink, size_of::<u32>()).is_ok());\n        assert!(s.write_all_to(2, &mut sink, size_of::<u32>()).is_err());\n        assert!(s\n            .write_all_to(core::usize::MAX, &mut sink, size_of::<u32>())\n            .is_err());\n        format!(\"{:?}\", s.write_all_to(2, &mut sink, size_of::<u32>()));\n        if cfg!(unix) {\n            assert_eq!(sink, vec![0; size_of::<u32>()]);\n        } else {\n            assert_eq!(sink, vec![0x4d, 0x5a, 0x90, 0x00]);\n        };\n    }\n\n    #[test]\n    fn unaligned_read_and_write() {\n        let mut backing = vec![0u8; 7];\n        let a = VolatileSlice::from(backing.as_mut_slice());\n        let s = a.as_volatile_slice();\n        let sample_buf: [u8; 7] = [1, 2, 0xAA, 0xAA, 0xAA, 0xAA, 4];\n        assert!(s.write_slice(&sample_buf, 0).is_ok());\n        let r = a.get_ref::<u32>(2).unwrap();\n        assert_eq!(r.load(), 0xAAAA_AAAA);\n\n        r.store(0x5555_5555);\n        let sample_buf: [u8; 7] = [1, 2, 0x55, 0x55, 0x55, 0x55, 4];\n        let mut buf: [u8; 7] = Default::default();\n        assert!(s.read_slice(&mut buf, 0).is_ok());\n        assert_eq!(buf, sample_buf);\n    }\n\n    #[test]\n    fn test_read_from_exceeds_size() {\n        #[derive(Debug, Default, Copy, Clone)]\n        struct BytesToRead {\n            _val1: u128, // 16 bytes\n            _val2: u128, // 16 bytes\n        }\n        unsafe impl ByteValued for BytesToRead {}\n        let cursor_size = 20;\n        let mut image = Cursor::new(vec![1u8; cursor_size]);\n\n        // Trying to read more bytes than we have available in the cursor should\n        // make the read_from function return maximum cursor size (i.e. 20).\n        let mut bytes_to_read = BytesToRead::default();\n        let size_of_bytes = size_of_val(&bytes_to_read);\n        assert_eq!(\n            bytes_to_read\n                .as_bytes()\n                .read_from(0, &mut image, size_of_bytes)\n                .unwrap(),\n            cursor_size\n        );\n    }\n\n    #[test]\n    fn ref_array_from_slice() {\n        let mut a = [2, 4, 6, 8, 10];\n        let a_vec = a.to_vec();\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        let a_slice = a_ref.get_slice(0, a_ref.len()).unwrap();\n        let a_array_ref: VolatileArrayRef<u8, ()> = a_slice.into();\n        for (i, entry) in a_vec.iter().enumerate() {\n            assert_eq!(&a_array_ref.load(i), entry);\n        }\n    }\n\n    #[test]\n    fn ref_array_store() {\n        let mut a = [0u8; 5];\n        {\n            let a_ref = VolatileSlice::from(&mut a[..]);\n            let v_ref = a_ref.get_array_ref(1, 4).unwrap();\n            v_ref.store(1, 2u8);\n            v_ref.store(2, 4u8);\n            v_ref.store(3, 6u8);\n        }\n        let expected = [2u8, 4u8, 6u8];\n        assert_eq!(a[2..=4], expected);\n    }\n\n    #[test]\n    fn ref_array_load() {\n        let mut a = [0, 0, 2, 3, 10];\n        {\n            let a_ref = VolatileSlice::from(&mut a[..]);\n            let c = {\n                let v_ref = a_ref.get_array_ref::<u8>(1, 4).unwrap();\n                assert_eq!(v_ref.load(1), 2u8);\n                assert_eq!(v_ref.load(2), 3u8);\n                assert_eq!(v_ref.load(3), 10u8);\n                v_ref\n            };\n            // To make sure we can take a v_ref out of the scope we made it in:\n            c.load(0);\n            // but not too far:\n            // c\n        } //.load()\n        ;\n    }\n\n    #[test]\n    fn ref_array_overflow() {\n        let mut a = [0, 0, 2, 3, 10];\n        let a_ref = VolatileSlice::from(&mut a[..]);\n        let res = a_ref.get_array_ref::<u32>(4, usize::MAX).unwrap_err();\n        assert_matches!(\n            res,\n            Error::TooBig {\n                nelements: usize::MAX,\n                size: 4,\n            }\n        );\n    }\n\n    #[test]\n    fn alignment() {\n        let a = [0u8; 64];\n        let a = &a[a.as_ptr().align_offset(32)] as *const u8 as usize;\n        assert!(super::alignment(a) >= 32);\n        assert_eq!(super::alignment(a + 9), 1);\n        assert_eq!(super::alignment(a + 30), 2);\n        assert_eq!(super::alignment(a + 12), 4);\n        assert_eq!(super::alignment(a + 8), 8);\n    }\n\n    #[test]\n    fn test_atomic_accesses() {\n        let len = 0x1000;\n        let buf = unsafe { std::alloc::alloc_zeroed(Layout::from_size_align(len, 8).unwrap()) };\n        let a = unsafe { VolatileSlice::new(buf, len) };\n\n        crate::bytes::tests::check_atomic_accesses(a, 0, 0x1000);\n        unsafe {\n            std::alloc::dealloc(buf, Layout::from_size_align(len, 8).unwrap());\n        }\n    }\n\n    #[test]\n    fn split_at() {\n        let mut mem = [0u8; 32];\n        let mem_ref = VolatileSlice::from(&mut mem[..]);\n        let vslice = mem_ref.get_slice(0, 32).unwrap();\n        let (start, end) = vslice.split_at(8).unwrap();\n        assert_eq!(start.len(), 8);\n        assert_eq!(end.len(), 24);\n        let (start, end) = vslice.split_at(0).unwrap();\n        assert_eq!(start.len(), 0);\n        assert_eq!(end.len(), 32);\n        let (start, end) = vslice.split_at(31).unwrap();\n        assert_eq!(start.len(), 31);\n        assert_eq!(end.len(), 1);\n        let (start, end) = vslice.split_at(32).unwrap();\n        assert_eq!(start.len(), 32);\n        assert_eq!(end.len(), 0);\n        let err = vslice.split_at(33).unwrap_err();\n        assert_matches!(err, Error::OutOfBounds { addr: _ })\n    }\n\n    #[test]\n    fn test_volatile_slice_dirty_tracking() {\n        let val = 123u64;\n        let dirty_offset = 0x1000;\n        let dirty_len = size_of_val(&val);\n        let page_size = 0x1000;\n\n        let len = 0x10000;\n        let buf = unsafe { std::alloc::alloc_zeroed(Layout::from_size_align(len, 8).unwrap()) };\n\n        // Invoke the `Bytes` test helper function.\n        {\n            let bitmap = AtomicBitmap::new(len, page_size);\n            let slice = unsafe { VolatileSlice::with_bitmap(buf, len, bitmap.slice_at(0), None) };\n\n            test_bytes(\n                &slice,\n                |s: &VolatileSlice<RefSlice<AtomicBitmap>>,\n                 start: usize,\n                 len: usize,\n                 clean: bool| { check_range(s.bitmap(), start, len, clean) },\n                |offset| offset,\n                0x1000,\n            );\n        }\n\n        // Invoke the `VolatileMemory` test helper function.\n        {\n            let bitmap = AtomicBitmap::new(len, page_size);\n            let slice = unsafe { VolatileSlice::with_bitmap(buf, len, bitmap.slice_at(0), None) };\n            test_volatile_memory(&slice);\n        }\n\n        let bitmap = AtomicBitmap::new(len, page_size);\n        let slice = unsafe { VolatileSlice::with_bitmap(buf, len, bitmap.slice_at(0), None) };\n\n        let bitmap2 = AtomicBitmap::new(len, page_size);\n        let slice2 = unsafe { VolatileSlice::with_bitmap(buf, len, bitmap2.slice_at(0), None) };\n\n        let bitmap3 = AtomicBitmap::new(len, page_size);\n        let slice3 = unsafe { VolatileSlice::with_bitmap(buf, len, bitmap3.slice_at(0), None) };\n\n        assert!(range_is_clean(slice.bitmap(), 0, slice.len()));\n        assert!(range_is_clean(slice2.bitmap(), 0, slice2.len()));\n\n        slice.write_obj(val, dirty_offset).unwrap();\n        assert!(range_is_dirty(slice.bitmap(), dirty_offset, dirty_len));\n\n        slice.copy_to_volatile_slice(slice2);\n        assert!(range_is_dirty(slice2.bitmap(), 0, slice2.len()));\n\n        {\n            let (s1, s2) = slice.split_at(dirty_offset).unwrap();\n            assert!(range_is_clean(s1.bitmap(), 0, s1.len()));\n            assert!(range_is_dirty(s2.bitmap(), 0, dirty_len));\n        }\n\n        {\n            let s = slice.subslice(dirty_offset, dirty_len).unwrap();\n            assert!(range_is_dirty(s.bitmap(), 0, s.len()));\n        }\n\n        {\n            let s = slice.offset(dirty_offset).unwrap();\n            assert!(range_is_dirty(s.bitmap(), 0, dirty_len));\n        }\n\n        // Test `copy_from` for size_of::<T> == 1.\n        {\n            let buf = vec![1u8; dirty_offset];\n\n            assert!(range_is_clean(slice.bitmap(), 0, dirty_offset));\n            slice.copy_from(&buf);\n            assert!(range_is_dirty(slice.bitmap(), 0, dirty_offset));\n        }\n\n        // Test `copy_from` for size_of::<T> > 1.\n        {\n            let val = 1u32;\n            let buf = vec![val; dirty_offset / size_of_val(&val)];\n\n            assert!(range_is_clean(slice3.bitmap(), 0, dirty_offset));\n            slice3.copy_from(&buf);\n            assert!(range_is_dirty(slice3.bitmap(), 0, dirty_offset));\n        }\n\n        unsafe {\n            std::alloc::dealloc(buf, Layout::from_size_align(len, 8).unwrap());\n        }\n    }\n\n    #[test]\n    fn test_volatile_ref_dirty_tracking() {\n        let val = 123u64;\n        let mut buf = vec![val];\n        let page_size = 0x1000;\n\n        let bitmap = AtomicBitmap::new(size_of_val(&val), page_size);\n        let vref = unsafe {\n            VolatileRef::with_bitmap(buf.as_mut_ptr() as *mut u8, bitmap.slice_at(0), None)\n        };\n\n        assert!(range_is_clean(vref.bitmap(), 0, vref.len()));\n        vref.store(val);\n        assert!(range_is_dirty(vref.bitmap(), 0, vref.len()));\n    }\n\n    fn test_volatile_array_ref_copy_from_tracking<T>(buf: &mut [T], index: usize, page_size: usize)\n    where\n        T: ByteValued + From<u8>,\n    {\n        let bitmap = AtomicBitmap::new(size_of_val(buf), page_size);\n        let arr = unsafe {\n            VolatileArrayRef::with_bitmap(\n                buf.as_mut_ptr() as *mut u8,\n                index + 1,\n                bitmap.slice_at(0),\n                None,\n            )\n        };\n\n        let val = T::from(123);\n        let copy_buf = vec![val; index + 1];\n\n        assert!(range_is_clean(arr.bitmap(), 0, arr.len() * size_of::<T>()));\n        arr.copy_from(copy_buf.as_slice());\n        assert!(range_is_dirty(arr.bitmap(), 0, size_of_val(buf)));\n    }\n\n    #[test]\n    fn test_volatile_array_ref_dirty_tracking() {\n        let val = 123u64;\n        let dirty_len = size_of_val(&val);\n        let index = 0x1000;\n        let dirty_offset = dirty_len * index;\n        let page_size = 0x1000;\n\n        let mut buf = vec![0u64; index + 1];\n        let mut byte_buf = vec![0u8; index + 1];\n\n        // Test `ref_at`.\n        {\n            let bitmap = AtomicBitmap::new(buf.len() * size_of_val(&val), page_size);\n            let arr = unsafe {\n                VolatileArrayRef::with_bitmap(\n                    buf.as_mut_ptr() as *mut u8,\n                    index + 1,\n                    bitmap.slice_at(0),\n                    None,\n                )\n            };\n\n            assert!(range_is_clean(arr.bitmap(), 0, arr.len() * dirty_len));\n            arr.ref_at(index).store(val);\n            assert!(range_is_dirty(arr.bitmap(), dirty_offset, dirty_len));\n        }\n\n        // Test `store`.\n        {\n            let bitmap = AtomicBitmap::new(buf.len() * size_of_val(&val), page_size);\n            let arr = unsafe {\n                VolatileArrayRef::with_bitmap(\n                    buf.as_mut_ptr() as *mut u8,\n                    index + 1,\n                    bitmap.slice_at(0),\n                    None,\n                )\n            };\n\n            let slice = arr.to_slice();\n            assert!(range_is_clean(slice.bitmap(), 0, slice.len()));\n            arr.store(index, val);\n            assert!(range_is_dirty(slice.bitmap(), dirty_offset, dirty_len));\n        }\n\n        // Test `copy_from` when size_of::<T>() == 1.\n        test_volatile_array_ref_copy_from_tracking(&mut byte_buf, index, page_size);\n        // Test `copy_from` when size_of::<T>() > 1.\n        test_volatile_array_ref_copy_from_tracking(&mut buf, index, page_size);\n    }\n}\n"], "filenames": ["CHANGELOG.md", "Cargo.toml", "src/volatile_memory.rs"], "buggy_code_start_loc": [2, 3, 111], "buggy_code_end_loc": [25, 4, 211], "fixing_code_start_loc": [1, 3, 112], "fixing_code_end_loc": [19, 4, 273], "type": "CWE-125", "message": "In a typical Virtual Machine Monitor (VMM) there are several components, such as boot loader, virtual device drivers, virtio backend drivers and vhost drivers, that need to access the VM physical memory. The vm-memory rust crate provides a set of traits to decouple VM memory consumers from VM memory providers. An issue was discovered in the default implementations of the `VolatileMemory::{get_atomic_ref, aligned_as_ref, aligned_as_mut, get_ref, get_array_ref}` trait functions, which allows out-of-bounds memory access if the `VolatileMemory::get_slice` function returns a `VolatileSlice` whose length is less than the function\u2019s `count` argument. No implementations of `get_slice` provided in `vm_memory` are affected. Users of custom `VolatileMemory` implementations may be impacted if the custom implementation does not adhere to `get_slice`'s documentation. The issue started in version 0.1.0 but was fixed in version 0.12.2 by inserting a check that verifies that the `VolatileSlice` returned by `get_slice` is of the correct length. Users are advised to upgrade. There are no known workarounds for this issue.\n", "other": {"cve": {"id": "CVE-2023-41051", "sourceIdentifier": "security-advisories@github.com", "published": "2023-09-01T19:15:42.883", "lastModified": "2023-09-28T03:15:10.457", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "In a typical Virtual Machine Monitor (VMM) there are several components, such as boot loader, virtual device drivers, virtio backend drivers and vhost drivers, that need to access the VM physical memory. The vm-memory rust crate provides a set of traits to decouple VM memory consumers from VM memory providers. An issue was discovered in the default implementations of the `VolatileMemory::{get_atomic_ref, aligned_as_ref, aligned_as_mut, get_ref, get_array_ref}` trait functions, which allows out-of-bounds memory access if the `VolatileMemory::get_slice` function returns a `VolatileSlice` whose length is less than the function\u2019s `count` argument. No implementations of `get_slice` provided in `vm_memory` are affected. Users of custom `VolatileMemory` implementations may be impacted if the custom implementation does not adhere to `get_slice`'s documentation. The issue started in version 0.1.0 but was fixed in version 0.12.2 by inserting a check that verifies that the `VolatileSlice` returned by `get_slice` is of the correct length. Users are advised to upgrade. There are no known workarounds for this issue.\n"}, {"lang": "es", "value": "En un Monitor de M\u00e1quina Virtual (VMM) t\u00edpico hay varios componentes, como el cargador de arranque, los controladores de dispositivos virtuales, los controladores virtio backend y los controladores vhost, que necesitan acceder a la memoria f\u00edsica de la VM. La caja rust vm-memory proporciona un conjunto de rasgos para desacoplar los consumidores de memoria VM de los proveedores de memoria VM. Se ha descubierto un problema en las implementaciones por defecto de las funciones trait `VolatileMemory::{get_atomic_ref, aligned_as_ref, aligned_as_mut, get_ref, get_array_ref}`, que permite el acceso a memoria fuera de los l\u00edmites si la funci\u00f3n `VolatileMemory::get_slice` devuelve una `VolatileSlice` cuya longitud es menor que el argumento `count` de la funci\u00f3n.No afecta a las implementaciones de `get_slice` proporcionadas en `vm_memory`.Los usuarios de implementaciones personalizadas de `VolatileMemory` pueden verse afectados si la implementaci\u00f3n personalizada no se adhiere a la documentaci\u00f3n de `get_slice`. El problema comenz\u00f3 en la versi\u00f3n 0.1.0 pero se solucion\u00f3 en la versi\u00f3n 0.12.2 insertando una comprobaci\u00f3n que verifica que la `VolatileSlice` devuelta por `get_slice` tiene la longitud correcta. Se recomienda a los usuarios que actualicen. No se conocen soluciones para este problema."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:N/UI:R/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 4.7, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.0, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:N/UI:R/S:U/C:N/I:N/A:L", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "NONE", "userInteraction": "REQUIRED", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "LOW", "baseScore": 2.5, "baseSeverity": "LOW"}, "exploitabilityScore": 1.0, "impactScore": 1.4}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-125"}]}, {"source": "nvd@nist.gov", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-125"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:vm-memory_project:vm-memory:*:*:*:*:*:rust:*:*", "versionStartIncluding": "0.1.0", "versionEndExcluding": "0.12.2", "matchCriteriaId": "48D4C531-5DF9-4E76-8274-F133E20944B4"}]}]}], "references": [{"url": "https://crates.io/crates/vm-memory/0.12.2", "source": "security-advisories@github.com", "tags": ["Product"]}, {"url": "https://github.com/rust-vmm/vm-memory/commit/aff1dd4a5259f7deba56692840f7a2d9ca34c9c8", "source": "security-advisories@github.com", "tags": ["Patch"]}, {"url": "https://github.com/rust-vmm/vm-memory/security/advisories/GHSA-49hh-fprx-m68g", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/IPXRXD5VXBZHBGMUM77B52CJJMG7EJGI/", "source": "security-advisories@github.com"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/SYM6CYW2DWRHRAVL2HYTQPXC3J2V77J4/", "source": "security-advisories@github.com"}, {"url": "https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/XZGJL6BQLU4XCPQLLTW4GSSBTNQXB3TI/", "source": "security-advisories@github.com"}]}, "github_commit_url": "https://github.com/rust-vmm/vm-memory/commit/aff1dd4a5259f7deba56692840f7a2d9ca34c9c8"}}