{"buggy_code": ["from __future__ import unicode_literals\nfrom itertools import chain\nimport re\nimport string\n\nimport six\nfrom xml.sax.saxutils import unescape\n\nimport html5lib\nfrom html5lib.constants import (\n    entities,\n    namespaces,\n    prefixes,\n    tokenTypes,\n)\ntry:\n    from html5lib.constants import ReparseException\nexcept ImportError:\n    # html5lib-python 1.0 changed the name\n    from html5lib.constants import _ReparseException as ReparseException\nfrom html5lib.filters.base import Filter\nfrom html5lib.filters import sanitizer\nfrom html5lib.serializer import HTMLSerializer\nfrom html5lib._tokenizer import HTMLTokenizer\nfrom html5lib._trie import Trie\n\nfrom bleach.utils import alphabetize_attributes, force_unicode\n\n\n#: Trie of html entity string -> character representation\nENTITIES_TRIE = Trie(entities)\n\n#: List of allowed tags\nALLOWED_TAGS = [\n    'a',\n    'abbr',\n    'acronym',\n    'b',\n    'blockquote',\n    'code',\n    'em',\n    'i',\n    'li',\n    'ol',\n    'strong',\n    'ul',\n]\n\n\n#: Map of allowed attributes by tag\nALLOWED_ATTRIBUTES = {\n    'a': ['href', 'title'],\n    'abbr': ['title'],\n    'acronym': ['title'],\n}\n\n\n#: List of allowed styles\nALLOWED_STYLES = []\n\n\n#: List of allowed protocols\nALLOWED_PROTOCOLS = ['http', 'https', 'mailto']\n\n\nAMP_SPLIT_RE = re.compile('(&)')\n\n#: Invisible characters--0 to and including 31 except 9 (tab), 10 (lf), and 13 (cr)\nINVISIBLE_CHARACTERS = ''.join([chr(c) for c in chain(range(0, 9), range(11, 13), range(14, 32))])\n\n#: Regexp for characters that are invisible\nINVISIBLE_CHARACTERS_RE = re.compile(\n    '[' + INVISIBLE_CHARACTERS + ']',\n    re.UNICODE\n)\n\n#: String to replace invisible characters with. This can be a character, a\n#: string, or even a function that takes a Python re matchobj\nINVISIBLE_REPLACEMENT_CHAR = '?'\n\n\nclass BleachHTMLTokenizer(HTMLTokenizer):\n    def consumeEntity(self, allowedChar=None, fromAttribute=False):\n        # We don't want to consume and convert entities, so this overrides the\n        # html5lib tokenizer's consumeEntity so that it's now a no-op.\n        #\n        # However, when that gets called, it's consumed an &, so we put that in\n        # the steam.\n        if fromAttribute:\n            self.currentToken['data'][-1][1] += '&'\n\n        else:\n            self.tokenQueue.append({\"type\": tokenTypes['Characters'], \"data\": '&'})\n\n\nclass BleachHTMLParser(html5lib.HTMLParser):\n    def _parse(self, stream, innerHTML=False, container=\"div\", scripting=False, **kwargs):\n        # Override HTMLParser so we can swap out the tokenizer for our own.\n        self.innerHTMLMode = innerHTML\n        self.container = container\n        self.scripting = scripting\n        self.tokenizer = BleachHTMLTokenizer(stream, parser=self, **kwargs)\n        self.reset()\n\n        try:\n            self.mainLoop()\n        except ReparseException:\n            self.reset()\n            self.mainLoop()\n\n\nclass Cleaner(object):\n    \"\"\"Cleaner for cleaning HTML fragments of malicious content\n\n    This cleaner is a security-focused function whose sole purpose is to remove\n    malicious content from a string such that it can be displayed as content in\n    a web page.\n\n    This cleaner is not designed to use to transform content to be used in\n    non-web-page contexts.\n\n    To use::\n\n        from bleach.sanitizer import Cleaner\n\n        cleaner = Cleaner()\n\n        for text in all_the_yucky_things:\n            sanitized = cleaner.clean(text)\n\n    \"\"\"\n\n    def __init__(self, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES,\n                 styles=ALLOWED_STYLES, protocols=ALLOWED_PROTOCOLS, strip=False,\n                 strip_comments=True, filters=None):\n        \"\"\"Initializes a Cleaner\n\n        :arg list tags: allowed list of tags; defaults to\n            ``bleach.sanitizer.ALLOWED_TAGS``\n\n        :arg dict attributes: allowed attributes; can be a callable, list or dict;\n            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``\n\n        :arg list styles: allowed list of css styles; defaults to\n            ``bleach.sanitizer.ALLOWED_STYLES``\n\n        :arg list protocols: allowed list of protocols for links; defaults\n            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``\n\n        :arg bool strip: whether or not to strip disallowed elements\n\n        :arg bool strip_comments: whether or not to strip HTML comments\n\n        :arg list filters: list of html5lib Filter classes to pass streamed content through\n\n            .. seealso:: http://html5lib.readthedocs.io/en/latest/movingparts.html#filters\n\n            .. Warning::\n\n               Using filters changes the output of ``bleach.Cleaner.clean``.\n               Make sure the way the filters change the output are secure.\n\n        \"\"\"\n        self.tags = tags\n        self.attributes = attributes\n        self.styles = styles\n        self.protocols = protocols\n        self.strip = strip\n        self.strip_comments = strip_comments\n        self.filters = filters or []\n\n        self.parser = BleachHTMLParser(namespaceHTMLElements=False)\n        self.walker = html5lib.getTreeWalker('etree')\n        self.serializer = BleachHTMLSerializer(\n            quote_attr_values='always',\n            omit_optional_tags=False,\n            escape_lt_in_attrs=True,\n\n            # We want to leave entities as they are without escaping or\n            # resolving or expanding\n            resolve_entities=False,\n\n            # Bleach has its own sanitizer, so don't use the html5lib one\n            sanitize=False,\n\n            # Bleach sanitizer alphabetizes already, so don't use the html5lib one\n            alphabetical_attributes=False,\n        )\n\n    def clean(self, text):\n        \"\"\"Cleans text and returns sanitized result as unicode\n\n        :arg str text: text to be cleaned\n\n        :returns: sanitized text as unicode\n\n        :raises TypeError: if ``text`` is not a text type\n\n        \"\"\"\n        if not isinstance(text, six.string_types):\n            message = \"argument cannot be of '{name}' type, must be of text type\".format(\n                name=text.__class__.__name__)\n            raise TypeError(message)\n\n        if not text:\n            return u''\n\n        text = force_unicode(text)\n\n        dom = self.parser.parseFragment(text)\n        filtered = BleachSanitizerFilter(\n            source=self.walker(dom),\n\n            # Bleach-sanitizer-specific things\n            attributes=self.attributes,\n            strip_disallowed_elements=self.strip,\n            strip_html_comments=self.strip_comments,\n\n            # html5lib-sanitizer things\n            allowed_elements=self.tags,\n            allowed_css_properties=self.styles,\n            allowed_protocols=self.protocols,\n            allowed_svg_properties=[],\n        )\n\n        # Apply any filters after the BleachSanitizerFilter\n        for filter_class in self.filters:\n            filtered = filter_class(source=filtered)\n\n        return self.serializer.render(filtered)\n\n\ndef attribute_filter_factory(attributes):\n    \"\"\"Generates attribute filter function for the given attributes value\n\n    The attributes value can take one of several shapes. This returns a filter\n    function appropriate to the attributes value. One nice thing about this is\n    that there's less if/then shenanigans in the ``allow_token`` method.\n\n    \"\"\"\n    if callable(attributes):\n        return attributes\n\n    if isinstance(attributes, dict):\n        def _attr_filter(tag, attr, value):\n            if tag in attributes:\n                attr_val = attributes[tag]\n                if callable(attr_val):\n                    return attr_val(tag, attr, value)\n\n                if attr in attr_val:\n                    return True\n\n            if '*' in attributes:\n                attr_val = attributes['*']\n                if callable(attr_val):\n                    return attr_val(tag, attr, value)\n\n                return attr in attr_val\n\n            return False\n\n        return _attr_filter\n\n    if isinstance(attributes, list):\n        def _attr_filter(tag, attr, value):\n            return attr in attributes\n\n        return _attr_filter\n\n    raise ValueError('attributes needs to be a callable, a list or a dict')\n\n\ndef match_entity(stream):\n    \"\"\"Returns first entity in stream or None if no entity exists\n\n    Note: For Bleach purposes, entities must start with a \"&\" and end with\n    a \";\".\n\n    :arg stream: the character stream\n\n    :returns: ``None`` or the entity string without \"&\" or \";\"\n\n    \"\"\"\n    # Nix the & at the beginning\n    if stream[0] != '&':\n        raise ValueError('Stream should begin with \"&\"')\n\n    stream = stream[1:]\n\n    stream = list(stream)\n    possible_entity = ''\n    end_characters = '<&=;' + string.whitespace\n\n    # Handle number entities\n    if stream and stream[0] == '#':\n        possible_entity = '#'\n        stream.pop(0)\n\n        if stream and stream[0] in ('x', 'X'):\n            allowed = '0123456789abcdefABCDEF'\n            possible_entity += stream.pop(0)\n        else:\n            allowed = '0123456789'\n\n        # FIXME(willkg): Do we want to make sure these are valid number\n        # entities? This doesn't do that currently.\n        while stream and stream[0] not in end_characters:\n            c = stream.pop(0)\n            if c not in allowed:\n                break\n            possible_entity += c\n\n        if possible_entity and stream and stream[0] == ';':\n            return possible_entity\n        return None\n\n    # Handle character entities\n    while stream and stream[0] not in end_characters:\n        c = stream.pop(0)\n        if not ENTITIES_TRIE.has_keys_with_prefix(possible_entity):\n            break\n        possible_entity += c\n\n    if possible_entity and stream and stream[0] == ';':\n        return possible_entity\n\n    return None\n\n\ndef next_possible_entity(text):\n    \"\"\"Takes a text and generates a list of possible entities\n\n    :arg text: the text to look at\n\n    :returns: generator where each part (except the first) starts with an\n        \"&\"\n\n    \"\"\"\n    for i, part in enumerate(AMP_SPLIT_RE.split(text)):\n        if i == 0:\n            yield part\n        elif i % 2 == 0:\n            yield '&' + part\n\n\nclass BleachSanitizerFilter(sanitizer.Filter):\n    \"\"\"html5lib Filter that sanitizes text\n\n    This filter can be used anywhere html5lib filters can be used.\n\n    \"\"\"\n    def __init__(self, source, attributes=ALLOWED_ATTRIBUTES,\n                 strip_disallowed_elements=False, strip_html_comments=True,\n                 **kwargs):\n        \"\"\"Creates a BleachSanitizerFilter instance\n\n        :arg Treewalker source: stream\n\n        :arg list tags: allowed list of tags; defaults to\n            ``bleach.sanitizer.ALLOWED_TAGS``\n\n        :arg dict attributes: allowed attributes; can be a callable, list or dict;\n            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``\n\n        :arg list styles: allowed list of css styles; defaults to\n            ``bleach.sanitizer.ALLOWED_STYLES``\n\n        :arg list protocols: allowed list of protocols for links; defaults\n            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``\n\n        :arg bool strip_disallowed_elements: whether or not to strip disallowed\n            elements\n\n        :arg bool strip_html_comments: whether or not to strip HTML comments\n\n        \"\"\"\n        self.attr_filter = attribute_filter_factory(attributes)\n\n        self.strip_disallowed_elements = strip_disallowed_elements\n        self.strip_html_comments = strip_html_comments\n\n        return super(BleachSanitizerFilter, self).__init__(source, **kwargs)\n\n    def __iter__(self):\n        for token in Filter.__iter__(self):\n            ret = self.sanitize_token(token)\n\n            if not ret:\n                continue\n\n            if isinstance(ret, list):\n                for subtoken in ret:\n                    yield subtoken\n            else:\n                yield ret\n\n    def sanitize_token(self, token):\n        \"\"\"Sanitize a token either by HTML-encoding or dropping.\n\n        Unlike sanitizer.Filter, allowed_attributes can be a dict of {'tag':\n        ['attribute', 'pairs'], 'tag': callable}.\n\n        Here callable is a function with two arguments of attribute name and\n        value. It should return true of false.\n\n        Also gives the option to strip tags instead of encoding.\n\n        :arg dict token: token to sanitize\n\n        :returns: token or list of tokens\n\n        \"\"\"\n        token_type = token['type']\n        if token_type in ['StartTag', 'EndTag', 'EmptyTag']:\n            if token['name'] in self.allowed_elements:\n                return self.allow_token(token)\n\n            elif self.strip_disallowed_elements:\n                return None\n\n            else:\n                if 'data' in token:\n                    # Alphabetize the attributes before calling .disallowed_token()\n                    # so that the resulting string is stable\n                    token['data'] = alphabetize_attributes(token['data'])\n                return self.disallowed_token(token)\n\n        elif token_type == 'Comment':\n            if not self.strip_html_comments:\n                return token\n            else:\n                return None\n\n        elif token_type == 'Characters':\n            return self.sanitize_characters(token)\n\n        else:\n            return token\n\n    def sanitize_characters(self, token):\n        \"\"\"Handles Characters tokens\n\n        Our overridden tokenizer doesn't do anything with entities. However,\n        that means that the serializer will convert all ``&`` in Characters\n        tokens to ``&amp;``.\n\n        Since we don't want that, we extract entities here and convert them to\n        Entity tokens so the serializer will let them be.\n\n        :arg token: the Characters token to work on\n\n        :returns: a list of tokens\n\n        \"\"\"\n        data = token.get('data', '')\n\n        if not data:\n            return token\n\n        data = INVISIBLE_CHARACTERS_RE.sub(INVISIBLE_REPLACEMENT_CHAR, data)\n        token['data'] = data\n\n        # If there isn't a & in the data, we can return now\n        if '&' not in data:\n            return token\n\n        new_tokens = []\n\n        # For each possible entity that starts with a \"&\", we try to extract an\n        # actual entity and re-tokenize accordingly\n        for part in next_possible_entity(data):\n            if not part:\n                continue\n\n            if part.startswith('&'):\n                entity = match_entity(part)\n                if entity is not None:\n                    new_tokens.append({'type': 'Entity', 'name': entity})\n                    # Length of the entity plus 2--one for & at the beginning\n                    # and and one for ; at the end\n                    part = part[len(entity) + 2:]\n                    if part:\n                        new_tokens.append({'type': 'Characters', 'data': part})\n                    continue\n\n            new_tokens.append({'type': 'Characters', 'data': part})\n\n        return new_tokens\n\n    def allow_token(self, token):\n        \"\"\"Handles the case where we're allowing the tag\"\"\"\n        if 'data' in token:\n            # Loop through all the attributes and drop the ones that are not\n            # allowed, are unsafe or break other rules. Additionally, fix\n            # attribute values that need fixing.\n            #\n            # At the end of this loop, we have the final set of attributes\n            # we're keeping.\n            attrs = {}\n            for namespaced_name, val in token['data'].items():\n                namespace, name = namespaced_name\n\n                # Drop attributes that are not explicitly allowed\n                #\n                # NOTE(willkg): We pass in the attribute name--not a namespaced\n                # name.\n                if not self.attr_filter(token['name'], name, val):\n                    continue\n\n                # Look at attributes that have uri values\n                if namespaced_name in self.attr_val_is_uri:\n                    val_unescaped = re.sub(\n                        \"[`\\000-\\040\\177-\\240\\s]+\",\n                        '',\n                        unescape(val)).lower()\n\n                    # Remove replacement characters from unescaped characters.\n                    val_unescaped = val_unescaped.replace(\"\\ufffd\", \"\")\n\n                    # Drop attributes with uri values that have protocols that\n                    # aren't allowed\n                    if (re.match(r'^[a-z0-9][-+.a-z0-9]*:', val_unescaped) and\n                            (val_unescaped.split(':')[0] not in self.allowed_protocols)):\n                        continue\n\n                # Drop values in svg attrs with non-local IRIs\n                if namespaced_name in self.svg_attr_val_allows_ref:\n                    new_val = re.sub(r'url\\s*\\(\\s*[^#\\s][^)]+?\\)',\n                                     ' ',\n                                     unescape(val))\n                    new_val = new_val.strip()\n                    if not new_val:\n                        continue\n\n                    else:\n                        # Replace the val with the unescaped version because\n                        # it's a iri\n                        val = new_val\n\n                # Drop href and xlink:href attr for svg elements with non-local IRIs\n                if (None, token['name']) in self.svg_allow_local_href:\n                    if namespaced_name in [(None, 'href'), (namespaces['xlink'], 'href')]:\n                        if re.search(r'^\\s*[^#\\s]', val):\n                            continue\n\n                # If it's a style attribute, sanitize it\n                if namespaced_name == (None, u'style'):\n                    val = self.sanitize_css(val)\n\n                # At this point, we want to keep the attribute, so add it in\n                attrs[namespaced_name] = val\n\n            token['data'] = alphabetize_attributes(attrs)\n\n        return token\n\n    def disallowed_token(self, token):\n        token_type = token[\"type\"]\n        if token_type == \"EndTag\":\n            token[\"data\"] = \"</%s>\" % token[\"name\"]\n\n        elif token[\"data\"]:\n            assert token_type in (\"StartTag\", \"EmptyTag\")\n            attrs = []\n            for (ns, name), v in token[\"data\"].items():\n                attrs.append(' %s=\"%s\"' % (\n                    name if ns is None else \"%s:%s\" % (prefixes[ns], name),\n                    # NOTE(willkg): HTMLSerializer escapes attribute values\n                    # already, so if we do it here (like HTMLSerializer does),\n                    # then we end up double-escaping.\n                    v)\n                )\n            token[\"data\"] = \"<%s%s>\" % (token[\"name\"], ''.join(attrs))\n\n        else:\n            token[\"data\"] = \"<%s>\" % token[\"name\"]\n\n        if token.get(\"selfClosing\"):\n            token[\"data\"] = token[\"data\"][:-1] + \"/>\"\n\n        token[\"type\"] = \"Characters\"\n\n        del token[\"name\"]\n        return token\n\n    def sanitize_css(self, style):\n        \"\"\"Sanitizes css in style tags\"\"\"\n        # disallow urls\n        style = re.compile('url\\s*\\(\\s*[^\\s)]+?\\s*\\)\\s*').sub(' ', style)\n\n        # gauntlet\n\n        # Validate the css in the style tag and if it's not valid, then drop\n        # the whole thing.\n        parts = style.split(';')\n        gauntlet = re.compile(\n            r\"\"\"^([-/:,#%.'\"\\sa-zA-Z0-9!]|\\w-\\w|'[\\s\\w]+'\\s*|\"[\\s\\w]+\"|\\([\\d,%\\.\\s]+\\))*$\"\"\"\n        )\n\n        for part in parts:\n            if not gauntlet.match(part):\n                return ''\n\n        if not re.match(\"^\\s*([-\\w]+\\s*:[^:;]*(;\\s*|$))*$\", style):\n            return ''\n\n        clean = []\n        for prop, value in re.findall('([-\\w]+)\\s*:\\s*([^:;]*)', style):\n            if not value:\n                continue\n\n            if prop.lower() in self.allowed_css_properties:\n                clean.append(prop + ': ' + value + ';')\n\n            elif prop.lower() in self.allowed_svg_properties:\n                clean.append(prop + ': ' + value + ';')\n\n        return ' '.join(clean)\n\n\nclass BleachHTMLSerializer(HTMLSerializer):\n    \"\"\"Wraps the HTMLSerializer and undoes & -> &amp; in attributes\"\"\"\n    def escape_base_amp(self, stoken):\n        \"\"\"Escapes bare & in HTML attribute values\"\"\"\n        # First, undo what the HTMLSerializer did\n        stoken = stoken.replace('&amp;', '&')\n\n        # Then, escape any bare &\n        for part in next_possible_entity(stoken):\n            if not part:\n                continue\n\n            if part.startswith('&'):\n                entity = match_entity(part)\n                if entity is not None:\n                    yield '&' + entity + ';'\n\n                    # Length of the entity plus 2--one for & at the beginning\n                    # and and one for ; at the end\n                    part = part[len(entity) + 2:]\n                    if part:\n                        yield part\n                    continue\n\n            yield part.replace('&', '&amp;')\n\n    def serialize(self, treewalker, encoding=None):\n        \"\"\"Wrap HTMLSerializer.serialize and escape bare & in attributes\"\"\"\n        in_tag = False\n        after_equals = False\n\n        for stoken in super(BleachHTMLSerializer, self).serialize(treewalker, encoding):\n            if in_tag:\n                if stoken == '>':\n                    in_tag = False\n\n                elif after_equals:\n                    if stoken != '\"':\n                        for part in self.escape_base_amp(stoken):\n                            yield part\n\n                        after_equals = False\n                        continue\n\n                elif stoken == '=':\n                    after_equals = True\n\n                yield stoken\n            else:\n                if stoken.startswith('<'):\n                    in_tag = True\n                yield stoken\n", "import os\n\nfrom html5lib.filters.base import Filter\nimport pytest\n\nfrom bleach import clean\nfrom bleach.sanitizer import Cleaner\n\n\ndef test_clean_idempotent():\n    \"\"\"Make sure that applying the filter twice doesn't change anything.\"\"\"\n    dirty = '<span>invalid & </span> < extra http://link.com<em>'\n    assert clean(clean(dirty)) == clean(dirty)\n\n\ndef test_only_text_is_cleaned():\n    some_text = 'text'\n    some_type = int\n    no_type = None\n\n    assert clean(some_text) == some_text\n\n    with pytest.raises(TypeError) as e:\n        clean(some_type)\n    assert \"argument cannot be of 'type' type\" in str(e)\n\n    with pytest.raises(TypeError) as e:\n        clean(no_type)\n    assert \"NoneType\" in str(e)\n\n\ndef test_empty():\n    assert clean('') == ''\n\n\ndef test_content_has_no_html():\n    assert clean('no html string') == 'no html string'\n\n\n@pytest.mark.parametrize('data, expected', [\n    (\n        'an <strong>allowed</strong> tag',\n        'an <strong>allowed</strong> tag'\n    ),\n\n    (\n        'another <em>good</em> tag',\n        'another <em>good</em> tag'\n    )\n])\ndef test_content_has_allowed_html(data, expected):\n    assert clean(data) == expected\n\n\ndef test_html_is_lowercased():\n    assert (\n        clean('<A HREF=\"http://example.com\">foo</A>') ==\n        '<a href=\"http://example.com\">foo</a>'\n    )\n\n\n@pytest.mark.parametrize('data, should_strip, expected', [\n    # Regular comment\n    (\n        '<!-- this is a comment -->',\n        True,\n        ''\n    ),\n\n    # Open comment with no close comment bit\n    (\n        '<!-- open comment',\n        True,\n        ''\n    ),\n    (\n        '<!--open comment',\n        True,\n        ''\n    ),\n    (\n        '<!-- open comment',\n        False,\n        '<!-- open comment-->'\n    ),\n    (\n        '<!--open comment',\n        False,\n        '<!--open comment-->'\n    ),\n\n    # Comment with text to the right\n    (\n        '<!-- comment -->text',\n        True,\n        'text'\n    ),\n    (\n        '<!--comment-->text',\n        True,\n        'text'\n    ),\n    (\n        '<!-- comment -->text',\n        False,\n        '<!-- comment -->text'\n    ),\n    (\n        '<!--comment-->text',\n        False,\n        '<!--comment-->text'\n    ),\n\n    # Comment with text to the left\n    (\n        'text<!-- comment -->',\n        True,\n        'text'\n    ),\n    (\n        'text<!--comment-->',\n        True,\n        'text'\n    ),\n    (\n        'text<!-- comment -->',\n        False,\n        'text<!-- comment -->'\n    ),\n    (\n        'text<!--comment-->',\n        False,\n        'text<!--comment-->'\n    )\n])\ndef test_comments(data, should_strip, expected):\n    assert clean(data, strip_comments=should_strip) == expected\n\n\n@pytest.mark.parametrize('data, expected', [\n    # Disallowed tag is escaped\n    ('<img src=\"javascript:alert(\\'XSS\\');\">', '&lt;img src=\"javascript:alert(\\'XSS\\');\"&gt;'),\n\n    # Test with parens\n    ('a <script>safe()</script> test', 'a &lt;script&gt;safe()&lt;/script&gt; test'),\n\n    # Test with braces\n    ('a <style>body{}</style> test', 'a &lt;style&gt;body{}&lt;/style&gt; test'),\n])\ndef test_disallowed_tags(data, expected):\n    assert clean(data) == expected\n\n\ndef test_invalid_char_in_tag():\n    # NOTE(willkg): Two possible outcomes because attrs aren't ordered\n    assert (\n        clean('<script/xss src=\"http://xx.com/xss.js\"></script>') in\n        [\n            '&lt;script src=\"http://xx.com/xss.js\" xss=\"\"&gt;&lt;/script&gt;',\n            '&lt;script xss=\"\" src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n        ]\n    )\n    assert (\n        clean('<script/src=\"http://xx.com/xss.js\"></script>') ==\n        '&lt;script src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n    )\n\n\ndef test_unclosed_tag():\n    assert (\n        clean('a <em>fixed tag') ==\n        'a <em>fixed tag</em>'\n    )\n    assert (\n        clean('<script src=http://xx.com/xss.js<b>') ==\n        '&lt;script src=\"http://xx.com/xss.js&lt;b\"&gt;&lt;/script&gt;'\n    )\n    # NOTE(willkg): Two possible outcomes because attrs aren't ordered\n    assert (\n        clean('<script src=\"http://xx.com/xss.js\"<b>') in\n        [\n            '&lt;script src=\"http://xx.com/xss.js\" &lt;b=\"\"&gt;&lt;/script&gt;',\n            '&lt;script &lt;b=\"\" src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n        ]\n    )\n    # NOTE(willkg): Two possible outcomes because attrs aren't ordered\n    assert (\n        clean('<script src=\"http://xx.com/xss.js\" <b>') in\n        [\n            '&lt;script src=\"http://xx.com/xss.js\" &lt;b=\"\"&gt;&lt;/script&gt;',\n            '&lt;script &lt;b=\"\" src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n        ]\n    )\n\n\ndef test_nested_script_tag():\n    assert (\n        clean('<<script>script>evil()<</script>/script>') ==\n        '&lt;&lt;script&gt;script&gt;evil()&lt;&lt;/script&gt;/script&gt;'\n    )\n    assert (\n        clean('<<x>script>evil()<</x>/script>') ==\n        '&lt;&lt;x&gt;script&gt;evil()&lt;&lt;/x&gt;/script&gt;'\n    )\n    assert (\n        clean('<script<script>>evil()</script</script>>') ==\n        '&lt;script&lt;script&gt;&gt;evil()&gt;&lt;/script&lt;script&gt;'\n    )\n\n\n@pytest.mark.parametrize('text, expected', [\n    ('an & entity', 'an &amp; entity'),\n    ('an < entity', 'an &lt; entity'),\n    ('tag < <em>and</em> entity', 'tag &lt; <em>and</em> entity'),\n])\ndef test_bare_entities(text, expected):\n    assert clean(text) == expected\n\n\n@pytest.mark.parametrize('text, expected', [\n    # Test character entities\n    ('&amp;', '&amp;'),\n    ('&nbsp;', '&nbsp;'),\n    ('&nbsp; test string &nbsp;', '&nbsp; test string &nbsp;'),\n    ('&lt;em&gt;strong&lt;/em&gt;', '&lt;em&gt;strong&lt;/em&gt;'),\n\n    # Test character entity at beginning of string\n    ('&amp;is cool', '&amp;is cool'),\n\n    # Test it at the end of the string\n    ('cool &amp;', 'cool &amp;'),\n\n    # Test bare ampersands and entities at beginning\n    ('&&amp; is cool', '&amp;&amp; is cool'),\n\n    # Test entities and bare ampersand at end\n    ('&amp; is cool &amp;&', '&amp; is cool &amp;&amp;'),\n\n    # Test missing semi-colon means we don't treat it like an entity\n    ('this &amp that', 'this &amp;amp that'),\n\n    # Test a thing that looks like a character entity, but isn't because it's\n    # missing a ; (&curren)\n    (\n        'http://example.com?active=true&current=true',\n        'http://example.com?active=true&amp;current=true'\n    ),\n\n    # Test entities in HTML attributes\n    (\n        '<a href=\"?art&amp;copy\">foo</a>',\n        '<a href=\"?art&amp;copy\">foo</a>'\n    ),\n    (\n        '<a href=\"?this=&gt;that\">foo</a>',\n        '<a href=\"?this=&gt;that\">foo</a>'\n    ),\n    (\n        '<a href=\"http://example.com?active=true&current=true\">foo</a>',\n        '<a href=\"http://example.com?active=true&amp;current=true\">foo</a>'\n    ),\n\n    # Test numeric entities\n    ('&#39;', '&#39;'),\n    ('&#34;', '&#34;'),\n    ('&#123;', '&#123;'),\n    ('&#x0007b;', '&#x0007b;'),\n    ('&#x0007B;', '&#x0007B;'),\n\n    # Test non-numeric entities\n    ('&#', '&amp;#'),\n    ('&#<', '&amp;#&lt;'),\n\n    # html5lib tokenizer unescapes character entities, so these would become '\n    # and \" which makes it possible to break out of html attributes.\n    #\n    # Verify that clean() doesn't unescape entities.\n    ('&#39;&#34;', '&#39;&#34;'),\n])\ndef test_character_entities(text, expected):\n    assert clean(text) == expected\n\n\n@pytest.mark.parametrize('data, kwargs, expected', [\n    # All tags are allowed, so it strips nothing\n    (\n        'a test <em>with</em> <b>html</b> tags',\n        {'strip': True},\n        'a test <em>with</em> <b>html</b> tags'\n    ),\n\n    # img tag is disallowed, so it's stripped\n    (\n        'a test <em>with</em> <img src=\"http://example.com/\"> <b>html</b> tags',\n        {'strip': True},\n        'a test <em>with</em>  <b>html</b> tags'\n    ),\n\n    # a tag is disallowed, so it's stripped\n    (\n        '<p><a href=\"http://example.com/\">link text</a></p>',\n        {'tags': ['p'], 'strip': True},\n        '<p>link text</p>'\n    ),\n\n    # handle nested disallowed tag\n    (\n        '<p><span>multiply <span>nested <span>text</span></span></span></p>',\n        {'tags': ['p'], 'strip': True},\n        '<p>multiply nested text</p>'\n    ),\n\n    # handle disallowed tag that's deep in the tree\n    (\n        '<p><a href=\"http://example.com/\"><img src=\"http://example.com/\"></a></p>',\n        {'tags': ['a', 'p'], 'strip': True},\n        '<p><a href=\"http://example.com/\"></a></p>'\n    ),\n])\ndef test_stripping_tags(data, kwargs, expected):\n    assert clean(data, **kwargs) == expected\n\n\n@pytest.mark.parametrize('data, expected', [\n    (\n        '<scri<script>pt>alert(1)</scr</script>ipt>',\n        'pt&gt;alert(1)ipt&gt;'\n    ),\n    (\n        '<scri<scri<script>pt>pt>alert(1)</script>',\n        'pt&gt;pt&gt;alert(1)'\n    ),\n])\ndef test_stripping_tags_is_safe(data, expected):\n    \"\"\"Test stripping tags shouldn't result in malicious content\"\"\"\n    assert clean(data, strip=True) == expected\n\n\ndef test_allowed_styles():\n    \"\"\"Test allowed styles\"\"\"\n    ATTRS = ['style']\n    STYLE = ['color']\n\n    assert (\n        clean('<b style=\"top:0\"></b>', attributes=ATTRS) ==\n        '<b style=\"\"></b>'\n    )\n\n    text = '<b style=\"color: blue;\"></b>'\n    assert clean(text, attributes=ATTRS, styles=STYLE) == text\n\n    text = '<b style=\"top: 0; color: blue;\"></b>'\n    assert (\n        clean(text, attributes=ATTRS, styles=STYLE) ==\n        '<b style=\"color: blue;\"></b>'\n    )\n\n\ndef test_href_with_wrong_tag():\n    assert (\n        clean('<em href=\"fail\">no link</em>') ==\n        '<em>no link</em>'\n    )\n\n\ndef test_disallowed_attr():\n    IMG = ['img', ]\n    IMG_ATTR = ['src']\n\n    assert (\n        clean('<a onclick=\"evil\" href=\"test\">test</a>') ==\n        '<a href=\"test\">test</a>'\n    )\n    assert (\n        clean('<img onclick=\"evil\" src=\"test\" />', tags=IMG, attributes=IMG_ATTR) ==\n        '<img src=\"test\">'\n    )\n    assert (\n        clean('<img href=\"invalid\" src=\"test\" />', tags=IMG, attributes=IMG_ATTR) ==\n        '<img src=\"test\">'\n    )\n\n\ndef test_unquoted_attr_values_are_quoted():\n    assert (\n        clean('<abbr title=mytitle>myabbr</abbr>') ==\n        '<abbr title=\"mytitle\">myabbr</abbr>'\n    )\n\n\ndef test_unquoted_event_handler_attr_value():\n    assert (\n        clean('<a href=\"http://xx.com\" onclick=foo()>xx.com</a>') ==\n        '<a href=\"http://xx.com\">xx.com</a>'\n    )\n\n\ndef test_invalid_filter_attr():\n    IMG = ['img', ]\n    IMG_ATTR = {\n        'img': lambda tag, name, val: name == 'src' and val == \"http://example.com/\"\n    }\n\n    assert (\n        clean('<img onclick=\"evil\" src=\"http://example.com/\" />', tags=IMG, attributes=IMG_ATTR) ==\n        '<img src=\"http://example.com/\">'\n    )\n    assert (\n        clean('<img onclick=\"evil\" src=\"http://badhost.com/\" />', tags=IMG, attributes=IMG_ATTR) ==\n        '<img>'\n    )\n\n\ndef test_poster_attribute():\n    \"\"\"Poster attributes should not allow javascript.\"\"\"\n    tags = ['video']\n    attrs = {'video': ['poster']}\n\n    test = '<video poster=\"javascript:alert(1)\"></video>'\n    assert clean(test, tags=tags, attributes=attrs) == '<video></video>'\n\n    ok = '<video poster=\"/foo.png\"></video>'\n    assert clean(ok, tags=tags, attributes=attrs) == ok\n\n\ndef test_attributes_callable():\n    \"\"\"Verify attributes can take a callable\"\"\"\n    ATTRS = lambda tag, name, val: name == 'title'\n    TAGS = ['a']\n\n    text = u'<a href=\"/foo\" title=\"blah\">example</a>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        u'<a title=\"blah\">example</a>'\n    )\n\n\ndef test_attributes_wildcard():\n    \"\"\"Verify attributes[*] works\"\"\"\n    ATTRS = {\n        '*': ['id'],\n        'img': ['src'],\n    }\n    TAGS = ['img', 'em']\n\n    text = 'both <em id=\"foo\" style=\"color: black\">can</em> have <img id=\"bar\" src=\"foo\"/>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        'both <em id=\"foo\">can</em> have <img id=\"bar\" src=\"foo\">'\n    )\n\n\ndef test_attributes_wildcard_callable():\n    \"\"\"Verify attributes[*] callable works\"\"\"\n    ATTRS = {\n        '*': lambda tag, name, val: name == 'title'\n    }\n    TAGS = ['a']\n\n    assert (\n        clean(u'<a href=\"/foo\" title=\"blah\">example</a>', tags=TAGS, attributes=ATTRS) ==\n        u'<a title=\"blah\">example</a>'\n    )\n\n\ndef test_attributes_tag_callable():\n    \"\"\"Verify attributes[tag] callable works\"\"\"\n    def img_test(tag, name, val):\n        return name == 'src' and val.startswith('https')\n\n    ATTRS = {\n        'img': img_test,\n    }\n    TAGS = ['img']\n\n    text = 'foo <img src=\"http://example.com\" alt=\"blah\"> baz'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        u'foo <img> baz'\n    )\n    text = 'foo <img src=\"https://example.com\" alt=\"blah\"> baz'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        u'foo <img src=\"https://example.com\"> baz'\n    )\n\n\ndef test_attributes_tag_list():\n    \"\"\"Verify attributes[tag] list works\"\"\"\n    ATTRS = {\n        'a': ['title']\n    }\n    TAGS = ['a']\n\n    assert (\n        clean(u'<a href=\"/foo\" title=\"blah\">example</a>', tags=TAGS, attributes=ATTRS) ==\n        u'<a title=\"blah\">example</a>'\n    )\n\n\ndef test_attributes_list():\n    \"\"\"Verify attributes list works\"\"\"\n    ATTRS = ['title']\n    TAGS = ['a']\n\n    text = u'<a href=\"/foo\" title=\"blah\">example</a>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        u'<a title=\"blah\">example</a>'\n    )\n\n\n@pytest.mark.parametrize('data, kwargs, expected', [\n    # javascript: is not allowed by default\n    (\n        '<a href=\"javascript:alert(\\'XSS\\')\">xss</a>',\n        {},\n        '<a>xss</a>'\n    ),\n\n    # File protocol is not allowed by default\n    (\n        '<a href=\"file:///tmp/foo\">foo</a>',\n        {},\n        '<a>foo</a>'\n    ),\n\n    # Specified protocols are allowed\n    (\n        '<a href=\"myprotocol://more_text\">allowed href</a>',\n        {'protocols': ['myprotocol']},\n        '<a href=\"myprotocol://more_text\">allowed href</a>'\n    ),\n\n    # Unspecified protocols are not allowed\n    (\n        '<a href=\"http://xx.com\">invalid href</a>',\n        {'protocols': ['myprotocol']},\n        '<a>invalid href</a>'\n    )\n])\ndef test_uri_value_allowed_protocols(data, kwargs, expected):\n    assert clean(data, **kwargs) == expected\n\n\ndef test_svg_attr_val_allows_ref():\n    \"\"\"Unescape values in svg attrs that allow url references\"\"\"\n    # Local IRI, so keep it\n    TAGS = ['svg', 'rect']\n    ATTRS = {\n        'rect': ['fill'],\n    }\n\n    text = '<svg><rect fill=\"url(#foo)\" /></svg>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        '<svg><rect fill=\"url(#foo)\"></rect></svg>'\n    )\n\n    # Non-local IRI, so drop it\n    TAGS = ['svg', 'rect']\n    ATTRS = {\n        'rect': ['fill'],\n    }\n    text = '<svg><rect fill=\"url(http://example.com#foo)\" /></svg>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        '<svg><rect></rect></svg>'\n    )\n\n\n@pytest.mark.parametrize('text, expected', [\n    (\n        '<svg><pattern id=\"patt1\" href=\"#patt2\"></pattern></svg>',\n        '<svg><pattern href=\"#patt2\" id=\"patt1\"></pattern></svg>'\n    ),\n    (\n        '<svg><pattern id=\"patt1\" xlink:href=\"#patt2\"></pattern></svg>',\n        # NOTE(willkg): Bug in html5lib serializer drops the xlink part\n        '<svg><pattern id=\"patt1\" href=\"#patt2\"></pattern></svg>'\n    ),\n])\ndef test_svg_allow_local_href(text, expected):\n    \"\"\"Keep local hrefs for svg elements\"\"\"\n    TAGS = ['svg', 'pattern']\n    ATTRS = {\n        'pattern': ['id', 'href'],\n    }\n    assert clean(text, tags=TAGS, attributes=ATTRS) == expected\n\n\n@pytest.mark.parametrize('text, expected', [\n    (\n        '<svg><pattern id=\"patt1\" href=\"https://example.com/patt\"></pattern></svg>',\n        '<svg><pattern id=\"patt1\"></pattern></svg>'\n    ),\n    (\n        '<svg><pattern id=\"patt1\" xlink:href=\"https://example.com/patt\"></pattern></svg>',\n        '<svg><pattern id=\"patt1\"></pattern></svg>'\n    ),\n])\ndef test_svg_allow_local_href_nonlocal(text, expected):\n    \"\"\"Drop non-local hrefs for svg elements\"\"\"\n    TAGS = ['svg', 'pattern']\n    ATTRS = {\n        'pattern': ['id', 'href'],\n    }\n    assert clean(text, tags=TAGS, attributes=ATTRS) == expected\n\n\ndef test_weird_strings():\n    s = '</3'\n    assert clean(s) == ''\n\n\n@pytest.mark.xfail(reason='regression from bleach 1.5')\ndef test_sarcasm():\n    \"\"\"Jokes should crash.<sarcasm/>\"\"\"\n    assert (\n        clean('Yeah right <sarcasm/>') ==\n        'Yeah right &lt;sarcasm/&gt;'\n    )\n\n\n@pytest.mark.parametrize('data, expected', [\n    # Convert bell\n    ('1\\a23', '1?23'),\n\n    # Convert backpsace\n    ('1\\b23', '1?23'),\n\n    # Convert formfeed\n    ('1\\v23', '1?23'),\n\n    # Convert vertical tab\n    ('1\\f23', '1?23'),\n\n    # Convert a bunch of characters in a string\n    ('import y\\bose\\bm\\bi\\bt\\be\\b', 'import y?ose?m?i?t?e?'),\n])\ndef test_invisible_characters(data, expected):\n    assert clean(data) == expected\n\n\ndef get_tests():\n    \"\"\"Retrieves regression tests from data/ directory\n\n    :returns: list of ``(filename, filedata)`` tuples\n\n    \"\"\"\n    datadir = os.path.join(os.path.dirname(__file__), 'data')\n    tests = [\n        os.path.join(datadir, fn) for fn in os.listdir(datadir)\n        if fn.endswith('.test')\n    ]\n    # Sort numerically which makes it easier to iterate through them\n    tests.sort(key=lambda x: int(os.path.basename(x).split('.', 1)[0]))\n\n    testcases = [\n        (fn, open(fn, 'r').read()) for fn in tests\n    ]\n\n    return testcases\n\n\n@pytest.mark.parametrize('fn, test_case', get_tests())\ndef test_regressions(fn, test_case):\n    \"\"\"Regression tests for clean so we can see if there are issues\"\"\"\n    test_data, expected = test_case.split('\\n--\\n')\n\n    # NOTE(willkg): This strips input and expected which makes it easier to\n    # maintain the files. If there comes a time when the input needs whitespace\n    # at the beginning or end, then we'll have to figure out something else.\n    test_data = test_data.strip()\n    expected = expected.strip()\n\n    assert clean(test_data) == expected\n\n\nclass TestCleaner:\n    def test_basics(self):\n        TAGS = ['span', 'br']\n        ATTRS = {'span': ['style']}\n\n        cleaner = Cleaner(tags=TAGS, attributes=ATTRS)\n\n        assert (\n            cleaner.clean('a <br/><span style=\"color:red\">test</span>') ==\n            'a <br><span style=\"\">test</span>'\n        )\n\n    def test_filters(self):\n        # Create a Filter that changes all the attr values to \"moo\"\n        class MooFilter(Filter):\n            def __iter__(self):\n                for token in Filter.__iter__(self):\n                    if token['type'] in ['StartTag', 'EmptyTag'] and token['data']:\n                        for attr, value in token['data'].items():\n                            token['data'][attr] = 'moo'\n\n                    yield token\n\n        ATTRS = {\n            'img': ['rel', 'src']\n        }\n        TAGS = ['img']\n\n        cleaner = Cleaner(tags=TAGS, attributes=ATTRS, filters=[MooFilter])\n\n        dirty = 'this is cute! <img src=\"http://example.com/puppy.jpg\" rel=\"nofollow\">'\n        assert (\n            cleaner.clean(dirty) ==\n            'this is cute! <img rel=\"moo\" src=\"moo\">'\n        )\n"], "fixing_code": ["from __future__ import unicode_literals\nfrom itertools import chain\nimport re\nimport string\n\nimport six\nfrom six.moves.urllib.parse import urlparse\nfrom xml.sax.saxutils import unescape\n\nimport html5lib\nfrom html5lib.constants import (\n    entities,\n    namespaces,\n    prefixes,\n    tokenTypes,\n)\ntry:\n    from html5lib.constants import ReparseException\nexcept ImportError:\n    # html5lib-python 1.0 changed the name\n    from html5lib.constants import _ReparseException as ReparseException\nfrom html5lib.filters.base import Filter\nfrom html5lib.filters import sanitizer\nfrom html5lib.serializer import HTMLSerializer\nfrom html5lib._tokenizer import HTMLTokenizer\nfrom html5lib._trie import Trie\n\nfrom bleach.utils import alphabetize_attributes, force_unicode\n\n\n#: Map of entity name to expanded entity\nENTITIES = entities\n\n#: Trie of html entity string -> character representation\nENTITIES_TRIE = Trie(ENTITIES)\n\n#: List of allowed tags\nALLOWED_TAGS = [\n    'a',\n    'abbr',\n    'acronym',\n    'b',\n    'blockquote',\n    'code',\n    'em',\n    'i',\n    'li',\n    'ol',\n    'strong',\n    'ul',\n]\n\n\n#: Map of allowed attributes by tag\nALLOWED_ATTRIBUTES = {\n    'a': ['href', 'title'],\n    'abbr': ['title'],\n    'acronym': ['title'],\n}\n\n\n#: List of allowed styles\nALLOWED_STYLES = []\n\n\n#: List of allowed protocols\nALLOWED_PROTOCOLS = ['http', 'https', 'mailto']\n\n\nAMP_SPLIT_RE = re.compile('(&)')\n\n#: Invisible characters--0 to and including 31 except 9 (tab), 10 (lf), and 13 (cr)\nINVISIBLE_CHARACTERS = ''.join([chr(c) for c in chain(range(0, 9), range(11, 13), range(14, 32))])\n\n#: Regexp for characters that are invisible\nINVISIBLE_CHARACTERS_RE = re.compile(\n    '[' + INVISIBLE_CHARACTERS + ']',\n    re.UNICODE\n)\n\n#: String to replace invisible characters with. This can be a character, a\n#: string, or even a function that takes a Python re matchobj\nINVISIBLE_REPLACEMENT_CHAR = '?'\n\n\ndef convert_entity(value):\n    \"\"\"Convert an entity (minus the & and ; part) into what it represents\n\n    This handles numeric, hex, and text entities.\n\n    :arg value: the string (minus the ``&`` and ``;`` part) to convert\n\n    :returns: unicode character\n\n    \"\"\"\n    if value[0] == '#':\n        if value[1] in ('x', 'X'):\n            return six.unichr(int(value[2:], 16))\n        return six.unichr(int(value[1:], 10))\n\n    return ENTITIES[value]\n\n\ndef convert_entities(text):\n    \"\"\"Converts all found entities in the text\n\n    :arg text: the text to convert entities in\n\n    :returns: unicode text with converted entities\n\n    \"\"\"\n    if '&' not in text:\n        return text\n\n    new_text = []\n    for part in next_possible_entity(text):\n        if not part:\n            continue\n\n        if part.startswith('&'):\n            entity = match_entity(part)\n            if entity is not None:\n                new_text.append(convert_entity(entity))\n                remainder = part[len(entity) + 2:]\n                if part:\n                    new_text.append(remainder)\n                continue\n\n        new_text.append(part)\n\n    return u''.join(new_text)\n\n\nclass BleachHTMLTokenizer(HTMLTokenizer):\n    def consumeEntity(self, allowedChar=None, fromAttribute=False):\n        # We don't want to consume and convert entities, so this overrides the\n        # html5lib tokenizer's consumeEntity so that it's now a no-op.\n        #\n        # However, when that gets called, it's consumed an &, so we put that in\n        # the stream.\n        if fromAttribute:\n            self.currentToken['data'][-1][1] += '&'\n\n        else:\n            self.tokenQueue.append({\"type\": tokenTypes['Characters'], \"data\": '&'})\n\n\nclass BleachHTMLParser(html5lib.HTMLParser):\n    def _parse(self, stream, innerHTML=False, container=\"div\", scripting=False, **kwargs):\n        # Override HTMLParser so we can swap out the tokenizer for our own.\n        self.innerHTMLMode = innerHTML\n        self.container = container\n        self.scripting = scripting\n        self.tokenizer = BleachHTMLTokenizer(stream, parser=self, **kwargs)\n        self.reset()\n\n        try:\n            self.mainLoop()\n        except ReparseException:\n            self.reset()\n            self.mainLoop()\n\n\nclass Cleaner(object):\n    \"\"\"Cleaner for cleaning HTML fragments of malicious content\n\n    This cleaner is a security-focused function whose sole purpose is to remove\n    malicious content from a string such that it can be displayed as content in\n    a web page.\n\n    This cleaner is not designed to use to transform content to be used in\n    non-web-page contexts.\n\n    To use::\n\n        from bleach.sanitizer import Cleaner\n\n        cleaner = Cleaner()\n\n        for text in all_the_yucky_things:\n            sanitized = cleaner.clean(text)\n\n    \"\"\"\n\n    def __init__(self, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES,\n                 styles=ALLOWED_STYLES, protocols=ALLOWED_PROTOCOLS, strip=False,\n                 strip_comments=True, filters=None):\n        \"\"\"Initializes a Cleaner\n\n        :arg list tags: allowed list of tags; defaults to\n            ``bleach.sanitizer.ALLOWED_TAGS``\n\n        :arg dict attributes: allowed attributes; can be a callable, list or dict;\n            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``\n\n        :arg list styles: allowed list of css styles; defaults to\n            ``bleach.sanitizer.ALLOWED_STYLES``\n\n        :arg list protocols: allowed list of protocols for links; defaults\n            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``\n\n        :arg bool strip: whether or not to strip disallowed elements\n\n        :arg bool strip_comments: whether or not to strip HTML comments\n\n        :arg list filters: list of html5lib Filter classes to pass streamed content through\n\n            .. seealso:: http://html5lib.readthedocs.io/en/latest/movingparts.html#filters\n\n            .. Warning::\n\n               Using filters changes the output of ``bleach.Cleaner.clean``.\n               Make sure the way the filters change the output are secure.\n\n        \"\"\"\n        self.tags = tags\n        self.attributes = attributes\n        self.styles = styles\n        self.protocols = protocols\n        self.strip = strip\n        self.strip_comments = strip_comments\n        self.filters = filters or []\n\n        self.parser = BleachHTMLParser(namespaceHTMLElements=False)\n        self.walker = html5lib.getTreeWalker('etree')\n        self.serializer = BleachHTMLSerializer(\n            quote_attr_values='always',\n            omit_optional_tags=False,\n            escape_lt_in_attrs=True,\n\n            # We want to leave entities as they are without escaping or\n            # resolving or expanding\n            resolve_entities=False,\n\n            # Bleach has its own sanitizer, so don't use the html5lib one\n            sanitize=False,\n\n            # Bleach sanitizer alphabetizes already, so don't use the html5lib one\n            alphabetical_attributes=False,\n        )\n\n    def clean(self, text):\n        \"\"\"Cleans text and returns sanitized result as unicode\n\n        :arg str text: text to be cleaned\n\n        :returns: sanitized text as unicode\n\n        :raises TypeError: if ``text`` is not a text type\n\n        \"\"\"\n        if not isinstance(text, six.string_types):\n            message = \"argument cannot be of '{name}' type, must be of text type\".format(\n                name=text.__class__.__name__)\n            raise TypeError(message)\n\n        if not text:\n            return u''\n\n        text = force_unicode(text)\n\n        dom = self.parser.parseFragment(text)\n        filtered = BleachSanitizerFilter(\n            source=self.walker(dom),\n\n            # Bleach-sanitizer-specific things\n            attributes=self.attributes,\n            strip_disallowed_elements=self.strip,\n            strip_html_comments=self.strip_comments,\n\n            # html5lib-sanitizer things\n            allowed_elements=self.tags,\n            allowed_css_properties=self.styles,\n            allowed_protocols=self.protocols,\n            allowed_svg_properties=[],\n        )\n\n        # Apply any filters after the BleachSanitizerFilter\n        for filter_class in self.filters:\n            filtered = filter_class(source=filtered)\n\n        return self.serializer.render(filtered)\n\n\ndef attribute_filter_factory(attributes):\n    \"\"\"Generates attribute filter function for the given attributes value\n\n    The attributes value can take one of several shapes. This returns a filter\n    function appropriate to the attributes value. One nice thing about this is\n    that there's less if/then shenanigans in the ``allow_token`` method.\n\n    \"\"\"\n    if callable(attributes):\n        return attributes\n\n    if isinstance(attributes, dict):\n        def _attr_filter(tag, attr, value):\n            if tag in attributes:\n                attr_val = attributes[tag]\n                if callable(attr_val):\n                    return attr_val(tag, attr, value)\n\n                if attr in attr_val:\n                    return True\n\n            if '*' in attributes:\n                attr_val = attributes['*']\n                if callable(attr_val):\n                    return attr_val(tag, attr, value)\n\n                return attr in attr_val\n\n            return False\n\n        return _attr_filter\n\n    if isinstance(attributes, list):\n        def _attr_filter(tag, attr, value):\n            return attr in attributes\n\n        return _attr_filter\n\n    raise ValueError('attributes needs to be a callable, a list or a dict')\n\n\ndef match_entity(stream):\n    \"\"\"Returns first entity in stream or None if no entity exists\n\n    Note: For Bleach purposes, entities must start with a \"&\" and end with\n    a \";\".\n\n    :arg stream: the character stream\n\n    :returns: ``None`` or the entity string without \"&\" or \";\"\n\n    \"\"\"\n    # Nix the & at the beginning\n    if stream[0] != '&':\n        raise ValueError('Stream should begin with \"&\"')\n\n    stream = stream[1:]\n\n    stream = list(stream)\n    possible_entity = ''\n    end_characters = '<&=;' + string.whitespace\n\n    # Handle number entities\n    if stream and stream[0] == '#':\n        possible_entity = '#'\n        stream.pop(0)\n\n        if stream and stream[0] in ('x', 'X'):\n            allowed = '0123456789abcdefABCDEF'\n            possible_entity += stream.pop(0)\n        else:\n            allowed = '0123456789'\n\n        # FIXME(willkg): Do we want to make sure these are valid number\n        # entities? This doesn't do that currently.\n        while stream and stream[0] not in end_characters:\n            c = stream.pop(0)\n            if c not in allowed:\n                break\n            possible_entity += c\n\n        if possible_entity and stream and stream[0] == ';':\n            return possible_entity\n        return None\n\n    # Handle character entities\n    while stream and stream[0] not in end_characters:\n        c = stream.pop(0)\n        if not ENTITIES_TRIE.has_keys_with_prefix(possible_entity):\n            break\n        possible_entity += c\n\n    if possible_entity and stream and stream[0] == ';':\n        return possible_entity\n\n    return None\n\n\ndef next_possible_entity(text):\n    \"\"\"Takes a text and generates a list of possible entities\n\n    :arg text: the text to look at\n\n    :returns: generator where each part (except the first) starts with an\n        \"&\"\n\n    \"\"\"\n    for i, part in enumerate(AMP_SPLIT_RE.split(text)):\n        if i == 0:\n            yield part\n        elif i % 2 == 0:\n            yield '&' + part\n\n\nclass BleachSanitizerFilter(sanitizer.Filter):\n    \"\"\"html5lib Filter that sanitizes text\n\n    This filter can be used anywhere html5lib filters can be used.\n\n    \"\"\"\n    def __init__(self, source, attributes=ALLOWED_ATTRIBUTES,\n                 strip_disallowed_elements=False, strip_html_comments=True,\n                 **kwargs):\n        \"\"\"Creates a BleachSanitizerFilter instance\n\n        :arg Treewalker source: stream\n\n        :arg list tags: allowed list of tags; defaults to\n            ``bleach.sanitizer.ALLOWED_TAGS``\n\n        :arg dict attributes: allowed attributes; can be a callable, list or dict;\n            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``\n\n        :arg list styles: allowed list of css styles; defaults to\n            ``bleach.sanitizer.ALLOWED_STYLES``\n\n        :arg list protocols: allowed list of protocols for links; defaults\n            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``\n\n        :arg bool strip_disallowed_elements: whether or not to strip disallowed\n            elements\n\n        :arg bool strip_html_comments: whether or not to strip HTML comments\n\n        \"\"\"\n        self.attr_filter = attribute_filter_factory(attributes)\n\n        self.strip_disallowed_elements = strip_disallowed_elements\n        self.strip_html_comments = strip_html_comments\n\n        return super(BleachSanitizerFilter, self).__init__(source, **kwargs)\n\n    def __iter__(self):\n        for token in Filter.__iter__(self):\n            ret = self.sanitize_token(token)\n\n            if not ret:\n                continue\n\n            if isinstance(ret, list):\n                for subtoken in ret:\n                    yield subtoken\n            else:\n                yield ret\n\n    def sanitize_token(self, token):\n        \"\"\"Sanitize a token either by HTML-encoding or dropping.\n\n        Unlike sanitizer.Filter, allowed_attributes can be a dict of {'tag':\n        ['attribute', 'pairs'], 'tag': callable}.\n\n        Here callable is a function with two arguments of attribute name and\n        value. It should return true of false.\n\n        Also gives the option to strip tags instead of encoding.\n\n        :arg dict token: token to sanitize\n\n        :returns: token or list of tokens\n\n        \"\"\"\n        token_type = token['type']\n        if token_type in ['StartTag', 'EndTag', 'EmptyTag']:\n            if token['name'] in self.allowed_elements:\n                return self.allow_token(token)\n\n            elif self.strip_disallowed_elements:\n                return None\n\n            else:\n                if 'data' in token:\n                    # Alphabetize the attributes before calling .disallowed_token()\n                    # so that the resulting string is stable\n                    token['data'] = alphabetize_attributes(token['data'])\n                return self.disallowed_token(token)\n\n        elif token_type == 'Comment':\n            if not self.strip_html_comments:\n                return token\n            else:\n                return None\n\n        elif token_type == 'Characters':\n            return self.sanitize_characters(token)\n\n        else:\n            return token\n\n    def sanitize_characters(self, token):\n        \"\"\"Handles Characters tokens\n\n        Our overridden tokenizer doesn't do anything with entities. However,\n        that means that the serializer will convert all ``&`` in Characters\n        tokens to ``&amp;``.\n\n        Since we don't want that, we extract entities here and convert them to\n        Entity tokens so the serializer will let them be.\n\n        :arg token: the Characters token to work on\n\n        :returns: a list of tokens\n\n        \"\"\"\n        data = token.get('data', '')\n\n        if not data:\n            return token\n\n        data = INVISIBLE_CHARACTERS_RE.sub(INVISIBLE_REPLACEMENT_CHAR, data)\n        token['data'] = data\n\n        # If there isn't a & in the data, we can return now\n        if '&' not in data:\n            return token\n\n        new_tokens = []\n\n        # For each possible entity that starts with a \"&\", we try to extract an\n        # actual entity and re-tokenize accordingly\n        for part in next_possible_entity(data):\n            if not part:\n                continue\n\n            if part.startswith('&'):\n                entity = match_entity(part)\n                if entity is not None:\n                    new_tokens.append({'type': 'Entity', 'name': entity})\n                    # Length of the entity plus 2--one for & at the beginning\n                    # and and one for ; at the end\n                    remainder = part[len(entity) + 2:]\n                    if remainder:\n                        new_tokens.append({'type': 'Characters', 'data': remainder})\n                    continue\n\n            new_tokens.append({'type': 'Characters', 'data': part})\n\n        return new_tokens\n\n    def sanitize_uri_value(self, value, allowed_protocols):\n        \"\"\"Checks a uri value to see if it's allowed\n\n        :arg value: the uri value to sanitize\n        :arg allowed_protocols: list of allowed protocols\n\n        :returns: allowed value or None\n\n        \"\"\"\n        # NOTE(willkg): This transforms the value into one that's easier to\n        # match and verify, but shouldn't get returned since it's vastly\n        # different than the original value.\n\n        # Convert all character entities in the value\n        new_value = convert_entities(value)\n\n        # Nix backtick, space characters, and control characters\n        new_value = re.sub(\n            \"[`\\000-\\040\\177-\\240\\s]+\",\n            '',\n            new_value\n        )\n\n        # Remove REPLACEMENT characters\n        new_value = new_value.replace('\\ufffd', '')\n\n        # Lowercase it--this breaks the value, but makes it easier to match\n        # against\n        new_value = new_value.lower()\n\n        # Drop attributes with uri values that have protocols that aren't\n        # allowed\n        parsed = urlparse(new_value)\n        if parsed.scheme:\n            # If urlparse found a scheme, check that\n            if parsed.scheme in allowed_protocols:\n                return value\n\n        else:\n            # Allow uris that are just an anchor\n            if new_value.startswith('#'):\n                return value\n\n            # Handle protocols that urlparse doesn't recognize like \"myprotocol\"\n            if ':' in new_value and new_value.split(':')[0] in allowed_protocols:\n                return value\n\n            # If there's no protocol/scheme specified, then assume it's \"http\"\n            # and see if that's allowed\n            if 'http' in allowed_protocols:\n                return value\n\n        return None\n\n    def allow_token(self, token):\n        \"\"\"Handles the case where we're allowing the tag\"\"\"\n        if 'data' in token:\n            # Loop through all the attributes and drop the ones that are not\n            # allowed, are unsafe or break other rules. Additionally, fix\n            # attribute values that need fixing.\n            #\n            # At the end of this loop, we have the final set of attributes\n            # we're keeping.\n            attrs = {}\n            for namespaced_name, val in token['data'].items():\n                namespace, name = namespaced_name\n\n                # Drop attributes that are not explicitly allowed\n                #\n                # NOTE(willkg): We pass in the attribute name--not a namespaced\n                # name.\n                if not self.attr_filter(token['name'], name, val):\n                    continue\n\n                # Drop attributes with uri values that use a disallowed protocol\n                # Sanitize attributes with uri values\n                if namespaced_name in self.attr_val_is_uri:\n                    new_value = self.sanitize_uri_value(val, self.allowed_protocols)\n                    if new_value is None:\n                        continue\n                    val = new_value\n\n                # Drop values in svg attrs with non-local IRIs\n                if namespaced_name in self.svg_attr_val_allows_ref:\n                    new_val = re.sub(r'url\\s*\\(\\s*[^#\\s][^)]+?\\)',\n                                     ' ',\n                                     unescape(val))\n                    new_val = new_val.strip()\n                    if not new_val:\n                        continue\n\n                    else:\n                        # Replace the val with the unescaped version because\n                        # it's a iri\n                        val = new_val\n\n                # Drop href and xlink:href attr for svg elements with non-local IRIs\n                if (None, token['name']) in self.svg_allow_local_href:\n                    if namespaced_name in [(None, 'href'), (namespaces['xlink'], 'href')]:\n                        if re.search(r'^\\s*[^#\\s]', val):\n                            continue\n\n                # If it's a style attribute, sanitize it\n                if namespaced_name == (None, u'style'):\n                    val = self.sanitize_css(val)\n\n                # At this point, we want to keep the attribute, so add it in\n                attrs[namespaced_name] = val\n\n            token['data'] = alphabetize_attributes(attrs)\n\n        return token\n\n    def disallowed_token(self, token):\n        token_type = token[\"type\"]\n        if token_type == \"EndTag\":\n            token[\"data\"] = \"</%s>\" % token[\"name\"]\n\n        elif token[\"data\"]:\n            assert token_type in (\"StartTag\", \"EmptyTag\")\n            attrs = []\n            for (ns, name), v in token[\"data\"].items():\n                attrs.append(' %s=\"%s\"' % (\n                    name if ns is None else \"%s:%s\" % (prefixes[ns], name),\n                    # NOTE(willkg): HTMLSerializer escapes attribute values\n                    # already, so if we do it here (like HTMLSerializer does),\n                    # then we end up double-escaping.\n                    v)\n                )\n            token[\"data\"] = \"<%s%s>\" % (token[\"name\"], ''.join(attrs))\n\n        else:\n            token[\"data\"] = \"<%s>\" % token[\"name\"]\n\n        if token.get(\"selfClosing\"):\n            token[\"data\"] = token[\"data\"][:-1] + \"/>\"\n\n        token[\"type\"] = \"Characters\"\n\n        del token[\"name\"]\n        return token\n\n    def sanitize_css(self, style):\n        \"\"\"Sanitizes css in style tags\"\"\"\n        # disallow urls\n        style = re.compile('url\\s*\\(\\s*[^\\s)]+?\\s*\\)\\s*').sub(' ', style)\n\n        # gauntlet\n\n        # Validate the css in the style tag and if it's not valid, then drop\n        # the whole thing.\n        parts = style.split(';')\n        gauntlet = re.compile(\n            r\"\"\"^([-/:,#%.'\"\\sa-zA-Z0-9!]|\\w-\\w|'[\\s\\w]+'\\s*|\"[\\s\\w]+\"|\\([\\d,%\\.\\s]+\\))*$\"\"\"\n        )\n\n        for part in parts:\n            if not gauntlet.match(part):\n                return ''\n\n        if not re.match(\"^\\s*([-\\w]+\\s*:[^:;]*(;\\s*|$))*$\", style):\n            return ''\n\n        clean = []\n        for prop, value in re.findall('([-\\w]+)\\s*:\\s*([^:;]*)', style):\n            if not value:\n                continue\n\n            if prop.lower() in self.allowed_css_properties:\n                clean.append(prop + ': ' + value + ';')\n\n            elif prop.lower() in self.allowed_svg_properties:\n                clean.append(prop + ': ' + value + ';')\n\n        return ' '.join(clean)\n\n\nclass BleachHTMLSerializer(HTMLSerializer):\n    \"\"\"Wraps the HTMLSerializer and undoes & -> &amp; in attributes\"\"\"\n    def escape_base_amp(self, stoken):\n        \"\"\"Escapes bare & in HTML attribute values\"\"\"\n        # First, undo what the HTMLSerializer did\n        stoken = stoken.replace('&amp;', '&')\n\n        # Then, escape any bare &\n        for part in next_possible_entity(stoken):\n            if not part:\n                continue\n\n            if part.startswith('&'):\n                entity = match_entity(part)\n                if entity is not None:\n                    yield '&' + entity + ';'\n\n                    # Length of the entity plus 2--one for & at the beginning\n                    # and and one for ; at the end\n                    part = part[len(entity) + 2:]\n                    if part:\n                        yield part\n                    continue\n\n            yield part.replace('&', '&amp;')\n\n    def serialize(self, treewalker, encoding=None):\n        \"\"\"Wrap HTMLSerializer.serialize and escape bare & in attributes\"\"\"\n        in_tag = False\n        after_equals = False\n\n        for stoken in super(BleachHTMLSerializer, self).serialize(treewalker, encoding):\n            if in_tag:\n                if stoken == '>':\n                    in_tag = False\n\n                elif after_equals:\n                    if stoken != '\"':\n                        for part in self.escape_base_amp(stoken):\n                            yield part\n\n                        after_equals = False\n                        continue\n\n                elif stoken == '=':\n                    after_equals = True\n\n                yield stoken\n            else:\n                if stoken.startswith('<'):\n                    in_tag = True\n                yield stoken\n", "import os\n\nfrom html5lib.filters.base import Filter\nimport pytest\n\nfrom bleach import clean\nfrom bleach.sanitizer import Cleaner\n\n\ndef test_clean_idempotent():\n    \"\"\"Make sure that applying the filter twice doesn't change anything.\"\"\"\n    dirty = '<span>invalid & </span> < extra http://link.com<em>'\n    assert clean(clean(dirty)) == clean(dirty)\n\n\ndef test_only_text_is_cleaned():\n    some_text = 'text'\n    some_type = int\n    no_type = None\n\n    assert clean(some_text) == some_text\n\n    with pytest.raises(TypeError) as e:\n        clean(some_type)\n    assert \"argument cannot be of 'type' type\" in str(e)\n\n    with pytest.raises(TypeError) as e:\n        clean(no_type)\n    assert \"NoneType\" in str(e)\n\n\ndef test_empty():\n    assert clean('') == ''\n\n\ndef test_content_has_no_html():\n    assert clean('no html string') == 'no html string'\n\n\n@pytest.mark.parametrize('data, expected', [\n    (\n        'an <strong>allowed</strong> tag',\n        'an <strong>allowed</strong> tag'\n    ),\n\n    (\n        'another <em>good</em> tag',\n        'another <em>good</em> tag'\n    )\n])\ndef test_content_has_allowed_html(data, expected):\n    assert clean(data) == expected\n\n\ndef test_html_is_lowercased():\n    assert (\n        clean('<A HREF=\"http://example.com\">foo</A>') ==\n        '<a href=\"http://example.com\">foo</a>'\n    )\n\n\n@pytest.mark.parametrize('data, should_strip, expected', [\n    # Regular comment\n    (\n        '<!-- this is a comment -->',\n        True,\n        ''\n    ),\n\n    # Open comment with no close comment bit\n    (\n        '<!-- open comment',\n        True,\n        ''\n    ),\n    (\n        '<!--open comment',\n        True,\n        ''\n    ),\n    (\n        '<!-- open comment',\n        False,\n        '<!-- open comment-->'\n    ),\n    (\n        '<!--open comment',\n        False,\n        '<!--open comment-->'\n    ),\n\n    # Comment with text to the right\n    (\n        '<!-- comment -->text',\n        True,\n        'text'\n    ),\n    (\n        '<!--comment-->text',\n        True,\n        'text'\n    ),\n    (\n        '<!-- comment -->text',\n        False,\n        '<!-- comment -->text'\n    ),\n    (\n        '<!--comment-->text',\n        False,\n        '<!--comment-->text'\n    ),\n\n    # Comment with text to the left\n    (\n        'text<!-- comment -->',\n        True,\n        'text'\n    ),\n    (\n        'text<!--comment-->',\n        True,\n        'text'\n    ),\n    (\n        'text<!-- comment -->',\n        False,\n        'text<!-- comment -->'\n    ),\n    (\n        'text<!--comment-->',\n        False,\n        'text<!--comment-->'\n    )\n])\ndef test_comments(data, should_strip, expected):\n    assert clean(data, strip_comments=should_strip) == expected\n\n\n@pytest.mark.parametrize('data, expected', [\n    # Disallowed tag is escaped\n    ('<img src=\"javascript:alert(\\'XSS\\');\">', '&lt;img src=\"javascript:alert(\\'XSS\\');\"&gt;'),\n\n    # Test with parens\n    ('a <script>safe()</script> test', 'a &lt;script&gt;safe()&lt;/script&gt; test'),\n\n    # Test with braces\n    ('a <style>body{}</style> test', 'a &lt;style&gt;body{}&lt;/style&gt; test'),\n])\ndef test_disallowed_tags(data, expected):\n    assert clean(data) == expected\n\n\ndef test_invalid_char_in_tag():\n    # NOTE(willkg): Two possible outcomes because attrs aren't ordered\n    assert (\n        clean('<script/xss src=\"http://xx.com/xss.js\"></script>') in\n        [\n            '&lt;script src=\"http://xx.com/xss.js\" xss=\"\"&gt;&lt;/script&gt;',\n            '&lt;script xss=\"\" src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n        ]\n    )\n    assert (\n        clean('<script/src=\"http://xx.com/xss.js\"></script>') ==\n        '&lt;script src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n    )\n\n\ndef test_unclosed_tag():\n    assert (\n        clean('a <em>fixed tag') ==\n        'a <em>fixed tag</em>'\n    )\n    assert (\n        clean('<script src=http://xx.com/xss.js<b>') ==\n        '&lt;script src=\"http://xx.com/xss.js&lt;b\"&gt;&lt;/script&gt;'\n    )\n    # NOTE(willkg): Two possible outcomes because attrs aren't ordered\n    assert (\n        clean('<script src=\"http://xx.com/xss.js\"<b>') in\n        [\n            '&lt;script src=\"http://xx.com/xss.js\" &lt;b=\"\"&gt;&lt;/script&gt;',\n            '&lt;script &lt;b=\"\" src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n        ]\n    )\n    # NOTE(willkg): Two possible outcomes because attrs aren't ordered\n    assert (\n        clean('<script src=\"http://xx.com/xss.js\" <b>') in\n        [\n            '&lt;script src=\"http://xx.com/xss.js\" &lt;b=\"\"&gt;&lt;/script&gt;',\n            '&lt;script &lt;b=\"\" src=\"http://xx.com/xss.js\"&gt;&lt;/script&gt;'\n        ]\n    )\n\n\ndef test_nested_script_tag():\n    assert (\n        clean('<<script>script>evil()<</script>/script>') ==\n        '&lt;&lt;script&gt;script&gt;evil()&lt;&lt;/script&gt;/script&gt;'\n    )\n    assert (\n        clean('<<x>script>evil()<</x>/script>') ==\n        '&lt;&lt;x&gt;script&gt;evil()&lt;&lt;/x&gt;/script&gt;'\n    )\n    assert (\n        clean('<script<script>>evil()</script</script>>') ==\n        '&lt;script&lt;script&gt;&gt;evil()&gt;&lt;/script&lt;script&gt;'\n    )\n\n\n@pytest.mark.parametrize('text, expected', [\n    ('an & entity', 'an &amp; entity'),\n    ('an < entity', 'an &lt; entity'),\n    ('tag < <em>and</em> entity', 'tag &lt; <em>and</em> entity'),\n])\ndef test_bare_entities_get_escaped_correctly(text, expected):\n    assert clean(text) == expected\n\n\n@pytest.mark.parametrize('text, expected', [\n    # Test character entities\n    ('&amp;', '&amp;'),\n    ('&nbsp;', '&nbsp;'),\n    ('&nbsp; test string &nbsp;', '&nbsp; test string &nbsp;'),\n    ('&lt;em&gt;strong&lt;/em&gt;', '&lt;em&gt;strong&lt;/em&gt;'),\n\n    # Test character entity at beginning of string\n    ('&amp;is cool', '&amp;is cool'),\n\n    # Test it at the end of the string\n    ('cool &amp;', 'cool &amp;'),\n\n    # Test bare ampersands and entities at beginning\n    ('&&amp; is cool', '&amp;&amp; is cool'),\n\n    # Test entities and bare ampersand at end\n    ('&amp; is cool &amp;&', '&amp; is cool &amp;&amp;'),\n\n    # Test missing semi-colon means we don't treat it like an entity\n    ('this &amp that', 'this &amp;amp that'),\n\n    # Test a thing that looks like a character entity, but isn't because it's\n    # missing a ; (&curren)\n    (\n        'http://example.com?active=true&current=true',\n        'http://example.com?active=true&amp;current=true'\n    ),\n\n    # Test entities in HTML attributes\n    (\n        '<a href=\"?art&amp;copy\">foo</a>',\n        '<a href=\"?art&amp;copy\">foo</a>'\n    ),\n    (\n        '<a href=\"?this=&gt;that\">foo</a>',\n        '<a href=\"?this=&gt;that\">foo</a>'\n    ),\n    (\n        '<a href=\"http://example.com?active=true&current=true\">foo</a>',\n        '<a href=\"http://example.com?active=true&amp;current=true\">foo</a>'\n    ),\n\n    # Test numeric entities\n    ('&#39;', '&#39;'),\n    ('&#34;', '&#34;'),\n    ('&#123;', '&#123;'),\n    ('&#x0007b;', '&#x0007b;'),\n    ('&#x0007B;', '&#x0007B;'),\n\n    # Test non-numeric entities\n    ('&#', '&amp;#'),\n    ('&#<', '&amp;#&lt;'),\n\n    # html5lib tokenizer unescapes character entities, so these would become '\n    # and \" which makes it possible to break out of html attributes.\n    #\n    # Verify that clean() doesn't unescape entities.\n    ('&#39;&#34;', '&#39;&#34;'),\n])\ndef test_character_entities_handling(text, expected):\n    assert clean(text) == expected\n\n\n@pytest.mark.parametrize('data, kwargs, expected', [\n    # All tags are allowed, so it strips nothing\n    (\n        'a test <em>with</em> <b>html</b> tags',\n        {'strip': True},\n        'a test <em>with</em> <b>html</b> tags'\n    ),\n\n    # img tag is disallowed, so it's stripped\n    (\n        'a test <em>with</em> <img src=\"http://example.com/\"> <b>html</b> tags',\n        {'strip': True},\n        'a test <em>with</em>  <b>html</b> tags'\n    ),\n\n    # a tag is disallowed, so it's stripped\n    (\n        '<p><a href=\"http://example.com/\">link text</a></p>',\n        {'tags': ['p'], 'strip': True},\n        '<p>link text</p>'\n    ),\n\n    # handle nested disallowed tag\n    (\n        '<p><span>multiply <span>nested <span>text</span></span></span></p>',\n        {'tags': ['p'], 'strip': True},\n        '<p>multiply nested text</p>'\n    ),\n\n    # handle disallowed tag that's deep in the tree\n    (\n        '<p><a href=\"http://example.com/\"><img src=\"http://example.com/\"></a></p>',\n        {'tags': ['a', 'p'], 'strip': True},\n        '<p><a href=\"http://example.com/\"></a></p>'\n    ),\n])\ndef test_stripping_tags(data, kwargs, expected):\n    assert clean(data, **kwargs) == expected\n\n\n@pytest.mark.parametrize('data, expected', [\n    (\n        '<scri<script>pt>alert(1)</scr</script>ipt>',\n        'pt&gt;alert(1)ipt&gt;'\n    ),\n    (\n        '<scri<scri<script>pt>pt>alert(1)</script>',\n        'pt&gt;pt&gt;alert(1)'\n    ),\n])\ndef test_stripping_tags_is_safe(data, expected):\n    \"\"\"Test stripping tags shouldn't result in malicious content\"\"\"\n    assert clean(data, strip=True) == expected\n\n\ndef test_allowed_styles():\n    \"\"\"Test allowed styles\"\"\"\n    ATTRS = ['style']\n    STYLE = ['color']\n\n    assert (\n        clean('<b style=\"top:0\"></b>', attributes=ATTRS) ==\n        '<b style=\"\"></b>'\n    )\n\n    text = '<b style=\"color: blue;\"></b>'\n    assert clean(text, attributes=ATTRS, styles=STYLE) == text\n\n    text = '<b style=\"top: 0; color: blue;\"></b>'\n    assert (\n        clean(text, attributes=ATTRS, styles=STYLE) ==\n        '<b style=\"color: blue;\"></b>'\n    )\n\n\ndef test_href_with_wrong_tag():\n    assert (\n        clean('<em href=\"fail\">no link</em>') ==\n        '<em>no link</em>'\n    )\n\n\ndef test_disallowed_attr():\n    IMG = ['img', ]\n    IMG_ATTR = ['src']\n\n    assert (\n        clean('<a onclick=\"evil\" href=\"test\">test</a>') ==\n        '<a href=\"test\">test</a>'\n    )\n    assert (\n        clean('<img onclick=\"evil\" src=\"test\" />', tags=IMG, attributes=IMG_ATTR) ==\n        '<img src=\"test\">'\n    )\n    assert (\n        clean('<img href=\"invalid\" src=\"test\" />', tags=IMG, attributes=IMG_ATTR) ==\n        '<img src=\"test\">'\n    )\n\n\ndef test_unquoted_attr_values_are_quoted():\n    assert (\n        clean('<abbr title=mytitle>myabbr</abbr>') ==\n        '<abbr title=\"mytitle\">myabbr</abbr>'\n    )\n\n\ndef test_unquoted_event_handler_attr_value():\n    assert (\n        clean('<a href=\"http://xx.com\" onclick=foo()>xx.com</a>') ==\n        '<a href=\"http://xx.com\">xx.com</a>'\n    )\n\n\ndef test_invalid_filter_attr():\n    IMG = ['img', ]\n    IMG_ATTR = {\n        'img': lambda tag, name, val: name == 'src' and val == \"http://example.com/\"\n    }\n\n    assert (\n        clean('<img onclick=\"evil\" src=\"http://example.com/\" />', tags=IMG, attributes=IMG_ATTR) ==\n        '<img src=\"http://example.com/\">'\n    )\n    assert (\n        clean('<img onclick=\"evil\" src=\"http://badhost.com/\" />', tags=IMG, attributes=IMG_ATTR) ==\n        '<img>'\n    )\n\n\ndef test_poster_attribute():\n    \"\"\"Poster attributes should not allow javascript.\"\"\"\n    tags = ['video']\n    attrs = {'video': ['poster']}\n\n    test = '<video poster=\"javascript:alert(1)\"></video>'\n    assert clean(test, tags=tags, attributes=attrs) == '<video></video>'\n\n    ok = '<video poster=\"/foo.png\"></video>'\n    assert clean(ok, tags=tags, attributes=attrs) == ok\n\n\ndef test_attributes_callable():\n    \"\"\"Verify attributes can take a callable\"\"\"\n    ATTRS = lambda tag, name, val: name == 'title'\n    TAGS = ['a']\n\n    text = u'<a href=\"/foo\" title=\"blah\">example</a>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        u'<a title=\"blah\">example</a>'\n    )\n\n\ndef test_attributes_wildcard():\n    \"\"\"Verify attributes[*] works\"\"\"\n    ATTRS = {\n        '*': ['id'],\n        'img': ['src'],\n    }\n    TAGS = ['img', 'em']\n\n    text = 'both <em id=\"foo\" style=\"color: black\">can</em> have <img id=\"bar\" src=\"foo\"/>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        'both <em id=\"foo\">can</em> have <img id=\"bar\" src=\"foo\">'\n    )\n\n\ndef test_attributes_wildcard_callable():\n    \"\"\"Verify attributes[*] callable works\"\"\"\n    ATTRS = {\n        '*': lambda tag, name, val: name == 'title'\n    }\n    TAGS = ['a']\n\n    assert (\n        clean(u'<a href=\"/foo\" title=\"blah\">example</a>', tags=TAGS, attributes=ATTRS) ==\n        u'<a title=\"blah\">example</a>'\n    )\n\n\ndef test_attributes_tag_callable():\n    \"\"\"Verify attributes[tag] callable works\"\"\"\n    def img_test(tag, name, val):\n        return name == 'src' and val.startswith('https')\n\n    ATTRS = {\n        'img': img_test,\n    }\n    TAGS = ['img']\n\n    text = 'foo <img src=\"http://example.com\" alt=\"blah\"> baz'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        u'foo <img> baz'\n    )\n    text = 'foo <img src=\"https://example.com\" alt=\"blah\"> baz'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        u'foo <img src=\"https://example.com\"> baz'\n    )\n\n\ndef test_attributes_tag_list():\n    \"\"\"Verify attributes[tag] list works\"\"\"\n    ATTRS = {\n        'a': ['title']\n    }\n    TAGS = ['a']\n\n    assert (\n        clean(u'<a href=\"/foo\" title=\"blah\">example</a>', tags=TAGS, attributes=ATTRS) ==\n        u'<a title=\"blah\">example</a>'\n    )\n\n\ndef test_attributes_list():\n    \"\"\"Verify attributes list works\"\"\"\n    ATTRS = ['title']\n    TAGS = ['a']\n\n    text = u'<a href=\"/foo\" title=\"blah\">example</a>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        u'<a title=\"blah\">example</a>'\n    )\n\n\n@pytest.mark.parametrize('data, kwargs, expected', [\n    # javascript: is not allowed by default\n    (\n        '<a href=\"javascript:alert(\\'XSS\\')\">xss</a>',\n        {},\n        '<a>xss</a>'\n    ),\n\n    # File protocol is not allowed by default\n    (\n        '<a href=\"file:///tmp/foo\">foo</a>',\n        {},\n        '<a>foo</a>'\n    ),\n\n    # Specified protocols are allowed\n    (\n        '<a href=\"myprotocol://more_text\">allowed href</a>',\n        {'protocols': ['myprotocol']},\n        '<a href=\"myprotocol://more_text\">allowed href</a>'\n    ),\n\n    # Unspecified protocols are not allowed\n    (\n        '<a href=\"http://example.com\">invalid href</a>',\n        {'protocols': ['myprotocol']},\n        '<a>invalid href</a>'\n    ),\n\n    # Anchors are ok\n    (\n        '<a href=\"#example.com\">foo</a>',\n        {'protocols': []},\n        '<a href=\"#example.com\">foo</a>'\n    ),\n\n    # Allow implicit http if allowed\n    (\n        '<a href=\"example.com\">valid</a>',\n        {'protocols': ['http']},\n        '<a href=\"example.com\">valid</a>'\n    ),\n    (\n        '<a href=\"example.com:8000\">valid</a>',\n        {'protocols': ['http']},\n        '<a href=\"example.com:8000\">valid</a>'\n    ),\n    (\n        '<a href=\"localhost\">valid</a>',\n        {'protocols': ['http']},\n        '<a href=\"localhost\">valid</a>'\n    ),\n    (\n        '<a href=\"localhost:8000\">valid</a>',\n        {'protocols': ['http']},\n        '<a href=\"localhost:8000\">valid</a>'\n    ),\n    (\n        '<a href=\"192.168.100.100\">valid</a>',\n        {'protocols': ['http']},\n        '<a href=\"192.168.100.100\">valid</a>'\n    ),\n    (\n        '<a href=\"192.168.100.100:8000\">valid</a>',\n        {'protocols': ['http']},\n        '<a href=\"192.168.100.100:8000\">valid</a>'\n    ),\n\n    # Disallow implicit http if disallowed\n    (\n        '<a href=\"example.com\">foo</a>',\n        {'protocols': []},\n        '<a>foo</a>'\n    ),\n    (\n        '<a href=\"example.com:8000\">foo</a>',\n        {'protocols': []},\n        '<a>foo</a>'\n    ),\n    (\n        '<a href=\"localhost\">foo</a>',\n        {'protocols': []},\n        '<a>foo</a>'\n    ),\n    (\n        '<a href=\"localhost:8000\">foo</a>',\n        {'protocols': []},\n        '<a>foo</a>'\n    ),\n    (\n        '<a href=\"192.168.100.100\">foo</a>',\n        {'protocols': []},\n        '<a>foo</a>'\n    ),\n    (\n        '<a href=\"192.168.100.100:8000\">foo</a>',\n        {'protocols': []},\n        '<a>foo</a>'\n    ),\n\n    # Disallowed protocols with sneaky character entities\n    (\n        '<a href=\"javas&#x09;cript:alert(1)\">alert</a>',\n        {},\n        '<a>alert</a>'\n    ),\n    (\n        '<a href=\"&#14;javascript:alert(1)\">alert</a>',\n        {},\n        '<a>alert</a>'\n    ),\n\n    # Checking the uri should change it at all\n    (\n        '<a href=\"http://example.com/?foo&nbsp;bar\">foo</a>',\n        {},\n        '<a href=\"http://example.com/?foo&nbsp;bar\">foo</a>'\n    ),\n])\ndef test_uri_value_allowed_protocols(data, kwargs, expected):\n    assert clean(data, **kwargs) == expected\n\n\ndef test_svg_attr_val_allows_ref():\n    \"\"\"Unescape values in svg attrs that allow url references\"\"\"\n    # Local IRI, so keep it\n    TAGS = ['svg', 'rect']\n    ATTRS = {\n        'rect': ['fill'],\n    }\n\n    text = '<svg><rect fill=\"url(#foo)\" /></svg>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        '<svg><rect fill=\"url(#foo)\"></rect></svg>'\n    )\n\n    # Non-local IRI, so drop it\n    TAGS = ['svg', 'rect']\n    ATTRS = {\n        'rect': ['fill'],\n    }\n    text = '<svg><rect fill=\"url(http://example.com#foo)\" /></svg>'\n    assert (\n        clean(text, tags=TAGS, attributes=ATTRS) ==\n        '<svg><rect></rect></svg>'\n    )\n\n\n@pytest.mark.parametrize('text, expected', [\n    (\n        '<svg><pattern id=\"patt1\" href=\"#patt2\"></pattern></svg>',\n        '<svg><pattern href=\"#patt2\" id=\"patt1\"></pattern></svg>'\n    ),\n    (\n        '<svg><pattern id=\"patt1\" xlink:href=\"#patt2\"></pattern></svg>',\n        # NOTE(willkg): Bug in html5lib serializer drops the xlink part\n        '<svg><pattern id=\"patt1\" href=\"#patt2\"></pattern></svg>'\n    ),\n])\ndef test_svg_allow_local_href(text, expected):\n    \"\"\"Keep local hrefs for svg elements\"\"\"\n    TAGS = ['svg', 'pattern']\n    ATTRS = {\n        'pattern': ['id', 'href'],\n    }\n    assert clean(text, tags=TAGS, attributes=ATTRS) == expected\n\n\n@pytest.mark.parametrize('text, expected', [\n    (\n        '<svg><pattern id=\"patt1\" href=\"https://example.com/patt\"></pattern></svg>',\n        '<svg><pattern id=\"patt1\"></pattern></svg>'\n    ),\n    (\n        '<svg><pattern id=\"patt1\" xlink:href=\"https://example.com/patt\"></pattern></svg>',\n        '<svg><pattern id=\"patt1\"></pattern></svg>'\n    ),\n])\ndef test_svg_allow_local_href_nonlocal(text, expected):\n    \"\"\"Drop non-local hrefs for svg elements\"\"\"\n    TAGS = ['svg', 'pattern']\n    ATTRS = {\n        'pattern': ['id', 'href'],\n    }\n    assert clean(text, tags=TAGS, attributes=ATTRS) == expected\n\n\ndef test_weird_strings():\n    s = '</3'\n    assert clean(s) == ''\n\n\n@pytest.mark.xfail(reason='regression from bleach 1.5')\ndef test_sarcasm():\n    \"\"\"Jokes should crash.<sarcasm/>\"\"\"\n    assert (\n        clean('Yeah right <sarcasm/>') ==\n        'Yeah right &lt;sarcasm/&gt;'\n    )\n\n\n@pytest.mark.parametrize('data, expected', [\n    # Convert bell\n    ('1\\a23', '1?23'),\n\n    # Convert backpsace\n    ('1\\b23', '1?23'),\n\n    # Convert formfeed\n    ('1\\v23', '1?23'),\n\n    # Convert vertical tab\n    ('1\\f23', '1?23'),\n\n    # Convert a bunch of characters in a string\n    ('import y\\bose\\bm\\bi\\bt\\be\\b', 'import y?ose?m?i?t?e?'),\n])\ndef test_invisible_characters(data, expected):\n    assert clean(data) == expected\n\n\ndef get_tests():\n    \"\"\"Retrieves regression tests from data/ directory\n\n    :returns: list of ``(filename, filedata)`` tuples\n\n    \"\"\"\n    datadir = os.path.join(os.path.dirname(__file__), 'data')\n    tests = [\n        os.path.join(datadir, fn) for fn in os.listdir(datadir)\n        if fn.endswith('.test')\n    ]\n    # Sort numerically which makes it easier to iterate through them\n    tests.sort(key=lambda x: int(os.path.basename(x).split('.', 1)[0]))\n\n    testcases = [\n        (fn, open(fn, 'r').read()) for fn in tests\n    ]\n\n    return testcases\n\n\n@pytest.mark.parametrize('fn, test_case', get_tests())\ndef test_regressions(fn, test_case):\n    \"\"\"Regression tests for clean so we can see if there are issues\"\"\"\n    test_data, expected = test_case.split('\\n--\\n')\n\n    # NOTE(willkg): This strips input and expected which makes it easier to\n    # maintain the files. If there comes a time when the input needs whitespace\n    # at the beginning or end, then we'll have to figure out something else.\n    test_data = test_data.strip()\n    expected = expected.strip()\n\n    assert clean(test_data) == expected\n\n\nclass TestCleaner:\n    def test_basics(self):\n        TAGS = ['span', 'br']\n        ATTRS = {'span': ['style']}\n\n        cleaner = Cleaner(tags=TAGS, attributes=ATTRS)\n\n        assert (\n            cleaner.clean('a <br/><span style=\"color:red\">test</span>') ==\n            'a <br><span style=\"\">test</span>'\n        )\n\n    def test_filters(self):\n        # Create a Filter that changes all the attr values to \"moo\"\n        class MooFilter(Filter):\n            def __iter__(self):\n                for token in Filter.__iter__(self):\n                    if token['type'] in ['StartTag', 'EmptyTag'] and token['data']:\n                        for attr, value in token['data'].items():\n                            token['data'][attr] = 'moo'\n\n                    yield token\n\n        ATTRS = {\n            'img': ['rel', 'src']\n        }\n        TAGS = ['img']\n\n        cleaner = Cleaner(tags=TAGS, attributes=ATTRS, filters=[MooFilter])\n\n        dirty = 'this is cute! <img src=\"http://example.com/puppy.jpg\" rel=\"nofollow\">'\n        assert (\n            cleaner.clean(dirty) ==\n            'this is cute! <img rel=\"moo\" src=\"moo\">'\n        )\n"], "filenames": ["bleach/sanitizer.py", "tests/test_clean.py"], "buggy_code_start_loc": [6, 216], "buggy_code_end_loc": [525, 541], "fixing_code_start_loc": [7, 216], "fixing_code_end_loc": [624, 631], "type": "CWE-20", "message": "An issue was discovered in Bleach 2.1.x before 2.1.3. Attributes that have URI values weren't properly sanitized if the values contained character entities. Using character entities, it was possible to construct a URI value with a scheme that was not allowed that would slide through unsanitized.", "other": {"cve": {"id": "CVE-2018-7753", "sourceIdentifier": "cve@mitre.org", "published": "2018-03-07T23:29:00.273", "lastModified": "2018-03-29T13:50:45.870", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "An issue was discovered in Bleach 2.1.x before 2.1.3. Attributes that have URI values weren't properly sanitized if the values contained character entities. Using character entities, it was possible to construct a URI value with a scheme that was not allowed that would slide through unsanitized."}, {"lang": "es", "value": "Se ha descubierto un problema en Bleach, en versiones 2.1.x anteriores a la 2.1.3. Los atributos que tienen valores URI no se sanearon correctamente si los valores conten\u00edan entidades de caracteres. Mediante el uso de entidades de caracteres, era posible construir un valor de URI con un esquema no permitido que pasar\u00eda sin sanearse."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 9.8, "baseSeverity": "CRITICAL"}, "exploitabilityScore": 3.9, "impactScore": 5.9}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:P/A:P", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "PARTIAL", "availabilityImpact": "PARTIAL", "baseScore": 7.5}, "baseSeverity": "HIGH", "exploitabilityScore": 10.0, "impactScore": 6.4, "acInsufInfo": true, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-20"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:mozilla:bleach:2.1:*:*:*:*:*:*:*", "matchCriteriaId": "31CB79CB-359C-4F1A-8B0E-AA0A108C0A3F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mozilla:bleach:2.1.1:*:*:*:*:*:*:*", "matchCriteriaId": "6A61E9E7-3E4F-4D3C-8D48-D726EDC9D0DF"}, {"vulnerable": true, "criteria": "cpe:2.3:a:mozilla:bleach:2.1.2:*:*:*:*:*:*:*", "matchCriteriaId": "2CE79483-67BD-474D-BF05-CD8AF8A2A1AB"}]}]}], "references": [{"url": "https://bugs.debian.org/892252", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/mozilla/bleach/commit/c5df5789ec3471a31311f42c2d19fc2cf21b35ef", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/mozilla/bleach/releases/tag/v2.1.3", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}]}, "github_commit_url": "https://github.com/mozilla/bleach/commit/c5df5789ec3471a31311f42c2d19fc2cf21b35ef"}}