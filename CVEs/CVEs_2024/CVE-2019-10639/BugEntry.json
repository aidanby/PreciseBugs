{"buggy_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Operations on the network namespace\n */\n#ifndef __NET_NET_NAMESPACE_H\n#define __NET_NET_NAMESPACE_H\n\n#include <linux/atomic.h>\n#include <linux/refcount.h>\n#include <linux/workqueue.h>\n#include <linux/list.h>\n#include <linux/sysctl.h>\n#include <linux/uidgid.h>\n\n#include <net/flow.h>\n#include <net/netns/core.h>\n#include <net/netns/mib.h>\n#include <net/netns/unix.h>\n#include <net/netns/packet.h>\n#include <net/netns/ipv4.h>\n#include <net/netns/ipv6.h>\n#include <net/netns/ieee802154_6lowpan.h>\n#include <net/netns/sctp.h>\n#include <net/netns/dccp.h>\n#include <net/netns/netfilter.h>\n#include <net/netns/x_tables.h>\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n#include <net/netns/conntrack.h>\n#endif\n#include <net/netns/nftables.h>\n#include <net/netns/xfrm.h>\n#include <net/netns/mpls.h>\n#include <net/netns/can.h>\n#include <net/netns/xdp.h>\n#include <linux/ns_common.h>\n#include <linux/idr.h>\n#include <linux/skbuff.h>\n\nstruct user_namespace;\nstruct proc_dir_entry;\nstruct net_device;\nstruct sock;\nstruct ctl_table_header;\nstruct net_generic;\nstruct uevent_sock;\nstruct netns_ipvs;\nstruct bpf_prog;\n\n\n#define NETDEV_HASHBITS    8\n#define NETDEV_HASHENTRIES (1 << NETDEV_HASHBITS)\n\nstruct net {\n\trefcount_t\t\tpassive;\t/* To decided when the network\n\t\t\t\t\t\t * namespace should be freed.\n\t\t\t\t\t\t */\n\trefcount_t\t\tcount;\t\t/* To decided when the network\n\t\t\t\t\t\t *  namespace should be shut down.\n\t\t\t\t\t\t */\n\tspinlock_t\t\trules_mod_lock;\n\n\tatomic64_t\t\tcookie_gen;\n\n\tstruct list_head\tlist;\t\t/* list of network namespaces */\n\tstruct list_head\texit_list;\t/* To linked to call pernet exit\n\t\t\t\t\t\t * methods on dead net (\n\t\t\t\t\t\t * pernet_ops_rwsem read locked),\n\t\t\t\t\t\t * or to unregister pernet ops\n\t\t\t\t\t\t * (pernet_ops_rwsem write locked).\n\t\t\t\t\t\t */\n\tstruct llist_node\tcleanup_list;\t/* namespaces on death row */\n\n\tstruct user_namespace   *user_ns;\t/* Owning user namespace */\n\tstruct ucounts\t\t*ucounts;\n\tspinlock_t\t\tnsid_lock;\n\tstruct idr\t\tnetns_ids;\n\n\tstruct ns_common\tns;\n\n\tstruct proc_dir_entry \t*proc_net;\n\tstruct proc_dir_entry \t*proc_net_stat;\n\n#ifdef CONFIG_SYSCTL\n\tstruct ctl_table_set\tsysctls;\n#endif\n\n\tstruct sock \t\t*rtnl;\t\t\t/* rtnetlink socket */\n\tstruct sock\t\t*genl_sock;\n\n\tstruct uevent_sock\t*uevent_sock;\t\t/* uevent socket */\n\n\tstruct list_head \tdev_base_head;\n\tstruct hlist_head \t*dev_name_head;\n\tstruct hlist_head\t*dev_index_head;\n\tunsigned int\t\tdev_base_seq;\t/* protected by rtnl_mutex */\n\tint\t\t\tifindex;\n\tunsigned int\t\tdev_unreg_count;\n\n\t/* core fib_rules */\n\tstruct list_head\trules_ops;\n\n\tstruct list_head\tfib_notifier_ops;  /* Populated by\n\t\t\t\t\t\t    * register_pernet_subsys()\n\t\t\t\t\t\t    */\n\tstruct net_device       *loopback_dev;          /* The loopback */\n\tstruct netns_core\tcore;\n\tstruct netns_mib\tmib;\n\tstruct netns_packet\tpacket;\n\tstruct netns_unix\tunx;\n\tstruct netns_ipv4\tipv4;\n#if IS_ENABLED(CONFIG_IPV6)\n\tstruct netns_ipv6\tipv6;\n#endif\n#if IS_ENABLED(CONFIG_IEEE802154_6LOWPAN)\n\tstruct netns_ieee802154_lowpan\tieee802154_lowpan;\n#endif\n#if defined(CONFIG_IP_SCTP) || defined(CONFIG_IP_SCTP_MODULE)\n\tstruct netns_sctp\tsctp;\n#endif\n#if defined(CONFIG_IP_DCCP) || defined(CONFIG_IP_DCCP_MODULE)\n\tstruct netns_dccp\tdccp;\n#endif\n#ifdef CONFIG_NETFILTER\n\tstruct netns_nf\t\tnf;\n\tstruct netns_xt\t\txt;\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tstruct netns_ct\t\tct;\n#endif\n#if defined(CONFIG_NF_TABLES) || defined(CONFIG_NF_TABLES_MODULE)\n\tstruct netns_nftables\tnft;\n#endif\n#if IS_ENABLED(CONFIG_NF_DEFRAG_IPV6)\n\tstruct netns_nf_frag\tnf_frag;\n\tstruct ctl_table_header *nf_frag_frags_hdr;\n#endif\n\tstruct sock\t\t*nfnl;\n\tstruct sock\t\t*nfnl_stash;\n#if IS_ENABLED(CONFIG_NETFILTER_NETLINK_ACCT)\n\tstruct list_head        nfnl_acct_list;\n#endif\n#if IS_ENABLED(CONFIG_NF_CT_NETLINK_TIMEOUT)\n\tstruct list_head\tnfct_timeout_list;\n#endif\n#endif\n#ifdef CONFIG_WEXT_CORE\n\tstruct sk_buff_head\twext_nlevents;\n#endif\n\tstruct net_generic __rcu\t*gen;\n\n\tstruct bpf_prog __rcu\t*flow_dissector_prog;\n\n\t/* Note : following structs are cache line aligned */\n#ifdef CONFIG_XFRM\n\tstruct netns_xfrm\txfrm;\n#endif\n#if IS_ENABLED(CONFIG_IP_VS)\n\tstruct netns_ipvs\t*ipvs;\n#endif\n#if IS_ENABLED(CONFIG_MPLS)\n\tstruct netns_mpls\tmpls;\n#endif\n#if IS_ENABLED(CONFIG_CAN)\n\tstruct netns_can\tcan;\n#endif\n#ifdef CONFIG_XDP_SOCKETS\n\tstruct netns_xdp\txdp;\n#endif\n\tstruct sock\t\t*diag_nlsk;\n\tatomic_t\t\tfnhe_genid;\n} __randomize_layout;\n\n#include <linux/seq_file_net.h>\n\n/* Init's network namespace */\nextern struct net init_net;\n\n#ifdef CONFIG_NET_NS\nstruct net *copy_net_ns(unsigned long flags, struct user_namespace *user_ns,\n\t\t\tstruct net *old_net);\n\nvoid net_ns_get_ownership(const struct net *net, kuid_t *uid, kgid_t *gid);\n\nvoid net_ns_barrier(void);\n#else /* CONFIG_NET_NS */\n#include <linux/sched.h>\n#include <linux/nsproxy.h>\nstatic inline struct net *copy_net_ns(unsigned long flags,\n\tstruct user_namespace *user_ns, struct net *old_net)\n{\n\tif (flags & CLONE_NEWNET)\n\t\treturn ERR_PTR(-EINVAL);\n\treturn old_net;\n}\n\nstatic inline void net_ns_get_ownership(const struct net *net,\n\t\t\t\t\tkuid_t *uid, kgid_t *gid)\n{\n\t*uid = GLOBAL_ROOT_UID;\n\t*gid = GLOBAL_ROOT_GID;\n}\n\nstatic inline void net_ns_barrier(void) {}\n#endif /* CONFIG_NET_NS */\n\n\nextern struct list_head net_namespace_list;\n\nstruct net *get_net_ns_by_pid(pid_t pid);\nstruct net *get_net_ns_by_fd(int fd);\n\n#ifdef CONFIG_SYSCTL\nvoid ipx_register_sysctl(void);\nvoid ipx_unregister_sysctl(void);\n#else\n#define ipx_register_sysctl()\n#define ipx_unregister_sysctl()\n#endif\n\n#ifdef CONFIG_NET_NS\nvoid __put_net(struct net *net);\n\nstatic inline struct net *get_net(struct net *net)\n{\n\trefcount_inc(&net->count);\n\treturn net;\n}\n\nstatic inline struct net *maybe_get_net(struct net *net)\n{\n\t/* Used when we know struct net exists but we\n\t * aren't guaranteed a previous reference count\n\t * exists.  If the reference count is zero this\n\t * function fails and returns NULL.\n\t */\n\tif (!refcount_inc_not_zero(&net->count))\n\t\tnet = NULL;\n\treturn net;\n}\n\nstatic inline void put_net(struct net *net)\n{\n\tif (refcount_dec_and_test(&net->count))\n\t\t__put_net(net);\n}\n\nstatic inline\nint net_eq(const struct net *net1, const struct net *net2)\n{\n\treturn net1 == net2;\n}\n\nstatic inline int check_net(const struct net *net)\n{\n\treturn refcount_read(&net->count) != 0;\n}\n\nvoid net_drop_ns(void *);\n\n#else\n\nstatic inline struct net *get_net(struct net *net)\n{\n\treturn net;\n}\n\nstatic inline void put_net(struct net *net)\n{\n}\n\nstatic inline struct net *maybe_get_net(struct net *net)\n{\n\treturn net;\n}\n\nstatic inline\nint net_eq(const struct net *net1, const struct net *net2)\n{\n\treturn 1;\n}\n\nstatic inline int check_net(const struct net *net)\n{\n\treturn 1;\n}\n\n#define net_drop_ns NULL\n#endif\n\n\ntypedef struct {\n#ifdef CONFIG_NET_NS\n\tstruct net *net;\n#endif\n} possible_net_t;\n\nstatic inline void write_pnet(possible_net_t *pnet, struct net *net)\n{\n#ifdef CONFIG_NET_NS\n\tpnet->net = net;\n#endif\n}\n\nstatic inline struct net *read_pnet(const possible_net_t *pnet)\n{\n#ifdef CONFIG_NET_NS\n\treturn pnet->net;\n#else\n\treturn &init_net;\n#endif\n}\n\n/* Protected by net_rwsem */\n#define for_each_net(VAR)\t\t\t\t\\\n\tlist_for_each_entry(VAR, &net_namespace_list, list)\n\n#define for_each_net_rcu(VAR)\t\t\t\t\\\n\tlist_for_each_entry_rcu(VAR, &net_namespace_list, list)\n\n#ifdef CONFIG_NET_NS\n#define __net_init\n#define __net_exit\n#define __net_initdata\n#define __net_initconst\n#else\n#define __net_init\t__init\n#define __net_exit\t__ref\n#define __net_initdata\t__initdata\n#define __net_initconst\t__initconst\n#endif\n\nint peernet2id_alloc(struct net *net, struct net *peer);\nint peernet2id(struct net *net, struct net *peer);\nbool peernet_has_id(struct net *net, struct net *peer);\nstruct net *get_net_ns_by_id(struct net *net, int id);\n\nstruct pernet_operations {\n\tstruct list_head list;\n\t/*\n\t * Below methods are called without any exclusive locks.\n\t * More than one net may be constructed and destructed\n\t * in parallel on several cpus. Every pernet_operations\n\t * have to keep in mind all other pernet_operations and\n\t * to introduce a locking, if they share common resources.\n\t *\n\t * The only time they are called with exclusive lock is\n\t * from register_pernet_subsys(), unregister_pernet_subsys()\n\t * register_pernet_device() and unregister_pernet_device().\n\t *\n\t * Exit methods using blocking RCU primitives, such as\n\t * synchronize_rcu(), should be implemented via exit_batch.\n\t * Then, destruction of a group of net requires single\n\t * synchronize_rcu() related to these pernet_operations,\n\t * instead of separate synchronize_rcu() for every net.\n\t * Please, avoid synchronize_rcu() at all, where it's possible.\n\t */\n\tint (*init)(struct net *net);\n\tvoid (*exit)(struct net *net);\n\tvoid (*exit_batch)(struct list_head *net_exit_list);\n\tunsigned int *id;\n\tsize_t size;\n};\n\n/*\n * Use these carefully.  If you implement a network device and it\n * needs per network namespace operations use device pernet operations,\n * otherwise use pernet subsys operations.\n *\n * Network interfaces need to be removed from a dying netns _before_\n * subsys notifiers can be called, as most of the network code cleanup\n * (which is done from subsys notifiers) runs with the assumption that\n * dev_remove_pack has been called so no new packets will arrive during\n * and after the cleanup functions have been called.  dev_remove_pack\n * is not per namespace so instead the guarantee of no more packets\n * arriving in a network namespace is provided by ensuring that all\n * network devices and all sockets have left the network namespace\n * before the cleanup methods are called.\n *\n * For the longest time the ipv4 icmp code was registered as a pernet\n * device which caused kernel oops, and panics during network\n * namespace cleanup.   So please don't get this wrong.\n */\nint register_pernet_subsys(struct pernet_operations *);\nvoid unregister_pernet_subsys(struct pernet_operations *);\nint register_pernet_device(struct pernet_operations *);\nvoid unregister_pernet_device(struct pernet_operations *);\n\nstruct ctl_table;\nstruct ctl_table_header;\n\n#ifdef CONFIG_SYSCTL\nint net_sysctl_init(void);\nstruct ctl_table_header *register_net_sysctl(struct net *net, const char *path,\n\t\t\t\t\t     struct ctl_table *table);\nvoid unregister_net_sysctl_table(struct ctl_table_header *header);\n#else\nstatic inline int net_sysctl_init(void) { return 0; }\nstatic inline struct ctl_table_header *register_net_sysctl(struct net *net,\n\tconst char *path, struct ctl_table *table)\n{\n\treturn NULL;\n}\nstatic inline void unregister_net_sysctl_table(struct ctl_table_header *header)\n{\n}\n#endif\n\nstatic inline int rt_genid_ipv4(struct net *net)\n{\n\treturn atomic_read(&net->ipv4.rt_genid);\n}\n\nstatic inline void rt_genid_bump_ipv4(struct net *net)\n{\n\tatomic_inc(&net->ipv4.rt_genid);\n}\n\nextern void (*__fib6_flush_trees)(struct net *net);\nstatic inline void rt_genid_bump_ipv6(struct net *net)\n{\n\tif (__fib6_flush_trees)\n\t\t__fib6_flush_trees(net);\n}\n\n#if IS_ENABLED(CONFIG_IEEE802154_6LOWPAN)\nstatic inline struct netns_ieee802154_lowpan *\nnet_ieee802154_lowpan(struct net *net)\n{\n\treturn &net->ieee802154_lowpan;\n}\n#endif\n\n/* For callers who don't really care about whether it's IPv4 or IPv6 */\nstatic inline void rt_genid_bump_all(struct net *net)\n{\n\trt_genid_bump_ipv4(net);\n\trt_genid_bump_ipv6(net);\n}\n\nstatic inline int fnhe_genid(struct net *net)\n{\n\treturn atomic_read(&net->fnhe_genid);\n}\n\nstatic inline void fnhe_genid_bump(struct net *net)\n{\n\tatomic_inc(&net->fnhe_genid);\n}\n\n#endif /* __NET_NET_NAMESPACE_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __NET_NS_HASH_H__\n#define __NET_NS_HASH_H__\n\n#include <asm/cache.h>\n\nstruct net;\n\nstatic inline u32 net_hash_mix(const struct net *net)\n{\n#ifdef CONFIG_NET_NS\n\treturn (u32)(((unsigned long)net) >> ilog2(sizeof(*net)));\n#else\n\treturn 0;\n#endif\n}\n#endif\n", "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/workqueue.h>\n#include <linux/rtnetlink.h>\n#include <linux/cache.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/delay.h>\n#include <linux/sched.h>\n#include <linux/idr.h>\n#include <linux/rculist.h>\n#include <linux/nsproxy.h>\n#include <linux/fs.h>\n#include <linux/proc_ns.h>\n#include <linux/file.h>\n#include <linux/export.h>\n#include <linux/user_namespace.h>\n#include <linux/net_namespace.h>\n#include <linux/sched/task.h>\n#include <linux/uidgid.h>\n\n#include <net/sock.h>\n#include <net/netlink.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n\n/*\n *\tOur network namespace constructor/destructor lists\n */\n\nstatic LIST_HEAD(pernet_list);\nstatic struct list_head *first_device = &pernet_list;\n\nLIST_HEAD(net_namespace_list);\nEXPORT_SYMBOL_GPL(net_namespace_list);\n\n/* Protects net_namespace_list. Nests iside rtnl_lock() */\nDECLARE_RWSEM(net_rwsem);\nEXPORT_SYMBOL_GPL(net_rwsem);\n\nstruct net init_net = {\n\t.count\t\t= REFCOUNT_INIT(1),\n\t.dev_base_head\t= LIST_HEAD_INIT(init_net.dev_base_head),\n};\nEXPORT_SYMBOL(init_net);\n\nstatic bool init_net_initialized;\n/*\n * pernet_ops_rwsem: protects: pernet_list, net_generic_ids,\n * init_net_initialized and first_device pointer.\n * This is internal net namespace object. Please, don't use it\n * outside.\n */\nDECLARE_RWSEM(pernet_ops_rwsem);\nEXPORT_SYMBOL_GPL(pernet_ops_rwsem);\n\n#define MIN_PERNET_OPS_ID\t\\\n\t((sizeof(struct net_generic) + sizeof(void *) - 1) / sizeof(void *))\n\n#define INITIAL_NET_GEN_PTRS\t13 /* +1 for len +2 for rcu_head */\n\nstatic unsigned int max_gen_ptrs = INITIAL_NET_GEN_PTRS;\n\nstatic struct net_generic *net_alloc_generic(void)\n{\n\tstruct net_generic *ng;\n\tunsigned int generic_size = offsetof(struct net_generic, ptr[max_gen_ptrs]);\n\n\tng = kzalloc(generic_size, GFP_KERNEL);\n\tif (ng)\n\t\tng->s.len = max_gen_ptrs;\n\n\treturn ng;\n}\n\nstatic int net_assign_generic(struct net *net, unsigned int id, void *data)\n{\n\tstruct net_generic *ng, *old_ng;\n\n\tBUG_ON(id < MIN_PERNET_OPS_ID);\n\n\told_ng = rcu_dereference_protected(net->gen,\n\t\t\t\t\t   lockdep_is_held(&pernet_ops_rwsem));\n\tif (old_ng->s.len > id) {\n\t\told_ng->ptr[id] = data;\n\t\treturn 0;\n\t}\n\n\tng = net_alloc_generic();\n\tif (ng == NULL)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Some synchronisation notes:\n\t *\n\t * The net_generic explores the net->gen array inside rcu\n\t * read section. Besides once set the net->gen->ptr[x]\n\t * pointer never changes (see rules in netns/generic.h).\n\t *\n\t * That said, we simply duplicate this array and schedule\n\t * the old copy for kfree after a grace period.\n\t */\n\n\tmemcpy(&ng->ptr[MIN_PERNET_OPS_ID], &old_ng->ptr[MIN_PERNET_OPS_ID],\n\t       (old_ng->s.len - MIN_PERNET_OPS_ID) * sizeof(void *));\n\tng->ptr[id] = data;\n\n\trcu_assign_pointer(net->gen, ng);\n\tkfree_rcu(old_ng, s.rcu);\n\treturn 0;\n}\n\nstatic int ops_init(const struct pernet_operations *ops, struct net *net)\n{\n\tint err = -ENOMEM;\n\tvoid *data = NULL;\n\n\tif (ops->id && ops->size) {\n\t\tdata = kzalloc(ops->size, GFP_KERNEL);\n\t\tif (!data)\n\t\t\tgoto out;\n\n\t\terr = net_assign_generic(net, *ops->id, data);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t}\n\terr = 0;\n\tif (ops->init)\n\t\terr = ops->init(net);\n\tif (!err)\n\t\treturn 0;\n\ncleanup:\n\tkfree(data);\n\nout:\n\treturn err;\n}\n\nstatic void ops_free(const struct pernet_operations *ops, struct net *net)\n{\n\tif (ops->id && ops->size) {\n\t\tkfree(net_generic(net, *ops->id));\n\t}\n}\n\nstatic void ops_exit_list(const struct pernet_operations *ops,\n\t\t\t  struct list_head *net_exit_list)\n{\n\tstruct net *net;\n\tif (ops->exit) {\n\t\tlist_for_each_entry(net, net_exit_list, exit_list)\n\t\t\tops->exit(net);\n\t}\n\tif (ops->exit_batch)\n\t\tops->exit_batch(net_exit_list);\n}\n\nstatic void ops_free_list(const struct pernet_operations *ops,\n\t\t\t  struct list_head *net_exit_list)\n{\n\tstruct net *net;\n\tif (ops->size && ops->id) {\n\t\tlist_for_each_entry(net, net_exit_list, exit_list)\n\t\t\tops_free(ops, net);\n\t}\n}\n\n/* should be called with nsid_lock held */\nstatic int alloc_netid(struct net *net, struct net *peer, int reqid)\n{\n\tint min = 0, max = 0;\n\n\tif (reqid >= 0) {\n\t\tmin = reqid;\n\t\tmax = reqid + 1;\n\t}\n\n\treturn idr_alloc(&net->netns_ids, peer, min, max, GFP_ATOMIC);\n}\n\n/* This function is used by idr_for_each(). If net is equal to peer, the\n * function returns the id so that idr_for_each() stops. Because we cannot\n * returns the id 0 (idr_for_each() will not stop), we return the magic value\n * NET_ID_ZERO (-1) for it.\n */\n#define NET_ID_ZERO -1\nstatic int net_eq_idr(int id, void *net, void *peer)\n{\n\tif (net_eq(net, peer))\n\t\treturn id ? : NET_ID_ZERO;\n\treturn 0;\n}\n\n/* Should be called with nsid_lock held. If a new id is assigned, the bool alloc\n * is set to true, thus the caller knows that the new id must be notified via\n * rtnl.\n */\nstatic int __peernet2id_alloc(struct net *net, struct net *peer, bool *alloc)\n{\n\tint id = idr_for_each(&net->netns_ids, net_eq_idr, peer);\n\tbool alloc_it = *alloc;\n\n\t*alloc = false;\n\n\t/* Magic value for id 0. */\n\tif (id == NET_ID_ZERO)\n\t\treturn 0;\n\tif (id > 0)\n\t\treturn id;\n\n\tif (alloc_it) {\n\t\tid = alloc_netid(net, peer, -1);\n\t\t*alloc = true;\n\t\treturn id >= 0 ? id : NETNSA_NSID_NOT_ASSIGNED;\n\t}\n\n\treturn NETNSA_NSID_NOT_ASSIGNED;\n}\n\n/* should be called with nsid_lock held */\nstatic int __peernet2id(struct net *net, struct net *peer)\n{\n\tbool no = false;\n\n\treturn __peernet2id_alloc(net, peer, &no);\n}\n\nstatic void rtnl_net_notifyid(struct net *net, int cmd, int id);\n/* This function returns the id of a peer netns. If no id is assigned, one will\n * be allocated and returned.\n */\nint peernet2id_alloc(struct net *net, struct net *peer)\n{\n\tbool alloc = false, alive = false;\n\tint id;\n\n\tif (refcount_read(&net->count) == 0)\n\t\treturn NETNSA_NSID_NOT_ASSIGNED;\n\tspin_lock_bh(&net->nsid_lock);\n\t/*\n\t * When peer is obtained from RCU lists, we may race with\n\t * its cleanup. Check whether it's alive, and this guarantees\n\t * we never hash a peer back to net->netns_ids, after it has\n\t * just been idr_remove()'d from there in cleanup_net().\n\t */\n\tif (maybe_get_net(peer))\n\t\talive = alloc = true;\n\tid = __peernet2id_alloc(net, peer, &alloc);\n\tspin_unlock_bh(&net->nsid_lock);\n\tif (alloc && id >= 0)\n\t\trtnl_net_notifyid(net, RTM_NEWNSID, id);\n\tif (alive)\n\t\tput_net(peer);\n\treturn id;\n}\nEXPORT_SYMBOL_GPL(peernet2id_alloc);\n\n/* This function returns, if assigned, the id of a peer netns. */\nint peernet2id(struct net *net, struct net *peer)\n{\n\tint id;\n\n\tspin_lock_bh(&net->nsid_lock);\n\tid = __peernet2id(net, peer);\n\tspin_unlock_bh(&net->nsid_lock);\n\treturn id;\n}\nEXPORT_SYMBOL(peernet2id);\n\n/* This function returns true is the peer netns has an id assigned into the\n * current netns.\n */\nbool peernet_has_id(struct net *net, struct net *peer)\n{\n\treturn peernet2id(net, peer) >= 0;\n}\n\nstruct net *get_net_ns_by_id(struct net *net, int id)\n{\n\tstruct net *peer;\n\n\tif (id < 0)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tpeer = idr_find(&net->netns_ids, id);\n\tif (peer)\n\t\tpeer = maybe_get_net(peer);\n\trcu_read_unlock();\n\n\treturn peer;\n}\n\n/*\n * setup_net runs the initializers for the network namespace object.\n */\nstatic __net_init int setup_net(struct net *net, struct user_namespace *user_ns)\n{\n\t/* Must be called with pernet_ops_rwsem held */\n\tconst struct pernet_operations *ops, *saved_ops;\n\tint error = 0;\n\tLIST_HEAD(net_exit_list);\n\n\trefcount_set(&net->count, 1);\n\trefcount_set(&net->passive, 1);\n\tnet->dev_base_seq = 1;\n\tnet->user_ns = user_ns;\n\tidr_init(&net->netns_ids);\n\tspin_lock_init(&net->nsid_lock);\n\tmutex_init(&net->ipv4.ra_mutex);\n\n\tlist_for_each_entry(ops, &pernet_list, list) {\n\t\terror = ops_init(ops, net);\n\t\tif (error < 0)\n\t\t\tgoto out_undo;\n\t}\n\tdown_write(&net_rwsem);\n\tlist_add_tail_rcu(&net->list, &net_namespace_list);\n\tup_write(&net_rwsem);\nout:\n\treturn error;\n\nout_undo:\n\t/* Walk through the list backwards calling the exit functions\n\t * for the pernet modules whose init functions did not fail.\n\t */\n\tlist_add(&net->exit_list, &net_exit_list);\n\tsaved_ops = ops;\n\tlist_for_each_entry_continue_reverse(ops, &pernet_list, list)\n\t\tops_exit_list(ops, &net_exit_list);\n\n\tops = saved_ops;\n\tlist_for_each_entry_continue_reverse(ops, &pernet_list, list)\n\t\tops_free_list(ops, &net_exit_list);\n\n\trcu_barrier();\n\tgoto out;\n}\n\nstatic int __net_init net_defaults_init_net(struct net *net)\n{\n\tnet->core.sysctl_somaxconn = SOMAXCONN;\n\treturn 0;\n}\n\nstatic struct pernet_operations net_defaults_ops = {\n\t.init = net_defaults_init_net,\n};\n\nstatic __init int net_defaults_init(void)\n{\n\tif (register_pernet_subsys(&net_defaults_ops))\n\t\tpanic(\"Cannot initialize net default settings\");\n\n\treturn 0;\n}\n\ncore_initcall(net_defaults_init);\n\n#ifdef CONFIG_NET_NS\nstatic struct ucounts *inc_net_namespaces(struct user_namespace *ns)\n{\n\treturn inc_ucount(ns, current_euid(), UCOUNT_NET_NAMESPACES);\n}\n\nstatic void dec_net_namespaces(struct ucounts *ucounts)\n{\n\tdec_ucount(ucounts, UCOUNT_NET_NAMESPACES);\n}\n\nstatic struct kmem_cache *net_cachep __ro_after_init;\nstatic struct workqueue_struct *netns_wq;\n\nstatic struct net *net_alloc(void)\n{\n\tstruct net *net = NULL;\n\tstruct net_generic *ng;\n\n\tng = net_alloc_generic();\n\tif (!ng)\n\t\tgoto out;\n\n\tnet = kmem_cache_zalloc(net_cachep, GFP_KERNEL);\n\tif (!net)\n\t\tgoto out_free;\n\n\trcu_assign_pointer(net->gen, ng);\nout:\n\treturn net;\n\nout_free:\n\tkfree(ng);\n\tgoto out;\n}\n\nstatic void net_free(struct net *net)\n{\n\tkfree(rcu_access_pointer(net->gen));\n\tkmem_cache_free(net_cachep, net);\n}\n\nvoid net_drop_ns(void *p)\n{\n\tstruct net *ns = p;\n\tif (ns && refcount_dec_and_test(&ns->passive))\n\t\tnet_free(ns);\n}\n\nstruct net *copy_net_ns(unsigned long flags,\n\t\t\tstruct user_namespace *user_ns, struct net *old_net)\n{\n\tstruct ucounts *ucounts;\n\tstruct net *net;\n\tint rv;\n\n\tif (!(flags & CLONE_NEWNET))\n\t\treturn get_net(old_net);\n\n\tucounts = inc_net_namespaces(user_ns);\n\tif (!ucounts)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tnet = net_alloc();\n\tif (!net) {\n\t\trv = -ENOMEM;\n\t\tgoto dec_ucounts;\n\t}\n\trefcount_set(&net->passive, 1);\n\tnet->ucounts = ucounts;\n\tget_user_ns(user_ns);\n\n\trv = down_read_killable(&pernet_ops_rwsem);\n\tif (rv < 0)\n\t\tgoto put_userns;\n\n\trv = setup_net(net, user_ns);\n\n\tup_read(&pernet_ops_rwsem);\n\n\tif (rv < 0) {\nput_userns:\n\t\tput_user_ns(user_ns);\n\t\tnet_drop_ns(net);\ndec_ucounts:\n\t\tdec_net_namespaces(ucounts);\n\t\treturn ERR_PTR(rv);\n\t}\n\treturn net;\n}\n\n/**\n * net_ns_get_ownership - get sysfs ownership data for @net\n * @net: network namespace in question (can be NULL)\n * @uid: kernel user ID for sysfs objects\n * @gid: kernel group ID for sysfs objects\n *\n * Returns the uid/gid pair of root in the user namespace associated with the\n * given network namespace.\n */\nvoid net_ns_get_ownership(const struct net *net, kuid_t *uid, kgid_t *gid)\n{\n\tif (net) {\n\t\tkuid_t ns_root_uid = make_kuid(net->user_ns, 0);\n\t\tkgid_t ns_root_gid = make_kgid(net->user_ns, 0);\n\n\t\tif (uid_valid(ns_root_uid))\n\t\t\t*uid = ns_root_uid;\n\n\t\tif (gid_valid(ns_root_gid))\n\t\t\t*gid = ns_root_gid;\n\t} else {\n\t\t*uid = GLOBAL_ROOT_UID;\n\t\t*gid = GLOBAL_ROOT_GID;\n\t}\n}\nEXPORT_SYMBOL_GPL(net_ns_get_ownership);\n\nstatic void unhash_nsid(struct net *net, struct net *last)\n{\n\tstruct net *tmp;\n\t/* This function is only called from cleanup_net() work,\n\t * and this work is the only process, that may delete\n\t * a net from net_namespace_list. So, when the below\n\t * is executing, the list may only grow. Thus, we do not\n\t * use for_each_net_rcu() or net_rwsem.\n\t */\n\tfor_each_net(tmp) {\n\t\tint id;\n\n\t\tspin_lock_bh(&tmp->nsid_lock);\n\t\tid = __peernet2id(tmp, net);\n\t\tif (id >= 0)\n\t\t\tidr_remove(&tmp->netns_ids, id);\n\t\tspin_unlock_bh(&tmp->nsid_lock);\n\t\tif (id >= 0)\n\t\t\trtnl_net_notifyid(tmp, RTM_DELNSID, id);\n\t\tif (tmp == last)\n\t\t\tbreak;\n\t}\n\tspin_lock_bh(&net->nsid_lock);\n\tidr_destroy(&net->netns_ids);\n\tspin_unlock_bh(&net->nsid_lock);\n}\n\nstatic LLIST_HEAD(cleanup_list);\n\nstatic void cleanup_net(struct work_struct *work)\n{\n\tconst struct pernet_operations *ops;\n\tstruct net *net, *tmp, *last;\n\tstruct llist_node *net_kill_list;\n\tLIST_HEAD(net_exit_list);\n\n\t/* Atomically snapshot the list of namespaces to cleanup */\n\tnet_kill_list = llist_del_all(&cleanup_list);\n\n\tdown_read(&pernet_ops_rwsem);\n\n\t/* Don't let anyone else find us. */\n\tdown_write(&net_rwsem);\n\tllist_for_each_entry(net, net_kill_list, cleanup_list)\n\t\tlist_del_rcu(&net->list);\n\t/* Cache last net. After we unlock rtnl, no one new net\n\t * added to net_namespace_list can assign nsid pointer\n\t * to a net from net_kill_list (see peernet2id_alloc()).\n\t * So, we skip them in unhash_nsid().\n\t *\n\t * Note, that unhash_nsid() does not delete nsid links\n\t * between net_kill_list's nets, as they've already\n\t * deleted from net_namespace_list. But, this would be\n\t * useless anyway, as netns_ids are destroyed there.\n\t */\n\tlast = list_last_entry(&net_namespace_list, struct net, list);\n\tup_write(&net_rwsem);\n\n\tllist_for_each_entry(net, net_kill_list, cleanup_list) {\n\t\tunhash_nsid(net, last);\n\t\tlist_add_tail(&net->exit_list, &net_exit_list);\n\t}\n\n\t/*\n\t * Another CPU might be rcu-iterating the list, wait for it.\n\t * This needs to be before calling the exit() notifiers, so\n\t * the rcu_barrier() below isn't sufficient alone.\n\t */\n\tsynchronize_rcu();\n\n\t/* Run all of the network namespace exit methods */\n\tlist_for_each_entry_reverse(ops, &pernet_list, list)\n\t\tops_exit_list(ops, &net_exit_list);\n\n\t/* Free the net generic variables */\n\tlist_for_each_entry_reverse(ops, &pernet_list, list)\n\t\tops_free_list(ops, &net_exit_list);\n\n\tup_read(&pernet_ops_rwsem);\n\n\t/* Ensure there are no outstanding rcu callbacks using this\n\t * network namespace.\n\t */\n\trcu_barrier();\n\n\t/* Finally it is safe to free my network namespace structure */\n\tlist_for_each_entry_safe(net, tmp, &net_exit_list, exit_list) {\n\t\tlist_del_init(&net->exit_list);\n\t\tdec_net_namespaces(net->ucounts);\n\t\tput_user_ns(net->user_ns);\n\t\tnet_drop_ns(net);\n\t}\n}\n\n/**\n * net_ns_barrier - wait until concurrent net_cleanup_work is done\n *\n * cleanup_net runs from work queue and will first remove namespaces\n * from the global list, then run net exit functions.\n *\n * Call this in module exit path to make sure that all netns\n * ->exit ops have been invoked before the function is removed.\n */\nvoid net_ns_barrier(void)\n{\n\tdown_write(&pernet_ops_rwsem);\n\tup_write(&pernet_ops_rwsem);\n}\nEXPORT_SYMBOL(net_ns_barrier);\n\nstatic DECLARE_WORK(net_cleanup_work, cleanup_net);\n\nvoid __put_net(struct net *net)\n{\n\t/* Cleanup the network namespace in process context */\n\tif (llist_add(&net->cleanup_list, &cleanup_list))\n\t\tqueue_work(netns_wq, &net_cleanup_work);\n}\nEXPORT_SYMBOL_GPL(__put_net);\n\nstruct net *get_net_ns_by_fd(int fd)\n{\n\tstruct file *file;\n\tstruct ns_common *ns;\n\tstruct net *net;\n\n\tfile = proc_ns_fget(fd);\n\tif (IS_ERR(file))\n\t\treturn ERR_CAST(file);\n\n\tns = get_proc_ns(file_inode(file));\n\tif (ns->ops == &netns_operations)\n\t\tnet = get_net(container_of(ns, struct net, ns));\n\telse\n\t\tnet = ERR_PTR(-EINVAL);\n\n\tfput(file);\n\treturn net;\n}\n\n#else\nstruct net *get_net_ns_by_fd(int fd)\n{\n\treturn ERR_PTR(-EINVAL);\n}\n#endif\nEXPORT_SYMBOL_GPL(get_net_ns_by_fd);\n\nstruct net *get_net_ns_by_pid(pid_t pid)\n{\n\tstruct task_struct *tsk;\n\tstruct net *net;\n\n\t/* Lookup the network namespace */\n\tnet = ERR_PTR(-ESRCH);\n\trcu_read_lock();\n\ttsk = find_task_by_vpid(pid);\n\tif (tsk) {\n\t\tstruct nsproxy *nsproxy;\n\t\ttask_lock(tsk);\n\t\tnsproxy = tsk->nsproxy;\n\t\tif (nsproxy)\n\t\t\tnet = get_net(nsproxy->net_ns);\n\t\ttask_unlock(tsk);\n\t}\n\trcu_read_unlock();\n\treturn net;\n}\nEXPORT_SYMBOL_GPL(get_net_ns_by_pid);\n\nstatic __net_init int net_ns_net_init(struct net *net)\n{\n#ifdef CONFIG_NET_NS\n\tnet->ns.ops = &netns_operations;\n#endif\n\treturn ns_alloc_inum(&net->ns);\n}\n\nstatic __net_exit void net_ns_net_exit(struct net *net)\n{\n\tns_free_inum(&net->ns);\n}\n\nstatic struct pernet_operations __net_initdata net_ns_ops = {\n\t.init = net_ns_net_init,\n\t.exit = net_ns_net_exit,\n};\n\nstatic const struct nla_policy rtnl_net_policy[NETNSA_MAX + 1] = {\n\t[NETNSA_NONE]\t\t= { .type = NLA_UNSPEC },\n\t[NETNSA_NSID]\t\t= { .type = NLA_S32 },\n\t[NETNSA_PID]\t\t= { .type = NLA_U32 },\n\t[NETNSA_FD]\t\t= { .type = NLA_U32 },\n\t[NETNSA_TARGET_NSID]\t= { .type = NLA_S32 },\n};\n\nstatic int rtnl_net_newid(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[NETNSA_MAX + 1];\n\tstruct nlattr *nla;\n\tstruct net *peer;\n\tint nsid, err;\n\n\terr = nlmsg_parse(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,\n\t\t\t  rtnl_net_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\tif (!tb[NETNSA_NSID]) {\n\t\tNL_SET_ERR_MSG(extack, \"nsid is missing\");\n\t\treturn -EINVAL;\n\t}\n\tnsid = nla_get_s32(tb[NETNSA_NSID]);\n\n\tif (tb[NETNSA_PID]) {\n\t\tpeer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));\n\t\tnla = tb[NETNSA_PID];\n\t} else if (tb[NETNSA_FD]) {\n\t\tpeer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));\n\t\tnla = tb[NETNSA_FD];\n\t} else {\n\t\tNL_SET_ERR_MSG(extack, \"Peer netns reference is missing\");\n\t\treturn -EINVAL;\n\t}\n\tif (IS_ERR(peer)) {\n\t\tNL_SET_BAD_ATTR(extack, nla);\n\t\tNL_SET_ERR_MSG(extack, \"Peer netns reference is invalid\");\n\t\treturn PTR_ERR(peer);\n\t}\n\n\tspin_lock_bh(&net->nsid_lock);\n\tif (__peernet2id(net, peer) >= 0) {\n\t\tspin_unlock_bh(&net->nsid_lock);\n\t\terr = -EEXIST;\n\t\tNL_SET_BAD_ATTR(extack, nla);\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Peer netns already has a nsid assigned\");\n\t\tgoto out;\n\t}\n\n\terr = alloc_netid(net, peer, nsid);\n\tspin_unlock_bh(&net->nsid_lock);\n\tif (err >= 0) {\n\t\trtnl_net_notifyid(net, RTM_NEWNSID, err);\n\t\terr = 0;\n\t} else if (err == -ENOSPC && nsid >= 0) {\n\t\terr = -EEXIST;\n\t\tNL_SET_BAD_ATTR(extack, tb[NETNSA_NSID]);\n\t\tNL_SET_ERR_MSG(extack, \"The specified nsid is already used\");\n\t}\nout:\n\tput_net(peer);\n\treturn err;\n}\n\nstatic int rtnl_net_get_size(void)\n{\n\treturn NLMSG_ALIGN(sizeof(struct rtgenmsg))\n\t       + nla_total_size(sizeof(s32)) /* NETNSA_NSID */\n\t       + nla_total_size(sizeof(s32)) /* NETNSA_CURRENT_NSID */\n\t       ;\n}\n\nstruct net_fill_args {\n\tu32 portid;\n\tu32 seq;\n\tint flags;\n\tint cmd;\n\tint nsid;\n\tbool add_ref;\n\tint ref_nsid;\n};\n\nstatic int rtnl_net_fill(struct sk_buff *skb, struct net_fill_args *args)\n{\n\tstruct nlmsghdr *nlh;\n\tstruct rtgenmsg *rth;\n\n\tnlh = nlmsg_put(skb, args->portid, args->seq, args->cmd, sizeof(*rth),\n\t\t\targs->flags);\n\tif (!nlh)\n\t\treturn -EMSGSIZE;\n\n\trth = nlmsg_data(nlh);\n\trth->rtgen_family = AF_UNSPEC;\n\n\tif (nla_put_s32(skb, NETNSA_NSID, args->nsid))\n\t\tgoto nla_put_failure;\n\n\tif (args->add_ref &&\n\t    nla_put_s32(skb, NETNSA_CURRENT_NSID, args->ref_nsid))\n\t\tgoto nla_put_failure;\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n\nnla_put_failure:\n\tnlmsg_cancel(skb, nlh);\n\treturn -EMSGSIZE;\n}\n\nstatic int rtnl_net_valid_getid_req(struct sk_buff *skb,\n\t\t\t\t    const struct nlmsghdr *nlh,\n\t\t\t\t    struct nlattr **tb,\n\t\t\t\t    struct netlink_ext_ack *extack)\n{\n\tint i, err;\n\n\tif (!netlink_strict_get_check(skb))\n\t\treturn nlmsg_parse(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,\n\t\t\t\t   rtnl_net_policy, extack);\n\n\terr = nlmsg_parse_strict(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,\n\t\t\t\t rtnl_net_policy, extack);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i <= NETNSA_MAX; i++) {\n\t\tif (!tb[i])\n\t\t\tcontinue;\n\n\t\tswitch (i) {\n\t\tcase NETNSA_PID:\n\t\tcase NETNSA_FD:\n\t\tcase NETNSA_NSID:\n\t\tcase NETNSA_TARGET_NSID:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNL_SET_ERR_MSG(extack, \"Unsupported attribute in peer netns getid request\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int rtnl_net_getid(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[NETNSA_MAX + 1];\n\tstruct net_fill_args fillargs = {\n\t\t.portid = NETLINK_CB(skb).portid,\n\t\t.seq = nlh->nlmsg_seq,\n\t\t.cmd = RTM_NEWNSID,\n\t};\n\tstruct net *peer, *target = net;\n\tstruct nlattr *nla;\n\tstruct sk_buff *msg;\n\tint err;\n\n\terr = rtnl_net_valid_getid_req(skb, nlh, tb, extack);\n\tif (err < 0)\n\t\treturn err;\n\tif (tb[NETNSA_PID]) {\n\t\tpeer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));\n\t\tnla = tb[NETNSA_PID];\n\t} else if (tb[NETNSA_FD]) {\n\t\tpeer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));\n\t\tnla = tb[NETNSA_FD];\n\t} else if (tb[NETNSA_NSID]) {\n\t\tpeer = get_net_ns_by_id(net, nla_get_u32(tb[NETNSA_NSID]));\n\t\tif (!peer)\n\t\t\tpeer = ERR_PTR(-ENOENT);\n\t\tnla = tb[NETNSA_NSID];\n\t} else {\n\t\tNL_SET_ERR_MSG(extack, \"Peer netns reference is missing\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (IS_ERR(peer)) {\n\t\tNL_SET_BAD_ATTR(extack, nla);\n\t\tNL_SET_ERR_MSG(extack, \"Peer netns reference is invalid\");\n\t\treturn PTR_ERR(peer);\n\t}\n\n\tif (tb[NETNSA_TARGET_NSID]) {\n\t\tint id = nla_get_s32(tb[NETNSA_TARGET_NSID]);\n\n\t\ttarget = rtnl_get_net_ns_capable(NETLINK_CB(skb).sk, id);\n\t\tif (IS_ERR(target)) {\n\t\t\tNL_SET_BAD_ATTR(extack, tb[NETNSA_TARGET_NSID]);\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Target netns reference is invalid\");\n\t\t\terr = PTR_ERR(target);\n\t\t\tgoto out;\n\t\t}\n\t\tfillargs.add_ref = true;\n\t\tfillargs.ref_nsid = peernet2id(net, peer);\n\t}\n\n\tmsg = nlmsg_new(rtnl_net_get_size(), GFP_KERNEL);\n\tif (!msg) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfillargs.nsid = peernet2id(target, peer);\n\terr = rtnl_net_fill(msg, &fillargs);\n\tif (err < 0)\n\t\tgoto err_out;\n\n\terr = rtnl_unicast(msg, net, NETLINK_CB(skb).portid);\n\tgoto out;\n\nerr_out:\n\tnlmsg_free(msg);\nout:\n\tif (fillargs.add_ref)\n\t\tput_net(target);\n\tput_net(peer);\n\treturn err;\n}\n\nstruct rtnl_net_dump_cb {\n\tstruct net *tgt_net;\n\tstruct net *ref_net;\n\tstruct sk_buff *skb;\n\tstruct net_fill_args fillargs;\n\tint idx;\n\tint s_idx;\n};\n\nstatic int rtnl_net_dumpid_one(int id, void *peer, void *data)\n{\n\tstruct rtnl_net_dump_cb *net_cb = (struct rtnl_net_dump_cb *)data;\n\tint ret;\n\n\tif (net_cb->idx < net_cb->s_idx)\n\t\tgoto cont;\n\n\tnet_cb->fillargs.nsid = id;\n\tif (net_cb->fillargs.add_ref)\n\t\tnet_cb->fillargs.ref_nsid = __peernet2id(net_cb->ref_net, peer);\n\tret = rtnl_net_fill(net_cb->skb, &net_cb->fillargs);\n\tif (ret < 0)\n\t\treturn ret;\n\ncont:\n\tnet_cb->idx++;\n\treturn 0;\n}\n\nstatic int rtnl_valid_dump_net_req(const struct nlmsghdr *nlh, struct sock *sk,\n\t\t\t\t   struct rtnl_net_dump_cb *net_cb,\n\t\t\t\t   struct netlink_callback *cb)\n{\n\tstruct netlink_ext_ack *extack = cb->extack;\n\tstruct nlattr *tb[NETNSA_MAX + 1];\n\tint err, i;\n\n\terr = nlmsg_parse_strict(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,\n\t\t\t\t rtnl_net_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tfor (i = 0; i <= NETNSA_MAX; i++) {\n\t\tif (!tb[i])\n\t\t\tcontinue;\n\n\t\tif (i == NETNSA_TARGET_NSID) {\n\t\t\tstruct net *net;\n\n\t\t\tnet = rtnl_get_net_ns_capable(sk, nla_get_s32(tb[i]));\n\t\t\tif (IS_ERR(net)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, tb[i]);\n\t\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t\t       \"Invalid target network namespace id\");\n\t\t\t\treturn PTR_ERR(net);\n\t\t\t}\n\t\t\tnet_cb->fillargs.add_ref = true;\n\t\t\tnet_cb->ref_net = net_cb->tgt_net;\n\t\t\tnet_cb->tgt_net = net;\n\t\t} else {\n\t\t\tNL_SET_BAD_ATTR(extack, tb[i]);\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Unsupported attribute in dump request\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int rtnl_net_dumpid(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct rtnl_net_dump_cb net_cb = {\n\t\t.tgt_net = sock_net(skb->sk),\n\t\t.skb = skb,\n\t\t.fillargs = {\n\t\t\t.portid = NETLINK_CB(cb->skb).portid,\n\t\t\t.seq = cb->nlh->nlmsg_seq,\n\t\t\t.flags = NLM_F_MULTI,\n\t\t\t.cmd = RTM_NEWNSID,\n\t\t},\n\t\t.idx = 0,\n\t\t.s_idx = cb->args[0],\n\t};\n\tint err = 0;\n\n\tif (cb->strict_check) {\n\t\terr = rtnl_valid_dump_net_req(cb->nlh, skb->sk, &net_cb, cb);\n\t\tif (err < 0)\n\t\t\tgoto end;\n\t}\n\n\tspin_lock_bh(&net_cb.tgt_net->nsid_lock);\n\tif (net_cb.fillargs.add_ref &&\n\t    !net_eq(net_cb.ref_net, net_cb.tgt_net) &&\n\t    !spin_trylock_bh(&net_cb.ref_net->nsid_lock)) {\n\t\tspin_unlock_bh(&net_cb.tgt_net->nsid_lock);\n\t\terr = -EAGAIN;\n\t\tgoto end;\n\t}\n\tidr_for_each(&net_cb.tgt_net->netns_ids, rtnl_net_dumpid_one, &net_cb);\n\tif (net_cb.fillargs.add_ref &&\n\t    !net_eq(net_cb.ref_net, net_cb.tgt_net))\n\t\tspin_unlock_bh(&net_cb.ref_net->nsid_lock);\n\tspin_unlock_bh(&net_cb.tgt_net->nsid_lock);\n\n\tcb->args[0] = net_cb.idx;\nend:\n\tif (net_cb.fillargs.add_ref)\n\t\tput_net(net_cb.tgt_net);\n\treturn err < 0 ? err : skb->len;\n}\n\nstatic void rtnl_net_notifyid(struct net *net, int cmd, int id)\n{\n\tstruct net_fill_args fillargs = {\n\t\t.cmd = cmd,\n\t\t.nsid = id,\n\t};\n\tstruct sk_buff *msg;\n\tint err = -ENOMEM;\n\n\tmsg = nlmsg_new(rtnl_net_get_size(), GFP_KERNEL);\n\tif (!msg)\n\t\tgoto out;\n\n\terr = rtnl_net_fill(msg, &fillargs);\n\tif (err < 0)\n\t\tgoto err_out;\n\n\trtnl_notify(msg, net, 0, RTNLGRP_NSID, NULL, 0);\n\treturn;\n\nerr_out:\n\tnlmsg_free(msg);\nout:\n\trtnl_set_sk_err(net, RTNLGRP_NSID, err);\n}\n\nstatic int __init net_ns_init(void)\n{\n\tstruct net_generic *ng;\n\n#ifdef CONFIG_NET_NS\n\tnet_cachep = kmem_cache_create(\"net_namespace\", sizeof(struct net),\n\t\t\t\t\tSMP_CACHE_BYTES,\n\t\t\t\t\tSLAB_PANIC|SLAB_ACCOUNT, NULL);\n\n\t/* Create workqueue for cleanup */\n\tnetns_wq = create_singlethread_workqueue(\"netns\");\n\tif (!netns_wq)\n\t\tpanic(\"Could not create netns workq\");\n#endif\n\n\tng = net_alloc_generic();\n\tif (!ng)\n\t\tpanic(\"Could not allocate generic netns\");\n\n\trcu_assign_pointer(init_net.gen, ng);\n\n\tdown_write(&pernet_ops_rwsem);\n\tif (setup_net(&init_net, &init_user_ns))\n\t\tpanic(\"Could not setup the initial network namespace\");\n\n\tinit_net_initialized = true;\n\tup_write(&pernet_ops_rwsem);\n\n\tif (register_pernet_subsys(&net_ns_ops))\n\t\tpanic(\"Could not register network namespace subsystems\");\n\n\trtnl_register(PF_UNSPEC, RTM_NEWNSID, rtnl_net_newid, NULL,\n\t\t      RTNL_FLAG_DOIT_UNLOCKED);\n\trtnl_register(PF_UNSPEC, RTM_GETNSID, rtnl_net_getid, rtnl_net_dumpid,\n\t\t      RTNL_FLAG_DOIT_UNLOCKED);\n\n\treturn 0;\n}\n\npure_initcall(net_ns_init);\n\n#ifdef CONFIG_NET_NS\nstatic int __register_pernet_operations(struct list_head *list,\n\t\t\t\t\tstruct pernet_operations *ops)\n{\n\tstruct net *net;\n\tint error;\n\tLIST_HEAD(net_exit_list);\n\n\tlist_add_tail(&ops->list, list);\n\tif (ops->init || (ops->id && ops->size)) {\n\t\t/* We held write locked pernet_ops_rwsem, and parallel\n\t\t * setup_net() and cleanup_net() are not possible.\n\t\t */\n\t\tfor_each_net(net) {\n\t\t\terror = ops_init(ops, net);\n\t\t\tif (error)\n\t\t\t\tgoto out_undo;\n\t\t\tlist_add_tail(&net->exit_list, &net_exit_list);\n\t\t}\n\t}\n\treturn 0;\n\nout_undo:\n\t/* If I have an error cleanup all namespaces I initialized */\n\tlist_del(&ops->list);\n\tops_exit_list(ops, &net_exit_list);\n\tops_free_list(ops, &net_exit_list);\n\treturn error;\n}\n\nstatic void __unregister_pernet_operations(struct pernet_operations *ops)\n{\n\tstruct net *net;\n\tLIST_HEAD(net_exit_list);\n\n\tlist_del(&ops->list);\n\t/* See comment in __register_pernet_operations() */\n\tfor_each_net(net)\n\t\tlist_add_tail(&net->exit_list, &net_exit_list);\n\tops_exit_list(ops, &net_exit_list);\n\tops_free_list(ops, &net_exit_list);\n}\n\n#else\n\nstatic int __register_pernet_operations(struct list_head *list,\n\t\t\t\t\tstruct pernet_operations *ops)\n{\n\tif (!init_net_initialized) {\n\t\tlist_add_tail(&ops->list, list);\n\t\treturn 0;\n\t}\n\n\treturn ops_init(ops, &init_net);\n}\n\nstatic void __unregister_pernet_operations(struct pernet_operations *ops)\n{\n\tif (!init_net_initialized) {\n\t\tlist_del(&ops->list);\n\t} else {\n\t\tLIST_HEAD(net_exit_list);\n\t\tlist_add(&init_net.exit_list, &net_exit_list);\n\t\tops_exit_list(ops, &net_exit_list);\n\t\tops_free_list(ops, &net_exit_list);\n\t}\n}\n\n#endif /* CONFIG_NET_NS */\n\nstatic DEFINE_IDA(net_generic_ids);\n\nstatic int register_pernet_operations(struct list_head *list,\n\t\t\t\t      struct pernet_operations *ops)\n{\n\tint error;\n\n\tif (ops->id) {\n\t\terror = ida_alloc_min(&net_generic_ids, MIN_PERNET_OPS_ID,\n\t\t\t\tGFP_KERNEL);\n\t\tif (error < 0)\n\t\t\treturn error;\n\t\t*ops->id = error;\n\t\tmax_gen_ptrs = max(max_gen_ptrs, *ops->id + 1);\n\t}\n\terror = __register_pernet_operations(list, ops);\n\tif (error) {\n\t\trcu_barrier();\n\t\tif (ops->id)\n\t\t\tida_free(&net_generic_ids, *ops->id);\n\t}\n\n\treturn error;\n}\n\nstatic void unregister_pernet_operations(struct pernet_operations *ops)\n{\n\t__unregister_pernet_operations(ops);\n\trcu_barrier();\n\tif (ops->id)\n\t\tida_free(&net_generic_ids, *ops->id);\n}\n\n/**\n *      register_pernet_subsys - register a network namespace subsystem\n *\t@ops:  pernet operations structure for the subsystem\n *\n *\tRegister a subsystem which has init and exit functions\n *\tthat are called when network namespaces are created and\n *\tdestroyed respectively.\n *\n *\tWhen registered all network namespace init functions are\n *\tcalled for every existing network namespace.  Allowing kernel\n *\tmodules to have a race free view of the set of network namespaces.\n *\n *\tWhen a new network namespace is created all of the init\n *\tmethods are called in the order in which they were registered.\n *\n *\tWhen a network namespace is destroyed all of the exit methods\n *\tare called in the reverse of the order with which they were\n *\tregistered.\n */\nint register_pernet_subsys(struct pernet_operations *ops)\n{\n\tint error;\n\tdown_write(&pernet_ops_rwsem);\n\terror =  register_pernet_operations(first_device, ops);\n\tup_write(&pernet_ops_rwsem);\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(register_pernet_subsys);\n\n/**\n *      unregister_pernet_subsys - unregister a network namespace subsystem\n *\t@ops: pernet operations structure to manipulate\n *\n *\tRemove the pernet operations structure from the list to be\n *\tused when network namespaces are created or destroyed.  In\n *\taddition run the exit method for all existing network\n *\tnamespaces.\n */\nvoid unregister_pernet_subsys(struct pernet_operations *ops)\n{\n\tdown_write(&pernet_ops_rwsem);\n\tunregister_pernet_operations(ops);\n\tup_write(&pernet_ops_rwsem);\n}\nEXPORT_SYMBOL_GPL(unregister_pernet_subsys);\n\n/**\n *      register_pernet_device - register a network namespace device\n *\t@ops:  pernet operations structure for the subsystem\n *\n *\tRegister a device which has init and exit functions\n *\tthat are called when network namespaces are created and\n *\tdestroyed respectively.\n *\n *\tWhen registered all network namespace init functions are\n *\tcalled for every existing network namespace.  Allowing kernel\n *\tmodules to have a race free view of the set of network namespaces.\n *\n *\tWhen a new network namespace is created all of the init\n *\tmethods are called in the order in which they were registered.\n *\n *\tWhen a network namespace is destroyed all of the exit methods\n *\tare called in the reverse of the order with which they were\n *\tregistered.\n */\nint register_pernet_device(struct pernet_operations *ops)\n{\n\tint error;\n\tdown_write(&pernet_ops_rwsem);\n\terror = register_pernet_operations(&pernet_list, ops);\n\tif (!error && (first_device == &pernet_list))\n\t\tfirst_device = &ops->list;\n\tup_write(&pernet_ops_rwsem);\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(register_pernet_device);\n\n/**\n *      unregister_pernet_device - unregister a network namespace netdevice\n *\t@ops: pernet operations structure to manipulate\n *\n *\tRemove the pernet operations structure from the list to be\n *\tused when network namespaces are created or destroyed.  In\n *\taddition run the exit method for all existing network\n *\tnamespaces.\n */\nvoid unregister_pernet_device(struct pernet_operations *ops)\n{\n\tdown_write(&pernet_ops_rwsem);\n\tif (&ops->list == first_device)\n\t\tfirst_device = first_device->next;\n\tunregister_pernet_operations(ops);\n\tup_write(&pernet_ops_rwsem);\n}\nEXPORT_SYMBOL_GPL(unregister_pernet_device);\n\n#ifdef CONFIG_NET_NS\nstatic struct ns_common *netns_get(struct task_struct *task)\n{\n\tstruct net *net = NULL;\n\tstruct nsproxy *nsproxy;\n\n\ttask_lock(task);\n\tnsproxy = task->nsproxy;\n\tif (nsproxy)\n\t\tnet = get_net(nsproxy->net_ns);\n\ttask_unlock(task);\n\n\treturn net ? &net->ns : NULL;\n}\n\nstatic inline struct net *to_net_ns(struct ns_common *ns)\n{\n\treturn container_of(ns, struct net, ns);\n}\n\nstatic void netns_put(struct ns_common *ns)\n{\n\tput_net(to_net_ns(ns));\n}\n\nstatic int netns_install(struct nsproxy *nsproxy, struct ns_common *ns)\n{\n\tstruct net *net = to_net_ns(ns);\n\n\tif (!ns_capable(net->user_ns, CAP_SYS_ADMIN) ||\n\t    !ns_capable(current_user_ns(), CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tput_net(nsproxy->net_ns);\n\tnsproxy->net_ns = get_net(net);\n\treturn 0;\n}\n\nstatic struct user_namespace *netns_owner(struct ns_common *ns)\n{\n\treturn to_net_ns(ns)->user_ns;\n}\n\nconst struct proc_ns_operations netns_operations = {\n\t.name\t\t= \"net\",\n\t.type\t\t= CLONE_NEWNET,\n\t.get\t\t= netns_get,\n\t.put\t\t= netns_put,\n\t.install\t= netns_install,\n\t.owner\t\t= netns_owner,\n};\n#endif\n"], "fixing_code": ["/* SPDX-License-Identifier: GPL-2.0 */\n/*\n * Operations on the network namespace\n */\n#ifndef __NET_NET_NAMESPACE_H\n#define __NET_NET_NAMESPACE_H\n\n#include <linux/atomic.h>\n#include <linux/refcount.h>\n#include <linux/workqueue.h>\n#include <linux/list.h>\n#include <linux/sysctl.h>\n#include <linux/uidgid.h>\n\n#include <net/flow.h>\n#include <net/netns/core.h>\n#include <net/netns/mib.h>\n#include <net/netns/unix.h>\n#include <net/netns/packet.h>\n#include <net/netns/ipv4.h>\n#include <net/netns/ipv6.h>\n#include <net/netns/ieee802154_6lowpan.h>\n#include <net/netns/sctp.h>\n#include <net/netns/dccp.h>\n#include <net/netns/netfilter.h>\n#include <net/netns/x_tables.h>\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n#include <net/netns/conntrack.h>\n#endif\n#include <net/netns/nftables.h>\n#include <net/netns/xfrm.h>\n#include <net/netns/mpls.h>\n#include <net/netns/can.h>\n#include <net/netns/xdp.h>\n#include <linux/ns_common.h>\n#include <linux/idr.h>\n#include <linux/skbuff.h>\n\nstruct user_namespace;\nstruct proc_dir_entry;\nstruct net_device;\nstruct sock;\nstruct ctl_table_header;\nstruct net_generic;\nstruct uevent_sock;\nstruct netns_ipvs;\nstruct bpf_prog;\n\n\n#define NETDEV_HASHBITS    8\n#define NETDEV_HASHENTRIES (1 << NETDEV_HASHBITS)\n\nstruct net {\n\trefcount_t\t\tpassive;\t/* To decided when the network\n\t\t\t\t\t\t * namespace should be freed.\n\t\t\t\t\t\t */\n\trefcount_t\t\tcount;\t\t/* To decided when the network\n\t\t\t\t\t\t *  namespace should be shut down.\n\t\t\t\t\t\t */\n\tspinlock_t\t\trules_mod_lock;\n\n\tu32\t\t\thash_mix;\n\tatomic64_t\t\tcookie_gen;\n\n\tstruct list_head\tlist;\t\t/* list of network namespaces */\n\tstruct list_head\texit_list;\t/* To linked to call pernet exit\n\t\t\t\t\t\t * methods on dead net (\n\t\t\t\t\t\t * pernet_ops_rwsem read locked),\n\t\t\t\t\t\t * or to unregister pernet ops\n\t\t\t\t\t\t * (pernet_ops_rwsem write locked).\n\t\t\t\t\t\t */\n\tstruct llist_node\tcleanup_list;\t/* namespaces on death row */\n\n\tstruct user_namespace   *user_ns;\t/* Owning user namespace */\n\tstruct ucounts\t\t*ucounts;\n\tspinlock_t\t\tnsid_lock;\n\tstruct idr\t\tnetns_ids;\n\n\tstruct ns_common\tns;\n\n\tstruct proc_dir_entry \t*proc_net;\n\tstruct proc_dir_entry \t*proc_net_stat;\n\n#ifdef CONFIG_SYSCTL\n\tstruct ctl_table_set\tsysctls;\n#endif\n\n\tstruct sock \t\t*rtnl;\t\t\t/* rtnetlink socket */\n\tstruct sock\t\t*genl_sock;\n\n\tstruct uevent_sock\t*uevent_sock;\t\t/* uevent socket */\n\n\tstruct list_head \tdev_base_head;\n\tstruct hlist_head \t*dev_name_head;\n\tstruct hlist_head\t*dev_index_head;\n\tunsigned int\t\tdev_base_seq;\t/* protected by rtnl_mutex */\n\tint\t\t\tifindex;\n\tunsigned int\t\tdev_unreg_count;\n\n\t/* core fib_rules */\n\tstruct list_head\trules_ops;\n\n\tstruct list_head\tfib_notifier_ops;  /* Populated by\n\t\t\t\t\t\t    * register_pernet_subsys()\n\t\t\t\t\t\t    */\n\tstruct net_device       *loopback_dev;          /* The loopback */\n\tstruct netns_core\tcore;\n\tstruct netns_mib\tmib;\n\tstruct netns_packet\tpacket;\n\tstruct netns_unix\tunx;\n\tstruct netns_ipv4\tipv4;\n#if IS_ENABLED(CONFIG_IPV6)\n\tstruct netns_ipv6\tipv6;\n#endif\n#if IS_ENABLED(CONFIG_IEEE802154_6LOWPAN)\n\tstruct netns_ieee802154_lowpan\tieee802154_lowpan;\n#endif\n#if defined(CONFIG_IP_SCTP) || defined(CONFIG_IP_SCTP_MODULE)\n\tstruct netns_sctp\tsctp;\n#endif\n#if defined(CONFIG_IP_DCCP) || defined(CONFIG_IP_DCCP_MODULE)\n\tstruct netns_dccp\tdccp;\n#endif\n#ifdef CONFIG_NETFILTER\n\tstruct netns_nf\t\tnf;\n\tstruct netns_xt\t\txt;\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tstruct netns_ct\t\tct;\n#endif\n#if defined(CONFIG_NF_TABLES) || defined(CONFIG_NF_TABLES_MODULE)\n\tstruct netns_nftables\tnft;\n#endif\n#if IS_ENABLED(CONFIG_NF_DEFRAG_IPV6)\n\tstruct netns_nf_frag\tnf_frag;\n\tstruct ctl_table_header *nf_frag_frags_hdr;\n#endif\n\tstruct sock\t\t*nfnl;\n\tstruct sock\t\t*nfnl_stash;\n#if IS_ENABLED(CONFIG_NETFILTER_NETLINK_ACCT)\n\tstruct list_head        nfnl_acct_list;\n#endif\n#if IS_ENABLED(CONFIG_NF_CT_NETLINK_TIMEOUT)\n\tstruct list_head\tnfct_timeout_list;\n#endif\n#endif\n#ifdef CONFIG_WEXT_CORE\n\tstruct sk_buff_head\twext_nlevents;\n#endif\n\tstruct net_generic __rcu\t*gen;\n\n\tstruct bpf_prog __rcu\t*flow_dissector_prog;\n\n\t/* Note : following structs are cache line aligned */\n#ifdef CONFIG_XFRM\n\tstruct netns_xfrm\txfrm;\n#endif\n#if IS_ENABLED(CONFIG_IP_VS)\n\tstruct netns_ipvs\t*ipvs;\n#endif\n#if IS_ENABLED(CONFIG_MPLS)\n\tstruct netns_mpls\tmpls;\n#endif\n#if IS_ENABLED(CONFIG_CAN)\n\tstruct netns_can\tcan;\n#endif\n#ifdef CONFIG_XDP_SOCKETS\n\tstruct netns_xdp\txdp;\n#endif\n\tstruct sock\t\t*diag_nlsk;\n\tatomic_t\t\tfnhe_genid;\n} __randomize_layout;\n\n#include <linux/seq_file_net.h>\n\n/* Init's network namespace */\nextern struct net init_net;\n\n#ifdef CONFIG_NET_NS\nstruct net *copy_net_ns(unsigned long flags, struct user_namespace *user_ns,\n\t\t\tstruct net *old_net);\n\nvoid net_ns_get_ownership(const struct net *net, kuid_t *uid, kgid_t *gid);\n\nvoid net_ns_barrier(void);\n#else /* CONFIG_NET_NS */\n#include <linux/sched.h>\n#include <linux/nsproxy.h>\nstatic inline struct net *copy_net_ns(unsigned long flags,\n\tstruct user_namespace *user_ns, struct net *old_net)\n{\n\tif (flags & CLONE_NEWNET)\n\t\treturn ERR_PTR(-EINVAL);\n\treturn old_net;\n}\n\nstatic inline void net_ns_get_ownership(const struct net *net,\n\t\t\t\t\tkuid_t *uid, kgid_t *gid)\n{\n\t*uid = GLOBAL_ROOT_UID;\n\t*gid = GLOBAL_ROOT_GID;\n}\n\nstatic inline void net_ns_barrier(void) {}\n#endif /* CONFIG_NET_NS */\n\n\nextern struct list_head net_namespace_list;\n\nstruct net *get_net_ns_by_pid(pid_t pid);\nstruct net *get_net_ns_by_fd(int fd);\n\n#ifdef CONFIG_SYSCTL\nvoid ipx_register_sysctl(void);\nvoid ipx_unregister_sysctl(void);\n#else\n#define ipx_register_sysctl()\n#define ipx_unregister_sysctl()\n#endif\n\n#ifdef CONFIG_NET_NS\nvoid __put_net(struct net *net);\n\nstatic inline struct net *get_net(struct net *net)\n{\n\trefcount_inc(&net->count);\n\treturn net;\n}\n\nstatic inline struct net *maybe_get_net(struct net *net)\n{\n\t/* Used when we know struct net exists but we\n\t * aren't guaranteed a previous reference count\n\t * exists.  If the reference count is zero this\n\t * function fails and returns NULL.\n\t */\n\tif (!refcount_inc_not_zero(&net->count))\n\t\tnet = NULL;\n\treturn net;\n}\n\nstatic inline void put_net(struct net *net)\n{\n\tif (refcount_dec_and_test(&net->count))\n\t\t__put_net(net);\n}\n\nstatic inline\nint net_eq(const struct net *net1, const struct net *net2)\n{\n\treturn net1 == net2;\n}\n\nstatic inline int check_net(const struct net *net)\n{\n\treturn refcount_read(&net->count) != 0;\n}\n\nvoid net_drop_ns(void *);\n\n#else\n\nstatic inline struct net *get_net(struct net *net)\n{\n\treturn net;\n}\n\nstatic inline void put_net(struct net *net)\n{\n}\n\nstatic inline struct net *maybe_get_net(struct net *net)\n{\n\treturn net;\n}\n\nstatic inline\nint net_eq(const struct net *net1, const struct net *net2)\n{\n\treturn 1;\n}\n\nstatic inline int check_net(const struct net *net)\n{\n\treturn 1;\n}\n\n#define net_drop_ns NULL\n#endif\n\n\ntypedef struct {\n#ifdef CONFIG_NET_NS\n\tstruct net *net;\n#endif\n} possible_net_t;\n\nstatic inline void write_pnet(possible_net_t *pnet, struct net *net)\n{\n#ifdef CONFIG_NET_NS\n\tpnet->net = net;\n#endif\n}\n\nstatic inline struct net *read_pnet(const possible_net_t *pnet)\n{\n#ifdef CONFIG_NET_NS\n\treturn pnet->net;\n#else\n\treturn &init_net;\n#endif\n}\n\n/* Protected by net_rwsem */\n#define for_each_net(VAR)\t\t\t\t\\\n\tlist_for_each_entry(VAR, &net_namespace_list, list)\n\n#define for_each_net_rcu(VAR)\t\t\t\t\\\n\tlist_for_each_entry_rcu(VAR, &net_namespace_list, list)\n\n#ifdef CONFIG_NET_NS\n#define __net_init\n#define __net_exit\n#define __net_initdata\n#define __net_initconst\n#else\n#define __net_init\t__init\n#define __net_exit\t__ref\n#define __net_initdata\t__initdata\n#define __net_initconst\t__initconst\n#endif\n\nint peernet2id_alloc(struct net *net, struct net *peer);\nint peernet2id(struct net *net, struct net *peer);\nbool peernet_has_id(struct net *net, struct net *peer);\nstruct net *get_net_ns_by_id(struct net *net, int id);\n\nstruct pernet_operations {\n\tstruct list_head list;\n\t/*\n\t * Below methods are called without any exclusive locks.\n\t * More than one net may be constructed and destructed\n\t * in parallel on several cpus. Every pernet_operations\n\t * have to keep in mind all other pernet_operations and\n\t * to introduce a locking, if they share common resources.\n\t *\n\t * The only time they are called with exclusive lock is\n\t * from register_pernet_subsys(), unregister_pernet_subsys()\n\t * register_pernet_device() and unregister_pernet_device().\n\t *\n\t * Exit methods using blocking RCU primitives, such as\n\t * synchronize_rcu(), should be implemented via exit_batch.\n\t * Then, destruction of a group of net requires single\n\t * synchronize_rcu() related to these pernet_operations,\n\t * instead of separate synchronize_rcu() for every net.\n\t * Please, avoid synchronize_rcu() at all, where it's possible.\n\t */\n\tint (*init)(struct net *net);\n\tvoid (*exit)(struct net *net);\n\tvoid (*exit_batch)(struct list_head *net_exit_list);\n\tunsigned int *id;\n\tsize_t size;\n};\n\n/*\n * Use these carefully.  If you implement a network device and it\n * needs per network namespace operations use device pernet operations,\n * otherwise use pernet subsys operations.\n *\n * Network interfaces need to be removed from a dying netns _before_\n * subsys notifiers can be called, as most of the network code cleanup\n * (which is done from subsys notifiers) runs with the assumption that\n * dev_remove_pack has been called so no new packets will arrive during\n * and after the cleanup functions have been called.  dev_remove_pack\n * is not per namespace so instead the guarantee of no more packets\n * arriving in a network namespace is provided by ensuring that all\n * network devices and all sockets have left the network namespace\n * before the cleanup methods are called.\n *\n * For the longest time the ipv4 icmp code was registered as a pernet\n * device which caused kernel oops, and panics during network\n * namespace cleanup.   So please don't get this wrong.\n */\nint register_pernet_subsys(struct pernet_operations *);\nvoid unregister_pernet_subsys(struct pernet_operations *);\nint register_pernet_device(struct pernet_operations *);\nvoid unregister_pernet_device(struct pernet_operations *);\n\nstruct ctl_table;\nstruct ctl_table_header;\n\n#ifdef CONFIG_SYSCTL\nint net_sysctl_init(void);\nstruct ctl_table_header *register_net_sysctl(struct net *net, const char *path,\n\t\t\t\t\t     struct ctl_table *table);\nvoid unregister_net_sysctl_table(struct ctl_table_header *header);\n#else\nstatic inline int net_sysctl_init(void) { return 0; }\nstatic inline struct ctl_table_header *register_net_sysctl(struct net *net,\n\tconst char *path, struct ctl_table *table)\n{\n\treturn NULL;\n}\nstatic inline void unregister_net_sysctl_table(struct ctl_table_header *header)\n{\n}\n#endif\n\nstatic inline int rt_genid_ipv4(struct net *net)\n{\n\treturn atomic_read(&net->ipv4.rt_genid);\n}\n\nstatic inline void rt_genid_bump_ipv4(struct net *net)\n{\n\tatomic_inc(&net->ipv4.rt_genid);\n}\n\nextern void (*__fib6_flush_trees)(struct net *net);\nstatic inline void rt_genid_bump_ipv6(struct net *net)\n{\n\tif (__fib6_flush_trees)\n\t\t__fib6_flush_trees(net);\n}\n\n#if IS_ENABLED(CONFIG_IEEE802154_6LOWPAN)\nstatic inline struct netns_ieee802154_lowpan *\nnet_ieee802154_lowpan(struct net *net)\n{\n\treturn &net->ieee802154_lowpan;\n}\n#endif\n\n/* For callers who don't really care about whether it's IPv4 or IPv6 */\nstatic inline void rt_genid_bump_all(struct net *net)\n{\n\trt_genid_bump_ipv4(net);\n\trt_genid_bump_ipv6(net);\n}\n\nstatic inline int fnhe_genid(struct net *net)\n{\n\treturn atomic_read(&net->fnhe_genid);\n}\n\nstatic inline void fnhe_genid_bump(struct net *net)\n{\n\tatomic_inc(&net->fnhe_genid);\n}\n\n#endif /* __NET_NET_NAMESPACE_H */\n", "/* SPDX-License-Identifier: GPL-2.0 */\n#ifndef __NET_NS_HASH_H__\n#define __NET_NS_HASH_H__\n\n#include <net/net_namespace.h>\n\nstatic inline u32 net_hash_mix(const struct net *net)\n{\n\treturn net->hash_mix;\n}\n#endif\n", "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/workqueue.h>\n#include <linux/rtnetlink.h>\n#include <linux/cache.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/delay.h>\n#include <linux/sched.h>\n#include <linux/idr.h>\n#include <linux/rculist.h>\n#include <linux/nsproxy.h>\n#include <linux/fs.h>\n#include <linux/proc_ns.h>\n#include <linux/file.h>\n#include <linux/export.h>\n#include <linux/user_namespace.h>\n#include <linux/net_namespace.h>\n#include <linux/sched/task.h>\n#include <linux/uidgid.h>\n\n#include <net/sock.h>\n#include <net/netlink.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n\n/*\n *\tOur network namespace constructor/destructor lists\n */\n\nstatic LIST_HEAD(pernet_list);\nstatic struct list_head *first_device = &pernet_list;\n\nLIST_HEAD(net_namespace_list);\nEXPORT_SYMBOL_GPL(net_namespace_list);\n\n/* Protects net_namespace_list. Nests iside rtnl_lock() */\nDECLARE_RWSEM(net_rwsem);\nEXPORT_SYMBOL_GPL(net_rwsem);\n\nstruct net init_net = {\n\t.count\t\t= REFCOUNT_INIT(1),\n\t.dev_base_head\t= LIST_HEAD_INIT(init_net.dev_base_head),\n};\nEXPORT_SYMBOL(init_net);\n\nstatic bool init_net_initialized;\n/*\n * pernet_ops_rwsem: protects: pernet_list, net_generic_ids,\n * init_net_initialized and first_device pointer.\n * This is internal net namespace object. Please, don't use it\n * outside.\n */\nDECLARE_RWSEM(pernet_ops_rwsem);\nEXPORT_SYMBOL_GPL(pernet_ops_rwsem);\n\n#define MIN_PERNET_OPS_ID\t\\\n\t((sizeof(struct net_generic) + sizeof(void *) - 1) / sizeof(void *))\n\n#define INITIAL_NET_GEN_PTRS\t13 /* +1 for len +2 for rcu_head */\n\nstatic unsigned int max_gen_ptrs = INITIAL_NET_GEN_PTRS;\n\nstatic struct net_generic *net_alloc_generic(void)\n{\n\tstruct net_generic *ng;\n\tunsigned int generic_size = offsetof(struct net_generic, ptr[max_gen_ptrs]);\n\n\tng = kzalloc(generic_size, GFP_KERNEL);\n\tif (ng)\n\t\tng->s.len = max_gen_ptrs;\n\n\treturn ng;\n}\n\nstatic int net_assign_generic(struct net *net, unsigned int id, void *data)\n{\n\tstruct net_generic *ng, *old_ng;\n\n\tBUG_ON(id < MIN_PERNET_OPS_ID);\n\n\told_ng = rcu_dereference_protected(net->gen,\n\t\t\t\t\t   lockdep_is_held(&pernet_ops_rwsem));\n\tif (old_ng->s.len > id) {\n\t\told_ng->ptr[id] = data;\n\t\treturn 0;\n\t}\n\n\tng = net_alloc_generic();\n\tif (ng == NULL)\n\t\treturn -ENOMEM;\n\n\t/*\n\t * Some synchronisation notes:\n\t *\n\t * The net_generic explores the net->gen array inside rcu\n\t * read section. Besides once set the net->gen->ptr[x]\n\t * pointer never changes (see rules in netns/generic.h).\n\t *\n\t * That said, we simply duplicate this array and schedule\n\t * the old copy for kfree after a grace period.\n\t */\n\n\tmemcpy(&ng->ptr[MIN_PERNET_OPS_ID], &old_ng->ptr[MIN_PERNET_OPS_ID],\n\t       (old_ng->s.len - MIN_PERNET_OPS_ID) * sizeof(void *));\n\tng->ptr[id] = data;\n\n\trcu_assign_pointer(net->gen, ng);\n\tkfree_rcu(old_ng, s.rcu);\n\treturn 0;\n}\n\nstatic int ops_init(const struct pernet_operations *ops, struct net *net)\n{\n\tint err = -ENOMEM;\n\tvoid *data = NULL;\n\n\tif (ops->id && ops->size) {\n\t\tdata = kzalloc(ops->size, GFP_KERNEL);\n\t\tif (!data)\n\t\t\tgoto out;\n\n\t\terr = net_assign_generic(net, *ops->id, data);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t}\n\terr = 0;\n\tif (ops->init)\n\t\terr = ops->init(net);\n\tif (!err)\n\t\treturn 0;\n\ncleanup:\n\tkfree(data);\n\nout:\n\treturn err;\n}\n\nstatic void ops_free(const struct pernet_operations *ops, struct net *net)\n{\n\tif (ops->id && ops->size) {\n\t\tkfree(net_generic(net, *ops->id));\n\t}\n}\n\nstatic void ops_exit_list(const struct pernet_operations *ops,\n\t\t\t  struct list_head *net_exit_list)\n{\n\tstruct net *net;\n\tif (ops->exit) {\n\t\tlist_for_each_entry(net, net_exit_list, exit_list)\n\t\t\tops->exit(net);\n\t}\n\tif (ops->exit_batch)\n\t\tops->exit_batch(net_exit_list);\n}\n\nstatic void ops_free_list(const struct pernet_operations *ops,\n\t\t\t  struct list_head *net_exit_list)\n{\n\tstruct net *net;\n\tif (ops->size && ops->id) {\n\t\tlist_for_each_entry(net, net_exit_list, exit_list)\n\t\t\tops_free(ops, net);\n\t}\n}\n\n/* should be called with nsid_lock held */\nstatic int alloc_netid(struct net *net, struct net *peer, int reqid)\n{\n\tint min = 0, max = 0;\n\n\tif (reqid >= 0) {\n\t\tmin = reqid;\n\t\tmax = reqid + 1;\n\t}\n\n\treturn idr_alloc(&net->netns_ids, peer, min, max, GFP_ATOMIC);\n}\n\n/* This function is used by idr_for_each(). If net is equal to peer, the\n * function returns the id so that idr_for_each() stops. Because we cannot\n * returns the id 0 (idr_for_each() will not stop), we return the magic value\n * NET_ID_ZERO (-1) for it.\n */\n#define NET_ID_ZERO -1\nstatic int net_eq_idr(int id, void *net, void *peer)\n{\n\tif (net_eq(net, peer))\n\t\treturn id ? : NET_ID_ZERO;\n\treturn 0;\n}\n\n/* Should be called with nsid_lock held. If a new id is assigned, the bool alloc\n * is set to true, thus the caller knows that the new id must be notified via\n * rtnl.\n */\nstatic int __peernet2id_alloc(struct net *net, struct net *peer, bool *alloc)\n{\n\tint id = idr_for_each(&net->netns_ids, net_eq_idr, peer);\n\tbool alloc_it = *alloc;\n\n\t*alloc = false;\n\n\t/* Magic value for id 0. */\n\tif (id == NET_ID_ZERO)\n\t\treturn 0;\n\tif (id > 0)\n\t\treturn id;\n\n\tif (alloc_it) {\n\t\tid = alloc_netid(net, peer, -1);\n\t\t*alloc = true;\n\t\treturn id >= 0 ? id : NETNSA_NSID_NOT_ASSIGNED;\n\t}\n\n\treturn NETNSA_NSID_NOT_ASSIGNED;\n}\n\n/* should be called with nsid_lock held */\nstatic int __peernet2id(struct net *net, struct net *peer)\n{\n\tbool no = false;\n\n\treturn __peernet2id_alloc(net, peer, &no);\n}\n\nstatic void rtnl_net_notifyid(struct net *net, int cmd, int id);\n/* This function returns the id of a peer netns. If no id is assigned, one will\n * be allocated and returned.\n */\nint peernet2id_alloc(struct net *net, struct net *peer)\n{\n\tbool alloc = false, alive = false;\n\tint id;\n\n\tif (refcount_read(&net->count) == 0)\n\t\treturn NETNSA_NSID_NOT_ASSIGNED;\n\tspin_lock_bh(&net->nsid_lock);\n\t/*\n\t * When peer is obtained from RCU lists, we may race with\n\t * its cleanup. Check whether it's alive, and this guarantees\n\t * we never hash a peer back to net->netns_ids, after it has\n\t * just been idr_remove()'d from there in cleanup_net().\n\t */\n\tif (maybe_get_net(peer))\n\t\talive = alloc = true;\n\tid = __peernet2id_alloc(net, peer, &alloc);\n\tspin_unlock_bh(&net->nsid_lock);\n\tif (alloc && id >= 0)\n\t\trtnl_net_notifyid(net, RTM_NEWNSID, id);\n\tif (alive)\n\t\tput_net(peer);\n\treturn id;\n}\nEXPORT_SYMBOL_GPL(peernet2id_alloc);\n\n/* This function returns, if assigned, the id of a peer netns. */\nint peernet2id(struct net *net, struct net *peer)\n{\n\tint id;\n\n\tspin_lock_bh(&net->nsid_lock);\n\tid = __peernet2id(net, peer);\n\tspin_unlock_bh(&net->nsid_lock);\n\treturn id;\n}\nEXPORT_SYMBOL(peernet2id);\n\n/* This function returns true is the peer netns has an id assigned into the\n * current netns.\n */\nbool peernet_has_id(struct net *net, struct net *peer)\n{\n\treturn peernet2id(net, peer) >= 0;\n}\n\nstruct net *get_net_ns_by_id(struct net *net, int id)\n{\n\tstruct net *peer;\n\n\tif (id < 0)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tpeer = idr_find(&net->netns_ids, id);\n\tif (peer)\n\t\tpeer = maybe_get_net(peer);\n\trcu_read_unlock();\n\n\treturn peer;\n}\n\n/*\n * setup_net runs the initializers for the network namespace object.\n */\nstatic __net_init int setup_net(struct net *net, struct user_namespace *user_ns)\n{\n\t/* Must be called with pernet_ops_rwsem held */\n\tconst struct pernet_operations *ops, *saved_ops;\n\tint error = 0;\n\tLIST_HEAD(net_exit_list);\n\n\trefcount_set(&net->count, 1);\n\trefcount_set(&net->passive, 1);\n\tget_random_bytes(&net->hash_mix, sizeof(u32));\n\tnet->dev_base_seq = 1;\n\tnet->user_ns = user_ns;\n\tidr_init(&net->netns_ids);\n\tspin_lock_init(&net->nsid_lock);\n\tmutex_init(&net->ipv4.ra_mutex);\n\n\tlist_for_each_entry(ops, &pernet_list, list) {\n\t\terror = ops_init(ops, net);\n\t\tif (error < 0)\n\t\t\tgoto out_undo;\n\t}\n\tdown_write(&net_rwsem);\n\tlist_add_tail_rcu(&net->list, &net_namespace_list);\n\tup_write(&net_rwsem);\nout:\n\treturn error;\n\nout_undo:\n\t/* Walk through the list backwards calling the exit functions\n\t * for the pernet modules whose init functions did not fail.\n\t */\n\tlist_add(&net->exit_list, &net_exit_list);\n\tsaved_ops = ops;\n\tlist_for_each_entry_continue_reverse(ops, &pernet_list, list)\n\t\tops_exit_list(ops, &net_exit_list);\n\n\tops = saved_ops;\n\tlist_for_each_entry_continue_reverse(ops, &pernet_list, list)\n\t\tops_free_list(ops, &net_exit_list);\n\n\trcu_barrier();\n\tgoto out;\n}\n\nstatic int __net_init net_defaults_init_net(struct net *net)\n{\n\tnet->core.sysctl_somaxconn = SOMAXCONN;\n\treturn 0;\n}\n\nstatic struct pernet_operations net_defaults_ops = {\n\t.init = net_defaults_init_net,\n};\n\nstatic __init int net_defaults_init(void)\n{\n\tif (register_pernet_subsys(&net_defaults_ops))\n\t\tpanic(\"Cannot initialize net default settings\");\n\n\treturn 0;\n}\n\ncore_initcall(net_defaults_init);\n\n#ifdef CONFIG_NET_NS\nstatic struct ucounts *inc_net_namespaces(struct user_namespace *ns)\n{\n\treturn inc_ucount(ns, current_euid(), UCOUNT_NET_NAMESPACES);\n}\n\nstatic void dec_net_namespaces(struct ucounts *ucounts)\n{\n\tdec_ucount(ucounts, UCOUNT_NET_NAMESPACES);\n}\n\nstatic struct kmem_cache *net_cachep __ro_after_init;\nstatic struct workqueue_struct *netns_wq;\n\nstatic struct net *net_alloc(void)\n{\n\tstruct net *net = NULL;\n\tstruct net_generic *ng;\n\n\tng = net_alloc_generic();\n\tif (!ng)\n\t\tgoto out;\n\n\tnet = kmem_cache_zalloc(net_cachep, GFP_KERNEL);\n\tif (!net)\n\t\tgoto out_free;\n\n\trcu_assign_pointer(net->gen, ng);\nout:\n\treturn net;\n\nout_free:\n\tkfree(ng);\n\tgoto out;\n}\n\nstatic void net_free(struct net *net)\n{\n\tkfree(rcu_access_pointer(net->gen));\n\tkmem_cache_free(net_cachep, net);\n}\n\nvoid net_drop_ns(void *p)\n{\n\tstruct net *ns = p;\n\tif (ns && refcount_dec_and_test(&ns->passive))\n\t\tnet_free(ns);\n}\n\nstruct net *copy_net_ns(unsigned long flags,\n\t\t\tstruct user_namespace *user_ns, struct net *old_net)\n{\n\tstruct ucounts *ucounts;\n\tstruct net *net;\n\tint rv;\n\n\tif (!(flags & CLONE_NEWNET))\n\t\treturn get_net(old_net);\n\n\tucounts = inc_net_namespaces(user_ns);\n\tif (!ucounts)\n\t\treturn ERR_PTR(-ENOSPC);\n\n\tnet = net_alloc();\n\tif (!net) {\n\t\trv = -ENOMEM;\n\t\tgoto dec_ucounts;\n\t}\n\trefcount_set(&net->passive, 1);\n\tnet->ucounts = ucounts;\n\tget_user_ns(user_ns);\n\n\trv = down_read_killable(&pernet_ops_rwsem);\n\tif (rv < 0)\n\t\tgoto put_userns;\n\n\trv = setup_net(net, user_ns);\n\n\tup_read(&pernet_ops_rwsem);\n\n\tif (rv < 0) {\nput_userns:\n\t\tput_user_ns(user_ns);\n\t\tnet_drop_ns(net);\ndec_ucounts:\n\t\tdec_net_namespaces(ucounts);\n\t\treturn ERR_PTR(rv);\n\t}\n\treturn net;\n}\n\n/**\n * net_ns_get_ownership - get sysfs ownership data for @net\n * @net: network namespace in question (can be NULL)\n * @uid: kernel user ID for sysfs objects\n * @gid: kernel group ID for sysfs objects\n *\n * Returns the uid/gid pair of root in the user namespace associated with the\n * given network namespace.\n */\nvoid net_ns_get_ownership(const struct net *net, kuid_t *uid, kgid_t *gid)\n{\n\tif (net) {\n\t\tkuid_t ns_root_uid = make_kuid(net->user_ns, 0);\n\t\tkgid_t ns_root_gid = make_kgid(net->user_ns, 0);\n\n\t\tif (uid_valid(ns_root_uid))\n\t\t\t*uid = ns_root_uid;\n\n\t\tif (gid_valid(ns_root_gid))\n\t\t\t*gid = ns_root_gid;\n\t} else {\n\t\t*uid = GLOBAL_ROOT_UID;\n\t\t*gid = GLOBAL_ROOT_GID;\n\t}\n}\nEXPORT_SYMBOL_GPL(net_ns_get_ownership);\n\nstatic void unhash_nsid(struct net *net, struct net *last)\n{\n\tstruct net *tmp;\n\t/* This function is only called from cleanup_net() work,\n\t * and this work is the only process, that may delete\n\t * a net from net_namespace_list. So, when the below\n\t * is executing, the list may only grow. Thus, we do not\n\t * use for_each_net_rcu() or net_rwsem.\n\t */\n\tfor_each_net(tmp) {\n\t\tint id;\n\n\t\tspin_lock_bh(&tmp->nsid_lock);\n\t\tid = __peernet2id(tmp, net);\n\t\tif (id >= 0)\n\t\t\tidr_remove(&tmp->netns_ids, id);\n\t\tspin_unlock_bh(&tmp->nsid_lock);\n\t\tif (id >= 0)\n\t\t\trtnl_net_notifyid(tmp, RTM_DELNSID, id);\n\t\tif (tmp == last)\n\t\t\tbreak;\n\t}\n\tspin_lock_bh(&net->nsid_lock);\n\tidr_destroy(&net->netns_ids);\n\tspin_unlock_bh(&net->nsid_lock);\n}\n\nstatic LLIST_HEAD(cleanup_list);\n\nstatic void cleanup_net(struct work_struct *work)\n{\n\tconst struct pernet_operations *ops;\n\tstruct net *net, *tmp, *last;\n\tstruct llist_node *net_kill_list;\n\tLIST_HEAD(net_exit_list);\n\n\t/* Atomically snapshot the list of namespaces to cleanup */\n\tnet_kill_list = llist_del_all(&cleanup_list);\n\n\tdown_read(&pernet_ops_rwsem);\n\n\t/* Don't let anyone else find us. */\n\tdown_write(&net_rwsem);\n\tllist_for_each_entry(net, net_kill_list, cleanup_list)\n\t\tlist_del_rcu(&net->list);\n\t/* Cache last net. After we unlock rtnl, no one new net\n\t * added to net_namespace_list can assign nsid pointer\n\t * to a net from net_kill_list (see peernet2id_alloc()).\n\t * So, we skip them in unhash_nsid().\n\t *\n\t * Note, that unhash_nsid() does not delete nsid links\n\t * between net_kill_list's nets, as they've already\n\t * deleted from net_namespace_list. But, this would be\n\t * useless anyway, as netns_ids are destroyed there.\n\t */\n\tlast = list_last_entry(&net_namespace_list, struct net, list);\n\tup_write(&net_rwsem);\n\n\tllist_for_each_entry(net, net_kill_list, cleanup_list) {\n\t\tunhash_nsid(net, last);\n\t\tlist_add_tail(&net->exit_list, &net_exit_list);\n\t}\n\n\t/*\n\t * Another CPU might be rcu-iterating the list, wait for it.\n\t * This needs to be before calling the exit() notifiers, so\n\t * the rcu_barrier() below isn't sufficient alone.\n\t */\n\tsynchronize_rcu();\n\n\t/* Run all of the network namespace exit methods */\n\tlist_for_each_entry_reverse(ops, &pernet_list, list)\n\t\tops_exit_list(ops, &net_exit_list);\n\n\t/* Free the net generic variables */\n\tlist_for_each_entry_reverse(ops, &pernet_list, list)\n\t\tops_free_list(ops, &net_exit_list);\n\n\tup_read(&pernet_ops_rwsem);\n\n\t/* Ensure there are no outstanding rcu callbacks using this\n\t * network namespace.\n\t */\n\trcu_barrier();\n\n\t/* Finally it is safe to free my network namespace structure */\n\tlist_for_each_entry_safe(net, tmp, &net_exit_list, exit_list) {\n\t\tlist_del_init(&net->exit_list);\n\t\tdec_net_namespaces(net->ucounts);\n\t\tput_user_ns(net->user_ns);\n\t\tnet_drop_ns(net);\n\t}\n}\n\n/**\n * net_ns_barrier - wait until concurrent net_cleanup_work is done\n *\n * cleanup_net runs from work queue and will first remove namespaces\n * from the global list, then run net exit functions.\n *\n * Call this in module exit path to make sure that all netns\n * ->exit ops have been invoked before the function is removed.\n */\nvoid net_ns_barrier(void)\n{\n\tdown_write(&pernet_ops_rwsem);\n\tup_write(&pernet_ops_rwsem);\n}\nEXPORT_SYMBOL(net_ns_barrier);\n\nstatic DECLARE_WORK(net_cleanup_work, cleanup_net);\n\nvoid __put_net(struct net *net)\n{\n\t/* Cleanup the network namespace in process context */\n\tif (llist_add(&net->cleanup_list, &cleanup_list))\n\t\tqueue_work(netns_wq, &net_cleanup_work);\n}\nEXPORT_SYMBOL_GPL(__put_net);\n\nstruct net *get_net_ns_by_fd(int fd)\n{\n\tstruct file *file;\n\tstruct ns_common *ns;\n\tstruct net *net;\n\n\tfile = proc_ns_fget(fd);\n\tif (IS_ERR(file))\n\t\treturn ERR_CAST(file);\n\n\tns = get_proc_ns(file_inode(file));\n\tif (ns->ops == &netns_operations)\n\t\tnet = get_net(container_of(ns, struct net, ns));\n\telse\n\t\tnet = ERR_PTR(-EINVAL);\n\n\tfput(file);\n\treturn net;\n}\n\n#else\nstruct net *get_net_ns_by_fd(int fd)\n{\n\treturn ERR_PTR(-EINVAL);\n}\n#endif\nEXPORT_SYMBOL_GPL(get_net_ns_by_fd);\n\nstruct net *get_net_ns_by_pid(pid_t pid)\n{\n\tstruct task_struct *tsk;\n\tstruct net *net;\n\n\t/* Lookup the network namespace */\n\tnet = ERR_PTR(-ESRCH);\n\trcu_read_lock();\n\ttsk = find_task_by_vpid(pid);\n\tif (tsk) {\n\t\tstruct nsproxy *nsproxy;\n\t\ttask_lock(tsk);\n\t\tnsproxy = tsk->nsproxy;\n\t\tif (nsproxy)\n\t\t\tnet = get_net(nsproxy->net_ns);\n\t\ttask_unlock(tsk);\n\t}\n\trcu_read_unlock();\n\treturn net;\n}\nEXPORT_SYMBOL_GPL(get_net_ns_by_pid);\n\nstatic __net_init int net_ns_net_init(struct net *net)\n{\n#ifdef CONFIG_NET_NS\n\tnet->ns.ops = &netns_operations;\n#endif\n\treturn ns_alloc_inum(&net->ns);\n}\n\nstatic __net_exit void net_ns_net_exit(struct net *net)\n{\n\tns_free_inum(&net->ns);\n}\n\nstatic struct pernet_operations __net_initdata net_ns_ops = {\n\t.init = net_ns_net_init,\n\t.exit = net_ns_net_exit,\n};\n\nstatic const struct nla_policy rtnl_net_policy[NETNSA_MAX + 1] = {\n\t[NETNSA_NONE]\t\t= { .type = NLA_UNSPEC },\n\t[NETNSA_NSID]\t\t= { .type = NLA_S32 },\n\t[NETNSA_PID]\t\t= { .type = NLA_U32 },\n\t[NETNSA_FD]\t\t= { .type = NLA_U32 },\n\t[NETNSA_TARGET_NSID]\t= { .type = NLA_S32 },\n};\n\nstatic int rtnl_net_newid(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[NETNSA_MAX + 1];\n\tstruct nlattr *nla;\n\tstruct net *peer;\n\tint nsid, err;\n\n\terr = nlmsg_parse(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,\n\t\t\t  rtnl_net_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\tif (!tb[NETNSA_NSID]) {\n\t\tNL_SET_ERR_MSG(extack, \"nsid is missing\");\n\t\treturn -EINVAL;\n\t}\n\tnsid = nla_get_s32(tb[NETNSA_NSID]);\n\n\tif (tb[NETNSA_PID]) {\n\t\tpeer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));\n\t\tnla = tb[NETNSA_PID];\n\t} else if (tb[NETNSA_FD]) {\n\t\tpeer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));\n\t\tnla = tb[NETNSA_FD];\n\t} else {\n\t\tNL_SET_ERR_MSG(extack, \"Peer netns reference is missing\");\n\t\treturn -EINVAL;\n\t}\n\tif (IS_ERR(peer)) {\n\t\tNL_SET_BAD_ATTR(extack, nla);\n\t\tNL_SET_ERR_MSG(extack, \"Peer netns reference is invalid\");\n\t\treturn PTR_ERR(peer);\n\t}\n\n\tspin_lock_bh(&net->nsid_lock);\n\tif (__peernet2id(net, peer) >= 0) {\n\t\tspin_unlock_bh(&net->nsid_lock);\n\t\terr = -EEXIST;\n\t\tNL_SET_BAD_ATTR(extack, nla);\n\t\tNL_SET_ERR_MSG(extack,\n\t\t\t       \"Peer netns already has a nsid assigned\");\n\t\tgoto out;\n\t}\n\n\terr = alloc_netid(net, peer, nsid);\n\tspin_unlock_bh(&net->nsid_lock);\n\tif (err >= 0) {\n\t\trtnl_net_notifyid(net, RTM_NEWNSID, err);\n\t\terr = 0;\n\t} else if (err == -ENOSPC && nsid >= 0) {\n\t\terr = -EEXIST;\n\t\tNL_SET_BAD_ATTR(extack, tb[NETNSA_NSID]);\n\t\tNL_SET_ERR_MSG(extack, \"The specified nsid is already used\");\n\t}\nout:\n\tput_net(peer);\n\treturn err;\n}\n\nstatic int rtnl_net_get_size(void)\n{\n\treturn NLMSG_ALIGN(sizeof(struct rtgenmsg))\n\t       + nla_total_size(sizeof(s32)) /* NETNSA_NSID */\n\t       + nla_total_size(sizeof(s32)) /* NETNSA_CURRENT_NSID */\n\t       ;\n}\n\nstruct net_fill_args {\n\tu32 portid;\n\tu32 seq;\n\tint flags;\n\tint cmd;\n\tint nsid;\n\tbool add_ref;\n\tint ref_nsid;\n};\n\nstatic int rtnl_net_fill(struct sk_buff *skb, struct net_fill_args *args)\n{\n\tstruct nlmsghdr *nlh;\n\tstruct rtgenmsg *rth;\n\n\tnlh = nlmsg_put(skb, args->portid, args->seq, args->cmd, sizeof(*rth),\n\t\t\targs->flags);\n\tif (!nlh)\n\t\treturn -EMSGSIZE;\n\n\trth = nlmsg_data(nlh);\n\trth->rtgen_family = AF_UNSPEC;\n\n\tif (nla_put_s32(skb, NETNSA_NSID, args->nsid))\n\t\tgoto nla_put_failure;\n\n\tif (args->add_ref &&\n\t    nla_put_s32(skb, NETNSA_CURRENT_NSID, args->ref_nsid))\n\t\tgoto nla_put_failure;\n\n\tnlmsg_end(skb, nlh);\n\treturn 0;\n\nnla_put_failure:\n\tnlmsg_cancel(skb, nlh);\n\treturn -EMSGSIZE;\n}\n\nstatic int rtnl_net_valid_getid_req(struct sk_buff *skb,\n\t\t\t\t    const struct nlmsghdr *nlh,\n\t\t\t\t    struct nlattr **tb,\n\t\t\t\t    struct netlink_ext_ack *extack)\n{\n\tint i, err;\n\n\tif (!netlink_strict_get_check(skb))\n\t\treturn nlmsg_parse(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,\n\t\t\t\t   rtnl_net_policy, extack);\n\n\terr = nlmsg_parse_strict(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,\n\t\t\t\t rtnl_net_policy, extack);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i <= NETNSA_MAX; i++) {\n\t\tif (!tb[i])\n\t\t\tcontinue;\n\n\t\tswitch (i) {\n\t\tcase NETNSA_PID:\n\t\tcase NETNSA_FD:\n\t\tcase NETNSA_NSID:\n\t\tcase NETNSA_TARGET_NSID:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tNL_SET_ERR_MSG(extack, \"Unsupported attribute in peer netns getid request\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int rtnl_net_getid(struct sk_buff *skb, struct nlmsghdr *nlh,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct net *net = sock_net(skb->sk);\n\tstruct nlattr *tb[NETNSA_MAX + 1];\n\tstruct net_fill_args fillargs = {\n\t\t.portid = NETLINK_CB(skb).portid,\n\t\t.seq = nlh->nlmsg_seq,\n\t\t.cmd = RTM_NEWNSID,\n\t};\n\tstruct net *peer, *target = net;\n\tstruct nlattr *nla;\n\tstruct sk_buff *msg;\n\tint err;\n\n\terr = rtnl_net_valid_getid_req(skb, nlh, tb, extack);\n\tif (err < 0)\n\t\treturn err;\n\tif (tb[NETNSA_PID]) {\n\t\tpeer = get_net_ns_by_pid(nla_get_u32(tb[NETNSA_PID]));\n\t\tnla = tb[NETNSA_PID];\n\t} else if (tb[NETNSA_FD]) {\n\t\tpeer = get_net_ns_by_fd(nla_get_u32(tb[NETNSA_FD]));\n\t\tnla = tb[NETNSA_FD];\n\t} else if (tb[NETNSA_NSID]) {\n\t\tpeer = get_net_ns_by_id(net, nla_get_u32(tb[NETNSA_NSID]));\n\t\tif (!peer)\n\t\t\tpeer = ERR_PTR(-ENOENT);\n\t\tnla = tb[NETNSA_NSID];\n\t} else {\n\t\tNL_SET_ERR_MSG(extack, \"Peer netns reference is missing\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (IS_ERR(peer)) {\n\t\tNL_SET_BAD_ATTR(extack, nla);\n\t\tNL_SET_ERR_MSG(extack, \"Peer netns reference is invalid\");\n\t\treturn PTR_ERR(peer);\n\t}\n\n\tif (tb[NETNSA_TARGET_NSID]) {\n\t\tint id = nla_get_s32(tb[NETNSA_TARGET_NSID]);\n\n\t\ttarget = rtnl_get_net_ns_capable(NETLINK_CB(skb).sk, id);\n\t\tif (IS_ERR(target)) {\n\t\t\tNL_SET_BAD_ATTR(extack, tb[NETNSA_TARGET_NSID]);\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Target netns reference is invalid\");\n\t\t\terr = PTR_ERR(target);\n\t\t\tgoto out;\n\t\t}\n\t\tfillargs.add_ref = true;\n\t\tfillargs.ref_nsid = peernet2id(net, peer);\n\t}\n\n\tmsg = nlmsg_new(rtnl_net_get_size(), GFP_KERNEL);\n\tif (!msg) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfillargs.nsid = peernet2id(target, peer);\n\terr = rtnl_net_fill(msg, &fillargs);\n\tif (err < 0)\n\t\tgoto err_out;\n\n\terr = rtnl_unicast(msg, net, NETLINK_CB(skb).portid);\n\tgoto out;\n\nerr_out:\n\tnlmsg_free(msg);\nout:\n\tif (fillargs.add_ref)\n\t\tput_net(target);\n\tput_net(peer);\n\treturn err;\n}\n\nstruct rtnl_net_dump_cb {\n\tstruct net *tgt_net;\n\tstruct net *ref_net;\n\tstruct sk_buff *skb;\n\tstruct net_fill_args fillargs;\n\tint idx;\n\tint s_idx;\n};\n\nstatic int rtnl_net_dumpid_one(int id, void *peer, void *data)\n{\n\tstruct rtnl_net_dump_cb *net_cb = (struct rtnl_net_dump_cb *)data;\n\tint ret;\n\n\tif (net_cb->idx < net_cb->s_idx)\n\t\tgoto cont;\n\n\tnet_cb->fillargs.nsid = id;\n\tif (net_cb->fillargs.add_ref)\n\t\tnet_cb->fillargs.ref_nsid = __peernet2id(net_cb->ref_net, peer);\n\tret = rtnl_net_fill(net_cb->skb, &net_cb->fillargs);\n\tif (ret < 0)\n\t\treturn ret;\n\ncont:\n\tnet_cb->idx++;\n\treturn 0;\n}\n\nstatic int rtnl_valid_dump_net_req(const struct nlmsghdr *nlh, struct sock *sk,\n\t\t\t\t   struct rtnl_net_dump_cb *net_cb,\n\t\t\t\t   struct netlink_callback *cb)\n{\n\tstruct netlink_ext_ack *extack = cb->extack;\n\tstruct nlattr *tb[NETNSA_MAX + 1];\n\tint err, i;\n\n\terr = nlmsg_parse_strict(nlh, sizeof(struct rtgenmsg), tb, NETNSA_MAX,\n\t\t\t\t rtnl_net_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\tfor (i = 0; i <= NETNSA_MAX; i++) {\n\t\tif (!tb[i])\n\t\t\tcontinue;\n\n\t\tif (i == NETNSA_TARGET_NSID) {\n\t\t\tstruct net *net;\n\n\t\t\tnet = rtnl_get_net_ns_capable(sk, nla_get_s32(tb[i]));\n\t\t\tif (IS_ERR(net)) {\n\t\t\t\tNL_SET_BAD_ATTR(extack, tb[i]);\n\t\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t\t       \"Invalid target network namespace id\");\n\t\t\t\treturn PTR_ERR(net);\n\t\t\t}\n\t\t\tnet_cb->fillargs.add_ref = true;\n\t\t\tnet_cb->ref_net = net_cb->tgt_net;\n\t\t\tnet_cb->tgt_net = net;\n\t\t} else {\n\t\t\tNL_SET_BAD_ATTR(extack, tb[i]);\n\t\t\tNL_SET_ERR_MSG(extack,\n\t\t\t\t       \"Unsupported attribute in dump request\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int rtnl_net_dumpid(struct sk_buff *skb, struct netlink_callback *cb)\n{\n\tstruct rtnl_net_dump_cb net_cb = {\n\t\t.tgt_net = sock_net(skb->sk),\n\t\t.skb = skb,\n\t\t.fillargs = {\n\t\t\t.portid = NETLINK_CB(cb->skb).portid,\n\t\t\t.seq = cb->nlh->nlmsg_seq,\n\t\t\t.flags = NLM_F_MULTI,\n\t\t\t.cmd = RTM_NEWNSID,\n\t\t},\n\t\t.idx = 0,\n\t\t.s_idx = cb->args[0],\n\t};\n\tint err = 0;\n\n\tif (cb->strict_check) {\n\t\terr = rtnl_valid_dump_net_req(cb->nlh, skb->sk, &net_cb, cb);\n\t\tif (err < 0)\n\t\t\tgoto end;\n\t}\n\n\tspin_lock_bh(&net_cb.tgt_net->nsid_lock);\n\tif (net_cb.fillargs.add_ref &&\n\t    !net_eq(net_cb.ref_net, net_cb.tgt_net) &&\n\t    !spin_trylock_bh(&net_cb.ref_net->nsid_lock)) {\n\t\tspin_unlock_bh(&net_cb.tgt_net->nsid_lock);\n\t\terr = -EAGAIN;\n\t\tgoto end;\n\t}\n\tidr_for_each(&net_cb.tgt_net->netns_ids, rtnl_net_dumpid_one, &net_cb);\n\tif (net_cb.fillargs.add_ref &&\n\t    !net_eq(net_cb.ref_net, net_cb.tgt_net))\n\t\tspin_unlock_bh(&net_cb.ref_net->nsid_lock);\n\tspin_unlock_bh(&net_cb.tgt_net->nsid_lock);\n\n\tcb->args[0] = net_cb.idx;\nend:\n\tif (net_cb.fillargs.add_ref)\n\t\tput_net(net_cb.tgt_net);\n\treturn err < 0 ? err : skb->len;\n}\n\nstatic void rtnl_net_notifyid(struct net *net, int cmd, int id)\n{\n\tstruct net_fill_args fillargs = {\n\t\t.cmd = cmd,\n\t\t.nsid = id,\n\t};\n\tstruct sk_buff *msg;\n\tint err = -ENOMEM;\n\n\tmsg = nlmsg_new(rtnl_net_get_size(), GFP_KERNEL);\n\tif (!msg)\n\t\tgoto out;\n\n\terr = rtnl_net_fill(msg, &fillargs);\n\tif (err < 0)\n\t\tgoto err_out;\n\n\trtnl_notify(msg, net, 0, RTNLGRP_NSID, NULL, 0);\n\treturn;\n\nerr_out:\n\tnlmsg_free(msg);\nout:\n\trtnl_set_sk_err(net, RTNLGRP_NSID, err);\n}\n\nstatic int __init net_ns_init(void)\n{\n\tstruct net_generic *ng;\n\n#ifdef CONFIG_NET_NS\n\tnet_cachep = kmem_cache_create(\"net_namespace\", sizeof(struct net),\n\t\t\t\t\tSMP_CACHE_BYTES,\n\t\t\t\t\tSLAB_PANIC|SLAB_ACCOUNT, NULL);\n\n\t/* Create workqueue for cleanup */\n\tnetns_wq = create_singlethread_workqueue(\"netns\");\n\tif (!netns_wq)\n\t\tpanic(\"Could not create netns workq\");\n#endif\n\n\tng = net_alloc_generic();\n\tif (!ng)\n\t\tpanic(\"Could not allocate generic netns\");\n\n\trcu_assign_pointer(init_net.gen, ng);\n\n\tdown_write(&pernet_ops_rwsem);\n\tif (setup_net(&init_net, &init_user_ns))\n\t\tpanic(\"Could not setup the initial network namespace\");\n\n\tinit_net_initialized = true;\n\tup_write(&pernet_ops_rwsem);\n\n\tif (register_pernet_subsys(&net_ns_ops))\n\t\tpanic(\"Could not register network namespace subsystems\");\n\n\trtnl_register(PF_UNSPEC, RTM_NEWNSID, rtnl_net_newid, NULL,\n\t\t      RTNL_FLAG_DOIT_UNLOCKED);\n\trtnl_register(PF_UNSPEC, RTM_GETNSID, rtnl_net_getid, rtnl_net_dumpid,\n\t\t      RTNL_FLAG_DOIT_UNLOCKED);\n\n\treturn 0;\n}\n\npure_initcall(net_ns_init);\n\n#ifdef CONFIG_NET_NS\nstatic int __register_pernet_operations(struct list_head *list,\n\t\t\t\t\tstruct pernet_operations *ops)\n{\n\tstruct net *net;\n\tint error;\n\tLIST_HEAD(net_exit_list);\n\n\tlist_add_tail(&ops->list, list);\n\tif (ops->init || (ops->id && ops->size)) {\n\t\t/* We held write locked pernet_ops_rwsem, and parallel\n\t\t * setup_net() and cleanup_net() are not possible.\n\t\t */\n\t\tfor_each_net(net) {\n\t\t\terror = ops_init(ops, net);\n\t\t\tif (error)\n\t\t\t\tgoto out_undo;\n\t\t\tlist_add_tail(&net->exit_list, &net_exit_list);\n\t\t}\n\t}\n\treturn 0;\n\nout_undo:\n\t/* If I have an error cleanup all namespaces I initialized */\n\tlist_del(&ops->list);\n\tops_exit_list(ops, &net_exit_list);\n\tops_free_list(ops, &net_exit_list);\n\treturn error;\n}\n\nstatic void __unregister_pernet_operations(struct pernet_operations *ops)\n{\n\tstruct net *net;\n\tLIST_HEAD(net_exit_list);\n\n\tlist_del(&ops->list);\n\t/* See comment in __register_pernet_operations() */\n\tfor_each_net(net)\n\t\tlist_add_tail(&net->exit_list, &net_exit_list);\n\tops_exit_list(ops, &net_exit_list);\n\tops_free_list(ops, &net_exit_list);\n}\n\n#else\n\nstatic int __register_pernet_operations(struct list_head *list,\n\t\t\t\t\tstruct pernet_operations *ops)\n{\n\tif (!init_net_initialized) {\n\t\tlist_add_tail(&ops->list, list);\n\t\treturn 0;\n\t}\n\n\treturn ops_init(ops, &init_net);\n}\n\nstatic void __unregister_pernet_operations(struct pernet_operations *ops)\n{\n\tif (!init_net_initialized) {\n\t\tlist_del(&ops->list);\n\t} else {\n\t\tLIST_HEAD(net_exit_list);\n\t\tlist_add(&init_net.exit_list, &net_exit_list);\n\t\tops_exit_list(ops, &net_exit_list);\n\t\tops_free_list(ops, &net_exit_list);\n\t}\n}\n\n#endif /* CONFIG_NET_NS */\n\nstatic DEFINE_IDA(net_generic_ids);\n\nstatic int register_pernet_operations(struct list_head *list,\n\t\t\t\t      struct pernet_operations *ops)\n{\n\tint error;\n\n\tif (ops->id) {\n\t\terror = ida_alloc_min(&net_generic_ids, MIN_PERNET_OPS_ID,\n\t\t\t\tGFP_KERNEL);\n\t\tif (error < 0)\n\t\t\treturn error;\n\t\t*ops->id = error;\n\t\tmax_gen_ptrs = max(max_gen_ptrs, *ops->id + 1);\n\t}\n\terror = __register_pernet_operations(list, ops);\n\tif (error) {\n\t\trcu_barrier();\n\t\tif (ops->id)\n\t\t\tida_free(&net_generic_ids, *ops->id);\n\t}\n\n\treturn error;\n}\n\nstatic void unregister_pernet_operations(struct pernet_operations *ops)\n{\n\t__unregister_pernet_operations(ops);\n\trcu_barrier();\n\tif (ops->id)\n\t\tida_free(&net_generic_ids, *ops->id);\n}\n\n/**\n *      register_pernet_subsys - register a network namespace subsystem\n *\t@ops:  pernet operations structure for the subsystem\n *\n *\tRegister a subsystem which has init and exit functions\n *\tthat are called when network namespaces are created and\n *\tdestroyed respectively.\n *\n *\tWhen registered all network namespace init functions are\n *\tcalled for every existing network namespace.  Allowing kernel\n *\tmodules to have a race free view of the set of network namespaces.\n *\n *\tWhen a new network namespace is created all of the init\n *\tmethods are called in the order in which they were registered.\n *\n *\tWhen a network namespace is destroyed all of the exit methods\n *\tare called in the reverse of the order with which they were\n *\tregistered.\n */\nint register_pernet_subsys(struct pernet_operations *ops)\n{\n\tint error;\n\tdown_write(&pernet_ops_rwsem);\n\terror =  register_pernet_operations(first_device, ops);\n\tup_write(&pernet_ops_rwsem);\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(register_pernet_subsys);\n\n/**\n *      unregister_pernet_subsys - unregister a network namespace subsystem\n *\t@ops: pernet operations structure to manipulate\n *\n *\tRemove the pernet operations structure from the list to be\n *\tused when network namespaces are created or destroyed.  In\n *\taddition run the exit method for all existing network\n *\tnamespaces.\n */\nvoid unregister_pernet_subsys(struct pernet_operations *ops)\n{\n\tdown_write(&pernet_ops_rwsem);\n\tunregister_pernet_operations(ops);\n\tup_write(&pernet_ops_rwsem);\n}\nEXPORT_SYMBOL_GPL(unregister_pernet_subsys);\n\n/**\n *      register_pernet_device - register a network namespace device\n *\t@ops:  pernet operations structure for the subsystem\n *\n *\tRegister a device which has init and exit functions\n *\tthat are called when network namespaces are created and\n *\tdestroyed respectively.\n *\n *\tWhen registered all network namespace init functions are\n *\tcalled for every existing network namespace.  Allowing kernel\n *\tmodules to have a race free view of the set of network namespaces.\n *\n *\tWhen a new network namespace is created all of the init\n *\tmethods are called in the order in which they were registered.\n *\n *\tWhen a network namespace is destroyed all of the exit methods\n *\tare called in the reverse of the order with which they were\n *\tregistered.\n */\nint register_pernet_device(struct pernet_operations *ops)\n{\n\tint error;\n\tdown_write(&pernet_ops_rwsem);\n\terror = register_pernet_operations(&pernet_list, ops);\n\tif (!error && (first_device == &pernet_list))\n\t\tfirst_device = &ops->list;\n\tup_write(&pernet_ops_rwsem);\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(register_pernet_device);\n\n/**\n *      unregister_pernet_device - unregister a network namespace netdevice\n *\t@ops: pernet operations structure to manipulate\n *\n *\tRemove the pernet operations structure from the list to be\n *\tused when network namespaces are created or destroyed.  In\n *\taddition run the exit method for all existing network\n *\tnamespaces.\n */\nvoid unregister_pernet_device(struct pernet_operations *ops)\n{\n\tdown_write(&pernet_ops_rwsem);\n\tif (&ops->list == first_device)\n\t\tfirst_device = first_device->next;\n\tunregister_pernet_operations(ops);\n\tup_write(&pernet_ops_rwsem);\n}\nEXPORT_SYMBOL_GPL(unregister_pernet_device);\n\n#ifdef CONFIG_NET_NS\nstatic struct ns_common *netns_get(struct task_struct *task)\n{\n\tstruct net *net = NULL;\n\tstruct nsproxy *nsproxy;\n\n\ttask_lock(task);\n\tnsproxy = task->nsproxy;\n\tif (nsproxy)\n\t\tnet = get_net(nsproxy->net_ns);\n\ttask_unlock(task);\n\n\treturn net ? &net->ns : NULL;\n}\n\nstatic inline struct net *to_net_ns(struct ns_common *ns)\n{\n\treturn container_of(ns, struct net, ns);\n}\n\nstatic void netns_put(struct ns_common *ns)\n{\n\tput_net(to_net_ns(ns));\n}\n\nstatic int netns_install(struct nsproxy *nsproxy, struct ns_common *ns)\n{\n\tstruct net *net = to_net_ns(ns);\n\n\tif (!ns_capable(net->user_ns, CAP_SYS_ADMIN) ||\n\t    !ns_capable(current_user_ns(), CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tput_net(nsproxy->net_ns);\n\tnsproxy->net_ns = get_net(net);\n\treturn 0;\n}\n\nstatic struct user_namespace *netns_owner(struct ns_common *ns)\n{\n\treturn to_net_ns(ns)->user_ns;\n}\n\nconst struct proc_ns_operations netns_operations = {\n\t.name\t\t= \"net\",\n\t.type\t\t= CLONE_NEWNET,\n\t.get\t\t= netns_get,\n\t.put\t\t= netns_put,\n\t.install\t= netns_install,\n\t.owner\t\t= netns_owner,\n};\n#endif\n"], "filenames": ["include/net/net_namespace.h", "include/net/netns/hash.h", "net/core/net_namespace.c"], "buggy_code_start_loc": [61, 5, 306], "buggy_code_end_loc": [61, 16, 306], "fixing_code_start_loc": [62, 5, 307], "fixing_code_end_loc": [63, 10, 308], "type": "CWE-326", "message": "The Linux kernel 4.x (starting from 4.1) and 5.x before 5.0.8 allows Information Exposure (partial kernel address disclosure), leading to a KASLR bypass. Specifically, it is possible to extract the KASLR kernel image offset using the IP ID values the kernel produces for connection-less protocols (e.g., UDP and ICMP). When such traffic is sent to multiple destination IP addresses, it is possible to obtain hash collisions (of indices to the counter array) and thereby obtain the hashing key (via enumeration). This key contains enough bits from a kernel address (of a static variable) so when the key is extracted (via enumeration), the offset of the kernel image is exposed. This attack can be carried out remotely, by the attacker forcing the target device to send UDP or ICMP (or certain other) traffic to attacker-controlled IP addresses. Forcing a server to send UDP traffic is trivial if the server is a DNS server. ICMP traffic is trivial if the server answers ICMP Echo requests (ping). For client targets, if the target visits the attacker's web page, then WebRTC or gQUIC can be used to force UDP traffic to attacker-controlled IP addresses. NOTE: this attack against KASLR became viable in 4.1 because IP ID generation was changed to have a dependency on an address associated with a network namespace.", "other": {"cve": {"id": "CVE-2019-10639", "sourceIdentifier": "cve@mitre.org", "published": "2019-07-05T23:15:10.477", "lastModified": "2021-06-14T18:15:15.397", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The Linux kernel 4.x (starting from 4.1) and 5.x before 5.0.8 allows Information Exposure (partial kernel address disclosure), leading to a KASLR bypass. Specifically, it is possible to extract the KASLR kernel image offset using the IP ID values the kernel produces for connection-less protocols (e.g., UDP and ICMP). When such traffic is sent to multiple destination IP addresses, it is possible to obtain hash collisions (of indices to the counter array) and thereby obtain the hashing key (via enumeration). This key contains enough bits from a kernel address (of a static variable) so when the key is extracted (via enumeration), the offset of the kernel image is exposed. This attack can be carried out remotely, by the attacker forcing the target device to send UDP or ICMP (or certain other) traffic to attacker-controlled IP addresses. Forcing a server to send UDP traffic is trivial if the server is a DNS server. ICMP traffic is trivial if the server answers ICMP Echo requests (ping). For client targets, if the target visits the attacker's web page, then WebRTC or gQUIC can be used to force UDP traffic to attacker-controlled IP addresses. NOTE: this attack against KASLR became viable in 4.1 because IP ID generation was changed to have a dependency on an address associated with a network namespace."}, {"lang": "es", "value": "El kernel de Linux versi\u00f3n 4.x (a partir de versi\u00f3n 4.1) y versi\u00f3n 5.x anterior a 5.0.8, permite la Exposici\u00f3n de Informaci\u00f3n (divulgaci\u00f3n parcial de la direcci\u00f3n del kernel), lo que conlleva a una omisi\u00f3n de la funcionalidad KASLR. Espec\u00edficamente, es posible extraer el desplazamiento de la imagen del kernel KASLR utilizando los valores ID de IP que el kernel produce para los protocolos sin conexi\u00f3n (por ejemplo, UDP e ICMP). Cuando dicho tr\u00e1fico se env\u00eda a m\u00faltiples direcciones IP de destino, es posible obtener colisiones de hash (de \u00edndices en la matriz counter) y, por lo tanto, obtener la clave de hashing (mediante enumeraci\u00f3n). Esta clave contiene suficientes bits de una direcci\u00f3n de kernel (de una variable est\u00e1tica) de manera que cuando se extrae la clave (por medio de la enumeraci\u00f3n), queda expuesto el desplazamiento de la imagen del kernel. Este ataque se puede llevar a cabo remotamente, ya que el atacante forza al dispositivo de destino a enviar tr\u00e1fico UDP o ICMP (o algunos otros) hacia direcciones IP controladas por el atacante. Forzar a un servidor a enviar tr\u00e1fico UDP es trivial si el servidor es un servidor DNS. El tr\u00e1fico de ICMP es trivial si el servidor responde a las peticiones de Eco de ICMP (ping). Para los objetivos cliente, si el objetivo visita la p\u00e1gina web del atacante, entonces puede ser usado WebRTC o gQUIC para forzar el tr\u00e1fico UDP hacia las direcciones IP controladas por el atacante. NOTA: este ataque contra KASLR se volvi\u00f3 viable en versi\u00f3n 4.1 porque la generaci\u00f3n de ID de IP fue cambiada para tener una dependencia en una direcci\u00f3n asociada con un espacio de nombre de red."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:P/I:N/A:N", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "PARTIAL", "integrityImpact": "NONE", "availabilityImpact": "NONE", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-326"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "4.1", "versionEndIncluding": "4.20.9", "matchCriteriaId": "ED11DE6F-3BE1-4212-A97A-BB052A591CB2"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionStartIncluding": "5.0", "versionEndExcluding": "5.0.8", "matchCriteriaId": "B8C3EB64-4B85-4C0E-B9BD-5342B604A466"}]}]}], "references": [{"url": "http://lists.opensuse.org/opensuse-security-announce/2019-07/msg00014.html", "source": "cve@mitre.org"}, {"url": "http://lists.opensuse.org/opensuse-security-announce/2019-07/msg00025.html", "source": "cve@mitre.org"}, {"url": "https://arxiv.org/pdf/1906.10478.pdf", "source": "cve@mitre.org", "tags": ["Third Party Advisory"]}, {"url": "https://cdn.kernel.org/pub/linux/kernel/v5.x/ChangeLog-5.0.8", "source": "cve@mitre.org", "tags": ["Mailing List", "Release Notes", "Vendor Advisory"]}, {"url": "https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=355b98553789b646ed97ad801a619ff898471b92", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Vendor Advisory"]}, {"url": "https://github.com/torvalds/linux/commit/355b98553789b646ed97ad801a619ff898471b92", "source": "cve@mitre.org", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2019/07/msg00022.html", "source": "cve@mitre.org"}, {"url": "https://lists.debian.org/debian-lts-announce/2019/08/msg00017.html", "source": "cve@mitre.org"}, {"url": "https://seclists.org/bugtraq/2019/Aug/18", "source": "cve@mitre.org"}, {"url": "https://security.netapp.com/advisory/ntap-20190806-0001/", "source": "cve@mitre.org"}, {"url": "https://support.f5.com/csp/article/K32804955", "source": "cve@mitre.org"}, {"url": "https://support.f5.com/csp/article/K32804955?utm_source=f5support&amp;utm_medium=RSS", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4115-1/", "source": "cve@mitre.org"}, {"url": "https://usn.ubuntu.com/4118-1/", "source": "cve@mitre.org"}, {"url": "https://www.debian.org/security/2019/dsa-4497", "source": "cve@mitre.org"}, {"url": "https://www.oracle.com/security-alerts/cpuApr2021.html", "source": "cve@mitre.org"}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/355b98553789b646ed97ad801a619ff898471b92"}}