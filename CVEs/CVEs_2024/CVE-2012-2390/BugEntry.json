{"buggy_code": ["/*\n * Generic hugetlb support.\n * (C) William Irwin, April 2004\n */\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/seq_file.h>\n#include <linux/sysctl.h>\n#include <linux/highmem.h>\n#include <linux/mmu_notifier.h>\n#include <linux/nodemask.h>\n#include <linux/pagemap.h>\n#include <linux/mempolicy.h>\n#include <linux/cpuset.h>\n#include <linux/mutex.h>\n#include <linux/bootmem.h>\n#include <linux/sysfs.h>\n#include <linux/slab.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <linux/io.h>\n\n#include <linux/hugetlb.h>\n#include <linux/node.h>\n#include \"internal.h\"\n\nconst unsigned long hugetlb_zero = 0, hugetlb_infinity = ~0UL;\nstatic gfp_t htlb_alloc_mask = GFP_HIGHUSER;\nunsigned long hugepages_treat_as_movable;\n\nstatic int max_hstate;\nunsigned int default_hstate_idx;\nstruct hstate hstates[HUGE_MAX_HSTATE];\n\n__initdata LIST_HEAD(huge_boot_pages);\n\n/* for command line parsing */\nstatic struct hstate * __initdata parsed_hstate;\nstatic unsigned long __initdata default_hstate_max_huge_pages;\nstatic unsigned long __initdata default_hstate_size;\n\n#define for_each_hstate(h) \\\n\tfor ((h) = hstates; (h) < &hstates[max_hstate]; (h)++)\n\n/*\n * Protects updates to hugepage_freelists, nr_huge_pages, and free_huge_pages\n */\nstatic DEFINE_SPINLOCK(hugetlb_lock);\n\nstatic inline void unlock_or_release_subpool(struct hugepage_subpool *spool)\n{\n\tbool free = (spool->count == 0) && (spool->used_hpages == 0);\n\n\tspin_unlock(&spool->lock);\n\n\t/* If no pages are used, and no other handles to the subpool\n\t * remain, free the subpool the subpool remain */\n\tif (free)\n\t\tkfree(spool);\n}\n\nstruct hugepage_subpool *hugepage_new_subpool(long nr_blocks)\n{\n\tstruct hugepage_subpool *spool;\n\n\tspool = kmalloc(sizeof(*spool), GFP_KERNEL);\n\tif (!spool)\n\t\treturn NULL;\n\n\tspin_lock_init(&spool->lock);\n\tspool->count = 1;\n\tspool->max_hpages = nr_blocks;\n\tspool->used_hpages = 0;\n\n\treturn spool;\n}\n\nvoid hugepage_put_subpool(struct hugepage_subpool *spool)\n{\n\tspin_lock(&spool->lock);\n\tBUG_ON(!spool->count);\n\tspool->count--;\n\tunlock_or_release_subpool(spool);\n}\n\nstatic int hugepage_subpool_get_pages(struct hugepage_subpool *spool,\n\t\t\t\t      long delta)\n{\n\tint ret = 0;\n\n\tif (!spool)\n\t\treturn 0;\n\n\tspin_lock(&spool->lock);\n\tif ((spool->used_hpages + delta) <= spool->max_hpages) {\n\t\tspool->used_hpages += delta;\n\t} else {\n\t\tret = -ENOMEM;\n\t}\n\tspin_unlock(&spool->lock);\n\n\treturn ret;\n}\n\nstatic void hugepage_subpool_put_pages(struct hugepage_subpool *spool,\n\t\t\t\t       long delta)\n{\n\tif (!spool)\n\t\treturn;\n\n\tspin_lock(&spool->lock);\n\tspool->used_hpages -= delta;\n\t/* If hugetlbfs_put_super couldn't free spool due to\n\t* an outstanding quota reference, free it now. */\n\tunlock_or_release_subpool(spool);\n}\n\nstatic inline struct hugepage_subpool *subpool_inode(struct inode *inode)\n{\n\treturn HUGETLBFS_SB(inode->i_sb)->spool;\n}\n\nstatic inline struct hugepage_subpool *subpool_vma(struct vm_area_struct *vma)\n{\n\treturn subpool_inode(vma->vm_file->f_dentry->d_inode);\n}\n\n/*\n * Region tracking -- allows tracking of reservations and instantiated pages\n *                    across the pages in a mapping.\n *\n * The region data structures are protected by a combination of the mmap_sem\n * and the hugetlb_instantion_mutex.  To access or modify a region the caller\n * must either hold the mmap_sem for write, or the mmap_sem for read and\n * the hugetlb_instantiation mutex:\n *\n *\tdown_write(&mm->mmap_sem);\n * or\n *\tdown_read(&mm->mmap_sem);\n *\tmutex_lock(&hugetlb_instantiation_mutex);\n */\nstruct file_region {\n\tstruct list_head link;\n\tlong from;\n\tlong to;\n};\n\nstatic long region_add(struct list_head *head, long f, long t)\n{\n\tstruct file_region *rg, *nrg, *trg;\n\n\t/* Locate the region we are either in or before. */\n\tlist_for_each_entry(rg, head, link)\n\t\tif (f <= rg->to)\n\t\t\tbreak;\n\n\t/* Round our left edge to the current segment if it encloses us. */\n\tif (f > rg->from)\n\t\tf = rg->from;\n\n\t/* Check for and consume any regions we now overlap with. */\n\tnrg = rg;\n\tlist_for_each_entry_safe(rg, trg, rg->link.prev, link) {\n\t\tif (&rg->link == head)\n\t\t\tbreak;\n\t\tif (rg->from > t)\n\t\t\tbreak;\n\n\t\t/* If this area reaches higher then extend our area to\n\t\t * include it completely.  If this is not the first area\n\t\t * which we intend to reuse, free it. */\n\t\tif (rg->to > t)\n\t\t\tt = rg->to;\n\t\tif (rg != nrg) {\n\t\t\tlist_del(&rg->link);\n\t\t\tkfree(rg);\n\t\t}\n\t}\n\tnrg->from = f;\n\tnrg->to = t;\n\treturn 0;\n}\n\nstatic long region_chg(struct list_head *head, long f, long t)\n{\n\tstruct file_region *rg, *nrg;\n\tlong chg = 0;\n\n\t/* Locate the region we are before or in. */\n\tlist_for_each_entry(rg, head, link)\n\t\tif (f <= rg->to)\n\t\t\tbreak;\n\n\t/* If we are below the current region then a new region is required.\n\t * Subtle, allocate a new region at the position but make it zero\n\t * size such that we can guarantee to record the reservation. */\n\tif (&rg->link == head || t < rg->from) {\n\t\tnrg = kmalloc(sizeof(*nrg), GFP_KERNEL);\n\t\tif (!nrg)\n\t\t\treturn -ENOMEM;\n\t\tnrg->from = f;\n\t\tnrg->to   = f;\n\t\tINIT_LIST_HEAD(&nrg->link);\n\t\tlist_add(&nrg->link, rg->link.prev);\n\n\t\treturn t - f;\n\t}\n\n\t/* Round our left edge to the current segment if it encloses us. */\n\tif (f > rg->from)\n\t\tf = rg->from;\n\tchg = t - f;\n\n\t/* Check for and consume any regions we now overlap with. */\n\tlist_for_each_entry(rg, rg->link.prev, link) {\n\t\tif (&rg->link == head)\n\t\t\tbreak;\n\t\tif (rg->from > t)\n\t\t\treturn chg;\n\n\t\t/* We overlap with this area, if it extends further than\n\t\t * us then we must extend ourselves.  Account for its\n\t\t * existing reservation. */\n\t\tif (rg->to > t) {\n\t\t\tchg += rg->to - t;\n\t\t\tt = rg->to;\n\t\t}\n\t\tchg -= rg->to - rg->from;\n\t}\n\treturn chg;\n}\n\nstatic long region_truncate(struct list_head *head, long end)\n{\n\tstruct file_region *rg, *trg;\n\tlong chg = 0;\n\n\t/* Locate the region we are either in or before. */\n\tlist_for_each_entry(rg, head, link)\n\t\tif (end <= rg->to)\n\t\t\tbreak;\n\tif (&rg->link == head)\n\t\treturn 0;\n\n\t/* If we are in the middle of a region then adjust it. */\n\tif (end > rg->from) {\n\t\tchg = rg->to - end;\n\t\trg->to = end;\n\t\trg = list_entry(rg->link.next, typeof(*rg), link);\n\t}\n\n\t/* Drop any remaining regions. */\n\tlist_for_each_entry_safe(rg, trg, rg->link.prev, link) {\n\t\tif (&rg->link == head)\n\t\t\tbreak;\n\t\tchg += rg->to - rg->from;\n\t\tlist_del(&rg->link);\n\t\tkfree(rg);\n\t}\n\treturn chg;\n}\n\nstatic long region_count(struct list_head *head, long f, long t)\n{\n\tstruct file_region *rg;\n\tlong chg = 0;\n\n\t/* Locate each segment we overlap with, and count that overlap. */\n\tlist_for_each_entry(rg, head, link) {\n\t\tlong seg_from;\n\t\tlong seg_to;\n\n\t\tif (rg->to <= f)\n\t\t\tcontinue;\n\t\tif (rg->from >= t)\n\t\t\tbreak;\n\n\t\tseg_from = max(rg->from, f);\n\t\tseg_to = min(rg->to, t);\n\n\t\tchg += seg_to - seg_from;\n\t}\n\n\treturn chg;\n}\n\n/*\n * Convert the address within this vma to the page offset within\n * the mapping, in pagecache page units; huge pages here.\n */\nstatic pgoff_t vma_hugecache_offset(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\treturn ((address - vma->vm_start) >> huge_page_shift(h)) +\n\t\t\t(vma->vm_pgoff >> huge_page_order(h));\n}\n\npgoff_t linear_hugepage_index(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long address)\n{\n\treturn vma_hugecache_offset(hstate_vma(vma), vma, address);\n}\n\n/*\n * Return the size of the pages allocated when backing a VMA. In the majority\n * cases this will be same size as used by the page table entries.\n */\nunsigned long vma_kernel_pagesize(struct vm_area_struct *vma)\n{\n\tstruct hstate *hstate;\n\n\tif (!is_vm_hugetlb_page(vma))\n\t\treturn PAGE_SIZE;\n\n\thstate = hstate_vma(vma);\n\n\treturn 1UL << (hstate->order + PAGE_SHIFT);\n}\nEXPORT_SYMBOL_GPL(vma_kernel_pagesize);\n\n/*\n * Return the page size being used by the MMU to back a VMA. In the majority\n * of cases, the page size used by the kernel matches the MMU size. On\n * architectures where it differs, an architecture-specific version of this\n * function is required.\n */\n#ifndef vma_mmu_pagesize\nunsigned long vma_mmu_pagesize(struct vm_area_struct *vma)\n{\n\treturn vma_kernel_pagesize(vma);\n}\n#endif\n\n/*\n * Flags for MAP_PRIVATE reservations.  These are stored in the bottom\n * bits of the reservation map pointer, which are always clear due to\n * alignment.\n */\n#define HPAGE_RESV_OWNER    (1UL << 0)\n#define HPAGE_RESV_UNMAPPED (1UL << 1)\n#define HPAGE_RESV_MASK (HPAGE_RESV_OWNER | HPAGE_RESV_UNMAPPED)\n\n/*\n * These helpers are used to track how many pages are reserved for\n * faults in a MAP_PRIVATE mapping. Only the process that called mmap()\n * is guaranteed to have their future faults succeed.\n *\n * With the exception of reset_vma_resv_huge_pages() which is called at fork(),\n * the reserve counters are updated with the hugetlb_lock held. It is safe\n * to reset the VMA at fork() time as it is not in use yet and there is no\n * chance of the global counters getting corrupted as a result of the values.\n *\n * The private mapping reservation is represented in a subtly different\n * manner to a shared mapping.  A shared mapping has a region map associated\n * with the underlying file, this region map represents the backing file\n * pages which have ever had a reservation assigned which this persists even\n * after the page is instantiated.  A private mapping has a region map\n * associated with the original mmap which is attached to all VMAs which\n * reference it, this region map represents those offsets which have consumed\n * reservation ie. where pages have been instantiated.\n */\nstatic unsigned long get_vma_private_data(struct vm_area_struct *vma)\n{\n\treturn (unsigned long)vma->vm_private_data;\n}\n\nstatic void set_vma_private_data(struct vm_area_struct *vma,\n\t\t\t\t\t\t\tunsigned long value)\n{\n\tvma->vm_private_data = (void *)value;\n}\n\nstruct resv_map {\n\tstruct kref refs;\n\tstruct list_head regions;\n};\n\nstatic struct resv_map *resv_map_alloc(void)\n{\n\tstruct resv_map *resv_map = kmalloc(sizeof(*resv_map), GFP_KERNEL);\n\tif (!resv_map)\n\t\treturn NULL;\n\n\tkref_init(&resv_map->refs);\n\tINIT_LIST_HEAD(&resv_map->regions);\n\n\treturn resv_map;\n}\n\nstatic void resv_map_release(struct kref *ref)\n{\n\tstruct resv_map *resv_map = container_of(ref, struct resv_map, refs);\n\n\t/* Clear out any active regions before we release the map. */\n\tregion_truncate(&resv_map->regions, 0);\n\tkfree(resv_map);\n}\n\nstatic struct resv_map *vma_resv_map(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON(!is_vm_hugetlb_page(vma));\n\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\treturn (struct resv_map *)(get_vma_private_data(vma) &\n\t\t\t\t\t\t\t~HPAGE_RESV_MASK);\n\treturn NULL;\n}\n\nstatic void set_vma_resv_map(struct vm_area_struct *vma, struct resv_map *map)\n{\n\tVM_BUG_ON(!is_vm_hugetlb_page(vma));\n\tVM_BUG_ON(vma->vm_flags & VM_MAYSHARE);\n\n\tset_vma_private_data(vma, (get_vma_private_data(vma) &\n\t\t\t\tHPAGE_RESV_MASK) | (unsigned long)map);\n}\n\nstatic void set_vma_resv_flags(struct vm_area_struct *vma, unsigned long flags)\n{\n\tVM_BUG_ON(!is_vm_hugetlb_page(vma));\n\tVM_BUG_ON(vma->vm_flags & VM_MAYSHARE);\n\n\tset_vma_private_data(vma, get_vma_private_data(vma) | flags);\n}\n\nstatic int is_vma_resv_set(struct vm_area_struct *vma, unsigned long flag)\n{\n\tVM_BUG_ON(!is_vm_hugetlb_page(vma));\n\n\treturn (get_vma_private_data(vma) & flag) != 0;\n}\n\n/* Decrement the reserved pages in the hugepage pool by one */\nstatic void decrement_hugepage_resv_vma(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma)\n{\n\tif (vma->vm_flags & VM_NORESERVE)\n\t\treturn;\n\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t/* Shared mappings always use reserves */\n\t\th->resv_huge_pages--;\n\t} else if (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\t/*\n\t\t * Only the process that called mmap() has reserves for\n\t\t * private mappings.\n\t\t */\n\t\th->resv_huge_pages--;\n\t}\n}\n\n/* Reset counters to 0 and clear all HPAGE_RESV_* flags */\nvoid reset_vma_resv_huge_pages(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON(!is_vm_hugetlb_page(vma));\n\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\tvma->vm_private_data = (void *)0;\n}\n\n/* Returns true if the VMA has associated reserve pages */\nstatic int vma_has_reserves(struct vm_area_struct *vma)\n{\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\treturn 1;\n\tif (is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void copy_gigantic_page(struct page *dst, struct page *src)\n{\n\tint i;\n\tstruct hstate *h = page_hstate(src);\n\tstruct page *dst_base = dst;\n\tstruct page *src_base = src;\n\n\tfor (i = 0; i < pages_per_huge_page(h); ) {\n\t\tcond_resched();\n\t\tcopy_highpage(dst, src);\n\n\t\ti++;\n\t\tdst = mem_map_next(dst, dst_base, i);\n\t\tsrc = mem_map_next(src, src_base, i);\n\t}\n}\n\nvoid copy_huge_page(struct page *dst, struct page *src)\n{\n\tint i;\n\tstruct hstate *h = page_hstate(src);\n\n\tif (unlikely(pages_per_huge_page(h) > MAX_ORDER_NR_PAGES)) {\n\t\tcopy_gigantic_page(dst, src);\n\t\treturn;\n\t}\n\n\tmight_sleep();\n\tfor (i = 0; i < pages_per_huge_page(h); i++) {\n\t\tcond_resched();\n\t\tcopy_highpage(dst + i, src + i);\n\t}\n}\n\nstatic void enqueue_huge_page(struct hstate *h, struct page *page)\n{\n\tint nid = page_to_nid(page);\n\tlist_add(&page->lru, &h->hugepage_freelists[nid]);\n\th->free_huge_pages++;\n\th->free_huge_pages_node[nid]++;\n}\n\nstatic struct page *dequeue_huge_page_node(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tif (list_empty(&h->hugepage_freelists[nid]))\n\t\treturn NULL;\n\tpage = list_entry(h->hugepage_freelists[nid].next, struct page, lru);\n\tlist_del(&page->lru);\n\tset_page_refcounted(page);\n\th->free_huge_pages--;\n\th->free_huge_pages_node[nid]--;\n\treturn page;\n}\n\nstatic struct page *dequeue_huge_page_vma(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long address, int avoid_reserve)\n{\n\tstruct page *page = NULL;\n\tstruct mempolicy *mpol;\n\tnodemask_t *nodemask;\n\tstruct zonelist *zonelist;\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tunsigned int cpuset_mems_cookie;\n\nretry_cpuset:\n\tcpuset_mems_cookie = get_mems_allowed();\n\tzonelist = huge_zonelist(vma, address,\n\t\t\t\t\thtlb_alloc_mask, &mpol, &nodemask);\n\t/*\n\t * A child process with MAP_PRIVATE mappings created by their parent\n\t * have no page reserves. This check ensures that reservations are\n\t * not \"stolen\". The child may still get SIGKILLed\n\t */\n\tif (!vma_has_reserves(vma) &&\n\t\t\th->free_huge_pages - h->resv_huge_pages == 0)\n\t\tgoto err;\n\n\t/* If reserves cannot be used, ensure enough pages are in the pool */\n\tif (avoid_reserve && h->free_huge_pages - h->resv_huge_pages == 0)\n\t\tgoto err;\n\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n\t\t\t\t\t\tMAX_NR_ZONES - 1, nodemask) {\n\t\tif (cpuset_zone_allowed_softwall(zone, htlb_alloc_mask)) {\n\t\t\tpage = dequeue_huge_page_node(h, zone_to_nid(zone));\n\t\t\tif (page) {\n\t\t\t\tif (!avoid_reserve)\n\t\t\t\t\tdecrement_hugepage_resv_vma(h, vma);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tmpol_cond_put(mpol);\n\tif (unlikely(!put_mems_allowed(cpuset_mems_cookie) && !page))\n\t\tgoto retry_cpuset;\n\treturn page;\n\nerr:\n\tmpol_cond_put(mpol);\n\treturn NULL;\n}\n\nstatic void update_and_free_page(struct hstate *h, struct page *page)\n{\n\tint i;\n\n\tVM_BUG_ON(h->order >= MAX_ORDER);\n\n\th->nr_huge_pages--;\n\th->nr_huge_pages_node[page_to_nid(page)]--;\n\tfor (i = 0; i < pages_per_huge_page(h); i++) {\n\t\tpage[i].flags &= ~(1 << PG_locked | 1 << PG_error |\n\t\t\t\t1 << PG_referenced | 1 << PG_dirty |\n\t\t\t\t1 << PG_active | 1 << PG_reserved |\n\t\t\t\t1 << PG_private | 1 << PG_writeback);\n\t}\n\tset_compound_page_dtor(page, NULL);\n\tset_page_refcounted(page);\n\tarch_release_hugepage(page);\n\t__free_pages(page, huge_page_order(h));\n}\n\nstruct hstate *size_to_hstate(unsigned long size)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tif (huge_page_size(h) == size)\n\t\t\treturn h;\n\t}\n\treturn NULL;\n}\n\nstatic void free_huge_page(struct page *page)\n{\n\t/*\n\t * Can't pass hstate in here because it is called from the\n\t * compound page destructor.\n\t */\n\tstruct hstate *h = page_hstate(page);\n\tint nid = page_to_nid(page);\n\tstruct hugepage_subpool *spool =\n\t\t(struct hugepage_subpool *)page_private(page);\n\n\tset_page_private(page, 0);\n\tpage->mapping = NULL;\n\tBUG_ON(page_count(page));\n\tBUG_ON(page_mapcount(page));\n\tINIT_LIST_HEAD(&page->lru);\n\n\tspin_lock(&hugetlb_lock);\n\tif (h->surplus_huge_pages_node[nid] && huge_page_order(h) < MAX_ORDER) {\n\t\tupdate_and_free_page(h, page);\n\t\th->surplus_huge_pages--;\n\t\th->surplus_huge_pages_node[nid]--;\n\t} else {\n\t\tenqueue_huge_page(h, page);\n\t}\n\tspin_unlock(&hugetlb_lock);\n\thugepage_subpool_put_pages(spool, 1);\n}\n\nstatic void prep_new_huge_page(struct hstate *h, struct page *page, int nid)\n{\n\tset_compound_page_dtor(page, free_huge_page);\n\tspin_lock(&hugetlb_lock);\n\th->nr_huge_pages++;\n\th->nr_huge_pages_node[nid]++;\n\tspin_unlock(&hugetlb_lock);\n\tput_page(page); /* free it into the hugepage allocator */\n}\n\nstatic void prep_compound_gigantic_page(struct page *page, unsigned long order)\n{\n\tint i;\n\tint nr_pages = 1 << order;\n\tstruct page *p = page + 1;\n\n\t/* we rely on prep_new_huge_page to set the destructor */\n\tset_compound_order(page, order);\n\t__SetPageHead(page);\n\tfor (i = 1; i < nr_pages; i++, p = mem_map_next(p, page, i)) {\n\t\t__SetPageTail(p);\n\t\tset_page_count(p, 0);\n\t\tp->first_page = page;\n\t}\n}\n\nint PageHuge(struct page *page)\n{\n\tcompound_page_dtor *dtor;\n\n\tif (!PageCompound(page))\n\t\treturn 0;\n\n\tpage = compound_head(page);\n\tdtor = get_compound_page_dtor(page);\n\n\treturn dtor == free_huge_page;\n}\nEXPORT_SYMBOL_GPL(PageHuge);\n\nstatic struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tif (h->order >= MAX_ORDER)\n\t\treturn NULL;\n\n\tpage = alloc_pages_exact_node(nid,\n\t\thtlb_alloc_mask|__GFP_COMP|__GFP_THISNODE|\n\t\t\t\t\t\t__GFP_REPEAT|__GFP_NOWARN,\n\t\thuge_page_order(h));\n\tif (page) {\n\t\tif (arch_prepare_hugepage(page)) {\n\t\t\t__free_pages(page, huge_page_order(h));\n\t\t\treturn NULL;\n\t\t}\n\t\tprep_new_huge_page(h, page, nid);\n\t}\n\n\treturn page;\n}\n\n/*\n * common helper functions for hstate_next_node_to_{alloc|free}.\n * We may have allocated or freed a huge page based on a different\n * nodes_allowed previously, so h->next_node_to_{alloc|free} might\n * be outside of *nodes_allowed.  Ensure that we use an allowed\n * node for alloc or free.\n */\nstatic int next_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tnid = next_node(nid, *nodes_allowed);\n\tif (nid == MAX_NUMNODES)\n\t\tnid = first_node(*nodes_allowed);\n\tVM_BUG_ON(nid >= MAX_NUMNODES);\n\n\treturn nid;\n}\n\nstatic int get_valid_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tif (!node_isset(nid, *nodes_allowed))\n\t\tnid = next_node_allowed(nid, nodes_allowed);\n\treturn nid;\n}\n\n/*\n * returns the previously saved node [\"this node\"] from which to\n * allocate a persistent huge page for the pool and advance the\n * next node from which to allocate, handling wrap at end of node\n * mask.\n */\nstatic int hstate_next_node_to_alloc(struct hstate *h,\n\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_alloc, nodes_allowed);\n\th->next_nid_to_alloc = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\nstatic int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)\n{\n\tstruct page *page;\n\tint start_nid;\n\tint next_nid;\n\tint ret = 0;\n\n\tstart_nid = hstate_next_node_to_alloc(h, nodes_allowed);\n\tnext_nid = start_nid;\n\n\tdo {\n\t\tpage = alloc_fresh_huge_page_node(h, next_nid);\n\t\tif (page) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t\tnext_nid = hstate_next_node_to_alloc(h, nodes_allowed);\n\t} while (next_nid != start_nid);\n\n\tif (ret)\n\t\tcount_vm_event(HTLB_BUDDY_PGALLOC);\n\telse\n\t\tcount_vm_event(HTLB_BUDDY_PGALLOC_FAIL);\n\n\treturn ret;\n}\n\n/*\n * helper for free_pool_huge_page() - return the previously saved\n * node [\"this node\"] from which to free a huge page.  Advance the\n * next node id whether or not we find a free huge page to free so\n * that the next attempt to free addresses the next node.\n */\nstatic int hstate_next_node_to_free(struct hstate *h, nodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_free, nodes_allowed);\n\th->next_nid_to_free = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\n/*\n * Free huge page from pool from next node to free.\n * Attempt to keep persistent huge pages more or less\n * balanced over allowed nodes.\n * Called with hugetlb_lock locked.\n */\nstatic int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\t\t\t\t bool acct_surplus)\n{\n\tint start_nid;\n\tint next_nid;\n\tint ret = 0;\n\n\tstart_nid = hstate_next_node_to_free(h, nodes_allowed);\n\tnext_nid = start_nid;\n\n\tdo {\n\t\t/*\n\t\t * If we're returning unused surplus pages, only examine\n\t\t * nodes with surplus pages.\n\t\t */\n\t\tif ((!acct_surplus || h->surplus_huge_pages_node[next_nid]) &&\n\t\t    !list_empty(&h->hugepage_freelists[next_nid])) {\n\t\t\tstruct page *page =\n\t\t\t\tlist_entry(h->hugepage_freelists[next_nid].next,\n\t\t\t\t\t  struct page, lru);\n\t\t\tlist_del(&page->lru);\n\t\t\th->free_huge_pages--;\n\t\t\th->free_huge_pages_node[next_nid]--;\n\t\t\tif (acct_surplus) {\n\t\t\t\th->surplus_huge_pages--;\n\t\t\t\th->surplus_huge_pages_node[next_nid]--;\n\t\t\t}\n\t\t\tupdate_and_free_page(h, page);\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t\tnext_nid = hstate_next_node_to_free(h, nodes_allowed);\n\t} while (next_nid != start_nid);\n\n\treturn ret;\n}\n\nstatic struct page *alloc_buddy_huge_page(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\tunsigned int r_nid;\n\n\tif (h->order >= MAX_ORDER)\n\t\treturn NULL;\n\n\t/*\n\t * Assume we will successfully allocate the surplus page to\n\t * prevent racing processes from causing the surplus to exceed\n\t * overcommit\n\t *\n\t * This however introduces a different race, where a process B\n\t * tries to grow the static hugepage pool while alloc_pages() is\n\t * called by process A. B will only examine the per-node\n\t * counters in determining if surplus huge pages can be\n\t * converted to normal huge pages in adjust_pool_surplus(). A\n\t * won't be able to increment the per-node counter, until the\n\t * lock is dropped by B, but B doesn't drop hugetlb_lock until\n\t * no more huge pages can be converted from surplus to normal\n\t * state (and doesn't try to convert again). Thus, we have a\n\t * case where a surplus huge page exists, the pool is grown, and\n\t * the surplus huge page still exists after, even though it\n\t * should just have been converted to a normal huge page. This\n\t * does not leak memory, though, as the hugepage will be freed\n\t * once it is out of use. It also does not allow the counters to\n\t * go out of whack in adjust_pool_surplus() as we don't modify\n\t * the node values until we've gotten the hugepage and only the\n\t * per-node value is checked there.\n\t */\n\tspin_lock(&hugetlb_lock);\n\tif (h->surplus_huge_pages >= h->nr_overcommit_huge_pages) {\n\t\tspin_unlock(&hugetlb_lock);\n\t\treturn NULL;\n\t} else {\n\t\th->nr_huge_pages++;\n\t\th->surplus_huge_pages++;\n\t}\n\tspin_unlock(&hugetlb_lock);\n\n\tif (nid == NUMA_NO_NODE)\n\t\tpage = alloc_pages(htlb_alloc_mask|__GFP_COMP|\n\t\t\t\t   __GFP_REPEAT|__GFP_NOWARN,\n\t\t\t\t   huge_page_order(h));\n\telse\n\t\tpage = alloc_pages_exact_node(nid,\n\t\t\thtlb_alloc_mask|__GFP_COMP|__GFP_THISNODE|\n\t\t\t__GFP_REPEAT|__GFP_NOWARN, huge_page_order(h));\n\n\tif (page && arch_prepare_hugepage(page)) {\n\t\t__free_pages(page, huge_page_order(h));\n\t\tpage = NULL;\n\t}\n\n\tspin_lock(&hugetlb_lock);\n\tif (page) {\n\t\tr_nid = page_to_nid(page);\n\t\tset_compound_page_dtor(page, free_huge_page);\n\t\t/*\n\t\t * We incremented the global counters already\n\t\t */\n\t\th->nr_huge_pages_node[r_nid]++;\n\t\th->surplus_huge_pages_node[r_nid]++;\n\t\t__count_vm_event(HTLB_BUDDY_PGALLOC);\n\t} else {\n\t\th->nr_huge_pages--;\n\t\th->surplus_huge_pages--;\n\t\t__count_vm_event(HTLB_BUDDY_PGALLOC_FAIL);\n\t}\n\tspin_unlock(&hugetlb_lock);\n\n\treturn page;\n}\n\n/*\n * This allocation function is useful in the context where vma is irrelevant.\n * E.g. soft-offlining uses this function because it only cares physical\n * address of error page.\n */\nstruct page *alloc_huge_page_node(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tspin_lock(&hugetlb_lock);\n\tpage = dequeue_huge_page_node(h, nid);\n\tspin_unlock(&hugetlb_lock);\n\n\tif (!page)\n\t\tpage = alloc_buddy_huge_page(h, nid);\n\n\treturn page;\n}\n\n/*\n * Increase the hugetlb pool such that it can accommodate a reservation\n * of size 'delta'.\n */\nstatic int gather_surplus_pages(struct hstate *h, int delta)\n{\n\tstruct list_head surplus_list;\n\tstruct page *page, *tmp;\n\tint ret, i;\n\tint needed, allocated;\n\tbool alloc_ok = true;\n\n\tneeded = (h->resv_huge_pages + delta) - h->free_huge_pages;\n\tif (needed <= 0) {\n\t\th->resv_huge_pages += delta;\n\t\treturn 0;\n\t}\n\n\tallocated = 0;\n\tINIT_LIST_HEAD(&surplus_list);\n\n\tret = -ENOMEM;\nretry:\n\tspin_unlock(&hugetlb_lock);\n\tfor (i = 0; i < needed; i++) {\n\t\tpage = alloc_buddy_huge_page(h, NUMA_NO_NODE);\n\t\tif (!page) {\n\t\t\talloc_ok = false;\n\t\t\tbreak;\n\t\t}\n\t\tlist_add(&page->lru, &surplus_list);\n\t}\n\tallocated += i;\n\n\t/*\n\t * After retaking hugetlb_lock, we need to recalculate 'needed'\n\t * because either resv_huge_pages or free_huge_pages may have changed.\n\t */\n\tspin_lock(&hugetlb_lock);\n\tneeded = (h->resv_huge_pages + delta) -\n\t\t\t(h->free_huge_pages + allocated);\n\tif (needed > 0) {\n\t\tif (alloc_ok)\n\t\t\tgoto retry;\n\t\t/*\n\t\t * We were not able to allocate enough pages to\n\t\t * satisfy the entire reservation so we free what\n\t\t * we've allocated so far.\n\t\t */\n\t\tgoto free;\n\t}\n\t/*\n\t * The surplus_list now contains _at_least_ the number of extra pages\n\t * needed to accommodate the reservation.  Add the appropriate number\n\t * of pages to the hugetlb pool and free the extras back to the buddy\n\t * allocator.  Commit the entire reservation here to prevent another\n\t * process from stealing the pages as they are added to the pool but\n\t * before they are reserved.\n\t */\n\tneeded += allocated;\n\th->resv_huge_pages += delta;\n\tret = 0;\n\n\t/* Free the needed pages to the hugetlb pool */\n\tlist_for_each_entry_safe(page, tmp, &surplus_list, lru) {\n\t\tif ((--needed) < 0)\n\t\t\tbreak;\n\t\tlist_del(&page->lru);\n\t\t/*\n\t\t * This page is now managed by the hugetlb allocator and has\n\t\t * no users -- drop the buddy allocator's reference.\n\t\t */\n\t\tput_page_testzero(page);\n\t\tVM_BUG_ON(page_count(page));\n\t\tenqueue_huge_page(h, page);\n\t}\nfree:\n\tspin_unlock(&hugetlb_lock);\n\n\t/* Free unnecessary surplus pages to the buddy allocator */\n\tif (!list_empty(&surplus_list)) {\n\t\tlist_for_each_entry_safe(page, tmp, &surplus_list, lru) {\n\t\t\tlist_del(&page->lru);\n\t\t\tput_page(page);\n\t\t}\n\t}\n\tspin_lock(&hugetlb_lock);\n\n\treturn ret;\n}\n\n/*\n * When releasing a hugetlb pool reservation, any surplus pages that were\n * allocated to satisfy the reservation must be explicitly freed if they were\n * never used.\n * Called with hugetlb_lock held.\n */\nstatic void return_unused_surplus_pages(struct hstate *h,\n\t\t\t\t\tunsigned long unused_resv_pages)\n{\n\tunsigned long nr_pages;\n\n\t/* Uncommit the reservation */\n\th->resv_huge_pages -= unused_resv_pages;\n\n\t/* Cannot return gigantic pages currently */\n\tif (h->order >= MAX_ORDER)\n\t\treturn;\n\n\tnr_pages = min(unused_resv_pages, h->surplus_huge_pages);\n\n\t/*\n\t * We want to release as many surplus pages as possible, spread\n\t * evenly across all nodes with memory. Iterate across these nodes\n\t * until we can no longer free unreserved surplus pages. This occurs\n\t * when the nodes with surplus pages have no free pages.\n\t * free_pool_huge_page() will balance the the freed pages across the\n\t * on-line nodes with memory and will handle the hstate accounting.\n\t */\n\twhile (nr_pages--) {\n\t\tif (!free_pool_huge_page(h, &node_states[N_HIGH_MEMORY], 1))\n\t\t\tbreak;\n\t}\n}\n\n/*\n * Determine if the huge page at addr within the vma has an associated\n * reservation.  Where it does not we will need to logically increase\n * reservation and actually increase subpool usage before an allocation\n * can occur.  Where any new reservation would be required the\n * reservation change is prepared, but not committed.  Once the page\n * has been allocated from the subpool and instantiated the change should\n * be committed via vma_commit_reservation.  No action is required on\n * failure.\n */\nstatic long vma_needs_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\tpgoff_t idx = vma_hugecache_offset(h, vma, addr);\n\t\treturn region_chg(&inode->i_mapping->private_list,\n\t\t\t\t\t\t\tidx, idx + 1);\n\n\t} else if (!is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\treturn 1;\n\n\t} else  {\n\t\tlong err;\n\t\tpgoff_t idx = vma_hugecache_offset(h, vma, addr);\n\t\tstruct resv_map *reservations = vma_resv_map(vma);\n\n\t\terr = region_chg(&reservations->regions, idx, idx + 1);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\treturn 0;\n\t}\n}\nstatic void vma_commit_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\tpgoff_t idx = vma_hugecache_offset(h, vma, addr);\n\t\tregion_add(&inode->i_mapping->private_list, idx, idx + 1);\n\n\t} else if (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\tpgoff_t idx = vma_hugecache_offset(h, vma, addr);\n\t\tstruct resv_map *reservations = vma_resv_map(vma);\n\n\t\t/* Mark this page used in the map. */\n\t\tregion_add(&reservations->regions, idx, idx + 1);\n\t}\n}\n\nstatic struct page *alloc_huge_page(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long addr, int avoid_reserve)\n{\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct page *page;\n\tlong chg;\n\n\t/*\n\t * Processes that did not create the mapping will have no\n\t * reserves and will not have accounted against subpool\n\t * limit. Check that the subpool limit can be made before\n\t * satisfying the allocation MAP_NORESERVE mappings may also\n\t * need pages and subpool limit allocated allocated if no reserve\n\t * mapping overlaps.\n\t */\n\tchg = vma_needs_reservation(h, vma, addr);\n\tif (chg < 0)\n\t\treturn ERR_PTR(-VM_FAULT_OOM);\n\tif (chg)\n\t\tif (hugepage_subpool_get_pages(spool, chg))\n\t\t\treturn ERR_PTR(-VM_FAULT_SIGBUS);\n\n\tspin_lock(&hugetlb_lock);\n\tpage = dequeue_huge_page_vma(h, vma, addr, avoid_reserve);\n\tspin_unlock(&hugetlb_lock);\n\n\tif (!page) {\n\t\tpage = alloc_buddy_huge_page(h, NUMA_NO_NODE);\n\t\tif (!page) {\n\t\t\thugepage_subpool_put_pages(spool, chg);\n\t\t\treturn ERR_PTR(-VM_FAULT_SIGBUS);\n\t\t}\n\t}\n\n\tset_page_private(page, (unsigned long)spool);\n\n\tvma_commit_reservation(h, vma, addr);\n\n\treturn page;\n}\n\nint __weak alloc_bootmem_huge_page(struct hstate *h)\n{\n\tstruct huge_bootmem_page *m;\n\tint nr_nodes = nodes_weight(node_states[N_HIGH_MEMORY]);\n\n\twhile (nr_nodes) {\n\t\tvoid *addr;\n\n\t\taddr = __alloc_bootmem_node_nopanic(\n\t\t\t\tNODE_DATA(hstate_next_node_to_alloc(h,\n\t\t\t\t\t\t&node_states[N_HIGH_MEMORY])),\n\t\t\t\thuge_page_size(h), huge_page_size(h), 0);\n\n\t\tif (addr) {\n\t\t\t/*\n\t\t\t * Use the beginning of the huge page to store the\n\t\t\t * huge_bootmem_page struct (until gather_bootmem\n\t\t\t * puts them into the mem_map).\n\t\t\t */\n\t\t\tm = addr;\n\t\t\tgoto found;\n\t\t}\n\t\tnr_nodes--;\n\t}\n\treturn 0;\n\nfound:\n\tBUG_ON((unsigned long)virt_to_phys(m) & (huge_page_size(h) - 1));\n\t/* Put them into a private list first because mem_map is not up yet */\n\tlist_add(&m->list, &huge_boot_pages);\n\tm->hstate = h;\n\treturn 1;\n}\n\nstatic void prep_compound_huge_page(struct page *page, int order)\n{\n\tif (unlikely(order > (MAX_ORDER - 1)))\n\t\tprep_compound_gigantic_page(page, order);\n\telse\n\t\tprep_compound_page(page, order);\n}\n\n/* Put bootmem huge pages into the standard lists after mem_map is up */\nstatic void __init gather_bootmem_prealloc(void)\n{\n\tstruct huge_bootmem_page *m;\n\n\tlist_for_each_entry(m, &huge_boot_pages, list) {\n\t\tstruct hstate *h = m->hstate;\n\t\tstruct page *page;\n\n#ifdef CONFIG_HIGHMEM\n\t\tpage = pfn_to_page(m->phys >> PAGE_SHIFT);\n\t\tfree_bootmem_late((unsigned long)m,\n\t\t\t\t  sizeof(struct huge_bootmem_page));\n#else\n\t\tpage = virt_to_page(m);\n#endif\n\t\t__ClearPageReserved(page);\n\t\tWARN_ON(page_count(page) != 1);\n\t\tprep_compound_huge_page(page, h->order);\n\t\tprep_new_huge_page(h, page, page_to_nid(page));\n\t\t/*\n\t\t * If we had gigantic hugepages allocated at boot time, we need\n\t\t * to restore the 'stolen' pages to totalram_pages in order to\n\t\t * fix confusing memory reports from free(1) and another\n\t\t * side-effects, like CommitLimit going negative.\n\t\t */\n\t\tif (h->order > (MAX_ORDER - 1))\n\t\t\ttotalram_pages += 1 << h->order;\n\t}\n}\n\nstatic void __init hugetlb_hstate_alloc_pages(struct hstate *h)\n{\n\tunsigned long i;\n\n\tfor (i = 0; i < h->max_huge_pages; ++i) {\n\t\tif (h->order >= MAX_ORDER) {\n\t\t\tif (!alloc_bootmem_huge_page(h))\n\t\t\t\tbreak;\n\t\t} else if (!alloc_fresh_huge_page(h,\n\t\t\t\t\t &node_states[N_HIGH_MEMORY]))\n\t\t\tbreak;\n\t}\n\th->max_huge_pages = i;\n}\n\nstatic void __init hugetlb_init_hstates(void)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\t/* oversize hugepages were init'ed in early boot */\n\t\tif (h->order < MAX_ORDER)\n\t\t\thugetlb_hstate_alloc_pages(h);\n\t}\n}\n\nstatic char * __init memfmt(char *buf, unsigned long n)\n{\n\tif (n >= (1UL << 30))\n\t\tsprintf(buf, \"%lu GB\", n >> 30);\n\telse if (n >= (1UL << 20))\n\t\tsprintf(buf, \"%lu MB\", n >> 20);\n\telse\n\t\tsprintf(buf, \"%lu KB\", n >> 10);\n\treturn buf;\n}\n\nstatic void __init report_hugepages(void)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tchar buf[32];\n\t\tprintk(KERN_INFO \"HugeTLB registered %s page size, \"\n\t\t\t\t \"pre-allocated %ld pages\\n\",\n\t\t\tmemfmt(buf, huge_page_size(h)),\n\t\t\th->free_huge_pages);\n\t}\n}\n\n#ifdef CONFIG_HIGHMEM\nstatic void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint i;\n\n\tif (h->order >= MAX_ORDER)\n\t\treturn;\n\n\tfor_each_node_mask(i, *nodes_allowed) {\n\t\tstruct page *page, *next;\n\t\tstruct list_head *freel = &h->hugepage_freelists[i];\n\t\tlist_for_each_entry_safe(page, next, freel, lru) {\n\t\t\tif (count >= h->nr_huge_pages)\n\t\t\t\treturn;\n\t\t\tif (PageHighMem(page))\n\t\t\t\tcontinue;\n\t\t\tlist_del(&page->lru);\n\t\t\tupdate_and_free_page(h, page);\n\t\t\th->free_huge_pages--;\n\t\t\th->free_huge_pages_node[page_to_nid(page)]--;\n\t\t}\n\t}\n}\n#else\nstatic inline void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n}\n#endif\n\n/*\n * Increment or decrement surplus_huge_pages.  Keep node-specific counters\n * balanced by operating on them in a round-robin fashion.\n * Returns 1 if an adjustment was made.\n */\nstatic int adjust_pool_surplus(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\tint delta)\n{\n\tint start_nid, next_nid;\n\tint ret = 0;\n\n\tVM_BUG_ON(delta != -1 && delta != 1);\n\n\tif (delta < 0)\n\t\tstart_nid = hstate_next_node_to_alloc(h, nodes_allowed);\n\telse\n\t\tstart_nid = hstate_next_node_to_free(h, nodes_allowed);\n\tnext_nid = start_nid;\n\n\tdo {\n\t\tint nid = next_nid;\n\t\tif (delta < 0)  {\n\t\t\t/*\n\t\t\t * To shrink on this node, there must be a surplus page\n\t\t\t */\n\t\t\tif (!h->surplus_huge_pages_node[nid]) {\n\t\t\t\tnext_nid = hstate_next_node_to_alloc(h,\n\t\t\t\t\t\t\t\tnodes_allowed);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (delta > 0) {\n\t\t\t/*\n\t\t\t * Surplus cannot exceed the total number of pages\n\t\t\t */\n\t\t\tif (h->surplus_huge_pages_node[nid] >=\n\t\t\t\t\t\th->nr_huge_pages_node[nid]) {\n\t\t\t\tnext_nid = hstate_next_node_to_free(h,\n\t\t\t\t\t\t\t\tnodes_allowed);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\th->surplus_huge_pages += delta;\n\t\th->surplus_huge_pages_node[nid] += delta;\n\t\tret = 1;\n\t\tbreak;\n\t} while (next_nid != start_nid);\n\n\treturn ret;\n}\n\n#define persistent_huge_pages(h) (h->nr_huge_pages - h->surplus_huge_pages)\nstatic unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tunsigned long min_count, ret;\n\n\tif (h->order >= MAX_ORDER)\n\t\treturn h->max_huge_pages;\n\n\t/*\n\t * Increase the pool size\n\t * First take pages out of surplus state.  Then make up the\n\t * remaining difference by allocating fresh huge pages.\n\t *\n\t * We might race with alloc_buddy_huge_page() here and be unable\n\t * to convert a surplus huge page to a normal huge page. That is\n\t * not critical, though, it just means the overall size of the\n\t * pool might be one hugepage larger than it needs to be, but\n\t * within all the constraints specified by the sysctls.\n\t */\n\tspin_lock(&hugetlb_lock);\n\twhile (h->surplus_huge_pages && count > persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, -1))\n\t\t\tbreak;\n\t}\n\n\twhile (count > persistent_huge_pages(h)) {\n\t\t/*\n\t\t * If this allocation races such that we no longer need the\n\t\t * page, free_huge_page will handle it by freeing the page\n\t\t * and reducing the surplus.\n\t\t */\n\t\tspin_unlock(&hugetlb_lock);\n\t\tret = alloc_fresh_huge_page(h, nodes_allowed);\n\t\tspin_lock(&hugetlb_lock);\n\t\tif (!ret)\n\t\t\tgoto out;\n\n\t\t/* Bail for signals. Probably ctrl-c from user */\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * Decrease the pool size\n\t * First return free pages to the buddy allocator (being careful\n\t * to keep enough around to satisfy reservations).  Then place\n\t * pages into surplus state as needed so the pool will shrink\n\t * to the desired size as pages become free.\n\t *\n\t * By placing pages into the surplus state independent of the\n\t * overcommit value, we are allowing the surplus pool size to\n\t * exceed overcommit. There are few sane options here. Since\n\t * alloc_buddy_huge_page() is checking the global counter,\n\t * though, we'll note that we're not allowed to exceed surplus\n\t * and won't grow the pool anywhere else. Not until one of the\n\t * sysctls are changed, or the surplus pages go out of use.\n\t */\n\tmin_count = h->resv_huge_pages + h->nr_huge_pages - h->free_huge_pages;\n\tmin_count = max(count, min_count);\n\ttry_to_free_low(h, min_count, nodes_allowed);\n\twhile (min_count < persistent_huge_pages(h)) {\n\t\tif (!free_pool_huge_page(h, nodes_allowed, 0))\n\t\t\tbreak;\n\t}\n\twhile (count < persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, 1))\n\t\t\tbreak;\n\t}\nout:\n\tret = persistent_huge_pages(h);\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\n#define HSTATE_ATTR_RO(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_RO(_name)\n\n#define HSTATE_ATTR(_name) \\\n\tstatic struct kobj_attribute _name##_attr = \\\n\t\t__ATTR(_name, 0644, _name##_show, _name##_store)\n\nstatic struct kobject *hugepages_kobj;\nstatic struct kobject *hstate_kobjs[HUGE_MAX_HSTATE];\n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp);\n\nstatic struct hstate *kobj_to_hstate(struct kobject *kobj, int *nidp)\n{\n\tint i;\n\n\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\tif (hstate_kobjs[i] == kobj) {\n\t\t\tif (nidp)\n\t\t\t\t*nidp = NUMA_NO_NODE;\n\t\t\treturn &hstates[i];\n\t\t}\n\n\treturn kobj_to_node_hstate(kobj, nidp);\n}\n\nstatic ssize_t nr_hugepages_show_common(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long nr_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tnr_huge_pages = h->nr_huge_pages;\n\telse\n\t\tnr_huge_pages = h->nr_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", nr_huge_pages);\n}\n\nstatic ssize_t nr_hugepages_store_common(bool obey_mempolicy,\n\t\t\tstruct kobject *kobj, struct kobj_attribute *attr,\n\t\t\tconst char *buf, size_t len)\n{\n\tint err;\n\tint nid;\n\tunsigned long count;\n\tstruct hstate *h;\n\tNODEMASK_ALLOC(nodemask_t, nodes_allowed, GFP_KERNEL | __GFP_NORETRY);\n\n\terr = strict_strtoul(buf, 10, &count);\n\tif (err)\n\t\tgoto out;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (h->order >= MAX_ORDER) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (nid == NUMA_NO_NODE) {\n\t\t/*\n\t\t * global hstate attribute\n\t\t */\n\t\tif (!(obey_mempolicy &&\n\t\t\t\tinit_nodemask_of_mempolicy(nodes_allowed))) {\n\t\t\tNODEMASK_FREE(nodes_allowed);\n\t\t\tnodes_allowed = &node_states[N_HIGH_MEMORY];\n\t\t}\n\t} else if (nodes_allowed) {\n\t\t/*\n\t\t * per node hstate attribute: adjust count to global,\n\t\t * but restrict alloc/free to the specified node.\n\t\t */\n\t\tcount += h->nr_huge_pages - h->nr_huge_pages_node[nid];\n\t\tinit_nodemask_of_node(nodes_allowed, nid);\n\t} else\n\t\tnodes_allowed = &node_states[N_HIGH_MEMORY];\n\n\th->max_huge_pages = set_max_huge_pages(h, count, nodes_allowed);\n\n\tif (nodes_allowed != &node_states[N_HIGH_MEMORY])\n\t\tNODEMASK_FREE(nodes_allowed);\n\n\treturn len;\nout:\n\tNODEMASK_FREE(nodes_allowed);\n\treturn err;\n}\n\nstatic ssize_t nr_hugepages_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(false, kobj, attr, buf, len);\n}\nHSTATE_ATTR(nr_hugepages);\n\n#ifdef CONFIG_NUMA\n\n/*\n * hstate attribute for optionally mempolicy-based constraint on persistent\n * huge page alloc/free.\n */\nstatic ssize_t nr_hugepages_mempolicy_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_mempolicy_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(true, kobj, attr, buf, len);\n}\nHSTATE_ATTR(nr_hugepages_mempolicy);\n#endif\n\n\nstatic ssize_t nr_overcommit_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sprintf(buf, \"%lu\\n\", h->nr_overcommit_huge_pages);\n}\n\nstatic ssize_t nr_overcommit_hugepages_store(struct kobject *kobj,\n\t\tstruct kobj_attribute *attr, const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long input;\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\n\tif (h->order >= MAX_ORDER)\n\t\treturn -EINVAL;\n\n\terr = strict_strtoul(buf, 10, &input);\n\tif (err)\n\t\treturn err;\n\n\tspin_lock(&hugetlb_lock);\n\th->nr_overcommit_huge_pages = input;\n\tspin_unlock(&hugetlb_lock);\n\n\treturn count;\n}\nHSTATE_ATTR(nr_overcommit_hugepages);\n\nstatic ssize_t free_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long free_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tfree_huge_pages = h->free_huge_pages;\n\telse\n\t\tfree_huge_pages = h->free_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", free_huge_pages);\n}\nHSTATE_ATTR_RO(free_hugepages);\n\nstatic ssize_t resv_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sprintf(buf, \"%lu\\n\", h->resv_huge_pages);\n}\nHSTATE_ATTR_RO(resv_hugepages);\n\nstatic ssize_t surplus_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long surplus_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tsurplus_huge_pages = h->surplus_huge_pages;\n\telse\n\t\tsurplus_huge_pages = h->surplus_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", surplus_huge_pages);\n}\nHSTATE_ATTR_RO(surplus_hugepages);\n\nstatic struct attribute *hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&nr_overcommit_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&resv_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n#ifdef CONFIG_NUMA\n\t&nr_hugepages_mempolicy_attr.attr,\n#endif\n\tNULL,\n};\n\nstatic struct attribute_group hstate_attr_group = {\n\t.attrs = hstate_attrs,\n};\n\nstatic int hugetlb_sysfs_add_hstate(struct hstate *h, struct kobject *parent,\n\t\t\t\t    struct kobject **hstate_kobjs,\n\t\t\t\t    struct attribute_group *hstate_attr_group)\n{\n\tint retval;\n\tint hi = h - hstates;\n\n\thstate_kobjs[hi] = kobject_create_and_add(h->name, parent);\n\tif (!hstate_kobjs[hi])\n\t\treturn -ENOMEM;\n\n\tretval = sysfs_create_group(hstate_kobjs[hi], hstate_attr_group);\n\tif (retval)\n\t\tkobject_put(hstate_kobjs[hi]);\n\n\treturn retval;\n}\n\nstatic void __init hugetlb_sysfs_init(void)\n{\n\tstruct hstate *h;\n\tint err;\n\n\thugepages_kobj = kobject_create_and_add(\"hugepages\", mm_kobj);\n\tif (!hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, hugepages_kobj,\n\t\t\t\t\t hstate_kobjs, &hstate_attr_group);\n\t\tif (err)\n\t\t\tprintk(KERN_ERR \"Hugetlb: Unable to add hstate %s\",\n\t\t\t\t\t\t\t\th->name);\n\t}\n}\n\n#ifdef CONFIG_NUMA\n\n/*\n * node_hstate/s - associate per node hstate attributes, via their kobjects,\n * with node devices in node_devices[] using a parallel array.  The array\n * index of a node device or _hstate == node id.\n * This is here to avoid any static dependency of the node device driver, in\n * the base kernel, on the hugetlb module.\n */\nstruct node_hstate {\n\tstruct kobject\t\t*hugepages_kobj;\n\tstruct kobject\t\t*hstate_kobjs[HUGE_MAX_HSTATE];\n};\nstruct node_hstate node_hstates[MAX_NUMNODES];\n\n/*\n * A subset of global hstate attributes for node devices\n */\nstatic struct attribute *per_node_hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group per_node_hstate_attr_group = {\n\t.attrs = per_node_hstate_attrs,\n};\n\n/*\n * kobj_to_node_hstate - lookup global hstate for node device hstate attr kobj.\n * Returns node id via non-NULL nidp.\n */\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tint nid;\n\n\tfor (nid = 0; nid < nr_node_ids; nid++) {\n\t\tstruct node_hstate *nhs = &node_hstates[nid];\n\t\tint i;\n\t\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\t\tif (nhs->hstate_kobjs[i] == kobj) {\n\t\t\t\tif (nidp)\n\t\t\t\t\t*nidp = nid;\n\t\t\t\treturn &hstates[i];\n\t\t\t}\n\t}\n\n\tBUG();\n\treturn NULL;\n}\n\n/*\n * Unregister hstate attributes from a single node device.\n * No-op if no hstate attributes attached.\n */\nvoid hugetlb_unregister_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\t\t/* no hstate attributes */\n\n\tfor_each_hstate(h)\n\t\tif (nhs->hstate_kobjs[h - hstates]) {\n\t\t\tkobject_put(nhs->hstate_kobjs[h - hstates]);\n\t\t\tnhs->hstate_kobjs[h - hstates] = NULL;\n\t\t}\n\n\tkobject_put(nhs->hugepages_kobj);\n\tnhs->hugepages_kobj = NULL;\n}\n\n/*\n * hugetlb module exit:  unregister hstate attributes from node devices\n * that have them.\n */\nstatic void hugetlb_unregister_all_nodes(void)\n{\n\tint nid;\n\n\t/*\n\t * disable node device registrations.\n\t */\n\tregister_hugetlbfs_with_node(NULL, NULL);\n\n\t/*\n\t * remove hstate attributes from any nodes that have them.\n\t */\n\tfor (nid = 0; nid < nr_node_ids; nid++)\n\t\thugetlb_unregister_node(&node_devices[nid]);\n}\n\n/*\n * Register hstate attributes for a single node device.\n * No-op if attributes already registered.\n */\nvoid hugetlb_register_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\tint err;\n\n\tif (nhs->hugepages_kobj)\n\t\treturn;\t\t/* already allocated */\n\n\tnhs->hugepages_kobj = kobject_create_and_add(\"hugepages\",\n\t\t\t\t\t\t\t&node->dev.kobj);\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, nhs->hugepages_kobj,\n\t\t\t\t\t\tnhs->hstate_kobjs,\n\t\t\t\t\t\t&per_node_hstate_attr_group);\n\t\tif (err) {\n\t\t\tprintk(KERN_ERR \"Hugetlb: Unable to add hstate %s\"\n\t\t\t\t\t\" for node %d\\n\",\n\t\t\t\t\t\th->name, node->dev.id);\n\t\t\thugetlb_unregister_node(node);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/*\n * hugetlb init time:  register hstate attributes for all registered node\n * devices of nodes that have memory.  All on-line nodes should have\n * registered their associated device by this time.\n */\nstatic void hugetlb_register_all_nodes(void)\n{\n\tint nid;\n\n\tfor_each_node_state(nid, N_HIGH_MEMORY) {\n\t\tstruct node *node = &node_devices[nid];\n\t\tif (node->dev.id == nid)\n\t\t\thugetlb_register_node(node);\n\t}\n\n\t/*\n\t * Let the node device driver know we're here so it can\n\t * [un]register hstate attributes on node hotplug.\n\t */\n\tregister_hugetlbfs_with_node(hugetlb_register_node,\n\t\t\t\t     hugetlb_unregister_node);\n}\n#else\t/* !CONFIG_NUMA */\n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tBUG();\n\tif (nidp)\n\t\t*nidp = -1;\n\treturn NULL;\n}\n\nstatic void hugetlb_unregister_all_nodes(void) { }\n\nstatic void hugetlb_register_all_nodes(void) { }\n\n#endif\n\nstatic void __exit hugetlb_exit(void)\n{\n\tstruct hstate *h;\n\n\thugetlb_unregister_all_nodes();\n\n\tfor_each_hstate(h) {\n\t\tkobject_put(hstate_kobjs[h - hstates]);\n\t}\n\n\tkobject_put(hugepages_kobj);\n}\nmodule_exit(hugetlb_exit);\n\nstatic int __init hugetlb_init(void)\n{\n\t/* Some platform decide whether they support huge pages at boot\n\t * time. On these, such as powerpc, HPAGE_SHIFT is set to 0 when\n\t * there is no such support\n\t */\n\tif (HPAGE_SHIFT == 0)\n\t\treturn 0;\n\n\tif (!size_to_hstate(default_hstate_size)) {\n\t\tdefault_hstate_size = HPAGE_SIZE;\n\t\tif (!size_to_hstate(default_hstate_size))\n\t\t\thugetlb_add_hstate(HUGETLB_PAGE_ORDER);\n\t}\n\tdefault_hstate_idx = size_to_hstate(default_hstate_size) - hstates;\n\tif (default_hstate_max_huge_pages)\n\t\tdefault_hstate.max_huge_pages = default_hstate_max_huge_pages;\n\n\thugetlb_init_hstates();\n\n\tgather_bootmem_prealloc();\n\n\treport_hugepages();\n\n\thugetlb_sysfs_init();\n\n\thugetlb_register_all_nodes();\n\n\treturn 0;\n}\nmodule_init(hugetlb_init);\n\n/* Should be called on processing a hugepagesz=... option */\nvoid __init hugetlb_add_hstate(unsigned order)\n{\n\tstruct hstate *h;\n\tunsigned long i;\n\n\tif (size_to_hstate(PAGE_SIZE << order)) {\n\t\tprintk(KERN_WARNING \"hugepagesz= specified twice, ignoring\\n\");\n\t\treturn;\n\t}\n\tBUG_ON(max_hstate >= HUGE_MAX_HSTATE);\n\tBUG_ON(order == 0);\n\th = &hstates[max_hstate++];\n\th->order = order;\n\th->mask = ~((1ULL << (order + PAGE_SHIFT)) - 1);\n\th->nr_huge_pages = 0;\n\th->free_huge_pages = 0;\n\tfor (i = 0; i < MAX_NUMNODES; ++i)\n\t\tINIT_LIST_HEAD(&h->hugepage_freelists[i]);\n\th->next_nid_to_alloc = first_node(node_states[N_HIGH_MEMORY]);\n\th->next_nid_to_free = first_node(node_states[N_HIGH_MEMORY]);\n\tsnprintf(h->name, HSTATE_NAME_LEN, \"hugepages-%lukB\",\n\t\t\t\t\thuge_page_size(h)/1024);\n\n\tparsed_hstate = h;\n}\n\nstatic int __init hugetlb_nrpages_setup(char *s)\n{\n\tunsigned long *mhp;\n\tstatic unsigned long *last_mhp;\n\n\t/*\n\t * !max_hstate means we haven't parsed a hugepagesz= parameter yet,\n\t * so this hugepages= parameter goes to the \"default hstate\".\n\t */\n\tif (!max_hstate)\n\t\tmhp = &default_hstate_max_huge_pages;\n\telse\n\t\tmhp = &parsed_hstate->max_huge_pages;\n\n\tif (mhp == last_mhp) {\n\t\tprintk(KERN_WARNING \"hugepages= specified twice without \"\n\t\t\t\"interleaving hugepagesz=, ignoring\\n\");\n\t\treturn 1;\n\t}\n\n\tif (sscanf(s, \"%lu\", mhp) <= 0)\n\t\t*mhp = 0;\n\n\t/*\n\t * Global state is always initialized later in hugetlb_init.\n\t * But we need to allocate >= MAX_ORDER hstates here early to still\n\t * use the bootmem allocator.\n\t */\n\tif (max_hstate && parsed_hstate->order >= MAX_ORDER)\n\t\thugetlb_hstate_alloc_pages(parsed_hstate);\n\n\tlast_mhp = mhp;\n\n\treturn 1;\n}\n__setup(\"hugepages=\", hugetlb_nrpages_setup);\n\nstatic int __init hugetlb_default_setup(char *s)\n{\n\tdefault_hstate_size = memparse(s, &s);\n\treturn 1;\n}\n__setup(\"default_hugepagesz=\", hugetlb_default_setup);\n\nstatic unsigned int cpuset_mems_nr(unsigned int *array)\n{\n\tint node;\n\tunsigned int nr = 0;\n\n\tfor_each_node_mask(node, cpuset_current_mems_allowed)\n\t\tnr += array[node];\n\n\treturn nr;\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void __user *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\ttmp = h->max_huge_pages;\n\n\tif (write && h->order >= MAX_ORDER)\n\t\treturn -EINVAL;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tNODEMASK_ALLOC(nodemask_t, nodes_allowed,\n\t\t\t\t\t\tGFP_KERNEL | __GFP_NORETRY);\n\t\tif (!(obey_mempolicy &&\n\t\t\t       init_nodemask_of_mempolicy(nodes_allowed))) {\n\t\t\tNODEMASK_FREE(nodes_allowed);\n\t\t\tnodes_allowed = &node_states[N_HIGH_MEMORY];\n\t\t}\n\t\th->max_huge_pages = set_max_huge_pages(h, tmp, nodes_allowed);\n\n\t\tif (nodes_allowed != &node_states[N_HIGH_MEMORY])\n\t\t\tNODEMASK_FREE(nodes_allowed);\n\t}\nout:\n\treturn ret;\n}\n\nint hugetlb_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void __user *buffer, size_t *length, loff_t *ppos)\n{\n\n\treturn hugetlb_sysctl_handler_common(false, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n\n#ifdef CONFIG_NUMA\nint hugetlb_mempolicy_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void __user *buffer, size_t *length, loff_t *ppos)\n{\n\treturn hugetlb_sysctl_handler_common(true, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n#endif /* CONFIG_NUMA */\n\nint hugetlb_treat_movable_handler(struct ctl_table *table, int write,\n\t\t\tvoid __user *buffer,\n\t\t\tsize_t *length, loff_t *ppos)\n{\n\tproc_dointvec(table, write, buffer, length, ppos);\n\tif (hugepages_treat_as_movable)\n\t\thtlb_alloc_mask = GFP_HIGHUSER_MOVABLE;\n\telse\n\t\thtlb_alloc_mask = GFP_HIGHUSER;\n\treturn 0;\n}\n\nint hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\t\tvoid __user *buffer,\n\t\t\tsize_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && h->order >= MAX_ORDER)\n\t\treturn -EINVAL;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}\n\n#endif /* CONFIG_SYSCTL */\n\nvoid hugetlb_report_meminfo(struct seq_file *m)\n{\n\tstruct hstate *h = &default_hstate;\n\tseq_printf(m,\n\t\t\t\"HugePages_Total:   %5lu\\n\"\n\t\t\t\"HugePages_Free:    %5lu\\n\"\n\t\t\t\"HugePages_Rsvd:    %5lu\\n\"\n\t\t\t\"HugePages_Surp:    %5lu\\n\"\n\t\t\t\"Hugepagesize:   %8lu kB\\n\",\n\t\t\th->nr_huge_pages,\n\t\t\th->free_huge_pages,\n\t\t\th->resv_huge_pages,\n\t\t\th->surplus_huge_pages,\n\t\t\t1UL << (huge_page_order(h) + PAGE_SHIFT - 10));\n}\n\nint hugetlb_report_node_meminfo(int nid, char *buf)\n{\n\tstruct hstate *h = &default_hstate;\n\treturn sprintf(buf,\n\t\t\"Node %d HugePages_Total: %5u\\n\"\n\t\t\"Node %d HugePages_Free:  %5u\\n\"\n\t\t\"Node %d HugePages_Surp:  %5u\\n\",\n\t\tnid, h->nr_huge_pages_node[nid],\n\t\tnid, h->free_huge_pages_node[nid],\n\t\tnid, h->surplus_huge_pages_node[nid]);\n}\n\n/* Return the number pages of memory we physically have, in PAGE_SIZE units. */\nunsigned long hugetlb_total_pages(void)\n{\n\tstruct hstate *h = &default_hstate;\n\treturn h->nr_huge_pages * pages_per_huge_page(h);\n}\n\nstatic int hugetlb_acct_memory(struct hstate *h, long delta)\n{\n\tint ret = -ENOMEM;\n\n\tspin_lock(&hugetlb_lock);\n\t/*\n\t * When cpuset is configured, it breaks the strict hugetlb page\n\t * reservation as the accounting is done on a global variable. Such\n\t * reservation is completely rubbish in the presence of cpuset because\n\t * the reservation is not checked against page availability for the\n\t * current cpuset. Application can still potentially OOM'ed by kernel\n\t * with lack of free htlb page in cpuset that the task is in.\n\t * Attempt to enforce strict accounting with cpuset is almost\n\t * impossible (or too ugly) because cpuset is too fluid that\n\t * task or memory node can be dynamically moved between cpusets.\n\t *\n\t * The change of semantics for shared hugetlb mapping with cpuset is\n\t * undesirable. However, in order to preserve some of the semantics,\n\t * we fall back to check against current free page availability as\n\t * a best attempt and hopefully to minimize the impact of changing\n\t * semantics that cpuset has.\n\t */\n\tif (delta > 0) {\n\t\tif (gather_surplus_pages(h, delta) < 0)\n\t\t\tgoto out;\n\n\t\tif (delta > cpuset_mems_nr(h->free_huge_pages_node)) {\n\t\t\treturn_unused_surplus_pages(h, delta);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = 0;\n\tif (delta < 0)\n\t\treturn_unused_surplus_pages(h, (unsigned long) -delta);\n\nout:\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\nstatic void hugetlb_vm_op_open(struct vm_area_struct *vma)\n{\n\tstruct resv_map *reservations = vma_resv_map(vma);\n\n\t/*\n\t * This new VMA should share its siblings reservation map if present.\n\t * The VMA will only ever have a valid reservation map pointer where\n\t * it is being copied for another still existing VMA.  As that VMA\n\t * has a reference to the reservation map it cannot disappear until\n\t * after this open call completes.  It is therefore safe to take a\n\t * new reference here without additional locking.\n\t */\n\tif (reservations)\n\t\tkref_get(&reservations->refs);\n}\n\nstatic void hugetlb_vm_op_close(struct vm_area_struct *vma)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct resv_map *reservations = vma_resv_map(vma);\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tunsigned long reserve;\n\tunsigned long start;\n\tunsigned long end;\n\n\tif (reservations) {\n\t\tstart = vma_hugecache_offset(h, vma, vma->vm_start);\n\t\tend = vma_hugecache_offset(h, vma, vma->vm_end);\n\n\t\treserve = (end - start) -\n\t\t\tregion_count(&reservations->regions, start, end);\n\n\t\tkref_put(&reservations->refs, resv_map_release);\n\n\t\tif (reserve) {\n\t\t\thugetlb_acct_memory(h, -reserve);\n\t\t\thugepage_subpool_put_pages(spool, reserve);\n\t\t}\n\t}\n}\n\n/*\n * We cannot handle pagefaults against hugetlb pages at all.  They cause\n * handle_mm_fault() to try to instantiate regular-sized pages in the\n * hugegpage VMA.  do_page_fault() is supposed to trap this, so BUG is we get\n * this far.\n */\nstatic int hugetlb_vm_op_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tBUG();\n\treturn 0;\n}\n\nconst struct vm_operations_struct hugetlb_vm_ops = {\n\t.fault = hugetlb_vm_op_fault,\n\t.open = hugetlb_vm_op_open,\n\t.close = hugetlb_vm_op_close,\n};\n\nstatic pte_t make_huge_pte(struct vm_area_struct *vma, struct page *page,\n\t\t\t\tint writable)\n{\n\tpte_t entry;\n\n\tif (writable) {\n\t\tentry =\n\t\t    pte_mkwrite(pte_mkdirty(mk_pte(page, vma->vm_page_prot)));\n\t} else {\n\t\tentry = huge_pte_wrprotect(mk_pte(page, vma->vm_page_prot));\n\t}\n\tentry = pte_mkyoung(entry);\n\tentry = pte_mkhuge(entry);\n\tentry = arch_make_huge_pte(entry, vma, page, writable);\n\n\treturn entry;\n}\n\nstatic void set_huge_ptep_writable(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address, pte_t *ptep)\n{\n\tpte_t entry;\n\n\tentry = pte_mkwrite(pte_mkdirty(huge_ptep_get(ptep)));\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry, 1))\n\t\tupdate_mmu_cache(vma, address, ptep);\n}\n\n\nint copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,\n\t\t\t    struct vm_area_struct *vma)\n{\n\tpte_t *src_pte, *dst_pte, entry;\n\tstruct page *ptepage;\n\tunsigned long addr;\n\tint cow;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\n\tcow = (vma->vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;\n\n\tfor (addr = vma->vm_start; addr < vma->vm_end; addr += sz) {\n\t\tsrc_pte = huge_pte_offset(src, addr);\n\t\tif (!src_pte)\n\t\t\tcontinue;\n\t\tdst_pte = huge_pte_alloc(dst, addr, sz);\n\t\tif (!dst_pte)\n\t\t\tgoto nomem;\n\n\t\t/* If the pagetables are shared don't copy or take references */\n\t\tif (dst_pte == src_pte)\n\t\t\tcontinue;\n\n\t\tspin_lock(&dst->page_table_lock);\n\t\tspin_lock_nested(&src->page_table_lock, SINGLE_DEPTH_NESTING);\n\t\tif (!huge_pte_none(huge_ptep_get(src_pte))) {\n\t\t\tif (cow)\n\t\t\t\thuge_ptep_set_wrprotect(src, addr, src_pte);\n\t\t\tentry = huge_ptep_get(src_pte);\n\t\t\tptepage = pte_page(entry);\n\t\t\tget_page(ptepage);\n\t\t\tpage_dup_rmap(ptepage);\n\t\t\tset_huge_pte_at(dst, addr, dst_pte, entry);\n\t\t}\n\t\tspin_unlock(&src->page_table_lock);\n\t\tspin_unlock(&dst->page_table_lock);\n\t}\n\treturn 0;\n\nnomem:\n\treturn -ENOMEM;\n}\n\nstatic int is_hugetlb_entry_migration(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn 0;\n\tswp = pte_to_swp_entry(pte);\n\tif (non_swap_entry(swp) && is_migration_entry(swp))\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n\nstatic int is_hugetlb_entry_hwpoisoned(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn 0;\n\tswp = pte_to_swp_entry(pte);\n\tif (non_swap_entry(swp) && is_hwpoison_entry(swp))\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n\nvoid __unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,\n\t\t\t    unsigned long end, struct page *ref_page)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tstruct page *page;\n\tstruct page *tmp;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\n\t/*\n\t * A page gathering list, protected by per file i_mmap_mutex. The\n\t * lock is used to avoid list corruption from multiple unmapping\n\t * of the same page since we are using page->lru.\n\t */\n\tLIST_HEAD(page_list);\n\n\tWARN_ON(!is_vm_hugetlb_page(vma));\n\tBUG_ON(start & ~huge_page_mask(h));\n\tBUG_ON(end & ~huge_page_mask(h));\n\n\tmmu_notifier_invalidate_range_start(mm, start, end);\n\tspin_lock(&mm->page_table_lock);\n\tfor (address = start; address < end; address += sz) {\n\t\tptep = huge_pte_offset(mm, address);\n\t\tif (!ptep)\n\t\t\tcontinue;\n\n\t\tif (huge_pmd_unshare(mm, &address, ptep))\n\t\t\tcontinue;\n\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (huge_pte_none(pte))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * HWPoisoned hugepage is already unmapped and dropped reference\n\t\t */\n\t\tif (unlikely(is_hugetlb_entry_hwpoisoned(pte)))\n\t\t\tcontinue;\n\n\t\tpage = pte_page(pte);\n\t\t/*\n\t\t * If a reference page is supplied, it is because a specific\n\t\t * page is being unmapped, not a range. Ensure the page we\n\t\t * are about to unmap is the actual page of interest.\n\t\t */\n\t\tif (ref_page) {\n\t\t\tif (page != ref_page)\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * Mark the VMA as having unmapped its page so that\n\t\t\t * future faults in this VMA will fail rather than\n\t\t\t * looking like data was lost\n\t\t\t */\n\t\t\tset_vma_resv_flags(vma, HPAGE_RESV_UNMAPPED);\n\t\t}\n\n\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\tif (pte_dirty(pte))\n\t\t\tset_page_dirty(page);\n\t\tlist_add(&page->lru, &page_list);\n\n\t\t/* Bail out after unmapping reference page if supplied */\n\t\tif (ref_page)\n\t\t\tbreak;\n\t}\n\tflush_tlb_range(vma, start, end);\n\tspin_unlock(&mm->page_table_lock);\n\tmmu_notifier_invalidate_range_end(mm, start, end);\n\tlist_for_each_entry_safe(page, tmp, &page_list, lru) {\n\t\tpage_remove_rmap(page);\n\t\tlist_del(&page->lru);\n\t\tput_page(page);\n\t}\n}\n\nvoid unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,\n\t\t\t  unsigned long end, struct page *ref_page)\n{\n\tmutex_lock(&vma->vm_file->f_mapping->i_mmap_mutex);\n\t__unmap_hugepage_range(vma, start, end, ref_page);\n\tmutex_unlock(&vma->vm_file->f_mapping->i_mmap_mutex);\n}\n\n/*\n * This is called when the original mapper is failing to COW a MAP_PRIVATE\n * mappping it owns the reserve page for. The intention is to unmap the page\n * from other VMAs and let the children be SIGKILLed if they are faulting the\n * same region.\n */\nstatic int unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t\tstruct page *page, unsigned long address)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct vm_area_struct *iter_vma;\n\tstruct address_space *mapping;\n\tstruct prio_tree_iter iter;\n\tpgoff_t pgoff;\n\n\t/*\n\t * vm_pgoff is in PAGE_SIZE units, hence the different calculation\n\t * from page cache lookup which is in HPAGE_SIZE units.\n\t */\n\taddress = address & huge_page_mask(h);\n\tpgoff = vma_hugecache_offset(h, vma, address);\n\tmapping = vma->vm_file->f_dentry->d_inode->i_mapping;\n\n\t/*\n\t * Take the mapping lock for the duration of the table walk. As\n\t * this mapping should be shared between all the VMAs,\n\t * __unmap_hugepage_range() is called as the lock is already held\n\t */\n\tmutex_lock(&mapping->i_mmap_mutex);\n\tvma_prio_tree_foreach(iter_vma, &iter, &mapping->i_mmap, pgoff, pgoff) {\n\t\t/* Do not unmap the current VMA */\n\t\tif (iter_vma == vma)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Unmap the page from other VMAs without their own reserves.\n\t\t * They get marked to be SIGKILLed if they fault in these\n\t\t * areas. This is because a future no-page fault on this VMA\n\t\t * could insert a zeroed page instead of the data existing\n\t\t * from the time of fork. This would look like data corruption\n\t\t */\n\t\tif (!is_vma_resv_set(iter_vma, HPAGE_RESV_OWNER))\n\t\t\t__unmap_hugepage_range(iter_vma,\n\t\t\t\taddress, address + huge_page_size(h),\n\t\t\t\tpage);\n\t}\n\tmutex_unlock(&mapping->i_mmap_mutex);\n\n\treturn 1;\n}\n\n/*\n * Hugetlb_cow() should be called with page lock of the original hugepage held.\n * Called with hugetlb_instantiation_mutex held and pte_page locked so we\n * cannot race with other handlers or page migration.\n * Keep the pte_same checks anyway to make transition from the mutex easier.\n */\nstatic int hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, pte_t *ptep, pte_t pte,\n\t\t\tstruct page *pagecache_page)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct page *old_page, *new_page;\n\tint avoidcopy;\n\tint outside_reserve = 0;\n\n\told_page = pte_page(pte);\n\nretry_avoidcopy:\n\t/* If no-one else is actually using this page, avoid the copy\n\t * and just make the page writable */\n\tavoidcopy = (page_mapcount(old_page) == 1);\n\tif (avoidcopy) {\n\t\tif (PageAnon(old_page))\n\t\t\tpage_move_anon_rmap(old_page, vma, address);\n\t\tset_huge_ptep_writable(vma, address, ptep);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If the process that created a MAP_PRIVATE mapping is about to\n\t * perform a COW due to a shared page count, attempt to satisfy\n\t * the allocation without using the existing reserves. The pagecache\n\t * page is used to determine if the reserve at this address was\n\t * consumed or not. If reserves were used, a partial faulted mapping\n\t * at the time of fork() could consume its reserves on COW instead\n\t * of the full address range.\n\t */\n\tif (!(vma->vm_flags & VM_MAYSHARE) &&\n\t\t\tis_vma_resv_set(vma, HPAGE_RESV_OWNER) &&\n\t\t\told_page != pagecache_page)\n\t\toutside_reserve = 1;\n\n\tpage_cache_get(old_page);\n\n\t/* Drop page_table_lock as buddy allocator may be called */\n\tspin_unlock(&mm->page_table_lock);\n\tnew_page = alloc_huge_page(vma, address, outside_reserve);\n\n\tif (IS_ERR(new_page)) {\n\t\tpage_cache_release(old_page);\n\n\t\t/*\n\t\t * If a process owning a MAP_PRIVATE mapping fails to COW,\n\t\t * it is due to references held by a child and an insufficient\n\t\t * huge page pool. To guarantee the original mappers\n\t\t * reliability, unmap the page from child processes. The child\n\t\t * may get SIGKILLed if it later faults.\n\t\t */\n\t\tif (outside_reserve) {\n\t\t\tBUG_ON(huge_pte_none(pte));\n\t\t\tif (unmap_ref_private(mm, vma, old_page, address)) {\n\t\t\t\tBUG_ON(huge_pte_none(pte));\n\t\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\t\tptep = huge_pte_offset(mm, address & huge_page_mask(h));\n\t\t\t\tif (likely(pte_same(huge_ptep_get(ptep), pte)))\n\t\t\t\t\tgoto retry_avoidcopy;\n\t\t\t\t/*\n\t\t\t\t * race occurs while re-acquiring page_table_lock, and\n\t\t\t\t * our job is done.\n\t\t\t\t */\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tWARN_ON_ONCE(1);\n\t\t}\n\n\t\t/* Caller expects lock to be held */\n\t\tspin_lock(&mm->page_table_lock);\n\t\treturn -PTR_ERR(new_page);\n\t}\n\n\t/*\n\t * When the original hugepage is shared one, it does not have\n\t * anon_vma prepared.\n\t */\n\tif (unlikely(anon_vma_prepare(vma))) {\n\t\tpage_cache_release(new_page);\n\t\tpage_cache_release(old_page);\n\t\t/* Caller expects lock to be held */\n\t\tspin_lock(&mm->page_table_lock);\n\t\treturn VM_FAULT_OOM;\n\t}\n\n\tcopy_user_huge_page(new_page, old_page, address, vma,\n\t\t\t    pages_per_huge_page(h));\n\t__SetPageUptodate(new_page);\n\n\t/*\n\t * Retake the page_table_lock to check for racing updates\n\t * before the page tables are altered\n\t */\n\tspin_lock(&mm->page_table_lock);\n\tptep = huge_pte_offset(mm, address & huge_page_mask(h));\n\tif (likely(pte_same(huge_ptep_get(ptep), pte))) {\n\t\t/* Break COW */\n\t\tmmu_notifier_invalidate_range_start(mm,\n\t\t\taddress & huge_page_mask(h),\n\t\t\t(address & huge_page_mask(h)) + huge_page_size(h));\n\t\thuge_ptep_clear_flush(vma, address, ptep);\n\t\tset_huge_pte_at(mm, address, ptep,\n\t\t\t\tmake_huge_pte(vma, new_page, 1));\n\t\tpage_remove_rmap(old_page);\n\t\thugepage_add_new_anon_rmap(new_page, vma, address);\n\t\t/* Make the old page be freed below */\n\t\tnew_page = old_page;\n\t\tmmu_notifier_invalidate_range_end(mm,\n\t\t\taddress & huge_page_mask(h),\n\t\t\t(address & huge_page_mask(h)) + huge_page_size(h));\n\t}\n\tpage_cache_release(new_page);\n\tpage_cache_release(old_page);\n\treturn 0;\n}\n\n/* Return the pagecache page at a given address within a VMA */\nstatic struct page *hugetlbfs_pagecache_page(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\treturn find_lock_page(mapping, idx);\n}\n\n/*\n * Return whether there is a pagecache page to back given address within VMA.\n * Caller follow_hugetlb_page() holds page_table_lock so we cannot lock_page.\n */\nstatic bool hugetlbfs_pagecache_present(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\tstruct page *page;\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\tpage = find_get_page(mapping, idx);\n\tif (page)\n\t\tput_page(page);\n\treturn page != NULL;\n}\n\nstatic int hugetlb_no_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, pte_t *ptep, unsigned int flags)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tint ret = VM_FAULT_SIGBUS;\n\tint anon_rmap = 0;\n\tpgoff_t idx;\n\tunsigned long size;\n\tstruct page *page;\n\tstruct address_space *mapping;\n\tpte_t new_pte;\n\n\t/*\n\t * Currently, we are forced to kill the process in the event the\n\t * original mapper has unmapped pages from the child due to a failed\n\t * COW. Warn that such a situation has occurred as it may not be obvious\n\t */\n\tif (is_vma_resv_set(vma, HPAGE_RESV_UNMAPPED)) {\n\t\tprintk(KERN_WARNING\n\t\t\t\"PID %d killed due to inadequate hugepage pool\\n\",\n\t\t\tcurrent->pid);\n\t\treturn ret;\n\t}\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\t/*\n\t * Use page lock to guard against racing truncation\n\t * before we get page_table_lock.\n\t */\nretry:\n\tpage = find_lock_page(mapping, idx);\n\tif (!page) {\n\t\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\t\tif (idx >= size)\n\t\t\tgoto out;\n\t\tpage = alloc_huge_page(vma, address, 0);\n\t\tif (IS_ERR(page)) {\n\t\t\tret = -PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\n\t\tclear_huge_page(page, address, pages_per_huge_page(h));\n\t\t__SetPageUptodate(page);\n\n\t\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t\tint err;\n\t\t\tstruct inode *inode = mapping->host;\n\n\t\t\terr = add_to_page_cache(page, mapping, idx, GFP_KERNEL);\n\t\t\tif (err) {\n\t\t\t\tput_page(page);\n\t\t\t\tif (err == -EEXIST)\n\t\t\t\t\tgoto retry;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tinode->i_blocks += blocks_per_huge_page(h);\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t} else {\n\t\t\tlock_page(page);\n\t\t\tif (unlikely(anon_vma_prepare(vma))) {\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\tgoto backout_unlocked;\n\t\t\t}\n\t\t\tanon_rmap = 1;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If memory error occurs between mmap() and fault, some process\n\t\t * don't have hwpoisoned swap entry for errored virtual address.\n\t\t * So we need to block hugepage fault by PG_hwpoison bit check.\n\t\t */\n\t\tif (unlikely(PageHWPoison(page))) {\n\t\t\tret = VM_FAULT_HWPOISON |\n\t\t\t      VM_FAULT_SET_HINDEX(h - hstates);\n\t\t\tgoto backout_unlocked;\n\t\t}\n\t}\n\n\t/*\n\t * If we are going to COW a private mapping later, we examine the\n\t * pending reservations for this page now. This will ensure that\n\t * any allocations necessary to record that reservation occur outside\n\t * the spinlock.\n\t */\n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED))\n\t\tif (vma_needs_reservation(h, vma, address) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto backout_unlocked;\n\t\t}\n\n\tspin_lock(&mm->page_table_lock);\n\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\tif (idx >= size)\n\t\tgoto backout;\n\n\tret = 0;\n\tif (!huge_pte_none(huge_ptep_get(ptep)))\n\t\tgoto backout;\n\n\tif (anon_rmap)\n\t\thugepage_add_new_anon_rmap(page, vma, address);\n\telse\n\t\tpage_dup_rmap(page);\n\tnew_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)\n\t\t\t\t&& (vma->vm_flags & VM_SHARED)));\n\tset_huge_pte_at(mm, address, ptep, new_pte);\n\n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {\n\t\t/* Optimization, do the COW without a second fault */\n\t\tret = hugetlb_cow(mm, vma, address, ptep, new_pte, page);\n\t}\n\n\tspin_unlock(&mm->page_table_lock);\n\tunlock_page(page);\nout:\n\treturn ret;\n\nbackout:\n\tspin_unlock(&mm->page_table_lock);\nbackout_unlocked:\n\tunlock_page(page);\n\tput_page(page);\n\tgoto out;\n}\n\nint hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, unsigned int flags)\n{\n\tpte_t *ptep;\n\tpte_t entry;\n\tint ret;\n\tstruct page *page = NULL;\n\tstruct page *pagecache_page = NULL;\n\tstatic DEFINE_MUTEX(hugetlb_instantiation_mutex);\n\tstruct hstate *h = hstate_vma(vma);\n\n\taddress &= huge_page_mask(h);\n\n\tptep = huge_pte_offset(mm, address);\n\tif (ptep) {\n\t\tentry = huge_ptep_get(ptep);\n\t\tif (unlikely(is_hugetlb_entry_migration(entry))) {\n\t\t\tmigration_entry_wait(mm, (pmd_t *)ptep, address);\n\t\t\treturn 0;\n\t\t} else if (unlikely(is_hugetlb_entry_hwpoisoned(entry)))\n\t\t\treturn VM_FAULT_HWPOISON_LARGE |\n\t\t\t       VM_FAULT_SET_HINDEX(h - hstates);\n\t}\n\n\tptep = huge_pte_alloc(mm, address, huge_page_size(h));\n\tif (!ptep)\n\t\treturn VM_FAULT_OOM;\n\n\t/*\n\t * Serialize hugepage allocation and instantiation, so that we don't\n\t * get spurious allocation failures if two CPUs race to instantiate\n\t * the same page in the page cache.\n\t */\n\tmutex_lock(&hugetlb_instantiation_mutex);\n\tentry = huge_ptep_get(ptep);\n\tif (huge_pte_none(entry)) {\n\t\tret = hugetlb_no_page(mm, vma, address, ptep, flags);\n\t\tgoto out_mutex;\n\t}\n\n\tret = 0;\n\n\t/*\n\t * If we are going to COW the mapping later, we examine the pending\n\t * reservations for this page now. This will ensure that any\n\t * allocations necessary to record that reservation occur outside the\n\t * spinlock. For private mappings, we also lookup the pagecache\n\t * page now as it is used to determine if a reservation has been\n\t * consumed.\n\t */\n\tif ((flags & FAULT_FLAG_WRITE) && !pte_write(entry)) {\n\t\tif (vma_needs_reservation(h, vma, address) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_mutex;\n\t\t}\n\n\t\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\t\tpagecache_page = hugetlbfs_pagecache_page(h,\n\t\t\t\t\t\t\t\tvma, address);\n\t}\n\n\t/*\n\t * hugetlb_cow() requires page locks of pte_page(entry) and\n\t * pagecache_page, so here we need take the former one\n\t * when page != pagecache_page or !pagecache_page.\n\t * Note that locking order is always pagecache_page -> page,\n\t * so no worry about deadlock.\n\t */\n\tpage = pte_page(entry);\n\tget_page(page);\n\tif (page != pagecache_page)\n\t\tlock_page(page);\n\n\tspin_lock(&mm->page_table_lock);\n\t/* Check for a racing update before calling hugetlb_cow */\n\tif (unlikely(!pte_same(entry, huge_ptep_get(ptep))))\n\t\tgoto out_page_table_lock;\n\n\n\tif (flags & FAULT_FLAG_WRITE) {\n\t\tif (!pte_write(entry)) {\n\t\t\tret = hugetlb_cow(mm, vma, address, ptep, entry,\n\t\t\t\t\t\t\tpagecache_page);\n\t\t\tgoto out_page_table_lock;\n\t\t}\n\t\tentry = pte_mkdirty(entry);\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry,\n\t\t\t\t\t\tflags & FAULT_FLAG_WRITE))\n\t\tupdate_mmu_cache(vma, address, ptep);\n\nout_page_table_lock:\n\tspin_unlock(&mm->page_table_lock);\n\n\tif (pagecache_page) {\n\t\tunlock_page(pagecache_page);\n\t\tput_page(pagecache_page);\n\t}\n\tif (page != pagecache_page)\n\t\tunlock_page(page);\n\tput_page(page);\n\nout_mutex:\n\tmutex_unlock(&hugetlb_instantiation_mutex);\n\n\treturn ret;\n}\n\n/* Can be overriden by architectures */\n__attribute__((weak)) struct page *\nfollow_huge_pud(struct mm_struct *mm, unsigned long address,\n\t       pud_t *pud, int write)\n{\n\tBUG();\n\treturn NULL;\n}\n\nint follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tstruct page **pages, struct vm_area_struct **vmas,\n\t\t\tunsigned long *position, int *length, int i,\n\t\t\tunsigned int flags)\n{\n\tunsigned long pfn_offset;\n\tunsigned long vaddr = *position;\n\tint remainder = *length;\n\tstruct hstate *h = hstate_vma(vma);\n\n\tspin_lock(&mm->page_table_lock);\n\twhile (vaddr < vma->vm_end && remainder) {\n\t\tpte_t *pte;\n\t\tint absent;\n\t\tstruct page *page;\n\n\t\t/*\n\t\t * Some archs (sparc64, sh*) have multiple pte_ts to\n\t\t * each hugepage.  We have to make sure we get the\n\t\t * first, for the page indexing below to work.\n\t\t */\n\t\tpte = huge_pte_offset(mm, vaddr & huge_page_mask(h));\n\t\tabsent = !pte || huge_pte_none(huge_ptep_get(pte));\n\n\t\t/*\n\t\t * When coredumping, it suits get_dump_page if we just return\n\t\t * an error where there's an empty slot with no huge pagecache\n\t\t * to back it.  This way, we avoid allocating a hugepage, and\n\t\t * the sparse dumpfile avoids allocating disk blocks, but its\n\t\t * huge holes still show up with zeroes where they need to be.\n\t\t */\n\t\tif (absent && (flags & FOLL_DUMP) &&\n\t\t    !hugetlbfs_pagecache_present(h, vma, vaddr)) {\n\t\t\tremainder = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (absent ||\n\t\t    ((flags & FOLL_WRITE) && !pte_write(huge_ptep_get(pte)))) {\n\t\t\tint ret;\n\n\t\t\tspin_unlock(&mm->page_table_lock);\n\t\t\tret = hugetlb_fault(mm, vma, vaddr,\n\t\t\t\t(flags & FOLL_WRITE) ? FAULT_FLAG_WRITE : 0);\n\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\tif (!(ret & VM_FAULT_ERROR))\n\t\t\t\tcontinue;\n\n\t\t\tremainder = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tpfn_offset = (vaddr & ~huge_page_mask(h)) >> PAGE_SHIFT;\n\t\tpage = pte_page(huge_ptep_get(pte));\nsame_page:\n\t\tif (pages) {\n\t\t\tpages[i] = mem_map_offset(page, pfn_offset);\n\t\t\tget_page(pages[i]);\n\t\t}\n\n\t\tif (vmas)\n\t\t\tvmas[i] = vma;\n\n\t\tvaddr += PAGE_SIZE;\n\t\t++pfn_offset;\n\t\t--remainder;\n\t\t++i;\n\t\tif (vaddr < vma->vm_end && remainder &&\n\t\t\t\tpfn_offset < pages_per_huge_page(h)) {\n\t\t\t/*\n\t\t\t * We use pfn_offset to avoid touching the pageframes\n\t\t\t * of this compound page.\n\t\t\t */\n\t\t\tgoto same_page;\n\t\t}\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\t*length = remainder;\n\t*position = vaddr;\n\n\treturn i ? i : -EFAULT;\n}\n\nvoid hugetlb_change_protection(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned long end, pgprot_t newprot)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long start = address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tstruct hstate *h = hstate_vma(vma);\n\n\tBUG_ON(address >= end);\n\tflush_cache_range(vma, address, end);\n\n\tmutex_lock(&vma->vm_file->f_mapping->i_mmap_mutex);\n\tspin_lock(&mm->page_table_lock);\n\tfor (; address < end; address += huge_page_size(h)) {\n\t\tptep = huge_pte_offset(mm, address);\n\t\tif (!ptep)\n\t\t\tcontinue;\n\t\tif (huge_pmd_unshare(mm, &address, ptep))\n\t\t\tcontinue;\n\t\tif (!huge_pte_none(huge_ptep_get(ptep))) {\n\t\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\t\tpte = pte_mkhuge(pte_modify(pte, newprot));\n\t\t\tset_huge_pte_at(mm, address, ptep, pte);\n\t\t}\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\tmutex_unlock(&vma->vm_file->f_mapping->i_mmap_mutex);\n\n\tflush_tlb_range(vma, start, end);\n}\n\nint hugetlb_reserve_pages(struct inode *inode,\n\t\t\t\t\tlong from, long to,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tvm_flags_t vm_flags)\n{\n\tlong ret, chg;\n\tstruct hstate *h = hstate_inode(inode);\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\n\t/*\n\t * Only apply hugepage reservation if asked. At fault time, an\n\t * attempt will be made for VM_NORESERVE to allocate a page\n\t * without using reserves\n\t */\n\tif (vm_flags & VM_NORESERVE)\n\t\treturn 0;\n\n\t/*\n\t * Shared mappings base their reservation on the number of pages that\n\t * are already allocated on behalf of the file. Private mappings need\n\t * to reserve the full area even if read-only as mprotect() may be\n\t * called to make the mapping read-write. Assume !vma is a shm mapping\n\t */\n\tif (!vma || vma->vm_flags & VM_MAYSHARE)\n\t\tchg = region_chg(&inode->i_mapping->private_list, from, to);\n\telse {\n\t\tstruct resv_map *resv_map = resv_map_alloc();\n\t\tif (!resv_map)\n\t\t\treturn -ENOMEM;\n\n\t\tchg = to - from;\n\n\t\tset_vma_resv_map(vma, resv_map);\n\t\tset_vma_resv_flags(vma, HPAGE_RESV_OWNER);\n\t}\n\n\tif (chg < 0)\n\t\treturn chg;\n\n\t/* There must be enough pages in the subpool for the mapping */\n\tif (hugepage_subpool_get_pages(spool, chg))\n\t\treturn -ENOSPC;\n\n\t/*\n\t * Check enough hugepages are available for the reservation.\n\t * Hand the pages back to the subpool if there are not\n\t */\n\tret = hugetlb_acct_memory(h, chg);\n\tif (ret < 0) {\n\t\thugepage_subpool_put_pages(spool, chg);\n\t\treturn ret;\n\t}\n\n\t/*\n\t * Account for the reservations made. Shared mappings record regions\n\t * that have reservations as they are shared by multiple VMAs.\n\t * When the last VMA disappears, the region map says how much\n\t * the reservation was and the page cache tells how much of\n\t * the reservation was consumed. Private mappings are per-VMA and\n\t * only the consumed reservations are tracked. When the VMA\n\t * disappears, the original reservation is the VMA size and the\n\t * consumed reservations are stored in the map. Hence, nothing\n\t * else has to be done for private mappings here\n\t */\n\tif (!vma || vma->vm_flags & VM_MAYSHARE)\n\t\tregion_add(&inode->i_mapping->private_list, from, to);\n\treturn 0;\n}\n\nvoid hugetlb_unreserve_pages(struct inode *inode, long offset, long freed)\n{\n\tstruct hstate *h = hstate_inode(inode);\n\tlong chg = region_truncate(&inode->i_mapping->private_list, offset);\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks -= (blocks_per_huge_page(h) * freed);\n\tspin_unlock(&inode->i_lock);\n\n\thugepage_subpool_put_pages(spool, (chg - freed));\n\thugetlb_acct_memory(h, -(chg - freed));\n}\n\n#ifdef CONFIG_MEMORY_FAILURE\n\n/* Should be called in hugetlb_lock */\nstatic int is_hugepage_on_freelist(struct page *hpage)\n{\n\tstruct page *page;\n\tstruct page *tmp;\n\tstruct hstate *h = page_hstate(hpage);\n\tint nid = page_to_nid(hpage);\n\n\tlist_for_each_entry_safe(page, tmp, &h->hugepage_freelists[nid], lru)\n\t\tif (page == hpage)\n\t\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function is called from memory failure code.\n * Assume the caller holds page lock of the head page.\n */\nint dequeue_hwpoisoned_huge_page(struct page *hpage)\n{\n\tstruct hstate *h = page_hstate(hpage);\n\tint nid = page_to_nid(hpage);\n\tint ret = -EBUSY;\n\n\tspin_lock(&hugetlb_lock);\n\tif (is_hugepage_on_freelist(hpage)) {\n\t\tlist_del(&hpage->lru);\n\t\tset_page_refcounted(hpage);\n\t\th->free_huge_pages--;\n\t\th->free_huge_pages_node[nid]--;\n\t\tret = 0;\n\t}\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n#endif\n"], "fixing_code": ["/*\n * Generic hugetlb support.\n * (C) William Irwin, April 2004\n */\n#include <linux/list.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/seq_file.h>\n#include <linux/sysctl.h>\n#include <linux/highmem.h>\n#include <linux/mmu_notifier.h>\n#include <linux/nodemask.h>\n#include <linux/pagemap.h>\n#include <linux/mempolicy.h>\n#include <linux/cpuset.h>\n#include <linux/mutex.h>\n#include <linux/bootmem.h>\n#include <linux/sysfs.h>\n#include <linux/slab.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/swapops.h>\n\n#include <asm/page.h>\n#include <asm/pgtable.h>\n#include <linux/io.h>\n\n#include <linux/hugetlb.h>\n#include <linux/node.h>\n#include \"internal.h\"\n\nconst unsigned long hugetlb_zero = 0, hugetlb_infinity = ~0UL;\nstatic gfp_t htlb_alloc_mask = GFP_HIGHUSER;\nunsigned long hugepages_treat_as_movable;\n\nstatic int max_hstate;\nunsigned int default_hstate_idx;\nstruct hstate hstates[HUGE_MAX_HSTATE];\n\n__initdata LIST_HEAD(huge_boot_pages);\n\n/* for command line parsing */\nstatic struct hstate * __initdata parsed_hstate;\nstatic unsigned long __initdata default_hstate_max_huge_pages;\nstatic unsigned long __initdata default_hstate_size;\n\n#define for_each_hstate(h) \\\n\tfor ((h) = hstates; (h) < &hstates[max_hstate]; (h)++)\n\n/*\n * Protects updates to hugepage_freelists, nr_huge_pages, and free_huge_pages\n */\nstatic DEFINE_SPINLOCK(hugetlb_lock);\n\nstatic inline void unlock_or_release_subpool(struct hugepage_subpool *spool)\n{\n\tbool free = (spool->count == 0) && (spool->used_hpages == 0);\n\n\tspin_unlock(&spool->lock);\n\n\t/* If no pages are used, and no other handles to the subpool\n\t * remain, free the subpool the subpool remain */\n\tif (free)\n\t\tkfree(spool);\n}\n\nstruct hugepage_subpool *hugepage_new_subpool(long nr_blocks)\n{\n\tstruct hugepage_subpool *spool;\n\n\tspool = kmalloc(sizeof(*spool), GFP_KERNEL);\n\tif (!spool)\n\t\treturn NULL;\n\n\tspin_lock_init(&spool->lock);\n\tspool->count = 1;\n\tspool->max_hpages = nr_blocks;\n\tspool->used_hpages = 0;\n\n\treturn spool;\n}\n\nvoid hugepage_put_subpool(struct hugepage_subpool *spool)\n{\n\tspin_lock(&spool->lock);\n\tBUG_ON(!spool->count);\n\tspool->count--;\n\tunlock_or_release_subpool(spool);\n}\n\nstatic int hugepage_subpool_get_pages(struct hugepage_subpool *spool,\n\t\t\t\t      long delta)\n{\n\tint ret = 0;\n\n\tif (!spool)\n\t\treturn 0;\n\n\tspin_lock(&spool->lock);\n\tif ((spool->used_hpages + delta) <= spool->max_hpages) {\n\t\tspool->used_hpages += delta;\n\t} else {\n\t\tret = -ENOMEM;\n\t}\n\tspin_unlock(&spool->lock);\n\n\treturn ret;\n}\n\nstatic void hugepage_subpool_put_pages(struct hugepage_subpool *spool,\n\t\t\t\t       long delta)\n{\n\tif (!spool)\n\t\treturn;\n\n\tspin_lock(&spool->lock);\n\tspool->used_hpages -= delta;\n\t/* If hugetlbfs_put_super couldn't free spool due to\n\t* an outstanding quota reference, free it now. */\n\tunlock_or_release_subpool(spool);\n}\n\nstatic inline struct hugepage_subpool *subpool_inode(struct inode *inode)\n{\n\treturn HUGETLBFS_SB(inode->i_sb)->spool;\n}\n\nstatic inline struct hugepage_subpool *subpool_vma(struct vm_area_struct *vma)\n{\n\treturn subpool_inode(vma->vm_file->f_dentry->d_inode);\n}\n\n/*\n * Region tracking -- allows tracking of reservations and instantiated pages\n *                    across the pages in a mapping.\n *\n * The region data structures are protected by a combination of the mmap_sem\n * and the hugetlb_instantion_mutex.  To access or modify a region the caller\n * must either hold the mmap_sem for write, or the mmap_sem for read and\n * the hugetlb_instantiation mutex:\n *\n *\tdown_write(&mm->mmap_sem);\n * or\n *\tdown_read(&mm->mmap_sem);\n *\tmutex_lock(&hugetlb_instantiation_mutex);\n */\nstruct file_region {\n\tstruct list_head link;\n\tlong from;\n\tlong to;\n};\n\nstatic long region_add(struct list_head *head, long f, long t)\n{\n\tstruct file_region *rg, *nrg, *trg;\n\n\t/* Locate the region we are either in or before. */\n\tlist_for_each_entry(rg, head, link)\n\t\tif (f <= rg->to)\n\t\t\tbreak;\n\n\t/* Round our left edge to the current segment if it encloses us. */\n\tif (f > rg->from)\n\t\tf = rg->from;\n\n\t/* Check for and consume any regions we now overlap with. */\n\tnrg = rg;\n\tlist_for_each_entry_safe(rg, trg, rg->link.prev, link) {\n\t\tif (&rg->link == head)\n\t\t\tbreak;\n\t\tif (rg->from > t)\n\t\t\tbreak;\n\n\t\t/* If this area reaches higher then extend our area to\n\t\t * include it completely.  If this is not the first area\n\t\t * which we intend to reuse, free it. */\n\t\tif (rg->to > t)\n\t\t\tt = rg->to;\n\t\tif (rg != nrg) {\n\t\t\tlist_del(&rg->link);\n\t\t\tkfree(rg);\n\t\t}\n\t}\n\tnrg->from = f;\n\tnrg->to = t;\n\treturn 0;\n}\n\nstatic long region_chg(struct list_head *head, long f, long t)\n{\n\tstruct file_region *rg, *nrg;\n\tlong chg = 0;\n\n\t/* Locate the region we are before or in. */\n\tlist_for_each_entry(rg, head, link)\n\t\tif (f <= rg->to)\n\t\t\tbreak;\n\n\t/* If we are below the current region then a new region is required.\n\t * Subtle, allocate a new region at the position but make it zero\n\t * size such that we can guarantee to record the reservation. */\n\tif (&rg->link == head || t < rg->from) {\n\t\tnrg = kmalloc(sizeof(*nrg), GFP_KERNEL);\n\t\tif (!nrg)\n\t\t\treturn -ENOMEM;\n\t\tnrg->from = f;\n\t\tnrg->to   = f;\n\t\tINIT_LIST_HEAD(&nrg->link);\n\t\tlist_add(&nrg->link, rg->link.prev);\n\n\t\treturn t - f;\n\t}\n\n\t/* Round our left edge to the current segment if it encloses us. */\n\tif (f > rg->from)\n\t\tf = rg->from;\n\tchg = t - f;\n\n\t/* Check for and consume any regions we now overlap with. */\n\tlist_for_each_entry(rg, rg->link.prev, link) {\n\t\tif (&rg->link == head)\n\t\t\tbreak;\n\t\tif (rg->from > t)\n\t\t\treturn chg;\n\n\t\t/* We overlap with this area, if it extends further than\n\t\t * us then we must extend ourselves.  Account for its\n\t\t * existing reservation. */\n\t\tif (rg->to > t) {\n\t\t\tchg += rg->to - t;\n\t\t\tt = rg->to;\n\t\t}\n\t\tchg -= rg->to - rg->from;\n\t}\n\treturn chg;\n}\n\nstatic long region_truncate(struct list_head *head, long end)\n{\n\tstruct file_region *rg, *trg;\n\tlong chg = 0;\n\n\t/* Locate the region we are either in or before. */\n\tlist_for_each_entry(rg, head, link)\n\t\tif (end <= rg->to)\n\t\t\tbreak;\n\tif (&rg->link == head)\n\t\treturn 0;\n\n\t/* If we are in the middle of a region then adjust it. */\n\tif (end > rg->from) {\n\t\tchg = rg->to - end;\n\t\trg->to = end;\n\t\trg = list_entry(rg->link.next, typeof(*rg), link);\n\t}\n\n\t/* Drop any remaining regions. */\n\tlist_for_each_entry_safe(rg, trg, rg->link.prev, link) {\n\t\tif (&rg->link == head)\n\t\t\tbreak;\n\t\tchg += rg->to - rg->from;\n\t\tlist_del(&rg->link);\n\t\tkfree(rg);\n\t}\n\treturn chg;\n}\n\nstatic long region_count(struct list_head *head, long f, long t)\n{\n\tstruct file_region *rg;\n\tlong chg = 0;\n\n\t/* Locate each segment we overlap with, and count that overlap. */\n\tlist_for_each_entry(rg, head, link) {\n\t\tlong seg_from;\n\t\tlong seg_to;\n\n\t\tif (rg->to <= f)\n\t\t\tcontinue;\n\t\tif (rg->from >= t)\n\t\t\tbreak;\n\n\t\tseg_from = max(rg->from, f);\n\t\tseg_to = min(rg->to, t);\n\n\t\tchg += seg_to - seg_from;\n\t}\n\n\treturn chg;\n}\n\n/*\n * Convert the address within this vma to the page offset within\n * the mapping, in pagecache page units; huge pages here.\n */\nstatic pgoff_t vma_hugecache_offset(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\treturn ((address - vma->vm_start) >> huge_page_shift(h)) +\n\t\t\t(vma->vm_pgoff >> huge_page_order(h));\n}\n\npgoff_t linear_hugepage_index(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long address)\n{\n\treturn vma_hugecache_offset(hstate_vma(vma), vma, address);\n}\n\n/*\n * Return the size of the pages allocated when backing a VMA. In the majority\n * cases this will be same size as used by the page table entries.\n */\nunsigned long vma_kernel_pagesize(struct vm_area_struct *vma)\n{\n\tstruct hstate *hstate;\n\n\tif (!is_vm_hugetlb_page(vma))\n\t\treturn PAGE_SIZE;\n\n\thstate = hstate_vma(vma);\n\n\treturn 1UL << (hstate->order + PAGE_SHIFT);\n}\nEXPORT_SYMBOL_GPL(vma_kernel_pagesize);\n\n/*\n * Return the page size being used by the MMU to back a VMA. In the majority\n * of cases, the page size used by the kernel matches the MMU size. On\n * architectures where it differs, an architecture-specific version of this\n * function is required.\n */\n#ifndef vma_mmu_pagesize\nunsigned long vma_mmu_pagesize(struct vm_area_struct *vma)\n{\n\treturn vma_kernel_pagesize(vma);\n}\n#endif\n\n/*\n * Flags for MAP_PRIVATE reservations.  These are stored in the bottom\n * bits of the reservation map pointer, which are always clear due to\n * alignment.\n */\n#define HPAGE_RESV_OWNER    (1UL << 0)\n#define HPAGE_RESV_UNMAPPED (1UL << 1)\n#define HPAGE_RESV_MASK (HPAGE_RESV_OWNER | HPAGE_RESV_UNMAPPED)\n\n/*\n * These helpers are used to track how many pages are reserved for\n * faults in a MAP_PRIVATE mapping. Only the process that called mmap()\n * is guaranteed to have their future faults succeed.\n *\n * With the exception of reset_vma_resv_huge_pages() which is called at fork(),\n * the reserve counters are updated with the hugetlb_lock held. It is safe\n * to reset the VMA at fork() time as it is not in use yet and there is no\n * chance of the global counters getting corrupted as a result of the values.\n *\n * The private mapping reservation is represented in a subtly different\n * manner to a shared mapping.  A shared mapping has a region map associated\n * with the underlying file, this region map represents the backing file\n * pages which have ever had a reservation assigned which this persists even\n * after the page is instantiated.  A private mapping has a region map\n * associated with the original mmap which is attached to all VMAs which\n * reference it, this region map represents those offsets which have consumed\n * reservation ie. where pages have been instantiated.\n */\nstatic unsigned long get_vma_private_data(struct vm_area_struct *vma)\n{\n\treturn (unsigned long)vma->vm_private_data;\n}\n\nstatic void set_vma_private_data(struct vm_area_struct *vma,\n\t\t\t\t\t\t\tunsigned long value)\n{\n\tvma->vm_private_data = (void *)value;\n}\n\nstruct resv_map {\n\tstruct kref refs;\n\tstruct list_head regions;\n};\n\nstatic struct resv_map *resv_map_alloc(void)\n{\n\tstruct resv_map *resv_map = kmalloc(sizeof(*resv_map), GFP_KERNEL);\n\tif (!resv_map)\n\t\treturn NULL;\n\n\tkref_init(&resv_map->refs);\n\tINIT_LIST_HEAD(&resv_map->regions);\n\n\treturn resv_map;\n}\n\nstatic void resv_map_release(struct kref *ref)\n{\n\tstruct resv_map *resv_map = container_of(ref, struct resv_map, refs);\n\n\t/* Clear out any active regions before we release the map. */\n\tregion_truncate(&resv_map->regions, 0);\n\tkfree(resv_map);\n}\n\nstatic struct resv_map *vma_resv_map(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON(!is_vm_hugetlb_page(vma));\n\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\treturn (struct resv_map *)(get_vma_private_data(vma) &\n\t\t\t\t\t\t\t~HPAGE_RESV_MASK);\n\treturn NULL;\n}\n\nstatic void set_vma_resv_map(struct vm_area_struct *vma, struct resv_map *map)\n{\n\tVM_BUG_ON(!is_vm_hugetlb_page(vma));\n\tVM_BUG_ON(vma->vm_flags & VM_MAYSHARE);\n\n\tset_vma_private_data(vma, (get_vma_private_data(vma) &\n\t\t\t\tHPAGE_RESV_MASK) | (unsigned long)map);\n}\n\nstatic void set_vma_resv_flags(struct vm_area_struct *vma, unsigned long flags)\n{\n\tVM_BUG_ON(!is_vm_hugetlb_page(vma));\n\tVM_BUG_ON(vma->vm_flags & VM_MAYSHARE);\n\n\tset_vma_private_data(vma, get_vma_private_data(vma) | flags);\n}\n\nstatic int is_vma_resv_set(struct vm_area_struct *vma, unsigned long flag)\n{\n\tVM_BUG_ON(!is_vm_hugetlb_page(vma));\n\n\treturn (get_vma_private_data(vma) & flag) != 0;\n}\n\n/* Decrement the reserved pages in the hugepage pool by one */\nstatic void decrement_hugepage_resv_vma(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma)\n{\n\tif (vma->vm_flags & VM_NORESERVE)\n\t\treturn;\n\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t/* Shared mappings always use reserves */\n\t\th->resv_huge_pages--;\n\t} else if (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\t/*\n\t\t * Only the process that called mmap() has reserves for\n\t\t * private mappings.\n\t\t */\n\t\th->resv_huge_pages--;\n\t}\n}\n\n/* Reset counters to 0 and clear all HPAGE_RESV_* flags */\nvoid reset_vma_resv_huge_pages(struct vm_area_struct *vma)\n{\n\tVM_BUG_ON(!is_vm_hugetlb_page(vma));\n\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\tvma->vm_private_data = (void *)0;\n}\n\n/* Returns true if the VMA has associated reserve pages */\nstatic int vma_has_reserves(struct vm_area_struct *vma)\n{\n\tif (vma->vm_flags & VM_MAYSHARE)\n\t\treturn 1;\n\tif (is_vma_resv_set(vma, HPAGE_RESV_OWNER))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void copy_gigantic_page(struct page *dst, struct page *src)\n{\n\tint i;\n\tstruct hstate *h = page_hstate(src);\n\tstruct page *dst_base = dst;\n\tstruct page *src_base = src;\n\n\tfor (i = 0; i < pages_per_huge_page(h); ) {\n\t\tcond_resched();\n\t\tcopy_highpage(dst, src);\n\n\t\ti++;\n\t\tdst = mem_map_next(dst, dst_base, i);\n\t\tsrc = mem_map_next(src, src_base, i);\n\t}\n}\n\nvoid copy_huge_page(struct page *dst, struct page *src)\n{\n\tint i;\n\tstruct hstate *h = page_hstate(src);\n\n\tif (unlikely(pages_per_huge_page(h) > MAX_ORDER_NR_PAGES)) {\n\t\tcopy_gigantic_page(dst, src);\n\t\treturn;\n\t}\n\n\tmight_sleep();\n\tfor (i = 0; i < pages_per_huge_page(h); i++) {\n\t\tcond_resched();\n\t\tcopy_highpage(dst + i, src + i);\n\t}\n}\n\nstatic void enqueue_huge_page(struct hstate *h, struct page *page)\n{\n\tint nid = page_to_nid(page);\n\tlist_add(&page->lru, &h->hugepage_freelists[nid]);\n\th->free_huge_pages++;\n\th->free_huge_pages_node[nid]++;\n}\n\nstatic struct page *dequeue_huge_page_node(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tif (list_empty(&h->hugepage_freelists[nid]))\n\t\treturn NULL;\n\tpage = list_entry(h->hugepage_freelists[nid].next, struct page, lru);\n\tlist_del(&page->lru);\n\tset_page_refcounted(page);\n\th->free_huge_pages--;\n\th->free_huge_pages_node[nid]--;\n\treturn page;\n}\n\nstatic struct page *dequeue_huge_page_vma(struct hstate *h,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long address, int avoid_reserve)\n{\n\tstruct page *page = NULL;\n\tstruct mempolicy *mpol;\n\tnodemask_t *nodemask;\n\tstruct zonelist *zonelist;\n\tstruct zone *zone;\n\tstruct zoneref *z;\n\tunsigned int cpuset_mems_cookie;\n\nretry_cpuset:\n\tcpuset_mems_cookie = get_mems_allowed();\n\tzonelist = huge_zonelist(vma, address,\n\t\t\t\t\thtlb_alloc_mask, &mpol, &nodemask);\n\t/*\n\t * A child process with MAP_PRIVATE mappings created by their parent\n\t * have no page reserves. This check ensures that reservations are\n\t * not \"stolen\". The child may still get SIGKILLed\n\t */\n\tif (!vma_has_reserves(vma) &&\n\t\t\th->free_huge_pages - h->resv_huge_pages == 0)\n\t\tgoto err;\n\n\t/* If reserves cannot be used, ensure enough pages are in the pool */\n\tif (avoid_reserve && h->free_huge_pages - h->resv_huge_pages == 0)\n\t\tgoto err;\n\n\tfor_each_zone_zonelist_nodemask(zone, z, zonelist,\n\t\t\t\t\t\tMAX_NR_ZONES - 1, nodemask) {\n\t\tif (cpuset_zone_allowed_softwall(zone, htlb_alloc_mask)) {\n\t\t\tpage = dequeue_huge_page_node(h, zone_to_nid(zone));\n\t\t\tif (page) {\n\t\t\t\tif (!avoid_reserve)\n\t\t\t\t\tdecrement_hugepage_resv_vma(h, vma);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tmpol_cond_put(mpol);\n\tif (unlikely(!put_mems_allowed(cpuset_mems_cookie) && !page))\n\t\tgoto retry_cpuset;\n\treturn page;\n\nerr:\n\tmpol_cond_put(mpol);\n\treturn NULL;\n}\n\nstatic void update_and_free_page(struct hstate *h, struct page *page)\n{\n\tint i;\n\n\tVM_BUG_ON(h->order >= MAX_ORDER);\n\n\th->nr_huge_pages--;\n\th->nr_huge_pages_node[page_to_nid(page)]--;\n\tfor (i = 0; i < pages_per_huge_page(h); i++) {\n\t\tpage[i].flags &= ~(1 << PG_locked | 1 << PG_error |\n\t\t\t\t1 << PG_referenced | 1 << PG_dirty |\n\t\t\t\t1 << PG_active | 1 << PG_reserved |\n\t\t\t\t1 << PG_private | 1 << PG_writeback);\n\t}\n\tset_compound_page_dtor(page, NULL);\n\tset_page_refcounted(page);\n\tarch_release_hugepage(page);\n\t__free_pages(page, huge_page_order(h));\n}\n\nstruct hstate *size_to_hstate(unsigned long size)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tif (huge_page_size(h) == size)\n\t\t\treturn h;\n\t}\n\treturn NULL;\n}\n\nstatic void free_huge_page(struct page *page)\n{\n\t/*\n\t * Can't pass hstate in here because it is called from the\n\t * compound page destructor.\n\t */\n\tstruct hstate *h = page_hstate(page);\n\tint nid = page_to_nid(page);\n\tstruct hugepage_subpool *spool =\n\t\t(struct hugepage_subpool *)page_private(page);\n\n\tset_page_private(page, 0);\n\tpage->mapping = NULL;\n\tBUG_ON(page_count(page));\n\tBUG_ON(page_mapcount(page));\n\tINIT_LIST_HEAD(&page->lru);\n\n\tspin_lock(&hugetlb_lock);\n\tif (h->surplus_huge_pages_node[nid] && huge_page_order(h) < MAX_ORDER) {\n\t\tupdate_and_free_page(h, page);\n\t\th->surplus_huge_pages--;\n\t\th->surplus_huge_pages_node[nid]--;\n\t} else {\n\t\tenqueue_huge_page(h, page);\n\t}\n\tspin_unlock(&hugetlb_lock);\n\thugepage_subpool_put_pages(spool, 1);\n}\n\nstatic void prep_new_huge_page(struct hstate *h, struct page *page, int nid)\n{\n\tset_compound_page_dtor(page, free_huge_page);\n\tspin_lock(&hugetlb_lock);\n\th->nr_huge_pages++;\n\th->nr_huge_pages_node[nid]++;\n\tspin_unlock(&hugetlb_lock);\n\tput_page(page); /* free it into the hugepage allocator */\n}\n\nstatic void prep_compound_gigantic_page(struct page *page, unsigned long order)\n{\n\tint i;\n\tint nr_pages = 1 << order;\n\tstruct page *p = page + 1;\n\n\t/* we rely on prep_new_huge_page to set the destructor */\n\tset_compound_order(page, order);\n\t__SetPageHead(page);\n\tfor (i = 1; i < nr_pages; i++, p = mem_map_next(p, page, i)) {\n\t\t__SetPageTail(p);\n\t\tset_page_count(p, 0);\n\t\tp->first_page = page;\n\t}\n}\n\nint PageHuge(struct page *page)\n{\n\tcompound_page_dtor *dtor;\n\n\tif (!PageCompound(page))\n\t\treturn 0;\n\n\tpage = compound_head(page);\n\tdtor = get_compound_page_dtor(page);\n\n\treturn dtor == free_huge_page;\n}\nEXPORT_SYMBOL_GPL(PageHuge);\n\nstatic struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tif (h->order >= MAX_ORDER)\n\t\treturn NULL;\n\n\tpage = alloc_pages_exact_node(nid,\n\t\thtlb_alloc_mask|__GFP_COMP|__GFP_THISNODE|\n\t\t\t\t\t\t__GFP_REPEAT|__GFP_NOWARN,\n\t\thuge_page_order(h));\n\tif (page) {\n\t\tif (arch_prepare_hugepage(page)) {\n\t\t\t__free_pages(page, huge_page_order(h));\n\t\t\treturn NULL;\n\t\t}\n\t\tprep_new_huge_page(h, page, nid);\n\t}\n\n\treturn page;\n}\n\n/*\n * common helper functions for hstate_next_node_to_{alloc|free}.\n * We may have allocated or freed a huge page based on a different\n * nodes_allowed previously, so h->next_node_to_{alloc|free} might\n * be outside of *nodes_allowed.  Ensure that we use an allowed\n * node for alloc or free.\n */\nstatic int next_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tnid = next_node(nid, *nodes_allowed);\n\tif (nid == MAX_NUMNODES)\n\t\tnid = first_node(*nodes_allowed);\n\tVM_BUG_ON(nid >= MAX_NUMNODES);\n\n\treturn nid;\n}\n\nstatic int get_valid_node_allowed(int nid, nodemask_t *nodes_allowed)\n{\n\tif (!node_isset(nid, *nodes_allowed))\n\t\tnid = next_node_allowed(nid, nodes_allowed);\n\treturn nid;\n}\n\n/*\n * returns the previously saved node [\"this node\"] from which to\n * allocate a persistent huge page for the pool and advance the\n * next node from which to allocate, handling wrap at end of node\n * mask.\n */\nstatic int hstate_next_node_to_alloc(struct hstate *h,\n\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_alloc, nodes_allowed);\n\th->next_nid_to_alloc = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\nstatic int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)\n{\n\tstruct page *page;\n\tint start_nid;\n\tint next_nid;\n\tint ret = 0;\n\n\tstart_nid = hstate_next_node_to_alloc(h, nodes_allowed);\n\tnext_nid = start_nid;\n\n\tdo {\n\t\tpage = alloc_fresh_huge_page_node(h, next_nid);\n\t\tif (page) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t\tnext_nid = hstate_next_node_to_alloc(h, nodes_allowed);\n\t} while (next_nid != start_nid);\n\n\tif (ret)\n\t\tcount_vm_event(HTLB_BUDDY_PGALLOC);\n\telse\n\t\tcount_vm_event(HTLB_BUDDY_PGALLOC_FAIL);\n\n\treturn ret;\n}\n\n/*\n * helper for free_pool_huge_page() - return the previously saved\n * node [\"this node\"] from which to free a huge page.  Advance the\n * next node id whether or not we find a free huge page to free so\n * that the next attempt to free addresses the next node.\n */\nstatic int hstate_next_node_to_free(struct hstate *h, nodemask_t *nodes_allowed)\n{\n\tint nid;\n\n\tVM_BUG_ON(!nodes_allowed);\n\n\tnid = get_valid_node_allowed(h->next_nid_to_free, nodes_allowed);\n\th->next_nid_to_free = next_node_allowed(nid, nodes_allowed);\n\n\treturn nid;\n}\n\n/*\n * Free huge page from pool from next node to free.\n * Attempt to keep persistent huge pages more or less\n * balanced over allowed nodes.\n * Called with hugetlb_lock locked.\n */\nstatic int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\t\t\t\t bool acct_surplus)\n{\n\tint start_nid;\n\tint next_nid;\n\tint ret = 0;\n\n\tstart_nid = hstate_next_node_to_free(h, nodes_allowed);\n\tnext_nid = start_nid;\n\n\tdo {\n\t\t/*\n\t\t * If we're returning unused surplus pages, only examine\n\t\t * nodes with surplus pages.\n\t\t */\n\t\tif ((!acct_surplus || h->surplus_huge_pages_node[next_nid]) &&\n\t\t    !list_empty(&h->hugepage_freelists[next_nid])) {\n\t\t\tstruct page *page =\n\t\t\t\tlist_entry(h->hugepage_freelists[next_nid].next,\n\t\t\t\t\t  struct page, lru);\n\t\t\tlist_del(&page->lru);\n\t\t\th->free_huge_pages--;\n\t\t\th->free_huge_pages_node[next_nid]--;\n\t\t\tif (acct_surplus) {\n\t\t\t\th->surplus_huge_pages--;\n\t\t\t\th->surplus_huge_pages_node[next_nid]--;\n\t\t\t}\n\t\t\tupdate_and_free_page(h, page);\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t\tnext_nid = hstate_next_node_to_free(h, nodes_allowed);\n\t} while (next_nid != start_nid);\n\n\treturn ret;\n}\n\nstatic struct page *alloc_buddy_huge_page(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\tunsigned int r_nid;\n\n\tif (h->order >= MAX_ORDER)\n\t\treturn NULL;\n\n\t/*\n\t * Assume we will successfully allocate the surplus page to\n\t * prevent racing processes from causing the surplus to exceed\n\t * overcommit\n\t *\n\t * This however introduces a different race, where a process B\n\t * tries to grow the static hugepage pool while alloc_pages() is\n\t * called by process A. B will only examine the per-node\n\t * counters in determining if surplus huge pages can be\n\t * converted to normal huge pages in adjust_pool_surplus(). A\n\t * won't be able to increment the per-node counter, until the\n\t * lock is dropped by B, but B doesn't drop hugetlb_lock until\n\t * no more huge pages can be converted from surplus to normal\n\t * state (and doesn't try to convert again). Thus, we have a\n\t * case where a surplus huge page exists, the pool is grown, and\n\t * the surplus huge page still exists after, even though it\n\t * should just have been converted to a normal huge page. This\n\t * does not leak memory, though, as the hugepage will be freed\n\t * once it is out of use. It also does not allow the counters to\n\t * go out of whack in adjust_pool_surplus() as we don't modify\n\t * the node values until we've gotten the hugepage and only the\n\t * per-node value is checked there.\n\t */\n\tspin_lock(&hugetlb_lock);\n\tif (h->surplus_huge_pages >= h->nr_overcommit_huge_pages) {\n\t\tspin_unlock(&hugetlb_lock);\n\t\treturn NULL;\n\t} else {\n\t\th->nr_huge_pages++;\n\t\th->surplus_huge_pages++;\n\t}\n\tspin_unlock(&hugetlb_lock);\n\n\tif (nid == NUMA_NO_NODE)\n\t\tpage = alloc_pages(htlb_alloc_mask|__GFP_COMP|\n\t\t\t\t   __GFP_REPEAT|__GFP_NOWARN,\n\t\t\t\t   huge_page_order(h));\n\telse\n\t\tpage = alloc_pages_exact_node(nid,\n\t\t\thtlb_alloc_mask|__GFP_COMP|__GFP_THISNODE|\n\t\t\t__GFP_REPEAT|__GFP_NOWARN, huge_page_order(h));\n\n\tif (page && arch_prepare_hugepage(page)) {\n\t\t__free_pages(page, huge_page_order(h));\n\t\tpage = NULL;\n\t}\n\n\tspin_lock(&hugetlb_lock);\n\tif (page) {\n\t\tr_nid = page_to_nid(page);\n\t\tset_compound_page_dtor(page, free_huge_page);\n\t\t/*\n\t\t * We incremented the global counters already\n\t\t */\n\t\th->nr_huge_pages_node[r_nid]++;\n\t\th->surplus_huge_pages_node[r_nid]++;\n\t\t__count_vm_event(HTLB_BUDDY_PGALLOC);\n\t} else {\n\t\th->nr_huge_pages--;\n\t\th->surplus_huge_pages--;\n\t\t__count_vm_event(HTLB_BUDDY_PGALLOC_FAIL);\n\t}\n\tspin_unlock(&hugetlb_lock);\n\n\treturn page;\n}\n\n/*\n * This allocation function is useful in the context where vma is irrelevant.\n * E.g. soft-offlining uses this function because it only cares physical\n * address of error page.\n */\nstruct page *alloc_huge_page_node(struct hstate *h, int nid)\n{\n\tstruct page *page;\n\n\tspin_lock(&hugetlb_lock);\n\tpage = dequeue_huge_page_node(h, nid);\n\tspin_unlock(&hugetlb_lock);\n\n\tif (!page)\n\t\tpage = alloc_buddy_huge_page(h, nid);\n\n\treturn page;\n}\n\n/*\n * Increase the hugetlb pool such that it can accommodate a reservation\n * of size 'delta'.\n */\nstatic int gather_surplus_pages(struct hstate *h, int delta)\n{\n\tstruct list_head surplus_list;\n\tstruct page *page, *tmp;\n\tint ret, i;\n\tint needed, allocated;\n\tbool alloc_ok = true;\n\n\tneeded = (h->resv_huge_pages + delta) - h->free_huge_pages;\n\tif (needed <= 0) {\n\t\th->resv_huge_pages += delta;\n\t\treturn 0;\n\t}\n\n\tallocated = 0;\n\tINIT_LIST_HEAD(&surplus_list);\n\n\tret = -ENOMEM;\nretry:\n\tspin_unlock(&hugetlb_lock);\n\tfor (i = 0; i < needed; i++) {\n\t\tpage = alloc_buddy_huge_page(h, NUMA_NO_NODE);\n\t\tif (!page) {\n\t\t\talloc_ok = false;\n\t\t\tbreak;\n\t\t}\n\t\tlist_add(&page->lru, &surplus_list);\n\t}\n\tallocated += i;\n\n\t/*\n\t * After retaking hugetlb_lock, we need to recalculate 'needed'\n\t * because either resv_huge_pages or free_huge_pages may have changed.\n\t */\n\tspin_lock(&hugetlb_lock);\n\tneeded = (h->resv_huge_pages + delta) -\n\t\t\t(h->free_huge_pages + allocated);\n\tif (needed > 0) {\n\t\tif (alloc_ok)\n\t\t\tgoto retry;\n\t\t/*\n\t\t * We were not able to allocate enough pages to\n\t\t * satisfy the entire reservation so we free what\n\t\t * we've allocated so far.\n\t\t */\n\t\tgoto free;\n\t}\n\t/*\n\t * The surplus_list now contains _at_least_ the number of extra pages\n\t * needed to accommodate the reservation.  Add the appropriate number\n\t * of pages to the hugetlb pool and free the extras back to the buddy\n\t * allocator.  Commit the entire reservation here to prevent another\n\t * process from stealing the pages as they are added to the pool but\n\t * before they are reserved.\n\t */\n\tneeded += allocated;\n\th->resv_huge_pages += delta;\n\tret = 0;\n\n\t/* Free the needed pages to the hugetlb pool */\n\tlist_for_each_entry_safe(page, tmp, &surplus_list, lru) {\n\t\tif ((--needed) < 0)\n\t\t\tbreak;\n\t\tlist_del(&page->lru);\n\t\t/*\n\t\t * This page is now managed by the hugetlb allocator and has\n\t\t * no users -- drop the buddy allocator's reference.\n\t\t */\n\t\tput_page_testzero(page);\n\t\tVM_BUG_ON(page_count(page));\n\t\tenqueue_huge_page(h, page);\n\t}\nfree:\n\tspin_unlock(&hugetlb_lock);\n\n\t/* Free unnecessary surplus pages to the buddy allocator */\n\tif (!list_empty(&surplus_list)) {\n\t\tlist_for_each_entry_safe(page, tmp, &surplus_list, lru) {\n\t\t\tlist_del(&page->lru);\n\t\t\tput_page(page);\n\t\t}\n\t}\n\tspin_lock(&hugetlb_lock);\n\n\treturn ret;\n}\n\n/*\n * When releasing a hugetlb pool reservation, any surplus pages that were\n * allocated to satisfy the reservation must be explicitly freed if they were\n * never used.\n * Called with hugetlb_lock held.\n */\nstatic void return_unused_surplus_pages(struct hstate *h,\n\t\t\t\t\tunsigned long unused_resv_pages)\n{\n\tunsigned long nr_pages;\n\n\t/* Uncommit the reservation */\n\th->resv_huge_pages -= unused_resv_pages;\n\n\t/* Cannot return gigantic pages currently */\n\tif (h->order >= MAX_ORDER)\n\t\treturn;\n\n\tnr_pages = min(unused_resv_pages, h->surplus_huge_pages);\n\n\t/*\n\t * We want to release as many surplus pages as possible, spread\n\t * evenly across all nodes with memory. Iterate across these nodes\n\t * until we can no longer free unreserved surplus pages. This occurs\n\t * when the nodes with surplus pages have no free pages.\n\t * free_pool_huge_page() will balance the the freed pages across the\n\t * on-line nodes with memory and will handle the hstate accounting.\n\t */\n\twhile (nr_pages--) {\n\t\tif (!free_pool_huge_page(h, &node_states[N_HIGH_MEMORY], 1))\n\t\t\tbreak;\n\t}\n}\n\n/*\n * Determine if the huge page at addr within the vma has an associated\n * reservation.  Where it does not we will need to logically increase\n * reservation and actually increase subpool usage before an allocation\n * can occur.  Where any new reservation would be required the\n * reservation change is prepared, but not committed.  Once the page\n * has been allocated from the subpool and instantiated the change should\n * be committed via vma_commit_reservation.  No action is required on\n * failure.\n */\nstatic long vma_needs_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\tpgoff_t idx = vma_hugecache_offset(h, vma, addr);\n\t\treturn region_chg(&inode->i_mapping->private_list,\n\t\t\t\t\t\t\tidx, idx + 1);\n\n\t} else if (!is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\treturn 1;\n\n\t} else  {\n\t\tlong err;\n\t\tpgoff_t idx = vma_hugecache_offset(h, vma, addr);\n\t\tstruct resv_map *reservations = vma_resv_map(vma);\n\n\t\terr = region_chg(&reservations->regions, idx, idx + 1);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\treturn 0;\n\t}\n}\nstatic void vma_commit_reservation(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct address_space *mapping = vma->vm_file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\n\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\tpgoff_t idx = vma_hugecache_offset(h, vma, addr);\n\t\tregion_add(&inode->i_mapping->private_list, idx, idx + 1);\n\n\t} else if (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {\n\t\tpgoff_t idx = vma_hugecache_offset(h, vma, addr);\n\t\tstruct resv_map *reservations = vma_resv_map(vma);\n\n\t\t/* Mark this page used in the map. */\n\t\tregion_add(&reservations->regions, idx, idx + 1);\n\t}\n}\n\nstatic struct page *alloc_huge_page(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long addr, int avoid_reserve)\n{\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct page *page;\n\tlong chg;\n\n\t/*\n\t * Processes that did not create the mapping will have no\n\t * reserves and will not have accounted against subpool\n\t * limit. Check that the subpool limit can be made before\n\t * satisfying the allocation MAP_NORESERVE mappings may also\n\t * need pages and subpool limit allocated allocated if no reserve\n\t * mapping overlaps.\n\t */\n\tchg = vma_needs_reservation(h, vma, addr);\n\tif (chg < 0)\n\t\treturn ERR_PTR(-VM_FAULT_OOM);\n\tif (chg)\n\t\tif (hugepage_subpool_get_pages(spool, chg))\n\t\t\treturn ERR_PTR(-VM_FAULT_SIGBUS);\n\n\tspin_lock(&hugetlb_lock);\n\tpage = dequeue_huge_page_vma(h, vma, addr, avoid_reserve);\n\tspin_unlock(&hugetlb_lock);\n\n\tif (!page) {\n\t\tpage = alloc_buddy_huge_page(h, NUMA_NO_NODE);\n\t\tif (!page) {\n\t\t\thugepage_subpool_put_pages(spool, chg);\n\t\t\treturn ERR_PTR(-VM_FAULT_SIGBUS);\n\t\t}\n\t}\n\n\tset_page_private(page, (unsigned long)spool);\n\n\tvma_commit_reservation(h, vma, addr);\n\n\treturn page;\n}\n\nint __weak alloc_bootmem_huge_page(struct hstate *h)\n{\n\tstruct huge_bootmem_page *m;\n\tint nr_nodes = nodes_weight(node_states[N_HIGH_MEMORY]);\n\n\twhile (nr_nodes) {\n\t\tvoid *addr;\n\n\t\taddr = __alloc_bootmem_node_nopanic(\n\t\t\t\tNODE_DATA(hstate_next_node_to_alloc(h,\n\t\t\t\t\t\t&node_states[N_HIGH_MEMORY])),\n\t\t\t\thuge_page_size(h), huge_page_size(h), 0);\n\n\t\tif (addr) {\n\t\t\t/*\n\t\t\t * Use the beginning of the huge page to store the\n\t\t\t * huge_bootmem_page struct (until gather_bootmem\n\t\t\t * puts them into the mem_map).\n\t\t\t */\n\t\t\tm = addr;\n\t\t\tgoto found;\n\t\t}\n\t\tnr_nodes--;\n\t}\n\treturn 0;\n\nfound:\n\tBUG_ON((unsigned long)virt_to_phys(m) & (huge_page_size(h) - 1));\n\t/* Put them into a private list first because mem_map is not up yet */\n\tlist_add(&m->list, &huge_boot_pages);\n\tm->hstate = h;\n\treturn 1;\n}\n\nstatic void prep_compound_huge_page(struct page *page, int order)\n{\n\tif (unlikely(order > (MAX_ORDER - 1)))\n\t\tprep_compound_gigantic_page(page, order);\n\telse\n\t\tprep_compound_page(page, order);\n}\n\n/* Put bootmem huge pages into the standard lists after mem_map is up */\nstatic void __init gather_bootmem_prealloc(void)\n{\n\tstruct huge_bootmem_page *m;\n\n\tlist_for_each_entry(m, &huge_boot_pages, list) {\n\t\tstruct hstate *h = m->hstate;\n\t\tstruct page *page;\n\n#ifdef CONFIG_HIGHMEM\n\t\tpage = pfn_to_page(m->phys >> PAGE_SHIFT);\n\t\tfree_bootmem_late((unsigned long)m,\n\t\t\t\t  sizeof(struct huge_bootmem_page));\n#else\n\t\tpage = virt_to_page(m);\n#endif\n\t\t__ClearPageReserved(page);\n\t\tWARN_ON(page_count(page) != 1);\n\t\tprep_compound_huge_page(page, h->order);\n\t\tprep_new_huge_page(h, page, page_to_nid(page));\n\t\t/*\n\t\t * If we had gigantic hugepages allocated at boot time, we need\n\t\t * to restore the 'stolen' pages to totalram_pages in order to\n\t\t * fix confusing memory reports from free(1) and another\n\t\t * side-effects, like CommitLimit going negative.\n\t\t */\n\t\tif (h->order > (MAX_ORDER - 1))\n\t\t\ttotalram_pages += 1 << h->order;\n\t}\n}\n\nstatic void __init hugetlb_hstate_alloc_pages(struct hstate *h)\n{\n\tunsigned long i;\n\n\tfor (i = 0; i < h->max_huge_pages; ++i) {\n\t\tif (h->order >= MAX_ORDER) {\n\t\t\tif (!alloc_bootmem_huge_page(h))\n\t\t\t\tbreak;\n\t\t} else if (!alloc_fresh_huge_page(h,\n\t\t\t\t\t &node_states[N_HIGH_MEMORY]))\n\t\t\tbreak;\n\t}\n\th->max_huge_pages = i;\n}\n\nstatic void __init hugetlb_init_hstates(void)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\t/* oversize hugepages were init'ed in early boot */\n\t\tif (h->order < MAX_ORDER)\n\t\t\thugetlb_hstate_alloc_pages(h);\n\t}\n}\n\nstatic char * __init memfmt(char *buf, unsigned long n)\n{\n\tif (n >= (1UL << 30))\n\t\tsprintf(buf, \"%lu GB\", n >> 30);\n\telse if (n >= (1UL << 20))\n\t\tsprintf(buf, \"%lu MB\", n >> 20);\n\telse\n\t\tsprintf(buf, \"%lu KB\", n >> 10);\n\treturn buf;\n}\n\nstatic void __init report_hugepages(void)\n{\n\tstruct hstate *h;\n\n\tfor_each_hstate(h) {\n\t\tchar buf[32];\n\t\tprintk(KERN_INFO \"HugeTLB registered %s page size, \"\n\t\t\t\t \"pre-allocated %ld pages\\n\",\n\t\t\tmemfmt(buf, huge_page_size(h)),\n\t\t\th->free_huge_pages);\n\t}\n}\n\n#ifdef CONFIG_HIGHMEM\nstatic void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tint i;\n\n\tif (h->order >= MAX_ORDER)\n\t\treturn;\n\n\tfor_each_node_mask(i, *nodes_allowed) {\n\t\tstruct page *page, *next;\n\t\tstruct list_head *freel = &h->hugepage_freelists[i];\n\t\tlist_for_each_entry_safe(page, next, freel, lru) {\n\t\t\tif (count >= h->nr_huge_pages)\n\t\t\t\treturn;\n\t\t\tif (PageHighMem(page))\n\t\t\t\tcontinue;\n\t\t\tlist_del(&page->lru);\n\t\t\tupdate_and_free_page(h, page);\n\t\t\th->free_huge_pages--;\n\t\t\th->free_huge_pages_node[page_to_nid(page)]--;\n\t\t}\n\t}\n}\n#else\nstatic inline void try_to_free_low(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n}\n#endif\n\n/*\n * Increment or decrement surplus_huge_pages.  Keep node-specific counters\n * balanced by operating on them in a round-robin fashion.\n * Returns 1 if an adjustment was made.\n */\nstatic int adjust_pool_surplus(struct hstate *h, nodemask_t *nodes_allowed,\n\t\t\t\tint delta)\n{\n\tint start_nid, next_nid;\n\tint ret = 0;\n\n\tVM_BUG_ON(delta != -1 && delta != 1);\n\n\tif (delta < 0)\n\t\tstart_nid = hstate_next_node_to_alloc(h, nodes_allowed);\n\telse\n\t\tstart_nid = hstate_next_node_to_free(h, nodes_allowed);\n\tnext_nid = start_nid;\n\n\tdo {\n\t\tint nid = next_nid;\n\t\tif (delta < 0)  {\n\t\t\t/*\n\t\t\t * To shrink on this node, there must be a surplus page\n\t\t\t */\n\t\t\tif (!h->surplus_huge_pages_node[nid]) {\n\t\t\t\tnext_nid = hstate_next_node_to_alloc(h,\n\t\t\t\t\t\t\t\tnodes_allowed);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (delta > 0) {\n\t\t\t/*\n\t\t\t * Surplus cannot exceed the total number of pages\n\t\t\t */\n\t\t\tif (h->surplus_huge_pages_node[nid] >=\n\t\t\t\t\t\th->nr_huge_pages_node[nid]) {\n\t\t\t\tnext_nid = hstate_next_node_to_free(h,\n\t\t\t\t\t\t\t\tnodes_allowed);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\th->surplus_huge_pages += delta;\n\t\th->surplus_huge_pages_node[nid] += delta;\n\t\tret = 1;\n\t\tbreak;\n\t} while (next_nid != start_nid);\n\n\treturn ret;\n}\n\n#define persistent_huge_pages(h) (h->nr_huge_pages - h->surplus_huge_pages)\nstatic unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,\n\t\t\t\t\t\tnodemask_t *nodes_allowed)\n{\n\tunsigned long min_count, ret;\n\n\tif (h->order >= MAX_ORDER)\n\t\treturn h->max_huge_pages;\n\n\t/*\n\t * Increase the pool size\n\t * First take pages out of surplus state.  Then make up the\n\t * remaining difference by allocating fresh huge pages.\n\t *\n\t * We might race with alloc_buddy_huge_page() here and be unable\n\t * to convert a surplus huge page to a normal huge page. That is\n\t * not critical, though, it just means the overall size of the\n\t * pool might be one hugepage larger than it needs to be, but\n\t * within all the constraints specified by the sysctls.\n\t */\n\tspin_lock(&hugetlb_lock);\n\twhile (h->surplus_huge_pages && count > persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, -1))\n\t\t\tbreak;\n\t}\n\n\twhile (count > persistent_huge_pages(h)) {\n\t\t/*\n\t\t * If this allocation races such that we no longer need the\n\t\t * page, free_huge_page will handle it by freeing the page\n\t\t * and reducing the surplus.\n\t\t */\n\t\tspin_unlock(&hugetlb_lock);\n\t\tret = alloc_fresh_huge_page(h, nodes_allowed);\n\t\tspin_lock(&hugetlb_lock);\n\t\tif (!ret)\n\t\t\tgoto out;\n\n\t\t/* Bail for signals. Probably ctrl-c from user */\n\t\tif (signal_pending(current))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * Decrease the pool size\n\t * First return free pages to the buddy allocator (being careful\n\t * to keep enough around to satisfy reservations).  Then place\n\t * pages into surplus state as needed so the pool will shrink\n\t * to the desired size as pages become free.\n\t *\n\t * By placing pages into the surplus state independent of the\n\t * overcommit value, we are allowing the surplus pool size to\n\t * exceed overcommit. There are few sane options here. Since\n\t * alloc_buddy_huge_page() is checking the global counter,\n\t * though, we'll note that we're not allowed to exceed surplus\n\t * and won't grow the pool anywhere else. Not until one of the\n\t * sysctls are changed, or the surplus pages go out of use.\n\t */\n\tmin_count = h->resv_huge_pages + h->nr_huge_pages - h->free_huge_pages;\n\tmin_count = max(count, min_count);\n\ttry_to_free_low(h, min_count, nodes_allowed);\n\twhile (min_count < persistent_huge_pages(h)) {\n\t\tif (!free_pool_huge_page(h, nodes_allowed, 0))\n\t\t\tbreak;\n\t}\n\twhile (count < persistent_huge_pages(h)) {\n\t\tif (!adjust_pool_surplus(h, nodes_allowed, 1))\n\t\t\tbreak;\n\t}\nout:\n\tret = persistent_huge_pages(h);\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\n#define HSTATE_ATTR_RO(_name) \\\n\tstatic struct kobj_attribute _name##_attr = __ATTR_RO(_name)\n\n#define HSTATE_ATTR(_name) \\\n\tstatic struct kobj_attribute _name##_attr = \\\n\t\t__ATTR(_name, 0644, _name##_show, _name##_store)\n\nstatic struct kobject *hugepages_kobj;\nstatic struct kobject *hstate_kobjs[HUGE_MAX_HSTATE];\n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp);\n\nstatic struct hstate *kobj_to_hstate(struct kobject *kobj, int *nidp)\n{\n\tint i;\n\n\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\tif (hstate_kobjs[i] == kobj) {\n\t\t\tif (nidp)\n\t\t\t\t*nidp = NUMA_NO_NODE;\n\t\t\treturn &hstates[i];\n\t\t}\n\n\treturn kobj_to_node_hstate(kobj, nidp);\n}\n\nstatic ssize_t nr_hugepages_show_common(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long nr_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tnr_huge_pages = h->nr_huge_pages;\n\telse\n\t\tnr_huge_pages = h->nr_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", nr_huge_pages);\n}\n\nstatic ssize_t nr_hugepages_store_common(bool obey_mempolicy,\n\t\t\tstruct kobject *kobj, struct kobj_attribute *attr,\n\t\t\tconst char *buf, size_t len)\n{\n\tint err;\n\tint nid;\n\tunsigned long count;\n\tstruct hstate *h;\n\tNODEMASK_ALLOC(nodemask_t, nodes_allowed, GFP_KERNEL | __GFP_NORETRY);\n\n\terr = strict_strtoul(buf, 10, &count);\n\tif (err)\n\t\tgoto out;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (h->order >= MAX_ORDER) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (nid == NUMA_NO_NODE) {\n\t\t/*\n\t\t * global hstate attribute\n\t\t */\n\t\tif (!(obey_mempolicy &&\n\t\t\t\tinit_nodemask_of_mempolicy(nodes_allowed))) {\n\t\t\tNODEMASK_FREE(nodes_allowed);\n\t\t\tnodes_allowed = &node_states[N_HIGH_MEMORY];\n\t\t}\n\t} else if (nodes_allowed) {\n\t\t/*\n\t\t * per node hstate attribute: adjust count to global,\n\t\t * but restrict alloc/free to the specified node.\n\t\t */\n\t\tcount += h->nr_huge_pages - h->nr_huge_pages_node[nid];\n\t\tinit_nodemask_of_node(nodes_allowed, nid);\n\t} else\n\t\tnodes_allowed = &node_states[N_HIGH_MEMORY];\n\n\th->max_huge_pages = set_max_huge_pages(h, count, nodes_allowed);\n\n\tif (nodes_allowed != &node_states[N_HIGH_MEMORY])\n\t\tNODEMASK_FREE(nodes_allowed);\n\n\treturn len;\nout:\n\tNODEMASK_FREE(nodes_allowed);\n\treturn err;\n}\n\nstatic ssize_t nr_hugepages_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(false, kobj, attr, buf, len);\n}\nHSTATE_ATTR(nr_hugepages);\n\n#ifdef CONFIG_NUMA\n\n/*\n * hstate attribute for optionally mempolicy-based constraint on persistent\n * huge page alloc/free.\n */\nstatic ssize_t nr_hugepages_mempolicy_show(struct kobject *kobj,\n\t\t\t\t       struct kobj_attribute *attr, char *buf)\n{\n\treturn nr_hugepages_show_common(kobj, attr, buf);\n}\n\nstatic ssize_t nr_hugepages_mempolicy_store(struct kobject *kobj,\n\t       struct kobj_attribute *attr, const char *buf, size_t len)\n{\n\treturn nr_hugepages_store_common(true, kobj, attr, buf, len);\n}\nHSTATE_ATTR(nr_hugepages_mempolicy);\n#endif\n\n\nstatic ssize_t nr_overcommit_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sprintf(buf, \"%lu\\n\", h->nr_overcommit_huge_pages);\n}\n\nstatic ssize_t nr_overcommit_hugepages_store(struct kobject *kobj,\n\t\tstruct kobj_attribute *attr, const char *buf, size_t count)\n{\n\tint err;\n\tunsigned long input;\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\n\tif (h->order >= MAX_ORDER)\n\t\treturn -EINVAL;\n\n\terr = strict_strtoul(buf, 10, &input);\n\tif (err)\n\t\treturn err;\n\n\tspin_lock(&hugetlb_lock);\n\th->nr_overcommit_huge_pages = input;\n\tspin_unlock(&hugetlb_lock);\n\n\treturn count;\n}\nHSTATE_ATTR(nr_overcommit_hugepages);\n\nstatic ssize_t free_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long free_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tfree_huge_pages = h->free_huge_pages;\n\telse\n\t\tfree_huge_pages = h->free_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", free_huge_pages);\n}\nHSTATE_ATTR_RO(free_hugepages);\n\nstatic ssize_t resv_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h = kobj_to_hstate(kobj, NULL);\n\treturn sprintf(buf, \"%lu\\n\", h->resv_huge_pages);\n}\nHSTATE_ATTR_RO(resv_hugepages);\n\nstatic ssize_t surplus_hugepages_show(struct kobject *kobj,\n\t\t\t\t\tstruct kobj_attribute *attr, char *buf)\n{\n\tstruct hstate *h;\n\tunsigned long surplus_huge_pages;\n\tint nid;\n\n\th = kobj_to_hstate(kobj, &nid);\n\tif (nid == NUMA_NO_NODE)\n\t\tsurplus_huge_pages = h->surplus_huge_pages;\n\telse\n\t\tsurplus_huge_pages = h->surplus_huge_pages_node[nid];\n\n\treturn sprintf(buf, \"%lu\\n\", surplus_huge_pages);\n}\nHSTATE_ATTR_RO(surplus_hugepages);\n\nstatic struct attribute *hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&nr_overcommit_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&resv_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n#ifdef CONFIG_NUMA\n\t&nr_hugepages_mempolicy_attr.attr,\n#endif\n\tNULL,\n};\n\nstatic struct attribute_group hstate_attr_group = {\n\t.attrs = hstate_attrs,\n};\n\nstatic int hugetlb_sysfs_add_hstate(struct hstate *h, struct kobject *parent,\n\t\t\t\t    struct kobject **hstate_kobjs,\n\t\t\t\t    struct attribute_group *hstate_attr_group)\n{\n\tint retval;\n\tint hi = h - hstates;\n\n\thstate_kobjs[hi] = kobject_create_and_add(h->name, parent);\n\tif (!hstate_kobjs[hi])\n\t\treturn -ENOMEM;\n\n\tretval = sysfs_create_group(hstate_kobjs[hi], hstate_attr_group);\n\tif (retval)\n\t\tkobject_put(hstate_kobjs[hi]);\n\n\treturn retval;\n}\n\nstatic void __init hugetlb_sysfs_init(void)\n{\n\tstruct hstate *h;\n\tint err;\n\n\thugepages_kobj = kobject_create_and_add(\"hugepages\", mm_kobj);\n\tif (!hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, hugepages_kobj,\n\t\t\t\t\t hstate_kobjs, &hstate_attr_group);\n\t\tif (err)\n\t\t\tprintk(KERN_ERR \"Hugetlb: Unable to add hstate %s\",\n\t\t\t\t\t\t\t\th->name);\n\t}\n}\n\n#ifdef CONFIG_NUMA\n\n/*\n * node_hstate/s - associate per node hstate attributes, via their kobjects,\n * with node devices in node_devices[] using a parallel array.  The array\n * index of a node device or _hstate == node id.\n * This is here to avoid any static dependency of the node device driver, in\n * the base kernel, on the hugetlb module.\n */\nstruct node_hstate {\n\tstruct kobject\t\t*hugepages_kobj;\n\tstruct kobject\t\t*hstate_kobjs[HUGE_MAX_HSTATE];\n};\nstruct node_hstate node_hstates[MAX_NUMNODES];\n\n/*\n * A subset of global hstate attributes for node devices\n */\nstatic struct attribute *per_node_hstate_attrs[] = {\n\t&nr_hugepages_attr.attr,\n\t&free_hugepages_attr.attr,\n\t&surplus_hugepages_attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group per_node_hstate_attr_group = {\n\t.attrs = per_node_hstate_attrs,\n};\n\n/*\n * kobj_to_node_hstate - lookup global hstate for node device hstate attr kobj.\n * Returns node id via non-NULL nidp.\n */\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tint nid;\n\n\tfor (nid = 0; nid < nr_node_ids; nid++) {\n\t\tstruct node_hstate *nhs = &node_hstates[nid];\n\t\tint i;\n\t\tfor (i = 0; i < HUGE_MAX_HSTATE; i++)\n\t\t\tif (nhs->hstate_kobjs[i] == kobj) {\n\t\t\t\tif (nidp)\n\t\t\t\t\t*nidp = nid;\n\t\t\t\treturn &hstates[i];\n\t\t\t}\n\t}\n\n\tBUG();\n\treturn NULL;\n}\n\n/*\n * Unregister hstate attributes from a single node device.\n * No-op if no hstate attributes attached.\n */\nvoid hugetlb_unregister_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\t\t/* no hstate attributes */\n\n\tfor_each_hstate(h)\n\t\tif (nhs->hstate_kobjs[h - hstates]) {\n\t\t\tkobject_put(nhs->hstate_kobjs[h - hstates]);\n\t\t\tnhs->hstate_kobjs[h - hstates] = NULL;\n\t\t}\n\n\tkobject_put(nhs->hugepages_kobj);\n\tnhs->hugepages_kobj = NULL;\n}\n\n/*\n * hugetlb module exit:  unregister hstate attributes from node devices\n * that have them.\n */\nstatic void hugetlb_unregister_all_nodes(void)\n{\n\tint nid;\n\n\t/*\n\t * disable node device registrations.\n\t */\n\tregister_hugetlbfs_with_node(NULL, NULL);\n\n\t/*\n\t * remove hstate attributes from any nodes that have them.\n\t */\n\tfor (nid = 0; nid < nr_node_ids; nid++)\n\t\thugetlb_unregister_node(&node_devices[nid]);\n}\n\n/*\n * Register hstate attributes for a single node device.\n * No-op if attributes already registered.\n */\nvoid hugetlb_register_node(struct node *node)\n{\n\tstruct hstate *h;\n\tstruct node_hstate *nhs = &node_hstates[node->dev.id];\n\tint err;\n\n\tif (nhs->hugepages_kobj)\n\t\treturn;\t\t/* already allocated */\n\n\tnhs->hugepages_kobj = kobject_create_and_add(\"hugepages\",\n\t\t\t\t\t\t\t&node->dev.kobj);\n\tif (!nhs->hugepages_kobj)\n\t\treturn;\n\n\tfor_each_hstate(h) {\n\t\terr = hugetlb_sysfs_add_hstate(h, nhs->hugepages_kobj,\n\t\t\t\t\t\tnhs->hstate_kobjs,\n\t\t\t\t\t\t&per_node_hstate_attr_group);\n\t\tif (err) {\n\t\t\tprintk(KERN_ERR \"Hugetlb: Unable to add hstate %s\"\n\t\t\t\t\t\" for node %d\\n\",\n\t\t\t\t\t\th->name, node->dev.id);\n\t\t\thugetlb_unregister_node(node);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/*\n * hugetlb init time:  register hstate attributes for all registered node\n * devices of nodes that have memory.  All on-line nodes should have\n * registered their associated device by this time.\n */\nstatic void hugetlb_register_all_nodes(void)\n{\n\tint nid;\n\n\tfor_each_node_state(nid, N_HIGH_MEMORY) {\n\t\tstruct node *node = &node_devices[nid];\n\t\tif (node->dev.id == nid)\n\t\t\thugetlb_register_node(node);\n\t}\n\n\t/*\n\t * Let the node device driver know we're here so it can\n\t * [un]register hstate attributes on node hotplug.\n\t */\n\tregister_hugetlbfs_with_node(hugetlb_register_node,\n\t\t\t\t     hugetlb_unregister_node);\n}\n#else\t/* !CONFIG_NUMA */\n\nstatic struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)\n{\n\tBUG();\n\tif (nidp)\n\t\t*nidp = -1;\n\treturn NULL;\n}\n\nstatic void hugetlb_unregister_all_nodes(void) { }\n\nstatic void hugetlb_register_all_nodes(void) { }\n\n#endif\n\nstatic void __exit hugetlb_exit(void)\n{\n\tstruct hstate *h;\n\n\thugetlb_unregister_all_nodes();\n\n\tfor_each_hstate(h) {\n\t\tkobject_put(hstate_kobjs[h - hstates]);\n\t}\n\n\tkobject_put(hugepages_kobj);\n}\nmodule_exit(hugetlb_exit);\n\nstatic int __init hugetlb_init(void)\n{\n\t/* Some platform decide whether they support huge pages at boot\n\t * time. On these, such as powerpc, HPAGE_SHIFT is set to 0 when\n\t * there is no such support\n\t */\n\tif (HPAGE_SHIFT == 0)\n\t\treturn 0;\n\n\tif (!size_to_hstate(default_hstate_size)) {\n\t\tdefault_hstate_size = HPAGE_SIZE;\n\t\tif (!size_to_hstate(default_hstate_size))\n\t\t\thugetlb_add_hstate(HUGETLB_PAGE_ORDER);\n\t}\n\tdefault_hstate_idx = size_to_hstate(default_hstate_size) - hstates;\n\tif (default_hstate_max_huge_pages)\n\t\tdefault_hstate.max_huge_pages = default_hstate_max_huge_pages;\n\n\thugetlb_init_hstates();\n\n\tgather_bootmem_prealloc();\n\n\treport_hugepages();\n\n\thugetlb_sysfs_init();\n\n\thugetlb_register_all_nodes();\n\n\treturn 0;\n}\nmodule_init(hugetlb_init);\n\n/* Should be called on processing a hugepagesz=... option */\nvoid __init hugetlb_add_hstate(unsigned order)\n{\n\tstruct hstate *h;\n\tunsigned long i;\n\n\tif (size_to_hstate(PAGE_SIZE << order)) {\n\t\tprintk(KERN_WARNING \"hugepagesz= specified twice, ignoring\\n\");\n\t\treturn;\n\t}\n\tBUG_ON(max_hstate >= HUGE_MAX_HSTATE);\n\tBUG_ON(order == 0);\n\th = &hstates[max_hstate++];\n\th->order = order;\n\th->mask = ~((1ULL << (order + PAGE_SHIFT)) - 1);\n\th->nr_huge_pages = 0;\n\th->free_huge_pages = 0;\n\tfor (i = 0; i < MAX_NUMNODES; ++i)\n\t\tINIT_LIST_HEAD(&h->hugepage_freelists[i]);\n\th->next_nid_to_alloc = first_node(node_states[N_HIGH_MEMORY]);\n\th->next_nid_to_free = first_node(node_states[N_HIGH_MEMORY]);\n\tsnprintf(h->name, HSTATE_NAME_LEN, \"hugepages-%lukB\",\n\t\t\t\t\thuge_page_size(h)/1024);\n\n\tparsed_hstate = h;\n}\n\nstatic int __init hugetlb_nrpages_setup(char *s)\n{\n\tunsigned long *mhp;\n\tstatic unsigned long *last_mhp;\n\n\t/*\n\t * !max_hstate means we haven't parsed a hugepagesz= parameter yet,\n\t * so this hugepages= parameter goes to the \"default hstate\".\n\t */\n\tif (!max_hstate)\n\t\tmhp = &default_hstate_max_huge_pages;\n\telse\n\t\tmhp = &parsed_hstate->max_huge_pages;\n\n\tif (mhp == last_mhp) {\n\t\tprintk(KERN_WARNING \"hugepages= specified twice without \"\n\t\t\t\"interleaving hugepagesz=, ignoring\\n\");\n\t\treturn 1;\n\t}\n\n\tif (sscanf(s, \"%lu\", mhp) <= 0)\n\t\t*mhp = 0;\n\n\t/*\n\t * Global state is always initialized later in hugetlb_init.\n\t * But we need to allocate >= MAX_ORDER hstates here early to still\n\t * use the bootmem allocator.\n\t */\n\tif (max_hstate && parsed_hstate->order >= MAX_ORDER)\n\t\thugetlb_hstate_alloc_pages(parsed_hstate);\n\n\tlast_mhp = mhp;\n\n\treturn 1;\n}\n__setup(\"hugepages=\", hugetlb_nrpages_setup);\n\nstatic int __init hugetlb_default_setup(char *s)\n{\n\tdefault_hstate_size = memparse(s, &s);\n\treturn 1;\n}\n__setup(\"default_hugepagesz=\", hugetlb_default_setup);\n\nstatic unsigned int cpuset_mems_nr(unsigned int *array)\n{\n\tint node;\n\tunsigned int nr = 0;\n\n\tfor_each_node_mask(node, cpuset_current_mems_allowed)\n\t\tnr += array[node];\n\n\treturn nr;\n}\n\n#ifdef CONFIG_SYSCTL\nstatic int hugetlb_sysctl_handler_common(bool obey_mempolicy,\n\t\t\t struct ctl_table *table, int write,\n\t\t\t void __user *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\ttmp = h->max_huge_pages;\n\n\tif (write && h->order >= MAX_ORDER)\n\t\treturn -EINVAL;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tNODEMASK_ALLOC(nodemask_t, nodes_allowed,\n\t\t\t\t\t\tGFP_KERNEL | __GFP_NORETRY);\n\t\tif (!(obey_mempolicy &&\n\t\t\t       init_nodemask_of_mempolicy(nodes_allowed))) {\n\t\t\tNODEMASK_FREE(nodes_allowed);\n\t\t\tnodes_allowed = &node_states[N_HIGH_MEMORY];\n\t\t}\n\t\th->max_huge_pages = set_max_huge_pages(h, tmp, nodes_allowed);\n\n\t\tif (nodes_allowed != &node_states[N_HIGH_MEMORY])\n\t\t\tNODEMASK_FREE(nodes_allowed);\n\t}\nout:\n\treturn ret;\n}\n\nint hugetlb_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void __user *buffer, size_t *length, loff_t *ppos)\n{\n\n\treturn hugetlb_sysctl_handler_common(false, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n\n#ifdef CONFIG_NUMA\nint hugetlb_mempolicy_sysctl_handler(struct ctl_table *table, int write,\n\t\t\t  void __user *buffer, size_t *length, loff_t *ppos)\n{\n\treturn hugetlb_sysctl_handler_common(true, table, write,\n\t\t\t\t\t\t\tbuffer, length, ppos);\n}\n#endif /* CONFIG_NUMA */\n\nint hugetlb_treat_movable_handler(struct ctl_table *table, int write,\n\t\t\tvoid __user *buffer,\n\t\t\tsize_t *length, loff_t *ppos)\n{\n\tproc_dointvec(table, write, buffer, length, ppos);\n\tif (hugepages_treat_as_movable)\n\t\thtlb_alloc_mask = GFP_HIGHUSER_MOVABLE;\n\telse\n\t\thtlb_alloc_mask = GFP_HIGHUSER;\n\treturn 0;\n}\n\nint hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\t\tvoid __user *buffer,\n\t\t\tsize_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && h->order >= MAX_ORDER)\n\t\treturn -EINVAL;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}\n\n#endif /* CONFIG_SYSCTL */\n\nvoid hugetlb_report_meminfo(struct seq_file *m)\n{\n\tstruct hstate *h = &default_hstate;\n\tseq_printf(m,\n\t\t\t\"HugePages_Total:   %5lu\\n\"\n\t\t\t\"HugePages_Free:    %5lu\\n\"\n\t\t\t\"HugePages_Rsvd:    %5lu\\n\"\n\t\t\t\"HugePages_Surp:    %5lu\\n\"\n\t\t\t\"Hugepagesize:   %8lu kB\\n\",\n\t\t\th->nr_huge_pages,\n\t\t\th->free_huge_pages,\n\t\t\th->resv_huge_pages,\n\t\t\th->surplus_huge_pages,\n\t\t\t1UL << (huge_page_order(h) + PAGE_SHIFT - 10));\n}\n\nint hugetlb_report_node_meminfo(int nid, char *buf)\n{\n\tstruct hstate *h = &default_hstate;\n\treturn sprintf(buf,\n\t\t\"Node %d HugePages_Total: %5u\\n\"\n\t\t\"Node %d HugePages_Free:  %5u\\n\"\n\t\t\"Node %d HugePages_Surp:  %5u\\n\",\n\t\tnid, h->nr_huge_pages_node[nid],\n\t\tnid, h->free_huge_pages_node[nid],\n\t\tnid, h->surplus_huge_pages_node[nid]);\n}\n\n/* Return the number pages of memory we physically have, in PAGE_SIZE units. */\nunsigned long hugetlb_total_pages(void)\n{\n\tstruct hstate *h = &default_hstate;\n\treturn h->nr_huge_pages * pages_per_huge_page(h);\n}\n\nstatic int hugetlb_acct_memory(struct hstate *h, long delta)\n{\n\tint ret = -ENOMEM;\n\n\tspin_lock(&hugetlb_lock);\n\t/*\n\t * When cpuset is configured, it breaks the strict hugetlb page\n\t * reservation as the accounting is done on a global variable. Such\n\t * reservation is completely rubbish in the presence of cpuset because\n\t * the reservation is not checked against page availability for the\n\t * current cpuset. Application can still potentially OOM'ed by kernel\n\t * with lack of free htlb page in cpuset that the task is in.\n\t * Attempt to enforce strict accounting with cpuset is almost\n\t * impossible (or too ugly) because cpuset is too fluid that\n\t * task or memory node can be dynamically moved between cpusets.\n\t *\n\t * The change of semantics for shared hugetlb mapping with cpuset is\n\t * undesirable. However, in order to preserve some of the semantics,\n\t * we fall back to check against current free page availability as\n\t * a best attempt and hopefully to minimize the impact of changing\n\t * semantics that cpuset has.\n\t */\n\tif (delta > 0) {\n\t\tif (gather_surplus_pages(h, delta) < 0)\n\t\t\tgoto out;\n\n\t\tif (delta > cpuset_mems_nr(h->free_huge_pages_node)) {\n\t\t\treturn_unused_surplus_pages(h, delta);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = 0;\n\tif (delta < 0)\n\t\treturn_unused_surplus_pages(h, (unsigned long) -delta);\n\nout:\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n\nstatic void hugetlb_vm_op_open(struct vm_area_struct *vma)\n{\n\tstruct resv_map *reservations = vma_resv_map(vma);\n\n\t/*\n\t * This new VMA should share its siblings reservation map if present.\n\t * The VMA will only ever have a valid reservation map pointer where\n\t * it is being copied for another still existing VMA.  As that VMA\n\t * has a reference to the reservation map it cannot disappear until\n\t * after this open call completes.  It is therefore safe to take a\n\t * new reference here without additional locking.\n\t */\n\tif (reservations)\n\t\tkref_get(&reservations->refs);\n}\n\nstatic void resv_map_put(struct vm_area_struct *vma)\n{\n\tstruct resv_map *reservations = vma_resv_map(vma);\n\n\tif (!reservations)\n\t\treturn;\n\tkref_put(&reservations->refs, resv_map_release);\n}\n\nstatic void hugetlb_vm_op_close(struct vm_area_struct *vma)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct resv_map *reservations = vma_resv_map(vma);\n\tstruct hugepage_subpool *spool = subpool_vma(vma);\n\tunsigned long reserve;\n\tunsigned long start;\n\tunsigned long end;\n\n\tif (reservations) {\n\t\tstart = vma_hugecache_offset(h, vma, vma->vm_start);\n\t\tend = vma_hugecache_offset(h, vma, vma->vm_end);\n\n\t\treserve = (end - start) -\n\t\t\tregion_count(&reservations->regions, start, end);\n\n\t\tresv_map_put(vma);\n\n\t\tif (reserve) {\n\t\t\thugetlb_acct_memory(h, -reserve);\n\t\t\thugepage_subpool_put_pages(spool, reserve);\n\t\t}\n\t}\n}\n\n/*\n * We cannot handle pagefaults against hugetlb pages at all.  They cause\n * handle_mm_fault() to try to instantiate regular-sized pages in the\n * hugegpage VMA.  do_page_fault() is supposed to trap this, so BUG is we get\n * this far.\n */\nstatic int hugetlb_vm_op_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tBUG();\n\treturn 0;\n}\n\nconst struct vm_operations_struct hugetlb_vm_ops = {\n\t.fault = hugetlb_vm_op_fault,\n\t.open = hugetlb_vm_op_open,\n\t.close = hugetlb_vm_op_close,\n};\n\nstatic pte_t make_huge_pte(struct vm_area_struct *vma, struct page *page,\n\t\t\t\tint writable)\n{\n\tpte_t entry;\n\n\tif (writable) {\n\t\tentry =\n\t\t    pte_mkwrite(pte_mkdirty(mk_pte(page, vma->vm_page_prot)));\n\t} else {\n\t\tentry = huge_pte_wrprotect(mk_pte(page, vma->vm_page_prot));\n\t}\n\tentry = pte_mkyoung(entry);\n\tentry = pte_mkhuge(entry);\n\tentry = arch_make_huge_pte(entry, vma, page, writable);\n\n\treturn entry;\n}\n\nstatic void set_huge_ptep_writable(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long address, pte_t *ptep)\n{\n\tpte_t entry;\n\n\tentry = pte_mkwrite(pte_mkdirty(huge_ptep_get(ptep)));\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry, 1))\n\t\tupdate_mmu_cache(vma, address, ptep);\n}\n\n\nint copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,\n\t\t\t    struct vm_area_struct *vma)\n{\n\tpte_t *src_pte, *dst_pte, entry;\n\tstruct page *ptepage;\n\tunsigned long addr;\n\tint cow;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\n\tcow = (vma->vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;\n\n\tfor (addr = vma->vm_start; addr < vma->vm_end; addr += sz) {\n\t\tsrc_pte = huge_pte_offset(src, addr);\n\t\tif (!src_pte)\n\t\t\tcontinue;\n\t\tdst_pte = huge_pte_alloc(dst, addr, sz);\n\t\tif (!dst_pte)\n\t\t\tgoto nomem;\n\n\t\t/* If the pagetables are shared don't copy or take references */\n\t\tif (dst_pte == src_pte)\n\t\t\tcontinue;\n\n\t\tspin_lock(&dst->page_table_lock);\n\t\tspin_lock_nested(&src->page_table_lock, SINGLE_DEPTH_NESTING);\n\t\tif (!huge_pte_none(huge_ptep_get(src_pte))) {\n\t\t\tif (cow)\n\t\t\t\thuge_ptep_set_wrprotect(src, addr, src_pte);\n\t\t\tentry = huge_ptep_get(src_pte);\n\t\t\tptepage = pte_page(entry);\n\t\t\tget_page(ptepage);\n\t\t\tpage_dup_rmap(ptepage);\n\t\t\tset_huge_pte_at(dst, addr, dst_pte, entry);\n\t\t}\n\t\tspin_unlock(&src->page_table_lock);\n\t\tspin_unlock(&dst->page_table_lock);\n\t}\n\treturn 0;\n\nnomem:\n\treturn -ENOMEM;\n}\n\nstatic int is_hugetlb_entry_migration(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn 0;\n\tswp = pte_to_swp_entry(pte);\n\tif (non_swap_entry(swp) && is_migration_entry(swp))\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n\nstatic int is_hugetlb_entry_hwpoisoned(pte_t pte)\n{\n\tswp_entry_t swp;\n\n\tif (huge_pte_none(pte) || pte_present(pte))\n\t\treturn 0;\n\tswp = pte_to_swp_entry(pte);\n\tif (non_swap_entry(swp) && is_hwpoison_entry(swp))\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n\nvoid __unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,\n\t\t\t    unsigned long end, struct page *ref_page)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tstruct page *page;\n\tstruct page *tmp;\n\tstruct hstate *h = hstate_vma(vma);\n\tunsigned long sz = huge_page_size(h);\n\n\t/*\n\t * A page gathering list, protected by per file i_mmap_mutex. The\n\t * lock is used to avoid list corruption from multiple unmapping\n\t * of the same page since we are using page->lru.\n\t */\n\tLIST_HEAD(page_list);\n\n\tWARN_ON(!is_vm_hugetlb_page(vma));\n\tBUG_ON(start & ~huge_page_mask(h));\n\tBUG_ON(end & ~huge_page_mask(h));\n\n\tmmu_notifier_invalidate_range_start(mm, start, end);\n\tspin_lock(&mm->page_table_lock);\n\tfor (address = start; address < end; address += sz) {\n\t\tptep = huge_pte_offset(mm, address);\n\t\tif (!ptep)\n\t\t\tcontinue;\n\n\t\tif (huge_pmd_unshare(mm, &address, ptep))\n\t\t\tcontinue;\n\n\t\tpte = huge_ptep_get(ptep);\n\t\tif (huge_pte_none(pte))\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * HWPoisoned hugepage is already unmapped and dropped reference\n\t\t */\n\t\tif (unlikely(is_hugetlb_entry_hwpoisoned(pte)))\n\t\t\tcontinue;\n\n\t\tpage = pte_page(pte);\n\t\t/*\n\t\t * If a reference page is supplied, it is because a specific\n\t\t * page is being unmapped, not a range. Ensure the page we\n\t\t * are about to unmap is the actual page of interest.\n\t\t */\n\t\tif (ref_page) {\n\t\t\tif (page != ref_page)\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * Mark the VMA as having unmapped its page so that\n\t\t\t * future faults in this VMA will fail rather than\n\t\t\t * looking like data was lost\n\t\t\t */\n\t\t\tset_vma_resv_flags(vma, HPAGE_RESV_UNMAPPED);\n\t\t}\n\n\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\tif (pte_dirty(pte))\n\t\t\tset_page_dirty(page);\n\t\tlist_add(&page->lru, &page_list);\n\n\t\t/* Bail out after unmapping reference page if supplied */\n\t\tif (ref_page)\n\t\t\tbreak;\n\t}\n\tflush_tlb_range(vma, start, end);\n\tspin_unlock(&mm->page_table_lock);\n\tmmu_notifier_invalidate_range_end(mm, start, end);\n\tlist_for_each_entry_safe(page, tmp, &page_list, lru) {\n\t\tpage_remove_rmap(page);\n\t\tlist_del(&page->lru);\n\t\tput_page(page);\n\t}\n}\n\nvoid unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,\n\t\t\t  unsigned long end, struct page *ref_page)\n{\n\tmutex_lock(&vma->vm_file->f_mapping->i_mmap_mutex);\n\t__unmap_hugepage_range(vma, start, end, ref_page);\n\tmutex_unlock(&vma->vm_file->f_mapping->i_mmap_mutex);\n}\n\n/*\n * This is called when the original mapper is failing to COW a MAP_PRIVATE\n * mappping it owns the reserve page for. The intention is to unmap the page\n * from other VMAs and let the children be SIGKILLed if they are faulting the\n * same region.\n */\nstatic int unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t\tstruct page *page, unsigned long address)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct vm_area_struct *iter_vma;\n\tstruct address_space *mapping;\n\tstruct prio_tree_iter iter;\n\tpgoff_t pgoff;\n\n\t/*\n\t * vm_pgoff is in PAGE_SIZE units, hence the different calculation\n\t * from page cache lookup which is in HPAGE_SIZE units.\n\t */\n\taddress = address & huge_page_mask(h);\n\tpgoff = vma_hugecache_offset(h, vma, address);\n\tmapping = vma->vm_file->f_dentry->d_inode->i_mapping;\n\n\t/*\n\t * Take the mapping lock for the duration of the table walk. As\n\t * this mapping should be shared between all the VMAs,\n\t * __unmap_hugepage_range() is called as the lock is already held\n\t */\n\tmutex_lock(&mapping->i_mmap_mutex);\n\tvma_prio_tree_foreach(iter_vma, &iter, &mapping->i_mmap, pgoff, pgoff) {\n\t\t/* Do not unmap the current VMA */\n\t\tif (iter_vma == vma)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Unmap the page from other VMAs without their own reserves.\n\t\t * They get marked to be SIGKILLed if they fault in these\n\t\t * areas. This is because a future no-page fault on this VMA\n\t\t * could insert a zeroed page instead of the data existing\n\t\t * from the time of fork. This would look like data corruption\n\t\t */\n\t\tif (!is_vma_resv_set(iter_vma, HPAGE_RESV_OWNER))\n\t\t\t__unmap_hugepage_range(iter_vma,\n\t\t\t\taddress, address + huge_page_size(h),\n\t\t\t\tpage);\n\t}\n\tmutex_unlock(&mapping->i_mmap_mutex);\n\n\treturn 1;\n}\n\n/*\n * Hugetlb_cow() should be called with page lock of the original hugepage held.\n * Called with hugetlb_instantiation_mutex held and pte_page locked so we\n * cannot race with other handlers or page migration.\n * Keep the pte_same checks anyway to make transition from the mutex easier.\n */\nstatic int hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, pte_t *ptep, pte_t pte,\n\t\t\tstruct page *pagecache_page)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tstruct page *old_page, *new_page;\n\tint avoidcopy;\n\tint outside_reserve = 0;\n\n\told_page = pte_page(pte);\n\nretry_avoidcopy:\n\t/* If no-one else is actually using this page, avoid the copy\n\t * and just make the page writable */\n\tavoidcopy = (page_mapcount(old_page) == 1);\n\tif (avoidcopy) {\n\t\tif (PageAnon(old_page))\n\t\t\tpage_move_anon_rmap(old_page, vma, address);\n\t\tset_huge_ptep_writable(vma, address, ptep);\n\t\treturn 0;\n\t}\n\n\t/*\n\t * If the process that created a MAP_PRIVATE mapping is about to\n\t * perform a COW due to a shared page count, attempt to satisfy\n\t * the allocation without using the existing reserves. The pagecache\n\t * page is used to determine if the reserve at this address was\n\t * consumed or not. If reserves were used, a partial faulted mapping\n\t * at the time of fork() could consume its reserves on COW instead\n\t * of the full address range.\n\t */\n\tif (!(vma->vm_flags & VM_MAYSHARE) &&\n\t\t\tis_vma_resv_set(vma, HPAGE_RESV_OWNER) &&\n\t\t\told_page != pagecache_page)\n\t\toutside_reserve = 1;\n\n\tpage_cache_get(old_page);\n\n\t/* Drop page_table_lock as buddy allocator may be called */\n\tspin_unlock(&mm->page_table_lock);\n\tnew_page = alloc_huge_page(vma, address, outside_reserve);\n\n\tif (IS_ERR(new_page)) {\n\t\tpage_cache_release(old_page);\n\n\t\t/*\n\t\t * If a process owning a MAP_PRIVATE mapping fails to COW,\n\t\t * it is due to references held by a child and an insufficient\n\t\t * huge page pool. To guarantee the original mappers\n\t\t * reliability, unmap the page from child processes. The child\n\t\t * may get SIGKILLed if it later faults.\n\t\t */\n\t\tif (outside_reserve) {\n\t\t\tBUG_ON(huge_pte_none(pte));\n\t\t\tif (unmap_ref_private(mm, vma, old_page, address)) {\n\t\t\t\tBUG_ON(huge_pte_none(pte));\n\t\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\t\tptep = huge_pte_offset(mm, address & huge_page_mask(h));\n\t\t\t\tif (likely(pte_same(huge_ptep_get(ptep), pte)))\n\t\t\t\t\tgoto retry_avoidcopy;\n\t\t\t\t/*\n\t\t\t\t * race occurs while re-acquiring page_table_lock, and\n\t\t\t\t * our job is done.\n\t\t\t\t */\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tWARN_ON_ONCE(1);\n\t\t}\n\n\t\t/* Caller expects lock to be held */\n\t\tspin_lock(&mm->page_table_lock);\n\t\treturn -PTR_ERR(new_page);\n\t}\n\n\t/*\n\t * When the original hugepage is shared one, it does not have\n\t * anon_vma prepared.\n\t */\n\tif (unlikely(anon_vma_prepare(vma))) {\n\t\tpage_cache_release(new_page);\n\t\tpage_cache_release(old_page);\n\t\t/* Caller expects lock to be held */\n\t\tspin_lock(&mm->page_table_lock);\n\t\treturn VM_FAULT_OOM;\n\t}\n\n\tcopy_user_huge_page(new_page, old_page, address, vma,\n\t\t\t    pages_per_huge_page(h));\n\t__SetPageUptodate(new_page);\n\n\t/*\n\t * Retake the page_table_lock to check for racing updates\n\t * before the page tables are altered\n\t */\n\tspin_lock(&mm->page_table_lock);\n\tptep = huge_pte_offset(mm, address & huge_page_mask(h));\n\tif (likely(pte_same(huge_ptep_get(ptep), pte))) {\n\t\t/* Break COW */\n\t\tmmu_notifier_invalidate_range_start(mm,\n\t\t\taddress & huge_page_mask(h),\n\t\t\t(address & huge_page_mask(h)) + huge_page_size(h));\n\t\thuge_ptep_clear_flush(vma, address, ptep);\n\t\tset_huge_pte_at(mm, address, ptep,\n\t\t\t\tmake_huge_pte(vma, new_page, 1));\n\t\tpage_remove_rmap(old_page);\n\t\thugepage_add_new_anon_rmap(new_page, vma, address);\n\t\t/* Make the old page be freed below */\n\t\tnew_page = old_page;\n\t\tmmu_notifier_invalidate_range_end(mm,\n\t\t\taddress & huge_page_mask(h),\n\t\t\t(address & huge_page_mask(h)) + huge_page_size(h));\n\t}\n\tpage_cache_release(new_page);\n\tpage_cache_release(old_page);\n\treturn 0;\n}\n\n/* Return the pagecache page at a given address within a VMA */\nstatic struct page *hugetlbfs_pagecache_page(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\treturn find_lock_page(mapping, idx);\n}\n\n/*\n * Return whether there is a pagecache page to back given address within VMA.\n * Caller follow_hugetlb_page() holds page_table_lock so we cannot lock_page.\n */\nstatic bool hugetlbfs_pagecache_present(struct hstate *h,\n\t\t\tstruct vm_area_struct *vma, unsigned long address)\n{\n\tstruct address_space *mapping;\n\tpgoff_t idx;\n\tstruct page *page;\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\tpage = find_get_page(mapping, idx);\n\tif (page)\n\t\tput_page(page);\n\treturn page != NULL;\n}\n\nstatic int hugetlb_no_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, pte_t *ptep, unsigned int flags)\n{\n\tstruct hstate *h = hstate_vma(vma);\n\tint ret = VM_FAULT_SIGBUS;\n\tint anon_rmap = 0;\n\tpgoff_t idx;\n\tunsigned long size;\n\tstruct page *page;\n\tstruct address_space *mapping;\n\tpte_t new_pte;\n\n\t/*\n\t * Currently, we are forced to kill the process in the event the\n\t * original mapper has unmapped pages from the child due to a failed\n\t * COW. Warn that such a situation has occurred as it may not be obvious\n\t */\n\tif (is_vma_resv_set(vma, HPAGE_RESV_UNMAPPED)) {\n\t\tprintk(KERN_WARNING\n\t\t\t\"PID %d killed due to inadequate hugepage pool\\n\",\n\t\t\tcurrent->pid);\n\t\treturn ret;\n\t}\n\n\tmapping = vma->vm_file->f_mapping;\n\tidx = vma_hugecache_offset(h, vma, address);\n\n\t/*\n\t * Use page lock to guard against racing truncation\n\t * before we get page_table_lock.\n\t */\nretry:\n\tpage = find_lock_page(mapping, idx);\n\tif (!page) {\n\t\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\t\tif (idx >= size)\n\t\t\tgoto out;\n\t\tpage = alloc_huge_page(vma, address, 0);\n\t\tif (IS_ERR(page)) {\n\t\t\tret = -PTR_ERR(page);\n\t\t\tgoto out;\n\t\t}\n\t\tclear_huge_page(page, address, pages_per_huge_page(h));\n\t\t__SetPageUptodate(page);\n\n\t\tif (vma->vm_flags & VM_MAYSHARE) {\n\t\t\tint err;\n\t\t\tstruct inode *inode = mapping->host;\n\n\t\t\terr = add_to_page_cache(page, mapping, idx, GFP_KERNEL);\n\t\t\tif (err) {\n\t\t\t\tput_page(page);\n\t\t\t\tif (err == -EEXIST)\n\t\t\t\t\tgoto retry;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tinode->i_blocks += blocks_per_huge_page(h);\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t} else {\n\t\t\tlock_page(page);\n\t\t\tif (unlikely(anon_vma_prepare(vma))) {\n\t\t\t\tret = VM_FAULT_OOM;\n\t\t\t\tgoto backout_unlocked;\n\t\t\t}\n\t\t\tanon_rmap = 1;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If memory error occurs between mmap() and fault, some process\n\t\t * don't have hwpoisoned swap entry for errored virtual address.\n\t\t * So we need to block hugepage fault by PG_hwpoison bit check.\n\t\t */\n\t\tif (unlikely(PageHWPoison(page))) {\n\t\t\tret = VM_FAULT_HWPOISON |\n\t\t\t      VM_FAULT_SET_HINDEX(h - hstates);\n\t\t\tgoto backout_unlocked;\n\t\t}\n\t}\n\n\t/*\n\t * If we are going to COW a private mapping later, we examine the\n\t * pending reservations for this page now. This will ensure that\n\t * any allocations necessary to record that reservation occur outside\n\t * the spinlock.\n\t */\n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED))\n\t\tif (vma_needs_reservation(h, vma, address) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto backout_unlocked;\n\t\t}\n\n\tspin_lock(&mm->page_table_lock);\n\tsize = i_size_read(mapping->host) >> huge_page_shift(h);\n\tif (idx >= size)\n\t\tgoto backout;\n\n\tret = 0;\n\tif (!huge_pte_none(huge_ptep_get(ptep)))\n\t\tgoto backout;\n\n\tif (anon_rmap)\n\t\thugepage_add_new_anon_rmap(page, vma, address);\n\telse\n\t\tpage_dup_rmap(page);\n\tnew_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)\n\t\t\t\t&& (vma->vm_flags & VM_SHARED)));\n\tset_huge_pte_at(mm, address, ptep, new_pte);\n\n\tif ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {\n\t\t/* Optimization, do the COW without a second fault */\n\t\tret = hugetlb_cow(mm, vma, address, ptep, new_pte, page);\n\t}\n\n\tspin_unlock(&mm->page_table_lock);\n\tunlock_page(page);\nout:\n\treturn ret;\n\nbackout:\n\tspin_unlock(&mm->page_table_lock);\nbackout_unlocked:\n\tunlock_page(page);\n\tput_page(page);\n\tgoto out;\n}\n\nint hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, unsigned int flags)\n{\n\tpte_t *ptep;\n\tpte_t entry;\n\tint ret;\n\tstruct page *page = NULL;\n\tstruct page *pagecache_page = NULL;\n\tstatic DEFINE_MUTEX(hugetlb_instantiation_mutex);\n\tstruct hstate *h = hstate_vma(vma);\n\n\taddress &= huge_page_mask(h);\n\n\tptep = huge_pte_offset(mm, address);\n\tif (ptep) {\n\t\tentry = huge_ptep_get(ptep);\n\t\tif (unlikely(is_hugetlb_entry_migration(entry))) {\n\t\t\tmigration_entry_wait(mm, (pmd_t *)ptep, address);\n\t\t\treturn 0;\n\t\t} else if (unlikely(is_hugetlb_entry_hwpoisoned(entry)))\n\t\t\treturn VM_FAULT_HWPOISON_LARGE |\n\t\t\t       VM_FAULT_SET_HINDEX(h - hstates);\n\t}\n\n\tptep = huge_pte_alloc(mm, address, huge_page_size(h));\n\tif (!ptep)\n\t\treturn VM_FAULT_OOM;\n\n\t/*\n\t * Serialize hugepage allocation and instantiation, so that we don't\n\t * get spurious allocation failures if two CPUs race to instantiate\n\t * the same page in the page cache.\n\t */\n\tmutex_lock(&hugetlb_instantiation_mutex);\n\tentry = huge_ptep_get(ptep);\n\tif (huge_pte_none(entry)) {\n\t\tret = hugetlb_no_page(mm, vma, address, ptep, flags);\n\t\tgoto out_mutex;\n\t}\n\n\tret = 0;\n\n\t/*\n\t * If we are going to COW the mapping later, we examine the pending\n\t * reservations for this page now. This will ensure that any\n\t * allocations necessary to record that reservation occur outside the\n\t * spinlock. For private mappings, we also lookup the pagecache\n\t * page now as it is used to determine if a reservation has been\n\t * consumed.\n\t */\n\tif ((flags & FAULT_FLAG_WRITE) && !pte_write(entry)) {\n\t\tif (vma_needs_reservation(h, vma, address) < 0) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_mutex;\n\t\t}\n\n\t\tif (!(vma->vm_flags & VM_MAYSHARE))\n\t\t\tpagecache_page = hugetlbfs_pagecache_page(h,\n\t\t\t\t\t\t\t\tvma, address);\n\t}\n\n\t/*\n\t * hugetlb_cow() requires page locks of pte_page(entry) and\n\t * pagecache_page, so here we need take the former one\n\t * when page != pagecache_page or !pagecache_page.\n\t * Note that locking order is always pagecache_page -> page,\n\t * so no worry about deadlock.\n\t */\n\tpage = pte_page(entry);\n\tget_page(page);\n\tif (page != pagecache_page)\n\t\tlock_page(page);\n\n\tspin_lock(&mm->page_table_lock);\n\t/* Check for a racing update before calling hugetlb_cow */\n\tif (unlikely(!pte_same(entry, huge_ptep_get(ptep))))\n\t\tgoto out_page_table_lock;\n\n\n\tif (flags & FAULT_FLAG_WRITE) {\n\t\tif (!pte_write(entry)) {\n\t\t\tret = hugetlb_cow(mm, vma, address, ptep, entry,\n\t\t\t\t\t\t\tpagecache_page);\n\t\t\tgoto out_page_table_lock;\n\t\t}\n\t\tentry = pte_mkdirty(entry);\n\t}\n\tentry = pte_mkyoung(entry);\n\tif (huge_ptep_set_access_flags(vma, address, ptep, entry,\n\t\t\t\t\t\tflags & FAULT_FLAG_WRITE))\n\t\tupdate_mmu_cache(vma, address, ptep);\n\nout_page_table_lock:\n\tspin_unlock(&mm->page_table_lock);\n\n\tif (pagecache_page) {\n\t\tunlock_page(pagecache_page);\n\t\tput_page(pagecache_page);\n\t}\n\tif (page != pagecache_page)\n\t\tunlock_page(page);\n\tput_page(page);\n\nout_mutex:\n\tmutex_unlock(&hugetlb_instantiation_mutex);\n\n\treturn ret;\n}\n\n/* Can be overriden by architectures */\n__attribute__((weak)) struct page *\nfollow_huge_pud(struct mm_struct *mm, unsigned long address,\n\t       pud_t *pud, int write)\n{\n\tBUG();\n\treturn NULL;\n}\n\nint follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tstruct page **pages, struct vm_area_struct **vmas,\n\t\t\tunsigned long *position, int *length, int i,\n\t\t\tunsigned int flags)\n{\n\tunsigned long pfn_offset;\n\tunsigned long vaddr = *position;\n\tint remainder = *length;\n\tstruct hstate *h = hstate_vma(vma);\n\n\tspin_lock(&mm->page_table_lock);\n\twhile (vaddr < vma->vm_end && remainder) {\n\t\tpte_t *pte;\n\t\tint absent;\n\t\tstruct page *page;\n\n\t\t/*\n\t\t * Some archs (sparc64, sh*) have multiple pte_ts to\n\t\t * each hugepage.  We have to make sure we get the\n\t\t * first, for the page indexing below to work.\n\t\t */\n\t\tpte = huge_pte_offset(mm, vaddr & huge_page_mask(h));\n\t\tabsent = !pte || huge_pte_none(huge_ptep_get(pte));\n\n\t\t/*\n\t\t * When coredumping, it suits get_dump_page if we just return\n\t\t * an error where there's an empty slot with no huge pagecache\n\t\t * to back it.  This way, we avoid allocating a hugepage, and\n\t\t * the sparse dumpfile avoids allocating disk blocks, but its\n\t\t * huge holes still show up with zeroes where they need to be.\n\t\t */\n\t\tif (absent && (flags & FOLL_DUMP) &&\n\t\t    !hugetlbfs_pagecache_present(h, vma, vaddr)) {\n\t\t\tremainder = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (absent ||\n\t\t    ((flags & FOLL_WRITE) && !pte_write(huge_ptep_get(pte)))) {\n\t\t\tint ret;\n\n\t\t\tspin_unlock(&mm->page_table_lock);\n\t\t\tret = hugetlb_fault(mm, vma, vaddr,\n\t\t\t\t(flags & FOLL_WRITE) ? FAULT_FLAG_WRITE : 0);\n\t\t\tspin_lock(&mm->page_table_lock);\n\t\t\tif (!(ret & VM_FAULT_ERROR))\n\t\t\t\tcontinue;\n\n\t\t\tremainder = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tpfn_offset = (vaddr & ~huge_page_mask(h)) >> PAGE_SHIFT;\n\t\tpage = pte_page(huge_ptep_get(pte));\nsame_page:\n\t\tif (pages) {\n\t\t\tpages[i] = mem_map_offset(page, pfn_offset);\n\t\t\tget_page(pages[i]);\n\t\t}\n\n\t\tif (vmas)\n\t\t\tvmas[i] = vma;\n\n\t\tvaddr += PAGE_SIZE;\n\t\t++pfn_offset;\n\t\t--remainder;\n\t\t++i;\n\t\tif (vaddr < vma->vm_end && remainder &&\n\t\t\t\tpfn_offset < pages_per_huge_page(h)) {\n\t\t\t/*\n\t\t\t * We use pfn_offset to avoid touching the pageframes\n\t\t\t * of this compound page.\n\t\t\t */\n\t\t\tgoto same_page;\n\t\t}\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\t*length = remainder;\n\t*position = vaddr;\n\n\treturn i ? i : -EFAULT;\n}\n\nvoid hugetlb_change_protection(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned long end, pgprot_t newprot)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tunsigned long start = address;\n\tpte_t *ptep;\n\tpte_t pte;\n\tstruct hstate *h = hstate_vma(vma);\n\n\tBUG_ON(address >= end);\n\tflush_cache_range(vma, address, end);\n\n\tmutex_lock(&vma->vm_file->f_mapping->i_mmap_mutex);\n\tspin_lock(&mm->page_table_lock);\n\tfor (; address < end; address += huge_page_size(h)) {\n\t\tptep = huge_pte_offset(mm, address);\n\t\tif (!ptep)\n\t\t\tcontinue;\n\t\tif (huge_pmd_unshare(mm, &address, ptep))\n\t\t\tcontinue;\n\t\tif (!huge_pte_none(huge_ptep_get(ptep))) {\n\t\t\tpte = huge_ptep_get_and_clear(mm, address, ptep);\n\t\t\tpte = pte_mkhuge(pte_modify(pte, newprot));\n\t\t\tset_huge_pte_at(mm, address, ptep, pte);\n\t\t}\n\t}\n\tspin_unlock(&mm->page_table_lock);\n\tmutex_unlock(&vma->vm_file->f_mapping->i_mmap_mutex);\n\n\tflush_tlb_range(vma, start, end);\n}\n\nint hugetlb_reserve_pages(struct inode *inode,\n\t\t\t\t\tlong from, long to,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tvm_flags_t vm_flags)\n{\n\tlong ret, chg;\n\tstruct hstate *h = hstate_inode(inode);\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\n\t/*\n\t * Only apply hugepage reservation if asked. At fault time, an\n\t * attempt will be made for VM_NORESERVE to allocate a page\n\t * without using reserves\n\t */\n\tif (vm_flags & VM_NORESERVE)\n\t\treturn 0;\n\n\t/*\n\t * Shared mappings base their reservation on the number of pages that\n\t * are already allocated on behalf of the file. Private mappings need\n\t * to reserve the full area even if read-only as mprotect() may be\n\t * called to make the mapping read-write. Assume !vma is a shm mapping\n\t */\n\tif (!vma || vma->vm_flags & VM_MAYSHARE)\n\t\tchg = region_chg(&inode->i_mapping->private_list, from, to);\n\telse {\n\t\tstruct resv_map *resv_map = resv_map_alloc();\n\t\tif (!resv_map)\n\t\t\treturn -ENOMEM;\n\n\t\tchg = to - from;\n\n\t\tset_vma_resv_map(vma, resv_map);\n\t\tset_vma_resv_flags(vma, HPAGE_RESV_OWNER);\n\t}\n\n\tif (chg < 0) {\n\t\tret = chg;\n\t\tgoto out_err;\n\t}\n\n\t/* There must be enough pages in the subpool for the mapping */\n\tif (hugepage_subpool_get_pages(spool, chg)) {\n\t\tret = -ENOSPC;\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * Check enough hugepages are available for the reservation.\n\t * Hand the pages back to the subpool if there are not\n\t */\n\tret = hugetlb_acct_memory(h, chg);\n\tif (ret < 0) {\n\t\thugepage_subpool_put_pages(spool, chg);\n\t\tgoto out_err;\n\t}\n\n\t/*\n\t * Account for the reservations made. Shared mappings record regions\n\t * that have reservations as they are shared by multiple VMAs.\n\t * When the last VMA disappears, the region map says how much\n\t * the reservation was and the page cache tells how much of\n\t * the reservation was consumed. Private mappings are per-VMA and\n\t * only the consumed reservations are tracked. When the VMA\n\t * disappears, the original reservation is the VMA size and the\n\t * consumed reservations are stored in the map. Hence, nothing\n\t * else has to be done for private mappings here\n\t */\n\tif (!vma || vma->vm_flags & VM_MAYSHARE)\n\t\tregion_add(&inode->i_mapping->private_list, from, to);\n\treturn 0;\nout_err:\n\tresv_map_put(vma);\n\treturn ret;\n}\n\nvoid hugetlb_unreserve_pages(struct inode *inode, long offset, long freed)\n{\n\tstruct hstate *h = hstate_inode(inode);\n\tlong chg = region_truncate(&inode->i_mapping->private_list, offset);\n\tstruct hugepage_subpool *spool = subpool_inode(inode);\n\n\tspin_lock(&inode->i_lock);\n\tinode->i_blocks -= (blocks_per_huge_page(h) * freed);\n\tspin_unlock(&inode->i_lock);\n\n\thugepage_subpool_put_pages(spool, (chg - freed));\n\thugetlb_acct_memory(h, -(chg - freed));\n}\n\n#ifdef CONFIG_MEMORY_FAILURE\n\n/* Should be called in hugetlb_lock */\nstatic int is_hugepage_on_freelist(struct page *hpage)\n{\n\tstruct page *page;\n\tstruct page *tmp;\n\tstruct hstate *h = page_hstate(hpage);\n\tint nid = page_to_nid(hpage);\n\n\tlist_for_each_entry_safe(page, tmp, &h->hugepage_freelists[nid], lru)\n\t\tif (page == hpage)\n\t\t\treturn 1;\n\treturn 0;\n}\n\n/*\n * This function is called from memory failure code.\n * Assume the caller holds page lock of the head page.\n */\nint dequeue_hwpoisoned_huge_page(struct page *hpage)\n{\n\tstruct hstate *h = page_hstate(hpage);\n\tint nid = page_to_nid(hpage);\n\tint ret = -EBUSY;\n\n\tspin_lock(&hugetlb_lock);\n\tif (is_hugepage_on_freelist(hpage)) {\n\t\tlist_del(&hpage->lru);\n\t\tset_page_refcounted(hpage);\n\t\th->free_huge_pages--;\n\t\th->free_huge_pages_node[nid]--;\n\t\tret = 0;\n\t}\n\tspin_unlock(&hugetlb_lock);\n\treturn ret;\n}\n#endif\n"], "filenames": ["mm/hugetlb.c"], "buggy_code_start_loc": [2159], "buggy_code_end_loc": [3024], "fixing_code_start_loc": [2160], "fixing_code_end_loc": [3041], "type": "CWE-399", "message": "Memory leak in mm/hugetlb.c in the Linux kernel before 3.4.2 allows local users to cause a denial of service (memory consumption or system crash) via invalid MAP_HUGETLB mmap operations.", "other": {"cve": {"id": "CVE-2012-2390", "sourceIdentifier": "secalert@redhat.com", "published": "2012-06-13T10:24:56.093", "lastModified": "2023-02-13T04:33:34.320", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "Memory leak in mm/hugetlb.c in the Linux kernel before 3.4.2 allows local users to cause a denial of service (memory consumption or system crash) via invalid MAP_HUGETLB mmap operations."}, {"lang": "es", "value": "Memoria no liberada (memory leak) en mm/hugetlb.c del kernel de Linux en versiones anteriores a la 3.4.2. Permite a usuarios locales provocar una denegaci\u00f3n de servicio (consumo de la memoria o ca\u00edda del sistema) a traves de operaciones mmap MAP_HUGETLB inv\u00e1lidas."}], "metrics": {"cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-399"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "3.4.1", "matchCriteriaId": "6F590FA6-B147-4E44-96E4-A647629DBCF4"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:*:*:*:*:*:*:*", "matchCriteriaId": "0F960FA6-F904-4A4E-B483-44C70090E9A1"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc1:*:*:*:*:*:*", "matchCriteriaId": "261C1B41-C9E0-414F-8368-51C0C0B8AD38"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc2:*:*:*:*:*:*", "matchCriteriaId": "5CCA261D-2B97-492F-89A0-5F209A804350"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc3:*:*:*:*:*:*", "matchCriteriaId": "1B1C0C68-9194-473F-BE5E-EC7F184899FA"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc4:*:*:*:*:*:*", "matchCriteriaId": "D7A6AC9E-BEA6-44B0-B3B3-F0F94E32424A"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc5:*:*:*:*:*:*", "matchCriteriaId": "16038328-9399-4B85-B777-BA4757D02C9B"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc6:*:*:*:*:*:*", "matchCriteriaId": "16CA2757-FA8D-43D9-96E8-D3C0EB6E1DEF"}, {"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:3.4:rc7:*:*:*:*:*:*", "matchCriteriaId": "E8CB5481-5EAE-401E-BD7E-D3095CCA9E94"}]}]}], "references": [{"url": "http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git%3Ba=commit%3Bh=c50ac050811d6485616a193eb0f37bfbd191cc89", "source": "secalert@redhat.com"}, {"url": "http://www.kernel.org/pub/linux/kernel/v3.x/ChangeLog-3.4.2", "source": "secalert@redhat.com"}, {"url": "http://www.openwall.com/lists/oss-security/2012/05/23/14", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1515-1", "source": "secalert@redhat.com"}, {"url": "http://www.ubuntu.com/usn/USN-1535-1", "source": "secalert@redhat.com"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=824345", "source": "secalert@redhat.com"}, {"url": "https://github.com/torvalds/linux/commit/c50ac050811d6485616a193eb0f37bfbd191cc89", "source": "secalert@redhat.com", "tags": ["Exploit", "Patch"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/c50ac050811d6485616a193eb0f37bfbd191cc89"}}