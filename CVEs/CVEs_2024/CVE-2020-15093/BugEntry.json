{"buggy_code": ["#!/usr/bin/env python\n\n# Copyright 2012 - 2017, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\n<Program Name>\n  test_sig.py\n\n<Author>\n  Geremy Condra\n  Vladimir Diaz <vladimir.v.diaz@gmail.com>\n\n<Started>\n  February 28, 2012.  Based on a previous version of this module.\n\n<Copyright>\n  See LICENSE-MIT OR LICENSE for licensing information.\n\n<Purpose>\n  Test cases for sig.py.\n\"\"\"\n\n# Help with Python 3 compatibility, where the print statement is a function, an\n# implicit relative import is invalid, and the '/' operator performs true\n# division.  Example:  print 'hello world' raises a 'SyntaxError' exception.\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nimport unittest\nimport logging\n\nimport tuf\nimport tuf.log\nimport tuf.formats\nimport tuf.keydb\nimport tuf.roledb\nimport tuf.sig\nimport tuf.exceptions\n\nimport securesystemslib\nimport securesystemslib.keys\n\nlogger = logging.getLogger('tuf.test_sig')\n\n# Setup the keys to use in our test cases.\nKEYS = []\nfor _ in range(3):\n  KEYS.append(securesystemslib.keys.generate_rsa_key(2048))\n\n\n\nclass TestSig(unittest.TestCase):\n  def setUp(self):\n    pass\n\n  def tearDown(self):\n    tuf.roledb.clear_roledb()\n    tuf.keydb.clear_keydb()\n\n\n  def test_get_signature_status_no_role(self):\n    signable = {'signed': 'test', 'signatures': []}\n\n    # A valid, but empty signature status.\n    sig_status = tuf.sig.get_signature_status(signable)\n    self.assertTrue(tuf.formats.SIGNATURESTATUS_SCHEMA.matches(sig_status))\n\n    self.assertEqual(0, sig_status['threshold'])\n    self.assertEqual([], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    # A valid signable, but non-existent role argument.\n    self.assertRaises(tuf.exceptions.UnknownRoleError,\n      tuf.sig.get_signature_status, signable, 'unknown_role')\n\n    # Should verify we are not adding a duplicate signature\n    # when doing the following action.  Here we know 'signable'\n    # has only one signature so it's okay.\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n\n    # Improperly formatted role.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n      tuf.sig.get_signature_status, signable, 1)\n\n    # Not allowed to call verify() without having specified a role.\n    args = (signable, None)\n    self.assertRaises(securesystemslib.exceptions.Error, tuf.sig.verify, *args)\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n\n\n  def test_get_signature_status_bad_sig(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n    signable['signed'] += 'signature no longer matches signed data'\n\n    tuf.keydb.add_key(KEYS[0])\n    threshold = 1\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA, keyids=[KEYS[0]['keyid']], threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertEqual(1, sig_status['threshold'])\n    self.assertEqual([], sig_status['good_sigs'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    self.assertFalse(tuf.sig.verify(signable, 'Root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    # Remove the role.\n    tuf.roledb.remove_role('Root')\n\n\n  def test_get_signature_status_unknown_signing_scheme(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    valid_scheme = KEYS[0]['scheme']\n    KEYS[0]['scheme'] = 'unknown_signing_scheme'\n    tuf.keydb.add_key(KEYS[0])\n    threshold = 1\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA, keyids=[KEYS[0]['keyid']], threshold=threshold)\n\n    tuf.roledb.add_role('root', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'root')\n\n    self.assertEqual(1, sig_status['threshold'])\n    self.assertEqual([], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([KEYS[0]['keyid']],\n                    sig_status['unknown_signing_schemes'])\n\n    self.assertFalse(tuf.sig.verify(signable, 'root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    KEYS[0]['scheme'] = valid_scheme\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    # Remove the role.\n    tuf.roledb.remove_role('root')\n\n\n  def test_get_signature_status_single_key(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    threshold = 1\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA, keyids=[KEYS[0]['keyid']], threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n    tuf.keydb.add_key(KEYS[0])\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertEqual(1, sig_status['threshold'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    self.assertTrue(tuf.sig.verify(signable, 'Root'))\n\n    # Test for an unknown signature when 'role' is left unspecified.\n    sig_status = tuf.sig.get_signature_status(signable)\n\n    self.assertEqual(0, sig_status['threshold'])\n    self.assertEqual([], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    # Remove the role.\n    tuf.roledb.remove_role('Root')\n\n\n  def test_get_signature_status_below_threshold(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n    threshold = 2\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA,\n        keyids=[KEYS[0]['keyid'], KEYS[2]['keyid']],\n        threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertEqual(2, sig_status['threshold'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    self.assertFalse(tuf.sig.verify(signable, 'Root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n\n    # Remove the role.\n    tuf.roledb.remove_role('Root')\n\n\n  def test_get_signature_status_below_threshold_unrecognized_sigs(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    # Two keys sign it, but only one of them will be trusted.\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[2], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n    tuf.keydb.add_key(KEYS[1])\n    threshold = 2\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA,\n        keyids=[KEYS[0]['keyid'], KEYS[1]['keyid']],\n        threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertEqual(2, sig_status['threshold'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([KEYS[2]['keyid']], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    self.assertFalse(tuf.sig.verify(signable, 'Root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    tuf.keydb.remove_key(KEYS[1]['keyid'])\n\n    # Remove the role.\n    tuf.roledb.remove_role('Root')\n\n\n  def test_get_signature_status_below_threshold_unauthorized_sigs(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n    # Two keys sign it, but one of them is only trusted for a different\n    # role.\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[1], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n    tuf.keydb.add_key(KEYS[1])\n    threshold = 2\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA,\n        keyids=[KEYS[0]['keyid'], KEYS[2]['keyid']],\n        threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA,\n        keyids=[KEYS[1]['keyid'], KEYS[2]['keyid']],\n        threshold=threshold)\n\n    tuf.roledb.add_role('Release', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertEqual(2, sig_status['threshold'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([KEYS[1]['keyid']], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    self.assertFalse(tuf.sig.verify(signable, 'Root'))\n\n    self.assertRaises(tuf.exceptions.UnknownRoleError,\n                      tuf.sig.get_signature_status, signable, 'unknown_role')\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    tuf.keydb.remove_key(KEYS[1]['keyid'])\n\n    # Remove the roles.\n    tuf.roledb.remove_role('Root')\n    tuf.roledb.remove_role('Release')\n\n\n\n  def test_check_signatures_no_role(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n\n    # No specific role we're considering. It's invalid to use the\n    # function tuf.sig.verify() without a role specified because\n    # tuf.sig.verify() is checking trust, as well.\n    args = (signable, None)\n    self.assertRaises(securesystemslib.exceptions.Error, tuf.sig.verify, *args)\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n\n\n\n  def test_verify_single_key(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n    threshold = 1\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA, keyids=[KEYS[0]['keyid']], threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    # This will call verify() and return True if 'signable' is valid,\n    # False otherwise.\n    self.assertTrue(tuf.sig.verify(signable, 'Root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n\n    # Remove the roles.\n    tuf.roledb.remove_role('Root')\n\n\n  def test_verify_unrecognized_sig(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    # Two keys sign it, but only one of them will be trusted.\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[2], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n    tuf.keydb.add_key(KEYS[1])\n    threshold = 2\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA,\n        keyids=[KEYS[0]['keyid'], KEYS[1]['keyid']],\n        threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    self.assertFalse(tuf.sig.verify(signable, 'Root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    tuf.keydb.remove_key(KEYS[1]['keyid'])\n\n    # Remove the roles.\n    tuf.roledb.remove_role('Root')\n\n\n\n  def test_generate_rsa_signature(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    self.assertEqual(1, len(signable['signatures']))\n    signature = signable['signatures'][0]\n    self.assertEqual(KEYS[0]['keyid'], signature['keyid'])\n\n    returned_signature = tuf.sig.generate_rsa_signature(signable['signed'], KEYS[0])\n    self.assertTrue(securesystemslib.formats.SIGNATURE_SCHEMA.matches(returned_signature))\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[1], signed))\n\n    self.assertEqual(2, len(signable['signatures']))\n    signature = signable['signatures'][1]\n    self.assertEqual(KEYS[1]['keyid'], signature['keyid'])\n\n\n\n  def test_may_need_new_keys(self):\n    # One untrusted key in 'signable'.\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    tuf.keydb.add_key(KEYS[1])\n    threshold = 1\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA, keyids=[KEYS[1]['keyid']], threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertTrue(tuf.sig.may_need_new_keys(sig_status))\n\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[1]['keyid'])\n\n    # Remove the roles.\n    tuf.roledb.remove_role('Root')\n\n\n  def test_signable_has_invalid_format(self):\n    # get_signature_status() and verify() validate 'signable' before continuing.\n    # 'signable' must be of the form: {'signed': , 'signatures': [{}]}.\n    # Object types are checked as well.\n    signable = {'not_signed' : 'test', 'signatures' : []}\n    args = (signable['not_signed'], KEYS[0])\n    self.assertRaises(securesystemslib.exceptions.FormatError, tuf.sig.get_signature_status, *args)\n\n    # 'signatures' value must be a list.  Let's try a dict.\n    signable = {'signed' : 'test', 'signatures' : {}}\n    args = (signable['signed'], KEYS[0])\n    self.assertRaises(securesystemslib.exceptions.FormatError, tuf.sig.get_signature_status, *args)\n\n\n\n# Run unit test.\nif __name__ == '__main__':\n  unittest.main()\n", "#!/usr/bin/env python\n\n# Copyright 2012 - 2017, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\n<Program Name>\n  test_updater.py\n\n<Author>\n  Konstantin Andrianov.\n\n<Started>\n  October 15, 2012.\n\n  March 11, 2014.\n    Refactored to remove mocked modules and old repository tool dependence, use\n    exact repositories, and add realistic retrieval of files. -vladimir.v.diaz\n\n<Copyright>\n  See LICENSE-MIT OR LICENSE for licensing information.\n\n<Purpose>\n  'test_updater.py' provides a collection of methods that test the public /\n  non-public methods and functions of 'tuf.client.updater.py'.\n\n  The 'unittest_toolbox.py' module was created to provide additional testing\n  tools, such as automatically deleting temporary files created in test cases.\n  For more information, see 'tests/unittest_toolbox.py'.\n\n<Methodology>\n  Test cases here should follow a specific order (i.e., independent methods are\n  tested before dependent methods). More accurately, least dependent methods\n  are tested before most dependent methods.  There is no reason to rewrite or\n  construct other methods that replicate already-tested methods solely for\n  testing purposes.  This is possible because the 'unittest.TestCase' class\n  guarantees the order of unit tests.  The 'test_something_A' method would\n  be tested before 'test_something_B'.  To ensure the expected order of tests,\n  a number is placed after 'test' and before methods name like so:\n  'test_1_check_directory'.  The number is a measure of dependence, where 1 is\n  less dependent than 2.\n\"\"\"\n\n# Help with Python 3 compatibility, where the print statement is a function, an\n# implicit relative import is invalid, and the '/' operator performs true\n# division.  Example:  print 'hello world' raises a 'SyntaxError' exception.\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nimport os\nimport time\nimport shutil\nimport copy\nimport tempfile\nimport logging\nimport random\nimport subprocess\nimport sys\nimport errno\nimport unittest\n\nimport tuf\nimport tuf.exceptions\nimport tuf.log\nimport tuf.formats\nimport tuf.keydb\nimport tuf.roledb\nimport tuf.repository_tool as repo_tool\nimport tuf.repository_lib as repo_lib\nimport tuf.unittest_toolbox as unittest_toolbox\nimport tuf.client.updater as updater\n\nimport securesystemslib\nimport six\n\nlogger = logging.getLogger('tuf.test_updater')\nrepo_tool.disable_console_log_messages()\n\n\nclass TestUpdater(unittest_toolbox.Modified_TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    # setUpClass() is called before tests in an individual class are executed.\n\n    # Create a temporary directory to store the repository, metadata, and target\n    # files.  'temporary_directory' must be deleted in TearDownModule() so that\n    # temporary files are always removed, even when exceptions occur.\n    cls.temporary_directory = tempfile.mkdtemp(dir=os.getcwd())\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served\n    # by the SimpleHTTPServer launched here.  The test cases of 'test_updater.py'\n    # assume the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n    cls.SERVER_PORT = random.randint(30000, 45000)\n    command = ['python', '-m', 'tuf.scripts.simple_server', str(cls.SERVER_PORT)]\n    cls.server_process = subprocess.Popen(command, stderr=subprocess.PIPE)\n    logger.info('\\n\\tServer process started.')\n    logger.info('\\tServer process id: '+str(cls.server_process.pid))\n    logger.info('\\tServing on port: '+str(cls.SERVER_PORT))\n    cls.url = 'http://localhost:'+str(cls.SERVER_PORT) + os.path.sep\n\n    # NOTE: Following error is raised if a delay is not long enough to allow\n    # the server process to set up and start listening:\n    #     <urlopen error [Errno 111] Connection refused>\n    # or, on Windows:\n    #     Failed to establish a new connection: [Errno 111] Connection refused'\n    # While 0.3s has consistently worked on Travis and local builds, it led to\n    # occasional failures in AppVeyor builds, so increasing this to 2s, sadly.\n    time.sleep(2)\n\n\n\n  @classmethod\n  def tearDownClass(cls):\n    # tearDownModule() is called after all the tests have run.\n    # http://docs.python.org/2/library/unittest.html#class-and-module-fixtures\n\n    # Kill the SimpleHTTPServer process.\n    if cls.server_process.returncode is None:\n      logger.info('\\tServer process ' + str(cls.server_process.pid) + ' terminated.')\n      cls.server_process.kill()\n\n    # Remove the temporary repository directory, which should contain all the\n    # metadata, targets, and key files generated for the test cases.  sleep\n    # for a bit to allow the kill'd server process to terminate.\n    time.sleep(.3)\n    shutil.rmtree(cls.temporary_directory)\n\n\n\n  def setUp(self):\n    # We are inheriting from custom class.\n    unittest_toolbox.Modified_TestCase.setUp(self)\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    self.repository_name = 'test_repository1'\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf.tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n    temporary_repository_root = \\\n      self.make_temp_directory(directory=self.temporary_directory)\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_client = os.path.join(original_repository_files, 'client')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = \\\n      os.path.join(temporary_repository_root, 'repository')\n    self.keystore_directory = \\\n      os.path.join(temporary_repository_root, 'keystore')\n\n    self.client_directory = os.path.join(temporary_repository_root,\n        'client')\n    self.client_metadata = os.path.join(self.client_directory,\n        self.repository_name, 'metadata')\n    self.client_metadata_current = os.path.join(self.client_metadata,\n        'current')\n    self.client_metadata_previous = os.path.join(self.client_metadata,\n        'previous')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = \\\n      'http://localhost:' + str(self.SERVER_PORT) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n                                           'metadata_path': 'metadata',\n                                           'targets_path': 'targets',\n                                           'confined_target_dirs': ['']}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n                                              self.repository_mirrors)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n    # We are inheriting from custom class.\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n\n\n\n  # UNIT TESTS.\n\n  def test_1__init__exceptions(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, 8,\n                      self.repository_mirrors)\n\n    # Invalid 'repository_mirrors' argument.  'tuf.formats.MIRRORDICT_SCHEMA'\n    # expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, updater.Updater, 8)\n\n\n    # 'tuf.client.updater.py' requires that the client's repositories directory\n    # be configured in 'tuf.settings.py'.\n    tuf.settings.repositories_directory = None\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n\n    # Test: empty client repository (i.e., no metadata directory).\n    metadata_backup = self.client_metadata + '.backup'\n    shutil.move(self.client_metadata, metadata_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's metadata directory.\n    shutil.move(metadata_backup, self.client_metadata)\n\n\n    # Test: repository with only a '{repository_directory}/metadata' directory.\n    # (i.e., missing the required 'current' and 'previous' sub-directories).\n    current_backup = self.client_metadata_current + '.backup'\n    previous_backup = self.client_metadata_previous + '.backup'\n\n    shutil.move(self.client_metadata_current, current_backup)\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n\n    # Restore the client's previous directory.  The required 'current' directory\n    # is still missing.\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test: repository with only a '{repository_directory}/metadata/previous'\n    # directory.\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's current directory.\n    shutil.move(current_backup, self.client_metadata_current)\n\n    # Test: repository with a '{repository_directory}/metadata/current'\n    # directory, but the 'previous' directory is missing.\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test:  repository missing the required 'root.json' file.\n    client_root_file = os.path.join(self.client_metadata_current, 'root.json')\n    backup_root_file = client_root_file + '.backup'\n    shutil.move(client_root_file, backup_root_file)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's 'root.json file.\n    shutil.move(backup_root_file, client_root_file)\n\n    # Test: Normal 'tuf.client.updater.Updater' instantiation.\n    updater.Updater('test_repository1', self.repository_mirrors)\n\n\n\n\n\n  def test_1__load_metadata_from_file(self):\n\n    # Setup\n    # Get the 'role1.json' filepath.  Manually load the role metadata, and\n    # compare it against the loaded metadata by '_load_metadata_from_file()'.\n    role1_filepath = \\\n      os.path.join(self.client_metadata_current, 'role1.json')\n    role1_meta = securesystemslib.util.load_json_file(role1_filepath)\n\n    # Load the 'role1.json' file with _load_metadata_from_file, which should\n    # store the loaded metadata in the 'self.repository_updater.metadata'\n    # store.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n\n    # Verify that the correct number of metadata objects has been loaded\n    # (i.e., only the 'root.json' file should have been loaded.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Verify that the content of root metadata is valid.\n    self.assertEqual(self.repository_updater.metadata['current']['role1'],\n                     role1_meta['signed'])\n\n    # Verify that _load_metadata_from_file() doesn't raise an exception for\n    # improperly formatted metadata, and doesn't load the bad file.\n    with open(role1_filepath, 'ab') as file_object:\n      file_object.write(b'bad JSON data')\n\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Test if we fail gracefully if we can't deserialize a meta file\n    self.repository_updater._load_metadata_from_file('current', 'empty_file')\n    self.assertFalse('empty_file' in self.repository_updater.metadata['current'])\n\n    # Test invalid metadata set argument (must be either\n    # 'current' or 'previous'.)\n    self.assertRaises(securesystemslib.exceptions.Error,\n                      self.repository_updater._load_metadata_from_file,\n                      'bad_metadata_set', 'role1')\n\n\n\n\n  def test_1__rebuild_key_and_role_db(self):\n    # Setup\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_threshold = root_metadata['roles']['root']['threshold']\n    number_of_root_keys = len(root_metadata['keys'])\n\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # Ensure we add 2 to the number of root keys (actually, the number of root\n    # keys multiplied by the number of keyid hash algorithms), to include the\n    # delegated targets key (+1 for its sha512 keyid).  The delegated roles of\n    # 'targets.json' are also loaded when the repository object is\n    # instantiated.\n\n    self.assertEqual(number_of_root_keys * 2 + 2, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: normal case.\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # _rebuild_key_and_role_db() will only rebuild the keys and roles specified\n    # in the 'root.json' file, unlike __init__().  Instantiating an updater\n    # object calls both _rebuild_key_and_role_db() and _import_delegations().\n    self.assertEqual(number_of_root_keys * 2, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: properly updated roledb and keydb dicts if the Root role changes.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_metadata['roles']['root']['threshold'] = 8\n    root_metadata['keys'].popitem()\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], 8)\n    self.assertEqual(number_of_root_keys * 2 - 2, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n\n\n\n  def test_1__update_versioninfo(self):\n    # Tests\n    # Verify that the 'self.versioninfo' dictionary is empty (it starts off\n    # empty and is only populated if _update_versioninfo() is called.\n    versioninfo_dict = self.repository_updater.versioninfo\n    self.assertEqual(len(versioninfo_dict), 0)\n\n    # Load the versioninfo of the top-level Targets role.  This action\n    # populates the 'self.versioninfo' dictionary.\n    self.repository_updater._update_versioninfo('targets.json')\n    self.assertEqual(len(versioninfo_dict), 1)\n    self.assertTrue(tuf.formats.FILEINFODICT_SCHEMA.matches(versioninfo_dict))\n\n    # The Snapshot role stores the version numbers of all the roles available\n    # on the repository.  Load Snapshot to extract Root's version number\n    # and compare it against the one loaded by 'self.repository_updater'.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    # Verify that the manually loaded version number of root.json matches\n    # the one loaded by the updater object.\n    self.assertTrue('targets.json' in versioninfo_dict)\n    self.assertEqual(versioninfo_dict['targets.json'], targets_versioninfo)\n\n    # Verify that 'self.versioninfo' is incremented if another role is updated.\n    self.repository_updater._update_versioninfo('role1.json')\n    self.assertEqual(len(versioninfo_dict), 2)\n\n    # Verify that 'self.versioninfo' is incremented if a non-existent role is\n    # requested, and has its versioninfo entry set to 'None'.\n    self.repository_updater._update_versioninfo('bad_role.json')\n    self.assertEqual(len(versioninfo_dict), 3)\n    self.assertEqual(versioninfo_dict['bad_role.json'], None)\n\n    # Verify that the versioninfo specified in Timestamp is used if the Snapshot\n    # role hasn't been downloaded yet.\n    del self.repository_updater.metadata['current']['snapshot']\n    #self.assertRaises(self.repository_updater._update_versioninfo('snapshot.json'))\n    self.repository_updater._update_versioninfo('snapshot.json')\n    self.assertEqual(versioninfo_dict['snapshot.json']['version'], 1)\n\n\n\n\n  def test_1__update_fileinfo(self):\n      # Tests\n      # Verify that the 'self.fileinfo' dictionary is empty (its starts off empty\n      # and is only populated if _update_fileinfo() is called.\n      fileinfo_dict = self.repository_updater.fileinfo\n      self.assertEqual(len(fileinfo_dict), 0)\n\n      # Load the fileinfo of the top-level root role.  This populates the\n      # 'self.fileinfo' dictionary.\n      self.repository_updater._update_fileinfo('root.json')\n      self.assertEqual(len(fileinfo_dict), 1)\n      self.assertTrue(tuf.formats.FILEDICT_SCHEMA.matches(fileinfo_dict))\n      root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n      length, hashes = securesystemslib.util.get_file_details(root_filepath)\n      root_fileinfo = tuf.formats.make_fileinfo(length, hashes)\n      self.assertTrue('root.json' in fileinfo_dict)\n      self.assertEqual(fileinfo_dict['root.json'], root_fileinfo)\n\n      # Verify that 'self.fileinfo' is incremented if another role is updated.\n      self.repository_updater._update_fileinfo('targets.json')\n      self.assertEqual(len(fileinfo_dict), 2)\n\n      # Verify that 'self.fileinfo' is inremented if a non-existent role is\n      # requested, and has its fileinfo entry set to 'None'.\n      self.repository_updater._update_fileinfo('bad_role.json')\n      self.assertEqual(len(fileinfo_dict), 3)\n      self.assertEqual(fileinfo_dict['bad_role.json'], None)\n\n\n\n\n  def test_2__fileinfo_has_changed(self):\n      #  Verify that the method returns 'False' if file info was not changed.\n      root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n      length, hashes = securesystemslib.util.get_file_details(root_filepath)\n      root_fileinfo = tuf.formats.make_fileinfo(length, hashes)\n      self.assertFalse(self.repository_updater._fileinfo_has_changed('root.json',\n                                                             root_fileinfo))\n\n      # Verify that the method returns 'True' if length or hashes were changed.\n      new_length = 8\n      new_root_fileinfo = tuf.formats.make_fileinfo(new_length, hashes)\n      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n                                                             new_root_fileinfo))\n      # Hashes were changed.\n      new_hashes = {'sha256': self.random_string()}\n      new_root_fileinfo = tuf.formats.make_fileinfo(length, new_hashes)\n      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n                                                             new_root_fileinfo))\n\n      # Verify that _fileinfo_has_changed() returns True if no fileinfo (or set\n      # to None) exists for some role.\n      self.assertTrue(self.repository_updater._fileinfo_has_changed('bad.json',\n          new_root_fileinfo))\n\n      saved_fileinfo = self.repository_updater.fileinfo['root.json']\n      self.repository_updater.fileinfo['root.json'] = None\n      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n          new_root_fileinfo))\n\n\n      self.repository_updater.fileinfo['root.json'] = saved_fileinfo\n      new_root_fileinfo['hashes']['sha666'] = '666'\n      self.repository_updater._fileinfo_has_changed('root.json',\n          new_root_fileinfo)\n\n\n\n  def test_2__import_delegations(self):\n    # Setup.\n    # In order to test '_import_delegations' the parent of the delegation\n    # has to be in Repository.metadata['current'], but it has to be inserted\n    # there without using '_load_metadata_from_file()' since it calls\n    # '_import_delegations()'.\n    repository_name = self.repository_updater.repository_name\n    tuf.keydb.clear_keydb(repository_name)\n    tuf.roledb.clear_roledb(repository_name)\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 0)\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 0)\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n\n    # Take into account the number of keyids algorithms supported by default,\n    # which this test condition expects to be two (sha256 and sha512).\n    self.assertEqual(4 * 2, len(tuf.keydb._keydb_dict[repository_name]))\n\n    # Test: pass a role without delegations.\n    self.repository_updater._import_delegations('root')\n\n    # Verify that there was no change to the roledb and keydb dictionaries by\n    # checking the number of elements in the dictionaries.\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n    # Take into account the number of keyid hash algorithms, which this\n    # test condition expects to be two (for sha256 and sha512).\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4 * 2)\n\n    # Test: normal case, first level delegation.\n    self.repository_updater._import_delegations('targets')\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 5)\n    # The number of root keys (times the number of key hash algorithms) +\n    # delegation's key (+1 for its sha512 keyid).\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4 * 2 + 2)\n\n    # Verify that roledb dictionary was added.\n    self.assertTrue('role1' in tuf.roledb._roledb_dict[repository_name])\n\n    # Verify that keydb dictionary was updated.\n    role1_signable = \\\n      securesystemslib.util.load_json_file(os.path.join(self.client_metadata_current,\n                                           'role1.json'))\n    keyids = []\n    for signature in role1_signable['signatures']:\n      keyids.append(signature['keyid'])\n\n    for keyid in keyids:\n      self.assertTrue(keyid in tuf.keydb._keydb_dict[repository_name])\n\n    # Verify that _import_delegations() ignores invalid keytypes in the 'keys'\n    # field of parent role's 'delegations'.\n    existing_keyid = keyids[0]\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'bad_keytype'\n    self.repository_updater._import_delegations('targets')\n\n    # Restore the keytype of 'existing_keyid'.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'ed25519'\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated keys is malformed.\n    valid_keyval = self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval']\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = valid_keyval\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated roles is malformed.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['roles'][0]['name'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n\n\n  def test_2__versioninfo_has_been_updated(self):\n    # Verify that the method returns 'False' if a versioninfo was not changed.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    self.assertFalse(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n    # Verify that the method returns 'True' if Root's version number changes.\n    targets_versioninfo['version'] = 8\n    self.assertTrue(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n\n\n\n\n  def test_2__move_current_to_previous(self):\n    # Test case will consist of removing a metadata file from client's\n    # '{client_repository}/metadata/previous' directory, executing the method\n    # and then verifying that the 'previous' directory contains the snapshot\n    # file.\n    previous_snapshot_filepath = os.path.join(self.client_metadata_previous,\n                                              'snapshot.json')\n    os.remove(previous_snapshot_filepath)\n    self.assertFalse(os.path.exists(previous_snapshot_filepath))\n\n    # Verify that the current 'snapshot.json' is moved to the previous directory.\n    self.repository_updater._move_current_to_previous('snapshot')\n    self.assertTrue(os.path.exists(previous_snapshot_filepath))\n\n\n\n\n\n  def test_2__delete_metadata(self):\n    # This test will verify that 'root' metadata is never deleted.  When a role\n    # is deleted verify that the file is not present in the\n    # 'self.repository_updater.metadata' dictionary.\n    self.repository_updater._delete_metadata('root')\n    self.assertTrue('root' in self.repository_updater.metadata['current'])\n\n    self.repository_updater._delete_metadata('timestamp')\n    self.assertFalse('timestamp' in self.repository_updater.metadata['current'])\n\n\n\n\n\n  def test_2__ensure_not_expired(self):\n    # This test condition will verify that nothing is raised when a metadata\n    # file has a future expiration date.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    self.repository_updater._ensure_not_expired(root_metadata, 'root')\n\n    # 'tuf.exceptions.ExpiredMetadataError' should be raised in this next test condition,\n    # because the expiration_date has expired by 10 seconds.\n    expires = tuf.formats.unix_timestamp_to_datetime(int(time.time() - 10))\n    expires = expires.isoformat() + 'Z'\n    root_metadata['expires'] = expires\n\n    # Ensure the 'expires' value of the root file is valid by checking the\n    # the formats of the 'root.json' object.\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater._ensure_not_expired,\n                      root_metadata, 'root')\n\n\n\n\n\n  def test_3__update_metadata(self):\n    # Setup\n    # _update_metadata() downloads, verifies, and installs the specified\n    # metadata role.  Remove knowledge of currently installed metadata and\n    # verify that they are re-installed after calling _update_metadata().\n\n    # This is the default metadata that we would create for the timestamp role,\n    # because it has no signed metadata for itself.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    # This is the upper bound length for Targets metadata.\n    DEFAULT_TARGETS_FILELENGTH = tuf.settings.DEFAULT_TARGETS_REQUIRED_LENGTH\n\n    # Save the versioninfo of 'targets.json,' needed later when re-installing\n    # with _update_metadata().\n    targets_versioninfo = \\\n      self.repository_updater.metadata['current']['snapshot']['meta']\\\n                                      ['targets.json']\n\n    # Remove the currently installed metadata from the store and disk.  Verify\n    # that the metadata dictionary is re-populated after calling\n    # _update_metadata().\n    del self.repository_updater.metadata['current']['timestamp']\n    del self.repository_updater.metadata['current']['targets']\n\n    timestamp_filepath = \\\n      os.path.join(self.client_metadata_current, 'timestamp.json')\n    targets_filepath = os.path.join(self.client_metadata_current, 'targets.json')\n    root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n    os.remove(timestamp_filepath)\n    os.remove(targets_filepath)\n\n    # Test: normal case.\n    # Verify 'timestamp.json' is properly installed.\n    self.assertFalse('timestamp' in self.repository_updater.metadata)\n\n    logger.info('\\nroleinfo: ' + repr(tuf.roledb.get_rolenames(self.repository_name)))\n    self.repository_updater._update_metadata('timestamp',\n                                             DEFAULT_TIMESTAMP_FILELENGTH)\n    self.assertTrue('timestamp' in self.repository_updater.metadata['current'])\n    os.path.exists(timestamp_filepath)\n\n    # Verify 'targets.json' is properly installed.\n    self.assertFalse('targets' in self.repository_updater.metadata['current'])\n    self.repository_updater._update_metadata('targets',\n                                DEFAULT_TARGETS_FILELENGTH,\n                                targets_versioninfo['version'])\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n\n    targets_signable = securesystemslib.util.load_json_file(targets_filepath)\n    loaded_targets_version = targets_signable['signed']['version']\n    self.assertEqual(targets_versioninfo['version'], loaded_targets_version)\n\n    # Test: Invalid / untrusted version numbers.\n    # Invalid version number for 'targets.json'.\n    self.assertRaises(tuf.exceptions.NoWorkingMirrorError,\n        self.repository_updater._update_metadata,\n        'targets', DEFAULT_TARGETS_FILELENGTH, 88)\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH, 88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in six.itervalues(e.mirror_errors):\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.  The version number is checked, so the specific error in\n    # this case should be 'tuf.exceptions.BadVersionNumberError'.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH,\n                                               88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in six.itervalues(e.mirror_errors):\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n\n\n\n\n  def test_3__get_metadata_file(self):\n\n    '''\n    This test focuses on making sure that the updater rejects unknown or\n    badly-formatted TUF specification version numbers....\n    '''\n\n    # Make note of the correct supported TUF specification version.\n    correct_specification_version = tuf.SPECIFICATION_VERSION\n\n    # Change it long enough to write new metadata.\n    tuf.SPECIFICATION_VERSION = '0.9.0'\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # Change the supported TUF specification version back to what it should be\n    # so that we can parse the metadata and see that the spec version in the\n    # metadata does not match the code's expected spec version.\n    tuf.SPECIFICATION_VERSION = correct_specification_version\n\n    upperbound_filelength = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n    try:\n      self.repository_updater._get_metadata_file('timestamp', 'timestamp.json',\n      upperbound_filelength, 1)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      # Note that this test provides a piece of metadata which would fail to\n      # be accepted -- with a different error -- if the specification version\n      # number were not a problem.\n      for mirror_error in six.itervalues(e.mirror_errors):\n        assert isinstance(\n            mirror_error, tuf.exceptions.UnsupportedSpecificationError)\n\n    else:\n      self.fail(\n          'Expected a failure to verify metadata when the metadata had a '\n          'specification version number that was unexpected.  '\n          'No error was raised.')\n\n\n\n\n\n  def test_3__update_metadata_if_changed(self):\n    # Setup.\n    # The client repository is initially loaded with only four top-level roles.\n    # Verify that the metadata store contains the metadata of only these four\n    # roles before updating the metadata of 'targets.json'.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Test: normal case.  Update 'targets.json'.  The version number should not\n    # change.\n    self.repository_updater._update_metadata_if_changed('targets')\n\n    # Verify the current version of 'targets.json' has not changed.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = os.path.join(self.repository_directory, 'targets', 'file3.txt')\n\n    repository.targets.add_target(target3)\n    repository.root.version = repository.root.version + 1\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update 'targets.json' and verify that the client's current 'targets.json'\n    # has been updated.  'timestamp' and 'snapshot' must be manually updated\n    # so that new 'targets' can be recognized.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    self.repository_updater._update_metadata('timestamp', DEFAULT_TIMESTAMP_FILELENGTH)\n    self.repository_updater._update_metadata_if_changed('snapshot', 'timestamp')\n    self.repository_updater._update_metadata_if_changed('targets')\n    self.repository_updater._update_metadata_if_changed('root')\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertTrue(self.repository_updater.metadata['current']['targets'])\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 2)\n\n    # Test for an invalid 'referenced_metadata' argument.\n    self.assertRaises(tuf.exceptions.RepositoryError,\n        self.repository_updater._update_metadata_if_changed, 'snapshot', 'bad_role')\n\n\n\n  def test_3__targets_of_role(self):\n    # Setup.\n    # Extract the list of targets from 'targets.json', to be compared to what\n    # is returned by _targets_of_role('targets').\n    targets_in_metadata = \\\n      self.repository_updater.metadata['current']['targets']['targets']\n\n    # Test: normal case.\n    targetinfos_list = self.repository_updater._targets_of_role('targets')\n\n    # Verify that the list of targets was returned, and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos_list))\n    for targetinfo in targetinfos_list:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in six.iteritems(targets_in_metadata))\n\n\n\n\n\n  def test_4_refresh(self):\n    # This unit test is based on adding an extra target file to the\n    # server and rebuilding all server-side metadata.  All top-level metadata\n    # should be updated when the client calls refresh().\n\n    # First verify that an expired root metadata is updated.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.repository_updater.refresh()\n\n    # Second, verify that expired root metadata is not updated if\n    # 'unsafely_update_root_if_necessary' is explicitly set to 'False'.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater.refresh,\n                      unsafely_update_root_if_necessary=False)\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = os.path.join(self.repository_directory, 'targets', 'file3.txt')\n\n    repository.targets.add_target(target3)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Reference 'self.Repository.metadata['current']['targets']'.  Ensure\n    # 'target3' is not already specified.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    self.assertFalse(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the roles to be modified.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 1)\n\n    # Test: normal case.  'targes.json' should now specify 'target3', and the\n    # following top-level metadata should have also been updated:\n    # 'snapshot.json' and 'timestamp.json'.\n    self.repository_updater.refresh()\n\n    # Verify that the client's metadata was updated.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    targets_directory = os.path.join(self.repository_directory, 'targets')\n    target3 = target3[len(targets_directory) + 1:]\n    self.assertTrue(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the updated roles.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 2)\n\n\n\n\n\n  def test_4__refresh_targets_metadata(self):\n    # Setup.\n    # It is assumed that the client repository has only loaded the top-level\n    # metadata.  Refresh the 'targets.json' metadata, including all delegated\n    # roles (i.e., the client should add the missing 'role1.json' metadata.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n\n    # Test: normal case.\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n    # Verify that client's metadata files were refreshed successfully.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 6)\n\n    # Test for non-existing rolename.\n    self.repository_updater._refresh_targets_metadata('bad_rolename',\n        refresh_all_delegated_roles=False)\n\n    # Test that non-json metadata in Snapshot is ignored.\n    self.repository_updater.metadata['current']['snapshot']['meta']['bad_role.xml'] = {}\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n\n\n  def test_5_all_targets(self):\n   # Setup\n   # As with '_refresh_targets_metadata()',\n\n   # Update top-level metadata before calling one of the \"targets\" methods, as\n   # recommended by 'updater.py'.\n   self.repository_updater.refresh()\n\n   # Test: normal case.\n   all_targets = self.repository_updater.all_targets()\n\n   # Verify format of 'all_targets', it should correspond to\n   # 'TARGETINFOS_SCHEMA'.\n   self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(all_targets))\n\n   # Verify that there is a correct number of records in 'all_targets' list,\n   # and the expected filepaths specified in the metadata.  On the targets\n   # directory of the repository, there should be 3 target files (2 of\n   # which are specified by 'targets.json'.)  The delegated role 'role1'\n   # specifies 1 target file.  The expected total number targets in\n   # 'all_targets' should be 3.\n   self.assertEqual(len(all_targets), 3)\n\n   target_filepaths = []\n   for target in all_targets:\n    target_filepaths.append(target['filepath'])\n\n   self.assertTrue('file1.txt' in target_filepaths)\n   self.assertTrue('file2.txt' in target_filepaths)\n   self.assertTrue('file3.txt' in target_filepaths)\n\n\n\n\n\n  def test_5_targets_of_role(self):\n    # Setup\n    # Remove knowledge of 'targets.json' from the metadata store.\n    self.repository_updater.metadata['current']['targets']\n\n    # Remove the metadata of the delegated roles.\n    #shutil.rmtree(os.path.join(self.client_metadata, 'targets'))\n    os.remove(os.path.join(self.client_metadata_current, 'targets.json'))\n\n    # Extract the target files specified by the delegated role, 'role1.json',\n    # as available on the server-side version of the role.\n    role1_filepath = os.path.join(self.repository_directory, 'metadata',\n                                  'role1.json')\n    role1_signable = securesystemslib.util.load_json_file(role1_filepath)\n    expected_targets = role1_signable['signed']['targets']\n\n\n    # Test: normal case.\n    targetinfos = self.repository_updater.targets_of_role('role1')\n\n    # Verify that the expected role files were downloaded and installed.\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets.json'))\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets',\n                   'role1.json'))\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    self.assertTrue('role1' in self.repository_updater.metadata['current'])\n\n    #  Verify that list of targets was returned and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos))\n    for targetinfo in targetinfos:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in six.iteritems(expected_targets))\n\n    # Test: Invalid arguments.\n    # targets_of_role() expected a string rolename.\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater.targets_of_role,\n                      8)\n    self.assertRaises(tuf.exceptions.UnknownRoleError, self.repository_updater.targets_of_role,\n                      'unknown_rolename')\n\n\n\n\n\n  def test_6_get_one_valid_targetinfo(self):\n    # Setup\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n    SERVER_PORT = random.randint(30000, 45000)\n    command = ['python', '-m', 'tuf.scripts.simple_server', str(SERVER_PORT)]\n    server_process = subprocess.Popen(command, stderr=subprocess.PIPE)\n\n    # NOTE: Following error is raised if a delay is not long enough:\n    # <urlopen error [Errno 111] Connection refused>\n    # or, on Windows:\n    # Failed to establish a new connection: [Errno 111] Connection refused'\n    # While 0.3s has consistently worked on Travis and local builds, it led to\n    # occasional failures in AppVeyor builds, so increasing this to 2s, sadly.\n    time.sleep(2)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = \\\n      'http://localhost:' + str(SERVER_PORT) + repository_basepath\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets',\n        'confined_target_dirs': ['']}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Extract the file information of the targets specified in 'targets.json'.\n    self.repository_updater.refresh()\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n\n    target_files = targets_metadata['targets']\n    # Extract random target from 'target_files', which will be compared to what\n    # is returned by get_one_valid_targetinfo().  Restore the popped target\n    # (dict value stored in the metadata store) so that it can be found later.\n    filepath, fileinfo = target_files.popitem()\n    target_files[filepath] = fileinfo\n\n    target_targetinfo = self.repository_updater.get_one_valid_targetinfo(filepath)\n    self.assertTrue(tuf.formats.TARGETINFO_SCHEMA.matches(target_targetinfo))\n    self.assertEqual(target_targetinfo['filepath'], filepath)\n    self.assertEqual(target_targetinfo['fileinfo'], fileinfo)\n\n    # Test: invalid target path.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        self.repository_updater.get_one_valid_targetinfo,\n        self.random_path().lstrip(os.sep).lstrip('/'))\n\n    # Test updater.get_one_valid_targetinfo() backtracking behavior (enabled by\n    # default.)\n    targets_directory = os.path.join(self.repository_directory, 'targets')\n    foo_directory = os.path.join(targets_directory, 'foo')\n    foo_pattern = 'foo/foo*.tar.gz'\n    os.makedirs(foo_directory)\n\n    foo_package = os.path.join(foo_directory, 'foo1.1.tar.gz')\n    with open(foo_package, 'wb') as file_object:\n      file_object.write(b'new release')\n\n    # Modify delegations on the remote repository to test backtracking behavior.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n    repository.targets('role4').add_target(foo_package)\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # updater.get_one_valid_targetinfo() should find 'foo1.1.tar.gz' by\n    # backtracking to 'role3'.  'role2' allows backtracking.\n    self.repository_updater.refresh()\n    self.repository_updater.get_one_valid_targetinfo('foo/foo1.1.tar.gz')\n\n    # A leading path separator is disallowed.\n    self.assertRaises(tuf.exceptions.FormatError,\n    self.repository_updater.get_one_valid_targetinfo, '/foo/foo1.1.tar.gz')\n\n    # Test when 'role2' does *not* allow backtracking.  If 'foo/foo1.1.tar.gz'\n    # is not provided by the authoritative 'role2',\n    # updater.get_one_valid_targetinfo() should return a\n    # 'tuf.exceptions.UnknownTargetError' exception.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    repository.targets.revoke('role3')\n    repository.targets.revoke('role4')\n\n    # Ensure we delegate in trusted order (i.e., 'role2' has higher priority.)\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern], terminating=True, list_of_targets=[])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Verify that 'tuf.exceptions.UnknownTargetError' is raised by\n    # updater.get_one_valid_targetinfo().\n    self.repository_updater.refresh()\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n                      self.repository_updater.get_one_valid_targetinfo,\n                      'foo/foo1.1.tar.gz')\n\n    # Verify that a 'tuf.exceptions.FormatError' is raised for delegated paths\n    # that contain a leading path separator.\n    self.assertRaises(tuf.exceptions.FormatError,\n        self.repository_updater.get_one_valid_targetinfo,\n        '/foo/foo1.1.tar.gz')\n\n    server_process.kill()\n\n\n\n\n  def test_6_download_target(self):\n    # Create temporary directory (destination directory of downloaded targets)\n    # that will be passed as an argument to 'download_target()'.\n    destination_directory = self.make_temp_directory()\n    target_filepaths = \\\n      list(self.repository_updater.metadata['current']['targets']['targets'].keys())\n\n    # Test: normal case.\n    # Get the target info, which is an argument to 'download_target()'.\n\n    # 'target_filepaths' is expected to have at least two targets.  The first\n    # target will be used to test against download_target().  The second\n    # will be used to test against download_target() and a repository with\n    # 'consistent_snapshot' set to True.\n    target_filepath1 = target_filepaths.pop()\n    targetinfo = self.repository_updater.get_one_valid_targetinfo(target_filepath1)\n    self.repository_updater.download_target(targetinfo,\n                                            destination_directory)\n\n    download_filepath = \\\n      os.path.join(destination_directory, target_filepath1.lstrip('/'))\n    self.assertTrue(os.path.exists(download_filepath))\n    length, hashes = \\\n      securesystemslib.util.get_file_details(download_filepath,\n        securesystemslib.settings.HASH_ALGORITHMS)\n    download_targetfileinfo = tuf.formats.make_fileinfo(length, hashes)\n\n    # Add any 'custom' data from the repository's target fileinfo to the\n    # 'download_targetfileinfo' object being tested.\n    if 'custom' in targetinfo['fileinfo']:\n      download_targetfileinfo['custom'] = targetinfo['fileinfo']['custom']\n\n    self.assertEqual(targetinfo['fileinfo'], download_targetfileinfo)\n\n    # Test when consistent snapshots is set.  First, create a valid\n    # repository with consistent snapshots set (root.json contains a\n    # \"consistent_snapshot\" entry that the updater uses to correctly fetch\n    # snapshots.  The updater expects the existence of\n    # '<version_number>.filename' files if root.json sets 'consistent_snapshot\n    # = True'.\n\n    # The repository must be rewritten with 'consistent_snapshot' set.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    # Write metadata for all the top-level roles , since consistent snapshot\n    # is now being set to true (i.e., the pre-generated repository isn't set\n    # to support consistent snapshots.  A new version of targets.json is needed\n    # to ensure <digest>.filename target files are written to disk.\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n\n    repository.writeall(consistent_snapshot=True)\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # And ensure the client has the latest top-level metadata.\n    self.repository_updater.refresh()\n\n    target_filepath2 = target_filepaths.pop()\n    targetinfo2 = self.repository_updater.get_one_valid_targetinfo(target_filepath2)\n    self.repository_updater.download_target(targetinfo2,\n                                            destination_directory)\n\n    # Test for a destination that cannot be written to (apart from a target\n    # file that already exists at the destination) and which raises an\n    # exception.\n    bad_destination_directory = 'bad' * 2000\n\n    try:\n      self.repository_updater.download_target(targetinfo, bad_destination_directory)\n\n    except OSError as e:\n      self.assertTrue(e.errno == errno.ENAMETOOLONG or e.errno == errno.ENOENT)\n\n    else:\n      self.fail('Expected an OSError of type ENAMETOOLONG or ENOENT')\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      targetinfo, 8)\n\n    # Test:\n    # Attempt a file download of a valid target, however, a download exception\n    # occurs because the target is not within the mirror's confined target\n    # directories.  Adjust mirrors dictionary, so that 'confined_target_dirs'\n    # field contains at least one confined target and excludes needed target\n    # file.\n    mirrors = self.repository_updater.mirrors\n    for mirror_name, mirror_info in six.iteritems(mirrors):\n      mirrors[mirror_name]['confined_target_dirs'] = [self.random_path()]\n\n    try:\n      self.repository_updater.download_target(targetinfo,\n                                              destination_directory)\n\n    except tuf.exceptions.NoWorkingMirrorError as exception:\n      # Ensure that no mirrors were found due to mismatch in confined target\n      # directories.  get_list_of_mirrors() returns an empty list in this case,\n      # which does not generate specific exception errors.\n      self.assertEqual(len(exception.mirror_errors), 0)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError with zero mirror errors in it.')\n\n\n\n\n\n  def test_7_updated_targets(self):\n    # Verify that the list of targets returned by updated_targets() contains\n    # all the files that need to be updated, these files include modified and\n    # new target files.  Also, confirm that files that need not to be updated\n    # are absent from the list.\n    # Setup\n\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n    SERVER_PORT = random.randint(30000, 45000)\n    command = ['python', '-m', 'tuf.scripts.simple_server', str(SERVER_PORT)]\n    server_process = subprocess.Popen(command, stderr=subprocess.PIPE)\n\n    # NOTE: Following error is raised if a delay is not long enough to allow\n    # the server process to set up and start listening:\n    #     <urlopen error [Errno 111] Connection refused>\n    # or, on Windows:\n    #     Failed to establish a new connection: [Errno 111] Connection refused'\n    # While 0.3s has consistently worked on Travis and local builds, it led to\n    # occasional failures in AppVeyor builds, so increasing this to 2s, sadly.\n    time.sleep(2)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = \\\n      'http://localhost:' + str(SERVER_PORT) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets',\n        'confined_target_dirs': ['']}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory which will hold client's target files.\n    destination_directory = self.make_temp_directory()\n\n    # Get the list of target files.  It will be used as an argument to the\n    # 'updated_targets()' function.\n    all_targets = self.repository_updater.all_targets()\n\n    # Test for duplicates and targets in the root directory of the repository.\n    additional_target = all_targets[0].copy()\n    all_targets.append(additional_target)\n    additional_target_in_root_directory = additional_target.copy()\n    additional_target_in_root_directory['filepath'] = 'file1.txt'\n    all_targets.append(additional_target_in_root_directory)\n\n    #  At this point client needs to update and download all targets.\n    # Test: normal cases.\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    all_targets = self.repository_updater.all_targets()\n\n    # Assumed the pre-generated repository specifies two target files in\n    # 'targets.json' and one delegated target file in 'role1.json'.\n    self.assertEqual(len(updated_targets), 3)\n\n    # Test: download one of the targets.\n    download_target = copy.deepcopy(updated_targets).pop()\n    self.repository_updater.download_target(download_target,\n                                            destination_directory)\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 2)\n\n    # Test: download all the targets.\n    for download_target in all_targets:\n      self.repository_updater.download_target(download_target,\n                                              destination_directory)\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 0)\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      all_targets, 8)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(target1)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(target1)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Ensure the client has up-to-date metadata.\n    self.repository_updater.refresh()\n\n    # Verify that the new target file is considered updated.\n    all_targets = self.repository_updater.all_targets()\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n    self.assertEqual(len(updated_targets), 1)\n\n    server_process.kill()\n\n\n\n\n  def test_8_remove_obsolete_targets(self):\n    # Setup.\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n    SERVER_PORT = random.randint(30000, 45000)\n    command = ['python', '-m', 'tuf.scripts.simple_server', str(SERVER_PORT)]\n    server_process = subprocess.Popen(command, stderr=subprocess.PIPE)\n\n    # NOTE: Following error is raised if a delay is not long enough to allow\n    # the server process to set up and start listening:\n    #     <urlopen error [Errno 111] Connection refused>\n    # or, on Windows:\n    #     Failed to establish a new connection: [Errno 111] Connection refused'\n    # While 0.3s has consistently worked on Travis and local builds, it led to\n    # occasional failures in AppVeyor builds, so increasing this to 2s, sadly.\n    time.sleep(2)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = \\\n      'http://localhost:' + str(SERVER_PORT) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets',\n        'confined_target_dirs': ['']}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory that will hold the client's target files.\n    destination_directory = self.make_temp_directory()\n\n    #  Populate 'destination_direction' with all target files.\n    all_targets = self.repository_updater.all_targets()\n\n    self.assertEqual(len(os.listdir(destination_directory)), 0)\n\n    for target in all_targets:\n      self.repository_updater.download_target(target, destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n\n    # Remove two target files from the server's repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update client's metadata.\n    self.repository_updater.refresh()\n\n    # Test: normal case.\n    # Verify number of target files in 'destination_directory' (should be 1\n    # after the update made to the remote repository), and call\n    # 'remove_obsolete_targets()'.\n    all_targets = self.repository_updater.all_targets()\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets,\n                                              destination_directory)\n\n    for updated_target in updated_targets:\n      self.repository_updater.download_target(updated_target,\n                                              destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    #  Verify that, if there are no obsolete files, the number of files\n    #  in 'destination_directory' remains the same.\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    # Test coverage for a destination path that causes an exception not due\n    # to an already removed target.\n    bad_destination_directory = 'bad' * 2000\n    self.repository_updater.remove_obsolete_targets(bad_destination_directory)\n\n    # Test coverage for a target that is not specified in current metadata.\n    del self.repository_updater.metadata['current']['targets']['targets']['file2.txt']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    # Test coverage for a role that doesn't exist in the previously trusted set\n    # of metadata.\n    del self.repository_updater.metadata['previous']['targets']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    server_process.kill()\n\n\n\n  def test_9__get_target_hash(self):\n    # Test normal case.\n    # Test target filepaths with ascii and non-ascii characters.\n    expected_target_hashes = {\n      '/file1.txt': 'e3a3d89eb3b70ce3fbce6017d7b8c12d4abd5635427a0e8a238f53157df85b3d',\n      '/Jalape\\xc3\\xb1o': '78bfd5c314680545eb48ecad508aceb861f8d6e680f4fe1b791da45c298cda88'\n    }\n    for filepath, target_hash in six.iteritems(expected_target_hashes):\n      self.assertTrue(tuf.formats.RELPATH_SCHEMA.matches(filepath))\n      self.assertTrue(securesystemslib.formats.HASH_SCHEMA.matches(target_hash))\n      self.assertEqual(self.repository_updater._get_target_hash(filepath), target_hash)\n\n    # Test for improperly formatted argument.\n    #self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._get_target_hash, 8)\n\n\n\n\n  def test_10__hard_check_file_length(self):\n    # Test for exception if file object is not equal to trusted file length.\n    temp_file_object = tempfile.TemporaryFile()\n    temp_file_object.write(b'X')\n    temp_file_object.seek(0)\n    self.assertRaises(tuf.exceptions.DownloadLengthMismatchError,\n                     self.repository_updater._hard_check_file_length,\n                     temp_file_object, 10)\n\n\n\n\n\n  def test_10__soft_check_file_length(self):\n    # Test for exception if file object is not equal to trusted file length.\n    temp_file_object = tempfile.TemporaryFile()\n    temp_file_object.write(b'XXX')\n    temp_file_object.seek(0)\n    self.assertRaises(tuf.exceptions.DownloadLengthMismatchError,\n                     self.repository_updater._soft_check_file_length,\n                     temp_file_object, 1)\n\n    # Verify that an exception is not raised if the file length <= the observed\n    # file length.\n    temp_file_object.seek(0)\n    self.repository_updater._soft_check_file_length(temp_file_object, 3)\n    temp_file_object.seek(0)\n    self.repository_updater._soft_check_file_length(temp_file_object, 4)\n\n\n\n\n  def test_10__targets_of_role(self):\n    # Test for non-existent role.\n    self.assertRaises(tuf.exceptions.UnknownRoleError,\n                      self.repository_updater._targets_of_role,\n                      'non-existent-role')\n\n    # Test for role that hasn't been loaded yet.\n    del self.repository_updater.metadata['current']['targets']\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets',\n                                                        skip_refresh=True)), 0)\n\n    # 'targets.json' tracks two targets.\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets')),\n                     2)\n\n\n\n  def test_10__preorder_depth_first_walk(self):\n\n    # Test that infinite loop is prevented if the target file is not found and\n    # the max number of delegations is reached.\n    valid_max_number_of_delegations = tuf.settings.MAX_NUMBER_OF_DELEGATIONS\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = 0\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('unknown.txt'))\n\n    # Reset the setting for max number of delegations so that subsequent unit\n    # tests reference the expected setting.\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = valid_max_number_of_delegations\n\n    # Attempt to create a circular delegation, where role1 performs a\n    # delegation to the top-level Targets role.  The updater should ignore the\n    # delegation and not raise an exception.\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    targets_metadata = securesystemslib.util.load_json_file(targets_path)\n    targets_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(targets_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(targets_metadata))\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['name'] = 'targets'\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    role2_path = os.path.join(self.client_metadata_current, 'role2.json')\n    role2_metadata = securesystemslib.util.load_json_file(role2_path)\n    role2_metadata['signed']['delegations']['roles'] = role1_metadata['signed']['delegations']['roles']\n    role2_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role2_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role2_metadata))\n\n    logger.debug('attempting circular delegation')\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('/file8.txt'))\n\n\n\n\n\n\n  def test_10__visit_child_role(self):\n    # Call _visit_child_role and test the dict keys: 'paths',\n    # 'path_hash_prefixes', and if both are missing.\n\n    targets_role = self.repository_updater.metadata['current']['targets']\n    targets_role['delegations']['roles'][0]['paths'] = ['/*.txt', '/target.exe']\n    child_role = targets_role['delegations']['roles'][0]\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/*.exe']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/target.exe'), child_role['name'])\n\n    # Test for a valid path hash prefix...\n    child_role['path_hash_prefixes'] = ['8baf']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), child_role['name'])\n\n    # ... and an invalid one, as well.\n    child_role['path_hash_prefixes'] = ['badd']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), None)\n\n    # Test for a forbidden target.\n    del child_role['path_hash_prefixes']\n    self.repository_updater._visit_child_role(child_role, '/forbidden.tgz')\n\n    # Verify that unequal path_hash_prefixes are skipped.\n    child_role['path_hash_prefixes'] = ['bad', 'bad']\n    self.assertEqual(None, self.repository_updater._visit_child_role(child_role,\n        '/unknown.exe'))\n\n    # Test if both 'path' and 'path_hash_prefixes' are missing.\n    del child_role['paths']\n    del child_role['path_hash_prefixes']\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._visit_child_role,\n        child_role, child_role['name'])\n\n\n\n  def test_11__verify_uncompressed_metadata_file(self):\n    # Test for invalid metadata content.\n    metadata_file_object = tempfile.TemporaryFile()\n    metadata_file_object.write(b'X')\n    metadata_file_object.seek(0)\n\n    self.assertRaises(tuf.exceptions.InvalidMetadataJSONError,\n        self.repository_updater._verify_uncompressed_metadata_file,\n        metadata_file_object, 'root')\n\n\n\n  def test_12__verify_root_chain_link(self):\n    # Test for an invalid signature in the chain link.\n    # current = (i.e., 1.root.json)\n    # next = signable for the next metadata in the chain (i.e., 2.root.json)\n    rolename = 'root'\n    current_root = self.repository_updater.metadata['current']['root']\n\n    targets_path = os.path.join(self.repository_directory, 'metadata', 'targets.json')\n\n    # 'next_invalid_root' is a Targets signable, as written to disk.\n    # We use the Targets metadata here to ensure the signatures are invalid.\n    next_invalid_root = securesystemslib.util.load_json_file(targets_path)\n\n    self.assertRaises(securesystemslib.exceptions.BadSignatureError,\n        self.repository_updater._verify_root_chain_link, rolename, current_root,\n        next_invalid_root)\n\n\n\n  def test_13__get_file(self):\n    # Test for an \"unsafe\" download, where the file is downloaded up to\n    # a required length (and no more).  The \"safe\" download approach\n    # downloads an exact required length.\n    targets_path = os.path.join(self.repository_directory, 'metadata', 'targets.json')\n\n    file_size, file_hashes = securesystemslib.util.get_file_details(targets_path)\n    file_type = 'meta'\n\n    def verify_target_file(targets_path):\n      # Every target file must have its length and hashes inspected.\n      self.repository_updater._hard_check_file_length(targets_path, file_size)\n      self.repository_updater._check_hashes(targets_path, file_hashes)\n\n    self.repository_updater._get_file('targets.json', verify_target_file,\n        file_type, file_size, download_safely=True)\n\n    self.repository_updater._get_file('targets.json', verify_target_file,\n        file_type, file_size, download_safely=False)\n\n  def test_14__targets_of_role(self):\n    # Test case where a list of targets is given.  By default, the 'targets'\n    # parameter is None.\n    targets = [{'filepath': 'file1.txt', 'fileinfo': {'length': 1, 'hashes': {'sha256': 'abc'}}}]\n    self.repository_updater._targets_of_role('targets',\n        targets=targets, skip_refresh=False)\n\n\n\n\nclass TestMultiRepoUpdater(unittest_toolbox.Modified_TestCase):\n\n  def setUp(self):\n    # We are inheriting from custom class.\n    unittest_toolbox.Modified_TestCase.setUp(self)\n\n    self.temporary_directory = tempfile.mkdtemp(dir=os.getcwd())\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf/tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n\n    self.temporary_repository_root = self.make_temp_directory(directory=\n        self.temporary_directory)\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_client = os.path.join(original_repository_files, 'client', 'test_repository1')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_map_file = os.path.join(original_repository_files, 'map.json')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = os.path.join(self.temporary_repository_root,\n        'repository_server1')\n    self.repository_directory2 = os.path.join(self.temporary_repository_root,\n        'repository_server2')\n\n    # Setting 'tuf.settings.repositories_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.temporary_repository_root\n\n    repository_name = 'test_repository1'\n    repository_name2 = 'test_repository2'\n\n    self.client_directory = os.path.join(self.temporary_repository_root,\n        repository_name)\n    self.client_directory2 = os.path.join(self.temporary_repository_root,\n        repository_name2)\n\n    self.keystore_directory = os.path.join(self.temporary_repository_root,\n        'keystore')\n    self.map_file = os.path.join(self.client_directory, 'map.json')\n    self.map_file2 = os.path.join(self.client_directory2, 'map.json')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_repository, self.repository_directory2)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_client, self.client_directory2)\n    shutil.copyfile(original_map_file, self.map_file)\n    shutil.copyfile(original_map_file, self.map_file2)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served by the\n    # SimpleHTTPServer launched here.  The test cases of this unit test assume\n    # the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n    self.SERVER_PORT = 30001\n    self.SERVER_PORT2 = 30002\n\n    command = ['python', '-m', 'tuf.scripts.simple_server', str(self.SERVER_PORT)]\n    command2 = ['python', '-m', 'tuf.scripts.simple_server', str(self.SERVER_PORT2)]\n\n    self.server_process = subprocess.Popen(command, stderr=subprocess.PIPE,\n        cwd=self.repository_directory)\n\n    logger.debug('Server process started.')\n    logger.debug('Server process id: ' + str(self.server_process.pid))\n    logger.debug('Serving on port: ' + str(self.SERVER_PORT))\n\n    self.server_process2 = subprocess.Popen(command2, stderr=subprocess.PIPE,\n        cwd=self.repository_directory2)\n\n    logger.debug('Server process 2 started.')\n    logger.debug('Server 2 process id: ' + str(self.server_process2.pid))\n    logger.debug('Serving 2 on port: ' + str(self.SERVER_PORT2))\n    self.url = 'http://localhost:' + str(self.SERVER_PORT) + os.path.sep\n    self.url2 = 'http://localhost:' + str(self.SERVER_PORT2) + os.path.sep\n\n    # NOTE: Following error is raised if a delay is not long enough to allow\n    # the server process to set up and start listening:\n    #     <urlopen error [Errno 111] Connection refused>\n    # or, on Windows:\n    #     Failed to establish a new connection: [Errno 111] Connection refused'\n    # While 0.3s has consistently worked on Travis and local builds, it led to\n    # occasional failures in AppVeyor builds, so increasing this to 2s, sadly.\n    time.sleep(2)\n\n    url_prefix = 'http://localhost:' + str(self.SERVER_PORT)\n    url_prefix2 = 'http://localhost:' + str(self.SERVER_PORT2)\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets',\n        'confined_target_dirs': ['']}}\n\n    self.repository_mirrors2 = {'mirror1': {'url_prefix': url_prefix2,\n        'metadata_path': 'metadata', 'targets_path': 'targets',\n        'confined_target_dirs': ['']}}\n\n    # Create the repository instances.  The test cases will use these client\n    # updaters to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(repository_name,\n        self.repository_mirrors)\n    self.repository_updater2 = updater.Updater(repository_name2,\n        self.repository_mirrors2)\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.multi_repo_updater = updater.MultiRepoUpdater(self.map_file)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n    # Modified_TestCase.tearDown() automatically deletes temporary files and\n    # directories that may have been created during each test case.\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n    # Kill the SimpleHTTPServer process.\n    if self.server_process.returncode is None:\n      logger.info('Server process ' + str(self.server_process.pid) + ' terminated.')\n      self.server_process.kill()\n\n    if self.server_process2.returncode is None:\n      logger.info('Server 2 process ' + str(self.server_process2.pid) + ' terminated.')\n      self.server_process2.kill()\n\n    # updater.Updater() populates the roledb with the name \"test_repository1\"\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    # Remove the temporary repository directory, which should contain all the\n    # metadata, targets, and key files generated of all the test cases.  sleep\n    # for a bit to allow the kill'd server processes to terminate.\n    time.sleep(.3)\n    shutil.rmtree(self.temporary_directory)\n\n\n\n  # UNIT TESTS.\n  def test__init__(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, 8)\n\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n    # Test for a non-existent map file.\n    self.assertRaises(tuf.exceptions.Error, updater.MultiRepoUpdater,\n        'non-existent.json')\n\n    # Test for a map file that doesn't contain the required fields.\n    root_filepath = os.path.join(\n        self.repository_directory, 'metadata', 'root.json')\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, root_filepath)\n\n    # Test for a valid instantiation.\n    map_file = os.path.join(self.client_directory, 'map.json')\n    multi_repo_updater = updater.MultiRepoUpdater(map_file)\n\n\n\n  def test__target_matches_path_pattern(self):\n    map_file = os.path.join(self.client_directory, 'map.json')\n    multi_repo_updater = updater.MultiRepoUpdater(map_file)\n    paths = ['foo*.tgz', 'bar*.tgz', 'file1.txt']\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('bar-1.0.tgz', paths))\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('file1.txt', paths))\n    self.assertFalse(\n        multi_repo_updater._target_matches_path_pattern('baz-1.0.tgz', paths))\n\n\n\n  def test_get_valid_targetinfo(self):\n    map_file = os.path.join(self.client_directory, 'map.json')\n    multi_repo_updater = updater.MultiRepoUpdater(map_file)\n\n    # Verify the multi repo updater refuses to save targetinfo if\n    # required local repositories are missing.\n    repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1')\n    backup_repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1.backup')\n    shutil.move(repo_dir, backup_repo_dir)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the client's repository directory.\n    shutil.move(backup_repo_dir, repo_dir)\n\n    # Verify that the Root file must exist.\n    root_filepath = os.path.join(repo_dir, 'metadata', 'current', 'root.json')\n    backup_root_filepath = os.path.join(root_filepath, root_filepath + '.backup')\n    shutil.move(root_filepath, backup_root_filepath)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the Root file.\n    shutil.move(backup_root_filepath, root_filepath)\n\n    # Test that the first mapping is skipped if it's irrelevant to the target\n    # file.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n\n    # Verify that a targetinfo is not returned for a non-existent target.\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = False\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = True\n\n    # Test for a mapping that sets terminating = True, and that appears before\n    # the final mapping.\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = True\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'bad3.txt')\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = False\n\n    # Test for the case where multiple repos sign for the same target.\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    # Verify that valid targetinfo is matched for two repositories that provide\n    # different custom field.  Make sure to set the 'match_custom_field'\n    # argument to 'False' when calling get_valid_targetinfo().\n    repository = repo_tool.load_repository(self.repository_directory2)\n\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    custom_field = {\"custom\": \"my_custom_data\"}\n    repository.targets.add_target(target1, custom_field)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Do we get the expected match for the two targetinfo that only differ\n    # by the custom field?\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo(\n        'file1.txt', match_custom_field=False)\n\n    # Verify the case where two repositories provide different targetinfo.\n    # Modify file1.txt so that different length and hashes are reported by the\n    # two repositories.\n    repository = repo_tool.load_repository(self.repository_directory2)\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.add_target(target1)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Ensure the threshold is modified to 2 (assumed to be 1, by default) and\n    # verify that get_valid_targetinfo() raises an UnknownTargetError\n    # despite both repos signing for file1.txt.\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'file1.txt')\n\n\n\n\n\n  def test_get_updater(self):\n    map_file = os.path.join(self.client_directory, 'map.json')\n    multi_repo_updater = updater.MultiRepoUpdater(map_file)\n\n    # Test for a non-existent repository name.\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n\n    # Test get_updater indirectly via the \"private\" _update_from_repository().\n    self.assertRaises(tuf.exceptions.Error, multi_repo_updater._update_from_repository, 'bad_repo_name', 'file3.txt')\n\n    # Test for a repository that doesn't exist.\n    multi_repo_updater.map_file['repositories']['bad_repo_name'] = ['https://bogus:30002']\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n\n\n\ndef _load_role_keys(keystore_directory):\n\n  # Populating 'self.role_keys' by importing the required public and private\n  # keys of 'tuf/tests/repository_data/'.  The role keys are needed when\n  # modifying the remote repository used by the test cases in this unit test.\n\n  # The pre-generated key files in 'repository_data/keystore' are all encrypted with\n  # a 'password' passphrase.\n  EXPECTED_KEYFILE_PASSWORD = 'password'\n\n  # Store and return the cryptography keys of the top-level roles, including 1\n  # delegated role.\n  role_keys = {}\n\n  root_key_file = os.path.join(keystore_directory, 'root_key')\n  targets_key_file = os.path.join(keystore_directory, 'targets_key')\n  snapshot_key_file = os.path.join(keystore_directory, 'snapshot_key')\n  timestamp_key_file = os.path.join(keystore_directory, 'timestamp_key')\n  delegation_key_file = os.path.join(keystore_directory, 'delegation_key')\n\n  role_keys = {'root': {}, 'targets': {}, 'snapshot': {}, 'timestamp': {},\n               'role1': {}}\n\n  # Import the top-level and delegated role public keys.\n  role_keys['root']['public'] = \\\n    repo_tool.import_rsa_publickey_from_file(root_key_file+'.pub')\n  role_keys['targets']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(targets_key_file+'.pub')\n  role_keys['snapshot']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(snapshot_key_file+'.pub')\n  role_keys['timestamp']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(timestamp_key_file+'.pub')\n  role_keys['role1']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(delegation_key_file+'.pub')\n\n  # Import the private keys of the top-level and delegated roles.\n  role_keys['root']['private'] = \\\n    repo_tool.import_rsa_privatekey_from_file(root_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['targets']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(targets_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['snapshot']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(snapshot_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['timestamp']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(timestamp_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['role1']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(delegation_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n\n  return role_keys\n\n\nif __name__ == '__main__':\n  unittest.main()\n", "#!/usr/bin/env python\n\n# Copyright 2012 - 2017, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\n<Program Name>\n  sig.py\n\n<Author>\n  Vladimir Diaz <vladimir.v.diaz@gmail.com>\n\n<Started>\n  February 28, 2012.   Based on a previous version by Geremy Condra.\n\n<Copyright>\n  See LICENSE-MIT OR LICENSE for licensing information.\n\n<Purpose>\n  Survivable key compromise is one feature of a secure update system\n  incorporated into TUF's design. Responsibility separation through\n  the use of multiple roles, multi-signature trust, and explicit and\n  implicit key revocation are some of the mechanisms employed towards\n  this goal of survivability.  These mechanisms can all be seen in\n  play by the functions available in this module.\n\n  The signed metadata files utilized by TUF to download target files\n  securely are used and represented here as the 'signable' object.\n  More precisely, the signature structures contained within these metadata\n  files are packaged into 'signable' dictionaries.  This module makes it\n  possible to capture the states of these signatures by organizing the\n  keys into different categories.  As keys are added and removed, the\n  system must securely and efficiently verify the status of these signatures.\n  For instance, a bunch of keys have recently expired. How many valid keys\n  are now available to the Snapshot role?  This question can be answered by\n  get_signature_status(), which will return a full 'status report' of these\n  'signable' dicts.  This module also provides a convenient verify() function\n  that will determine if a role still has a sufficient number of valid keys.\n  If a caller needs to update the signatures of a 'signable' object, there\n  is also a function for that.\n\"\"\"\n\n# Help with Python 3 compatibility, where the print statement is a function, an\n# implicit relative import is invalid, and the '/' operator performs true\n# division.  Example:  print 'hello world' raises a 'SyntaxError' exception.\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nimport logging\n\nimport tuf\nimport tuf.keydb\nimport tuf.roledb\nimport tuf.formats\n\nimport securesystemslib\n\n# See 'log.py' to learn how logging is handled in TUF.\nlogger = logging.getLogger('tuf.sig')\n\n# Disable 'iso8601' logger messages to prevent 'iso8601' from clogging the\n# log file.\niso8601_logger = logging.getLogger('iso8601')\niso8601_logger.disabled = True\n\n\ndef get_signature_status(signable, role=None, repository_name='default',\n    threshold=None, keyids=None):\n  \"\"\"\n  <Purpose>\n    Return a dictionary representing the status of the signatures listed in\n    'signable'.  Given an object conformant to SIGNABLE_SCHEMA, a set of public\n    keys in 'tuf.keydb', a set of roles in 'tuf.roledb', and a role,\n    the status of these signatures can be determined.  This method will iterate\n    the signatures in 'signable' and enumerate all the keys that are valid,\n    invalid, unrecognized, or unauthorized.\n\n  <Arguments>\n    signable:\n      A dictionary containing a list of signatures and a 'signed' identifier.\n      signable = {'signed': 'signer',\n                  'signatures': [{'keyid': keyid,\n                                  'sig': sig}]}\n\n      Conformant to tuf.formats.SIGNABLE_SCHEMA.\n\n    role:\n      TUF role (e.g., 'root', 'targets', 'snapshot').\n\n    threshold:\n      Rather than reference the role's threshold as set in tuf.roledb.py, use\n      the given 'threshold' to calculate the signature status of 'signable'.\n      'threshold' is an integer value that sets the role's threshold value, or\n      the minimum number of signatures needed for metadata to be considered\n      fully signed.\n\n    keyids:\n      Similar to the 'threshold' argument, use the supplied list of 'keyids'\n      to calculate the signature status, instead of referencing the keyids\n      in tuf.roledb.py for 'role'.\n\n  <Exceptions>\n    securesystemslib.exceptions.FormatError, if 'signable' does not have the\n    correct format.\n\n    tuf.exceptions.UnknownRoleError, if 'role' is not recognized.\n\n  <Side Effects>\n    None.\n\n  <Returns>\n    A dictionary representing the status of the signatures in 'signable'.\n    Conformant to tuf.formats.SIGNATURESTATUS_SCHEMA.\n  \"\"\"\n\n  # Do the arguments have the correct format?  This check will ensure that\n  # arguments have the appropriate number of objects and object types, and that\n  # all dict keys are properly named.  Raise\n  # 'securesystemslib.exceptions.FormatError' if the check fails.\n  tuf.formats.SIGNABLE_SCHEMA.check_match(signable)\n  securesystemslib.formats.NAME_SCHEMA.check_match(repository_name)\n\n  if role is not None:\n    tuf.formats.ROLENAME_SCHEMA.check_match(role)\n\n  if threshold is not None:\n    tuf.formats.THRESHOLD_SCHEMA.check_match(threshold)\n\n  if keyids is not None:\n    securesystemslib.formats.KEYIDS_SCHEMA.check_match(keyids)\n\n  # The signature status dictionary returned.\n  signature_status = {}\n\n  # The fields of the signature_status dict, where each field stores keyids.  A\n  # description of each field:\n  #\n  # good_sigs = keys confirmed to have produced 'sig' using 'signed', which are\n  # associated with 'role';\n  #\n  # bad_sigs = negation of good_sigs;\n  #\n  # unknown_sigs = keys not found in the 'keydb' database;\n  #\n  # untrusted_sigs = keys that are not in the list of keyids associated with\n  # 'role';\n  #\n  # unknown_signing_scheme = signing schemes specified in keys that are\n  # unsupported;\n  good_sigs = []\n  bad_sigs = []\n  unknown_sigs = []\n  untrusted_sigs = []\n  unknown_signing_schemes = []\n\n  # Extract the relevant fields from 'signable' that will allow us to identify\n  # the different classes of keys (i.e., good_sigs, bad_sigs, etc.).\n  signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n  signatures = signable['signatures']\n\n  # Iterate the signatures and enumerate the signature_status fields.\n  # (i.e., good_sigs, bad_sigs, etc.).\n  for signature in signatures:\n    keyid = signature['keyid']\n\n    # Does the signature use an unrecognized key?\n    try:\n      key = tuf.keydb.get_key(keyid, repository_name)\n\n    except tuf.exceptions.UnknownKeyError:\n      unknown_sigs.append(keyid)\n      continue\n\n    # Does the signature use an unknown/unsupported signing scheme?\n    try:\n      valid_sig = securesystemslib.keys.verify_signature(key, signature, signed)\n\n    except securesystemslib.exceptions.UnsupportedAlgorithmError:\n      unknown_signing_schemes.append(keyid)\n      continue\n\n    # We are now dealing with either a trusted or untrusted key...\n    if valid_sig:\n      if role is not None:\n\n        # Is this an unauthorized key? (a keyid associated with 'role')\n        # Note that if the role is not known, tuf.exceptions.UnknownRoleError\n        # is raised here.\n        if keyids is None:\n          keyids = tuf.roledb.get_role_keyids(role, repository_name)\n\n        if keyid not in keyids:\n          untrusted_sigs.append(keyid)\n          continue\n\n      # This is an unset role, thus an unknown signature.\n      else:\n        unknown_sigs.append(keyid)\n        continue\n\n      # Identify good/authorized key.\n      good_sigs.append(keyid)\n\n    else:\n      # This is a bad signature for a trusted key.\n      bad_sigs.append(keyid)\n\n  # Retrieve the threshold value for 'role'.  Raise\n  # tuf.exceptions.UnknownRoleError if we were given an invalid role.\n  if role is not None:\n    if threshold is None:\n      # Note that if the role is not known, tuf.exceptions.UnknownRoleError is\n      # raised here.\n      threshold = tuf.roledb.get_role_threshold(\n          role, repository_name=repository_name)\n\n    else:\n      logger.debug('Not using roledb.py\\'s threshold for ' + repr(role))\n\n  else:\n    threshold = 0\n\n  # Build the signature_status dict.\n  signature_status['threshold'] = threshold\n  signature_status['good_sigs'] = good_sigs\n  signature_status['bad_sigs'] = bad_sigs\n  signature_status['unknown_sigs'] = unknown_sigs\n  signature_status['untrusted_sigs'] = untrusted_sigs\n  signature_status['unknown_signing_schemes'] = unknown_signing_schemes\n\n  return signature_status\n\n\n\n\n\ndef verify(signable, role, repository_name='default', threshold=None,\n    keyids=None):\n  \"\"\"\n  <Purpose>\n    Verify whether the authorized signatures of 'signable' meet the minimum\n    required by 'role'.  Authorized signatures are those with valid keys\n    associated with 'role'.  'signable' must conform to SIGNABLE_SCHEMA\n    and 'role' must not equal 'None' or be less than zero.\n\n  <Arguments>\n    signable:\n      A dictionary containing a list of signatures and a 'signed' identifier.\n      signable = {'signed':, 'signatures': [{'keyid':, 'method':, 'sig':}]}\n\n    role:\n      TUF role (e.g., 'root', 'targets', 'snapshot').\n\n    threshold:\n      Rather than reference the role's threshold as set in tuf.roledb.py, use\n      the given 'threshold' to calculate the signature status of 'signable'.\n      'threshold' is an integer value that sets the role's threshold value, or\n      the minimum number of signatures needed for metadata to be considered\n      fully signed.\n\n    keyids:\n      Similar to the 'threshold' argument, use the supplied list of 'keyids'\n      to calculate the signature status, instead of referencing the keyids\n      in tuf.roledb.py for 'role'.\n\n  <Exceptions>\n    tuf.exceptions.UnknownRoleError, if 'role' is not recognized.\n\n    securesystemslib.exceptions.FormatError, if 'signable' is not formatted\n    correctly.\n\n    securesystemslib.exceptions.Error, if an invalid threshold is encountered.\n\n  <Side Effects>\n    tuf.sig.get_signature_status() called.  Any exceptions thrown by\n    get_signature_status() will be caught here and re-raised.\n\n  <Returns>\n    Boolean.  True if the number of good signatures >= the role's threshold,\n    False otherwise.\n  \"\"\"\n\n  tuf.formats.SIGNABLE_SCHEMA.check_match(signable)\n  tuf.formats.ROLENAME_SCHEMA.check_match(role)\n  securesystemslib.formats.NAME_SCHEMA.check_match(repository_name)\n\n  # Retrieve the signature status.  tuf.sig.get_signature_status() raises:\n  # tuf.exceptions.UnknownRoleError\n  # securesystemslib.exceptions.FormatError.  'threshold' and 'keyids' are also\n  # validated.\n  status = get_signature_status(signable, role, repository_name, threshold, keyids)\n\n  # Retrieve the role's threshold and the authorized keys of 'status'\n  threshold = status['threshold']\n  good_sigs = status['good_sigs']\n\n  # Does 'status' have the required threshold of signatures?\n  # First check for invalid threshold values before returning result.\n  # Note: get_signature_status() is expected to verify that 'threshold' is\n  # not None or <= 0.\n  if threshold is None or threshold <= 0: #pragma: no cover\n    raise securesystemslib.exceptions.Error(\"Invalid threshold: \" + repr(threshold))\n\n  return len(good_sigs) >= threshold\n\n\n\n\n\ndef may_need_new_keys(signature_status):\n  \"\"\"\n  <Purpose>\n    Return true iff downloading a new set of keys might tip this\n    signature status over to valid.  This is determined by checking\n    if either the number of unknown or untrusted keys is > 0.\n\n  <Arguments>\n    signature_status:\n      The dictionary returned by tuf.sig.get_signature_status().\n\n  <Exceptions>\n    securesystemslib.exceptions.FormatError, if 'signature_status does not have\n    the correct format.\n\n  <Side Effects>\n    None.\n\n  <Returns>\n    Boolean.\n  \"\"\"\n\n  # Does 'signature_status' have the correct format?\n  # This check will ensure 'signature_status' has the appropriate number\n  # of objects and object types, and that all dict keys are properly named.\n  # Raise 'securesystemslib.exceptions.FormatError' if the check fails.\n  tuf.formats.SIGNATURESTATUS_SCHEMA.check_match(signature_status)\n\n  unknown = signature_status['unknown_sigs']\n  untrusted = signature_status['untrusted_sigs']\n\n  return len(unknown) or len(untrusted)\n\n\n\n\n\ndef generate_rsa_signature(signed, rsakey_dict):\n  \"\"\"\n  <Purpose>\n    Generate a new signature dict presumably to be added to the 'signatures'\n    field of 'signable'.  The 'signable' dict is of the form:\n\n    {'signed': 'signer',\n               'signatures': [{'keyid': keyid,\n                               'method': 'evp',\n                               'sig': sig}]}\n\n    The 'signed' argument is needed here for the signing process.\n    The 'rsakey_dict' argument is used to generate 'keyid', 'method', and 'sig'.\n\n    The caller should ensure the returned signature is not already in\n    'signable'.\n\n  <Arguments>\n    signed:\n      The data used by 'securesystemslib.keys.create_signature()' to generate\n      signatures.  It is stored in the 'signed' field of 'signable'.\n\n    rsakey_dict:\n      The RSA key, a 'securesystemslib.formats.RSAKEY_SCHEMA' dictionary.\n      Used here to produce 'keyid', 'method', and 'sig'.\n\n  <Exceptions>\n    securesystemslib.exceptions.FormatError, if 'rsakey_dict' does not have the\n    correct format.\n\n    TypeError, if a private key is not defined for 'rsakey_dict'.\n\n  <Side Effects>\n    None.\n\n  <Returns>\n    Signature dictionary conformant to securesystemslib.formats.SIGNATURE_SCHEMA.\n    Has the form:\n    {'keyid': keyid, 'method': 'evp', 'sig': sig}\n  \"\"\"\n\n  # We need 'signed' in canonical JSON format to generate\n  # the 'method' and 'sig' fields of the signature.\n  signed = securesystemslib.formats.encode_canonical(signed).encode('utf-8')\n\n  # Generate the RSA signature.\n  # Raises securesystemslib.exceptions.FormatError and TypeError.\n  signature = securesystemslib.keys.create_signature(rsakey_dict, signed)\n\n  return signature\n"], "fixing_code": ["#!/usr/bin/env python\n\n# Copyright 2012 - 2017, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\n<Program Name>\n  test_sig.py\n\n<Author>\n  Geremy Condra\n  Vladimir Diaz <vladimir.v.diaz@gmail.com>\n\n<Started>\n  February 28, 2012.  Based on a previous version of this module.\n\n<Copyright>\n  See LICENSE-MIT OR LICENSE for licensing information.\n\n<Purpose>\n  Test cases for sig.py.\n\"\"\"\n\n# Help with Python 3 compatibility, where the print statement is a function, an\n# implicit relative import is invalid, and the '/' operator performs true\n# division.  Example:  print 'hello world' raises a 'SyntaxError' exception.\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nimport unittest\nimport logging\nimport copy\n\nimport tuf\nimport tuf.log\nimport tuf.formats\nimport tuf.keydb\nimport tuf.roledb\nimport tuf.sig\nimport tuf.exceptions\n\nimport securesystemslib\nimport securesystemslib.keys\n\nlogger = logging.getLogger('tuf.test_sig')\n\n# Setup the keys to use in our test cases.\nKEYS = []\nfor _ in range(3):\n  KEYS.append(securesystemslib.keys.generate_rsa_key(2048))\n\n\n\nclass TestSig(unittest.TestCase):\n  def setUp(self):\n    pass\n\n  def tearDown(self):\n    tuf.roledb.clear_roledb()\n    tuf.keydb.clear_keydb()\n\n\n  def test_get_signature_status_no_role(self):\n    signable = {'signed': 'test', 'signatures': []}\n\n    # A valid, but empty signature status.\n    sig_status = tuf.sig.get_signature_status(signable)\n    self.assertTrue(tuf.formats.SIGNATURESTATUS_SCHEMA.matches(sig_status))\n\n    self.assertEqual(0, sig_status['threshold'])\n    self.assertEqual([], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    # A valid signable, but non-existent role argument.\n    self.assertRaises(tuf.exceptions.UnknownRoleError,\n      tuf.sig.get_signature_status, signable, 'unknown_role')\n\n    # Should verify we are not adding a duplicate signature\n    # when doing the following action.  Here we know 'signable'\n    # has only one signature so it's okay.\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n\n    # Improperly formatted role.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n      tuf.sig.get_signature_status, signable, 1)\n\n    # Not allowed to call verify() without having specified a role.\n    args = (signable, None)\n    self.assertRaises(securesystemslib.exceptions.Error, tuf.sig.verify, *args)\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n\n\n  def test_get_signature_status_bad_sig(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n    signable['signed'] += 'signature no longer matches signed data'\n\n    tuf.keydb.add_key(KEYS[0])\n    threshold = 1\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA, keyids=[KEYS[0]['keyid']], threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertEqual(1, sig_status['threshold'])\n    self.assertEqual([], sig_status['good_sigs'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    self.assertFalse(tuf.sig.verify(signable, 'Root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    # Remove the role.\n    tuf.roledb.remove_role('Root')\n\n\n  def test_get_signature_status_unknown_signing_scheme(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    valid_scheme = KEYS[0]['scheme']\n    KEYS[0]['scheme'] = 'unknown_signing_scheme'\n    tuf.keydb.add_key(KEYS[0])\n    threshold = 1\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA, keyids=[KEYS[0]['keyid']], threshold=threshold)\n\n    tuf.roledb.add_role('root', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'root')\n\n    self.assertEqual(1, sig_status['threshold'])\n    self.assertEqual([], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([KEYS[0]['keyid']],\n                    sig_status['unknown_signing_schemes'])\n\n    self.assertFalse(tuf.sig.verify(signable, 'root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    KEYS[0]['scheme'] = valid_scheme\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    # Remove the role.\n    tuf.roledb.remove_role('root')\n\n\n  def test_get_signature_status_single_key(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    threshold = 1\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA, keyids=[KEYS[0]['keyid']], threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n    tuf.keydb.add_key(KEYS[0])\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertEqual(1, sig_status['threshold'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    self.assertTrue(tuf.sig.verify(signable, 'Root'))\n\n    # Test for an unknown signature when 'role' is left unspecified.\n    sig_status = tuf.sig.get_signature_status(signable)\n\n    self.assertEqual(0, sig_status['threshold'])\n    self.assertEqual([], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    # Remove the role.\n    tuf.roledb.remove_role('Root')\n\n\n  def test_get_signature_status_below_threshold(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n    threshold = 2\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA,\n        keyids=[KEYS[0]['keyid'], KEYS[2]['keyid']],\n        threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertEqual(2, sig_status['threshold'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    self.assertFalse(tuf.sig.verify(signable, 'Root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n\n    # Remove the role.\n    tuf.roledb.remove_role('Root')\n\n\n  def test_get_signature_status_below_threshold_unrecognized_sigs(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    # Two keys sign it, but only one of them will be trusted.\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[2], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n    tuf.keydb.add_key(KEYS[1])\n    threshold = 2\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA,\n        keyids=[KEYS[0]['keyid'], KEYS[1]['keyid']],\n        threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertEqual(2, sig_status['threshold'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([KEYS[2]['keyid']], sig_status['unknown_sigs'])\n    self.assertEqual([], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    self.assertFalse(tuf.sig.verify(signable, 'Root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    tuf.keydb.remove_key(KEYS[1]['keyid'])\n\n    # Remove the role.\n    tuf.roledb.remove_role('Root')\n\n\n  def test_get_signature_status_below_threshold_unauthorized_sigs(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n    # Two keys sign it, but one of them is only trusted for a different\n    # role.\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[1], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n    tuf.keydb.add_key(KEYS[1])\n    threshold = 2\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA,\n        keyids=[KEYS[0]['keyid'], KEYS[2]['keyid']],\n        threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA,\n        keyids=[KEYS[1]['keyid'], KEYS[2]['keyid']],\n        threshold=threshold)\n\n    tuf.roledb.add_role('Release', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertEqual(2, sig_status['threshold'])\n    self.assertEqual([KEYS[0]['keyid']], sig_status['good_sigs'])\n    self.assertEqual([], sig_status['bad_sigs'])\n    self.assertEqual([], sig_status['unknown_sigs'])\n    self.assertEqual([KEYS[1]['keyid']], sig_status['untrusted_sigs'])\n    self.assertEqual([], sig_status['unknown_signing_schemes'])\n\n    self.assertFalse(tuf.sig.verify(signable, 'Root'))\n\n    self.assertRaises(tuf.exceptions.UnknownRoleError,\n                      tuf.sig.get_signature_status, signable, 'unknown_role')\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    tuf.keydb.remove_key(KEYS[1]['keyid'])\n\n    # Remove the roles.\n    tuf.roledb.remove_role('Root')\n    tuf.roledb.remove_role('Release')\n\n\n\n  def test_check_signatures_no_role(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n\n    # No specific role we're considering. It's invalid to use the\n    # function tuf.sig.verify() without a role specified because\n    # tuf.sig.verify() is checking trust, as well.\n    args = (signable, None)\n    self.assertRaises(securesystemslib.exceptions.Error, tuf.sig.verify, *args)\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n\n\n\n  def test_verify_single_key(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n    threshold = 1\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA, keyids=[KEYS[0]['keyid']], threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    # This will call verify() and return True if 'signable' is valid,\n    # False otherwise.\n    self.assertTrue(tuf.sig.verify(signable, 'Root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n\n    # Remove the roles.\n    tuf.roledb.remove_role('Root')\n\n\n\n  def test_verify_must_not_count_duplicate_keyids_towards_threshold(self):\n    # Create and sign dummy metadata twice with same key\n    # Note that we use the non-deterministic rsassa-pss signing scheme, so\n    # creating the signature twice shows that we don't only detect duplicate\n    # signatures but also different signatures from the same key.\n    signable = {\"signed\" : \"test\", \"signatures\" : []}\n    signed = securesystemslib.formats.encode_canonical(\n        signable[\"signed\"]).encode(\"utf-8\")\n    signable[\"signatures\"].append(\n        securesystemslib.keys.create_signature(KEYS[0], signed))\n    signable[\"signatures\"].append(\n        securesystemslib.keys.create_signature(KEYS[0], signed))\n\n    # 'get_signature_status' uses keys from keydb for verification\n    tuf.keydb.add_key(KEYS[0])\n\n    # Assert that 'get_signature_status' returns two good signatures ...\n    status = tuf.sig.get_signature_status(\n        signable, \"root\", keyids=[KEYS[0][\"keyid\"]], threshold=2)\n    self.assertTrue(len(status[\"good_sigs\"]) == 2)\n\n    # ... but only one counts towards the threshold\n    self.assertFalse(\n        tuf.sig.verify(signable, \"root\", keyids=[KEYS[0][\"keyid\"]], threshold=2))\n\n    # Clean-up keydb\n    tuf.keydb.remove_key(KEYS[0][\"keyid\"])\n\n\n\n  def test_verify_count_different_keyids_for_same_key_towards_threshold(self):\n    # Create and sign dummy metadata twice with same key but different keyids\n    signable = {\"signed\" : \"test\", \"signatures\" : []}\n    key_sha256 = copy.deepcopy(KEYS[0])\n    key_sha256[\"keyid\"] = \"deadbeef256\"\n\n    key_sha512 = copy.deepcopy(KEYS[0])\n    key_sha512[\"keyid\"] = \"deadbeef512\"\n\n    signed = securesystemslib.formats.encode_canonical(\n        signable[\"signed\"]).encode(\"utf-8\")\n    signable[\"signatures\"].append(\n        securesystemslib.keys.create_signature(key_sha256, signed))\n    signable[\"signatures\"].append(\n        securesystemslib.keys.create_signature(key_sha512, signed))\n\n    # 'get_signature_status' uses keys from keydb for verification\n    tuf.keydb.add_key(key_sha256)\n    tuf.keydb.add_key(key_sha512)\n\n    # Assert that both keys count towards threshold although its the same key\n    keyids = [key_sha256[\"keyid\"], key_sha512[\"keyid\"]]\n    self.assertTrue(\n        tuf.sig.verify(signable, \"root\", keyids=keyids, threshold=2))\n\n    # Clean-up keydb\n    tuf.keydb.remove_key(key_sha256[\"keyid\"])\n    tuf.keydb.remove_key(key_sha512[\"keyid\"])\n\n\n\n  def test_verify_unrecognized_sig(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    # Two keys sign it, but only one of them will be trusted.\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[2], signed))\n\n    tuf.keydb.add_key(KEYS[0])\n    tuf.keydb.add_key(KEYS[1])\n    threshold = 2\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA,\n        keyids=[KEYS[0]['keyid'], KEYS[1]['keyid']],\n        threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    self.assertFalse(tuf.sig.verify(signable, 'Root'))\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[0]['keyid'])\n    tuf.keydb.remove_key(KEYS[1]['keyid'])\n\n    # Remove the roles.\n    tuf.roledb.remove_role('Root')\n\n\n\n  def test_generate_rsa_signature(self):\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    self.assertEqual(1, len(signable['signatures']))\n    signature = signable['signatures'][0]\n    self.assertEqual(KEYS[0]['keyid'], signature['keyid'])\n\n    returned_signature = tuf.sig.generate_rsa_signature(signable['signed'], KEYS[0])\n    self.assertTrue(securesystemslib.formats.SIGNATURE_SCHEMA.matches(returned_signature))\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[1], signed))\n\n    self.assertEqual(2, len(signable['signatures']))\n    signature = signable['signatures'][1]\n    self.assertEqual(KEYS[1]['keyid'], signature['keyid'])\n\n\n\n  def test_may_need_new_keys(self):\n    # One untrusted key in 'signable'.\n    signable = {'signed' : 'test', 'signatures' : []}\n    signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n\n    signable['signatures'].append(securesystemslib.keys.create_signature(\n                                  KEYS[0], signed))\n\n    tuf.keydb.add_key(KEYS[1])\n    threshold = 1\n\n    roleinfo = tuf.formats.build_dict_conforming_to_schema(\n        tuf.formats.ROLE_SCHEMA, keyids=[KEYS[1]['keyid']], threshold=threshold)\n\n    tuf.roledb.add_role('Root', roleinfo)\n\n    sig_status = tuf.sig.get_signature_status(signable, 'Root')\n\n    self.assertTrue(tuf.sig.may_need_new_keys(sig_status))\n\n\n    # Done.  Let's remove the added key(s) from the key database.\n    tuf.keydb.remove_key(KEYS[1]['keyid'])\n\n    # Remove the roles.\n    tuf.roledb.remove_role('Root')\n\n\n  def test_signable_has_invalid_format(self):\n    # get_signature_status() and verify() validate 'signable' before continuing.\n    # 'signable' must be of the form: {'signed': , 'signatures': [{}]}.\n    # Object types are checked as well.\n    signable = {'not_signed' : 'test', 'signatures' : []}\n    args = (signable['not_signed'], KEYS[0])\n    self.assertRaises(securesystemslib.exceptions.FormatError, tuf.sig.get_signature_status, *args)\n\n    # 'signatures' value must be a list.  Let's try a dict.\n    signable = {'signed' : 'test', 'signatures' : {}}\n    args = (signable['signed'], KEYS[0])\n    self.assertRaises(securesystemslib.exceptions.FormatError, tuf.sig.get_signature_status, *args)\n\n\n\n# Run unit test.\nif __name__ == '__main__':\n  unittest.main()\n", "#!/usr/bin/env python\n\n# Copyright 2012 - 2017, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\n<Program Name>\n  test_updater.py\n\n<Author>\n  Konstantin Andrianov.\n\n<Started>\n  October 15, 2012.\n\n  March 11, 2014.\n    Refactored to remove mocked modules and old repository tool dependence, use\n    exact repositories, and add realistic retrieval of files. -vladimir.v.diaz\n\n<Copyright>\n  See LICENSE-MIT OR LICENSE for licensing information.\n\n<Purpose>\n  'test_updater.py' provides a collection of methods that test the public /\n  non-public methods and functions of 'tuf.client.updater.py'.\n\n  The 'unittest_toolbox.py' module was created to provide additional testing\n  tools, such as automatically deleting temporary files created in test cases.\n  For more information, see 'tests/unittest_toolbox.py'.\n\n<Methodology>\n  Test cases here should follow a specific order (i.e., independent methods are\n  tested before dependent methods). More accurately, least dependent methods\n  are tested before most dependent methods.  There is no reason to rewrite or\n  construct other methods that replicate already-tested methods solely for\n  testing purposes.  This is possible because the 'unittest.TestCase' class\n  guarantees the order of unit tests.  The 'test_something_A' method would\n  be tested before 'test_something_B'.  To ensure the expected order of tests,\n  a number is placed after 'test' and before methods name like so:\n  'test_1_check_directory'.  The number is a measure of dependence, where 1 is\n  less dependent than 2.\n\"\"\"\n\n# Help with Python 3 compatibility, where the print statement is a function, an\n# implicit relative import is invalid, and the '/' operator performs true\n# division.  Example:  print 'hello world' raises a 'SyntaxError' exception.\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nimport os\nimport time\nimport shutil\nimport copy\nimport tempfile\nimport logging\nimport random\nimport subprocess\nimport sys\nimport errno\nimport unittest\n\nimport tuf\nimport tuf.exceptions\nimport tuf.log\nimport tuf.formats\nimport tuf.keydb\nimport tuf.roledb\nimport tuf.repository_tool as repo_tool\nimport tuf.repository_lib as repo_lib\nimport tuf.unittest_toolbox as unittest_toolbox\nimport tuf.client.updater as updater\n\nimport securesystemslib\nimport six\n\nlogger = logging.getLogger('tuf.test_updater')\nrepo_tool.disable_console_log_messages()\n\n\nclass TestUpdater(unittest_toolbox.Modified_TestCase):\n\n  @classmethod\n  def setUpClass(cls):\n    # setUpClass() is called before tests in an individual class are executed.\n\n    # Create a temporary directory to store the repository, metadata, and target\n    # files.  'temporary_directory' must be deleted in TearDownModule() so that\n    # temporary files are always removed, even when exceptions occur.\n    cls.temporary_directory = tempfile.mkdtemp(dir=os.getcwd())\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served\n    # by the SimpleHTTPServer launched here.  The test cases of 'test_updater.py'\n    # assume the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n    cls.SERVER_PORT = random.randint(30000, 45000)\n    command = ['python', '-m', 'tuf.scripts.simple_server', str(cls.SERVER_PORT)]\n    cls.server_process = subprocess.Popen(command, stderr=subprocess.PIPE)\n    logger.info('\\n\\tServer process started.')\n    logger.info('\\tServer process id: '+str(cls.server_process.pid))\n    logger.info('\\tServing on port: '+str(cls.SERVER_PORT))\n    cls.url = 'http://localhost:'+str(cls.SERVER_PORT) + os.path.sep\n\n    # NOTE: Following error is raised if a delay is not long enough to allow\n    # the server process to set up and start listening:\n    #     <urlopen error [Errno 111] Connection refused>\n    # or, on Windows:\n    #     Failed to establish a new connection: [Errno 111] Connection refused'\n    # While 0.3s has consistently worked on Travis and local builds, it led to\n    # occasional failures in AppVeyor builds, so increasing this to 2s, sadly.\n    time.sleep(2)\n\n\n\n  @classmethod\n  def tearDownClass(cls):\n    # tearDownModule() is called after all the tests have run.\n    # http://docs.python.org/2/library/unittest.html#class-and-module-fixtures\n\n    # Kill the SimpleHTTPServer process.\n    if cls.server_process.returncode is None:\n      logger.info('\\tServer process ' + str(cls.server_process.pid) + ' terminated.')\n      cls.server_process.kill()\n\n    # Remove the temporary repository directory, which should contain all the\n    # metadata, targets, and key files generated for the test cases.  sleep\n    # for a bit to allow the kill'd server process to terminate.\n    time.sleep(.3)\n    shutil.rmtree(cls.temporary_directory)\n\n\n\n  def setUp(self):\n    # We are inheriting from custom class.\n    unittest_toolbox.Modified_TestCase.setUp(self)\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    self.repository_name = 'test_repository1'\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf.tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n    temporary_repository_root = \\\n      self.make_temp_directory(directory=self.temporary_directory)\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_client = os.path.join(original_repository_files, 'client')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = \\\n      os.path.join(temporary_repository_root, 'repository')\n    self.keystore_directory = \\\n      os.path.join(temporary_repository_root, 'keystore')\n\n    self.client_directory = os.path.join(temporary_repository_root,\n        'client')\n    self.client_metadata = os.path.join(self.client_directory,\n        self.repository_name, 'metadata')\n    self.client_metadata_current = os.path.join(self.client_metadata,\n        'current')\n    self.client_metadata_previous = os.path.join(self.client_metadata,\n        'previous')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = \\\n      'http://localhost:' + str(self.SERVER_PORT) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n                                           'metadata_path': 'metadata',\n                                           'targets_path': 'targets',\n                                           'confined_target_dirs': ['']}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n                                              self.repository_mirrors)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n    # We are inheriting from custom class.\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n\n\n\n  # UNIT TESTS.\n\n  def test_1__init__exceptions(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, 8,\n                      self.repository_mirrors)\n\n    # Invalid 'repository_mirrors' argument.  'tuf.formats.MIRRORDICT_SCHEMA'\n    # expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError, updater.Updater, updater.Updater, 8)\n\n\n    # 'tuf.client.updater.py' requires that the client's repositories directory\n    # be configured in 'tuf.settings.py'.\n    tuf.settings.repositories_directory = None\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n\n    # Test: empty client repository (i.e., no metadata directory).\n    metadata_backup = self.client_metadata + '.backup'\n    shutil.move(self.client_metadata, metadata_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's metadata directory.\n    shutil.move(metadata_backup, self.client_metadata)\n\n\n    # Test: repository with only a '{repository_directory}/metadata' directory.\n    # (i.e., missing the required 'current' and 'previous' sub-directories).\n    current_backup = self.client_metadata_current + '.backup'\n    previous_backup = self.client_metadata_previous + '.backup'\n\n    shutil.move(self.client_metadata_current, current_backup)\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n\n    # Restore the client's previous directory.  The required 'current' directory\n    # is still missing.\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test: repository with only a '{repository_directory}/metadata/previous'\n    # directory.\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's current directory.\n    shutil.move(current_backup, self.client_metadata_current)\n\n    # Test: repository with a '{repository_directory}/metadata/current'\n    # directory, but the 'previous' directory is missing.\n    shutil.move(self.client_metadata_previous, previous_backup)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    shutil.move(previous_backup, self.client_metadata_previous)\n\n    # Test:  repository missing the required 'root.json' file.\n    client_root_file = os.path.join(self.client_metadata_current, 'root.json')\n    backup_root_file = client_root_file + '.backup'\n    shutil.move(client_root_file, backup_root_file)\n    self.assertRaises(tuf.exceptions.RepositoryError, updater.Updater, 'test_repository1',\n                      self.repository_mirrors)\n    # Restore the client's 'root.json file.\n    shutil.move(backup_root_file, client_root_file)\n\n    # Test: Normal 'tuf.client.updater.Updater' instantiation.\n    updater.Updater('test_repository1', self.repository_mirrors)\n\n\n\n\n\n  def test_1__load_metadata_from_file(self):\n\n    # Setup\n    # Get the 'role1.json' filepath.  Manually load the role metadata, and\n    # compare it against the loaded metadata by '_load_metadata_from_file()'.\n    role1_filepath = \\\n      os.path.join(self.client_metadata_current, 'role1.json')\n    role1_meta = securesystemslib.util.load_json_file(role1_filepath)\n\n    # Load the 'role1.json' file with _load_metadata_from_file, which should\n    # store the loaded metadata in the 'self.repository_updater.metadata'\n    # store.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n\n    # Verify that the correct number of metadata objects has been loaded\n    # (i.e., only the 'root.json' file should have been loaded.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Verify that the content of root metadata is valid.\n    self.assertEqual(self.repository_updater.metadata['current']['role1'],\n                     role1_meta['signed'])\n\n    # Verify that _load_metadata_from_file() doesn't raise an exception for\n    # improperly formatted metadata, and doesn't load the bad file.\n    with open(role1_filepath, 'ab') as file_object:\n      file_object.write(b'bad JSON data')\n\n    self.repository_updater._load_metadata_from_file('current', 'role1')\n    self.assertEqual(len(self.repository_updater.metadata['current']), 5)\n\n    # Test if we fail gracefully if we can't deserialize a meta file\n    self.repository_updater._load_metadata_from_file('current', 'empty_file')\n    self.assertFalse('empty_file' in self.repository_updater.metadata['current'])\n\n    # Test invalid metadata set argument (must be either\n    # 'current' or 'previous'.)\n    self.assertRaises(securesystemslib.exceptions.Error,\n                      self.repository_updater._load_metadata_from_file,\n                      'bad_metadata_set', 'role1')\n\n\n\n\n  def test_1__rebuild_key_and_role_db(self):\n    # Setup\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_threshold = root_metadata['roles']['root']['threshold']\n    number_of_root_keys = len(root_metadata['keys'])\n\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # Ensure we add 2 to the number of root keys (actually, the number of root\n    # keys multiplied by the number of keyid hash algorithms), to include the\n    # delegated targets key (+1 for its sha512 keyid).  The delegated roles of\n    # 'targets.json' are also loaded when the repository object is\n    # instantiated.\n\n    self.assertEqual(number_of_root_keys * 2 + 2, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: normal case.\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], root_threshold)\n\n    # _rebuild_key_and_role_db() will only rebuild the keys and roles specified\n    # in the 'root.json' file, unlike __init__().  Instantiating an updater\n    # object calls both _rebuild_key_and_role_db() and _import_delegations().\n    self.assertEqual(number_of_root_keys * 2, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n    # Test: properly updated roledb and keydb dicts if the Root role changes.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    root_metadata['roles']['root']['threshold'] = 8\n    root_metadata['keys'].popitem()\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    root_roleinfo = tuf.roledb.get_roleinfo('root', self.repository_name)\n    self.assertEqual(root_roleinfo['threshold'], 8)\n    self.assertEqual(number_of_root_keys * 2 - 2, len(tuf.keydb._keydb_dict[self.repository_name]))\n\n\n\n\n  def test_1__update_versioninfo(self):\n    # Tests\n    # Verify that the 'self.versioninfo' dictionary is empty (it starts off\n    # empty and is only populated if _update_versioninfo() is called.\n    versioninfo_dict = self.repository_updater.versioninfo\n    self.assertEqual(len(versioninfo_dict), 0)\n\n    # Load the versioninfo of the top-level Targets role.  This action\n    # populates the 'self.versioninfo' dictionary.\n    self.repository_updater._update_versioninfo('targets.json')\n    self.assertEqual(len(versioninfo_dict), 1)\n    self.assertTrue(tuf.formats.FILEINFODICT_SCHEMA.matches(versioninfo_dict))\n\n    # The Snapshot role stores the version numbers of all the roles available\n    # on the repository.  Load Snapshot to extract Root's version number\n    # and compare it against the one loaded by 'self.repository_updater'.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    # Verify that the manually loaded version number of root.json matches\n    # the one loaded by the updater object.\n    self.assertTrue('targets.json' in versioninfo_dict)\n    self.assertEqual(versioninfo_dict['targets.json'], targets_versioninfo)\n\n    # Verify that 'self.versioninfo' is incremented if another role is updated.\n    self.repository_updater._update_versioninfo('role1.json')\n    self.assertEqual(len(versioninfo_dict), 2)\n\n    # Verify that 'self.versioninfo' is incremented if a non-existent role is\n    # requested, and has its versioninfo entry set to 'None'.\n    self.repository_updater._update_versioninfo('bad_role.json')\n    self.assertEqual(len(versioninfo_dict), 3)\n    self.assertEqual(versioninfo_dict['bad_role.json'], None)\n\n    # Verify that the versioninfo specified in Timestamp is used if the Snapshot\n    # role hasn't been downloaded yet.\n    del self.repository_updater.metadata['current']['snapshot']\n    #self.assertRaises(self.repository_updater._update_versioninfo('snapshot.json'))\n    self.repository_updater._update_versioninfo('snapshot.json')\n    self.assertEqual(versioninfo_dict['snapshot.json']['version'], 1)\n\n\n\n  def test_1__refresh_must_not_count_duplicate_keyids_towards_threshold(self):\n    # Update root threshold on the server repository and sign twice with 1 key\n    repository = repo_tool.load_repository(self.repository_directory)\n    repository.root.threshold = 2\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n\n    # The client uses the threshold from the previous root file to verify the\n    # new root. Thus we need to make two updates so that the threshold used for\n    # verification becomes 2. I.e. we bump the version, sign twice with the\n    # same key and write to disk '2.root.json' and '3.root.json'.\n    for version in [2, 3]:\n      repository.root.version = version\n      info = tuf.roledb.get_roleinfo(\"root\")\n      metadata = repo_lib.generate_root_metadata(\n          info[\"version\"], info[\"expires\"], False)\n      signed_metadata = repo_lib.sign_metadata(\n          metadata, info[\"keyids\"], \"root.json\", \"default\")\n      signed_metadata[\"signatures\"].append(signed_metadata[\"signatures\"][0])\n      live_root_path = os.path.join(\n          self.repository_directory, \"metadata\", \"root.json\")\n\n      # Bypass server side verification in 'write' or 'writeall', which would\n      # catch the unmet threshold.\n      # We also skip writing to 'metadata.staged' and copying to 'metadata' and\n      # instead write directly to 'metadata'\n      repo_lib.write_metadata_file(signed_metadata, live_root_path, info[\"version\"], True)\n\n\n    # Update from current '1.root.json' to '3.root.json' on client and assert\n    # raise of 'BadSignatureError' (caused by unmet signature threshold).\n    try:\n      self.repository_updater.refresh()\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      mirror_errors = list(e.mirror_errors.values())\n      self.assertTrue(len(mirror_errors) == 1)\n      self.assertTrue(\n          isinstance(mirror_errors[0],\n          securesystemslib.exceptions.BadSignatureError))\n      self.assertEqual(\n          str(mirror_errors[0]),\n          repr(\"root\") + \" metadata has bad signature.\")\n\n    else:\n      self.fail(\n          \"Expected a NoWorkingMirrorError composed of one BadSignatureError\")\n\n\n  def test_1__update_fileinfo(self):\n      # Tests\n      # Verify that the 'self.fileinfo' dictionary is empty (its starts off empty\n      # and is only populated if _update_fileinfo() is called.\n      fileinfo_dict = self.repository_updater.fileinfo\n      self.assertEqual(len(fileinfo_dict), 0)\n\n      # Load the fileinfo of the top-level root role.  This populates the\n      # 'self.fileinfo' dictionary.\n      self.repository_updater._update_fileinfo('root.json')\n      self.assertEqual(len(fileinfo_dict), 1)\n      self.assertTrue(tuf.formats.FILEDICT_SCHEMA.matches(fileinfo_dict))\n      root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n      length, hashes = securesystemslib.util.get_file_details(root_filepath)\n      root_fileinfo = tuf.formats.make_fileinfo(length, hashes)\n      self.assertTrue('root.json' in fileinfo_dict)\n      self.assertEqual(fileinfo_dict['root.json'], root_fileinfo)\n\n      # Verify that 'self.fileinfo' is incremented if another role is updated.\n      self.repository_updater._update_fileinfo('targets.json')\n      self.assertEqual(len(fileinfo_dict), 2)\n\n      # Verify that 'self.fileinfo' is inremented if a non-existent role is\n      # requested, and has its fileinfo entry set to 'None'.\n      self.repository_updater._update_fileinfo('bad_role.json')\n      self.assertEqual(len(fileinfo_dict), 3)\n      self.assertEqual(fileinfo_dict['bad_role.json'], None)\n\n\n\n\n  def test_2__fileinfo_has_changed(self):\n      #  Verify that the method returns 'False' if file info was not changed.\n      root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n      length, hashes = securesystemslib.util.get_file_details(root_filepath)\n      root_fileinfo = tuf.formats.make_fileinfo(length, hashes)\n      self.assertFalse(self.repository_updater._fileinfo_has_changed('root.json',\n                                                             root_fileinfo))\n\n      # Verify that the method returns 'True' if length or hashes were changed.\n      new_length = 8\n      new_root_fileinfo = tuf.formats.make_fileinfo(new_length, hashes)\n      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n                                                             new_root_fileinfo))\n      # Hashes were changed.\n      new_hashes = {'sha256': self.random_string()}\n      new_root_fileinfo = tuf.formats.make_fileinfo(length, new_hashes)\n      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n                                                             new_root_fileinfo))\n\n      # Verify that _fileinfo_has_changed() returns True if no fileinfo (or set\n      # to None) exists for some role.\n      self.assertTrue(self.repository_updater._fileinfo_has_changed('bad.json',\n          new_root_fileinfo))\n\n      saved_fileinfo = self.repository_updater.fileinfo['root.json']\n      self.repository_updater.fileinfo['root.json'] = None\n      self.assertTrue(self.repository_updater._fileinfo_has_changed('root.json',\n          new_root_fileinfo))\n\n\n      self.repository_updater.fileinfo['root.json'] = saved_fileinfo\n      new_root_fileinfo['hashes']['sha666'] = '666'\n      self.repository_updater._fileinfo_has_changed('root.json',\n          new_root_fileinfo)\n\n\n\n  def test_2__import_delegations(self):\n    # Setup.\n    # In order to test '_import_delegations' the parent of the delegation\n    # has to be in Repository.metadata['current'], but it has to be inserted\n    # there without using '_load_metadata_from_file()' since it calls\n    # '_import_delegations()'.\n    repository_name = self.repository_updater.repository_name\n    tuf.keydb.clear_keydb(repository_name)\n    tuf.roledb.clear_roledb(repository_name)\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 0)\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 0)\n\n    self.repository_updater._rebuild_key_and_role_db()\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n\n    # Take into account the number of keyids algorithms supported by default,\n    # which this test condition expects to be two (sha256 and sha512).\n    self.assertEqual(4 * 2, len(tuf.keydb._keydb_dict[repository_name]))\n\n    # Test: pass a role without delegations.\n    self.repository_updater._import_delegations('root')\n\n    # Verify that there was no change to the roledb and keydb dictionaries by\n    # checking the number of elements in the dictionaries.\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 4)\n    # Take into account the number of keyid hash algorithms, which this\n    # test condition expects to be two (for sha256 and sha512).\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4 * 2)\n\n    # Test: normal case, first level delegation.\n    self.repository_updater._import_delegations('targets')\n\n    self.assertEqual(len(tuf.roledb._roledb_dict[repository_name]), 5)\n    # The number of root keys (times the number of key hash algorithms) +\n    # delegation's key (+1 for its sha512 keyid).\n    self.assertEqual(len(tuf.keydb._keydb_dict[repository_name]), 4 * 2 + 2)\n\n    # Verify that roledb dictionary was added.\n    self.assertTrue('role1' in tuf.roledb._roledb_dict[repository_name])\n\n    # Verify that keydb dictionary was updated.\n    role1_signable = \\\n      securesystemslib.util.load_json_file(os.path.join(self.client_metadata_current,\n                                           'role1.json'))\n    keyids = []\n    for signature in role1_signable['signatures']:\n      keyids.append(signature['keyid'])\n\n    for keyid in keyids:\n      self.assertTrue(keyid in tuf.keydb._keydb_dict[repository_name])\n\n    # Verify that _import_delegations() ignores invalid keytypes in the 'keys'\n    # field of parent role's 'delegations'.\n    existing_keyid = keyids[0]\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'bad_keytype'\n    self.repository_updater._import_delegations('targets')\n\n    # Restore the keytype of 'existing_keyid'.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keytype'] = 'ed25519'\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated keys is malformed.\n    valid_keyval = self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval']\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['keys'][existing_keyid]['keyval'] = valid_keyval\n\n    # Verify that _import_delegations() raises an exception if one of the\n    # delegated roles is malformed.\n    self.repository_updater.metadata['current']['targets']\\\n      ['delegations']['roles'][0]['name'] = 1\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._import_delegations, 'targets')\n\n\n\n  def test_2__versioninfo_has_been_updated(self):\n    # Verify that the method returns 'False' if a versioninfo was not changed.\n    snapshot_filepath = os.path.join(self.client_metadata_current, 'snapshot.json')\n    snapshot_signable = securesystemslib.util.load_json_file(snapshot_filepath)\n    targets_versioninfo = snapshot_signable['signed']['meta']['targets.json']\n\n    self.assertFalse(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n    # Verify that the method returns 'True' if Root's version number changes.\n    targets_versioninfo['version'] = 8\n    self.assertTrue(self.repository_updater._versioninfo_has_been_updated('targets.json',\n                                                           targets_versioninfo))\n\n\n\n\n\n  def test_2__move_current_to_previous(self):\n    # Test case will consist of removing a metadata file from client's\n    # '{client_repository}/metadata/previous' directory, executing the method\n    # and then verifying that the 'previous' directory contains the snapshot\n    # file.\n    previous_snapshot_filepath = os.path.join(self.client_metadata_previous,\n                                              'snapshot.json')\n    os.remove(previous_snapshot_filepath)\n    self.assertFalse(os.path.exists(previous_snapshot_filepath))\n\n    # Verify that the current 'snapshot.json' is moved to the previous directory.\n    self.repository_updater._move_current_to_previous('snapshot')\n    self.assertTrue(os.path.exists(previous_snapshot_filepath))\n\n\n\n\n\n  def test_2__delete_metadata(self):\n    # This test will verify that 'root' metadata is never deleted.  When a role\n    # is deleted verify that the file is not present in the\n    # 'self.repository_updater.metadata' dictionary.\n    self.repository_updater._delete_metadata('root')\n    self.assertTrue('root' in self.repository_updater.metadata['current'])\n\n    self.repository_updater._delete_metadata('timestamp')\n    self.assertFalse('timestamp' in self.repository_updater.metadata['current'])\n\n\n\n\n\n  def test_2__ensure_not_expired(self):\n    # This test condition will verify that nothing is raised when a metadata\n    # file has a future expiration date.\n    root_metadata = self.repository_updater.metadata['current']['root']\n    self.repository_updater._ensure_not_expired(root_metadata, 'root')\n\n    # 'tuf.exceptions.ExpiredMetadataError' should be raised in this next test condition,\n    # because the expiration_date has expired by 10 seconds.\n    expires = tuf.formats.unix_timestamp_to_datetime(int(time.time() - 10))\n    expires = expires.isoformat() + 'Z'\n    root_metadata['expires'] = expires\n\n    # Ensure the 'expires' value of the root file is valid by checking the\n    # the formats of the 'root.json' object.\n    self.assertTrue(tuf.formats.ROOT_SCHEMA.matches(root_metadata))\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater._ensure_not_expired,\n                      root_metadata, 'root')\n\n\n\n\n\n  def test_3__update_metadata(self):\n    # Setup\n    # _update_metadata() downloads, verifies, and installs the specified\n    # metadata role.  Remove knowledge of currently installed metadata and\n    # verify that they are re-installed after calling _update_metadata().\n\n    # This is the default metadata that we would create for the timestamp role,\n    # because it has no signed metadata for itself.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    # This is the upper bound length for Targets metadata.\n    DEFAULT_TARGETS_FILELENGTH = tuf.settings.DEFAULT_TARGETS_REQUIRED_LENGTH\n\n    # Save the versioninfo of 'targets.json,' needed later when re-installing\n    # with _update_metadata().\n    targets_versioninfo = \\\n      self.repository_updater.metadata['current']['snapshot']['meta']\\\n                                      ['targets.json']\n\n    # Remove the currently installed metadata from the store and disk.  Verify\n    # that the metadata dictionary is re-populated after calling\n    # _update_metadata().\n    del self.repository_updater.metadata['current']['timestamp']\n    del self.repository_updater.metadata['current']['targets']\n\n    timestamp_filepath = \\\n      os.path.join(self.client_metadata_current, 'timestamp.json')\n    targets_filepath = os.path.join(self.client_metadata_current, 'targets.json')\n    root_filepath = os.path.join(self.client_metadata_current, 'root.json')\n    os.remove(timestamp_filepath)\n    os.remove(targets_filepath)\n\n    # Test: normal case.\n    # Verify 'timestamp.json' is properly installed.\n    self.assertFalse('timestamp' in self.repository_updater.metadata)\n\n    logger.info('\\nroleinfo: ' + repr(tuf.roledb.get_rolenames(self.repository_name)))\n    self.repository_updater._update_metadata('timestamp',\n                                             DEFAULT_TIMESTAMP_FILELENGTH)\n    self.assertTrue('timestamp' in self.repository_updater.metadata['current'])\n    os.path.exists(timestamp_filepath)\n\n    # Verify 'targets.json' is properly installed.\n    self.assertFalse('targets' in self.repository_updater.metadata['current'])\n    self.repository_updater._update_metadata('targets',\n                                DEFAULT_TARGETS_FILELENGTH,\n                                targets_versioninfo['version'])\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n\n    targets_signable = securesystemslib.util.load_json_file(targets_filepath)\n    loaded_targets_version = targets_signable['signed']['version']\n    self.assertEqual(targets_versioninfo['version'], loaded_targets_version)\n\n    # Test: Invalid / untrusted version numbers.\n    # Invalid version number for 'targets.json'.\n    self.assertRaises(tuf.exceptions.NoWorkingMirrorError,\n        self.repository_updater._update_metadata,\n        'targets', DEFAULT_TARGETS_FILELENGTH, 88)\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH, 88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in six.itervalues(e.mirror_errors):\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n    # Verify that the specific exception raised is correct for the previous\n    # case.  The version number is checked, so the specific error in\n    # this case should be 'tuf.exceptions.BadVersionNumberError'.\n    try:\n      self.repository_updater._update_metadata('targets',\n                                               DEFAULT_TARGETS_FILELENGTH,\n                                               88)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      for mirror_error in six.itervalues(e.mirror_errors):\n        assert isinstance(mirror_error, tuf.exceptions.BadVersionNumberError)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError composed of BadVersionNumberErrors')\n\n\n\n\n\n  def test_3__get_metadata_file(self):\n\n    '''\n    This test focuses on making sure that the updater rejects unknown or\n    badly-formatted TUF specification version numbers....\n    '''\n\n    # Make note of the correct supported TUF specification version.\n    correct_specification_version = tuf.SPECIFICATION_VERSION\n\n    # Change it long enough to write new metadata.\n    tuf.SPECIFICATION_VERSION = '0.9.0'\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # Change the supported TUF specification version back to what it should be\n    # so that we can parse the metadata and see that the spec version in the\n    # metadata does not match the code's expected spec version.\n    tuf.SPECIFICATION_VERSION = correct_specification_version\n\n    upperbound_filelength = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n    try:\n      self.repository_updater._get_metadata_file('timestamp', 'timestamp.json',\n      upperbound_filelength, 1)\n\n    except tuf.exceptions.NoWorkingMirrorError as e:\n      # Note that this test provides a piece of metadata which would fail to\n      # be accepted -- with a different error -- if the specification version\n      # number were not a problem.\n      for mirror_error in six.itervalues(e.mirror_errors):\n        assert isinstance(\n            mirror_error, tuf.exceptions.UnsupportedSpecificationError)\n\n    else:\n      self.fail(\n          'Expected a failure to verify metadata when the metadata had a '\n          'specification version number that was unexpected.  '\n          'No error was raised.')\n\n\n\n\n\n  def test_3__update_metadata_if_changed(self):\n    # Setup.\n    # The client repository is initially loaded with only four top-level roles.\n    # Verify that the metadata store contains the metadata of only these four\n    # roles before updating the metadata of 'targets.json'.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Test: normal case.  Update 'targets.json'.  The version number should not\n    # change.\n    self.repository_updater._update_metadata_if_changed('targets')\n\n    # Verify the current version of 'targets.json' has not changed.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 1)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = os.path.join(self.repository_directory, 'targets', 'file3.txt')\n\n    repository.targets.add_target(target3)\n    repository.root.version = repository.root.version + 1\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update 'targets.json' and verify that the client's current 'targets.json'\n    # has been updated.  'timestamp' and 'snapshot' must be manually updated\n    # so that new 'targets' can be recognized.\n    DEFAULT_TIMESTAMP_FILELENGTH = tuf.settings.DEFAULT_TIMESTAMP_REQUIRED_LENGTH\n\n    self.repository_updater._update_metadata('timestamp', DEFAULT_TIMESTAMP_FILELENGTH)\n    self.repository_updater._update_metadata_if_changed('snapshot', 'timestamp')\n    self.repository_updater._update_metadata_if_changed('targets')\n    self.repository_updater._update_metadata_if_changed('root')\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    self.assertTrue(os.path.exists(targets_path))\n    self.assertTrue(self.repository_updater.metadata['current']['targets'])\n    self.assertEqual(self.repository_updater.metadata['current']['targets']['version'], 2)\n\n    # Test for an invalid 'referenced_metadata' argument.\n    self.assertRaises(tuf.exceptions.RepositoryError,\n        self.repository_updater._update_metadata_if_changed, 'snapshot', 'bad_role')\n\n\n\n  def test_3__targets_of_role(self):\n    # Setup.\n    # Extract the list of targets from 'targets.json', to be compared to what\n    # is returned by _targets_of_role('targets').\n    targets_in_metadata = \\\n      self.repository_updater.metadata['current']['targets']['targets']\n\n    # Test: normal case.\n    targetinfos_list = self.repository_updater._targets_of_role('targets')\n\n    # Verify that the list of targets was returned, and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos_list))\n    for targetinfo in targetinfos_list:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in six.iteritems(targets_in_metadata))\n\n\n\n\n\n  def test_4_refresh(self):\n    # This unit test is based on adding an extra target file to the\n    # server and rebuilding all server-side metadata.  All top-level metadata\n    # should be updated when the client calls refresh().\n\n    # First verify that an expired root metadata is updated.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.repository_updater.refresh()\n\n    # Second, verify that expired root metadata is not updated if\n    # 'unsafely_update_root_if_necessary' is explicitly set to 'False'.\n    expired_date = '1960-01-01T12:00:00Z'\n    self.repository_updater.metadata['current']['root']['expires'] = expired_date\n    self.assertRaises(tuf.exceptions.ExpiredMetadataError,\n                      self.repository_updater.refresh,\n                      unsafely_update_root_if_necessary=False)\n\n    repository = repo_tool.load_repository(self.repository_directory)\n    target3 = os.path.join(self.repository_directory, 'targets', 'file3.txt')\n\n    repository.targets.add_target(target3)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Reference 'self.Repository.metadata['current']['targets']'.  Ensure\n    # 'target3' is not already specified.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    self.assertFalse(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the roles to be modified.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 1)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 1)\n\n    # Test: normal case.  'targes.json' should now specify 'target3', and the\n    # following top-level metadata should have also been updated:\n    # 'snapshot.json' and 'timestamp.json'.\n    self.repository_updater.refresh()\n\n    # Verify that the client's metadata was updated.\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n    targets_directory = os.path.join(self.repository_directory, 'targets')\n    target3 = target3[len(targets_directory) + 1:]\n    self.assertTrue(target3 in targets_metadata['targets'])\n\n    # Verify the expected version numbers of the updated roles.\n    self.assertEqual(self.repository_updater.metadata['current']['targets']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['snapshot']\\\n                                                    ['version'], 2)\n    self.assertEqual(self.repository_updater.metadata['current']['timestamp']\\\n                                                    ['version'], 2)\n\n\n\n\n\n  def test_4__refresh_targets_metadata(self):\n    # Setup.\n    # It is assumed that the client repository has only loaded the top-level\n    # metadata.  Refresh the 'targets.json' metadata, including all delegated\n    # roles (i.e., the client should add the missing 'role1.json' metadata.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 4)\n\n    # Test: normal case.\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n    # Verify that client's metadata files were refreshed successfully.\n    self.assertEqual(len(self.repository_updater.metadata['current']), 6)\n\n    # Test for non-existing rolename.\n    self.repository_updater._refresh_targets_metadata('bad_rolename',\n        refresh_all_delegated_roles=False)\n\n    # Test that non-json metadata in Snapshot is ignored.\n    self.repository_updater.metadata['current']['snapshot']['meta']['bad_role.xml'] = {}\n    self.repository_updater._refresh_targets_metadata(refresh_all_delegated_roles=True)\n\n\n\n  def test_5_all_targets(self):\n   # Setup\n   # As with '_refresh_targets_metadata()',\n\n   # Update top-level metadata before calling one of the \"targets\" methods, as\n   # recommended by 'updater.py'.\n   self.repository_updater.refresh()\n\n   # Test: normal case.\n   all_targets = self.repository_updater.all_targets()\n\n   # Verify format of 'all_targets', it should correspond to\n   # 'TARGETINFOS_SCHEMA'.\n   self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(all_targets))\n\n   # Verify that there is a correct number of records in 'all_targets' list,\n   # and the expected filepaths specified in the metadata.  On the targets\n   # directory of the repository, there should be 3 target files (2 of\n   # which are specified by 'targets.json'.)  The delegated role 'role1'\n   # specifies 1 target file.  The expected total number targets in\n   # 'all_targets' should be 3.\n   self.assertEqual(len(all_targets), 3)\n\n   target_filepaths = []\n   for target in all_targets:\n    target_filepaths.append(target['filepath'])\n\n   self.assertTrue('file1.txt' in target_filepaths)\n   self.assertTrue('file2.txt' in target_filepaths)\n   self.assertTrue('file3.txt' in target_filepaths)\n\n\n\n\n\n  def test_5_targets_of_role(self):\n    # Setup\n    # Remove knowledge of 'targets.json' from the metadata store.\n    self.repository_updater.metadata['current']['targets']\n\n    # Remove the metadata of the delegated roles.\n    #shutil.rmtree(os.path.join(self.client_metadata, 'targets'))\n    os.remove(os.path.join(self.client_metadata_current, 'targets.json'))\n\n    # Extract the target files specified by the delegated role, 'role1.json',\n    # as available on the server-side version of the role.\n    role1_filepath = os.path.join(self.repository_directory, 'metadata',\n                                  'role1.json')\n    role1_signable = securesystemslib.util.load_json_file(role1_filepath)\n    expected_targets = role1_signable['signed']['targets']\n\n\n    # Test: normal case.\n    targetinfos = self.repository_updater.targets_of_role('role1')\n\n    # Verify that the expected role files were downloaded and installed.\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets.json'))\n    os.path.exists(os.path.join(self.client_metadata_current, 'targets',\n                   'role1.json'))\n    self.assertTrue('targets' in self.repository_updater.metadata['current'])\n    self.assertTrue('role1' in self.repository_updater.metadata['current'])\n\n    #  Verify that list of targets was returned and that it contains valid\n    # target files.\n    self.assertTrue(tuf.formats.TARGETINFOS_SCHEMA.matches(targetinfos))\n    for targetinfo in targetinfos:\n      self.assertTrue((targetinfo['filepath'], targetinfo['fileinfo']) in six.iteritems(expected_targets))\n\n    # Test: Invalid arguments.\n    # targets_of_role() expected a string rolename.\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater.targets_of_role,\n                      8)\n    self.assertRaises(tuf.exceptions.UnknownRoleError, self.repository_updater.targets_of_role,\n                      'unknown_rolename')\n\n\n\n\n\n  def test_6_get_one_valid_targetinfo(self):\n    # Setup\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n    SERVER_PORT = random.randint(30000, 45000)\n    command = ['python', '-m', 'tuf.scripts.simple_server', str(SERVER_PORT)]\n    server_process = subprocess.Popen(command, stderr=subprocess.PIPE)\n\n    # NOTE: Following error is raised if a delay is not long enough:\n    # <urlopen error [Errno 111] Connection refused>\n    # or, on Windows:\n    # Failed to establish a new connection: [Errno 111] Connection refused'\n    # While 0.3s has consistently worked on Travis and local builds, it led to\n    # occasional failures in AppVeyor builds, so increasing this to 2s, sadly.\n    time.sleep(2)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = \\\n      'http://localhost:' + str(SERVER_PORT) + repository_basepath\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets',\n        'confined_target_dirs': ['']}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Extract the file information of the targets specified in 'targets.json'.\n    self.repository_updater.refresh()\n    targets_metadata = self.repository_updater.metadata['current']['targets']\n\n    target_files = targets_metadata['targets']\n    # Extract random target from 'target_files', which will be compared to what\n    # is returned by get_one_valid_targetinfo().  Restore the popped target\n    # (dict value stored in the metadata store) so that it can be found later.\n    filepath, fileinfo = target_files.popitem()\n    target_files[filepath] = fileinfo\n\n    target_targetinfo = self.repository_updater.get_one_valid_targetinfo(filepath)\n    self.assertTrue(tuf.formats.TARGETINFO_SCHEMA.matches(target_targetinfo))\n    self.assertEqual(target_targetinfo['filepath'], filepath)\n    self.assertEqual(target_targetinfo['fileinfo'], fileinfo)\n\n    # Test: invalid target path.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        self.repository_updater.get_one_valid_targetinfo,\n        self.random_path().lstrip(os.sep).lstrip('/'))\n\n    # Test updater.get_one_valid_targetinfo() backtracking behavior (enabled by\n    # default.)\n    targets_directory = os.path.join(self.repository_directory, 'targets')\n    foo_directory = os.path.join(targets_directory, 'foo')\n    foo_pattern = 'foo/foo*.tar.gz'\n    os.makedirs(foo_directory)\n\n    foo_package = os.path.join(foo_directory, 'foo1.1.tar.gz')\n    with open(foo_package, 'wb') as file_object:\n      file_object.write(b'new release')\n\n    # Modify delegations on the remote repository to test backtracking behavior.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n    repository.targets('role4').add_target(foo_package)\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n\n    # updater.get_one_valid_targetinfo() should find 'foo1.1.tar.gz' by\n    # backtracking to 'role3'.  'role2' allows backtracking.\n    self.repository_updater.refresh()\n    self.repository_updater.get_one_valid_targetinfo('foo/foo1.1.tar.gz')\n\n    # A leading path separator is disallowed.\n    self.assertRaises(tuf.exceptions.FormatError,\n    self.repository_updater.get_one_valid_targetinfo, '/foo/foo1.1.tar.gz')\n\n    # Test when 'role2' does *not* allow backtracking.  If 'foo/foo1.1.tar.gz'\n    # is not provided by the authoritative 'role2',\n    # updater.get_one_valid_targetinfo() should return a\n    # 'tuf.exceptions.UnknownTargetError' exception.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    repository.targets.revoke('role3')\n    repository.targets.revoke('role4')\n\n    # Ensure we delegate in trusted order (i.e., 'role2' has higher priority.)\n    repository.targets.delegate('role3', [self.role_keys['targets']['public']],\n        [foo_pattern], terminating=True, list_of_targets=[])\n\n    repository.targets.delegate('role4', [self.role_keys['targets']['public']],\n        [foo_pattern], list_of_targets=[foo_package])\n\n    repository.targets('role3').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets('role4').load_signing_key(self.role_keys['targets']['private'])\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Verify that 'tuf.exceptions.UnknownTargetError' is raised by\n    # updater.get_one_valid_targetinfo().\n    self.repository_updater.refresh()\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n                      self.repository_updater.get_one_valid_targetinfo,\n                      'foo/foo1.1.tar.gz')\n\n    # Verify that a 'tuf.exceptions.FormatError' is raised for delegated paths\n    # that contain a leading path separator.\n    self.assertRaises(tuf.exceptions.FormatError,\n        self.repository_updater.get_one_valid_targetinfo,\n        '/foo/foo1.1.tar.gz')\n\n    server_process.kill()\n\n\n\n\n  def test_6_download_target(self):\n    # Create temporary directory (destination directory of downloaded targets)\n    # that will be passed as an argument to 'download_target()'.\n    destination_directory = self.make_temp_directory()\n    target_filepaths = \\\n      list(self.repository_updater.metadata['current']['targets']['targets'].keys())\n\n    # Test: normal case.\n    # Get the target info, which is an argument to 'download_target()'.\n\n    # 'target_filepaths' is expected to have at least two targets.  The first\n    # target will be used to test against download_target().  The second\n    # will be used to test against download_target() and a repository with\n    # 'consistent_snapshot' set to True.\n    target_filepath1 = target_filepaths.pop()\n    targetinfo = self.repository_updater.get_one_valid_targetinfo(target_filepath1)\n    self.repository_updater.download_target(targetinfo,\n                                            destination_directory)\n\n    download_filepath = \\\n      os.path.join(destination_directory, target_filepath1.lstrip('/'))\n    self.assertTrue(os.path.exists(download_filepath))\n    length, hashes = \\\n      securesystemslib.util.get_file_details(download_filepath,\n        securesystemslib.settings.HASH_ALGORITHMS)\n    download_targetfileinfo = tuf.formats.make_fileinfo(length, hashes)\n\n    # Add any 'custom' data from the repository's target fileinfo to the\n    # 'download_targetfileinfo' object being tested.\n    if 'custom' in targetinfo['fileinfo']:\n      download_targetfileinfo['custom'] = targetinfo['fileinfo']['custom']\n\n    self.assertEqual(targetinfo['fileinfo'], download_targetfileinfo)\n\n    # Test when consistent snapshots is set.  First, create a valid\n    # repository with consistent snapshots set (root.json contains a\n    # \"consistent_snapshot\" entry that the updater uses to correctly fetch\n    # snapshots.  The updater expects the existence of\n    # '<version_number>.filename' files if root.json sets 'consistent_snapshot\n    # = True'.\n\n    # The repository must be rewritten with 'consistent_snapshot' set.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    # Write metadata for all the top-level roles , since consistent snapshot\n    # is now being set to true (i.e., the pre-generated repository isn't set\n    # to support consistent snapshots.  A new version of targets.json is needed\n    # to ensure <digest>.filename target files are written to disk.\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.root.load_signing_key(self.role_keys['root']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n\n    repository.writeall(consistent_snapshot=True)\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # And ensure the client has the latest top-level metadata.\n    self.repository_updater.refresh()\n\n    target_filepath2 = target_filepaths.pop()\n    targetinfo2 = self.repository_updater.get_one_valid_targetinfo(target_filepath2)\n    self.repository_updater.download_target(targetinfo2,\n                                            destination_directory)\n\n    # Test for a destination that cannot be written to (apart from a target\n    # file that already exists at the destination) and which raises an\n    # exception.\n    bad_destination_directory = 'bad' * 2000\n\n    try:\n      self.repository_updater.download_target(targetinfo, bad_destination_directory)\n\n    except OSError as e:\n      self.assertTrue(e.errno == errno.ENAMETOOLONG or e.errno == errno.ENOENT)\n\n    else:\n      self.fail('Expected an OSError of type ENAMETOOLONG or ENOENT')\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.download_target,\n                      targetinfo, 8)\n\n    # Test:\n    # Attempt a file download of a valid target, however, a download exception\n    # occurs because the target is not within the mirror's confined target\n    # directories.  Adjust mirrors dictionary, so that 'confined_target_dirs'\n    # field contains at least one confined target and excludes needed target\n    # file.\n    mirrors = self.repository_updater.mirrors\n    for mirror_name, mirror_info in six.iteritems(mirrors):\n      mirrors[mirror_name]['confined_target_dirs'] = [self.random_path()]\n\n    try:\n      self.repository_updater.download_target(targetinfo,\n                                              destination_directory)\n\n    except tuf.exceptions.NoWorkingMirrorError as exception:\n      # Ensure that no mirrors were found due to mismatch in confined target\n      # directories.  get_list_of_mirrors() returns an empty list in this case,\n      # which does not generate specific exception errors.\n      self.assertEqual(len(exception.mirror_errors), 0)\n\n    else:\n      self.fail(\n          'Expected a NoWorkingMirrorError with zero mirror errors in it.')\n\n\n\n\n\n  def test_7_updated_targets(self):\n    # Verify that the list of targets returned by updated_targets() contains\n    # all the files that need to be updated, these files include modified and\n    # new target files.  Also, confirm that files that need not to be updated\n    # are absent from the list.\n    # Setup\n\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n    SERVER_PORT = random.randint(30000, 45000)\n    command = ['python', '-m', 'tuf.scripts.simple_server', str(SERVER_PORT)]\n    server_process = subprocess.Popen(command, stderr=subprocess.PIPE)\n\n    # NOTE: Following error is raised if a delay is not long enough to allow\n    # the server process to set up and start listening:\n    #     <urlopen error [Errno 111] Connection refused>\n    # or, on Windows:\n    #     Failed to establish a new connection: [Errno 111] Connection refused'\n    # While 0.3s has consistently worked on Travis and local builds, it led to\n    # occasional failures in AppVeyor builds, so increasing this to 2s, sadly.\n    time.sleep(2)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = \\\n      'http://localhost:' + str(SERVER_PORT) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets',\n        'confined_target_dirs': ['']}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory which will hold client's target files.\n    destination_directory = self.make_temp_directory()\n\n    # Get the list of target files.  It will be used as an argument to the\n    # 'updated_targets()' function.\n    all_targets = self.repository_updater.all_targets()\n\n    # Test for duplicates and targets in the root directory of the repository.\n    additional_target = all_targets[0].copy()\n    all_targets.append(additional_target)\n    additional_target_in_root_directory = additional_target.copy()\n    additional_target_in_root_directory['filepath'] = 'file1.txt'\n    all_targets.append(additional_target_in_root_directory)\n\n    #  At this point client needs to update and download all targets.\n    # Test: normal cases.\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    all_targets = self.repository_updater.all_targets()\n\n    # Assumed the pre-generated repository specifies two target files in\n    # 'targets.json' and one delegated target file in 'role1.json'.\n    self.assertEqual(len(updated_targets), 3)\n\n    # Test: download one of the targets.\n    download_target = copy.deepcopy(updated_targets).pop()\n    self.repository_updater.download_target(download_target,\n                                            destination_directory)\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 2)\n\n    # Test: download all the targets.\n    for download_target in all_targets:\n      self.repository_updater.download_target(download_target,\n                                              destination_directory)\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n\n    self.assertEqual(len(updated_targets), 0)\n\n\n    # Test: Invalid arguments.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      8, destination_directory)\n\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n                      self.repository_updater.updated_targets,\n                      all_targets, 8)\n\n    # Modify one target file on the remote repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(target1)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    length, hashes = securesystemslib.util.get_file_details(target1)\n\n    repository.targets.add_target(target1)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Ensure the client has up-to-date metadata.\n    self.repository_updater.refresh()\n\n    # Verify that the new target file is considered updated.\n    all_targets = self.repository_updater.all_targets()\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets, destination_directory)\n    self.assertEqual(len(updated_targets), 1)\n\n    server_process.kill()\n\n\n\n\n  def test_8_remove_obsolete_targets(self):\n    # Setup.\n    # Unlike some of the other tests, start up a fresh server here.\n    # The SimpleHTTPServer started in the setupclass has a tendency to\n    # timeout in Windows after a few tests.\n    SERVER_PORT = random.randint(30000, 45000)\n    command = ['python', '-m', 'tuf.scripts.simple_server', str(SERVER_PORT)]\n    server_process = subprocess.Popen(command, stderr=subprocess.PIPE)\n\n    # NOTE: Following error is raised if a delay is not long enough to allow\n    # the server process to set up and start listening:\n    #     <urlopen error [Errno 111] Connection refused>\n    # or, on Windows:\n    #     Failed to establish a new connection: [Errno 111] Connection refused'\n    # While 0.3s has consistently worked on Travis and local builds, it led to\n    # occasional failures in AppVeyor builds, so increasing this to 2s, sadly.\n    time.sleep(2)\n\n    # 'path/to/tmp/repository' -> 'localhost:8001/tmp/repository'.\n    repository_basepath = self.repository_directory[len(os.getcwd()):]\n    url_prefix = \\\n      'http://localhost:' + str(SERVER_PORT) + repository_basepath\n\n    # Setting 'tuf.settings.repository_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.client_directory\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets',\n        'confined_target_dirs': ['']}}\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(self.repository_name,\n        self.repository_mirrors)\n\n    # Create temporary directory that will hold the client's target files.\n    destination_directory = self.make_temp_directory()\n\n    #  Populate 'destination_direction' with all target files.\n    all_targets = self.repository_updater.all_targets()\n\n    self.assertEqual(len(os.listdir(destination_directory)), 0)\n\n    for target in all_targets:\n      self.repository_updater.download_target(target, destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n\n    # Remove two target files from the server's repository.\n    repository = repo_tool.load_repository(self.repository_directory)\n    target1 = os.path.join(self.repository_directory, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory, 'metadata.staged'),\n                    os.path.join(self.repository_directory, 'metadata'))\n\n    # Update client's metadata.\n    self.repository_updater.refresh()\n\n    # Test: normal case.\n    # Verify number of target files in 'destination_directory' (should be 1\n    # after the update made to the remote repository), and call\n    # 'remove_obsolete_targets()'.\n    all_targets = self.repository_updater.all_targets()\n\n    updated_targets = \\\n      self.repository_updater.updated_targets(all_targets,\n                                              destination_directory)\n\n    for updated_target in updated_targets:\n      self.repository_updater.download_target(updated_target,\n                                              destination_directory)\n\n    self.assertEqual(len(os.listdir(destination_directory)), 3)\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    #  Verify that, if there are no obsolete files, the number of files\n    #  in 'destination_directory' remains the same.\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n    self.assertEqual(len(os.listdir(destination_directory)), 2)\n\n    # Test coverage for a destination path that causes an exception not due\n    # to an already removed target.\n    bad_destination_directory = 'bad' * 2000\n    self.repository_updater.remove_obsolete_targets(bad_destination_directory)\n\n    # Test coverage for a target that is not specified in current metadata.\n    del self.repository_updater.metadata['current']['targets']['targets']['file2.txt']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    # Test coverage for a role that doesn't exist in the previously trusted set\n    # of metadata.\n    del self.repository_updater.metadata['previous']['targets']\n    self.repository_updater.remove_obsolete_targets(destination_directory)\n\n    server_process.kill()\n\n\n\n  def test_9__get_target_hash(self):\n    # Test normal case.\n    # Test target filepaths with ascii and non-ascii characters.\n    expected_target_hashes = {\n      '/file1.txt': 'e3a3d89eb3b70ce3fbce6017d7b8c12d4abd5635427a0e8a238f53157df85b3d',\n      '/Jalape\\xc3\\xb1o': '78bfd5c314680545eb48ecad508aceb861f8d6e680f4fe1b791da45c298cda88'\n    }\n    for filepath, target_hash in six.iteritems(expected_target_hashes):\n      self.assertTrue(tuf.formats.RELPATH_SCHEMA.matches(filepath))\n      self.assertTrue(securesystemslib.formats.HASH_SCHEMA.matches(target_hash))\n      self.assertEqual(self.repository_updater._get_target_hash(filepath), target_hash)\n\n    # Test for improperly formatted argument.\n    #self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._get_target_hash, 8)\n\n\n\n\n  def test_10__hard_check_file_length(self):\n    # Test for exception if file object is not equal to trusted file length.\n    temp_file_object = tempfile.TemporaryFile()\n    temp_file_object.write(b'X')\n    temp_file_object.seek(0)\n    self.assertRaises(tuf.exceptions.DownloadLengthMismatchError,\n                     self.repository_updater._hard_check_file_length,\n                     temp_file_object, 10)\n\n\n\n\n\n  def test_10__soft_check_file_length(self):\n    # Test for exception if file object is not equal to trusted file length.\n    temp_file_object = tempfile.TemporaryFile()\n    temp_file_object.write(b'XXX')\n    temp_file_object.seek(0)\n    self.assertRaises(tuf.exceptions.DownloadLengthMismatchError,\n                     self.repository_updater._soft_check_file_length,\n                     temp_file_object, 1)\n\n    # Verify that an exception is not raised if the file length <= the observed\n    # file length.\n    temp_file_object.seek(0)\n    self.repository_updater._soft_check_file_length(temp_file_object, 3)\n    temp_file_object.seek(0)\n    self.repository_updater._soft_check_file_length(temp_file_object, 4)\n\n\n\n\n  def test_10__targets_of_role(self):\n    # Test for non-existent role.\n    self.assertRaises(tuf.exceptions.UnknownRoleError,\n                      self.repository_updater._targets_of_role,\n                      'non-existent-role')\n\n    # Test for role that hasn't been loaded yet.\n    del self.repository_updater.metadata['current']['targets']\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets',\n                                                        skip_refresh=True)), 0)\n\n    # 'targets.json' tracks two targets.\n    self.assertEqual(len(self.repository_updater._targets_of_role('targets')),\n                     2)\n\n\n\n  def test_10__preorder_depth_first_walk(self):\n\n    # Test that infinite loop is prevented if the target file is not found and\n    # the max number of delegations is reached.\n    valid_max_number_of_delegations = tuf.settings.MAX_NUMBER_OF_DELEGATIONS\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = 0\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('unknown.txt'))\n\n    # Reset the setting for max number of delegations so that subsequent unit\n    # tests reference the expected setting.\n    tuf.settings.MAX_NUMBER_OF_DELEGATIONS = valid_max_number_of_delegations\n\n    # Attempt to create a circular delegation, where role1 performs a\n    # delegation to the top-level Targets role.  The updater should ignore the\n    # delegation and not raise an exception.\n    targets_path = os.path.join(self.client_metadata_current, 'targets.json')\n    targets_metadata = securesystemslib.util.load_json_file(targets_path)\n    targets_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(targets_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(targets_metadata))\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['name'] = 'targets'\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    role2_path = os.path.join(self.client_metadata_current, 'role2.json')\n    role2_metadata = securesystemslib.util.load_json_file(role2_path)\n    role2_metadata['signed']['delegations']['roles'] = role1_metadata['signed']['delegations']['roles']\n    role2_metadata['signed']['delegations']['roles'][0]['paths'] = ['/file8.txt']\n    with open(role2_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role2_metadata))\n\n    logger.debug('attempting circular delegation')\n    self.assertEqual(None, self.repository_updater._preorder_depth_first_walk('/file8.txt'))\n\n\n\n\n\n\n  def test_10__visit_child_role(self):\n    # Call _visit_child_role and test the dict keys: 'paths',\n    # 'path_hash_prefixes', and if both are missing.\n\n    targets_role = self.repository_updater.metadata['current']['targets']\n    targets_role['delegations']['roles'][0]['paths'] = ['/*.txt', '/target.exe']\n    child_role = targets_role['delegations']['roles'][0]\n\n    role1_path = os.path.join(self.client_metadata_current, 'role1.json')\n    role1_metadata = securesystemslib.util.load_json_file(role1_path)\n    role1_metadata['signed']['delegations']['roles'][0]['paths'] = ['/*.exe']\n    with open(role1_path, 'wb') as file_object:\n      file_object.write(repo_lib._get_written_metadata(role1_metadata))\n\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/target.exe'), child_role['name'])\n\n    # Test for a valid path hash prefix...\n    child_role['path_hash_prefixes'] = ['8baf']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), child_role['name'])\n\n    # ... and an invalid one, as well.\n    child_role['path_hash_prefixes'] = ['badd']\n    self.assertEqual(self.repository_updater._visit_child_role(child_role,\n        '/file3.txt'), None)\n\n    # Test for a forbidden target.\n    del child_role['path_hash_prefixes']\n    self.repository_updater._visit_child_role(child_role, '/forbidden.tgz')\n\n    # Verify that unequal path_hash_prefixes are skipped.\n    child_role['path_hash_prefixes'] = ['bad', 'bad']\n    self.assertEqual(None, self.repository_updater._visit_child_role(child_role,\n        '/unknown.exe'))\n\n    # Test if both 'path' and 'path_hash_prefixes' are missing.\n    del child_role['paths']\n    del child_role['path_hash_prefixes']\n    self.assertRaises(securesystemslib.exceptions.FormatError, self.repository_updater._visit_child_role,\n        child_role, child_role['name'])\n\n\n\n  def test_11__verify_uncompressed_metadata_file(self):\n    # Test for invalid metadata content.\n    metadata_file_object = tempfile.TemporaryFile()\n    metadata_file_object.write(b'X')\n    metadata_file_object.seek(0)\n\n    self.assertRaises(tuf.exceptions.InvalidMetadataJSONError,\n        self.repository_updater._verify_uncompressed_metadata_file,\n        metadata_file_object, 'root')\n\n\n\n  def test_12__verify_root_chain_link(self):\n    # Test for an invalid signature in the chain link.\n    # current = (i.e., 1.root.json)\n    # next = signable for the next metadata in the chain (i.e., 2.root.json)\n    rolename = 'root'\n    current_root = self.repository_updater.metadata['current']['root']\n\n    targets_path = os.path.join(self.repository_directory, 'metadata', 'targets.json')\n\n    # 'next_invalid_root' is a Targets signable, as written to disk.\n    # We use the Targets metadata here to ensure the signatures are invalid.\n    next_invalid_root = securesystemslib.util.load_json_file(targets_path)\n\n    self.assertRaises(securesystemslib.exceptions.BadSignatureError,\n        self.repository_updater._verify_root_chain_link, rolename, current_root,\n        next_invalid_root)\n\n\n\n  def test_13__get_file(self):\n    # Test for an \"unsafe\" download, where the file is downloaded up to\n    # a required length (and no more).  The \"safe\" download approach\n    # downloads an exact required length.\n    targets_path = os.path.join(self.repository_directory, 'metadata', 'targets.json')\n\n    file_size, file_hashes = securesystemslib.util.get_file_details(targets_path)\n    file_type = 'meta'\n\n    def verify_target_file(targets_path):\n      # Every target file must have its length and hashes inspected.\n      self.repository_updater._hard_check_file_length(targets_path, file_size)\n      self.repository_updater._check_hashes(targets_path, file_hashes)\n\n    self.repository_updater._get_file('targets.json', verify_target_file,\n        file_type, file_size, download_safely=True)\n\n    self.repository_updater._get_file('targets.json', verify_target_file,\n        file_type, file_size, download_safely=False)\n\n  def test_14__targets_of_role(self):\n    # Test case where a list of targets is given.  By default, the 'targets'\n    # parameter is None.\n    targets = [{'filepath': 'file1.txt', 'fileinfo': {'length': 1, 'hashes': {'sha256': 'abc'}}}]\n    self.repository_updater._targets_of_role('targets',\n        targets=targets, skip_refresh=False)\n\n\n\n\nclass TestMultiRepoUpdater(unittest_toolbox.Modified_TestCase):\n\n  def setUp(self):\n    # We are inheriting from custom class.\n    unittest_toolbox.Modified_TestCase.setUp(self)\n\n    self.temporary_directory = tempfile.mkdtemp(dir=os.getcwd())\n\n    # Copy the original repository files provided in the test folder so that\n    # any modifications made to repository files are restricted to the copies.\n    # The 'repository_data' directory is expected to exist in 'tuf/tests/'.\n    original_repository_files = os.path.join(os.getcwd(), 'repository_data')\n\n    self.temporary_repository_root = self.make_temp_directory(directory=\n        self.temporary_directory)\n\n    # The original repository, keystore, and client directories will be copied\n    # for each test case.\n    original_repository = os.path.join(original_repository_files, 'repository')\n    original_client = os.path.join(original_repository_files, 'client', 'test_repository1')\n    original_keystore = os.path.join(original_repository_files, 'keystore')\n    original_map_file = os.path.join(original_repository_files, 'map.json')\n\n    # Save references to the often-needed client repository directories.\n    # Test cases need these references to access metadata and target files.\n    self.repository_directory = os.path.join(self.temporary_repository_root,\n        'repository_server1')\n    self.repository_directory2 = os.path.join(self.temporary_repository_root,\n        'repository_server2')\n\n    # Setting 'tuf.settings.repositories_directory' with the temporary client\n    # directory copied from the original repository files.\n    tuf.settings.repositories_directory = self.temporary_repository_root\n\n    repository_name = 'test_repository1'\n    repository_name2 = 'test_repository2'\n\n    self.client_directory = os.path.join(self.temporary_repository_root,\n        repository_name)\n    self.client_directory2 = os.path.join(self.temporary_repository_root,\n        repository_name2)\n\n    self.keystore_directory = os.path.join(self.temporary_repository_root,\n        'keystore')\n    self.map_file = os.path.join(self.client_directory, 'map.json')\n    self.map_file2 = os.path.join(self.client_directory2, 'map.json')\n\n    # Copy the original 'repository', 'client', and 'keystore' directories\n    # to the temporary repository the test cases can use.\n    shutil.copytree(original_repository, self.repository_directory)\n    shutil.copytree(original_repository, self.repository_directory2)\n    shutil.copytree(original_client, self.client_directory)\n    shutil.copytree(original_client, self.client_directory2)\n    shutil.copyfile(original_map_file, self.map_file)\n    shutil.copyfile(original_map_file, self.map_file2)\n    shutil.copytree(original_keystore, self.keystore_directory)\n\n    # Launch a SimpleHTTPServer (serves files in the current directory).\n    # Test cases will request metadata and target files that have been\n    # pre-generated in 'tuf/tests/repository_data', which will be served by the\n    # SimpleHTTPServer launched here.  The test cases of this unit test assume\n    # the pre-generated metadata files have a specific structure, such\n    # as a delegated role 'targets/role1', three target files, five key files,\n    # etc.\n    self.SERVER_PORT = 30001\n    self.SERVER_PORT2 = 30002\n\n    command = ['python', '-m', 'tuf.scripts.simple_server', str(self.SERVER_PORT)]\n    command2 = ['python', '-m', 'tuf.scripts.simple_server', str(self.SERVER_PORT2)]\n\n    self.server_process = subprocess.Popen(command, stderr=subprocess.PIPE,\n        cwd=self.repository_directory)\n\n    logger.debug('Server process started.')\n    logger.debug('Server process id: ' + str(self.server_process.pid))\n    logger.debug('Serving on port: ' + str(self.SERVER_PORT))\n\n    self.server_process2 = subprocess.Popen(command2, stderr=subprocess.PIPE,\n        cwd=self.repository_directory2)\n\n    logger.debug('Server process 2 started.')\n    logger.debug('Server 2 process id: ' + str(self.server_process2.pid))\n    logger.debug('Serving 2 on port: ' + str(self.SERVER_PORT2))\n    self.url = 'http://localhost:' + str(self.SERVER_PORT) + os.path.sep\n    self.url2 = 'http://localhost:' + str(self.SERVER_PORT2) + os.path.sep\n\n    # NOTE: Following error is raised if a delay is not long enough to allow\n    # the server process to set up and start listening:\n    #     <urlopen error [Errno 111] Connection refused>\n    # or, on Windows:\n    #     Failed to establish a new connection: [Errno 111] Connection refused'\n    # While 0.3s has consistently worked on Travis and local builds, it led to\n    # occasional failures in AppVeyor builds, so increasing this to 2s, sadly.\n    time.sleep(2)\n\n    url_prefix = 'http://localhost:' + str(self.SERVER_PORT)\n    url_prefix2 = 'http://localhost:' + str(self.SERVER_PORT2)\n\n    self.repository_mirrors = {'mirror1': {'url_prefix': url_prefix,\n        'metadata_path': 'metadata', 'targets_path': 'targets',\n        'confined_target_dirs': ['']}}\n\n    self.repository_mirrors2 = {'mirror1': {'url_prefix': url_prefix2,\n        'metadata_path': 'metadata', 'targets_path': 'targets',\n        'confined_target_dirs': ['']}}\n\n    # Create the repository instances.  The test cases will use these client\n    # updaters to refresh metadata, fetch target files, etc.\n    self.repository_updater = updater.Updater(repository_name,\n        self.repository_mirrors)\n    self.repository_updater2 = updater.Updater(repository_name2,\n        self.repository_mirrors2)\n\n    # Creating a repository instance.  The test cases will use this client\n    # updater to refresh metadata, fetch target files, etc.\n    self.multi_repo_updater = updater.MultiRepoUpdater(self.map_file)\n\n    # Metadata role keys are needed by the test cases to make changes to the\n    # repository (e.g., adding a new target file to 'targets.json' and then\n    # requesting a refresh()).\n    self.role_keys = _load_role_keys(self.keystore_directory)\n\n\n\n  def tearDown(self):\n    # Modified_TestCase.tearDown() automatically deletes temporary files and\n    # directories that may have been created during each test case.\n    unittest_toolbox.Modified_TestCase.tearDown(self)\n\n    # Kill the SimpleHTTPServer process.\n    if self.server_process.returncode is None:\n      logger.info('Server process ' + str(self.server_process.pid) + ' terminated.')\n      self.server_process.kill()\n\n    if self.server_process2.returncode is None:\n      logger.info('Server 2 process ' + str(self.server_process2.pid) + ' terminated.')\n      self.server_process2.kill()\n\n    # updater.Updater() populates the roledb with the name \"test_repository1\"\n    tuf.roledb.clear_roledb(clear_all=True)\n    tuf.keydb.clear_keydb(clear_all=True)\n\n    # Remove the temporary repository directory, which should contain all the\n    # metadata, targets, and key files generated of all the test cases.  sleep\n    # for a bit to allow the kill'd server processes to terminate.\n    time.sleep(.3)\n    shutil.rmtree(self.temporary_directory)\n\n\n\n  # UNIT TESTS.\n  def test__init__(self):\n    # The client's repository requires a metadata directory (and the 'current'\n    # and 'previous' sub-directories), and at least the 'root.json' file.\n    # setUp(), called before each test case, instantiates the required updater\n    # objects and keys.  The needed objects/data is available in\n    # 'self.repository_updater', 'self.client_directory', etc.\n\n    # Test: Invalid arguments.\n    # Invalid 'updater_name' argument.  String expected.\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, 8)\n\n    # Restore 'tuf.settings.repositories_directory' to the original client\n    # directory.\n    tuf.settings.repositories_directory = self.client_directory\n\n    # Test for a non-existent map file.\n    self.assertRaises(tuf.exceptions.Error, updater.MultiRepoUpdater,\n        'non-existent.json')\n\n    # Test for a map file that doesn't contain the required fields.\n    root_filepath = os.path.join(\n        self.repository_directory, 'metadata', 'root.json')\n    self.assertRaises(securesystemslib.exceptions.FormatError,\n        updater.MultiRepoUpdater, root_filepath)\n\n    # Test for a valid instantiation.\n    map_file = os.path.join(self.client_directory, 'map.json')\n    multi_repo_updater = updater.MultiRepoUpdater(map_file)\n\n\n\n  def test__target_matches_path_pattern(self):\n    map_file = os.path.join(self.client_directory, 'map.json')\n    multi_repo_updater = updater.MultiRepoUpdater(map_file)\n    paths = ['foo*.tgz', 'bar*.tgz', 'file1.txt']\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('bar-1.0.tgz', paths))\n    self.assertTrue(\n        multi_repo_updater._target_matches_path_pattern('file1.txt', paths))\n    self.assertFalse(\n        multi_repo_updater._target_matches_path_pattern('baz-1.0.tgz', paths))\n\n\n\n  def test_get_valid_targetinfo(self):\n    map_file = os.path.join(self.client_directory, 'map.json')\n    multi_repo_updater = updater.MultiRepoUpdater(map_file)\n\n    # Verify the multi repo updater refuses to save targetinfo if\n    # required local repositories are missing.\n    repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1')\n    backup_repo_dir = os.path.join(tuf.settings.repositories_directory,\n        'test_repository1.backup')\n    shutil.move(repo_dir, backup_repo_dir)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the client's repository directory.\n    shutil.move(backup_repo_dir, repo_dir)\n\n    # Verify that the Root file must exist.\n    root_filepath = os.path.join(repo_dir, 'metadata', 'current', 'root.json')\n    backup_root_filepath = os.path.join(root_filepath, root_filepath + '.backup')\n    shutil.move(root_filepath, backup_root_filepath)\n    self.assertRaises(tuf.exceptions.Error,\n        multi_repo_updater.get_valid_targetinfo, 'file3.txt')\n\n    # Restore the Root file.\n    shutil.move(backup_root_filepath, root_filepath)\n\n    # Test that the first mapping is skipped if it's irrelevant to the target\n    # file.\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n\n    # Verify that a targetinfo is not returned for a non-existent target.\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = False\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'non-existent.txt')\n    multi_repo_updater.map_file['mapping'][1]['terminating'] = True\n\n    # Test for a mapping that sets terminating = True, and that appears before\n    # the final mapping.\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = True\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'bad3.txt')\n    multi_repo_updater.map_file['mapping'][0]['terminating'] = False\n\n    # Test for the case where multiple repos sign for the same target.\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo('file1.txt')\n\n    # Verify that valid targetinfo is matched for two repositories that provide\n    # different custom field.  Make sure to set the 'match_custom_field'\n    # argument to 'False' when calling get_valid_targetinfo().\n    repository = repo_tool.load_repository(self.repository_directory2)\n\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    repository.targets.remove_target(os.path.basename(target1))\n\n    custom_field = {\"custom\": \"my_custom_data\"}\n    repository.targets.add_target(target1, custom_field)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Do we get the expected match for the two targetinfo that only differ\n    # by the custom field?\n    valid_targetinfo = multi_repo_updater.get_valid_targetinfo(\n        'file1.txt', match_custom_field=False)\n\n    # Verify the case where two repositories provide different targetinfo.\n    # Modify file1.txt so that different length and hashes are reported by the\n    # two repositories.\n    repository = repo_tool.load_repository(self.repository_directory2)\n    target1 = os.path.join(self.repository_directory2, 'targets', 'file1.txt')\n    with open(target1, 'ab') as file_object:\n      file_object.write(b'append extra text')\n\n    repository.targets.remove_target(os.path.basename(target1))\n\n    repository.targets.add_target(target1)\n    repository.targets.load_signing_key(self.role_keys['targets']['private'])\n    repository.snapshot.load_signing_key(self.role_keys['snapshot']['private'])\n    repository.timestamp.load_signing_key(self.role_keys['timestamp']['private'])\n    repository.writeall()\n\n    # Move the staged metadata to the \"live\" metadata.\n    shutil.rmtree(os.path.join(self.repository_directory2, 'metadata'))\n    shutil.copytree(os.path.join(self.repository_directory2, 'metadata.staged'),\n        os.path.join(self.repository_directory2, 'metadata'))\n\n    # Ensure the threshold is modified to 2 (assumed to be 1, by default) and\n    # verify that get_valid_targetinfo() raises an UnknownTargetError\n    # despite both repos signing for file1.txt.\n    multi_repo_updater.map_file['mapping'][0]['threshold'] = 2\n    self.assertRaises(tuf.exceptions.UnknownTargetError,\n        multi_repo_updater.get_valid_targetinfo, 'file1.txt')\n\n\n\n\n\n  def test_get_updater(self):\n    map_file = os.path.join(self.client_directory, 'map.json')\n    multi_repo_updater = updater.MultiRepoUpdater(map_file)\n\n    # Test for a non-existent repository name.\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n\n    # Test get_updater indirectly via the \"private\" _update_from_repository().\n    self.assertRaises(tuf.exceptions.Error, multi_repo_updater._update_from_repository, 'bad_repo_name', 'file3.txt')\n\n    # Test for a repository that doesn't exist.\n    multi_repo_updater.map_file['repositories']['bad_repo_name'] = ['https://bogus:30002']\n    self.assertEqual(None, multi_repo_updater.get_updater('bad_repo_name'))\n\n\n\ndef _load_role_keys(keystore_directory):\n\n  # Populating 'self.role_keys' by importing the required public and private\n  # keys of 'tuf/tests/repository_data/'.  The role keys are needed when\n  # modifying the remote repository used by the test cases in this unit test.\n\n  # The pre-generated key files in 'repository_data/keystore' are all encrypted with\n  # a 'password' passphrase.\n  EXPECTED_KEYFILE_PASSWORD = 'password'\n\n  # Store and return the cryptography keys of the top-level roles, including 1\n  # delegated role.\n  role_keys = {}\n\n  root_key_file = os.path.join(keystore_directory, 'root_key')\n  targets_key_file = os.path.join(keystore_directory, 'targets_key')\n  snapshot_key_file = os.path.join(keystore_directory, 'snapshot_key')\n  timestamp_key_file = os.path.join(keystore_directory, 'timestamp_key')\n  delegation_key_file = os.path.join(keystore_directory, 'delegation_key')\n\n  role_keys = {'root': {}, 'targets': {}, 'snapshot': {}, 'timestamp': {},\n               'role1': {}}\n\n  # Import the top-level and delegated role public keys.\n  role_keys['root']['public'] = \\\n    repo_tool.import_rsa_publickey_from_file(root_key_file+'.pub')\n  role_keys['targets']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(targets_key_file+'.pub')\n  role_keys['snapshot']['public'] = \\\n    repo_tool.import_ed25519_publickey_from_file(snapshot_key_file+'.pub')\n  role_keys['timestamp']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(timestamp_key_file+'.pub')\n  role_keys['role1']['public'] = \\\n      repo_tool.import_ed25519_publickey_from_file(delegation_key_file+'.pub')\n\n  # Import the private keys of the top-level and delegated roles.\n  role_keys['root']['private'] = \\\n    repo_tool.import_rsa_privatekey_from_file(root_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['targets']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(targets_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['snapshot']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(snapshot_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['timestamp']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(timestamp_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n  role_keys['role1']['private'] = \\\n    repo_tool.import_ed25519_privatekey_from_file(delegation_key_file,\n                                              EXPECTED_KEYFILE_PASSWORD)\n\n  return role_keys\n\n\nif __name__ == '__main__':\n  unittest.main()\n", "#!/usr/bin/env python\n\n# Copyright 2012 - 2017, New York University and the TUF contributors\n# SPDX-License-Identifier: MIT OR Apache-2.0\n\n\"\"\"\n<Program Name>\n  sig.py\n\n<Author>\n  Vladimir Diaz <vladimir.v.diaz@gmail.com>\n\n<Started>\n  February 28, 2012.   Based on a previous version by Geremy Condra.\n\n<Copyright>\n  See LICENSE-MIT OR LICENSE for licensing information.\n\n<Purpose>\n  Survivable key compromise is one feature of a secure update system\n  incorporated into TUF's design. Responsibility separation through\n  the use of multiple roles, multi-signature trust, and explicit and\n  implicit key revocation are some of the mechanisms employed towards\n  this goal of survivability.  These mechanisms can all be seen in\n  play by the functions available in this module.\n\n  The signed metadata files utilized by TUF to download target files\n  securely are used and represented here as the 'signable' object.\n  More precisely, the signature structures contained within these metadata\n  files are packaged into 'signable' dictionaries.  This module makes it\n  possible to capture the states of these signatures by organizing the\n  keys into different categories.  As keys are added and removed, the\n  system must securely and efficiently verify the status of these signatures.\n  For instance, a bunch of keys have recently expired. How many valid keys\n  are now available to the Snapshot role?  This question can be answered by\n  get_signature_status(), which will return a full 'status report' of these\n  'signable' dicts.  This module also provides a convenient verify() function\n  that will determine if a role still has a sufficient number of valid keys.\n  If a caller needs to update the signatures of a 'signable' object, there\n  is also a function for that.\n\"\"\"\n\n# Help with Python 3 compatibility, where the print statement is a function, an\n# implicit relative import is invalid, and the '/' operator performs true\n# division.  Example:  print 'hello world' raises a 'SyntaxError' exception.\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import unicode_literals\n\nimport logging\n\nimport tuf\nimport tuf.keydb\nimport tuf.roledb\nimport tuf.formats\n\nimport securesystemslib\n\n# See 'log.py' to learn how logging is handled in TUF.\nlogger = logging.getLogger('tuf.sig')\n\n# Disable 'iso8601' logger messages to prevent 'iso8601' from clogging the\n# log file.\niso8601_logger = logging.getLogger('iso8601')\niso8601_logger.disabled = True\n\n\ndef get_signature_status(signable, role=None, repository_name='default',\n    threshold=None, keyids=None):\n  \"\"\"\n  <Purpose>\n    Return a dictionary representing the status of the signatures listed in\n    'signable'. Signatures in the returned dictionary are identified by the\n    signature keyid and can have a status of either:\n\n    * bad -- Invalid signature\n    * good -- Valid signature from key that is available in 'tuf.keydb', and is\n      authorized for the passed role as per 'tuf.roledb' (authorization may be\n      overwritten by passed 'keyids').\n    * unknown -- Signature from key that is not available in 'tuf.keydb', or if\n      'role' is None.\n    * unknown signing schemes -- Signature from key with unknown signing\n      scheme.\n    * untrusted -- Valid signature from key that is available in 'tuf.keydb',\n      but is not trusted for the passed role as per 'tuf.roledb' or the passed\n      'keyids'.\n\n    NOTE: The result may contain duplicate keyids or keyids that reference the\n    same key, if 'signable' lists multiple signatures from the same key.\n\n  <Arguments>\n    signable:\n      A dictionary containing a list of signatures and a 'signed' identifier.\n      signable = {'signed': 'signer',\n                  'signatures': [{'keyid': keyid,\n                                  'sig': sig}]}\n\n      Conformant to tuf.formats.SIGNABLE_SCHEMA.\n\n    role:\n      TUF role string (e.g. 'root', 'targets', 'snapshot' or timestamp).\n\n    threshold:\n      Rather than reference the role's threshold as set in tuf.roledb.py, use\n      the given 'threshold' to calculate the signature status of 'signable'.\n      'threshold' is an integer value that sets the role's threshold value, or\n      the minimum number of signatures needed for metadata to be considered\n      fully signed.\n\n    keyids:\n      Similar to the 'threshold' argument, use the supplied list of 'keyids'\n      to calculate the signature status, instead of referencing the keyids\n      in tuf.roledb.py for 'role'.\n\n  <Exceptions>\n    securesystemslib.exceptions.FormatError, if 'signable' does not have the\n    correct format.\n\n    tuf.exceptions.UnknownRoleError, if 'role' is not recognized.\n\n  <Side Effects>\n    None.\n\n  <Returns>\n    A dictionary representing the status of the signatures in 'signable'.\n    Conformant to tuf.formats.SIGNATURESTATUS_SCHEMA.\n  \"\"\"\n\n  # Do the arguments have the correct format?  This check will ensure that\n  # arguments have the appropriate number of objects and object types, and that\n  # all dict keys are properly named.  Raise\n  # 'securesystemslib.exceptions.FormatError' if the check fails.\n  tuf.formats.SIGNABLE_SCHEMA.check_match(signable)\n  securesystemslib.formats.NAME_SCHEMA.check_match(repository_name)\n\n  if role is not None:\n    tuf.formats.ROLENAME_SCHEMA.check_match(role)\n\n  if threshold is not None:\n    tuf.formats.THRESHOLD_SCHEMA.check_match(threshold)\n\n  if keyids is not None:\n    securesystemslib.formats.KEYIDS_SCHEMA.check_match(keyids)\n\n  # The signature status dictionary returned.\n  signature_status = {}\n  good_sigs = []\n  bad_sigs = []\n  unknown_sigs = []\n  untrusted_sigs = []\n  unknown_signing_schemes = []\n\n  # Extract the relevant fields from 'signable' that will allow us to identify\n  # the different classes of keys (i.e., good_sigs, bad_sigs, etc.).\n  signed = securesystemslib.formats.encode_canonical(signable['signed']).encode('utf-8')\n  signatures = signable['signatures']\n\n  # Iterate the signatures and enumerate the signature_status fields.\n  # (i.e., good_sigs, bad_sigs, etc.).\n  for signature in signatures:\n    keyid = signature['keyid']\n\n    # Does the signature use an unrecognized key?\n    try:\n      key = tuf.keydb.get_key(keyid, repository_name)\n\n    except tuf.exceptions.UnknownKeyError:\n      unknown_sigs.append(keyid)\n      continue\n\n    # Does the signature use an unknown/unsupported signing scheme?\n    try:\n      valid_sig = securesystemslib.keys.verify_signature(key, signature, signed)\n\n    except securesystemslib.exceptions.UnsupportedAlgorithmError:\n      unknown_signing_schemes.append(keyid)\n      continue\n\n    # We are now dealing with either a trusted or untrusted key...\n    if valid_sig:\n      if role is not None:\n\n        # Is this an unauthorized key? (a keyid associated with 'role')\n        # Note that if the role is not known, tuf.exceptions.UnknownRoleError\n        # is raised here.\n        if keyids is None:\n          keyids = tuf.roledb.get_role_keyids(role, repository_name)\n\n        if keyid not in keyids:\n          untrusted_sigs.append(keyid)\n          continue\n\n      # This is an unset role, thus an unknown signature.\n      else:\n        unknown_sigs.append(keyid)\n        continue\n\n      # Identify good/authorized key.\n      good_sigs.append(keyid)\n\n    else:\n      # This is a bad signature for a trusted key.\n      bad_sigs.append(keyid)\n\n  # Retrieve the threshold value for 'role'.  Raise\n  # tuf.exceptions.UnknownRoleError if we were given an invalid role.\n  if role is not None:\n    if threshold is None:\n      # Note that if the role is not known, tuf.exceptions.UnknownRoleError is\n      # raised here.\n      threshold = tuf.roledb.get_role_threshold(\n          role, repository_name=repository_name)\n\n    else:\n      logger.debug('Not using roledb.py\\'s threshold for ' + repr(role))\n\n  else:\n    threshold = 0\n\n  # Build the signature_status dict.\n  signature_status['threshold'] = threshold\n  signature_status['good_sigs'] = good_sigs\n  signature_status['bad_sigs'] = bad_sigs\n  signature_status['unknown_sigs'] = unknown_sigs\n  signature_status['untrusted_sigs'] = untrusted_sigs\n  signature_status['unknown_signing_schemes'] = unknown_signing_schemes\n\n  return signature_status\n\n\n\n\n\ndef verify(signable, role, repository_name='default', threshold=None,\n    keyids=None):\n  \"\"\"\n  <Purpose>\n    Verify that 'signable' has a valid threshold of authorized signatures\n    identified by unique keyids. The threshold and whether a keyid is\n    authorized is determined by querying the 'threshold' and 'keyids' info for\n    the passed 'role' in 'tuf.roledb'. Both values can be overwritten by\n    passing the 'threshold' or 'keyids' arguments.\n\n    NOTE:\n    - Signatures with identical authorized keyids only count towards the\n      threshold once.\n    - Signatures with different authorized keyids each count towards the\n      threshold, even if the keyids identify the same key.\n\n  <Arguments>\n    signable:\n      A dictionary containing a list of signatures and a 'signed' identifier\n      that conforms to SIGNABLE_SCHEMA, e.g.:\n      signable = {'signed':, 'signatures': [{'keyid':, 'method':, 'sig':}]}\n\n    role:\n      TUF role string (e.g. 'root', 'targets', 'snapshot' or timestamp).\n\n    threshold:\n      Rather than reference the role's threshold as set in tuf.roledb.py, use\n      the given 'threshold' to calculate the signature status of 'signable'.\n      'threshold' is an integer value that sets the role's threshold value, or\n      the minimum number of signatures needed for metadata to be considered\n      fully signed.\n\n    keyids:\n      Similar to the 'threshold' argument, use the supplied list of 'keyids'\n      to calculate the signature status, instead of referencing the keyids\n      in tuf.roledb.py for 'role'.\n\n  <Exceptions>\n    tuf.exceptions.UnknownRoleError, if 'role' is not recognized.\n\n    securesystemslib.exceptions.FormatError, if 'signable' is not formatted\n    correctly.\n\n    securesystemslib.exceptions.Error, if an invalid threshold is encountered.\n\n  <Side Effects>\n    tuf.sig.get_signature_status() called.  Any exceptions thrown by\n    get_signature_status() will be caught here and re-raised.\n\n  <Returns>\n    Boolean.  True if the number of good unique (by keyid) signatures >= the\n    role's threshold, False otherwise.\n  \"\"\"\n\n  tuf.formats.SIGNABLE_SCHEMA.check_match(signable)\n  tuf.formats.ROLENAME_SCHEMA.check_match(role)\n  securesystemslib.formats.NAME_SCHEMA.check_match(repository_name)\n\n  # Retrieve the signature status.  tuf.sig.get_signature_status() raises:\n  # tuf.exceptions.UnknownRoleError\n  # securesystemslib.exceptions.FormatError.  'threshold' and 'keyids' are also\n  # validated.\n  status = get_signature_status(signable, role, repository_name, threshold, keyids)\n\n  # Retrieve the role's threshold and the authorized keys of 'status'\n  threshold = status['threshold']\n  good_sigs = status['good_sigs']\n\n  # Does 'status' have the required threshold of signatures?\n  # First check for invalid threshold values before returning result.\n  # Note: get_signature_status() is expected to verify that 'threshold' is\n  # not None or <= 0.\n  if threshold is None or threshold <= 0: #pragma: no cover\n    raise securesystemslib.exceptions.Error(\"Invalid threshold: \" + repr(threshold))\n\n  return len(set(good_sigs)) >= threshold\n\n\n\n\n\ndef may_need_new_keys(signature_status):\n  \"\"\"\n  <Purpose>\n    Return true iff downloading a new set of keys might tip this\n    signature status over to valid.  This is determined by checking\n    if either the number of unknown or untrusted keys is > 0.\n\n  <Arguments>\n    signature_status:\n      The dictionary returned by tuf.sig.get_signature_status().\n\n  <Exceptions>\n    securesystemslib.exceptions.FormatError, if 'signature_status does not have\n    the correct format.\n\n  <Side Effects>\n    None.\n\n  <Returns>\n    Boolean.\n  \"\"\"\n\n  # Does 'signature_status' have the correct format?\n  # This check will ensure 'signature_status' has the appropriate number\n  # of objects and object types, and that all dict keys are properly named.\n  # Raise 'securesystemslib.exceptions.FormatError' if the check fails.\n  tuf.formats.SIGNATURESTATUS_SCHEMA.check_match(signature_status)\n\n  unknown = signature_status['unknown_sigs']\n  untrusted = signature_status['untrusted_sigs']\n\n  return len(unknown) or len(untrusted)\n\n\n\n\n\ndef generate_rsa_signature(signed, rsakey_dict):\n  \"\"\"\n  <Purpose>\n    Generate a new signature dict presumably to be added to the 'signatures'\n    field of 'signable'.  The 'signable' dict is of the form:\n\n    {'signed': 'signer',\n               'signatures': [{'keyid': keyid,\n                               'method': 'evp',\n                               'sig': sig}]}\n\n    The 'signed' argument is needed here for the signing process.\n    The 'rsakey_dict' argument is used to generate 'keyid', 'method', and 'sig'.\n\n    The caller should ensure the returned signature is not already in\n    'signable'.\n\n  <Arguments>\n    signed:\n      The data used by 'securesystemslib.keys.create_signature()' to generate\n      signatures.  It is stored in the 'signed' field of 'signable'.\n\n    rsakey_dict:\n      The RSA key, a 'securesystemslib.formats.RSAKEY_SCHEMA' dictionary.\n      Used here to produce 'keyid', 'method', and 'sig'.\n\n  <Exceptions>\n    securesystemslib.exceptions.FormatError, if 'rsakey_dict' does not have the\n    correct format.\n\n    TypeError, if a private key is not defined for 'rsakey_dict'.\n\n  <Side Effects>\n    None.\n\n  <Returns>\n    Signature dictionary conformant to securesystemslib.formats.SIGNATURE_SCHEMA.\n    Has the form:\n    {'keyid': keyid, 'method': 'evp', 'sig': sig}\n  \"\"\"\n\n  # We need 'signed' in canonical JSON format to generate\n  # the 'method' and 'sig' fields of the signature.\n  signed = securesystemslib.formats.encode_canonical(signed).encode('utf-8')\n\n  # Generate the RSA signature.\n  # Raises securesystemslib.exceptions.FormatError and TypeError.\n  signature = securesystemslib.keys.create_signature(rsakey_dict, signed)\n\n  return signature\n"], "filenames": ["tests/test_sig.py", "tests/test_updater.py", "tuf/sig.py"], "buggy_code_start_loc": [33, 426, 74], "buggy_code_end_loc": [386, 426, 307], "fixing_code_start_loc": [34, 427, 74], "fixing_code_end_loc": [450, 474, 311], "type": "CWE-347", "message": "The tough library (Rust/crates.io) prior to version 0.7.1 does not properly verify the threshold of cryptographic signatures. It allows an attacker to duplicate a valid signature in order to circumvent TUF requiring a minimum threshold of unique signatures before the metadata is considered valid. A fix is available in version 0.7.1. CVE-2020-6174 is assigned to the same vulnerability in the TUF reference implementation.", "other": {"cve": {"id": "CVE-2020-15093", "sourceIdentifier": "security-advisories@github.com", "published": "2020-07-09T19:15:11.413", "lastModified": "2021-10-26T19:58:35.917", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "The tough library (Rust/crates.io) prior to version 0.7.1 does not properly verify the threshold of cryptographic signatures. It allows an attacker to duplicate a valid signature in order to circumvent TUF requiring a minimum threshold of unique signatures before the metadata is considered valid. A fix is available in version 0.7.1. CVE-2020-6174 is assigned to the same vulnerability in the TUF reference implementation."}, {"lang": "es", "value": "La biblioteca tough (Rust/crates.io) anterior a la versi\u00f3n 0.7.1, no verifica apropiadamente el umbral de firmas criptogr\u00e1ficas. Permite a un atacante duplicar una firma v\u00e1lida a fin de eludir TUF que requiere un umbral m\u00ednimo de firmas \u00fanicas antes de que los metadatos se consideraran v\u00e1lidos. Una correcci\u00f3n est\u00e1 disponible en la versi\u00f3n 0.7.1. El CVE-2020-6174 es asignado a la misma vulnerabilidad en la implementaci\u00f3n de la referencia TUF"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 8.6, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 4.0}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:C/C:N/I:H/A:N", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "NONE", "integrityImpact": "HIGH", "availabilityImpact": "NONE", "baseScore": 8.6, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 4.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:N/AC:L/Au:N/C:N/I:P/A:N", "accessVector": "NETWORK", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "PARTIAL", "availabilityImpact": "NONE", "baseScore": 5.0}, "baseSeverity": "MEDIUM", "exploitabilityScore": 10.0, "impactScore": 2.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-347"}]}, {"source": "security-advisories@github.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-347"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:amazon:tough:*:*:*:*:*:rust:*:*", "versionEndExcluding": "0.7.1", "matchCriteriaId": "F72751E9-9A5B-46F0-93CF-AC75DDEF1C17"}]}]}], "references": [{"url": "https://crates.io/crates/tough", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/awslabs/tough/security/advisories/GHSA-5q2r-92f9-4m49", "source": "security-advisories@github.com", "tags": ["Third Party Advisory"]}, {"url": "https://github.com/theupdateframework/tuf/commit/2977188139d065ff3356c3cb4aec60c582b57e0e", "source": "security-advisories@github.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://github.com/theupdateframework/tuf/pull/974", "source": "security-advisories@github.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/theupdateframework/tuf/commit/2977188139d065ff3356c3cb4aec60c582b57e0e"}}