{"buggy_code": ["/*\n * Virtio Network Device\n *\n * Copyright IBM, Corp. 2007\n *\n * Authors:\n *  Anthony Liguori   <aliguori@us.ibm.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n#include \"qemu/osdep.h\"\n#include \"qemu/atomic.h\"\n#include \"qemu/iov.h\"\n#include \"qemu/main-loop.h\"\n#include \"qemu/module.h\"\n#include \"hw/virtio/virtio.h\"\n#include \"net/net.h\"\n#include \"net/checksum.h\"\n#include \"net/tap.h\"\n#include \"qemu/error-report.h\"\n#include \"qemu/timer.h\"\n#include \"qemu/option.h\"\n#include \"qemu/option_int.h\"\n#include \"qemu/config-file.h\"\n#include \"qapi/qmp/qdict.h\"\n#include \"hw/virtio/virtio-net.h\"\n#include \"net/vhost_net.h\"\n#include \"net/announce.h\"\n#include \"hw/virtio/virtio-bus.h\"\n#include \"qapi/error.h\"\n#include \"qapi/qapi-events-net.h\"\n#include \"hw/qdev-properties.h\"\n#include \"qapi/qapi-types-migration.h\"\n#include \"qapi/qapi-events-migration.h\"\n#include \"hw/virtio/virtio-access.h\"\n#include \"migration/misc.h\"\n#include \"standard-headers/linux/ethtool.h\"\n#include \"sysemu/sysemu.h\"\n#include \"trace.h\"\n#include \"monitor/qdev.h\"\n#include \"hw/pci/pci.h\"\n#include \"net_rx_pkt.h\"\n#include \"hw/virtio/vhost.h\"\n\n#define VIRTIO_NET_VM_VERSION    11\n\n#define MAC_TABLE_ENTRIES    64\n#define MAX_VLAN    (1 << 12)   /* Per 802.1Q definition */\n\n/* previously fixed value */\n#define VIRTIO_NET_RX_QUEUE_DEFAULT_SIZE 256\n#define VIRTIO_NET_TX_QUEUE_DEFAULT_SIZE 256\n\n/* for now, only allow larger queues; with virtio-1, guest can downsize */\n#define VIRTIO_NET_RX_QUEUE_MIN_SIZE VIRTIO_NET_RX_QUEUE_DEFAULT_SIZE\n#define VIRTIO_NET_TX_QUEUE_MIN_SIZE VIRTIO_NET_TX_QUEUE_DEFAULT_SIZE\n\n#define VIRTIO_NET_IP4_ADDR_SIZE   8        /* ipv4 saddr + daddr */\n\n#define VIRTIO_NET_TCP_FLAG         0x3F\n#define VIRTIO_NET_TCP_HDR_LENGTH   0xF000\n\n/* IPv4 max payload, 16 bits in the header */\n#define VIRTIO_NET_MAX_IP4_PAYLOAD (65535 - sizeof(struct ip_header))\n#define VIRTIO_NET_MAX_TCP_PAYLOAD 65535\n\n/* header length value in ip header without option */\n#define VIRTIO_NET_IP4_HEADER_LENGTH 5\n\n#define VIRTIO_NET_IP6_ADDR_SIZE   32      /* ipv6 saddr + daddr */\n#define VIRTIO_NET_MAX_IP6_PAYLOAD VIRTIO_NET_MAX_TCP_PAYLOAD\n\n/* Purge coalesced packets timer interval, This value affects the performance\n   a lot, and should be tuned carefully, '300000'(300us) is the recommended\n   value to pass the WHQL test, '50000' can gain 2x netperf throughput with\n   tso/gso/gro 'off'. */\n#define VIRTIO_NET_RSC_DEFAULT_INTERVAL 300000\n\n#define VIRTIO_NET_RSS_SUPPORTED_HASHES (VIRTIO_NET_RSS_HASH_TYPE_IPv4 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_TCPv4 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_UDPv4 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_IPv6 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_TCPv6 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_UDPv6 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_IP_EX | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_TCP_EX | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_UDP_EX)\n\nstatic const VirtIOFeature feature_sizes[] = {\n    {.flags = 1ULL << VIRTIO_NET_F_MAC,\n     .end = endof(struct virtio_net_config, mac)},\n    {.flags = 1ULL << VIRTIO_NET_F_STATUS,\n     .end = endof(struct virtio_net_config, status)},\n    {.flags = 1ULL << VIRTIO_NET_F_MQ,\n     .end = endof(struct virtio_net_config, max_virtqueue_pairs)},\n    {.flags = 1ULL << VIRTIO_NET_F_MTU,\n     .end = endof(struct virtio_net_config, mtu)},\n    {.flags = 1ULL << VIRTIO_NET_F_SPEED_DUPLEX,\n     .end = endof(struct virtio_net_config, duplex)},\n    {.flags = (1ULL << VIRTIO_NET_F_RSS) | (1ULL << VIRTIO_NET_F_HASH_REPORT),\n     .end = endof(struct virtio_net_config, supported_hash_types)},\n    {}\n};\n\nstatic VirtIONetQueue *virtio_net_get_subqueue(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n\n    return &n->vqs[nc->queue_index];\n}\n\nstatic int vq2q(int queue_index)\n{\n    return queue_index / 2;\n}\n\n/* TODO\n * - we could suppress RX interrupt if we were so inclined.\n */\n\nstatic void virtio_net_get_config(VirtIODevice *vdev, uint8_t *config)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    struct virtio_net_config netcfg;\n    NetClientState *nc = qemu_get_queue(n->nic);\n    static const MACAddr zero = { .a = { 0, 0, 0, 0, 0, 0 } };\n\n    int ret = 0;\n    memset(&netcfg, 0 , sizeof(struct virtio_net_config));\n    virtio_stw_p(vdev, &netcfg.status, n->status);\n    virtio_stw_p(vdev, &netcfg.max_virtqueue_pairs, n->max_queues);\n    virtio_stw_p(vdev, &netcfg.mtu, n->net_conf.mtu);\n    memcpy(netcfg.mac, n->mac, ETH_ALEN);\n    virtio_stl_p(vdev, &netcfg.speed, n->net_conf.speed);\n    netcfg.duplex = n->net_conf.duplex;\n    netcfg.rss_max_key_size = VIRTIO_NET_RSS_MAX_KEY_SIZE;\n    virtio_stw_p(vdev, &netcfg.rss_max_indirection_table_length,\n                 virtio_host_has_feature(vdev, VIRTIO_NET_F_RSS) ?\n                 VIRTIO_NET_RSS_MAX_TABLE_LEN : 1);\n    virtio_stl_p(vdev, &netcfg.supported_hash_types,\n                 VIRTIO_NET_RSS_SUPPORTED_HASHES);\n    memcpy(config, &netcfg, n->config_size);\n\n    /*\n     * Is this VDPA? No peer means not VDPA: there's no way to\n     * disconnect/reconnect a VDPA peer.\n     */\n    if (nc->peer && nc->peer->info->type == NET_CLIENT_DRIVER_VHOST_VDPA) {\n        ret = vhost_net_get_config(get_vhost_net(nc->peer), (uint8_t *)&netcfg,\n                                   n->config_size);\n        if (ret != -1) {\n            /*\n             * Some NIC/kernel combinations present 0 as the mac address.  As\n             * that is not a legal address, try to proceed with the\n             * address from the QEMU command line in the hope that the\n             * address has been configured correctly elsewhere - just not\n             * reported by the device.\n             */\n            if (memcmp(&netcfg.mac, &zero, sizeof(zero)) == 0) {\n                info_report(\"Zero hardware mac address detected. Ignoring.\");\n                memcpy(netcfg.mac, n->mac, ETH_ALEN);\n            }\n            memcpy(config, &netcfg, n->config_size);\n        }\n    }\n}\n\nstatic void virtio_net_set_config(VirtIODevice *vdev, const uint8_t *config)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    struct virtio_net_config netcfg = {};\n    NetClientState *nc = qemu_get_queue(n->nic);\n\n    memcpy(&netcfg, config, n->config_size);\n\n    if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_MAC_ADDR) &&\n        !virtio_vdev_has_feature(vdev, VIRTIO_F_VERSION_1) &&\n        memcmp(netcfg.mac, n->mac, ETH_ALEN)) {\n        memcpy(n->mac, netcfg.mac, ETH_ALEN);\n        qemu_format_nic_info_str(qemu_get_queue(n->nic), n->mac);\n    }\n\n    /*\n     * Is this VDPA? No peer means not VDPA: there's no way to\n     * disconnect/reconnect a VDPA peer.\n     */\n    if (nc->peer && nc->peer->info->type == NET_CLIENT_DRIVER_VHOST_VDPA) {\n        vhost_net_set_config(get_vhost_net(nc->peer),\n                             (uint8_t *)&netcfg, 0, n->config_size,\n                             VHOST_SET_CONFIG_TYPE_MASTER);\n      }\n}\n\nstatic bool virtio_net_started(VirtIONet *n, uint8_t status)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    return (status & VIRTIO_CONFIG_S_DRIVER_OK) &&\n        (n->status & VIRTIO_NET_S_LINK_UP) && vdev->vm_running;\n}\n\nstatic void virtio_net_announce_notify(VirtIONet *net)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(net);\n    trace_virtio_net_announce_notify();\n\n    net->status |= VIRTIO_NET_S_ANNOUNCE;\n    virtio_notify_config(vdev);\n}\n\nstatic void virtio_net_announce_timer(void *opaque)\n{\n    VirtIONet *n = opaque;\n    trace_virtio_net_announce_timer(n->announce_timer.round);\n\n    n->announce_timer.round--;\n    virtio_net_announce_notify(n);\n}\n\nstatic void virtio_net_announce(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n\n    /*\n     * Make sure the virtio migration announcement timer isn't running\n     * If it is, let it trigger announcement so that we do not cause\n     * confusion.\n     */\n    if (n->announce_timer.round) {\n        return;\n    }\n\n    if (virtio_vdev_has_feature(vdev, VIRTIO_NET_F_GUEST_ANNOUNCE) &&\n        virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_VQ)) {\n            virtio_net_announce_notify(n);\n    }\n}\n\nstatic void virtio_net_vhost_status(VirtIONet *n, uint8_t status)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    NetClientState *nc = qemu_get_queue(n->nic);\n    int queues = n->multiqueue ? n->max_queues : 1;\n\n    if (!get_vhost_net(nc->peer)) {\n        return;\n    }\n\n    if ((virtio_net_started(n, status) && !nc->peer->link_down) ==\n        !!n->vhost_started) {\n        return;\n    }\n    if (!n->vhost_started) {\n        int r, i;\n\n        if (n->needs_vnet_hdr_swap) {\n            error_report(\"backend does not support %s vnet headers; \"\n                         \"falling back on userspace virtio\",\n                         virtio_is_big_endian(vdev) ? \"BE\" : \"LE\");\n            return;\n        }\n\n        /* Any packets outstanding? Purge them to avoid touching rings\n         * when vhost is running.\n         */\n        for (i = 0;  i < queues; i++) {\n            NetClientState *qnc = qemu_get_subqueue(n->nic, i);\n\n            /* Purge both directions: TX and RX. */\n            qemu_net_queue_purge(qnc->peer->incoming_queue, qnc);\n            qemu_net_queue_purge(qnc->incoming_queue, qnc->peer);\n        }\n\n        if (virtio_has_feature(vdev->guest_features, VIRTIO_NET_F_MTU)) {\n            r = vhost_net_set_mtu(get_vhost_net(nc->peer), n->net_conf.mtu);\n            if (r < 0) {\n                error_report(\"%uBytes MTU not supported by the backend\",\n                             n->net_conf.mtu);\n\n                return;\n            }\n        }\n\n        n->vhost_started = 1;\n        r = vhost_net_start(vdev, n->nic->ncs, queues);\n        if (r < 0) {\n            error_report(\"unable to start vhost net: %d: \"\n                         \"falling back on userspace virtio\", -r);\n            n->vhost_started = 0;\n        }\n    } else {\n        vhost_net_stop(vdev, n->nic->ncs, queues);\n        n->vhost_started = 0;\n    }\n}\n\nstatic int virtio_net_set_vnet_endian_one(VirtIODevice *vdev,\n                                          NetClientState *peer,\n                                          bool enable)\n{\n    if (virtio_is_big_endian(vdev)) {\n        return qemu_set_vnet_be(peer, enable);\n    } else {\n        return qemu_set_vnet_le(peer, enable);\n    }\n}\n\nstatic bool virtio_net_set_vnet_endian(VirtIODevice *vdev, NetClientState *ncs,\n                                       int queues, bool enable)\n{\n    int i;\n\n    for (i = 0; i < queues; i++) {\n        if (virtio_net_set_vnet_endian_one(vdev, ncs[i].peer, enable) < 0 &&\n            enable) {\n            while (--i >= 0) {\n                virtio_net_set_vnet_endian_one(vdev, ncs[i].peer, false);\n            }\n\n            return true;\n        }\n    }\n\n    return false;\n}\n\nstatic void virtio_net_vnet_endian_status(VirtIONet *n, uint8_t status)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    int queues = n->multiqueue ? n->max_queues : 1;\n\n    if (virtio_net_started(n, status)) {\n        /* Before using the device, we tell the network backend about the\n         * endianness to use when parsing vnet headers. If the backend\n         * can't do it, we fallback onto fixing the headers in the core\n         * virtio-net code.\n         */\n        n->needs_vnet_hdr_swap = virtio_net_set_vnet_endian(vdev, n->nic->ncs,\n                                                            queues, true);\n    } else if (virtio_net_started(n, vdev->status)) {\n        /* After using the device, we need to reset the network backend to\n         * the default (guest native endianness), otherwise the guest may\n         * lose network connectivity if it is rebooted into a different\n         * endianness.\n         */\n        virtio_net_set_vnet_endian(vdev, n->nic->ncs, queues, false);\n    }\n}\n\nstatic void virtio_net_drop_tx_queue_data(VirtIODevice *vdev, VirtQueue *vq)\n{\n    unsigned int dropped = virtqueue_drop_all(vq);\n    if (dropped) {\n        virtio_notify(vdev, vq);\n    }\n}\n\nstatic void virtio_net_set_status(struct VirtIODevice *vdev, uint8_t status)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    VirtIONetQueue *q;\n    int i;\n    uint8_t queue_status;\n\n    virtio_net_vnet_endian_status(n, status);\n    virtio_net_vhost_status(n, status);\n\n    for (i = 0; i < n->max_queues; i++) {\n        NetClientState *ncs = qemu_get_subqueue(n->nic, i);\n        bool queue_started;\n        q = &n->vqs[i];\n\n        if ((!n->multiqueue && i != 0) || i >= n->curr_queues) {\n            queue_status = 0;\n        } else {\n            queue_status = status;\n        }\n        queue_started =\n            virtio_net_started(n, queue_status) && !n->vhost_started;\n\n        if (queue_started) {\n            qemu_flush_queued_packets(ncs);\n        }\n\n        if (!q->tx_waiting) {\n            continue;\n        }\n\n        if (queue_started) {\n            if (q->tx_timer) {\n                timer_mod(q->tx_timer,\n                               qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL) + n->tx_timeout);\n            } else {\n                qemu_bh_schedule(q->tx_bh);\n            }\n        } else {\n            if (q->tx_timer) {\n                timer_del(q->tx_timer);\n            } else {\n                qemu_bh_cancel(q->tx_bh);\n            }\n            if ((n->status & VIRTIO_NET_S_LINK_UP) == 0 &&\n                (queue_status & VIRTIO_CONFIG_S_DRIVER_OK) &&\n                vdev->vm_running) {\n                /* if tx is waiting we are likely have some packets in tx queue\n                 * and disabled notification */\n                q->tx_waiting = 0;\n                virtio_queue_set_notification(q->tx_vq, 1);\n                virtio_net_drop_tx_queue_data(vdev, q->tx_vq);\n            }\n        }\n    }\n}\n\nstatic void virtio_net_set_link_status(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    uint16_t old_status = n->status;\n\n    if (nc->link_down)\n        n->status &= ~VIRTIO_NET_S_LINK_UP;\n    else\n        n->status |= VIRTIO_NET_S_LINK_UP;\n\n    if (n->status != old_status)\n        virtio_notify_config(vdev);\n\n    virtio_net_set_status(vdev, vdev->status);\n}\n\nstatic void rxfilter_notify(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n\n    if (nc->rxfilter_notify_enabled) {\n        char *path = object_get_canonical_path(OBJECT(n->qdev));\n        qapi_event_send_nic_rx_filter_changed(!!n->netclient_name,\n                                              n->netclient_name, path);\n        g_free(path);\n\n        /* disable event notification to avoid events flooding */\n        nc->rxfilter_notify_enabled = 0;\n    }\n}\n\nstatic intList *get_vlan_table(VirtIONet *n)\n{\n    intList *list;\n    int i, j;\n\n    list = NULL;\n    for (i = 0; i < MAX_VLAN >> 5; i++) {\n        for (j = 0; n->vlans[i] && j <= 0x1f; j++) {\n            if (n->vlans[i] & (1U << j)) {\n                QAPI_LIST_PREPEND(list, (i << 5) + j);\n            }\n        }\n    }\n\n    return list;\n}\n\nstatic RxFilterInfo *virtio_net_query_rxfilter(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    RxFilterInfo *info;\n    strList *str_list;\n    int i;\n\n    info = g_malloc0(sizeof(*info));\n    info->name = g_strdup(nc->name);\n    info->promiscuous = n->promisc;\n\n    if (n->nouni) {\n        info->unicast = RX_STATE_NONE;\n    } else if (n->alluni) {\n        info->unicast = RX_STATE_ALL;\n    } else {\n        info->unicast = RX_STATE_NORMAL;\n    }\n\n    if (n->nomulti) {\n        info->multicast = RX_STATE_NONE;\n    } else if (n->allmulti) {\n        info->multicast = RX_STATE_ALL;\n    } else {\n        info->multicast = RX_STATE_NORMAL;\n    }\n\n    info->broadcast_allowed = n->nobcast;\n    info->multicast_overflow = n->mac_table.multi_overflow;\n    info->unicast_overflow = n->mac_table.uni_overflow;\n\n    info->main_mac = qemu_mac_strdup_printf(n->mac);\n\n    str_list = NULL;\n    for (i = 0; i < n->mac_table.first_multi; i++) {\n        QAPI_LIST_PREPEND(str_list,\n                      qemu_mac_strdup_printf(n->mac_table.macs + i * ETH_ALEN));\n    }\n    info->unicast_table = str_list;\n\n    str_list = NULL;\n    for (i = n->mac_table.first_multi; i < n->mac_table.in_use; i++) {\n        QAPI_LIST_PREPEND(str_list,\n                      qemu_mac_strdup_printf(n->mac_table.macs + i * ETH_ALEN));\n    }\n    info->multicast_table = str_list;\n    info->vlan_table = get_vlan_table(n);\n\n    if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_VLAN)) {\n        info->vlan = RX_STATE_ALL;\n    } else if (!info->vlan_table) {\n        info->vlan = RX_STATE_NONE;\n    } else {\n        info->vlan = RX_STATE_NORMAL;\n    }\n\n    /* enable event notification after query */\n    nc->rxfilter_notify_enabled = 1;\n\n    return info;\n}\n\nstatic void virtio_net_reset(VirtIODevice *vdev)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    int i;\n\n    /* Reset back to compatibility mode */\n    n->promisc = 1;\n    n->allmulti = 0;\n    n->alluni = 0;\n    n->nomulti = 0;\n    n->nouni = 0;\n    n->nobcast = 0;\n    /* multiqueue is disabled by default */\n    n->curr_queues = 1;\n    timer_del(n->announce_timer.tm);\n    n->announce_timer.round = 0;\n    n->status &= ~VIRTIO_NET_S_ANNOUNCE;\n\n    /* Flush any MAC and VLAN filter table state */\n    n->mac_table.in_use = 0;\n    n->mac_table.first_multi = 0;\n    n->mac_table.multi_overflow = 0;\n    n->mac_table.uni_overflow = 0;\n    memset(n->mac_table.macs, 0, MAC_TABLE_ENTRIES * ETH_ALEN);\n    memcpy(&n->mac[0], &n->nic->conf->macaddr, sizeof(n->mac));\n    qemu_format_nic_info_str(qemu_get_queue(n->nic), n->mac);\n    memset(n->vlans, 0, MAX_VLAN >> 3);\n\n    /* Flush any async TX */\n    for (i = 0;  i < n->max_queues; i++) {\n        NetClientState *nc = qemu_get_subqueue(n->nic, i);\n\n        if (nc->peer) {\n            qemu_flush_or_purge_queued_packets(nc->peer, true);\n            assert(!virtio_net_get_subqueue(nc)->async_tx.elem);\n        }\n    }\n}\n\nstatic void peer_test_vnet_hdr(VirtIONet *n)\n{\n    NetClientState *nc = qemu_get_queue(n->nic);\n    if (!nc->peer) {\n        return;\n    }\n\n    n->has_vnet_hdr = qemu_has_vnet_hdr(nc->peer);\n}\n\nstatic int peer_has_vnet_hdr(VirtIONet *n)\n{\n    return n->has_vnet_hdr;\n}\n\nstatic int peer_has_ufo(VirtIONet *n)\n{\n    if (!peer_has_vnet_hdr(n))\n        return 0;\n\n    n->has_ufo = qemu_has_ufo(qemu_get_queue(n->nic)->peer);\n\n    return n->has_ufo;\n}\n\nstatic void virtio_net_set_mrg_rx_bufs(VirtIONet *n, int mergeable_rx_bufs,\n                                       int version_1, int hash_report)\n{\n    int i;\n    NetClientState *nc;\n\n    n->mergeable_rx_bufs = mergeable_rx_bufs;\n\n    if (version_1) {\n        n->guest_hdr_len = hash_report ?\n            sizeof(struct virtio_net_hdr_v1_hash) :\n            sizeof(struct virtio_net_hdr_mrg_rxbuf);\n        n->rss_data.populate_hash = !!hash_report;\n    } else {\n        n->guest_hdr_len = n->mergeable_rx_bufs ?\n            sizeof(struct virtio_net_hdr_mrg_rxbuf) :\n            sizeof(struct virtio_net_hdr);\n    }\n\n    for (i = 0; i < n->max_queues; i++) {\n        nc = qemu_get_subqueue(n->nic, i);\n\n        if (peer_has_vnet_hdr(n) &&\n            qemu_has_vnet_hdr_len(nc->peer, n->guest_hdr_len)) {\n            qemu_set_vnet_hdr_len(nc->peer, n->guest_hdr_len);\n            n->host_hdr_len = n->guest_hdr_len;\n        }\n    }\n}\n\nstatic int virtio_net_max_tx_queue_size(VirtIONet *n)\n{\n    NetClientState *peer = n->nic_conf.peers.ncs[0];\n\n    /*\n     * Backends other than vhost-user don't support max queue size.\n     */\n    if (!peer) {\n        return VIRTIO_NET_TX_QUEUE_DEFAULT_SIZE;\n    }\n\n    if (peer->info->type != NET_CLIENT_DRIVER_VHOST_USER) {\n        return VIRTIO_NET_TX_QUEUE_DEFAULT_SIZE;\n    }\n\n    return VIRTQUEUE_MAX_SIZE;\n}\n\nstatic int peer_attach(VirtIONet *n, int index)\n{\n    NetClientState *nc = qemu_get_subqueue(n->nic, index);\n\n    if (!nc->peer) {\n        return 0;\n    }\n\n    if (nc->peer->info->type == NET_CLIENT_DRIVER_VHOST_USER) {\n        vhost_set_vring_enable(nc->peer, 1);\n    }\n\n    if (nc->peer->info->type != NET_CLIENT_DRIVER_TAP) {\n        return 0;\n    }\n\n    if (n->max_queues == 1) {\n        return 0;\n    }\n\n    return tap_enable(nc->peer);\n}\n\nstatic int peer_detach(VirtIONet *n, int index)\n{\n    NetClientState *nc = qemu_get_subqueue(n->nic, index);\n\n    if (!nc->peer) {\n        return 0;\n    }\n\n    if (nc->peer->info->type == NET_CLIENT_DRIVER_VHOST_USER) {\n        vhost_set_vring_enable(nc->peer, 0);\n    }\n\n    if (nc->peer->info->type !=  NET_CLIENT_DRIVER_TAP) {\n        return 0;\n    }\n\n    return tap_disable(nc->peer);\n}\n\nstatic void virtio_net_set_queues(VirtIONet *n)\n{\n    int i;\n    int r;\n\n    if (n->nic->peer_deleted) {\n        return;\n    }\n\n    for (i = 0; i < n->max_queues; i++) {\n        if (i < n->curr_queues) {\n            r = peer_attach(n, i);\n            assert(!r);\n        } else {\n            r = peer_detach(n, i);\n            assert(!r);\n        }\n    }\n}\n\nstatic void virtio_net_set_multiqueue(VirtIONet *n, int multiqueue);\n\nstatic uint64_t virtio_net_get_features(VirtIODevice *vdev, uint64_t features,\n                                        Error **errp)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    NetClientState *nc = qemu_get_queue(n->nic);\n\n    /* Firstly sync all virtio-net possible supported features */\n    features |= n->host_features;\n\n    virtio_add_feature(&features, VIRTIO_NET_F_MAC);\n\n    if (!peer_has_vnet_hdr(n)) {\n        virtio_clear_feature(&features, VIRTIO_NET_F_CSUM);\n        virtio_clear_feature(&features, VIRTIO_NET_F_HOST_TSO4);\n        virtio_clear_feature(&features, VIRTIO_NET_F_HOST_TSO6);\n        virtio_clear_feature(&features, VIRTIO_NET_F_HOST_ECN);\n\n        virtio_clear_feature(&features, VIRTIO_NET_F_GUEST_CSUM);\n        virtio_clear_feature(&features, VIRTIO_NET_F_GUEST_TSO4);\n        virtio_clear_feature(&features, VIRTIO_NET_F_GUEST_TSO6);\n        virtio_clear_feature(&features, VIRTIO_NET_F_GUEST_ECN);\n\n        virtio_clear_feature(&features, VIRTIO_NET_F_HASH_REPORT);\n    }\n\n    if (!peer_has_vnet_hdr(n) || !peer_has_ufo(n)) {\n        virtio_clear_feature(&features, VIRTIO_NET_F_GUEST_UFO);\n        virtio_clear_feature(&features, VIRTIO_NET_F_HOST_UFO);\n    }\n\n    if (!get_vhost_net(nc->peer)) {\n        return features;\n    }\n\n    if (!ebpf_rss_is_loaded(&n->ebpf_rss)) {\n        virtio_clear_feature(&features, VIRTIO_NET_F_RSS);\n    }\n    features = vhost_net_get_features(get_vhost_net(nc->peer), features);\n    vdev->backend_features = features;\n\n    if (n->mtu_bypass_backend &&\n            (n->host_features & 1ULL << VIRTIO_NET_F_MTU)) {\n        features |= (1ULL << VIRTIO_NET_F_MTU);\n    }\n\n    return features;\n}\n\nstatic uint64_t virtio_net_bad_features(VirtIODevice *vdev)\n{\n    uint64_t features = 0;\n\n    /* Linux kernel 2.6.25.  It understood MAC (as everyone must),\n     * but also these: */\n    virtio_add_feature(&features, VIRTIO_NET_F_MAC);\n    virtio_add_feature(&features, VIRTIO_NET_F_CSUM);\n    virtio_add_feature(&features, VIRTIO_NET_F_HOST_TSO4);\n    virtio_add_feature(&features, VIRTIO_NET_F_HOST_TSO6);\n    virtio_add_feature(&features, VIRTIO_NET_F_HOST_ECN);\n\n    return features;\n}\n\nstatic void virtio_net_apply_guest_offloads(VirtIONet *n)\n{\n    qemu_set_offload(qemu_get_queue(n->nic)->peer,\n            !!(n->curr_guest_offloads & (1ULL << VIRTIO_NET_F_GUEST_CSUM)),\n            !!(n->curr_guest_offloads & (1ULL << VIRTIO_NET_F_GUEST_TSO4)),\n            !!(n->curr_guest_offloads & (1ULL << VIRTIO_NET_F_GUEST_TSO6)),\n            !!(n->curr_guest_offloads & (1ULL << VIRTIO_NET_F_GUEST_ECN)),\n            !!(n->curr_guest_offloads & (1ULL << VIRTIO_NET_F_GUEST_UFO)));\n}\n\nstatic uint64_t virtio_net_guest_offloads_by_features(uint32_t features)\n{\n    static const uint64_t guest_offloads_mask =\n        (1ULL << VIRTIO_NET_F_GUEST_CSUM) |\n        (1ULL << VIRTIO_NET_F_GUEST_TSO4) |\n        (1ULL << VIRTIO_NET_F_GUEST_TSO6) |\n        (1ULL << VIRTIO_NET_F_GUEST_ECN)  |\n        (1ULL << VIRTIO_NET_F_GUEST_UFO);\n\n    return guest_offloads_mask & features;\n}\n\nstatic inline uint64_t virtio_net_supported_guest_offloads(VirtIONet *n)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    return virtio_net_guest_offloads_by_features(vdev->guest_features);\n}\n\ntypedef struct {\n    VirtIONet *n;\n    char *id;\n} FailoverId;\n\n/**\n * Set the id of the failover primary device\n *\n * @opaque: FailoverId to setup\n * @opts: opts for device we are handling\n * @errp: returns an error if this function fails\n */\nstatic int failover_set_primary(void *opaque, QemuOpts *opts, Error **errp)\n{\n    FailoverId *fid = opaque;\n    const char *standby_id = qemu_opt_get(opts, \"failover_pair_id\");\n\n    if (g_strcmp0(standby_id, fid->n->netclient_name) == 0) {\n        fid->id = g_strdup(opts->id);\n        return 1;\n    }\n\n    return 0;\n}\n\n/**\n * Find the primary device id for this failover virtio-net\n *\n * @n: VirtIONet device\n * @errp: returns an error if this function fails\n */\nstatic char *failover_find_primary_device_id(VirtIONet *n)\n{\n    Error *err = NULL;\n    FailoverId fid;\n\n    fid.n = n;\n    if (!qemu_opts_foreach(qemu_find_opts(\"device\"),\n                           failover_set_primary, &fid, &err)) {\n        return NULL;\n    }\n    return fid.id;\n}\n\n/**\n * Find the primary device for this failover virtio-net\n *\n * @n: VirtIONet device\n * @errp: returns an error if this function fails\n */\nstatic DeviceState *failover_find_primary_device(VirtIONet *n)\n{\n    char *id = failover_find_primary_device_id(n);\n\n    if (!id) {\n        return NULL;\n    }\n\n    return qdev_find_recursive(sysbus_get_default(), id);\n}\n\nstatic void failover_add_primary(VirtIONet *n, Error **errp)\n{\n    Error *err = NULL;\n    QemuOpts *opts;\n    char *id;\n    DeviceState *dev = failover_find_primary_device(n);\n\n    if (dev) {\n        return;\n    }\n\n    id = failover_find_primary_device_id(n);\n    if (!id) {\n        error_setg(errp, \"Primary device not found\");\n        error_append_hint(errp, \"Virtio-net failover will not work. Make \"\n                          \"sure primary device has parameter\"\n                          \" failover_pair_id=%s\\n\", n->netclient_name);\n        return;\n    }\n    opts = qemu_opts_find(qemu_find_opts(\"device\"), id);\n    g_assert(opts); /* cannot be NULL because id was found using opts list */\n    dev = qdev_device_add(opts, &err);\n    if (err) {\n        qemu_opts_del(opts);\n    } else {\n        object_unref(OBJECT(dev));\n    }\n    error_propagate(errp, err);\n}\n\nstatic void virtio_net_set_features(VirtIODevice *vdev, uint64_t features)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    Error *err = NULL;\n    int i;\n\n    if (n->mtu_bypass_backend &&\n            !virtio_has_feature(vdev->backend_features, VIRTIO_NET_F_MTU)) {\n        features &= ~(1ULL << VIRTIO_NET_F_MTU);\n    }\n\n    virtio_net_set_multiqueue(n,\n                              virtio_has_feature(features, VIRTIO_NET_F_RSS) ||\n                              virtio_has_feature(features, VIRTIO_NET_F_MQ));\n\n    virtio_net_set_mrg_rx_bufs(n,\n                               virtio_has_feature(features,\n                                                  VIRTIO_NET_F_MRG_RXBUF),\n                               virtio_has_feature(features,\n                                                  VIRTIO_F_VERSION_1),\n                               virtio_has_feature(features,\n                                                  VIRTIO_NET_F_HASH_REPORT));\n\n    n->rsc4_enabled = virtio_has_feature(features, VIRTIO_NET_F_RSC_EXT) &&\n        virtio_has_feature(features, VIRTIO_NET_F_GUEST_TSO4);\n    n->rsc6_enabled = virtio_has_feature(features, VIRTIO_NET_F_RSC_EXT) &&\n        virtio_has_feature(features, VIRTIO_NET_F_GUEST_TSO6);\n    n->rss_data.redirect = virtio_has_feature(features, VIRTIO_NET_F_RSS);\n\n    if (n->has_vnet_hdr) {\n        n->curr_guest_offloads =\n            virtio_net_guest_offloads_by_features(features);\n        virtio_net_apply_guest_offloads(n);\n    }\n\n    for (i = 0;  i < n->max_queues; i++) {\n        NetClientState *nc = qemu_get_subqueue(n->nic, i);\n\n        if (!get_vhost_net(nc->peer)) {\n            continue;\n        }\n        vhost_net_ack_features(get_vhost_net(nc->peer), features);\n    }\n\n    if (virtio_has_feature(features, VIRTIO_NET_F_CTRL_VLAN)) {\n        memset(n->vlans, 0, MAX_VLAN >> 3);\n    } else {\n        memset(n->vlans, 0xff, MAX_VLAN >> 3);\n    }\n\n    if (virtio_has_feature(features, VIRTIO_NET_F_STANDBY)) {\n        qapi_event_send_failover_negotiated(n->netclient_name);\n        qatomic_set(&n->failover_primary_hidden, false);\n        failover_add_primary(n, &err);\n        if (err) {\n            warn_report_err(err);\n        }\n    }\n}\n\nstatic int virtio_net_handle_rx_mode(VirtIONet *n, uint8_t cmd,\n                                     struct iovec *iov, unsigned int iov_cnt)\n{\n    uint8_t on;\n    size_t s;\n    NetClientState *nc = qemu_get_queue(n->nic);\n\n    s = iov_to_buf(iov, iov_cnt, 0, &on, sizeof(on));\n    if (s != sizeof(on)) {\n        return VIRTIO_NET_ERR;\n    }\n\n    if (cmd == VIRTIO_NET_CTRL_RX_PROMISC) {\n        n->promisc = on;\n    } else if (cmd == VIRTIO_NET_CTRL_RX_ALLMULTI) {\n        n->allmulti = on;\n    } else if (cmd == VIRTIO_NET_CTRL_RX_ALLUNI) {\n        n->alluni = on;\n    } else if (cmd == VIRTIO_NET_CTRL_RX_NOMULTI) {\n        n->nomulti = on;\n    } else if (cmd == VIRTIO_NET_CTRL_RX_NOUNI) {\n        n->nouni = on;\n    } else if (cmd == VIRTIO_NET_CTRL_RX_NOBCAST) {\n        n->nobcast = on;\n    } else {\n        return VIRTIO_NET_ERR;\n    }\n\n    rxfilter_notify(nc);\n\n    return VIRTIO_NET_OK;\n}\n\nstatic int virtio_net_handle_offloads(VirtIONet *n, uint8_t cmd,\n                                     struct iovec *iov, unsigned int iov_cnt)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    uint64_t offloads;\n    size_t s;\n\n    if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_GUEST_OFFLOADS)) {\n        return VIRTIO_NET_ERR;\n    }\n\n    s = iov_to_buf(iov, iov_cnt, 0, &offloads, sizeof(offloads));\n    if (s != sizeof(offloads)) {\n        return VIRTIO_NET_ERR;\n    }\n\n    if (cmd == VIRTIO_NET_CTRL_GUEST_OFFLOADS_SET) {\n        uint64_t supported_offloads;\n\n        offloads = virtio_ldq_p(vdev, &offloads);\n\n        if (!n->has_vnet_hdr) {\n            return VIRTIO_NET_ERR;\n        }\n\n        n->rsc4_enabled = virtio_has_feature(offloads, VIRTIO_NET_F_RSC_EXT) &&\n            virtio_has_feature(offloads, VIRTIO_NET_F_GUEST_TSO4);\n        n->rsc6_enabled = virtio_has_feature(offloads, VIRTIO_NET_F_RSC_EXT) &&\n            virtio_has_feature(offloads, VIRTIO_NET_F_GUEST_TSO6);\n        virtio_clear_feature(&offloads, VIRTIO_NET_F_RSC_EXT);\n\n        supported_offloads = virtio_net_supported_guest_offloads(n);\n        if (offloads & ~supported_offloads) {\n            return VIRTIO_NET_ERR;\n        }\n\n        n->curr_guest_offloads = offloads;\n        virtio_net_apply_guest_offloads(n);\n\n        return VIRTIO_NET_OK;\n    } else {\n        return VIRTIO_NET_ERR;\n    }\n}\n\nstatic int virtio_net_handle_mac(VirtIONet *n, uint8_t cmd,\n                                 struct iovec *iov, unsigned int iov_cnt)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    struct virtio_net_ctrl_mac mac_data;\n    size_t s;\n    NetClientState *nc = qemu_get_queue(n->nic);\n\n    if (cmd == VIRTIO_NET_CTRL_MAC_ADDR_SET) {\n        if (iov_size(iov, iov_cnt) != sizeof(n->mac)) {\n            return VIRTIO_NET_ERR;\n        }\n        s = iov_to_buf(iov, iov_cnt, 0, &n->mac, sizeof(n->mac));\n        assert(s == sizeof(n->mac));\n        qemu_format_nic_info_str(qemu_get_queue(n->nic), n->mac);\n        rxfilter_notify(nc);\n\n        return VIRTIO_NET_OK;\n    }\n\n    if (cmd != VIRTIO_NET_CTRL_MAC_TABLE_SET) {\n        return VIRTIO_NET_ERR;\n    }\n\n    int in_use = 0;\n    int first_multi = 0;\n    uint8_t uni_overflow = 0;\n    uint8_t multi_overflow = 0;\n    uint8_t *macs = g_malloc0(MAC_TABLE_ENTRIES * ETH_ALEN);\n\n    s = iov_to_buf(iov, iov_cnt, 0, &mac_data.entries,\n                   sizeof(mac_data.entries));\n    mac_data.entries = virtio_ldl_p(vdev, &mac_data.entries);\n    if (s != sizeof(mac_data.entries)) {\n        goto error;\n    }\n    iov_discard_front(&iov, &iov_cnt, s);\n\n    if (mac_data.entries * ETH_ALEN > iov_size(iov, iov_cnt)) {\n        goto error;\n    }\n\n    if (mac_data.entries <= MAC_TABLE_ENTRIES) {\n        s = iov_to_buf(iov, iov_cnt, 0, macs,\n                       mac_data.entries * ETH_ALEN);\n        if (s != mac_data.entries * ETH_ALEN) {\n            goto error;\n        }\n        in_use += mac_data.entries;\n    } else {\n        uni_overflow = 1;\n    }\n\n    iov_discard_front(&iov, &iov_cnt, mac_data.entries * ETH_ALEN);\n\n    first_multi = in_use;\n\n    s = iov_to_buf(iov, iov_cnt, 0, &mac_data.entries,\n                   sizeof(mac_data.entries));\n    mac_data.entries = virtio_ldl_p(vdev, &mac_data.entries);\n    if (s != sizeof(mac_data.entries)) {\n        goto error;\n    }\n\n    iov_discard_front(&iov, &iov_cnt, s);\n\n    if (mac_data.entries * ETH_ALEN != iov_size(iov, iov_cnt)) {\n        goto error;\n    }\n\n    if (mac_data.entries <= MAC_TABLE_ENTRIES - in_use) {\n        s = iov_to_buf(iov, iov_cnt, 0, &macs[in_use * ETH_ALEN],\n                       mac_data.entries * ETH_ALEN);\n        if (s != mac_data.entries * ETH_ALEN) {\n            goto error;\n        }\n        in_use += mac_data.entries;\n    } else {\n        multi_overflow = 1;\n    }\n\n    n->mac_table.in_use = in_use;\n    n->mac_table.first_multi = first_multi;\n    n->mac_table.uni_overflow = uni_overflow;\n    n->mac_table.multi_overflow = multi_overflow;\n    memcpy(n->mac_table.macs, macs, MAC_TABLE_ENTRIES * ETH_ALEN);\n    g_free(macs);\n    rxfilter_notify(nc);\n\n    return VIRTIO_NET_OK;\n\nerror:\n    g_free(macs);\n    return VIRTIO_NET_ERR;\n}\n\nstatic int virtio_net_handle_vlan_table(VirtIONet *n, uint8_t cmd,\n                                        struct iovec *iov, unsigned int iov_cnt)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    uint16_t vid;\n    size_t s;\n    NetClientState *nc = qemu_get_queue(n->nic);\n\n    s = iov_to_buf(iov, iov_cnt, 0, &vid, sizeof(vid));\n    vid = virtio_lduw_p(vdev, &vid);\n    if (s != sizeof(vid)) {\n        return VIRTIO_NET_ERR;\n    }\n\n    if (vid >= MAX_VLAN)\n        return VIRTIO_NET_ERR;\n\n    if (cmd == VIRTIO_NET_CTRL_VLAN_ADD)\n        n->vlans[vid >> 5] |= (1U << (vid & 0x1f));\n    else if (cmd == VIRTIO_NET_CTRL_VLAN_DEL)\n        n->vlans[vid >> 5] &= ~(1U << (vid & 0x1f));\n    else\n        return VIRTIO_NET_ERR;\n\n    rxfilter_notify(nc);\n\n    return VIRTIO_NET_OK;\n}\n\nstatic int virtio_net_handle_announce(VirtIONet *n, uint8_t cmd,\n                                      struct iovec *iov, unsigned int iov_cnt)\n{\n    trace_virtio_net_handle_announce(n->announce_timer.round);\n    if (cmd == VIRTIO_NET_CTRL_ANNOUNCE_ACK &&\n        n->status & VIRTIO_NET_S_ANNOUNCE) {\n        n->status &= ~VIRTIO_NET_S_ANNOUNCE;\n        if (n->announce_timer.round) {\n            qemu_announce_timer_step(&n->announce_timer);\n        }\n        return VIRTIO_NET_OK;\n    } else {\n        return VIRTIO_NET_ERR;\n    }\n}\n\nstatic void virtio_net_detach_epbf_rss(VirtIONet *n);\n\nstatic void virtio_net_disable_rss(VirtIONet *n)\n{\n    if (n->rss_data.enabled) {\n        trace_virtio_net_rss_disable();\n    }\n    n->rss_data.enabled = false;\n\n    virtio_net_detach_epbf_rss(n);\n}\n\nstatic bool virtio_net_attach_ebpf_to_backend(NICState *nic, int prog_fd)\n{\n    NetClientState *nc = qemu_get_peer(qemu_get_queue(nic), 0);\n    if (nc == NULL || nc->info->set_steering_ebpf == NULL) {\n        return false;\n    }\n\n    return nc->info->set_steering_ebpf(nc, prog_fd);\n}\n\nstatic void rss_data_to_rss_config(struct VirtioNetRssData *data,\n                                   struct EBPFRSSConfig *config)\n{\n    config->redirect = data->redirect;\n    config->populate_hash = data->populate_hash;\n    config->hash_types = data->hash_types;\n    config->indirections_len = data->indirections_len;\n    config->default_queue = data->default_queue;\n}\n\nstatic bool virtio_net_attach_epbf_rss(VirtIONet *n)\n{\n    struct EBPFRSSConfig config = {};\n\n    if (!ebpf_rss_is_loaded(&n->ebpf_rss)) {\n        return false;\n    }\n\n    rss_data_to_rss_config(&n->rss_data, &config);\n\n    if (!ebpf_rss_set_all(&n->ebpf_rss, &config,\n                          n->rss_data.indirections_table, n->rss_data.key)) {\n        return false;\n    }\n\n    if (!virtio_net_attach_ebpf_to_backend(n->nic, n->ebpf_rss.program_fd)) {\n        return false;\n    }\n\n    return true;\n}\n\nstatic void virtio_net_detach_epbf_rss(VirtIONet *n)\n{\n    virtio_net_attach_ebpf_to_backend(n->nic, -1);\n}\n\nstatic bool virtio_net_load_ebpf(VirtIONet *n)\n{\n    if (!virtio_net_attach_ebpf_to_backend(n->nic, -1)) {\n        /* backend does't support steering ebpf */\n        return false;\n    }\n\n    return ebpf_rss_load(&n->ebpf_rss);\n}\n\nstatic void virtio_net_unload_ebpf(VirtIONet *n)\n{\n    virtio_net_attach_ebpf_to_backend(n->nic, -1);\n    ebpf_rss_unload(&n->ebpf_rss);\n}\n\nstatic uint16_t virtio_net_handle_rss(VirtIONet *n,\n                                      struct iovec *iov,\n                                      unsigned int iov_cnt,\n                                      bool do_rss)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    struct virtio_net_rss_config cfg;\n    size_t s, offset = 0, size_get;\n    uint16_t queues, i;\n    struct {\n        uint16_t us;\n        uint8_t b;\n    } QEMU_PACKED temp;\n    const char *err_msg = \"\";\n    uint32_t err_value = 0;\n\n    if (do_rss && !virtio_vdev_has_feature(vdev, VIRTIO_NET_F_RSS)) {\n        err_msg = \"RSS is not negotiated\";\n        goto error;\n    }\n    if (!do_rss && !virtio_vdev_has_feature(vdev, VIRTIO_NET_F_HASH_REPORT)) {\n        err_msg = \"Hash report is not negotiated\";\n        goto error;\n    }\n    size_get = offsetof(struct virtio_net_rss_config, indirection_table);\n    s = iov_to_buf(iov, iov_cnt, offset, &cfg, size_get);\n    if (s != size_get) {\n        err_msg = \"Short command buffer\";\n        err_value = (uint32_t)s;\n        goto error;\n    }\n    n->rss_data.hash_types = virtio_ldl_p(vdev, &cfg.hash_types);\n    n->rss_data.indirections_len =\n        virtio_lduw_p(vdev, &cfg.indirection_table_mask);\n    n->rss_data.indirections_len++;\n    if (!do_rss) {\n        n->rss_data.indirections_len = 1;\n    }\n    if (!is_power_of_2(n->rss_data.indirections_len)) {\n        err_msg = \"Invalid size of indirection table\";\n        err_value = n->rss_data.indirections_len;\n        goto error;\n    }\n    if (n->rss_data.indirections_len > VIRTIO_NET_RSS_MAX_TABLE_LEN) {\n        err_msg = \"Too large indirection table\";\n        err_value = n->rss_data.indirections_len;\n        goto error;\n    }\n    n->rss_data.default_queue = do_rss ?\n        virtio_lduw_p(vdev, &cfg.unclassified_queue) : 0;\n    if (n->rss_data.default_queue >= n->max_queues) {\n        err_msg = \"Invalid default queue\";\n        err_value = n->rss_data.default_queue;\n        goto error;\n    }\n    offset += size_get;\n    size_get = sizeof(uint16_t) * n->rss_data.indirections_len;\n    g_free(n->rss_data.indirections_table);\n    n->rss_data.indirections_table = g_malloc(size_get);\n    if (!n->rss_data.indirections_table) {\n        err_msg = \"Can't allocate indirections table\";\n        err_value = n->rss_data.indirections_len;\n        goto error;\n    }\n    s = iov_to_buf(iov, iov_cnt, offset,\n                   n->rss_data.indirections_table, size_get);\n    if (s != size_get) {\n        err_msg = \"Short indirection table buffer\";\n        err_value = (uint32_t)s;\n        goto error;\n    }\n    for (i = 0; i < n->rss_data.indirections_len; ++i) {\n        uint16_t val = n->rss_data.indirections_table[i];\n        n->rss_data.indirections_table[i] = virtio_lduw_p(vdev, &val);\n    }\n    offset += size_get;\n    size_get = sizeof(temp);\n    s = iov_to_buf(iov, iov_cnt, offset, &temp, size_get);\n    if (s != size_get) {\n        err_msg = \"Can't get queues\";\n        err_value = (uint32_t)s;\n        goto error;\n    }\n    queues = do_rss ? virtio_lduw_p(vdev, &temp.us) : n->curr_queues;\n    if (queues == 0 || queues > n->max_queues) {\n        err_msg = \"Invalid number of queues\";\n        err_value = queues;\n        goto error;\n    }\n    if (temp.b > VIRTIO_NET_RSS_MAX_KEY_SIZE) {\n        err_msg = \"Invalid key size\";\n        err_value = temp.b;\n        goto error;\n    }\n    if (!temp.b && n->rss_data.hash_types) {\n        err_msg = \"No key provided\";\n        err_value = 0;\n        goto error;\n    }\n    if (!temp.b && !n->rss_data.hash_types) {\n        virtio_net_disable_rss(n);\n        return queues;\n    }\n    offset += size_get;\n    size_get = temp.b;\n    s = iov_to_buf(iov, iov_cnt, offset, n->rss_data.key, size_get);\n    if (s != size_get) {\n        err_msg = \"Can get key buffer\";\n        err_value = (uint32_t)s;\n        goto error;\n    }\n    n->rss_data.enabled = true;\n\n    if (!n->rss_data.populate_hash) {\n        if (!virtio_net_attach_epbf_rss(n)) {\n            /* EBPF must be loaded for vhost */\n            if (get_vhost_net(qemu_get_queue(n->nic)->peer)) {\n                warn_report(\"Can't load eBPF RSS for vhost\");\n                goto error;\n            }\n            /* fallback to software RSS */\n            warn_report(\"Can't load eBPF RSS - fallback to software RSS\");\n            n->rss_data.enabled_software_rss = true;\n        }\n    } else {\n        /* use software RSS for hash populating */\n        /* and detach eBPF if was loaded before */\n        virtio_net_detach_epbf_rss(n);\n        n->rss_data.enabled_software_rss = true;\n    }\n\n    trace_virtio_net_rss_enable(n->rss_data.hash_types,\n                                n->rss_data.indirections_len,\n                                temp.b);\n    return queues;\nerror:\n    trace_virtio_net_rss_error(err_msg, err_value);\n    virtio_net_disable_rss(n);\n    return 0;\n}\n\nstatic int virtio_net_handle_mq(VirtIONet *n, uint8_t cmd,\n                                struct iovec *iov, unsigned int iov_cnt)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    uint16_t queues;\n\n    virtio_net_disable_rss(n);\n    if (cmd == VIRTIO_NET_CTRL_MQ_HASH_CONFIG) {\n        queues = virtio_net_handle_rss(n, iov, iov_cnt, false);\n        return queues ? VIRTIO_NET_OK : VIRTIO_NET_ERR;\n    }\n    if (cmd == VIRTIO_NET_CTRL_MQ_RSS_CONFIG) {\n        queues = virtio_net_handle_rss(n, iov, iov_cnt, true);\n    } else if (cmd == VIRTIO_NET_CTRL_MQ_VQ_PAIRS_SET) {\n        struct virtio_net_ctrl_mq mq;\n        size_t s;\n        if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_MQ)) {\n            return VIRTIO_NET_ERR;\n        }\n        s = iov_to_buf(iov, iov_cnt, 0, &mq, sizeof(mq));\n        if (s != sizeof(mq)) {\n            return VIRTIO_NET_ERR;\n        }\n        queues = virtio_lduw_p(vdev, &mq.virtqueue_pairs);\n\n    } else {\n        return VIRTIO_NET_ERR;\n    }\n\n    if (queues < VIRTIO_NET_CTRL_MQ_VQ_PAIRS_MIN ||\n        queues > VIRTIO_NET_CTRL_MQ_VQ_PAIRS_MAX ||\n        queues > n->max_queues ||\n        !n->multiqueue) {\n        return VIRTIO_NET_ERR;\n    }\n\n    n->curr_queues = queues;\n    /* stop the backend before changing the number of queues to avoid handling a\n     * disabled queue */\n    virtio_net_set_status(vdev, vdev->status);\n    virtio_net_set_queues(n);\n\n    return VIRTIO_NET_OK;\n}\n\nstatic void virtio_net_handle_ctrl(VirtIODevice *vdev, VirtQueue *vq)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    struct virtio_net_ctrl_hdr ctrl;\n    virtio_net_ctrl_ack status = VIRTIO_NET_ERR;\n    VirtQueueElement *elem;\n    size_t s;\n    struct iovec *iov, *iov2;\n    unsigned int iov_cnt;\n\n    for (;;) {\n        elem = virtqueue_pop(vq, sizeof(VirtQueueElement));\n        if (!elem) {\n            break;\n        }\n        if (iov_size(elem->in_sg, elem->in_num) < sizeof(status) ||\n            iov_size(elem->out_sg, elem->out_num) < sizeof(ctrl)) {\n            virtio_error(vdev, \"virtio-net ctrl missing headers\");\n            virtqueue_detach_element(vq, elem, 0);\n            g_free(elem);\n            break;\n        }\n\n        iov_cnt = elem->out_num;\n        iov2 = iov = g_memdup(elem->out_sg, sizeof(struct iovec) * elem->out_num);\n        s = iov_to_buf(iov, iov_cnt, 0, &ctrl, sizeof(ctrl));\n        iov_discard_front(&iov, &iov_cnt, sizeof(ctrl));\n        if (s != sizeof(ctrl)) {\n            status = VIRTIO_NET_ERR;\n        } else if (ctrl.class == VIRTIO_NET_CTRL_RX) {\n            status = virtio_net_handle_rx_mode(n, ctrl.cmd, iov, iov_cnt);\n        } else if (ctrl.class == VIRTIO_NET_CTRL_MAC) {\n            status = virtio_net_handle_mac(n, ctrl.cmd, iov, iov_cnt);\n        } else if (ctrl.class == VIRTIO_NET_CTRL_VLAN) {\n            status = virtio_net_handle_vlan_table(n, ctrl.cmd, iov, iov_cnt);\n        } else if (ctrl.class == VIRTIO_NET_CTRL_ANNOUNCE) {\n            status = virtio_net_handle_announce(n, ctrl.cmd, iov, iov_cnt);\n        } else if (ctrl.class == VIRTIO_NET_CTRL_MQ) {\n            status = virtio_net_handle_mq(n, ctrl.cmd, iov, iov_cnt);\n        } else if (ctrl.class == VIRTIO_NET_CTRL_GUEST_OFFLOADS) {\n            status = virtio_net_handle_offloads(n, ctrl.cmd, iov, iov_cnt);\n        }\n\n        s = iov_from_buf(elem->in_sg, elem->in_num, 0, &status, sizeof(status));\n        assert(s == sizeof(status));\n\n        virtqueue_push(vq, elem, sizeof(status));\n        virtio_notify(vdev, vq);\n        g_free(iov2);\n        g_free(elem);\n    }\n}\n\n/* RX */\n\nstatic void virtio_net_handle_rx(VirtIODevice *vdev, VirtQueue *vq)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    int queue_index = vq2q(virtio_get_queue_index(vq));\n\n    qemu_flush_queued_packets(qemu_get_subqueue(n->nic, queue_index));\n}\n\nstatic bool virtio_net_can_receive(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    VirtIONetQueue *q = virtio_net_get_subqueue(nc);\n\n    if (!vdev->vm_running) {\n        return false;\n    }\n\n    if (nc->queue_index >= n->curr_queues) {\n        return false;\n    }\n\n    if (!virtio_queue_ready(q->rx_vq) ||\n        !(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK)) {\n        return false;\n    }\n\n    return true;\n}\n\nstatic int virtio_net_has_buffers(VirtIONetQueue *q, int bufsize)\n{\n    VirtIONet *n = q->n;\n    if (virtio_queue_empty(q->rx_vq) ||\n        (n->mergeable_rx_bufs &&\n         !virtqueue_avail_bytes(q->rx_vq, bufsize, 0))) {\n        virtio_queue_set_notification(q->rx_vq, 1);\n\n        /* To avoid a race condition where the guest has made some buffers\n         * available after the above check but before notification was\n         * enabled, check for available buffers again.\n         */\n        if (virtio_queue_empty(q->rx_vq) ||\n            (n->mergeable_rx_bufs &&\n             !virtqueue_avail_bytes(q->rx_vq, bufsize, 0))) {\n            return 0;\n        }\n    }\n\n    virtio_queue_set_notification(q->rx_vq, 0);\n    return 1;\n}\n\nstatic void virtio_net_hdr_swap(VirtIODevice *vdev, struct virtio_net_hdr *hdr)\n{\n    virtio_tswap16s(vdev, &hdr->hdr_len);\n    virtio_tswap16s(vdev, &hdr->gso_size);\n    virtio_tswap16s(vdev, &hdr->csum_start);\n    virtio_tswap16s(vdev, &hdr->csum_offset);\n}\n\n/* dhclient uses AF_PACKET but doesn't pass auxdata to the kernel so\n * it never finds out that the packets don't have valid checksums.  This\n * causes dhclient to get upset.  Fedora's carried a patch for ages to\n * fix this with Xen but it hasn't appeared in an upstream release of\n * dhclient yet.\n *\n * To avoid breaking existing guests, we catch udp packets and add\n * checksums.  This is terrible but it's better than hacking the guest\n * kernels.\n *\n * N.B. if we introduce a zero-copy API, this operation is no longer free so\n * we should provide a mechanism to disable it to avoid polluting the host\n * cache.\n */\nstatic void work_around_broken_dhclient(struct virtio_net_hdr *hdr,\n                                        uint8_t *buf, size_t size)\n{\n    if ((hdr->flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) && /* missing csum */\n        (size > 27 && size < 1500) && /* normal sized MTU */\n        (buf[12] == 0x08 && buf[13] == 0x00) && /* ethertype == IPv4 */\n        (buf[23] == 17) && /* ip.protocol == UDP */\n        (buf[34] == 0 && buf[35] == 67)) { /* udp.srcport == bootps */\n        net_checksum_calculate(buf, size, CSUM_UDP);\n        hdr->flags &= ~VIRTIO_NET_HDR_F_NEEDS_CSUM;\n    }\n}\n\nstatic void receive_header(VirtIONet *n, const struct iovec *iov, int iov_cnt,\n                           const void *buf, size_t size)\n{\n    if (n->has_vnet_hdr) {\n        /* FIXME this cast is evil */\n        void *wbuf = (void *)buf;\n        work_around_broken_dhclient(wbuf, wbuf + n->host_hdr_len,\n                                    size - n->host_hdr_len);\n\n        if (n->needs_vnet_hdr_swap) {\n            virtio_net_hdr_swap(VIRTIO_DEVICE(n), wbuf);\n        }\n        iov_from_buf(iov, iov_cnt, 0, buf, sizeof(struct virtio_net_hdr));\n    } else {\n        struct virtio_net_hdr hdr = {\n            .flags = 0,\n            .gso_type = VIRTIO_NET_HDR_GSO_NONE\n        };\n        iov_from_buf(iov, iov_cnt, 0, &hdr, sizeof hdr);\n    }\n}\n\nstatic int receive_filter(VirtIONet *n, const uint8_t *buf, int size)\n{\n    static const uint8_t bcast[] = {0xff, 0xff, 0xff, 0xff, 0xff, 0xff};\n    static const uint8_t vlan[] = {0x81, 0x00};\n    uint8_t *ptr = (uint8_t *)buf;\n    int i;\n\n    if (n->promisc)\n        return 1;\n\n    ptr += n->host_hdr_len;\n\n    if (!memcmp(&ptr[12], vlan, sizeof(vlan))) {\n        int vid = lduw_be_p(ptr + 14) & 0xfff;\n        if (!(n->vlans[vid >> 5] & (1U << (vid & 0x1f))))\n            return 0;\n    }\n\n    if (ptr[0] & 1) { // multicast\n        if (!memcmp(ptr, bcast, sizeof(bcast))) {\n            return !n->nobcast;\n        } else if (n->nomulti) {\n            return 0;\n        } else if (n->allmulti || n->mac_table.multi_overflow) {\n            return 1;\n        }\n\n        for (i = n->mac_table.first_multi; i < n->mac_table.in_use; i++) {\n            if (!memcmp(ptr, &n->mac_table.macs[i * ETH_ALEN], ETH_ALEN)) {\n                return 1;\n            }\n        }\n    } else { // unicast\n        if (n->nouni) {\n            return 0;\n        } else if (n->alluni || n->mac_table.uni_overflow) {\n            return 1;\n        } else if (!memcmp(ptr, n->mac, ETH_ALEN)) {\n            return 1;\n        }\n\n        for (i = 0; i < n->mac_table.first_multi; i++) {\n            if (!memcmp(ptr, &n->mac_table.macs[i * ETH_ALEN], ETH_ALEN)) {\n                return 1;\n            }\n        }\n    }\n\n    return 0;\n}\n\nstatic uint8_t virtio_net_get_hash_type(bool isip4,\n                                        bool isip6,\n                                        bool isudp,\n                                        bool istcp,\n                                        uint32_t types)\n{\n    if (isip4) {\n        if (istcp && (types & VIRTIO_NET_RSS_HASH_TYPE_TCPv4)) {\n            return NetPktRssIpV4Tcp;\n        }\n        if (isudp && (types & VIRTIO_NET_RSS_HASH_TYPE_UDPv4)) {\n            return NetPktRssIpV4Udp;\n        }\n        if (types & VIRTIO_NET_RSS_HASH_TYPE_IPv4) {\n            return NetPktRssIpV4;\n        }\n    } else if (isip6) {\n        uint32_t mask = VIRTIO_NET_RSS_HASH_TYPE_TCP_EX |\n                        VIRTIO_NET_RSS_HASH_TYPE_TCPv6;\n\n        if (istcp && (types & mask)) {\n            return (types & VIRTIO_NET_RSS_HASH_TYPE_TCP_EX) ?\n                NetPktRssIpV6TcpEx : NetPktRssIpV6Tcp;\n        }\n        mask = VIRTIO_NET_RSS_HASH_TYPE_UDP_EX | VIRTIO_NET_RSS_HASH_TYPE_UDPv6;\n        if (isudp && (types & mask)) {\n            return (types & VIRTIO_NET_RSS_HASH_TYPE_UDP_EX) ?\n                NetPktRssIpV6UdpEx : NetPktRssIpV6Udp;\n        }\n        mask = VIRTIO_NET_RSS_HASH_TYPE_IP_EX | VIRTIO_NET_RSS_HASH_TYPE_IPv6;\n        if (types & mask) {\n            return (types & VIRTIO_NET_RSS_HASH_TYPE_IP_EX) ?\n                NetPktRssIpV6Ex : NetPktRssIpV6;\n        }\n    }\n    return 0xff;\n}\n\nstatic void virtio_set_packet_hash(const uint8_t *buf, uint8_t report,\n                                   uint32_t hash)\n{\n    struct virtio_net_hdr_v1_hash *hdr = (void *)buf;\n    hdr->hash_value = hash;\n    hdr->hash_report = report;\n}\n\nstatic int virtio_net_process_rss(NetClientState *nc, const uint8_t *buf,\n                                  size_t size)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    unsigned int index = nc->queue_index, new_index = index;\n    struct NetRxPkt *pkt = n->rx_pkt;\n    uint8_t net_hash_type;\n    uint32_t hash;\n    bool isip4, isip6, isudp, istcp;\n    static const uint8_t reports[NetPktRssIpV6UdpEx + 1] = {\n        VIRTIO_NET_HASH_REPORT_IPv4,\n        VIRTIO_NET_HASH_REPORT_TCPv4,\n        VIRTIO_NET_HASH_REPORT_TCPv6,\n        VIRTIO_NET_HASH_REPORT_IPv6,\n        VIRTIO_NET_HASH_REPORT_IPv6_EX,\n        VIRTIO_NET_HASH_REPORT_TCPv6_EX,\n        VIRTIO_NET_HASH_REPORT_UDPv4,\n        VIRTIO_NET_HASH_REPORT_UDPv6,\n        VIRTIO_NET_HASH_REPORT_UDPv6_EX\n    };\n\n    net_rx_pkt_set_protocols(pkt, buf + n->host_hdr_len,\n                             size - n->host_hdr_len);\n    net_rx_pkt_get_protocols(pkt, &isip4, &isip6, &isudp, &istcp);\n    if (isip4 && (net_rx_pkt_get_ip4_info(pkt)->fragment)) {\n        istcp = isudp = false;\n    }\n    if (isip6 && (net_rx_pkt_get_ip6_info(pkt)->fragment)) {\n        istcp = isudp = false;\n    }\n    net_hash_type = virtio_net_get_hash_type(isip4, isip6, isudp, istcp,\n                                             n->rss_data.hash_types);\n    if (net_hash_type > NetPktRssIpV6UdpEx) {\n        if (n->rss_data.populate_hash) {\n            virtio_set_packet_hash(buf, VIRTIO_NET_HASH_REPORT_NONE, 0);\n        }\n        return n->rss_data.redirect ? n->rss_data.default_queue : -1;\n    }\n\n    hash = net_rx_pkt_calc_rss_hash(pkt, net_hash_type, n->rss_data.key);\n\n    if (n->rss_data.populate_hash) {\n        virtio_set_packet_hash(buf, reports[net_hash_type], hash);\n    }\n\n    if (n->rss_data.redirect) {\n        new_index = hash & (n->rss_data.indirections_len - 1);\n        new_index = n->rss_data.indirections_table[new_index];\n    }\n\n    return (index == new_index) ? -1 : new_index;\n}\n\nstatic ssize_t virtio_net_receive_rcu(NetClientState *nc, const uint8_t *buf,\n                                      size_t size, bool no_rss)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIONetQueue *q = virtio_net_get_subqueue(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    struct iovec mhdr_sg[VIRTQUEUE_MAX_SIZE];\n    struct virtio_net_hdr_mrg_rxbuf mhdr;\n    unsigned mhdr_cnt = 0;\n    size_t offset, i, guest_offset;\n\n    if (!virtio_net_can_receive(nc)) {\n        return -1;\n    }\n\n    if (!no_rss && n->rss_data.enabled && n->rss_data.enabled_software_rss) {\n        int index = virtio_net_process_rss(nc, buf, size);\n        if (index >= 0) {\n            NetClientState *nc2 = qemu_get_subqueue(n->nic, index);\n            return virtio_net_receive_rcu(nc2, buf, size, true);\n        }\n    }\n\n    /* hdr_len refers to the header we supply to the guest */\n    if (!virtio_net_has_buffers(q, size + n->guest_hdr_len - n->host_hdr_len)) {\n        return 0;\n    }\n\n    if (!receive_filter(n, buf, size))\n        return size;\n\n    offset = i = 0;\n\n    while (offset < size) {\n        VirtQueueElement *elem;\n        int len, total;\n        const struct iovec *sg;\n\n        total = 0;\n\n        elem = virtqueue_pop(q->rx_vq, sizeof(VirtQueueElement));\n        if (!elem) {\n            if (i) {\n                virtio_error(vdev, \"virtio-net unexpected empty queue: \"\n                             \"i %zd mergeable %d offset %zd, size %zd, \"\n                             \"guest hdr len %zd, host hdr len %zd \"\n                             \"guest features 0x%\" PRIx64,\n                             i, n->mergeable_rx_bufs, offset, size,\n                             n->guest_hdr_len, n->host_hdr_len,\n                             vdev->guest_features);\n            }\n            return -1;\n        }\n\n        if (elem->in_num < 1) {\n            virtio_error(vdev,\n                         \"virtio-net receive queue contains no in buffers\");\n            virtqueue_detach_element(q->rx_vq, elem, 0);\n            g_free(elem);\n            return -1;\n        }\n\n        sg = elem->in_sg;\n        if (i == 0) {\n            assert(offset == 0);\n            if (n->mergeable_rx_bufs) {\n                mhdr_cnt = iov_copy(mhdr_sg, ARRAY_SIZE(mhdr_sg),\n                                    sg, elem->in_num,\n                                    offsetof(typeof(mhdr), num_buffers),\n                                    sizeof(mhdr.num_buffers));\n            }\n\n            receive_header(n, sg, elem->in_num, buf, size);\n            if (n->rss_data.populate_hash) {\n                offset = sizeof(mhdr);\n                iov_from_buf(sg, elem->in_num, offset,\n                             buf + offset, n->host_hdr_len - sizeof(mhdr));\n            }\n            offset = n->host_hdr_len;\n            total += n->guest_hdr_len;\n            guest_offset = n->guest_hdr_len;\n        } else {\n            guest_offset = 0;\n        }\n\n        /* copy in packet.  ugh */\n        len = iov_from_buf(sg, elem->in_num, guest_offset,\n                           buf + offset, size - offset);\n        total += len;\n        offset += len;\n        /* If buffers can't be merged, at this point we\n         * must have consumed the complete packet.\n         * Otherwise, drop it. */\n        if (!n->mergeable_rx_bufs && offset < size) {\n            virtqueue_unpop(q->rx_vq, elem, total);\n            g_free(elem);\n            return size;\n        }\n\n        /* signal other side */\n        virtqueue_fill(q->rx_vq, elem, total, i++);\n        g_free(elem);\n    }\n\n    if (mhdr_cnt) {\n        virtio_stw_p(vdev, &mhdr.num_buffers, i);\n        iov_from_buf(mhdr_sg, mhdr_cnt,\n                     0,\n                     &mhdr.num_buffers, sizeof mhdr.num_buffers);\n    }\n\n    virtqueue_flush(q->rx_vq, i);\n    virtio_notify(vdev, q->rx_vq);\n\n    return size;\n}\n\nstatic ssize_t virtio_net_do_receive(NetClientState *nc, const uint8_t *buf,\n                                  size_t size)\n{\n    RCU_READ_LOCK_GUARD();\n\n    return virtio_net_receive_rcu(nc, buf, size, false);\n}\n\nstatic void virtio_net_rsc_extract_unit4(VirtioNetRscChain *chain,\n                                         const uint8_t *buf,\n                                         VirtioNetRscUnit *unit)\n{\n    uint16_t ip_hdrlen;\n    struct ip_header *ip;\n\n    ip = (struct ip_header *)(buf + chain->n->guest_hdr_len\n                              + sizeof(struct eth_header));\n    unit->ip = (void *)ip;\n    ip_hdrlen = (ip->ip_ver_len & 0xF) << 2;\n    unit->ip_plen = &ip->ip_len;\n    unit->tcp = (struct tcp_header *)(((uint8_t *)unit->ip) + ip_hdrlen);\n    unit->tcp_hdrlen = (htons(unit->tcp->th_offset_flags) & 0xF000) >> 10;\n    unit->payload = htons(*unit->ip_plen) - ip_hdrlen - unit->tcp_hdrlen;\n}\n\nstatic void virtio_net_rsc_extract_unit6(VirtioNetRscChain *chain,\n                                         const uint8_t *buf,\n                                         VirtioNetRscUnit *unit)\n{\n    struct ip6_header *ip6;\n\n    ip6 = (struct ip6_header *)(buf + chain->n->guest_hdr_len\n                                 + sizeof(struct eth_header));\n    unit->ip = ip6;\n    unit->ip_plen = &(ip6->ip6_ctlun.ip6_un1.ip6_un1_plen);\n    unit->tcp = (struct tcp_header *)(((uint8_t *)unit->ip)\n                                        + sizeof(struct ip6_header));\n    unit->tcp_hdrlen = (htons(unit->tcp->th_offset_flags) & 0xF000) >> 10;\n\n    /* There is a difference between payload lenght in ipv4 and v6,\n       ip header is excluded in ipv6 */\n    unit->payload = htons(*unit->ip_plen) - unit->tcp_hdrlen;\n}\n\nstatic size_t virtio_net_rsc_drain_seg(VirtioNetRscChain *chain,\n                                       VirtioNetRscSeg *seg)\n{\n    int ret;\n    struct virtio_net_hdr_v1 *h;\n\n    h = (struct virtio_net_hdr_v1 *)seg->buf;\n    h->flags = 0;\n    h->gso_type = VIRTIO_NET_HDR_GSO_NONE;\n\n    if (seg->is_coalesced) {\n        h->rsc.segments = seg->packets;\n        h->rsc.dup_acks = seg->dup_ack;\n        h->flags = VIRTIO_NET_HDR_F_RSC_INFO;\n        if (chain->proto == ETH_P_IP) {\n            h->gso_type = VIRTIO_NET_HDR_GSO_TCPV4;\n        } else {\n            h->gso_type = VIRTIO_NET_HDR_GSO_TCPV6;\n        }\n    }\n\n    ret = virtio_net_do_receive(seg->nc, seg->buf, seg->size);\n    QTAILQ_REMOVE(&chain->buffers, seg, next);\n    g_free(seg->buf);\n    g_free(seg);\n\n    return ret;\n}\n\nstatic void virtio_net_rsc_purge(void *opq)\n{\n    VirtioNetRscSeg *seg, *rn;\n    VirtioNetRscChain *chain = (VirtioNetRscChain *)opq;\n\n    QTAILQ_FOREACH_SAFE(seg, &chain->buffers, next, rn) {\n        if (virtio_net_rsc_drain_seg(chain, seg) == 0) {\n            chain->stat.purge_failed++;\n            continue;\n        }\n    }\n\n    chain->stat.timer++;\n    if (!QTAILQ_EMPTY(&chain->buffers)) {\n        timer_mod(chain->drain_timer,\n              qemu_clock_get_ns(QEMU_CLOCK_HOST) + chain->n->rsc_timeout);\n    }\n}\n\nstatic void virtio_net_rsc_cleanup(VirtIONet *n)\n{\n    VirtioNetRscChain *chain, *rn_chain;\n    VirtioNetRscSeg *seg, *rn_seg;\n\n    QTAILQ_FOREACH_SAFE(chain, &n->rsc_chains, next, rn_chain) {\n        QTAILQ_FOREACH_SAFE(seg, &chain->buffers, next, rn_seg) {\n            QTAILQ_REMOVE(&chain->buffers, seg, next);\n            g_free(seg->buf);\n            g_free(seg);\n        }\n\n        timer_free(chain->drain_timer);\n        QTAILQ_REMOVE(&n->rsc_chains, chain, next);\n        g_free(chain);\n    }\n}\n\nstatic void virtio_net_rsc_cache_buf(VirtioNetRscChain *chain,\n                                     NetClientState *nc,\n                                     const uint8_t *buf, size_t size)\n{\n    uint16_t hdr_len;\n    VirtioNetRscSeg *seg;\n\n    hdr_len = chain->n->guest_hdr_len;\n    seg = g_malloc(sizeof(VirtioNetRscSeg));\n    seg->buf = g_malloc(hdr_len + sizeof(struct eth_header)\n        + sizeof(struct ip6_header) + VIRTIO_NET_MAX_TCP_PAYLOAD);\n    memcpy(seg->buf, buf, size);\n    seg->size = size;\n    seg->packets = 1;\n    seg->dup_ack = 0;\n    seg->is_coalesced = 0;\n    seg->nc = nc;\n\n    QTAILQ_INSERT_TAIL(&chain->buffers, seg, next);\n    chain->stat.cache++;\n\n    switch (chain->proto) {\n    case ETH_P_IP:\n        virtio_net_rsc_extract_unit4(chain, seg->buf, &seg->unit);\n        break;\n    case ETH_P_IPV6:\n        virtio_net_rsc_extract_unit6(chain, seg->buf, &seg->unit);\n        break;\n    default:\n        g_assert_not_reached();\n    }\n}\n\nstatic int32_t virtio_net_rsc_handle_ack(VirtioNetRscChain *chain,\n                                         VirtioNetRscSeg *seg,\n                                         const uint8_t *buf,\n                                         struct tcp_header *n_tcp,\n                                         struct tcp_header *o_tcp)\n{\n    uint32_t nack, oack;\n    uint16_t nwin, owin;\n\n    nack = htonl(n_tcp->th_ack);\n    nwin = htons(n_tcp->th_win);\n    oack = htonl(o_tcp->th_ack);\n    owin = htons(o_tcp->th_win);\n\n    if ((nack - oack) >= VIRTIO_NET_MAX_TCP_PAYLOAD) {\n        chain->stat.ack_out_of_win++;\n        return RSC_FINAL;\n    } else if (nack == oack) {\n        /* duplicated ack or window probe */\n        if (nwin == owin) {\n            /* duplicated ack, add dup ack count due to whql test up to 1 */\n            chain->stat.dup_ack++;\n            return RSC_FINAL;\n        } else {\n            /* Coalesce window update */\n            o_tcp->th_win = n_tcp->th_win;\n            chain->stat.win_update++;\n            return RSC_COALESCE;\n        }\n    } else {\n        /* pure ack, go to 'C', finalize*/\n        chain->stat.pure_ack++;\n        return RSC_FINAL;\n    }\n}\n\nstatic int32_t virtio_net_rsc_coalesce_data(VirtioNetRscChain *chain,\n                                            VirtioNetRscSeg *seg,\n                                            const uint8_t *buf,\n                                            VirtioNetRscUnit *n_unit)\n{\n    void *data;\n    uint16_t o_ip_len;\n    uint32_t nseq, oseq;\n    VirtioNetRscUnit *o_unit;\n\n    o_unit = &seg->unit;\n    o_ip_len = htons(*o_unit->ip_plen);\n    nseq = htonl(n_unit->tcp->th_seq);\n    oseq = htonl(o_unit->tcp->th_seq);\n\n    /* out of order or retransmitted. */\n    if ((nseq - oseq) > VIRTIO_NET_MAX_TCP_PAYLOAD) {\n        chain->stat.data_out_of_win++;\n        return RSC_FINAL;\n    }\n\n    data = ((uint8_t *)n_unit->tcp) + n_unit->tcp_hdrlen;\n    if (nseq == oseq) {\n        if ((o_unit->payload == 0) && n_unit->payload) {\n            /* From no payload to payload, normal case, not a dup ack or etc */\n            chain->stat.data_after_pure_ack++;\n            goto coalesce;\n        } else {\n            return virtio_net_rsc_handle_ack(chain, seg, buf,\n                                             n_unit->tcp, o_unit->tcp);\n        }\n    } else if ((nseq - oseq) != o_unit->payload) {\n        /* Not a consistent packet, out of order */\n        chain->stat.data_out_of_order++;\n        return RSC_FINAL;\n    } else {\ncoalesce:\n        if ((o_ip_len + n_unit->payload) > chain->max_payload) {\n            chain->stat.over_size++;\n            return RSC_FINAL;\n        }\n\n        /* Here comes the right data, the payload length in v4/v6 is different,\n           so use the field value to update and record the new data len */\n        o_unit->payload += n_unit->payload; /* update new data len */\n\n        /* update field in ip header */\n        *o_unit->ip_plen = htons(o_ip_len + n_unit->payload);\n\n        /* Bring 'PUSH' big, the whql test guide says 'PUSH' can be coalesced\n           for windows guest, while this may change the behavior for linux\n           guest (only if it uses RSC feature). */\n        o_unit->tcp->th_offset_flags = n_unit->tcp->th_offset_flags;\n\n        o_unit->tcp->th_ack = n_unit->tcp->th_ack;\n        o_unit->tcp->th_win = n_unit->tcp->th_win;\n\n        memmove(seg->buf + seg->size, data, n_unit->payload);\n        seg->size += n_unit->payload;\n        seg->packets++;\n        chain->stat.coalesced++;\n        return RSC_COALESCE;\n    }\n}\n\nstatic int32_t virtio_net_rsc_coalesce4(VirtioNetRscChain *chain,\n                                        VirtioNetRscSeg *seg,\n                                        const uint8_t *buf, size_t size,\n                                        VirtioNetRscUnit *unit)\n{\n    struct ip_header *ip1, *ip2;\n\n    ip1 = (struct ip_header *)(unit->ip);\n    ip2 = (struct ip_header *)(seg->unit.ip);\n    if ((ip1->ip_src ^ ip2->ip_src) || (ip1->ip_dst ^ ip2->ip_dst)\n        || (unit->tcp->th_sport ^ seg->unit.tcp->th_sport)\n        || (unit->tcp->th_dport ^ seg->unit.tcp->th_dport)) {\n        chain->stat.no_match++;\n        return RSC_NO_MATCH;\n    }\n\n    return virtio_net_rsc_coalesce_data(chain, seg, buf, unit);\n}\n\nstatic int32_t virtio_net_rsc_coalesce6(VirtioNetRscChain *chain,\n                                        VirtioNetRscSeg *seg,\n                                        const uint8_t *buf, size_t size,\n                                        VirtioNetRscUnit *unit)\n{\n    struct ip6_header *ip1, *ip2;\n\n    ip1 = (struct ip6_header *)(unit->ip);\n    ip2 = (struct ip6_header *)(seg->unit.ip);\n    if (memcmp(&ip1->ip6_src, &ip2->ip6_src, sizeof(struct in6_address))\n        || memcmp(&ip1->ip6_dst, &ip2->ip6_dst, sizeof(struct in6_address))\n        || (unit->tcp->th_sport ^ seg->unit.tcp->th_sport)\n        || (unit->tcp->th_dport ^ seg->unit.tcp->th_dport)) {\n            chain->stat.no_match++;\n            return RSC_NO_MATCH;\n    }\n\n    return virtio_net_rsc_coalesce_data(chain, seg, buf, unit);\n}\n\n/* Packets with 'SYN' should bypass, other flag should be sent after drain\n * to prevent out of order */\nstatic int virtio_net_rsc_tcp_ctrl_check(VirtioNetRscChain *chain,\n                                         struct tcp_header *tcp)\n{\n    uint16_t tcp_hdr;\n    uint16_t tcp_flag;\n\n    tcp_flag = htons(tcp->th_offset_flags);\n    tcp_hdr = (tcp_flag & VIRTIO_NET_TCP_HDR_LENGTH) >> 10;\n    tcp_flag &= VIRTIO_NET_TCP_FLAG;\n    if (tcp_flag & TH_SYN) {\n        chain->stat.tcp_syn++;\n        return RSC_BYPASS;\n    }\n\n    if (tcp_flag & (TH_FIN | TH_URG | TH_RST | TH_ECE | TH_CWR)) {\n        chain->stat.tcp_ctrl_drain++;\n        return RSC_FINAL;\n    }\n\n    if (tcp_hdr > sizeof(struct tcp_header)) {\n        chain->stat.tcp_all_opt++;\n        return RSC_FINAL;\n    }\n\n    return RSC_CANDIDATE;\n}\n\nstatic size_t virtio_net_rsc_do_coalesce(VirtioNetRscChain *chain,\n                                         NetClientState *nc,\n                                         const uint8_t *buf, size_t size,\n                                         VirtioNetRscUnit *unit)\n{\n    int ret;\n    VirtioNetRscSeg *seg, *nseg;\n\n    if (QTAILQ_EMPTY(&chain->buffers)) {\n        chain->stat.empty_cache++;\n        virtio_net_rsc_cache_buf(chain, nc, buf, size);\n        timer_mod(chain->drain_timer,\n              qemu_clock_get_ns(QEMU_CLOCK_HOST) + chain->n->rsc_timeout);\n        return size;\n    }\n\n    QTAILQ_FOREACH_SAFE(seg, &chain->buffers, next, nseg) {\n        if (chain->proto == ETH_P_IP) {\n            ret = virtio_net_rsc_coalesce4(chain, seg, buf, size, unit);\n        } else {\n            ret = virtio_net_rsc_coalesce6(chain, seg, buf, size, unit);\n        }\n\n        if (ret == RSC_FINAL) {\n            if (virtio_net_rsc_drain_seg(chain, seg) == 0) {\n                /* Send failed */\n                chain->stat.final_failed++;\n                return 0;\n            }\n\n            /* Send current packet */\n            return virtio_net_do_receive(nc, buf, size);\n        } else if (ret == RSC_NO_MATCH) {\n            continue;\n        } else {\n            /* Coalesced, mark coalesced flag to tell calc cksum for ipv4 */\n            seg->is_coalesced = 1;\n            return size;\n        }\n    }\n\n    chain->stat.no_match_cache++;\n    virtio_net_rsc_cache_buf(chain, nc, buf, size);\n    return size;\n}\n\n/* Drain a connection data, this is to avoid out of order segments */\nstatic size_t virtio_net_rsc_drain_flow(VirtioNetRscChain *chain,\n                                        NetClientState *nc,\n                                        const uint8_t *buf, size_t size,\n                                        uint16_t ip_start, uint16_t ip_size,\n                                        uint16_t tcp_port)\n{\n    VirtioNetRscSeg *seg, *nseg;\n    uint32_t ppair1, ppair2;\n\n    ppair1 = *(uint32_t *)(buf + tcp_port);\n    QTAILQ_FOREACH_SAFE(seg, &chain->buffers, next, nseg) {\n        ppair2 = *(uint32_t *)(seg->buf + tcp_port);\n        if (memcmp(buf + ip_start, seg->buf + ip_start, ip_size)\n            || (ppair1 != ppair2)) {\n            continue;\n        }\n        if (virtio_net_rsc_drain_seg(chain, seg) == 0) {\n            chain->stat.drain_failed++;\n        }\n\n        break;\n    }\n\n    return virtio_net_do_receive(nc, buf, size);\n}\n\nstatic int32_t virtio_net_rsc_sanity_check4(VirtioNetRscChain *chain,\n                                            struct ip_header *ip,\n                                            const uint8_t *buf, size_t size)\n{\n    uint16_t ip_len;\n\n    /* Not an ipv4 packet */\n    if (((ip->ip_ver_len & 0xF0) >> 4) != IP_HEADER_VERSION_4) {\n        chain->stat.ip_option++;\n        return RSC_BYPASS;\n    }\n\n    /* Don't handle packets with ip option */\n    if ((ip->ip_ver_len & 0xF) != VIRTIO_NET_IP4_HEADER_LENGTH) {\n        chain->stat.ip_option++;\n        return RSC_BYPASS;\n    }\n\n    if (ip->ip_p != IPPROTO_TCP) {\n        chain->stat.bypass_not_tcp++;\n        return RSC_BYPASS;\n    }\n\n    /* Don't handle packets with ip fragment */\n    if (!(htons(ip->ip_off) & IP_DF)) {\n        chain->stat.ip_frag++;\n        return RSC_BYPASS;\n    }\n\n    /* Don't handle packets with ecn flag */\n    if (IPTOS_ECN(ip->ip_tos)) {\n        chain->stat.ip_ecn++;\n        return RSC_BYPASS;\n    }\n\n    ip_len = htons(ip->ip_len);\n    if (ip_len < (sizeof(struct ip_header) + sizeof(struct tcp_header))\n        || ip_len > (size - chain->n->guest_hdr_len -\n                     sizeof(struct eth_header))) {\n        chain->stat.ip_hacked++;\n        return RSC_BYPASS;\n    }\n\n    return RSC_CANDIDATE;\n}\n\nstatic size_t virtio_net_rsc_receive4(VirtioNetRscChain *chain,\n                                      NetClientState *nc,\n                                      const uint8_t *buf, size_t size)\n{\n    int32_t ret;\n    uint16_t hdr_len;\n    VirtioNetRscUnit unit;\n\n    hdr_len = ((VirtIONet *)(chain->n))->guest_hdr_len;\n\n    if (size < (hdr_len + sizeof(struct eth_header) + sizeof(struct ip_header)\n        + sizeof(struct tcp_header))) {\n        chain->stat.bypass_not_tcp++;\n        return virtio_net_do_receive(nc, buf, size);\n    }\n\n    virtio_net_rsc_extract_unit4(chain, buf, &unit);\n    if (virtio_net_rsc_sanity_check4(chain, unit.ip, buf, size)\n        != RSC_CANDIDATE) {\n        return virtio_net_do_receive(nc, buf, size);\n    }\n\n    ret = virtio_net_rsc_tcp_ctrl_check(chain, unit.tcp);\n    if (ret == RSC_BYPASS) {\n        return virtio_net_do_receive(nc, buf, size);\n    } else if (ret == RSC_FINAL) {\n        return virtio_net_rsc_drain_flow(chain, nc, buf, size,\n                ((hdr_len + sizeof(struct eth_header)) + 12),\n                VIRTIO_NET_IP4_ADDR_SIZE,\n                hdr_len + sizeof(struct eth_header) + sizeof(struct ip_header));\n    }\n\n    return virtio_net_rsc_do_coalesce(chain, nc, buf, size, &unit);\n}\n\nstatic int32_t virtio_net_rsc_sanity_check6(VirtioNetRscChain *chain,\n                                            struct ip6_header *ip6,\n                                            const uint8_t *buf, size_t size)\n{\n    uint16_t ip_len;\n\n    if (((ip6->ip6_ctlun.ip6_un1.ip6_un1_flow & 0xF0) >> 4)\n        != IP_HEADER_VERSION_6) {\n        return RSC_BYPASS;\n    }\n\n    /* Both option and protocol is checked in this */\n    if (ip6->ip6_ctlun.ip6_un1.ip6_un1_nxt != IPPROTO_TCP) {\n        chain->stat.bypass_not_tcp++;\n        return RSC_BYPASS;\n    }\n\n    ip_len = htons(ip6->ip6_ctlun.ip6_un1.ip6_un1_plen);\n    if (ip_len < sizeof(struct tcp_header) ||\n        ip_len > (size - chain->n->guest_hdr_len - sizeof(struct eth_header)\n                  - sizeof(struct ip6_header))) {\n        chain->stat.ip_hacked++;\n        return RSC_BYPASS;\n    }\n\n    /* Don't handle packets with ecn flag */\n    if (IP6_ECN(ip6->ip6_ctlun.ip6_un3.ip6_un3_ecn)) {\n        chain->stat.ip_ecn++;\n        return RSC_BYPASS;\n    }\n\n    return RSC_CANDIDATE;\n}\n\nstatic size_t virtio_net_rsc_receive6(void *opq, NetClientState *nc,\n                                      const uint8_t *buf, size_t size)\n{\n    int32_t ret;\n    uint16_t hdr_len;\n    VirtioNetRscChain *chain;\n    VirtioNetRscUnit unit;\n\n    chain = (VirtioNetRscChain *)opq;\n    hdr_len = ((VirtIONet *)(chain->n))->guest_hdr_len;\n\n    if (size < (hdr_len + sizeof(struct eth_header) + sizeof(struct ip6_header)\n        + sizeof(tcp_header))) {\n        return virtio_net_do_receive(nc, buf, size);\n    }\n\n    virtio_net_rsc_extract_unit6(chain, buf, &unit);\n    if (RSC_CANDIDATE != virtio_net_rsc_sanity_check6(chain,\n                                                 unit.ip, buf, size)) {\n        return virtio_net_do_receive(nc, buf, size);\n    }\n\n    ret = virtio_net_rsc_tcp_ctrl_check(chain, unit.tcp);\n    if (ret == RSC_BYPASS) {\n        return virtio_net_do_receive(nc, buf, size);\n    } else if (ret == RSC_FINAL) {\n        return virtio_net_rsc_drain_flow(chain, nc, buf, size,\n                ((hdr_len + sizeof(struct eth_header)) + 8),\n                VIRTIO_NET_IP6_ADDR_SIZE,\n                hdr_len + sizeof(struct eth_header)\n                + sizeof(struct ip6_header));\n    }\n\n    return virtio_net_rsc_do_coalesce(chain, nc, buf, size, &unit);\n}\n\nstatic VirtioNetRscChain *virtio_net_rsc_lookup_chain(VirtIONet *n,\n                                                      NetClientState *nc,\n                                                      uint16_t proto)\n{\n    VirtioNetRscChain *chain;\n\n    if ((proto != (uint16_t)ETH_P_IP) && (proto != (uint16_t)ETH_P_IPV6)) {\n        return NULL;\n    }\n\n    QTAILQ_FOREACH(chain, &n->rsc_chains, next) {\n        if (chain->proto == proto) {\n            return chain;\n        }\n    }\n\n    chain = g_malloc(sizeof(*chain));\n    chain->n = n;\n    chain->proto = proto;\n    if (proto == (uint16_t)ETH_P_IP) {\n        chain->max_payload = VIRTIO_NET_MAX_IP4_PAYLOAD;\n        chain->gso_type = VIRTIO_NET_HDR_GSO_TCPV4;\n    } else {\n        chain->max_payload = VIRTIO_NET_MAX_IP6_PAYLOAD;\n        chain->gso_type = VIRTIO_NET_HDR_GSO_TCPV6;\n    }\n    chain->drain_timer = timer_new_ns(QEMU_CLOCK_HOST,\n                                      virtio_net_rsc_purge, chain);\n    memset(&chain->stat, 0, sizeof(chain->stat));\n\n    QTAILQ_INIT(&chain->buffers);\n    QTAILQ_INSERT_TAIL(&n->rsc_chains, chain, next);\n\n    return chain;\n}\n\nstatic ssize_t virtio_net_rsc_receive(NetClientState *nc,\n                                      const uint8_t *buf,\n                                      size_t size)\n{\n    uint16_t proto;\n    VirtioNetRscChain *chain;\n    struct eth_header *eth;\n    VirtIONet *n;\n\n    n = qemu_get_nic_opaque(nc);\n    if (size < (n->host_hdr_len + sizeof(struct eth_header))) {\n        return virtio_net_do_receive(nc, buf, size);\n    }\n\n    eth = (struct eth_header *)(buf + n->guest_hdr_len);\n    proto = htons(eth->h_proto);\n\n    chain = virtio_net_rsc_lookup_chain(n, nc, proto);\n    if (chain) {\n        chain->stat.received++;\n        if (proto == (uint16_t)ETH_P_IP && n->rsc4_enabled) {\n            return virtio_net_rsc_receive4(chain, nc, buf, size);\n        } else if (proto == (uint16_t)ETH_P_IPV6 && n->rsc6_enabled) {\n            return virtio_net_rsc_receive6(chain, nc, buf, size);\n        }\n    }\n    return virtio_net_do_receive(nc, buf, size);\n}\n\nstatic ssize_t virtio_net_receive(NetClientState *nc, const uint8_t *buf,\n                                  size_t size)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    if ((n->rsc4_enabled || n->rsc6_enabled)) {\n        return virtio_net_rsc_receive(nc, buf, size);\n    } else {\n        return virtio_net_do_receive(nc, buf, size);\n    }\n}\n\nstatic int32_t virtio_net_flush_tx(VirtIONetQueue *q);\n\nstatic void virtio_net_tx_complete(NetClientState *nc, ssize_t len)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIONetQueue *q = virtio_net_get_subqueue(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n\n    virtqueue_push(q->tx_vq, q->async_tx.elem, 0);\n    virtio_notify(vdev, q->tx_vq);\n\n    g_free(q->async_tx.elem);\n    q->async_tx.elem = NULL;\n\n    virtio_queue_set_notification(q->tx_vq, 1);\n    virtio_net_flush_tx(q);\n}\n\n/* TX */\nstatic int32_t virtio_net_flush_tx(VirtIONetQueue *q)\n{\n    VirtIONet *n = q->n;\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    VirtQueueElement *elem;\n    int32_t num_packets = 0;\n    int queue_index = vq2q(virtio_get_queue_index(q->tx_vq));\n    if (!(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK)) {\n        return num_packets;\n    }\n\n    if (q->async_tx.elem) {\n        virtio_queue_set_notification(q->tx_vq, 0);\n        return num_packets;\n    }\n\n    for (;;) {\n        ssize_t ret;\n        unsigned int out_num;\n        struct iovec sg[VIRTQUEUE_MAX_SIZE], sg2[VIRTQUEUE_MAX_SIZE + 1], *out_sg;\n        struct virtio_net_hdr_mrg_rxbuf mhdr;\n\n        elem = virtqueue_pop(q->tx_vq, sizeof(VirtQueueElement));\n        if (!elem) {\n            break;\n        }\n\n        out_num = elem->out_num;\n        out_sg = elem->out_sg;\n        if (out_num < 1) {\n            virtio_error(vdev, \"virtio-net header not in first element\");\n            virtqueue_detach_element(q->tx_vq, elem, 0);\n            g_free(elem);\n            return -EINVAL;\n        }\n\n        if (n->has_vnet_hdr) {\n            if (iov_to_buf(out_sg, out_num, 0, &mhdr, n->guest_hdr_len) <\n                n->guest_hdr_len) {\n                virtio_error(vdev, \"virtio-net header incorrect\");\n                virtqueue_detach_element(q->tx_vq, elem, 0);\n                g_free(elem);\n                return -EINVAL;\n            }\n            if (n->needs_vnet_hdr_swap) {\n                virtio_net_hdr_swap(vdev, (void *) &mhdr);\n                sg2[0].iov_base = &mhdr;\n                sg2[0].iov_len = n->guest_hdr_len;\n                out_num = iov_copy(&sg2[1], ARRAY_SIZE(sg2) - 1,\n                                   out_sg, out_num,\n                                   n->guest_hdr_len, -1);\n                if (out_num == VIRTQUEUE_MAX_SIZE) {\n                    goto drop;\n                }\n                out_num += 1;\n                out_sg = sg2;\n            }\n        }\n        /*\n         * If host wants to see the guest header as is, we can\n         * pass it on unchanged. Otherwise, copy just the parts\n         * that host is interested in.\n         */\n        assert(n->host_hdr_len <= n->guest_hdr_len);\n        if (n->host_hdr_len != n->guest_hdr_len) {\n            unsigned sg_num = iov_copy(sg, ARRAY_SIZE(sg),\n                                       out_sg, out_num,\n                                       0, n->host_hdr_len);\n            sg_num += iov_copy(sg + sg_num, ARRAY_SIZE(sg) - sg_num,\n                             out_sg, out_num,\n                             n->guest_hdr_len, -1);\n            out_num = sg_num;\n            out_sg = sg;\n        }\n\n        ret = qemu_sendv_packet_async(qemu_get_subqueue(n->nic, queue_index),\n                                      out_sg, out_num, virtio_net_tx_complete);\n        if (ret == 0) {\n            virtio_queue_set_notification(q->tx_vq, 0);\n            q->async_tx.elem = elem;\n            return -EBUSY;\n        }\n\ndrop:\n        virtqueue_push(q->tx_vq, elem, 0);\n        virtio_notify(vdev, q->tx_vq);\n        g_free(elem);\n\n        if (++num_packets >= n->tx_burst) {\n            break;\n        }\n    }\n    return num_packets;\n}\n\nstatic void virtio_net_handle_tx_timer(VirtIODevice *vdev, VirtQueue *vq)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    VirtIONetQueue *q = &n->vqs[vq2q(virtio_get_queue_index(vq))];\n\n    if (unlikely((n->status & VIRTIO_NET_S_LINK_UP) == 0)) {\n        virtio_net_drop_tx_queue_data(vdev, vq);\n        return;\n    }\n\n    /* This happens when device was stopped but VCPU wasn't. */\n    if (!vdev->vm_running) {\n        q->tx_waiting = 1;\n        return;\n    }\n\n    if (q->tx_waiting) {\n        virtio_queue_set_notification(vq, 1);\n        timer_del(q->tx_timer);\n        q->tx_waiting = 0;\n        if (virtio_net_flush_tx(q) == -EINVAL) {\n            return;\n        }\n    } else {\n        timer_mod(q->tx_timer,\n                       qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL) + n->tx_timeout);\n        q->tx_waiting = 1;\n        virtio_queue_set_notification(vq, 0);\n    }\n}\n\nstatic void virtio_net_handle_tx_bh(VirtIODevice *vdev, VirtQueue *vq)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    VirtIONetQueue *q = &n->vqs[vq2q(virtio_get_queue_index(vq))];\n\n    if (unlikely((n->status & VIRTIO_NET_S_LINK_UP) == 0)) {\n        virtio_net_drop_tx_queue_data(vdev, vq);\n        return;\n    }\n\n    if (unlikely(q->tx_waiting)) {\n        return;\n    }\n    q->tx_waiting = 1;\n    /* This happens when device was stopped but VCPU wasn't. */\n    if (!vdev->vm_running) {\n        return;\n    }\n    virtio_queue_set_notification(vq, 0);\n    qemu_bh_schedule(q->tx_bh);\n}\n\nstatic void virtio_net_tx_timer(void *opaque)\n{\n    VirtIONetQueue *q = opaque;\n    VirtIONet *n = q->n;\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    /* This happens when device was stopped but BH wasn't. */\n    if (!vdev->vm_running) {\n        /* Make sure tx waiting is set, so we'll run when restarted. */\n        assert(q->tx_waiting);\n        return;\n    }\n\n    q->tx_waiting = 0;\n\n    /* Just in case the driver is not ready on more */\n    if (!(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK)) {\n        return;\n    }\n\n    virtio_queue_set_notification(q->tx_vq, 1);\n    virtio_net_flush_tx(q);\n}\n\nstatic void virtio_net_tx_bh(void *opaque)\n{\n    VirtIONetQueue *q = opaque;\n    VirtIONet *n = q->n;\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    int32_t ret;\n\n    /* This happens when device was stopped but BH wasn't. */\n    if (!vdev->vm_running) {\n        /* Make sure tx waiting is set, so we'll run when restarted. */\n        assert(q->tx_waiting);\n        return;\n    }\n\n    q->tx_waiting = 0;\n\n    /* Just in case the driver is not ready on more */\n    if (unlikely(!(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK))) {\n        return;\n    }\n\n    ret = virtio_net_flush_tx(q);\n    if (ret == -EBUSY || ret == -EINVAL) {\n        return; /* Notification re-enable handled by tx_complete or device\n                 * broken */\n    }\n\n    /* If we flush a full burst of packets, assume there are\n     * more coming and immediately reschedule */\n    if (ret >= n->tx_burst) {\n        qemu_bh_schedule(q->tx_bh);\n        q->tx_waiting = 1;\n        return;\n    }\n\n    /* If less than a full burst, re-enable notification and flush\n     * anything that may have come in while we weren't looking.  If\n     * we find something, assume the guest is still active and reschedule */\n    virtio_queue_set_notification(q->tx_vq, 1);\n    ret = virtio_net_flush_tx(q);\n    if (ret == -EINVAL) {\n        return;\n    } else if (ret > 0) {\n        virtio_queue_set_notification(q->tx_vq, 0);\n        qemu_bh_schedule(q->tx_bh);\n        q->tx_waiting = 1;\n    }\n}\n\nstatic void virtio_net_add_queue(VirtIONet *n, int index)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n\n    n->vqs[index].rx_vq = virtio_add_queue(vdev, n->net_conf.rx_queue_size,\n                                           virtio_net_handle_rx);\n\n    if (n->net_conf.tx && !strcmp(n->net_conf.tx, \"timer\")) {\n        n->vqs[index].tx_vq =\n            virtio_add_queue(vdev, n->net_conf.tx_queue_size,\n                             virtio_net_handle_tx_timer);\n        n->vqs[index].tx_timer = timer_new_ns(QEMU_CLOCK_VIRTUAL,\n                                              virtio_net_tx_timer,\n                                              &n->vqs[index]);\n    } else {\n        n->vqs[index].tx_vq =\n            virtio_add_queue(vdev, n->net_conf.tx_queue_size,\n                             virtio_net_handle_tx_bh);\n        n->vqs[index].tx_bh = qemu_bh_new(virtio_net_tx_bh, &n->vqs[index]);\n    }\n\n    n->vqs[index].tx_waiting = 0;\n    n->vqs[index].n = n;\n}\n\nstatic void virtio_net_del_queue(VirtIONet *n, int index)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    VirtIONetQueue *q = &n->vqs[index];\n    NetClientState *nc = qemu_get_subqueue(n->nic, index);\n\n    qemu_purge_queued_packets(nc);\n\n    virtio_del_queue(vdev, index * 2);\n    if (q->tx_timer) {\n        timer_free(q->tx_timer);\n        q->tx_timer = NULL;\n    } else {\n        qemu_bh_delete(q->tx_bh);\n        q->tx_bh = NULL;\n    }\n    q->tx_waiting = 0;\n    virtio_del_queue(vdev, index * 2 + 1);\n}\n\nstatic void virtio_net_change_num_queues(VirtIONet *n, int new_max_queues)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    int old_num_queues = virtio_get_num_queues(vdev);\n    int new_num_queues = new_max_queues * 2 + 1;\n    int i;\n\n    assert(old_num_queues >= 3);\n    assert(old_num_queues % 2 == 1);\n\n    if (old_num_queues == new_num_queues) {\n        return;\n    }\n\n    /*\n     * We always need to remove and add ctrl vq if\n     * old_num_queues != new_num_queues. Remove ctrl_vq first,\n     * and then we only enter one of the following two loops.\n     */\n    virtio_del_queue(vdev, old_num_queues - 1);\n\n    for (i = new_num_queues - 1; i < old_num_queues - 1; i += 2) {\n        /* new_num_queues < old_num_queues */\n        virtio_net_del_queue(n, i / 2);\n    }\n\n    for (i = old_num_queues - 1; i < new_num_queues - 1; i += 2) {\n        /* new_num_queues > old_num_queues */\n        virtio_net_add_queue(n, i / 2);\n    }\n\n    /* add ctrl_vq last */\n    n->ctrl_vq = virtio_add_queue(vdev, 64, virtio_net_handle_ctrl);\n}\n\nstatic void virtio_net_set_multiqueue(VirtIONet *n, int multiqueue)\n{\n    int max = multiqueue ? n->max_queues : 1;\n\n    n->multiqueue = multiqueue;\n    virtio_net_change_num_queues(n, max);\n\n    virtio_net_set_queues(n);\n}\n\nstatic int virtio_net_post_load_device(void *opaque, int version_id)\n{\n    VirtIONet *n = opaque;\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    int i, link_down;\n\n    trace_virtio_net_post_load_device();\n    virtio_net_set_mrg_rx_bufs(n, n->mergeable_rx_bufs,\n                               virtio_vdev_has_feature(vdev,\n                                                       VIRTIO_F_VERSION_1),\n                               virtio_vdev_has_feature(vdev,\n                                                       VIRTIO_NET_F_HASH_REPORT));\n\n    /* MAC_TABLE_ENTRIES may be different from the saved image */\n    if (n->mac_table.in_use > MAC_TABLE_ENTRIES) {\n        n->mac_table.in_use = 0;\n    }\n\n    if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_GUEST_OFFLOADS)) {\n        n->curr_guest_offloads = virtio_net_supported_guest_offloads(n);\n    }\n\n    /*\n     * curr_guest_offloads will be later overwritten by the\n     * virtio_set_features_nocheck call done from the virtio_load.\n     * Here we make sure it is preserved and restored accordingly\n     * in the virtio_net_post_load_virtio callback.\n     */\n    n->saved_guest_offloads = n->curr_guest_offloads;\n\n    virtio_net_set_queues(n);\n\n    /* Find the first multicast entry in the saved MAC filter */\n    for (i = 0; i < n->mac_table.in_use; i++) {\n        if (n->mac_table.macs[i * ETH_ALEN] & 1) {\n            break;\n        }\n    }\n    n->mac_table.first_multi = i;\n\n    /* nc.link_down can't be migrated, so infer link_down according\n     * to link status bit in n->status */\n    link_down = (n->status & VIRTIO_NET_S_LINK_UP) == 0;\n    for (i = 0; i < n->max_queues; i++) {\n        qemu_get_subqueue(n->nic, i)->link_down = link_down;\n    }\n\n    if (virtio_vdev_has_feature(vdev, VIRTIO_NET_F_GUEST_ANNOUNCE) &&\n        virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_VQ)) {\n        qemu_announce_timer_reset(&n->announce_timer, migrate_announce_params(),\n                                  QEMU_CLOCK_VIRTUAL,\n                                  virtio_net_announce_timer, n);\n        if (n->announce_timer.round) {\n            timer_mod(n->announce_timer.tm,\n                      qemu_clock_get_ms(n->announce_timer.type));\n        } else {\n            qemu_announce_timer_del(&n->announce_timer, false);\n        }\n    }\n\n    if (n->rss_data.enabled) {\n        n->rss_data.enabled_software_rss = n->rss_data.populate_hash;\n        if (!n->rss_data.populate_hash) {\n            if (!virtio_net_attach_epbf_rss(n)) {\n                if (get_vhost_net(qemu_get_queue(n->nic)->peer)) {\n                    warn_report(\"Can't post-load eBPF RSS for vhost\");\n                } else {\n                    warn_report(\"Can't post-load eBPF RSS - \"\n                                \"fallback to software RSS\");\n                    n->rss_data.enabled_software_rss = true;\n                }\n            }\n        }\n\n        trace_virtio_net_rss_enable(n->rss_data.hash_types,\n                                    n->rss_data.indirections_len,\n                                    sizeof(n->rss_data.key));\n    } else {\n        trace_virtio_net_rss_disable();\n    }\n    return 0;\n}\n\nstatic int virtio_net_post_load_virtio(VirtIODevice *vdev)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    /*\n     * The actual needed state is now in saved_guest_offloads,\n     * see virtio_net_post_load_device for detail.\n     * Restore it back and apply the desired offloads.\n     */\n    n->curr_guest_offloads = n->saved_guest_offloads;\n    if (peer_has_vnet_hdr(n)) {\n        virtio_net_apply_guest_offloads(n);\n    }\n\n    return 0;\n}\n\n/* tx_waiting field of a VirtIONetQueue */\nstatic const VMStateDescription vmstate_virtio_net_queue_tx_waiting = {\n    .name = \"virtio-net-queue-tx_waiting\",\n    .fields = (VMStateField[]) {\n        VMSTATE_UINT32(tx_waiting, VirtIONetQueue),\n        VMSTATE_END_OF_LIST()\n   },\n};\n\nstatic bool max_queues_gt_1(void *opaque, int version_id)\n{\n    return VIRTIO_NET(opaque)->max_queues > 1;\n}\n\nstatic bool has_ctrl_guest_offloads(void *opaque, int version_id)\n{\n    return virtio_vdev_has_feature(VIRTIO_DEVICE(opaque),\n                                   VIRTIO_NET_F_CTRL_GUEST_OFFLOADS);\n}\n\nstatic bool mac_table_fits(void *opaque, int version_id)\n{\n    return VIRTIO_NET(opaque)->mac_table.in_use <= MAC_TABLE_ENTRIES;\n}\n\nstatic bool mac_table_doesnt_fit(void *opaque, int version_id)\n{\n    return !mac_table_fits(opaque, version_id);\n}\n\n/* This temporary type is shared by all the WITH_TMP methods\n * although only some fields are used by each.\n */\nstruct VirtIONetMigTmp {\n    VirtIONet      *parent;\n    VirtIONetQueue *vqs_1;\n    uint16_t        curr_queues_1;\n    uint8_t         has_ufo;\n    uint32_t        has_vnet_hdr;\n};\n\n/* The 2nd and subsequent tx_waiting flags are loaded later than\n * the 1st entry in the queues and only if there's more than one\n * entry.  We use the tmp mechanism to calculate a temporary\n * pointer and count and also validate the count.\n */\n\nstatic int virtio_net_tx_waiting_pre_save(void *opaque)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    tmp->vqs_1 = tmp->parent->vqs + 1;\n    tmp->curr_queues_1 = tmp->parent->curr_queues - 1;\n    if (tmp->parent->curr_queues == 0) {\n        tmp->curr_queues_1 = 0;\n    }\n\n    return 0;\n}\n\nstatic int virtio_net_tx_waiting_pre_load(void *opaque)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    /* Reuse the pointer setup from save */\n    virtio_net_tx_waiting_pre_save(opaque);\n\n    if (tmp->parent->curr_queues > tmp->parent->max_queues) {\n        error_report(\"virtio-net: curr_queues %x > max_queues %x\",\n            tmp->parent->curr_queues, tmp->parent->max_queues);\n\n        return -EINVAL;\n    }\n\n    return 0; /* all good */\n}\n\nstatic const VMStateDescription vmstate_virtio_net_tx_waiting = {\n    .name      = \"virtio-net-tx_waiting\",\n    .pre_load  = virtio_net_tx_waiting_pre_load,\n    .pre_save  = virtio_net_tx_waiting_pre_save,\n    .fields    = (VMStateField[]) {\n        VMSTATE_STRUCT_VARRAY_POINTER_UINT16(vqs_1, struct VirtIONetMigTmp,\n                                     curr_queues_1,\n                                     vmstate_virtio_net_queue_tx_waiting,\n                                     struct VirtIONetQueue),\n        VMSTATE_END_OF_LIST()\n    },\n};\n\n/* the 'has_ufo' flag is just tested; if the incoming stream has the\n * flag set we need to check that we have it\n */\nstatic int virtio_net_ufo_post_load(void *opaque, int version_id)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    if (tmp->has_ufo && !peer_has_ufo(tmp->parent)) {\n        error_report(\"virtio-net: saved image requires TUN_F_UFO support\");\n        return -EINVAL;\n    }\n\n    return 0;\n}\n\nstatic int virtio_net_ufo_pre_save(void *opaque)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    tmp->has_ufo = tmp->parent->has_ufo;\n\n    return 0;\n}\n\nstatic const VMStateDescription vmstate_virtio_net_has_ufo = {\n    .name      = \"virtio-net-ufo\",\n    .post_load = virtio_net_ufo_post_load,\n    .pre_save  = virtio_net_ufo_pre_save,\n    .fields    = (VMStateField[]) {\n        VMSTATE_UINT8(has_ufo, struct VirtIONetMigTmp),\n        VMSTATE_END_OF_LIST()\n    },\n};\n\n/* the 'has_vnet_hdr' flag is just tested; if the incoming stream has the\n * flag set we need to check that we have it\n */\nstatic int virtio_net_vnet_post_load(void *opaque, int version_id)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    if (tmp->has_vnet_hdr && !peer_has_vnet_hdr(tmp->parent)) {\n        error_report(\"virtio-net: saved image requires vnet_hdr=on\");\n        return -EINVAL;\n    }\n\n    return 0;\n}\n\nstatic int virtio_net_vnet_pre_save(void *opaque)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    tmp->has_vnet_hdr = tmp->parent->has_vnet_hdr;\n\n    return 0;\n}\n\nstatic const VMStateDescription vmstate_virtio_net_has_vnet = {\n    .name      = \"virtio-net-vnet\",\n    .post_load = virtio_net_vnet_post_load,\n    .pre_save  = virtio_net_vnet_pre_save,\n    .fields    = (VMStateField[]) {\n        VMSTATE_UINT32(has_vnet_hdr, struct VirtIONetMigTmp),\n        VMSTATE_END_OF_LIST()\n    },\n};\n\nstatic bool virtio_net_rss_needed(void *opaque)\n{\n    return VIRTIO_NET(opaque)->rss_data.enabled;\n}\n\nstatic const VMStateDescription vmstate_virtio_net_rss = {\n    .name      = \"virtio-net-device/rss\",\n    .version_id = 1,\n    .minimum_version_id = 1,\n    .needed = virtio_net_rss_needed,\n    .fields = (VMStateField[]) {\n        VMSTATE_BOOL(rss_data.enabled, VirtIONet),\n        VMSTATE_BOOL(rss_data.redirect, VirtIONet),\n        VMSTATE_BOOL(rss_data.populate_hash, VirtIONet),\n        VMSTATE_UINT32(rss_data.hash_types, VirtIONet),\n        VMSTATE_UINT16(rss_data.indirections_len, VirtIONet),\n        VMSTATE_UINT16(rss_data.default_queue, VirtIONet),\n        VMSTATE_UINT8_ARRAY(rss_data.key, VirtIONet,\n                            VIRTIO_NET_RSS_MAX_KEY_SIZE),\n        VMSTATE_VARRAY_UINT16_ALLOC(rss_data.indirections_table, VirtIONet,\n                                    rss_data.indirections_len, 0,\n                                    vmstate_info_uint16, uint16_t),\n        VMSTATE_END_OF_LIST()\n    },\n};\n\nstatic const VMStateDescription vmstate_virtio_net_device = {\n    .name = \"virtio-net-device\",\n    .version_id = VIRTIO_NET_VM_VERSION,\n    .minimum_version_id = VIRTIO_NET_VM_VERSION,\n    .post_load = virtio_net_post_load_device,\n    .fields = (VMStateField[]) {\n        VMSTATE_UINT8_ARRAY(mac, VirtIONet, ETH_ALEN),\n        VMSTATE_STRUCT_POINTER(vqs, VirtIONet,\n                               vmstate_virtio_net_queue_tx_waiting,\n                               VirtIONetQueue),\n        VMSTATE_UINT32(mergeable_rx_bufs, VirtIONet),\n        VMSTATE_UINT16(status, VirtIONet),\n        VMSTATE_UINT8(promisc, VirtIONet),\n        VMSTATE_UINT8(allmulti, VirtIONet),\n        VMSTATE_UINT32(mac_table.in_use, VirtIONet),\n\n        /* Guarded pair: If it fits we load it, else we throw it away\n         * - can happen if source has a larger MAC table.; post-load\n         *  sets flags in this case.\n         */\n        VMSTATE_VBUFFER_MULTIPLY(mac_table.macs, VirtIONet,\n                                0, mac_table_fits, mac_table.in_use,\n                                 ETH_ALEN),\n        VMSTATE_UNUSED_VARRAY_UINT32(VirtIONet, mac_table_doesnt_fit, 0,\n                                     mac_table.in_use, ETH_ALEN),\n\n        /* Note: This is an array of uint32's that's always been saved as a\n         * buffer; hold onto your endiannesses; it's actually used as a bitmap\n         * but based on the uint.\n         */\n        VMSTATE_BUFFER_POINTER_UNSAFE(vlans, VirtIONet, 0, MAX_VLAN >> 3),\n        VMSTATE_WITH_TMP(VirtIONet, struct VirtIONetMigTmp,\n                         vmstate_virtio_net_has_vnet),\n        VMSTATE_UINT8(mac_table.multi_overflow, VirtIONet),\n        VMSTATE_UINT8(mac_table.uni_overflow, VirtIONet),\n        VMSTATE_UINT8(alluni, VirtIONet),\n        VMSTATE_UINT8(nomulti, VirtIONet),\n        VMSTATE_UINT8(nouni, VirtIONet),\n        VMSTATE_UINT8(nobcast, VirtIONet),\n        VMSTATE_WITH_TMP(VirtIONet, struct VirtIONetMigTmp,\n                         vmstate_virtio_net_has_ufo),\n        VMSTATE_SINGLE_TEST(max_queues, VirtIONet, max_queues_gt_1, 0,\n                            vmstate_info_uint16_equal, uint16_t),\n        VMSTATE_UINT16_TEST(curr_queues, VirtIONet, max_queues_gt_1),\n        VMSTATE_WITH_TMP(VirtIONet, struct VirtIONetMigTmp,\n                         vmstate_virtio_net_tx_waiting),\n        VMSTATE_UINT64_TEST(curr_guest_offloads, VirtIONet,\n                            has_ctrl_guest_offloads),\n        VMSTATE_END_OF_LIST()\n   },\n    .subsections = (const VMStateDescription * []) {\n        &vmstate_virtio_net_rss,\n        NULL\n    }\n};\n\nstatic NetClientInfo net_virtio_info = {\n    .type = NET_CLIENT_DRIVER_NIC,\n    .size = sizeof(NICState),\n    .can_receive = virtio_net_can_receive,\n    .receive = virtio_net_receive,\n    .link_status_changed = virtio_net_set_link_status,\n    .query_rx_filter = virtio_net_query_rxfilter,\n    .announce = virtio_net_announce,\n};\n\nstatic bool virtio_net_guest_notifier_pending(VirtIODevice *vdev, int idx)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    NetClientState *nc = qemu_get_subqueue(n->nic, vq2q(idx));\n    assert(n->vhost_started);\n    return vhost_net_virtqueue_pending(get_vhost_net(nc->peer), idx);\n}\n\nstatic void virtio_net_guest_notifier_mask(VirtIODevice *vdev, int idx,\n                                           bool mask)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    NetClientState *nc = qemu_get_subqueue(n->nic, vq2q(idx));\n    assert(n->vhost_started);\n    vhost_net_virtqueue_mask(get_vhost_net(nc->peer),\n                             vdev, idx, mask);\n}\n\nstatic void virtio_net_set_config_size(VirtIONet *n, uint64_t host_features)\n{\n    virtio_add_feature(&host_features, VIRTIO_NET_F_MAC);\n\n    n->config_size = virtio_feature_get_config_size(feature_sizes,\n                                                    host_features);\n}\n\nvoid virtio_net_set_netclient_name(VirtIONet *n, const char *name,\n                                   const char *type)\n{\n    /*\n     * The name can be NULL, the netclient name will be type.x.\n     */\n    assert(type != NULL);\n\n    g_free(n->netclient_name);\n    g_free(n->netclient_type);\n    n->netclient_name = g_strdup(name);\n    n->netclient_type = g_strdup(type);\n}\n\nstatic bool failover_unplug_primary(VirtIONet *n, DeviceState *dev)\n{\n    HotplugHandler *hotplug_ctrl;\n    PCIDevice *pci_dev;\n    Error *err = NULL;\n\n    hotplug_ctrl = qdev_get_hotplug_handler(dev);\n    if (hotplug_ctrl) {\n        pci_dev = PCI_DEVICE(dev);\n        pci_dev->partially_hotplugged = true;\n        hotplug_handler_unplug_request(hotplug_ctrl, dev, &err);\n        if (err) {\n            error_report_err(err);\n            return false;\n        }\n    } else {\n        return false;\n    }\n    return true;\n}\n\nstatic bool failover_replug_primary(VirtIONet *n, DeviceState *dev,\n                                    Error **errp)\n{\n    Error *err = NULL;\n    HotplugHandler *hotplug_ctrl;\n    PCIDevice *pdev = PCI_DEVICE(dev);\n    BusState *primary_bus;\n\n    if (!pdev->partially_hotplugged) {\n        return true;\n    }\n    primary_bus = dev->parent_bus;\n    if (!primary_bus) {\n        error_setg(errp, \"virtio_net: couldn't find primary bus\");\n        return false;\n    }\n    qdev_set_parent_bus(dev, primary_bus, &error_abort);\n    qatomic_set(&n->failover_primary_hidden, false);\n    hotplug_ctrl = qdev_get_hotplug_handler(dev);\n    if (hotplug_ctrl) {\n        hotplug_handler_pre_plug(hotplug_ctrl, dev, &err);\n        if (err) {\n            goto out;\n        }\n        hotplug_handler_plug(hotplug_ctrl, dev, &err);\n    }\n    pdev->partially_hotplugged = false;\n\nout:\n    error_propagate(errp, err);\n    return !err;\n}\n\nstatic void virtio_net_handle_migration_primary(VirtIONet *n, MigrationState *s)\n{\n    bool should_be_hidden;\n    Error *err = NULL;\n    DeviceState *dev = failover_find_primary_device(n);\n\n    if (!dev) {\n        return;\n    }\n\n    should_be_hidden = qatomic_read(&n->failover_primary_hidden);\n\n    if (migration_in_setup(s) && !should_be_hidden) {\n        if (failover_unplug_primary(n, dev)) {\n            vmstate_unregister(VMSTATE_IF(dev), qdev_get_vmsd(dev), dev);\n            qapi_event_send_unplug_primary(dev->id);\n            qatomic_set(&n->failover_primary_hidden, true);\n        } else {\n            warn_report(\"couldn't unplug primary device\");\n        }\n    } else if (migration_has_failed(s)) {\n        /* We already unplugged the device let's plug it back */\n        if (!failover_replug_primary(n, dev, &err)) {\n            if (err) {\n                error_report_err(err);\n            }\n        }\n    }\n}\n\nstatic void virtio_net_migration_state_notifier(Notifier *notifier, void *data)\n{\n    MigrationState *s = data;\n    VirtIONet *n = container_of(notifier, VirtIONet, migration_state);\n    virtio_net_handle_migration_primary(n, s);\n}\n\nstatic bool failover_hide_primary_device(DeviceListener *listener,\n                                         QemuOpts *device_opts)\n{\n    VirtIONet *n = container_of(listener, VirtIONet, primary_listener);\n    const char *standby_id;\n\n    if (!device_opts) {\n        return false;\n    }\n    standby_id = qemu_opt_get(device_opts, \"failover_pair_id\");\n    if (g_strcmp0(standby_id, n->netclient_name) != 0) {\n        return false;\n    }\n\n    /* failover_primary_hidden is set during feature negotiation */\n    return qatomic_read(&n->failover_primary_hidden);\n}\n\nstatic void virtio_net_device_realize(DeviceState *dev, Error **errp)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(dev);\n    VirtIONet *n = VIRTIO_NET(dev);\n    NetClientState *nc;\n    int i;\n\n    if (n->net_conf.mtu) {\n        n->host_features |= (1ULL << VIRTIO_NET_F_MTU);\n    }\n\n    if (n->net_conf.duplex_str) {\n        if (strncmp(n->net_conf.duplex_str, \"half\", 5) == 0) {\n            n->net_conf.duplex = DUPLEX_HALF;\n        } else if (strncmp(n->net_conf.duplex_str, \"full\", 5) == 0) {\n            n->net_conf.duplex = DUPLEX_FULL;\n        } else {\n            error_setg(errp, \"'duplex' must be 'half' or 'full'\");\n            return;\n        }\n        n->host_features |= (1ULL << VIRTIO_NET_F_SPEED_DUPLEX);\n    } else {\n        n->net_conf.duplex = DUPLEX_UNKNOWN;\n    }\n\n    if (n->net_conf.speed < SPEED_UNKNOWN) {\n        error_setg(errp, \"'speed' must be between 0 and INT_MAX\");\n        return;\n    }\n    if (n->net_conf.speed >= 0) {\n        n->host_features |= (1ULL << VIRTIO_NET_F_SPEED_DUPLEX);\n    }\n\n    if (n->failover) {\n        n->primary_listener.hide_device = failover_hide_primary_device;\n        qatomic_set(&n->failover_primary_hidden, true);\n        device_listener_register(&n->primary_listener);\n        n->migration_state.notify = virtio_net_migration_state_notifier;\n        add_migration_state_change_notifier(&n->migration_state);\n        n->host_features |= (1ULL << VIRTIO_NET_F_STANDBY);\n    }\n\n    virtio_net_set_config_size(n, n->host_features);\n    virtio_init(vdev, \"virtio-net\", VIRTIO_ID_NET, n->config_size);\n\n    /*\n     * We set a lower limit on RX queue size to what it always was.\n     * Guests that want a smaller ring can always resize it without\n     * help from us (using virtio 1 and up).\n     */\n    if (n->net_conf.rx_queue_size < VIRTIO_NET_RX_QUEUE_MIN_SIZE ||\n        n->net_conf.rx_queue_size > VIRTQUEUE_MAX_SIZE ||\n        !is_power_of_2(n->net_conf.rx_queue_size)) {\n        error_setg(errp, \"Invalid rx_queue_size (= %\" PRIu16 \"), \"\n                   \"must be a power of 2 between %d and %d.\",\n                   n->net_conf.rx_queue_size, VIRTIO_NET_RX_QUEUE_MIN_SIZE,\n                   VIRTQUEUE_MAX_SIZE);\n        virtio_cleanup(vdev);\n        return;\n    }\n\n    if (n->net_conf.tx_queue_size < VIRTIO_NET_TX_QUEUE_MIN_SIZE ||\n        n->net_conf.tx_queue_size > VIRTQUEUE_MAX_SIZE ||\n        !is_power_of_2(n->net_conf.tx_queue_size)) {\n        error_setg(errp, \"Invalid tx_queue_size (= %\" PRIu16 \"), \"\n                   \"must be a power of 2 between %d and %d\",\n                   n->net_conf.tx_queue_size, VIRTIO_NET_TX_QUEUE_MIN_SIZE,\n                   VIRTQUEUE_MAX_SIZE);\n        virtio_cleanup(vdev);\n        return;\n    }\n\n    n->max_queues = MAX(n->nic_conf.peers.queues, 1);\n    if (n->max_queues * 2 + 1 > VIRTIO_QUEUE_MAX) {\n        error_setg(errp, \"Invalid number of queues (= %\" PRIu32 \"), \"\n                   \"must be a positive integer less than %d.\",\n                   n->max_queues, (VIRTIO_QUEUE_MAX - 1) / 2);\n        virtio_cleanup(vdev);\n        return;\n    }\n    n->vqs = g_malloc0(sizeof(VirtIONetQueue) * n->max_queues);\n    n->curr_queues = 1;\n    n->tx_timeout = n->net_conf.txtimer;\n\n    if (n->net_conf.tx && strcmp(n->net_conf.tx, \"timer\")\n                       && strcmp(n->net_conf.tx, \"bh\")) {\n        warn_report(\"virtio-net: \"\n                    \"Unknown option tx=%s, valid options: \\\"timer\\\" \\\"bh\\\"\",\n                    n->net_conf.tx);\n        error_printf(\"Defaulting to \\\"bh\\\"\");\n    }\n\n    n->net_conf.tx_queue_size = MIN(virtio_net_max_tx_queue_size(n),\n                                    n->net_conf.tx_queue_size);\n\n    for (i = 0; i < n->max_queues; i++) {\n        virtio_net_add_queue(n, i);\n    }\n\n    n->ctrl_vq = virtio_add_queue(vdev, 64, virtio_net_handle_ctrl);\n    qemu_macaddr_default_if_unset(&n->nic_conf.macaddr);\n    memcpy(&n->mac[0], &n->nic_conf.macaddr, sizeof(n->mac));\n    n->status = VIRTIO_NET_S_LINK_UP;\n    qemu_announce_timer_reset(&n->announce_timer, migrate_announce_params(),\n                              QEMU_CLOCK_VIRTUAL,\n                              virtio_net_announce_timer, n);\n    n->announce_timer.round = 0;\n\n    if (n->netclient_type) {\n        /*\n         * Happen when virtio_net_set_netclient_name has been called.\n         */\n        n->nic = qemu_new_nic(&net_virtio_info, &n->nic_conf,\n                              n->netclient_type, n->netclient_name, n);\n    } else {\n        n->nic = qemu_new_nic(&net_virtio_info, &n->nic_conf,\n                              object_get_typename(OBJECT(dev)), dev->id, n);\n    }\n\n    for (i = 0; i < n->max_queues; i++) {\n        n->nic->ncs[i].do_not_pad = true;\n    }\n\n    peer_test_vnet_hdr(n);\n    if (peer_has_vnet_hdr(n)) {\n        for (i = 0; i < n->max_queues; i++) {\n            qemu_using_vnet_hdr(qemu_get_subqueue(n->nic, i)->peer, true);\n        }\n        n->host_hdr_len = sizeof(struct virtio_net_hdr);\n    } else {\n        n->host_hdr_len = 0;\n    }\n\n    qemu_format_nic_info_str(qemu_get_queue(n->nic), n->nic_conf.macaddr.a);\n\n    n->vqs[0].tx_waiting = 0;\n    n->tx_burst = n->net_conf.txburst;\n    virtio_net_set_mrg_rx_bufs(n, 0, 0, 0);\n    n->promisc = 1; /* for compatibility */\n\n    n->mac_table.macs = g_malloc0(MAC_TABLE_ENTRIES * ETH_ALEN);\n\n    n->vlans = g_malloc0(MAX_VLAN >> 3);\n\n    nc = qemu_get_queue(n->nic);\n    nc->rxfilter_notify_enabled = 1;\n\n   if (nc->peer && nc->peer->info->type == NET_CLIENT_DRIVER_VHOST_VDPA) {\n        struct virtio_net_config netcfg = {};\n        memcpy(&netcfg.mac, &n->nic_conf.macaddr, ETH_ALEN);\n        vhost_net_set_config(get_vhost_net(nc->peer),\n            (uint8_t *)&netcfg, 0, ETH_ALEN, VHOST_SET_CONFIG_TYPE_MASTER);\n    }\n    QTAILQ_INIT(&n->rsc_chains);\n    n->qdev = dev;\n\n    net_rx_pkt_init(&n->rx_pkt, false);\n\n    if (virtio_has_feature(n->host_features, VIRTIO_NET_F_RSS)) {\n        virtio_net_load_ebpf(n);\n    }\n}\n\nstatic void virtio_net_device_unrealize(DeviceState *dev)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(dev);\n    VirtIONet *n = VIRTIO_NET(dev);\n    int i, max_queues;\n\n    if (virtio_has_feature(n->host_features, VIRTIO_NET_F_RSS)) {\n        virtio_net_unload_ebpf(n);\n    }\n\n    /* This will stop vhost backend if appropriate. */\n    virtio_net_set_status(vdev, 0);\n\n    g_free(n->netclient_name);\n    n->netclient_name = NULL;\n    g_free(n->netclient_type);\n    n->netclient_type = NULL;\n\n    g_free(n->mac_table.macs);\n    g_free(n->vlans);\n\n    if (n->failover) {\n        device_listener_unregister(&n->primary_listener);\n        remove_migration_state_change_notifier(&n->migration_state);\n    }\n\n    max_queues = n->multiqueue ? n->max_queues : 1;\n    for (i = 0; i < max_queues; i++) {\n        virtio_net_del_queue(n, i);\n    }\n    /* delete also control vq */\n    virtio_del_queue(vdev, max_queues * 2);\n    qemu_announce_timer_del(&n->announce_timer, false);\n    g_free(n->vqs);\n    qemu_del_nic(n->nic);\n    virtio_net_rsc_cleanup(n);\n    g_free(n->rss_data.indirections_table);\n    net_rx_pkt_uninit(n->rx_pkt);\n    virtio_cleanup(vdev);\n}\n\nstatic void virtio_net_instance_init(Object *obj)\n{\n    VirtIONet *n = VIRTIO_NET(obj);\n\n    /*\n     * The default config_size is sizeof(struct virtio_net_config).\n     * Can be overriden with virtio_net_set_config_size.\n     */\n    n->config_size = sizeof(struct virtio_net_config);\n    device_add_bootindex_property(obj, &n->nic_conf.bootindex,\n                                  \"bootindex\", \"/ethernet-phy@0\",\n                                  DEVICE(n));\n\n    ebpf_rss_init(&n->ebpf_rss);\n}\n\nstatic int virtio_net_pre_save(void *opaque)\n{\n    VirtIONet *n = opaque;\n\n    /* At this point, backend must be stopped, otherwise\n     * it might keep writing to memory. */\n    assert(!n->vhost_started);\n\n    return 0;\n}\n\nstatic bool primary_unplug_pending(void *opaque)\n{\n    DeviceState *dev = opaque;\n    DeviceState *primary;\n    VirtIODevice *vdev = VIRTIO_DEVICE(dev);\n    VirtIONet *n = VIRTIO_NET(vdev);\n\n    if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_STANDBY)) {\n        return false;\n    }\n    primary = failover_find_primary_device(n);\n    return primary ? primary->pending_deleted_event : false;\n}\n\nstatic bool dev_unplug_pending(void *opaque)\n{\n    DeviceState *dev = opaque;\n    VirtioDeviceClass *vdc = VIRTIO_DEVICE_GET_CLASS(dev);\n\n    return vdc->primary_unplug_pending(dev);\n}\n\nstatic const VMStateDescription vmstate_virtio_net = {\n    .name = \"virtio-net\",\n    .minimum_version_id = VIRTIO_NET_VM_VERSION,\n    .version_id = VIRTIO_NET_VM_VERSION,\n    .fields = (VMStateField[]) {\n        VMSTATE_VIRTIO_DEVICE,\n        VMSTATE_END_OF_LIST()\n    },\n    .pre_save = virtio_net_pre_save,\n    .dev_unplug_pending = dev_unplug_pending,\n};\n\nstatic Property virtio_net_properties[] = {\n    DEFINE_PROP_BIT64(\"csum\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CSUM, true),\n    DEFINE_PROP_BIT64(\"guest_csum\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_CSUM, true),\n    DEFINE_PROP_BIT64(\"gso\", VirtIONet, host_features, VIRTIO_NET_F_GSO, true),\n    DEFINE_PROP_BIT64(\"guest_tso4\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_TSO4, true),\n    DEFINE_PROP_BIT64(\"guest_tso6\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_TSO6, true),\n    DEFINE_PROP_BIT64(\"guest_ecn\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_ECN, true),\n    DEFINE_PROP_BIT64(\"guest_ufo\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_UFO, true),\n    DEFINE_PROP_BIT64(\"guest_announce\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_ANNOUNCE, true),\n    DEFINE_PROP_BIT64(\"host_tso4\", VirtIONet, host_features,\n                    VIRTIO_NET_F_HOST_TSO4, true),\n    DEFINE_PROP_BIT64(\"host_tso6\", VirtIONet, host_features,\n                    VIRTIO_NET_F_HOST_TSO6, true),\n    DEFINE_PROP_BIT64(\"host_ecn\", VirtIONet, host_features,\n                    VIRTIO_NET_F_HOST_ECN, true),\n    DEFINE_PROP_BIT64(\"host_ufo\", VirtIONet, host_features,\n                    VIRTIO_NET_F_HOST_UFO, true),\n    DEFINE_PROP_BIT64(\"mrg_rxbuf\", VirtIONet, host_features,\n                    VIRTIO_NET_F_MRG_RXBUF, true),\n    DEFINE_PROP_BIT64(\"status\", VirtIONet, host_features,\n                    VIRTIO_NET_F_STATUS, true),\n    DEFINE_PROP_BIT64(\"ctrl_vq\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_VQ, true),\n    DEFINE_PROP_BIT64(\"ctrl_rx\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_RX, true),\n    DEFINE_PROP_BIT64(\"ctrl_vlan\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_VLAN, true),\n    DEFINE_PROP_BIT64(\"ctrl_rx_extra\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_RX_EXTRA, true),\n    DEFINE_PROP_BIT64(\"ctrl_mac_addr\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_MAC_ADDR, true),\n    DEFINE_PROP_BIT64(\"ctrl_guest_offloads\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_GUEST_OFFLOADS, true),\n    DEFINE_PROP_BIT64(\"mq\", VirtIONet, host_features, VIRTIO_NET_F_MQ, false),\n    DEFINE_PROP_BIT64(\"rss\", VirtIONet, host_features,\n                    VIRTIO_NET_F_RSS, false),\n    DEFINE_PROP_BIT64(\"hash\", VirtIONet, host_features,\n                    VIRTIO_NET_F_HASH_REPORT, false),\n    DEFINE_PROP_BIT64(\"guest_rsc_ext\", VirtIONet, host_features,\n                    VIRTIO_NET_F_RSC_EXT, false),\n    DEFINE_PROP_UINT32(\"rsc_interval\", VirtIONet, rsc_timeout,\n                       VIRTIO_NET_RSC_DEFAULT_INTERVAL),\n    DEFINE_NIC_PROPERTIES(VirtIONet, nic_conf),\n    DEFINE_PROP_UINT32(\"x-txtimer\", VirtIONet, net_conf.txtimer,\n                       TX_TIMER_INTERVAL),\n    DEFINE_PROP_INT32(\"x-txburst\", VirtIONet, net_conf.txburst, TX_BURST),\n    DEFINE_PROP_STRING(\"tx\", VirtIONet, net_conf.tx),\n    DEFINE_PROP_UINT16(\"rx_queue_size\", VirtIONet, net_conf.rx_queue_size,\n                       VIRTIO_NET_RX_QUEUE_DEFAULT_SIZE),\n    DEFINE_PROP_UINT16(\"tx_queue_size\", VirtIONet, net_conf.tx_queue_size,\n                       VIRTIO_NET_TX_QUEUE_DEFAULT_SIZE),\n    DEFINE_PROP_UINT16(\"host_mtu\", VirtIONet, net_conf.mtu, 0),\n    DEFINE_PROP_BOOL(\"x-mtu-bypass-backend\", VirtIONet, mtu_bypass_backend,\n                     true),\n    DEFINE_PROP_INT32(\"speed\", VirtIONet, net_conf.speed, SPEED_UNKNOWN),\n    DEFINE_PROP_STRING(\"duplex\", VirtIONet, net_conf.duplex_str),\n    DEFINE_PROP_BOOL(\"failover\", VirtIONet, failover, false),\n    DEFINE_PROP_END_OF_LIST(),\n};\n\nstatic void virtio_net_class_init(ObjectClass *klass, void *data)\n{\n    DeviceClass *dc = DEVICE_CLASS(klass);\n    VirtioDeviceClass *vdc = VIRTIO_DEVICE_CLASS(klass);\n\n    device_class_set_props(dc, virtio_net_properties);\n    dc->vmsd = &vmstate_virtio_net;\n    set_bit(DEVICE_CATEGORY_NETWORK, dc->categories);\n    vdc->realize = virtio_net_device_realize;\n    vdc->unrealize = virtio_net_device_unrealize;\n    vdc->get_config = virtio_net_get_config;\n    vdc->set_config = virtio_net_set_config;\n    vdc->get_features = virtio_net_get_features;\n    vdc->set_features = virtio_net_set_features;\n    vdc->bad_features = virtio_net_bad_features;\n    vdc->reset = virtio_net_reset;\n    vdc->set_status = virtio_net_set_status;\n    vdc->guest_notifier_mask = virtio_net_guest_notifier_mask;\n    vdc->guest_notifier_pending = virtio_net_guest_notifier_pending;\n    vdc->legacy_features |= (0x1 << VIRTIO_NET_F_GSO);\n    vdc->post_load = virtio_net_post_load_virtio;\n    vdc->vmsd = &vmstate_virtio_net_device;\n    vdc->primary_unplug_pending = primary_unplug_pending;\n}\n\nstatic const TypeInfo virtio_net_info = {\n    .name = TYPE_VIRTIO_NET,\n    .parent = TYPE_VIRTIO_DEVICE,\n    .instance_size = sizeof(VirtIONet),\n    .instance_init = virtio_net_instance_init,\n    .class_init = virtio_net_class_init,\n};\n\nstatic void virtio_register_types(void)\n{\n    type_register_static(&virtio_net_info);\n}\n\ntype_init(virtio_register_types)\n"], "fixing_code": ["/*\n * Virtio Network Device\n *\n * Copyright IBM, Corp. 2007\n *\n * Authors:\n *  Anthony Liguori   <aliguori@us.ibm.com>\n *\n * This work is licensed under the terms of the GNU GPL, version 2.  See\n * the COPYING file in the top-level directory.\n *\n */\n\n#include \"qemu/osdep.h\"\n#include \"qemu/atomic.h\"\n#include \"qemu/iov.h\"\n#include \"qemu/main-loop.h\"\n#include \"qemu/module.h\"\n#include \"hw/virtio/virtio.h\"\n#include \"net/net.h\"\n#include \"net/checksum.h\"\n#include \"net/tap.h\"\n#include \"qemu/error-report.h\"\n#include \"qemu/timer.h\"\n#include \"qemu/option.h\"\n#include \"qemu/option_int.h\"\n#include \"qemu/config-file.h\"\n#include \"qapi/qmp/qdict.h\"\n#include \"hw/virtio/virtio-net.h\"\n#include \"net/vhost_net.h\"\n#include \"net/announce.h\"\n#include \"hw/virtio/virtio-bus.h\"\n#include \"qapi/error.h\"\n#include \"qapi/qapi-events-net.h\"\n#include \"hw/qdev-properties.h\"\n#include \"qapi/qapi-types-migration.h\"\n#include \"qapi/qapi-events-migration.h\"\n#include \"hw/virtio/virtio-access.h\"\n#include \"migration/misc.h\"\n#include \"standard-headers/linux/ethtool.h\"\n#include \"sysemu/sysemu.h\"\n#include \"trace.h\"\n#include \"monitor/qdev.h\"\n#include \"hw/pci/pci.h\"\n#include \"net_rx_pkt.h\"\n#include \"hw/virtio/vhost.h\"\n\n#define VIRTIO_NET_VM_VERSION    11\n\n#define MAC_TABLE_ENTRIES    64\n#define MAX_VLAN    (1 << 12)   /* Per 802.1Q definition */\n\n/* previously fixed value */\n#define VIRTIO_NET_RX_QUEUE_DEFAULT_SIZE 256\n#define VIRTIO_NET_TX_QUEUE_DEFAULT_SIZE 256\n\n/* for now, only allow larger queues; with virtio-1, guest can downsize */\n#define VIRTIO_NET_RX_QUEUE_MIN_SIZE VIRTIO_NET_RX_QUEUE_DEFAULT_SIZE\n#define VIRTIO_NET_TX_QUEUE_MIN_SIZE VIRTIO_NET_TX_QUEUE_DEFAULT_SIZE\n\n#define VIRTIO_NET_IP4_ADDR_SIZE   8        /* ipv4 saddr + daddr */\n\n#define VIRTIO_NET_TCP_FLAG         0x3F\n#define VIRTIO_NET_TCP_HDR_LENGTH   0xF000\n\n/* IPv4 max payload, 16 bits in the header */\n#define VIRTIO_NET_MAX_IP4_PAYLOAD (65535 - sizeof(struct ip_header))\n#define VIRTIO_NET_MAX_TCP_PAYLOAD 65535\n\n/* header length value in ip header without option */\n#define VIRTIO_NET_IP4_HEADER_LENGTH 5\n\n#define VIRTIO_NET_IP6_ADDR_SIZE   32      /* ipv6 saddr + daddr */\n#define VIRTIO_NET_MAX_IP6_PAYLOAD VIRTIO_NET_MAX_TCP_PAYLOAD\n\n/* Purge coalesced packets timer interval, This value affects the performance\n   a lot, and should be tuned carefully, '300000'(300us) is the recommended\n   value to pass the WHQL test, '50000' can gain 2x netperf throughput with\n   tso/gso/gro 'off'. */\n#define VIRTIO_NET_RSC_DEFAULT_INTERVAL 300000\n\n#define VIRTIO_NET_RSS_SUPPORTED_HASHES (VIRTIO_NET_RSS_HASH_TYPE_IPv4 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_TCPv4 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_UDPv4 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_IPv6 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_TCPv6 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_UDPv6 | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_IP_EX | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_TCP_EX | \\\n                                         VIRTIO_NET_RSS_HASH_TYPE_UDP_EX)\n\nstatic const VirtIOFeature feature_sizes[] = {\n    {.flags = 1ULL << VIRTIO_NET_F_MAC,\n     .end = endof(struct virtio_net_config, mac)},\n    {.flags = 1ULL << VIRTIO_NET_F_STATUS,\n     .end = endof(struct virtio_net_config, status)},\n    {.flags = 1ULL << VIRTIO_NET_F_MQ,\n     .end = endof(struct virtio_net_config, max_virtqueue_pairs)},\n    {.flags = 1ULL << VIRTIO_NET_F_MTU,\n     .end = endof(struct virtio_net_config, mtu)},\n    {.flags = 1ULL << VIRTIO_NET_F_SPEED_DUPLEX,\n     .end = endof(struct virtio_net_config, duplex)},\n    {.flags = (1ULL << VIRTIO_NET_F_RSS) | (1ULL << VIRTIO_NET_F_HASH_REPORT),\n     .end = endof(struct virtio_net_config, supported_hash_types)},\n    {}\n};\n\nstatic VirtIONetQueue *virtio_net_get_subqueue(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n\n    return &n->vqs[nc->queue_index];\n}\n\nstatic int vq2q(int queue_index)\n{\n    return queue_index / 2;\n}\n\n/* TODO\n * - we could suppress RX interrupt if we were so inclined.\n */\n\nstatic void virtio_net_get_config(VirtIODevice *vdev, uint8_t *config)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    struct virtio_net_config netcfg;\n    NetClientState *nc = qemu_get_queue(n->nic);\n    static const MACAddr zero = { .a = { 0, 0, 0, 0, 0, 0 } };\n\n    int ret = 0;\n    memset(&netcfg, 0 , sizeof(struct virtio_net_config));\n    virtio_stw_p(vdev, &netcfg.status, n->status);\n    virtio_stw_p(vdev, &netcfg.max_virtqueue_pairs, n->max_queues);\n    virtio_stw_p(vdev, &netcfg.mtu, n->net_conf.mtu);\n    memcpy(netcfg.mac, n->mac, ETH_ALEN);\n    virtio_stl_p(vdev, &netcfg.speed, n->net_conf.speed);\n    netcfg.duplex = n->net_conf.duplex;\n    netcfg.rss_max_key_size = VIRTIO_NET_RSS_MAX_KEY_SIZE;\n    virtio_stw_p(vdev, &netcfg.rss_max_indirection_table_length,\n                 virtio_host_has_feature(vdev, VIRTIO_NET_F_RSS) ?\n                 VIRTIO_NET_RSS_MAX_TABLE_LEN : 1);\n    virtio_stl_p(vdev, &netcfg.supported_hash_types,\n                 VIRTIO_NET_RSS_SUPPORTED_HASHES);\n    memcpy(config, &netcfg, n->config_size);\n\n    /*\n     * Is this VDPA? No peer means not VDPA: there's no way to\n     * disconnect/reconnect a VDPA peer.\n     */\n    if (nc->peer && nc->peer->info->type == NET_CLIENT_DRIVER_VHOST_VDPA) {\n        ret = vhost_net_get_config(get_vhost_net(nc->peer), (uint8_t *)&netcfg,\n                                   n->config_size);\n        if (ret != -1) {\n            /*\n             * Some NIC/kernel combinations present 0 as the mac address.  As\n             * that is not a legal address, try to proceed with the\n             * address from the QEMU command line in the hope that the\n             * address has been configured correctly elsewhere - just not\n             * reported by the device.\n             */\n            if (memcmp(&netcfg.mac, &zero, sizeof(zero)) == 0) {\n                info_report(\"Zero hardware mac address detected. Ignoring.\");\n                memcpy(netcfg.mac, n->mac, ETH_ALEN);\n            }\n            memcpy(config, &netcfg, n->config_size);\n        }\n    }\n}\n\nstatic void virtio_net_set_config(VirtIODevice *vdev, const uint8_t *config)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    struct virtio_net_config netcfg = {};\n    NetClientState *nc = qemu_get_queue(n->nic);\n\n    memcpy(&netcfg, config, n->config_size);\n\n    if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_MAC_ADDR) &&\n        !virtio_vdev_has_feature(vdev, VIRTIO_F_VERSION_1) &&\n        memcmp(netcfg.mac, n->mac, ETH_ALEN)) {\n        memcpy(n->mac, netcfg.mac, ETH_ALEN);\n        qemu_format_nic_info_str(qemu_get_queue(n->nic), n->mac);\n    }\n\n    /*\n     * Is this VDPA? No peer means not VDPA: there's no way to\n     * disconnect/reconnect a VDPA peer.\n     */\n    if (nc->peer && nc->peer->info->type == NET_CLIENT_DRIVER_VHOST_VDPA) {\n        vhost_net_set_config(get_vhost_net(nc->peer),\n                             (uint8_t *)&netcfg, 0, n->config_size,\n                             VHOST_SET_CONFIG_TYPE_MASTER);\n      }\n}\n\nstatic bool virtio_net_started(VirtIONet *n, uint8_t status)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    return (status & VIRTIO_CONFIG_S_DRIVER_OK) &&\n        (n->status & VIRTIO_NET_S_LINK_UP) && vdev->vm_running;\n}\n\nstatic void virtio_net_announce_notify(VirtIONet *net)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(net);\n    trace_virtio_net_announce_notify();\n\n    net->status |= VIRTIO_NET_S_ANNOUNCE;\n    virtio_notify_config(vdev);\n}\n\nstatic void virtio_net_announce_timer(void *opaque)\n{\n    VirtIONet *n = opaque;\n    trace_virtio_net_announce_timer(n->announce_timer.round);\n\n    n->announce_timer.round--;\n    virtio_net_announce_notify(n);\n}\n\nstatic void virtio_net_announce(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n\n    /*\n     * Make sure the virtio migration announcement timer isn't running\n     * If it is, let it trigger announcement so that we do not cause\n     * confusion.\n     */\n    if (n->announce_timer.round) {\n        return;\n    }\n\n    if (virtio_vdev_has_feature(vdev, VIRTIO_NET_F_GUEST_ANNOUNCE) &&\n        virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_VQ)) {\n            virtio_net_announce_notify(n);\n    }\n}\n\nstatic void virtio_net_vhost_status(VirtIONet *n, uint8_t status)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    NetClientState *nc = qemu_get_queue(n->nic);\n    int queues = n->multiqueue ? n->max_queues : 1;\n\n    if (!get_vhost_net(nc->peer)) {\n        return;\n    }\n\n    if ((virtio_net_started(n, status) && !nc->peer->link_down) ==\n        !!n->vhost_started) {\n        return;\n    }\n    if (!n->vhost_started) {\n        int r, i;\n\n        if (n->needs_vnet_hdr_swap) {\n            error_report(\"backend does not support %s vnet headers; \"\n                         \"falling back on userspace virtio\",\n                         virtio_is_big_endian(vdev) ? \"BE\" : \"LE\");\n            return;\n        }\n\n        /* Any packets outstanding? Purge them to avoid touching rings\n         * when vhost is running.\n         */\n        for (i = 0;  i < queues; i++) {\n            NetClientState *qnc = qemu_get_subqueue(n->nic, i);\n\n            /* Purge both directions: TX and RX. */\n            qemu_net_queue_purge(qnc->peer->incoming_queue, qnc);\n            qemu_net_queue_purge(qnc->incoming_queue, qnc->peer);\n        }\n\n        if (virtio_has_feature(vdev->guest_features, VIRTIO_NET_F_MTU)) {\n            r = vhost_net_set_mtu(get_vhost_net(nc->peer), n->net_conf.mtu);\n            if (r < 0) {\n                error_report(\"%uBytes MTU not supported by the backend\",\n                             n->net_conf.mtu);\n\n                return;\n            }\n        }\n\n        n->vhost_started = 1;\n        r = vhost_net_start(vdev, n->nic->ncs, queues);\n        if (r < 0) {\n            error_report(\"unable to start vhost net: %d: \"\n                         \"falling back on userspace virtio\", -r);\n            n->vhost_started = 0;\n        }\n    } else {\n        vhost_net_stop(vdev, n->nic->ncs, queues);\n        n->vhost_started = 0;\n    }\n}\n\nstatic int virtio_net_set_vnet_endian_one(VirtIODevice *vdev,\n                                          NetClientState *peer,\n                                          bool enable)\n{\n    if (virtio_is_big_endian(vdev)) {\n        return qemu_set_vnet_be(peer, enable);\n    } else {\n        return qemu_set_vnet_le(peer, enable);\n    }\n}\n\nstatic bool virtio_net_set_vnet_endian(VirtIODevice *vdev, NetClientState *ncs,\n                                       int queues, bool enable)\n{\n    int i;\n\n    for (i = 0; i < queues; i++) {\n        if (virtio_net_set_vnet_endian_one(vdev, ncs[i].peer, enable) < 0 &&\n            enable) {\n            while (--i >= 0) {\n                virtio_net_set_vnet_endian_one(vdev, ncs[i].peer, false);\n            }\n\n            return true;\n        }\n    }\n\n    return false;\n}\n\nstatic void virtio_net_vnet_endian_status(VirtIONet *n, uint8_t status)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    int queues = n->multiqueue ? n->max_queues : 1;\n\n    if (virtio_net_started(n, status)) {\n        /* Before using the device, we tell the network backend about the\n         * endianness to use when parsing vnet headers. If the backend\n         * can't do it, we fallback onto fixing the headers in the core\n         * virtio-net code.\n         */\n        n->needs_vnet_hdr_swap = virtio_net_set_vnet_endian(vdev, n->nic->ncs,\n                                                            queues, true);\n    } else if (virtio_net_started(n, vdev->status)) {\n        /* After using the device, we need to reset the network backend to\n         * the default (guest native endianness), otherwise the guest may\n         * lose network connectivity if it is rebooted into a different\n         * endianness.\n         */\n        virtio_net_set_vnet_endian(vdev, n->nic->ncs, queues, false);\n    }\n}\n\nstatic void virtio_net_drop_tx_queue_data(VirtIODevice *vdev, VirtQueue *vq)\n{\n    unsigned int dropped = virtqueue_drop_all(vq);\n    if (dropped) {\n        virtio_notify(vdev, vq);\n    }\n}\n\nstatic void virtio_net_set_status(struct VirtIODevice *vdev, uint8_t status)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    VirtIONetQueue *q;\n    int i;\n    uint8_t queue_status;\n\n    virtio_net_vnet_endian_status(n, status);\n    virtio_net_vhost_status(n, status);\n\n    for (i = 0; i < n->max_queues; i++) {\n        NetClientState *ncs = qemu_get_subqueue(n->nic, i);\n        bool queue_started;\n        q = &n->vqs[i];\n\n        if ((!n->multiqueue && i != 0) || i >= n->curr_queues) {\n            queue_status = 0;\n        } else {\n            queue_status = status;\n        }\n        queue_started =\n            virtio_net_started(n, queue_status) && !n->vhost_started;\n\n        if (queue_started) {\n            qemu_flush_queued_packets(ncs);\n        }\n\n        if (!q->tx_waiting) {\n            continue;\n        }\n\n        if (queue_started) {\n            if (q->tx_timer) {\n                timer_mod(q->tx_timer,\n                               qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL) + n->tx_timeout);\n            } else {\n                qemu_bh_schedule(q->tx_bh);\n            }\n        } else {\n            if (q->tx_timer) {\n                timer_del(q->tx_timer);\n            } else {\n                qemu_bh_cancel(q->tx_bh);\n            }\n            if ((n->status & VIRTIO_NET_S_LINK_UP) == 0 &&\n                (queue_status & VIRTIO_CONFIG_S_DRIVER_OK) &&\n                vdev->vm_running) {\n                /* if tx is waiting we are likely have some packets in tx queue\n                 * and disabled notification */\n                q->tx_waiting = 0;\n                virtio_queue_set_notification(q->tx_vq, 1);\n                virtio_net_drop_tx_queue_data(vdev, q->tx_vq);\n            }\n        }\n    }\n}\n\nstatic void virtio_net_set_link_status(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    uint16_t old_status = n->status;\n\n    if (nc->link_down)\n        n->status &= ~VIRTIO_NET_S_LINK_UP;\n    else\n        n->status |= VIRTIO_NET_S_LINK_UP;\n\n    if (n->status != old_status)\n        virtio_notify_config(vdev);\n\n    virtio_net_set_status(vdev, vdev->status);\n}\n\nstatic void rxfilter_notify(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n\n    if (nc->rxfilter_notify_enabled) {\n        char *path = object_get_canonical_path(OBJECT(n->qdev));\n        qapi_event_send_nic_rx_filter_changed(!!n->netclient_name,\n                                              n->netclient_name, path);\n        g_free(path);\n\n        /* disable event notification to avoid events flooding */\n        nc->rxfilter_notify_enabled = 0;\n    }\n}\n\nstatic intList *get_vlan_table(VirtIONet *n)\n{\n    intList *list;\n    int i, j;\n\n    list = NULL;\n    for (i = 0; i < MAX_VLAN >> 5; i++) {\n        for (j = 0; n->vlans[i] && j <= 0x1f; j++) {\n            if (n->vlans[i] & (1U << j)) {\n                QAPI_LIST_PREPEND(list, (i << 5) + j);\n            }\n        }\n    }\n\n    return list;\n}\n\nstatic RxFilterInfo *virtio_net_query_rxfilter(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    RxFilterInfo *info;\n    strList *str_list;\n    int i;\n\n    info = g_malloc0(sizeof(*info));\n    info->name = g_strdup(nc->name);\n    info->promiscuous = n->promisc;\n\n    if (n->nouni) {\n        info->unicast = RX_STATE_NONE;\n    } else if (n->alluni) {\n        info->unicast = RX_STATE_ALL;\n    } else {\n        info->unicast = RX_STATE_NORMAL;\n    }\n\n    if (n->nomulti) {\n        info->multicast = RX_STATE_NONE;\n    } else if (n->allmulti) {\n        info->multicast = RX_STATE_ALL;\n    } else {\n        info->multicast = RX_STATE_NORMAL;\n    }\n\n    info->broadcast_allowed = n->nobcast;\n    info->multicast_overflow = n->mac_table.multi_overflow;\n    info->unicast_overflow = n->mac_table.uni_overflow;\n\n    info->main_mac = qemu_mac_strdup_printf(n->mac);\n\n    str_list = NULL;\n    for (i = 0; i < n->mac_table.first_multi; i++) {\n        QAPI_LIST_PREPEND(str_list,\n                      qemu_mac_strdup_printf(n->mac_table.macs + i * ETH_ALEN));\n    }\n    info->unicast_table = str_list;\n\n    str_list = NULL;\n    for (i = n->mac_table.first_multi; i < n->mac_table.in_use; i++) {\n        QAPI_LIST_PREPEND(str_list,\n                      qemu_mac_strdup_printf(n->mac_table.macs + i * ETH_ALEN));\n    }\n    info->multicast_table = str_list;\n    info->vlan_table = get_vlan_table(n);\n\n    if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_VLAN)) {\n        info->vlan = RX_STATE_ALL;\n    } else if (!info->vlan_table) {\n        info->vlan = RX_STATE_NONE;\n    } else {\n        info->vlan = RX_STATE_NORMAL;\n    }\n\n    /* enable event notification after query */\n    nc->rxfilter_notify_enabled = 1;\n\n    return info;\n}\n\nstatic void virtio_net_reset(VirtIODevice *vdev)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    int i;\n\n    /* Reset back to compatibility mode */\n    n->promisc = 1;\n    n->allmulti = 0;\n    n->alluni = 0;\n    n->nomulti = 0;\n    n->nouni = 0;\n    n->nobcast = 0;\n    /* multiqueue is disabled by default */\n    n->curr_queues = 1;\n    timer_del(n->announce_timer.tm);\n    n->announce_timer.round = 0;\n    n->status &= ~VIRTIO_NET_S_ANNOUNCE;\n\n    /* Flush any MAC and VLAN filter table state */\n    n->mac_table.in_use = 0;\n    n->mac_table.first_multi = 0;\n    n->mac_table.multi_overflow = 0;\n    n->mac_table.uni_overflow = 0;\n    memset(n->mac_table.macs, 0, MAC_TABLE_ENTRIES * ETH_ALEN);\n    memcpy(&n->mac[0], &n->nic->conf->macaddr, sizeof(n->mac));\n    qemu_format_nic_info_str(qemu_get_queue(n->nic), n->mac);\n    memset(n->vlans, 0, MAX_VLAN >> 3);\n\n    /* Flush any async TX */\n    for (i = 0;  i < n->max_queues; i++) {\n        NetClientState *nc = qemu_get_subqueue(n->nic, i);\n\n        if (nc->peer) {\n            qemu_flush_or_purge_queued_packets(nc->peer, true);\n            assert(!virtio_net_get_subqueue(nc)->async_tx.elem);\n        }\n    }\n}\n\nstatic void peer_test_vnet_hdr(VirtIONet *n)\n{\n    NetClientState *nc = qemu_get_queue(n->nic);\n    if (!nc->peer) {\n        return;\n    }\n\n    n->has_vnet_hdr = qemu_has_vnet_hdr(nc->peer);\n}\n\nstatic int peer_has_vnet_hdr(VirtIONet *n)\n{\n    return n->has_vnet_hdr;\n}\n\nstatic int peer_has_ufo(VirtIONet *n)\n{\n    if (!peer_has_vnet_hdr(n))\n        return 0;\n\n    n->has_ufo = qemu_has_ufo(qemu_get_queue(n->nic)->peer);\n\n    return n->has_ufo;\n}\n\nstatic void virtio_net_set_mrg_rx_bufs(VirtIONet *n, int mergeable_rx_bufs,\n                                       int version_1, int hash_report)\n{\n    int i;\n    NetClientState *nc;\n\n    n->mergeable_rx_bufs = mergeable_rx_bufs;\n\n    if (version_1) {\n        n->guest_hdr_len = hash_report ?\n            sizeof(struct virtio_net_hdr_v1_hash) :\n            sizeof(struct virtio_net_hdr_mrg_rxbuf);\n        n->rss_data.populate_hash = !!hash_report;\n    } else {\n        n->guest_hdr_len = n->mergeable_rx_bufs ?\n            sizeof(struct virtio_net_hdr_mrg_rxbuf) :\n            sizeof(struct virtio_net_hdr);\n    }\n\n    for (i = 0; i < n->max_queues; i++) {\n        nc = qemu_get_subqueue(n->nic, i);\n\n        if (peer_has_vnet_hdr(n) &&\n            qemu_has_vnet_hdr_len(nc->peer, n->guest_hdr_len)) {\n            qemu_set_vnet_hdr_len(nc->peer, n->guest_hdr_len);\n            n->host_hdr_len = n->guest_hdr_len;\n        }\n    }\n}\n\nstatic int virtio_net_max_tx_queue_size(VirtIONet *n)\n{\n    NetClientState *peer = n->nic_conf.peers.ncs[0];\n\n    /*\n     * Backends other than vhost-user don't support max queue size.\n     */\n    if (!peer) {\n        return VIRTIO_NET_TX_QUEUE_DEFAULT_SIZE;\n    }\n\n    if (peer->info->type != NET_CLIENT_DRIVER_VHOST_USER) {\n        return VIRTIO_NET_TX_QUEUE_DEFAULT_SIZE;\n    }\n\n    return VIRTQUEUE_MAX_SIZE;\n}\n\nstatic int peer_attach(VirtIONet *n, int index)\n{\n    NetClientState *nc = qemu_get_subqueue(n->nic, index);\n\n    if (!nc->peer) {\n        return 0;\n    }\n\n    if (nc->peer->info->type == NET_CLIENT_DRIVER_VHOST_USER) {\n        vhost_set_vring_enable(nc->peer, 1);\n    }\n\n    if (nc->peer->info->type != NET_CLIENT_DRIVER_TAP) {\n        return 0;\n    }\n\n    if (n->max_queues == 1) {\n        return 0;\n    }\n\n    return tap_enable(nc->peer);\n}\n\nstatic int peer_detach(VirtIONet *n, int index)\n{\n    NetClientState *nc = qemu_get_subqueue(n->nic, index);\n\n    if (!nc->peer) {\n        return 0;\n    }\n\n    if (nc->peer->info->type == NET_CLIENT_DRIVER_VHOST_USER) {\n        vhost_set_vring_enable(nc->peer, 0);\n    }\n\n    if (nc->peer->info->type !=  NET_CLIENT_DRIVER_TAP) {\n        return 0;\n    }\n\n    return tap_disable(nc->peer);\n}\n\nstatic void virtio_net_set_queues(VirtIONet *n)\n{\n    int i;\n    int r;\n\n    if (n->nic->peer_deleted) {\n        return;\n    }\n\n    for (i = 0; i < n->max_queues; i++) {\n        if (i < n->curr_queues) {\n            r = peer_attach(n, i);\n            assert(!r);\n        } else {\n            r = peer_detach(n, i);\n            assert(!r);\n        }\n    }\n}\n\nstatic void virtio_net_set_multiqueue(VirtIONet *n, int multiqueue);\n\nstatic uint64_t virtio_net_get_features(VirtIODevice *vdev, uint64_t features,\n                                        Error **errp)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    NetClientState *nc = qemu_get_queue(n->nic);\n\n    /* Firstly sync all virtio-net possible supported features */\n    features |= n->host_features;\n\n    virtio_add_feature(&features, VIRTIO_NET_F_MAC);\n\n    if (!peer_has_vnet_hdr(n)) {\n        virtio_clear_feature(&features, VIRTIO_NET_F_CSUM);\n        virtio_clear_feature(&features, VIRTIO_NET_F_HOST_TSO4);\n        virtio_clear_feature(&features, VIRTIO_NET_F_HOST_TSO6);\n        virtio_clear_feature(&features, VIRTIO_NET_F_HOST_ECN);\n\n        virtio_clear_feature(&features, VIRTIO_NET_F_GUEST_CSUM);\n        virtio_clear_feature(&features, VIRTIO_NET_F_GUEST_TSO4);\n        virtio_clear_feature(&features, VIRTIO_NET_F_GUEST_TSO6);\n        virtio_clear_feature(&features, VIRTIO_NET_F_GUEST_ECN);\n\n        virtio_clear_feature(&features, VIRTIO_NET_F_HASH_REPORT);\n    }\n\n    if (!peer_has_vnet_hdr(n) || !peer_has_ufo(n)) {\n        virtio_clear_feature(&features, VIRTIO_NET_F_GUEST_UFO);\n        virtio_clear_feature(&features, VIRTIO_NET_F_HOST_UFO);\n    }\n\n    if (!get_vhost_net(nc->peer)) {\n        return features;\n    }\n\n    if (!ebpf_rss_is_loaded(&n->ebpf_rss)) {\n        virtio_clear_feature(&features, VIRTIO_NET_F_RSS);\n    }\n    features = vhost_net_get_features(get_vhost_net(nc->peer), features);\n    vdev->backend_features = features;\n\n    if (n->mtu_bypass_backend &&\n            (n->host_features & 1ULL << VIRTIO_NET_F_MTU)) {\n        features |= (1ULL << VIRTIO_NET_F_MTU);\n    }\n\n    return features;\n}\n\nstatic uint64_t virtio_net_bad_features(VirtIODevice *vdev)\n{\n    uint64_t features = 0;\n\n    /* Linux kernel 2.6.25.  It understood MAC (as everyone must),\n     * but also these: */\n    virtio_add_feature(&features, VIRTIO_NET_F_MAC);\n    virtio_add_feature(&features, VIRTIO_NET_F_CSUM);\n    virtio_add_feature(&features, VIRTIO_NET_F_HOST_TSO4);\n    virtio_add_feature(&features, VIRTIO_NET_F_HOST_TSO6);\n    virtio_add_feature(&features, VIRTIO_NET_F_HOST_ECN);\n\n    return features;\n}\n\nstatic void virtio_net_apply_guest_offloads(VirtIONet *n)\n{\n    qemu_set_offload(qemu_get_queue(n->nic)->peer,\n            !!(n->curr_guest_offloads & (1ULL << VIRTIO_NET_F_GUEST_CSUM)),\n            !!(n->curr_guest_offloads & (1ULL << VIRTIO_NET_F_GUEST_TSO4)),\n            !!(n->curr_guest_offloads & (1ULL << VIRTIO_NET_F_GUEST_TSO6)),\n            !!(n->curr_guest_offloads & (1ULL << VIRTIO_NET_F_GUEST_ECN)),\n            !!(n->curr_guest_offloads & (1ULL << VIRTIO_NET_F_GUEST_UFO)));\n}\n\nstatic uint64_t virtio_net_guest_offloads_by_features(uint32_t features)\n{\n    static const uint64_t guest_offloads_mask =\n        (1ULL << VIRTIO_NET_F_GUEST_CSUM) |\n        (1ULL << VIRTIO_NET_F_GUEST_TSO4) |\n        (1ULL << VIRTIO_NET_F_GUEST_TSO6) |\n        (1ULL << VIRTIO_NET_F_GUEST_ECN)  |\n        (1ULL << VIRTIO_NET_F_GUEST_UFO);\n\n    return guest_offloads_mask & features;\n}\n\nstatic inline uint64_t virtio_net_supported_guest_offloads(VirtIONet *n)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    return virtio_net_guest_offloads_by_features(vdev->guest_features);\n}\n\ntypedef struct {\n    VirtIONet *n;\n    char *id;\n} FailoverId;\n\n/**\n * Set the id of the failover primary device\n *\n * @opaque: FailoverId to setup\n * @opts: opts for device we are handling\n * @errp: returns an error if this function fails\n */\nstatic int failover_set_primary(void *opaque, QemuOpts *opts, Error **errp)\n{\n    FailoverId *fid = opaque;\n    const char *standby_id = qemu_opt_get(opts, \"failover_pair_id\");\n\n    if (g_strcmp0(standby_id, fid->n->netclient_name) == 0) {\n        fid->id = g_strdup(opts->id);\n        return 1;\n    }\n\n    return 0;\n}\n\n/**\n * Find the primary device id for this failover virtio-net\n *\n * @n: VirtIONet device\n * @errp: returns an error if this function fails\n */\nstatic char *failover_find_primary_device_id(VirtIONet *n)\n{\n    Error *err = NULL;\n    FailoverId fid;\n\n    fid.n = n;\n    if (!qemu_opts_foreach(qemu_find_opts(\"device\"),\n                           failover_set_primary, &fid, &err)) {\n        return NULL;\n    }\n    return fid.id;\n}\n\n/**\n * Find the primary device for this failover virtio-net\n *\n * @n: VirtIONet device\n * @errp: returns an error if this function fails\n */\nstatic DeviceState *failover_find_primary_device(VirtIONet *n)\n{\n    char *id = failover_find_primary_device_id(n);\n\n    if (!id) {\n        return NULL;\n    }\n\n    return qdev_find_recursive(sysbus_get_default(), id);\n}\n\nstatic void failover_add_primary(VirtIONet *n, Error **errp)\n{\n    Error *err = NULL;\n    QemuOpts *opts;\n    char *id;\n    DeviceState *dev = failover_find_primary_device(n);\n\n    if (dev) {\n        return;\n    }\n\n    id = failover_find_primary_device_id(n);\n    if (!id) {\n        error_setg(errp, \"Primary device not found\");\n        error_append_hint(errp, \"Virtio-net failover will not work. Make \"\n                          \"sure primary device has parameter\"\n                          \" failover_pair_id=%s\\n\", n->netclient_name);\n        return;\n    }\n    opts = qemu_opts_find(qemu_find_opts(\"device\"), id);\n    g_assert(opts); /* cannot be NULL because id was found using opts list */\n    dev = qdev_device_add(opts, &err);\n    if (err) {\n        qemu_opts_del(opts);\n    } else {\n        object_unref(OBJECT(dev));\n    }\n    error_propagate(errp, err);\n}\n\nstatic void virtio_net_set_features(VirtIODevice *vdev, uint64_t features)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    Error *err = NULL;\n    int i;\n\n    if (n->mtu_bypass_backend &&\n            !virtio_has_feature(vdev->backend_features, VIRTIO_NET_F_MTU)) {\n        features &= ~(1ULL << VIRTIO_NET_F_MTU);\n    }\n\n    virtio_net_set_multiqueue(n,\n                              virtio_has_feature(features, VIRTIO_NET_F_RSS) ||\n                              virtio_has_feature(features, VIRTIO_NET_F_MQ));\n\n    virtio_net_set_mrg_rx_bufs(n,\n                               virtio_has_feature(features,\n                                                  VIRTIO_NET_F_MRG_RXBUF),\n                               virtio_has_feature(features,\n                                                  VIRTIO_F_VERSION_1),\n                               virtio_has_feature(features,\n                                                  VIRTIO_NET_F_HASH_REPORT));\n\n    n->rsc4_enabled = virtio_has_feature(features, VIRTIO_NET_F_RSC_EXT) &&\n        virtio_has_feature(features, VIRTIO_NET_F_GUEST_TSO4);\n    n->rsc6_enabled = virtio_has_feature(features, VIRTIO_NET_F_RSC_EXT) &&\n        virtio_has_feature(features, VIRTIO_NET_F_GUEST_TSO6);\n    n->rss_data.redirect = virtio_has_feature(features, VIRTIO_NET_F_RSS);\n\n    if (n->has_vnet_hdr) {\n        n->curr_guest_offloads =\n            virtio_net_guest_offloads_by_features(features);\n        virtio_net_apply_guest_offloads(n);\n    }\n\n    for (i = 0;  i < n->max_queues; i++) {\n        NetClientState *nc = qemu_get_subqueue(n->nic, i);\n\n        if (!get_vhost_net(nc->peer)) {\n            continue;\n        }\n        vhost_net_ack_features(get_vhost_net(nc->peer), features);\n    }\n\n    if (virtio_has_feature(features, VIRTIO_NET_F_CTRL_VLAN)) {\n        memset(n->vlans, 0, MAX_VLAN >> 3);\n    } else {\n        memset(n->vlans, 0xff, MAX_VLAN >> 3);\n    }\n\n    if (virtio_has_feature(features, VIRTIO_NET_F_STANDBY)) {\n        qapi_event_send_failover_negotiated(n->netclient_name);\n        qatomic_set(&n->failover_primary_hidden, false);\n        failover_add_primary(n, &err);\n        if (err) {\n            warn_report_err(err);\n        }\n    }\n}\n\nstatic int virtio_net_handle_rx_mode(VirtIONet *n, uint8_t cmd,\n                                     struct iovec *iov, unsigned int iov_cnt)\n{\n    uint8_t on;\n    size_t s;\n    NetClientState *nc = qemu_get_queue(n->nic);\n\n    s = iov_to_buf(iov, iov_cnt, 0, &on, sizeof(on));\n    if (s != sizeof(on)) {\n        return VIRTIO_NET_ERR;\n    }\n\n    if (cmd == VIRTIO_NET_CTRL_RX_PROMISC) {\n        n->promisc = on;\n    } else if (cmd == VIRTIO_NET_CTRL_RX_ALLMULTI) {\n        n->allmulti = on;\n    } else if (cmd == VIRTIO_NET_CTRL_RX_ALLUNI) {\n        n->alluni = on;\n    } else if (cmd == VIRTIO_NET_CTRL_RX_NOMULTI) {\n        n->nomulti = on;\n    } else if (cmd == VIRTIO_NET_CTRL_RX_NOUNI) {\n        n->nouni = on;\n    } else if (cmd == VIRTIO_NET_CTRL_RX_NOBCAST) {\n        n->nobcast = on;\n    } else {\n        return VIRTIO_NET_ERR;\n    }\n\n    rxfilter_notify(nc);\n\n    return VIRTIO_NET_OK;\n}\n\nstatic int virtio_net_handle_offloads(VirtIONet *n, uint8_t cmd,\n                                     struct iovec *iov, unsigned int iov_cnt)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    uint64_t offloads;\n    size_t s;\n\n    if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_GUEST_OFFLOADS)) {\n        return VIRTIO_NET_ERR;\n    }\n\n    s = iov_to_buf(iov, iov_cnt, 0, &offloads, sizeof(offloads));\n    if (s != sizeof(offloads)) {\n        return VIRTIO_NET_ERR;\n    }\n\n    if (cmd == VIRTIO_NET_CTRL_GUEST_OFFLOADS_SET) {\n        uint64_t supported_offloads;\n\n        offloads = virtio_ldq_p(vdev, &offloads);\n\n        if (!n->has_vnet_hdr) {\n            return VIRTIO_NET_ERR;\n        }\n\n        n->rsc4_enabled = virtio_has_feature(offloads, VIRTIO_NET_F_RSC_EXT) &&\n            virtio_has_feature(offloads, VIRTIO_NET_F_GUEST_TSO4);\n        n->rsc6_enabled = virtio_has_feature(offloads, VIRTIO_NET_F_RSC_EXT) &&\n            virtio_has_feature(offloads, VIRTIO_NET_F_GUEST_TSO6);\n        virtio_clear_feature(&offloads, VIRTIO_NET_F_RSC_EXT);\n\n        supported_offloads = virtio_net_supported_guest_offloads(n);\n        if (offloads & ~supported_offloads) {\n            return VIRTIO_NET_ERR;\n        }\n\n        n->curr_guest_offloads = offloads;\n        virtio_net_apply_guest_offloads(n);\n\n        return VIRTIO_NET_OK;\n    } else {\n        return VIRTIO_NET_ERR;\n    }\n}\n\nstatic int virtio_net_handle_mac(VirtIONet *n, uint8_t cmd,\n                                 struct iovec *iov, unsigned int iov_cnt)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    struct virtio_net_ctrl_mac mac_data;\n    size_t s;\n    NetClientState *nc = qemu_get_queue(n->nic);\n\n    if (cmd == VIRTIO_NET_CTRL_MAC_ADDR_SET) {\n        if (iov_size(iov, iov_cnt) != sizeof(n->mac)) {\n            return VIRTIO_NET_ERR;\n        }\n        s = iov_to_buf(iov, iov_cnt, 0, &n->mac, sizeof(n->mac));\n        assert(s == sizeof(n->mac));\n        qemu_format_nic_info_str(qemu_get_queue(n->nic), n->mac);\n        rxfilter_notify(nc);\n\n        return VIRTIO_NET_OK;\n    }\n\n    if (cmd != VIRTIO_NET_CTRL_MAC_TABLE_SET) {\n        return VIRTIO_NET_ERR;\n    }\n\n    int in_use = 0;\n    int first_multi = 0;\n    uint8_t uni_overflow = 0;\n    uint8_t multi_overflow = 0;\n    uint8_t *macs = g_malloc0(MAC_TABLE_ENTRIES * ETH_ALEN);\n\n    s = iov_to_buf(iov, iov_cnt, 0, &mac_data.entries,\n                   sizeof(mac_data.entries));\n    mac_data.entries = virtio_ldl_p(vdev, &mac_data.entries);\n    if (s != sizeof(mac_data.entries)) {\n        goto error;\n    }\n    iov_discard_front(&iov, &iov_cnt, s);\n\n    if (mac_data.entries * ETH_ALEN > iov_size(iov, iov_cnt)) {\n        goto error;\n    }\n\n    if (mac_data.entries <= MAC_TABLE_ENTRIES) {\n        s = iov_to_buf(iov, iov_cnt, 0, macs,\n                       mac_data.entries * ETH_ALEN);\n        if (s != mac_data.entries * ETH_ALEN) {\n            goto error;\n        }\n        in_use += mac_data.entries;\n    } else {\n        uni_overflow = 1;\n    }\n\n    iov_discard_front(&iov, &iov_cnt, mac_data.entries * ETH_ALEN);\n\n    first_multi = in_use;\n\n    s = iov_to_buf(iov, iov_cnt, 0, &mac_data.entries,\n                   sizeof(mac_data.entries));\n    mac_data.entries = virtio_ldl_p(vdev, &mac_data.entries);\n    if (s != sizeof(mac_data.entries)) {\n        goto error;\n    }\n\n    iov_discard_front(&iov, &iov_cnt, s);\n\n    if (mac_data.entries * ETH_ALEN != iov_size(iov, iov_cnt)) {\n        goto error;\n    }\n\n    if (mac_data.entries <= MAC_TABLE_ENTRIES - in_use) {\n        s = iov_to_buf(iov, iov_cnt, 0, &macs[in_use * ETH_ALEN],\n                       mac_data.entries * ETH_ALEN);\n        if (s != mac_data.entries * ETH_ALEN) {\n            goto error;\n        }\n        in_use += mac_data.entries;\n    } else {\n        multi_overflow = 1;\n    }\n\n    n->mac_table.in_use = in_use;\n    n->mac_table.first_multi = first_multi;\n    n->mac_table.uni_overflow = uni_overflow;\n    n->mac_table.multi_overflow = multi_overflow;\n    memcpy(n->mac_table.macs, macs, MAC_TABLE_ENTRIES * ETH_ALEN);\n    g_free(macs);\n    rxfilter_notify(nc);\n\n    return VIRTIO_NET_OK;\n\nerror:\n    g_free(macs);\n    return VIRTIO_NET_ERR;\n}\n\nstatic int virtio_net_handle_vlan_table(VirtIONet *n, uint8_t cmd,\n                                        struct iovec *iov, unsigned int iov_cnt)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    uint16_t vid;\n    size_t s;\n    NetClientState *nc = qemu_get_queue(n->nic);\n\n    s = iov_to_buf(iov, iov_cnt, 0, &vid, sizeof(vid));\n    vid = virtio_lduw_p(vdev, &vid);\n    if (s != sizeof(vid)) {\n        return VIRTIO_NET_ERR;\n    }\n\n    if (vid >= MAX_VLAN)\n        return VIRTIO_NET_ERR;\n\n    if (cmd == VIRTIO_NET_CTRL_VLAN_ADD)\n        n->vlans[vid >> 5] |= (1U << (vid & 0x1f));\n    else if (cmd == VIRTIO_NET_CTRL_VLAN_DEL)\n        n->vlans[vid >> 5] &= ~(1U << (vid & 0x1f));\n    else\n        return VIRTIO_NET_ERR;\n\n    rxfilter_notify(nc);\n\n    return VIRTIO_NET_OK;\n}\n\nstatic int virtio_net_handle_announce(VirtIONet *n, uint8_t cmd,\n                                      struct iovec *iov, unsigned int iov_cnt)\n{\n    trace_virtio_net_handle_announce(n->announce_timer.round);\n    if (cmd == VIRTIO_NET_CTRL_ANNOUNCE_ACK &&\n        n->status & VIRTIO_NET_S_ANNOUNCE) {\n        n->status &= ~VIRTIO_NET_S_ANNOUNCE;\n        if (n->announce_timer.round) {\n            qemu_announce_timer_step(&n->announce_timer);\n        }\n        return VIRTIO_NET_OK;\n    } else {\n        return VIRTIO_NET_ERR;\n    }\n}\n\nstatic void virtio_net_detach_epbf_rss(VirtIONet *n);\n\nstatic void virtio_net_disable_rss(VirtIONet *n)\n{\n    if (n->rss_data.enabled) {\n        trace_virtio_net_rss_disable();\n    }\n    n->rss_data.enabled = false;\n\n    virtio_net_detach_epbf_rss(n);\n}\n\nstatic bool virtio_net_attach_ebpf_to_backend(NICState *nic, int prog_fd)\n{\n    NetClientState *nc = qemu_get_peer(qemu_get_queue(nic), 0);\n    if (nc == NULL || nc->info->set_steering_ebpf == NULL) {\n        return false;\n    }\n\n    return nc->info->set_steering_ebpf(nc, prog_fd);\n}\n\nstatic void rss_data_to_rss_config(struct VirtioNetRssData *data,\n                                   struct EBPFRSSConfig *config)\n{\n    config->redirect = data->redirect;\n    config->populate_hash = data->populate_hash;\n    config->hash_types = data->hash_types;\n    config->indirections_len = data->indirections_len;\n    config->default_queue = data->default_queue;\n}\n\nstatic bool virtio_net_attach_epbf_rss(VirtIONet *n)\n{\n    struct EBPFRSSConfig config = {};\n\n    if (!ebpf_rss_is_loaded(&n->ebpf_rss)) {\n        return false;\n    }\n\n    rss_data_to_rss_config(&n->rss_data, &config);\n\n    if (!ebpf_rss_set_all(&n->ebpf_rss, &config,\n                          n->rss_data.indirections_table, n->rss_data.key)) {\n        return false;\n    }\n\n    if (!virtio_net_attach_ebpf_to_backend(n->nic, n->ebpf_rss.program_fd)) {\n        return false;\n    }\n\n    return true;\n}\n\nstatic void virtio_net_detach_epbf_rss(VirtIONet *n)\n{\n    virtio_net_attach_ebpf_to_backend(n->nic, -1);\n}\n\nstatic bool virtio_net_load_ebpf(VirtIONet *n)\n{\n    if (!virtio_net_attach_ebpf_to_backend(n->nic, -1)) {\n        /* backend does't support steering ebpf */\n        return false;\n    }\n\n    return ebpf_rss_load(&n->ebpf_rss);\n}\n\nstatic void virtio_net_unload_ebpf(VirtIONet *n)\n{\n    virtio_net_attach_ebpf_to_backend(n->nic, -1);\n    ebpf_rss_unload(&n->ebpf_rss);\n}\n\nstatic uint16_t virtio_net_handle_rss(VirtIONet *n,\n                                      struct iovec *iov,\n                                      unsigned int iov_cnt,\n                                      bool do_rss)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    struct virtio_net_rss_config cfg;\n    size_t s, offset = 0, size_get;\n    uint16_t queues, i;\n    struct {\n        uint16_t us;\n        uint8_t b;\n    } QEMU_PACKED temp;\n    const char *err_msg = \"\";\n    uint32_t err_value = 0;\n\n    if (do_rss && !virtio_vdev_has_feature(vdev, VIRTIO_NET_F_RSS)) {\n        err_msg = \"RSS is not negotiated\";\n        goto error;\n    }\n    if (!do_rss && !virtio_vdev_has_feature(vdev, VIRTIO_NET_F_HASH_REPORT)) {\n        err_msg = \"Hash report is not negotiated\";\n        goto error;\n    }\n    size_get = offsetof(struct virtio_net_rss_config, indirection_table);\n    s = iov_to_buf(iov, iov_cnt, offset, &cfg, size_get);\n    if (s != size_get) {\n        err_msg = \"Short command buffer\";\n        err_value = (uint32_t)s;\n        goto error;\n    }\n    n->rss_data.hash_types = virtio_ldl_p(vdev, &cfg.hash_types);\n    n->rss_data.indirections_len =\n        virtio_lduw_p(vdev, &cfg.indirection_table_mask);\n    n->rss_data.indirections_len++;\n    if (!do_rss) {\n        n->rss_data.indirections_len = 1;\n    }\n    if (!is_power_of_2(n->rss_data.indirections_len)) {\n        err_msg = \"Invalid size of indirection table\";\n        err_value = n->rss_data.indirections_len;\n        goto error;\n    }\n    if (n->rss_data.indirections_len > VIRTIO_NET_RSS_MAX_TABLE_LEN) {\n        err_msg = \"Too large indirection table\";\n        err_value = n->rss_data.indirections_len;\n        goto error;\n    }\n    n->rss_data.default_queue = do_rss ?\n        virtio_lduw_p(vdev, &cfg.unclassified_queue) : 0;\n    if (n->rss_data.default_queue >= n->max_queues) {\n        err_msg = \"Invalid default queue\";\n        err_value = n->rss_data.default_queue;\n        goto error;\n    }\n    offset += size_get;\n    size_get = sizeof(uint16_t) * n->rss_data.indirections_len;\n    g_free(n->rss_data.indirections_table);\n    n->rss_data.indirections_table = g_malloc(size_get);\n    if (!n->rss_data.indirections_table) {\n        err_msg = \"Can't allocate indirections table\";\n        err_value = n->rss_data.indirections_len;\n        goto error;\n    }\n    s = iov_to_buf(iov, iov_cnt, offset,\n                   n->rss_data.indirections_table, size_get);\n    if (s != size_get) {\n        err_msg = \"Short indirection table buffer\";\n        err_value = (uint32_t)s;\n        goto error;\n    }\n    for (i = 0; i < n->rss_data.indirections_len; ++i) {\n        uint16_t val = n->rss_data.indirections_table[i];\n        n->rss_data.indirections_table[i] = virtio_lduw_p(vdev, &val);\n    }\n    offset += size_get;\n    size_get = sizeof(temp);\n    s = iov_to_buf(iov, iov_cnt, offset, &temp, size_get);\n    if (s != size_get) {\n        err_msg = \"Can't get queues\";\n        err_value = (uint32_t)s;\n        goto error;\n    }\n    queues = do_rss ? virtio_lduw_p(vdev, &temp.us) : n->curr_queues;\n    if (queues == 0 || queues > n->max_queues) {\n        err_msg = \"Invalid number of queues\";\n        err_value = queues;\n        goto error;\n    }\n    if (temp.b > VIRTIO_NET_RSS_MAX_KEY_SIZE) {\n        err_msg = \"Invalid key size\";\n        err_value = temp.b;\n        goto error;\n    }\n    if (!temp.b && n->rss_data.hash_types) {\n        err_msg = \"No key provided\";\n        err_value = 0;\n        goto error;\n    }\n    if (!temp.b && !n->rss_data.hash_types) {\n        virtio_net_disable_rss(n);\n        return queues;\n    }\n    offset += size_get;\n    size_get = temp.b;\n    s = iov_to_buf(iov, iov_cnt, offset, n->rss_data.key, size_get);\n    if (s != size_get) {\n        err_msg = \"Can get key buffer\";\n        err_value = (uint32_t)s;\n        goto error;\n    }\n    n->rss_data.enabled = true;\n\n    if (!n->rss_data.populate_hash) {\n        if (!virtio_net_attach_epbf_rss(n)) {\n            /* EBPF must be loaded for vhost */\n            if (get_vhost_net(qemu_get_queue(n->nic)->peer)) {\n                warn_report(\"Can't load eBPF RSS for vhost\");\n                goto error;\n            }\n            /* fallback to software RSS */\n            warn_report(\"Can't load eBPF RSS - fallback to software RSS\");\n            n->rss_data.enabled_software_rss = true;\n        }\n    } else {\n        /* use software RSS for hash populating */\n        /* and detach eBPF if was loaded before */\n        virtio_net_detach_epbf_rss(n);\n        n->rss_data.enabled_software_rss = true;\n    }\n\n    trace_virtio_net_rss_enable(n->rss_data.hash_types,\n                                n->rss_data.indirections_len,\n                                temp.b);\n    return queues;\nerror:\n    trace_virtio_net_rss_error(err_msg, err_value);\n    virtio_net_disable_rss(n);\n    return 0;\n}\n\nstatic int virtio_net_handle_mq(VirtIONet *n, uint8_t cmd,\n                                struct iovec *iov, unsigned int iov_cnt)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    uint16_t queues;\n\n    virtio_net_disable_rss(n);\n    if (cmd == VIRTIO_NET_CTRL_MQ_HASH_CONFIG) {\n        queues = virtio_net_handle_rss(n, iov, iov_cnt, false);\n        return queues ? VIRTIO_NET_OK : VIRTIO_NET_ERR;\n    }\n    if (cmd == VIRTIO_NET_CTRL_MQ_RSS_CONFIG) {\n        queues = virtio_net_handle_rss(n, iov, iov_cnt, true);\n    } else if (cmd == VIRTIO_NET_CTRL_MQ_VQ_PAIRS_SET) {\n        struct virtio_net_ctrl_mq mq;\n        size_t s;\n        if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_MQ)) {\n            return VIRTIO_NET_ERR;\n        }\n        s = iov_to_buf(iov, iov_cnt, 0, &mq, sizeof(mq));\n        if (s != sizeof(mq)) {\n            return VIRTIO_NET_ERR;\n        }\n        queues = virtio_lduw_p(vdev, &mq.virtqueue_pairs);\n\n    } else {\n        return VIRTIO_NET_ERR;\n    }\n\n    if (queues < VIRTIO_NET_CTRL_MQ_VQ_PAIRS_MIN ||\n        queues > VIRTIO_NET_CTRL_MQ_VQ_PAIRS_MAX ||\n        queues > n->max_queues ||\n        !n->multiqueue) {\n        return VIRTIO_NET_ERR;\n    }\n\n    n->curr_queues = queues;\n    /* stop the backend before changing the number of queues to avoid handling a\n     * disabled queue */\n    virtio_net_set_status(vdev, vdev->status);\n    virtio_net_set_queues(n);\n\n    return VIRTIO_NET_OK;\n}\n\nstatic void virtio_net_handle_ctrl(VirtIODevice *vdev, VirtQueue *vq)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    struct virtio_net_ctrl_hdr ctrl;\n    virtio_net_ctrl_ack status = VIRTIO_NET_ERR;\n    VirtQueueElement *elem;\n    size_t s;\n    struct iovec *iov, *iov2;\n    unsigned int iov_cnt;\n\n    for (;;) {\n        elem = virtqueue_pop(vq, sizeof(VirtQueueElement));\n        if (!elem) {\n            break;\n        }\n        if (iov_size(elem->in_sg, elem->in_num) < sizeof(status) ||\n            iov_size(elem->out_sg, elem->out_num) < sizeof(ctrl)) {\n            virtio_error(vdev, \"virtio-net ctrl missing headers\");\n            virtqueue_detach_element(vq, elem, 0);\n            g_free(elem);\n            break;\n        }\n\n        iov_cnt = elem->out_num;\n        iov2 = iov = g_memdup(elem->out_sg, sizeof(struct iovec) * elem->out_num);\n        s = iov_to_buf(iov, iov_cnt, 0, &ctrl, sizeof(ctrl));\n        iov_discard_front(&iov, &iov_cnt, sizeof(ctrl));\n        if (s != sizeof(ctrl)) {\n            status = VIRTIO_NET_ERR;\n        } else if (ctrl.class == VIRTIO_NET_CTRL_RX) {\n            status = virtio_net_handle_rx_mode(n, ctrl.cmd, iov, iov_cnt);\n        } else if (ctrl.class == VIRTIO_NET_CTRL_MAC) {\n            status = virtio_net_handle_mac(n, ctrl.cmd, iov, iov_cnt);\n        } else if (ctrl.class == VIRTIO_NET_CTRL_VLAN) {\n            status = virtio_net_handle_vlan_table(n, ctrl.cmd, iov, iov_cnt);\n        } else if (ctrl.class == VIRTIO_NET_CTRL_ANNOUNCE) {\n            status = virtio_net_handle_announce(n, ctrl.cmd, iov, iov_cnt);\n        } else if (ctrl.class == VIRTIO_NET_CTRL_MQ) {\n            status = virtio_net_handle_mq(n, ctrl.cmd, iov, iov_cnt);\n        } else if (ctrl.class == VIRTIO_NET_CTRL_GUEST_OFFLOADS) {\n            status = virtio_net_handle_offloads(n, ctrl.cmd, iov, iov_cnt);\n        }\n\n        s = iov_from_buf(elem->in_sg, elem->in_num, 0, &status, sizeof(status));\n        assert(s == sizeof(status));\n\n        virtqueue_push(vq, elem, sizeof(status));\n        virtio_notify(vdev, vq);\n        g_free(iov2);\n        g_free(elem);\n    }\n}\n\n/* RX */\n\nstatic void virtio_net_handle_rx(VirtIODevice *vdev, VirtQueue *vq)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    int queue_index = vq2q(virtio_get_queue_index(vq));\n\n    qemu_flush_queued_packets(qemu_get_subqueue(n->nic, queue_index));\n}\n\nstatic bool virtio_net_can_receive(NetClientState *nc)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    VirtIONetQueue *q = virtio_net_get_subqueue(nc);\n\n    if (!vdev->vm_running) {\n        return false;\n    }\n\n    if (nc->queue_index >= n->curr_queues) {\n        return false;\n    }\n\n    if (!virtio_queue_ready(q->rx_vq) ||\n        !(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK)) {\n        return false;\n    }\n\n    return true;\n}\n\nstatic int virtio_net_has_buffers(VirtIONetQueue *q, int bufsize)\n{\n    VirtIONet *n = q->n;\n    if (virtio_queue_empty(q->rx_vq) ||\n        (n->mergeable_rx_bufs &&\n         !virtqueue_avail_bytes(q->rx_vq, bufsize, 0))) {\n        virtio_queue_set_notification(q->rx_vq, 1);\n\n        /* To avoid a race condition where the guest has made some buffers\n         * available after the above check but before notification was\n         * enabled, check for available buffers again.\n         */\n        if (virtio_queue_empty(q->rx_vq) ||\n            (n->mergeable_rx_bufs &&\n             !virtqueue_avail_bytes(q->rx_vq, bufsize, 0))) {\n            return 0;\n        }\n    }\n\n    virtio_queue_set_notification(q->rx_vq, 0);\n    return 1;\n}\n\nstatic void virtio_net_hdr_swap(VirtIODevice *vdev, struct virtio_net_hdr *hdr)\n{\n    virtio_tswap16s(vdev, &hdr->hdr_len);\n    virtio_tswap16s(vdev, &hdr->gso_size);\n    virtio_tswap16s(vdev, &hdr->csum_start);\n    virtio_tswap16s(vdev, &hdr->csum_offset);\n}\n\n/* dhclient uses AF_PACKET but doesn't pass auxdata to the kernel so\n * it never finds out that the packets don't have valid checksums.  This\n * causes dhclient to get upset.  Fedora's carried a patch for ages to\n * fix this with Xen but it hasn't appeared in an upstream release of\n * dhclient yet.\n *\n * To avoid breaking existing guests, we catch udp packets and add\n * checksums.  This is terrible but it's better than hacking the guest\n * kernels.\n *\n * N.B. if we introduce a zero-copy API, this operation is no longer free so\n * we should provide a mechanism to disable it to avoid polluting the host\n * cache.\n */\nstatic void work_around_broken_dhclient(struct virtio_net_hdr *hdr,\n                                        uint8_t *buf, size_t size)\n{\n    if ((hdr->flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) && /* missing csum */\n        (size > 27 && size < 1500) && /* normal sized MTU */\n        (buf[12] == 0x08 && buf[13] == 0x00) && /* ethertype == IPv4 */\n        (buf[23] == 17) && /* ip.protocol == UDP */\n        (buf[34] == 0 && buf[35] == 67)) { /* udp.srcport == bootps */\n        net_checksum_calculate(buf, size, CSUM_UDP);\n        hdr->flags &= ~VIRTIO_NET_HDR_F_NEEDS_CSUM;\n    }\n}\n\nstatic void receive_header(VirtIONet *n, const struct iovec *iov, int iov_cnt,\n                           const void *buf, size_t size)\n{\n    if (n->has_vnet_hdr) {\n        /* FIXME this cast is evil */\n        void *wbuf = (void *)buf;\n        work_around_broken_dhclient(wbuf, wbuf + n->host_hdr_len,\n                                    size - n->host_hdr_len);\n\n        if (n->needs_vnet_hdr_swap) {\n            virtio_net_hdr_swap(VIRTIO_DEVICE(n), wbuf);\n        }\n        iov_from_buf(iov, iov_cnt, 0, buf, sizeof(struct virtio_net_hdr));\n    } else {\n        struct virtio_net_hdr hdr = {\n            .flags = 0,\n            .gso_type = VIRTIO_NET_HDR_GSO_NONE\n        };\n        iov_from_buf(iov, iov_cnt, 0, &hdr, sizeof hdr);\n    }\n}\n\nstatic int receive_filter(VirtIONet *n, const uint8_t *buf, int size)\n{\n    static const uint8_t bcast[] = {0xff, 0xff, 0xff, 0xff, 0xff, 0xff};\n    static const uint8_t vlan[] = {0x81, 0x00};\n    uint8_t *ptr = (uint8_t *)buf;\n    int i;\n\n    if (n->promisc)\n        return 1;\n\n    ptr += n->host_hdr_len;\n\n    if (!memcmp(&ptr[12], vlan, sizeof(vlan))) {\n        int vid = lduw_be_p(ptr + 14) & 0xfff;\n        if (!(n->vlans[vid >> 5] & (1U << (vid & 0x1f))))\n            return 0;\n    }\n\n    if (ptr[0] & 1) { // multicast\n        if (!memcmp(ptr, bcast, sizeof(bcast))) {\n            return !n->nobcast;\n        } else if (n->nomulti) {\n            return 0;\n        } else if (n->allmulti || n->mac_table.multi_overflow) {\n            return 1;\n        }\n\n        for (i = n->mac_table.first_multi; i < n->mac_table.in_use; i++) {\n            if (!memcmp(ptr, &n->mac_table.macs[i * ETH_ALEN], ETH_ALEN)) {\n                return 1;\n            }\n        }\n    } else { // unicast\n        if (n->nouni) {\n            return 0;\n        } else if (n->alluni || n->mac_table.uni_overflow) {\n            return 1;\n        } else if (!memcmp(ptr, n->mac, ETH_ALEN)) {\n            return 1;\n        }\n\n        for (i = 0; i < n->mac_table.first_multi; i++) {\n            if (!memcmp(ptr, &n->mac_table.macs[i * ETH_ALEN], ETH_ALEN)) {\n                return 1;\n            }\n        }\n    }\n\n    return 0;\n}\n\nstatic uint8_t virtio_net_get_hash_type(bool isip4,\n                                        bool isip6,\n                                        bool isudp,\n                                        bool istcp,\n                                        uint32_t types)\n{\n    if (isip4) {\n        if (istcp && (types & VIRTIO_NET_RSS_HASH_TYPE_TCPv4)) {\n            return NetPktRssIpV4Tcp;\n        }\n        if (isudp && (types & VIRTIO_NET_RSS_HASH_TYPE_UDPv4)) {\n            return NetPktRssIpV4Udp;\n        }\n        if (types & VIRTIO_NET_RSS_HASH_TYPE_IPv4) {\n            return NetPktRssIpV4;\n        }\n    } else if (isip6) {\n        uint32_t mask = VIRTIO_NET_RSS_HASH_TYPE_TCP_EX |\n                        VIRTIO_NET_RSS_HASH_TYPE_TCPv6;\n\n        if (istcp && (types & mask)) {\n            return (types & VIRTIO_NET_RSS_HASH_TYPE_TCP_EX) ?\n                NetPktRssIpV6TcpEx : NetPktRssIpV6Tcp;\n        }\n        mask = VIRTIO_NET_RSS_HASH_TYPE_UDP_EX | VIRTIO_NET_RSS_HASH_TYPE_UDPv6;\n        if (isudp && (types & mask)) {\n            return (types & VIRTIO_NET_RSS_HASH_TYPE_UDP_EX) ?\n                NetPktRssIpV6UdpEx : NetPktRssIpV6Udp;\n        }\n        mask = VIRTIO_NET_RSS_HASH_TYPE_IP_EX | VIRTIO_NET_RSS_HASH_TYPE_IPv6;\n        if (types & mask) {\n            return (types & VIRTIO_NET_RSS_HASH_TYPE_IP_EX) ?\n                NetPktRssIpV6Ex : NetPktRssIpV6;\n        }\n    }\n    return 0xff;\n}\n\nstatic void virtio_set_packet_hash(const uint8_t *buf, uint8_t report,\n                                   uint32_t hash)\n{\n    struct virtio_net_hdr_v1_hash *hdr = (void *)buf;\n    hdr->hash_value = hash;\n    hdr->hash_report = report;\n}\n\nstatic int virtio_net_process_rss(NetClientState *nc, const uint8_t *buf,\n                                  size_t size)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    unsigned int index = nc->queue_index, new_index = index;\n    struct NetRxPkt *pkt = n->rx_pkt;\n    uint8_t net_hash_type;\n    uint32_t hash;\n    bool isip4, isip6, isudp, istcp;\n    static const uint8_t reports[NetPktRssIpV6UdpEx + 1] = {\n        VIRTIO_NET_HASH_REPORT_IPv4,\n        VIRTIO_NET_HASH_REPORT_TCPv4,\n        VIRTIO_NET_HASH_REPORT_TCPv6,\n        VIRTIO_NET_HASH_REPORT_IPv6,\n        VIRTIO_NET_HASH_REPORT_IPv6_EX,\n        VIRTIO_NET_HASH_REPORT_TCPv6_EX,\n        VIRTIO_NET_HASH_REPORT_UDPv4,\n        VIRTIO_NET_HASH_REPORT_UDPv6,\n        VIRTIO_NET_HASH_REPORT_UDPv6_EX\n    };\n\n    net_rx_pkt_set_protocols(pkt, buf + n->host_hdr_len,\n                             size - n->host_hdr_len);\n    net_rx_pkt_get_protocols(pkt, &isip4, &isip6, &isudp, &istcp);\n    if (isip4 && (net_rx_pkt_get_ip4_info(pkt)->fragment)) {\n        istcp = isudp = false;\n    }\n    if (isip6 && (net_rx_pkt_get_ip6_info(pkt)->fragment)) {\n        istcp = isudp = false;\n    }\n    net_hash_type = virtio_net_get_hash_type(isip4, isip6, isudp, istcp,\n                                             n->rss_data.hash_types);\n    if (net_hash_type > NetPktRssIpV6UdpEx) {\n        if (n->rss_data.populate_hash) {\n            virtio_set_packet_hash(buf, VIRTIO_NET_HASH_REPORT_NONE, 0);\n        }\n        return n->rss_data.redirect ? n->rss_data.default_queue : -1;\n    }\n\n    hash = net_rx_pkt_calc_rss_hash(pkt, net_hash_type, n->rss_data.key);\n\n    if (n->rss_data.populate_hash) {\n        virtio_set_packet_hash(buf, reports[net_hash_type], hash);\n    }\n\n    if (n->rss_data.redirect) {\n        new_index = hash & (n->rss_data.indirections_len - 1);\n        new_index = n->rss_data.indirections_table[new_index];\n    }\n\n    return (index == new_index) ? -1 : new_index;\n}\n\nstatic ssize_t virtio_net_receive_rcu(NetClientState *nc, const uint8_t *buf,\n                                      size_t size, bool no_rss)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIONetQueue *q = virtio_net_get_subqueue(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    VirtQueueElement *elems[VIRTQUEUE_MAX_SIZE];\n    size_t lens[VIRTQUEUE_MAX_SIZE];\n    struct iovec mhdr_sg[VIRTQUEUE_MAX_SIZE];\n    struct virtio_net_hdr_mrg_rxbuf mhdr;\n    unsigned mhdr_cnt = 0;\n    size_t offset, i, guest_offset, j;\n    ssize_t err;\n\n    if (!virtio_net_can_receive(nc)) {\n        return -1;\n    }\n\n    if (!no_rss && n->rss_data.enabled && n->rss_data.enabled_software_rss) {\n        int index = virtio_net_process_rss(nc, buf, size);\n        if (index >= 0) {\n            NetClientState *nc2 = qemu_get_subqueue(n->nic, index);\n            return virtio_net_receive_rcu(nc2, buf, size, true);\n        }\n    }\n\n    /* hdr_len refers to the header we supply to the guest */\n    if (!virtio_net_has_buffers(q, size + n->guest_hdr_len - n->host_hdr_len)) {\n        return 0;\n    }\n\n    if (!receive_filter(n, buf, size))\n        return size;\n\n    offset = i = 0;\n\n    while (offset < size) {\n        VirtQueueElement *elem;\n        int len, total;\n        const struct iovec *sg;\n\n        total = 0;\n\n        if (i == VIRTQUEUE_MAX_SIZE) {\n            virtio_error(vdev, \"virtio-net unexpected long buffer chain\");\n            err = size;\n            goto err;\n        }\n\n        elem = virtqueue_pop(q->rx_vq, sizeof(VirtQueueElement));\n        if (!elem) {\n            if (i) {\n                virtio_error(vdev, \"virtio-net unexpected empty queue: \"\n                             \"i %zd mergeable %d offset %zd, size %zd, \"\n                             \"guest hdr len %zd, host hdr len %zd \"\n                             \"guest features 0x%\" PRIx64,\n                             i, n->mergeable_rx_bufs, offset, size,\n                             n->guest_hdr_len, n->host_hdr_len,\n                             vdev->guest_features);\n            }\n            err = -1;\n            goto err;\n        }\n\n        if (elem->in_num < 1) {\n            virtio_error(vdev,\n                         \"virtio-net receive queue contains no in buffers\");\n            virtqueue_detach_element(q->rx_vq, elem, 0);\n            g_free(elem);\n            err = -1;\n            goto err;\n        }\n\n        sg = elem->in_sg;\n        if (i == 0) {\n            assert(offset == 0);\n            if (n->mergeable_rx_bufs) {\n                mhdr_cnt = iov_copy(mhdr_sg, ARRAY_SIZE(mhdr_sg),\n                                    sg, elem->in_num,\n                                    offsetof(typeof(mhdr), num_buffers),\n                                    sizeof(mhdr.num_buffers));\n            }\n\n            receive_header(n, sg, elem->in_num, buf, size);\n            if (n->rss_data.populate_hash) {\n                offset = sizeof(mhdr);\n                iov_from_buf(sg, elem->in_num, offset,\n                             buf + offset, n->host_hdr_len - sizeof(mhdr));\n            }\n            offset = n->host_hdr_len;\n            total += n->guest_hdr_len;\n            guest_offset = n->guest_hdr_len;\n        } else {\n            guest_offset = 0;\n        }\n\n        /* copy in packet.  ugh */\n        len = iov_from_buf(sg, elem->in_num, guest_offset,\n                           buf + offset, size - offset);\n        total += len;\n        offset += len;\n        /* If buffers can't be merged, at this point we\n         * must have consumed the complete packet.\n         * Otherwise, drop it. */\n        if (!n->mergeable_rx_bufs && offset < size) {\n            virtqueue_unpop(q->rx_vq, elem, total);\n            g_free(elem);\n            err = size;\n            goto err;\n        }\n\n        elems[i] = elem;\n        lens[i] = total;\n        i++;\n    }\n\n    if (mhdr_cnt) {\n        virtio_stw_p(vdev, &mhdr.num_buffers, i);\n        iov_from_buf(mhdr_sg, mhdr_cnt,\n                     0,\n                     &mhdr.num_buffers, sizeof mhdr.num_buffers);\n    }\n\n    for (j = 0; j < i; j++) {\n        /* signal other side */\n        virtqueue_fill(q->rx_vq, elems[j], lens[j], j);\n        g_free(elems[j]);\n    }\n\n    virtqueue_flush(q->rx_vq, i);\n    virtio_notify(vdev, q->rx_vq);\n\n    return size;\n\nerr:\n    for (j = 0; j < i; j++) {\n        g_free(elems[j]);\n    }\n\n    return err;\n}\n\nstatic ssize_t virtio_net_do_receive(NetClientState *nc, const uint8_t *buf,\n                                  size_t size)\n{\n    RCU_READ_LOCK_GUARD();\n\n    return virtio_net_receive_rcu(nc, buf, size, false);\n}\n\nstatic void virtio_net_rsc_extract_unit4(VirtioNetRscChain *chain,\n                                         const uint8_t *buf,\n                                         VirtioNetRscUnit *unit)\n{\n    uint16_t ip_hdrlen;\n    struct ip_header *ip;\n\n    ip = (struct ip_header *)(buf + chain->n->guest_hdr_len\n                              + sizeof(struct eth_header));\n    unit->ip = (void *)ip;\n    ip_hdrlen = (ip->ip_ver_len & 0xF) << 2;\n    unit->ip_plen = &ip->ip_len;\n    unit->tcp = (struct tcp_header *)(((uint8_t *)unit->ip) + ip_hdrlen);\n    unit->tcp_hdrlen = (htons(unit->tcp->th_offset_flags) & 0xF000) >> 10;\n    unit->payload = htons(*unit->ip_plen) - ip_hdrlen - unit->tcp_hdrlen;\n}\n\nstatic void virtio_net_rsc_extract_unit6(VirtioNetRscChain *chain,\n                                         const uint8_t *buf,\n                                         VirtioNetRscUnit *unit)\n{\n    struct ip6_header *ip6;\n\n    ip6 = (struct ip6_header *)(buf + chain->n->guest_hdr_len\n                                 + sizeof(struct eth_header));\n    unit->ip = ip6;\n    unit->ip_plen = &(ip6->ip6_ctlun.ip6_un1.ip6_un1_plen);\n    unit->tcp = (struct tcp_header *)(((uint8_t *)unit->ip)\n                                        + sizeof(struct ip6_header));\n    unit->tcp_hdrlen = (htons(unit->tcp->th_offset_flags) & 0xF000) >> 10;\n\n    /* There is a difference between payload lenght in ipv4 and v6,\n       ip header is excluded in ipv6 */\n    unit->payload = htons(*unit->ip_plen) - unit->tcp_hdrlen;\n}\n\nstatic size_t virtio_net_rsc_drain_seg(VirtioNetRscChain *chain,\n                                       VirtioNetRscSeg *seg)\n{\n    int ret;\n    struct virtio_net_hdr_v1 *h;\n\n    h = (struct virtio_net_hdr_v1 *)seg->buf;\n    h->flags = 0;\n    h->gso_type = VIRTIO_NET_HDR_GSO_NONE;\n\n    if (seg->is_coalesced) {\n        h->rsc.segments = seg->packets;\n        h->rsc.dup_acks = seg->dup_ack;\n        h->flags = VIRTIO_NET_HDR_F_RSC_INFO;\n        if (chain->proto == ETH_P_IP) {\n            h->gso_type = VIRTIO_NET_HDR_GSO_TCPV4;\n        } else {\n            h->gso_type = VIRTIO_NET_HDR_GSO_TCPV6;\n        }\n    }\n\n    ret = virtio_net_do_receive(seg->nc, seg->buf, seg->size);\n    QTAILQ_REMOVE(&chain->buffers, seg, next);\n    g_free(seg->buf);\n    g_free(seg);\n\n    return ret;\n}\n\nstatic void virtio_net_rsc_purge(void *opq)\n{\n    VirtioNetRscSeg *seg, *rn;\n    VirtioNetRscChain *chain = (VirtioNetRscChain *)opq;\n\n    QTAILQ_FOREACH_SAFE(seg, &chain->buffers, next, rn) {\n        if (virtio_net_rsc_drain_seg(chain, seg) == 0) {\n            chain->stat.purge_failed++;\n            continue;\n        }\n    }\n\n    chain->stat.timer++;\n    if (!QTAILQ_EMPTY(&chain->buffers)) {\n        timer_mod(chain->drain_timer,\n              qemu_clock_get_ns(QEMU_CLOCK_HOST) + chain->n->rsc_timeout);\n    }\n}\n\nstatic void virtio_net_rsc_cleanup(VirtIONet *n)\n{\n    VirtioNetRscChain *chain, *rn_chain;\n    VirtioNetRscSeg *seg, *rn_seg;\n\n    QTAILQ_FOREACH_SAFE(chain, &n->rsc_chains, next, rn_chain) {\n        QTAILQ_FOREACH_SAFE(seg, &chain->buffers, next, rn_seg) {\n            QTAILQ_REMOVE(&chain->buffers, seg, next);\n            g_free(seg->buf);\n            g_free(seg);\n        }\n\n        timer_free(chain->drain_timer);\n        QTAILQ_REMOVE(&n->rsc_chains, chain, next);\n        g_free(chain);\n    }\n}\n\nstatic void virtio_net_rsc_cache_buf(VirtioNetRscChain *chain,\n                                     NetClientState *nc,\n                                     const uint8_t *buf, size_t size)\n{\n    uint16_t hdr_len;\n    VirtioNetRscSeg *seg;\n\n    hdr_len = chain->n->guest_hdr_len;\n    seg = g_malloc(sizeof(VirtioNetRscSeg));\n    seg->buf = g_malloc(hdr_len + sizeof(struct eth_header)\n        + sizeof(struct ip6_header) + VIRTIO_NET_MAX_TCP_PAYLOAD);\n    memcpy(seg->buf, buf, size);\n    seg->size = size;\n    seg->packets = 1;\n    seg->dup_ack = 0;\n    seg->is_coalesced = 0;\n    seg->nc = nc;\n\n    QTAILQ_INSERT_TAIL(&chain->buffers, seg, next);\n    chain->stat.cache++;\n\n    switch (chain->proto) {\n    case ETH_P_IP:\n        virtio_net_rsc_extract_unit4(chain, seg->buf, &seg->unit);\n        break;\n    case ETH_P_IPV6:\n        virtio_net_rsc_extract_unit6(chain, seg->buf, &seg->unit);\n        break;\n    default:\n        g_assert_not_reached();\n    }\n}\n\nstatic int32_t virtio_net_rsc_handle_ack(VirtioNetRscChain *chain,\n                                         VirtioNetRscSeg *seg,\n                                         const uint8_t *buf,\n                                         struct tcp_header *n_tcp,\n                                         struct tcp_header *o_tcp)\n{\n    uint32_t nack, oack;\n    uint16_t nwin, owin;\n\n    nack = htonl(n_tcp->th_ack);\n    nwin = htons(n_tcp->th_win);\n    oack = htonl(o_tcp->th_ack);\n    owin = htons(o_tcp->th_win);\n\n    if ((nack - oack) >= VIRTIO_NET_MAX_TCP_PAYLOAD) {\n        chain->stat.ack_out_of_win++;\n        return RSC_FINAL;\n    } else if (nack == oack) {\n        /* duplicated ack or window probe */\n        if (nwin == owin) {\n            /* duplicated ack, add dup ack count due to whql test up to 1 */\n            chain->stat.dup_ack++;\n            return RSC_FINAL;\n        } else {\n            /* Coalesce window update */\n            o_tcp->th_win = n_tcp->th_win;\n            chain->stat.win_update++;\n            return RSC_COALESCE;\n        }\n    } else {\n        /* pure ack, go to 'C', finalize*/\n        chain->stat.pure_ack++;\n        return RSC_FINAL;\n    }\n}\n\nstatic int32_t virtio_net_rsc_coalesce_data(VirtioNetRscChain *chain,\n                                            VirtioNetRscSeg *seg,\n                                            const uint8_t *buf,\n                                            VirtioNetRscUnit *n_unit)\n{\n    void *data;\n    uint16_t o_ip_len;\n    uint32_t nseq, oseq;\n    VirtioNetRscUnit *o_unit;\n\n    o_unit = &seg->unit;\n    o_ip_len = htons(*o_unit->ip_plen);\n    nseq = htonl(n_unit->tcp->th_seq);\n    oseq = htonl(o_unit->tcp->th_seq);\n\n    /* out of order or retransmitted. */\n    if ((nseq - oseq) > VIRTIO_NET_MAX_TCP_PAYLOAD) {\n        chain->stat.data_out_of_win++;\n        return RSC_FINAL;\n    }\n\n    data = ((uint8_t *)n_unit->tcp) + n_unit->tcp_hdrlen;\n    if (nseq == oseq) {\n        if ((o_unit->payload == 0) && n_unit->payload) {\n            /* From no payload to payload, normal case, not a dup ack or etc */\n            chain->stat.data_after_pure_ack++;\n            goto coalesce;\n        } else {\n            return virtio_net_rsc_handle_ack(chain, seg, buf,\n                                             n_unit->tcp, o_unit->tcp);\n        }\n    } else if ((nseq - oseq) != o_unit->payload) {\n        /* Not a consistent packet, out of order */\n        chain->stat.data_out_of_order++;\n        return RSC_FINAL;\n    } else {\ncoalesce:\n        if ((o_ip_len + n_unit->payload) > chain->max_payload) {\n            chain->stat.over_size++;\n            return RSC_FINAL;\n        }\n\n        /* Here comes the right data, the payload length in v4/v6 is different,\n           so use the field value to update and record the new data len */\n        o_unit->payload += n_unit->payload; /* update new data len */\n\n        /* update field in ip header */\n        *o_unit->ip_plen = htons(o_ip_len + n_unit->payload);\n\n        /* Bring 'PUSH' big, the whql test guide says 'PUSH' can be coalesced\n           for windows guest, while this may change the behavior for linux\n           guest (only if it uses RSC feature). */\n        o_unit->tcp->th_offset_flags = n_unit->tcp->th_offset_flags;\n\n        o_unit->tcp->th_ack = n_unit->tcp->th_ack;\n        o_unit->tcp->th_win = n_unit->tcp->th_win;\n\n        memmove(seg->buf + seg->size, data, n_unit->payload);\n        seg->size += n_unit->payload;\n        seg->packets++;\n        chain->stat.coalesced++;\n        return RSC_COALESCE;\n    }\n}\n\nstatic int32_t virtio_net_rsc_coalesce4(VirtioNetRscChain *chain,\n                                        VirtioNetRscSeg *seg,\n                                        const uint8_t *buf, size_t size,\n                                        VirtioNetRscUnit *unit)\n{\n    struct ip_header *ip1, *ip2;\n\n    ip1 = (struct ip_header *)(unit->ip);\n    ip2 = (struct ip_header *)(seg->unit.ip);\n    if ((ip1->ip_src ^ ip2->ip_src) || (ip1->ip_dst ^ ip2->ip_dst)\n        || (unit->tcp->th_sport ^ seg->unit.tcp->th_sport)\n        || (unit->tcp->th_dport ^ seg->unit.tcp->th_dport)) {\n        chain->stat.no_match++;\n        return RSC_NO_MATCH;\n    }\n\n    return virtio_net_rsc_coalesce_data(chain, seg, buf, unit);\n}\n\nstatic int32_t virtio_net_rsc_coalesce6(VirtioNetRscChain *chain,\n                                        VirtioNetRscSeg *seg,\n                                        const uint8_t *buf, size_t size,\n                                        VirtioNetRscUnit *unit)\n{\n    struct ip6_header *ip1, *ip2;\n\n    ip1 = (struct ip6_header *)(unit->ip);\n    ip2 = (struct ip6_header *)(seg->unit.ip);\n    if (memcmp(&ip1->ip6_src, &ip2->ip6_src, sizeof(struct in6_address))\n        || memcmp(&ip1->ip6_dst, &ip2->ip6_dst, sizeof(struct in6_address))\n        || (unit->tcp->th_sport ^ seg->unit.tcp->th_sport)\n        || (unit->tcp->th_dport ^ seg->unit.tcp->th_dport)) {\n            chain->stat.no_match++;\n            return RSC_NO_MATCH;\n    }\n\n    return virtio_net_rsc_coalesce_data(chain, seg, buf, unit);\n}\n\n/* Packets with 'SYN' should bypass, other flag should be sent after drain\n * to prevent out of order */\nstatic int virtio_net_rsc_tcp_ctrl_check(VirtioNetRscChain *chain,\n                                         struct tcp_header *tcp)\n{\n    uint16_t tcp_hdr;\n    uint16_t tcp_flag;\n\n    tcp_flag = htons(tcp->th_offset_flags);\n    tcp_hdr = (tcp_flag & VIRTIO_NET_TCP_HDR_LENGTH) >> 10;\n    tcp_flag &= VIRTIO_NET_TCP_FLAG;\n    if (tcp_flag & TH_SYN) {\n        chain->stat.tcp_syn++;\n        return RSC_BYPASS;\n    }\n\n    if (tcp_flag & (TH_FIN | TH_URG | TH_RST | TH_ECE | TH_CWR)) {\n        chain->stat.tcp_ctrl_drain++;\n        return RSC_FINAL;\n    }\n\n    if (tcp_hdr > sizeof(struct tcp_header)) {\n        chain->stat.tcp_all_opt++;\n        return RSC_FINAL;\n    }\n\n    return RSC_CANDIDATE;\n}\n\nstatic size_t virtio_net_rsc_do_coalesce(VirtioNetRscChain *chain,\n                                         NetClientState *nc,\n                                         const uint8_t *buf, size_t size,\n                                         VirtioNetRscUnit *unit)\n{\n    int ret;\n    VirtioNetRscSeg *seg, *nseg;\n\n    if (QTAILQ_EMPTY(&chain->buffers)) {\n        chain->stat.empty_cache++;\n        virtio_net_rsc_cache_buf(chain, nc, buf, size);\n        timer_mod(chain->drain_timer,\n              qemu_clock_get_ns(QEMU_CLOCK_HOST) + chain->n->rsc_timeout);\n        return size;\n    }\n\n    QTAILQ_FOREACH_SAFE(seg, &chain->buffers, next, nseg) {\n        if (chain->proto == ETH_P_IP) {\n            ret = virtio_net_rsc_coalesce4(chain, seg, buf, size, unit);\n        } else {\n            ret = virtio_net_rsc_coalesce6(chain, seg, buf, size, unit);\n        }\n\n        if (ret == RSC_FINAL) {\n            if (virtio_net_rsc_drain_seg(chain, seg) == 0) {\n                /* Send failed */\n                chain->stat.final_failed++;\n                return 0;\n            }\n\n            /* Send current packet */\n            return virtio_net_do_receive(nc, buf, size);\n        } else if (ret == RSC_NO_MATCH) {\n            continue;\n        } else {\n            /* Coalesced, mark coalesced flag to tell calc cksum for ipv4 */\n            seg->is_coalesced = 1;\n            return size;\n        }\n    }\n\n    chain->stat.no_match_cache++;\n    virtio_net_rsc_cache_buf(chain, nc, buf, size);\n    return size;\n}\n\n/* Drain a connection data, this is to avoid out of order segments */\nstatic size_t virtio_net_rsc_drain_flow(VirtioNetRscChain *chain,\n                                        NetClientState *nc,\n                                        const uint8_t *buf, size_t size,\n                                        uint16_t ip_start, uint16_t ip_size,\n                                        uint16_t tcp_port)\n{\n    VirtioNetRscSeg *seg, *nseg;\n    uint32_t ppair1, ppair2;\n\n    ppair1 = *(uint32_t *)(buf + tcp_port);\n    QTAILQ_FOREACH_SAFE(seg, &chain->buffers, next, nseg) {\n        ppair2 = *(uint32_t *)(seg->buf + tcp_port);\n        if (memcmp(buf + ip_start, seg->buf + ip_start, ip_size)\n            || (ppair1 != ppair2)) {\n            continue;\n        }\n        if (virtio_net_rsc_drain_seg(chain, seg) == 0) {\n            chain->stat.drain_failed++;\n        }\n\n        break;\n    }\n\n    return virtio_net_do_receive(nc, buf, size);\n}\n\nstatic int32_t virtio_net_rsc_sanity_check4(VirtioNetRscChain *chain,\n                                            struct ip_header *ip,\n                                            const uint8_t *buf, size_t size)\n{\n    uint16_t ip_len;\n\n    /* Not an ipv4 packet */\n    if (((ip->ip_ver_len & 0xF0) >> 4) != IP_HEADER_VERSION_4) {\n        chain->stat.ip_option++;\n        return RSC_BYPASS;\n    }\n\n    /* Don't handle packets with ip option */\n    if ((ip->ip_ver_len & 0xF) != VIRTIO_NET_IP4_HEADER_LENGTH) {\n        chain->stat.ip_option++;\n        return RSC_BYPASS;\n    }\n\n    if (ip->ip_p != IPPROTO_TCP) {\n        chain->stat.bypass_not_tcp++;\n        return RSC_BYPASS;\n    }\n\n    /* Don't handle packets with ip fragment */\n    if (!(htons(ip->ip_off) & IP_DF)) {\n        chain->stat.ip_frag++;\n        return RSC_BYPASS;\n    }\n\n    /* Don't handle packets with ecn flag */\n    if (IPTOS_ECN(ip->ip_tos)) {\n        chain->stat.ip_ecn++;\n        return RSC_BYPASS;\n    }\n\n    ip_len = htons(ip->ip_len);\n    if (ip_len < (sizeof(struct ip_header) + sizeof(struct tcp_header))\n        || ip_len > (size - chain->n->guest_hdr_len -\n                     sizeof(struct eth_header))) {\n        chain->stat.ip_hacked++;\n        return RSC_BYPASS;\n    }\n\n    return RSC_CANDIDATE;\n}\n\nstatic size_t virtio_net_rsc_receive4(VirtioNetRscChain *chain,\n                                      NetClientState *nc,\n                                      const uint8_t *buf, size_t size)\n{\n    int32_t ret;\n    uint16_t hdr_len;\n    VirtioNetRscUnit unit;\n\n    hdr_len = ((VirtIONet *)(chain->n))->guest_hdr_len;\n\n    if (size < (hdr_len + sizeof(struct eth_header) + sizeof(struct ip_header)\n        + sizeof(struct tcp_header))) {\n        chain->stat.bypass_not_tcp++;\n        return virtio_net_do_receive(nc, buf, size);\n    }\n\n    virtio_net_rsc_extract_unit4(chain, buf, &unit);\n    if (virtio_net_rsc_sanity_check4(chain, unit.ip, buf, size)\n        != RSC_CANDIDATE) {\n        return virtio_net_do_receive(nc, buf, size);\n    }\n\n    ret = virtio_net_rsc_tcp_ctrl_check(chain, unit.tcp);\n    if (ret == RSC_BYPASS) {\n        return virtio_net_do_receive(nc, buf, size);\n    } else if (ret == RSC_FINAL) {\n        return virtio_net_rsc_drain_flow(chain, nc, buf, size,\n                ((hdr_len + sizeof(struct eth_header)) + 12),\n                VIRTIO_NET_IP4_ADDR_SIZE,\n                hdr_len + sizeof(struct eth_header) + sizeof(struct ip_header));\n    }\n\n    return virtio_net_rsc_do_coalesce(chain, nc, buf, size, &unit);\n}\n\nstatic int32_t virtio_net_rsc_sanity_check6(VirtioNetRscChain *chain,\n                                            struct ip6_header *ip6,\n                                            const uint8_t *buf, size_t size)\n{\n    uint16_t ip_len;\n\n    if (((ip6->ip6_ctlun.ip6_un1.ip6_un1_flow & 0xF0) >> 4)\n        != IP_HEADER_VERSION_6) {\n        return RSC_BYPASS;\n    }\n\n    /* Both option and protocol is checked in this */\n    if (ip6->ip6_ctlun.ip6_un1.ip6_un1_nxt != IPPROTO_TCP) {\n        chain->stat.bypass_not_tcp++;\n        return RSC_BYPASS;\n    }\n\n    ip_len = htons(ip6->ip6_ctlun.ip6_un1.ip6_un1_plen);\n    if (ip_len < sizeof(struct tcp_header) ||\n        ip_len > (size - chain->n->guest_hdr_len - sizeof(struct eth_header)\n                  - sizeof(struct ip6_header))) {\n        chain->stat.ip_hacked++;\n        return RSC_BYPASS;\n    }\n\n    /* Don't handle packets with ecn flag */\n    if (IP6_ECN(ip6->ip6_ctlun.ip6_un3.ip6_un3_ecn)) {\n        chain->stat.ip_ecn++;\n        return RSC_BYPASS;\n    }\n\n    return RSC_CANDIDATE;\n}\n\nstatic size_t virtio_net_rsc_receive6(void *opq, NetClientState *nc,\n                                      const uint8_t *buf, size_t size)\n{\n    int32_t ret;\n    uint16_t hdr_len;\n    VirtioNetRscChain *chain;\n    VirtioNetRscUnit unit;\n\n    chain = (VirtioNetRscChain *)opq;\n    hdr_len = ((VirtIONet *)(chain->n))->guest_hdr_len;\n\n    if (size < (hdr_len + sizeof(struct eth_header) + sizeof(struct ip6_header)\n        + sizeof(tcp_header))) {\n        return virtio_net_do_receive(nc, buf, size);\n    }\n\n    virtio_net_rsc_extract_unit6(chain, buf, &unit);\n    if (RSC_CANDIDATE != virtio_net_rsc_sanity_check6(chain,\n                                                 unit.ip, buf, size)) {\n        return virtio_net_do_receive(nc, buf, size);\n    }\n\n    ret = virtio_net_rsc_tcp_ctrl_check(chain, unit.tcp);\n    if (ret == RSC_BYPASS) {\n        return virtio_net_do_receive(nc, buf, size);\n    } else if (ret == RSC_FINAL) {\n        return virtio_net_rsc_drain_flow(chain, nc, buf, size,\n                ((hdr_len + sizeof(struct eth_header)) + 8),\n                VIRTIO_NET_IP6_ADDR_SIZE,\n                hdr_len + sizeof(struct eth_header)\n                + sizeof(struct ip6_header));\n    }\n\n    return virtio_net_rsc_do_coalesce(chain, nc, buf, size, &unit);\n}\n\nstatic VirtioNetRscChain *virtio_net_rsc_lookup_chain(VirtIONet *n,\n                                                      NetClientState *nc,\n                                                      uint16_t proto)\n{\n    VirtioNetRscChain *chain;\n\n    if ((proto != (uint16_t)ETH_P_IP) && (proto != (uint16_t)ETH_P_IPV6)) {\n        return NULL;\n    }\n\n    QTAILQ_FOREACH(chain, &n->rsc_chains, next) {\n        if (chain->proto == proto) {\n            return chain;\n        }\n    }\n\n    chain = g_malloc(sizeof(*chain));\n    chain->n = n;\n    chain->proto = proto;\n    if (proto == (uint16_t)ETH_P_IP) {\n        chain->max_payload = VIRTIO_NET_MAX_IP4_PAYLOAD;\n        chain->gso_type = VIRTIO_NET_HDR_GSO_TCPV4;\n    } else {\n        chain->max_payload = VIRTIO_NET_MAX_IP6_PAYLOAD;\n        chain->gso_type = VIRTIO_NET_HDR_GSO_TCPV6;\n    }\n    chain->drain_timer = timer_new_ns(QEMU_CLOCK_HOST,\n                                      virtio_net_rsc_purge, chain);\n    memset(&chain->stat, 0, sizeof(chain->stat));\n\n    QTAILQ_INIT(&chain->buffers);\n    QTAILQ_INSERT_TAIL(&n->rsc_chains, chain, next);\n\n    return chain;\n}\n\nstatic ssize_t virtio_net_rsc_receive(NetClientState *nc,\n                                      const uint8_t *buf,\n                                      size_t size)\n{\n    uint16_t proto;\n    VirtioNetRscChain *chain;\n    struct eth_header *eth;\n    VirtIONet *n;\n\n    n = qemu_get_nic_opaque(nc);\n    if (size < (n->host_hdr_len + sizeof(struct eth_header))) {\n        return virtio_net_do_receive(nc, buf, size);\n    }\n\n    eth = (struct eth_header *)(buf + n->guest_hdr_len);\n    proto = htons(eth->h_proto);\n\n    chain = virtio_net_rsc_lookup_chain(n, nc, proto);\n    if (chain) {\n        chain->stat.received++;\n        if (proto == (uint16_t)ETH_P_IP && n->rsc4_enabled) {\n            return virtio_net_rsc_receive4(chain, nc, buf, size);\n        } else if (proto == (uint16_t)ETH_P_IPV6 && n->rsc6_enabled) {\n            return virtio_net_rsc_receive6(chain, nc, buf, size);\n        }\n    }\n    return virtio_net_do_receive(nc, buf, size);\n}\n\nstatic ssize_t virtio_net_receive(NetClientState *nc, const uint8_t *buf,\n                                  size_t size)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    if ((n->rsc4_enabled || n->rsc6_enabled)) {\n        return virtio_net_rsc_receive(nc, buf, size);\n    } else {\n        return virtio_net_do_receive(nc, buf, size);\n    }\n}\n\nstatic int32_t virtio_net_flush_tx(VirtIONetQueue *q);\n\nstatic void virtio_net_tx_complete(NetClientState *nc, ssize_t len)\n{\n    VirtIONet *n = qemu_get_nic_opaque(nc);\n    VirtIONetQueue *q = virtio_net_get_subqueue(nc);\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n\n    virtqueue_push(q->tx_vq, q->async_tx.elem, 0);\n    virtio_notify(vdev, q->tx_vq);\n\n    g_free(q->async_tx.elem);\n    q->async_tx.elem = NULL;\n\n    virtio_queue_set_notification(q->tx_vq, 1);\n    virtio_net_flush_tx(q);\n}\n\n/* TX */\nstatic int32_t virtio_net_flush_tx(VirtIONetQueue *q)\n{\n    VirtIONet *n = q->n;\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    VirtQueueElement *elem;\n    int32_t num_packets = 0;\n    int queue_index = vq2q(virtio_get_queue_index(q->tx_vq));\n    if (!(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK)) {\n        return num_packets;\n    }\n\n    if (q->async_tx.elem) {\n        virtio_queue_set_notification(q->tx_vq, 0);\n        return num_packets;\n    }\n\n    for (;;) {\n        ssize_t ret;\n        unsigned int out_num;\n        struct iovec sg[VIRTQUEUE_MAX_SIZE], sg2[VIRTQUEUE_MAX_SIZE + 1], *out_sg;\n        struct virtio_net_hdr_mrg_rxbuf mhdr;\n\n        elem = virtqueue_pop(q->tx_vq, sizeof(VirtQueueElement));\n        if (!elem) {\n            break;\n        }\n\n        out_num = elem->out_num;\n        out_sg = elem->out_sg;\n        if (out_num < 1) {\n            virtio_error(vdev, \"virtio-net header not in first element\");\n            virtqueue_detach_element(q->tx_vq, elem, 0);\n            g_free(elem);\n            return -EINVAL;\n        }\n\n        if (n->has_vnet_hdr) {\n            if (iov_to_buf(out_sg, out_num, 0, &mhdr, n->guest_hdr_len) <\n                n->guest_hdr_len) {\n                virtio_error(vdev, \"virtio-net header incorrect\");\n                virtqueue_detach_element(q->tx_vq, elem, 0);\n                g_free(elem);\n                return -EINVAL;\n            }\n            if (n->needs_vnet_hdr_swap) {\n                virtio_net_hdr_swap(vdev, (void *) &mhdr);\n                sg2[0].iov_base = &mhdr;\n                sg2[0].iov_len = n->guest_hdr_len;\n                out_num = iov_copy(&sg2[1], ARRAY_SIZE(sg2) - 1,\n                                   out_sg, out_num,\n                                   n->guest_hdr_len, -1);\n                if (out_num == VIRTQUEUE_MAX_SIZE) {\n                    goto drop;\n                }\n                out_num += 1;\n                out_sg = sg2;\n            }\n        }\n        /*\n         * If host wants to see the guest header as is, we can\n         * pass it on unchanged. Otherwise, copy just the parts\n         * that host is interested in.\n         */\n        assert(n->host_hdr_len <= n->guest_hdr_len);\n        if (n->host_hdr_len != n->guest_hdr_len) {\n            unsigned sg_num = iov_copy(sg, ARRAY_SIZE(sg),\n                                       out_sg, out_num,\n                                       0, n->host_hdr_len);\n            sg_num += iov_copy(sg + sg_num, ARRAY_SIZE(sg) - sg_num,\n                             out_sg, out_num,\n                             n->guest_hdr_len, -1);\n            out_num = sg_num;\n            out_sg = sg;\n        }\n\n        ret = qemu_sendv_packet_async(qemu_get_subqueue(n->nic, queue_index),\n                                      out_sg, out_num, virtio_net_tx_complete);\n        if (ret == 0) {\n            virtio_queue_set_notification(q->tx_vq, 0);\n            q->async_tx.elem = elem;\n            return -EBUSY;\n        }\n\ndrop:\n        virtqueue_push(q->tx_vq, elem, 0);\n        virtio_notify(vdev, q->tx_vq);\n        g_free(elem);\n\n        if (++num_packets >= n->tx_burst) {\n            break;\n        }\n    }\n    return num_packets;\n}\n\nstatic void virtio_net_handle_tx_timer(VirtIODevice *vdev, VirtQueue *vq)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    VirtIONetQueue *q = &n->vqs[vq2q(virtio_get_queue_index(vq))];\n\n    if (unlikely((n->status & VIRTIO_NET_S_LINK_UP) == 0)) {\n        virtio_net_drop_tx_queue_data(vdev, vq);\n        return;\n    }\n\n    /* This happens when device was stopped but VCPU wasn't. */\n    if (!vdev->vm_running) {\n        q->tx_waiting = 1;\n        return;\n    }\n\n    if (q->tx_waiting) {\n        virtio_queue_set_notification(vq, 1);\n        timer_del(q->tx_timer);\n        q->tx_waiting = 0;\n        if (virtio_net_flush_tx(q) == -EINVAL) {\n            return;\n        }\n    } else {\n        timer_mod(q->tx_timer,\n                       qemu_clock_get_ns(QEMU_CLOCK_VIRTUAL) + n->tx_timeout);\n        q->tx_waiting = 1;\n        virtio_queue_set_notification(vq, 0);\n    }\n}\n\nstatic void virtio_net_handle_tx_bh(VirtIODevice *vdev, VirtQueue *vq)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    VirtIONetQueue *q = &n->vqs[vq2q(virtio_get_queue_index(vq))];\n\n    if (unlikely((n->status & VIRTIO_NET_S_LINK_UP) == 0)) {\n        virtio_net_drop_tx_queue_data(vdev, vq);\n        return;\n    }\n\n    if (unlikely(q->tx_waiting)) {\n        return;\n    }\n    q->tx_waiting = 1;\n    /* This happens when device was stopped but VCPU wasn't. */\n    if (!vdev->vm_running) {\n        return;\n    }\n    virtio_queue_set_notification(vq, 0);\n    qemu_bh_schedule(q->tx_bh);\n}\n\nstatic void virtio_net_tx_timer(void *opaque)\n{\n    VirtIONetQueue *q = opaque;\n    VirtIONet *n = q->n;\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    /* This happens when device was stopped but BH wasn't. */\n    if (!vdev->vm_running) {\n        /* Make sure tx waiting is set, so we'll run when restarted. */\n        assert(q->tx_waiting);\n        return;\n    }\n\n    q->tx_waiting = 0;\n\n    /* Just in case the driver is not ready on more */\n    if (!(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK)) {\n        return;\n    }\n\n    virtio_queue_set_notification(q->tx_vq, 1);\n    virtio_net_flush_tx(q);\n}\n\nstatic void virtio_net_tx_bh(void *opaque)\n{\n    VirtIONetQueue *q = opaque;\n    VirtIONet *n = q->n;\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    int32_t ret;\n\n    /* This happens when device was stopped but BH wasn't. */\n    if (!vdev->vm_running) {\n        /* Make sure tx waiting is set, so we'll run when restarted. */\n        assert(q->tx_waiting);\n        return;\n    }\n\n    q->tx_waiting = 0;\n\n    /* Just in case the driver is not ready on more */\n    if (unlikely(!(vdev->status & VIRTIO_CONFIG_S_DRIVER_OK))) {\n        return;\n    }\n\n    ret = virtio_net_flush_tx(q);\n    if (ret == -EBUSY || ret == -EINVAL) {\n        return; /* Notification re-enable handled by tx_complete or device\n                 * broken */\n    }\n\n    /* If we flush a full burst of packets, assume there are\n     * more coming and immediately reschedule */\n    if (ret >= n->tx_burst) {\n        qemu_bh_schedule(q->tx_bh);\n        q->tx_waiting = 1;\n        return;\n    }\n\n    /* If less than a full burst, re-enable notification and flush\n     * anything that may have come in while we weren't looking.  If\n     * we find something, assume the guest is still active and reschedule */\n    virtio_queue_set_notification(q->tx_vq, 1);\n    ret = virtio_net_flush_tx(q);\n    if (ret == -EINVAL) {\n        return;\n    } else if (ret > 0) {\n        virtio_queue_set_notification(q->tx_vq, 0);\n        qemu_bh_schedule(q->tx_bh);\n        q->tx_waiting = 1;\n    }\n}\n\nstatic void virtio_net_add_queue(VirtIONet *n, int index)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n\n    n->vqs[index].rx_vq = virtio_add_queue(vdev, n->net_conf.rx_queue_size,\n                                           virtio_net_handle_rx);\n\n    if (n->net_conf.tx && !strcmp(n->net_conf.tx, \"timer\")) {\n        n->vqs[index].tx_vq =\n            virtio_add_queue(vdev, n->net_conf.tx_queue_size,\n                             virtio_net_handle_tx_timer);\n        n->vqs[index].tx_timer = timer_new_ns(QEMU_CLOCK_VIRTUAL,\n                                              virtio_net_tx_timer,\n                                              &n->vqs[index]);\n    } else {\n        n->vqs[index].tx_vq =\n            virtio_add_queue(vdev, n->net_conf.tx_queue_size,\n                             virtio_net_handle_tx_bh);\n        n->vqs[index].tx_bh = qemu_bh_new(virtio_net_tx_bh, &n->vqs[index]);\n    }\n\n    n->vqs[index].tx_waiting = 0;\n    n->vqs[index].n = n;\n}\n\nstatic void virtio_net_del_queue(VirtIONet *n, int index)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    VirtIONetQueue *q = &n->vqs[index];\n    NetClientState *nc = qemu_get_subqueue(n->nic, index);\n\n    qemu_purge_queued_packets(nc);\n\n    virtio_del_queue(vdev, index * 2);\n    if (q->tx_timer) {\n        timer_free(q->tx_timer);\n        q->tx_timer = NULL;\n    } else {\n        qemu_bh_delete(q->tx_bh);\n        q->tx_bh = NULL;\n    }\n    q->tx_waiting = 0;\n    virtio_del_queue(vdev, index * 2 + 1);\n}\n\nstatic void virtio_net_change_num_queues(VirtIONet *n, int new_max_queues)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    int old_num_queues = virtio_get_num_queues(vdev);\n    int new_num_queues = new_max_queues * 2 + 1;\n    int i;\n\n    assert(old_num_queues >= 3);\n    assert(old_num_queues % 2 == 1);\n\n    if (old_num_queues == new_num_queues) {\n        return;\n    }\n\n    /*\n     * We always need to remove and add ctrl vq if\n     * old_num_queues != new_num_queues. Remove ctrl_vq first,\n     * and then we only enter one of the following two loops.\n     */\n    virtio_del_queue(vdev, old_num_queues - 1);\n\n    for (i = new_num_queues - 1; i < old_num_queues - 1; i += 2) {\n        /* new_num_queues < old_num_queues */\n        virtio_net_del_queue(n, i / 2);\n    }\n\n    for (i = old_num_queues - 1; i < new_num_queues - 1; i += 2) {\n        /* new_num_queues > old_num_queues */\n        virtio_net_add_queue(n, i / 2);\n    }\n\n    /* add ctrl_vq last */\n    n->ctrl_vq = virtio_add_queue(vdev, 64, virtio_net_handle_ctrl);\n}\n\nstatic void virtio_net_set_multiqueue(VirtIONet *n, int multiqueue)\n{\n    int max = multiqueue ? n->max_queues : 1;\n\n    n->multiqueue = multiqueue;\n    virtio_net_change_num_queues(n, max);\n\n    virtio_net_set_queues(n);\n}\n\nstatic int virtio_net_post_load_device(void *opaque, int version_id)\n{\n    VirtIONet *n = opaque;\n    VirtIODevice *vdev = VIRTIO_DEVICE(n);\n    int i, link_down;\n\n    trace_virtio_net_post_load_device();\n    virtio_net_set_mrg_rx_bufs(n, n->mergeable_rx_bufs,\n                               virtio_vdev_has_feature(vdev,\n                                                       VIRTIO_F_VERSION_1),\n                               virtio_vdev_has_feature(vdev,\n                                                       VIRTIO_NET_F_HASH_REPORT));\n\n    /* MAC_TABLE_ENTRIES may be different from the saved image */\n    if (n->mac_table.in_use > MAC_TABLE_ENTRIES) {\n        n->mac_table.in_use = 0;\n    }\n\n    if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_GUEST_OFFLOADS)) {\n        n->curr_guest_offloads = virtio_net_supported_guest_offloads(n);\n    }\n\n    /*\n     * curr_guest_offloads will be later overwritten by the\n     * virtio_set_features_nocheck call done from the virtio_load.\n     * Here we make sure it is preserved and restored accordingly\n     * in the virtio_net_post_load_virtio callback.\n     */\n    n->saved_guest_offloads = n->curr_guest_offloads;\n\n    virtio_net_set_queues(n);\n\n    /* Find the first multicast entry in the saved MAC filter */\n    for (i = 0; i < n->mac_table.in_use; i++) {\n        if (n->mac_table.macs[i * ETH_ALEN] & 1) {\n            break;\n        }\n    }\n    n->mac_table.first_multi = i;\n\n    /* nc.link_down can't be migrated, so infer link_down according\n     * to link status bit in n->status */\n    link_down = (n->status & VIRTIO_NET_S_LINK_UP) == 0;\n    for (i = 0; i < n->max_queues; i++) {\n        qemu_get_subqueue(n->nic, i)->link_down = link_down;\n    }\n\n    if (virtio_vdev_has_feature(vdev, VIRTIO_NET_F_GUEST_ANNOUNCE) &&\n        virtio_vdev_has_feature(vdev, VIRTIO_NET_F_CTRL_VQ)) {\n        qemu_announce_timer_reset(&n->announce_timer, migrate_announce_params(),\n                                  QEMU_CLOCK_VIRTUAL,\n                                  virtio_net_announce_timer, n);\n        if (n->announce_timer.round) {\n            timer_mod(n->announce_timer.tm,\n                      qemu_clock_get_ms(n->announce_timer.type));\n        } else {\n            qemu_announce_timer_del(&n->announce_timer, false);\n        }\n    }\n\n    if (n->rss_data.enabled) {\n        n->rss_data.enabled_software_rss = n->rss_data.populate_hash;\n        if (!n->rss_data.populate_hash) {\n            if (!virtio_net_attach_epbf_rss(n)) {\n                if (get_vhost_net(qemu_get_queue(n->nic)->peer)) {\n                    warn_report(\"Can't post-load eBPF RSS for vhost\");\n                } else {\n                    warn_report(\"Can't post-load eBPF RSS - \"\n                                \"fallback to software RSS\");\n                    n->rss_data.enabled_software_rss = true;\n                }\n            }\n        }\n\n        trace_virtio_net_rss_enable(n->rss_data.hash_types,\n                                    n->rss_data.indirections_len,\n                                    sizeof(n->rss_data.key));\n    } else {\n        trace_virtio_net_rss_disable();\n    }\n    return 0;\n}\n\nstatic int virtio_net_post_load_virtio(VirtIODevice *vdev)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    /*\n     * The actual needed state is now in saved_guest_offloads,\n     * see virtio_net_post_load_device for detail.\n     * Restore it back and apply the desired offloads.\n     */\n    n->curr_guest_offloads = n->saved_guest_offloads;\n    if (peer_has_vnet_hdr(n)) {\n        virtio_net_apply_guest_offloads(n);\n    }\n\n    return 0;\n}\n\n/* tx_waiting field of a VirtIONetQueue */\nstatic const VMStateDescription vmstate_virtio_net_queue_tx_waiting = {\n    .name = \"virtio-net-queue-tx_waiting\",\n    .fields = (VMStateField[]) {\n        VMSTATE_UINT32(tx_waiting, VirtIONetQueue),\n        VMSTATE_END_OF_LIST()\n   },\n};\n\nstatic bool max_queues_gt_1(void *opaque, int version_id)\n{\n    return VIRTIO_NET(opaque)->max_queues > 1;\n}\n\nstatic bool has_ctrl_guest_offloads(void *opaque, int version_id)\n{\n    return virtio_vdev_has_feature(VIRTIO_DEVICE(opaque),\n                                   VIRTIO_NET_F_CTRL_GUEST_OFFLOADS);\n}\n\nstatic bool mac_table_fits(void *opaque, int version_id)\n{\n    return VIRTIO_NET(opaque)->mac_table.in_use <= MAC_TABLE_ENTRIES;\n}\n\nstatic bool mac_table_doesnt_fit(void *opaque, int version_id)\n{\n    return !mac_table_fits(opaque, version_id);\n}\n\n/* This temporary type is shared by all the WITH_TMP methods\n * although only some fields are used by each.\n */\nstruct VirtIONetMigTmp {\n    VirtIONet      *parent;\n    VirtIONetQueue *vqs_1;\n    uint16_t        curr_queues_1;\n    uint8_t         has_ufo;\n    uint32_t        has_vnet_hdr;\n};\n\n/* The 2nd and subsequent tx_waiting flags are loaded later than\n * the 1st entry in the queues and only if there's more than one\n * entry.  We use the tmp mechanism to calculate a temporary\n * pointer and count and also validate the count.\n */\n\nstatic int virtio_net_tx_waiting_pre_save(void *opaque)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    tmp->vqs_1 = tmp->parent->vqs + 1;\n    tmp->curr_queues_1 = tmp->parent->curr_queues - 1;\n    if (tmp->parent->curr_queues == 0) {\n        tmp->curr_queues_1 = 0;\n    }\n\n    return 0;\n}\n\nstatic int virtio_net_tx_waiting_pre_load(void *opaque)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    /* Reuse the pointer setup from save */\n    virtio_net_tx_waiting_pre_save(opaque);\n\n    if (tmp->parent->curr_queues > tmp->parent->max_queues) {\n        error_report(\"virtio-net: curr_queues %x > max_queues %x\",\n            tmp->parent->curr_queues, tmp->parent->max_queues);\n\n        return -EINVAL;\n    }\n\n    return 0; /* all good */\n}\n\nstatic const VMStateDescription vmstate_virtio_net_tx_waiting = {\n    .name      = \"virtio-net-tx_waiting\",\n    .pre_load  = virtio_net_tx_waiting_pre_load,\n    .pre_save  = virtio_net_tx_waiting_pre_save,\n    .fields    = (VMStateField[]) {\n        VMSTATE_STRUCT_VARRAY_POINTER_UINT16(vqs_1, struct VirtIONetMigTmp,\n                                     curr_queues_1,\n                                     vmstate_virtio_net_queue_tx_waiting,\n                                     struct VirtIONetQueue),\n        VMSTATE_END_OF_LIST()\n    },\n};\n\n/* the 'has_ufo' flag is just tested; if the incoming stream has the\n * flag set we need to check that we have it\n */\nstatic int virtio_net_ufo_post_load(void *opaque, int version_id)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    if (tmp->has_ufo && !peer_has_ufo(tmp->parent)) {\n        error_report(\"virtio-net: saved image requires TUN_F_UFO support\");\n        return -EINVAL;\n    }\n\n    return 0;\n}\n\nstatic int virtio_net_ufo_pre_save(void *opaque)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    tmp->has_ufo = tmp->parent->has_ufo;\n\n    return 0;\n}\n\nstatic const VMStateDescription vmstate_virtio_net_has_ufo = {\n    .name      = \"virtio-net-ufo\",\n    .post_load = virtio_net_ufo_post_load,\n    .pre_save  = virtio_net_ufo_pre_save,\n    .fields    = (VMStateField[]) {\n        VMSTATE_UINT8(has_ufo, struct VirtIONetMigTmp),\n        VMSTATE_END_OF_LIST()\n    },\n};\n\n/* the 'has_vnet_hdr' flag is just tested; if the incoming stream has the\n * flag set we need to check that we have it\n */\nstatic int virtio_net_vnet_post_load(void *opaque, int version_id)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    if (tmp->has_vnet_hdr && !peer_has_vnet_hdr(tmp->parent)) {\n        error_report(\"virtio-net: saved image requires vnet_hdr=on\");\n        return -EINVAL;\n    }\n\n    return 0;\n}\n\nstatic int virtio_net_vnet_pre_save(void *opaque)\n{\n    struct VirtIONetMigTmp *tmp = opaque;\n\n    tmp->has_vnet_hdr = tmp->parent->has_vnet_hdr;\n\n    return 0;\n}\n\nstatic const VMStateDescription vmstate_virtio_net_has_vnet = {\n    .name      = \"virtio-net-vnet\",\n    .post_load = virtio_net_vnet_post_load,\n    .pre_save  = virtio_net_vnet_pre_save,\n    .fields    = (VMStateField[]) {\n        VMSTATE_UINT32(has_vnet_hdr, struct VirtIONetMigTmp),\n        VMSTATE_END_OF_LIST()\n    },\n};\n\nstatic bool virtio_net_rss_needed(void *opaque)\n{\n    return VIRTIO_NET(opaque)->rss_data.enabled;\n}\n\nstatic const VMStateDescription vmstate_virtio_net_rss = {\n    .name      = \"virtio-net-device/rss\",\n    .version_id = 1,\n    .minimum_version_id = 1,\n    .needed = virtio_net_rss_needed,\n    .fields = (VMStateField[]) {\n        VMSTATE_BOOL(rss_data.enabled, VirtIONet),\n        VMSTATE_BOOL(rss_data.redirect, VirtIONet),\n        VMSTATE_BOOL(rss_data.populate_hash, VirtIONet),\n        VMSTATE_UINT32(rss_data.hash_types, VirtIONet),\n        VMSTATE_UINT16(rss_data.indirections_len, VirtIONet),\n        VMSTATE_UINT16(rss_data.default_queue, VirtIONet),\n        VMSTATE_UINT8_ARRAY(rss_data.key, VirtIONet,\n                            VIRTIO_NET_RSS_MAX_KEY_SIZE),\n        VMSTATE_VARRAY_UINT16_ALLOC(rss_data.indirections_table, VirtIONet,\n                                    rss_data.indirections_len, 0,\n                                    vmstate_info_uint16, uint16_t),\n        VMSTATE_END_OF_LIST()\n    },\n};\n\nstatic const VMStateDescription vmstate_virtio_net_device = {\n    .name = \"virtio-net-device\",\n    .version_id = VIRTIO_NET_VM_VERSION,\n    .minimum_version_id = VIRTIO_NET_VM_VERSION,\n    .post_load = virtio_net_post_load_device,\n    .fields = (VMStateField[]) {\n        VMSTATE_UINT8_ARRAY(mac, VirtIONet, ETH_ALEN),\n        VMSTATE_STRUCT_POINTER(vqs, VirtIONet,\n                               vmstate_virtio_net_queue_tx_waiting,\n                               VirtIONetQueue),\n        VMSTATE_UINT32(mergeable_rx_bufs, VirtIONet),\n        VMSTATE_UINT16(status, VirtIONet),\n        VMSTATE_UINT8(promisc, VirtIONet),\n        VMSTATE_UINT8(allmulti, VirtIONet),\n        VMSTATE_UINT32(mac_table.in_use, VirtIONet),\n\n        /* Guarded pair: If it fits we load it, else we throw it away\n         * - can happen if source has a larger MAC table.; post-load\n         *  sets flags in this case.\n         */\n        VMSTATE_VBUFFER_MULTIPLY(mac_table.macs, VirtIONet,\n                                0, mac_table_fits, mac_table.in_use,\n                                 ETH_ALEN),\n        VMSTATE_UNUSED_VARRAY_UINT32(VirtIONet, mac_table_doesnt_fit, 0,\n                                     mac_table.in_use, ETH_ALEN),\n\n        /* Note: This is an array of uint32's that's always been saved as a\n         * buffer; hold onto your endiannesses; it's actually used as a bitmap\n         * but based on the uint.\n         */\n        VMSTATE_BUFFER_POINTER_UNSAFE(vlans, VirtIONet, 0, MAX_VLAN >> 3),\n        VMSTATE_WITH_TMP(VirtIONet, struct VirtIONetMigTmp,\n                         vmstate_virtio_net_has_vnet),\n        VMSTATE_UINT8(mac_table.multi_overflow, VirtIONet),\n        VMSTATE_UINT8(mac_table.uni_overflow, VirtIONet),\n        VMSTATE_UINT8(alluni, VirtIONet),\n        VMSTATE_UINT8(nomulti, VirtIONet),\n        VMSTATE_UINT8(nouni, VirtIONet),\n        VMSTATE_UINT8(nobcast, VirtIONet),\n        VMSTATE_WITH_TMP(VirtIONet, struct VirtIONetMigTmp,\n                         vmstate_virtio_net_has_ufo),\n        VMSTATE_SINGLE_TEST(max_queues, VirtIONet, max_queues_gt_1, 0,\n                            vmstate_info_uint16_equal, uint16_t),\n        VMSTATE_UINT16_TEST(curr_queues, VirtIONet, max_queues_gt_1),\n        VMSTATE_WITH_TMP(VirtIONet, struct VirtIONetMigTmp,\n                         vmstate_virtio_net_tx_waiting),\n        VMSTATE_UINT64_TEST(curr_guest_offloads, VirtIONet,\n                            has_ctrl_guest_offloads),\n        VMSTATE_END_OF_LIST()\n   },\n    .subsections = (const VMStateDescription * []) {\n        &vmstate_virtio_net_rss,\n        NULL\n    }\n};\n\nstatic NetClientInfo net_virtio_info = {\n    .type = NET_CLIENT_DRIVER_NIC,\n    .size = sizeof(NICState),\n    .can_receive = virtio_net_can_receive,\n    .receive = virtio_net_receive,\n    .link_status_changed = virtio_net_set_link_status,\n    .query_rx_filter = virtio_net_query_rxfilter,\n    .announce = virtio_net_announce,\n};\n\nstatic bool virtio_net_guest_notifier_pending(VirtIODevice *vdev, int idx)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    NetClientState *nc = qemu_get_subqueue(n->nic, vq2q(idx));\n    assert(n->vhost_started);\n    return vhost_net_virtqueue_pending(get_vhost_net(nc->peer), idx);\n}\n\nstatic void virtio_net_guest_notifier_mask(VirtIODevice *vdev, int idx,\n                                           bool mask)\n{\n    VirtIONet *n = VIRTIO_NET(vdev);\n    NetClientState *nc = qemu_get_subqueue(n->nic, vq2q(idx));\n    assert(n->vhost_started);\n    vhost_net_virtqueue_mask(get_vhost_net(nc->peer),\n                             vdev, idx, mask);\n}\n\nstatic void virtio_net_set_config_size(VirtIONet *n, uint64_t host_features)\n{\n    virtio_add_feature(&host_features, VIRTIO_NET_F_MAC);\n\n    n->config_size = virtio_feature_get_config_size(feature_sizes,\n                                                    host_features);\n}\n\nvoid virtio_net_set_netclient_name(VirtIONet *n, const char *name,\n                                   const char *type)\n{\n    /*\n     * The name can be NULL, the netclient name will be type.x.\n     */\n    assert(type != NULL);\n\n    g_free(n->netclient_name);\n    g_free(n->netclient_type);\n    n->netclient_name = g_strdup(name);\n    n->netclient_type = g_strdup(type);\n}\n\nstatic bool failover_unplug_primary(VirtIONet *n, DeviceState *dev)\n{\n    HotplugHandler *hotplug_ctrl;\n    PCIDevice *pci_dev;\n    Error *err = NULL;\n\n    hotplug_ctrl = qdev_get_hotplug_handler(dev);\n    if (hotplug_ctrl) {\n        pci_dev = PCI_DEVICE(dev);\n        pci_dev->partially_hotplugged = true;\n        hotplug_handler_unplug_request(hotplug_ctrl, dev, &err);\n        if (err) {\n            error_report_err(err);\n            return false;\n        }\n    } else {\n        return false;\n    }\n    return true;\n}\n\nstatic bool failover_replug_primary(VirtIONet *n, DeviceState *dev,\n                                    Error **errp)\n{\n    Error *err = NULL;\n    HotplugHandler *hotplug_ctrl;\n    PCIDevice *pdev = PCI_DEVICE(dev);\n    BusState *primary_bus;\n\n    if (!pdev->partially_hotplugged) {\n        return true;\n    }\n    primary_bus = dev->parent_bus;\n    if (!primary_bus) {\n        error_setg(errp, \"virtio_net: couldn't find primary bus\");\n        return false;\n    }\n    qdev_set_parent_bus(dev, primary_bus, &error_abort);\n    qatomic_set(&n->failover_primary_hidden, false);\n    hotplug_ctrl = qdev_get_hotplug_handler(dev);\n    if (hotplug_ctrl) {\n        hotplug_handler_pre_plug(hotplug_ctrl, dev, &err);\n        if (err) {\n            goto out;\n        }\n        hotplug_handler_plug(hotplug_ctrl, dev, &err);\n    }\n    pdev->partially_hotplugged = false;\n\nout:\n    error_propagate(errp, err);\n    return !err;\n}\n\nstatic void virtio_net_handle_migration_primary(VirtIONet *n, MigrationState *s)\n{\n    bool should_be_hidden;\n    Error *err = NULL;\n    DeviceState *dev = failover_find_primary_device(n);\n\n    if (!dev) {\n        return;\n    }\n\n    should_be_hidden = qatomic_read(&n->failover_primary_hidden);\n\n    if (migration_in_setup(s) && !should_be_hidden) {\n        if (failover_unplug_primary(n, dev)) {\n            vmstate_unregister(VMSTATE_IF(dev), qdev_get_vmsd(dev), dev);\n            qapi_event_send_unplug_primary(dev->id);\n            qatomic_set(&n->failover_primary_hidden, true);\n        } else {\n            warn_report(\"couldn't unplug primary device\");\n        }\n    } else if (migration_has_failed(s)) {\n        /* We already unplugged the device let's plug it back */\n        if (!failover_replug_primary(n, dev, &err)) {\n            if (err) {\n                error_report_err(err);\n            }\n        }\n    }\n}\n\nstatic void virtio_net_migration_state_notifier(Notifier *notifier, void *data)\n{\n    MigrationState *s = data;\n    VirtIONet *n = container_of(notifier, VirtIONet, migration_state);\n    virtio_net_handle_migration_primary(n, s);\n}\n\nstatic bool failover_hide_primary_device(DeviceListener *listener,\n                                         QemuOpts *device_opts)\n{\n    VirtIONet *n = container_of(listener, VirtIONet, primary_listener);\n    const char *standby_id;\n\n    if (!device_opts) {\n        return false;\n    }\n    standby_id = qemu_opt_get(device_opts, \"failover_pair_id\");\n    if (g_strcmp0(standby_id, n->netclient_name) != 0) {\n        return false;\n    }\n\n    /* failover_primary_hidden is set during feature negotiation */\n    return qatomic_read(&n->failover_primary_hidden);\n}\n\nstatic void virtio_net_device_realize(DeviceState *dev, Error **errp)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(dev);\n    VirtIONet *n = VIRTIO_NET(dev);\n    NetClientState *nc;\n    int i;\n\n    if (n->net_conf.mtu) {\n        n->host_features |= (1ULL << VIRTIO_NET_F_MTU);\n    }\n\n    if (n->net_conf.duplex_str) {\n        if (strncmp(n->net_conf.duplex_str, \"half\", 5) == 0) {\n            n->net_conf.duplex = DUPLEX_HALF;\n        } else if (strncmp(n->net_conf.duplex_str, \"full\", 5) == 0) {\n            n->net_conf.duplex = DUPLEX_FULL;\n        } else {\n            error_setg(errp, \"'duplex' must be 'half' or 'full'\");\n            return;\n        }\n        n->host_features |= (1ULL << VIRTIO_NET_F_SPEED_DUPLEX);\n    } else {\n        n->net_conf.duplex = DUPLEX_UNKNOWN;\n    }\n\n    if (n->net_conf.speed < SPEED_UNKNOWN) {\n        error_setg(errp, \"'speed' must be between 0 and INT_MAX\");\n        return;\n    }\n    if (n->net_conf.speed >= 0) {\n        n->host_features |= (1ULL << VIRTIO_NET_F_SPEED_DUPLEX);\n    }\n\n    if (n->failover) {\n        n->primary_listener.hide_device = failover_hide_primary_device;\n        qatomic_set(&n->failover_primary_hidden, true);\n        device_listener_register(&n->primary_listener);\n        n->migration_state.notify = virtio_net_migration_state_notifier;\n        add_migration_state_change_notifier(&n->migration_state);\n        n->host_features |= (1ULL << VIRTIO_NET_F_STANDBY);\n    }\n\n    virtio_net_set_config_size(n, n->host_features);\n    virtio_init(vdev, \"virtio-net\", VIRTIO_ID_NET, n->config_size);\n\n    /*\n     * We set a lower limit on RX queue size to what it always was.\n     * Guests that want a smaller ring can always resize it without\n     * help from us (using virtio 1 and up).\n     */\n    if (n->net_conf.rx_queue_size < VIRTIO_NET_RX_QUEUE_MIN_SIZE ||\n        n->net_conf.rx_queue_size > VIRTQUEUE_MAX_SIZE ||\n        !is_power_of_2(n->net_conf.rx_queue_size)) {\n        error_setg(errp, \"Invalid rx_queue_size (= %\" PRIu16 \"), \"\n                   \"must be a power of 2 between %d and %d.\",\n                   n->net_conf.rx_queue_size, VIRTIO_NET_RX_QUEUE_MIN_SIZE,\n                   VIRTQUEUE_MAX_SIZE);\n        virtio_cleanup(vdev);\n        return;\n    }\n\n    if (n->net_conf.tx_queue_size < VIRTIO_NET_TX_QUEUE_MIN_SIZE ||\n        n->net_conf.tx_queue_size > VIRTQUEUE_MAX_SIZE ||\n        !is_power_of_2(n->net_conf.tx_queue_size)) {\n        error_setg(errp, \"Invalid tx_queue_size (= %\" PRIu16 \"), \"\n                   \"must be a power of 2 between %d and %d\",\n                   n->net_conf.tx_queue_size, VIRTIO_NET_TX_QUEUE_MIN_SIZE,\n                   VIRTQUEUE_MAX_SIZE);\n        virtio_cleanup(vdev);\n        return;\n    }\n\n    n->max_queues = MAX(n->nic_conf.peers.queues, 1);\n    if (n->max_queues * 2 + 1 > VIRTIO_QUEUE_MAX) {\n        error_setg(errp, \"Invalid number of queues (= %\" PRIu32 \"), \"\n                   \"must be a positive integer less than %d.\",\n                   n->max_queues, (VIRTIO_QUEUE_MAX - 1) / 2);\n        virtio_cleanup(vdev);\n        return;\n    }\n    n->vqs = g_malloc0(sizeof(VirtIONetQueue) * n->max_queues);\n    n->curr_queues = 1;\n    n->tx_timeout = n->net_conf.txtimer;\n\n    if (n->net_conf.tx && strcmp(n->net_conf.tx, \"timer\")\n                       && strcmp(n->net_conf.tx, \"bh\")) {\n        warn_report(\"virtio-net: \"\n                    \"Unknown option tx=%s, valid options: \\\"timer\\\" \\\"bh\\\"\",\n                    n->net_conf.tx);\n        error_printf(\"Defaulting to \\\"bh\\\"\");\n    }\n\n    n->net_conf.tx_queue_size = MIN(virtio_net_max_tx_queue_size(n),\n                                    n->net_conf.tx_queue_size);\n\n    for (i = 0; i < n->max_queues; i++) {\n        virtio_net_add_queue(n, i);\n    }\n\n    n->ctrl_vq = virtio_add_queue(vdev, 64, virtio_net_handle_ctrl);\n    qemu_macaddr_default_if_unset(&n->nic_conf.macaddr);\n    memcpy(&n->mac[0], &n->nic_conf.macaddr, sizeof(n->mac));\n    n->status = VIRTIO_NET_S_LINK_UP;\n    qemu_announce_timer_reset(&n->announce_timer, migrate_announce_params(),\n                              QEMU_CLOCK_VIRTUAL,\n                              virtio_net_announce_timer, n);\n    n->announce_timer.round = 0;\n\n    if (n->netclient_type) {\n        /*\n         * Happen when virtio_net_set_netclient_name has been called.\n         */\n        n->nic = qemu_new_nic(&net_virtio_info, &n->nic_conf,\n                              n->netclient_type, n->netclient_name, n);\n    } else {\n        n->nic = qemu_new_nic(&net_virtio_info, &n->nic_conf,\n                              object_get_typename(OBJECT(dev)), dev->id, n);\n    }\n\n    for (i = 0; i < n->max_queues; i++) {\n        n->nic->ncs[i].do_not_pad = true;\n    }\n\n    peer_test_vnet_hdr(n);\n    if (peer_has_vnet_hdr(n)) {\n        for (i = 0; i < n->max_queues; i++) {\n            qemu_using_vnet_hdr(qemu_get_subqueue(n->nic, i)->peer, true);\n        }\n        n->host_hdr_len = sizeof(struct virtio_net_hdr);\n    } else {\n        n->host_hdr_len = 0;\n    }\n\n    qemu_format_nic_info_str(qemu_get_queue(n->nic), n->nic_conf.macaddr.a);\n\n    n->vqs[0].tx_waiting = 0;\n    n->tx_burst = n->net_conf.txburst;\n    virtio_net_set_mrg_rx_bufs(n, 0, 0, 0);\n    n->promisc = 1; /* for compatibility */\n\n    n->mac_table.macs = g_malloc0(MAC_TABLE_ENTRIES * ETH_ALEN);\n\n    n->vlans = g_malloc0(MAX_VLAN >> 3);\n\n    nc = qemu_get_queue(n->nic);\n    nc->rxfilter_notify_enabled = 1;\n\n   if (nc->peer && nc->peer->info->type == NET_CLIENT_DRIVER_VHOST_VDPA) {\n        struct virtio_net_config netcfg = {};\n        memcpy(&netcfg.mac, &n->nic_conf.macaddr, ETH_ALEN);\n        vhost_net_set_config(get_vhost_net(nc->peer),\n            (uint8_t *)&netcfg, 0, ETH_ALEN, VHOST_SET_CONFIG_TYPE_MASTER);\n    }\n    QTAILQ_INIT(&n->rsc_chains);\n    n->qdev = dev;\n\n    net_rx_pkt_init(&n->rx_pkt, false);\n\n    if (virtio_has_feature(n->host_features, VIRTIO_NET_F_RSS)) {\n        virtio_net_load_ebpf(n);\n    }\n}\n\nstatic void virtio_net_device_unrealize(DeviceState *dev)\n{\n    VirtIODevice *vdev = VIRTIO_DEVICE(dev);\n    VirtIONet *n = VIRTIO_NET(dev);\n    int i, max_queues;\n\n    if (virtio_has_feature(n->host_features, VIRTIO_NET_F_RSS)) {\n        virtio_net_unload_ebpf(n);\n    }\n\n    /* This will stop vhost backend if appropriate. */\n    virtio_net_set_status(vdev, 0);\n\n    g_free(n->netclient_name);\n    n->netclient_name = NULL;\n    g_free(n->netclient_type);\n    n->netclient_type = NULL;\n\n    g_free(n->mac_table.macs);\n    g_free(n->vlans);\n\n    if (n->failover) {\n        device_listener_unregister(&n->primary_listener);\n        remove_migration_state_change_notifier(&n->migration_state);\n    }\n\n    max_queues = n->multiqueue ? n->max_queues : 1;\n    for (i = 0; i < max_queues; i++) {\n        virtio_net_del_queue(n, i);\n    }\n    /* delete also control vq */\n    virtio_del_queue(vdev, max_queues * 2);\n    qemu_announce_timer_del(&n->announce_timer, false);\n    g_free(n->vqs);\n    qemu_del_nic(n->nic);\n    virtio_net_rsc_cleanup(n);\n    g_free(n->rss_data.indirections_table);\n    net_rx_pkt_uninit(n->rx_pkt);\n    virtio_cleanup(vdev);\n}\n\nstatic void virtio_net_instance_init(Object *obj)\n{\n    VirtIONet *n = VIRTIO_NET(obj);\n\n    /*\n     * The default config_size is sizeof(struct virtio_net_config).\n     * Can be overriden with virtio_net_set_config_size.\n     */\n    n->config_size = sizeof(struct virtio_net_config);\n    device_add_bootindex_property(obj, &n->nic_conf.bootindex,\n                                  \"bootindex\", \"/ethernet-phy@0\",\n                                  DEVICE(n));\n\n    ebpf_rss_init(&n->ebpf_rss);\n}\n\nstatic int virtio_net_pre_save(void *opaque)\n{\n    VirtIONet *n = opaque;\n\n    /* At this point, backend must be stopped, otherwise\n     * it might keep writing to memory. */\n    assert(!n->vhost_started);\n\n    return 0;\n}\n\nstatic bool primary_unplug_pending(void *opaque)\n{\n    DeviceState *dev = opaque;\n    DeviceState *primary;\n    VirtIODevice *vdev = VIRTIO_DEVICE(dev);\n    VirtIONet *n = VIRTIO_NET(vdev);\n\n    if (!virtio_vdev_has_feature(vdev, VIRTIO_NET_F_STANDBY)) {\n        return false;\n    }\n    primary = failover_find_primary_device(n);\n    return primary ? primary->pending_deleted_event : false;\n}\n\nstatic bool dev_unplug_pending(void *opaque)\n{\n    DeviceState *dev = opaque;\n    VirtioDeviceClass *vdc = VIRTIO_DEVICE_GET_CLASS(dev);\n\n    return vdc->primary_unplug_pending(dev);\n}\n\nstatic const VMStateDescription vmstate_virtio_net = {\n    .name = \"virtio-net\",\n    .minimum_version_id = VIRTIO_NET_VM_VERSION,\n    .version_id = VIRTIO_NET_VM_VERSION,\n    .fields = (VMStateField[]) {\n        VMSTATE_VIRTIO_DEVICE,\n        VMSTATE_END_OF_LIST()\n    },\n    .pre_save = virtio_net_pre_save,\n    .dev_unplug_pending = dev_unplug_pending,\n};\n\nstatic Property virtio_net_properties[] = {\n    DEFINE_PROP_BIT64(\"csum\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CSUM, true),\n    DEFINE_PROP_BIT64(\"guest_csum\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_CSUM, true),\n    DEFINE_PROP_BIT64(\"gso\", VirtIONet, host_features, VIRTIO_NET_F_GSO, true),\n    DEFINE_PROP_BIT64(\"guest_tso4\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_TSO4, true),\n    DEFINE_PROP_BIT64(\"guest_tso6\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_TSO6, true),\n    DEFINE_PROP_BIT64(\"guest_ecn\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_ECN, true),\n    DEFINE_PROP_BIT64(\"guest_ufo\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_UFO, true),\n    DEFINE_PROP_BIT64(\"guest_announce\", VirtIONet, host_features,\n                    VIRTIO_NET_F_GUEST_ANNOUNCE, true),\n    DEFINE_PROP_BIT64(\"host_tso4\", VirtIONet, host_features,\n                    VIRTIO_NET_F_HOST_TSO4, true),\n    DEFINE_PROP_BIT64(\"host_tso6\", VirtIONet, host_features,\n                    VIRTIO_NET_F_HOST_TSO6, true),\n    DEFINE_PROP_BIT64(\"host_ecn\", VirtIONet, host_features,\n                    VIRTIO_NET_F_HOST_ECN, true),\n    DEFINE_PROP_BIT64(\"host_ufo\", VirtIONet, host_features,\n                    VIRTIO_NET_F_HOST_UFO, true),\n    DEFINE_PROP_BIT64(\"mrg_rxbuf\", VirtIONet, host_features,\n                    VIRTIO_NET_F_MRG_RXBUF, true),\n    DEFINE_PROP_BIT64(\"status\", VirtIONet, host_features,\n                    VIRTIO_NET_F_STATUS, true),\n    DEFINE_PROP_BIT64(\"ctrl_vq\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_VQ, true),\n    DEFINE_PROP_BIT64(\"ctrl_rx\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_RX, true),\n    DEFINE_PROP_BIT64(\"ctrl_vlan\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_VLAN, true),\n    DEFINE_PROP_BIT64(\"ctrl_rx_extra\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_RX_EXTRA, true),\n    DEFINE_PROP_BIT64(\"ctrl_mac_addr\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_MAC_ADDR, true),\n    DEFINE_PROP_BIT64(\"ctrl_guest_offloads\", VirtIONet, host_features,\n                    VIRTIO_NET_F_CTRL_GUEST_OFFLOADS, true),\n    DEFINE_PROP_BIT64(\"mq\", VirtIONet, host_features, VIRTIO_NET_F_MQ, false),\n    DEFINE_PROP_BIT64(\"rss\", VirtIONet, host_features,\n                    VIRTIO_NET_F_RSS, false),\n    DEFINE_PROP_BIT64(\"hash\", VirtIONet, host_features,\n                    VIRTIO_NET_F_HASH_REPORT, false),\n    DEFINE_PROP_BIT64(\"guest_rsc_ext\", VirtIONet, host_features,\n                    VIRTIO_NET_F_RSC_EXT, false),\n    DEFINE_PROP_UINT32(\"rsc_interval\", VirtIONet, rsc_timeout,\n                       VIRTIO_NET_RSC_DEFAULT_INTERVAL),\n    DEFINE_NIC_PROPERTIES(VirtIONet, nic_conf),\n    DEFINE_PROP_UINT32(\"x-txtimer\", VirtIONet, net_conf.txtimer,\n                       TX_TIMER_INTERVAL),\n    DEFINE_PROP_INT32(\"x-txburst\", VirtIONet, net_conf.txburst, TX_BURST),\n    DEFINE_PROP_STRING(\"tx\", VirtIONet, net_conf.tx),\n    DEFINE_PROP_UINT16(\"rx_queue_size\", VirtIONet, net_conf.rx_queue_size,\n                       VIRTIO_NET_RX_QUEUE_DEFAULT_SIZE),\n    DEFINE_PROP_UINT16(\"tx_queue_size\", VirtIONet, net_conf.tx_queue_size,\n                       VIRTIO_NET_TX_QUEUE_DEFAULT_SIZE),\n    DEFINE_PROP_UINT16(\"host_mtu\", VirtIONet, net_conf.mtu, 0),\n    DEFINE_PROP_BOOL(\"x-mtu-bypass-backend\", VirtIONet, mtu_bypass_backend,\n                     true),\n    DEFINE_PROP_INT32(\"speed\", VirtIONet, net_conf.speed, SPEED_UNKNOWN),\n    DEFINE_PROP_STRING(\"duplex\", VirtIONet, net_conf.duplex_str),\n    DEFINE_PROP_BOOL(\"failover\", VirtIONet, failover, false),\n    DEFINE_PROP_END_OF_LIST(),\n};\n\nstatic void virtio_net_class_init(ObjectClass *klass, void *data)\n{\n    DeviceClass *dc = DEVICE_CLASS(klass);\n    VirtioDeviceClass *vdc = VIRTIO_DEVICE_CLASS(klass);\n\n    device_class_set_props(dc, virtio_net_properties);\n    dc->vmsd = &vmstate_virtio_net;\n    set_bit(DEVICE_CATEGORY_NETWORK, dc->categories);\n    vdc->realize = virtio_net_device_realize;\n    vdc->unrealize = virtio_net_device_unrealize;\n    vdc->get_config = virtio_net_get_config;\n    vdc->set_config = virtio_net_set_config;\n    vdc->get_features = virtio_net_get_features;\n    vdc->set_features = virtio_net_set_features;\n    vdc->bad_features = virtio_net_bad_features;\n    vdc->reset = virtio_net_reset;\n    vdc->set_status = virtio_net_set_status;\n    vdc->guest_notifier_mask = virtio_net_guest_notifier_mask;\n    vdc->guest_notifier_pending = virtio_net_guest_notifier_pending;\n    vdc->legacy_features |= (0x1 << VIRTIO_NET_F_GSO);\n    vdc->post_load = virtio_net_post_load_virtio;\n    vdc->vmsd = &vmstate_virtio_net_device;\n    vdc->primary_unplug_pending = primary_unplug_pending;\n}\n\nstatic const TypeInfo virtio_net_info = {\n    .name = TYPE_VIRTIO_NET,\n    .parent = TYPE_VIRTIO_DEVICE,\n    .instance_size = sizeof(VirtIONet),\n    .instance_init = virtio_net_instance_init,\n    .class_init = virtio_net_class_init,\n};\n\nstatic void virtio_register_types(void)\n{\n    type_register_static(&virtio_net_info);\n}\n\ntype_init(virtio_register_types)\n"], "filenames": ["hw/net/virtio-net.c"], "buggy_code_start_loc": [1748], "buggy_code_end_loc": [1857], "fixing_code_start_loc": [1749], "fixing_code_end_loc": [1883], "type": "CWE-416", "message": "A use-after-free vulnerability was found in the virtio-net device of QEMU. It could occur when the descriptor's address belongs to the non direct access region, due to num_buffers being set after the virtqueue elem has been unmapped. A malicious guest could use this flaw to crash QEMU, resulting in a denial of service condition, or potentially execute code on the host with the privileges of the QEMU process.", "other": {"cve": {"id": "CVE-2021-3748", "sourceIdentifier": "secalert@redhat.com", "published": "2022-03-23T20:15:09.893", "lastModified": "2023-01-03T15:16:38.483", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "A use-after-free vulnerability was found in the virtio-net device of QEMU. It could occur when the descriptor's address belongs to the non direct access region, due to num_buffers being set after the virtqueue elem has been unmapped. A malicious guest could use this flaw to crash QEMU, resulting in a denial of service condition, or potentially execute code on the host with the privileges of the QEMU process."}, {"lang": "es", "value": "Se ha encontrado una vulnerabilidad de uso de memoria previamente liberada en el dispositivo virtio-net de QEMU. Podr\u00eda ocurrir cuando la direcci\u00f3n del descriptor pertenece a la regi\u00f3n de acceso no directo, debido a que num_buffers es establecido despu\u00e9s de que el elemento virtqueue haya sido desmapeado. Un hu\u00e9sped malicioso podr\u00eda usar este fallo para bloquear QEMU, resultando en una condici\u00f3n de denegaci\u00f3n de servicio, o potencialmente ejecutar c\u00f3digo en el host con los privilegios del proceso QEMU"}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:L/AC:H/PR:H/UI:N/S:C/C:H/I:H/A:H", "attackVector": "LOCAL", "attackComplexity": "HIGH", "privilegesRequired": "HIGH", "userInteraction": "NONE", "scope": "CHANGED", "confidentialityImpact": "HIGH", "integrityImpact": "HIGH", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 0.8, "impactScore": 6.0}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:M/Au:N/C:C/I:C/A:C", "accessVector": "LOCAL", "accessComplexity": "MEDIUM", "authentication": "NONE", "confidentialityImpact": "COMPLETE", "integrityImpact": "COMPLETE", "availabilityImpact": "COMPLETE", "baseScore": 6.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.4, "impactScore": 10.0, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}, {"source": "secalert@redhat.com", "type": "Secondary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:qemu:qemu:*:*:*:*:*:*:*:*", "versionStartIncluding": "0.10.0", "versionEndExcluding": "6.2.0", "matchCriteriaId": "69067884-3C1C-4933-8955-489BC2EB5BD5"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:9.0:*:*:*:*:*:*:*", "matchCriteriaId": "DEECE5FC-CACF-4496-A3E7-164736409252"}, {"vulnerable": true, "criteria": "cpe:2.3:o:debian:debian_linux:10.0:*:*:*:*:*:*:*", "matchCriteriaId": "07B237A9-69A3-4A9C-9DA0-4E06BD37AE73"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:18.04:*:*:*:lts:*:*:*", "matchCriteriaId": "23A7C53F-B80F-4E6A-AFA9-58EEA84BE11D"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:20.04:*:*:*:lts:*:*:*", "matchCriteriaId": "902B8056-9E37-443B-8905-8AA93E2447FB"}, {"vulnerable": true, "criteria": "cpe:2.3:o:canonical:ubuntu_linux:21.10:*:*:*:*:*:*:*", "matchCriteriaId": "AAE4D2D0-CEEB-416F-8BC5-A7987DF56190"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:fedoraproject:fedora:34:*:*:*:*:*:*:*", "matchCriteriaId": "A930E247-0B43-43CB-98FF-6CE7B8189835"}]}]}, {"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux:8.0:*:*:*:advanced_virtualization:*:*:*", "matchCriteriaId": "3AA08768-75AF-4791-B229-AE938C780959"}, {"vulnerable": true, "criteria": "cpe:2.3:o:redhat:enterprise_linux_advanced_virtualization_eus:8.4:*:*:*:*:*:*:*", "matchCriteriaId": "04F853F5-C907-48A3-BDED-2AC3923E4010"}]}]}], "references": [{"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1998514", "source": "secalert@redhat.com", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://github.com/qemu/qemu/commit/bedd7e93d01961fcb16a97ae45d93acf357e11f6", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/04/msg00002.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.debian.org/debian-lts-announce/2022/09/msg00008.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Third Party Advisory"]}, {"url": "https://lists.nongnu.org/archive/html/qemu-devel/2021-09/msg00388.html", "source": "secalert@redhat.com", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}, {"url": "https://security.gentoo.org/glsa/202208-27", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://security.netapp.com/advisory/ntap-20220425-0004/", "source": "secalert@redhat.com", "tags": ["Third Party Advisory"]}, {"url": "https://ubuntu.com/security/CVE-2021-3748", "source": "secalert@redhat.com", "tags": ["Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/qemu/qemu/commit/bedd7e93d01961fcb16a97ae45d93acf357e11f6"}}