{"buggy_code": ["date: Pending\n\nbehavior_changes:\n# *Changes that are expected to cause an incompatibility if applicable; deployment changes are likely required*\n\nminor_behavior_changes:\n# *Changes that may cause incompatibilities for some users, but should not for most*\n- area: adaptive concurrency filter stats\n  change: |\n    Multiply the gradient value stat by 1000 to make it more granular (values will range between 500 and 2000).\n- area: dns\n  change: |\n    Allowing <envoy_v3_api_field_extensions.common.dynamic_forward_proxy.v3.DnsCacheConfig.dns_min_refresh_rate>` to go as low as 1s.\n- area: upstream\n  change: |\n    Upstream now excludes hosts set to ``DRAINING`` state via EDS from load balancing and panic routing\n    threshold calculation. This feature can be disabled by setting\n    ``envoy.reloadable_features.exclude_host_in_eds_status_draining`` to false.\n- area: golang\n  change: |\n    Change ``RegisterHttpFilterConfigFactoryAndParser`` to ``RegisterHttpFilterFactoryAndConfigParser``.\n- area: QUIC\n  change: |\n    Port migration is default turned off. QUIC client connections will no longer attempt to migrate to a new port when connections\n    is degrading. Can be manually turned on via\n    :ref:`port_migration <envoy_v3_api_field_config.core.v3.QuicProtocolOptions.num_timeouts_to_trigger_port_migration>`.\n- area: aws\n  change: |\n    AWS region string is now retrieved from environment and profile consistently within aws_request_signer and\n    grpc_credentials/aws_iam extensions. Region field in aws_request_signer is now optional, explicitly configured\n    xDS region will take preference. aws_request_signer documentation now reflects the region chain.\n\nbug_fixes:\n# *Changes expected to improve the state of the world and are unlikely to have negative effects*\n- area: tracers\n  change: |\n    use unary RPC calls for OpenTelemetry trace exports, rather than client-side streaming connections.\n- area: load balancing\n  change: |\n    Added randomization in locality load-balancing initialization. This helps desynchronizing Envoys across\n    a fleet by randomizing the scheduler starting point. This can be temporarily reverted by setting runtime guard\n    ``envoy.reloadable_features.edf_lb_locality_scheduler_init_fix`` to false.\n- area: UDP and TCP tunneling\n  change: |\n    fixed a bug where second HTTP response headers received would cause Envoy to crash in cases where\n    ``propagate_response_headers`` and retry configurations are enabled at the same time, and an upstream\n    request is retried multiple times.\n- area: tracing\n  change: |\n    Prevent Envoy from crashing at start up when the OpenTelemetry environment resource detector cannot detect any attributes.\n- area: proxy protocol\n  change: |\n    Fixed a crash when Envoy is configured for PROXY protocol on both a listener and cluster, and the listener receives\n    a PROXY protocol header with address type LOCAL (typically used for health checks).\n- area: url matching\n  change: |\n    Fixed excessive CPU utilization when using regex URL template matcher.\n\nremoved_config_or_runtime:\n# *Normally occurs at the end of the* :ref:`deprecation period <deprecated>`\n- area: http\n  change: |\n    Removed ``envoy.reloadable_features.allow_absolute_url_with_mixed_scheme`` runtime flag and legacy code paths.\n- area: active health check\n  change: |\n    Removed ``envoy.reloadable_features.keep_endpoint_active_hc_status_on_locality_update`` runtime flag and legacy code paths.\n- area: http1\n  change: |\n    Removed ``envoy.reloadable_features.http1_allow_codec_error_response_after_1xx_headers`` runtime flag and legacy code paths.\n- area: overload manager\n  change: |\n    removed ``envoy.reloadable_features.overload_manager_error_unknown_action`` and legacy code paths.\n- area: http\n  change: |\n    Removed ``envoy_reloadable_features_append_xfh_idempotent`` runtime flag and legacy code paths.\n- area: resource_monitors\n  change: |\n    removed ``envoy.reloadable_features.count_unused_mapped_pages_as_free`` runtime flag  and legacy code paths.\n\nnew_features:\n- area: aws_request_signing\n  change: |\n    Update ``aws_request_signing`` filter to support use as an upstream HTTP filter. This allows successful calculation of\n    signatures after the forwarding stage has completed, particularly if the path element is modified.\n- area: aws_lambda\n  change: |\n    Update ``aws_lambda`` filter to support use as an upstream HTTP filter. This allows successful calculation of\n    signatures after the forwarding stage has completed, particularly if the path element is modified.\n- area: grpc reverse bridge\n  change: |\n    Change HTTP status to 200 to respect the gRPC protocol. This may cause problems for incorrect gRPC clients expecting the filter\n    to preserve HTTP 1.1 responses.  This behavioral change can be temporarily reverted by setting runtime guard\n    ``envoy.reloadable_features.grpc_http1_reverse_bridge_change_http_status`` to false.\n- area: quic\n  change: |\n    Added QUIC protocol option :ref:`send_disable_active_migration\n    <envoy_v3_api_field_config.listener.v3.QuicProtocolOptions.send_disable_active_migration>` to make the server send clients a transport\n    parameter to discourage client endpoints from active migration.\n- area: ext_proc\n  change: |\n    implemented\n    :ref:`request_attributes <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.ExternalProcessor.request_attributes>`\n    and\n    :ref:`response_attributes <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.ExternalProcessor.response_attributes>`\n    config APIs to enable sending and receiving attributes to/from the external processing server.\n- area: access log\n  change: |\n    added support for :ref:`%UPSTREAM_CONNECTION_ID% <config_access_log_format_upstream_connection_id>` for the upstream connection\n    identifier.\n- area: aws_lambda\n  change: |\n    Added :ref:`host_rewrite <envoy_v3_api_field_extensions.filters.http.aws_lambda.v3.Config.host_rewrite>` config to be used\n    during signature.\n- area: ext_proc\n  change: |\n    added\n    :ref:`metadata_options <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.ExternalProcessor.metadata_options>`\n    config API to enable sending and receiving metadata from/to the external processing server. Both typed and untyped dynamic\n    metadata may be sent to the server. If\n    :ref:`receiving_namespaces <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.MetadataOptions.receiving_namespaces>`\n    is defined, returned metadata may be written to the specified allowed namespaces.\n- area: monitoring\n  change: |\n    Add ``Envoy::ExecutionContext``, which is notified by ``ScopeTrackerScopeState``'s constructor and destructor. This feature is\n    disabled by default, it can be enabled by runtime feature flag ``envoy.restart_features.enable_execution_context``. For more details,\n    please see https://github.com/envoyproxy/envoy/issues/32012.\n- area: rbac\n  change: |\n    Added :ref:`uri_template<envoy_v3_api_field_config.rbac.v3.Permission.uri_template>` which uses existing\n    :ref:`UriTemplateMatchConfig<envoy_v3_api_msg_extensions.path.match.uri_template.v3.UriTemplateMatchConfig>`\n    to allow use of glob patterns for URI path matching in RBAC.\n- area: upstream\n  change: |\n    Added :ref:`selection_method <envoy_v3_api_msg_extensions.load_balancing_policies.least_request.v3.LeastRequest>`\n    option to the least request load balancer. If set to ``FULL_SCAN``,\n    Envoy will select the host with the fewest active requests from the entire host set rather than\n    :ref:`choice_count <envoy_v3_api_msg_extensions.load_balancing_policies.least_request.v3.LeastRequest>`\n    random choices.\n\ndeprecated:\n", "#include \"source/common/router/router.h\"\n\n#include <algorithm>\n#include <chrono>\n#include <cstdint>\n#include <functional>\n#include <memory>\n#include <string>\n\n#include \"envoy/event/dispatcher.h\"\n#include \"envoy/event/timer.h\"\n#include \"envoy/grpc/status.h\"\n#include \"envoy/http/conn_pool.h\"\n#include \"envoy/runtime/runtime.h\"\n#include \"envoy/upstream/cluster_manager.h\"\n#include \"envoy/upstream/health_check_host_monitor.h\"\n#include \"envoy/upstream/upstream.h\"\n\n#include \"source/common/common/assert.h\"\n#include \"source/common/common/cleanup.h\"\n#include \"source/common/common/empty_string.h\"\n#include \"source/common/common/enum_to_int.h\"\n#include \"source/common/common/scope_tracker.h\"\n#include \"source/common/common/utility.h\"\n#include \"source/common/config/utility.h\"\n#include \"source/common/grpc/common.h\"\n#include \"source/common/http/codes.h\"\n#include \"source/common/http/header_map_impl.h\"\n#include \"source/common/http/headers.h\"\n#include \"source/common/http/message_impl.h\"\n#include \"source/common/http/utility.h\"\n#include \"source/common/network/application_protocol.h\"\n#include \"source/common/network/socket_option_factory.h\"\n#include \"source/common/network/transport_socket_options_impl.h\"\n#include \"source/common/network/upstream_server_name.h\"\n#include \"source/common/network/upstream_socket_options_filter_state.h\"\n#include \"source/common/network/upstream_subject_alt_names.h\"\n#include \"source/common/router/config_impl.h\"\n#include \"source/common/router/debug_config.h\"\n#include \"source/common/router/retry_state_impl.h\"\n#include \"source/common/runtime/runtime_features.h\"\n#include \"source/common/stream_info/uint32_accessor_impl.h\"\n#include \"source/common/tracing/http_tracer_impl.h\"\n\nnamespace Envoy {\nnamespace Router {\nnamespace {\nconstexpr char NumInternalRedirectsFilterStateName[] = \"num_internal_redirects\";\n\nuint32_t getLength(const Buffer::Instance* instance) { return instance ? instance->length() : 0; }\n\nbool schemeIsHttp(const Http::RequestHeaderMap& downstream_headers,\n                  OptRef<const Network::Connection> connection) {\n  if (Http::Utility::schemeIsHttp(downstream_headers.getSchemeValue())) {\n    return true;\n  }\n  if (connection.has_value() && !connection->ssl()) {\n    return true;\n  }\n  return false;\n}\n\nconstexpr uint64_t TimeoutPrecisionFactor = 100;\n\n} // namespace\n\nFilterConfig::FilterConfig(Stats::StatName stat_prefix,\n                           Server::Configuration::FactoryContext& context,\n                           ShadowWriterPtr&& shadow_writer,\n                           const envoy::extensions::filters::http::router::v3::Router& config)\n    : FilterConfig(\n          stat_prefix, context.serverFactoryContext().localInfo(), context.scope(),\n          context.serverFactoryContext().clusterManager(), context.serverFactoryContext().runtime(),\n          context.serverFactoryContext().api().randomGenerator(), std::move(shadow_writer),\n          PROTOBUF_GET_WRAPPED_OR_DEFAULT(config, dynamic_stats, true), config.start_child_span(),\n          config.suppress_envoy_headers(), config.respect_expected_rq_timeout(),\n          config.suppress_grpc_request_failure_code_stats(),\n          config.has_upstream_log_options()\n              ? config.upstream_log_options().flush_upstream_log_on_upstream_stream()\n              : false,\n          config.strict_check_headers(), context.serverFactoryContext().api().timeSource(),\n          context.serverFactoryContext().httpContext(),\n          context.serverFactoryContext().routerContext()) {\n  for (const auto& upstream_log : config.upstream_log()) {\n    upstream_logs_.push_back(AccessLog::AccessLogFactory::fromProto(upstream_log, context));\n  }\n\n  if (config.has_upstream_log_options() &&\n      config.upstream_log_options().has_upstream_log_flush_interval()) {\n    upstream_log_flush_interval_ = std::chrono::milliseconds(DurationUtil::durationToMilliseconds(\n        config.upstream_log_options().upstream_log_flush_interval()));\n  }\n\n  if (config.upstream_http_filters_size() > 0) {\n    auto& server_factory_ctx = context.serverFactoryContext();\n    const Http::FilterChainUtility::FiltersList& upstream_http_filters =\n        config.upstream_http_filters();\n    std::shared_ptr<Http::UpstreamFilterConfigProviderManager> filter_config_provider_manager =\n        Http::FilterChainUtility::createSingletonUpstreamFilterConfigProviderManager(\n            server_factory_ctx);\n    std::string prefix = context.scope().symbolTable().toString(context.scope().prefix());\n    upstream_ctx_ = std::make_unique<Upstream::UpstreamFactoryContextImpl>(\n        server_factory_ctx, context.initManager(), context.scope());\n    Http::FilterChainHelper<Server::Configuration::UpstreamFactoryContext,\n                            Server::Configuration::UpstreamHttpFilterConfigFactory>\n        helper(*filter_config_provider_manager, server_factory_ctx,\n               context.serverFactoryContext().clusterManager(), *upstream_ctx_, prefix);\n    THROW_IF_NOT_OK(helper.processFilters(upstream_http_filters, \"router upstream http\",\n                                          \"router upstream http\", upstream_http_filter_factories_));\n  }\n}\n\n// Express percentage as [0, TimeoutPrecisionFactor] because stats do not accept floating point\n// values, and getting multiple significant figures on the histogram would be nice.\nuint64_t FilterUtility::percentageOfTimeout(const std::chrono::milliseconds response_time,\n                                            const std::chrono::milliseconds timeout) {\n  // Timeouts of 0 are considered infinite. Any portion of an infinite timeout used is still\n  // none of it.\n  if (timeout.count() == 0) {\n    return 0;\n  }\n\n  return static_cast<uint64_t>(response_time.count() * TimeoutPrecisionFactor / timeout.count());\n}\n\nvoid FilterUtility::setUpstreamScheme(Http::RequestHeaderMap& headers, bool downstream_secure) {\n  if (Http::Utility::schemeIsValid(headers.getSchemeValue())) {\n    return;\n  }\n  // After all the changes in https://github.com/envoyproxy/envoy/issues/14587\n  // this path should only occur if a buggy filter has removed the :scheme\n  // header. In that case best-effort set from X-Forwarded-Proto.\n  absl::string_view xfp = headers.getForwardedProtoValue();\n  if (Http::Utility::schemeIsValid(xfp)) {\n    headers.setScheme(xfp);\n    return;\n  }\n\n  if (downstream_secure) {\n    headers.setReferenceScheme(Http::Headers::get().SchemeValues.Https);\n  } else {\n    headers.setReferenceScheme(Http::Headers::get().SchemeValues.Http);\n  }\n}\n\nbool FilterUtility::shouldShadow(const ShadowPolicy& policy, Runtime::Loader& runtime,\n                                 uint64_t stable_random) {\n\n  // The policy's default value is set correctly regardless of whether there is a runtime key\n  // or not, thus this call is sufficient for all cases (100% if no runtime set, otherwise\n  // using the default value within the runtime fractional percent setting).\n  return runtime.snapshot().featureEnabled(policy.runtimeKey(), policy.defaultValue(),\n                                           stable_random);\n}\n\nTimeoutData FilterUtility::finalTimeout(const RouteEntry& route,\n                                        Http::RequestHeaderMap& request_headers,\n                                        bool insert_envoy_expected_request_timeout_ms,\n                                        bool grpc_request, bool per_try_timeout_hedging_enabled,\n                                        bool respect_expected_rq_timeout) {\n  // See if there is a user supplied timeout in a request header. If there is we take that.\n  // Otherwise if the request is gRPC and a maximum gRPC timeout is configured we use the timeout\n  // in the gRPC headers (or infinity when gRPC headers have no timeout), but cap that timeout to\n  // the configured maximum gRPC timeout (which may also be infinity, represented by a 0 value),\n  // or the default from the route config otherwise.\n  TimeoutData timeout;\n  if (!route.usingNewTimeouts()) {\n    if (grpc_request && route.maxGrpcTimeout()) {\n      const std::chrono::milliseconds max_grpc_timeout = route.maxGrpcTimeout().value();\n      auto header_timeout = Grpc::Common::getGrpcTimeout(request_headers);\n      std::chrono::milliseconds grpc_timeout =\n          header_timeout ? header_timeout.value() : std::chrono::milliseconds(0);\n      if (route.grpcTimeoutOffset()) {\n        // We only apply the offset if it won't result in grpc_timeout hitting 0 or below, as\n        // setting it to 0 means infinity and a negative timeout makes no sense.\n        const auto offset = *route.grpcTimeoutOffset();\n        if (offset < grpc_timeout) {\n          grpc_timeout -= offset;\n        }\n      }\n\n      // Cap gRPC timeout to the configured maximum considering that 0 means infinity.\n      if (max_grpc_timeout != std::chrono::milliseconds(0) &&\n          (grpc_timeout == std::chrono::milliseconds(0) || grpc_timeout > max_grpc_timeout)) {\n        grpc_timeout = max_grpc_timeout;\n      }\n      timeout.global_timeout_ = grpc_timeout;\n    } else {\n      timeout.global_timeout_ = route.timeout();\n    }\n  }\n  timeout.per_try_timeout_ = route.retryPolicy().perTryTimeout();\n  timeout.per_try_idle_timeout_ = route.retryPolicy().perTryIdleTimeout();\n\n  uint64_t header_timeout;\n\n  if (respect_expected_rq_timeout) {\n    // Check if there is timeout set by egress Envoy.\n    // If present, use that value as route timeout and don't override\n    // *x-envoy-expected-rq-timeout-ms* header. At this point *x-envoy-upstream-rq-timeout-ms*\n    // header should have been sanitized by egress Envoy.\n    const Http::HeaderEntry* header_expected_timeout_entry =\n        request_headers.EnvoyExpectedRequestTimeoutMs();\n    if (header_expected_timeout_entry) {\n      trySetGlobalTimeout(*header_expected_timeout_entry, timeout);\n    } else {\n      const Http::HeaderEntry* header_timeout_entry =\n          request_headers.EnvoyUpstreamRequestTimeoutMs();\n\n      if (header_timeout_entry) {\n        trySetGlobalTimeout(*header_timeout_entry, timeout);\n        request_headers.removeEnvoyUpstreamRequestTimeoutMs();\n      }\n    }\n  } else {\n    const Http::HeaderEntry* header_timeout_entry = request_headers.EnvoyUpstreamRequestTimeoutMs();\n\n    if (header_timeout_entry) {\n      trySetGlobalTimeout(*header_timeout_entry, timeout);\n      request_headers.removeEnvoyUpstreamRequestTimeoutMs();\n    }\n  }\n\n  // See if there is a per try/retry timeout. If it's >= global we just ignore it.\n  const absl::string_view per_try_timeout_entry =\n      request_headers.getEnvoyUpstreamRequestPerTryTimeoutMsValue();\n  if (!per_try_timeout_entry.empty()) {\n    if (absl::SimpleAtoi(per_try_timeout_entry, &header_timeout)) {\n      timeout.per_try_timeout_ = std::chrono::milliseconds(header_timeout);\n    }\n    request_headers.removeEnvoyUpstreamRequestPerTryTimeoutMs();\n  }\n\n  if (timeout.per_try_timeout_ >= timeout.global_timeout_ && timeout.global_timeout_.count() != 0) {\n    timeout.per_try_timeout_ = std::chrono::milliseconds(0);\n  }\n\n  setTimeoutHeaders(0, timeout, route, request_headers, insert_envoy_expected_request_timeout_ms,\n                    grpc_request, per_try_timeout_hedging_enabled);\n\n  return timeout;\n}\n\nvoid FilterUtility::setTimeoutHeaders(uint64_t elapsed_time, const TimeoutData& timeout,\n                                      const RouteEntry& route,\n                                      Http::RequestHeaderMap& request_headers,\n                                      bool insert_envoy_expected_request_timeout_ms,\n                                      bool grpc_request, bool per_try_timeout_hedging_enabled) {\n\n  const uint64_t global_timeout = timeout.global_timeout_.count();\n\n  // See if there is any timeout to write in the expected timeout header.\n  uint64_t expected_timeout = timeout.per_try_timeout_.count();\n\n  // Use the global timeout if no per try timeout was specified or if we're\n  // doing hedging when there are per try timeouts. Either of these scenarios\n  // mean that the upstream server can use the full global timeout.\n  if (per_try_timeout_hedging_enabled || expected_timeout == 0) {\n    expected_timeout = global_timeout;\n  }\n\n  // If the expected timeout is 0 set no timeout, as Envoy treats 0 as infinite timeout.\n  if (expected_timeout > 0) {\n\n    if (global_timeout > 0) {\n      if (elapsed_time >= global_timeout) {\n        // We are out of time, but 0 would be an infinite timeout. So instead we send a 1ms timeout\n        // and assume the timers armed by onRequestComplete() will fire very soon.\n        expected_timeout = 1;\n      } else {\n        expected_timeout = std::min(expected_timeout, global_timeout - elapsed_time);\n      }\n    }\n\n    if (insert_envoy_expected_request_timeout_ms) {\n      request_headers.setEnvoyExpectedRequestTimeoutMs(expected_timeout);\n    }\n\n    // If we've configured max_grpc_timeout, override the grpc-timeout header with\n    // the expected timeout. This ensures that the optional per try timeout is reflected\n    // in grpc-timeout, ensuring that the upstream gRPC server is aware of the actual timeout.\n    if (grpc_request && !route.usingNewTimeouts() && route.maxGrpcTimeout()) {\n      Grpc::Common::toGrpcTimeout(std::chrono::milliseconds(expected_timeout), request_headers);\n    }\n  }\n}\n\nabsl::optional<std::chrono::milliseconds>\nFilterUtility::tryParseHeaderTimeout(const Http::HeaderEntry& header_timeout_entry) {\n  uint64_t header_timeout;\n  if (absl::SimpleAtoi(header_timeout_entry.value().getStringView(), &header_timeout)) {\n    return std::chrono::milliseconds(header_timeout);\n  }\n  return absl::nullopt;\n}\n\nvoid FilterUtility::trySetGlobalTimeout(const Http::HeaderEntry& header_timeout_entry,\n                                        TimeoutData& timeout) {\n  const auto timeout_ms = tryParseHeaderTimeout(header_timeout_entry);\n  if (timeout_ms.has_value()) {\n    timeout.global_timeout_ = timeout_ms.value();\n  }\n}\n\nFilterUtility::HedgingParams\nFilterUtility::finalHedgingParams(const RouteEntry& route,\n                                  Http::RequestHeaderMap& request_headers) {\n  HedgingParams hedging_params;\n  hedging_params.hedge_on_per_try_timeout_ = route.hedgePolicy().hedgeOnPerTryTimeout();\n\n  const Http::HeaderEntry* hedge_on_per_try_timeout_entry =\n      request_headers.EnvoyHedgeOnPerTryTimeout();\n  if (hedge_on_per_try_timeout_entry) {\n    if (hedge_on_per_try_timeout_entry->value() == \"true\") {\n      hedging_params.hedge_on_per_try_timeout_ = true;\n    }\n    if (hedge_on_per_try_timeout_entry->value() == \"false\") {\n      hedging_params.hedge_on_per_try_timeout_ = false;\n    }\n\n    request_headers.removeEnvoyHedgeOnPerTryTimeout();\n  }\n\n  return hedging_params;\n}\n\nFilter::~Filter() {\n  // Upstream resources should already have been cleaned.\n  ASSERT(upstream_requests_.empty());\n  ASSERT(!retry_state_);\n}\n\nconst FilterUtility::StrictHeaderChecker::HeaderCheckResult\nFilterUtility::StrictHeaderChecker::checkHeader(Http::RequestHeaderMap& headers,\n                                                const Http::LowerCaseString& target_header) {\n  if (target_header == Http::Headers::get().EnvoyUpstreamRequestTimeoutMs) {\n    return isInteger(headers.EnvoyUpstreamRequestTimeoutMs());\n  } else if (target_header == Http::Headers::get().EnvoyUpstreamRequestPerTryTimeoutMs) {\n    return isInteger(headers.EnvoyUpstreamRequestPerTryTimeoutMs());\n  } else if (target_header == Http::Headers::get().EnvoyMaxRetries) {\n    return isInteger(headers.EnvoyMaxRetries());\n  } else if (target_header == Http::Headers::get().EnvoyRetryOn) {\n    return hasValidRetryFields(headers.EnvoyRetryOn(), &Router::RetryStateImpl::parseRetryOn);\n  } else if (target_header == Http::Headers::get().EnvoyRetryGrpcOn) {\n    return hasValidRetryFields(headers.EnvoyRetryGrpcOn(),\n                               &Router::RetryStateImpl::parseRetryGrpcOn);\n  }\n  // Should only validate headers for which we have implemented a validator.\n  PANIC(\"unexpectedly reached\");\n}\n\nStats::StatName Filter::upstreamZone(Upstream::HostDescriptionConstSharedPtr upstream_host) {\n  return upstream_host ? upstream_host->localityZoneStatName() : config_.empty_stat_name_;\n}\n\nvoid Filter::chargeUpstreamCode(uint64_t response_status_code,\n                                const Http::ResponseHeaderMap& response_headers,\n                                Upstream::HostDescriptionConstSharedPtr upstream_host,\n                                bool dropped) {\n  // Passing the response_status_code explicitly is an optimization to avoid\n  // multiple calls to slow Http::Utility::getResponseStatus.\n  ASSERT(response_status_code == Http::Utility::getResponseStatus(response_headers));\n  if (config_.emit_dynamic_stats_ && !callbacks_->streamInfo().healthCheck()) {\n    const Http::HeaderEntry* upstream_canary_header = response_headers.EnvoyUpstreamCanary();\n    const bool is_canary = (upstream_canary_header && upstream_canary_header->value() == \"true\") ||\n                           (upstream_host ? upstream_host->canary() : false);\n    const bool internal_request = Http::HeaderUtility::isEnvoyInternalRequest(*downstream_headers_);\n\n    Stats::StatName upstream_zone = upstreamZone(upstream_host);\n    Http::CodeStats::ResponseStatInfo info{\n        config_.scope_,\n        cluster_->statsScope(),\n        config_.empty_stat_name_,\n        response_status_code,\n        internal_request,\n        route_entry_->virtualHost().statName(),\n        request_vcluster_ ? request_vcluster_->statName() : config_.empty_stat_name_,\n        route_stats_context_.has_value() ? route_stats_context_->statName()\n                                         : config_.empty_stat_name_,\n        config_.zone_name_,\n        upstream_zone,\n        is_canary};\n\n    Http::CodeStats& code_stats = httpContext().codeStats();\n    code_stats.chargeResponseStat(info, exclude_http_code_stats_);\n\n    if (alt_stat_prefix_ != nullptr) {\n      Http::CodeStats::ResponseStatInfo alt_info{config_.scope_,\n                                                 cluster_->statsScope(),\n                                                 alt_stat_prefix_->statName(),\n                                                 response_status_code,\n                                                 internal_request,\n                                                 config_.empty_stat_name_,\n                                                 config_.empty_stat_name_,\n                                                 config_.empty_stat_name_,\n                                                 config_.zone_name_,\n                                                 upstream_zone,\n                                                 is_canary};\n      code_stats.chargeResponseStat(alt_info, exclude_http_code_stats_);\n    }\n\n    if (dropped) {\n      cluster_->loadReportStats().upstream_rq_dropped_.inc();\n    }\n    if (upstream_host && Http::CodeUtility::is5xx(response_status_code)) {\n      upstream_host->stats().rq_error_.inc();\n    }\n  }\n}\n\nvoid Filter::chargeUpstreamCode(Http::Code code,\n                                Upstream::HostDescriptionConstSharedPtr upstream_host,\n                                bool dropped) {\n  const uint64_t response_status_code = enumToInt(code);\n  const auto fake_response_headers = Http::createHeaderMap<Http::ResponseHeaderMapImpl>(\n      {{Http::Headers::get().Status, std::to_string(response_status_code)}});\n  chargeUpstreamCode(response_status_code, *fake_response_headers, upstream_host, dropped);\n}\n\nHttp::FilterHeadersStatus Filter::decodeHeaders(Http::RequestHeaderMap& headers, bool end_stream) {\n  downstream_headers_ = &headers;\n\n  // Extract debug configuration from filter state. This is used further along to determine whether\n  // we should append cluster and host headers to the response, and whether to forward the request\n  // upstream.\n  const StreamInfo::FilterStateSharedPtr& filter_state = callbacks_->streamInfo().filterState();\n  const DebugConfig* debug_config = filter_state->getDataReadOnly<DebugConfig>(DebugConfig::key());\n\n  // TODO: Maybe add a filter API for this.\n  grpc_request_ = Grpc::Common::isGrpcRequestHeaders(headers);\n  exclude_http_code_stats_ = grpc_request_ && config_.suppress_grpc_request_failure_code_stats_;\n\n  // Only increment rq total stat if we actually decode headers here. This does not count requests\n  // that get handled by earlier filters.\n  stats_.rq_total_.inc();\n\n  // Initialize the `modify_headers` function as a no-op (so we don't have to remember to check it\n  // against nullptr before calling it), and feed it behavior later if/when we have cluster info\n  // headers to append.\n  std::function<void(Http::ResponseHeaderMap&)> modify_headers = [](Http::ResponseHeaderMap&) {};\n\n  // Determine if there is a route entry or a direct response for the request.\n  route_ = callbacks_->route();\n  if (!route_) {\n    stats_.no_route_.inc();\n    ENVOY_STREAM_LOG(debug, \"no route match for URL '{}'\", *callbacks_, headers.getPathValue());\n\n    callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::NoRouteFound);\n    callbacks_->sendLocalReply(Http::Code::NotFound, \"\", modify_headers, absl::nullopt,\n                               StreamInfo::ResponseCodeDetails::get().RouteNotFound);\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n\n  // Determine if there is a direct response for the request.\n  const auto* direct_response = route_->directResponseEntry();\n  if (direct_response != nullptr) {\n    stats_.rq_direct_response_.inc();\n    direct_response->rewritePathHeader(headers, !config_.suppress_envoy_headers_);\n    callbacks_->sendLocalReply(\n        direct_response->responseCode(), direct_response->responseBody(),\n        [this, direct_response,\n         &request_headers = headers](Http::ResponseHeaderMap& response_headers) -> void {\n          std::string new_uri;\n          if (request_headers.Path()) {\n            new_uri = direct_response->newUri(request_headers);\n          }\n          // See https://tools.ietf.org/html/rfc7231#section-7.1.2.\n          const auto add_location =\n              direct_response->responseCode() == Http::Code::Created ||\n              Http::CodeUtility::is3xx(enumToInt(direct_response->responseCode()));\n          if (!new_uri.empty() && add_location) {\n            response_headers.addReferenceKey(Http::Headers::get().Location, new_uri);\n          }\n          direct_response->finalizeResponseHeaders(response_headers, callbacks_->streamInfo());\n        },\n        absl::nullopt, StreamInfo::ResponseCodeDetails::get().DirectResponse);\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n\n  // A route entry matches for the request.\n  route_entry_ = route_->routeEntry();\n  // If there's a route specific limit and it's smaller than general downstream\n  // limits, apply the new cap.\n  retry_shadow_buffer_limit_ =\n      std::min(retry_shadow_buffer_limit_, route_entry_->retryShadowBufferLimit());\n  if (debug_config && debug_config->append_cluster_) {\n    // The cluster name will be appended to any local or upstream responses from this point.\n    modify_headers = [this, debug_config](Http::ResponseHeaderMap& headers) {\n      headers.addCopy(debug_config->cluster_header_.value_or(Http::Headers::get().EnvoyCluster),\n                      route_entry_->clusterName());\n    };\n  }\n  Upstream::ThreadLocalCluster* cluster =\n      config_.cm_.getThreadLocalCluster(route_entry_->clusterName());\n  if (!cluster) {\n    stats_.no_cluster_.inc();\n    ENVOY_STREAM_LOG(debug, \"unknown cluster '{}'\", *callbacks_, route_entry_->clusterName());\n\n    callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::NoClusterFound);\n    callbacks_->sendLocalReply(route_entry_->clusterNotFoundResponseCode(), \"\", modify_headers,\n                               absl::nullopt,\n                               StreamInfo::ResponseCodeDetails::get().ClusterNotFound);\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n  cluster_ = cluster->info();\n\n  // Set up stat prefixes, etc.\n  request_vcluster_ = route_entry_->virtualCluster(headers);\n  if (request_vcluster_ != nullptr) {\n    callbacks_->streamInfo().setVirtualClusterName(request_vcluster_->name());\n  }\n  route_stats_context_ = route_entry_->routeStatsContext();\n  ENVOY_STREAM_LOG(debug, \"cluster '{}' match for URL '{}'\", *callbacks_,\n                   route_entry_->clusterName(), headers.getPathValue());\n\n  if (config_.strict_check_headers_ != nullptr) {\n    for (const auto& header : *config_.strict_check_headers_) {\n      const auto res = FilterUtility::StrictHeaderChecker::checkHeader(headers, header);\n      if (!res.valid_) {\n        callbacks_->streamInfo().setResponseFlag(\n            StreamInfo::CoreResponseFlag::InvalidEnvoyRequestHeaders);\n        const std::string body = fmt::format(\"invalid header '{}' with value '{}'\",\n                                             std::string(res.entry_->key().getStringView()),\n                                             std::string(res.entry_->value().getStringView()));\n        const std::string details =\n            absl::StrCat(StreamInfo::ResponseCodeDetails::get().InvalidEnvoyRequestHeaders, \"{\",\n                         StringUtil::replaceAllEmptySpace(res.entry_->key().getStringView()), \"}\");\n        callbacks_->sendLocalReply(Http::Code::BadRequest, body, nullptr, absl::nullopt, details);\n        return Http::FilterHeadersStatus::StopIteration;\n      }\n    }\n  }\n\n  const Http::HeaderEntry* request_alt_name = headers.EnvoyUpstreamAltStatName();\n  if (request_alt_name) {\n    alt_stat_prefix_ = std::make_unique<Stats::StatNameDynamicStorage>(\n        request_alt_name->value().getStringView(), config_.scope_.symbolTable());\n    headers.removeEnvoyUpstreamAltStatName();\n  }\n\n  // See if we are supposed to immediately kill some percentage of this cluster's traffic.\n  if (cluster_->maintenanceMode()) {\n    callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::UpstreamOverflow);\n    chargeUpstreamCode(Http::Code::ServiceUnavailable, nullptr, true);\n    callbacks_->sendLocalReply(\n        Http::Code::ServiceUnavailable, \"maintenance mode\",\n        [modify_headers, this](Http::ResponseHeaderMap& headers) {\n          if (!config_.suppress_envoy_headers_) {\n            headers.addReference(Http::Headers::get().EnvoyOverloaded,\n                                 Http::Headers::get().EnvoyOverloadedValues.True);\n          }\n          // Note: append_cluster_info does not respect suppress_envoy_headers.\n          modify_headers(headers);\n        },\n        absl::nullopt, StreamInfo::ResponseCodeDetails::get().MaintenanceMode);\n    cluster_->trafficStats()->upstream_rq_maintenance_mode_.inc();\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n\n  // Support DROP_OVERLOAD config from control plane to drop certain percentage of traffic.\n  if (checkDropOverload(*cluster, modify_headers)) {\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n\n  // Fetch a connection pool for the upstream cluster.\n  const auto& upstream_http_protocol_options = cluster_->upstreamHttpProtocolOptions();\n\n  if (upstream_http_protocol_options.has_value() &&\n      (upstream_http_protocol_options.value().auto_sni() ||\n       upstream_http_protocol_options.value().auto_san_validation())) {\n    // Default the header to Host/Authority header.\n    absl::string_view header_value = headers.getHostValue();\n\n    // Check whether `override_auto_sni_header` is specified.\n    const auto override_auto_sni_header =\n        upstream_http_protocol_options.value().override_auto_sni_header();\n    if (!override_auto_sni_header.empty()) {\n      // Use the header value from `override_auto_sni_header` to set the SNI value.\n      const auto overridden_header_value = Http::HeaderUtility::getAllOfHeaderAsString(\n          headers, Http::LowerCaseString(override_auto_sni_header));\n      if (overridden_header_value.result().has_value() &&\n          !overridden_header_value.result().value().empty()) {\n        header_value = overridden_header_value.result().value();\n      }\n    }\n    const auto parsed_authority = Http::Utility::parseAuthority(header_value);\n    bool should_set_sni = !parsed_authority.is_ip_address_;\n    // `host_` returns a string_view so doing this should be safe.\n    absl::string_view sni_value = parsed_authority.host_;\n\n    if (should_set_sni && upstream_http_protocol_options.value().auto_sni() &&\n        !callbacks_->streamInfo().filterState()->hasDataWithName(\n            Network::UpstreamServerName::key())) {\n      callbacks_->streamInfo().filterState()->setData(\n          Network::UpstreamServerName::key(),\n          std::make_unique<Network::UpstreamServerName>(sni_value),\n          StreamInfo::FilterState::StateType::Mutable);\n    }\n\n    if (upstream_http_protocol_options.value().auto_san_validation() &&\n        !callbacks_->streamInfo().filterState()->hasDataWithName(\n            Network::UpstreamSubjectAltNames::key())) {\n      callbacks_->streamInfo().filterState()->setData(\n          Network::UpstreamSubjectAltNames::key(),\n          std::make_unique<Network::UpstreamSubjectAltNames>(\n              std::vector<std::string>{std::string(sni_value)}),\n          StreamInfo::FilterState::StateType::Mutable);\n    }\n  }\n\n  transport_socket_options_ = Network::TransportSocketOptionsUtility::fromFilterState(\n      *callbacks_->streamInfo().filterState());\n\n  if (auto downstream_connection = downstreamConnection(); downstream_connection != nullptr) {\n    if (auto typed_state = downstream_connection->streamInfo()\n                               .filterState()\n                               .getDataReadOnly<Network::UpstreamSocketOptionsFilterState>(\n                                   Network::UpstreamSocketOptionsFilterState::key());\n        typed_state != nullptr) {\n      auto downstream_options = typed_state->value();\n      if (!upstream_options_) {\n        upstream_options_ = std::make_shared<Network::Socket::Options>();\n      }\n      Network::Socket::appendOptions(upstream_options_, downstream_options);\n    }\n  }\n\n  if (upstream_options_ && callbacks_->getUpstreamSocketOptions()) {\n    Network::Socket::appendOptions(upstream_options_, callbacks_->getUpstreamSocketOptions());\n  }\n\n  std::unique_ptr<GenericConnPool> generic_conn_pool = createConnPool(*cluster);\n\n  if (!generic_conn_pool) {\n    sendNoHealthyUpstreamResponse();\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n  Upstream::HostDescriptionConstSharedPtr host = generic_conn_pool->host();\n\n  if (debug_config && debug_config->append_upstream_host_) {\n    // The hostname and address will be appended to any local or upstream responses from this point,\n    // possibly in addition to the cluster name.\n    modify_headers = [modify_headers, debug_config, host](Http::ResponseHeaderMap& headers) {\n      modify_headers(headers);\n      headers.addCopy(\n          debug_config->hostname_header_.value_or(Http::Headers::get().EnvoyUpstreamHostname),\n          host->hostname());\n      headers.addCopy(debug_config->host_address_header_.value_or(\n                          Http::Headers::get().EnvoyUpstreamHostAddress),\n                      host->address()->asString());\n    };\n  }\n\n  // If we've been instructed not to forward the request upstream, send an empty local response.\n  if (debug_config && debug_config->do_not_forward_) {\n    modify_headers = [modify_headers, debug_config](Http::ResponseHeaderMap& headers) {\n      modify_headers(headers);\n      headers.addCopy(\n          debug_config->not_forwarded_header_.value_or(Http::Headers::get().EnvoyNotForwarded),\n          \"true\");\n    };\n    callbacks_->sendLocalReply(Http::Code::NoContent, \"\", modify_headers, absl::nullopt, \"\");\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n\n  hedging_params_ = FilterUtility::finalHedgingParams(*route_entry_, headers);\n\n  timeout_ = FilterUtility::finalTimeout(*route_entry_, headers, !config_.suppress_envoy_headers_,\n                                         grpc_request_, hedging_params_.hedge_on_per_try_timeout_,\n                                         config_.respect_expected_rq_timeout_);\n\n  const Http::HeaderEntry* header_max_stream_duration_entry =\n      headers.EnvoyUpstreamStreamDurationMs();\n  if (header_max_stream_duration_entry) {\n    dynamic_max_stream_duration_ =\n        FilterUtility::tryParseHeaderTimeout(*header_max_stream_duration_entry);\n    headers.removeEnvoyUpstreamStreamDurationMs();\n  }\n\n  // If this header is set with any value, use an alternate response code on timeout\n  if (headers.EnvoyUpstreamRequestTimeoutAltResponse()) {\n    timeout_response_code_ = Http::Code::NoContent;\n    headers.removeEnvoyUpstreamRequestTimeoutAltResponse();\n  }\n\n  include_attempt_count_in_request_ = route_entry_->includeAttemptCountInRequest();\n  if (include_attempt_count_in_request_) {\n    headers.setEnvoyAttemptCount(attempt_count_);\n  }\n\n  // The router has reached a point where it is going to try to send a request upstream,\n  // so now modify_headers should attach x-envoy-attempt-count to the downstream response if the\n  // config flag is true.\n  if (route_entry_->includeAttemptCountInResponse()) {\n    modify_headers = [modify_headers, this](Http::ResponseHeaderMap& headers) {\n      modify_headers(headers);\n\n      // This header is added without checking for config_.suppress_envoy_headers_ to mirror what is\n      // done for upstream requests.\n      headers.setEnvoyAttemptCount(attempt_count_);\n    };\n  }\n  callbacks_->streamInfo().setAttemptCount(attempt_count_);\n\n  route_entry_->finalizeRequestHeaders(headers, callbacks_->streamInfo(),\n                                       !config_.suppress_envoy_headers_);\n  FilterUtility::setUpstreamScheme(\n      headers, callbacks_->streamInfo().downstreamAddressProvider().sslConnection() != nullptr);\n\n  // Ensure an http transport scheme is selected before continuing with decoding.\n  ASSERT(headers.Scheme());\n\n  retry_state_ =\n      createRetryState(route_entry_->retryPolicy(), headers, *cluster_, request_vcluster_,\n                       route_stats_context_, config_.runtime_, config_.random_,\n                       callbacks_->dispatcher(), config_.timeSource(), route_entry_->priority());\n\n  // Determine which shadow policies to use. It's possible that we don't do any shadowing due to\n  // runtime keys. Also the method CONNECT doesn't support shadowing.\n  auto method = headers.getMethodValue();\n  if (method != Http::Headers::get().MethodValues.Connect) {\n    for (const auto& shadow_policy : route_entry_->shadowPolicies()) {\n      const auto& policy_ref = *shadow_policy;\n      if (FilterUtility::shouldShadow(policy_ref, config_.runtime_, callbacks_->streamId())) {\n        active_shadow_policies_.push_back(std::cref(policy_ref));\n        shadow_headers_ = Http::createHeaderMap<Http::RequestHeaderMapImpl>(*downstream_headers_);\n      }\n    }\n  }\n\n  ENVOY_STREAM_LOG(debug, \"router decoding headers:\\n{}\", *callbacks_, headers);\n\n  // Hang onto the modify_headers function for later use in handling upstream responses.\n  modify_headers_ = modify_headers;\n\n  const bool can_send_early_data =\n      route_entry_->earlyDataPolicy().allowsEarlyDataForRequest(*downstream_headers_);\n\n  include_timeout_retry_header_in_request_ =\n      route_entry_->virtualHost().includeIsTimeoutRetryHeader();\n\n  // Set initial HTTP/3 use based on the presence of HTTP/1.1 proxy config.\n  // For retries etc, HTTP/3 usability may transition from true to false, but\n  // will never transition from false to true.\n  bool can_use_http3 =\n      !transport_socket_options_ || !transport_socket_options_->http11ProxyInfo().has_value();\n  UpstreamRequestPtr upstream_request = std::make_unique<UpstreamRequest>(\n      *this, std::move(generic_conn_pool), can_send_early_data, can_use_http3);\n  LinkedList::moveIntoList(std::move(upstream_request), upstream_requests_);\n  upstream_requests_.front()->acceptHeadersFromRouter(end_stream);\n  if (streaming_shadows_) {\n    // start the shadow streams.\n    for (const auto& shadow_policy_wrapper : active_shadow_policies_) {\n      const auto& shadow_policy = shadow_policy_wrapper.get();\n      const absl::optional<absl::string_view> shadow_cluster_name =\n          getShadowCluster(shadow_policy, *downstream_headers_);\n      if (!shadow_cluster_name.has_value()) {\n        continue;\n      }\n      auto shadow_headers = Http::createHeaderMap<Http::RequestHeaderMapImpl>(*shadow_headers_);\n      auto options =\n          Http::AsyncClient::RequestOptions()\n              .setTimeout(timeout_.global_timeout_)\n              .setParentSpan(callbacks_->activeSpan())\n              .setChildSpanName(\"mirror\")\n              .setSampled(shadow_policy.traceSampled())\n              .setIsShadow(true)\n              .setBufferAccount(callbacks_->account())\n              // A buffer limit of 1 is set in the case that retry_shadow_buffer_limit_ == 0,\n              // because a buffer limit of zero on async clients is interpreted as no buffer limit.\n              .setBufferLimit(1 > retry_shadow_buffer_limit_ ? 1 : retry_shadow_buffer_limit_);\n      options.setFilterConfig(config_);\n      if (end_stream) {\n        // This is a header-only request, and can be dispatched immediately to the shadow\n        // without waiting.\n        Http::RequestMessagePtr request(new Http::RequestMessageImpl(\n            Http::createHeaderMap<Http::RequestHeaderMapImpl>(*shadow_headers_)));\n        config_.shadowWriter().shadow(std::string(shadow_cluster_name.value()), std::move(request),\n                                      options);\n      } else {\n        Http::AsyncClient::OngoingRequest* shadow_stream = config_.shadowWriter().streamingShadow(\n            std::string(shadow_cluster_name.value()), std::move(shadow_headers), options);\n        if (shadow_stream != nullptr) {\n          shadow_streams_.insert(shadow_stream);\n          shadow_stream->setDestructorCallback(\n              [this, shadow_stream]() { shadow_streams_.erase(shadow_stream); });\n          shadow_stream->setWatermarkCallbacks(*callbacks_);\n        }\n      }\n    }\n  }\n  if (end_stream) {\n    onRequestComplete();\n  }\n\n  return Http::FilterHeadersStatus::StopIteration;\n}\n\nstd::unique_ptr<GenericConnPool>\nFilter::createConnPool(Upstream::ThreadLocalCluster& thread_local_cluster) {\n  GenericConnPoolFactory* factory = nullptr;\n  if (cluster_->upstreamConfig().has_value()) {\n    factory = Envoy::Config::Utility::getFactory<GenericConnPoolFactory>(\n        cluster_->upstreamConfig().ref());\n    ENVOY_BUG(factory != nullptr,\n              fmt::format(\"invalid factory type '{}', failing over to default upstream\",\n                          cluster_->upstreamConfig().ref().DebugString()));\n  }\n  if (!factory) {\n    factory = &config_.router_context_.genericConnPoolFactory();\n  }\n\n  using UpstreamProtocol = Envoy::Router::GenericConnPoolFactory::UpstreamProtocol;\n  UpstreamProtocol upstream_protocol = UpstreamProtocol::HTTP;\n  if (route_entry_->connectConfig().has_value()) {\n    auto method = downstream_headers_->getMethodValue();\n    if (Runtime::runtimeFeatureEnabled(\"envoy.reloadable_features.enable_connect_udp_support\") &&\n        Http::HeaderUtility::isConnectUdpRequest(*downstream_headers_)) {\n      upstream_protocol = UpstreamProtocol::UDP;\n    } else if (method == Http::Headers::get().MethodValues.Connect ||\n               (route_entry_->connectConfig()->allow_post() &&\n                method == Http::Headers::get().MethodValues.Post)) {\n      // Allow POST for proxying raw TCP if it is configured.\n      upstream_protocol = UpstreamProtocol::TCP;\n    }\n  }\n  return factory->createGenericConnPool(thread_local_cluster, upstream_protocol,\n                                        route_entry_->priority(),\n                                        callbacks_->streamInfo().protocol(), this);\n}\n\nvoid Filter::sendNoHealthyUpstreamResponse() {\n  callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::NoHealthyUpstream);\n  chargeUpstreamCode(Http::Code::ServiceUnavailable, nullptr, false);\n  callbacks_->sendLocalReply(Http::Code::ServiceUnavailable, \"no healthy upstream\", modify_headers_,\n                             absl::nullopt,\n                             StreamInfo::ResponseCodeDetails::get().NoHealthyUpstream);\n}\n\nHttp::FilterDataStatus Filter::decodeData(Buffer::Instance& data, bool end_stream) {\n  // upstream_requests_.size() cannot be > 1 because that only happens when a per\n  // try timeout occurs with hedge_on_per_try_timeout enabled but the per\n  // try timeout timer is not started until onRequestComplete(). It could be zero\n  // if the first request attempt has already failed and a retry is waiting for\n  // a backoff timer.\n  ASSERT(upstream_requests_.size() <= 1);\n\n  bool buffering = (retry_state_ && retry_state_->enabled()) ||\n                   (!active_shadow_policies_.empty() && !streaming_shadows_) ||\n                   (route_entry_ && route_entry_->internalRedirectPolicy().enabled());\n  if (buffering &&\n      getLength(callbacks_->decodingBuffer()) + data.length() > retry_shadow_buffer_limit_) {\n    ENVOY_LOG(debug,\n              \"The request payload has at least {} bytes data which exceeds buffer limit {}. Give \"\n              \"up on the retry/shadow.\",\n              getLength(callbacks_->decodingBuffer()) + data.length(), retry_shadow_buffer_limit_);\n    cluster_->trafficStats()->retry_or_shadow_abandoned_.inc();\n    retry_state_.reset();\n    buffering = false;\n    active_shadow_policies_.clear();\n    request_buffer_overflowed_ = true;\n\n    // If we had to abandon buffering and there's no request in progress, abort the request and\n    // clean up. This happens if the initial upstream request failed, and we are currently waiting\n    // for a backoff timer before starting the next upstream attempt.\n    if (upstream_requests_.empty()) {\n      cleanup();\n      callbacks_->sendLocalReply(\n          Http::Code::InsufficientStorage, \"exceeded request buffer limit while retrying upstream\",\n          modify_headers_, absl::nullopt,\n          StreamInfo::ResponseCodeDetails::get().RequestPayloadExceededRetryBufferLimit);\n      return Http::FilterDataStatus::StopIterationNoBuffer;\n    }\n  }\n\n  // If we aren't buffering and there is no active request, an abort should have occurred\n  // already.\n  ASSERT(buffering || !upstream_requests_.empty());\n\n  for (auto* shadow_stream : shadow_streams_) {\n    if (end_stream) {\n      shadow_stream->removeDestructorCallback();\n      shadow_stream->removeWatermarkCallbacks();\n    }\n    Buffer::OwnedImpl copy(data);\n    shadow_stream->sendData(copy, end_stream);\n  }\n  if (end_stream) {\n    shadow_streams_.clear();\n  }\n  if (buffering) {\n    if (!upstream_requests_.empty()) {\n      Buffer::OwnedImpl copy(data);\n      upstream_requests_.front()->acceptDataFromRouter(copy, end_stream);\n    }\n\n    // If we are potentially going to retry or buffer shadow this request we need to buffer.\n    // This will not cause the connection manager to 413 because before we hit the\n    // buffer limit we give up on retries and buffering. We must buffer using addDecodedData()\n    // so that all buffered data is available by the time we do request complete processing and\n    // potentially shadow. Additionally, we can't do a copy here because there's a check down\n    // this stack for whether `data` is the same buffer as already buffered data.\n    callbacks_->addDecodedData(data, true);\n  } else {\n    upstream_requests_.front()->acceptDataFromRouter(data, end_stream);\n  }\n\n  if (end_stream) {\n    onRequestComplete();\n  }\n\n  return Http::FilterDataStatus::StopIterationNoBuffer;\n}\n\nHttp::FilterTrailersStatus Filter::decodeTrailers(Http::RequestTrailerMap& trailers) {\n  ENVOY_STREAM_LOG(debug, \"router decoding trailers:\\n{}\", *callbacks_, trailers);\n\n  if (shadow_headers_) {\n    shadow_trailers_ = Http::createHeaderMap<Http::RequestTrailerMapImpl>(trailers);\n  }\n\n  // upstream_requests_.size() cannot be > 1 because that only happens when a per\n  // try timeout occurs with hedge_on_per_try_timeout enabled but the per\n  // try timeout timer is not started until onRequestComplete(). It could be zero\n  // if the first request attempt has already failed and a retry is waiting for\n  // a backoff timer.\n  ASSERT(upstream_requests_.size() <= 1);\n  downstream_trailers_ = &trailers;\n  if (!upstream_requests_.empty()) {\n    upstream_requests_.front()->acceptTrailersFromRouter(trailers);\n  }\n  for (auto* shadow_stream : shadow_streams_) {\n    shadow_stream->removeDestructorCallback();\n    shadow_stream->removeWatermarkCallbacks();\n    shadow_stream->captureAndSendTrailers(\n        Http::createHeaderMap<Http::RequestTrailerMapImpl>(*shadow_trailers_));\n  }\n  shadow_streams_.clear();\n\n  onRequestComplete();\n  return Http::FilterTrailersStatus::StopIteration;\n}\n\nHttp::FilterMetadataStatus Filter::decodeMetadata(Http::MetadataMap& metadata_map) {\n  Http::MetadataMapPtr metadata_map_ptr = std::make_unique<Http::MetadataMap>(metadata_map);\n  if (!upstream_requests_.empty()) {\n    // TODO(soya3129): Save metadata for retry, redirect and shadowing case.\n    upstream_requests_.front()->acceptMetadataFromRouter(std::move(metadata_map_ptr));\n  }\n  return Http::FilterMetadataStatus::Continue;\n}\n\nvoid Filter::setDecoderFilterCallbacks(Http::StreamDecoderFilterCallbacks& callbacks) {\n  callbacks_ = &callbacks;\n  // As the decoder filter only pushes back via watermarks once data has reached\n  // it, it can latch the current buffer limit and does not need to update the\n  // limit if another filter increases it.\n  //\n  // The default is \"do not limit\". If there are configured (non-zero) buffer\n  // limits, apply them here.\n  if (callbacks_->decoderBufferLimit() != 0) {\n    retry_shadow_buffer_limit_ = callbacks_->decoderBufferLimit();\n  }\n}\n\nvoid Filter::cleanup() {\n  // All callers of cleanup() should have cleaned out the upstream_requests_\n  // list as appropriate.\n  ASSERT(upstream_requests_.empty());\n\n  retry_state_.reset();\n  if (response_timeout_) {\n    response_timeout_->disableTimer();\n    response_timeout_.reset();\n  }\n}\n\nabsl::optional<absl::string_view> Filter::getShadowCluster(const ShadowPolicy& policy,\n                                                           const Http::HeaderMap& headers) const {\n  if (!policy.cluster().empty()) {\n    return policy.cluster();\n  } else {\n    ASSERT(!policy.clusterHeader().get().empty());\n    const auto entry = headers.get(policy.clusterHeader());\n    if (!entry.empty() && !entry[0]->value().empty()) {\n      return entry[0]->value().getStringView();\n    }\n    ENVOY_STREAM_LOG(debug, \"There is no cluster name in header: {}\", *callbacks_,\n                     policy.clusterHeader());\n    return absl::nullopt;\n  }\n}\n\nvoid Filter::maybeDoShadowing() {\n  for (const auto& shadow_policy_wrapper : active_shadow_policies_) {\n    const auto& shadow_policy = shadow_policy_wrapper.get();\n\n    const absl::optional<absl::string_view> shadow_cluster_name =\n        getShadowCluster(shadow_policy, *downstream_headers_);\n\n    // The cluster name got from headers is empty.\n    if (!shadow_cluster_name.has_value()) {\n      continue;\n    }\n\n    Http::RequestMessagePtr request(new Http::RequestMessageImpl(\n        Http::createHeaderMap<Http::RequestHeaderMapImpl>(*shadow_headers_)));\n    if (callbacks_->decodingBuffer()) {\n      request->body().add(*callbacks_->decodingBuffer());\n    }\n    if (shadow_trailers_) {\n      request->trailers(Http::createHeaderMap<Http::RequestTrailerMapImpl>(*shadow_trailers_));\n    }\n\n    auto options = Http::AsyncClient::RequestOptions()\n                       .setTimeout(timeout_.global_timeout_)\n                       .setParentSpan(callbacks_->activeSpan())\n                       .setChildSpanName(\"mirror\")\n                       .setSampled(shadow_policy.traceSampled())\n                       .setIsShadow(true);\n    options.setFilterConfig(config_);\n    config_.shadowWriter().shadow(std::string(shadow_cluster_name.value()), std::move(request),\n                                  options);\n  }\n}\n\nvoid Filter::onRequestComplete() {\n  // This should be called exactly once, when the downstream request has been received in full.\n  ASSERT(!downstream_end_stream_);\n  downstream_end_stream_ = true;\n  Event::Dispatcher& dispatcher = callbacks_->dispatcher();\n  downstream_request_complete_time_ = dispatcher.timeSource().monotonicTime();\n\n  // Possible that we got an immediate reset.\n  if (!upstream_requests_.empty()) {\n    // Even if we got an immediate reset, we could still shadow, but that is a riskier change and\n    // seems unnecessary right now.\n    if (!streaming_shadows_) {\n      maybeDoShadowing();\n    }\n\n    if (timeout_.global_timeout_.count() > 0) {\n      response_timeout_ = dispatcher.createTimer([this]() -> void { onResponseTimeout(); });\n      response_timeout_->enableTimer(timeout_.global_timeout_);\n    }\n\n    for (auto& upstream_request : upstream_requests_) {\n      if (upstream_request->createPerTryTimeoutOnRequestComplete()) {\n        upstream_request->setupPerTryTimeout();\n      }\n    }\n  }\n}\n\nvoid Filter::onDestroy() {\n  // Reset any in-flight upstream requests.\n  resetAll();\n\n  // Unregister from shadow stream notifications and cancel active streams.\n  for (auto* shadow_stream : shadow_streams_) {\n    shadow_stream->removeDestructorCallback();\n    shadow_stream->removeWatermarkCallbacks();\n    shadow_stream->cancel();\n  }\n\n  cleanup();\n}\n\nvoid Filter::onResponseTimeout() {\n  ENVOY_STREAM_LOG(debug, \"upstream timeout\", *callbacks_);\n\n  // Reset any upstream requests that are still in flight.\n  while (!upstream_requests_.empty()) {\n    UpstreamRequestPtr upstream_request =\n        upstream_requests_.back()->removeFromList(upstream_requests_);\n\n    // We want to record the upstream timeouts and increase the stats counters in all the cases.\n    // For example, we also want to record the stats in the case of BiDi streaming APIs where we\n    // might have already seen the headers.\n    cluster_->trafficStats()->upstream_rq_timeout_.inc();\n    if (request_vcluster_) {\n      request_vcluster_->stats().upstream_rq_timeout_.inc();\n    }\n    if (route_stats_context_.has_value()) {\n      route_stats_context_->stats().upstream_rq_timeout_.inc();\n    }\n\n    if (upstream_request->upstreamHost()) {\n      upstream_request->upstreamHost()->stats().rq_timeout_.inc();\n    }\n\n    if (upstream_request->awaitingHeaders()) {\n      if (cluster_->timeoutBudgetStats().has_value()) {\n        // Cancel firing per-try timeout information, because the per-try timeout did not come into\n        // play when the global timeout was hit.\n        upstream_request->recordTimeoutBudget(false);\n      }\n\n      // If this upstream request already hit a \"soft\" timeout, then it\n      // already recorded a timeout into outlier detection. Don't do it again.\n      if (!upstream_request->outlierDetectionTimeoutRecorded()) {\n        updateOutlierDetection(Upstream::Outlier::Result::LocalOriginTimeout, *upstream_request,\n                               absl::optional<uint64_t>(enumToInt(timeout_response_code_)));\n      }\n\n      chargeUpstreamAbort(timeout_response_code_, false, *upstream_request);\n    }\n    upstream_request->resetStream();\n  }\n\n  onUpstreamTimeoutAbort(StreamInfo::CoreResponseFlag::UpstreamRequestTimeout,\n                         StreamInfo::ResponseCodeDetails::get().ResponseTimeout);\n}\n\n// Called when the per try timeout is hit but we didn't reset the request\n// (hedge_on_per_try_timeout enabled).\nvoid Filter::onSoftPerTryTimeout(UpstreamRequest& upstream_request) {\n  // Track this as a timeout for outlier detection purposes even though we didn't\n  // cancel the request yet and might get a 2xx later.\n  updateOutlierDetection(Upstream::Outlier::Result::LocalOriginTimeout, upstream_request,\n                         absl::optional<uint64_t>(enumToInt(timeout_response_code_)));\n  upstream_request.outlierDetectionTimeoutRecorded(true);\n\n  if (!downstream_response_started_ && retry_state_) {\n    RetryStatus retry_status = retry_state_->shouldHedgeRetryPerTryTimeout(\n        [this, can_use_http3 = upstream_request.upstreamStreamOptions().can_use_http3_]() -> void {\n          // Without any knowledge about what's going on in the connection pool, retry the request\n          // with the safest settings which is no early data but keep using or not using alt-svc as\n          // before. In this way, QUIC won't be falsely marked as broken.\n          doRetry(/*can_send_early_data*/ false, can_use_http3, TimeoutRetry::Yes);\n        });\n\n    if (retry_status == RetryStatus::Yes) {\n      runRetryOptionsPredicates(upstream_request);\n      pending_retries_++;\n\n      // Don't increment upstream_host->stats().rq_error_ here, we'll do that\n      // later if 1) we hit global timeout or 2) we get bad response headers\n      // back.\n      upstream_request.retried(true);\n\n      // TODO: cluster stat for hedge attempted.\n    } else if (retry_status == RetryStatus::NoOverflow) {\n      callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::UpstreamOverflow);\n    } else if (retry_status == RetryStatus::NoRetryLimitExceeded) {\n      callbacks_->streamInfo().setResponseFlag(\n          StreamInfo::CoreResponseFlag::UpstreamRetryLimitExceeded);\n    }\n  }\n}\n\nvoid Filter::onPerTryIdleTimeout(UpstreamRequest& upstream_request) {\n  onPerTryTimeoutCommon(upstream_request,\n                        cluster_->trafficStats()->upstream_rq_per_try_idle_timeout_,\n                        StreamInfo::ResponseCodeDetails::get().UpstreamPerTryIdleTimeout);\n}\n\nvoid Filter::onPerTryTimeout(UpstreamRequest& upstream_request) {\n  onPerTryTimeoutCommon(upstream_request, cluster_->trafficStats()->upstream_rq_per_try_timeout_,\n                        StreamInfo::ResponseCodeDetails::get().UpstreamPerTryTimeout);\n}\n\nvoid Filter::onPerTryTimeoutCommon(UpstreamRequest& upstream_request, Stats::Counter& error_counter,\n                                   const std::string& response_code_details) {\n  if (hedging_params_.hedge_on_per_try_timeout_) {\n    onSoftPerTryTimeout(upstream_request);\n    return;\n  }\n\n  error_counter.inc();\n  if (upstream_request.upstreamHost()) {\n    upstream_request.upstreamHost()->stats().rq_timeout_.inc();\n  }\n\n  upstream_request.resetStream();\n\n  updateOutlierDetection(Upstream::Outlier::Result::LocalOriginTimeout, upstream_request,\n                         absl::optional<uint64_t>(enumToInt(timeout_response_code_)));\n\n  if (maybeRetryReset(Http::StreamResetReason::LocalReset, upstream_request, TimeoutRetry::Yes)) {\n    return;\n  }\n\n  chargeUpstreamAbort(timeout_response_code_, false, upstream_request);\n\n  // Remove this upstream request from the list now that we're done with it.\n  upstream_request.removeFromList(upstream_requests_);\n  onUpstreamTimeoutAbort(StreamInfo::CoreResponseFlag::UpstreamRequestTimeout,\n                         response_code_details);\n}\n\nvoid Filter::onStreamMaxDurationReached(UpstreamRequest& upstream_request) {\n  upstream_request.resetStream();\n\n  if (maybeRetryReset(Http::StreamResetReason::LocalReset, upstream_request, TimeoutRetry::No)) {\n    return;\n  }\n\n  upstream_request.removeFromList(upstream_requests_);\n  cleanup();\n\n  callbacks_->streamInfo().setResponseFlag(\n      StreamInfo::CoreResponseFlag::UpstreamMaxStreamDurationReached);\n  // Grab the const ref to call the const method of StreamInfo.\n  const auto& stream_info = callbacks_->streamInfo();\n  const bool downstream_decode_complete =\n      stream_info.downstreamTiming().has_value() &&\n      stream_info.downstreamTiming().value().get().lastDownstreamRxByteReceived().has_value();\n\n  // sendLocalReply may instead reset the stream if downstream_response_started_ is true.\n  callbacks_->sendLocalReply(\n      Http::Utility::maybeRequestTimeoutCode(downstream_decode_complete),\n      \"upstream max stream duration reached\", modify_headers_, absl::nullopt,\n      StreamInfo::ResponseCodeDetails::get().UpstreamMaxStreamDurationReached);\n}\n\nvoid Filter::updateOutlierDetection(Upstream::Outlier::Result result,\n                                    UpstreamRequest& upstream_request,\n                                    absl::optional<uint64_t> code) {\n  if (upstream_request.upstreamHost()) {\n    upstream_request.upstreamHost()->outlierDetector().putResult(result, code);\n  }\n}\n\nvoid Filter::chargeUpstreamAbort(Http::Code code, bool dropped, UpstreamRequest& upstream_request) {\n  if (downstream_response_started_) {\n    if (upstream_request.grpcRqSuccessDeferred()) {\n      upstream_request.upstreamHost()->stats().rq_error_.inc();\n      stats_.rq_reset_after_downstream_response_started_.inc();\n    }\n  } else {\n    Upstream::HostDescriptionConstSharedPtr upstream_host = upstream_request.upstreamHost();\n\n    chargeUpstreamCode(code, upstream_host, dropped);\n    // If we had non-5xx but still have been reset by backend or timeout before\n    // starting response, we treat this as an error. We only get non-5xx when\n    // timeout_response_code_ is used for code above, where this member can\n    // assume values such as 204 (NoContent).\n    if (upstream_host != nullptr && !Http::CodeUtility::is5xx(enumToInt(code))) {\n      upstream_host->stats().rq_error_.inc();\n    }\n  }\n}\n\nvoid Filter::onUpstreamTimeoutAbort(StreamInfo::CoreResponseFlag response_flags,\n                                    absl::string_view details) {\n  Upstream::ClusterTimeoutBudgetStatsOptRef tb_stats = cluster()->timeoutBudgetStats();\n  if (tb_stats.has_value()) {\n    Event::Dispatcher& dispatcher = callbacks_->dispatcher();\n    std::chrono::milliseconds response_time = std::chrono::duration_cast<std::chrono::milliseconds>(\n        dispatcher.timeSource().monotonicTime() - downstream_request_complete_time_);\n\n    tb_stats->get().upstream_rq_timeout_budget_percent_used_.recordValue(\n        FilterUtility::percentageOfTimeout(response_time, timeout_.global_timeout_));\n  }\n\n  const absl::string_view body =\n      timeout_response_code_ == Http::Code::GatewayTimeout ? \"upstream request timeout\" : \"\";\n  onUpstreamAbort(timeout_response_code_, response_flags, body, false, details);\n}\n\nvoid Filter::onUpstreamAbort(Http::Code code, StreamInfo::CoreResponseFlag response_flags,\n                             absl::string_view body, bool dropped, absl::string_view details) {\n  // If we have not yet sent anything downstream, send a response with an appropriate status code.\n  // Otherwise just reset the ongoing response.\n  callbacks_->streamInfo().setResponseFlag(response_flags);\n  // This will destroy any created retry timers.\n  cleanup();\n  // sendLocalReply may instead reset the stream if downstream_response_started_ is true.\n  callbacks_->sendLocalReply(\n      code, body,\n      [dropped, this](Http::ResponseHeaderMap& headers) {\n        if (dropped && !config_.suppress_envoy_headers_) {\n          headers.addReference(Http::Headers::get().EnvoyOverloaded,\n                               Http::Headers::get().EnvoyOverloadedValues.True);\n        }\n        modify_headers_(headers);\n      },\n      absl::nullopt, details);\n}\n\nbool Filter::maybeRetryReset(Http::StreamResetReason reset_reason,\n                             UpstreamRequest& upstream_request, TimeoutRetry is_timeout_retry) {\n  // We don't retry if we already started the response, don't have a retry policy defined,\n  // or if we've already retried this upstream request (currently only possible if a per\n  // try timeout occurred and hedge_on_per_try_timeout is enabled).\n  if (downstream_response_started_ || !retry_state_ || upstream_request.retried()) {\n    return false;\n  }\n  RetryState::Http3Used was_using_http3 = RetryState::Http3Used::Unknown;\n  if (upstream_request.hadUpstream()) {\n    was_using_http3 = (upstream_request.streamInfo().protocol().has_value() &&\n                       upstream_request.streamInfo().protocol().value() == Http::Protocol::Http3)\n                          ? RetryState::Http3Used::Yes\n                          : RetryState::Http3Used::No;\n  }\n  const RetryStatus retry_status = retry_state_->shouldRetryReset(\n      reset_reason, was_using_http3,\n      [this, can_send_early_data = upstream_request.upstreamStreamOptions().can_send_early_data_,\n       can_use_http3 = upstream_request.upstreamStreamOptions().can_use_http3_,\n       is_timeout_retry](bool disable_http3) -> void {\n        // This retry might be because of ConnectionFailure of 0-RTT handshake. In this case, though\n        // the original request is retried with the same can_send_early_data setting, it will not be\n        // sent as early data by the underlying connection pool grid.\n        doRetry(can_send_early_data, disable_http3 ? false : can_use_http3, is_timeout_retry);\n      });\n  if (retry_status == RetryStatus::Yes) {\n    runRetryOptionsPredicates(upstream_request);\n    pending_retries_++;\n\n    if (upstream_request.upstreamHost()) {\n      upstream_request.upstreamHost()->stats().rq_error_.inc();\n    }\n\n    auto request_ptr = upstream_request.removeFromList(upstream_requests_);\n    callbacks_->dispatcher().deferredDelete(std::move(request_ptr));\n    return true;\n  } else if (retry_status == RetryStatus::NoOverflow) {\n    callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::UpstreamOverflow);\n  } else if (retry_status == RetryStatus::NoRetryLimitExceeded) {\n    callbacks_->streamInfo().setResponseFlag(\n        StreamInfo::CoreResponseFlag::UpstreamRetryLimitExceeded);\n  }\n\n  return false;\n}\n\nvoid Filter::onUpstreamReset(Http::StreamResetReason reset_reason,\n                             absl::string_view transport_failure_reason,\n                             UpstreamRequest& upstream_request) {\n  ENVOY_STREAM_LOG(debug, \"upstream reset: reset reason: {}, transport failure reason: {}\",\n                   *callbacks_, Http::Utility::resetReasonToString(reset_reason),\n                   transport_failure_reason);\n\n  const bool dropped = reset_reason == Http::StreamResetReason::Overflow;\n\n  // Ignore upstream reset caused by a resource overflow.\n  // Currently, circuit breakers can only produce this reset reason.\n  // It means that this reason is cluster-wise, not upstream-related.\n  // Therefore removing an upstream in the case of an overloaded cluster\n  // would make the situation even worse.\n  // https://github.com/envoyproxy/envoy/issues/25487\n  if (!dropped) {\n    // TODO: The reset may also come from upstream over the wire. In this case it should be\n    // treated as external origin error and distinguished from local origin error.\n    // This matters only when running OutlierDetection with split_external_local_origin_errors\n    // config param set to true.\n    updateOutlierDetection(Upstream::Outlier::Result::LocalOriginConnectFailed, upstream_request,\n                           absl::nullopt);\n  }\n\n  if (maybeRetryReset(reset_reason, upstream_request, TimeoutRetry::No)) {\n    return;\n  }\n\n  const Http::Code error_code = (reset_reason == Http::StreamResetReason::ProtocolError)\n                                    ? Http::Code::BadGateway\n                                    : Http::Code::ServiceUnavailable;\n  chargeUpstreamAbort(error_code, dropped, upstream_request);\n  auto request_ptr = upstream_request.removeFromList(upstream_requests_);\n  callbacks_->dispatcher().deferredDelete(std::move(request_ptr));\n\n  // If there are other in-flight requests that might see an upstream response,\n  // don't return anything downstream.\n  if (numRequestsAwaitingHeaders() > 0 || pending_retries_ > 0) {\n    return;\n  }\n\n  const StreamInfo::CoreResponseFlag response_flags = streamResetReasonToResponseFlag(reset_reason);\n\n  const std::string body =\n      absl::StrCat(\"upstream connect error or disconnect/reset before headers. \",\n                   (is_retry_ ? \"retried and the latest \" : \"\"),\n                   \"reset reason: \", Http::Utility::resetReasonToString(reset_reason),\n                   !transport_failure_reason.empty() ? \", transport failure reason: \" : \"\",\n                   transport_failure_reason);\n  const std::string& basic_details =\n      downstream_response_started_ ? StreamInfo::ResponseCodeDetails::get().LateUpstreamReset\n                                   : StreamInfo::ResponseCodeDetails::get().EarlyUpstreamReset;\n  const std::string details = StringUtil::replaceAllEmptySpace(absl::StrCat(\n      basic_details, \"{\", Http::Utility::resetReasonToString(reset_reason),\n      transport_failure_reason.empty() ? \"\" : absl::StrCat(\",\", transport_failure_reason), \"}\"));\n  onUpstreamAbort(error_code, response_flags, body, dropped, details);\n}\n\nvoid Filter::onUpstreamHostSelected(Upstream::HostDescriptionConstSharedPtr host,\n                                    bool pool_success) {\n  if (retry_state_ && host) {\n    retry_state_->onHostAttempted(host);\n  }\n\n  if (!pool_success) {\n    return;\n  }\n\n  if (request_vcluster_) {\n    // The cluster increases its upstream_rq_total_ counter right before firing this onPoolReady\n    // callback. Hence, the upstream request increases the virtual cluster's upstream_rq_total_ stat\n    // here.\n    request_vcluster_->stats().upstream_rq_total_.inc();\n  }\n  if (route_stats_context_.has_value()) {\n    // The cluster increases its upstream_rq_total_ counter right before firing this onPoolReady\n    // callback. Hence, the upstream request increases the route level upstream_rq_total_ stat\n    // here.\n    route_stats_context_->stats().upstream_rq_total_.inc();\n  }\n}\n\nStreamInfo::CoreResponseFlag\nFilter::streamResetReasonToResponseFlag(Http::StreamResetReason reset_reason) {\n  switch (reset_reason) {\n  case Http::StreamResetReason::LocalConnectionFailure:\n  case Http::StreamResetReason::RemoteConnectionFailure:\n  case Http::StreamResetReason::ConnectionTimeout:\n    return StreamInfo::CoreResponseFlag::UpstreamConnectionFailure;\n  case Http::StreamResetReason::ConnectionTermination:\n    return StreamInfo::CoreResponseFlag::UpstreamConnectionTermination;\n  case Http::StreamResetReason::LocalReset:\n  case Http::StreamResetReason::LocalRefusedStreamReset:\n    return StreamInfo::CoreResponseFlag::LocalReset;\n  case Http::StreamResetReason::Overflow:\n    return StreamInfo::CoreResponseFlag::UpstreamOverflow;\n  case Http::StreamResetReason::RemoteReset:\n  case Http::StreamResetReason::RemoteRefusedStreamReset:\n  case Http::StreamResetReason::ConnectError:\n    return StreamInfo::CoreResponseFlag::UpstreamRemoteReset;\n  case Http::StreamResetReason::ProtocolError:\n    return StreamInfo::CoreResponseFlag::UpstreamProtocolError;\n  case Http::StreamResetReason::OverloadManager:\n    return StreamInfo::CoreResponseFlag::OverloadManager;\n  }\n\n  PANIC_DUE_TO_CORRUPT_ENUM;\n}\n\nvoid Filter::handleNon5xxResponseHeaders(absl::optional<Grpc::Status::GrpcStatus> grpc_status,\n                                         UpstreamRequest& upstream_request, bool end_stream,\n                                         uint64_t grpc_to_http_status) {\n  // We need to defer gRPC success until after we have processed grpc-status in\n  // the trailers.\n  if (grpc_request_) {\n    if (end_stream) {\n      if (grpc_status && !Http::CodeUtility::is5xx(grpc_to_http_status)) {\n        upstream_request.upstreamHost()->stats().rq_success_.inc();\n      } else {\n        upstream_request.upstreamHost()->stats().rq_error_.inc();\n      }\n    } else {\n      upstream_request.grpcRqSuccessDeferred(true);\n    }\n  } else {\n    upstream_request.upstreamHost()->stats().rq_success_.inc();\n  }\n}\n\nvoid Filter::onUpstream1xxHeaders(Http::ResponseHeaderMapPtr&& headers,\n                                  UpstreamRequest& upstream_request) {\n  const uint64_t response_code = Http::Utility::getResponseStatus(*headers);\n  chargeUpstreamCode(response_code, *headers, upstream_request.upstreamHost(), false);\n  ENVOY_STREAM_LOG(debug, \"upstream 1xx ({}).\", *callbacks_, response_code);\n\n  downstream_response_started_ = true;\n  final_upstream_request_ = &upstream_request;\n  resetOtherUpstreams(upstream_request);\n\n  // Don't send retries after 100-Continue has been sent on. Arguably we could attempt to do a\n  // retry, assume the next upstream would also send an 100-Continue and swallow the second one\n  // but it's sketchy (as the subsequent upstream might not send a 100-Continue) and not worth\n  // the complexity until someone asks for it.\n  retry_state_.reset();\n\n  callbacks_->encode1xxHeaders(std::move(headers));\n}\n\nvoid Filter::resetAll() {\n  while (!upstream_requests_.empty()) {\n    auto request_ptr = upstream_requests_.back()->removeFromList(upstream_requests_);\n    request_ptr->resetStream();\n    callbacks_->dispatcher().deferredDelete(std::move(request_ptr));\n  }\n}\n\nvoid Filter::resetOtherUpstreams(UpstreamRequest& upstream_request) {\n  // Pop each upstream request on the list and reset it if it's not the one\n  // provided. At the end we'll move it back into the list.\n  UpstreamRequestPtr final_upstream_request;\n  while (!upstream_requests_.empty()) {\n    UpstreamRequestPtr upstream_request_tmp =\n        upstream_requests_.back()->removeFromList(upstream_requests_);\n    if (upstream_request_tmp.get() != &upstream_request) {\n      upstream_request_tmp->resetStream();\n      // TODO: per-host stat for hedge abandoned.\n      // TODO: cluster stat for hedge abandoned.\n    } else {\n      final_upstream_request = std::move(upstream_request_tmp);\n    }\n  }\n\n  ASSERT(final_upstream_request);\n  // Now put the final request back on this list.\n  LinkedList::moveIntoList(std::move(final_upstream_request), upstream_requests_);\n}\n\nvoid Filter::onUpstreamHeaders(uint64_t response_code, Http::ResponseHeaderMapPtr&& headers,\n                               UpstreamRequest& upstream_request, bool end_stream) {\n  ENVOY_STREAM_LOG(debug, \"upstream headers complete: end_stream={}\", *callbacks_, end_stream);\n\n  modify_headers_(*headers);\n  // When grpc-status appears in response headers, convert grpc-status to HTTP status code\n  // for outlier detection. This does not currently change any stats or logging and does not\n  // handle the case when an error grpc-status is sent as a trailer.\n  absl::optional<Grpc::Status::GrpcStatus> grpc_status;\n  uint64_t grpc_to_http_status = 0;\n  if (grpc_request_) {\n    grpc_status = Grpc::Common::getGrpcStatus(*headers);\n    if (grpc_status.has_value()) {\n      grpc_to_http_status = Grpc::Utility::grpcToHttpStatus(grpc_status.value());\n    }\n  }\n\n  if (grpc_status.has_value()) {\n    upstream_request.upstreamHost()->outlierDetector().putHttpResponseCode(grpc_to_http_status);\n  } else {\n    upstream_request.upstreamHost()->outlierDetector().putHttpResponseCode(response_code);\n  }\n\n  if (headers->EnvoyImmediateHealthCheckFail() != nullptr) {\n    upstream_request.upstreamHost()->healthChecker().setUnhealthy(\n        Upstream::HealthCheckHostMonitor::UnhealthyType::ImmediateHealthCheckFail);\n  }\n\n  bool could_not_retry = false;\n\n  // Check if this upstream request was already retried, for instance after\n  // hitting a per try timeout. Don't retry it if we already have.\n  if (retry_state_) {\n    if (upstream_request.retried()) {\n      // We already retried this request (presumably for a per try timeout) so\n      // we definitely won't retry it again. Check if we would have retried it\n      // if we could.\n      bool retry_as_early_data; // Not going to be used as we are not retrying.\n      could_not_retry = retry_state_->wouldRetryFromHeaders(*headers, *downstream_headers_,\n                                                            retry_as_early_data) !=\n                        RetryState::RetryDecision::NoRetry;\n    } else {\n      const RetryStatus retry_status = retry_state_->shouldRetryHeaders(\n          *headers, *downstream_headers_,\n          [this, can_use_http3 = upstream_request.upstreamStreamOptions().can_use_http3_,\n           had_early_data = upstream_request.upstreamStreamOptions().can_send_early_data_](\n              bool disable_early_data) -> void {\n            doRetry((disable_early_data ? false : had_early_data), can_use_http3, TimeoutRetry::No);\n          });\n      if (retry_status == RetryStatus::Yes) {\n        runRetryOptionsPredicates(upstream_request);\n        pending_retries_++;\n        upstream_request.upstreamHost()->stats().rq_error_.inc();\n        Http::CodeStats& code_stats = httpContext().codeStats();\n        code_stats.chargeBasicResponseStat(cluster_->statsScope(), stats_.stat_names_.retry_,\n                                           static_cast<Http::Code>(response_code),\n                                           exclude_http_code_stats_);\n\n        if (!end_stream || !upstream_request.encodeComplete()) {\n          upstream_request.resetStream();\n        }\n        auto request_ptr = upstream_request.removeFromList(upstream_requests_);\n        callbacks_->dispatcher().deferredDelete(std::move(request_ptr));\n        return;\n      } else if (retry_status == RetryStatus::NoOverflow) {\n        callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::UpstreamOverflow);\n        could_not_retry = true;\n      } else if (retry_status == RetryStatus::NoRetryLimitExceeded) {\n        callbacks_->streamInfo().setResponseFlag(\n            StreamInfo::CoreResponseFlag::UpstreamRetryLimitExceeded);\n        could_not_retry = true;\n      }\n    }\n  }\n\n  if (route_entry_->internalRedirectPolicy().enabled() &&\n      route_entry_->internalRedirectPolicy().shouldRedirectForResponseCode(\n          static_cast<Http::Code>(response_code)) &&\n      setupRedirect(*headers)) {\n    return;\n    // If the redirect could not be handled, fail open and let it pass to the\n    // next downstream.\n  }\n\n  // Check if we got a \"bad\" response, but there are still upstream requests in\n  // flight awaiting headers or scheduled retries. If so, exit to give them a\n  // chance to return before returning a response downstream.\n  if (could_not_retry && (numRequestsAwaitingHeaders() > 0 || pending_retries_ > 0)) {\n    upstream_request.upstreamHost()->stats().rq_error_.inc();\n\n    // Reset the stream because there are other in-flight requests that we'll\n    // wait around for and we're not interested in consuming any body/trailers.\n    auto request_ptr = upstream_request.removeFromList(upstream_requests_);\n    request_ptr->resetStream();\n    callbacks_->dispatcher().deferredDelete(std::move(request_ptr));\n    return;\n  }\n\n  // Make sure any retry timers are destroyed since we may not call cleanup() if end_stream is\n  // false.\n  if (retry_state_) {\n    retry_state_.reset();\n  }\n\n  // Only send upstream service time if we received the complete request and this is not a\n  // premature response.\n  if (DateUtil::timePointValid(downstream_request_complete_time_)) {\n    Event::Dispatcher& dispatcher = callbacks_->dispatcher();\n    MonotonicTime response_received_time = dispatcher.timeSource().monotonicTime();\n    std::chrono::milliseconds ms = std::chrono::duration_cast<std::chrono::milliseconds>(\n        response_received_time - downstream_request_complete_time_);\n    if (!config_.suppress_envoy_headers_) {\n      headers->setEnvoyUpstreamServiceTime(ms.count());\n    }\n  }\n\n  upstream_request.upstreamCanary(\n      (headers->EnvoyUpstreamCanary() && headers->EnvoyUpstreamCanary()->value() == \"true\") ||\n      upstream_request.upstreamHost()->canary());\n  chargeUpstreamCode(response_code, *headers, upstream_request.upstreamHost(), false);\n  if (!Http::CodeUtility::is5xx(response_code)) {\n    handleNon5xxResponseHeaders(grpc_status, upstream_request, end_stream, grpc_to_http_status);\n  }\n\n  // Append routing cookies\n  for (const auto& header_value : downstream_set_cookies_) {\n    headers->addReferenceKey(Http::Headers::get().SetCookie, header_value);\n  }\n\n  callbacks_->streamInfo().setResponseCodeDetails(\n      StreamInfo::ResponseCodeDetails::get().ViaUpstream);\n\n  if (Runtime::runtimeFeatureEnabled(\n          \"envoy.reloadable_features.copy_response_code_to_downstream_stream_info\")) {\n    callbacks_->streamInfo().setResponseCode(response_code);\n  }\n\n  // TODO(zuercher): If access to response_headers_to_add (at any level) is ever needed outside\n  // Router::Filter we'll need to find a better location for this work. One possibility is to\n  // provide finalizeResponseHeaders functions on the Router::Config and VirtualHost interfaces.\n  route_entry_->finalizeResponseHeaders(*headers, callbacks_->streamInfo());\n\n  downstream_response_started_ = true;\n  final_upstream_request_ = &upstream_request;\n  // Make sure that for request hedging, we end up with the correct final upstream info.\n  callbacks_->streamInfo().setUpstreamInfo(final_upstream_request_->streamInfo().upstreamInfo());\n  resetOtherUpstreams(upstream_request);\n  if (end_stream) {\n    onUpstreamComplete(upstream_request);\n  }\n\n  callbacks_->encodeHeaders(std::move(headers), end_stream,\n                            StreamInfo::ResponseCodeDetails::get().ViaUpstream);\n}\n\nvoid Filter::onUpstreamData(Buffer::Instance& data, UpstreamRequest& upstream_request,\n                            bool end_stream) {\n  // This should be true because when we saw headers we either reset the stream\n  // (hence wouldn't have made it to onUpstreamData) or all other in-flight\n  // streams.\n  ASSERT(upstream_requests_.size() == 1);\n  if (end_stream) {\n    // gRPC request termination without trailers is an error.\n    if (upstream_request.grpcRqSuccessDeferred()) {\n      upstream_request.upstreamHost()->stats().rq_error_.inc();\n    }\n    onUpstreamComplete(upstream_request);\n  }\n\n  callbacks_->encodeData(data, end_stream);\n}\n\nvoid Filter::onUpstreamTrailers(Http::ResponseTrailerMapPtr&& trailers,\n                                UpstreamRequest& upstream_request) {\n  // This should be true because when we saw headers we either reset the stream\n  // (hence wouldn't have made it to onUpstreamTrailers) or all other in-flight\n  // streams.\n  ASSERT(upstream_requests_.size() == 1);\n\n  if (upstream_request.grpcRqSuccessDeferred()) {\n    absl::optional<Grpc::Status::GrpcStatus> grpc_status = Grpc::Common::getGrpcStatus(*trailers);\n    if (grpc_status &&\n        !Http::CodeUtility::is5xx(Grpc::Utility::grpcToHttpStatus(grpc_status.value()))) {\n      upstream_request.upstreamHost()->stats().rq_success_.inc();\n    } else {\n      upstream_request.upstreamHost()->stats().rq_error_.inc();\n    }\n  }\n\n  onUpstreamComplete(upstream_request);\n\n  callbacks_->encodeTrailers(std::move(trailers));\n}\n\nvoid Filter::onUpstreamMetadata(Http::MetadataMapPtr&& metadata_map) {\n  callbacks_->encodeMetadata(std::move(metadata_map));\n}\n\nvoid Filter::onUpstreamComplete(UpstreamRequest& upstream_request) {\n  if (!downstream_end_stream_) {\n    upstream_request.resetStream();\n  }\n  Event::Dispatcher& dispatcher = callbacks_->dispatcher();\n  std::chrono::milliseconds response_time = std::chrono::duration_cast<std::chrono::milliseconds>(\n      dispatcher.timeSource().monotonicTime() - downstream_request_complete_time_);\n\n  Upstream::ClusterTimeoutBudgetStatsOptRef tb_stats = cluster()->timeoutBudgetStats();\n  if (tb_stats.has_value()) {\n    tb_stats->get().upstream_rq_timeout_budget_percent_used_.recordValue(\n        FilterUtility::percentageOfTimeout(response_time, timeout_.global_timeout_));\n  }\n\n  if (config_.emit_dynamic_stats_ && !callbacks_->streamInfo().healthCheck() &&\n      DateUtil::timePointValid(downstream_request_complete_time_)) {\n    upstream_request.upstreamHost()->outlierDetector().putResponseTime(response_time);\n    const bool internal_request = Http::HeaderUtility::isEnvoyInternalRequest(*downstream_headers_);\n\n    Http::CodeStats& code_stats = httpContext().codeStats();\n    Http::CodeStats::ResponseTimingInfo info{\n        config_.scope_,\n        cluster_->statsScope(),\n        config_.empty_stat_name_,\n        response_time,\n        upstream_request.upstreamCanary(),\n        internal_request,\n        route_entry_->virtualHost().statName(),\n        request_vcluster_ ? request_vcluster_->statName() : config_.empty_stat_name_,\n        route_stats_context_.has_value() ? route_stats_context_->statName()\n                                         : config_.empty_stat_name_,\n        config_.zone_name_,\n        upstreamZone(upstream_request.upstreamHost())};\n\n    code_stats.chargeResponseTiming(info);\n\n    if (alt_stat_prefix_ != nullptr) {\n      Http::CodeStats::ResponseTimingInfo info{config_.scope_,\n                                               cluster_->statsScope(),\n                                               alt_stat_prefix_->statName(),\n                                               response_time,\n                                               upstream_request.upstreamCanary(),\n                                               internal_request,\n                                               config_.empty_stat_name_,\n                                               config_.empty_stat_name_,\n                                               config_.empty_stat_name_,\n                                               config_.zone_name_,\n                                               upstreamZone(upstream_request.upstreamHost())};\n\n      code_stats.chargeResponseTiming(info);\n    }\n  }\n\n  // Defer deletion as this is generally called under the stack of the upstream\n  // request, and immediate deletion is dangerous.\n  callbacks_->dispatcher().deferredDelete(upstream_request.removeFromList(upstream_requests_));\n  cleanup();\n}\n\nbool Filter::setupRedirect(const Http::ResponseHeaderMap& headers) {\n  ENVOY_STREAM_LOG(debug, \"attempting internal redirect\", *callbacks_);\n  const Http::HeaderEntry* location = headers.Location();\n\n  const uint64_t status_code = Http::Utility::getResponseStatus(headers);\n\n  // Redirects are not supported for streaming requests yet.\n  if (downstream_end_stream_ && (!request_buffer_overflowed_ || !callbacks_->decodingBuffer()) &&\n      location != nullptr &&\n      convertRequestHeadersForInternalRedirect(*downstream_headers_, headers, *location,\n                                               status_code) &&\n      callbacks_->recreateStream(&headers)) {\n    ENVOY_STREAM_LOG(debug, \"Internal redirect succeeded\", *callbacks_);\n    cluster_->trafficStats()->upstream_internal_redirect_succeeded_total_.inc();\n    return true;\n  }\n  // convertRequestHeadersForInternalRedirect logs failure reasons but log\n  // details for other failure modes here.\n  if (!downstream_end_stream_) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: request incomplete\", *callbacks_);\n  } else if (request_buffer_overflowed_) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: request body overflow\", *callbacks_);\n  } else if (location == nullptr) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: missing location header\", *callbacks_);\n  }\n\n  cluster_->trafficStats()->upstream_internal_redirect_failed_total_.inc();\n  return false;\n}\n\nbool Filter::convertRequestHeadersForInternalRedirect(\n    Http::RequestHeaderMap& downstream_headers, const Http::ResponseHeaderMap& upstream_headers,\n    const Http::HeaderEntry& internal_redirect, uint64_t status_code) {\n  if (!downstream_headers.Path()) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: no path in downstream_headers\", *callbacks_);\n    return false;\n  }\n\n  absl::string_view redirect_url = internal_redirect.value().getStringView();\n  // Make sure the redirect response contains a URL to redirect to.\n  if (redirect_url.empty()) {\n    stats_.passthrough_internal_redirect_bad_location_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: empty location\", *callbacks_);\n    return false;\n  }\n  Http::Utility::Url absolute_url;\n  if (!absolute_url.initialize(redirect_url, false)) {\n    stats_.passthrough_internal_redirect_bad_location_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: invalid location {}\", *callbacks_,\n                     redirect_url);\n    return false;\n  }\n\n  const auto& policy = route_entry_->internalRedirectPolicy();\n  // Don't change the scheme from the original request\n  const bool scheme_is_http = schemeIsHttp(downstream_headers, callbacks_->connection());\n  const bool target_is_http = Http::Utility::schemeIsHttp(absolute_url.scheme());\n  if (!policy.isCrossSchemeRedirectAllowed() && scheme_is_http != target_is_http) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: incorrect scheme for {}\", *callbacks_,\n                     redirect_url);\n    stats_.passthrough_internal_redirect_unsafe_scheme_.inc();\n    return false;\n  }\n\n  const StreamInfo::FilterStateSharedPtr& filter_state = callbacks_->streamInfo().filterState();\n  // Make sure that performing the redirect won't result in exceeding the configured number of\n  // redirects allowed for this route.\n  StreamInfo::UInt32Accessor* num_internal_redirect{};\n\n  if (num_internal_redirect = filter_state->getDataMutable<StreamInfo::UInt32Accessor>(\n          NumInternalRedirectsFilterStateName);\n      num_internal_redirect == nullptr) {\n    auto state = std::make_shared<StreamInfo::UInt32AccessorImpl>(0);\n    num_internal_redirect = state.get();\n\n    filter_state->setData(NumInternalRedirectsFilterStateName, std::move(state),\n                          StreamInfo::FilterState::StateType::Mutable,\n                          StreamInfo::FilterState::LifeSpan::Request);\n  }\n\n  if (num_internal_redirect->value() >= policy.maxInternalRedirects()) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: redirect limits exceeded.\", *callbacks_);\n    stats_.passthrough_internal_redirect_too_many_redirects_.inc();\n    return false;\n  }\n  // Copy the old values, so they can be restored if the redirect fails.\n  const bool scheme_is_set = (downstream_headers.Scheme() != nullptr);\n\n  std::unique_ptr<Http::RequestHeaderMapImpl> saved_headers = Http::RequestHeaderMapImpl::create();\n  Http::RequestHeaderMapImpl::copyFrom(*saved_headers, downstream_headers);\n\n  for (const Http::LowerCaseString& header :\n       route_entry_->internalRedirectPolicy().responseHeadersToCopy()) {\n    Http::HeaderMap::GetResult result = upstream_headers.get(header);\n    Http::HeaderMap::GetResult downstream_result = downstream_headers.get(header);\n    if (result.empty()) {\n      // Clear headers if present, else do nothing:\n      if (downstream_result.empty()) {\n        continue;\n      }\n      downstream_headers.remove(header);\n    } else {\n      // The header exists in the response, copy into the downstream headers\n      if (!downstream_result.empty()) {\n        downstream_headers.remove(header);\n      }\n      for (size_t idx = 0; idx < result.size(); idx++) {\n        downstream_headers.addCopy(header, result[idx]->value().getStringView());\n      }\n    }\n  }\n\n  Cleanup restore_original_headers(\n      [&downstream_headers, scheme_is_set, scheme_is_http, &saved_headers]() {\n        downstream_headers.clear();\n        if (scheme_is_set) {\n          downstream_headers.setScheme(scheme_is_http ? Http::Headers::get().SchemeValues.Http\n                                                      : Http::Headers::get().SchemeValues.Https);\n        }\n\n        Http::RequestHeaderMapImpl::copyFrom(downstream_headers, *saved_headers);\n      });\n\n  // Replace the original host, scheme and path.\n  downstream_headers.setScheme(absolute_url.scheme());\n  downstream_headers.setHost(absolute_url.hostAndPort());\n\n  auto path_and_query = absolute_url.pathAndQueryParams();\n  if (Runtime::runtimeFeatureEnabled(\"envoy.reloadable_features.http_reject_path_with_fragment\")) {\n    // Envoy treats internal redirect as a new request and will reject it if URI path\n    // contains #fragment. However the Location header is allowed to have #fragment in URI path. To\n    // prevent Envoy from rejecting internal redirect, strip the #fragment from Location URI if it\n    // is present.\n    auto fragment_pos = path_and_query.find('#');\n    path_and_query = path_and_query.substr(0, fragment_pos);\n  }\n  downstream_headers.setPath(path_and_query);\n\n  // Only clear the route cache if there are downstream callbacks. There aren't, for example,\n  // for async connections.\n  if (callbacks_->downstreamCallbacks()) {\n    callbacks_->downstreamCallbacks()->clearRouteCache();\n  }\n  const auto route = callbacks_->route();\n  // Don't allow a redirect to a non existing route.\n  if (!route) {\n    stats_.passthrough_internal_redirect_no_route_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: no route found\", *callbacks_);\n    return false;\n  }\n\n  const auto& route_name = route->routeName();\n  for (const auto& predicate : policy.predicates()) {\n    if (!predicate->acceptTargetRoute(*filter_state, route_name, !scheme_is_http,\n                                      !target_is_http)) {\n      stats_.passthrough_internal_redirect_predicate_.inc();\n      ENVOY_STREAM_LOG(trace,\n                       \"Internal redirect failed: rejecting redirect targeting {}, by {} predicate\",\n                       *callbacks_, route_name, predicate->name());\n      return false;\n    }\n  }\n\n  // See https://tools.ietf.org/html/rfc7231#section-6.4.4.\n  if (status_code == enumToInt(Http::Code::SeeOther) &&\n      downstream_headers.getMethodValue() != Http::Headers::get().MethodValues.Get &&\n      downstream_headers.getMethodValue() != Http::Headers::get().MethodValues.Head) {\n    downstream_headers.setMethod(Http::Headers::get().MethodValues.Get);\n    downstream_headers.remove(Http::Headers::get().ContentLength);\n    callbacks_->modifyDecodingBuffer([](Buffer::Instance& data) { data.drain(data.length()); });\n  }\n\n  num_internal_redirect->increment();\n  restore_original_headers.cancel();\n  // Preserve the original request URL for the second pass.\n  downstream_headers.setEnvoyOriginalUrl(\n      absl::StrCat(scheme_is_http ? Http::Headers::get().SchemeValues.Http\n                                  : Http::Headers::get().SchemeValues.Https,\n                   \"://\", saved_headers->getHostValue(), saved_headers->getPathValue()));\n  return true;\n}\n\nvoid Filter::runRetryOptionsPredicates(UpstreamRequest& retriable_request) {\n  for (const auto& options_predicate : route_entry_->retryPolicy().retryOptionsPredicates()) {\n    const Upstream::RetryOptionsPredicate::UpdateOptionsParameters parameters{\n        retriable_request.streamInfo(), upstreamSocketOptions()};\n    auto ret = options_predicate->updateOptions(parameters);\n    if (ret.new_upstream_socket_options_.has_value()) {\n      upstream_options_ = ret.new_upstream_socket_options_.value();\n    }\n  }\n}\n\nvoid Filter::doRetry(bool can_send_early_data, bool can_use_http3, TimeoutRetry is_timeout_retry) {\n  ENVOY_STREAM_LOG(debug, \"performing retry\", *callbacks_);\n\n  is_retry_ = true;\n  attempt_count_++;\n  callbacks_->streamInfo().setAttemptCount(attempt_count_);\n  ASSERT(pending_retries_ > 0);\n  pending_retries_--;\n\n  // Clusters can technically get removed by CDS during a retry. Make sure it still exists.\n  const auto cluster = config_.cm_.getThreadLocalCluster(route_entry_->clusterName());\n  std::unique_ptr<GenericConnPool> generic_conn_pool;\n  if (cluster != nullptr) {\n    cluster_ = cluster->info();\n    generic_conn_pool = createConnPool(*cluster);\n  }\n\n  if (!generic_conn_pool) {\n    sendNoHealthyUpstreamResponse();\n    cleanup();\n    return;\n  }\n  UpstreamRequestPtr upstream_request = std::make_unique<UpstreamRequest>(\n      *this, std::move(generic_conn_pool), can_send_early_data, can_use_http3);\n\n  if (include_attempt_count_in_request_) {\n    downstream_headers_->setEnvoyAttemptCount(attempt_count_);\n  }\n\n  if (include_timeout_retry_header_in_request_) {\n    downstream_headers_->setEnvoyIsTimeoutRetry(is_timeout_retry == TimeoutRetry::Yes ? \"true\"\n                                                                                      : \"false\");\n  }\n\n  // The request timeouts only account for time elapsed since the downstream request completed\n  // which might not have happened yet (in which case zero time has elapsed.)\n  std::chrono::milliseconds elapsed_time = std::chrono::milliseconds::zero();\n\n  if (DateUtil::timePointValid(downstream_request_complete_time_)) {\n    Event::Dispatcher& dispatcher = callbacks_->dispatcher();\n    elapsed_time = std::chrono::duration_cast<std::chrono::milliseconds>(\n        dispatcher.timeSource().monotonicTime() - downstream_request_complete_time_);\n  }\n\n  FilterUtility::setTimeoutHeaders(elapsed_time.count(), timeout_, *route_entry_,\n                                   *downstream_headers_, !config_.suppress_envoy_headers_,\n                                   grpc_request_, hedging_params_.hedge_on_per_try_timeout_);\n\n  UpstreamRequest* upstream_request_tmp = upstream_request.get();\n  LinkedList::moveIntoList(std::move(upstream_request), upstream_requests_);\n  upstream_requests_.front()->acceptHeadersFromRouter(\n      !callbacks_->decodingBuffer() && !downstream_trailers_ && downstream_end_stream_);\n  // It's possible we got immediately reset which means the upstream request we just\n  // added to the front of the list might have been removed, so we need to check to make\n  // sure we don't send data on the wrong request.\n  if (!upstream_requests_.empty() && (upstream_requests_.front().get() == upstream_request_tmp)) {\n    if (callbacks_->decodingBuffer()) {\n      // If we are doing a retry we need to make a copy.\n      Buffer::OwnedImpl copy(*callbacks_->decodingBuffer());\n      upstream_requests_.front()->acceptDataFromRouter(copy, !downstream_trailers_ &&\n                                                                 downstream_end_stream_);\n    }\n\n    if (downstream_trailers_) {\n      upstream_requests_.front()->acceptTrailersFromRouter(*downstream_trailers_);\n    }\n  }\n}\n\nuint32_t Filter::numRequestsAwaitingHeaders() {\n  return std::count_if(upstream_requests_.begin(), upstream_requests_.end(),\n                       [](const auto& req) -> bool { return req->awaitingHeaders(); });\n}\n\nbool Filter::checkDropOverload(Upstream::ThreadLocalCluster& cluster,\n                               std::function<void(Http::ResponseHeaderMap&)>& modify_headers) {\n  if (cluster.dropOverload().value()) {\n    ENVOY_STREAM_LOG(debug, \"Router filter: cluster DROP_OVERLOAD configuration: {}\", *callbacks_,\n                     cluster.dropOverload().value());\n    if (config_.random_.bernoulli(cluster.dropOverload())) {\n      ENVOY_STREAM_LOG(debug, \"The request is dropped by DROP_OVERLOAD\", *callbacks_);\n      callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::DropOverLoad);\n      chargeUpstreamCode(Http::Code::ServiceUnavailable, nullptr, true);\n      callbacks_->sendLocalReply(\n          Http::Code::ServiceUnavailable, \"drop overload\",\n          [modify_headers, this](Http::ResponseHeaderMap& headers) {\n            if (!config_.suppress_envoy_headers_) {\n              headers.addReference(Http::Headers::get().EnvoyDropOverload,\n                                   Http::Headers::get().EnvoyDropOverloadValues.True);\n            }\n            modify_headers(headers);\n          },\n          absl::nullopt, StreamInfo::ResponseCodeDetails::get().DropOverload);\n\n      cluster.info()->loadReportStats().upstream_rq_drop_overload_.inc();\n      return true;\n    }\n  }\n  return false;\n}\n\nRetryStatePtr\nProdFilter::createRetryState(const RetryPolicy& policy, Http::RequestHeaderMap& request_headers,\n                             const Upstream::ClusterInfo& cluster, const VirtualCluster* vcluster,\n                             RouteStatsContextOptRef route_stats_context, Runtime::Loader& runtime,\n                             Random::RandomGenerator& random, Event::Dispatcher& dispatcher,\n                             TimeSource& time_source, Upstream::ResourcePriority priority) {\n  std::unique_ptr<RetryStateImpl> retry_state =\n      RetryStateImpl::create(policy, request_headers, cluster, vcluster, route_stats_context,\n                             runtime, random, dispatcher, time_source, priority);\n  if (retry_state != nullptr && retry_state->isAutomaticallyConfiguredForHttp3()) {\n    // Since doing retry will make Envoy to buffer the request body, if upstream using HTTP/3 is the\n    // only reason for doing retry, set the retry shadow buffer limit to 0 so that we don't retry or\n    // buffer safe requests with body which is not common.\n    setRetryShadowBufferLimit(0);\n  }\n  return retry_state;\n}\n\n} // namespace Router\n} // namespace Envoy\n", "#include \"source/common/router/upstream_request.h\"\n\n#include <chrono>\n#include <cstdint>\n#include <functional>\n#include <memory>\n#include <string>\n\n#include \"envoy/event/dispatcher.h\"\n#include \"envoy/event/timer.h\"\n#include \"envoy/grpc/status.h\"\n#include \"envoy/http/conn_pool.h\"\n#include \"envoy/http/header_map.h\"\n#include \"envoy/runtime/runtime.h\"\n#include \"envoy/upstream/cluster_manager.h\"\n#include \"envoy/upstream/upstream.h\"\n\n#include \"source/common/common/assert.h\"\n#include \"source/common/common/dump_state_utils.h\"\n#include \"source/common/common/empty_string.h\"\n#include \"source/common/common/enum_to_int.h\"\n#include \"source/common/common/scope_tracker.h\"\n#include \"source/common/common/utility.h\"\n#include \"source/common/grpc/common.h\"\n#include \"source/common/http/codes.h\"\n#include \"source/common/http/header_map_impl.h\"\n#include \"source/common/http/headers.h\"\n#include \"source/common/http/message_impl.h\"\n#include \"source/common/http/utility.h\"\n#include \"source/common/network/application_protocol.h\"\n#include \"source/common/network/transport_socket_options_impl.h\"\n#include \"source/common/network/upstream_server_name.h\"\n#include \"source/common/network/upstream_subject_alt_names.h\"\n#include \"source/common/router/config_impl.h\"\n#include \"source/common/router/debug_config.h\"\n#include \"source/common/router/router.h\"\n#include \"source/common/stream_info/uint32_accessor_impl.h\"\n#include \"source/common/tracing/http_tracer_impl.h\"\n#include \"source/extensions/common/proxy_protocol/proxy_protocol_header.h\"\n\nnamespace Envoy {\nnamespace Router {\n\n// The upstream HTTP filter manager class.\nclass UpstreamFilterManager : public Http::FilterManager {\npublic:\n  UpstreamFilterManager(Http::FilterManagerCallbacks& filter_manager_callbacks,\n                        Event::Dispatcher& dispatcher, OptRef<const Network::Connection> connection,\n                        uint64_t stream_id, Buffer::BufferMemoryAccountSharedPtr account,\n                        bool proxy_100_continue, uint32_t buffer_limit,\n                        const Http::FilterChainFactory& filter_chain_factory,\n                        UpstreamRequest& request)\n      : FilterManager(filter_manager_callbacks, dispatcher, connection, stream_id, account,\n                      proxy_100_continue, buffer_limit, filter_chain_factory),\n        upstream_request_(request) {}\n\n  StreamInfo::StreamInfo& streamInfo() override {\n    return upstream_request_.parent_.callbacks()->streamInfo();\n  }\n  const StreamInfo::StreamInfo& streamInfo() const override {\n    return upstream_request_.parent_.callbacks()->streamInfo();\n  }\n  // Send local replies via the downstream HTTP filter manager.\n  // Local replies will not be seen by upstream HTTP filters.\n  void sendLocalReply(Http::Code code, absl::string_view body,\n                      const std::function<void(Http::ResponseHeaderMap& headers)>& modify_headers,\n                      const absl::optional<Grpc::Status::GrpcStatus> grpc_status,\n                      absl::string_view details) override {\n    state().decoder_filter_chain_aborted_ = true;\n    state().encoder_filter_chain_aborted_ = true;\n    state().remote_encode_complete_ = true;\n    state().local_complete_ = true;\n    // TODO(alyssawilk) this should be done through the router to play well with hedging.\n    upstream_request_.parent_.callbacks()->sendLocalReply(code, body, modify_headers, grpc_status,\n                                                          details);\n  }\n  void executeLocalReplyIfPrepared() override {}\n  UpstreamRequest& upstream_request_;\n};\n\nUpstreamRequest::UpstreamRequest(RouterFilterInterface& parent,\n                                 std::unique_ptr<GenericConnPool>&& conn_pool,\n                                 bool can_send_early_data, bool can_use_http3)\n    : parent_(parent), conn_pool_(std::move(conn_pool)),\n      stream_info_(parent_.callbacks()->dispatcher().timeSource(), nullptr),\n      start_time_(parent_.callbacks()->dispatcher().timeSource().monotonicTime()),\n      calling_encode_headers_(false), upstream_canary_(false), router_sent_end_stream_(false),\n      encode_trailers_(false), retried_(false), awaiting_headers_(true),\n      outlier_detection_timeout_recorded_(false),\n      create_per_try_timeout_on_request_complete_(false), paused_for_connect_(false),\n      reset_stream_(false),\n      record_timeout_budget_(parent_.cluster()->timeoutBudgetStats().has_value()),\n      cleaned_up_(false), had_upstream_(false),\n      stream_options_({can_send_early_data, can_use_http3}), grpc_rq_success_deferred_(false),\n      upstream_wait_for_response_headers_before_disabling_read_(Runtime::runtimeFeatureEnabled(\n          \"envoy.reloadable_features.upstream_wait_for_response_headers_before_disabling_read\")) {\n  if (auto tracing_config = parent_.callbacks()->tracingConfig(); tracing_config.has_value()) {\n    if (tracing_config->spawnUpstreamSpan() || parent_.config().start_child_span_) {\n      span_ = parent_.callbacks()->activeSpan().spawnChild(\n          tracing_config.value().get(),\n          absl::StrCat(\"router \", parent.cluster()->observabilityName(), \" egress\"),\n          parent_.callbacks()->dispatcher().timeSource().systemTime());\n      if (parent.attemptCount() != 1) {\n        // This is a retry request, add this metadata to span.\n        span_->setTag(Tracing::Tags::get().RetryCount, std::to_string(parent.attemptCount() - 1));\n      }\n    }\n  }\n\n  // The router checks that the connection pool is non-null before creating the upstream request.\n  auto upstream_host = conn_pool_->host();\n  Tracing::HttpTraceContext trace_context(*parent_.downstreamHeaders());\n  if (span_ != nullptr) {\n    span_->injectContext(trace_context, upstream_host);\n  } else {\n    // No independent child span for current upstream request then inject the parent span's tracing\n    // context into the request headers.\n    // The injectContext() of the parent span may be called repeatedly when the request is retried.\n    parent_.callbacks()->activeSpan().injectContext(trace_context, upstream_host);\n  }\n\n  stream_info_.setUpstreamInfo(std::make_shared<StreamInfo::UpstreamInfoImpl>());\n  stream_info_.route_ = parent_.callbacks()->route();\n  parent_.callbacks()->streamInfo().setUpstreamInfo(stream_info_.upstreamInfo());\n\n  stream_info_.healthCheck(parent_.callbacks()->streamInfo().healthCheck());\n  stream_info_.setIsShadow(parent_.callbacks()->streamInfo().isShadow());\n  absl::optional<Upstream::ClusterInfoConstSharedPtr> cluster_info =\n      parent_.callbacks()->streamInfo().upstreamClusterInfo();\n  if (cluster_info.has_value()) {\n    stream_info_.setUpstreamClusterInfo(*cluster_info);\n  }\n\n  // Set up the upstream HTTP filter manager.\n  filter_manager_callbacks_ = std::make_unique<UpstreamRequestFilterManagerCallbacks>(*this);\n  filter_manager_ = std::make_unique<UpstreamFilterManager>(\n      *filter_manager_callbacks_, parent_.callbacks()->dispatcher(), connection(),\n      parent_.callbacks()->streamId(), parent_.callbacks()->account(), true,\n      parent_.callbacks()->decoderBufferLimit(), *parent_.cluster(), *this);\n  // Attempt to create custom cluster-specified filter chain\n  bool created = parent_.cluster()->createFilterChain(*filter_manager_,\n                                                      /*only_create_if_configured=*/true);\n  if (!created) {\n    // Attempt to create custom router-specified filter chain.\n    created = parent_.config().createFilterChain(*filter_manager_);\n  }\n  if (!created) {\n    // Neither cluster nor router have a custom filter chain; add the default\n    // cluster filter chain, which only consists of the codec filter.\n    created = parent_.cluster()->createFilterChain(*filter_manager_, false);\n  }\n  // There will always be a codec filter present, which sets the upstream\n  // interface. Fast-fail any tests that don't set up mocks correctly.\n  ASSERT(created && upstream_interface_.has_value());\n}\n\nUpstreamRequest::~UpstreamRequest() { cleanUp(); }\n\nvoid UpstreamRequest::cleanUp() {\n  if (cleaned_up_) {\n    return;\n  }\n  cleaned_up_ = true;\n\n  filter_manager_->destroyFilters();\n\n  if (span_ != nullptr) {\n    auto tracing_config = parent_.callbacks()->tracingConfig();\n    ASSERT(tracing_config.has_value());\n    Tracing::HttpTracerUtility::finalizeUpstreamSpan(*span_, stream_info_,\n                                                     tracing_config.value().get());\n  }\n\n  if (per_try_timeout_ != nullptr) {\n    // Allows for testing.\n    per_try_timeout_->disableTimer();\n  }\n\n  if (per_try_idle_timeout_ != nullptr) {\n    // Allows for testing.\n    per_try_idle_timeout_->disableTimer();\n  }\n\n  if (max_stream_duration_timer_ != nullptr) {\n    max_stream_duration_timer_->disableTimer();\n  }\n\n  if (upstream_log_flush_timer_ != nullptr) {\n    upstream_log_flush_timer_->disableTimer();\n  }\n\n  clearRequestEncoder();\n\n  // If desired, fire the per-try histogram when the UpstreamRequest\n  // completes.\n  if (record_timeout_budget_) {\n    Event::Dispatcher& dispatcher = parent_.callbacks()->dispatcher();\n    const MonotonicTime end_time = dispatcher.timeSource().monotonicTime();\n    const std::chrono::milliseconds response_time =\n        std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time_);\n    Upstream::ClusterTimeoutBudgetStatsOptRef tb_stats = parent_.cluster()->timeoutBudgetStats();\n    tb_stats->get().upstream_rq_timeout_budget_per_try_percent_used_.recordValue(\n        FilterUtility::percentageOfTimeout(response_time, parent_.timeout().per_try_timeout_));\n  }\n\n  // Ditto for request/response size histograms.\n  Upstream::ClusterRequestResponseSizeStatsOptRef req_resp_stats_opt =\n      parent_.cluster()->requestResponseSizeStats();\n  if (req_resp_stats_opt.has_value() && parent_.downstreamHeaders()) {\n    auto& req_resp_stats = req_resp_stats_opt->get();\n    req_resp_stats.upstream_rq_headers_size_.recordValue(parent_.downstreamHeaders()->byteSize());\n    req_resp_stats.upstream_rq_body_size_.recordValue(stream_info_.bytesSent());\n\n    if (response_headers_size_.has_value()) {\n      req_resp_stats.upstream_rs_headers_size_.recordValue(response_headers_size_.value());\n      req_resp_stats.upstream_rs_body_size_.recordValue(stream_info_.bytesReceived());\n    }\n  }\n\n  stream_info_.onRequestComplete();\n  upstreamLog(AccessLog::AccessLogType::UpstreamEnd);\n\n  while (downstream_data_disabled_ != 0) {\n    parent_.callbacks()->onDecoderFilterBelowWriteBufferLowWatermark();\n    parent_.cluster()->trafficStats()->upstream_flow_control_drained_total_.inc();\n    --downstream_data_disabled_;\n  }\n  // The upstream HTTP filter chain callbacks own headers/trailers while they are traversing the\n  // filter chain. Make sure to not delete them immediately when the stream ends, as the stream\n  // often ends during filter chain processing and it causes use-after-free violations.\n  parent_.callbacks()->dispatcher().deferredDelete(std::move(filter_manager_callbacks_));\n}\n\nvoid UpstreamRequest::upstreamLog(AccessLog::AccessLogType access_log_type) {\n  const Formatter::HttpFormatterContext log_context{parent_.downstreamHeaders(),\n                                                    upstream_headers_.get(),\n                                                    upstream_trailers_.get(),\n                                                    {},\n                                                    access_log_type};\n\n  for (const auto& upstream_log : parent_.config().upstream_logs_) {\n    upstream_log->log(log_context, stream_info_);\n  }\n}\n\n// This is called by the FilterManager when all filters have processed 1xx headers. Forward them\n// on to the router.\nvoid UpstreamRequest::decode1xxHeaders(Http::ResponseHeaderMapPtr&& headers) {\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n\n  ASSERT(Http::HeaderUtility::isSpecial1xx(*headers));\n  addResponseHeadersSize(headers->byteSize());\n  maybeHandleDeferredReadDisable();\n  parent_.onUpstream1xxHeaders(std::move(headers), *this);\n}\n\n// This is called by the FilterManager when all filters have processed headers. Forward them\n// on to the router.\nvoid UpstreamRequest::decodeHeaders(Http::ResponseHeaderMapPtr&& headers, bool end_stream) {\n  ASSERT(headers.get());\n  ENVOY_STREAM_LOG(trace, \"upstream response headers:\\n{}\", *parent_.callbacks(), *headers);\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n\n  resetPerTryIdleTimer();\n\n  addResponseHeadersSize(headers->byteSize());\n\n  // We drop unsupported 1xx on the floor here. 101 upgrade headers need to be passed to the client\n  // as part of the final response. Most 1xx headers are handled in onUpstream1xxHeaders.\n  //\n  // We could in principle handle other headers here, but this might result in the double invocation\n  // of decodeHeaders() (once for informational, again for non-informational), which is likely an\n  // easy to miss corner case in the filter and HCM contract.\n  //\n  // This filtering is done early in upstream request, unlike 100 coalescing which is performed in\n  // the router filter, since the filtering only depends on the state of a single upstream, and we\n  // don't want to confuse accounting such as onFirstUpstreamRxByteReceived() with informational\n  // headers.\n  const uint64_t response_code = Http::Utility::getResponseStatus(*headers);\n  if (Http::CodeUtility::is1xx(response_code) &&\n      response_code != enumToInt(Http::Code::SwitchingProtocols)) {\n    return;\n  }\n\n  awaiting_headers_ = false;\n  if (span_ != nullptr) {\n    Tracing::HttpTracerUtility::onUpstreamResponseHeaders(*span_, headers.get());\n  }\n  if (!parent_.config().upstream_logs_.empty()) {\n    upstream_headers_ = Http::createHeaderMap<Http::ResponseHeaderMapImpl>(*headers);\n  }\n  stream_info_.setResponseCode(static_cast<uint32_t>(response_code));\n\n  maybeHandleDeferredReadDisable();\n  ASSERT(headers.get());\n\n  parent_.onUpstreamHeaders(response_code, std::move(headers), *this, end_stream);\n}\n\nvoid UpstreamRequest::maybeHandleDeferredReadDisable() {\n  for (; deferred_read_disabling_count_ > 0; --deferred_read_disabling_count_) {\n    // If the deferred read disabling count hasn't been cancelled out by read\n    // enabling count so far, stop the upstream from reading the rest response.\n    // Because readDisable keeps track of how many time it is called with\n    // \"true\" or \"false\", here it has to be called with \"true\" the same number\n    // of times as it would be called with \"false\" in the future.\n    parent_.cluster()->trafficStats()->upstream_flow_control_paused_reading_total_.inc();\n    upstream_->readDisable(true);\n  }\n}\n\nvoid UpstreamRequest::decodeData(Buffer::Instance& data, bool end_stream) {\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n\n  resetPerTryIdleTimer();\n  stream_info_.addBytesReceived(data.length());\n  parent_.onUpstreamData(data, *this, end_stream);\n}\n\nvoid UpstreamRequest::decodeTrailers(Http::ResponseTrailerMapPtr&& trailers) {\n  ENVOY_STREAM_LOG(trace, \"upstream response trailers:\\n{}\", *parent_.callbacks(), *trailers);\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n\n  if (span_ != nullptr) {\n    Tracing::HttpTracerUtility::onUpstreamResponseTrailers(*span_, trailers.get());\n  }\n  if (!parent_.config().upstream_logs_.empty()) {\n    upstream_trailers_ = Http::createHeaderMap<Http::ResponseTrailerMapImpl>(*trailers);\n  }\n  parent_.onUpstreamTrailers(std::move(trailers), *this);\n}\n\nvoid UpstreamRequest::dumpState(std::ostream& os, int indent_level) const {\n  const char* spaces = spacesForLevel(indent_level);\n  os << spaces << \"UpstreamRequest \" << this << \"\\n\";\n  if (connection()) {\n    const auto addressProvider = connection()->connectionInfoProviderSharedPtr();\n    DUMP_DETAILS(addressProvider);\n  }\n  const Http::RequestHeaderMap* request_headers = parent_.downstreamHeaders();\n  DUMP_DETAILS(request_headers);\n  if (filter_manager_) {\n    filter_manager_->dumpState(os, indent_level);\n  }\n}\n\nconst Route& UpstreamRequest::route() const { return *parent_.callbacks()->route(); }\n\nOptRef<const Network::Connection> UpstreamRequest::connection() const {\n  return parent_.callbacks()->connection();\n}\n\nvoid UpstreamRequest::decodeMetadata(Http::MetadataMapPtr&& metadata_map) {\n  parent_.onUpstreamMetadata(std::move(metadata_map));\n}\n\nvoid UpstreamRequest::maybeEndDecode(bool end_stream) {\n  if (end_stream) {\n    upstreamTiming().onLastUpstreamRxByteReceived(parent_.callbacks()->dispatcher().timeSource());\n  }\n}\n\nvoid UpstreamRequest::onUpstreamHostSelected(Upstream::HostDescriptionConstSharedPtr host,\n                                             bool pool_success) {\n  StreamInfo::UpstreamInfo& upstream_info = *streamInfo().upstreamInfo();\n  upstream_info.setUpstreamHost(host);\n  upstream_host_ = host;\n  parent_.onUpstreamHostSelected(host, pool_success);\n}\n\nvoid UpstreamRequest::acceptHeadersFromRouter(bool end_stream) {\n  ASSERT(!router_sent_end_stream_);\n  router_sent_end_stream_ = end_stream;\n\n  // Make sure that when we are forwarding CONNECT payload we do not do so until\n  // the upstream has accepted the CONNECT request.\n  // This must be done before conn_pool->newStream, as onPoolReady un-pauses for CONNECT\n  // termination.\n  auto* headers = parent_.downstreamHeaders();\n  if (headers->getMethodValue() == Http::Headers::get().MethodValues.Connect) {\n    paused_for_connect_ = true;\n  }\n\n  // Kick off creation of the upstream connection immediately upon receiving headers.\n  // In future it may be possible for upstream HTTP filters to delay this, or influence connection\n  // creation but for now optimize for minimal latency and fetch the connection\n  // as soon as possible.\n  conn_pool_->newStream(this);\n\n  if (parent_.config().upstream_log_flush_interval_.has_value()) {\n    upstream_log_flush_timer_ = parent_.callbacks()->dispatcher().createTimer([this]() -> void {\n      // If the request is complete, we've already done the stream-end upstream log, and shouldn't\n      // do the periodic log.\n      if (!streamInfo().requestComplete().has_value()) {\n        upstreamLog(AccessLog::AccessLogType::UpstreamPeriodic);\n        resetUpstreamLogFlushTimer();\n      }\n      // Both downstream and upstream bytes meters may not be initialized when\n      // the timer goes off, e.g. if it takes longer than the interval for a\n      // connection to be initialized; check for nullptr.\n      auto& downstream_bytes_meter = stream_info_.getDownstreamBytesMeter();\n      auto& upstream_bytes_meter = stream_info_.getUpstreamBytesMeter();\n      const SystemTime now = parent_.callbacks()->dispatcher().timeSource().systemTime();\n      if (downstream_bytes_meter) {\n        downstream_bytes_meter->takeUpstreamPeriodicLoggingSnapshot(now);\n      }\n      if (upstream_bytes_meter) {\n        upstream_bytes_meter->takeUpstreamPeriodicLoggingSnapshot(now);\n      }\n    });\n\n    resetUpstreamLogFlushTimer();\n  }\n\n  filter_manager_->requestHeadersInitialized();\n  filter_manager_->streamInfo().setRequestHeaders(*parent_.downstreamHeaders());\n  filter_manager_->decodeHeaders(*parent_.downstreamHeaders(), end_stream);\n}\n\nvoid UpstreamRequest::acceptDataFromRouter(Buffer::Instance& data, bool end_stream) {\n  ASSERT(!router_sent_end_stream_);\n  router_sent_end_stream_ = end_stream;\n\n  filter_manager_->decodeData(data, end_stream);\n}\n\nvoid UpstreamRequest::acceptTrailersFromRouter(Http::RequestTrailerMap& trailers) {\n  ASSERT(!router_sent_end_stream_);\n  router_sent_end_stream_ = true;\n  encode_trailers_ = true;\n\n  filter_manager_->decodeTrailers(trailers);\n}\n\nvoid UpstreamRequest::acceptMetadataFromRouter(Http::MetadataMapPtr&& metadata_map_ptr) {\n  filter_manager_->decodeMetadata(*metadata_map_ptr);\n}\n\nvoid UpstreamRequest::onResetStream(Http::StreamResetReason reason,\n                                    absl::string_view transport_failure_reason) {\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n\n  if (span_ != nullptr) {\n    // Add tags about reset.\n    span_->setTag(Tracing::Tags::get().Error, Tracing::Tags::get().True);\n    span_->setTag(Tracing::Tags::get().ErrorReason, Http::Utility::resetReasonToString(reason));\n  }\n  clearRequestEncoder();\n  awaiting_headers_ = false;\n  if (!calling_encode_headers_) {\n    stream_info_.setResponseFlag(Filter::streamResetReasonToResponseFlag(reason));\n    parent_.onUpstreamReset(reason, transport_failure_reason, *this);\n  } else {\n    deferred_reset_reason_ = reason;\n  }\n}\n\nvoid UpstreamRequest::resetStream() {\n  if (conn_pool_->cancelAnyPendingStream()) {\n    ENVOY_STREAM_LOG(debug, \"canceled pool request\", *parent_.callbacks());\n    ASSERT(!upstream_);\n  }\n\n  // Don't reset the stream if we're already done with it.\n  if (upstreamTiming().last_upstream_tx_byte_sent_.has_value() &&\n      upstreamTiming().last_upstream_rx_byte_received_.has_value()) {\n    return;\n  }\n\n  if (span_ != nullptr) {\n    // Add tags about the cancellation.\n    span_->setTag(Tracing::Tags::get().Canceled, Tracing::Tags::get().True);\n  }\n\n  if (upstream_) {\n    ENVOY_STREAM_LOG(debug, \"resetting pool request\", *parent_.callbacks());\n    upstream_->resetStream();\n    clearRequestEncoder();\n  }\n  reset_stream_ = true;\n}\n\nvoid UpstreamRequest::resetPerTryIdleTimer() {\n  if (per_try_idle_timeout_ != nullptr) {\n    per_try_idle_timeout_->enableTimer(parent_.timeout().per_try_idle_timeout_);\n  }\n}\n\nvoid UpstreamRequest::resetUpstreamLogFlushTimer() {\n  if (upstream_log_flush_timer_ != nullptr) {\n    upstream_log_flush_timer_->enableTimer(parent_.config().upstream_log_flush_interval_.value());\n  }\n}\n\nvoid UpstreamRequest::setupPerTryTimeout() {\n  ASSERT(!per_try_timeout_);\n  if (parent_.timeout().per_try_timeout_.count() > 0) {\n    per_try_timeout_ =\n        parent_.callbacks()->dispatcher().createTimer([this]() -> void { onPerTryTimeout(); });\n    per_try_timeout_->enableTimer(parent_.timeout().per_try_timeout_);\n  }\n\n  ASSERT(!per_try_idle_timeout_);\n  if (parent_.timeout().per_try_idle_timeout_.count() > 0) {\n    per_try_idle_timeout_ =\n        parent_.callbacks()->dispatcher().createTimer([this]() -> void { onPerTryIdleTimeout(); });\n    resetPerTryIdleTimer();\n  }\n}\n\nvoid UpstreamRequest::onPerTryIdleTimeout() {\n  ENVOY_STREAM_LOG(debug, \"upstream per try idle timeout\", *parent_.callbacks());\n  stream_info_.setResponseFlag(StreamInfo::CoreResponseFlag::StreamIdleTimeout);\n  parent_.onPerTryIdleTimeout(*this);\n}\n\nvoid UpstreamRequest::onPerTryTimeout() {\n  // If we've sent anything downstream, ignore the per try timeout and let the response continue\n  // up to the global timeout\n  if (!parent_.downstreamResponseStarted()) {\n    ENVOY_STREAM_LOG(debug, \"upstream per try timeout\", *parent_.callbacks());\n\n    stream_info_.setResponseFlag(StreamInfo::CoreResponseFlag::UpstreamRequestTimeout);\n    parent_.onPerTryTimeout(*this);\n  } else {\n    ENVOY_STREAM_LOG(debug,\n                     \"ignored upstream per try timeout due to already started downstream response\",\n                     *parent_.callbacks());\n  }\n}\n\nvoid UpstreamRequest::recordConnectionPoolCallbackLatency() {\n  upstreamTiming().recordConnectionPoolCallbackLatency(\n      start_time_, parent_.callbacks()->dispatcher().timeSource());\n}\n\nvoid UpstreamRequest::onPoolFailure(ConnectionPool::PoolFailureReason reason,\n                                    absl::string_view transport_failure_reason,\n                                    Upstream::HostDescriptionConstSharedPtr host) {\n  recordConnectionPoolCallbackLatency();\n  Http::StreamResetReason reset_reason = [](ConnectionPool::PoolFailureReason reason) {\n    switch (reason) {\n    case ConnectionPool::PoolFailureReason::Overflow:\n      return Http::StreamResetReason::Overflow;\n    case ConnectionPool::PoolFailureReason::RemoteConnectionFailure:\n      return Http::StreamResetReason::RemoteConnectionFailure;\n    case ConnectionPool::PoolFailureReason::LocalConnectionFailure:\n      return Http::StreamResetReason::LocalConnectionFailure;\n    case ConnectionPool::PoolFailureReason::Timeout:\n      return Http::StreamResetReason::ConnectionTimeout;\n    }\n    PANIC_DUE_TO_CORRUPT_ENUM;\n  }(reason);\n\n  stream_info_.upstreamInfo()->setUpstreamTransportFailureReason(transport_failure_reason);\n\n  // Mimic an upstream reset.\n  onUpstreamHostSelected(host, false);\n  onResetStream(reset_reason, transport_failure_reason);\n}\n\nvoid UpstreamRequest::onPoolReady(std::unique_ptr<GenericUpstream>&& upstream,\n                                  Upstream::HostDescriptionConstSharedPtr host,\n                                  const Network::ConnectionInfoProvider& address_provider,\n                                  StreamInfo::StreamInfo& info,\n                                  absl::optional<Http::Protocol> protocol) {\n  // This may be called under an existing ScopeTrackerScopeState but it will unwind correctly.\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n  ENVOY_STREAM_LOG(debug, \"pool ready\", *parent_.callbacks());\n  recordConnectionPoolCallbackLatency();\n  upstream_ = std::move(upstream);\n  had_upstream_ = true;\n  // Have the upstream use the account of the downstream.\n  upstream_->setAccount(parent_.callbacks()->account());\n\n  host->outlierDetector().putResult(Upstream::Outlier::Result::LocalOriginConnectSuccess);\n\n  onUpstreamHostSelected(host, true);\n\n  if (protocol) {\n    stream_info_.protocol(protocol.value());\n  } else {\n    // We only pause for CONNECT for HTTP upstreams. If this is a TCP upstream, unpause.\n    paused_for_connect_ = false;\n  }\n\n  StreamInfo::UpstreamInfo& upstream_info = *stream_info_.upstreamInfo();\n  if (info.upstreamInfo()) {\n    auto& upstream_timing = info.upstreamInfo()->upstreamTiming();\n    upstreamTiming().upstream_connect_start_ = upstream_timing.upstream_connect_start_;\n    upstreamTiming().upstream_connect_complete_ = upstream_timing.upstream_connect_complete_;\n    upstreamTiming().upstream_handshake_complete_ = upstream_timing.upstream_handshake_complete_;\n    upstream_info.setUpstreamNumStreams(info.upstreamInfo()->upstreamNumStreams());\n  }\n\n  // Upstream HTTP filters might have already created/set a filter state.\n  const StreamInfo::FilterStateSharedPtr& filter_state = info.filterState();\n  if (!filter_state) {\n    upstream_info.setUpstreamFilterState(\n        std::make_shared<StreamInfo::FilterStateImpl>(StreamInfo::FilterState::LifeSpan::Request));\n  } else {\n    upstream_info.setUpstreamFilterState(filter_state);\n  }\n  upstream_info.setUpstreamLocalAddress(address_provider.localAddress());\n  upstream_info.setUpstreamRemoteAddress(address_provider.remoteAddress());\n  upstream_info.setUpstreamSslConnection(info.downstreamAddressProvider().sslConnection());\n\n  if (info.downstreamAddressProvider().connectionID().has_value()) {\n    upstream_info.setUpstreamConnectionId(info.downstreamAddressProvider().connectionID().value());\n  }\n\n  if (info.downstreamAddressProvider().interfaceName().has_value()) {\n    upstream_info.setUpstreamInterfaceName(\n        info.downstreamAddressProvider().interfaceName().value());\n  }\n\n  stream_info_.setUpstreamBytesMeter(upstream_->bytesMeter());\n  StreamInfo::StreamInfo::syncUpstreamAndDownstreamBytesMeter(parent_.callbacks()->streamInfo(),\n                                                              stream_info_);\n  if (protocol) {\n    upstream_info.setUpstreamProtocol(protocol.value());\n  }\n\n  if (parent_.downstreamEndStream()) {\n    setupPerTryTimeout();\n  } else {\n    create_per_try_timeout_on_request_complete_ = true;\n  }\n\n  // Make sure the connection manager will inform the downstream watermark manager when the\n  // downstream buffers are overrun. This may result in immediate watermark callbacks referencing\n  // the encoder.\n  parent_.callbacks()->addDownstreamWatermarkCallbacks(downstream_watermark_manager_);\n\n  absl::optional<std::chrono::milliseconds> max_stream_duration;\n  if (parent_.dynamicMaxStreamDuration().has_value()) {\n    max_stream_duration = parent_.dynamicMaxStreamDuration().value();\n  } else if (upstream_host_->cluster().commonHttpProtocolOptions().has_max_stream_duration()) {\n    max_stream_duration = std::chrono::milliseconds(DurationUtil::durationToMilliseconds(\n        upstream_host_->cluster().commonHttpProtocolOptions().max_stream_duration()));\n  }\n  if (max_stream_duration.has_value() && max_stream_duration->count()) {\n    max_stream_duration_timer_ = parent_.callbacks()->dispatcher().createTimer(\n        [this]() -> void { onStreamMaxDurationReached(); });\n    max_stream_duration_timer_->enableTimer(*max_stream_duration);\n  }\n\n  const auto* route_entry = route().routeEntry();\n  if (route_entry->autoHostRewrite() && !host->hostname().empty()) {\n    Http::Utility::updateAuthority(*parent_.downstreamHeaders(), host->hostname(),\n                                   route_entry->appendXfh());\n  }\n\n  stream_info_.setRequestHeaders(*parent_.downstreamHeaders());\n\n  if (parent_.config().flush_upstream_log_on_upstream_stream_) {\n    upstreamLog(AccessLog::AccessLogType::UpstreamPoolReady);\n  }\n\n  if (address_provider.connectionID() && stream_info_.downstreamAddressProvider().connectionID()) {\n    ENVOY_LOG(debug, \"Attached upstream connection [C{}] to downstream connection [C{}]\",\n              address_provider.connectionID().value(),\n              stream_info_.downstreamAddressProvider().connectionID().value());\n  }\n\n  for (auto* callback : upstream_callbacks_) {\n    callback->onUpstreamConnectionEstablished();\n  }\n}\n\nUpstreamToDownstream& UpstreamRequest::upstreamToDownstream() { return *upstream_interface_; }\n\nvoid UpstreamRequest::onStreamMaxDurationReached() {\n  upstream_host_->cluster().trafficStats()->upstream_rq_max_duration_reached_.inc();\n\n  // The upstream had closed then try to retry along with retry policy.\n  parent_.onStreamMaxDurationReached(*this);\n}\n\nvoid UpstreamRequest::clearRequestEncoder() {\n  // Before clearing the encoder, unsubscribe from callbacks.\n  if (upstream_) {\n    parent_.callbacks()->removeDownstreamWatermarkCallbacks(downstream_watermark_manager_);\n  }\n  upstream_.reset();\n}\n\nvoid UpstreamRequest::readDisableOrDefer(bool disable) {\n  if (!upstream_wait_for_response_headers_before_disabling_read_) {\n    if (disable) {\n      parent_.cluster()->trafficStats()->upstream_flow_control_paused_reading_total_.inc();\n      upstream_->readDisable(true);\n    } else {\n      parent_.cluster()->trafficStats()->upstream_flow_control_resumed_reading_total_.inc();\n      upstream_->readDisable(false);\n    }\n    return;\n  }\n\n  if (disable) {\n    // See comments on deferred_read_disabling_count_ for when we do and don't defer.\n    if (parent_.downstreamResponseStarted()) {\n      // The downstream connection is overrun. Pause reads from upstream.\n      // If there are multiple calls to readDisable either the codec (H2) or the\n      // underlying Network::Connection (H1) will handle reference counting.\n      parent_.cluster()->trafficStats()->upstream_flow_control_paused_reading_total_.inc();\n      upstream_->readDisable(disable);\n    } else {\n      ++deferred_read_disabling_count_;\n    }\n    return;\n  }\n\n  // One source of connection blockage has buffer available.\n  if (deferred_read_disabling_count_ > 0) {\n    ASSERT(!parent_.downstreamResponseStarted());\n    // Cancel out an existing deferred read disabling.\n    --deferred_read_disabling_count_;\n    return;\n  }\n  ASSERT(parent_.downstreamResponseStarted());\n  // Pass this on to the stream, which\n  // will resume reads if this was the last remaining high watermark.\n  parent_.cluster()->trafficStats()->upstream_flow_control_resumed_reading_total_.inc();\n  upstream_->readDisable(disable);\n}\n\nvoid UpstreamRequest::DownstreamWatermarkManager::onAboveWriteBufferHighWatermark() {\n  ASSERT(parent_.upstream_);\n  parent_.readDisableOrDefer(true);\n}\n\nvoid UpstreamRequest::DownstreamWatermarkManager::onBelowWriteBufferLowWatermark() {\n  ASSERT(parent_.upstream_);\n  parent_.readDisableOrDefer(false);\n}\n\nvoid UpstreamRequest::disableDataFromDownstreamForFlowControl() {\n  parent_.cluster()->trafficStats()->upstream_flow_control_backed_up_total_.inc();\n  parent_.callbacks()->onDecoderFilterAboveWriteBufferHighWatermark();\n  ++downstream_data_disabled_;\n}\n\nvoid UpstreamRequest::enableDataFromDownstreamForFlowControl() {\n  parent_.cluster()->trafficStats()->upstream_flow_control_drained_total_.inc();\n  parent_.callbacks()->onDecoderFilterBelowWriteBufferLowWatermark();\n  ASSERT(downstream_data_disabled_ != 0);\n  if (downstream_data_disabled_ > 0) {\n    --downstream_data_disabled_;\n  }\n}\n\nHttp::RequestHeaderMapOptRef UpstreamRequestFilterManagerCallbacks::requestHeaders() {\n  return {*upstream_request_.parent_.downstreamHeaders()};\n}\n\nHttp::RequestTrailerMapOptRef UpstreamRequestFilterManagerCallbacks::requestTrailers() {\n  if (upstream_request_.parent_.downstreamTrailers()) {\n    return {*upstream_request_.parent_.downstreamTrailers()};\n  }\n  if (trailers_) {\n    return {*trailers_};\n  }\n  return {};\n}\n\nconst ScopeTrackedObject& UpstreamRequestFilterManagerCallbacks::scope() {\n  return upstream_request_.parent_.callbacks()->scope();\n}\n\nOptRef<const Tracing::Config> UpstreamRequestFilterManagerCallbacks::tracingConfig() const {\n  return upstream_request_.parent_.callbacks()->tracingConfig();\n}\n\nTracing::Span& UpstreamRequestFilterManagerCallbacks::activeSpan() {\n  return upstream_request_.parent_.callbacks()->activeSpan();\n}\n\nvoid UpstreamRequestFilterManagerCallbacks::resetStream(\n    Http::StreamResetReason reset_reason, absl::string_view transport_failure_reason) {\n  // The filter manager needs to disambiguate between a filter-driven reset,\n  // which should force reset the stream, and a codec driven reset, which should\n  // tell the router the stream reset, and let the router make the decision to\n  // send a local reply, or retry the stream.\n  if (reset_reason == Http::StreamResetReason::LocalReset &&\n      transport_failure_reason != \"codec_error\") {\n    upstream_request_.parent_.callbacks()->resetStream();\n    return;\n  }\n  return upstream_request_.onResetStream(reset_reason, transport_failure_reason);\n}\n\nUpstream::ClusterInfoConstSharedPtr UpstreamRequestFilterManagerCallbacks::clusterInfo() {\n  return upstream_request_.parent_.callbacks()->clusterInfo();\n}\n\nHttp::Http1StreamEncoderOptionsOptRef\nUpstreamRequestFilterManagerCallbacks::http1StreamEncoderOptions() {\n  return upstream_request_.parent_.callbacks()->http1StreamEncoderOptions();\n}\n\n} // namespace Router\n} // namespace Envoy\n", "#include \"test/integration/http_timeout_integration_test.h\"\n\n#include \"test/test_common/test_runtime.h\"\n\n#include \"gtest/gtest.h\"\n\nnamespace Envoy {\n\nusing testing::HasSubstr;\n\nINSTANTIATE_TEST_SUITE_P(IpVersions, HttpTimeoutIntegrationTest,\n                         testing::ValuesIn(TestEnvironment::getIpVersionsForTest()),\n                         TestUtility::ipTestParamsToString);\n\n// Sends a request with a global timeout specified, sleeps for longer than the\n// timeout, and ensures that a timeout is received.\nTEST_P(HttpTimeoutIntegrationTest, GlobalTimeout) {\n  config_helper_.addConfigModifier(configureProxyStatus());\n  initialize();\n\n  TestScopedRuntime scoped_runtime;\n  scoped_runtime.mergeValues(\n      {{\"envoy.reloadable_features.proxy_status_upstream_request_timeout\", \"true\"}});\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger global timeout.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(501));\n\n  // Ensure we got a timeout downstream and canceled the upstream request.\n  ASSERT_TRUE(response->waitForEndStream());\n  ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"504\", response->headers().getStatusValue());\n  EXPECT_EQ(response->headers().getProxyStatusValue(),\n            \"envoy; error=http_response_timeout; details=\\\"response_timeout; UT\\\"\");\n}\n\n// Testing that `x-envoy-expected-timeout-ms` header, set by egress envoy, is respected by ingress\n// envoy when `respect_expected_rq_timeout` field is enabled. Sends a request with a global timeout\n// specified, sleeps for longer than the timeout, and ensures that a timeout is received.\nTEST_P(HttpTimeoutIntegrationTest, UseTimeoutSetByEgressEnvoy) {\n  enableRespectExpectedRqTimeout(true);\n  initialize();\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"},\n                                     {\"x-envoy-expected-rq-timeout-ms\", \"300\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger global timeout, populated from `x-envoy-expected-rq-timeout-ms` header.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(301));\n\n  // Ensure we got a timeout downstream and canceled the upstream request.\n  ASSERT_TRUE(response->waitForEndStream());\n  ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"504\", response->headers().getStatusValue());\n}\n\n// Testing that ingress envoy derives new timeout value and sets `x-envoy-expected-timeout-ms`\n// header, when timeout has not been set by egress envoy and `respect_expected_rq_timeout` field is\n// enabled. Sends a request with a global timeout specified, sleeps for longer than the timeout, and\n// ensures that a timeout is received.\nTEST_P(HttpTimeoutIntegrationTest, DeriveTimeoutInIngressEnvoy) {\n  enableRespectExpectedRqTimeout(true);\n  initialize();\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger global timeout, populated from `x-envoy-expected-rq-timeout-ms` header.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(501));\n\n  // Ensure we got a timeout downstream and canceled the upstream request.\n  ASSERT_TRUE(response->waitForEndStream());\n  ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"504\", response->headers().getStatusValue());\n}\n\n// Testing that `x-envoy-expected-timeout-ms` header, set by egress envoy, is ignored by ingress\n// envoy and new value is derived. Sends a request with a global timeout specified,\n// sleeps for longer than the timeout, and ensures that a timeout is received.\nTEST_P(HttpTimeoutIntegrationTest, IgnoreTimeoutSetByEgressEnvoy) {\n  enableRespectExpectedRqTimeout(false);\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"},\n                                     {\"x-envoy-expected-rq-timeout-ms\", \"600\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger global timeout, populated from `x-envoy-expected-rq-timeout-ms` header.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(501));\n\n  // Ensure we got a timeout downstream and canceled the upstream request.\n  ASSERT_TRUE(response->waitForEndStream());\n  ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"504\", response->headers().getStatusValue());\n}\n\n// Regression test for https://github.com/envoyproxy/envoy/issues/7154 in which\n// resetStream() was only called after a response timeout for upstream requests\n// that had not received headers yet. This meant that decodeData might be\n// called on a destroyed UpstreamRequest.\nTEST_P(HttpTimeoutIntegrationTest, GlobalTimeoutAfterHeadersBeforeBodyResetsUpstream) {\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  Http::TestRequestHeaderMapImpl request_headers{{\":method\", \"POST\"},\n                                                 {\":path\", \"/test/long/url\"},\n                                                 {\":scheme\", \"http\"},\n                                                 {\":authority\", \"host\"},\n                                                 {\"x-forwarded-for\", \"10.0.0.1\"},\n                                                 {\"x-envoy-upstream-rq-timeout-ms\", \"100\"}};\n  auto encoder_decoder = codec_client_->startRequest(request_headers);\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n\n  codec_client_->sendData(*request_encoder_, 100, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Respond with headers, not end of stream.\n  Http::TestResponseHeaderMapImpl response_headers{{\":status\", \"200\"}};\n  upstream_request_->encodeHeaders(response_headers, false);\n\n  response->waitForHeaders();\n  EXPECT_EQ(\"200\", response->headers().getStatusValue());\n\n  // Trigger global timeout.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(200));\n\n  ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n\n  ASSERT_TRUE(response->waitForReset());\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n}\n\n// Sends a request with a global timeout and per try timeout specified, sleeps\n// for longer than the per try but slightly less than the global timeout.\n// Ensures that two requests are attempted and a timeout is returned\n// downstream.\nTEST_P(HttpTimeoutIntegrationTest, PerTryTimeout) {\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-retry-on\", \"5xx\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"},\n                                     {\"x-envoy-upstream-rq-per-try-timeout-ms\", \"400\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger per try timeout (but not global timeout) and wait for reset.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(400));\n  ASSERT_TRUE(upstream_request_->waitForReset());\n\n  // Wait for a second request to be sent upstream. Max retry backoff is 25ms so advance time that\n  // much.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(25));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger global timeout.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(100));\n  ASSERT_TRUE(response->waitForEndStream());\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"504\", response->headers().getStatusValue());\n}\n\n// Sends a request with a per try timeout specified but no global timeout.\n// Ensures that two requests are attempted and a timeout is returned\n// downstream.\nTEST_P(HttpTimeoutIntegrationTest, PerTryTimeoutWithoutGlobalTimeout) {\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-retry-on\", \"5xx\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"0\"},\n                                     {\"x-envoy-upstream-rq-per-try-timeout-ms\", \"50\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger per try timeout (but not global timeout) and wait for reset.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(50));\n  ASSERT_TRUE(upstream_request_->waitForReset());\n\n  // Wait for a second request to be sent upstream. Max retry backoff is 25ms so advance time that\n  // much. This is always less than the next request's per try timeout.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(25));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Encode 200 response headers for the first (timed out) request.\n  Http::TestResponseHeaderMapImpl response_headers{{\":status\", \"200\"}};\n  upstream_request_->encodeHeaders(response_headers, true);\n\n  ASSERT_TRUE(response->waitForEndStream());\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"200\", response->headers().getStatusValue());\n}\n\nvoid HttpTimeoutIntegrationTest::testIsTimeoutRetryHeader(bool use_hedged_retry) {\n  auto host = config_helper_.createVirtualHost(\"example.com\", \"/test_retry\");\n  host.set_include_is_timeout_retry_header(true);\n  config_helper_.addVirtualHost(host);\n\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(Http::TestRequestHeaderMapImpl{\n      {\":method\", \"POST\"},\n      {\":path\", \"/test_retry\"},\n      {\":scheme\", \"http\"},\n      {\":authority\", \"example.com\"},\n      {\"x-forwarded-for\", \"10.0.0.1\"},\n      {\"x-envoy-retry-on\", \"5xx\"},\n      {\"x-envoy-hedge-on-per-try-timeout\", use_hedged_retry ? \"true\" : \"fase\"},\n      {\"x-envoy-upstream-rq-timeout-ms\", \"500\"},\n      {\"x-envoy-upstream-rq-per-try-timeout-ms\", \"400\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger per try timeout (but not global timeout).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(400));\n\n  // Trigger retry (there's a 25ms backoff before it's issued).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(26));\n\n  // Wait for a second request to be sent upstream\n  FakeStreamPtr upstream_request2;\n\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request2));\n\n  ASSERT_TRUE(upstream_request2->waitForHeadersComplete());\n\n  // Expect the x-envoy-is-timeout-header to set to indicate to the upstream this is a retry\n  // initiated by a previous per try timeout.\n  EXPECT_EQ(upstream_request2->headers().getEnvoyIsTimeoutRetryValue(), \"true\");\n\n  ASSERT_TRUE(upstream_request2->waitForEndStream(*dispatcher_));\n\n  Http::TestResponseHeaderMapImpl response_headers{{\":status\", \"200\"}};\n\n  if (use_hedged_retry) {\n    upstream_request_->encodeHeaders(response_headers, true);\n    ASSERT_TRUE(response->waitForEndStream());\n    // The second request should be reset since we used the response from the first request.\n    ASSERT_TRUE(upstream_request2->waitForReset(std::chrono::seconds(15)));\n  } else {\n    upstream_request2->encodeHeaders(response_headers, true);\n    ASSERT_TRUE(response->waitForEndStream());\n  }\n\n  codec_client_->close();\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"200\", response->headers().getStatusValue());\n}\n\n// With hedge_on_per_try_timeout enabled via config, sends a request with a\n// global timeout and per try timeout specified, sleeps for longer than the per\n// try but slightly less than the global timeout. We expect a retry to be sent\n// upstream and the x-envoy-is-timeout-retry request header to be set to true.\nTEST_P(HttpTimeoutIntegrationTest, IsTimeoutRetryHeaderHedgedRetry) {\n  testIsTimeoutRetryHeader(true);\n}\n\n// Sends a request with a per try timeout specified, sleeps for longer than the per\n// try but slightly less than the global timeout. We expect a retry to be sent\n// upstream and the x-envoy-is-timeout-retry request header to be set to true.\nTEST_P(HttpTimeoutIntegrationTest, IsTimeoutRetryHeaderPerTryTimeout) {\n  testIsTimeoutRetryHeader(false);\n}\n\n// With hedge_on_per_try_timeout enabled via config, sends a request with a\n// global timeout and per try timeout specified, sleeps for longer than the per\n// try but slightly less than the global timeout. We then have the first\n// upstream request return headers and expect those to be returned downstream\n// (which proves the request was not canceled when the timeout was hit).\nTEST_P(HttpTimeoutIntegrationTest, HedgedPerTryTimeout) {\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-retry-on\", \"5xx\"},\n                                     {\"x-envoy-hedge-on-per-try-timeout\", \"true\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"},\n                                     {\"x-envoy-upstream-rq-per-try-timeout-ms\", \"400\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger per try timeout (but not global timeout).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(400));\n\n  // Trigger retry (there's a 25ms backoff before it's issued).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(26));\n\n  // Wait for a second request to be sent upstream\n  FakeStreamPtr upstream_request2;\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request2));\n  ASSERT_TRUE(upstream_request2->waitForHeadersComplete());\n  ASSERT_TRUE(upstream_request2->waitForEndStream(*dispatcher_));\n\n  // Encode 200 response headers for the first (timed out) request.\n  Http::TestResponseHeaderMapImpl response_headers{{\":status\", \"200\"}};\n  upstream_request_->encodeHeaders(response_headers, true);\n\n  ASSERT_TRUE(response->waitForEndStream());\n\n  // The second request should be reset since we used the response from the first request.\n  ASSERT_TRUE(upstream_request2->waitForReset(std::chrono::seconds(15)));\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"200\", response->headers().getStatusValue());\n}\n\nTEST_P(HttpTimeoutIntegrationTest, HedgedPerTryTimeoutWithBodyNoBufferFirstRequestWins) {\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024, 512, true);\n}\n\nTEST_P(HttpTimeoutIntegrationTest, HedgedPerTryTimeoutWithBodyNoBufferSecondRequestWins) {\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024, 512, false);\n}\n\nTEST_P(HttpTimeoutIntegrationTest,\n       HedgedPerTryTimeoutLowUpstreamBufferLimitLargeRequestFirstRequestWins) {\n  config_helper_.setBufferLimits(1024, 1024 * 1024); // Set buffer limits upstream and downstream.\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024 * 1024, 1024, true);\n}\n\nTEST_P(HttpTimeoutIntegrationTest,\n       HedgedPerTryTimeoutLowUpstreamBufferLimitLargeRequestSecondRequestWins) {\n  config_helper_.setBufferLimits(1024, 1024 * 1024); // Set buffer limits upstream and downstream.\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024 * 1024, 1024, false);\n}\n\nTEST_P(HttpTimeoutIntegrationTest,\n       HedgedPerTryTimeoutLowDownstreamBufferLimitLargeResponseFirstRequestWins) {\n  config_helper_.setBufferLimits(1024 * 1024, 1024); // Set buffer limits upstream and downstream.\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024, 1024 * 1024, true);\n}\n\nTEST_P(HttpTimeoutIntegrationTest,\n       HedgedPerTryTimeoutLowDownstreamBufferLimitLargeResponseSecondRequestWins) {\n  config_helper_.setBufferLimits(1024 * 1024, 1024); // Set buffer limits upstream and downstream.\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024, 1024 * 1024, false);\n}\n\n// Sends a request with x-envoy-hedge-on-per-try-timeout, sleeps (with\n// simulated time) for longer than the per try timeout but shorter than the\n// global timeout, asserts that a retry is sent, and then responds with a 200\n// response on the original request and ensures the downstream sees it.\n// Request/response/header size are configurable to test flow control. If\n// first_request_wins is true, then the \"winning\" response will be sent in\n// response to the first (timed out) request. If false, the second request will\n// get the good response.\nvoid HttpTimeoutIntegrationTest::testRouterRequestAndResponseWithHedgedPerTryTimeout(\n    uint64_t request_size, uint64_t response_size, bool first_request_wins) {\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  Http::TestRequestHeaderMapImpl request_headers{{\":method\", \"POST\"},\n                                                 {\":path\", \"/test/long/url\"},\n                                                 {\":scheme\", \"http\"},\n                                                 {\":authority\", \"host\"},\n                                                 {\"x-forwarded-for\", \"10.0.0.1\"},\n                                                 {\"x-envoy-retry-on\", \"5xx\"},\n                                                 {\"x-envoy-hedge-on-per-try-timeout\", \"true\"},\n                                                 {\"x-envoy-upstream-rq-timeout-ms\", \"5000\"},\n                                                 {\"x-envoy-upstream-rq-per-try-timeout-ms\", \"400\"}};\n  auto encoder_decoder = codec_client_->startRequest(request_headers);\n\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n\n  codec_client_->sendData(*request_encoder_, request_size, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger per try timeout (but not global timeout).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(400));\n\n  FakeStreamPtr upstream_request2;\n  // Trigger retry (there's a 25ms backoff before it's issued).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(26));\n\n  // Wait for a second request to be sent upstream\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request2));\n  ASSERT_TRUE(upstream_request2->waitForHeadersComplete());\n  ASSERT_TRUE(upstream_request2->waitForEndStream(*dispatcher_));\n\n  Http::TestResponseHeaderMapImpl response_headers{{\":status\", \"200\"}};\n  if (first_request_wins) {\n    // Encode 200 response headers for the first (timed out) request.\n    upstream_request_->encodeHeaders(response_headers, response_size == 0);\n  } else {\n    // Encode 200 response headers for the second request.\n    upstream_request2->encodeHeaders(response_headers, response_size == 0);\n  }\n\n  response->waitForHeaders();\n\n  if (first_request_wins) {\n    // The second request should be reset since we used the response from the first request.\n    ASSERT_TRUE(upstream_request2->waitForReset(std::chrono::seconds(15)));\n  } else {\n    // The first request should be reset since we used the response from the second request.\n    ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n  }\n\n  if (response_size) {\n    if (first_request_wins) {\n      upstream_request_->encodeData(response_size, true);\n    } else {\n      upstream_request2->encodeData(response_size, true);\n    }\n  }\n\n  ASSERT_TRUE(response->waitForEndStream());\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_TRUE(upstream_request2->complete());\n  if (first_request_wins) {\n    EXPECT_EQ(request_size, upstream_request_->bodyLength());\n  } else {\n    EXPECT_EQ(request_size, upstream_request2->bodyLength());\n  }\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"200\", response->headers().getStatusValue());\n}\n\n// Starts a request with a header timeout specified, sleeps for longer than the\n// timeout, and ensures that a timeout is received.\nTEST_P(HttpTimeoutIntegrationTest, RequestHeaderTimeout) {\n  if (downstreamProtocol() != Http::CodecType::HTTP1) {\n    // This test requires that the downstream be using HTTP1.\n    return;\n  }\n\n  config_helper_.addConfigModifier(\n      [&](envoy::extensions::filters::network::http_connection_manager::v3::HttpConnectionManager&\n              hcm) {\n        auto* request_headers_timeout = hcm.mutable_request_headers_timeout();\n        request_headers_timeout->set_seconds(1);\n        request_headers_timeout->set_nanos(0);\n      });\n  initialize();\n\n  const std::string input_request = (\"GET / HTTP/1.1\\r\\n\"\n                                     // Omit trailing \\r\\n that would indicate the end of headers.\n                                     \"Host: localhost\\r\\n\");\n  std::string response;\n\n  auto connection_driver = createConnectionDriver(\n      lookupPort(\"http\"), input_request,\n      [&response](Network::ClientConnection&, const Buffer::Instance& data) -> void {\n        response.append(data.toString());\n      });\n\n  while (!connection_driver->allBytesSent()) {\n    ASSERT_TRUE(connection_driver->run(Event::Dispatcher::RunType::NonBlock));\n  }\n  test_server_->waitForGaugeGe(\"http.config_test.downstream_rq_active\", 1);\n  ASSERT_FALSE(connection_driver->closed());\n\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(1001));\n  ASSERT_TRUE(connection_driver->run());\n\n  // The upstream should send a 40x response and send a local reply.\n  EXPECT_TRUE(connection_driver->closed());\n  EXPECT_THAT(response, AllOf(HasSubstr(\"408\"), HasSubstr(\"header\")));\n}\n\n} // namespace Envoy\n"], "fixing_code": ["date: Pending\n\nbehavior_changes:\n# *Changes that are expected to cause an incompatibility if applicable; deployment changes are likely required*\n\nminor_behavior_changes:\n# *Changes that may cause incompatibilities for some users, but should not for most*\n- area: adaptive concurrency filter stats\n  change: |\n    Multiply the gradient value stat by 1000 to make it more granular (values will range between 500 and 2000).\n- area: dns\n  change: |\n    Allowing <envoy_v3_api_field_extensions.common.dynamic_forward_proxy.v3.DnsCacheConfig.dns_min_refresh_rate>` to go as low as 1s.\n- area: upstream\n  change: |\n    Upstream now excludes hosts set to ``DRAINING`` state via EDS from load balancing and panic routing\n    threshold calculation. This feature can be disabled by setting\n    ``envoy.reloadable_features.exclude_host_in_eds_status_draining`` to false.\n- area: golang\n  change: |\n    Change ``RegisterHttpFilterConfigFactoryAndParser`` to ``RegisterHttpFilterFactoryAndConfigParser``.\n- area: QUIC\n  change: |\n    Port migration is default turned off. QUIC client connections will no longer attempt to migrate to a new port when connections\n    is degrading. Can be manually turned on via\n    :ref:`port_migration <envoy_v3_api_field_config.core.v3.QuicProtocolOptions.num_timeouts_to_trigger_port_migration>`.\n- area: aws\n  change: |\n    AWS region string is now retrieved from environment and profile consistently within aws_request_signer and\n    grpc_credentials/aws_iam extensions. Region field in aws_request_signer is now optional, explicitly configured\n    xDS region will take preference. aws_request_signer documentation now reflects the region chain.\n\nbug_fixes:\n# *Changes expected to improve the state of the world and are unlikely to have negative effects*\n- area: tracers\n  change: |\n    use unary RPC calls for OpenTelemetry trace exports, rather than client-side streaming connections.\n- area: load balancing\n  change: |\n    Added randomization in locality load-balancing initialization. This helps desynchronizing Envoys across\n    a fleet by randomizing the scheduler starting point. This can be temporarily reverted by setting runtime guard\n    ``envoy.reloadable_features.edf_lb_locality_scheduler_init_fix`` to false.\n- area: UDP and TCP tunneling\n  change: |\n    fixed a bug where second HTTP response headers received would cause Envoy to crash in cases where\n    ``propagate_response_headers`` and retry configurations are enabled at the same time, and an upstream\n    request is retried multiple times.\n- area: tracing\n  change: |\n    Prevent Envoy from crashing at start up when the OpenTelemetry environment resource detector cannot detect any attributes.\n- area: proxy protocol\n  change: |\n    Fixed a crash when Envoy is configured for PROXY protocol on both a listener and cluster, and the listener receives\n    a PROXY protocol header with address type LOCAL (typically used for health checks).\n- area: url matching\n  change: |\n    Fixed excessive CPU utilization when using regex URL template matcher.\n- area: http\n  change: |\n    Fixed crash when HTTP request idle and per try timeouts occurs within backoff interval.\n\nremoved_config_or_runtime:\n# *Normally occurs at the end of the* :ref:`deprecation period <deprecated>`\n- area: http\n  change: |\n    Removed ``envoy.reloadable_features.allow_absolute_url_with_mixed_scheme`` runtime flag and legacy code paths.\n- area: active health check\n  change: |\n    Removed ``envoy.reloadable_features.keep_endpoint_active_hc_status_on_locality_update`` runtime flag and legacy code paths.\n- area: http1\n  change: |\n    Removed ``envoy.reloadable_features.http1_allow_codec_error_response_after_1xx_headers`` runtime flag and legacy code paths.\n- area: overload manager\n  change: |\n    removed ``envoy.reloadable_features.overload_manager_error_unknown_action`` and legacy code paths.\n- area: http\n  change: |\n    Removed ``envoy_reloadable_features_append_xfh_idempotent`` runtime flag and legacy code paths.\n- area: resource_monitors\n  change: |\n    removed ``envoy.reloadable_features.count_unused_mapped_pages_as_free`` runtime flag  and legacy code paths.\n\nnew_features:\n- area: aws_request_signing\n  change: |\n    Update ``aws_request_signing`` filter to support use as an upstream HTTP filter. This allows successful calculation of\n    signatures after the forwarding stage has completed, particularly if the path element is modified.\n- area: aws_lambda\n  change: |\n    Update ``aws_lambda`` filter to support use as an upstream HTTP filter. This allows successful calculation of\n    signatures after the forwarding stage has completed, particularly if the path element is modified.\n- area: grpc reverse bridge\n  change: |\n    Change HTTP status to 200 to respect the gRPC protocol. This may cause problems for incorrect gRPC clients expecting the filter\n    to preserve HTTP 1.1 responses.  This behavioral change can be temporarily reverted by setting runtime guard\n    ``envoy.reloadable_features.grpc_http1_reverse_bridge_change_http_status`` to false.\n- area: quic\n  change: |\n    Added QUIC protocol option :ref:`send_disable_active_migration\n    <envoy_v3_api_field_config.listener.v3.QuicProtocolOptions.send_disable_active_migration>` to make the server send clients a transport\n    parameter to discourage client endpoints from active migration.\n- area: ext_proc\n  change: |\n    implemented\n    :ref:`request_attributes <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.ExternalProcessor.request_attributes>`\n    and\n    :ref:`response_attributes <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.ExternalProcessor.response_attributes>`\n    config APIs to enable sending and receiving attributes to/from the external processing server.\n- area: access log\n  change: |\n    added support for :ref:`%UPSTREAM_CONNECTION_ID% <config_access_log_format_upstream_connection_id>` for the upstream connection\n    identifier.\n- area: aws_lambda\n  change: |\n    Added :ref:`host_rewrite <envoy_v3_api_field_extensions.filters.http.aws_lambda.v3.Config.host_rewrite>` config to be used\n    during signature.\n- area: ext_proc\n  change: |\n    added\n    :ref:`metadata_options <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.ExternalProcessor.metadata_options>`\n    config API to enable sending and receiving metadata from/to the external processing server. Both typed and untyped dynamic\n    metadata may be sent to the server. If\n    :ref:`receiving_namespaces <envoy_v3_api_field_extensions.filters.http.ext_proc.v3.MetadataOptions.receiving_namespaces>`\n    is defined, returned metadata may be written to the specified allowed namespaces.\n- area: monitoring\n  change: |\n    Add ``Envoy::ExecutionContext``, which is notified by ``ScopeTrackerScopeState``'s constructor and destructor. This feature is\n    disabled by default, it can be enabled by runtime feature flag ``envoy.restart_features.enable_execution_context``. For more details,\n    please see https://github.com/envoyproxy/envoy/issues/32012.\n- area: rbac\n  change: |\n    Added :ref:`uri_template<envoy_v3_api_field_config.rbac.v3.Permission.uri_template>` which uses existing\n    :ref:`UriTemplateMatchConfig<envoy_v3_api_msg_extensions.path.match.uri_template.v3.UriTemplateMatchConfig>`\n    to allow use of glob patterns for URI path matching in RBAC.\n- area: upstream\n  change: |\n    Added :ref:`selection_method <envoy_v3_api_msg_extensions.load_balancing_policies.least_request.v3.LeastRequest>`\n    option to the least request load balancer. If set to ``FULL_SCAN``,\n    Envoy will select the host with the fewest active requests from the entire host set rather than\n    :ref:`choice_count <envoy_v3_api_msg_extensions.load_balancing_policies.least_request.v3.LeastRequest>`\n    random choices.\n\ndeprecated:\n", "#include \"source/common/router/router.h\"\n\n#include <algorithm>\n#include <chrono>\n#include <cstdint>\n#include <functional>\n#include <memory>\n#include <string>\n\n#include \"envoy/event/dispatcher.h\"\n#include \"envoy/event/timer.h\"\n#include \"envoy/grpc/status.h\"\n#include \"envoy/http/conn_pool.h\"\n#include \"envoy/runtime/runtime.h\"\n#include \"envoy/upstream/cluster_manager.h\"\n#include \"envoy/upstream/health_check_host_monitor.h\"\n#include \"envoy/upstream/upstream.h\"\n\n#include \"source/common/common/assert.h\"\n#include \"source/common/common/cleanup.h\"\n#include \"source/common/common/empty_string.h\"\n#include \"source/common/common/enum_to_int.h\"\n#include \"source/common/common/scope_tracker.h\"\n#include \"source/common/common/utility.h\"\n#include \"source/common/config/utility.h\"\n#include \"source/common/grpc/common.h\"\n#include \"source/common/http/codes.h\"\n#include \"source/common/http/header_map_impl.h\"\n#include \"source/common/http/headers.h\"\n#include \"source/common/http/message_impl.h\"\n#include \"source/common/http/utility.h\"\n#include \"source/common/network/application_protocol.h\"\n#include \"source/common/network/socket_option_factory.h\"\n#include \"source/common/network/transport_socket_options_impl.h\"\n#include \"source/common/network/upstream_server_name.h\"\n#include \"source/common/network/upstream_socket_options_filter_state.h\"\n#include \"source/common/network/upstream_subject_alt_names.h\"\n#include \"source/common/router/config_impl.h\"\n#include \"source/common/router/debug_config.h\"\n#include \"source/common/router/retry_state_impl.h\"\n#include \"source/common/runtime/runtime_features.h\"\n#include \"source/common/stream_info/uint32_accessor_impl.h\"\n#include \"source/common/tracing/http_tracer_impl.h\"\n\nnamespace Envoy {\nnamespace Router {\nnamespace {\nconstexpr char NumInternalRedirectsFilterStateName[] = \"num_internal_redirects\";\n\nuint32_t getLength(const Buffer::Instance* instance) { return instance ? instance->length() : 0; }\n\nbool schemeIsHttp(const Http::RequestHeaderMap& downstream_headers,\n                  OptRef<const Network::Connection> connection) {\n  if (Http::Utility::schemeIsHttp(downstream_headers.getSchemeValue())) {\n    return true;\n  }\n  if (connection.has_value() && !connection->ssl()) {\n    return true;\n  }\n  return false;\n}\n\nconstexpr uint64_t TimeoutPrecisionFactor = 100;\n\n} // namespace\n\nFilterConfig::FilterConfig(Stats::StatName stat_prefix,\n                           Server::Configuration::FactoryContext& context,\n                           ShadowWriterPtr&& shadow_writer,\n                           const envoy::extensions::filters::http::router::v3::Router& config)\n    : FilterConfig(\n          stat_prefix, context.serverFactoryContext().localInfo(), context.scope(),\n          context.serverFactoryContext().clusterManager(), context.serverFactoryContext().runtime(),\n          context.serverFactoryContext().api().randomGenerator(), std::move(shadow_writer),\n          PROTOBUF_GET_WRAPPED_OR_DEFAULT(config, dynamic_stats, true), config.start_child_span(),\n          config.suppress_envoy_headers(), config.respect_expected_rq_timeout(),\n          config.suppress_grpc_request_failure_code_stats(),\n          config.has_upstream_log_options()\n              ? config.upstream_log_options().flush_upstream_log_on_upstream_stream()\n              : false,\n          config.strict_check_headers(), context.serverFactoryContext().api().timeSource(),\n          context.serverFactoryContext().httpContext(),\n          context.serverFactoryContext().routerContext()) {\n  for (const auto& upstream_log : config.upstream_log()) {\n    upstream_logs_.push_back(AccessLog::AccessLogFactory::fromProto(upstream_log, context));\n  }\n\n  if (config.has_upstream_log_options() &&\n      config.upstream_log_options().has_upstream_log_flush_interval()) {\n    upstream_log_flush_interval_ = std::chrono::milliseconds(DurationUtil::durationToMilliseconds(\n        config.upstream_log_options().upstream_log_flush_interval()));\n  }\n\n  if (config.upstream_http_filters_size() > 0) {\n    auto& server_factory_ctx = context.serverFactoryContext();\n    const Http::FilterChainUtility::FiltersList& upstream_http_filters =\n        config.upstream_http_filters();\n    std::shared_ptr<Http::UpstreamFilterConfigProviderManager> filter_config_provider_manager =\n        Http::FilterChainUtility::createSingletonUpstreamFilterConfigProviderManager(\n            server_factory_ctx);\n    std::string prefix = context.scope().symbolTable().toString(context.scope().prefix());\n    upstream_ctx_ = std::make_unique<Upstream::UpstreamFactoryContextImpl>(\n        server_factory_ctx, context.initManager(), context.scope());\n    Http::FilterChainHelper<Server::Configuration::UpstreamFactoryContext,\n                            Server::Configuration::UpstreamHttpFilterConfigFactory>\n        helper(*filter_config_provider_manager, server_factory_ctx,\n               context.serverFactoryContext().clusterManager(), *upstream_ctx_, prefix);\n    THROW_IF_NOT_OK(helper.processFilters(upstream_http_filters, \"router upstream http\",\n                                          \"router upstream http\", upstream_http_filter_factories_));\n  }\n}\n\n// Express percentage as [0, TimeoutPrecisionFactor] because stats do not accept floating point\n// values, and getting multiple significant figures on the histogram would be nice.\nuint64_t FilterUtility::percentageOfTimeout(const std::chrono::milliseconds response_time,\n                                            const std::chrono::milliseconds timeout) {\n  // Timeouts of 0 are considered infinite. Any portion of an infinite timeout used is still\n  // none of it.\n  if (timeout.count() == 0) {\n    return 0;\n  }\n\n  return static_cast<uint64_t>(response_time.count() * TimeoutPrecisionFactor / timeout.count());\n}\n\nvoid FilterUtility::setUpstreamScheme(Http::RequestHeaderMap& headers, bool downstream_secure) {\n  if (Http::Utility::schemeIsValid(headers.getSchemeValue())) {\n    return;\n  }\n  // After all the changes in https://github.com/envoyproxy/envoy/issues/14587\n  // this path should only occur if a buggy filter has removed the :scheme\n  // header. In that case best-effort set from X-Forwarded-Proto.\n  absl::string_view xfp = headers.getForwardedProtoValue();\n  if (Http::Utility::schemeIsValid(xfp)) {\n    headers.setScheme(xfp);\n    return;\n  }\n\n  if (downstream_secure) {\n    headers.setReferenceScheme(Http::Headers::get().SchemeValues.Https);\n  } else {\n    headers.setReferenceScheme(Http::Headers::get().SchemeValues.Http);\n  }\n}\n\nbool FilterUtility::shouldShadow(const ShadowPolicy& policy, Runtime::Loader& runtime,\n                                 uint64_t stable_random) {\n\n  // The policy's default value is set correctly regardless of whether there is a runtime key\n  // or not, thus this call is sufficient for all cases (100% if no runtime set, otherwise\n  // using the default value within the runtime fractional percent setting).\n  return runtime.snapshot().featureEnabled(policy.runtimeKey(), policy.defaultValue(),\n                                           stable_random);\n}\n\nTimeoutData FilterUtility::finalTimeout(const RouteEntry& route,\n                                        Http::RequestHeaderMap& request_headers,\n                                        bool insert_envoy_expected_request_timeout_ms,\n                                        bool grpc_request, bool per_try_timeout_hedging_enabled,\n                                        bool respect_expected_rq_timeout) {\n  // See if there is a user supplied timeout in a request header. If there is we take that.\n  // Otherwise if the request is gRPC and a maximum gRPC timeout is configured we use the timeout\n  // in the gRPC headers (or infinity when gRPC headers have no timeout), but cap that timeout to\n  // the configured maximum gRPC timeout (which may also be infinity, represented by a 0 value),\n  // or the default from the route config otherwise.\n  TimeoutData timeout;\n  if (!route.usingNewTimeouts()) {\n    if (grpc_request && route.maxGrpcTimeout()) {\n      const std::chrono::milliseconds max_grpc_timeout = route.maxGrpcTimeout().value();\n      auto header_timeout = Grpc::Common::getGrpcTimeout(request_headers);\n      std::chrono::milliseconds grpc_timeout =\n          header_timeout ? header_timeout.value() : std::chrono::milliseconds(0);\n      if (route.grpcTimeoutOffset()) {\n        // We only apply the offset if it won't result in grpc_timeout hitting 0 or below, as\n        // setting it to 0 means infinity and a negative timeout makes no sense.\n        const auto offset = *route.grpcTimeoutOffset();\n        if (offset < grpc_timeout) {\n          grpc_timeout -= offset;\n        }\n      }\n\n      // Cap gRPC timeout to the configured maximum considering that 0 means infinity.\n      if (max_grpc_timeout != std::chrono::milliseconds(0) &&\n          (grpc_timeout == std::chrono::milliseconds(0) || grpc_timeout > max_grpc_timeout)) {\n        grpc_timeout = max_grpc_timeout;\n      }\n      timeout.global_timeout_ = grpc_timeout;\n    } else {\n      timeout.global_timeout_ = route.timeout();\n    }\n  }\n  timeout.per_try_timeout_ = route.retryPolicy().perTryTimeout();\n  timeout.per_try_idle_timeout_ = route.retryPolicy().perTryIdleTimeout();\n\n  uint64_t header_timeout;\n\n  if (respect_expected_rq_timeout) {\n    // Check if there is timeout set by egress Envoy.\n    // If present, use that value as route timeout and don't override\n    // *x-envoy-expected-rq-timeout-ms* header. At this point *x-envoy-upstream-rq-timeout-ms*\n    // header should have been sanitized by egress Envoy.\n    const Http::HeaderEntry* header_expected_timeout_entry =\n        request_headers.EnvoyExpectedRequestTimeoutMs();\n    if (header_expected_timeout_entry) {\n      trySetGlobalTimeout(*header_expected_timeout_entry, timeout);\n    } else {\n      const Http::HeaderEntry* header_timeout_entry =\n          request_headers.EnvoyUpstreamRequestTimeoutMs();\n\n      if (header_timeout_entry) {\n        trySetGlobalTimeout(*header_timeout_entry, timeout);\n        request_headers.removeEnvoyUpstreamRequestTimeoutMs();\n      }\n    }\n  } else {\n    const Http::HeaderEntry* header_timeout_entry = request_headers.EnvoyUpstreamRequestTimeoutMs();\n\n    if (header_timeout_entry) {\n      trySetGlobalTimeout(*header_timeout_entry, timeout);\n      request_headers.removeEnvoyUpstreamRequestTimeoutMs();\n    }\n  }\n\n  // See if there is a per try/retry timeout. If it's >= global we just ignore it.\n  const absl::string_view per_try_timeout_entry =\n      request_headers.getEnvoyUpstreamRequestPerTryTimeoutMsValue();\n  if (!per_try_timeout_entry.empty()) {\n    if (absl::SimpleAtoi(per_try_timeout_entry, &header_timeout)) {\n      timeout.per_try_timeout_ = std::chrono::milliseconds(header_timeout);\n    }\n    request_headers.removeEnvoyUpstreamRequestPerTryTimeoutMs();\n  }\n\n  if (timeout.per_try_timeout_ >= timeout.global_timeout_ && timeout.global_timeout_.count() != 0) {\n    timeout.per_try_timeout_ = std::chrono::milliseconds(0);\n  }\n\n  setTimeoutHeaders(0, timeout, route, request_headers, insert_envoy_expected_request_timeout_ms,\n                    grpc_request, per_try_timeout_hedging_enabled);\n\n  return timeout;\n}\n\nvoid FilterUtility::setTimeoutHeaders(uint64_t elapsed_time, const TimeoutData& timeout,\n                                      const RouteEntry& route,\n                                      Http::RequestHeaderMap& request_headers,\n                                      bool insert_envoy_expected_request_timeout_ms,\n                                      bool grpc_request, bool per_try_timeout_hedging_enabled) {\n\n  const uint64_t global_timeout = timeout.global_timeout_.count();\n\n  // See if there is any timeout to write in the expected timeout header.\n  uint64_t expected_timeout = timeout.per_try_timeout_.count();\n\n  // Use the global timeout if no per try timeout was specified or if we're\n  // doing hedging when there are per try timeouts. Either of these scenarios\n  // mean that the upstream server can use the full global timeout.\n  if (per_try_timeout_hedging_enabled || expected_timeout == 0) {\n    expected_timeout = global_timeout;\n  }\n\n  // If the expected timeout is 0 set no timeout, as Envoy treats 0 as infinite timeout.\n  if (expected_timeout > 0) {\n\n    if (global_timeout > 0) {\n      if (elapsed_time >= global_timeout) {\n        // We are out of time, but 0 would be an infinite timeout. So instead we send a 1ms timeout\n        // and assume the timers armed by onRequestComplete() will fire very soon.\n        expected_timeout = 1;\n      } else {\n        expected_timeout = std::min(expected_timeout, global_timeout - elapsed_time);\n      }\n    }\n\n    if (insert_envoy_expected_request_timeout_ms) {\n      request_headers.setEnvoyExpectedRequestTimeoutMs(expected_timeout);\n    }\n\n    // If we've configured max_grpc_timeout, override the grpc-timeout header with\n    // the expected timeout. This ensures that the optional per try timeout is reflected\n    // in grpc-timeout, ensuring that the upstream gRPC server is aware of the actual timeout.\n    if (grpc_request && !route.usingNewTimeouts() && route.maxGrpcTimeout()) {\n      Grpc::Common::toGrpcTimeout(std::chrono::milliseconds(expected_timeout), request_headers);\n    }\n  }\n}\n\nabsl::optional<std::chrono::milliseconds>\nFilterUtility::tryParseHeaderTimeout(const Http::HeaderEntry& header_timeout_entry) {\n  uint64_t header_timeout;\n  if (absl::SimpleAtoi(header_timeout_entry.value().getStringView(), &header_timeout)) {\n    return std::chrono::milliseconds(header_timeout);\n  }\n  return absl::nullopt;\n}\n\nvoid FilterUtility::trySetGlobalTimeout(const Http::HeaderEntry& header_timeout_entry,\n                                        TimeoutData& timeout) {\n  const auto timeout_ms = tryParseHeaderTimeout(header_timeout_entry);\n  if (timeout_ms.has_value()) {\n    timeout.global_timeout_ = timeout_ms.value();\n  }\n}\n\nFilterUtility::HedgingParams\nFilterUtility::finalHedgingParams(const RouteEntry& route,\n                                  Http::RequestHeaderMap& request_headers) {\n  HedgingParams hedging_params;\n  hedging_params.hedge_on_per_try_timeout_ = route.hedgePolicy().hedgeOnPerTryTimeout();\n\n  const Http::HeaderEntry* hedge_on_per_try_timeout_entry =\n      request_headers.EnvoyHedgeOnPerTryTimeout();\n  if (hedge_on_per_try_timeout_entry) {\n    if (hedge_on_per_try_timeout_entry->value() == \"true\") {\n      hedging_params.hedge_on_per_try_timeout_ = true;\n    }\n    if (hedge_on_per_try_timeout_entry->value() == \"false\") {\n      hedging_params.hedge_on_per_try_timeout_ = false;\n    }\n\n    request_headers.removeEnvoyHedgeOnPerTryTimeout();\n  }\n\n  return hedging_params;\n}\n\nFilter::~Filter() {\n  // Upstream resources should already have been cleaned.\n  ASSERT(upstream_requests_.empty());\n  ASSERT(!retry_state_);\n}\n\nconst FilterUtility::StrictHeaderChecker::HeaderCheckResult\nFilterUtility::StrictHeaderChecker::checkHeader(Http::RequestHeaderMap& headers,\n                                                const Http::LowerCaseString& target_header) {\n  if (target_header == Http::Headers::get().EnvoyUpstreamRequestTimeoutMs) {\n    return isInteger(headers.EnvoyUpstreamRequestTimeoutMs());\n  } else if (target_header == Http::Headers::get().EnvoyUpstreamRequestPerTryTimeoutMs) {\n    return isInteger(headers.EnvoyUpstreamRequestPerTryTimeoutMs());\n  } else if (target_header == Http::Headers::get().EnvoyMaxRetries) {\n    return isInteger(headers.EnvoyMaxRetries());\n  } else if (target_header == Http::Headers::get().EnvoyRetryOn) {\n    return hasValidRetryFields(headers.EnvoyRetryOn(), &Router::RetryStateImpl::parseRetryOn);\n  } else if (target_header == Http::Headers::get().EnvoyRetryGrpcOn) {\n    return hasValidRetryFields(headers.EnvoyRetryGrpcOn(),\n                               &Router::RetryStateImpl::parseRetryGrpcOn);\n  }\n  // Should only validate headers for which we have implemented a validator.\n  PANIC(\"unexpectedly reached\");\n}\n\nStats::StatName Filter::upstreamZone(Upstream::HostDescriptionConstSharedPtr upstream_host) {\n  return upstream_host ? upstream_host->localityZoneStatName() : config_.empty_stat_name_;\n}\n\nvoid Filter::chargeUpstreamCode(uint64_t response_status_code,\n                                const Http::ResponseHeaderMap& response_headers,\n                                Upstream::HostDescriptionConstSharedPtr upstream_host,\n                                bool dropped) {\n  // Passing the response_status_code explicitly is an optimization to avoid\n  // multiple calls to slow Http::Utility::getResponseStatus.\n  ASSERT(response_status_code == Http::Utility::getResponseStatus(response_headers));\n  if (config_.emit_dynamic_stats_ && !callbacks_->streamInfo().healthCheck()) {\n    const Http::HeaderEntry* upstream_canary_header = response_headers.EnvoyUpstreamCanary();\n    const bool is_canary = (upstream_canary_header && upstream_canary_header->value() == \"true\") ||\n                           (upstream_host ? upstream_host->canary() : false);\n    const bool internal_request = Http::HeaderUtility::isEnvoyInternalRequest(*downstream_headers_);\n\n    Stats::StatName upstream_zone = upstreamZone(upstream_host);\n    Http::CodeStats::ResponseStatInfo info{\n        config_.scope_,\n        cluster_->statsScope(),\n        config_.empty_stat_name_,\n        response_status_code,\n        internal_request,\n        route_entry_->virtualHost().statName(),\n        request_vcluster_ ? request_vcluster_->statName() : config_.empty_stat_name_,\n        route_stats_context_.has_value() ? route_stats_context_->statName()\n                                         : config_.empty_stat_name_,\n        config_.zone_name_,\n        upstream_zone,\n        is_canary};\n\n    Http::CodeStats& code_stats = httpContext().codeStats();\n    code_stats.chargeResponseStat(info, exclude_http_code_stats_);\n\n    if (alt_stat_prefix_ != nullptr) {\n      Http::CodeStats::ResponseStatInfo alt_info{config_.scope_,\n                                                 cluster_->statsScope(),\n                                                 alt_stat_prefix_->statName(),\n                                                 response_status_code,\n                                                 internal_request,\n                                                 config_.empty_stat_name_,\n                                                 config_.empty_stat_name_,\n                                                 config_.empty_stat_name_,\n                                                 config_.zone_name_,\n                                                 upstream_zone,\n                                                 is_canary};\n      code_stats.chargeResponseStat(alt_info, exclude_http_code_stats_);\n    }\n\n    if (dropped) {\n      cluster_->loadReportStats().upstream_rq_dropped_.inc();\n    }\n    if (upstream_host && Http::CodeUtility::is5xx(response_status_code)) {\n      upstream_host->stats().rq_error_.inc();\n    }\n  }\n}\n\nvoid Filter::chargeUpstreamCode(Http::Code code,\n                                Upstream::HostDescriptionConstSharedPtr upstream_host,\n                                bool dropped) {\n  const uint64_t response_status_code = enumToInt(code);\n  const auto fake_response_headers = Http::createHeaderMap<Http::ResponseHeaderMapImpl>(\n      {{Http::Headers::get().Status, std::to_string(response_status_code)}});\n  chargeUpstreamCode(response_status_code, *fake_response_headers, upstream_host, dropped);\n}\n\nHttp::FilterHeadersStatus Filter::decodeHeaders(Http::RequestHeaderMap& headers, bool end_stream) {\n  downstream_headers_ = &headers;\n\n  // Extract debug configuration from filter state. This is used further along to determine whether\n  // we should append cluster and host headers to the response, and whether to forward the request\n  // upstream.\n  const StreamInfo::FilterStateSharedPtr& filter_state = callbacks_->streamInfo().filterState();\n  const DebugConfig* debug_config = filter_state->getDataReadOnly<DebugConfig>(DebugConfig::key());\n\n  // TODO: Maybe add a filter API for this.\n  grpc_request_ = Grpc::Common::isGrpcRequestHeaders(headers);\n  exclude_http_code_stats_ = grpc_request_ && config_.suppress_grpc_request_failure_code_stats_;\n\n  // Only increment rq total stat if we actually decode headers here. This does not count requests\n  // that get handled by earlier filters.\n  stats_.rq_total_.inc();\n\n  // Initialize the `modify_headers` function as a no-op (so we don't have to remember to check it\n  // against nullptr before calling it), and feed it behavior later if/when we have cluster info\n  // headers to append.\n  std::function<void(Http::ResponseHeaderMap&)> modify_headers = [](Http::ResponseHeaderMap&) {};\n\n  // Determine if there is a route entry or a direct response for the request.\n  route_ = callbacks_->route();\n  if (!route_) {\n    stats_.no_route_.inc();\n    ENVOY_STREAM_LOG(debug, \"no route match for URL '{}'\", *callbacks_, headers.getPathValue());\n\n    callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::NoRouteFound);\n    callbacks_->sendLocalReply(Http::Code::NotFound, \"\", modify_headers, absl::nullopt,\n                               StreamInfo::ResponseCodeDetails::get().RouteNotFound);\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n\n  // Determine if there is a direct response for the request.\n  const auto* direct_response = route_->directResponseEntry();\n  if (direct_response != nullptr) {\n    stats_.rq_direct_response_.inc();\n    direct_response->rewritePathHeader(headers, !config_.suppress_envoy_headers_);\n    callbacks_->sendLocalReply(\n        direct_response->responseCode(), direct_response->responseBody(),\n        [this, direct_response,\n         &request_headers = headers](Http::ResponseHeaderMap& response_headers) -> void {\n          std::string new_uri;\n          if (request_headers.Path()) {\n            new_uri = direct_response->newUri(request_headers);\n          }\n          // See https://tools.ietf.org/html/rfc7231#section-7.1.2.\n          const auto add_location =\n              direct_response->responseCode() == Http::Code::Created ||\n              Http::CodeUtility::is3xx(enumToInt(direct_response->responseCode()));\n          if (!new_uri.empty() && add_location) {\n            response_headers.addReferenceKey(Http::Headers::get().Location, new_uri);\n          }\n          direct_response->finalizeResponseHeaders(response_headers, callbacks_->streamInfo());\n        },\n        absl::nullopt, StreamInfo::ResponseCodeDetails::get().DirectResponse);\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n\n  // A route entry matches for the request.\n  route_entry_ = route_->routeEntry();\n  // If there's a route specific limit and it's smaller than general downstream\n  // limits, apply the new cap.\n  retry_shadow_buffer_limit_ =\n      std::min(retry_shadow_buffer_limit_, route_entry_->retryShadowBufferLimit());\n  if (debug_config && debug_config->append_cluster_) {\n    // The cluster name will be appended to any local or upstream responses from this point.\n    modify_headers = [this, debug_config](Http::ResponseHeaderMap& headers) {\n      headers.addCopy(debug_config->cluster_header_.value_or(Http::Headers::get().EnvoyCluster),\n                      route_entry_->clusterName());\n    };\n  }\n  Upstream::ThreadLocalCluster* cluster =\n      config_.cm_.getThreadLocalCluster(route_entry_->clusterName());\n  if (!cluster) {\n    stats_.no_cluster_.inc();\n    ENVOY_STREAM_LOG(debug, \"unknown cluster '{}'\", *callbacks_, route_entry_->clusterName());\n\n    callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::NoClusterFound);\n    callbacks_->sendLocalReply(route_entry_->clusterNotFoundResponseCode(), \"\", modify_headers,\n                               absl::nullopt,\n                               StreamInfo::ResponseCodeDetails::get().ClusterNotFound);\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n  cluster_ = cluster->info();\n\n  // Set up stat prefixes, etc.\n  request_vcluster_ = route_entry_->virtualCluster(headers);\n  if (request_vcluster_ != nullptr) {\n    callbacks_->streamInfo().setVirtualClusterName(request_vcluster_->name());\n  }\n  route_stats_context_ = route_entry_->routeStatsContext();\n  ENVOY_STREAM_LOG(debug, \"cluster '{}' match for URL '{}'\", *callbacks_,\n                   route_entry_->clusterName(), headers.getPathValue());\n\n  if (config_.strict_check_headers_ != nullptr) {\n    for (const auto& header : *config_.strict_check_headers_) {\n      const auto res = FilterUtility::StrictHeaderChecker::checkHeader(headers, header);\n      if (!res.valid_) {\n        callbacks_->streamInfo().setResponseFlag(\n            StreamInfo::CoreResponseFlag::InvalidEnvoyRequestHeaders);\n        const std::string body = fmt::format(\"invalid header '{}' with value '{}'\",\n                                             std::string(res.entry_->key().getStringView()),\n                                             std::string(res.entry_->value().getStringView()));\n        const std::string details =\n            absl::StrCat(StreamInfo::ResponseCodeDetails::get().InvalidEnvoyRequestHeaders, \"{\",\n                         StringUtil::replaceAllEmptySpace(res.entry_->key().getStringView()), \"}\");\n        callbacks_->sendLocalReply(Http::Code::BadRequest, body, nullptr, absl::nullopt, details);\n        return Http::FilterHeadersStatus::StopIteration;\n      }\n    }\n  }\n\n  const Http::HeaderEntry* request_alt_name = headers.EnvoyUpstreamAltStatName();\n  if (request_alt_name) {\n    alt_stat_prefix_ = std::make_unique<Stats::StatNameDynamicStorage>(\n        request_alt_name->value().getStringView(), config_.scope_.symbolTable());\n    headers.removeEnvoyUpstreamAltStatName();\n  }\n\n  // See if we are supposed to immediately kill some percentage of this cluster's traffic.\n  if (cluster_->maintenanceMode()) {\n    callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::UpstreamOverflow);\n    chargeUpstreamCode(Http::Code::ServiceUnavailable, nullptr, true);\n    callbacks_->sendLocalReply(\n        Http::Code::ServiceUnavailable, \"maintenance mode\",\n        [modify_headers, this](Http::ResponseHeaderMap& headers) {\n          if (!config_.suppress_envoy_headers_) {\n            headers.addReference(Http::Headers::get().EnvoyOverloaded,\n                                 Http::Headers::get().EnvoyOverloadedValues.True);\n          }\n          // Note: append_cluster_info does not respect suppress_envoy_headers.\n          modify_headers(headers);\n        },\n        absl::nullopt, StreamInfo::ResponseCodeDetails::get().MaintenanceMode);\n    cluster_->trafficStats()->upstream_rq_maintenance_mode_.inc();\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n\n  // Support DROP_OVERLOAD config from control plane to drop certain percentage of traffic.\n  if (checkDropOverload(*cluster, modify_headers)) {\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n\n  // Fetch a connection pool for the upstream cluster.\n  const auto& upstream_http_protocol_options = cluster_->upstreamHttpProtocolOptions();\n\n  if (upstream_http_protocol_options.has_value() &&\n      (upstream_http_protocol_options.value().auto_sni() ||\n       upstream_http_protocol_options.value().auto_san_validation())) {\n    // Default the header to Host/Authority header.\n    absl::string_view header_value = headers.getHostValue();\n\n    // Check whether `override_auto_sni_header` is specified.\n    const auto override_auto_sni_header =\n        upstream_http_protocol_options.value().override_auto_sni_header();\n    if (!override_auto_sni_header.empty()) {\n      // Use the header value from `override_auto_sni_header` to set the SNI value.\n      const auto overridden_header_value = Http::HeaderUtility::getAllOfHeaderAsString(\n          headers, Http::LowerCaseString(override_auto_sni_header));\n      if (overridden_header_value.result().has_value() &&\n          !overridden_header_value.result().value().empty()) {\n        header_value = overridden_header_value.result().value();\n      }\n    }\n    const auto parsed_authority = Http::Utility::parseAuthority(header_value);\n    bool should_set_sni = !parsed_authority.is_ip_address_;\n    // `host_` returns a string_view so doing this should be safe.\n    absl::string_view sni_value = parsed_authority.host_;\n\n    if (should_set_sni && upstream_http_protocol_options.value().auto_sni() &&\n        !callbacks_->streamInfo().filterState()->hasDataWithName(\n            Network::UpstreamServerName::key())) {\n      callbacks_->streamInfo().filterState()->setData(\n          Network::UpstreamServerName::key(),\n          std::make_unique<Network::UpstreamServerName>(sni_value),\n          StreamInfo::FilterState::StateType::Mutable);\n    }\n\n    if (upstream_http_protocol_options.value().auto_san_validation() &&\n        !callbacks_->streamInfo().filterState()->hasDataWithName(\n            Network::UpstreamSubjectAltNames::key())) {\n      callbacks_->streamInfo().filterState()->setData(\n          Network::UpstreamSubjectAltNames::key(),\n          std::make_unique<Network::UpstreamSubjectAltNames>(\n              std::vector<std::string>{std::string(sni_value)}),\n          StreamInfo::FilterState::StateType::Mutable);\n    }\n  }\n\n  transport_socket_options_ = Network::TransportSocketOptionsUtility::fromFilterState(\n      *callbacks_->streamInfo().filterState());\n\n  if (auto downstream_connection = downstreamConnection(); downstream_connection != nullptr) {\n    if (auto typed_state = downstream_connection->streamInfo()\n                               .filterState()\n                               .getDataReadOnly<Network::UpstreamSocketOptionsFilterState>(\n                                   Network::UpstreamSocketOptionsFilterState::key());\n        typed_state != nullptr) {\n      auto downstream_options = typed_state->value();\n      if (!upstream_options_) {\n        upstream_options_ = std::make_shared<Network::Socket::Options>();\n      }\n      Network::Socket::appendOptions(upstream_options_, downstream_options);\n    }\n  }\n\n  if (upstream_options_ && callbacks_->getUpstreamSocketOptions()) {\n    Network::Socket::appendOptions(upstream_options_, callbacks_->getUpstreamSocketOptions());\n  }\n\n  std::unique_ptr<GenericConnPool> generic_conn_pool = createConnPool(*cluster);\n\n  if (!generic_conn_pool) {\n    sendNoHealthyUpstreamResponse();\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n  Upstream::HostDescriptionConstSharedPtr host = generic_conn_pool->host();\n\n  if (debug_config && debug_config->append_upstream_host_) {\n    // The hostname and address will be appended to any local or upstream responses from this point,\n    // possibly in addition to the cluster name.\n    modify_headers = [modify_headers, debug_config, host](Http::ResponseHeaderMap& headers) {\n      modify_headers(headers);\n      headers.addCopy(\n          debug_config->hostname_header_.value_or(Http::Headers::get().EnvoyUpstreamHostname),\n          host->hostname());\n      headers.addCopy(debug_config->host_address_header_.value_or(\n                          Http::Headers::get().EnvoyUpstreamHostAddress),\n                      host->address()->asString());\n    };\n  }\n\n  // If we've been instructed not to forward the request upstream, send an empty local response.\n  if (debug_config && debug_config->do_not_forward_) {\n    modify_headers = [modify_headers, debug_config](Http::ResponseHeaderMap& headers) {\n      modify_headers(headers);\n      headers.addCopy(\n          debug_config->not_forwarded_header_.value_or(Http::Headers::get().EnvoyNotForwarded),\n          \"true\");\n    };\n    callbacks_->sendLocalReply(Http::Code::NoContent, \"\", modify_headers, absl::nullopt, \"\");\n    return Http::FilterHeadersStatus::StopIteration;\n  }\n\n  hedging_params_ = FilterUtility::finalHedgingParams(*route_entry_, headers);\n\n  timeout_ = FilterUtility::finalTimeout(*route_entry_, headers, !config_.suppress_envoy_headers_,\n                                         grpc_request_, hedging_params_.hedge_on_per_try_timeout_,\n                                         config_.respect_expected_rq_timeout_);\n\n  const Http::HeaderEntry* header_max_stream_duration_entry =\n      headers.EnvoyUpstreamStreamDurationMs();\n  if (header_max_stream_duration_entry) {\n    dynamic_max_stream_duration_ =\n        FilterUtility::tryParseHeaderTimeout(*header_max_stream_duration_entry);\n    headers.removeEnvoyUpstreamStreamDurationMs();\n  }\n\n  // If this header is set with any value, use an alternate response code on timeout\n  if (headers.EnvoyUpstreamRequestTimeoutAltResponse()) {\n    timeout_response_code_ = Http::Code::NoContent;\n    headers.removeEnvoyUpstreamRequestTimeoutAltResponse();\n  }\n\n  include_attempt_count_in_request_ = route_entry_->includeAttemptCountInRequest();\n  if (include_attempt_count_in_request_) {\n    headers.setEnvoyAttemptCount(attempt_count_);\n  }\n\n  // The router has reached a point where it is going to try to send a request upstream,\n  // so now modify_headers should attach x-envoy-attempt-count to the downstream response if the\n  // config flag is true.\n  if (route_entry_->includeAttemptCountInResponse()) {\n    modify_headers = [modify_headers, this](Http::ResponseHeaderMap& headers) {\n      modify_headers(headers);\n\n      // This header is added without checking for config_.suppress_envoy_headers_ to mirror what is\n      // done for upstream requests.\n      headers.setEnvoyAttemptCount(attempt_count_);\n    };\n  }\n  callbacks_->streamInfo().setAttemptCount(attempt_count_);\n\n  route_entry_->finalizeRequestHeaders(headers, callbacks_->streamInfo(),\n                                       !config_.suppress_envoy_headers_);\n  FilterUtility::setUpstreamScheme(\n      headers, callbacks_->streamInfo().downstreamAddressProvider().sslConnection() != nullptr);\n\n  // Ensure an http transport scheme is selected before continuing with decoding.\n  ASSERT(headers.Scheme());\n\n  retry_state_ =\n      createRetryState(route_entry_->retryPolicy(), headers, *cluster_, request_vcluster_,\n                       route_stats_context_, config_.runtime_, config_.random_,\n                       callbacks_->dispatcher(), config_.timeSource(), route_entry_->priority());\n\n  // Determine which shadow policies to use. It's possible that we don't do any shadowing due to\n  // runtime keys. Also the method CONNECT doesn't support shadowing.\n  auto method = headers.getMethodValue();\n  if (method != Http::Headers::get().MethodValues.Connect) {\n    for (const auto& shadow_policy : route_entry_->shadowPolicies()) {\n      const auto& policy_ref = *shadow_policy;\n      if (FilterUtility::shouldShadow(policy_ref, config_.runtime_, callbacks_->streamId())) {\n        active_shadow_policies_.push_back(std::cref(policy_ref));\n        shadow_headers_ = Http::createHeaderMap<Http::RequestHeaderMapImpl>(*downstream_headers_);\n      }\n    }\n  }\n\n  ENVOY_STREAM_LOG(debug, \"router decoding headers:\\n{}\", *callbacks_, headers);\n\n  // Hang onto the modify_headers function for later use in handling upstream responses.\n  modify_headers_ = modify_headers;\n\n  const bool can_send_early_data =\n      route_entry_->earlyDataPolicy().allowsEarlyDataForRequest(*downstream_headers_);\n\n  include_timeout_retry_header_in_request_ =\n      route_entry_->virtualHost().includeIsTimeoutRetryHeader();\n\n  // Set initial HTTP/3 use based on the presence of HTTP/1.1 proxy config.\n  // For retries etc, HTTP/3 usability may transition from true to false, but\n  // will never transition from false to true.\n  bool can_use_http3 =\n      !transport_socket_options_ || !transport_socket_options_->http11ProxyInfo().has_value();\n  UpstreamRequestPtr upstream_request = std::make_unique<UpstreamRequest>(\n      *this, std::move(generic_conn_pool), can_send_early_data, can_use_http3);\n  LinkedList::moveIntoList(std::move(upstream_request), upstream_requests_);\n  upstream_requests_.front()->acceptHeadersFromRouter(end_stream);\n  if (streaming_shadows_) {\n    // start the shadow streams.\n    for (const auto& shadow_policy_wrapper : active_shadow_policies_) {\n      const auto& shadow_policy = shadow_policy_wrapper.get();\n      const absl::optional<absl::string_view> shadow_cluster_name =\n          getShadowCluster(shadow_policy, *downstream_headers_);\n      if (!shadow_cluster_name.has_value()) {\n        continue;\n      }\n      auto shadow_headers = Http::createHeaderMap<Http::RequestHeaderMapImpl>(*shadow_headers_);\n      auto options =\n          Http::AsyncClient::RequestOptions()\n              .setTimeout(timeout_.global_timeout_)\n              .setParentSpan(callbacks_->activeSpan())\n              .setChildSpanName(\"mirror\")\n              .setSampled(shadow_policy.traceSampled())\n              .setIsShadow(true)\n              .setBufferAccount(callbacks_->account())\n              // A buffer limit of 1 is set in the case that retry_shadow_buffer_limit_ == 0,\n              // because a buffer limit of zero on async clients is interpreted as no buffer limit.\n              .setBufferLimit(1 > retry_shadow_buffer_limit_ ? 1 : retry_shadow_buffer_limit_);\n      options.setFilterConfig(config_);\n      if (end_stream) {\n        // This is a header-only request, and can be dispatched immediately to the shadow\n        // without waiting.\n        Http::RequestMessagePtr request(new Http::RequestMessageImpl(\n            Http::createHeaderMap<Http::RequestHeaderMapImpl>(*shadow_headers_)));\n        config_.shadowWriter().shadow(std::string(shadow_cluster_name.value()), std::move(request),\n                                      options);\n      } else {\n        Http::AsyncClient::OngoingRequest* shadow_stream = config_.shadowWriter().streamingShadow(\n            std::string(shadow_cluster_name.value()), std::move(shadow_headers), options);\n        if (shadow_stream != nullptr) {\n          shadow_streams_.insert(shadow_stream);\n          shadow_stream->setDestructorCallback(\n              [this, shadow_stream]() { shadow_streams_.erase(shadow_stream); });\n          shadow_stream->setWatermarkCallbacks(*callbacks_);\n        }\n      }\n    }\n  }\n  if (end_stream) {\n    onRequestComplete();\n  }\n\n  return Http::FilterHeadersStatus::StopIteration;\n}\n\nstd::unique_ptr<GenericConnPool>\nFilter::createConnPool(Upstream::ThreadLocalCluster& thread_local_cluster) {\n  GenericConnPoolFactory* factory = nullptr;\n  if (cluster_->upstreamConfig().has_value()) {\n    factory = Envoy::Config::Utility::getFactory<GenericConnPoolFactory>(\n        cluster_->upstreamConfig().ref());\n    ENVOY_BUG(factory != nullptr,\n              fmt::format(\"invalid factory type '{}', failing over to default upstream\",\n                          cluster_->upstreamConfig().ref().DebugString()));\n  }\n  if (!factory) {\n    factory = &config_.router_context_.genericConnPoolFactory();\n  }\n\n  using UpstreamProtocol = Envoy::Router::GenericConnPoolFactory::UpstreamProtocol;\n  UpstreamProtocol upstream_protocol = UpstreamProtocol::HTTP;\n  if (route_entry_->connectConfig().has_value()) {\n    auto method = downstream_headers_->getMethodValue();\n    if (Runtime::runtimeFeatureEnabled(\"envoy.reloadable_features.enable_connect_udp_support\") &&\n        Http::HeaderUtility::isConnectUdpRequest(*downstream_headers_)) {\n      upstream_protocol = UpstreamProtocol::UDP;\n    } else if (method == Http::Headers::get().MethodValues.Connect ||\n               (route_entry_->connectConfig()->allow_post() &&\n                method == Http::Headers::get().MethodValues.Post)) {\n      // Allow POST for proxying raw TCP if it is configured.\n      upstream_protocol = UpstreamProtocol::TCP;\n    }\n  }\n  return factory->createGenericConnPool(thread_local_cluster, upstream_protocol,\n                                        route_entry_->priority(),\n                                        callbacks_->streamInfo().protocol(), this);\n}\n\nvoid Filter::sendNoHealthyUpstreamResponse() {\n  callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::NoHealthyUpstream);\n  chargeUpstreamCode(Http::Code::ServiceUnavailable, nullptr, false);\n  callbacks_->sendLocalReply(Http::Code::ServiceUnavailable, \"no healthy upstream\", modify_headers_,\n                             absl::nullopt,\n                             StreamInfo::ResponseCodeDetails::get().NoHealthyUpstream);\n}\n\nHttp::FilterDataStatus Filter::decodeData(Buffer::Instance& data, bool end_stream) {\n  // upstream_requests_.size() cannot be > 1 because that only happens when a per\n  // try timeout occurs with hedge_on_per_try_timeout enabled but the per\n  // try timeout timer is not started until onRequestComplete(). It could be zero\n  // if the first request attempt has already failed and a retry is waiting for\n  // a backoff timer.\n  ASSERT(upstream_requests_.size() <= 1);\n\n  bool buffering = (retry_state_ && retry_state_->enabled()) ||\n                   (!active_shadow_policies_.empty() && !streaming_shadows_) ||\n                   (route_entry_ && route_entry_->internalRedirectPolicy().enabled());\n  if (buffering &&\n      getLength(callbacks_->decodingBuffer()) + data.length() > retry_shadow_buffer_limit_) {\n    ENVOY_LOG(debug,\n              \"The request payload has at least {} bytes data which exceeds buffer limit {}. Give \"\n              \"up on the retry/shadow.\",\n              getLength(callbacks_->decodingBuffer()) + data.length(), retry_shadow_buffer_limit_);\n    cluster_->trafficStats()->retry_or_shadow_abandoned_.inc();\n    retry_state_.reset();\n    buffering = false;\n    active_shadow_policies_.clear();\n    request_buffer_overflowed_ = true;\n\n    // If we had to abandon buffering and there's no request in progress, abort the request and\n    // clean up. This happens if the initial upstream request failed, and we are currently waiting\n    // for a backoff timer before starting the next upstream attempt.\n    if (upstream_requests_.empty()) {\n      cleanup();\n      callbacks_->sendLocalReply(\n          Http::Code::InsufficientStorage, \"exceeded request buffer limit while retrying upstream\",\n          modify_headers_, absl::nullopt,\n          StreamInfo::ResponseCodeDetails::get().RequestPayloadExceededRetryBufferLimit);\n      return Http::FilterDataStatus::StopIterationNoBuffer;\n    }\n  }\n\n  // If we aren't buffering and there is no active request, an abort should have occurred\n  // already.\n  ASSERT(buffering || !upstream_requests_.empty());\n\n  for (auto* shadow_stream : shadow_streams_) {\n    if (end_stream) {\n      shadow_stream->removeDestructorCallback();\n      shadow_stream->removeWatermarkCallbacks();\n    }\n    Buffer::OwnedImpl copy(data);\n    shadow_stream->sendData(copy, end_stream);\n  }\n  if (end_stream) {\n    shadow_streams_.clear();\n  }\n  if (buffering) {\n    if (!upstream_requests_.empty()) {\n      Buffer::OwnedImpl copy(data);\n      upstream_requests_.front()->acceptDataFromRouter(copy, end_stream);\n    }\n\n    // If we are potentially going to retry or buffer shadow this request we need to buffer.\n    // This will not cause the connection manager to 413 because before we hit the\n    // buffer limit we give up on retries and buffering. We must buffer using addDecodedData()\n    // so that all buffered data is available by the time we do request complete processing and\n    // potentially shadow. Additionally, we can't do a copy here because there's a check down\n    // this stack for whether `data` is the same buffer as already buffered data.\n    callbacks_->addDecodedData(data, true);\n  } else {\n    upstream_requests_.front()->acceptDataFromRouter(data, end_stream);\n  }\n\n  if (end_stream) {\n    onRequestComplete();\n  }\n\n  return Http::FilterDataStatus::StopIterationNoBuffer;\n}\n\nHttp::FilterTrailersStatus Filter::decodeTrailers(Http::RequestTrailerMap& trailers) {\n  ENVOY_STREAM_LOG(debug, \"router decoding trailers:\\n{}\", *callbacks_, trailers);\n\n  if (shadow_headers_) {\n    shadow_trailers_ = Http::createHeaderMap<Http::RequestTrailerMapImpl>(trailers);\n  }\n\n  // upstream_requests_.size() cannot be > 1 because that only happens when a per\n  // try timeout occurs with hedge_on_per_try_timeout enabled but the per\n  // try timeout timer is not started until onRequestComplete(). It could be zero\n  // if the first request attempt has already failed and a retry is waiting for\n  // a backoff timer.\n  ASSERT(upstream_requests_.size() <= 1);\n  downstream_trailers_ = &trailers;\n  if (!upstream_requests_.empty()) {\n    upstream_requests_.front()->acceptTrailersFromRouter(trailers);\n  }\n  for (auto* shadow_stream : shadow_streams_) {\n    shadow_stream->removeDestructorCallback();\n    shadow_stream->removeWatermarkCallbacks();\n    shadow_stream->captureAndSendTrailers(\n        Http::createHeaderMap<Http::RequestTrailerMapImpl>(*shadow_trailers_));\n  }\n  shadow_streams_.clear();\n\n  onRequestComplete();\n  return Http::FilterTrailersStatus::StopIteration;\n}\n\nHttp::FilterMetadataStatus Filter::decodeMetadata(Http::MetadataMap& metadata_map) {\n  Http::MetadataMapPtr metadata_map_ptr = std::make_unique<Http::MetadataMap>(metadata_map);\n  if (!upstream_requests_.empty()) {\n    // TODO(soya3129): Save metadata for retry, redirect and shadowing case.\n    upstream_requests_.front()->acceptMetadataFromRouter(std::move(metadata_map_ptr));\n  }\n  return Http::FilterMetadataStatus::Continue;\n}\n\nvoid Filter::setDecoderFilterCallbacks(Http::StreamDecoderFilterCallbacks& callbacks) {\n  callbacks_ = &callbacks;\n  // As the decoder filter only pushes back via watermarks once data has reached\n  // it, it can latch the current buffer limit and does not need to update the\n  // limit if another filter increases it.\n  //\n  // The default is \"do not limit\". If there are configured (non-zero) buffer\n  // limits, apply them here.\n  if (callbacks_->decoderBufferLimit() != 0) {\n    retry_shadow_buffer_limit_ = callbacks_->decoderBufferLimit();\n  }\n}\n\nvoid Filter::cleanup() {\n  // All callers of cleanup() should have cleaned out the upstream_requests_\n  // list as appropriate.\n  ASSERT(upstream_requests_.empty());\n\n  retry_state_.reset();\n  if (response_timeout_) {\n    response_timeout_->disableTimer();\n    response_timeout_.reset();\n  }\n}\n\nabsl::optional<absl::string_view> Filter::getShadowCluster(const ShadowPolicy& policy,\n                                                           const Http::HeaderMap& headers) const {\n  if (!policy.cluster().empty()) {\n    return policy.cluster();\n  } else {\n    ASSERT(!policy.clusterHeader().get().empty());\n    const auto entry = headers.get(policy.clusterHeader());\n    if (!entry.empty() && !entry[0]->value().empty()) {\n      return entry[0]->value().getStringView();\n    }\n    ENVOY_STREAM_LOG(debug, \"There is no cluster name in header: {}\", *callbacks_,\n                     policy.clusterHeader());\n    return absl::nullopt;\n  }\n}\n\nvoid Filter::maybeDoShadowing() {\n  for (const auto& shadow_policy_wrapper : active_shadow_policies_) {\n    const auto& shadow_policy = shadow_policy_wrapper.get();\n\n    const absl::optional<absl::string_view> shadow_cluster_name =\n        getShadowCluster(shadow_policy, *downstream_headers_);\n\n    // The cluster name got from headers is empty.\n    if (!shadow_cluster_name.has_value()) {\n      continue;\n    }\n\n    Http::RequestMessagePtr request(new Http::RequestMessageImpl(\n        Http::createHeaderMap<Http::RequestHeaderMapImpl>(*shadow_headers_)));\n    if (callbacks_->decodingBuffer()) {\n      request->body().add(*callbacks_->decodingBuffer());\n    }\n    if (shadow_trailers_) {\n      request->trailers(Http::createHeaderMap<Http::RequestTrailerMapImpl>(*shadow_trailers_));\n    }\n\n    auto options = Http::AsyncClient::RequestOptions()\n                       .setTimeout(timeout_.global_timeout_)\n                       .setParentSpan(callbacks_->activeSpan())\n                       .setChildSpanName(\"mirror\")\n                       .setSampled(shadow_policy.traceSampled())\n                       .setIsShadow(true);\n    options.setFilterConfig(config_);\n    config_.shadowWriter().shadow(std::string(shadow_cluster_name.value()), std::move(request),\n                                  options);\n  }\n}\n\nvoid Filter::onRequestComplete() {\n  // This should be called exactly once, when the downstream request has been received in full.\n  ASSERT(!downstream_end_stream_);\n  downstream_end_stream_ = true;\n  Event::Dispatcher& dispatcher = callbacks_->dispatcher();\n  downstream_request_complete_time_ = dispatcher.timeSource().monotonicTime();\n\n  // Possible that we got an immediate reset.\n  if (!upstream_requests_.empty()) {\n    // Even if we got an immediate reset, we could still shadow, but that is a riskier change and\n    // seems unnecessary right now.\n    if (!streaming_shadows_) {\n      maybeDoShadowing();\n    }\n\n    if (timeout_.global_timeout_.count() > 0) {\n      response_timeout_ = dispatcher.createTimer([this]() -> void { onResponseTimeout(); });\n      response_timeout_->enableTimer(timeout_.global_timeout_);\n    }\n\n    for (auto& upstream_request : upstream_requests_) {\n      if (upstream_request->createPerTryTimeoutOnRequestComplete()) {\n        upstream_request->setupPerTryTimeout();\n      }\n    }\n  }\n}\n\nvoid Filter::onDestroy() {\n  // Reset any in-flight upstream requests.\n  resetAll();\n\n  // Unregister from shadow stream notifications and cancel active streams.\n  for (auto* shadow_stream : shadow_streams_) {\n    shadow_stream->removeDestructorCallback();\n    shadow_stream->removeWatermarkCallbacks();\n    shadow_stream->cancel();\n  }\n\n  cleanup();\n}\n\nvoid Filter::onResponseTimeout() {\n  ENVOY_STREAM_LOG(debug, \"upstream timeout\", *callbacks_);\n\n  // Reset any upstream requests that are still in flight.\n  while (!upstream_requests_.empty()) {\n    UpstreamRequestPtr upstream_request =\n        upstream_requests_.back()->removeFromList(upstream_requests_);\n\n    // We want to record the upstream timeouts and increase the stats counters in all the cases.\n    // For example, we also want to record the stats in the case of BiDi streaming APIs where we\n    // might have already seen the headers.\n    cluster_->trafficStats()->upstream_rq_timeout_.inc();\n    if (request_vcluster_) {\n      request_vcluster_->stats().upstream_rq_timeout_.inc();\n    }\n    if (route_stats_context_.has_value()) {\n      route_stats_context_->stats().upstream_rq_timeout_.inc();\n    }\n\n    if (upstream_request->upstreamHost()) {\n      upstream_request->upstreamHost()->stats().rq_timeout_.inc();\n    }\n\n    if (upstream_request->awaitingHeaders()) {\n      if (cluster_->timeoutBudgetStats().has_value()) {\n        // Cancel firing per-try timeout information, because the per-try timeout did not come into\n        // play when the global timeout was hit.\n        upstream_request->recordTimeoutBudget(false);\n      }\n\n      // If this upstream request already hit a \"soft\" timeout, then it\n      // already recorded a timeout into outlier detection. Don't do it again.\n      if (!upstream_request->outlierDetectionTimeoutRecorded()) {\n        updateOutlierDetection(Upstream::Outlier::Result::LocalOriginTimeout, *upstream_request,\n                               absl::optional<uint64_t>(enumToInt(timeout_response_code_)));\n      }\n\n      chargeUpstreamAbort(timeout_response_code_, false, *upstream_request);\n    }\n    upstream_request->resetStream();\n  }\n\n  onUpstreamTimeoutAbort(StreamInfo::CoreResponseFlag::UpstreamRequestTimeout,\n                         StreamInfo::ResponseCodeDetails::get().ResponseTimeout);\n}\n\n// Called when the per try timeout is hit but we didn't reset the request\n// (hedge_on_per_try_timeout enabled).\nvoid Filter::onSoftPerTryTimeout(UpstreamRequest& upstream_request) {\n  ASSERT(!upstream_request.retried());\n  // Track this as a timeout for outlier detection purposes even though we didn't\n  // cancel the request yet and might get a 2xx later.\n  updateOutlierDetection(Upstream::Outlier::Result::LocalOriginTimeout, upstream_request,\n                         absl::optional<uint64_t>(enumToInt(timeout_response_code_)));\n  upstream_request.outlierDetectionTimeoutRecorded(true);\n\n  if (!downstream_response_started_ && retry_state_) {\n    RetryStatus retry_status = retry_state_->shouldHedgeRetryPerTryTimeout(\n        [this, can_use_http3 = upstream_request.upstreamStreamOptions().can_use_http3_]() -> void {\n          // Without any knowledge about what's going on in the connection pool, retry the request\n          // with the safest settings which is no early data but keep using or not using alt-svc as\n          // before. In this way, QUIC won't be falsely marked as broken.\n          doRetry(/*can_send_early_data*/ false, can_use_http3, TimeoutRetry::Yes);\n        });\n\n    if (retry_status == RetryStatus::Yes) {\n      runRetryOptionsPredicates(upstream_request);\n      pending_retries_++;\n\n      // Don't increment upstream_host->stats().rq_error_ here, we'll do that\n      // later if 1) we hit global timeout or 2) we get bad response headers\n      // back.\n      upstream_request.retried(true);\n\n      // TODO: cluster stat for hedge attempted.\n    } else if (retry_status == RetryStatus::NoOverflow) {\n      callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::UpstreamOverflow);\n    } else if (retry_status == RetryStatus::NoRetryLimitExceeded) {\n      callbacks_->streamInfo().setResponseFlag(\n          StreamInfo::CoreResponseFlag::UpstreamRetryLimitExceeded);\n    }\n  }\n}\n\nvoid Filter::onPerTryIdleTimeout(UpstreamRequest& upstream_request) {\n  onPerTryTimeoutCommon(upstream_request,\n                        cluster_->trafficStats()->upstream_rq_per_try_idle_timeout_,\n                        StreamInfo::ResponseCodeDetails::get().UpstreamPerTryIdleTimeout);\n}\n\nvoid Filter::onPerTryTimeout(UpstreamRequest& upstream_request) {\n  onPerTryTimeoutCommon(upstream_request, cluster_->trafficStats()->upstream_rq_per_try_timeout_,\n                        StreamInfo::ResponseCodeDetails::get().UpstreamPerTryTimeout);\n}\n\nvoid Filter::onPerTryTimeoutCommon(UpstreamRequest& upstream_request, Stats::Counter& error_counter,\n                                   const std::string& response_code_details) {\n  if (hedging_params_.hedge_on_per_try_timeout_) {\n    onSoftPerTryTimeout(upstream_request);\n    return;\n  }\n\n  error_counter.inc();\n  if (upstream_request.upstreamHost()) {\n    upstream_request.upstreamHost()->stats().rq_timeout_.inc();\n  }\n\n  upstream_request.resetStream();\n\n  updateOutlierDetection(Upstream::Outlier::Result::LocalOriginTimeout, upstream_request,\n                         absl::optional<uint64_t>(enumToInt(timeout_response_code_)));\n\n  if (maybeRetryReset(Http::StreamResetReason::LocalReset, upstream_request, TimeoutRetry::Yes)) {\n    return;\n  }\n\n  chargeUpstreamAbort(timeout_response_code_, false, upstream_request);\n\n  // Remove this upstream request from the list now that we're done with it.\n  upstream_request.removeFromList(upstream_requests_);\n  onUpstreamTimeoutAbort(StreamInfo::CoreResponseFlag::UpstreamRequestTimeout,\n                         response_code_details);\n}\n\nvoid Filter::onStreamMaxDurationReached(UpstreamRequest& upstream_request) {\n  upstream_request.resetStream();\n\n  if (maybeRetryReset(Http::StreamResetReason::LocalReset, upstream_request, TimeoutRetry::No)) {\n    return;\n  }\n\n  upstream_request.removeFromList(upstream_requests_);\n  cleanup();\n\n  callbacks_->streamInfo().setResponseFlag(\n      StreamInfo::CoreResponseFlag::UpstreamMaxStreamDurationReached);\n  // Grab the const ref to call the const method of StreamInfo.\n  const auto& stream_info = callbacks_->streamInfo();\n  const bool downstream_decode_complete =\n      stream_info.downstreamTiming().has_value() &&\n      stream_info.downstreamTiming().value().get().lastDownstreamRxByteReceived().has_value();\n\n  // sendLocalReply may instead reset the stream if downstream_response_started_ is true.\n  callbacks_->sendLocalReply(\n      Http::Utility::maybeRequestTimeoutCode(downstream_decode_complete),\n      \"upstream max stream duration reached\", modify_headers_, absl::nullopt,\n      StreamInfo::ResponseCodeDetails::get().UpstreamMaxStreamDurationReached);\n}\n\nvoid Filter::updateOutlierDetection(Upstream::Outlier::Result result,\n                                    UpstreamRequest& upstream_request,\n                                    absl::optional<uint64_t> code) {\n  if (upstream_request.upstreamHost()) {\n    upstream_request.upstreamHost()->outlierDetector().putResult(result, code);\n  }\n}\n\nvoid Filter::chargeUpstreamAbort(Http::Code code, bool dropped, UpstreamRequest& upstream_request) {\n  if (downstream_response_started_) {\n    if (upstream_request.grpcRqSuccessDeferred()) {\n      upstream_request.upstreamHost()->stats().rq_error_.inc();\n      stats_.rq_reset_after_downstream_response_started_.inc();\n    }\n  } else {\n    Upstream::HostDescriptionConstSharedPtr upstream_host = upstream_request.upstreamHost();\n\n    chargeUpstreamCode(code, upstream_host, dropped);\n    // If we had non-5xx but still have been reset by backend or timeout before\n    // starting response, we treat this as an error. We only get non-5xx when\n    // timeout_response_code_ is used for code above, where this member can\n    // assume values such as 204 (NoContent).\n    if (upstream_host != nullptr && !Http::CodeUtility::is5xx(enumToInt(code))) {\n      upstream_host->stats().rq_error_.inc();\n    }\n  }\n}\n\nvoid Filter::onUpstreamTimeoutAbort(StreamInfo::CoreResponseFlag response_flags,\n                                    absl::string_view details) {\n  Upstream::ClusterTimeoutBudgetStatsOptRef tb_stats = cluster()->timeoutBudgetStats();\n  if (tb_stats.has_value()) {\n    Event::Dispatcher& dispatcher = callbacks_->dispatcher();\n    std::chrono::milliseconds response_time = std::chrono::duration_cast<std::chrono::milliseconds>(\n        dispatcher.timeSource().monotonicTime() - downstream_request_complete_time_);\n\n    tb_stats->get().upstream_rq_timeout_budget_percent_used_.recordValue(\n        FilterUtility::percentageOfTimeout(response_time, timeout_.global_timeout_));\n  }\n\n  const absl::string_view body =\n      timeout_response_code_ == Http::Code::GatewayTimeout ? \"upstream request timeout\" : \"\";\n  onUpstreamAbort(timeout_response_code_, response_flags, body, false, details);\n}\n\nvoid Filter::onUpstreamAbort(Http::Code code, StreamInfo::CoreResponseFlag response_flags,\n                             absl::string_view body, bool dropped, absl::string_view details) {\n  // If we have not yet sent anything downstream, send a response with an appropriate status code.\n  // Otherwise just reset the ongoing response.\n  callbacks_->streamInfo().setResponseFlag(response_flags);\n  // This will destroy any created retry timers.\n  cleanup();\n  // sendLocalReply may instead reset the stream if downstream_response_started_ is true.\n  callbacks_->sendLocalReply(\n      code, body,\n      [dropped, this](Http::ResponseHeaderMap& headers) {\n        if (dropped && !config_.suppress_envoy_headers_) {\n          headers.addReference(Http::Headers::get().EnvoyOverloaded,\n                               Http::Headers::get().EnvoyOverloadedValues.True);\n        }\n        modify_headers_(headers);\n      },\n      absl::nullopt, details);\n}\n\nbool Filter::maybeRetryReset(Http::StreamResetReason reset_reason,\n                             UpstreamRequest& upstream_request, TimeoutRetry is_timeout_retry) {\n  // We don't retry if we already started the response, don't have a retry policy defined,\n  // or if we've already retried this upstream request (currently only possible if a per\n  // try timeout occurred and hedge_on_per_try_timeout is enabled).\n  if (downstream_response_started_ || !retry_state_ || upstream_request.retried()) {\n    return false;\n  }\n  RetryState::Http3Used was_using_http3 = RetryState::Http3Used::Unknown;\n  if (upstream_request.hadUpstream()) {\n    was_using_http3 = (upstream_request.streamInfo().protocol().has_value() &&\n                       upstream_request.streamInfo().protocol().value() == Http::Protocol::Http3)\n                          ? RetryState::Http3Used::Yes\n                          : RetryState::Http3Used::No;\n  }\n  const RetryStatus retry_status = retry_state_->shouldRetryReset(\n      reset_reason, was_using_http3,\n      [this, can_send_early_data = upstream_request.upstreamStreamOptions().can_send_early_data_,\n       can_use_http3 = upstream_request.upstreamStreamOptions().can_use_http3_,\n       is_timeout_retry](bool disable_http3) -> void {\n        // This retry might be because of ConnectionFailure of 0-RTT handshake. In this case, though\n        // the original request is retried with the same can_send_early_data setting, it will not be\n        // sent as early data by the underlying connection pool grid.\n        doRetry(can_send_early_data, disable_http3 ? false : can_use_http3, is_timeout_retry);\n      });\n  if (retry_status == RetryStatus::Yes) {\n    runRetryOptionsPredicates(upstream_request);\n    pending_retries_++;\n\n    if (upstream_request.upstreamHost()) {\n      upstream_request.upstreamHost()->stats().rq_error_.inc();\n    }\n\n    auto request_ptr = upstream_request.removeFromList(upstream_requests_);\n    callbacks_->dispatcher().deferredDelete(std::move(request_ptr));\n    return true;\n  } else if (retry_status == RetryStatus::NoOverflow) {\n    callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::UpstreamOverflow);\n  } else if (retry_status == RetryStatus::NoRetryLimitExceeded) {\n    callbacks_->streamInfo().setResponseFlag(\n        StreamInfo::CoreResponseFlag::UpstreamRetryLimitExceeded);\n  }\n\n  return false;\n}\n\nvoid Filter::onUpstreamReset(Http::StreamResetReason reset_reason,\n                             absl::string_view transport_failure_reason,\n                             UpstreamRequest& upstream_request) {\n  ENVOY_STREAM_LOG(debug, \"upstream reset: reset reason: {}, transport failure reason: {}\",\n                   *callbacks_, Http::Utility::resetReasonToString(reset_reason),\n                   transport_failure_reason);\n\n  const bool dropped = reset_reason == Http::StreamResetReason::Overflow;\n\n  // Ignore upstream reset caused by a resource overflow.\n  // Currently, circuit breakers can only produce this reset reason.\n  // It means that this reason is cluster-wise, not upstream-related.\n  // Therefore removing an upstream in the case of an overloaded cluster\n  // would make the situation even worse.\n  // https://github.com/envoyproxy/envoy/issues/25487\n  if (!dropped) {\n    // TODO: The reset may also come from upstream over the wire. In this case it should be\n    // treated as external origin error and distinguished from local origin error.\n    // This matters only when running OutlierDetection with split_external_local_origin_errors\n    // config param set to true.\n    updateOutlierDetection(Upstream::Outlier::Result::LocalOriginConnectFailed, upstream_request,\n                           absl::nullopt);\n  }\n\n  if (maybeRetryReset(reset_reason, upstream_request, TimeoutRetry::No)) {\n    return;\n  }\n\n  const Http::Code error_code = (reset_reason == Http::StreamResetReason::ProtocolError)\n                                    ? Http::Code::BadGateway\n                                    : Http::Code::ServiceUnavailable;\n  chargeUpstreamAbort(error_code, dropped, upstream_request);\n  auto request_ptr = upstream_request.removeFromList(upstream_requests_);\n  callbacks_->dispatcher().deferredDelete(std::move(request_ptr));\n\n  // If there are other in-flight requests that might see an upstream response,\n  // don't return anything downstream.\n  if (numRequestsAwaitingHeaders() > 0 || pending_retries_ > 0) {\n    return;\n  }\n\n  const StreamInfo::CoreResponseFlag response_flags = streamResetReasonToResponseFlag(reset_reason);\n\n  const std::string body =\n      absl::StrCat(\"upstream connect error or disconnect/reset before headers. \",\n                   (is_retry_ ? \"retried and the latest \" : \"\"),\n                   \"reset reason: \", Http::Utility::resetReasonToString(reset_reason),\n                   !transport_failure_reason.empty() ? \", transport failure reason: \" : \"\",\n                   transport_failure_reason);\n  const std::string& basic_details =\n      downstream_response_started_ ? StreamInfo::ResponseCodeDetails::get().LateUpstreamReset\n                                   : StreamInfo::ResponseCodeDetails::get().EarlyUpstreamReset;\n  const std::string details = StringUtil::replaceAllEmptySpace(absl::StrCat(\n      basic_details, \"{\", Http::Utility::resetReasonToString(reset_reason),\n      transport_failure_reason.empty() ? \"\" : absl::StrCat(\",\", transport_failure_reason), \"}\"));\n  onUpstreamAbort(error_code, response_flags, body, dropped, details);\n}\n\nvoid Filter::onUpstreamHostSelected(Upstream::HostDescriptionConstSharedPtr host,\n                                    bool pool_success) {\n  if (retry_state_ && host) {\n    retry_state_->onHostAttempted(host);\n  }\n\n  if (!pool_success) {\n    return;\n  }\n\n  if (request_vcluster_) {\n    // The cluster increases its upstream_rq_total_ counter right before firing this onPoolReady\n    // callback. Hence, the upstream request increases the virtual cluster's upstream_rq_total_ stat\n    // here.\n    request_vcluster_->stats().upstream_rq_total_.inc();\n  }\n  if (route_stats_context_.has_value()) {\n    // The cluster increases its upstream_rq_total_ counter right before firing this onPoolReady\n    // callback. Hence, the upstream request increases the route level upstream_rq_total_ stat\n    // here.\n    route_stats_context_->stats().upstream_rq_total_.inc();\n  }\n}\n\nStreamInfo::CoreResponseFlag\nFilter::streamResetReasonToResponseFlag(Http::StreamResetReason reset_reason) {\n  switch (reset_reason) {\n  case Http::StreamResetReason::LocalConnectionFailure:\n  case Http::StreamResetReason::RemoteConnectionFailure:\n  case Http::StreamResetReason::ConnectionTimeout:\n    return StreamInfo::CoreResponseFlag::UpstreamConnectionFailure;\n  case Http::StreamResetReason::ConnectionTermination:\n    return StreamInfo::CoreResponseFlag::UpstreamConnectionTermination;\n  case Http::StreamResetReason::LocalReset:\n  case Http::StreamResetReason::LocalRefusedStreamReset:\n    return StreamInfo::CoreResponseFlag::LocalReset;\n  case Http::StreamResetReason::Overflow:\n    return StreamInfo::CoreResponseFlag::UpstreamOverflow;\n  case Http::StreamResetReason::RemoteReset:\n  case Http::StreamResetReason::RemoteRefusedStreamReset:\n  case Http::StreamResetReason::ConnectError:\n    return StreamInfo::CoreResponseFlag::UpstreamRemoteReset;\n  case Http::StreamResetReason::ProtocolError:\n    return StreamInfo::CoreResponseFlag::UpstreamProtocolError;\n  case Http::StreamResetReason::OverloadManager:\n    return StreamInfo::CoreResponseFlag::OverloadManager;\n  }\n\n  PANIC_DUE_TO_CORRUPT_ENUM;\n}\n\nvoid Filter::handleNon5xxResponseHeaders(absl::optional<Grpc::Status::GrpcStatus> grpc_status,\n                                         UpstreamRequest& upstream_request, bool end_stream,\n                                         uint64_t grpc_to_http_status) {\n  // We need to defer gRPC success until after we have processed grpc-status in\n  // the trailers.\n  if (grpc_request_) {\n    if (end_stream) {\n      if (grpc_status && !Http::CodeUtility::is5xx(grpc_to_http_status)) {\n        upstream_request.upstreamHost()->stats().rq_success_.inc();\n      } else {\n        upstream_request.upstreamHost()->stats().rq_error_.inc();\n      }\n    } else {\n      upstream_request.grpcRqSuccessDeferred(true);\n    }\n  } else {\n    upstream_request.upstreamHost()->stats().rq_success_.inc();\n  }\n}\n\nvoid Filter::onUpstream1xxHeaders(Http::ResponseHeaderMapPtr&& headers,\n                                  UpstreamRequest& upstream_request) {\n  const uint64_t response_code = Http::Utility::getResponseStatus(*headers);\n  chargeUpstreamCode(response_code, *headers, upstream_request.upstreamHost(), false);\n  ENVOY_STREAM_LOG(debug, \"upstream 1xx ({}).\", *callbacks_, response_code);\n\n  downstream_response_started_ = true;\n  final_upstream_request_ = &upstream_request;\n  resetOtherUpstreams(upstream_request);\n\n  // Don't send retries after 100-Continue has been sent on. Arguably we could attempt to do a\n  // retry, assume the next upstream would also send an 100-Continue and swallow the second one\n  // but it's sketchy (as the subsequent upstream might not send a 100-Continue) and not worth\n  // the complexity until someone asks for it.\n  retry_state_.reset();\n\n  callbacks_->encode1xxHeaders(std::move(headers));\n}\n\nvoid Filter::resetAll() {\n  while (!upstream_requests_.empty()) {\n    auto request_ptr = upstream_requests_.back()->removeFromList(upstream_requests_);\n    request_ptr->resetStream();\n    callbacks_->dispatcher().deferredDelete(std::move(request_ptr));\n  }\n}\n\nvoid Filter::resetOtherUpstreams(UpstreamRequest& upstream_request) {\n  // Pop each upstream request on the list and reset it if it's not the one\n  // provided. At the end we'll move it back into the list.\n  UpstreamRequestPtr final_upstream_request;\n  while (!upstream_requests_.empty()) {\n    UpstreamRequestPtr upstream_request_tmp =\n        upstream_requests_.back()->removeFromList(upstream_requests_);\n    if (upstream_request_tmp.get() != &upstream_request) {\n      upstream_request_tmp->resetStream();\n      // TODO: per-host stat for hedge abandoned.\n      // TODO: cluster stat for hedge abandoned.\n    } else {\n      final_upstream_request = std::move(upstream_request_tmp);\n    }\n  }\n\n  ASSERT(final_upstream_request);\n  // Now put the final request back on this list.\n  LinkedList::moveIntoList(std::move(final_upstream_request), upstream_requests_);\n}\n\nvoid Filter::onUpstreamHeaders(uint64_t response_code, Http::ResponseHeaderMapPtr&& headers,\n                               UpstreamRequest& upstream_request, bool end_stream) {\n  ENVOY_STREAM_LOG(debug, \"upstream headers complete: end_stream={}\", *callbacks_, end_stream);\n\n  modify_headers_(*headers);\n  // When grpc-status appears in response headers, convert grpc-status to HTTP status code\n  // for outlier detection. This does not currently change any stats or logging and does not\n  // handle the case when an error grpc-status is sent as a trailer.\n  absl::optional<Grpc::Status::GrpcStatus> grpc_status;\n  uint64_t grpc_to_http_status = 0;\n  if (grpc_request_) {\n    grpc_status = Grpc::Common::getGrpcStatus(*headers);\n    if (grpc_status.has_value()) {\n      grpc_to_http_status = Grpc::Utility::grpcToHttpStatus(grpc_status.value());\n    }\n  }\n\n  if (grpc_status.has_value()) {\n    upstream_request.upstreamHost()->outlierDetector().putHttpResponseCode(grpc_to_http_status);\n  } else {\n    upstream_request.upstreamHost()->outlierDetector().putHttpResponseCode(response_code);\n  }\n\n  if (headers->EnvoyImmediateHealthCheckFail() != nullptr) {\n    upstream_request.upstreamHost()->healthChecker().setUnhealthy(\n        Upstream::HealthCheckHostMonitor::UnhealthyType::ImmediateHealthCheckFail);\n  }\n\n  bool could_not_retry = false;\n\n  // Check if this upstream request was already retried, for instance after\n  // hitting a per try timeout. Don't retry it if we already have.\n  if (retry_state_) {\n    if (upstream_request.retried()) {\n      // We already retried this request (presumably for a per try timeout) so\n      // we definitely won't retry it again. Check if we would have retried it\n      // if we could.\n      bool retry_as_early_data; // Not going to be used as we are not retrying.\n      could_not_retry = retry_state_->wouldRetryFromHeaders(*headers, *downstream_headers_,\n                                                            retry_as_early_data) !=\n                        RetryState::RetryDecision::NoRetry;\n    } else {\n      const RetryStatus retry_status = retry_state_->shouldRetryHeaders(\n          *headers, *downstream_headers_,\n          [this, can_use_http3 = upstream_request.upstreamStreamOptions().can_use_http3_,\n           had_early_data = upstream_request.upstreamStreamOptions().can_send_early_data_](\n              bool disable_early_data) -> void {\n            doRetry((disable_early_data ? false : had_early_data), can_use_http3, TimeoutRetry::No);\n          });\n      if (retry_status == RetryStatus::Yes) {\n        runRetryOptionsPredicates(upstream_request);\n        pending_retries_++;\n        upstream_request.upstreamHost()->stats().rq_error_.inc();\n        Http::CodeStats& code_stats = httpContext().codeStats();\n        code_stats.chargeBasicResponseStat(cluster_->statsScope(), stats_.stat_names_.retry_,\n                                           static_cast<Http::Code>(response_code),\n                                           exclude_http_code_stats_);\n\n        if (!end_stream || !upstream_request.encodeComplete()) {\n          upstream_request.resetStream();\n        }\n        auto request_ptr = upstream_request.removeFromList(upstream_requests_);\n        callbacks_->dispatcher().deferredDelete(std::move(request_ptr));\n        return;\n      } else if (retry_status == RetryStatus::NoOverflow) {\n        callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::UpstreamOverflow);\n        could_not_retry = true;\n      } else if (retry_status == RetryStatus::NoRetryLimitExceeded) {\n        callbacks_->streamInfo().setResponseFlag(\n            StreamInfo::CoreResponseFlag::UpstreamRetryLimitExceeded);\n        could_not_retry = true;\n      }\n    }\n  }\n\n  if (route_entry_->internalRedirectPolicy().enabled() &&\n      route_entry_->internalRedirectPolicy().shouldRedirectForResponseCode(\n          static_cast<Http::Code>(response_code)) &&\n      setupRedirect(*headers)) {\n    return;\n    // If the redirect could not be handled, fail open and let it pass to the\n    // next downstream.\n  }\n\n  // Check if we got a \"bad\" response, but there are still upstream requests in\n  // flight awaiting headers or scheduled retries. If so, exit to give them a\n  // chance to return before returning a response downstream.\n  if (could_not_retry && (numRequestsAwaitingHeaders() > 0 || pending_retries_ > 0)) {\n    upstream_request.upstreamHost()->stats().rq_error_.inc();\n\n    // Reset the stream because there are other in-flight requests that we'll\n    // wait around for and we're not interested in consuming any body/trailers.\n    auto request_ptr = upstream_request.removeFromList(upstream_requests_);\n    request_ptr->resetStream();\n    callbacks_->dispatcher().deferredDelete(std::move(request_ptr));\n    return;\n  }\n\n  // Make sure any retry timers are destroyed since we may not call cleanup() if end_stream is\n  // false.\n  if (retry_state_) {\n    retry_state_.reset();\n  }\n\n  // Only send upstream service time if we received the complete request and this is not a\n  // premature response.\n  if (DateUtil::timePointValid(downstream_request_complete_time_)) {\n    Event::Dispatcher& dispatcher = callbacks_->dispatcher();\n    MonotonicTime response_received_time = dispatcher.timeSource().monotonicTime();\n    std::chrono::milliseconds ms = std::chrono::duration_cast<std::chrono::milliseconds>(\n        response_received_time - downstream_request_complete_time_);\n    if (!config_.suppress_envoy_headers_) {\n      headers->setEnvoyUpstreamServiceTime(ms.count());\n    }\n  }\n\n  upstream_request.upstreamCanary(\n      (headers->EnvoyUpstreamCanary() && headers->EnvoyUpstreamCanary()->value() == \"true\") ||\n      upstream_request.upstreamHost()->canary());\n  chargeUpstreamCode(response_code, *headers, upstream_request.upstreamHost(), false);\n  if (!Http::CodeUtility::is5xx(response_code)) {\n    handleNon5xxResponseHeaders(grpc_status, upstream_request, end_stream, grpc_to_http_status);\n  }\n\n  // Append routing cookies\n  for (const auto& header_value : downstream_set_cookies_) {\n    headers->addReferenceKey(Http::Headers::get().SetCookie, header_value);\n  }\n\n  callbacks_->streamInfo().setResponseCodeDetails(\n      StreamInfo::ResponseCodeDetails::get().ViaUpstream);\n\n  if (Runtime::runtimeFeatureEnabled(\n          \"envoy.reloadable_features.copy_response_code_to_downstream_stream_info\")) {\n    callbacks_->streamInfo().setResponseCode(response_code);\n  }\n\n  // TODO(zuercher): If access to response_headers_to_add (at any level) is ever needed outside\n  // Router::Filter we'll need to find a better location for this work. One possibility is to\n  // provide finalizeResponseHeaders functions on the Router::Config and VirtualHost interfaces.\n  route_entry_->finalizeResponseHeaders(*headers, callbacks_->streamInfo());\n\n  downstream_response_started_ = true;\n  final_upstream_request_ = &upstream_request;\n  // Make sure that for request hedging, we end up with the correct final upstream info.\n  callbacks_->streamInfo().setUpstreamInfo(final_upstream_request_->streamInfo().upstreamInfo());\n  resetOtherUpstreams(upstream_request);\n  if (end_stream) {\n    onUpstreamComplete(upstream_request);\n  }\n\n  callbacks_->encodeHeaders(std::move(headers), end_stream,\n                            StreamInfo::ResponseCodeDetails::get().ViaUpstream);\n}\n\nvoid Filter::onUpstreamData(Buffer::Instance& data, UpstreamRequest& upstream_request,\n                            bool end_stream) {\n  // This should be true because when we saw headers we either reset the stream\n  // (hence wouldn't have made it to onUpstreamData) or all other in-flight\n  // streams.\n  ASSERT(upstream_requests_.size() == 1);\n  if (end_stream) {\n    // gRPC request termination without trailers is an error.\n    if (upstream_request.grpcRqSuccessDeferred()) {\n      upstream_request.upstreamHost()->stats().rq_error_.inc();\n    }\n    onUpstreamComplete(upstream_request);\n  }\n\n  callbacks_->encodeData(data, end_stream);\n}\n\nvoid Filter::onUpstreamTrailers(Http::ResponseTrailerMapPtr&& trailers,\n                                UpstreamRequest& upstream_request) {\n  // This should be true because when we saw headers we either reset the stream\n  // (hence wouldn't have made it to onUpstreamTrailers) or all other in-flight\n  // streams.\n  ASSERT(upstream_requests_.size() == 1);\n\n  if (upstream_request.grpcRqSuccessDeferred()) {\n    absl::optional<Grpc::Status::GrpcStatus> grpc_status = Grpc::Common::getGrpcStatus(*trailers);\n    if (grpc_status &&\n        !Http::CodeUtility::is5xx(Grpc::Utility::grpcToHttpStatus(grpc_status.value()))) {\n      upstream_request.upstreamHost()->stats().rq_success_.inc();\n    } else {\n      upstream_request.upstreamHost()->stats().rq_error_.inc();\n    }\n  }\n\n  onUpstreamComplete(upstream_request);\n\n  callbacks_->encodeTrailers(std::move(trailers));\n}\n\nvoid Filter::onUpstreamMetadata(Http::MetadataMapPtr&& metadata_map) {\n  callbacks_->encodeMetadata(std::move(metadata_map));\n}\n\nvoid Filter::onUpstreamComplete(UpstreamRequest& upstream_request) {\n  if (!downstream_end_stream_) {\n    upstream_request.resetStream();\n  }\n  Event::Dispatcher& dispatcher = callbacks_->dispatcher();\n  std::chrono::milliseconds response_time = std::chrono::duration_cast<std::chrono::milliseconds>(\n      dispatcher.timeSource().monotonicTime() - downstream_request_complete_time_);\n\n  Upstream::ClusterTimeoutBudgetStatsOptRef tb_stats = cluster()->timeoutBudgetStats();\n  if (tb_stats.has_value()) {\n    tb_stats->get().upstream_rq_timeout_budget_percent_used_.recordValue(\n        FilterUtility::percentageOfTimeout(response_time, timeout_.global_timeout_));\n  }\n\n  if (config_.emit_dynamic_stats_ && !callbacks_->streamInfo().healthCheck() &&\n      DateUtil::timePointValid(downstream_request_complete_time_)) {\n    upstream_request.upstreamHost()->outlierDetector().putResponseTime(response_time);\n    const bool internal_request = Http::HeaderUtility::isEnvoyInternalRequest(*downstream_headers_);\n\n    Http::CodeStats& code_stats = httpContext().codeStats();\n    Http::CodeStats::ResponseTimingInfo info{\n        config_.scope_,\n        cluster_->statsScope(),\n        config_.empty_stat_name_,\n        response_time,\n        upstream_request.upstreamCanary(),\n        internal_request,\n        route_entry_->virtualHost().statName(),\n        request_vcluster_ ? request_vcluster_->statName() : config_.empty_stat_name_,\n        route_stats_context_.has_value() ? route_stats_context_->statName()\n                                         : config_.empty_stat_name_,\n        config_.zone_name_,\n        upstreamZone(upstream_request.upstreamHost())};\n\n    code_stats.chargeResponseTiming(info);\n\n    if (alt_stat_prefix_ != nullptr) {\n      Http::CodeStats::ResponseTimingInfo info{config_.scope_,\n                                               cluster_->statsScope(),\n                                               alt_stat_prefix_->statName(),\n                                               response_time,\n                                               upstream_request.upstreamCanary(),\n                                               internal_request,\n                                               config_.empty_stat_name_,\n                                               config_.empty_stat_name_,\n                                               config_.empty_stat_name_,\n                                               config_.zone_name_,\n                                               upstreamZone(upstream_request.upstreamHost())};\n\n      code_stats.chargeResponseTiming(info);\n    }\n  }\n\n  // Defer deletion as this is generally called under the stack of the upstream\n  // request, and immediate deletion is dangerous.\n  callbacks_->dispatcher().deferredDelete(upstream_request.removeFromList(upstream_requests_));\n  cleanup();\n}\n\nbool Filter::setupRedirect(const Http::ResponseHeaderMap& headers) {\n  ENVOY_STREAM_LOG(debug, \"attempting internal redirect\", *callbacks_);\n  const Http::HeaderEntry* location = headers.Location();\n\n  const uint64_t status_code = Http::Utility::getResponseStatus(headers);\n\n  // Redirects are not supported for streaming requests yet.\n  if (downstream_end_stream_ && (!request_buffer_overflowed_ || !callbacks_->decodingBuffer()) &&\n      location != nullptr &&\n      convertRequestHeadersForInternalRedirect(*downstream_headers_, headers, *location,\n                                               status_code) &&\n      callbacks_->recreateStream(&headers)) {\n    ENVOY_STREAM_LOG(debug, \"Internal redirect succeeded\", *callbacks_);\n    cluster_->trafficStats()->upstream_internal_redirect_succeeded_total_.inc();\n    return true;\n  }\n  // convertRequestHeadersForInternalRedirect logs failure reasons but log\n  // details for other failure modes here.\n  if (!downstream_end_stream_) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: request incomplete\", *callbacks_);\n  } else if (request_buffer_overflowed_) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: request body overflow\", *callbacks_);\n  } else if (location == nullptr) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: missing location header\", *callbacks_);\n  }\n\n  cluster_->trafficStats()->upstream_internal_redirect_failed_total_.inc();\n  return false;\n}\n\nbool Filter::convertRequestHeadersForInternalRedirect(\n    Http::RequestHeaderMap& downstream_headers, const Http::ResponseHeaderMap& upstream_headers,\n    const Http::HeaderEntry& internal_redirect, uint64_t status_code) {\n  if (!downstream_headers.Path()) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: no path in downstream_headers\", *callbacks_);\n    return false;\n  }\n\n  absl::string_view redirect_url = internal_redirect.value().getStringView();\n  // Make sure the redirect response contains a URL to redirect to.\n  if (redirect_url.empty()) {\n    stats_.passthrough_internal_redirect_bad_location_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: empty location\", *callbacks_);\n    return false;\n  }\n  Http::Utility::Url absolute_url;\n  if (!absolute_url.initialize(redirect_url, false)) {\n    stats_.passthrough_internal_redirect_bad_location_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: invalid location {}\", *callbacks_,\n                     redirect_url);\n    return false;\n  }\n\n  const auto& policy = route_entry_->internalRedirectPolicy();\n  // Don't change the scheme from the original request\n  const bool scheme_is_http = schemeIsHttp(downstream_headers, callbacks_->connection());\n  const bool target_is_http = Http::Utility::schemeIsHttp(absolute_url.scheme());\n  if (!policy.isCrossSchemeRedirectAllowed() && scheme_is_http != target_is_http) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: incorrect scheme for {}\", *callbacks_,\n                     redirect_url);\n    stats_.passthrough_internal_redirect_unsafe_scheme_.inc();\n    return false;\n  }\n\n  const StreamInfo::FilterStateSharedPtr& filter_state = callbacks_->streamInfo().filterState();\n  // Make sure that performing the redirect won't result in exceeding the configured number of\n  // redirects allowed for this route.\n  StreamInfo::UInt32Accessor* num_internal_redirect{};\n\n  if (num_internal_redirect = filter_state->getDataMutable<StreamInfo::UInt32Accessor>(\n          NumInternalRedirectsFilterStateName);\n      num_internal_redirect == nullptr) {\n    auto state = std::make_shared<StreamInfo::UInt32AccessorImpl>(0);\n    num_internal_redirect = state.get();\n\n    filter_state->setData(NumInternalRedirectsFilterStateName, std::move(state),\n                          StreamInfo::FilterState::StateType::Mutable,\n                          StreamInfo::FilterState::LifeSpan::Request);\n  }\n\n  if (num_internal_redirect->value() >= policy.maxInternalRedirects()) {\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: redirect limits exceeded.\", *callbacks_);\n    stats_.passthrough_internal_redirect_too_many_redirects_.inc();\n    return false;\n  }\n  // Copy the old values, so they can be restored if the redirect fails.\n  const bool scheme_is_set = (downstream_headers.Scheme() != nullptr);\n\n  std::unique_ptr<Http::RequestHeaderMapImpl> saved_headers = Http::RequestHeaderMapImpl::create();\n  Http::RequestHeaderMapImpl::copyFrom(*saved_headers, downstream_headers);\n\n  for (const Http::LowerCaseString& header :\n       route_entry_->internalRedirectPolicy().responseHeadersToCopy()) {\n    Http::HeaderMap::GetResult result = upstream_headers.get(header);\n    Http::HeaderMap::GetResult downstream_result = downstream_headers.get(header);\n    if (result.empty()) {\n      // Clear headers if present, else do nothing:\n      if (downstream_result.empty()) {\n        continue;\n      }\n      downstream_headers.remove(header);\n    } else {\n      // The header exists in the response, copy into the downstream headers\n      if (!downstream_result.empty()) {\n        downstream_headers.remove(header);\n      }\n      for (size_t idx = 0; idx < result.size(); idx++) {\n        downstream_headers.addCopy(header, result[idx]->value().getStringView());\n      }\n    }\n  }\n\n  Cleanup restore_original_headers(\n      [&downstream_headers, scheme_is_set, scheme_is_http, &saved_headers]() {\n        downstream_headers.clear();\n        if (scheme_is_set) {\n          downstream_headers.setScheme(scheme_is_http ? Http::Headers::get().SchemeValues.Http\n                                                      : Http::Headers::get().SchemeValues.Https);\n        }\n\n        Http::RequestHeaderMapImpl::copyFrom(downstream_headers, *saved_headers);\n      });\n\n  // Replace the original host, scheme and path.\n  downstream_headers.setScheme(absolute_url.scheme());\n  downstream_headers.setHost(absolute_url.hostAndPort());\n\n  auto path_and_query = absolute_url.pathAndQueryParams();\n  if (Runtime::runtimeFeatureEnabled(\"envoy.reloadable_features.http_reject_path_with_fragment\")) {\n    // Envoy treats internal redirect as a new request and will reject it if URI path\n    // contains #fragment. However the Location header is allowed to have #fragment in URI path. To\n    // prevent Envoy from rejecting internal redirect, strip the #fragment from Location URI if it\n    // is present.\n    auto fragment_pos = path_and_query.find('#');\n    path_and_query = path_and_query.substr(0, fragment_pos);\n  }\n  downstream_headers.setPath(path_and_query);\n\n  // Only clear the route cache if there are downstream callbacks. There aren't, for example,\n  // for async connections.\n  if (callbacks_->downstreamCallbacks()) {\n    callbacks_->downstreamCallbacks()->clearRouteCache();\n  }\n  const auto route = callbacks_->route();\n  // Don't allow a redirect to a non existing route.\n  if (!route) {\n    stats_.passthrough_internal_redirect_no_route_.inc();\n    ENVOY_STREAM_LOG(trace, \"Internal redirect failed: no route found\", *callbacks_);\n    return false;\n  }\n\n  const auto& route_name = route->routeName();\n  for (const auto& predicate : policy.predicates()) {\n    if (!predicate->acceptTargetRoute(*filter_state, route_name, !scheme_is_http,\n                                      !target_is_http)) {\n      stats_.passthrough_internal_redirect_predicate_.inc();\n      ENVOY_STREAM_LOG(trace,\n                       \"Internal redirect failed: rejecting redirect targeting {}, by {} predicate\",\n                       *callbacks_, route_name, predicate->name());\n      return false;\n    }\n  }\n\n  // See https://tools.ietf.org/html/rfc7231#section-6.4.4.\n  if (status_code == enumToInt(Http::Code::SeeOther) &&\n      downstream_headers.getMethodValue() != Http::Headers::get().MethodValues.Get &&\n      downstream_headers.getMethodValue() != Http::Headers::get().MethodValues.Head) {\n    downstream_headers.setMethod(Http::Headers::get().MethodValues.Get);\n    downstream_headers.remove(Http::Headers::get().ContentLength);\n    callbacks_->modifyDecodingBuffer([](Buffer::Instance& data) { data.drain(data.length()); });\n  }\n\n  num_internal_redirect->increment();\n  restore_original_headers.cancel();\n  // Preserve the original request URL for the second pass.\n  downstream_headers.setEnvoyOriginalUrl(\n      absl::StrCat(scheme_is_http ? Http::Headers::get().SchemeValues.Http\n                                  : Http::Headers::get().SchemeValues.Https,\n                   \"://\", saved_headers->getHostValue(), saved_headers->getPathValue()));\n  return true;\n}\n\nvoid Filter::runRetryOptionsPredicates(UpstreamRequest& retriable_request) {\n  for (const auto& options_predicate : route_entry_->retryPolicy().retryOptionsPredicates()) {\n    const Upstream::RetryOptionsPredicate::UpdateOptionsParameters parameters{\n        retriable_request.streamInfo(), upstreamSocketOptions()};\n    auto ret = options_predicate->updateOptions(parameters);\n    if (ret.new_upstream_socket_options_.has_value()) {\n      upstream_options_ = ret.new_upstream_socket_options_.value();\n    }\n  }\n}\n\nvoid Filter::doRetry(bool can_send_early_data, bool can_use_http3, TimeoutRetry is_timeout_retry) {\n  ENVOY_STREAM_LOG(debug, \"performing retry\", *callbacks_);\n\n  is_retry_ = true;\n  attempt_count_++;\n  callbacks_->streamInfo().setAttemptCount(attempt_count_);\n  ASSERT(pending_retries_ > 0);\n  pending_retries_--;\n\n  // Clusters can technically get removed by CDS during a retry. Make sure it still exists.\n  const auto cluster = config_.cm_.getThreadLocalCluster(route_entry_->clusterName());\n  std::unique_ptr<GenericConnPool> generic_conn_pool;\n  if (cluster != nullptr) {\n    cluster_ = cluster->info();\n    generic_conn_pool = createConnPool(*cluster);\n  }\n\n  if (!generic_conn_pool) {\n    sendNoHealthyUpstreamResponse();\n    cleanup();\n    return;\n  }\n  UpstreamRequestPtr upstream_request = std::make_unique<UpstreamRequest>(\n      *this, std::move(generic_conn_pool), can_send_early_data, can_use_http3);\n\n  if (include_attempt_count_in_request_) {\n    downstream_headers_->setEnvoyAttemptCount(attempt_count_);\n  }\n\n  if (include_timeout_retry_header_in_request_) {\n    downstream_headers_->setEnvoyIsTimeoutRetry(is_timeout_retry == TimeoutRetry::Yes ? \"true\"\n                                                                                      : \"false\");\n  }\n\n  // The request timeouts only account for time elapsed since the downstream request completed\n  // which might not have happened yet (in which case zero time has elapsed.)\n  std::chrono::milliseconds elapsed_time = std::chrono::milliseconds::zero();\n\n  if (DateUtil::timePointValid(downstream_request_complete_time_)) {\n    Event::Dispatcher& dispatcher = callbacks_->dispatcher();\n    elapsed_time = std::chrono::duration_cast<std::chrono::milliseconds>(\n        dispatcher.timeSource().monotonicTime() - downstream_request_complete_time_);\n  }\n\n  FilterUtility::setTimeoutHeaders(elapsed_time.count(), timeout_, *route_entry_,\n                                   *downstream_headers_, !config_.suppress_envoy_headers_,\n                                   grpc_request_, hedging_params_.hedge_on_per_try_timeout_);\n\n  UpstreamRequest* upstream_request_tmp = upstream_request.get();\n  LinkedList::moveIntoList(std::move(upstream_request), upstream_requests_);\n  upstream_requests_.front()->acceptHeadersFromRouter(\n      !callbacks_->decodingBuffer() && !downstream_trailers_ && downstream_end_stream_);\n  // It's possible we got immediately reset which means the upstream request we just\n  // added to the front of the list might have been removed, so we need to check to make\n  // sure we don't send data on the wrong request.\n  if (!upstream_requests_.empty() && (upstream_requests_.front().get() == upstream_request_tmp)) {\n    if (callbacks_->decodingBuffer()) {\n      // If we are doing a retry we need to make a copy.\n      Buffer::OwnedImpl copy(*callbacks_->decodingBuffer());\n      upstream_requests_.front()->acceptDataFromRouter(copy, !downstream_trailers_ &&\n                                                                 downstream_end_stream_);\n    }\n\n    if (downstream_trailers_) {\n      upstream_requests_.front()->acceptTrailersFromRouter(*downstream_trailers_);\n    }\n  }\n}\n\nuint32_t Filter::numRequestsAwaitingHeaders() {\n  return std::count_if(upstream_requests_.begin(), upstream_requests_.end(),\n                       [](const auto& req) -> bool { return req->awaitingHeaders(); });\n}\n\nbool Filter::checkDropOverload(Upstream::ThreadLocalCluster& cluster,\n                               std::function<void(Http::ResponseHeaderMap&)>& modify_headers) {\n  if (cluster.dropOverload().value()) {\n    ENVOY_STREAM_LOG(debug, \"Router filter: cluster DROP_OVERLOAD configuration: {}\", *callbacks_,\n                     cluster.dropOverload().value());\n    if (config_.random_.bernoulli(cluster.dropOverload())) {\n      ENVOY_STREAM_LOG(debug, \"The request is dropped by DROP_OVERLOAD\", *callbacks_);\n      callbacks_->streamInfo().setResponseFlag(StreamInfo::CoreResponseFlag::DropOverLoad);\n      chargeUpstreamCode(Http::Code::ServiceUnavailable, nullptr, true);\n      callbacks_->sendLocalReply(\n          Http::Code::ServiceUnavailable, \"drop overload\",\n          [modify_headers, this](Http::ResponseHeaderMap& headers) {\n            if (!config_.suppress_envoy_headers_) {\n              headers.addReference(Http::Headers::get().EnvoyDropOverload,\n                                   Http::Headers::get().EnvoyDropOverloadValues.True);\n            }\n            modify_headers(headers);\n          },\n          absl::nullopt, StreamInfo::ResponseCodeDetails::get().DropOverload);\n\n      cluster.info()->loadReportStats().upstream_rq_drop_overload_.inc();\n      return true;\n    }\n  }\n  return false;\n}\n\nRetryStatePtr\nProdFilter::createRetryState(const RetryPolicy& policy, Http::RequestHeaderMap& request_headers,\n                             const Upstream::ClusterInfo& cluster, const VirtualCluster* vcluster,\n                             RouteStatsContextOptRef route_stats_context, Runtime::Loader& runtime,\n                             Random::RandomGenerator& random, Event::Dispatcher& dispatcher,\n                             TimeSource& time_source, Upstream::ResourcePriority priority) {\n  std::unique_ptr<RetryStateImpl> retry_state =\n      RetryStateImpl::create(policy, request_headers, cluster, vcluster, route_stats_context,\n                             runtime, random, dispatcher, time_source, priority);\n  if (retry_state != nullptr && retry_state->isAutomaticallyConfiguredForHttp3()) {\n    // Since doing retry will make Envoy to buffer the request body, if upstream using HTTP/3 is the\n    // only reason for doing retry, set the retry shadow buffer limit to 0 so that we don't retry or\n    // buffer safe requests with body which is not common.\n    setRetryShadowBufferLimit(0);\n  }\n  return retry_state;\n}\n\n} // namespace Router\n} // namespace Envoy\n", "#include \"source/common/router/upstream_request.h\"\n\n#include <chrono>\n#include <cstdint>\n#include <functional>\n#include <memory>\n#include <string>\n\n#include \"envoy/event/dispatcher.h\"\n#include \"envoy/event/timer.h\"\n#include \"envoy/grpc/status.h\"\n#include \"envoy/http/conn_pool.h\"\n#include \"envoy/http/header_map.h\"\n#include \"envoy/runtime/runtime.h\"\n#include \"envoy/upstream/cluster_manager.h\"\n#include \"envoy/upstream/upstream.h\"\n\n#include \"source/common/common/assert.h\"\n#include \"source/common/common/dump_state_utils.h\"\n#include \"source/common/common/empty_string.h\"\n#include \"source/common/common/enum_to_int.h\"\n#include \"source/common/common/scope_tracker.h\"\n#include \"source/common/common/utility.h\"\n#include \"source/common/grpc/common.h\"\n#include \"source/common/http/codes.h\"\n#include \"source/common/http/header_map_impl.h\"\n#include \"source/common/http/headers.h\"\n#include \"source/common/http/message_impl.h\"\n#include \"source/common/http/utility.h\"\n#include \"source/common/network/application_protocol.h\"\n#include \"source/common/network/transport_socket_options_impl.h\"\n#include \"source/common/network/upstream_server_name.h\"\n#include \"source/common/network/upstream_subject_alt_names.h\"\n#include \"source/common/router/config_impl.h\"\n#include \"source/common/router/debug_config.h\"\n#include \"source/common/router/router.h\"\n#include \"source/common/stream_info/uint32_accessor_impl.h\"\n#include \"source/common/tracing/http_tracer_impl.h\"\n#include \"source/extensions/common/proxy_protocol/proxy_protocol_header.h\"\n\nnamespace Envoy {\nnamespace Router {\n\n// The upstream HTTP filter manager class.\nclass UpstreamFilterManager : public Http::FilterManager {\npublic:\n  UpstreamFilterManager(Http::FilterManagerCallbacks& filter_manager_callbacks,\n                        Event::Dispatcher& dispatcher, OptRef<const Network::Connection> connection,\n                        uint64_t stream_id, Buffer::BufferMemoryAccountSharedPtr account,\n                        bool proxy_100_continue, uint32_t buffer_limit,\n                        const Http::FilterChainFactory& filter_chain_factory,\n                        UpstreamRequest& request)\n      : FilterManager(filter_manager_callbacks, dispatcher, connection, stream_id, account,\n                      proxy_100_continue, buffer_limit, filter_chain_factory),\n        upstream_request_(request) {}\n\n  StreamInfo::StreamInfo& streamInfo() override {\n    return upstream_request_.parent_.callbacks()->streamInfo();\n  }\n  const StreamInfo::StreamInfo& streamInfo() const override {\n    return upstream_request_.parent_.callbacks()->streamInfo();\n  }\n  // Send local replies via the downstream HTTP filter manager.\n  // Local replies will not be seen by upstream HTTP filters.\n  void sendLocalReply(Http::Code code, absl::string_view body,\n                      const std::function<void(Http::ResponseHeaderMap& headers)>& modify_headers,\n                      const absl::optional<Grpc::Status::GrpcStatus> grpc_status,\n                      absl::string_view details) override {\n    state().decoder_filter_chain_aborted_ = true;\n    state().encoder_filter_chain_aborted_ = true;\n    state().remote_encode_complete_ = true;\n    state().local_complete_ = true;\n    // TODO(alyssawilk) this should be done through the router to play well with hedging.\n    upstream_request_.parent_.callbacks()->sendLocalReply(code, body, modify_headers, grpc_status,\n                                                          details);\n  }\n  void executeLocalReplyIfPrepared() override {}\n  UpstreamRequest& upstream_request_;\n};\n\nUpstreamRequest::UpstreamRequest(RouterFilterInterface& parent,\n                                 std::unique_ptr<GenericConnPool>&& conn_pool,\n                                 bool can_send_early_data, bool can_use_http3)\n    : parent_(parent), conn_pool_(std::move(conn_pool)),\n      stream_info_(parent_.callbacks()->dispatcher().timeSource(), nullptr),\n      start_time_(parent_.callbacks()->dispatcher().timeSource().monotonicTime()),\n      calling_encode_headers_(false), upstream_canary_(false), router_sent_end_stream_(false),\n      encode_trailers_(false), retried_(false), awaiting_headers_(true),\n      outlier_detection_timeout_recorded_(false),\n      create_per_try_timeout_on_request_complete_(false), paused_for_connect_(false),\n      reset_stream_(false),\n      record_timeout_budget_(parent_.cluster()->timeoutBudgetStats().has_value()),\n      cleaned_up_(false), had_upstream_(false),\n      stream_options_({can_send_early_data, can_use_http3}), grpc_rq_success_deferred_(false),\n      upstream_wait_for_response_headers_before_disabling_read_(Runtime::runtimeFeatureEnabled(\n          \"envoy.reloadable_features.upstream_wait_for_response_headers_before_disabling_read\")) {\n  if (auto tracing_config = parent_.callbacks()->tracingConfig(); tracing_config.has_value()) {\n    if (tracing_config->spawnUpstreamSpan() || parent_.config().start_child_span_) {\n      span_ = parent_.callbacks()->activeSpan().spawnChild(\n          tracing_config.value().get(),\n          absl::StrCat(\"router \", parent.cluster()->observabilityName(), \" egress\"),\n          parent_.callbacks()->dispatcher().timeSource().systemTime());\n      if (parent.attemptCount() != 1) {\n        // This is a retry request, add this metadata to span.\n        span_->setTag(Tracing::Tags::get().RetryCount, std::to_string(parent.attemptCount() - 1));\n      }\n    }\n  }\n\n  // The router checks that the connection pool is non-null before creating the upstream request.\n  auto upstream_host = conn_pool_->host();\n  Tracing::HttpTraceContext trace_context(*parent_.downstreamHeaders());\n  if (span_ != nullptr) {\n    span_->injectContext(trace_context, upstream_host);\n  } else {\n    // No independent child span for current upstream request then inject the parent span's tracing\n    // context into the request headers.\n    // The injectContext() of the parent span may be called repeatedly when the request is retried.\n    parent_.callbacks()->activeSpan().injectContext(trace_context, upstream_host);\n  }\n\n  stream_info_.setUpstreamInfo(std::make_shared<StreamInfo::UpstreamInfoImpl>());\n  stream_info_.route_ = parent_.callbacks()->route();\n  parent_.callbacks()->streamInfo().setUpstreamInfo(stream_info_.upstreamInfo());\n\n  stream_info_.healthCheck(parent_.callbacks()->streamInfo().healthCheck());\n  stream_info_.setIsShadow(parent_.callbacks()->streamInfo().isShadow());\n  absl::optional<Upstream::ClusterInfoConstSharedPtr> cluster_info =\n      parent_.callbacks()->streamInfo().upstreamClusterInfo();\n  if (cluster_info.has_value()) {\n    stream_info_.setUpstreamClusterInfo(*cluster_info);\n  }\n\n  // Set up the upstream HTTP filter manager.\n  filter_manager_callbacks_ = std::make_unique<UpstreamRequestFilterManagerCallbacks>(*this);\n  filter_manager_ = std::make_unique<UpstreamFilterManager>(\n      *filter_manager_callbacks_, parent_.callbacks()->dispatcher(), connection(),\n      parent_.callbacks()->streamId(), parent_.callbacks()->account(), true,\n      parent_.callbacks()->decoderBufferLimit(), *parent_.cluster(), *this);\n  // Attempt to create custom cluster-specified filter chain\n  bool created = parent_.cluster()->createFilterChain(*filter_manager_,\n                                                      /*only_create_if_configured=*/true);\n  if (!created) {\n    // Attempt to create custom router-specified filter chain.\n    created = parent_.config().createFilterChain(*filter_manager_);\n  }\n  if (!created) {\n    // Neither cluster nor router have a custom filter chain; add the default\n    // cluster filter chain, which only consists of the codec filter.\n    created = parent_.cluster()->createFilterChain(*filter_manager_, false);\n  }\n  // There will always be a codec filter present, which sets the upstream\n  // interface. Fast-fail any tests that don't set up mocks correctly.\n  ASSERT(created && upstream_interface_.has_value());\n}\n\nUpstreamRequest::~UpstreamRequest() { cleanUp(); }\n\nvoid UpstreamRequest::cleanUp() {\n  if (cleaned_up_) {\n    return;\n  }\n  cleaned_up_ = true;\n\n  filter_manager_->destroyFilters();\n\n  if (span_ != nullptr) {\n    auto tracing_config = parent_.callbacks()->tracingConfig();\n    ASSERT(tracing_config.has_value());\n    Tracing::HttpTracerUtility::finalizeUpstreamSpan(*span_, stream_info_,\n                                                     tracing_config.value().get());\n  }\n\n  if (per_try_timeout_ != nullptr) {\n    // Allows for testing.\n    per_try_timeout_->disableTimer();\n  }\n\n  if (per_try_idle_timeout_ != nullptr) {\n    // Allows for testing.\n    per_try_idle_timeout_->disableTimer();\n  }\n\n  if (max_stream_duration_timer_ != nullptr) {\n    max_stream_duration_timer_->disableTimer();\n  }\n\n  if (upstream_log_flush_timer_ != nullptr) {\n    upstream_log_flush_timer_->disableTimer();\n  }\n\n  clearRequestEncoder();\n\n  // If desired, fire the per-try histogram when the UpstreamRequest\n  // completes.\n  if (record_timeout_budget_) {\n    Event::Dispatcher& dispatcher = parent_.callbacks()->dispatcher();\n    const MonotonicTime end_time = dispatcher.timeSource().monotonicTime();\n    const std::chrono::milliseconds response_time =\n        std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time_);\n    Upstream::ClusterTimeoutBudgetStatsOptRef tb_stats = parent_.cluster()->timeoutBudgetStats();\n    tb_stats->get().upstream_rq_timeout_budget_per_try_percent_used_.recordValue(\n        FilterUtility::percentageOfTimeout(response_time, parent_.timeout().per_try_timeout_));\n  }\n\n  // Ditto for request/response size histograms.\n  Upstream::ClusterRequestResponseSizeStatsOptRef req_resp_stats_opt =\n      parent_.cluster()->requestResponseSizeStats();\n  if (req_resp_stats_opt.has_value() && parent_.downstreamHeaders()) {\n    auto& req_resp_stats = req_resp_stats_opt->get();\n    req_resp_stats.upstream_rq_headers_size_.recordValue(parent_.downstreamHeaders()->byteSize());\n    req_resp_stats.upstream_rq_body_size_.recordValue(stream_info_.bytesSent());\n\n    if (response_headers_size_.has_value()) {\n      req_resp_stats.upstream_rs_headers_size_.recordValue(response_headers_size_.value());\n      req_resp_stats.upstream_rs_body_size_.recordValue(stream_info_.bytesReceived());\n    }\n  }\n\n  stream_info_.onRequestComplete();\n  upstreamLog(AccessLog::AccessLogType::UpstreamEnd);\n\n  while (downstream_data_disabled_ != 0) {\n    parent_.callbacks()->onDecoderFilterBelowWriteBufferLowWatermark();\n    parent_.cluster()->trafficStats()->upstream_flow_control_drained_total_.inc();\n    --downstream_data_disabled_;\n  }\n  // The upstream HTTP filter chain callbacks own headers/trailers while they are traversing the\n  // filter chain. Make sure to not delete them immediately when the stream ends, as the stream\n  // often ends during filter chain processing and it causes use-after-free violations.\n  parent_.callbacks()->dispatcher().deferredDelete(std::move(filter_manager_callbacks_));\n}\n\nvoid UpstreamRequest::upstreamLog(AccessLog::AccessLogType access_log_type) {\n  const Formatter::HttpFormatterContext log_context{parent_.downstreamHeaders(),\n                                                    upstream_headers_.get(),\n                                                    upstream_trailers_.get(),\n                                                    {},\n                                                    access_log_type};\n\n  for (const auto& upstream_log : parent_.config().upstream_logs_) {\n    upstream_log->log(log_context, stream_info_);\n  }\n}\n\n// This is called by the FilterManager when all filters have processed 1xx headers. Forward them\n// on to the router.\nvoid UpstreamRequest::decode1xxHeaders(Http::ResponseHeaderMapPtr&& headers) {\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n\n  ASSERT(Http::HeaderUtility::isSpecial1xx(*headers));\n  addResponseHeadersSize(headers->byteSize());\n  maybeHandleDeferredReadDisable();\n  parent_.onUpstream1xxHeaders(std::move(headers), *this);\n}\n\n// This is called by the FilterManager when all filters have processed headers. Forward them\n// on to the router.\nvoid UpstreamRequest::decodeHeaders(Http::ResponseHeaderMapPtr&& headers, bool end_stream) {\n  ASSERT(headers.get());\n  ENVOY_STREAM_LOG(trace, \"upstream response headers:\\n{}\", *parent_.callbacks(), *headers);\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n\n  resetPerTryIdleTimer();\n\n  addResponseHeadersSize(headers->byteSize());\n\n  // We drop unsupported 1xx on the floor here. 101 upgrade headers need to be passed to the client\n  // as part of the final response. Most 1xx headers are handled in onUpstream1xxHeaders.\n  //\n  // We could in principle handle other headers here, but this might result in the double invocation\n  // of decodeHeaders() (once for informational, again for non-informational), which is likely an\n  // easy to miss corner case in the filter and HCM contract.\n  //\n  // This filtering is done early in upstream request, unlike 100 coalescing which is performed in\n  // the router filter, since the filtering only depends on the state of a single upstream, and we\n  // don't want to confuse accounting such as onFirstUpstreamRxByteReceived() with informational\n  // headers.\n  const uint64_t response_code = Http::Utility::getResponseStatus(*headers);\n  if (Http::CodeUtility::is1xx(response_code) &&\n      response_code != enumToInt(Http::Code::SwitchingProtocols)) {\n    return;\n  }\n\n  awaiting_headers_ = false;\n  if (span_ != nullptr) {\n    Tracing::HttpTracerUtility::onUpstreamResponseHeaders(*span_, headers.get());\n  }\n  if (!parent_.config().upstream_logs_.empty()) {\n    upstream_headers_ = Http::createHeaderMap<Http::ResponseHeaderMapImpl>(*headers);\n  }\n  stream_info_.setResponseCode(static_cast<uint32_t>(response_code));\n\n  maybeHandleDeferredReadDisable();\n  ASSERT(headers.get());\n\n  parent_.onUpstreamHeaders(response_code, std::move(headers), *this, end_stream);\n}\n\nvoid UpstreamRequest::maybeHandleDeferredReadDisable() {\n  for (; deferred_read_disabling_count_ > 0; --deferred_read_disabling_count_) {\n    // If the deferred read disabling count hasn't been cancelled out by read\n    // enabling count so far, stop the upstream from reading the rest response.\n    // Because readDisable keeps track of how many time it is called with\n    // \"true\" or \"false\", here it has to be called with \"true\" the same number\n    // of times as it would be called with \"false\" in the future.\n    parent_.cluster()->trafficStats()->upstream_flow_control_paused_reading_total_.inc();\n    upstream_->readDisable(true);\n  }\n}\n\nvoid UpstreamRequest::decodeData(Buffer::Instance& data, bool end_stream) {\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n\n  resetPerTryIdleTimer();\n  stream_info_.addBytesReceived(data.length());\n  parent_.onUpstreamData(data, *this, end_stream);\n}\n\nvoid UpstreamRequest::decodeTrailers(Http::ResponseTrailerMapPtr&& trailers) {\n  ENVOY_STREAM_LOG(trace, \"upstream response trailers:\\n{}\", *parent_.callbacks(), *trailers);\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n\n  if (span_ != nullptr) {\n    Tracing::HttpTracerUtility::onUpstreamResponseTrailers(*span_, trailers.get());\n  }\n  if (!parent_.config().upstream_logs_.empty()) {\n    upstream_trailers_ = Http::createHeaderMap<Http::ResponseTrailerMapImpl>(*trailers);\n  }\n  parent_.onUpstreamTrailers(std::move(trailers), *this);\n}\n\nvoid UpstreamRequest::dumpState(std::ostream& os, int indent_level) const {\n  const char* spaces = spacesForLevel(indent_level);\n  os << spaces << \"UpstreamRequest \" << this << \"\\n\";\n  if (connection()) {\n    const auto addressProvider = connection()->connectionInfoProviderSharedPtr();\n    DUMP_DETAILS(addressProvider);\n  }\n  const Http::RequestHeaderMap* request_headers = parent_.downstreamHeaders();\n  DUMP_DETAILS(request_headers);\n  if (filter_manager_) {\n    filter_manager_->dumpState(os, indent_level);\n  }\n}\n\nconst Route& UpstreamRequest::route() const { return *parent_.callbacks()->route(); }\n\nOptRef<const Network::Connection> UpstreamRequest::connection() const {\n  return parent_.callbacks()->connection();\n}\n\nvoid UpstreamRequest::decodeMetadata(Http::MetadataMapPtr&& metadata_map) {\n  parent_.onUpstreamMetadata(std::move(metadata_map));\n}\n\nvoid UpstreamRequest::maybeEndDecode(bool end_stream) {\n  if (end_stream) {\n    upstreamTiming().onLastUpstreamRxByteReceived(parent_.callbacks()->dispatcher().timeSource());\n  }\n}\n\nvoid UpstreamRequest::onUpstreamHostSelected(Upstream::HostDescriptionConstSharedPtr host,\n                                             bool pool_success) {\n  StreamInfo::UpstreamInfo& upstream_info = *streamInfo().upstreamInfo();\n  upstream_info.setUpstreamHost(host);\n  upstream_host_ = host;\n  parent_.onUpstreamHostSelected(host, pool_success);\n}\n\nvoid UpstreamRequest::acceptHeadersFromRouter(bool end_stream) {\n  ASSERT(!router_sent_end_stream_);\n  router_sent_end_stream_ = end_stream;\n\n  // Make sure that when we are forwarding CONNECT payload we do not do so until\n  // the upstream has accepted the CONNECT request.\n  // This must be done before conn_pool->newStream, as onPoolReady un-pauses for CONNECT\n  // termination.\n  auto* headers = parent_.downstreamHeaders();\n  if (headers->getMethodValue() == Http::Headers::get().MethodValues.Connect) {\n    paused_for_connect_ = true;\n  }\n\n  // Kick off creation of the upstream connection immediately upon receiving headers.\n  // In future it may be possible for upstream HTTP filters to delay this, or influence connection\n  // creation but for now optimize for minimal latency and fetch the connection\n  // as soon as possible.\n  conn_pool_->newStream(this);\n\n  if (parent_.config().upstream_log_flush_interval_.has_value()) {\n    upstream_log_flush_timer_ = parent_.callbacks()->dispatcher().createTimer([this]() -> void {\n      // If the request is complete, we've already done the stream-end upstream log, and shouldn't\n      // do the periodic log.\n      if (!streamInfo().requestComplete().has_value()) {\n        upstreamLog(AccessLog::AccessLogType::UpstreamPeriodic);\n        resetUpstreamLogFlushTimer();\n      }\n      // Both downstream and upstream bytes meters may not be initialized when\n      // the timer goes off, e.g. if it takes longer than the interval for a\n      // connection to be initialized; check for nullptr.\n      auto& downstream_bytes_meter = stream_info_.getDownstreamBytesMeter();\n      auto& upstream_bytes_meter = stream_info_.getUpstreamBytesMeter();\n      const SystemTime now = parent_.callbacks()->dispatcher().timeSource().systemTime();\n      if (downstream_bytes_meter) {\n        downstream_bytes_meter->takeUpstreamPeriodicLoggingSnapshot(now);\n      }\n      if (upstream_bytes_meter) {\n        upstream_bytes_meter->takeUpstreamPeriodicLoggingSnapshot(now);\n      }\n    });\n\n    resetUpstreamLogFlushTimer();\n  }\n\n  filter_manager_->requestHeadersInitialized();\n  filter_manager_->streamInfo().setRequestHeaders(*parent_.downstreamHeaders());\n  filter_manager_->decodeHeaders(*parent_.downstreamHeaders(), end_stream);\n}\n\nvoid UpstreamRequest::acceptDataFromRouter(Buffer::Instance& data, bool end_stream) {\n  ASSERT(!router_sent_end_stream_);\n  router_sent_end_stream_ = end_stream;\n\n  filter_manager_->decodeData(data, end_stream);\n}\n\nvoid UpstreamRequest::acceptTrailersFromRouter(Http::RequestTrailerMap& trailers) {\n  ASSERT(!router_sent_end_stream_);\n  router_sent_end_stream_ = true;\n  encode_trailers_ = true;\n\n  filter_manager_->decodeTrailers(trailers);\n}\n\nvoid UpstreamRequest::acceptMetadataFromRouter(Http::MetadataMapPtr&& metadata_map_ptr) {\n  filter_manager_->decodeMetadata(*metadata_map_ptr);\n}\n\nvoid UpstreamRequest::onResetStream(Http::StreamResetReason reason,\n                                    absl::string_view transport_failure_reason) {\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n\n  if (span_ != nullptr) {\n    // Add tags about reset.\n    span_->setTag(Tracing::Tags::get().Error, Tracing::Tags::get().True);\n    span_->setTag(Tracing::Tags::get().ErrorReason, Http::Utility::resetReasonToString(reason));\n  }\n  clearRequestEncoder();\n  awaiting_headers_ = false;\n  if (!calling_encode_headers_) {\n    stream_info_.setResponseFlag(Filter::streamResetReasonToResponseFlag(reason));\n    parent_.onUpstreamReset(reason, transport_failure_reason, *this);\n  } else {\n    deferred_reset_reason_ = reason;\n  }\n}\n\nvoid UpstreamRequest::resetStream() {\n  if (conn_pool_->cancelAnyPendingStream()) {\n    ENVOY_STREAM_LOG(debug, \"canceled pool request\", *parent_.callbacks());\n    ASSERT(!upstream_);\n  }\n\n  // Don't reset the stream if we're already done with it.\n  if (upstreamTiming().last_upstream_tx_byte_sent_.has_value() &&\n      upstreamTiming().last_upstream_rx_byte_received_.has_value()) {\n    return;\n  }\n\n  if (span_ != nullptr) {\n    // Add tags about the cancellation.\n    span_->setTag(Tracing::Tags::get().Canceled, Tracing::Tags::get().True);\n  }\n\n  if (upstream_) {\n    ENVOY_STREAM_LOG(debug, \"resetting pool request\", *parent_.callbacks());\n    upstream_->resetStream();\n    clearRequestEncoder();\n  }\n  reset_stream_ = true;\n}\n\nvoid UpstreamRequest::resetPerTryIdleTimer() {\n  if (per_try_idle_timeout_ != nullptr) {\n    per_try_idle_timeout_->enableTimer(parent_.timeout().per_try_idle_timeout_);\n  }\n}\n\nvoid UpstreamRequest::resetUpstreamLogFlushTimer() {\n  if (upstream_log_flush_timer_ != nullptr) {\n    upstream_log_flush_timer_->enableTimer(parent_.config().upstream_log_flush_interval_.value());\n  }\n}\n\nvoid UpstreamRequest::setupPerTryTimeout() {\n  ASSERT(!per_try_timeout_);\n  if (parent_.timeout().per_try_timeout_.count() > 0) {\n    per_try_timeout_ =\n        parent_.callbacks()->dispatcher().createTimer([this]() -> void { onPerTryTimeout(); });\n    per_try_timeout_->enableTimer(parent_.timeout().per_try_timeout_);\n  }\n\n  ASSERT(!per_try_idle_timeout_);\n  if (parent_.timeout().per_try_idle_timeout_.count() > 0) {\n    per_try_idle_timeout_ =\n        parent_.callbacks()->dispatcher().createTimer([this]() -> void { onPerTryIdleTimeout(); });\n    resetPerTryIdleTimer();\n  }\n}\n\nvoid UpstreamRequest::onPerTryIdleTimeout() {\n  ENVOY_STREAM_LOG(debug, \"upstream per try idle timeout\", *parent_.callbacks());\n  if (per_try_timeout_) {\n    // Disable the per try idle timer, so it does not trigger further retries\n    per_try_timeout_->disableTimer();\n  }\n  stream_info_.setResponseFlag(StreamInfo::CoreResponseFlag::StreamIdleTimeout);\n  parent_.onPerTryIdleTimeout(*this);\n}\n\nvoid UpstreamRequest::onPerTryTimeout() {\n  if (per_try_idle_timeout_) {\n    // Delete the per try idle timer, so it does not trigger further retries.\n    // The timer has to be deleted to prevent data flow from re-arming it.\n    per_try_idle_timeout_.reset();\n  }\n  // If we've sent anything downstream, ignore the per try timeout and let the response continue\n  // up to the global timeout\n  if (!parent_.downstreamResponseStarted()) {\n    ENVOY_STREAM_LOG(debug, \"upstream per try timeout\", *parent_.callbacks());\n\n    stream_info_.setResponseFlag(StreamInfo::CoreResponseFlag::UpstreamRequestTimeout);\n    parent_.onPerTryTimeout(*this);\n  } else {\n    ENVOY_STREAM_LOG(debug,\n                     \"ignored upstream per try timeout due to already started downstream response\",\n                     *parent_.callbacks());\n  }\n}\n\nvoid UpstreamRequest::recordConnectionPoolCallbackLatency() {\n  upstreamTiming().recordConnectionPoolCallbackLatency(\n      start_time_, parent_.callbacks()->dispatcher().timeSource());\n}\n\nvoid UpstreamRequest::onPoolFailure(ConnectionPool::PoolFailureReason reason,\n                                    absl::string_view transport_failure_reason,\n                                    Upstream::HostDescriptionConstSharedPtr host) {\n  recordConnectionPoolCallbackLatency();\n  Http::StreamResetReason reset_reason = [](ConnectionPool::PoolFailureReason reason) {\n    switch (reason) {\n    case ConnectionPool::PoolFailureReason::Overflow:\n      return Http::StreamResetReason::Overflow;\n    case ConnectionPool::PoolFailureReason::RemoteConnectionFailure:\n      return Http::StreamResetReason::RemoteConnectionFailure;\n    case ConnectionPool::PoolFailureReason::LocalConnectionFailure:\n      return Http::StreamResetReason::LocalConnectionFailure;\n    case ConnectionPool::PoolFailureReason::Timeout:\n      return Http::StreamResetReason::ConnectionTimeout;\n    }\n    PANIC_DUE_TO_CORRUPT_ENUM;\n  }(reason);\n\n  stream_info_.upstreamInfo()->setUpstreamTransportFailureReason(transport_failure_reason);\n\n  // Mimic an upstream reset.\n  onUpstreamHostSelected(host, false);\n  onResetStream(reset_reason, transport_failure_reason);\n}\n\nvoid UpstreamRequest::onPoolReady(std::unique_ptr<GenericUpstream>&& upstream,\n                                  Upstream::HostDescriptionConstSharedPtr host,\n                                  const Network::ConnectionInfoProvider& address_provider,\n                                  StreamInfo::StreamInfo& info,\n                                  absl::optional<Http::Protocol> protocol) {\n  // This may be called under an existing ScopeTrackerScopeState but it will unwind correctly.\n  ScopeTrackerScopeState scope(&parent_.callbacks()->scope(), parent_.callbacks()->dispatcher());\n  ENVOY_STREAM_LOG(debug, \"pool ready\", *parent_.callbacks());\n  recordConnectionPoolCallbackLatency();\n  upstream_ = std::move(upstream);\n  had_upstream_ = true;\n  // Have the upstream use the account of the downstream.\n  upstream_->setAccount(parent_.callbacks()->account());\n\n  host->outlierDetector().putResult(Upstream::Outlier::Result::LocalOriginConnectSuccess);\n\n  onUpstreamHostSelected(host, true);\n\n  if (protocol) {\n    stream_info_.protocol(protocol.value());\n  } else {\n    // We only pause for CONNECT for HTTP upstreams. If this is a TCP upstream, unpause.\n    paused_for_connect_ = false;\n  }\n\n  StreamInfo::UpstreamInfo& upstream_info = *stream_info_.upstreamInfo();\n  if (info.upstreamInfo()) {\n    auto& upstream_timing = info.upstreamInfo()->upstreamTiming();\n    upstreamTiming().upstream_connect_start_ = upstream_timing.upstream_connect_start_;\n    upstreamTiming().upstream_connect_complete_ = upstream_timing.upstream_connect_complete_;\n    upstreamTiming().upstream_handshake_complete_ = upstream_timing.upstream_handshake_complete_;\n    upstream_info.setUpstreamNumStreams(info.upstreamInfo()->upstreamNumStreams());\n  }\n\n  // Upstream HTTP filters might have already created/set a filter state.\n  const StreamInfo::FilterStateSharedPtr& filter_state = info.filterState();\n  if (!filter_state) {\n    upstream_info.setUpstreamFilterState(\n        std::make_shared<StreamInfo::FilterStateImpl>(StreamInfo::FilterState::LifeSpan::Request));\n  } else {\n    upstream_info.setUpstreamFilterState(filter_state);\n  }\n  upstream_info.setUpstreamLocalAddress(address_provider.localAddress());\n  upstream_info.setUpstreamRemoteAddress(address_provider.remoteAddress());\n  upstream_info.setUpstreamSslConnection(info.downstreamAddressProvider().sslConnection());\n\n  if (info.downstreamAddressProvider().connectionID().has_value()) {\n    upstream_info.setUpstreamConnectionId(info.downstreamAddressProvider().connectionID().value());\n  }\n\n  if (info.downstreamAddressProvider().interfaceName().has_value()) {\n    upstream_info.setUpstreamInterfaceName(\n        info.downstreamAddressProvider().interfaceName().value());\n  }\n\n  stream_info_.setUpstreamBytesMeter(upstream_->bytesMeter());\n  StreamInfo::StreamInfo::syncUpstreamAndDownstreamBytesMeter(parent_.callbacks()->streamInfo(),\n                                                              stream_info_);\n  if (protocol) {\n    upstream_info.setUpstreamProtocol(protocol.value());\n  }\n\n  if (parent_.downstreamEndStream()) {\n    setupPerTryTimeout();\n  } else {\n    create_per_try_timeout_on_request_complete_ = true;\n  }\n\n  // Make sure the connection manager will inform the downstream watermark manager when the\n  // downstream buffers are overrun. This may result in immediate watermark callbacks referencing\n  // the encoder.\n  parent_.callbacks()->addDownstreamWatermarkCallbacks(downstream_watermark_manager_);\n\n  absl::optional<std::chrono::milliseconds> max_stream_duration;\n  if (parent_.dynamicMaxStreamDuration().has_value()) {\n    max_stream_duration = parent_.dynamicMaxStreamDuration().value();\n  } else if (upstream_host_->cluster().commonHttpProtocolOptions().has_max_stream_duration()) {\n    max_stream_duration = std::chrono::milliseconds(DurationUtil::durationToMilliseconds(\n        upstream_host_->cluster().commonHttpProtocolOptions().max_stream_duration()));\n  }\n  if (max_stream_duration.has_value() && max_stream_duration->count()) {\n    max_stream_duration_timer_ = parent_.callbacks()->dispatcher().createTimer(\n        [this]() -> void { onStreamMaxDurationReached(); });\n    max_stream_duration_timer_->enableTimer(*max_stream_duration);\n  }\n\n  const auto* route_entry = route().routeEntry();\n  if (route_entry->autoHostRewrite() && !host->hostname().empty()) {\n    Http::Utility::updateAuthority(*parent_.downstreamHeaders(), host->hostname(),\n                                   route_entry->appendXfh());\n  }\n\n  stream_info_.setRequestHeaders(*parent_.downstreamHeaders());\n\n  if (parent_.config().flush_upstream_log_on_upstream_stream_) {\n    upstreamLog(AccessLog::AccessLogType::UpstreamPoolReady);\n  }\n\n  if (address_provider.connectionID() && stream_info_.downstreamAddressProvider().connectionID()) {\n    ENVOY_LOG(debug, \"Attached upstream connection [C{}] to downstream connection [C{}]\",\n              address_provider.connectionID().value(),\n              stream_info_.downstreamAddressProvider().connectionID().value());\n  }\n\n  for (auto* callback : upstream_callbacks_) {\n    callback->onUpstreamConnectionEstablished();\n  }\n}\n\nUpstreamToDownstream& UpstreamRequest::upstreamToDownstream() { return *upstream_interface_; }\n\nvoid UpstreamRequest::onStreamMaxDurationReached() {\n  upstream_host_->cluster().trafficStats()->upstream_rq_max_duration_reached_.inc();\n\n  // The upstream had closed then try to retry along with retry policy.\n  parent_.onStreamMaxDurationReached(*this);\n}\n\nvoid UpstreamRequest::clearRequestEncoder() {\n  // Before clearing the encoder, unsubscribe from callbacks.\n  if (upstream_) {\n    parent_.callbacks()->removeDownstreamWatermarkCallbacks(downstream_watermark_manager_);\n  }\n  upstream_.reset();\n}\n\nvoid UpstreamRequest::readDisableOrDefer(bool disable) {\n  if (!upstream_wait_for_response_headers_before_disabling_read_) {\n    if (disable) {\n      parent_.cluster()->trafficStats()->upstream_flow_control_paused_reading_total_.inc();\n      upstream_->readDisable(true);\n    } else {\n      parent_.cluster()->trafficStats()->upstream_flow_control_resumed_reading_total_.inc();\n      upstream_->readDisable(false);\n    }\n    return;\n  }\n\n  if (disable) {\n    // See comments on deferred_read_disabling_count_ for when we do and don't defer.\n    if (parent_.downstreamResponseStarted()) {\n      // The downstream connection is overrun. Pause reads from upstream.\n      // If there are multiple calls to readDisable either the codec (H2) or the\n      // underlying Network::Connection (H1) will handle reference counting.\n      parent_.cluster()->trafficStats()->upstream_flow_control_paused_reading_total_.inc();\n      upstream_->readDisable(disable);\n    } else {\n      ++deferred_read_disabling_count_;\n    }\n    return;\n  }\n\n  // One source of connection blockage has buffer available.\n  if (deferred_read_disabling_count_ > 0) {\n    ASSERT(!parent_.downstreamResponseStarted());\n    // Cancel out an existing deferred read disabling.\n    --deferred_read_disabling_count_;\n    return;\n  }\n  ASSERT(parent_.downstreamResponseStarted());\n  // Pass this on to the stream, which\n  // will resume reads if this was the last remaining high watermark.\n  parent_.cluster()->trafficStats()->upstream_flow_control_resumed_reading_total_.inc();\n  upstream_->readDisable(disable);\n}\n\nvoid UpstreamRequest::DownstreamWatermarkManager::onAboveWriteBufferHighWatermark() {\n  ASSERT(parent_.upstream_);\n  parent_.readDisableOrDefer(true);\n}\n\nvoid UpstreamRequest::DownstreamWatermarkManager::onBelowWriteBufferLowWatermark() {\n  ASSERT(parent_.upstream_);\n  parent_.readDisableOrDefer(false);\n}\n\nvoid UpstreamRequest::disableDataFromDownstreamForFlowControl() {\n  parent_.cluster()->trafficStats()->upstream_flow_control_backed_up_total_.inc();\n  parent_.callbacks()->onDecoderFilterAboveWriteBufferHighWatermark();\n  ++downstream_data_disabled_;\n}\n\nvoid UpstreamRequest::enableDataFromDownstreamForFlowControl() {\n  parent_.cluster()->trafficStats()->upstream_flow_control_drained_total_.inc();\n  parent_.callbacks()->onDecoderFilterBelowWriteBufferLowWatermark();\n  ASSERT(downstream_data_disabled_ != 0);\n  if (downstream_data_disabled_ > 0) {\n    --downstream_data_disabled_;\n  }\n}\n\nHttp::RequestHeaderMapOptRef UpstreamRequestFilterManagerCallbacks::requestHeaders() {\n  return {*upstream_request_.parent_.downstreamHeaders()};\n}\n\nHttp::RequestTrailerMapOptRef UpstreamRequestFilterManagerCallbacks::requestTrailers() {\n  if (upstream_request_.parent_.downstreamTrailers()) {\n    return {*upstream_request_.parent_.downstreamTrailers()};\n  }\n  if (trailers_) {\n    return {*trailers_};\n  }\n  return {};\n}\n\nconst ScopeTrackedObject& UpstreamRequestFilterManagerCallbacks::scope() {\n  return upstream_request_.parent_.callbacks()->scope();\n}\n\nOptRef<const Tracing::Config> UpstreamRequestFilterManagerCallbacks::tracingConfig() const {\n  return upstream_request_.parent_.callbacks()->tracingConfig();\n}\n\nTracing::Span& UpstreamRequestFilterManagerCallbacks::activeSpan() {\n  return upstream_request_.parent_.callbacks()->activeSpan();\n}\n\nvoid UpstreamRequestFilterManagerCallbacks::resetStream(\n    Http::StreamResetReason reset_reason, absl::string_view transport_failure_reason) {\n  // The filter manager needs to disambiguate between a filter-driven reset,\n  // which should force reset the stream, and a codec driven reset, which should\n  // tell the router the stream reset, and let the router make the decision to\n  // send a local reply, or retry the stream.\n  if (reset_reason == Http::StreamResetReason::LocalReset &&\n      transport_failure_reason != \"codec_error\") {\n    upstream_request_.parent_.callbacks()->resetStream();\n    return;\n  }\n  return upstream_request_.onResetStream(reset_reason, transport_failure_reason);\n}\n\nUpstream::ClusterInfoConstSharedPtr UpstreamRequestFilterManagerCallbacks::clusterInfo() {\n  return upstream_request_.parent_.callbacks()->clusterInfo();\n}\n\nHttp::Http1StreamEncoderOptionsOptRef\nUpstreamRequestFilterManagerCallbacks::http1StreamEncoderOptions() {\n  return upstream_request_.parent_.callbacks()->http1StreamEncoderOptions();\n}\n\n} // namespace Router\n} // namespace Envoy\n", "#include \"test/integration/http_timeout_integration_test.h\"\n\n#include \"test/test_common/test_runtime.h\"\n\n#include \"gtest/gtest.h\"\n\nnamespace Envoy {\n\nusing testing::HasSubstr;\n\nINSTANTIATE_TEST_SUITE_P(IpVersions, HttpTimeoutIntegrationTest,\n                         testing::ValuesIn(TestEnvironment::getIpVersionsForTest()),\n                         TestUtility::ipTestParamsToString);\n\n// Sends a request with a global timeout specified, sleeps for longer than the\n// timeout, and ensures that a timeout is received.\nTEST_P(HttpTimeoutIntegrationTest, GlobalTimeout) {\n  config_helper_.addConfigModifier(configureProxyStatus());\n  initialize();\n\n  TestScopedRuntime scoped_runtime;\n  scoped_runtime.mergeValues(\n      {{\"envoy.reloadable_features.proxy_status_upstream_request_timeout\", \"true\"}});\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger global timeout.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(501));\n\n  // Ensure we got a timeout downstream and canceled the upstream request.\n  ASSERT_TRUE(response->waitForEndStream());\n  ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"504\", response->headers().getStatusValue());\n  EXPECT_EQ(response->headers().getProxyStatusValue(),\n            \"envoy; error=http_response_timeout; details=\\\"response_timeout; UT\\\"\");\n}\n\n// Testing that `x-envoy-expected-timeout-ms` header, set by egress envoy, is respected by ingress\n// envoy when `respect_expected_rq_timeout` field is enabled. Sends a request with a global timeout\n// specified, sleeps for longer than the timeout, and ensures that a timeout is received.\nTEST_P(HttpTimeoutIntegrationTest, UseTimeoutSetByEgressEnvoy) {\n  enableRespectExpectedRqTimeout(true);\n  initialize();\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"},\n                                     {\"x-envoy-expected-rq-timeout-ms\", \"300\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger global timeout, populated from `x-envoy-expected-rq-timeout-ms` header.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(301));\n\n  // Ensure we got a timeout downstream and canceled the upstream request.\n  ASSERT_TRUE(response->waitForEndStream());\n  ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"504\", response->headers().getStatusValue());\n}\n\n// Testing that ingress envoy derives new timeout value and sets `x-envoy-expected-timeout-ms`\n// header, when timeout has not been set by egress envoy and `respect_expected_rq_timeout` field is\n// enabled. Sends a request with a global timeout specified, sleeps for longer than the timeout, and\n// ensures that a timeout is received.\nTEST_P(HttpTimeoutIntegrationTest, DeriveTimeoutInIngressEnvoy) {\n  enableRespectExpectedRqTimeout(true);\n  initialize();\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger global timeout, populated from `x-envoy-expected-rq-timeout-ms` header.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(501));\n\n  // Ensure we got a timeout downstream and canceled the upstream request.\n  ASSERT_TRUE(response->waitForEndStream());\n  ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"504\", response->headers().getStatusValue());\n}\n\n// Testing that `x-envoy-expected-timeout-ms` header, set by egress envoy, is ignored by ingress\n// envoy and new value is derived. Sends a request with a global timeout specified,\n// sleeps for longer than the timeout, and ensures that a timeout is received.\nTEST_P(HttpTimeoutIntegrationTest, IgnoreTimeoutSetByEgressEnvoy) {\n  enableRespectExpectedRqTimeout(false);\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"},\n                                     {\"x-envoy-expected-rq-timeout-ms\", \"600\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger global timeout, populated from `x-envoy-expected-rq-timeout-ms` header.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(501));\n\n  // Ensure we got a timeout downstream and canceled the upstream request.\n  ASSERT_TRUE(response->waitForEndStream());\n  ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"504\", response->headers().getStatusValue());\n}\n\n// Regression test for https://github.com/envoyproxy/envoy/issues/7154 in which\n// resetStream() was only called after a response timeout for upstream requests\n// that had not received headers yet. This meant that decodeData might be\n// called on a destroyed UpstreamRequest.\nTEST_P(HttpTimeoutIntegrationTest, GlobalTimeoutAfterHeadersBeforeBodyResetsUpstream) {\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  Http::TestRequestHeaderMapImpl request_headers{{\":method\", \"POST\"},\n                                                 {\":path\", \"/test/long/url\"},\n                                                 {\":scheme\", \"http\"},\n                                                 {\":authority\", \"host\"},\n                                                 {\"x-forwarded-for\", \"10.0.0.1\"},\n                                                 {\"x-envoy-upstream-rq-timeout-ms\", \"100\"}};\n  auto encoder_decoder = codec_client_->startRequest(request_headers);\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n\n  codec_client_->sendData(*request_encoder_, 100, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Respond with headers, not end of stream.\n  Http::TestResponseHeaderMapImpl response_headers{{\":status\", \"200\"}};\n  upstream_request_->encodeHeaders(response_headers, false);\n\n  response->waitForHeaders();\n  EXPECT_EQ(\"200\", response->headers().getStatusValue());\n\n  // Trigger global timeout.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(200));\n\n  ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n\n  ASSERT_TRUE(response->waitForReset());\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n}\n\n// Sends a request with a global timeout and per try timeout specified, sleeps\n// for longer than the per try but slightly less than the global timeout.\n// Ensures that two requests are attempted and a timeout is returned\n// downstream.\nTEST_P(HttpTimeoutIntegrationTest, PerTryTimeout) {\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-retry-on\", \"5xx\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"},\n                                     {\"x-envoy-upstream-rq-per-try-timeout-ms\", \"400\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger per try timeout (but not global timeout) and wait for reset.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(400));\n  ASSERT_TRUE(upstream_request_->waitForReset());\n\n  // Wait for a second request to be sent upstream. Max retry backoff is 25ms so advance time that\n  // much.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(25));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger global timeout.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(100));\n  ASSERT_TRUE(response->waitForEndStream());\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"504\", response->headers().getStatusValue());\n}\n\n// Sends a request with a per try timeout specified but no global timeout.\n// Ensures that two requests are attempted and a timeout is returned\n// downstream.\nTEST_P(HttpTimeoutIntegrationTest, PerTryTimeoutWithoutGlobalTimeout) {\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-retry-on\", \"5xx\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"0\"},\n                                     {\"x-envoy-upstream-rq-per-try-timeout-ms\", \"50\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger per try timeout (but not global timeout) and wait for reset.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(50));\n  ASSERT_TRUE(upstream_request_->waitForReset());\n\n  // Wait for a second request to be sent upstream. Max retry backoff is 25ms so advance time that\n  // much. This is always less than the next request's per try timeout.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(25));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Encode 200 response headers for the first (timed out) request.\n  Http::TestResponseHeaderMapImpl response_headers{{\":status\", \"200\"}};\n  upstream_request_->encodeHeaders(response_headers, true);\n\n  ASSERT_TRUE(response->waitForEndStream());\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"200\", response->headers().getStatusValue());\n}\n\nvoid HttpTimeoutIntegrationTest::testIsTimeoutRetryHeader(bool use_hedged_retry) {\n  auto host = config_helper_.createVirtualHost(\"example.com\", \"/test_retry\");\n  host.set_include_is_timeout_retry_header(true);\n  config_helper_.addVirtualHost(host);\n\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(Http::TestRequestHeaderMapImpl{\n      {\":method\", \"POST\"},\n      {\":path\", \"/test_retry\"},\n      {\":scheme\", \"http\"},\n      {\":authority\", \"example.com\"},\n      {\"x-forwarded-for\", \"10.0.0.1\"},\n      {\"x-envoy-retry-on\", \"5xx\"},\n      {\"x-envoy-hedge-on-per-try-timeout\", use_hedged_retry ? \"true\" : \"fase\"},\n      {\"x-envoy-upstream-rq-timeout-ms\", \"500\"},\n      {\"x-envoy-upstream-rq-per-try-timeout-ms\", \"400\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger per try timeout (but not global timeout).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(400));\n\n  // Trigger retry (there's a 25ms backoff before it's issued).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(26));\n\n  // Wait for a second request to be sent upstream\n  FakeStreamPtr upstream_request2;\n\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request2));\n\n  ASSERT_TRUE(upstream_request2->waitForHeadersComplete());\n\n  // Expect the x-envoy-is-timeout-header to set to indicate to the upstream this is a retry\n  // initiated by a previous per try timeout.\n  EXPECT_EQ(upstream_request2->headers().getEnvoyIsTimeoutRetryValue(), \"true\");\n\n  ASSERT_TRUE(upstream_request2->waitForEndStream(*dispatcher_));\n\n  Http::TestResponseHeaderMapImpl response_headers{{\":status\", \"200\"}};\n\n  if (use_hedged_retry) {\n    upstream_request_->encodeHeaders(response_headers, true);\n    ASSERT_TRUE(response->waitForEndStream());\n    // The second request should be reset since we used the response from the first request.\n    ASSERT_TRUE(upstream_request2->waitForReset(std::chrono::seconds(15)));\n  } else {\n    upstream_request2->encodeHeaders(response_headers, true);\n    ASSERT_TRUE(response->waitForEndStream());\n  }\n\n  codec_client_->close();\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"200\", response->headers().getStatusValue());\n}\n\n// With hedge_on_per_try_timeout enabled via config, sends a request with a\n// global timeout and per try timeout specified, sleeps for longer than the per\n// try but slightly less than the global timeout. We expect a retry to be sent\n// upstream and the x-envoy-is-timeout-retry request header to be set to true.\nTEST_P(HttpTimeoutIntegrationTest, IsTimeoutRetryHeaderHedgedRetry) {\n  testIsTimeoutRetryHeader(true);\n}\n\n// Sends a request with a per try timeout specified, sleeps for longer than the per\n// try but slightly less than the global timeout. We expect a retry to be sent\n// upstream and the x-envoy-is-timeout-retry request header to be set to true.\nTEST_P(HttpTimeoutIntegrationTest, IsTimeoutRetryHeaderPerTryTimeout) {\n  testIsTimeoutRetryHeader(false);\n}\n\n// With hedge_on_per_try_timeout enabled via config, sends a request with a\n// global timeout and per try timeout specified, sleeps for longer than the per\n// try but slightly less than the global timeout. We then have the first\n// upstream request return headers and expect those to be returned downstream\n// (which proves the request was not canceled when the timeout was hit).\nTEST_P(HttpTimeoutIntegrationTest, HedgedPerTryTimeout) {\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(\n      Http::TestRequestHeaderMapImpl{{\":method\", \"POST\"},\n                                     {\":path\", \"/test/long/url\"},\n                                     {\":scheme\", \"http\"},\n                                     {\":authority\", \"host\"},\n                                     {\"x-forwarded-for\", \"10.0.0.1\"},\n                                     {\"x-envoy-retry-on\", \"5xx\"},\n                                     {\"x-envoy-hedge-on-per-try-timeout\", \"true\"},\n                                     {\"x-envoy-upstream-rq-timeout-ms\", \"500\"},\n                                     {\"x-envoy-upstream-rq-per-try-timeout-ms\", \"400\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger per try timeout (but not global timeout).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(400));\n\n  // Trigger retry (there's a 25ms backoff before it's issued).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(26));\n\n  // Wait for a second request to be sent upstream\n  FakeStreamPtr upstream_request2;\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request2));\n  ASSERT_TRUE(upstream_request2->waitForHeadersComplete());\n  ASSERT_TRUE(upstream_request2->waitForEndStream(*dispatcher_));\n\n  // Encode 200 response headers for the first (timed out) request.\n  Http::TestResponseHeaderMapImpl response_headers{{\":status\", \"200\"}};\n  upstream_request_->encodeHeaders(response_headers, true);\n\n  ASSERT_TRUE(response->waitForEndStream());\n\n  // The second request should be reset since we used the response from the first request.\n  ASSERT_TRUE(upstream_request2->waitForReset(std::chrono::seconds(15)));\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_EQ(0U, upstream_request_->bodyLength());\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"200\", response->headers().getStatusValue());\n}\n\nTEST_P(HttpTimeoutIntegrationTest, HedgedPerTryTimeoutWithBodyNoBufferFirstRequestWins) {\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024, 512, true);\n}\n\nTEST_P(HttpTimeoutIntegrationTest, HedgedPerTryTimeoutWithBodyNoBufferSecondRequestWins) {\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024, 512, false);\n}\n\nTEST_P(HttpTimeoutIntegrationTest,\n       HedgedPerTryTimeoutLowUpstreamBufferLimitLargeRequestFirstRequestWins) {\n  config_helper_.setBufferLimits(1024, 1024 * 1024); // Set buffer limits upstream and downstream.\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024 * 1024, 1024, true);\n}\n\nTEST_P(HttpTimeoutIntegrationTest,\n       HedgedPerTryTimeoutLowUpstreamBufferLimitLargeRequestSecondRequestWins) {\n  config_helper_.setBufferLimits(1024, 1024 * 1024); // Set buffer limits upstream and downstream.\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024 * 1024, 1024, false);\n}\n\nTEST_P(HttpTimeoutIntegrationTest,\n       HedgedPerTryTimeoutLowDownstreamBufferLimitLargeResponseFirstRequestWins) {\n  config_helper_.setBufferLimits(1024 * 1024, 1024); // Set buffer limits upstream and downstream.\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024, 1024 * 1024, true);\n}\n\nTEST_P(HttpTimeoutIntegrationTest,\n       HedgedPerTryTimeoutLowDownstreamBufferLimitLargeResponseSecondRequestWins) {\n  config_helper_.setBufferLimits(1024 * 1024, 1024); // Set buffer limits upstream and downstream.\n  testRouterRequestAndResponseWithHedgedPerTryTimeout(1024, 1024 * 1024, false);\n}\n\n// Sends a request with x-envoy-hedge-on-per-try-timeout, sleeps (with\n// simulated time) for longer than the per try timeout but shorter than the\n// global timeout, asserts that a retry is sent, and then responds with a 200\n// response on the original request and ensures the downstream sees it.\n// Request/response/header size are configurable to test flow control. If\n// first_request_wins is true, then the \"winning\" response will be sent in\n// response to the first (timed out) request. If false, the second request will\n// get the good response.\nvoid HttpTimeoutIntegrationTest::testRouterRequestAndResponseWithHedgedPerTryTimeout(\n    uint64_t request_size, uint64_t response_size, bool first_request_wins) {\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  Http::TestRequestHeaderMapImpl request_headers{{\":method\", \"POST\"},\n                                                 {\":path\", \"/test/long/url\"},\n                                                 {\":scheme\", \"http\"},\n                                                 {\":authority\", \"host\"},\n                                                 {\"x-forwarded-for\", \"10.0.0.1\"},\n                                                 {\"x-envoy-retry-on\", \"5xx\"},\n                                                 {\"x-envoy-hedge-on-per-try-timeout\", \"true\"},\n                                                 {\"x-envoy-upstream-rq-timeout-ms\", \"5000\"},\n                                                 {\"x-envoy-upstream-rq-per-try-timeout-ms\", \"400\"}};\n  auto encoder_decoder = codec_client_->startRequest(request_headers);\n\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n\n  codec_client_->sendData(*request_encoder_, request_size, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger per try timeout (but not global timeout).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(400));\n\n  FakeStreamPtr upstream_request2;\n  // Trigger retry (there's a 25ms backoff before it's issued).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(26));\n\n  // Wait for a second request to be sent upstream\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request2));\n  ASSERT_TRUE(upstream_request2->waitForHeadersComplete());\n  ASSERT_TRUE(upstream_request2->waitForEndStream(*dispatcher_));\n\n  Http::TestResponseHeaderMapImpl response_headers{{\":status\", \"200\"}};\n  if (first_request_wins) {\n    // Encode 200 response headers for the first (timed out) request.\n    upstream_request_->encodeHeaders(response_headers, response_size == 0);\n  } else {\n    // Encode 200 response headers for the second request.\n    upstream_request2->encodeHeaders(response_headers, response_size == 0);\n  }\n\n  response->waitForHeaders();\n\n  if (first_request_wins) {\n    // The second request should be reset since we used the response from the first request.\n    ASSERT_TRUE(upstream_request2->waitForReset(std::chrono::seconds(15)));\n  } else {\n    // The first request should be reset since we used the response from the second request.\n    ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n  }\n\n  if (response_size) {\n    if (first_request_wins) {\n      upstream_request_->encodeData(response_size, true);\n    } else {\n      upstream_request2->encodeData(response_size, true);\n    }\n  }\n\n  ASSERT_TRUE(response->waitForEndStream());\n\n  codec_client_->close();\n\n  EXPECT_TRUE(upstream_request_->complete());\n  EXPECT_TRUE(upstream_request2->complete());\n  if (first_request_wins) {\n    EXPECT_EQ(request_size, upstream_request_->bodyLength());\n  } else {\n    EXPECT_EQ(request_size, upstream_request2->bodyLength());\n  }\n\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"200\", response->headers().getStatusValue());\n}\n\n// Starts a request with a header timeout specified, sleeps for longer than the\n// timeout, and ensures that a timeout is received.\nTEST_P(HttpTimeoutIntegrationTest, RequestHeaderTimeout) {\n  if (downstreamProtocol() != Http::CodecType::HTTP1) {\n    // This test requires that the downstream be using HTTP1.\n    return;\n  }\n\n  config_helper_.addConfigModifier(\n      [&](envoy::extensions::filters::network::http_connection_manager::v3::HttpConnectionManager&\n              hcm) {\n        auto* request_headers_timeout = hcm.mutable_request_headers_timeout();\n        request_headers_timeout->set_seconds(1);\n        request_headers_timeout->set_nanos(0);\n      });\n  initialize();\n\n  const std::string input_request = (\"GET / HTTP/1.1\\r\\n\"\n                                     // Omit trailing \\r\\n that would indicate the end of headers.\n                                     \"Host: localhost\\r\\n\");\n  std::string response;\n\n  auto connection_driver = createConnectionDriver(\n      lookupPort(\"http\"), input_request,\n      [&response](Network::ClientConnection&, const Buffer::Instance& data) -> void {\n        response.append(data.toString());\n      });\n\n  while (!connection_driver->allBytesSent()) {\n    ASSERT_TRUE(connection_driver->run(Event::Dispatcher::RunType::NonBlock));\n  }\n  test_server_->waitForGaugeGe(\"http.config_test.downstream_rq_active\", 1);\n  ASSERT_FALSE(connection_driver->closed());\n\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(1001));\n  ASSERT_TRUE(connection_driver->run());\n\n  // The upstream should send a 40x response and send a local reply.\n  EXPECT_TRUE(connection_driver->closed());\n  EXPECT_THAT(response, AllOf(HasSubstr(\"408\"), HasSubstr(\"header\")));\n}\n\n// Validate that Envoy correctly handles per try and per try IDLE timeouts\n// that are firing within the backoff interval.\nTEST_P(HttpTimeoutIntegrationTest, OriginalRequestCompletesBeforeBackoffTimer) {\n  auto host = config_helper_.createVirtualHost(\"example.com\", \"/test_retry\");\n  host.set_include_is_timeout_retry_header(true);\n  config_helper_.addVirtualHost(host);\n  config_helper_.addConfigModifier(\n      [&](envoy::extensions::filters::network::http_connection_manager::v3::HttpConnectionManager&\n              hcm) -> void {\n        auto* route_config = hcm.mutable_route_config();\n        auto* virtual_host = route_config->mutable_virtual_hosts(1);\n        auto* route = virtual_host->mutable_routes(0)->mutable_route();\n        auto* retry_policy = route->mutable_retry_policy();\n        retry_policy->mutable_per_try_idle_timeout()->set_seconds(0);\n        // per try IDLE timeout is 400 ms\n        retry_policy->mutable_per_try_idle_timeout()->set_nanos(400 * 1000 * 1000);\n      });\n  initialize();\n\n  codec_client_ = makeHttpConnection(makeClientConnection(lookupPort(\"http\")));\n  auto encoder_decoder = codec_client_->startRequest(Http::TestRequestHeaderMapImpl{\n      {\":method\", \"POST\"},\n      {\":path\", \"/test_retry\"},\n      {\":scheme\", \"http\"},\n      {\":authority\", \"example.com\"},\n      {\"x-forwarded-for\", \"10.0.0.1\"},\n      {\"x-envoy-retry-on\", \"5xx\"},\n      // Enable hedge_on_per_try_timeout so that original request is not reset\n      {\"x-envoy-hedge-on-per-try-timeout\", \"true\"},\n      {\"x-envoy-upstream-rq-timeout-ms\", \"500\"},\n      // Make per try timeout the same as the per try idle timeout\n      // NOTE: it can be a bit longer, within the back off interval\n      {\"x-envoy-upstream-rq-per-try-timeout-ms\", \"400\"}});\n  auto response = std::move(encoder_decoder.second);\n  request_encoder_ = &encoder_decoder.first;\n\n  ASSERT_TRUE(fake_upstreams_[0]->waitForHttpConnection(*dispatcher_, fake_upstream_connection_));\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request_));\n  ASSERT_TRUE(upstream_request_->waitForHeadersComplete());\n  codec_client_->sendData(*request_encoder_, 0, true);\n\n  ASSERT_TRUE(upstream_request_->waitForEndStream(*dispatcher_));\n\n  // Trigger per try timeout (but not global timeout). This will actually trigger\n  // both IDLE and request timeouts in the same I/O operation.\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(400));\n\n  // Trigger retry (there's a 25ms backoff before it's issued).\n  timeSystem().advanceTimeWait(std::chrono::milliseconds(26));\n\n  // Wait for a second request to be sent upstream\n  FakeStreamPtr upstream_request2;\n\n  ASSERT_TRUE(fake_upstream_connection_->waitForNewStream(*dispatcher_, upstream_request2));\n\n  ASSERT_TRUE(upstream_request2->waitForHeadersComplete());\n\n  // Expect the x-envoy-is-timeout-header to set to indicate to the upstream this is a retry\n  // initiated by a previous per try timeout.\n  EXPECT_EQ(upstream_request2->headers().getEnvoyIsTimeoutRetryValue(), \"true\");\n\n  ASSERT_TRUE(upstream_request2->waitForEndStream(*dispatcher_));\n\n  Http::TestResponseHeaderMapImpl response_headers{{\":status\", \"200\"}};\n\n  // Respond to the second request (it does not matter which request gets response).\n  upstream_request2->encodeHeaders(response_headers, true);\n  ASSERT_TRUE(response->waitForEndStream());\n\n  // The first request should be reset since we used the response from the second request.\n  ASSERT_TRUE(upstream_request_->waitForReset(std::chrono::seconds(15)));\n\n  codec_client_->close();\n  EXPECT_TRUE(response->complete());\n  EXPECT_EQ(\"200\", response->headers().getStatusValue());\n}\n\n} // namespace Envoy\n"], "filenames": ["changelogs/current.yaml", "source/common/router/router.cc", "source/common/router/upstream_request.cc", "test/integration/http_timeout_integration_test.cc"], "buggy_code_start_loc": [57, 1117, 512, 633], "buggy_code_end_loc": [57, 1117, 517, 633], "fixing_code_start_loc": [58, 1118, 513, 634], "fixing_code_end_loc": [61, 1119, 527, 711], "type": "CWE-416", "message": "Envoy is a high-performance edge/middle/service proxy. Envoy will crash when certain timeouts happen within the same interval. The crash occurs when the following are true: 1. hedge_on_per_try_timeout is enabled, 2. per_try_idle_timeout is enabled (it can only be done in configuration), 3. per-try-timeout is enabled, either through headers or configuration and its value is equal, or within the backoff interval of the per_try_idle_timeout. This issue has been addressed in released 1.29.1, 1.28.1, 1.27.3, and 1.26.7. Users are advised to upgrade. There are no known workarounds for this vulnerability.", "other": {"cve": {"id": "CVE-2024-23322", "sourceIdentifier": "security-advisories@github.com", "published": "2024-02-09T23:15:08.747", "lastModified": "2024-02-15T04:48:20.247", "vulnStatus": "Analyzed", "descriptions": [{"lang": "en", "value": "Envoy is a high-performance edge/middle/service proxy. Envoy will crash when certain timeouts happen within the same interval. The crash occurs when the following are true: 1. hedge_on_per_try_timeout is enabled, 2. per_try_idle_timeout is enabled (it can only be done in configuration), 3. per-try-timeout is enabled, either through headers or configuration and its value is equal, or within the backoff interval of the per_try_idle_timeout. This issue has been addressed in released 1.29.1, 1.28.1, 1.27.3, and 1.26.7. Users are advised to upgrade. There are no known workarounds for this vulnerability."}, {"lang": "es", "value": "Envoy es un proxy de servicio/intermedio/perimetral de alto rendimiento. Envoy se bloquear\u00e1 cuando se produzcan ciertos tiempos de espera dentro del mismo intervalo. El bloqueo ocurre cuando se cumple lo siguiente: 1. hedge_on_per_try_timeout est\u00e1 habilitado, 2. per_try_idle_timeout est\u00e1 habilitado (solo se puede hacer en la configuraci\u00f3n), 3. per-try-timeout est\u00e1 habilitado, ya sea a trav\u00e9s de encabezados o configuraci\u00f3n y su valor es igual , o dentro del intervalo de espera de per_try_idle_timeout. Este problema se solucion\u00f3 en las versiones 1.29.1, 1.28.1, 1.27.3 y 1.26.7. Se recomienda a los usuarios que actualicen. No se conocen workarounds para esta vulnerabilidad."}], "metrics": {"cvssMetricV31": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}, {"source": "security-advisories@github.com", "type": "Secondary", "cvssData": {"version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H", "attackVector": "NETWORK", "attackComplexity": "LOW", "privilegesRequired": "NONE", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 7.5, "baseSeverity": "HIGH"}, "exploitabilityScore": 3.9, "impactScore": 3.6}]}, "weaknesses": [{"source": "security-advisories@github.com", "type": "Primary", "description": [{"lang": "en", "value": "CWE-416"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:a:envoyproxy:envoy:*:*:*:*:*:*:*:*", "versionStartIncluding": "1.26.0", "versionEndExcluding": "1.26.7", "matchCriteriaId": "0324E095-98B4-4B78-9242-989EC45E011F"}, {"vulnerable": true, "criteria": "cpe:2.3:a:envoyproxy:envoy:*:*:*:*:*:*:*:*", "versionStartIncluding": "1.27.0", "versionEndExcluding": "1.27.3", "matchCriteriaId": "2E838B16-C6DC-4701-B955-D96D4CAEF4F6"}, {"vulnerable": true, "criteria": "cpe:2.3:a:envoyproxy:envoy:*:*:*:*:*:*:*:*", "versionStartIncluding": "1.28.0", "versionEndExcluding": "1.28.1", "matchCriteriaId": "770D5713-48E3-4F9B-B05C-9CB9C6B272E3"}, {"vulnerable": true, "criteria": "cpe:2.3:a:envoyproxy:envoy:*:*:*:*:*:*:*:*", "versionStartIncluding": "1.29.0", "versionEndExcluding": "1.29.1", "matchCriteriaId": "638F3351-3ACD-47C8-9B8F-568A930FAECA"}]}]}], "references": [{"url": "https://github.com/envoyproxy/envoy/commit/843f9e6a123ed47ce139b421c14e7126f2ac685e", "source": "security-advisories@github.com", "tags": ["Patch", "Vendor Advisory"]}, {"url": "https://github.com/envoyproxy/envoy/security/advisories/GHSA-6p83-mfmh-qv38", "source": "security-advisories@github.com", "tags": ["Vendor Advisory"]}]}, "github_commit_url": "https://github.com/envoyproxy/envoy/commit/843f9e6a123ed47ce139b421c14e7126f2ac685e"}}